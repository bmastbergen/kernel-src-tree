perf tools: Add stat config event read function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jiri Olsa <jolsa@kernel.org>
commit 8e381596b67af53564a69f16440d3e5d5a73d034
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/8e381596.failed

Introducing the perf_event__read_stat_config function to read a struct
perf_stat_config object data from a stat config event.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Tested-by: Kan Liang <kan.liang@intel.com>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
Link: http://lkml.kernel.org/r/1445784728-21732-14-git-send-email-jolsa@kernel.org
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit 8e381596b67af53564a69f16440d3e5d5a73d034)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/tests/stat.c
#	tools/perf/util/event.c
#	tools/perf/util/event.h
diff --cc tools/perf/util/event.c
index 763fbcde76fc,223deaf2fba7..000000000000
--- a/tools/perf/util/event.c
+++ b/tools/perf/util/event.c
@@@ -687,6 -704,235 +687,238 @@@ int perf_event__synthesize_kernel_mmap(
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ int perf_event__synthesize_thread_map2(struct perf_tool *tool,
+ 				      struct thread_map *threads,
+ 				      perf_event__handler_t process,
+ 				      struct machine *machine)
+ {
+ 	union perf_event *event;
+ 	int i, err, size;
+ 
+ 	size  = sizeof(event->thread_map);
+ 	size +=	threads->nr * sizeof(event->thread_map.entries[0]);
+ 
+ 	event = zalloc(size);
+ 	if (!event)
+ 		return -ENOMEM;
+ 
+ 	event->header.type = PERF_RECORD_THREAD_MAP;
+ 	event->header.size = size;
+ 	event->thread_map.nr = threads->nr;
+ 
+ 	for (i = 0; i < threads->nr; i++) {
+ 		struct thread_map_event_entry *entry = &event->thread_map.entries[i];
+ 		char *comm = thread_map__comm(threads, i);
+ 
+ 		if (!comm)
+ 			comm = (char *) "";
+ 
+ 		entry->pid = thread_map__pid(threads, i);
+ 		strncpy((char *) &entry->comm, comm, sizeof(entry->comm));
+ 	}
+ 
+ 	err = process(tool, event, NULL, machine);
+ 
+ 	free(event);
+ 	return err;
+ }
+ 
+ static void synthesize_cpus(struct cpu_map_entries *cpus,
+ 			    struct cpu_map *map)
+ {
+ 	int i;
+ 
+ 	cpus->nr = map->nr;
+ 
+ 	for (i = 0; i < map->nr; i++)
+ 		cpus->cpu[i] = map->map[i];
+ }
+ 
+ static void synthesize_mask(struct cpu_map_mask *mask,
+ 			    struct cpu_map *map, int max)
+ {
+ 	int i;
+ 
+ 	mask->nr = BITS_TO_LONGS(max);
+ 	mask->long_size = sizeof(long);
+ 
+ 	for (i = 0; i < map->nr; i++)
+ 		set_bit(map->map[i], mask->mask);
+ }
+ 
+ static size_t cpus_size(struct cpu_map *map)
+ {
+ 	return sizeof(struct cpu_map_entries) + map->nr * sizeof(u16);
+ }
+ 
+ static size_t mask_size(struct cpu_map *map, int *max)
+ {
+ 	int i;
+ 
+ 	*max = 0;
+ 
+ 	for (i = 0; i < map->nr; i++) {
+ 		/* bit possition of the cpu is + 1 */
+ 		int bit = map->map[i] + 1;
+ 
+ 		if (bit > *max)
+ 			*max = bit;
+ 	}
+ 
+ 	return sizeof(struct cpu_map_mask) + BITS_TO_LONGS(*max) * sizeof(long);
+ }
+ 
+ void *cpu_map_data__alloc(struct cpu_map *map, size_t *size, u16 *type, int *max)
+ {
+ 	size_t size_cpus, size_mask;
+ 	bool is_dummy = cpu_map__empty(map);
+ 
+ 	/*
+ 	 * Both array and mask data have variable size based
+ 	 * on the number of cpus and their actual values.
+ 	 * The size of the 'struct cpu_map_data' is:
+ 	 *
+ 	 *   array = size of 'struct cpu_map_entries' +
+ 	 *           number of cpus * sizeof(u64)
+ 	 *
+ 	 *   mask  = size of 'struct cpu_map_mask' +
+ 	 *           maximum cpu bit converted to size of longs
+ 	 *
+ 	 * and finaly + the size of 'struct cpu_map_data'.
+ 	 */
+ 	size_cpus = cpus_size(map);
+ 	size_mask = mask_size(map, max);
+ 
+ 	if (is_dummy || (size_cpus < size_mask)) {
+ 		*size += size_cpus;
+ 		*type  = PERF_CPU_MAP__CPUS;
+ 	} else {
+ 		*size += size_mask;
+ 		*type  = PERF_CPU_MAP__MASK;
+ 	}
+ 
+ 	*size += sizeof(struct cpu_map_data);
+ 	return zalloc(*size);
+ }
+ 
+ void cpu_map_data__synthesize(struct cpu_map_data *data, struct cpu_map *map,
+ 			      u16 type, int max)
+ {
+ 	data->type = type;
+ 
+ 	switch (type) {
+ 	case PERF_CPU_MAP__CPUS:
+ 		synthesize_cpus((struct cpu_map_entries *) data->data, map);
+ 		break;
+ 	case PERF_CPU_MAP__MASK:
+ 		synthesize_mask((struct cpu_map_mask *) data->data, map, max);
+ 	default:
+ 		break;
+ 	};
+ }
+ 
+ static struct cpu_map_event* cpu_map_event__new(struct cpu_map *map)
+ {
+ 	size_t size = sizeof(struct cpu_map_event);
+ 	struct cpu_map_event *event;
+ 	int max;
+ 	u16 type;
+ 
+ 	event = cpu_map_data__alloc(map, &size, &type, &max);
+ 	if (!event)
+ 		return NULL;
+ 
+ 	event->header.type = PERF_RECORD_CPU_MAP;
+ 	event->header.size = size;
+ 	event->data.type   = type;
+ 
+ 	cpu_map_data__synthesize(&event->data, map, type, max);
+ 	return event;
+ }
+ 
+ int perf_event__synthesize_cpu_map(struct perf_tool *tool,
+ 				   struct cpu_map *map,
+ 				   perf_event__handler_t process,
+ 				   struct machine *machine)
+ {
+ 	struct cpu_map_event *event;
+ 	int err;
+ 
+ 	event = cpu_map_event__new(map);
+ 	if (!event)
+ 		return -ENOMEM;
+ 
+ 	err = process(tool, (union perf_event *) event, NULL, machine);
+ 
+ 	free(event);
+ 	return err;
+ }
+ 
+ int perf_event__synthesize_stat_config(struct perf_tool *tool,
+ 				       struct perf_stat_config *config,
+ 				       perf_event__handler_t process,
+ 				       struct machine *machine)
+ {
+ 	struct stat_config_event *event;
+ 	int size, i = 0, err;
+ 
+ 	size  = sizeof(*event);
+ 	size += (PERF_STAT_CONFIG_TERM__MAX * sizeof(event->data[0]));
+ 
+ 	event = zalloc(size);
+ 	if (!event)
+ 		return -ENOMEM;
+ 
+ 	event->header.type = PERF_RECORD_STAT_CONFIG;
+ 	event->header.size = size;
+ 	event->nr          = PERF_STAT_CONFIG_TERM__MAX;
+ 
+ #define ADD(__term, __val)					\
+ 	event->data[i].tag = PERF_STAT_CONFIG_TERM__##__term;	\
+ 	event->data[i].val = __val;				\
+ 	i++;
+ 
+ 	ADD(AGGR_MODE,	config->aggr_mode)
+ 	ADD(INTERVAL,	config->interval)
+ 	ADD(SCALE,	config->scale)
+ 
+ 	WARN_ONCE(i != PERF_STAT_CONFIG_TERM__MAX,
+ 		  "stat config terms unbalanced\n");
+ #undef ADD
+ 
+ 	err = process(tool, (union perf_event *) event, NULL, machine);
+ 
+ 	free(event);
+ 	return err;
+ }
+ 
+ void perf_event__read_stat_config(struct perf_stat_config *config,
+ 				  struct stat_config_event *event)
+ {
+ 	unsigned i;
+ 
+ 	for (i = 0; i < event->nr; i++) {
+ 
+ 		switch (event->data[i].tag) {
+ #define CASE(__term, __val)					\
+ 		case PERF_STAT_CONFIG_TERM__##__term:		\
+ 			config->__val = event->data[i].val;	\
+ 			break;
+ 
+ 		CASE(AGGR_MODE, aggr_mode)
+ 		CASE(SCALE,     scale)
+ 		CASE(INTERVAL,  interval)
+ #undef CASE
+ 		default:
+ 			pr_warning("unknown stat config term %" PRIu64 "\n",
+ 				   event->data[i].tag);
+ 		}
+ 	}
+ }
+ 
++>>>>>>> 8e381596b67a (perf tools: Add stat config event read function)
  size_t perf_event__fprintf_comm(union perf_event *event, FILE *fp)
  {
  	const char *s;
diff --cc tools/perf/util/event.h
index 9400ef1c1335,4e87be2e1afa..000000000000
--- a/tools/perf/util/event.h
+++ b/tools/perf/util/event.h
@@@ -393,6 -473,12 +393,15 @@@ int perf_event__synthesize_threads(stru
  int perf_event__synthesize_kernel_mmap(struct perf_tool *tool,
  				       perf_event__handler_t process,
  				       struct machine *machine);
++<<<<<<< HEAD
++=======
+ int perf_event__synthesize_stat_config(struct perf_tool *tool,
+ 				       struct perf_stat_config *config,
+ 				       perf_event__handler_t process,
+ 				       struct machine *machine);
+ void perf_event__read_stat_config(struct perf_stat_config *config,
+ 				  struct stat_config_event *event);
++>>>>>>> 8e381596b67a (perf tools: Add stat config event read function)
  
  int perf_event__synthesize_modules(struct perf_tool *tool,
  				   perf_event__handler_t process,
* Unmerged path tools/perf/tests/stat.c
* Unmerged path tools/perf/tests/stat.c
* Unmerged path tools/perf/util/event.c
* Unmerged path tools/perf/util/event.h
