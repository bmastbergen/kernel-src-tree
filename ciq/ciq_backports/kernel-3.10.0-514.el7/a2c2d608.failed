staging/rdma/hfi1: Remove create_qp functionality

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [infiniband] rdma/hfi1: Remove create_qp functionality (Alex Estrin) [1272062 1273170]
Rebuild_FUZZ: 91.11%
commit-author Dennis Dalessandro <dennis.dalessandro@intel.com>
commit a2c2d608957c1b6f444e092fa7f49c1f1ac7fa0a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a2c2d608.failed

Rely on rdmavt to provide queue pair creation.

	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Reviewed-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit a2c2d608957c1b6f444e092fa7f49c1f1ac7fa0a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/qp.c
#	drivers/staging/hfi1/qp.h
#	drivers/staging/hfi1/verbs.c
diff --cc drivers/staging/hfi1/qp.c
index 9ffed6e14d8e,a336d2a40d58..000000000000
--- a/drivers/staging/hfi1/qp.c
+++ b/drivers/staging/hfi1/qp.c
@@@ -60,10 -60,7 +60,14 @@@
  #include "trace.h"
  #include "sdma.h"
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +#define BITS_PER_PAGE           (PAGE_SIZE*BITS_PER_BYTE)
 +#define BITS_PER_PAGE_MASK      (BITS_PER_PAGE-1)
 +
 +static unsigned int hfi1_qp_table_size = 256;
++=======
+ unsigned int hfi1_qp_table_size = 256;
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/qp.c
  module_param_named(qp_table_size, hfi1_qp_table_size, uint, S_IRUGO);
  MODULE_PARM_DESC(qp_table_size, "QP table size");
  
@@@ -118,111 -115,13 +122,115 @@@ static const u16 credit_table[31] = 
  	32768                   /* 1E */
  };
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +static void get_map_page(struct hfi1_qpn_table *qpt, struct qpn_map *map)
 +{
 +	unsigned long page = get_zeroed_page(GFP_KERNEL);
 +
 +	/*
 +	 * Free the page if someone raced with us installing it.
 +	 */
 +
 +	spin_lock(&qpt->lock);
 +	if (map->page)
 +		free_page(page);
 +	else
 +		map->page = (void *)page;
 +	spin_unlock(&qpt->lock);
 +}
 +
 +/*
 + * Allocate the next available QPN or
 + * zero/one for QP type IB_QPT_SMI/IB_QPT_GSI.
 + */
 +static int alloc_qpn(struct hfi1_devdata *dd, struct hfi1_qpn_table *qpt,
 +		     enum ib_qp_type type, u8 port)
 +{
 +	u32 i, offset, max_scan, qpn;
 +	struct qpn_map *map;
 +	u32 ret;
 +
 +	if (type == IB_QPT_SMI || type == IB_QPT_GSI) {
 +		unsigned n;
 +
 +		ret = type == IB_QPT_GSI;
 +		n = 1 << (ret + 2 * (port - 1));
 +		spin_lock(&qpt->lock);
 +		if (qpt->flags & n)
 +			ret = -EINVAL;
 +		else
 +			qpt->flags |= n;
 +		spin_unlock(&qpt->lock);
 +		goto bail;
 +	}
 +
 +	qpn = qpt->last + qpt->incr;
 +	if (qpn >= QPN_MAX)
 +		qpn = qpt->incr | ((qpt->last & 1) ^ 1);
 +	/* offset carries bit 0 */
 +	offset = qpn & BITS_PER_PAGE_MASK;
 +	map = &qpt->map[qpn / BITS_PER_PAGE];
 +	max_scan = qpt->nmaps - !offset;
 +	for (i = 0;;) {
 +		if (unlikely(!map->page)) {
 +			get_map_page(qpt, map);
 +			if (unlikely(!map->page))
 +				break;
 +		}
 +		do {
 +			if (!test_and_set_bit(offset, map->page)) {
 +				qpt->last = qpn;
 +				ret = qpn;
 +				goto bail;
 +			}
 +			offset += qpt->incr;
 +			/*
 +			 * This qpn might be bogus if offset >= BITS_PER_PAGE.
 +			 * That is OK.   It gets re-assigned below
 +			 */
 +			qpn = mk_qpn(qpt, map, offset);
 +		} while (offset < BITS_PER_PAGE && qpn < QPN_MAX);
 +		/*
 +		 * In order to keep the number of pages allocated to a
 +		 * minimum, we scan the all existing pages before increasing
 +		 * the size of the bitmap table.
 +		 */
 +		if (++i > max_scan) {
 +			if (qpt->nmaps == QPNMAP_ENTRIES)
 +				break;
 +			map = &qpt->map[qpt->nmaps++];
 +			/* start at incr with current bit 0 */
 +			offset = qpt->incr | (offset & 1);
 +		} else if (map < &qpt->map[qpt->nmaps]) {
 +			++map;
 +			/* start at incr with current bit 0 */
 +			offset = qpt->incr | (offset & 1);
 +		} else {
 +			map = &qpt->map[0];
 +			/* wrap to first map page, invert bit 0 */
 +			offset = qpt->incr | ((offset & 1) ^ 1);
 +		}
 +		/* there can be no bits at shift and below */
 +		WARN_ON(offset & (dd->qos_shift - 1));
 +		qpn = mk_qpn(qpt, map, offset);
 +	}
 +
 +	ret = -ENOMEM;
 +
 +bail:
 +	return ret;
 +}
 +
 +static void free_qpn(struct hfi1_qpn_table *qpt, u32 qpn)
++=======
+ static void free_qpn(struct rvt_qpn_table *qpt, u32 qpn)
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/qp.c
  {
 -	struct rvt_qpn_map *map;
 +	struct qpn_map *map;
  
 -	map = qpt->map + qpn / RVT_BITS_PER_PAGE;
 +	map = qpt->map + qpn / BITS_PER_PAGE;
  	if (map->page)
 -		clear_bit(qpn & RVT_BITS_PER_PAGE_MASK, map->page);
 +		clear_bit(qpn & BITS_PER_PAGE_MASK, map->page);
  }
  
  /*
@@@ -297,112 -197,7 +305,116 @@@ static void remove_qp(struct hfi1_ibde
  	}
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +/**
 + * free_all_qps - check for QPs still in use
 + * @qpt: the QP table to empty
 + *
 + * There should not be any QPs still in use.
 + * Free memory for table.
 + */
 +static unsigned free_all_qps(struct hfi1_devdata *dd)
 +{
 +	struct hfi1_ibdev *dev = &dd->verbs_dev;
 +	unsigned long flags;
 +	struct hfi1_qp *qp;
 +	unsigned n, qp_inuse = 0;
 +
 +	for (n = 0; n < dd->num_pports; n++) {
 +		struct hfi1_ibport *ibp = &dd->pport[n].ibport_data;
 +
 +		if (!hfi1_mcast_tree_empty(ibp))
 +			qp_inuse++;
 +		rcu_read_lock();
 +		if (rcu_dereference(ibp->qp[0]))
 +			qp_inuse++;
 +		if (rcu_dereference(ibp->qp[1]))
 +			qp_inuse++;
 +		rcu_read_unlock();
 +	}
 +
 +	if (!dev->qp_dev)
 +		goto bail;
 +	spin_lock_irqsave(&dev->qp_dev->qpt_lock, flags);
 +	for (n = 0; n < dev->qp_dev->qp_table_size; n++) {
 +		qp = rcu_dereference_protected(dev->qp_dev->qp_table[n],
 +			lockdep_is_held(&dev->qp_dev->qpt_lock));
 +		RCU_INIT_POINTER(dev->qp_dev->qp_table[n], NULL);
 +
 +		for (; qp; qp = rcu_dereference_protected(qp->next,
 +				lockdep_is_held(&dev->qp_dev->qpt_lock)))
 +			qp_inuse++;
 +	}
 +	spin_unlock_irqrestore(&dev->qp_dev->qpt_lock, flags);
 +	synchronize_rcu();
 +bail:
 +	return qp_inuse;
 +}
 +
 +/**
 + * reset_qp - initialize the QP state to the reset state
 + * @qp: the QP to reset
 + * @type: the QP type
 + */
 +static void reset_qp(struct hfi1_qp *qp, enum ib_qp_type type)
 +{
 +	struct hfi1_qp_priv *priv = qp->priv;
 +	qp->remote_qpn = 0;
 +	qp->qkey = 0;
 +	qp->qp_access_flags = 0;
 +	iowait_init(
 +		&priv->s_iowait,
 +		1,
 +		hfi1_do_send,
 +		iowait_sleep,
 +		iowait_wakeup);
 +	qp->s_flags &= HFI1_S_SIGNAL_REQ_WR;
 +	qp->s_hdrwords = 0;
 +	qp->s_wqe = NULL;
 +	qp->s_draining = 0;
 +	qp->s_next_psn = 0;
 +	qp->s_last_psn = 0;
 +	qp->s_sending_psn = 0;
 +	qp->s_sending_hpsn = 0;
 +	qp->s_psn = 0;
 +	qp->r_psn = 0;
 +	qp->r_msn = 0;
 +	if (type == IB_QPT_RC) {
 +		qp->s_state = IB_OPCODE_RC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_RC_SEND_LAST;
 +	} else {
 +		qp->s_state = IB_OPCODE_UC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_UC_SEND_LAST;
 +	}
 +	qp->s_ack_state = IB_OPCODE_RC_ACKNOWLEDGE;
 +	qp->r_nak_state = 0;
 +	priv->r_adefered = 0;
 +	qp->r_aflags = 0;
 +	qp->r_flags = 0;
 +	qp->s_head = 0;
 +	qp->s_tail = 0;
 +	qp->s_cur = 0;
 +	qp->s_acked = 0;
 +	qp->s_last = 0;
 +	qp->s_ssn = 1;
 +	qp->s_lsn = 0;
 +	clear_ahg(qp);
 +	qp->s_mig_state = IB_MIG_MIGRATED;
 +	memset(qp->s_ack_queue, 0, sizeof(qp->s_ack_queue));
 +	qp->r_head_ack_queue = 0;
 +	qp->s_tail_ack_queue = 0;
 +	qp->s_num_rd_atomic = 0;
 +	if (qp->r_rq.wq) {
 +		qp->r_rq.wq->head = 0;
 +		qp->r_rq.wq->tail = 0;
 +	}
 +	qp->r_sge.num_sge = 0;
 +}
 +
 +static void clear_mr_refs(struct hfi1_qp *qp, int clr_sends)
++=======
+ static void clear_mr_refs(struct rvt_qp *qp, int clr_sends)
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/qp.c
  {
  	unsigned n;
  
@@@ -1023,253 -819,6 +1035,256 @@@ __be32 hfi1_compute_aeth(struct hfi1_q
  }
  
  /**
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 + * hfi1_create_qp - create a queue pair for a device
 + * @ibpd: the protection domain who's device we create the queue pair for
 + * @init_attr: the attributes of the queue pair
 + * @udata: user data for libibverbs.so
 + *
 + * Returns the queue pair on success, otherwise returns an errno.
 + *
 + * Called by the ib_create_qp() core verbs function.
 + */
 +struct ib_qp *hfi1_create_qp(struct ib_pd *ibpd,
 +			     struct ib_qp_init_attr *init_attr,
 +			     struct ib_udata *udata)
 +{
 +	struct hfi1_qp *qp;
 +	struct hfi1_qp_priv *priv;
 +	int err;
 +	struct hfi1_swqe *swq = NULL;
 +	struct hfi1_ibdev *dev;
 +	struct hfi1_devdata *dd;
 +	size_t sz;
 +	size_t sg_list_sz;
 +	struct ib_qp *ret;
 +
 +	if (init_attr->cap.max_send_sge > hfi1_max_sges ||
 +	    init_attr->cap.max_send_wr > hfi1_max_qp_wrs ||
 +	    init_attr->create_flags) {
 +		ret = ERR_PTR(-EINVAL);
 +		goto bail;
 +	}
 +
 +	/* Check receive queue parameters if no SRQ is specified. */
 +	if (!init_attr->srq) {
 +		if (init_attr->cap.max_recv_sge > hfi1_max_sges ||
 +		    init_attr->cap.max_recv_wr > hfi1_max_qp_wrs) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +		if (init_attr->cap.max_send_sge +
 +		    init_attr->cap.max_send_wr +
 +		    init_attr->cap.max_recv_sge +
 +		    init_attr->cap.max_recv_wr == 0) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +	}
 +
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_SMI:
 +	case IB_QPT_GSI:
 +		if (init_attr->port_num == 0 ||
 +		    init_attr->port_num > ibpd->device->phys_port_cnt) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +	case IB_QPT_UC:
 +	case IB_QPT_RC:
 +	case IB_QPT_UD:
 +		sz = sizeof(struct hfi1_sge) *
 +			init_attr->cap.max_send_sge +
 +			sizeof(struct hfi1_swqe);
 +		swq = vmalloc((init_attr->cap.max_send_wr + 1) * sz);
 +		if (swq == NULL) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail;
 +		}
 +		sz = sizeof(*qp);
 +		sg_list_sz = 0;
 +		if (init_attr->srq) {
 +			struct hfi1_srq *srq = to_isrq(init_attr->srq);
 +
 +			if (srq->rq.max_sge > 1)
 +				sg_list_sz = sizeof(*qp->r_sg_list) *
 +					(srq->rq.max_sge - 1);
 +		} else if (init_attr->cap.max_recv_sge > 1)
 +			sg_list_sz = sizeof(*qp->r_sg_list) *
 +				(init_attr->cap.max_recv_sge - 1);
 +		qp = kzalloc(sz + sg_list_sz, GFP_KERNEL);
 +		if (!qp) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_swq;
 +		}
 +		RCU_INIT_POINTER(qp->next, NULL);
 +		priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 +		if (!priv) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp_priv;
 +		}
 +		priv->owner = qp;
 +		priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), GFP_KERNEL);
 +		if (!priv->s_hdr) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp;
 +		}
 +		qp->priv = priv;
 +		qp->timeout_jiffies =
 +			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
 +				1000UL);
 +		if (init_attr->srq)
 +			sz = 0;
 +		else {
 +			qp->r_rq.size = init_attr->cap.max_recv_wr + 1;
 +			qp->r_rq.max_sge = init_attr->cap.max_recv_sge;
 +			sz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +
 +				sizeof(struct hfi1_rwqe);
 +			qp->r_rq.wq = vmalloc_user(sizeof(struct hfi1_rwq) +
 +						   qp->r_rq.size * sz);
 +			if (!qp->r_rq.wq) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_qp;
 +			}
 +		}
 +
 +		/*
 +		 * ib_create_qp() will initialize qp->ibqp
 +		 * except for qp->ibqp.qp_num.
 +		 */
 +		spin_lock_init(&qp->r_lock);
 +		spin_lock_init(&qp->s_lock);
 +		spin_lock_init(&qp->r_rq.lock);
 +		atomic_set(&qp->refcount, 0);
 +		init_waitqueue_head(&qp->wait);
 +		init_timer(&qp->s_timer);
 +		qp->s_timer.data = (unsigned long)qp;
 +		INIT_LIST_HEAD(&qp->rspwait);
 +		qp->state = IB_QPS_RESET;
 +		qp->s_wq = swq;
 +		qp->s_size = init_attr->cap.max_send_wr + 1;
 +		qp->s_max_sge = init_attr->cap.max_send_sge;
 +		if (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)
 +			qp->s_flags = HFI1_S_SIGNAL_REQ_WR;
 +		dev = to_idev(ibpd->device);
 +		dd = dd_from_dev(dev);
 +		err = alloc_qpn(dd, &dev->qp_dev->qpn_table, init_attr->qp_type,
 +				init_attr->port_num);
 +		if (err < 0) {
 +			ret = ERR_PTR(err);
 +			vfree(qp->r_rq.wq);
 +			goto bail_qp;
 +		}
 +		qp->ibqp.qp_num = err;
 +		qp->port_num = init_attr->port_num;
 +		reset_qp(qp, init_attr->qp_type);
 +
 +		break;
 +
 +	default:
 +		/* Don't support raw QPs */
 +		ret = ERR_PTR(-ENOSYS);
 +		goto bail;
 +	}
 +
 +	init_attr->cap.max_inline_data = 0;
 +
 +	/*
 +	 * Return the address of the RWQ as the offset to mmap.
 +	 * See hfi1_mmap() for details.
 +	 */
 +	if (udata && udata->outlen >= sizeof(__u64)) {
 +		if (!qp->r_rq.wq) {
 +			__u64 offset = 0;
 +
 +			err = ib_copy_to_udata(udata, &offset,
 +					       sizeof(offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		} else {
 +			u32 s = sizeof(struct hfi1_rwq) + qp->r_rq.size * sz;
 +
 +			qp->ip = hfi1_create_mmap_info(dev, s,
 +						      ibpd->uobject->context,
 +						      qp->r_rq.wq);
 +			if (!qp->ip) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_ip;
 +			}
 +
 +			err = ib_copy_to_udata(udata, &(qp->ip->offset),
 +					       sizeof(qp->ip->offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		}
 +	}
 +
 +	spin_lock(&dev->n_qps_lock);
 +	if (dev->n_qps_allocated == hfi1_max_qps) {
 +		spin_unlock(&dev->n_qps_lock);
 +		ret = ERR_PTR(-ENOMEM);
 +		goto bail_ip;
 +	}
 +
 +	dev->n_qps_allocated++;
 +	spin_unlock(&dev->n_qps_lock);
 +
 +	if (qp->ip) {
 +		spin_lock_irq(&dev->pending_lock);
 +		list_add(&qp->ip->pending_mmaps, &dev->pending_mmaps);
 +		spin_unlock_irq(&dev->pending_lock);
 +	}
 +
 +	ret = &qp->ibqp;
 +
 +	/*
 +	 * We have our QP and its good, now keep track of what types of opcodes
 +	 * can be processed on this QP. We do this by keeping track of what the
 +	 * 3 high order bits of the opcode are.
 +	 */
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_SMI:
 +	case IB_QPT_GSI:
 +	case IB_QPT_UD:
 +		qp->allowed_ops = IB_OPCODE_UD_SEND_ONLY & OPCODE_QP_MASK;
 +		break;
 +	case IB_QPT_RC:
 +		qp->allowed_ops = IB_OPCODE_RC_SEND_ONLY & OPCODE_QP_MASK;
 +		break;
 +	case IB_QPT_UC:
 +		qp->allowed_ops = IB_OPCODE_UC_SEND_ONLY & OPCODE_QP_MASK;
 +		break;
 +	default:
 +		ret = ERR_PTR(-EINVAL);
 +		goto bail_ip;
 +	}
 +
 +	goto bail;
 +
 +bail_ip:
 +	if (qp->ip)
 +		kref_put(&qp->ip->ref, hfi1_release_mmap_info);
 +	else
 +		vfree(qp->r_rq.wq);
 +	free_qpn(&dev->qp_dev->qpn_table, qp->ibqp.qp_num);
 +bail_qp:
 +	kfree(priv->s_hdr);
 +	kfree(priv);
 +bail_qp_priv:
 +	kfree(qp);
 +bail_swq:
 +	vfree(swq);
 +bail:
 +	return ret;
 +}
 +
 +/**
++=======
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/qp.c
   * hfi1_destroy_qp - destroy a queue pair
   * @ibqp: the queue pair to destroy
   *
@@@ -1325,61 -874,6 +1340,64 @@@ int hfi1_destroy_qp(struct ib_qp *ibqp
  }
  
  /**
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 + * init_qpn_table - initialize the QP number table for a device
 + * @qpt: the QPN table
 + */
 +static int init_qpn_table(struct hfi1_devdata *dd, struct hfi1_qpn_table *qpt)
 +{
 +	u32 offset, qpn, i;
 +	struct qpn_map *map;
 +	int ret = 0;
 +
 +	spin_lock_init(&qpt->lock);
 +
 +	qpt->last = 0;
 +	qpt->incr = 1 << dd->qos_shift;
 +
 +	/* insure we don't assign QPs from KDETH 64K window */
 +	qpn = kdeth_qp << 16;
 +	qpt->nmaps = qpn / BITS_PER_PAGE;
 +	/* This should always be zero */
 +	offset = qpn & BITS_PER_PAGE_MASK;
 +	map = &qpt->map[qpt->nmaps];
 +	dd_dev_info(dd, "Reserving QPNs for KDETH window from 0x%x to 0x%x\n",
 +		qpn, qpn + 65535);
 +	for (i = 0; i < 65536; i++) {
 +		if (!map->page) {
 +			get_map_page(qpt, map);
 +			if (!map->page) {
 +				ret = -ENOMEM;
 +				break;
 +			}
 +		}
 +		set_bit(offset, map->page);
 +		offset++;
 +		if (offset == BITS_PER_PAGE) {
 +			/* next page */
 +			qpt->nmaps++;
 +			map++;
 +			offset = 0;
 +		}
 +	}
 +	return ret;
 +}
 +
 +/**
 + * free_qpn_table - free the QP number table for a device
 + * @qpt: the QPN table
 + */
 +static void free_qpn_table(struct hfi1_qpn_table *qpt)
 +{
 +	int i;
 +
 +	for (i = 0; i < ARRAY_SIZE(qpt->map); i++)
 +		free_page((unsigned long) qpt->map[i].page);
 +}
 +
 +/**
++=======
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/qp.c
   * hfi1_get_credit - flush the send work queue of a QP
   * @qp: the qp who's send work queue to flush
   * @aeth: the Acknowledge Extended Transport Header
@@@ -1490,64 -984,12 +1508,67 @@@ eagain
  
  static void iowait_wakeup(struct iowait *wait, int reason)
  {
 -	struct rvt_qp *qp = iowait_to_qp(wait);
 +	struct hfi1_qp *qp = iowait_to_qp(wait);
  
  	WARN_ON(reason != SDMA_AVAIL_REASON);
 -	hfi1_qp_wakeup(qp, RVT_S_WAIT_DMA_DESC);
 +	hfi1_qp_wakeup(qp, HFI1_S_WAIT_DMA_DESC);
 +}
 +
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +int hfi1_qp_init(struct hfi1_ibdev *dev)
 +{
 +	struct hfi1_devdata *dd = dd_from_dev(dev);
 +	int i;
 +	int ret = -ENOMEM;
 +
 +	/* allocate parent object */
 +	dev->qp_dev = kzalloc(sizeof(*dev->qp_dev), GFP_KERNEL);
 +	if (!dev->qp_dev)
 +		goto nomem;
 +	/* allocate hash table */
 +	dev->qp_dev->qp_table_size = hfi1_qp_table_size;
 +	dev->qp_dev->qp_table_bits = ilog2(hfi1_qp_table_size);
 +	dev->qp_dev->qp_table =
 +		kmalloc(dev->qp_dev->qp_table_size *
 +				sizeof(*dev->qp_dev->qp_table),
 +			GFP_KERNEL);
 +	if (!dev->qp_dev->qp_table)
 +		goto nomem;
 +	for (i = 0; i < dev->qp_dev->qp_table_size; i++)
 +		RCU_INIT_POINTER(dev->qp_dev->qp_table[i], NULL);
 +	spin_lock_init(&dev->qp_dev->qpt_lock);
 +	/* initialize qpn map */
 +	ret = init_qpn_table(dd, &dev->qp_dev->qpn_table);
 +	if (ret)
 +		goto nomem;
 +	return ret;
 +nomem:
 +	if (dev->qp_dev) {
 +		kfree(dev->qp_dev->qp_table);
 +		free_qpn_table(&dev->qp_dev->qpn_table);
 +		kfree(dev->qp_dev);
 +	}
 +	return ret;
 +}
 +
 +void hfi1_qp_exit(struct hfi1_ibdev *dev)
 +{
 +	struct hfi1_devdata *dd = dd_from_dev(dev);
 +	u32 qps_inuse;
 +
 +	qps_inuse = free_all_qps(dd);
 +	if (qps_inuse)
 +		dd_dev_err(dd, "QP memory leak! %u still in use\n",
 +			   qps_inuse);
 +	if (dev->qp_dev) {
 +		kfree(dev->qp_dev->qp_table);
 +		free_qpn_table(&dev->qp_dev->qpn_table);
 +		kfree(dev->qp_dev);
 +	}
  }
  
++=======
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/qp.c
  /**
   *
   * qp_to_sdma_engine - map a qp to a send engine
diff --cc drivers/staging/hfi1/qp.h
index 1144470a6bc0,b825cb347ee1..000000000000
--- a/drivers/staging/hfi1/qp.h
+++ b/drivers/staging/hfi1/qp.h
@@@ -54,38 -54,10 +54,44 @@@
  #include "verbs.h"
  #include "sdma.h"
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.h
 +#define QPN_MAX                 (1 << 24)
 +#define QPNMAP_ENTRIES          (QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)
 +
 +/*
 + * QPN-map pages start out as NULL, they get allocated upon
 + * first use and are never deallocated. This way,
 + * large bitmaps are not allocated unless large numbers of QPs are used.
 + */
 +struct qpn_map {
 +	void *page;
 +};
 +
 +struct hfi1_qpn_table {
 +	spinlock_t lock; /* protect changes in this struct */
 +	unsigned flags;         /* flags for QP0/1 allocated for each port */
 +	u32 last;               /* last QP number allocated */
 +	u32 nmaps;              /* size of the map table */
 +	u16 limit;
 +	u8  incr;
 +	/* bit map of free QP numbers other than 0/1 */
 +	struct qpn_map map[QPNMAP_ENTRIES];
 +};
 +
 +struct hfi1_qp_ibdev {
 +	u32 qp_table_size;
 +	u32 qp_table_bits;
 +	struct hfi1_qp __rcu **qp_table;
 +	spinlock_t qpt_lock;
 +	struct hfi1_qpn_table qpn_table;
 +};
 +
 +static inline u32 qpn_hash(struct hfi1_qp_ibdev *dev, u32 qpn)
++=======
+ extern unsigned int hfi1_qp_table_size;
+ 
+ static inline u32 qpn_hash(struct rvt_qp_ibdev *dev, u32 qpn)
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/qp.h
  {
  	return hash_32(qpn, dev->qp_table_bits);
  }
@@@ -198,20 -169,8 +204,8 @@@ int hfi1_destroy_qp(struct ib_qp *ibqp)
   *
   * The QP s_lock should be held.
   */
 -void hfi1_get_credit(struct rvt_qp *qp, u32 aeth);
 +void hfi1_get_credit(struct hfi1_qp *qp, u32 aeth);
  
- /**
-  * hfi1_qp_init - allocate QP tables
-  * @dev: a pointer to the hfi1_ibdev
-  */
- int hfi1_qp_init(struct hfi1_ibdev *dev);
- 
- /**
-  * hfi1_qp_exit - free the QP related structures
-  * @dev: a pointer to the hfi1_ibdev
-  */
- void hfi1_qp_exit(struct hfi1_ibdev *dev);
- 
  /**
   * hfi1_qp_wakeup - wake up on the indicated event
   * @qp: the QP
@@@ -284,6 -243,15 +278,15 @@@ static inline void hfi1_schedule_send(s
  		_hfi1_schedule_send(qp);
  }
  
 -void hfi1_migrate_qp(struct rvt_qp *qp);
 +void hfi1_migrate_qp(struct hfi1_qp *qp);
  
+ /*
+  * Functions provided by hfi1 driver for rdmavt to use
+  */
+ void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+ 		    gfp_t gfp);
+ void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp);
+ unsigned free_all_qps(struct rvt_dev_info *rdi);
+ void notify_qp_reset(struct rvt_qp *qp);
+ 
  #endif /* _QP_H */
diff --cc drivers/staging/hfi1/verbs.c
index d228eb7fc4f0,3f02d0a013c4..000000000000
--- a/drivers/staging/hfi1/verbs.c
+++ b/drivers/staging/hfi1/verbs.c
@@@ -2000,7 -1846,47 +1995,51 @@@ int hfi1_register_ib_device(struct hfi1
  	strncpy(ibdev->node_desc, init_utsname()->nodename,
  		sizeof(ibdev->node_desc));
  
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	ret = ib_register_device(ibdev, hfi1_create_port_files);
++=======
+ 	/*
+ 	 * Fill in rvt info object.
+ 	 */
+ 	dd->verbs_dev.rdi.driver_f.port_callback = hfi1_create_port_files;
+ 	dd->verbs_dev.rdi.driver_f.get_card_name = get_card_name;
+ 	dd->verbs_dev.rdi.driver_f.get_pci_dev = get_pci_dev;
+ 	dd->verbs_dev.rdi.driver_f.check_ah = hfi1_check_ah;
+ 	dd->verbs_dev.rdi.driver_f.notify_new_ah = hfi1_notify_new_ah;
+ 	dd->verbs_dev.rdi.dparms.props.max_ah = hfi1_max_ahs;
+ 	dd->verbs_dev.rdi.dparms.props.max_pd = hfi1_max_pds;
+ 	dd->verbs_dev.rdi.dparms.props.max_sge = hfi1_max_sges;
+ 
+ 	/* queue pair */
+ 	dd->verbs_dev.rdi.dparms.props.max_qp = hfi1_max_qps;
+ 	dd->verbs_dev.rdi.dparms.props.max_qp_wr = hfi1_max_qp_wrs;
+ 	dd->verbs_dev.rdi.dparms.qp_table_size = hfi1_qp_table_size;
+ 	dd->verbs_dev.rdi.dparms.qpn_start = 0;
+ 	dd->verbs_dev.rdi.dparms.qpn_inc = 1;
+ 	dd->verbs_dev.rdi.dparms.qos_shift = dd->qos_shift;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_start = kdeth_qp << 16;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_end =
+ 		dd->verbs_dev.rdi.dparms.qpn_res_start + 65535;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_alloc = qp_priv_alloc;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_free = qp_priv_free;
+ 	dd->verbs_dev.rdi.driver_f.free_all_qps = free_all_qps;
+ 	dd->verbs_dev.rdi.driver_f.notify_qp_reset = notify_qp_reset;
+ 
+ 	/* misc settings */
+ 	dd->verbs_dev.rdi.flags = RVT_FLAG_CQ_INIT_DRIVER;
+ 	dd->verbs_dev.rdi.dparms.lkey_table_size = hfi1_lkey_table_size;
+ 	dd->verbs_dev.rdi.dparms.nports = dd->num_pports;
+ 	dd->verbs_dev.rdi.dparms.npkeys = hfi1_get_npkeys(dd);
+ 
+ 	ppd = dd->pport;
+ 	for (i = 0; i < dd->num_pports; i++, ppd++)
+ 		rvt_init_port(&dd->verbs_dev.rdi,
+ 			      &ppd->ibport_data.rvp,
+ 			      i,
+ 			      ppd->pkeys);
+ 
+ 	ret = rvt_register_device(&dd->verbs_dev.rdi);
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/verbs.c
  	if (ret)
  		goto err_reg;
  
@@@ -2021,10 -1907,6 +2060,13 @@@ err_agents
  err_reg:
  err_verbs_txreq:
  	kmem_cache_destroy(dev->verbs_txreq_cache);
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	vfree(dev->lk_table.table);
 +err_lk:
 +	hfi1_qp_exit(dev);
 +err_qp_init:
++=======
++>>>>>>> a2c2d608957c (staging/rdma/hfi1: Remove create_qp functionality):drivers/staging/rdma/hfi1/verbs.c
  	dd_dev_err(dd, "cannot register verbs: %d!\n", -ret);
  bail:
  	return ret;
@@@ -2045,13 -1926,9 +2087,12 @@@ void hfi1_unregister_ib_device(struct h
  		dd_dev_err(dd, "txwait list not empty!\n");
  	if (!list_empty(&dev->memwait))
  		dd_dev_err(dd, "memwait list not empty!\n");
 +	if (dev->dma_mr)
 +		dd_dev_err(dd, "DMA MR not NULL!\n");
  
- 	hfi1_qp_exit(dev);
  	del_timer_sync(&dev->mem_timer);
  	kmem_cache_destroy(dev->verbs_txreq_cache);
 +	vfree(dev->lk_table.table);
  }
  
  void hfi1_cnp_rcv(struct hfi1_packet *packet)
* Unmerged path drivers/staging/hfi1/qp.c
* Unmerged path drivers/staging/hfi1/qp.h
* Unmerged path drivers/staging/hfi1/verbs.c
