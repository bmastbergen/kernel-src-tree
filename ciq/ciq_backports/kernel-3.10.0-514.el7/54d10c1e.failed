staging/rdma/hfi1: Use rdmavt send flags and recv flags

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [infiniband] rdma/hfi1: Use rdmavt send flags and recv flags (Alex Estrin) [1272062 1273170]
Rebuild_FUZZ: 92.16%
commit-author Dennis Dalessandro <dennis.dalessandro@intel.com>
commit 54d10c1eb1dc381e62361213bbd100a433b733c9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/54d10c1e.failed

Use the definitions of the s_flags and r_flags which are now in rdmavt.

	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Reviewed-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 54d10c1eb1dc381e62361213bbd100a433b733c9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/qp.c
#	drivers/staging/hfi1/rc.c
#	drivers/staging/hfi1/ud.c
#	drivers/staging/hfi1/verbs.h
diff --cc drivers/staging/hfi1/qp.c
index 9ffed6e14d8e,d5620babd36a..000000000000
--- a/drivers/staging/hfi1/qp.c
+++ b/drivers/staging/hfi1/qp.c
@@@ -487,10 -488,10 +487,10 @@@ int hfi1_error_qp(struct hfi1_qp *qp, e
  	}
  	write_sequnlock(&dev->iowait_lock);
  
- 	if (!(qp->s_flags & HFI1_S_BUSY)) {
+ 	if (!(qp->s_flags & RVT_S_BUSY)) {
  		qp->s_hdrwords = 0;
  		if (qp->s_rdma_mr) {
 -			rvt_put_mr(qp->s_rdma_mr);
 +			hfi1_put_mr(qp->s_rdma_mr);
  			qp->s_rdma_mr = NULL;
  		}
  		flush_tx_list(qp);
@@@ -1466,10 -1468,10 +1466,15 @@@ static int iowait_sleep
  			struct hfi1_ibport *ibp =
  				to_iport(qp->ibqp.device, qp->port_num);
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +			ibp->n_dmawait++;
 +			qp->s_flags |= HFI1_S_WAIT_DMA_DESC;
++=======
+ 			ibp->rvp.n_dmawait++;
+ 			qp->s_flags |= RVT_S_WAIT_DMA_DESC;
++>>>>>>> 54d10c1eb1dc (staging/rdma/hfi1: Use rdmavt send flags and recv flags):drivers/staging/rdma/hfi1/qp.c
  			list_add_tail(&priv->s_iowait.list, &sde->dmawait);
- 			trace_hfi1_qpsleep(qp, HFI1_S_WAIT_DMA_DESC);
+ 			trace_hfi1_qpsleep(qp, RVT_S_WAIT_DMA_DESC);
  			atomic_inc(&qp->refcount);
  		}
  		write_sequnlock(&dev->iowait_lock);
@@@ -1490,10 -1492,10 +1495,10 @@@ eagain
  
  static void iowait_wakeup(struct iowait *wait, int reason)
  {
 -	struct rvt_qp *qp = iowait_to_qp(wait);
 +	struct hfi1_qp *qp = iowait_to_qp(wait);
  
  	WARN_ON(reason != SDMA_AVAIL_REASON);
- 	hfi1_qp_wakeup(qp, HFI1_S_WAIT_DMA_DESC);
+ 	hfi1_qp_wakeup(qp, RVT_S_WAIT_DMA_DESC);
  }
  
  int hfi1_qp_init(struct hfi1_ibdev *dev)
@@@ -1708,9 -1710,9 +1713,9 @@@ void qp_iter_print(struct seq_file *s, 
  		   sde ? sde->this_idx : 0);
  }
  
 -void qp_comm_est(struct rvt_qp *qp)
 +void qp_comm_est(struct hfi1_qp *qp)
  {
- 	qp->r_flags |= HFI1_R_COMM_EST;
+ 	qp->r_flags |= RVT_R_COMM_EST;
  	if (qp->ibqp.event_handler) {
  		struct ib_event ev;
  
diff --cc drivers/staging/hfi1/rc.c
index dd57d65aa9b2,bd504decc46d..000000000000
--- a/drivers/staging/hfi1/rc.c
+++ b/drivers/staging/hfi1/rc.c
@@@ -74,9 -74,9 +74,9 @@@ static u32 restart_sge(struct hfi1_sge_
  	return wqe->length - len;
  }
  
 -static void start_timer(struct rvt_qp *qp)
 +static void start_timer(struct hfi1_qp *qp)
  {
- 	qp->s_flags |= HFI1_S_TIMER;
+ 	qp->s_flags |= RVT_S_TIMER;
  	qp->s_timer.function = rc_timeout;
  	/* 4.096 usec. * (1 << qp->timeout) */
  	qp->s_timer.expires = jiffies + qp->timeout_jiffies;
@@@ -772,9 -772,9 +772,9 @@@ void hfi1_send_rc_ack(struct hfi1_ctxtd
  	return;
  
  queue_ack:
 -	this_cpu_inc(*ibp->rvp.rc_qacks);
 +	this_cpu_inc(*ibp->rc_qacks);
  	spin_lock_irqsave(&qp->s_lock, flags);
- 	qp->s_flags |= HFI1_S_ACK_PENDING | HFI1_S_RESP_PENDING;
+ 	qp->s_flags |= RVT_S_ACK_PENDING | RVT_S_RESP_PENDING;
  	qp->s_nak_state = qp->r_nak_state;
  	qp->s_ack_psn = qp->r_ack_psn;
  	if (is_fecn)
@@@ -900,15 -900,15 +900,15 @@@ static void restart_rc(struct hfi1_qp *
  
  	ibp = to_iport(qp->ibqp.device, qp->port_num);
  	if (wqe->wr.opcode == IB_WR_RDMA_READ)
 -		ibp->rvp.n_rc_resends++;
 +		ibp->n_rc_resends++;
  	else
 -		ibp->rvp.n_rc_resends += delta_psn(qp->s_psn, psn);
 +		ibp->n_rc_resends += delta_psn(qp->s_psn, psn);
  
- 	qp->s_flags &= ~(HFI1_S_WAIT_FENCE | HFI1_S_WAIT_RDMAR |
- 			 HFI1_S_WAIT_SSN_CREDIT | HFI1_S_WAIT_PSN |
- 			 HFI1_S_WAIT_ACK);
+ 	qp->s_flags &= ~(RVT_S_WAIT_FENCE | RVT_S_WAIT_RDMAR |
+ 			 RVT_S_WAIT_SSN_CREDIT | RVT_S_WAIT_PSN |
+ 			 RVT_S_WAIT_ACK);
  	if (wait)
- 		qp->s_flags |= HFI1_S_SEND_ONE;
+ 		qp->s_flags |= RVT_S_SEND_ONE;
  	reset_psn(qp, psn);
  }
  
@@@ -923,10 -923,10 +923,15 @@@ static void rc_timeout(unsigned long ar
  
  	spin_lock_irqsave(&qp->r_lock, flags);
  	spin_lock(&qp->s_lock);
- 	if (qp->s_flags & HFI1_S_TIMER) {
+ 	if (qp->s_flags & RVT_S_TIMER) {
  		ibp = to_iport(qp->ibqp.device, qp->port_num);
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +		ibp->n_rc_timeouts++;
 +		qp->s_flags &= ~HFI1_S_TIMER;
++=======
+ 		ibp->rvp.n_rc_timeouts++;
+ 		qp->s_flags &= ~RVT_S_TIMER;
++>>>>>>> 54d10c1eb1dc (staging/rdma/hfi1: Use rdmavt send flags and recv flags):drivers/staging/rdma/hfi1/rc.c
  		del_timer(&qp->s_timer);
  		trace_hfi1_rc_timeout(qp, qp->s_last_psn + 1);
  		restart_rc(qp, qp->s_last_psn + 1, 1);
@@@ -1027,12 -1027,12 +1032,12 @@@ void hfi1_rc_send_complete(struct hfi1_
  		    cmp_psn(qp->s_sending_psn, qp->s_sending_hpsn) <= 0)
  			break;
  		for (i = 0; i < wqe->wr.num_sge; i++) {
 -			struct rvt_sge *sge = &wqe->sg_list[i];
 +			struct hfi1_sge *sge = &wqe->sg_list[i];
  
 -			rvt_put_mr(sge->mr);
 +			hfi1_put_mr(sge->mr);
  		}
  		/* Post a send completion queue entry if requested. */
- 		if (!(qp->s_flags & HFI1_S_SIGNAL_REQ_WR) ||
+ 		if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
  		    (wqe->wr.send_flags & IB_SEND_SIGNALED)) {
  			memset(&wc, 0, sizeof(wc));
  			wc.wr_id = wqe->wr.wr_id;
@@@ -1084,12 -1084,12 +1089,12 @@@ static struct hfi1_swqe *do_rc_completi
  	if (cmp_psn(wqe->lpsn, qp->s_sending_psn) < 0 ||
  	    cmp_psn(qp->s_sending_psn, qp->s_sending_hpsn) > 0) {
  		for (i = 0; i < wqe->wr.num_sge; i++) {
 -			struct rvt_sge *sge = &wqe->sg_list[i];
 +			struct hfi1_sge *sge = &wqe->sg_list[i];
  
 -			rvt_put_mr(sge->mr);
 +			hfi1_put_mr(sge->mr);
  		}
  		/* Post a send completion queue entry if requested. */
- 		if (!(qp->s_flags & HFI1_S_SIGNAL_REQ_WR) ||
+ 		if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
  		    (wqe->wr.send_flags & IB_SEND_SIGNALED)) {
  			memset(&wc, 0, sizeof(wc));
  			wc.wr_id = wqe->wr.wr_id;
@@@ -1292,10 -1292,10 +1297,10 @@@ static int do_rc_ack(struct hfi1_qp *qp
  		goto bail;
  
  	case 1:         /* RNR NAK */
 -		ibp->rvp.n_rnr_naks++;
 +		ibp->n_rnr_naks++;
  		if (qp->s_acked == qp->s_tail)
  			goto bail;
- 		if (qp->s_flags & HFI1_S_WAIT_RNR)
+ 		if (qp->s_flags & RVT_S_WAIT_RNR)
  			goto bail;
  		if (qp->s_rnr_retry == 0) {
  			status = IB_WC_RNR_RETRY_EXC_ERR;
@@@ -1381,14 -1381,14 +1386,14 @@@ bail
   * We have seen an out of sequence RDMA read middle or last packet.
   * This ACKs SENDs and RDMA writes up to the first RDMA read or atomic SWQE.
   */
 -static void rdma_seq_err(struct rvt_qp *qp, struct hfi1_ibport *ibp, u32 psn,
 +static void rdma_seq_err(struct hfi1_qp *qp, struct hfi1_ibport *ibp, u32 psn,
  			 struct hfi1_ctxtdata *rcd)
  {
 -	struct rvt_swqe *wqe;
 +	struct hfi1_swqe *wqe;
  
  	/* Remove QP from retry timer */
- 	if (qp->s_flags & (HFI1_S_TIMER | HFI1_S_WAIT_RNR)) {
- 		qp->s_flags &= ~(HFI1_S_TIMER | HFI1_S_WAIT_RNR);
+ 	if (qp->s_flags & (RVT_S_TIMER | RVT_S_WAIT_RNR)) {
+ 		qp->s_flags &= ~(RVT_S_TIMER | RVT_S_WAIT_RNR);
  		del_timer(&qp->s_timer);
  	}
  
@@@ -1402,11 -1402,11 +1407,16 @@@
  		wqe = do_rc_completion(qp, wqe, ibp);
  	}
  
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +	ibp->n_rdma_seq++;
 +	qp->r_flags |= HFI1_R_RDMAR_SEQ;
++=======
+ 	ibp->rvp.n_rdma_seq++;
+ 	qp->r_flags |= RVT_R_RDMAR_SEQ;
++>>>>>>> 54d10c1eb1dc (staging/rdma/hfi1: Use rdmavt send flags and recv flags):drivers/staging/rdma/hfi1/rc.c
  	restart_rc(qp, qp->s_last_psn + 1, 0);
  	if (list_empty(&qp->rspwait)) {
- 		qp->r_flags |= HFI1_R_RSP_SEND;
+ 		qp->r_flags |= RVT_R_RSP_SEND;
  		atomic_inc(&qp->refcount);
  		list_add_tail(&qp->rspwait, &rcd->qp_wait_list);
  	}
@@@ -1610,10 -1610,10 +1620,10 @@@ bail
  }
  
  static inline void rc_defered_ack(struct hfi1_ctxtdata *rcd,
 -				  struct rvt_qp *qp)
 +				  struct hfi1_qp *qp)
  {
  	if (list_empty(&qp->rspwait)) {
- 		qp->r_flags |= HFI1_R_RSP_DEFERED_ACK;
+ 		qp->r_flags |= RVT_R_RSP_NAK;
  		atomic_inc(&qp->refcount);
  		list_add_tail(&qp->rspwait, &rcd->qp_wait_list);
  	}
diff --cc drivers/staging/hfi1/ud.c
index a7f67b0111da,a0e62229d7a1..000000000000
--- a/drivers/staging/hfi1/ud.c
+++ b/drivers/staging/hfi1/ud.c
@@@ -179,8 -179,8 +179,13 @@@ static void ud_loopback(struct hfi1_qp 
  	}
  	/* Silently drop packets which are too big. */
  	if (unlikely(wc.byte_len > qp->r_len)) {
++<<<<<<< HEAD:drivers/staging/hfi1/ud.c
 +		qp->r_flags |= HFI1_R_REUSE_SGE;
 +		ibp->n_pkt_drops++;
++=======
+ 		qp->r_flags |= RVT_R_REUSE_SGE;
+ 		ibp->rvp.n_pkt_drops++;
++>>>>>>> 54d10c1eb1dc (staging/rdma/hfi1: Use rdmavt send flags and recv flags):drivers/staging/rdma/hfi1/ud.c
  		goto bail_unlock;
  	}
  
diff --cc drivers/staging/hfi1/verbs.h
index 34fa7beced4f,b9843a5ef0d2..000000000000
--- a/drivers/staging/hfi1/verbs.h
+++ b/drivers/staging/hfi1/verbs.h
@@@ -62,6 -62,8 +62,11 @@@
  #include <rdma/ib_pack.h>
  #include <rdma/ib_user_verbs.h>
  #include <rdma/ib_mad.h>
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.h
++=======
+ #include <rdma/rdma_vt.h>
+ #include <rdma/rdmavt_qp.h>
++>>>>>>> 54d10c1eb1dc (staging/rdma/hfi1: Use rdmavt send flags and recv flags):drivers/staging/rdma/hfi1/verbs.h
  
  struct hfi1_ctxtdata;
  struct hfi1_pportdata;
@@@ -805,11 -428,11 +732,11 @@@ static inline struct hfi1_qp *iowait_to
   * Send if not busy or waiting for I/O and either
   * a RC response is pending or we can process send work requests.
   */
 -static inline int hfi1_send_ok(struct rvt_qp *qp)
 +static inline int hfi1_send_ok(struct hfi1_qp *qp)
  {
- 	return !(qp->s_flags & (HFI1_S_BUSY | HFI1_S_ANY_WAIT_IO)) &&
- 		(qp->s_hdrwords || (qp->s_flags & HFI1_S_RESP_PENDING) ||
- 		 !(qp->s_flags & HFI1_S_ANY_WAIT_SEND));
+ 	return !(qp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT_IO)) &&
+ 		(qp->s_hdrwords || (qp->s_flags & RVT_S_RESP_PENDING) ||
+ 		 !(qp->s_flags & RVT_S_ANY_WAIT_SEND));
  }
  
  /*
diff --git a/drivers/staging/hfi1/driver.c b/drivers/staging/hfi1/driver.c
index fca20e92c79b..49477f09ce2b 100644
--- a/drivers/staging/hfi1/driver.c
+++ b/drivers/staging/hfi1/driver.c
@@ -764,14 +764,14 @@ static inline void process_rcv_qp_work(struct hfi1_packet *packet)
 	 */
 	list_for_each_entry_safe(qp, nqp, &rcd->qp_wait_list, rspwait) {
 		list_del_init(&qp->rspwait);
-		if (qp->r_flags & HFI1_R_RSP_DEFERED_ACK) {
-			qp->r_flags &= ~HFI1_R_RSP_DEFERED_ACK;
+		if (qp->r_flags & RVT_R_RSP_NAK) {
+			qp->r_flags &= ~RVT_R_RSP_NAK;
 			hfi1_send_rc_ack(rcd, qp, 0);
 		}
-		if (qp->r_flags & HFI1_R_RSP_SEND) {
+		if (qp->r_flags & RVT_R_RSP_SEND) {
 			unsigned long flags;
 
-			qp->r_flags &= ~HFI1_R_RSP_SEND;
+			qp->r_flags &= ~RVT_R_RSP_SEND;
 			spin_lock_irqsave(&qp->s_lock, flags);
 			if (ib_hfi1_state_ops[qp->state] &
 					HFI1_PROCESS_OR_FLUSH_SEND)
diff --git a/drivers/staging/hfi1/pio.c b/drivers/staging/hfi1/pio.c
index 25d65f9a0b94..39aba0eb0eb1 100644
--- a/drivers/staging/hfi1/pio.c
+++ b/drivers/staging/hfi1/pio.c
@@ -1564,7 +1564,7 @@ full:
 	write_sequnlock_irqrestore(&dev->iowait_lock, flags);
 
 	for (i = 0; i < n; i++)
-		hfi1_qp_wakeup(qps[i], HFI1_S_WAIT_PIO);
+		hfi1_qp_wakeup(qps[i], RVT_S_WAIT_PIO);
 }
 
 /* translate a send credit update to a bit code of reasons */
* Unmerged path drivers/staging/hfi1/qp.c
diff --git a/drivers/staging/hfi1/qp.h b/drivers/staging/hfi1/qp.h
index 1144470a6bc0..fbdd963fa0a3 100644
--- a/drivers/staging/hfi1/qp.h
+++ b/drivers/staging/hfi1/qp.h
@@ -126,7 +126,7 @@ static inline void clear_ahg(struct hfi1_qp *qp)
 	struct hfi1_qp_priv *priv = qp->priv;
 
 	priv->s_hdr->ahgcount = 0;
-	qp->s_flags &= ~(HFI1_S_AHG_VALID | HFI1_S_AHG_CLEAR);
+	qp->s_flags &= ~(RVT_S_AHG_VALID | RVT_S_AHG_CLEAR);
 	if (priv->s_sde && qp->s_ahgidx >= 0)
 		sdma_ahg_free(priv->s_sde, qp->s_ahgidx);
 	qp->s_ahgidx = -1;
* Unmerged path drivers/staging/hfi1/rc.c
diff --git a/drivers/staging/hfi1/ruc.c b/drivers/staging/hfi1/ruc.c
index c4280b6f47d4..5228cdeef2f2 100644
--- a/drivers/staging/hfi1/ruc.c
+++ b/drivers/staging/hfi1/ruc.c
@@ -208,7 +208,7 @@ int hfi1_get_rwqe(struct hfi1_qp *qp, int wr_id_only)
 	qp->r_wr_id = wqe->wr_id;
 
 	ret = 1;
-	set_bit(HFI1_R_WRID_VALID, &qp->r_aflags);
+	set_bit(RVT_R_WRID_VALID, &qp->r_aflags);
 	if (handler) {
 		u32 n;
 
@@ -380,11 +380,11 @@ static void ruc_loopback(struct hfi1_qp *sqp)
 	spin_lock_irqsave(&sqp->s_lock, flags);
 
 	/* Return if we are already busy processing a work request. */
-	if ((sqp->s_flags & (HFI1_S_BUSY | HFI1_S_ANY_WAIT)) ||
+	if ((sqp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT)) ||
 	    !(ib_hfi1_state_ops[sqp->state] & HFI1_PROCESS_OR_FLUSH_SEND))
 		goto unlock;
 
-	sqp->s_flags |= HFI1_S_BUSY;
+	sqp->s_flags |= RVT_S_BUSY;
 
 again:
 	if (sqp->s_last == sqp->s_head)
@@ -549,7 +549,7 @@ again:
 	if (release)
 		hfi1_put_ss(&qp->r_sge);
 
-	if (!test_and_clear_bit(HFI1_R_WRID_VALID, &qp->r_aflags))
+	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
 		goto send_comp;
 
 	if (wqe->wr.opcode == IB_WR_RDMA_WRITE_WITH_IMM)
@@ -594,7 +594,7 @@ rnr_nak:
 	spin_lock_irqsave(&sqp->s_lock, flags);
 	if (!(ib_hfi1_state_ops[sqp->state] & HFI1_PROCESS_RECV_OK))
 		goto clr_busy;
-	sqp->s_flags |= HFI1_S_WAIT_RNR;
+	sqp->s_flags |= RVT_S_WAIT_RNR;
 	sqp->s_timer.function = hfi1_rc_rnr_retry;
 	sqp->s_timer.expires = jiffies +
 		usecs_to_jiffies(ib_hfi1_rnr_table[qp->r_min_rnr_timer]);
@@ -624,7 +624,7 @@ serr:
 	if (sqp->ibqp.qp_type == IB_QPT_RC) {
 		int lastwqe = hfi1_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
 
-		sqp->s_flags &= ~HFI1_S_BUSY;
+		sqp->s_flags &= ~RVT_S_BUSY;
 		spin_unlock_irqrestore(&sqp->s_lock, flags);
 		if (lastwqe) {
 			struct ib_event ev;
@@ -637,7 +637,7 @@ serr:
 		goto done;
 	}
 clr_busy:
-	sqp->s_flags &= ~HFI1_S_BUSY;
+	sqp->s_flags &= ~RVT_S_BUSY;
 unlock:
 	spin_unlock_irqrestore(&sqp->s_lock, flags);
 done:
@@ -693,9 +693,9 @@ u32 hfi1_make_grh(struct hfi1_ibport *ibp, struct ib_grh *hdr,
 static inline void build_ahg(struct hfi1_qp *qp, u32 npsn)
 {
 	struct hfi1_qp_priv *priv = qp->priv;
-	if (unlikely(qp->s_flags & HFI1_S_AHG_CLEAR))
+	if (unlikely(qp->s_flags & RVT_S_AHG_CLEAR))
 		clear_ahg(qp);
-	if (!(qp->s_flags & HFI1_S_AHG_VALID)) {
+	if (!(qp->s_flags & RVT_S_AHG_VALID)) {
 		/* first middle that needs copy  */
 		if (qp->s_ahgidx < 0)
 			qp->s_ahgidx = sdma_ahg_alloc(priv->s_sde);
@@ -705,7 +705,7 @@ static inline void build_ahg(struct hfi1_qp *qp, u32 npsn)
 			/* save to protect a change in another thread */
 			priv->s_hdr->sde = priv->s_sde;
 			priv->s_hdr->ahgidx = qp->s_ahgidx;
-			qp->s_flags |= HFI1_S_AHG_VALID;
+			qp->s_flags |= RVT_S_AHG_VALID;
 		}
 	} else {
 		/* subsequent middle after valid */
@@ -778,7 +778,7 @@ void hfi1_make_ruc_header(struct hfi1_qp *qp, struct hfi1_other_headers *ohdr,
 	if (middle)
 		build_ahg(qp, bth2);
 	else
-		qp->s_flags &= ~HFI1_S_AHG_VALID;
+		qp->s_flags &= ~RVT_S_AHG_VALID;
 	priv->s_hdr->ibh.lrh[0] = cpu_to_be16(lrh0);
 	priv->s_hdr->ibh.lrh[1] = cpu_to_be16(qp->remote_ah_attr.dlid);
 	priv->s_hdr->ibh.lrh[2] =
@@ -789,8 +789,8 @@ void hfi1_make_ruc_header(struct hfi1_qp *qp, struct hfi1_other_headers *ohdr,
 	bth0 |= extra_bytes << 20;
 	ohdr->bth[0] = cpu_to_be32(bth0);
 	bth1 = qp->remote_qpn;
-	if (qp->s_flags & HFI1_S_ECN) {
-		qp->s_flags &= ~HFI1_S_ECN;
+	if (qp->s_flags & RVT_S_ECN) {
+		qp->s_flags &= ~RVT_S_ECN;
 		/* we recently received a FECN, so return a BECN */
 		bth1 |= (HFI1_BECN_MASK << HFI1_BECN_SHIFT);
 	}
@@ -846,7 +846,7 @@ void hfi1_do_send(struct work_struct *work)
 		return;
 	}
 
-	qp->s_flags |= HFI1_S_BUSY;
+	qp->s_flags |= RVT_S_BUSY;
 
 	spin_unlock_irqrestore(&qp->s_lock, flags);
 
@@ -896,7 +896,7 @@ void hfi1_send_complete(struct hfi1_qp *qp, struct hfi1_swqe *wqe,
 		atomic_dec(&to_iah(wqe->wr.wr.ud.ah)->refcount);
 
 	/* See ch. 11.2.4.1 and 10.7.3.1 */
-	if (!(qp->s_flags & HFI1_S_SIGNAL_REQ_WR) ||
+	if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
 	    (wqe->wr.send_flags & IB_SEND_SIGNALED) ||
 	    status != IB_WC_SUCCESS) {
 		struct ib_wc wc;
diff --git a/drivers/staging/hfi1/uc.c b/drivers/staging/hfi1/uc.c
index fc90d4f544e4..9664dd52a823 100644
--- a/drivers/staging/hfi1/uc.c
+++ b/drivers/staging/hfi1/uc.c
@@ -84,7 +84,7 @@ int hfi1_make_uc_req(struct hfi1_qp *qp)
 			goto bail;
 		/* If DMAs are in progress, we can't flush immediately. */
 		if (atomic_read(&priv->s_iowait.sdma_busy)) {
-			qp->s_flags |= HFI1_S_WAIT_DMA;
+			qp->s_flags |= RVT_S_WAIT_DMA;
 			goto bail;
 		}
 		clear_ahg(qp);
@@ -241,7 +241,7 @@ done:
 	goto unlock;
 
 bail:
-	qp->s_flags &= ~HFI1_S_BUSY;
+	qp->s_flags &= ~RVT_S_BUSY;
 unlock:
 	spin_unlock_irqrestore(&qp->s_lock, flags);
 	return ret;
@@ -332,7 +332,7 @@ void hfi1_uc_rcv(struct hfi1_packet *packet)
 inv:
 		if (qp->r_state == OP(SEND_FIRST) ||
 		    qp->r_state == OP(SEND_MIDDLE)) {
-			set_bit(HFI1_R_REWIND_SGE, &qp->r_aflags);
+			set_bit(RVT_R_REWIND_SGE, &qp->r_aflags);
 			qp->r_sge.num_sge = 0;
 		} else
 			hfi1_put_ss(&qp->r_sge);
@@ -382,7 +382,7 @@ inv:
 		goto inv;
 	}
 
-	if (qp->state == IB_QPS_RTR && !(qp->r_flags & HFI1_R_COMM_EST))
+	if (qp->state == IB_QPS_RTR && !(qp->r_flags & RVT_R_COMM_EST))
 		qp_comm_est(qp);
 
 	/* OK, process the packet. */
@@ -391,7 +391,7 @@ inv:
 	case OP(SEND_ONLY):
 	case OP(SEND_ONLY_WITH_IMMEDIATE):
 send_first:
-		if (test_and_clear_bit(HFI1_R_REWIND_SGE, &qp->r_aflags))
+		if (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))
 			qp->r_sge = qp->s_rdma_read_sge;
 		else {
 			ret = hfi1_get_rwqe(qp, 0);
@@ -536,7 +536,7 @@ rdma_last_imm:
 		tlen -= (hdrsize + pad + 4);
 		if (unlikely(tlen + qp->r_rcv_len != qp->r_len))
 			goto drop;
-		if (test_and_clear_bit(HFI1_R_REWIND_SGE, &qp->r_aflags))
+		if (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))
 			hfi1_put_ss(&qp->s_rdma_read_sge);
 		else {
 			ret = hfi1_get_rwqe(qp, 1);
@@ -576,7 +576,7 @@ rdma_last:
 	return;
 
 rewind:
-	set_bit(HFI1_R_REWIND_SGE, &qp->r_aflags);
+	set_bit(RVT_R_REWIND_SGE, &qp->r_aflags);
 	qp->r_sge.num_sge = 0;
 drop:
 	ibp->n_pkt_drops++;
* Unmerged path drivers/staging/hfi1/ud.c
diff --git a/drivers/staging/hfi1/verbs.c b/drivers/staging/hfi1/verbs.c
index d228eb7fc4f0..0ede9ee13f90 100644
--- a/drivers/staging/hfi1/verbs.c
+++ b/drivers/staging/hfi1/verbs.c
@@ -702,7 +702,7 @@ static void mem_timer(unsigned long data)
 	write_sequnlock_irqrestore(&dev->iowait_lock, flags);
 
 	if (qp)
-		hfi1_qp_wakeup(qp, HFI1_S_WAIT_KMEM);
+		hfi1_qp_wakeup(qp, RVT_S_WAIT_KMEM);
 }
 
 void update_sge(struct hfi1_sge_state *ss, u32 length)
@@ -740,12 +740,12 @@ static noinline struct verbs_txreq *__get_txreq(struct hfi1_ibdev *dev,
 		if (ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_RECV_OK &&
 		    list_empty(&priv->s_iowait.list)) {
 			dev->n_txwait++;
-			qp->s_flags |= HFI1_S_WAIT_TX;
+			qp->s_flags |= RVT_S_WAIT_TX;
 			list_add_tail(&priv->s_iowait.list, &dev->txwait);
-			trace_hfi1_qpsleep(qp, HFI1_S_WAIT_TX);
+			trace_hfi1_qpsleep(qp, RVT_S_WAIT_TX);
 			atomic_inc(&qp->refcount);
 		}
-		qp->s_flags &= ~HFI1_S_BUSY;
+		qp->s_flags &= ~RVT_S_BUSY;
 		write_sequnlock(&dev->iowait_lock);
 		spin_unlock_irqrestore(&qp->s_lock, flags);
 		tx = ERR_PTR(-EBUSY);
@@ -803,7 +803,7 @@ void hfi1_put_txreq(struct verbs_txreq *tx)
 			list_del_init(&priv->s_iowait.list);
 			/* refcount held until actual wake up */
 			write_sequnlock_irqrestore(&dev->iowait_lock, flags);
-			hfi1_qp_wakeup(qp, HFI1_S_WAIT_TX);
+			hfi1_qp_wakeup(qp, RVT_S_WAIT_TX);
 			break;
 		}
 	} while (read_seqretry(&dev->iowait_lock, seq));
@@ -838,8 +838,8 @@ static void verbs_sdma_complete(
 		 * do the flush work until that QP's
 		 * sdma work has finished.
 		 */
-		if (qp->s_flags & HFI1_S_WAIT_DMA) {
-			qp->s_flags &= ~HFI1_S_WAIT_DMA;
+		if (qp->s_flags & RVT_S_WAIT_DMA) {
+			qp->s_flags &= ~RVT_S_WAIT_DMA;
 			hfi1_schedule_send(qp);
 		}
 	}
@@ -860,13 +860,13 @@ static int wait_kmem(struct hfi1_ibdev *dev, struct hfi1_qp *qp)
 		if (list_empty(&priv->s_iowait.list)) {
 			if (list_empty(&dev->memwait))
 				mod_timer(&dev->mem_timer, jiffies + 1);
-			qp->s_flags |= HFI1_S_WAIT_KMEM;
+			qp->s_flags |= RVT_S_WAIT_KMEM;
 			list_add_tail(&priv->s_iowait.list, &dev->memwait);
-			trace_hfi1_qpsleep(qp, HFI1_S_WAIT_KMEM);
+			trace_hfi1_qpsleep(qp, RVT_S_WAIT_KMEM);
 			atomic_inc(&qp->refcount);
 		}
 		write_sequnlock(&dev->iowait_lock);
-		qp->s_flags &= ~HFI1_S_BUSY;
+		qp->s_flags &= ~RVT_S_BUSY;
 		ret = -EBUSY;
 	}
 	spin_unlock_irqrestore(&qp->s_lock, flags);
@@ -1092,17 +1092,17 @@ static int no_bufs_available(struct hfi1_qp *qp, struct send_context *sc)
 			int was_empty;
 
 			dev->n_piowait++;
-			qp->s_flags |= HFI1_S_WAIT_PIO;
+			qp->s_flags |= RVT_S_WAIT_PIO;
 			was_empty = list_empty(&sc->piowait);
 			list_add_tail(&priv->s_iowait.list, &sc->piowait);
-			trace_hfi1_qpsleep(qp, HFI1_S_WAIT_PIO);
+			trace_hfi1_qpsleep(qp, RVT_S_WAIT_PIO);
 			atomic_inc(&qp->refcount);
 			/* counting: only call wantpiobuf_intr if first user */
 			if (was_empty)
 				hfi1_sc_wantpiobuf_intr(sc, 1);
 		}
 		write_sequnlock(&dev->iowait_lock);
-		qp->s_flags &= ~HFI1_S_BUSY;
+		qp->s_flags &= ~RVT_S_BUSY;
 		ret = -EBUSY;
 	}
 	spin_unlock_irqrestore(&qp->s_lock, flags);
@@ -1307,7 +1307,7 @@ bad:
  * @ps: the state of the packet to send
  *
  * Return zero if packet is sent or queued OK.
- * Return non-zero and clear qp->s_flags HFI1_S_BUSY otherwise.
+ * Return non-zero and clear qp->s_flags RVT_S_BUSY otherwise.
  */
 int hfi1_verbs_send(struct hfi1_qp *qp, struct hfi1_pkt_state *ps)
 {
* Unmerged path drivers/staging/hfi1/verbs.h
