slub: avoid irqoff/on in bulk allocation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Christoph Lameter <cl@linux.com>
commit 87098373e244840e00bd1c93884c1d917411597e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/87098373.failed

Use the new function that can do allocation while interrupts are disabled.
Avoids irq on/off sequences.

	Signed-off-by: Christoph Lameter <cl@linux.com>
	Cc: Jesper Dangaard Brouer <brouer@redhat.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Alexander Duyck <alexander.h.duyck@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 87098373e244840e00bd1c93884c1d917411597e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 9bb60eec298f,23f9d8d26422..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -2659,6 -2758,111 +2659,114 @@@ void kmem_cache_free(struct kmem_cache 
  }
  EXPORT_SYMBOL(kmem_cache_free);
  
++<<<<<<< HEAD
++=======
+ /* Note that interrupts must be enabled when calling this function. */
+ void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)
+ {
+ 	struct kmem_cache_cpu *c;
+ 	struct page *page;
+ 	int i;
+ 
+ 	local_irq_disable();
+ 	c = this_cpu_ptr(s->cpu_slab);
+ 
+ 	for (i = 0; i < size; i++) {
+ 		void *object = p[i];
+ 
+ 		BUG_ON(!object);
+ 		/* kmem cache debug support */
+ 		s = cache_from_obj(s, object);
+ 		if (unlikely(!s))
+ 			goto exit;
+ 		slab_free_hook(s, object);
+ 
+ 		page = virt_to_head_page(object);
+ 
+ 		if (c->page == page) {
+ 			/* Fastpath: local CPU free */
+ 			set_freepointer(s, object, c->freelist);
+ 			c->freelist = object;
+ 		} else {
+ 			c->tid = next_tid(c->tid);
+ 			local_irq_enable();
+ 			/* Slowpath: overhead locked cmpxchg_double_slab */
+ 			__slab_free(s, page, object, _RET_IP_);
+ 			local_irq_disable();
+ 			c = this_cpu_ptr(s->cpu_slab);
+ 		}
+ 	}
+ exit:
+ 	c->tid = next_tid(c->tid);
+ 	local_irq_enable();
+ }
+ EXPORT_SYMBOL(kmem_cache_free_bulk);
+ 
+ /* Note that interrupts must be enabled when calling this function. */
+ bool kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
+ 			   void **p)
+ {
+ 	struct kmem_cache_cpu *c;
+ 	int i;
+ 
+ 	/*
+ 	 * Drain objects in the per cpu slab, while disabling local
+ 	 * IRQs, which protects against PREEMPT and interrupts
+ 	 * handlers invoking normal fastpath.
+ 	 */
+ 	local_irq_disable();
+ 	c = this_cpu_ptr(s->cpu_slab);
+ 
+ 	for (i = 0; i < size; i++) {
+ 		void *object = c->freelist;
+ 
+ 		if (unlikely(!object)) {
+ 			/*
+ 			 * Invoking slow path likely have side-effect
+ 			 * of re-populating per CPU c->freelist
+ 			 */
+ 			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
+ 					    _RET_IP_, c);
+ 			if (unlikely(!p[i]))
+ 				goto error;
+ 
+ 			c = this_cpu_ptr(s->cpu_slab);
+ 			continue; /* goto for-loop */
+ 		}
+ 
+ 		/* kmem_cache debug support */
+ 		s = slab_pre_alloc_hook(s, flags);
+ 		if (unlikely(!s))
+ 			goto error;
+ 
+ 		c->freelist = get_freepointer(s, object);
+ 		p[i] = object;
+ 
+ 		/* kmem_cache debug support */
+ 		slab_post_alloc_hook(s, flags, object);
+ 	}
+ 	c->tid = next_tid(c->tid);
+ 	local_irq_enable();
+ 
+ 	/* Clear memory outside IRQ disabled fastpath loop */
+ 	if (unlikely(flags & __GFP_ZERO)) {
+ 		int j;
+ 
+ 		for (j = 0; j < i; j++)
+ 			memset(p[j], 0, s->object_size);
+ 	}
+ 
+ 	return true;
+ 
+ error:
+ 	__kmem_cache_free_bulk(s, i, p);
+ 	local_irq_enable();
+ 	return false;
+ }
+ EXPORT_SYMBOL(kmem_cache_alloc_bulk);
+ 
+ 
++>>>>>>> 87098373e244 (slub: avoid irqoff/on in bulk allocation)
  /*
   * Object placement in a slab is made very easy because we always start at
   * offset 0. If we tune the size of the object to the alignment then we can
* Unmerged path mm/slub.c
