mm: migrate: check movability of hugepage in unmap_and_move_huge_page()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] migrate: check movability of hugepage in unmap_and_move_huge_page() (Tomoaki Nishimura) [1287322]
Rebuild_FUZZ: 97.10%
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit 83467efbdb7948146581a56cbd683a22a0684bbb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/83467efb.failed

Currently hugepage migration works well only for pmd-based hugepages
(mainly due to lack of testing,) so we had better not enable migration of
other levels of hugepages until we are ready for it.

Some users of hugepage migration (mbind, move_pages, and migrate_pages) do
page table walk and check pud/pmd_huge() there, so they are safe.  But the
other users (softoffline and memory hotremove) don't do this, so without
this patch they can try to migrate unexpected types of hugepages.

To prevent this, we introduce hugepage_migration_support() as an
architecture dependent check of whether hugepage are implemented on a pmd
basis or not.  And on some architecture multiple sizes of hugepages are
available, so hugepage_migration_support() also checks hugepage size.

	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Hillf Danton <dhillf@gmail.com>
	Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 83467efbdb7948146581a56cbd683a22a0684bbb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/mm/hugetlbpage.c
#	arch/arm64/mm/hugetlbpage.c
#	arch/x86/mm/hugetlbpage.c
#	include/linux/hugetlb.h
diff --cc arch/x86/mm/hugetlbpage.c
index e4afb30191c1,9d980d88b747..000000000000
--- a/arch/x86/mm/hugetlbpage.c
+++ b/arch/x86/mm/hugetlbpage.c
@@@ -242,30 -81,10 +246,37 @@@ int pud_huge(pud_t pud
  	return !!(pud_val(pud) & _PAGE_PSE);
  }
  
++<<<<<<< HEAD
 +struct page *
 +follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 +		pmd_t *pmd, int write)
 +{
 +	struct page *page;
 +
 +	page = pte_page(*(pte_t *)pmd);
 +	if (page)
 +		page += ((address & ~PMD_MASK) >> PAGE_SHIFT);
 +	return page;
 +}
 +
 +struct page *
 +follow_huge_pud(struct mm_struct *mm, unsigned long address,
 +		pud_t *pud, int write)
 +{
 +	struct page *page;
 +
 +	page = pte_page(*(pte_t *)pud);
 +	if (page)
 +		page += ((address & ~PUD_MASK) >> PAGE_SHIFT);
 +	return page;
 +}
 +
++=======
+ int pmd_huge_support(void)
+ {
+ 	return 1;
+ }
++>>>>>>> 83467efbdb79 (mm: migrate: check movability of hugepage in unmap_and_move_huge_page())
  #endif
  
  /* x86_64 also uses this file */
diff --cc include/linux/hugetlb.h
index f71f104feba3,0393270466c3..000000000000
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@@ -398,23 -379,17 +398,37 @@@ static inline pgoff_t basepage_index(st
  	return __basepage_index(page);
  }
  
++<<<<<<< HEAD
 +static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 +					   struct mm_struct *mm, pte_t *pte)
 +{
 +	if (huge_page_size(h) == PMD_SIZE)
 +		return pmd_lockptr(mm, (pmd_t *) pte);
 +	VM_BUG_ON(huge_page_size(h) == PAGE_SIZE);
 +	return &mm->page_table_lock;
 +}
 +
 +static inline bool hugepages_supported(void)
 +{
 +	/*
 +	 * Some platform decide whether they support huge pages at boot
 +	 * time. On these, such as powerpc, HPAGE_SHIFT is set to 0 when
 +	 * there is no such support
 +	 */
 +	return HPAGE_SHIFT != 0;
++=======
+ extern void dissolve_free_huge_pages(unsigned long start_pfn,
+ 				     unsigned long end_pfn);
+ int pmd_huge_support(void);
+ /*
+  * Currently hugepage migration is enabled only for pmd-based hugepage.
+  * This function will be updated when hugepage migration is more widely
+  * supported.
+  */
+ static inline int hugepage_migration_support(struct hstate *h)
+ {
+ 	return pmd_huge_support() && (huge_page_shift(h) == PMD_SHIFT);
++>>>>>>> 83467efbdb79 (mm: migrate: check movability of hugepage in unmap_and_move_huge_page())
  }
  
  #else	/* CONFIG_HUGETLB_PAGE */
@@@ -444,22 -418,9 +458,28 @@@ static inline pgoff_t basepage_index(st
  {
  	return page->index;
  }
++<<<<<<< HEAD
 +
 +static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 +					   struct mm_struct *mm, pte_t *pte)
 +{
 +	return &mm->page_table_lock;
 +}
++=======
+ #define dissolve_free_huge_pages(s, e)	do {} while (0)
+ #define pmd_huge_support()	0
+ #define hugepage_migration_support(h)	0
++>>>>>>> 83467efbdb79 (mm: migrate: check movability of hugepage in unmap_and_move_huge_page())
  #endif	/* CONFIG_HUGETLB_PAGE */
  
 +static inline spinlock_t *huge_pte_lock(struct hstate *h,
 +					struct mm_struct *mm, pte_t *pte)
 +{
 +	spinlock_t *ptl;
 +
 +	ptl = huge_pte_lockptr(h, mm, pte);
 +	spin_lock(ptl);
 +	return ptl;
 +}
 +
  #endif /* _LINUX_HUGETLB_H */
* Unmerged path arch/arm/mm/hugetlbpage.c
* Unmerged path arch/arm64/mm/hugetlbpage.c
* Unmerged path arch/arm/mm/hugetlbpage.c
* Unmerged path arch/arm64/mm/hugetlbpage.c
diff --git a/arch/ia64/mm/hugetlbpage.c b/arch/ia64/mm/hugetlbpage.c
index 76069c18ee42..68232db98baa 100644
--- a/arch/ia64/mm/hugetlbpage.c
+++ b/arch/ia64/mm/hugetlbpage.c
@@ -114,6 +114,11 @@ int pud_huge(pud_t pud)
 	return 0;
 }
 
+int pmd_huge_support(void)
+{
+	return 0;
+}
+
 struct page *
 follow_huge_pmd(struct mm_struct *mm, unsigned long address, pmd_t *pmd, int write)
 {
diff --git a/arch/metag/mm/hugetlbpage.c b/arch/metag/mm/hugetlbpage.c
index 3c52fa6d0f8e..042431509b56 100644
--- a/arch/metag/mm/hugetlbpage.c
+++ b/arch/metag/mm/hugetlbpage.c
@@ -110,6 +110,11 @@ int pud_huge(pud_t pud)
 	return 0;
 }
 
+int pmd_huge_support(void)
+{
+	return 1;
+}
+
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 			     pmd_t *pmd, int write)
 {
diff --git a/arch/mips/mm/hugetlbpage.c b/arch/mips/mm/hugetlbpage.c
index a7fee0dfb7a9..01fda4419ed0 100644
--- a/arch/mips/mm/hugetlbpage.c
+++ b/arch/mips/mm/hugetlbpage.c
@@ -85,6 +85,11 @@ int pud_huge(pud_t pud)
 	return (pud_val(pud) & _PAGE_HUGE) != 0;
 }
 
+int pmd_huge_support(void)
+{
+	return 1;
+}
+
 struct page *
 follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 		pmd_t *pmd, int write)
diff --git a/arch/powerpc/mm/hugetlbpage.c b/arch/powerpc/mm/hugetlbpage.c
index 65fa8c71b889..52a1d61844dd 100644
--- a/arch/powerpc/mm/hugetlbpage.c
+++ b/arch/powerpc/mm/hugetlbpage.c
@@ -86,6 +86,11 @@ int pgd_huge(pgd_t pgd)
 	 */
 	return ((pgd_val(pgd) & 0x3) != 0x0);
 }
+
+int pmd_huge_support(void)
+{
+	return 1;
+}
 #else
 int pmd_huge(pmd_t pmd)
 {
@@ -101,6 +106,11 @@ int pgd_huge(pgd_t pgd)
 {
 	return 0;
 }
+
+int pmd_huge_support(void)
+{
+	return 0;
+}
 #endif
 
 pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
diff --git a/arch/s390/mm/hugetlbpage.c b/arch/s390/mm/hugetlbpage.c
index 248445f92604..d261c62e40a6 100644
--- a/arch/s390/mm/hugetlbpage.c
+++ b/arch/s390/mm/hugetlbpage.c
@@ -223,6 +223,11 @@ int pud_huge(pud_t pud)
 	return 0;
 }
 
+int pmd_huge_support(void)
+{
+	return 1;
+}
+
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 			     pmd_t *pmdp, int write)
 {
diff --git a/arch/sh/mm/hugetlbpage.c b/arch/sh/mm/hugetlbpage.c
index d7762349ea48..0d676a41081e 100644
--- a/arch/sh/mm/hugetlbpage.c
+++ b/arch/sh/mm/hugetlbpage.c
@@ -83,6 +83,11 @@ int pud_huge(pud_t pud)
 	return 0;
 }
 
+int pmd_huge_support(void)
+{
+	return 0;
+}
+
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 			     pmd_t *pmd, int write)
 {
diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c
index d2b59441ebdd..96399646570a 100644
--- a/arch/sparc/mm/hugetlbpage.c
+++ b/arch/sparc/mm/hugetlbpage.c
@@ -234,6 +234,11 @@ int pud_huge(pud_t pud)
 	return 0;
 }
 
+int pmd_huge_support(void)
+{
+	return 0;
+}
+
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 			     pmd_t *pmd, int write)
 {
diff --git a/arch/tile/mm/hugetlbpage.c b/arch/tile/mm/hugetlbpage.c
index 650ccff8378c..0ac3599e5784 100644
--- a/arch/tile/mm/hugetlbpage.c
+++ b/arch/tile/mm/hugetlbpage.c
@@ -198,6 +198,11 @@ int pud_huge(pud_t pud)
 	return !!(pud_val(pud) & _PAGE_HUGE_PAGE);
 }
 
+int pmd_huge_support(void)
+{
+	return 1;
+}
+
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 			     pmd_t *pmd, int write)
 {
* Unmerged path arch/x86/mm/hugetlbpage.c
* Unmerged path include/linux/hugetlb.h
diff --git a/mm/migrate.c b/mm/migrate.c
index c1313d07c550..1ba8c877e860 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1009,6 +1009,16 @@ static int unmap_and_move_huge_page(new_page_t get_new_page,
 	struct page *new_hpage = get_new_page(hpage, private, &result);
 	struct anon_vma *anon_vma = NULL;
 
+	/*
+	 * Movability of hugepages depends on architectures and hugepage size.
+	 * This check is necessary because some callers of hugepage migration
+	 * like soft offline and memory hotremove don't walk through page
+	 * tables or check whether the hugepage is pmd-based or not before
+	 * kicking migration.
+	 */
+	if (!hugepage_migration_support(page_hstate(hpage)))
+		return -ENOSYS;
+
 	if (!new_hpage)
 		return -ENOMEM;
 
