IB/qib: Use rdmavt send and receive flags

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit 01ba79d4dddcf4ca3669f8dc4658322342793fee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/01ba79d4.failed

Use the definitions of the s_flags and r_flags which are now in rdmavt.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 01ba79d4dddcf4ca3669f8dc4658322342793fee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/qib_qp.c
#	drivers/infiniband/hw/qib/qib_rc.c
#	drivers/infiniband/hw/qib/qib_sdma.c
#	drivers/infiniband/hw/qib/qib_ud.c
#	drivers/infiniband/hw/qib/qib_verbs.c
diff --cc drivers/infiniband/hw/qib/qib_qp.c
index cf1dd6e9d434,c5e9cf5c90da..000000000000
--- a/drivers/infiniband/hw/qib/qib_qp.c
+++ b/drivers/infiniband/hw/qib/qib_qp.c
@@@ -490,20 -501,20 +490,26 @@@ int qib_error_qp(struct qib_qp *qp, enu
  		del_timer(&qp->s_timer);
  	}
  
- 	if (qp->s_flags & QIB_S_ANY_WAIT_SEND)
- 		qp->s_flags &= ~QIB_S_ANY_WAIT_SEND;
+ 	if (qp->s_flags & RVT_S_ANY_WAIT_SEND)
+ 		qp->s_flags &= ~RVT_S_ANY_WAIT_SEND;
  
++<<<<<<< HEAD
 +	spin_lock(&dev->pending_lock);
 +	if (!list_empty(&priv->iowait) && !(qp->s_flags & QIB_S_BUSY)) {
 +		qp->s_flags &= ~QIB_S_ANY_WAIT_IO;
++=======
+ 	spin_lock(&dev->rdi.pending_lock);
+ 	if (!list_empty(&priv->iowait) && !(qp->s_flags & RVT_S_BUSY)) {
+ 		qp->s_flags &= ~RVT_S_ANY_WAIT_IO;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		list_del_init(&priv->iowait);
  	}
 -	spin_unlock(&dev->rdi.pending_lock);
 +	spin_unlock(&dev->pending_lock);
  
- 	if (!(qp->s_flags & QIB_S_BUSY)) {
+ 	if (!(qp->s_flags & RVT_S_BUSY)) {
  		qp->s_hdrwords = 0;
  		if (qp->s_rdma_mr) {
 -			rvt_put_mr(qp->s_rdma_mr);
 +			qib_put_mr(qp->s_rdma_mr);
  			qp->s_rdma_mr = NULL;
  		}
  		if (priv->s_tx) {
@@@ -700,11 -712,11 +706,16 @@@ int qib_modify_qp(struct ib_qp *ibqp, s
  	case IB_QPS_RESET:
  		if (qp->state != IB_QPS_RESET) {
  			qp->state = IB_QPS_RESET;
 -			spin_lock(&dev->rdi.pending_lock);
 +			spin_lock(&dev->pending_lock);
  			if (!list_empty(&priv->iowait))
  				list_del_init(&priv->iowait);
++<<<<<<< HEAD
 +			spin_unlock(&dev->pending_lock);
 +			qp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);
++=======
+ 			spin_unlock(&dev->rdi.pending_lock);
+ 			qp->s_flags &= ~(RVT_S_TIMER | RVT_S_ANY_WAIT);
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  			spin_unlock(&qp->s_lock);
  			spin_unlock_irq(&qp->r_lock);
  			/* Stop the sending work queue and retry timer */
@@@ -1116,11 -1128,11 +1127,11 @@@ struct ib_qp *qib_create_qp(struct ib_p
  		qp->s_size = init_attr->cap.max_send_wr + 1;
  		qp->s_max_sge = init_attr->cap.max_send_sge;
  		if (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)
- 			qp->s_flags = QIB_S_SIGNAL_REQ_WR;
+ 			qp->s_flags = RVT_S_SIGNAL_REQ_WR;
  		dev = to_idev(ibpd->device);
  		dd = dd_from_dev(dev);
 -		err = alloc_qpn(dd, &dev->rdi.qp_dev->qpn_table,
 -				init_attr->qp_type, init_attr->port_num, gfp);
 +		err = alloc_qpn(dd, &dev->qpn_table, init_attr->qp_type,
 +				init_attr->port_num, gfp);
  		if (err < 0) {
  			ret = ERR_PTR(err);
  			vfree(qp->r_rq.wq);
@@@ -1228,11 -1240,11 +1239,16 @@@ int qib_destroy_qp(struct ib_qp *ibqp
  	spin_lock_irq(&qp->s_lock);
  	if (qp->state != IB_QPS_RESET) {
  		qp->state = IB_QPS_RESET;
 -		spin_lock(&dev->rdi.pending_lock);
 +		spin_lock(&dev->pending_lock);
  		if (!list_empty(&priv->iowait))
  			list_del_init(&priv->iowait);
++<<<<<<< HEAD
 +		spin_unlock(&dev->pending_lock);
 +		qp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);
++=======
+ 		spin_unlock(&dev->rdi.pending_lock);
+ 		qp->s_flags &= ~(RVT_S_TIMER | RVT_S_ANY_WAIT);
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		spin_unlock_irq(&qp->s_lock);
  		cancel_work_sync(&priv->s_work);
  		del_timer_sync(&qp->s_timer);
diff --cc drivers/infiniband/hw/qib/qib_rc.c
index c23ede5294da,e118004fab17..000000000000
--- a/drivers/infiniband/hw/qib/qib_rc.c
+++ b/drivers/infiniband/hw/qib/qib_rc.c
@@@ -54,9 -54,9 +54,9 @@@ static u32 restart_sge(struct qib_sge_s
  	return wqe->length - len;
  }
  
 -static void start_timer(struct rvt_qp *qp)
 +static void start_timer(struct qib_qp *qp)
  {
- 	qp->s_flags |= QIB_S_TIMER;
+ 	qp->s_flags |= RVT_S_TIMER;
  	qp->s_timer.function = rc_timeout;
  	/* 4.096 usec. * (1 << qp->timeout) */
  	qp->s_timer.expires = jiffies + qp->timeout_jiffies;
@@@ -369,15 -369,16 +369,15 @@@ int qib_make_rc_req(struct qib_qp *qp
  			/* FALLTHROUGH */
  		case IB_WR_RDMA_WRITE_WITH_IMM:
  			/* If no credit, return. */
- 			if (!(qp->s_flags & QIB_S_UNLIMITED_CREDIT) &&
+ 			if (!(qp->s_flags & RVT_S_UNLIMITED_CREDIT) &&
  			    qib_cmp24(wqe->ssn, qp->s_lsn + 1) > 0) {
- 				qp->s_flags |= QIB_S_WAIT_SSN_CREDIT;
+ 				qp->s_flags |= RVT_S_WAIT_SSN_CREDIT;
  				goto bail;
  			}
 -
  			ohdr->u.rc.reth.vaddr =
 -				cpu_to_be64(wqe->rdma_wr.remote_addr);
 +				cpu_to_be64(wqe->wr.wr.rdma.remote_addr);
  			ohdr->u.rc.reth.rkey =
 -				cpu_to_be32(wqe->rdma_wr.rkey);
 +				cpu_to_be32(wqe->wr.wr.rdma.rkey);
  			ohdr->u.rc.reth.length = cpu_to_be32(len);
  			hwords += sizeof(struct ib_reth) / sizeof(u32);
  			wqe->lpsn = wqe->psn;
@@@ -758,8 -760,8 +758,13 @@@ void qib_send_rc_ack(struct qib_qp *qp
  
  queue_ack:
  	if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {
++<<<<<<< HEAD
 +		ibp->n_rc_qacks++;
 +		qp->s_flags |= QIB_S_ACK_PENDING | QIB_S_RESP_PENDING;
++=======
+ 		this_cpu_inc(*ibp->rvp.rc_qacks);
+ 		qp->s_flags |= RVT_S_ACK_PENDING | RVT_S_RESP_PENDING;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		qp->s_nak_state = qp->r_nak_state;
  		qp->s_ack_psn = qp->r_ack_psn;
  
@@@ -886,15 -888,15 +891,15 @@@ static void qib_restart_rc(struct qib_q
  
  	ibp = to_iport(qp->ibqp.device, qp->port_num);
  	if (wqe->wr.opcode == IB_WR_RDMA_READ)
 -		ibp->rvp.n_rc_resends++;
 +		ibp->n_rc_resends++;
  	else
 -		ibp->rvp.n_rc_resends += (qp->s_psn - psn) & QIB_PSN_MASK;
 +		ibp->n_rc_resends += (qp->s_psn - psn) & QIB_PSN_MASK;
  
- 	qp->s_flags &= ~(QIB_S_WAIT_FENCE | QIB_S_WAIT_RDMAR |
- 			 QIB_S_WAIT_SSN_CREDIT | QIB_S_WAIT_PSN |
- 			 QIB_S_WAIT_ACK);
+ 	qp->s_flags &= ~(RVT_S_WAIT_FENCE | RVT_S_WAIT_RDMAR |
+ 			 RVT_S_WAIT_SSN_CREDIT | RVT_S_WAIT_PSN |
+ 			 RVT_S_WAIT_ACK);
  	if (wait)
- 		qp->s_flags |= QIB_S_SEND_ONE;
+ 		qp->s_flags |= RVT_S_SEND_ONE;
  	reset_psn(qp, psn);
  }
  
@@@ -909,10 -911,10 +914,15 @@@ static void rc_timeout(unsigned long ar
  
  	spin_lock_irqsave(&qp->r_lock, flags);
  	spin_lock(&qp->s_lock);
- 	if (qp->s_flags & QIB_S_TIMER) {
+ 	if (qp->s_flags & RVT_S_TIMER) {
  		ibp = to_iport(qp->ibqp.device, qp->port_num);
++<<<<<<< HEAD
 +		ibp->n_rc_timeouts++;
 +		qp->s_flags &= ~QIB_S_TIMER;
++=======
+ 		ibp->rvp.n_rc_timeouts++;
+ 		qp->s_flags &= ~RVT_S_TIMER;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		del_timer(&qp->s_timer);
  		qib_restart_rc(qp, qp->s_last_psn + 1, 1);
  		qib_schedule_send(qp);
@@@ -1011,12 -1013,12 +1021,12 @@@ void qib_rc_send_complete(struct qib_q
  		    qib_cmp24(qp->s_sending_psn, qp->s_sending_hpsn) <= 0)
  			break;
  		for (i = 0; i < wqe->wr.num_sge; i++) {
 -			struct rvt_sge *sge = &wqe->sg_list[i];
 +			struct qib_sge *sge = &wqe->sg_list[i];
  
 -			rvt_put_mr(sge->mr);
 +			qib_put_mr(sge->mr);
  		}
  		/* Post a send completion queue entry if requested. */
- 		if (!(qp->s_flags & QIB_S_SIGNAL_REQ_WR) ||
+ 		if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
  		    (wqe->wr.send_flags & IB_SEND_SIGNALED)) {
  			memset(&wc, 0, sizeof(wc));
  			wc.wr_id = wqe->wr.wr_id;
@@@ -1067,12 -1069,12 +1077,12 @@@ static struct qib_swqe *do_rc_completio
  	if (qib_cmp24(wqe->lpsn, qp->s_sending_psn) < 0 ||
  	    qib_cmp24(qp->s_sending_psn, qp->s_sending_hpsn) > 0) {
  		for (i = 0; i < wqe->wr.num_sge; i++) {
 -			struct rvt_sge *sge = &wqe->sg_list[i];
 +			struct qib_sge *sge = &wqe->sg_list[i];
  
 -			rvt_put_mr(sge->mr);
 +			qib_put_mr(sge->mr);
  		}
  		/* Post a send completion queue entry if requested. */
- 		if (!(qp->s_flags & QIB_S_SIGNAL_REQ_WR) ||
+ 		if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
  		    (wqe->wr.send_flags & IB_SEND_SIGNALED)) {
  			memset(&wc, 0, sizeof(wc));
  			wc.wr_id = wqe->wr.wr_id;
@@@ -1259,10 -1261,10 +1269,10 @@@ static int do_rc_ack(struct qib_qp *qp
  		goto bail;
  
  	case 1:         /* RNR NAK */
 -		ibp->rvp.n_rnr_naks++;
 +		ibp->n_rnr_naks++;
  		if (qp->s_acked == qp->s_tail)
  			goto bail;
- 		if (qp->s_flags & QIB_S_WAIT_RNR)
+ 		if (qp->s_flags & RVT_S_WAIT_RNR)
  			goto bail;
  		if (qp->s_rnr_retry == 0) {
  			status = IB_WC_RNR_RETRY_EXC_ERR;
@@@ -1348,14 -1350,14 +1358,14 @@@ bail
   * We have seen an out of sequence RDMA read middle or last packet.
   * This ACKs SENDs and RDMA writes up to the first RDMA read or atomic SWQE.
   */
 -static void rdma_seq_err(struct rvt_qp *qp, struct qib_ibport *ibp, u32 psn,
 +static void rdma_seq_err(struct qib_qp *qp, struct qib_ibport *ibp, u32 psn,
  			 struct qib_ctxtdata *rcd)
  {
 -	struct rvt_swqe *wqe;
 +	struct qib_swqe *wqe;
  
  	/* Remove QP from retry timer */
- 	if (qp->s_flags & (QIB_S_TIMER | QIB_S_WAIT_RNR)) {
- 		qp->s_flags &= ~(QIB_S_TIMER | QIB_S_WAIT_RNR);
+ 	if (qp->s_flags & (RVT_S_TIMER | RVT_S_WAIT_RNR)) {
+ 		qp->s_flags &= ~(RVT_S_TIMER | RVT_S_WAIT_RNR);
  		del_timer(&qp->s_timer);
  	}
  
@@@ -1369,11 -1371,11 +1379,16 @@@
  		wqe = do_rc_completion(qp, wqe, ibp);
  	}
  
++<<<<<<< HEAD
 +	ibp->n_rdma_seq++;
 +	qp->r_flags |= QIB_R_RDMAR_SEQ;
++=======
+ 	ibp->rvp.n_rdma_seq++;
+ 	qp->r_flags |= RVT_R_RDMAR_SEQ;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  	qib_restart_rc(qp, qp->s_last_psn + 1, 0);
  	if (list_empty(&qp->rspwait)) {
- 		qp->r_flags |= QIB_R_RSP_SEND;
+ 		qp->r_flags |= RVT_R_RSP_SEND;
  		atomic_inc(&qp->refcount);
  		list_add_tail(&qp->rspwait, &rcd->qp_wait_list);
  	}
diff --cc drivers/infiniband/hw/qib/qib_sdma.c
index ac4fcad97505,3819a6de9c33..000000000000
--- a/drivers/infiniband/hw/qib/qib_sdma.c
+++ b/drivers/infiniband/hw/qib/qib_sdma.c
@@@ -702,12 -702,12 +702,21 @@@ busy
  			struct qib_ibport *ibp;
  
  			ibp = &ppd->ibport_data;
++<<<<<<< HEAD
 +			ibp->n_dmawait++;
 +			qp->s_flags |= QIB_S_WAIT_DMA_DESC;
 +			list_add_tail(&priv->iowait, &dev->dmawait);
 +		}
 +		spin_unlock(&dev->pending_lock);
 +		qp->s_flags &= ~QIB_S_BUSY;
++=======
+ 			ibp->rvp.n_dmawait++;
+ 			qp->s_flags |= RVT_S_WAIT_DMA_DESC;
+ 			list_add_tail(&priv->iowait, &dev->dmawait);
+ 		}
+ 		spin_unlock(&dev->rdi.pending_lock);
+ 		qp->s_flags &= ~RVT_S_BUSY;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		spin_unlock(&qp->s_lock);
  		ret = -EBUSY;
  	} else {
diff --cc drivers/infiniband/hw/qib/qib_ud.c
index 75faa5bd8dd6,76f854e7aee8..000000000000
--- a/drivers/infiniband/hw/qib/qib_ud.c
+++ b/drivers/infiniband/hw/qib/qib_ud.c
@@@ -158,8 -159,8 +158,13 @@@ static void qib_ud_loopback(struct qib_
  	}
  	/* Silently drop packets which are too big. */
  	if (unlikely(wc.byte_len > qp->r_len)) {
++<<<<<<< HEAD
 +		qp->r_flags |= QIB_R_REUSE_SGE;
 +		ibp->n_pkt_drops++;
++=======
+ 		qp->r_flags |= RVT_R_REUSE_SGE;
+ 		ibp->rvp.n_pkt_drops++;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		goto bail_unlock;
  	}
  
diff --cc drivers/infiniband/hw/qib/qib_verbs.c
index c4417a1f33be,c1905348754a..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.c
+++ b/drivers/infiniband/hw/qib/qib_verbs.c
@@@ -950,11 -958,11 +950,16 @@@ static noinline struct qib_verbs_txreq 
  		if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK &&
  		    list_empty(&priv->iowait)) {
  			dev->n_txwait++;
- 			qp->s_flags |= QIB_S_WAIT_TX;
+ 			qp->s_flags |= RVT_S_WAIT_TX;
  			list_add_tail(&priv->iowait, &dev->txwait);
  		}
++<<<<<<< HEAD
 +		qp->s_flags &= ~QIB_S_BUSY;
 +		spin_unlock(&dev->pending_lock);
++=======
+ 		qp->s_flags &= ~RVT_S_BUSY;
+ 		spin_unlock(&dev->rdi.pending_lock);
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		spin_unlock_irqrestore(&qp->s_lock, flags);
  		tx = ERR_PTR(-EBUSY);
  	}
@@@ -1019,11 -1027,11 +1024,11 @@@ void qib_put_txreq(struct qib_verbs_txr
  		qp = priv->owner;
  		list_del_init(&priv->iowait);
  		atomic_inc(&qp->refcount);
 -		spin_unlock_irqrestore(&dev->rdi.pending_lock, flags);
 +		spin_unlock_irqrestore(&dev->pending_lock, flags);
  
  		spin_lock_irqsave(&qp->s_lock, flags);
- 		if (qp->s_flags & QIB_S_WAIT_TX) {
- 			qp->s_flags &= ~QIB_S_WAIT_TX;
+ 		if (qp->s_flags & RVT_S_WAIT_TX) {
+ 			qp->s_flags &= ~RVT_S_WAIT_TX;
  			qib_schedule_send(qp);
  		}
  		spin_unlock_irqrestore(&qp->s_lock, flags);
@@@ -1133,11 -1141,11 +1138,16 @@@ static int wait_kmem(struct qib_ibdev *
  		if (list_empty(&priv->iowait)) {
  			if (list_empty(&dev->memwait))
  				mod_timer(&dev->mem_timer, jiffies + 1);
- 			qp->s_flags |= QIB_S_WAIT_KMEM;
+ 			qp->s_flags |= RVT_S_WAIT_KMEM;
  			list_add_tail(&priv->iowait, &dev->memwait);
  		}
++<<<<<<< HEAD
 +		spin_unlock(&dev->pending_lock);
 +		qp->s_flags &= ~QIB_S_BUSY;
++=======
+ 		spin_unlock(&dev->rdi.pending_lock);
+ 		qp->s_flags &= ~RVT_S_BUSY;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		ret = -EBUSY;
  	}
  	spin_unlock_irqrestore(&qp->s_lock, flags);
@@@ -1266,16 -1274,16 +1276,21 @@@ static int no_bufs_available(struct qib
  	 */
  	spin_lock_irqsave(&qp->s_lock, flags);
  	if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {
 -		spin_lock(&dev->rdi.pending_lock);
 +		spin_lock(&dev->pending_lock);
  		if (list_empty(&priv->iowait)) {
  			dev->n_piowait++;
- 			qp->s_flags |= QIB_S_WAIT_PIO;
+ 			qp->s_flags |= RVT_S_WAIT_PIO;
  			list_add_tail(&priv->iowait, &dev->piowait);
  			dd = dd_from_dev(dev);
  			dd->f_wantpiobuf_intr(dd, 1);
  		}
++<<<<<<< HEAD
 +		spin_unlock(&dev->pending_lock);
 +		qp->s_flags &= ~QIB_S_BUSY;
++=======
+ 		spin_unlock(&dev->rdi.pending_lock);
+ 		qp->s_flags &= ~RVT_S_BUSY;
++>>>>>>> 01ba79d4dddc (IB/qib: Use rdmavt send and receive flags)
  		ret = -EBUSY;
  	}
  	spin_unlock_irqrestore(&qp->s_lock, flags);
@@@ -1388,10 -1396,10 +1403,10 @@@ done
   * @len: the length of the packet in bytes
   *
   * Return zero if packet is sent or queued OK.
-  * Return non-zero and clear qp->s_flags QIB_S_BUSY otherwise.
+  * Return non-zero and clear qp->s_flags RVT_S_BUSY otherwise.
   */
 -int qib_verbs_send(struct rvt_qp *qp, struct qib_ib_header *hdr,
 -		   u32 hdrwords, struct rvt_sge_state *ss, u32 len)
 +int qib_verbs_send(struct qib_qp *qp, struct qib_ib_header *hdr,
 +		   u32 hdrwords, struct qib_sge_state *ss, u32 len)
  {
  	struct qib_devdata *dd = dd_from_ibdev(qp->ibqp.device);
  	u32 plen;
diff --git a/drivers/infiniband/hw/qib/qib_driver.c b/drivers/infiniband/hw/qib/qib_driver.c
index 323352f4bb60..9060283224f0 100644
--- a/drivers/infiniband/hw/qib/qib_driver.c
+++ b/drivers/infiniband/hw/qib/qib_driver.c
@@ -414,7 +414,7 @@ static u32 qib_rcv_hdrerr(struct qib_ctxtdata *rcd, struct qib_pportdata *ppd,
 						 */
 						if (list_empty(&qp->rspwait)) {
 							qp->r_flags |=
-								QIB_R_RSP_NAK;
+								RVT_R_RSP_NAK;
 							atomic_inc(
 								&qp->refcount);
 							list_add_tail(
@@ -583,14 +583,14 @@ move_along:
 	 */
 	list_for_each_entry_safe(qp, nqp, &rcd->qp_wait_list, rspwait) {
 		list_del_init(&qp->rspwait);
-		if (qp->r_flags & QIB_R_RSP_NAK) {
-			qp->r_flags &= ~QIB_R_RSP_NAK;
+		if (qp->r_flags & RVT_R_RSP_NAK) {
+			qp->r_flags &= ~RVT_R_RSP_NAK;
 			qib_send_rc_ack(qp);
 		}
-		if (qp->r_flags & QIB_R_RSP_SEND) {
+		if (qp->r_flags & RVT_R_RSP_SEND) {
 			unsigned long flags;
 
-			qp->r_flags &= ~QIB_R_RSP_SEND;
+			qp->r_flags &= ~RVT_R_RSP_SEND;
 			spin_lock_irqsave(&qp->s_lock, flags);
 			if (ib_qib_state_ops[qp->state] &
 					QIB_PROCESS_OR_FLUSH_SEND)
* Unmerged path drivers/infiniband/hw/qib/qib_qp.c
* Unmerged path drivers/infiniband/hw/qib/qib_rc.c
diff --git a/drivers/infiniband/hw/qib/qib_ruc.c b/drivers/infiniband/hw/qib/qib_ruc.c
index e9132f7a68b0..30400ab66d3b 100644
--- a/drivers/infiniband/hw/qib/qib_ruc.c
+++ b/drivers/infiniband/hw/qib/qib_ruc.c
@@ -190,7 +190,7 @@ int qib_get_rwqe(struct qib_qp *qp, int wr_id_only)
 	qp->r_wr_id = wqe->wr_id;
 
 	ret = 1;
-	set_bit(QIB_R_WRID_VALID, &qp->r_aflags);
+	set_bit(RVT_R_WRID_VALID, &qp->r_aflags);
 	if (handler) {
 		u32 n;
 
@@ -376,11 +376,11 @@ static void qib_ruc_loopback(struct qib_qp *sqp)
 	spin_lock_irqsave(&sqp->s_lock, flags);
 
 	/* Return if we are already busy processing a work request. */
-	if ((sqp->s_flags & (QIB_S_BUSY | QIB_S_ANY_WAIT)) ||
+	if ((sqp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT)) ||
 	    !(ib_qib_state_ops[sqp->state] & QIB_PROCESS_OR_FLUSH_SEND))
 		goto unlock;
 
-	sqp->s_flags |= QIB_S_BUSY;
+	sqp->s_flags |= RVT_S_BUSY;
 
 again:
 	if (sqp->s_last == sqp->s_head)
@@ -545,7 +545,7 @@ again:
 	if (release)
 		qib_put_ss(&qp->r_sge);
 
-	if (!test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags))
+	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
 		goto send_comp;
 
 	if (wqe->wr.opcode == IB_WR_RDMA_WRITE_WITH_IMM)
@@ -590,7 +590,7 @@ rnr_nak:
 	spin_lock_irqsave(&sqp->s_lock, flags);
 	if (!(ib_qib_state_ops[sqp->state] & QIB_PROCESS_RECV_OK))
 		goto clr_busy;
-	sqp->s_flags |= QIB_S_WAIT_RNR;
+	sqp->s_flags |= RVT_S_WAIT_RNR;
 	sqp->s_timer.function = qib_rc_rnr_retry;
 	sqp->s_timer.expires = jiffies +
 		usecs_to_jiffies(ib_qib_rnr_table[qp->r_min_rnr_timer]);
@@ -620,7 +620,7 @@ serr:
 	if (sqp->ibqp.qp_type == IB_QPT_RC) {
 		int lastwqe = qib_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
 
-		sqp->s_flags &= ~QIB_S_BUSY;
+		sqp->s_flags &= ~RVT_S_BUSY;
 		spin_unlock_irqrestore(&sqp->s_lock, flags);
 		if (lastwqe) {
 			struct ib_event ev;
@@ -633,7 +633,7 @@ serr:
 		goto done;
 	}
 clr_busy:
-	sqp->s_flags &= ~QIB_S_BUSY;
+	sqp->s_flags &= ~RVT_S_BUSY;
 unlock:
 	spin_unlock_irqrestore(&sqp->s_lock, flags);
 done:
@@ -749,7 +749,7 @@ void qib_do_send(struct work_struct *work)
 		return;
 	}
 
-	qp->s_flags |= QIB_S_BUSY;
+	qp->s_flags |= RVT_S_BUSY;
 
 	spin_unlock_irqrestore(&qp->s_lock, flags);
 
@@ -792,7 +792,7 @@ void qib_send_complete(struct qib_qp *qp, struct qib_swqe *wqe,
 		atomic_dec(&to_iah(wqe->wr.wr.ud.ah)->refcount);
 
 	/* See ch. 11.2.4.1 and 10.7.3.1 */
-	if (!(qp->s_flags & QIB_S_SIGNAL_REQ_WR) ||
+	if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
 	    (wqe->wr.send_flags & IB_SEND_SIGNALED) ||
 	    status != IB_WC_SUCCESS) {
 		struct ib_wc wc;
* Unmerged path drivers/infiniband/hw/qib/qib_sdma.c
diff --git a/drivers/infiniband/hw/qib/qib_uc.c b/drivers/infiniband/hw/qib/qib_uc.c
index bab9aeb5dd9e..44f3b89169fd 100644
--- a/drivers/infiniband/hw/qib/qib_uc.c
+++ b/drivers/infiniband/hw/qib/qib_uc.c
@@ -65,7 +65,7 @@ int qib_make_uc_req(struct qib_qp *qp)
 			goto bail;
 		/* If DMAs are in progress, we can't flush immediately. */
 		if (atomic_read(&priv->s_dma_busy)) {
-			qp->s_flags |= QIB_S_WAIT_DMA;
+			qp->s_flags |= RVT_S_WAIT_DMA;
 			goto bail;
 		}
 		wqe = get_swqe_ptr(qp, qp->s_last);
@@ -221,7 +221,7 @@ done:
 	goto unlock;
 
 bail:
-	qp->s_flags &= ~QIB_S_BUSY;
+	qp->s_flags &= ~RVT_S_BUSY;
 unlock:
 	spin_unlock_irqrestore(&qp->s_lock, flags);
 	return ret;
@@ -279,7 +279,7 @@ void qib_uc_rcv(struct qib_ibport *ibp, struct qib_ib_header *hdr,
 inv:
 		if (qp->r_state == OP(SEND_FIRST) ||
 		    qp->r_state == OP(SEND_MIDDLE)) {
-			set_bit(QIB_R_REWIND_SGE, &qp->r_aflags);
+			set_bit(RVT_R_REWIND_SGE, &qp->r_aflags);
 			qp->r_sge.num_sge = 0;
 		} else
 			qib_put_ss(&qp->r_sge);
@@ -329,8 +329,8 @@ inv:
 		goto inv;
 	}
 
-	if (qp->state == IB_QPS_RTR && !(qp->r_flags & QIB_R_COMM_EST)) {
-		qp->r_flags |= QIB_R_COMM_EST;
+	if (qp->state == IB_QPS_RTR && !(qp->r_flags & RVT_R_COMM_EST)) {
+		qp->r_flags |= RVT_R_COMM_EST;
 		if (qp->ibqp.event_handler) {
 			struct ib_event ev;
 
@@ -347,7 +347,7 @@ inv:
 	case OP(SEND_ONLY):
 	case OP(SEND_ONLY_WITH_IMMEDIATE):
 send_first:
-		if (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))
+		if (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))
 			qp->r_sge = qp->s_rdma_read_sge;
 		else {
 			ret = qib_get_rwqe(qp, 0);
@@ -484,7 +484,7 @@ rdma_last_imm:
 		tlen -= (hdrsize + pad + 4);
 		if (unlikely(tlen + qp->r_rcv_len != qp->r_len))
 			goto drop;
-		if (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))
+		if (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))
 			qib_put_ss(&qp->s_rdma_read_sge);
 		else {
 			ret = qib_get_rwqe(qp, 1);
@@ -524,7 +524,7 @@ rdma_last:
 	return;
 
 rewind:
-	set_bit(QIB_R_REWIND_SGE, &qp->r_aflags);
+	set_bit(RVT_R_REWIND_SGE, &qp->r_aflags);
 	qp->r_sge.num_sge = 0;
 drop:
 	ibp->n_pkt_drops++;
* Unmerged path drivers/infiniband/hw/qib/qib_ud.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.c
diff --git a/drivers/infiniband/hw/qib/qib_verbs.h b/drivers/infiniband/hw/qib/qib_verbs.h
index ca366073af4f..953e525577d1 100644
--- a/drivers/infiniband/hw/qib/qib_verbs.h
+++ b/drivers/infiniband/hw/qib/qib_verbs.h
@@ -539,78 +539,6 @@ struct qib_qp {
 		____cacheline_aligned_in_smp;
 };
 
-/*
- * Atomic bit definitions for r_aflags.
- */
-#define QIB_R_WRID_VALID        0
-#define QIB_R_REWIND_SGE        1
-
-/*
- * Bit definitions for r_flags.
- */
-#define QIB_R_REUSE_SGE 0x01
-#define QIB_R_RDMAR_SEQ 0x02
-#define QIB_R_RSP_NAK   0x04
-#define QIB_R_RSP_SEND  0x08
-#define QIB_R_COMM_EST  0x10
-
-/*
- * Bit definitions for s_flags.
- *
- * QIB_S_SIGNAL_REQ_WR - set if QP send WRs contain completion signaled
- * QIB_S_BUSY - send tasklet is processing the QP
- * QIB_S_TIMER - the RC retry timer is active
- * QIB_S_ACK_PENDING - an ACK is waiting to be sent after RDMA read/atomics
- * QIB_S_WAIT_FENCE - waiting for all prior RDMA read or atomic SWQEs
- *                         before processing the next SWQE
- * QIB_S_WAIT_RDMAR - waiting for a RDMA read or atomic SWQE to complete
- *                         before processing the next SWQE
- * QIB_S_WAIT_RNR - waiting for RNR timeout
- * QIB_S_WAIT_SSN_CREDIT - waiting for RC credits to process next SWQE
- * QIB_S_WAIT_DMA - waiting for send DMA queue to drain before generating
- *                  next send completion entry not via send DMA
- * QIB_S_WAIT_PIO - waiting for a send buffer to be available
- * QIB_S_WAIT_TX - waiting for a struct qib_verbs_txreq to be available
- * QIB_S_WAIT_DMA_DESC - waiting for DMA descriptors to be available
- * QIB_S_WAIT_KMEM - waiting for kernel memory to be available
- * QIB_S_WAIT_PSN - waiting for a packet to exit the send DMA queue
- * QIB_S_WAIT_ACK - waiting for an ACK packet before sending more requests
- * QIB_S_SEND_ONE - send one packet, request ACK, then wait for ACK
- */
-#define QIB_S_SIGNAL_REQ_WR	0x0001
-#define QIB_S_BUSY		0x0002
-#define QIB_S_TIMER		0x0004
-#define QIB_S_RESP_PENDING	0x0008
-#define QIB_S_ACK_PENDING	0x0010
-#define QIB_S_WAIT_FENCE	0x0020
-#define QIB_S_WAIT_RDMAR	0x0040
-#define QIB_S_WAIT_RNR		0x0080
-#define QIB_S_WAIT_SSN_CREDIT	0x0100
-#define QIB_S_WAIT_DMA		0x0200
-#define QIB_S_WAIT_PIO		0x0400
-#define QIB_S_WAIT_TX		0x0800
-#define QIB_S_WAIT_DMA_DESC	0x1000
-#define QIB_S_WAIT_KMEM		0x2000
-#define QIB_S_WAIT_PSN		0x4000
-#define QIB_S_WAIT_ACK		0x8000
-#define QIB_S_SEND_ONE		0x10000
-#define QIB_S_UNLIMITED_CREDIT	0x20000
-
-/*
- * Wait flags that would prevent any packet type from being sent.
- */
-#define QIB_S_ANY_WAIT_IO (QIB_S_WAIT_PIO | QIB_S_WAIT_TX | \
-	QIB_S_WAIT_DMA_DESC | QIB_S_WAIT_KMEM)
-
-/*
- * Wait flags that would prevent send work requests from making progress.
- */
-#define QIB_S_ANY_WAIT_SEND (QIB_S_WAIT_FENCE | QIB_S_WAIT_RDMAR | \
-	QIB_S_WAIT_RNR | QIB_S_WAIT_SSN_CREDIT | QIB_S_WAIT_DMA | \
-	QIB_S_WAIT_PSN | QIB_S_WAIT_ACK)
-
-#define QIB_S_ANY_WAIT (QIB_S_ANY_WAIT_IO | QIB_S_ANY_WAIT_SEND)
-
 #define QIB_PSN_CREDIT  16
 
 /*
@@ -860,9 +788,9 @@ static inline struct qib_ibdev *to_idev(struct ib_device *ibdev)
  */
 static inline int qib_send_ok(struct qib_qp *qp)
 {
-	return !(qp->s_flags & (QIB_S_BUSY | QIB_S_ANY_WAIT_IO)) &&
-		(qp->s_hdrwords || (qp->s_flags & QIB_S_RESP_PENDING) ||
-		 !(qp->s_flags & QIB_S_ANY_WAIT_SEND));
+	return !(qp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT_IO)) &&
+		(qp->s_hdrwords || (qp->s_flags & RVT_S_RESP_PENDING) ||
+		 !(qp->s_flags & RVT_S_ANY_WAIT_SEND));
 }
 
 /*
