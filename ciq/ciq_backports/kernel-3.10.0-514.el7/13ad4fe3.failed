xfs: xfs_filemap_pmd_fault treats read faults as write faults

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit 13ad4fe3e087ab66a140f1e00d98f28aa4e3bb28
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/13ad4fe3.failed

The code initially committed didn't have the same checks for write
faults as the dax_pmd_fault code and hence treats all faults as
write faults. We can get read faults through this path because they
is no pmd_mkwrite path for write faults similar to the normal page
fault path. Hence we need to ensure that we only do c/mtime updates
on write faults, and freeze protection is unnecessary for read
faults.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 13ad4fe3e087ab66a140f1e00d98f28aa4e3bb28)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_file.c
index ff72b4ea0a57,ce208e3896aa..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -1530,26 -1476,11 +1530,33 @@@ xfs_file_llseek
   * ordering of:
   *
   * mmap_sem (MM)
++<<<<<<< HEAD
 + *   i_mmap_lock (XFS - truncate serialisation)
 + *     page_lock (MM)
 + *       i_lock (XFS - extent map serialisation)
++=======
+  *   sb_start_pagefault(vfs, freeze)
+  *     i_mmaplock (XFS - truncate serialisation)
+  *       page_lock (MM)
+  *         i_lock (XFS - extent map serialisation)
++>>>>>>> 13ad4fe3e087 (xfs: xfs_filemap_pmd_fault treats read faults as write faults)
   */
 +STATIC int
 +xfs_filemap_fault(
 +	struct vm_area_struct	*vma,
 +	struct vm_fault		*vmf)
 +{
 +	struct xfs_inode	*ip = XFS_I(vma->vm_file->f_mapping->host);
 +	int			error;
 +
 +	trace_xfs_filemap_fault(ip);
 +
 +	xfs_ilock(ip, XFS_MMAPLOCK_SHARED);
 +	error = filemap_fault(vma, vmf);
 +	xfs_iunlock(ip, XFS_MMAPLOCK_SHARED);
 +
 +	return error;
 +}
  
  /*
   * mmap()d file has taken write protection fault and is being made writable. We
@@@ -1562,21 -1493,149 +1569,132 @@@ xfs_filemap_page_mkwrite
  	struct vm_area_struct	*vma,
  	struct vm_fault		*vmf)
  {
 -	struct inode		*inode = file_inode(vma->vm_file);
 +	struct xfs_inode	*ip = XFS_I(vma->vm_file->f_mapping->host);
  	int			ret;
  
 -	trace_xfs_filemap_page_mkwrite(XFS_I(inode));
 +	trace_xfs_filemap_page_mkwrite(ip);
  
 -	sb_start_pagefault(inode->i_sb);
 +	sb_start_pagefault(VFS_I(ip)->i_sb);
  	file_update_time(vma->vm_file);
++<<<<<<< HEAD
++=======
+ 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 
+ 	if (IS_DAX(inode)) {
+ 		ret = __dax_mkwrite(vma, vmf, xfs_get_blocks_dax_fault, NULL);
+ 	} else {
+ 		ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
+ 		ret = block_page_mkwrite_return(ret);
+ 	}
+ 
+ 	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	sb_end_pagefault(inode->i_sb);
+ 
+ 	return ret;
+ }
+ 
+ STATIC int
+ xfs_filemap_fault(
+ 	struct vm_area_struct	*vma,
+ 	struct vm_fault		*vmf)
+ {
+ 	struct inode		*inode = file_inode(vma->vm_file);
+ 	int			ret;
+ 
+ 	trace_xfs_filemap_fault(XFS_I(inode));
+ 
+ 	/* DAX can shortcut the normal fault path on write faults! */
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && IS_DAX(inode))
+ 		return xfs_filemap_page_mkwrite(vma, vmf);
+ 
+ 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	if (IS_DAX(inode)) {
+ 		/*
+ 		 * we do not want to trigger unwritten extent conversion on read
+ 		 * faults - that is unnecessary overhead and would also require
+ 		 * changes to xfs_get_blocks_direct() to map unwritten extent
+ 		 * ioend for conversion on read-only mappings.
+ 		 */
+ 		ret = __dax_fault(vma, vmf, xfs_get_blocks_dax_fault, NULL);
+ 	} else
+ 		ret = filemap_fault(vma, vmf);
+ 	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Similar to xfs_filemap_fault(), the DAX fault path can call into here on
+  * both read and write faults. Hence we need to handle both cases. There is no
+  * ->pmd_mkwrite callout for huge pages, so we have a single function here to
+  * handle both cases here. @flags carries the information on the type of fault
+  * occuring.
+  */
+ STATIC int
+ xfs_filemap_pmd_fault(
+ 	struct vm_area_struct	*vma,
+ 	unsigned long		addr,
+ 	pmd_t			*pmd,
+ 	unsigned int		flags)
+ {
+ 	struct inode		*inode = file_inode(vma->vm_file);
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	int			ret;
+ 
+ 	if (!IS_DAX(inode))
+ 		return VM_FAULT_FALLBACK;
+ 
+ 	trace_xfs_filemap_pmd_fault(ip);
+ 
+ 	if (flags & FAULT_FLAG_WRITE) {
+ 		sb_start_pagefault(inode->i_sb);
+ 		file_update_time(vma->vm_file);
+ 	}
+ 
+ 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	ret = __dax_pmd_fault(vma, addr, pmd, flags, xfs_get_blocks_dax_fault,
+ 			      NULL);
+ 	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 
+ 	if (flags & FAULT_FLAG_WRITE)
+ 		sb_end_pagefault(inode->i_sb);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * pfn_mkwrite was originally inteneded to ensure we capture time stamp
+  * updates on write faults. In reality, it's need to serialise against
+  * truncate similar to page_mkwrite. Hence we open-code dax_pfn_mkwrite()
+  * here and cycle the XFS_MMAPLOCK_SHARED to ensure we serialise the fault
+  * barrier in place.
+  */
+ static int
+ xfs_filemap_pfn_mkwrite(
+ 	struct vm_area_struct	*vma,
+ 	struct vm_fault		*vmf)
+ {
+ 
+ 	struct inode		*inode = file_inode(vma->vm_file);
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	int			ret = VM_FAULT_NOPAGE;
+ 	loff_t			size;
+ 
+ 	trace_xfs_filemap_pfn_mkwrite(ip);
+ 
+ 	sb_start_pagefault(inode->i_sb);
+ 	file_update_time(vma->vm_file);
+ 
+ 	/* check if the faulting page hasn't raced with truncate */
++>>>>>>> 13ad4fe3e087 (xfs: xfs_filemap_pmd_fault treats read faults as write faults)
  	xfs_ilock(ip, XFS_MMAPLOCK_SHARED);
 -	size = (i_size_read(inode) + PAGE_SIZE - 1) >> PAGE_SHIFT;
 -	if (vmf->pgoff >= size)
 -		ret = VM_FAULT_SIGBUS;
 -	xfs_iunlock(ip, XFS_MMAPLOCK_SHARED);
 -	sb_end_pagefault(inode->i_sb);
 -	return ret;
  
 -}
 +	ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
  
 -static const struct vm_operations_struct xfs_file_vm_ops = {
 -	.fault		= xfs_filemap_fault,
 -	.pmd_fault	= xfs_filemap_pmd_fault,
 -	.map_pages	= filemap_map_pages,
 -	.page_mkwrite	= xfs_filemap_page_mkwrite,
 -	.pfn_mkwrite	= xfs_filemap_pfn_mkwrite,
 -};
 +	xfs_iunlock(ip, XFS_MMAPLOCK_SHARED);
 +	sb_end_pagefault(VFS_I(ip)->i_sb);
  
 -STATIC int
 -xfs_file_mmap(
 -	struct file	*filp,
 -	struct vm_area_struct *vma)
 -{
 -	file_accessed(filp);
 -	vma->vm_ops = &xfs_file_vm_ops;
 -	if (IS_DAX(file_inode(filp)))
 -		vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
 -	return 0;
 +	return block_page_mkwrite_return(ret);
  }
  
  const struct file_operations xfs_file_operations = {
* Unmerged path fs/xfs/xfs_file.c
