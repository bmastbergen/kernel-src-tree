mm: refactor do_wp_page, extract the reuse case

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] refactor do_wp_page, extract the reuse case (Eric Sandeen) [1274459]
Rebuild_FUZZ: 95.56%
commit-author Shachar Raindel <raindel@mellanox.com>
commit 4e047f897771222215ee572e1c0b25e9417376eb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/4e047f89.failed

Currently do_wp_page contains 265 code lines.  It also contains 9 goto
statements, of which 5 are targeting labels which are not cleanup
related.  This makes the function extremely difficult to understand.

The following patches are an attempt at breaking the function to its
basic components, and making it easier to understand.

The patches are straight forward function extractions from do_wp_page.
As we extract functions, we remove unneeded parameters and simplify the
code as much as possible.  However, the functionality is supposed to
remain completely unchanged.  The patches also attempt to document the
functionality of each extracted function.  In patch 2, we split the
unlock logic to the contain logic relevant to specific needs of each use
case, instead of having huge number of conditional decisions in a single
unlock flow.

This patch (of 4):

When do_wp_page is ending, in several cases it needs to reuse the existing
page.  This is achieved by making the page table writable, and possibly
updating the page-cache state.

Currently, this logic was "called" by using a goto jump.  This makes
following the control flow of the function harder.  It is also against the
coding style guidelines for using goto.

As the code can easily be refactored into a specialized function, refactor
it out and simplify the code flow in do_wp_page.

	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Rik van Riel <riel@redhat.com>
	Acked-by: Andi Kleen <ak@linux.intel.com>
	Acked-by: Haggai Eran <haggaie@mellanox.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Peter Feiner <pfeiner@google.com>
	Cc: Michel Lespinasse <walken@google.com>
	Reviewed-by: Michal Hocko <mhocko@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4e047f897771222215ee572e1c0b25e9417376eb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,e70685f3e836..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2792,6 -1950,98 +2792,101 @@@ static inline void cow_user_page(struc
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Notify the address space that the page is about to become writable so that
+  * it can prohibit this or wait for the page to get into an appropriate state.
+  *
+  * We do this without the lock held, so that it can sleep if it needs to.
+  */
+ static int do_page_mkwrite(struct vm_area_struct *vma, struct page *page,
+ 	       unsigned long address)
+ {
+ 	struct vm_fault vmf;
+ 	int ret;
+ 
+ 	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
+ 	vmf.pgoff = page->index;
+ 	vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
+ 	vmf.page = page;
+ 	vmf.cow_page = NULL;
+ 
+ 	ret = vma->vm_ops->page_mkwrite(vma, &vmf);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))
+ 		return ret;
+ 	if (unlikely(!(ret & VM_FAULT_LOCKED))) {
+ 		lock_page(page);
+ 		if (!page->mapping) {
+ 			unlock_page(page);
+ 			return 0; /* retry */
+ 		}
+ 		ret |= VM_FAULT_LOCKED;
+ 	} else
+ 		VM_BUG_ON_PAGE(!PageLocked(page), page);
+ 	return ret;
+ }
+ 
+ /*
+  * Handle write page faults for pages that can be reused in the current vma
+  *
+  * This can happen either due to the mapping being with the VM_SHARED flag,
+  * or due to us being the last reference standing to the page. In either
+  * case, all we need to do here is to mark the page as writable and update
+  * any related book-keeping.
+  */
+ static inline int wp_page_reuse(struct mm_struct *mm,
+ 			struct vm_area_struct *vma, unsigned long address,
+ 			pte_t *page_table, spinlock_t *ptl, pte_t orig_pte,
+ 			struct page *page, int page_mkwrite,
+ 			int dirty_shared)
+ 	__releases(ptl)
+ {
+ 	pte_t entry;
+ 	/*
+ 	 * Clear the pages cpupid information as the existing
+ 	 * information potentially belongs to a now completely
+ 	 * unrelated process.
+ 	 */
+ 	if (page)
+ 		page_cpupid_xchg_last(page, (1 << LAST_CPUPID_SHIFT) - 1);
+ 
+ 	flush_cache_page(vma, address, pte_pfn(orig_pte));
+ 	entry = pte_mkyoung(orig_pte);
+ 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	if (ptep_set_access_flags(vma, address, page_table, entry, 1))
+ 		update_mmu_cache(vma, address, page_table);
+ 	pte_unmap_unlock(page_table, ptl);
+ 
+ 	if (dirty_shared) {
+ 		struct address_space *mapping;
+ 		int dirtied;
+ 
+ 		if (!page_mkwrite)
+ 			lock_page(page);
+ 
+ 		dirtied = set_page_dirty(page);
+ 		VM_BUG_ON_PAGE(PageAnon(page), page);
+ 		mapping = page->mapping;
+ 		unlock_page(page);
+ 		page_cache_release(page);
+ 
+ 		if ((dirtied || page_mkwrite) && mapping) {
+ 			/*
+ 			 * Some device drivers do not set page.mapping
+ 			 * but still dirty their pages
+ 			 */
+ 			balance_dirty_pages_ratelimited(mapping);
+ 		}
+ 
+ 		if (!page_mkwrite)
+ 			file_update_time(vma->vm_file);
+ 	}
+ 
+ 	return VM_FAULT_WRITE;
+ }
+ 
+ /*
++>>>>>>> 4e047f897771 (mm: refactor do_wp_page, extract the reuse case)
   * This routine handles present pages, when users try to write
   * to a shared page. It is done by copying the page to a new address
   * and decrementing the shared-page counter for the old page.
@@@ -2817,10 -2067,9 +2912,13 @@@ static int do_wp_page(struct mm_struct 
  	struct page *old_page, *new_page = NULL;
  	pte_t entry;
  	int ret = 0;
++<<<<<<< HEAD
 +	int page_mkwrite = 0;
 +	struct page *dirty_page = NULL;
++=======
++>>>>>>> 4e047f897771 (mm: refactor do_wp_page, extract the reuse case)
  	unsigned long mmun_start = 0;	/* For mmu_notifiers */
  	unsigned long mmun_end = 0;	/* For mmu_notifiers */
 -	struct mem_cgroup *memcg;
  
  	old_page = vm_normal_page(vma, address, orig_pte);
  	if (!old_page) {
@@@ -2867,6 -2119,10 +2967,13 @@@
  		unlock_page(old_page);
  	} else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
  					(VM_WRITE|VM_SHARED))) {
++<<<<<<< HEAD
++=======
+ 		int page_mkwrite = 0;
+ 
+ 		page_cache_get(old_page);
+ 
++>>>>>>> 4e047f897771 (mm: refactor do_wp_page, extract the reuse case)
  		/*
  		 * Only catch write-faults on shared writable pages,
  		 * read-only shared pages can get COWed by
@@@ -2921,64 -2150,11 +3028,70 @@@
  				unlock_page(old_page);
  				goto unlock;
  			}
 +
  			page_mkwrite = 1;
  		}
++<<<<<<< HEAD
 +		dirty_page = old_page;
 +		get_page(dirty_page);
 +
 +reuse:
 +		/*
 +		 * Clear the pages cpupid information as the existing
 +		 * information potentially belongs to a now completely
 +		 * unrelated process.
 +		 */
 +		if (old_page)
 +			page_cpupid_xchg_last(old_page, (1 << LAST_CPUPID_SHIFT) - 1);
 +
 +		flush_cache_page(vma, address, pte_pfn(orig_pte));
 +		entry = pte_mkyoung(orig_pte);
 +		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +		if (ptep_set_access_flags(vma, address, page_table, entry,1))
 +			update_mmu_cache(vma, address, page_table);
 +		pte_unmap_unlock(page_table, ptl);
 +		ret |= VM_FAULT_WRITE;
 +
 +		if (!dirty_page)
 +			return ret;
 +
 +		/*
 +		 * Yes, Virginia, this is actually required to prevent a race
 +		 * with clear_page_dirty_for_io() from clearing the page dirty
 +		 * bit after it clear all dirty ptes, but before a racing
 +		 * do_wp_page installs a dirty pte.
 +		 *
 +		 * __do_fault is protected similarly.
 +		 */
 +		if (!page_mkwrite) {
 +			wait_on_page_locked(dirty_page);
 +			set_page_dirty_balance(dirty_page, page_mkwrite);
 +			/* file_update_time outside page_lock */
 +			if (vma->vm_file)
 +				file_update_time(vma->vm_file);
 +		}
 +		put_page(dirty_page);
 +		if (page_mkwrite) {
 +			struct address_space *mapping = dirty_page->mapping;
 +
 +			set_page_dirty(dirty_page);
 +			unlock_page(dirty_page);
 +			page_cache_release(dirty_page);
 +			if (mapping)	{
 +				/*
 +				 * Some device drivers do not set page.mapping
 +				 * but still dirty their pages
 +				 */
 +				balance_dirty_pages_ratelimited(mapping);
 +			}
 +		}
 +
 +		return ret;
++=======
+ 
+ 		return wp_page_reuse(mm, vma, address, page_table, ptl,
+ 				     orig_pte, old_page, page_mkwrite, 1);
++>>>>>>> 4e047f897771 (mm: refactor do_wp_page, extract the reuse case)
  	}
  
  	/*
* Unmerged path mm/memory.c
