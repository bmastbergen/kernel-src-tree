timekeeping: Create struct tk_read_base and use it in struct timekeeper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit d28ede83791defee9a81e558540699dc46dbbe13
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/d28ede83.failed

The members of the new struct are the required ones for the new NMI
safe accessor to clcok monotonic. In order to reuse the existing
timekeeping code and to make the update of the fast NMI safe
timekeepers a simple memcpy use the struct for the timekeeper as well
and convert all users.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
	Signed-off-by: John Stultz <john.stultz@linaro.org>
(cherry picked from commit d28ede83791defee9a81e558540699dc46dbbe13)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/vdso.c
#	arch/s390/kernel/time.c
#	arch/tile/kernel/time.c
#	arch/x86/kernel/vsyscall_gtod.c
#	arch/x86/kvm/x86.c
#	include/linux/timekeeper_internal.h
#	kernel/time/timekeeping.c
diff --cc arch/arm64/kernel/vdso.c
index 6a389dc1bd49,8296f7f5f0ba..000000000000
--- a/arch/arm64/kernel/vdso.c
+++ b/arch/arm64/kernel/vdso.c
@@@ -235,15 -220,15 +235,23 @@@ void update_vsyscall(struct timekeeper 
  	vdso_data->use_syscall			= use_syscall;
  	vdso_data->xtime_coarse_sec		= xtime_coarse.tv_sec;
  	vdso_data->xtime_coarse_nsec		= xtime_coarse.tv_nsec;
 -	vdso_data->wtm_clock_sec		= tk->wall_to_monotonic.tv_sec;
 -	vdso_data->wtm_clock_nsec		= tk->wall_to_monotonic.tv_nsec;
  
  	if (!use_syscall) {
++<<<<<<< HEAD
 +		vdso_data->cs_cycle_last	= tk->clock->cycle_last;
 +		vdso_data->xtime_clock_sec	= tk->xtime_sec;
 +		vdso_data->xtime_clock_nsec	= tk->xtime_nsec;
 +		vdso_data->cs_mult		= tk->mult;
 +		vdso_data->cs_shift		= tk->shift;
 +		vdso_data->wtm_clock_sec	= tk->wall_to_monotonic.tv_sec;
 +		vdso_data->wtm_clock_nsec	= tk->wall_to_monotonic.tv_nsec;
++=======
+ 		vdso_data->cs_cycle_last	= tk->tkr.cycle_last;
+ 		vdso_data->xtime_clock_sec	= tk->xtime_sec;
+ 		vdso_data->xtime_clock_nsec	= tk->tkr.xtime_nsec;
+ 		vdso_data->cs_mult		= tk->tkr.mult;
+ 		vdso_data->cs_shift		= tk->tkr.shift;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	}
  
  	smp_wmb();
diff --cc arch/s390/kernel/time.c
index 386d37a228bb,4cef607f3711..000000000000
--- a/arch/s390/kernel/time.c
+++ b/arch/s390/kernel/time.c
@@@ -220,14 -220,14 +220,24 @@@ void update_vsyscall(struct timekeeper 
  	/* Make userspace gettimeofday spin until we're done. */
  	++vdso_data->tb_update_count;
  	smp_wmb();
++<<<<<<< HEAD
 +	vdso_data->xtime_tod_stamp = tk->clock->cycle_last;
++=======
+ 	vdso_data->xtime_tod_stamp = tk->tkr.cycle_last;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	vdso_data->xtime_clock_sec = tk->xtime_sec;
- 	vdso_data->xtime_clock_nsec = tk->xtime_nsec;
+ 	vdso_data->xtime_clock_nsec = tk->tkr.xtime_nsec;
  	vdso_data->wtom_clock_sec =
  		tk->xtime_sec + tk->wall_to_monotonic.tv_sec;
++<<<<<<< HEAD
 +	vdso_data->wtom_clock_nsec = tk->xtime_nsec +
 +		+ (tk->wall_to_monotonic.tv_nsec << tk->shift);
 +	nsecps = (u64) NSEC_PER_SEC << tk->shift;
++=======
+ 	vdso_data->wtom_clock_nsec = tk->tkr.xtime_nsec +
+ 		+ ((u64) tk->wall_to_monotonic.tv_nsec << tk->tkr.shift);
+ 	nsecps = (u64) NSEC_PER_SEC << tk->tkr.shift;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	while (vdso_data->wtom_clock_nsec >= nsecps) {
  		vdso_data->wtom_clock_nsec -= nsecps;
  		vdso_data->wtom_clock_sec++;
diff --cc arch/tile/kernel/time.c
index 5ac397ec6986,d8fbc289e680..000000000000
--- a/arch/tile/kernel/time.c
+++ b/arch/tile/kernel/time.c
@@@ -235,5 -236,46 +235,50 @@@ cycles_t ns2cycles(unsigned long nsecs
  	 * clock frequency.
  	 */
  	struct clock_event_device *dev = &__raw_get_cpu_var(tile_timer);
++<<<<<<< HEAD
 +	return ((u64)nsecs * dev->mult) >> dev->shift;
++=======
+ 
+ 	/*
+ 	 * as in clocksource.h and x86's timer.h, we split the calculation
+ 	 * into 2 parts to avoid unecessary overflow of the intermediate
+ 	 * value. This will not lead to any loss of precision.
+ 	 */
+ 	u64 quot = (u64)nsecs >> dev->shift;
+ 	u64 rem  = (u64)nsecs & ((1ULL << dev->shift) - 1);
+ 	return quot * dev->mult + ((rem * dev->mult) >> dev->shift);
+ }
+ 
+ void update_vsyscall_tz(void)
+ {
+ 	/* Userspace gettimeofday will spin while this value is odd. */
+ 	++vdso_data->tz_update_count;
+ 	smp_wmb();
+ 	vdso_data->tz_minuteswest = sys_tz.tz_minuteswest;
+ 	vdso_data->tz_dsttime = sys_tz.tz_dsttime;
+ 	smp_wmb();
+ 	++vdso_data->tz_update_count;
+ }
+ 
+ void update_vsyscall(struct timekeeper *tk)
+ {
+ 	struct timespec *wtm = &tk->wall_to_monotonic;
+ 	struct clocksource *clock = tk->tkr.clock;
+ 
+ 	if (clock != &cycle_counter_cs)
+ 		return;
+ 
+ 	/* Userspace gettimeofday will spin while this value is odd. */
+ 	++vdso_data->tb_update_count;
+ 	smp_wmb();
+ 	vdso_data->xtime_tod_stamp = tk->tkr.cycle_last;
+ 	vdso_data->xtime_clock_sec = tk->xtime_sec;
+ 	vdso_data->xtime_clock_nsec = tk->tkr.xtime_nsec;
+ 	vdso_data->wtom_clock_sec = wtm->tv_sec;
+ 	vdso_data->wtom_clock_nsec = wtm->tv_nsec;
+ 	vdso_data->mult = tk->tkr.mult;
+ 	vdso_data->shift = tk->tkr.shift;
+ 	smp_wmb();
+ 	++vdso_data->tb_update_count;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  }
diff --cc arch/x86/kvm/x86.c
index 9891381973d7,b7e57946d1c1..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1100,22 -995,19 +1100,37 @@@ static void update_pvclock_gtod(struct 
  	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
  	u64 boot_ns;
  
++<<<<<<< HEAD
 +	boot_ns = timespec_to_ns(&tk->total_sleep_time)
 +		+ tk->wall_to_monotonic.tv_sec * (u64)NSEC_PER_SEC
 +		+ tk->wall_to_monotonic.tv_nsec
 +		+ tk->xtime_sec * (u64)NSEC_PER_SEC;
++=======
+ 	boot_ns = ktime_to_ns(ktime_add(tk->tkr.base_mono, tk->offs_boot));
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
  	write_seqcount_begin(&vdata->seq);
  
  	/* copy pvclock gtod data */
++<<<<<<< HEAD
 +	vdata->clock.vclock_mode	= tk->clock->archdata.vclock_mode;
 +	vdata->clock.cycle_last		= tk->clock->cycle_last;
 +	vdata->clock.mask		= tk->clock->mask;
 +	vdata->clock.mult		= tk->mult;
 +	vdata->clock.shift		= tk->shift;
 +
 +	vdata->boot_ns                  = boot_ns;
 +	vdata->nsec_base		= tk->xtime_nsec;
++=======
+ 	vdata->clock.vclock_mode	= tk->tkr.clock->archdata.vclock_mode;
+ 	vdata->clock.cycle_last		= tk->tkr.cycle_last;
+ 	vdata->clock.mask		= tk->tkr.mask;
+ 	vdata->clock.mult		= tk->tkr.mult;
+ 	vdata->clock.shift		= tk->tkr.shift;
+ 
+ 	vdata->boot_ns			= boot_ns;
+ 	vdata->nsec_base		= tk->tkr.xtime_nsec;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
  	write_seqcount_end(&vdata->seq);
  }
diff --cc include/linux/timekeeper_internal.h
index a44b704da541,97381997625b..000000000000
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@@ -10,92 -10,90 +10,174 @@@
  #include <linux/jiffies.h>
  #include <linux/time.h>
  
++<<<<<<< HEAD
 +/* Structure holding internal timekeeping values. */
 +struct timekeeper {
 +	/* Current clocksource used for timekeeping. */
 +	struct clocksource	*clock;
 +	/* NTP adjusted clock multiplier */
 +	u32			mult;
 +	/* The shift value of the current clocksource. */
 +	u32			shift;
 +	/* Number of clock cycles in one NTP interval. */
 +	cycle_t			cycle_interval;
 +	/* Last cycle value (also stored in clock->cycle_last) */
 +	cycle_t			cycle_last;
 +	/* Number of clock shifted nano seconds in one NTP interval. */
++=======
+ /**
+  * struct tk_read_base - base structure for timekeeping readout
+  * @clock:	Current clocksource used for timekeeping.
+  * @read:	Read function of @clock
+  * @mask:	Bitmask for two's complement subtraction of non 64bit clocks
+  * @cycle_last: @clock cycle value at last update
+  * @mult:	NTP adjusted multiplier for scaled math conversion
+  * @shift:	Shift value for scaled math conversion
+  * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+  * @base_mono:  ktime_t (nanoseconds) base time for readout
+  *
+  * This struct has size 56 byte on 64 bit. Together with a seqcount it
+  * occupies a single 64byte cache line.
+  *
+  * The struct is separate from struct timekeeper as it is also used
+  * for a fast NMI safe accessor to clock monotonic.
+  */
+ struct tk_read_base {
+ 	struct clocksource	*clock;
+ 	cycle_t			(*read)(struct clocksource *cs);
+ 	cycle_t			mask;
+ 	cycle_t			cycle_last;
+ 	u32			mult;
+ 	u32			shift;
+ 	u64			xtime_nsec;
+ 	ktime_t			base_mono;
+ };
+ 
+ /**
+  * struct timekeeper - Structure holding internal timekeeping values.
+  * @tkr:		The readout base structure
+  * @xtime_sec:		Current CLOCK_REALTIME time in seconds
+  * @wall_to_monotonic:	CLOCK_REALTIME to CLOCK_MONOTONIC offset
+  * @offs_real:		Offset clock monotonic -> clock realtime
+  * @offs_boot:		Offset clock monotonic -> clock boottime
+  * @offs_tai:		Offset clock monotonic -> clock tai
+  * @tai_offset:		The current UTC to TAI offset in seconds
+  * @base_raw:		Monotonic raw base time in ktime_t format
+  * @raw_time:		Monotonic raw base time in timespec64 format
+  * @cycle_interval:	Number of clock cycles in one NTP interval
+  * @xtime_interval:	Number of clock shifted nano seconds in one NTP
+  *			interval.
+  * @xtime_remainder:	Shifted nano seconds left over when rounding
+  *			@cycle_interval
+  * @raw_interval:	Raw nano seconds accumulated per NTP interval.
+  * @ntp_error:		Difference between accumulated time and NTP time in ntp
+  *			shifted nano seconds.
+  * @ntp_error_shift:	Shift conversion between clock shifted nano seconds and
+  *			ntp shifted nano seconds.
+  *
+  * Note: For timespec(64) based interfaces wall_to_monotonic is what
+  * we need to add to xtime (or xtime corrected for sub jiffie times)
+  * to get to monotonic time.  Monotonic is pegged at zero at system
+  * boot time, so wall_to_monotonic will be negative, however, we will
+  * ALWAYS keep the tv_nsec part positive so we can use the usual
+  * normalization.
+  *
+  * wall_to_monotonic is moved after resume from suspend for the
+  * monotonic time not to jump. We need to add total_sleep_time to
+  * wall_to_monotonic to get the real boot based time offset.
+  *
+  * wall_to_monotonic is no longer the boot time, getboottime must be
+  * used instead.
+  */
+ struct timekeeper {
+ 	struct tk_read_base	tkr;
+ 	u64			xtime_sec;
+ 	struct timespec64	wall_to_monotonic;
+ 	ktime_t			offs_real;
+ 	ktime_t			offs_boot;
+ 	ktime_t			offs_tai;
+ 	s32			tai_offset;
+ 	ktime_t			base_raw;
+ 	struct timespec64	raw_time;
+ 
+ 	/* The following members are for timekeeping internal use */
+ 	cycle_t			cycle_interval;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	u64			xtime_interval;
- 	/* shifted nano seconds left over when rounding cycle_interval */
  	s64			xtime_remainder;
- 	/* Raw nano seconds accumulated per NTP interval. */
  	u32			raw_interval;
++<<<<<<< HEAD
 +
 +	/* Current CLOCK_REALTIME time in seconds */
 +	u64			xtime_sec;
 +	/* Clock shifted nano seconds */
 +	u64			xtime_nsec;
 +
 +	/* Monotonic base time */
 +	ktime_t                 base_mono;
 +
 +	/* Difference between accumulated time and NTP time in ntp
 +	 * shifted nano seconds. */
 +	s64			ntp_error;
 +	/* Shift conversion between clock shifted nano seconds and
 +	 * ntp shifted nano seconds. */
++=======
+ 	s64			ntp_error;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	u32			ntp_error_shift;
 +
 +	/*
 +	 * wall_to_monotonic is what we need to add to xtime (or xtime corrected
 +	 * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
 +	 * at zero at system boot time, so wall_to_monotonic will be negative,
 +	 * however, we will ALWAYS keep the tv_nsec part positive so we can use
 +	 * the usual normalization.
 +	 *
 +	 * wall_to_monotonic is moved after resume from suspend for the
 +	 * monotonic time not to jump. We need to add total_sleep_time to
 +	 * wall_to_monotonic to get the real boot based time offset.
 +	 *
 +	 * - wall_to_monotonic is no longer the boot time, getboottime must be
 +	 * used instead.
 +	 */
 +	struct timespec64	wall_to_monotonic;
 +	/* Offset clock monotonic -> clock realtime */
 +	ktime_t			offs_real;
 +	/* time spent in suspend */
 +	struct timespec64	total_sleep_time;
 +	/* Offset clock monotonic -> clock boottime */
 +	ktime_t			offs_boot;
 +	/* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
 +	struct timespec64	raw_time;
 +	/* The current UTC to TAI offset in seconds */
 +	s32			tai_offset;
 +	/* Offset clock monotonic -> clock tai */
 +	ktime_t			offs_tai;
 +
 +	/* The ntp_tick_length() value currently being used.
 +	 * This cached copy ensures we consistently apply the tick
 +	 * length for an entire tick, as ntp_tick_length may change
 +	 * mid-tick, and we don't want to apply that new value to
 +	 * the tick in progress.
 +	 */
 +	u64			ntp_tick;
 +	u32			ntp_err_mult;
 +	/* The sequence number of clock was set events */
 +	unsigned int		clock_was_set_seq;
 +	/* CLOCK_MONOTONIC time value of a pending leap-second */
 +	ktime_t			next_leap_ktime;
  };
  
 +static inline struct timespec64 tk_xtime(struct timekeeper *tk)
 +{
 +	struct timespec64 ts;
 +
 +	ts.tv_sec = tk->xtime_sec;
 +	ts.tv_nsec = (long)(tk->xtime_nsec >> tk->shift);
 +	return ts;
 +}
 +
 +
  #ifdef CONFIG_GENERIC_TIME_VSYSCALL
  
  extern void update_vsyscall(struct timekeeper *tk);
diff --cc kernel/time/timekeeping.c
index 7251ed9665bf,ccb69980ef7e..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -50,6 -58,15 +50,18 @@@ static inline void tk_normalize_xtime(s
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static inline struct timespec64 tk_xtime(struct timekeeper *tk)
+ {
+ 	struct timespec64 ts;
+ 
+ 	ts.tv_sec = tk->xtime_sec;
+ 	ts.tv_nsec = (long)(tk->tkr.xtime_nsec >> tk->tkr.shift);
+ 	return ts;
+ }
+ 
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  static void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)
  {
  	tk->xtime_sec = ts->tv_sec;
@@@ -105,9 -119,11 +117,17 @@@ static void tk_setup_internals(struct t
  	u64 tmp, ntpinterval;
  	struct clocksource *old_clock;
  
++<<<<<<< HEAD
 +	old_clock = tk->clock;
 +	tk->clock = clock;
 +	tk->cycle_last = clock->cycle_last = clock->read(clock);
++=======
+ 	old_clock = tk->tkr.clock;
+ 	tk->tkr.clock = clock;
+ 	tk->tkr.read = clock->read;
+ 	tk->tkr.mask = clock->mask;
+ 	tk->tkr.cycle_last = tk->tkr.read(clock);
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
  	/* Do the ns -> cycle conversion first, using original mult */
  	tmp = NTP_INTERVAL_LENGTH;
@@@ -146,8 -161,7 +166,12 @@@
  	 * active clocksource. These value will be adjusted via NTP
  	 * to counteract clock drifting.
  	 */
++<<<<<<< HEAD
 +	tk->mult = clock->mult;
 +	tk->ntp_err_mult = 0;
++=======
+ 	tk->tkr.mult = clock->mult;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  }
  
  /* Timekeeper helper functions. */
@@@ -172,39 -179,72 +196,95 @@@ static inline s64 timekeeping_get_ns(st
  	s64 nsec;
  
  	/* read clocksource: */
++<<<<<<< HEAD
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +
 +	/* calculate the delta since the last update_wall_time: */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
++=======
+ 	cycle_now = tk->tkr.read(tk->tkr.clock);
  
- 	nsec = delta * tk->mult + tk->xtime_nsec;
- 	nsec >>= tk->shift;
+ 	/* calculate the delta since the last update_wall_time: */
+ 	delta = clocksource_delta(cycle_now, tk->tkr.cycle_last, tk->tkr.mask);
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
+ 
+ 	nsec = delta * tk->tkr.mult + tk->tkr.xtime_nsec;
+ 	nsec >>= tk->tkr.shift;
  
  	/* If arch requires, add in get_arch_timeoffset() */
 -	return nsec + arch_gettimeoffset();
 +	return nsec + get_arch_timeoffset();
  }
  
  static inline s64 timekeeping_get_ns_raw(struct timekeeper *tk)
  {
++<<<<<<< HEAD
++=======
+ 	struct clocksource *clock = tk->tkr.clock;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
  	s64 nsec;
  
  	/* read clocksource: */
++<<<<<<< HEAD
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +
 +	/* calculate the delta since the last update_wall_time: */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
++=======
+ 	cycle_now = tk->tkr.read(clock);
+ 
+ 	/* calculate the delta since the last update_wall_time: */
+ 	delta = clocksource_delta(cycle_now, tk->tkr.cycle_last, tk->tkr.mask);
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
  	/* convert delta to nanoseconds. */
  	nsec = clocksource_cyc2ns(delta, clock->mult, clock->shift);
  
  	/* If arch requires, add in get_arch_timeoffset() */
 -	return nsec + arch_gettimeoffset();
 +	return nsec + get_arch_timeoffset();
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 
+ static inline void update_vsyscall(struct timekeeper *tk)
+ {
+ 	struct timespec xt;
+ 
+ 	xt = tk_xtime(tk);
+ 	update_vsyscall_old(&xt, &tk->wall_to_monotonic, tk->tkr.clock, tk->tkr.mult,
+ 			    tk->tkr.cycle_last);
+ }
+ 
+ static inline void old_vsyscall_fixup(struct timekeeper *tk)
+ {
+ 	s64 remainder;
+ 
+ 	/*
+ 	* Store only full nanoseconds into xtime_nsec after rounding
+ 	* it up and add the remainder to the error difference.
+ 	* XXX - This is necessary to avoid small 1ns inconsistnecies caused
+ 	* by truncating the remainder in vsyscalls. However, it causes
+ 	* additional work to be done in timekeeping_adjust(). Once
+ 	* the vsyscall implementations are converted to use xtime_nsec
+ 	* (shifted nanoseconds), and CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 	* users are removed, this can be killed.
+ 	*/
+ 	remainder = tk->tkr.xtime_nsec & ((1ULL << tk->tkr.shift) - 1);
+ 	tk->tkr.xtime_nsec -= remainder;
+ 	tk->tkr.xtime_nsec += 1ULL << tk->tkr.shift;
+ 	tk->ntp_error += remainder << tk->ntp_error_shift;
+ 	tk->ntp_error -= (1ULL << tk->tkr.shift) << tk->ntp_error_shift;
+ }
+ #else
+ #define old_vsyscall_fixup(tk)
+ #endif
+ 
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
  
  static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
@@@ -275,7 -304,10 +355,14 @@@ static inline void tk_update_ktime_data
  	nsec = (s64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
  	nsec *= NSEC_PER_SEC;
  	nsec += tk->wall_to_monotonic.tv_nsec;
++<<<<<<< HEAD
 +	tk->base_mono = ns_to_ktime(nsec);
++=======
+ 	tk->tkr.base_mono = ns_to_ktime(nsec);
+ 
+ 	/* Update the monotonic raw base */
+ 	tk->base_raw = timespec64_to_ktime(tk->raw_time);
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  }
  
  /* must hold timekeeper_lock */
@@@ -308,19 -336,18 +395,33 @@@ static void timekeeping_update(struct t
   */
  static void timekeeping_forward_now(struct timekeeper *tk)
  {
++<<<<<<< HEAD
++=======
+ 	struct clocksource *clock = tk->tkr.clock;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
  	s64 nsec;
  
++<<<<<<< HEAD
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
 +	tk->cycle_last = clock->cycle_last = cycle_now;
++=======
+ 	cycle_now = tk->tkr.read(clock);
+ 	delta = clocksource_delta(cycle_now, tk->tkr.cycle_last, tk->tkr.mask);
+ 	tk->tkr.cycle_last = cycle_now;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
- 	tk->xtime_nsec += delta * tk->mult;
+ 	tk->tkr.xtime_nsec += delta * tk->tkr.mult;
  
  	/* If arch requires, add in get_arch_timeoffset() */
++<<<<<<< HEAD
 +	tk->xtime_nsec += (u64)get_arch_timeoffset() << tk->shift;
++=======
+ 	tk->tkr.xtime_nsec += (u64)arch_gettimeoffset() << tk->tkr.shift;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
  	tk_normalize_xtime(tk);
  
@@@ -383,19 -411,84 +484,96 @@@ ktime_t ktime_get(void
  	WARN_ON(timekeeping_suspended);
  
  	do {
++<<<<<<< HEAD
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +		secs = tk->xtime_sec + tk->wall_to_monotonic.tv_sec;
 +		nsecs = timekeeping_get_ns(tk) + tk->wall_to_monotonic.tv_nsec;
++=======
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = tk->tkr.base_mono;
+ 		nsecs = timekeeping_get_ns(tk);
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 -
 -	return ktime_add_ns(base, nsecs);
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
 +	/*
 +	 * Use ktime_set/ktime_add_ns to create a proper ktime on
 +	 * 32-bit architectures without CONFIG_KTIME_SCALAR.
 +	 */
 +	return ktime_add_ns(ktime_set(secs, 0), nsecs);
  }
  EXPORT_SYMBOL_GPL(ktime_get);
  
++<<<<<<< HEAD
++=======
+ static ktime_t *offsets[TK_OFFS_MAX] = {
+ 	[TK_OFFS_REAL]	= &tk_core.timekeeper.offs_real,
+ 	[TK_OFFS_BOOT]	= &tk_core.timekeeper.offs_boot,
+ 	[TK_OFFS_TAI]	= &tk_core.timekeeper.offs_tai,
+ };
+ 
+ ktime_t ktime_get_with_offset(enum tk_offsets offs)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base, *offset = offsets[offs];
+ 	s64 nsecs;
+ 
+ 	WARN_ON(timekeeping_suspended);
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = ktime_add(tk->tkr.base_mono, *offset);
+ 		nsecs = timekeeping_get_ns(tk);
+ 
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ 
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_with_offset);
+ 
+ /**
+  * ktime_mono_to_any() - convert mononotic time to any other time
+  * @tmono:	time to convert.
+  * @offs:	which offset to use
+  */
+ ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)
+ {
+ 	ktime_t *offset = offsets[offs];
+ 	unsigned long seq;
+ 	ktime_t tconv;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		tconv = ktime_add(tmono, *offset);
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return tconv;
+ }
+ EXPORT_SYMBOL_GPL(ktime_mono_to_any);
+ 
+ /**
+  * ktime_get_raw - Returns the raw monotonic time in ktime_t format
+  */
+ ktime_t ktime_get_raw(void)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base;
+ 	s64 nsecs;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = tk->base_raw;
+ 		nsecs = timekeeping_get_ns_raw(tk);
+ 
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_raw);
+ 
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  /**
   * ktime_get_ts64 - get the monotonic clock in timespec64 format
   * @ts:		pointer to timespec variable
@@@ -710,30 -757,15 +888,30 @@@ static int change_clocksource(void *dat
   */
  int timekeeping_notify(struct clocksource *clock)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	struct timekeeper *tk = &timekeeper;
  
- 	if (tk->clock == clock)
+ 	if (tk->tkr.clock == clock)
  		return 0;
  	stop_machine(change_clocksource, clock, NULL);
  	tick_clock_notify();
- 	return tk->clock == clock ? 0 : -1;
+ 	return tk->tkr.clock == clock ? 0 : -1;
  }
  
 +/**
 + * ktime_get_real - get the real (wall-) time in ktime_t format
 + *
 + * returns the time in ktime_t format
 + */
 +ktime_t ktime_get_real(void)
 +{
 +	struct timespec64 now;
 +
 +	getnstimeofday64(&now);
 +
 +	return timespec64_to_ktime(now);
 +}
 +EXPORT_SYMBOL_GPL(ktime_get_real);
 +
  /**
   * getrawmonotonic - Returns the raw monotonic time in a timespec
   * @ts:		pointer to the timespec to be set
@@@ -769,11 -801,11 +947,11 @@@ int timekeeping_valid_for_hres(void
  	int ret;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
- 		ret = tk->clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;
+ 		ret = tk->tkr.clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	return ret;
  }
@@@ -788,11 -820,11 +966,11 @@@ u64 timekeeping_max_deferment(void
  	u64 ret;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
- 		ret = tk->clock->max_idle_ns;
+ 		ret = tk->tkr.clock->max_idle_ns;
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	return ret;
  }
@@@ -957,8 -988,8 +1135,13 @@@ void timekeeping_inject_sleeptime(struc
   */
  static void timekeeping_resume(void)
  {
++<<<<<<< HEAD
 +	struct timekeeper *tk = &timekeeper;
 +	struct clocksource *clock = tk->clock;
++=======
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	struct clocksource *clock = tk->tkr.clock;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	unsigned long flags;
  	struct timespec64 ts_new, ts_delta;
  	struct timespec tmp;
@@@ -986,16 -1017,16 +1169,27 @@@
  	 * The less preferred source will only be tried if there is no better
  	 * usable source. The rtc part is handled separately in rtc core code.
  	 */
++<<<<<<< HEAD
 +	cycle_now = clock->read(clock);
 +	if ((clock->flags & CLOCK_SOURCE_SUSPEND_NONSTOP) &&
 +		cycle_now > clock->cycle_last) {
++=======
+ 	cycle_now = tk->tkr.read(clock);
+ 	if ((clock->flags & CLOCK_SOURCE_SUSPEND_NONSTOP) &&
+ 		cycle_now > tk->tkr.cycle_last) {
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  		u64 num, max = ULLONG_MAX;
  		u32 mult = clock->mult;
  		u32 shift = clock->shift;
  		s64 nsec = 0;
  
++<<<<<<< HEAD
 +		cycle_delta = clocksource_delta(cycle_now, clock->cycle_last,
 +						clock->mask);
++=======
+ 		cycle_delta = clocksource_delta(cycle_now, tk->tkr.cycle_last,
+ 						tk->tkr.mask);
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  
  		/*
  		 * "cycle_delta * mutl" may cause 64 bits overflow, if the
@@@ -1021,7 -1052,7 +1215,11 @@@
  		__timekeeping_inject_sleeptime(tk, &ts_delta);
  
  	/* Re-base the last cycle value */
++<<<<<<< HEAD
 +	tk->cycle_last = clock->cycle_last = cycle_now;
++=======
+ 	tk->tkr.cycle_last = cycle_now;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	tk->ntp_error = 0;
  	timekeeping_suspended = 0;
  	timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
@@@ -1103,25 -1136,116 +1301,92 @@@ static int __init timekeeping_init_ops(
  device_initcall(timekeeping_init_ops);
  
  /*
 - * If the error is already larger, we look ahead even further
 - * to compensate for late or lost adjustments.
 + * Apply a multiplier adjustment to the timekeeper
   */
 -static __always_inline int timekeeping_bigadjust(struct timekeeper *tk,
 -						 s64 error, s64 *interval,
 -						 s64 *offset)
 +static __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,
 +							 s64 offset,
 +							 bool negative,
 +							 int adj_scale)
  {
 -	s64 tick_error, i;
 -	u32 look_ahead, adj;
 -	s32 error2, mult;
 -
 -	/*
 -	 * Use the current error value to determine how much to look ahead.
 -	 * The larger the error the slower we adjust for it to avoid problems
 -	 * with losing too many ticks, otherwise we would overadjust and
 -	 * produce an even larger error.  The smaller the adjustment the
 -	 * faster we try to adjust for it, as lost ticks can do less harm
 -	 * here.  This is tuned so that an error of about 1 msec is adjusted
 -	 * within about 1 sec (or 2^20 nsec in 2^SHIFT_HZ ticks).
 -	 */
 -	error2 = tk->ntp_error >> (NTP_SCALE_SHIFT + 22 - 2 * SHIFT_HZ);
 -	error2 = abs(error2);
 -	for (look_ahead = 0; error2 > 0; look_ahead++)
 -		error2 >>= 2;
 +	s64 interval = tk->cycle_interval;
 +	s32 mult_adj = 1;
  
 -	/*
 -	 * Now calculate the error in (1 << look_ahead) ticks, but first
 -	 * remove the single look ahead already included in the error.
 -	 */
 -	tick_error = ntp_tick_length() >> (tk->ntp_error_shift + 1);
 -	tick_error -= tk->xtime_interval >> 1;
 -	error = ((error - tick_error) >> look_ahead) + tick_error;
 -
 -	/* Finally calculate the adjustment shift value.  */
 -	i = *interval;
 -	mult = 1;
 -	if (error < 0) {
 -		error = -error;
 -		*interval = -*interval;
 -		*offset = -*offset;
 -		mult = -1;
 +	if (negative) {
 +		mult_adj = -mult_adj;
 +		interval = -interval;
 +		offset  = -offset;
  	}
 -	for (adj = 0; error > i; adj++)
 -		error >>= 1;
 +	mult_adj <<= adj_scale;
 +	interval <<= adj_scale;
 +	offset <<= adj_scale;
  
++<<<<<<< HEAD
++=======
+ 	*interval <<= adj;
+ 	*offset <<= adj;
+ 	return mult << adj;
+ }
+ 
+ /*
+  * Adjust the multiplier to reduce the error value,
+  * this is optimized for the most common adjustments of -1,0,1,
+  * for other values we can do a bit more work.
+  */
+ static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
+ {
+ 	s64 error, interval = tk->cycle_interval;
+ 	int adj;
+ 
+ 	/*
+ 	 * The point of this is to check if the error is greater than half
+ 	 * an interval.
+ 	 *
+ 	 * First we shift it down from NTP_SHIFT to clocksource->shifted nsecs.
+ 	 *
+ 	 * Note we subtract one in the shift, so that error is really error*2.
+ 	 * This "saves" dividing(shifting) interval twice, but keeps the
+ 	 * (error > interval) comparison as still measuring if error is
+ 	 * larger than half an interval.
+ 	 *
+ 	 * Note: It does not "save" on aggravation when reading the code.
+ 	 */
+ 	error = tk->ntp_error >> (tk->ntp_error_shift - 1);
+ 	if (error > interval) {
+ 		/*
+ 		 * We now divide error by 4(via shift), which checks if
+ 		 * the error is greater than twice the interval.
+ 		 * If it is greater, we need a bigadjust, if its smaller,
+ 		 * we can adjust by 1.
+ 		 */
+ 		error >>= 2;
+ 		if (likely(error <= interval))
+ 			adj = 1;
+ 		else
+ 			adj = timekeeping_bigadjust(tk, error, &interval, &offset);
+ 	} else {
+ 		if (error < -interval) {
+ 			/* See comment above, this is just switched for the negative */
+ 			error >>= 2;
+ 			if (likely(error >= -interval)) {
+ 				adj = -1;
+ 				interval = -interval;
+ 				offset = -offset;
+ 			} else {
+ 				adj = timekeeping_bigadjust(tk, error, &interval, &offset);
+ 			}
+ 		} else {
+ 			goto out_adjust;
+ 		}
+ 	}
+ 
+ 	if (unlikely(tk->tkr.clock->maxadj &&
+ 		(tk->tkr.mult + adj > tk->tkr.clock->mult + tk->tkr.clock->maxadj))) {
+ 		printk_deferred_once(KERN_WARNING
+ 			"Adjusting %s more than 11%% (%ld vs %ld)\n",
+ 			tk->tkr.clock->name, (long)tk->tkr.mult + adj,
+ 			(long)tk->tkr.clock->mult + tk->tkr.clock->maxadj);
+ 	}
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	/*
  	 * So the following can be confusing.
  	 *
@@@ -1171,78 -1295,12 +1436,82 @@@
  	 *
  	 * XXX - TODO: Doc ntp_error calculation.
  	 */
++<<<<<<< HEAD
 +	tk->mult += mult_adj;
++=======
+ 	tk->tkr.mult += adj;
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  	tk->xtime_interval += interval;
- 	tk->xtime_nsec -= offset;
+ 	tk->tkr.xtime_nsec -= offset;
  	tk->ntp_error -= (interval - offset) << tk->ntp_error_shift;
 +}
 +
 +/*
 + * Calculate the multiplier adjustment needed to match the frequency
 + * specified by NTP
 + */
 +static __always_inline void timekeeping_freqadjust(struct timekeeper *tk,
 +							s64 offset)
 +{
 +	s64 interval = tk->cycle_interval;
 +	s64 xinterval = tk->xtime_interval;
 +	s64 tick_error;
 +	bool negative;
 +	u32 adj;
 +
 +	/* Remove any current error adj from freq calculation */
 +	if (tk->ntp_err_mult)
 +		xinterval -= tk->cycle_interval;
 +
 +	tk->ntp_tick = ntp_tick_length();
 +
 +	/* Calculate current error per tick */
 +	tick_error = ntp_tick_length() >> tk->ntp_error_shift;
 +	tick_error -= (xinterval + tk->xtime_remainder);
 +
 +	/* Don't worry about correcting it if its small */
 +	if (likely((tick_error >= 0) && (tick_error <= interval)))
 +		return;
 +
 +	/* preserve the direction of correction */
 +	negative = (tick_error < 0);
 +
 +	/* Sort out the magnitude of the correction */
 +	tick_error = abs(tick_error);
 +	for (adj = 0; tick_error > interval; adj++)
 +		tick_error >>= 1;
 +
 +	/* scale the corrections */
 +	timekeeping_apply_adjustment(tk, offset, negative, adj);
 +}
 +
 +/*
 + * Adjust the timekeeper's multiplier to the correct frequency
 + * and also to reduce the accumulated error value.
 + */
 +static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
 +{
 +	/* Correct for the current frequency error */
 +	timekeeping_freqadjust(tk, offset);
 +
 +	/* Next make a small adjustment to fix any cumulative error */
 +	if (!tk->ntp_err_mult && (tk->ntp_error > 0)) {
 +		tk->ntp_err_mult = 1;
 +		timekeeping_apply_adjustment(tk, offset, 0, 0);
 +	} else if (tk->ntp_err_mult && (tk->ntp_error <= 0)) {
 +		/* Undo any existing error adjustment */
 +		timekeeping_apply_adjustment(tk, offset, 1, 0);
 +		tk->ntp_err_mult = 0;
 +	}
 +
 +	if (unlikely(tk->clock->maxadj &&
 +		(tk->mult > tk->clock->mult + tk->clock->maxadj))) {
 +		printk_once(KERN_WARNING
 +			"Adjusting %s more than 11%% (%ld vs %ld)\n",
 +			tk->clock->name, (long)tk->mult,
 +			(long)tk->clock->mult + tk->clock->maxadj);
 +	}
  
 -out_adjust:
  	/*
  	 * It may be possible that when we entered this function, xtime_nsec
  	 * was very small.  Further, if we're slightly speeding the clocksource
@@@ -1257,11 -1315,12 +1526,11 @@@
  	 * We'll correct this error next time through this function, when
  	 * xtime_nsec is not as small.
  	 */
- 	if (unlikely((s64)tk->xtime_nsec < 0)) {
- 		s64 neg = -(s64)tk->xtime_nsec;
- 		tk->xtime_nsec = 0;
+ 	if (unlikely((s64)tk->tkr.xtime_nsec < 0)) {
+ 		s64 neg = -(s64)tk->tkr.xtime_nsec;
+ 		tk->tkr.xtime_nsec = 0;
  		tk->ntp_error += neg << tk->ntp_error_shift;
  	}
 -
  }
  
  /**
@@@ -1400,8 -1429,8 +1669,13 @@@ void update_wall_time(void
  #ifdef CONFIG_ARCH_USES_GETTIMEOFFSET
  	offset = real_tk->cycle_interval;
  #else
++<<<<<<< HEAD
 +	offset = clocksource_delta(clock->read(clock), clock->cycle_last,
 +				   clock->mask);
++=======
+ 	offset = clocksource_delta(tk->tkr.read(tk->tkr.clock),
+ 				   tk->tkr.cycle_last, tk->tkr.mask);
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
  #endif
  
  	/* Check if there's really nothing to do */
@@@ -1617,65 -1573,67 +1891,99 @@@ void do_timer(unsigned long ticks
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * ktime_get_update_offsets_tick - hrtimer helper
+  * @offs_real:	pointer to storage for monotonic -> realtime offset
+  * @offs_boot:	pointer to storage for monotonic -> boottime offset
+  * @offs_tai:	pointer to storage for monotonic -> clock tai offset
+  *
+  * Returns monotonic time at last tick and various offsets
+  */
+ ktime_t ktime_get_update_offsets_tick(ktime_t *offs_real, ktime_t *offs_boot,
+ 							ktime_t *offs_tai)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base;
+ 	u64 nsecs;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 
+ 		base = tk->tkr.base_mono;
+ 		nsecs = tk->tkr.xtime_nsec >> tk->tkr.shift;
+ 
+ 		*offs_real = tk->offs_real;
+ 		*offs_boot = tk->offs_boot;
+ 		*offs_tai = tk->offs_tai;
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ }
+ 
+ #ifdef CONFIG_HIGH_RES_TIMERS
+ /**
++>>>>>>> d28ede83791d (timekeeping: Create struct tk_read_base and use it in struct timekeeper)
   * ktime_get_update_offsets_now - hrtimer helper
 + * @cwsseq:	pointer to check and store the clock was set sequence number
   * @offs_real:	pointer to storage for monotonic -> realtime offset
   * @offs_boot:	pointer to storage for monotonic -> boottime offset
 - * @offs_tai:	pointer to storage for monotonic -> clock tai offset
   *
 - * Returns current monotonic time and updates the offsets
 - * Called from hrtimer_interrupt() or retrigger_next_event()
 + * Returns current monotonic time and updates the offsets if the
 + * sequence number in @cwsseq and timekeeper.clock_was_set_seq are
 + * different.
 + *
 + * Called from hrtimer_interupt() or retrigger_next_event()
   */
 -ktime_t ktime_get_update_offsets_now(ktime_t *offs_real, ktime_t *offs_boot,
 -							ktime_t *offs_tai)
 +ktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,
 +				     ktime_t *offs_boot, ktime_t *offs_tai)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	struct timekeeper *tk = &timekeeper;
  	unsigned int seq;
  	ktime_t base;
  	u64 nsecs;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
- 		base = tk->base_mono;
+ 		base = tk->tkr.base_mono;
  		nsecs = timekeeping_get_ns(tk);
 +		base = ktime_add_ns(base, nsecs);
  
 -		*offs_real = tk->offs_real;
 -		*offs_boot = tk->offs_boot;
 -		*offs_tai = tk->offs_tai;
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +		if (*cwsseq != tk->clock_was_set_seq) {
 +			*cwsseq = tk->clock_was_set_seq;
 +			*offs_real = tk->offs_real;
 +			*offs_boot = tk->offs_boot;
 +			*offs_tai = tk->offs_tai;
 +		}
  
 -	return ktime_add_ns(base, nsecs);
 +		/* Handle leapsecond insertion adjustments */
 +		if (unlikely(base.tv64 >= tk->next_leap_ktime.tv64))
 +			*offs_real = ktime_sub(tk->offs_real, ktime_set(1, 0));
 +
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
 +
 +	return base;
  }
 -#endif
 +
 +/**
 + * ktime_get_monotonic_offset() - get wall_to_monotonic in ktime_t format
 + */
 +ktime_t ktime_get_monotonic_offset(void)
 +{
 +	struct timekeeper *tk = &timekeeper;
 +	unsigned long seq;
 +	struct timespec64 wtom;
 +
 +	do {
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +		wtom = tk->wall_to_monotonic;
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
 +
 +	return timespec64_to_ktime(wtom);
 +}
 +EXPORT_SYMBOL_GPL(ktime_get_monotonic_offset);
  
  /**
   * do_adjtimex() - Accessor function to NTP __do_adjtimex function
* Unmerged path arch/x86/kernel/vsyscall_gtod.c
* Unmerged path arch/arm64/kernel/vdso.c
* Unmerged path arch/s390/kernel/time.c
* Unmerged path arch/tile/kernel/time.c
* Unmerged path arch/x86/kernel/vsyscall_gtod.c
* Unmerged path arch/x86/kvm/x86.c
* Unmerged path include/linux/timekeeper_internal.h
* Unmerged path kernel/time/timekeeping.c
