libceph, rbd: ceph_osd_linger_request, watch/notify v2

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Ilya Dryomov <idryomov@gmail.com>
commit 922dab6134178cae317ae00de86376cba59f3147
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/922dab61.failed

This adds support and switches rbd to a new, more reliable version of
watch/notify protocol.  As with the OSD client update, this is mostly
about getting the right structures linked into the right places so that
reconnects are properly sent when needed.  watch/notify v2 also
requires sending regular pings to the OSDs - send_linger_ping().

A major change from the old watch/notify implementation is the
introduction of ceph_osd_linger_request - linger requests no longer
piggy back on ceph_osd_request.  ceph_osd_event has been merged into
ceph_osd_linger_request.

All the details are now hidden within libceph, the interface consists
of a simple pair of watch/unwatch functions and ceph_osdc_notify_ack().
ceph_osdc_watch() does return ceph_osd_linger_request, but only to keep
the lifetime management simple.

ceph_osdc_notify_ack() accepts an optional data payload, which is
relayed back to the notifier.

Portions of this patch are loosely based on work by Douglas Fuller
<dfuller@redhat.com> and Mike Christie <michaelc@cs.wisc.edu>.

	Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
(cherry picked from commit 922dab6134178cae317ae00de86376cba59f3147)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/ceph/osd_client.h
#	net/ceph/debugfs.c
#	net/ceph/osd_client.c
diff --cc include/linux/ceph/osd_client.h
index cc16ab3e4c14,cd2dcb8939de..000000000000
--- a/include/linux/ceph/osd_client.h
+++ b/include/linux/ceph/osd_client.h
@@@ -32,8 -33,8 +32,13 @@@ struct ceph_osd 
  	int o_incarnation;
  	struct rb_node o_node;
  	struct ceph_connection o_con;
++<<<<<<< HEAD
 +	struct list_head o_requests;
 +	struct list_head o_linger_requests;
++=======
+ 	struct rb_root o_requests;
+ 	struct rb_root o_linger_requests;
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	struct list_head o_osd_lru;
  	struct ceph_auth_handshake o_auth;
  	unsigned long lru_ttl;
@@@ -122,34 -146,21 +128,41 @@@ struct ceph_osd_req_op 
  struct ceph_osd_request {
  	u64             r_tid;              /* unique for this client */
  	struct rb_node  r_node;
++<<<<<<< HEAD
 +	struct list_head r_req_lru_item;
 +	struct list_head r_osd_item;
 +	struct list_head r_linger_item;
 +	struct list_head r_linger_osd_item;
++=======
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	struct ceph_osd *r_osd;
 -
 -	struct ceph_osd_request_target r_t;
 -#define r_base_oid	r_t.base_oid
 -#define r_base_oloc	r_t.base_oloc
 -#define r_flags		r_t.flags
 +	struct ceph_pg   r_pgid;
 +	int              r_pg_osds[CEPH_PG_MAX_SIZE];
 +	int              r_num_pg_osds;
  
  	struct ceph_msg  *r_request, *r_reply;
 +	int               r_flags;     /* any additional flags for the osd */
  	u32               r_sent;      /* >0 if r_request is sending/sent */
  
  	/* request osd ops array  */
  	unsigned int		r_num_ops;
  
 +	/* these are updated on each send */
 +	__le32           *r_request_osdmap_epoch;
 +	__le32           *r_request_flags;
 +	__le64           *r_request_pool;
 +	void             *r_request_pgid;
 +	__le32           *r_request_attempts;
 +	bool              r_paused;
 +	struct ceph_eversion *r_request_reassert_version;
 +
  	int               r_result;
++<<<<<<< HEAD
 +	int               r_got_reply;
 +	int		  r_linger;
++=======
+ 	bool              r_got_reply;
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  
  	struct ceph_osd_client *r_osdc;
  	struct kref       r_kref;
@@@ -163,15 -174,18 +176,24 @@@
  	struct inode *r_inode;         	      /* for use by callbacks */
  	void *r_priv;			      /* ditto */
  
++<<<<<<< HEAD
 +	struct ceph_object_locator r_base_oloc;
 +	struct ceph_object_id r_base_oid;
 +	struct ceph_object_locator r_target_oloc;
 +	struct ceph_object_id r_target_oid;
++=======
+ 	/* set by submitter */
+ 	u64 r_snapid;                         /* for reads, CEPH_NOSNAP o/w */
+ 	struct ceph_snap_context *r_snapc;    /* for writes */
+ 	struct timespec r_mtime;              /* ditto */
+ 	u64 r_data_offset;                    /* ditto */
+ 	bool r_linger;                        /* don't resend on failure */
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  
 -	/* internal */
 -	unsigned long r_stamp;                /* jiffies, send or check time */
 -	int r_attempts;
 -	struct ceph_eversion r_replay_version; /* aka reassert_version */
 -	u32 r_last_force_resend;
 +	u64               r_snapid;
 +	unsigned long     r_stamp;            /* send OR check time */
 +
 +	struct ceph_snap_context *r_snapc;    /* snap context for writes */
  
  	struct ceph_osd_req_op r_ops[];
  };
@@@ -203,18 -234,17 +242,28 @@@ struct ceph_osd_client 
  	struct ceph_client     *client;
  
  	struct ceph_osdmap     *osdmap;       /* current map */
 -	struct rw_semaphore    lock;
 +	struct rw_semaphore    map_sem;
  
 +	struct mutex           request_mutex;
  	struct rb_root         osds;          /* osds */
  	struct list_head       osd_lru;       /* idle osds */
++<<<<<<< HEAD
 +	u64                    last_tid;      /* tid of last request */
 +	struct rb_root         requests;      /* pending requests */
 +	struct list_head       req_lru;	      /* in-flight lru */
 +	struct list_head       req_unsent;    /* unsent/need-resend queue */
 +	struct list_head       req_notarget;  /* map to no osd */
 +	struct list_head       req_linger;    /* lingering requests */
 +	int                    num_requests;
++=======
+ 	spinlock_t             osd_lru_lock;
+ 	struct ceph_osd        homeless_osd;
+ 	atomic64_t             last_tid;      /* tid of last request */
+ 	u64                    last_linger_id;
+ 	struct rb_root         linger_requests; /* lingering requests */
+ 	atomic_t               num_requests;
+ 	atomic_t               num_homeless;
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	struct delayed_work    timeout_work;
  	struct delayed_work    osds_timeout_work;
  #ifdef CONFIG_DEBUG_FS
diff --cc net/ceph/debugfs.c
index 1633b622f0f7,e64cb8583533..000000000000
--- a/net/ceph/debugfs.c
+++ b/net/ceph/debugfs.c
@@@ -140,43 -145,112 +140,140 @@@ static int monc_show(struct seq_file *s
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static void dump_target(struct seq_file *s, struct ceph_osd_request_target *t)
+ {
+ 	int i;
+ 
+ 	seq_printf(s, "osd%d\t%llu.%x\t[", t->osd, t->pgid.pool, t->pgid.seed);
+ 	for (i = 0; i < t->up.size; i++)
+ 		seq_printf(s, "%s%d", (!i ? "" : ","), t->up.osds[i]);
+ 	seq_printf(s, "]/%d\t[", t->up.primary);
+ 	for (i = 0; i < t->acting.size; i++)
+ 		seq_printf(s, "%s%d", (!i ? "" : ","), t->acting.osds[i]);
+ 	seq_printf(s, "]/%d\t%*pE\t0x%x", t->acting.primary,
+ 		   t->target_oid.name_len, t->target_oid.name, t->flags);
+ 	if (t->paused)
+ 		seq_puts(s, "\tP");
+ }
+ 
+ static void dump_request(struct seq_file *s, struct ceph_osd_request *req)
+ {
+ 	int i;
+ 
+ 	seq_printf(s, "%llu\t", req->r_tid);
+ 	dump_target(s, &req->r_t);
+ 
+ 	seq_printf(s, "\t%d\t%u'%llu", req->r_attempts,
+ 		   le32_to_cpu(req->r_replay_version.epoch),
+ 		   le64_to_cpu(req->r_replay_version.version));
+ 
+ 	for (i = 0; i < req->r_num_ops; i++) {
+ 		struct ceph_osd_req_op *op = &req->r_ops[i];
+ 
+ 		seq_printf(s, "%s%s", (i == 0 ? "\t" : ","),
+ 			   ceph_osd_op_name(op->op));
+ 		if (op->op == CEPH_OSD_OP_WATCH)
+ 			seq_printf(s, "-%s",
+ 				   ceph_osd_watch_op_name(op->watch.op));
+ 	}
+ 
+ 	seq_putc(s, '\n');
+ }
+ 
+ static void dump_requests(struct seq_file *s, struct ceph_osd *osd)
+ {
+ 	struct rb_node *n;
+ 
+ 	mutex_lock(&osd->lock);
+ 	for (n = rb_first(&osd->o_requests); n; n = rb_next(n)) {
+ 		struct ceph_osd_request *req =
+ 		    rb_entry(n, struct ceph_osd_request, r_node);
+ 
+ 		dump_request(s, req);
+ 	}
+ 
+ 	mutex_unlock(&osd->lock);
+ }
+ 
+ static void dump_linger_request(struct seq_file *s,
+ 				struct ceph_osd_linger_request *lreq)
+ {
+ 	seq_printf(s, "%llu\t", lreq->linger_id);
+ 	dump_target(s, &lreq->t);
+ 
+ 	seq_printf(s, "\t%u\t%s/%d\n", lreq->register_gen,
+ 		   lreq->committed ? "C" : "", lreq->last_error);
+ }
+ 
+ static void dump_linger_requests(struct seq_file *s, struct ceph_osd *osd)
+ {
+ 	struct rb_node *n;
+ 
+ 	mutex_lock(&osd->lock);
+ 	for (n = rb_first(&osd->o_linger_requests); n; n = rb_next(n)) {
+ 		struct ceph_osd_linger_request *lreq =
+ 		    rb_entry(n, struct ceph_osd_linger_request, node);
+ 
+ 		dump_linger_request(s, lreq);
+ 	}
+ 
+ 	mutex_unlock(&osd->lock);
+ }
+ 
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  static int osdc_show(struct seq_file *s, void *pp)
  {
  	struct ceph_client *client = s->private;
  	struct ceph_osd_client *osdc = &client->osdc;
 -	struct rb_node *n;
 +	struct rb_node *p;
  
 -	down_read(&osdc->lock);
 -	seq_printf(s, "REQUESTS %d homeless %d\n",
 -		   atomic_read(&osdc->num_requests),
 -		   atomic_read(&osdc->num_homeless));
 -	for (n = rb_first(&osdc->osds); n; n = rb_next(n)) {
 -		struct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);
 +	mutex_lock(&osdc->request_mutex);
 +	for (p = rb_first(&osdc->requests); p; p = rb_next(p)) {
 +		struct ceph_osd_request *req;
 +		unsigned int i;
 +		int opcode;
 +
 +		req = rb_entry(p, struct ceph_osd_request, r_node);
 +
 +		seq_printf(s, "%lld\tosd%d\t%lld.%x\t", req->r_tid,
 +			   req->r_osd ? req->r_osd->o_osd : -1,
 +			   req->r_pgid.pool, req->r_pgid.seed);
 +
 +		seq_printf(s, "%.*s", req->r_base_oid.name_len,
 +			   req->r_base_oid.name);
 +
 +		if (req->r_reassert_version.epoch)
 +			seq_printf(s, "\t%u'%llu",
 +			   (unsigned int)le32_to_cpu(req->r_reassert_version.epoch),
 +			   le64_to_cpu(req->r_reassert_version.version));
 +		else
 +			seq_printf(s, "\t");
 +
 +		for (i = 0; i < req->r_num_ops; i++) {
 +			opcode = req->r_ops[i].op;
 +			seq_printf(s, "%s%s", (i == 0 ? "\t" : ","),
 +				   ceph_osd_op_name(opcode));
 +		}
  
 -		dump_requests(s, osd);
 +		seq_printf(s, "\n");
  	}
++<<<<<<< HEAD
 +	mutex_unlock(&osdc->request_mutex);
++=======
+ 	dump_requests(s, &osdc->homeless_osd);
+ 
+ 	seq_puts(s, "LINGER REQUESTS\n");
+ 	for (n = rb_first(&osdc->osds); n; n = rb_next(n)) {
+ 		struct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);
+ 
+ 		dump_linger_requests(s, osd);
+ 	}
+ 	dump_linger_requests(s, &osdc->homeless_osd);
+ 
+ 	up_read(&osdc->lock);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	return 0;
  }
  
diff --cc net/ceph/osd_client.c
index b1bd089d52f0,ca0a7b58ba4f..000000000000
--- a/net/ceph/osd_client.c
+++ b/net/ceph/osd_client.c
@@@ -56,6 -43,52 +56,55 @@@ static void __send_request(struct ceph_
   * channel with an OSD is reset.
   */
  
++<<<<<<< HEAD
++=======
+ static void link_request(struct ceph_osd *osd, struct ceph_osd_request *req);
+ static void unlink_request(struct ceph_osd *osd, struct ceph_osd_request *req);
+ static void link_linger(struct ceph_osd *osd,
+ 			struct ceph_osd_linger_request *lreq);
+ static void unlink_linger(struct ceph_osd *osd,
+ 			  struct ceph_osd_linger_request *lreq);
+ 
+ #if 1
+ static inline bool rwsem_is_wrlocked(struct rw_semaphore *sem)
+ {
+ 	bool wrlocked = true;
+ 
+ 	if (unlikely(down_read_trylock(sem))) {
+ 		wrlocked = false;
+ 		up_read(sem);
+ 	}
+ 
+ 	return wrlocked;
+ }
+ static inline void verify_osdc_locked(struct ceph_osd_client *osdc)
+ {
+ 	WARN_ON(!rwsem_is_locked(&osdc->lock));
+ }
+ static inline void verify_osdc_wrlocked(struct ceph_osd_client *osdc)
+ {
+ 	WARN_ON(!rwsem_is_wrlocked(&osdc->lock));
+ }
+ static inline void verify_osd_locked(struct ceph_osd *osd)
+ {
+ 	struct ceph_osd_client *osdc = osd->o_osdc;
+ 
+ 	WARN_ON(!(mutex_is_locked(&osd->lock) &&
+ 		  rwsem_is_locked(&osdc->lock)) &&
+ 		!rwsem_is_wrlocked(&osdc->lock));
+ }
+ static inline void verify_lreq_locked(struct ceph_osd_linger_request *lreq)
+ {
+ 	WARN_ON(!mutex_is_locked(&lreq->lock));
+ }
+ #else
+ static inline void verify_osdc_locked(struct ceph_osd_client *osdc) { }
+ static inline void verify_osdc_wrlocked(struct ceph_osd_client *osdc) { }
+ static inline void verify_osd_locked(struct ceph_osd *osd) { }
+ static inline void verify_lreq_locked(struct ceph_osd_linger_request *lreq) { }
+ #endif
+ 
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  /*
   * calculate the mapping of a file extent onto an object, and fill out the
   * request accordingly.  shorten extent as necessary if it crosses an
@@@ -299,9 -339,63 +351,68 @@@ static void osd_req_op_data_release(str
  	}
  }
  
+ /*
++<<<<<<< HEAD
++ * requests
++ */
++=======
+  * Assumes @t is zero-initialized.
+  */
+ static void target_init(struct ceph_osd_request_target *t)
+ {
+ 	ceph_oid_init(&t->base_oid);
+ 	ceph_oloc_init(&t->base_oloc);
+ 	ceph_oid_init(&t->target_oid);
+ 	ceph_oloc_init(&t->target_oloc);
+ 
+ 	ceph_osds_init(&t->acting);
+ 	ceph_osds_init(&t->up);
+ 	t->size = -1;
+ 	t->min_size = -1;
+ 
+ 	t->osd = CEPH_HOMELESS_OSD;
+ }
+ 
+ static void target_copy(struct ceph_osd_request_target *dest,
+ 			const struct ceph_osd_request_target *src)
+ {
+ 	ceph_oid_copy(&dest->base_oid, &src->base_oid);
+ 	ceph_oloc_copy(&dest->base_oloc, &src->base_oloc);
+ 	ceph_oid_copy(&dest->target_oid, &src->target_oid);
+ 	ceph_oloc_copy(&dest->target_oloc, &src->target_oloc);
+ 
+ 	dest->pgid = src->pgid; /* struct */
+ 	dest->pg_num = src->pg_num;
+ 	dest->pg_num_mask = src->pg_num_mask;
+ 	ceph_osds_copy(&dest->acting, &src->acting);
+ 	ceph_osds_copy(&dest->up, &src->up);
+ 	dest->size = src->size;
+ 	dest->min_size = src->min_size;
+ 	dest->sort_bitwise = src->sort_bitwise;
+ 
+ 	dest->flags = src->flags;
+ 	dest->paused = src->paused;
+ 
+ 	dest->osd = src->osd;
+ }
+ 
+ static void target_destroy(struct ceph_osd_request_target *t)
+ {
+ 	ceph_oid_destroy(&t->base_oid);
+ 	ceph_oid_destroy(&t->target_oid);
+ }
+ 
  /*
   * requests
   */
+ static void request_release_checks(struct ceph_osd_request *req)
+ {
+ 	WARN_ON(!RB_EMPTY_NODE(&req->r_node));
+ 	WARN_ON(!list_empty(&req->r_unsafe_item));
+ 	WARN_ON(req->r_osd);
+ }
+ 
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  static void ceph_osdc_release_request(struct kref *kref)
  {
  	struct ceph_osd_request *req = container_of(kref,
@@@ -352,6 -443,57 +463,60 @@@ void ceph_osdc_put_request(struct ceph_
  }
  EXPORT_SYMBOL(ceph_osdc_put_request);
  
++<<<<<<< HEAD
++=======
+ static void request_init(struct ceph_osd_request *req)
+ {
+ 	/* req only, each op is zeroed in _osd_req_op_init() */
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	kref_init(&req->r_kref);
+ 	init_completion(&req->r_completion);
+ 	init_completion(&req->r_safe_completion);
+ 	RB_CLEAR_NODE(&req->r_node);
+ 	INIT_LIST_HEAD(&req->r_unsafe_item);
+ 
+ 	target_init(&req->r_t);
+ }
+ 
+ /*
+  * This is ugly, but it allows us to reuse linger registration and ping
+  * requests, keeping the structure of the code around send_linger{_ping}()
+  * reasonable.  Setting up a min_nr=2 mempool for each linger request
+  * and dealing with copying ops (this blasts req only, watch op remains
+  * intact) isn't any better.
+  */
+ static void request_reinit(struct ceph_osd_request *req)
+ {
+ 	struct ceph_osd_client *osdc = req->r_osdc;
+ 	bool mempool = req->r_mempool;
+ 	unsigned int num_ops = req->r_num_ops;
+ 	u64 snapid = req->r_snapid;
+ 	struct ceph_snap_context *snapc = req->r_snapc;
+ 	bool linger = req->r_linger;
+ 	struct ceph_msg *request_msg = req->r_request;
+ 	struct ceph_msg *reply_msg = req->r_reply;
+ 
+ 	dout("%s req %p\n", __func__, req);
+ 	WARN_ON(atomic_read(&req->r_kref.refcount) != 1);
+ 	request_release_checks(req);
+ 
+ 	WARN_ON(atomic_read(&request_msg->kref.refcount) != 1);
+ 	WARN_ON(atomic_read(&reply_msg->kref.refcount) != 1);
+ 	target_destroy(&req->r_t);
+ 
+ 	request_init(req);
+ 	req->r_osdc = osdc;
+ 	req->r_mempool = mempool;
+ 	req->r_num_ops = num_ops;
+ 	req->r_snapid = snapid;
+ 	req->r_snapc = snapc;
+ 	req->r_linger = linger;
+ 	req->r_request = request_msg;
+ 	req->r_reply = reply_msg;
+ }
+ 
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  struct ceph_osd_request *ceph_osdc_alloc_request(struct ceph_osd_client *osdc,
  					       struct ceph_snap_context *snapc,
  					       unsigned int num_ops,
@@@ -1012,8 -982,8 +1177,13 @@@ static void osd_init(struct ceph_osd *o
  {
  	atomic_set(&osd->o_ref, 1);
  	RB_CLEAR_NODE(&osd->o_node);
++<<<<<<< HEAD
 +	INIT_LIST_HEAD(&osd->o_requests);
 +	INIT_LIST_HEAD(&osd->o_linger_requests);
++=======
+ 	osd->o_requests = RB_ROOT;
+ 	osd->o_linger_requests = RB_ROOT;
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	INIT_LIST_HEAD(&osd->o_osd_lru);
  	INIT_LIST_HEAD(&osd->o_keepalive_item);
  	osd->o_incarnation = 1;
@@@ -1022,8 -993,8 +1192,13 @@@
  static void osd_cleanup(struct ceph_osd *osd)
  {
  	WARN_ON(!RB_EMPTY_NODE(&osd->o_node));
++<<<<<<< HEAD
 +	WARN_ON(!list_empty(&osd->o_requests));
 +	WARN_ON(!list_empty(&osd->o_linger_requests));
++=======
+ 	WARN_ON(!RB_EMPTY_ROOT(&osd->o_requests));
+ 	WARN_ON(!RB_EMPTY_ROOT(&osd->o_linger_requests));
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	WARN_ON(!list_empty(&osd->o_osd_lru));
  	WARN_ON(!list_empty(&osd->o_keepalive_item));
  
@@@ -1076,81 -1047,78 +1251,105 @@@ static void put_osd(struct ceph_osd *os
  
  DEFINE_RB_FUNCS(osd, struct ceph_osd, o_osd, o_node)
  
 -static void __move_osd_to_lru(struct ceph_osd *osd)
 +/*
 + * remove an osd from our map
 + */
 +static void __remove_osd(struct ceph_osd_client *osdc, struct ceph_osd *osd)
  {
 -	struct ceph_osd_client *osdc = osd->o_osdc;
 +	dout("%s %p osd%d\n", __func__, osd, osd->o_osd);
 +	WARN_ON(!list_empty(&osd->o_requests));
 +	WARN_ON(!list_empty(&osd->o_linger_requests));
  
 -	dout("%s osd %p osd%d\n", __func__, osd, osd->o_osd);
 +	list_del_init(&osd->o_osd_lru);
 +	erase_osd(&osdc->osds, osd);
 +}
 +
 +static void remove_osd(struct ceph_osd_client *osdc, struct ceph_osd *osd)
 +{
++<<<<<<< HEAD
 +	dout("%s %p osd%d\n", __func__, osd, osd->o_osd);
 +
 +	if (!RB_EMPTY_NODE(&osd->o_node)) {
 +		ceph_con_close(&osd->o_con);
 +		__remove_osd(osdc, osd);
 +		put_osd(osd);
 +	}
 +}
 +
 +static void remove_all_osds(struct ceph_osd_client *osdc)
 +{
 +	dout("%s %p\n", __func__, osdc);
 +	mutex_lock(&osdc->request_mutex);
 +	while (!RB_EMPTY_ROOT(&osdc->osds)) {
 +		struct ceph_osd *osd = rb_entry(rb_first(&osdc->osds),
 +						struct ceph_osd, o_node);
 +		remove_osd(osdc, osd);
 +	}
 +	mutex_unlock(&osdc->request_mutex);
 +}
 +
 +static void __move_osd_to_lru(struct ceph_osd_client *osdc,
 +			      struct ceph_osd *osd)
 +{
 +	dout("%s %p\n", __func__, osd);
  	BUG_ON(!list_empty(&osd->o_osd_lru));
  
 -	spin_lock(&osdc->osd_lru_lock);
  	list_add_tail(&osd->o_osd_lru, &osdc->osd_lru);
 -	spin_unlock(&osdc->osd_lru_lock);
 -
 -	osd->lru_ttl = jiffies + osdc->client->options->osd_idle_ttl;
 +	osd->lru_ttl = jiffies + osdc->client->options->osd_idle_ttl * HZ;
  }
  
 -static void maybe_move_osd_to_lru(struct ceph_osd *osd)
 +static void maybe_move_osd_to_lru(struct ceph_osd_client *osdc,
 +				  struct ceph_osd *osd)
  {
 +	dout("%s %p\n", __func__, osd);
 +
 +	if (list_empty(&osd->o_requests) &&
 +	    list_empty(&osd->o_linger_requests))
 +		__move_osd_to_lru(osdc, osd);
++=======
+ 	if (RB_EMPTY_ROOT(&osd->o_requests) &&
+ 	    RB_EMPTY_ROOT(&osd->o_linger_requests))
+ 		__move_osd_to_lru(osd);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  }
  
  static void __remove_osd_from_lru(struct ceph_osd *osd)
  {
 -	struct ceph_osd_client *osdc = osd->o_osdc;
 -
 -	dout("%s osd %p osd%d\n", __func__, osd, osd->o_osd);
 -
 -	spin_lock(&osdc->osd_lru_lock);
 +	dout("__remove_osd_from_lru %p\n", osd);
  	if (!list_empty(&osd->o_osd_lru))
  		list_del_init(&osd->o_osd_lru);
 -	spin_unlock(&osdc->osd_lru_lock);
  }
  
 -/*
 - * Close the connection and assign any leftover requests to the
 - * homeless session.
 - */
 -static void close_osd(struct ceph_osd *osd)
 +static void remove_old_osds(struct ceph_osd_client *osdc)
  {
 -	struct ceph_osd_client *osdc = osd->o_osdc;
 -	struct rb_node *n;
 -
 -	verify_osdc_wrlocked(osdc);
 -	dout("%s osd %p osd%d\n", __func__, osd, osd->o_osd);
 -
 -	ceph_con_close(&osd->o_con);
 -
 -	for (n = rb_first(&osd->o_requests); n; ) {
 -		struct ceph_osd_request *req =
 -		    rb_entry(n, struct ceph_osd_request, r_node);
 -
 -		n = rb_next(n); /* unlink_request() */
 +	struct ceph_osd *osd, *nosd;
  
 -		dout(" reassigning req %p tid %llu\n", req, req->r_tid);
 -		unlink_request(osd, req);
 -		link_request(&osdc->homeless_osd, req);
 +	dout("__remove_old_osds %p\n", osdc);
 +	mutex_lock(&osdc->request_mutex);
 +	list_for_each_entry_safe(osd, nosd, &osdc->osd_lru, o_osd_lru) {
 +		if (time_before(jiffies, osd->lru_ttl))
 +			break;
 +		remove_osd(osdc, osd);
  	}
++<<<<<<< HEAD
 +	mutex_unlock(&osdc->request_mutex);
++=======
+ 	for (n = rb_first(&osd->o_linger_requests); n; ) {
+ 		struct ceph_osd_linger_request *lreq =
+ 		    rb_entry(n, struct ceph_osd_linger_request, node);
+ 
+ 		n = rb_next(n); /* unlink_linger() */
+ 
+ 		dout(" reassigning lreq %p linger_id %llu\n", lreq,
+ 		     lreq->linger_id);
+ 		unlink_linger(osd, lreq);
+ 		link_linger(&osdc->homeless_osd, lreq);
+ 	}
+ 
+ 	__remove_osd_from_lru(osd);
+ 	erase_osd(&osdc->osds, osd);
+ 	put_osd(osd);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  }
  
  /*
@@@ -1160,10 -1128,11 +1359,18 @@@ static int __reset_osd(struct ceph_osd_
  {
  	struct ceph_entity_addr *peer_addr;
  
++<<<<<<< HEAD
 +	dout("__reset_osd %p osd%d\n", osd, osd->o_osd);
 +	if (list_empty(&osd->o_requests) &&
 +	    list_empty(&osd->o_linger_requests)) {
 +		remove_osd(osdc, osd);
++=======
+ 	dout("%s osd %p osd%d\n", __func__, osd, osd->o_osd);
+ 
+ 	if (RB_EMPTY_ROOT(&osd->o_requests) &&
+ 	    RB_EMPTY_ROOT(&osd->o_linger_requests)) {
+ 		close_osd(osd);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  		return -ENODEV;
  	}
  
@@@ -1188,127 -1160,103 +1395,160 @@@
  	return 0;
  }
  
 -static struct ceph_osd *lookup_create_osd(struct ceph_osd_client *osdc, int o,
 -					  bool wrlocked)
 +static void __schedule_osd_timeout(struct ceph_osd_client *osdc)
  {
 -	struct ceph_osd *osd;
 +	schedule_delayed_work(&osdc->timeout_work,
 +			osdc->client->options->osd_keepalive_timeout * HZ);
 +}
  
 -	if (wrlocked)
 -		verify_osdc_wrlocked(osdc);
 -	else
 -		verify_osdc_locked(osdc);
 +static void __cancel_osd_timeout(struct ceph_osd_client *osdc)
 +{
 +	cancel_delayed_work(&osdc->timeout_work);
 +}
  
 -	if (o != CEPH_HOMELESS_OSD)
 -		osd = lookup_osd(&osdc->osds, o);
 -	else
 -		osd = &osdc->homeless_osd;
 -	if (!osd) {
 -		if (!wrlocked)
 -			return ERR_PTR(-EAGAIN);
 +/*
 + * Register request, assign tid.  If this is the first request, set up
 + * the timeout event.
 + */
 +static void __register_request(struct ceph_osd_client *osdc,
 +			       struct ceph_osd_request *req)
 +{
 +	req->r_tid = ++osdc->last_tid;
 +	req->r_request->hdr.tid = cpu_to_le64(req->r_tid);
 +	dout("__register_request %p tid %lld\n", req, req->r_tid);
 +	insert_request(&osdc->requests, req);
 +	ceph_osdc_get_request(req);
 +	osdc->num_requests++;
 +	if (osdc->num_requests == 1) {
 +		dout(" first request, scheduling timeout\n");
 +		__schedule_osd_timeout(osdc);
 +	}
 +}
 +
 +/*
 + * called under osdc->request_mutex
 + */
 +static void __unregister_request(struct ceph_osd_client *osdc,
 +				 struct ceph_osd_request *req)
 +{
 +	if (RB_EMPTY_NODE(&req->r_node)) {
 +		dout("__unregister_request %p tid %lld not registered\n",
 +			req, req->r_tid);
 +		return;
 +	}
 +
 +	dout("__unregister_request %p tid %lld\n", req, req->r_tid);
 +	erase_request(&osdc->requests, req);
 +	osdc->num_requests--;
 +
 +	if (req->r_osd) {
 +		/* make sure the original request isn't in flight. */
 +		ceph_msg_revoke(req->r_request);
  
 -		osd = create_osd(osdc, o);
 -		insert_osd(&osdc->osds, osd);
 -		ceph_con_open(&osd->o_con, CEPH_ENTITY_TYPE_OSD, osd->o_osd,
 -			      &osdc->osdmap->osd_addr[osd->o_osd]);
 +		list_del_init(&req->r_osd_item);
 +		maybe_move_osd_to_lru(osdc, req->r_osd);
 +		if (list_empty(&req->r_linger_osd_item))
 +			req->r_osd = NULL;
  	}
  
 -	dout("%s osdc %p osd%d -> osd %p\n", __func__, osdc, o, osd);
 -	return osd;
 +	list_del_init(&req->r_req_lru_item);
 +	ceph_osdc_put_request(req);
 +
 +	if (osdc->num_requests == 0) {
 +		dout(" no requests, canceling timeout\n");
 +		__cancel_osd_timeout(osdc);
 +	}
  }
  
++<<<<<<< HEAD
  /*
 - * Create request <-> OSD session relation.
 - *
 - * @req has to be assigned a tid, @osd may be homeless.
 + * Cancel a previously queued request message
   */
 -static void link_request(struct ceph_osd *osd, struct ceph_osd_request *req)
 +static void __cancel_request(struct ceph_osd_request *req)
  {
 -	verify_osd_locked(osd);
 -	WARN_ON(!req->r_tid || req->r_osd);
 -	dout("%s osd %p osd%d req %p tid %llu\n", __func__, osd, osd->o_osd,
 -	     req, req->r_tid);
 +	if (req->r_sent && req->r_osd) {
 +		ceph_msg_revoke(req->r_request);
 +		req->r_sent = 0;
 +	}
 +}
  
 -	if (!osd_homeless(osd))
 -		__remove_osd_from_lru(osd);
 -	else
 -		atomic_inc(&osd->o_osdc->num_homeless);
 +static void __register_linger_request(struct ceph_osd_client *osdc,
 +				    struct ceph_osd_request *req)
 +{
 +	dout("%s %p tid %llu\n", __func__, req, req->r_tid);
 +	WARN_ON(!req->r_linger);
  
 -	get_osd(osd);
 -	insert_request(&osd->o_requests, req);
 -	req->r_osd = osd;
 +	ceph_osdc_get_request(req);
 +	list_add_tail(&req->r_linger_item, &osdc->req_linger);
 +	if (req->r_osd)
 +		list_add_tail(&req->r_linger_osd_item,
 +			      &req->r_osd->o_linger_requests);
  }
  
 -static void unlink_request(struct ceph_osd *osd, struct ceph_osd_request *req)
 +static void __unregister_linger_request(struct ceph_osd_client *osdc,
 +					struct ceph_osd_request *req)
  {
 -	verify_osd_locked(osd);
 -	WARN_ON(req->r_osd != osd);
 -	dout("%s osd %p osd%d req %p tid %llu\n", __func__, osd, osd->o_osd,
 -	     req, req->r_tid);
 +	WARN_ON(!req->r_linger);
  
 -	req->r_osd = NULL;
 -	erase_request(&osd->o_requests, req);
 -	put_osd(osd);
 +	if (list_empty(&req->r_linger_item)) {
 +		dout("%s %p tid %llu not registered\n", __func__, req,
 +		     req->r_tid);
 +		return;
 +	}
  
 -	if (!osd_homeless(osd))
 -		maybe_move_osd_to_lru(osd);
 -	else
 -		atomic_dec(&osd->o_osdc->num_homeless);
 +	dout("%s %p tid %llu\n", __func__, req, req->r_tid);
 +	list_del_init(&req->r_linger_item);
 +
 +	if (req->r_osd) {
 +		list_del_init(&req->r_linger_osd_item);
 +		maybe_move_osd_to_lru(osdc, req->r_osd);
 +		if (list_empty(&req->r_osd_item))
 +			req->r_osd = NULL;
 +	}
 +	ceph_osdc_put_request(req);
  }
  
 +void ceph_osdc_set_request_linger(struct ceph_osd_client *osdc,
 +				  struct ceph_osd_request *req)
 +{
 +	if (!req->r_linger) {
 +		dout("set_request_linger %p\n", req);
 +		req->r_linger = 1;
 +	}
 +}
 +EXPORT_SYMBOL(ceph_osdc_set_request_linger);
++=======
+ static bool __pool_full(struct ceph_pg_pool_info *pi)
+ {
+ 	return pi->flags & CEPH_POOL_FLAG_FULL;
+ }
+ 
+ static bool have_pool_full(struct ceph_osd_client *osdc)
+ {
+ 	struct rb_node *n;
+ 
+ 	for (n = rb_first(&osdc->osdmap->pg_pools); n; n = rb_next(n)) {
+ 		struct ceph_pg_pool_info *pi =
+ 		    rb_entry(n, struct ceph_pg_pool_info, node);
+ 
+ 		if (__pool_full(pi))
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static bool pool_full(struct ceph_osd_client *osdc, s64 pool_id)
+ {
+ 	struct ceph_pg_pool_info *pi;
+ 
+ 	pi = ceph_pg_pool_by_id(osdc->osdmap, pool_id);
+ 	if (!pi)
+ 		return false;
+ 
+ 	return __pool_full(pi);
+ }
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  
  /*
   * Returns whether a request should be blocked from being sent
@@@ -1346,213 -1324,861 +1586,752 @@@ static int __calc_request_pg(struct cep
  	}
  
  	if (need_check_tiering &&
 -	    (t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) {
 -		if (t->flags & CEPH_OSD_FLAG_READ && pi->read_tier >= 0)
 -			t->target_oloc.pool = pi->read_tier;
 -		if (t->flags & CEPH_OSD_FLAG_WRITE && pi->write_tier >= 0)
 -			t->target_oloc.pool = pi->write_tier;
 +	    (req->r_flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) {
 +		struct ceph_pg_pool_info *pi;
 +
 +		pi = ceph_pg_pool_by_id(osdmap, req->r_target_oloc.pool);
 +		if (pi) {
 +			if ((req->r_flags & CEPH_OSD_FLAG_READ) &&
 +			    pi->read_tier >= 0)
 +				req->r_target_oloc.pool = pi->read_tier;
 +			if ((req->r_flags & CEPH_OSD_FLAG_WRITE) &&
 +			    pi->write_tier >= 0)
 +				req->r_target_oloc.pool = pi->write_tier;
 +		}
 +		/* !pi is caught in ceph_oloc_oid_to_pg() */
  	}
  
 -	ret = ceph_object_locator_to_pg(osdc->osdmap, &t->target_oid,
 -					&t->target_oloc, &pgid);
 -	if (ret) {
 -		WARN_ON(ret != -ENOENT);
 -		t->osd = CEPH_HOMELESS_OSD;
 -		ct_res = CALC_TARGET_POOL_DNE;
 -		goto out;
 -	}
 -	last_pgid.pool = pgid.pool;
 -	last_pgid.seed = ceph_stable_mod(pgid.seed, t->pg_num, t->pg_num_mask);
 +	return ceph_oloc_oid_to_pg(osdmap, &req->r_target_oloc,
 +				   &req->r_target_oid, pg_out);
 +}
  
 -	ceph_pg_to_up_acting_osds(osdc->osdmap, &pgid, &up, &acting);
 -	if (any_change &&
 -	    ceph_is_new_interval(&t->acting,
 -				 &acting,
 -				 &t->up,
 -				 &up,
 -				 t->size,
 -				 pi->size,
 -				 t->min_size,
 -				 pi->min_size,
 -				 t->pg_num,
 -				 pi->pg_num,
 -				 t->sort_bitwise,
 -				 sort_bitwise,
 -				 &last_pgid))
 -		force_resend = true;
 -
 -	if (t->paused && !target_should_be_paused(osdc, t, pi)) {
 -		t->paused = false;
 -		need_resend = true;
 -	}
 +static void __enqueue_request(struct ceph_osd_request *req)
 +{
 +	struct ceph_osd_client *osdc = req->r_osdc;
  
 +	dout("%s %p tid %llu to osd%d\n", __func__, req, req->r_tid,
 +	     req->r_osd ? req->r_osd->o_osd : -1);
 +
 +	if (req->r_osd) {
 +		__remove_osd_from_lru(req->r_osd);
 +		list_add_tail(&req->r_osd_item, &req->r_osd->o_requests);
 +		list_move_tail(&req->r_req_lru_item, &osdc->req_unsent);
 +	} else {
 +		list_move_tail(&req->r_req_lru_item, &osdc->req_notarget);
 +	}
 +}
 +
 +/*
 + * Pick an osd (the first 'up' osd in the pg), allocate the osd struct
 + * (as needed), and set the request r_osd appropriately.  If there is
 + * no up osd, set r_osd to NULL.  Move the request to the appropriate list
 + * (unsent, homeless) or leave on in-flight lru.
 + *
 + * Return 0 if unchanged, 1 if changed, or negative on error.
 + *
 + * Caller should hold map_sem for read and request_mutex.
 + */
 +static int __map_request(struct ceph_osd_client *osdc,
 +			 struct ceph_osd_request *req, int force_resend)
 +{
 +	struct ceph_pg pgid;
 +	struct ceph_osds up, acting;
 +	int err;
 +	bool was_paused;
 +
 +	dout("map_request %p tid %lld\n", req, req->r_tid);
 +
 +	err = __calc_request_pg(osdc->osdmap, req, &pgid);
 +	if (err) {
 +		list_move(&req->r_req_lru_item, &osdc->req_notarget);
 +		return err;
 +	}
 +	req->r_pgid = pgid;
 +
 +	ceph_pg_to_up_acting_osds(osdc->osdmap, &pgid, &up, &acting);
 +
 +	was_paused = req->r_paused;
 +	req->r_paused = __req_should_be_paused(osdc, req);
 +	if (was_paused && !req->r_paused)
 +		force_resend = 1;
 +
 +	if ((!force_resend &&
 +	     req->r_osd && req->r_osd->o_osd == acting.primary &&
 +	     req->r_sent >= req->r_osd->o_incarnation &&
 +	     req->r_num_pg_osds == acting.size &&
 +	     memcmp(req->r_pg_osds, acting.osds,
 +		    acting.size * sizeof(acting.osds[0])) == 0) ||
 +	    (req->r_osd == NULL && acting.primary == -1) ||
 +	    req->r_paused)
 +		return 0;  /* no change */
 +
 +	dout("map_request tid %llu pgid %lld.%x osd%d (was osd%d)\n",
 +	     req->r_tid, pgid.pool, pgid.seed, acting.primary,
 +	     req->r_osd ? req->r_osd->o_osd : -1);
 +
 +	/* record full pg acting set */
 +	memcpy(req->r_pg_osds, acting.osds,
 +	       acting.size * sizeof(acting.osds[0]));
 +	req->r_num_pg_osds = acting.size;
 +
 +	if (req->r_osd) {
 +		__cancel_request(req);
 +		list_del_init(&req->r_osd_item);
 +		list_del_init(&req->r_linger_osd_item);
 +		req->r_osd = NULL;
 +	}
 +
++<<<<<<< HEAD
 +	req->r_osd = lookup_osd(&osdc->osds, acting.primary);
 +	if (!req->r_osd && acting.primary >= 0) {
 +		err = -ENOMEM;
 +		req->r_osd = create_osd(osdc, acting.primary);
 +		if (!req->r_osd) {
 +			list_move(&req->r_req_lru_item, &osdc->req_notarget);
 +			goto out;
++=======
+ 	if (ceph_pg_compare(&t->pgid, &pgid) ||
+ 	    ceph_osds_changed(&t->acting, &acting, any_change) ||
+ 	    force_resend) {
+ 		t->pgid = pgid; /* struct */
+ 		ceph_osds_copy(&t->acting, &acting);
+ 		ceph_osds_copy(&t->up, &up);
+ 		t->size = pi->size;
+ 		t->min_size = pi->min_size;
+ 		t->pg_num = pi->pg_num;
+ 		t->pg_num_mask = pi->pg_num_mask;
+ 		t->sort_bitwise = sort_bitwise;
+ 
+ 		t->osd = acting.primary;
+ 		need_resend = true;
+ 	}
+ 
+ 	ct_res = need_resend ? CALC_TARGET_NEED_RESEND : CALC_TARGET_NO_ACTION;
+ out:
+ 	dout("%s t %p -> ct_res %d osd %d\n", __func__, t, ct_res, t->osd);
+ 	return ct_res;
+ }
+ 
+ static void setup_request_data(struct ceph_osd_request *req,
+ 			       struct ceph_msg *msg)
+ {
+ 	u32 data_len = 0;
+ 	int i;
+ 
+ 	if (!list_empty(&msg->data))
+ 		return;
+ 
+ 	WARN_ON(msg->data_length);
+ 	for (i = 0; i < req->r_num_ops; i++) {
+ 		struct ceph_osd_req_op *op = &req->r_ops[i];
+ 
+ 		switch (op->op) {
+ 		/* request */
+ 		case CEPH_OSD_OP_WRITE:
+ 		case CEPH_OSD_OP_WRITEFULL:
+ 			WARN_ON(op->indata_len != op->extent.length);
+ 			ceph_osdc_msg_data_add(msg, &op->extent.osd_data);
+ 			break;
+ 		case CEPH_OSD_OP_SETXATTR:
+ 		case CEPH_OSD_OP_CMPXATTR:
+ 			WARN_ON(op->indata_len != op->xattr.name_len +
+ 						  op->xattr.value_len);
+ 			ceph_osdc_msg_data_add(msg, &op->xattr.osd_data);
+ 			break;
+ 		case CEPH_OSD_OP_NOTIFY_ACK:
+ 			ceph_osdc_msg_data_add(msg,
+ 					       &op->notify_ack.request_data);
+ 			break;
+ 
+ 		/* reply */
+ 		case CEPH_OSD_OP_STAT:
+ 			ceph_osdc_msg_data_add(req->r_reply,
+ 					       &op->raw_data_in);
+ 			break;
+ 		case CEPH_OSD_OP_READ:
+ 			ceph_osdc_msg_data_add(req->r_reply,
+ 					       &op->extent.osd_data);
+ 			break;
+ 
+ 		/* both */
+ 		case CEPH_OSD_OP_CALL:
+ 			WARN_ON(op->indata_len != op->cls.class_len +
+ 						  op->cls.method_len +
+ 						  op->cls.indata_len);
+ 			ceph_osdc_msg_data_add(msg, &op->cls.request_info);
+ 			/* optional, can be NONE */
+ 			ceph_osdc_msg_data_add(msg, &op->cls.request_data);
+ 			/* optional, can be NONE */
+ 			ceph_osdc_msg_data_add(req->r_reply,
+ 					       &op->cls.response_data);
+ 			break;
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  		}
  
 -		data_len += op->indata_len;
 -	}
 -
 -	WARN_ON(data_len != msg->data_length);
 -}
 -
 -static void encode_request(struct ceph_osd_request *req, struct ceph_msg *msg)
 -{
 -	void *p = msg->front.iov_base;
 -	void *const end = p + msg->front_alloc_len;
 -	u32 data_len = 0;
 -	int i;
 -
 -	if (req->r_flags & CEPH_OSD_FLAG_WRITE) {
 -		/* snapshots aren't writeable */
 -		WARN_ON(req->r_snapid != CEPH_NOSNAP);
 -	} else {
 -		WARN_ON(req->r_mtime.tv_sec || req->r_mtime.tv_nsec ||
 -			req->r_data_offset || req->r_snapc);
 -	}
 -
 -	setup_request_data(req, msg);
 -
 -	ceph_encode_32(&p, 1); /* client_inc, always 1 */
 -	ceph_encode_32(&p, req->r_osdc->osdmap->epoch);
 -	ceph_encode_32(&p, req->r_flags);
 -	ceph_encode_timespec(p, &req->r_mtime);
 -	p += sizeof(struct ceph_timespec);
 -	/* aka reassert_version */
 -	memcpy(p, &req->r_replay_version, sizeof(req->r_replay_version));
 -	p += sizeof(req->r_replay_version);
 -
 -	/* oloc */
 -	ceph_encode_8(&p, 4);
 -	ceph_encode_8(&p, 4);
 -	ceph_encode_32(&p, 8 + 4 + 4);
 -	ceph_encode_64(&p, req->r_t.target_oloc.pool);
 -	ceph_encode_32(&p, -1); /* preferred */
 -	ceph_encode_32(&p, 0); /* key len */
 -
 -	/* pgid */
 -	ceph_encode_8(&p, 1);
 -	ceph_encode_64(&p, req->r_t.pgid.pool);
 -	ceph_encode_32(&p, req->r_t.pgid.seed);
 -	ceph_encode_32(&p, -1); /* preferred */
 +		dout("map_request osd %p is osd%d\n", req->r_osd,
 +		     acting.primary);
 +		insert_osd(&osdc->osds, req->r_osd);
  
 -	/* oid */
 -	ceph_encode_32(&p, req->r_t.target_oid.name_len);
 -	memcpy(p, req->r_t.target_oid.name, req->r_t.target_oid.name_len);
 -	p += req->r_t.target_oid.name_len;
 -
 -	/* ops, can imply data */
 -	ceph_encode_16(&p, req->r_num_ops);
 -	for (i = 0; i < req->r_num_ops; i++) {
 -		data_len += osd_req_encode_op(p, &req->r_ops[i]);
 -		p += sizeof(struct ceph_osd_op);
 +		ceph_con_open(&req->r_osd->o_con,
 +			      CEPH_ENTITY_TYPE_OSD, acting.primary,
 +			      &osdc->osdmap->osd_addr[acting.primary]);
  	}
  
 -	ceph_encode_64(&p, req->r_snapid); /* snapid */
 -	if (req->r_snapc) {
 -		ceph_encode_64(&p, req->r_snapc->seq);
 -		ceph_encode_32(&p, req->r_snapc->num_snaps);
 -		for (i = 0; i < req->r_snapc->num_snaps; i++)
 -			ceph_encode_64(&p, req->r_snapc->snaps[i]);
 -	} else {
 -		ceph_encode_64(&p, 0); /* snap_seq */
 -		ceph_encode_32(&p, 0); /* snaps len */
 -	}
 -
 -	ceph_encode_32(&p, req->r_attempts); /* retry_attempt */
 +	__enqueue_request(req);
 +	err = 1;   /* osd or pg changed */
  
 -	BUG_ON(p > end);
 -	msg->front.iov_len = p - msg->front.iov_base;
 -	msg->hdr.version = cpu_to_le16(4); /* MOSDOp v4 */
 -	msg->hdr.front_len = cpu_to_le32(msg->front.iov_len);
 -	msg->hdr.data_len = cpu_to_le32(data_len);
 -	/*
 -	 * The header "data_off" is a hint to the receiver allowing it
 -	 * to align received data into its buffers such that there's no
 -	 * need to re-copy it before writing it to disk (direct I/O).
 -	 */
 -	msg->hdr.data_off = cpu_to_le16(req->r_data_offset);
 -
 -	dout("%s req %p oid %*pE oid_len %d front %zu data %u\n", __func__,
 -	     req, req->r_t.target_oid.name_len, req->r_t.target_oid.name,
 -	     req->r_t.target_oid.name_len, msg->front.iov_len, data_len);
 +out:
 +	return err;
  }
  
  /*
 - * @req has to be assigned a tid and registered.
 + * caller should hold map_sem (for read) and request_mutex
   */
 -static void send_request(struct ceph_osd_request *req)
 +static void __send_request(struct ceph_osd_client *osdc,
 +			   struct ceph_osd_request *req)
  {
 -	struct ceph_osd *osd = req->r_osd;
 +	void *p;
  
 -	verify_osd_locked(osd);
 -	WARN_ON(osd->o_osd != req->r_t.osd);
 +	dout("send_request %p tid %llu to osd%d flags %d pg %lld.%x\n",
 +	     req, req->r_tid, req->r_osd->o_osd, req->r_flags,
 +	     (unsigned long long)req->r_pgid.pool, req->r_pgid.seed);
  
 -	/*
 -	 * We may have a previously queued request message hanging
 -	 * around.  Cancel it to avoid corrupting the msgr.
 -	 */
 -	if (req->r_sent)
 -		ceph_msg_revoke(req->r_request);
 +	/* fill in message content that changes each time we send it */
 +	put_unaligned_le32(osdc->osdmap->epoch, req->r_request_osdmap_epoch);
 +	put_unaligned_le32(req->r_flags, req->r_request_flags);
 +	put_unaligned_le64(req->r_target_oloc.pool, req->r_request_pool);
 +	p = req->r_request_pgid;
 +	ceph_encode_64(&p, req->r_pgid.pool);
 +	ceph_encode_32(&p, req->r_pgid.seed);
 +	put_unaligned_le64(1, req->r_request_attempts);  /* FIXME */
 +	memcpy(req->r_request_reassert_version, &req->r_reassert_version,
 +	       sizeof(req->r_reassert_version));
  
 -	req->r_flags |= CEPH_OSD_FLAG_KNOWN_REDIR;
 -	if (req->r_attempts)
 -		req->r_flags |= CEPH_OSD_FLAG_RETRY;
 -	else
 -		WARN_ON(req->r_flags & CEPH_OSD_FLAG_RETRY);
 -
 -	encode_request(req, req->r_request);
 +	req->r_stamp = jiffies;
 +	list_move_tail(&req->r_req_lru_item, &osdc->req_lru);
  
 -	dout("%s req %p tid %llu to pg %llu.%x osd%d flags 0x%x attempt %d\n",
 -	     __func__, req, req->r_tid, req->r_t.pgid.pool, req->r_t.pgid.seed,
 -	     req->r_t.osd, req->r_flags, req->r_attempts);
 +	ceph_msg_get(req->r_request); /* send consumes a ref */
  
 -	req->r_t.paused = false;
 -	req->r_stamp = jiffies;
 -	req->r_attempts++;
 +	req->r_sent = req->r_osd->o_incarnation;
  
 -	req->r_sent = osd->o_incarnation;
 -	req->r_request->hdr.tid = cpu_to_le64(req->r_tid);
 -	ceph_con_send(&osd->o_con, ceph_msg_get(req->r_request));
 +	ceph_con_send(&req->r_osd->o_con, req->r_request);
  }
  
 -static void maybe_request_map(struct ceph_osd_client *osdc)
 +/*
++<<<<<<< HEAD
 + * Send any requests in the queue (req_unsent).
 + */
 +static void __send_queued(struct ceph_osd_client *osdc)
  {
 -	bool continuous = false;
 +	struct ceph_osd_request *req, *tmp;
  
 -	verify_osdc_locked(osdc);
 -	WARN_ON(!osdc->osdmap->epoch);
 +	dout("__send_queued\n");
 +	list_for_each_entry_safe(req, tmp, &osdc->req_unsent, r_req_lru_item)
 +		__send_request(osdc, req);
 +}
  
 -	if (ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_FULL) ||
 -	    ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSERD) ||
 -	    ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSEWR)) {
 -		dout("%s osdc %p continuous\n", __func__, osdc);
 -		continuous = true;
 -	} else {
 -		dout("%s osdc %p onetime\n", __func__, osdc);
 +/*
 + * Caller should hold map_sem for read and request_mutex.
 + */
 +static int __ceph_osdc_start_request(struct ceph_osd_client *osdc,
 +				     struct ceph_osd_request *req,
 +				     bool nofail)
 +{
 +	int rc;
 +
 +	__register_request(osdc, req);
 +	req->r_sent = 0;
 +	req->r_got_reply = 0;
 +	rc = __map_request(osdc, req, 0);
 +	if (rc < 0) {
 +		if (nofail) {
 +			dout("osdc_start_request failed map, "
 +				" will retry %lld\n", req->r_tid);
 +			rc = 0;
 +		} else {
 +			__unregister_request(osdc, req);
 +		}
 +		return rc;
  	}
  
 -	if (ceph_monc_want_map(&osdc->client->monc, CEPH_SUB_OSDMAP,
 -			       osdc->osdmap->epoch + 1, continuous))
 -		ceph_monc_renew_subs(&osdc->client->monc);
 -}
 -
 -static void __submit_request(struct ceph_osd_request *req, bool wrlocked)
 -{
 -	struct ceph_osd_client *osdc = req->r_osdc;
 -	struct ceph_osd *osd;
 -	bool need_send = false;
 -	bool promoted = false;
 -
 -	WARN_ON(req->r_tid || req->r_got_reply);
 -	dout("%s req %p wrlocked %d\n", __func__, req, wrlocked);
 -
 -again:
 -	calc_target(osdc, &req->r_t, &req->r_last_force_resend, false);
 -	osd = lookup_create_osd(osdc, req->r_t.osd, wrlocked);
 -	if (IS_ERR(osd)) {
 -		WARN_ON(PTR_ERR(osd) != -EAGAIN || wrlocked);
 -		goto promote;
 -	}
 -
 -	if ((req->r_flags & CEPH_OSD_FLAG_WRITE) &&
 -	    ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSEWR)) {
 -		dout("req %p pausewr\n", req);
 -		req->r_t.paused = true;
 -		maybe_request_map(osdc);
 -	} else if ((req->r_flags & CEPH_OSD_FLAG_READ) &&
 -		   ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSERD)) {
 -		dout("req %p pauserd\n", req);
 -		req->r_t.paused = true;
 -		maybe_request_map(osdc);
 -	} else if ((req->r_flags & CEPH_OSD_FLAG_WRITE) &&
 -		   !(req->r_flags & (CEPH_OSD_FLAG_FULL_TRY |
 -				     CEPH_OSD_FLAG_FULL_FORCE)) &&
 -		   (ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_FULL) ||
 -		    pool_full(osdc, req->r_t.base_oloc.pool))) {
 -		dout("req %p full/pool_full\n", req);
 -		pr_warn_ratelimited("FULL or reached pool quota\n");
 -		req->r_t.paused = true;
 -		maybe_request_map(osdc);
 -	} else if (!osd_homeless(osd)) {
 -		need_send = true;
 +	if (req->r_osd == NULL) {
 +		dout("send_request %p no up osds in pg\n", req);
 +		ceph_monc_request_next_osdmap(&osdc->client->monc);
  	} else {
 -		maybe_request_map(osdc);
 +		__send_queued(osdc);
  	}
  
 -	mutex_lock(&osd->lock);
 -	/*
 -	 * Assign the tid atomically with send_request() to protect
 -	 * multiple writes to the same object from racing with each
 -	 * other, resulting in out of order ops on the OSDs.
 -	 */
 -	req->r_tid = atomic64_inc_return(&osdc->last_tid);
 -	link_request(osd, req);
 -	if (need_send)
 -		send_request(req);
 -	mutex_unlock(&osd->lock);
 -
 -	if (promoted)
 -		downgrade_write(&osdc->lock);
 -	return;
 -
 -promote:
 -	up_read(&osdc->lock);
 -	down_write(&osdc->lock);
 -	wrlocked = true;
 -	promoted = true;
 -	goto again;
 -}
 -
 -static void account_request(struct ceph_osd_request *req)
 -{
 -	unsigned int mask = CEPH_OSD_FLAG_ACK | CEPH_OSD_FLAG_ONDISK;
 -
 -	if (req->r_flags & CEPH_OSD_FLAG_READ) {
 -		WARN_ON(req->r_flags & mask);
 -		req->r_flags |= CEPH_OSD_FLAG_ACK;
 -	} else if (req->r_flags & CEPH_OSD_FLAG_WRITE)
 -		WARN_ON(!(req->r_flags & mask));
 -	else
 -		WARN_ON(1);
 -
 -	WARN_ON(req->r_unsafe_callback && (req->r_flags & mask) != mask);
 -	atomic_inc(&req->r_osdc->num_requests);
 -}
 -
 -static void submit_request(struct ceph_osd_request *req, bool wrlocked)
 -{
 -	ceph_osdc_get_request(req);
 -	account_request(req);
 -	__submit_request(req, wrlocked);
 -}
 -
 -static void __finish_request(struct ceph_osd_request *req)
 -{
 -	struct ceph_osd_client *osdc = req->r_osdc;
 -	struct ceph_osd *osd = req->r_osd;
 -
 -	verify_osd_locked(osd);
 -	dout("%s req %p tid %llu\n", __func__, req, req->r_tid);
 -
 -	unlink_request(osd, req);
 -	atomic_dec(&osdc->num_requests);
 -
 -	/*
 -	 * If an OSD has failed or returned and a request has been sent
 -	 * twice, it's possible to get a reply and end up here while the
 -	 * request message is queued for delivery.  We will ignore the
 -	 * reply, so not a big deal, but better to try and catch it.
 -	 */
 -	ceph_msg_revoke(req->r_request);
 -	ceph_msg_revoke_incoming(req->r_reply);
 -}
 -
 -static void finish_request(struct ceph_osd_request *req)
 -{
 -	__finish_request(req);
 -	ceph_osdc_put_request(req);
 -}
 -
 -static void __complete_request(struct ceph_osd_request *req)
 -{
 -	if (req->r_callback)
 -		req->r_callback(req);
 -	else
 -		complete_all(&req->r_completion);
 -}
 -
 -static void cancel_request(struct ceph_osd_request *req)
 -{
 -	dout("%s req %p tid %llu\n", __func__, req, req->r_tid);
 -
 -	finish_request(req);
 +	return 0;
  }
  
  /*
 + * Timeout callback, called every N seconds when 1 or more osd
 + * requests has been active for more than N seconds.  When this
 + * happens, we ping all OSDs with requests who have timed out to
 + * ensure any communications channel reset is detected.  Reset the
 + * request timeouts another N seconds in the future as we go.
 + * Reschedule the timeout event another N seconds in future (unless
 + * there are no open requests).
++=======
+  * lingering requests, watch/notify v2 infrastructure
+  */
+ static void linger_release(struct kref *kref)
+ {
+ 	struct ceph_osd_linger_request *lreq =
+ 	    container_of(kref, struct ceph_osd_linger_request, kref);
+ 
+ 	dout("%s lreq %p reg_req %p ping_req %p\n", __func__, lreq,
+ 	     lreq->reg_req, lreq->ping_req);
+ 	WARN_ON(!RB_EMPTY_NODE(&lreq->node));
+ 	WARN_ON(!RB_EMPTY_NODE(&lreq->osdc_node));
+ 	WARN_ON(!list_empty(&lreq->scan_item));
+ 	WARN_ON(lreq->osd);
+ 
+ 	if (lreq->reg_req)
+ 		ceph_osdc_put_request(lreq->reg_req);
+ 	if (lreq->ping_req)
+ 		ceph_osdc_put_request(lreq->ping_req);
+ 	target_destroy(&lreq->t);
+ 	kfree(lreq);
+ }
+ 
+ static void linger_put(struct ceph_osd_linger_request *lreq)
+ {
+ 	if (lreq)
+ 		kref_put(&lreq->kref, linger_release);
+ }
+ 
+ static struct ceph_osd_linger_request *
+ linger_get(struct ceph_osd_linger_request *lreq)
+ {
+ 	kref_get(&lreq->kref);
+ 	return lreq;
+ }
+ 
+ static struct ceph_osd_linger_request *
+ linger_alloc(struct ceph_osd_client *osdc)
+ {
+ 	struct ceph_osd_linger_request *lreq;
+ 
+ 	lreq = kzalloc(sizeof(*lreq), GFP_NOIO);
+ 	if (!lreq)
+ 		return NULL;
+ 
+ 	kref_init(&lreq->kref);
+ 	mutex_init(&lreq->lock);
+ 	RB_CLEAR_NODE(&lreq->node);
+ 	RB_CLEAR_NODE(&lreq->osdc_node);
+ 	INIT_LIST_HEAD(&lreq->scan_item);
+ 	init_completion(&lreq->reg_commit_wait);
+ 
+ 	lreq->osdc = osdc;
+ 	target_init(&lreq->t);
+ 
+ 	dout("%s lreq %p\n", __func__, lreq);
+ 	return lreq;
+ }
+ 
+ DEFINE_RB_INSDEL_FUNCS(linger, struct ceph_osd_linger_request, linger_id, node)
+ DEFINE_RB_FUNCS(linger_osdc, struct ceph_osd_linger_request, linger_id, osdc_node)
+ 
+ /*
+  * Create linger request <-> OSD session relation.
+  *
+  * @lreq has to be registered, @osd may be homeless.
+  */
+ static void link_linger(struct ceph_osd *osd,
+ 			struct ceph_osd_linger_request *lreq)
+ {
+ 	verify_osd_locked(osd);
+ 	WARN_ON(!lreq->linger_id || lreq->osd);
+ 	dout("%s osd %p osd%d lreq %p linger_id %llu\n", __func__, osd,
+ 	     osd->o_osd, lreq, lreq->linger_id);
+ 
+ 	if (!osd_homeless(osd))
+ 		__remove_osd_from_lru(osd);
+ 	else
+ 		atomic_inc(&osd->o_osdc->num_homeless);
+ 
+ 	get_osd(osd);
+ 	insert_linger(&osd->o_linger_requests, lreq);
+ 	lreq->osd = osd;
+ }
+ 
+ static void unlink_linger(struct ceph_osd *osd,
+ 			  struct ceph_osd_linger_request *lreq)
+ {
+ 	verify_osd_locked(osd);
+ 	WARN_ON(lreq->osd != osd);
+ 	dout("%s osd %p osd%d lreq %p linger_id %llu\n", __func__, osd,
+ 	     osd->o_osd, lreq, lreq->linger_id);
+ 
+ 	lreq->osd = NULL;
+ 	erase_linger(&osd->o_linger_requests, lreq);
+ 	put_osd(osd);
+ 
+ 	if (!osd_homeless(osd))
+ 		maybe_move_osd_to_lru(osd);
+ 	else
+ 		atomic_dec(&osd->o_osdc->num_homeless);
+ }
+ 
+ static bool __linger_registered(struct ceph_osd_linger_request *lreq)
+ {
+ 	verify_osdc_locked(lreq->osdc);
+ 
+ 	return !RB_EMPTY_NODE(&lreq->osdc_node);
+ }
+ 
+ static bool linger_registered(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 	bool registered;
+ 
+ 	down_read(&osdc->lock);
+ 	registered = __linger_registered(lreq);
+ 	up_read(&osdc->lock);
+ 
+ 	return registered;
+ }
+ 
+ static void linger_register(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 
+ 	verify_osdc_wrlocked(osdc);
+ 	WARN_ON(lreq->linger_id);
+ 
+ 	linger_get(lreq);
+ 	lreq->linger_id = ++osdc->last_linger_id;
+ 	insert_linger_osdc(&osdc->linger_requests, lreq);
+ }
+ 
+ static void linger_unregister(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 
+ 	verify_osdc_wrlocked(osdc);
+ 
+ 	erase_linger_osdc(&osdc->linger_requests, lreq);
+ 	linger_put(lreq);
+ }
+ 
+ static void cancel_linger_request(struct ceph_osd_request *req)
+ {
+ 	struct ceph_osd_linger_request *lreq = req->r_priv;
+ 
+ 	WARN_ON(!req->r_linger);
+ 	cancel_request(req);
+ 	linger_put(lreq);
+ }
+ 
+ struct linger_work {
+ 	struct work_struct work;
+ 	struct ceph_osd_linger_request *lreq;
+ 
+ 	union {
+ 		struct {
+ 			u64 notify_id;
+ 			u64 notifier_id;
+ 			void *payload; /* points into @msg front */
+ 			size_t payload_len;
+ 
+ 			struct ceph_msg *msg; /* for ceph_msg_put() */
+ 		} notify;
+ 		struct {
+ 			int err;
+ 		} error;
+ 	};
+ };
+ 
+ static struct linger_work *lwork_alloc(struct ceph_osd_linger_request *lreq,
+ 				       work_func_t workfn)
+ {
+ 	struct linger_work *lwork;
+ 
+ 	lwork = kzalloc(sizeof(*lwork), GFP_NOIO);
+ 	if (!lwork)
+ 		return NULL;
+ 
+ 	INIT_WORK(&lwork->work, workfn);
+ 	lwork->lreq = linger_get(lreq);
+ 
+ 	return lwork;
+ }
+ 
+ static void lwork_free(struct linger_work *lwork)
+ {
+ 	struct ceph_osd_linger_request *lreq = lwork->lreq;
+ 
+ 	linger_put(lreq);
+ 	kfree(lwork);
+ }
+ 
+ static void lwork_queue(struct linger_work *lwork)
+ {
+ 	struct ceph_osd_linger_request *lreq = lwork->lreq;
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 
+ 	verify_lreq_locked(lreq);
+ 	queue_work(osdc->notify_wq, &lwork->work);
+ }
+ 
+ static void do_watch_notify(struct work_struct *w)
+ {
+ 	struct linger_work *lwork = container_of(w, struct linger_work, work);
+ 	struct ceph_osd_linger_request *lreq = lwork->lreq;
+ 
+ 	if (!linger_registered(lreq)) {
+ 		dout("%s lreq %p not registered\n", __func__, lreq);
+ 		goto out;
+ 	}
+ 
+ 	dout("%s lreq %p notify_id %llu notifier_id %llu payload_len %zu\n",
+ 	     __func__, lreq, lwork->notify.notify_id, lwork->notify.notifier_id,
+ 	     lwork->notify.payload_len);
+ 	lreq->wcb(lreq->data, lwork->notify.notify_id, lreq->linger_id,
+ 		  lwork->notify.notifier_id, lwork->notify.payload,
+ 		  lwork->notify.payload_len);
+ 
+ out:
+ 	ceph_msg_put(lwork->notify.msg);
+ 	lwork_free(lwork);
+ }
+ 
+ static void do_watch_error(struct work_struct *w)
+ {
+ 	struct linger_work *lwork = container_of(w, struct linger_work, work);
+ 	struct ceph_osd_linger_request *lreq = lwork->lreq;
+ 
+ 	if (!linger_registered(lreq)) {
+ 		dout("%s lreq %p not registered\n", __func__, lreq);
+ 		goto out;
+ 	}
+ 
+ 	dout("%s lreq %p err %d\n", __func__, lreq, lwork->error.err);
+ 	lreq->errcb(lreq->data, lreq->linger_id, lwork->error.err);
+ 
+ out:
+ 	lwork_free(lwork);
+ }
+ 
+ static void queue_watch_error(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct linger_work *lwork;
+ 
+ 	lwork = lwork_alloc(lreq, do_watch_error);
+ 	if (!lwork) {
+ 		pr_err("failed to allocate error-lwork\n");
+ 		return;
+ 	}
+ 
+ 	lwork->error.err = lreq->last_error;
+ 	lwork_queue(lwork);
+ }
+ 
+ static void linger_reg_commit_complete(struct ceph_osd_linger_request *lreq,
+ 				       int result)
+ {
+ 	if (!completion_done(&lreq->reg_commit_wait)) {
+ 		lreq->reg_commit_error = (result <= 0 ? result : 0);
+ 		complete_all(&lreq->reg_commit_wait);
+ 	}
+ }
+ 
+ static void linger_commit_cb(struct ceph_osd_request *req)
+ {
+ 	struct ceph_osd_linger_request *lreq = req->r_priv;
+ 
+ 	mutex_lock(&lreq->lock);
+ 	dout("%s lreq %p linger_id %llu result %d\n", __func__, lreq,
+ 	     lreq->linger_id, req->r_result);
+ 	WARN_ON(!__linger_registered(lreq));
+ 	linger_reg_commit_complete(lreq, req->r_result);
+ 	lreq->committed = true;
+ 
+ 	mutex_unlock(&lreq->lock);
+ 	linger_put(lreq);
+ }
+ 
+ static int normalize_watch_error(int err)
+ {
+ 	/*
+ 	 * Translate ENOENT -> ENOTCONN so that a delete->disconnection
+ 	 * notification and a failure to reconnect because we raced with
+ 	 * the delete appear the same to the user.
+ 	 */
+ 	if (err == -ENOENT)
+ 		err = -ENOTCONN;
+ 
+ 	return err;
+ }
+ 
+ static void linger_reconnect_cb(struct ceph_osd_request *req)
+ {
+ 	struct ceph_osd_linger_request *lreq = req->r_priv;
+ 
+ 	mutex_lock(&lreq->lock);
+ 	dout("%s lreq %p linger_id %llu result %d last_error %d\n", __func__,
+ 	     lreq, lreq->linger_id, req->r_result, lreq->last_error);
+ 	if (req->r_result < 0) {
+ 		if (!lreq->last_error) {
+ 			lreq->last_error = normalize_watch_error(req->r_result);
+ 			queue_watch_error(lreq);
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&lreq->lock);
+ 	linger_put(lreq);
+ }
+ 
+ static void send_linger(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_request *req = lreq->reg_req;
+ 	struct ceph_osd_req_op *op = &req->r_ops[0];
+ 
+ 	verify_osdc_wrlocked(req->r_osdc);
+ 	dout("%s lreq %p linger_id %llu\n", __func__, lreq, lreq->linger_id);
+ 
+ 	if (req->r_osd)
+ 		cancel_linger_request(req);
+ 
+ 	request_reinit(req);
+ 	ceph_oid_copy(&req->r_base_oid, &lreq->t.base_oid);
+ 	ceph_oloc_copy(&req->r_base_oloc, &lreq->t.base_oloc);
+ 	req->r_flags = lreq->t.flags;
+ 	req->r_mtime = lreq->mtime;
+ 
+ 	mutex_lock(&lreq->lock);
+ 	if (lreq->committed) {
+ 		WARN_ON(op->op != CEPH_OSD_OP_WATCH ||
+ 			op->watch.cookie != lreq->linger_id);
+ 		op->watch.op = CEPH_OSD_WATCH_OP_RECONNECT;
+ 		op->watch.gen = ++lreq->register_gen;
+ 		dout("lreq %p reconnect register_gen %u\n", lreq,
+ 		     op->watch.gen);
+ 		req->r_callback = linger_reconnect_cb;
+ 	} else {
+ 		WARN_ON(op->watch.op != CEPH_OSD_WATCH_OP_WATCH);
+ 		dout("lreq %p register\n", lreq);
+ 		req->r_callback = linger_commit_cb;
+ 	}
+ 	mutex_unlock(&lreq->lock);
+ 
+ 	req->r_priv = linger_get(lreq);
+ 	req->r_linger = true;
+ 
+ 	submit_request(req, true);
+ }
+ 
+ static void linger_ping_cb(struct ceph_osd_request *req)
+ {
+ 	struct ceph_osd_linger_request *lreq = req->r_priv;
+ 
+ 	mutex_lock(&lreq->lock);
+ 	dout("%s lreq %p linger_id %llu result %d ping_sent %lu last_error %d\n",
+ 	     __func__, lreq, lreq->linger_id, req->r_result, lreq->ping_sent,
+ 	     lreq->last_error);
+ 	if (lreq->register_gen == req->r_ops[0].watch.gen) {
+ 		if (req->r_result && !lreq->last_error) {
+ 			lreq->last_error = normalize_watch_error(req->r_result);
+ 			queue_watch_error(lreq);
+ 		}
+ 	} else {
+ 		dout("lreq %p register_gen %u ignoring old pong %u\n", lreq,
+ 		     lreq->register_gen, req->r_ops[0].watch.gen);
+ 	}
+ 
+ 	mutex_unlock(&lreq->lock);
+ 	linger_put(lreq);
+ }
+ 
+ static void send_linger_ping(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 	struct ceph_osd_request *req = lreq->ping_req;
+ 	struct ceph_osd_req_op *op = &req->r_ops[0];
+ 
+ 	if (ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSERD)) {
+ 		dout("%s PAUSERD\n", __func__);
+ 		return;
+ 	}
+ 
+ 	lreq->ping_sent = jiffies;
+ 	dout("%s lreq %p linger_id %llu ping_sent %lu register_gen %u\n",
+ 	     __func__, lreq, lreq->linger_id, lreq->ping_sent,
+ 	     lreq->register_gen);
+ 
+ 	if (req->r_osd)
+ 		cancel_linger_request(req);
+ 
+ 	request_reinit(req);
+ 	target_copy(&req->r_t, &lreq->t);
+ 
+ 	WARN_ON(op->op != CEPH_OSD_OP_WATCH ||
+ 		op->watch.cookie != lreq->linger_id ||
+ 		op->watch.op != CEPH_OSD_WATCH_OP_PING);
+ 	op->watch.gen = lreq->register_gen;
+ 	req->r_callback = linger_ping_cb;
+ 	req->r_priv = linger_get(lreq);
+ 	req->r_linger = true;
+ 
+ 	ceph_osdc_get_request(req);
+ 	account_request(req);
+ 	req->r_tid = atomic64_inc_return(&osdc->last_tid);
+ 	link_request(lreq->osd, req);
+ 	send_request(req);
+ }
+ 
+ static void linger_submit(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 	struct ceph_osd *osd;
+ 
+ 	calc_target(osdc, &lreq->t, &lreq->last_force_resend, false);
+ 	osd = lookup_create_osd(osdc, lreq->t.osd, true);
+ 	link_linger(osd, lreq);
+ 
+ 	send_linger(lreq);
+ }
+ 
+ /*
+  * @lreq has to be both registered and linked.
+  */
+ static void __linger_cancel(struct ceph_osd_linger_request *lreq)
+ {
+ 	if (lreq->ping_req->r_osd)
+ 		cancel_linger_request(lreq->ping_req);
+ 	if (lreq->reg_req->r_osd)
+ 		cancel_linger_request(lreq->reg_req);
+ 	unlink_linger(lreq->osd, lreq);
+ 	linger_unregister(lreq);
+ }
+ 
+ static void linger_cancel(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 
+ 	down_write(&osdc->lock);
+ 	if (__linger_registered(lreq))
+ 		__linger_cancel(lreq);
+ 	up_write(&osdc->lock);
+ }
+ 
+ static int linger_reg_commit_wait(struct ceph_osd_linger_request *lreq)
+ {
+ 	int ret;
+ 
+ 	dout("%s lreq %p linger_id %llu\n", __func__, lreq, lreq->linger_id);
+ 	ret = wait_for_completion_interruptible(&lreq->reg_commit_wait);
+ 	return ret ?: lreq->reg_commit_error;
+ }
+ 
+ /*
+  * Timeout callback, called every N seconds.  When 1 or more OSD
+  * requests has been active for more than N seconds, we send a keepalive
+  * (tag + timestamp) to its OSD to ensure any communications channel
+  * reset is detected.
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
   */
  static void handle_timeout(struct work_struct *work)
  {
@@@ -1575,20 -2197,45 +2354,49 @@@
  	 * is a break in the TCP connection we will notice, and reopen
  	 * a connection with that osd (from the fault callback).
  	 */
 -	for (n = rb_first(&osdc->osds); n; n = rb_next(n)) {
 -		struct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);
 -		bool found = false;
 +	INIT_LIST_HEAD(&slow_osds);
 +	list_for_each_entry(req, &osdc->req_lru, r_req_lru_item) {
 +		if (time_before(jiffies, req->r_stamp + keepalive))
 +			break;
  
++<<<<<<< HEAD
 +		osd = req->r_osd;
 +		BUG_ON(!osd);
 +		dout(" tid %llu is slow, will send keepalive on osd%d\n",
 +		     req->r_tid, osd->o_osd);
 +		list_move_tail(&osd->o_keepalive_item, &slow_osds);
++=======
+ 		for (p = rb_first(&osd->o_requests); p; p = rb_next(p)) {
+ 			struct ceph_osd_request *req =
+ 			    rb_entry(p, struct ceph_osd_request, r_node);
+ 
+ 			if (time_before(req->r_stamp, cutoff)) {
+ 				dout(" req %p tid %llu on osd%d is laggy\n",
+ 				     req, req->r_tid, osd->o_osd);
+ 				found = true;
+ 			}
+ 		}
+ 		for (p = rb_first(&osd->o_linger_requests); p; p = rb_next(p)) {
+ 			struct ceph_osd_linger_request *lreq =
+ 			    rb_entry(p, struct ceph_osd_linger_request, node);
+ 
+ 			dout(" lreq %p linger_id %llu is served by osd%d\n",
+ 			     lreq, lreq->linger_id, osd->o_osd);
+ 			found = true;
+ 
+ 			mutex_lock(&lreq->lock);
+ 			if (lreq->committed && !lreq->last_error)
+ 				send_linger_ping(lreq);
+ 			mutex_unlock(&lreq->lock);
+ 		}
+ 
+ 		if (found)
+ 			list_move_tail(&osd->o_keepalive_item, &slow_osds);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	}
 -
 -	if (atomic_read(&osdc->num_homeless) || !list_empty(&slow_osds))
 -		maybe_request_map(osdc);
 -
  	while (!list_empty(&slow_osds)) {
 -		struct ceph_osd *osd = list_first_entry(&slow_osds,
 -							struct ceph_osd,
 -							o_keepalive_item);
 +		osd = list_entry(slow_osds.next, struct ceph_osd,
 +				 o_keepalive_item);
  		list_del_init(&osd->o_keepalive_item);
  		ceph_con_keepalive(&osd->o_con);
  	}
@@@ -1604,14 -2250,21 +2412,23 @@@ static void handle_osds_timeout(struct 
  	struct ceph_osd_client *osdc =
  		container_of(work, struct ceph_osd_client,
  			     osds_timeout_work.work);
 -	unsigned long delay = osdc->client->options->osd_idle_ttl / 4;
 -	struct ceph_osd *osd, *nosd;
 +	unsigned long delay =
 +		osdc->client->options->osd_idle_ttl * HZ >> 2;
  
 -	dout("%s osdc %p\n", __func__, osdc);
 -	down_write(&osdc->lock);
 -	list_for_each_entry_safe(osd, nosd, &osdc->osd_lru, o_osd_lru) {
 -		if (time_before(jiffies, osd->lru_ttl))
 -			break;
 +	dout("osds timeout\n");
 +	down_read(&osdc->map_sem);
 +	remove_old_osds(osdc);
 +	up_read(&osdc->map_sem);
  
++<<<<<<< HEAD
++=======
+ 		WARN_ON(!RB_EMPTY_ROOT(&osd->o_requests));
+ 		WARN_ON(!RB_EMPTY_ROOT(&osd->o_linger_requests));
+ 		close_osd(osd);
+ 	}
+ 
+ 	up_write(&osdc->lock);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	schedule_delayed_work(&osdc->osds_timeout_work,
  			      round_jiffies_relative(delay));
  }
@@@ -1827,224 -2449,406 +2644,373 @@@ static void handle_reply(struct ceph_os
  	}
  
  	if (decode_redir) {
 -		ret = ceph_redirect_decode(&p, end, &m->redirect);
 -		if (ret)
 -			return ret;
 +		err = ceph_redirect_decode(&p, end, &redir);
 +		if (err)
 +			goto bad_put;
  	} else {
 -		ceph_oloc_init(&m->redirect.oloc);
 +		redir.oloc.pool = -1;
  	}
  
 -	return 0;
 +	if (redir.oloc.pool != -1) {
 +		dout("redirect pool %lld\n", redir.oloc.pool);
  
 -e_inval:
 -	return -EINVAL;
 -}
 +		__unregister_request(osdc, req);
  
 -/*
 - * We are done with @req if
 - *   - @m is a safe reply, or
 - *   - @m is an unsafe reply and we didn't want a safe one
 - */
 -static bool done_request(const struct ceph_osd_request *req,
 -			 const struct MOSDOpReply *m)
 -{
 -	return (m->result < 0 ||
 -		(m->flags & CEPH_OSD_FLAG_ONDISK) ||
 -		!(req->r_flags & CEPH_OSD_FLAG_ONDISK));
 -}
 +		req->r_target_oloc = redir.oloc; /* struct */
  
 -/*
 - * handle osd op reply.  either call the callback if it is specified,
 - * or do the completion to wake up the waiting thread.
 - *
 - * ->r_unsafe_callback is set?	yes			no
 - *
 - * first reply is OK (needed	r_cb/r_completion,	r_cb/r_completion,
 - * any or needed/got safe)	r_safe_completion	r_safe_completion
 - *
 - * first reply is unsafe	r_unsafe_cb(true)	(nothing)
 - *
 - * when we get the safe reply	r_unsafe_cb(false),	r_cb/r_completion,
 - *				r_safe_completion	r_safe_completion
 - */
 -static void handle_reply(struct ceph_osd *osd, struct ceph_msg *msg)
 -{
 -	struct ceph_osd_client *osdc = osd->o_osdc;
 -	struct ceph_osd_request *req;
 -	struct MOSDOpReply m;
 -	u64 tid = le64_to_cpu(msg->hdr.tid);
 -	u32 data_len = 0;
 -	bool already_acked;
 -	int ret;
 -	int i;
 -
 -	dout("%s msg %p tid %llu\n", __func__, msg, tid);
 +		/*
 +		 * Start redirect requests with nofail=true.  If
 +		 * mapping fails, request will end up on the notarget
 +		 * list, waiting for the new osdmap (which can take
 +		 * a while), even though the original request mapped
 +		 * successfully.  In the future we might want to follow
 +		 * original request's nofail setting here.
 +		 */
 +		err = __ceph_osdc_start_request(osdc, req, true);
 +		BUG_ON(err);
  
 -	down_read(&osdc->lock);
 -	if (!osd_registered(osd)) {
 -		dout("%s osd%d unknown\n", __func__, osd->o_osd);
 -		goto out_unlock_osdc;
 +		goto out_unlock;
  	}
 -	WARN_ON(osd->o_osd != le64_to_cpu(msg->hdr.src.num));
  
 -	mutex_lock(&osd->lock);
 -	req = lookup_request(&osd->o_requests, tid);
 -	if (!req) {
 -		dout("%s osd%d tid %llu unknown\n", __func__, osd->o_osd, tid);
 -		goto out_unlock_session;
 +	already_completed = req->r_got_reply;
 +	if (!req->r_got_reply) {
 +		req->r_result = result;
 +		dout("handle_reply result %d bytes %d\n", req->r_result,
 +		     bytes);
 +		if (req->r_result == 0)
 +			req->r_result = bytes;
 +
 +		/* in case this is a write and we need to replay, */
 +		req->r_reassert_version.epoch = cpu_to_le32(reassert_epoch);
 +		req->r_reassert_version.version = cpu_to_le64(reassert_version);
 +
 +		req->r_got_reply = 1;
 +	} else if ((flags & CEPH_OSD_FLAG_ONDISK) == 0) {
 +		dout("handle_reply tid %llu dup ack\n", tid);
 +		goto out_unlock;
  	}
  
 -	ret = decode_MOSDOpReply(msg, &m);
 -	if (ret) {
 -		pr_err("failed to decode MOSDOpReply for tid %llu: %d\n",
 -		       req->r_tid, ret);
 -		ceph_msg_dump(msg);
 -		goto fail_request;
 -	}
 -	dout("%s req %p tid %llu flags 0x%llx pgid %llu.%x epoch %u attempt %d v %u'%llu uv %llu\n",
 -	     __func__, req, req->r_tid, m.flags, m.pgid.pool, m.pgid.seed,
 -	     m.epoch, m.retry_attempt, le32_to_cpu(m.replay_version.epoch),
 -	     le64_to_cpu(m.replay_version.version), m.user_version);
 -
 -	if (m.retry_attempt >= 0) {
 -		if (m.retry_attempt != req->r_attempts - 1) {
 -			dout("req %p tid %llu retry_attempt %d != %d, ignoring\n",
 -			     req, req->r_tid, m.retry_attempt,
 -			     req->r_attempts - 1);
 -			goto out_unlock_session;
 -		}
 -	} else {
 -		WARN_ON(1); /* MOSDOpReply v4 is assumed */
 -	}
 +	dout("handle_reply tid %llu flags %d\n", tid, flags);
  
 -	if (!ceph_oloc_empty(&m.redirect.oloc)) {
 -		dout("req %p tid %llu redirect pool %lld\n", req, req->r_tid,
 -		     m.redirect.oloc.pool);
 -		unlink_request(osd, req);
 -		mutex_unlock(&osd->lock);
 +	if (req->r_linger && (flags & CEPH_OSD_FLAG_ONDISK))
 +		__register_linger_request(osdc, req);
  
 -		ceph_oloc_copy(&req->r_t.target_oloc, &m.redirect.oloc);
 -		req->r_flags |= CEPH_OSD_FLAG_REDIRECTED;
 -		req->r_tid = 0;
 -		__submit_request(req, false);
 -		goto out_unlock_osdc;
 -	}
 +	/* either this is a read, or we got the safe response */
 +	if (result < 0 ||
 +	    (flags & CEPH_OSD_FLAG_ONDISK) ||
 +	    ((flags & CEPH_OSD_FLAG_WRITE) == 0))
 +		__unregister_request(osdc, req);
  
 -	if (m.num_ops != req->r_num_ops) {
 -		pr_err("num_ops %d != %d for tid %llu\n", m.num_ops,
 -		       req->r_num_ops, req->r_tid);
 -		goto fail_request;
 -	}
 -	for (i = 0; i < req->r_num_ops; i++) {
 -		dout(" req %p tid %llu op %d rval %d len %u\n", req,
 -		     req->r_tid, i, m.rval[i], m.outdata_len[i]);
 -		req->r_ops[i].rval = m.rval[i];
 -		req->r_ops[i].outdata_len = m.outdata_len[i];
 -		data_len += m.outdata_len[i];
 -	}
 -	if (data_len != le32_to_cpu(msg->hdr.data_len)) {
 -		pr_err("sum of lens %u != %u for tid %llu\n", data_len,
 -		       le32_to_cpu(msg->hdr.data_len), req->r_tid);
 -		goto fail_request;
 -	}
 -	dout("%s req %p tid %llu acked %d result %d data_len %u\n", __func__,
 -	     req, req->r_tid, req->r_got_reply, m.result, data_len);
 -
 -	already_acked = req->r_got_reply;
 -	if (!already_acked) {
 -		req->r_result = m.result ?: data_len;
 -		req->r_replay_version = m.replay_version; /* struct */
 -		req->r_got_reply = true;
 -	} else if (!(m.flags & CEPH_OSD_FLAG_ONDISK)) {
 -		dout("req %p tid %llu dup ack\n", req, req->r_tid);
 -		goto out_unlock_session;
 -	}
 +	mutex_unlock(&osdc->request_mutex);
 +	up_read(&osdc->map_sem);
  
++<<<<<<< HEAD
 +	if (!already_completed) {
 +		if (req->r_unsafe_callback &&
 +		    result >= 0 && !(flags & CEPH_OSD_FLAG_ONDISK))
++=======
+ 	if (done_request(req, &m)) {
+ 		__finish_request(req);
+ 		if (req->r_linger) {
+ 			WARN_ON(req->r_unsafe_callback);
+ 			dout("req %p tid %llu cb (locked)\n", req, req->r_tid);
+ 			__complete_request(req);
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&osd->lock);
+ 	up_read(&osdc->lock);
+ 
+ 	if (done_request(req, &m)) {
+ 		if (already_acked && req->r_unsafe_callback) {
+ 			dout("req %p tid %llu safe-cb\n", req, req->r_tid);
+ 			req->r_unsafe_callback(req, false);
+ 		} else if (!req->r_linger) {
+ 			dout("req %p tid %llu cb\n", req, req->r_tid);
+ 			__complete_request(req);
+ 		}
+ 	} else {
+ 		if (req->r_unsafe_callback) {
+ 			dout("req %p tid %llu unsafe-cb\n", req, req->r_tid);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  			req->r_unsafe_callback(req, true);
 -		} else {
 -			WARN_ON(1);
 -		}
 +		if (req->r_callback)
 +			req->r_callback(req, msg);
 +		else
 +			complete_all(&req->r_completion);
 +	}
 +
 +	if (flags & CEPH_OSD_FLAG_ONDISK) {
 +		if (req->r_unsafe_callback && already_completed)
 +			req->r_unsafe_callback(req, false);
 +		complete_request(req);
  	}
 -	if (m.flags & CEPH_OSD_FLAG_ONDISK)
 -		complete_all(&req->r_safe_completion);
  
 +out:
 +	dout("req=%p req->r_linger=%d\n", req, req->r_linger);
  	ceph_osdc_put_request(req);
  	return;
 +out_unlock:
 +	mutex_unlock(&osdc->request_mutex);
 +	up_read(&osdc->map_sem);
 +	goto out;
  
 -fail_request:
 +bad_put:
  	req->r_result = -EIO;
 -	__finish_request(req);
 -	__complete_request(req);
 -	complete_all(&req->r_safe_completion);
 -out_unlock_session:
 -	mutex_unlock(&osd->lock);
 -out_unlock_osdc:
 -	up_read(&osdc->lock);
 +	__unregister_request(osdc, req);
 +	if (req->r_callback)
 +		req->r_callback(req, msg);
 +	else
 +		complete_all(&req->r_completion);
 +	complete_request(req);
 +	ceph_osdc_put_request(req);
 +bad_mutex:
 +	mutex_unlock(&osdc->request_mutex);
 +	up_read(&osdc->map_sem);
 +bad:
 +	pr_err("corrupt osd_op_reply got %d %d\n",
 +	       (int)msg->front.iov_len, le32_to_cpu(msg->hdr.front_len));
 +	ceph_msg_dump(msg);
  }
  
 -static void set_pool_was_full(struct ceph_osd_client *osdc)
 +static void reset_changed_osds(struct ceph_osd_client *osdc)
  {
 -	struct rb_node *n;
 +	struct rb_node *p, *n;
  
 -	for (n = rb_first(&osdc->osdmap->pg_pools); n; n = rb_next(n)) {
 -		struct ceph_pg_pool_info *pi =
 -		    rb_entry(n, struct ceph_pg_pool_info, node);
 +	dout("%s %p\n", __func__, osdc);
 +	for (p = rb_first(&osdc->osds); p; p = n) {
 +		struct ceph_osd *osd = rb_entry(p, struct ceph_osd, o_node);
  
 -		pi->was_full = __pool_full(pi);
 +		n = rb_next(p);
 +		if (!ceph_osd_is_up(osdc->osdmap, osd->o_osd) ||
 +		    memcmp(&osd->o_con.peer_addr,
 +			   ceph_osd_addr(osdc->osdmap,
 +					 osd->o_osd),
 +			   sizeof(struct ceph_entity_addr)) != 0)
 +			__reset_osd(osdc, osd);
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static bool pool_cleared_full(struct ceph_osd_client *osdc, s64 pool_id)
+ {
+ 	struct ceph_pg_pool_info *pi;
+ 
+ 	pi = ceph_pg_pool_by_id(osdc->osdmap, pool_id);
+ 	if (!pi)
+ 		return false;
+ 
+ 	return pi->was_full && !__pool_full(pi);
+ }
+ 
+ static enum calc_target_result
+ recalc_linger_target(struct ceph_osd_linger_request *lreq)
+ {
+ 	struct ceph_osd_client *osdc = lreq->osdc;
+ 	enum calc_target_result ct_res;
+ 
+ 	ct_res = calc_target(osdc, &lreq->t, &lreq->last_force_resend, true);
+ 	if (ct_res == CALC_TARGET_NEED_RESEND) {
+ 		struct ceph_osd *osd;
+ 
+ 		osd = lookup_create_osd(osdc, lreq->t.osd, true);
+ 		if (osd != lreq->osd) {
+ 			unlink_linger(lreq->osd, lreq);
+ 			link_linger(osd, lreq);
+ 		}
+ 	}
+ 
+ 	return ct_res;
+ }
+ 
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  /*
 - * Requeue requests whose mapping to an OSD has changed.
 + * Requeue requests whose mapping to an OSD has changed.  If requests map to
 + * no osd, request a new map.
 + *
 + * Caller should hold map_sem for read.
   */
 -static void scan_requests(struct ceph_osd *osd,
 -			  bool force_resend,
 -			  bool cleared_full,
 -			  bool check_pool_cleared_full,
 -			  struct rb_root *need_resend,
 -			  struct list_head *need_resend_linger)
 +static void kick_requests(struct ceph_osd_client *osdc, bool force_resend,
 +			  bool force_resend_writes)
  {
 -	struct ceph_osd_client *osdc = osd->o_osdc;
 -	struct rb_node *n;
 -	bool force_resend_writes;
 -
 +	struct ceph_osd_request *req, *nreq;
 +	struct rb_node *p;
 +	int needmap = 0;
 +	int err;
 +	bool force_resend_req;
 +
++<<<<<<< HEAD
 +	dout("kick_requests %s %s\n", force_resend ? " (force resend)" : "",
 +		force_resend_writes ? " (force resend writes)" : "");
 +	mutex_lock(&osdc->request_mutex);
 +	for (p = rb_first(&osdc->requests); p; ) {
 +		req = rb_entry(p, struct ceph_osd_request, r_node);
 +		p = rb_next(p);
++=======
+ 	for (n = rb_first(&osd->o_linger_requests); n; ) {
+ 		struct ceph_osd_linger_request *lreq =
+ 		    rb_entry(n, struct ceph_osd_linger_request, node);
+ 		enum calc_target_result ct_res;
+ 
+ 		n = rb_next(n); /* recalc_linger_target() */
+ 
+ 		dout("%s lreq %p linger_id %llu\n", __func__, lreq,
+ 		     lreq->linger_id);
+ 		ct_res = recalc_linger_target(lreq);
+ 		switch (ct_res) {
+ 		case CALC_TARGET_NO_ACTION:
+ 			force_resend_writes = cleared_full ||
+ 			    (check_pool_cleared_full &&
+ 			     pool_cleared_full(osdc, lreq->t.base_oloc.pool));
+ 			if (!force_resend && !force_resend_writes)
+ 				break;
+ 
+ 			/* fall through */
+ 		case CALC_TARGET_NEED_RESEND:
+ 			/*
+ 			 * scan_requests() for the previous epoch(s)
+ 			 * may have already added it to the list, since
+ 			 * it's not unlinked here.
+ 			 */
+ 			if (list_empty(&lreq->scan_item))
+ 				list_add_tail(&lreq->scan_item, need_resend_linger);
+ 			break;
+ 		case CALC_TARGET_POOL_DNE:
+ 			break;
+ 		}
+ 	}
+ 
+ 	for (n = rb_first(&osd->o_requests); n; ) {
+ 		struct ceph_osd_request *req =
+ 		    rb_entry(n, struct ceph_osd_request, r_node);
+ 		enum calc_target_result ct_res;
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  
 -		n = rb_next(n); /* unlink_request() */
 -
 -		dout("%s req %p tid %llu\n", __func__, req, req->r_tid);
 -		ct_res = calc_target(osdc, &req->r_t,
 -				     &req->r_last_force_resend, false);
 -		switch (ct_res) {
 -		case CALC_TARGET_NO_ACTION:
 -			force_resend_writes = cleared_full ||
 -			    (check_pool_cleared_full &&
 -			     pool_cleared_full(osdc, req->r_t.base_oloc.pool));
 -			if (!force_resend &&
 -			    (!(req->r_flags & CEPH_OSD_FLAG_WRITE) ||
 -			     !force_resend_writes))
 -				break;
 -
 -			/* fall through */
 -		case CALC_TARGET_NEED_RESEND:
 -			unlink_request(osd, req);
 -			insert_request(need_resend, req);
 -			break;
 -		case CALC_TARGET_POOL_DNE:
 -			break;
 -		}
 -	}
 -}
 -
 -static int handle_one_map(struct ceph_osd_client *osdc,
 -			  void *p, void *end, bool incremental,
 -			  struct rb_root *need_resend,
 -			  struct list_head *need_resend_linger)
 -{
 -	struct ceph_osdmap *newmap;
 -	struct rb_node *n;
 -	bool skipped_map = false;
 -	bool was_full;
 -
 -	was_full = ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_FULL);
 -	set_pool_was_full(osdc);
 -
 -	if (incremental)
 -		newmap = osdmap_apply_incremental(&p, end, osdc->osdmap);
 -	else
 -		newmap = ceph_osdmap_decode(&p, end);
 -	if (IS_ERR(newmap))
 -		return PTR_ERR(newmap);
 -
 -	if (newmap != osdc->osdmap) {
  		/*
 -		 * Preserve ->was_full before destroying the old map.
 -		 * For pools that weren't in the old map, ->was_full
 -		 * should be false.
 +		 * For linger requests that have not yet been
 +		 * registered, move them to the linger list; they'll
 +		 * be sent to the osd in the loop below.  Unregister
 +		 * the request before re-registering it as a linger
 +		 * request to ensure the __map_request() below
 +		 * will decide it needs to be sent.
  		 */
 -		for (n = rb_first(&newmap->pg_pools); n; n = rb_next(n)) {
 -			struct ceph_pg_pool_info *pi =
 -			    rb_entry(n, struct ceph_pg_pool_info, node);
 -			struct ceph_pg_pool_info *old_pi;
 -
 -			old_pi = ceph_pg_pool_by_id(osdc->osdmap, pi->id);
 -			if (old_pi)
 -				pi->was_full = old_pi->was_full;
 -			else
 -				WARN_ON(pi->was_full);
 +		if (req->r_linger && list_empty(&req->r_linger_item)) {
 +			dout("%p tid %llu restart on osd%d\n",
 +			     req, req->r_tid,
 +			     req->r_osd ? req->r_osd->o_osd : -1);
 +			ceph_osdc_get_request(req);
 +			__unregister_request(osdc, req);
 +			__register_linger_request(osdc, req);
 +			ceph_osdc_put_request(req);
 +			continue;
  		}
  
 -		if (osdc->osdmap->epoch &&
 -		    osdc->osdmap->epoch + 1 < newmap->epoch) {
 -			WARN_ON(incremental);
 -			skipped_map = true;
 +		force_resend_req = force_resend ||
 +			(force_resend_writes &&
 +				req->r_flags & CEPH_OSD_FLAG_WRITE);
 +		err = __map_request(osdc, req, force_resend_req);
 +		if (err < 0)
 +			continue;  /* error */
 +		if (req->r_osd == NULL) {
 +			dout("%p tid %llu maps to no osd\n", req, req->r_tid);
 +			needmap++;  /* request a newer map */
 +		} else if (err > 0) {
 +			if (!req->r_linger) {
 +				dout("%p tid %llu requeued on osd%d\n", req,
 +				     req->r_tid,
 +				     req->r_osd ? req->r_osd->o_osd : -1);
 +				req->r_flags |= CEPH_OSD_FLAG_RETRY;
 +			}
  		}
 -
 -		ceph_osdmap_destroy(osdc->osdmap);
 -		osdc->osdmap = newmap;
  	}
  
 -	was_full &= !ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_FULL);
 -	scan_requests(&osdc->homeless_osd, skipped_map, was_full, true,
 -		      need_resend, need_resend_linger);
 +	list_for_each_entry_safe(req, nreq, &osdc->req_linger,
 +				 r_linger_item) {
 +		dout("linger req=%p req->r_osd=%p\n", req, req->r_osd);
 +
 +		err = __map_request(osdc, req,
 +				    force_resend || force_resend_writes);
 +		dout("__map_request returned %d\n", err);
 +		if (err < 0)
 +			continue;  /* hrm! */
 +		if (req->r_osd == NULL || err > 0) {
 +			if (req->r_osd == NULL) {
 +				dout("lingering %p tid %llu maps to no osd\n",
 +				     req, req->r_tid);
 +				/*
 +				 * A homeless lingering request makes
 +				 * no sense, as it's job is to keep
 +				 * a particular OSD connection open.
 +				 * Request a newer map and kick the
 +				 * request, knowing that it won't be
 +				 * resent until we actually get a map
 +				 * that can tell us where to send it.
 +				 */
 +				needmap++;
 +			}
  
 -	for (n = rb_first(&osdc->osds); n; ) {
 -		struct ceph_osd *osd = rb_entry(n, struct ceph_osd, o_node);
++<<<<<<< HEAD
 +			dout("kicking lingering %p tid %llu osd%d\n", req,
 +			     req->r_tid, req->r_osd ? req->r_osd->o_osd : -1);
 +			__register_request(osdc, req);
 +			__unregister_linger_request(osdc, req);
 +		}
 +	}
 +	reset_changed_osds(osdc);
 +	mutex_unlock(&osdc->request_mutex);
  
 +	if (needmap) {
 +		dout("%d requests for down osds, need new map\n", needmap);
 +		ceph_monc_request_next_osdmap(&osdc->client->monc);
++=======
+ 		n = rb_next(n); /* close_osd() */
+ 
+ 		scan_requests(osd, skipped_map, was_full, true, need_resend,
+ 			      need_resend_linger);
+ 		if (!ceph_osd_is_up(osdc->osdmap, osd->o_osd) ||
+ 		    memcmp(&osd->o_con.peer_addr,
+ 			   ceph_osd_addr(osdc->osdmap, osd->o_osd),
+ 			   sizeof(struct ceph_entity_addr)))
+ 			close_osd(osd);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void kick_requests(struct ceph_osd_client *osdc,
+ 			  struct rb_root *need_resend,
+ 			  struct list_head *need_resend_linger)
+ {
+ 	struct ceph_osd_linger_request *lreq, *nlreq;
+ 	struct rb_node *n;
+ 
+ 	for (n = rb_first(need_resend); n; ) {
+ 		struct ceph_osd_request *req =
+ 		    rb_entry(n, struct ceph_osd_request, r_node);
+ 		struct ceph_osd *osd;
+ 
+ 		n = rb_next(n);
+ 		erase_request(need_resend, req); /* before link_request() */
+ 
+ 		WARN_ON(req->r_osd);
+ 		calc_target(osdc, &req->r_t, NULL, false);
+ 		osd = lookup_create_osd(osdc, req->r_t.osd, true);
+ 		link_request(osd, req);
+ 		if (!req->r_linger) {
+ 			if (!osd_homeless(osd) && !req->r_t.paused)
+ 				send_request(req);
+ 		} else {
+ 			cancel_linger_request(req);
+ 		}
+ 	}
+ 
+ 	list_for_each_entry_safe(lreq, nlreq, need_resend_linger, scan_item) {
+ 		if (!osd_homeless(lreq->osd))
+ 			send_linger(lreq);
+ 
+ 		list_del_init(&lreq->scan_item);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	}
  }
  
@@@ -2184,8 -2964,7 +3150,11 @@@ done
  bad:
  	pr_err("osdc handle_map corrupt msg\n");
  	ceph_msg_dump(msg);
++<<<<<<< HEAD
 +	up_write(&osdc->map_sem);
 +	return;
++=======
+ 	up_write(&osdc->lock);
  }
  
  /*
@@@ -2268,66 -2995,30 +3185,31 @@@ static void kick_osd_requests(struct ce
  	}
  }
  
- int ceph_osdc_create_event(struct ceph_osd_client *osdc,
- 			   void (*event_cb)(u64, u64, u8, void *),
- 			   void *data, struct ceph_osd_event **pevent)
- {
- 	struct ceph_osd_event *event;
- 
- 	event = kmalloc(sizeof(*event), GFP_NOIO);
- 	if (!event)
- 		return -ENOMEM;
- 
- 	dout("create_event %p\n", event);
- 	event->cb = event_cb;
- 	event->one_shot = 0;
- 	event->data = data;
- 	event->osdc = osdc;
- 	INIT_LIST_HEAD(&event->osd_node);
- 	RB_CLEAR_NODE(&event->node);
- 	kref_init(&event->kref);   /* one ref for us */
- 	kref_get(&event->kref);    /* one ref for the caller */
- 
- 	spin_lock(&osdc->event_lock);
- 	event->cookie = ++osdc->event_count;
- 	__insert_event(osdc, event);
- 	spin_unlock(&osdc->event_lock);
- 
- 	*pevent = event;
- 	return 0;
- }
- EXPORT_SYMBOL(ceph_osdc_create_event);
- 
- void ceph_osdc_cancel_event(struct ceph_osd_event *event)
+ /*
+  * If the osd connection drops, we need to resubmit all requests.
+  */
+ static void osd_fault(struct ceph_connection *con)
  {
- 	struct ceph_osd_client *osdc = event->osdc;
+ 	struct ceph_osd *osd = con->private;
+ 	struct ceph_osd_client *osdc = osd->o_osdc;
  
- 	dout("cancel_event %p\n", event);
- 	spin_lock(&osdc->event_lock);
- 	__remove_event(event);
- 	spin_unlock(&osdc->event_lock);
- 	ceph_osdc_put_event(event); /* caller's */
- }
- EXPORT_SYMBOL(ceph_osdc_cancel_event);
+ 	dout("%s osd %p osd%d\n", __func__, osd, osd->o_osd);
  
+ 	down_write(&osdc->lock);
+ 	if (!osd_registered(osd)) {
+ 		dout("%s osd%d unknown\n", __func__, osd->o_osd);
+ 		goto out_unlock;
+ 	}
  
- static void do_event_work(struct work_struct *work)
- {
- 	struct ceph_osd_event_work *event_work =
- 		container_of(work, struct ceph_osd_event_work, work);
- 	struct ceph_osd_event *event = event_work->event;
- 	u64 ver = event_work->ver;
- 	u64 notify_id = event_work->notify_id;
- 	u8 opcode = event_work->opcode;
+ 	if (!reopen_osd(osd))
+ 		kick_osd_requests(osd);
+ 	maybe_request_map(osdc);
  
- 	dout("do_event_work completing %p\n", event);
- 	event->cb(ver, notify_id, opcode, event->data);
- 	dout("do_event_work completed %p\n", event);
- 	ceph_osdc_put_event(event);
- 	kfree(event_work);
+ out_unlock:
+ 	up_write(&osdc->lock);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  }
  
- 
  /*
   * Process osd watch notifications
   */
@@@ -2511,13 -3120,10 +3417,20 @@@ void ceph_osdc_cancel_request(struct ce
  {
  	struct ceph_osd_client *osdc = req->r_osdc;
  
++<<<<<<< HEAD
 +	mutex_lock(&osdc->request_mutex);
 +	if (req->r_linger)
 +		__unregister_linger_request(osdc, req);
 +	__unregister_request(osdc, req);
 +	mutex_unlock(&osdc->request_mutex);
 +
 +	dout("%s %p tid %llu canceled\n", __func__, req, req->r_tid);
++=======
+ 	down_write(&osdc->lock);
+ 	if (req->r_osd)
+ 		cancel_request(req);
+ 	up_write(&osdc->lock);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  }
  EXPORT_SYMBOL(ceph_osdc_cancel_request);
  
@@@ -2599,28 -3415,22 +3704,33 @@@ int ceph_osdc_init(struct ceph_osd_clie
  
  	dout("init\n");
  	osdc->client = client;
 -	init_rwsem(&osdc->lock);
 +	osdc->osdmap = NULL;
 +	init_rwsem(&osdc->map_sem);
 +	mutex_init(&osdc->request_mutex);
 +	osdc->last_tid = 0;
  	osdc->osds = RB_ROOT;
  	INIT_LIST_HEAD(&osdc->osd_lru);
++<<<<<<< HEAD
 +	osdc->requests = RB_ROOT;
 +	INIT_LIST_HEAD(&osdc->req_lru);
 +	INIT_LIST_HEAD(&osdc->req_unsent);
 +	INIT_LIST_HEAD(&osdc->req_notarget);
 +	INIT_LIST_HEAD(&osdc->req_linger);
 +	osdc->num_requests = 0;
++=======
+ 	spin_lock_init(&osdc->osd_lru_lock);
+ 	osd_init(&osdc->homeless_osd);
+ 	osdc->homeless_osd.o_osdc = osdc;
+ 	osdc->homeless_osd.o_osd = CEPH_HOMELESS_OSD;
+ 	osdc->linger_requests = RB_ROOT;
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	INIT_DELAYED_WORK(&osdc->timeout_work, handle_timeout);
  	INIT_DELAYED_WORK(&osdc->osds_timeout_work, handle_osds_timeout);
- 	spin_lock_init(&osdc->event_lock);
- 	osdc->event_tree = RB_ROOT;
- 	osdc->event_count = 0;
  
 -	err = -ENOMEM;
 -	osdc->osdmap = ceph_osdmap_alloc();
 -	if (!osdc->osdmap)
 -		goto out;
 +	schedule_delayed_work(&osdc->osds_timeout_work,
 +	   round_jiffies_relative(osdc->client->options->osd_idle_ttl * HZ));
  
 +	err = -ENOMEM;
  	osdc->req_mempool = mempool_create_slab_pool(10,
  						     ceph_osd_request_cache);
  	if (!osdc->req_mempool)
@@@ -2660,11 -3475,23 +3770,24 @@@ void ceph_osdc_stop(struct ceph_osd_cli
  	destroy_workqueue(osdc->notify_wq);
  	cancel_delayed_work_sync(&osdc->timeout_work);
  	cancel_delayed_work_sync(&osdc->osds_timeout_work);
 -
 -	down_write(&osdc->lock);
 -	while (!RB_EMPTY_ROOT(&osdc->osds)) {
 -		struct ceph_osd *osd = rb_entry(rb_first(&osdc->osds),
 -						struct ceph_osd, o_node);
 -		close_osd(osd);
 +	if (osdc->osdmap) {
 +		ceph_osdmap_destroy(osdc->osdmap);
 +		osdc->osdmap = NULL;
  	}
++<<<<<<< HEAD
 +	remove_all_osds(osdc);
++=======
+ 	up_write(&osdc->lock);
+ 	WARN_ON(atomic_read(&osdc->homeless_osd.o_ref) != 1);
+ 	osd_cleanup(&osdc->homeless_osd);
+ 
+ 	WARN_ON(!list_empty(&osdc->osd_lru));
+ 	WARN_ON(!RB_EMPTY_ROOT(&osdc->linger_requests));
+ 	WARN_ON(atomic_read(&osdc->num_requests));
+ 	WARN_ON(atomic_read(&osdc->num_homeless));
+ 
+ 	ceph_osdmap_destroy(osdc->osdmap);
++>>>>>>> 922dab613417 (libceph, rbd: ceph_osd_linger_request, watch/notify v2)
  	mempool_destroy(osdc->req_mempool);
  	ceph_msgpool_destroy(&osdc->msgpool_op);
  	ceph_msgpool_destroy(&osdc->msgpool_op_reply);
diff --git a/drivers/block/rbd.c b/drivers/block/rbd.c
index 48987296f95c..f7821db61a63 100644
--- a/drivers/block/rbd.c
+++ b/drivers/block/rbd.c
@@ -351,11 +351,11 @@ struct rbd_device {
 	struct rbd_options	*opts;
 
 	struct ceph_object_id	header_oid;
+	struct ceph_object_locator header_oloc;
 
 	struct ceph_file_layout	layout;
 
-	struct ceph_osd_event   *watch_event;
-	struct rbd_obj_request	*watch_request;
+	struct ceph_osd_linger_request *watch_handle;
 
 	struct rbd_spec		*parent_spec;
 	u64			parent_overlap;
@@ -1640,12 +1640,6 @@ static int rbd_obj_request_wait(struct rbd_obj_request *obj_request)
 	return __rbd_obj_request_wait(obj_request, 0);
 }
 
-static int rbd_obj_request_wait_timeout(struct rbd_obj_request *obj_request,
-					unsigned long timeout)
-{
-	return __rbd_obj_request_wait(obj_request, timeout);
-}
-
 static void rbd_img_request_complete(struct rbd_img_request *img_request)
 {
 
@@ -1795,12 +1789,6 @@ static void rbd_obj_request_complete(struct rbd_obj_request *obj_request)
 		complete_all(&obj_request->completion);
 }
 
-static void rbd_osd_trivial_callback(struct rbd_obj_request *obj_request)
-{
-	dout("%s: obj %p\n", __func__, obj_request);
-	obj_request_done_set(obj_request);
-}
-
 static void rbd_osd_read_callback(struct rbd_obj_request *obj_request)
 {
 	struct rbd_img_request *img_request = NULL;
@@ -1922,10 +1910,6 @@ static void rbd_osd_req_callback(struct ceph_osd_request *osd_req,
 	case CEPH_OSD_OP_CALL:
 		rbd_osd_call_callback(obj_request);
 		break;
-	case CEPH_OSD_OP_NOTIFY_ACK:
-	case CEPH_OSD_OP_WATCH:
-		rbd_osd_trivial_callback(obj_request);
-		break;
 	default:
 		rbd_warn(NULL, "%s: unsupported op %hu",
 			obj_request->object_name, (unsigned short) opcode);
@@ -3136,45 +3120,18 @@ out_err:
 	obj_request_done_set(obj_request);
 }
 
-static int rbd_obj_notify_ack_sync(struct rbd_device *rbd_dev, u64 notify_id)
-{
-	struct rbd_obj_request *obj_request;
-	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
-	int ret;
-
-	obj_request = rbd_obj_request_create(rbd_dev->header_oid.name, 0, 0,
-							OBJ_REQUEST_NODATA);
-	if (!obj_request)
-		return -ENOMEM;
+static int rbd_dev_header_watch_sync(struct rbd_device *rbd_dev);
+static void __rbd_dev_header_unwatch_sync(struct rbd_device *rbd_dev);
 
-	ret = -ENOMEM;
-	obj_request->osd_req = rbd_osd_req_create(rbd_dev, OBJ_OP_READ, 1,
-						  obj_request);
-	if (!obj_request->osd_req)
-		goto out;
-
-	osd_req_op_watch_init(obj_request->osd_req, 0, CEPH_OSD_OP_NOTIFY_ACK,
-					notify_id, 0, 0);
-	rbd_osd_req_format_read(obj_request);
-
-	ret = rbd_obj_request_submit(osdc, obj_request);
-	if (ret)
-		goto out;
-	ret = rbd_obj_request_wait(obj_request);
-out:
-	rbd_obj_request_put(obj_request);
-
-	return ret;
-}
-
-static void rbd_watch_cb(u64 ver, u64 notify_id, u8 opcode, void *data)
+static void rbd_watch_cb(void *arg, u64 notify_id, u64 cookie,
+			 u64 notifier_id, void *data, size_t data_len)
 {
-	struct rbd_device *rbd_dev = (struct rbd_device *)data;
+	struct rbd_device *rbd_dev = arg;
+	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
 	int ret;
 
-	dout("%s: \"%s\" notify_id %llu opcode %u\n", __func__,
-		rbd_dev->header_oid.name, (unsigned long long)notify_id,
-		(unsigned int)opcode);
+	dout("%s rbd_dev %p cookie %llu notify_id %llu\n", __func__, rbd_dev,
+	     cookie, notify_id);
 
 	/*
 	 * Until adequate refresh error handling is in place, there is
@@ -3186,63 +3143,31 @@ static void rbd_watch_cb(u64 ver, u64 notify_id, u8 opcode, void *data)
 	if (ret)
 		rbd_warn(rbd_dev, "refresh failed: %d", ret);
 
-	ret = rbd_obj_notify_ack_sync(rbd_dev, notify_id);
+	ret = ceph_osdc_notify_ack(osdc, &rbd_dev->header_oid,
+				   &rbd_dev->header_oloc, notify_id, cookie,
+				   NULL, 0);
 	if (ret)
 		rbd_warn(rbd_dev, "notify_ack ret %d", ret);
 }
 
-/*
- * Send a (un)watch request and wait for the ack.  Return a request
- * with a ref held on success or error.
- */
-static struct rbd_obj_request *rbd_obj_watch_request_helper(
-						struct rbd_device *rbd_dev,
-						bool watch)
+static void rbd_watch_errcb(void *arg, u64 cookie, int err)
 {
-	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
-	struct ceph_options *opts = osdc->client->options;
-	struct rbd_obj_request *obj_request;
+	struct rbd_device *rbd_dev = arg;
 	int ret;
 
-	obj_request = rbd_obj_request_create(rbd_dev->header_oid.name, 0, 0,
-					     OBJ_REQUEST_NODATA);
-	if (!obj_request)
-		return ERR_PTR(-ENOMEM);
-
-	obj_request->osd_req = rbd_osd_req_create(rbd_dev, OBJ_OP_WRITE, 1,
-						  obj_request);
-	if (!obj_request->osd_req) {
-		ret = -ENOMEM;
-		goto out;
-	}
-
-	osd_req_op_watch_init(obj_request->osd_req, 0, CEPH_OSD_OP_WATCH,
-			      rbd_dev->watch_event->cookie, 0, watch);
-	rbd_osd_req_format_write(obj_request);
-
-	if (watch)
-		ceph_osdc_set_request_linger(osdc, obj_request->osd_req);
-
-	ret = rbd_obj_request_submit(osdc, obj_request);
-	if (ret)
-		goto out;
+	rbd_warn(rbd_dev, "encountered watch error: %d", err);
 
-	ret = rbd_obj_request_wait_timeout(obj_request, opts->mount_timeout);
-	if (ret)
-		goto out;
+	__rbd_dev_header_unwatch_sync(rbd_dev);
 
-	ret = obj_request->result;
+	ret = rbd_dev_header_watch_sync(rbd_dev);
 	if (ret) {
-		if (watch)
-			rbd_obj_request_end(obj_request);
-		goto out;
+		rbd_warn(rbd_dev, "failed to reregister watch: %d", ret);
+		return;
 	}
 
-	return obj_request;
-
-out:
-	rbd_obj_request_put(obj_request);
-	return ERR_PTR(ret);
+	ret = rbd_dev_refresh(rbd_dev);
+	if (ret)
+		rbd_warn(rbd_dev, "reregisteration refresh failed: %d", ret);
 }
 
 /*
@@ -3251,57 +3176,33 @@ out:
 static int rbd_dev_header_watch_sync(struct rbd_device *rbd_dev)
 {
 	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
-	struct rbd_obj_request *obj_request;
-	int ret;
+	struct ceph_osd_linger_request *handle;
 
-	rbd_assert(!rbd_dev->watch_event);
-	rbd_assert(!rbd_dev->watch_request);
-
-	ret = ceph_osdc_create_event(osdc, rbd_watch_cb, rbd_dev,
-				     &rbd_dev->watch_event);
-	if (ret < 0)
-		return ret;
-
-	obj_request = rbd_obj_watch_request_helper(rbd_dev, true);
-	if (IS_ERR(obj_request)) {
-		ceph_osdc_cancel_event(rbd_dev->watch_event);
-		rbd_dev->watch_event = NULL;
-		return PTR_ERR(obj_request);
-	}
+	rbd_assert(!rbd_dev->watch_handle);
 
-	/*
-	 * A watch request is set to linger, so the underlying osd
-	 * request won't go away until we unregister it.  We retain
-	 * a pointer to the object request during that time (in
-	 * rbd_dev->watch_request), so we'll keep a reference to it.
-	 * We'll drop that reference after we've unregistered it in
-	 * rbd_dev_header_unwatch_sync().
-	 */
-	rbd_dev->watch_request = obj_request;
+	handle = ceph_osdc_watch(osdc, &rbd_dev->header_oid,
+				 &rbd_dev->header_oloc, rbd_watch_cb,
+				 rbd_watch_errcb, rbd_dev);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
 
+	rbd_dev->watch_handle = handle;
 	return 0;
 }
 
 static void __rbd_dev_header_unwatch_sync(struct rbd_device *rbd_dev)
 {
-	struct rbd_obj_request *obj_request;
-
-	rbd_assert(rbd_dev->watch_event);
-	rbd_assert(rbd_dev->watch_request);
+	struct ceph_osd_client *osdc = &rbd_dev->rbd_client->client->osdc;
+	int ret;
 
-	rbd_obj_request_end(rbd_dev->watch_request);
-	rbd_obj_request_put(rbd_dev->watch_request);
-	rbd_dev->watch_request = NULL;
+	if (!rbd_dev->watch_handle)
+		return;
 
-	obj_request = rbd_obj_watch_request_helper(rbd_dev, false);
-	if (!IS_ERR(obj_request))
-		rbd_obj_request_put(obj_request);
-	else
-		rbd_warn(rbd_dev, "unable to tear down watch request (%ld)",
-			 PTR_ERR(obj_request));
+	ret = ceph_osdc_unwatch(osdc, rbd_dev->watch_handle);
+	if (ret)
+		rbd_warn(rbd_dev, "failed to unwatch: %d", ret);
 
-	ceph_osdc_cancel_event(rbd_dev->watch_event);
-	rbd_dev->watch_event = NULL;
+	rbd_dev->watch_handle = NULL;
 }
 
 /*
@@ -4161,6 +4062,7 @@ static struct rbd_device *rbd_dev_create(struct rbd_client *rbdc,
 	init_rwsem(&rbd_dev->header_rwsem);
 
 	ceph_oid_init(&rbd_dev->header_oid);
+	ceph_oloc_init(&rbd_dev->header_oloc);
 
 	rbd_dev->dev.bus = &rbd_bus_type;
 	rbd_dev->dev.type = &rbd_device_type;
@@ -5364,6 +5266,7 @@ static int rbd_dev_header_name(struct rbd_device *rbd_dev)
 
 	rbd_assert(rbd_image_format_valid(rbd_dev->image_format));
 
+	rbd_dev->header_oloc.pool = ceph_file_layout_pg_pool(rbd_dev->layout);
 	if (rbd_dev->image_format == 1)
 		ret = ceph_oid_aprintf(&rbd_dev->header_oid, GFP_KERNEL, "%s%s",
 				       spec->image_name, RBD_SUFFIX);
diff --git a/include/linux/ceph/ceph_fs.h b/include/linux/ceph/ceph_fs.h
index d7d072a25c27..9ba1e356df30 100644
--- a/include/linux/ceph/ceph_fs.h
+++ b/include/linux/ceph/ceph_fs.h
@@ -153,8 +153,9 @@ struct ceph_dir_layout {
 
 /* watch-notify operations */
 enum {
-  WATCH_NOTIFY				= 1, /* notifying watcher */
-  WATCH_NOTIFY_COMPLETE			= 2, /* notifier notified when done */
+	CEPH_WATCH_EVENT_NOTIFY		  = 1, /* notifying watcher */
+	CEPH_WATCH_EVENT_NOTIFY_COMPLETE  = 2, /* notifier notified when done */
+	CEPH_WATCH_EVENT_DISCONNECT       = 3, /* we were disconnected */
 };
 
 
* Unmerged path include/linux/ceph/osd_client.h
diff --git a/include/linux/ceph/rados.h b/include/linux/ceph/rados.h
index 913c87c26d33..557441ab1424 100644
--- a/include/linux/ceph/rados.h
+++ b/include/linux/ceph/rados.h
@@ -415,7 +415,17 @@ enum {
 	CEPH_OSD_CMPXATTR_MODE_U64    = 2
 };
 
-#define RADOS_NOTIFY_VER	1
+enum {
+	CEPH_OSD_WATCH_OP_UNWATCH = 0,
+	CEPH_OSD_WATCH_OP_LEGACY_WATCH = 1,
+	/* note: use only ODD ids to prevent pre-giant code from
+	   interpreting the op as UNWATCH */
+	CEPH_OSD_WATCH_OP_WATCH = 3,
+	CEPH_OSD_WATCH_OP_RECONNECT = 5,
+	CEPH_OSD_WATCH_OP_PING = 7,
+};
+
+const char *ceph_osd_watch_op_name(int o);
 
 /*
  * an individual object operation.  each may be accompanied by some data
@@ -450,8 +460,9 @@ struct ceph_osd_op {
 	        } __attribute__ ((packed)) snap;
 		struct {
 			__le64 cookie;
-			__le64 ver;
-			__u8 flag;	/* 0 = unwatch, 1 = watch */
+			__le64 ver;     /* no longer used */
+			__u8 op;	/* CEPH_OSD_WATCH_OP_* */
+			__le32 gen;     /* registration generation */
 		} __attribute__ ((packed)) watch;
 		struct {
 			__le64 offset, length;
diff --git a/net/ceph/ceph_strings.c b/net/ceph/ceph_strings.c
index 139a9cb19b0c..3773a4fa11e3 100644
--- a/net/ceph/ceph_strings.c
+++ b/net/ceph/ceph_strings.c
@@ -27,6 +27,22 @@ __CEPH_FORALL_OSD_OPS(GENERATE_CASE)
 	}
 }
 
+const char *ceph_osd_watch_op_name(int o)
+{
+	switch (o) {
+	case CEPH_OSD_WATCH_OP_UNWATCH:
+		return "unwatch";
+	case CEPH_OSD_WATCH_OP_WATCH:
+		return "watch";
+	case CEPH_OSD_WATCH_OP_RECONNECT:
+		return "reconnect";
+	case CEPH_OSD_WATCH_OP_PING:
+		return "ping";
+	default:
+		return "???";
+	}
+}
+
 const char *ceph_osd_state_name(int s)
 {
 	switch (s) {
* Unmerged path net/ceph/debugfs.c
* Unmerged path net/ceph/osd_client.c
