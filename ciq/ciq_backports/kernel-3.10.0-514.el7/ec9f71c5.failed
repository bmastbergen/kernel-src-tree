rhashtable: Remove obj_raw_hashfn

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit ec9f71c59e00388efc1337307511b59cc4c48394
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ec9f71c5.failed

Now that the only caller of obj_raw_hashfn is head_hashfn, we can
simply kill it and fold it into the latter.

This patch also moves the common shift from head_hashfn/key_hashfn
into rht_bucket_index.

	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
	Acked-by: Thomas Graf <tgraf@suug.ch>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ec9f71c59e00388efc1337307511b59cc4c48394)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rhashtable.c
diff --cc lib/rhashtable.c
index 6d0c4774001c,6ffc793145f3..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -51,52 -63,91 +51,69 @@@ static void *rht_obj(const struct rhash
  
  static u32 rht_bucket_index(const struct bucket_table *tbl, u32 hash)
  {
++<<<<<<< HEAD
 +	return hash & (tbl->size - 1);
 +}
 +
 +static u32 obj_raw_hashfn(const struct rhashtable *ht, const void *ptr)
 +{
 +	u32 hash;
 +
 +	if (unlikely(!ht->p.key_len))
 +		hash = ht->p.obj_hashfn(ptr, ht->p.hash_rnd);
 +	else
 +		hash = ht->p.hashfn(ptr + ht->p.key_offset, ht->p.key_len,
 +				    ht->p.hash_rnd);
 +
 +	return hash;
++=======
+ 	return (hash >> HASH_RESERVED_SPACE) & (tbl->size - 1);
++>>>>>>> ec9f71c59e00 (rhashtable: Remove obj_raw_hashfn)
  }
  
 -static u32 key_hashfn(struct rhashtable *ht, const struct bucket_table *tbl,
 -		      const void *key)
 +static u32 key_hashfn(const struct rhashtable *ht, const void *key, u32 len)
  {
++<<<<<<< HEAD
 +	struct bucket_table *tbl = rht_dereference_rcu(ht->tbl, ht);
 +	u32 hash;
 +
 +	hash = ht->p.hashfn(key, len, ht->p.hash_rnd);
 +
 +	return rht_bucket_index(tbl, hash);
++=======
+ 	return rht_bucket_index(tbl, ht->p.hashfn(key, ht->p.key_len,
+ 						  tbl->hash_rnd));
++>>>>>>> ec9f71c59e00 (rhashtable: Remove obj_raw_hashfn)
  }
  
 -static u32 head_hashfn(struct rhashtable *ht,
 +static u32 head_hashfn(const struct rhashtable *ht,
  		       const struct bucket_table *tbl,
  		       const struct rhash_head *he)
  {
++<<<<<<< HEAD
 +	return rht_bucket_index(tbl, obj_raw_hashfn(ht, rht_obj(ht, he)));
++=======
+ 	const char *ptr = rht_obj(ht, he);
+ 
+ 	return likely(ht->p.key_len) ?
+ 	       key_hashfn(ht, tbl, ptr + ht->p.key_offset) :
+ 	       rht_bucket_index(tbl, ht->p.obj_hashfn(ptr, tbl->hash_rnd));
++>>>>>>> ec9f71c59e00 (rhashtable: Remove obj_raw_hashfn)
  }
  
 -#ifdef CONFIG_PROVE_LOCKING
 -#define ASSERT_RHT_MUTEX(HT) BUG_ON(!lockdep_rht_mutex_is_held(HT))
 -
 -int lockdep_rht_mutex_is_held(struct rhashtable *ht)
 +static struct rhash_head __rcu **bucket_tail(struct bucket_table *tbl, u32 n)
  {
 -	return (debug_locks) ? lockdep_is_held(&ht->mutex) : 1;
 -}
 -EXPORT_SYMBOL_GPL(lockdep_rht_mutex_is_held);
 -
 -int lockdep_rht_bucket_is_held(const struct bucket_table *tbl, u32 hash)
 -{
 -	spinlock_t *lock = bucket_lock(tbl, hash);
 -
 -	return (debug_locks) ? lockdep_is_held(lock) : 1;
 -}
 -EXPORT_SYMBOL_GPL(lockdep_rht_bucket_is_held);
 -#else
 -#define ASSERT_RHT_MUTEX(HT)
 -#endif
 -
 -
 -static int alloc_bucket_locks(struct rhashtable *ht, struct bucket_table *tbl)
 -{
 -	unsigned int i, size;
 -#if defined(CONFIG_PROVE_LOCKING)
 -	unsigned int nr_pcpus = 2;
 -#else
 -	unsigned int nr_pcpus = num_possible_cpus();
 -#endif
 -
 -	nr_pcpus = min_t(unsigned int, nr_pcpus, 32UL);
 -	size = roundup_pow_of_two(nr_pcpus * ht->p.locks_mul);
 -
 -	/* Never allocate more than 0.5 locks per bucket */
 -	size = min_t(unsigned int, size, tbl->size >> 1);
 -
 -	if (sizeof(spinlock_t) != 0) {
 -#ifdef CONFIG_NUMA
 -		if (size * sizeof(spinlock_t) > PAGE_SIZE)
 -			tbl->locks = vmalloc(size * sizeof(spinlock_t));
 -		else
 -#endif
 -		tbl->locks = kmalloc_array(size, sizeof(spinlock_t),
 -					   GFP_KERNEL);
 -		if (!tbl->locks)
 -			return -ENOMEM;
 -		for (i = 0; i < size; i++)
 -			spin_lock_init(&tbl->locks[i]);
 -	}
 -	tbl->locks_mask = size - 1;
 -
 -	return 0;
 -}
 +	struct rhash_head __rcu **pprev;
  
 -static void bucket_table_free(const struct bucket_table *tbl)
 -{
 -	if (tbl)
 -		kvfree(tbl->locks);
 +	for (pprev = &tbl->buckets[n];
 +	     rht_dereference_bucket(*pprev, tbl, n);
 +	     pprev = &rht_dereference_bucket(*pprev, tbl, n)->next)
 +		;
  
 -	kvfree(tbl);
 +	return pprev;
  }
  
 -static struct bucket_table *bucket_table_alloc(struct rhashtable *ht,
 -					       size_t nbuckets)
 +static struct bucket_table *bucket_table_alloc(size_t nbuckets)
  {
  	struct bucket_table *tbl = NULL;
  	size_t size;
* Unmerged path lib/rhashtable.c
