x86/tsc: Always Running Timer (ART) correlated clocksource

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] tsc: Always Running Timer correlated clocksource (Prarit Bhargava) [1273198]
Rebuild_FUZZ: 90.57%
commit-author Christopher S. Hall <christopher.s.hall@intel.com>
commit f9677e0f83080bb4186865868c359e72e1fac1ea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f9677e0f.failed

On modern Intel systems TSC is derived from the new Always Running Timer
(ART). ART can be captured simultaneous to the capture of
audio and network device clocks, allowing a correlation between timebases
to be constructed. Upon capture, the driver converts the captured ART
value to the appropriate system clock using the correlated clocksource
mechanism.

On systems that support ART a new CPUID leaf (0x15) returns parameters
“m” and “n” such that:

TSC_value = (ART_value * m) / n + k [n >= 1]

[k is an offset that can adjusted by a privileged agent. The
IA32_TSC_ADJUST MSR is an example of an interface to adjust k.
See 17.14.4 of the Intel SDM for more details]

	Cc: Prarit Bhargava <prarit@redhat.com>
	Cc: Richard Cochran <richardcochran@gmail.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: kevin.b.stanton@intel.com
	Cc: kevin.j.clarke@intel.com
	Cc: hpa@zytor.com
	Cc: jeffrey.t.kirsher@intel.com
	Cc: netdev@vger.kernel.org
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Christopher S. Hall <christopher.s.hall@intel.com>
[jstultz: Tweaked to fix build issue, also reworked math for
64bit division on 32bit systems, as well as !CONFIG_CPU_FREQ build
fixes]
	Signed-off-by: John Stultz <john.stultz@linaro.org>
(cherry picked from commit f9677e0f83080bb4186865868c359e72e1fac1ea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeature.h
#	arch/x86/kernel/tsc.c
diff --cc arch/x86/include/asm/cpufeature.h
index ca3dea2fde08,ff557b49ab77..000000000000
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@@ -70,107 -74,110 +70,137 @@@
  
  /* Other features, Linux-defined mapping, word 3 */
  /* This range is used for feature bits which conflict or are synthesized */
 -#define X86_FEATURE_CXMMX	( 3*32+ 0) /* Cyrix MMX extensions */
 -#define X86_FEATURE_K6_MTRR	( 3*32+ 1) /* AMD K6 nonstandard MTRRs */
 -#define X86_FEATURE_CYRIX_ARR	( 3*32+ 2) /* Cyrix ARRs (= MTRRs) */
 -#define X86_FEATURE_CENTAUR_MCR	( 3*32+ 3) /* Centaur MCRs (= MTRRs) */
 +#define X86_FEATURE_CXMMX	(3*32+ 0) /* Cyrix MMX extensions */
 +#define X86_FEATURE_K6_MTRR	(3*32+ 1) /* AMD K6 nonstandard MTRRs */
 +#define X86_FEATURE_CYRIX_ARR	(3*32+ 2) /* Cyrix ARRs (= MTRRs) */
 +#define X86_FEATURE_CENTAUR_MCR	(3*32+ 3) /* Centaur MCRs (= MTRRs) */
  /* cpu types for specific tunings: */
++<<<<<<< HEAD
 +#define X86_FEATURE_K8		(3*32+ 4) /* "" Opteron, Athlon64 */
 +#define X86_FEATURE_K7		(3*32+ 5) /* "" Athlon */
 +#define X86_FEATURE_P3		(3*32+ 6) /* "" P3 */
 +#define X86_FEATURE_P4		(3*32+ 7) /* "" P4 */
 +#define X86_FEATURE_CONSTANT_TSC (3*32+ 8) /* TSC ticks at a constant rate */
 +#define X86_FEATURE_UP		(3*32+ 9) /* smp kernel running on up */
 +#define X86_FEATURE_FXSAVE_LEAK (3*32+10) /* "" FXSAVE leaks FOP/FIP/FOP */
 +#define X86_FEATURE_ARCH_PERFMON (3*32+11) /* Intel Architectural PerfMon */
 +#define X86_FEATURE_PEBS	(3*32+12) /* Precise-Event Based Sampling */
 +#define X86_FEATURE_BTS		(3*32+13) /* Branch Trace Store */
 +#define X86_FEATURE_SYSCALL32	(3*32+14) /* "" syscall in ia32 userspace */
 +#define X86_FEATURE_SYSENTER32	(3*32+15) /* "" sysenter in ia32 userspace */
 +#define X86_FEATURE_REP_GOOD	(3*32+16) /* rep microcode works well */
 +#define X86_FEATURE_MFENCE_RDTSC (3*32+17) /* "" Mfence synchronizes RDTSC */
 +#define X86_FEATURE_LFENCE_RDTSC (3*32+18) /* "" Lfence synchronizes RDTSC */
 +#define X86_FEATURE_11AP	(3*32+19) /* "" Bad local APIC aka 11AP */
 +#define X86_FEATURE_NOPL	(3*32+20) /* The NOPL (0F 1F) instructions */
 +					  /* 21 available, was AMD_C1E */
 +#define X86_FEATURE_XTOPOLOGY	(3*32+22) /* cpu topology enum extensions */
 +#define X86_FEATURE_TSC_RELIABLE (3*32+23) /* TSC is known to be reliable */
 +#define X86_FEATURE_NONSTOP_TSC	(3*32+24) /* TSC does not stop in C states */
 +#define X86_FEATURE_CLFLUSH_MONITOR (3*32+25) /* "" clflush reqd with monitor */
 +#define X86_FEATURE_EXTD_APICID	(3*32+26) /* has extended APICID (8 bits) */
 +#define X86_FEATURE_AMD_DCM     (3*32+27) /* multi-node processor */
 +#define X86_FEATURE_APERFMPERF	(3*32+28) /* APERFMPERF */
 +#define X86_FEATURE_EAGER_FPU	(3*32+29) /* "eagerfpu" Non lazy FPU restore */
 +#define X86_FEATURE_NONSTOP_TSC_S3 (3*32+30) /* TSC doesn't stop in S3 state */
++=======
+ #define X86_FEATURE_K8		( 3*32+ 4) /* "" Opteron, Athlon64 */
+ #define X86_FEATURE_K7		( 3*32+ 5) /* "" Athlon */
+ #define X86_FEATURE_P3		( 3*32+ 6) /* "" P3 */
+ #define X86_FEATURE_P4		( 3*32+ 7) /* "" P4 */
+ #define X86_FEATURE_CONSTANT_TSC ( 3*32+ 8) /* TSC ticks at a constant rate */
+ #define X86_FEATURE_UP		( 3*32+ 9) /* smp kernel running on up */
+ #define X86_FEATURE_ART		(3*32+10) /* Platform has always running timer (ART) */
+ #define X86_FEATURE_ARCH_PERFMON ( 3*32+11) /* Intel Architectural PerfMon */
+ #define X86_FEATURE_PEBS	( 3*32+12) /* Precise-Event Based Sampling */
+ #define X86_FEATURE_BTS		( 3*32+13) /* Branch Trace Store */
+ #define X86_FEATURE_SYSCALL32	( 3*32+14) /* "" syscall in ia32 userspace */
+ #define X86_FEATURE_SYSENTER32	( 3*32+15) /* "" sysenter in ia32 userspace */
+ #define X86_FEATURE_REP_GOOD	( 3*32+16) /* rep microcode works well */
+ #define X86_FEATURE_MFENCE_RDTSC ( 3*32+17) /* "" Mfence synchronizes RDTSC */
+ #define X86_FEATURE_LFENCE_RDTSC ( 3*32+18) /* "" Lfence synchronizes RDTSC */
+ /* free, was #define X86_FEATURE_11AP	( 3*32+19) * "" Bad local APIC aka 11AP */
+ #define X86_FEATURE_NOPL	( 3*32+20) /* The NOPL (0F 1F) instructions */
+ #define X86_FEATURE_ALWAYS	( 3*32+21) /* "" Always-present feature */
+ #define X86_FEATURE_XTOPOLOGY	( 3*32+22) /* cpu topology enum extensions */
+ #define X86_FEATURE_TSC_RELIABLE ( 3*32+23) /* TSC is known to be reliable */
+ #define X86_FEATURE_NONSTOP_TSC	( 3*32+24) /* TSC does not stop in C states */
+ /* free, was #define X86_FEATURE_CLFLUSH_MONITOR ( 3*32+25) * "" clflush reqd with monitor */
+ #define X86_FEATURE_EXTD_APICID	( 3*32+26) /* has extended APICID (8 bits) */
+ #define X86_FEATURE_AMD_DCM     ( 3*32+27) /* multi-node processor */
+ #define X86_FEATURE_APERFMPERF	( 3*32+28) /* APERFMPERF */
+ #define X86_FEATURE_EAGER_FPU	( 3*32+29) /* "eagerfpu" Non lazy FPU restore */
+ #define X86_FEATURE_NONSTOP_TSC_S3 ( 3*32+30) /* TSC doesn't stop in S3 state */
++>>>>>>> f9677e0f8308 (x86/tsc: Always Running Timer (ART) correlated clocksource)
  
  /* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */
 -#define X86_FEATURE_XMM3	( 4*32+ 0) /* "pni" SSE-3 */
 -#define X86_FEATURE_PCLMULQDQ	( 4*32+ 1) /* PCLMULQDQ instruction */
 -#define X86_FEATURE_DTES64	( 4*32+ 2) /* 64-bit Debug Store */
 -#define X86_FEATURE_MWAIT	( 4*32+ 3) /* "monitor" Monitor/Mwait support */
 -#define X86_FEATURE_DSCPL	( 4*32+ 4) /* "ds_cpl" CPL Qual. Debug Store */
 -#define X86_FEATURE_VMX		( 4*32+ 5) /* Hardware virtualization */
 -#define X86_FEATURE_SMX		( 4*32+ 6) /* Safer mode */
 -#define X86_FEATURE_EST		( 4*32+ 7) /* Enhanced SpeedStep */
 -#define X86_FEATURE_TM2		( 4*32+ 8) /* Thermal Monitor 2 */
 -#define X86_FEATURE_SSSE3	( 4*32+ 9) /* Supplemental SSE-3 */
 -#define X86_FEATURE_CID		( 4*32+10) /* Context ID */
 -#define X86_FEATURE_SDBG	( 4*32+11) /* Silicon Debug */
 -#define X86_FEATURE_FMA		( 4*32+12) /* Fused multiply-add */
 -#define X86_FEATURE_CX16	( 4*32+13) /* CMPXCHG16B */
 -#define X86_FEATURE_XTPR	( 4*32+14) /* Send Task Priority Messages */
 -#define X86_FEATURE_PDCM	( 4*32+15) /* Performance Capabilities */
 -#define X86_FEATURE_PCID	( 4*32+17) /* Process Context Identifiers */
 -#define X86_FEATURE_DCA		( 4*32+18) /* Direct Cache Access */
 -#define X86_FEATURE_XMM4_1	( 4*32+19) /* "sse4_1" SSE-4.1 */
 -#define X86_FEATURE_XMM4_2	( 4*32+20) /* "sse4_2" SSE-4.2 */
 -#define X86_FEATURE_X2APIC	( 4*32+21) /* x2APIC */
 -#define X86_FEATURE_MOVBE	( 4*32+22) /* MOVBE instruction */
 -#define X86_FEATURE_POPCNT      ( 4*32+23) /* POPCNT instruction */
 -#define X86_FEATURE_TSC_DEADLINE_TIMER	( 4*32+24) /* Tsc deadline timer */
 -#define X86_FEATURE_AES		( 4*32+25) /* AES instructions */
 -#define X86_FEATURE_XSAVE	( 4*32+26) /* XSAVE/XRSTOR/XSETBV/XGETBV */
 -#define X86_FEATURE_OSXSAVE	( 4*32+27) /* "" XSAVE enabled in the OS */
 -#define X86_FEATURE_AVX		( 4*32+28) /* Advanced Vector Extensions */
 -#define X86_FEATURE_F16C	( 4*32+29) /* 16-bit fp conversions */
 -#define X86_FEATURE_RDRAND	( 4*32+30) /* The RDRAND instruction */
 -#define X86_FEATURE_HYPERVISOR	( 4*32+31) /* Running on a hypervisor */
 +#define X86_FEATURE_XMM3	(4*32+ 0) /* "pni" SSE-3 */
 +#define X86_FEATURE_PCLMULQDQ	(4*32+ 1) /* PCLMULQDQ instruction */
 +#define X86_FEATURE_DTES64	(4*32+ 2) /* 64-bit Debug Store */
 +#define X86_FEATURE_MWAIT	(4*32+ 3) /* "monitor" Monitor/Mwait support */
 +#define X86_FEATURE_DSCPL	(4*32+ 4) /* "ds_cpl" CPL Qual. Debug Store */
 +#define X86_FEATURE_VMX		(4*32+ 5) /* Hardware virtualization */
 +#define X86_FEATURE_SMX		(4*32+ 6) /* Safer mode */
 +#define X86_FEATURE_EST		(4*32+ 7) /* Enhanced SpeedStep */
 +#define X86_FEATURE_TM2		(4*32+ 8) /* Thermal Monitor 2 */
 +#define X86_FEATURE_SSSE3	(4*32+ 9) /* Supplemental SSE-3 */
 +#define X86_FEATURE_CID		(4*32+10) /* Context ID */
 +#define X86_FEATURE_FMA		(4*32+12) /* Fused multiply-add */
 +#define X86_FEATURE_CX16	(4*32+13) /* CMPXCHG16B */
 +#define X86_FEATURE_XTPR	(4*32+14) /* Send Task Priority Messages */
 +#define X86_FEATURE_PDCM	(4*32+15) /* Performance Capabilities */
 +#define X86_FEATURE_PCID	(4*32+17) /* Process Context Identifiers */
 +#define X86_FEATURE_DCA		(4*32+18) /* Direct Cache Access */
 +#define X86_FEATURE_XMM4_1	(4*32+19) /* "sse4_1" SSE-4.1 */
 +#define X86_FEATURE_XMM4_2	(4*32+20) /* "sse4_2" SSE-4.2 */
 +#define X86_FEATURE_X2APIC	(4*32+21) /* x2APIC */
 +#define X86_FEATURE_MOVBE	(4*32+22) /* MOVBE instruction */
 +#define X86_FEATURE_POPCNT      (4*32+23) /* POPCNT instruction */
 +#define X86_FEATURE_TSC_DEADLINE_TIMER	(4*32+24) /* Tsc deadline timer */
 +#define X86_FEATURE_AES		(4*32+25) /* AES instructions */
 +#define X86_FEATURE_XSAVE	(4*32+26) /* XSAVE/XRSTOR/XSETBV/XGETBV */
 +#define X86_FEATURE_OSXSAVE	(4*32+27) /* "" XSAVE enabled in the OS */
 +#define X86_FEATURE_AVX		(4*32+28) /* Advanced Vector Extensions */
 +#define X86_FEATURE_F16C	(4*32+29) /* 16-bit fp conversions */
 +#define X86_FEATURE_RDRAND	(4*32+30) /* The RDRAND instruction */
 +#define X86_FEATURE_HYPERVISOR	(4*32+31) /* Running on a hypervisor */
  
  /* VIA/Cyrix/Centaur-defined CPU features, CPUID level 0xC0000001, word 5 */
 -#define X86_FEATURE_XSTORE	( 5*32+ 2) /* "rng" RNG present (xstore) */
 -#define X86_FEATURE_XSTORE_EN	( 5*32+ 3) /* "rng_en" RNG enabled */
 -#define X86_FEATURE_XCRYPT	( 5*32+ 6) /* "ace" on-CPU crypto (xcrypt) */
 -#define X86_FEATURE_XCRYPT_EN	( 5*32+ 7) /* "ace_en" on-CPU crypto enabled */
 -#define X86_FEATURE_ACE2	( 5*32+ 8) /* Advanced Cryptography Engine v2 */
 -#define X86_FEATURE_ACE2_EN	( 5*32+ 9) /* ACE v2 enabled */
 -#define X86_FEATURE_PHE		( 5*32+10) /* PadLock Hash Engine */
 -#define X86_FEATURE_PHE_EN	( 5*32+11) /* PHE enabled */
 -#define X86_FEATURE_PMM		( 5*32+12) /* PadLock Montgomery Multiplier */
 -#define X86_FEATURE_PMM_EN	( 5*32+13) /* PMM enabled */
 +#define X86_FEATURE_XSTORE	(5*32+ 2) /* "rng" RNG present (xstore) */
 +#define X86_FEATURE_XSTORE_EN	(5*32+ 3) /* "rng_en" RNG enabled */
 +#define X86_FEATURE_XCRYPT	(5*32+ 6) /* "ace" on-CPU crypto (xcrypt) */
 +#define X86_FEATURE_XCRYPT_EN	(5*32+ 7) /* "ace_en" on-CPU crypto enabled */
 +#define X86_FEATURE_ACE2	(5*32+ 8) /* Advanced Cryptography Engine v2 */
 +#define X86_FEATURE_ACE2_EN	(5*32+ 9) /* ACE v2 enabled */
 +#define X86_FEATURE_PHE		(5*32+10) /* PadLock Hash Engine */
 +#define X86_FEATURE_PHE_EN	(5*32+11) /* PHE enabled */
 +#define X86_FEATURE_PMM		(5*32+12) /* PadLock Montgomery Multiplier */
 +#define X86_FEATURE_PMM_EN	(5*32+13) /* PMM enabled */
  
  /* More extended AMD flags: CPUID level 0x80000001, ecx, word 6 */
 -#define X86_FEATURE_LAHF_LM	( 6*32+ 0) /* LAHF/SAHF in long mode */
 -#define X86_FEATURE_CMP_LEGACY	( 6*32+ 1) /* If yes HyperThreading not valid */
 -#define X86_FEATURE_SVM		( 6*32+ 2) /* Secure virtual machine */
 -#define X86_FEATURE_EXTAPIC	( 6*32+ 3) /* Extended APIC space */
 -#define X86_FEATURE_CR8_LEGACY	( 6*32+ 4) /* CR8 in 32-bit mode */
 -#define X86_FEATURE_ABM		( 6*32+ 5) /* Advanced bit manipulation */
 -#define X86_FEATURE_SSE4A	( 6*32+ 6) /* SSE-4A */
 -#define X86_FEATURE_MISALIGNSSE ( 6*32+ 7) /* Misaligned SSE mode */
 -#define X86_FEATURE_3DNOWPREFETCH ( 6*32+ 8) /* 3DNow prefetch instructions */
 -#define X86_FEATURE_OSVW	( 6*32+ 9) /* OS Visible Workaround */
 -#define X86_FEATURE_IBS		( 6*32+10) /* Instruction Based Sampling */
 -#define X86_FEATURE_XOP		( 6*32+11) /* extended AVX instructions */
 -#define X86_FEATURE_SKINIT	( 6*32+12) /* SKINIT/STGI instructions */
 -#define X86_FEATURE_WDT		( 6*32+13) /* Watchdog timer */
 -#define X86_FEATURE_LWP		( 6*32+15) /* Light Weight Profiling */
 -#define X86_FEATURE_FMA4	( 6*32+16) /* 4 operands MAC instructions */
 -#define X86_FEATURE_TCE		( 6*32+17) /* translation cache extension */
 -#define X86_FEATURE_NODEID_MSR	( 6*32+19) /* NodeId MSR */
 -#define X86_FEATURE_TBM		( 6*32+21) /* trailing bit manipulations */
 -#define X86_FEATURE_TOPOEXT	( 6*32+22) /* topology extensions CPUID leafs */
 -#define X86_FEATURE_PERFCTR_CORE ( 6*32+23) /* core performance counter extensions */
 -#define X86_FEATURE_PERFCTR_NB  ( 6*32+24) /* NB performance counter extensions */
 -#define X86_FEATURE_BPEXT	(6*32+26) /* data breakpoint extension */
 -#define X86_FEATURE_PERFCTR_L2	( 6*32+28) /* L2 performance counter extensions */
 -#define X86_FEATURE_MWAITX	( 6*32+29) /* MWAIT extension (MONITORX/MWAITX) */
 +#define X86_FEATURE_LAHF_LM	(6*32+ 0) /* LAHF/SAHF in long mode */
 +#define X86_FEATURE_CMP_LEGACY	(6*32+ 1) /* If yes HyperThreading not valid */
 +#define X86_FEATURE_SVM		(6*32+ 2) /* Secure virtual machine */
 +#define X86_FEATURE_EXTAPIC	(6*32+ 3) /* Extended APIC space */
 +#define X86_FEATURE_CR8_LEGACY	(6*32+ 4) /* CR8 in 32-bit mode */
 +#define X86_FEATURE_ABM		(6*32+ 5) /* Advanced bit manipulation */
 +#define X86_FEATURE_SSE4A	(6*32+ 6) /* SSE-4A */
 +#define X86_FEATURE_MISALIGNSSE (6*32+ 7) /* Misaligned SSE mode */
 +#define X86_FEATURE_3DNOWPREFETCH (6*32+ 8) /* 3DNow prefetch instructions */
 +#define X86_FEATURE_OSVW	(6*32+ 9) /* OS Visible Workaround */
 +#define X86_FEATURE_IBS		(6*32+10) /* Instruction Based Sampling */
 +#define X86_FEATURE_XOP		(6*32+11) /* extended AVX instructions */
 +#define X86_FEATURE_SKINIT	(6*32+12) /* SKINIT/STGI instructions */
 +#define X86_FEATURE_WDT		(6*32+13) /* Watchdog timer */
 +#define X86_FEATURE_LWP		(6*32+15) /* Light Weight Profiling */
 +#define X86_FEATURE_FMA4	(6*32+16) /* 4 operands MAC instructions */
 +#define X86_FEATURE_TCE		(6*32+17) /* translation cache extension */
 +#define X86_FEATURE_NODEID_MSR	(6*32+19) /* NodeId MSR */
 +#define X86_FEATURE_TBM		(6*32+21) /* trailing bit manipulations */
 +#define X86_FEATURE_TOPOEXT	(6*32+22) /* topology extensions CPUID leafs */
 +#define X86_FEATURE_PERFCTR_CORE (6*32+23) /* core performance counter extensions */
 +#define X86_FEATURE_PERFCTR_NB  (6*32+24) /* NB performance counter extensions */
 +#define X86_FEATURE_PERFCTR_L2	(6*32+28) /* L2 performance counter extensions */
  
  /*
   * Auxiliary flags: Linux defined - For features scattered in various
diff --cc arch/x86/kernel/tsc.c
index 7605aa369d68,80d761e420c5..000000000000
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@@ -38,9 -39,251 +38,254 @@@ static int __read_mostly tsc_unstable
     erroneous rdtsc usage on !cpu_has_tsc processors */
  static int __read_mostly tsc_disabled = -1;
  
 -static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 +static struct static_key __use_tsc = STATIC_KEY_INIT;
  
  int tsc_clocksource_reliable;
++<<<<<<< HEAD
++=======
+ 
+ static u32 art_to_tsc_numerator;
+ static u32 art_to_tsc_denominator;
+ static u64 art_to_tsc_offset;
+ struct clocksource *art_related_clocksource;
+ 
+ /*
+  * Use a ring-buffer like data structure, where a writer advances the head by
+  * writing a new data entry and a reader advances the tail when it observes a
+  * new entry.
+  *
+  * Writers are made to wait on readers until there's space to write a new
+  * entry.
+  *
+  * This means that we can always use an {offset, mul} pair to compute a ns
+  * value that is 'roughly' in the right direction, even if we're writing a new
+  * {offset, mul} pair during the clock read.
+  *
+  * The down-side is that we can no longer guarantee strict monotonicity anymore
+  * (assuming the TSC was that to begin with), because while we compute the
+  * intersection point of the two clock slopes and make sure the time is
+  * continuous at the point of switching; we can no longer guarantee a reader is
+  * strictly before or after the switch point.
+  *
+  * It does mean a reader no longer needs to disable IRQs in order to avoid
+  * CPU-Freq updates messing with his times, and similarly an NMI reader will
+  * no longer run the risk of hitting half-written state.
+  */
+ 
+ struct cyc2ns {
+ 	struct cyc2ns_data data[2];	/*  0 + 2*24 = 48 */
+ 	struct cyc2ns_data *head;	/* 48 + 8    = 56 */
+ 	struct cyc2ns_data *tail;	/* 56 + 8    = 64 */
+ }; /* exactly fits one cacheline */
+ 
+ static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
+ 
+ struct cyc2ns_data *cyc2ns_read_begin(void)
+ {
+ 	struct cyc2ns_data *head;
+ 
+ 	preempt_disable();
+ 
+ 	head = this_cpu_read(cyc2ns.head);
+ 	/*
+ 	 * Ensure we observe the entry when we observe the pointer to it.
+ 	 * matches the wmb from cyc2ns_write_end().
+ 	 */
+ 	smp_read_barrier_depends();
+ 	head->__count++;
+ 	barrier();
+ 
+ 	return head;
+ }
+ 
+ void cyc2ns_read_end(struct cyc2ns_data *head)
+ {
+ 	barrier();
+ 	/*
+ 	 * If we're the outer most nested read; update the tail pointer
+ 	 * when we're done. This notifies possible pending writers
+ 	 * that we've observed the head pointer and that the other
+ 	 * entry is now free.
+ 	 */
+ 	if (!--head->__count) {
+ 		/*
+ 		 * x86-TSO does not reorder writes with older reads;
+ 		 * therefore once this write becomes visible to another
+ 		 * cpu, we must be finished reading the cyc2ns_data.
+ 		 *
+ 		 * matches with cyc2ns_write_begin().
+ 		 */
+ 		this_cpu_write(cyc2ns.tail, head);
+ 	}
+ 	preempt_enable();
+ }
+ 
+ /*
+  * Begin writing a new @data entry for @cpu.
+  *
+  * Assumes some sort of write side lock; currently 'provided' by the assumption
+  * that cpufreq will call its notifiers sequentially.
+  */
+ static struct cyc2ns_data *cyc2ns_write_begin(int cpu)
+ {
+ 	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+ 	struct cyc2ns_data *data = c2n->data;
+ 
+ 	if (data == c2n->head)
+ 		data++;
+ 
+ 	/* XXX send an IPI to @cpu in order to guarantee a read? */
+ 
+ 	/*
+ 	 * When we observe the tail write from cyc2ns_read_end(),
+ 	 * the cpu must be done with that entry and its safe
+ 	 * to start writing to it.
+ 	 */
+ 	while (c2n->tail == data)
+ 		cpu_relax();
+ 
+ 	return data;
+ }
+ 
+ static void cyc2ns_write_end(int cpu, struct cyc2ns_data *data)
+ {
+ 	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+ 
+ 	/*
+ 	 * Ensure the @data writes are visible before we publish the
+ 	 * entry. Matches the data-depencency in cyc2ns_read_begin().
+ 	 */
+ 	smp_wmb();
+ 
+ 	ACCESS_ONCE(c2n->head) = data;
+ }
+ 
+ /*
+  * Accelerators for sched_clock()
+  * convert from cycles(64bits) => nanoseconds (64bits)
+  *  basic equation:
+  *              ns = cycles / (freq / ns_per_sec)
+  *              ns = cycles * (ns_per_sec / freq)
+  *              ns = cycles * (10^9 / (cpu_khz * 10^3))
+  *              ns = cycles * (10^6 / cpu_khz)
+  *
+  *      Then we use scaling math (suggested by george@mvista.com) to get:
+  *              ns = cycles * (10^6 * SC / cpu_khz) / SC
+  *              ns = cycles * cyc2ns_scale / SC
+  *
+  *      And since SC is a constant power of two, we can convert the div
+  *  into a shift. The larger SC is, the more accurate the conversion, but
+  *  cyc2ns_scale needs to be a 32-bit value so that 32-bit multiplication
+  *  (64-bit result) can be used.
+  *
+  *  We can use khz divisor instead of mhz to keep a better precision.
+  *  (mathieu.desnoyers@polymtl.ca)
+  *
+  *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
+  */
+ 
+ static void cyc2ns_data_init(struct cyc2ns_data *data)
+ {
+ 	data->cyc2ns_mul = 0;
+ 	data->cyc2ns_shift = 0;
+ 	data->cyc2ns_offset = 0;
+ 	data->__count = 0;
+ }
+ 
+ static void cyc2ns_init(int cpu)
+ {
+ 	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+ 
+ 	cyc2ns_data_init(&c2n->data[0]);
+ 	cyc2ns_data_init(&c2n->data[1]);
+ 
+ 	c2n->head = c2n->data;
+ 	c2n->tail = c2n->data;
+ }
+ 
+ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
+ {
+ 	struct cyc2ns_data *data, *tail;
+ 	unsigned long long ns;
+ 
+ 	/*
+ 	 * See cyc2ns_read_*() for details; replicated in order to avoid
+ 	 * an extra few instructions that came with the abstraction.
+ 	 * Notable, it allows us to only do the __count and tail update
+ 	 * dance when its actually needed.
+ 	 */
+ 
+ 	preempt_disable_notrace();
+ 	data = this_cpu_read(cyc2ns.head);
+ 	tail = this_cpu_read(cyc2ns.tail);
+ 
+ 	if (likely(data == tail)) {
+ 		ns = data->cyc2ns_offset;
+ 		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, data->cyc2ns_shift);
+ 	} else {
+ 		data->__count++;
+ 
+ 		barrier();
+ 
+ 		ns = data->cyc2ns_offset;
+ 		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, data->cyc2ns_shift);
+ 
+ 		barrier();
+ 
+ 		if (!--data->__count)
+ 			this_cpu_write(cyc2ns.tail, data);
+ 	}
+ 	preempt_enable_notrace();
+ 
+ 	return ns;
+ }
+ 
+ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
+ {
+ 	unsigned long long tsc_now, ns_now;
+ 	struct cyc2ns_data *data;
+ 	unsigned long flags;
+ 
+ 	local_irq_save(flags);
+ 	sched_clock_idle_sleep_event();
+ 
+ 	if (!cpu_khz)
+ 		goto done;
+ 
+ 	data = cyc2ns_write_begin(cpu);
+ 
+ 	tsc_now = rdtsc();
+ 	ns_now = cycles_2_ns(tsc_now);
+ 
+ 	/*
+ 	 * Compute a new multiplier as per the above comment and ensure our
+ 	 * time function is continuous; see the comment near struct
+ 	 * cyc2ns_data.
+ 	 */
+ 	clocks_calc_mult_shift(&data->cyc2ns_mul, &data->cyc2ns_shift, cpu_khz,
+ 			       NSEC_PER_MSEC, 0);
+ 
+ 	/*
+ 	 * cyc2ns_shift is exported via arch_perf_update_userpage() where it is
+ 	 * not expected to be greater than 31 due to the original published
+ 	 * conversion algorithm shifting a 32-bit value (now specifies a 64-bit
+ 	 * value) - refer perf_event_mmap_page documentation in perf_event.h.
+ 	 */
+ 	if (data->cyc2ns_shift == 32) {
+ 		data->cyc2ns_shift = 31;
+ 		data->cyc2ns_mul >>= 1;
+ 	}
+ 
+ 	data->cyc2ns_offset = ns_now -
+ 		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, data->cyc2ns_shift);
+ 
+ 	cyc2ns_write_end(cpu, data);
+ 
+ done:
+ 	sched_clock_idle_wakeup_event(0);
+ 	local_irq_restore(flags);
+ }
++>>>>>>> f9677e0f8308 (x86/tsc: Always Running Timer (ART) correlated clocksource)
  /*
   * Scheduler clock - returns current time in nanosec units.
   */
* Unmerged path arch/x86/include/asm/cpufeature.h
diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 772b677f6617..a3a68a5c85ee 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -45,6 +45,8 @@ static __always_inline cycles_t vget_cycles(void)
 	return (cycles_t)__native_read_tsc();
 }
 
+extern struct system_counterval_t convert_art_to_tsc(cycle_t art);
+
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
* Unmerged path arch/x86/kernel/tsc.c
