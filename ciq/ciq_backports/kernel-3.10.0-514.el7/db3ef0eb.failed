IB/qib: Use rdmavt version of post_send

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit db3ef0eb84947e341b923c435ace2520d097d014
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/db3ef0eb.failed

This patch removes the post_send and post_one_send from the qib driver.
The "posting" of sends will be done by rdmavt which will walk a WQE and
queue work. This patch will still provide the capability to schedule that
work as well as kick the progress. These are provided to the rdmavt layer.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit db3ef0eb84947e341b923c435ace2520d097d014)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/qib_driver.c
#	drivers/infiniband/hw/qib/qib_qp.c
#	drivers/infiniband/hw/qib/qib_rc.c
#	drivers/infiniband/hw/qib/qib_ruc.c
#	drivers/infiniband/hw/qib/qib_ud.c
#	drivers/infiniband/hw/qib/qib_verbs.c
#	drivers/infiniband/hw/qib/qib_verbs.h
#	drivers/infiniband/hw/qib/qib_verbs_mcast.c
diff --cc drivers/infiniband/hw/qib/qib_driver.c
index 323352f4bb60,a11de8e44e8b..000000000000
--- a/drivers/infiniband/hw/qib/qib_driver.c
+++ b/drivers/infiniband/hw/qib/qib_driver.c
@@@ -377,9 -377,9 +377,15 @@@ static u32 qib_rcv_hdrerr(struct qib_ct
  			spin_lock(&qp->r_lock);
  
  			/* Check for valid receive state. */
++<<<<<<< HEAD
 +			if (!(ib_qib_state_ops[qp->state] &
 +			      QIB_PROCESS_RECV_OK)) {
 +				ibp->n_pkt_drops++;
++=======
+ 			if (!(ib_rvt_state_ops[qp->state] &
+ 			      RVT_PROCESS_RECV_OK)) {
+ 				ibp->rvp.n_pkt_drops++;
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  				goto unlock;
  			}
  
@@@ -583,17 -583,17 +589,17 @@@ move_along
  	 */
  	list_for_each_entry_safe(qp, nqp, &rcd->qp_wait_list, rspwait) {
  		list_del_init(&qp->rspwait);
 -		if (qp->r_flags & RVT_R_RSP_NAK) {
 -			qp->r_flags &= ~RVT_R_RSP_NAK;
 +		if (qp->r_flags & QIB_R_RSP_NAK) {
 +			qp->r_flags &= ~QIB_R_RSP_NAK;
  			qib_send_rc_ack(qp);
  		}
 -		if (qp->r_flags & RVT_R_RSP_SEND) {
 +		if (qp->r_flags & QIB_R_RSP_SEND) {
  			unsigned long flags;
  
 -			qp->r_flags &= ~RVT_R_RSP_SEND;
 +			qp->r_flags &= ~QIB_R_RSP_SEND;
  			spin_lock_irqsave(&qp->s_lock, flags);
- 			if (ib_qib_state_ops[qp->state] &
- 					QIB_PROCESS_OR_FLUSH_SEND)
+ 			if (ib_rvt_state_ops[qp->state] &
+ 					RVT_PROCESS_OR_FLUSH_SEND)
  				qib_schedule_send(qp);
  			spin_unlock_irqrestore(&qp->s_lock, flags);
  		}
diff --cc drivers/infiniband/hw/qib/qib_qp.c
index cf1dd6e9d434,65b752c52190..000000000000
--- a/drivers/infiniband/hw/qib/qib_qp.c
+++ b/drivers/infiniband/hw/qib/qib_qp.c
@@@ -428,7 -375,7 +428,11 @@@ static void clear_mr_refs(struct qib_q
  
  	if (clr_sends) {
  		while (qp->s_last != qp->s_head) {
++<<<<<<< HEAD
 +			struct qib_swqe *wqe = get_swqe_ptr(qp, qp->s_last);
++=======
+ 			struct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, qp->s_last);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  			unsigned i;
  
  			for (i = 0; i < wqe->wr.num_sge; i++) {
@@@ -573,7 -521,7 +577,11 @@@ int qib_modify_qp(struct ib_qp *ibqp, s
  		  int attr_mask, struct ib_udata *udata)
  {
  	struct qib_ibdev *dev = to_idev(ibqp->device);
++<<<<<<< HEAD
 +	struct qib_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	struct qib_qp_priv *priv = qp->priv;
  	enum ib_qp_state cur_state, new_state;
  	struct ib_event ev;
@@@ -860,7 -809,7 +868,11 @@@ bail
  int qib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
  		 int attr_mask, struct ib_qp_init_attr *init_attr)
  {
++<<<<<<< HEAD
 +	struct qib_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  
  	attr->qp_state = qp->state;
  	attr->cur_qp_state = attr->qp_state;
@@@ -967,246 -916,33 +979,252 @@@ __be32 qib_compute_aeth(struct qib_qp *
  	return cpu_to_be32(aeth);
  }
  
 -void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp, gfp_t gfp)
 +/**
 + * qib_create_qp - create a queue pair for a device
 + * @ibpd: the protection domain who's device we create the queue pair for
 + * @init_attr: the attributes of the queue pair
 + * @udata: user data for libibverbs.so
 + *
 + * Returns the queue pair on success, otherwise returns an errno.
 + *
 + * Called by the ib_create_qp() core verbs function.
 + */
 +struct ib_qp *qib_create_qp(struct ib_pd *ibpd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata)
  {
 +	struct qib_qp *qp;
 +	int err;
 +	struct qib_swqe *swq = NULL;
 +	struct qib_ibdev *dev;
 +	struct qib_devdata *dd;
 +	size_t sz;
 +	size_t sg_list_sz;
 +	struct ib_qp *ret;
 +	gfp_t gfp;
  	struct qib_qp_priv *priv;
  
 -	priv = kzalloc(sizeof(*priv), gfp);
 -	if (!priv)
 -		return ERR_PTR(-ENOMEM);
 -	priv->owner = qp;
 -
 -	priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 -	if (!priv->s_hdr) {
 -		kfree(priv);
 -		return ERR_PTR(-ENOMEM);
 +	if (init_attr->cap.max_send_sge > ib_qib_max_sges ||
 +	    init_attr->cap.max_send_wr > ib_qib_max_qp_wrs ||
 +	    init_attr->create_flags & ~(IB_QP_CREATE_USE_GFP_NOIO))
 +		return ERR_PTR(-EINVAL);
 +
 +	/* GFP_NOIO is applicable in RC QPs only */
 +	if (init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO &&
 +	    init_attr->qp_type != IB_QPT_RC)
 +		return ERR_PTR(-EINVAL);
 +
 +	gfp = init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO ?
 +			GFP_NOIO : GFP_KERNEL;
 +
 +	/* Check receive queue parameters if no SRQ is specified. */
 +	if (!init_attr->srq) {
 +		if (init_attr->cap.max_recv_sge > ib_qib_max_sges ||
 +		    init_attr->cap.max_recv_wr > ib_qib_max_qp_wrs) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +		if (init_attr->cap.max_send_sge +
 +		    init_attr->cap.max_send_wr +
 +		    init_attr->cap.max_recv_sge +
 +		    init_attr->cap.max_recv_wr == 0) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
  	}
++<<<<<<< HEAD
++=======
+ 	init_waitqueue_head(&priv->wait_dma);
+ 	INIT_WORK(&priv->s_work, _qib_do_send);
+ 	INIT_LIST_HEAD(&priv->iowait);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
 +
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_SMI:
 +	case IB_QPT_GSI:
 +		if (init_attr->port_num == 0 ||
 +		    init_attr->port_num > ibpd->device->phys_port_cnt) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +	case IB_QPT_UC:
 +	case IB_QPT_RC:
 +	case IB_QPT_UD:
 +		sz = sizeof(struct qib_sge) *
 +			init_attr->cap.max_send_sge +
 +			sizeof(struct qib_swqe);
 +		swq = __vmalloc((init_attr->cap.max_send_wr + 1) * sz,
 +				gfp, PAGE_KERNEL);
 +		if (swq == NULL) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail;
 +		}
 +		sz = sizeof(*qp);
 +		sg_list_sz = 0;
 +		if (init_attr->srq) {
 +			struct qib_srq *srq = to_isrq(init_attr->srq);
 +
 +			if (srq->rq.max_sge > 1)
 +				sg_list_sz = sizeof(*qp->r_sg_list) *
 +					(srq->rq.max_sge - 1);
 +		} else if (init_attr->cap.max_recv_sge > 1)
 +			sg_list_sz = sizeof(*qp->r_sg_list) *
 +				(init_attr->cap.max_recv_sge - 1);
 +		qp = kzalloc(sz + sg_list_sz, gfp);
 +		if (!qp) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_swq;
 +		}
 +		RCU_INIT_POINTER(qp->next, NULL);
 +		priv = kzalloc(sizeof(*priv), gfp);
 +		if (!priv) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp_hdr;
 +		}
 +		priv->owner = qp;
 +		priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 +		if (!priv->s_hdr) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp;
 +		}
 +		qp->priv = priv;
 +		qp->timeout_jiffies =
 +			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
 +				1000UL);
 +		if (init_attr->srq)
 +			sz = 0;
 +		else {
 +			qp->r_rq.size = init_attr->cap.max_recv_wr + 1;
 +			qp->r_rq.max_sge = init_attr->cap.max_recv_sge;
 +			sz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +
 +				sizeof(struct qib_rwqe);
 +			if (gfp != GFP_NOIO)
 +				qp->r_rq.wq = vmalloc_user(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz);
 +			else
 +				qp->r_rq.wq = __vmalloc(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz,
 +						gfp, PAGE_KERNEL);
 +
 +			if (!qp->r_rq.wq) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_qp;
 +			}
 +		}
  
 -	return priv;
 -}
 +		/*
 +		 * ib_create_qp() will initialize qp->ibqp
 +		 * except for qp->ibqp.qp_num.
 +		 */
 +		spin_lock_init(&qp->r_lock);
 +		spin_lock_init(&qp->s_lock);
 +		spin_lock_init(&qp->r_rq.lock);
 +		atomic_set(&qp->refcount, 0);
 +		init_waitqueue_head(&qp->wait);
 +		init_waitqueue_head(&priv->wait_dma);
 +		init_timer(&qp->s_timer);
 +		qp->s_timer.data = (unsigned long)qp;
 +		INIT_WORK(&priv->s_work, qib_do_send);
 +		INIT_LIST_HEAD(&priv->iowait);
 +		INIT_LIST_HEAD(&qp->rspwait);
 +		qp->state = IB_QPS_RESET;
 +		qp->s_wq = swq;
 +		qp->s_size = init_attr->cap.max_send_wr + 1;
 +		qp->s_max_sge = init_attr->cap.max_send_sge;
 +		if (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)
 +			qp->s_flags = QIB_S_SIGNAL_REQ_WR;
 +		dev = to_idev(ibpd->device);
 +		dd = dd_from_dev(dev);
 +		err = alloc_qpn(dd, &dev->qpn_table, init_attr->qp_type,
 +				init_attr->port_num, gfp);
 +		if (err < 0) {
 +			ret = ERR_PTR(err);
 +			vfree(qp->r_rq.wq);
 +			goto bail_qp;
 +		}
 +		qp->ibqp.qp_num = err;
 +		qp->port_num = init_attr->port_num;
 +		qib_reset_qp(qp, init_attr->qp_type);
 +		break;
  
 -void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)
 -{
 -	struct qib_qp_priv *priv = qp->priv;
 +	default:
 +		/* Don't support raw QPs */
 +		ret = ERR_PTR(-ENOSYS);
 +		goto bail;
 +	}
 +
 +	init_attr->cap.max_inline_data = 0;
 +
 +	/*
 +	 * Return the address of the RWQ as the offset to mmap.
 +	 * See qib_mmap() for details.
 +	 */
 +	if (udata && udata->outlen >= sizeof(__u64)) {
 +		if (!qp->r_rq.wq) {
 +			__u64 offset = 0;
 +
 +			err = ib_copy_to_udata(udata, &offset,
 +					       sizeof(offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		} else {
 +			u32 s = sizeof(struct qib_rwq) + qp->r_rq.size * sz;
 +
 +			qp->ip = qib_create_mmap_info(dev, s,
 +						      ibpd->uobject->context,
 +						      qp->r_rq.wq);
 +			if (!qp->ip) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_ip;
 +			}
 +
 +			err = ib_copy_to_udata(udata, &(qp->ip->offset),
 +					       sizeof(qp->ip->offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		}
 +	}
 +
 +	spin_lock(&dev->n_qps_lock);
 +	if (dev->n_qps_allocated == ib_qib_max_qps) {
 +		spin_unlock(&dev->n_qps_lock);
 +		ret = ERR_PTR(-ENOMEM);
 +		goto bail_ip;
 +	}
 +
 +	dev->n_qps_allocated++;
 +	spin_unlock(&dev->n_qps_lock);
  
 +	if (qp->ip) {
 +		spin_lock_irq(&dev->pending_lock);
 +		list_add(&qp->ip->pending_mmaps, &dev->pending_mmaps);
 +		spin_unlock_irq(&dev->pending_lock);
 +	}
 +
 +	ret = &qp->ibqp;
 +	goto bail;
 +
 +bail_ip:
 +	if (qp->ip)
 +		kref_put(&qp->ip->ref, qib_release_mmap_info);
 +	else
 +		vfree(qp->r_rq.wq);
 +	free_qpn(&dev->qpn_table, qp->ibqp.qp_num);
 +bail_qp:
  	kfree(priv->s_hdr);
  	kfree(priv);
 +bail_qp_hdr:
 +	kfree(qp);
 +bail_swq:
 +	vfree(swq);
 +bail:
 +	return ret;
  }
  
  /**
@@@ -1220,7 -956,7 +1238,11 @@@
   */
  int qib_destroy_qp(struct ib_qp *ibqp)
  {
++<<<<<<< HEAD
 +	struct qib_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	struct qib_ibdev *dev = to_idev(ibqp->device);
  	struct qib_qp_priv *priv = qp->priv;
  
@@@ -1380,11 -1091,11 +1402,11 @@@ static const char * const qp_type_str[
  
  void qib_qp_iter_print(struct seq_file *s, struct qib_qp_iter *iter)
  {
 -	struct rvt_swqe *wqe;
 -	struct rvt_qp *qp = iter->qp;
 +	struct qib_swqe *wqe;
 +	struct qib_qp *qp = iter->qp;
  	struct qib_qp_priv *priv = qp->priv;
  
- 	wqe = get_swqe_ptr(qp, qp->s_last);
+ 	wqe = rvt_get_swqe_ptr(qp, qp->s_last);
  	seq_printf(s,
  		   "N %d QP%u %s %u %u %u f=%x %u %u %u %u %u PSN %x %x %x %x %x (%u %u %u %u %u %u) QP%u LID %x\n",
  		   iter->n,
diff --cc drivers/infiniband/hw/qib/qib_rc.c
index c23ede5294da,78ae93e1f812..000000000000
--- a/drivers/infiniband/hw/qib/qib_rc.c
+++ b/drivers/infiniband/hw/qib/qib_rc.c
@@@ -268,10 -268,10 +268,10 @@@ int qib_make_rc_req(struct qib_qp *qp
  			goto bail;
  		/* If DMAs are in progress, we can't flush immediately. */
  		if (atomic_read(&priv->s_dma_busy)) {
 -			qp->s_flags |= RVT_S_WAIT_DMA;
 +			qp->s_flags |= QIB_S_WAIT_DMA;
  			goto bail;
  		}
- 		wqe = get_swqe_ptr(qp, qp->s_last);
+ 		wqe = rvt_get_swqe_ptr(qp, qp->s_last);
  		qib_send_complete(qp, wqe, qp->s_last != qp->s_acked ?
  			IB_WC_SUCCESS : IB_WC_WR_FLUSH_ERR);
  		/* will get called again */
@@@ -757,9 -759,9 +757,15 @@@ void qib_send_rc_ack(struct qib_qp *qp
  	goto done;
  
  queue_ack:
++<<<<<<< HEAD
 +	if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {
 +		ibp->n_rc_qacks++;
 +		qp->s_flags |= QIB_S_ACK_PENDING | QIB_S_RESP_PENDING;
++=======
+ 	if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {
+ 		this_cpu_inc(*ibp->rvp.rc_qacks);
+ 		qp->s_flags |= RVT_S_ACK_PENDING | RVT_S_RESP_PENDING;
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  		qp->s_nak_state = qp->r_nak_state;
  		qp->s_ack_psn = qp->r_ack_psn;
  
@@@ -781,10 -783,10 +787,14 @@@ done
   * for the given QP.
   * Called at interrupt level with the QP s_lock held.
   */
 -static void reset_psn(struct rvt_qp *qp, u32 psn)
 +static void reset_psn(struct qib_qp *qp, u32 psn)
  {
  	u32 n = qp->s_acked;
++<<<<<<< HEAD
 +	struct qib_swqe *wqe = get_swqe_ptr(qp, n);
++=======
+ 	struct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, n);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	u32 opcode;
  
  	qp->s_cur = n;
@@@ -866,9 -868,9 +876,13 @@@ done
   * Back up requester to resend the last un-ACKed request.
   * The QP r_lock and s_lock should be held and interrupts disabled.
   */
 -static void qib_restart_rc(struct rvt_qp *qp, u32 psn, int wait)
 +static void qib_restart_rc(struct qib_qp *qp, u32 psn, int wait)
  {
++<<<<<<< HEAD
 +	struct qib_swqe *wqe = get_swqe_ptr(qp, qp->s_acked);
++=======
+ 	struct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, qp->s_acked);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	struct qib_ibport *ibp;
  
  	if (qp->s_retry == 0) {
@@@ -1001,8 -1003,8 +1015,13 @@@ void qib_rc_send_complete(struct qib_q
  	 * there are still requests that haven't been acked.
  	 */
  	if ((psn & IB_BTH_REQ_ACK) && qp->s_acked != qp->s_tail &&
++<<<<<<< HEAD
 +	    !(qp->s_flags & (QIB_S_TIMER | QIB_S_WAIT_RNR | QIB_S_WAIT_PSN)) &&
 +	    (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK))
++=======
+ 	    !(qp->s_flags & (RVT_S_TIMER | RVT_S_WAIT_RNR | RVT_S_WAIT_PSN)) &&
+ 	    (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK))
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  		start_timer(qp);
  
  	while (qp->s_last != qp->s_acked) {
diff --cc drivers/infiniband/hw/qib/qib_ruc.c
index e9132f7a68b0,4961a548d3c9..000000000000
--- a/drivers/infiniband/hw/qib/qib_ruc.c
+++ b/drivers/infiniband/hw/qib/qib_ruc.c
@@@ -376,11 -378,11 +376,16 @@@ static void qib_ruc_loopback(struct qib
  	spin_lock_irqsave(&sqp->s_lock, flags);
  
  	/* Return if we are already busy processing a work request. */
++<<<<<<< HEAD
 +	if ((sqp->s_flags & (QIB_S_BUSY | QIB_S_ANY_WAIT)) ||
 +	    !(ib_qib_state_ops[sqp->state] & QIB_PROCESS_OR_FLUSH_SEND))
++=======
+ 	if ((sqp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT)) ||
+ 	    !(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_OR_FLUSH_SEND))
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  		goto unlock;
  
 -	sqp->s_flags |= RVT_S_BUSY;
 +	sqp->s_flags |= QIB_S_BUSY;
  
  again:
  	if (sqp->s_last == sqp->s_head)
@@@ -407,9 -409,9 +412,9 @@@
  	}
  	spin_unlock_irqrestore(&sqp->s_lock, flags);
  
- 	if (!qp || !(ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) ||
+ 	if (!qp || !(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) ||
  	    qp->ibqp.qp_type != sqp->ibqp.qp_type) {
 -		ibp->rvp.n_pkt_drops++;
 +		ibp->n_pkt_drops++;
  		/*
  		 * For RC, the requester would timeout and retry so
  		 * shortcut the timeouts and just signal too many retries.
@@@ -588,9 -590,9 +593,9 @@@ rnr_nak
  	if (sqp->s_rnr_retry_cnt < 7)
  		sqp->s_rnr_retry--;
  	spin_lock_irqsave(&sqp->s_lock, flags);
- 	if (!(ib_qib_state_ops[sqp->state] & QIB_PROCESS_RECV_OK))
+ 	if (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_RECV_OK))
  		goto clr_busy;
 -	sqp->s_flags |= RVT_S_WAIT_RNR;
 +	sqp->s_flags |= QIB_S_WAIT_RNR;
  	sqp->s_timer.function = qib_rc_rnr_retry;
  	sqp->s_timer.expires = jiffies +
  		usecs_to_jiffies(ib_qib_rnr_table[qp->r_min_rnr_timer]);
@@@ -717,14 -728,12 +731,18 @@@ void _qib_do_send(struct work_struct *w
   * exhausted.  Only allow one CPU to send a packet per QP (tasklet).
   * Otherwise, two threads could send packets out of order.
   */
- void qib_do_send(struct work_struct *work)
+ void qib_do_send(struct rvt_qp *qp)
  {
++<<<<<<< HEAD
 +	struct qib_qp_priv *priv = container_of(work, struct qib_qp_priv,
 +						s_work);
 +	struct qib_qp *qp = priv->owner;
++=======
+ 	struct qib_qp_priv *priv = qp->priv;
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	struct qib_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);
  	struct qib_pportdata *ppd = ppd_from_ibp(ibp);
 -	int (*make_req)(struct rvt_qp *qp);
 +	int (*make_req)(struct qib_qp *qp);
  	unsigned long flags;
  
  	if ((qp->ibqp.qp_type == IB_QPT_RC ||
diff --cc drivers/infiniband/hw/qib/qib_ud.c
index 75faa5bd8dd6,f0ea0022ee4f..000000000000
--- a/drivers/infiniband/hw/qib/qib_ud.c
+++ b/drivers/infiniband/hw/qib/qib_ud.c
@@@ -71,8 -72,8 +71,13 @@@ static void qib_ud_loopback(struct qib_
  			IB_QPT_UD : qp->ibqp.qp_type;
  
  	if (dqptype != sqptype ||
++<<<<<<< HEAD
 +	    !(ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK)) {
 +		ibp->n_pkt_drops++;
++=======
+ 	    !(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)) {
+ 		ibp->rvp.n_pkt_drops++;
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  		goto drop;
  	}
  
@@@ -259,10 -260,10 +264,10 @@@ int qib_make_ud_req(struct qib_qp *qp
  			goto bail;
  		/* If DMAs are in progress, we can't flush immediately. */
  		if (atomic_read(&priv->s_dma_busy)) {
 -			qp->s_flags |= RVT_S_WAIT_DMA;
 +			qp->s_flags |= QIB_S_WAIT_DMA;
  			goto bail;
  		}
- 		wqe = get_swqe_ptr(qp, qp->s_last);
+ 		wqe = rvt_get_swqe_ptr(qp, qp->s_last);
  		qib_send_complete(qp, wqe, IB_WC_WR_FLUSH_ERR);
  		goto done;
  	}
diff --cc drivers/infiniband/hw/qib/qib_verbs.c
index c4417a1f33be,3766ea43c3ad..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.c
+++ b/drivers/infiniband/hw/qib/qib_verbs.c
@@@ -114,36 -114,6 +114,39 @@@ module_param_named(disable_sma, ib_qib_
  MODULE_PARM_DESC(disable_sma, "Disable the SMA");
  
  /*
++<<<<<<< HEAD
 + * Note that it is OK to post send work requests in the SQE and ERR
 + * states; qib_do_send() will process them and generate error
 + * completions as per IB 1.2 C10-96.
 + */
 +const int ib_qib_state_ops[IB_QPS_ERR + 1] = {
 +	[IB_QPS_RESET] = 0,
 +	[IB_QPS_INIT] = QIB_POST_RECV_OK,
 +	[IB_QPS_RTR] = QIB_POST_RECV_OK | QIB_PROCESS_RECV_OK,
 +	[IB_QPS_RTS] = QIB_POST_RECV_OK | QIB_PROCESS_RECV_OK |
 +	    QIB_POST_SEND_OK | QIB_PROCESS_SEND_OK |
 +	    QIB_PROCESS_NEXT_SEND_OK,
 +	[IB_QPS_SQD] = QIB_POST_RECV_OK | QIB_PROCESS_RECV_OK |
 +	    QIB_POST_SEND_OK | QIB_PROCESS_SEND_OK,
 +	[IB_QPS_SQE] = QIB_POST_RECV_OK | QIB_PROCESS_RECV_OK |
 +	    QIB_POST_SEND_OK | QIB_FLUSH_SEND,
 +	[IB_QPS_ERR] = QIB_POST_RECV_OK | QIB_FLUSH_RECV |
 +	    QIB_POST_SEND_OK | QIB_FLUSH_SEND,
 +};
 +
 +struct qib_ucontext {
 +	struct ib_ucontext ibucontext;
 +};
 +
 +static inline struct qib_ucontext *to_iucontext(struct ib_ucontext
 +						  *ibucontext)
 +{
 +	return container_of(ibucontext, struct qib_ucontext, ibucontext);
 +}
 +
 +/*
++=======
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
   * Translate ib_wr_opcode into ib_wc_opcode.
   */
  const enum ib_wc_opcode ib_qib_wc_opcode[] = {
@@@ -331,161 -301,7 +334,165 @@@ static void qib_copy_from_sge(void *dat
  }
  
  /**
++<<<<<<< HEAD
 + * qib_post_one_send - post one RC, UC, or UD send work request
 + * @qp: the QP to post on
 + * @wr: the work request to send
 + */
 +static int qib_post_one_send(struct qib_qp *qp, struct ib_send_wr *wr,
 +	int *scheduled)
 +{
 +	struct qib_swqe *wqe;
 +	u32 next;
 +	int i;
 +	int j;
 +	int acc;
 +	int ret;
 +	unsigned long flags;
 +	struct qib_lkey_table *rkt;
 +	struct qib_pd *pd;
  
 +	spin_lock_irqsave(&qp->s_lock, flags);
 +
 +	/* Check that state is OK to post send. */
 +	if (unlikely(!(ib_qib_state_ops[qp->state] & QIB_POST_SEND_OK)))
 +		goto bail_inval;
 +
 +	/* IB spec says that num_sge == 0 is OK. */
 +	if (wr->num_sge > qp->s_max_sge)
 +		goto bail_inval;
 +
 +	/*
 +	 * Don't allow RDMA reads or atomic operations on UC or
 +	 * undefined operations.
 +	 * Make sure buffer is large enough to hold the result for atomics.
 +	 */
 +	if (wr->opcode == IB_WR_FAST_REG_MR) {
 +		if (qib_fast_reg_mr(qp, wr))
 +			goto bail_inval;
 +	} else if (qp->ibqp.qp_type == IB_QPT_UC) {
 +		if ((unsigned) wr->opcode >= IB_WR_RDMA_READ)
 +			goto bail_inval;
 +	} else if (qp->ibqp.qp_type != IB_QPT_RC) {
 +		/* Check IB_QPT_SMI, IB_QPT_GSI, IB_QPT_UD opcode */
 +		if (wr->opcode != IB_WR_SEND &&
 +		    wr->opcode != IB_WR_SEND_WITH_IMM)
 +			goto bail_inval;
 +		/* Check UD destination address PD */
 +		if (qp->ibqp.pd != wr->wr.ud.ah->pd)
 +			goto bail_inval;
 +	} else if ((unsigned) wr->opcode > IB_WR_ATOMIC_FETCH_AND_ADD)
 +		goto bail_inval;
 +	else if (wr->opcode >= IB_WR_ATOMIC_CMP_AND_SWP &&
 +		   (wr->num_sge == 0 ||
 +		    wr->sg_list[0].length < sizeof(u64) ||
 +		    wr->sg_list[0].addr & (sizeof(u64) - 1)))
 +		goto bail_inval;
 +	else if (wr->opcode >= IB_WR_RDMA_READ && !qp->s_max_rd_atomic)
 +		goto bail_inval;
 +
 +	next = qp->s_head + 1;
 +	if (next >= qp->s_size)
 +		next = 0;
 +	if (next == qp->s_last) {
 +		ret = -ENOMEM;
 +		goto bail;
 +	}
 +
 +	rkt = &to_idev(qp->ibqp.device)->lk_table;
 +	pd = to_ipd(qp->ibqp.pd);
 +	wqe = get_swqe_ptr(qp, qp->s_head);
 +	wqe->wr = *wr;
 +	wqe->length = 0;
 +	j = 0;
 +	if (wr->num_sge) {
 +		acc = wr->opcode >= IB_WR_RDMA_READ ?
 +			IB_ACCESS_LOCAL_WRITE : 0;
 +		for (i = 0; i < wr->num_sge; i++) {
 +			u32 length = wr->sg_list[i].length;
 +			int ok;
 +
 +			if (length == 0)
 +				continue;
 +			ok = qib_lkey_ok(rkt, pd, &wqe->sg_list[j],
 +					 &wr->sg_list[i], acc);
 +			if (!ok)
 +				goto bail_inval_free;
 +			wqe->length += length;
 +			j++;
 +		}
 +		wqe->wr.num_sge = j;
 +	}
 +	if (qp->ibqp.qp_type == IB_QPT_UC ||
 +	    qp->ibqp.qp_type == IB_QPT_RC) {
 +		if (wqe->length > 0x80000000U)
 +			goto bail_inval_free;
 +	} else if (wqe->length > (dd_from_ibdev(qp->ibqp.device)->pport +
 +				  qp->port_num - 1)->ibmtu)
 +		goto bail_inval_free;
 +	else
 +		atomic_inc(&to_iah(wr->wr.ud.ah)->refcount);
 +	wqe->ssn = qp->s_ssn++;
 +	qp->s_head = next;
 +
 +	ret = 0;
 +	goto bail;
 +
 +bail_inval_free:
 +	while (j) {
 +		struct qib_sge *sge = &wqe->sg_list[--j];
 +
 +		qib_put_mr(sge->mr);
 +	}
 +bail_inval:
 +	ret = -EINVAL;
 +bail:
 +	if (!ret && !wr->next &&
 +	 !qib_sdma_empty(
 +	   dd_from_ibdev(qp->ibqp.device)->pport + qp->port_num - 1)) {
 +		qib_schedule_send(qp);
 +		*scheduled = 1;
 +	}
 +	spin_unlock_irqrestore(&qp->s_lock, flags);
 +	return ret;
 +}
 +
 +/**
 + * qib_post_send - post a send on a QP
 + * @ibqp: the QP to post the send on
 + * @wr: the list of work requests to post
 + * @bad_wr: the first bad WR is put here
 + *
 + * This may be called from interrupt context.
 + */
 +static int qib_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 +			 struct ib_send_wr **bad_wr)
 +{
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_qp_priv *priv = qp->priv;
 +	int err = 0;
 +	int scheduled = 0;
 +
 +	for (; wr; wr = wr->next) {
 +		err = qib_post_one_send(qp, wr, &scheduled);
 +		if (err) {
 +			*bad_wr = wr;
 +			goto bail;
 +		}
 +	}
 +
 +	/* Try to do the send work in the caller's context. */
 +	if (!scheduled)
 +		qib_do_send(&priv->s_work);
 +
 +bail:
 +	return err;
 +}
 +
 +/**
++=======
++
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
   * qib_post_receive - post a receive on a QP
   * @ibqp: the QP to post the receive on
   * @wr: the WR to post
@@@ -496,8 -312,8 +503,13 @@@
  static int qib_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,
  			    struct ib_recv_wr **bad_wr)
  {
++<<<<<<< HEAD
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_rwq *wq = qp->r_rq.wq;
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
+ 	struct rvt_rwq *wq = qp->r_rq.wq;
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	unsigned long flags;
  	int ret;
  
@@@ -567,8 -383,8 +579,13 @@@ static void qib_qp_rcv(struct qib_ctxtd
  	spin_lock(&qp->r_lock);
  
  	/* Check for valid receive state. */
++<<<<<<< HEAD
 +	if (!(ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK)) {
 +		ibp->n_pkt_drops++;
++=======
+ 	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)) {
+ 		ibp->rvp.n_pkt_drops++;
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  		goto unlock;
  	}
  
@@@ -947,14 -763,14 +964,14 @@@ static noinline struct qib_verbs_txreq 
  		spin_unlock_irqrestore(&qp->s_lock, flags);
  		tx = list_entry(l, struct qib_verbs_txreq, txreq.list);
  	} else {
- 		if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK &&
+ 		if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK &&
  		    list_empty(&priv->iowait)) {
  			dev->n_txwait++;
 -			qp->s_flags |= RVT_S_WAIT_TX;
 +			qp->s_flags |= QIB_S_WAIT_TX;
  			list_add_tail(&priv->iowait, &dev->txwait);
  		}
 -		qp->s_flags &= ~RVT_S_BUSY;
 -		spin_unlock(&dev->rdi.pending_lock);
 +		qp->s_flags &= ~QIB_S_BUSY;
 +		spin_unlock(&dev->pending_lock);
  		spin_unlock_irqrestore(&qp->s_lock, flags);
  		tx = ERR_PTR(-EBUSY);
  	}
@@@ -1128,8 -944,8 +1145,13 @@@ static int wait_kmem(struct qib_ibdev *
  	int ret = 0;
  
  	spin_lock_irqsave(&qp->s_lock, flags);
++<<<<<<< HEAD
 +	if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {
 +		spin_lock(&dev->pending_lock);
++=======
+ 	if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {
+ 		spin_lock(&dev->rdi.pending_lock);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  		if (list_empty(&priv->iowait)) {
  			if (list_empty(&dev->memwait))
  				mod_timer(&dev->mem_timer, jiffies + 1);
@@@ -1265,11 -1081,11 +1287,16 @@@ static int no_bufs_available(struct qib
  	 * enabling the PIO avail interrupt.
  	 */
  	spin_lock_irqsave(&qp->s_lock, flags);
++<<<<<<< HEAD
 +	if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {
 +		spin_lock(&dev->pending_lock);
++=======
+ 	if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {
+ 		spin_lock(&dev->rdi.pending_lock);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  		if (list_empty(&priv->iowait)) {
  			dev->n_piowait++;
 -			qp->s_flags |= RVT_S_WAIT_PIO;
 +			qp->s_flags |= QIB_S_WAIT_PIO;
  			list_add_tail(&priv->iowait, &dev->piowait);
  			dd = dd_from_dev(dev);
  			dd->f_wantpiobuf_intr(dd, 1);
@@@ -2248,25 -1825,23 +2275,25 @@@ int qib_register_ib_device(struct qib_d
  	ibdev->modify_qp = qib_modify_qp;
  	ibdev->query_qp = qib_query_qp;
  	ibdev->destroy_qp = qib_destroy_qp;
- 	ibdev->post_send = qib_post_send;
+ 	ibdev->post_send = NULL;
  	ibdev->post_recv = qib_post_receive;
  	ibdev->post_srq_recv = qib_post_srq_receive;
 -	ibdev->create_cq = NULL;
 -	ibdev->destroy_cq = NULL;
 -	ibdev->resize_cq = NULL;
 -	ibdev->poll_cq = NULL;
 -	ibdev->req_notify_cq = NULL;
 -	ibdev->get_dma_mr = NULL;
 -	ibdev->reg_user_mr = NULL;
 -	ibdev->dereg_mr = NULL;
 -	ibdev->alloc_mr = NULL;
 -	ibdev->map_mr_sg = NULL;
 -	ibdev->alloc_fmr = NULL;
 -	ibdev->map_phys_fmr = NULL;
 -	ibdev->unmap_fmr = NULL;
 -	ibdev->dealloc_fmr = NULL;
 +	ibdev->create_cq = qib_create_cq;
 +	ibdev->destroy_cq = qib_destroy_cq;
 +	ibdev->resize_cq = qib_resize_cq;
 +	ibdev->poll_cq = qib_poll_cq;
 +	ibdev->req_notify_cq = qib_req_notify_cq;
 +	ibdev->get_dma_mr = qib_get_dma_mr;
 +	ibdev->reg_phys_mr = qib_reg_phys_mr;
 +	ibdev->reg_user_mr = qib_reg_user_mr;
 +	ibdev->dereg_mr = qib_dereg_mr;
 +	ibdev->alloc_mr = qib_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = qib_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = qib_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = qib_alloc_fmr;
 +	ibdev->map_phys_fmr = qib_map_phys_fmr;
 +	ibdev->unmap_fmr = qib_unmap_fmr;
 +	ibdev->dealloc_fmr = qib_dealloc_fmr;
  	ibdev->attach_mcast = qib_multicast_attach;
  	ibdev->detach_mcast = qib_multicast_detach;
  	ibdev->process_mad = qib_process_mad;
@@@ -2283,11 -1858,42 +2310,23 @@@
  	dd->verbs_dev.rdi.driver_f.port_callback = qib_create_port_files;
  	dd->verbs_dev.rdi.driver_f.get_card_name = qib_get_card_name;
  	dd->verbs_dev.rdi.driver_f.get_pci_dev = qib_get_pci_dev;
++<<<<<<< HEAD
 +	dd->verbs_dev.rdi.dparms.props.max_pd = ib_qib_max_pds;
 +	dd->verbs_dev.rdi.flags = (RVT_FLAG_MR_INIT_DRIVER |
 +				   RVT_FLAG_QP_INIT_DRIVER |
 +				   RVT_FLAG_CQ_INIT_DRIVER);
++=======
+ 	dd->verbs_dev.rdi.driver_f.check_ah = qib_check_ah;
+ 	dd->verbs_dev.rdi.driver_f.notify_new_ah = qib_notify_new_ah;
+ 	dd->verbs_dev.rdi.driver_f.alloc_qpn = alloc_qpn;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_alloc = qp_priv_alloc;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_free = qp_priv_free;
+ 	dd->verbs_dev.rdi.driver_f.free_all_qps = qib_free_all_qps;
+ 	dd->verbs_dev.rdi.driver_f.notify_qp_reset = notify_qp_reset;
+ 	dd->verbs_dev.rdi.driver_f.do_send = qib_do_send;
+ 	dd->verbs_dev.rdi.driver_f.schedule_send = qib_schedule_send;
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  
 -	dd->verbs_dev.rdi.flags = 0;
 -
 -	dd->verbs_dev.rdi.dparms.lkey_table_size = qib_lkey_table_size;
 -	dd->verbs_dev.rdi.dparms.qp_table_size = ib_qib_qp_table_size;
 -	dd->verbs_dev.rdi.dparms.qpn_start = 1;
 -	dd->verbs_dev.rdi.dparms.qpn_res_start = QIB_KD_QP;
 -	dd->verbs_dev.rdi.dparms.qpn_res_end = QIB_KD_QP; /* Reserve one QP */
 -	dd->verbs_dev.rdi.dparms.qpn_inc = 1;
 -	dd->verbs_dev.rdi.dparms.qos_shift = 1;
 -	dd->verbs_dev.rdi.dparms.nports = dd->num_pports;
 -	dd->verbs_dev.rdi.dparms.npkeys = qib_get_npkeys(dd);
 -	dd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;
 -	snprintf(dd->verbs_dev.rdi.dparms.cq_name,
 -		 sizeof(dd->verbs_dev.rdi.dparms.cq_name),
 -		 "qib_cq%d", dd->unit);
 -
 -	qib_fill_device_attr(dd);
 -
 -	ppd = dd->pport;
 -	for (i = 0; i < dd->num_pports; i++, ppd++) {
 -		ctxt = ppd->hw_pidx;
 -		rvt_init_port(&dd->verbs_dev.rdi,
 -			      &ppd->ibport_data.rvp,
 -			      i,
 -			      dd->rcd[ctxt]->pkeys);
 -	}
  
  	ret = rvt_register_device(&dd->verbs_dev.rdi);
  	if (ret)
diff --cc drivers/infiniband/hw/qib/qib_verbs.h
index ca366073af4f,71c8db453e15..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.h
+++ b/drivers/infiniband/hw/qib/qib_verbs.h
@@@ -614,27 -220,14 +603,32 @@@ struct qib_qp 
  #define QIB_PSN_CREDIT  16
  
  /*
++<<<<<<< HEAD
 + * Since struct qib_swqe is not a fixed size, we can't simply index into
 + * struct qib_qp.s_wq.  This function does the array index computation.
 + */
 +static inline struct qib_swqe *get_swqe_ptr(struct qib_qp *qp,
 +					      unsigned n)
 +{
 +	return (struct qib_swqe *)((char *)qp->s_wq +
 +				     (sizeof(struct qib_swqe) +
 +				      qp->s_max_sge *
 +				      sizeof(struct qib_sge)) * n);
 +}
 +
 +/*
 + * Since struct qib_rwqe is not a fixed size, we can't simply index into
 + * struct qib_rwq.wq.  This function does the array index computation.
++=======
+  * Since struct rvt_rwqe is not a fixed size, we can't simply index into
+  * struct rvt_rwq.wq.  This function does the array index computation.
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
   */
 -static inline struct rvt_rwqe *get_rwqe_ptr(struct rvt_rq *rq, unsigned n)
 +static inline struct qib_rwqe *get_rwqe_ptr(struct qib_rq *rq, unsigned n)
  {
 -	return (struct rvt_rwqe *)
 +	return (struct qib_rwqe *)
  		((char *) rq->wq->wq +
 -		 (sizeof(struct rvt_rwqe) +
 +		 (sizeof(struct qib_rwqe) +
  		  rq->max_sge * sizeof(struct ib_sge)) * n);
  }
  
@@@ -816,36 -315,6 +810,39 @@@ struct qib_verbs_counters 
  	u32 vl15_dropped;
  };
  
++<<<<<<< HEAD
 +static inline struct qib_mr *to_imr(struct ib_mr *ibmr)
 +{
 +	return container_of(ibmr, struct qib_mr, ibmr);
 +}
 +
 +static inline struct qib_pd *to_ipd(struct ib_pd *ibpd)
 +{
 +	return container_of(ibpd, struct qib_pd, ibpd);
 +}
 +
 +static inline struct qib_ah *to_iah(struct ib_ah *ibah)
 +{
 +	return container_of(ibah, struct qib_ah, ibah);
 +}
 +
 +static inline struct qib_cq *to_icq(struct ib_cq *ibcq)
 +{
 +	return container_of(ibcq, struct qib_cq, ibcq);
 +}
 +
 +static inline struct qib_srq *to_isrq(struct ib_srq *ibsrq)
 +{
 +	return container_of(ibsrq, struct qib_srq, ibsrq);
 +}
 +
 +static inline struct qib_qp *to_iqp(struct ib_qp *ibqp)
 +{
 +	return container_of(ibqp, struct qib_qp, ibqp);
 +}
 +
++=======
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  static inline struct qib_ibdev *to_idev(struct ib_device *ibdev)
  {
  	struct rvt_dev_info *rdi;
@@@ -1113,21 -496,23 +1110,23 @@@ int qib_ruc_check_hdr(struct qib_ibpor
  u32 qib_make_grh(struct qib_ibport *ibp, struct ib_grh *hdr,
  		 struct ib_global_route *grh, u32 hwords, u32 nwords);
  
 -void qib_make_ruc_header(struct rvt_qp *qp, struct qib_other_headers *ohdr,
 +void qib_make_ruc_header(struct qib_qp *qp, struct qib_other_headers *ohdr,
  			 u32 bth0, u32 bth2);
  
- void qib_do_send(struct work_struct *work);
+ void _qib_do_send(struct work_struct *work);
+ 
+ void qib_do_send(struct rvt_qp *qp);
  
 -void qib_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,
 +void qib_send_complete(struct qib_qp *qp, struct qib_swqe *wqe,
  		       enum ib_wc_status status);
  
 -void qib_send_rc_ack(struct rvt_qp *qp);
 +void qib_send_rc_ack(struct qib_qp *qp);
  
 -int qib_make_rc_req(struct rvt_qp *qp);
 +int qib_make_rc_req(struct qib_qp *qp);
  
 -int qib_make_uc_req(struct rvt_qp *qp);
 +int qib_make_uc_req(struct qib_qp *qp);
  
 -int qib_make_ud_req(struct rvt_qp *qp);
 +int qib_make_ud_req(struct qib_qp *qp);
  
  int qib_register_ib_device(struct qib_devdata *);
  
diff --cc drivers/infiniband/hw/qib/qib_verbs_mcast.c
index b2fb5286dbd9,cf5b88d55f6f..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs_mcast.c
+++ b/drivers/infiniband/hw/qib/qib_verbs_mcast.c
@@@ -224,7 -224,7 +224,11 @@@ bail
  
  int qib_multicast_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
  {
++<<<<<<< HEAD
 +	struct qib_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	struct qib_ibdev *dev = to_idev(ibqp->device);
  	struct qib_ibport *ibp;
  	struct qib_mcast *mcast;
@@@ -282,7 -282,7 +286,11 @@@ bail
  
  int qib_multicast_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
  {
++<<<<<<< HEAD
 +	struct qib_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> db3ef0eb8494 (IB/qib: Use rdmavt version of post_send)
  	struct qib_ibdev *dev = to_idev(ibqp->device);
  	struct qib_ibport *ibp = to_iport(ibqp->device, qp->port_num);
  	struct qib_mcast *mcast = NULL;
* Unmerged path drivers/infiniband/hw/qib/qib_driver.c
* Unmerged path drivers/infiniband/hw/qib/qib_qp.c
* Unmerged path drivers/infiniband/hw/qib/qib_rc.c
* Unmerged path drivers/infiniband/hw/qib/qib_ruc.c
diff --git a/drivers/infiniband/hw/qib/qib_sdma.c b/drivers/infiniband/hw/qib/qib_sdma.c
index ac4fcad97505..77869d81c407 100644
--- a/drivers/infiniband/hw/qib/qib_sdma.c
+++ b/drivers/infiniband/hw/qib/qib_sdma.c
@@ -672,7 +672,7 @@ unmap:
 	spin_lock(&qp->s_lock);
 	if (qp->ibqp.qp_type == IB_QPT_RC) {
 		/* XXX what about error sending RDMA read responses? */
-		if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK)
+		if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)
 			qib_error_qp(qp, IB_WC_GENERAL_ERR);
 	} else if (qp->s_wqe)
 		qib_send_complete(qp, qp->s_wqe, IB_WC_GENERAL_ERR);
@@ -685,7 +685,7 @@ busy:
 	qp = tx->qp;
 	priv = qp->priv;
 	spin_lock(&qp->s_lock);
-	if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {
+	if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {
 		struct qib_ibdev *dev;
 
 		/*
diff --git a/drivers/infiniband/hw/qib/qib_uc.c b/drivers/infiniband/hw/qib/qib_uc.c
index bab9aeb5dd9e..5810c46746c3 100644
--- a/drivers/infiniband/hw/qib/qib_uc.c
+++ b/drivers/infiniband/hw/qib/qib_uc.c
@@ -57,8 +57,8 @@ int qib_make_uc_req(struct qib_qp *qp)
 
 	spin_lock_irqsave(&qp->s_lock, flags);
 
-	if (!(ib_qib_state_ops[qp->state] & QIB_PROCESS_SEND_OK)) {
-		if (!(ib_qib_state_ops[qp->state] & QIB_FLUSH_SEND))
+	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_SEND_OK)) {
+		if (!(ib_rvt_state_ops[qp->state] & RVT_FLUSH_SEND))
 			goto bail;
 		/* We are in the error state, flush the work request. */
 		if (qp->s_last == qp->s_head)
@@ -68,7 +68,7 @@ int qib_make_uc_req(struct qib_qp *qp)
 			qp->s_flags |= QIB_S_WAIT_DMA;
 			goto bail;
 		}
-		wqe = get_swqe_ptr(qp, qp->s_last);
+		wqe = rvt_get_swqe_ptr(qp, qp->s_last);
 		qib_send_complete(qp, wqe, IB_WC_WR_FLUSH_ERR);
 		goto done;
 	}
@@ -82,12 +82,12 @@ int qib_make_uc_req(struct qib_qp *qp)
 	bth0 = 0;
 
 	/* Get the next send request. */
-	wqe = get_swqe_ptr(qp, qp->s_cur);
+	wqe = rvt_get_swqe_ptr(qp, qp->s_cur);
 	qp->s_wqe = NULL;
 	switch (qp->s_state) {
 	default:
-		if (!(ib_qib_state_ops[qp->state] &
-		    QIB_PROCESS_NEXT_SEND_OK))
+		if (!(ib_rvt_state_ops[qp->state] &
+		    RVT_PROCESS_NEXT_SEND_OK))
 			goto bail;
 		/* Check if send work queue is empty. */
 		if (qp->s_cur == qp->s_head)
* Unmerged path drivers/infiniband/hw/qib/qib_ud.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.h
* Unmerged path drivers/infiniband/hw/qib/qib_verbs_mcast.c
