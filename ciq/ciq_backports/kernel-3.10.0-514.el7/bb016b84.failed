mm/sparse: use memblock apis for early memory allocations

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] sparse: use memblock apis for early memory allocations (Koki Sanagi) [1375453]
Rebuild_FUZZ: 97.30%
commit-author Santosh Shilimkar <santosh.shilimkar@ti.com>
commit bb016b84164554725899aef544331085e08cb402
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/bb016b84.failed

Switch to memblock interfaces for early memory allocator instead of
bootmem allocator.  No functional change in beahvior than what it is in
current code from bootmem users points of view.

Archs already converted to NO_BOOTMEM now directly use memblock
interfaces instead of bootmem wrappers build on top of memblock.  And
the archs which still uses bootmem, these new apis just fallback to
exiting bootmem APIs.

	Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
	Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Christoph Lameter <cl@linux-foundation.org>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Grygorii Strashko <grygorii.strashko@ti.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Paul Walmsley <paul@pwsan.com>
	Cc: Pavel Machek <pavel@ucw.cz>
	Cc: Russell King <linux@arm.linux.org.uk>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Tony Lindgren <tony@atomide.com>
	Cc: Yinghai Lu <yinghai@kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit bb016b84164554725899aef544331085e08cb402)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/sparse.c
diff --cc mm/sparse.c
index 1c91f0d3f6ab,63c3ea5c119c..000000000000
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@@ -573,8 -510,57 +576,62 @@@ void __init sparse_init(void
  		map_count = 1;
  	}
  	/* ok, last chunk */
++<<<<<<< HEAD
 +	sparse_early_mem_maps_alloc_node(map_map, pnum_begin, NR_MEM_SECTIONS,
 +					 map_count, nodeid_begin);
++=======
+ 	alloc_func(data, pnum_begin, NR_MEM_SECTIONS,
+ 						map_count, nodeid_begin);
+ }
+ 
+ /*
+  * Allocate the accumulated non-linear sections, allocate a mem_map
+  * for each and record the physical to section mapping.
+  */
+ void __init sparse_init(void)
+ {
+ 	unsigned long pnum;
+ 	struct page *map;
+ 	unsigned long *usemap;
+ 	unsigned long **usemap_map;
+ 	int size;
+ #ifdef CONFIG_SPARSEMEM_ALLOC_MEM_MAP_TOGETHER
+ 	int size2;
+ 	struct page **map_map;
+ #endif
+ 
+ 	/* see include/linux/mmzone.h 'struct mem_section' definition */
+ 	BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
+ 
+ 	/* Setup pageblock_order for HUGETLB_PAGE_SIZE_VARIABLE */
+ 	set_pageblock_order();
+ 
+ 	/*
+ 	 * map is using big page (aka 2M in x86 64 bit)
+ 	 * usemap is less one page (aka 24 bytes)
+ 	 * so alloc 2M (with 2M align) and 24 bytes in turn will
+ 	 * make next 2M slip to one more 2M later.
+ 	 * then in big system, the memory will have a lot of holes...
+ 	 * here try to allocate 2M pages continuously.
+ 	 *
+ 	 * powerpc need to call sparse_init_one_section right after each
+ 	 * sparse_early_mem_map_alloc, so allocate usemap_map at first.
+ 	 */
+ 	size = sizeof(unsigned long *) * NR_MEM_SECTIONS;
+ 	usemap_map = memblock_virt_alloc(size, 0);
+ 	if (!usemap_map)
+ 		panic("can not allocate usemap_map\n");
+ 	alloc_usemap_and_memmap(sparse_early_usemaps_alloc_node,
+ 							(void *)usemap_map);
+ 
+ #ifdef CONFIG_SPARSEMEM_ALLOC_MEM_MAP_TOGETHER
+ 	size2 = sizeof(struct page *) * NR_MEM_SECTIONS;
+ 	map_map = memblock_virt_alloc(size2, 0);
+ 	if (!map_map)
+ 		panic("can not allocate map_map\n");
+ 	alloc_usemap_and_memmap(sparse_early_mem_maps_alloc_node,
+ 							(void *)map_map);
++>>>>>>> bb016b841645 (mm/sparse: use memblock apis for early memory allocations)
  #endif
  
  	for (pnum = 0; pnum < NR_MEM_SECTIONS; pnum++) {
diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c
index 27eeab3be757..4cba9c2783a1 100644
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@ -40,7 +40,8 @@ static void * __init_refok __earlyonly_bootmem_alloc(int node,
 				unsigned long align,
 				unsigned long goal)
 {
-	return __alloc_bootmem_node_high(NODE_DATA(node), size, align, goal);
+	return memblock_virt_alloc_try_nid(size, align, goal,
+					    BOOTMEM_ALLOC_ACCESSIBLE, node);
 }
 
 static void *vmemmap_buf;
@@ -226,7 +227,8 @@ void __init sparse_mem_maps_populate_node(struct page **map_map,
 
 	if (vmemmap_buf_start) {
 		/* need to free left buf */
-		free_bootmem(__pa(vmemmap_buf), vmemmap_buf_end - vmemmap_buf);
+		memblock_free_early(__pa(vmemmap_buf),
+				    vmemmap_buf_end - vmemmap_buf);
 		vmemmap_buf = NULL;
 		vmemmap_buf_end = NULL;
 	}
* Unmerged path mm/sparse.c
