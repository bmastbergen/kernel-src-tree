IB/qib: Remove qib_post_receive and use rdmavt version

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit a7d34a47f212ae6bd7f4748aebcc4f1192a048d1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a7d34a47.failed

This patch removes the simple post recv function in favor of using rdmavt.
The packet receive processing still lives in the driver though.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit a7d34a47f212ae6bd7f4748aebcc4f1192a048d1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/qib_verbs.c
diff --cc drivers/infiniband/hw/qib/qib_verbs.c
index c4417a1f33be,add899b67057..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.c
+++ b/drivers/infiniband/hw/qib/qib_verbs.c
@@@ -331,222 -301,6 +331,225 @@@ static void qib_copy_from_sge(void *dat
  }
  
  /**
++<<<<<<< HEAD
 + * qib_post_one_send - post one RC, UC, or UD send work request
 + * @qp: the QP to post on
 + * @wr: the work request to send
 + */
 +static int qib_post_one_send(struct qib_qp *qp, struct ib_send_wr *wr,
 +	int *scheduled)
 +{
 +	struct qib_swqe *wqe;
 +	u32 next;
 +	int i;
 +	int j;
 +	int acc;
 +	int ret;
 +	unsigned long flags;
 +	struct qib_lkey_table *rkt;
 +	struct qib_pd *pd;
 +
 +	spin_lock_irqsave(&qp->s_lock, flags);
 +
 +	/* Check that state is OK to post send. */
 +	if (unlikely(!(ib_qib_state_ops[qp->state] & QIB_POST_SEND_OK)))
 +		goto bail_inval;
 +
 +	/* IB spec says that num_sge == 0 is OK. */
 +	if (wr->num_sge > qp->s_max_sge)
 +		goto bail_inval;
 +
 +	/*
 +	 * Don't allow RDMA reads or atomic operations on UC or
 +	 * undefined operations.
 +	 * Make sure buffer is large enough to hold the result for atomics.
 +	 */
 +	if (wr->opcode == IB_WR_FAST_REG_MR) {
 +		if (qib_fast_reg_mr(qp, wr))
 +			goto bail_inval;
 +	} else if (qp->ibqp.qp_type == IB_QPT_UC) {
 +		if ((unsigned) wr->opcode >= IB_WR_RDMA_READ)
 +			goto bail_inval;
 +	} else if (qp->ibqp.qp_type != IB_QPT_RC) {
 +		/* Check IB_QPT_SMI, IB_QPT_GSI, IB_QPT_UD opcode */
 +		if (wr->opcode != IB_WR_SEND &&
 +		    wr->opcode != IB_WR_SEND_WITH_IMM)
 +			goto bail_inval;
 +		/* Check UD destination address PD */
 +		if (qp->ibqp.pd != wr->wr.ud.ah->pd)
 +			goto bail_inval;
 +	} else if ((unsigned) wr->opcode > IB_WR_ATOMIC_FETCH_AND_ADD)
 +		goto bail_inval;
 +	else if (wr->opcode >= IB_WR_ATOMIC_CMP_AND_SWP &&
 +		   (wr->num_sge == 0 ||
 +		    wr->sg_list[0].length < sizeof(u64) ||
 +		    wr->sg_list[0].addr & (sizeof(u64) - 1)))
 +		goto bail_inval;
 +	else if (wr->opcode >= IB_WR_RDMA_READ && !qp->s_max_rd_atomic)
 +		goto bail_inval;
 +
 +	next = qp->s_head + 1;
 +	if (next >= qp->s_size)
 +		next = 0;
 +	if (next == qp->s_last) {
 +		ret = -ENOMEM;
 +		goto bail;
 +	}
 +
 +	rkt = &to_idev(qp->ibqp.device)->lk_table;
 +	pd = to_ipd(qp->ibqp.pd);
 +	wqe = get_swqe_ptr(qp, qp->s_head);
 +	wqe->wr = *wr;
 +	wqe->length = 0;
 +	j = 0;
 +	if (wr->num_sge) {
 +		acc = wr->opcode >= IB_WR_RDMA_READ ?
 +			IB_ACCESS_LOCAL_WRITE : 0;
 +		for (i = 0; i < wr->num_sge; i++) {
 +			u32 length = wr->sg_list[i].length;
 +			int ok;
 +
 +			if (length == 0)
 +				continue;
 +			ok = qib_lkey_ok(rkt, pd, &wqe->sg_list[j],
 +					 &wr->sg_list[i], acc);
 +			if (!ok)
 +				goto bail_inval_free;
 +			wqe->length += length;
 +			j++;
 +		}
 +		wqe->wr.num_sge = j;
 +	}
 +	if (qp->ibqp.qp_type == IB_QPT_UC ||
 +	    qp->ibqp.qp_type == IB_QPT_RC) {
 +		if (wqe->length > 0x80000000U)
 +			goto bail_inval_free;
 +	} else if (wqe->length > (dd_from_ibdev(qp->ibqp.device)->pport +
 +				  qp->port_num - 1)->ibmtu)
 +		goto bail_inval_free;
 +	else
 +		atomic_inc(&to_iah(wr->wr.ud.ah)->refcount);
 +	wqe->ssn = qp->s_ssn++;
 +	qp->s_head = next;
 +
 +	ret = 0;
 +	goto bail;
 +
 +bail_inval_free:
 +	while (j) {
 +		struct qib_sge *sge = &wqe->sg_list[--j];
 +
 +		qib_put_mr(sge->mr);
 +	}
 +bail_inval:
 +	ret = -EINVAL;
 +bail:
 +	if (!ret && !wr->next &&
 +	 !qib_sdma_empty(
 +	   dd_from_ibdev(qp->ibqp.device)->pport + qp->port_num - 1)) {
 +		qib_schedule_send(qp);
 +		*scheduled = 1;
 +	}
 +	spin_unlock_irqrestore(&qp->s_lock, flags);
 +	return ret;
 +}
 +
 +/**
 + * qib_post_send - post a send on a QP
 + * @ibqp: the QP to post the send on
 + * @wr: the list of work requests to post
 + * @bad_wr: the first bad WR is put here
 + *
 + * This may be called from interrupt context.
 + */
 +static int qib_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 +			 struct ib_send_wr **bad_wr)
 +{
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_qp_priv *priv = qp->priv;
 +	int err = 0;
 +	int scheduled = 0;
 +
 +	for (; wr; wr = wr->next) {
 +		err = qib_post_one_send(qp, wr, &scheduled);
 +		if (err) {
 +			*bad_wr = wr;
 +			goto bail;
 +		}
 +	}
 +
 +	/* Try to do the send work in the caller's context. */
 +	if (!scheduled)
 +		qib_do_send(&priv->s_work);
 +
 +bail:
 +	return err;
 +}
 +
 +/**
 + * qib_post_receive - post a receive on a QP
 + * @ibqp: the QP to post the receive on
 + * @wr: the WR to post
 + * @bad_wr: the first bad WR is put here
 + *
 + * This may be called from interrupt context.
 + */
 +static int qib_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,
 +			    struct ib_recv_wr **bad_wr)
 +{
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_rwq *wq = qp->r_rq.wq;
 +	unsigned long flags;
 +	int ret;
 +
 +	/* Check that state is OK to post receive. */
 +	if (!(ib_qib_state_ops[qp->state] & QIB_POST_RECV_OK) || !wq) {
 +		*bad_wr = wr;
 +		ret = -EINVAL;
 +		goto bail;
 +	}
 +
 +	for (; wr; wr = wr->next) {
 +		struct qib_rwqe *wqe;
 +		u32 next;
 +		int i;
 +
 +		if ((unsigned) wr->num_sge > qp->r_rq.max_sge) {
 +			*bad_wr = wr;
 +			ret = -EINVAL;
 +			goto bail;
 +		}
 +
 +		spin_lock_irqsave(&qp->r_rq.lock, flags);
 +		next = wq->head + 1;
 +		if (next >= qp->r_rq.size)
 +			next = 0;
 +		if (next == wq->tail) {
 +			spin_unlock_irqrestore(&qp->r_rq.lock, flags);
 +			*bad_wr = wr;
 +			ret = -ENOMEM;
 +			goto bail;
 +		}
 +
 +		wqe = get_rwqe_ptr(&qp->r_rq, wq->head);
 +		wqe->wr_id = wr->wr_id;
 +		wqe->num_sge = wr->num_sge;
 +		for (i = 0; i < wr->num_sge; i++)
 +			wqe->sg_list[i] = wr->sg_list[i];
 +		/* Make sure queue entry is written before the head index. */
 +		smp_wmb();
 +		wq->head = next;
 +		spin_unlock_irqrestore(&qp->r_rq.lock, flags);
 +	}
 +	ret = 0;
 +
 +bail:
 +	return ret;
 +}
 +
 +/**
++=======
++>>>>>>> a7d34a47f212 (IB/qib: Remove qib_post_receive and use rdmavt version)
   * qib_qp_rcv - processing an incoming packet on a QP
   * @rcd: the context pointer
   * @hdr: the packet header
@@@ -2248,25 -1763,23 +2251,30 @@@ int qib_register_ib_device(struct qib_d
  	ibdev->modify_qp = qib_modify_qp;
  	ibdev->query_qp = qib_query_qp;
  	ibdev->destroy_qp = qib_destroy_qp;
++<<<<<<< HEAD
 +	ibdev->post_send = qib_post_send;
 +	ibdev->post_recv = qib_post_receive;
++=======
+ 	ibdev->post_send = NULL;
+ 	ibdev->post_recv = NULL;
++>>>>>>> a7d34a47f212 (IB/qib: Remove qib_post_receive and use rdmavt version)
  	ibdev->post_srq_recv = qib_post_srq_receive;
 -	ibdev->create_cq = NULL;
 -	ibdev->destroy_cq = NULL;
 -	ibdev->resize_cq = NULL;
 -	ibdev->poll_cq = NULL;
 -	ibdev->req_notify_cq = NULL;
 -	ibdev->get_dma_mr = NULL;
 -	ibdev->reg_user_mr = NULL;
 -	ibdev->dereg_mr = NULL;
 -	ibdev->alloc_mr = NULL;
 -	ibdev->map_mr_sg = NULL;
 -	ibdev->alloc_fmr = NULL;
 -	ibdev->map_phys_fmr = NULL;
 -	ibdev->unmap_fmr = NULL;
 -	ibdev->dealloc_fmr = NULL;
 +	ibdev->create_cq = qib_create_cq;
 +	ibdev->destroy_cq = qib_destroy_cq;
 +	ibdev->resize_cq = qib_resize_cq;
 +	ibdev->poll_cq = qib_poll_cq;
 +	ibdev->req_notify_cq = qib_req_notify_cq;
 +	ibdev->get_dma_mr = qib_get_dma_mr;
 +	ibdev->reg_phys_mr = qib_reg_phys_mr;
 +	ibdev->reg_user_mr = qib_reg_user_mr;
 +	ibdev->dereg_mr = qib_dereg_mr;
 +	ibdev->alloc_mr = qib_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = qib_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = qib_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = qib_alloc_fmr;
 +	ibdev->map_phys_fmr = qib_map_phys_fmr;
 +	ibdev->unmap_fmr = qib_unmap_fmr;
 +	ibdev->dealloc_fmr = qib_dealloc_fmr;
  	ibdev->attach_mcast = qib_multicast_attach;
  	ibdev->detach_mcast = qib_multicast_detach;
  	ibdev->process_mad = qib_process_mad;
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.c
