IB/hfi1: Prevent unpinning of wrong pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mitko Haralanov <mitko.haralanov@intel.com>
commit 849e3e9398608c26a7c54bf9fbf3288f7ced6bfb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/849e3e93.failed

The routine used by the SDMA cache to handle already
cached nodes can extend an already existing node.

In its error handling code, the routine will unpin pages
when not all pages of the buffer extension were pinned.

There was a bug in that part of the routine, which would
mistakenly unpin pages from the original set rather than
the newly pinned pages.

This commit fixes that bug by offsetting the page array
to the proper place pointing at the beginning of the newly
pinned pages.

	Reviewed-by: Dean Luick <dean.luick@intel.com>
	Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 849e3e9398608c26a7c54bf9fbf3288f7ced6bfb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/user_sdma.c
diff --cc drivers/staging/hfi1/user_sdma.c
index 6967deb7956a,d53a659548e0..000000000000
--- a/drivers/staging/hfi1/user_sdma.c
+++ b/drivers/staging/hfi1/user_sdma.c
@@@ -278,7 -278,8 +278,12 @@@ static inline void pq_update(struct hfi
  static void user_sdma_free_request(struct user_sdma_request *, bool);
  static int pin_vector_pages(struct user_sdma_request *,
  			    struct user_sdma_iovec *);
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +static void unpin_vector_pages(struct user_sdma_iovec *);
++=======
+ static void unpin_vector_pages(struct mm_struct *, struct page **, unsigned,
+ 			       unsigned);
++>>>>>>> 849e3e939860 (IB/hfi1: Prevent unpinning of wrong pages):drivers/staging/rdma/hfi1/user_sdma.c
  static int check_header_template(struct user_sdma_request *,
  				 struct hfi1_pkt_header *, u32, u32);
  static int set_txreq_header(struct user_sdma_request *,
@@@ -1036,40 -1030,132 +1041,95 @@@ static inline int num_user_pages(const 
  	return 1 + ((epage - spage) >> PAGE_SHIFT);
  }
  
 -/* Caller must hold pq->evict_lock */
 -static u32 sdma_cache_evict(struct hfi1_user_sdma_pkt_q *pq, u32 npages)
 -{
 -	u32 cleared = 0;
 -	struct sdma_mmu_node *node, *ptr;
 -
 -	list_for_each_entry_safe_reverse(node, ptr, &pq->evict, list) {
 -		/* Make sure that no one is still using the node. */
 -		if (!atomic_read(&node->refcount)) {
 -			/*
 -			 * Need to use the page count now as the remove callback
 -			 * will free the node.
 -			 */
 -			cleared += node->npages;
 -			spin_unlock(&pq->evict_lock);
 -			hfi1_mmu_rb_remove(&pq->sdma_rb_root, &node->rb);
 -			spin_lock(&pq->evict_lock);
 -			if (cleared >= npages)
 -				break;
 -		}
 -	}
 -	return cleared;
 -}
 -
  static int pin_vector_pages(struct user_sdma_request *req,
  			    struct user_sdma_iovec *iovec) {
 -	int ret = 0, pinned, npages, cleared;
 -	struct page **pages;
 -	struct hfi1_user_sdma_pkt_q *pq = req->pq;
 -	struct sdma_mmu_node *node = NULL;
 -	struct mmu_rb_node *rb_node;
 -
 -	rb_node = hfi1_mmu_rb_search(&pq->sdma_rb_root,
 -				     (unsigned long)iovec->iov.iov_base,
 -				     iovec->iov.iov_len);
 -	if (rb_node && !IS_ERR(rb_node))
 -		node = container_of(rb_node, struct sdma_mmu_node, rb);
 -	else
 -		rb_node = NULL;
 -
 -	if (!node) {
 -		node = kzalloc(sizeof(*node), GFP_KERNEL);
 -		if (!node)
 -			return -ENOMEM;
 -
 -		node->rb.addr = (unsigned long)iovec->iov.iov_base;
 -		node->rb.len = iovec->iov.iov_len;
 -		node->pq = pq;
 -		atomic_set(&node->refcount, 0);
 -		INIT_LIST_HEAD(&node->list);
 -	}
 +	int pinned, npages;
  
  	npages = num_user_pages(&iovec->iov);
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +	iovec->pages = kcalloc(npages, sizeof(*iovec->pages), GFP_KERNEL);
 +	if (!iovec->pages) {
 +		SDMA_DBG(req, "Failed page array alloc");
 +		return -ENOMEM;
++=======
+ 	if (node->npages < npages) {
+ 		pages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);
+ 		if (!pages) {
+ 			SDMA_DBG(req, "Failed page array alloc");
+ 			ret = -ENOMEM;
+ 			goto bail;
+ 		}
+ 		memcpy(pages, node->pages, node->npages * sizeof(*pages));
+ 
+ 		npages -= node->npages;
+ retry:
+ 		if (!hfi1_can_pin_pages(pq->dd, pq->n_locked, npages)) {
+ 			spin_lock(&pq->evict_lock);
+ 			cleared = sdma_cache_evict(pq, npages);
+ 			spin_unlock(&pq->evict_lock);
+ 			if (cleared >= npages)
+ 				goto retry;
+ 		}
+ 		pinned = hfi1_acquire_user_pages(
+ 			((unsigned long)iovec->iov.iov_base +
+ 			 (node->npages * PAGE_SIZE)), npages, 0,
+ 			pages + node->npages);
+ 		if (pinned < 0) {
+ 			kfree(pages);
+ 			ret = pinned;
+ 			goto bail;
+ 		}
+ 		if (pinned != npages) {
+ 			unpin_vector_pages(current->mm, pages, node->npages,
+ 					   pinned);
+ 			ret = -EFAULT;
+ 			goto bail;
+ 		}
+ 		kfree(node->pages);
+ 		node->pages = pages;
+ 		node->npages += pinned;
+ 		npages = node->npages;
+ 		spin_lock(&pq->evict_lock);
+ 		if (!rb_node)
+ 			list_add(&node->list, &pq->evict);
+ 		else
+ 			list_move(&node->list, &pq->evict);
+ 		pq->n_locked += pinned;
+ 		spin_unlock(&pq->evict_lock);
++>>>>>>> 849e3e939860 (IB/hfi1: Prevent unpinning of wrong pages):drivers/staging/rdma/hfi1/user_sdma.c
  	}
 -	iovec->pages = node->pages;
 -	iovec->npages = npages;
  
 -	if (!rb_node) {
 -		ret = hfi1_mmu_rb_insert(&req->pq->sdma_rb_root, &node->rb);
 -		if (ret) {
 -			spin_lock(&pq->evict_lock);
 -			list_del(&node->list);
 -			pq->n_locked -= node->npages;
 -			spin_unlock(&pq->evict_lock);
 -			ret = 0;
 -			goto bail;
 -		}
 -	} else {
 -		atomic_inc(&node->refcount);
 +	pinned = hfi1_acquire_user_pages((unsigned long)iovec->iov.iov_base,
 +					 npages, 0, iovec->pages);
 +
 +	if (pinned < 0)
 +		return pinned;
 +
 +	iovec->npages = pinned;
 +	if (pinned != npages) {
 +		SDMA_DBG(req, "Failed to pin pages (%d/%u)", pinned, npages);
 +		unpin_vector_pages(iovec);
 +		return -EFAULT;
  	}
  	return 0;
 -bail:
 -	if (!rb_node)
 -		kfree(node);
 -	return ret;
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +static void unpin_vector_pages(struct user_sdma_iovec *iovec)
 +{
 +	hfi1_release_user_pages(iovec->pages, iovec->npages, 0);
 +
 +	kfree(iovec->pages);
 +	iovec->pages = NULL;
 +	iovec->npages = 0;
 +	iovec->offset = 0;
++=======
+ static void unpin_vector_pages(struct mm_struct *mm, struct page **pages,
+ 			       unsigned start, unsigned npages)
+ {
+ 	hfi1_release_user_pages(mm, pages + start, npages, 0);
+ 	kfree(pages);
++>>>>>>> 849e3e939860 (IB/hfi1: Prevent unpinning of wrong pages):drivers/staging/rdma/hfi1/user_sdma.c
  }
  
  static int check_header_template(struct user_sdma_request *req,
@@@ -1451,3 -1535,56 +1511,59 @@@ static inline void set_comp_state(struc
  	trace_hfi1_sdma_user_completion(pq->dd, pq->ctxt, pq->subctxt,
  					idx, state, ret);
  }
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 
+ static bool sdma_rb_filter(struct mmu_rb_node *node, unsigned long addr,
+ 			   unsigned long len)
+ {
+ 	return (bool)(node->addr == addr);
+ }
+ 
+ static int sdma_rb_insert(struct rb_root *root, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	atomic_inc(&node->refcount);
+ 	return 0;
+ }
+ 
+ static void sdma_rb_remove(struct rb_root *root, struct mmu_rb_node *mnode,
+ 			   struct mm_struct *mm)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	spin_lock(&node->pq->evict_lock);
+ 	list_del(&node->list);
+ 	node->pq->n_locked -= node->npages;
+ 	spin_unlock(&node->pq->evict_lock);
+ 
+ 	/*
+ 	 * If mm is set, we are being called by the MMU notifier and we
+ 	 * should not pass a mm_struct to unpin_vector_page(). This is to
+ 	 * prevent a deadlock when hfi1_release_user_pages() attempts to
+ 	 * take the mmap_sem, which the MMU notifier has already taken.
+ 	 */
+ 	unpin_vector_pages(mm ? NULL : current->mm, node->pages, 0,
+ 			   node->npages);
+ 	/*
+ 	 * If called by the MMU notifier, we have to adjust the pinned
+ 	 * page count ourselves.
+ 	 */
+ 	if (mm)
+ 		mm->pinned_vm -= node->npages;
+ 	kfree(node);
+ }
+ 
+ static int sdma_rb_invalidate(struct rb_root *root, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	if (!atomic_read(&node->refcount))
+ 		return 1;
+ 	return 0;
+ }
++>>>>>>> 849e3e939860 (IB/hfi1: Prevent unpinning of wrong pages):drivers/staging/rdma/hfi1/user_sdma.c
* Unmerged path drivers/staging/hfi1/user_sdma.c
