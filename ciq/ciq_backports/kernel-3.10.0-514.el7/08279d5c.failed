staging/rdma/hfi1: use new RNR timer

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [infiniband] rdma/hfi1: use new RNR timer (Alex Estrin) [1272062 1273170]
Rebuild_FUZZ: 87.50%
commit-author Mike Marciniszyn <mike.marciniszyn@intel.com>
commit 08279d5c9424afd710c90d0b6df95612d2bb5a3f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/08279d5c.failed

Use the new RNR timer for hfi1.

For qib, this timer doesn't exist, so exploit driver
callbacks to use the new timer as appropriate.

	Reviewed-by: Jubin John <jubin.john@intel.com>
	Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 08279d5c9424afd710c90d0b6df95612d2bb5a3f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/qib_qp.c
#	drivers/staging/hfi1/rc.c
#	drivers/staging/rdma/hfi1/qp.c
#	drivers/staging/rdma/hfi1/verbs.h
diff --cc drivers/infiniband/hw/qib/qib_qp.c
index cf1dd6e9d434,787116f59395..000000000000
--- a/drivers/infiniband/hw/qib/qib_qp.c
+++ b/drivers/infiniband/hw/qib/qib_qp.c
@@@ -967,326 -378,63 +967,331 @@@ __be32 qib_compute_aeth(struct qib_qp *
  	return cpu_to_be32(aeth);
  }
  
 -void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp, gfp_t gfp)
 +/**
 + * qib_create_qp - create a queue pair for a device
 + * @ibpd: the protection domain who's device we create the queue pair for
 + * @init_attr: the attributes of the queue pair
 + * @udata: user data for libibverbs.so
 + *
 + * Returns the queue pair on success, otherwise returns an errno.
 + *
 + * Called by the ib_create_qp() core verbs function.
 + */
 +struct ib_qp *qib_create_qp(struct ib_pd *ibpd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata)
  {
 +	struct qib_qp *qp;
 +	int err;
 +	struct qib_swqe *swq = NULL;
 +	struct qib_ibdev *dev;
 +	struct qib_devdata *dd;
 +	size_t sz;
 +	size_t sg_list_sz;
 +	struct ib_qp *ret;
 +	gfp_t gfp;
  	struct qib_qp_priv *priv;
  
 -	priv = kzalloc(sizeof(*priv), gfp);
 -	if (!priv)
 -		return ERR_PTR(-ENOMEM);
 -	priv->owner = qp;
 +	if (init_attr->cap.max_send_sge > ib_qib_max_sges ||
 +	    init_attr->cap.max_send_wr > ib_qib_max_qp_wrs ||
 +	    init_attr->create_flags & ~(IB_QP_CREATE_USE_GFP_NOIO))
 +		return ERR_PTR(-EINVAL);
  
 -	priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 -	if (!priv->s_hdr) {
 -		kfree(priv);
 -		return ERR_PTR(-ENOMEM);
 +	/* GFP_NOIO is applicable in RC QPs only */
 +	if (init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO &&
 +	    init_attr->qp_type != IB_QPT_RC)
 +		return ERR_PTR(-EINVAL);
 +
 +	gfp = init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO ?
 +			GFP_NOIO : GFP_KERNEL;
 +
 +	/* Check receive queue parameters if no SRQ is specified. */
 +	if (!init_attr->srq) {
 +		if (init_attr->cap.max_recv_sge > ib_qib_max_sges ||
 +		    init_attr->cap.max_recv_wr > ib_qib_max_qp_wrs) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +		if (init_attr->cap.max_send_sge +
 +		    init_attr->cap.max_send_wr +
 +		    init_attr->cap.max_recv_sge +
 +		    init_attr->cap.max_recv_wr == 0) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
  	}
 -	init_waitqueue_head(&priv->wait_dma);
 -	INIT_WORK(&priv->s_work, _qib_do_send);
 -	INIT_LIST_HEAD(&priv->iowait);
  
 -	return priv;
 -}
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_SMI:
 +	case IB_QPT_GSI:
 +		if (init_attr->port_num == 0 ||
 +		    init_attr->port_num > ibpd->device->phys_port_cnt) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +	case IB_QPT_UC:
 +	case IB_QPT_RC:
 +	case IB_QPT_UD:
 +		sz = sizeof(struct qib_sge) *
 +			init_attr->cap.max_send_sge +
 +			sizeof(struct qib_swqe);
 +		swq = __vmalloc((init_attr->cap.max_send_wr + 1) * sz,
 +				gfp, PAGE_KERNEL);
 +		if (swq == NULL) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail;
 +		}
 +		sz = sizeof(*qp);
 +		sg_list_sz = 0;
 +		if (init_attr->srq) {
 +			struct qib_srq *srq = to_isrq(init_attr->srq);
  
 -void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)
 -{
 -	struct qib_qp_priv *priv = qp->priv;
 +			if (srq->rq.max_sge > 1)
 +				sg_list_sz = sizeof(*qp->r_sg_list) *
 +					(srq->rq.max_sge - 1);
 +		} else if (init_attr->cap.max_recv_sge > 1)
 +			sg_list_sz = sizeof(*qp->r_sg_list) *
 +				(init_attr->cap.max_recv_sge - 1);
 +		qp = kzalloc(sz + sg_list_sz, gfp);
 +		if (!qp) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_swq;
 +		}
 +		RCU_INIT_POINTER(qp->next, NULL);
 +		priv = kzalloc(sizeof(*priv), gfp);
 +		if (!priv) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp_hdr;
 +		}
 +		priv->owner = qp;
 +		priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 +		if (!priv->s_hdr) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp;
 +		}
 +		qp->priv = priv;
 +		qp->timeout_jiffies =
 +			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
 +				1000UL);
 +		if (init_attr->srq)
 +			sz = 0;
 +		else {
 +			qp->r_rq.size = init_attr->cap.max_recv_wr + 1;
 +			qp->r_rq.max_sge = init_attr->cap.max_recv_sge;
 +			sz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +
 +				sizeof(struct qib_rwqe);
 +			if (gfp != GFP_NOIO)
 +				qp->r_rq.wq = vmalloc_user(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz);
 +			else
 +				qp->r_rq.wq = __vmalloc(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz,
 +						gfp, PAGE_KERNEL);
 +
 +			if (!qp->r_rq.wq) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_qp;
 +			}
 +		}
 +
 +		/*
 +		 * ib_create_qp() will initialize qp->ibqp
 +		 * except for qp->ibqp.qp_num.
 +		 */
 +		spin_lock_init(&qp->r_lock);
 +		spin_lock_init(&qp->s_lock);
 +		spin_lock_init(&qp->r_rq.lock);
 +		atomic_set(&qp->refcount, 0);
 +		init_waitqueue_head(&qp->wait);
 +		init_waitqueue_head(&priv->wait_dma);
 +		init_timer(&qp->s_timer);
 +		qp->s_timer.data = (unsigned long)qp;
 +		INIT_WORK(&priv->s_work, qib_do_send);
 +		INIT_LIST_HEAD(&priv->iowait);
 +		INIT_LIST_HEAD(&qp->rspwait);
 +		qp->state = IB_QPS_RESET;
 +		qp->s_wq = swq;
 +		qp->s_size = init_attr->cap.max_send_wr + 1;
 +		qp->s_max_sge = init_attr->cap.max_send_sge;
 +		if (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)
 +			qp->s_flags = QIB_S_SIGNAL_REQ_WR;
 +		dev = to_idev(ibpd->device);
 +		dd = dd_from_dev(dev);
 +		err = alloc_qpn(dd, &dev->qpn_table, init_attr->qp_type,
 +				init_attr->port_num, gfp);
 +		if (err < 0) {
 +			ret = ERR_PTR(err);
 +			vfree(qp->r_rq.wq);
 +			goto bail_qp;
 +		}
 +		qp->ibqp.qp_num = err;
 +		qp->port_num = init_attr->port_num;
 +		qib_reset_qp(qp, init_attr->qp_type);
 +		break;
 +
 +	default:
 +		/* Don't support raw QPs */
 +		ret = ERR_PTR(-ENOSYS);
 +		goto bail;
 +	}
 +
 +	init_attr->cap.max_inline_data = 0;
 +
 +	/*
 +	 * Return the address of the RWQ as the offset to mmap.
 +	 * See qib_mmap() for details.
 +	 */
 +	if (udata && udata->outlen >= sizeof(__u64)) {
 +		if (!qp->r_rq.wq) {
 +			__u64 offset = 0;
 +
 +			err = ib_copy_to_udata(udata, &offset,
 +					       sizeof(offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		} else {
 +			u32 s = sizeof(struct qib_rwq) + qp->r_rq.size * sz;
 +
 +			qp->ip = qib_create_mmap_info(dev, s,
 +						      ibpd->uobject->context,
 +						      qp->r_rq.wq);
 +			if (!qp->ip) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_ip;
 +			}
 +
 +			err = ib_copy_to_udata(udata, &(qp->ip->offset),
 +					       sizeof(qp->ip->offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		}
 +	}
 +
 +	spin_lock(&dev->n_qps_lock);
 +	if (dev->n_qps_allocated == ib_qib_max_qps) {
 +		spin_unlock(&dev->n_qps_lock);
 +		ret = ERR_PTR(-ENOMEM);
 +		goto bail_ip;
 +	}
  
 +	dev->n_qps_allocated++;
 +	spin_unlock(&dev->n_qps_lock);
 +
 +	if (qp->ip) {
 +		spin_lock_irq(&dev->pending_lock);
 +		list_add(&qp->ip->pending_mmaps, &dev->pending_mmaps);
 +		spin_unlock_irq(&dev->pending_lock);
 +	}
 +
 +	ret = &qp->ibqp;
 +	goto bail;
 +
 +bail_ip:
 +	if (qp->ip)
 +		kref_put(&qp->ip->ref, qib_release_mmap_info);
 +	else
 +		vfree(qp->r_rq.wq);
 +	free_qpn(&dev->qpn_table, qp->ibqp.qp_num);
 +bail_qp:
  	kfree(priv->s_hdr);
  	kfree(priv);
 +bail_qp_hdr:
 +	kfree(qp);
 +bail_swq:
 +	vfree(swq);
 +bail:
 +	return ret;
  }
  
 -void stop_send_queue(struct rvt_qp *qp)
 +/**
 + * qib_destroy_qp - destroy a queue pair
 + * @ibqp: the queue pair to destroy
 + *
 + * Returns 0 on success.
 + *
 + * Note that this can be called while the QP is actively sending or
 + * receiving!
 + */
 +int qib_destroy_qp(struct ib_qp *ibqp)
  {
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_ibdev *dev = to_idev(ibqp->device);
  	struct qib_qp_priv *priv = qp->priv;
  
++<<<<<<< HEAD
 +	/* Make sure HW and driver activity is stopped. */
 +	spin_lock_irq(&qp->s_lock);
 +	if (qp->state != IB_QPS_RESET) {
 +		qp->state = IB_QPS_RESET;
 +		spin_lock(&dev->pending_lock);
 +		if (!list_empty(&priv->iowait))
 +			list_del_init(&priv->iowait);
 +		spin_unlock(&dev->pending_lock);
 +		qp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);
 +		spin_unlock_irq(&qp->s_lock);
 +		cancel_work_sync(&priv->s_work);
 +		del_timer_sync(&qp->s_timer);
 +		wait_event(priv->wait_dma, !atomic_read(&priv->s_dma_busy));
 +		if (priv->s_tx) {
 +			qib_put_txreq(priv->s_tx);
 +			priv->s_tx = NULL;
 +		}
 +		remove_qp(dev, qp);
 +		wait_event(qp->wait, !atomic_read(&qp->refcount));
 +		clear_mr_refs(qp, 1);
 +	} else
 +		spin_unlock_irq(&qp->s_lock);
 +
 +	/* all user's cleaned up, mark it available */
 +	free_qpn(&dev->qpn_table, qp->ibqp.qp_num);
 +	spin_lock(&dev->n_qps_lock);
 +	dev->n_qps_allocated--;
 +	spin_unlock(&dev->n_qps_lock);
 +
 +	if (qp->ip)
 +		kref_put(&qp->ip->ref, qib_release_mmap_info);
 +	else
 +		vfree(qp->r_rq.wq);
 +	vfree(qp->s_wq);
 +	kfree(priv->s_hdr);
 +	kfree(priv);
 +	kfree(qp);
 +	return 0;
++=======
+ 	cancel_work_sync(&priv->s_work);
+ 	del_timer_sync(&qp->s_timer);
++>>>>>>> 08279d5c9424 (staging/rdma/hfi1: use new RNR timer)
  }
  
 -void quiesce_qp(struct rvt_qp *qp)
 +/**
 + * qib_init_qpn_table - initialize the QP number table for a device
 + * @qpt: the QPN table
 + */
 +void qib_init_qpn_table(struct qib_devdata *dd, struct qib_qpn_table *qpt)
  {
 -	struct qib_qp_priv *priv = qp->priv;
 -
 -	wait_event(priv->wait_dma, !atomic_read(&priv->s_dma_busy));
 -	if (priv->s_tx) {
 -		qib_put_txreq(priv->s_tx);
 -		priv->s_tx = NULL;
 -	}
 +	spin_lock_init(&qpt->lock);
 +	qpt->last = 1;          /* start with QPN 2 */
 +	qpt->nmaps = 1;
 +	qpt->mask = dd->qpn_mask;
  }
  
 -void flush_qp_waiters(struct rvt_qp *qp)
 +/**
 + * qib_free_qpn_table - free the QP number table for a device
 + * @qpt: the QPN table
 + */
 +void qib_free_qpn_table(struct qib_qpn_table *qpt)
  {
 -	struct qib_qp_priv *priv = qp->priv;
 -	struct qib_ibdev *dev = to_idev(qp->ibqp.device);
 +	int i;
  
 -	spin_lock(&dev->rdi.pending_lock);
 -	if (!list_empty(&priv->iowait))
 -		list_del_init(&priv->iowait);
 -	spin_unlock(&dev->rdi.pending_lock);
 +	for (i = 0; i < ARRAY_SIZE(qpt->map); i++)
 +		if (qpt->map[i].page)
 +			free_page((unsigned long) qpt->map[i].page);
  }
  
  /**
diff --cc drivers/staging/hfi1/rc.c
index dd57d65aa9b2,2c46491746bb..000000000000
--- a/drivers/staging/hfi1/rc.c
+++ b/drivers/staging/hfi1/rc.c
@@@ -58,9 -60,120 +58,126 @@@
  /* cut down ridiculously long IB macro names */
  #define OP(x) IB_OPCODE_RC_##x
  
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +static void rc_timeout(unsigned long arg);
 +
 +static u32 restart_sge(struct hfi1_sge_state *ss, struct hfi1_swqe *wqe,
++=======
+ /**
+  * hfi1_add_retry_timer - add/start a retry timer
+  * @qp - the QP
+  *
+  * add a retry timer on the QP
+  */
+ static inline void hfi1_add_retry_timer(struct rvt_qp *qp)
+ {
+ 	qp->s_flags |= RVT_S_TIMER;
+ 	/* 4.096 usec. * (1 << qp->timeout) */
+ 	qp->s_timer.expires = jiffies + qp->timeout_jiffies;
+ 	add_timer(&qp->s_timer);
+ }
+ 
+ /**
+  * hfi1_add_rnr_timer - add/start an rnr timer
+  * @qp - the QP
+  * @to - timeout in usecs
+  *
+  * add an rnr timer on the QP
+  */
+ static inline void hfi1_add_rnr_timer(struct rvt_qp *qp, u32 to)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	qp->s_flags |= RVT_S_WAIT_RNR;
+ 	qp->s_timer.expires = jiffies + usecs_to_jiffies(to);
+ 	add_timer(&priv->s_rnr_timer);
+ }
+ 
+ /**
+  * hfi1_mod_retry_timer - mod a retry timer
+  * @qp - the QP
+  *
+  * Modify a potentially already running retry
+  * timer
+  */
+ static inline void hfi1_mod_retry_timer(struct rvt_qp *qp)
+ {
+ 	qp->s_flags |= RVT_S_TIMER;
+ 	/* 4.096 usec. * (1 << qp->timeout) */
+ 	mod_timer(&qp->s_timer, jiffies + qp->timeout_jiffies);
+ }
+ 
+ /**
+  * hfi1_stop_retry_timer - stop a retry timer
+  * @qp - the QP
+  *
+  * stop a retry timer and return if the timer
+  * had been pending.
+  */
+ static inline int hfi1_stop_retry_timer(struct rvt_qp *qp)
+ {
+ 	int rval = 0;
+ 
+ 	/* Remove QP from retry */
+ 	if (qp->s_flags & RVT_S_TIMER) {
+ 		qp->s_flags &= ~RVT_S_TIMER;
+ 		rval = del_timer(&qp->s_timer);
+ 	}
+ 	return rval;
+ }
+ 
+ /**
+  * hfi1_stop_rc_timers - stop all timers
+  * @qp - the QP
+  *
+  * stop any pending timers
+  */
+ void hfi1_stop_rc_timers(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	/* Remove QP from all timers */
+ 	if (qp->s_flags & (RVT_S_TIMER | RVT_S_WAIT_RNR)) {
+ 		qp->s_flags &= ~(RVT_S_TIMER | RVT_S_WAIT_RNR);
+ 		del_timer(&qp->s_timer);
+ 		del_timer(&priv->s_rnr_timer);
+ 	}
+ }
+ 
+ /**
+  * hfi1_stop_rnr_timer - stop an rnr timer
+  * @qp - the QP
+  *
+  * stop an rnr timer and return if the timer
+  * had been pending.
+  */
+ static inline int hfi1_stop_rnr_timer(struct rvt_qp *qp)
+ {
+ 	int rval = 0;
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	/* Remove QP from rnr timer */
+ 	if (qp->s_flags & RVT_S_WAIT_RNR) {
+ 		qp->s_flags &= ~RVT_S_WAIT_RNR;
+ 		rval = del_timer(&priv->s_rnr_timer);
+ 	}
+ 	return rval;
+ }
+ 
+ /**
+  * hfi1_del_timers_sync - wait for any timeout routines to exit
+  * @qp - the QP
+  */
+ void hfi1_del_timers_sync(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	del_timer_sync(&qp->s_timer);
+ 	del_timer_sync(&priv->s_rnr_timer);
+ }
+ 
+ static u32 restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe,
++>>>>>>> 08279d5c9424 (staging/rdma/hfi1: use new RNR timer):drivers/staging/rdma/hfi1/rc.c
  		       u32 psn, u32 pmtu)
  {
  	u32 len;
@@@ -915,9 -1019,9 +1032,9 @@@ static void restart_rc(struct hfi1_qp *
  /*
   * This is called from s_timer for missing responses.
   */
- static void rc_timeout(unsigned long arg)
+ void hfi1_rc_timeout(unsigned long arg)
  {
 -	struct rvt_qp *qp = (struct rvt_qp *)arg;
 +	struct hfi1_qp *qp = (struct hfi1_qp *)arg;
  	struct hfi1_ibport *ibp;
  	unsigned long flags;
  
* Unmerged path drivers/staging/rdma/hfi1/qp.c
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
* Unmerged path drivers/infiniband/hw/qib/qib_qp.c
diff --git a/drivers/infiniband/sw/rdmavt/qp.c b/drivers/infiniband/sw/rdmavt/qp.c
index e8d0da89ea8e..14036b4079e2 100644
--- a/drivers/infiniband/sw/rdmavt/qp.c
+++ b/drivers/infiniband/sw/rdmavt/qp.c
@@ -405,7 +405,6 @@ void rvt_reset_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp,
 
 		/* Stop the send queue and the retry timer */
 		rdi->driver_f.stop_send_queue(qp);
-		del_timer_sync(&qp->s_timer);
 
 		/* Wait for things to stop */
 		rdi->driver_f.quiesce_qp(qp);
* Unmerged path drivers/staging/hfi1/rc.c
* Unmerged path drivers/staging/rdma/hfi1/qp.c
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
