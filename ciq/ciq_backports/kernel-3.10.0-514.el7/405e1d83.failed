ftrace: Synchronize setting function_trace_op with ftrace_trace_function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Steven Rostedt (Red Hat) <rostedt@goodmis.org>
commit 405e1d834807e51b2ebd3dea81cb51e53fb61504
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/405e1d83.failed

ftrace_trace_function is a variable that holds what function will be called
directly by the assembly code (mcount). If just a single function is
registered and it handles recursion itself, then the assembly will call that
function directly without any helper function. It also passes in the
ftrace_op that was registered with the callback. The ftrace_op to send is
stored in the function_trace_op variable.

The ftrace_trace_function and function_trace_op needs to be coordinated such
that the called callback wont be called with the wrong ftrace_op, otherwise
bad things can happen if it expected a different op. Luckily, there's no
callback that doesn't use the helper functions that requires this. But
there soon will be and this needs to be fixed.

Use a set_function_trace_op to store the ftrace_op to set the
function_trace_op to when it is safe to do so (during the update function
within the breakpoint or stop machine calls). Or if dynamic ftrace is not
being used (static tracing) then we have to do a bit more synchronization
when the ftrace_trace_function is set as that takes affect immediately
(as oppose to dynamic ftrace doing it with the modification of the trampoline).

	Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
(cherry picked from commit 405e1d834807e51b2ebd3dea81cb51e53fb61504)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/trace/ftrace.c
diff --cc kernel/trace/ftrace.c
index 979b7eac7331,0ffb811cbb1f..000000000000
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@@ -80,8 -80,13 +80,10 @@@ static struct ftrace_ops ftrace_list_en
  int ftrace_enabled __read_mostly;
  static int last_ftrace_enabled;
  
 -/* Quick disabling of function tracer. */
 -int function_trace_stop __read_mostly;
 -
  /* Current function tracing op */
  struct ftrace_ops *function_trace_op __read_mostly = &ftrace_list_end;
+ /* What to set function_trace_op to */
+ static struct ftrace_ops *set_function_trace_op;
  
  /* List for set_ftrace_pid's pids. */
  LIST_HEAD(ftrace_pids);
@@@ -1938,8 -2030,14 +2000,18 @@@ void ftrace_modify_all_code(int command
  	else if (command & FTRACE_DISABLE_CALLS)
  		ftrace_replace_code(0);
  
++<<<<<<< HEAD
 +	if (command & FTRACE_UPDATE_TRACE_FUNC)
++=======
+ 	if (update && ftrace_trace_function != ftrace_ops_list_func) {
+ 		function_trace_op = set_function_trace_op;
+ 		smp_wmb();
+ 		/* If irqs are disabled, we are in stop machine */
+ 		if (!irqs_disabled())
+ 			smp_call_function(ftrace_sync_ipi, NULL, 1);
++>>>>>>> 405e1d834807 (ftrace: Synchronize setting function_trace_op with ftrace_trace_function)
  		ftrace_update_ftrace_func(ftrace_trace_function);
+ 	}
  
  	if (command & FTRACE_START_FUNC_RET)
  		ftrace_enable_ftrace_graph_caller();
* Unmerged path kernel/trace/ftrace.c
