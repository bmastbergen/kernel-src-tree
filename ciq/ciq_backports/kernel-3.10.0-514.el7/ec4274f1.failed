staging/rdma/hfi1: Remove modify queue pair from hfi1

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [infiniband] rdma/hfi1: Remove modify queue pair from hfi1 (Alex Estrin) [1272062 1273170]
Rebuild_FUZZ: 91.84%
commit-author Dennis Dalessandro <dennis.dalessandro@intel.com>
commit ec4274f1aeb5e5012c1e46ba11ceef7767af8b3d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ec4274f1.failed

In addition to removing the modify queue pair verb from hfi1 we also
remove ancillary functions which existed only for modify queue pair and
are also already present in hfi1.

	Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit ec4274f1aeb5e5012c1e46ba11ceef7767af8b3d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/driver.c
#	drivers/staging/hfi1/qp.c
#	drivers/staging/hfi1/qp.h
#	drivers/staging/hfi1/ruc.c
#	drivers/staging/hfi1/trace.h
#	drivers/staging/hfi1/uc.c
#	drivers/staging/hfi1/ud.c
#	drivers/staging/hfi1/verbs.c
#	drivers/staging/rdma/hfi1/verbs.h
diff --cc drivers/staging/hfi1/driver.c
index fca20e92c79b,d848cc01f07a..000000000000
--- a/drivers/staging/hfi1/driver.c
+++ b/drivers/staging/hfi1/driver.c
@@@ -299,9 -318,9 +301,15 @@@ static void rcv_hdrerr(struct hfi1_ctxt
  			goto drop;
  
  		/* Get the destination QP number. */
++<<<<<<< HEAD:drivers/staging/hfi1/driver.c
 +		qp_num = be32_to_cpu(ohdr->bth[1]) & HFI1_QPN_MASK;
 +		if (lid < HFI1_MULTICAST_LID_BASE) {
 +			struct hfi1_qp *qp;
++=======
+ 		qp_num = be32_to_cpu(ohdr->bth[1]) & RVT_QPN_MASK;
+ 		if (lid < be16_to_cpu(IB_MULTICAST_LID_BASE)) {
+ 			struct rvt_qp *qp;
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/driver.c
  			unsigned long flags;
  
  			rcu_read_lock();
diff --cc drivers/staging/hfi1/qp.c
index 9ffed6e14d8e,1e6ca4fb7925..000000000000
--- a/drivers/staging/hfi1/qp.c
+++ b/drivers/staging/hfi1/qp.c
@@@ -118,433 -117,7 +120,437 @@@ static const u16 credit_table[31] = 
  	32768                   /* 1E */
  };
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +static void get_map_page(struct hfi1_qpn_table *qpt, struct qpn_map *map)
 +{
 +	unsigned long page = get_zeroed_page(GFP_KERNEL);
 +
 +	/*
 +	 * Free the page if someone raced with us installing it.
 +	 */
 +
 +	spin_lock(&qpt->lock);
 +	if (map->page)
 +		free_page(page);
 +	else
 +		map->page = (void *)page;
 +	spin_unlock(&qpt->lock);
 +}
 +
 +/*
 + * Allocate the next available QPN or
 + * zero/one for QP type IB_QPT_SMI/IB_QPT_GSI.
 + */
 +static int alloc_qpn(struct hfi1_devdata *dd, struct hfi1_qpn_table *qpt,
 +		     enum ib_qp_type type, u8 port)
 +{
 +	u32 i, offset, max_scan, qpn;
 +	struct qpn_map *map;
 +	u32 ret;
 +
 +	if (type == IB_QPT_SMI || type == IB_QPT_GSI) {
 +		unsigned n;
 +
 +		ret = type == IB_QPT_GSI;
 +		n = 1 << (ret + 2 * (port - 1));
 +		spin_lock(&qpt->lock);
 +		if (qpt->flags & n)
 +			ret = -EINVAL;
 +		else
 +			qpt->flags |= n;
 +		spin_unlock(&qpt->lock);
 +		goto bail;
 +	}
 +
 +	qpn = qpt->last + qpt->incr;
 +	if (qpn >= QPN_MAX)
 +		qpn = qpt->incr | ((qpt->last & 1) ^ 1);
 +	/* offset carries bit 0 */
 +	offset = qpn & BITS_PER_PAGE_MASK;
 +	map = &qpt->map[qpn / BITS_PER_PAGE];
 +	max_scan = qpt->nmaps - !offset;
 +	for (i = 0;;) {
 +		if (unlikely(!map->page)) {
 +			get_map_page(qpt, map);
 +			if (unlikely(!map->page))
 +				break;
 +		}
 +		do {
 +			if (!test_and_set_bit(offset, map->page)) {
 +				qpt->last = qpn;
 +				ret = qpn;
 +				goto bail;
 +			}
 +			offset += qpt->incr;
 +			/*
 +			 * This qpn might be bogus if offset >= BITS_PER_PAGE.
 +			 * That is OK.   It gets re-assigned below
 +			 */
 +			qpn = mk_qpn(qpt, map, offset);
 +		} while (offset < BITS_PER_PAGE && qpn < QPN_MAX);
 +		/*
 +		 * In order to keep the number of pages allocated to a
 +		 * minimum, we scan the all existing pages before increasing
 +		 * the size of the bitmap table.
 +		 */
 +		if (++i > max_scan) {
 +			if (qpt->nmaps == QPNMAP_ENTRIES)
 +				break;
 +			map = &qpt->map[qpt->nmaps++];
 +			/* start at incr with current bit 0 */
 +			offset = qpt->incr | (offset & 1);
 +		} else if (map < &qpt->map[qpt->nmaps]) {
 +			++map;
 +			/* start at incr with current bit 0 */
 +			offset = qpt->incr | (offset & 1);
 +		} else {
 +			map = &qpt->map[0];
 +			/* wrap to first map page, invert bit 0 */
 +			offset = qpt->incr | ((offset & 1) ^ 1);
 +		}
 +		/* there can be no bits at shift and below */
 +		WARN_ON(offset & (dd->qos_shift - 1));
 +		qpn = mk_qpn(qpt, map, offset);
 +	}
 +
 +	ret = -ENOMEM;
 +
 +bail:
 +	return ret;
 +}
 +
 +static void free_qpn(struct hfi1_qpn_table *qpt, u32 qpn)
 +{
 +	struct qpn_map *map;
 +
 +	map = qpt->map + qpn / BITS_PER_PAGE;
 +	if (map->page)
 +		clear_bit(qpn & BITS_PER_PAGE_MASK, map->page);
 +}
 +
 +/*
 + * Put the QP into the hash table.
 + * The hash table holds a reference to the QP.
 + */
 +static void insert_qp(struct hfi1_ibdev *dev, struct hfi1_qp *qp)
 +{
 +	struct hfi1_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);
 +	unsigned long flags;
 +
 +	atomic_inc(&qp->refcount);
 +	spin_lock_irqsave(&dev->qp_dev->qpt_lock, flags);
 +
 +	if (qp->ibqp.qp_num <= 1) {
 +		rcu_assign_pointer(ibp->qp[qp->ibqp.qp_num], qp);
 +	} else {
 +		u32 n = qpn_hash(dev->qp_dev, qp->ibqp.qp_num);
 +
 +		qp->next = dev->qp_dev->qp_table[n];
 +		rcu_assign_pointer(dev->qp_dev->qp_table[n], qp);
 +		trace_hfi1_qpinsert(qp, n);
 +	}
 +
 +	spin_unlock_irqrestore(&dev->qp_dev->qpt_lock, flags);
 +}
 +
 +/*
 + * Remove the QP from the table so it can't be found asynchronously by
 + * the receive interrupt routine.
 + */
 +static void remove_qp(struct hfi1_ibdev *dev, struct hfi1_qp *qp)
 +{
 +	struct hfi1_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);
 +	u32 n = qpn_hash(dev->qp_dev, qp->ibqp.qp_num);
 +	unsigned long flags;
 +	int removed = 1;
 +
 +	spin_lock_irqsave(&dev->qp_dev->qpt_lock, flags);
 +
 +	if (rcu_dereference_protected(ibp->qp[0],
 +			lockdep_is_held(&dev->qp_dev->qpt_lock)) == qp) {
 +		RCU_INIT_POINTER(ibp->qp[0], NULL);
 +	} else if (rcu_dereference_protected(ibp->qp[1],
 +			lockdep_is_held(&dev->qp_dev->qpt_lock)) == qp) {
 +		RCU_INIT_POINTER(ibp->qp[1], NULL);
 +	} else {
 +		struct hfi1_qp *q;
 +		struct hfi1_qp __rcu **qpp;
 +
 +		removed = 0;
 +		qpp = &dev->qp_dev->qp_table[n];
 +		for (; (q = rcu_dereference_protected(*qpp,
 +				lockdep_is_held(&dev->qp_dev->qpt_lock)))
 +					!= NULL;
 +				qpp = &q->next)
 +			if (q == qp) {
 +				RCU_INIT_POINTER(*qpp,
 +				 rcu_dereference_protected(qp->next,
 +				 lockdep_is_held(&dev->qp_dev->qpt_lock)));
 +				removed = 1;
 +				trace_hfi1_qpremove(qp, n);
 +				break;
 +			}
 +	}
 +
 +	spin_unlock_irqrestore(&dev->qp_dev->qpt_lock, flags);
 +	if (removed) {
 +		synchronize_rcu();
 +		if (atomic_dec_and_test(&qp->refcount))
 +			wake_up(&qp->wait);
 +	}
 +}
 +
 +/**
 + * free_all_qps - check for QPs still in use
 + * @qpt: the QP table to empty
 + *
 + * There should not be any QPs still in use.
 + * Free memory for table.
 + */
 +static unsigned free_all_qps(struct hfi1_devdata *dd)
 +{
 +	struct hfi1_ibdev *dev = &dd->verbs_dev;
 +	unsigned long flags;
 +	struct hfi1_qp *qp;
 +	unsigned n, qp_inuse = 0;
 +
 +	for (n = 0; n < dd->num_pports; n++) {
 +		struct hfi1_ibport *ibp = &dd->pport[n].ibport_data;
 +
 +		if (!hfi1_mcast_tree_empty(ibp))
 +			qp_inuse++;
 +		rcu_read_lock();
 +		if (rcu_dereference(ibp->qp[0]))
 +			qp_inuse++;
 +		if (rcu_dereference(ibp->qp[1]))
 +			qp_inuse++;
 +		rcu_read_unlock();
 +	}
 +
 +	if (!dev->qp_dev)
 +		goto bail;
 +	spin_lock_irqsave(&dev->qp_dev->qpt_lock, flags);
 +	for (n = 0; n < dev->qp_dev->qp_table_size; n++) {
 +		qp = rcu_dereference_protected(dev->qp_dev->qp_table[n],
 +			lockdep_is_held(&dev->qp_dev->qpt_lock));
 +		RCU_INIT_POINTER(dev->qp_dev->qp_table[n], NULL);
 +
 +		for (; qp; qp = rcu_dereference_protected(qp->next,
 +				lockdep_is_held(&dev->qp_dev->qpt_lock)))
 +			qp_inuse++;
 +	}
 +	spin_unlock_irqrestore(&dev->qp_dev->qpt_lock, flags);
 +	synchronize_rcu();
 +bail:
 +	return qp_inuse;
 +}
 +
 +/**
 + * reset_qp - initialize the QP state to the reset state
 + * @qp: the QP to reset
 + * @type: the QP type
 + */
 +static void reset_qp(struct hfi1_qp *qp, enum ib_qp_type type)
 +{
 +	struct hfi1_qp_priv *priv = qp->priv;
 +	qp->remote_qpn = 0;
 +	qp->qkey = 0;
 +	qp->qp_access_flags = 0;
 +	iowait_init(
 +		&priv->s_iowait,
 +		1,
 +		hfi1_do_send,
 +		iowait_sleep,
 +		iowait_wakeup);
 +	qp->s_flags &= HFI1_S_SIGNAL_REQ_WR;
 +	qp->s_hdrwords = 0;
 +	qp->s_wqe = NULL;
 +	qp->s_draining = 0;
 +	qp->s_next_psn = 0;
 +	qp->s_last_psn = 0;
 +	qp->s_sending_psn = 0;
 +	qp->s_sending_hpsn = 0;
 +	qp->s_psn = 0;
 +	qp->r_psn = 0;
 +	qp->r_msn = 0;
 +	if (type == IB_QPT_RC) {
 +		qp->s_state = IB_OPCODE_RC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_RC_SEND_LAST;
 +	} else {
 +		qp->s_state = IB_OPCODE_UC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_UC_SEND_LAST;
 +	}
 +	qp->s_ack_state = IB_OPCODE_RC_ACKNOWLEDGE;
 +	qp->r_nak_state = 0;
 +	priv->r_adefered = 0;
 +	qp->r_aflags = 0;
 +	qp->r_flags = 0;
 +	qp->s_head = 0;
 +	qp->s_tail = 0;
 +	qp->s_cur = 0;
 +	qp->s_acked = 0;
 +	qp->s_last = 0;
 +	qp->s_ssn = 1;
 +	qp->s_lsn = 0;
 +	clear_ahg(qp);
 +	qp->s_mig_state = IB_MIG_MIGRATED;
 +	memset(qp->s_ack_queue, 0, sizeof(qp->s_ack_queue));
 +	qp->r_head_ack_queue = 0;
 +	qp->s_tail_ack_queue = 0;
 +	qp->s_num_rd_atomic = 0;
 +	if (qp->r_rq.wq) {
 +		qp->r_rq.wq->head = 0;
 +		qp->r_rq.wq->tail = 0;
 +	}
 +	qp->r_sge.num_sge = 0;
 +}
 +
 +static void clear_mr_refs(struct hfi1_qp *qp, int clr_sends)
 +{
 +	unsigned n;
 +
 +	if (test_and_clear_bit(HFI1_R_REWIND_SGE, &qp->r_aflags))
 +		hfi1_put_ss(&qp->s_rdma_read_sge);
 +
 +	hfi1_put_ss(&qp->r_sge);
 +
 +	if (clr_sends) {
 +		while (qp->s_last != qp->s_head) {
 +			struct hfi1_swqe *wqe = get_swqe_ptr(qp, qp->s_last);
 +			unsigned i;
 +
 +			for (i = 0; i < wqe->wr.num_sge; i++) {
 +				struct hfi1_sge *sge = &wqe->sg_list[i];
 +
 +				hfi1_put_mr(sge->mr);
 +			}
 +			if (qp->ibqp.qp_type == IB_QPT_UD ||
 +			    qp->ibqp.qp_type == IB_QPT_SMI ||
 +			    qp->ibqp.qp_type == IB_QPT_GSI)
 +				atomic_dec(&to_iah(wqe->wr.wr.ud.ah)->refcount);
 +			if (++qp->s_last >= qp->s_size)
 +				qp->s_last = 0;
 +		}
 +		if (qp->s_rdma_mr) {
 +			hfi1_put_mr(qp->s_rdma_mr);
 +			qp->s_rdma_mr = NULL;
 +		}
 +	}
 +
 +	if (qp->ibqp.qp_type != IB_QPT_RC)
 +		return;
 +
 +	for (n = 0; n < ARRAY_SIZE(qp->s_ack_queue); n++) {
 +		struct hfi1_ack_entry *e = &qp->s_ack_queue[n];
 +
 +		if (e->opcode == IB_OPCODE_RC_RDMA_READ_REQUEST &&
 +		    e->rdma_sge.mr) {
 +			hfi1_put_mr(e->rdma_sge.mr);
 +			e->rdma_sge.mr = NULL;
 +		}
 +	}
 +}
 +
 +/**
 + * hfi1_error_qp - put a QP into the error state
 + * @qp: the QP to put into the error state
 + * @err: the receive completion error to signal if a RWQE is active
 + *
 + * Flushes both send and receive work queues.
 + * Returns true if last WQE event should be generated.
 + * The QP r_lock and s_lock should be held and interrupts disabled.
 + * If we are already in error state, just return.
 + */
 +int hfi1_error_qp(struct hfi1_qp *qp, enum ib_wc_status err)
 +{
 +	struct hfi1_ibdev *dev = to_idev(qp->ibqp.device);
 +	struct hfi1_qp_priv *priv = qp->priv;
 +	struct ib_wc wc;
 +	int ret = 0;
 +
 +	if (qp->state == IB_QPS_ERR || qp->state == IB_QPS_RESET)
 +		goto bail;
 +
 +	qp->state = IB_QPS_ERR;
 +
 +	if (qp->s_flags & (HFI1_S_TIMER | HFI1_S_WAIT_RNR)) {
 +		qp->s_flags &= ~(HFI1_S_TIMER | HFI1_S_WAIT_RNR);
 +		del_timer(&qp->s_timer);
 +	}
 +
 +	if (qp->s_flags & HFI1_S_ANY_WAIT_SEND)
 +		qp->s_flags &= ~HFI1_S_ANY_WAIT_SEND;
 +
 +	write_seqlock(&dev->iowait_lock);
 +	if (!list_empty(&priv->s_iowait.list) && !(qp->s_flags & HFI1_S_BUSY)) {
 +		qp->s_flags &= ~HFI1_S_ANY_WAIT_IO;
 +		list_del_init(&priv->s_iowait.list);
 +		if (atomic_dec_and_test(&qp->refcount))
 +			wake_up(&qp->wait);
 +	}
 +	write_sequnlock(&dev->iowait_lock);
 +
 +	if (!(qp->s_flags & HFI1_S_BUSY)) {
 +		qp->s_hdrwords = 0;
 +		if (qp->s_rdma_mr) {
 +			hfi1_put_mr(qp->s_rdma_mr);
 +			qp->s_rdma_mr = NULL;
 +		}
 +		flush_tx_list(qp);
 +	}
 +
 +	/* Schedule the sending tasklet to drain the send work queue. */
 +	if (qp->s_last != qp->s_head)
 +		hfi1_schedule_send(qp);
 +
 +	clear_mr_refs(qp, 0);
 +
 +	memset(&wc, 0, sizeof(wc));
 +	wc.qp = &qp->ibqp;
 +	wc.opcode = IB_WC_RECV;
 +
 +	if (test_and_clear_bit(HFI1_R_WRID_VALID, &qp->r_aflags)) {
 +		wc.wr_id = qp->r_wr_id;
 +		wc.status = err;
 +		hfi1_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
 +	}
 +	wc.status = IB_WC_WR_FLUSH_ERR;
 +
 +	if (qp->r_rq.wq) {
 +		struct hfi1_rwq *wq;
 +		u32 head;
 +		u32 tail;
 +
 +		spin_lock(&qp->r_rq.lock);
 +
 +		/* sanity check pointers before trusting them */
 +		wq = qp->r_rq.wq;
 +		head = wq->head;
 +		if (head >= qp->r_rq.size)
 +			head = 0;
 +		tail = wq->tail;
 +		if (tail >= qp->r_rq.size)
 +			tail = 0;
 +		while (tail != head) {
 +			wc.wr_id = get_rwqe_ptr(&qp->r_rq, tail)->wr_id;
 +			if (++tail >= qp->r_rq.size)
 +				tail = 0;
 +			hfi1_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
 +		}
 +		wq->tail = tail;
 +
 +		spin_unlock(&qp->r_rq.lock);
 +	} else if (qp->ibqp.event_handler)
 +		ret = 1;
 +
 +bail:
 +	return ret;
 +}
 +
 +static void flush_tx_list(struct hfi1_qp *qp)
++=======
+ static void flush_tx_list(struct rvt_qp *qp)
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  {
  	struct hfi1_qp_priv *priv = qp->priv;
  
@@@ -602,48 -175,15 +608,35 @@@ static inline int verbs_mtu_enum_to_int
  	return ib_mtu_enum_to_int(mtu);
  }
  
- 
- /**
-  * hfi1_modify_qp - modify the attributes of a queue pair
-  * @ibqp: the queue pair who's attributes we're modifying
-  * @attr: the new attributes
-  * @attr_mask: the mask of attributes to modify
-  * @udata: user data for libibverbs.so
-  *
-  * Returns 0 on success, otherwise returns an errno.
-  */
- int hfi1_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
- 		   int attr_mask, struct ib_udata *udata)
+ int hfi1_check_modify_qp(struct rvt_qp *qp, struct ib_qp_attr *attr,
+ 			 int attr_mask, struct ib_udata *udata)
  {
+ 	struct ib_qp *ibqp = &qp->ibqp;
  	struct hfi1_ibdev *dev = to_idev(ibqp->device);
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +	struct hfi1_qp *qp = to_iqp(ibqp);
 +	struct hfi1_qp_priv *priv = qp->priv;
 +	enum ib_qp_state cur_state, new_state;
 +	struct ib_event ev;
 +	int lastwqe = 0;
 +	int mig = 0;
 +	int ret;
 +	u32 pmtu = 0; /* for gcc warning only */
++=======
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  	struct hfi1_devdata *dd = dd_from_dev(dev);
- 
- 	spin_lock_irq(&qp->r_lock);
- 	spin_lock(&qp->s_lock);
- 
- 	cur_state = attr_mask & IB_QP_CUR_STATE ?
- 		attr->cur_qp_state : qp->state;
- 	new_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;
- 
- 	if (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type,
- 				attr_mask, IB_LINK_LAYER_UNSPECIFIED))
- 		goto inval;
+ 	u8 sc;
  
  	if (attr_mask & IB_QP_AV) {
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +		u8 sc;
 +
 +		if (attr->ah_attr.dlid >= HFI1_MULTICAST_LID_BASE)
 +			goto inval;
 +		if (hfi1_check_ah(qp->ibqp.device, &attr->ah_attr))
 +			goto inval;
++=======
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  		sc = ah_to_sc(ibqp->device, &attr->ah_attr);
  		if (!qp_to_sdma_engine(qp, sc) &&
  		    dd->flags & HFI1_HAS_SEND_DMA)
@@@ -651,264 -191,33 +644,277 @@@
  	}
  
  	if (attr_mask & IB_QP_ALT_PATH) {
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +		u8 sc;
 +
 +		if (attr->alt_ah_attr.dlid >= HFI1_MULTICAST_LID_BASE)
 +			goto inval;
 +		if (hfi1_check_ah(qp->ibqp.device, &attr->alt_ah_attr))
 +			goto inval;
 +		if (attr->alt_pkey_index >= hfi1_get_npkeys(dd))
 +			goto inval;
++=======
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  		sc = ah_to_sc(ibqp->device, &attr->alt_ah_attr);
  		if (!qp_to_sdma_engine(qp, sc) &&
  		    dd->flags & HFI1_HAS_SEND_DMA)
- 			goto inval;
+ 			return -EINVAL;
  	}
  
- 	if (attr_mask & IB_QP_PKEY_INDEX)
- 		if (attr->pkey_index >= hfi1_get_npkeys(dd))
- 			goto inval;
+ 	return 0;
+ }
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +	if (attr_mask & IB_QP_MIN_RNR_TIMER)
 +		if (attr->min_rnr_timer > 31)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_PORT)
 +		if (qp->ibqp.qp_type == IB_QPT_SMI ||
 +		    qp->ibqp.qp_type == IB_QPT_GSI ||
 +		    attr->port_num == 0 ||
 +		    attr->port_num > ibqp->device->phys_port_cnt)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_DEST_QPN)
 +		if (attr->dest_qp_num > HFI1_QPN_MASK)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_RETRY_CNT)
 +		if (attr->retry_cnt > 7)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_RNR_RETRY)
 +		if (attr->rnr_retry > 7)
 +			goto inval;
 +
 +	/*
 +	 * Don't allow invalid path_mtu values.  OK to set greater
 +	 * than the active mtu (or even the max_cap, if we have tuned
 +	 * that to a small mtu.  We'll set qp->path_mtu
 +	 * to the lesser of requested attribute mtu and active,
 +	 * for packetizing messages.
 +	 * Note that the QP port has to be set in INIT and MTU in RTR.
 +	 */
 +	if (attr_mask & IB_QP_PATH_MTU) {
 +		int mtu, pidx = qp->port_num - 1;
 +
 +		dd = dd_from_dev(dev);
 +		mtu = verbs_mtu_enum_to_int(ibqp->device, attr->path_mtu);
 +		if (mtu == -1)
 +			goto inval;
 +
 +		if (mtu > dd->pport[pidx].ibmtu)
 +			pmtu = mtu_to_enum(dd->pport[pidx].ibmtu, IB_MTU_2048);
 +		else
 +			pmtu = attr->path_mtu;
 +	}
 +
 +	if (attr_mask & IB_QP_PATH_MIG_STATE) {
 +		if (attr->path_mig_state == IB_MIG_REARM) {
 +			if (qp->s_mig_state == IB_MIG_ARMED)
 +				goto inval;
 +			if (new_state != IB_QPS_RTS)
 +				goto inval;
 +		} else if (attr->path_mig_state == IB_MIG_MIGRATED) {
 +			if (qp->s_mig_state == IB_MIG_REARM)
 +				goto inval;
 +			if (new_state != IB_QPS_RTS && new_state != IB_QPS_SQD)
 +				goto inval;
 +			if (qp->s_mig_state == IB_MIG_ARMED)
 +				mig = 1;
 +		} else
 +			goto inval;
 +	}
 +
 +	if (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)
 +		if (attr->max_dest_rd_atomic > HFI1_MAX_RDMA_ATOMIC)
 +			goto inval;
 +
 +	switch (new_state) {
 +	case IB_QPS_RESET:
 +		if (qp->state != IB_QPS_RESET) {
 +			qp->state = IB_QPS_RESET;
 +			flush_iowait(qp);
 +			qp->s_flags &= ~(HFI1_S_TIMER | HFI1_S_ANY_WAIT);
 +			spin_unlock(&qp->s_lock);
 +			spin_unlock_irq(&qp->r_lock);
 +			/* Stop the sending work queue and retry timer */
 +			cancel_work_sync(&priv->s_iowait.iowork);
 +			del_timer_sync(&qp->s_timer);
 +			iowait_sdma_drain(&priv->s_iowait);
 +			flush_tx_list(qp);
 +			remove_qp(dev, qp);
 +			wait_event(qp->wait, !atomic_read(&qp->refcount));
 +			spin_lock_irq(&qp->r_lock);
 +			spin_lock(&qp->s_lock);
 +			clear_mr_refs(qp, 1);
 +			clear_ahg(qp);
 +			reset_qp(qp, ibqp->qp_type);
 +		}
 +		break;
 +
 +	case IB_QPS_RTR:
 +		/* Allow event to re-trigger if QP set to RTR more than once */
 +		qp->r_flags &= ~HFI1_R_COMM_EST;
 +		qp->state = new_state;
 +		break;
 +
 +	case IB_QPS_SQD:
 +		qp->s_draining = qp->s_last != qp->s_cur;
 +		qp->state = new_state;
 +		break;
 +
 +	case IB_QPS_SQE:
 +		if (qp->ibqp.qp_type == IB_QPT_RC)
 +			goto inval;
 +		qp->state = new_state;
 +		break;
 +
 +	case IB_QPS_ERR:
 +		lastwqe = hfi1_error_qp(qp, IB_WC_WR_FLUSH_ERR);
 +		break;
 +
 +	default:
 +		qp->state = new_state;
 +		break;
 +	}
 +
 +	if (attr_mask & IB_QP_PKEY_INDEX)
 +		qp->s_pkey_index = attr->pkey_index;
 +
 +	if (attr_mask & IB_QP_PORT)
 +		qp->port_num = attr->port_num;
 +
 +	if (attr_mask & IB_QP_DEST_QPN)
 +		qp->remote_qpn = attr->dest_qp_num;
 +
 +	if (attr_mask & IB_QP_SQ_PSN) {
 +		qp->s_next_psn = attr->sq_psn & PSN_MODIFY_MASK;
 +		qp->s_psn = qp->s_next_psn;
 +		qp->s_sending_psn = qp->s_next_psn;
 +		qp->s_last_psn = qp->s_next_psn - 1;
 +		qp->s_sending_hpsn = qp->s_last_psn;
 +	}
 +
 +	if (attr_mask & IB_QP_RQ_PSN)
 +		qp->r_psn = attr->rq_psn & PSN_MODIFY_MASK;
 +
 +	if (attr_mask & IB_QP_ACCESS_FLAGS)
 +		qp->qp_access_flags = attr->qp_access_flags;
++=======
+ void hfi1_modify_qp(struct rvt_qp *qp, struct ib_qp_attr *attr,
+ 		    int attr_mask, struct ib_udata *udata)
+ {
+ 	struct ib_qp *ibqp = &qp->ibqp;
+ 	struct hfi1_qp_priv *priv = qp->priv;
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  
  	if (attr_mask & IB_QP_AV) {
- 		qp->remote_ah_attr = attr->ah_attr;
- 		qp->s_srate = attr->ah_attr.static_rate;
- 		qp->srate_mbps = ib_rate_to_mbps(qp->s_srate);
  		priv->s_sc = ah_to_sc(ibqp->device, &qp->remote_ah_attr);
  		priv->s_sde = qp_to_sdma_engine(qp, priv->s_sc);
  	}
  
- 	if (attr_mask & IB_QP_ALT_PATH) {
- 		qp->alt_ah_attr = attr->alt_ah_attr;
- 		qp->s_alt_pkey_index = attr->alt_pkey_index;
+ 	if (attr_mask & IB_QP_PATH_MIG_STATE &&
+ 	    attr->path_mig_state == IB_MIG_MIGRATED &&
+ 	    qp->s_mig_state == IB_MIG_ARMED) {
+ 		qp->s_flags |= RVT_S_AHG_CLEAR;
+ 		priv->s_sc = ah_to_sc(ibqp->device, &qp->remote_ah_attr);
+ 		priv->s_sde = qp_to_sdma_engine(qp, priv->s_sc);
  	}
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +
 +	if (attr_mask & IB_QP_PATH_MIG_STATE) {
 +		qp->s_mig_state = attr->path_mig_state;
 +		if (mig) {
 +			qp->remote_ah_attr = qp->alt_ah_attr;
 +			qp->port_num = qp->alt_ah_attr.port_num;
 +			qp->s_pkey_index = qp->s_alt_pkey_index;
 +			qp->s_flags |= HFI1_S_AHG_CLEAR;
 +			priv->s_sc = ah_to_sc(ibqp->device, &qp->remote_ah_attr);
 +			priv->s_sde = qp_to_sdma_engine(qp, priv->s_sc);
 +		}
 +	}
 +
 +	if (attr_mask & IB_QP_PATH_MTU) {
 +		struct hfi1_ibport *ibp;
 +		u8 sc, vl;
 +		u32 mtu;
 +
 +		dd = dd_from_dev(dev);
 +		ibp = &dd->pport[qp->port_num - 1].ibport_data;
 +
 +		sc = ibp->sl_to_sc[qp->remote_ah_attr.sl];
 +		vl = sc_to_vlt(dd, sc);
 +
 +		mtu = verbs_mtu_enum_to_int(ibqp->device, pmtu);
 +		if (vl < PER_VL_SEND_CONTEXTS)
 +			mtu = min_t(u32, mtu, dd->vld[vl].mtu);
 +		pmtu = mtu_to_enum(mtu, OPA_MTU_8192);
 +
 +		qp->path_mtu = pmtu;
 +		qp->pmtu = mtu;
 +	}
 +
 +	if (attr_mask & IB_QP_RETRY_CNT) {
 +		qp->s_retry_cnt = attr->retry_cnt;
 +		qp->s_retry = attr->retry_cnt;
 +	}
 +
 +	if (attr_mask & IB_QP_RNR_RETRY) {
 +		qp->s_rnr_retry_cnt = attr->rnr_retry;
 +		qp->s_rnr_retry = attr->rnr_retry;
 +	}
 +
 +	if (attr_mask & IB_QP_MIN_RNR_TIMER)
 +		qp->r_min_rnr_timer = attr->min_rnr_timer;
 +
 +	if (attr_mask & IB_QP_TIMEOUT) {
 +		qp->timeout = attr->timeout;
 +		qp->timeout_jiffies =
 +			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
 +				1000UL);
 +	}
 +
 +	if (attr_mask & IB_QP_QKEY)
 +		qp->qkey = attr->qkey;
 +
 +	if (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)
 +		qp->r_max_rd_atomic = attr->max_dest_rd_atomic;
 +
 +	if (attr_mask & IB_QP_MAX_QP_RD_ATOMIC)
 +		qp->s_max_rd_atomic = attr->max_rd_atomic;
 +
 +	spin_unlock(&qp->s_lock);
 +	spin_unlock_irq(&qp->r_lock);
 +
 +	if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT)
 +		insert_qp(dev, qp);
 +
 +	if (lastwqe) {
 +		ev.device = qp->ibqp.device;
 +		ev.element.qp = &qp->ibqp;
 +		ev.event = IB_EVENT_QP_LAST_WQE_REACHED;
 +		qp->ibqp.event_handler(&ev, qp->ibqp.qp_context);
 +	}
 +	if (mig) {
 +		ev.device = qp->ibqp.device;
 +		ev.element.qp = &qp->ibqp;
 +		ev.event = IB_EVENT_PATH_MIG;
 +		qp->ibqp.event_handler(&ev, qp->ibqp.qp_context);
 +	}
 +	ret = 0;
 +	goto bail;
 +
 +inval:
 +	spin_unlock(&qp->s_lock);
 +	spin_unlock_irq(&qp->r_lock);
 +	ret = -EINVAL;
 +
 +bail:
 +	return ret;
++=======
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  }
  
  int hfi1_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
@@@ -1308,13 -370,11 +1314,18 @@@ int hfi1_destroy_qp(struct ib_qp *ibqp
  	spin_unlock_irq(&qp->r_lock);
  
  	/* all user's cleaned up, mark it available */
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +	free_qpn(&dev->qp_dev->qpn_table, qp->ibqp.qp_num);
 +	spin_lock(&dev->n_qps_lock);
 +	dev->n_qps_allocated--;
 +	spin_unlock(&dev->n_qps_lock);
++=======
+ 	rvt_free_qpn(&dev->rdi.qp_dev->qpn_table, qp->ibqp.qp_num);
+ 	rvt_dec_qp_cnt(&dev->rdi);
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  
  	if (qp->ip)
 -		kref_put(&qp->ip->ref, rvt_release_mmap_info);
 +		kref_put(&qp->ip->ref, hfi1_release_mmap_info);
  	else
  		vfree(qp->r_rq.wq);
  	vfree(qp->s_wq);
@@@ -1721,6 -674,93 +1732,96 @@@ void qp_comm_est(struct hfi1_qp *qp
  	}
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
++=======
+ void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+ 		    gfp_t gfp)
+ {
+ 	struct hfi1_qp_priv *priv;
+ 
+ 	priv = kzalloc(sizeof(*priv), gfp);
+ 	if (!priv)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	priv->owner = qp;
+ 
+ 	priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
+ 	if (!priv->s_hdr) {
+ 		kfree(priv);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+ 	return priv;
+ }
+ 
+ void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	kfree(priv->s_hdr);
+ 	kfree(priv);
+ }
+ 
+ unsigned free_all_qps(struct rvt_dev_info *rdi)
+ {
+ 	struct hfi1_ibdev *verbs_dev = container_of(rdi,
+ 						    struct hfi1_ibdev,
+ 						    rdi);
+ 	struct hfi1_devdata *dd = container_of(verbs_dev,
+ 					       struct hfi1_devdata,
+ 					       verbs_dev);
+ 	int n;
+ 	unsigned qp_inuse = 0;
+ 
+ 	for (n = 0; n < dd->num_pports; n++) {
+ 		struct hfi1_ibport *ibp = &dd->pport[n].ibport_data;
+ 
+ 		rcu_read_lock();
+ 		if (rcu_dereference(ibp->rvp.qp[0]))
+ 			qp_inuse++;
+ 		if (rcu_dereference(ibp->rvp.qp[1]))
+ 			qp_inuse++;
+ 		rcu_read_unlock();
+ 	}
+ 
+ 	return qp_inuse;
+ }
+ 
+ void flush_qp_waiters(struct rvt_qp *qp)
+ {
+ 	flush_iowait(qp);
+ }
+ 
+ void stop_send_queue(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	cancel_work_sync(&priv->s_iowait.iowork);
+ }
+ 
+ void quiesce_qp(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	iowait_sdma_drain(&priv->s_iowait);
+ 	flush_tx_list(qp);
+ }
+ 
+ void notify_qp_reset(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	iowait_init(
+ 		&priv->s_iowait,
+ 		1,
+ 		_hfi1_do_send,
+ 		iowait_sleep,
+ 		iowait_wakeup);
+ 	priv->r_adefered = 0;
+ 	clear_ahg(qp);
+ }
+ 
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.c
  /*
   * Switch to alternate path.
   * The QP s_lock should be held and interrupts disabled.
diff --cc drivers/staging/hfi1/qp.h
index 1144470a6bc0,d6bfb987b830..000000000000
--- a/drivers/staging/hfi1/qp.h
+++ b/drivers/staging/hfi1/qp.h
@@@ -54,74 -54,13 +54,78 @@@
  #include "verbs.h"
  #include "sdma.h"
  
 -extern unsigned int hfi1_qp_table_size;
 +#define QPN_MAX                 (1 << 24)
 +#define QPNMAP_ENTRIES          (QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)
  
  /*
++<<<<<<< HEAD:drivers/staging/hfi1/qp.h
 + * QPN-map pages start out as NULL, they get allocated upon
 + * first use and are never deallocated. This way,
 + * large bitmaps are not allocated unless large numbers of QPs are used.
 + */
 +struct qpn_map {
 +	void *page;
 +};
 +
 +struct hfi1_qpn_table {
 +	spinlock_t lock; /* protect changes in this struct */
 +	unsigned flags;         /* flags for QP0/1 allocated for each port */
 +	u32 last;               /* last QP number allocated */
 +	u32 nmaps;              /* size of the map table */
 +	u16 limit;
 +	u8  incr;
 +	/* bit map of free QP numbers other than 0/1 */
 +	struct qpn_map map[QPNMAP_ENTRIES];
 +};
 +
 +struct hfi1_qp_ibdev {
 +	u32 qp_table_size;
 +	u32 qp_table_bits;
 +	struct hfi1_qp __rcu **qp_table;
 +	spinlock_t qpt_lock;
 +	struct hfi1_qpn_table qpn_table;
 +};
 +
 +static inline u32 qpn_hash(struct hfi1_qp_ibdev *dev, u32 qpn)
 +{
 +	return hash_32(qpn, dev->qp_table_bits);
 +}
 +
 +/**
 + * hfi1_lookup_qpn - return the QP with the given QPN
 + * @ibp: the ibport
 + * @qpn: the QP number to look up
 + *
 + * The caller must hold the rcu_read_lock(), and keep the lock until
 + * the returned qp is no longer in use.
 + */
 +static inline struct hfi1_qp *hfi1_lookup_qpn(struct hfi1_ibport *ibp,
 +				u32 qpn) __must_hold(RCU)
 +{
 +	struct hfi1_qp *qp = NULL;
 +
 +	if (unlikely(qpn <= 1)) {
 +		qp = rcu_dereference(ibp->qp[qpn]);
 +	} else {
 +		struct hfi1_ibdev *dev = &ppd_from_ibp(ibp)->dd->verbs_dev;
 +		u32 n = qpn_hash(dev->qp_dev, qpn);
 +
 +		for (qp = rcu_dereference(dev->qp_dev->qp_table[n]); qp;
 +			qp = rcu_dereference(qp->next))
 +			if (qp->ibqp.qp_num == qpn)
 +				break;
 +	}
 +	return qp;
 +}
 +
 +/**
 + * clear_ahg - reset ahg status in qp
 + * @qp - qp pointer
++=======
+  * free_ahg - clear ahg from QP
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.h
   */
 -static inline void clear_ahg(struct rvt_qp *qp)
 +static inline void clear_ahg(struct hfi1_qp *qp)
  {
  	struct hfi1_qp_priv *priv = qp->priv;
  
@@@ -132,30 -71,6 +136,33 @@@
  	qp->s_ahgidx = -1;
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.h
 +/**
 + * hfi1_error_qp - put a QP into the error state
 + * @qp: the QP to put into the error state
 + * @err: the receive completion error to signal if a RWQE is active
 + *
 + * Flushes both send and receive work queues.
 + * Returns true if last WQE event should be generated.
 + * The QP r_lock and s_lock should be held and interrupts disabled.
 + * If we are already in error state, just return.
 + */
 +int hfi1_error_qp(struct hfi1_qp *qp, enum ib_wc_status err);
 +
 +/**
 + * hfi1_modify_qp - modify the attributes of a queue pair
 + * @ibqp: the queue pair who's attributes we're modifying
 + * @attr: the new attributes
 + * @attr_mask: the mask of attributes to modify
 + * @udata: user data for libibverbs.so
 + *
 + * Returns 0 on success, otherwise returns an errno.
 + */
 +int hfi1_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		   int attr_mask, struct ib_udata *udata);
 +
++=======
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.h
  int hfi1_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
  		  int attr_mask, struct ib_qp_init_attr *init_attr);
  
@@@ -284,6 -187,22 +291,27 @@@ static inline void hfi1_schedule_send(s
  		_hfi1_schedule_send(qp);
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.h
 +void hfi1_migrate_qp(struct hfi1_qp *qp);
 +
++=======
+ void hfi1_migrate_qp(struct rvt_qp *qp);
+ 
+ /*
+  * Functions provided by hfi1 driver for rdmavt to use
+  */
+ void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+ 		    gfp_t gfp);
+ void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp);
+ unsigned free_all_qps(struct rvt_dev_info *rdi);
+ void notify_qp_reset(struct rvt_qp *qp);
+ int get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+ 		       struct ib_qp_attr *attr);
+ void flush_qp_waiters(struct rvt_qp *qp);
+ void notify_error_qp(struct rvt_qp *qp);
+ void stop_send_queue(struct rvt_qp *qp);
+ void quiesce_qp(struct rvt_qp *qp);
+ u32 mtu_from_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp, u32 pmtu);
+ int mtu_to_path_mtu(u32 mtu);
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/qp.h
  #endif /* _QP_H */
diff --cc drivers/staging/hfi1/ruc.c
index c4280b6f47d4,6379df53fa72..000000000000
--- a/drivers/staging/hfi1/ruc.c
+++ b/drivers/staging/hfi1/ruc.c
@@@ -154,13 -154,13 +154,17 @@@ bail
   *
   * Can be called from interrupt level.
   */
++<<<<<<< HEAD:drivers/staging/hfi1/ruc.c
 +int hfi1_get_rwqe(struct hfi1_qp *qp, int wr_id_only)
++=======
+ int hfi1_rvt_get_rwqe(struct rvt_qp *qp, int wr_id_only)
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/ruc.c
  {
  	unsigned long flags;
 -	struct rvt_rq *rq;
 -	struct rvt_rwq *wq;
 -	struct rvt_srq *srq;
 -	struct rvt_rwqe *wqe;
 +	struct hfi1_rq *rq;
 +	struct hfi1_rwq *wq;
 +	struct hfi1_srq *srq;
 +	struct hfi1_rwqe *wqe;
  	void (*handler)(struct ib_event *, void *);
  	u32 tail;
  	int ret;
@@@ -547,9 -549,9 +552,9 @@@ again
  		sqp->s_len -= len;
  	}
  	if (release)
- 		hfi1_put_ss(&qp->r_sge);
+ 		rvt_put_ss(&qp->r_sge);
  
 -	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
 +	if (!test_and_clear_bit(HFI1_R_WRID_VALID, &qp->r_aflags))
  		goto send_comp;
  
  	if (wqe->wr.opcode == IB_WR_RDMA_WRITE_WITH_IMM)
@@@ -622,9 -624,9 +627,9 @@@ serr
  	spin_lock_irqsave(&sqp->s_lock, flags);
  	hfi1_send_complete(sqp, wqe, send_status);
  	if (sqp->ibqp.qp_type == IB_QPT_RC) {
- 		int lastwqe = hfi1_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
+ 		int lastwqe = rvt_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
  
 -		sqp->s_flags &= ~RVT_S_BUSY;
 +		sqp->s_flags &= ~HFI1_S_BUSY;
  		spin_unlock_irqrestore(&sqp->s_lock, flags);
  		if (lastwqe) {
  			struct ib_event ev;
diff --cc drivers/staging/hfi1/trace.h
index 86c12ebfd4f0,fcae96e5b784..000000000000
--- a/drivers/staging/hfi1/trace.h
+++ b/drivers/staging/hfi1/trace.h
@@@ -326,37 -364,6 +326,40 @@@ DEFINE_EVENT(hfi1_qpsleepwakeup_templat
  	     TP_ARGS(qp, flags));
  
  #undef TRACE_SYSTEM
++<<<<<<< HEAD:drivers/staging/hfi1/trace.h
 +#define TRACE_SYSTEM hfi1_qphash
 +DECLARE_EVENT_CLASS(hfi1_qphash_template,
 +	TP_PROTO(struct hfi1_qp *qp, u32 bucket),
 +	TP_ARGS(qp, bucket),
 +	TP_STRUCT__entry(
 +		DD_DEV_ENTRY(dd_from_ibdev(qp->ibqp.device))
 +		__field(u32, qpn)
 +		__field(u32, bucket)
 +	),
 +	TP_fast_assign(
 +		DD_DEV_ASSIGN(dd_from_ibdev(qp->ibqp.device))
 +		__entry->qpn = qp->ibqp.qp_num;
 +		__entry->bucket = bucket;
 +	),
 +	TP_printk(
 +		"[%s] qpn 0x%x bucket %u",
 +		__get_str(dev),
 +		__entry->qpn,
 +		__entry->bucket
 +	)
 +);
 +
 +DEFINE_EVENT(hfi1_qphash_template, hfi1_qpinsert,
 +	TP_PROTO(struct hfi1_qp *qp, u32 bucket),
 +	TP_ARGS(qp, bucket));
 +
 +DEFINE_EVENT(hfi1_qphash_template, hfi1_qpremove,
 +	TP_PROTO(struct hfi1_qp *qp, u32 bucket),
 +	TP_ARGS(qp, bucket));
 +
 +#undef TRACE_SYSTEM
++=======
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/trace.h
  #define TRACE_SYSTEM hfi1_ibhdrs
  
  u8 ibhdr_exhdr_len(struct hfi1_ib_header *hdr);
diff --cc drivers/staging/hfi1/uc.c
index fc90d4f544e4,1e50d303c024..000000000000
--- a/drivers/staging/hfi1/uc.c
+++ b/drivers/staging/hfi1/uc.c
@@@ -332,10 -332,10 +332,10 @@@ void hfi1_uc_rcv(struct hfi1_packet *pa
  inv:
  		if (qp->r_state == OP(SEND_FIRST) ||
  		    qp->r_state == OP(SEND_MIDDLE)) {
 -			set_bit(RVT_R_REWIND_SGE, &qp->r_aflags);
 +			set_bit(HFI1_R_REWIND_SGE, &qp->r_aflags);
  			qp->r_sge.num_sge = 0;
  		} else
- 			hfi1_put_ss(&qp->r_sge);
+ 			rvt_put_ss(&qp->r_sge);
  		qp->r_state = OP(SEND_LAST);
  		switch (opcode) {
  		case OP(SEND_FIRST):
@@@ -391,10 -391,10 +391,10 @@@
  	case OP(SEND_ONLY):
  	case OP(SEND_ONLY_WITH_IMMEDIATE):
  send_first:
 -		if (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))
 +		if (test_and_clear_bit(HFI1_R_REWIND_SGE, &qp->r_aflags))
  			qp->r_sge = qp->s_rdma_read_sge;
  		else {
- 			ret = hfi1_get_rwqe(qp, 0);
+ 			ret = hfi1_rvt_get_rwqe(qp, 0);
  			if (ret < 0)
  				goto op_err;
  			if (!ret)
@@@ -536,10 -536,10 +536,15 @@@ rdma_last_imm
  		tlen -= (hdrsize + pad + 4);
  		if (unlikely(tlen + qp->r_rcv_len != qp->r_len))
  			goto drop;
++<<<<<<< HEAD:drivers/staging/hfi1/uc.c
 +		if (test_and_clear_bit(HFI1_R_REWIND_SGE, &qp->r_aflags))
 +			hfi1_put_ss(&qp->s_rdma_read_sge);
++=======
+ 		if (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))
+ 			rvt_put_ss(&qp->s_rdma_read_sge);
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/uc.c
  		else {
- 			ret = hfi1_get_rwqe(qp, 1);
+ 			ret = hfi1_rvt_get_rwqe(qp, 1);
  			if (ret < 0)
  				goto op_err;
  			if (!ret)
diff --cc drivers/staging/hfi1/ud.c
index a7f67b0111da,2eae16769688..000000000000
--- a/drivers/staging/hfi1/ud.c
+++ b/drivers/staging/hfi1/ud.c
@@@ -80,9 -80,10 +80,14 @@@ static void ud_loopback(struct hfi1_qp 
  
  	rcu_read_lock();
  
++<<<<<<< HEAD:drivers/staging/hfi1/ud.c
 +	qp = hfi1_lookup_qpn(ibp, swqe->wr.wr.ud.remote_qpn);
++=======
+ 	qp = rvt_lookup_qpn(ib_to_rvt(sqp->ibqp.device), &ibp->rvp,
+ 			    swqe->ud_wr.remote_qpn);
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/ud.c
  	if (!qp) {
 -		ibp->rvp.n_pkt_drops++;
 +		ibp->n_pkt_drops++;
  		rcu_read_unlock();
  		return;
  	}
@@@ -222,8 -223,8 +227,13 @@@
  		}
  		length -= len;
  	}
++<<<<<<< HEAD:drivers/staging/hfi1/ud.c
 +	hfi1_put_ss(&qp->r_sge);
 +	if (!test_and_clear_bit(HFI1_R_WRID_VALID, &qp->r_aflags))
++=======
+ 	rvt_put_ss(&qp->r_sge);
+ 	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/ud.c
  		goto bail_unlock;
  	wc.wr_id = qp->r_wr_id;
  	wc.status = IB_WC_SUCCESS;
@@@ -664,18 -665,18 +674,18 @@@ void hfi1_ud_rcv(struct hfi1_packet *pa
  	struct ib_grh *grh = NULL;
  
  	qkey = be32_to_cpu(ohdr->u.ud.deth[0]);
- 	src_qp = be32_to_cpu(ohdr->u.ud.deth[1]) & HFI1_QPN_MASK;
+ 	src_qp = be32_to_cpu(ohdr->u.ud.deth[1]) & RVT_QPN_MASK;
  	dlid = be16_to_cpu(hdr->lrh[1]);
 -	is_mcast = (dlid > be16_to_cpu(IB_MULTICAST_LID_BASE)) &&
 -			(dlid != be16_to_cpu(IB_LID_PERMISSIVE));
 +	is_mcast = (dlid > HFI1_MULTICAST_LID_BASE) &&
 +			(dlid != HFI1_PERMISSIVE_LID);
  	bth1 = be32_to_cpu(ohdr->bth[1]);
  	if (unlikely(bth1 & HFI1_BECN_SMASK)) {
  		/*
  		 * In pre-B0 h/w the CNP_OPCODE is handled via an
 -		 * error path.
 +		 * error path (errata 291394).
  		 */
  		struct hfi1_pportdata *ppd = ppd_from_ibp(ibp);
- 		u32 lqpn =  be32_to_cpu(ohdr->bth[1]) & HFI1_QPN_MASK;
+ 		u32 lqpn =  be32_to_cpu(ohdr->bth[1]) & RVT_QPN_MASK;
  		u8 sl, sc5;
  
  		sc5 = (be16_to_cpu(hdr->lrh[0]) >> 12) & 0xf;
@@@ -840,8 -841,8 +850,13 @@@
  	} else
  		hfi1_skip_sge(&qp->r_sge, sizeof(struct ib_grh), 1);
  	hfi1_copy_sge(&qp->r_sge, data, wc.byte_len - sizeof(struct ib_grh), 1);
++<<<<<<< HEAD:drivers/staging/hfi1/ud.c
 +	hfi1_put_ss(&qp->r_sge);
 +	if (!test_and_clear_bit(HFI1_R_WRID_VALID, &qp->r_aflags))
++=======
+ 	rvt_put_ss(&qp->r_sge);
+ 	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/ud.c
  		return;
  	wc.wr_id = qp->r_wr_id;
  	wc.status = IB_WC_SUCCESS;
diff --cc drivers/staging/hfi1/verbs.c
index d228eb7fc4f0,e51f8270553d..000000000000
--- a/drivers/staging/hfi1/verbs.c
+++ b/drivers/staging/hfi1/verbs.c
@@@ -632,12 -448,12 +633,12 @@@ void hfi1_ib_rcv(struct hfi1_packet *pa
  	inc_opstats(tlen, &rcd->opstats->stats[opcode]);
  
  	/* Get the destination QP number. */
- 	qp_num = be32_to_cpu(packet->ohdr->bth[1]) & HFI1_QPN_MASK;
+ 	qp_num = be32_to_cpu(packet->ohdr->bth[1]) & RVT_QPN_MASK;
  	lid = be16_to_cpu(hdr->lrh[1]);
 -	if (unlikely((lid >= be16_to_cpu(IB_MULTICAST_LID_BASE)) &&
 -		     (lid != be16_to_cpu(IB_LID_PERMISSIVE)))) {
 -		struct rvt_mcast *mcast;
 -		struct rvt_mcast_qp *p;
 +	if (unlikely((lid >= HFI1_MULTICAST_LID_BASE) &&
 +	    (lid != HFI1_PERMISSIVE_LID))) {
 +		struct hfi1_mcast *mcast;
 +		struct hfi1_mcast_qp *p;
  
  		if (lnh != HFI1_LRH_GRH)
  			goto drop;
@@@ -1849,11 -1535,7 +1850,14 @@@ int hfi1_register_ib_device(struct hfi1
  
  	/* Only need to initialize non-zero fields. */
  
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	spin_lock_init(&dev->n_ahs_lock);
 +	spin_lock_init(&dev->n_cqs_lock);
 +	spin_lock_init(&dev->n_qps_lock);
++=======
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/verbs.c
  	spin_lock_init(&dev->n_srqs_lock);
 +	spin_lock_init(&dev->n_mcast_grps_lock);
  	init_timer(&dev->mem_timer);
  	dev->mem_timer.function = mem_timer;
  	dev->mem_timer.data = (unsigned long) dev;
@@@ -1967,40 -1622,99 +1971,110 @@@
  	ibdev->modify_srq = hfi1_modify_srq;
  	ibdev->query_srq = hfi1_query_srq;
  	ibdev->destroy_srq = hfi1_destroy_srq;
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	ibdev->create_qp = hfi1_create_qp;
 +	ibdev->modify_qp = hfi1_modify_qp;
++=======
+ 	ibdev->create_qp = NULL;
+ 	ibdev->modify_qp = NULL;
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/verbs.c
  	ibdev->query_qp = hfi1_query_qp;
  	ibdev->destroy_qp = hfi1_destroy_qp;
 -	ibdev->post_send = NULL;
 +	ibdev->post_send = post_send;
  	ibdev->post_recv = post_receive;
  	ibdev->post_srq_recv = hfi1_post_srq_receive;
 -	ibdev->create_cq = NULL;
 -	ibdev->destroy_cq = NULL;
 -	ibdev->resize_cq = NULL;
 -	ibdev->poll_cq = NULL;
 -	ibdev->req_notify_cq = NULL;
 -	ibdev->get_dma_mr = NULL;
 -	ibdev->reg_user_mr = NULL;
 -	ibdev->dereg_mr = NULL;
 -	ibdev->alloc_mr = NULL;
 -	ibdev->map_mr_sg = NULL;
 -	ibdev->alloc_fmr = NULL;
 -	ibdev->map_phys_fmr = NULL;
 -	ibdev->unmap_fmr = NULL;
 -	ibdev->dealloc_fmr = NULL;
 -	ibdev->attach_mcast = NULL;
 -	ibdev->detach_mcast = NULL;
 +	ibdev->create_cq = hfi1_create_cq;
 +	ibdev->destroy_cq = hfi1_destroy_cq;
 +	ibdev->resize_cq = hfi1_resize_cq;
 +	ibdev->poll_cq = hfi1_poll_cq;
 +	ibdev->req_notify_cq = hfi1_req_notify_cq;
 +	ibdev->get_dma_mr = hfi1_get_dma_mr;
 +	ibdev->reg_phys_mr = hfi1_reg_phys_mr;
 +	ibdev->reg_user_mr = hfi1_reg_user_mr;
 +	ibdev->dereg_mr = hfi1_dereg_mr;
 +	ibdev->alloc_mr = hfi1_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = hfi1_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = hfi1_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = hfi1_alloc_fmr;
 +	ibdev->map_phys_fmr = hfi1_map_phys_fmr;
 +	ibdev->unmap_fmr = hfi1_unmap_fmr;
 +	ibdev->dealloc_fmr = hfi1_dealloc_fmr;
 +	ibdev->attach_mcast = hfi1_multicast_attach;
 +	ibdev->detach_mcast = hfi1_multicast_detach;
  	ibdev->process_mad = hfi1_process_mad;
 -	ibdev->mmap = NULL;
 -	ibdev->dma_ops = NULL;
 +	ibdev->mmap = hfi1_mmap;
 +	ibdev->dma_ops = &hfi1_dma_mapping_ops;
  	ibdev->get_port_immutable = port_immutable;
  
  	strncpy(ibdev->node_desc, init_utsname()->nodename,
  		sizeof(ibdev->node_desc));
  
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	ret = ib_register_device(ibdev, hfi1_create_port_files);
++=======
+ 	/*
+ 	 * Fill in rvt info object.
+ 	 */
+ 	dd->verbs_dev.rdi.driver_f.port_callback = hfi1_create_port_files;
+ 	dd->verbs_dev.rdi.driver_f.get_card_name = get_card_name;
+ 	dd->verbs_dev.rdi.driver_f.get_pci_dev = get_pci_dev;
+ 	dd->verbs_dev.rdi.driver_f.check_ah = hfi1_check_ah;
+ 	dd->verbs_dev.rdi.driver_f.notify_new_ah = hfi1_notify_new_ah;
+ 	/*
+ 	 * Fill in rvt info device attributes.
+ 	 */
+ 	hfi1_fill_device_attr(dd);
+ 
+ 	/* queue pair */
+ 	dd->verbs_dev.rdi.dparms.qp_table_size = hfi1_qp_table_size;
+ 	dd->verbs_dev.rdi.dparms.qpn_start = 0;
+ 	dd->verbs_dev.rdi.dparms.qpn_inc = 1;
+ 	dd->verbs_dev.rdi.dparms.qos_shift = dd->qos_shift;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_start = kdeth_qp << 16;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_end =
+ 	dd->verbs_dev.rdi.dparms.qpn_res_start + 65535;
+ 	dd->verbs_dev.rdi.dparms.max_rdma_atomic = HFI1_MAX_RDMA_ATOMIC;
+ 	dd->verbs_dev.rdi.dparms.psn_mask = PSN_MASK;
+ 	dd->verbs_dev.rdi.dparms.psn_shift = PSN_SHIFT;
+ 	dd->verbs_dev.rdi.dparms.psn_modify_mask = PSN_MODIFY_MASK;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_alloc = qp_priv_alloc;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_free = qp_priv_free;
+ 	dd->verbs_dev.rdi.driver_f.free_all_qps = free_all_qps;
+ 	dd->verbs_dev.rdi.driver_f.notify_qp_reset = notify_qp_reset;
+ 	dd->verbs_dev.rdi.driver_f.do_send = hfi1_do_send;
+ 	dd->verbs_dev.rdi.driver_f.schedule_send = hfi1_schedule_send;
+ 	dd->verbs_dev.rdi.driver_f.get_pmtu_from_attr = get_pmtu_from_attr;
+ 	dd->verbs_dev.rdi.driver_f.notify_error_qp = notify_error_qp;
+ 	dd->verbs_dev.rdi.driver_f.flush_qp_waiters = flush_qp_waiters;
+ 	dd->verbs_dev.rdi.driver_f.stop_send_queue = stop_send_queue;
+ 	dd->verbs_dev.rdi.driver_f.quiesce_qp = quiesce_qp;
+ 	dd->verbs_dev.rdi.driver_f.notify_error_qp = notify_error_qp;
+ 	dd->verbs_dev.rdi.driver_f.mtu_from_qp = mtu_from_qp;
+ 	dd->verbs_dev.rdi.driver_f.mtu_to_path_mtu = mtu_to_path_mtu;
+ 	dd->verbs_dev.rdi.driver_f.check_modify_qp = hfi1_check_modify_qp;
+ 	dd->verbs_dev.rdi.driver_f.modify_qp = hfi1_modify_qp;
+ 
+ 	/* completeion queue */
+ 	snprintf(dd->verbs_dev.rdi.dparms.cq_name,
+ 		 sizeof(dd->verbs_dev.rdi.dparms.cq_name),
+ 		 "hfi1_cq%d", dd->unit);
+ 	dd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;
+ 
+ 	/* misc settings */
+ 	dd->verbs_dev.rdi.flags = 0; /* Let rdmavt handle it all */
+ 	dd->verbs_dev.rdi.dparms.lkey_table_size = hfi1_lkey_table_size;
+ 	dd->verbs_dev.rdi.dparms.nports = dd->num_pports;
+ 	dd->verbs_dev.rdi.dparms.npkeys = hfi1_get_npkeys(dd);
+ 
+ 	ppd = dd->pport;
+ 	for (i = 0; i < dd->num_pports; i++, ppd++)
+ 		rvt_init_port(&dd->verbs_dev.rdi,
+ 			      &ppd->ibport_data.rvp,
+ 			      i,
+ 			      ppd->pkeys);
+ 
+ 	ret = rvt_register_device(&dd->verbs_dev.rdi);
++>>>>>>> ec4274f1aeb5 (staging/rdma/hfi1: Remove modify queue pair from hfi1):drivers/staging/rdma/hfi1/verbs.c
  	if (ret)
  		goto err_reg;
  
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
diff --git a/drivers/infiniband/sw/rdmavt/qp.c b/drivers/infiniband/sw/rdmavt/qp.c
index e8d0da89ea8e..322de64164f7 100644
--- a/drivers/infiniband/sw/rdmavt/qp.c
+++ b/drivers/infiniband/sw/rdmavt/qp.c
@@ -1133,13 +1133,6 @@ int rvt_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 			qp->remote_ah_attr = qp->alt_ah_attr;
 			qp->port_num = qp->alt_ah_attr.port_num;
 			qp->s_pkey_index = qp->s_alt_pkey_index;
-
-			/*
-			 * Ignored by drivers which do not support it. Not
-			 * really worth creating a call back into the driver
-			 * just to set a flag.
-			 */
-			qp->s_flags |= RVT_S_AHG_CLEAR;
 		}
 	}
 
diff --git a/drivers/staging/hfi1/common.h b/drivers/staging/hfi1/common.h
index 5dd92720faae..7e5c4b568a30 100644
--- a/drivers/staging/hfi1/common.h
+++ b/drivers/staging/hfi1/common.h
@@ -346,7 +346,6 @@ struct hfi1_message_header {
 #define HFI1_AETH_CREDIT_MASK 0x1F
 #define HFI1_AETH_CREDIT_INVAL 0x1F
 #define HFI1_MSN_MASK 0xFFFFFF
-#define HFI1_QPN_MASK 0xFFFFFF
 #define HFI1_FECN_SHIFT 31
 #define HFI1_FECN_MASK 1
 #define HFI1_FECN_SMASK (1 << HFI1_FECN_SHIFT)
* Unmerged path drivers/staging/hfi1/driver.c
* Unmerged path drivers/staging/hfi1/qp.c
* Unmerged path drivers/staging/hfi1/qp.h
diff --git a/drivers/staging/hfi1/rc.c b/drivers/staging/hfi1/rc.c
index dd57d65aa9b2..637742ea8184 100644
--- a/drivers/staging/hfi1/rc.c
+++ b/drivers/staging/hfi1/rc.c
@@ -49,6 +49,8 @@
  */
 
 #include <linux/io.h>
+#include <rdma/rdma_vt.h>
+#include <rdma/rdmavt_qp.h>
 
 #include "hfi.h"
 #include "qp.h"
@@ -891,7 +893,7 @@ static void restart_rc(struct hfi1_qp *qp, u32 psn, int wait)
 			qp->s_retry = qp->s_retry_cnt;
 		} else if (qp->s_last == qp->s_acked) {
 			hfi1_send_complete(qp, wqe, IB_WC_RETRY_EXC_ERR);
-			hfi1_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+			rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);
 			return;
 		} else /* need to handle delayed completion */
 			return;
@@ -1355,7 +1357,7 @@ static int do_rc_ack(struct hfi1_qp *qp, u32 aeth, u32 psn, int opcode,
 class_b:
 			if (qp->s_last == qp->s_acked) {
 				hfi1_send_complete(qp, wqe, status);
-				hfi1_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+				rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);
 			}
 			break;
 
@@ -1601,7 +1603,7 @@ ack_len_err:
 ack_err:
 	if (qp->s_last == qp->s_acked) {
 		hfi1_send_complete(qp, wqe, status);
-		hfi1_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+		rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);
 	}
 ack_done:
 	spin_unlock_irqrestore(&qp->s_lock, flags);
@@ -1832,7 +1834,7 @@ void hfi1_rc_error(struct hfi1_qp *qp, enum ib_wc_status err)
 	int lastwqe;
 
 	spin_lock_irqsave(&qp->s_lock, flags);
-	lastwqe = hfi1_error_qp(qp, err);
+	lastwqe = rvt_error_qp(qp, err);
 	spin_unlock_irqrestore(&qp->s_lock, flags);
 
 	if (lastwqe) {
@@ -1873,8 +1875,8 @@ static void log_cca_event(struct hfi1_pportdata *ppd, u8 sl, u32 rlid,
 	cc_event = &ppd->cc_events[ppd->cc_log_idx++];
 	if (ppd->cc_log_idx == OPA_CONG_LOG_ELEMS)
 		ppd->cc_log_idx = 0;
-	cc_event->lqpn = lqpn & HFI1_QPN_MASK;
-	cc_event->rqpn = rqpn & HFI1_QPN_MASK;
+	cc_event->lqpn = lqpn & RVT_QPN_MASK;
+	cc_event->rqpn = rqpn & RVT_QPN_MASK;
 	cc_event->sl = sl;
 	cc_event->svc_type = svc_type;
 	cc_event->rlid = rlid;
@@ -2063,7 +2065,7 @@ void hfi1_rc_rcv(struct hfi1_packet *packet)
 	/* OK, process the packet. */
 	switch (opcode) {
 	case OP(SEND_FIRST):
-		ret = hfi1_get_rwqe(qp, 0);
+		ret = hfi1_rvt_get_rwqe(qp, 0);
 		if (ret < 0)
 			goto nack_op_err;
 		if (!ret)
@@ -2084,7 +2086,7 @@ send_middle:
 
 	case OP(RDMA_WRITE_LAST_WITH_IMMEDIATE):
 		/* consume RWQE */
-		ret = hfi1_get_rwqe(qp, 1);
+		ret = hfi1_rvt_get_rwqe(qp, 1);
 		if (ret < 0)
 			goto nack_op_err;
 		if (!ret)
@@ -2093,7 +2095,7 @@ send_middle:
 
 	case OP(SEND_ONLY):
 	case OP(SEND_ONLY_WITH_IMMEDIATE):
-		ret = hfi1_get_rwqe(qp, 0);
+		ret = hfi1_rvt_get_rwqe(qp, 0);
 		if (ret < 0)
 			goto nack_op_err;
 		if (!ret)
@@ -2125,7 +2127,7 @@ send_last:
 		if (unlikely(wc.byte_len > qp->r_len))
 			goto nack_inv;
 		hfi1_copy_sge(&qp->r_sge, data, tlen, 1);
-		hfi1_put_ss(&qp->r_sge);
+		rvt_put_ss(&qp->r_sge);
 		qp->r_msn++;
 		if (!test_and_clear_bit(HFI1_R_WRID_VALID, &qp->r_aflags))
 			break;
@@ -2193,7 +2195,7 @@ send_last:
 			goto send_middle;
 		else if (opcode == OP(RDMA_WRITE_ONLY))
 			goto no_immediate_data;
-		ret = hfi1_get_rwqe(qp, 1);
+		ret = hfi1_rvt_get_rwqe(qp, 1);
 		if (ret < 0)
 			goto nack_op_err;
 		if (!ret)
* Unmerged path drivers/staging/hfi1/ruc.c
diff --git a/drivers/staging/hfi1/srq.c b/drivers/staging/hfi1/srq.c
index 67786d417493..5fe5d928ad84 100644
--- a/drivers/staging/hfi1/srq.c
+++ b/drivers/staging/hfi1/srq.c
@@ -93,7 +93,7 @@ int hfi1_post_srq_receive(struct ib_srq *ibsrq, struct ib_recv_wr *wr,
 			goto bail;
 		}
 
-		wqe = get_rwqe_ptr(&srq->rq, wq->head);
+		wqe = rvt_get_rwqe_ptr(&srq->rq, wq->head);
 		wqe->wr_id = wr->wr_id;
 		wqe->num_sge = wr->num_sge;
 		for (i = 0; i < wr->num_sge; i++)
@@ -299,7 +299,7 @@ int hfi1_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,
 			struct hfi1_rwqe *wqe;
 			int i;
 
-			wqe = get_rwqe_ptr(&srq->rq, tail);
+			wqe = rvt_get_rwqe_ptr(&srq->rq, tail);
 			p->wr_id = wqe->wr_id;
 			p->num_sge = wqe->num_sge;
 			for (i = 0; i < wqe->num_sge; i++)
diff --git a/drivers/staging/hfi1/trace.c b/drivers/staging/hfi1/trace.c
index 10122e84cb2f..9eadec5be3b0 100644
--- a/drivers/staging/hfi1/trace.c
+++ b/drivers/staging/hfi1/trace.c
@@ -166,7 +166,7 @@ const char *parse_everbs_hdrs(
 	case OP(UD, SEND_ONLY_WITH_IMMEDIATE):
 		trace_seq_printf(p, DETH_PRN,
 			be32_to_cpu(eh->ud.deth[0]),
-			be32_to_cpu(eh->ud.deth[1]) & HFI1_QPN_MASK);
+			be32_to_cpu(eh->ud.deth[1]) & RVT_QPN_MASK);
 		break;
 	}
 	trace_seq_putc(p, 0);
* Unmerged path drivers/staging/hfi1/trace.h
* Unmerged path drivers/staging/hfi1/uc.c
* Unmerged path drivers/staging/hfi1/ud.c
* Unmerged path drivers/staging/hfi1/verbs.c
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
