mm, proc: reduce cost of /proc/pid/smaps for unpopulated shmem mappings

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] proc: reduce cost of /proc/pid/smaps for unpopulated shmem mappings (Jerome Marchand) [838926]
Rebuild_FUZZ: 97.10%
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 48131e03ca4ed71d73fbe55c311a258c6fa2a090
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/48131e03.failed

Following the previous patch, further reduction of /proc/pid/smaps cost
is possible for private writable shmem mappings with unpopulated areas
where the page walk invokes the .pte_hole function.  We can use radix
tree iterator for each such area instead of calling find_get_entry() in
a loop.  This is possible at the extra maintenance cost of introducing
another shmem function shmem_partial_swap_usage().

To demonstrate the diference, I have measured this on a process that
creates a private writable 2GB mapping of a partially swapped out
/dev/shm/file (which cannot employ the optimizations from the prvious
patch) and doesn't populate it at all.  I time how long does it take to
cat /proc/pid/smaps of this process 100 times.

Before this patch:

real    0m3.831s
user    0m0.180s
sys     0m3.212s

After this patch:

real    0m1.176s
user    0m0.180s
sys     0m0.684s

The time is similar to the case where a radix tree iterator is employed
on the whole mapping.

	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jerome Marchand <jmarchan@redhat.com>
	Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 48131e03ca4ed71d73fbe55c311a258c6fa2a090)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/task_mmu.c
#	include/linux/shmem_fs.h
#	mm/shmem.c
diff --cc fs/proc/task_mmu.c
index c6ef0c3195c7,8a03759bda38..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -497,6 -487,89 +497,92 @@@ static void smaps_pte_entry(pte_t ptent
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_SHMEM
+ static int smaps_pte_hole(unsigned long addr, unsigned long end,
+ 		struct mm_walk *walk)
+ {
+ 	struct mem_size_stats *mss = walk->private;
+ 
+ 	mss->swap += shmem_partial_swap_usage(
+ 			walk->vma->vm_file->f_mapping, addr, end);
+ 
+ 	return 0;
+ }
+ #endif
+ 
+ static void smaps_pte_entry(pte_t *pte, unsigned long addr,
+ 		struct mm_walk *walk)
+ {
+ 	struct mem_size_stats *mss = walk->private;
+ 	struct vm_area_struct *vma = walk->vma;
+ 	struct page *page = NULL;
+ 
+ 	if (pte_present(*pte)) {
+ 		page = vm_normal_page(vma, addr, *pte);
+ 	} else if (is_swap_pte(*pte)) {
+ 		swp_entry_t swpent = pte_to_swp_entry(*pte);
+ 
+ 		if (!non_swap_entry(swpent)) {
+ 			int mapcount;
+ 
+ 			mss->swap += PAGE_SIZE;
+ 			mapcount = swp_swapcount(swpent);
+ 			if (mapcount >= 2) {
+ 				u64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;
+ 
+ 				do_div(pss_delta, mapcount);
+ 				mss->swap_pss += pss_delta;
+ 			} else {
+ 				mss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;
+ 			}
+ 		} else if (is_migration_entry(swpent))
+ 			page = migration_entry_to_page(swpent);
+ 	} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) && mss->check_shmem_swap
+ 							&& pte_none(*pte))) {
+ 		page = find_get_entry(vma->vm_file->f_mapping,
+ 						linear_page_index(vma, addr));
+ 		if (!page)
+ 			return;
+ 
+ 		if (radix_tree_exceptional_entry(page))
+ 			mss->swap += PAGE_SIZE;
+ 		else
+ 			page_cache_release(page);
+ 
+ 		return;
+ 	}
+ 
+ 	if (!page)
+ 		return;
+ 	smaps_account(mss, page, PAGE_SIZE, pte_young(*pte), pte_dirty(*pte));
+ }
+ 
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ static void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,
+ 		struct mm_walk *walk)
+ {
+ 	struct mem_size_stats *mss = walk->private;
+ 	struct vm_area_struct *vma = walk->vma;
+ 	struct page *page;
+ 
+ 	/* FOLL_DUMP will return -EFAULT on huge zero page */
+ 	page = follow_trans_huge_pmd(vma, addr, pmd, FOLL_DUMP);
+ 	if (IS_ERR_OR_NULL(page))
+ 		return;
+ 	mss->anonymous_thp += HPAGE_PMD_SIZE;
+ 	smaps_account(mss, page, HPAGE_PMD_SIZE,
+ 			pmd_young(*pmd), pmd_dirty(*pmd));
+ }
+ #else
+ static void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,
+ 		struct mm_walk *walk)
+ {
+ }
+ #endif
+ 
++>>>>>>> 48131e03ca4e (mm, proc: reduce cost of /proc/pid/smaps for unpopulated shmem mappings)
  static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
  			   struct mm_walk *walk)
  {
diff --cc include/linux/shmem_fs.h
index 50777b5b1e4c,a43f41cb3c43..000000000000
--- a/include/linux/shmem_fs.h
+++ b/include/linux/shmem_fs.h
@@@ -60,6 -60,10 +60,13 @@@ extern struct page *shmem_read_mapping_
  extern void shmem_truncate_range(struct inode *inode, loff_t start, loff_t end);
  extern int shmem_unuse(swp_entry_t entry, struct page *page);
  
++<<<<<<< HEAD
++=======
+ extern unsigned long shmem_swap_usage(struct vm_area_struct *vma);
+ extern unsigned long shmem_partial_swap_usage(struct address_space *mapping,
+ 						pgoff_t start, pgoff_t end);
+ 
++>>>>>>> 48131e03ca4e (mm, proc: reduce cost of /proc/pid/smaps for unpopulated shmem mappings)
  static inline struct page *shmem_read_mapping_page(
  				struct address_space *mapping, pgoff_t index)
  {
diff --cc mm/shmem.c
index dd679fc343f9,760d90cf2a41..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -350,6 -360,87 +350,90 @@@ static int shmem_free_swap(struct addre
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Determine (in bytes) how many of the shmem object's pages mapped by the
+  * given offsets are swapped out.
+  *
+  * This is safe to call without i_mutex or mapping->tree_lock thanks to RCU,
+  * as long as the inode doesn't go away and racy results are not a problem.
+  */
+ unsigned long shmem_partial_swap_usage(struct address_space *mapping,
+ 						pgoff_t start, pgoff_t end)
+ {
+ 	struct radix_tree_iter iter;
+ 	void **slot;
+ 	struct page *page;
+ 	unsigned long swapped = 0;
+ 
+ 	rcu_read_lock();
+ 
+ restart:
+ 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
+ 		if (iter.index >= end)
+ 			break;
+ 
+ 		page = radix_tree_deref_slot(slot);
+ 
+ 		/*
+ 		 * This should only be possible to happen at index 0, so we
+ 		 * don't need to reset the counter, nor do we risk infinite
+ 		 * restarts.
+ 		 */
+ 		if (radix_tree_deref_retry(page))
+ 			goto restart;
+ 
+ 		if (radix_tree_exceptional_entry(page))
+ 			swapped++;
+ 
+ 		if (need_resched()) {
+ 			cond_resched_rcu();
+ 			start = iter.index + 1;
+ 			goto restart;
+ 		}
+ 	}
+ 
+ 	rcu_read_unlock();
+ 
+ 	return swapped << PAGE_SHIFT;
+ }
+ 
+ /*
+  * Determine (in bytes) how many of the shmem object's pages mapped by the
+  * given vma is swapped out.
+  *
+  * This is safe to call without i_mutex or mapping->tree_lock thanks to RCU,
+  * as long as the inode doesn't go away and racy results are not a problem.
+  */
+ unsigned long shmem_swap_usage(struct vm_area_struct *vma)
+ {
+ 	struct inode *inode = file_inode(vma->vm_file);
+ 	struct shmem_inode_info *info = SHMEM_I(inode);
+ 	struct address_space *mapping = inode->i_mapping;
+ 	unsigned long swapped;
+ 
+ 	/* Be careful as we don't hold info->lock */
+ 	swapped = READ_ONCE(info->swapped);
+ 
+ 	/*
+ 	 * The easier cases are when the shmem object has nothing in swap, or
+ 	 * the vma maps it whole. Then we can simply use the stats that we
+ 	 * already track.
+ 	 */
+ 	if (!swapped)
+ 		return 0;
+ 
+ 	if (!vma->vm_pgoff && vma->vm_end - vma->vm_start >= inode->i_size)
+ 		return swapped << PAGE_SHIFT;
+ 
+ 	/* Here comes the more involved part */
+ 	return shmem_partial_swap_usage(mapping,
+ 			linear_page_index(vma, vma->vm_start),
+ 			linear_page_index(vma, vma->vm_end));
+ }
+ 
+ /*
++>>>>>>> 48131e03ca4e (mm, proc: reduce cost of /proc/pid/smaps for unpopulated shmem mappings)
   * SysV IPC SHM_UNLOCK restore Unevictable pages to their evictable lists.
   */
  void shmem_unlock_mapping(struct address_space *mapping)
* Unmerged path fs/proc/task_mmu.c
* Unmerged path include/linux/shmem_fs.h
* Unmerged path mm/shmem.c
