staging/rdma/hfi1: Remove post_recv and use rdmavt version

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [infiniband] rdma/hfi1: Remove post_recv and use rdmavt version (Alex Estrin) [1272062 1273170]
Rebuild_FUZZ: 92.59%
commit-author Dennis Dalessandro <dennis.dalessandro@intel.com>
commit 1897ce219143cae13a87e0544b3b467ad3932964
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/1897ce21.failed

This patch removes the simple post recv function in favor of using rdmavt.
The packet receive processing still lives in the driver though.

	Reviewed-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 1897ce219143cae13a87e0544b3b467ad3932964)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/verbs.c
diff --cc drivers/staging/hfi1/verbs.c
index d228eb7fc4f0,b72eb7b9c8fd..000000000000
--- a/drivers/staging/hfi1/verbs.c
+++ b/drivers/staging/hfi1/verbs.c
@@@ -355,220 -323,6 +355,223 @@@ void hfi1_skip_sge(struct hfi1_sge_stat
  	}
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +/**
 + * post_one_send - post one RC, UC, or UD send work request
 + * @qp: the QP to post on
 + * @wr: the work request to send
 + */
 +static int post_one_send(struct hfi1_qp *qp, struct ib_send_wr *wr)
 +{
 +	struct hfi1_swqe *wqe;
 +	u32 next;
 +	int i;
 +	int j;
 +	int acc;
 +	struct rvt_lkey_table *rkt;
 +	struct rvt_pd *pd;
 +	struct hfi1_devdata *dd = dd_from_ibdev(qp->ibqp.device);
 +	struct hfi1_pportdata *ppd;
 +	struct hfi1_ibport *ibp;
 +
 +	/* IB spec says that num_sge == 0 is OK. */
 +	if (unlikely(wr->num_sge > qp->s_max_sge))
 +		return -EINVAL;
 +
 +	ppd = &dd->pport[qp->port_num - 1];
 +	ibp = &ppd->ibport_data;
 +
 +	/*
 +	 * Don't allow RDMA reads or atomic operations on UC or
 +	 * undefined operations.
 +	 * Make sure buffer is large enough to hold the result for atomics.
 +	 */
 +	if (wr->opcode == IB_WR_FAST_REG_MR) {
 +		return -EINVAL;
 +	} else if (qp->ibqp.qp_type == IB_QPT_UC) {
 +		if ((unsigned) wr->opcode >= IB_WR_RDMA_READ)
 +			return -EINVAL;
 +	} else if (qp->ibqp.qp_type != IB_QPT_RC) {
 +		/* Check IB_QPT_SMI, IB_QPT_GSI, IB_QPT_UD opcode */
 +		if (wr->opcode != IB_WR_SEND &&
 +		    wr->opcode != IB_WR_SEND_WITH_IMM)
 +			return -EINVAL;
 +		/* Check UD destination address PD */
 +		if (qp->ibqp.pd != wr->wr.ud.ah->pd)
 +			return -EINVAL;
 +	} else if ((unsigned) wr->opcode > IB_WR_ATOMIC_FETCH_AND_ADD)
 +		return -EINVAL;
 +	else if (wr->opcode >= IB_WR_ATOMIC_CMP_AND_SWP &&
 +		   (wr->num_sge == 0 ||
 +		    wr->sg_list[0].length < sizeof(u64) ||
 +		    wr->sg_list[0].addr & (sizeof(u64) - 1)))
 +		return -EINVAL;
 +	else if (wr->opcode >= IB_WR_RDMA_READ && !qp->s_max_rd_atomic)
 +		return -EINVAL;
 +
 +	next = qp->s_head + 1;
 +	if (next >= qp->s_size)
 +		next = 0;
 +	if (next == qp->s_last)
 +		return -ENOMEM;
 +
 +	rkt = &to_idev(qp->ibqp.device)->lk_table;
 +	pd = ibpd_to_rvtpd(qp->ibqp.pd);
 +	wqe = get_swqe_ptr(qp, qp->s_head);
 +	wqe->wr = *wr;
 +	wqe->length = 0;
 +	j = 0;
 +	if (wr->num_sge) {
 +		acc = wr->opcode >= IB_WR_RDMA_READ ?
 +			IB_ACCESS_LOCAL_WRITE : 0;
 +		for (i = 0; i < wr->num_sge; i++) {
 +			u32 length = wr->sg_list[i].length;
 +			int ok;
 +
 +			if (length == 0)
 +				continue;
 +			ok = hfi1_lkey_ok(rkt, pd, &wqe->sg_list[j],
 +					  &wr->sg_list[i], acc);
 +			if (!ok)
 +				goto bail_inval_free;
 +			wqe->length += length;
 +			j++;
 +		}
 +		wqe->wr.num_sge = j;
 +	}
 +	if (qp->ibqp.qp_type == IB_QPT_UC ||
 +	    qp->ibqp.qp_type == IB_QPT_RC) {
 +		if (wqe->length > 0x80000000U)
 +			goto bail_inval_free;
 +	} else {
 +		struct hfi1_ah *ah = to_iah(wr->wr.ud.ah);
 +
 +		atomic_inc(&ah->refcount);
 +	}
 +	wqe->ssn = qp->s_ssn++;
 +	qp->s_head = next;
 +
 +	return 0;
 +
 +bail_inval_free:
 +	/* release mr holds */
 +	while (j) {
 +		struct hfi1_sge *sge = &wqe->sg_list[--j];
 +
 +		hfi1_put_mr(sge->mr);
 +	}
 +	return -EINVAL;
 +}
 +
 +/**
 + * post_send - post a send on a QP
 + * @ibqp: the QP to post the send on
 + * @wr: the list of work requests to post
 + * @bad_wr: the first bad WR is put here
 + *
 + * This may be called from interrupt context.
 + */
 +static int post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 +		     struct ib_send_wr **bad_wr)
 +{
 +	struct hfi1_qp *qp = to_iqp(ibqp);
 +	struct hfi1_qp_priv *priv = qp->priv;
 +	int err = 0;
 +	int call_send;
 +	unsigned long flags;
 +	unsigned nreq = 0;
 +
 +	spin_lock_irqsave(&qp->s_lock, flags);
 +
 +	/* Check that state is OK to post send. */
 +	if (unlikely(!(ib_hfi1_state_ops[qp->state] & HFI1_POST_SEND_OK))) {
 +		spin_unlock_irqrestore(&qp->s_lock, flags);
 +		return -EINVAL;
 +	}
 +
 +	/* sq empty and not list -> call send */
 +	call_send = qp->s_head == qp->s_last && !wr->next;
 +
 +	for (; wr; wr = wr->next) {
 +		err = post_one_send(qp, wr);
 +		if (unlikely(err)) {
 +			*bad_wr = wr;
 +			goto bail;
 +		}
 +		nreq++;
 +	}
 +bail:
 +	spin_unlock_irqrestore(&qp->s_lock, flags);
 +	if (nreq && !call_send)
 +		_hfi1_schedule_send(qp);
 +	if (nreq && call_send)
 +		hfi1_do_send(&priv->s_iowait.iowork);
 +	return err;
 +}
 +
 +/**
 + * post_receive - post a receive on a QP
 + * @ibqp: the QP to post the receive on
 + * @wr: the WR to post
 + * @bad_wr: the first bad WR is put here
 + *
 + * This may be called from interrupt context.
 + */
 +static int post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,
 +			struct ib_recv_wr **bad_wr)
 +{
 +	struct hfi1_qp *qp = to_iqp(ibqp);
 +	struct hfi1_rwq *wq = qp->r_rq.wq;
 +	unsigned long flags;
 +	int ret;
 +
 +	/* Check that state is OK to post receive. */
 +	if (!(ib_hfi1_state_ops[qp->state] & HFI1_POST_RECV_OK) || !wq) {
 +		*bad_wr = wr;
 +		ret = -EINVAL;
 +		goto bail;
 +	}
 +
 +	for (; wr; wr = wr->next) {
 +		struct hfi1_rwqe *wqe;
 +		u32 next;
 +		int i;
 +
 +		if ((unsigned) wr->num_sge > qp->r_rq.max_sge) {
 +			*bad_wr = wr;
 +			ret = -EINVAL;
 +			goto bail;
 +		}
 +
 +		spin_lock_irqsave(&qp->r_rq.lock, flags);
 +		next = wq->head + 1;
 +		if (next >= qp->r_rq.size)
 +			next = 0;
 +		if (next == wq->tail) {
 +			spin_unlock_irqrestore(&qp->r_rq.lock, flags);
 +			*bad_wr = wr;
 +			ret = -ENOMEM;
 +			goto bail;
 +		}
 +
 +		wqe = get_rwqe_ptr(&qp->r_rq, wq->head);
 +		wqe->wr_id = wr->wr_id;
 +		wqe->num_sge = wr->num_sge;
 +		for (i = 0; i < wr->num_sge; i++)
 +			wqe->sg_list[i] = wr->sg_list[i];
 +		/* Make sure queue entry is written before the head index. */
 +		smp_wmb();
 +		wq->head = next;
 +		spin_unlock_irqrestore(&qp->r_rq.lock, flags);
 +	}
 +	ret = 0;
 +
 +bail:
 +	return ret;
 +}
 +
++=======
++>>>>>>> 1897ce219143 (staging/rdma/hfi1: Remove post_recv and use rdmavt version):drivers/staging/rdma/hfi1/verbs.c
  /*
   * Make sure the QP is ready and able to accept the given opcode.
   */
@@@ -1967,34 -1561,32 +1970,40 @@@ int hfi1_register_ib_device(struct hfi1
  	ibdev->modify_srq = hfi1_modify_srq;
  	ibdev->query_srq = hfi1_query_srq;
  	ibdev->destroy_srq = hfi1_destroy_srq;
 -	ibdev->create_qp = NULL;
 -	ibdev->modify_qp = NULL;
 +	ibdev->create_qp = hfi1_create_qp;
 +	ibdev->modify_qp = hfi1_modify_qp;
  	ibdev->query_qp = hfi1_query_qp;
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	ibdev->destroy_qp = hfi1_destroy_qp;
 +	ibdev->post_send = post_send;
 +	ibdev->post_recv = post_receive;
++=======
+ 	ibdev->destroy_qp = NULL;
+ 	ibdev->post_send = NULL;
+ 	ibdev->post_recv = NULL;
++>>>>>>> 1897ce219143 (staging/rdma/hfi1: Remove post_recv and use rdmavt version):drivers/staging/rdma/hfi1/verbs.c
  	ibdev->post_srq_recv = hfi1_post_srq_receive;
 -	ibdev->create_cq = NULL;
 -	ibdev->destroy_cq = NULL;
 -	ibdev->resize_cq = NULL;
 -	ibdev->poll_cq = NULL;
 -	ibdev->req_notify_cq = NULL;
 -	ibdev->get_dma_mr = NULL;
 -	ibdev->reg_user_mr = NULL;
 -	ibdev->dereg_mr = NULL;
 -	ibdev->alloc_mr = NULL;
 -	ibdev->map_mr_sg = NULL;
 -	ibdev->alloc_fmr = NULL;
 -	ibdev->map_phys_fmr = NULL;
 -	ibdev->unmap_fmr = NULL;
 -	ibdev->dealloc_fmr = NULL;
 -	ibdev->attach_mcast = NULL;
 -	ibdev->detach_mcast = NULL;
 +	ibdev->create_cq = hfi1_create_cq;
 +	ibdev->destroy_cq = hfi1_destroy_cq;
 +	ibdev->resize_cq = hfi1_resize_cq;
 +	ibdev->poll_cq = hfi1_poll_cq;
 +	ibdev->req_notify_cq = hfi1_req_notify_cq;
 +	ibdev->get_dma_mr = hfi1_get_dma_mr;
 +	ibdev->reg_phys_mr = hfi1_reg_phys_mr;
 +	ibdev->reg_user_mr = hfi1_reg_user_mr;
 +	ibdev->dereg_mr = hfi1_dereg_mr;
 +	ibdev->alloc_mr = hfi1_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = hfi1_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = hfi1_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = hfi1_alloc_fmr;
 +	ibdev->map_phys_fmr = hfi1_map_phys_fmr;
 +	ibdev->unmap_fmr = hfi1_unmap_fmr;
 +	ibdev->dealloc_fmr = hfi1_dealloc_fmr;
 +	ibdev->attach_mcast = hfi1_multicast_attach;
 +	ibdev->detach_mcast = hfi1_multicast_detach;
  	ibdev->process_mad = hfi1_process_mad;
 -	ibdev->mmap = NULL;
 -	ibdev->dma_ops = NULL;
 +	ibdev->mmap = hfi1_mmap;
 +	ibdev->dma_ops = &hfi1_dma_mapping_ops;
  	ibdev->get_port_immutable = port_immutable;
  
  	strncpy(ibdev->node_desc, init_utsname()->nodename,
* Unmerged path drivers/staging/hfi1/verbs.c
