netdevice: move netdev_cap_txqueue for shared usage to header

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Daniel Borkmann <dborkman@redhat.com>
commit b9507bdaf40e91fea2b1c0c1ee7dc627c8ee6fd6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/b9507bda.failed

In order to allow users to invoke netdev_cap_txqueue, it needs to
be moved into netdevice.h header file. While at it, also add kernel
doc header to document the API.

	Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit b9507bdaf40e91fea2b1c0c1ee7dc627c8ee6fd6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/flow_dissector.c
diff --cc net/core/flow_dissector.c
index 243f397103e5,e29e810663d7..000000000000
--- a/net/core/flow_dissector.c
+++ b/net/core/flow_dissector.c
@@@ -347,32 -323,6 +347,35 @@@ u32 __skb_get_poff(const struct sk_buf
  	return poff;
  }
  
++<<<<<<< HEAD
 +/* skb_get_poff() returns the offset to the payload as far as it could
 + * be dissected. The main user is currently BPF, so that we can dynamically
 + * truncate packets without needing to push actual payload to the user
 + * space and can analyze headers only, instead.
 + */
 +u32 skb_get_poff(const struct sk_buff *skb)
 +{
 +	struct flow_keys keys;
 +
 +	if (!skb_flow_dissect(skb, &keys))
 +		return 0;
 +
 +	return __skb_get_poff(skb, skb->data, &keys, skb_headlen(skb));
 +}
 +
 +static inline u16 dev_cap_txqueue(struct net_device *dev, u16 queue_index)
 +{
 +	if (unlikely(queue_index >= dev->real_num_tx_queues)) {
 +		net_warn_ratelimited("%s selects TX queue %d, but real number of TX queues is %d\n",
 +				     dev->name, queue_index,
 +				     dev->real_num_tx_queues);
 +		return 0;
 +	}
 +	return queue_index;
 +}
 +
++=======
++>>>>>>> b9507bdaf40e (netdevice: move netdev_cap_txqueue for shared usage to header)
  static inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)
  {
  #ifdef CONFIG_XPS
@@@ -434,10 -391,13 +437,16 @@@ struct netdev_queue *netdev_pick_tx(str
  	if (dev->real_num_tx_queues != 1) {
  		const struct net_device_ops *ops = dev->netdev_ops;
  		if (ops->ndo_select_queue)
 -			queue_index = ops->ndo_select_queue(dev, skb, accel_priv,
 -							    __netdev_pick_tx);
 +			queue_index = ops->ndo_select_queue(dev, skb);
  		else
  			queue_index = __netdev_pick_tx(dev, skb);
++<<<<<<< HEAD
 +		queue_index = dev_cap_txqueue(dev, queue_index);
++=======
+ 
+ 		if (!accel_priv)
+ 			queue_index = netdev_cap_txqueue(dev, queue_index);
++>>>>>>> b9507bdaf40e (netdevice: move netdev_cap_txqueue for shared usage to header)
  	}
  
  	skb_set_queue_mapping(skb, queue_index);
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 7b10147de03d..af9d5651f726 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -2543,6 +2543,26 @@ static inline void netdev_reset_queue(struct net_device *dev_queue)
 	netdev_tx_reset_queue(netdev_get_tx_queue(dev_queue, 0));
 }
 
+/**
+ * 	netdev_cap_txqueue - check if selected tx queue exceeds device queues
+ * 	@dev: network device
+ * 	@queue_index: given tx queue index
+ *
+ * 	Returns 0 if given tx queue index >= number of device tx queues,
+ * 	otherwise returns the originally passed tx queue index.
+ */
+static inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)
+{
+	if (unlikely(queue_index >= dev->real_num_tx_queues)) {
+		net_warn_ratelimited("%s selects TX queue %d, but real number of TX queues is %d\n",
+				     dev->name, queue_index,
+				     dev->real_num_tx_queues);
+		return 0;
+	}
+
+	return queue_index;
+}
+
 /**
  *	netif_running - test if up
  *	@dev: network device
* Unmerged path net/core/flow_dissector.c
