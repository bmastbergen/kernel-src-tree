nvme: move block_device_operations and ns/ctrl freeing to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 1673f1f08c8876f3942b4fa5e8f6a40215f15a94
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/1673f1f0.failed

This moves the block_device_operations over to common code mostly
as-is.  The only change is that the ns and ctrl refcounting got some
small refcounting to have wrappers around the kref_put operations.

A new free_ctrl operation is added to allow the PCI driver to free
it's ressources on the final drop.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
[Moved the integrity and pr changes due to merge conflict]
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 1673f1f08c8876f3942b4fa5e8f6a40215f15a94)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
#	drivers/nvme/host/core.c
#	drivers/nvme/host/nvme.h
diff --cc drivers/block/nvme-core.c
index 2a6eb55ad96c,e0f40afbf01b..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -73,7 -75,10 +73,14 @@@ module_param(nvme_char_major, int, 0)
  static int use_threaded_interrupts;
  module_param(use_threaded_interrupts, int, 0);
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static DEFINE_SPINLOCK(dev_list_lock);
++=======
+ static bool use_cmb_sqes = true;
+ module_param(use_cmb_sqes, bool, 0644);
+ MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
+ 
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
  static LIST_HEAD(dev_list);
  static struct task_struct *nvme_thread;
  static struct workqueue_struct *nvme_workq;
@@@ -95,6 -106,48 +102,51 @@@ struct async_cmd_info 
  };
  
  /*
++<<<<<<< HEAD:drivers/block/nvme-core.c
++=======
+  * Represents an NVM Express device.  Each nvme_dev is a PCI function.
+  */
+ struct nvme_dev {
+ 	struct list_head node;
+ 	struct nvme_queue **queues;
+ 	struct blk_mq_tag_set tagset;
+ 	struct blk_mq_tag_set admin_tagset;
+ 	u32 __iomem *dbs;
+ 	struct device *dev;
+ 	struct dma_pool *prp_page_pool;
+ 	struct dma_pool *prp_small_pool;
+ 	unsigned queue_count;
+ 	unsigned online_queues;
+ 	unsigned max_qid;
+ 	int q_depth;
+ 	u32 db_stride;
+ 	u32 ctrl_config;
+ 	struct msix_entry *entry;
+ 	void __iomem *bar;
+ 	struct list_head namespaces;
+ 	struct device *device;
+ 	struct work_struct reset_work;
+ 	struct work_struct probe_work;
+ 	struct work_struct scan_work;
+ 	bool subsystem;
+ 	u32 max_hw_sectors;
+ 	u32 stripe_size;
+ 	u32 page_size;
+ 	void __iomem *cmb;
+ 	dma_addr_t cmb_dma_addr;
+ 	u64 cmb_size;
+ 	u32 cmbsz;
+ 
+ 	struct nvme_ctrl ctrl;
+ };
+ 
+ static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
+ {
+ 	return container_of(ctrl, struct nvme_dev, ctrl);
+ }
+ 
+ /*
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
   * An NVM Express queue.  Each device has at least two (one for admin
   * commands and one for I/O commands).
   */
@@@ -470,41 -545,69 +522,53 @@@ void nvme_free_iod(struct nvme_dev *dev
  		kfree(iod);
  }
  
 -#ifdef CONFIG_BLK_DEV_INTEGRITY
 -static void nvme_dif_prep(u32 p, u32 v, struct t10_pi_tuple *pi)
 +static int nvme_error_status(u16 status)
  {
 -	if (be32_to_cpu(pi->ref_tag) == v)
 -		pi->ref_tag = cpu_to_be32(p);
 +	switch (status & 0x7ff) {
 +	case NVME_SC_SUCCESS:
 +		return 0;
 +	case NVME_SC_CAP_EXCEEDED:
 +		return -ENOSPC;
 +	default:
 +		return -EIO;
 +	}
  }
++<<<<<<< HEAD:drivers/block/nvme-core.c
  
 -static void nvme_dif_complete(u32 p, u32 v, struct t10_pi_tuple *pi)
 +#ifdef CONFIG_BLK_DEV_INTEGRITY
 +static int nvme_noop_verify(struct blk_integrity_exchg *exg)
  {
 -	if (be32_to_cpu(pi->ref_tag) == p)
 -		pi->ref_tag = cpu_to_be32(v);
 +	return 0;
  }
  
 -/**
 - * nvme_dif_remap - remaps ref tags to bip seed and physical lba
 - *
 - * The virtual start sector is the one that was originally submitted by the
 - * block layer.	Due to partitioning, MD/DM cloning, etc. the actual physical
 - * start sector may be different. Remap protection information to match the
 - * physical LBA on writes, and back to the original seed on reads.
 - *
 - * Type 0 and 3 do not have a ref tag, so no remapping required.
 - */
 -static void nvme_dif_remap(struct request *req,
 -			void (*dif_swap)(u32 p, u32 v, struct t10_pi_tuple *pi))
 +static void nvme_noop_generate(struct blk_integrity_exchg *exg)
  {
 -	struct nvme_ns *ns = req->rq_disk->private_data;
 -	struct bio_integrity_payload *bip;
 -	struct t10_pi_tuple *pi;
 -	void *p, *pmap;
 -	u32 i, nlb, ts, phys, virt;
 -
 -	if (!ns->pi_type || ns->pi_type == NVME_NS_DPS_PI_TYPE3)
 -		return;
 -
 -	bip = bio_integrity(req->bio);
 -	if (!bip)
 -		return;
 -
 -	pmap = kmap_atomic(bip->bip_vec->bv_page) + bip->bip_vec->bv_offset;
 -
 -	p = pmap;
 -	virt = bip_get_seed(bip);
 -	phys = nvme_block_nr(ns, blk_rq_pos(req));
 -	nlb = (blk_rq_bytes(req) >> ns->lba_shift);
 -	ts = ns->disk->queue->integrity.tuple_size;
 +}
  
 -	for (i = 0; i < nlb; i++, virt++, phys++) {
 -		pi = (struct t10_pi_tuple *)p;
 -		dif_swap(phys, virt, pi);
 -		p += ts;
 -	}
 -	kunmap_atomic(pmap);
 +struct blk_integrity nvme_meta_noop = {
 +	.name            = "NVME_META_NOOP",
 +	.generate_fn        = nvme_noop_generate,
 +	.verify_fn        = nvme_noop_verify,
 +};
 +static void nvme_init_integrity(struct nvme_ns *ns)
 +{
 +	nvme_meta_noop.tuple_size = ns->ms;
 +	blk_integrity_register(ns->disk, &nvme_meta_noop);
 +	blk_queue_max_integrity_segments(ns->queue, 1);
  }
  #else /* CONFIG_BLK_DEV_INTEGRITY */
 +static void nvme_init_integrity(struct nvme_ns *ns)
++=======
++#else /* CONFIG_BLK_DEV_INTEGRITY */
+ static void nvme_dif_remap(struct request *req,
+ 			void (*dif_swap)(u32 p, u32 v, struct t10_pi_tuple *pi))
+ {
+ }
+ static void nvme_dif_prep(u32 p, u32 v, struct t10_pi_tuple *pi)
+ {
+ }
+ static void nvme_dif_complete(u32 p, u32 v, struct t10_pi_tuple *pi)
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
  {
  }
  #endif
@@@ -1562,550 -1607,57 +1626,556 @@@ static int nvme_configure_admin_queue(s
  	return result;
  }
  
 -static int nvme_subsys_reset(struct nvme_dev *dev)
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
 +				unsigned long addr, unsigned length)
  {
 -	if (!dev->subsystem)
 -		return -ENOTTY;
 +	int i, err, count, nents, offset;
 +	struct scatterlist *sg;
 +	struct page **pages;
 +	struct nvme_iod *iod;
  
 -	writel(0x4E564D65, dev->bar + NVME_REG_NSSR); /* "NVMe" */
 -	return 0;
 -}
 +	if (addr & 3)
 +		return ERR_PTR(-EINVAL);
 +	if (!length || length > INT_MAX - PAGE_SIZE)
 +		return ERR_PTR(-EINVAL);
  
 -static int nvme_kthread(void *data)
 -{
 -	struct nvme_dev *dev, *next;
 +	offset = offset_in_page(addr);
 +	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
 +	pages = kcalloc(count, sizeof(*pages), GFP_KERNEL);
 +	if (!pages)
 +		return ERR_PTR(-ENOMEM);
  
 -	while (!kthread_should_stop()) {
 -		set_current_state(TASK_INTERRUPTIBLE);
 -		spin_lock(&dev_list_lock);
 -		list_for_each_entry_safe(dev, next, &dev_list, node) {
 -			int i;
 -			u32 csts = readl(dev->bar + NVME_REG_CSTS);
 +	err = get_user_pages_fast(addr, count, 1, pages);
 +	if (err < count) {
 +		count = err;
 +		err = -EFAULT;
 +		goto put_pages;
 +	}
  
 -			if ((dev->subsystem && (csts & NVME_CSTS_NSSRO)) ||
 -							csts & NVME_CSTS_CFS) {
 -				if (!__nvme_reset(dev)) {
 -					dev_warn(dev->dev,
 -						"Failed status: %x, reset controller\n",
 -						readl(dev->bar + NVME_REG_CSTS));
 -				}
 -				continue;
 -			}
 -			for (i = 0; i < dev->queue_count; i++) {
 -				struct nvme_queue *nvmeq = dev->queues[i];
 -				if (!nvmeq)
 -					continue;
 -				spin_lock_irq(&nvmeq->q_lock);
 -				nvme_process_cq(nvmeq);
 +	err = -ENOMEM;
 +	iod = __nvme_alloc_iod(count, length, dev, 0, GFP_KERNEL);
 +	if (!iod)
 +		goto put_pages;
 +
 +	sg = iod->sg;
 +	sg_init_table(sg, count);
 +	for (i = 0; i < count; i++) {
 +		sg_set_page(&sg[i], pages[i],
 +			    min_t(unsigned, length, PAGE_SIZE - offset),
 +			    offset);
 +		length -= (PAGE_SIZE - offset);
 +		offset = 0;
 +	}
 +	sg_mark_end(&sg[i - 1]);
 +	iod->nents = count;
 +
 +	nents = dma_map_sg(&dev->pci_dev->dev, sg, count,
 +				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 +	if (!nents)
 +		goto free_iod;
 +
 +	kfree(pages);
 +	return iod;
  
 -				while (i == 0 && dev->ctrl.event_limit > 0) {
 -					if (nvme_submit_async_admin_req(dev))
 -						break;
 -					dev->ctrl.event_limit--;
 -				}
 -				spin_unlock_irq(&nvmeq->q_lock);
 -			}
 -		}
 -		spin_unlock(&dev_list_lock);
 -		schedule_timeout(round_jiffies_relative(HZ));
 -	}
 -	return 0;
 + free_iod:
 +	kfree(iod);
 + put_pages:
 +	for (i = 0; i < count; i++)
 +		put_page(pages[i]);
 +	kfree(pages);
 +	return ERR_PTR(err);
  }
  
 -static void nvme_alloc_ns(struct nvme_dev *dev, unsigned nsid)
 +void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
 +			struct nvme_iod *iod)
 +{
 +	int i;
 +
 +	dma_unmap_sg(&dev->pci_dev->dev, iod->sg, iod->nents,
 +				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 +
 +	for (i = 0; i < iod->nents; i++)
 +		put_page(sg_page(&iod->sg[i]));
 +}
 +
 +static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 +{
 +	struct nvme_dev *dev = ns->dev;
 +	struct nvme_user_io io;
 +	struct nvme_command c;
 +	unsigned length, meta_len, prp_len;
 +	int status, write;
 +	struct nvme_iod *iod;
 +	dma_addr_t meta_dma = 0;
 +	void *meta = NULL;
 +
 +	if (copy_from_user(&io, uio, sizeof(io)))
 +		return -EFAULT;
 +	length = (io.nblocks + 1) << ns->lba_shift;
 +	meta_len = (io.nblocks + 1) * ns->ms;
 +
 +	if (meta_len && ((io.metadata & 3) || !io.metadata) && !ns->ext)
 +		return -EINVAL;
 +	else if (meta_len && ns->ext) {
 +		length += meta_len;
 +		meta_len = 0;
 +	}
 +
 +	write = io.opcode & 1;
 +
 +	switch (io.opcode) {
 +	case nvme_cmd_write:
 +	case nvme_cmd_read:
 +	case nvme_cmd_compare:
 +		iod = nvme_map_user_pages(dev, write, io.addr, length);
 +		break;
 +	default:
 +		return -EINVAL;
 +	}
 +
 +	if (IS_ERR(iod))
 +		return PTR_ERR(iod);
 +
 +	prp_len = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
 +	if (length != prp_len) {
 +		status = -ENOMEM;
 +		goto unmap;
 +	}
 +	if (meta_len) {
 +		meta = dma_alloc_coherent(&dev->pci_dev->dev, meta_len,
 +					  &meta_dma, GFP_KERNEL);
 +		if (!meta) {
 +			status = -ENOMEM;
 +			goto unmap;
 +		}
 +		if (write) {
 +			if (copy_from_user(meta, (void __user *)io.metadata,
 +					   meta_len)) {
 +				status = -EFAULT;
 +				goto unmap;
 +			}
 +		}
 +	}
 +
 +	memset(&c, 0, sizeof(c));
 +	c.rw.opcode = io.opcode;
 +	c.rw.flags = io.flags;
 +	c.rw.nsid = cpu_to_le32(ns->ns_id);
 +	c.rw.slba = cpu_to_le64(io.slba);
 +	c.rw.length = cpu_to_le16(io.nblocks);
 +	c.rw.control = cpu_to_le16(io.control);
 +	c.rw.dsmgmt = cpu_to_le32(io.dsmgmt);
 +	c.rw.reftag = cpu_to_le32(io.reftag);
 +	c.rw.apptag = cpu_to_le16(io.apptag);
 +	c.rw.appmask = cpu_to_le16(io.appmask);
 +	c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 +	c.rw.prp2 = cpu_to_le64(iod->first_dma);
 +	c.rw.metadata = cpu_to_le64(meta_dma);
 +	status = nvme_submit_sync_cmd(ns->queue, &c);
 + unmap:
 +	nvme_unmap_user_pages(dev, write, iod);
 +	nvme_free_iod(dev, iod);
 +
 +	if (meta) {
 +		if (status == NVME_SC_SUCCESS && !write) {
 +			if (copy_to_user((void __user *)io.metadata, meta,
 +					 meta_len))
 +				status = -EFAULT;
 +		}
 +		dma_free_coherent(&dev->pci_dev->dev, meta_len, meta, meta_dma);
 +	}
 +	return status;
 +}
 +
 +static int nvme_user_cmd(struct nvme_dev *dev, struct nvme_ns *ns,
 +			struct nvme_passthru_cmd __user *ucmd)
 +{
 +	struct nvme_passthru_cmd cmd;
 +	struct nvme_command c;
 +	int status, length;
 +	struct nvme_iod *uninitialized_var(iod);
 +	unsigned timeout;
 +
 +	if (!capable(CAP_SYS_ADMIN))
 +		return -EACCES;
 +	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
 +		return -EFAULT;
 +
 +	memset(&c, 0, sizeof(c));
 +	c.common.opcode = cmd.opcode;
 +	c.common.flags = cmd.flags;
 +	c.common.nsid = cpu_to_le32(cmd.nsid);
 +	c.common.cdw2[0] = cpu_to_le32(cmd.cdw2);
 +	c.common.cdw2[1] = cpu_to_le32(cmd.cdw3);
 +	c.common.cdw10[0] = cpu_to_le32(cmd.cdw10);
 +	c.common.cdw10[1] = cpu_to_le32(cmd.cdw11);
 +	c.common.cdw10[2] = cpu_to_le32(cmd.cdw12);
 +	c.common.cdw10[3] = cpu_to_le32(cmd.cdw13);
 +	c.common.cdw10[4] = cpu_to_le32(cmd.cdw14);
 +	c.common.cdw10[5] = cpu_to_le32(cmd.cdw15);
 +
 +	length = cmd.data_len;
 +	if (cmd.data_len) {
 +		iod = nvme_map_user_pages(dev, cmd.opcode & 1, cmd.addr,
 +								length);
 +		if (IS_ERR(iod))
 +			return PTR_ERR(iod);
 +		length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
 +		c.common.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 +		c.common.prp2 = cpu_to_le64(iod->first_dma);
 +	}
 +
 +	timeout = cmd.timeout_ms ? msecs_to_jiffies(cmd.timeout_ms) :
 +								ADMIN_TIMEOUT;
 +
 +	if (length != cmd.data_len) {
 +		status = -ENOMEM;
 +		goto out;
 +	}
 +
 +	status = __nvme_submit_sync_cmd(ns ? ns->queue : dev->admin_q, &c,
 +					&cmd.result, timeout);
 +
 +out:
 +	if (cmd.data_len) {
 +		nvme_unmap_user_pages(dev, cmd.opcode & 1, iod);
 +		nvme_free_iod(dev, iod);
 +	}
 +
 +	if ((status >= 0) && copy_to_user(&ucmd->result, &cmd.result,
 +							sizeof(cmd.result)))
 +		status = -EFAULT;
 +
 +	return status;
 +}
 +
++=======
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
 +static int nvme_subsys_reset(struct nvme_dev *dev)
 +{
 +	if (!dev->subsystem)
 +		return -ENOTTY;
 +
 +	writel(0x4E564D65, &dev->bar->nssr); /* "NVMe" */
 +	return 0;
 +}
 +
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static int nvme_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,
 +							unsigned long arg)
 +{
 +	struct nvme_ns *ns = bdev->bd_disk->private_data;
 +
 +	switch (cmd) {
 +	case NVME_IOCTL_ID:
 +		force_successful_syscall_return();
 +		return ns->ns_id;
 +	case NVME_IOCTL_ADMIN_CMD:
 +		return nvme_user_cmd(ns->dev, NULL, (void __user *)arg);
 +	case NVME_IOCTL_IO_CMD:
 +		return nvme_user_cmd(ns->dev, ns, (void __user *)arg);
 +	case NVME_IOCTL_SUBMIT_IO:
 +		return nvme_submit_io(ns, (void __user *)arg);
 +	case SG_GET_VERSION_NUM:
 +		return nvme_sg_get_version_num((void __user *)arg);
 +	case SG_IO:
 +		return nvme_sg_io(ns, (void __user *)arg);
 +	default:
 +		return -ENOTTY;
 +	}
 +}
 +
 +#ifdef CONFIG_COMPAT
 +static int nvme_compat_ioctl(struct block_device *bdev, fmode_t mode,
 +					unsigned int cmd, unsigned long arg)
 +{
 +	switch (cmd) {
 +	case SG_IO:
 +		return -ENOIOCTLCMD;
 +	}
 +	return nvme_ioctl(bdev, mode, cmd, arg);
 +}
 +#else
 +#define nvme_compat_ioctl	NULL
 +#endif
 +
 +static void nvme_free_dev(struct kref *kref);
 +static void nvme_free_ns(struct kref *kref)
 +{
 +	struct nvme_ns *ns = container_of(kref, struct nvme_ns, kref);
 +
 +	spin_lock(&dev_list_lock);
 +	ns->disk->private_data = NULL;
 +	spin_unlock(&dev_list_lock);
 +
 +	kref_put(&ns->dev->kref, nvme_free_dev);
 +	put_disk(ns->disk);
 +	kfree(ns);
 +}
 +
 +static int nvme_open(struct block_device *bdev, fmode_t mode)
 +{
 +	int ret = 0;
 +	struct nvme_ns *ns;
 +
 +	spin_lock(&dev_list_lock);
 +	ns = bdev->bd_disk->private_data;
 +	if (!ns)
 +		ret = -ENXIO;
 +	else if (!kref_get_unless_zero(&ns->kref))
 +		ret = -ENXIO;
 +	spin_unlock(&dev_list_lock);
 +
 +	return ret;
 +}
 +
 +static void nvme_release(struct gendisk *disk, fmode_t mode)
 +{
 +	struct nvme_ns *ns = disk->private_data;
 +	kref_put(&ns->kref, nvme_free_ns);
 +}
 +
 +static int nvme_getgeo(struct block_device *bd, struct hd_geometry *geo)
 +{
 +	/* some standard values */
 +	geo->heads = 1 << 6;
 +	geo->sectors = 1 << 5;
 +	geo->cylinders = get_capacity(bd->bd_disk) >> 11;
 +	return 0;
 +}
 +
 +static void nvme_config_discard(struct nvme_ns *ns)
 +{
 +	u32 logical_block_size = queue_logical_block_size(ns->queue);
 +	ns->queue->limits.discard_zeroes_data = 0;
 +	ns->queue->limits.discard_alignment = logical_block_size;
 +	ns->queue->limits.discard_granularity = logical_block_size;
 +	ns->queue->limits.max_discard_sectors = 0xffffffff;
 +	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, ns->queue);
 +}
 +
 +static int nvme_revalidate_disk(struct gendisk *disk)
 +{
 +	struct nvme_ns *ns = disk->private_data;
 +	struct nvme_dev *dev = ns->dev;
 +	struct nvme_id_ns *id;
 +	dma_addr_t dma_addr;
 +	u8 lbaf;
 +	u16 old_ms;
 +	unsigned short bs;
 +
 +	id = dma_alloc_coherent(&dev->pci_dev->dev, 4096, &dma_addr,
 +								GFP_KERNEL);
 +	if (!id) {
 +		dev_warn(&dev->pci_dev->dev, "%s: Memory alocation failure\n",
 +								__func__);
 +		return 0;
 +	}
 +
 +	if (nvme_identify(dev, ns->ns_id, 0, dma_addr)) {
 +		dev_warn(&dev->pci_dev->dev,
 +			"identify failed ns:%d, setting capacity to 0\n",
 +			ns->ns_id);
 +		memset(id, 0, sizeof(*id));
 +	}
 +
 +	if (id->ncap == 0) {
 +		dma_free_coherent(&dev->pci_dev->dev, 4096, id, dma_addr);
 +		return -ENODEV;
 +	}
 +
 +	old_ms = ns->ms;
 +	lbaf = id->flbas & NVME_NS_FLBAS_LBA_MASK;
 +	ns->lba_shift = id->lbaf[lbaf].ds;
 +
 +	ns->ms = le16_to_cpu(id->lbaf[lbaf].ms);
 +	ns->ext = ns->ms && (id->flbas & NVME_NS_FLBAS_META_EXT);
 +
 +	/*
 +	 * If identify namespace failed, use default 512 byte block size so
 +	 * block layer can use before failing read/write for 0 capacity.
 +	 */
 +	if (ns->lba_shift == 0)
 +		ns->lba_shift = 9;
 +	bs = 1 << ns->lba_shift;
 +
 +	if (blk_get_integrity(disk) && (ns->ms != old_ms ||
 +				bs != queue_logical_block_size(disk->queue) ||
 +				(ns->ms && ns->ext)))
 +		blk_integrity_unregister(disk);
 +
 +	ns->pi_type = ns->ms == 8 ? id->dps & NVME_NS_DPS_PI_MASK : 0;
 +	blk_queue_logical_block_size(ns->queue, bs);
 +
 +	if (ns->ms && !blk_get_integrity(disk) && (disk->flags & GENHD_FL_UP) &&
 +								!ns->ext)
 +		nvme_init_integrity(ns);
 +
 +	if ((ns->ms && !blk_get_integrity(disk) &&
 +					!(ns->pi_type && ns->ms == 8)))
 +		set_capacity(disk, 0);
 +	else
 +		set_capacity(disk, le64_to_cpup(&id->nsze) << (ns->lba_shift - 9));
 +
 +	if (dev->oncs & NVME_CTRL_ONCS_DSM)
 +		nvme_config_discard(ns);
 +
 +	dma_free_coherent(&dev->pci_dev->dev, 4096, id, dma_addr);
 +	return 0;
 +}
 +
 +static char nvme_pr_type(enum pr_type type)
 +{
 +	switch (type) {
 +	case PR_WRITE_EXCLUSIVE:
 +		return 1;
 +	case PR_EXCLUSIVE_ACCESS:
 +		return 2;
 +	case PR_WRITE_EXCLUSIVE_REG_ONLY:
 +		return 3;
 +	case PR_EXCLUSIVE_ACCESS_REG_ONLY:
 +		return 4;
 +	case PR_WRITE_EXCLUSIVE_ALL_REGS:
 +		return 5;
 +	case PR_EXCLUSIVE_ACCESS_ALL_REGS:
 +		return 6;
 +	default:
 +		return 0;
 +	}
 +};
 +
 +static int nvme_pr_command(struct block_device *bdev, u32 cdw10,
 +				u64 key, u64 sa_key, u8 op)
 +{
 +	struct nvme_ns *ns = bdev->bd_disk->private_data;
 +	struct nvme_command c;
 +	u8 data[16] = { 0, };
 +
 +	put_unaligned_le64(key, &data[0]);
 +	put_unaligned_le64(sa_key, &data[8]);
 +
 +	memset(&c, 0, sizeof(c));
 +	c.common.opcode = op;
 +	c.common.nsid = cpu_to_le32(ns->ns_id);
 +	c.common.cdw10[0] = cpu_to_le32(cdw10);
 +
 +	return nvme_submit_sync_cmd(ns->queue, &c, data, 16);
 +}
 +
 +static int nvme_pr_register(struct block_device *bdev, u64 old,
 +		u64 new, unsigned flags)
 +{
 +	u32 cdw10;
 +
 +	if (flags & ~PR_FL_IGNORE_KEY)
 +		return -EOPNOTSUPP;
 +
 +	cdw10 = old ? 2 : 0;
 +	cdw10 |= (flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0;
 +	cdw10 |= (1 << 30) | (1 << 31); /* PTPL=1 */
 +	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_register);
 +}
 +
 +static int nvme_pr_reserve(struct block_device *bdev, u64 key,
 +		enum pr_type type, unsigned flags)
 +{
 +	u32 cdw10;
 +
 +	if (flags & ~PR_FL_IGNORE_KEY)
 +		return -EOPNOTSUPP;
 +
 +	cdw10 = nvme_pr_type(type) << 8;
 +	cdw10 |= ((flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0);
 +	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_acquire);
 +}
 +
 +static int nvme_pr_preempt(struct block_device *bdev, u64 old, u64 new,
 +		enum pr_type type, bool abort)
 +{
 +	u32 cdw10 = nvme_pr_type(type) << 8 | abort ? 2 : 1;
 +	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_acquire);
 +}
 +
 +static int nvme_pr_clear(struct block_device *bdev, u64 key)
 +{
 +	u32 cdw10 = 1 | (key ? 1 << 3 : 0);
 +	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_register);
 +}
 +
 +static int nvme_pr_release(struct block_device *bdev, u64 key, enum pr_type type)
 +{
 +	u32 cdw10 = nvme_pr_type(type) << 8 | key ? 1 << 3 : 0;
 +	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_release);
 +}
 +
 +static const struct pr_ops nvme_pr_ops = {
 +	.pr_register	= nvme_pr_register,
 +	.pr_reserve	= nvme_pr_reserve,
 +	.pr_release	= nvme_pr_release,
 +	.pr_preempt	= nvme_pr_preempt,
 +	.pr_clear	= nvme_pr_clear,
 +};
 +
 +static const struct block_device_operations nvme_fops = {
 +	.owner		= THIS_MODULE,
 +	.ioctl		= nvme_ioctl,
 +	.compat_ioctl	= nvme_compat_ioctl,
 +	.open		= nvme_open,
 +	.release	= nvme_release,
 +	.getgeo		= nvme_getgeo,
 +	.revalidate_disk= nvme_revalidate_disk,
 +	.pr_ops		= &nvme_pr_ops,
 +};
 +
++=======
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
 +static int nvme_kthread(void *data)
 +{
 +	struct nvme_dev *dev, *next;
 +
 +	while (!kthread_should_stop()) {
 +		set_current_state(TASK_INTERRUPTIBLE);
 +		spin_lock(&dev_list_lock);
 +		list_for_each_entry_safe(dev, next, &dev_list, node) {
 +			int i;
 +			u32 csts = readl(&dev->bar->csts);
 +
 +			if ((dev->subsystem && (csts & NVME_CSTS_NSSRO)) ||
 +							csts & NVME_CSTS_CFS) {
 +				if (work_busy(&dev->reset_work))
 +					continue;
 +				list_del_init(&dev->node);
 +				dev_warn(&dev->pci_dev->dev,
 +					"Failed status: %x, reset controller\n",
 +					readl(&dev->bar->csts));
 +				PREPARE_WORK(&dev->reset_work,
 +							nvme_reset_failed_dev);
 +				queue_work(nvme_workq, &dev->reset_work);
 +				continue;
 +			}
 +			for (i = 0; i < dev->queue_count; i++) {
 +				struct nvme_queue *nvmeq = dev->queues[i];
 +				if (!nvmeq)
 +					continue;
 +				spin_lock_irq(&nvmeq->q_lock);
 +				nvme_process_cq(nvmeq);
 +
 +				while ((i == 0) && (dev->event_limit > 0)) {
 +					if (nvme_submit_async_admin_req(dev))
 +						break;
 +					dev->event_limit--;
 +				}
 +				spin_unlock_irq(&nvmeq->q_lock);
 +			}
 +		}
 +		spin_unlock(&dev_list_lock);
 +		schedule_timeout(round_jiffies_relative(HZ));
 +	}
 +	return 0;
 +}
 +
 +static void nvme_alloc_ns(struct nvme_dev *dev, unsigned nsid)
  {
  	struct nvme_ns *ns;
  	struct gendisk *disk;
@@@ -2164,18 -1716,20 +2234,34 @@@
  	if (nvme_revalidate_disk(ns->disk))
  		goto out_free_disk;
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	kref_get(&dev->kref);
 +	add_disk(ns->disk);
 +	if (ns->ms) {
 +		struct block_device *bd = bdget_disk(ns->disk, 0);
 +		if (!bd)
 +			return;
 +		if (blkdev_get(bd, FMODE_READ, NULL)) {
 +			bdput(bd);
 +			return;
++=======
+ 	kref_get(&dev->ctrl.kref);
+ 	if (ns->type != NVME_NS_LIGHTNVM) {
+ 		add_disk(ns->disk);
+ 		if (ns->ms) {
+ 			struct block_device *bd = bdget_disk(ns->disk, 0);
+ 			if (!bd)
+ 				return;
+ 			if (blkdev_get(bd, FMODE_READ, NULL)) {
+ 				bdput(bd);
+ 				return;
+ 			}
+ 			blkdev_reread_part(bd);
+ 			blkdev_put(bd, FMODE_READ);
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
  		}
 +		blkdev_reread_part(bd);
 +		blkdev_put(bd, FMODE_READ);
  	}
  	return;
   out_free_disk:
@@@ -2829,11 -2439,11 +2915,11 @@@ static void nvme_release_instance(struc
  	spin_unlock(&dev_list_lock);
  }
  
- static void nvme_free_dev(struct kref *kref)
+ static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
  {
- 	struct nvme_dev *dev = container_of(kref, struct nvme_dev, kref);
+ 	struct nvme_dev *dev = to_nvme_dev(ctrl);
  
 -	put_device(dev->dev);
 +	pci_dev_put(dev->pci_dev);
  	put_device(dev->device);
  	nvme_release_instance(dev);
  	if (dev->tagset.tags)
@@@ -2994,43 -2602,15 +3080,48 @@@ static int nvme_remove_dead_ctrl(void *
  	return 0;
  }
  
 +static void nvme_remove_disks(struct work_struct *ws)
 +{
 +	struct nvme_dev *dev = container_of(ws, struct nvme_dev, reset_work);
 +
 +	nvme_free_queues(dev, 1);
 +	nvme_dev_remove(dev);
 +}
 +
 +static int nvme_dev_resume(struct nvme_dev *dev)
 +{
 +	int ret;
 +
 +	ret = nvme_dev_start(dev);
 +	if (ret)
 +		return ret;
 +	if (dev->online_queues < 2) {
 +		spin_lock(&dev_list_lock);
 +		PREPARE_WORK(&dev->reset_work, nvme_remove_disks);
 +		queue_work(nvme_workq, &dev->reset_work);
 +		spin_unlock(&dev_list_lock);
 +	} else {
 +		nvme_unfreeze_queues(dev);
 +		nvme_dev_add(dev);
 +		nvme_set_irq_hints(dev);
 +	}
 +	return 0;
 +}
 +
  static void nvme_dead_ctrl(struct nvme_dev *dev)
  {
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	dev_warn(&dev->pci_dev->dev, "Device failed to resume\n");
 +	kref_get(&dev->kref);
++=======
+ 	dev_warn(dev->dev, "Device failed to resume\n");
+ 	kref_get(&dev->ctrl.kref);
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
  	if (IS_ERR(kthread_run(nvme_remove_dead_ctrl, dev, "nvme%d",
 -						dev->ctrl.instance))) {
 -		dev_err(dev->dev,
 +						dev->instance))) {
 +		dev_err(&dev->pci_dev->dev,
  			"Failed to start controller remove task\n");
- 		kref_put(&dev->kref, nvme_free_dev);
+ 		nvme_put_ctrl(&dev->ctrl);
  	}
  }
  
@@@ -3100,7 -2680,17 +3191,21 @@@ static ssize_t nvme_sysfs_reset(struct 
  }
  static DEVICE_ATTR(reset_controller, S_IWUSR, NULL, nvme_sysfs_reset);
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static void nvme_async_probe(struct work_struct *work);
++=======
+ static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
+ {
+ 	*val = readl(to_nvme_dev(ctrl)->bar + off);
+ 	return 0;
+ }
+ 
+ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
+ 	.reg_read32		= nvme_pci_reg_read32,
+ 	.free_ctrl		= nvme_pci_free_ctrl,
+ };
+ 
++>>>>>>> 1673f1f08c88 (nvme: move block_device_operations and ns/ctrl freeing to common code):drivers/nvme/host/pci.c
  static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
  {
  	int node, result = -ENOMEM;
@@@ -3134,10 -2728,10 +3239,10 @@@
  	if (result)
  		goto release;
  
- 	kref_init(&dev->kref);
+ 	kref_init(&dev->ctrl.kref);
  	dev->device = device_create(nvme_class, &pdev->dev,
 -				MKDEV(nvme_char_major, dev->ctrl.instance),
 -				dev, "nvme%d", dev->ctrl.instance);
 +				MKDEV(nvme_char_major, dev->instance),
 +				dev, "nvme%d", dev->instance);
  	if (IS_ERR(dev->device)) {
  		result = PTR_ERR(dev->device);
  		goto release_pools;
@@@ -3211,10 -2797,11 +3316,10 @@@ static void nvme_remove(struct pci_dev 
  	nvme_dev_remove(dev);
  	nvme_dev_shutdown(dev);
  	nvme_dev_remove_admin(dev);
 -	device_destroy(nvme_class, MKDEV(nvme_char_major, dev->ctrl.instance));
 +	device_destroy(nvme_class, MKDEV(nvme_char_major, dev->instance));
  	nvme_free_queues(dev, 0);
 -	nvme_release_cmb(dev);
  	nvme_release_prp_pools(dev);
- 	kref_put(&dev->kref, nvme_free_dev);
+ 	nvme_put_ctrl(&dev->ctrl);
  }
  
  /* These functions are yet to be implemented */
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
* Unmerged path drivers/block/nvme-core.c
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
