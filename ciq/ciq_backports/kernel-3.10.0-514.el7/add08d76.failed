IB/mlx5: Convert UMR CQ to new CQ API

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Christoph Hellwig <hch@lst.de>
commit add08d765e942eab8eb15a592baeb372a3dd6831
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/add08d76.failed

Simplifies the code, and makes it more fair vs other users by using a
softirq for polling.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Haggai Eran <haggaie@mellanox.com>
	Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit add08d765e942eab8eb15a592baeb372a3dd6831)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 9a8b4ea88eb7,dd9231494f63..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -894,14 -890,15 +888,22 @@@ static struct mlx5_ib_mr *reg_umr(struc
  	if (err)
  		goto free_mr;
  
++<<<<<<< HEAD
 +	memset(&wr, 0, sizeof(wr));
 +	wr.wr_id = (u64)(unsigned long)&umr_context;
 +	prep_umr_reg_wqe(pd, &wr, &sg, dma, npages, mr->mmr.key, page_shift,
 +			 virt_addr, len, access_flags);
- 
++=======
  	mlx5_ib_init_umr_context(&umr_context);
+ 
+ 	memset(&umrwr, 0, sizeof(umrwr));
+ 	umrwr.wr.wr_cqe = &umr_context.cqe;
+ 	prep_umr_reg_wqe(pd, &umrwr.wr, &sg, dma, npages, mr->mmkey.key,
+ 			 page_shift, virt_addr, len, access_flags);
++>>>>>>> add08d765e94 (IB/mlx5: Convert UMR CQ to new CQ API)
+ 
  	down(&umrc->sem);
 -	err = ib_post_send(umrc->qp, &umrwr.wr, &bad);
 +	err = ib_post_send(umrc->qp, &wr, &bad);
  	if (err) {
  		mlx5_ib_warn(dev, "post send failed, err %d\n", err);
  		goto unmap_dma;
@@@ -1011,27 -1008,28 +1013,32 @@@ int mlx5_ib_update_mtt(struct mlx5_ib_m
  
  		dma_sync_single_for_device(ddev, dma, size, DMA_TO_DEVICE);
  
+ 		mlx5_ib_init_umr_context(&umr_context);
+ 
  		memset(&wr, 0, sizeof(wr));
++<<<<<<< HEAD
 +		wr.wr_id = (u64)(unsigned long)&umr_context;
++=======
+ 		wr.wr.wr_cqe = &umr_context.cqe;
++>>>>>>> add08d765e94 (IB/mlx5: Convert UMR CQ to new CQ API)
  
  		sg.addr = dma;
  		sg.length = ALIGN(npages * sizeof(u64),
  				MLX5_UMR_MTT_ALIGNMENT);
  		sg.lkey = dev->umrc.pd->local_dma_lkey;
  
 -		wr.wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE |
 +		wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE |
  				MLX5_IB_SEND_UMR_UPDATE_MTT;
 -		wr.wr.sg_list = &sg;
 -		wr.wr.num_sge = 1;
 -		wr.wr.opcode = MLX5_IB_WR_UMR;
 -		wr.npages = sg.length / sizeof(u64);
 -		wr.page_shift = PAGE_SHIFT;
 -		wr.mkey = mr->mmkey.key;
 -		wr.target.offset = start_page_index;
 +		wr.sg_list = &sg;
 +		wr.num_sge = 1;
 +		wr.opcode = MLX5_IB_WR_UMR;
 +		umrwr->npages = sg.length / sizeof(u64);
 +		umrwr->page_shift = PAGE_SHIFT;
 +		umrwr->mkey = mr->mmr.key;
 +		umrwr->target.offset = start_page_index;
  
- 		mlx5_ib_init_umr_context(&umr_context);
  		down(&umrc->sem);
 -		err = ib_post_send(umrc->qp, &wr.wr, &bad);
 +		err = ib_post_send(umrc->qp, &wr, &bad);
  		if (err) {
  			mlx5_ib_err(dev, "UMR post send failed, err %d\n", err);
  		} else {
@@@ -1198,16 -1196,18 +1205,23 @@@ static int unreg_umr(struct mlx5_ib_de
  {
  	struct umr_common *umrc = &dev->umrc;
  	struct mlx5_ib_umr_context umr_context;
 -	struct mlx5_umr_wr umrwr;
 -	struct ib_send_wr *bad;
 +	struct ib_send_wr wr, *bad;
  	int err;
  
++<<<<<<< HEAD
 +	memset(&wr, 0, sizeof(wr));
 +	wr.wr_id = (u64)(unsigned long)&umr_context;
 +	prep_umr_unreg_wqe(dev, &wr, mr->mmr.key);
- 
++=======
  	mlx5_ib_init_umr_context(&umr_context);
+ 
+ 	memset(&umrwr.wr, 0, sizeof(umrwr));
+ 	umrwr.wr.wr_cqe = &umr_context.cqe;
+ 	prep_umr_unreg_wqe(dev, &umrwr.wr, mr->mmkey.key);
++>>>>>>> add08d765e94 (IB/mlx5: Convert UMR CQ to new CQ API)
+ 
  	down(&umrc->sem);
 -	err = ib_post_send(umrc->qp, &umrwr.wr, &bad);
 +	err = ib_post_send(umrc->qp, &wr, &bad);
  	if (err) {
  		up(&umrc->sem);
  		mlx5_ib_dbg(dev, "err %d\n", err);
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index 4a2beff52a0a..bd8d73e163a2 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -1842,7 +1842,7 @@ static void destroy_umrc_res(struct mlx5_ib_dev *dev)
 		mlx5_ib_warn(dev, "mr cache cleanup failed\n");
 
 	mlx5_ib_destroy_qp(dev->umrc.qp);
-	ib_destroy_cq(dev->umrc.cq);
+	ib_free_cq(dev->umrc.cq);
 	ib_dealloc_pd(dev->umrc.pd);
 }
 
@@ -1857,7 +1857,6 @@ static int create_umr_res(struct mlx5_ib_dev *dev)
 	struct ib_pd *pd;
 	struct ib_cq *cq;
 	struct ib_qp *qp;
-	struct ib_cq_init_attr cq_attr = {};
 	int ret;
 
 	attr = kzalloc(sizeof(*attr), GFP_KERNEL);
@@ -1874,15 +1873,12 @@ static int create_umr_res(struct mlx5_ib_dev *dev)
 		goto error_0;
 	}
 
-	cq_attr.cqe = 128;
-	cq = ib_create_cq(&dev->ib_dev, mlx5_umr_cq_handler, NULL, NULL,
-			  &cq_attr);
+	cq = ib_alloc_cq(&dev->ib_dev, NULL, 128, 0, IB_POLL_SOFTIRQ);
 	if (IS_ERR(cq)) {
 		mlx5_ib_dbg(dev, "Couldn't create CQ for sync UMR QP\n");
 		ret = PTR_ERR(cq);
 		goto error_2;
 	}
-	ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);
 
 	init_attr->send_cq = cq;
 	init_attr->recv_cq = cq;
@@ -1949,7 +1945,7 @@ error_4:
 	mlx5_ib_destroy_qp(qp);
 
 error_3:
-	ib_destroy_cq(cq);
+	ib_free_cq(cq);
 
 error_2:
 	ib_dealloc_pd(pd);
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 262b860a11ee..e5ae7bc9d439 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -412,16 +412,11 @@ struct mlx5_ib_fast_reg_page_list {
 };
 
 struct mlx5_ib_umr_context {
+	struct ib_cqe		cqe;
 	enum ib_wc_status	status;
 	struct completion	done;
 };
 
-static inline void mlx5_ib_init_umr_context(struct mlx5_ib_umr_context *context)
-{
-	context->status = -1;
-	init_completion(&context->done);
-}
-
 struct umr_common {
 	struct ib_pd	*pd;
 	struct ib_cq	*cq;
@@ -694,7 +689,6 @@ int mlx5_ib_get_cqe_size(struct mlx5_ib_dev *dev, struct ib_cq *ibcq);
 int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
 int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 int mlx5_mr_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift);
-void mlx5_umr_cq_handler(struct ib_cq *cq, void *cq_context);
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
