mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] memblock: add extra "flags" to memblock to allow selection of memory based on attribute (Yasuaki Ishimatsu) [1270209]
Rebuild_FUZZ: 98.31%
commit-author Tony Luck <tony.luck@intel.com>
commit fc6daaf93151877748f8096af6b3fddb147f22d6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/fc6daaf9.failed

Some high end Intel Xeon systems report uncorrectable memory errors as a
recoverable machine check.  Linux has included code for some time to
process these and just signal the affected processes (or even recover
completely if the error was in a read only page that can be replaced by
reading from disk).

But we have no recovery path for errors encountered during kernel code
execution.  Except for some very specific cases were are unlikely to ever
be able to recover.

Enter memory mirroring. Actually 3rd generation of memory mirroing.

Gen1: All memory is mirrored
	Pro: No s/w enabling - h/w just gets good data from other side of the
	     mirror
	Con: Halves effective memory capacity available to OS/applications

Gen2: Partial memory mirror - just mirror memory begind some memory controllers
	Pro: Keep more of the capacity
	Con: Nightmare to enable. Have to choose between allocating from
	     mirrored memory for safety vs. NUMA local memory for performance

Gen3: Address range partial memory mirror - some mirror on each memory
      controller
	Pro: Can tune the amount of mirror and keep NUMA performance
	Con: I have to write memory management code to implement

The current plan is just to use mirrored memory for kernel allocations.
This has been broken into two phases:

1) This patch series - find the mirrored memory, use it for boot time
   allocations

2) Wade into mm/page_alloc.c and define a ZONE_MIRROR to pick up the
   unused mirrored memory from mm/memblock.c and only give it out to
   select kernel allocations (this is still being scoped because
   page_alloc.c is scary).

This patch (of 3):

Add extra "flags" to memblock to allow selection of memory based on
attribute.  No functional changes

	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Cc: Xishi Qiu <qiuxishi@huawei.com>
	Cc: Hanjun Guo <guohanjun@huawei.com>
	Cc: Xiexiuqi <xiexiuqi@huawei.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Yinghai Lu <yinghai@kernel.org>
	Cc: Naoya Horiguchi <nao.horiguchi@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fc6daaf93151877748f8096af6b3fddb147f22d6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/kernel/crash_dump.c
#	arch/sparc/mm/init_64.c
#	arch/x86/mm/memtest.c
#	include/linux/memblock.h
#	mm/cma.c
#	mm/memblock.c
#	mm/nobootmem.c
diff --cc arch/s390/kernel/crash_dump.c
index 270935f18838,7a75ad4594e3..000000000000
--- a/arch/s390/kernel/crash_dump.c
+++ b/arch/s390/kernel/crash_dump.c
@@@ -22,6 -23,25 +22,28 @@@
  #define PTR_SUB(x, y) (((char *) (x)) - ((unsigned long) (y)))
  #define PTR_DIFF(x, y) ((unsigned long)(((char *) (x)) - ((unsigned long) (y))))
  
++<<<<<<< HEAD
++=======
+ static struct memblock_region oldmem_region;
+ 
+ static struct memblock_type oldmem_type = {
+ 	.cnt = 1,
+ 	.max = 1,
+ 	.total_size = 0,
+ 	.regions = &oldmem_region,
+ };
+ 
+ #define for_each_dump_mem_range(i, nid, p_start, p_end, p_nid)		\
+ 	for (i = 0, __next_mem_range(&i, nid, MEMBLOCK_NONE,		\
+ 				     &memblock.physmem,			\
+ 				     &oldmem_type, p_start,		\
+ 				     p_end, p_nid);			\
+ 	     i != (u64)ULLONG_MAX;					\
+ 	     __next_mem_range(&i, nid, MEMBLOCK_NONE, &memblock.physmem,\
+ 			      &oldmem_type,				\
+ 			      p_start, p_end, p_nid))
+ 
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  struct dump_save_areas dump_save_areas;
  
  /*
diff --cc arch/sparc/mm/init_64.c
index 634044d82c0b,4ac88b757514..000000000000
--- a/arch/sparc/mm/init_64.c
+++ b/arch/sparc/mm/init_64.c
@@@ -1758,6 -1960,61 +1758,64 @@@ pgd_t swapper_pg_dir[2048]
  static void sun4u_pgprot_init(void);
  static void sun4v_pgprot_init(void);
  
++<<<<<<< HEAD
++=======
+ static phys_addr_t __init available_memory(void)
+ {
+ 	phys_addr_t available = 0ULL;
+ 	phys_addr_t pa_start, pa_end;
+ 	u64 i;
+ 
+ 	for_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &pa_start,
+ 				&pa_end, NULL)
+ 		available = available + (pa_end  - pa_start);
+ 
+ 	return available;
+ }
+ 
+ #define _PAGE_CACHE_4U	(_PAGE_CP_4U | _PAGE_CV_4U)
+ #define _PAGE_CACHE_4V	(_PAGE_CP_4V | _PAGE_CV_4V)
+ #define __DIRTY_BITS_4U	 (_PAGE_MODIFIED_4U | _PAGE_WRITE_4U | _PAGE_W_4U)
+ #define __DIRTY_BITS_4V	 (_PAGE_MODIFIED_4V | _PAGE_WRITE_4V | _PAGE_W_4V)
+ #define __ACCESS_BITS_4U (_PAGE_ACCESSED_4U | _PAGE_READ_4U | _PAGE_R)
+ #define __ACCESS_BITS_4V (_PAGE_ACCESSED_4V | _PAGE_READ_4V | _PAGE_R)
+ 
+ /* We need to exclude reserved regions. This exclusion will include
+  * vmlinux and initrd. To be more precise the initrd size could be used to
+  * compute a new lower limit because it is freed later during initialization.
+  */
+ static void __init reduce_memory(phys_addr_t limit_ram)
+ {
+ 	phys_addr_t avail_ram = available_memory();
+ 	phys_addr_t pa_start, pa_end;
+ 	u64 i;
+ 
+ 	if (limit_ram >= avail_ram)
+ 		return;
+ 
+ 	for_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &pa_start,
+ 				&pa_end, NULL) {
+ 		phys_addr_t region_size = pa_end - pa_start;
+ 		phys_addr_t clip_start = pa_start;
+ 
+ 		avail_ram = avail_ram - region_size;
+ 		/* Are we consuming too much? */
+ 		if (avail_ram < limit_ram) {
+ 			phys_addr_t give_back = limit_ram - avail_ram;
+ 
+ 			region_size = region_size - give_back;
+ 			clip_start = clip_start + give_back;
+ 		}
+ 
+ 		memblock_remove(clip_start, region_size);
+ 
+ 		if (avail_ram <= limit_ram)
+ 			break;
+ 		i = 0UL;
+ 	}
+ }
+ 
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  void __init paging_init(void)
  {
  	unsigned long end_pfn, shift, phys_base;
diff --cc arch/x86/mm/memtest.c
index 1e9da795767a,0a1cc133f6d7..000000000000
--- a/arch/x86/mm/memtest.c
+++ b/arch/x86/mm/memtest.c
@@@ -74,9 -74,10 +74,16 @@@ static void __init do_one_pass(u64 patt
  	u64 i;
  	phys_addr_t this_start, this_end;
  
++<<<<<<< HEAD:arch/x86/mm/memtest.c
 +	for_each_free_mem_range(i, NUMA_NO_NODE, &this_start, &this_end, NULL) {
 +		this_start = clamp_t(phys_addr_t, this_start, start, end);
 +		this_end = clamp_t(phys_addr_t, this_end, start, end);
++=======
+ 	for_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &this_start,
+ 				&this_end, NULL) {
+ 		this_start = clamp(this_start, start, end);
+ 		this_end = clamp(this_end, start, end);
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute):mm/memtest.c
  		if (this_start < this_end) {
  			printk(KERN_INFO "  %010llx - %010llx pattern %016llx\n",
  			       (unsigned long long)this_start,
diff --cc include/linux/memblock.h
index 39082db5d5cd,7aeec0cb4c27..000000000000
--- a/include/linux/memblock.h
+++ b/include/linux/memblock.h
@@@ -18,9 -18,13 +18,12 @@@
  #include <linux/mm.h>
  
  #define INIT_MEMBLOCK_REGIONS	128
 -#define INIT_PHYSMEM_REGIONS	4
  
  /* Definition of memblock flags. */
- #define MEMBLOCK_HOTPLUG	0x1	/* hotpluggable region */
+ enum {
+ 	MEMBLOCK_NONE		= 0x0,	/* No special request */
+ 	MEMBLOCK_HOTPLUG	= 0x1,	/* hotpluggable region */
+ };
  
  struct memblock_region {
  	phys_addr_t base;
@@@ -70,6 -78,67 +73,70 @@@ int memblock_reserve(phys_addr_t base, 
  void memblock_trim_memory(phys_addr_t align);
  int memblock_mark_hotplug(phys_addr_t base, phys_addr_t size);
  int memblock_clear_hotplug(phys_addr_t base, phys_addr_t size);
++<<<<<<< HEAD
++=======
+ 
+ /* Low level functions */
+ int memblock_add_range(struct memblock_type *type,
+ 		       phys_addr_t base, phys_addr_t size,
+ 		       int nid, unsigned long flags);
+ 
+ int memblock_remove_range(struct memblock_type *type,
+ 			  phys_addr_t base,
+ 			  phys_addr_t size);
+ 
+ void __next_mem_range(u64 *idx, int nid, ulong flags,
+ 		      struct memblock_type *type_a,
+ 		      struct memblock_type *type_b, phys_addr_t *out_start,
+ 		      phys_addr_t *out_end, int *out_nid);
+ 
+ void __next_mem_range_rev(u64 *idx, int nid, ulong flags,
+ 			  struct memblock_type *type_a,
+ 			  struct memblock_type *type_b, phys_addr_t *out_start,
+ 			  phys_addr_t *out_end, int *out_nid);
+ 
+ /**
+  * for_each_mem_range - iterate through memblock areas from type_a and not
+  * included in type_b. Or just type_a if type_b is NULL.
+  * @i: u64 used as loop variable
+  * @type_a: ptr to memblock_type to iterate
+  * @type_b: ptr to memblock_type which excludes from the iteration
+  * @nid: node selector, %NUMA_NO_NODE for all nodes
+  * @flags: pick from blocks based on memory attributes
+  * @p_start: ptr to phys_addr_t for start address of the range, can be %NULL
+  * @p_end: ptr to phys_addr_t for end address of the range, can be %NULL
+  * @p_nid: ptr to int for nid of the range, can be %NULL
+  */
+ #define for_each_mem_range(i, type_a, type_b, nid, flags,		\
+ 			   p_start, p_end, p_nid)			\
+ 	for (i = 0, __next_mem_range(&i, nid, flags, type_a, type_b,	\
+ 				     p_start, p_end, p_nid);		\
+ 	     i != (u64)ULLONG_MAX;					\
+ 	     __next_mem_range(&i, nid, flags, type_a, type_b,		\
+ 			      p_start, p_end, p_nid))
+ 
+ /**
+  * for_each_mem_range_rev - reverse iterate through memblock areas from
+  * type_a and not included in type_b. Or just type_a if type_b is NULL.
+  * @i: u64 used as loop variable
+  * @type_a: ptr to memblock_type to iterate
+  * @type_b: ptr to memblock_type which excludes from the iteration
+  * @nid: node selector, %NUMA_NO_NODE for all nodes
+  * @flags: pick from blocks based on memory attributes
+  * @p_start: ptr to phys_addr_t for start address of the range, can be %NULL
+  * @p_end: ptr to phys_addr_t for end address of the range, can be %NULL
+  * @p_nid: ptr to int for nid of the range, can be %NULL
+  */
+ #define for_each_mem_range_rev(i, type_a, type_b, nid, flags,		\
+ 			       p_start, p_end, p_nid)			\
+ 	for (i = (u64)ULLONG_MAX,					\
+ 		     __next_mem_range_rev(&i, nid, flags, type_a, type_b,\
+ 					 p_start, p_end, p_nid);	\
+ 	     i != (u64)ULLONG_MAX;					\
+ 	     __next_mem_range_rev(&i, nid, flags, type_a, type_b,	\
+ 				  p_start, p_end, p_nid))
+ 
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  #ifdef CONFIG_MOVABLE_NODE
  static inline bool memblock_is_hotpluggable(struct memblock_region *m)
  {
@@@ -124,14 -193,9 +192,20 @@@ void __next_free_mem_range(u64 *idx, in
   * Walks over free (memory && !reserved) areas of memblock.  Available as
   * soon as memblock is initialized.
   */
++<<<<<<< HEAD
 +#define for_each_free_mem_range(i, nid, p_start, p_end, p_nid)		\
 +	for (i = 0,							\
 +	     __next_free_mem_range(&i, nid, p_start, p_end, p_nid);	\
 +	     i != (u64)ULLONG_MAX;					\
 +	     __next_free_mem_range(&i, nid, p_start, p_end, p_nid))
 +
 +void __next_free_mem_range_rev(u64 *idx, int nid, phys_addr_t *out_start,
 +			       phys_addr_t *out_end, int *out_nid);
++=======
+ #define for_each_free_mem_range(i, nid, flags, p_start, p_end, p_nid)	\
+ 	for_each_mem_range(i, &memblock.memory, &memblock.reserved,	\
+ 			   nid, flags, p_start, p_end, p_nid)
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  
  /**
   * for_each_free_mem_range_reverse - rev-iterate through free memblock areas
@@@ -144,11 -209,10 +219,18 @@@
   * Walks over free (memory && !reserved) areas of memblock in reverse
   * order.  Available as soon as memblock is initialized.
   */
++<<<<<<< HEAD
 +#define for_each_free_mem_range_reverse(i, nid, p_start, p_end, p_nid)	\
 +	for (i = (u64)ULLONG_MAX,					\
 +	     __next_free_mem_range_rev(&i, nid, p_start, p_end, p_nid);	\
 +	     i != (u64)ULLONG_MAX;					\
 +	     __next_free_mem_range_rev(&i, nid, p_start, p_end, p_nid))
++=======
+ #define for_each_free_mem_range_reverse(i, nid, flags, p_start, p_end,	\
+ 					p_nid)				\
+ 	for_each_mem_range_rev(i, &memblock.memory, &memblock.reserved,	\
+ 			       nid, flags, p_start, p_end, p_nid)
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  
  static inline void memblock_set_region_flags(struct memblock_region *r,
  					     unsigned long flags)
@@@ -218,6 -282,9 +300,12 @@@ static inline bool memblock_bottom_up(v
  #define MEMBLOCK_ALLOC_ANYWHERE	(~(phys_addr_t)0)
  #define MEMBLOCK_ALLOC_ACCESSIBLE	0
  
++<<<<<<< HEAD
++=======
+ phys_addr_t __init memblock_alloc_range(phys_addr_t size, phys_addr_t align,
+ 					phys_addr_t start, phys_addr_t end,
+ 					ulong flags);
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  phys_addr_t memblock_alloc_base(phys_addr_t size, phys_addr_t align,
  				phys_addr_t max_addr);
  phys_addr_t __memblock_alloc_base(phys_addr_t size, phys_addr_t align,
diff --cc mm/memblock.c
index 4158cb85f9e7,b9ff2f4f0285..000000000000
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@@ -735,9 -786,12 +742,15 @@@ int __init_memblock memblock_clear_hotp
  }
  
  /**
 - * __next__mem_range - next function for for_each_free_mem_range() etc.
 + * __next_free_mem_range - next function for for_each_free_mem_range()
   * @idx: pointer to u64 loop variable
   * @nid: node selector, %NUMA_NO_NODE for all nodes
++<<<<<<< HEAD
++=======
+  * @flags: pick from blocks based on memory attributes
+  * @type_a: pointer to memblock_type from where the range is taken
+  * @type_b: pointer to memblock_type which excludes memory from being taken
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
   * @out_start: ptr to phys_addr_t for start address of the range, can be %NULL
   * @out_end: ptr to phys_addr_t for end address of the range, can be %NULL
   * @out_nid: ptr to int for nid of the range, can be %NULL
@@@ -757,36 -811,61 +770,44 @@@
   * As both region arrays are sorted, the function advances the two indices
   * in lockstep and returns each intersection.
   */
++<<<<<<< HEAD
 +void __init_memblock __next_free_mem_range(u64 *idx, int nid,
 +					   phys_addr_t *out_start,
 +					   phys_addr_t *out_end, int *out_nid)
++=======
+ void __init_memblock __next_mem_range(u64 *idx, int nid, ulong flags,
+ 				      struct memblock_type *type_a,
+ 				      struct memblock_type *type_b,
+ 				      phys_addr_t *out_start,
+ 				      phys_addr_t *out_end, int *out_nid)
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  {
 -	int idx_a = *idx & 0xffffffff;
 -	int idx_b = *idx >> 32;
 -
 -	if (WARN_ONCE(nid == MAX_NUMNODES,
 -	"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
 -		nid = NUMA_NO_NODE;
 +	struct memblock_type *mem = &memblock.memory;
 +	struct memblock_type *rsv = &memblock.reserved;
 +	int mi = *idx & 0xffffffff;
 +	int ri = *idx >> 32;
 +	bool check_node = (nid != NUMA_NO_NODE) && (nid != MAX_NUMNODES);
  
 -	for (; idx_a < type_a->cnt; idx_a++) {
 -		struct memblock_region *m = &type_a->regions[idx_a];
 +	if (nid == MAX_NUMNODES)
 +		pr_warn_once("%s: Usage of MAX_NUMNODES is depricated. Use NUMA_NO_NODE instead\n",
 +			     __func__);
  
 +	for ( ; mi < mem->cnt; mi++) {
 +		struct memblock_region *m = &mem->regions[mi];
  		phys_addr_t m_start = m->base;
  		phys_addr_t m_end = m->base + m->size;
 -		int	    m_nid = memblock_get_region_node(m);
  
  		/* only memory regions are associated with nodes, check it */
 -		if (nid != NUMA_NO_NODE && nid != m_nid)
 +		if (check_node && nid != memblock_get_region_node(m))
  			continue;
  
 -		/* skip hotpluggable memory regions if needed */
 -		if (movable_node_is_enabled() && memblock_is_hotpluggable(m))
 -			continue;
 -
 -		if (!type_b) {
 -			if (out_start)
 -				*out_start = m_start;
 -			if (out_end)
 -				*out_end = m_end;
 -			if (out_nid)
 -				*out_nid = m_nid;
 -			idx_a++;
 -			*idx = (u32)idx_a | (u64)idx_b << 32;
 -			return;
 -		}
 +		/* scan areas before each reservation for intersection */
 +		for ( ; ri < rsv->cnt + 1; ri++) {
 +			struct memblock_region *r = &rsv->regions[ri];
 +			phys_addr_t r_start = ri ? r[-1].base + r[-1].size : 0;
 +			phys_addr_t r_end = ri < rsv->cnt ? r->base : ULLONG_MAX;
  
 -		/* scan areas before each reservation */
 -		for (; idx_b < type_b->cnt + 1; idx_b++) {
 -			struct memblock_region *r;
 -			phys_addr_t r_start;
 -			phys_addr_t r_end;
 -
 -			r = &type_b->regions[idx_b];
 -			r_start = idx_b ? r[-1].base + r[-1].size : 0;
 -			r_end = idx_b < type_b->cnt ?
 -				r->base : ULLONG_MAX;
 -
 -			/*
 -			 * if idx_b advanced past idx_a,
 -			 * break out to advance idx_a
 -			 */
 +			/* if ri advanced past mi, break out to advance mi */
  			if (r_start >= m_end)
  				break;
  			/* if the two regions intersect, we're done */
@@@ -816,46 -896,48 +837,60 @@@
  }
  
  /**
 - * __next_mem_range_rev - generic next function for for_each_*_range_rev()
 - *
 - * Finds the next range from type_a which is not marked as unsuitable
 - * in type_b.
 - *
 + * __next_free_mem_range_rev - next function for for_each_free_mem_range_reverse()
   * @idx: pointer to u64 loop variable
   * @nid: nid: node selector, %NUMA_NO_NODE for all nodes
++<<<<<<< HEAD
++=======
+  * @flags: pick from blocks based on memory attributes
+  * @type_a: pointer to memblock_type from where the range is taken
+  * @type_b: pointer to memblock_type which excludes memory from being taken
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
   * @out_start: ptr to phys_addr_t for start address of the range, can be %NULL
   * @out_end: ptr to phys_addr_t for end address of the range, can be %NULL
   * @out_nid: ptr to int for nid of the range, can be %NULL
   *
 - * Reverse of __next_mem_range().
 + * Reverse of __next_free_mem_range().
 + *
 + * Linux kernel cannot migrate pages used by itself. Memory hotplug users won't
 + * be able to hot-remove hotpluggable memory used by the kernel. So this
 + * function skip hotpluggable regions if needed when allocating memory for the
 + * kernel.
   */
++<<<<<<< HEAD
 +void __init_memblock __next_free_mem_range_rev(u64 *idx, int nid,
 +					   phys_addr_t *out_start,
 +					   phys_addr_t *out_end, int *out_nid)
++=======
+ void __init_memblock __next_mem_range_rev(u64 *idx, int nid, ulong flags,
+ 					  struct memblock_type *type_a,
+ 					  struct memblock_type *type_b,
+ 					  phys_addr_t *out_start,
+ 					  phys_addr_t *out_end, int *out_nid)
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  {
 -	int idx_a = *idx & 0xffffffff;
 -	int idx_b = *idx >> 32;
 +	struct memblock_type *mem = &memblock.memory;
 +	struct memblock_type *rsv = &memblock.reserved;
 +	int mi = *idx & 0xffffffff;
 +	int ri = *idx >> 32;
 +	bool check_node = (nid != NUMA_NO_NODE) && (nid != MAX_NUMNODES);
  
 -	if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
 -		nid = NUMA_NO_NODE;
 +	if (nid == MAX_NUMNODES)
 +		pr_warn_once("%s: Usage of MAX_NUMNODES is depricated. Use NUMA_NO_NODE instead\n",
 +			     __func__);
  
  	if (*idx == (u64)ULLONG_MAX) {
 -		idx_a = type_a->cnt - 1;
 -		idx_b = type_b->cnt;
 +		mi = mem->cnt - 1;
 +		ri = rsv->cnt;
  	}
  
 -	for (; idx_a >= 0; idx_a--) {
 -		struct memblock_region *m = &type_a->regions[idx_a];
 -
 +	for ( ; mi >= 0; mi--) {
 +		struct memblock_region *m = &mem->regions[mi];
  		phys_addr_t m_start = m->base;
  		phys_addr_t m_end = m->base + m->size;
 -		int m_nid = memblock_get_region_node(m);
  
  		/* only memory regions are associated with nodes, check it */
 -		if (nid != NUMA_NO_NODE && nid != m_nid)
 +		if (check_node && nid != memblock_get_region_node(m))
  			continue;
  
  		/* skip hotpluggable memory regions if needed */
@@@ -956,23 -1057,41 +991,60 @@@ int __init_memblock memblock_set_node(p
  }
  #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
  
++<<<<<<< HEAD
++=======
+ static phys_addr_t __init memblock_alloc_range_nid(phys_addr_t size,
+ 					phys_addr_t align, phys_addr_t start,
+ 					phys_addr_t end, int nid, ulong flags)
+ {
+ 	phys_addr_t found;
+ 
+ 	if (!align)
+ 		align = SMP_CACHE_BYTES;
+ 
+ 	found = memblock_find_in_range_node(size, align, start, end, nid,
+ 					    flags);
+ 	if (found && !memblock_reserve(found, size)) {
+ 		/*
+ 		 * The min_count is set to 0 so that memblock allocations are
+ 		 * never reported as leaks.
+ 		 */
+ 		kmemleak_alloc(__va(found), size, 0, 0);
+ 		return found;
+ 	}
+ 	return 0;
+ }
+ 
+ phys_addr_t __init memblock_alloc_range(phys_addr_t size, phys_addr_t align,
+ 					phys_addr_t start, phys_addr_t end,
+ 					ulong flags)
+ {
+ 	return memblock_alloc_range_nid(size, align, start, end, NUMA_NO_NODE,
+ 					flags);
+ }
+ 
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  static phys_addr_t __init memblock_alloc_base_nid(phys_addr_t size,
  					phys_addr_t align, phys_addr_t max_addr,
- 					int nid)
+ 					int nid, ulong flags)
  {
++<<<<<<< HEAD
 +	phys_addr_t found;
 +
 +	if (WARN_ON(!align))
 +		align = __alignof__(long long);
 +
 +	/* align @size to avoid excessive fragmentation on reserved array */
 +	size = round_up(size, align);
 +
 +	found = memblock_find_in_range_node(size, align, 0, max_addr, nid);
 +	if (found && !memblock_reserve(found, size))
 +		return found;
 +
 +	return 0;
++=======
+ 	return memblock_alloc_range_nid(size, align, 0, max_addr, nid, flags);
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  }
  
  phys_addr_t __init memblock_alloc_nid(phys_addr_t size, phys_addr_t align, int nid)
@@@ -1012,6 -1133,208 +1086,211 @@@ phys_addr_t __init memblock_alloc_try_n
  	return memblock_alloc_base(size, align, MEMBLOCK_ALLOC_ACCESSIBLE);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * memblock_virt_alloc_internal - allocate boot memory block
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region to allocate (phys address)
+  * @max_addr: the upper bound of the memory region to allocate (phys address)
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * The @min_addr limit is dropped if it can not be satisfied and the allocation
+  * will fall back to memory below @min_addr. Also, allocation may fall back
+  * to any node in the system if the specified node can not
+  * hold the requested memory.
+  *
+  * The allocation is performed from memory region limited by
+  * memblock.current_limit if @max_addr == %BOOTMEM_ALLOC_ACCESSIBLE.
+  *
+  * The memory block is aligned on SMP_CACHE_BYTES if @align == 0.
+  *
+  * The phys address of allocated boot memory block is converted to virtual and
+  * allocated memory is reset to 0.
+  *
+  * In addition, function sets the min_count to 0 using kmemleak_alloc for
+  * allocated boot memory block, so that it is never reported as leaks.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ static void * __init memblock_virt_alloc_internal(
+ 				phys_addr_t size, phys_addr_t align,
+ 				phys_addr_t min_addr, phys_addr_t max_addr,
+ 				int nid)
+ {
+ 	phys_addr_t alloc;
+ 	void *ptr;
+ 
+ 	if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
+ 		nid = NUMA_NO_NODE;
+ 
+ 	/*
+ 	 * Detect any accidental use of these APIs after slab is ready, as at
+ 	 * this moment memblock may be deinitialized already and its
+ 	 * internal data may be destroyed (after execution of free_all_bootmem)
+ 	 */
+ 	if (WARN_ON_ONCE(slab_is_available()))
+ 		return kzalloc_node(size, GFP_NOWAIT, nid);
+ 
+ 	if (!align)
+ 		align = SMP_CACHE_BYTES;
+ 
+ 	if (max_addr > memblock.current_limit)
+ 		max_addr = memblock.current_limit;
+ 
+ again:
+ 	alloc = memblock_find_in_range_node(size, align, min_addr, max_addr,
+ 					    nid, MEMBLOCK_NONE);
+ 	if (alloc)
+ 		goto done;
+ 
+ 	if (nid != NUMA_NO_NODE) {
+ 		alloc = memblock_find_in_range_node(size, align, min_addr,
+ 						    max_addr, NUMA_NO_NODE,
+ 						    MEMBLOCK_NONE);
+ 		if (alloc)
+ 			goto done;
+ 	}
+ 
+ 	if (min_addr) {
+ 		min_addr = 0;
+ 		goto again;
+ 	} else {
+ 		goto error;
+ 	}
+ 
+ done:
+ 	memblock_reserve(alloc, size);
+ 	ptr = phys_to_virt(alloc);
+ 	memset(ptr, 0, size);
+ 
+ 	/*
+ 	 * The min_count is set to 0 so that bootmem allocated blocks
+ 	 * are never reported as leaks. This is because many of these blocks
+ 	 * are only referred via the physical address which is not
+ 	 * looked up by kmemleak.
+ 	 */
+ 	kmemleak_alloc(ptr, size, 0, 0);
+ 
+ 	return ptr;
+ 
+ error:
+ 	return NULL;
+ }
+ 
+ /**
+  * memblock_virt_alloc_try_nid_nopanic - allocate boot memory block
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region from where the allocation
+  *	  is preferred (phys address)
+  * @max_addr: the upper bound of the memory region from where the allocation
+  *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+  *	      allocate only from memory limited by memblock.current_limit value
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * Public version of _memblock_virt_alloc_try_nid_nopanic() which provides
+  * additional debug information (including caller info), if enabled.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ void * __init memblock_virt_alloc_try_nid_nopanic(
+ 				phys_addr_t size, phys_addr_t align,
+ 				phys_addr_t min_addr, phys_addr_t max_addr,
+ 				int nid)
+ {
+ 	memblock_dbg("%s: %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx %pF\n",
+ 		     __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 		     (u64)max_addr, (void *)_RET_IP_);
+ 	return memblock_virt_alloc_internal(size, align, min_addr,
+ 					     max_addr, nid);
+ }
+ 
+ /**
+  * memblock_virt_alloc_try_nid - allocate boot memory block with panicking
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region from where the allocation
+  *	  is preferred (phys address)
+  * @max_addr: the upper bound of the memory region from where the allocation
+  *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+  *	      allocate only from memory limited by memblock.current_limit value
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * Public panicking version of _memblock_virt_alloc_try_nid_nopanic()
+  * which provides debug information (including caller info), if enabled,
+  * and panics if the request can not be satisfied.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ void * __init memblock_virt_alloc_try_nid(
+ 			phys_addr_t size, phys_addr_t align,
+ 			phys_addr_t min_addr, phys_addr_t max_addr,
+ 			int nid)
+ {
+ 	void *ptr;
+ 
+ 	memblock_dbg("%s: %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx %pF\n",
+ 		     __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 		     (u64)max_addr, (void *)_RET_IP_);
+ 	ptr = memblock_virt_alloc_internal(size, align,
+ 					   min_addr, max_addr, nid);
+ 	if (ptr)
+ 		return ptr;
+ 
+ 	panic("%s: Failed to allocate %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx\n",
+ 	      __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 	      (u64)max_addr);
+ 	return NULL;
+ }
+ 
+ /**
+  * __memblock_free_early - free boot memory block
+  * @base: phys starting address of the  boot memory block
+  * @size: size of the boot memory block in bytes
+  *
+  * Free boot memory block previously allocated by memblock_virt_alloc_xx() API.
+  * The freeing memory will not be released to the buddy allocator.
+  */
+ void __init __memblock_free_early(phys_addr_t base, phys_addr_t size)
+ {
+ 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
+ 		     __func__, (u64)base, (u64)base + size - 1,
+ 		     (void *)_RET_IP_);
+ 	kmemleak_free_part(__va(base), size);
+ 	memblock_remove_range(&memblock.reserved, base, size);
+ }
+ 
+ /*
+  * __memblock_free_late - free bootmem block pages directly to buddy allocator
+  * @addr: phys starting address of the  boot memory block
+  * @size: size of the boot memory block in bytes
+  *
+  * This is only useful when the bootmem allocator has already been torn
+  * down, but we are still initializing the system.  Pages are released directly
+  * to the buddy allocator, no bootmem metadata is updated because it is gone.
+  */
+ void __init __memblock_free_late(phys_addr_t base, phys_addr_t size)
+ {
+ 	u64 cursor, end;
+ 
+ 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
+ 		     __func__, (u64)base, (u64)base + size - 1,
+ 		     (void *)_RET_IP_);
+ 	kmemleak_free_part(__va(base), size);
+ 	cursor = PFN_UP(base);
+ 	end = PFN_DOWN(base + size);
+ 
+ 	for (; cursor < end; cursor++) {
+ 		__free_pages_bootmem(pfn_to_page(cursor), 0);
+ 		totalram_pages++;
+ 	}
+ }
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  
  /*
   * Remaining API functions
diff --cc mm/nobootmem.c
index dbdd2db64f47,ad3641dcdbe7..000000000000
--- a/mm/nobootmem.c
+++ b/mm/nobootmem.c
@@@ -124,16 -117,30 +125,23 @@@ static unsigned long __init __free_memo
  static unsigned long __init free_low_memory_core_early(void)
  {
  	unsigned long count = 0;
 -	phys_addr_t start, end;
 +	phys_addr_t start, end, size;
  	u64 i;
  
++<<<<<<< HEAD
 +	for_each_free_mem_range(i, NUMA_NO_NODE, &start, &end, NULL)
++=======
+ 	memblock_clear_hotplug(0, -1);
+ 
+ 	for_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end,
+ 				NULL)
++>>>>>>> fc6daaf93151 (mm/memblock: add extra "flags" to memblock to allow selection of memory based on attribute)
  		count += __free_memory_core(start, end);
  
 -#ifdef CONFIG_ARCH_DISCARD_MEMBLOCK
 -	{
 -		phys_addr_t size;
 -
 -		/* Free memblock.reserved array if it was allocated */
 -		size = get_allocated_memblock_reserved_regions_info(&start);
 -		if (size)
 -			count += __free_memory_core(start, start + size);
 -
 -		/* Free memblock.memory array if it was allocated */
 -		size = get_allocated_memblock_memory_regions_info(&start);
 -		if (size)
 -			count += __free_memory_core(start, start + size);
 -	}
 -#endif
 +	/* free range that is used for reserved array if we allocate it */
 +	size = get_allocated_memblock_reserved_regions_info(&start);
 +	if (size)
 +		count += __free_memory_core(start, start + size);
  
  	return count;
  }
* Unmerged path mm/cma.c
* Unmerged path arch/s390/kernel/crash_dump.c
* Unmerged path arch/sparc/mm/init_64.c
diff --git a/arch/x86/kernel/check.c b/arch/x86/kernel/check.c
index 83a7995625a6..58118e207a69 100644
--- a/arch/x86/kernel/check.c
+++ b/arch/x86/kernel/check.c
@@ -91,7 +91,8 @@ void __init setup_bios_corruption_check(void)
 
 	corruption_check_size = round_up(corruption_check_size, PAGE_SIZE);
 
-	for_each_free_mem_range(i, NUMA_NO_NODE, &start, &end, NULL) {
+	for_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end,
+				NULL) {
 		start = clamp_t(phys_addr_t, round_up(start, PAGE_SIZE),
 				PAGE_SIZE, corruption_check_size);
 		end = clamp_t(phys_addr_t, round_down(end, PAGE_SIZE),
diff --git a/arch/x86/kernel/e820.c b/arch/x86/kernel/e820.c
index 988c00a1f60d..03152b906165 100644
--- a/arch/x86/kernel/e820.c
+++ b/arch/x86/kernel/e820.c
@@ -1120,7 +1120,8 @@ void __init memblock_find_dma_reserve(void)
 		nr_pages += end_pfn - start_pfn;
 	}
 
-	for_each_free_mem_range(u, NUMA_NO_NODE, &start, &end, NULL) {
+	for_each_free_mem_range(u, NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end,
+				NULL) {
 		start_pfn = min_t(unsigned long, PFN_UP(start), MAX_DMA_PFN);
 		end_pfn = min_t(unsigned long, PFN_DOWN(end), MAX_DMA_PFN);
 		if (start_pfn < end_pfn)
diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c
index 152d0c729562..9fddbeed8c43 100644
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@ -433,7 +433,7 @@ void __init add_highpages_with_active_regions(int nid,
 	phys_addr_t start, end;
 	u64 i;
 
-	for_each_free_mem_range(i, nid, &start, &end, NULL) {
+	for_each_free_mem_range(i, nid, MEMBLOCK_NONE, &start, &end, NULL) {
 		unsigned long pfn = clamp_t(unsigned long, PFN_UP(start),
 					    start_pfn, end_pfn);
 		unsigned long e_pfn = clamp_t(unsigned long, PFN_DOWN(end),
* Unmerged path arch/x86/mm/memtest.c
* Unmerged path include/linux/memblock.h
* Unmerged path mm/cma.c
* Unmerged path mm/memblock.c
* Unmerged path mm/nobootmem.c
