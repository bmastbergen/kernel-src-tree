sched/balancing: Fix 'local->avg_load > busiest->avg_load' case in fix_small_imbalance()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [kernel] sched: Fix 'local->avg_load > busiest->avg_load' case in fix_small_imbalance() (Jiri Olsa) [1211784]
Rebuild_FUZZ: 93.98%
commit-author Vladimir Davydov <vdavydov@parallels.com>
commit 3029ede39373c368f402a76896600d85a4f7121b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/3029ede3.failed

In busiest->group_imb case we can come to fix_small_imbalance() with
local->avg_load > busiest->avg_load. This can result in wrong imbalance
fix-up, because there is the following check there where all the
members are unsigned:

if (busiest->avg_load - local->avg_load + scaled_busy_load_per_task >=
    (scaled_busy_load_per_task * imbn)) {
	env->imbalance = busiest->load_per_task;
	return;
}

As a result we can end up constantly bouncing tasks from one cpu to
another if there are pinned tasks.

Fix it by substituting the subtraction with an equivalent addition in
the check.

[ The bug can be caught by running 2*N cpuhogs pinned to two logical cpus
  belonging to different cores on an HT-enabled machine with N logical
  cpus: just look at se.nr_migrations growth. ]

	Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/ef167822e5c5b2d96cf5b0e3e4f4bdff3f0414a2.1379252740.git.vdavydov@parallels.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 3029ede39373c368f402a76896600d85a4f7121b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index e5b7e88448c9,2aedaccebcc8..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5901,8 -4823,8 +5901,13 @@@ void fix_small_imbalance(struct lb_env 
  		(busiest->load_per_task * SCHED_POWER_SCALE) /
  		busiest->group_power;
  
++<<<<<<< HEAD
 +	if (busiest->avg_load - this->avg_load + scaled_busy_load_per_task >=
 +	    (scaled_busy_load_per_task * imbn)) {
++=======
+ 	if (busiest->avg_load + scaled_busy_load_per_task >=
+ 	    local->avg_load + (scaled_busy_load_per_task * imbn)) {
++>>>>>>> 3029ede39373 (sched/balancing: Fix 'local->avg_load > busiest->avg_load' case in fix_small_imbalance())
  		env->imbalance = busiest->load_per_task;
  		return;
  	}
* Unmerged path kernel/sched/fair.c
