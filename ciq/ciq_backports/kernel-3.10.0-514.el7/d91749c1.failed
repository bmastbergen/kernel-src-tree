mm/page_alloc.c: calculate zone_start_pfn at zone_spanned_pages_in_node()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] page_alloc.c: calculate zone_start_pfn at zone_spanned_pages_in_node() (Yasuaki Ishimatsu) [1270209]
Rebuild_FUZZ: 97.90%
commit-author Taku Izumi <izumi.taku@jp.fujitsu.com>
commit d91749c1dda71a7030c054a5ab8dc5419bc6730b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/d91749c1.failed

Xeon E7 v3 based systems supports Address Range Mirroring and UEFI BIOS
complied with UEFI spec 2.5 can notify which ranges are mirrored
(reliable) via EFI memory map.  Now Linux kernel utilize its information
and allocates boot time memory from reliable region.

My requirement is:
  - allocate kernel memory from mirrored region
  - allocate user memory from non-mirrored region

In order to meet my requirement, ZONE_MOVABLE is useful.  By arranging
non-mirrored range into ZONE_MOVABLE, mirrored memory is used for kernel
allocations.

My idea is to extend existing "kernelcore" option and introduces
kernelcore=mirror option.  By specifying "mirror" instead of specifying
the amount of memory, non-mirrored region will be arranged into
ZONE_MOVABLE.

Earlier discussions are at:
 https://lkml.org/lkml/2015/10/9/24
 https://lkml.org/lkml/2015/10/15/9
 https://lkml.org/lkml/2015/11/27/18
 https://lkml.org/lkml/2015/12/8/836

For example, suppose 2-nodes system with the following memory range:

  node 0 [mem 0x0000000000001000-0x000000109fffffff]
  node 1 [mem 0x00000010a0000000-0x000000209fffffff]
and the following ranges are marked as reliable (mirrored):
  [0x0000000000000000-0x0000000100000000]
  [0x0000000100000000-0x0000000180000000]
  [0x0000000800000000-0x0000000880000000]
  [0x00000010a0000000-0x0000001120000000]
  [0x00000017a0000000-0x0000001820000000]

If you specify kernelcore=mirror, ZONE_NORMAL and ZONE_MOVABLE are
arranged like bellow:

 - node 0:
  ZONE_NORMAL : [0x0000000100000000-0x00000010a0000000]
  ZONE_MOVABLE: [0x0000000180000000-0x00000010a0000000]
 - node 1:
  ZONE_NORMAL : [0x00000010a0000000-0x00000020a0000000]
  ZONE_MOVABLE: [0x0000001120000000-0x00000020a0000000]

In overlapped range, pages to be ZONE_MOVABLE in ZONE_NORMAL are treated
as absent pages, and vice versa.

This patch (of 2):

Currently each zone's zone_start_pfn is calculated at
free_area_init_core().  However zone's range is fixed at the time when
invoking zone_spanned_pages_in_node().

This patch changes how each zone->zone_start_pfn is calculated in
zone_spanned_pages_in_node().

	Signed-off-by: Taku Izumi <izumi.taku@jp.fujitsu.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Xishi Qiu <qiuxishi@huawei.com>
	Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
	Cc: Mel Gorman <mel@csn.ul.ie>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Steve Capper <steve.capper@linaro.org>
	Cc: Sudeep Holla <sudeep.holla@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d91749c1dda71a7030c054a5ab8dc5419bc6730b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index f60ded95bce9,0d20a19151a4..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -4462,21 -4951,25 +4462,37 @@@ static void __meminit adjust_zone_range
   */
  static unsigned long __meminit zone_spanned_pages_in_node(int nid,
  					unsigned long zone_type,
++<<<<<<< HEAD
 +					unsigned long *ignored)
 +{
 +	unsigned long node_start_pfn, node_end_pfn;
 +	unsigned long zone_start_pfn, zone_end_pfn;
 +
 +	/* Get the start and end of the node and zone */
 +	get_pfn_range_for_nid(nid, &node_start_pfn, &node_end_pfn);
 +	zone_start_pfn = arch_zone_lowest_possible_pfn[zone_type];
 +	zone_end_pfn = arch_zone_highest_possible_pfn[zone_type];
++=======
+ 					unsigned long node_start_pfn,
+ 					unsigned long node_end_pfn,
+ 					unsigned long *zone_start_pfn,
+ 					unsigned long *zone_end_pfn,
+ 					unsigned long *ignored)
+ {
+ 	/* When hotadd a new node from cpu_up(), the node should be empty */
+ 	if (!node_start_pfn && !node_end_pfn)
+ 		return 0;
+ 
+ 	/* Get the start and end of the zone */
+ 	*zone_start_pfn = arch_zone_lowest_possible_pfn[zone_type];
+ 	*zone_end_pfn = arch_zone_highest_possible_pfn[zone_type];
++>>>>>>> d91749c1dda7 (mm/page_alloc.c: calculate zone_start_pfn at zone_spanned_pages_in_node())
  	adjust_zone_range_for_zone_movable(nid, zone_type,
  				node_start_pfn, node_end_pfn,
- 				&zone_start_pfn, &zone_end_pfn);
+ 				zone_start_pfn, zone_end_pfn);
  
  	/* Check that this node has pages within the zone's required range */
- 	if (zone_end_pfn < node_start_pfn || zone_start_pfn > node_end_pfn)
+ 	if (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)
  		return 0;
  
  	/* Move the zone boundaries inside the node if necessary */
@@@ -4543,8 -5040,20 +4559,23 @@@ static unsigned long __meminit zone_abs
  #else /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
  static inline unsigned long __meminit zone_spanned_pages_in_node(int nid,
  					unsigned long zone_type,
++<<<<<<< HEAD
++=======
+ 					unsigned long node_start_pfn,
+ 					unsigned long node_end_pfn,
+ 					unsigned long *zone_start_pfn,
+ 					unsigned long *zone_end_pfn,
++>>>>>>> d91749c1dda7 (mm/page_alloc.c: calculate zone_start_pfn at zone_spanned_pages_in_node())
  					unsigned long *zones_size)
  {
+ 	unsigned int zone;
+ 
+ 	*zone_start_pfn = node_start_pfn;
+ 	for (zone = 0; zone < zone_type; zone++)
+ 		*zone_start_pfn += zones_size[zone];
+ 
+ 	*zone_end_pfn = *zone_start_pfn + zones_size[zone_type];
+ 
  	return zones_size[zone_type];
  }
  
@@@ -4561,21 -5072,40 +4592,49 @@@ static inline unsigned long __meminit z
  #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
  
  static void __meminit calculate_node_totalpages(struct pglist_data *pgdat,
 -						unsigned long node_start_pfn,
 -						unsigned long node_end_pfn,
 -						unsigned long *zones_size,
 -						unsigned long *zholes_size)
 +		unsigned long *zones_size, unsigned long *zholes_size)
  {
 -	unsigned long realtotalpages = 0, totalpages = 0;
 +	unsigned long realtotalpages, totalpages = 0;
  	enum zone_type i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < MAX_NR_ZONES; i++)
 +		totalpages += zone_spanned_pages_in_node(pgdat->node_id, i,
 +								zones_size);
++=======
+ 	for (i = 0; i < MAX_NR_ZONES; i++) {
+ 		struct zone *zone = pgdat->node_zones + i;
+ 		unsigned long zone_start_pfn, zone_end_pfn;
+ 		unsigned long size, real_size;
+ 
+ 		size = zone_spanned_pages_in_node(pgdat->node_id, i,
+ 						  node_start_pfn,
+ 						  node_end_pfn,
+ 						  &zone_start_pfn,
+ 						  &zone_end_pfn,
+ 						  zones_size);
+ 		real_size = size - zone_absent_pages_in_node(pgdat->node_id, i,
+ 						  node_start_pfn, node_end_pfn,
+ 						  zholes_size);
+ 		if (size)
+ 			zone->zone_start_pfn = zone_start_pfn;
+ 		else
+ 			zone->zone_start_pfn = 0;
+ 		zone->spanned_pages = size;
+ 		zone->present_pages = real_size;
+ 
+ 		totalpages += size;
+ 		realtotalpages += real_size;
+ 	}
+ 
++>>>>>>> d91749c1dda7 (mm/page_alloc.c: calculate zone_start_pfn at zone_spanned_pages_in_node())
  	pgdat->node_spanned_pages = totalpages;
 +
 +	realtotalpages = totalpages;
 +	for (i = 0; i < MAX_NR_ZONES; i++)
 +		realtotalpages -=
 +			zone_absent_pages_in_node(pgdat->node_id, i,
 +								zholes_size);
  	pgdat->node_present_pages = realtotalpages;
  	printk(KERN_DEBUG "On node %d totalpages: %lu\n", pgdat->node_id,
  							realtotalpages);
@@@ -4704,10 -5238,10 +4762,11 @@@ static void __paginginit free_area_init
  	for (j = 0; j < MAX_NR_ZONES; j++) {
  		struct zone *zone = pgdat->node_zones + j;
  		unsigned long size, realsize, freesize, memmap_pages;
+ 		unsigned long zone_start_pfn = zone->zone_start_pfn;
  
 -		size = zone->spanned_pages;
 -		realsize = freesize = zone->present_pages;
 +		size = zone_spanned_pages_in_node(nid, j, zones_size);
 +		realsize = freesize = size - zone_absent_pages_in_node(nid, j,
 +								zholes_size);
  
  		/*
  		 * Adjust freesize so that it accounts for how much memory
@@@ -4770,11 -5304,9 +4829,10 @@@
  
  		set_pageblock_order();
  		setup_usemap(pgdat, zone, zone_start_pfn, size);
 -		ret = init_currently_empty_zone(zone, zone_start_pfn, size);
 +		ret = init_currently_empty_zone(zone, zone_start_pfn,
 +						size, MEMMAP_EARLY);
  		BUG_ON(ret);
  		memmap_init(size, nid, j, zone_start_pfn);
- 		zone_start_pfn += size;
  	}
  }
  
@@@ -4827,9 -5366,19 +4885,22 @@@ void __paginginit free_area_init_node(i
  	/* pg_data_t should be reset to zero when it's allocated */
  	WARN_ON(pgdat->nr_zones || pgdat->classzone_idx);
  
 -	reset_deferred_meminit(pgdat);
  	pgdat->node_id = nid;
  	pgdat->node_start_pfn = node_start_pfn;
++<<<<<<< HEAD
 +	calculate_node_totalpages(pgdat, zones_size, zholes_size);
++=======
+ #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+ 	get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
+ 	pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,
+ 		(u64)start_pfn << PAGE_SHIFT,
+ 		end_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);
+ #else
+ 	start_pfn = node_start_pfn;
+ #endif
+ 	calculate_node_totalpages(pgdat, start_pfn, end_pfn,
+ 				  zones_size, zholes_size);
++>>>>>>> d91749c1dda7 (mm/page_alloc.c: calculate zone_start_pfn at zone_spanned_pages_in_node())
  
  	alloc_node_mem_map(pgdat);
  #ifdef CONFIG_FLAT_NODE_MEM_MAP
* Unmerged path mm/page_alloc.c
