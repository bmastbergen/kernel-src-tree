RDS/IW: Convert to new memory registration API

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Sagi Grimberg <sagig@mellanox.com>
commit 9ddc87374af90ee694bcc8d3412c6f2b51df1fea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/9ddc8737.failed

Get rid of fast_reg page list and its construction.
Instead, just pass the RDS sg list to ib_map_mr_sg
and post the new ib_reg_wr.

This is done both for server IW RDMA_READ registration
and the client remote key registration.

	Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
	Acked-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 9ddc87374af90ee694bcc8d3412c6f2b51df1fea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/rds/iw.h
#	net/rds/iw_rdma.c
#	net/rds/iw_send.c
diff --cc net/rds/iw.h
index 04ce3b193f79,5af01d1758b3..000000000000
--- a/net/rds/iw.h
+++ b/net/rds/iw.h
@@@ -74,10 -74,13 +74,17 @@@ struct rds_iw_send_work 
  	struct rm_rdma_op	*s_op;
  	struct rds_iw_mapping	*s_mapping;
  	struct ib_mr		*s_mr;
- 	struct ib_fast_reg_page_list *s_page_list;
  	unsigned char		s_remap_count;
  
++<<<<<<< HEAD
 +	struct ib_send_wr	s_wr;
++=======
+ 	union {
+ 		struct ib_send_wr	s_send_wr;
+ 		struct ib_rdma_wr	s_rdma_wr;
+ 		struct ib_reg_wr	s_reg_wr;
+ 	};
++>>>>>>> 9ddc87374af9 (RDS/IW: Convert to new memory registration API)
  	struct ib_sge		s_sge[RDS_IW_MAX_SGE];
  	unsigned long		s_queued;
  };
diff --cc net/rds/iw_rdma.c
index b6ec2a6c7add,47bd68451ff7..000000000000
--- a/net/rds/iw_rdma.c
+++ b/net/rds/iw_rdma.c
@@@ -689,38 -660,40 +656,65 @@@ static int rds_iw_init_reg(struct rds_i
  	return 0;
  }
  
- static int rds_iw_rdma_build_fastreg(struct rds_iw_mapping *mapping)
+ static int rds_iw_rdma_reg_mr(struct rds_iw_mapping *mapping)
  {
  	struct rds_iw_mr *ibmr = mapping->m_mr;
++<<<<<<< HEAD
 +	struct ib_send_wr f_wr, *failed_wr;
 +	int ret;
++=======
+ 	struct rds_iw_scatterlist *m_sg = &mapping->m_sg;
+ 	struct ib_reg_wr reg_wr;
+ 	struct ib_send_wr *failed_wr;
+ 	int ret, n;
+ 
+ 	n = ib_map_mr_sg_zbva(ibmr->mr, m_sg->list, m_sg->len, PAGE_SIZE);
+ 	if (unlikely(n != m_sg->len))
+ 		return n < 0 ? n : -EINVAL;
+ 
+ 	reg_wr.wr.next = NULL;
+ 	reg_wr.wr.opcode = IB_WR_REG_MR;
+ 	reg_wr.wr.wr_id = RDS_IW_REG_WR_ID;
+ 	reg_wr.wr.num_sge = 0;
+ 	reg_wr.mr = ibmr->mr;
+ 	reg_wr.key = mapping->m_rkey;
+ 	reg_wr.access = IB_ACCESS_LOCAL_WRITE |
+ 			IB_ACCESS_REMOTE_READ |
+ 			IB_ACCESS_REMOTE_WRITE;
++>>>>>>> 9ddc87374af9 (RDS/IW: Convert to new memory registration API)
  
  	/*
- 	 * Perform a WR for the fast_reg_mr. Each individual page
+ 	 * Perform a WR for the reg_mr. Each individual page
  	 * in the sg list is added to the fast reg page list and placed
- 	 * inside the fast_reg_mr WR.  The key used is a rolling 8bit
+ 	 * inside the reg_mr WR.  The key used is a rolling 8bit
  	 * counter, which should guarantee uniqueness.
  	 */
  	ib_update_fast_reg_key(ibmr->mr, ibmr->remap_count++);
  	mapping->m_rkey = ibmr->mr->rkey;
  
++<<<<<<< HEAD
 +	memset(&f_wr, 0, sizeof(f_wr));
 +	f_wr.wr_id = RDS_IW_FAST_REG_WR_ID;
 +	f_wr.opcode = IB_WR_FAST_REG_MR;
 +	f_wr.wr.fast_reg.length = mapping->m_sg.bytes;
 +	f_wr.wr.fast_reg.rkey = mapping->m_rkey;
 +	f_wr.wr.fast_reg.page_list = ibmr->page_list;
 +	f_wr.wr.fast_reg.page_list_len = mapping->m_sg.dma_len;
 +	f_wr.wr.fast_reg.page_shift = PAGE_SHIFT;
 +	f_wr.wr.fast_reg.access_flags = IB_ACCESS_LOCAL_WRITE |
 +				IB_ACCESS_REMOTE_READ |
 +				IB_ACCESS_REMOTE_WRITE;
 +	f_wr.wr.fast_reg.iova_start = 0;
 +	f_wr.send_flags = IB_SEND_SIGNALED;
 +
 +	failed_wr = &f_wr;
 +	ret = ib_post_send(ibmr->cm_id->qp, &f_wr, &failed_wr);
 +	BUG_ON(failed_wr != &f_wr);
++=======
+ 	failed_wr = &reg_wr.wr;
+ 	ret = ib_post_send(ibmr->cm_id->qp, &reg_wr.wr, &failed_wr);
+ 	BUG_ON(failed_wr != &reg_wr.wr);
++>>>>>>> 9ddc87374af9 (RDS/IW: Convert to new memory registration API)
  	if (ret)
  		printk_ratelimited(KERN_WARNING "RDS/IW: %s:%d ib_post_send returned %d\n",
  			__func__, __LINE__, ret);
diff --cc net/rds/iw_send.c
index 4626926033f8,e20bd503f4bd..000000000000
--- a/net/rds/iw_send.c
+++ b/net/rds/iw_send.c
@@@ -177,9 -170,7 +170,13 @@@ void rds_iw_send_clear_ring(struct rds_
  	for (i = 0, send = ic->i_sends; i < ic->i_send_ring.w_nr; i++, send++) {
  		BUG_ON(!send->s_mr);
  		ib_dereg_mr(send->s_mr);
++<<<<<<< HEAD
 +		BUG_ON(!send->s_page_list);
 +		ib_free_fast_reg_page_list(send->s_page_list);
 +		if (send->s_wr.opcode == 0xdead)
++=======
+ 		if (send->s_send_wr.opcode == 0xdead)
++>>>>>>> 9ddc87374af9 (RDS/IW: Convert to new memory registration API)
  			continue;
  		if (send->s_rm)
  			rds_iw_send_unmap_rm(ic, send, IB_WC_WR_FLUSH_ERR);
@@@ -770,24 -761,26 +767,43 @@@ out
  	return ret;
  }
  
- static void rds_iw_build_send_fastreg(struct rds_iw_device *rds_iwdev, struct rds_iw_connection *ic, struct rds_iw_send_work *send, int nent, int len, u64 sg_addr)
+ static int rds_iw_build_send_reg(struct rds_iw_send_work *send,
+ 				 struct scatterlist *sg,
+ 				 int sg_nents)
  {
++<<<<<<< HEAD
 +	BUG_ON(nent > send->s_page_list->max_page_list_len);
 +	/*
 +	 * Perform a WR for the fast_reg_mr. Each individual page
 +	 * in the sg list is added to the fast reg page list and placed
 +	 * inside the fast_reg_mr WR.
 +	 */
 +	send->s_wr.opcode = IB_WR_FAST_REG_MR;
 +	send->s_wr.wr.fast_reg.length = len;
 +	send->s_wr.wr.fast_reg.rkey = send->s_mr->rkey;
 +	send->s_wr.wr.fast_reg.page_list = send->s_page_list;
 +	send->s_wr.wr.fast_reg.page_list_len = nent;
 +	send->s_wr.wr.fast_reg.page_shift = PAGE_SHIFT;
 +	send->s_wr.wr.fast_reg.access_flags = IB_ACCESS_REMOTE_WRITE;
 +	send->s_wr.wr.fast_reg.iova_start = sg_addr;
++=======
+ 	int n;
+ 
+ 	n = ib_map_mr_sg(send->s_mr, sg, sg_nents, PAGE_SIZE);
+ 	if (unlikely(n != sg_nents))
+ 		return n < 0 ? n : -EINVAL;
+ 
+ 	send->s_reg_wr.wr.opcode = IB_WR_REG_MR;
+ 	send->s_reg_wr.wr.wr_id = 0;
+ 	send->s_reg_wr.wr.num_sge = 0;
+ 	send->s_reg_wr.mr = send->s_mr;
+ 	send->s_reg_wr.key = send->s_mr->rkey;
+ 	send->s_reg_wr.access = IB_ACCESS_REMOTE_WRITE;
++>>>>>>> 9ddc87374af9 (RDS/IW: Convert to new memory registration API)
  
  	ib_update_fast_reg_key(send->s_mr, send->s_remap_count++);
+ 
+ 	return 0;
  }
  
  int rds_iw_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op)
@@@ -861,9 -855,10 +878,10 @@@
  	scat = &op->op_sg[0];
  	sent = 0;
  	num_sge = op->op_count;
+ 	sg_nents = 0;
  
  	for (i = 0; i < work_alloc && scat != &op->op_sg[op->op_count]; i++) {
 -		send->s_rdma_wr.wr.send_flags = 0;
 +		send->s_wr.send_flags = 0;
  		send->s_queued = jiffies;
  
  		/*
@@@ -889,21 -884,22 +907,26 @@@
  		send->s_op = op;
  
  		if (num_sge > rds_iwdev->max_sge) {
 -			send->s_rdma_wr.wr.num_sge = rds_iwdev->max_sge;
 +			send->s_wr.num_sge = rds_iwdev->max_sge;
  			num_sge -= rds_iwdev->max_sge;
  		} else
 -			send->s_rdma_wr.wr.num_sge = num_sge;
 +			send->s_wr.num_sge = num_sge;
  
 -		send->s_rdma_wr.wr.next = NULL;
 +		send->s_wr.next = NULL;
  
  		if (prev)
 -			prev->s_send_wr.next = &send->s_rdma_wr.wr;
 +			prev->s_wr.next = &send->s_wr;
  
 -		for (j = 0; j < send->s_rdma_wr.wr.num_sge &&
 -		     scat != &op->op_sg[op->op_count]; j++) {
 +		for (j = 0; j < send->s_wr.num_sge && scat != &op->op_sg[op->op_count]; j++) {
  			len = ib_sg_dma_len(ic->i_cm_id->device, scat);
  
++<<<<<<< HEAD
 +			if (send->s_wr.opcode == IB_WR_RDMA_READ_WITH_INV)
 +				send->s_page_list->page_list[j] = ib_sg_dma_address(ic->i_cm_id->device, scat);
++=======
+ 			if (send->s_rdma_wr.wr.opcode == IB_WR_RDMA_READ_WITH_INV)
+ 				sg_nents++;
++>>>>>>> 9ddc87374af9 (RDS/IW: Convert to new memory registration API)
  			else {
  				send->s_sge[j].addr = ib_sg_dma_address(ic->i_cm_id->device, scat);
  				send->s_sge[j].length = len;
* Unmerged path net/rds/iw.h
* Unmerged path net/rds/iw_rdma.c
* Unmerged path net/rds/iw_send.c
