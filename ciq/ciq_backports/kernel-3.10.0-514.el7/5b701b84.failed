thp: decrement refcount on huge zero page if it is split

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit 5b701b846aad7909d20693bcced2522d0ce8d1bc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5b701b84.failed

The DAX code neglected to put the refcount on the huge zero page.
Also we must notify on splits.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Signed-off-by: Matthew Wilcox <willy@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5b701b846aad7909d20693bcced2522d0ce8d1bc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index 43c4703f033a,7510b6f683e9..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -2927,20 -2975,20 +2927,35 @@@ void __split_huge_page_pmd(struct vm_ar
  again:
  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
  	ptl = pmd_lock(mm, pmd);
++<<<<<<< HEAD
 +	if (unlikely(!pmd_trans_huge(*pmd))) {
 +		spin_unlock(ptl);
 +		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 +		return;
++=======
+ 	if (unlikely(!pmd_trans_huge(*pmd)))
+ 		goto unlock;
+ 	if (vma_is_dax(vma)) {
+ 		pmd_t _pmd = pmdp_huge_clear_flush_notify(vma, haddr, pmd);
+ 		if (is_huge_zero_pmd(_pmd))
+ 			put_huge_zero_page();
+ 	} else if (is_huge_zero_pmd(*pmd)) {
+ 		__split_huge_zero_page_pmd(vma, haddr, pmd);
+ 	} else {
+ 		page = pmd_page(*pmd);
+ 		VM_BUG_ON_PAGE(!page_count(page), page);
+ 		get_page(page);
++>>>>>>> 5b701b846aad (thp: decrement refcount on huge zero page if it is split)
 +	}
 +	if (is_huge_zero_pmd(*pmd)) {
 +		__split_huge_zero_page_pmd(vma, haddr, pmd);
 +		spin_unlock(ptl);
 +		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 +		return;
  	}
 - unlock:
 +	page = pmd_page(*pmd);
 +	VM_BUG_ON_PAGE(!page_count(page), page);
 +	get_page(page);
  	spin_unlock(ptl);
  	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
  
* Unmerged path mm/huge_memory.c
