IB/core: Add arbitrary sg_list support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Sagi Grimberg <sagig@mellanox.com>
commit f5aa9159a418726d74b67c8815ffd2739afb4c7a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f5aa9159.failed

Devices that are capable in registering SG lists
with gaps can now expose it in the core to ULPs
using a new device capability IB_DEVICE_SG_GAPS_REG
(in a new field device_cap_flags_ex in the device attributes
as we ran out of bits), and a new mr_type IB_MR_TYPE_SG_GAPS_REG
which allocates a memory region which is capable of handling
SG lists with gaps.

	Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit f5aa9159a418726d74b67c8815ffd2739afb4c7a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/verbs.c
diff --cc drivers/infiniband/core/verbs.c
index 6ff33ede8c15,16f3fb1b9d75..000000000000
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@@ -1456,3 -1550,112 +1456,115 @@@ int ib_check_mr_status(struct ib_mr *mr
  		mr->device->check_mr_status(mr, check_mask, mr_status) : -ENOSYS;
  }
  EXPORT_SYMBOL(ib_check_mr_status);
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * ib_map_mr_sg() - Map the largest prefix of a dma mapped SG list
+  *     and set it the memory region.
+  * @mr:            memory region
+  * @sg:            dma mapped scatterlist
+  * @sg_nents:      number of entries in sg
+  * @page_size:     page vector desired page size
+  *
+  * Constraints:
+  * - The first sg element is allowed to have an offset.
+  * - Each sg element must be aligned to page_size (or physically
+  *   contiguous to the previous element). In case an sg element has a
+  *   non contiguous offset, the mapping prefix will not include it.
+  * - The last sg element is allowed to have length less than page_size.
+  * - If sg_nents total byte length exceeds the mr max_num_sge * page_size
+  *   then only max_num_sg entries will be mapped.
+  * - If the MR was allocated with type IB_MR_TYPE_SG_GAPS_REG, non of these
+  *   constraints holds and the page_size argument is ignored.
+  *
+  * Returns the number of sg elements that were mapped to the memory region.
+  *
+  * After this completes successfully, the  memory region
+  * is ready for registration.
+  */
+ int ib_map_mr_sg(struct ib_mr *mr,
+ 		 struct scatterlist *sg,
+ 		 int sg_nents,
+ 		 unsigned int page_size)
+ {
+ 	if (unlikely(!mr->device->map_mr_sg))
+ 		return -ENOSYS;
+ 
+ 	mr->page_size = page_size;
+ 
+ 	return mr->device->map_mr_sg(mr, sg, sg_nents);
+ }
+ EXPORT_SYMBOL(ib_map_mr_sg);
+ 
+ /**
+  * ib_sg_to_pages() - Convert the largest prefix of a sg list
+  *     to a page vector
+  * @mr:            memory region
+  * @sgl:           dma mapped scatterlist
+  * @sg_nents:      number of entries in sg
+  * @set_page:      driver page assignment function pointer
+  *
+  * Core service helper for drivers to convert the largest
+  * prefix of given sg list to a page vector. The sg list
+  * prefix converted is the prefix that meet the requirements
+  * of ib_map_mr_sg.
+  *
+  * Returns the number of sg elements that were assigned to
+  * a page vector.
+  */
+ int ib_sg_to_pages(struct ib_mr *mr,
+ 		   struct scatterlist *sgl,
+ 		   int sg_nents,
+ 		   int (*set_page)(struct ib_mr *, u64))
+ {
+ 	struct scatterlist *sg;
+ 	u64 last_end_dma_addr = 0;
+ 	unsigned int last_page_off = 0;
+ 	u64 page_mask = ~((u64)mr->page_size - 1);
+ 	int i, ret;
+ 
+ 	mr->iova = sg_dma_address(&sgl[0]);
+ 	mr->length = 0;
+ 
+ 	for_each_sg(sgl, sg, sg_nents, i) {
+ 		u64 dma_addr = sg_dma_address(sg);
+ 		unsigned int dma_len = sg_dma_len(sg);
+ 		u64 end_dma_addr = dma_addr + dma_len;
+ 		u64 page_addr = dma_addr & page_mask;
+ 
+ 		/*
+ 		 * For the second and later elements, check whether either the
+ 		 * end of element i-1 or the start of element i is not aligned
+ 		 * on a page boundary.
+ 		 */
+ 		if (i && (last_page_off != 0 || page_addr != dma_addr)) {
+ 			/* Stop mapping if there is a gap. */
+ 			if (last_end_dma_addr != dma_addr)
+ 				break;
+ 
+ 			/*
+ 			 * Coalesce this element with the last. If it is small
+ 			 * enough just update mr->length. Otherwise start
+ 			 * mapping from the next page.
+ 			 */
+ 			goto next_page;
+ 		}
+ 
+ 		do {
+ 			ret = set_page(mr, page_addr);
+ 			if (unlikely(ret < 0))
+ 				return i ? : ret;
+ next_page:
+ 			page_addr += mr->page_size;
+ 		} while (page_addr < end_dma_addr);
+ 
+ 		mr->length += dma_len;
+ 		last_end_dma_addr = end_dma_addr;
+ 		last_page_off = end_dma_addr & ~page_mask;
+ 	}
+ 
+ 	return i;
+ }
+ EXPORT_SYMBOL(ib_sg_to_pages);
++>>>>>>> f5aa9159a418 (IB/core: Add arbitrary sg_list support)
* Unmerged path drivers/infiniband/core/verbs.c
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b833561dbb05..5bfe9d69cadc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -169,6 +169,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
 	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
 	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
+	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
 };
 
 enum ib_signature_prot_cap {
@@ -601,10 +602,15 @@ __attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
  * @IB_MR_TYPE_SIGNATURE:     memory region that is used for
  *                            signature operations (data-integrity
  *                            capable regions)
+ * @IB_MR_TYPE_SG_GAPS:       memory region that is capable to
+ *                            register any arbitrary sg lists (without
+ *                            the normal mr constraints - see
+ *                            ib_map_mr_sg)
  */
 enum ib_mr_type {
 	IB_MR_TYPE_MEM_REG,
 	IB_MR_TYPE_SIGNATURE,
+	IB_MR_TYPE_SG_GAPS,
 };
 
 /**
