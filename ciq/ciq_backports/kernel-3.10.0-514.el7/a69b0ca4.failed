perf: Fix cloning

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit a69b0ca4ac3bf5427b571f11cbf33f0a32b728d5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a69b0ca4.failed

Alexander reported that when the 'original' context gets destroyed, no
new clones happen.

This can happen irrespective of the ctx switch optimization, any task
can die, even the parent, and we want to continue monitoring the task
hierarchy until we either close the event or no tasks are left in the
hierarchy.

perf_event_init_context() will attempt to pin the 'parent' context
during clone(). At that point current is the parent, and since current
cannot have exited while executing clone(), its context cannot have
passed through perf_event_exit_task_context(). Therefore
perf_pin_task_context() cannot observe ctx->task == TASK_TOMBSTONE.

However, since inherit_event() does:

	if (parent_event->parent)
		parent_event = parent_event->parent;

it looks at the 'original' event when it does: is_orphaned_event().
This can return true if the context that contains the this event has
passed through perf_event_exit_task_context(). And thus we'll fail to
clone the perf context.

Fix this by adding a new state: STATE_DEAD, which is set by
perf_release() to indicate that the filedesc (or kernel reference) is
dead and there are no observers for our data left.

Only for STATE_DEAD will is_orphaned_event() be true and inhibit
cloning.

STATE_EXIT is otherwise preserved such that is_event_hup() remains
functional and will report when the observed task hierarchy becomes
empty.

	Reported-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Tested-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: dvyukov@google.com
	Cc: eranian@google.com
	Cc: oleg@redhat.com
	Cc: panand@redhat.com
	Cc: sasha.levin@oracle.com
	Cc: vince@deater.net
Fixes: c6e5b73242d2 ("perf: Synchronously clean up child events")
Link: http://lkml.kernel.org/r/20160224174947.919845295@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a69b0ca4ac3bf5427b571f11cbf33f0a32b728d5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/perf_event.h
#	kernel/events/core.c
diff --cc include/linux/perf_event.h
index 41fd463bc411,39156619e108..000000000000
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@@ -301,9 -397,8 +301,13 @@@ struct pmu 
   * enum perf_event_active_state - the states of a event
   */
  enum perf_event_active_state {
++<<<<<<< HEAD
 +#ifndef __GENKSYMS__
++=======
+ 	PERF_EVENT_STATE_DEAD		= -4,
++>>>>>>> a69b0ca4ac3b (perf: Fix cloning)
  	PERF_EVENT_STATE_EXIT		= -3,
 +#endif
  	PERF_EVENT_STATE_ERROR		= -2,
  	PERF_EVENT_STATE_OFF		= -1,
  	PERF_EVENT_STATE_INACTIVE	=  0,
diff --cc kernel/events/core.c
index 0e60facd7da7,92d6999a4f2f..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -1512,45 -1643,17 +1512,49 @@@ out
  		perf_event__header_size(tmp);
  }
  
 +/*
 + * User event without the task.
 + */
  static bool is_orphaned_event(struct perf_event *event)
  {
++<<<<<<< HEAD
 +	return event && !is_kernel_event(event) && !event->owner;
++=======
+ 	return event->state == PERF_EVENT_STATE_DEAD;
++>>>>>>> a69b0ca4ac3b (perf: Fix cloning)
 +}
 +
 +/*
 + * Event has a parent but parent's task finished and it's
 + * alive only because of children holding refference.
 + */
 +static bool is_orphaned_child(struct perf_event *event)
 +{
 +	return is_orphaned_event(event->parent);
 +}
 +
 +static void orphans_remove_work(struct work_struct *work);
 +
 +static void schedule_orphans_remove(struct perf_event_context *ctx)
 +{
 +	if (!ctx->task || ctx->orphans_remove_sched || !perf_wq)
 +		return;
 +
 +	if (queue_delayed_work(perf_wq, &ctx->orphans_remove, 1)) {
 +		get_ctx(ctx);
 +		ctx->orphans_remove_sched = true;
 +	}
  }
  
 -static inline int pmu_filter_match(struct perf_event *event)
 +static int __init perf_workqueue_init(void)
  {
 -	struct pmu *pmu = event->pmu;
 -	return pmu->filter_match ? pmu->filter_match(event) : 1;
 +	perf_wq = create_singlethread_workqueue("perf");
 +	WARN(!perf_wq, "failed to create perf workqueue\n");
 +	return perf_wq ? 0 : -1;
  }
  
 +core_initcall(perf_workqueue_init);
 +
  static inline int
  event_filter_match(struct perf_event *event)
  {
@@@ -1631,21 -1731,7 +1635,25 @@@ group_sched_out(struct perf_event *grou
  		cpuctx->exclusive = 0;
  }
  
++<<<<<<< HEAD
 +struct remove_event {
 +	struct perf_event *event;
 +	bool detach_group;
 +};
 +
 +static void ___perf_remove_from_context(void *info)
 +{
 +	struct remove_event *re = info;
 +	struct perf_event *event = re->event;
 +	struct perf_event_context *ctx = event->ctx;
 +
 +	if (re->detach_group)
 +		perf_group_detach(event);
 +	list_del_event(event, ctx);
 +}
++=======
+ #define DETACH_GROUP	0x01UL
++>>>>>>> a69b0ca4ac3b (perf: Fix cloning)
  
  /*
   * Cross CPU call to remove a performance event
@@@ -1653,25 -1739,26 +1661,30 @@@
   * We disable the event on the hardware level first. After that we
   * remove it from the context list.
   */
 -static void
 -__perf_remove_from_context(struct perf_event *event,
 -			   struct perf_cpu_context *cpuctx,
 -			   struct perf_event_context *ctx,
 -			   void *info)
 +static int __perf_remove_from_context(void *info)
  {
 -	unsigned long flags = (unsigned long)info;
 +	struct remove_event *re = info;
 +	struct perf_event *event = re->event;
 +	struct perf_event_context *ctx = event->ctx;
 +	struct perf_cpu_context *cpuctx = __get_cpu_context(ctx);
  
 +	raw_spin_lock(&ctx->lock);
  	event_sched_out(event, cpuctx, ctx);
 -	if (flags & DETACH_GROUP)
 +	if (re->detach_group)
  		perf_group_detach(event);
  	list_del_event(event, ctx);
++<<<<<<< HEAD
 +	if (!ctx->nr_events && cpuctx->task_ctx == ctx) {
++=======
+ 
+ 	if (!ctx->nr_events && ctx->is_active) {
++>>>>>>> a69b0ca4ac3b (perf: Fix cloning)
  		ctx->is_active = 0;
 -		if (ctx->task) {
 -			WARN_ON_ONCE(cpuctx->task_ctx != ctx);
 -			cpuctx->task_ctx = NULL;
 -		}
 +		cpuctx->task_ctx = NULL;
  	}
 +	raw_spin_unlock(&ctx->lock);
 +
 +	return 0;
  }
  
  /*
@@@ -3599,29 -3744,105 +3612,50 @@@ static void put_event(struct perf_even
  	if (!is_kernel_event(event))
  		perf_remove_from_owner(event);
  
 -	ctx = perf_event_ctx_lock(event);
 +	/*
 +	 * There are two ways this annotation is useful:
 +	 *
 +	 *  1) there is a lock recursion from perf_event_exit_task
 +	 *     see the comment there.
 +	 *
 +	 *  2) there is a lock-inversion with mmap_sem through
 +	 *     perf_read_group(), which takes faults while
 +	 *     holding ctx->mutex, however this is called after
 +	 *     the last filedesc died, so there is no possibility
 +	 *     to trigger the AB-BA case.
 +	 */
 +	ctx = perf_event_ctx_lock_nested(event, SINGLE_DEPTH_NESTING);
  	WARN_ON_ONCE(ctx->parent_ctx);
++<<<<<<< HEAD
 +	perf_remove_from_context(event, true);
 +	perf_event_ctx_unlock(event, ctx);
 +
 +	_free_event(event);
 +}
++=======
+ 	perf_remove_from_context(event, DETACH_GROUP);
+ 
+ 	raw_spin_lock_irq(&ctx->lock);
+ 	/*
+ 	 * Mark this even as STATE_DEAD, there is no external reference to it
+ 	 * anymore.
+ 	 *
+ 	 * Anybody acquiring event->child_mutex after the below loop _must_
+ 	 * also see this, most importantly inherit_event() which will avoid
+ 	 * placing more children on the list.
+ 	 *
+ 	 * Thus this guarantees that we will in fact observe and kill _ALL_
+ 	 * child events.
 -	 */
 -	event->state = PERF_EVENT_STATE_DEAD;
 -	raw_spin_unlock_irq(&ctx->lock);
 -
 -	perf_event_ctx_unlock(event, ctx);
 -
 -again:
 -	mutex_lock(&event->child_mutex);
 -	list_for_each_entry(child, &event->child_list, child_list) {
 -
 -		/*
 -		 * Cannot change, child events are not migrated, see the
 -		 * comment with perf_event_ctx_lock_nested().
 -		 */
 -		ctx = lockless_dereference(child->ctx);
 -		/*
 -		 * Since child_mutex nests inside ctx::mutex, we must jump
 -		 * through hoops. We start by grabbing a reference on the ctx.
 -		 *
 -		 * Since the event cannot get freed while we hold the
 -		 * child_mutex, the context must also exist and have a !0
 -		 * reference count.
 -		 */
 -		get_ctx(ctx);
 -
 -		/*
 -		 * Now that we have a ctx ref, we can drop child_mutex, and
 -		 * acquire ctx::mutex without fear of it going away. Then we
 -		 * can re-acquire child_mutex.
 -		 */
 -		mutex_unlock(&event->child_mutex);
 -		mutex_lock(&ctx->mutex);
 -		mutex_lock(&event->child_mutex);
 -
 -		/*
 -		 * Now that we hold ctx::mutex and child_mutex, revalidate our
 -		 * state, if child is still the first entry, it didn't get freed
 -		 * and we can continue doing so.
 -		 */
 -		tmp = list_first_entry_or_null(&event->child_list,
 -					       struct perf_event, child_list);
 -		if (tmp == child) {
 -			perf_remove_from_context(child, DETACH_GROUP);
 -			list_del(&child->child_list);
 -			free_event(child);
 -			/*
 -			 * This matches the refcount bump in inherit_event();
 -			 * this can't be the last reference.
 -			 */
 -			put_event(event);
 -		}
 -
 -		mutex_unlock(&event->child_mutex);
 -		mutex_unlock(&ctx->mutex);
 -		put_ctx(ctx);
 -		goto again;
 -	}
 -	mutex_unlock(&event->child_mutex);
++	 */
++	event->state = PERF_EVENT_STATE_DEAD;
++	raw_spin_unlock_irq(&ctx->lock);
++
++	perf_event_ctx_unlock(event, ctx);
++>>>>>>> a69b0ca4ac3b (perf: Fix cloning)
  
 -no_ctx:
 -	put_event(event); /* Must be the 'last' reference */
 +int perf_event_release_kernel(struct perf_event *event)
 +{
 +	put_event(event);
  	return 0;
  }
  EXPORT_SYMBOL_GPL(perf_event_release_kernel);
@@@ -8114,9 -8723,10 +8148,13 @@@ __perf_event_exit_task(struct perf_even
  	raw_spin_lock_irq(&child_ctx->lock);
  	WARN_ON_ONCE(child_ctx->is_active);
  
 -	if (parent_event)
 +	if (!!child_event->parent)
  		perf_group_detach(child_event);
  	list_del_event(child_event, child_ctx);
++<<<<<<< HEAD
++=======
+ 	child_event->state = PERF_EVENT_STATE_EXIT; /* is_event_hup() */
++>>>>>>> a69b0ca4ac3b (perf: Fix cloning)
  	raw_spin_unlock_irq(&child_ctx->lock);
  
  	/*
* Unmerged path include/linux/perf_event.h
* Unmerged path kernel/events/core.c
