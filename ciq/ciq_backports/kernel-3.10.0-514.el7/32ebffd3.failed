ext4: fix races between buffered IO and collapse / insert range

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jan Kara <jack@suse.com>
commit 32ebffd3bbb4162da5ff88f9a35dd32d0a28ea70
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/32ebffd3.failed

Current code implementing FALLOC_FL_COLLAPSE_RANGE and
FALLOC_FL_INSERT_RANGE is prone to races with buffered writes and page
faults. If buffered write or write via mmap manages to squeeze between
filemap_write_and_wait_range() and truncate_pagecache() in the fallocate
implementations, the written data is simply discarded by
truncate_pagecache() although it should have been shifted.

Fix the problem by moving filemap_write_and_wait_range() call inside
i_mutex and i_mmap_sem. That way we are protected against races with
both buffered writes and page faults.

	Signed-off-by: Jan Kara <jack@suse.com>
	Signed-off-by: Theodore Ts'o <tytso@mit.edu>
(cherry picked from commit 32ebffd3bbb4162da5ff88f9a35dd32d0a28ea70)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/extents.c
diff --cc fs/ext4/extents.c
index d936da6a075b,4b105c96df08..000000000000
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@@ -5413,27 -5487,7 +5413,17 @@@ int ext4_collapse_range(struct inode *i
  			return ret;
  	}
  
- 	/*
- 	 * Need to round down offset to be aligned with page size boundary
- 	 * for page size > block size.
- 	 */
- 	ioffset = round_down(offset, PAGE_SIZE);
- 
- 	/* Write out all dirty pages */
- 	ret = filemap_write_and_wait_range(inode->i_mapping, ioffset,
- 					   LLONG_MAX);
- 	if (ret)
- 		return ret;
- 
- 	/* Take mutex lock */
  	mutex_lock(&inode->i_mutex);
++<<<<<<< HEAD
 +
 +	/* It's not possible punch hole on append only file */
 +	if (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {
 +		ret = -EPERM;
 +		goto out_mutex;
 +	}
 +
++=======
++>>>>>>> 32ebffd3bbb4 (ext4: fix races between buffered IO and collapse / insert range)
  	/*
  	 * There is no need to overlap collapse range with EOF, in which case
  	 * it is effectively a truncate operation
@@@ -5455,6 -5507,34 +5445,37 @@@
  	ext4_inode_block_unlocked_dio(inode);
  	inode_dio_wait(inode);
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Prevent page faults from reinstantiating pages we have released from
+ 	 * page cache.
+ 	 */
+ 	down_write(&EXT4_I(inode)->i_mmap_sem);
+ 	/*
+ 	 * Need to round down offset to be aligned with page size boundary
+ 	 * for page size > block size.
+ 	 */
+ 	ioffset = round_down(offset, PAGE_SIZE);
+ 	/*
+ 	 * Write tail of the last page before removed range since it will get
+ 	 * removed from the page cache below.
+ 	 */
+ 	ret = filemap_write_and_wait_range(inode->i_mapping, ioffset, offset);
+ 	if (ret)
+ 		goto out_mmap;
+ 	/*
+ 	 * Write data that will be shifted to preserve them when discarding
+ 	 * page cache below. We are also protected from pages becoming dirty
+ 	 * by i_mmap_sem.
+ 	 */
+ 	ret = filemap_write_and_wait_range(inode->i_mapping, offset + len,
+ 					   LLONG_MAX);
+ 	if (ret)
+ 		goto out_mmap;
+ 	truncate_pagecache(inode, ioffset);
+ 
++>>>>>>> 32ebffd3bbb4 (ext4: fix races between buffered IO and collapse / insert range)
  	credits = ext4_writepage_trans_blocks(inode);
  	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
  	if (IS_ERR(handle)) {
@@@ -5504,3 -5585,369 +5525,372 @@@ out_mutex
  	mutex_unlock(&inode->i_mutex);
  	return ret;
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * ext4_insert_range:
+  * This function implements the FALLOC_FL_INSERT_RANGE flag of fallocate.
+  * The data blocks starting from @offset to the EOF are shifted by @len
+  * towards right to create a hole in the @inode. Inode size is increased
+  * by len bytes.
+  * Returns 0 on success, error otherwise.
+  */
+ int ext4_insert_range(struct inode *inode, loff_t offset, loff_t len)
+ {
+ 	struct super_block *sb = inode->i_sb;
+ 	handle_t *handle;
+ 	struct ext4_ext_path *path;
+ 	struct ext4_extent *extent;
+ 	ext4_lblk_t offset_lblk, len_lblk, ee_start_lblk = 0;
+ 	unsigned int credits, ee_len;
+ 	int ret = 0, depth, split_flag = 0;
+ 	loff_t ioffset;
+ 
+ 	/*
+ 	 * We need to test this early because xfstests assumes that an
+ 	 * insert range of (0, 1) will return EOPNOTSUPP if the file
+ 	 * system does not support insert range.
+ 	 */
+ 	if (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+ 		return -EOPNOTSUPP;
+ 
+ 	/* Insert range works only on fs block size aligned offsets. */
+ 	if (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||
+ 			len & (EXT4_CLUSTER_SIZE(sb) - 1))
+ 		return -EINVAL;
+ 
+ 	if (!S_ISREG(inode->i_mode))
+ 		return -EOPNOTSUPP;
+ 
+ 	trace_ext4_insert_range(inode, offset, len);
+ 
+ 	offset_lblk = offset >> EXT4_BLOCK_SIZE_BITS(sb);
+ 	len_lblk = len >> EXT4_BLOCK_SIZE_BITS(sb);
+ 
+ 	/* Call ext4_force_commit to flush all data in case of data=journal */
+ 	if (ext4_should_journal_data(inode)) {
+ 		ret = ext4_force_commit(inode->i_sb);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	mutex_lock(&inode->i_mutex);
+ 	/* Currently just for extent based files */
+ 	if (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {
+ 		ret = -EOPNOTSUPP;
+ 		goto out_mutex;
+ 	}
+ 
+ 	/* Check for wrap through zero */
+ 	if (inode->i_size + len > inode->i_sb->s_maxbytes) {
+ 		ret = -EFBIG;
+ 		goto out_mutex;
+ 	}
+ 
+ 	/* Offset should be less than i_size */
+ 	if (offset >= i_size_read(inode)) {
+ 		ret = -EINVAL;
+ 		goto out_mutex;
+ 	}
+ 
+ 	/* Wait for existing dio to complete */
+ 	ext4_inode_block_unlocked_dio(inode);
+ 	inode_dio_wait(inode);
+ 
+ 	/*
+ 	 * Prevent page faults from reinstantiating pages we have released from
+ 	 * page cache.
+ 	 */
+ 	down_write(&EXT4_I(inode)->i_mmap_sem);
+ 	/*
+ 	 * Need to round down to align start offset to page size boundary
+ 	 * for page size > block size.
+ 	 */
+ 	ioffset = round_down(offset, PAGE_SIZE);
+ 	/* Write out all dirty pages */
+ 	ret = filemap_write_and_wait_range(inode->i_mapping, ioffset,
+ 			LLONG_MAX);
+ 	if (ret)
+ 		goto out_mmap;
+ 	truncate_pagecache(inode, ioffset);
+ 
+ 	credits = ext4_writepage_trans_blocks(inode);
+ 	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
+ 	if (IS_ERR(handle)) {
+ 		ret = PTR_ERR(handle);
+ 		goto out_mmap;
+ 	}
+ 
+ 	/* Expand file to avoid data loss if there is error while shifting */
+ 	inode->i_size += len;
+ 	EXT4_I(inode)->i_disksize += len;
+ 	inode->i_mtime = inode->i_ctime = ext4_current_time(inode);
+ 	ret = ext4_mark_inode_dirty(handle, inode);
+ 	if (ret)
+ 		goto out_stop;
+ 
+ 	down_write(&EXT4_I(inode)->i_data_sem);
+ 	ext4_discard_preallocations(inode);
+ 
+ 	path = ext4_find_extent(inode, offset_lblk, NULL, 0);
+ 	if (IS_ERR(path)) {
+ 		up_write(&EXT4_I(inode)->i_data_sem);
+ 		goto out_stop;
+ 	}
+ 
+ 	depth = ext_depth(inode);
+ 	extent = path[depth].p_ext;
+ 	if (extent) {
+ 		ee_start_lblk = le32_to_cpu(extent->ee_block);
+ 		ee_len = ext4_ext_get_actual_len(extent);
+ 
+ 		/*
+ 		 * If offset_lblk is not the starting block of extent, split
+ 		 * the extent @offset_lblk
+ 		 */
+ 		if ((offset_lblk > ee_start_lblk) &&
+ 				(offset_lblk < (ee_start_lblk + ee_len))) {
+ 			if (ext4_ext_is_unwritten(extent))
+ 				split_flag = EXT4_EXT_MARK_UNWRIT1 |
+ 					EXT4_EXT_MARK_UNWRIT2;
+ 			ret = ext4_split_extent_at(handle, inode, &path,
+ 					offset_lblk, split_flag,
+ 					EXT4_EX_NOCACHE |
+ 					EXT4_GET_BLOCKS_PRE_IO |
+ 					EXT4_GET_BLOCKS_METADATA_NOFAIL);
+ 		}
+ 
+ 		ext4_ext_drop_refs(path);
+ 		kfree(path);
+ 		if (ret < 0) {
+ 			up_write(&EXT4_I(inode)->i_data_sem);
+ 			goto out_stop;
+ 		}
+ 	}
+ 
+ 	ret = ext4_es_remove_extent(inode, offset_lblk,
+ 			EXT_MAX_BLOCKS - offset_lblk);
+ 	if (ret) {
+ 		up_write(&EXT4_I(inode)->i_data_sem);
+ 		goto out_stop;
+ 	}
+ 
+ 	/*
+ 	 * if offset_lblk lies in a hole which is at start of file, use
+ 	 * ee_start_lblk to shift extents
+ 	 */
+ 	ret = ext4_ext_shift_extents(inode, handle,
+ 		ee_start_lblk > offset_lblk ? ee_start_lblk : offset_lblk,
+ 		len_lblk, SHIFT_RIGHT);
+ 
+ 	up_write(&EXT4_I(inode)->i_data_sem);
+ 	if (IS_SYNC(inode))
+ 		ext4_handle_sync(handle);
+ 
+ out_stop:
+ 	ext4_journal_stop(handle);
+ out_mmap:
+ 	up_write(&EXT4_I(inode)->i_mmap_sem);
+ 	ext4_inode_resume_unlocked_dio(inode);
+ out_mutex:
+ 	mutex_unlock(&inode->i_mutex);
+ 	return ret;
+ }
+ 
+ /**
+  * ext4_swap_extents - Swap extents between two inodes
+  *
+  * @inode1:	First inode
+  * @inode2:	Second inode
+  * @lblk1:	Start block for first inode
+  * @lblk2:	Start block for second inode
+  * @count:	Number of blocks to swap
+  * @mark_unwritten: Mark second inode's extents as unwritten after swap
+  * @erp:	Pointer to save error value
+  *
+  * This helper routine does exactly what is promise "swap extents". All other
+  * stuff such as page-cache locking consistency, bh mapping consistency or
+  * extent's data copying must be performed by caller.
+  * Locking:
+  * 		i_mutex is held for both inodes
+  * 		i_data_sem is locked for write for both inodes
+  * Assumptions:
+  *		All pages from requested range are locked for both inodes
+  */
+ int
+ ext4_swap_extents(handle_t *handle, struct inode *inode1,
+ 		     struct inode *inode2, ext4_lblk_t lblk1, ext4_lblk_t lblk2,
+ 		  ext4_lblk_t count, int unwritten, int *erp)
+ {
+ 	struct ext4_ext_path *path1 = NULL;
+ 	struct ext4_ext_path *path2 = NULL;
+ 	int replaced_count = 0;
+ 
+ 	BUG_ON(!rwsem_is_locked(&EXT4_I(inode1)->i_data_sem));
+ 	BUG_ON(!rwsem_is_locked(&EXT4_I(inode2)->i_data_sem));
+ 	BUG_ON(!mutex_is_locked(&inode1->i_mutex));
+ 	BUG_ON(!mutex_is_locked(&inode2->i_mutex));
+ 
+ 	*erp = ext4_es_remove_extent(inode1, lblk1, count);
+ 	if (unlikely(*erp))
+ 		return 0;
+ 	*erp = ext4_es_remove_extent(inode2, lblk2, count);
+ 	if (unlikely(*erp))
+ 		return 0;
+ 
+ 	while (count) {
+ 		struct ext4_extent *ex1, *ex2, tmp_ex;
+ 		ext4_lblk_t e1_blk, e2_blk;
+ 		int e1_len, e2_len, len;
+ 		int split = 0;
+ 
+ 		path1 = ext4_find_extent(inode1, lblk1, NULL, EXT4_EX_NOCACHE);
+ 		if (IS_ERR(path1)) {
+ 			*erp = PTR_ERR(path1);
+ 			path1 = NULL;
+ 		finish:
+ 			count = 0;
+ 			goto repeat;
+ 		}
+ 		path2 = ext4_find_extent(inode2, lblk2, NULL, EXT4_EX_NOCACHE);
+ 		if (IS_ERR(path2)) {
+ 			*erp = PTR_ERR(path2);
+ 			path2 = NULL;
+ 			goto finish;
+ 		}
+ 		ex1 = path1[path1->p_depth].p_ext;
+ 		ex2 = path2[path2->p_depth].p_ext;
+ 		/* Do we have somthing to swap ? */
+ 		if (unlikely(!ex2 || !ex1))
+ 			goto finish;
+ 
+ 		e1_blk = le32_to_cpu(ex1->ee_block);
+ 		e2_blk = le32_to_cpu(ex2->ee_block);
+ 		e1_len = ext4_ext_get_actual_len(ex1);
+ 		e2_len = ext4_ext_get_actual_len(ex2);
+ 
+ 		/* Hole handling */
+ 		if (!in_range(lblk1, e1_blk, e1_len) ||
+ 		    !in_range(lblk2, e2_blk, e2_len)) {
+ 			ext4_lblk_t next1, next2;
+ 
+ 			/* if hole after extent, then go to next extent */
+ 			next1 = ext4_ext_next_allocated_block(path1);
+ 			next2 = ext4_ext_next_allocated_block(path2);
+ 			/* If hole before extent, then shift to that extent */
+ 			if (e1_blk > lblk1)
+ 				next1 = e1_blk;
+ 			if (e2_blk > lblk2)
+ 				next2 = e1_blk;
+ 			/* Do we have something to swap */
+ 			if (next1 == EXT_MAX_BLOCKS || next2 == EXT_MAX_BLOCKS)
+ 				goto finish;
+ 			/* Move to the rightest boundary */
+ 			len = next1 - lblk1;
+ 			if (len < next2 - lblk2)
+ 				len = next2 - lblk2;
+ 			if (len > count)
+ 				len = count;
+ 			lblk1 += len;
+ 			lblk2 += len;
+ 			count -= len;
+ 			goto repeat;
+ 		}
+ 
+ 		/* Prepare left boundary */
+ 		if (e1_blk < lblk1) {
+ 			split = 1;
+ 			*erp = ext4_force_split_extent_at(handle, inode1,
+ 						&path1, lblk1, 0);
+ 			if (unlikely(*erp))
+ 				goto finish;
+ 		}
+ 		if (e2_blk < lblk2) {
+ 			split = 1;
+ 			*erp = ext4_force_split_extent_at(handle, inode2,
+ 						&path2,  lblk2, 0);
+ 			if (unlikely(*erp))
+ 				goto finish;
+ 		}
+ 		/* ext4_split_extent_at() may result in leaf extent split,
+ 		 * path must to be revalidated. */
+ 		if (split)
+ 			goto repeat;
+ 
+ 		/* Prepare right boundary */
+ 		len = count;
+ 		if (len > e1_blk + e1_len - lblk1)
+ 			len = e1_blk + e1_len - lblk1;
+ 		if (len > e2_blk + e2_len - lblk2)
+ 			len = e2_blk + e2_len - lblk2;
+ 
+ 		if (len != e1_len) {
+ 			split = 1;
+ 			*erp = ext4_force_split_extent_at(handle, inode1,
+ 						&path1, lblk1 + len, 0);
+ 			if (unlikely(*erp))
+ 				goto finish;
+ 		}
+ 		if (len != e2_len) {
+ 			split = 1;
+ 			*erp = ext4_force_split_extent_at(handle, inode2,
+ 						&path2, lblk2 + len, 0);
+ 			if (*erp)
+ 				goto finish;
+ 		}
+ 		/* ext4_split_extent_at() may result in leaf extent split,
+ 		 * path must to be revalidated. */
+ 		if (split)
+ 			goto repeat;
+ 
+ 		BUG_ON(e2_len != e1_len);
+ 		*erp = ext4_ext_get_access(handle, inode1, path1 + path1->p_depth);
+ 		if (unlikely(*erp))
+ 			goto finish;
+ 		*erp = ext4_ext_get_access(handle, inode2, path2 + path2->p_depth);
+ 		if (unlikely(*erp))
+ 			goto finish;
+ 
+ 		/* Both extents are fully inside boundaries. Swap it now */
+ 		tmp_ex = *ex1;
+ 		ext4_ext_store_pblock(ex1, ext4_ext_pblock(ex2));
+ 		ext4_ext_store_pblock(ex2, ext4_ext_pblock(&tmp_ex));
+ 		ex1->ee_len = cpu_to_le16(e2_len);
+ 		ex2->ee_len = cpu_to_le16(e1_len);
+ 		if (unwritten)
+ 			ext4_ext_mark_unwritten(ex2);
+ 		if (ext4_ext_is_unwritten(&tmp_ex))
+ 			ext4_ext_mark_unwritten(ex1);
+ 
+ 		ext4_ext_try_to_merge(handle, inode2, path2, ex2);
+ 		ext4_ext_try_to_merge(handle, inode1, path1, ex1);
+ 		*erp = ext4_ext_dirty(handle, inode2, path2 +
+ 				      path2->p_depth);
+ 		if (unlikely(*erp))
+ 			goto finish;
+ 		*erp = ext4_ext_dirty(handle, inode1, path1 +
+ 				      path1->p_depth);
+ 		/*
+ 		 * Looks scarry ah..? second inode already points to new blocks,
+ 		 * and it was successfully dirtied. But luckily error may happen
+ 		 * only due to journal error, so full transaction will be
+ 		 * aborted anyway.
+ 		 */
+ 		if (unlikely(*erp))
+ 			goto finish;
+ 		lblk1 += len;
+ 		lblk2 += len;
+ 		replaced_count += len;
+ 		count -= len;
+ 
+ 	repeat:
+ 		ext4_ext_drop_refs(path1);
+ 		kfree(path1);
+ 		ext4_ext_drop_refs(path2);
+ 		kfree(path2);
+ 		path1 = path2 = NULL;
+ 	}
+ 	return replaced_count;
+ }
++>>>>>>> 32ebffd3bbb4 (ext4: fix races between buffered IO and collapse / insert range)
* Unmerged path fs/ext4/extents.c
