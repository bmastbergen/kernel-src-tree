libceph: redo callbacks and factor out MOSDOpReply decoding

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Ilya Dryomov <idryomov@gmail.com>
commit fe5da05e979830b43b115d8a18ead521d507c783
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/fe5da05e.failed

If you specify ACK | ONDISK and set ->r_unsafe_callback, both
->r_callback and ->r_unsafe_callback(true) are called on ack.  This is
very confusing.  Redo this so that only one of them is called:

    ->r_unsafe_callback(true), on ack
    ->r_unsafe_callback(false), on commit

or

    ->r_callback, on ack|commit

Decode everything in decode_MOSDOpReply() to reduce clutter.

	Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
(cherry picked from commit fe5da05e979830b43b115d8a18ead521d507c783)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ceph/addr.c
#	net/ceph/osd_client.c
diff --cc fs/ceph/addr.c
index 5265e35034fa,f47418477629..000000000000
--- a/fs/ceph/addr.c
+++ b/fs/ceph/addr.c
@@@ -1670,3 -1688,208 +1670,211 @@@ int ceph_mmap(struct file *file, struc
  	vma->vm_ops = &ceph_vmops;
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ enum {
+ 	POOL_READ	= 1,
+ 	POOL_WRITE	= 2,
+ };
+ 
+ static int __ceph_pool_perm_get(struct ceph_inode_info *ci, u32 pool)
+ {
+ 	struct ceph_fs_client *fsc = ceph_inode_to_client(&ci->vfs_inode);
+ 	struct ceph_mds_client *mdsc = fsc->mdsc;
+ 	struct ceph_osd_request *rd_req = NULL, *wr_req = NULL;
+ 	struct rb_node **p, *parent;
+ 	struct ceph_pool_perm *perm;
+ 	struct page **pages;
+ 	int err = 0, err2 = 0, have = 0;
+ 
+ 	down_read(&mdsc->pool_perm_rwsem);
+ 	p = &mdsc->pool_perm_tree.rb_node;
+ 	while (*p) {
+ 		perm = rb_entry(*p, struct ceph_pool_perm, node);
+ 		if (pool < perm->pool)
+ 			p = &(*p)->rb_left;
+ 		else if (pool > perm->pool)
+ 			p = &(*p)->rb_right;
+ 		else {
+ 			have = perm->perm;
+ 			break;
+ 		}
+ 	}
+ 	up_read(&mdsc->pool_perm_rwsem);
+ 	if (*p)
+ 		goto out;
+ 
+ 	dout("__ceph_pool_perm_get pool %u no perm cached\n", pool);
+ 
+ 	down_write(&mdsc->pool_perm_rwsem);
+ 	parent = NULL;
+ 	while (*p) {
+ 		parent = *p;
+ 		perm = rb_entry(parent, struct ceph_pool_perm, node);
+ 		if (pool < perm->pool)
+ 			p = &(*p)->rb_left;
+ 		else if (pool > perm->pool)
+ 			p = &(*p)->rb_right;
+ 		else {
+ 			have = perm->perm;
+ 			break;
+ 		}
+ 	}
+ 	if (*p) {
+ 		up_write(&mdsc->pool_perm_rwsem);
+ 		goto out;
+ 	}
+ 
+ 	rd_req = ceph_osdc_alloc_request(&fsc->client->osdc, NULL,
+ 					 1, false, GFP_NOFS);
+ 	if (!rd_req) {
+ 		err = -ENOMEM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	rd_req->r_flags = CEPH_OSD_FLAG_READ;
+ 	osd_req_op_init(rd_req, 0, CEPH_OSD_OP_STAT, 0);
+ 	rd_req->r_base_oloc.pool = pool;
+ 	ceph_oid_printf(&rd_req->r_base_oid, "%llx.00000000", ci->i_vino.ino);
+ 
+ 	err = ceph_osdc_alloc_messages(rd_req, GFP_NOFS);
+ 	if (err)
+ 		goto out_unlock;
+ 
+ 	wr_req = ceph_osdc_alloc_request(&fsc->client->osdc, NULL,
+ 					 1, false, GFP_NOFS);
+ 	if (!wr_req) {
+ 		err = -ENOMEM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	wr_req->r_flags = CEPH_OSD_FLAG_WRITE | CEPH_OSD_FLAG_ACK;
+ 	osd_req_op_init(wr_req, 0, CEPH_OSD_OP_CREATE, CEPH_OSD_OP_FLAG_EXCL);
+ 	ceph_oloc_copy(&wr_req->r_base_oloc, &rd_req->r_base_oloc);
+ 	ceph_oid_copy(&wr_req->r_base_oid, &rd_req->r_base_oid);
+ 
+ 	err = ceph_osdc_alloc_messages(wr_req, GFP_NOFS);
+ 	if (err)
+ 		goto out_unlock;
+ 
+ 	/* one page should be large enough for STAT data */
+ 	pages = ceph_alloc_page_vector(1, GFP_KERNEL);
+ 	if (IS_ERR(pages)) {
+ 		err = PTR_ERR(pages);
+ 		goto out_unlock;
+ 	}
+ 
+ 	osd_req_op_raw_data_in_pages(rd_req, 0, pages, PAGE_SIZE,
+ 				     0, false, true);
+ 	err = ceph_osdc_start_request(&fsc->client->osdc, rd_req, false);
+ 
+ 	wr_req->r_mtime = ci->vfs_inode.i_mtime;
+ 	err2 = ceph_osdc_start_request(&fsc->client->osdc, wr_req, false);
+ 
+ 	if (!err)
+ 		err = ceph_osdc_wait_request(&fsc->client->osdc, rd_req);
+ 	if (!err2)
+ 		err2 = ceph_osdc_wait_request(&fsc->client->osdc, wr_req);
+ 
+ 	if (err >= 0 || err == -ENOENT)
+ 		have |= POOL_READ;
+ 	else if (err != -EPERM)
+ 		goto out_unlock;
+ 
+ 	if (err2 == 0 || err2 == -EEXIST)
+ 		have |= POOL_WRITE;
+ 	else if (err2 != -EPERM) {
+ 		err = err2;
+ 		goto out_unlock;
+ 	}
+ 
+ 	perm = kmalloc(sizeof(*perm), GFP_NOFS);
+ 	if (!perm) {
+ 		err = -ENOMEM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	perm->pool = pool;
+ 	perm->perm = have;
+ 	rb_link_node(&perm->node, parent, p);
+ 	rb_insert_color(&perm->node, &mdsc->pool_perm_tree);
+ 	err = 0;
+ out_unlock:
+ 	up_write(&mdsc->pool_perm_rwsem);
+ 
+ 	ceph_osdc_put_request(rd_req);
+ 	ceph_osdc_put_request(wr_req);
+ out:
+ 	if (!err)
+ 		err = have;
+ 	dout("__ceph_pool_perm_get pool %u result = %d\n", pool, err);
+ 	return err;
+ }
+ 
+ int ceph_pool_perm_check(struct ceph_inode_info *ci, int need)
+ {
+ 	u32 pool;
+ 	int ret, flags;
+ 
+ 	/* does not support pool namespace yet */
+ 	if (ci->i_pool_ns_len)
+ 		return -EIO;
+ 
+ 	if (ceph_test_mount_opt(ceph_inode_to_client(&ci->vfs_inode),
+ 				NOPOOLPERM))
+ 		return 0;
+ 
+ 	spin_lock(&ci->i_ceph_lock);
+ 	flags = ci->i_ceph_flags;
+ 	pool = ceph_file_layout_pg_pool(ci->i_layout);
+ 	spin_unlock(&ci->i_ceph_lock);
+ check:
+ 	if (flags & CEPH_I_POOL_PERM) {
+ 		if ((need & CEPH_CAP_FILE_RD) && !(flags & CEPH_I_POOL_RD)) {
+ 			dout("ceph_pool_perm_check pool %u no read perm\n",
+ 			     pool);
+ 			return -EPERM;
+ 		}
+ 		if ((need & CEPH_CAP_FILE_WR) && !(flags & CEPH_I_POOL_WR)) {
+ 			dout("ceph_pool_perm_check pool %u no write perm\n",
+ 			     pool);
+ 			return -EPERM;
+ 		}
+ 		return 0;
+ 	}
+ 
+ 	ret = __ceph_pool_perm_get(ci, pool);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	flags = CEPH_I_POOL_PERM;
+ 	if (ret & POOL_READ)
+ 		flags |= CEPH_I_POOL_RD;
+ 	if (ret & POOL_WRITE)
+ 		flags |= CEPH_I_POOL_WR;
+ 
+ 	spin_lock(&ci->i_ceph_lock);
+ 	if (pool == ceph_file_layout_pg_pool(ci->i_layout)) {
+ 		ci->i_ceph_flags = flags;
+         } else {
+ 		pool = ceph_file_layout_pg_pool(ci->i_layout);
+ 		flags = ci->i_ceph_flags;
+ 	}
+ 	spin_unlock(&ci->i_ceph_lock);
+ 	goto check;
+ }
+ 
+ void ceph_pool_perm_destroy(struct ceph_mds_client *mdsc)
+ {
+ 	struct ceph_pool_perm *perm;
+ 	struct rb_node *n;
+ 
+ 	while (!RB_EMPTY_ROOT(&mdsc->pool_perm_tree)) {
+ 		n = rb_first(&mdsc->pool_perm_tree);
+ 		perm = rb_entry(n, struct ceph_pool_perm, node);
+ 		rb_erase(n, &mdsc->pool_perm_tree);
+ 		kfree(perm);
+ 	}
+ }
++>>>>>>> fe5da05e9798 (libceph: redo callbacks and factor out MOSDOpReply decoding)
diff --cc net/ceph/osd_client.c
index 4e649b707367,baf2844b00d6..000000000000
--- a/net/ceph/osd_client.c
+++ b/net/ceph/osd_client.c
@@@ -1802,19 -1960,96 +1779,105 @@@ static int decode_MOSDOpReply(const str
  	}
  
  	if (decode_redir) {
- 		err = ceph_redirect_decode(&p, end, &redir);
- 		if (err)
- 			goto bad_put;
+ 		ret = ceph_redirect_decode(&p, end, &m->redirect);
+ 		if (ret)
+ 			return ret;
  	} else {
- 		redir.oloc.pool = -1;
+ 		ceph_oloc_init(&m->redirect.oloc);
  	}
  
++<<<<<<< HEAD
 +	if (redir.oloc.pool != -1) {
 +		dout("redirect pool %lld\n", redir.oloc.pool);
++=======
+ 	return 0;
++>>>>>>> fe5da05e9798 (libceph: redo callbacks and factor out MOSDOpReply decoding)
+ 
+ e_inval:
+ 	return -EINVAL;
+ }
  
+ /*
+  * We are done with @req if
+  *   - @m is a safe reply, or
+  *   - @m is an unsafe reply and we didn't want a safe one
+  */
+ static bool done_request(const struct ceph_osd_request *req,
+ 			 const struct MOSDOpReply *m)
+ {
+ 	return (m->result < 0 ||
+ 		(m->flags & CEPH_OSD_FLAG_ONDISK) ||
+ 		!(req->r_flags & CEPH_OSD_FLAG_ONDISK));
+ }
+ 
+ /*
+  * handle osd op reply.  either call the callback if it is specified,
+  * or do the completion to wake up the waiting thread.
+  *
+  * ->r_unsafe_callback is set?	yes			no
+  *
+  * first reply is OK (needed	r_cb/r_completion,	r_cb/r_completion,
+  * any or needed/got safe)	r_safe_completion	r_safe_completion
+  *
+  * first reply is unsafe	r_unsafe_cb(true)	(nothing)
+  *
+  * when we get the safe reply	r_unsafe_cb(false),	r_cb/r_completion,
+  *				r_safe_completion	r_safe_completion
+  */
+ static void handle_reply(struct ceph_osd_client *osdc, struct ceph_msg *msg)
+ {
+ 	struct ceph_osd_request *req;
+ 	struct MOSDOpReply m;
+ 	u64 tid = le64_to_cpu(msg->hdr.tid);
+ 	u32 data_len = 0;
+ 	bool already_acked;
+ 	int ret;
+ 	int i;
+ 
+ 	dout("%s msg %p tid %llu\n", __func__, msg, tid);
+ 
+ 	down_read(&osdc->map_sem);
+ 	mutex_lock(&osdc->request_mutex);
+ 	req = lookup_request(&osdc->requests, tid);
+ 	if (!req) {
+ 		dout("%s no tid %llu\n", __func__, tid);
+ 		goto out_unlock;
+ 	}
+ 	ceph_osdc_get_request(req);
+ 
+ 	ret = decode_MOSDOpReply(msg, &m);
+ 	if (ret) {
+ 		pr_err("failed to decode MOSDOpReply for tid %llu: %d\n",
+ 		       req->r_tid, ret);
+ 		ceph_msg_dump(msg);
+ 		goto fail_request;
+ 	}
+ 	dout("%s req %p tid %llu flags 0x%llx pgid %llu.%x epoch %u attempt %d v %u'%llu uv %llu\n",
+ 	     __func__, req, req->r_tid, m.flags, m.pgid.pool, m.pgid.seed,
+ 	     m.epoch, m.retry_attempt, le32_to_cpu(m.replay_version.epoch),
+ 	     le64_to_cpu(m.replay_version.version), m.user_version);
+ 
+ 	if (m.retry_attempt >= 0) {
+ 		if (m.retry_attempt != req->r_attempts - 1) {
+ 			dout("req %p tid %llu retry_attempt %d != %d, ignoring\n",
+ 			     req, req->r_tid, m.retry_attempt,
+ 			     req->r_attempts - 1);
+ 			goto out_put;
+ 		}
+ 	} else {
+ 		WARN_ON(1); /* MOSDOpReply v4 is assumed */
+ 	}
+ 
+ 	if (!ceph_oloc_empty(&m.redirect.oloc)) {
+ 		dout("req %p tid %llu redirect pool %lld\n", req, req->r_tid,
+ 		     m.redirect.oloc.pool);
  		__unregister_request(osdc, req);
  
++<<<<<<< HEAD
 +		req->r_target_oloc = redir.oloc; /* struct */
++=======
+ 		ceph_oloc_copy(&req->r_t.target_oloc, &m.redirect.oloc);
++>>>>>>> fe5da05e9798 (libceph: redo callbacks and factor out MOSDOpReply decoding)
  
  		/*
  		 * Start redirect requests with nofail=true.  If
@@@ -1824,85 -2059,85 +1887,132 @@@
  		 * successfully.  In the future we might want to follow
  		 * original request's nofail setting here.
  		 */
- 		err = __ceph_osdc_start_request(osdc, req, true);
- 		BUG_ON(err);
+ 		ret = __ceph_osdc_start_request(osdc, req, true);
+ 		BUG_ON(ret);
  
- 		goto out_unlock;
+ 		goto out_put;
  	}
  
- 	already_completed = req->r_got_reply;
- 	if (!req->r_got_reply) {
- 		req->r_result = result;
- 		dout("handle_reply result %d bytes %d\n", req->r_result,
- 		     bytes);
- 		if (req->r_result == 0)
- 			req->r_result = bytes;
+ 	if (m.num_ops != req->r_num_ops) {
+ 		pr_err("num_ops %d != %d for tid %llu\n", m.num_ops,
+ 		       req->r_num_ops, req->r_tid);
+ 		goto fail_request;
+ 	}
+ 	for (i = 0; i < req->r_num_ops; i++) {
+ 		dout(" req %p tid %llu op %d rval %d len %u\n", req,
+ 		     req->r_tid, i, m.rval[i], m.outdata_len[i]);
+ 		req->r_ops[i].rval = m.rval[i];
+ 		req->r_ops[i].outdata_len = m.outdata_len[i];
+ 		data_len += m.outdata_len[i];
+ 	}
+ 	if (data_len != le32_to_cpu(msg->hdr.data_len)) {
+ 		pr_err("sum of lens %u != %u for tid %llu\n", data_len,
+ 		       le32_to_cpu(msg->hdr.data_len), req->r_tid);
+ 		goto fail_request;
+ 	}
+ 	dout("%s req %p tid %llu acked %d result %d data_len %u\n", __func__,
+ 	     req, req->r_tid, req->r_got_reply, m.result, data_len);
  
++<<<<<<< HEAD
 +		/* in case this is a write and we need to replay, */
 +		req->r_reassert_version.epoch = cpu_to_le32(reassert_epoch);
 +		req->r_reassert_version.version = cpu_to_le64(reassert_version);
 +
 +		req->r_got_reply = 1;
 +	} else if ((flags & CEPH_OSD_FLAG_ONDISK) == 0) {
 +		dout("handle_reply tid %llu dup ack\n", tid);
 +		goto out_unlock;
- 	}
- 
- 	dout("handle_reply tid %llu flags %d\n", tid, flags);
- 
- 	if (req->r_linger && (flags & CEPH_OSD_FLAG_ONDISK))
- 		__register_linger_request(osdc, req);
- 
- 	/* either this is a read, or we got the safe response */
- 	if (result < 0 ||
- 	    (flags & CEPH_OSD_FLAG_ONDISK) ||
- 	    ((flags & CEPH_OSD_FLAG_WRITE) == 0))
++=======
+ 	already_acked = req->r_got_reply;
+ 	if (!already_acked) {
+ 		req->r_result = m.result ?: data_len;
+ 		req->r_replay_version = m.replay_version; /* struct */
+ 		req->r_got_reply = true;
+ 	} else if (!(m.flags & CEPH_OSD_FLAG_ONDISK)) {
+ 		dout("req %p tid %llu dup ack\n", req, req->r_tid);
+ 		goto out_put;
++>>>>>>> fe5da05e9798 (libceph: redo callbacks and factor out MOSDOpReply decoding)
+ 	}
+ 
+ 	if (done_request(req, &m)) {
  		__unregister_request(osdc, req);
+ 		if (req->r_linger) {
+ 			WARN_ON(req->r_unsafe_callback);
+ 			__register_linger_request(osdc, req);
+ 		}
+ 	}
  
  	mutex_unlock(&osdc->request_mutex);
  	up_read(&osdc->map_sem);
  
++<<<<<<< HEAD
 +	if (!already_completed) {
 +		if (req->r_unsafe_callback &&
 +		    result >= 0 && !(flags & CEPH_OSD_FLAG_ONDISK))
 +			req->r_unsafe_callback(req, true);
 +		if (req->r_callback)
 +			req->r_callback(req, msg);
 +		else
 +			complete_all(&req->r_completion);
 +	}
 +
 +	if (flags & CEPH_OSD_FLAG_ONDISK) {
 +		if (req->r_unsafe_callback && already_completed)
++=======
+ 	if (done_request(req, &m)) {
+ 		if (already_acked && req->r_unsafe_callback) {
+ 			dout("req %p tid %llu safe-cb\n", req, req->r_tid);
++>>>>>>> fe5da05e9798 (libceph: redo callbacks and factor out MOSDOpReply decoding)
  			req->r_unsafe_callback(req, false);
- 		complete_request(req);
+ 		} else {
+ 			dout("req %p tid %llu cb\n", req, req->r_tid);
+ 			__complete_request(req);
+ 		}
+ 	} else {
+ 		if (req->r_unsafe_callback) {
+ 			dout("req %p tid %llu unsafe-cb\n", req, req->r_tid);
+ 			req->r_unsafe_callback(req, true);
+ 		} else {
+ 			WARN_ON(1);
+ 		}
  	}
+ 	if (m.flags & CEPH_OSD_FLAG_ONDISK)
+ 		complete_all(&req->r_safe_completion);
  
- out:
- 	dout("req=%p req->r_linger=%d\n", req, req->r_linger);
  	ceph_osdc_put_request(req);
  	return;
+ 
+ fail_request:
+ 	req->r_result = -EIO;
+ 	__unregister_request(osdc, req);
+ 	__complete_request(req);
+ 	complete_all(&req->r_safe_completion);
+ out_put:
+ 	ceph_osdc_put_request(req);
  out_unlock:
  	mutex_unlock(&osdc->request_mutex);
  	up_read(&osdc->map_sem);
++<<<<<<< HEAD
 +	goto out;
 +
 +bad_put:
 +	req->r_result = -EIO;
 +	__unregister_request(osdc, req);
 +	if (req->r_callback)
 +		req->r_callback(req, msg);
 +	else
 +		complete_all(&req->r_completion);
 +	complete_request(req);
 +	ceph_osdc_put_request(req);
 +bad_mutex:
 +	mutex_unlock(&osdc->request_mutex);
 +	up_read(&osdc->map_sem);
 +bad:
 +	pr_err("corrupt osd_op_reply got %d %d\n",
 +	       (int)msg->front.iov_len, le32_to_cpu(msg->hdr.front_len));
 +	ceph_msg_dump(msg);
++=======
++>>>>>>> fe5da05e9798 (libceph: redo callbacks and factor out MOSDOpReply decoding)
  }
  
  static void reset_changed_osds(struct ceph_osd_client *osdc)
* Unmerged path fs/ceph/addr.c
diff --git a/fs/ceph/file.c b/fs/ceph/file.c
index 0c3070bb755c..095679a226b5 100644
--- a/fs/ceph/file.c
+++ b/fs/ceph/file.c
@@ -515,6 +515,8 @@ static void ceph_sync_write_unsafe(struct ceph_osd_request *req, bool unsafe)
 		list_add_tail(&req->r_unsafe_item,
 			      &ci->i_unsafe_writes);
 		spin_unlock(&ci->i_unsafe_lock);
+
+		complete_all(&req->r_completion);
 	} else {
 		spin_lock(&ci->i_unsafe_lock);
 		list_del_init(&req->r_unsafe_item);
diff --git a/include/linux/ceph/osd_client.h b/include/linux/ceph/osd_client.h
index cc16ab3e4c14..956eab932c58 100644
--- a/include/linux/ceph/osd_client.h
+++ b/include/linux/ceph/osd_client.h
@@ -148,13 +148,14 @@ struct ceph_osd_request {
 	struct ceph_eversion *r_request_reassert_version;
 
 	int               r_result;
-	int               r_got_reply;
+	bool              r_got_reply;
 	int		  r_linger;
 
 	struct ceph_osd_client *r_osdc;
 	struct kref       r_kref;
 	bool              r_mempool;
-	struct completion r_completion, r_safe_completion;
+	struct completion r_completion;
+	struct completion r_safe_completion;  /* fsync waiter */
 	ceph_osdc_callback_t r_callback;
 	ceph_osdc_unsafe_callback_t r_unsafe_callback;
 	struct ceph_eversion r_reassert_version;
* Unmerged path net/ceph/osd_client.c
