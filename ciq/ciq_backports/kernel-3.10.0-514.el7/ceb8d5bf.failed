net: Rearrange loop in net_rx_action

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [net] Rearrange loop in net_rx_action (Ivan Vecera) [1334372]
Rebuild_FUZZ: 92.54%
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit ceb8d5bf17d366534f32d2f60f41d905a5bc864b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ceb8d5bf.failed

This patch rearranges the loop in net_rx_action to reduce the
amount of jumping back and forth when reading the code.

	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ceb8d5bf17d366534f32d2f60f41d905a5bc864b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/dev.c
diff --cc net/core/dev.c
index ce5758e98847,c97ae6fec040..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -4470,91 -4557,112 +4470,127 @@@ void netif_napi_del(struct napi_struct 
  }
  EXPORT_SYMBOL(netif_napi_del);
  
 -static int napi_poll(struct napi_struct *n, struct list_head *repoll)
 +static void net_rx_action(struct softirq_action *h)
  {
 +	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
 +	unsigned long time_limit = jiffies + 2;
 +	int budget = netdev_budget;
  	void *have;
 -	int work, weight;
  
 -	list_del_init(&n->poll_list);
 +	local_irq_disable();
  
 -	have = netpoll_poll_lock(n);
++<<<<<<< HEAD
 +	while (!list_empty(&sd->poll_list)) {
++=======
++	for (;;) {
++>>>>>>> ceb8d5bf17d3 (net: Rearrange loop in net_rx_action)
 +		struct napi_struct *n;
 +		int work, weight;
  
 -	weight = n->weight;
++<<<<<<< HEAD
 +		/* If softirq window is exhuasted then punt.
 +		 * Allow this to run for 2 jiffies since which will allow
 +		 * an average latency of 1.5/HZ.
 +		 */
 +		if (unlikely(budget <= 0 || time_after_eq(jiffies, time_limit)))
 +			goto softnet_break;
  
 -	/* This NAPI_STATE_SCHED test is for avoiding a race
 -	 * with netpoll's poll_napi().  Only the entity which
 -	 * obtains the lock and sees NAPI_STATE_SCHED set will
 -	 * actually make the ->poll() call.  Therefore we avoid
 -	 * accidentally calling ->poll() when NAPI is not scheduled.
 -	 */
 -	work = 0;
 -	if (test_bit(NAPI_STATE_SCHED, &n->state)) {
 -		work = n->poll(n, weight);
 -		trace_napi_poll(n);
 -	}
 +		local_irq_enable();
  
 -	WARN_ON_ONCE(work > weight);
 +		/* Even though interrupts have been re-enabled, this
 +		 * access is safe because interrupts can only add new
 +		 * entries to the tail of this list, and only ->poll()
 +		 * calls can remove this head entry from the list.
 +		 */
 +		n = list_first_entry(&sd->poll_list, struct napi_struct, poll_list);
  
 -	if (likely(work < weight))
 -		goto out_unlock;
 +		have = netpoll_poll_lock(n);
  
 -	/* Drivers must not modify the NAPI state if they
 -	 * consume the entire weight.  In such cases this code
 -	 * still "owns" the NAPI instance and therefore can
 -	 * move the instance around on the list at-will.
 -	 */
 -	if (unlikely(napi_disable_pending(n))) {
 -		napi_complete(n);
 -		goto out_unlock;
 -	}
 +		weight = n->weight;
  
 -	if (n->gro_list) {
 -		/* flush too old packets
 -		 * If HZ < 1000, flush all packets.
 +		/* This NAPI_STATE_SCHED test is for avoiding a race
 +		 * with netpoll's poll_napi().  Only the entity which
 +		 * obtains the lock and sees NAPI_STATE_SCHED set will
 +		 * actually make the ->poll() call.  Therefore we avoid
 +		 * accidentally calling ->poll() when NAPI is not scheduled.
  		 */
 -		napi_gro_flush(n, HZ >= 1000);
 -	}
 -
 -	/* Some drivers may have called napi_schedule
 -	 * prior to exhausting their budget.
 -	 */
 -	if (unlikely(!list_empty(&n->poll_list))) {
 -		pr_warn_once("%s: Budget exhausted after napi rescheduled\n",
 -			     n->dev ? n->dev->name : "backlog");
 -		goto out_unlock;
 -	}
 +		work = 0;
 +		if (test_bit(NAPI_STATE_SCHED, &n->state)) {
 +			work = n->poll(n, weight);
 +			trace_napi_poll(n);
 +		}
  
 -	list_add_tail(&n->poll_list, repoll);
 +		WARN_ON_ONCE(work > weight);
  
 -out_unlock:
 -	netpoll_poll_unlock(have);
 +		budget -= work;
  
 -	return work;
 -}
 +		local_irq_disable();
  
 -static void net_rx_action(struct softirq_action *h)
 -{
 -	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
 -	unsigned long time_limit = jiffies + 2;
 -	int budget = netdev_budget;
 -	LIST_HEAD(list);
 -	LIST_HEAD(repoll);
 +		/* Drivers must not modify the NAPI state if they
 +		 * consume the entire weight.  In such cases this code
 +		 * still "owns" the NAPI instance and therefore can
 +		 * move the instance around on the list at-will.
 +		 */
 +		if (unlikely(work == weight)) {
 +			if (unlikely(napi_disable_pending(n))) {
 +				local_irq_enable();
 +				napi_complete(n);
 +				local_irq_disable();
 +			} else {
 +				if (n->gro_list) {
 +					/* flush too old packets
 +					 * If HZ < 1000, flush all packets.
 +					 */
 +					local_irq_enable();
 +					napi_gro_flush(n, HZ >= 1000);
 +					local_irq_disable();
 +				}
 +				list_move_tail(&n->poll_list, &sd->poll_list);
 +			}
 +		}
  
 -	local_irq_disable();
 -	list_splice_init(&sd->poll_list, &list);
 -	local_irq_enable();
 +		netpoll_poll_unlock(have);
 +	}
 +out:
 +	net_rps_action_and_irq_enable(sd);
  
 -	for (;;) {
 -		struct napi_struct *n;
 +	return;
  
 +softnet_break:
 +	sd->time_squeeze++;
 +	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
 +	goto out;
++=======
+ 		if (list_empty(&list)) {
+ 			if (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))
+ 				return;
+ 			break;
+ 		}
+ 
+ 		n = list_first_entry(&list, struct napi_struct, poll_list);
+ 		budget -= napi_poll(n, &repoll);
+ 
+ 		/* If softirq window is exhausted then punt.
+ 		 * Allow this to run for 2 jiffies since which will allow
+ 		 * an average latency of 1.5/HZ.
+ 		 */
+ 		if (unlikely(budget <= 0 ||
+ 			     time_after_eq(jiffies, time_limit))) {
+ 			sd->time_squeeze++;
+ 			break;
+ 		}
+ 	}
+ 
+ 	local_irq_disable();
+ 
+ 	list_splice_tail_init(&sd->poll_list, &list);
+ 	list_splice_tail(&repoll, &list);
+ 	list_splice(&list, &sd->poll_list);
+ 	if (!list_empty(&sd->poll_list))
+ 		__raise_softirq_irqoff(NET_RX_SOFTIRQ);
+ 
+ 	net_rps_action_and_irq_enable(sd);
++>>>>>>> ceb8d5bf17d3 (net: Rearrange loop in net_rx_action)
  }
  
  struct netdev_adjacent {
* Unmerged path net/core/dev.c
