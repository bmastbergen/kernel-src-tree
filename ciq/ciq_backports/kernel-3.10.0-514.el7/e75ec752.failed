nvme: store a struct device pointer in struct nvme_dev

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Christoph Hellwig <hch@lst.de>
commit e75ec752d725b7b612c0b2db1bca50a9e53c0879
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e75ec752.failed

Most users want the generic device, so store that in struct nvme_dev
instead of the pci_dev.  This also happens to be a nice step towards
making some code reusable for non-PCI transports.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit e75ec752d725b7b612c0b2db1bca50a9e53c0879)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index a8d9055d2103,870a926e1ddc..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -534,11 -615,14 +534,18 @@@ static void req_completion(struct nvme_
  			status);
  
  	if (iod->nents) {
- 		dma_unmap_sg(&nvmeq->dev->pci_dev->dev, iod->sg, iod->nents,
+ 		dma_unmap_sg(nvmeq->dev->dev, iod->sg, iod->nents,
  			rq_data_dir(req) ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
++<<<<<<< HEAD
 +		if (blk_integrity_rq(req))
 +			dma_unmap_sg(&nvmeq->dev->pci_dev->dev, iod->meta_sg, 1,
++=======
+ 		if (blk_integrity_rq(req)) {
+ 			if (!rq_data_dir(req))
+ 				nvme_dif_remap(req, nvme_dif_complete);
+ 			dma_unmap_sg(nvmeq->dev->dev, iod->meta_sg, 1,
++>>>>>>> e75ec752d725 (nvme: store a struct device pointer in struct nvme_dev)
  				rq_data_dir(req) ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 -		}
  	}
  	nvme_free_iod(nvmeq->dev, iod);
  
@@@ -1094,10 -1192,9 +1101,9 @@@ static void nvme_abort_req(struct reque
  		if (work_busy(&dev->reset_work))
  			goto out;
  		list_del_init(&dev->node);
- 		dev_warn(&dev->pci_dev->dev,
- 			"I/O %d QID %d timeout, reset controller\n",
+ 		dev_warn(dev->dev, "I/O %d QID %d timeout, reset controller\n",
  							req->tag, nvmeq->qid);
 -		dev->reset_workfn = nvme_reset_failed_dev;
 +		PREPARE_WORK(&dev->reset_work, nvme_reset_failed_dev);
  		queue_work(nvme_workq, &dev->reset_work);
   out:
  		spin_unlock_irqrestore(&dev_list_lock, flags);
@@@ -1266,13 -1365,12 +1271,18 @@@ static struct nvme_queue *nvme_alloc_qu
  	if (!nvmeq)
  		return NULL;
  
++<<<<<<< HEAD
 +	nvmeq->cqes = dma_alloc_coherent(dmadev, CQ_SIZE(depth),
 +					&nvmeq->cq_dma_addr, GFP_KERNEL);
++=======
+ 	nvmeq->cqes = dma_zalloc_coherent(dev->dev, CQ_SIZE(depth),
+ 					  &nvmeq->cq_dma_addr, GFP_KERNEL);
++>>>>>>> e75ec752d725 (nvme: store a struct device pointer in struct nvme_dev)
  	if (!nvmeq->cqes)
  		goto free_nvmeq;
 +	memset((void *)nvmeq->cqes, 0, CQ_SIZE(depth));
  
- 	nvmeq->sq_cmds = dma_alloc_coherent(dmadev, SQ_SIZE(depth),
+ 	nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(depth),
  					&nvmeq->sq_dma_addr, GFP_KERNEL);
  	if (!nvmeq->sq_cmds)
  		goto free_cqdma;
@@@ -1660,8 -1760,8 +1670,13 @@@ static int nvme_submit_io(struct nvme_n
  		goto unmap;
  	}
  	if (meta_len) {
++<<<<<<< HEAD
 +		meta = dma_alloc_coherent(&dev->pci_dev->dev, meta_len,
 +					  &meta_dma, GFP_KERNEL);
++=======
+ 		meta = dma_alloc_coherent(dev->dev, meta_len,
+ 						&meta_dma, GFP_KERNEL);
++>>>>>>> e75ec752d725 (nvme: store a struct device pointer in struct nvme_dev)
  		if (!meta) {
  			status = -ENOMEM;
  			goto unmap;
@@@ -1697,10 -1796,10 +1712,10 @@@
  	if (meta) {
  		if (status == NVME_SC_SUCCESS && !write) {
  			if (copy_to_user((void __user *)io.metadata, meta,
 -								meta_len))
 +					 meta_len))
  				status = -EFAULT;
  		}
- 		dma_free_coherent(&dev->pci_dev->dev, meta_len, meta, meta_dma);
+ 		dma_free_coherent(dev->dev, meta_len, meta, meta_dma);
  	}
  	return status;
  }
@@@ -1860,16 -1959,13 +1875,14 @@@ static int nvme_revalidate_disk(struct 
  	u16 old_ms;
  	unsigned short bs;
  
- 	id = dma_alloc_coherent(&dev->pci_dev->dev, 4096, &dma_addr,
- 								GFP_KERNEL);
+ 	id = dma_alloc_coherent(dev->dev, 4096, &dma_addr, GFP_KERNEL);
  	if (!id) {
- 		dev_warn(&dev->pci_dev->dev, "%s: Memory alocation failure\n",
- 								__func__);
+ 		dev_warn(dev->dev, "%s: Memory alocation failure\n", __func__);
  		return 0;
  	}
 +
  	if (nvme_identify(dev, ns->ns_id, 0, dma_addr)) {
- 		dev_warn(&dev->pci_dev->dev,
+ 		dev_warn(dev->dev,
  			"identify failed ns:%d, setting capacity to 0\n",
  			ns->ns_id);
  		memset(id, 0, sizeof(*id));
@@@ -1943,11 -2037,10 +1956,11 @@@ static int nvme_kthread(void *data
  				if (work_busy(&dev->reset_work))
  					continue;
  				list_del_init(&dev->node);
- 				dev_warn(&dev->pci_dev->dev,
+ 				dev_warn(dev->dev,
  					"Failed status: %x, reset controller\n",
  					readl(&dev->bar->csts));
 -				dev->reset_workfn = nvme_reset_failed_dev;
 +				PREPARE_WORK(&dev->reset_work,
 +							nvme_reset_failed_dev);
  				queue_work(nvme_workq, &dev->reset_work);
  				continue;
  			}
@@@ -2276,9 -2246,9 +2288,9 @@@ static void nvme_dev_scan(struct work_s
   */
  static int nvme_dev_add(struct nvme_dev *dev)
  {
- 	struct pci_dev *pdev = dev->pci_dev;
+ 	struct pci_dev *pdev = to_pci_dev(dev->dev);
  	int res;
 -	unsigned nn, i;
 +	unsigned nn;
  	struct nvme_id_ctrl *ctrl;
  	void *mem;
  	dma_addr_t dma_addr;
@@@ -2317,23 -2287,24 +2329,42 @@@
  		} else
  			dev->max_hw_sectors = max_hw_sectors;
  	}
- 	dma_free_coherent(&dev->pci_dev->dev, 4096, mem, dma_addr);
+ 	dma_free_coherent(dev->dev, 4096, mem, dma_addr);
  
++<<<<<<< HEAD
 +	if (!dev->tagset.tags) {
 +		dev->tagset.ops = &nvme_mq_ops;
 +		dev->tagset.nr_hw_queues = dev->online_queues - 1;
 +		dev->tagset.timeout = NVME_IO_TIMEOUT;
 +		dev->tagset.numa_node = dev_to_node(&dev->pci_dev->dev);
 +		dev->tagset.queue_depth =
 +					min_t(int, dev->q_depth, BLK_MQ_MAX_DEPTH) - 1;
 +		dev->tagset.cmd_size = nvme_cmd_size(dev);
 +		dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
 +		dev->tagset.driver_data = dev;
 +		
 +		if (blk_mq_alloc_tag_set(&dev->tagset))
 +			return 0;
 +	}
 +	schedule_work(&dev->scan_work);
++=======
+ 	dev->tagset.ops = &nvme_mq_ops;
+ 	dev->tagset.nr_hw_queues = dev->online_queues - 1;
+ 	dev->tagset.timeout = NVME_IO_TIMEOUT;
+ 	dev->tagset.numa_node = dev_to_node(dev->dev);
+ 	dev->tagset.queue_depth =
+ 				min_t(int, dev->q_depth, BLK_MQ_MAX_DEPTH) - 1;
+ 	dev->tagset.cmd_size = nvme_cmd_size(dev);
+ 	dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
+ 	dev->tagset.driver_data = dev;
+ 
+ 	if (blk_mq_alloc_tag_set(&dev->tagset))
+ 		return 0;
+ 
+ 	for (i = 1; i <= nn; i++)
+ 		nvme_alloc_ns(dev, i);
+ 
++>>>>>>> e75ec752d725 (nvme: store a struct device pointer in struct nvme_dev)
  	return 0;
  }
  
@@@ -2883,37 -2868,19 +2915,49 @@@ static int nvme_dev_resume(struct nvme_
  	return 0;
  }
  
 +static void nvme_dead_ctrl(struct nvme_dev *dev)
 +{
 +	dev_warn(&dev->pci_dev->dev, "Device failed to resume\n");
 +	kref_get(&dev->kref);
 +	if (IS_ERR(kthread_run(nvme_remove_dead_ctrl, dev, "nvme%d",
 +						dev->instance))) {
 +		dev_err(&dev->pci_dev->dev,
 +			"Failed to start controller remove task\n");
 +		kref_put(&dev->kref, nvme_free_dev);
 +	}
 +}
 +
  static void nvme_dev_reset(struct nvme_dev *dev)
  {
 +	bool in_probe = work_busy(&dev->probe_work);
 +
  	nvme_dev_shutdown(dev);
++<<<<<<< HEAD
 +
 +	/* Synchronize with device probe so that work will see failure status
 +	 * and exit gracefully without trying to schedule another reset */
 +	flush_work(&dev->probe_work);
 +
 +	/* Fail this device if reset occured during probe to avoid
 +	 * infinite initialization loops. */
 +	if (in_probe) {
 +		nvme_dead_ctrl(dev);
 +		return;
++=======
+ 	if (nvme_dev_resume(dev)) {
+ 		dev_warn(dev->dev, "Device failed to resume\n");
+ 		kref_get(&dev->kref);
+ 		if (IS_ERR(kthread_run(nvme_remove_dead_ctrl, dev, "nvme%d",
+ 							dev->instance))) {
+ 			dev_err(dev->dev,
+ 				"Failed to start controller remove task\n");
+ 			kref_put(&dev->kref, nvme_free_dev);
+ 		}
++>>>>>>> e75ec752d725 (nvme: store a struct device pointer in struct nvme_dev)
  	}
 +	/* Schedule device resume asynchronously so the reset work is available
 +	 * to cleanup errors that may occur during reinitialization */
 +	schedule_work(&dev->probe_work);
  }
  
  static void nvme_reset_failed_dev(struct work_struct *ws)
@@@ -2984,8 -2918,9 +3028,14 @@@ static int nvme_probe(struct pci_dev *p
  		goto free;
  
  	INIT_LIST_HEAD(&dev->namespaces);
++<<<<<<< HEAD
 +	INIT_WORK(&dev->reset_work, nvme_reset_failed_dev);
 +	dev->pci_dev = pci_dev_get(pdev);
++=======
+ 	dev->reset_workfn = nvme_reset_failed_dev;
+ 	INIT_WORK(&dev->reset_work, nvme_reset_workfn);
+ 	dev->dev = get_device(&pdev->dev);
++>>>>>>> e75ec752d725 (nvme: store a struct device pointer in struct nvme_dev)
  	pci_set_drvdata(pdev, dev);
  	result = nvme_set_instance(dev);
  	if (result)
* Unmerged path drivers/block/nvme-core.c
diff --git a/drivers/block/nvme-scsi.c b/drivers/block/nvme-scsi.c
index 705b2b784a86..33a3ecd5971f 100644
--- a/drivers/block/nvme-scsi.c
+++ b/drivers/block/nvme-scsi.c
@@ -684,7 +684,7 @@ static int nvme_trans_standard_inquiry_page(struct nvme_ns *ns,
 	u8 cmdque = 0x01 << 1;
 	u8 fw_offset = sizeof(dev->firmware_rev);
 
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ns),
 				&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -728,8 +728,7 @@ static int nvme_trans_standard_inquiry_page(struct nvme_ns *ns,
 	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
 
  out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ns), mem, dma_addr);
  out_dma:
 	return res;
 }
@@ -787,7 +786,7 @@ static int nvme_trans_device_id_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	int xfer_len;
 	__be32 tmp_id = cpu_to_be32(ns->ns_id);
 
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ns),
 					&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -842,7 +841,7 @@ static int nvme_trans_device_id_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 		inq_response[6] = 0x00;    /* Rsvd */
 		inq_response[7] = 0x44;    /* Designator Length */
 
-		sprintf(&inq_response[8], "%04x", dev->pci_dev->vendor);
+		sprintf(&inq_response[8], "%04x", to_pci_dev(dev->dev)->vendor);
 		memcpy(&inq_response[12], dev->model, sizeof(dev->model));
 		sprintf(&inq_response[52], "%04x", tmp_id);
 		memcpy(&inq_response[56], dev->serial, sizeof(dev->serial));
@@ -851,8 +850,7 @@ static int nvme_trans_device_id_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
 
  out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ns), mem, dma_addr);
  out_dma:
 	return res;
 }
@@ -883,7 +881,7 @@ static int nvme_trans_ext_inq_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 		goto out_mem;
 	}
 
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ns),
 							&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -933,8 +931,7 @@ static int nvme_trans_ext_inq_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
 
  out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ns), mem, dma_addr);
  out_dma:
 	kfree(inq_response);
  out_mem:
@@ -1039,8 +1036,7 @@ static int nvme_trans_log_info_exceptions(struct nvme_ns *ns,
 		goto out_mem;
 	}
 
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_smart_log),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_smart_log),
 					&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -1078,7 +1074,7 @@ static int nvme_trans_log_info_exceptions(struct nvme_ns *ns,
 	xfer_len = min(alloc_len, LOG_INFO_EXCP_PAGE_LENGTH);
 	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
 
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_smart_log),
+	dma_free_coherent(dev->dev, sizeof(struct nvme_smart_log),
 			  mem, dma_addr);
  out_dma:
 	kfree(log_response);
@@ -1107,8 +1103,7 @@ static int nvme_trans_log_temperature(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 		goto out_mem;
 	}
 
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_smart_log),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_smart_log),
 					&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -1159,7 +1154,7 @@ static int nvme_trans_log_temperature(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	xfer_len = min(alloc_len, LOG_TEMP_PAGE_LENGTH);
 	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
 
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_smart_log),
+	dma_free_coherent(dev->dev, sizeof(struct nvme_smart_log),
 			  mem, dma_addr);
  out_dma:
 	kfree(log_response);
@@ -1210,7 +1205,7 @@ static int nvme_trans_fill_blk_desc(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	else if (llbaa > 0 && len < MODE_PAGE_LLBAA_BLK_DES_LEN)
 		return SNTI_INTERNAL_ERROR;
 
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ns),
 							&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -1247,8 +1242,7 @@ static int nvme_trans_fill_blk_desc(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	}
 
  out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ns), mem, dma_addr);
  out:
 	return res;
 }
@@ -1495,8 +1489,7 @@ static int nvme_trans_power_state(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	unsigned ps_desired = 0;
 
 	/* NVMe Controller Identify */
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-				sizeof(struct nvme_id_ctrl),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ctrl),
 				&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -1557,8 +1550,7 @@ static int nvme_trans_power_state(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	if (nvme_sc)
 		res = nvme_sc;
  out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ctrl), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ctrl), mem, dma_addr);
  out:
 	return res;
 }
@@ -1821,7 +1813,7 @@ static int nvme_trans_fmt_set_blk_size_count(struct nvme_ns *ns,
 	 */
 
 	if (ns->mode_select_num_blocks == 0 || ns->mode_select_block_len == 0) {
-		mem = dma_alloc_coherent(&dev->pci_dev->dev,
+		mem = dma_alloc_coherent(dev->dev,
 			sizeof(struct nvme_id_ns), &dma_addr, GFP_KERNEL);
 		if (mem == NULL) {
 			res = -ENOMEM;
@@ -1846,7 +1838,7 @@ static int nvme_trans_fmt_set_blk_size_count(struct nvme_ns *ns,
 						(1 << (id_ns->lbaf[flbas].ds));
 		}
  out_dma:
-		dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
+		dma_free_coherent(dev->dev, sizeof(struct nvme_id_ns),
 				  mem, dma_addr);
 	}
  out:
@@ -1929,7 +1921,7 @@ static int nvme_trans_fmt_send_cmd(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	struct nvme_command c;
 
 	/* Loop thru LBAF's in id_ns to match reqd lbaf, put in cdw10 */
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ns),
 							&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -1980,8 +1972,7 @@ static int nvme_trans_fmt_send_cmd(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 		res = nvme_sc;
 
  out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ns), mem, dma_addr);
  out:
 	return res;
 }
@@ -2487,7 +2478,7 @@ static int nvme_trans_read_capacity(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 		resp_size = READ_CAP_16_RESP_SIZE;
 	}
 
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
+	mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ns),
 							&dma_addr, GFP_KERNEL);
 	if (mem == NULL) {
 		res = -ENOMEM;
@@ -2516,8 +2507,7 @@ static int nvme_trans_read_capacity(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 
 	kfree(response);
  out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ns), mem, dma_addr);
  out:
 	return res;
 }
@@ -2550,8 +2540,7 @@ static int nvme_trans_report_luns(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 		goto out;
 	} else {
 		/* NVMe Controller Identify */
-		mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_id_ctrl),
+		mem = dma_alloc_coherent(dev->dev, sizeof(struct nvme_id_ctrl),
 					&dma_addr, GFP_KERNEL);
 		if (mem == NULL) {
 			res = -ENOMEM;
@@ -2602,8 +2591,7 @@ static int nvme_trans_report_luns(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 
 	kfree(response);
  out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ctrl), mem,
-			  dma_addr);
+	dma_free_coherent(dev->dev, sizeof(struct nvme_id_ctrl), mem, dma_addr);
  out:
 	return res;
 }
@@ -2915,7 +2903,7 @@ static int nvme_trans_unmap(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 		goto out;
 	}
 
-	range = dma_alloc_coherent(&dev->pci_dev->dev, ndesc * sizeof(*range),
+	range = dma_alloc_coherent(dev->dev, ndesc * sizeof(*range),
 							&dma_addr, GFP_KERNEL);
 	if (!range)
 		goto out;
@@ -2936,8 +2924,7 @@ static int nvme_trans_unmap(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	nvme_sc = nvme_submit_sync_cmd(ns->queue, &c);
 	res = nvme_trans_status_code(hdr, nvme_sc);
 
-	dma_free_coherent(&dev->pci_dev->dev, ndesc * sizeof(*range),
-							range, dma_addr);
+	dma_free_coherent(dev->dev, ndesc * sizeof(*range), range, dma_addr);
  out:
 	kfree(plist);
 	return res;
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index c0535e04f9c4..e3acef95762e 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -74,7 +74,7 @@ struct nvme_dev {
 	struct blk_mq_tag_set tagset;
 	struct blk_mq_tag_set admin_tagset;
 	u32 __iomem *dbs;
-	struct pci_dev *pci_dev;
+	struct device *dev;
 	struct dma_pool *prp_page_pool;
 	struct dma_pool *prp_small_pool;
 	int instance;
