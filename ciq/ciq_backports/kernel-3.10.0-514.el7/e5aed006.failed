udp: prevent skbs lingering in tunnel socket queues

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Hannes Frederic Sowa <hannes@stressinduktion.org>
commit e5aed006be918af163eb397e45aa5ea6cefd5e01
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e5aed006.failed

In case we find a socket with encapsulation enabled we should call
the encap_recv function even if just a udp header without payload is
available. The callbacks are responsible for correctly verifying and
dropping the packets.

Also, in case the header validation fails for geneve and vxlan we
shouldn't put the skb back into the socket queue, no one will pick
them up there.  Instead we can simply discard them in the respective
encap_recv functions.

	Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e5aed006be918af163eb397e45aa5ea6cefd5e01)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/vxlan.c
#	net/ipv4/udp.c
#	net/ipv6/udp.c
diff --cc drivers/net/vxlan.c
index c9c40f07cb6d,8ff30c3bdfce..000000000000
--- a/drivers/net/vxlan.c
+++ b/drivers/net/vxlan.c
@@@ -1128,58 -1142,181 +1128,71 @@@ static int vxlan_igmp_leave(struct vxla
  	return ret;
  }
  
 -static bool vxlan_remcsum(struct vxlanhdr *unparsed,
 -			  struct sk_buff *skb, u32 vxflags)
 -{
 -	size_t start, offset;
 -
 -	if (!(unparsed->vx_flags & VXLAN_HF_RCO) || skb->remcsum_offload)
 -		goto out;
 -
 -	start = vxlan_rco_start(unparsed->vx_vni);
 -	offset = start + vxlan_rco_offset(unparsed->vx_vni);
 -
 -	if (!pskb_may_pull(skb, offset + sizeof(u16)))
 -		return false;
 -
 -	skb_remcsum_process(skb, (void *)(vxlan_hdr(skb) + 1), start, offset,
 -			    !!(vxflags & VXLAN_F_REMCSUM_NOPARTIAL));
 -out:
 -	unparsed->vx_flags &= ~VXLAN_HF_RCO;
 -	unparsed->vx_vni &= VXLAN_VNI_MASK;
 -	return true;
 -}
 -
 -static void vxlan_parse_gbp_hdr(struct vxlanhdr *unparsed,
 -				struct sk_buff *skb, u32 vxflags,
 -				struct vxlan_metadata *md)
 +static struct vxlanhdr *vxlan_remcsum(struct sk_buff *skb, struct vxlanhdr *vh,
 +				      size_t hdrlen, u32 data, bool nopartial)
  {
 -	struct vxlanhdr_gbp *gbp = (struct vxlanhdr_gbp *)unparsed;
 -	struct metadata_dst *tun_dst;
 +	size_t start, offset, plen;
  
 -	if (!(unparsed->vx_flags & VXLAN_HF_GBP))
 -		goto out;
 +	if (skb->remcsum_offload)
 +		return vh;
  
 -	md->gbp = ntohs(gbp->policy_id);
 +	start = (data & VXLAN_RCO_MASK) << VXLAN_RCO_SHIFT;
 +	offset = start + ((data & VXLAN_RCO_UDP) ?
 +			  offsetof(struct udphdr, check) :
 +			  offsetof(struct tcphdr, check));
  
 -	tun_dst = (struct metadata_dst *)skb_dst(skb);
 -	if (tun_dst) {
 -		tun_dst->u.tun_info.key.tun_flags |= TUNNEL_VXLAN_OPT;
 -		tun_dst->u.tun_info.options_len = sizeof(*md);
 -	}
 -	if (gbp->dont_learn)
 -		md->gbp |= VXLAN_GBP_DONT_LEARN;
 +	plen = hdrlen + offset + sizeof(u16);
  
 -	if (gbp->policy_applied)
 -		md->gbp |= VXLAN_GBP_POLICY_APPLIED;
 +	if (!pskb_may_pull(skb, plen))
 +		return NULL;
  
 -	/* In flow-based mode, GBP is carried in dst_metadata */
 -	if (!(vxflags & VXLAN_F_COLLECT_METADATA))
 -		skb->mark = md->gbp;
 -out:
 -	unparsed->vx_flags &= ~VXLAN_GBP_USED_BITS;
 -}
 +	vh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
  
 -static bool vxlan_parse_gpe_hdr(struct vxlanhdr *unparsed,
 -				__be16 *protocol,
 -				struct sk_buff *skb, u32 vxflags)
 -{
 -	struct vxlanhdr_gpe *gpe = (struct vxlanhdr_gpe *)unparsed;
 +	skb_remcsum_process(skb, (void *)vh + hdrlen, start, offset,
 +			    nopartial);
  
 -	/* Need to have Next Protocol set for interfaces in GPE mode. */
 -	if (!gpe->np_applied)
 -		return false;
 -	/* "The initial version is 0. If a receiver does not support the
 -	 * version indicated it MUST drop the packet.
 -	 */
 -	if (gpe->version != 0)
 -		return false;
 -	/* "When the O bit is set to 1, the packet is an OAM packet and OAM
 -	 * processing MUST occur." However, we don't implement OAM
 -	 * processing, thus drop the packet.
 -	 */
 -	if (gpe->oam_flag)
 -		return false;
 -
 -	switch (gpe->next_protocol) {
 -	case VXLAN_GPE_NP_IPV4:
 -		*protocol = htons(ETH_P_IP);
 -		break;
 -	case VXLAN_GPE_NP_IPV6:
 -		*protocol = htons(ETH_P_IPV6);
 -		break;
 -	case VXLAN_GPE_NP_ETHERNET:
 -		*protocol = htons(ETH_P_TEB);
 -		break;
 -	default:
 -		return false;
 -	}
 -
 -	unparsed->vx_flags &= ~VXLAN_GPE_USED_BITS;
 -	return true;
 +	return vh;
  }
  
 -static bool vxlan_set_mac(struct vxlan_dev *vxlan,
 -			  struct vxlan_sock *vs,
 -			  struct sk_buff *skb)
 +/* Callback from net/ipv4/udp.c to receive packets */
 +static int vxlan_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
  {
 -	union vxlan_addr saddr;
 +	struct vxlan_sock *vs;
 +	struct vxlanhdr *vxh;
 +	u32 flags, vni;
 +	struct vxlan_metadata md = {0};
  
 -	skb_reset_mac_header(skb);
 -	skb->protocol = eth_type_trans(skb, vxlan->dev);
 -	skb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);
 +	/* Need Vxlan and inner Ethernet header to be present */
 +	if (!pskb_may_pull(skb, VXLAN_HLEN))
++<<<<<<< HEAD
 +		goto error;
  
 -	/* Ignore packet loops (and multicast echo) */
 -	if (ether_addr_equal(eth_hdr(skb)->h_source, vxlan->dev->dev_addr))
 -		return false;
 +	vxh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
 +	flags = ntohl(vxh->vx_flags);
 +	vni = ntohl(vxh->vx_vni);
  
 -	/* Get address from the outer IP header */
 -	if (vxlan_get_sk_family(vs) == AF_INET) {
 -		saddr.sin.sin_addr.s_addr = ip_hdr(skb)->saddr;
 -		saddr.sa.sa_family = AF_INET;
 -#if IS_ENABLED(CONFIG_IPV6)
 +	if (flags & VXLAN_HF_VNI) {
 +		flags &= ~VXLAN_HF_VNI;
  	} else {
 -		saddr.sin6.sin6_addr = ipv6_hdr(skb)->saddr;
 -		saddr.sa.sa_family = AF_INET6;
 -#endif
 -	}
 -
 -	if ((vxlan->flags & VXLAN_F_LEARN) &&
 -	    vxlan_snoop(skb->dev, &saddr, eth_hdr(skb)->h_source))
 -		return false;
 -
 -	return true;
 -}
 -
 -static bool vxlan_ecn_decapsulate(struct vxlan_sock *vs, void *oiph,
 -				  struct sk_buff *skb)
 -{
 -	int err = 0;
 -
 -	if (vxlan_get_sk_family(vs) == AF_INET)
 -		err = IP_ECN_decapsulate(oiph, skb);
 -#if IS_ENABLED(CONFIG_IPV6)
 -	else
 -		err = IP6_ECN_decapsulate(oiph, skb);
 -#endif
 -
 -	if (unlikely(err) && log_ecn_error) {
 -		if (vxlan_get_sk_family(vs) == AF_INET)
 -			net_info_ratelimited("non-ECT from %pI4 with TOS=%#x\n",
 -					     &((struct iphdr *)oiph)->saddr,
 -					     ((struct iphdr *)oiph)->tos);
 -		else
 -			net_info_ratelimited("non-ECT from %pI6\n",
 -					     &((struct ipv6hdr *)oiph)->saddr);
 -	}
 -	return err <= 1;
 -}
 -
 -/* Callback from net/ipv4/udp.c to receive packets */
 -static int vxlan_rcv(struct sock *sk, struct sk_buff *skb)
 -{
 -	struct pcpu_sw_netstats *stats;
 -	struct vxlan_dev *vxlan;
 -	struct vxlan_sock *vs;
 -	struct vxlanhdr unparsed;
 -	struct vxlan_metadata _md;
 -	struct vxlan_metadata *md = &_md;
 -	__be16 protocol = htons(ETH_P_TEB);
 -	bool raw_proto = false;
 -	void *oiph;
 -
 -	/* Need UDP and VXLAN header to be present */
 -	if (!pskb_may_pull(skb, VXLAN_HLEN))
 +		/* VNI flag always required to be set */
 +		goto bad_flags;
++=======
+ 		goto drop;
+ 
+ 	unparsed = *vxlan_hdr(skb);
+ 	/* VNI flag always required to be set */
+ 	if (!(unparsed.vx_flags & VXLAN_HF_VNI)) {
+ 		netdev_dbg(skb->dev, "invalid vxlan flags=%#x vni=%#x\n",
+ 			   ntohl(vxlan_hdr(skb)->vx_flags),
+ 			   ntohl(vxlan_hdr(skb)->vx_vni));
+ 		/* Return non vxlan pkt */
+ 		goto drop;
++>>>>>>> e5aed006be91 (udp: prevent skbs lingering in tunnel socket queues)
  	}
 -	unparsed.vx_flags &= ~VXLAN_HF_VNI;
 -	unparsed.vx_vni &= ~VXLAN_VNI_MASK;
 +
 +	if (iptunnel_pull_header(skb, VXLAN_HLEN, htons(ETH_P_TEB)))
 +		goto drop;
 +	vxh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
  
  	vs = rcu_dereference_sk_user_data(sk);
  	if (!vs)
diff --cc net/ipv4/udp.c
index 556580e2c4f8,d56c0559b477..000000000000
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@@ -1518,7 -1565,7 +1518,11 @@@ int udp_queue_rcv_skb(struct sock *sk, 
  
  		/* if we're overly short, let UDP handle it */
  		encap_rcv = ACCESS_ONCE(up->encap_rcv);
++<<<<<<< HEAD
 +		if (skb->len > sizeof(struct udphdr) && encap_rcv != NULL) {
++=======
+ 		if (encap_rcv) {
++>>>>>>> e5aed006be91 (udp: prevent skbs lingering in tunnel socket queues)
  			int ret;
  
  			/* Verify checksum before giving to encap */
diff --cc net/ipv6/udp.c
index 2c9dae38dffd,2da1896af934..000000000000
--- a/net/ipv6/udp.c
+++ b/net/ipv6/udp.c
@@@ -632,7 -617,7 +632,11 @@@ int udpv6_queue_rcv_skb(struct sock *sk
  
  		/* if we're overly short, let UDP handle it */
  		encap_rcv = ACCESS_ONCE(up->encap_rcv);
++<<<<<<< HEAD
 +		if (skb->len > sizeof(struct udphdr) && encap_rcv != NULL) {
++=======
+ 		if (encap_rcv) {
++>>>>>>> e5aed006be91 (udp: prevent skbs lingering in tunnel socket queues)
  			int ret;
  
  			/* Verify checksum before giving to encap */
diff --git a/drivers/net/geneve.c b/drivers/net/geneve.c
index 02b7bf0b4bec..5e6ceb625319 100644
--- a/drivers/net/geneve.c
+++ b/drivers/net/geneve.c
@@ -244,15 +244,15 @@ static int geneve_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 
 	/* Need Geneve and inner Ethernet header to be present */
 	if (unlikely(!pskb_may_pull(skb, GENEVE_BASE_HLEN)))
-		goto error;
+		goto drop;
 
 	/* Return packets with reserved bits set */
 	geneveh = geneve_hdr(skb);
 	if (unlikely(geneveh->ver != GENEVE_VER))
-		goto error;
+		goto drop;
 
 	if (unlikely(geneveh->proto_type != htons(ETH_P_TEB)))
-		goto error;
+		goto drop;
 
 	opts_len = geneveh->opt_len * 4;
 	if (iptunnel_pull_header(skb, GENEVE_BASE_HLEN + opts_len,
@@ -270,10 +270,6 @@ drop:
 	/* Consume bad packet */
 	kfree_skb(skb);
 	return 0;
-
-error:
-	/* Let the UDP layer deal with the skb */
-	return 1;
 }
 
 static struct socket *geneve_create_sock(struct net *net, bool ipv6,
* Unmerged path drivers/net/vxlan.c
* Unmerged path net/ipv4/udp.c
* Unmerged path net/ipv6/udp.c
