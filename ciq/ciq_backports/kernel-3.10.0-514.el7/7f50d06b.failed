ftrace: Update dynamic ftrace calls only if necessary

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jiri Olsa <jolsa@kernel.org>
commit 7f50d06bb6b825d34f069c6c7a1aab96ad0b94d9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/7f50d06b.failed

Currently dynamic ftrace calls are updated any time
the ftrace_ops is un/registered. If we do  this update
only when it's needed, we save lot of time for perf
system wide ftrace function sampling/counting.

The reason is that for system wide sampling/counting,
perf creates event for each cpu in the system.

Each event then registers separate copy of ftrace_ops,
which ends up in FTRACE_UPDATE_CALLS updates. On servers
with many cpus that means serious stall (240 cpus server):

Counting:
  # time ./perf stat -e ftrace:function -a sleep 1

   Performance counter stats for 'system wide':

              370,663      ftrace:function

          1.401427505 seconds time elapsed

  real    3m51.743s
  user    0m0.023s
  sys     3m48.569s

Sampling:
  # time ./perf record -e ftrace:function -a sleep 1
  [ perf record: Woken up 0 times to write data ]
  Warning:
  Processed 141200 events and lost 5 chunks!

  [ perf record: Captured and wrote 10.703 MB perf.data (135950 samples) ]

  real    2m31.429s
  user    0m0.213s
  sys     2m29.494s

There's no reason to do the FTRACE_UPDATE_CALLS update
for each event in perf case, because all the ftrace_ops
always share the same filter, so the updated calls are
always the same.

It's required that only first ftrace_ops registration
does the FTRACE_UPDATE_CALLS update (also sometimes
the second if the first one used the trampoline), but
the rest can be only cheaply linked into the ftrace_ops
list.

Counting:
  # time ./perf stat -e ftrace:function -a sleep 1

   Performance counter stats for 'system wide':

             398,571      ftrace:function

         1.377503733 seconds time elapsed

  real    0m2.787s
  user    0m0.005s
  sys     0m1.883s

Sampling:
  # time ./perf record -e ftrace:function -a sleep 1
  [ perf record: Woken up 0 times to write data ]
  Warning:
  Processed 261730 events and lost 9 chunks!

  [ perf record: Captured and wrote 19.907 MB perf.data (256293 samples) ]

  real    1m31.948s
  user    0m0.309s
  sys     1m32.051s

Link: http://lkml.kernel.org/r/1458138873-1553-6-git-send-email-jolsa@kernel.org

	Acked-by: Namhyung Kim <namhyung@kernel.org>
	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
(cherry picked from commit 7f50d06bb6b825d34f069c6c7a1aab96ad0b94d9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/trace/ftrace.c
diff --cc kernel/trace/ftrace.c
index 71cd8eb49200,d3850cbb840f..000000000000
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@@ -2042,20 -2651,28 +2042,24 @@@ static int ftrace_startup(struct ftrace
  		return ret;
  
  	ftrace_start_up++;
- 	command |= FTRACE_UPDATE_CALLS;
  
 -	/*
 -	 * Note that ftrace probes uses this to start up
 -	 * and modify functions it will probe. But we still
 -	 * set the ADDING flag for modification, as probes
 -	 * do not have trampolines. If they add them in the
 -	 * future, then the probes will need to distinguish
 -	 * between adding and updating probes.
 -	 */
 -	ops->flags |= FTRACE_OPS_FL_ENABLED | FTRACE_OPS_FL_ADDING;
 -
 -	ret = ftrace_hash_ipmodify_enable(ops);
 -	if (ret < 0) {
 -		/* Rollback registration process */
 -		__unregister_ftrace_function(ops);
 -		ftrace_start_up--;
 -		ops->flags &= ~FTRACE_OPS_FL_ENABLED;
 -		return ret;
 +	/* ops marked global share the filter hashes */
 +	if (ops->flags & FTRACE_OPS_FL_GLOBAL) {
 +		ops = &global_ops;
 +		/* Don't update hash if global is already set */
 +		if (global_start_up)
 +			hash_enable = false;
 +		global_start_up++;
  	}
  
++<<<<<<< HEAD
 +	ops->flags |= FTRACE_OPS_FL_ENABLED;
 +	if (hash_enable)
 +		ftrace_hash_rec_enable(ops, 1);
++=======
+ 	if (ftrace_hash_rec_enable(ops, 1))
+ 		command |= FTRACE_UPDATE_CALLS;
++>>>>>>> 7f50d06bb6b8 (ftrace: Update dynamic ftrace calls only if necessary)
  
  	ftrace_startup_enable(command);
  
@@@ -2082,25 -2700,14 +2086,31 @@@ static int ftrace_shutdown(struct ftrac
  	 */
  	WARN_ON_ONCE(ftrace_start_up < 0);
  
++<<<<<<< HEAD
 +	if (ops->flags & FTRACE_OPS_FL_GLOBAL) {
 +		ops = &global_ops;
 +		global_start_up--;
 +		WARN_ON_ONCE(global_start_up < 0);
 +		/* Don't update hash if global still has users */
 +		if (global_start_up) {
 +			WARN_ON_ONCE(!ftrace_start_up);
 +			hash_disable = false;
 +		}
 +	}
++=======
+ 	/* Disabling ipmodify never fails */
+ 	ftrace_hash_ipmodify_disable(ops);
+ 
+ 	if (ftrace_hash_rec_disable(ops, 1))
+ 		command |= FTRACE_UPDATE_CALLS;
++>>>>>>> 7f50d06bb6b8 (ftrace: Update dynamic ftrace calls only if necessary)
  
 -	ops->flags &= ~FTRACE_OPS_FL_ENABLED;
 +	if (hash_disable)
 +		ftrace_hash_rec_disable(ops, 1);
 +
 +	if (ops != &global_ops || !global_start_up)
 +		ops->flags &= ~FTRACE_OPS_FL_ENABLED;
  
- 	command |= FTRACE_UPDATE_CALLS;
- 
  	if (saved_ftrace_func != ftrace_trace_function) {
  		saved_ftrace_func = ftrace_trace_function;
  		command |= FTRACE_UPDATE_TRACE_FUNC;
* Unmerged path kernel/trace/ftrace.c
