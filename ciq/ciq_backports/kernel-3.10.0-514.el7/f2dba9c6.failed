rhashtable: Introduce rhashtable_walk_*

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit f2dba9c6ff0d9a515b4c3f1b037cd65c8b2a868c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f2dba9c6.failed

Some existing rhashtable users get too intimate with it by walking
the buckets directly.  This prevents us from easily changing the
internals of rhashtable.

This patch adds the helpers rhashtable_walk_init/exit/start/next/stop
which will replace these custom walkers.

They are meant to be usable for both procfs seq_file walks as well
as walking by a netlink dump.  The iterator structure should fit
inside a netlink dump cb structure, with at least one element to
spare.

	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit f2dba9c6ff0d9a515b4c3f1b037cd65c8b2a868c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rhashtable.h
#	lib/rhashtable.c
diff --cc include/linux/rhashtable.h
index 0839d7b8cd60,58851275fed9..000000000000
--- a/include/linux/rhashtable.h
+++ b/include/linux/rhashtable.h
@@@ -18,7 -18,29 +18,33 @@@
  #ifndef _LINUX_RHASHTABLE_H
  #define _LINUX_RHASHTABLE_H
  
++<<<<<<< HEAD
 +#include <linux/rculist.h>
++=======
+ #include <linux/compiler.h>
+ #include <linux/list_nulls.h>
+ #include <linux/workqueue.h>
+ #include <linux/mutex.h>
+ 
+ /*
+  * The end of the chain is marked with a special nulls marks which has
+  * the following format:
+  *
+  * +-------+-----------------------------------------------------+-+
+  * | Base  |                      Hash                           |1|
+  * +-------+-----------------------------------------------------+-+
+  *
+  * Base (4 bits) : Reserved to distinguish between multiple tables.
+  *                 Specified via &struct rhashtable_params.nulls_base.
+  * Hash (27 bits): Full hash (unmasked) of first element added to bucket
+  * 1 (1 bit)     : Nulls marker (always set)
+  *
+  * The remaining bits of the next pointer remain unused for now.
+  */
+ #define RHT_BASE_BITS		4
+ #define RHT_HASH_BITS		27
+ #define RHT_BASE_SHIFT		RHT_HASH_BITS
++>>>>>>> f2dba9c6ff0d (rhashtable: Introduce rhashtable_walk_*)
  
  struct rhash_head {
  	struct rhash_head __rcu		*next;
@@@ -74,19 -109,73 +100,79 @@@ struct rhashtable_params 
   * @nelems: Number of elements in table
   * @shift: Current size (1 << shift)
   * @p: Configuration parameters
++<<<<<<< HEAD
++=======
+  * @run_work: Deferred worker to expand/shrink asynchronously
+  * @mutex: Mutex to protect current/future table swapping
+  * @walkers: List of active walkers
+  * @being_destroyed: True if table is set up for destruction
++>>>>>>> f2dba9c6ff0d (rhashtable: Introduce rhashtable_walk_*)
   */
  struct rhashtable {
  	struct bucket_table __rcu	*tbl;
 -	struct bucket_table __rcu       *future_tbl;
 -	atomic_t			nelems;
 -	atomic_t			shift;
 +	size_t				nelems;
 +	size_t				shift;
  	struct rhashtable_params	p;
++<<<<<<< HEAD
++};
++
++=======
+ 	struct work_struct		run_work;
+ 	struct mutex                    mutex;
+ 	struct list_head		walkers;
+ 	bool                            being_destroyed;
+ };
+ 
+ /**
+  * struct rhashtable_walker - Hash table walker
+  * @list: List entry on list of walkers
+  * @resize: Resize event occured
+  */
+ struct rhashtable_walker {
+ 	struct list_head list;
+ 	bool resize;
+ };
+ 
+ /**
+  * struct rhashtable_iter - Hash table iterator, fits into netlink cb
+  * @ht: Table to iterate through
+  * @p: Current pointer
+  * @walker: Associated rhashtable walker
+  * @slot: Current slot
+  * @skip: Number of entries to skip in slot
+  */
+ struct rhashtable_iter {
+ 	struct rhashtable *ht;
+ 	struct rhash_head *p;
+ 	struct rhashtable_walker *walker;
+ 	unsigned int slot;
+ 	unsigned int skip;
  };
  
+ static inline unsigned long rht_marker(const struct rhashtable *ht, u32 hash)
+ {
+ 	return NULLS_MARKER(ht->p.nulls_base + hash);
+ }
+ 
+ #define INIT_RHT_NULLS_HEAD(ptr, ht, hash) \
+ 	((ptr) = (typeof(ptr)) rht_marker(ht, hash))
+ 
+ static inline bool rht_is_a_nulls(const struct rhash_head *ptr)
+ {
+ 	return ((unsigned long) ptr & 1);
+ }
+ 
+ static inline unsigned long rht_get_nulls_value(const struct rhash_head *ptr)
+ {
+ 	return ((unsigned long) ptr) >> 1;
+ }
+ 
++>>>>>>> f2dba9c6ff0d (rhashtable: Introduce rhashtable_walk_*)
  #ifdef CONFIG_PROVE_LOCKING
 -int lockdep_rht_mutex_is_held(struct rhashtable *ht);
 +int lockdep_rht_mutex_is_held(const struct rhashtable *ht);
  int lockdep_rht_bucket_is_held(const struct bucket_table *tbl, u32 hash);
  #else
 -static inline int lockdep_rht_mutex_is_held(struct rhashtable *ht)
 +static inline int lockdep_rht_mutex_is_held(const struct rhashtable *ht)
  {
  	return 1;
  }
@@@ -109,11 -198,23 +195,27 @@@ bool rht_shrink_below_30(const struct r
  int rhashtable_expand(struct rhashtable *ht);
  int rhashtable_shrink(struct rhashtable *ht);
  
 -void *rhashtable_lookup(struct rhashtable *ht, const void *key);
 -void *rhashtable_lookup_compare(struct rhashtable *ht, const void *key,
 +void *rhashtable_lookup(const struct rhashtable *ht, const void *key);
 +void *rhashtable_lookup_compare(const struct rhashtable *ht, const void *key,
  				bool (*compare)(void *, void *), void *arg);
  
++<<<<<<< HEAD
 +void rhashtable_destroy(const struct rhashtable *ht);
++=======
+ bool rhashtable_lookup_insert(struct rhashtable *ht, struct rhash_head *obj);
+ bool rhashtable_lookup_compare_insert(struct rhashtable *ht,
+ 				      struct rhash_head *obj,
+ 				      bool (*compare)(void *, void *),
+ 				      void *arg);
+ 
+ int rhashtable_walk_init(struct rhashtable *ht, struct rhashtable_iter *iter);
+ void rhashtable_walk_exit(struct rhashtable_iter *iter);
+ int rhashtable_walk_start(struct rhashtable_iter *iter) __acquires(RCU);
+ void *rhashtable_walk_next(struct rhashtable_iter *iter);
+ void rhashtable_walk_stop(struct rhashtable_iter *iter) __releases(RCU);
+ 
+ void rhashtable_destroy(struct rhashtable *ht);
++>>>>>>> f2dba9c6ff0d (rhashtable: Introduce rhashtable_walk_*)
  
  #define rht_dereference(p, ht) \
  	rcu_dereference_protected(p, lockdep_rht_mutex_is_held(ht))
diff --cc lib/rhashtable.c
index be20e9720492,057919164e23..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -316,8 -480,64 +316,67 @@@ int rhashtable_shrink(struct rhashtabl
  }
  EXPORT_SYMBOL_GPL(rhashtable_shrink);
  
++<<<<<<< HEAD
++=======
+ static void rht_deferred_worker(struct work_struct *work)
+ {
+ 	struct rhashtable *ht;
+ 	struct bucket_table *tbl;
+ 	struct rhashtable_walker *walker;
+ 
+ 	ht = container_of(work, struct rhashtable, run_work);
+ 	mutex_lock(&ht->mutex);
+ 	if (ht->being_destroyed)
+ 		goto unlock;
+ 
+ 	tbl = rht_dereference(ht->tbl, ht);
+ 
+ 	list_for_each_entry(walker, &ht->walkers, list)
+ 		walker->resize = true;
+ 
+ 	if (ht->p.grow_decision && ht->p.grow_decision(ht, tbl->size))
+ 		rhashtable_expand(ht);
+ 	else if (ht->p.shrink_decision && ht->p.shrink_decision(ht, tbl->size))
+ 		rhashtable_shrink(ht);
+ 
+ unlock:
+ 	mutex_unlock(&ht->mutex);
+ }
+ 
+ static void rhashtable_wakeup_worker(struct rhashtable *ht)
+ {
+ 	struct bucket_table *tbl = rht_dereference_rcu(ht->tbl, ht);
+ 	struct bucket_table *new_tbl = rht_dereference_rcu(ht->future_tbl, ht);
+ 	size_t size = tbl->size;
+ 
+ 	/* Only adjust the table if no resizing is currently in progress. */
+ 	if (tbl == new_tbl &&
+ 	    ((ht->p.grow_decision && ht->p.grow_decision(ht, size)) ||
+ 	     (ht->p.shrink_decision && ht->p.shrink_decision(ht, size))))
+ 		schedule_work(&ht->run_work);
+ }
+ 
+ static void __rhashtable_insert(struct rhashtable *ht, struct rhash_head *obj,
+ 				struct bucket_table *tbl, u32 hash)
+ {
+ 	struct rhash_head *head = rht_dereference_bucket(tbl->buckets[hash],
+ 							 tbl, hash);
+ 
+ 	if (rht_is_a_nulls(head))
+ 		INIT_RHT_NULLS_HEAD(obj->next, ht, hash);
+ 	else
+ 		RCU_INIT_POINTER(obj->next, head);
+ 
+ 	rcu_assign_pointer(tbl->buckets[hash], obj);
+ 
+ 	atomic_inc(&ht->nelems);
+ 
+ 	rhashtable_wakeup_worker(ht);
+ }
+ 
++>>>>>>> f2dba9c6ff0d (rhashtable: Introduce rhashtable_walk_*)
  /**
 - * rhashtable_insert - insert object into hash table
 + * rhashtable_insert - insert object into hash hash table
   * @ht:		hash table
   * @obj:	pointer to hash head inside object
   *
@@@ -458,6 -728,262 +517,265 @@@ void *rhashtable_lookup_compare(const s
  }
  EXPORT_SYMBOL_GPL(rhashtable_lookup_compare);
  
++<<<<<<< HEAD
++=======
+ /**
+  * rhashtable_lookup_insert - lookup and insert object into hash table
+  * @ht:		hash table
+  * @obj:	pointer to hash head inside object
+  *
+  * Locks down the bucket chain in both the old and new table if a resize
+  * is in progress to ensure that writers can't remove from the old table
+  * and can't insert to the new table during the atomic operation of search
+  * and insertion. Searches for duplicates in both the old and new table if
+  * a resize is in progress.
+  *
+  * This lookup function may only be used for fixed key hash table (key_len
+  * parameter set). It will BUG() if used inappropriately.
+  *
+  * It is safe to call this function from atomic context.
+  *
+  * Will trigger an automatic deferred table resizing if the size grows
+  * beyond the watermark indicated by grow_decision() which can be passed
+  * to rhashtable_init().
+  */
+ bool rhashtable_lookup_insert(struct rhashtable *ht, struct rhash_head *obj)
+ {
+ 	struct rhashtable_compare_arg arg = {
+ 		.ht = ht,
+ 		.key = rht_obj(ht, obj) + ht->p.key_offset,
+ 	};
+ 
+ 	BUG_ON(!ht->p.key_len);
+ 
+ 	return rhashtable_lookup_compare_insert(ht, obj, &rhashtable_compare,
+ 						&arg);
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_lookup_insert);
+ 
+ /**
+  * rhashtable_lookup_compare_insert - search and insert object to hash table
+  *                                    with compare function
+  * @ht:		hash table
+  * @obj:	pointer to hash head inside object
+  * @compare:	compare function, must return true on match
+  * @arg:	argument passed on to compare function
+  *
+  * Locks down the bucket chain in both the old and new table if a resize
+  * is in progress to ensure that writers can't remove from the old table
+  * and can't insert to the new table during the atomic operation of search
+  * and insertion. Searches for duplicates in both the old and new table if
+  * a resize is in progress.
+  *
+  * Lookups may occur in parallel with hashtable mutations and resizing.
+  *
+  * Will trigger an automatic deferred table resizing if the size grows
+  * beyond the watermark indicated by grow_decision() which can be passed
+  * to rhashtable_init().
+  */
+ bool rhashtable_lookup_compare_insert(struct rhashtable *ht,
+ 				      struct rhash_head *obj,
+ 				      bool (*compare)(void *, void *),
+ 				      void *arg)
+ {
+ 	struct bucket_table *new_tbl, *old_tbl;
+ 	spinlock_t *new_bucket_lock, *old_bucket_lock;
+ 	u32 new_hash, old_hash;
+ 	bool success = true;
+ 
+ 	BUG_ON(!ht->p.key_len);
+ 
+ 	rcu_read_lock();
+ 
+ 	old_tbl = rht_dereference_rcu(ht->tbl, ht);
+ 	old_hash = head_hashfn(ht, old_tbl, obj);
+ 	old_bucket_lock = bucket_lock(old_tbl, old_hash);
+ 	spin_lock_bh(old_bucket_lock);
+ 
+ 	new_tbl = rht_dereference_rcu(ht->future_tbl, ht);
+ 	new_hash = head_hashfn(ht, new_tbl, obj);
+ 	new_bucket_lock = bucket_lock(new_tbl, new_hash);
+ 	if (unlikely(old_tbl != new_tbl))
+ 		spin_lock_bh_nested(new_bucket_lock, RHT_LOCK_NESTED);
+ 
+ 	if (rhashtable_lookup_compare(ht, rht_obj(ht, obj) + ht->p.key_offset,
+ 				      compare, arg)) {
+ 		success = false;
+ 		goto exit;
+ 	}
+ 
+ 	__rhashtable_insert(ht, obj, new_tbl, new_hash);
+ 
+ exit:
+ 	if (unlikely(old_tbl != new_tbl))
+ 		spin_unlock_bh(new_bucket_lock);
+ 	spin_unlock_bh(old_bucket_lock);
+ 
+ 	rcu_read_unlock();
+ 
+ 	return success;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_lookup_compare_insert);
+ 
+ /**
+  * rhashtable_walk_init - Initialise an iterator
+  * @ht:		Table to walk over
+  * @iter:	Hash table Iterator
+  *
+  * This function prepares a hash table walk.
+  *
+  * Note that if you restart a walk after rhashtable_walk_stop you
+  * may see the same object twice.  Also, you may miss objects if
+  * there are removals in between rhashtable_walk_stop and the next
+  * call to rhashtable_walk_start.
+  *
+  * For a completely stable walk you should construct your own data
+  * structure outside the hash table.
+  *
+  * This function may sleep so you must not call it from interrupt
+  * context or with spin locks held.
+  *
+  * You must call rhashtable_walk_exit if this function returns
+  * successfully.
+  */
+ int rhashtable_walk_init(struct rhashtable *ht, struct rhashtable_iter *iter)
+ {
+ 	iter->ht = ht;
+ 	iter->p = NULL;
+ 	iter->slot = 0;
+ 	iter->skip = 0;
+ 
+ 	iter->walker = kmalloc(sizeof(*iter->walker), GFP_KERNEL);
+ 	if (!iter->walker)
+ 		return -ENOMEM;
+ 
+ 	mutex_lock(&ht->mutex);
+ 	list_add(&iter->walker->list, &ht->walkers);
+ 	mutex_unlock(&ht->mutex);
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_init);
+ 
+ /**
+  * rhashtable_walk_exit - Free an iterator
+  * @iter:	Hash table Iterator
+  *
+  * This function frees resources allocated by rhashtable_walk_init.
+  */
+ void rhashtable_walk_exit(struct rhashtable_iter *iter)
+ {
+ 	mutex_lock(&iter->ht->mutex);
+ 	list_del(&iter->walker->list);
+ 	mutex_unlock(&iter->ht->mutex);
+ 	kfree(iter->walker);
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_exit);
+ 
+ /**
+  * rhashtable_walk_start - Start a hash table walk
+  * @iter:	Hash table iterator
+  *
+  * Start a hash table walk.  Note that we take the RCU lock in all
+  * cases including when we return an error.  So you must always call
+  * rhashtable_walk_stop to clean up.
+  *
+  * Returns zero if successful.
+  *
+  * Returns -EAGAIN if resize event occured.  Note that the iterator
+  * will rewind back to the beginning and you may use it immediately
+  * by calling rhashtable_walk_next.
+  */
+ int rhashtable_walk_start(struct rhashtable_iter *iter)
+ {
+ 	rcu_read_lock();
+ 
+ 	if (iter->walker->resize) {
+ 		iter->slot = 0;
+ 		iter->skip = 0;
+ 		iter->walker->resize = false;
+ 		return -EAGAIN;
+ 	}
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_start);
+ 
+ /**
+  * rhashtable_walk_next - Return the next object and advance the iterator
+  * @iter:	Hash table iterator
+  *
+  * Note that you must call rhashtable_walk_stop when you are finished
+  * with the walk.
+  *
+  * Returns the next object or NULL when the end of the table is reached.
+  *
+  * Returns -EAGAIN if resize event occured.  Note that the iterator
+  * will rewind back to the beginning and you may continue to use it.
+  */
+ void *rhashtable_walk_next(struct rhashtable_iter *iter)
+ {
+ 	const struct bucket_table *tbl;
+ 	struct rhashtable *ht = iter->ht;
+ 	struct rhash_head *p = iter->p;
+ 	void *obj = NULL;
+ 
+ 	tbl = rht_dereference_rcu(ht->tbl, ht);
+ 
+ 	if (p) {
+ 		p = rht_dereference_bucket_rcu(p->next, tbl, iter->slot);
+ 		goto next;
+ 	}
+ 
+ 	for (; iter->slot < tbl->size; iter->slot++) {
+ 		int skip = iter->skip;
+ 
+ 		rht_for_each_rcu(p, tbl, iter->slot) {
+ 			if (!skip)
+ 				break;
+ 			skip--;
+ 		}
+ 
+ next:
+ 		if (!rht_is_a_nulls(p)) {
+ 			iter->skip++;
+ 			iter->p = p;
+ 			obj = rht_obj(ht, p);
+ 			goto out;
+ 		}
+ 
+ 		iter->skip = 0;
+ 	}
+ 
+ 	iter->p = NULL;
+ 
+ out:
+ 	if (iter->walker->resize) {
+ 		iter->p = NULL;
+ 		iter->slot = 0;
+ 		iter->skip = 0;
+ 		iter->walker->resize = false;
+ 		return ERR_PTR(-EAGAIN);
+ 	}
+ 
+ 	return obj;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_next);
+ 
+ /**
+  * rhashtable_walk_stop - Finish a hash table walk
+  * @iter:	Hash table iterator
+  *
+  * Finish a hash table walk.
+  */
+ void rhashtable_walk_stop(struct rhashtable_iter *iter)
+ {
+ 	rcu_read_unlock();
+ 	iter->p = NULL;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_stop);
+ 
++>>>>>>> f2dba9c6ff0d (rhashtable: Introduce rhashtable_walk_*)
  static size_t rounded_hashtable_size(struct rhashtable_params *params)
  {
  	return max(roundup_pow_of_two(params->nelem_hint * 4 / 3),
@@@ -525,7 -1053,17 +843,21 @@@ int rhashtable_init(struct rhashtable *
  	if (params->nelem_hint)
  		size = rounded_hashtable_size(params);
  
++<<<<<<< HEAD
 +	tbl = bucket_table_alloc(size);
++=======
+ 	memset(ht, 0, sizeof(*ht));
+ 	mutex_init(&ht->mutex);
+ 	memcpy(&ht->p, params, sizeof(*params));
+ 	INIT_LIST_HEAD(&ht->walkers);
+ 
+ 	if (params->locks_mul)
+ 		ht->p.locks_mul = roundup_pow_of_two(params->locks_mul);
+ 	else
+ 		ht->p.locks_mul = BUCKET_LOCKS_PER_CPU;
+ 
+ 	tbl = bucket_table_alloc(ht, size);
++>>>>>>> f2dba9c6ff0d (rhashtable: Introduce rhashtable_walk_*)
  	if (tbl == NULL)
  		return -ENOMEM;
  
* Unmerged path include/linux/rhashtable.h
* Unmerged path lib/rhashtable.c
