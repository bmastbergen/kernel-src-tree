perf: Support high-order allocations for AUX space

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Alexander Shishkin <alexander.shishkin@linux.intel.com>
commit 0a4e38e64f5e91ce131cc42ee5bb3925377ec840
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/0a4e38e6.failed

Some pmus (such as BTS or Intel PT without multiple-entry ToPA capability)
don't support scatter-gather and will prefer larger contiguous areas for
their output regions.

This patch adds a new pmu capability to request higher order allocations.

	Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Kaixu Xia <kaixu.xia@linaro.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Mike Galbraith <efault@gmx.de>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Robert Richter <rric@kernel.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: acme@infradead.org
	Cc: adrian.hunter@intel.com
	Cc: kan.liang@intel.com
	Cc: markus.t.metzger@intel.com
	Cc: mathieu.poirier@linaro.org
Link: http://lkml.kernel.org/r/1421237903-181015-4-git-send-email-alexander.shishkin@linux.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 0a4e38e64f5e91ce131cc42ee5bb3925377ec840)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/perf_event.h
#	kernel/events/ring_buffer.c
diff --cc include/linux/perf_event.h
index 97ec7d6d934a,d5a4a8e95808..000000000000
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@@ -205,6 -173,8 +205,11 @@@ struct perf_event
   * pmu::capabilities flags
   */
  #define PERF_PMU_CAP_NO_INTERRUPT		0x01
++<<<<<<< HEAD
++=======
+ #define PERF_PMU_CAP_NO_NMI			0x02
+ #define PERF_PMU_CAP_AUX_NO_SG			0x04
++>>>>>>> 0a4e38e64f5e (perf: Support high-order allocations for AUX space)
  
  /**
   * struct pmu - generic performance monitoring unit
diff --cc kernel/events/ring_buffer.c
index eadb95ce7aac,ed0859e33b2f..000000000000
--- a/kernel/events/ring_buffer.c
+++ b/kernel/events/ring_buffer.c
@@@ -243,6 -243,123 +243,126 @@@ ring_buffer_init(struct ring_buffer *rb
  	spin_lock_init(&rb->event_lock);
  }
  
++<<<<<<< HEAD
++=======
+ #define PERF_AUX_GFP	(GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY)
+ 
+ static struct page *rb_alloc_aux_page(int node, int order)
+ {
+ 	struct page *page;
+ 
+ 	if (order > MAX_ORDER)
+ 		order = MAX_ORDER;
+ 
+ 	do {
+ 		page = alloc_pages_node(node, PERF_AUX_GFP, order);
+ 	} while (!page && order--);
+ 
+ 	if (page && order) {
+ 		/*
+ 		 * Communicate the allocation size to the driver
+ 		 */
+ 		split_page(page, order);
+ 		SetPagePrivate(page);
+ 		set_page_private(page, order);
+ 	}
+ 
+ 	return page;
+ }
+ 
+ static void rb_free_aux_page(struct ring_buffer *rb, int idx)
+ {
+ 	struct page *page = virt_to_page(rb->aux_pages[idx]);
+ 
+ 	ClearPagePrivate(page);
+ 	page->mapping = NULL;
+ 	__free_page(page);
+ }
+ 
+ int rb_alloc_aux(struct ring_buffer *rb, struct perf_event *event,
+ 		 pgoff_t pgoff, int nr_pages, int flags)
+ {
+ 	bool overwrite = !(flags & RING_BUFFER_WRITABLE);
+ 	int node = (event->cpu == -1) ? -1 : cpu_to_node(event->cpu);
+ 	int ret = -ENOMEM, max_order = 0;
+ 
+ 	if (!has_aux(event))
+ 		return -ENOTSUPP;
+ 
+ 	if (event->pmu->capabilities & PERF_PMU_CAP_AUX_NO_SG)
+ 		/*
+ 		 * We need to start with the max_order that fits in nr_pages,
+ 		 * not the other way around, hence ilog2() and not get_order.
+ 		 */
+ 		max_order = ilog2(nr_pages);
+ 
+ 	rb->aux_pages = kzalloc_node(nr_pages * sizeof(void *), GFP_KERNEL, node);
+ 	if (!rb->aux_pages)
+ 		return -ENOMEM;
+ 
+ 	rb->free_aux = event->pmu->free_aux;
+ 	for (rb->aux_nr_pages = 0; rb->aux_nr_pages < nr_pages;) {
+ 		struct page *page;
+ 		int last, order;
+ 
+ 		order = min(max_order, ilog2(nr_pages - rb->aux_nr_pages));
+ 		page = rb_alloc_aux_page(node, order);
+ 		if (!page)
+ 			goto out;
+ 
+ 		for (last = rb->aux_nr_pages + (1 << page_private(page));
+ 		     last > rb->aux_nr_pages; rb->aux_nr_pages++)
+ 			rb->aux_pages[rb->aux_nr_pages] = page_address(page++);
+ 	}
+ 
+ 	rb->aux_priv = event->pmu->setup_aux(event->cpu, rb->aux_pages, nr_pages,
+ 					     overwrite);
+ 	if (!rb->aux_priv)
+ 		goto out;
+ 
+ 	ret = 0;
+ 
+ 	/*
+ 	 * aux_pages (and pmu driver's private data, aux_priv) will be
+ 	 * referenced in both producer's and consumer's contexts, thus
+ 	 * we keep a refcount here to make sure either of the two can
+ 	 * reference them safely.
+ 	 */
+ 	atomic_set(&rb->aux_refcount, 1);
+ 
+ out:
+ 	if (!ret)
+ 		rb->aux_pgoff = pgoff;
+ 	else
+ 		rb_free_aux(rb);
+ 
+ 	return ret;
+ }
+ 
+ static void __rb_free_aux(struct ring_buffer *rb)
+ {
+ 	int pg;
+ 
+ 	if (rb->aux_priv) {
+ 		rb->free_aux(rb->aux_priv);
+ 		rb->free_aux = NULL;
+ 		rb->aux_priv = NULL;
+ 	}
+ 
+ 	for (pg = 0; pg < rb->aux_nr_pages; pg++)
+ 		rb_free_aux_page(rb, pg);
+ 
+ 	kfree(rb->aux_pages);
+ 	rb->aux_nr_pages = 0;
+ }
+ 
+ void rb_free_aux(struct ring_buffer *rb)
+ {
+ 	if (atomic_dec_and_test(&rb->aux_refcount))
+ 		__rb_free_aux(rb);
+ }
+ 
++>>>>>>> 0a4e38e64f5e (perf: Support high-order allocations for AUX space)
  #ifndef CONFIG_PERF_USE_VMALLOC
  
  /*
* Unmerged path include/linux/perf_event.h
* Unmerged path kernel/events/ring_buffer.c
