KVM: x86: disable MPX if host did not enable MPX XSAVE features

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit a87036add09283e6c4f4103a15c596c67b86ab86
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a87036ad.failed

When eager FPU is disabled, KVM will still see the MPX bit in CPUID and
presumably the MPX vmentry and vmexit controls.  However, it will not
be able to expose the MPX XSAVE features to the guest, because the guest's
accessible XSAVE features are always a subset of host_xcr0.

In this case, we should disable the MPX CPUID bit, the BNDCFGS MSR,
and the MPX vmentry and vmexit controls for nested virtualization.
It is then unnecessary to enable guest eager FPU if the guest has the
MPX CPUID bit set.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit a87036add09283e6c4f4103a15c596c67b86ab86)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/cpuid.c
#	arch/x86/kvm/cpuid.h
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/cpuid.c
index 76b72da25d9c,fa241d4fda98..000000000000
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@@ -51,8 -57,8 +58,13 @@@ u64 kvm_supported_xcr0(void
  {
  	u64 xcr0 = KVM_SUPPORTED_XCR0 & host_xcr0;
  
++<<<<<<< HEAD
 +	if (!kvm_x86_ops->mpx_supported())
 +		xcr0 &= ~(XSTATE_BNDREGS | XSTATE_BNDCSR);
++=======
+ 	if (!kvm_mpx_supported())
+ 		xcr0 &= ~(XFEATURE_MASK_BNDREGS | XFEATURE_MASK_BNDCSR);
++>>>>>>> a87036add092 (KVM: x86: disable MPX if host did not enable MPX XSAVE features)
  
  	return xcr0;
  }
@@@ -280,7 -302,8 +292,12 @@@ static inline int __do_cpuid_ent(struc
  #endif
  	unsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;
  	unsigned f_invpcid = kvm_x86_ops->invpcid_supported() ? F(INVPCID) : 0;
++<<<<<<< HEAD
 +	unsigned f_mpx = kvm_x86_ops->mpx_supported() ? F(MPX) : 0;
++=======
+ 	unsigned f_mpx = kvm_mpx_supported() ? F(MPX) : 0;
+ 	unsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;
++>>>>>>> a87036add092 (KVM: x86: disable MPX if host did not enable MPX XSAVE features)
  
  	/* cpuid 1.edx */
  	const u32 kvm_supported_word0_x86_features =
diff --cc arch/x86/kvm/cpuid.h
index c346c610bd63,66a6581724ad..000000000000
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@@ -2,8 -2,10 +2,9 @@@
  #define ARCH_X86_KVM_CPUID_H
  
  #include "x86.h"
 -#include <asm/cpu.h>
  
  int kvm_update_cpuid(struct kvm_vcpu *vcpu);
+ bool kvm_mpx_supported(void);
  struct kvm_cpuid_entry2 *kvm_find_cpuid_entry(struct kvm_vcpu *vcpu,
  					      u32 function, u32 index);
  int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
@@@ -120,11 -136,73 +121,82 @@@ static inline bool guest_cpuid_has_rtm(
  	return best && (best->ebx & bit(X86_FEATURE_RTM));
  }
  
++<<<<<<< HEAD
 +static inline bool guest_cpuid_has_mpx(struct kvm_vcpu *vcpu)
++=======
+ static inline bool guest_cpuid_has_pcommit(struct kvm_vcpu *vcpu)
++>>>>>>> a87036add092 (KVM: x86: disable MPX if host did not enable MPX XSAVE features)
  {
  	struct kvm_cpuid_entry2 *best;
  
  	best = kvm_find_cpuid_entry(vcpu, 7, 0);
++<<<<<<< HEAD
 +	return best && (best->ebx & bit(X86_FEATURE_MPX));
 +}
++=======
+ 	return best && (best->ebx & bit(X86_FEATURE_PCOMMIT));
+ }
+ 
+ static inline bool guest_cpuid_has_rdtscp(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_cpuid_entry2 *best;
+ 
+ 	best = kvm_find_cpuid_entry(vcpu, 0x80000001, 0);
+ 	return best && (best->edx & bit(X86_FEATURE_RDTSCP));
+ }
+ 
+ /*
+  * NRIPS is provided through cpuidfn 0x8000000a.edx bit 3
+  */
+ #define BIT_NRIPS	3
+ 
+ static inline bool guest_cpuid_has_nrips(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_cpuid_entry2 *best;
+ 
+ 	best = kvm_find_cpuid_entry(vcpu, 0x8000000a, 0);
+ 
+ 	/*
+ 	 * NRIPS is a scattered cpuid feature, so we can't use
+ 	 * X86_FEATURE_NRIPS here (X86_FEATURE_NRIPS would be bit
+ 	 * position 8, not 3).
+ 	 */
+ 	return best && (best->edx & bit(BIT_NRIPS));
+ }
+ #undef BIT_NRIPS
+ 
+ static inline int guest_cpuid_family(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_cpuid_entry2 *best;
+ 
+ 	best = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ 	if (!best)
+ 		return -1;
+ 
+ 	return x86_family(best->eax);
+ }
+ 
+ static inline int guest_cpuid_model(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_cpuid_entry2 *best;
+ 
+ 	best = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ 	if (!best)
+ 		return -1;
+ 
+ 	return x86_model(best->eax);
+ }
+ 
+ static inline int guest_cpuid_stepping(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_cpuid_entry2 *best;
+ 
+ 	best = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ 	if (!best)
+ 		return -1;
+ 
+ 	return x86_stepping(best->eax);
+ }
+ 
++>>>>>>> a87036add092 (KVM: x86: disable MPX if host did not enable MPX XSAVE features)
  #endif
diff --cc arch/x86/kvm/vmx.c
index 843549a0fe2a,e512aa7ed874..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -810,7 -861,7 +810,11 @@@ static unsigned long nested_ept_get_cr3
  static u64 construct_eptp(unsigned long root_hpa);
  static void kvm_cpu_vmxon(u64 addr);
  static void kvm_cpu_vmxoff(void);
++<<<<<<< HEAD
 +static bool vmx_mpx_supported(void);
++=======
+ static bool vmx_xsaves_supported(void);
++>>>>>>> a87036add092 (KVM: x86: disable MPX if host did not enable MPX XSAVE features)
  static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr);
  static void vmx_set_segment(struct kvm_vcpu *vcpu,
  			    struct kvm_segment *var, int seg);
@@@ -2400,11 -2594,12 +2404,16 @@@ static __init void nested_vmx_setup_ctl
  		VM_EXIT_LOAD_IA32_EFER | VM_EXIT_SAVE_IA32_EFER |
  		VM_EXIT_SAVE_VMX_PREEMPTION_TIMER | VM_EXIT_ACK_INTR_ON_EXIT;
  
++<<<<<<< HEAD
 +	if (vmx_mpx_supported())
 +		nested_vmx_exit_ctls_high |= VM_EXIT_CLEAR_BNDCFGS;
++=======
+ 	if (kvm_mpx_supported())
+ 		vmx->nested.nested_vmx_exit_ctls_high |= VM_EXIT_CLEAR_BNDCFGS;
++>>>>>>> a87036add092 (KVM: x86: disable MPX if host did not enable MPX XSAVE features)
  
  	/* We support free control of debug control saving. */
 -	vmx->nested.nested_vmx_true_exit_ctls_low =
 -		vmx->nested.nested_vmx_exit_ctls_low &
 +	nested_vmx_true_exit_ctls_low = nested_vmx_exit_ctls_low &
  		~VM_EXIT_SAVE_DEBUG_CONTROLS;
  
  	/* entry controls */
@@@ -2416,13 -2613,14 +2425,20 @@@
  		VM_ENTRY_IA32E_MODE |
  #endif
  		VM_ENTRY_LOAD_IA32_PAT;
++<<<<<<< HEAD
 +	nested_vmx_entry_ctls_high |= (VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR |
 +				       VM_ENTRY_LOAD_IA32_EFER);
 +	if (vmx_mpx_supported())
 +		nested_vmx_entry_ctls_high |= VM_ENTRY_LOAD_BNDCFGS;
++=======
+ 	vmx->nested.nested_vmx_entry_ctls_high |=
+ 		(VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR | VM_ENTRY_LOAD_IA32_EFER);
+ 	if (kvm_mpx_supported())
+ 		vmx->nested.nested_vmx_entry_ctls_high |= VM_ENTRY_LOAD_BNDCFGS;
++>>>>>>> a87036add092 (KVM: x86: disable MPX if host did not enable MPX XSAVE features)
  
  	/* We support free control of debug control loading. */
 -	vmx->nested.nested_vmx_true_entry_ctls_low =
 -		vmx->nested.nested_vmx_entry_ctls_low &
 +	nested_vmx_true_entry_ctls_low = nested_vmx_entry_ctls_low &
  		~VM_ENTRY_LOAD_DEBUG_CONTROLS;
  
  	/* cpu-based controls */
@@@ -9516,8 -10264,10 +9532,8 @@@ static void prepare_vmcs12(struct kvm_v
  	vmcs12->guest_sysenter_cs = vmcs_read32(GUEST_SYSENTER_CS);
  	vmcs12->guest_sysenter_esp = vmcs_readl(GUEST_SYSENTER_ESP);
  	vmcs12->guest_sysenter_eip = vmcs_readl(GUEST_SYSENTER_EIP);
- 	if (vmx_mpx_supported())
+ 	if (kvm_mpx_supported())
  		vmcs12->guest_bndcfgs = vmcs_read64(GUEST_BNDCFGS);
 -	if (nested_cpu_has_xsaves(vmcs12))
 -		vmcs12->xss_exit_bitmap = vmcs_read64(XSS_EXIT_BITMAP);
  
  	/* update exit information fields: */
  
* Unmerged path arch/x86/kvm/cpuid.c
* Unmerged path arch/x86/kvm/cpuid.h
* Unmerged path arch/x86/kvm/vmx.c
