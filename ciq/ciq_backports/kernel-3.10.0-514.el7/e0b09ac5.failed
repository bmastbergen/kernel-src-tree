IB/hfi1: Make the cache handler own its rb tree root

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dean Luick <dean.luick@intel.com>
commit e0b09ac55d51bb9bf6a4a320bf4029e40bdabd6c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e0b09ac5.failed

The objects which use cache handling should reference their own handler
object not the internal data structure it uses to track the nodes.

Have the "users" of the mmu notifier code pass opaque objects which can
then be properly used in the mmu callbacks depending on the owners needs.

This patch has the additional benefit that operations no longer require a
look up in a list to find the handlers.

	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Signed-off-by: Dean Luick <dean.luick@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit e0b09ac55d51bb9bf6a4a320bf4029e40bdabd6c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/mmu_rb.c
#	drivers/infiniband/hw/hfi1/user_exp_rcv.c
#	drivers/staging/hfi1/debugfs.h
#	drivers/staging/hfi1/hfi.h
#	drivers/staging/hfi1/user_sdma.c
#	drivers/staging/hfi1/user_sdma.h
diff --cc drivers/staging/hfi1/debugfs.h
index 92d6fe146714,2cedfbe2189e..000000000000
--- a/drivers/staging/hfi1/debugfs.h
+++ b/drivers/staging/hfi1/debugfs.h
@@@ -49,30 -44,36 +49,52 @@@
   * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
   *
   */
 -#ifndef _HFI1_MMU_RB_H
 -#define _HFI1_MMU_RB_H
  
 -#include "hfi.h"
 +struct hfi1_ibdev;
 +#ifdef CONFIG_DEBUG_FS
 +void hfi1_dbg_ibdev_init(struct hfi1_ibdev *ibd);
 +void hfi1_dbg_ibdev_exit(struct hfi1_ibdev *ibd);
 +void hfi1_dbg_init(void);
 +void hfi1_dbg_exit(void);
 +#else
 +static inline void hfi1_dbg_ibdev_init(struct hfi1_ibdev *ibd)
 +{
 +}
  
 -struct mmu_rb_node {
 -	unsigned long addr;
 -	unsigned long len;
 -	unsigned long __last;
 -	struct rb_node node;
 -};
 +void hfi1_dbg_ibdev_exit(struct hfi1_ibdev *ibd)
 +{
 +}
  
++<<<<<<< HEAD:drivers/staging/hfi1/debugfs.h
 +void hfi1_dbg_init(void)
 +{
 +}
 +
 +void hfi1_dbg_exit(void)
 +{
 +}
++=======
+ struct mmu_rb_ops {
+ 	bool (*filter)(struct mmu_rb_node *node, unsigned long addr,
+ 		       unsigned long len);
+ 	int (*insert)(void *ops_arg, struct mmu_rb_node *mnode);
+ 	void (*remove)(void *ops_arg, struct mmu_rb_node *mnode,
+ 		       struct mm_struct *mm);
+ 	int (*invalidate)(void *ops_arg, struct mmu_rb_node *node);
+ };
+ 
+ int hfi1_mmu_rb_register(void *ops_arg, struct mm_struct *mm,
+ 			 struct mmu_rb_ops *ops,
+ 			 struct mmu_rb_handler **handler);
+ void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler);
+ int hfi1_mmu_rb_insert(struct mmu_rb_handler *handler,
+ 		       struct mmu_rb_node *mnode);
+ void hfi1_mmu_rb_remove(struct mmu_rb_handler *handler,
+ 			struct mmu_rb_node *mnode);
+ struct mmu_rb_node *hfi1_mmu_rb_extract(struct mmu_rb_handler *handler,
+ 					unsigned long addr, unsigned long len);
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/mmu_rb.h
 +
 +#endif
  
 -#endif /* _HFI1_MMU_RB_H */
 +#endif                          /* _HFI1_DEBUGFS_H */
diff --cc drivers/staging/hfi1/hfi.h
index 1a5d37c2e56a,ba9083602cbd..000000000000
--- a/drivers/staging/hfi1/hfi.h
+++ b/drivers/staging/hfi1/hfi.h
@@@ -1144,6 -1184,10 +1144,13 @@@ struct hfi1_devdata 
  #define PT_EAGER    1
  #define PT_INVALID  2
  
++<<<<<<< HEAD:drivers/staging/hfi1/hfi.h
++=======
+ struct tid_rb_node;
+ struct mmu_rb_node;
+ struct mmu_rb_handler;
+ 
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/hfi.h
  /* Private data for file operations */
  struct hfi1_filedata {
  	struct hfi1_ctxtdata *uctxt;
@@@ -1152,8 -1196,9 +1159,14 @@@
  	struct hfi1_user_sdma_pkt_q *pq;
  	/* for cpu affinity; -1 if none */
  	int rec_cpu_num;
++<<<<<<< HEAD:drivers/staging/hfi1/hfi.h
 +	struct mmu_notifier mn;
 +	struct rb_root tid_rb_root;
++=======
+ 	u32 tid_n_pinned;
+ 	struct mmu_rb_handler *handler;
+ 	struct tid_rb_node **entry_to_rb;
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/hfi.h
  	spinlock_t tid_lock; /* protect tid_[limit,used] counters */
  	u32 tid_limit;
  	u32 tid_used;
diff --cc drivers/staging/hfi1/user_sdma.c
index 47c9c87af47a,8be095e1a538..000000000000
--- a/drivers/staging/hfi1/user_sdma.c
+++ b/drivers/staging/hfi1/user_sdma.c
@@@ -297,6 -304,18 +297,21 @@@ static int defer_packet_queue
  	struct sdma_txreq *,
  	unsigned seq);
  static void activate_packet_queue(struct iowait *, int);
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ static bool sdma_rb_filter(struct mmu_rb_node *, unsigned long, unsigned long);
+ static int sdma_rb_insert(void *, struct mmu_rb_node *);
+ static void sdma_rb_remove(void *, struct mmu_rb_node *,
+ 			   struct mm_struct *);
+ static int sdma_rb_invalidate(void *, struct mmu_rb_node *);
+ 
+ static struct mmu_rb_ops sdma_rb_ops = {
+ 	.filter = sdma_rb_filter,
+ 	.insert = sdma_rb_insert,
+ 	.remove = sdma_rb_remove,
+ 	.invalidate = sdma_rb_invalidate
+ };
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.c
  
  static int defer_packet_queue(
  	struct sdma_engine *sde,
@@@ -386,9 -410,12 +401,15 @@@ int hfi1_user_sdma_alloc_queues(struct 
  	pq->state = SDMA_PKT_Q_INACTIVE;
  	atomic_set(&pq->n_reqs, 0);
  	init_waitqueue_head(&pq->wait);
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 	INIT_LIST_HEAD(&pq->evict);
+ 	spin_lock_init(&pq->evict_lock);
+ 	pq->mm = fd->mm;
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.c
  
  	iowait_init(&pq->busy, 0, NULL, defer_packet_queue,
 -		    activate_packet_queue, NULL);
 +		    activate_packet_queue);
  	pq->reqidx = 0;
  	snprintf(buf, 64, "txreq-kmem-cache-%u-%u-%u", dd->unit, uctxt->ctxt,
  		 fd->subctxt);
@@@ -415,6 -442,12 +436,15 @@@
  	cq->nentries = hfi1_sdma_comp_ring_size;
  	fd->cq = cq;
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 	ret = hfi1_mmu_rb_register(pq, pq->mm, &sdma_rb_ops, &pq->handler);
+ 	if (ret) {
+ 		dd_dev_err(dd, "Failed to register with MMU %d", ret);
+ 		goto done;
+ 	}
+ 
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.c
  	spin_lock_irqsave(&uctxt->sdma_qlock, flags);
  	list_add(&pq->list, &uctxt->sdma_queues);
  	spin_unlock_irqrestore(&uctxt->sdma_qlock, flags);
@@@ -1069,40 -1124,145 +1100,108 @@@ static inline int num_user_pages(const 
  	return 1 + ((epage - spage) >> PAGE_SHIFT);
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +static int pin_vector_pages(struct user_sdma_request *req,
 +			    struct user_sdma_iovec *iovec) {
 +	int pinned, npages;
++=======
+ static u32 sdma_cache_evict(struct hfi1_user_sdma_pkt_q *pq, u32 npages)
+ {
+ 	u32 cleared = 0;
+ 	struct sdma_mmu_node *node, *ptr;
+ 	struct list_head to_evict = LIST_HEAD_INIT(to_evict);
+ 
+ 	spin_lock(&pq->evict_lock);
+ 	list_for_each_entry_safe_reverse(node, ptr, &pq->evict, list) {
+ 		/* Make sure that no one is still using the node. */
+ 		if (!atomic_read(&node->refcount)) {
+ 			set_bit(SDMA_CACHE_NODE_EVICT, &node->flags);
+ 			list_del_init(&node->list);
+ 			list_add(&node->list, &to_evict);
+ 			cleared += node->npages;
+ 			if (cleared >= npages)
+ 				break;
+ 		}
+ 	}
+ 	spin_unlock(&pq->evict_lock);
+ 
+ 	list_for_each_entry_safe(node, ptr, &to_evict, list)
+ 		hfi1_mmu_rb_remove(pq->handler, &node->rb);
+ 
+ 	return cleared;
+ }
+ 
+ static int pin_vector_pages(struct user_sdma_request *req,
+ 			    struct user_sdma_iovec *iovec)
+ {
+ 	int ret = 0, pinned, npages, cleared;
+ 	struct page **pages;
+ 	struct hfi1_user_sdma_pkt_q *pq = req->pq;
+ 	struct sdma_mmu_node *node = NULL;
+ 	struct mmu_rb_node *rb_node;
+ 
+ 	rb_node = hfi1_mmu_rb_extract(pq->handler,
+ 				      (unsigned long)iovec->iov.iov_base,
+ 				      iovec->iov.iov_len);
+ 	if (rb_node && !IS_ERR(rb_node))
+ 		node = container_of(rb_node, struct sdma_mmu_node, rb);
+ 	else
+ 		rb_node = NULL;
+ 
+ 	if (!node) {
+ 		node = kzalloc(sizeof(*node), GFP_KERNEL);
+ 		if (!node)
+ 			return -ENOMEM;
+ 
+ 		node->rb.addr = (unsigned long)iovec->iov.iov_base;
+ 		node->pq = pq;
+ 		atomic_set(&node->refcount, 0);
+ 		INIT_LIST_HEAD(&node->list);
+ 	}
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.c
  
  	npages = num_user_pages(&iovec->iov);
 -	if (node->npages < npages) {
 -		pages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);
 -		if (!pages) {
 -			SDMA_DBG(req, "Failed page array alloc");
 -			ret = -ENOMEM;
 -			goto bail;
 -		}
 -		memcpy(pages, node->pages, node->npages * sizeof(*pages));
 +	iovec->pages = kcalloc(npages, sizeof(*iovec->pages), GFP_KERNEL);
 +	if (!iovec->pages) {
 +		SDMA_DBG(req, "Failed page array alloc");
 +		return -ENOMEM;
 +	}
  
 -		npages -= node->npages;
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +	pinned = hfi1_acquire_user_pages((unsigned long)iovec->iov.iov_base,
 +					 npages, 0, iovec->pages);
  
 -		/*
 -		 * If rb_node is NULL, it means that this is brand new node
 -		 * and, therefore not on the eviction list.
 -		 * If, however, the rb_node is non-NULL, it means that the
 -		 * node is already in RB tree and, therefore on the eviction
 -		 * list (nodes are unconditionally inserted in the eviction
 -		 * list). In that case, we have to remove the node prior to
 -		 * calling the eviction function in order to prevent it from
 -		 * freeing this node.
 -		 */
 -		if (rb_node) {
 -			spin_lock(&pq->evict_lock);
 -			list_del_init(&node->list);
 -			spin_unlock(&pq->evict_lock);
 -		}
 -retry:
 -		if (!hfi1_can_pin_pages(pq->dd, pq->mm, pq->n_locked, npages)) {
 -			cleared = sdma_cache_evict(pq, npages);
 -			if (cleared >= npages)
 -				goto retry;
 -		}
 -		pinned = hfi1_acquire_user_pages(pq->mm,
 -			((unsigned long)iovec->iov.iov_base +
 -			 (node->npages * PAGE_SIZE)), npages, 0,
 -			pages + node->npages);
 -		if (pinned < 0) {
 -			kfree(pages);
 -			ret = pinned;
 -			goto bail;
 -		}
 -		if (pinned != npages) {
 -			unpin_vector_pages(pq->mm, pages, node->npages,
 -					   pinned);
 -			ret = -EFAULT;
 -			goto bail;
 -		}
 -		kfree(node->pages);
 -		node->rb.len = iovec->iov.iov_len;
 -		node->pages = pages;
 -		node->npages += pinned;
 -		npages = node->npages;
 -		spin_lock(&pq->evict_lock);
 -		list_add(&node->list, &pq->evict);
 -		pq->n_locked += pinned;
 -		spin_unlock(&pq->evict_lock);
 -	}
 -	iovec->pages = node->pages;
 -	iovec->npages = npages;
 -	iovec->node = node;
 +	if (pinned < 0)
 +		return pinned;
  
 +	iovec->npages = pinned;
 +	if (pinned != npages) {
 +		SDMA_DBG(req, "Failed to pin pages (%d/%u)", pinned, npages);
 +		unpin_vector_pages(iovec);
 +		return -EFAULT;
++=======
+ 	ret = hfi1_mmu_rb_insert(req->pq->handler, &node->rb);
+ 	if (ret) {
+ 		spin_lock(&pq->evict_lock);
+ 		if (!list_empty(&node->list))
+ 			list_del(&node->list);
+ 		pq->n_locked -= node->npages;
+ 		spin_unlock(&pq->evict_lock);
+ 		iovec->node = NULL;
+ 		goto bail;
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.c
  	}
  	return 0;
 -bail:
 -	if (rb_node)
 -		unpin_vector_pages(pq->mm, node->pages, 0, node->npages);
 -	kfree(node);
 -	return ret;
  }
  
 -static void unpin_vector_pages(struct mm_struct *mm, struct page **pages,
 -			       unsigned start, unsigned npages)
 +static void unpin_vector_pages(struct user_sdma_iovec *iovec)
  {
 -	hfi1_release_user_pages(mm, pages + start, npages, false);
 -	kfree(pages);
 +	hfi1_release_user_pages(iovec->pages, iovec->npages, 0);
 +
 +	kfree(iovec->pages);
 +	iovec->pages = NULL;
 +	iovec->npages = 0;
 +	iovec->offset = 0;
  }
  
  static int check_header_template(struct user_sdma_request *req,
@@@ -1460,15 -1602,24 +1559,29 @@@ static void user_sdma_free_request(stru
  			kmem_cache_free(req->pq->txreq_cache, tx);
  		}
  	}
 -	if (req->data_iovs) {
 -		struct sdma_mmu_node *node;
 +	if (req->data_iovs && unpin) {
  		int i;
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +		for (i = 0; i < req->data_iovs; i++)
 +			if (req->iovs[i].npages && req->iovs[i].pages)
 +				unpin_vector_pages(&req->iovs[i]);
++=======
+ 		for (i = 0; i < req->data_iovs; i++) {
+ 			node = req->iovs[i].node;
+ 			if (!node)
+ 				continue;
+ 
+ 			if (unpin)
+ 				hfi1_mmu_rb_remove(req->pq->handler,
+ 						   &node->rb);
+ 			else
+ 				atomic_dec(&node->refcount);
+ 		}
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.c
  	}
  	kfree(req->tids);
 -	clear_bit(req->info.comp_idx, req->pq->req_in_use);
 +	clear_bit(SDMA_REQ_IN_USE, &req->flags);
  }
  
  static inline void set_comp_state(struct hfi1_user_sdma_pkt_q *pq,
@@@ -1484,3 -1635,69 +1597,72 @@@
  	trace_hfi1_sdma_user_completion(pq->dd, pq->ctxt, pq->subctxt,
  					idx, state, ret);
  }
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 
+ static bool sdma_rb_filter(struct mmu_rb_node *node, unsigned long addr,
+ 			   unsigned long len)
+ {
+ 	return (bool)(node->addr == addr);
+ }
+ 
+ static int sdma_rb_insert(void *arg, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	atomic_inc(&node->refcount);
+ 	return 0;
+ }
+ 
+ static void sdma_rb_remove(void *arg, struct mmu_rb_node *mnode,
+ 			   struct mm_struct *mm)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	spin_lock(&node->pq->evict_lock);
+ 	/*
+ 	 * We've been called by the MMU notifier but this node has been
+ 	 * scheduled for eviction. The eviction function will take care
+ 	 * of freeing this node.
+ 	 * We have to take the above lock first because we are racing
+ 	 * against the setting of the bit in the eviction function.
+ 	 */
+ 	if (mm && test_bit(SDMA_CACHE_NODE_EVICT, &node->flags)) {
+ 		spin_unlock(&node->pq->evict_lock);
+ 		return;
+ 	}
+ 
+ 	if (!list_empty(&node->list))
+ 		list_del(&node->list);
+ 	node->pq->n_locked -= node->npages;
+ 	spin_unlock(&node->pq->evict_lock);
+ 
+ 	/*
+ 	 * If mm is set, we are being called by the MMU notifier and we
+ 	 * should not pass a mm_struct to unpin_vector_page(). This is to
+ 	 * prevent a deadlock when hfi1_release_user_pages() attempts to
+ 	 * take the mmap_sem, which the MMU notifier has already taken.
+ 	 */
+ 	unpin_vector_pages(mm ? NULL : current->mm, node->pages, 0,
+ 			   node->npages);
+ 	/*
+ 	 * If called by the MMU notifier, we have to adjust the pinned
+ 	 * page count ourselves.
+ 	 */
+ 	if (mm)
+ 		mm->pinned_vm -= node->npages;
+ 	kfree(node);
+ }
+ 
+ static int sdma_rb_invalidate(void *arg, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	if (!atomic_read(&node->refcount))
+ 		return 1;
+ 	return 0;
+ }
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.c
diff --cc drivers/staging/hfi1/user_sdma.h
index 7ebbc4634989,bcdc9e8ae1f0..000000000000
--- a/drivers/staging/hfi1/user_sdma.h
+++ b/drivers/staging/hfi1/user_sdma.h
@@@ -78,6 -68,11 +78,14 @@@ struct hfi1_user_sdma_pkt_q 
  	unsigned state;
  	wait_queue_head_t wait;
  	unsigned long unpinned;
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.h
++=======
+ 	struct mmu_rb_handler *handler;
+ 	u32 n_locked;
+ 	struct list_head evict;
+ 	spinlock_t evict_lock; /* protect evict and n_locked */
+ 	struct mm_struct *mm;
++>>>>>>> e0b09ac55d51 (IB/hfi1: Make the cache handler own its rb tree root):drivers/infiniband/hw/hfi1/user_sdma.h
  };
  
  struct hfi1_user_sdma_comp_q {
* Unmerged path drivers/infiniband/hw/hfi1/mmu_rb.c
* Unmerged path drivers/infiniband/hw/hfi1/user_exp_rcv.c
* Unmerged path drivers/infiniband/hw/hfi1/mmu_rb.c
* Unmerged path drivers/infiniband/hw/hfi1/user_exp_rcv.c
* Unmerged path drivers/staging/hfi1/debugfs.h
* Unmerged path drivers/staging/hfi1/hfi.h
* Unmerged path drivers/staging/hfi1/user_sdma.c
* Unmerged path drivers/staging/hfi1/user_sdma.h
