xprtrdma: Port to new memory registration API

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Sagi Grimberg <sagig@mellanox.com>
commit 4143f34e01e9cdf1882f98c54d9073e4de8c28fb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/4143f34e.failed

Instead of maintaining a fastreg page list, keep an sg table
and convert an array of pages to a sg list. Then call ib_map_mr_sg
and construct ib_reg_wr.

	Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
	Acked-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Steve Wise <swise@opengridcomputing.com>
	Tested-by: Selvin Xavier <selvin.xavier@avagotech.com>
	Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 4143f34e01e9cdf1882f98c54d9073e4de8c28fb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/frwr_ops.c
diff --cc net/sunrpc/xprtrdma/frwr_ops.c
index 944b72ffed6c,a1434447b0d6..000000000000
--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@@ -248,78 -310,103 +252,159 @@@ frwr_op_map(struct rpcrdma_xprt *r_xprt
  	    int nsegs, bool writing)
  {
  	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 -	struct ib_device *device = ia->ri_device;
 +	struct ib_device *device = ia->ri_id->device;
  	enum dma_data_direction direction = rpcrdma_data_dir(writing);
  	struct rpcrdma_mr_seg *seg1 = seg;
++<<<<<<< HEAD
 +	struct rpcrdma_mw *mw = seg1->rl_mw;
 +	struct rpcrdma_frmr *frmr = &mw->r.frmr;
 +	struct ib_mr *mr = frmr->fr_mr;
 +	struct ib_send_wr fastreg_wr, *bad_wr;
++=======
+ 	struct rpcrdma_mw *mw;
+ 	struct rpcrdma_frmr *frmr;
+ 	struct ib_mr *mr;
+ 	struct ib_reg_wr reg_wr;
+ 	struct ib_send_wr *bad_wr;
+ 	int rc, i, n, dma_nents;
++>>>>>>> 4143f34e01e9 (xprtrdma: Port to new memory registration API)
  	u8 key;
- 	int len, pageoff;
- 	int i, rc;
- 	int seg_len;
- 	u64 pa;
- 	int page_no;
  
++<<<<<<< HEAD
 +	pageoff = offset_in_page(seg1->mr_offset);
 +	seg1->mr_offset -= pageoff;	/* start of page */
 +	seg1->mr_len += pageoff;
 +	len = -pageoff;
 +	if (nsegs > ia->ri_max_frmr_depth)
 +		nsegs = ia->ri_max_frmr_depth;
 +	for (page_no = i = 0; i < nsegs;) {
 +		rpcrdma_map_one(device, seg, direction);
 +		pa = seg->mr_dma;
 +		for (seg_len = seg->mr_len; seg_len > 0; seg_len -= PAGE_SIZE) {
 +			frmr->fr_pgl->page_list[page_no++] = pa;
 +			pa += PAGE_SIZE;
 +		}
 +		len += seg->mr_len;
++=======
+ 	mw = seg1->rl_mw;
+ 	seg1->rl_mw = NULL;
+ 	do {
+ 		if (mw)
+ 			__frwr_queue_recovery(mw);
+ 		mw = rpcrdma_get_mw(r_xprt);
+ 		if (!mw)
+ 			return -ENOMEM;
+ 	} while (mw->r.frmr.fr_state != FRMR_IS_INVALID);
+ 	frmr = &mw->r.frmr;
+ 	frmr->fr_state = FRMR_IS_VALID;
+ 	mr = frmr->fr_mr;
+ 
+ 	if (nsegs > ia->ri_max_frmr_depth)
+ 		nsegs = ia->ri_max_frmr_depth;
+ 
+ 	for (i = 0; i < nsegs;) {
+ 		if (seg->mr_page)
+ 			sg_set_page(&frmr->sg[i],
+ 				    seg->mr_page,
+ 				    seg->mr_len,
+ 				    offset_in_page(seg->mr_offset));
+ 		else
+ 			sg_set_buf(&frmr->sg[i], seg->mr_offset,
+ 				   seg->mr_len);
+ 
++>>>>>>> 4143f34e01e9 (xprtrdma: Port to new memory registration API)
  		++seg;
  		++i;
+ 
  		/* Check for holes */
  		if ((i < nsegs && offset_in_page(seg->mr_offset)) ||
  		    offset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))
  			break;
  	}
- 	dprintk("RPC:       %s: Using frmr %p to map %d segments (%d bytes)\n",
- 		__func__, mw, i, len);
+ 	frmr->sg_nents = i;
+ 
+ 	dma_nents = ib_dma_map_sg(device, frmr->sg, frmr->sg_nents, direction);
+ 	if (!dma_nents) {
+ 		pr_err("RPC:       %s: failed to dma map sg %p sg_nents %u\n",
+ 		       __func__, frmr->sg, frmr->sg_nents);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	n = ib_map_mr_sg(mr, frmr->sg, frmr->sg_nents, PAGE_SIZE);
+ 	if (unlikely(n != frmr->sg_nents)) {
+ 		pr_err("RPC:       %s: failed to map mr %p (%u/%u)\n",
+ 		       __func__, frmr->fr_mr, n, frmr->sg_nents);
+ 		rc = n < 0 ? n : -EINVAL;
+ 		goto out_senderr;
+ 	}
  
+ 	dprintk("RPC:       %s: Using frmr %p to map %u segments (%u bytes)\n",
+ 		__func__, mw, frmr->sg_nents, mr->length);
+ 
++<<<<<<< HEAD
 +	frmr->fr_state = FRMR_IS_VALID;
 +
 +	memset(&fastreg_wr, 0, sizeof(fastreg_wr));
 +	fastreg_wr.wr_id = (unsigned long)(void *)mw;
 +	fastreg_wr.opcode = IB_WR_FAST_REG_MR;
 +	fastreg_wr.wr.fast_reg.iova_start = seg1->mr_dma + pageoff;
 +	fastreg_wr.wr.fast_reg.page_list = frmr->fr_pgl;
 +	fastreg_wr.wr.fast_reg.page_shift = PAGE_SHIFT;
 +	fastreg_wr.wr.fast_reg.page_list_len = page_no;
 +	fastreg_wr.wr.fast_reg.length = len;
 +	fastreg_wr.wr.fast_reg.access_flags = writing ?
 +				IB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :
 +				IB_ACCESS_REMOTE_READ;
 +	key = (u8)(mr->rkey & 0x000000FF);
 +	ib_update_fast_reg_key(mr, ++key);
 +	fastreg_wr.wr.fast_reg.rkey = mr->rkey;
 +
 +	DECR_CQCOUNT(&r_xprt->rx_ep);
 +	rc = ib_post_send(ia->ri_id->qp, &fastreg_wr, &bad_wr);
 +	if (rc)
 +		goto out_senderr;
 +
++=======
+ 	key = (u8)(mr->rkey & 0x000000FF);
+ 	ib_update_fast_reg_key(mr, ++key);
+ 
+ 	reg_wr.wr.next = NULL;
+ 	reg_wr.wr.opcode = IB_WR_REG_MR;
+ 	reg_wr.wr.wr_id = (uintptr_t)mw;
+ 	reg_wr.wr.num_sge = 0;
+ 	reg_wr.wr.send_flags = 0;
+ 	reg_wr.mr = mr;
+ 	reg_wr.key = mr->rkey;
+ 	reg_wr.access = writing ?
+ 			IB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :
+ 			IB_ACCESS_REMOTE_READ;
+ 
+ 	DECR_CQCOUNT(&r_xprt->rx_ep);
+ 	rc = ib_post_send(ia->ri_id->qp, &reg_wr.wr, &bad_wr);
+ 	if (rc)
+ 		goto out_senderr;
+ 
+ 	seg1->mr_dir = direction;
+ 	seg1->rl_mw = mw;
++>>>>>>> 4143f34e01e9 (xprtrdma: Port to new memory registration API)
  	seg1->mr_rkey = mr->rkey;
- 	seg1->mr_base = seg1->mr_dma + pageoff;
- 	seg1->mr_nsegs = i;
- 	seg1->mr_len = len;
- 	return i;
+ 	seg1->mr_base = mr->iova;
+ 	seg1->mr_nsegs = frmr->sg_nents;
+ 	seg1->mr_len = mr->length;
+ 
+ 	return frmr->sg_nents;
  
  out_senderr:
  	dprintk("RPC:       %s: ib_post_send status %i\n", __func__, rc);
++<<<<<<< HEAD
 +	ib_update_fast_reg_key(mr, --key);
 +	frmr->fr_state = FRMR_IS_INVALID;
 +	while (i--)
 +		rpcrdma_unmap_one(device, --seg);
++=======
+ 	ib_dma_unmap_sg(device, frmr->sg, dma_nents, direction);
+ 	__frwr_queue_recovery(mw);
++>>>>>>> 4143f34e01e9 (xprtrdma: Port to new memory registration API)
  	return rc;
  }
  
@@@ -331,22 -418,24 +416,41 @@@ frwr_op_unmap(struct rpcrdma_xprt *r_xp
  {
  	struct rpcrdma_mr_seg *seg1 = seg;
  	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
++<<<<<<< HEAD
++=======
+ 	struct rpcrdma_mw *mw = seg1->rl_mw;
+ 	struct rpcrdma_frmr *frmr = &mw->r.frmr;
++>>>>>>> 4143f34e01e9 (xprtrdma: Port to new memory registration API)
  	struct ib_send_wr invalidate_wr, *bad_wr;
  	int rc, nsegs = seg->mr_nsegs;
 +	struct ib_device *device;
  
++<<<<<<< HEAD
 +	seg1->rl_mw->r.frmr.fr_state = FRMR_IS_INVALID;
++=======
+ 	dprintk("RPC:       %s: FRMR %p\n", __func__, mw);
+ 
+ 	seg1->rl_mw = NULL;
+ 	frmr->fr_state = FRMR_IS_INVALID;
++>>>>>>> 4143f34e01e9 (xprtrdma: Port to new memory registration API)
  
  	memset(&invalidate_wr, 0, sizeof(invalidate_wr));
 -	invalidate_wr.wr_id = (unsigned long)(void *)mw;
 +	invalidate_wr.wr_id = (unsigned long)(void *)seg1->rl_mw;
  	invalidate_wr.opcode = IB_WR_LOCAL_INV;
++<<<<<<< HEAD
 +	invalidate_wr.ex.invalidate_rkey = seg1->rl_mw->r.frmr.fr_mr->rkey;
 +	DECR_CQCOUNT(&r_xprt->rx_ep);
 +
++=======
+ 	invalidate_wr.ex.invalidate_rkey = frmr->fr_mr->rkey;
+ 	DECR_CQCOUNT(&r_xprt->rx_ep);
+ 
+ 	ib_dma_unmap_sg(ia->ri_device, frmr->sg, frmr->sg_nents, seg1->mr_dir);
++>>>>>>> 4143f34e01e9 (xprtrdma: Port to new memory registration API)
  	read_lock(&ia->ri_qplock);
 +	device = ia->ri_id->device;
 +	while (seg1->mr_nsegs--)
 +		rpcrdma_unmap_one(device, seg++);
  	rc = ib_post_send(ia->ri_id->qp, &invalidate_wr, &bad_wr);
  	read_unlock(&ia->ri_qplock);
  	if (rc)
* Unmerged path net/sunrpc/xprtrdma/frwr_ops.c
diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b2d2c86a7023..d1db55300bd6 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -195,7 +195,8 @@ enum rpcrdma_frmr_state {
 };
 
 struct rpcrdma_frmr {
-	struct ib_fast_reg_page_list	*fr_pgl;
+	struct scatterlist		*sg;
+	int				sg_nents;
 	struct ib_mr			*fr_mr;
 	enum rpcrdma_frmr_state		fr_state;
 	struct work_struct		fr_work;
