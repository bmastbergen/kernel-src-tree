time: Avoid signed overflow in timekeeping_get_ns()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author David Gibson <david@gibson.dropbear.id.au>
commit 35a4933a895927990772ae96fdcfd2f806929ee2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/35a4933a.failed

1e75fa8 "time: Condense timekeeper.xtime into xtime_sec" replaced a call to
clocksource_cyc2ns() from timekeeping_get_ns() with an open-coded version
of the same logic to avoid keeping a semi-redundant struct timespec
in struct timekeeper.

However, the commit also introduced a subtle semantic change - where
clocksource_cyc2ns() uses purely unsigned math, the new version introduces
a signed temporary, meaning that if (delta * tk->mult) has a 63-bit
overflow the following shift will still give a negative result.  The
choice of 'maxsec' in __clocksource_updatefreq_scale() means this will
generally happen if there's a ~10 minute pause in examining the
clocksource.

This can be triggered on a powerpc KVM guest by stopping it from qemu for
a bit over 10 minutes.  After resuming time has jumped backwards several
minutes causing numerous problems (jiffies does not advance, msleep()s can
be extended by minutes..).  It doesn't happen on x86 KVM guests, because
the guest TSC is effectively frozen while the guest is stopped, which is
not the case for the powerpc timebase.

Obviously an unsigned (64 bit) overflow will only take twice as long as a
signed, 63-bit overflow.  I don't know the time code well enough to know
if that will still cause incorrect calculations, or if a 64-bit overflow
is avoided elsewhere.

Still, an incorrect forwards clock adjustment will cause less trouble than
time going backwards.  So, this patch removes the potential for
intermediate signed overflow.

	Cc: stable@vger.kernel.org  (3.7+)
	Suggested-by: Laurent Vivier <lvivier@redhat.com>
	Tested-by: Laurent Vivier <lvivier@redhat.com>
	Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: John Stultz <john.stultz@linaro.org>
(cherry picked from commit 35a4933a895927990772ae96fdcfd2f806929ee2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/time/timekeeping.c
diff --cc kernel/time/timekeeping.c
index c7aa9b53eafb,99188ee5d9d0..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -153,57 -292,184 +153,61 @@@ static void tk_setup_internals(struct t
  /* Timekeeper helper functions. */
  
  #ifdef CONFIG_ARCH_USES_GETTIMEOFFSET
 -static u32 default_arch_gettimeoffset(void) { return 0; }
 -u32 (*arch_gettimeoffset)(void) = default_arch_gettimeoffset;
 +u32 (*arch_gettimeoffset)(void);
 +
 +u32 get_arch_timeoffset(void)
 +{
 +	if (likely(arch_gettimeoffset))
 +		return arch_gettimeoffset();
 +	return 0;
 +}
  #else
 -static inline u32 arch_gettimeoffset(void) { return 0; }
 +static inline u32 get_arch_timeoffset(void) { return 0; }
  #endif
  
 -static inline s64 timekeeping_get_ns(struct tk_read_base *tkr)
 +static inline s64 timekeeping_get_ns(struct timekeeper *tk)
  {
 -	cycle_t delta;
 +	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
  	s64 nsec;
  
 -	delta = timekeeping_get_delta(tkr);
 +	/* read clocksource: */
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +
++<<<<<<< HEAD
 +	/* calculate the delta since the last update_wall_time: */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
  
 +	nsec = delta * tk->mult + tk->xtime_nsec;
 +	nsec >>= tk->shift;
++=======
+ 	nsec = (delta * tkr->mult + tkr->xtime_nsec) >> tkr->shift;
++>>>>>>> 35a4933a8959 (time: Avoid signed overflow in timekeeping_get_ns())
  
  	/* If arch requires, add in get_arch_timeoffset() */
 -	return nsec + arch_gettimeoffset();
 -}
 -
 -/**
 - * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.
 - * @tkr: Timekeeping readout base from which we take the update
 - *
 - * We want to use this from any context including NMI and tracing /
 - * instrumenting the timekeeping code itself.
 - *
 - * Employ the latch technique; see @raw_write_seqcount_latch.
 - *
 - * So if a NMI hits the update of base[0] then it will use base[1]
 - * which is still consistent. In the worst case this can result is a
 - * slightly wrong timestamp (a few nanoseconds). See
 - * @ktime_get_mono_fast_ns.
 - */
 -static void update_fast_timekeeper(struct tk_read_base *tkr, struct tk_fast *tkf)
 -{
 -	struct tk_read_base *base = tkf->base;
 -
 -	/* Force readers off to base[1] */
 -	raw_write_seqcount_latch(&tkf->seq);
 -
 -	/* Update base[0] */
 -	memcpy(base, tkr, sizeof(*base));
 -
 -	/* Force readers back to base[0] */
 -	raw_write_seqcount_latch(&tkf->seq);
 -
 -	/* Update base[1] */
 -	memcpy(base + 1, base, sizeof(*base));
 -}
 -
 -/**
 - * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic
 - *
 - * This timestamp is not guaranteed to be monotonic across an update.
 - * The timestamp is calculated by:
 - *
 - *	now = base_mono + clock_delta * slope
 - *
 - * So if the update lowers the slope, readers who are forced to the
 - * not yet updated second array are still using the old steeper slope.
 - *
 - * tmono
 - * ^
 - * |    o  n
 - * |   o n
 - * |  u
 - * | o
 - * |o
 - * |12345678---> reader order
 - *
 - * o = old slope
 - * u = update
 - * n = new slope
 - *
 - * So reader 6 will observe time going backwards versus reader 5.
 - *
 - * While other CPUs are likely to be able observe that, the only way
 - * for a CPU local observation is when an NMI hits in the middle of
 - * the update. Timestamps taken from that NMI context might be ahead
 - * of the following timestamps. Callers need to be aware of that and
 - * deal with it.
 - */
 -static __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)
 -{
 -	struct tk_read_base *tkr;
 -	unsigned int seq;
 -	u64 now;
 -
 -	do {
 -		seq = raw_read_seqcount_latch(&tkf->seq);
 -		tkr = tkf->base + (seq & 0x01);
 -		now = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);
 -	} while (read_seqcount_retry(&tkf->seq, seq));
 -
 -	return now;
 -}
 -
 -u64 ktime_get_mono_fast_ns(void)
 -{
 -	return __ktime_get_fast_ns(&tk_fast_mono);
 -}
 -EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);
 -
 -u64 ktime_get_raw_fast_ns(void)
 -{
 -	return __ktime_get_fast_ns(&tk_fast_raw);
 +	return nsec + get_arch_timeoffset();
  }
 -EXPORT_SYMBOL_GPL(ktime_get_raw_fast_ns);
 -
 -/* Suspend-time cycles value for halted fast timekeeper. */
 -static cycle_t cycles_at_suspend;
  
 -static cycle_t dummy_clock_read(struct clocksource *cs)
 +static inline s64 timekeeping_get_ns_raw(struct timekeeper *tk)
  {
 -	return cycles_at_suspend;
 -}
 -
 -/**
 - * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.
 - * @tk: Timekeeper to snapshot.
 - *
 - * It generally is unsafe to access the clocksource after timekeeping has been
 - * suspended, so take a snapshot of the readout base of @tk and use it as the
 - * fast timekeeper's readout base while suspended.  It will return the same
 - * number of cycles every time until timekeeping is resumed at which time the
 - * proper readout base for the fast timekeeper will be restored automatically.
 - */
 -static void halt_fast_timekeeper(struct timekeeper *tk)
 -{
 -	static struct tk_read_base tkr_dummy;
 -	struct tk_read_base *tkr = &tk->tkr_mono;
 -
 -	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
 -	cycles_at_suspend = tkr->read(tkr->clock);
 -	tkr_dummy.read = dummy_clock_read;
 -	update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);
 -
 -	tkr = &tk->tkr_raw;
 -	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
 -	tkr_dummy.read = dummy_clock_read;
 -	update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
 -}
 -
 -#ifdef CONFIG_GENERIC_TIME_VSYSCALL_OLD
 +	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
 +	s64 nsec;
  
 -static inline void update_vsyscall(struct timekeeper *tk)
 -{
 -	struct timespec xt, wm;
 +	/* read clocksource: */
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
  
 -	xt = timespec64_to_timespec(tk_xtime(tk));
 -	wm = timespec64_to_timespec(tk->wall_to_monotonic);
 -	update_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,
 -			    tk->tkr_mono.cycle_last);
 -}
 +	/* calculate the delta since the last update_wall_time: */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
  
 -static inline void old_vsyscall_fixup(struct timekeeper *tk)
 -{
 -	s64 remainder;
 +	/* convert delta to nanoseconds. */
 +	nsec = clocksource_cyc2ns(delta, clock->mult, clock->shift);
  
 -	/*
 -	* Store only full nanoseconds into xtime_nsec after rounding
 -	* it up and add the remainder to the error difference.
 -	* XXX - This is necessary to avoid small 1ns inconsistnecies caused
 -	* by truncating the remainder in vsyscalls. However, it causes
 -	* additional work to be done in timekeeping_adjust(). Once
 -	* the vsyscall implementations are converted to use xtime_nsec
 -	* (shifted nanoseconds), and CONFIG_GENERIC_TIME_VSYSCALL_OLD
 -	* users are removed, this can be killed.
 -	*/
 -	remainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);
 -	tk->tkr_mono.xtime_nsec -= remainder;
 -	tk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;
 -	tk->ntp_error += remainder << tk->ntp_error_shift;
 -	tk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;
 +	/* If arch requires, add in get_arch_timeoffset() */
 +	return nsec + get_arch_timeoffset();
  }
 -#else
 -#define old_vsyscall_fixup(tk)
 -#endif
  
  static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
  
* Unmerged path kernel/time/timekeeping.c
