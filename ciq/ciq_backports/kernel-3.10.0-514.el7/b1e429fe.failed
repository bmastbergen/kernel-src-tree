drm/i915: implement WaClearTdlStateAckDirtyBits

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [drm] i915: implement WaClearTdlStateAckDirtyBits (Rob Clark) [1348329 1349064]
Rebuild_FUZZ: 95.56%
commit-author Tim Gore <tim.gore@intel.com>
commit b1e429fe3ba7b10b8c6875b6dec1d62e1c252729
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/b1e429fe.failed

This is to fix a GPU hang seen with mid thread pre-emption
and pooled EUs.

v2. Use IS_BXT_REVID instead of IS_BROXTON and INTEL_REVID

v3. And use correct type for register addresses

	Signed-off-by: Tim Gore <tim.gore@intel.com>
	Reviewed-by: Arun Siluvery <arun.siluvery@linux.intel.com>
	Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Link: http://patchwork.freedesktop.org/patch/msgid/1458571049-854-1-git-send-email-tim.gore@intel.com
(cherry picked from commit b1e429fe3ba7b10b8c6875b6dec1d62e1c252729)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/i915_reg.h
#	drivers/gpu/drm/i915/intel_lrc.c
diff --cc drivers/gpu/drm/i915/i915_reg.h
index bcce9fc478ee,cea5a390d8c9..000000000000
--- a/drivers/gpu/drm/i915/i915_reg.h
+++ b/drivers/gpu/drm/i915/i915_reg.h
@@@ -1365,13 -1787,26 +1365,31 @@@ enum skl_disp_power_wells 
  #define   GEN6_WIZ_HASHING_16x4				GEN6_WIZ_HASHING(1, 0)
  #define   GEN6_WIZ_HASHING_MASK				GEN6_WIZ_HASHING(1, 1)
  #define   GEN6_TD_FOUR_ROW_DISPATCH_DISABLE		(1 << 5)
 -#define   GEN9_IZ_HASHING_MASK(slice)			(0x3 << ((slice) * 2))
 -#define   GEN9_IZ_HASHING(slice, val)			((val) << ((slice) * 2))
 -
 +#define   GEN9_IZ_HASHING_MASK(slice)			(0x3 << (slice * 2))
 +#define   GEN9_IZ_HASHING(slice, val)			((val) << (slice * 2))
 +
++<<<<<<< HEAD
 +#define GFX_MODE	0x02520
 +#define GFX_MODE_GEN7	0x0229c
 +#define RING_MODE_GEN7(ring)	((ring)->mmio_base+0x29c)
++=======
+ /* WaClearTdlStateAckDirtyBits */
+ #define GEN8_STATE_ACK		_MMIO(0x20F0)
+ #define GEN9_STATE_ACK_SLICE1	_MMIO(0x20F8)
+ #define GEN9_STATE_ACK_SLICE2	_MMIO(0x2100)
+ #define   GEN9_STATE_ACK_TDL0 (1 << 12)
+ #define   GEN9_STATE_ACK_TDL1 (1 << 13)
+ #define   GEN9_STATE_ACK_TDL2 (1 << 14)
+ #define   GEN9_STATE_ACK_TDL3 (1 << 15)
+ #define   GEN9_SUBSLICE_TDL_ACK_BITS \
+ 	(GEN9_STATE_ACK_TDL3 | GEN9_STATE_ACK_TDL2 | \
+ 	 GEN9_STATE_ACK_TDL1 | GEN9_STATE_ACK_TDL0)
+ 
+ #define GFX_MODE	_MMIO(0x2520)
+ #define GFX_MODE_GEN7	_MMIO(0x229c)
+ #define RING_MODE_GEN7(ring)	_MMIO((ring)->mmio_base+0x29c)
++>>>>>>> b1e429fe3ba7 (drm/i915: implement WaClearTdlStateAckDirtyBits)
  #define   GFX_RUN_LIST_ENABLE		(1<<15)
 -#define   GFX_INTERRUPT_STEERING	(1<<14)
  #define   GFX_TLB_INVALIDATE_EXPLICIT	(1<<13)
  #define   GFX_SURFACE_FAULT_ENABLE	(1<<12)
  #define   GFX_REPLAY_MODE		(1<<11)
diff --cc drivers/gpu/drm/i915/intel_lrc.c
index 424e62197787,0d6dc5ec4a46..000000000000
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@@ -1126,28 -1208,422 +1126,373 @@@ static int intel_logical_ring_workaroun
  	return 0;
  }
  
 -#define wa_ctx_emit(batch, index, cmd)					\
 -	do {								\
 -		int __index = (index)++;				\
 -		if (WARN_ON(__index >= (PAGE_SIZE / sizeof(uint32_t)))) { \
 -			return -ENOSPC;					\
 -		}							\
 -		batch[__index] = (cmd);					\
 -	} while (0)
 -
 -#define wa_ctx_emit_reg(batch, index, reg) \
 -	wa_ctx_emit((batch), (index), i915_mmio_reg_offset(reg))
 -
 -/*
 - * In this WA we need to set GEN8_L3SQCREG4[21:21] and reset it after
 - * PIPE_CONTROL instruction. This is required for the flush to happen correctly
 - * but there is a slight complication as this is applied in WA batch where the
 - * values are only initialized once so we cannot take register value at the
 - * beginning and reuse it further; hence we save its value to memory, upload a
 - * constant value with bit21 set and then we restore it back with the saved value.
 - * To simplify the WA, a constant value is formed by using the default value
 - * of this register. This shouldn't be a problem because we are only modifying
 - * it for a short period and this batch in non-premptible. We can ofcourse
 - * use additional instructions that read the actual value of the register
 - * at that time and set our bit of interest but it makes the WA complicated.
 - *
 - * This WA is also required for Gen9 so extracting as a function avoids
 - * code duplication.
 - */
 -static inline int gen8_emit_flush_coherentl3_wa(struct intel_engine_cs *engine,
 -						uint32_t *const batch,
 -						uint32_t index)
 +static int gen8_init_common_ring(struct intel_engine_cs *ring)
  {
++<<<<<<< HEAD
 +	struct drm_device *dev = ring->dev;
++=======
+ 	uint32_t l3sqc4_flush = (0x40400000 | GEN8_LQSC_FLUSH_COHERENT_LINES);
+ 
+ 	/*
+ 	 * WaDisableLSQCROPERFforOCL:skl
+ 	 * This WA is implemented in skl_init_clock_gating() but since
+ 	 * this batch updates GEN8_L3SQCREG4 with default value we need to
+ 	 * set this bit here to retain the WA during flush.
+ 	 */
+ 	if (IS_SKL_REVID(engine->dev, 0, SKL_REVID_E0))
+ 		l3sqc4_flush |= GEN8_LQSC_RO_PERF_DIS;
+ 
+ 	wa_ctx_emit(batch, index, (MI_STORE_REGISTER_MEM_GEN8 |
+ 				   MI_SRM_LRM_GLOBAL_GTT));
+ 	wa_ctx_emit_reg(batch, index, GEN8_L3SQCREG4);
+ 	wa_ctx_emit(batch, index, engine->scratch.gtt_offset + 256);
+ 	wa_ctx_emit(batch, index, 0);
+ 
+ 	wa_ctx_emit(batch, index, MI_LOAD_REGISTER_IMM(1));
+ 	wa_ctx_emit_reg(batch, index, GEN8_L3SQCREG4);
+ 	wa_ctx_emit(batch, index, l3sqc4_flush);
+ 
+ 	wa_ctx_emit(batch, index, GFX_OP_PIPE_CONTROL(6));
+ 	wa_ctx_emit(batch, index, (PIPE_CONTROL_CS_STALL |
+ 				   PIPE_CONTROL_DC_FLUSH_ENABLE));
+ 	wa_ctx_emit(batch, index, 0);
+ 	wa_ctx_emit(batch, index, 0);
+ 	wa_ctx_emit(batch, index, 0);
+ 	wa_ctx_emit(batch, index, 0);
+ 
+ 	wa_ctx_emit(batch, index, (MI_LOAD_REGISTER_MEM_GEN8 |
+ 				   MI_SRM_LRM_GLOBAL_GTT));
+ 	wa_ctx_emit_reg(batch, index, GEN8_L3SQCREG4);
+ 	wa_ctx_emit(batch, index, engine->scratch.gtt_offset + 256);
+ 	wa_ctx_emit(batch, index, 0);
+ 
+ 	return index;
+ }
+ 
+ static inline uint32_t wa_ctx_start(struct i915_wa_ctx_bb *wa_ctx,
+ 				    uint32_t offset,
+ 				    uint32_t start_alignment)
+ {
+ 	return wa_ctx->offset = ALIGN(offset, start_alignment);
+ }
+ 
+ static inline int wa_ctx_end(struct i915_wa_ctx_bb *wa_ctx,
+ 			     uint32_t offset,
+ 			     uint32_t size_alignment)
+ {
+ 	wa_ctx->size = offset - wa_ctx->offset;
+ 
+ 	WARN(wa_ctx->size % size_alignment,
+ 	     "wa_ctx_bb failed sanity checks: size %d is not aligned to %d\n",
+ 	     wa_ctx->size, size_alignment);
+ 	return 0;
+ }
+ 
+ /**
+  * gen8_init_indirectctx_bb() - initialize indirect ctx batch with WA
+  *
+  * @ring: only applicable for RCS
+  * @wa_ctx: structure representing wa_ctx
+  *  offset: specifies start of the batch, should be cache-aligned. This is updated
+  *    with the offset value received as input.
+  *  size: size of the batch in DWORDS but HW expects in terms of cachelines
+  * @batch: page in which WA are loaded
+  * @offset: This field specifies the start of the batch, it should be
+  *  cache-aligned otherwise it is adjusted accordingly.
+  *  Typically we only have one indirect_ctx and per_ctx batch buffer which are
+  *  initialized at the beginning and shared across all contexts but this field
+  *  helps us to have multiple batches at different offsets and select them based
+  *  on a criteria. At the moment this batch always start at the beginning of the page
+  *  and at this point we don't have multiple wa_ctx batch buffers.
+  *
+  *  The number of WA applied are not known at the beginning; we use this field
+  *  to return the no of DWORDS written.
+  *
+  *  It is to be noted that this batch does not contain MI_BATCH_BUFFER_END
+  *  so it adds NOOPs as padding to make it cacheline aligned.
+  *  MI_BATCH_BUFFER_END will be added to perctx batch and both of them together
+  *  makes a complete batch buffer.
+  *
+  * Return: non-zero if we exceed the PAGE_SIZE limit.
+  */
+ 
+ static int gen8_init_indirectctx_bb(struct intel_engine_cs *engine,
+ 				    struct i915_wa_ctx_bb *wa_ctx,
+ 				    uint32_t *const batch,
+ 				    uint32_t *offset)
+ {
+ 	uint32_t scratch_addr;
+ 	uint32_t index = wa_ctx_start(wa_ctx, *offset, CACHELINE_DWORDS);
+ 
+ 	/* WaDisableCtxRestoreArbitration:bdw,chv */
+ 	wa_ctx_emit(batch, index, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+ 
+ 	/* WaFlushCoherentL3CacheLinesAtContextSwitch:bdw */
+ 	if (IS_BROADWELL(engine->dev)) {
+ 		int rc = gen8_emit_flush_coherentl3_wa(engine, batch, index);
+ 		if (rc < 0)
+ 			return rc;
+ 		index = rc;
+ 	}
+ 
+ 	/* WaClearSlmSpaceAtContextSwitch:bdw,chv */
+ 	/* Actual scratch location is at 128 bytes offset */
+ 	scratch_addr = engine->scratch.gtt_offset + 2*CACHELINE_BYTES;
+ 
+ 	wa_ctx_emit(batch, index, GFX_OP_PIPE_CONTROL(6));
+ 	wa_ctx_emit(batch, index, (PIPE_CONTROL_FLUSH_L3 |
+ 				   PIPE_CONTROL_GLOBAL_GTT_IVB |
+ 				   PIPE_CONTROL_CS_STALL |
+ 				   PIPE_CONTROL_QW_WRITE));
+ 	wa_ctx_emit(batch, index, scratch_addr);
+ 	wa_ctx_emit(batch, index, 0);
+ 	wa_ctx_emit(batch, index, 0);
+ 	wa_ctx_emit(batch, index, 0);
+ 
+ 	/* Pad to end of cacheline */
+ 	while (index % CACHELINE_DWORDS)
+ 		wa_ctx_emit(batch, index, MI_NOOP);
+ 
+ 	/*
+ 	 * MI_BATCH_BUFFER_END is not required in Indirect ctx BB because
+ 	 * execution depends on the length specified in terms of cache lines
+ 	 * in the register CTX_RCS_INDIRECT_CTX
+ 	 */
+ 
+ 	return wa_ctx_end(wa_ctx, *offset = index, CACHELINE_DWORDS);
+ }
+ 
+ /**
+  * gen8_init_perctx_bb() - initialize per ctx batch with WA
+  *
+  * @ring: only applicable for RCS
+  * @wa_ctx: structure representing wa_ctx
+  *  offset: specifies start of the batch, should be cache-aligned.
+  *  size: size of the batch in DWORDS but HW expects in terms of cachelines
+  * @batch: page in which WA are loaded
+  * @offset: This field specifies the start of this batch.
+  *   This batch is started immediately after indirect_ctx batch. Since we ensure
+  *   that indirect_ctx ends on a cacheline this batch is aligned automatically.
+  *
+  *   The number of DWORDS written are returned using this field.
+  *
+  *  This batch is terminated with MI_BATCH_BUFFER_END and so we need not add padding
+  *  to align it with cacheline as padding after MI_BATCH_BUFFER_END is redundant.
+  */
+ static int gen8_init_perctx_bb(struct intel_engine_cs *engine,
+ 			       struct i915_wa_ctx_bb *wa_ctx,
+ 			       uint32_t *const batch,
+ 			       uint32_t *offset)
+ {
+ 	uint32_t index = wa_ctx_start(wa_ctx, *offset, CACHELINE_DWORDS);
+ 
+ 	/* WaDisableCtxRestoreArbitration:bdw,chv */
+ 	wa_ctx_emit(batch, index, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+ 
+ 	wa_ctx_emit(batch, index, MI_BATCH_BUFFER_END);
+ 
+ 	return wa_ctx_end(wa_ctx, *offset = index, 1);
+ }
+ 
+ static int gen9_init_indirectctx_bb(struct intel_engine_cs *engine,
+ 				    struct i915_wa_ctx_bb *wa_ctx,
+ 				    uint32_t *const batch,
+ 				    uint32_t *offset)
+ {
+ 	int ret;
+ 	struct drm_device *dev = engine->dev;
+ 	uint32_t index = wa_ctx_start(wa_ctx, *offset, CACHELINE_DWORDS);
+ 
+ 	/* WaDisableCtxRestoreArbitration:skl,bxt */
+ 	if (IS_SKL_REVID(dev, 0, SKL_REVID_D0) ||
+ 	    IS_BXT_REVID(dev, 0, BXT_REVID_A1))
+ 		wa_ctx_emit(batch, index, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+ 
+ 	/* WaFlushCoherentL3CacheLinesAtContextSwitch:skl,bxt */
+ 	ret = gen8_emit_flush_coherentl3_wa(engine, batch, index);
+ 	if (ret < 0)
+ 		return ret;
+ 	index = ret;
+ 
+ 	/* Pad to end of cacheline */
+ 	while (index % CACHELINE_DWORDS)
+ 		wa_ctx_emit(batch, index, MI_NOOP);
+ 
+ 	return wa_ctx_end(wa_ctx, *offset = index, CACHELINE_DWORDS);
+ }
+ 
+ static int gen9_init_perctx_bb(struct intel_engine_cs *engine,
+ 			       struct i915_wa_ctx_bb *wa_ctx,
+ 			       uint32_t *const batch,
+ 			       uint32_t *offset)
+ {
+ 	struct drm_device *dev = engine->dev;
+ 	uint32_t index = wa_ctx_start(wa_ctx, *offset, CACHELINE_DWORDS);
+ 
+ 	/* WaSetDisablePixMaskCammingAndRhwoInCommonSliceChicken:skl,bxt */
+ 	if (IS_SKL_REVID(dev, 0, SKL_REVID_B0) ||
+ 	    IS_BXT_REVID(dev, 0, BXT_REVID_A1)) {
+ 		wa_ctx_emit(batch, index, MI_LOAD_REGISTER_IMM(1));
+ 		wa_ctx_emit_reg(batch, index, GEN9_SLICE_COMMON_ECO_CHICKEN0);
+ 		wa_ctx_emit(batch, index,
+ 			    _MASKED_BIT_ENABLE(DISABLE_PIXEL_MASK_CAMMING));
+ 		wa_ctx_emit(batch, index, MI_NOOP);
+ 	}
+ 
+ 	/* WaClearTdlStateAckDirtyBits:bxt */
+ 	if (IS_BXT_REVID(dev, 0, BXT_REVID_B0)) {
+ 		wa_ctx_emit(batch, index, MI_LOAD_REGISTER_IMM(4));
+ 
+ 		wa_ctx_emit_reg(batch, index, GEN8_STATE_ACK);
+ 		wa_ctx_emit(batch, index, _MASKED_BIT_DISABLE(GEN9_SUBSLICE_TDL_ACK_BITS));
+ 
+ 		wa_ctx_emit_reg(batch, index, GEN9_STATE_ACK_SLICE1);
+ 		wa_ctx_emit(batch, index, _MASKED_BIT_DISABLE(GEN9_SUBSLICE_TDL_ACK_BITS));
+ 
+ 		wa_ctx_emit_reg(batch, index, GEN9_STATE_ACK_SLICE2);
+ 		wa_ctx_emit(batch, index, _MASKED_BIT_DISABLE(GEN9_SUBSLICE_TDL_ACK_BITS));
+ 
+ 		wa_ctx_emit_reg(batch, index, GEN7_ROW_CHICKEN2);
+ 		/* dummy write to CS, mask bits are 0 to ensure the register is not modified */
+ 		wa_ctx_emit(batch, index, 0x0);
+ 		wa_ctx_emit(batch, index, MI_NOOP);
+ 	}
+ 
+ 	/* WaDisableCtxRestoreArbitration:skl,bxt */
+ 	if (IS_SKL_REVID(dev, 0, SKL_REVID_D0) ||
+ 	    IS_BXT_REVID(dev, 0, BXT_REVID_A1))
+ 		wa_ctx_emit(batch, index, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+ 
+ 	wa_ctx_emit(batch, index, MI_BATCH_BUFFER_END);
+ 
+ 	return wa_ctx_end(wa_ctx, *offset = index, 1);
+ }
+ 
+ static int lrc_setup_wa_ctx_obj(struct intel_engine_cs *engine, u32 size)
+ {
+ 	int ret;
+ 
+ 	engine->wa_ctx.obj = i915_gem_alloc_object(engine->dev,
+ 						   PAGE_ALIGN(size));
+ 	if (!engine->wa_ctx.obj) {
+ 		DRM_DEBUG_DRIVER("alloc LRC WA ctx backing obj failed.\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	ret = i915_gem_obj_ggtt_pin(engine->wa_ctx.obj, PAGE_SIZE, 0);
+ 	if (ret) {
+ 		DRM_DEBUG_DRIVER("pin LRC WA ctx backing obj failed: %d\n",
+ 				 ret);
+ 		drm_gem_object_unreference(&engine->wa_ctx.obj->base);
+ 		return ret;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void lrc_destroy_wa_ctx_obj(struct intel_engine_cs *engine)
+ {
+ 	if (engine->wa_ctx.obj) {
+ 		i915_gem_object_ggtt_unpin(engine->wa_ctx.obj);
+ 		drm_gem_object_unreference(&engine->wa_ctx.obj->base);
+ 		engine->wa_ctx.obj = NULL;
+ 	}
+ }
+ 
+ static int intel_init_workaround_bb(struct intel_engine_cs *engine)
+ {
+ 	int ret;
+ 	uint32_t *batch;
+ 	uint32_t offset;
+ 	struct page *page;
+ 	struct i915_ctx_workarounds *wa_ctx = &engine->wa_ctx;
+ 
+ 	WARN_ON(engine->id != RCS);
+ 
+ 	/* update this when WA for higher Gen are added */
+ 	if (INTEL_INFO(engine->dev)->gen > 9) {
+ 		DRM_ERROR("WA batch buffer is not initialized for Gen%d\n",
+ 			  INTEL_INFO(engine->dev)->gen);
+ 		return 0;
+ 	}
+ 
+ 	/* some WA perform writes to scratch page, ensure it is valid */
+ 	if (engine->scratch.obj == NULL) {
+ 		DRM_ERROR("scratch page not allocated for %s\n", engine->name);
+ 		return -EINVAL;
+ 	}
+ 
+ 	ret = lrc_setup_wa_ctx_obj(engine, PAGE_SIZE);
+ 	if (ret) {
+ 		DRM_DEBUG_DRIVER("Failed to setup context WA page: %d\n", ret);
+ 		return ret;
+ 	}
+ 
+ 	page = i915_gem_object_get_dirty_page(wa_ctx->obj, 0);
+ 	batch = kmap_atomic(page);
+ 	offset = 0;
+ 
+ 	if (INTEL_INFO(engine->dev)->gen == 8) {
+ 		ret = gen8_init_indirectctx_bb(engine,
+ 					       &wa_ctx->indirect_ctx,
+ 					       batch,
+ 					       &offset);
+ 		if (ret)
+ 			goto out;
+ 
+ 		ret = gen8_init_perctx_bb(engine,
+ 					  &wa_ctx->per_ctx,
+ 					  batch,
+ 					  &offset);
+ 		if (ret)
+ 			goto out;
+ 	} else if (INTEL_INFO(engine->dev)->gen == 9) {
+ 		ret = gen9_init_indirectctx_bb(engine,
+ 					       &wa_ctx->indirect_ctx,
+ 					       batch,
+ 					       &offset);
+ 		if (ret)
+ 			goto out;
+ 
+ 		ret = gen9_init_perctx_bb(engine,
+ 					  &wa_ctx->per_ctx,
+ 					  batch,
+ 					  &offset);
+ 		if (ret)
+ 			goto out;
+ 	}
+ 
+ out:
+ 	kunmap_atomic(batch);
+ 	if (ret)
+ 		lrc_destroy_wa_ctx_obj(engine);
+ 
+ 	return ret;
+ }
+ 
+ static int gen8_init_common_ring(struct intel_engine_cs *engine)
+ {
+ 	struct drm_device *dev = engine->dev;
++>>>>>>> b1e429fe3ba7 (drm/i915: implement WaClearTdlStateAckDirtyBits)
  	struct drm_i915_private *dev_priv = dev->dev_private;
 -	unsigned int next_context_status_buffer_hw;
  
 -	lrc_setup_hardware_status_page(engine,
 -				       dev_priv->kernel_context->engine[engine->id].state);
 +	I915_WRITE_IMR(ring, ~(ring->irq_enable_mask | ring->irq_keep_mask));
 +	I915_WRITE(RING_HWSTAM(ring->mmio_base), 0xffffffff);
  
 -	I915_WRITE_IMR(engine,
 -		       ~(engine->irq_enable_mask | engine->irq_keep_mask));
 -	I915_WRITE(RING_HWSTAM(engine->mmio_base), 0xffffffff);
 +	if (ring->status_page.obj) {
 +		I915_WRITE(RING_HWS_PGA(ring->mmio_base),
 +			   (u32)ring->status_page.gfx_addr);
 +		POSTING_READ(RING_HWS_PGA(ring->mmio_base));
 +	}
  
 -	I915_WRITE(RING_MODE_GEN7(engine),
 +	I915_WRITE(RING_MODE_GEN7(ring),
  		   _MASKED_BIT_DISABLE(GFX_REPLAY_MODE) |
  		   _MASKED_BIT_ENABLE(GFX_RUN_LIST_ENABLE));
 -	POSTING_READ(RING_MODE_GEN7(engine));
 -
 -	/*
 -	 * Instead of resetting the Context Status Buffer (CSB) read pointer to
 -	 * zero, we need to read the write pointer from hardware and use its
 -	 * value because "this register is power context save restored".
 -	 * Effectively, these states have been observed:
 -	 *
 -	 *      | Suspend-to-idle (freeze) | Suspend-to-RAM (mem) |
 -	 * BDW  | CSB regs not reset       | CSB regs reset       |
 -	 * CHT  | CSB regs not reset       | CSB regs not reset   |
 -	 * SKL  |         ?                |         ?            |
 -	 * BXT  |         ?                |         ?            |
 -	 */
 -	next_context_status_buffer_hw =
 -		GEN8_CSB_WRITE_PTR(I915_READ(RING_CONTEXT_STATUS_PTR(engine)));
 -
 -	/*
 -	 * When the CSB registers are reset (also after power-up / gpu reset),
 -	 * CSB write pointer is set to all 1's, which is not valid, use '5' in
 -	 * this special case, so the first element read is CSB[0].
 -	 */
 -	if (next_context_status_buffer_hw == GEN8_CSB_PTR_MASK)
 -		next_context_status_buffer_hw = (GEN8_CSB_ENTRIES - 1);
 -
 -	engine->next_context_status_buffer = next_context_status_buffer_hw;
 -	DRM_DEBUG_DRIVER("Execlists enabled for %s\n", engine->name);
 +	POSTING_READ(RING_MODE_GEN7(ring));
 +	ring->next_context_status_buffer = 0;
 +	DRM_DEBUG_DRIVER("Execlists enabled for %s\n", ring->name);
  
 -	intel_engine_init_hangcheck(engine);
 +	memset(&ring->hangcheck, 0, sizeof(ring->hangcheck));
  
  	return 0;
  }
* Unmerged path drivers/gpu/drm/i915/i915_reg.h
* Unmerged path drivers/gpu/drm/i915/intel_lrc.c
