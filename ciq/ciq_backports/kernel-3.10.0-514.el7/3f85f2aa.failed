IB/mlx4: Add diagnostic hardware counters

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mark Bloch <markb@mellanox.com>
commit 3f85f2aaabf785e53bbcd242cb92aeda28990ef5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/3f85f2aa.failed

Expose IB diagnostic hardware counters.
The counters count IB events and are applicable for IB and RoCE.

The counters can be divided into two groups, per device and per port.
Device counters are always exposed.
Port counters are exposed only if the firmware supports per port counters.

rq_num_dup and sq_num_to are only exposed if we have firmware support
for them, if we do, we expose them per device and per port.
rq_num_udsdprd and num_cqovf are device only counters.

rq - denotes responder.
sq - denotes requester.

|-----------------------|---------------------------------------|
|	Name		|	Description			|
|-----------------------|---------------------------------------|
|rq_num_lle		| Number of local length errors		|
|-----------------------|---------------------------------------|
|sq_num_lle		| number of local length errors		|
|-----------------------|---------------------------------------|
|rq_num_lqpoe		| Number of local QP operation errors	|
|-----------------------|---------------------------------------|
|sq_num_lqpoe		| Number of local QP operation errors	|
|-----------------------|---------------------------------------|
|rq_num_lpe		| Number of local protection errors	|
|-----------------------|---------------------------------------|
|sq_num_lpe		| Number of local protection errors	|
|-----------------------|---------------------------------------|
|rq_num_wrfe		| Number of CQEs with error		|
|-----------------------|---------------------------------------|
|sq_num_wrfe		| Number of CQEs with error		|
|-----------------------|---------------------------------------|
|sq_num_mwbe		| Number of Memory Window bind errors	|
|-----------------------|---------------------------------------|
|sq_num_bre		| Number of bad response errors		|
|-----------------------|---------------------------------------|
|sq_num_rire		| Number of Remote Invalid request	|
|			| errors				|
|-----------------------|---------------------------------------|
|rq_num_rire		| Number of Remote Invalid request	|
|			| errors				|
|-----------------------|---------------------------------------|
|sq_num_rae		| Number of remote access errors	|
|-----------------------|---------------------------------------|
|rq_num_rae		| Number of remote access errors	|
|-----------------------|---------------------------------------|
|sq_num_roe		| Number of remote operation errors	|
|-----------------------|---------------------------------------|
|sq_num_tree		| Number of transport retries exceeded	|
|			| errors				|
|-----------------------|---------------------------------------|
|sq_num_rree		| Number of RNR NAK retries exceeded	|
|			| errors				|
|-----------------------|---------------------------------------|
|rq_num_rnr		| Number of RNR NAKs sent		|
|-----------------------|---------------------------------------|
|sq_num_rnr		| Number of RNR NAKs received		|
|-----------------------|---------------------------------------|
|rq_num_oos		| Number of Out of Sequence requests	|
|			| received				|
|-----------------------|---------------------------------------|
|sq_num_oos		| Number of Out of Sequence NAKs	|
|			| received				|
|-----------------------|---------------------------------------|
|rq_num_udsdprd		| Number of UD packets silently		|
|			| discarded on the Receive Queue due to	|
|			| lack of receive descriptor		|
|-----------------------|---------------------------------------|
|rq_num_dup		| Number of duplicate requests received	|
|-----------------------|---------------------------------------|
|sq_num_to		| Number of time out received		|
|-----------------------|---------------------------------------|
|num_cqovf		| Number of CQ overflows		|
|-----------------------|---------------------------------------|

	Signed-off-by: Mark Bloch <markb@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 3f85f2aaabf785e53bbcd242cb92aeda28990ef5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx4/main.c
diff --cc drivers/infiniband/hw/mlx4/main.c
index 55f2129240fc,d0bb383bed8e..000000000000
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@@ -2057,272 -2061,195 +2057,464 @@@ static struct device_attribute *mlx4_cl
  	&dev_attr_board_id
  };
  
++<<<<<<< HEAD
 +static void mlx4_addrconf_ifid_eui48(u8 *eui, u16 vlan_id,
 +				     struct net_device *dev)
 +{
 +	memcpy(eui, dev->dev_addr, 3);
 +	memcpy(eui + 5, dev->dev_addr + 3, 3);
 +	if (vlan_id < 0x1000) {
 +		eui[3] = vlan_id >> 8;
 +		eui[4] = vlan_id & 0xff;
 +	} else {
 +		eui[3] = 0xff;
 +		eui[4] = 0xfe;
 +	}
 +	eui[0] ^= 2;
 +}
 +
 +static void update_gids_task(struct work_struct *work)
 +{
 +	struct update_gid_work *gw = container_of(work, struct update_gid_work, work);
 +	struct mlx4_cmd_mailbox *mailbox;
 +	union ib_gid *gids;
 +	int err;
 +	struct mlx4_dev	*dev = gw->dev->dev;
 +	int is_bonded = mlx4_is_bonded(dev);
 +
 +	if (!gw->dev->ib_active)
 +		return;
 +
 +	mailbox = mlx4_alloc_cmd_mailbox(dev);
 +	if (IS_ERR(mailbox)) {
 +		pr_warn("update gid table failed %ld\n", PTR_ERR(mailbox));
 +		return;
 +	}
 +
 +	gids = mailbox->buf;
 +	memcpy(gids, gw->gids, sizeof gw->gids);
 +
 +	err = mlx4_cmd(dev, mailbox->dma, MLX4_SET_PORT_GID_TABLE << 8 | gw->port,
 +		       MLX4_SET_PORT_ETH_OPCODE, MLX4_CMD_SET_PORT,
 +		       MLX4_CMD_TIME_CLASS_B, MLX4_CMD_WRAPPED);
 +	if (err)
 +		pr_warn("set port command failed\n");
 +	else
 +		if ((gw->port == 1) || !is_bonded)
 +			mlx4_ib_dispatch_event(gw->dev,
 +					       is_bonded ? 1 : gw->port,
 +					       IB_EVENT_GID_CHANGE);
 +
 +	mlx4_free_cmd_mailbox(dev, mailbox);
 +	kfree(gw);
 +}
 +
 +static void reset_gids_task(struct work_struct *work)
 +{
 +	struct update_gid_work *gw =
 +			container_of(work, struct update_gid_work, work);
 +	struct mlx4_cmd_mailbox *mailbox;
 +	union ib_gid *gids;
 +	int err;
 +	struct mlx4_dev	*dev = gw->dev->dev;
 +
 +	if (!gw->dev->ib_active)
 +		return;
 +
 +	mailbox = mlx4_alloc_cmd_mailbox(dev);
 +	if (IS_ERR(mailbox)) {
 +		pr_warn("reset gid table failed\n");
 +		goto free;
 +	}
 +
 +	gids = mailbox->buf;
 +	memcpy(gids, gw->gids, sizeof(gw->gids));
 +
 +	if (mlx4_ib_port_link_layer(&gw->dev->ib_dev, gw->port) ==
 +				    IB_LINK_LAYER_ETHERNET) {
 +		err = mlx4_cmd(dev, mailbox->dma,
 +			       MLX4_SET_PORT_GID_TABLE << 8 | gw->port,
 +			       MLX4_SET_PORT_ETH_OPCODE, MLX4_CMD_SET_PORT,
 +			       MLX4_CMD_TIME_CLASS_B,
 +			       MLX4_CMD_WRAPPED);
 +		if (err)
 +			pr_warn("set port %d command failed\n", gw->port);
 +	}
 +
 +	mlx4_free_cmd_mailbox(dev, mailbox);
 +free:
 +	kfree(gw);
 +}
 +
 +static int update_gid_table(struct mlx4_ib_dev *dev, int port,
 +			    union ib_gid *gid, int clear,
 +			    int default_gid)
 +{
 +	struct update_gid_work *work;
 +	int i;
 +	int need_update = 0;
 +	int free = -1;
 +	int found = -1;
 +	int max_gids;
 +
 +	if (default_gid) {
 +		free = 0;
 +	} else {
 +		max_gids = dev->dev->caps.gid_table_len[port];
 +		for (i = 1; i < max_gids; ++i) {
 +			if (!memcmp(&dev->iboe.gid_table[port - 1][i], gid,
 +				    sizeof(*gid)))
 +				found = i;
 +
 +			if (clear) {
 +				if (found >= 0) {
 +					need_update = 1;
 +					dev->iboe.gid_table[port - 1][found] =
 +						zgid;
 +					break;
 +				}
 +			} else {
 +				if (found >= 0)
 +					break;
 +
 +				if (free < 0 &&
 +				    !memcmp(&dev->iboe.gid_table[port - 1][i],
 +					    &zgid, sizeof(*gid)))
 +					free = i;
 +			}
 +		}
 +	}
 +
 +	if (found == -1 && !clear && free >= 0) {
 +		dev->iboe.gid_table[port - 1][free] = *gid;
 +		need_update = 1;
 +	}
 +
 +	if (!need_update)
 +		return 0;
 +
 +	work = kzalloc(sizeof(*work), GFP_ATOMIC);
 +	if (!work)
 +		return -ENOMEM;
 +
 +	memcpy(work->gids, dev->iboe.gid_table[port - 1], sizeof(work->gids));
 +	INIT_WORK(&work->work, update_gids_task);
 +	work->port = port;
 +	work->dev = dev;
 +	queue_work(wq, &work->work);
 +
 +	return 0;
 +}
 +
 +static void mlx4_make_default_gid(struct  net_device *dev, union ib_gid *gid)
 +{
 +	gid->global.subnet_prefix = cpu_to_be64(0xfe80000000000000LL);
 +	mlx4_addrconf_ifid_eui48(&gid->raw[8], 0xffff, dev);
 +}
 +
 +
 +static int reset_gid_table(struct mlx4_ib_dev *dev, u8 port)
 +{
 +	struct update_gid_work *work;
 +
 +	work = kzalloc(sizeof(*work), GFP_ATOMIC);
 +	if (!work)
 +		return -ENOMEM;
 +
 +	memset(dev->iboe.gid_table[port - 1], 0, sizeof(work->gids));
 +	memset(work->gids, 0, sizeof(work->gids));
 +	INIT_WORK(&work->work, reset_gids_task);
 +	work->dev = dev;
 +	work->port = port;
 +	queue_work(wq, &work->work);
 +	return 0;
 +}
 +
 +static int mlx4_ib_addr_event(int event, struct net_device *event_netdev,
 +			      struct mlx4_ib_dev *ibdev, union ib_gid *gid)
 +{
 +	struct mlx4_ib_iboe *iboe;
 +	int port = 0;
 +	struct net_device *real_dev = rdma_vlan_dev_real_dev(event_netdev) ?
 +				rdma_vlan_dev_real_dev(event_netdev) :
 +				event_netdev;
 +	union ib_gid default_gid;
 +
 +	mlx4_make_default_gid(real_dev, &default_gid);
 +
 +	if (!memcmp(gid, &default_gid, sizeof(*gid)))
 +		return 0;
 +
 +	if (event != NETDEV_DOWN && event != NETDEV_UP)
 +		return 0;
 +
 +	if ((real_dev != event_netdev) &&
 +	    (event == NETDEV_DOWN) &&
 +	    rdma_link_local_addr((struct in6_addr *)gid))
 +		return 0;
 +
 +	iboe = &ibdev->iboe;
 +	spin_lock_bh(&iboe->lock);
 +
 +	for (port = 1; port <= ibdev->dev->caps.num_ports; ++port)
 +		if ((netif_is_bond_master(real_dev) &&
 +		     (real_dev == iboe->masters[port - 1])) ||
 +		     (!netif_is_bond_master(real_dev) &&
 +		     (real_dev == iboe->netdevs[port - 1])))
 +			update_gid_table(ibdev, port, gid,
 +					 event == NETDEV_DOWN, 0);
 +
 +	spin_unlock_bh(&iboe->lock);
 +	return 0;
 +
 +}
 +
 +static u8 mlx4_ib_get_dev_port(struct net_device *dev,
 +			       struct mlx4_ib_dev *ibdev)
 +{
 +	u8 port = 0;
 +	struct mlx4_ib_iboe *iboe;
 +	struct net_device *real_dev = rdma_vlan_dev_real_dev(dev) ?
 +				rdma_vlan_dev_real_dev(dev) : dev;
 +
 +	iboe = &ibdev->iboe;
 +
 +	for (port = 1; port <= ibdev->dev->caps.num_ports; ++port)
 +		if ((netif_is_bond_master(real_dev) &&
 +		     (real_dev == iboe->masters[port - 1])) ||
 +		     (!netif_is_bond_master(real_dev) &&
 +		     (real_dev == iboe->netdevs[port - 1])))
 +			break;
 +
 +	if ((port == 0) || (port > ibdev->dev->caps.num_ports))
 +		return 0;
 +	else
 +		return port;
 +}
 +
 +static int mlx4_ib_inet_event(struct notifier_block *this, unsigned long event,
 +				void *ptr)
 +{
 +	struct mlx4_ib_dev *ibdev;
 +	struct in_ifaddr *ifa = ptr;
 +	union ib_gid gid;
 +	struct net_device *event_netdev = ifa->ifa_dev->dev;
 +
 +	ipv6_addr_set_v4mapped(ifa->ifa_address, (struct in6_addr *)&gid);
 +
 +	ibdev = container_of(this, struct mlx4_ib_dev, iboe.nb_inet);
 +
 +	mlx4_ib_addr_event(event, event_netdev, ibdev, &gid);
 +	return NOTIFY_DONE;
 +}
 +
 +#if IS_ENABLED(CONFIG_IPV6)
 +static int mlx4_ib_inet6_event(struct notifier_block *this, unsigned long event,
 +				void *ptr)
 +{
 +	struct mlx4_ib_dev *ibdev;
 +	struct inet6_ifaddr *ifa = ptr;
 +	union  ib_gid *gid = (union ib_gid *)&ifa->addr;
 +	struct net_device *event_netdev = ifa->idev->dev;
 +
 +	ibdev = container_of(this, struct mlx4_ib_dev, iboe.nb_inet6);
 +
 +	mlx4_ib_addr_event(event, event_netdev, ibdev, gid);
 +	return NOTIFY_DONE;
 +}
 +#endif
 +
++=======
+ struct diag_counter {
+ 	const char *name;
+ 	u32 offset;
+ };
+ 
+ #define DIAG_COUNTER(_name, _offset)			\
+ 	{ .name = #_name, .offset = _offset }
+ 
+ static const struct diag_counter diag_basic[] = {
+ 	DIAG_COUNTER(rq_num_lle, 0x00),
+ 	DIAG_COUNTER(sq_num_lle, 0x04),
+ 	DIAG_COUNTER(rq_num_lqpoe, 0x08),
+ 	DIAG_COUNTER(sq_num_lqpoe, 0x0C),
+ 	DIAG_COUNTER(rq_num_lpe, 0x18),
+ 	DIAG_COUNTER(sq_num_lpe, 0x1C),
+ 	DIAG_COUNTER(rq_num_wrfe, 0x20),
+ 	DIAG_COUNTER(sq_num_wrfe, 0x24),
+ 	DIAG_COUNTER(sq_num_mwbe, 0x2C),
+ 	DIAG_COUNTER(sq_num_bre, 0x34),
+ 	DIAG_COUNTER(sq_num_rire, 0x44),
+ 	DIAG_COUNTER(rq_num_rire, 0x48),
+ 	DIAG_COUNTER(sq_num_rae, 0x4C),
+ 	DIAG_COUNTER(rq_num_rae, 0x50),
+ 	DIAG_COUNTER(sq_num_roe, 0x54),
+ 	DIAG_COUNTER(sq_num_tree, 0x5C),
+ 	DIAG_COUNTER(sq_num_rree, 0x64),
+ 	DIAG_COUNTER(rq_num_rnr, 0x68),
+ 	DIAG_COUNTER(sq_num_rnr, 0x6C),
+ 	DIAG_COUNTER(rq_num_oos, 0x100),
+ 	DIAG_COUNTER(sq_num_oos, 0x104),
+ };
+ 
+ static const struct diag_counter diag_ext[] = {
+ 	DIAG_COUNTER(rq_num_dup, 0x130),
+ 	DIAG_COUNTER(sq_num_to, 0x134),
+ };
+ 
+ static const struct diag_counter diag_device_only[] = {
+ 	DIAG_COUNTER(num_cqovf, 0x1A0),
+ 	DIAG_COUNTER(rq_num_udsdprd, 0x118),
+ };
+ 
+ static struct rdma_hw_stats *mlx4_ib_alloc_hw_stats(struct ib_device *ibdev,
+ 						    u8 port_num)
+ {
+ 	struct mlx4_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx4_ib_diag_counters *diag = dev->diag_counters;
+ 
+ 	if (!diag[!!port_num].name)
+ 		return NULL;
+ 
+ 	return rdma_alloc_hw_stats_struct(diag[!!port_num].name,
+ 					  diag[!!port_num].num_counters,
+ 					  RDMA_HW_STATS_DEFAULT_LIFESPAN);
+ }
+ 
+ static int mlx4_ib_get_hw_stats(struct ib_device *ibdev,
+ 				struct rdma_hw_stats *stats,
+ 				u8 port, int index)
+ {
+ 	struct mlx4_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx4_ib_diag_counters *diag = dev->diag_counters;
+ 	u32 hw_value[ARRAY_SIZE(diag_device_only) +
+ 		ARRAY_SIZE(diag_ext) + ARRAY_SIZE(diag_basic)] = {};
+ 	int ret;
+ 	int i;
+ 
+ 	ret = mlx4_query_diag_counters(dev->dev,
+ 				       MLX4_OP_MOD_QUERY_TRANSPORT_CI_ERRORS,
+ 				       diag[!!port].offset, hw_value,
+ 				       diag[!!port].num_counters, port);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	for (i = 0; i < diag[!!port].num_counters; i++)
+ 		stats->value[i] = hw_value[i];
+ 
+ 	return diag[!!port].num_counters;
+ }
+ 
+ static int __mlx4_ib_alloc_diag_counters(struct mlx4_ib_dev *ibdev,
+ 					 const char ***name,
+ 					 u32 **offset,
+ 					 u32 *num,
+ 					 bool port)
+ {
+ 	u32 num_counters;
+ 
+ 	num_counters = ARRAY_SIZE(diag_basic);
+ 
+ 	if (ibdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_DIAG_PER_PORT)
+ 		num_counters += ARRAY_SIZE(diag_ext);
+ 
+ 	if (!port)
+ 		num_counters += ARRAY_SIZE(diag_device_only);
+ 
+ 	*name = kcalloc(num_counters, sizeof(**name), GFP_KERNEL);
+ 	if (!*name)
+ 		return -ENOMEM;
+ 
+ 	*offset = kcalloc(num_counters, sizeof(**offset), GFP_KERNEL);
+ 	if (!*offset)
+ 		goto err_name;
+ 
+ 	*num = num_counters;
+ 
+ 	return 0;
+ 
+ err_name:
+ 	kfree(*name);
+ 	return -ENOMEM;
+ }
+ 
+ static void mlx4_ib_fill_diag_counters(struct mlx4_ib_dev *ibdev,
+ 				       const char **name,
+ 				       u32 *offset,
+ 				       bool port)
+ {
+ 	int i;
+ 	int j;
+ 
+ 	for (i = 0, j = 0; i < ARRAY_SIZE(diag_basic); i++, j++) {
+ 		name[i] = diag_basic[i].name;
+ 		offset[i] = diag_basic[i].offset;
+ 	}
+ 
+ 	if (ibdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_DIAG_PER_PORT) {
+ 		for (i = 0; i < ARRAY_SIZE(diag_ext); i++, j++) {
+ 			name[j] = diag_ext[i].name;
+ 			offset[j] = diag_ext[i].offset;
+ 		}
+ 	}
+ 
+ 	if (!port) {
+ 		for (i = 0; i < ARRAY_SIZE(diag_device_only); i++, j++) {
+ 			name[j] = diag_device_only[i].name;
+ 			offset[j] = diag_device_only[i].offset;
+ 		}
+ 	}
+ }
+ 
+ static int mlx4_ib_alloc_diag_counters(struct mlx4_ib_dev *ibdev)
+ {
+ 	struct mlx4_ib_diag_counters *diag = ibdev->diag_counters;
+ 	int i;
+ 	int ret;
+ 	bool per_port = !!(ibdev->dev->caps.flags2 &
+ 		MLX4_DEV_CAP_FLAG2_DIAG_PER_PORT);
+ 
+ 	for (i = 0; i < MLX4_DIAG_COUNTERS_TYPES; i++) {
+ 		/* i == 1 means we are building port counters */
+ 		if (i && !per_port)
+ 			continue;
+ 
+ 		ret = __mlx4_ib_alloc_diag_counters(ibdev, &diag[i].name,
+ 						    &diag[i].offset,
+ 						    &diag[i].num_counters, i);
+ 		if (ret)
+ 			goto err_alloc;
+ 
+ 		mlx4_ib_fill_diag_counters(ibdev, diag[i].name,
+ 					   diag[i].offset, i);
+ 	}
+ 
+ 	ibdev->ib_dev.get_hw_stats	= mlx4_ib_get_hw_stats;
+ 	ibdev->ib_dev.alloc_hw_stats	= mlx4_ib_alloc_hw_stats;
+ 
+ 	return 0;
+ 
+ err_alloc:
+ 	if (i) {
+ 		kfree(diag[i - 1].name);
+ 		kfree(diag[i - 1].offset);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void mlx4_ib_diag_cleanup(struct mlx4_ib_dev *ibdev)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < MLX4_DIAG_COUNTERS_TYPES; i++) {
+ 		kfree(ibdev->diag_counters[i].offset);
+ 		kfree(ibdev->diag_counters[i].name);
+ 	}
+ }
+ 
++>>>>>>> 3f85f2aaabf7 (IB/mlx4: Add diagnostic hardware counters)
  #define MLX4_IB_INVALID_MAC	((u64)-1)
  static void mlx4_ib_update_qps(struct mlx4_ib_dev *ibdev,
  			       struct net_device *dev,
* Unmerged path drivers/infiniband/hw/mlx4/main.c
diff --git a/drivers/infiniband/hw/mlx4/mlx4_ib.h b/drivers/infiniband/hw/mlx4/mlx4_ib.h
index c3377d0b54d4..07b798636444 100644
--- a/drivers/infiniband/hw/mlx4/mlx4_ib.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib.h
@@ -551,6 +551,14 @@ struct mlx4_ib_counters {
 	u32			default_counter;
 };
 
+#define MLX4_DIAG_COUNTERS_TYPES 2
+
+struct mlx4_ib_diag_counters {
+	const char **name;
+	u32 *offset;
+	u32 num_counters;
+};
+
 struct mlx4_ib_dev {
 	struct ib_device	ib_dev;
 	struct mlx4_dev	       *dev;
@@ -587,6 +595,7 @@ struct mlx4_ib_dev {
 	/* protect resources needed as part of reset flow */
 	spinlock_t		reset_flow_resource_lock;
 	struct list_head		qp_list;
+	struct mlx4_ib_diag_counters diag_counters[MLX4_DIAG_COUNTERS_TYPES];
 };
 
 struct ib_event_work {
diff --git a/include/linux/mlx4/device.h b/include/linux/mlx4/device.h
index 5ca4c09c0f58..43a723c1f0a8 100644
--- a/include/linux/mlx4/device.h
+++ b/include/linux/mlx4/device.h
@@ -1340,6 +1340,9 @@ enum {
 	VXLAN_STEER_BY_INNER_VLAN	= 1 << 4,
 };
 
+enum {
+	MLX4_OP_MOD_QUERY_TRANSPORT_CI_ERRORS = 0x2,
+};
 
 int mlx4_flow_steer_promisc_add(struct mlx4_dev *dev, u8 port, u32 qpn,
 				enum mlx4_net_trans_promisc_mode mode);
