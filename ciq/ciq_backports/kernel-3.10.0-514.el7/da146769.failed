thp: fix zap_huge_pmd() for DAX

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit da146769004e1dd5ed06853e6d009be8ca675d5f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/da146769.failed

The original DAX code assumed that pgtable_t was a pointer, which isn't
true on all architectures.  Restructure the code to not rely on that
assumption.

[willy@linux.intel.com: further fixes integrated into this patch]
	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Signed-off-by: Matthew Wilcox <willy@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit da146769004e1dd5ed06853e6d009be8ca675d5f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index 43c4703f033a,96dfd9d81fcb..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1484,62 -1456,43 +1484,95 @@@ out
  int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
  		 pmd_t *pmd, unsigned long addr)
  {
+ 	pmd_t orig_pmd;
  	spinlock_t *ptl;
- 	int ret = 0;
  
++<<<<<<< HEAD
 +	if (__pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
 +		struct page *page;
 +		pgtable_t pgtable;
 +		pmd_t orig_pmd;
 +		/*
 +		 * For architectures like ppc64 we look at deposited pgtable
 +		 * when calling pmdp_get_and_clear. So do the
 +		 * pgtable_trans_huge_withdraw after finishing pmdp related
 +		 * operations.
 +		 */
 +		orig_pmd = pmdp_get_and_clear(tlb->mm, addr, pmd);
 +		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
 +		pgtable = pgtable_trans_huge_withdraw(tlb->mm, pmd);
 +		if (is_huge_zero_pmd(orig_pmd)) {
 +			atomic_long_dec(&tlb->mm->nr_ptes);
 +			spin_unlock(ptl);
 +			put_huge_zero_page();
 +		} else {
 +			page = pmd_page(orig_pmd);
 +			page_remove_rmap(page);
 +			VM_BUG_ON_PAGE(page_mapcount(page) < 0, page);
 +			add_mm_counter(tlb->mm, MM_ANONPAGES, -HPAGE_PMD_NR);
 +			VM_BUG_ON_PAGE(!PageHead(page), page);
 +			atomic_long_dec(&tlb->mm->nr_ptes);
 +			spin_unlock(ptl);
 +			tlb_remove_page(tlb, page);
 +		}
 +		pte_free(tlb->mm, pgtable);
 +		ret = 1;
++=======
+ 	if (__pmd_trans_huge_lock(pmd, vma, &ptl) != 1)
+ 		return 0;
+ 	/*
+ 	 * For architectures like ppc64 we look at deposited pgtable
+ 	 * when calling pmdp_huge_get_and_clear. So do the
+ 	 * pgtable_trans_huge_withdraw after finishing pmdp related
+ 	 * operations.
+ 	 */
+ 	orig_pmd = pmdp_huge_get_and_clear_full(tlb->mm, addr, pmd,
+ 			tlb->fullmm);
+ 	tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
+ 	if (vma_is_dax(vma)) {
+ 		spin_unlock(ptl);
+ 		if (is_huge_zero_pmd(orig_pmd))
+ 			put_huge_zero_page();
+ 	} else if (is_huge_zero_pmd(orig_pmd)) {
+ 		pte_free(tlb->mm, pgtable_trans_huge_withdraw(tlb->mm, pmd));
+ 		atomic_long_dec(&tlb->mm->nr_ptes);
+ 		spin_unlock(ptl);
+ 		put_huge_zero_page();
+ 	} else {
+ 		struct page *page = pmd_page(orig_pmd);
+ 		page_remove_rmap(page);
+ 		VM_BUG_ON_PAGE(page_mapcount(page) < 0, page);
+ 		add_mm_counter(tlb->mm, MM_ANONPAGES, -HPAGE_PMD_NR);
+ 		VM_BUG_ON_PAGE(!PageHead(page), page);
+ 		pte_free(tlb->mm, pgtable_trans_huge_withdraw(tlb->mm, pmd));
+ 		atomic_long_dec(&tlb->mm->nr_ptes);
+ 		spin_unlock(ptl);
+ 		tlb_remove_page(tlb, page);
++>>>>>>> da146769004e (thp: fix zap_huge_pmd() for DAX)
  	}
- 	return ret;
+ 	return 1;
  }
  
 +int mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 +		unsigned long addr, unsigned long end,
 +		unsigned char *vec)
 +{
 +	spinlock_t *ptl;
 +	int ret = 0;
 +
 +	if (__pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
 +		/*
 +		 * All logical pages in the range are present
 +		 * if backed by a huge page.
 +		 */
 +		spin_unlock(ptl);
 +		memset(vec, 1, (end - addr) >> PAGE_SHIFT);
 +		ret = 1;
 +	}
 +
 +	return ret;
 +}
 +
  int move_huge_pmd(struct vm_area_struct *vma, struct vm_area_struct *new_vma,
  		  unsigned long old_addr,
  		  unsigned long new_addr, unsigned long old_end,
* Unmerged path mm/huge_memory.c
