mm: meminit: inline some helper functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] meminit: inline some helper functions (George Beshers) [727269]
Rebuild_FUZZ: 94.87%
commit-author Mel Gorman <mgorman@suse.de>
commit 75a592a47129dcfc1aec40e7d3cdf239a767d441
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/75a592a4.failed

early_pfn_in_nid() and meminit_pfn_in_nid() are small functions that are
unnecessarily visible outside memory initialisation.  As well as
unnecessary visibility, it's unnecessary function call overhead when
initialising pages.  This patch moves the helpers inline.

[akpm@linux-foundation.org: fix build]
[mhocko@suse.cz: fix build]
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Nate Zimmer <nzimmer@sgi.com>
	Tested-by: Waiman Long <waiman.long@hp.com>
	Tested-by: Daniel J Blueman <daniel@numascale.com>
	Acked-by: Pekka Enberg <penberg@kernel.org>
	Cc: Robin Holt <robinmholt@gmail.com>
	Cc: Nate Zimmer <nzimmer@sgi.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Waiman Long <waiman.long@hp.com>
	Cc: Scott Norton <scott.norton@hp.com>
	Cc: "Luck, Tony" <tony.luck@intel.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Michal Hocko <mhocko@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 75a592a47129dcfc1aec40e7d3cdf239a767d441)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmzone.h
#	mm/page_alloc.c
diff --cc include/linux/mmzone.h
index 2d47b2da44e8,1e05dc7449cd..000000000000
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@@ -1272,11 -1216,16 +1272,24 @@@ void sparse_init(void)
  #define sparse_index_init(_sec, _nid)  do {} while (0)
  #endif /* CONFIG_SPARSEMEM */
  
++<<<<<<< HEAD
 +#ifdef CONFIG_NODES_SPAN_OTHER_NODES
 +bool early_pfn_in_nid(unsigned long pfn, int nid);
 +#else
 +#define early_pfn_in_nid(pfn, nid)	(1)
 +#endif
++=======
+ /*
+  * During memory init memblocks map pfns to nids. The search is expensive and
+  * this caches recent lookups. The implementation of __early_pfn_to_nid
+  * may treat start/end as pfns or sections.
+  */
+ struct mminit_pfnnid_cache {
+ 	unsigned long last_start;
+ 	unsigned long last_end;
+ 	int last_nid;
+ };
++>>>>>>> 75a592a47129 (mm: meminit: inline some helper functions)
  
  #ifndef early_pfn_valid
  #define early_pfn_valid(pfn)	(1)
diff --cc mm/page_alloc.c
index 20d353397e7d,12a81870815f..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -781,8 -899,60 +781,60 @@@ void __init __free_pages_bootmem(struc
  	__free_pages(page, order);
  }
  
+ #if defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) || \
+ 	defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;
+ 
+ int __meminit early_pfn_to_nid(unsigned long pfn)
+ {
+ 	int nid;
+ 
+ 	/* The system will behave unpredictably otherwise */
+ 	BUG_ON(system_state != SYSTEM_BOOTING);
+ 
+ 	nid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);
+ 	if (nid >= 0)
+ 		return nid;
+ 	/* just returns 0 */
+ 	return 0;
+ }
+ #endif
+ 
+ #ifdef CONFIG_NODES_SPAN_OTHER_NODES
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	int nid;
+ 
+ 	nid = __early_pfn_to_nid(pfn, state);
+ 	if (nid >= 0 && nid != node)
+ 		return false;
+ 	return true;
+ }
+ 
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return meminit_pfn_in_nid(pfn, node, &early_pfnnid_cache);
+ }
+ 
+ #else
+ 
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return true;
+ }
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
  #ifdef CONFIG_CMA
 -/* Free whole pageblock and set its migration type to MIGRATE_CMA. */
 +/* Free whole pageblock and set it's migration type to MIGRATE_CMA. */
  void __init init_cma_reserved_pageblock(struct page *page)
  {
  	unsigned i = pageblock_nr_pages;
@@@ -4304,37 -4627,14 +4356,40 @@@ int __meminit __early_pfn_to_nid(unsign
  }
  #endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */
  
++<<<<<<< HEAD
 +int __meminit early_pfn_to_nid(unsigned long pfn)
 +{
 +	int nid;
 +
 +	nid = __early_pfn_to_nid(pfn);
 +	if (nid >= 0)
 +		return nid;
 +	/* just returns 0 */
 +	return 0;
 +}
 +
 +#ifdef CONFIG_NODES_SPAN_OTHER_NODES
 +bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
 +{
 +	int nid;
 +
 +	nid = __early_pfn_to_nid(pfn);
 +	if (nid >= 0 && nid != node)
 +		return false;
 +	return true;
 +}
 +#endif
 +
++=======
++>>>>>>> 75a592a47129 (mm: meminit: inline some helper functions)
  /**
 - * free_bootmem_with_active_regions - Call memblock_free_early_nid for each active range
 + * free_bootmem_with_active_regions - Call free_bootmem_node for each active range
   * @nid: The node to free memory on. If MAX_NUMNODES, all nodes are freed.
 - * @max_low_pfn: The highest PFN that will be passed to memblock_free_early_nid
 + * @max_low_pfn: The highest PFN that will be passed to free_bootmem_node
   *
 - * If an architecture guarantees that all ranges registered contain no holes
 - * and may be freed, this this function may be used instead of calling
 - * memblock_free_early_nid() manually.
 + * If an architecture guarantees that all ranges registered with
 + * add_active_ranges() contain no holes and may be freed, this
 + * this function may be used instead of calling free_bootmem() manually.
   */
  void __init free_bootmem_with_active_regions(int nid, unsigned long max_low_pfn)
  {
* Unmerged path include/linux/mmzone.h
* Unmerged path mm/page_alloc.c
