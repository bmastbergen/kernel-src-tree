sched: Use dl_bw_of() under RCU read lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Kirill Tkhai <ktkhai@parallels.com>
commit 66339c31bc3978d5fff9c4b4cb590a861def4db2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/66339c31.failed

dl_bw_of() dereferences rq->rd which has to have RCU read lock held.
Probability of use-after-free isn't zero here.

Also add lockdep assert into dl_bw_cpus().

	Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: <stable@vger.kernel.org> # v3.14+
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
Link: http://lkml.kernel.org/r/20140922183624.11015.71558.stgit@localhost
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 66339c31bc3978d5fff9c4b4cb590a861def4db2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index f8654b1100de,f0adb038170b..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -1974,8 -1998,125 +1974,126 @@@ void sched_fork(unsigned long clone_fla
  #endif
  
  	put_cpu();
 -	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ unsigned long to_ratio(u64 period, u64 runtime)
+ {
+ 	if (runtime == RUNTIME_INF)
+ 		return 1ULL << 20;
+ 
+ 	/*
+ 	 * Doing this here saves a lot of checks in all
+ 	 * the calling paths, and returning zero seems
+ 	 * safe for them anyway.
+ 	 */
+ 	if (period == 0)
+ 		return 0;
+ 
+ 	return div64_u64(runtime << 20, period);
+ }
+ 
+ #ifdef CONFIG_SMP
+ inline struct dl_bw *dl_bw_of(int i)
+ {
+ 	rcu_lockdep_assert(rcu_read_lock_sched_held(),
+ 			   "sched RCU must be held");
+ 	return &cpu_rq(i)->rd->dl_bw;
+ }
+ 
+ static inline int dl_bw_cpus(int i)
+ {
+ 	struct root_domain *rd = cpu_rq(i)->rd;
+ 	int cpus = 0;
+ 
+ 	rcu_lockdep_assert(rcu_read_lock_sched_held(),
+ 			   "sched RCU must be held");
+ 	for_each_cpu_and(i, rd->span, cpu_active_mask)
+ 		cpus++;
+ 
+ 	return cpus;
+ }
+ #else
+ inline struct dl_bw *dl_bw_of(int i)
+ {
+ 	return &cpu_rq(i)->dl.dl_bw;
+ }
+ 
+ static inline int dl_bw_cpus(int i)
+ {
+ 	return 1;
+ }
+ #endif
+ 
+ static inline
+ void __dl_clear(struct dl_bw *dl_b, u64 tsk_bw)
+ {
+ 	dl_b->total_bw -= tsk_bw;
+ }
+ 
+ static inline
+ void __dl_add(struct dl_bw *dl_b, u64 tsk_bw)
+ {
+ 	dl_b->total_bw += tsk_bw;
+ }
+ 
+ static inline
+ bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
+ {
+ 	return dl_b->bw != -1 &&
+ 	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
+ }
+ 
+ /*
+  * We must be sure that accepting a new task (or allowing changing the
+  * parameters of an existing one) is consistent with the bandwidth
+  * constraints. If yes, this function also accordingly updates the currently
+  * allocated bandwidth to reflect the new situation.
+  *
+  * This function is called while holding p's rq->lock.
+  */
+ static int dl_overflow(struct task_struct *p, int policy,
+ 		       const struct sched_attr *attr)
+ {
+ 
+ 	struct dl_bw *dl_b = dl_bw_of(task_cpu(p));
+ 	u64 period = attr->sched_period ?: attr->sched_deadline;
+ 	u64 runtime = attr->sched_runtime;
+ 	u64 new_bw = dl_policy(policy) ? to_ratio(period, runtime) : 0;
+ 	int cpus, err = -1;
+ 
+ 	if (new_bw == p->dl.dl_bw)
+ 		return 0;
+ 
+ 	/*
+ 	 * Either if a task, enters, leave, or stays -deadline but changes
+ 	 * its parameters, we may need to update accordingly the total
+ 	 * allocated bandwidth of the container.
+ 	 */
+ 	raw_spin_lock(&dl_b->lock);
+ 	cpus = dl_bw_cpus(task_cpu(p));
+ 	if (dl_policy(policy) && !task_has_dl_policy(p) &&
+ 	    !__dl_overflow(dl_b, cpus, 0, new_bw)) {
+ 		__dl_add(dl_b, new_bw);
+ 		err = 0;
+ 	} else if (dl_policy(policy) && task_has_dl_policy(p) &&
+ 		   !__dl_overflow(dl_b, cpus, p->dl.dl_bw, new_bw)) {
+ 		__dl_clear(dl_b, p->dl.dl_bw);
+ 		__dl_add(dl_b, new_bw);
+ 		err = 0;
+ 	} else if (!dl_policy(policy) && task_has_dl_policy(p)) {
+ 		__dl_clear(dl_b, p->dl.dl_bw);
+ 		err = 0;
+ 	}
+ 	raw_spin_unlock(&dl_b->lock);
+ 
+ 	return err;
+ }
+ 
+ extern void init_dl_bw(struct dl_bw *dl_b);
+ 
++>>>>>>> 66339c31bc39 (sched: Use dl_bw_of() under RCU read lock)
  /*
   * wake_up_new_task - wake up a newly created task for the first time.
   *
@@@ -7988,11 -7637,91 +8106,89 @@@ static int sched_rt_global_constraints(
  	}
  	raw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);
  
++<<<<<<< HEAD
++=======
+ 	return ret;
+ }
+ #endif /* CONFIG_RT_GROUP_SCHED */
+ 
+ static int sched_dl_global_constraints(void)
+ {
+ 	u64 runtime = global_rt_runtime();
+ 	u64 period = global_rt_period();
+ 	u64 new_bw = to_ratio(period, runtime);
+ 	int cpu, ret = 0;
+ 	unsigned long flags;
+ 
+ 	rcu_read_lock();
+ 
+ 	/*
+ 	 * Here we want to check the bandwidth not being set to some
+ 	 * value smaller than the currently allocated bandwidth in
+ 	 * any of the root_domains.
+ 	 *
+ 	 * FIXME: Cycling on all the CPUs is overdoing, but simpler than
+ 	 * cycling on root_domains... Discussion on different/better
+ 	 * solutions is welcome!
+ 	 */
+ 	for_each_possible_cpu(cpu) {
+ 		struct dl_bw *dl_b = dl_bw_of(cpu);
+ 
+ 		raw_spin_lock_irqsave(&dl_b->lock, flags);
+ 		if (new_bw < dl_b->total_bw)
+ 			ret = -EBUSY;
+ 		raw_spin_unlock_irqrestore(&dl_b->lock, flags);
+ 
+ 		if (ret)
+ 			break;
+ 	}
+ 
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ 
+ static void sched_dl_do_global(void)
+ {
+ 	u64 new_bw = -1;
+ 	int cpu;
+ 	unsigned long flags;
+ 
+ 	def_dl_bandwidth.dl_period = global_rt_period();
+ 	def_dl_bandwidth.dl_runtime = global_rt_runtime();
+ 
+ 	if (global_rt_runtime() != RUNTIME_INF)
+ 		new_bw = to_ratio(global_rt_period(), global_rt_runtime());
+ 
+ 	rcu_read_lock();
+ 	/*
+ 	 * FIXME: As above...
+ 	 */
+ 	for_each_possible_cpu(cpu) {
+ 		struct dl_bw *dl_b = dl_bw_of(cpu);
+ 
+ 		raw_spin_lock_irqsave(&dl_b->lock, flags);
+ 		dl_b->bw = new_bw;
+ 		raw_spin_unlock_irqrestore(&dl_b->lock, flags);
+ 	}
+ 	rcu_read_unlock();
+ }
+ 
+ static int sched_rt_global_validate(void)
+ {
+ 	if (sysctl_sched_rt_period <= 0)
+ 		return -EINVAL;
+ 
+ 	if ((sysctl_sched_rt_runtime != RUNTIME_INF) &&
+ 		(sysctl_sched_rt_runtime > sysctl_sched_rt_period))
+ 		return -EINVAL;
+ 
++>>>>>>> 66339c31bc39 (sched: Use dl_bw_of() under RCU read lock)
  	return 0;
  }
 +#endif /* CONFIG_RT_GROUP_SCHED */
  
 -static void sched_rt_do_global(void)
 -{
 -	def_rt_bandwidth.rt_runtime = global_rt_runtime();
 -	def_rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());
 -}
 -
 -int sched_rt_handler(struct ctl_table *table, int write,
 +int sched_rr_handler(struct ctl_table *table, int write,
  		void __user *buffer, size_t *lenp,
  		loff_t *ppos)
  {
* Unmerged path kernel/sched/core.c
