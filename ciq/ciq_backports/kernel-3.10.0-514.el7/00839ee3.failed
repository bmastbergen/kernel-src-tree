x86/mm: Move swap offset/type up in PTE to work around erratum

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] Move swap offset/type up in PTE to work around erratum (Larry Woodman) [1347159]
Rebuild_FUZZ: 93.10%
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit 00839ee3b299303c6a5e26a0a2485427a3afcbbf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/00839ee3.failed

This erratum can result in Accessed/Dirty getting set by the hardware
when we do not expect them to be (on !Present PTEs).

Instead of trying to fix them up after this happens, we just
allow the bits to get set and try to ignore them.  We do this by
shifting the layout of the bits we use for swap offset/type in
our 64-bit PTEs.

It looks like this:

 bitnrs: |     ...            | 11| 10|  9|8|7|6|5| 4| 3|2|1|0|
 names:  |     ...            |SW3|SW2|SW1|G|L|D|A|CD|WT|U|W|P|
 before: |         OFFSET (9-63)          |0|X|X| TYPE(1-5) |0|
  after: | OFFSET (14-63)  |  TYPE (9-13) |0|X|X|X| X| X|X|X|0|

Note that D was already a don't care (X) even before.  We just
move TYPE up and turn its old spot (which could be hit by the
A bit) into all don't cares.

We take 5 bits away from the offset, but that still leaves us
with 50 bits which lets us index into a 62-bit swapfile (4 EiB).
I think that's probably fine for the moment.  We could
theoretically reclaim 5 of the bits (1, 2, 3, 4, 7) but it
doesn't gain us anything.

	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Dave Hansen <dave@sr71.net>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Luis R. Rodriguez <mcgrof@suse.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Toshi Kani <toshi.kani@hp.com>
	Cc: dave.hansen@intel.com
	Cc: linux-mm@kvack.org
	Cc: mhocko@suse.com
Link: http://lkml.kernel.org/r/20160708001911.9A3FD2B6@viggo.jf.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 00839ee3b299303c6a5e26a0a2485427a3afcbbf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/pgtable_64.h
diff --cc arch/x86/include/asm/pgtable_64.h
index cdf76bfa8e19,7e8ec7ae10fa..000000000000
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@@ -151,14 -140,23 +151,34 @@@ static inline int pgd_large(pgd_t pgd) 
  #define pte_offset_map(dir, address) pte_offset_kernel((dir), (address))
  #define pte_unmap(pte) ((void)(pte))/* NOP */
  
++<<<<<<< HEAD
 +/* Encode and de-code a swap entry */
 +#if _PAGE_BIT_FILE < _PAGE_BIT_PROTNONE
 +#define SWP_TYPE_BITS (_PAGE_BIT_FILE - _PAGE_BIT_PRESENT - 1)
 +#define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 1)
 +#else
 +#define SWP_TYPE_BITS (_PAGE_BIT_PROTNONE - _PAGE_BIT_PRESENT - 1)
 +#define SWP_OFFSET_SHIFT (_PAGE_BIT_FILE + 1)
 +#endif
++=======
+ /*
+  * Encode and de-code a swap entry
+  *
+  * |     ...            | 11| 10|  9|8|7|6|5| 4| 3|2|1|0| <- bit number
+  * |     ...            |SW3|SW2|SW1|G|L|D|A|CD|WT|U|W|P| <- bit names
+  * | OFFSET (14->63) | TYPE (10-13) |0|X|X|X| X| X|X|X|0| <- swp entry
+  *
+  * G (8) is aliased and used as a PROT_NONE indicator for
+  * !present ptes.  We need to start storing swap entries above
+  * there.  We also need to avoid using A and D because of an
+  * erratum where they can be incorrectly set by hardware on
+  * non-present PTEs.
+  */
+ #define SWP_TYPE_FIRST_BIT (_PAGE_BIT_PROTNONE + 1)
+ #define SWP_TYPE_BITS 5
+ /* Place the offset above the type: */
+ #define SWP_OFFSET_FIRST_BIT (SWP_TYPE_FIRST_BIT + SWP_TYPE_BITS + 1)
++>>>>>>> 00839ee3b299 (x86/mm: Move swap offset/type up in PTE to work around erratum)
  
  #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > SWP_TYPE_BITS)
  
* Unmerged path arch/x86/include/asm/pgtable_64.h
