IB/qib: Remove qib multicast verbs functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit 18f6c582b366d3ec76317458f498e24a4379c299
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/18f6c582.failed

Multicast is now supported by rdmavt. Remove the verbs multicast functions
and use that.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 18f6c582b366d3ec76317458f498e24a4379c299)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/qib_verbs.c
#	drivers/infiniband/hw/qib/qib_verbs.h
#	drivers/infiniband/hw/qib/qib_verbs_mcast.c
diff --cc drivers/infiniband/hw/qib/qib_verbs.c
index c4417a1f33be,cbf5f88af882..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.c
+++ b/drivers/infiniband/hw/qib/qib_verbs.c
@@@ -2109,13 -1655,8 +2109,12 @@@ int qib_register_ib_device(struct qib_d
  		init_ibport(ppd + i);
  
  	/* Only need to initialize non-zero fields. */
 +	spin_lock_init(&dev->qpt_lock);
 +	spin_lock_init(&dev->n_pds_lock);
 +	spin_lock_init(&dev->n_ahs_lock);
 +	spin_lock_init(&dev->n_cqs_lock);
  	spin_lock_init(&dev->n_qps_lock);
  	spin_lock_init(&dev->n_srqs_lock);
- 	spin_lock_init(&dev->n_mcast_grps_lock);
  	init_timer(&dev->mem_timer);
  	dev->mem_timer.function = mem_timer;
  	dev->mem_timer.data = (unsigned long) dev;
@@@ -2248,29 -1762,27 +2247,48 @@@
  	ibdev->modify_qp = qib_modify_qp;
  	ibdev->query_qp = qib_query_qp;
  	ibdev->destroy_qp = qib_destroy_qp;
 -	ibdev->post_send = NULL;
 -	ibdev->post_recv = NULL;
 +	ibdev->post_send = qib_post_send;
 +	ibdev->post_recv = qib_post_receive;
  	ibdev->post_srq_recv = qib_post_srq_receive;
++<<<<<<< HEAD
 +	ibdev->create_cq = qib_create_cq;
 +	ibdev->destroy_cq = qib_destroy_cq;
 +	ibdev->resize_cq = qib_resize_cq;
 +	ibdev->poll_cq = qib_poll_cq;
 +	ibdev->req_notify_cq = qib_req_notify_cq;
 +	ibdev->get_dma_mr = qib_get_dma_mr;
 +	ibdev->reg_phys_mr = qib_reg_phys_mr;
 +	ibdev->reg_user_mr = qib_reg_user_mr;
 +	ibdev->dereg_mr = qib_dereg_mr;
 +	ibdev->alloc_mr = qib_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = qib_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = qib_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = qib_alloc_fmr;
 +	ibdev->map_phys_fmr = qib_map_phys_fmr;
 +	ibdev->unmap_fmr = qib_unmap_fmr;
 +	ibdev->dealloc_fmr = qib_dealloc_fmr;
 +	ibdev->attach_mcast = qib_multicast_attach;
 +	ibdev->detach_mcast = qib_multicast_detach;
++=======
+ 	ibdev->create_cq = NULL;
+ 	ibdev->destroy_cq = NULL;
+ 	ibdev->resize_cq = NULL;
+ 	ibdev->poll_cq = NULL;
+ 	ibdev->req_notify_cq = NULL;
+ 	ibdev->get_dma_mr = NULL;
+ 	ibdev->reg_user_mr = NULL;
+ 	ibdev->dereg_mr = NULL;
+ 	ibdev->alloc_mr = NULL;
+ 	ibdev->map_mr_sg = NULL;
+ 	ibdev->alloc_fmr = NULL;
+ 	ibdev->map_phys_fmr = NULL;
+ 	ibdev->unmap_fmr = NULL;
+ 	ibdev->dealloc_fmr = NULL;
+ 	ibdev->attach_mcast = NULL;
+ 	ibdev->detach_mcast = NULL;
++>>>>>>> 18f6c582b366 (IB/qib: Remove qib multicast verbs functions)
  	ibdev->process_mad = qib_process_mad;
 -	ibdev->mmap = NULL;
 +	ibdev->mmap = qib_mmap;
  	ibdev->dma_ops = NULL;
  	ibdev->get_port_immutable = qib_port_immutable;
  
diff --cc drivers/infiniband/hw/qib/qib_verbs.h
index ca366073af4f,e3610df1d8e3..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.h
+++ b/drivers/infiniband/hw/qib/qib_verbs.h
@@@ -204,213 -185,6 +204,216 @@@ struct qib_pio_header 
  } __packed;
  
  /*
++<<<<<<< HEAD
 + * There is one struct qib_mcast for each multicast GID.
 + * All attached QPs are then stored as a list of
 + * struct qib_mcast_qp.
 + */
 +struct qib_mcast_qp {
 +	struct list_head list;
 +	struct qib_qp *qp;
 +};
 +
 +struct qib_mcast {
 +	struct rb_node rb_node;
 +	union ib_gid mgid;
 +	struct list_head qp_list;
 +	wait_queue_head_t wait;
 +	atomic_t refcount;
 +	int n_attached;
 +};
 +
 +/* Protection domain */
 +struct qib_pd {
 +	struct ib_pd ibpd;
 +	int user;               /* non-zero if created from user space */
 +};
 +
 +/* Address Handle */
 +struct qib_ah {
 +	struct ib_ah ibah;
 +	struct ib_ah_attr attr;
 +	atomic_t refcount;
 +};
 +
 +/*
 + * This structure is used by qib_mmap() to validate an offset
 + * when an mmap() request is made.  The vm_area_struct then uses
 + * this as its vm_private_data.
 + */
 +struct qib_mmap_info {
 +	struct list_head pending_mmaps;
 +	struct ib_ucontext *context;
 +	void *obj;
 +	__u64 offset;
 +	struct kref ref;
 +	unsigned size;
 +};
 +
 +/*
 + * This structure is used to contain the head pointer, tail pointer,
 + * and completion queue entries as a single memory allocation so
 + * it can be mmap'ed into user space.
 + */
 +struct qib_cq_wc {
 +	u32 head;               /* index of next entry to fill */
 +	u32 tail;               /* index of next ib_poll_cq() entry */
 +	union {
 +		/* these are actually size ibcq.cqe + 1 */
 +		struct ib_uverbs_wc uqueue[0];
 +		struct ib_wc kqueue[0];
 +	};
 +};
 +
 +/*
 + * The completion queue structure.
 + */
 +struct qib_cq {
 +	struct ib_cq ibcq;
 +	struct kthread_work comptask;
 +	struct qib_devdata *dd;
 +	spinlock_t lock; /* protect changes in this struct */
 +	u8 notify;
 +	u8 triggered;
 +	struct qib_cq_wc *queue;
 +	struct qib_mmap_info *ip;
 +};
 +
 +/*
 + * A segment is a linear region of low physical memory.
 + * XXX Maybe we should use phys addr here and kmap()/kunmap().
 + * Used by the verbs layer.
 + */
 +struct qib_seg {
 +	void *vaddr;
 +	size_t length;
 +};
 +
 +/* The number of qib_segs that fit in a page. */
 +#define QIB_SEGSZ     (PAGE_SIZE / sizeof(struct qib_seg))
 +
 +struct qib_segarray {
 +	struct qib_seg segs[QIB_SEGSZ];
 +};
 +
 +struct qib_mregion {
 +	struct ib_pd *pd;       /* shares refcnt of ibmr.pd */
 +	u64 user_base;          /* User's address for this region */
 +	u64 iova;               /* IB start address of this region */
 +	size_t length;
 +	u32 lkey;
 +	u32 offset;             /* offset (bytes) to start of region */
 +	int access_flags;
 +	u32 max_segs;           /* number of qib_segs in all the arrays */
 +	u32 mapsz;              /* size of the map array */
 +	u8  page_shift;         /* 0 - non unform/non powerof2 sizes */
 +	u8  lkey_published;     /* in global table */
 +	struct completion comp; /* complete when refcount goes to zero */
 +	struct rcu_head list;
 +	atomic_t refcount;
 +	struct qib_segarray *map[0];    /* the segments */
 +};
 +
 +/*
 + * These keep track of the copy progress within a memory region.
 + * Used by the verbs layer.
 + */
 +struct qib_sge {
 +	struct qib_mregion *mr;
 +	void *vaddr;            /* kernel virtual address of segment */
 +	u32 sge_length;         /* length of the SGE */
 +	u32 length;             /* remaining length of the segment */
 +	u16 m;                  /* current index: mr->map[m] */
 +	u16 n;                  /* current index: mr->map[m]->segs[n] */
 +};
 +
 +/* Memory region */
 +struct qib_mr {
 +	struct ib_mr ibmr;
 +	struct ib_umem *umem;
 +	struct qib_mregion mr;  /* must be last */
 +};
 +
 +/*
 + * Send work request queue entry.
 + * The size of the sg_list is determined when the QP is created and stored
 + * in qp->s_max_sge.
 + */
 +struct qib_swqe {
 +	struct ib_send_wr wr;   /* don't use wr.sg_list */
 +	u32 psn;                /* first packet sequence number */
 +	u32 lpsn;               /* last packet sequence number */
 +	u32 ssn;                /* send sequence number */
 +	u32 length;             /* total length of data in sg_list */
 +	struct qib_sge sg_list[0];
 +};
 +
 +/*
 + * Receive work request queue entry.
 + * The size of the sg_list is determined when the QP (or SRQ) is created
 + * and stored in qp->r_rq.max_sge (or srq->rq.max_sge).
 + */
 +struct qib_rwqe {
 +	u64 wr_id;
 +	u8 num_sge;
 +	struct ib_sge sg_list[0];
 +};
 +
 +/*
 + * This structure is used to contain the head pointer, tail pointer,
 + * and receive work queue entries as a single memory allocation so
 + * it can be mmap'ed into user space.
 + * Note that the wq array elements are variable size so you can't
 + * just index into the array to get the N'th element;
 + * use get_rwqe_ptr() instead.
 + */
 +struct qib_rwq {
 +	u32 head;               /* new work requests posted to the head */
 +	u32 tail;               /* receives pull requests from here. */
 +	struct qib_rwqe wq[0];
 +};
 +
 +struct qib_rq {
 +	struct qib_rwq *wq;
 +	u32 size;               /* size of RWQE array */
 +	u8 max_sge;
 +	spinlock_t lock /* protect changes in this struct */
 +		____cacheline_aligned_in_smp;
 +};
 +
 +struct qib_srq {
 +	struct ib_srq ibsrq;
 +	struct qib_rq rq;
 +	struct qib_mmap_info *ip;
 +	/* send signal when number of RWQEs < limit */
 +	u32 limit;
 +};
 +
 +struct qib_sge_state {
 +	struct qib_sge *sg_list;      /* next SGE to be used if any */
 +	struct qib_sge sge;   /* progress state for the current SGE */
 +	u32 total_len;
 +	u8 num_sge;
 +};
 +
 +/*
 + * This structure holds the information that the send tasklet needs
 + * to send a RDMA read response or atomic operation.
 + */
 +struct qib_ack_entry {
 +	u8 opcode;
 +	u8 sent;
 +	u32 psn;
 +	u32 lpsn;
 +	union {
 +		struct qib_sge rdma_sge;
 +		u64 atomic_data;
 +	};
 +};
 +
 +/*
++=======
++>>>>>>> 18f6c582b366 (IB/qib: Remove qib multicast verbs functions)
   * qib specific data structure that will be hidden from rvt after the queue pair
   * is made common.
   */
@@@ -913,19 -359,9 +912,23 @@@ int qib_snapshot_counters(struct qib_pp
  int qib_get_counters(struct qib_pportdata *ppd,
  		     struct qib_verbs_counters *cntrs);
  
++<<<<<<< HEAD
 +int qib_multicast_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid);
 +
 +int qib_multicast_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid);
 +
 +int qib_mcast_tree_empty(struct qib_ibport *ibp);
 +
 +__be32 qib_compute_aeth(struct qib_qp *qp);
++=======
+ __be32 qib_compute_aeth(struct rvt_qp *qp);
++>>>>>>> 18f6c582b366 (IB/qib: Remove qib multicast verbs functions)
 +
 +struct qib_qp *qib_lookup_qpn(struct qib_ibport *ibp, u32 qpn);
  
 -struct rvt_qp *qib_lookup_qpn(struct qib_ibport *ibp, u32 qpn);
 +struct ib_qp *qib_create_qp(struct ib_pd *ibpd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata);
  
  int qib_destroy_qp(struct ib_qp *ibqp);
  
* Unmerged path drivers/infiniband/hw/qib/qib_verbs_mcast.c
diff --git a/drivers/infiniband/hw/qib/Makefile b/drivers/infiniband/hw/qib/Makefile
index 8a8f892de7df..9b66a4cad620 100644
--- a/drivers/infiniband/hw/qib/Makefile
+++ b/drivers/infiniband/hw/qib/Makefile
@@ -5,7 +5,7 @@ ib_qib-y := qib_cq.o qib_diag.o qib_driver.o qib_eeprom.o \
 	qib_mad.o qib_mmap.o qib_mr.o qib_pcie.o qib_pio_copy.o \
 	qib_qp.o qib_qsfp.o qib_rc.o qib_ruc.o qib_sdma.o qib_srq.o \
 	qib_sysfs.o qib_twsi.o qib_tx.o qib_uc.o qib_ud.o \
-	qib_user_pages.o qib_user_sdma.o qib_verbs_mcast.o qib_iba7220.o \
+	qib_user_pages.o qib_user_sdma.o qib_iba7220.o \
 	qib_sd7220.o qib_iba7322.o qib_verbs.o
 
 # 6120 has no fallback if no MSI interrupts, others can do INTx
diff --git a/drivers/infiniband/hw/qib/qib_qp.c b/drivers/infiniband/hw/qib/qib_qp.c
index cf1dd6e9d434..45625f85472b 100644
--- a/drivers/infiniband/hw/qib/qib_qp.c
+++ b/drivers/infiniband/hw/qib/qib_qp.c
@@ -303,8 +303,6 @@ unsigned qib_free_all_qps(struct qib_devdata *dd)
 	for (n = 0; n < dd->num_pports; n++) {
 		struct qib_ibport *ibp = &dd->pport[n].ibport_data;
 
-		if (!qib_mcast_tree_empty(ibp))
-			qp_inuse++;
 		rcu_read_lock();
 		if (rcu_dereference(ibp->qp0))
 			qp_inuse++;
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.h
* Unmerged path drivers/infiniband/hw/qib/qib_verbs_mcast.c
