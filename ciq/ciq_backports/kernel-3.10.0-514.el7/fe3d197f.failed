x86, mpx: On-demand kernel allocation of bounds tables

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] mpx: On-demand kernel allocation of bounds tables (Rui Wang) [1138650]
Rebuild_FUZZ: 95.15%
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit fe3d197f84319d3bce379a9c0dc17b1f48ad358c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/fe3d197f.failed

This is really the meat of the MPX patch set.  If there is one patch to
review in the entire series, this is the one.  There is a new ABI here
and this kernel code also interacts with userspace memory in a
relatively unusual manner.  (small FAQ below).

Long Description:

This patch adds two prctl() commands to provide enable or disable the
management of bounds tables in kernel, including on-demand kernel
allocation (See the patch "on-demand kernel allocation of bounds tables")
and cleanup (See the patch "cleanup unused bound tables"). Applications
do not strictly need the kernel to manage bounds tables and we expect
some applications to use MPX without taking advantage of this kernel
support. This means the kernel can not simply infer whether an application
needs bounds table management from the MPX registers.  The prctl() is an
explicit signal from userspace.

PR_MPX_ENABLE_MANAGEMENT is meant to be a signal from userspace to
require kernel's help in managing bounds tables.

PR_MPX_DISABLE_MANAGEMENT is the opposite, meaning that userspace don't
want kernel's help any more. With PR_MPX_DISABLE_MANAGEMENT, the kernel
won't allocate and free bounds tables even if the CPU supports MPX.

PR_MPX_ENABLE_MANAGEMENT will fetch the base address of the bounds
directory out of a userspace register (bndcfgu) and then cache it into
a new field (->bd_addr) in  the 'mm_struct'.  PR_MPX_DISABLE_MANAGEMENT
will set "bd_addr" to an invalid address.  Using this scheme, we can
use "bd_addr" to determine whether the management of bounds tables in
kernel is enabled.

Also, the only way to access that bndcfgu register is via an xsaves,
which can be expensive.  Caching "bd_addr" like this also helps reduce
the cost of those xsaves when doing table cleanup at munmap() time.
Unfortunately, we can not apply this optimization to #BR fault time
because we need an xsave to get the value of BNDSTATUS.

==== Why does the hardware even have these Bounds Tables? ====

MPX only has 4 hardware registers for storing bounds information.
If MPX-enabled code needs more than these 4 registers, it needs to
spill them somewhere. It has two special instructions for this
which allow the bounds to be moved between the bounds registers
and some new "bounds tables".

They are similar conceptually to a page fault and will be raised by
the MPX hardware during both bounds violations or when the tables
are not present. This patch handles those #BR exceptions for
not-present tables by carving the space out of the normal processes
address space (essentially calling the new mmap() interface indroduced
earlier in this patch set.) and then pointing the bounds-directory
over to it.

The tables *need* to be accessed and controlled by userspace because
the instructions for moving bounds in and out of them are extremely
frequent. They potentially happen every time a register pointing to
memory is dereferenced. Any direct kernel involvement (like a syscall)
to access the tables would obviously destroy performance.

==== Why not do this in userspace? ====

This patch is obviously doing this allocation in the kernel.
However, MPX does not strictly *require* anything in the kernel.
It can theoretically be done completely from userspace. Here are
a few ways this *could* be done. I don't think any of them are
practical in the real-world, but here they are.

Q: Can virtual space simply be reserved for the bounds tables so
   that we never have to allocate them?
A: As noted earlier, these tables are *HUGE*. An X-GB virtual
   area needs 4*X GB of virtual space, plus 2GB for the bounds
   directory. If we were to preallocate them for the 128TB of
   user virtual address space, we would need to reserve 512TB+2GB,
   which is larger than the entire virtual address space today.
   This means they can not be reserved ahead of time. Also, a
   single process's pre-popualated bounds directory consumes 2GB
   of virtual *AND* physical memory. IOW, it's completely
   infeasible to prepopulate bounds directories.

Q: Can we preallocate bounds table space at the same time memory
   is allocated which might contain pointers that might eventually
   need bounds tables?
A: This would work if we could hook the site of each and every
   memory allocation syscall. This can be done for small,
   constrained applications. But, it isn't practical at a larger
   scale since a given app has no way of controlling how all the
   parts of the app might allocate memory (think libraries). The
   kernel is really the only place to intercept these calls.

Q: Could a bounds fault be handed to userspace and the tables
   allocated there in a signal handler instead of in the kernel?
A: (thanks to tglx) mmap() is not on the list of safe async
   handler functions and even if mmap() would work it still
   requires locking or nasty tricks to keep track of the
   allocation state there.

Having ruled out all of the userspace-only approaches for managing
bounds tables that we could think of, we create them on demand in
the kernel.

Based-on-patch-by: Qiaowei Ren <qiaowei.ren@intel.com>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: linux-mm@kvack.org
	Cc: linux-mips@linux-mips.org
	Cc: Dave Hansen <dave@sr71.net>
Link: http://lkml.kernel.org/r/20141114151829.AD4310DE@viggo.jf.intel.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit fe3d197f84319d3bce379a9c0dc17b1f48ad358c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mpx.h
#	arch/x86/kernel/traps.c
#	arch/x86/mm/mpx.c
#	include/linux/mm_types.h
diff --cc arch/x86/kernel/traps.c
index 9823443df079,651d5d4f7558..000000000000
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@@ -58,6 -59,8 +58,11 @@@
  #include <asm/mce.h>
  #include <asm/fixmap.h>
  #include <asm/mach_traps.h>
++<<<<<<< HEAD
++=======
+ #include <asm/alternative.h>
+ #include <asm/mpx.h>
++>>>>>>> fe3d197f8431 (x86, mpx: On-demand kernel allocation of bounds tables)
  
  #ifdef CONFIG_X86_64
  #include <asm/x86_init.h>
@@@ -167,62 -201,42 +172,75 @@@ do_trap(int trapnr, int signr, char *st
  	}
  #endif
  
 -	force_sig_info(signr, info ?: SEND_SIG_PRIV, tsk);
 +	if (info)
 +		force_sig_info(signr, info, tsk);
 +	else
 +		force_sig(signr, tsk);
  }
 -NOKPROBE_SYMBOL(do_trap);
  
 -static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 -			  unsigned long trapnr, int signr)
 -{
 -	enum ctx_state prev_state = exception_enter();
 -	siginfo_t info;
 -
 -	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 -			NOTIFY_STOP) {
 -		conditional_sti(regs);
 -		do_trap(trapnr, signr, str, regs, error_code,
 -			fill_trap_info(regs, signr, trapnr, &info));
 -	}
 -
 -	exception_exit(prev_state);
 +#define DO_ERROR(trapnr, signr, str, name)				\
 +dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
 +{									\
 +	enum ctx_state prev_state;					\
 +									\
 +	prev_state = exception_enter();					\
 +	if (notify_die(DIE_TRAP, str, regs, error_code,			\
 +			trapnr, signr) == NOTIFY_STOP) {		\
 +		exception_exit(prev_state);				\
 +		return;							\
 +	}								\
 +	conditional_sti(regs);						\
 +	do_trap(trapnr, signr, str, regs, error_code, NULL);		\
 +	exception_exit(prev_state);					\
  }
  
 -#define DO_ERROR(trapnr, signr, str, name)				\
++<<<<<<< HEAD
 +#define DO_ERROR_INFO(trapnr, signr, str, name, sicode, siaddr)		\
  dotraplinkage void do_##name(struct pt_regs *regs, long error_code)	\
  {									\
 -	do_error_trap(regs, error_code, str, trapnr, signr);		\
 +	siginfo_t info;							\
 +	enum ctx_state prev_state;					\
 +									\
 +	info.si_signo = signr;						\
 +	info.si_errno = 0;						\
 +	info.si_code = sicode;						\
 +	info.si_addr = (void __user *)siaddr;				\
 +	prev_state = exception_enter();					\
 +	if (notify_die(DIE_TRAP, str, regs, error_code,			\
 +			trapnr, signr) == NOTIFY_STOP) {		\
 +		exception_exit(prev_state);				\
 +		return;							\
 +	}								\
 +	conditional_sti(regs);						\
 +	do_trap(trapnr, signr, str, regs, error_code, &info);		\
 +	exception_exit(prev_state);					\
  }
  
 +DO_ERROR_INFO(X86_TRAP_DE, SIGFPE, "divide error", divide_error, FPE_INTDIV,
 +		regs->ip)
 +DO_ERROR(X86_TRAP_OF, SIGSEGV, "overflow", overflow)
 +DO_ERROR(X86_TRAP_BR, SIGSEGV, "bounds", bounds)
 +DO_ERROR_INFO(X86_TRAP_UD, SIGILL, "invalid opcode", invalid_op, ILL_ILLOPN,
 +		regs->ip)
 +DO_ERROR(X86_TRAP_OLD_MF, SIGFPE, "coprocessor segment overrun",
 +		coprocessor_segment_overrun)
 +DO_ERROR(X86_TRAP_TS, SIGSEGV, "invalid TSS", invalid_TSS)
 +DO_ERROR(X86_TRAP_NP, SIGBUS, "segment not present", segment_not_present)
 +DO_ERROR(X86_TRAP_SS, SIGBUS, "stack segment", stack_segment)
 +DO_ERROR_INFO(X86_TRAP_AC, SIGBUS, "alignment check", alignment_check,
 +		BUS_ADRALN, 0)
++=======
+ DO_ERROR(X86_TRAP_DE,     SIGFPE,  "divide error",		divide_error)
+ DO_ERROR(X86_TRAP_OF,     SIGSEGV, "overflow",			overflow)
+ DO_ERROR(X86_TRAP_UD,     SIGILL,  "invalid opcode",		invalid_op)
+ DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  "coprocessor segment overrun",coprocessor_segment_overrun)
+ DO_ERROR(X86_TRAP_TS,     SIGSEGV, "invalid TSS",		invalid_TSS)
+ DO_ERROR(X86_TRAP_NP,     SIGBUS,  "segment not present",	segment_not_present)
+ #ifdef CONFIG_X86_32
+ DO_ERROR(X86_TRAP_SS,     SIGBUS,  "stack segment",		stack_segment)
+ #endif
+ DO_ERROR(X86_TRAP_AC,     SIGBUS,  "alignment check",		alignment_check)
++>>>>>>> fe3d197f8431 (x86, mpx: On-demand kernel allocation of bounds tables)
  
  #ifdef CONFIG_X86_64
  /* Runs on IST stack */
@@@ -247,7 -278,90 +265,94 @@@ dotraplinkage void do_double_fault(stru
  }
  #endif
  
++<<<<<<< HEAD
 +dotraplinkage void __kprobes
++=======
+ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
+ {
+ 	struct task_struct *tsk = current;
+ 	struct xsave_struct *xsave_buf;
+ 	enum ctx_state prev_state;
+ 	struct bndcsr *bndcsr;
+ 	siginfo_t *info;
+ 
+ 	prev_state = exception_enter();
+ 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
+ 			X86_TRAP_BR, SIGSEGV) == NOTIFY_STOP)
+ 		goto exit;
+ 	conditional_sti(regs);
+ 
+ 	if (!user_mode(regs))
+ 		die("bounds", regs, error_code);
+ 
+ 	if (!cpu_feature_enabled(X86_FEATURE_MPX)) {
+ 		/* The exception is not from Intel MPX */
+ 		goto exit_trap;
+ 	}
+ 
+ 	/*
+ 	 * We need to look at BNDSTATUS to resolve this exception.
+ 	 * It is not directly accessible, though, so we need to
+ 	 * do an xsave and then pull it out of the xsave buffer.
+ 	 */
+ 	fpu_save_init(&tsk->thread.fpu);
+ 	xsave_buf = &(tsk->thread.fpu.state->xsave);
+ 	bndcsr = get_xsave_addr(xsave_buf, XSTATE_BNDCSR);
+ 	if (!bndcsr)
+ 		goto exit_trap;
+ 
+ 	/*
+ 	 * The error code field of the BNDSTATUS register communicates status
+ 	 * information of a bound range exception #BR or operation involving
+ 	 * bound directory.
+ 	 */
+ 	switch (bndcsr->bndstatus & MPX_BNDSTA_ERROR_CODE) {
+ 	case 2:	/* Bound directory has invalid entry. */
+ 		if (mpx_handle_bd_fault(xsave_buf))
+ 			goto exit_trap;
+ 		break; /* Success, it was handled */
+ 	case 1: /* Bound violation. */
+ 		info = mpx_generate_siginfo(regs, xsave_buf);
+ 		if (PTR_ERR(info)) {
+ 			/*
+ 			 * We failed to decode the MPX instruction.  Act as if
+ 			 * the exception was not caused by MPX.
+ 			 */
+ 			goto exit_trap;
+ 		}
+ 		/*
+ 		 * Success, we decoded the instruction and retrieved
+ 		 * an 'info' containing the address being accessed
+ 		 * which caused the exception.  This information
+ 		 * allows and application to possibly handle the
+ 		 * #BR exception itself.
+ 		 */
+ 		do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, info);
+ 		kfree(info);
+ 		break;
+ 	case 0: /* No exception caused by Intel MPX operations. */
+ 		goto exit_trap;
+ 	default:
+ 		die("bounds", regs, error_code);
+ 	}
+ 
+ exit:
+ 	exception_exit(prev_state);
+ 	return;
+ exit_trap:
+ 	/*
+ 	 * This path out is for all the cases where we could not
+ 	 * handle the exception in some way (like allocating a
+ 	 * table or telling userspace about it.  We will also end
+ 	 * up here if the kernel has MPX turned off at compile
+ 	 * time..
+ 	 */
+ 	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, NULL);
+ 	exception_exit(prev_state);
+ }
+ 
+ dotraplinkage void
++>>>>>>> fe3d197f8431 (x86, mpx: On-demand kernel allocation of bounds tables)
  do_general_protection(struct pt_regs *regs, long error_code)
  {
  	struct task_struct *tsk;
diff --cc include/linux/mm_types.h
index 43f423b198b4,004e9d17b47e..000000000000
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@@ -466,23 -454,10 +466,30 @@@ struct mm_struct 
  	bool tlb_flush_pending;
  #endif
  	struct uprobes_state uprobes_state;
++<<<<<<< HEAD
 +
 +	/* reserved for Red Hat */
 +#if defined(__GENKSYMS__) || !defined(CONFIG_SPAPR_TCE_IOMMU)
 +	/* We're adding a list_head, so we need to take two reserved
 +	 * fields, unfortunately there are no handy RH_KABI macros for
 +	 * that case */
 +	RH_KABI_RESERVE(1)
 +	RH_KABI_RESERVE(2)
 +#else
 +	struct list_head iommu_group_mem_list;
 +#endif
 +	RH_KABI_RESERVE(3)
 +	RH_KABI_RESERVE(4)
 +	RH_KABI_RESERVE(5)
 +	RH_KABI_RESERVE(6)
 +	RH_KABI_RESERVE(7)
 +	RH_KABI_RESERVE(8)
++=======
+ #ifdef CONFIG_X86_INTEL_MPX
+ 	/* address of the bounds directory */
+ 	void __user *bd_addr;
+ #endif
++>>>>>>> fe3d197f8431 (x86, mpx: On-demand kernel allocation of bounds tables)
  };
  
  static inline void mm_init_cpumask(struct mm_struct *mm)
* Unmerged path arch/x86/include/asm/mpx.h
* Unmerged path arch/x86/mm/mpx.c
diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
index be12c534fd59..24ebf19eae0a 100644
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@ -6,6 +6,7 @@
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
 #include <asm/paravirt.h>
+#include <asm/mpx.h>
 #ifndef CONFIG_PARAVIRT
 #include <asm-generic/mm_hooks.h>
 
@@ -96,4 +97,10 @@ do {						\
 } while (0)
 #endif
 
+static inline void arch_bprm_mm_init(struct mm_struct *mm,
+		struct vm_area_struct *vma)
+{
+	mpx_mm_init(mm);
+}
+
 #endif /* _ASM_X86_MMU_CONTEXT_H */
* Unmerged path arch/x86/include/asm/mpx.h
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index ddad3b312de4..134c66b68604 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -950,6 +950,24 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 extern int get_tsc_mode(unsigned long adr);
 extern int set_tsc_mode(unsigned int val);
 
+/* Register/unregister a process' MPX related resource */
+#define MPX_ENABLE_MANAGEMENT(tsk)	mpx_enable_management((tsk))
+#define MPX_DISABLE_MANAGEMENT(tsk)	mpx_disable_management((tsk))
+
+#ifdef CONFIG_X86_INTEL_MPX
+extern int mpx_enable_management(struct task_struct *tsk);
+extern int mpx_disable_management(struct task_struct *tsk);
+#else
+static inline int mpx_enable_management(struct task_struct *tsk)
+{
+	return -EINVAL;
+}
+static inline int mpx_disable_management(struct task_struct *tsk)
+{
+	return -EINVAL;
+}
+#endif /* CONFIG_X86_INTEL_MPX */
+
 extern u16 amd_get_nb_id(int cpu);
 
 static inline uint32_t hypervisor_cpuid_base(const char *sig, uint32_t leaves)
diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c
index ae85211f032f..c9200e068e68 100644
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@ -1024,6 +1024,8 @@ void __init setup_arch(char **cmdline_p)
 	init_mm.end_data = (unsigned long) _edata;
 	init_mm.brk = _brk_end;
 
+	mpx_mm_init(&init_mm);
+
 	code_resource.start = __pa_symbol(_text);
 	code_resource.end = __pa_symbol(_etext)-1;
 	data_resource.start = __pa_symbol(_etext);
* Unmerged path arch/x86/kernel/traps.c
* Unmerged path arch/x86/mm/mpx.c
diff --git a/fs/exec.c b/fs/exec.c
index a2f61737d490..3e6d00d51167 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -59,6 +59,7 @@
 #include <asm/uaccess.h>
 #include <asm/mmu_context.h>
 #include <asm/tlb.h>
+#include <asm/mpx.h>
 
 #include <trace/events/task.h>
 #include "internal.h"
@@ -274,6 +275,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 		goto err;
 
 	mm->stack_vm = mm->total_vm = 1;
+	arch_bprm_mm_init(mm, vma);
 	up_write(&mm->mmap_sem);
 	bprm->p = vma->vm_end - sizeof(void *);
 	return 0;
diff --git a/include/asm-generic/mmu_context.h b/include/asm-generic/mmu_context.h
index a7eec910ba6c..1f2a8f9c9264 100644
--- a/include/asm-generic/mmu_context.h
+++ b/include/asm-generic/mmu_context.h
@@ -42,4 +42,9 @@ static inline void activate_mm(struct mm_struct *prev_mm,
 {
 }
 
+static inline void arch_bprm_mm_init(struct mm_struct *mm,
+			struct vm_area_struct *vma)
+{
+}
+
 #endif /* __ASM_GENERIC_MMU_CONTEXT_H */
* Unmerged path include/linux/mm_types.h
diff --git a/include/uapi/linux/prctl.h b/include/uapi/linux/prctl.h
index 513df75d0fc9..89f63503f903 100644
--- a/include/uapi/linux/prctl.h
+++ b/include/uapi/linux/prctl.h
@@ -179,4 +179,10 @@ struct prctl_mm_map {
 #define PR_SET_THP_DISABLE	41
 #define PR_GET_THP_DISABLE	42
 
+/*
+ * Tell the kernel to start/stop helping userspace manage bounds tables.
+ */
+#define PR_MPX_ENABLE_MANAGEMENT  43
+#define PR_MPX_DISABLE_MANAGEMENT 44
+
 #endif /* _LINUX_PRCTL_H */
diff --git a/kernel/sys.c b/kernel/sys.c
index 5149ddb9041c..34b2599a953d 100644
--- a/kernel/sys.c
+++ b/kernel/sys.c
@@ -92,6 +92,12 @@
 #ifndef SET_TSC_CTL
 # define SET_TSC_CTL(a)		(-EINVAL)
 #endif
+#ifndef MPX_ENABLE_MANAGEMENT
+# define MPX_ENABLE_MANAGEMENT(a)	(-EINVAL)
+#endif
+#ifndef MPX_DISABLE_MANAGEMENT
+# define MPX_DISABLE_MANAGEMENT(a)	(-EINVAL)
+#endif
 
 /*
  * this is where the system-wide overflow UID and GID are defined, for
@@ -2464,6 +2470,12 @@ SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,
 			me->mm->def_flags &= ~VM_NOHUGEPAGE;
 		up_write(&me->mm->mmap_sem);
 		break;
+	case PR_MPX_ENABLE_MANAGEMENT:
+		error = MPX_ENABLE_MANAGEMENT(me);
+		break;
+	case PR_MPX_DISABLE_MANAGEMENT:
+		error = MPX_DISABLE_MANAGEMENT(me);
+		break;
 	default:
 		error = -EINVAL;
 		break;
