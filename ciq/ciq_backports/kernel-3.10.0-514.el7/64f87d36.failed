vxlan: consolidate GBP handling even more

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jiri Benc <jbenc@redhat.com>
commit 64f87d3616a01c53262c6e0e948d62df15923f1c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/64f87d36.failed

Now when the packet is scrubbed early, skb->mark can be set in the GBP
handling code.

	Signed-off-by: Jiri Benc <jbenc@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 64f87d3616a01c53262c6e0e948d62df15923f1c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/vxlan.c
diff --cc drivers/net/vxlan.c
index 000e4c57a81e,656a303c0ac8..000000000000
--- a/drivers/net/vxlan.c
+++ b/drivers/net/vxlan.c
@@@ -1133,119 -1144,56 +1133,134 @@@ static struct vxlanhdr *vxlan_remcsum(s
  {
  	size_t start, offset, plen;
  
 -	if (!(unparsed->vx_flags & VXLAN_HF_RCO) || skb->remcsum_offload)
 -		goto out;
 +	if (skb->remcsum_offload)
 +		return vh;
  
 -	start = vxlan_rco_start(unparsed->vx_vni);
 -	offset = start + vxlan_rco_offset(unparsed->vx_vni);
 +	start = (data & VXLAN_RCO_MASK) << VXLAN_RCO_SHIFT;
 +	offset = start + ((data & VXLAN_RCO_UDP) ?
 +			  offsetof(struct udphdr, check) :
 +			  offsetof(struct tcphdr, check));
  
 -	plen = sizeof(struct vxlanhdr) + offset + sizeof(u16);
 +	plen = hdrlen + offset + sizeof(u16);
  
  	if (!pskb_may_pull(skb, plen))
 -		return false;
 +		return NULL;
  
 -	skb_remcsum_process(skb, (void *)(vxlan_hdr(skb) + 1), start, offset,
 -			    !!(vxflags & VXLAN_F_REMCSUM_NOPARTIAL));
 -out:
 -	unparsed->vx_flags &= ~VXLAN_HF_RCO;
 -	unparsed->vx_vni &= VXLAN_VNI_MASK;
 -	return true;
 +	vh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
 +
 +	skb_remcsum_process(skb, (void *)vh + hdrlen, start, offset,
 +			    nopartial);
 +
 +	return vh;
  }
  
++<<<<<<< HEAD
 +/* Callback from net/ipv4/udp.c to receive packets */
 +static int vxlan_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
++=======
+ static void vxlan_parse_gbp_hdr(struct vxlanhdr *unparsed,
+ 				struct sk_buff *skb, u32 vxflags,
+ 				struct vxlan_metadata *md,
+ 				struct metadata_dst *tun_dst)
++>>>>>>> 64f87d3616a0 (vxlan: consolidate GBP handling even more)
  {
 -	struct vxlanhdr_gbp *gbp = (struct vxlanhdr_gbp *)unparsed;
 +	struct vxlan_sock *vs;
 +	struct vxlanhdr *vxh;
 +	u32 flags, vni;
 +	struct vxlan_metadata md = {0};
  
 -	if (!(unparsed->vx_flags & VXLAN_HF_GBP))
 -		goto out;
 +	/* Need Vxlan and inner Ethernet header to be present */
 +	if (!pskb_may_pull(skb, VXLAN_HLEN))
 +		goto error;
  
 -	md->gbp = ntohs(gbp->policy_id);
 +	vxh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
 +	flags = ntohl(vxh->vx_flags);
 +	vni = ntohl(vxh->vx_vni);
  
 -	if (tun_dst)
 -		tun_dst->u.tun_info.key.tun_flags |= TUNNEL_VXLAN_OPT;
 +	if (flags & VXLAN_HF_VNI) {
 +		flags &= ~VXLAN_HF_VNI;
 +	} else {
 +		/* VNI flag always required to be set */
 +		goto bad_flags;
 +	}
 +
 +	if (iptunnel_pull_header(skb, VXLAN_HLEN, htons(ETH_P_TEB)))
 +		goto drop;
 +	vxh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
 +
 +	vs = rcu_dereference_sk_user_data(sk);
 +	if (!vs)
 +		goto drop;
 +
++<<<<<<< HEAD
 +	if ((flags & VXLAN_HF_RCO) && (vs->flags & VXLAN_F_REMCSUM_RX)) {
 +		vxh = vxlan_remcsum(skb, vxh, sizeof(struct vxlanhdr), vni,
 +				    !!(vs->flags & VXLAN_F_REMCSUM_NOPARTIAL));
 +		if (!vxh)
 +			goto drop;
 +
 +		flags &= ~VXLAN_HF_RCO;
 +		vni &= VXLAN_VNI_MASK;
 +	}
 +
 +	/* For backwards compatibility, only allow reserved fields to be
 +	 * used by VXLAN extensions if explicitly requested.
 +	 */
 +	if ((flags & VXLAN_HF_GBP) && (vs->flags & VXLAN_F_GBP)) {
 +		struct vxlanhdr_gbp *gbp;
 +
 +		gbp = (struct vxlanhdr_gbp *)vxh;
 +		md.gbp = ntohs(gbp->policy_id);
 +
 +		if (gbp->dont_learn)
 +			md.gbp |= VXLAN_GBP_DONT_LEARN;
 +
 +		if (gbp->policy_applied)
 +			md.gbp |= VXLAN_GBP_POLICY_APPLIED;
 +
 +		flags &= ~VXLAN_GBP_USED_BITS;
 +	}
 +
 +	if (flags || vni & ~VXLAN_VNI_MASK) {
 +		/* If there are any unprocessed flags remaining treat
 +		 * this as a malformed packet. This behavior diverges from
 +		 * VXLAN RFC (RFC7348) which stipulates that bits in reserved
 +		 * in reserved fields are to be ignored. The approach here
 +		 * maintains compatibility with previous stack code, and also
 +		 * is more robust and provides a little more security in
 +		 * adding extensions to VXLAN.
 +		 */
 +
 +		goto bad_flags;
 +	}
 +
 +	md.vni = vxh->vx_vni;
 +	vs->rcv(vs, skb, &md);
 +	return 0;
  
 -	if (gbp->dont_learn)
 -		md->gbp |= VXLAN_GBP_DONT_LEARN;
 +drop:
 +	/* Consume bad packet */
 +	kfree_skb(skb);
 +	return 0;
  
 -	if (gbp->policy_applied)
 -		md->gbp |= VXLAN_GBP_POLICY_APPLIED;
 +bad_flags:
 +	netdev_dbg(skb->dev, "invalid vxlan flags=%#x vni=%#x\n",
 +		   ntohl(vxh->vx_flags), ntohl(vxh->vx_vni));
  
 +error:
 +	/* Return non vxlan pkt */
 +	return 1;
++=======
+ 	/* In flow-based mode, GBP is carried in dst_metadata */
+ 	if (!(vxflags & VXLAN_F_COLLECT_METADATA))
+ 		skb->mark = md->gbp;
+ out:
+ 	unparsed->vx_flags &= ~VXLAN_GBP_USED_BITS;
++>>>>>>> 64f87d3616a0 (vxlan: consolidate GBP handling even more)
  }
  
 -static void vxlan_rcv(struct vxlan_dev *vxlan, struct vxlan_sock *vs,
 -		      struct sk_buff *skb, struct vxlan_metadata *md,
 -		      struct metadata_dst *tun_dst)
 +static void vxlan_rcv(struct vxlan_sock *vs, struct sk_buff *skb,
 +		      struct vxlan_metadata *md)
  {
  	struct iphdr *oip = NULL;
  	struct ipv6hdr *oip6 = NULL;
@@@ -1290,7 -1232,6 +1305,10 @@@
  		goto drop;
  
  	skb_reset_network_header(skb);
++<<<<<<< HEAD
 +	skb->mark = md->gbp;
++=======
++>>>>>>> 64f87d3616a0 (vxlan: consolidate GBP handling even more)
  
  	if (oip6)
  		err = IP6_ECN_decapsulate(oip6, skb);
@@@ -1327,6 -1271,91 +1345,94 @@@ drop
  	kfree_skb(skb);
  }
  
++<<<<<<< HEAD
++=======
+ /* Callback from net/ipv4/udp.c to receive packets */
+ static int vxlan_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct metadata_dst *tun_dst = NULL;
+ 	struct vxlan_dev *vxlan;
+ 	struct vxlan_sock *vs;
+ 	struct vxlanhdr unparsed;
+ 	struct vxlan_metadata _md;
+ 	struct vxlan_metadata *md = &_md;
+ 
+ 	/* Need Vxlan and inner Ethernet header to be present */
+ 	if (!pskb_may_pull(skb, VXLAN_HLEN))
+ 		return 1;
+ 
+ 	unparsed = *vxlan_hdr(skb);
+ 	/* VNI flag always required to be set */
+ 	if (!(unparsed.vx_flags & VXLAN_HF_VNI)) {
+ 		netdev_dbg(skb->dev, "invalid vxlan flags=%#x vni=%#x\n",
+ 			   ntohl(vxlan_hdr(skb)->vx_flags),
+ 			   ntohl(vxlan_hdr(skb)->vx_vni));
+ 		/* Return non vxlan pkt */
+ 		return 1;
+ 	}
+ 	unparsed.vx_flags &= ~VXLAN_HF_VNI;
+ 	unparsed.vx_vni &= ~VXLAN_VNI_MASK;
+ 
+ 	vs = rcu_dereference_sk_user_data(sk);
+ 	if (!vs)
+ 		goto drop;
+ 
+ 	vxlan = vxlan_vs_find_vni(vs, vxlan_vni(vxlan_hdr(skb)->vx_vni));
+ 	if (!vxlan)
+ 		goto drop;
+ 
+ 	if (iptunnel_pull_header(skb, VXLAN_HLEN, htons(ETH_P_TEB),
+ 				 !net_eq(vxlan->net, dev_net(vxlan->dev))))
+ 		goto drop;
+ 
+ 	if (vxlan_collect_metadata(vs)) {
+ 		__be32 vni = vxlan_vni(vxlan_hdr(skb)->vx_vni);
+ 
+ 		tun_dst = udp_tun_rx_dst(skb, vxlan_get_sk_family(vs), TUNNEL_KEY,
+ 					 vxlan_vni_to_tun_id(vni), sizeof(*md));
+ 
+ 		if (!tun_dst)
+ 			goto drop;
+ 
+ 		md = ip_tunnel_info_opts(&tun_dst->u.tun_info);
+ 	} else {
+ 		memset(md, 0, sizeof(*md));
+ 	}
+ 
+ 	/* For backwards compatibility, only allow reserved fields to be
+ 	 * used by VXLAN extensions if explicitly requested.
+ 	 */
+ 	if (vs->flags & VXLAN_F_REMCSUM_RX)
+ 		if (!vxlan_remcsum(&unparsed, skb, vs->flags))
+ 			goto drop;
+ 	if (vs->flags & VXLAN_F_GBP)
+ 		vxlan_parse_gbp_hdr(&unparsed, skb, vs->flags, md, tun_dst);
+ 
+ 	if (unparsed.vx_flags || unparsed.vx_vni) {
+ 		/* If there are any unprocessed flags remaining treat
+ 		 * this as a malformed packet. This behavior diverges from
+ 		 * VXLAN RFC (RFC7348) which stipulates that bits in reserved
+ 		 * in reserved fields are to be ignored. The approach here
+ 		 * maintains compatibility with previous stack code, and also
+ 		 * is more robust and provides a little more security in
+ 		 * adding extensions to VXLAN.
+ 		 */
+ 		goto drop;
+ 	}
+ 
+ 	vxlan_rcv(vxlan, vs, skb, md, tun_dst);
+ 	return 0;
+ 
+ drop:
+ 	if (tun_dst)
+ 		dst_release((struct dst_entry *)tun_dst);
+ 
+ 	/* Consume bad packet */
+ 	kfree_skb(skb);
+ 	return 0;
+ }
+ 
++>>>>>>> 64f87d3616a0 (vxlan: consolidate GBP handling even more)
  static int arp_reduce(struct net_device *dev, struct sk_buff *skb)
  {
  	struct vxlan_dev *vxlan = netdev_priv(dev);
* Unmerged path drivers/net/vxlan.c
