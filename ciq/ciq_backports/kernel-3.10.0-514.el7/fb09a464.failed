mm: consolidate code to call vm_ops->page_mkwrite()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] consolidate code to call vm_ops->page_mkwrite() (Eric Sandeen) [1274459]
Rebuild_FUZZ: 95.92%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit fb09a46425823604bf337d2c9999756f9b753cf1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/fb09a464.failed

There are two functions which need to call vm_ops->page_mkwrite():
do_shared_fault() and do_wp_page().  We can consolidate preparation
code.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fb09a46425823604bf337d2c9999756f9b753cf1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,ac3990d77ec9..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3582,128 -3314,167 +3583,236 @@@ static int __do_fault(struct mm_struct 
  	else
  		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
  
 -	*page = vmf.page;
 -	return ret;
 -}
 -
 -static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 -		unsigned long address, pmd_t *pmd,
 -		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
 -{
 -	struct page *fault_page;
 -	spinlock_t *ptl;
 -	pte_t entry, *pte;
 -	int ret;
 -
 -	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
 -	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 -		return ret;
 +	/*
 +	 * Should we do an early C-O-W break?
 +	 */
 +	page = vmf.page;
 +	if (flags & FAULT_FLAG_WRITE) {
 +		if (!(vma->vm_flags & VM_SHARED)) {
 +			page = cow_page;
 +			anon = 1;
 +			copy_user_highpage(page, vmf.page, address, vma);
 +			__SetPageUptodate(page);
 +		} else {
 +			/*
 +			 * If the page will be shareable, see if the backing
 +			 * address space wants to know that the page is about
 +			 * to become writable
 +			 */
 +			if (vma->vm_ops->page_mkwrite) {
 +				int tmp;
 +
 +				unlock_page(page);
 +				vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
 +				tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
 +				if (unlikely(tmp &
 +					  (VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
 +					ret = tmp;
 +					goto unwritable_page;
 +				}
 +				if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
 +					lock_page(page);
 +					if (!page->mapping) {
 +						ret = 0; /* retry the fault */
 +						unlock_page(page);
 +						goto unwritable_page;
 +					}
 +				} else
 +					VM_BUG_ON_PAGE(!PageLocked(page), page);
 +				page_mkwrite = 1;
 +			}
 +		}
  
 -	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 -	if (unlikely(!pte_same(*pte, orig_pte))) {
 -		pte_unmap_unlock(pte, ptl);
 -		unlock_page(fault_page);
 -		page_cache_release(fault_page);
 -		return ret;
  	}
  
++<<<<<<< HEAD
 +	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
++=======
+ 	flush_icache_page(vma, fault_page);
+ 	entry = mk_pte(fault_page, vma->vm_page_prot);
+ 	if (pte_file(orig_pte) && pte_file_soft_dirty(orig_pte))
+ 		pte_mksoft_dirty(entry);
+ 	inc_mm_counter_fast(mm, MM_FILEPAGES);
+ 	page_add_file_rmap(fault_page);
+ 	set_pte_at(mm, address, pte, entry);
+ 
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 	pte_unmap_unlock(pte, ptl);
+ 	unlock_page(fault_page);
+ 
+ 	return ret;
+ }
+ 
+ static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pmd_t *pmd,
+ 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
+ {
+ 	struct page *fault_page, *new_page;
+ 	spinlock_t *ptl;
+ 	pte_t entry, *pte;
+ 	int ret;
+ 
+ 	if (unlikely(anon_vma_prepare(vma)))
+ 		return VM_FAULT_OOM;
+ 
+ 	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
+ 	if (!new_page)
+ 		return VM_FAULT_OOM;
+ 
+ 	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL)) {
+ 		page_cache_release(new_page);
+ 		return VM_FAULT_OOM;
+ 	}
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		goto uncharge_out;
+ 
+ 	copy_user_highpage(new_page, fault_page, address, vma);
+ 	__SetPageUptodate(new_page);
+ 
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		goto uncharge_out;
+ 	}
+ 
+ 	flush_icache_page(vma, new_page);
+ 	entry = mk_pte(new_page, vma->vm_page_prot);
+ 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	inc_mm_counter_fast(mm, MM_ANONPAGES);
+ 	page_add_new_anon_rmap(new_page, vma, address);
+ 	set_pte_at(mm, address, pte, entry);
+ 
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 
+ 	pte_unmap_unlock(pte, ptl);
+ 	unlock_page(fault_page);
+ 	page_cache_release(fault_page);
+ 	return ret;
+ uncharge_out:
+ 	mem_cgroup_uncharge_page(new_page);
+ 	page_cache_release(new_page);
+ 	return ret;
+ }
+ 
+ static int do_shared_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pmd_t *pmd,
+ 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
+ {
+ 	struct page *fault_page;
+ 	struct address_space *mapping;
+ 	spinlock_t *ptl;
+ 	pte_t entry, *pte;
+ 	int dirtied = 0;
+ 	int ret, tmp;
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
++>>>>>>> fb09a4642582 (mm: consolidate code to call vm_ops->page_mkwrite())
  
  	/*
 -	 * Check if the backing address space wants to know that the page is
 -	 * about to become writable
 +	 * This silly early PAGE_DIRTY setting removes a race
 +	 * due to the bad i386 page protection. But it's valid
 +	 * for other architectures too.
 +	 *
 +	 * Note that if FAULT_FLAG_WRITE is set, we either now have
 +	 * an exclusive copy of the page, or this is a shared mapping,
 +	 * so we can make it writable and dirty to avoid having to
 +	 * handle that later.
  	 */
++<<<<<<< HEAD
 +	/* Only go through if we didn't race with anybody else... */
 +	if (likely(pte_same(*page_table, orig_pte))) {
 +		flush_icache_page(vma, page);
 +		entry = mk_pte(page, vma->vm_page_prot);
 +		if (flags & FAULT_FLAG_WRITE)
 +			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +		if (anon) {
 +			inc_mm_counter_fast(mm, MM_ANONPAGES);
 +			page_add_new_anon_rmap(page, vma, address);
 +		} else {
 +			inc_mm_counter_fast(mm, MM_FILEPAGES);
 +			page_add_file_rmap(page);
 +			if (flags & FAULT_FLAG_WRITE) {
 +				dirty_page = page;
 +				get_page(dirty_page);
 +			}
 +		}
 +		set_pte_at(mm, address, page_table, entry);
 +
 +		/* no need to invalidate: a not-present page won't be cached */
 +		update_mmu_cache(vma, address, page_table);
 +	} else {
 +		if (cow_page)
 +			mem_cgroup_uncharge_page(cow_page);
 +		if (anon)
 +			page_cache_release(page);
 +		else
 +			anon = 1; /* no anon but release faulted_page */
++=======
+ 	if (vma->vm_ops->page_mkwrite) {
+ 		unlock_page(fault_page);
+ 		tmp = do_page_mkwrite(vma, fault_page, address);
+ 		if (unlikely(!tmp ||
+ 				(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
+ 			page_cache_release(fault_page);
+ 			return tmp;
+ 		}
+ 	}
+ 
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		return ret;
++>>>>>>> fb09a4642582 (mm: consolidate code to call vm_ops->page_mkwrite())
  	}
  
 -	flush_icache_page(vma, fault_page);
 -	entry = mk_pte(fault_page, vma->vm_page_prot);
 -	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 -	inc_mm_counter_fast(mm, MM_FILEPAGES);
 -	page_add_file_rmap(fault_page);
 -	set_pte_at(mm, address, pte, entry);
 +	pte_unmap_unlock(page_table, ptl);
  
 -	/* no need to invalidate: a not-present page won't be cached */
 -	update_mmu_cache(vma, address, pte);
 -	pte_unmap_unlock(pte, ptl);
 +	if (dirty_page) {
 +		struct address_space *mapping = page->mapping;
 +		int dirtied = 0;
  
 -	if (set_page_dirty(fault_page))
 -		dirtied = 1;
 -	mapping = fault_page->mapping;
 -	unlock_page(fault_page);
 -	if ((dirtied || vma->vm_ops->page_mkwrite) && mapping) {
 -		/*
 -		 * Some device drivers do not set page.mapping but still
 -		 * dirty their pages
 -		 */
 -		balance_dirty_pages_ratelimited(mapping);
 +		if (set_page_dirty(dirty_page))
 +			dirtied = 1;
 +		unlock_page(dirty_page);
 +		put_page(dirty_page);
 +		if ((dirtied || page_mkwrite) && mapping) {
 +			/*
 +			 * Some device drivers do not set page.mapping but still
 +			 * dirty their pages
 +			 */
 +			balance_dirty_pages_ratelimited(mapping);
 +		}
 +
 +		/* file_update_time outside page_lock */
 +		if (vma->vm_file && !page_mkwrite)
 +			file_update_time(vma->vm_file);
 +	} else {
 +		unlock_page(vmf.page);
 +		if (anon)
 +			page_cache_release(vmf.page);
  	}
  
 -	/* file_update_time outside page_lock */
 -	if (vma->vm_file && !vma->vm_ops->page_mkwrite)
 -		file_update_time(vma->vm_file);
 +	return ret;
  
 +unwritable_page:
 +	page_cache_release(page);
 +	return ret;
 +uncharge_out:
 +	/* fs's fault handler get error */
 +	if (cow_page) {
 +		mem_cgroup_uncharge_page(cow_page);
 +		page_cache_release(cow_page);
 +	}
  	return ret;
  }
  
* Unmerged path mm/memory.c
