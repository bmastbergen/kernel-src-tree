ceph: remove useless BUG_ON

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Yan, Zheng <zyan@redhat.com>
commit a587d71b0a4b222762e9c3a2352f076532685d9f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a587d71b.failed

ceph_osdc_start_request() never return -EOLDSNAP

	Signed-off-by: Yan, Zheng <zyan@redhat.com>
(cherry picked from commit a587d71b0a4b222762e9c3a2352f076532685d9f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ceph/file.c
diff --cc fs/ceph/file.c
index 0c3070bb755c,6738766f0c12..000000000000
--- a/fs/ceph/file.c
+++ b/fs/ceph/file.c
@@@ -491,6 -547,192 +491,195 @@@ static ssize_t ceph_sync_read(struct ki
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ struct ceph_aio_request {
+ 	struct kiocb *iocb;
+ 	size_t total_len;
+ 	int write;
+ 	int error;
+ 	struct list_head osd_reqs;
+ 	unsigned num_reqs;
+ 	atomic_t pending_reqs;
+ 	struct timespec mtime;
+ 	struct ceph_cap_flush *prealloc_cf;
+ };
+ 
+ struct ceph_aio_work {
+ 	struct work_struct work;
+ 	struct ceph_osd_request *req;
+ };
+ 
+ static void ceph_aio_retry_work(struct work_struct *work);
+ 
+ static void ceph_aio_complete(struct inode *inode,
+ 			      struct ceph_aio_request *aio_req)
+ {
+ 	struct ceph_inode_info *ci = ceph_inode(inode);
+ 	int ret;
+ 
+ 	if (!atomic_dec_and_test(&aio_req->pending_reqs))
+ 		return;
+ 
+ 	ret = aio_req->error;
+ 	if (!ret)
+ 		ret = aio_req->total_len;
+ 
+ 	dout("ceph_aio_complete %p rc %d\n", inode, ret);
+ 
+ 	if (ret >= 0 && aio_req->write) {
+ 		int dirty;
+ 
+ 		loff_t endoff = aio_req->iocb->ki_pos + aio_req->total_len;
+ 		if (endoff > i_size_read(inode)) {
+ 			if (ceph_inode_set_size(inode, endoff))
+ 				ceph_check_caps(ci, CHECK_CAPS_AUTHONLY, NULL);
+ 		}
+ 
+ 		spin_lock(&ci->i_ceph_lock);
+ 		ci->i_inline_version = CEPH_INLINE_NONE;
+ 		dirty = __ceph_mark_dirty_caps(ci, CEPH_CAP_FILE_WR,
+ 					       &aio_req->prealloc_cf);
+ 		spin_unlock(&ci->i_ceph_lock);
+ 		if (dirty)
+ 			__mark_inode_dirty(inode, dirty);
+ 
+ 	}
+ 
+ 	ceph_put_cap_refs(ci, (aio_req->write ? CEPH_CAP_FILE_WR :
+ 						CEPH_CAP_FILE_RD));
+ 
+ 	aio_req->iocb->ki_complete(aio_req->iocb, ret, 0);
+ 
+ 	ceph_free_cap_flush(aio_req->prealloc_cf);
+ 	kfree(aio_req);
+ }
+ 
+ static void ceph_aio_complete_req(struct ceph_osd_request *req,
+ 				  struct ceph_msg *msg)
+ {
+ 	int rc = req->r_result;
+ 	struct inode *inode = req->r_inode;
+ 	struct ceph_aio_request *aio_req = req->r_priv;
+ 	struct ceph_osd_data *osd_data = osd_req_op_extent_osd_data(req, 0);
+ 	int num_pages = calc_pages_for((u64)osd_data->alignment,
+ 				       osd_data->length);
+ 
+ 	dout("ceph_aio_complete_req %p rc %d bytes %llu\n",
+ 	     inode, rc, osd_data->length);
+ 
+ 	if (rc == -EOLDSNAPC) {
+ 		struct ceph_aio_work *aio_work;
+ 		BUG_ON(!aio_req->write);
+ 
+ 		aio_work = kmalloc(sizeof(*aio_work), GFP_NOFS);
+ 		if (aio_work) {
+ 			INIT_WORK(&aio_work->work, ceph_aio_retry_work);
+ 			aio_work->req = req;
+ 			queue_work(ceph_inode_to_client(inode)->wb_wq,
+ 				   &aio_work->work);
+ 			return;
+ 		}
+ 		rc = -ENOMEM;
+ 	} else if (!aio_req->write) {
+ 		if (rc == -ENOENT)
+ 			rc = 0;
+ 		if (rc >= 0 && osd_data->length > rc) {
+ 			int zoff = osd_data->alignment + rc;
+ 			int zlen = osd_data->length - rc;
+ 			/*
+ 			 * If read is satisfied by single OSD request,
+ 			 * it can pass EOF. Otherwise read is within
+ 			 * i_size.
+ 			 */
+ 			if (aio_req->num_reqs == 1) {
+ 				loff_t i_size = i_size_read(inode);
+ 				loff_t endoff = aio_req->iocb->ki_pos + rc;
+ 				if (endoff < i_size)
+ 					zlen = min_t(size_t, zlen,
+ 						     i_size - endoff);
+ 				aio_req->total_len = rc + zlen;
+ 			}
+ 
+ 			if (zlen > 0)
+ 				ceph_zero_page_vector_range(zoff, zlen,
+ 							    osd_data->pages);
+ 		}
+ 	}
+ 
+ 	ceph_put_page_vector(osd_data->pages, num_pages, false);
+ 	ceph_osdc_put_request(req);
+ 
+ 	if (rc < 0)
+ 		cmpxchg(&aio_req->error, 0, rc);
+ 
+ 	ceph_aio_complete(inode, aio_req);
+ 	return;
+ }
+ 
+ static void ceph_aio_retry_work(struct work_struct *work)
+ {
+ 	struct ceph_aio_work *aio_work =
+ 		container_of(work, struct ceph_aio_work, work);
+ 	struct ceph_osd_request *orig_req = aio_work->req;
+ 	struct ceph_aio_request *aio_req = orig_req->r_priv;
+ 	struct inode *inode = orig_req->r_inode;
+ 	struct ceph_inode_info *ci = ceph_inode(inode);
+ 	struct ceph_snap_context *snapc;
+ 	struct ceph_osd_request *req;
+ 	int ret;
+ 
+ 	spin_lock(&ci->i_ceph_lock);
+ 	if (__ceph_have_pending_cap_snap(ci)) {
+ 		struct ceph_cap_snap *capsnap =
+ 			list_last_entry(&ci->i_cap_snaps,
+ 					struct ceph_cap_snap,
+ 					ci_item);
+ 		snapc = ceph_get_snap_context(capsnap->context);
+ 	} else {
+ 		BUG_ON(!ci->i_head_snapc);
+ 		snapc = ceph_get_snap_context(ci->i_head_snapc);
+ 	}
+ 	spin_unlock(&ci->i_ceph_lock);
+ 
+ 	req = ceph_osdc_alloc_request(orig_req->r_osdc, snapc, 2,
+ 			false, GFP_NOFS);
+ 	if (!req) {
+ 		ret = -ENOMEM;
+ 		req = orig_req;
+ 		goto out;
+ 	}
+ 
+ 	req->r_flags =	CEPH_OSD_FLAG_ORDERSNAP |
+ 			CEPH_OSD_FLAG_ONDISK |
+ 			CEPH_OSD_FLAG_WRITE;
+ 	req->r_base_oloc = orig_req->r_base_oloc;
+ 	req->r_base_oid = orig_req->r_base_oid;
+ 
+ 	req->r_ops[0] = orig_req->r_ops[0];
+ 	osd_req_op_init(req, 1, CEPH_OSD_OP_STARTSYNC, 0);
+ 
+ 	ceph_osdc_build_request(req, req->r_ops[0].extent.offset,
+ 				snapc, CEPH_NOSNAP, &aio_req->mtime);
+ 
+ 	ceph_osdc_put_request(orig_req);
+ 
+ 	req->r_callback = ceph_aio_complete_req;
+ 	req->r_inode = inode;
+ 	req->r_priv = aio_req;
+ 
+ 	ret = ceph_osdc_start_request(req->r_osdc, req, false);
+ out:
+ 	if (ret < 0) {
+ 		req->r_result = ret;
+ 		ceph_aio_complete_req(req, NULL);
+ 	}
+ 
+ 	ceph_put_snap_context(snapc);
+ 	kfree(aio_work);
+ }
+ 
++>>>>>>> a587d71b0a4b (ceph: remove useless BUG_ON)
  /*
   * Write commit request unsafe callback, called to tell us when a
   * request is unsafe (that is, in flight--has been handed to the
@@@ -524,6 -766,202 +713,205 @@@ static void ceph_sync_write_unsafe(stru
  }
  
  
++<<<<<<< HEAD
++=======
+ static ssize_t
+ ceph_direct_read_write(struct kiocb *iocb, struct iov_iter *iter,
+ 		       struct ceph_snap_context *snapc,
+ 		       struct ceph_cap_flush **pcf)
+ {
+ 	struct file *file = iocb->ki_filp;
+ 	struct inode *inode = file_inode(file);
+ 	struct ceph_inode_info *ci = ceph_inode(inode);
+ 	struct ceph_fs_client *fsc = ceph_inode_to_client(inode);
+ 	struct ceph_vino vino;
+ 	struct ceph_osd_request *req;
+ 	struct page **pages;
+ 	struct ceph_aio_request *aio_req = NULL;
+ 	int num_pages = 0;
+ 	int flags;
+ 	int ret;
+ 	struct timespec mtime = CURRENT_TIME;
+ 	size_t count = iov_iter_count(iter);
+ 	loff_t pos = iocb->ki_pos;
+ 	bool write = iov_iter_rw(iter) == WRITE;
+ 
+ 	if (write && ceph_snap(file_inode(file)) != CEPH_NOSNAP)
+ 		return -EROFS;
+ 
+ 	dout("sync_direct_read_write (%s) on file %p %lld~%u\n",
+ 	     (write ? "write" : "read"), file, pos, (unsigned)count);
+ 
+ 	ret = filemap_write_and_wait_range(inode->i_mapping, pos, pos + count);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (write) {
+ 		ret = invalidate_inode_pages2_range(inode->i_mapping,
+ 					pos >> PAGE_CACHE_SHIFT,
+ 					(pos + count) >> PAGE_CACHE_SHIFT);
+ 		if (ret < 0)
+ 			dout("invalidate_inode_pages2_range returned %d\n", ret);
+ 
+ 		flags = CEPH_OSD_FLAG_ORDERSNAP |
+ 			CEPH_OSD_FLAG_ONDISK |
+ 			CEPH_OSD_FLAG_WRITE;
+ 	} else {
+ 		flags = CEPH_OSD_FLAG_READ;
+ 	}
+ 
+ 	while (iov_iter_count(iter) > 0) {
+ 		u64 size = dio_get_pagev_size(iter);
+ 		size_t start = 0;
+ 		ssize_t len;
+ 
+ 		vino = ceph_vino(inode);
+ 		req = ceph_osdc_new_request(&fsc->client->osdc, &ci->i_layout,
+ 					    vino, pos, &size, 0,
+ 					    /*include a 'startsync' command*/
+ 					    write ? 2 : 1,
+ 					    write ? CEPH_OSD_OP_WRITE :
+ 						    CEPH_OSD_OP_READ,
+ 					    flags, snapc,
+ 					    ci->i_truncate_seq,
+ 					    ci->i_truncate_size,
+ 					    false);
+ 		if (IS_ERR(req)) {
+ 			ret = PTR_ERR(req);
+ 			break;
+ 		}
+ 
+ 		len = size;
+ 		pages = dio_get_pages_alloc(iter, len, &start, &num_pages);
+ 		if (IS_ERR(pages)) {
+ 			ceph_osdc_put_request(req);
+ 			ret = PTR_ERR(pages);
+ 			break;
+ 		}
+ 
+ 		/*
+ 		 * To simplify error handling, allow AIO when IO within i_size
+ 		 * or IO can be satisfied by single OSD request.
+ 		 */
+ 		if (pos == iocb->ki_pos && !is_sync_kiocb(iocb) &&
+ 		    (len == count || pos + count <= i_size_read(inode))) {
+ 			aio_req = kzalloc(sizeof(*aio_req), GFP_KERNEL);
+ 			if (aio_req) {
+ 				aio_req->iocb = iocb;
+ 				aio_req->write = write;
+ 				INIT_LIST_HEAD(&aio_req->osd_reqs);
+ 				if (write) {
+ 					aio_req->mtime = mtime;
+ 					swap(aio_req->prealloc_cf, *pcf);
+ 				}
+ 			}
+ 			/* ignore error */
+ 		}
+ 
+ 		if (write) {
+ 			/*
+ 			 * throw out any page cache pages in this range. this
+ 			 * may block.
+ 			 */
+ 			truncate_inode_pages_range(inode->i_mapping, pos,
+ 					(pos+len) | (PAGE_CACHE_SIZE - 1));
+ 
+ 			osd_req_op_init(req, 1, CEPH_OSD_OP_STARTSYNC, 0);
+ 		}
+ 
+ 
+ 		osd_req_op_extent_osd_data_pages(req, 0, pages, len, start,
+ 						 false, false);
+ 
+ 		ceph_osdc_build_request(req, pos, snapc, vino.snap, &mtime);
+ 
+ 		if (aio_req) {
+ 			aio_req->total_len += len;
+ 			aio_req->num_reqs++;
+ 			atomic_inc(&aio_req->pending_reqs);
+ 
+ 			req->r_callback = ceph_aio_complete_req;
+ 			req->r_inode = inode;
+ 			req->r_priv = aio_req;
+ 			list_add_tail(&req->r_unsafe_item, &aio_req->osd_reqs);
+ 
+ 			pos += len;
+ 			iov_iter_advance(iter, len);
+ 			continue;
+ 		}
+ 
+ 		ret = ceph_osdc_start_request(req->r_osdc, req, false);
+ 		if (!ret)
+ 			ret = ceph_osdc_wait_request(&fsc->client->osdc, req);
+ 
+ 		size = i_size_read(inode);
+ 		if (!write) {
+ 			if (ret == -ENOENT)
+ 				ret = 0;
+ 			if (ret >= 0 && ret < len && pos + ret < size) {
+ 				int zlen = min_t(size_t, len - ret,
+ 						 size - pos - ret);
+ 				ceph_zero_page_vector_range(start + ret, zlen,
+ 							    pages);
+ 				ret += zlen;
+ 			}
+ 			if (ret >= 0)
+ 				len = ret;
+ 		}
+ 
+ 		ceph_put_page_vector(pages, num_pages, false);
+ 
+ 		ceph_osdc_put_request(req);
+ 		if (ret < 0)
+ 			break;
+ 
+ 		pos += len;
+ 		iov_iter_advance(iter, len);
+ 
+ 		if (!write && pos >= size)
+ 			break;
+ 
+ 		if (write && pos > size) {
+ 			if (ceph_inode_set_size(inode, pos))
+ 				ceph_check_caps(ceph_inode(inode),
+ 						CHECK_CAPS_AUTHONLY,
+ 						NULL);
+ 		}
+ 	}
+ 
+ 	if (aio_req) {
+ 		if (aio_req->num_reqs == 0) {
+ 			kfree(aio_req);
+ 			return ret;
+ 		}
+ 
+ 		ceph_get_cap_refs(ci, write ? CEPH_CAP_FILE_WR :
+ 					      CEPH_CAP_FILE_RD);
+ 
+ 		while (!list_empty(&aio_req->osd_reqs)) {
+ 			req = list_first_entry(&aio_req->osd_reqs,
+ 					       struct ceph_osd_request,
+ 					       r_unsafe_item);
+ 			list_del_init(&req->r_unsafe_item);
+ 			if (ret >= 0)
+ 				ret = ceph_osdc_start_request(req->r_osdc,
+ 							      req, false);
+ 			if (ret < 0) {
+ 				req->r_result = ret;
+ 				ceph_aio_complete_req(req, NULL);
+ 			}
+ 		}
+ 		return -EIOCBQUEUED;
+ 	}
+ 
+ 	if (ret != -EOLDSNAPC && pos > iocb->ki_pos) {
+ 		ret = pos - iocb->ki_pos;
+ 		iocb->ki_pos = pos;
+ 	}
+ 	return ret;
+ }
+ 
++>>>>>>> a587d71b0a4b (ceph: remove useless BUG_ON)
  /*
   * Synchronous write, straight from __user pointer or user pages.
   *
* Unmerged path fs/ceph/file.c
