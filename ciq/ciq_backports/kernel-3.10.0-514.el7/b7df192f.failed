IB/hfi1: Use evict mmu rb operation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dean Luick <dean.luick@intel.com>
commit b7df192f74a8cde22f6dc0680a2daa40540ed72f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/b7df192f.failed

Use the new cache evict operation in the SDMA code.  This allows the cache
to properly coordinate evicts and removes, preventing any race.  With this
change, the separate list, lock, and race flag are not needed.

	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Signed-off-by: Dean Luick <dean.luick@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit b7df192f74a8cde22f6dc0680a2daa40540ed72f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/user_sdma.c
#	drivers/staging/hfi1/user_sdma.h
diff --cc drivers/staging/hfi1/user_sdma.c
index 47c9c87af47a,3d76222d1aac..000000000000
--- a/drivers/staging/hfi1/user_sdma.c
+++ b/drivers/staging/hfi1/user_sdma.c
@@@ -179,9 -175,26 +179,27 @@@ struct user_sdma_iovec 
  	unsigned npages;
  	/* array of pinned pages for this vector */
  	struct page **pages;
 -	/*
 -	 * offset into the virtual address space of the vector at
 -	 * which we last left off.
 -	 */
 +	/* offset into the virtual address space of the vector at
 +	 * which we last left off. */
  	u64 offset;
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 	struct sdma_mmu_node *node;
+ };
+ 
+ struct sdma_mmu_node {
+ 	struct mmu_rb_node rb;
+ 	struct hfi1_user_sdma_pkt_q *pq;
+ 	atomic_t refcount;
+ 	struct page **pages;
+ 	unsigned npages;
+ };
+ 
+ /* evict operation argument */
+ struct evict_data {
+ 	u32 cleared;	/* count evicted so far */
+ 	u32 target;	/* target count to evict */
++>>>>>>> b7df192f74a8 (IB/hfi1: Use evict mmu rb operation):drivers/infiniband/hw/hfi1/user_sdma.c
  };
  
  struct user_sdma_request {
@@@ -297,6 -306,21 +315,24 @@@ static int defer_packet_queue
  	struct sdma_txreq *,
  	unsigned seq);
  static void activate_packet_queue(struct iowait *, int);
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ static bool sdma_rb_filter(struct mmu_rb_node *, unsigned long, unsigned long);
+ static int sdma_rb_insert(void *, struct mmu_rb_node *);
+ static int sdma_rb_evict(void *arg, struct mmu_rb_node *mnode,
+ 			 void *arg2, bool *stop);
+ static void sdma_rb_remove(void *, struct mmu_rb_node *,
+ 			   struct mm_struct *);
+ static int sdma_rb_invalidate(void *, struct mmu_rb_node *);
+ 
+ static struct mmu_rb_ops sdma_rb_ops = {
+ 	.filter = sdma_rb_filter,
+ 	.insert = sdma_rb_insert,
+ 	.evict = sdma_rb_evict,
+ 	.remove = sdma_rb_remove,
+ 	.invalidate = sdma_rb_invalidate
+ };
++>>>>>>> b7df192f74a8 (IB/hfi1: Use evict mmu rb operation):drivers/infiniband/hw/hfi1/user_sdma.c
  
  static int defer_packet_queue(
  	struct sdma_engine *sde,
@@@ -386,9 -415,11 +422,14 @@@ int hfi1_user_sdma_alloc_queues(struct 
  	pq->state = SDMA_PKT_Q_INACTIVE;
  	atomic_set(&pq->n_reqs, 0);
  	init_waitqueue_head(&pq->wait);
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 	atomic_set(&pq->n_locked, 0);
+ 	pq->mm = fd->mm;
++>>>>>>> b7df192f74a8 (IB/hfi1: Use evict mmu rb operation):drivers/infiniband/hw/hfi1/user_sdma.c
  
  	iowait_init(&pq->busy, 0, NULL, defer_packet_queue,
 -		    activate_packet_queue, NULL);
 +		    activate_packet_queue);
  	pq->reqidx = 0;
  	snprintf(buf, 64, "txreq-kmem-cache-%u-%u-%u", dd->unit, uctxt->ctxt,
  		 fd->subctxt);
@@@ -1069,40 -1128,107 +1110,128 @@@ static inline int num_user_pages(const 
  	return 1 + ((epage - spage) >> PAGE_SHIFT);
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +static int pin_vector_pages(struct user_sdma_request *req,
 +			    struct user_sdma_iovec *iovec) {
 +	int pinned, npages;
 +
 +	npages = num_user_pages(&iovec->iov);
 +	iovec->pages = kcalloc(npages, sizeof(*iovec->pages), GFP_KERNEL);
 +	if (!iovec->pages) {
 +		SDMA_DBG(req, "Failed page array alloc");
 +		return -ENOMEM;
++=======
+ static u32 sdma_cache_evict(struct hfi1_user_sdma_pkt_q *pq, u32 npages)
+ {
+ 	struct evict_data evict_data;
+ 
+ 	evict_data.cleared = 0;
+ 	evict_data.target = npages;
+ 	hfi1_mmu_rb_evict(pq->handler, &evict_data);
+ 	return evict_data.cleared;
+ }
+ 
+ static int pin_vector_pages(struct user_sdma_request *req,
+ 			    struct user_sdma_iovec *iovec)
+ {
+ 	int ret = 0, pinned, npages, cleared;
+ 	struct page **pages;
+ 	struct hfi1_user_sdma_pkt_q *pq = req->pq;
+ 	struct sdma_mmu_node *node = NULL;
+ 	struct mmu_rb_node *rb_node;
+ 
+ 	rb_node = hfi1_mmu_rb_extract(pq->handler,
+ 				      (unsigned long)iovec->iov.iov_base,
+ 				      iovec->iov.iov_len);
+ 	if (rb_node && !IS_ERR(rb_node))
+ 		node = container_of(rb_node, struct sdma_mmu_node, rb);
+ 	else
+ 		rb_node = NULL;
+ 
+ 	if (!node) {
+ 		node = kzalloc(sizeof(*node), GFP_KERNEL);
+ 		if (!node)
+ 			return -ENOMEM;
+ 
+ 		node->rb.addr = (unsigned long)iovec->iov.iov_base;
+ 		node->pq = pq;
+ 		atomic_set(&node->refcount, 0);
+ 	}
+ 
+ 	npages = num_user_pages(&iovec->iov);
+ 	if (node->npages < npages) {
+ 		pages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);
+ 		if (!pages) {
+ 			SDMA_DBG(req, "Failed page array alloc");
+ 			ret = -ENOMEM;
+ 			goto bail;
+ 		}
+ 		memcpy(pages, node->pages, node->npages * sizeof(*pages));
+ 
+ 		npages -= node->npages;
+ 
+ retry:
+ 		if (!hfi1_can_pin_pages(pq->dd, pq->mm,
+ 					atomic_read(&pq->n_locked), npages)) {
+ 			cleared = sdma_cache_evict(pq, npages);
+ 			if (cleared >= npages)
+ 				goto retry;
+ 		}
+ 		pinned = hfi1_acquire_user_pages(pq->mm,
+ 			((unsigned long)iovec->iov.iov_base +
+ 			 (node->npages * PAGE_SIZE)), npages, 0,
+ 			pages + node->npages);
+ 		if (pinned < 0) {
+ 			kfree(pages);
+ 			ret = pinned;
+ 			goto bail;
+ 		}
+ 		if (pinned != npages) {
+ 			unpin_vector_pages(pq->mm, pages, node->npages,
+ 					   pinned);
+ 			ret = -EFAULT;
+ 			goto bail;
+ 		}
+ 		kfree(node->pages);
+ 		node->rb.len = iovec->iov.iov_len;
+ 		node->pages = pages;
+ 		node->npages += pinned;
+ 		npages = node->npages;
+ 		atomic_add(pinned, &pq->n_locked);
++>>>>>>> b7df192f74a8 (IB/hfi1: Use evict mmu rb operation):drivers/infiniband/hw/hfi1/user_sdma.c
  	}
 -	iovec->pages = node->pages;
 -	iovec->npages = npages;
 -	iovec->node = node;
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +	pinned = hfi1_acquire_user_pages((unsigned long)iovec->iov.iov_base,
 +					 npages, 0, iovec->pages);
 +
 +	if (pinned < 0)
 +		return pinned;
 +
 +	iovec->npages = pinned;
 +	if (pinned != npages) {
 +		SDMA_DBG(req, "Failed to pin pages (%d/%u)", pinned, npages);
 +		unpin_vector_pages(iovec);
 +		return -EFAULT;
++=======
+ 	ret = hfi1_mmu_rb_insert(req->pq->handler, &node->rb);
+ 	if (ret) {
+ 		atomic_sub(node->npages, &pq->n_locked);
+ 		iovec->node = NULL;
+ 		goto bail;
++>>>>>>> b7df192f74a8 (IB/hfi1: Use evict mmu rb operation):drivers/infiniband/hw/hfi1/user_sdma.c
  	}
  	return 0;
 -bail:
 -	if (rb_node)
 -		unpin_vector_pages(pq->mm, node->pages, 0, node->npages);
 -	kfree(node);
 -	return ret;
  }
  
 -static void unpin_vector_pages(struct mm_struct *mm, struct page **pages,
 -			       unsigned start, unsigned npages)
 +static void unpin_vector_pages(struct user_sdma_iovec *iovec)
  {
 -	hfi1_release_user_pages(mm, pages + start, npages, false);
 -	kfree(pages);
 +	hfi1_release_user_pages(iovec->pages, iovec->npages, 0);
 +
 +	kfree(iovec->pages);
 +	iovec->pages = NULL;
 +	iovec->npages = 0;
 +	iovec->offset = 0;
  }
  
  static int check_header_template(struct user_sdma_request *req,
@@@ -1484,3 -1601,79 +1613,82 @@@ static inline void set_comp_state(struc
  	trace_hfi1_sdma_user_completion(pq->dd, pq->ctxt, pq->subctxt,
  					idx, state, ret);
  }
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 
+ static bool sdma_rb_filter(struct mmu_rb_node *node, unsigned long addr,
+ 			   unsigned long len)
+ {
+ 	return (bool)(node->addr == addr);
+ }
+ 
+ static int sdma_rb_insert(void *arg, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	atomic_inc(&node->refcount);
+ 	return 0;
+ }
+ 
+ /*
+  * Return 1 to remove the node from the rb tree and call the remove op.
+  *
+  * Called with the rb tree lock held.
+  */
+ static int sdma_rb_evict(void *arg, struct mmu_rb_node *mnode,
+ 			 void *evict_arg, bool *stop)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 	struct evict_data *evict_data = evict_arg;
+ 
+ 	/* is this node still being used? */
+ 	if (atomic_read(&node->refcount))
+ 		return 0; /* keep this node */
+ 
+ 	/* this node will be evicted, add its pages to our count */
+ 	evict_data->cleared += node->npages;
+ 
+ 	/* have enough pages been cleared? */
+ 	if (evict_data->cleared >= evict_data->target)
+ 		*stop = true;
+ 
+ 	return 1; /* remove this node */
+ }
+ 
+ static void sdma_rb_remove(void *arg, struct mmu_rb_node *mnode,
+ 			   struct mm_struct *mm)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	atomic_sub(node->npages, &node->pq->n_locked);
+ 
+ 	/*
+ 	 * If mm is set, we are being called by the MMU notifier and we
+ 	 * should not pass a mm_struct to unpin_vector_page(). This is to
+ 	 * prevent a deadlock when hfi1_release_user_pages() attempts to
+ 	 * take the mmap_sem, which the MMU notifier has already taken.
+ 	 */
+ 	unpin_vector_pages(mm ? NULL : current->mm, node->pages, 0,
+ 			   node->npages);
+ 	/*
+ 	 * If called by the MMU notifier, we have to adjust the pinned
+ 	 * page count ourselves.
+ 	 */
+ 	if (mm)
+ 		mm->pinned_vm -= node->npages;
+ 	kfree(node);
+ }
+ 
+ static int sdma_rb_invalidate(void *arg, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	if (!atomic_read(&node->refcount))
+ 		return 1;
+ 	return 0;
+ }
++>>>>>>> b7df192f74a8 (IB/hfi1: Use evict mmu rb operation):drivers/infiniband/hw/hfi1/user_sdma.c
diff --cc drivers/staging/hfi1/user_sdma.h
index 7ebbc4634989,39001714f551..000000000000
--- a/drivers/staging/hfi1/user_sdma.h
+++ b/drivers/staging/hfi1/user_sdma.h
@@@ -78,6 -68,9 +78,12 @@@ struct hfi1_user_sdma_pkt_q 
  	unsigned state;
  	wait_queue_head_t wait;
  	unsigned long unpinned;
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.h
++=======
+ 	struct mmu_rb_handler *handler;
+ 	atomic_t n_locked;
+ 	struct mm_struct *mm;
++>>>>>>> b7df192f74a8 (IB/hfi1: Use evict mmu rb operation):drivers/infiniband/hw/hfi1/user_sdma.h
  };
  
  struct hfi1_user_sdma_comp_q {
* Unmerged path drivers/staging/hfi1/user_sdma.c
* Unmerged path drivers/staging/hfi1/user_sdma.h
