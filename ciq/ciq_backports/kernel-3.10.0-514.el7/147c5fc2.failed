sched/fair: Shrink sg_lb_stats and play memset games

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [kernel] sched: Shrink sg_lb_stats and play memset games (Jiri Olsa) [1211784]
Rebuild_FUZZ: 94.95%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 147c5fc2bad780d8093b547f2baa204e78107faf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/147c5fc2.failed

We can shrink sg_lb_stats because rq::nr_running is an unsigned int
and cpu numbers are 'int'

Before:
  sgs:        /* size: 72, cachelines: 2, members: 10 */
  sds:        /* size: 184, cachelines: 3, members: 7 */

After:
  sgs:        /* size: 56, cachelines: 1, members: 10 */
  sds:        /* size: 152, cachelines: 3, members: 7 */

Further we can avoid clearing all of sds since we do a total
clear/assignment of sg_stats in update_sg_lb_stats() with exception of
busiest_stat.avg_load which is referenced in update_sd_pick_busiest().

	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/n/tip-0klzmz9okll8wc0nsudguc9p@git.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 147c5fc2bad780d8093b547f2baa204e78107faf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 3c2f426dff2b,4c6a8a5a789a..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5317,7 -4239,6 +5317,10 @@@ struct sg_lb_stats 
  	unsigned long group_load; /* Total load over the CPUs of the group */
  	unsigned long sum_weighted_load; /* Weighted load of group's tasks */
  	unsigned long load_per_task;
++<<<<<<< HEAD
 +	unsigned long group_power;
++=======
++>>>>>>> 147c5fc2bad7 (sched/fair: Shrink sg_lb_stats and play memset games)
  	unsigned int sum_nr_running; /* Nr tasks running in the group */
  	unsigned int group_capacity;
  	unsigned int idle_cpus;
@@@ -5342,35 -4259,26 +5345,56 @@@ struct sd_lb_stats 
  	unsigned long avg_load;	/* Average load across all groups in sd */
  
  	struct sg_lb_stats busiest_stat;/* Statistics of the busiest group */
++<<<<<<< HEAD
 +	struct sg_lb_stats this_stat;	/* Statistics of this group */
++=======
+ 	struct sg_lb_stats local_stat;	/* Statistics of the local group */
++>>>>>>> 147c5fc2bad7 (sched/fair: Shrink sg_lb_stats and play memset games)
  };
  
  static inline void init_sd_lb_stats(struct sd_lb_stats *sds)
  {
  	/*
++<<<<<<< HEAD
 +	 * struct sd_lb_stats {
 +	 *	   struct sched_group *       busiest;             //     0  8
 +	 *	   struct sched_group *       this;                //     8  8
 +	 *	   long unsigned int          total_load;          //    16  8
 +	 *	   long unsigned int          total_pwr;           //    24  8
 +	 *	   long unsigned int          avg_load;            //    32  8
 +	 *	   struct sg_lb_stats {
 +	 *		   long unsigned int  avg_load;            //    40  8
 +	 *		   long unsigned int  group_load;          //    48  8
 +	 *	           ...
 +	 *	   } busiest_stat;                                 //    40 64
 +	 *	   struct sg_lb_stats	      this_stat;	   //   104 64
 +	 *
 +	 *	   // size: 168, cachelines: 3, members: 7
 +	 *	   // last cacheline: 40 bytes
 +	 * };
 +	 *
 +	 * Skimp on the clearing to avoid duplicate work. We can avoid clearing
 +	 * this_stat because update_sg_lb_stats() does a full clear/assignment.
 +	 * We must however clear busiest_stat::avg_load because
 +	 * update_sd_pick_busiest() reads this before assignment.
 +	 */
 +	memset(sds, 0, offsetof(struct sd_lb_stats, busiest_stat.group_load));
++=======
+ 	 * Skimp on the clearing to avoid duplicate work. We can avoid clearing
+ 	 * local_stat because update_sg_lb_stats() does a full clear/assignment.
+ 	 * We must however clear busiest_stat::avg_load because
+ 	 * update_sd_pick_busiest() reads this before assignment.
+ 	 */
+ 	*sds = (struct sd_lb_stats){
+ 		.busiest = NULL,
+ 		.local = NULL,
+ 		.total_load = 0UL,
+ 		.total_pwr = 0UL,
+ 		.busiest_stat = {
+ 			.avg_load = 0UL,
+ 		},
+ 	};
++>>>>>>> 147c5fc2bad7 (sched/fair: Shrink sg_lb_stats and play memset games)
  }
  
  /**
@@@ -5776,7 -4633,7 +5800,11 @@@ static inline void update_sd_lb_stats(s
  		 * with a large weight task outweighs the tasks on the system).
  		 */
  		if (prefer_sibling && !local_group &&
++<<<<<<< HEAD
 +				sds->this && sds->this_stat.group_has_capacity)
++=======
+ 				sds->local && sds->local_stat.group_has_capacity)
++>>>>>>> 147c5fc2bad7 (sched/fair: Shrink sg_lb_stats and play memset games)
  			sgs->group_capacity = min(sgs->group_capacity, 1U);
  
  		/* Now, start updating sd_lb_stats */
* Unmerged path kernel/sched/fair.c
