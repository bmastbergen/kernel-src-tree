IB/qib: Rename several functions by adding a "qib_" prefix

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit 20f333b61300fa658952713ca9b8b4b72bbaed9f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/20f333b6.failed

This would avoid conflict with the functions in hfi1 that have similar
names when both qib and hfi1 drivers are configured to be built into
the kernel. This issue came up in the 0-day build report.

	Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 20f333b61300fa658952713ca9b8b4b72bbaed9f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/qib.h
#	drivers/infiniband/hw/qib/qib_qp.c
#	drivers/infiniband/hw/qib/qib_verbs.c
#	drivers/infiniband/hw/qib/qib_verbs.h
diff --cc drivers/infiniband/hw/qib/qib.h
index 2c9672d7da79,bbf0a163aeab..000000000000
--- a/drivers/infiniband/hw/qib/qib.h
+++ b/drivers/infiniband/hw/qib/qib.h
@@@ -1544,4 -1539,14 +1544,17 @@@ struct qib_hwerror_msgs 
  void qib_format_hwerrors(u64 hwerrs,
  			 const struct qib_hwerror_msgs *hwerrmsgs,
  			 size_t nhwerrmsgs, char *msg, size_t lmsg);
++<<<<<<< HEAD
++=======
+ 
+ void qib_stop_send_queue(struct rvt_qp *qp);
+ void qib_quiesce_qp(struct rvt_qp *qp);
+ void qib_flush_qp_waiters(struct rvt_qp *qp);
+ int qib_mtu_to_path_mtu(u32 mtu);
+ u32 qib_mtu_from_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp, u32 pmtu);
+ void qib_notify_error_qp(struct rvt_qp *qp);
+ int qib_get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+ 			   struct ib_qp_attr *attr);
+ 
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  #endif                          /* _QIB_KERNEL_H */
diff --cc drivers/infiniband/hw/qib/qib_qp.c
index cf1dd6e9d434,01d49dc91de2..000000000000
--- a/drivers/infiniband/hw/qib/qib_qp.c
+++ b/drivers/infiniband/hw/qib/qib_qp.c
@@@ -122,12 -127,15 +122,17 @@@ static void get_map_page(struct qib_qpn
   * Allocate the next available QPN or
   * zero/one for QP type IB_QPT_SMI/IB_QPT_GSI.
   */
++<<<<<<< HEAD
 +static int alloc_qpn(struct qib_devdata *dd, struct qib_qpn_table *qpt,
 +		     enum ib_qp_type type, u8 port, gfp_t gfp)
++=======
+ int qib_alloc_qpn(struct rvt_dev_info *rdi, struct rvt_qpn_table *qpt,
+ 		  enum ib_qp_type type, u8 port, gfp_t gfp)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
  	u32 i, offset, max_scan, qpn;
 -	struct rvt_qpn_map *map;
 +	struct qpn_map *map;
  	u32 ret;
 -	struct qib_ibdev *verbs_dev = container_of(rdi, struct qib_ibdev, rdi);
 -	struct qib_devdata *dd = container_of(verbs_dev, struct qib_devdata,
 -					      verbs_dev);
  
  	if (type == IB_QPT_SMI || type == IB_QPT_GSI) {
  		unsigned n;
@@@ -329,151 -232,14 +334,159 @@@ unsigned qib_free_all_qps(struct qib_de
  	return qp_inuse;
  }
  
++<<<<<<< HEAD
 +/**
 + * qib_lookup_qpn - return the QP with the given QPN
 + * @qpt: the QP table
 + * @qpn: the QP number to look up
 + *
 + * The caller is responsible for decrementing the QP reference count
 + * when done.
 + */
 +struct qib_qp *qib_lookup_qpn(struct qib_ibport *ibp, u32 qpn)
++=======
+ void qib_notify_qp_reset(struct rvt_qp *qp)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
 -	struct qib_qp_priv *priv = qp->priv;
 +	struct qib_qp *qp = NULL;
  
 +	rcu_read_lock();
 +	if (unlikely(qpn <= 1)) {
 +		if (qpn == 0)
 +			qp = rcu_dereference(ibp->qp0);
 +		else
 +			qp = rcu_dereference(ibp->qp1);
 +		if (qp)
 +			atomic_inc(&qp->refcount);
 +	} else {
 +		struct qib_ibdev *dev = &ppd_from_ibp(ibp)->dd->verbs_dev;
 +		unsigned n = qpn_hash(dev, qpn);
 +
 +		for (qp = rcu_dereference(dev->qp_table[n]); qp;
 +			qp = rcu_dereference(qp->next))
 +			if (qp->ibqp.qp_num == qpn) {
 +				atomic_inc(&qp->refcount);
 +				break;
 +			}
 +	}
 +	rcu_read_unlock();
 +	return qp;
 +}
 +
++<<<<<<< HEAD
 +/**
 + * qib_reset_qp - initialize the QP state to the reset state
 + * @qp: the QP to reset
 + * @type: the QP type
 + */
 +static void qib_reset_qp(struct qib_qp *qp, enum ib_qp_type type)
 +{
 +	struct qib_qp_priv *priv = qp->priv;
 +	qp->remote_qpn = 0;
 +	qp->qkey = 0;
 +	qp->qp_access_flags = 0;
  	atomic_set(&priv->s_dma_busy, 0);
 +	qp->s_flags &= QIB_S_SIGNAL_REQ_WR;
 +	qp->s_hdrwords = 0;
 +	qp->s_wqe = NULL;
 +	qp->s_draining = 0;
 +	qp->s_next_psn = 0;
 +	qp->s_last_psn = 0;
 +	qp->s_sending_psn = 0;
 +	qp->s_sending_hpsn = 0;
 +	qp->s_psn = 0;
 +	qp->r_psn = 0;
 +	qp->r_msn = 0;
 +	if (type == IB_QPT_RC) {
 +		qp->s_state = IB_OPCODE_RC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_RC_SEND_LAST;
 +	} else {
 +		qp->s_state = IB_OPCODE_UC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_UC_SEND_LAST;
 +	}
 +	qp->s_ack_state = IB_OPCODE_RC_ACKNOWLEDGE;
 +	qp->r_nak_state = 0;
 +	qp->r_aflags = 0;
 +	qp->r_flags = 0;
 +	qp->s_head = 0;
 +	qp->s_tail = 0;
 +	qp->s_cur = 0;
 +	qp->s_acked = 0;
 +	qp->s_last = 0;
 +	qp->s_ssn = 1;
 +	qp->s_lsn = 0;
 +	qp->s_mig_state = IB_MIG_MIGRATED;
 +	memset(qp->s_ack_queue, 0, sizeof(qp->s_ack_queue));
 +	qp->r_head_ack_queue = 0;
 +	qp->s_tail_ack_queue = 0;
 +	qp->s_num_rd_atomic = 0;
 +	if (qp->r_rq.wq) {
 +		qp->r_rq.wq->head = 0;
 +		qp->r_rq.wq->tail = 0;
 +	}
 +	qp->r_sge.num_sge = 0;
  }
  
 +static void clear_mr_refs(struct qib_qp *qp, int clr_sends)
 +{
 +	unsigned n;
 +
 +	if (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))
 +		qib_put_ss(&qp->s_rdma_read_sge);
 +
 +	qib_put_ss(&qp->r_sge);
 +
 +	if (clr_sends) {
 +		while (qp->s_last != qp->s_head) {
 +			struct qib_swqe *wqe = get_swqe_ptr(qp, qp->s_last);
 +			unsigned i;
 +
 +			for (i = 0; i < wqe->wr.num_sge; i++) {
 +				struct qib_sge *sge = &wqe->sg_list[i];
 +
 +				qib_put_mr(sge->mr);
 +			}
 +			if (qp->ibqp.qp_type == IB_QPT_UD ||
 +			    qp->ibqp.qp_type == IB_QPT_SMI ||
 +			    qp->ibqp.qp_type == IB_QPT_GSI)
 +				atomic_dec(&to_iah(wqe->wr.wr.ud.ah)->refcount);
 +			if (++qp->s_last >= qp->s_size)
 +				qp->s_last = 0;
 +		}
 +		if (qp->s_rdma_mr) {
 +			qib_put_mr(qp->s_rdma_mr);
 +			qp->s_rdma_mr = NULL;
 +		}
 +	}
 +
 +	if (qp->ibqp.qp_type != IB_QPT_RC)
 +		return;
 +
 +	for (n = 0; n < ARRAY_SIZE(qp->s_ack_queue); n++) {
 +		struct qib_ack_entry *e = &qp->s_ack_queue[n];
 +
 +		if (e->opcode == IB_OPCODE_RC_RDMA_READ_REQUEST &&
 +		    e->rdma_sge.mr) {
 +			qib_put_mr(e->rdma_sge.mr);
 +			e->rdma_sge.mr = NULL;
 +		}
 +	}
 +}
 +
 +/**
 + * qib_error_qp - put a QP into the error state
 + * @qp: the QP to put into the error state
 + * @err: the receive completion error to signal if a RWQE is active
 + *
 + * Flushes both send and receive work queues.
 + * Returns true if last WQE event should be generated.
 + * The QP r_lock and s_lock should be held and interrupts disabled.
 + * If we are already in error state, just return.
 + */
 +int qib_error_qp(struct qib_qp *qp, enum ib_wc_status err)
++=======
+ void qib_notify_error_qp(struct rvt_qp *qp)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
  	struct qib_qp_priv *priv = qp->priv;
  	struct qib_ibdev *dev = to_idev(qp->ibqp.device);
@@@ -511,400 -262,60 +524,420 @@@
  			priv->s_tx = NULL;
  		}
  	}
 +
 +	/* Schedule the sending tasklet to drain the send work queue. */
 +	if (qp->s_last != qp->s_head)
 +		qib_schedule_send(qp);
 +
 +	clear_mr_refs(qp, 0);
 +
 +	memset(&wc, 0, sizeof(wc));
 +	wc.qp = &qp->ibqp;
 +	wc.opcode = IB_WC_RECV;
 +
 +	if (test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags)) {
 +		wc.wr_id = qp->r_wr_id;
 +		wc.status = err;
 +		qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
 +	}
 +	wc.status = IB_WC_WR_FLUSH_ERR;
 +
 +	if (qp->r_rq.wq) {
 +		struct qib_rwq *wq;
 +		u32 head;
 +		u32 tail;
 +
 +		spin_lock(&qp->r_rq.lock);
 +
 +		/* sanity check pointers before trusting them */
 +		wq = qp->r_rq.wq;
 +		head = wq->head;
 +		if (head >= qp->r_rq.size)
 +			head = 0;
 +		tail = wq->tail;
 +		if (tail >= qp->r_rq.size)
 +			tail = 0;
 +		while (tail != head) {
 +			wc.wr_id = get_rwqe_ptr(&qp->r_rq, tail)->wr_id;
 +			if (++tail >= qp->r_rq.size)
 +				tail = 0;
 +			qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
 +		}
 +		wq->tail = tail;
 +
 +		spin_unlock(&qp->r_rq.lock);
 +	} else if (qp->ibqp.event_handler)
 +		ret = 1;
 +
 +bail:
 +	return ret;
  }
  
 -static int mtu_to_enum(u32 mtu)
++<<<<<<< HEAD
 +/**
 + * qib_modify_qp - modify the attributes of a queue pair
 + * @ibqp: the queue pair who's attributes we're modifying
 + * @attr: the new attributes
 + * @attr_mask: the mask of attributes to modify
 + * @udata: user data for libibverbs.so
 + *
 + * Returns 0 on success, otherwise returns an errno.
 + */
 +int qib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		  int attr_mask, struct ib_udata *udata)
++=======
++int qib_get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,
++			   struct ib_qp_attr *attr)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
 -	int enum_mtu;
 +	struct qib_ibdev *dev = to_idev(ibqp->device);
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_qp_priv *priv = qp->priv;
 +	enum ib_qp_state cur_state, new_state;
 +	struct ib_event ev;
 +	int lastwqe = 0;
 +	int mig = 0;
 +	int ret;
 +	u32 pmtu = 0; /* for gcc warning only */
 +
 +	spin_lock_irq(&qp->r_lock);
 +	spin_lock(&qp->s_lock);
 +
 +	cur_state = attr_mask & IB_QP_CUR_STATE ?
 +		attr->cur_qp_state : qp->state;
 +	new_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;
 +
 +	if (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type,
 +				attr_mask, IB_LINK_LAYER_UNSPECIFIED))
 +		goto inval;
 +
 +	if (attr_mask & IB_QP_AV) {
 +		if (attr->ah_attr.dlid >= QIB_MULTICAST_LID_BASE)
 +			goto inval;
 +		if (qib_check_ah(qp->ibqp.device, &attr->ah_attr))
 +			goto inval;
 +	}
 +
 +	if (attr_mask & IB_QP_ALT_PATH) {
 +		if (attr->alt_ah_attr.dlid >= QIB_MULTICAST_LID_BASE)
 +			goto inval;
 +		if (qib_check_ah(qp->ibqp.device, &attr->alt_ah_attr))
 +			goto inval;
 +		if (attr->alt_pkey_index >= qib_get_npkeys(dd_from_dev(dev)))
 +			goto inval;
 +	}
  
 -	switch (mtu) {
 -	case 4096:
 -		enum_mtu = IB_MTU_4096;
 +	if (attr_mask & IB_QP_PKEY_INDEX)
 +		if (attr->pkey_index >= qib_get_npkeys(dd_from_dev(dev)))
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_MIN_RNR_TIMER)
 +		if (attr->min_rnr_timer > 31)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_PORT)
 +		if (qp->ibqp.qp_type == IB_QPT_SMI ||
 +		    qp->ibqp.qp_type == IB_QPT_GSI ||
 +		    attr->port_num == 0 ||
 +		    attr->port_num > ibqp->device->phys_port_cnt)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_DEST_QPN)
 +		if (attr->dest_qp_num > QIB_QPN_MASK)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_RETRY_CNT)
 +		if (attr->retry_cnt > 7)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_RNR_RETRY)
 +		if (attr->rnr_retry > 7)
 +			goto inval;
 +
 +	/*
 +	 * Don't allow invalid path_mtu values.  OK to set greater
 +	 * than the active mtu (or even the max_cap, if we have tuned
 +	 * that to a small mtu.  We'll set qp->path_mtu
 +	 * to the lesser of requested attribute mtu and active,
 +	 * for packetizing messages.
 +	 * Note that the QP port has to be set in INIT and MTU in RTR.
 +	 */
 +	if (attr_mask & IB_QP_PATH_MTU) {
 +		struct qib_devdata *dd = dd_from_dev(dev);
 +		int mtu, pidx = qp->port_num - 1;
 +
 +		mtu = ib_mtu_enum_to_int(attr->path_mtu);
 +		if (mtu == -1)
 +			goto inval;
 +		if (mtu > dd->pport[pidx].ibmtu) {
 +			switch (dd->pport[pidx].ibmtu) {
 +			case 4096:
 +				pmtu = IB_MTU_4096;
 +				break;
 +			case 2048:
 +				pmtu = IB_MTU_2048;
 +				break;
 +			case 1024:
 +				pmtu = IB_MTU_1024;
 +				break;
 +			case 512:
 +				pmtu = IB_MTU_512;
 +				break;
 +			case 256:
 +				pmtu = IB_MTU_256;
 +				break;
 +			default:
 +				pmtu = IB_MTU_2048;
 +			}
 +		} else
 +			pmtu = attr->path_mtu;
 +	}
 +
 +	if (attr_mask & IB_QP_PATH_MIG_STATE) {
 +		if (attr->path_mig_state == IB_MIG_REARM) {
 +			if (qp->s_mig_state == IB_MIG_ARMED)
 +				goto inval;
 +			if (new_state != IB_QPS_RTS)
 +				goto inval;
 +		} else if (attr->path_mig_state == IB_MIG_MIGRATED) {
 +			if (qp->s_mig_state == IB_MIG_REARM)
 +				goto inval;
 +			if (new_state != IB_QPS_RTS && new_state != IB_QPS_SQD)
 +				goto inval;
 +			if (qp->s_mig_state == IB_MIG_ARMED)
 +				mig = 1;
 +		} else
 +			goto inval;
 +	}
 +
 +	if (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)
 +		if (attr->max_dest_rd_atomic > QIB_MAX_RDMA_ATOMIC)
 +			goto inval;
 +
 +	switch (new_state) {
 +	case IB_QPS_RESET:
 +		if (qp->state != IB_QPS_RESET) {
 +			qp->state = IB_QPS_RESET;
 +			spin_lock(&dev->pending_lock);
 +			if (!list_empty(&priv->iowait))
 +				list_del_init(&priv->iowait);
 +			spin_unlock(&dev->pending_lock);
 +			qp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);
 +			spin_unlock(&qp->s_lock);
 +			spin_unlock_irq(&qp->r_lock);
 +			/* Stop the sending work queue and retry timer */
 +			cancel_work_sync(&priv->s_work);
 +			del_timer_sync(&qp->s_timer);
 +			wait_event(priv->wait_dma,
 +				   !atomic_read(&priv->s_dma_busy));
 +			if (priv->s_tx) {
 +				qib_put_txreq(priv->s_tx);
 +				priv->s_tx = NULL;
 +			}
 +			remove_qp(dev, qp);
 +			wait_event(qp->wait, !atomic_read(&qp->refcount));
 +			spin_lock_irq(&qp->r_lock);
 +			spin_lock(&qp->s_lock);
 +			clear_mr_refs(qp, 1);
 +			qib_reset_qp(qp, ibqp->qp_type);
 +		}
  		break;
 -	case 2048:
 -		enum_mtu = IB_MTU_2048;
 +
 +	case IB_QPS_RTR:
 +		/* Allow event to retrigger if QP set to RTR more than once */
 +		qp->r_flags &= ~QIB_R_COMM_EST;
 +		qp->state = new_state;
  		break;
 -	case 1024:
 -		enum_mtu = IB_MTU_1024;
 +
 +	case IB_QPS_SQD:
 +		qp->s_draining = qp->s_last != qp->s_cur;
 +		qp->state = new_state;
  		break;
 -	case 512:
 -		enum_mtu = IB_MTU_512;
 +
 +	case IB_QPS_SQE:
 +		if (qp->ibqp.qp_type == IB_QPT_RC)
 +			goto inval;
 +		qp->state = new_state;
  		break;
 -	case 256:
 -		enum_mtu = IB_MTU_256;
 +
 +	case IB_QPS_ERR:
 +		lastwqe = qib_error_qp(qp, IB_WC_WR_FLUSH_ERR);
  		break;
 +
  	default:
 -		enum_mtu = IB_MTU_2048;
 +		qp->state = new_state;
 +		break;
 +	}
 +
 +	if (attr_mask & IB_QP_PKEY_INDEX)
 +		qp->s_pkey_index = attr->pkey_index;
 +
 +	if (attr_mask & IB_QP_PORT)
 +		qp->port_num = attr->port_num;
 +
 +	if (attr_mask & IB_QP_DEST_QPN)
 +		qp->remote_qpn = attr->dest_qp_num;
 +
 +	if (attr_mask & IB_QP_SQ_PSN) {
 +		qp->s_next_psn = attr->sq_psn & QIB_PSN_MASK;
 +		qp->s_psn = qp->s_next_psn;
 +		qp->s_sending_psn = qp->s_next_psn;
 +		qp->s_last_psn = qp->s_next_psn - 1;
 +		qp->s_sending_hpsn = qp->s_last_psn;
 +	}
 +
 +	if (attr_mask & IB_QP_RQ_PSN)
 +		qp->r_psn = attr->rq_psn & QIB_PSN_MASK;
 +
 +	if (attr_mask & IB_QP_ACCESS_FLAGS)
 +		qp->qp_access_flags = attr->qp_access_flags;
 +
 +	if (attr_mask & IB_QP_AV) {
 +		qp->remote_ah_attr = attr->ah_attr;
 +		qp->s_srate = attr->ah_attr.static_rate;
 +	}
 +
 +	if (attr_mask & IB_QP_ALT_PATH) {
 +		qp->alt_ah_attr = attr->alt_ah_attr;
 +		qp->s_alt_pkey_index = attr->alt_pkey_index;
 +	}
 +
 +	if (attr_mask & IB_QP_PATH_MIG_STATE) {
 +		qp->s_mig_state = attr->path_mig_state;
 +		if (mig) {
 +			qp->remote_ah_attr = qp->alt_ah_attr;
 +			qp->port_num = qp->alt_ah_attr.port_num;
 +			qp->s_pkey_index = qp->s_alt_pkey_index;
 +		}
 +	}
 +
 +	if (attr_mask & IB_QP_PATH_MTU) {
 +		qp->path_mtu = pmtu;
 +		qp->pmtu = ib_mtu_enum_to_int(pmtu);
 +	}
 +
 +	if (attr_mask & IB_QP_RETRY_CNT) {
 +		qp->s_retry_cnt = attr->retry_cnt;
 +		qp->s_retry = attr->retry_cnt;
 +	}
 +
 +	if (attr_mask & IB_QP_RNR_RETRY) {
 +		qp->s_rnr_retry_cnt = attr->rnr_retry;
 +		qp->s_rnr_retry = attr->rnr_retry;
 +	}
 +
 +	if (attr_mask & IB_QP_MIN_RNR_TIMER)
 +		qp->r_min_rnr_timer = attr->min_rnr_timer;
 +
 +	if (attr_mask & IB_QP_TIMEOUT) {
 +		qp->timeout = attr->timeout;
 +		qp->timeout_jiffies =
 +			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
 +				1000UL);
 +	}
 +
 +	if (attr_mask & IB_QP_QKEY)
 +		qp->qkey = attr->qkey;
 +
 +	if (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)
 +		qp->r_max_rd_atomic = attr->max_dest_rd_atomic;
 +
 +	if (attr_mask & IB_QP_MAX_QP_RD_ATOMIC)
 +		qp->s_max_rd_atomic = attr->max_rd_atomic;
 +
 +	spin_unlock(&qp->s_lock);
 +	spin_unlock_irq(&qp->r_lock);
 +
 +	if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT)
 +		insert_qp(dev, qp);
 +
 +	if (lastwqe) {
 +		ev.device = qp->ibqp.device;
 +		ev.element.qp = &qp->ibqp;
 +		ev.event = IB_EVENT_QP_LAST_WQE_REACHED;
 +		qp->ibqp.event_handler(&ev, qp->ibqp.qp_context);
  	}
 -	return enum_mtu;
 +	if (mig) {
 +		ev.device = qp->ibqp.device;
 +		ev.element.qp = &qp->ibqp;
 +		ev.event = IB_EVENT_PATH_MIG;
 +		qp->ibqp.event_handler(&ev, qp->ibqp.qp_context);
 +	}
 +	ret = 0;
 +	goto bail;
 +
 +inval:
 +	spin_unlock(&qp->s_lock);
 +	spin_unlock_irq(&qp->r_lock);
 +	ret = -EINVAL;
 +
 +bail:
 +	return ret;
  }
  
 -int qib_get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,
 -			   struct ib_qp_attr *attr)
 +int qib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		 int attr_mask, struct ib_qp_init_attr *init_attr)
  {
 -	int mtu, pmtu, pidx = qp->port_num - 1;
 -	struct qib_ibdev *verbs_dev = container_of(rdi, struct qib_ibdev, rdi);
 -	struct qib_devdata *dd = container_of(verbs_dev, struct qib_devdata,
 -					      verbs_dev);
 -	mtu = ib_mtu_enum_to_int(attr->path_mtu);
 -	if (mtu == -1)
 -		return -EINVAL;
 -
 -	if (mtu > dd->pport[pidx].ibmtu)
 -		pmtu = mtu_to_enum(dd->pport[pidx].ibmtu);
 +	struct qib_qp *qp = to_iqp(ibqp);
 +
 +	attr->qp_state = qp->state;
 +	attr->cur_qp_state = attr->qp_state;
 +	attr->path_mtu = qp->path_mtu;
 +	attr->path_mig_state = qp->s_mig_state;
 +	attr->qkey = qp->qkey;
 +	attr->rq_psn = qp->r_psn & QIB_PSN_MASK;
 +	attr->sq_psn = qp->s_next_psn & QIB_PSN_MASK;
 +	attr->dest_qp_num = qp->remote_qpn;
 +	attr->qp_access_flags = qp->qp_access_flags;
 +	attr->cap.max_send_wr = qp->s_size - 1;
 +	attr->cap.max_recv_wr = qp->ibqp.srq ? 0 : qp->r_rq.size - 1;
 +	attr->cap.max_send_sge = qp->s_max_sge;
 +	attr->cap.max_recv_sge = qp->r_rq.max_sge;
 +	attr->cap.max_inline_data = 0;
 +	attr->ah_attr = qp->remote_ah_attr;
 +	attr->alt_ah_attr = qp->alt_ah_attr;
 +	attr->pkey_index = qp->s_pkey_index;
 +	attr->alt_pkey_index = qp->s_alt_pkey_index;
 +	attr->en_sqd_async_notify = 0;
 +	attr->sq_draining = qp->s_draining;
 +	attr->max_rd_atomic = qp->s_max_rd_atomic;
 +	attr->max_dest_rd_atomic = qp->r_max_rd_atomic;
 +	attr->min_rnr_timer = qp->r_min_rnr_timer;
 +	attr->port_num = qp->port_num;
 +	attr->timeout = qp->timeout;
 +	attr->retry_cnt = qp->s_retry_cnt;
 +	attr->rnr_retry = qp->s_rnr_retry_cnt;
 +	attr->alt_port_num = qp->alt_ah_attr.port_num;
 +	attr->alt_timeout = qp->alt_timeout;
 +
 +	init_attr->event_handler = qp->ibqp.event_handler;
 +	init_attr->qp_context = qp->ibqp.qp_context;
 +	init_attr->send_cq = qp->ibqp.send_cq;
 +	init_attr->recv_cq = qp->ibqp.recv_cq;
 +	init_attr->srq = qp->ibqp.srq;
 +	init_attr->cap = attr->cap;
 +	if (qp->s_flags & QIB_S_SIGNAL_REQ_WR)
 +		init_attr->sq_sig_type = IB_SIGNAL_REQ_WR;
  	else
++<<<<<<< HEAD
 +		init_attr->sq_sig_type = IB_SIGNAL_ALL_WR;
 +	init_attr->qp_type = qp->ibqp.qp_type;
 +	init_attr->port_num = qp->port_num;
 +	return 0;
++=======
+ 		pmtu = attr->path_mtu;
+ 	return pmtu;
+ }
+ 
+ int qib_mtu_to_path_mtu(u32 mtu)
+ {
+ 	return mtu_to_enum(mtu);
+ }
+ 
+ u32 qib_mtu_from_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp, u32 pmtu)
+ {
+ 	return ib_mtu_enum_to_int(pmtu);
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  }
  
  /**
@@@ -967,326 -378,63 +1000,348 @@@ __be32 qib_compute_aeth(struct qib_qp *
  	return cpu_to_be32(aeth);
  }
  
++<<<<<<< HEAD
 +/**
 + * qib_create_qp - create a queue pair for a device
 + * @ibpd: the protection domain who's device we create the queue pair for
 + * @init_attr: the attributes of the queue pair
 + * @udata: user data for libibverbs.so
 + *
 + * Returns the queue pair on success, otherwise returns an errno.
 + *
 + * Called by the ib_create_qp() core verbs function.
 + */
 +struct ib_qp *qib_create_qp(struct ib_pd *ibpd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata)
++=======
+ void *qib_qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp, gfp_t gfp)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
 +	struct qib_qp *qp;
 +	int err;
 +	struct qib_swqe *swq = NULL;
 +	struct qib_ibdev *dev;
 +	struct qib_devdata *dd;
 +	size_t sz;
 +	size_t sg_list_sz;
 +	struct ib_qp *ret;
 +	gfp_t gfp;
  	struct qib_qp_priv *priv;
  
 -	priv = kzalloc(sizeof(*priv), gfp);
 -	if (!priv)
 -		return ERR_PTR(-ENOMEM);
 -	priv->owner = qp;
 +	if (init_attr->cap.max_send_sge > ib_qib_max_sges ||
 +	    init_attr->cap.max_send_wr > ib_qib_max_qp_wrs ||
 +	    init_attr->create_flags & ~(IB_QP_CREATE_USE_GFP_NOIO))
 +		return ERR_PTR(-EINVAL);
 +
 +	/* GFP_NOIO is applicable in RC QPs only */
 +	if (init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO &&
 +	    init_attr->qp_type != IB_QPT_RC)
 +		return ERR_PTR(-EINVAL);
 +
 +	gfp = init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO ?
 +			GFP_NOIO : GFP_KERNEL;
  
 -	priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 -	if (!priv->s_hdr) {
 -		kfree(priv);
 -		return ERR_PTR(-ENOMEM);
 +	/* Check receive queue parameters if no SRQ is specified. */
 +	if (!init_attr->srq) {
 +		if (init_attr->cap.max_recv_sge > ib_qib_max_sges ||
 +		    init_attr->cap.max_recv_wr > ib_qib_max_qp_wrs) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +		if (init_attr->cap.max_send_sge +
 +		    init_attr->cap.max_send_wr +
 +		    init_attr->cap.max_recv_sge +
 +		    init_attr->cap.max_recv_wr == 0) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
  	}
 -	init_waitqueue_head(&priv->wait_dma);
 -	INIT_WORK(&priv->s_work, _qib_do_send);
 -	INIT_LIST_HEAD(&priv->iowait);
  
 -	return priv;
 -}
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_SMI:
 +	case IB_QPT_GSI:
 +		if (init_attr->port_num == 0 ||
 +		    init_attr->port_num > ibpd->device->phys_port_cnt) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +	case IB_QPT_UC:
 +	case IB_QPT_RC:
 +	case IB_QPT_UD:
 +		sz = sizeof(struct qib_sge) *
 +			init_attr->cap.max_send_sge +
 +			sizeof(struct qib_swqe);
 +		swq = __vmalloc((init_attr->cap.max_send_wr + 1) * sz,
 +				gfp, PAGE_KERNEL);
 +		if (swq == NULL) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail;
 +		}
 +		sz = sizeof(*qp);
 +		sg_list_sz = 0;
 +		if (init_attr->srq) {
 +			struct qib_srq *srq = to_isrq(init_attr->srq);
  
++<<<<<<< HEAD
 +			if (srq->rq.max_sge > 1)
 +				sg_list_sz = sizeof(*qp->r_sg_list) *
 +					(srq->rq.max_sge - 1);
 +		} else if (init_attr->cap.max_recv_sge > 1)
 +			sg_list_sz = sizeof(*qp->r_sg_list) *
 +				(init_attr->cap.max_recv_sge - 1);
 +		qp = kzalloc(sz + sg_list_sz, gfp);
 +		if (!qp) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_swq;
 +		}
 +		RCU_INIT_POINTER(qp->next, NULL);
 +		priv = kzalloc(sizeof(*priv), gfp);
 +		if (!priv) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp_hdr;
 +		}
 +		priv->owner = qp;
 +		priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 +		if (!priv->s_hdr) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp;
 +		}
 +		qp->priv = priv;
 +		qp->timeout_jiffies =
 +			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
 +				1000UL);
 +		if (init_attr->srq)
 +			sz = 0;
 +		else {
 +			qp->r_rq.size = init_attr->cap.max_recv_wr + 1;
 +			qp->r_rq.max_sge = init_attr->cap.max_recv_sge;
 +			sz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +
 +				sizeof(struct qib_rwqe);
 +			if (gfp != GFP_NOIO)
 +				qp->r_rq.wq = vmalloc_user(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz);
 +			else
 +				qp->r_rq.wq = __vmalloc(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz,
 +						gfp, PAGE_KERNEL);
++=======
+ void qib_qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)
+ {
+ 	struct qib_qp_priv *priv = qp->priv;
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
 +
 +			if (!qp->r_rq.wq) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_qp;
 +			}
 +		}
 +
 +		/*
 +		 * ib_create_qp() will initialize qp->ibqp
 +		 * except for qp->ibqp.qp_num.
 +		 */
 +		spin_lock_init(&qp->r_lock);
 +		spin_lock_init(&qp->s_lock);
 +		spin_lock_init(&qp->r_rq.lock);
 +		atomic_set(&qp->refcount, 0);
 +		init_waitqueue_head(&qp->wait);
 +		init_waitqueue_head(&priv->wait_dma);
 +		init_timer(&qp->s_timer);
 +		qp->s_timer.data = (unsigned long)qp;
 +		INIT_WORK(&priv->s_work, qib_do_send);
 +		INIT_LIST_HEAD(&priv->iowait);
 +		INIT_LIST_HEAD(&qp->rspwait);
 +		qp->state = IB_QPS_RESET;
 +		qp->s_wq = swq;
 +		qp->s_size = init_attr->cap.max_send_wr + 1;
 +		qp->s_max_sge = init_attr->cap.max_send_sge;
 +		if (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)
 +			qp->s_flags = QIB_S_SIGNAL_REQ_WR;
 +		dev = to_idev(ibpd->device);
 +		dd = dd_from_dev(dev);
 +		err = alloc_qpn(dd, &dev->qpn_table, init_attr->qp_type,
 +				init_attr->port_num, gfp);
 +		if (err < 0) {
 +			ret = ERR_PTR(err);
 +			vfree(qp->r_rq.wq);
 +			goto bail_qp;
 +		}
 +		qp->ibqp.qp_num = err;
 +		qp->port_num = init_attr->port_num;
 +		qib_reset_qp(qp, init_attr->qp_type);
 +		break;
 +
 +	default:
 +		/* Don't support raw QPs */
 +		ret = ERR_PTR(-ENOSYS);
 +		goto bail;
 +	}
 +
 +	init_attr->cap.max_inline_data = 0;
 +
 +	/*
 +	 * Return the address of the RWQ as the offset to mmap.
 +	 * See qib_mmap() for details.
 +	 */
 +	if (udata && udata->outlen >= sizeof(__u64)) {
 +		if (!qp->r_rq.wq) {
 +			__u64 offset = 0;
 +
 +			err = ib_copy_to_udata(udata, &offset,
 +					       sizeof(offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		} else {
 +			u32 s = sizeof(struct qib_rwq) + qp->r_rq.size * sz;
 +
 +			qp->ip = qib_create_mmap_info(dev, s,
 +						      ibpd->uobject->context,
 +						      qp->r_rq.wq);
 +			if (!qp->ip) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_ip;
 +			}
 +
 +			err = ib_copy_to_udata(udata, &(qp->ip->offset),
 +					       sizeof(qp->ip->offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		}
 +	}
 +
 +	spin_lock(&dev->n_qps_lock);
 +	if (dev->n_qps_allocated == ib_qib_max_qps) {
 +		spin_unlock(&dev->n_qps_lock);
 +		ret = ERR_PTR(-ENOMEM);
 +		goto bail_ip;
 +	}
  
 +	dev->n_qps_allocated++;
 +	spin_unlock(&dev->n_qps_lock);
 +
 +	if (qp->ip) {
 +		spin_lock_irq(&dev->pending_lock);
 +		list_add(&qp->ip->pending_mmaps, &dev->pending_mmaps);
 +		spin_unlock_irq(&dev->pending_lock);
 +	}
 +
 +	ret = &qp->ibqp;
 +	goto bail;
 +
 +bail_ip:
 +	if (qp->ip)
 +		kref_put(&qp->ip->ref, qib_release_mmap_info);
 +	else
 +		vfree(qp->r_rq.wq);
 +	free_qpn(&dev->qpn_table, qp->ibqp.qp_num);
 +bail_qp:
  	kfree(priv->s_hdr);
  	kfree(priv);
 +bail_qp_hdr:
 +	kfree(qp);
 +bail_swq:
 +	vfree(swq);
 +bail:
 +	return ret;
  }
  
++<<<<<<< HEAD
 +/**
 + * qib_destroy_qp - destroy a queue pair
 + * @ibqp: the queue pair to destroy
 + *
 + * Returns 0 on success.
 + *
 + * Note that this can be called while the QP is actively sending or
 + * receiving!
 + */
 +int qib_destroy_qp(struct ib_qp *ibqp)
++=======
+ void qib_stop_send_queue(struct rvt_qp *qp)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_ibdev *dev = to_idev(ibqp->device);
  	struct qib_qp_priv *priv = qp->priv;
  
 -	cancel_work_sync(&priv->s_work);
 -	del_timer_sync(&qp->s_timer);
 +	/* Make sure HW and driver activity is stopped. */
 +	spin_lock_irq(&qp->s_lock);
 +	if (qp->state != IB_QPS_RESET) {
 +		qp->state = IB_QPS_RESET;
 +		spin_lock(&dev->pending_lock);
 +		if (!list_empty(&priv->iowait))
 +			list_del_init(&priv->iowait);
 +		spin_unlock(&dev->pending_lock);
 +		qp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);
 +		spin_unlock_irq(&qp->s_lock);
 +		cancel_work_sync(&priv->s_work);
 +		del_timer_sync(&qp->s_timer);
 +		wait_event(priv->wait_dma, !atomic_read(&priv->s_dma_busy));
 +		if (priv->s_tx) {
 +			qib_put_txreq(priv->s_tx);
 +			priv->s_tx = NULL;
 +		}
 +		remove_qp(dev, qp);
 +		wait_event(qp->wait, !atomic_read(&qp->refcount));
 +		clear_mr_refs(qp, 1);
 +	} else
 +		spin_unlock_irq(&qp->s_lock);
 +
 +	/* all user's cleaned up, mark it available */
 +	free_qpn(&dev->qpn_table, qp->ibqp.qp_num);
 +	spin_lock(&dev->n_qps_lock);
 +	dev->n_qps_allocated--;
 +	spin_unlock(&dev->n_qps_lock);
 +
 +	if (qp->ip)
 +		kref_put(&qp->ip->ref, qib_release_mmap_info);
 +	else
 +		vfree(qp->r_rq.wq);
 +	vfree(qp->s_wq);
 +	kfree(priv->s_hdr);
 +	kfree(priv);
 +	kfree(qp);
 +	return 0;
  }
  
++<<<<<<< HEAD
 +/**
 + * qib_init_qpn_table - initialize the QP number table for a device
 + * @qpt: the QPN table
 + */
 +void qib_init_qpn_table(struct qib_devdata *dd, struct qib_qpn_table *qpt)
++=======
+ void qib_quiesce_qp(struct rvt_qp *qp)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
 -	struct qib_qp_priv *priv = qp->priv;
 -
 -	wait_event(priv->wait_dma, !atomic_read(&priv->s_dma_busy));
 -	if (priv->s_tx) {
 -		qib_put_txreq(priv->s_tx);
 -		priv->s_tx = NULL;
 -	}
 +	spin_lock_init(&qpt->lock);
 +	qpt->last = 1;          /* start with QPN 2 */
 +	qpt->nmaps = 1;
 +	qpt->mask = dd->qpn_mask;
  }
  
++<<<<<<< HEAD
 +/**
 + * qib_free_qpn_table - free the QP number table for a device
 + * @qpt: the QPN table
 + */
 +void qib_free_qpn_table(struct qib_qpn_table *qpt)
++=======
+ void qib_flush_qp_waiters(struct rvt_qp *qp)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
 -	struct qib_qp_priv *priv = qp->priv;
 -	struct qib_ibdev *dev = to_idev(qp->ibqp.device);
 +	int i;
  
 -	spin_lock(&dev->rdi.pending_lock);
 -	if (!list_empty(&priv->iowait))
 -		list_del_init(&priv->iowait);
 -	spin_unlock(&dev->rdi.pending_lock);
 +	for (i = 0; i < ARRAY_SIZE(qpt->map); i++)
 +		if (qpt->map[i].page)
 +			free_page((unsigned long) qpt->map[i].page);
  }
  
  /**
diff --cc drivers/infiniband/hw/qib/qib_verbs.c
index c4417a1f33be,fa94f78073cf..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.c
+++ b/drivers/infiniband/hw/qib/qib_verbs.c
@@@ -1710,20 -1394,14 +1710,24 @@@ bail
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int qib_modify_port(struct ib_device *ibdev, u8 port,
 +			   int port_modify_mask, struct ib_port_modify *props)
++=======
+ static int qib_shut_down_port(struct rvt_dev_info *rdi, u8 port_num)
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  {
 -	struct qib_ibdev *ibdev = container_of(rdi, struct qib_ibdev, rdi);
 -	struct qib_devdata *dd = dd_from_dev(ibdev);
 -	struct qib_pportdata *ppd = &dd->pport[port_num - 1];
 -
 -	qib_set_linkstate(ppd, QIB_IB_LINKDOWN);
 +	struct qib_ibport *ibp = to_iport(ibdev, port);
 +	struct qib_pportdata *ppd = ppd_from_ibp(ibp);
  
 +	ibp->port_cap_flags |= props->set_port_cap_mask;
 +	ibp->port_cap_flags &= ~props->clr_port_cap_mask;
 +	if (props->set_port_cap_mask || props->clr_port_cap_mask)
 +		qib_cap_mask_chg(ibp);
 +	if (port_modify_mask & IB_PORT_SHUTDOWN)
 +		qib_set_linkstate(ppd, QIB_IB_LINKDOWN);
 +	if (port_modify_mask & IB_PORT_RESET_QKEY_CNTR)
 +		ibp->qkey_violations = 0;
  	return 0;
  }
  
@@@ -2283,11 -1661,58 +2287,33 @@@ int qib_register_ib_device(struct qib_d
  	dd->verbs_dev.rdi.driver_f.port_callback = qib_create_port_files;
  	dd->verbs_dev.rdi.driver_f.get_card_name = qib_get_card_name;
  	dd->verbs_dev.rdi.driver_f.get_pci_dev = qib_get_pci_dev;
++<<<<<<< HEAD
 +	dd->verbs_dev.rdi.dparms.props.max_pd = ib_qib_max_pds;
 +	dd->verbs_dev.rdi.flags = (RVT_FLAG_MR_INIT_DRIVER |
 +				   RVT_FLAG_QP_INIT_DRIVER |
 +				   RVT_FLAG_CQ_INIT_DRIVER);
++=======
+ 	dd->verbs_dev.rdi.driver_f.check_ah = qib_check_ah;
+ 	dd->verbs_dev.rdi.driver_f.notify_new_ah = qib_notify_new_ah;
+ 	dd->verbs_dev.rdi.driver_f.alloc_qpn = qib_alloc_qpn;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_alloc = qib_qp_priv_alloc;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_free = qib_qp_priv_free;
+ 	dd->verbs_dev.rdi.driver_f.free_all_qps = qib_free_all_qps;
+ 	dd->verbs_dev.rdi.driver_f.notify_qp_reset = qib_notify_qp_reset;
+ 	dd->verbs_dev.rdi.driver_f.do_send = qib_do_send;
+ 	dd->verbs_dev.rdi.driver_f.schedule_send = qib_schedule_send;
+ 	dd->verbs_dev.rdi.driver_f.quiesce_qp = qib_quiesce_qp;
+ 	dd->verbs_dev.rdi.driver_f.stop_send_queue = qib_stop_send_queue;
+ 	dd->verbs_dev.rdi.driver_f.flush_qp_waiters = qib_flush_qp_waiters;
+ 	dd->verbs_dev.rdi.driver_f.notify_error_qp = qib_notify_error_qp;
+ 	dd->verbs_dev.rdi.driver_f.mtu_to_path_mtu = qib_mtu_to_path_mtu;
+ 	dd->verbs_dev.rdi.driver_f.mtu_from_qp = qib_mtu_from_qp;
+ 	dd->verbs_dev.rdi.driver_f.get_pmtu_from_attr = qib_get_pmtu_from_attr;
+ 	dd->verbs_dev.rdi.driver_f.query_port_state = qib_query_port;
+ 	dd->verbs_dev.rdi.driver_f.shut_down_port = qib_shut_down_port;
+ 	dd->verbs_dev.rdi.driver_f.cap_mask_chg = qib_cap_mask_chg;
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  
 -	dd->verbs_dev.rdi.dparms.max_rdma_atomic = QIB_MAX_RDMA_ATOMIC;
 -	dd->verbs_dev.rdi.driver_f.get_guid_be = qib_get_guid_be;
 -	dd->verbs_dev.rdi.dparms.lkey_table_size = qib_lkey_table_size;
 -	dd->verbs_dev.rdi.dparms.qp_table_size = ib_qib_qp_table_size;
 -	dd->verbs_dev.rdi.dparms.qpn_start = 1;
 -	dd->verbs_dev.rdi.dparms.qpn_res_start = QIB_KD_QP;
 -	dd->verbs_dev.rdi.dparms.qpn_res_end = QIB_KD_QP; /* Reserve one QP */
 -	dd->verbs_dev.rdi.dparms.qpn_inc = 1;
 -	dd->verbs_dev.rdi.dparms.qos_shift = 1;
 -	dd->verbs_dev.rdi.dparms.psn_mask = QIB_PSN_MASK;
 -	dd->verbs_dev.rdi.dparms.psn_shift = QIB_PSN_SHIFT;
 -	dd->verbs_dev.rdi.dparms.psn_modify_mask = QIB_PSN_MASK;
 -	dd->verbs_dev.rdi.dparms.nports = dd->num_pports;
 -	dd->verbs_dev.rdi.dparms.npkeys = qib_get_npkeys(dd);
 -	dd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;
 -	dd->verbs_dev.rdi.dparms.core_cap_flags = RDMA_CORE_PORT_IBA_IB;
 -	dd->verbs_dev.rdi.dparms.max_mad_size = IB_MGMT_MAD_SIZE;
 -
 -	snprintf(dd->verbs_dev.rdi.dparms.cq_name,
 -		 sizeof(dd->verbs_dev.rdi.dparms.cq_name),
 -		 "qib_cq%d", dd->unit);
 -
 -	qib_fill_device_attr(dd);
 -
 -	ppd = dd->pport;
 -	for (i = 0; i < dd->num_pports; i++, ppd++) {
 -		ctxt = ppd->hw_pidx;
 -		rvt_init_port(&dd->verbs_dev.rdi,
 -			      &ppd->ibport_data.rvp,
 -			      i,
 -			      dd->rcd[ctxt]->pkeys);
 -	}
  
  	ret = rvt_register_device(&dd->verbs_dev.rdi);
  	if (ret)
diff --cc drivers/infiniband/hw/qib/qib_verbs.h
index ca366073af4f,b88e027b6cb0..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.h
+++ b/drivers/infiniband/hw/qib/qib_verbs.h
@@@ -913,35 -344,17 +913,47 @@@ int qib_snapshot_counters(struct qib_pp
  int qib_get_counters(struct qib_pportdata *ppd,
  		     struct qib_verbs_counters *cntrs);
  
 -__be32 qib_compute_aeth(struct rvt_qp *qp);
 +int qib_multicast_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid);
 +
++<<<<<<< HEAD
 +int qib_multicast_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid);
 +
 +int qib_mcast_tree_empty(struct qib_ibport *ibp);
 +
 +__be32 qib_compute_aeth(struct qib_qp *qp);
 +
 +struct qib_qp *qib_lookup_qpn(struct qib_ibport *ibp, u32 qpn);
 +
 +struct ib_qp *qib_create_qp(struct ib_pd *ibpd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata);
 +
 +int qib_destroy_qp(struct ib_qp *ibqp);
 +
 +int qib_error_qp(struct qib_qp *qp, enum ib_wc_status err);
 +
 +int qib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		  int attr_mask, struct ib_udata *udata);
 +
 +int qib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		 int attr_mask, struct ib_qp_init_attr *init_attr);
 +
 +unsigned qib_free_all_qps(struct qib_devdata *dd);
  
 +void qib_init_qpn_table(struct qib_devdata *dd, struct qib_qpn_table *qpt);
 +
 +void qib_free_qpn_table(struct qib_qpn_table *qpt);
++=======
+ /*
+  * Functions provided by qib driver for rdmavt to use
+  */
+ unsigned qib_free_all_qps(struct rvt_dev_info *rdi);
+ void *qib_qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp, gfp_t gfp);
+ void qib_qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp);
+ void qib_notify_qp_reset(struct rvt_qp *qp);
+ int qib_alloc_qpn(struct rvt_dev_info *rdi, struct rvt_qpn_table *qpt,
+ 		  enum ib_qp_type type, u8 port, gfp_t gfp);
++>>>>>>> 20f333b61300 (IB/qib: Rename several functions by adding a "qib_" prefix)
  
  #ifdef CONFIG_DEBUG_FS
  
* Unmerged path drivers/infiniband/hw/qib/qib.h
* Unmerged path drivers/infiniband/hw/qib/qib_qp.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.h
