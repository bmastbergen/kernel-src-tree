vxlan: simplify vxlan_remcsum

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jiri Benc <jbenc@redhat.com>
commit be5cfeab8f95995d5590ab919b9f4dde19d50ea7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/be5cfeab.failed

Part of the parameters is not needed. Simplify the caller of this function
in preparation of making vxlan rx more comprehensible.

	Signed-off-by: Jiri Benc <jbenc@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit be5cfeab8f95995d5590ab919b9f4dde19d50ea7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/vxlan.c
diff --cc drivers/net/vxlan.c
index a2751f4f523c,161e39ce3914..000000000000
--- a/drivers/net/vxlan.c
+++ b/drivers/net/vxlan.c
@@@ -1128,124 -1135,30 +1128,126 @@@ static int vxlan_igmp_leave(struct vxla
  	return ret;
  }
  
++<<<<<<< HEAD
 +static struct vxlanhdr *vxlan_remcsum(struct sk_buff *skb, struct vxlanhdr *vh,
 +				      size_t hdrlen, u32 data, bool nopartial)
++=======
+ static bool vxlan_remcsum(struct sk_buff *skb, u32 vxflags, __be32 vni_field)
++>>>>>>> be5cfeab8f95 (vxlan: simplify vxlan_remcsum)
  {
  	size_t start, offset, plen;
  
  	if (skb->remcsum_offload)
- 		return vh;
+ 		return true;
  
 -	start = vxlan_rco_start(vni_field);
 -	offset = start + vxlan_rco_offset(vni_field);
 +	start = (data & VXLAN_RCO_MASK) << VXLAN_RCO_SHIFT;
 +	offset = start + ((data & VXLAN_RCO_UDP) ?
 +			  offsetof(struct udphdr, check) :
 +			  offsetof(struct tcphdr, check));
  
- 	plen = hdrlen + offset + sizeof(u16);
+ 	plen = sizeof(struct vxlanhdr) + offset + sizeof(u16);
  
  	if (!pskb_may_pull(skb, plen))
- 		return NULL;
- 
- 	vh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
+ 		return false;
  
- 	skb_remcsum_process(skb, (void *)vh + hdrlen, start, offset,
- 			    nopartial);
+ 	skb_remcsum_process(skb, (void *)(vxlan_hdr(skb) + 1), start, offset,
+ 			    !!(vxflags & VXLAN_F_REMCSUM_NOPARTIAL));
  
- 	return vh;
+ 	return true;
  }
  
 +/* Callback from net/ipv4/udp.c to receive packets */
 +static int vxlan_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 +{
 +	struct vxlan_sock *vs;
 +	struct vxlanhdr *vxh;
 +	u32 flags, vni;
 +	struct vxlan_metadata md = {0};
 +
 +	/* Need Vxlan and inner Ethernet header to be present */
 +	if (!pskb_may_pull(skb, VXLAN_HLEN))
 +		goto error;
 +
 +	vxh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
 +	flags = ntohl(vxh->vx_flags);
 +	vni = ntohl(vxh->vx_vni);
 +
 +	if (flags & VXLAN_HF_VNI) {
 +		flags &= ~VXLAN_HF_VNI;
 +	} else {
 +		/* VNI flag always required to be set */
 +		goto bad_flags;
 +	}
 +
 +	if (iptunnel_pull_header(skb, VXLAN_HLEN, htons(ETH_P_TEB)))
 +		goto drop;
 +	vxh = (struct vxlanhdr *)(udp_hdr(skb) + 1);
 +
 +	vs = rcu_dereference_sk_user_data(sk);
 +	if (!vs)
 +		goto drop;
 +
 +	if ((flags & VXLAN_HF_RCO) && (vs->flags & VXLAN_F_REMCSUM_RX)) {
 +		vxh = vxlan_remcsum(skb, vxh, sizeof(struct vxlanhdr), vni,
 +				    !!(vs->flags & VXLAN_F_REMCSUM_NOPARTIAL));
 +		if (!vxh)
 +			goto drop;
 +
 +		flags &= ~VXLAN_HF_RCO;
 +		vni &= VXLAN_VNI_MASK;
 +	}
 +
 +	/* For backwards compatibility, only allow reserved fields to be
 +	 * used by VXLAN extensions if explicitly requested.
 +	 */
 +	if ((flags & VXLAN_HF_GBP) && (vs->flags & VXLAN_F_GBP)) {
 +		struct vxlanhdr_gbp *gbp;
 +
 +		gbp = (struct vxlanhdr_gbp *)vxh;
 +		md.gbp = ntohs(gbp->policy_id);
 +
 +		if (gbp->dont_learn)
 +			md.gbp |= VXLAN_GBP_DONT_LEARN;
 +
 +		if (gbp->policy_applied)
 +			md.gbp |= VXLAN_GBP_POLICY_APPLIED;
 +
 +		flags &= ~VXLAN_GBP_USED_BITS;
 +	}
 +
 +	if (flags || vni & ~VXLAN_VNI_MASK) {
 +		/* If there are any unprocessed flags remaining treat
 +		 * this as a malformed packet. This behavior diverges from
 +		 * VXLAN RFC (RFC7348) which stipulates that bits in reserved
 +		 * in reserved fields are to be ignored. The approach here
 +		 * maintains compatibility with previous stack code, and also
 +		 * is more robust and provides a little more security in
 +		 * adding extensions to VXLAN.
 +		 */
 +
 +		goto bad_flags;
 +	}
 +
 +	md.vni = vxh->vx_vni;
 +	vs->rcv(vs, skb, &md);
 +	return 0;
 +
 +drop:
 +	/* Consume bad packet */
 +	kfree_skb(skb);
 +	return 0;
 +
 +bad_flags:
 +	netdev_dbg(skb->dev, "invalid vxlan flags=%#x vni=%#x\n",
 +		   ntohl(vxh->vx_flags), ntohl(vxh->vx_vni));
 +
 +error:
 +	/* Return non vxlan pkt */
 +	return 1;
 +}
 +
  static void vxlan_rcv(struct vxlan_sock *vs, struct sk_buff *skb,
 -		      struct vxlan_metadata *md, __be32 vni,
 -		      struct metadata_dst *tun_dst)
 +		      struct vxlan_metadata *md)
  {
  	struct iphdr *oip = NULL;
  	struct ipv6hdr *oip6 = NULL;
@@@ -1327,6 -1250,111 +1329,114 @@@ drop
  	kfree_skb(skb);
  }
  
++<<<<<<< HEAD
++=======
+ /* Callback from net/ipv4/udp.c to receive packets */
+ static int vxlan_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct metadata_dst *tun_dst = NULL;
+ 	struct vxlan_sock *vs;
+ 	__be32 flags, vni_field;
+ 	struct vxlan_metadata _md;
+ 	struct vxlan_metadata *md = &_md;
+ 
+ 	/* Need Vxlan and inner Ethernet header to be present */
+ 	if (!pskb_may_pull(skb, VXLAN_HLEN))
+ 		goto error;
+ 
+ 	flags = vxlan_hdr(skb)->vx_flags;
+ 	vni_field = vxlan_hdr(skb)->vx_vni;
+ 
+ 	if (flags & VXLAN_HF_VNI) {
+ 		flags &= ~VXLAN_HF_VNI;
+ 	} else {
+ 		/* VNI flag always required to be set */
+ 		goto bad_flags;
+ 	}
+ 
+ 	if (iptunnel_pull_header(skb, VXLAN_HLEN, htons(ETH_P_TEB)))
+ 		goto drop;
+ 
+ 	vs = rcu_dereference_sk_user_data(sk);
+ 	if (!vs)
+ 		goto drop;
+ 
+ 	if ((flags & VXLAN_HF_RCO) && (vs->flags & VXLAN_F_REMCSUM_RX)) {
+ 		if (!vxlan_remcsum(skb, vs->flags, vni_field))
+ 			goto drop;
+ 
+ 		flags &= ~VXLAN_HF_RCO;
+ 		vni_field &= VXLAN_VNI_MASK;
+ 	}
+ 
+ 	if (vxlan_collect_metadata(vs)) {
+ 		tun_dst = udp_tun_rx_dst(skb, vxlan_get_sk_family(vs), TUNNEL_KEY,
+ 					 vxlan_vni(vni_field), sizeof(*md));
+ 
+ 		if (!tun_dst)
+ 			goto drop;
+ 
+ 		md = ip_tunnel_info_opts(&tun_dst->u.tun_info);
+ 	} else {
+ 		memset(md, 0, sizeof(*md));
+ 	}
+ 
+ 	/* For backwards compatibility, only allow reserved fields to be
+ 	 * used by VXLAN extensions if explicitly requested.
+ 	 */
+ 	if ((flags & VXLAN_HF_GBP) && (vs->flags & VXLAN_F_GBP)) {
+ 		struct vxlanhdr_gbp *gbp;
+ 
+ 		gbp = (struct vxlanhdr_gbp *)vxlan_hdr(skb);
+ 		md->gbp = ntohs(gbp->policy_id);
+ 
+ 		if (tun_dst)
+ 			tun_dst->u.tun_info.key.tun_flags |= TUNNEL_VXLAN_OPT;
+ 
+ 		if (gbp->dont_learn)
+ 			md->gbp |= VXLAN_GBP_DONT_LEARN;
+ 
+ 		if (gbp->policy_applied)
+ 			md->gbp |= VXLAN_GBP_POLICY_APPLIED;
+ 
+ 		flags &= ~VXLAN_GBP_USED_BITS;
+ 	}
+ 
+ 	if (flags || vni_field & ~VXLAN_VNI_MASK) {
+ 		/* If there are any unprocessed flags remaining treat
+ 		 * this as a malformed packet. This behavior diverges from
+ 		 * VXLAN RFC (RFC7348) which stipulates that bits in reserved
+ 		 * in reserved fields are to be ignored. The approach here
+ 		 * maintains compatibility with previous stack code, and also
+ 		 * is more robust and provides a little more security in
+ 		 * adding extensions to VXLAN.
+ 		 */
+ 
+ 		goto bad_flags;
+ 	}
+ 
+ 	vxlan_rcv(vs, skb, md, vxlan_vni(vni_field), tun_dst);
+ 	return 0;
+ 
+ drop:
+ 	/* Consume bad packet */
+ 	kfree_skb(skb);
+ 	return 0;
+ 
+ bad_flags:
+ 	netdev_dbg(skb->dev, "invalid vxlan flags=%#x vni=%#x\n",
+ 		   ntohl(vxlan_hdr(skb)->vx_flags),
+ 		   ntohl(vxlan_hdr(skb)->vx_vni));
+ 
+ error:
+ 	if (tun_dst)
+ 		dst_release((struct dst_entry *)tun_dst);
+ 
+ 	/* Return non vxlan pkt */
+ 	return 1;
+ }
+ 
++>>>>>>> be5cfeab8f95 (vxlan: simplify vxlan_remcsum)
  static int arp_reduce(struct net_device *dev, struct sk_buff *skb)
  {
  	struct vxlan_dev *vxlan = netdev_priv(dev);
* Unmerged path drivers/net/vxlan.c
