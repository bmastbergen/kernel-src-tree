IB/mlx5: Add Scatter FCS support for Raw Packet QP

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Majd Dibbiny <majd@mellanox.com>
commit 358e42ea66e26d30a7a3e2c967c78f01ec31fe4f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/358e42ea.failed

Enable Scatter FCS in the RQ context when the user passes
Scatter FCS create flag.

	Signed-off-by: Majd Dibbiny <majd@mellanox.com>
	Signed-off-by: Matan Barak <matanb@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 358e42ea66e26d30a7a3e2c967c78f01ec31fe4f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 9b6378921a8d,fb0a110b66c0..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -315,6 -354,9 +315,12 @@@ enum mlx5_ib_qp_flags 
  	MLX5_IB_QP_MANAGED_SEND             = IB_QP_CREATE_MANAGED_SEND,
  	MLX5_IB_QP_MANAGED_RECV             = IB_QP_CREATE_MANAGED_RECV,
  	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 5,
++<<<<<<< HEAD
++=======
+ 	/* QP uses 1 as its source QP number */
+ 	MLX5_IB_QP_SQPN_QP1			= 1 << 6,
+ 	MLX5_IB_QP_CAP_SCATTER_FCS		= 1 << 7,
++>>>>>>> 358e42ea66e2 (IB/mlx5: Add Scatter FCS support for Raw Packet QP)
  };
  
  struct mlx5_umr_wr {
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 93bacd3a81a6,504117657d41..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -871,6 -917,288 +871,291 @@@ static int is_connected(enum ib_qp_typ
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int create_raw_packet_qp_tis(struct mlx5_ib_dev *dev,
+ 				    struct mlx5_ib_sq *sq, u32 tdn)
+ {
+ 	u32 in[MLX5_ST_SZ_DW(create_tis_in)];
+ 	void *tisc = MLX5_ADDR_OF(create_tis_in, in, ctx);
+ 
+ 	memset(in, 0, sizeof(in));
+ 
+ 	MLX5_SET(tisc, tisc, transport_domain, tdn);
+ 
+ 	return mlx5_core_create_tis(dev->mdev, in, sizeof(in), &sq->tisn);
+ }
+ 
+ static void destroy_raw_packet_qp_tis(struct mlx5_ib_dev *dev,
+ 				      struct mlx5_ib_sq *sq)
+ {
+ 	mlx5_core_destroy_tis(dev->mdev, sq->tisn);
+ }
+ 
+ static int create_raw_packet_qp_sq(struct mlx5_ib_dev *dev,
+ 				   struct mlx5_ib_sq *sq, void *qpin,
+ 				   struct ib_pd *pd)
+ {
+ 	struct mlx5_ib_ubuffer *ubuffer = &sq->ubuffer;
+ 	__be64 *pas;
+ 	void *in;
+ 	void *sqc;
+ 	void *qpc = MLX5_ADDR_OF(create_qp_in, qpin, qpc);
+ 	void *wq;
+ 	int inlen;
+ 	int err;
+ 	int page_shift = 0;
+ 	int npages;
+ 	int ncont = 0;
+ 	u32 offset = 0;
+ 
+ 	err = mlx5_ib_umem_get(dev, pd, ubuffer->buf_addr, ubuffer->buf_size,
+ 			       &sq->ubuffer.umem, &npages, &page_shift,
+ 			       &ncont, &offset);
+ 	if (err)
+ 		return err;
+ 
+ 	inlen = MLX5_ST_SZ_BYTES(create_sq_in) + sizeof(u64) * ncont;
+ 	in = mlx5_vzalloc(inlen);
+ 	if (!in) {
+ 		err = -ENOMEM;
+ 		goto err_umem;
+ 	}
+ 
+ 	sqc = MLX5_ADDR_OF(create_sq_in, in, ctx);
+ 	MLX5_SET(sqc, sqc, flush_in_error_en, 1);
+ 	MLX5_SET(sqc, sqc, state, MLX5_SQC_STATE_RST);
+ 	MLX5_SET(sqc, sqc, user_index, MLX5_GET(qpc, qpc, user_index));
+ 	MLX5_SET(sqc, sqc, cqn, MLX5_GET(qpc, qpc, cqn_snd));
+ 	MLX5_SET(sqc, sqc, tis_lst_sz, 1);
+ 	MLX5_SET(sqc, sqc, tis_num_0, sq->tisn);
+ 
+ 	wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 	MLX5_SET(wq, wq, wq_type, MLX5_WQ_TYPE_CYCLIC);
+ 	MLX5_SET(wq, wq, pd, MLX5_GET(qpc, qpc, pd));
+ 	MLX5_SET(wq, wq, uar_page, MLX5_GET(qpc, qpc, uar_page));
+ 	MLX5_SET64(wq, wq, dbr_addr, MLX5_GET64(qpc, qpc, dbr_addr));
+ 	MLX5_SET(wq, wq, log_wq_stride, ilog2(MLX5_SEND_WQE_BB));
+ 	MLX5_SET(wq, wq, log_wq_sz, MLX5_GET(qpc, qpc, log_sq_size));
+ 	MLX5_SET(wq, wq, log_wq_pg_sz,  page_shift - MLX5_ADAPTER_PAGE_SHIFT);
+ 	MLX5_SET(wq, wq, page_offset, offset);
+ 
+ 	pas = (__be64 *)MLX5_ADDR_OF(wq, wq, pas);
+ 	mlx5_ib_populate_pas(dev, sq->ubuffer.umem, page_shift, pas, 0);
+ 
+ 	err = mlx5_core_create_sq_tracked(dev->mdev, in, inlen, &sq->base.mqp);
+ 
+ 	kvfree(in);
+ 
+ 	if (err)
+ 		goto err_umem;
+ 
+ 	return 0;
+ 
+ err_umem:
+ 	ib_umem_release(sq->ubuffer.umem);
+ 	sq->ubuffer.umem = NULL;
+ 
+ 	return err;
+ }
+ 
+ static void destroy_raw_packet_qp_sq(struct mlx5_ib_dev *dev,
+ 				     struct mlx5_ib_sq *sq)
+ {
+ 	mlx5_core_destroy_sq_tracked(dev->mdev, &sq->base.mqp);
+ 	ib_umem_release(sq->ubuffer.umem);
+ }
+ 
+ static int get_rq_pas_size(void *qpc)
+ {
+ 	u32 log_page_size = MLX5_GET(qpc, qpc, log_page_size) + 12;
+ 	u32 log_rq_stride = MLX5_GET(qpc, qpc, log_rq_stride);
+ 	u32 log_rq_size   = MLX5_GET(qpc, qpc, log_rq_size);
+ 	u32 page_offset   = MLX5_GET(qpc, qpc, page_offset);
+ 	u32 po_quanta	  = 1 << (log_page_size - 6);
+ 	u32 rq_sz	  = 1 << (log_rq_size + 4 + log_rq_stride);
+ 	u32 page_size	  = 1 << log_page_size;
+ 	u32 rq_sz_po      = rq_sz + (page_offset * po_quanta);
+ 	u32 rq_num_pas	  = (rq_sz_po + page_size - 1) / page_size;
+ 
+ 	return rq_num_pas * sizeof(u64);
+ }
+ 
+ static int create_raw_packet_qp_rq(struct mlx5_ib_dev *dev,
+ 				   struct mlx5_ib_rq *rq, void *qpin)
+ {
+ 	struct mlx5_ib_qp *mqp = rq->base.container_mibqp;
+ 	__be64 *pas;
+ 	__be64 *qp_pas;
+ 	void *in;
+ 	void *rqc;
+ 	void *wq;
+ 	void *qpc = MLX5_ADDR_OF(create_qp_in, qpin, qpc);
+ 	int inlen;
+ 	int err;
+ 	u32 rq_pas_size = get_rq_pas_size(qpc);
+ 
+ 	inlen = MLX5_ST_SZ_BYTES(create_rq_in) + rq_pas_size;
+ 	in = mlx5_vzalloc(inlen);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	rqc = MLX5_ADDR_OF(create_rq_in, in, ctx);
+ 	MLX5_SET(rqc, rqc, vsd, 1);
+ 	MLX5_SET(rqc, rqc, mem_rq_type, MLX5_RQC_MEM_RQ_TYPE_MEMORY_RQ_INLINE);
+ 	MLX5_SET(rqc, rqc, state, MLX5_RQC_STATE_RST);
+ 	MLX5_SET(rqc, rqc, flush_in_error_en, 1);
+ 	MLX5_SET(rqc, rqc, user_index, MLX5_GET(qpc, qpc, user_index));
+ 	MLX5_SET(rqc, rqc, cqn, MLX5_GET(qpc, qpc, cqn_rcv));
+ 
+ 	if (mqp->flags & MLX5_IB_QP_CAP_SCATTER_FCS)
+ 		MLX5_SET(rqc, rqc, scatter_fcs, 1);
+ 
+ 	wq = MLX5_ADDR_OF(rqc, rqc, wq);
+ 	MLX5_SET(wq, wq, wq_type, MLX5_WQ_TYPE_CYCLIC);
+ 	MLX5_SET(wq, wq, end_padding_mode,
+ 		 MLX5_GET(qpc, qpc, end_padding_mode));
+ 	MLX5_SET(wq, wq, page_offset, MLX5_GET(qpc, qpc, page_offset));
+ 	MLX5_SET(wq, wq, pd, MLX5_GET(qpc, qpc, pd));
+ 	MLX5_SET64(wq, wq, dbr_addr, MLX5_GET64(qpc, qpc, dbr_addr));
+ 	MLX5_SET(wq, wq, log_wq_stride, MLX5_GET(qpc, qpc, log_rq_stride) + 4);
+ 	MLX5_SET(wq, wq, log_wq_pg_sz, MLX5_GET(qpc, qpc, log_page_size));
+ 	MLX5_SET(wq, wq, log_wq_sz, MLX5_GET(qpc, qpc, log_rq_size));
+ 
+ 	pas = (__be64 *)MLX5_ADDR_OF(wq, wq, pas);
+ 	qp_pas = (__be64 *)MLX5_ADDR_OF(create_qp_in, qpin, pas);
+ 	memcpy(pas, qp_pas, rq_pas_size);
+ 
+ 	err = mlx5_core_create_rq_tracked(dev->mdev, in, inlen, &rq->base.mqp);
+ 
+ 	kvfree(in);
+ 
+ 	return err;
+ }
+ 
+ static void destroy_raw_packet_qp_rq(struct mlx5_ib_dev *dev,
+ 				     struct mlx5_ib_rq *rq)
+ {
+ 	mlx5_core_destroy_rq_tracked(dev->mdev, &rq->base.mqp);
+ }
+ 
+ static int create_raw_packet_qp_tir(struct mlx5_ib_dev *dev,
+ 				    struct mlx5_ib_rq *rq, u32 tdn)
+ {
+ 	u32 *in;
+ 	void *tirc;
+ 	int inlen;
+ 	int err;
+ 
+ 	inlen = MLX5_ST_SZ_BYTES(create_tir_in);
+ 	in = mlx5_vzalloc(inlen);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	tirc = MLX5_ADDR_OF(create_tir_in, in, ctx);
+ 	MLX5_SET(tirc, tirc, disp_type, MLX5_TIRC_DISP_TYPE_DIRECT);
+ 	MLX5_SET(tirc, tirc, inline_rqn, rq->base.mqp.qpn);
+ 	MLX5_SET(tirc, tirc, transport_domain, tdn);
+ 
+ 	err = mlx5_core_create_tir(dev->mdev, in, inlen, &rq->tirn);
+ 
+ 	kvfree(in);
+ 
+ 	return err;
+ }
+ 
+ static void destroy_raw_packet_qp_tir(struct mlx5_ib_dev *dev,
+ 				      struct mlx5_ib_rq *rq)
+ {
+ 	mlx5_core_destroy_tir(dev->mdev, rq->tirn);
+ }
+ 
+ static int create_raw_packet_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
+ 				struct mlx5_create_qp_mbox_in *in,
+ 				struct ib_pd *pd)
+ {
+ 	struct mlx5_ib_raw_packet_qp *raw_packet_qp = &qp->raw_packet_qp;
+ 	struct mlx5_ib_sq *sq = &raw_packet_qp->sq;
+ 	struct mlx5_ib_rq *rq = &raw_packet_qp->rq;
+ 	struct ib_uobject *uobj = pd->uobject;
+ 	struct ib_ucontext *ucontext = uobj->context;
+ 	struct mlx5_ib_ucontext *mucontext = to_mucontext(ucontext);
+ 	int err;
+ 	u32 tdn = mucontext->tdn;
+ 
+ 	if (qp->sq.wqe_cnt) {
+ 		err = create_raw_packet_qp_tis(dev, sq, tdn);
+ 		if (err)
+ 			return err;
+ 
+ 		err = create_raw_packet_qp_sq(dev, sq, in, pd);
+ 		if (err)
+ 			goto err_destroy_tis;
+ 
+ 		sq->base.container_mibqp = qp;
+ 	}
+ 
+ 	if (qp->rq.wqe_cnt) {
+ 		rq->base.container_mibqp = qp;
+ 
+ 		err = create_raw_packet_qp_rq(dev, rq, in);
+ 		if (err)
+ 			goto err_destroy_sq;
+ 
+ 
+ 		err = create_raw_packet_qp_tir(dev, rq, tdn);
+ 		if (err)
+ 			goto err_destroy_rq;
+ 	}
+ 
+ 	qp->trans_qp.base.mqp.qpn = qp->sq.wqe_cnt ? sq->base.mqp.qpn :
+ 						     rq->base.mqp.qpn;
+ 
+ 	return 0;
+ 
+ err_destroy_rq:
+ 	destroy_raw_packet_qp_rq(dev, rq);
+ err_destroy_sq:
+ 	if (!qp->sq.wqe_cnt)
+ 		return err;
+ 	destroy_raw_packet_qp_sq(dev, sq);
+ err_destroy_tis:
+ 	destroy_raw_packet_qp_tis(dev, sq);
+ 
+ 	return err;
+ }
+ 
+ static void destroy_raw_packet_qp(struct mlx5_ib_dev *dev,
+ 				  struct mlx5_ib_qp *qp)
+ {
+ 	struct mlx5_ib_raw_packet_qp *raw_packet_qp = &qp->raw_packet_qp;
+ 	struct mlx5_ib_sq *sq = &raw_packet_qp->sq;
+ 	struct mlx5_ib_rq *rq = &raw_packet_qp->rq;
+ 
+ 	if (qp->rq.wqe_cnt) {
+ 		destroy_raw_packet_qp_tir(dev, rq);
+ 		destroy_raw_packet_qp_rq(dev, rq);
+ 	}
+ 
+ 	if (qp->sq.wqe_cnt) {
+ 		destroy_raw_packet_qp_sq(dev, sq);
+ 		destroy_raw_packet_qp_tis(dev, sq);
+ 	}
+ }
+ 
+ static void raw_packet_qp_copy_info(struct mlx5_ib_qp *qp,
+ 				    struct mlx5_ib_raw_packet_qp *raw_packet_qp)
+ {
+ 	struct mlx5_ib_sq *sq = &raw_packet_qp->sq;
+ 	struct mlx5_ib_rq *rq = &raw_packet_qp->rq;
+ 
+ 	sq->sq = &qp->sq;
+ 	rq->rq = &qp->rq;
+ 	sq->doorbell = &qp->db;
+ 	rq->doorbell = &qp->db;
+ }
+ 
++>>>>>>> 358e42ea66e2 (IB/mlx5: Add Scatter FCS support for Raw Packet QP)
  static int create_qp_common(struct mlx5_ib_dev *dev, struct ib_pd *pd,
  			    struct ib_qp_init_attr *init_attr,
  			    struct ib_udata *udata, struct mlx5_ib_qp *qp)
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
