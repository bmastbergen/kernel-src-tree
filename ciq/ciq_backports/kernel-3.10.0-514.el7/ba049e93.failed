kvm: rename pfn_t to kvm_pfn_t

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit ba049e93aef7e8c571567088b1b73f4f5b99272a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ba049e93.failed

To date, we have implemented two I/O usage models for persistent memory,
PMEM (a persistent "ram disk") and DAX (mmap persistent memory into
userspace).  This series adds a third, DAX-GUP, that allows DAX mappings
to be the target of direct-i/o.  It allows userspace to coordinate
DMA/RDMA from/to persistent memory.

The implementation leverages the ZONE_DEVICE mm-zone that went into
4.3-rc1 (also discussed at kernel summit) to flag pages that are owned
and dynamically mapped by a device driver.  The pmem driver, after
mapping a persistent memory range into the system memmap via
devm_memremap_pages(), arranges for DAX to distinguish pfn-only versus
page-backed pmem-pfns via flags in the new pfn_t type.

The DAX code, upon seeing a PFN_DEV+PFN_MAP flagged pfn, flags the
resulting pte(s) inserted into the process page tables with a new
_PAGE_DEVMAP flag.  Later, when get_user_pages() is walking ptes it keys
off _PAGE_DEVMAP to pin the device hosting the page range active.
Finally, get_page() and put_page() are modified to take references
against the device driver established page mapping.

Finally, this need for "struct page" for persistent memory requires
memory capacity to store the memmap array.  Given the memmap array for a
large pool of persistent may exhaust available DRAM introduce a
mechanism to allocate the memmap from persistent memory.  The new
"struct vmem_altmap *" parameter to devm_memremap_pages() enables
arch_add_memory() to use reserved pmem capacity rather than the page
allocator.

This patch (of 18):

The core has developed a need for a "pfn_t" type [1].  Move the existing
pfn_t in KVM to kvm_pfn_t [2].

[1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html
[2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html

	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ba049e93aef7e8c571567088b1b73f4f5b99272a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/include/asm/kvm_mmu.h
#	arch/arm/kvm/mmu.c
#	arch/arm64/include/asm/kvm_mmu.h
#	arch/mips/kvm/kvm_tlb.c
#	arch/powerpc/kvm/e500_mmu_host.c
#	arch/x86/kvm/mmu.c
diff --cc arch/arm/include/asm/kvm_mmu.h
index 472ac7091003,a520b7987a29..000000000000
--- a/arch/arm/include/asm/kvm_mmu.h
+++ b/arch/arm/include/asm/kvm_mmu.h
@@@ -103,9 -110,82 +103,23 @@@ static inline void kvm_set_s2pte_writab
  	pte_val(*pte) |= L_PTE_S2_RDWR;
  }
  
 -static inline void kvm_set_s2pmd_writable(pmd_t *pmd)
 -{
 -	pmd_val(*pmd) |= L_PMD_S2_RDWR;
 -}
 -
 -static inline void kvm_set_s2pte_readonly(pte_t *pte)
 -{
 -	pte_val(*pte) = (pte_val(*pte) & ~L_PTE_S2_RDWR) | L_PTE_S2_RDONLY;
 -}
 -
 -static inline bool kvm_s2pte_readonly(pte_t *pte)
 -{
 -	return (pte_val(*pte) & L_PTE_S2_RDWR) == L_PTE_S2_RDONLY;
 -}
 -
 -static inline void kvm_set_s2pmd_readonly(pmd_t *pmd)
 -{
 -	pmd_val(*pmd) = (pmd_val(*pmd) & ~L_PMD_S2_RDWR) | L_PMD_S2_RDONLY;
 -}
 -
 -static inline bool kvm_s2pmd_readonly(pmd_t *pmd)
 -{
 -	return (pmd_val(*pmd) & L_PMD_S2_RDWR) == L_PMD_S2_RDONLY;
 -}
 -
 -
 -/* Open coded p*d_addr_end that can deal with 64bit addresses */
 -#define kvm_pgd_addr_end(addr, end)					\
 -({	u64 __boundary = ((addr) + PGDIR_SIZE) & PGDIR_MASK;		\
 -	(__boundary - 1 < (end) - 1)? __boundary: (end);		\
 -})
 -
 -#define kvm_pud_addr_end(addr,end)		(end)
 -
 -#define kvm_pmd_addr_end(addr, end)					\
 -({	u64 __boundary = ((addr) + PMD_SIZE) & PMD_MASK;		\
 -	(__boundary - 1 < (end) - 1)? __boundary: (end);		\
 -})
 -
 -#define kvm_pgd_index(addr)			pgd_index(addr)
 -
 -static inline bool kvm_page_empty(void *ptr)
 -{
 -	struct page *ptr_page = virt_to_page(ptr);
 -	return page_count(ptr_page) == 1;
 -}
 -
 -#define kvm_pte_table_empty(kvm, ptep) kvm_page_empty(ptep)
 -#define kvm_pmd_table_empty(kvm, pmdp) kvm_page_empty(pmdp)
 -#define kvm_pud_table_empty(kvm, pudp) (0)
 -
 -#define KVM_PREALLOC_LEVEL	0
 -
 -static inline void *kvm_get_hwpgd(struct kvm *kvm)
 -{
 -	return kvm->arch.pgd;
 -}
 -
 -static inline unsigned int kvm_get_hwpgd_size(void)
 -{
 -	return PTRS_PER_S2_PGD * sizeof(pgd_t);
 -}
 -
  struct kvm;
  
++<<<<<<< HEAD
 +static inline void coherent_icache_guest_page(struct kvm *kvm, gfn_t gfn)
++=======
+ #define kvm_flush_dcache_to_poc(a,l)	__cpuc_flush_dcache_area((a), (l))
+ 
+ static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)
+ {
+ 	return (vcpu->arch.cp15[c1_SCTLR] & 0b101) == 0b101;
+ }
+ 
+ static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu,
+ 					       kvm_pfn_t pfn,
+ 					       unsigned long size,
+ 					       bool ipa_uncached)
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  {
  	/*
  	 * If we are going to insert an instruction page and the icache is
@@@ -128,7 -235,55 +142,59 @@@
  	}
  }
  
++<<<<<<< HEAD
 +#define kvm_flush_dcache_to_poc(a,l)	__cpuc_flush_dcache_area((a), (l))
++=======
+ static inline void __kvm_flush_dcache_pte(pte_t pte)
+ {
+ 	void *va = kmap_atomic(pte_page(pte));
+ 
+ 	kvm_flush_dcache_to_poc(va, PAGE_SIZE);
+ 
+ 	kunmap_atomic(va);
+ }
+ 
+ static inline void __kvm_flush_dcache_pmd(pmd_t pmd)
+ {
+ 	unsigned long size = PMD_SIZE;
+ 	kvm_pfn_t pfn = pmd_pfn(pmd);
+ 
+ 	while (size) {
+ 		void *va = kmap_atomic_pfn(pfn);
+ 
+ 		kvm_flush_dcache_to_poc(va, PAGE_SIZE);
+ 
+ 		pfn++;
+ 		size -= PAGE_SIZE;
+ 
+ 		kunmap_atomic(va);
+ 	}
+ }
+ 
+ static inline void __kvm_flush_dcache_pud(pud_t pud)
+ {
+ }
+ 
+ #define kvm_virt_to_phys(x)		virt_to_idmap((unsigned long)(x))
+ 
+ void kvm_set_way_flush(struct kvm_vcpu *vcpu);
+ void kvm_toggle_cache(struct kvm_vcpu *vcpu, bool was_enabled);
+ 
+ static inline bool __kvm_cpu_uses_extended_idmap(void)
+ {
+ 	return false;
+ }
+ 
+ static inline void __kvm_extend_hypmap(pgd_t *boot_hyp_pgd,
+ 				       pgd_t *hyp_pgd,
+ 				       pgd_t *merged_hyp_pgd,
+ 				       unsigned long hyp_idmap_start) { }
+ 
+ static inline unsigned int kvm_get_vmid_bits(void)
+ {
+ 	return 8;
+ }
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  
  #endif	/* !__ASSEMBLY__ */
  
diff --cc arch/arm/kvm/mmu.c
index 84ba67b982c0,aba61fd3697a..000000000000
--- a/arch/arm/kvm/mmu.c
+++ b/arch/arm/kvm/mmu.c
@@@ -517,18 -992,239 +517,251 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)
+ {
+ 	kvm_pfn_t pfn = *pfnp;
+ 	gfn_t gfn = *ipap >> PAGE_SHIFT;
+ 
+ 	if (PageTransCompound(pfn_to_page(pfn))) {
+ 		unsigned long mask;
+ 		/*
+ 		 * The address we faulted on is backed by a transparent huge
+ 		 * page.  However, because we map the compound huge page and
+ 		 * not the individual tail page, we need to transfer the
+ 		 * refcount to the head page.  We have to be careful that the
+ 		 * THP doesn't start to split while we are adjusting the
+ 		 * refcounts.
+ 		 *
+ 		 * We are sure this doesn't happen, because mmu_notifier_retry
+ 		 * was successful and we are holding the mmu_lock, so if this
+ 		 * THP is trying to split, it will be blocked in the mmu
+ 		 * notifier before touching any of the pages, specifically
+ 		 * before being able to call __split_huge_page_refcount().
+ 		 *
+ 		 * We can therefore safely transfer the refcount from PG_tail
+ 		 * to PG_head and switch the pfn from a tail page to the head
+ 		 * page accordingly.
+ 		 */
+ 		mask = PTRS_PER_PMD - 1;
+ 		VM_BUG_ON((gfn & mask) != (pfn & mask));
+ 		if (pfn & mask) {
+ 			*ipap &= PMD_MASK;
+ 			kvm_release_pfn_clean(pfn);
+ 			pfn &= ~mask;
+ 			kvm_get_pfn(pfn);
+ 			*pfnp = pfn;
+ 		}
+ 
+ 		return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static bool kvm_is_write_fault(struct kvm_vcpu *vcpu)
+ {
+ 	if (kvm_vcpu_trap_is_iabt(vcpu))
+ 		return false;
+ 
+ 	return kvm_vcpu_dabt_iswrite(vcpu);
+ }
+ 
+ /**
+  * stage2_wp_ptes - write protect PMD range
+  * @pmd:	pointer to pmd entry
+  * @addr:	range start address
+  * @end:	range end address
+  */
+ static void stage2_wp_ptes(pmd_t *pmd, phys_addr_t addr, phys_addr_t end)
+ {
+ 	pte_t *pte;
+ 
+ 	pte = pte_offset_kernel(pmd, addr);
+ 	do {
+ 		if (!pte_none(*pte)) {
+ 			if (!kvm_s2pte_readonly(pte))
+ 				kvm_set_s2pte_readonly(pte);
+ 		}
+ 	} while (pte++, addr += PAGE_SIZE, addr != end);
+ }
+ 
+ /**
+  * stage2_wp_pmds - write protect PUD range
+  * @pud:	pointer to pud entry
+  * @addr:	range start address
+  * @end:	range end address
+  */
+ static void stage2_wp_pmds(pud_t *pud, phys_addr_t addr, phys_addr_t end)
+ {
+ 	pmd_t *pmd;
+ 	phys_addr_t next;
+ 
+ 	pmd = pmd_offset(pud, addr);
+ 
+ 	do {
+ 		next = kvm_pmd_addr_end(addr, end);
+ 		if (!pmd_none(*pmd)) {
+ 			if (kvm_pmd_huge(*pmd)) {
+ 				if (!kvm_s2pmd_readonly(pmd))
+ 					kvm_set_s2pmd_readonly(pmd);
+ 			} else {
+ 				stage2_wp_ptes(pmd, addr, next);
+ 			}
+ 		}
+ 	} while (pmd++, addr = next, addr != end);
+ }
+ 
+ /**
+   * stage2_wp_puds - write protect PGD range
+   * @pgd:	pointer to pgd entry
+   * @addr:	range start address
+   * @end:	range end address
+   *
+   * Process PUD entries, for a huge PUD we cause a panic.
+   */
+ static void  stage2_wp_puds(pgd_t *pgd, phys_addr_t addr, phys_addr_t end)
+ {
+ 	pud_t *pud;
+ 	phys_addr_t next;
+ 
+ 	pud = pud_offset(pgd, addr);
+ 	do {
+ 		next = kvm_pud_addr_end(addr, end);
+ 		if (!pud_none(*pud)) {
+ 			/* TODO:PUD not supported, revisit later if supported */
+ 			BUG_ON(kvm_pud_huge(*pud));
+ 			stage2_wp_pmds(pud, addr, next);
+ 		}
+ 	} while (pud++, addr = next, addr != end);
+ }
+ 
+ /**
+  * stage2_wp_range() - write protect stage2 memory region range
+  * @kvm:	The KVM pointer
+  * @addr:	Start address of range
+  * @end:	End address of range
+  */
+ static void stage2_wp_range(struct kvm *kvm, phys_addr_t addr, phys_addr_t end)
+ {
+ 	pgd_t *pgd;
+ 	phys_addr_t next;
+ 
+ 	pgd = kvm->arch.pgd + kvm_pgd_index(addr);
+ 	do {
+ 		/*
+ 		 * Release kvm_mmu_lock periodically if the memory region is
+ 		 * large. Otherwise, we may see kernel panics with
+ 		 * CONFIG_DETECT_HUNG_TASK, CONFIG_LOCKUP_DETECTOR,
+ 		 * CONFIG_LOCKDEP. Additionally, holding the lock too long
+ 		 * will also starve other vCPUs.
+ 		 */
+ 		if (need_resched() || spin_needbreak(&kvm->mmu_lock))
+ 			cond_resched_lock(&kvm->mmu_lock);
+ 
+ 		next = kvm_pgd_addr_end(addr, end);
+ 		if (pgd_present(*pgd))
+ 			stage2_wp_puds(pgd, addr, next);
+ 	} while (pgd++, addr = next, addr != end);
+ }
+ 
+ /**
+  * kvm_mmu_wp_memory_region() - write protect stage 2 entries for memory slot
+  * @kvm:	The KVM pointer
+  * @slot:	The memory slot to write protect
+  *
+  * Called to start logging dirty pages after memory region
+  * KVM_MEM_LOG_DIRTY_PAGES operation is called. After this function returns
+  * all present PMD and PTEs are write protected in the memory region.
+  * Afterwards read of dirty page log can be called.
+  *
+  * Acquires kvm_mmu_lock. Called with kvm->slots_lock mutex acquired,
+  * serializing operations for VM memory regions.
+  */
+ void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot)
+ {
+ 	struct kvm_memslots *slots = kvm_memslots(kvm);
+ 	struct kvm_memory_slot *memslot = id_to_memslot(slots, slot);
+ 	phys_addr_t start = memslot->base_gfn << PAGE_SHIFT;
+ 	phys_addr_t end = (memslot->base_gfn + memslot->npages) << PAGE_SHIFT;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	stage2_wp_range(kvm, start, end);
+ 	spin_unlock(&kvm->mmu_lock);
+ 	kvm_flush_remote_tlbs(kvm);
+ }
+ 
+ /**
+  * kvm_mmu_write_protect_pt_masked() - write protect dirty pages
+  * @kvm:	The KVM pointer
+  * @slot:	The memory slot associated with mask
+  * @gfn_offset:	The gfn offset in memory slot
+  * @mask:	The mask of dirty pages at offset 'gfn_offset' in this memory
+  *		slot to be write protected
+  *
+  * Walks bits set in mask write protects the associated pte's. Caller must
+  * acquire kvm_mmu_lock.
+  */
+ static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
+ 		struct kvm_memory_slot *slot,
+ 		gfn_t gfn_offset, unsigned long mask)
+ {
+ 	phys_addr_t base_gfn = slot->base_gfn + gfn_offset;
+ 	phys_addr_t start = (base_gfn +  __ffs(mask)) << PAGE_SHIFT;
+ 	phys_addr_t end = (base_gfn + __fls(mask) + 1) << PAGE_SHIFT;
+ 
+ 	stage2_wp_range(kvm, start, end);
+ }
+ 
+ /*
+  * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected
+  * dirty pages.
+  *
+  * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to
+  * enable dirty logging for them.
+  */
+ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
+ 		struct kvm_memory_slot *slot,
+ 		gfn_t gfn_offset, unsigned long mask)
+ {
+ 	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+ }
+ 
+ static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, kvm_pfn_t pfn,
+ 				      unsigned long size, bool uncached)
+ {
+ 	__coherent_cache_guest_page(vcpu, pfn, size, uncached);
+ }
+ 
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 -			  struct kvm_memory_slot *memslot, unsigned long hva,
 +			  gfn_t gfn, struct kvm_memory_slot *memslot,
  			  unsigned long fault_status)
  {
++<<<<<<< HEAD
 +	pte_t new_pte;
 +	pfn_t pfn;
 +	int ret;
 +	bool write_fault, writable;
 +	unsigned long mmu_seq;
 +	struct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;
++=======
+ 	int ret;
+ 	bool write_fault, writable, hugetlb = false, force_pte = false;
+ 	unsigned long mmu_seq;
+ 	gfn_t gfn = fault_ipa >> PAGE_SHIFT;
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;
+ 	struct vm_area_struct *vma;
+ 	kvm_pfn_t pfn;
+ 	pgprot_t mem_type = PAGE_S2;
+ 	bool fault_ipa_uncached;
+ 	bool logging_active = memslot_is_logging(memslot);
+ 	unsigned long flags = 0;
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  
 -	write_fault = kvm_is_write_fault(vcpu);
 +	write_fault = kvm_is_write_fault(kvm_vcpu_get_hsr(vcpu));
  	if (fault_status == FSC_PERM && !write_fault) {
  		kvm_err("Unexpected L2 read permission error\n");
  		return -EFAULT;
@@@ -555,22 -1280,101 +788,95 @@@
  	if (is_error_pfn(pfn))
  		return -EFAULT;
  
++<<<<<<< HEAD
 +	new_pte = pfn_pte(pfn, PAGE_S2);
 +	coherent_icache_guest_page(vcpu->kvm, gfn);
++=======
+ 	if (kvm_is_device_pfn(pfn)) {
+ 		mem_type = PAGE_S2_DEVICE;
+ 		flags |= KVM_S2PTE_FLAG_IS_IOMAP;
+ 	} else if (logging_active) {
+ 		/*
+ 		 * Faults on pages in a memslot with logging enabled
+ 		 * should not be mapped with huge pages (it introduces churn
+ 		 * and performance degradation), so force a pte mapping.
+ 		 */
+ 		force_pte = true;
+ 		flags |= KVM_S2_FLAG_LOGGING_ACTIVE;
+ 
+ 		/*
+ 		 * Only actually map the page as writable if this was a write
+ 		 * fault.
+ 		 */
+ 		if (!write_fault)
+ 			writable = false;
+ 	}
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	if (mmu_notifier_retry(kvm, mmu_seq))
+ 		goto out_unlock;
+ 
+ 	if (!hugetlb && !force_pte)
+ 		hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);
+ 
+ 	fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT;
+ 
+ 	if (hugetlb) {
+ 		pmd_t new_pmd = pfn_pmd(pfn, mem_type);
+ 		new_pmd = pmd_mkhuge(new_pmd);
+ 		if (writable) {
+ 			kvm_set_s2pmd_writable(&new_pmd);
+ 			kvm_set_pfn_dirty(pfn);
+ 		}
+ 		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);
+ 		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);
+ 	} else {
+ 		pte_t new_pte = pfn_pte(pfn, mem_type);
+ 
+ 		if (writable) {
+ 			kvm_set_s2pte_writable(&new_pte);
+ 			kvm_set_pfn_dirty(pfn);
+ 			mark_page_dirty(kvm, gfn);
+ 		}
+ 		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);
+ 		ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);
+ 	}
+ 
+ out_unlock:
+ 	spin_unlock(&kvm->mmu_lock);
+ 	kvm_set_pfn_accessed(pfn);
+ 	kvm_release_pfn_clean(pfn);
+ 	return ret;
+ }
+ 
+ /*
+  * Resolve the access fault by making the page young again.
+  * Note that because the faulting entry is guaranteed not to be
+  * cached in the TLB, we don't need to invalidate anything.
+  */
+ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
+ {
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 	kvm_pfn_t pfn;
+ 	bool pfn_valid = false;
+ 
+ 	trace_kvm_access_fault(fault_ipa);
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  
  	spin_lock(&vcpu->kvm->mmu_lock);
 -
 -	pmd = stage2_get_pmd(vcpu->kvm, NULL, fault_ipa);
 -	if (!pmd || pmd_none(*pmd))	/* Nothing there */
 -		goto out;
 -
 -	if (kvm_pmd_huge(*pmd)) {	/* THP, HugeTLB */
 -		*pmd = pmd_mkyoung(*pmd);
 -		pfn = pmd_pfn(*pmd);
 -		pfn_valid = true;
 -		goto out;
 +	if (mmu_notifier_retry(vcpu->kvm, mmu_seq))
 +		goto out_unlock;
 +	if (writable) {
 +		kvm_set_s2pte_writable(&new_pte);
 +		kvm_set_pfn_dirty(pfn);
  	}
 +	stage2_set_pte(vcpu->kvm, memcache, fault_ipa, &new_pte, false);
  
 -	pte = pte_offset_kernel(pmd, fault_ipa);
 -	if (pte_none(*pte))		/* Nothing there either */
 -		goto out;
 -
 -	*pte = pte_mkyoung(*pte);	/* Just a page... */
 -	pfn = pte_pfn(*pte);
 -	pfn_valid = true;
 -out:
 +out_unlock:
  	spin_unlock(&vcpu->kvm->mmu_lock);
 -	if (pfn_valid)
 -		kvm_set_pfn_accessed(pfn);
 +	kvm_release_pfn_clean(pfn);
 +	return 0;
  }
  
  /**
diff --cc arch/mips/kvm/kvm_tlb.c
index c777dd36d4a8,570479c03bdc..000000000000
--- a/arch/mips/kvm/kvm_tlb.c
+++ b/arch/mips/kvm/kvm_tlb.c
@@@ -42,13 -38,13 +42,23 @@@ atomic_t kvm_mips_instance
  EXPORT_SYMBOL(kvm_mips_instance);
  
  /* These function pointers are initialized once the KVM module is loaded */
++<<<<<<< HEAD:arch/mips/kvm/kvm_tlb.c
 +pfn_t(*kvm_mips_gfn_to_pfn) (struct kvm *kvm, gfn_t gfn);
 +EXPORT_SYMBOL(kvm_mips_gfn_to_pfn);
 +
 +void (*kvm_mips_release_pfn_clean) (pfn_t pfn);
 +EXPORT_SYMBOL(kvm_mips_release_pfn_clean);
 +
 +bool(*kvm_mips_is_error_pfn) (pfn_t pfn);
++=======
+ kvm_pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);
+ EXPORT_SYMBOL(kvm_mips_gfn_to_pfn);
+ 
+ void (*kvm_mips_release_pfn_clean)(kvm_pfn_t pfn);
+ EXPORT_SYMBOL(kvm_mips_release_pfn_clean);
+ 
+ bool (*kvm_mips_is_error_pfn)(kvm_pfn_t pfn);
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t):arch/mips/kvm/tlb.c
  EXPORT_SYMBOL(kvm_mips_is_error_pfn);
  
  uint32_t kvm_mips_get_kernel_asid(struct kvm_vcpu *vcpu)
@@@ -291,13 -257,12 +301,13 @@@ kvm_mips_host_tlb_write(struct kvm_vcp
  	return 0;
  }
  
 +
  /* XXXKYMA: Must be called with interrupts disabled */
  int kvm_mips_handle_kseg0_tlb_fault(unsigned long badvaddr,
 -				    struct kvm_vcpu *vcpu)
 +	struct kvm_vcpu *vcpu)
  {
  	gfn_t gfn;
- 	pfn_t pfn0, pfn1;
+ 	kvm_pfn_t pfn0, pfn1;
  	unsigned long vaddr = 0;
  	unsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;
  	int even;
@@@ -396,9 -360,8 +406,9 @@@ kvm_mips_handle_mapped_seg_tlb_fault(st
  {
  	unsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;
  	struct kvm *kvm = vcpu->kvm;
- 	pfn_t pfn0, pfn1;
+ 	kvm_pfn_t pfn0, pfn1;
  
 +
  	if ((tlb->tlb_hi & VPN2_MASK) == 0) {
  		pfn0 = 0;
  		pfn1 = 0;
diff --cc arch/powerpc/kvm/e500_mmu_host.c
index 82aafd61162a,b0333cc737dd..000000000000
--- a/arch/powerpc/kvm/e500_mmu_host.c
+++ b/arch/powerpc/kvm/e500_mmu_host.c
@@@ -248,10 -246,16 +248,14 @@@ static inline int tlbe_is_writable(stru
  
  static inline void kvmppc_e500_ref_setup(struct tlbe_ref *ref,
  					 struct kvm_book3e_206_tlb_entry *gtlbe,
++<<<<<<< HEAD
 +					 pfn_t pfn)
++=======
+ 					 kvm_pfn_t pfn, unsigned int wimg)
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  {
  	ref->pfn = pfn;
 -	ref->flags = E500_TLB_VALID;
 -
 -	/* Use guest supplied MAS2_G and MAS2_E */
 -	ref->flags |= (gtlbe->mas2 & MAS2_ATTRIB_MASK) | wimg;
 -
 -	/* Mark the page accessed */
 -	kvm_set_pfn_accessed(pfn);
 +	ref->flags |= E500_TLB_VALID;
  
  	if (tlbe_is_writable(gtlbe))
  		kvm_set_pfn_dirty(pfn);
diff --cc arch/x86/kvm/mmu.c
index 85dd7fadd16a,95a955de5964..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -2635,10 -2538,9 +2635,16 @@@ done
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 +			 unsigned pte_access, int write_fault, int *emulate,
 +			 int level, gfn_t gfn, pfn_t pfn, bool speculative,
 +			 bool host_writable)
++=======
+ static bool mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep, unsigned pte_access,
+ 			 int write_fault, int level, gfn_t gfn, kvm_pfn_t pfn,
+ 			 bool speculative, bool host_writable)
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  {
  	int was_rmapped = 0;
  	int rmap_count;
@@@ -2695,9 -2598,11 +2701,9 @@@
  	}
  
  	kvm_release_pfn_clean(pfn);
 -
 -	return emulate;
  }
  
- static pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,
+ static kvm_pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,
  				     bool no_dirty_log)
  {
  	struct kvm_memory_slot *slot;
@@@ -2779,9 -2683,8 +2785,14 @@@ static void direct_pte_prefetch(struct 
  	__direct_pte_prefetch(vcpu, sp, sptep);
  }
  
++<<<<<<< HEAD
 +static int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write,
 +			int map_writable, int level, gfn_t gfn, pfn_t pfn,
 +			bool prefault)
++=======
+ static int __direct_map(struct kvm_vcpu *vcpu, int write, int map_writable,
+ 			int level, gfn_t gfn, kvm_pfn_t pfn, bool prefault)
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  {
  	struct kvm_shadow_walk_iterator iterator;
  	struct kvm_mmu_page *sp;
@@@ -3053,8 -2956,8 +3065,13 @@@ static int nonpaging_map(struct kvm_vcp
  {
  	int r;
  	int level;
++<<<<<<< HEAD
 +	int force_pt_level;
 +	pfn_t pfn;
++=======
+ 	bool force_pt_level = false;
+ 	kvm_pfn_t pfn;
++>>>>>>> ba049e93aef7 (kvm: rename pfn_t to kvm_pfn_t)
  	unsigned long mmu_seq;
  	bool map_writable, write = error_code & PFERR_WRITE_MASK;
  
@@@ -3544,10 -3439,20 +3561,10 @@@ static bool try_async_pf(struct kvm_vcp
  static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
  			  bool prefault)
  {
- 	pfn_t pfn;
+ 	kvm_pfn_t pfn;
  	int r;
  	int level;
 -	bool force_pt_level;
 +	int force_pt_level;
  	gfn_t gfn = gpa >> PAGE_SHIFT;
  	unsigned long mmu_seq;
  	int write = error_code & PFERR_WRITE_MASK;
@@@ -4696,12 -4602,11 +4713,12 @@@ static bool kvm_mmu_zap_collapsible_spt
  	u64 *sptep;
  	struct rmap_iterator iter;
  	int need_tlb_flush = 0;
- 	pfn_t pfn;
+ 	kvm_pfn_t pfn;
  	struct kvm_mmu_page *sp;
  
 -restart:
 -	for_each_rmap_spte(rmap_head, &iter, sptep) {
 +	for (sptep = rmap_get_first(*rmapp, &iter); sptep;) {
 +		BUG_ON(!(*sptep & PT_PRESENT_MASK));
 +
  		sp = page_header(__pa(sptep));
  		pfn = spte_to_pfn(*sptep);
  
* Unmerged path arch/arm64/include/asm/kvm_mmu.h
* Unmerged path arch/arm/include/asm/kvm_mmu.h
* Unmerged path arch/arm/kvm/mmu.c
* Unmerged path arch/arm64/include/asm/kvm_mmu.h
diff --git a/arch/mips/include/asm/kvm_host.h b/arch/mips/include/asm/kvm_host.h
index 0767ce462fe8..9adbeadc86e7 100644
--- a/arch/mips/include/asm/kvm_host.h
+++ b/arch/mips/include/asm/kvm_host.h
@@ -72,9 +72,9 @@
 #define CAUSEF_DC       (_ULCAST_(1)   << 27)
 
 extern atomic_t kvm_mips_instance;
-extern pfn_t(*kvm_mips_gfn_to_pfn) (struct kvm *kvm, gfn_t gfn);
-extern void (*kvm_mips_release_pfn_clean) (pfn_t pfn);
-extern bool(*kvm_mips_is_error_pfn) (pfn_t pfn);
+extern kvm_pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);
+extern void (*kvm_mips_release_pfn_clean)(kvm_pfn_t pfn);
+extern bool (*kvm_mips_is_error_pfn)(kvm_pfn_t pfn);
 
 struct kvm_vm_stat {
 	u32 remote_tlb_flush;
diff --git a/arch/mips/kvm/kvm_mips_emul.c b/arch/mips/kvm/kvm_mips_emul.c
index 4b6274b47f33..8915458630cb 100644
--- a/arch/mips/kvm/kvm_mips_emul.c
+++ b/arch/mips/kvm/kvm_mips_emul.c
@@ -879,7 +879,7 @@ int kvm_mips_sync_icache(unsigned long va, struct kvm_vcpu *vcpu)
 	struct kvm *kvm = vcpu->kvm;
 	unsigned long pa;
 	gfn_t gfn;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 
 	gfn = va >> PAGE_SHIFT;
 
* Unmerged path arch/mips/kvm/kvm_tlb.c
diff --git a/arch/powerpc/include/asm/kvm_book3s.h b/arch/powerpc/include/asm/kvm_book3s.h
index ced12777cfc0..007f960ad14f 100644
--- a/arch/powerpc/include/asm/kvm_book3s.h
+++ b/arch/powerpc/include/asm/kvm_book3s.h
@@ -154,8 +154,8 @@ extern void kvmppc_set_bat(struct kvm_vcpu *vcpu, struct kvmppc_bat *bat,
 			   bool upper, u32 val);
 extern void kvmppc_giveup_ext(struct kvm_vcpu *vcpu, ulong msr);
 extern int kvmppc_emulate_paired_single(struct kvm_run *run, struct kvm_vcpu *vcpu);
-extern pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,
-			bool *writable);
+extern kvm_pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa,
+			bool writing, bool *writable);
 extern void kvmppc_add_revmap_chain(struct kvm *kvm, struct revmap_entry *rev,
 			unsigned long *rmap, long pte_index, int realmode);
 extern void kvmppc_update_rmap_change(unsigned long *rmap, unsigned long psize);
diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h
index f646fbd41a1b..61b201ec3ce6 100644
--- a/arch/powerpc/include/asm/kvm_ppc.h
+++ b/arch/powerpc/include/asm/kvm_ppc.h
@@ -502,7 +502,7 @@ void kvmppc_claim_lpid(long lpid);
 void kvmppc_free_lpid(long lpid);
 void kvmppc_init_lpid(unsigned long nr_lpids);
 
-static inline void kvmppc_mmu_flush_icache(pfn_t pfn)
+static inline void kvmppc_mmu_flush_icache(kvm_pfn_t pfn)
 {
 	struct page *page;
 	/*
diff --git a/arch/powerpc/kvm/book3s.c b/arch/powerpc/kvm/book3s.c
index 0ced74887539..f117bcfb9b53 100644
--- a/arch/powerpc/kvm/book3s.c
+++ b/arch/powerpc/kvm/book3s.c
@@ -364,7 +364,7 @@ int kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvmppc_core_prepare_to_enter);
 
-pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,
+kvm_pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,
 			bool *writable)
 {
 	ulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM;
@@ -377,9 +377,9 @@ pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,
 	gpa &= ~0xFFFULL;
 	if (unlikely(mp_pa) && unlikely((gpa & KVM_PAM) == mp_pa)) {
 		ulong shared_page = ((ulong)vcpu->arch.shared) & PAGE_MASK;
-		pfn_t pfn;
+		kvm_pfn_t pfn;
 
-		pfn = (pfn_t)virt_to_phys((void*)shared_page) >> PAGE_SHIFT;
+		pfn = (kvm_pfn_t)virt_to_phys((void*)shared_page) >> PAGE_SHIFT;
 		get_page(pfn_to_page(pfn));
 		if (writable)
 			*writable = true;
diff --git a/arch/powerpc/kvm/book3s_32_mmu_host.c b/arch/powerpc/kvm/book3s_32_mmu_host.c
index 2035d16a9262..21292c4f7bd9 100644
--- a/arch/powerpc/kvm/book3s_32_mmu_host.c
+++ b/arch/powerpc/kvm/book3s_32_mmu_host.c
@@ -141,7 +141,7 @@ extern char etext[];
 int kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte,
 			bool iswrite)
 {
-	pfn_t hpaddr;
+	kvm_pfn_t hpaddr;
 	u64 vpn;
 	u64 vsid;
 	struct kvmppc_sid_map *map;
diff --git a/arch/powerpc/kvm/book3s_64_mmu_host.c b/arch/powerpc/kvm/book3s_64_mmu_host.c
index b982d925c710..310f46e7330e 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_host.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_host.c
@@ -82,7 +82,7 @@ int kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte,
 			bool iswrite)
 {
 	unsigned long vpn;
-	pfn_t hpaddr;
+	kvm_pfn_t hpaddr;
 	ulong hash, hpteg;
 	u64 vsid;
 	int ret;
diff --git a/arch/powerpc/kvm/e500.h b/arch/powerpc/kvm/e500.h
index 277cb1851afc..112cfa69446c 100644
--- a/arch/powerpc/kvm/e500.h
+++ b/arch/powerpc/kvm/e500.h
@@ -38,7 +38,7 @@ enum vcpu_ftr {
 #define E500_TLB_TLB0		(1 << 2)
 
 struct tlbe_ref {
-	pfn_t pfn;		/* valid only for TLB0, except briefly */
+	kvm_pfn_t pfn;		/* valid only for TLB0, except briefly */
 	unsigned int flags;	/* E500_TLB_* */
 };
 
* Unmerged path arch/powerpc/kvm/e500_mmu_host.c
diff --git a/arch/powerpc/kvm/trace_pr.h b/arch/powerpc/kvm/trace_pr.h
index 810507cb688a..d44f324184fb 100644
--- a/arch/powerpc/kvm/trace_pr.h
+++ b/arch/powerpc/kvm/trace_pr.h
@@ -30,7 +30,7 @@ TRACE_EVENT(kvm_book3s_reenter,
 #ifdef CONFIG_PPC_BOOK3S_64
 
 TRACE_EVENT(kvm_book3s_64_mmu_map,
-	TP_PROTO(int rflags, ulong hpteg, ulong va, pfn_t hpaddr,
+	TP_PROTO(int rflags, ulong hpteg, ulong va, kvm_pfn_t hpaddr,
 		 struct kvmppc_pte *orig_pte),
 	TP_ARGS(rflags, hpteg, va, hpaddr, orig_pte),
 
diff --git a/arch/x86/kvm/iommu.c b/arch/x86/kvm/iommu.c
index e54e0d10dfd3..ea6da7933462 100644
--- a/arch/x86/kvm/iommu.c
+++ b/arch/x86/kvm/iommu.c
@@ -43,11 +43,11 @@ static int kvm_iommu_unmap_memslots(struct kvm *kvm);
 static void kvm_iommu_put_pages(struct kvm *kvm,
 				gfn_t base_gfn, unsigned long npages);
 
-static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,
+static kvm_pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,
 			   unsigned long npages)
 {
 	gfn_t end_gfn;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 
 	pfn     = gfn_to_pfn_memslot(slot, gfn);
 	end_gfn = gfn + npages;
@@ -62,7 +62,8 @@ static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,
 	return pfn;
 }
 
-static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)
+static void kvm_unpin_pages(struct kvm *kvm, kvm_pfn_t pfn,
+		unsigned long npages)
 {
 	unsigned long i;
 
@@ -73,7 +74,7 @@ static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)
 int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
 {
 	gfn_t gfn, end_gfn;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 	int r = 0;
 	struct iommu_domain *domain = kvm->arch.iommu_domain;
 	int flags;
@@ -275,7 +276,7 @@ static void kvm_iommu_put_pages(struct kvm *kvm,
 {
 	struct iommu_domain *domain;
 	gfn_t end_gfn, gfn;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 	u64 phys;
 
 	domain  = kvm->arch.iommu_domain;
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu_audit.c b/arch/x86/kvm/mmu_audit.c
index a4f62e6f2db2..fa2f2982157c 100644
--- a/arch/x86/kvm/mmu_audit.c
+++ b/arch/x86/kvm/mmu_audit.c
@@ -97,7 +97,7 @@ static void audit_mappings(struct kvm_vcpu *vcpu, u64 *sptep, int level)
 {
 	struct kvm_mmu_page *sp;
 	gfn_t gfn;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 	hpa_t hpa;
 
 	sp = page_header(__pa(sptep));
diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h
index 99c91d11903d..18c2dec509fd 100644
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@ -459,7 +459,7 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 {
 	unsigned pte_access;
 	gfn_t gfn;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 
 	if (FNAME(prefetch_invalid_gpte)(vcpu, sp, spte, gpte))
 		return false;
@@ -554,7 +554,7 @@ static void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,
 static int FNAME(fetch)(struct kvm_vcpu *vcpu, gva_t addr,
 			 struct guest_walker *gw,
 			 int write_fault, int hlevel,
-			 pfn_t pfn, bool map_writable, bool prefault)
+			 kvm_pfn_t pfn, bool map_writable, bool prefault)
 {
 	struct kvm_mmu_page *sp = NULL;
 	struct kvm_shadow_walk_iterator it;
@@ -699,7 +699,7 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,
 	int user_fault = error_code & PFERR_USER_MASK;
 	struct guest_walker walker;
 	int r;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 	int level = PT_PAGE_TABLE_LEVEL;
 	int force_pt_level;
 	unsigned long mmu_seq;
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 2a45db3ac9a4..5c56cf3a4d43 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -4001,7 +4001,7 @@ out:
 static int init_rmode_identity_map(struct kvm *kvm)
 {
 	int i, idx, r = 0;
-	pfn_t identity_map_pfn;
+	kvm_pfn_t identity_map_pfn;
 	u32 tmp;
 
 	if (!enable_ept)
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 733894b44f8d..b99e309979b8 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5357,7 +5357,7 @@ static bool reexecute_instruction(struct kvm_vcpu *vcpu, gva_t cr2,
 				  int emulation_type)
 {
 	gpa_t gpa = cr2;
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 
 	if (emulation_type & EMULTYPE_NO_REEXECUTE)
 		return false;
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index cf1b632f8991..c63ad5345d4e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -69,7 +69,7 @@
  * error pfns indicate that the gfn is in slot but faild to
  * translate it to pfn on host.
  */
-static inline bool is_error_pfn(pfn_t pfn)
+static inline bool is_error_pfn(kvm_pfn_t pfn)
 {
 	return !!(pfn & KVM_PFN_ERR_MASK);
 }
@@ -79,13 +79,13 @@ static inline bool is_error_pfn(pfn_t pfn)
  * translated to pfn - it is not in slot or failed to
  * translate it to pfn.
  */
-static inline bool is_error_noslot_pfn(pfn_t pfn)
+static inline bool is_error_noslot_pfn(kvm_pfn_t pfn)
 {
 	return !!(pfn & KVM_PFN_ERR_NOSLOT_MASK);
 }
 
 /* noslot pfn indicates that the gfn is not in slot. */
-static inline bool is_noslot_pfn(pfn_t pfn)
+static inline bool is_noslot_pfn(kvm_pfn_t pfn)
 {
 	return pfn == KVM_PFN_NOSLOT;
 }
@@ -561,19 +561,20 @@ void kvm_release_page_clean(struct page *page);
 void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
-pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
-pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
-pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
+kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
-pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
-pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
-pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,
-			   bool *async, bool write_fault, bool *writable);
+kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+			       bool atomic, bool *async, bool write_fault,
+			       bool *writable);
 
-void kvm_release_pfn_clean(pfn_t pfn);
-void kvm_set_pfn_dirty(pfn_t pfn);
-void kvm_set_pfn_accessed(pfn_t pfn);
-void kvm_get_pfn(pfn_t pfn);
+void kvm_release_pfn_clean(kvm_pfn_t pfn);
+void kvm_set_pfn_dirty(kvm_pfn_t pfn);
+void kvm_set_pfn_accessed(kvm_pfn_t pfn);
+void kvm_get_pfn(kvm_pfn_t pfn);
 
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
@@ -599,8 +600,8 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
 struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
-pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
-pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
+kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
+kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
@@ -767,7 +768,7 @@ void kvm_arch_sync_events(struct kvm *kvm);
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
-bool kvm_is_reserved_pfn(pfn_t pfn);
+bool kvm_is_reserved_pfn(kvm_pfn_t pfn);
 
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;
@@ -912,7 +913,7 @@ static inline gfn_t gpa_to_gfn(gpa_t gpa)
 	return (gfn_t)(gpa >> PAGE_SHIFT);
 }
 
-static inline hpa_t pfn_to_hpa(pfn_t pfn)
+static inline hpa_t pfn_to_hpa(kvm_pfn_t pfn)
 {
 	return (hpa_t)pfn << PAGE_SHIFT;
 }
diff --git a/include/linux/kvm_types.h b/include/linux/kvm_types.h
index 1b47a185c2f0..8bf259dae9f6 100644
--- a/include/linux/kvm_types.h
+++ b/include/linux/kvm_types.h
@@ -53,7 +53,7 @@ typedef unsigned long  hva_t;
 typedef u64            hpa_t;
 typedef u64            hfn_t;
 
-typedef hfn_t pfn_t;
+typedef hfn_t kvm_pfn_t;
 
 struct gfn_to_hva_cache {
 	u64 generation;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 60d061de3cb9..0b2768b69a3c 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -110,7 +110,7 @@ static void hardware_disable_all(void);
 
 static void kvm_io_bus_destroy(struct kvm_io_bus *bus);
 
-static void kvm_release_pfn_dirty(pfn_t pfn);
+static void kvm_release_pfn_dirty(kvm_pfn_t pfn);
 static void mark_page_dirty_in_slot(struct kvm_memory_slot *memslot, gfn_t gfn);
 
 __visible bool kvm_rebooting;
@@ -118,7 +118,7 @@ EXPORT_SYMBOL_GPL(kvm_rebooting);
 
 static bool largepages_enabled = true;
 
-bool kvm_is_reserved_pfn(pfn_t pfn)
+bool kvm_is_reserved_pfn(kvm_pfn_t pfn)
 {
 	if (pfn_valid(pfn))
 		return PageReserved(pfn_to_page(pfn));
@@ -1262,7 +1262,7 @@ static inline int check_user_page_hwpoison(unsigned long addr)
  * true indicates success, otherwise false is returned.
  */
 static bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,
-			    bool write_fault, bool *writable, pfn_t *pfn)
+			    bool write_fault, bool *writable, kvm_pfn_t *pfn)
 {
 	struct page *page[1];
 	int npages;
@@ -1295,7 +1295,7 @@ static bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,
  * 1 indicates success, -errno is returned if error is detected.
  */
 static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
-			   bool *writable, pfn_t *pfn)
+			   bool *writable, kvm_pfn_t *pfn)
 {
 	struct page *page[1];
 	int npages = 0;
@@ -1359,11 +1359,11 @@ static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)
  * 2): @write_fault = false && @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
-static pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
+static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
 			bool write_fault, bool *writable)
 {
 	struct vm_area_struct *vma;
-	pfn_t pfn = 0;
+	kvm_pfn_t pfn = 0;
 	int npages;
 
 	/* we can do it either atomically or asynchronously, not both */
@@ -1404,8 +1404,9 @@ exit:
 	return pfn;
 }
 
-pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,
-			   bool *async, bool write_fault, bool *writable)
+kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+			       bool atomic, bool *async, bool write_fault,
+			       bool *writable)
 {
 	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
 
@@ -1426,7 +1427,7 @@ pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,
 }
 EXPORT_SYMBOL_GPL(__gfn_to_pfn_memslot);
 
-pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
+kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable)
 {
 	return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, NULL,
@@ -1434,37 +1435,37 @@ pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_prot);
 
-pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
+kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	return __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot);
 
-pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)
+kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	return __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);
 
-pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)
+kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)
 {
 	return gfn_to_pfn_memslot_atomic(gfn_to_memslot(kvm, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_atomic);
 
-pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)
+kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	return gfn_to_pfn_memslot_atomic(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_atomic);
 
-pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)
+kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)
 {
 	return gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn);
 
-pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)
+kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	return gfn_to_pfn_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
 }
@@ -1487,7 +1488,7 @@ int gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,
 }
 EXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);
 
-static struct page *kvm_pfn_to_page(pfn_t pfn)
+static struct page *kvm_pfn_to_page(kvm_pfn_t pfn)
 {
 	if (is_error_noslot_pfn(pfn))
 		return KVM_ERR_PTR_BAD_PAGE;
@@ -1502,7 +1503,7 @@ static struct page *kvm_pfn_to_page(pfn_t pfn)
 
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)
 {
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 
 	pfn = gfn_to_pfn(kvm, gfn);
 
@@ -1512,7 +1513,7 @@ EXPORT_SYMBOL_GPL(gfn_to_page);
 
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
-	pfn_t pfn;
+	kvm_pfn_t pfn;
 
 	pfn = kvm_vcpu_gfn_to_pfn(vcpu, gfn);
 
@@ -1528,7 +1529,7 @@ void kvm_release_page_clean(struct page *page)
 }
 EXPORT_SYMBOL_GPL(kvm_release_page_clean);
 
-void kvm_release_pfn_clean(pfn_t pfn)
+void kvm_release_pfn_clean(kvm_pfn_t pfn)
 {
 	if (!is_error_noslot_pfn(pfn) && !kvm_is_reserved_pfn(pfn))
 		put_page(pfn_to_page(pfn));
@@ -1543,13 +1544,13 @@ void kvm_release_page_dirty(struct page *page)
 }
 EXPORT_SYMBOL_GPL(kvm_release_page_dirty);
 
-static void kvm_release_pfn_dirty(pfn_t pfn)
+static void kvm_release_pfn_dirty(kvm_pfn_t pfn)
 {
 	kvm_set_pfn_dirty(pfn);
 	kvm_release_pfn_clean(pfn);
 }
 
-void kvm_set_pfn_dirty(pfn_t pfn)
+void kvm_set_pfn_dirty(kvm_pfn_t pfn)
 {
 	if (!kvm_is_reserved_pfn(pfn)) {
 		struct page *page = pfn_to_page(pfn);
@@ -1560,14 +1561,14 @@ void kvm_set_pfn_dirty(pfn_t pfn)
 }
 EXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);
 
-void kvm_set_pfn_accessed(pfn_t pfn)
+void kvm_set_pfn_accessed(kvm_pfn_t pfn)
 {
 	if (!kvm_is_reserved_pfn(pfn))
 		mark_page_accessed(pfn_to_page(pfn));
 }
 EXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);
 
-void kvm_get_pfn(pfn_t pfn)
+void kvm_get_pfn(kvm_pfn_t pfn)
 {
 	if (!kvm_is_reserved_pfn(pfn))
 		get_page(pfn_to_page(pfn));
