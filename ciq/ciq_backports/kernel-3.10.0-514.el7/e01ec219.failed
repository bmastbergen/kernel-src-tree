hv_netvsc: Properly size the vrss queues

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author KY Srinivasan <kys@microsoft.com>
commit e01ec2199ef22e2cabd7d6e68a192f3eb728029f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e01ec219.failed

The current algorithm for deciding on the number of VRSS channels is
not optimal since we open up the min of number of CPUs online and the
number of VRSS channels the host is offering. So on a 32 VCPU guest
we could potentially open 32 VRSS subchannels. Experimentation has
shown that it is best to limit the number of VRSS channels to the number
of CPUs within a NUMA node.

Here is the new algorithm for deciding on the number of sub-channels we
would open up:
        1) Pick the minimum of what the host is offering and what the driver
           in the guest is specifying as the default value.
        2) Pick the minimum of (1) and the numbers of CPUs in the NUMA
           node the primary channel is bound to.

	Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e01ec2199ef22e2cabd7d6e68a192f3eb728029f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/hyperv/netvsc_drv.c
diff --cc drivers/net/hyperv/netvsc_drv.c
index f6bc78f79972,358475ed9b59..000000000000
--- a/drivers/net/hyperv/netvsc_drv.c
+++ b/drivers/net/hyperv/netvsc_drv.c
@@@ -53,6 -46,17 +53,20 @@@ static int ring_size = 128
  module_param(ring_size, int, S_IRUGO);
  MODULE_PARM_DESC(ring_size, "Ring buffer size (# of pages)");
  
++<<<<<<< HEAD
++=======
+ static int max_num_vrss_chns = 8;
+ 
+ static const u32 default_msg = NETIF_MSG_DRV | NETIF_MSG_PROBE |
+ 				NETIF_MSG_LINK | NETIF_MSG_IFUP |
+ 				NETIF_MSG_IFDOWN | NETIF_MSG_RX_ERR |
+ 				NETIF_MSG_TX_ERR;
+ 
+ static int debug = -1;
+ module_param(debug, int, S_IRUGO);
+ MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
+ 
++>>>>>>> e01ec2199ef2 (hv_netvsc: Properly size the vrss queues)
  static void do_set_multicast(struct work_struct *w)
  {
  	struct net_device_context *ndevctx =
@@@ -844,11 -966,19 +859,12 @@@ static int netvsc_probe(struct hv_devic
  	net->features = NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_SG | NETIF_F_RXCSUM |
  			NETIF_F_IP_CSUM | NETIF_F_TSO;
  
 -	net->ethtool_ops = &ethtool_ops;
 +	SET_ETHTOOL_OPS(net, &ethtool_ops);
  	SET_NETDEV_DEV(net, &dev->device);
  
 -	/*
 -	 * Request additional head room in the skb.
 -	 * We will use this space to build the rndis
 -	 * heaser and other state we need to maintain.
 -	 */
 -	net->needed_headroom = max_needed_headroom;
 -
  	/* Notify the netvsc driver of the new device */
  	device_info.ring_size = ring_size;
+ 	device_info.max_num_vrss_chns = max_num_vrss_chns;
  	ret = rndis_filter_device_add(dev, &device_info);
  	if (ret != 0) {
  		netdev_err(net, "unable to add netvsc device (ret %d)\n", ret);
diff --git a/drivers/net/hyperv/hyperv_net.h b/drivers/net/hyperv/hyperv_net.h
index 3362765215d6..df85d1268dc1 100644
--- a/drivers/net/hyperv/hyperv_net.h
+++ b/drivers/net/hyperv/hyperv_net.h
@@ -156,6 +156,7 @@ struct netvsc_device_info {
 	unsigned char mac_adr[ETH_ALEN];
 	bool link_state;	/* 0 - link up, 1 - link down */
 	int  ring_size;
+	u32  max_num_vrss_chns;
 };
 
 enum rndis_device_state {
* Unmerged path drivers/net/hyperv/netvsc_drv.c
diff --git a/drivers/net/hyperv/rndis_filter.c b/drivers/net/hyperv/rndis_filter.c
index 5d06e00ce98f..cbd00c0f06e6 100644
--- a/drivers/net/hyperv/rndis_filter.c
+++ b/drivers/net/hyperv/rndis_filter.c
@@ -1020,6 +1020,9 @@ int rndis_filter_device_add(struct hv_device *dev,
 	struct ndis_recv_scale_cap rsscap;
 	u32 rsscap_size = sizeof(struct ndis_recv_scale_cap);
 	u32 mtu, size;
+	u32 num_rss_qs;
+	const struct cpumask *node_cpu_mask;
+	u32 num_possible_rss_qs;
 
 	rndis_device = get_rndis_device();
 	if (!rndis_device)
@@ -1107,9 +1110,18 @@ int rndis_filter_device_add(struct hv_device *dev,
 	if (ret || rsscap.num_recv_que < 2)
 		goto out;
 
+	num_rss_qs = min(device_info->max_num_vrss_chns, rsscap.num_recv_que);
+
 	net_device->max_chn = rsscap.num_recv_que;
-	net_device->num_chn = (num_online_cpus() < rsscap.num_recv_que) ?
-			       num_online_cpus() : rsscap.num_recv_que;
+
+	/*
+	 * We will limit the VRSS channels to the number CPUs in the NUMA node
+	 * the primary channel is currently bound to.
+	 */
+	node_cpu_mask = cpumask_of_node(cpu_to_node(dev->channel->target_cpu));
+	num_possible_rss_qs = cpumask_weight(node_cpu_mask);
+	net_device->num_chn = min(num_possible_rss_qs, num_rss_qs);
+
 	if (net_device->num_chn == 1)
 		goto out;
 
