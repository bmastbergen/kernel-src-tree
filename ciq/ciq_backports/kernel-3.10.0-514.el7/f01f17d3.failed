mm, vmstat: make quiet_vmstat lighter

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] vmstat: make quiet_vmstat lighter (Jerome Marchand) [1294987]
Rebuild_FUZZ: 94.29%
commit-author Michal Hocko <mhocko@suse.com>
commit f01f17d3705bb6081c9e5728078f64067982be36
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f01f17d3.failed

Mike has reported a considerable overhead of refresh_cpu_vm_stats from
the idle entry during pipe test:

    12.89%  [kernel]       [k] refresh_cpu_vm_stats.isra.12
     4.75%  [kernel]       [k] __schedule
     4.70%  [kernel]       [k] mutex_unlock
     3.14%  [kernel]       [k] __switch_to

This is caused by commit 0eb77e988032 ("vmstat: make vmstat_updater
deferrable again and shut down on idle") which has placed quiet_vmstat
into cpu_idle_loop.  The main reason here seems to be that the idle
entry has to get over all zones and perform atomic operations for each
vmstat entry even though there might be no per cpu diffs.  This is a
pointless overhead for _each_ idle entry.

Make sure that quiet_vmstat is as light as possible.

First of all it doesn't make any sense to do any local sync if the
current cpu is already set in oncpu_stat_off because vmstat_update puts
itself there only if there is nothing to do.

Then we can check need_update which should be a cheap way to check for
potential per-cpu diffs and only then do refresh_cpu_vm_stats.

The original patch also did cancel_delayed_work which we are not doing
here.  There are two reasons for that.  Firstly cancel_delayed_work from
idle context will blow up on RT kernels (reported by Mike):

  CPU: 1 PID: 0 Comm: swapper/1 Not tainted 4.5.0-rt3 #7
  Hardware name: MEDION MS-7848/MS-7848, BIOS M7848W08.20C 09/23/2013
  Call Trace:
    dump_stack+0x49/0x67
    ___might_sleep+0xf5/0x180
    rt_spin_lock+0x20/0x50
    try_to_grab_pending+0x69/0x240
    cancel_delayed_work+0x26/0xe0
    quiet_vmstat+0x75/0xa0
    cpu_idle_loop+0x38/0x3e0
    cpu_startup_entry+0x13/0x20
    start_secondary+0x114/0x140

And secondly, even on !RT kernels it might add some non trivial overhead
which is not necessary.  Even if the vmstat worker wakes up and preempts
idle then it will be most likely a single shot noop because the stats
were already synced and so it would end up on the oncpu_stat_off anyway.
We just need to teach both vmstat_shepherd and vmstat_update to stop
scheduling the worker if there is nothing to do.

[mgalbraith@suse.de: cancel pending work of the cpu_stat_off CPU]
	Signed-off-by: Michal Hocko <mhocko@suse.com>
	Reported-by: Mike Galbraith <umgwanakikbuti@gmail.com>
	Acked-by: Christoph Lameter <cl@linux.com>
	Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f01f17d3705bb6081c9e5728078f64067982be36)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmstat.c
diff --cc mm/vmstat.c
index 3201bfa05819,1543f64df3e6..000000000000
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@@ -1236,10 -1396,16 +1236,22 @@@ static void vmstat_update(struct work_s
  		 * Counters were updated so we expect more updates
  		 * to occur in the future. Keep on running the
  		 * update worker thread.
+ 		 * If we were marked on cpu_stat_off clear the flag
+ 		 * so that vmstat_shepherd doesn't schedule us again.
  		 */
++<<<<<<< HEAD
 +		schedule_delayed_work(&__get_cpu_var(vmstat_work),
 +			round_jiffies_relative(sysctl_stat_interval));
 +	else {
++=======
+ 		if (!cpumask_test_and_clear_cpu(smp_processor_id(),
+ 						cpu_stat_off)) {
+ 			queue_delayed_work_on(smp_processor_id(), vmstat_wq,
+ 				this_cpu_ptr(&vmstat_work),
+ 				round_jiffies_relative(sysctl_stat_interval));
+ 		}
+ 	} else {
++>>>>>>> f01f17d3705b (mm, vmstat: make quiet_vmstat lighter)
  		/*
  		 * We did not update any counters so the app may be in
  		 * a mode where it does not cause counter updates.
@@@ -1252,6 -1418,11 +1264,14 @@@
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Switch off vmstat processing and then fold all the remaining differentials
+  * until the diffs stay at zero. The function is used by NOHZ and can only be
+  * invoked when tick processing is not active.
+  */
+ /*
++>>>>>>> f01f17d3705b (mm, vmstat: make quiet_vmstat lighter)
   * Check if the diffs for a certain cpu indicate that
   * an update is needed.
   */
@@@ -1291,13 -1486,21 +1335,30 @@@ static void vmstat_shepherd(struct work
  
  	get_online_cpus();
  	/* Check processors whose vmstat worker threads have been disabled */
++<<<<<<< HEAD
 +	for_each_cpu(cpu, cpu_stat_off)
 +		if (need_update(cpu) &&
 +			cpumask_test_and_clear_cpu(cpu, cpu_stat_off))
 +
 +			schedule_delayed_work_on(cpu,
 +				&per_cpu(vmstat_work, cpu), 0);
- 
++=======
+ 	for_each_cpu(cpu, cpu_stat_off) {
+ 		struct delayed_work *dw = &per_cpu(vmstat_work, cpu);
++>>>>>>> f01f17d3705b (mm, vmstat: make quiet_vmstat lighter)
+ 
+ 		if (need_update(cpu)) {
+ 			if (cpumask_test_and_clear_cpu(cpu, cpu_stat_off))
+ 				queue_delayed_work_on(cpu, vmstat_wq, dw, 0);
+ 		} else {
+ 			/*
+ 			 * Cancel the work if quiet_vmstat has put this
+ 			 * cpu on cpu_stat_off because the work item might
+ 			 * be still scheduled
+ 			 */
+ 			cancel_delayed_work(dw);
+ 		}
+ 	}
  	put_online_cpus();
  
  	schedule_delayed_work(&shepherd,
* Unmerged path mm/vmstat.c
