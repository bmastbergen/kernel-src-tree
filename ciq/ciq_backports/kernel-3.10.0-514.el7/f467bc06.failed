ixgbe: Add support for UDP-encapsulated tx checksum offload

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mark Rustad <mark.d.rustad@intel.com>
commit f467bc06022d4d37de459f9498ff4fbc7e9b0fca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f467bc06.failed

By using GSO for UDP-encapsulated packets, all ixgbe devices can
be directed to generate checksums for the inner headers because
the outer UDP checksum can be zero. So point the machinery at the
inner headers and have the hardware generate the checksum.

	Signed-off-by: Mark Rustad <mark.d.rustad@intel.com>
	Tested-by: Phil Schmitt <phillip.j.schmitt@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit f467bc06022d4d37de459f9498ff4fbc7e9b0fca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index aeecc9891f88,900562e023a7..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -7769,9 -8134,117 +7792,24 @@@ static int ixgbe_ndo_bridge_getlink(str
  		return 0;
  
  	return ndo_dflt_bridge_getlink(skb, pid, seq, dev,
 -				       adapter->bridge_mode, 0, 0, nlflags,
 -				       filter_mask, NULL);
 -}
 -
 -static void *ixgbe_fwd_add(struct net_device *pdev, struct net_device *vdev)
 -{
 -	struct ixgbe_fwd_adapter *fwd_adapter = NULL;
 -	struct ixgbe_adapter *adapter = netdev_priv(pdev);
 -	int used_pools = adapter->num_vfs + adapter->num_rx_pools;
 -	unsigned int limit;
 -	int pool, err;
 -
 -	/* Hardware has a limited number of available pools. Each VF, and the
 -	 * PF require a pool. Check to ensure we don't attempt to use more
 -	 * then the available number of pools.
 -	 */
 -	if (used_pools >= IXGBE_MAX_VF_FUNCTIONS)
 -		return ERR_PTR(-EINVAL);
 -
 -#ifdef CONFIG_RPS
 -	if (vdev->num_rx_queues != vdev->num_tx_queues) {
 -		netdev_info(pdev, "%s: Only supports a single queue count for TX and RX\n",
 -			    vdev->name);
 -		return ERR_PTR(-EINVAL);
 -	}
 -#endif
 -	/* Check for hardware restriction on number of rx/tx queues */
 -	if (vdev->num_tx_queues > IXGBE_MAX_L2A_QUEUES ||
 -	    vdev->num_tx_queues == IXGBE_BAD_L2A_QUEUE) {
 -		netdev_info(pdev,
 -			    "%s: Supports RX/TX Queue counts 1,2, and 4\n",
 -			    pdev->name);
 -		return ERR_PTR(-EINVAL);
 -	}
 -
 -	if (((adapter->flags & IXGBE_FLAG_DCB_ENABLED) &&
 -	      adapter->num_rx_pools > IXGBE_MAX_DCBMACVLANS - 1) ||
 -	    (adapter->num_rx_pools > IXGBE_MAX_MACVLANS))
 -		return ERR_PTR(-EBUSY);
 -
 -	fwd_adapter = kcalloc(1, sizeof(struct ixgbe_fwd_adapter), GFP_KERNEL);
 -	if (!fwd_adapter)
 -		return ERR_PTR(-ENOMEM);
 -
 -	pool = find_first_zero_bit(&adapter->fwd_bitmask, 32);
 -	adapter->num_rx_pools++;
 -	set_bit(pool, &adapter->fwd_bitmask);
 -	limit = find_last_bit(&adapter->fwd_bitmask, 32);
 -
 -	/* Enable VMDq flag so device will be set in VM mode */
 -	adapter->flags |= IXGBE_FLAG_VMDQ_ENABLED | IXGBE_FLAG_SRIOV_ENABLED;
 -	adapter->ring_feature[RING_F_VMDQ].limit = limit + 1;
 -	adapter->ring_feature[RING_F_RSS].limit = vdev->num_tx_queues;
 -
 -	/* Force reinit of ring allocation with VMDQ enabled */
 -	err = ixgbe_setup_tc(pdev, netdev_get_num_tc(pdev));
 -	if (err)
 -		goto fwd_add_err;
 -	fwd_adapter->pool = pool;
 -	fwd_adapter->real_adapter = adapter;
 -	err = ixgbe_fwd_ring_up(vdev, fwd_adapter);
 -	if (err)
 -		goto fwd_add_err;
 -	netif_tx_start_all_queues(vdev);
 -	return fwd_adapter;
 -fwd_add_err:
 -	/* unwind counter and free adapter struct */
 -	netdev_info(pdev,
 -		    "%s: dfwd hardware acceleration failed\n", vdev->name);
 -	clear_bit(pool, &adapter->fwd_bitmask);
 -	adapter->num_rx_pools--;
 -	kfree(fwd_adapter);
 -	return ERR_PTR(err);
 -}
 -
 -static void ixgbe_fwd_del(struct net_device *pdev, void *priv)
 -{
 -	struct ixgbe_fwd_adapter *fwd_adapter = priv;
 -	struct ixgbe_adapter *adapter = fwd_adapter->real_adapter;
 -	unsigned int limit;
 -
 -	clear_bit(fwd_adapter->pool, &adapter->fwd_bitmask);
 -	adapter->num_rx_pools--;
 -
 -	limit = find_last_bit(&adapter->fwd_bitmask, 32);
 -	adapter->ring_feature[RING_F_VMDQ].limit = limit + 1;
 -	ixgbe_fwd_ring_down(fwd_adapter->netdev, fwd_adapter);
 -	ixgbe_setup_tc(pdev, netdev_get_num_tc(pdev));
 -	netdev_dbg(pdev, "pool %i:%i queues %i:%i VSI bitmask %lx\n",
 -		   fwd_adapter->pool, adapter->num_rx_pools,
 -		   fwd_adapter->rx_base_queue,
 -		   fwd_adapter->rx_base_queue + adapter->num_rx_queues_per_pool,
 -		   adapter->fwd_bitmask);
 -	kfree(fwd_adapter);
 +				       adapter->bridge_mode, 0, 0);
  }
  
+ #define IXGBE_MAX_TUNNEL_HDR_LEN 80
+ static netdev_features_t
+ ixgbe_features_check(struct sk_buff *skb, struct net_device *dev,
+ 		     netdev_features_t features)
+ {
+ 	if (!skb->encapsulation)
+ 		return features;
+ 
+ 	if (unlikely(skb_inner_mac_header(skb) - skb_transport_header(skb) >
+ 		     IXGBE_MAX_TUNNEL_HDR_LEN))
+ 		return features & ~NETIF_F_ALL_CSUM;
+ 
+ 	return features;
+ }
+ 
  static const struct net_device_ops ixgbe_netdev_ops = {
  	.ndo_open		= ixgbe_open,
  	.ndo_stop		= ixgbe_close,
@@@ -7817,6 -8288,11 +7855,14 @@@
  	.ndo_fdb_add		= ixgbe_ndo_fdb_add,
  	.ndo_bridge_setlink	= ixgbe_ndo_bridge_setlink,
  	.ndo_bridge_getlink	= ixgbe_ndo_bridge_getlink,
++<<<<<<< HEAD
++=======
+ 	.ndo_dfwd_add_station	= ixgbe_fwd_add,
+ 	.ndo_dfwd_del_station	= ixgbe_fwd_del,
+ 	.ndo_add_vxlan_port	= ixgbe_add_vxlan_port,
+ 	.ndo_del_vxlan_port	= ixgbe_del_vxlan_port,
+ 	.ndo_features_check	= ixgbe_features_check,
++>>>>>>> f467bc06022d (ixgbe: Add support for UDP-encapsulated tx checksum offload)
  };
  
  /**
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
