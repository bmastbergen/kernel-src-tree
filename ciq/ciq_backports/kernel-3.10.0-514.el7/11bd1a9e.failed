ext4: huge page fault support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Matthew Wilcox <willy@linux.intel.com>
commit 11bd1a9ecdd687b8a4b9b360b7e4b74a1a5e2bd5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/11bd1a9e.failed

Use DAX to provide support for huge pages.

	Signed-off-by: Matthew Wilcox <willy@linux.intel.com>
	Cc: Hillf Danton <dhillf@gmail.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Theodore Ts'o <tytso@mit.edu>
	Cc: Jan Kara <jack@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 11bd1a9ecdd687b8a4b9b360b7e4b74a1a5e2bd5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext4/file.c
diff --cc fs/ext4/file.c
index 3034d6b4eaee,953d519e799c..000000000000
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@@ -200,16 -188,75 +200,69 @@@ ext4_file_write(struct kiocb *iocb, con
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_FS_DAX
+ static void ext4_end_io_unwritten(struct buffer_head *bh, int uptodate)
+ {
+ 	struct inode *inode = bh->b_assoc_map->host;
+ 	/* XXX: breaks on 32-bit > 16GB. Is that even supported? */
+ 	loff_t offset = (loff_t)(uintptr_t)bh->b_private << inode->i_blkbits;
+ 	int err;
+ 	if (!uptodate)
+ 		return;
+ 	WARN_ON(!buffer_unwritten(bh));
+ 	err = ext4_convert_unwritten_extents(NULL, inode, offset, bh->b_size);
+ }
+ 
+ static int ext4_dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+ {
+ 	return dax_fault(vma, vmf, ext4_get_block, ext4_end_io_unwritten);
+ 					/* Is this the right get_block? */
+ }
+ 
+ static int ext4_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
+ 						pmd_t *pmd, unsigned int flags)
+ {
+ 	return dax_pmd_fault(vma, addr, pmd, flags, ext4_get_block_write,
+ 				ext4_end_io_unwritten);
+ }
+ 
+ static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
+ {
+ 	return dax_mkwrite(vma, vmf, ext4_get_block, ext4_end_io_unwritten);
+ }
+ 
+ static const struct vm_operations_struct ext4_dax_vm_ops = {
+ 	.fault		= ext4_dax_fault,
+ 	.pmd_fault	= ext4_dax_pmd_fault,
+ 	.page_mkwrite	= ext4_dax_mkwrite,
+ 	.pfn_mkwrite	= dax_pfn_mkwrite,
+ };
+ #else
+ #define ext4_dax_vm_ops	ext4_file_vm_ops
+ #endif
+ 
++>>>>>>> 11bd1a9ecdd6 (ext4: huge page fault support)
  static const struct vm_operations_struct ext4_file_vm_ops = {
  	.fault		= filemap_fault,
 -	.map_pages	= filemap_map_pages,
  	.page_mkwrite   = ext4_page_mkwrite,
 +	.remap_pages	= generic_file_remap_pages,
  };
  
  static int ext4_file_mmap(struct file *file, struct vm_area_struct *vma)
  {
 -	struct inode *inode = file->f_mapping->host;
 -
 -	if (ext4_encrypted_inode(inode)) {
 -		int err = ext4_get_encryption_info(inode);
 -		if (err)
 -			return 0;
 -		if (ext4_encryption_info(inode) == NULL)
 -			return -ENOKEY;
 -	}
  	file_accessed(file);
++<<<<<<< HEAD
 +	vma->vm_ops = &ext4_file_vm_ops;
++=======
+ 	if (IS_DAX(file_inode(file))) {
+ 		vma->vm_ops = &ext4_dax_vm_ops;
+ 		vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
+ 	} else {
+ 		vma->vm_ops = &ext4_file_vm_ops;
+ 	}
++>>>>>>> 11bd1a9ecdd6 (ext4: huge page fault support)
  	return 0;
  }
  
* Unmerged path fs/ext4/file.c
