route: move lwtunnel state to dst_entry

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jiri Benc <jbenc@redhat.com>
commit 61adedf3e3f1d3f032c5a6a299978d91eff6d555
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/61adedf3.failed

Currently, the lwtunnel state resides in per-protocol data. This is
a problem if we encapsulate ipv6 traffic in an ipv4 tunnel (or vice versa).
The xmit function of the tunnel does not know whether the packet has been
routed to it by ipv4 or ipv6, yet it needs the lwtstate data. Moving the
lwtstate data to dst_entry makes such inter-protocol tunneling possible.

As a bonus, this brings a nice diffstat.

	Signed-off-by: Jiri Benc <jbenc@redhat.com>
	Acked-by: Roopa Prabhu <roopa@cumulusnetworks.com>
	Acked-by: Thomas Graf <tgraf@suug.ch>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 61adedf3e3f1d3f032c5a6a299978d91eff6d555)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/vrf.c
#	drivers/net/vxlan.c
#	include/net/dst_metadata.h
#	include/net/lwtunnel.h
#	include/net/route.h
#	net/core/dst.c
#	net/core/filter.c
#	net/core/lwtunnel.c
#	net/ipv4/ip_gre.c
#	net/ipv4/route.c
#	net/ipv6/ila.c
#	net/ipv6/ip6_fib.c
#	net/ipv6/route.c
#	net/mpls/mpls_iptunnel.c
diff --cc drivers/net/vxlan.c
index 37149323723f,93613ffd8d7e..000000000000
--- a/drivers/net/vxlan.c
+++ b/drivers/net/vxlan.c
@@@ -1888,10 -1906,28 +1888,33 @@@ static void vxlan_xmit_one(struct sk_bu
  	__be16 df = 0;
  	__u8 tos, ttl;
  	int err;
 -	u32 flags = vxlan->flags;
  
++<<<<<<< HEAD
 +	dst_port = rdst->remote_port ? rdst->remote_port : vxlan->dst_port;
 +	vni = rdst->remote_vni;
 +	dst = &rdst->remote_ip;
++=======
+ 	/* FIXME: Support IPv6 */
+ 	info = skb_tunnel_info(skb);
+ 
+ 	if (rdst) {
+ 		dst_port = rdst->remote_port ? rdst->remote_port : vxlan->cfg.dst_port;
+ 		vni = rdst->remote_vni;
+ 		dst = &rdst->remote_ip;
+ 	} else {
+ 		if (!info) {
+ 			WARN_ONCE(1, "%s: Missing encapsulation instructions\n",
+ 				  dev->name);
+ 			goto drop;
+ 		}
+ 
+ 		dst_port = info->key.tp_dst ? : vxlan->cfg.dst_port;
+ 		vni = be64_to_cpu(info->key.tun_id);
+ 		remote_ip.sin.sin_family = AF_INET;
+ 		remote_ip.sin.sin_addr.s_addr = info->key.u.ipv4.dst;
+ 		dst = &remote_ip;
+ 	}
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  
  	if (vxlan_addr_any(dst)) {
  		if (did_rsc) {
@@@ -2054,6 -2104,9 +2077,12 @@@ static netdev_tx_t vxlan_xmit(struct sk
  	struct vxlan_rdst *rdst, *fdst = NULL;
  	struct vxlan_fdb *f;
  
++<<<<<<< HEAD
++=======
+ 	/* FIXME: Support IPv6 */
+ 	info = skb_tunnel_info(skb);
+ 
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  	skb_reset_mac_header(skb);
  	eth = eth_hdr(skb);
  
diff --cc include/net/route.h
index 3765d2348b06,395d79bb556c..000000000000
--- a/include/net/route.h
+++ b/include/net/route.h
@@@ -64,6 -65,7 +64,10 @@@ struct rtable 
  	u32			rt_pmtu;
  
  	struct list_head	rt_uncached;
++<<<<<<< HEAD
++=======
+ 	struct uncached_list	*rt_uncached_list;
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  };
  
  static inline bool rt_is_input_route(const struct rtable *rt)
diff --cc net/core/dst.c
index 65dccfb58d44,50dcdbb0ee46..000000000000
--- a/net/core/dst.c
+++ b/net/core/dst.c
@@@ -20,8 -20,10 +20,9 @@@
  #include <net/net_namespace.h>
  #include <linux/sched.h>
  #include <linux/prefetch.h>
+ #include <net/lwtunnel.h>
  
  #include <net/dst.h>
 -#include <net/dst_metadata.h>
  
  /*
   * Theory of operations:
@@@ -248,7 -261,12 +250,16 @@@ again
  		dst->ops->destroy(dst);
  	if (dst->dev)
  		dev_put(dst->dev);
++<<<<<<< HEAD
 +	kmem_cache_free(dst->ops->kmem_cachep, dst);
++=======
+ 
+ 	if (dst->flags & DST_METADATA)
+ 		kfree(dst);
+ 	else
+ 		kmem_cache_free(dst->ops->kmem_cachep, dst);
+ 	lwtstate_put(dst->lwtstate);
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  
  	dst = child;
  	if (dst) {
diff --cc net/core/filter.c
index f4124aee170e,b4adc961413f..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -761,6 -1216,607 +761,610 @@@ int sk_attach_filter(struct sock_fprog 
  }
  EXPORT_SYMBOL_GPL(sk_attach_filter);
  
++<<<<<<< HEAD
++=======
+ int sk_attach_bpf(u32 ufd, struct sock *sk)
+ {
+ 	struct bpf_prog *prog;
+ 	int err;
+ 
+ 	if (sock_flag(sk, SOCK_FILTER_LOCKED))
+ 		return -EPERM;
+ 
+ 	prog = bpf_prog_get(ufd);
+ 	if (IS_ERR(prog))
+ 		return PTR_ERR(prog);
+ 
+ 	if (prog->type != BPF_PROG_TYPE_SOCKET_FILTER) {
+ 		bpf_prog_put(prog);
+ 		return -EINVAL;
+ 	}
+ 
+ 	err = __sk_attach_prog(prog, sk);
+ 	if (err < 0) {
+ 		bpf_prog_put(prog);
+ 		return err;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ #define BPF_RECOMPUTE_CSUM(flags)	((flags) & 1)
+ 
+ static u64 bpf_skb_store_bytes(u64 r1, u64 r2, u64 r3, u64 r4, u64 flags)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	int offset = (int) r2;
+ 	void *from = (void *) (long) r3;
+ 	unsigned int len = (unsigned int) r4;
+ 	char buf[16];
+ 	void *ptr;
+ 
+ 	/* bpf verifier guarantees that:
+ 	 * 'from' pointer points to bpf program stack
+ 	 * 'len' bytes of it were initialized
+ 	 * 'len' > 0
+ 	 * 'skb' is a valid pointer to 'struct sk_buff'
+ 	 *
+ 	 * so check for invalid 'offset' and too large 'len'
+ 	 */
+ 	if (unlikely((u32) offset > 0xffff || len > sizeof(buf)))
+ 		return -EFAULT;
+ 
+ 	if (unlikely(skb_cloned(skb) &&
+ 		     !skb_clone_writable(skb, offset + len)))
+ 		return -EFAULT;
+ 
+ 	ptr = skb_header_pointer(skb, offset, len, buf);
+ 	if (unlikely(!ptr))
+ 		return -EFAULT;
+ 
+ 	if (BPF_RECOMPUTE_CSUM(flags))
+ 		skb_postpull_rcsum(skb, ptr, len);
+ 
+ 	memcpy(ptr, from, len);
+ 
+ 	if (ptr == buf)
+ 		/* skb_store_bits cannot return -EFAULT here */
+ 		skb_store_bits(skb, offset, ptr, len);
+ 
+ 	if (BPF_RECOMPUTE_CSUM(flags) && skb->ip_summed == CHECKSUM_COMPLETE)
+ 		skb->csum = csum_add(skb->csum, csum_partial(ptr, len, 0));
+ 	return 0;
+ }
+ 
+ const struct bpf_func_proto bpf_skb_store_bytes_proto = {
+ 	.func		= bpf_skb_store_bytes,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_PTR_TO_STACK,
+ 	.arg4_type	= ARG_CONST_STACK_SIZE,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ #define BPF_HEADER_FIELD_SIZE(flags)	((flags) & 0x0f)
+ #define BPF_IS_PSEUDO_HEADER(flags)	((flags) & 0x10)
+ 
+ static u64 bpf_l3_csum_replace(u64 r1, u64 r2, u64 from, u64 to, u64 flags)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	int offset = (int) r2;
+ 	__sum16 sum, *ptr;
+ 
+ 	if (unlikely((u32) offset > 0xffff))
+ 		return -EFAULT;
+ 
+ 	if (unlikely(skb_cloned(skb) &&
+ 		     !skb_clone_writable(skb, offset + sizeof(sum))))
+ 		return -EFAULT;
+ 
+ 	ptr = skb_header_pointer(skb, offset, sizeof(sum), &sum);
+ 	if (unlikely(!ptr))
+ 		return -EFAULT;
+ 
+ 	switch (BPF_HEADER_FIELD_SIZE(flags)) {
+ 	case 2:
+ 		csum_replace2(ptr, from, to);
+ 		break;
+ 	case 4:
+ 		csum_replace4(ptr, from, to);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (ptr == &sum)
+ 		/* skb_store_bits guaranteed to not return -EFAULT here */
+ 		skb_store_bits(skb, offset, ptr, sizeof(sum));
+ 
+ 	return 0;
+ }
+ 
+ const struct bpf_func_proto bpf_l3_csum_replace_proto = {
+ 	.func		= bpf_l3_csum_replace,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_ANYTHING,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_l4_csum_replace(u64 r1, u64 r2, u64 from, u64 to, u64 flags)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	bool is_pseudo = !!BPF_IS_PSEUDO_HEADER(flags);
+ 	int offset = (int) r2;
+ 	__sum16 sum, *ptr;
+ 
+ 	if (unlikely((u32) offset > 0xffff))
+ 		return -EFAULT;
+ 
+ 	if (unlikely(skb_cloned(skb) &&
+ 		     !skb_clone_writable(skb, offset + sizeof(sum))))
+ 		return -EFAULT;
+ 
+ 	ptr = skb_header_pointer(skb, offset, sizeof(sum), &sum);
+ 	if (unlikely(!ptr))
+ 		return -EFAULT;
+ 
+ 	switch (BPF_HEADER_FIELD_SIZE(flags)) {
+ 	case 2:
+ 		inet_proto_csum_replace2(ptr, skb, from, to, is_pseudo);
+ 		break;
+ 	case 4:
+ 		inet_proto_csum_replace4(ptr, skb, from, to, is_pseudo);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (ptr == &sum)
+ 		/* skb_store_bits guaranteed to not return -EFAULT here */
+ 		skb_store_bits(skb, offset, ptr, sizeof(sum));
+ 
+ 	return 0;
+ }
+ 
+ const struct bpf_func_proto bpf_l4_csum_replace_proto = {
+ 	.func		= bpf_l4_csum_replace,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_ANYTHING,
+ 	.arg3_type	= ARG_ANYTHING,
+ 	.arg4_type	= ARG_ANYTHING,
+ 	.arg5_type	= ARG_ANYTHING,
+ };
+ 
+ #define BPF_IS_REDIRECT_INGRESS(flags)	((flags) & 1)
+ 
+ static u64 bpf_clone_redirect(u64 r1, u64 ifindex, u64 flags, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1, *skb2;
+ 	struct net_device *dev;
+ 
+ 	dev = dev_get_by_index_rcu(dev_net(skb->dev), ifindex);
+ 	if (unlikely(!dev))
+ 		return -EINVAL;
+ 
+ 	if (unlikely(!(dev->flags & IFF_UP)))
+ 		return -EINVAL;
+ 
+ 	skb2 = skb_clone(skb, GFP_ATOMIC);
+ 	if (unlikely(!skb2))
+ 		return -ENOMEM;
+ 
+ 	if (BPF_IS_REDIRECT_INGRESS(flags))
+ 		return dev_forward_skb(dev, skb2);
+ 
+ 	skb2->dev = dev;
+ 	return dev_queue_xmit(skb2);
+ }
+ 
+ const struct bpf_func_proto bpf_clone_redirect_proto = {
+ 	.func           = bpf_clone_redirect,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_ANYTHING,
+ 	.arg3_type      = ARG_ANYTHING,
+ };
+ 
+ static u64 bpf_get_cgroup_classid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	return task_get_classid((struct sk_buff *) (unsigned long) r1);
+ }
+ 
+ static const struct bpf_func_proto bpf_get_cgroup_classid_proto = {
+ 	.func           = bpf_get_cgroup_classid,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ };
+ 
+ static u64 bpf_skb_vlan_push(u64 r1, u64 r2, u64 vlan_tci, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	__be16 vlan_proto = (__force __be16) r2;
+ 
+ 	if (unlikely(vlan_proto != htons(ETH_P_8021Q) &&
+ 		     vlan_proto != htons(ETH_P_8021AD)))
+ 		vlan_proto = htons(ETH_P_8021Q);
+ 
+ 	return skb_vlan_push(skb, vlan_proto, vlan_tci);
+ }
+ 
+ const struct bpf_func_proto bpf_skb_vlan_push_proto = {
+ 	.func           = bpf_skb_vlan_push,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ 	.arg2_type      = ARG_ANYTHING,
+ 	.arg3_type      = ARG_ANYTHING,
+ };
+ EXPORT_SYMBOL_GPL(bpf_skb_vlan_push_proto);
+ 
+ static u64 bpf_skb_vlan_pop(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 
+ 	return skb_vlan_pop(skb);
+ }
+ 
+ const struct bpf_func_proto bpf_skb_vlan_pop_proto = {
+ 	.func           = bpf_skb_vlan_pop,
+ 	.gpl_only       = false,
+ 	.ret_type       = RET_INTEGER,
+ 	.arg1_type      = ARG_PTR_TO_CTX,
+ };
+ EXPORT_SYMBOL_GPL(bpf_skb_vlan_pop_proto);
+ 
+ bool bpf_helper_changes_skb_data(void *func)
+ {
+ 	if (func == bpf_skb_vlan_push)
+ 		return true;
+ 	if (func == bpf_skb_vlan_pop)
+ 		return true;
+ 	return false;
+ }
+ 
+ static u64 bpf_skb_get_tunnel_key(u64 r1, u64 r2, u64 size, u64 flags, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	struct bpf_tunnel_key *to = (struct bpf_tunnel_key *) (long) r2;
+ 	struct ip_tunnel_info *info = skb_tunnel_info(skb);
+ 
+ 	if (unlikely(size != sizeof(struct bpf_tunnel_key) || flags || !info))
+ 		return -EINVAL;
+ 
+ 	to->tunnel_id = be64_to_cpu(info->key.tun_id);
+ 	to->remote_ipv4 = be32_to_cpu(info->key.u.ipv4.src);
+ 
+ 	return 0;
+ }
+ 
+ const struct bpf_func_proto bpf_skb_get_tunnel_key_proto = {
+ 	.func		= bpf_skb_get_tunnel_key,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_STACK,
+ 	.arg3_type	= ARG_CONST_STACK_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ };
+ 
+ static struct metadata_dst __percpu *md_dst;
+ 
+ static u64 bpf_skb_set_tunnel_key(u64 r1, u64 r2, u64 size, u64 flags, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *) (long) r1;
+ 	struct bpf_tunnel_key *from = (struct bpf_tunnel_key *) (long) r2;
+ 	struct metadata_dst *md = this_cpu_ptr(md_dst);
+ 	struct ip_tunnel_info *info;
+ 
+ 	if (unlikely(size != sizeof(struct bpf_tunnel_key) || flags))
+ 		return -EINVAL;
+ 
+ 	skb_dst_drop(skb);
+ 	dst_hold((struct dst_entry *) md);
+ 	skb_dst_set(skb, (struct dst_entry *) md);
+ 
+ 	info = &md->u.tun_info;
+ 	info->mode = IP_TUNNEL_INFO_TX;
+ 	info->key.tun_id = cpu_to_be64(from->tunnel_id);
+ 	info->key.u.ipv4.dst = cpu_to_be32(from->remote_ipv4);
+ 
+ 	return 0;
+ }
+ 
+ const struct bpf_func_proto bpf_skb_set_tunnel_key_proto = {
+ 	.func		= bpf_skb_set_tunnel_key,
+ 	.gpl_only	= false,
+ 	.ret_type	= RET_INTEGER,
+ 	.arg1_type	= ARG_PTR_TO_CTX,
+ 	.arg2_type	= ARG_PTR_TO_STACK,
+ 	.arg3_type	= ARG_CONST_STACK_SIZE,
+ 	.arg4_type	= ARG_ANYTHING,
+ };
+ 
+ static const struct bpf_func_proto *bpf_get_skb_set_tunnel_key_proto(void)
+ {
+ 	if (!md_dst) {
+ 		/* race is not possible, since it's called from
+ 		 * verifier that is holding verifier mutex
+ 		 */
+ 		md_dst = metadata_dst_alloc_percpu(0, GFP_KERNEL);
+ 		if (!md_dst)
+ 			return NULL;
+ 	}
+ 	return &bpf_skb_set_tunnel_key_proto;
+ }
+ 
+ static const struct bpf_func_proto *
+ sk_filter_func_proto(enum bpf_func_id func_id)
+ {
+ 	switch (func_id) {
+ 	case BPF_FUNC_map_lookup_elem:
+ 		return &bpf_map_lookup_elem_proto;
+ 	case BPF_FUNC_map_update_elem:
+ 		return &bpf_map_update_elem_proto;
+ 	case BPF_FUNC_map_delete_elem:
+ 		return &bpf_map_delete_elem_proto;
+ 	case BPF_FUNC_get_prandom_u32:
+ 		return &bpf_get_prandom_u32_proto;
+ 	case BPF_FUNC_get_smp_processor_id:
+ 		return &bpf_get_smp_processor_id_proto;
+ 	case BPF_FUNC_tail_call:
+ 		return &bpf_tail_call_proto;
+ 	case BPF_FUNC_ktime_get_ns:
+ 		return &bpf_ktime_get_ns_proto;
+ 	case BPF_FUNC_trace_printk:
+ 		return bpf_get_trace_printk_proto();
+ 	default:
+ 		return NULL;
+ 	}
+ }
+ 
+ static const struct bpf_func_proto *
+ tc_cls_act_func_proto(enum bpf_func_id func_id)
+ {
+ 	switch (func_id) {
+ 	case BPF_FUNC_skb_store_bytes:
+ 		return &bpf_skb_store_bytes_proto;
+ 	case BPF_FUNC_l3_csum_replace:
+ 		return &bpf_l3_csum_replace_proto;
+ 	case BPF_FUNC_l4_csum_replace:
+ 		return &bpf_l4_csum_replace_proto;
+ 	case BPF_FUNC_clone_redirect:
+ 		return &bpf_clone_redirect_proto;
+ 	case BPF_FUNC_get_cgroup_classid:
+ 		return &bpf_get_cgroup_classid_proto;
+ 	case BPF_FUNC_skb_vlan_push:
+ 		return &bpf_skb_vlan_push_proto;
+ 	case BPF_FUNC_skb_vlan_pop:
+ 		return &bpf_skb_vlan_pop_proto;
+ 	case BPF_FUNC_skb_get_tunnel_key:
+ 		return &bpf_skb_get_tunnel_key_proto;
+ 	case BPF_FUNC_skb_set_tunnel_key:
+ 		return bpf_get_skb_set_tunnel_key_proto();
+ 	default:
+ 		return sk_filter_func_proto(func_id);
+ 	}
+ }
+ 
+ static bool __is_valid_access(int off, int size, enum bpf_access_type type)
+ {
+ 	/* check bounds */
+ 	if (off < 0 || off >= sizeof(struct __sk_buff))
+ 		return false;
+ 
+ 	/* disallow misaligned access */
+ 	if (off % size != 0)
+ 		return false;
+ 
+ 	/* all __sk_buff fields are __u32 */
+ 	if (size != 4)
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static bool sk_filter_is_valid_access(int off, int size,
+ 				      enum bpf_access_type type)
+ {
+ 	if (type == BPF_WRITE) {
+ 		switch (off) {
+ 		case offsetof(struct __sk_buff, cb[0]) ...
+ 			offsetof(struct __sk_buff, cb[4]):
+ 			break;
+ 		default:
+ 			return false;
+ 		}
+ 	}
+ 
+ 	return __is_valid_access(off, size, type);
+ }
+ 
+ static bool tc_cls_act_is_valid_access(int off, int size,
+ 				       enum bpf_access_type type)
+ {
+ 	if (type == BPF_WRITE) {
+ 		switch (off) {
+ 		case offsetof(struct __sk_buff, mark):
+ 		case offsetof(struct __sk_buff, tc_index):
+ 		case offsetof(struct __sk_buff, cb[0]) ...
+ 			offsetof(struct __sk_buff, cb[4]):
+ 			break;
+ 		default:
+ 			return false;
+ 		}
+ 	}
+ 	return __is_valid_access(off, size, type);
+ }
+ 
+ static u32 bpf_net_convert_ctx_access(enum bpf_access_type type, int dst_reg,
+ 				      int src_reg, int ctx_off,
+ 				      struct bpf_insn *insn_buf)
+ {
+ 	struct bpf_insn *insn = insn_buf;
+ 
+ 	switch (ctx_off) {
+ 	case offsetof(struct __sk_buff, len):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, len));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, protocol):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, protocol));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, vlan_proto):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_proto) != 2);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, vlan_proto));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, priority):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, priority) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, priority));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, ingress_ifindex):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, skb_iif) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, skb_iif));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, ifindex):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)),
+ 				      dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, dev));
+ 		*insn++ = BPF_JMP_IMM(BPF_JEQ, dst_reg, 0, 1);
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, dst_reg,
+ 				      offsetof(struct net_device, ifindex));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, hash):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
+ 
+ 		*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 				      offsetof(struct sk_buff, hash));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, mark):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
+ 
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_W, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, mark));
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, mark));
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, pkt_type):
+ 		return convert_skb_access(SKF_AD_PKTTYPE, dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, queue_mapping):
+ 		return convert_skb_access(SKF_AD_QUEUE, dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, vlan_present):
+ 		return convert_skb_access(SKF_AD_VLAN_TAG_PRESENT,
+ 					  dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, vlan_tci):
+ 		return convert_skb_access(SKF_AD_VLAN_TAG,
+ 					  dst_reg, src_reg, insn);
+ 
+ 	case offsetof(struct __sk_buff, cb[0]) ...
+ 		offsetof(struct __sk_buff, cb[4]):
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct qdisc_skb_cb, data) < 20);
+ 
+ 		ctx_off -= offsetof(struct __sk_buff, cb[0]);
+ 		ctx_off += offsetof(struct sk_buff, cb);
+ 		ctx_off += offsetof(struct qdisc_skb_cb, data);
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_W, dst_reg, src_reg, ctx_off);
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg, ctx_off);
+ 		break;
+ 
+ 	case offsetof(struct __sk_buff, tc_index):
+ #ifdef CONFIG_NET_SCHED
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, tc_index) != 2);
+ 
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_STX_MEM(BPF_H, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, tc_index));
+ 		else
+ 			*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,
+ 					      offsetof(struct sk_buff, tc_index));
+ 		break;
+ #else
+ 		if (type == BPF_WRITE)
+ 			*insn++ = BPF_MOV64_REG(dst_reg, dst_reg);
+ 		else
+ 			*insn++ = BPF_MOV64_IMM(dst_reg, 0);
+ 		break;
+ #endif
+ 	}
+ 
+ 	return insn - insn_buf;
+ }
+ 
+ static const struct bpf_verifier_ops sk_filter_ops = {
+ 	.get_func_proto = sk_filter_func_proto,
+ 	.is_valid_access = sk_filter_is_valid_access,
+ 	.convert_ctx_access = bpf_net_convert_ctx_access,
+ };
+ 
+ static const struct bpf_verifier_ops tc_cls_act_ops = {
+ 	.get_func_proto = tc_cls_act_func_proto,
+ 	.is_valid_access = tc_cls_act_is_valid_access,
+ 	.convert_ctx_access = bpf_net_convert_ctx_access,
+ };
+ 
+ static struct bpf_prog_type_list sk_filter_type __read_mostly = {
+ 	.ops = &sk_filter_ops,
+ 	.type = BPF_PROG_TYPE_SOCKET_FILTER,
+ };
+ 
+ static struct bpf_prog_type_list sched_cls_type __read_mostly = {
+ 	.ops = &tc_cls_act_ops,
+ 	.type = BPF_PROG_TYPE_SCHED_CLS,
+ };
+ 
+ static struct bpf_prog_type_list sched_act_type __read_mostly = {
+ 	.ops = &tc_cls_act_ops,
+ 	.type = BPF_PROG_TYPE_SCHED_ACT,
+ };
+ 
+ static int __init register_sk_filter_ops(void)
+ {
+ 	bpf_register_prog_type(&sk_filter_type);
+ 	bpf_register_prog_type(&sched_cls_type);
+ 	bpf_register_prog_type(&sched_act_type);
+ 
+ 	return 0;
+ }
+ late_initcall(register_sk_filter_ops);
+ 
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  int sk_detach_filter(struct sock *sk)
  {
  	int ret = -ENOENT;
diff --cc net/ipv4/ip_gre.c
index afc4a83f7ee7,1bf328182697..000000000000
--- a/net/ipv4/ip_gre.c
+++ b/net/ipv4/ip_gre.c
@@@ -246,6 -501,81 +246,83 @@@ static void __gre_xmit(struct sk_buff *
  	ip_tunnel_xmit(skb, dev, tnl_params, tnl_params->protocol);
  }
  
++<<<<<<< HEAD
++=======
+ static struct sk_buff *gre_handle_offloads(struct sk_buff *skb,
+ 					   bool csum)
+ {
+ 	return iptunnel_handle_offloads(skb, csum,
+ 					csum ? SKB_GSO_GRE_CSUM : SKB_GSO_GRE);
+ }
+ 
+ static void gre_fb_xmit(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	struct ip_tunnel_info *tun_info;
+ 	struct net *net = dev_net(dev);
+ 	const struct ip_tunnel_key *key;
+ 	struct flowi4 fl;
+ 	struct rtable *rt;
+ 	int min_headroom;
+ 	int tunnel_hlen;
+ 	__be16 df, flags;
+ 	int err;
+ 
+ 	tun_info = skb_tunnel_info(skb);
+ 	if (unlikely(!tun_info || tun_info->mode != IP_TUNNEL_INFO_TX))
+ 		goto err_free_skb;
+ 
+ 	key = &tun_info->key;
+ 	memset(&fl, 0, sizeof(fl));
+ 	fl.daddr = key->u.ipv4.dst;
+ 	fl.saddr = key->u.ipv4.src;
+ 	fl.flowi4_tos = RT_TOS(key->tos);
+ 	fl.flowi4_mark = skb->mark;
+ 	fl.flowi4_proto = IPPROTO_GRE;
+ 
+ 	rt = ip_route_output_key(net, &fl);
+ 	if (IS_ERR(rt))
+ 		goto err_free_skb;
+ 
+ 	tunnel_hlen = ip_gre_calc_hlen(key->tun_flags);
+ 
+ 	min_headroom = LL_RESERVED_SPACE(rt->dst.dev) + rt->dst.header_len
+ 			+ tunnel_hlen + sizeof(struct iphdr);
+ 	if (skb_headroom(skb) < min_headroom || skb_header_cloned(skb)) {
+ 		int head_delta = SKB_DATA_ALIGN(min_headroom -
+ 						skb_headroom(skb) +
+ 						16);
+ 		err = pskb_expand_head(skb, max_t(int, head_delta, 0),
+ 				       0, GFP_ATOMIC);
+ 		if (unlikely(err))
+ 			goto err_free_rt;
+ 	}
+ 
+ 	/* Push Tunnel header. */
+ 	skb = gre_handle_offloads(skb, !!(tun_info->key.tun_flags & TUNNEL_CSUM));
+ 	if (IS_ERR(skb)) {
+ 		skb = NULL;
+ 		goto err_free_rt;
+ 	}
+ 
+ 	flags = tun_info->key.tun_flags & (TUNNEL_CSUM | TUNNEL_KEY);
+ 	build_header(skb, tunnel_hlen, flags, htons(ETH_P_TEB),
+ 		     tunnel_id_to_key(tun_info->key.tun_id), 0);
+ 
+ 	df = key->tun_flags & TUNNEL_DONT_FRAGMENT ?  htons(IP_DF) : 0;
+ 	err = iptunnel_xmit(skb->sk, rt, skb, fl.saddr,
+ 			    key->u.ipv4.dst, IPPROTO_GRE,
+ 			    key->tos, key->ttl, df, false);
+ 	iptunnel_xmit_stats(err, &dev->stats, dev->tstats);
+ 	return;
+ 
+ err_free_rt:
+ 	ip_rt_put(rt);
+ err_free_skb:
+ 	kfree_skb(skb);
+ 	dev->stats.tx_dropped++;
+ }
+ 
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  static netdev_tx_t ipgre_xmit(struct sk_buff *skb,
  			      struct net_device *dev)
  {
diff --cc net/ipv4/route.c
index 31700d4dc025,f3087aaa6dd8..000000000000
--- a/net/ipv4/route.c
+++ b/net/ipv4/route.c
@@@ -1380,6 -1407,7 +1380,10 @@@ static void rt_set_nexthop(struct rtabl
  #ifdef CONFIG_IP_ROUTE_CLASSID
  		rt->dst.tclassid = nh->nh_tclassid;
  #endif
++<<<<<<< HEAD
++=======
+ 		rt->dst.lwtstate = lwtstate_get(nh->nh_lwtstate);
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  		if (unlikely(fnhe))
  			cached = rt_bind_exception(rt, fnhe, daddr);
  		else if (!(rt->dst.flags & DST_NOCACHE))
@@@ -1595,13 -1622,20 +1599,28 @@@ static int __mkroute_input(struct sk_bu
  	rth->rt_gateway	= 0;
  	rth->rt_uses_gateway = 0;
  	INIT_LIST_HEAD(&rth->rt_uncached);
++<<<<<<< HEAD
++=======
+ 	RT_CACHE_STAT_INC(in_slow_tot);
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  
  	rth->dst.input = ip_forward;
  	rth->dst.output = ip_output;
  
  	rt_set_nexthop(rth, daddr, res, fnhe, res->fi, res->type, itag);
++<<<<<<< HEAD
 +	if (lwtunnel_output_redirect(rth->rt_lwtstate))
 +		rth->dst.output = lwtunnel_output;
++=======
+ 	if (lwtunnel_output_redirect(rth->dst.lwtstate)) {
+ 		rth->dst.lwtstate->orig_output = rth->dst.output;
+ 		rth->dst.output = lwtunnel_output;
+ 	}
+ 	if (lwtunnel_input_redirect(rth->dst.lwtstate)) {
+ 		rth->dst.lwtstate->orig_input = rth->dst.input;
+ 		rth->dst.input = lwtunnel_input;
+ 	}
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  	skb_dst_set(skb, &rth->dst);
  out:
  	err = 0;
@@@ -1657,6 -1692,13 +1676,16 @@@ static int ip_route_input_slow(struct s
  	   by fib_lookup.
  	 */
  
++<<<<<<< HEAD
++=======
+ 	tun_info = skb_tunnel_info(skb);
+ 	if (tun_info && tun_info->mode == IP_TUNNEL_INFO_RX)
+ 		fl4.flowi4_tun_key.tun_id = tun_info->key.tun_id;
+ 	else
+ 		fl4.flowi4_tun_key.tun_id = 0;
+ 	skb_dst_drop(skb);
+ 
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  	if (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr))
  		goto martian_source;
  
@@@ -1768,6 -1812,8 +1797,11 @@@ local_input
  	rth->rt_gateway	= 0;
  	rth->rt_uses_gateway = 0;
  	INIT_LIST_HEAD(&rth->rt_uncached);
++<<<<<<< HEAD
++=======
+ 
+ 	RT_CACHE_STAT_INC(in_slow_tot);
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  	if (res.type == RTN_UNREACHABLE) {
  		rth->dst.input= ip_error;
  		rth->dst.error= -err;
@@@ -1957,7 -2002,6 +1991,10 @@@ add
  	rth->rt_gateway = 0;
  	rth->rt_uses_gateway = 0;
  	INIT_LIST_HEAD(&rth->rt_uncached);
++<<<<<<< HEAD
 +
++=======
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  	RT_CACHE_STAT_INC(out_slow_tot);
  
  	if (flags & RTCF_LOCAL)
@@@ -2239,7 -2288,6 +2276,10 @@@ struct dst_entry *ipv4_blackhole_route(
  		rt->rt_uses_gateway = ort->rt_uses_gateway;
  
  		INIT_LIST_HEAD(&rt->rt_uncached);
++<<<<<<< HEAD
 +
++=======
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  		dst_free(new);
  	}
  
diff --cc net/ipv6/ip6_fib.c
index f06e53333b5c,865e777ae20c..000000000000
--- a/net/ipv6/ip6_fib.c
+++ b/net/ipv6/ip6_fib.c
@@@ -161,10 -155,32 +161,35 @@@ static __inline__ void node_free(struc
  	kmem_cache_free(fib6_node_kmem, fn);
  }
  
 -static void rt6_free_pcpu(struct rt6_info *non_pcpu_rt)
 +static __inline__ void rt6_release(struct rt6_info *rt)
  {
++<<<<<<< HEAD
 +	if (atomic_dec_and_test(&rt->rt6i_ref))
++=======
+ 	int cpu;
+ 
+ 	if (!non_pcpu_rt->rt6i_pcpu)
+ 		return;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct rt6_info **ppcpu_rt;
+ 		struct rt6_info *pcpu_rt;
+ 
+ 		ppcpu_rt = per_cpu_ptr(non_pcpu_rt->rt6i_pcpu, cpu);
+ 		pcpu_rt = *ppcpu_rt;
+ 		if (pcpu_rt) {
+ 			dst_free(&pcpu_rt->dst);
+ 			*ppcpu_rt = NULL;
+ 		}
+ 	}
+ }
+ 
+ static void rt6_release(struct rt6_info *rt)
+ {
+ 	if (atomic_dec_and_test(&rt->rt6i_ref)) {
+ 		rt6_free_pcpu(rt);
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  		dst_free(&rt->dst);
 -	}
  }
  
  static void fib6_link_table(struct net *net, struct fib6_table *tb)
diff --cc net/ipv6/route.c
index 1b3524b3978c,e6bbcdee7707..000000000000
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@@ -1610,12 -1777,28 +1610,33 @@@ int ip6_route_add(struct fib6_config *c
  
  	rt->dst.output = ip6_output;
  
++<<<<<<< HEAD
++=======
+ 	if (cfg->fc_encap) {
+ 		struct lwtunnel_state *lwtstate;
+ 
+ 		err = lwtunnel_build_state(dev, cfg->fc_encap_type,
+ 					   cfg->fc_encap, &lwtstate);
+ 		if (err)
+ 			goto out;
+ 		rt->dst.lwtstate = lwtstate_get(lwtstate);
+ 		if (lwtunnel_output_redirect(rt->dst.lwtstate)) {
+ 			rt->dst.lwtstate->orig_output = rt->dst.output;
+ 			rt->dst.output = lwtunnel_output;
+ 		}
+ 		if (lwtunnel_input_redirect(rt->dst.lwtstate)) {
+ 			rt->dst.lwtstate->orig_input = rt->dst.input;
+ 			rt->dst.input = lwtunnel_input;
+ 		}
+ 	}
+ 
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  	ipv6_addr_prefix(&rt->rt6i_dst.addr, &cfg->fc_dst, cfg->fc_dst_len);
  	rt->rt6i_dst.plen = cfg->fc_dst_len;
 -	if (rt->rt6i_dst.plen == 128)
 +	if (rt->rt6i_dst.plen == 128) {
  		rt->dst.flags |= DST_HOST;
 +		dst_metrics_set_force_overwrite(&rt->dst);
 +	}
  
  #ifdef CONFIG_IPV6_SUBTREES
  	ipv6_addr_prefix(&rt->rt6i_src.addr, &cfg->fc_src, cfg->fc_src_len);
@@@ -1961,42 -2145,36 +1982,48 @@@ out
   *	Misc support functions
   */
  
 -static void rt6_set_from(struct rt6_info *rt, struct rt6_info *from)
 +static struct rt6_info *ip6_rt_copy(struct rt6_info *ort,
 +				    const struct in6_addr *dest)
  {
 -	BUG_ON(from->dst.from);
 +	struct net *net = dev_net(ort->dst.dev);
 +	struct rt6_info *rt = ip6_dst_alloc(net, ort->dst.dev, 0,
 +					    ort->rt6i_table);
  
 -	rt->rt6i_flags &= ~RTF_EXPIRES;
 -	dst_hold(&from->dst);
 -	rt->dst.from = &from->dst;
 -	dst_init_metrics(&rt->dst, dst_metrics_ptr(&from->dst), true);
 -}
 +	if (rt) {
 +		rt->dst.input = ort->dst.input;
 +		rt->dst.output = ort->dst.output;
 +		rt->dst.flags |= DST_HOST;
 +
 +		rt->rt6i_dst.addr = *dest;
 +		rt->rt6i_dst.plen = 128;
 +		dst_copy_metrics(&rt->dst, &ort->dst);
 +		rt->dst.error = ort->dst.error;
 +		rt->rt6i_idev = ort->rt6i_idev;
 +		if (rt->rt6i_idev)
 +			in6_dev_hold(rt->rt6i_idev);
 +		rt->dst.lastuse = jiffies;
 +
 +		if (ort->rt6i_flags & RTF_GATEWAY)
 +			rt->rt6i_gateway = ort->rt6i_gateway;
 +		else
 +			rt->rt6i_gateway = *dest;
 +		rt->rt6i_flags = ort->rt6i_flags;
 +		rt6_set_from(rt, ort);
 +		rt->rt6i_metric = 0;
  
 -static void ip6_rt_copy_init(struct rt6_info *rt, struct rt6_info *ort)
 -{
 -	rt->dst.input = ort->dst.input;
 -	rt->dst.output = ort->dst.output;
 -	rt->rt6i_dst = ort->rt6i_dst;
 -	rt->dst.error = ort->dst.error;
 -	rt->rt6i_idev = ort->rt6i_idev;
 -	if (rt->rt6i_idev)
 -		in6_dev_hold(rt->rt6i_idev);
 -	rt->dst.lastuse = jiffies;
 -	rt->rt6i_gateway = ort->rt6i_gateway;
 -	rt->rt6i_flags = ort->rt6i_flags;
 -	rt6_set_from(rt, ort);
 -	rt->rt6i_metric = ort->rt6i_metric;
  #ifdef CONFIG_IPV6_SUBTREES
 -	rt->rt6i_src = ort->rt6i_src;
 +		memcpy(&rt->rt6i_src, &ort->rt6i_src, sizeof(struct rt6key));
  #endif
++<<<<<<< HEAD
 +		memcpy(&rt->rt6i_prefsrc, &ort->rt6i_prefsrc, sizeof(struct rt6key));
 +		rt->rt6i_table = ort->rt6i_table;
 +	}
 +	return rt;
++=======
+ 	rt->rt6i_prefsrc = ort->rt6i_prefsrc;
+ 	rt->rt6i_table = ort->rt6i_table;
+ 	rt->dst.lwtstate = lwtstate_get(ort->dst.lwtstate);
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  }
  
  #ifdef CONFIG_IPV6_ROUTE_INFO
@@@ -2621,7 -2836,9 +2648,13 @@@ static inline size_t rt6_nlmsg_size(voi
  	       + nla_total_size(4) /* RTA_PRIORITY */
  	       + RTAX_MAX * nla_total_size(4) /* RTA_METRICS */
  	       + nla_total_size(sizeof(struct rta_cacheinfo))
++<<<<<<< HEAD
 +	       + nla_total_size(TCP_CA_NAME_MAX); /* RTAX_CC_ALGO */
++=======
+ 	       + nla_total_size(TCP_CA_NAME_MAX) /* RTAX_CC_ALGO */
+ 	       + nla_total_size(1) /* RTA_PREF */
+ 	       + lwtunnel_get_encap_size(rt->dst.lwtstate);
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  }
  
  static int rt6_fill_node(struct net *net,
@@@ -2762,7 -2988,13 +2795,17 @@@
  	if (rtnl_put_cacheinfo(skb, &rt->dst, 0, expires, rt->dst.error) < 0)
  		goto nla_put_failure;
  
++<<<<<<< HEAD
 +	return nlmsg_end(skb, nlh);
++=======
+ 	if (nla_put_u8(skb, RTA_PREF, IPV6_EXTRACT_PREF(rt->rt6i_flags)))
+ 		goto nla_put_failure;
+ 
+ 	lwtunnel_fill_encap(skb, rt->dst.lwtstate);
+ 
+ 	nlmsg_end(skb, nlh);
+ 	return 0;
++>>>>>>> 61adedf3e3f1 (route: move lwtunnel state to dst_entry)
  
  nla_put_failure:
  	nlmsg_cancel(skb, nlh);
* Unmerged path drivers/net/vrf.c
* Unmerged path include/net/dst_metadata.h
* Unmerged path include/net/lwtunnel.h
* Unmerged path net/core/lwtunnel.c
* Unmerged path net/ipv6/ila.c
* Unmerged path net/mpls/mpls_iptunnel.c
* Unmerged path drivers/net/vrf.c
* Unmerged path drivers/net/vxlan.c
diff --git a/include/net/dst.h b/include/net/dst.h
index 351d506e7745..974c5f18a77a 100644
--- a/include/net/dst.h
+++ b/include/net/dst.h
@@ -46,6 +46,7 @@ struct dst_entry {
 #else
 	void			*__pad1;
 #endif
+	struct lwtunnel_state   *lwtstate;
 	int			(*input)(struct sk_buff *);
 	RH_KABI_REPLACE(int			(*output)(struct sk_buff *),
 			int			(*output)(struct sock *sk, struct sk_buff *skb))
@@ -91,7 +92,7 @@ struct dst_entry {
 	 * (L1_CACHE_SIZE would be too much)
 	 */
 #ifdef CONFIG_64BIT
-	long			__pad_to_align_refcnt[2];
+	long			__pad_to_align_refcnt[1];
 #endif
 	/*
 	 * __refcnt wants to be on a different cache line from
* Unmerged path include/net/dst_metadata.h
* Unmerged path include/net/lwtunnel.h
* Unmerged path include/net/route.h
* Unmerged path net/core/dst.c
* Unmerged path net/core/filter.c
* Unmerged path net/core/lwtunnel.c
* Unmerged path net/ipv4/ip_gre.c
* Unmerged path net/ipv4/route.c
* Unmerged path net/ipv6/ila.c
* Unmerged path net/ipv6/ip6_fib.c
* Unmerged path net/ipv6/route.c
* Unmerged path net/mpls/mpls_iptunnel.c
diff --git a/net/openvswitch/vport-netdev.c b/net/openvswitch/vport-netdev.c
index 6c7a9d49beb5..127db71f5391 100644
--- a/net/openvswitch/vport-netdev.c
+++ b/net/openvswitch/vport-netdev.c
@@ -54,7 +54,7 @@ static void netdev_port_receive(struct vport *vport, struct sk_buff *skb)
 	skb_push(skb, ETH_HLEN);
 	ovs_skb_postpush_rcsum(skb, skb->data, ETH_HLEN);
 
-	ovs_vport_receive(vport, skb, skb_tunnel_info(skb, AF_INET));
+	ovs_vport_receive(vport, skb, skb_tunnel_info(skb));
 	return;
 
 error:
