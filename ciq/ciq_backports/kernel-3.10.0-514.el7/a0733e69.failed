perf: Remove __free_event()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit a0733e695b83a9c31f779e41dcaec8ef924716b5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a0733e69.failed

There is but a single caller, remove the function - we already have
_free_event(), the extra indirection is nonsensical..

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a0733e695b83a9c31f779e41dcaec8ef924716b5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/events/core.c
diff --cc kernel/events/core.c
index 5e52e23d5ae2,024adf0e34eb..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -3473,23 -3580,89 +3473,109 @@@ static void unaccount_event(struct perf
  	unaccount_event_cpu(event, event->cpu);
  }
  
++<<<<<<< HEAD
 +static void __free_event(struct perf_event *event)
 +{
 +	if (!event->parent) {
 +		if (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN)
 +			put_callchain_buffers();
 +	}
 +
 +	if (event->destroy)
 +		event->destroy(event);
 +
 +	if (event->ctx)
 +		put_ctx(event->ctx);
 +
 +	if (event->pmu)
 +		module_put(event->pmu->module);
 +
 +	call_rcu(&event->rcu_head, free_event_rcu);
++=======
+ /*
+  * The following implement mutual exclusion of events on "exclusive" pmus
+  * (PERF_PMU_CAP_EXCLUSIVE). Such pmus can only have one event scheduled
+  * at a time, so we disallow creating events that might conflict, namely:
+  *
+  *  1) cpu-wide events in the presence of per-task events,
+  *  2) per-task events in the presence of cpu-wide events,
+  *  3) two matching events on the same context.
+  *
+  * The former two cases are handled in the allocation path (perf_event_alloc(),
+  * _free_event()), the latter -- before the first perf_install_in_context().
+  */
+ static int exclusive_event_init(struct perf_event *event)
+ {
+ 	struct pmu *pmu = event->pmu;
+ 
+ 	if (!(pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE))
+ 		return 0;
+ 
+ 	/*
+ 	 * Prevent co-existence of per-task and cpu-wide events on the
+ 	 * same exclusive pmu.
+ 	 *
+ 	 * Negative pmu::exclusive_cnt means there are cpu-wide
+ 	 * events on this "exclusive" pmu, positive means there are
+ 	 * per-task events.
+ 	 *
+ 	 * Since this is called in perf_event_alloc() path, event::ctx
+ 	 * doesn't exist yet; it is, however, safe to use PERF_ATTACH_TASK
+ 	 * to mean "per-task event", because unlike other attach states it
+ 	 * never gets cleared.
+ 	 */
+ 	if (event->attach_state & PERF_ATTACH_TASK) {
+ 		if (!atomic_inc_unless_negative(&pmu->exclusive_cnt))
+ 			return -EBUSY;
+ 	} else {
+ 		if (!atomic_dec_unless_positive(&pmu->exclusive_cnt))
+ 			return -EBUSY;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void exclusive_event_destroy(struct perf_event *event)
+ {
+ 	struct pmu *pmu = event->pmu;
+ 
+ 	if (!(pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE))
+ 		return;
+ 
+ 	/* see comment in exclusive_event_init() */
+ 	if (event->attach_state & PERF_ATTACH_TASK)
+ 		atomic_dec(&pmu->exclusive_cnt);
+ 	else
+ 		atomic_inc(&pmu->exclusive_cnt);
+ }
+ 
+ static bool exclusive_event_match(struct perf_event *e1, struct perf_event *e2)
+ {
+ 	if ((e1->pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE) &&
+ 	    (e1->cpu == e2->cpu ||
+ 	     e1->cpu == -1 ||
+ 	     e2->cpu == -1))
+ 		return true;
+ 	return false;
+ }
+ 
+ /* Called under the same ctx::mutex as perf_install_in_context() */
+ static bool exclusive_event_installable(struct perf_event *event,
+ 					struct perf_event_context *ctx)
+ {
+ 	struct perf_event *iter_event;
+ 	struct pmu *pmu = event->pmu;
+ 
+ 	if (!(pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE))
+ 		return true;
+ 
+ 	list_for_each_entry(iter_event, &ctx->event_list, event_entry) {
+ 		if (exclusive_event_match(iter_event, event))
+ 			return false;
+ 	}
+ 
+ 	return true;
++>>>>>>> a0733e695b83 (perf: Remove __free_event())
  }
  
  static void _free_event(struct perf_event *event)
* Unmerged path kernel/events/core.c
