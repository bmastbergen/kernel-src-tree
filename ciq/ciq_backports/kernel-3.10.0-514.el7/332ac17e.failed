sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dario Faggioli <raistlin@linux.it>
commit 332ac17ef5bfcff4766dfdfd3b4cdf10b8f8f155
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/332ac17e.failed

In order of deadline scheduling to be effective and useful, it is
important that some method of having the allocation of the available
CPU bandwidth to tasks and task groups under control.
This is usually called "admission control" and if it is not performed
at all, no guarantee can be given on the actual scheduling of the
-deadline tasks.

Since when RT-throttling has been introduced each task group have a
bandwidth associated to itself, calculated as a certain amount of
runtime over a period. Moreover, to make it possible to manipulate
such bandwidth, readable/writable controls have been added to both
procfs (for system wide settings) and cgroupfs (for per-group
settings).

Therefore, the same interface is being used for controlling the
bandwidth distrubution to -deadline tasks and task groups, i.e.,
new controls but with similar names, equivalent meaning and with
the same usage paradigm are added.

However, more discussion is needed in order to figure out how
we want to manage SCHED_DEADLINE bandwidth at the task group level.
Therefore, this patch adds a less sophisticated, but actually
very sensible, mechanism to ensure that a certain utilization
cap is not overcome per each root_domain (the single rq for !SMP
configurations).

Another main difference between deadline bandwidth management and
RT-throttling is that -deadline tasks have bandwidth on their own
(while -rt ones doesn't!), and thus we don't need an higher level
throttling mechanism to enforce the desired bandwidth.

This patch, therefore:

 - adds system wide deadline bandwidth management by means of:
    * /proc/sys/kernel/sched_dl_runtime_us,
    * /proc/sys/kernel/sched_dl_period_us,
   that determine (i.e., runtime / period) the total bandwidth
   available on each CPU of each root_domain for -deadline tasks;

 - couples the RT and deadline bandwidth management, i.e., enforces
   that the sum of how much bandwidth is being devoted to -rt
   -deadline tasks to stay below 100%.

This means that, for a root_domain comprising M CPUs, -deadline tasks
can be created until the sum of their bandwidths stay below:

    M * (sched_dl_runtime_us / sched_dl_period_us)

It is also possible to disable this bandwidth management logic, and
be thus free of oversubscribing the system up to any arbitrary level.

	Signed-off-by: Dario Faggioli <raistlin@linux.it>
	Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1383831828-15501-12-git-send-email-juri.lelli@gmail.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 332ac17ef5bfcff4766dfdfd3b4cdf10b8f8f155)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched.h
#	include/linux/sched/sysctl.h
#	kernel/sched/core.c
#	kernel/sched/deadline.c
#	kernel/sched/sched.h
diff --cc include/linux/sched.h
index e57aba91f593,a196cb7fc6f2..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -1104,6 -1093,51 +1104,54 @@@ struct sched_rt_entity 
  #endif
  };
  
++<<<<<<< HEAD
++=======
+ struct sched_dl_entity {
+ 	struct rb_node	rb_node;
+ 
+ 	/*
+ 	 * Original scheduling parameters. Copied here from sched_attr
+ 	 * during sched_setscheduler2(), they will remain the same until
+ 	 * the next sched_setscheduler2().
+ 	 */
+ 	u64 dl_runtime;		/* maximum runtime for each instance	*/
+ 	u64 dl_deadline;	/* relative deadline of each instance	*/
+ 	u64 dl_period;		/* separation of two instances (period) */
+ 	u64 dl_bw;		/* dl_runtime / dl_deadline		*/
+ 
+ 	/*
+ 	 * Actual scheduling parameters. Initialized with the values above,
+ 	 * they are continously updated during task execution. Note that
+ 	 * the remaining runtime could be < 0 in case we are in overrun.
+ 	 */
+ 	s64 runtime;		/* remaining runtime for this instance	*/
+ 	u64 deadline;		/* absolute deadline for this instance	*/
+ 	unsigned int flags;	/* specifying the scheduler behaviour	*/
+ 
+ 	/*
+ 	 * Some bool flags:
+ 	 *
+ 	 * @dl_throttled tells if we exhausted the runtime. If so, the
+ 	 * task has to wait for a replenishment to be performed at the
+ 	 * next firing of dl_timer.
+ 	 *
+ 	 * @dl_new tells if a new instance arrived. If so we must
+ 	 * start executing it with full runtime and reset its absolute
+ 	 * deadline;
+ 	 *
+ 	 * @dl_boosted tells if we are boosted due to DI. If so we are
+ 	 * outside bandwidth enforcement mechanism (but only until we
+ 	 * exit the critical section).
+ 	 */
+ 	int dl_throttled, dl_new, dl_boosted;
+ 
+ 	/*
+ 	 * Bandwidth enforcement timer. Each -deadline task has its
+ 	 * own bandwidth to be enforced, thus we need one timer per task.
+ 	 */
+ 	struct hrtimer dl_timer;
+ };
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  
  struct rcu_node;
  
diff --cc include/linux/sched/sysctl.h
index 750624cd6f92,8070a83dbedc..000000000000
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@@ -100,8 -108,8 +109,14 @@@ extern int sched_rt_handler(struct ctl_
  		void __user *buffer, size_t *lenp,
  		loff_t *ppos);
  
++<<<<<<< HEAD
 +extern int sysctl_numa_balancing(struct ctl_table *table, int write,
 +				 void __user *buffer, size_t *lenp,
 +				 loff_t *ppos);
++=======
+ int sched_dl_handler(struct ctl_table *table, int write,
+ 		void __user *buffer, size_t *lenp,
+ 		loff_t *ppos);
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  
  #endif /* _SCHED_SYSCTL_H */
diff --cc kernel/sched/core.c
index f167fdc57a94,c7c68e6b5c51..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -1974,8 -1861,115 +1983,113 @@@ void sched_fork(unsigned long clone_fla
  #endif
  
  	put_cpu();
 -	return 0;
  }
  
+ unsigned long to_ratio(u64 period, u64 runtime)
+ {
+ 	if (runtime == RUNTIME_INF)
+ 		return 1ULL << 20;
+ 
+ 	/*
+ 	 * Doing this here saves a lot of checks in all
+ 	 * the calling paths, and returning zero seems
+ 	 * safe for them anyway.
+ 	 */
+ 	if (period == 0)
+ 		return 0;
+ 
+ 	return div64_u64(runtime << 20, period);
+ }
+ 
+ #ifdef CONFIG_SMP
+ inline struct dl_bw *dl_bw_of(int i)
+ {
+ 	return &cpu_rq(i)->rd->dl_bw;
+ }
+ 
+ static inline int __dl_span_weight(struct rq *rq)
+ {
+ 	return cpumask_weight(rq->rd->span);
+ }
+ #else
+ inline struct dl_bw *dl_bw_of(int i)
+ {
+ 	return &cpu_rq(i)->dl.dl_bw;
+ }
+ 
+ static inline int __dl_span_weight(struct rq *rq)
+ {
+ 	return 1;
+ }
+ #endif
+ 
+ static inline
+ void __dl_clear(struct dl_bw *dl_b, u64 tsk_bw)
+ {
+ 	dl_b->total_bw -= tsk_bw;
+ }
+ 
+ static inline
+ void __dl_add(struct dl_bw *dl_b, u64 tsk_bw)
+ {
+ 	dl_b->total_bw += tsk_bw;
+ }
+ 
+ static inline
+ bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
+ {
+ 	return dl_b->bw != -1 &&
+ 	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
+ }
+ 
+ /*
+  * We must be sure that accepting a new task (or allowing changing the
+  * parameters of an existing one) is consistent with the bandwidth
+  * constraints. If yes, this function also accordingly updates the currently
+  * allocated bandwidth to reflect the new situation.
+  *
+  * This function is called while holding p's rq->lock.
+  */
+ static int dl_overflow(struct task_struct *p, int policy,
+ 		       const struct sched_attr *attr)
+ {
+ 
+ 	struct dl_bw *dl_b = dl_bw_of(task_cpu(p));
+ 	u64 period = attr->sched_period;
+ 	u64 runtime = attr->sched_runtime;
+ 	u64 new_bw = dl_policy(policy) ? to_ratio(period, runtime) : 0;
+ 	int cpus = __dl_span_weight(task_rq(p));
+ 	int err = -1;
+ 
+ 	if (new_bw == p->dl.dl_bw)
+ 		return 0;
+ 
+ 	/*
+ 	 * Either if a task, enters, leave, or stays -deadline but changes
+ 	 * its parameters, we may need to update accordingly the total
+ 	 * allocated bandwidth of the container.
+ 	 */
+ 	raw_spin_lock(&dl_b->lock);
+ 	if (dl_policy(policy) && !task_has_dl_policy(p) &&
+ 	    !__dl_overflow(dl_b, cpus, 0, new_bw)) {
+ 		__dl_add(dl_b, new_bw);
+ 		err = 0;
+ 	} else if (dl_policy(policy) && task_has_dl_policy(p) &&
+ 		   !__dl_overflow(dl_b, cpus, p->dl.dl_bw, new_bw)) {
+ 		__dl_clear(dl_b, p->dl.dl_bw);
+ 		__dl_add(dl_b, new_bw);
+ 		err = 0;
+ 	} else if (!dl_policy(policy) && task_has_dl_policy(p)) {
+ 		__dl_clear(dl_b, p->dl.dl_bw);
+ 		err = 0;
+ 	}
+ 	raw_spin_unlock(&dl_b->lock);
+ 
+ 	return err;
+ }
+ 
+ extern void init_dl_bw(struct dl_bw *dl_b);
+ 
  /*
   * wake_up_new_task - wake up a newly created task for the first time.
   *
@@@ -4083,22 -3149,87 +4197,76 @@@ static struct task_struct *find_process
  	return pid ? find_task_by_vpid(pid) : current;
  }
  
 -/*
 - * This function initializes the sched_dl_entity of a newly becoming
 - * SCHED_DEADLINE task.
 - *
 - * Only the static values are considered here, the actual runtime and the
 - * absolute deadline will be properly calculated when the task is enqueued
 - * for the first time with its new policy.
 - */
 +/* Actually do priority change: must hold rq lock. */
  static void
 -__setparam_dl(struct task_struct *p, const struct sched_attr *attr)
 +__setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)
  {
++<<<<<<< HEAD
++=======
+ 	struct sched_dl_entity *dl_se = &p->dl;
+ 
+ 	init_dl_task_timer(dl_se);
+ 	dl_se->dl_runtime = attr->sched_runtime;
+ 	dl_se->dl_deadline = attr->sched_deadline;
+ 	dl_se->dl_period = attr->sched_period ?: dl_se->dl_deadline;
+ 	dl_se->flags = attr->sched_flags;
+ 	dl_se->dl_bw = to_ratio(dl_se->dl_period, dl_se->dl_runtime);
+ 	dl_se->dl_throttled = 0;
+ 	dl_se->dl_new = 1;
+ }
+ 
+ /* Actually do priority change: must hold pi & rq lock. */
+ static void __setscheduler(struct rq *rq, struct task_struct *p,
+ 			   const struct sched_attr *attr)
+ {
+ 	int policy = attr->sched_policy;
+ 
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  	p->policy = policy;
 -
 -	if (dl_policy(policy))
 -		__setparam_dl(p, attr);
 -	else if (rt_policy(policy))
 -		p->rt_priority = attr->sched_priority;
 -	else
 -		p->static_prio = NICE_TO_PRIO(attr->sched_nice);
 -
 +	p->rt_priority = prio;
  	p->normal_prio = normal_prio(p);
 +	/* we are holding p->pi_lock already */
  	p->prio = rt_mutex_getprio(p);
 -
 -	if (dl_prio(p->prio))
 -		p->sched_class = &dl_sched_class;
 -	else if (rt_prio(p->prio))
 +	if (rt_prio(p->prio))
  		p->sched_class = &rt_sched_class;
  	else
  		p->sched_class = &fair_sched_class;
  	set_load_weight(p);
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ __getparam_dl(struct task_struct *p, struct sched_attr *attr)
+ {
+ 	struct sched_dl_entity *dl_se = &p->dl;
+ 
+ 	attr->sched_priority = p->rt_priority;
+ 	attr->sched_runtime = dl_se->dl_runtime;
+ 	attr->sched_deadline = dl_se->dl_deadline;
+ 	attr->sched_period = dl_se->dl_period;
+ 	attr->sched_flags = dl_se->flags;
+ }
+ 
+ /*
+  * This function validates the new parameters of a -deadline task.
+  * We ask for the deadline not being zero, and greater or equal
+  * than the runtime, as well as the period of being zero or
+  * greater than deadline. Furthermore, we have to be sure that
+  * user parameters are above the internal resolution (1us); we
+  * check sched_runtime only since it is always the smaller one.
+  */
+ static bool
+ __checkparam_dl(const struct sched_attr *attr)
+ {
+ 	return attr && attr->sched_deadline != 0 &&
+ 		(attr->sched_period == 0 ||
+ 		(s64)(attr->sched_period   - attr->sched_deadline) >= 0) &&
+ 		(s64)(attr->sched_deadline - attr->sched_runtime ) >= 0  &&
+ 		attr->sched_runtime >= (2 << (DL_SCALE - 1));
+ }
+ 
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  /*
   * check the target process has a UID that matches the current process's
   */
@@@ -4220,9 -3366,10 +4388,9 @@@ recheck
  		task_rq_unlock(rq, p, &flags);
  		return 0;
  	}
 -change:
  
- #ifdef CONFIG_RT_GROUP_SCHED
  	if (user) {
+ #ifdef CONFIG_RT_GROUP_SCHED
  		/*
  		 * Do not allow realtime tasks into groups that have no runtime
  		 * assigned.
@@@ -5879,9 -5339,13 +6145,11 @@@ static int init_rootdomain(struct root_
  		goto out;
  	if (!alloc_cpumask_var(&rd->online, GFP_KERNEL))
  		goto free_span;
 -	if (!alloc_cpumask_var(&rd->dlo_mask, GFP_KERNEL))
 -		goto free_online;
  	if (!alloc_cpumask_var(&rd->rto_mask, GFP_KERNEL))
 -		goto free_dlo_mask;
 +		goto free_online;
  
+ 	init_dl_bw(&rd->dl_bw);
+ 
  	if (cpupri_init(&rd->cpupri) != 0)
  		goto free_rto_mask;
  	return 0;
@@@ -7317,14 -6767,19 +7585,19 @@@ void __init sched_init(void
  		ptr += nr_cpu_ids * sizeof(void **);
  
  #endif /* CONFIG_RT_GROUP_SCHED */
 +	}
  #ifdef CONFIG_CPUMASK_OFFSTACK
 -		for_each_possible_cpu(i) {
 -			per_cpu(load_balance_mask, i) = (void *)ptr;
 -			ptr += cpumask_size();
 -		}
 -#endif /* CONFIG_CPUMASK_OFFSTACK */
 +	for_each_possible_cpu(i) {
 +		per_cpu(load_balance_mask, i) = (cpumask_var_t)kzalloc_node(
 +			cpumask_size(), GFP_KERNEL, cpu_to_node(i));
  	}
 +#endif /* CONFIG_CPUMASK_OFFSTACK */
  
+ 	init_rt_bandwidth(&def_rt_bandwidth,
+ 			global_rt_period(), global_rt_runtime());
+ 	init_dl_bandwidth(&def_dl_bandwidth,
+ 			global_dl_period(), global_dl_runtime());
+ 
  #ifdef CONFIG_SMP
  	init_defrootdomain();
  #endif
diff --cc kernel/sched/sched.h
index b976abe32e72,ad4f4fbd002e..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -87,6 -109,25 +94,28 @@@ static inline int task_has_rt_policy(st
  	return rt_policy(p->policy);
  }
  
++<<<<<<< HEAD
++=======
+ static inline int task_has_dl_policy(struct task_struct *p)
+ {
+ 	return dl_policy(p->policy);
+ }
+ 
+ static inline bool dl_time_before(u64 a, u64 b)
+ {
+ 	return (s64)(a - b) < 0;
+ }
+ 
+ /*
+  * Tells if entity @a should preempt entity @b.
+  */
+ static inline bool
+ dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
+ {
+ 	return dl_time_before(a->deadline, b->deadline);
+ }
+ 
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  /*
   * This is the priority-queue data structure of the RT scheduling class:
   */
@@@ -361,6 -443,42 +434,45 @@@ struct rt_rq 
  #endif
  };
  
++<<<<<<< HEAD
++=======
+ /* Deadline class' related fields in a runqueue */
+ struct dl_rq {
+ 	/* runqueue is an rbtree, ordered by deadline */
+ 	struct rb_root rb_root;
+ 	struct rb_node *rb_leftmost;
+ 
+ 	unsigned long dl_nr_running;
+ 
+ #ifdef CONFIG_SMP
+ 	/*
+ 	 * Deadline values of the currently executing and the
+ 	 * earliest ready task on this rq. Caching these facilitates
+ 	 * the decision wether or not a ready but not running task
+ 	 * should migrate somewhere else.
+ 	 */
+ 	struct {
+ 		u64 curr;
+ 		u64 next;
+ 	} earliest_dl;
+ 
+ 	unsigned long dl_nr_migratory;
+ 	unsigned long dl_nr_total;
+ 	int overloaded;
+ 
+ 	/*
+ 	 * Tasks on this rq that can be pushed away. They are kept in
+ 	 * an rb-tree, ordered by tasks' deadlines, with caching
+ 	 * of the leftmost (earliest deadline) element.
+ 	 */
+ 	struct rb_root pushable_dl_tasks_root;
+ 	struct rb_node *pushable_dl_tasks_leftmost;
+ #else
+ 	struct dl_bw dl_bw;
+ #endif
+ };
+ 
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  #ifdef CONFIG_SMP
  
  /*
@@@ -379,6 -497,14 +491,17 @@@ struct root_domain 
  	cpumask_var_t online;
  
  	/*
++<<<<<<< HEAD
++=======
+ 	 * The bit corresponding to a CPU gets set here if such CPU has more
+ 	 * than one runnable -deadline task (as it is below for RT tasks).
+ 	 */
+ 	cpumask_var_t dlo_mask;
+ 	atomic_t dlo_count;
+ 	struct dl_bw dl_bw;
+ 
+ 	/*
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  	 * The "RT overload" flag: it gets set if a CPU has more than
  	 * one runnable RT task.
  	 */
@@@ -1103,8 -1206,11 +1237,9 @@@ static inline void idle_balance(int cpu
  extern void sysrq_sched_debug_show(void);
  extern void sched_init_granularity(void);
  extern void update_max_interval(void);
 -
 -extern void init_sched_dl_class(void);
  extern void init_sched_rt_class(void);
  extern void init_sched_fair_class(void);
+ extern void init_sched_dl_class(void);
  
  extern void resched_task(struct task_struct *p);
  extern void resched_cpu(int cpu);
@@@ -1112,8 -1218,16 +1247,17 @@@
  extern struct rt_bandwidth def_rt_bandwidth;
  extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
  
++<<<<<<< HEAD
++=======
+ extern struct dl_bandwidth def_dl_bandwidth;
+ extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
+ extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
+ 
+ unsigned long to_ratio(u64 period, u64 runtime);
+ 
++>>>>>>> 332ac17ef5bf (sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks)
  extern void update_idle_cpu_load(struct rq *this_rq);
  
 -extern void init_task_runnable_average(struct task_struct *p);
 -
  #ifdef CONFIG_PARAVIRT
  static inline u64 steal_ticks(u64 steal)
  {
* Unmerged path kernel/sched/deadline.c
* Unmerged path include/linux/sched.h
* Unmerged path include/linux/sched/sysctl.h
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/deadline.c
* Unmerged path kernel/sched/sched.h
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 650fd94446e0..1dd9c1ea700e 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -423,6 +423,20 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= sched_rr_handler,
 	},
+	{
+		.procname	= "sched_dl_period_us",
+		.data		= &sysctl_sched_dl_period,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= sched_dl_handler,
+	},
+	{
+		.procname	= "sched_dl_runtime_us",
+		.data		= &sysctl_sched_dl_runtime,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= sched_dl_handler,
+	},
 #ifdef CONFIG_SCHED_AUTOGROUP
 	{
 		.procname	= "sched_autogroup_enabled",
