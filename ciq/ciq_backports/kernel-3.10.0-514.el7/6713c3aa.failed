sched: Remove superfluous resetting of the p->dl_throttled flag

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Wanpeng Li <wanpeng.li@linux.intel.com>
commit 6713c3aa7f63626c0cecf9c509fb48d885b2dd12
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/6713c3aa.failed

Resetting the p->dl_throttled flag in rt_mutex_setprio() (for a task that is going
to be boosted) is superfluous, as the natural place to do so is in
replenish_dl_entity().

If the task was on the runqueue and it is boosted by a DL task, it will be enqueued
back with ENQUEUE_REPLENISH flag set, which can guarantee that dl_throttled is
reset in replenish_dl_entity().

This patch drops the resetting of throttled status in function rt_mutex_setprio().

	Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Juri Lelli <juri.lelli@arm.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/1431496867-4194-6-git-send-email-wanpeng.li@linux.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 6713c3aa7f63626c0cecf9c509fb48d885b2dd12)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 04c5c65570ca,10338ce78be4..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -3885,17 -3078,44 +3885,43 @@@ void rt_mutex_setprio(struct task_struc
  	trace_sched_pi_setprio(p, prio);
  	oldprio = p->prio;
  	prev_class = p->sched_class;
 -	queued = task_on_rq_queued(p);
 +	on_rq = p->on_rq;
  	running = task_current(rq, p);
 -	if (queued)
 +	if (on_rq)
  		dequeue_task(rq, p, 0);
  	if (running)
 -		put_prev_task(rq, p);
 +		p->sched_class->put_prev_task(rq, p);
  
++<<<<<<< HEAD
 +	if (rt_prio(prio))
++=======
+ 	/*
+ 	 * Boosting condition are:
+ 	 * 1. -rt task is running and holds mutex A
+ 	 *      --> -dl task blocks on mutex A
+ 	 *
+ 	 * 2. -dl task is running and holds mutex A
+ 	 *      --> -dl task blocks on mutex A and could preempt the
+ 	 *          running task
+ 	 */
+ 	if (dl_prio(prio)) {
+ 		struct task_struct *pi_task = rt_mutex_get_top_task(p);
+ 		if (!dl_prio(p->normal_prio) ||
+ 		    (pi_task && dl_entity_preempt(&pi_task->dl, &p->dl))) {
+ 			p->dl.dl_boosted = 1;
+ 			enqueue_flag = ENQUEUE_REPLENISH;
+ 		} else
+ 			p->dl.dl_boosted = 0;
+ 		p->sched_class = &dl_sched_class;
+ 	} else if (rt_prio(prio)) {
+ 		if (dl_prio(oldprio))
+ 			p->dl.dl_boosted = 0;
+ 		if (oldprio < prio)
+ 			enqueue_flag = ENQUEUE_HEAD;
++>>>>>>> 6713c3aa7f63 (sched: Remove superfluous resetting of the p->dl_throttled flag)
  		p->sched_class = &rt_sched_class;
 -	} else {
 -		if (dl_prio(oldprio))
 -			p->dl.dl_boosted = 0;
 -		if (rt_prio(oldprio))
 -			p->rt.timeout = 0;
 +	else
  		p->sched_class = &fair_sched_class;
 -	}
  
  	p->prio = prio;
  
* Unmerged path kernel/sched/core.c
