crypto: x86/sha - glue code for Intel SHA extensions optimized SHA1 & SHA256

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [crypto] x86/sha - glue code for Intel SHA extensions optimized SHA1 & SHA256 (Herbert Xu) [1267049]
Rebuild_FUZZ: 94.44%
commit-author tim <tim.c.chen@linux.intel.com>
commit 95fca7df0b4964fbe3fe159e3d6e681e6b5b7a53
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/95fca7df.failed

This patch adds the glue code to detect and utilize the Intel SHA
extensions optimized SHA1 and SHA256 update transforms when available.

This code has been tested on Broxton for functionality.

Originally-by: Chandramouli Narayanan <mouli_7982@yahoo.com>
	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Acked-by: David S. Miller <davem@davemloft.net>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 95fca7df0b4964fbe3fe159e3d6e681e6b5b7a53)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/crypto/sha256_ssse3_glue.c
diff --cc arch/x86/crypto/sha256_ssse3_glue.c
index 38d835b71fbb,9c7b22c489f6..000000000000
--- a/arch/x86/crypto/sha256_ssse3_glue.c
+++ b/arch/x86/crypto/sha256_ssse3_glue.c
@@@ -49,14 -47,18 +49,18 @@@ asmlinkage void sha256_transform_avx(co
  				     u64 rounds);
  #endif
  #ifdef CONFIG_AS_AVX2
 -asmlinkage void sha256_transform_rorx(u32 *digest, const char *data,
 -				      u64 rounds);
 +asmlinkage void sha256_transform_rorx(const char *data, u32 *digest,
 +				     u64 rounds);
  #endif
+ #ifdef CONFIG_AS_SHA256_NI
+ asmlinkage void sha256_ni_transform(u32 *digest, const char *data,
+ 				   u64 rounds); /*unsigned int rounds);*/
+ #endif
  
 -static void (*sha256_transform_asm)(u32 *, const char *, u64);
 +static asmlinkage void (*sha256_transform_asm)(const char *, u32 *, u64);
  
 -static int sha256_ssse3_update(struct shash_desc *desc, const u8 *data,
 -			     unsigned int len)
 +
 +static int sha256_ssse3_init(struct shash_desc *desc)
  {
  	struct sha256_state *sctx = shash_desc_ctx(desc);
  
@@@ -227,9 -146,13 +231,18 @@@ static bool __init avx_usable(void
  
  static int __init sha256_ssse3_mod_init(void)
  {
++<<<<<<< HEAD
 +	/* test for SSE3 first */
 +	if (cpu_has_ssse3)
++=======
+ 	char *algo;
+ 
+ 	/* test for SSSE3 first */
+ 	if (cpu_has_ssse3) {
++>>>>>>> 95fca7df0b49 (crypto: x86/sha - glue code for Intel SHA extensions optimized SHA1 & SHA256)
  		sha256_transform_asm = sha256_transform_ssse3;
+ 		algo = "SSSE3";
+ 	}
  
  #ifdef CONFIG_AS_AVX
  	/* allow AVX to override SSSE3, it's a little faster */
@@@ -244,19 -176,10 +266,24 @@@
  #endif
  
  	if (sha256_transform_asm) {
++<<<<<<< HEAD
 +#ifdef CONFIG_AS_AVX
 +		if (sha256_transform_asm == sha256_transform_avx)
 +			pr_info("Using AVX optimized SHA-256 implementation\n");
 +#ifdef CONFIG_AS_AVX2
 +		else if (sha256_transform_asm == sha256_transform_rorx)
 +			pr_info("Using AVX2 optimized SHA-256 implementation\n");
 +#endif
 +		else
 +#endif
 +			pr_info("Using SSSE3 optimized SHA-256 implementation\n");
 +		return crypto_register_shash(&alg);
++=======
+ 		pr_info("Using %s optimized SHA-256 implementation\n", algo);
+ 		return crypto_register_shashes(algs, ARRAY_SIZE(algs));
++>>>>>>> 95fca7df0b49 (crypto: x86/sha - glue code for Intel SHA extensions optimized SHA1 & SHA256)
  	}
- 	pr_info("Neither AVX nor SSSE3 is available/usable.\n");
+ 	pr_info("Neither AVX nor SSSE3/SHA-NI is available/usable.\n");
  
  	return -ENODEV;
  }
diff --git a/arch/x86/crypto/sha1_ssse3_glue.c b/arch/x86/crypto/sha1_ssse3_glue.c
index 33d1b9dc14cc..6a70f9407a1b 100644
--- a/arch/x86/crypto/sha1_ssse3_glue.c
+++ b/arch/x86/crypto/sha1_ssse3_glue.c
@@ -46,6 +46,10 @@ asmlinkage void sha1_transform_avx(u32 *digest, const char *data,
 asmlinkage void sha1_transform_avx2(u32 *digest, const char *data,
 				    unsigned int rounds);
 #endif
+#ifdef CONFIG_AS_SHA1_NI
+asmlinkage void sha1_ni_transform(u32 *digest, const char *data,
+				   unsigned int rounds);
+#endif
 
 static void (*sha1_transform_asm)(u32 *, const char *, unsigned int);
 
@@ -174,12 +178,18 @@ static int __init sha1_ssse3_mod_init(void)
 #endif
 	}
 #endif
+#ifdef CONFIG_AS_SHA1_NI
+	if (boot_cpu_has(X86_FEATURE_SHA_NI)) {
+		sha1_transform_asm = sha1_ni_transform;
+		algo_name = "SHA-NI";
+	}
+#endif
 
 	if (sha1_transform_asm) {
 		pr_info("Using %s optimized SHA-1 implementation\n", algo_name);
 		return crypto_register_shash(&alg);
 	}
-	pr_info("Neither AVX nor AVX2 nor SSSE3 is available/usable.\n");
+	pr_info("Neither AVX nor AVX2 nor SSSE3/SHA-NI is available/usable.\n");
 
 	return -ENODEV;
 }
* Unmerged path arch/x86/crypto/sha256_ssse3_glue.c
