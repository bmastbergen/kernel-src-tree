IB/iser: set block queue_virt_boundary

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Sagi Grimberg <sagig@mellanox.com>
commit dd0107a08996c0ab8cac2b98ddbed5313e118e81
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/dd0107a0.failed

The block layer can reliably guarantee that SG lists won't
contain gaps (page unaligned) if a driver set the queue
virt_boundary.

With this setting the block layer will:
- refuse merges if bios are not aligned to the virtual boundary
- split bios/requests that are not aligned to the virtual boundary
- or, bounce buffer SG_IOs that are not aligned to the virtual boundary

Since iser is working in 4K page size, set the virt_boundary to
4K pages. With this setting, we can now safely remove the bounce
buffering logic in iser.

	Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit dd0107a08996c0ab8cac2b98ddbed5313e118e81)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/ulp/iser/iser_initiator.c
#	drivers/infiniband/ulp/iser/iser_memory.c
diff --cc drivers/infiniband/ulp/iser/iser_initiator.c
index 8eb39f9ffbf7,ffd00c420729..000000000000
--- a/drivers/infiniband/ulp/iser/iser_initiator.c
+++ b/drivers/infiniband/ulp/iser/iser_initiator.c
@@@ -663,61 -661,25 +663,49 @@@ void iser_task_rdma_init(struct iscsi_i
  
  void iser_task_rdma_finalize(struct iscsi_iser_task *iser_task)
  {
++<<<<<<< HEAD
 +	struct iser_device *device = iser_task->iser_conn->ib_conn.device;
 +	int is_rdma_data_aligned = 1;
 +	int is_rdma_prot_aligned = 1;
++=======
++>>>>>>> dd0107a08996 (IB/iser: set block queue_virt_boundary)
  	int prot_count = scsi_prot_sg_count(iser_task->sc);
  
- 	/* if we were reading, copy back to unaligned sglist,
- 	 * anyway dma_unmap and free the copy
- 	 */
- 	if (iser_task->data[ISER_DIR_IN].orig_sg) {
- 		is_rdma_data_aligned = 0;
- 		iser_finalize_rdma_unaligned_sg(iser_task,
- 						&iser_task->data[ISER_DIR_IN],
- 						ISER_DIR_IN);
- 	}
- 
- 	if (iser_task->data[ISER_DIR_OUT].orig_sg) {
- 		is_rdma_data_aligned = 0;
- 		iser_finalize_rdma_unaligned_sg(iser_task,
- 						&iser_task->data[ISER_DIR_OUT],
- 						ISER_DIR_OUT);
- 	}
- 
- 	if (iser_task->prot[ISER_DIR_IN].orig_sg) {
- 		is_rdma_prot_aligned = 0;
- 		iser_finalize_rdma_unaligned_sg(iser_task,
- 						&iser_task->prot[ISER_DIR_IN],
- 						ISER_DIR_IN);
- 	}
- 
- 	if (iser_task->prot[ISER_DIR_OUT].orig_sg) {
- 		is_rdma_prot_aligned = 0;
- 		iser_finalize_rdma_unaligned_sg(iser_task,
- 						&iser_task->prot[ISER_DIR_OUT],
- 						ISER_DIR_OUT);
- 	}
- 
  	if (iser_task->dir[ISER_DIR_IN]) {
++<<<<<<< HEAD
 +		device->reg_ops->unreg_rdma_mem(iser_task, ISER_DIR_IN);
 +		if (is_rdma_data_aligned)
 +			iser_dma_unmap_task_data(iser_task,
 +						 &iser_task->data[ISER_DIR_IN],
 +						 DMA_FROM_DEVICE);
 +		if (prot_count && is_rdma_prot_aligned)
++=======
+ 		iser_unreg_rdma_mem(iser_task, ISER_DIR_IN);
+ 		iser_dma_unmap_task_data(iser_task,
+ 					 &iser_task->data[ISER_DIR_IN],
+ 					 DMA_FROM_DEVICE);
+ 		if (prot_count)
++>>>>>>> dd0107a08996 (IB/iser: set block queue_virt_boundary)
  			iser_dma_unmap_task_data(iser_task,
  						 &iser_task->prot[ISER_DIR_IN],
  						 DMA_FROM_DEVICE);
  	}
  
  	if (iser_task->dir[ISER_DIR_OUT]) {
++<<<<<<< HEAD
 +		device->reg_ops->unreg_rdma_mem(iser_task, ISER_DIR_OUT);
 +		if (is_rdma_data_aligned)
 +			iser_dma_unmap_task_data(iser_task,
 +						 &iser_task->data[ISER_DIR_OUT],
 +						 DMA_TO_DEVICE);
 +		if (prot_count && is_rdma_prot_aligned)
++=======
+ 		iser_unreg_rdma_mem(iser_task, ISER_DIR_OUT);
+ 		iser_dma_unmap_task_data(iser_task,
+ 					 &iser_task->data[ISER_DIR_OUT],
+ 					 DMA_TO_DEVICE);
+ 		if (prot_count)
++>>>>>>> dd0107a08996 (IB/iser: set block queue_virt_boundary)
  			iser_dma_unmap_task_data(iser_task,
  						 &iser_task->prot[ISER_DIR_OUT],
  						 DMA_TO_DEVICE);
diff --cc drivers/infiniband/ulp/iser/iser_memory.c
index b04a5e59e448,3e0452c4248f..000000000000
--- a/drivers/infiniband/ulp/iser/iser_memory.c
+++ b/drivers/infiniband/ulp/iser/iser_memory.c
@@@ -345,55 -192,6 +182,58 @@@ static int iser_sg_to_page_vec(struct i
  	return cur_page;
  }
  
++<<<<<<< HEAD
 +
 +/**
 + * iser_data_buf_aligned_len - Tries to determine the maximal correctly aligned
 + * for RDMA sub-list of a scatter-gather list of memory buffers, and  returns
 + * the number of entries which are aligned correctly. Supports the case where
 + * consecutive SG elements are actually fragments of the same physcial page.
 + */
 +static int iser_data_buf_aligned_len(struct iser_data_buf *data,
 +				      struct ib_device *ibdev)
 +{
 +	struct scatterlist *sg, *sgl, *next_sg = NULL;
 +	u64 start_addr, end_addr;
 +	int i, ret_len, start_check = 0;
 +
 +	if (data->dma_nents == 1)
 +		return 1;
 +
 +	sgl = data->sg;
 +	start_addr  = ib_sg_dma_address(ibdev, sgl);
 +
 +	for_each_sg(sgl, sg, data->dma_nents, i) {
 +		if (start_check && !IS_4K_ALIGNED(start_addr))
 +			break;
 +
 +		next_sg = sg_next(sg);
 +		if (!next_sg)
 +			break;
 +
 +		end_addr    = start_addr + ib_sg_dma_len(ibdev, sg);
 +		start_addr  = ib_sg_dma_address(ibdev, next_sg);
 +
 +		if (end_addr == start_addr) {
 +			start_check = 0;
 +			continue;
 +		} else
 +			start_check = 1;
 +
 +		if (!IS_4K_ALIGNED(end_addr))
 +			break;
 +	}
 +	ret_len = (next_sg) ? i : i+1;
 +
 +	if (unlikely(ret_len != data->dma_nents))
 +		iser_warn("rdma alignment violation (%d/%d aligned)\n",
 +			  ret_len, data->dma_nents);
 +
 +	return ret_len;
 +}
 +
++=======
++>>>>>>> dd0107a08996 (IB/iser: set block queue_virt_boundary)
  static void iser_data_buf_dump(struct iser_data_buf *data,
  			       struct ib_device *ibdev)
  {
@@@ -865,65 -530,76 +680,122 @@@ static int iser_fast_reg_mr(struct iscs
  		 " length=0x%x\n", reg->sge.lkey, reg->rkey,
  		 reg->sge.addr, reg->sge.length);
  
 -	return 0;
 +	return ret;
  }
  
++<<<<<<< HEAD
 +/**
 + * iser_reg_rdma_mem_fastreg - Registers memory intended for RDMA,
 + * using Fast Registration WR (if possible) obtaining rkey and va
 + *
 + * returns 0 on success, errno code on failure
 + */
 +int iser_reg_rdma_mem_fastreg(struct iscsi_iser_task *iser_task,
 +			      enum iser_data_dir cmd_dir)
 +{
 +	struct ib_conn *ib_conn = &iser_task->iser_conn->ib_conn;
 +	struct iser_device *device = ib_conn->device;
 +	struct ib_device *ibdev = device->ib_device;
 +	struct iser_data_buf *mem = &iser_task->data[cmd_dir];
 +	struct iser_mem_reg *mem_reg = &iser_task->rdma_reg[cmd_dir];
 +	struct iser_fr_desc *desc = NULL;
 +	int err, aligned_len;
 +
 +	aligned_len = iser_data_buf_aligned_len(mem, ibdev);
 +	if (aligned_len != mem->dma_nents) {
 +		err = fall_to_bounce_buf(iser_task, mem, cmd_dir);
 +		if (err) {
 +			iser_err("failed to allocate bounce buffer\n");
 +			return err;
 +		}
 +	}
 +
 +	if (mem->dma_nents != 1 ||
 +	    scsi_get_prot_op(iser_task->sc) != SCSI_PROT_NORMAL) {
++=======
+ static int
+ iser_reg_prot_sg(struct iscsi_iser_task *task,
+ 		 struct iser_data_buf *mem,
+ 		 struct iser_fr_desc *desc,
+ 		 bool use_dma_key,
+ 		 struct iser_mem_reg *reg)
+ {
+ 	struct iser_device *device = task->iser_conn->ib_conn.device;
+ 
+ 	if (use_dma_key)
+ 		return iser_reg_dma(device, mem, reg);
+ 
+ 	return device->reg_ops->reg_mem(task, mem, &desc->pi_ctx->rsc, reg);
+ }
+ 
+ static int
+ iser_reg_data_sg(struct iscsi_iser_task *task,
+ 		 struct iser_data_buf *mem,
+ 		 struct iser_fr_desc *desc,
+ 		 bool use_dma_key,
+ 		 struct iser_mem_reg *reg)
+ {
+ 	struct iser_device *device = task->iser_conn->ib_conn.device;
+ 
+ 	if (use_dma_key)
+ 		return iser_reg_dma(device, mem, reg);
+ 
+ 	return device->reg_ops->reg_mem(task, mem, &desc->rsc, reg);
+ }
+ 
+ int iser_reg_rdma_mem(struct iscsi_iser_task *task,
+ 		      enum iser_data_dir dir)
+ {
+ 	struct ib_conn *ib_conn = &task->iser_conn->ib_conn;
+ 	struct iser_device *device = ib_conn->device;
+ 	struct iser_data_buf *mem = &task->data[dir];
+ 	struct iser_mem_reg *reg = &task->rdma_reg[dir];
+ 	struct iser_mem_reg *data_reg;
+ 	struct iser_fr_desc *desc = NULL;
+ 	bool use_dma_key;
+ 	int err;
+ 
+ 	use_dma_key = (mem->dma_nents == 1 && !iser_always_reg &&
+ 		       scsi_get_prot_op(task->sc) == SCSI_PROT_NORMAL);
+ 
+ 	if (!use_dma_key) {
++>>>>>>> dd0107a08996 (IB/iser: set block queue_virt_boundary)
  		desc = device->reg_ops->reg_desc_get(ib_conn);
 -		reg->mem_h = desc;
 +		mem_reg->mem_h = desc;
  	}
  
 -	if (scsi_get_prot_op(task->sc) == SCSI_PROT_NORMAL)
 -		data_reg = reg;
 -	else
 -		data_reg = &task->desc.data_reg;
 -
 -	err = iser_reg_data_sg(task, mem, desc, use_dma_key, data_reg);
 -	if (unlikely(err))
 +	err = iser_fast_reg_mr(iser_task, mem,
 +			       desc ? &desc->rsc : NULL, mem_reg);
 +	if (err)
  		goto err_reg;
  
 -	if (scsi_get_prot_op(task->sc) != SCSI_PROT_NORMAL) {
 -		struct iser_mem_reg *prot_reg = &task->desc.prot_reg;
 -
 +	if (scsi_get_prot_op(iser_task->sc) != SCSI_PROT_NORMAL) {
 +		struct iser_mem_reg prot_reg;
 +
++<<<<<<< HEAD
 +		memset(&prot_reg, 0, sizeof(prot_reg));
 +		if (scsi_prot_sg_count(iser_task->sc)) {
 +			mem = &iser_task->prot[cmd_dir];
 +			aligned_len = iser_data_buf_aligned_len(mem, ibdev);
 +			if (aligned_len != mem->dma_nents) {
 +				err = fall_to_bounce_buf(iser_task, mem,
 +							 cmd_dir);
 +				if (err) {
 +					iser_err("failed to allocate bounce buffer\n");
 +					return err;
 +				}
 +			}
 +
 +			err = iser_fast_reg_mr(iser_task, mem,
 +					       &desc->pi_ctx->rsc, &prot_reg);
 +			if (err)
++=======
+ 		if (scsi_prot_sg_count(task->sc)) {
+ 			mem = &task->prot[dir];
+ 			err = iser_reg_prot_sg(task, mem, desc,
+ 					       use_dma_key, prot_reg);
+ 			if (unlikely(err))
++>>>>>>> dd0107a08996 (IB/iser: set block queue_virt_boundary)
  				goto err_reg;
  		}
  
diff --git a/drivers/infiniband/ulp/iser/iscsi_iser.c b/drivers/infiniband/ulp/iser/iscsi_iser.c
index 7276679eedca..43e3bef12980 100644
--- a/drivers/infiniband/ulp/iser/iscsi_iser.c
+++ b/drivers/infiniband/ulp/iser/iscsi_iser.c
@@ -752,9 +752,7 @@ iscsi_iser_conn_get_stats(struct iscsi_cls_conn *cls_conn, struct iscsi_stats *s
 	stats->r2t_pdus = conn->r2t_pdus_cnt; /* always 0 */
 	stats->tmfcmd_pdus = conn->tmfcmd_pdus_cnt;
 	stats->tmfrsp_pdus = conn->tmfrsp_pdus_cnt;
-	stats->custom_length = 1;
-	strcpy(stats->custom[0].desc, "fmr_unalign_cnt");
-	stats->custom[0].value = conn->fmr_unalign_cnt;
+	stats->custom_length = 0;
 }
 
 static int iscsi_iser_get_ep_param(struct iscsi_endpoint *ep,
@@ -959,6 +957,13 @@ static umode_t iser_attr_is_visible(int param_type, int param)
 	return 0;
 }
 
+static int iscsi_iser_slave_alloc(struct scsi_device *sdev)
+{
+	blk_queue_virt_boundary(sdev->request_queue, ~MASK_4K);
+
+	return 0;
+}
+
 static struct scsi_host_template iscsi_iser_sht = {
 	.module                 = THIS_MODULE,
 	.name                   = "iSCSI Initiator over iSER",
@@ -972,6 +977,7 @@ static struct scsi_host_template iscsi_iser_sht = {
 	.eh_target_reset_handler = iscsi_eh_recover_target,
 	.target_alloc		= iscsi_target_alloc,
 	.use_clustering         = DISABLE_CLUSTERING,
+	.slave_alloc            = iscsi_iser_slave_alloc,
 	.proc_name              = "iscsi_iser",
 	.this_id                = -1,
 };
diff --git a/drivers/infiniband/ulp/iser/iscsi_iser.h b/drivers/infiniband/ulp/iser/iscsi_iser.h
index b9dc02d4dd14..f1c015987756 100644
--- a/drivers/infiniband/ulp/iser/iscsi_iser.h
+++ b/drivers/infiniband/ulp/iser/iscsi_iser.h
@@ -222,18 +222,13 @@ enum iser_data_dir {
  * @size:         num entries of this sg
  * @data_len:     total beffer byte len
  * @dma_nents:    returned by dma_map_sg
- * @orig_sg:      pointer to the original sg list (in case
- *                we used a copy)
- * @orig_size:    num entris of orig sg list
  */
 struct iser_data_buf {
 	struct scatterlist *sg;
 	unsigned int       size;
 	unsigned long      data_len;
 	unsigned int       dma_nents;
-	struct scatterlist *orig_sg;
-	unsigned int       orig_size;
-  };
+};
 
 /* fwd declarations */
 struct iser_device;
* Unmerged path drivers/infiniband/ulp/iser/iser_initiator.c
* Unmerged path drivers/infiniband/ulp/iser/iser_memory.c
