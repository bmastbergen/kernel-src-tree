IB/hfi1: Add SDMA cache eviction algorithm

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mitko Haralanov <mitko.haralanov@intel.com>
commit 5511d7810752f426f0a9f999100fd249d352c2ef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5511d781.failed

This commit adds a cache eviction algorithm for the SDMA
user buffer cache.

Besides the interval RB tree used for node lookup, the cache
nodes are also arranged in a doubly-linked list. When a node is
used, it is put at the beginning of the list. Less frequently
used nodes naturally move to the tail of the list.

When the cache limit is reached, the eviction code starts
traversing the linked list in reverse, freeing buffers until
enough space has been freed to fit the new user buffer. This
guarantees that only the least used cache nodes will be removed
from the cache.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Reviewed-by: Dean Luick <dean.luick@intel.com>
	Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
	Signed-off-by: Jubin John <jubin.john@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 5511d7810752f426f0a9f999100fd249d352c2ef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/user_sdma.c
#	drivers/staging/hfi1/user_sdma.h
diff --cc drivers/staging/hfi1/user_sdma.c
index 6967deb7956a,46e254d52dad..000000000000
--- a/drivers/staging/hfi1/user_sdma.c
+++ b/drivers/staging/hfi1/user_sdma.c
@@@ -184,6 -181,15 +184,18 @@@ struct user_sdma_iovec 
  	u64 offset;
  };
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ struct sdma_mmu_node {
+ 	struct mmu_rb_node rb;
+ 	struct list_head list;
+ 	struct hfi1_user_sdma_pkt_q *pq;
+ 	atomic_t refcount;
+ 	struct page **pages;
+ 	unsigned npages;
+ };
+ 
++>>>>>>> 5511d7810752 (IB/hfi1: Add SDMA cache eviction algorithm):drivers/staging/rdma/hfi1/user_sdma.c
  struct user_sdma_request {
  	struct sdma_req_info info;
  	struct hfi1_user_sdma_pkt_q *pq;
@@@ -386,9 -398,12 +398,15 @@@ int hfi1_user_sdma_alloc_queues(struct 
  	pq->state = SDMA_PKT_Q_INACTIVE;
  	atomic_set(&pq->n_reqs, 0);
  	init_waitqueue_head(&pq->wait);
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 	pq->sdma_rb_root = RB_ROOT;
+ 	INIT_LIST_HEAD(&pq->evict);
+ 	spin_lock_init(&pq->evict_lock);
++>>>>>>> 5511d7810752 (IB/hfi1: Add SDMA cache eviction algorithm):drivers/staging/rdma/hfi1/user_sdma.c
  
  	iowait_init(&pq->busy, 0, NULL, defer_packet_queue,
 -		    activate_packet_queue, NULL);
 +		    activate_packet_queue);
  	pq->reqidx = 0;
  	snprintf(buf, 64, "txreq-kmem-cache-%u-%u-%u", dd->unit, uctxt->ctxt,
  		 fd->subctxt);
@@@ -1036,40 -1031,129 +1054,150 @@@ static inline int num_user_pages(const 
  	return 1 + ((epage - spage) >> PAGE_SHIFT);
  }
  
+ /* Caller must hold pq->evict_lock */
+ static u32 sdma_cache_evict(struct hfi1_user_sdma_pkt_q *pq, u32 npages)
+ {
+ 	u32 cleared = 0;
+ 	struct sdma_mmu_node *node, *ptr;
+ 
+ 	list_for_each_entry_safe_reverse(node, ptr, &pq->evict, list) {
+ 		/* Make sure that no one is still using the node. */
+ 		if (!atomic_read(&node->refcount)) {
+ 			/*
+ 			 * Need to use the page count now as the remove callback
+ 			 * will free the node.
+ 			 */
+ 			cleared += node->npages;
+ 			spin_unlock(&pq->evict_lock);
+ 			hfi1_mmu_rb_remove(&pq->sdma_rb_root, &node->rb);
+ 			spin_lock(&pq->evict_lock);
+ 			if (cleared >= npages)
+ 				break;
+ 		}
+ 	}
+ 	return cleared;
+ }
+ 
  static int pin_vector_pages(struct user_sdma_request *req,
  			    struct user_sdma_iovec *iovec) {
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +	int pinned, npages;
 +
 +	npages = num_user_pages(&iovec->iov);
 +	iovec->pages = kcalloc(npages, sizeof(*iovec->pages), GFP_KERNEL);
 +	if (!iovec->pages) {
 +		SDMA_DBG(req, "Failed page array alloc");
 +		return -ENOMEM;
++=======
+ 	int ret = 0, pinned, npages, cleared;
+ 	struct page **pages;
+ 	struct hfi1_user_sdma_pkt_q *pq = req->pq;
+ 	struct sdma_mmu_node *node = NULL;
+ 	struct mmu_rb_node *rb_node;
+ 
+ 	rb_node = hfi1_mmu_rb_search(&pq->sdma_rb_root,
+ 				     (unsigned long)iovec->iov.iov_base,
+ 				     iovec->iov.iov_len);
+ 	if (rb_node)
+ 		node = container_of(rb_node, struct sdma_mmu_node, rb);
+ 
+ 	if (!node) {
+ 		node = kzalloc(sizeof(*node), GFP_KERNEL);
+ 		if (!node)
+ 			return -ENOMEM;
+ 
+ 		node->rb.addr = (unsigned long)iovec->iov.iov_base;
+ 		node->rb.len = iovec->iov.iov_len;
+ 		node->pq = pq;
+ 		atomic_set(&node->refcount, 0);
+ 		INIT_LIST_HEAD(&node->list);
+ 	}
+ 
+ 	npages = num_user_pages(&iovec->iov);
+ 	if (node->npages < npages) {
+ 		pages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);
+ 		if (!pages) {
+ 			SDMA_DBG(req, "Failed page array alloc");
+ 			ret = -ENOMEM;
+ 			goto bail;
+ 		}
+ 		memcpy(pages, node->pages, node->npages * sizeof(*pages));
+ 
+ 		npages -= node->npages;
+ retry:
+ 		if (!hfi1_can_pin_pages(pq->dd, pq->n_locked, npages)) {
+ 			spin_lock(&pq->evict_lock);
+ 			cleared = sdma_cache_evict(pq, npages);
+ 			spin_unlock(&pq->evict_lock);
+ 			if (cleared >= npages)
+ 				goto retry;
+ 		}
+ 		pinned = hfi1_acquire_user_pages(
+ 			((unsigned long)iovec->iov.iov_base +
+ 			 (node->npages * PAGE_SIZE)), npages, 0,
+ 			pages + node->npages);
+ 		if (pinned < 0) {
+ 			kfree(pages);
+ 			ret = pinned;
+ 			goto bail;
+ 		}
+ 		if (pinned != npages) {
+ 			unpin_vector_pages(current->mm, pages, pinned);
+ 			ret = -EFAULT;
+ 			goto bail;
+ 		}
+ 		kfree(node->pages);
+ 		node->pages = pages;
+ 		node->npages += pinned;
+ 		npages = node->npages;
+ 		spin_lock(&pq->evict_lock);
+ 		if (!rb_node)
+ 			list_add(&node->list, &pq->evict);
+ 		else
+ 			list_move(&node->list, &pq->evict);
+ 		pq->n_locked += pinned;
+ 		spin_unlock(&pq->evict_lock);
++>>>>>>> 5511d7810752 (IB/hfi1: Add SDMA cache eviction algorithm):drivers/staging/rdma/hfi1/user_sdma.c
  	}
 -	iovec->pages = node->pages;
 -	iovec->npages = npages;
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +	pinned = hfi1_acquire_user_pages((unsigned long)iovec->iov.iov_base,
 +					 npages, 0, iovec->pages);
 +
 +	if (pinned < 0)
 +		return pinned;
 +
 +	iovec->npages = pinned;
 +	if (pinned != npages) {
 +		SDMA_DBG(req, "Failed to pin pages (%d/%u)", pinned, npages);
 +		unpin_vector_pages(iovec);
 +		return -EFAULT;
++=======
+ 	if (!rb_node) {
+ 		ret = hfi1_mmu_rb_insert(&req->pq->sdma_rb_root, &node->rb);
+ 		if (ret) {
+ 			spin_lock(&pq->evict_lock);
+ 			list_del(&node->list);
+ 			pq->n_locked -= node->npages;
+ 			spin_unlock(&pq->evict_lock);
+ 			ret = 0;
+ 			goto bail;
+ 		}
+ 	} else {
+ 		atomic_inc(&node->refcount);
++>>>>>>> 5511d7810752 (IB/hfi1: Add SDMA cache eviction algorithm):drivers/staging/rdma/hfi1/user_sdma.c
  	}
  	return 0;
 -bail:
 -	if (!rb_node)
 -		kfree(node);
 -	return ret;
  }
  
 -static void unpin_vector_pages(struct mm_struct *mm, struct page **pages,
 -			       unsigned npages)
 +static void unpin_vector_pages(struct user_sdma_iovec *iovec)
  {
 -	hfi1_release_user_pages(mm, pages, npages, 0);
 -	kfree(pages);
 +	hfi1_release_user_pages(iovec->pages, iovec->npages, 0);
 +
 +	kfree(iovec->pages);
 +	iovec->pages = NULL;
 +	iovec->npages = 0;
 +	iovec->offset = 0;
  }
  
  static int check_header_template(struct user_sdma_request *req,
@@@ -1451,3 -1533,50 +1579,53 @@@ static inline void set_comp_state(struc
  	trace_hfi1_sdma_user_completion(pq->dd, pq->ctxt, pq->subctxt,
  					idx, state, ret);
  }
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
++=======
+ 
+ static bool sdma_rb_filter(struct mmu_rb_node *node, unsigned long addr,
+ 			   unsigned long len)
+ {
+ 	return (bool)(node->addr == addr);
+ }
+ 
+ static int sdma_rb_insert(struct rb_root *root, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	atomic_inc(&node->refcount);
+ 	return 0;
+ }
+ 
+ static void sdma_rb_remove(struct rb_root *root, struct mmu_rb_node *mnode,
+ 			   bool notifier)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	spin_lock(&node->pq->evict_lock);
+ 	list_del(&node->list);
+ 	node->pq->n_locked -= node->npages;
+ 	spin_unlock(&node->pq->evict_lock);
+ 
+ 	unpin_vector_pages(notifier ? NULL : current->mm, node->pages,
+ 			   node->npages);
+ 	/*
+ 	 * If called by the MMU notifier, we have to adjust the pinned
+ 	 * page count ourselves.
+ 	 */
+ 	if (notifier)
+ 		current->mm->pinned_vm -= node->npages;
+ 	kfree(node);
+ }
+ 
+ static int sdma_rb_invalidate(struct rb_root *root, struct mmu_rb_node *mnode)
+ {
+ 	struct sdma_mmu_node *node =
+ 		container_of(mnode, struct sdma_mmu_node, rb);
+ 
+ 	if (!atomic_read(&node->refcount))
+ 		return 1;
+ 	return 0;
+ }
++>>>>>>> 5511d7810752 (IB/hfi1: Add SDMA cache eviction algorithm):drivers/staging/rdma/hfi1/user_sdma.c
diff --cc drivers/staging/hfi1/user_sdma.h
index 7ebbc4634989,b9240e351161..000000000000
--- a/drivers/staging/hfi1/user_sdma.h
+++ b/drivers/staging/hfi1/user_sdma.h
@@@ -78,6 -67,10 +78,13 @@@ struct hfi1_user_sdma_pkt_q 
  	unsigned state;
  	wait_queue_head_t wait;
  	unsigned long unpinned;
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.h
++=======
+ 	struct rb_root sdma_rb_root;
+ 	u32 n_locked;
+ 	struct list_head evict;
+ 	spinlock_t evict_lock; /* protect evict and n_locked */
++>>>>>>> 5511d7810752 (IB/hfi1: Add SDMA cache eviction algorithm):drivers/staging/rdma/hfi1/user_sdma.h
  };
  
  struct hfi1_user_sdma_comp_q {
* Unmerged path drivers/staging/hfi1/user_sdma.c
* Unmerged path drivers/staging/hfi1/user_sdma.h
