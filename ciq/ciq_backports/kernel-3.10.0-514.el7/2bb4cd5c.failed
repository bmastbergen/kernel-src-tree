block: have drivers use blk_queue_max_discard_sectors()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jens Axboe <axboe@fb.com>
commit 2bb4cd5cc472b191a46938becb7dafdd44644329
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/2bb4cd5c.failed

Some drivers use it now, others just set the limits field manually.
But in preparation for splitting this into a hard and soft limit,
ensure that they all call the proper function for setting the hw
limit for discards.

	Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 2bb4cd5cc472b191a46938becb7dafdd44644329)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/drbd/drbd_nl.c
#	drivers/block/skd_main.c
#	drivers/block/zram/zram_drv.c
#	drivers/md/bcache/super.c
diff --cc drivers/block/drbd/drbd_nl.c
index 9e3f441e7e84,e80cbefbc2b5..000000000000
--- a/drivers/block/drbd/drbd_nl.c
+++ b/drivers/block/drbd/drbd_nl.c
@@@ -1034,8 -1148,25 +1034,30 @@@ static void drbd_setup_queue_param(stru
  	blk_queue_max_segments(q, max_segments ? max_segments : BLK_MAX_SEGMENTS);
  	blk_queue_segment_boundary(q, PAGE_CACHE_SIZE-1);
  
++<<<<<<< HEAD
 +	if (get_ldev_if_state(mdev, D_ATTACHING)) {
 +		struct request_queue * const b = mdev->ldev->backing_bdev->bd_disk->queue;
++=======
+ 	if (b) {
+ 		struct drbd_connection *connection = first_peer_device(device)->connection;
+ 
+ 		if (blk_queue_discard(b) &&
+ 		    (connection->cstate < C_CONNECTED || connection->agreed_features & FF_TRIM)) {
+ 			/* For now, don't allow more than one activity log extent worth of data
+ 			 * to be discarded in one go. We may need to rework drbd_al_begin_io()
+ 			 * to allow for even larger discard ranges */
+ 			blk_queue_max_discard_sectors(q, DRBD_MAX_DISCARD_SECTORS);
+ 
+ 			queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);
+ 			/* REALLY? Is stacking secdiscard "legal"? */
+ 			if (blk_queue_secdiscard(b))
+ 				queue_flag_set_unlocked(QUEUE_FLAG_SECDISCARD, q);
+ 		} else {
+ 			blk_queue_max_discard_sectors(q, 0);
+ 			queue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);
+ 			queue_flag_clear_unlocked(QUEUE_FLAG_SECDISCARD, q);
+ 		}
++>>>>>>> 2bb4cd5cc472 (block: have drivers use blk_queue_max_discard_sectors())
  
  		blk_queue_stack_limits(q, b);
  
diff --cc drivers/block/zram/zram_drv.c
index 817b95847bab,f439ad2800da..000000000000
--- a/drivers/block/zram/zram_drv.c
+++ b/drivers/block/zram/zram_drv.c
@@@ -999,6 -1243,21 +999,24 @@@ static int create_device(struct zram *z
  					ZRAM_LOGICAL_BLOCK_SIZE);
  	blk_queue_io_min(zram->disk->queue, PAGE_SIZE);
  	blk_queue_io_opt(zram->disk->queue, PAGE_SIZE);
++<<<<<<< HEAD
++=======
+ 	zram->disk->queue->limits.discard_granularity = PAGE_SIZE;
+ 	blk_queue_max_discard_sectors(zram->disk->queue, UINT_MAX);
+ 	/*
+ 	 * zram_bio_discard() will clear all logical blocks if logical block
+ 	 * size is identical with physical block size(PAGE_SIZE). But if it is
+ 	 * different, we will skip discarding some parts of logical blocks in
+ 	 * the part of the request range which isn't aligned to physical block
+ 	 * size.  So we can't ensure that all discarded logical blocks are
+ 	 * zeroed.
+ 	 */
+ 	if (ZRAM_LOGICAL_BLOCK_SIZE == PAGE_SIZE)
+ 		zram->disk->queue->limits.discard_zeroes_data = 1;
+ 	else
+ 		zram->disk->queue->limits.discard_zeroes_data = 0;
+ 	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, zram->disk->queue);
++>>>>>>> 2bb4cd5cc472 (block: have drivers use blk_queue_max_discard_sectors())
  
  	add_disk(zram->disk);
  
diff --cc drivers/md/bcache/super.c
index b4713cea1913,fc8e545ced18..000000000000
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@@ -775,7 -830,8 +775,12 @@@ static int bcache_device_init(struct bc
  	q->limits.max_sectors		= UINT_MAX;
  	q->limits.max_segment_size	= UINT_MAX;
  	q->limits.max_segments		= BIO_MAX_PAGES;
++<<<<<<< HEAD
 +	q->limits.max_discard_sectors	= UINT_MAX;
++=======
+ 	blk_queue_max_discard_sectors(q, UINT_MAX);
+ 	q->limits.discard_granularity	= 512;
++>>>>>>> 2bb4cd5cc472 (block: have drivers use blk_queue_max_discard_sectors())
  	q->limits.io_min		= block_size;
  	q->limits.logical_block_size	= block_size;
  	q->limits.physical_block_size	= block_size;
* Unmerged path drivers/block/skd_main.c
diff --git a/drivers/block/brd.c b/drivers/block/brd.c
index 9bf4371755f2..7dea8a44f35f 100644
--- a/drivers/block/brd.c
+++ b/drivers/block/brd.c
@@ -479,7 +479,7 @@ static struct brd_device *brd_alloc(int i)
 	blk_queue_bounce_limit(brd->brd_queue, BLK_BOUNCE_ANY);
 
 	brd->brd_queue->limits.discard_granularity = PAGE_SIZE;
-	brd->brd_queue->limits.max_discard_sectors = UINT_MAX;
+	blk_queue_max_discard_sectors(brd->brd_queue, UINT_MAX);
 	brd->brd_queue->limits.discard_zeroes_data = 1;
 	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, brd->brd_queue);
 
* Unmerged path drivers/block/drbd/drbd_nl.c
diff --git a/drivers/block/loop.c b/drivers/block/loop.c
index d5de459aa504..6144b5190f23 100644
--- a/drivers/block/loop.c
+++ b/drivers/block/loop.c
@@ -828,7 +828,7 @@ static void loop_config_discard(struct loop_device *lo)
 	    lo->lo_encrypt_key_size) {
 		q->limits.discard_granularity = 0;
 		q->limits.discard_alignment = 0;
-		q->limits.max_discard_sectors = 0;
+		blk_queue_max_discard_sectors(q, 0);
 		q->limits.discard_zeroes_data = 0;
 		queue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);
 		return;
@@ -836,7 +836,7 @@ static void loop_config_discard(struct loop_device *lo)
 
 	q->limits.discard_granularity = inode->i_sb->s_blocksize;
 	q->limits.discard_alignment = 0;
-	q->limits.max_discard_sectors = UINT_MAX >> 9;
+	blk_queue_max_discard_sectors(q, UINT_MAX >> 9);
 	q->limits.discard_zeroes_data = 1;
 	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);
 }
diff --git a/drivers/block/nbd.c b/drivers/block/nbd.c
index e0c6b623585d..11eaa972eb44 100644
--- a/drivers/block/nbd.c
+++ b/drivers/block/nbd.c
@@ -861,7 +861,7 @@ static int __init nbd_init(void)
 		queue_flag_set_unlocked(QUEUE_FLAG_NONROT, disk->queue);
 		queue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, disk->queue);
 		disk->queue->limits.discard_granularity = 512;
-		disk->queue->limits.max_discard_sectors = UINT_MAX;
+		blk_queue_max_discard_sectors(disk->queue, UINT_MAX);
 		disk->queue->limits.discard_zeroes_data = 0;
 		blk_queue_max_hw_sectors(disk->queue, 65536);
 		disk->queue->limits.max_sectors = 256;
diff --git a/drivers/block/nvme-core.c b/drivers/block/nvme-core.c
index 2a6eb55ad96c..1f0557766f9d 100644
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@ -1883,7 +1883,7 @@ static void nvme_config_discard(struct nvme_ns *ns)
 	ns->queue->limits.discard_zeroes_data = 0;
 	ns->queue->limits.discard_alignment = logical_block_size;
 	ns->queue->limits.discard_granularity = logical_block_size;
-	ns->queue->limits.max_discard_sectors = 0xffffffff;
+	blk_queue_max_discard_sectors(ns->queue, 0xffffffff);
 	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, ns->queue);
 }
 
diff --git a/drivers/block/rbd.c b/drivers/block/rbd.c
index 833639aa89b1..03a72a964eb5 100644
--- a/drivers/block/rbd.c
+++ b/drivers/block/rbd.c
@@ -3824,7 +3824,7 @@ static int rbd_init_disk(struct rbd_device *rbd_dev)
 	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);
 	q->limits.discard_granularity = segment_size;
 	q->limits.discard_alignment = segment_size;
-	q->limits.max_discard_sectors = segment_size / SECTOR_SIZE;
+	blk_queue_max_discard_sectors(q, segment_size / SECTOR_SIZE);
 	q->limits.discard_zeroes_data = 1;
 
 	blk_queue_merge_bvec(q, rbd_merge_bvec);
* Unmerged path drivers/block/skd_main.c
* Unmerged path drivers/block/zram/zram_drv.c
* Unmerged path drivers/md/bcache/super.c
diff --git a/drivers/mmc/card/queue.c b/drivers/mmc/card/queue.c
index 756b1dca1c20..ebc1b1366198 100644
--- a/drivers/mmc/card/queue.c
+++ b/drivers/mmc/card/queue.c
@@ -164,7 +164,7 @@ static void mmc_queue_setup_discard(struct request_queue *q,
 		return;
 
 	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);
-	q->limits.max_discard_sectors = max_discard;
+	blk_queue_max_discard_sectors(q, max_discard);
 	if (card->erased_byte == 0 && !mmc_can_discard(card))
 		q->limits.discard_zeroes_data = 1;
 	q->limits.discard_granularity = card->pref_erase << 9;
diff --git a/drivers/mtd/mtd_blkdevs.c b/drivers/mtd/mtd_blkdevs.c
index a7ca4809d49e..befd096b4fa2 100644
--- a/drivers/mtd/mtd_blkdevs.c
+++ b/drivers/mtd/mtd_blkdevs.c
@@ -417,7 +417,7 @@ int add_mtd_blktrans_dev(struct mtd_blktrans_dev *new)
 
 	if (tr->discard) {
 		queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, new->rq);
-		new->rq->limits.max_discard_sectors = UINT_MAX;
+		blk_queue_max_discard_sectors(new->rq, UINT_MAX);
 	}
 
 	gd->queue = new->rq;
diff --git a/drivers/scsi/sd.c b/drivers/scsi/sd.c
index f531f3e9f348..a240039112be 100644
--- a/drivers/scsi/sd.c
+++ b/drivers/scsi/sd.c
@@ -652,7 +652,7 @@ static void sd_config_discard(struct scsi_disk *sdkp, unsigned int mode)
 	switch (mode) {
 
 	case SD_LBP_DISABLE:
-		q->limits.max_discard_sectors = 0;
+		blk_queue_max_discard_sectors(q, 0);
 		queue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);
 		return;
 
@@ -678,7 +678,7 @@ static void sd_config_discard(struct scsi_disk *sdkp, unsigned int mode)
 		break;
 	}
 
-	q->limits.max_discard_sectors = max_blocks * (logical_block_size >> 9);
+	blk_queue_max_discard_sectors(q, max_blocks * (logical_block_size >> 9));
 	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);
 }
 
