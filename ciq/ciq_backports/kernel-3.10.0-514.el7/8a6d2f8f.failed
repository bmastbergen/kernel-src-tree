perf/x86/intel/rapl: Utilize event->pmu_private

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 8a6d2f8f73caa8b8eb596a9e2d2e0b15d64751a4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/8a6d2f8f.failed

Store the PMU pointer in event->pmu_private and use it instead of the per CPU
data. Preparatory step to get rid of the per CPU allocations. The usage sites
are the perf fast path, so we keep that even after the conversion to per
package storage as a CPU to package lookup involves 3 loads versus 1 with the
pmu_private pointer.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andi Kleen <andi.kleen@intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Harish Chegondi <harish.chegondi@intel.com>
	Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Kan Liang <kan.liang@intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Cc: linux-kernel@vger.kernel.org
Link: http://lkml.kernel.org/r/20160222221012.748151799@linutronix.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 8a6d2f8f73caa8b8eb596a9e2d2e0b15d64751a4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel_rapl.c
diff --cc arch/x86/kernel/cpu/perf_event_intel_rapl.c
index c019c57572a7,753d90cbcce2..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel_rapl.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_rapl.c
@@@ -123,15 -120,17 +123,25 @@@ static struct perf_pmu_events_attr even
  };
  
  struct rapl_pmu {
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	spinlock_t	 lock;
 +	int		 n_active; /* number of active events */
 +	struct list_head active_list;
 +	struct pmu	 *pmu; /* pointer to rapl_pmu_class */
 +	ktime_t		 timer_interval; /* in ktime_t unit */
 +	struct hrtimer   hrtimer;
++=======
+ 	raw_spinlock_t		lock;
+ 	int			n_active;
+ 	int			cpu;
+ 	struct list_head	active_list;
+ 	struct pmu		*pmu;
+ 	ktime_t			timer_interval;
+ 	struct hrtimer		hrtimer;
++>>>>>>> 8a6d2f8f73ca (perf/x86/intel/rapl: Utilize event->pmu_private):arch/x86/events/intel/rapl.c
  };
  
 - /* 1/2^hw_unit Joule */
 -static int rapl_hw_unit[NR_RAPL_DOMAINS] __read_mostly;
 +static int rapl_hw_unit[NR_RAPL_DOMAINS] __read_mostly;  /* 1/2^hw_unit Joule */
  static struct pmu rapl_pmu_class;
  static cpumask_t rapl_cpu_mask;
  static int rapl_cntr_mask;
@@@ -212,14 -202,9 +222,18 @@@ static void rapl_start_hrtimer(struct r
  		     HRTIMER_MODE_REL_PINNED);
  }
  
 +static void rapl_stop_hrtimer(struct rapl_pmu *pmu)
 +{
 +	hrtimer_cancel(&pmu->hrtimer);
 +}
 +
  static enum hrtimer_restart rapl_hrtimer_handle(struct hrtimer *hrtimer)
  {
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	struct rapl_pmu *pmu = __get_cpu_var(rapl_pmu);
++=======
+ 	struct rapl_pmu *pmu = container_of(hrtimer, struct rapl_pmu, hrtimer);
++>>>>>>> 8a6d2f8f73ca (perf/x86/intel/rapl: Utilize event->pmu_private):arch/x86/events/intel/rapl.c
  	struct perf_event *event;
  	unsigned long flags;
  
@@@ -266,17 -250,17 +280,25 @@@ static void __rapl_pmu_event_start(stru
  
  static void rapl_pmu_event_start(struct perf_event *event, int mode)
  {
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	struct rapl_pmu *pmu = __get_cpu_var(rapl_pmu);
++=======
+ 	struct rapl_pmu *pmu = event->pmu_private;
++>>>>>>> 8a6d2f8f73ca (perf/x86/intel/rapl: Utilize event->pmu_private):arch/x86/events/intel/rapl.c
  	unsigned long flags;
  
 -	raw_spin_lock_irqsave(&pmu->lock, flags);
 +	spin_lock_irqsave(&pmu->lock, flags);
  	__rapl_pmu_event_start(pmu, event);
 -	raw_spin_unlock_irqrestore(&pmu->lock, flags);
 +	spin_unlock_irqrestore(&pmu->lock, flags);
  }
  
  static void rapl_pmu_event_stop(struct perf_event *event, int mode)
  {
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	struct rapl_pmu *pmu = __get_cpu_var(rapl_pmu);
++=======
+ 	struct rapl_pmu *pmu = event->pmu_private;
++>>>>>>> 8a6d2f8f73ca (perf/x86/intel/rapl: Utilize event->pmu_private):arch/x86/events/intel/rapl.c
  	struct hw_perf_event *hwc = &event->hw;
  	unsigned long flags;
  
@@@ -310,7 -294,7 +332,11 @@@
  
  static int rapl_pmu_event_add(struct perf_event *event, int mode)
  {
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	struct rapl_pmu *pmu = __get_cpu_var(rapl_pmu);
++=======
+ 	struct rapl_pmu *pmu = event->pmu_private;
++>>>>>>> 8a6d2f8f73ca (perf/x86/intel/rapl: Utilize event->pmu_private):arch/x86/events/intel/rapl.c
  	struct hw_perf_event *hwc = &event->hw;
  	unsigned long flags;
  
@@@ -605,20 -579,9 +637,21 @@@ static int rapl_cpu_prepare(int cpu
  	INIT_LIST_HEAD(&pmu->active_list);
  
  	pmu->pmu = &rapl_pmu_class;
+ 	pmu->cpu = cpu;
  
 -	pmu->timer_interval = ms_to_ktime(rapl_timer_ms);
 +	/*
 +	 * use reference of 200W for scaling the timeout
 +	 * to avoid missing counter overflows.
 +	 * 200W = 200 Joules/sec
 +	 * divide interval by 2 to avoid lockstep (2 * 100)
 +	 * if hw unit is 32, then we use 2 ms 1/200/2
 +	 */
 +	if (rapl_hw_unit[0] < 32)
 +		ms = (1000 / (2 * 100)) * (1ULL << (32 - rapl_hw_unit[0] - 1));
 +	else
 +		ms = 2;
 +
 +	pmu->timer_interval = ms_to_ktime(ms);
  
  	rapl_hrtimer_init(pmu);
  
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_rapl.c
