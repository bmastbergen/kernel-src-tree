xprtrdma: Move struct ib_send_wr off the stack

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 3cf4e169be95e1a3a70a063b6bd8103fbd5911f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/3cf4e169.failed

For FRWR FASTREG and LOCAL_INV, move the ib_*_wr structure off
the stack. This allows frwr_op_map and frwr_op_unmap to chain
WRs together without limit to register or invalidate a set of MRs
with a single ib_post_send().

(This will be for chaining LOCAL_INV requests).

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
	Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
	Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
(cherry picked from commit 3cf4e169be95e1a3a70a063b6bd8103fbd5911f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprtrdma/frwr_ops.c
diff --cc net/sunrpc/xprtrdma/frwr_ops.c
index 944b72ffed6c,31a45786137b..000000000000
--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@@ -248,63 -313,88 +248,106 @@@ frwr_op_map(struct rpcrdma_xprt *r_xprt
  	    int nsegs, bool writing)
  {
  	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 -	struct ib_device *device = ia->ri_device;
 +	struct ib_device *device = ia->ri_id->device;
  	enum dma_data_direction direction = rpcrdma_data_dir(writing);
  	struct rpcrdma_mr_seg *seg1 = seg;
++<<<<<<< HEAD
 +	struct rpcrdma_mw *mw = seg1->rl_mw;
 +	struct rpcrdma_frmr *frmr = &mw->r.frmr;
 +	struct ib_mr *mr = frmr->fr_mr;
 +	struct ib_send_wr fastreg_wr, *bad_wr;
++=======
+ 	struct rpcrdma_mw *mw;
+ 	struct rpcrdma_frmr *frmr;
+ 	struct ib_mr *mr;
+ 	struct ib_reg_wr *reg_wr;
+ 	struct ib_send_wr *bad_wr;
+ 	int rc, i, n, dma_nents;
++>>>>>>> 3cf4e169be95 (xprtrdma: Move struct ib_send_wr off the stack)
  	u8 key;
 -
 +	int len, pageoff;
 +	int i, rc;
 +	int seg_len;
 +	u64 pa;
 +	int page_no;
 +
++<<<<<<< HEAD
 +	pageoff = offset_in_page(seg1->mr_offset);
 +	seg1->mr_offset -= pageoff;	/* start of page */
 +	seg1->mr_len += pageoff;
 +	len = -pageoff;
++=======
+ 	mw = seg1->rl_mw;
+ 	seg1->rl_mw = NULL;
+ 	do {
+ 		if (mw)
+ 			__frwr_queue_recovery(mw);
+ 		mw = rpcrdma_get_mw(r_xprt);
+ 		if (!mw)
+ 			return -ENOMEM;
+ 	} while (mw->r.frmr.fr_state != FRMR_IS_INVALID);
+ 	frmr = &mw->r.frmr;
+ 	frmr->fr_state = FRMR_IS_VALID;
+ 	mr = frmr->fr_mr;
+ 	reg_wr = &frmr->fr_regwr;
+ 
++>>>>>>> 3cf4e169be95 (xprtrdma: Move struct ib_send_wr off the stack)
  	if (nsegs > ia->ri_max_frmr_depth)
  		nsegs = ia->ri_max_frmr_depth;
 -
 -	for (i = 0; i < nsegs;) {
 -		if (seg->mr_page)
 -			sg_set_page(&frmr->sg[i],
 -				    seg->mr_page,
 -				    seg->mr_len,
 -				    offset_in_page(seg->mr_offset));
 -		else
 -			sg_set_buf(&frmr->sg[i], seg->mr_offset,
 -				   seg->mr_len);
 -
 +	for (page_no = i = 0; i < nsegs;) {
 +		rpcrdma_map_one(device, seg, direction);
 +		pa = seg->mr_dma;
 +		for (seg_len = seg->mr_len; seg_len > 0; seg_len -= PAGE_SIZE) {
 +			frmr->fr_pgl->page_list[page_no++] = pa;
 +			pa += PAGE_SIZE;
 +		}
 +		len += seg->mr_len;
  		++seg;
  		++i;
 -
  		/* Check for holes */
  		if ((i < nsegs && offset_in_page(seg->mr_offset)) ||
  		    offset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))
  			break;
  	}
 -	frmr->sg_nents = i;
 +	dprintk("RPC:       %s: Using frmr %p to map %d segments (%d bytes)\n",
 +		__func__, mw, i, len);
  
 -	dma_nents = ib_dma_map_sg(device, frmr->sg, frmr->sg_nents, direction);
 -	if (!dma_nents) {
 -		pr_err("RPC:       %s: failed to dma map sg %p sg_nents %u\n",
 -		       __func__, frmr->sg, frmr->sg_nents);
 -		return -ENOMEM;
 -	}
 -
 -	n = ib_map_mr_sg(mr, frmr->sg, frmr->sg_nents, PAGE_SIZE);
 -	if (unlikely(n != frmr->sg_nents)) {
 -		pr_err("RPC:       %s: failed to map mr %p (%u/%u)\n",
 -		       __func__, frmr->fr_mr, n, frmr->sg_nents);
 -		rc = n < 0 ? n : -EINVAL;
 -		goto out_senderr;
 -	}
 -
 -	dprintk("RPC:       %s: Using frmr %p to map %u segments (%u bytes)\n",
 -		__func__, mw, frmr->sg_nents, mr->length);
 +	frmr->fr_state = FRMR_IS_VALID;
  
 +	memset(&fastreg_wr, 0, sizeof(fastreg_wr));
 +	fastreg_wr.wr_id = (unsigned long)(void *)mw;
 +	fastreg_wr.opcode = IB_WR_FAST_REG_MR;
 +	fastreg_wr.wr.fast_reg.iova_start = seg1->mr_dma + pageoff;
 +	fastreg_wr.wr.fast_reg.page_list = frmr->fr_pgl;
 +	fastreg_wr.wr.fast_reg.page_shift = PAGE_SHIFT;
 +	fastreg_wr.wr.fast_reg.page_list_len = page_no;
 +	fastreg_wr.wr.fast_reg.length = len;
 +	fastreg_wr.wr.fast_reg.access_flags = writing ?
 +				IB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :
 +				IB_ACCESS_REMOTE_READ;
  	key = (u8)(mr->rkey & 0x000000FF);
  	ib_update_fast_reg_key(mr, ++key);
++<<<<<<< HEAD
 +	fastreg_wr.wr.fast_reg.rkey = mr->rkey;
 +
 +	DECR_CQCOUNT(&r_xprt->rx_ep);
 +	rc = ib_post_send(ia->ri_id->qp, &fastreg_wr, &bad_wr);
++=======
+ 
+ 	reg_wr->wr.next = NULL;
+ 	reg_wr->wr.opcode = IB_WR_REG_MR;
+ 	reg_wr->wr.wr_id = (uintptr_t)mw;
+ 	reg_wr->wr.num_sge = 0;
+ 	reg_wr->wr.send_flags = 0;
+ 	reg_wr->mr = mr;
+ 	reg_wr->key = mr->rkey;
+ 	reg_wr->access = writing ?
+ 			 IB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :
+ 			 IB_ACCESS_REMOTE_READ;
+ 
+ 	DECR_CQCOUNT(&r_xprt->rx_ep);
+ 	rc = ib_post_send(ia->ri_id->qp, &reg_wr->wr, &bad_wr);
++>>>>>>> 3cf4e169be95 (xprtrdma: Move struct ib_send_wr off the stack)
  	if (rc)
  		goto out_senderr;
  
@@@ -331,23 -422,26 +374,46 @@@ frwr_op_unmap(struct rpcrdma_xprt *r_xp
  {
  	struct rpcrdma_mr_seg *seg1 = seg;
  	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
++<<<<<<< HEAD
 +	struct ib_send_wr invalidate_wr, *bad_wr;
++=======
+ 	struct rpcrdma_mw *mw = seg1->rl_mw;
+ 	struct rpcrdma_frmr *frmr = &mw->r.frmr;
+ 	struct ib_send_wr *invalidate_wr, *bad_wr;
++>>>>>>> 3cf4e169be95 (xprtrdma: Move struct ib_send_wr off the stack)
  	int rc, nsegs = seg->mr_nsegs;
 +	struct ib_device *device;
  
++<<<<<<< HEAD
 +	seg1->rl_mw->r.frmr.fr_state = FRMR_IS_INVALID;
 +
 +	memset(&invalidate_wr, 0, sizeof(invalidate_wr));
 +	invalidate_wr.wr_id = (unsigned long)(void *)seg1->rl_mw;
 +	invalidate_wr.opcode = IB_WR_LOCAL_INV;
 +	invalidate_wr.ex.invalidate_rkey = seg1->rl_mw->r.frmr.fr_mr->rkey;
++=======
+ 	dprintk("RPC:       %s: FRMR %p\n", __func__, mw);
+ 
+ 	seg1->rl_mw = NULL;
+ 	frmr->fr_state = FRMR_IS_INVALID;
+ 	invalidate_wr = &mw->r.frmr.fr_invwr;
+ 
+ 	memset(invalidate_wr, 0, sizeof(*invalidate_wr));
+ 	invalidate_wr->wr_id = (uintptr_t)mw;
+ 	invalidate_wr->opcode = IB_WR_LOCAL_INV;
+ 	invalidate_wr->ex.invalidate_rkey = frmr->fr_mr->rkey;
++>>>>>>> 3cf4e169be95 (xprtrdma: Move struct ib_send_wr off the stack)
  	DECR_CQCOUNT(&r_xprt->rx_ep);
  
 -	ib_dma_unmap_sg(ia->ri_device, frmr->sg, frmr->sg_nents, seg1->mr_dir);
  	read_lock(&ia->ri_qplock);
++<<<<<<< HEAD
 +	device = ia->ri_id->device;
 +	while (seg1->mr_nsegs--)
 +		rpcrdma_unmap_one(device, seg++);
 +	rc = ib_post_send(ia->ri_id->qp, &invalidate_wr, &bad_wr);
++=======
+ 	rc = ib_post_send(ia->ri_id->qp, invalidate_wr, &bad_wr);
++>>>>>>> 3cf4e169be95 (xprtrdma: Move struct ib_send_wr off the stack)
  	read_unlock(&ia->ri_qplock);
  	if (rc)
  		goto out_err;
* Unmerged path net/sunrpc/xprtrdma/frwr_ops.c
diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b2d2c86a7023..6c3a8bddb5f6 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -200,6 +200,10 @@ struct rpcrdma_frmr {
 	enum rpcrdma_frmr_state		fr_state;
 	struct work_struct		fr_work;
 	struct rpcrdma_xprt		*fr_xprt;
+	union {
+		struct ib_reg_wr	fr_regwr;
+		struct ib_send_wr	fr_invwr;
+	};
 };
 
 struct rpcrdma_mw {
