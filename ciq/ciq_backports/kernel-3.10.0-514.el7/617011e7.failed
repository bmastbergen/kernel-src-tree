rhashtable: Avoid calculating hash again to unlock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Graf <tgraf@suug.ch>
commit 617011e7d5559046e4fc8f87793c8a5d9c3431b0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/617011e7.failed

Caching the lock pointer avoids having to hash on the object
again to unlock the bucket locks.

	Signed-off-by: Thomas Graf <tgraf@suug.ch>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 617011e7d5559046e4fc8f87793c8a5d9c3431b0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rhashtable.c
diff --cc lib/rhashtable.c
index 6d0c4774001c,09a7ada89ade..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -319,8 -357,91 +319,94 @@@ int rhashtable_shrink(struct rhashtabl
  }
  EXPORT_SYMBOL_GPL(rhashtable_shrink);
  
++<<<<<<< HEAD
++=======
+ static void rht_deferred_worker(struct work_struct *work)
+ {
+ 	struct rhashtable *ht;
+ 	struct bucket_table *tbl;
+ 
+ 	ht = container_of(work, struct rhashtable, run_work);
+ 	mutex_lock(&ht->mutex);
+ 	if (ht->being_destroyed)
+ 		goto unlock;
+ 
+ 	tbl = rht_dereference(ht->tbl, ht);
+ 
+ 	if (rht_grow_above_75(ht, tbl))
+ 		rhashtable_expand(ht);
+ 	else if (rht_shrink_below_30(ht, tbl))
+ 		rhashtable_shrink(ht);
+ unlock:
+ 	mutex_unlock(&ht->mutex);
+ }
+ 
+ static bool __rhashtable_insert(struct rhashtable *ht, struct rhash_head *obj,
+ 				bool (*compare)(void *, void *), void *arg)
+ {
+ 	struct bucket_table *tbl, *old_tbl;
+ 	struct rhash_head *head;
+ 	bool no_resize_running;
+ 	unsigned hash;
+ 	spinlock_t *old_lock;
+ 	bool success = true;
+ 
+ 	rcu_read_lock();
+ 
+ 	old_tbl = rht_dereference_rcu(ht->tbl, ht);
+ 	hash = head_hashfn(ht, old_tbl, obj);
+ 	old_lock = bucket_lock(old_tbl, hash);
+ 
+ 	spin_lock_bh(old_lock);
+ 
+ 	/* Because we have already taken the bucket lock in old_tbl,
+ 	 * if we find that future_tbl is not yet visible then that
+ 	 * guarantees all other insertions of the same entry will
+ 	 * also grab the bucket lock in old_tbl because until the
+ 	 * rehash completes ht->tbl won't be changed.
+ 	 */
+ 	tbl = rht_dereference_rcu(old_tbl->future_tbl, ht) ?: old_tbl;
+ 	if (tbl != old_tbl) {
+ 		hash = head_hashfn(ht, tbl, obj);
+ 		spin_lock_nested(bucket_lock(tbl, hash), SINGLE_DEPTH_NESTING);
+ 	}
+ 
+ 	if (compare &&
+ 	    rhashtable_lookup_compare(ht, rht_obj(ht, obj) + ht->p.key_offset,
+ 				      compare, arg)) {
+ 		success = false;
+ 		goto exit;
+ 	}
+ 
+ 	no_resize_running = tbl == old_tbl;
+ 
+ 	head = rht_dereference_bucket(tbl->buckets[hash], tbl, hash);
+ 
+ 	if (rht_is_a_nulls(head))
+ 		INIT_RHT_NULLS_HEAD(obj->next, ht, hash);
+ 	else
+ 		RCU_INIT_POINTER(obj->next, head);
+ 
+ 	rcu_assign_pointer(tbl->buckets[hash], obj);
+ 
+ 	atomic_inc(&ht->nelems);
+ 	if (no_resize_running && rht_grow_above_75(ht, tbl))
+ 		schedule_work(&ht->run_work);
+ 
+ exit:
+ 	if (tbl != old_tbl)
+ 		spin_unlock(bucket_lock(tbl, hash));
+ 
+ 	spin_unlock_bh(old_lock);
+ 
+ 	rcu_read_unlock();
+ 
+ 	return success;
+ }
+ 
++>>>>>>> 617011e7d555 (rhashtable: Avoid calculating hash again to unlock)
  /**
 - * rhashtable_insert - insert object into hash table
 + * rhashtable_insert - insert object into hash hash table
   * @ht:		hash table
   * @obj:	pointer to hash head inside object
   *
* Unmerged path lib/rhashtable.c
