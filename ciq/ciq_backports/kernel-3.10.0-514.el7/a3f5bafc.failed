mm/memblock: allocate boot time data structures from mirrored memory

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] memblock: allocate boot time data structures from mirrored memory (Yasuaki Ishimatsu) [1270209]
Rebuild_FUZZ: 97.74%
commit-author Tony Luck <tony.luck@intel.com>
commit a3f5bafcc04aaf62990e0cf3ced1cc6d8dc6fe95
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a3f5bafc.failed

Try to allocate all boot time kernel data structures from mirrored
memory.

If we run out of mirrored memory print warnings, but fall back to using
non-mirrored memory to make sure that we still boot.

By number of bytes, most of what we allocate at boot time is the page
structures.  64 bytes per 4K page on x86_64 ...  or about 1.5% of total
system memory.  For workloads where the bulk of memory is allocated to
applications this may represent a useful improvement to system
availability since 1.5% of total memory might be a third of the memory
allocated to the kernel.

	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Cc: Xishi Qiu <qiuxishi@huawei.com>
	Cc: Hanjun Guo <guohanjun@huawei.com>
	Cc: Xiexiuqi <xiexiuqi@huawei.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Yinghai Lu <yinghai@kernel.org>
	Cc: Naoya Horiguchi <nao.horiguchi@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a3f5bafcc04aaf62990e0cf3ced1cc6d8dc6fe95)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memblock.h
#	mm/memblock.c
#	mm/nobootmem.c
diff --cc include/linux/memblock.h
index 39082db5d5cd,0215ffd63069..000000000000
--- a/include/linux/memblock.h
+++ b/include/linux/memblock.h
@@@ -18,9 -18,14 +18,17 @@@
  #include <linux/mm.h>
  
  #define INIT_MEMBLOCK_REGIONS	128
 -#define INIT_PHYSMEM_REGIONS	4
  
  /* Definition of memblock flags. */
++<<<<<<< HEAD
 +#define MEMBLOCK_HOTPLUG	0x1	/* hotpluggable region */
++=======
+ enum {
+ 	MEMBLOCK_NONE		= 0x0,	/* No special request */
+ 	MEMBLOCK_HOTPLUG	= 0x1,	/* hotpluggable region */
+ 	MEMBLOCK_MIRROR		= 0x2,	/* mirrored region */
+ };
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  
  struct memblock_region {
  	phys_addr_t base;
@@@ -70,6 -79,69 +78,72 @@@ int memblock_reserve(phys_addr_t base, 
  void memblock_trim_memory(phys_addr_t align);
  int memblock_mark_hotplug(phys_addr_t base, phys_addr_t size);
  int memblock_clear_hotplug(phys_addr_t base, phys_addr_t size);
++<<<<<<< HEAD
++=======
+ int memblock_mark_mirror(phys_addr_t base, phys_addr_t size);
+ ulong choose_memblock_flags(void);
+ 
+ /* Low level functions */
+ int memblock_add_range(struct memblock_type *type,
+ 		       phys_addr_t base, phys_addr_t size,
+ 		       int nid, unsigned long flags);
+ 
+ int memblock_remove_range(struct memblock_type *type,
+ 			  phys_addr_t base,
+ 			  phys_addr_t size);
+ 
+ void __next_mem_range(u64 *idx, int nid, ulong flags,
+ 		      struct memblock_type *type_a,
+ 		      struct memblock_type *type_b, phys_addr_t *out_start,
+ 		      phys_addr_t *out_end, int *out_nid);
+ 
+ void __next_mem_range_rev(u64 *idx, int nid, ulong flags,
+ 			  struct memblock_type *type_a,
+ 			  struct memblock_type *type_b, phys_addr_t *out_start,
+ 			  phys_addr_t *out_end, int *out_nid);
+ 
+ /**
+  * for_each_mem_range - iterate through memblock areas from type_a and not
+  * included in type_b. Or just type_a if type_b is NULL.
+  * @i: u64 used as loop variable
+  * @type_a: ptr to memblock_type to iterate
+  * @type_b: ptr to memblock_type which excludes from the iteration
+  * @nid: node selector, %NUMA_NO_NODE for all nodes
+  * @flags: pick from blocks based on memory attributes
+  * @p_start: ptr to phys_addr_t for start address of the range, can be %NULL
+  * @p_end: ptr to phys_addr_t for end address of the range, can be %NULL
+  * @p_nid: ptr to int for nid of the range, can be %NULL
+  */
+ #define for_each_mem_range(i, type_a, type_b, nid, flags,		\
+ 			   p_start, p_end, p_nid)			\
+ 	for (i = 0, __next_mem_range(&i, nid, flags, type_a, type_b,	\
+ 				     p_start, p_end, p_nid);		\
+ 	     i != (u64)ULLONG_MAX;					\
+ 	     __next_mem_range(&i, nid, flags, type_a, type_b,		\
+ 			      p_start, p_end, p_nid))
+ 
+ /**
+  * for_each_mem_range_rev - reverse iterate through memblock areas from
+  * type_a and not included in type_b. Or just type_a if type_b is NULL.
+  * @i: u64 used as loop variable
+  * @type_a: ptr to memblock_type to iterate
+  * @type_b: ptr to memblock_type which excludes from the iteration
+  * @nid: node selector, %NUMA_NO_NODE for all nodes
+  * @flags: pick from blocks based on memory attributes
+  * @p_start: ptr to phys_addr_t for start address of the range, can be %NULL
+  * @p_end: ptr to phys_addr_t for end address of the range, can be %NULL
+  * @p_nid: ptr to int for nid of the range, can be %NULL
+  */
+ #define for_each_mem_range_rev(i, type_a, type_b, nid, flags,		\
+ 			       p_start, p_end, p_nid)			\
+ 	for (i = (u64)ULLONG_MAX,					\
+ 		     __next_mem_range_rev(&i, nid, flags, type_a, type_b,\
+ 					 p_start, p_end, p_nid);	\
+ 	     i != (u64)ULLONG_MAX;					\
+ 	     __next_mem_range_rev(&i, nid, flags, type_a, type_b,	\
+ 				  p_start, p_end, p_nid))
+ 
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  #ifdef CONFIG_MOVABLE_NODE
  static inline bool memblock_is_hotpluggable(struct memblock_region *m)
  {
@@@ -91,7 -163,14 +165,12 @@@ static inline bool movable_node_is_enab
  }
  #endif
  
+ static inline bool memblock_is_mirror(struct memblock_region *m)
+ {
+ 	return m->flags & MEMBLOCK_MIRROR;
+ }
+ 
  #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 -int memblock_search_pfn_nid(unsigned long pfn, unsigned long *start_pfn,
 -			    unsigned long  *end_pfn);
  void __next_mem_pfn_range(int *idx, int nid, unsigned long *out_start_pfn,
  			  unsigned long *out_end_pfn, int *out_nid);
  
diff --cc mm/memblock.c
index 4158cb85f9e7,1b444c730846..000000000000
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@@ -240,8 -265,21 +246,26 @@@ phys_addr_t __init_memblock memblock_fi
  					phys_addr_t end, phys_addr_t size,
  					phys_addr_t align)
  {
++<<<<<<< HEAD
 +	return memblock_find_in_range_node(size, align, start, end,
 +					    NUMA_NO_NODE);
++=======
+ 	phys_addr_t ret;
+ 	ulong flags = choose_memblock_flags();
+ 
+ again:
+ 	ret = memblock_find_in_range_node(size, align, start, end,
+ 					    NUMA_NO_NODE, flags);
+ 
+ 	if (!ret && (flags & MEMBLOCK_MIRROR)) {
+ 		pr_warn("Could not allocate %pap bytes of mirrored memory\n",
+ 			&size);
+ 		flags &= ~MEMBLOCK_MIRROR;
+ 		goto again;
+ 	}
+ 
+ 	return ret;
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  }
  
  static void __init_memblock memblock_remove_region(struct memblock_type *type, unsigned long r)
@@@ -735,9 -805,27 +759,28 @@@ int __init_memblock memblock_clear_hotp
  }
  
  /**
++<<<<<<< HEAD
 + * __next_free_mem_range - next function for for_each_free_mem_range()
++=======
+  * memblock_mark_mirror - Mark mirrored memory with flag MEMBLOCK_MIRROR.
+  * @base: the base phys addr of the region
+  * @size: the size of the region
+  *
+  * Return 0 on succees, -errno on failure.
+  */
+ int __init_memblock memblock_mark_mirror(phys_addr_t base, phys_addr_t size)
+ {
+ 	system_has_some_mirror = true;
+ 
+ 	return memblock_setclr_flag(base, size, 1, MEMBLOCK_MIRROR);
+ }
+ 
+ 
+ /**
+  * __next__mem_range - next function for for_each_free_mem_range() etc.
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
   * @idx: pointer to u64 loop variable
   * @nid: node selector, %NUMA_NO_NODE for all nodes
 - * @flags: pick from blocks based on memory attributes
 - * @type_a: pointer to memblock_type from where the range is taken
 - * @type_b: pointer to memblock_type which excludes memory from being taken
   * @out_start: ptr to phys_addr_t for start address of the range, can be %NULL
   * @out_end: ptr to phys_addr_t for end address of the range, can be %NULL
   * @out_nid: ptr to int for nid of the range, can be %NULL
@@@ -757,36 -845,65 +800,70 @@@
   * As both region arrays are sorted, the function advances the two indices
   * in lockstep and returns each intersection.
   */
 -void __init_memblock __next_mem_range(u64 *idx, int nid, ulong flags,
 -				      struct memblock_type *type_a,
 -				      struct memblock_type *type_b,
 -				      phys_addr_t *out_start,
 -				      phys_addr_t *out_end, int *out_nid)
 -{
 -	int idx_a = *idx & 0xffffffff;
 -	int idx_b = *idx >> 32;
 -
 -	if (WARN_ONCE(nid == MAX_NUMNODES,
 -	"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
 -		nid = NUMA_NO_NODE;
 -
 -	for (; idx_a < type_a->cnt; idx_a++) {
 -		struct memblock_region *m = &type_a->regions[idx_a];
 -
 +void __init_memblock __next_free_mem_range(u64 *idx, int nid,
 +					   phys_addr_t *out_start,
 +					   phys_addr_t *out_end, int *out_nid)
 +{
 +	struct memblock_type *mem = &memblock.memory;
 +	struct memblock_type *rsv = &memblock.reserved;
 +	int mi = *idx & 0xffffffff;
 +	int ri = *idx >> 32;
 +	bool check_node = (nid != NUMA_NO_NODE) && (nid != MAX_NUMNODES);
 +
 +	if (nid == MAX_NUMNODES)
 +		pr_warn_once("%s: Usage of MAX_NUMNODES is depricated. Use NUMA_NO_NODE instead\n",
 +			     __func__);
 +
 +	for ( ; mi < mem->cnt; mi++) {
 +		struct memblock_region *m = &mem->regions[mi];
  		phys_addr_t m_start = m->base;
  		phys_addr_t m_end = m->base + m->size;
 -		int	    m_nid = memblock_get_region_node(m);
  
  		/* only memory regions are associated with nodes, check it */
 -		if (nid != NUMA_NO_NODE && nid != m_nid)
 +		if (check_node && nid != memblock_get_region_node(m))
  			continue;
  
 -		/* skip hotpluggable memory regions if needed */
 -		if (movable_node_is_enabled() && memblock_is_hotpluggable(m))
 -			continue;
 +		/* scan areas before each reservation for intersection */
 +		for ( ; ri < rsv->cnt + 1; ri++) {
 +			struct memblock_region *r = &rsv->regions[ri];
 +			phys_addr_t r_start = ri ? r[-1].base + r[-1].size : 0;
 +			phys_addr_t r_end = ri < rsv->cnt ? r->base : ULLONG_MAX;
  
++<<<<<<< HEAD
 +			/* if ri advanced past mi, break out to advance mi */
++=======
+ 		/* if we want mirror memory skip non-mirror memory regions */
+ 		if ((flags & MEMBLOCK_MIRROR) && !memblock_is_mirror(m))
+ 			continue;
+ 
+ 		if (!type_b) {
+ 			if (out_start)
+ 				*out_start = m_start;
+ 			if (out_end)
+ 				*out_end = m_end;
+ 			if (out_nid)
+ 				*out_nid = m_nid;
+ 			idx_a++;
+ 			*idx = (u32)idx_a | (u64)idx_b << 32;
+ 			return;
+ 		}
+ 
+ 		/* scan areas before each reservation */
+ 		for (; idx_b < type_b->cnt + 1; idx_b++) {
+ 			struct memblock_region *r;
+ 			phys_addr_t r_start;
+ 			phys_addr_t r_end;
+ 
+ 			r = &type_b->regions[idx_b];
+ 			r_start = idx_b ? r[-1].base + r[-1].size : 0;
+ 			r_end = idx_b < type_b->cnt ?
+ 				r->base : ULLONG_MAX;
+ 
+ 			/*
+ 			 * if idx_b advanced past idx_a,
+ 			 * break out to advance idx_a
+ 			 */
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  			if (r_start >= m_end)
  				break;
  			/* if the two regions intersect, we're done */
@@@ -862,13 -982,37 +939,46 @@@ void __init_memblock __next_free_mem_ra
  		if (movable_node_is_enabled() && memblock_is_hotpluggable(m))
  			continue;
  
++<<<<<<< HEAD
 +		/* scan areas before each reservation for intersection */
 +		for ( ; ri >= 0; ri--) {
 +			struct memblock_region *r = &rsv->regions[ri];
 +			phys_addr_t r_start = ri ? r[-1].base + r[-1].size : 0;
 +			phys_addr_t r_end = ri < rsv->cnt ? r->base : ULLONG_MAX;
++=======
+ 		/* if we want mirror memory skip non-mirror memory regions */
+ 		if ((flags & MEMBLOCK_MIRROR) && !memblock_is_mirror(m))
+ 			continue;
+ 
+ 		if (!type_b) {
+ 			if (out_start)
+ 				*out_start = m_start;
+ 			if (out_end)
+ 				*out_end = m_end;
+ 			if (out_nid)
+ 				*out_nid = m_nid;
+ 			idx_a++;
+ 			*idx = (u32)idx_a | (u64)idx_b << 32;
+ 			return;
+ 		}
+ 
+ 		/* scan areas before each reservation */
+ 		for (; idx_b >= 0; idx_b--) {
+ 			struct memblock_region *r;
+ 			phys_addr_t r_start;
+ 			phys_addr_t r_end;
+ 
+ 			r = &type_b->regions[idx_b];
+ 			r_start = idx_b ? r[-1].base + r[-1].size : 0;
+ 			r_end = idx_b < type_b->cnt ?
+ 				r->base : ULLONG_MAX;
+ 			/*
+ 			 * if idx_b advanced past idx_a,
+ 			 * break out to advance idx_a
+ 			 */
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  
 +			/* if ri advanced past mi, break out to advance mi */
  			if (r_end <= m_start)
  				break;
  			/* if the two regions intersect, we're done */
@@@ -977,7 -1138,18 +1087,22 @@@ static phys_addr_t __init memblock_allo
  
  phys_addr_t __init memblock_alloc_nid(phys_addr_t size, phys_addr_t align, int nid)
  {
++<<<<<<< HEAD
 +	return memblock_alloc_base_nid(size, align, MEMBLOCK_ALLOC_ACCESSIBLE, nid);
++=======
+ 	ulong flags = choose_memblock_flags();
+ 	phys_addr_t ret;
+ 
+ again:
+ 	ret = memblock_alloc_base_nid(size, align, MEMBLOCK_ALLOC_ACCESSIBLE,
+ 				      nid, flags);
+ 
+ 	if (!ret && (flags & MEMBLOCK_MIRROR)) {
+ 		flags &= ~MEMBLOCK_MIRROR;
+ 		goto again;
+ 	}
+ 	return ret;
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  }
  
  phys_addr_t __init __memblock_alloc_base(phys_addr_t size, phys_addr_t align, phys_addr_t max_addr)
@@@ -1012,6 -1185,212 +1137,215 @@@ phys_addr_t __init memblock_alloc_try_n
  	return memblock_alloc_base(size, align, MEMBLOCK_ALLOC_ACCESSIBLE);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * memblock_virt_alloc_internal - allocate boot memory block
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region to allocate (phys address)
+  * @max_addr: the upper bound of the memory region to allocate (phys address)
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * The @min_addr limit is dropped if it can not be satisfied and the allocation
+  * will fall back to memory below @min_addr. Also, allocation may fall back
+  * to any node in the system if the specified node can not
+  * hold the requested memory.
+  *
+  * The allocation is performed from memory region limited by
+  * memblock.current_limit if @max_addr == %BOOTMEM_ALLOC_ACCESSIBLE.
+  *
+  * The memory block is aligned on SMP_CACHE_BYTES if @align == 0.
+  *
+  * The phys address of allocated boot memory block is converted to virtual and
+  * allocated memory is reset to 0.
+  *
+  * In addition, function sets the min_count to 0 using kmemleak_alloc for
+  * allocated boot memory block, so that it is never reported as leaks.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ static void * __init memblock_virt_alloc_internal(
+ 				phys_addr_t size, phys_addr_t align,
+ 				phys_addr_t min_addr, phys_addr_t max_addr,
+ 				int nid)
+ {
+ 	phys_addr_t alloc;
+ 	void *ptr;
+ 	ulong flags = choose_memblock_flags();
+ 
+ 	if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
+ 		nid = NUMA_NO_NODE;
+ 
+ 	/*
+ 	 * Detect any accidental use of these APIs after slab is ready, as at
+ 	 * this moment memblock may be deinitialized already and its
+ 	 * internal data may be destroyed (after execution of free_all_bootmem)
+ 	 */
+ 	if (WARN_ON_ONCE(slab_is_available()))
+ 		return kzalloc_node(size, GFP_NOWAIT, nid);
+ 
+ 	if (!align)
+ 		align = SMP_CACHE_BYTES;
+ 
+ 	if (max_addr > memblock.current_limit)
+ 		max_addr = memblock.current_limit;
+ 
+ again:
+ 	alloc = memblock_find_in_range_node(size, align, min_addr, max_addr,
+ 					    nid, flags);
+ 	if (alloc)
+ 		goto done;
+ 
+ 	if (nid != NUMA_NO_NODE) {
+ 		alloc = memblock_find_in_range_node(size, align, min_addr,
+ 						    max_addr, NUMA_NO_NODE,
+ 						    flags);
+ 		if (alloc)
+ 			goto done;
+ 	}
+ 
+ 	if (min_addr) {
+ 		min_addr = 0;
+ 		goto again;
+ 	}
+ 
+ 	if (flags & MEMBLOCK_MIRROR) {
+ 		flags &= ~MEMBLOCK_MIRROR;
+ 		pr_warn("Could not allocate %pap bytes of mirrored memory\n",
+ 			&size);
+ 		goto again;
+ 	}
+ 
+ 	return NULL;
+ done:
+ 	memblock_reserve(alloc, size);
+ 	ptr = phys_to_virt(alloc);
+ 	memset(ptr, 0, size);
+ 
+ 	/*
+ 	 * The min_count is set to 0 so that bootmem allocated blocks
+ 	 * are never reported as leaks. This is because many of these blocks
+ 	 * are only referred via the physical address which is not
+ 	 * looked up by kmemleak.
+ 	 */
+ 	kmemleak_alloc(ptr, size, 0, 0);
+ 
+ 	return ptr;
+ }
+ 
+ /**
+  * memblock_virt_alloc_try_nid_nopanic - allocate boot memory block
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region from where the allocation
+  *	  is preferred (phys address)
+  * @max_addr: the upper bound of the memory region from where the allocation
+  *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+  *	      allocate only from memory limited by memblock.current_limit value
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * Public version of _memblock_virt_alloc_try_nid_nopanic() which provides
+  * additional debug information (including caller info), if enabled.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ void * __init memblock_virt_alloc_try_nid_nopanic(
+ 				phys_addr_t size, phys_addr_t align,
+ 				phys_addr_t min_addr, phys_addr_t max_addr,
+ 				int nid)
+ {
+ 	memblock_dbg("%s: %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx %pF\n",
+ 		     __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 		     (u64)max_addr, (void *)_RET_IP_);
+ 	return memblock_virt_alloc_internal(size, align, min_addr,
+ 					     max_addr, nid);
+ }
+ 
+ /**
+  * memblock_virt_alloc_try_nid - allocate boot memory block with panicking
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region from where the allocation
+  *	  is preferred (phys address)
+  * @max_addr: the upper bound of the memory region from where the allocation
+  *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+  *	      allocate only from memory limited by memblock.current_limit value
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * Public panicking version of _memblock_virt_alloc_try_nid_nopanic()
+  * which provides debug information (including caller info), if enabled,
+  * and panics if the request can not be satisfied.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ void * __init memblock_virt_alloc_try_nid(
+ 			phys_addr_t size, phys_addr_t align,
+ 			phys_addr_t min_addr, phys_addr_t max_addr,
+ 			int nid)
+ {
+ 	void *ptr;
+ 
+ 	memblock_dbg("%s: %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx %pF\n",
+ 		     __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 		     (u64)max_addr, (void *)_RET_IP_);
+ 	ptr = memblock_virt_alloc_internal(size, align,
+ 					   min_addr, max_addr, nid);
+ 	if (ptr)
+ 		return ptr;
+ 
+ 	panic("%s: Failed to allocate %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx\n",
+ 	      __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 	      (u64)max_addr);
+ 	return NULL;
+ }
+ 
+ /**
+  * __memblock_free_early - free boot memory block
+  * @base: phys starting address of the  boot memory block
+  * @size: size of the boot memory block in bytes
+  *
+  * Free boot memory block previously allocated by memblock_virt_alloc_xx() API.
+  * The freeing memory will not be released to the buddy allocator.
+  */
+ void __init __memblock_free_early(phys_addr_t base, phys_addr_t size)
+ {
+ 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
+ 		     __func__, (u64)base, (u64)base + size - 1,
+ 		     (void *)_RET_IP_);
+ 	kmemleak_free_part(__va(base), size);
+ 	memblock_remove_range(&memblock.reserved, base, size);
+ }
+ 
+ /*
+  * __memblock_free_late - free bootmem block pages directly to buddy allocator
+  * @addr: phys starting address of the  boot memory block
+  * @size: size of the boot memory block in bytes
+  *
+  * This is only useful when the bootmem allocator has already been torn
+  * down, but we are still initializing the system.  Pages are released directly
+  * to the buddy allocator, no bootmem metadata is updated because it is gone.
+  */
+ void __init __memblock_free_late(phys_addr_t base, phys_addr_t size)
+ {
+ 	u64 cursor, end;
+ 
+ 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
+ 		     __func__, (u64)base, (u64)base + size - 1,
+ 		     (void *)_RET_IP_);
+ 	kmemleak_free_part(__va(base), size);
+ 	cursor = PFN_UP(base);
+ 	end = PFN_DOWN(base + size);
+ 
+ 	for (; cursor < end; cursor++) {
+ 		__free_pages_bootmem(pfn_to_page(cursor), 0);
+ 		totalram_pages++;
+ 	}
+ }
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  
  /*
   * Remaining API functions
diff --cc mm/nobootmem.c
index dbdd2db64f47,5258386fa1be..000000000000
--- a/mm/nobootmem.c
+++ b/mm/nobootmem.c
@@@ -42,7 -42,15 +43,19 @@@ static void * __init __alloc_memory_cor
  	if (limit > memblock.current_limit)
  		limit = memblock.current_limit;
  
++<<<<<<< HEAD
 +	addr = memblock_find_in_range_node(size, align, goal, limit, nid);
++=======
+ again:
+ 	addr = memblock_find_in_range_node(size, align, goal, limit, nid,
+ 					   flags);
+ 	if (!addr && (flags & MEMBLOCK_MIRROR)) {
+ 		flags &= ~MEMBLOCK_MIRROR;
+ 		pr_warn("Could not allocate %pap bytes of mirrored memory\n",
+ 			&size);
+ 		goto again;
+ 	}
++>>>>>>> a3f5bafcc04a (mm/memblock: allocate boot time data structures from mirrored memory)
  	if (!addr)
  		return NULL;
  
* Unmerged path include/linux/memblock.h
* Unmerged path mm/memblock.c
* Unmerged path mm/nobootmem.c
