sched/deadline: Remove the sysctl_sched_dl knobs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 1724813d9f2c7ff702b46d3e4a4f6d9b10a8f8c2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/1724813d.failed

Remove the deadline specific sysctls for now. The problem with them is
that the interaction with the exisiting rt knobs is nearly impossible
to get right.

The current (as per before this patch) situation is that the rt and dl
bandwidth is completely separate and we enforce rt+dl < 100%. This is
undesirable because this means that the rt default of 95% leaves us
hardly any room, even though dl tasks are saver than rt tasks.

Another proposed solution was (a discarted patch) to have the dl
bandwidth be a fraction of the rt bandwidth. This is highly
confusing imo.

Furthermore neither proposal is consistent with the situation we
actually want; which is rt tasks ran from a dl server. In which case
the rt bandwidth is a direct subset of dl.

So whichever way we go, the introduction of dl controls at this point
is painful. Therefore remove them and instead share the rt budget.

This means that for now the rt knobs are used for dl admission control
and the dl runtime is accounted against the rt runtime. I realise that
this isn't entirely desirable either; but whatever we do we appear to
need to change the interface later, so better have a small interface
for now.

	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/n/tip-zpyqbqds1r0vyxtxza1e7rdc@git.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 1724813d9f2c7ff702b46d3e4a4f6d9b10a8f8c2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched/sysctl.h
#	kernel/sched/core.c
#	kernel/sched/deadline.c
#	kernel/sched/sched.h
diff --cc include/linux/sched/sysctl.h
index 750624cd6f92,31e0193cb0c5..000000000000
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@@ -100,8 -99,4 +100,11 @@@ extern int sched_rt_handler(struct ctl_
  		void __user *buffer, size_t *lenp,
  		loff_t *ppos);
  
++<<<<<<< HEAD
 +extern int sysctl_numa_balancing(struct ctl_table *table, int write,
 +				 void __user *buffer, size_t *lenp,
 +				 loff_t *ppos);
 +
++=======
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  #endif /* _SCHED_SYSCTL_H */
diff --cc kernel/sched/core.c
index f167fdc57a94,1d33eb8143cc..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -7317,13 -6760,18 +7317,21 @@@ void __init sched_init(void
  		ptr += nr_cpu_ids * sizeof(void **);
  
  #endif /* CONFIG_RT_GROUP_SCHED */
 +	}
++<<<<<<< HEAD
  #ifdef CONFIG_CPUMASK_OFFSTACK
 -		for_each_possible_cpu(i) {
 -			per_cpu(load_balance_mask, i) = (void *)ptr;
 -			ptr += cpumask_size();
 -		}
 -#endif /* CONFIG_CPUMASK_OFFSTACK */
 +	for_each_possible_cpu(i) {
 +		per_cpu(load_balance_mask, i) = (cpumask_var_t)kzalloc_node(
 +			cpumask_size(), GFP_KERNEL, cpu_to_node(i));
  	}
 +#endif /* CONFIG_CPUMASK_OFFSTACK */
++=======
+ 
+ 	init_rt_bandwidth(&def_rt_bandwidth,
+ 			global_rt_period(), global_rt_runtime());
+ 	init_dl_bandwidth(&def_dl_bandwidth,
+ 			global_rt_period(), global_rt_runtime());
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  
  #ifdef CONFIG_SMP
  	init_defrootdomain();
@@@ -7917,24 -7352,13 +7925,32 @@@ static long sched_group_rt_period(struc
  	do_div(rt_period_us, NSEC_PER_USEC);
  	return rt_period_us;
  }
 -#endif /* CONFIG_RT_GROUP_SCHED */
  
++<<<<<<< HEAD
 +static int sched_rt_global_constraints(void)
 +{
 +	u64 runtime, period;
 +	int ret = 0;
 +
 +	if (sysctl_sched_rt_period <= 0)
 +		return -EINVAL;
 +
 +	runtime = global_rt_runtime();
 +	period = global_rt_period();
 +
 +	/*
 +	 * Sanity check on the sysctl variables.
 +	 */
 +	if (runtime > period && runtime != RUNTIME_INF)
 +		return -EINVAL;
 +
++=======
+ #ifdef CONFIG_RT_GROUP_SCHED
+ static int sched_rt_global_constraints(void)
+ {
+ 	int ret = 0;
+ 
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  	mutex_lock(&rt_constraints_mutex);
  	read_lock(&tasklist_lock);
  	ret = __rt_schedulable(NULL, 0, 0);
@@@ -7957,18 -7381,8 +7973,23 @@@ static int sched_rt_can_attach(struct t
  static int sched_rt_global_constraints(void)
  {
  	unsigned long flags;
++<<<<<<< HEAD
 +	int i;
 +
 +	if (sysctl_sched_rt_period <= 0)
 +		return -EINVAL;
 +
 +	/*
 +	 * There's always some RT tasks in the root group
 +	 * -- migration, kstopmachine etc..
 +	 */
 +	if (sysctl_sched_rt_runtime == 0)
 +		return -EBUSY;
 +
++=======
+ 	int i, ret = 0;
+ 
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  	raw_spin_lock_irqsave(&def_rt_bandwidth.rt_runtime_lock, flags);
  	for_each_possible_cpu(i) {
  		struct rt_rq *rt_rq = &cpu_rq(i)->rt;
@@@ -7983,6 -7397,117 +8004,120 @@@
  }
  #endif /* CONFIG_RT_GROUP_SCHED */
  
++<<<<<<< HEAD
++=======
+ static int sched_dl_global_constraints(void)
+ {
+ 	u64 runtime = global_rt_runtime();
+ 	u64 period = global_rt_period();
+ 	u64 new_bw = to_ratio(period, runtime);
+ 	int cpu, ret = 0;
+ 
+ 	/*
+ 	 * Here we want to check the bandwidth not being set to some
+ 	 * value smaller than the currently allocated bandwidth in
+ 	 * any of the root_domains.
+ 	 *
+ 	 * FIXME: Cycling on all the CPUs is overdoing, but simpler than
+ 	 * cycling on root_domains... Discussion on different/better
+ 	 * solutions is welcome!
+ 	 */
+ 	for_each_possible_cpu(cpu) {
+ 		struct dl_bw *dl_b = dl_bw_of(cpu);
+ 
+ 		raw_spin_lock(&dl_b->lock);
+ 		if (new_bw < dl_b->total_bw)
+ 			ret = -EBUSY;
+ 		raw_spin_unlock(&dl_b->lock);
+ 
+ 		if (ret)
+ 			break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void sched_dl_do_global(void)
+ {
+ 	u64 new_bw = -1;
+ 	int cpu;
+ 
+ 	def_dl_bandwidth.dl_period = global_rt_period();
+ 	def_dl_bandwidth.dl_runtime = global_rt_runtime();
+ 
+ 	if (global_rt_runtime() != RUNTIME_INF)
+ 		new_bw = to_ratio(global_rt_period(), global_rt_runtime());
+ 
+ 	/*
+ 	 * FIXME: As above...
+ 	 */
+ 	for_each_possible_cpu(cpu) {
+ 		struct dl_bw *dl_b = dl_bw_of(cpu);
+ 
+ 		raw_spin_lock(&dl_b->lock);
+ 		dl_b->bw = new_bw;
+ 		raw_spin_unlock(&dl_b->lock);
+ 	}
+ }
+ 
+ static int sched_rt_global_validate(void)
+ {
+ 	if (sysctl_sched_rt_period <= 0)
+ 		return -EINVAL;
+ 
+ 	if (sysctl_sched_rt_runtime > sysctl_sched_rt_period)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static void sched_rt_do_global(void)
+ {
+ 	def_rt_bandwidth.rt_runtime = global_rt_runtime();
+ 	def_rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());
+ }
+ 
+ int sched_rt_handler(struct ctl_table *table, int write,
+ 		void __user *buffer, size_t *lenp,
+ 		loff_t *ppos)
+ {
+ 	int old_period, old_runtime;
+ 	static DEFINE_MUTEX(mutex);
+ 	int ret;
+ 
+ 	mutex_lock(&mutex);
+ 	old_period = sysctl_sched_rt_period;
+ 	old_runtime = sysctl_sched_rt_runtime;
+ 
+ 	ret = proc_dointvec(table, write, buffer, lenp, ppos);
+ 
+ 	if (!ret && write) {
+ 		ret = sched_rt_global_validate();
+ 		if (ret)
+ 			goto undo;
+ 
+ 		ret = sched_rt_global_constraints();
+ 		if (ret)
+ 			goto undo;
+ 
+ 		ret = sched_dl_global_constraints();
+ 		if (ret)
+ 			goto undo;
+ 
+ 		sched_rt_do_global();
+ 		sched_dl_do_global();
+ 	}
+ 	if (0) {
+ undo:
+ 		sysctl_sched_rt_period = old_period;
+ 		sysctl_sched_rt_runtime = old_runtime;
+ 	}
+ 	mutex_unlock(&mutex);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  int sched_rr_handler(struct ctl_table *table, int write,
  		void __user *buffer, size_t *lenp,
  		loff_t *ppos)
@@@ -8002,50 -7527,20 +8137,53 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
 +int sched_rt_handler(struct ctl_table *table, int write,
 +		void __user *buffer, size_t *lenp,
 +		loff_t *ppos)
 +{
 +	int ret;
 +	int old_period, old_runtime;
 +	static DEFINE_MUTEX(mutex);
 +
 +	mutex_lock(&mutex);
 +	old_period = sysctl_sched_rt_period;
 +	old_runtime = sysctl_sched_rt_runtime;
 +
 +	ret = proc_dointvec(table, write, buffer, lenp, ppos);
 +
 +	if (!ret && write) {
 +		ret = sched_rt_global_constraints();
 +		if (ret) {
 +			sysctl_sched_rt_period = old_period;
 +			sysctl_sched_rt_runtime = old_runtime;
 +		} else {
 +			def_rt_bandwidth.rt_runtime = global_rt_runtime();
 +			def_rt_bandwidth.rt_period =
 +				ns_to_ktime(global_rt_period());
 +		}
 +	}
 +	mutex_unlock(&mutex);
 +
 +	return ret;
 +}
 +
++=======
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  #ifdef CONFIG_CGROUP_SCHED
  
 -static inline struct task_group *css_tg(struct cgroup_subsys_state *css)
 +/* return corresponding task_group object of a cgroup */
 +static inline struct task_group *cgroup_tg(struct cgroup *cgrp)
  {
 -	return css ? container_of(css, struct task_group, css) : NULL;
 +	return container_of(cgroup_subsys_state(cgrp, cpu_cgroup_subsys_id),
 +			    struct task_group, css);
  }
  
 -static struct cgroup_subsys_state *
 -cpu_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)
 +static struct cgroup_subsys_state *cpu_cgroup_css_alloc(struct cgroup *cgrp)
  {
 -	struct task_group *parent = css_tg(parent_css);
 -	struct task_group *tg;
 +	struct task_group *tg, *parent;
  
 -	if (!parent) {
 +	if (!cgrp->parent) {
  		/* This is early initialization for the top cgroup */
  		return &root_task_group.css;
  	}
diff --cc kernel/sched/sched.h
index b976abe32e72,890339099550..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -102,6 -144,47 +102,50 @@@ struct rt_bandwidth 
  	u64			rt_runtime;
  	struct hrtimer		rt_period_timer;
  };
++<<<<<<< HEAD
++=======
+ /*
+  * To keep the bandwidth of -deadline tasks and groups under control
+  * we need some place where:
+  *  - store the maximum -deadline bandwidth of the system (the group);
+  *  - cache the fraction of that bandwidth that is currently allocated.
+  *
+  * This is all done in the data structure below. It is similar to the
+  * one used for RT-throttling (rt_bandwidth), with the main difference
+  * that, since here we are only interested in admission control, we
+  * do not decrease any runtime while the group "executes", neither we
+  * need a timer to replenish it.
+  *
+  * With respect to SMP, the bandwidth is given on a per-CPU basis,
+  * meaning that:
+  *  - dl_bw (< 100%) is the bandwidth of the system (group) on each CPU;
+  *  - dl_total_bw array contains, in the i-eth element, the currently
+  *    allocated bandwidth on the i-eth CPU.
+  * Moreover, groups consume bandwidth on each CPU, while tasks only
+  * consume bandwidth on the CPU they're running on.
+  * Finally, dl_total_bw_cpu is used to cache the index of dl_total_bw
+  * that will be shown the next time the proc or cgroup controls will
+  * be red. It on its turn can be changed by writing on its own
+  * control.
+  */
+ struct dl_bandwidth {
+ 	raw_spinlock_t dl_runtime_lock;
+ 	u64 dl_runtime;
+ 	u64 dl_period;
+ };
+ 
+ static inline int dl_bandwidth_enabled(void)
+ {
+ 	return sysctl_sched_rt_runtime >= 0;
+ }
+ 
+ extern struct dl_bw *dl_bw_of(int i);
+ 
+ struct dl_bw {
+ 	raw_spinlock_t lock;
+ 	u64 bw, total_bw;
+ };
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  
  extern struct mutex sched_domains_mutex;
  
@@@ -833,8 -950,6 +877,11 @@@ static inline u64 global_rt_runtime(voi
  	return (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;
  }
  
++<<<<<<< HEAD
 +
 +
++=======
++>>>>>>> 1724813d9f2c (sched/deadline: Remove the sysctl_sched_dl knobs)
  static inline int task_current(struct rq *rq, struct task_struct *p)
  {
  	return rq->curr == p;
* Unmerged path kernel/sched/deadline.c
* Unmerged path include/linux/sched/sysctl.h
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/deadline.c
* Unmerged path kernel/sched/sched.h
