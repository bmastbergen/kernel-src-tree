ext2: call dax_pfn_mkwrite() for DAX fsync/msync

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Ross Zwisler <ross.zwisler@linux.intel.com>
commit 80b4adcafc076d4179431656b7e83afea99ddec8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/80b4adca.failed

To properly support the new DAX fsync/msync infrastructure filesystems
need to call dax_pfn_mkwrite() so that DAX can track when user pages are
dirtied.

	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: "J. Bruce Fields" <bfields@fieldses.org>
	Cc: "Theodore Ts'o" <tytso@mit.edu>
	Cc: Alexander Viro <viro@zeniv.linux.org.uk>
	Cc: Andreas Dilger <adilger.kernel@dilger.ca>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jan Kara <jack@suse.com>
	Cc: Jeff Layton <jlayton@poochiereds.net>
	Cc: Matthew Wilcox <willy@linux.intel.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 80b4adcafc076d4179431656b7e83afea99ddec8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ext2/file.c
diff --cc fs/ext2/file.c
index a5b3a5db3120,2c88d683cd91..000000000000
--- a/fs/ext2/file.c
+++ b/fs/ext2/file.c
@@@ -25,6 -26,122 +25,125 @@@
  #include "xattr.h"
  #include "acl.h"
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_FS_DAX
+ /*
+  * The lock ordering for ext2 DAX fault paths is:
+  *
+  * mmap_sem (MM)
+  *   sb_start_pagefault (vfs, freeze)
+  *     ext2_inode_info->dax_sem
+  *       address_space->i_mmap_rwsem or page_lock (mutually exclusive in DAX)
+  *         ext2_inode_info->truncate_mutex
+  *
+  * The default page_lock and i_size verification done by non-DAX fault paths
+  * is sufficient because ext2 doesn't support hole punching.
+  */
+ static int ext2_dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+ {
+ 	struct inode *inode = file_inode(vma->vm_file);
+ 	struct ext2_inode_info *ei = EXT2_I(inode);
+ 	int ret;
+ 
+ 	if (vmf->flags & FAULT_FLAG_WRITE) {
+ 		sb_start_pagefault(inode->i_sb);
+ 		file_update_time(vma->vm_file);
+ 	}
+ 	down_read(&ei->dax_sem);
+ 
+ 	ret = __dax_fault(vma, vmf, ext2_get_block, NULL);
+ 
+ 	up_read(&ei->dax_sem);
+ 	if (vmf->flags & FAULT_FLAG_WRITE)
+ 		sb_end_pagefault(inode->i_sb);
+ 	return ret;
+ }
+ 
+ static int ext2_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
+ 						pmd_t *pmd, unsigned int flags)
+ {
+ 	struct inode *inode = file_inode(vma->vm_file);
+ 	struct ext2_inode_info *ei = EXT2_I(inode);
+ 	int ret;
+ 
+ 	if (flags & FAULT_FLAG_WRITE) {
+ 		sb_start_pagefault(inode->i_sb);
+ 		file_update_time(vma->vm_file);
+ 	}
+ 	down_read(&ei->dax_sem);
+ 
+ 	ret = __dax_pmd_fault(vma, addr, pmd, flags, ext2_get_block, NULL);
+ 
+ 	up_read(&ei->dax_sem);
+ 	if (flags & FAULT_FLAG_WRITE)
+ 		sb_end_pagefault(inode->i_sb);
+ 	return ret;
+ }
+ 
+ static int ext2_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
+ {
+ 	struct inode *inode = file_inode(vma->vm_file);
+ 	struct ext2_inode_info *ei = EXT2_I(inode);
+ 	int ret;
+ 
+ 	sb_start_pagefault(inode->i_sb);
+ 	file_update_time(vma->vm_file);
+ 	down_read(&ei->dax_sem);
+ 
+ 	ret = __dax_mkwrite(vma, vmf, ext2_get_block, NULL);
+ 
+ 	up_read(&ei->dax_sem);
+ 	sb_end_pagefault(inode->i_sb);
+ 	return ret;
+ }
+ 
+ static int ext2_dax_pfn_mkwrite(struct vm_area_struct *vma,
+ 		struct vm_fault *vmf)
+ {
+ 	struct inode *inode = file_inode(vma->vm_file);
+ 	struct ext2_inode_info *ei = EXT2_I(inode);
+ 	loff_t size;
+ 	int ret;
+ 
+ 	sb_start_pagefault(inode->i_sb);
+ 	file_update_time(vma->vm_file);
+ 	down_read(&ei->dax_sem);
+ 
+ 	/* check that the faulting page hasn't raced with truncate */
+ 	size = (i_size_read(inode) + PAGE_SIZE - 1) >> PAGE_SHIFT;
+ 	if (vmf->pgoff >= size)
+ 		ret = VM_FAULT_SIGBUS;
+ 	else
+ 		ret = dax_pfn_mkwrite(vma, vmf);
+ 
+ 	up_read(&ei->dax_sem);
+ 	sb_end_pagefault(inode->i_sb);
+ 	return ret;
+ }
+ 
+ static const struct vm_operations_struct ext2_dax_vm_ops = {
+ 	.fault		= ext2_dax_fault,
+ 	.pmd_fault	= ext2_dax_pmd_fault,
+ 	.page_mkwrite	= ext2_dax_mkwrite,
+ 	.pfn_mkwrite	= ext2_dax_pfn_mkwrite,
+ };
+ 
+ static int ext2_file_mmap(struct file *file, struct vm_area_struct *vma)
+ {
+ 	if (!IS_DAX(file_inode(file)))
+ 		return generic_file_mmap(file, vma);
+ 
+ 	file_accessed(file);
+ 	vma->vm_ops = &ext2_dax_vm_ops;
+ 	vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
+ 	return 0;
+ }
+ #else
+ #define ext2_file_mmap	generic_file_mmap
+ #endif
+ 
++>>>>>>> 80b4adcafc07 (ext2: call dax_pfn_mkwrite() for DAX fsync/msync)
  /*
   * Called when filp is released. This happens when all file descriptors
   * for a single struct file are closed. Note that different open() calls
* Unmerged path fs/ext2/file.c
