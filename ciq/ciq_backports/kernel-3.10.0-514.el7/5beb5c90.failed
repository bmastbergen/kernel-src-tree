rhashtable: use cond_resched()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 5beb5c90c1f54d745da040aa05634a5830ba4a4c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5beb5c90.failed

If a hash table has 128 slots and 16384 elems, expand to 256 slots
takes more than one second. For larger sets, a soft lockup is detected.

Holding cpu for that long, even in a work queue is a show stopper
for non preemptable kernels.

cond_resched() at strategic points to allow process scheduler
to reschedule us.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 5beb5c90c1f54d745da040aa05634a5830ba4a4c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rhashtable.c
diff --cc lib/rhashtable.c
index 6d0c4774001c,b5344ef4c684..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -233,16 -412,12 +234,21 @@@ int rhashtable_expand(struct rhashtabl
  				break;
  			}
  		}
++<<<<<<< HEAD
++=======
+ 		unlock_buckets(new_tbl, old_tbl, new_hash);
+ 		cond_resched();
++>>>>>>> 5beb5c90c1f5 (rhashtable: use cond_resched())
  	}
  
 +	/* Publish the new table pointer. Lookups may now traverse
 +	 * the new table, but they will not benefit from any
 +	 * additional efficiency until later steps unzip the buckets.
 +	 */
 +	rcu_assign_pointer(ht->tbl, new_tbl);
 +
  	/* Unzip interleaved hash chains */
 -	while (!complete && !ht->being_destroyed) {
 +	do {
  		/* Wait for readers. All new readers will see the new
  		 * table, and thus no references to the old table will
  		 * remain.
@@@ -254,13 -429,19 +260,19 @@@
  		 * table): ...
  		 */
  		complete = true;
 -		for (old_hash = 0; old_hash < old_tbl->size; old_hash++) {
 -			lock_buckets(new_tbl, old_tbl, old_hash);
 -
 -			if (hashtable_chain_unzip(ht, new_tbl, old_tbl,
 -						  old_hash))
 +		for (i = 0; i < old_tbl->size; i++) {
 +			hashtable_chain_unzip(ht, new_tbl, old_tbl, i);
 +			if (old_tbl->buckets[i] != NULL)
  				complete = false;
++<<<<<<< HEAD
++=======
+ 
+ 			unlock_buckets(new_tbl, old_tbl, old_hash);
+ 			cond_resched();
++>>>>>>> 5beb5c90c1f5 (rhashtable: use cond_resched())
  		}
 -	}
 +	} while (!complete);
  
 -	rcu_assign_pointer(ht->tbl, new_tbl);
  	synchronize_rcu();
  
  	bucket_table_free(old_tbl);
@@@ -285,24 -472,31 +297,35 @@@ int rhashtable_shrink(struct rhashtabl
  
  	ASSERT_RHT_MUTEX(ht);
  
 -	new_tbl = bucket_table_alloc(ht, tbl->size / 2);
 -	if (new_tbl == NULL)
 +	if (ht->shift <= ht->p.min_shift)
 +		return 0;
 +
 +	ntbl = bucket_table_alloc(tbl->size / 2);
 +	if (ntbl == NULL)
  		return -ENOMEM;
  
 -	rcu_assign_pointer(ht->future_tbl, new_tbl);
 -	synchronize_rcu();
 +	ht->shift--;
  
 -	/* Link the first entry in the old bucket to the end of the
 -	 * bucket in the new table. As entries are concurrently being
 -	 * added to the new table, lock down the new bucket. As we
 -	 * always divide the size in half when shrinking, each bucket
 -	 * in the new table maps to exactly two buckets in the old
 -	 * table.
 +	/* Link each bucket in the new table to the first bucket
 +	 * in the old table that contains entries which will hash
 +	 * to the new bucket.
  	 */
 -	for (new_hash = 0; new_hash < new_tbl->size; new_hash++) {
 -		lock_buckets(new_tbl, tbl, new_hash);
 +	for (i = 0; i < ntbl->size; i++) {
 +		ntbl->buckets[i] = tbl->buckets[i];
 +		RCU_INIT_POINTER(*bucket_tail(ntbl, i),
 +				 tbl->buckets[i + ntbl->size]);
  
++<<<<<<< HEAD
++=======
+ 		rcu_assign_pointer(*bucket_tail(new_tbl, new_hash),
+ 				   tbl->buckets[new_hash]);
+ 		ASSERT_BUCKET_LOCK(ht, tbl, new_hash + new_tbl->size);
+ 		rcu_assign_pointer(*bucket_tail(new_tbl, new_hash),
+ 				   tbl->buckets[new_hash + new_tbl->size]);
+ 
+ 		unlock_buckets(new_tbl, tbl, new_hash);
+ 		cond_resched();
++>>>>>>> 5beb5c90c1f5 (rhashtable: use cond_resched())
  	}
  
  	/* Publish the new, valid hash table */
* Unmerged path lib/rhashtable.c
