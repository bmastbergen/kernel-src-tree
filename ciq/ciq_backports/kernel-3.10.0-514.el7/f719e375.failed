ipvs: drop first packet to redirect conntrack

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Julian Anastasov <ja@ssi.bg>
commit f719e3754ee2f7275437e61a6afd520181fdd43b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f719e375.failed

Jiri Bohac is reporting for a problem where the attempt
to reschedule existing connection to another real server
needs proper redirect for the conntrack used by the IPVS
connection. For example, when IPVS connection is created
to NAT-ed real server we alter the reply direction of
conntrack. If we later decide to select different real
server we can not alter again the conntrack. And if we
expire the old connection, the new connection is left
without conntrack.

So, the only way to redirect both the IPVS connection and
the Netfilter's conntrack is to drop the SYN packet that
hits existing connection, to wait for the next jiffie
to expire the old connection and its conntrack and to rely
on client's retransmission to create new connection as
usually.

Jiri Bohac provided a fix that drops all SYNs on rescheduling,
I extended his patch to do such drops only for connections
that use conntrack. Here is the original report from Jiri Bohac:

Since commit dc7b3eb900aa ("ipvs: Fix reuse connection if real server
is dead"), new connections to dead servers are redistributed
immediately to new servers.  The old connection is expired using
ip_vs_conn_expire_now() which sets the connection timer to expire
immediately.

However, before the timer callback, ip_vs_conn_expire(), is run
to clean the connection's conntrack entry, the new redistributed
connection may already be established and its conntrack removed
instead.

Fix this by dropping the first packet of the new connection
instead, like we do when the destination server is not available.
The timer will have deleted the old conntrack entry long before
the first packet of the new connection is retransmitted.

Fixes: dc7b3eb900aa ("ipvs: Fix reuse connection if real server is dead")
	Signed-off-by: Jiri Bohac <jbohac@suse.cz>
	Signed-off-by: Julian Anastasov <ja@ssi.bg>
	Signed-off-by: Simon Horman <horms@verge.net.au>
(cherry picked from commit f719e3754ee2f7275437e61a6afd520181fdd43b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/ip_vs.h
diff --cc include/net/ip_vs.h
index cd8fbdafdc98,a6cc576fd467..000000000000
--- a/include/net/ip_vs.h
+++ b/include/net/ip_vs.h
@@@ -1604,14 -1586,29 +1604,35 @@@ static inline int ip_vs_confirm_conntra
  static inline void ip_vs_conn_drop_conntrack(struct ip_vs_conn *cp)
  {
  }
 -#endif /* CONFIG_IP_VS_NFCT */
 +/* CONFIG_IP_VS_NFCT */
 +#endif
  
++<<<<<<< HEAD
 +static inline unsigned int
++=======
+ /* Really using conntrack? */
+ static inline bool ip_vs_conn_uses_conntrack(struct ip_vs_conn *cp,
+ 					     struct sk_buff *skb)
+ {
+ #ifdef CONFIG_IP_VS_NFCT
+ 	enum ip_conntrack_info ctinfo;
+ 	struct nf_conn *ct;
+ 
+ 	if (!(cp->flags & IP_VS_CONN_F_NFCT))
+ 		return false;
+ 	ct = nf_ct_get(skb, &ctinfo);
+ 	if (ct && !nf_ct_is_untracked(ct))
+ 		return true;
+ #endif
+ 	return false;
+ }
+ 
+ static inline int
++>>>>>>> f719e3754ee2 (ipvs: drop first packet to redirect conntrack)
  ip_vs_dest_conn_overhead(struct ip_vs_dest *dest)
  {
 -	/* We think the overhead of processing active connections is 256
 +	/*
 +	 * We think the overhead of processing active connections is 256
  	 * times higher than that of inactive connections in average. (This
  	 * 256 times might not be accurate, we will change it later) We
  	 * use the following formula to estimate the overhead now:
* Unmerged path include/net/ip_vs.h
diff --git a/net/netfilter/ipvs/ip_vs_core.c b/net/netfilter/ipvs/ip_vs_core.c
index f9e5f557d613..29d69913bd90 100644
--- a/net/netfilter/ipvs/ip_vs_core.c
+++ b/net/netfilter/ipvs/ip_vs_core.c
@@ -1650,15 +1650,34 @@ ip_vs_in(unsigned int hooknum, struct sk_buff *skb, int af)
 	cp = pp->conn_in_get(af, skb, &iph, 0);
 
 	conn_reuse_mode = sysctl_conn_reuse_mode(ipvs);
-	if (conn_reuse_mode && !iph.fragoffs &&
-	    is_new_conn(skb, &iph) && cp &&
-	    ((unlikely(sysctl_expire_nodest_conn(ipvs)) && cp->dest &&
-	      unlikely(!atomic_read(&cp->dest->weight))) ||
-	     unlikely(is_new_conn_expected(cp, conn_reuse_mode)))) {
-		if (!atomic_read(&cp->n_control))
-			ip_vs_conn_expire_now(cp);
-		__ip_vs_conn_put(cp);
-		cp = NULL;
+	if (conn_reuse_mode && !iph.fragoffs && is_new_conn(skb, &iph) && cp) {
+		bool uses_ct = false, resched = false;
+
+		if (unlikely(sysctl_expire_nodest_conn(ipvs)) && cp->dest &&
+		    unlikely(!atomic_read(&cp->dest->weight))) {
+			resched = true;
+			uses_ct = ip_vs_conn_uses_conntrack(cp, skb);
+		} else if (is_new_conn_expected(cp, conn_reuse_mode)) {
+			uses_ct = ip_vs_conn_uses_conntrack(cp, skb);
+			if (!atomic_read(&cp->n_control)) {
+				resched = true;
+			} else {
+				/* Do not reschedule controlling connection
+				 * that uses conntrack while it is still
+				 * referenced by controlled connection(s).
+				 */
+				resched = !uses_ct;
+			}
+		}
+
+		if (resched) {
+			if (!atomic_read(&cp->n_control))
+				ip_vs_conn_expire_now(cp);
+			__ip_vs_conn_put(cp);
+			if (uses_ct)
+				return NF_DROP;
+			cp = NULL;
+		}
 	}
 
 	if (unlikely(!cp) && !iph.fragoffs) {
