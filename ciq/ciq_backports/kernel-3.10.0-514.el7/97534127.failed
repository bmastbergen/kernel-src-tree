mm/hugetlb: use pmd_page() in follow_huge_pmd()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] hugetlb: use pmd_page() in follow_huge_pmd() (Tomoaki Nishimura) [1287322]
Rebuild_FUZZ: 96.70%
commit-author Gerald Schaefer <gerald.schaefer@de.ibm.com>
commit 97534127012f0e396eddea4691f4c9b170aed74b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/97534127.failed

Commit 61f77eda9bbf ("mm/hugetlb: reduce arch dependent code around
follow_huge_*") broke follow_huge_pmd() on s390, where pmd and pte
layout differ and using pte_page() on a huge pmd will return wrong
results.  Using pmd_page() instead fixes this.

All architectures that were touched by that commit have pmd_page()
defined, so this should not break anything on other architectures.

Fixes: 61f77eda "mm/hugetlb: reduce arch dependent code around follow_huge_*"
	Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
	Acked-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Michal Hocko <mhocko@suse.cz>, Andrea Arcangeli <aarcange@redhat.com>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Acked-by: David Rientjes <rientjes@google.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 97534127012f0e396eddea4691f4c9b170aed74b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index af04714f89b5,caad3c5a926f..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -3706,15 -3706,51 +3706,46 @@@ pte_t *huge_pte_offset(struct mm_struc
  	return (pte_t *) pmd;
  }
  
 -#endif /* CONFIG_ARCH_WANT_GENERAL_HUGETLB */
 -
 -/*
 - * These functions are overwritable if your architecture needs its own
 - * behavior.
 - */
 -struct page * __weak
 -follow_huge_addr(struct mm_struct *mm, unsigned long address,
 -			      int write)
 -{
 -	return ERR_PTR(-EINVAL);
 -}
 -
 -struct page * __weak
 +struct page *
  follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 -		pmd_t *pmd, int flags)
 +		pmd_t *pmd, int write)
  {
++<<<<<<< HEAD
 +	struct page *page;
 +
 +	page = pte_page(*(pte_t *)pmd);
 +	if (page)
 +		page += ((address & ~PMD_MASK) >> PAGE_SHIFT);
++=======
+ 	struct page *page = NULL;
+ 	spinlock_t *ptl;
+ retry:
+ 	ptl = pmd_lockptr(mm, pmd);
+ 	spin_lock(ptl);
+ 	/*
+ 	 * make sure that the address range covered by this pmd is not
+ 	 * unmapped from other threads.
+ 	 */
+ 	if (!pmd_huge(*pmd))
+ 		goto out;
+ 	if (pmd_present(*pmd)) {
+ 		page = pmd_page(*pmd) + ((address & ~PMD_MASK) >> PAGE_SHIFT);
+ 		if (flags & FOLL_GET)
+ 			get_page(page);
+ 	} else {
+ 		if (is_hugetlb_entry_migration(huge_ptep_get((pte_t *)pmd))) {
+ 			spin_unlock(ptl);
+ 			__migration_entry_wait(mm, (pte_t *)pmd, ptl);
+ 			goto retry;
+ 		}
+ 		/*
+ 		 * hwpoisoned entry is treated as no_page_table in
+ 		 * follow_page_mask().
+ 		 */
+ 	}
+ out:
+ 	spin_unlock(ptl);
++>>>>>>> 97534127012f (mm/hugetlb: use pmd_page() in follow_huge_pmd())
  	return page;
  }
  
* Unmerged path mm/hugetlb.c
