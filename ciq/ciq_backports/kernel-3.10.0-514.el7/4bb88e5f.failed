IB/qib: Remove completion queue data structures and functions from qib

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit 4bb88e5f84326ff6343bc64a33040850f45b44d8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/4bb88e5f.failed

Use the completion queue functionality provided by rdmavt.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 4bb88e5f84326ff6343bc64a33040850f45b44d8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/Makefile
#	drivers/infiniband/hw/qib/qib_cq.c
#	drivers/infiniband/hw/qib/qib_verbs.c
#	drivers/infiniband/hw/qib/qib_verbs.h
diff --cc drivers/infiniband/hw/qib/Makefile
index 8a8f892de7df,d78f6889f82f..000000000000
--- a/drivers/infiniband/hw/qib/Makefile
+++ b/drivers/infiniband/hw/qib/Makefile
@@@ -1,8 -1,8 +1,14 @@@
  obj-$(CONFIG_INFINIBAND_QIB) += ib_qib.o
  
++<<<<<<< HEAD
 +ib_qib-y := qib_cq.o qib_diag.o qib_driver.o qib_eeprom.o \
 +	qib_file_ops.o qib_fs.o qib_init.o qib_intr.o qib_keys.o \
 +	qib_mad.o qib_mmap.o qib_mr.o qib_pcie.o qib_pio_copy.o \
++=======
+ ib_qib-y := qib_diag.o qib_driver.o qib_eeprom.o \
+ 	qib_file_ops.o qib_fs.o qib_init.o qib_intr.o \
+ 	qib_mad.o qib_pcie.o qib_pio_copy.o \
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
  	qib_qp.o qib_qsfp.o qib_rc.o qib_ruc.o qib_sdma.o qib_srq.o \
  	qib_sysfs.o qib_twsi.o qib_tx.o qib_uc.o qib_ud.o \
  	qib_user_pages.o qib_user_sdma.o qib_verbs_mcast.o qib_iba7220.o \
diff --cc drivers/infiniband/hw/qib/qib_verbs.c
index c4417a1f33be,a27166b67a78..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.c
+++ b/drivers/infiniband/hw/qib/qib_verbs.c
@@@ -2109,10 -1909,6 +2109,13 @@@ int qib_register_ib_device(struct qib_d
  		init_ibport(ppd + i);
  
  	/* Only need to initialize non-zero fields. */
++<<<<<<< HEAD
 +	spin_lock_init(&dev->qpt_lock);
 +	spin_lock_init(&dev->n_pds_lock);
 +	spin_lock_init(&dev->n_ahs_lock);
 +	spin_lock_init(&dev->n_cqs_lock);
++=======
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
  	spin_lock_init(&dev->n_qps_lock);
  	spin_lock_init(&dev->n_srqs_lock);
  	spin_lock_init(&dev->n_mcast_grps_lock);
@@@ -2251,22 -2020,20 +2254,39 @@@
  	ibdev->post_send = qib_post_send;
  	ibdev->post_recv = qib_post_receive;
  	ibdev->post_srq_recv = qib_post_srq_receive;
++<<<<<<< HEAD
 +	ibdev->create_cq = qib_create_cq;
 +	ibdev->destroy_cq = qib_destroy_cq;
 +	ibdev->resize_cq = qib_resize_cq;
 +	ibdev->poll_cq = qib_poll_cq;
 +	ibdev->req_notify_cq = qib_req_notify_cq;
 +	ibdev->get_dma_mr = qib_get_dma_mr;
 +	ibdev->reg_phys_mr = qib_reg_phys_mr;
 +	ibdev->reg_user_mr = qib_reg_user_mr;
 +	ibdev->dereg_mr = qib_dereg_mr;
 +	ibdev->alloc_mr = qib_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = qib_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = qib_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = qib_alloc_fmr;
 +	ibdev->map_phys_fmr = qib_map_phys_fmr;
 +	ibdev->unmap_fmr = qib_unmap_fmr;
 +	ibdev->dealloc_fmr = qib_dealloc_fmr;
++=======
+ 	ibdev->create_cq = NULL;
+ 	ibdev->destroy_cq = NULL;
+ 	ibdev->resize_cq = NULL;
+ 	ibdev->poll_cq = NULL;
+ 	ibdev->req_notify_cq = NULL;
+ 	ibdev->get_dma_mr = NULL;
+ 	ibdev->reg_user_mr = NULL;
+ 	ibdev->dereg_mr = NULL;
+ 	ibdev->alloc_mr = NULL;
+ 	ibdev->map_mr_sg = NULL;
+ 	ibdev->alloc_fmr = NULL;
+ 	ibdev->map_phys_fmr = NULL;
+ 	ibdev->unmap_fmr = NULL;
+ 	ibdev->dealloc_fmr = NULL;
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
  	ibdev->attach_mcast = qib_multicast_attach;
  	ibdev->detach_mcast = qib_multicast_detach;
  	ibdev->process_mad = qib_process_mad;
@@@ -2283,11 -2050,40 +2303,40 @@@
  	dd->verbs_dev.rdi.driver_f.port_callback = qib_create_port_files;
  	dd->verbs_dev.rdi.driver_f.get_card_name = qib_get_card_name;
  	dd->verbs_dev.rdi.driver_f.get_pci_dev = qib_get_pci_dev;
 -	dd->verbs_dev.rdi.driver_f.check_ah = qib_check_ah;
 -	dd->verbs_dev.rdi.driver_f.notify_new_ah = qib_notify_new_ah;
 -	dd->verbs_dev.rdi.driver_f.alloc_qpn = alloc_qpn;
 -	dd->verbs_dev.rdi.driver_f.qp_priv_alloc = qp_priv_alloc;
 -	dd->verbs_dev.rdi.driver_f.qp_priv_free = qp_priv_free;
 -	dd->verbs_dev.rdi.driver_f.free_all_qps = qib_free_all_qps;
 -	dd->verbs_dev.rdi.driver_f.notify_qp_reset = notify_qp_reset;
 +	dd->verbs_dev.rdi.dparms.props.max_pd = ib_qib_max_pds;
 +	dd->verbs_dev.rdi.flags = (RVT_FLAG_MR_INIT_DRIVER |
 +				   RVT_FLAG_QP_INIT_DRIVER |
 +				   RVT_FLAG_CQ_INIT_DRIVER);
  
++<<<<<<< HEAD
++=======
+ 	dd->verbs_dev.rdi.flags = 0;
+ 
+ 	dd->verbs_dev.rdi.dparms.lkey_table_size = qib_lkey_table_size;
+ 	dd->verbs_dev.rdi.dparms.qp_table_size = ib_qib_qp_table_size;
+ 	dd->verbs_dev.rdi.dparms.qpn_start = 1;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_start = QIB_KD_QP;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_end = QIB_KD_QP; /* Reserve one QP */
+ 	dd->verbs_dev.rdi.dparms.qpn_inc = 1;
+ 	dd->verbs_dev.rdi.dparms.qos_shift = 1;
+ 	dd->verbs_dev.rdi.dparms.nports = dd->num_pports;
+ 	dd->verbs_dev.rdi.dparms.npkeys = qib_get_npkeys(dd);
+ 	dd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;
+ 	snprintf(dd->verbs_dev.rdi.dparms.cq_name,
+ 		 sizeof(dd->verbs_dev.rdi.dparms.cq_name),
+ 		 "qib_cq%d", dd->unit);
+ 
+ 	qib_fill_device_attr(dd);
+ 
+ 	ppd = dd->pport;
+ 	for (i = 0; i < dd->num_pports; i++, ppd++) {
+ 		ctxt = ppd->hw_pidx;
+ 		rvt_init_port(&dd->verbs_dev.rdi,
+ 			      &ppd->ibport_data.rvp,
+ 			      i,
+ 			      dd->rcd[ctxt]->pkeys);
+ 	}
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
  
  	ret = rvt_register_device(&dd->verbs_dev.rdi);
  	if (ret)
diff --cc drivers/infiniband/hw/qib/qib_verbs.h
index ca366073af4f,818ac8717386..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.h
+++ b/drivers/infiniband/hw/qib/qib_verbs.h
@@@ -222,195 -214,7 +217,198 @@@ struct qib_mcast 
  	int n_attached;
  };
  
 +/* Protection domain */
 +struct qib_pd {
 +	struct ib_pd ibpd;
 +	int user;               /* non-zero if created from user space */
 +};
 +
 +/* Address Handle */
 +struct qib_ah {
 +	struct ib_ah ibah;
 +	struct ib_ah_attr attr;
 +	atomic_t refcount;
 +};
 +
 +/*
 + * This structure is used by qib_mmap() to validate an offset
 + * when an mmap() request is made.  The vm_area_struct then uses
 + * this as its vm_private_data.
 + */
 +struct qib_mmap_info {
 +	struct list_head pending_mmaps;
 +	struct ib_ucontext *context;
 +	void *obj;
 +	__u64 offset;
 +	struct kref ref;
 +	unsigned size;
 +};
 +
 +/*
++<<<<<<< HEAD
 + * This structure is used to contain the head pointer, tail pointer,
 + * and completion queue entries as a single memory allocation so
 + * it can be mmap'ed into user space.
 + */
 +struct qib_cq_wc {
 +	u32 head;               /* index of next entry to fill */
 +	u32 tail;               /* index of next ib_poll_cq() entry */
 +	union {
 +		/* these are actually size ibcq.cqe + 1 */
 +		struct ib_uverbs_wc uqueue[0];
 +		struct ib_wc kqueue[0];
 +	};
 +};
 +
 +/*
 + * The completion queue structure.
 + */
 +struct qib_cq {
 +	struct ib_cq ibcq;
 +	struct kthread_work comptask;
 +	struct qib_devdata *dd;
 +	spinlock_t lock; /* protect changes in this struct */
 +	u8 notify;
 +	u8 triggered;
 +	struct qib_cq_wc *queue;
 +	struct qib_mmap_info *ip;
 +};
 +
 +/*
 + * A segment is a linear region of low physical memory.
 + * XXX Maybe we should use phys addr here and kmap()/kunmap().
 + * Used by the verbs layer.
 + */
 +struct qib_seg {
 +	void *vaddr;
 +	size_t length;
 +};
 +
 +/* The number of qib_segs that fit in a page. */
 +#define QIB_SEGSZ     (PAGE_SIZE / sizeof(struct qib_seg))
 +
 +struct qib_segarray {
 +	struct qib_seg segs[QIB_SEGSZ];
 +};
 +
 +struct qib_mregion {
 +	struct ib_pd *pd;       /* shares refcnt of ibmr.pd */
 +	u64 user_base;          /* User's address for this region */
 +	u64 iova;               /* IB start address of this region */
 +	size_t length;
 +	u32 lkey;
 +	u32 offset;             /* offset (bytes) to start of region */
 +	int access_flags;
 +	u32 max_segs;           /* number of qib_segs in all the arrays */
 +	u32 mapsz;              /* size of the map array */
 +	u8  page_shift;         /* 0 - non unform/non powerof2 sizes */
 +	u8  lkey_published;     /* in global table */
 +	struct completion comp; /* complete when refcount goes to zero */
 +	struct rcu_head list;
 +	atomic_t refcount;
 +	struct qib_segarray *map[0];    /* the segments */
 +};
 +
 +/*
 + * These keep track of the copy progress within a memory region.
 + * Used by the verbs layer.
 + */
 +struct qib_sge {
 +	struct qib_mregion *mr;
 +	void *vaddr;            /* kernel virtual address of segment */
 +	u32 sge_length;         /* length of the SGE */
 +	u32 length;             /* remaining length of the segment */
 +	u16 m;                  /* current index: mr->map[m] */
 +	u16 n;                  /* current index: mr->map[m]->segs[n] */
 +};
 +
 +/* Memory region */
 +struct qib_mr {
 +	struct ib_mr ibmr;
 +	struct ib_umem *umem;
 +	struct qib_mregion mr;  /* must be last */
 +};
 +
 +/*
 + * Send work request queue entry.
 + * The size of the sg_list is determined when the QP is created and stored
 + * in qp->s_max_sge.
 + */
 +struct qib_swqe {
 +	struct ib_send_wr wr;   /* don't use wr.sg_list */
 +	u32 psn;                /* first packet sequence number */
 +	u32 lpsn;               /* last packet sequence number */
 +	u32 ssn;                /* send sequence number */
 +	u32 length;             /* total length of data in sg_list */
 +	struct qib_sge sg_list[0];
 +};
 +
 +/*
 + * Receive work request queue entry.
 + * The size of the sg_list is determined when the QP (or SRQ) is created
 + * and stored in qp->r_rq.max_sge (or srq->rq.max_sge).
 + */
 +struct qib_rwqe {
 +	u64 wr_id;
 +	u8 num_sge;
 +	struct ib_sge sg_list[0];
 +};
 +
 +/*
 + * This structure is used to contain the head pointer, tail pointer,
 + * and receive work queue entries as a single memory allocation so
 + * it can be mmap'ed into user space.
 + * Note that the wq array elements are variable size so you can't
 + * just index into the array to get the N'th element;
 + * use get_rwqe_ptr() instead.
 + */
 +struct qib_rwq {
 +	u32 head;               /* new work requests posted to the head */
 +	u32 tail;               /* receives pull requests from here. */
 +	struct qib_rwqe wq[0];
 +};
 +
 +struct qib_rq {
 +	struct qib_rwq *wq;
 +	u32 size;               /* size of RWQE array */
 +	u8 max_sge;
 +	spinlock_t lock /* protect changes in this struct */
 +		____cacheline_aligned_in_smp;
 +};
 +
 +struct qib_srq {
 +	struct ib_srq ibsrq;
 +	struct qib_rq rq;
 +	struct qib_mmap_info *ip;
 +	/* send signal when number of RWQEs < limit */
 +	u32 limit;
 +};
 +
 +struct qib_sge_state {
 +	struct qib_sge *sg_list;      /* next SGE to be used if any */
 +	struct qib_sge sge;   /* progress state for the current SGE */
 +	u32 total_len;
 +	u8 num_sge;
 +};
 +
 +/*
 + * This structure holds the information that the send tasklet needs
 + * to send a RDMA read response or atomic operation.
 + */
 +struct qib_ack_entry {
 +	u8 opcode;
 +	u8 sent;
 +	u32 psn;
 +	u32 lpsn;
 +	union {
 +		struct qib_sge rdma_sge;
 +		u64 atomic_data;
 +	};
 +};
 +
  /*
++=======
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
   * qib specific data structure that will be hidden from rvt after the queue pair
   * is made common.
   */
@@@ -782,12 -311,6 +780,15 @@@ struct qib_ibdev 
  	u32 n_piowait;
  	u32 n_txwait;
  
++<<<<<<< HEAD
 +	u32 n_pds_allocated;    /* number of PDs allocated for device */
 +	spinlock_t n_pds_lock;
 +	u32 n_ahs_allocated;    /* number of AHs allocated for device */
 +	spinlock_t n_ahs_lock;
 +	u32 n_cqs_allocated;    /* number of CQs allocated for device */
 +	spinlock_t n_cqs_lock;
++=======
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
  	u32 n_qps_allocated;    /* number of QPs allocated for device */
  	spinlock_t n_qps_lock;
  	u32 n_srqs_allocated;   /* number of SRQs allocated for device */
@@@ -816,34 -339,9 +817,38 @@@ struct qib_verbs_counters 
  	u32 vl15_dropped;
  };
  
++<<<<<<< HEAD
 +static inline struct qib_mr *to_imr(struct ib_mr *ibmr)
 +{
 +	return container_of(ibmr, struct qib_mr, ibmr);
 +}
 +
 +static inline struct qib_pd *to_ipd(struct ib_pd *ibpd)
 +{
 +	return container_of(ibpd, struct qib_pd, ibpd);
 +}
 +
 +static inline struct qib_ah *to_iah(struct ib_ah *ibah)
 +{
 +	return container_of(ibah, struct qib_ah, ibah);
 +}
 +
 +static inline struct qib_cq *to_icq(struct ib_cq *ibcq)
 +{
 +	return container_of(ibcq, struct qib_cq, ibcq);
 +}
 +
 +static inline struct qib_srq *to_isrq(struct ib_srq *ibsrq)
++=======
+ static inline struct rvt_qp *to_iqp(struct ib_qp *ibqp)
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
 +{
 +	return container_of(ibsrq, struct qib_srq, ibsrq);
 +}
 +
 +static inline struct qib_qp *to_iqp(struct ib_qp *ibqp)
  {
 -	return container_of(ibqp, struct rvt_qp, ibqp);
 +	return container_of(ibqp, struct qib_qp, ibqp);
  }
  
  static inline struct qib_ibdev *to_idev(struct ib_device *ibdev)
@@@ -1017,75 -504,12 +1022,78 @@@ int qib_query_srq(struct ib_srq *ibsrq
  
  int qib_destroy_srq(struct ib_srq *ibsrq);
  
++<<<<<<< HEAD
 +int qib_cq_init(struct qib_devdata *dd);
 +
 +void qib_cq_exit(struct qib_devdata *dd);
 +
 +void qib_cq_enter(struct qib_cq *cq, struct ib_wc *entry, int sig);
 +
 +int qib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *entry);
 +
 +struct ib_cq *qib_create_cq(struct ib_device *ibdev,
 +			    const struct ib_cq_init_attr *attr,
 +			    struct ib_ucontext *context,
 +			    struct ib_udata *udata);
 +
 +int qib_destroy_cq(struct ib_cq *ibcq);
 +
 +int qib_req_notify_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags notify_flags);
 +
 +int qib_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata);
 +
 +struct ib_mr *qib_get_dma_mr(struct ib_pd *pd, int acc);
 +
 +struct ib_mr *qib_reg_phys_mr(struct ib_pd *pd,
 +			      struct ib_phys_buf *buffer_list,
 +			      int num_phys_buf, int acc, u64 *iova_start);
 +
 +struct ib_mr *qib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 +			      u64 virt_addr, int mr_access_flags,
 +			      struct ib_udata *udata);
 +
 +int qib_dereg_mr(struct ib_mr *ibmr);
 +
 +struct ib_mr *qib_alloc_mr(struct ib_pd *pd,
 +			   enum ib_mr_type mr_type,
 +			   u32 max_entries);
 +
 +struct ib_fast_reg_page_list *qib_alloc_fast_reg_page_list(
 +				struct ib_device *ibdev, int page_list_len);
 +
 +void qib_free_fast_reg_page_list(struct ib_fast_reg_page_list *pl);
 +
 +int qib_fast_reg_mr(struct qib_qp *qp, struct ib_send_wr *wr);
 +
 +struct ib_fmr *qib_alloc_fmr(struct ib_pd *pd, int mr_access_flags,
 +			     struct ib_fmr_attr *fmr_attr);
 +
 +int qib_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,
 +		     int list_len, u64 iova);
 +
 +int qib_unmap_fmr(struct list_head *fmr_list);
 +
 +int qib_dealloc_fmr(struct ib_fmr *ibfmr);
 +
 +static inline void qib_get_mr(struct qib_mregion *mr)
 +{
 +	atomic_inc(&mr->refcount);
 +}
 +
++=======
++>>>>>>> 4bb88e5f8432 (IB/qib: Remove completion queue data structures and functions from qib)
  void mr_rcu_callback(struct rcu_head *list);
  
 -static inline void qib_put_ss(struct rvt_sge_state *ss)
 +static inline void qib_put_mr(struct qib_mregion *mr)
 +{
 +	if (unlikely(atomic_dec_and_test(&mr->refcount)))
 +		call_rcu(&mr->list, mr_rcu_callback);
 +}
 +
 +static inline void qib_put_ss(struct qib_sge_state *ss)
  {
  	while (ss->num_sge) {
 -		rvt_put_mr(ss->sge.mr);
 +		qib_put_mr(ss->sge.mr);
  		if (--ss->num_sge)
  			ss->sge = *ss->sg_list++;
  	}
* Unmerged path drivers/infiniband/hw/qib/qib_cq.c
* Unmerged path drivers/infiniband/hw/qib/Makefile
diff --git a/drivers/infiniband/hw/qib/qib.h b/drivers/infiniband/hw/qib/qib.h
index 2c9672d7da79..6e80db86d548 100644
--- a/drivers/infiniband/hw/qib/qib.h
+++ b/drivers/infiniband/hw/qib/qib.h
@@ -1097,8 +1097,6 @@ struct qib_devdata {
 	u16 psxmitwait_check_rate;
 	/* high volume overflow errors defered to tasklet */
 	struct tasklet_struct error_tasklet;
-	/* per device cq worker */
-	struct kthread_worker *worker;
 
 	int assigned_node_id; /* NUMA node closest to HCA */
 };
* Unmerged path drivers/infiniband/hw/qib/qib_cq.c
diff --git a/drivers/infiniband/hw/qib/qib_init.c b/drivers/infiniband/hw/qib/qib_init.c
index 58a08168816a..578f4723c3a3 100644
--- a/drivers/infiniband/hw/qib/qib_init.c
+++ b/drivers/infiniband/hw/qib/qib_init.c
@@ -459,8 +459,6 @@ static int loadtime_init(struct qib_devdata *dd)
 	init_timer(&dd->intrchk_timer);
 	dd->intrchk_timer.function = verify_interrupt;
 	dd->intrchk_timer.data = (unsigned long) dd;
-
-	ret = qib_cq_init(dd);
 done:
 	return ret;
 }
@@ -1435,7 +1433,6 @@ static void cleanup_device_data(struct qib_devdata *dd)
 	}
 	kfree(tmp);
 	kfree(dd->boardname);
-	qib_cq_exit(dd);
 }
 
 /*
diff --git a/drivers/infiniband/hw/qib/qib_qp.c b/drivers/infiniband/hw/qib/qib_qp.c
index cf1dd6e9d434..89e014401b57 100644
--- a/drivers/infiniband/hw/qib/qib_qp.c
+++ b/drivers/infiniband/hw/qib/qib_qp.c
@@ -525,7 +525,7 @@ int qib_error_qp(struct qib_qp *qp, enum ib_wc_status err)
 	if (test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags)) {
 		wc.wr_id = qp->r_wr_id;
 		wc.status = err;
-		qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
+		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);
 	}
 	wc.status = IB_WC_WR_FLUSH_ERR;
 
@@ -548,7 +548,7 @@ int qib_error_qp(struct qib_qp *qp, enum ib_wc_status err)
 			wc.wr_id = get_rwqe_ptr(&qp->r_rq, tail)->wr_id;
 			if (++tail >= qp->r_rq.size)
 				tail = 0;
-			qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
+			rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);
 		}
 		wq->tail = tail;
 
diff --git a/drivers/infiniband/hw/qib/qib_rc.c b/drivers/infiniband/hw/qib/qib_rc.c
index c23ede5294da..319a35df882e 100644
--- a/drivers/infiniband/hw/qib/qib_rc.c
+++ b/drivers/infiniband/hw/qib/qib_rc.c
@@ -1024,7 +1024,7 @@ void qib_rc_send_complete(struct qib_qp *qp, struct qib_ib_header *hdr)
 			wc.opcode = ib_qib_wc_opcode[wqe->wr.opcode];
 			wc.byte_len = wqe->length;
 			wc.qp = &qp->ibqp;
-			qib_cq_enter(to_icq(qp->ibqp.send_cq), &wc, 0);
+			rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &wc, 0);
 		}
 		if (++qp->s_last >= qp->s_size)
 			qp->s_last = 0;
@@ -1080,7 +1080,7 @@ static struct qib_swqe *do_rc_completion(struct qib_qp *qp,
 			wc.opcode = ib_qib_wc_opcode[wqe->wr.opcode];
 			wc.byte_len = wqe->length;
 			wc.qp = &qp->ibqp;
-			qib_cq_enter(to_icq(qp->ibqp.send_cq), &wc, 0);
+			rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &wc, 0);
 		}
 		if (++qp->s_last >= qp->s_size)
 			qp->s_last = 0;
@@ -2046,7 +2046,7 @@ send_last:
 		wc.dlid_path_bits = 0;
 		wc.port_num = 0;
 		/* Signal completion event if the solicited bit is set. */
-		qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc,
+		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc,
 			     (ohdr->bth[0] &
 			      cpu_to_be32(IB_BTH_SOLICITED)) != 0);
 		break;
diff --git a/drivers/infiniband/hw/qib/qib_ruc.c b/drivers/infiniband/hw/qib/qib_ruc.c
index e9132f7a68b0..9035b14bb3fb 100644
--- a/drivers/infiniband/hw/qib/qib_ruc.c
+++ b/drivers/infiniband/hw/qib/qib_ruc.c
@@ -120,7 +120,7 @@ bad_lkey:
 	wc.opcode = IB_WC_RECV;
 	wc.qp = &qp->ibqp;
 	/* Signal solicited completion event. */
-	qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
+	rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc, 1);
 	ret = 0;
 bail:
 	return ret;
@@ -561,8 +561,8 @@ again:
 	wc.sl = qp->remote_ah_attr.sl;
 	wc.port_num = 1;
 	/* Signal completion event if the solicited bit is set. */
-	qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc,
-		       wqe->wr.send_flags & IB_SEND_SOLICITED);
+	rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc,
+		     wqe->wr.send_flags & IB_SEND_SOLICITED);
 
 send_comp:
 	spin_lock_irqsave(&sqp->s_lock, flags);
@@ -804,7 +804,7 @@ void qib_send_complete(struct qib_qp *qp, struct qib_swqe *wqe,
 		wc.qp = &qp->ibqp;
 		if (status == IB_WC_SUCCESS)
 			wc.byte_len = wqe->length;
-		qib_cq_enter(to_icq(qp->ibqp.send_cq), &wc,
+		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &wc,
 			     status != IB_WC_SUCCESS);
 	}
 
diff --git a/drivers/infiniband/hw/qib/qib_uc.c b/drivers/infiniband/hw/qib/qib_uc.c
index bab9aeb5dd9e..66132a057830 100644
--- a/drivers/infiniband/hw/qib/qib_uc.c
+++ b/drivers/infiniband/hw/qib/qib_uc.c
@@ -415,7 +415,7 @@ last_imm:
 		wc.dlid_path_bits = 0;
 		wc.port_num = 0;
 		/* Signal completion event if the solicited bit is set. */
-		qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc,
+		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc,
 			     (ohdr->bth[0] &
 				cpu_to_be32(IB_BTH_SOLICITED)) != 0);
 		break;
diff --git a/drivers/infiniband/hw/qib/qib_ud.c b/drivers/infiniband/hw/qib/qib_ud.c
index 75faa5bd8dd6..9be7b5fb1c36 100644
--- a/drivers/infiniband/hw/qib/qib_ud.c
+++ b/drivers/infiniband/hw/qib/qib_ud.c
@@ -216,7 +216,7 @@ static void qib_ud_loopback(struct qib_qp *sqp, struct qib_swqe *swqe)
 	wc.dlid_path_bits = ah_attr->dlid & ((1 << ppd->lmc) - 1);
 	wc.port_num = qp->port_num;
 	/* Signal completion event if the solicited bit is set. */
-	qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc,
+	rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc,
 		     swqe->wr.send_flags & IB_SEND_SOLICITED);
 	ibp->n_loop_pkts++;
 bail_unlock:
@@ -582,7 +582,7 @@ void qib_ud_rcv(struct qib_ibport *ibp, struct qib_ib_header *hdr,
 		dlid & ((1 << ppd_from_ibp(ibp)->lmc) - 1);
 	wc.port_num = qp->port_num;
 	/* Signal completion event if the solicited bit is set. */
-	qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc,
+	rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc,
 		     (ohdr->bth[0] &
 			cpu_to_be32(IB_BTH_SOLICITED)) != 0);
 	return;
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.h
