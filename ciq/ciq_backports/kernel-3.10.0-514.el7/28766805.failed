mm: refactor do_wp_page - rewrite the unlock flow

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] refactor do_wp_page - rewrite the unlock flow (Eric Sandeen) [1274459]
Rebuild_FUZZ: 95.74%
commit-author Shachar Raindel <raindel@mellanox.com>
commit 28766805275c12c2298883cece3f98505ac764b4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/28766805.failed

When do_wp_page is ending, in several cases it needs to unlock the pages
and ptls it was accessing.

Currently, this logic was "called" by using a goto jump.  This makes
following the control flow of the function harder.  Readability was
further hampered by the unlock case containing large amount of logic
needed only in one of the 3 cases.

Using goto for cleanup is generally allowed.  However, moving the
trivial unlocking flows to the relevant call sites allow deeper
refactoring in the next patch.

	Signed-off-by: Shachar Raindel <raindel@mellanox.com>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Rik van Riel <riel@redhat.com>
	Acked-by: Andi Kleen <ak@linux.intel.com>
	Acked-by: Haggai Eran <haggaie@mellanox.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Peter Feiner <pfeiner@google.com>
	Cc: Michel Lespinasse <walken@google.com>
	Reviewed-by: Michal Hocko <mhocko@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 28766805275c12c2298883cece3f98505ac764b4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,0e28fddafdaf..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2816,11 -2066,10 +2816,15 @@@ static int do_wp_page(struct mm_struct 
  {
  	struct page *old_page, *new_page = NULL;
  	pte_t entry;
++<<<<<<< HEAD
 +	int ret = 0;
 +	int page_mkwrite = 0;
 +	struct page *dirty_page = NULL;
++=======
+ 	int page_copied = 0;
++>>>>>>> 28766805275c (mm: refactor do_wp_page - rewrite the unlock flow)
  	unsigned long mmun_start = 0;	/* For mmu_notifiers */
  	unsigned long mmun_end = 0;	/* For mmu_notifiers */
 -	struct mem_cgroup *memcg;
  
  	old_page = vm_normal_page(vma, address, orig_pte);
  	if (!old_page) {
@@@ -2919,66 -2150,15 +2925,68 @@@
  							 &ptl);
  			if (!pte_same(*page_table, orig_pte)) {
  				unlock_page(old_page);
- 				goto unlock;
+ 				pte_unmap_unlock(page_table, ptl);
+ 				page_cache_release(old_page);
+ 				return 0;
  			}
 +
  			page_mkwrite = 1;
  		}
 +		dirty_page = old_page;
 +		get_page(dirty_page);
 +
 +reuse:
 +		/*
 +		 * Clear the pages cpupid information as the existing
 +		 * information potentially belongs to a now completely
 +		 * unrelated process.
 +		 */
 +		if (old_page)
 +			page_cpupid_xchg_last(old_page, (1 << LAST_CPUPID_SHIFT) - 1);
 +
 +		flush_cache_page(vma, address, pte_pfn(orig_pte));
 +		entry = pte_mkyoung(orig_pte);
 +		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +		if (ptep_set_access_flags(vma, address, page_table, entry,1))
 +			update_mmu_cache(vma, address, page_table);
 +		pte_unmap_unlock(page_table, ptl);
 +		ret |= VM_FAULT_WRITE;
 +
 +		if (!dirty_page)
 +			return ret;
 +
 +		/*
 +		 * Yes, Virginia, this is actually required to prevent a race
 +		 * with clear_page_dirty_for_io() from clearing the page dirty
 +		 * bit after it clear all dirty ptes, but before a racing
 +		 * do_wp_page installs a dirty pte.
 +		 *
 +		 * __do_fault is protected similarly.
 +		 */
 +		if (!page_mkwrite) {
 +			wait_on_page_locked(dirty_page);
 +			set_page_dirty_balance(dirty_page, page_mkwrite);
 +			/* file_update_time outside page_lock */
 +			if (vma->vm_file)
 +				file_update_time(vma->vm_file);
 +		}
 +		put_page(dirty_page);
 +		if (page_mkwrite) {
 +			struct address_space *mapping = dirty_page->mapping;
 +
 +			set_page_dirty(dirty_page);
 +			unlock_page(dirty_page);
 +			page_cache_release(dirty_page);
 +			if (mapping)	{
 +				/*
 +				 * Some device drivers do not set page.mapping
 +				 * but still dirty their pages
 +				 */
 +				balance_dirty_pages_ratelimited(mapping);
 +			}
 +		}
  
 -		return wp_page_reuse(mm, vma, address, page_table, ptl,
 -				     orig_pte, old_page, page_mkwrite, 1);
 +		return ret;
  	}
  
  	/*
@@@ -3068,9 -2250,9 +3076,9 @@@ gotten
  
  		/* Free the old page.. */
  		new_page = old_page;
- 		ret |= VM_FAULT_WRITE;
+ 		page_copied = 1;
  	} else
 -		mem_cgroup_cancel_charge(new_page, memcg);
 +		mem_cgroup_uncharge_page(new_page);
  
  	if (new_page)
  		page_cache_release(new_page);
* Unmerged path mm/memory.c
