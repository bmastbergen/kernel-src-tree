time: Rename timekeeper::tkr to timekeeper::tkr_mono

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 876e78818def2983be55878b21f7152fbaebbd36
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/876e7881.failed

In preparation of adding another tkr field, rename this one to
tkr_mono. Also rename tk_read_base::base_mono to tk_read_base::base,
since the structure is not specific to CLOCK_MONOTONIC and the mono
name got added to the tk_read_base instance.

Lots of trivial churn.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: John Stultz <john.stultz@linaro.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20150319093400.344679419@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 876e78818def2983be55878b21f7152fbaebbd36)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/vdso.c
#	arch/s390/kernel/time.c
#	arch/tile/kernel/time.c
#	arch/x86/kernel/vsyscall_gtod.c
#	arch/x86/kvm/x86.c
#	include/linux/timekeeper_internal.h
#	kernel/time/timekeeping.c
diff --cc arch/arm64/kernel/vdso.c
index 6a389dc1bd49,ec37ab3f524f..000000000000
--- a/arch/arm64/kernel/vdso.c
+++ b/arch/arm64/kernel/vdso.c
@@@ -226,7 -200,7 +226,11 @@@ struct vm_area_struct *get_gate_vma(str
  void update_vsyscall(struct timekeeper *tk)
  {
  	struct timespec xtime_coarse;
++<<<<<<< HEAD
 +	u32 use_syscall = strcmp(tk->clock->name, "arch_sys_counter");
++=======
+ 	u32 use_syscall = strcmp(tk->tkr_mono.clock->name, "arch_sys_counter");
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  	++vdso_data->tb_seq_count;
  	smp_wmb();
@@@ -235,15 -209,15 +239,23 @@@
  	vdso_data->use_syscall			= use_syscall;
  	vdso_data->xtime_coarse_sec		= xtime_coarse.tv_sec;
  	vdso_data->xtime_coarse_nsec		= xtime_coarse.tv_nsec;
 -	vdso_data->wtm_clock_sec		= tk->wall_to_monotonic.tv_sec;
 -	vdso_data->wtm_clock_nsec		= tk->wall_to_monotonic.tv_nsec;
  
  	if (!use_syscall) {
++<<<<<<< HEAD
 +		vdso_data->cs_cycle_last	= tk->clock->cycle_last;
 +		vdso_data->xtime_clock_sec	= tk->xtime_sec;
 +		vdso_data->xtime_clock_nsec	= tk->xtime_nsec;
 +		vdso_data->cs_mult		= tk->mult;
 +		vdso_data->cs_shift		= tk->shift;
 +		vdso_data->wtm_clock_sec	= tk->wall_to_monotonic.tv_sec;
 +		vdso_data->wtm_clock_nsec	= tk->wall_to_monotonic.tv_nsec;
++=======
+ 		vdso_data->cs_cycle_last	= tk->tkr_mono.cycle_last;
+ 		vdso_data->xtime_clock_sec	= tk->xtime_sec;
+ 		vdso_data->xtime_clock_nsec	= tk->tkr_mono.xtime_nsec;
+ 		vdso_data->cs_mult		= tk->tkr_mono.mult;
+ 		vdso_data->cs_shift		= tk->tkr_mono.shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	}
  
  	smp_wmb();
diff --cc arch/s390/kernel/time.c
index 386d37a228bb,170ddd2018b3..000000000000
--- a/arch/s390/kernel/time.c
+++ b/arch/s390/kernel/time.c
@@@ -214,26 -215,39 +214,59 @@@ void update_vsyscall(struct timekeeper 
  {
  	u64 nsecps;
  
++<<<<<<< HEAD
 +	if (tk->clock != &clocksource_tod)
++=======
+ 	if (tk->tkr_mono.clock != &clocksource_tod)
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  		return;
  
  	/* Make userspace gettimeofday spin until we're done. */
  	++vdso_data->tb_update_count;
  	smp_wmb();
++<<<<<<< HEAD
 +	vdso_data->xtime_tod_stamp = tk->clock->cycle_last;
 +	vdso_data->xtime_clock_sec = tk->xtime_sec;
 +	vdso_data->xtime_clock_nsec = tk->xtime_nsec;
 +	vdso_data->wtom_clock_sec =
 +		tk->xtime_sec + tk->wall_to_monotonic.tv_sec;
 +	vdso_data->wtom_clock_nsec = tk->xtime_nsec +
 +		+ (tk->wall_to_monotonic.tv_nsec << tk->shift);
 +	nsecps = (u64) NSEC_PER_SEC << tk->shift;
++=======
+ 	vdso_data->xtime_tod_stamp = tk->tkr_mono.cycle_last;
+ 	vdso_data->xtime_clock_sec = tk->xtime_sec;
+ 	vdso_data->xtime_clock_nsec = tk->tkr_mono.xtime_nsec;
+ 	vdso_data->wtom_clock_sec =
+ 		tk->xtime_sec + tk->wall_to_monotonic.tv_sec;
+ 	vdso_data->wtom_clock_nsec = tk->tkr_mono.xtime_nsec +
+ 		+ ((u64) tk->wall_to_monotonic.tv_nsec << tk->tkr_mono.shift);
+ 	nsecps = (u64) NSEC_PER_SEC << tk->tkr_mono.shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	while (vdso_data->wtom_clock_nsec >= nsecps) {
  		vdso_data->wtom_clock_nsec -= nsecps;
  		vdso_data->wtom_clock_sec++;
  	}
++<<<<<<< HEAD
 +	vdso_data->tk_mult = tk->mult;
 +	vdso_data->tk_shift = tk->shift;
++=======
+ 
+ 	vdso_data->xtime_coarse_sec = tk->xtime_sec;
+ 	vdso_data->xtime_coarse_nsec =
+ 		(long)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ 	vdso_data->wtom_coarse_sec =
+ 		vdso_data->xtime_coarse_sec + tk->wall_to_monotonic.tv_sec;
+ 	vdso_data->wtom_coarse_nsec =
+ 		vdso_data->xtime_coarse_nsec + tk->wall_to_monotonic.tv_nsec;
+ 	while (vdso_data->wtom_coarse_nsec >= NSEC_PER_SEC) {
+ 		vdso_data->wtom_coarse_nsec -= NSEC_PER_SEC;
+ 		vdso_data->wtom_coarse_sec++;
+ 	}
+ 
+ 	vdso_data->tk_mult = tk->tkr_mono.mult;
+ 	vdso_data->tk_shift = tk->tkr_mono.shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	smp_wmb();
  	++vdso_data->tb_update_count;
  }
diff --cc arch/tile/kernel/time.c
index 5ac397ec6986,00178ecf9aea..000000000000
--- a/arch/tile/kernel/time.c
+++ b/arch/tile/kernel/time.c
@@@ -234,6 -235,66 +234,71 @@@ cycles_t ns2cycles(unsigned long nsecs
  	 * We do not have to disable preemption here as each core has the same
  	 * clock frequency.
  	 */
++<<<<<<< HEAD
 +	struct clock_event_device *dev = &__raw_get_cpu_var(tile_timer);
 +	return ((u64)nsecs * dev->mult) >> dev->shift;
++=======
+ 	struct clock_event_device *dev = raw_cpu_ptr(&tile_timer);
+ 
+ 	/*
+ 	 * as in clocksource.h and x86's timer.h, we split the calculation
+ 	 * into 2 parts to avoid unecessary overflow of the intermediate
+ 	 * value. This will not lead to any loss of precision.
+ 	 */
+ 	u64 quot = (u64)nsecs >> dev->shift;
+ 	u64 rem  = (u64)nsecs & ((1ULL << dev->shift) - 1);
+ 	return quot * dev->mult + ((rem * dev->mult) >> dev->shift);
+ }
+ 
+ void update_vsyscall_tz(void)
+ {
+ 	write_seqcount_begin(&vdso_data->tz_seq);
+ 	vdso_data->tz_minuteswest = sys_tz.tz_minuteswest;
+ 	vdso_data->tz_dsttime = sys_tz.tz_dsttime;
+ 	write_seqcount_end(&vdso_data->tz_seq);
+ }
+ 
+ void update_vsyscall(struct timekeeper *tk)
+ {
+ 	if (tk->tkr_mono.clock != &cycle_counter_cs)
+ 		return;
+ 
+ 	write_seqcount_begin(&vdso_data->tb_seq);
+ 
+ 	vdso_data->cycle_last		= tk->tkr_mono.cycle_last;
+ 	vdso_data->mask			= tk->tkr_mono.mask;
+ 	vdso_data->mult			= tk->tkr_mono.mult;
+ 	vdso_data->shift		= tk->tkr_mono.shift;
+ 
+ 	vdso_data->wall_time_sec	= tk->xtime_sec;
+ 	vdso_data->wall_time_snsec	= tk->tkr_mono.xtime_nsec;
+ 
+ 	vdso_data->monotonic_time_sec	= tk->xtime_sec
+ 					+ tk->wall_to_monotonic.tv_sec;
+ 	vdso_data->monotonic_time_snsec	= tk->tkr_mono.xtime_nsec
+ 					+ ((u64)tk->wall_to_monotonic.tv_nsec
+ 						<< tk->tkr_mono.shift);
+ 	while (vdso_data->monotonic_time_snsec >=
+ 					(((u64)NSEC_PER_SEC) << tk->tkr_mono.shift)) {
+ 		vdso_data->monotonic_time_snsec -=
+ 					((u64)NSEC_PER_SEC) << tk->tkr_mono.shift;
+ 		vdso_data->monotonic_time_sec++;
+ 	}
+ 
+ 	vdso_data->wall_time_coarse_sec	= tk->xtime_sec;
+ 	vdso_data->wall_time_coarse_nsec = (long)(tk->tkr_mono.xtime_nsec >>
+ 						 tk->tkr_mono.shift);
+ 
+ 	vdso_data->monotonic_time_coarse_sec =
+ 		vdso_data->wall_time_coarse_sec + tk->wall_to_monotonic.tv_sec;
+ 	vdso_data->monotonic_time_coarse_nsec =
+ 		vdso_data->wall_time_coarse_nsec + tk->wall_to_monotonic.tv_nsec;
+ 
+ 	while (vdso_data->monotonic_time_coarse_nsec >= NSEC_PER_SEC) {
+ 		vdso_data->monotonic_time_coarse_nsec -= NSEC_PER_SEC;
+ 		vdso_data->monotonic_time_coarse_sec++;
+ 	}
+ 
+ 	write_seqcount_end(&vdso_data->tb_seq);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  }
diff --cc arch/x86/kvm/x86.c
index 9891381973d7,d7a300e0147f..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1100,22 -1070,19 +1100,37 @@@ static void update_pvclock_gtod(struct 
  	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
  	u64 boot_ns;
  
++<<<<<<< HEAD
 +	boot_ns = timespec_to_ns(&tk->total_sleep_time)
 +		+ tk->wall_to_monotonic.tv_sec * (u64)NSEC_PER_SEC
 +		+ tk->wall_to_monotonic.tv_nsec
 +		+ tk->xtime_sec * (u64)NSEC_PER_SEC;
++=======
+ 	boot_ns = ktime_to_ns(ktime_add(tk->tkr_mono.base, tk->offs_boot));
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  	write_seqcount_begin(&vdata->seq);
  
  	/* copy pvclock gtod data */
++<<<<<<< HEAD
 +	vdata->clock.vclock_mode	= tk->clock->archdata.vclock_mode;
 +	vdata->clock.cycle_last		= tk->clock->cycle_last;
 +	vdata->clock.mask		= tk->clock->mask;
 +	vdata->clock.mult		= tk->mult;
 +	vdata->clock.shift		= tk->shift;
 +
 +	vdata->boot_ns                  = boot_ns;
 +	vdata->nsec_base		= tk->xtime_nsec;
++=======
+ 	vdata->clock.vclock_mode	= tk->tkr_mono.clock->archdata.vclock_mode;
+ 	vdata->clock.cycle_last		= tk->tkr_mono.cycle_last;
+ 	vdata->clock.mask		= tk->tkr_mono.mask;
+ 	vdata->clock.mult		= tk->tkr_mono.mult;
+ 	vdata->clock.shift		= tk->tkr_mono.shift;
+ 
+ 	vdata->boot_ns			= boot_ns;
+ 	vdata->nsec_base		= tk->tkr_mono.xtime_nsec;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  	write_seqcount_end(&vdata->seq);
  }
diff --cc include/linux/timekeeper_internal.h
index a44b704da541,73df17f1535f..000000000000
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@@ -10,68 -10,88 +10,149 @@@
  #include <linux/jiffies.h>
  #include <linux/time.h>
  
++<<<<<<< HEAD
 +/* Structure holding internal timekeeping values. */
 +struct timekeeper {
 +	/* Current clocksource used for timekeeping. */
 +	struct clocksource	*clock;
 +	/* NTP adjusted clock multiplier */
 +	u32			mult;
 +	/* The shift value of the current clocksource. */
 +	u32			shift;
 +	/* Number of clock cycles in one NTP interval. */
++=======
+ /**
+  * struct tk_read_base - base structure for timekeeping readout
+  * @clock:	Current clocksource used for timekeeping.
+  * @read:	Read function of @clock
+  * @mask:	Bitmask for two's complement subtraction of non 64bit clocks
+  * @cycle_last: @clock cycle value at last update
+  * @mult:	(NTP adjusted) multiplier for scaled math conversion
+  * @shift:	Shift value for scaled math conversion
+  * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+  * @base:	ktime_t (nanoseconds) base time for readout
+  *
+  * This struct has size 56 byte on 64 bit. Together with a seqcount it
+  * occupies a single 64byte cache line.
+  *
+  * The struct is separate from struct timekeeper as it is also used
+  * for a fast NMI safe accessors.
+  */
+ struct tk_read_base {
+ 	struct clocksource	*clock;
+ 	cycle_t			(*read)(struct clocksource *cs);
+ 	cycle_t			mask;
+ 	cycle_t			cycle_last;
+ 	u32			mult;
+ 	u32			shift;
+ 	u64			xtime_nsec;
+ 	ktime_t			base;
+ };
+ 
+ /**
+  * struct timekeeper - Structure holding internal timekeeping values.
+  * @tkr_mono:		The readout base structure for CLOCK_MONOTONIC
+  * @xtime_sec:		Current CLOCK_REALTIME time in seconds
+  * @ktime_sec:		Current CLOCK_MONOTONIC time in seconds
+  * @wall_to_monotonic:	CLOCK_REALTIME to CLOCK_MONOTONIC offset
+  * @offs_real:		Offset clock monotonic -> clock realtime
+  * @offs_boot:		Offset clock monotonic -> clock boottime
+  * @offs_tai:		Offset clock monotonic -> clock tai
+  * @tai_offset:		The current UTC to TAI offset in seconds
+  * @base_raw:		Monotonic raw base time in ktime_t format
+  * @raw_time:		Monotonic raw base time in timespec64 format
+  * @cycle_interval:	Number of clock cycles in one NTP interval
+  * @xtime_interval:	Number of clock shifted nano seconds in one NTP
+  *			interval.
+  * @xtime_remainder:	Shifted nano seconds left over when rounding
+  *			@cycle_interval
+  * @raw_interval:	Raw nano seconds accumulated per NTP interval.
+  * @ntp_error:		Difference between accumulated time and NTP time in ntp
+  *			shifted nano seconds.
+  * @ntp_error_shift:	Shift conversion between clock shifted nano seconds and
+  *			ntp shifted nano seconds.
+  *
+  * Note: For timespec(64) based interfaces wall_to_monotonic is what
+  * we need to add to xtime (or xtime corrected for sub jiffie times)
+  * to get to monotonic time.  Monotonic is pegged at zero at system
+  * boot time, so wall_to_monotonic will be negative, however, we will
+  * ALWAYS keep the tv_nsec part positive so we can use the usual
+  * normalization.
+  *
+  * wall_to_monotonic is moved after resume from suspend for the
+  * monotonic time not to jump. We need to add total_sleep_time to
+  * wall_to_monotonic to get the real boot based time offset.
+  *
+  * wall_to_monotonic is no longer the boot time, getboottime must be
+  * used instead.
+  */
+ struct timekeeper {
+ 	struct tk_read_base	tkr_mono;
+ 	u64			xtime_sec;
+ 	unsigned long		ktime_sec;
+ 	struct timespec64	wall_to_monotonic;
+ 	ktime_t			offs_real;
+ 	ktime_t			offs_boot;
+ 	ktime_t			offs_tai;
+ 	s32			tai_offset;
+ 	ktime_t			base_raw;
+ 	struct timespec64	raw_time;
+ 
+ 	/* The following members are for timekeeping internal use */
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	cycle_t			cycle_interval;
 +	/* Last cycle value (also stored in clock->cycle_last) */
 +	cycle_t			cycle_last;
 +	/* Number of clock shifted nano seconds in one NTP interval. */
  	u64			xtime_interval;
 +	/* shifted nano seconds left over when rounding cycle_interval */
  	s64			xtime_remainder;
 +	/* Raw nano seconds accumulated per NTP interval. */
  	u32			raw_interval;
 +
 +	/* Current CLOCK_REALTIME time in seconds */
 +	u64			xtime_sec;
 +	/* Clock shifted nano seconds */
 +	u64			xtime_nsec;
 +
 +	/* Monotonic base time */
 +	ktime_t                 base_mono;
 +
 +	/* Difference between accumulated time and NTP time in ntp
 +	 * shifted nano seconds. */
 +	s64			ntp_error;
 +	/* Shift conversion between clock shifted nano seconds and
 +	 * ntp shifted nano seconds. */
 +	u32			ntp_error_shift;
 +
 +	/*
 +	 * wall_to_monotonic is what we need to add to xtime (or xtime corrected
 +	 * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
 +	 * at zero at system boot time, so wall_to_monotonic will be negative,
 +	 * however, we will ALWAYS keep the tv_nsec part positive so we can use
 +	 * the usual normalization.
 +	 *
 +	 * wall_to_monotonic is moved after resume from suspend for the
 +	 * monotonic time not to jump. We need to add total_sleep_time to
 +	 * wall_to_monotonic to get the real boot based time offset.
 +	 *
 +	 * - wall_to_monotonic is no longer the boot time, getboottime must be
 +	 * used instead.
 +	 */
 +	struct timespec64	wall_to_monotonic;
 +	/* Offset clock monotonic -> clock realtime */
 +	ktime_t			offs_real;
 +	/* time spent in suspend */
 +	struct timespec64	total_sleep_time;
 +	/* Offset clock monotonic -> clock boottime */
 +	ktime_t			offs_boot;
 +	/* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
 +	struct timespec64	raw_time;
 +	/* The current UTC to TAI offset in seconds */
 +	s32			tai_offset;
 +	/* Offset clock monotonic -> clock tai */
 +	ktime_t			offs_tai;
 +
  	/* The ntp_tick_length() value currently being used.
  	 * This cached copy ensures we consistently apply the tick
  	 * length for an entire tick, as ntp_tick_length may change
diff --cc kernel/time/timekeeping.c
index 7251ed9665bf,1405091f3acb..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -44,22 -68,31 +44,47 @@@ bool __read_mostly persistent_clock_exi
  
  static inline void tk_normalize_xtime(struct timekeeper *tk)
  {
++<<<<<<< HEAD
 +	while (tk->xtime_nsec >= ((u64)NSEC_PER_SEC << tk->shift)) {
 +		tk->xtime_nsec -= (u64)NSEC_PER_SEC << tk->shift;
++=======
+ 	while (tk->tkr_mono.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_mono.shift)) {
+ 		tk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  		tk->xtime_sec++;
  	}
  }
  
++<<<<<<< HEAD
 +static void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)
 +{
 +	tk->xtime_sec = ts->tv_sec;
 +	tk->xtime_nsec = (u64)ts->tv_nsec << tk->shift;
++=======
+ static inline struct timespec64 tk_xtime(struct timekeeper *tk)
+ {
+ 	struct timespec64 ts;
+ 
+ 	ts.tv_sec = tk->xtime_sec;
+ 	ts.tv_nsec = (long)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ 	return ts;
+ }
+ 
+ static void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)
+ {
+ 	tk->xtime_sec = ts->tv_sec;
+ 	tk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  }
  
  static void tk_xtime_add(struct timekeeper *tk, const struct timespec64 *ts)
  {
  	tk->xtime_sec += ts->tv_sec;
++<<<<<<< HEAD
 +	tk->xtime_nsec += (u64)ts->tv_nsec << tk->shift;
++=======
+ 	tk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	tk_normalize_xtime(tk);
  }
  
@@@ -77,21 -110,129 +102,135 @@@ static void tk_set_wall_to_mono(struct 
  	tk->wall_to_monotonic = wtm;
  	set_normalized_timespec64(&tmp, -wtm.tv_sec, -wtm.tv_nsec);
  	tk->offs_real = timespec64_to_ktime(tmp);
 -	tk->offs_tai = ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0));
 +	tk->offs_tai = ktime_sub(tk->offs_real, ktime_set(tk->tai_offset, 0));
  }
  
 -static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)
 +static void tk_set_sleep_time(struct timekeeper *tk, struct timespec64 t)
  {
 -	tk->offs_boot = ktime_add(tk->offs_boot, delta);
 +	/* Verify consistency before modifying */
 +	WARN_ON_ONCE(tk->offs_boot.tv64 != timespec64_to_ktime(tk->total_sleep_time).tv64);
 +
 +	tk->total_sleep_time	= t;
 +	tk->offs_boot		= timespec64_to_ktime(t);
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DEBUG_TIMEKEEPING
+ #define WARNING_FREQ (HZ*300) /* 5 minute rate-limiting */
+ /*
+  * These simple flag variables are managed
+  * without locks, which is racy, but ok since
+  * we don't really care about being super
+  * precise about how many events were seen,
+  * just that a problem was observed.
+  */
+ static int timekeeping_underflow_seen;
+ static int timekeeping_overflow_seen;
+ 
+ /* last_warning is only modified under the timekeeping lock */
+ static long timekeeping_last_warning;
+ 
+ static void timekeeping_check_update(struct timekeeper *tk, cycle_t offset)
+ {
+ 
+ 	cycle_t max_cycles = tk->tkr_mono.clock->max_cycles;
+ 	const char *name = tk->tkr_mono.clock->name;
+ 
+ 	if (offset > max_cycles) {
+ 		printk_deferred("WARNING: timekeeping: Cycle offset (%lld) is larger than allowed by the '%s' clock's max_cycles value (%lld): time overflow danger\n",
+ 				offset, name, max_cycles);
+ 		printk_deferred("         timekeeping: Your kernel is sick, but tries to cope by capping time updates\n");
+ 	} else {
+ 		if (offset > (max_cycles >> 1)) {
+ 			printk_deferred("INFO: timekeeping: Cycle offset (%lld) is larger than the the '%s' clock's 50%% safety margin (%lld)\n",
+ 					offset, name, max_cycles >> 1);
+ 			printk_deferred("      timekeeping: Your kernel is still fine, but is feeling a bit nervous\n");
+ 		}
+ 	}
+ 
+ 	if (timekeeping_underflow_seen) {
+ 		if (jiffies - timekeeping_last_warning > WARNING_FREQ) {
+ 			printk_deferred("WARNING: Underflow in clocksource '%s' observed, time update ignored.\n", name);
+ 			printk_deferred("         Please report this, consider using a different clocksource, if possible.\n");
+ 			printk_deferred("         Your kernel is probably still fine.\n");
+ 			timekeeping_last_warning = jiffies;
+ 		}
+ 		timekeeping_underflow_seen = 0;
+ 	}
+ 
+ 	if (timekeeping_overflow_seen) {
+ 		if (jiffies - timekeeping_last_warning > WARNING_FREQ) {
+ 			printk_deferred("WARNING: Overflow in clocksource '%s' observed, time update capped.\n", name);
+ 			printk_deferred("         Please report this, consider using a different clocksource, if possible.\n");
+ 			printk_deferred("         Your kernel is probably still fine.\n");
+ 			timekeeping_last_warning = jiffies;
+ 		}
+ 		timekeeping_overflow_seen = 0;
+ 	}
+ }
+ 
+ static inline cycle_t timekeeping_get_delta(struct tk_read_base *tkr)
+ {
+ 	cycle_t now, last, mask, max, delta;
+ 	unsigned int seq;
+ 
+ 	/*
+ 	 * Since we're called holding a seqlock, the data may shift
+ 	 * under us while we're doing the calculation. This can cause
+ 	 * false positives, since we'd note a problem but throw the
+ 	 * results away. So nest another seqlock here to atomically
+ 	 * grab the points we are checking with.
+ 	 */
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		now = tkr->read(tkr->clock);
+ 		last = tkr->cycle_last;
+ 		mask = tkr->mask;
+ 		max = tkr->clock->max_cycles;
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	delta = clocksource_delta(now, last, mask);
+ 
+ 	/*
+ 	 * Try to catch underflows by checking if we are seeing small
+ 	 * mask-relative negative values.
+ 	 */
+ 	if (unlikely((~delta & mask) < (mask >> 3))) {
+ 		timekeeping_underflow_seen = 1;
+ 		delta = 0;
+ 	}
+ 
+ 	/* Cap delta value to the max_cycles values to avoid mult overflows */
+ 	if (unlikely(delta > max)) {
+ 		timekeeping_overflow_seen = 1;
+ 		delta = tkr->clock->max_cycles;
+ 	}
+ 
+ 	return delta;
+ }
+ #else
+ static inline void timekeeping_check_update(struct timekeeper *tk, cycle_t offset)
+ {
+ }
+ static inline cycle_t timekeeping_get_delta(struct tk_read_base *tkr)
+ {
+ 	cycle_t cycle_now, delta;
+ 
+ 	/* read clocksource */
+ 	cycle_now = tkr->read(tkr->clock);
+ 
+ 	/* calculate the delta since the last update_wall_time */
+ 	delta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);
+ 
+ 	return delta;
+ }
+ #endif
+ 
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  /**
 - * tk_setup_internals - Set up internals to use clocksource clock.
 + * timekeeper_setup_internals - Set up internals to use clocksource clock.
   *
 - * @tk:		The target timekeeper to setup.
   * @clock:		Pointer to clocksource.
   *
   * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment
@@@ -105,9 -246,11 +244,17 @@@ static void tk_setup_internals(struct t
  	u64 tmp, ntpinterval;
  	struct clocksource *old_clock;
  
++<<<<<<< HEAD
 +	old_clock = tk->clock;
 +	tk->clock = clock;
 +	tk->cycle_last = clock->cycle_last = clock->read(clock);
++=======
+ 	old_clock = tk->tkr_mono.clock;
+ 	tk->tkr_mono.clock = clock;
+ 	tk->tkr_mono.read = clock->read;
+ 	tk->tkr_mono.mask = clock->mask;
+ 	tk->tkr_mono.cycle_last = tk->tkr_mono.read(clock);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  	/* Do the ns -> cycle conversion first, using original mult */
  	tmp = NTP_INTERVAL_LENGTH;
@@@ -131,11 -274,11 +278,19 @@@
  	if (old_clock) {
  		int shift_change = clock->shift - old_clock->shift;
  		if (shift_change < 0)
++<<<<<<< HEAD
 +			tk->xtime_nsec >>= -shift_change;
 +		else
 +			tk->xtime_nsec <<= shift_change;
 +	}
 +	tk->shift = clock->shift;
++=======
+ 			tk->tkr_mono.xtime_nsec >>= -shift_change;
+ 		else
+ 			tk->tkr_mono.xtime_nsec <<= shift_change;
+ 	}
+ 	tk->tkr_mono.shift = clock->shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  	tk->ntp_error = 0;
  	tk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;
@@@ -146,7 -289,7 +301,11 @@@
  	 * active clocksource. These value will be adjusted via NTP
  	 * to counteract clock drifting.
  	 */
++<<<<<<< HEAD
 +	tk->mult = clock->mult;
++=======
+ 	tk->tkr_mono.mult = clock->mult;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	tk->ntp_err_mult = 0;
  }
  
@@@ -187,24 -318,188 +346,204 @@@ static inline s64 timekeeping_get_ns(st
  
  static inline s64 timekeeping_get_ns_raw(struct timekeeper *tk)
  {
++<<<<<<< HEAD
 +	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
 +	s64 nsec;
 +
 +	/* read clocksource: */
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +
 +	/* calculate the delta since the last update_wall_time: */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
++=======
+ 	struct clocksource *clock = tk->tkr_mono.clock;
+ 	cycle_t delta;
+ 	s64 nsec;
+ 
+ 	delta = timekeeping_get_delta(&tk->tkr_mono);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  	/* convert delta to nanoseconds. */
  	nsec = clocksource_cyc2ns(delta, clock->mult, clock->shift);
  
  	/* If arch requires, add in get_arch_timeoffset() */
 -	return nsec + arch_gettimeoffset();
 +	return nsec + get_arch_timeoffset();
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.
+  * @tkr: Timekeeping readout base from which we take the update
+  *
+  * We want to use this from any context including NMI and tracing /
+  * instrumenting the timekeeping code itself.
+  *
+  * So we handle this differently than the other timekeeping accessor
+  * functions which retry when the sequence count has changed. The
+  * update side does:
+  *
+  * smp_wmb();	<- Ensure that the last base[1] update is visible
+  * tkf->seq++;
+  * smp_wmb();	<- Ensure that the seqcount update is visible
+  * update(tkf->base[0], tkr);
+  * smp_wmb();	<- Ensure that the base[0] update is visible
+  * tkf->seq++;
+  * smp_wmb();	<- Ensure that the seqcount update is visible
+  * update(tkf->base[1], tkr);
+  *
+  * The reader side does:
+  *
+  * do {
+  *	seq = tkf->seq;
+  *	smp_rmb();
+  *	idx = seq & 0x01;
+  *	now = now(tkf->base[idx]);
+  *	smp_rmb();
+  * } while (seq != tkf->seq)
+  *
+  * As long as we update base[0] readers are forced off to
+  * base[1]. Once base[0] is updated readers are redirected to base[0]
+  * and the base[1] update takes place.
+  *
+  * So if a NMI hits the update of base[0] then it will use base[1]
+  * which is still consistent. In the worst case this can result is a
+  * slightly wrong timestamp (a few nanoseconds). See
+  * @ktime_get_mono_fast_ns.
+  */
+ static void update_fast_timekeeper(struct tk_read_base *tkr)
+ {
+ 	struct tk_read_base *base = tk_fast_mono.base;
+ 
+ 	/* Force readers off to base[1] */
+ 	raw_write_seqcount_latch(&tk_fast_mono.seq);
+ 
+ 	/* Update base[0] */
+ 	memcpy(base, tkr, sizeof(*base));
+ 
+ 	/* Force readers back to base[0] */
+ 	raw_write_seqcount_latch(&tk_fast_mono.seq);
+ 
+ 	/* Update base[1] */
+ 	memcpy(base + 1, base, sizeof(*base));
+ }
+ 
+ /**
+  * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic
+  *
+  * This timestamp is not guaranteed to be monotonic across an update.
+  * The timestamp is calculated by:
+  *
+  *	now = base_mono + clock_delta * slope
+  *
+  * So if the update lowers the slope, readers who are forced to the
+  * not yet updated second array are still using the old steeper slope.
+  *
+  * tmono
+  * ^
+  * |    o  n
+  * |   o n
+  * |  u
+  * | o
+  * |o
+  * |12345678---> reader order
+  *
+  * o = old slope
+  * u = update
+  * n = new slope
+  *
+  * So reader 6 will observe time going backwards versus reader 5.
+  *
+  * While other CPUs are likely to be able observe that, the only way
+  * for a CPU local observation is when an NMI hits in the middle of
+  * the update. Timestamps taken from that NMI context might be ahead
+  * of the following timestamps. Callers need to be aware of that and
+  * deal with it.
+  */
+ u64 notrace ktime_get_mono_fast_ns(void)
+ {
+ 	struct tk_read_base *tkr;
+ 	unsigned int seq;
+ 	u64 now;
+ 
+ 	do {
+ 		seq = raw_read_seqcount(&tk_fast_mono.seq);
+ 		tkr = tk_fast_mono.base + (seq & 0x01);
+ 		now = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);
+ 
+ 	} while (read_seqcount_retry(&tk_fast_mono.seq, seq));
+ 	return now;
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);
+ 
+ /* Suspend-time cycles value for halted fast timekeeper. */
+ static cycle_t cycles_at_suspend;
+ 
+ static cycle_t dummy_clock_read(struct clocksource *cs)
+ {
+ 	return cycles_at_suspend;
+ }
+ 
+ /**
+  * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.
+  * @tk: Timekeeper to snapshot.
+  *
+  * It generally is unsafe to access the clocksource after timekeeping has been
+  * suspended, so take a snapshot of the readout base of @tk and use it as the
+  * fast timekeeper's readout base while suspended.  It will return the same
+  * number of cycles every time until timekeeping is resumed at which time the
+  * proper readout base for the fast timekeeper will be restored automatically.
+  */
+ static void halt_fast_timekeeper(struct timekeeper *tk)
+ {
+ 	static struct tk_read_base tkr_dummy;
+ 	struct tk_read_base *tkr = &tk->tkr_mono;
+ 
+ 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
+ 	cycles_at_suspend = tkr->read(tkr->clock);
+ 	tkr_dummy.read = dummy_clock_read;
+ 	update_fast_timekeeper(&tkr_dummy);
+ }
+ 
+ #ifdef CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 
+ static inline void update_vsyscall(struct timekeeper *tk)
+ {
+ 	struct timespec xt, wm;
+ 
+ 	xt = timespec64_to_timespec(tk_xtime(tk));
+ 	wm = timespec64_to_timespec(tk->wall_to_monotonic);
+ 	update_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,
+ 			    tk->tkr_mono.cycle_last);
+ }
+ 
+ static inline void old_vsyscall_fixup(struct timekeeper *tk)
+ {
+ 	s64 remainder;
+ 
+ 	/*
+ 	* Store only full nanoseconds into xtime_nsec after rounding
+ 	* it up and add the remainder to the error difference.
+ 	* XXX - This is necessary to avoid small 1ns inconsistnecies caused
+ 	* by truncating the remainder in vsyscalls. However, it causes
+ 	* additional work to be done in timekeeping_adjust(). Once
+ 	* the vsyscall implementations are converted to use xtime_nsec
+ 	* (shifted nanoseconds), and CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 	* users are removed, this can be killed.
+ 	*/
+ 	remainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);
+ 	tk->tkr_mono.xtime_nsec -= remainder;
+ 	tk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;
+ 	tk->ntp_error += remainder << tk->ntp_error_shift;
+ 	tk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;
+ }
+ #else
+ #define old_vsyscall_fixup(tk)
+ #endif
+ 
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
  
  static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
@@@ -272,10 -557,22 +611,29 @@@ static inline void tk_update_ktime_data
  	 *	nsec = base_mono + now();
  	 * ==> base_mono = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec
  	 */
++<<<<<<< HEAD
 +	nsec = (s64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
 +	nsec *= NSEC_PER_SEC;
 +	nsec += tk->wall_to_monotonic.tv_nsec;
 +	tk->base_mono = ns_to_ktime(nsec);
++=======
+ 	seconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
+ 	nsec = (u32) tk->wall_to_monotonic.tv_nsec;
+ 	tk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);
+ 
+ 	/* Update the monotonic raw base */
+ 	tk->base_raw = timespec64_to_ktime(tk->raw_time);
+ 
+ 	/*
+ 	 * The sum of the nanoseconds portions of xtime and
+ 	 * wall_to_monotonic can be greater/equal one second. Take
+ 	 * this into account before updating tk->ktime_sec.
+ 	 */
+ 	nsec += (u32)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ 	if (nsec >= NSEC_PER_SEC)
+ 		seconds++;
+ 	tk->ktime_sec = seconds;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  }
  
  /* must hold timekeeper_lock */
@@@ -292,11 -588,11 +650,15 @@@ static void timekeeping_update(struct t
  	update_vsyscall(tk);
  	update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);
  
 -	if (action & TK_MIRROR)
 -		memcpy(&shadow_timekeeper, &tk_core.timekeeper,
 -		       sizeof(tk_core.timekeeper));
 +	if (action & TK_CLOCK_WAS_SET)
 +		tk->clock_was_set_seq++;
  
++<<<<<<< HEAD
 +	if (action & TK_MIRROR)
 +		memcpy(&shadow_timekeeper, &timekeeper, sizeof(timekeeper));
++=======
+ 	update_fast_timekeeper(&tk->tkr_mono);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  }
  
  /**
@@@ -308,19 -604,18 +670,34 @@@
   */
  static void timekeeping_forward_now(struct timekeeper *tk)
  {
++<<<<<<< HEAD
++=======
+ 	struct clocksource *clock = tk->tkr_mono.clock;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
  	s64 nsec;
  
++<<<<<<< HEAD
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
 +	tk->cycle_last = clock->cycle_last = cycle_now;
 +
 +	tk->xtime_nsec += delta * tk->mult;
 +
 +	/* If arch requires, add in get_arch_timeoffset() */
 +	tk->xtime_nsec += (u64)get_arch_timeoffset() << tk->shift;
++=======
+ 	cycle_now = tk->tkr_mono.read(clock);
+ 	delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
+ 	tk->tkr_mono.cycle_last = cycle_now;
+ 
+ 	tk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;
+ 
+ 	/* If arch requires, add in get_arch_timeoffset() */
+ 	tk->tkr_mono.xtime_nsec += (u64)arch_gettimeoffset() << tk->tkr_mono.shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  	tk_normalize_xtime(tk);
  
@@@ -342,12 -637,12 +719,16 @@@ int __getnstimeofday64(struct timespec6
  	s64 nsecs = 0;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
  		ts->tv_sec = tk->xtime_sec;
++<<<<<<< HEAD
 +		nsecs = timekeeping_get_ns(tk);
++=======
+ 		nsecs = timekeeping_get_ns(&tk->tkr_mono);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	ts->tv_nsec = 0;
  	timespec64_add_ns(ts, nsecs);
@@@ -383,19 -679,84 +764,96 @@@ ktime_t ktime_get(void
  	WARN_ON(timekeeping_suspended);
  
  	do {
++<<<<<<< HEAD
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +		secs = tk->xtime_sec + tk->wall_to_monotonic.tv_sec;
 +		nsecs = timekeeping_get_ns(tk) + tk->wall_to_monotonic.tv_nsec;
++=======
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = tk->tkr_mono.base;
+ 		nsecs = timekeeping_get_ns(&tk->tkr_mono);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 -
 -	return ktime_add_ns(base, nsecs);
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
 +	/*
 +	 * Use ktime_set/ktime_add_ns to create a proper ktime on
 +	 * 32-bit architectures without CONFIG_KTIME_SCALAR.
 +	 */
 +	return ktime_add_ns(ktime_set(secs, 0), nsecs);
  }
  EXPORT_SYMBOL_GPL(ktime_get);
  
++<<<<<<< HEAD
++=======
+ static ktime_t *offsets[TK_OFFS_MAX] = {
+ 	[TK_OFFS_REAL]	= &tk_core.timekeeper.offs_real,
+ 	[TK_OFFS_BOOT]	= &tk_core.timekeeper.offs_boot,
+ 	[TK_OFFS_TAI]	= &tk_core.timekeeper.offs_tai,
+ };
+ 
+ ktime_t ktime_get_with_offset(enum tk_offsets offs)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base, *offset = offsets[offs];
+ 	s64 nsecs;
+ 
+ 	WARN_ON(timekeeping_suspended);
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = ktime_add(tk->tkr_mono.base, *offset);
+ 		nsecs = timekeeping_get_ns(&tk->tkr_mono);
+ 
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ 
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_with_offset);
+ 
+ /**
+  * ktime_mono_to_any() - convert mononotic time to any other time
+  * @tmono:	time to convert.
+  * @offs:	which offset to use
+  */
+ ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)
+ {
+ 	ktime_t *offset = offsets[offs];
+ 	unsigned long seq;
+ 	ktime_t tconv;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		tconv = ktime_add(tmono, *offset);
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return tconv;
+ }
+ EXPORT_SYMBOL_GPL(ktime_mono_to_any);
+ 
+ /**
+  * ktime_get_raw - Returns the raw monotonic time in ktime_t format
+  */
+ ktime_t ktime_get_raw(void)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base;
+ 	s64 nsecs;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = tk->base_raw;
+ 		nsecs = timekeeping_get_ns_raw(tk);
+ 
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_raw);
+ 
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  /**
   * ktime_get_ts64 - get the monotonic clock in timespec64 format
   * @ts:		pointer to timespec variable
@@@ -414,12 -775,12 +872,16 @@@ void ktime_get_ts64(struct timespec64 *
  	WARN_ON(timekeeping_suspended);
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  		ts->tv_sec = tk->xtime_sec;
++<<<<<<< HEAD
 +		nsec = timekeeping_get_ns(tk);
++=======
+ 		nsec = timekeeping_get_ns(&tk->tkr_mono);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  		tomono = tk->wall_to_monotonic;
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	ts->tv_sec += tomono.tv_sec;
  	ts->tv_nsec = 0;
@@@ -500,9 -863,9 +962,13 @@@ void getnstime_raw_and_real(struct time
  		ts_real->tv_nsec = 0;
  
  		nsecs_raw = timekeeping_get_ns_raw(tk);
++<<<<<<< HEAD
 +		nsecs_real = timekeeping_get_ns(tk);
++=======
+ 		nsecs_real = timekeeping_get_ns(&tk->tkr_mono);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	timespec_add_ns(ts_raw, nsecs_raw);
  	timespec_add_ns(ts_real, nsecs_real);
@@@ -684,7 -1046,7 +1150,11 @@@ static int change_clocksource(void *dat
  	 */
  	if (try_module_get(new->owner)) {
  		if (!new->enable || new->enable(new) == 0) {
++<<<<<<< HEAD
 +			old = tk->clock;
++=======
+ 			old = tk->tkr_mono.clock;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  			tk_setup_internals(tk, new);
  			if (old->disable)
  				old->disable(old);
@@@ -710,33 -1072,18 +1180,41 @@@
   */
  int timekeeping_notify(struct clocksource *clock)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	struct timekeeper *tk = &timekeeper;
  
++<<<<<<< HEAD
 +	if (tk->clock == clock)
 +		return 0;
 +	stop_machine(change_clocksource, clock, NULL);
 +	tick_clock_notify();
 +	return tk->clock == clock ? 0 : -1;
++=======
+ 	if (tk->tkr_mono.clock == clock)
+ 		return 0;
+ 	stop_machine(change_clocksource, clock, NULL);
+ 	tick_clock_notify();
+ 	return tk->tkr_mono.clock == clock ? 0 : -1;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
 +}
 +
 +/**
 + * ktime_get_real - get the real (wall-) time in ktime_t format
 + *
 + * returns the time in ktime_t format
 + */
 +ktime_t ktime_get_real(void)
 +{
 +	struct timespec64 now;
 +
 +	getnstimeofday64(&now);
 +
 +	return timespec64_to_ktime(now);
  }
 +EXPORT_SYMBOL_GPL(ktime_get_real);
  
  /**
 - * getrawmonotonic64 - Returns the raw monotonic time in a timespec
 - * @ts:		pointer to the timespec64 to be set
 + * getrawmonotonic - Returns the raw monotonic time in a timespec
 + * @ts:		pointer to the timespec to be set
   *
   * Returns the raw monotonic time (completely un-modified by ntp)
   */
@@@ -769,11 -1117,11 +1247,15 @@@ int timekeeping_valid_for_hres(void
  	int ret;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
++<<<<<<< HEAD
 +		ret = tk->clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;
++=======
+ 		ret = tk->tkr_mono.clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	return ret;
  }
@@@ -788,11 -1136,11 +1270,15 @@@ u64 timekeeping_max_deferment(void
  	u64 ret;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
++<<<<<<< HEAD
 +		ret = tk->clock->max_idle_ns;
++=======
+ 		ret = tk->tkr_mono.clock->max_idle_ns;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	return ret;
  }
@@@ -955,10 -1300,10 +1441,15 @@@ void timekeeping_inject_sleeptime(struc
   * xtime/wall_to_monotonic/jiffies/etc are
   * still managed by arch specific suspend/resume code.
   */
 -void timekeeping_resume(void)
 +static void timekeeping_resume(void)
  {
++<<<<<<< HEAD
 +	struct timekeeper *tk = &timekeeper;
 +	struct clocksource *clock = tk->clock;
++=======
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	struct clocksource *clock = tk->tkr_mono.clock;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	unsigned long flags;
  	struct timespec64 ts_new, ts_delta;
  	struct timespec tmp;
@@@ -986,16 -1331,16 +1477,27 @@@
  	 * The less preferred source will only be tried if there is no better
  	 * usable source. The rtc part is handled separately in rtc core code.
  	 */
++<<<<<<< HEAD
 +	cycle_now = clock->read(clock);
 +	if ((clock->flags & CLOCK_SOURCE_SUSPEND_NONSTOP) &&
 +		cycle_now > clock->cycle_last) {
++=======
+ 	cycle_now = tk->tkr_mono.read(clock);
+ 	if ((clock->flags & CLOCK_SOURCE_SUSPEND_NONSTOP) &&
+ 		cycle_now > tk->tkr_mono.cycle_last) {
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  		u64 num, max = ULLONG_MAX;
  		u32 mult = clock->mult;
  		u32 shift = clock->shift;
  		s64 nsec = 0;
  
++<<<<<<< HEAD
 +		cycle_delta = clocksource_delta(cycle_now, clock->cycle_last,
 +						clock->mask);
++=======
+ 		cycle_delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last,
+ 						tk->tkr_mono.mask);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
  		/*
  		 * "cycle_delta * mutl" may cause 64 bits overflow, if the
@@@ -1021,7 -1366,7 +1523,11 @@@
  		__timekeeping_inject_sleeptime(tk, &ts_delta);
  
  	/* Re-base the last cycle value */
++<<<<<<< HEAD
 +	tk->cycle_last = clock->cycle_last = cycle_now;
++=======
+ 	tk->tkr_mono.cycle_last = cycle_now;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	tk->ntp_error = 0;
  	timekeeping_suspended = 0;
  	timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
@@@ -1171,9 -1519,15 +1677,21 @@@ static __always_inline void timekeeping
  	 *
  	 * XXX - TODO: Doc ntp_error calculation.
  	 */
++<<<<<<< HEAD
 +	tk->mult += mult_adj;
 +	tk->xtime_interval += interval;
 +	tk->xtime_nsec -= offset;
++=======
+ 	if ((mult_adj > 0) && (tk->tkr_mono.mult + mult_adj < mult_adj)) {
+ 		/* NTP adjustment caused clocksource mult overflow */
+ 		WARN_ON_ONCE(1);
+ 		return;
+ 	}
+ 
+ 	tk->tkr_mono.mult += mult_adj;
+ 	tk->xtime_interval += interval;
+ 	tk->tkr_mono.xtime_nsec -= offset;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	tk->ntp_error -= (interval - offset) << tk->ntp_error_shift;
  }
  
@@@ -1235,12 -1589,13 +1753,22 @@@ static void timekeeping_adjust(struct t
  		tk->ntp_err_mult = 0;
  	}
  
++<<<<<<< HEAD
 +	if (unlikely(tk->clock->maxadj &&
 +		(tk->mult > tk->clock->mult + tk->clock->maxadj))) {
 +		printk_once(KERN_WARNING
 +			"Adjusting %s more than 11%% (%ld vs %ld)\n",
 +			tk->clock->name, (long)tk->mult,
 +			(long)tk->clock->mult + tk->clock->maxadj);
++=======
+ 	if (unlikely(tk->tkr_mono.clock->maxadj &&
+ 		(abs(tk->tkr_mono.mult - tk->tkr_mono.clock->mult)
+ 			> tk->tkr_mono.clock->maxadj))) {
+ 		printk_once(KERN_WARNING
+ 			"Adjusting %s more than 11%% (%ld vs %ld)\n",
+ 			tk->tkr_mono.clock->name, (long)tk->tkr_mono.mult,
+ 			(long)tk->tkr_mono.clock->mult + tk->tkr_mono.clock->maxadj);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	}
  
  	/*
@@@ -1257,9 -1612,9 +1785,15 @@@
  	 * We'll correct this error next time through this function, when
  	 * xtime_nsec is not as small.
  	 */
++<<<<<<< HEAD
 +	if (unlikely((s64)tk->xtime_nsec < 0)) {
 +		s64 neg = -(s64)tk->xtime_nsec;
 +		tk->xtime_nsec = 0;
++=======
+ 	if (unlikely((s64)tk->tkr_mono.xtime_nsec < 0)) {
+ 		s64 neg = -(s64)tk->tkr_mono.xtime_nsec;
+ 		tk->tkr_mono.xtime_nsec = 0;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  		tk->ntp_error += neg << tk->ntp_error_shift;
  	}
  }
@@@ -1274,13 -1629,13 +1808,23 @@@
   */
  static inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)
  {
++<<<<<<< HEAD
 +	u64 nsecps = (u64)NSEC_PER_SEC << tk->shift;
 +	unsigned int clock_set = 0;
 +
 +	while (tk->xtime_nsec >= nsecps) {
 +		int leap;
 +
 +		tk->xtime_nsec -= nsecps;
++=======
+ 	u64 nsecps = (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
+ 	unsigned int clock_set = 0;
+ 
+ 	while (tk->tkr_mono.xtime_nsec >= nsecps) {
+ 		int leap;
+ 
+ 		tk->tkr_mono.xtime_nsec -= nsecps;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  		tk->xtime_sec++;
  
  		/* Figure out if its a leap sec and apply if needed */
@@@ -1325,9 -1680,9 +1869,15 @@@ static cycle_t logarithmic_accumulation
  
  	/* Accumulate one shifted interval */
  	offset -= interval;
++<<<<<<< HEAD
 +	tk->cycle_last += interval;
 +
 +	tk->xtime_nsec += tk->xtime_interval << shift;
++=======
+ 	tk->tkr_mono.cycle_last += interval;
+ 
+ 	tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  	*clock_set |= accumulate_nsecs_to_secs(tk);
  
  	/* Accumulate raw time */
@@@ -1400,8 -1725,8 +1950,13 @@@ void update_wall_time(void
  #ifdef CONFIG_ARCH_USES_GETTIMEOFFSET
  	offset = real_tk->cycle_interval;
  #else
++<<<<<<< HEAD
 +	offset = clocksource_delta(clock->read(clock), clock->cycle_last,
 +				   clock->mask);
++=======
+ 	offset = clocksource_delta(tk->tkr_mono.read(tk->tkr_mono.clock),
+ 				   tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  #endif
  
  	/* Check if there's really nothing to do */
@@@ -1617,65 -1872,67 +2172,104 @@@ void do_timer(unsigned long ticks
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * ktime_get_update_offsets_tick - hrtimer helper
+  * @offs_real:	pointer to storage for monotonic -> realtime offset
+  * @offs_boot:	pointer to storage for monotonic -> boottime offset
+  * @offs_tai:	pointer to storage for monotonic -> clock tai offset
+  *
+  * Returns monotonic time at last tick and various offsets
+  */
+ ktime_t ktime_get_update_offsets_tick(ktime_t *offs_real, ktime_t *offs_boot,
+ 							ktime_t *offs_tai)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base;
+ 	u64 nsecs;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 
+ 		base = tk->tkr_mono.base;
+ 		nsecs = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;
+ 
+ 		*offs_real = tk->offs_real;
+ 		*offs_boot = tk->offs_boot;
+ 		*offs_tai = tk->offs_tai;
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ }
+ 
+ #ifdef CONFIG_HIGH_RES_TIMERS
+ /**
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
   * ktime_get_update_offsets_now - hrtimer helper
 + * @cwsseq:	pointer to check and store the clock was set sequence number
   * @offs_real:	pointer to storage for monotonic -> realtime offset
   * @offs_boot:	pointer to storage for monotonic -> boottime offset
 - * @offs_tai:	pointer to storage for monotonic -> clock tai offset
   *
 - * Returns current monotonic time and updates the offsets
 - * Called from hrtimer_interrupt() or retrigger_next_event()
 + * Returns current monotonic time and updates the offsets if the
 + * sequence number in @cwsseq and timekeeper.clock_was_set_seq are
 + * different.
 + *
 + * Called from hrtimer_interupt() or retrigger_next_event()
   */
 -ktime_t ktime_get_update_offsets_now(ktime_t *offs_real, ktime_t *offs_boot,
 -							ktime_t *offs_tai)
 +ktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,
 +				     ktime_t *offs_boot, ktime_t *offs_tai)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	struct timekeeper *tk = &timekeeper;
  	unsigned int seq;
  	ktime_t base;
  	u64 nsecs;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
  
++<<<<<<< HEAD
 +		base = tk->base_mono;
 +		nsecs = timekeeping_get_ns(tk);
 +		base = ktime_add_ns(base, nsecs);
++=======
+ 		base = tk->tkr_mono.base;
+ 		nsecs = timekeeping_get_ns(&tk->tkr_mono);
++>>>>>>> 876e78818def (time: Rename timekeeper::tkr to timekeeper::tkr_mono)
  
 -		*offs_real = tk->offs_real;
 -		*offs_boot = tk->offs_boot;
 -		*offs_tai = tk->offs_tai;
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +		if (*cwsseq != tk->clock_was_set_seq) {
 +			*cwsseq = tk->clock_was_set_seq;
 +			*offs_real = tk->offs_real;
 +			*offs_boot = tk->offs_boot;
 +			*offs_tai = tk->offs_tai;
 +		}
  
 -	return ktime_add_ns(base, nsecs);
 +		/* Handle leapsecond insertion adjustments */
 +		if (unlikely(base.tv64 >= tk->next_leap_ktime.tv64))
 +			*offs_real = ktime_sub(tk->offs_real, ktime_set(1, 0));
 +
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
 +
 +	return base;
  }
 -#endif
 +
 +/**
 + * ktime_get_monotonic_offset() - get wall_to_monotonic in ktime_t format
 + */
 +ktime_t ktime_get_monotonic_offset(void)
 +{
 +	struct timekeeper *tk = &timekeeper;
 +	unsigned long seq;
 +	struct timespec64 wtom;
 +
 +	do {
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +		wtom = tk->wall_to_monotonic;
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
 +
 +	return timespec64_to_ktime(wtom);
 +}
 +EXPORT_SYMBOL_GPL(ktime_get_monotonic_offset);
  
  /**
   * do_adjtimex() - Accessor function to NTP __do_adjtimex function
* Unmerged path arch/x86/kernel/vsyscall_gtod.c
* Unmerged path arch/arm64/kernel/vdso.c
* Unmerged path arch/s390/kernel/time.c
* Unmerged path arch/tile/kernel/time.c
* Unmerged path arch/x86/kernel/vsyscall_gtod.c
* Unmerged path arch/x86/kvm/x86.c
* Unmerged path include/linux/timekeeper_internal.h
* Unmerged path kernel/time/timekeeping.c
