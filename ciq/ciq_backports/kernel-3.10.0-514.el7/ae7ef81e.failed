skbuff: introduce skb_gso_validate_mtu

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
commit ae7ef81ef000adeee7a87585b9135ff8a8064acc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ae7ef81e.failed

skb_gso_network_seglen is not enough for checking fragment sizes if
skb is using GSO_BY_FRAGS as we have to check frag per frag.

This patch introduces skb_gso_validate_mtu, based on the former, which
will wrap the use case inside it as all calls to skb_gso_network_seglen
were to validate if it fits on a given TMU, and improve the check.

	Signed-off-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
	Tested-by: Xin Long <lucien.xin@gmail.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ae7ef81ef000adeee7a87585b9135ff8a8064acc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skbuff.c
#	net/ipv4/ip_forward.c
#	net/ipv4/ip_output.c
#	net/mpls/af_mpls.c
diff --cc net/core/skbuff.c
index a695cad46f55,5ca562b56ec3..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -3885,6 -4392,90 +3885,93 @@@ unsigned int skb_gso_transport_seglen(c
  }
  EXPORT_SYMBOL_GPL(skb_gso_transport_seglen);
  
++<<<<<<< HEAD
++=======
+ /**
+  * skb_gso_validate_mtu - Return in case such skb fits a given MTU
+  *
+  * @skb: GSO skb
+  *
+  * skb_gso_validate_mtu validates if a given skb will fit a wanted MTU
+  * once split.
+  */
+ bool skb_gso_validate_mtu(const struct sk_buff *skb, unsigned int mtu)
+ {
+ 	const struct skb_shared_info *shinfo = skb_shinfo(skb);
+ 	const struct sk_buff *iter;
+ 	unsigned int hlen;
+ 
+ 	hlen = skb_gso_network_seglen(skb);
+ 
+ 	if (shinfo->gso_size != GSO_BY_FRAGS)
+ 		return hlen <= mtu;
+ 
+ 	/* Undo this so we can re-use header sizes */
+ 	hlen -= GSO_BY_FRAGS;
+ 
+ 	skb_walk_frags(skb, iter) {
+ 		if (hlen + skb_headlen(iter) > mtu)
+ 			return false;
+ 	}
+ 
+ 	return true;
+ }
+ EXPORT_SYMBOL_GPL(skb_gso_validate_mtu);
+ 
+ static struct sk_buff *skb_reorder_vlan_header(struct sk_buff *skb)
+ {
+ 	if (skb_cow(skb, skb_headroom(skb)) < 0) {
+ 		kfree_skb(skb);
+ 		return NULL;
+ 	}
+ 
+ 	memmove(skb->data - ETH_HLEN, skb->data - skb->mac_len - VLAN_HLEN,
+ 		2 * ETH_ALEN);
+ 	skb->mac_header += VLAN_HLEN;
+ 	return skb;
+ }
+ 
+ struct sk_buff *skb_vlan_untag(struct sk_buff *skb)
+ {
+ 	struct vlan_hdr *vhdr;
+ 	u16 vlan_tci;
+ 
+ 	if (unlikely(skb_vlan_tag_present(skb))) {
+ 		/* vlan_tci is already set-up so leave this for another time */
+ 		return skb;
+ 	}
+ 
+ 	skb = skb_share_check(skb, GFP_ATOMIC);
+ 	if (unlikely(!skb))
+ 		goto err_free;
+ 
+ 	if (unlikely(!pskb_may_pull(skb, VLAN_HLEN)))
+ 		goto err_free;
+ 
+ 	vhdr = (struct vlan_hdr *)skb->data;
+ 	vlan_tci = ntohs(vhdr->h_vlan_TCI);
+ 	__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);
+ 
+ 	skb_pull_rcsum(skb, VLAN_HLEN);
+ 	vlan_set_encap_proto(skb, vhdr);
+ 
+ 	skb = skb_reorder_vlan_header(skb);
+ 	if (unlikely(!skb))
+ 		goto err_free;
+ 
+ 	skb_reset_network_header(skb);
+ 	skb_reset_transport_header(skb);
+ 	skb_reset_mac_len(skb);
+ 
+ 	return skb;
+ 
+ err_free:
+ 	kfree_skb(skb);
+ 	return NULL;
+ }
+ EXPORT_SYMBOL(skb_vlan_untag);
+ 
++>>>>>>> ae7ef81ef000 (skbuff: introduce skb_gso_validate_mtu)
  int skb_ensure_writable(struct sk_buff *skb, int write_len)
  {
  	if (!pskb_may_pull(skb, write_len))
diff --cc net/ipv4/ip_forward.c
index f2fff640bd27,9f0a7b96646f..000000000000
--- a/net/ipv4/ip_forward.c
+++ b/net/ipv4/ip_forward.c
@@@ -50,7 -44,17 +50,21 @@@ static bool ip_exceeds_mtu(const struc
  	if (skb->len <= mtu)
  		return false;
  
++<<<<<<< HEAD
 +	if (skb_is_gso(skb) && skb_gso_network_seglen(skb) <= mtu)
++=======
+ 	if (unlikely((ip_hdr(skb)->frag_off & htons(IP_DF)) == 0))
+ 		return false;
+ 
+ 	/* original fragment exceeds mtu and DF is set */
+ 	if (unlikely(IPCB(skb)->frag_max_size > mtu))
+ 		return true;
+ 
+ 	if (skb->ignore_df)
+ 		return false;
+ 
+ 	if (skb_is_gso(skb) && skb_gso_validate_mtu(skb, mtu))
++>>>>>>> ae7ef81ef000 (skbuff: introduce skb_gso_validate_mtu)
  		return false;
  
  	return true;
diff --cc net/ipv4/ip_output.c
index 21b11ebed76f,cbac493c913a..000000000000
--- a/net/ipv4/ip_output.c
+++ b/net/ipv4/ip_output.c
@@@ -222,8 -223,10 +222,15 @@@ static int ip_finish_output_gso(struct 
  	struct sk_buff *segs;
  	int ret = 0;
  
++<<<<<<< HEAD
 +	if (skb_gso_network_seglen(skb) <= ip_skb_dst_mtu(skb))
 +		return ip_finish_output2(sk, skb);
++=======
+ 	/* common case: locally created skb or seglen is <= mtu */
+ 	if (((IPCB(skb)->flags & IPSKB_FORWARDED) == 0) ||
+ 	      skb_gso_validate_mtu(skb, mtu))
+ 		return ip_finish_output2(net, sk, skb);
++>>>>>>> ae7ef81ef000 (skbuff: introduce skb_gso_validate_mtu)
  
  	/* Slowpath -  GSO segment length is exceeding the dst MTU.
  	 *
* Unmerged path net/mpls/af_mpls.c
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 72292c04faa6..502d045b4614 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -2724,6 +2724,7 @@ void skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len);
 int skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen);
 void skb_scrub_packet(struct sk_buff *skb, bool xnet);
 unsigned int skb_gso_transport_seglen(const struct sk_buff *skb);
+bool skb_gso_validate_mtu(const struct sk_buff *skb, unsigned int mtu);
 struct sk_buff *skb_segment(struct sk_buff *skb, netdev_features_t features);
 int skb_ensure_writable(struct sk_buff *skb, int write_len);
 int skb_vlan_pop(struct sk_buff *skb);
* Unmerged path net/core/skbuff.c
* Unmerged path net/ipv4/ip_forward.c
* Unmerged path net/ipv4/ip_output.c
diff --git a/net/ipv6/ip6_output.c b/net/ipv6/ip6_output.c
index 6e24dc6ba803..eec59825d1dd 100644
--- a/net/ipv6/ip6_output.c
+++ b/net/ipv6/ip6_output.c
@@ -333,7 +333,7 @@ static bool ip6_pkt_too_big(const struct sk_buff *skb, unsigned int mtu)
 	if (skb->ignore_df)
 		return false;
 
-	if (skb_is_gso(skb) && skb_gso_network_seglen(skb) <= mtu)
+	if (skb_is_gso(skb) && skb_gso_validate_mtu(skb, mtu))
 		return false;
 
 	return true;
* Unmerged path net/mpls/af_mpls.c
