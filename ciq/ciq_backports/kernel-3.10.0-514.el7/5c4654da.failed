i40e/i40evf: Allow up to 12K bytes of data per Tx descriptor instead of 8K

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Alexander Duyck <aduyck@mirantis.com>
commit 5c4654daf2e2f25dfbd7fa572c59937ea6d4198b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5c4654da.failed

From what I can tell the practical limitation on the size of the Tx data
buffer is the fact that the Tx descriptor is limited to 14 bits.  As such
we cannot use 16K as is typically used on the other Intel drivers.  However
artificially limiting ourselves to 8K can be expensive as this means that
we will consume up to 10 descriptors (1 context, 1 for header, and 9 for
payload, non-8K aligned) in a single send.

I propose that we can reduce this by increasing the maximum data for a 4K
aligned block to 12K.  We can reduce the descriptors used for a 32K aligned
block by 1 by increasing the size like this.  In addition we still have the
4K - 1 of space that is still unused.  We can use this as a bit of extra
padding when dealing with data that is not aligned to 4K.

By aligning the descriptors after the first to 4K we can improve the
efficiency of PCIe accesses as we can avoid using byte enables and can fetch
full TLP transactions after the first fetch of the buffer.  This helps to
improve PCIe efficiency.  Below is the results of testing before and after
with this patch:

Recv   Send   Send                         Utilization      Service Demand
Socket Socket Message  Elapsed             Send     Recv    Send    Recv
Size   Size   Size     Time    Throughput  local    remote  local   remote
bytes  bytes  bytes    secs.   10^6bits/s  % S      % U     us/KB   us/KB
Before:
87380  16384  16384    10.00     33682.24  20.27    -1.00   0.592   -1.00
After:
87380  16384  16384    10.00     34204.08  20.54    -1.00   0.590   -1.00

So the net result of this patch is that we have a small gain in throughput
due to a reduction in overhead for putting together the frame.

	Signed-off-by: Alexander Duyck <aduyck@mirantis.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 5c4654daf2e2f25dfbd7fa572c59937ea6d4198b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_fcoe.c
#	drivers/net/ethernet/intel/i40e/i40e_txrx.c
#	drivers/net/ethernet/intel/i40e/i40e_txrx.h
#	drivers/net/ethernet/intel/i40evf/i40e_txrx.c
#	drivers/net/ethernet/intel/i40evf/i40e_txrx.h
diff --cc drivers/net/ethernet/intel/i40e/i40e_fcoe.c
index e61646fcfb11,92d2208d13c7..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_fcoe.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_fcoe.c
@@@ -1371,8 -1367,24 +1371,27 @@@ static netdev_tx_t i40e_fcoe_xmit_frame
  	if (i40e_fcoe_set_skb_header(skb))
  		goto out_drop;
  
++<<<<<<< HEAD
 +	if (!i40e_xmit_descriptor_count(skb, tx_ring))
++=======
+ 	count = i40e_xmit_descriptor_count(skb);
+ 	if (i40e_chk_linearize(skb, count)) {
+ 		if (__skb_linearize(skb))
+ 			goto out_drop;
+ 		count = i40e_txd_use_count(skb->len);
+ 		tx_ring->tx_stats.tx_linearize++;
+ 	}
+ 
+ 	/* need: 1 descriptor per page * PAGE_SIZE/I40E_MAX_DATA_PER_TXD,
+ 	 *       + 1 desc for skb_head_len/I40E_MAX_DATA_PER_TXD,
+ 	 *       + 4 desc gap to avoid the cache line where head is,
+ 	 *       + 1 desc for context descriptor,
+ 	 * otherwise try next time
+ 	 */
+ 	if (i40e_maybe_stop_tx(tx_ring, count + 4 + 1)) {
+ 		tx_ring->tx_stats.tx_busy++;
++>>>>>>> 5c4654daf2e2 (i40e/i40evf: Allow up to 12K bytes of data per Tx descriptor instead of 8K)
  		return NETDEV_TX_BUSY;
 -	}
  
  	/* prepare the xmit flags */
  	if (i40e_tx_prepare_vlan_flags(skb, tx_ring, &tx_flags))
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 5a3abb58e191,9af1411bd423..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@@ -2924,8 -2893,24 +2929,27 @@@ static netdev_tx_t i40e_xmit_frame_ring
  	/* prefetch the data, we'll need it later */
  	prefetch(skb->data);
  
++<<<<<<< HEAD
 +	if (0 == i40e_xmit_descriptor_count(skb, tx_ring))
++=======
+ 	count = i40e_xmit_descriptor_count(skb);
+ 	if (i40e_chk_linearize(skb, count)) {
+ 		if (__skb_linearize(skb))
+ 			goto out_drop;
+ 		count = i40e_txd_use_count(skb->len);
+ 		tx_ring->tx_stats.tx_linearize++;
+ 	}
+ 
+ 	/* need: 1 descriptor per page * PAGE_SIZE/I40E_MAX_DATA_PER_TXD,
+ 	 *       + 1 desc for skb_head_len/I40E_MAX_DATA_PER_TXD,
+ 	 *       + 4 desc gap to avoid the cache line where head is,
+ 	 *       + 1 desc for context descriptor,
+ 	 * otherwise try next time
+ 	 */
+ 	if (i40e_maybe_stop_tx(tx_ring, count + 4 + 1)) {
+ 		tx_ring->tx_stats.tx_busy++;
++>>>>>>> 5c4654daf2e2 (i40e/i40evf: Allow up to 12K bytes of data per Tx descriptor instead of 8K)
  		return NETDEV_TX_BUSY;
 -	}
  
  	/* prepare the xmit flags */
  	if (i40e_tx_prepare_vlan_flags(skb, tx_ring, &tx_flags))
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.h
index ae22c4e9162f,9e654e611642..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
@@@ -353,4 -389,63 +382,66 @@@ static inline u32 i40e_get_head(struct 
  
  	return le32_to_cpu(*(volatile __le32 *)head);
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * i40e_xmit_descriptor_count - calculate number of Tx descriptors needed
+  * @skb:     send buffer
+  * @tx_ring: ring to send buffer on
+  *
+  * Returns number of data descriptors needed for this skb. Returns 0 to indicate
+  * there is not enough descriptors available in this ring since we need at least
+  * one descriptor.
+  **/
+ static inline int i40e_xmit_descriptor_count(struct sk_buff *skb)
+ {
+ 	const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+ 	unsigned int nr_frags = skb_shinfo(skb)->nr_frags;
+ 	int count = 0, size = skb_headlen(skb);
+ 
+ 	for (;;) {
+ 		count += i40e_txd_use_count(size);
+ 
+ 		if (!nr_frags--)
+ 			break;
+ 
+ 		size = skb_frag_size(frag++);
+ 	}
+ 
+ 	return count;
+ }
+ 
+ /**
+  * i40e_maybe_stop_tx - 1st level check for Tx stop conditions
+  * @tx_ring: the ring to be checked
+  * @size:    the size buffer we want to assure is available
+  *
+  * Returns 0 if stop is not needed
+  **/
+ static inline int i40e_maybe_stop_tx(struct i40e_ring *tx_ring, int size)
+ {
+ 	if (likely(I40E_DESC_UNUSED(tx_ring) >= size))
+ 		return 0;
+ 	return __i40e_maybe_stop_tx(tx_ring, size);
+ }
+ 
+ /**
+  * i40e_chk_linearize - Check if there are more than 8 fragments per packet
+  * @skb:      send buffer
+  * @count:    number of buffers used
+  *
+  * Note: Our HW can't scatter-gather more than 8 fragments to build
+  * a packet on the wire and so we need to figure out the cases where we
+  * need to linearize the skb.
+  **/
+ static inline bool i40e_chk_linearize(struct sk_buff *skb, int count)
+ {
+ 	/* we can only support up to 8 data buffers for a single send */
+ 	if (likely(count <= I40E_MAX_BUFFER_TXD))
+ 		return false;
+ 
+ 	return __i40e_chk_linearize(skb);
+ }
++>>>>>>> 5c4654daf2e2 (i40e/i40evf: Allow up to 12K bytes of data per Tx descriptor instead of 8K)
  #endif /* _I40E_TXRX_H_ */
diff --cc drivers/net/ethernet/intel/i40evf/i40e_txrx.c
index f6384b0c8220,5f9c1bbab1fa..000000000000
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
@@@ -2122,8 -2111,24 +2127,27 @@@ static netdev_tx_t i40e_xmit_frame_ring
  	/* prefetch the data, we'll need it later */
  	prefetch(skb->data);
  
++<<<<<<< HEAD
 +	if (0 == i40evf_xmit_descriptor_count(skb, tx_ring))
++=======
+ 	count = i40e_xmit_descriptor_count(skb);
+ 	if (i40e_chk_linearize(skb, count)) {
+ 		if (__skb_linearize(skb))
+ 			goto out_drop;
+ 		count = i40e_txd_use_count(skb->len);
+ 		tx_ring->tx_stats.tx_linearize++;
+ 	}
+ 
+ 	/* need: 1 descriptor per page * PAGE_SIZE/I40E_MAX_DATA_PER_TXD,
+ 	 *       + 1 desc for skb_head_len/I40E_MAX_DATA_PER_TXD,
+ 	 *       + 4 desc gap to avoid the cache line where head is,
+ 	 *       + 1 desc for context descriptor,
+ 	 * otherwise try next time
+ 	 */
+ 	if (i40e_maybe_stop_tx(tx_ring, count + 4 + 1)) {
+ 		tx_ring->tx_stats.tx_busy++;
++>>>>>>> 5c4654daf2e2 (i40e/i40evf: Allow up to 12K bytes of data per Tx descriptor instead of 8K)
  		return NETDEV_TX_BUSY;
 -	}
  
  	/* prepare the xmit flags */
  	if (i40evf_tx_prepare_vlan_flags(skb, tx_ring, &tx_flags))
diff --cc drivers/net/ethernet/intel/i40evf/i40e_txrx.h
index 5467fcdf7670,3ec0ea5ea3db..000000000000
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
@@@ -341,4 -371,63 +370,66 @@@ static inline u32 i40e_get_head(struct 
  
  	return le32_to_cpu(*(volatile __le32 *)head);
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * i40e_xmit_descriptor_count - calculate number of Tx descriptors needed
+  * @skb:     send buffer
+  * @tx_ring: ring to send buffer on
+  *
+  * Returns number of data descriptors needed for this skb. Returns 0 to indicate
+  * there is not enough descriptors available in this ring since we need at least
+  * one descriptor.
+  **/
+ static inline int i40e_xmit_descriptor_count(struct sk_buff *skb)
+ {
+ 	const struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
+ 	unsigned int nr_frags = skb_shinfo(skb)->nr_frags;
+ 	int count = 0, size = skb_headlen(skb);
+ 
+ 	for (;;) {
+ 		count += i40e_txd_use_count(size);
+ 
+ 		if (!nr_frags--)
+ 			break;
+ 
+ 		size = skb_frag_size(frag++);
+ 	}
+ 
+ 	return count;
+ }
+ 
+ /**
+  * i40e_maybe_stop_tx - 1st level check for Tx stop conditions
+  * @tx_ring: the ring to be checked
+  * @size:    the size buffer we want to assure is available
+  *
+  * Returns 0 if stop is not needed
+  **/
+ static inline int i40e_maybe_stop_tx(struct i40e_ring *tx_ring, int size)
+ {
+ 	if (likely(I40E_DESC_UNUSED(tx_ring) >= size))
+ 		return 0;
+ 	return __i40evf_maybe_stop_tx(tx_ring, size);
+ }
+ 
+ /**
+  * i40e_chk_linearize - Check if there are more than 8 fragments per packet
+  * @skb:      send buffer
+  * @count:    number of buffers used
+  *
+  * Note: Our HW can't scatter-gather more than 8 fragments to build
+  * a packet on the wire and so we need to figure out the cases where we
+  * need to linearize the skb.
+  **/
+ static inline bool i40e_chk_linearize(struct sk_buff *skb, int count)
+ {
+ 	/* we can only support up to 8 data buffers for a single send */
+ 	if (likely(count <= I40E_MAX_BUFFER_TXD))
+ 		return false;
+ 
+ 	return __i40evf_chk_linearize(skb);
+ }
++>>>>>>> 5c4654daf2e2 (i40e/i40evf: Allow up to 12K bytes of data per Tx descriptor instead of 8K)
  #endif /* _I40E_TXRX_H_ */
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_fcoe.c
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.c
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.h
* Unmerged path drivers/net/ethernet/intel/i40evf/i40e_txrx.c
* Unmerged path drivers/net/ethernet/intel/i40evf/i40e_txrx.h
