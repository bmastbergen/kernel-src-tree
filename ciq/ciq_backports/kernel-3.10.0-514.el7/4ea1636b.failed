x86/asm/tsc: Rename native_read_tsc() to rdtsc()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] tsc: Rename native_read_tsc() to rdtsc() (Prarit Bhargava) [1302325]
Rebuild_FUZZ: 90.91%
commit-author Andy Lutomirski <luto@kernel.org>
commit 4ea1636b04dbd66536fa387bae2eea463efc705b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/4ea1636b.failed

Now that there is no paravirt TSC, the "native" is
inappropriate. The function does RDTSC, so give it the obvious
name: rdtsc().

	Suggested-by: Borislav Petkov <bp@suse.de>
	Signed-off-by: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Huang Rui <ray.huang@amd.com>
	Cc: John Stultz <john.stultz@linaro.org>
	Cc: Len Brown <lenb@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: kvm ML <kvm@vger.kernel.org>
Link: http://lkml.kernel.org/r/fd43e16281991f096c1e4d21574d9e1402c62d39.1434501121.git.luto@kernel.org
[ Ported it to v4.2-rc1. ]
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 4ea1636b04dbd66536fa387bae2eea463efc705b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/aslr.c
#	arch/x86/include/asm/msr.h
#	arch/x86/include/asm/pvclock.h
#	arch/x86/include/asm/stackprotector.h
#	arch/x86/include/asm/tsc.h
#	arch/x86/kernel/apb_timer.c
#	arch/x86/kernel/apic/apic.c
#	arch/x86/kernel/cpu/amd.c
#	arch/x86/kernel/cpu/mcheck/mce.c
#	arch/x86/kernel/espfix_64.c
#	arch/x86/kernel/hpet.c
#	arch/x86/kernel/trace_clock.c
#	arch/x86/kernel/tsc.c
#	arch/x86/kvm/vmx.c
#	arch/x86/kvm/x86.c
#	arch/x86/lib/delay.c
#	arch/x86/vdso/vclock_gettime.c
#	drivers/cpufreq/intel_pstate.c
#	drivers/input/gameport/gameport.c
#	drivers/input/joystick/analog.c
#	drivers/net/hamradio/baycom_epp.c
#	drivers/thermal/intel_powerclamp.c
#	tools/power/cpupower/debug/kernel/cpufreq-test_tsc.c
diff --cc arch/x86/include/asm/msr.h
index de36f22eb0b9,ff0c120dafe5..000000000000
--- a/arch/x86/include/asm/msr.h
+++ b/arch/x86/include/asm/msr.h
@@@ -110,7 -109,16 +110,20 @@@ extern unsigned long long native_read_t
  extern int rdmsr_safe_regs(u32 regs[8]);
  extern int wrmsr_safe_regs(u32 regs[8]);
  
++<<<<<<< HEAD
 +static __always_inline unsigned long long __native_read_tsc(void)
++=======
+ /**
+  * rdtsc() - returns the current TSC without ordering constraints
+  *
+  * rdtsc() returns the result of RDTSC as a 64-bit integer.  The
+  * only ordering constraint it supplies is the ordering implied by
+  * "asm volatile": it will put the RDTSC in the place you expect.  The
+  * CPU can and will speculatively execute that RDTSC, though, so the
+  * results can be non-monotonic if compared on different CPUs.
+  */
+ static __always_inline unsigned long long rdtsc(void)
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  {
  	DECLARE_ARGS(val, low, high);
  
diff --cc arch/x86/include/asm/pvclock.h
index 628954ceede1,5c490db62e32..000000000000
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@@ -62,7 -62,7 +62,11 @@@ static inline u64 pvclock_scale_delta(u
  static __always_inline
  u64 pvclock_get_nsec_offset(const struct pvclock_vcpu_time_info *src)
  {
++<<<<<<< HEAD
 +	u64 delta = __native_read_tsc() - src->tsc_timestamp;
++=======
+ 	u64 delta = rdtsc() - src->tsc_timestamp;
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	return pvclock_scale_delta(delta, src->tsc_to_system_mul,
  				   src->tsc_shift);
  }
diff --cc arch/x86/include/asm/stackprotector.h
index 6a998598f172,58505f01962f..000000000000
--- a/arch/x86/include/asm/stackprotector.h
+++ b/arch/x86/include/asm/stackprotector.h
@@@ -70,7 -72,7 +70,11 @@@ static __always_inline void boot_init_s
  	 * on during the bootup the random pool has true entropy too.
  	 */
  	get_random_bytes(&canary, sizeof(canary));
++<<<<<<< HEAD
 +	tsc = __native_read_tsc();
++=======
+ 	tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	canary += tsc + (tsc << 32UL);
  
  	current->stack_canary = canary;
diff --cc arch/x86/include/asm/tsc.h
index 235be70d5bb4,3df7675debcf..000000000000
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@@ -27,22 -25,8 +27,26 @@@ static inline cycles_t get_cycles(void
  	if (!cpu_has_tsc)
  		return 0;
  #endif
 +	rdtscll(ret);
 +
++<<<<<<< HEAD
 +	return ret;
 +}
  
 +static __always_inline cycles_t vget_cycles(void)
 +{
 +	/*
 +	 * We only do VDSOs on TSC capable CPUs, so this shouldn't
 +	 * access boot_cpu_data (which is not VDSO-safe):
 +	 */
 +#ifndef CONFIG_X86_TSC
 +	if (!cpu_has_tsc)
 +		return 0;
 +#endif
 +	return (cycles_t)__native_read_tsc();
++=======
+ 	return rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  }
  
  extern void tsc_init(void);
diff --cc arch/x86/kernel/apb_timer.c
index c9876efecafb,222a57076039..000000000000
--- a/arch/x86/kernel/apb_timer.c
+++ b/arch/x86/kernel/apb_timer.c
@@@ -277,7 -263,7 +277,11 @@@ static int apbt_clocksource_register(vo
  
  	/* Verify whether apbt counter works */
  	t1 = dw_apb_clocksource_read(clocksource_apbt);
++<<<<<<< HEAD
 +	rdtscll(start);
++=======
+ 	start = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	/*
  	 * We don't know the TSC frequency yet, but waiting for
@@@ -287,7 -273,7 +291,11 @@@
  	 */
  	do {
  		rep_nop();
++<<<<<<< HEAD
 +		rdtscll(now);
++=======
+ 		now = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	} while ((now - start) < 200000UL);
  
  	/* APBT is the only always on clocksource, it has to work! */
@@@ -404,13 -390,13 +412,21 @@@ unsigned long apbt_quick_calibrate(void
  	old = dw_apb_clocksource_read(clocksource_apbt);
  	old += loop;
  
++<<<<<<< HEAD
 +	t1 = __native_read_tsc();
++=======
+ 	t1 = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	do {
  		new = dw_apb_clocksource_read(clocksource_apbt);
  	} while (new < old);
  
++<<<<<<< HEAD
 +	t2 = __native_read_tsc();
++=======
+ 	t2 = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	shift = 5;
  	if (unlikely(loop >> shift == 0)) {
diff --cc arch/x86/kernel/apic/apic.c
index 17b1e00dfdd2,0d71cd9b4a50..000000000000
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@@ -484,7 -457,7 +484,11 @@@ static int lapic_next_deadline(unsigne
  {
  	u64 tsc;
  
++<<<<<<< HEAD
 +	rdtscll(tsc);
++=======
+ 	tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	wrmsrl(MSR_IA32_TSC_DEADLINE, tsc + (((u64) delta) * TSC_DIVISOR));
  	return 0;
  }
@@@ -619,7 -592,7 +623,11 @@@ static void __init lapic_cal_handler(st
  	unsigned long pm = acpi_pm_read_early();
  
  	if (cpu_has_tsc)
++<<<<<<< HEAD
 +		rdtscll(tsc);
++=======
+ 		tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	switch (lapic_cal_loops++) {
  	case 0:
@@@ -1294,10 -1206,10 +1302,14 @@@ void setup_local_APIC(void
  	unsigned int value, queued;
  	int i, j, acked = 0;
  	unsigned long long tsc = 0, ntsc;
 -	long long max_loops = cpu_khz ? cpu_khz : 1000000;
 +	long long max_loops = cpu_khz;
  
  	if (cpu_has_tsc)
++<<<<<<< HEAD
 +		rdtscll(tsc);
++=======
+ 		tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	if (disable_apic) {
  		disable_ioapic_support();
@@@ -1391,8 -1292,8 +1403,13 @@@
  			break;
  		}
  		if (queued) {
++<<<<<<< HEAD
 +			if (cpu_has_tsc) {
 +				rdtscll(ntsc);
++=======
+ 			if (cpu_has_tsc && cpu_khz) {
+ 				ntsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  				max_loops = (cpu_khz << 10) - (ntsc - tsc);
  			} else
  				max_loops--;
diff --cc arch/x86/kernel/cpu/amd.c
index 09cc08968ceb,51ad2af84a72..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -116,10 -125,10 +116,17 @@@ static void init_amd_k6(struct cpuinfo_
  
  		n = K6_BUG_LOOP;
  		f_vide = vide;
++<<<<<<< HEAD
 +		rdtscl(d);
 +		while (n--)
 +			f_vide();
 +		rdtscl(d2);
++=======
+ 		d = rdtsc();
+ 		while (n--)
+ 			f_vide();
+ 		d2 = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  		d = d2-d;
  
  		if (d > 20*K6_BUG_LOOP)
diff --cc arch/x86/kernel/cpu/mcheck/mce.c
index cedd2739979e,96cceccd11b4..000000000000
--- a/arch/x86/kernel/cpu/mcheck/mce.c
+++ b/arch/x86/kernel/cpu/mcheck/mce.c
@@@ -123,7 -125,7 +123,11 @@@ void mce_setup(struct mce *m
  {
  	memset(m, 0, sizeof(struct mce));
  	m->cpu = m->extcpu = smp_processor_id();
++<<<<<<< HEAD
 +	rdtscll(m->tsc);
++=======
+ 	m->tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	/* We hope get_seconds stays lockless */
  	m->time = get_seconds();
  	m->cpuvendor = boot_cpu_data.x86_vendor;
@@@ -1785,7 -1784,7 +1789,11 @@@ static void collect_tscs(void *data
  {
  	unsigned long *cpu_tsc = (unsigned long *)data;
  
++<<<<<<< HEAD
 +	rdtscll(cpu_tsc[smp_processor_id()]);
++=======
+ 	cpu_tsc[smp_processor_id()] = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  }
  
  static int mce_apei_read_done;
diff --cc arch/x86/kernel/hpet.c
index e706fc6d00ba,f75c5908c7a6..000000000000
--- a/arch/x86/kernel/hpet.c
+++ b/arch/x86/kernel/hpet.c
@@@ -766,7 -735,7 +766,11 @@@ static int hpet_clocksource_register(vo
  
  	/* Verify whether hpet counter works */
  	t1 = hpet_readl(HPET_COUNTER);
++<<<<<<< HEAD
 +	rdtscll(start);
++=======
+ 	start = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	/*
  	 * We don't know the TSC frequency yet, but waiting for
@@@ -776,7 -745,7 +780,11 @@@
  	 */
  	do {
  		rep_nop();
++<<<<<<< HEAD
 +		rdtscll(now);
++=======
+ 		now = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	} while ((now - start) < 200000UL);
  
  	if (t1 == hpet_readl(HPET_COUNTER)) {
diff --cc arch/x86/kernel/trace_clock.c
index 25b993729f9b,67efb8c96fc4..000000000000
--- a/arch/x86/kernel/trace_clock.c
+++ b/arch/x86/kernel/trace_clock.c
@@@ -15,7 -15,7 +15,11 @@@ u64 notrace trace_clock_x86_tsc(void
  	u64 ret;
  
  	rdtsc_barrier();
++<<<<<<< HEAD
 +	rdtscll(ret);
++=======
+ 	ret = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	return ret;
  }
diff --cc arch/x86/kernel/tsc.c
index 629006c0f4fe,21d6e04e3e82..000000000000
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@@ -41,6 -41,234 +41,237 @@@ static int __read_mostly tsc_disabled 
  static struct static_key __use_tsc = STATIC_KEY_INIT;
  
  int tsc_clocksource_reliable;
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Use a ring-buffer like data structure, where a writer advances the head by
+  * writing a new data entry and a reader advances the tail when it observes a
+  * new entry.
+  *
+  * Writers are made to wait on readers until there's space to write a new
+  * entry.
+  *
+  * This means that we can always use an {offset, mul} pair to compute a ns
+  * value that is 'roughly' in the right direction, even if we're writing a new
+  * {offset, mul} pair during the clock read.
+  *
+  * The down-side is that we can no longer guarantee strict monotonicity anymore
+  * (assuming the TSC was that to begin with), because while we compute the
+  * intersection point of the two clock slopes and make sure the time is
+  * continuous at the point of switching; we can no longer guarantee a reader is
+  * strictly before or after the switch point.
+  *
+  * It does mean a reader no longer needs to disable IRQs in order to avoid
+  * CPU-Freq updates messing with his times, and similarly an NMI reader will
+  * no longer run the risk of hitting half-written state.
+  */
+ 
+ struct cyc2ns {
+ 	struct cyc2ns_data data[2];	/*  0 + 2*24 = 48 */
+ 	struct cyc2ns_data *head;	/* 48 + 8    = 56 */
+ 	struct cyc2ns_data *tail;	/* 56 + 8    = 64 */
+ }; /* exactly fits one cacheline */
+ 
+ static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
+ 
+ struct cyc2ns_data *cyc2ns_read_begin(void)
+ {
+ 	struct cyc2ns_data *head;
+ 
+ 	preempt_disable();
+ 
+ 	head = this_cpu_read(cyc2ns.head);
+ 	/*
+ 	 * Ensure we observe the entry when we observe the pointer to it.
+ 	 * matches the wmb from cyc2ns_write_end().
+ 	 */
+ 	smp_read_barrier_depends();
+ 	head->__count++;
+ 	barrier();
+ 
+ 	return head;
+ }
+ 
+ void cyc2ns_read_end(struct cyc2ns_data *head)
+ {
+ 	barrier();
+ 	/*
+ 	 * If we're the outer most nested read; update the tail pointer
+ 	 * when we're done. This notifies possible pending writers
+ 	 * that we've observed the head pointer and that the other
+ 	 * entry is now free.
+ 	 */
+ 	if (!--head->__count) {
+ 		/*
+ 		 * x86-TSO does not reorder writes with older reads;
+ 		 * therefore once this write becomes visible to another
+ 		 * cpu, we must be finished reading the cyc2ns_data.
+ 		 *
+ 		 * matches with cyc2ns_write_begin().
+ 		 */
+ 		this_cpu_write(cyc2ns.tail, head);
+ 	}
+ 	preempt_enable();
+ }
+ 
+ /*
+  * Begin writing a new @data entry for @cpu.
+  *
+  * Assumes some sort of write side lock; currently 'provided' by the assumption
+  * that cpufreq will call its notifiers sequentially.
+  */
+ static struct cyc2ns_data *cyc2ns_write_begin(int cpu)
+ {
+ 	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+ 	struct cyc2ns_data *data = c2n->data;
+ 
+ 	if (data == c2n->head)
+ 		data++;
+ 
+ 	/* XXX send an IPI to @cpu in order to guarantee a read? */
+ 
+ 	/*
+ 	 * When we observe the tail write from cyc2ns_read_end(),
+ 	 * the cpu must be done with that entry and its safe
+ 	 * to start writing to it.
+ 	 */
+ 	while (c2n->tail == data)
+ 		cpu_relax();
+ 
+ 	return data;
+ }
+ 
+ static void cyc2ns_write_end(int cpu, struct cyc2ns_data *data)
+ {
+ 	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+ 
+ 	/*
+ 	 * Ensure the @data writes are visible before we publish the
+ 	 * entry. Matches the data-depencency in cyc2ns_read_begin().
+ 	 */
+ 	smp_wmb();
+ 
+ 	ACCESS_ONCE(c2n->head) = data;
+ }
+ 
+ /*
+  * Accelerators for sched_clock()
+  * convert from cycles(64bits) => nanoseconds (64bits)
+  *  basic equation:
+  *              ns = cycles / (freq / ns_per_sec)
+  *              ns = cycles * (ns_per_sec / freq)
+  *              ns = cycles * (10^9 / (cpu_khz * 10^3))
+  *              ns = cycles * (10^6 / cpu_khz)
+  *
+  *      Then we use scaling math (suggested by george@mvista.com) to get:
+  *              ns = cycles * (10^6 * SC / cpu_khz) / SC
+  *              ns = cycles * cyc2ns_scale / SC
+  *
+  *      And since SC is a constant power of two, we can convert the div
+  *  into a shift.
+  *
+  *  We can use khz divisor instead of mhz to keep a better precision, since
+  *  cyc2ns_scale is limited to 10^6 * 2^10, which fits in 32 bits.
+  *  (mathieu.desnoyers@polymtl.ca)
+  *
+  *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
+  */
+ 
+ #define CYC2NS_SCALE_FACTOR 10 /* 2^10, carefully chosen */
+ 
+ static void cyc2ns_data_init(struct cyc2ns_data *data)
+ {
+ 	data->cyc2ns_mul = 0;
+ 	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
+ 	data->cyc2ns_offset = 0;
+ 	data->__count = 0;
+ }
+ 
+ static void cyc2ns_init(int cpu)
+ {
+ 	struct cyc2ns *c2n = &per_cpu(cyc2ns, cpu);
+ 
+ 	cyc2ns_data_init(&c2n->data[0]);
+ 	cyc2ns_data_init(&c2n->data[1]);
+ 
+ 	c2n->head = c2n->data;
+ 	c2n->tail = c2n->data;
+ }
+ 
+ static inline unsigned long long cycles_2_ns(unsigned long long cyc)
+ {
+ 	struct cyc2ns_data *data, *tail;
+ 	unsigned long long ns;
+ 
+ 	/*
+ 	 * See cyc2ns_read_*() for details; replicated in order to avoid
+ 	 * an extra few instructions that came with the abstraction.
+ 	 * Notable, it allows us to only do the __count and tail update
+ 	 * dance when its actually needed.
+ 	 */
+ 
+ 	preempt_disable_notrace();
+ 	data = this_cpu_read(cyc2ns.head);
+ 	tail = this_cpu_read(cyc2ns.tail);
+ 
+ 	if (likely(data == tail)) {
+ 		ns = data->cyc2ns_offset;
+ 		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+ 	} else {
+ 		data->__count++;
+ 
+ 		barrier();
+ 
+ 		ns = data->cyc2ns_offset;
+ 		ns += mul_u64_u32_shr(cyc, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+ 
+ 		barrier();
+ 
+ 		if (!--data->__count)
+ 			this_cpu_write(cyc2ns.tail, data);
+ 	}
+ 	preempt_enable_notrace();
+ 
+ 	return ns;
+ }
+ 
+ static void set_cyc2ns_scale(unsigned long cpu_khz, int cpu)
+ {
+ 	unsigned long long tsc_now, ns_now;
+ 	struct cyc2ns_data *data;
+ 	unsigned long flags;
+ 
+ 	local_irq_save(flags);
+ 	sched_clock_idle_sleep_event();
+ 
+ 	if (!cpu_khz)
+ 		goto done;
+ 
+ 	data = cyc2ns_write_begin(cpu);
+ 
+ 	tsc_now = rdtsc();
+ 	ns_now = cycles_2_ns(tsc_now);
+ 
+ 	/*
+ 	 * Compute a new multiplier as per the above comment and ensure our
+ 	 * time function is continuous; see the comment near struct
+ 	 * cyc2ns_data.
+ 	 */
+ 	data->cyc2ns_mul =
+ 		DIV_ROUND_CLOSEST(NSEC_PER_MSEC << CYC2NS_SCALE_FACTOR,
+ 				  cpu_khz);
+ 	data->cyc2ns_shift = CYC2NS_SCALE_FACTOR;
+ 	data->cyc2ns_offset = ns_now -
+ 		mul_u64_u32_shr(tsc_now, data->cyc2ns_mul, CYC2NS_SCALE_FACTOR);
+ 
+ 	cyc2ns_write_end(cpu, data);
+ 
+ done:
+ 	sched_clock_idle_wakeup_event(0);
+ 	local_irq_restore(flags);
+ }
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  /*
   * Scheduler clock - returns current time in nanosec units.
   */
@@@ -62,10 -290,10 +293,14 @@@ u64 native_sched_clock(void
  	}
  
  	/* read the Time Stamp Counter: */
++<<<<<<< HEAD
 +	rdtscll(this_offset);
++=======
+ 	tsc_now = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	/* return the value in ns */
 -	return cycles_2_ns(tsc_now);
 +	return __cycles_2_ns(this_offset);
  }
  
  /* We need to define a real function for sched_clock, to override the
diff --cc arch/x86/kvm/vmx.c
index 5298e39bc414,10d69a6df14f..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -2166,7 -2236,7 +2166,11 @@@ static u64 guest_read_tsc(void
  {
  	u64 host_tsc, tsc_offset;
  
++<<<<<<< HEAD
 +	rdtscll(host_tsc);
++=======
+ 	host_tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	tsc_offset = vmcs_read64(TSC_OFFSET);
  	return host_tsc + tsc_offset;
  }
diff --cc arch/x86/kvm/x86.c
index 0df755714887,dfa97139282d..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1428,7 -1455,7 +1428,11 @@@ static cycle_t read_tsc(void
  	 * but no one has ever seen it happen.
  	 */
  	rdtsc_barrier();
++<<<<<<< HEAD
 +	ret = (cycle_t)vget_cycles();
++=======
+ 	ret = (cycle_t)rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	last = pvclock_gtod_data.clock.cycle_last;
  
diff --cc arch/x86/lib/delay.c
index e2dfb7e87c55,f24bc59ab0a0..000000000000
--- a/arch/x86/lib/delay.c
+++ b/arch/x86/lib/delay.c
@@@ -100,7 -100,7 +100,11 @@@ void use_tsc_delay(void
  int read_current_timer(unsigned long *timer_val)
  {
  	if (delay_fn == delay_tsc) {
++<<<<<<< HEAD
 +		rdtscll(*timer_val);
++=======
+ 		*timer_val = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  		return 0;
  	}
  	return -1;
diff --cc arch/x86/vdso/vclock_gettime.c
index 72074d528400,0340d93c18ca..000000000000
--- a/arch/x86/vdso/vclock_gettime.c
+++ b/arch/x86/vdso/vclock_gettime.c
@@@ -150,6 -162,48 +150,51 @@@ notrace static long vdso_fallback_gtod(
  	return ret;
  }
  
++<<<<<<< HEAD:arch/x86/vdso/vclock_gettime.c
++=======
+ #ifdef CONFIG_PARAVIRT_CLOCK
+ 
+ static notrace cycle_t vread_pvclock(int *mode)
+ {
+ 	*mode = VCLOCK_NONE;
+ 	return 0;
+ }
+ #endif
+ 
+ #endif
+ 
+ notrace static cycle_t vread_tsc(void)
+ {
+ 	cycle_t ret;
+ 	u64 last;
+ 
+ 	/*
+ 	 * Empirically, a fence (of type that depends on the CPU)
+ 	 * before rdtsc is enough to ensure that rdtsc is ordered
+ 	 * with respect to loads.  The various CPU manuals are unclear
+ 	 * as to whether rdtsc can be reordered with later loads,
+ 	 * but no one has ever seen it happen.
+ 	 */
+ 	rdtsc_barrier();
+ 	ret = (cycle_t)rdtsc();
+ 
+ 	last = gtod->cycle_last;
+ 
+ 	if (likely(ret >= last))
+ 		return ret;
+ 
+ 	/*
+ 	 * GCC likes to generate cmov here, but this branch is extremely
+ 	 * predictable (it's just a funciton of time and the likely is
+ 	 * very likely) and there's a data dependence, so force GCC
+ 	 * to generate a branch instead.  I don't barrier() because
+ 	 * we don't actually need a barrier, and if this function
+ 	 * ever gets inlined it will generate worse code.
+ 	 */
+ 	asm volatile ("");
+ 	return last;
+ }
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc()):arch/x86/entry/vdso/vclock_gettime.c
  
  notrace static inline u64 vgetsns(int *mode)
  {
diff --cc drivers/cpufreq/intel_pstate.c
index 9d42010c6969,7c56d7eaa671..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -761,6 -765,7 +761,10 @@@ static inline void intel_pstate_sample(
  	local_irq_save(flags);
  	rdmsrl(MSR_IA32_APERF, aperf);
  	rdmsrl(MSR_IA32_MPERF, mperf);
++<<<<<<< HEAD
++=======
+ 	tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	local_irq_restore(flags);
  
  	cpu->last_sample_time = cpu->sample.time;
diff --cc drivers/input/gameport/gameport.c
index da739d9d1905,4a2a9e370be7..000000000000
--- a/drivers/input/gameport/gameport.c
+++ b/drivers/input/gameport/gameport.c
@@@ -112,9 -149,9 +112,15 @@@ static int gameport_measure_speed(struc
  
  	for(i = 0; i < 50; i++) {
  		local_irq_save(flags);
++<<<<<<< HEAD
 +		rdtscl(t1);
 +		for (t = 0; t < 50; t++) gameport_read(gameport);
 +		rdtscl(t2);
++=======
+ 		t1 = rdtsc();
+ 		for (t = 0; t < 50; t++) gameport_read(gameport);
+ 		t2 = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  		local_irq_restore(flags);
  		udelay(i * 10);
  		if (t2 - t1 < tx) tx = t2 - t1;
diff --cc drivers/input/joystick/analog.c
index 9135606c8649,6f8b084e13d0..000000000000
--- a/drivers/input/joystick/analog.c
+++ b/drivers/input/joystick/analog.c
@@@ -138,7 -143,7 +138,11 @@@ struct analog_port 
  
  #include <linux/i8253.h>
  
++<<<<<<< HEAD
 +#define GET_TIME(x)	do { if (cpu_has_tsc) rdtscl(x); else x = get_time_pit(); } while (0)
++=======
+ #define GET_TIME(x)	do { if (cpu_has_tsc) x = (unsigned int)rdtsc(); else x = get_time_pit(); } while (0)
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  #define DELTA(x,y)	(cpu_has_tsc ? ((y) - (x)) : ((x) - (y) + ((x) < (y) ? PIT_TICK_RATE / HZ : 0)))
  #define TIME_NAME	(cpu_has_tsc?"TSC":"PIT")
  static unsigned int get_time_pit(void)
@@@ -155,10 -160,10 +159,14 @@@
          return count;
  }
  #elif defined(__x86_64__)
++<<<<<<< HEAD
 +#define GET_TIME(x)	rdtscl(x)
++=======
+ #define GET_TIME(x)	do { x = (unsigned int)rdtsc(); } while (0)
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  #define DELTA(x,y)	((y)-(x))
  #define TIME_NAME	"TSC"
 -#elif defined(__alpha__) || defined(CONFIG_MN10300) || defined(CONFIG_ARM) || defined(CONFIG_ARM64) || defined(CONFIG_TILE)
 +#elif defined(__alpha__) || defined(CONFIG_MN10300) || defined(CONFIG_ARM) || defined(CONFIG_TILE)
  #define GET_TIME(x)	do { x = get_cycles(); } while (0)
  #define DELTA(x,y)	((y)-(x))
  #define TIME_NAME	"get_cycles"
diff --cc drivers/net/hamradio/baycom_epp.c
index 484f77ec2ce1,72c9f1f352b4..000000000000
--- a/drivers/net/hamradio/baycom_epp.c
+++ b/drivers/net/hamradio/baycom_epp.c
@@@ -638,7 -638,7 +638,11 @@@ static int receive(struct net_device *d
  #define GETTICK(x)                                                \
  ({                                                                \
  	if (cpu_has_tsc)                                          \
++<<<<<<< HEAD
 +		rdtscl(x);                                        \
++=======
+ 		x = (unsigned int)rdtsc();		  \
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  })
  #else /* __i386__ */
  #define GETTICK(x)
diff --cc drivers/thermal/intel_powerclamp.c
index fcd5313e35bd,2ac0c704bcb8..000000000000
--- a/drivers/thermal/intel_powerclamp.c
+++ b/drivers/thermal/intel_powerclamp.c
@@@ -340,7 -340,7 +340,11 @@@ static bool powerclamp_adjust_controls(
  
  	/* check result for the last window */
  	msr_now = pkg_state_counter();
++<<<<<<< HEAD
 +	rdtscll(tsc_now);
++=======
+ 	tsc_now = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  
  	/* calculate pkg cstate vs tsc ratio */
  	if (!msr_last || !tsc_last)
@@@ -482,7 -482,7 +486,11 @@@ static void poll_pkg_cstate(struct work
  	u64 val64;
  
  	msr_now = pkg_state_counter();
++<<<<<<< HEAD
 +	rdtscll(tsc_now);
++=======
+ 	tsc_now = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  	jiffies_now = jiffies;
  
  	/* calculate pkg cstate vs tsc ratio */
diff --cc tools/power/cpupower/debug/kernel/cpufreq-test_tsc.c
index 34bdeb11cbd8,6ff8383f2941..000000000000
--- a/tools/power/cpupower/debug/kernel/cpufreq-test_tsc.c
+++ b/tools/power/cpupower/debug/kernel/cpufreq-test_tsc.c
@@@ -84,11 -81,11 +84,19 @@@ static int __init cpufreq_test_tsc(void
  
  	printk(KERN_DEBUG "start--> \n");
  	then = read_pmtmr();
++<<<<<<< HEAD
 +        rdtscll(then_tsc);
 +	for (i=0;i<20;i++) {
 +		mdelay(100);
 +		now = read_pmtmr();
 +		rdtscll(now_tsc);
++=======
+ 	then_tsc = rdtsc();
+ 	for (i=0;i<20;i++) {
+ 		mdelay(100);
+ 		now = read_pmtmr();
+ 		now_tsc = rdtsc();
++>>>>>>> 4ea1636b04db (x86/asm/tsc: Rename native_read_tsc() to rdtsc())
  		diff = (now - then) & 0xFFFFFF;
  		diff_tsc = now_tsc - then_tsc;
  		printk(KERN_DEBUG "t1: %08u t2: %08u diff_pmtmr: %08u diff_tsc: %016llu\n", then, now, diff, diff_tsc);
* Unmerged path arch/x86/boot/compressed/aslr.c
* Unmerged path arch/x86/kernel/espfix_64.c
* Unmerged path arch/x86/boot/compressed/aslr.c
* Unmerged path arch/x86/include/asm/msr.h
* Unmerged path arch/x86/include/asm/pvclock.h
* Unmerged path arch/x86/include/asm/stackprotector.h
* Unmerged path arch/x86/include/asm/tsc.h
* Unmerged path arch/x86/kernel/apb_timer.c
* Unmerged path arch/x86/kernel/apic/apic.c
* Unmerged path arch/x86/kernel/cpu/amd.c
* Unmerged path arch/x86/kernel/cpu/mcheck/mce.c
* Unmerged path arch/x86/kernel/espfix_64.c
* Unmerged path arch/x86/kernel/hpet.c
* Unmerged path arch/x86/kernel/trace_clock.c
* Unmerged path arch/x86/kernel/tsc.c
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 07284ef82f0a..465313879015 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1124,7 +1124,7 @@ void wait_lapic_expire(struct kvm_vcpu *vcpu)
 
 	tsc_deadline = apic->lapic_timer.expired_tscdeadline;
 	apic->lapic_timer.expired_tscdeadline = 0;
-	guest_tsc = kvm_x86_ops->read_l1_tsc(vcpu, native_read_tsc());
+	guest_tsc = kvm_x86_ops->read_l1_tsc(vcpu, rdtsc());
 	trace_kvm_wait_lapic_expire(vcpu->vcpu_id, guest_tsc - tsc_deadline);
 
 	/* __delay is delay_tsc whenever the hardware has TSC, thus always.  */
@@ -1222,7 +1222,7 @@ static void start_apic_timer(struct kvm_lapic *apic)
 		local_irq_save(flags);
 
 		now = apic->lapic_timer.timer.base->get_time();
-		guest_tsc = kvm_x86_ops->read_l1_tsc(vcpu, native_read_tsc());
+		guest_tsc = kvm_x86_ops->read_l1_tsc(vcpu, rdtsc());
 		if (likely(tscdeadline > guest_tsc)) {
 			ns = (tscdeadline - guest_tsc) * 1000000ULL;
 			do_div(ns, this_tsc_khz);
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index da9e425354fb..5af662ac6007 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -1080,7 +1080,7 @@ static u64 svm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
 
-	tsc = svm_scale_tsc(vcpu, native_read_tsc());
+	tsc = svm_scale_tsc(vcpu, rdtsc());
 
 	return target_tsc - tsc;
 }
@@ -3071,7 +3071,7 @@ static int svm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	switch (msr_info->index) {
 	case MSR_IA32_TSC: {
 		msr_info->data = svm->vmcb->control.tsc_offset +
-			svm_scale_tsc(vcpu, native_read_tsc());
+			svm_scale_tsc(vcpu, rdtsc());
 
 		break;
 	}
* Unmerged path arch/x86/kvm/vmx.c
* Unmerged path arch/x86/kvm/x86.c
* Unmerged path arch/x86/lib/delay.c
* Unmerged path arch/x86/vdso/vclock_gettime.c
* Unmerged path drivers/cpufreq/intel_pstate.c
* Unmerged path drivers/input/gameport/gameport.c
* Unmerged path drivers/input/joystick/analog.c
* Unmerged path drivers/net/hamradio/baycom_epp.c
* Unmerged path drivers/thermal/intel_powerclamp.c
* Unmerged path tools/power/cpupower/debug/kernel/cpufreq-test_tsc.c
