perf/core: Drop PERF_EVENT_TXN

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
commit 8f3e5684d3fbd91ead283916676fa3dac22615e5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/8f3e5684.failed

We currently use PERF_EVENT_TXN flag to determine if we are in the middle
of a transaction. If in a transaction, we defer the schedulability checks
from pmu->add() operation to the pmu->commit() operation.

Now that we have "transaction types" (PERF_PMU_TXN_ADD, PERF_PMU_TXN_READ)
we can use the type to determine if we are in a transaction and drop the
PERF_EVENT_TXN flag.

When PERF_EVENT_TXN is dropped, the cpuhw->group_flag on some architectures
becomes unused, so drop that field as well.

This is an extension of the Powerpc patch from Peter Zijlstra to s390,
Sparc and x86 architectures.

	Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
Link: http://lkml.kernel.org/r/1441336073-22750-11-git-send-email-sukadev@linux.vnet.ibm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 8f3e5684d3fbd91ead283916676fa3dac22615e5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/perf/core-book3s.c
#	arch/s390/kernel/perf_cpum_cf.c
#	arch/sparc/kernel/perf_event.c
#	arch/x86/kernel/cpu/perf_event.c
#	arch/x86/kernel/cpu/perf_event.h
#	include/linux/perf_event.h
diff --cc arch/powerpc/perf/core-book3s.c
index 33009684c666,d1e65ce545b3..000000000000
--- a/arch/powerpc/perf/core-book3s.c
+++ b/arch/powerpc/perf/core-book3s.c
@@@ -43,7 -48,7 +43,11 @@@ struct cpu_hw_events 
  	unsigned long amasks[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];
  	unsigned long avalues[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];
  
++<<<<<<< HEAD
 +	unsigned int group_flag;
++=======
+ 	unsigned int txn_flags;
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  	int n_txn_start;
  
  	/* BHRB bits */
@@@ -1552,13 -1586,22 +1556,12 @@@ static void power_pmu_stop(struct perf_
   * Start group events scheduling transaction
   * Set the flag to make pmu::enable() not perform the
   * schedulability test, it will be performed at commit time
 - *
 - * We only support PERF_PMU_TXN_ADD transactions. Save the
 - * transaction flags but otherwise ignore non-PERF_PMU_TXN_ADD
 - * transactions.
   */
 -static void power_pmu_start_txn(struct pmu *pmu, unsigned int txn_flags)
 +void power_pmu_start_txn(struct pmu *pmu)
  {
 -	struct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);
 -
 -	WARN_ON_ONCE(cpuhw->txn_flags);		/* txn already in flight */
 -
 -	cpuhw->txn_flags = txn_flags;
 -	if (txn_flags & ~PERF_PMU_TXN_ADD)
 -		return;
 +	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
  
  	perf_pmu_disable(pmu);
- 	cpuhw->group_flag |= PERF_EVENT_TXN;
  	cpuhw->n_txn_start = cpuhw->n_events;
  }
  
@@@ -1567,11 -1610,18 +1570,10 @@@
   * Clear the flag and pmu::enable() will perform the
   * schedulability test.
   */
 -static void power_pmu_cancel_txn(struct pmu *pmu)
 +void power_pmu_cancel_txn(struct pmu *pmu)
  {
 -	struct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);
 -	unsigned int txn_flags;
 -
 -	WARN_ON_ONCE(!cpuhw->txn_flags);	/* no txn in flight */
 -
 -	txn_flags = cpuhw->txn_flags;
 -	cpuhw->txn_flags = 0;
 -	if (txn_flags & ~PERF_PMU_TXN_ADD)
 -		return;
 +	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
  
- 	cpuhw->group_flag &= ~PERF_EVENT_TXN;
  	perf_pmu_enable(pmu);
  }
  
@@@ -1598,7 -1656,7 +1600,11 @@@ int power_pmu_commit_txn(struct pmu *pm
  	for (i = cpuhw->n_txn_start; i < n; ++i)
  		cpuhw->event[i]->hw.config = cpuhw->events[i];
  
++<<<<<<< HEAD
 +	cpuhw->group_flag &= ~PERF_EVENT_TXN;
++=======
+ 	cpuhw->txn_flags = 0;
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  	perf_pmu_enable(pmu);
  	return 0;
  }
diff --cc arch/s390/kernel/perf_cpum_cf.c
index ea75d011a6fc,cb774ff6e749..000000000000
--- a/arch/s390/kernel/perf_cpum_cf.c
+++ b/arch/s390/kernel/perf_cpum_cf.c
@@@ -578,13 -574,22 +578,12 @@@ static void cpumf_pmu_del(struct perf_e
  /*
   * Start group events scheduling transaction.
   * Set flags to perform a single test at commit time.
 - *
 - * We only support PERF_PMU_TXN_ADD transactions. Save the
 - * transaction flags but otherwise ignore non-PERF_PMU_TXN_ADD
 - * transactions.
   */
 -static void cpumf_pmu_start_txn(struct pmu *pmu, unsigned int txn_flags)
 +static void cpumf_pmu_start_txn(struct pmu *pmu)
  {
 -	struct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);
 -
 -	WARN_ON_ONCE(cpuhw->txn_flags);		/* txn already in flight */
 -
 -	cpuhw->txn_flags = txn_flags;
 -	if (txn_flags & ~PERF_PMU_TXN_ADD)
 -		return;
 +	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
  
  	perf_pmu_disable(pmu);
- 	cpuhw->flags |= PERF_EVENT_TXN;
  	cpuhw->tx_state = cpuhw->state;
  }
  
@@@ -619,7 -638,7 +617,11 @@@ static int cpumf_pmu_commit_txn(struct 
  	if ((state & cpuhw->info.auth_ctl) != state)
  		return -EPERM;
  
++<<<<<<< HEAD
 +	cpuhw->flags &= ~PERF_EVENT_TXN;
++=======
+ 	cpuhw->txn_flags = 0;
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  	perf_pmu_enable(pmu);
  	return 0;
  }
diff --cc arch/sparc/kernel/perf_event.c
index b5c38faa4ead,b0da5aedb336..000000000000
--- a/arch/sparc/kernel/perf_event.c
+++ b/arch/sparc/kernel/perf_event.c
@@@ -108,9 -108,9 +108,13 @@@ struct cpu_hw_events 
  	/* Enabled/disable state.  */
  	int			enabled;
  
++<<<<<<< HEAD
 +	unsigned int		group_flag;
++=======
+ 	unsigned int		txn_flags;
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  };
 -static DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = { .enabled = 1, };
 +DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = { .enabled = 1, };
  
  /* An event map describes the characteristics of a performance
   * counter event.  In particular it gives the encoding as well as
@@@ -1496,12 -1494,17 +1500,11 @@@ static int sparc_pmu_event_init(struct 
   * Set the flag to make pmu::enable() not perform the
   * schedulability test, it will be performed at commit time
   */
 -static void sparc_pmu_start_txn(struct pmu *pmu, unsigned int txn_flags)
 +static void sparc_pmu_start_txn(struct pmu *pmu)
  {
 -	struct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);
 -
 -	WARN_ON_ONCE(cpuhw->txn_flags);		/* txn already in flight */
 -
 -	cpuhw->txn_flags = txn_flags;
 -	if (txn_flags & ~PERF_PMU_TXN_ADD)
 -		return;
 +	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
  
  	perf_pmu_disable(pmu);
- 	cpuhw->group_flag |= PERF_EVENT_TXN;
  }
  
  /*
@@@ -1511,9 -1514,16 +1514,8 @@@
   */
  static void sparc_pmu_cancel_txn(struct pmu *pmu)
  {
 -	struct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);
 -	unsigned int txn_flags;
 -
 -	WARN_ON_ONCE(!cpuhw->txn_flags);	/* no txn in flight */
 -
 -	txn_flags = cpuhw->txn_flags;
 -	cpuhw->txn_flags = 0;
 -	if (txn_flags & ~PERF_PMU_TXN_ADD)
 -		return;
 +	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
  
- 	cpuhw->group_flag &= ~PERF_EVENT_TXN;
  	perf_pmu_enable(pmu);
  }
  
@@@ -1537,7 -1553,7 +1539,11 @@@ static int sparc_pmu_commit_txn(struct 
  	if (sparc_check_constraints(cpuc->event, cpuc->events, n))
  		return -EAGAIN;
  
++<<<<<<< HEAD
 +	cpuc->group_flag &= ~PERF_EVENT_TXN;
++=======
+ 	cpuc->txn_flags = 0;
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  	perf_pmu_enable(pmu);
  	return 0;
  }
diff --cc arch/x86/kernel/cpu/perf_event.c
index 422b52ebe193,4562cf070c27..000000000000
--- a/arch/x86/kernel/cpu/perf_event.c
+++ b/arch/x86/kernel/cpu/perf_event.c
@@@ -1716,11 -1748,22 +1716,10 @@@ static inline void x86_pmu_read(struct 
   * Start group events scheduling transaction
   * Set the flag to make pmu::enable() not perform the
   * schedulability test, it will be performed at commit time
 - *
 - * We only support PERF_PMU_TXN_ADD transactions. Save the
 - * transaction flags but otherwise ignore non-PERF_PMU_TXN_ADD
 - * transactions.
   */
 -static void x86_pmu_start_txn(struct pmu *pmu, unsigned int txn_flags)
 +static void x86_pmu_start_txn(struct pmu *pmu)
  {
 -	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
 -
 -	WARN_ON_ONCE(cpuc->txn_flags);		/* txn already in flight */
 -
 -	cpuc->txn_flags = txn_flags;
 -	if (txn_flags & ~PERF_PMU_TXN_ADD)
 -		return;
 -
  	perf_pmu_disable(pmu);
- 	__this_cpu_or(cpu_hw_events.group_flag, PERF_EVENT_TXN);
  	__this_cpu_write(cpu_hw_events.n_txn, 0);
  }
  
@@@ -1731,7 -1774,16 +1730,20 @@@
   */
  static void x86_pmu_cancel_txn(struct pmu *pmu)
  {
++<<<<<<< HEAD
 +	__this_cpu_and(cpu_hw_events.group_flag, ~PERF_EVENT_TXN);
++=======
+ 	unsigned int txn_flags;
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 
+ 	WARN_ON_ONCE(!cpuc->txn_flags);	/* no txn in flight */
+ 
+ 	txn_flags = cpuc->txn_flags;
+ 	cpuc->txn_flags = 0;
+ 	if (txn_flags & ~PERF_PMU_TXN_ADD)
+ 		return;
+ 
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  	/*
  	 * Truncate collected array by the number of events added in this
  	 * transaction. See x86_pmu_add() and x86_pmu_*_txn().
@@@ -1769,7 -1828,7 +1781,11 @@@ static int x86_pmu_commit_txn(struct pm
  	 */
  	memcpy(cpuc->assign, assign, n*sizeof(int));
  
++<<<<<<< HEAD
 +	cpuc->group_flag &= ~PERF_EVENT_TXN;
++=======
+ 	cpuc->txn_flags = 0;
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  	perf_pmu_enable(pmu);
  	return 0;
  }
diff --cc arch/x86/kernel/cpu/perf_event.h
index 22325d5f1d4e,953a0e4e3284..000000000000
--- a/arch/x86/kernel/cpu/perf_event.h
+++ b/arch/x86/kernel/cpu/perf_event.h
@@@ -195,7 -195,7 +195,11 @@@ struct cpu_hw_events 
  
  	int			n_excl; /* the number of exclusive events */
  
++<<<<<<< HEAD
 +	unsigned int		group_flag;
++=======
+ 	unsigned int		txn_flags;
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  	int			is_fake;
  
  	/*
diff --cc include/linux/perf_event.h
index 855e6baed4cb,d841d33bcdc9..000000000000
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@@ -175,7 -199,8 +175,12 @@@ struct perf_event
  /*
   * Common implementation detail of pmu::{start,commit,cancel}_txn
   */
++<<<<<<< HEAD
 +#define PERF_EVENT_TXN 0x1
++=======
+ #define PERF_PMU_TXN_ADD  0x1		/* txn to add/schedule event on PMU */
+ #define PERF_PMU_TXN_READ 0x2		/* txn to read event group from PMU */
++>>>>>>> 8f3e5684d3fb (perf/core: Drop PERF_EVENT_TXN)
  
  /**
   * pmu::capabilities flags
* Unmerged path arch/powerpc/perf/core-book3s.c
* Unmerged path arch/s390/kernel/perf_cpum_cf.c
* Unmerged path arch/sparc/kernel/perf_event.c
* Unmerged path arch/x86/kernel/cpu/perf_event.c
* Unmerged path arch/x86/kernel/cpu/perf_event.h
* Unmerged path include/linux/perf_event.h
