rhashtable: Per bucket locks & deferred expansion/shrinking

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Graf <tgraf@suug.ch>
commit 97defe1ecf868b8127f8e62395499d6a06e4c4b1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/97defe1e.failed

Introduces an array of spinlocks to protect bucket mutations. The number
of spinlocks per CPU is configurable and selected based on the hash of
the bucket. This allows for parallel insertions and removals of entries
which do not share a lock.

The patch also defers expansion and shrinking to a worker queue which
allows insertion and removal from atomic context. Insertions and
deletions may occur in parallel to it and are only held up briefly
while the particular bucket is linked or unzipped.

Mutations of the bucket table pointer is protected by a new mutex, read
access is RCU protected.

In the event of an expansion or shrinking, the new bucket table allocated
is exposed as a so called future table as soon as the resize process
starts.  Lookups, deletions, and insertions will briefly use both tables.
The future table becomes the main table after an RCU grace period and
initial linking of the old to the new table was performed. Optimization
of the chains to make use of the new number of buckets follows only the
new table is in use.

The side effect of this is that during that RCU grace period, a bucket
traversal using any rht_for_each() variant on the main table will not see
any insertions performed during the RCU grace period which would at that
point land in the future table. The lookup will see them as it searches
both tables if needed.

Having multiple insertions and removals occur in parallel requires nelems
to become an atomic counter.

	Signed-off-by: Thomas Graf <tgraf@suug.ch>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 97defe1ecf868b8127f8e62395499d6a06e4c4b1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rhashtable.h
#	lib/rhashtable.c
#	net/netfilter/nft_hash.c
#	net/netlink/af_netlink.c
diff --cc include/linux/rhashtable.h
index 0839d7b8cd60,a1688f0a6193..000000000000
--- a/include/linux/rhashtable.h
+++ b/include/linux/rhashtable.h
@@@ -65,7 -76,6 +76,10 @@@ struct rhashtable_params 
  						 size_t new_size);
  	bool			(*shrink_decision)(const struct rhashtable *ht,
  						   size_t new_size);
++<<<<<<< HEAD
 +	int			(*mutex_is_held)(void);
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  };
  
  /**
diff --cc lib/rhashtable.c
index be20e9720492,312e3437c7bc..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -26,13 -26,34 +26,38 @@@
  
  #define HASH_DEFAULT_SIZE	64UL
  #define HASH_MIN_SIZE		4UL
+ #define BUCKET_LOCKS_PER_CPU   128UL
+ 
+ enum {
+ 	RHT_LOCK_NORMAL,
+ 	RHT_LOCK_NESTED,
+ 	RHT_LOCK_NESTED2,
+ };
+ 
+ /* The bucket lock is selected based on the hash and protects mutations
+  * on a group of hash buckets.
+  *
+  * IMPORTANT: When holding the bucket lock of both the old and new table
+  * during expansions and shrinking, the old bucket lock must always be
+  * acquired first.
+  */
+ static spinlock_t *bucket_lock(const struct bucket_table *tbl, u32 hash)
+ {
+ 	return &tbl->locks[hash & tbl->locks_mask];
+ }
  
  #define ASSERT_RHT_MUTEX(HT) BUG_ON(!lockdep_rht_mutex_is_held(HT))
+ #define ASSERT_BUCKET_LOCK(TBL, HASH) \
+ 	BUG_ON(!lockdep_rht_bucket_is_held(TBL, HASH))
  
  #ifdef CONFIG_PROVE_LOCKING
- int lockdep_rht_mutex_is_held(const struct rhashtable *ht)
+ int lockdep_rht_mutex_is_held(struct rhashtable *ht)
  {
++<<<<<<< HEAD
 +	return ht->p.mutex_is_held();
++=======
+ 	return (debug_locks) ? lockdep_is_held(&ht->mutex) : 1;
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  }
  EXPORT_SYMBOL_GPL(lockdep_rht_mutex_is_held);
  
@@@ -485,7 -709,6 +713,10 @@@ static size_t rounded_hashtable_size(st
   *	.key_offset = offsetof(struct test_obj, key),
   *	.key_len = sizeof(int),
   *	.hashfn = jhash,
++<<<<<<< HEAD
 + *	.mutex_is_held = &my_mutex_is_held,
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
   * };
   *
   * Configuration Example 2: Variable length keys
@@@ -505,7 -728,6 +736,10 @@@
   *	.head_offset = offsetof(struct test_obj, node),
   *	.hashfn = jhash,
   *	.obj_hashfn = my_hash_fn,
++<<<<<<< HEAD
 + *	.mutex_is_held = &my_mutex_is_held,
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
   * };
   */
  int rhashtable_init(struct rhashtable *ht, struct rhashtable_params *params)
@@@ -566,11 -806,6 +818,14 @@@ EXPORT_SYMBOL_GPL(rhashtable_destroy)
  #define TEST_PTR	((void *) 0xdeadbeef)
  #define TEST_NEXPANDS	4
  
++<<<<<<< HEAD
 +static int test_mutex_is_held(void)
 +{
 +	return 1;
 +}
 +
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  struct test_obj {
  	void			*ptr;
  	int			value;
@@@ -735,7 -974,6 +994,10 @@@ static int __init test_rht_init(void
  		.key_offset = offsetof(struct test_obj, value),
  		.key_len = sizeof(int),
  		.hashfn = jhash,
++<<<<<<< HEAD
 +		.mutex_is_held = &test_mutex_is_held,
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  		.grow_decision = rht_grow_above_75,
  		.shrink_decision = rht_shrink_below_30,
  	};
diff --cc net/netfilter/nft_hash.c
index f14a5e14123a,75887d7d2c6a..000000000000
--- a/net/netfilter/nft_hash.c
+++ b/net/netfilter/nft_hash.c
@@@ -162,11 -162,6 +162,14 @@@ static unsigned int nft_hash_privsize(c
  	return sizeof(struct rhashtable);
  }
  
++<<<<<<< HEAD
 +static int lockdep_nfnl_lock_is_held(void)
 +{
 +	return lockdep_nfnl_is_held(NFNL_SUBSYS_NFTABLES);
 +}
 +
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  static int nft_hash_init(const struct nft_set *set,
  			 const struct nft_set_desc *desc,
  			 const struct nlattr * const tb[])
@@@ -180,7 -175,6 +183,10 @@@
  		.hashfn = jhash,
  		.grow_decision = rht_grow_above_75,
  		.shrink_decision = rht_shrink_below_30,
++<<<<<<< HEAD
 +		.mutex_is_held = lockdep_nfnl_lock_is_held,
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  	};
  
  	return rhashtable_init(priv, &params);
diff --cc net/netlink/af_netlink.c
index a078b4960b4a,738c3bfaa564..000000000000
--- a/net/netlink/af_netlink.c
+++ b/net/netlink/af_netlink.c
@@@ -115,15 -114,6 +115,18 @@@ static atomic_t nl_table_users = ATOMIC
  DEFINE_MUTEX(nl_sk_hash_lock);
  EXPORT_SYMBOL_GPL(nl_sk_hash_lock);
  
++<<<<<<< HEAD
 +static int lockdep_nl_sk_hash_is_held(void)
 +{
 +#ifdef CONFIG_LOCKDEP
 +	if (debug_locks)
 +		return lockdep_is_held(&nl_sk_hash_lock) || lockdep_is_held(&nl_table_lock);
 +#endif
 +	return 1;
 +}
 +
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  static ATOMIC_NOTIFIER_HEAD(netlink_chain);
  
  static DEFINE_SPINLOCK(netlink_tap_lock);
@@@ -3160,7 -3114,6 +3164,10 @@@ static int __init netlink_proto_init(vo
  		.max_shift = 16, /* 64K */
  		.grow_decision = rht_grow_above_75,
  		.shrink_decision = rht_shrink_below_30,
++<<<<<<< HEAD
 +		.mutex_is_held = lockdep_nl_sk_hash_is_held,
++=======
++>>>>>>> 97defe1ecf86 (rhashtable: Per bucket locks & deferred expansion/shrinking)
  	};
  
  	if (err != 0)
* Unmerged path include/linux/rhashtable.h
* Unmerged path lib/rhashtable.c
* Unmerged path net/netfilter/nft_hash.c
* Unmerged path net/netlink/af_netlink.c
