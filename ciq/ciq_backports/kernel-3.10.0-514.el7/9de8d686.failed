perf/x86/intel/rapl: Convert it to a per package facility

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 9de8d686955b0e8e27847ed4edbbcd280f3fd853
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/9de8d686.failed

RAPL is a per package facility and we already have a mechanism for a dedicated
per package reader. So there is no point to have multiple CPUs doing the
same. The current implementation actually starts two timers on two CPUs if one
does:

	perf stat -C1,2 -e -e power/energy-pkg ....

which makes the whole concept of 1 reader per package moot.

What's worse is that the above returns the double of the actual energy
consumption, but that's a different problem to address and cannot be solved by
removing the pointless per cpuness of that mechanism.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andi Kleen <andi.kleen@intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Harish Chegondi <harish.chegondi@intel.com>
	Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Kan Liang <kan.liang@intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Cc: linux-kernel@vger.kernel.org
Link: http://lkml.kernel.org/r/20160222221012.845369524@linutronix.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 9de8d686955b0e8e27847ed4edbbcd280f3fd853)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel_rapl.c
diff --cc arch/x86/kernel/cpu/perf_event_intel_rapl.c
index c019c57572a7,019e541fa988..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel_rapl.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_rapl.c
@@@ -123,23 -120,33 +123,40 @@@ static struct perf_pmu_events_attr even
  };
  
  struct rapl_pmu {
 -	raw_spinlock_t		lock;
 -	int			n_active;
 -	int			cpu;
 -	struct list_head	active_list;
 -	struct pmu		*pmu;
 -	ktime_t			timer_interval;
 -	struct hrtimer		hrtimer;
 +	spinlock_t	 lock;
 +	int		 n_active; /* number of active events */
 +	struct list_head active_list;
 +	struct pmu	 *pmu; /* pointer to rapl_pmu_class */
 +	ktime_t		 timer_interval; /* in ktime_t unit */
 +	struct hrtimer   hrtimer;
  };
  
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +static int rapl_hw_unit[NR_RAPL_DOMAINS] __read_mostly;  /* 1/2^hw_unit Joule */
 +static struct pmu rapl_pmu_class;
 +static cpumask_t rapl_cpu_mask;
 +static int rapl_cntr_mask;
++=======
+ struct rapl_pmus {
+ 	struct pmu		pmu;
+ 	unsigned int		maxpkg;
+ 	struct rapl_pmu		*pmus[];
+ };
  
- static DEFINE_PER_CPU(struct rapl_pmu *, rapl_pmu);
- static DEFINE_PER_CPU(struct rapl_pmu *, rapl_pmu_to_free);
+  /* 1/2^hw_unit Joule */
+ static int rapl_hw_unit[NR_RAPL_DOMAINS] __read_mostly;
+ static struct rapl_pmus *rapl_pmus;
+ static cpumask_t rapl_cpu_mask;
+ static unsigned int rapl_cntr_mask;
+ static u64 rapl_timer_ms;
++>>>>>>> 9de8d686955b (perf/x86/intel/rapl: Convert it to a per package facility):arch/x86/events/intel/rapl.c
+ 
+ static inline struct rapl_pmu *cpu_to_rapl_pmu(unsigned int cpu)
+ {
+ 	return rapl_pmus->pmus[topology_logical_package_id(cpu)];
+ }
  
 +static struct x86_pmu_quirk *rapl_quirks;
  static inline u64 rapl_read_counter(struct perf_event *event)
  {
  	u64 raw;
@@@ -382,6 -378,9 +400,12 @@@ static int rapl_pmu_event_init(struct p
  		return -EINVAL;
  
  	/* must be done before validate_group */
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
++=======
+ 	pmu = cpu_to_rapl_pmu(event->cpu);
+ 	event->cpu = pmu->cpu;
+ 	event->pmu_private = pmu;
++>>>>>>> 9de8d686955b (perf/x86/intel/rapl: Convert it to a per package facility):arch/x86/events/intel/rapl.c
  	event->hw.event_base = msr;
  	event->hw.config = cfg;
  	event->hw.idx = bit;
@@@ -516,139 -511,62 +540,109 @@@ const struct attribute_group *rapl_attr
  	NULL,
  };
  
- static struct pmu rapl_pmu_class = {
- 	.attr_groups	= rapl_attr_groups,
- 	.task_ctx_nr	= perf_invalid_context, /* system-wide only */
- 	.event_init	= rapl_pmu_event_init,
- 	.add		= rapl_pmu_event_add, /* must have */
- 	.del		= rapl_pmu_event_del, /* must have */
- 	.start		= rapl_pmu_event_start,
- 	.stop		= rapl_pmu_event_stop,
- 	.read		= rapl_pmu_event_read,
- };
- 
  static void rapl_cpu_exit(int cpu)
  {
- 	struct rapl_pmu *pmu = per_cpu(rapl_pmu, cpu);
- 	int i, phys_id = topology_physical_package_id(cpu);
- 	int target = -1;
+ 	struct rapl_pmu *pmu = cpu_to_rapl_pmu(cpu);
+ 	int target;
  
- 	/* find a new cpu on same package */
- 	for_each_online_cpu(i) {
- 		if (i == cpu)
- 			continue;
- 		if (phys_id == topology_physical_package_id(i)) {
- 			target = i;
- 			break;
- 		}
- 	}
- 	/*
- 	 * clear cpu from cpumask
- 	 * if was set in cpumask and still some cpu on package,
- 	 * then move to new cpu
- 	 */
- 	if (cpumask_test_and_clear_cpu(cpu, &rapl_cpu_mask) && target >= 0)
- 		cpumask_set_cpu(target, &rapl_cpu_mask);
+ 	/* Check if exiting cpu is used for collecting rapl events */
+ 	if (!cpumask_test_and_clear_cpu(cpu, &rapl_cpu_mask))
+ 		return;
  
- 	WARN_ON(cpumask_empty(&rapl_cpu_mask));
- 	/*
- 	 * migrate events and context to new cpu
- 	 */
- 	if (target >= 0)
+ 	pmu->cpu = -1;
+ 	/* Find a new cpu to collect rapl events */
+ 	target = cpumask_any_but(topology_core_cpumask(cpu), cpu);
+ 
+ 	/* Migrate rapl events to the new target */
+ 	if (target < nr_cpu_ids) {
+ 		cpumask_set_cpu(target, &rapl_cpu_mask);
+ 		pmu->cpu = target;
  		perf_pmu_migrate_context(pmu->pmu, cpu, target);
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +
 +	/* cancel overflow polling timer for CPU */
 +	rapl_stop_hrtimer(pmu);
++=======
+ 	}
++>>>>>>> 9de8d686955b (perf/x86/intel/rapl: Convert it to a per package facility):arch/x86/events/intel/rapl.c
  }
  
  static void rapl_cpu_init(int cpu)
  {
- 	int i, phys_id = topology_physical_package_id(cpu);
+ 	struct rapl_pmu *pmu = cpu_to_rapl_pmu(cpu);
+ 	int target;
+ 
+ 	/*
+ 	 * Check if there is an online cpu in the package which collects rapl
+ 	 * events already.
+ 	 */
+ 	target = cpumask_any_and(&rapl_cpu_mask, topology_core_cpumask(cpu));
+ 	if (target < nr_cpu_ids)
+ 		return;
  
- 	/* check if phys_is is already covered */
- 	for_each_cpu(i, &rapl_cpu_mask) {
- 		if (phys_id == topology_physical_package_id(i))
- 			return;
- 	}
- 	/* was not found, so add it */
  	cpumask_set_cpu(cpu, &rapl_cpu_mask);
+ 	pmu->cpu = cpu;
  }
  
 +static __init void rapl_hsw_server_quirk(void)
 +{
 +	/*
 +	 * DRAM domain on HSW server has fixed energy unit which can be
 +	 * different than the unit from power unit MSR.
 +	 * "Intel Xeon Processor E5-1600 and E5-2600 v3 Product Families, V2
 +	 * of 2. Datasheet, September 2014, Reference Number: 330784-001 "
 +	 */
 +	rapl_hw_unit[RAPL_IDX_RAM_NRG_STAT] = 16;
 +}
 +
  static int rapl_cpu_prepare(int cpu)
  {
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	struct rapl_pmu *pmu = per_cpu(rapl_pmu, cpu);
 +	int phys_id = topology_physical_package_id(cpu);
 +	u64 ms;
++=======
+ 	struct rapl_pmu *pmu = cpu_to_rapl_pmu(cpu);
++>>>>>>> 9de8d686955b (perf/x86/intel/rapl: Convert it to a per package facility):arch/x86/events/intel/rapl.c
  
  	if (pmu)
  		return 0;
  
- 	if (phys_id < 0)
- 		return -1;
- 
  	pmu = kzalloc_node(sizeof(*pmu), GFP_KERNEL, cpu_to_node(cpu));
  	if (!pmu)
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +		return -1;
 +	spin_lock_init(&pmu->lock);
 +
 +	INIT_LIST_HEAD(&pmu->active_list);
 +
 +	pmu->pmu = &rapl_pmu_class;
 +
 +	/*
 +	 * use reference of 200W for scaling the timeout
 +	 * to avoid missing counter overflows.
 +	 * 200W = 200 Joules/sec
 +	 * divide interval by 2 to avoid lockstep (2 * 100)
 +	 * if hw unit is 32, then we use 2 ms 1/200/2
 +	 */
 +	if (rapl_hw_unit[0] < 32)
 +		ms = (1000 / (2 * 100)) * (1ULL << (32 - rapl_hw_unit[0] - 1));
 +	else
 +		ms = 2;
 +
 +	pmu->timer_interval = ms_to_ktime(ms);
 +
- 	rapl_hrtimer_init(pmu);
- 
- 	/* set RAPL pmu for this cpu for now */
- 	per_cpu(rapl_pmu, cpu) = pmu;
- 	per_cpu(rapl_pmu_to_free, cpu) = NULL;
- 
- 	return 0;
- }
- 
- static void rapl_cpu_kfree(int cpu)
- {
- 	struct rapl_pmu *pmu = per_cpu(rapl_pmu_to_free, cpu);
- 
- 	kfree(pmu);
- 
- 	per_cpu(rapl_pmu_to_free, cpu) = NULL;
- }
- 
- static int rapl_cpu_dying(int cpu)
- {
- 	struct rapl_pmu *pmu = per_cpu(rapl_pmu, cpu);
- 
- 	if (!pmu)
- 		return 0;
- 
- 	per_cpu(rapl_pmu, cpu) = NULL;
- 
- 	per_cpu(rapl_pmu_to_free, cpu) = pmu;
++=======
+ 		return -ENOMEM;
  
+ 	raw_spin_lock_init(&pmu->lock);
+ 	INIT_LIST_HEAD(&pmu->active_list);
+ 	pmu->pmu = &rapl_pmus->pmu;
+ 	pmu->timer_interval = ms_to_ktime(rapl_timer_ms);
+ 	pmu->cpu = -1;
++>>>>>>> 9de8d686955b (perf/x86/intel/rapl: Convert it to a per package facility):arch/x86/events/intel/rapl.c
+ 	rapl_hrtimer_init(pmu);
+ 	rapl_pmus->pmus[topology_logical_package_id(cpu)] = pmu;
  	return 0;
  }
  
@@@ -696,7 -617,87 +682,75 @@@ static int rapl_check_hw_unit(void
  	return 0;
  }
  
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +static const struct x86_cpu_id rapl_cpu_match[] = {
++=======
+ static void __init rapl_advertise(void)
+ {
+ 	int i;
+ 
+ 	pr_info("API unit is 2^-32 Joules, %d fixed counters, %llu ms ovfl timer\n",
+ 		hweight32(rapl_cntr_mask), rapl_timer_ms);
+ 
+ 	for (i = 0; i < NR_RAPL_DOMAINS; i++) {
+ 		if (rapl_cntr_mask & (1 << i)) {
+ 			pr_info("hw unit of domain %s 2^-%d Joules\n",
+ 				rapl_domain_names[i], rapl_hw_unit[i]);
+ 		}
+ 	}
+ }
+ 
+ static int __init rapl_prepare_cpus(void)
+ {
+ 	unsigned int cpu, pkg;
+ 	int ret;
+ 
+ 	for_each_online_cpu(cpu) {
+ 		pkg = topology_logical_package_id(cpu);
+ 		if (rapl_pmus->pmus[pkg])
+ 			continue;
+ 
+ 		ret = rapl_cpu_prepare(cpu);
+ 		if (ret)
+ 			return ret;
+ 		rapl_cpu_init(cpu);
+ 	}
+ 	return 0;
+ }
+ 
+ static void __init cleanup_rapl_pmus(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < rapl_pmus->maxpkg; i++)
+ 		kfree(rapl_pmus->pmus + i);
+ 	kfree(rapl_pmus);
+ }
+ 
+ static int __init init_rapl_pmus(void)
+ {
+ 	int maxpkg = topology_max_packages();
+ 	size_t size;
+ 
+ 	size = sizeof(*rapl_pmus) + maxpkg * sizeof(struct rapl_pmu *);
+ 	rapl_pmus = kzalloc(size, GFP_KERNEL);
+ 	if (!rapl_pmus)
+ 		return -ENOMEM;
+ 
+ 	rapl_pmus->maxpkg		= maxpkg;
+ 	rapl_pmus->pmu.attr_groups	= rapl_attr_groups;
+ 	rapl_pmus->pmu.task_ctx_nr	= perf_invalid_context;
+ 	rapl_pmus->pmu.event_init	= rapl_pmu_event_init;
+ 	rapl_pmus->pmu.add		= rapl_pmu_event_add;
+ 	rapl_pmus->pmu.del		= rapl_pmu_event_del;
+ 	rapl_pmus->pmu.start		= rapl_pmu_event_start;
+ 	rapl_pmus->pmu.stop		= rapl_pmu_event_stop;
+ 	rapl_pmus->pmu.read		= rapl_pmu_event_read;
+ 	return 0;
+ }
+ 
+ static const struct x86_cpu_id rapl_cpu_match[] __initconst = {
++>>>>>>> 9de8d686955b (perf/x86/intel/rapl: Convert it to a per package facility):arch/x86/events/intel/rapl.c
  	[0] = { .vendor = X86_VENDOR_INTEL, .family = 6 },
  	[1] = {},
  };
@@@ -750,44 -745,29 +804,60 @@@ static int __init rapl_pmu_init(void
  	if (ret)
  		return ret;
  
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	/* run cpu model quirks */
 +	for (quirk = rapl_quirks; quirk; quirk = quirk->next)
 +		quirk->func();
 +	cpu_notifier_register_begin();
 +
 +	for_each_online_cpu(cpu) {
 +		ret = rapl_cpu_prepare(cpu);
 +		if (ret)
 +			goto out;
 +		rapl_cpu_init(cpu);
 +	}
++=======
+ 	ret = init_rapl_pmus();
+ 	if (ret)
+ 		return ret;
+ 
+ 	cpu_notifier_register_begin();
+ 
+ 	ret = rapl_prepare_cpus();
+ 	if (ret)
+ 		goto out;
+ 
+ 	ret = perf_pmu_register(&rapl_pmus->pmu, "power", -1);
+ 	if (ret)
+ 		goto out;
++>>>>>>> 9de8d686955b (perf/x86/intel/rapl: Convert it to a per package facility):arch/x86/events/intel/rapl.c
  
  	__perf_cpu_notifier(rapl_cpu_notifier);
 -	cpu_notifier_register_done();
 -	rapl_advertise();
 -	return 0;
  
 +	ret = perf_pmu_register(&rapl_pmu_class, "power", -1);
 +	if (WARN_ON(ret)) {
 +		pr_info("RAPL PMU detected, registration failed (%d), RAPL PMU disabled\n", ret);
 +		cpu_notifier_register_done();
 +		return -1;
 +	}
 +
 +	pmu = __get_cpu_var(rapl_pmu);
 +
 +	pr_info("RAPL PMU detected,"
 +		" API unit is 2^-32 Joules,"
 +		" %d fixed counters"
 +		" %llu ms ovfl timer\n",
 +		hweight32(rapl_cntr_mask),
 +		ktime_to_ms(pmu->timer_interval));
 +	for (i = 0; i < NR_RAPL_DOMAINS; i++) {
 +		if (rapl_cntr_mask & (1 << i)) {
 +			pr_info("hw unit of domain %s 2^-%d Joules\n",
 +				rapl_domain_names[i], rapl_hw_unit[i]);
 +		}
 +	}
  out:
 -	pr_warn("Initialization failed (%d), disabled\n", ret);
 -	cleanup_rapl_pmus();
  	cpu_notifier_register_done();
 -	return ret;
 +
 +	return 0;
  }
  device_initcall(rapl_pmu_init);
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_rapl.c
