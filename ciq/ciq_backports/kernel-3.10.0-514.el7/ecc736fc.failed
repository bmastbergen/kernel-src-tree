memcg: fix endless loop caused by mem_cgroup_iter

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Michal Hocko <mhocko@suse.cz>
commit ecc736fc3c71c411a9d201d8588c9e7e049e5d8c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ecc736fc.failed

Hugh has reported an endless loop when the hardlimit reclaim sees the
same group all the time.  This might happen when the reclaim races with
the memcg removal.

shrink_zone
                                                [rmdir root]
  mem_cgroup_iter(root, NULL, reclaim)
    // prev = NULL
    rcu_read_lock()
    mem_cgroup_iter_load
      last_visited = iter->last_visited   // gets root || NULL
      css_tryget(last_visited)            // failed
      last_visited = NULL                 [1]
    memcg = root = __mem_cgroup_iter_next(root, NULL)
    mem_cgroup_iter_update
      iter->last_visited = root;
    reclaim->generation = iter->generation

 mem_cgroup_iter(root, root, reclaim)
   // prev = root
   rcu_read_lock
    mem_cgroup_iter_load
      last_visited = iter->last_visited   // gets root
      css_tryget(last_visited)            // failed
    [1]

The issue seemed to be introduced by commit 5f5781619718 ("memcg: relax
memcg iter caching") which has replaced unconditional css_get/css_put by
css_tryget/css_put for the cached iterator.

This patch fixes the issue by skipping css_tryget on the root of the
tree walk in mem_cgroup_iter_load and symmetrically doesn't release it
in mem_cgroup_iter_update.

	Signed-off-by: Michal Hocko <mhocko@suse.cz>
	Reported-by: Hugh Dickins <hughd@google.com>
	Tested-by: Hugh Dickins <hughd@google.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Greg Thelen <gthelen@google.com>
	Cc: <stable@vger.kernel.org>	[3.10+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ecc736fc3c71c411a9d201d8588c9e7e049e5d8c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index 7aa8a6b42188,da07784dde87..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -1149,6 -1130,68 +1149,71 @@@ skip_node
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ static void mem_cgroup_iter_invalidate(struct mem_cgroup *root)
+ {
+ 	/*
+ 	 * When a group in the hierarchy below root is destroyed, the
+ 	 * hierarchy iterator can no longer be trusted since it might
+ 	 * have pointed to the destroyed group.  Invalidate it.
+ 	 */
+ 	atomic_inc(&root->dead_count);
+ }
+ 
+ static struct mem_cgroup *
+ mem_cgroup_iter_load(struct mem_cgroup_reclaim_iter *iter,
+ 		     struct mem_cgroup *root,
+ 		     int *sequence)
+ {
+ 	struct mem_cgroup *position = NULL;
+ 	/*
+ 	 * A cgroup destruction happens in two stages: offlining and
+ 	 * release.  They are separated by a RCU grace period.
+ 	 *
+ 	 * If the iterator is valid, we may still race with an
+ 	 * offlining.  The RCU lock ensures the object won't be
+ 	 * released, tryget will fail if we lost the race.
+ 	 */
+ 	*sequence = atomic_read(&root->dead_count);
+ 	if (iter->last_dead_count == *sequence) {
+ 		smp_rmb();
+ 		position = iter->last_visited;
+ 
+ 		/*
+ 		 * We cannot take a reference to root because we might race
+ 		 * with root removal and returning NULL would end up in
+ 		 * an endless loop on the iterator user level when root
+ 		 * would be returned all the time.
+ 		 */
+ 		if (position && position != root &&
+ 				!css_tryget(&position->css))
+ 			position = NULL;
+ 	}
+ 	return position;
+ }
+ 
+ static void mem_cgroup_iter_update(struct mem_cgroup_reclaim_iter *iter,
+ 				   struct mem_cgroup *last_visited,
+ 				   struct mem_cgroup *new_position,
+ 				   struct mem_cgroup *root,
+ 				   int sequence)
+ {
+ 	/* root reference counting symmetric to mem_cgroup_iter_load */
+ 	if (last_visited && last_visited != root)
+ 		css_put(&last_visited->css);
+ 	/*
+ 	 * We store the sequence count from the time @last_visited was
+ 	 * loaded successfully instead of rereading it here so that we
+ 	 * don't lose destruction events in between.  We could have
+ 	 * raced with the destruction of @new_position after all.
+ 	 */
+ 	iter->last_visited = new_position;
+ 	smp_wmb();
+ 	iter->last_dead_count = sequence;
+ }
+ 
++>>>>>>> ecc736fc3c71 (memcg: fix endless loop caused by mem_cgroup_iter)
  /**
   * mem_cgroup_iter - iterate over memory cgroup hierarchy
   * @root: hierarchy root
@@@ -1230,12 -1254,8 +1295,17 @@@ struct mem_cgroup *mem_cgroup_iter(stru
  		memcg = __mem_cgroup_iter_next(root, last_visited);
  
  		if (reclaim) {
++<<<<<<< HEAD
 +			if (last_visited)
 +				css_put(&last_visited->css);
 +
 +			iter->last_visited = memcg;
 +			smp_wmb();
 +			iter->last_dead_count = dead_count;
++=======
+ 			mem_cgroup_iter_update(iter, last_visited, memcg, root,
+ 					seq);
++>>>>>>> ecc736fc3c71 (memcg: fix endless loop caused by mem_cgroup_iter)
  
  			if (!memcg)
  				iter->generation++;
* Unmerged path mm/memcontrol.c
