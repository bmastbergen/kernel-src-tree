rhashtable: Move future_tbl into struct bucket_table

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit c4db8848af6af92f90462258603be844baeab44d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/c4db8848.failed

This patch moves future_tbl to open up the possibility of having
multiple rehashes on the same table.

	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c4db8848af6af92f90462258603be844baeab44d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rhashtable.h
#	lib/rhashtable.c
diff --cc include/linux/rhashtable.h
index 0839d7b8cd60,1695378b3c5b..000000000000
--- a/include/linux/rhashtable.h
+++ b/include/linux/rhashtable.h
@@@ -24,11 -46,32 +24,40 @@@ struct rhash_head 
  	struct rhash_head __rcu		*next;
  };
  
++<<<<<<< HEAD
 +#define INIT_HASH_HEAD(ptr) ((ptr)->next = NULL)
 +
 +struct bucket_table {
 +	size_t				size;
 +	struct rhash_head __rcu		*buckets[];
++=======
+ /**
+  * struct bucket_table - Table of hash buckets
+  * @size: Number of hash buckets
+  * @rehash: Current bucket being rehashed
+  * @hash_rnd: Random seed to fold into hash
+  * @shift: Current size (1 << shift)
+  * @locks_mask: Mask to apply before accessing locks[]
+  * @locks: Array of spinlocks protecting individual buckets
+  * @walkers: List of active walkers
+  * @rcu: RCU structure for freeing the table
+  * @future_tbl: Table under construction during rehashing
+  * @buckets: size * hash buckets
+  */
+ struct bucket_table {
+ 	unsigned int		size;
+ 	unsigned int		rehash;
+ 	u32			hash_rnd;
+ 	u32			shift;
+ 	unsigned int		locks_mask;
+ 	spinlock_t		*locks;
+ 	struct list_head	walkers;
+ 	struct rcu_head		rcu;
+ 
+ 	struct bucket_table __rcu *future_tbl;
+ 
+ 	struct rhash_head __rcu	*buckets[] ____cacheline_aligned_in_smp;
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  };
  
  typedef u32 (*rht_hashfn_t)(const void *data, u32 len, u32 seed);
@@@ -77,16 -113,65 +106,21 @@@ struct rhashtable_params 
   */
  struct rhashtable {
  	struct bucket_table __rcu	*tbl;
++<<<<<<< HEAD
 +	size_t				nelems;
 +	size_t				shift;
++=======
+ 	atomic_t			nelems;
+ 	bool                            being_destroyed;
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  	struct rhashtable_params	p;
 -	struct work_struct		run_work;
 -	struct mutex                    mutex;
 -};
 -
 -/**
 - * struct rhashtable_walker - Hash table walker
 - * @list: List entry on list of walkers
 - * @tbl: The table that we were walking over
 - */
 -struct rhashtable_walker {
 -	struct list_head list;
 -	struct bucket_table *tbl;
  };
  
 -/**
 - * struct rhashtable_iter - Hash table iterator, fits into netlink cb
 - * @ht: Table to iterate through
 - * @p: Current pointer
 - * @walker: Associated rhashtable walker
 - * @slot: Current slot
 - * @skip: Number of entries to skip in slot
 - */
 -struct rhashtable_iter {
 -	struct rhashtable *ht;
 -	struct rhash_head *p;
 -	struct rhashtable_walker *walker;
 -	unsigned int slot;
 -	unsigned int skip;
 -};
 -
 -static inline unsigned long rht_marker(const struct rhashtable *ht, u32 hash)
 -{
 -	return NULLS_MARKER(ht->p.nulls_base + hash);
 -}
 -
 -#define INIT_RHT_NULLS_HEAD(ptr, ht, hash) \
 -	((ptr) = (typeof(ptr)) rht_marker(ht, hash))
 -
 -static inline bool rht_is_a_nulls(const struct rhash_head *ptr)
 -{
 -	return ((unsigned long) ptr & 1);
 -}
 -
 -static inline unsigned long rht_get_nulls_value(const struct rhash_head *ptr)
 -{
 -	return ((unsigned long) ptr) >> 1;
 -}
 -
  #ifdef CONFIG_PROVE_LOCKING
 -int lockdep_rht_mutex_is_held(struct rhashtable *ht);
 +int lockdep_rht_mutex_is_held(const struct rhashtable *ht);
  int lockdep_rht_bucket_is_held(const struct bucket_table *tbl, u32 hash);
  #else
 -static inline int lockdep_rht_mutex_is_held(struct rhashtable *ht)
 +static inline int lockdep_rht_mutex_is_held(const struct rhashtable *ht)
  {
  	return 1;
  }
diff --cc lib/rhashtable.c
index 6d0c4774001c,9d53a46dcca9..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -134,56 -195,105 +134,138 @@@ EXPORT_SYMBOL_GPL(rht_grow_above_75)
  /**
   * rht_shrink_below_30 - returns true if nelems < 0.3 * table-size
   * @ht:		hash table
 - * @tbl:	current table
 + * @new_size:	new table size
   */
 -static bool rht_shrink_below_30(const struct rhashtable *ht,
 -				const struct bucket_table *tbl)
 +bool rht_shrink_below_30(const struct rhashtable *ht, size_t new_size)
  {
  	/* Shrink table beneath 30% load */
 -	return atomic_read(&ht->nelems) < (tbl->size * 3 / 10) &&
 -	       tbl->shift > ht->p.min_shift;
 +	return ht->nelems < (new_size * 3 / 10);
  }
 +EXPORT_SYMBOL_GPL(rht_shrink_below_30);
  
 -static int rhashtable_rehash_one(struct rhashtable *ht, unsigned old_hash)
 +static void hashtable_chain_unzip(const struct rhashtable *ht,
 +				  const struct bucket_table *new_tbl,
 +				  struct bucket_table *old_tbl, size_t n)
  {
++<<<<<<< HEAD
 +	struct rhash_head *he, *p, *next;
 +	unsigned int h;
++=======
+ 	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
+ 	struct bucket_table *new_tbl =
+ 		rht_dereference(old_tbl->future_tbl, ht) ?: old_tbl;
+ 	struct rhash_head __rcu **pprev = &old_tbl->buckets[old_hash];
+ 	int err = -ENOENT;
+ 	struct rhash_head *head, *next, *entry;
+ 	spinlock_t *new_bucket_lock;
+ 	unsigned new_hash;
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  
 -	rht_for_each(entry, old_tbl, old_hash) {
 -		err = 0;
 -		next = rht_dereference_bucket(entry->next, old_tbl, old_hash);
 +	/* Old bucket empty, no work needed. */
 +	p = rht_dereference(old_tbl->buckets[n], ht);
 +	if (!p)
 +		return;
  
 -		if (rht_is_a_nulls(next))
 +	/* Advance the old bucket pointer one or more times until it
 +	 * reaches a node that doesn't hash to the same bucket as the
 +	 * previous node p. Call the previous node p;
 +	 */
 +	h = head_hashfn(ht, new_tbl, p);
 +	rht_for_each_continue(he, p->next, old_tbl, n) {
 +		if (head_hashfn(ht, new_tbl, he) != h)
  			break;
 +		p = he;
 +	}
 +	RCU_INIT_POINTER(old_tbl->buckets[n], p->next);
  
 -		pprev = &entry->next;
 +	/* Find the subsequent node which does hash to the same
 +	 * bucket as node P, or NULL if no such node exists.
 +	 */
 +	next = NULL;
 +	if (he) {
 +		rht_for_each_continue(he, he->next, old_tbl, n) {
 +			if (head_hashfn(ht, new_tbl, he) == h) {
 +				next = he;
 +				break;
 +			}
 +		}
  	}
  
++<<<<<<< HEAD
 +	/* Set p's next pointer to that subsequent node pointer,
 +	 * bypassing the nodes which do not hash to p's bucket
 +	 */
 +	RCU_INIT_POINTER(p->next, next);
++=======
+ 	if (err)
+ 		goto out;
+ 
+ 	new_hash = head_hashfn(ht, new_tbl, entry);
+ 
+ 	new_bucket_lock = bucket_lock(new_tbl, new_hash);
+ 
+ 	spin_lock_nested(new_bucket_lock, SINGLE_DEPTH_NESTING);
+ 	head = rht_dereference_bucket(new_tbl->buckets[new_hash],
+ 				      new_tbl, new_hash);
+ 
+ 	if (rht_is_a_nulls(head))
+ 		INIT_RHT_NULLS_HEAD(entry->next, ht, new_hash);
+ 	else
+ 		RCU_INIT_POINTER(entry->next, head);
+ 
+ 	rcu_assign_pointer(new_tbl->buckets[new_hash], entry);
+ 	spin_unlock(new_bucket_lock);
+ 
+ 	rcu_assign_pointer(*pprev, next);
+ 
+ out:
+ 	return err;
+ }
+ 
+ static void rhashtable_rehash_chain(struct rhashtable *ht, unsigned old_hash)
+ {
+ 	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
+ 	spinlock_t *old_bucket_lock;
+ 
+ 	old_bucket_lock = bucket_lock(old_tbl, old_hash);
+ 
+ 	spin_lock_bh(old_bucket_lock);
+ 	while (!rhashtable_rehash_one(ht, old_hash))
+ 		;
+ 	old_tbl->rehash++;
+ 	spin_unlock_bh(old_bucket_lock);
+ }
+ 
+ static void rhashtable_rehash(struct rhashtable *ht,
+ 			      struct bucket_table *new_tbl)
+ {
+ 	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
+ 	struct rhashtable_walker *walker;
+ 	unsigned old_hash;
+ 
+ 	/* Make insertions go into the new, empty table right away. Deletions
+ 	 * and lookups will be attempted in both tables until we synchronize.
+ 	 */
+ 	rcu_assign_pointer(old_tbl->future_tbl, new_tbl);
+ 
+ 	/* Ensure the new table is visible to readers. */
+ 	smp_wmb();
+ 
+ 	for (old_hash = 0; old_hash < old_tbl->size; old_hash++)
+ 		rhashtable_rehash_chain(ht, old_hash);
+ 
+ 	/* Publish the new table pointer. */
+ 	rcu_assign_pointer(ht->tbl, new_tbl);
+ 
+ 	list_for_each_entry(walker, &old_tbl->walkers, list)
+ 		walker->tbl = NULL;
+ 
+ 	/* Wait for readers. All new readers will see the new
+ 	 * table, and thus no references to the old table will
+ 	 * remain.
+ 	 */
+ 	call_rcu(&old_tbl->rcu, bucket_table_free_rcu);
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  }
  
  /**
@@@ -319,8 -357,92 +401,95 @@@ int rhashtable_shrink(struct rhashtabl
  }
  EXPORT_SYMBOL_GPL(rhashtable_shrink);
  
++<<<<<<< HEAD
++=======
+ static void rht_deferred_worker(struct work_struct *work)
+ {
+ 	struct rhashtable *ht;
+ 	struct bucket_table *tbl;
+ 
+ 	ht = container_of(work, struct rhashtable, run_work);
+ 	mutex_lock(&ht->mutex);
+ 	if (ht->being_destroyed)
+ 		goto unlock;
+ 
+ 	tbl = rht_dereference(ht->tbl, ht);
+ 
+ 	if (rht_grow_above_75(ht, tbl))
+ 		rhashtable_expand(ht);
+ 	else if (rht_shrink_below_30(ht, tbl))
+ 		rhashtable_shrink(ht);
+ unlock:
+ 	mutex_unlock(&ht->mutex);
+ }
+ 
+ static bool __rhashtable_insert(struct rhashtable *ht, struct rhash_head *obj,
+ 				bool (*compare)(void *, void *), void *arg)
+ {
+ 	struct bucket_table *tbl, *old_tbl;
+ 	struct rhash_head *head;
+ 	bool no_resize_running;
+ 	unsigned hash;
+ 	bool success = true;
+ 
+ 	rcu_read_lock();
+ 
+ 	old_tbl = rht_dereference_rcu(ht->tbl, ht);
+ 	hash = head_hashfn(ht, old_tbl, obj);
+ 
+ 	spin_lock_bh(bucket_lock(old_tbl, hash));
+ 
+ 	/* Because we have already taken the bucket lock in old_tbl,
+ 	 * if we find that future_tbl is not yet visible then that
+ 	 * guarantees all other insertions of the same entry will
+ 	 * also grab the bucket lock in old_tbl because until the
+ 	 * rehash completes ht->tbl won't be changed.
+ 	 */
+ 	tbl = rht_dereference_rcu(old_tbl->future_tbl, ht) ?: old_tbl;
+ 	if (tbl != old_tbl) {
+ 		hash = head_hashfn(ht, tbl, obj);
+ 		spin_lock_nested(bucket_lock(tbl, hash), SINGLE_DEPTH_NESTING);
+ 	}
+ 
+ 	if (compare &&
+ 	    rhashtable_lookup_compare(ht, rht_obj(ht, obj) + ht->p.key_offset,
+ 				      compare, arg)) {
+ 		success = false;
+ 		goto exit;
+ 	}
+ 
+ 	no_resize_running = tbl == old_tbl;
+ 
+ 	head = rht_dereference_bucket(tbl->buckets[hash], tbl, hash);
+ 
+ 	if (rht_is_a_nulls(head))
+ 		INIT_RHT_NULLS_HEAD(obj->next, ht, hash);
+ 	else
+ 		RCU_INIT_POINTER(obj->next, head);
+ 
+ 	rcu_assign_pointer(tbl->buckets[hash], obj);
+ 
+ 	atomic_inc(&ht->nelems);
+ 	if (no_resize_running && rht_grow_above_75(ht, tbl))
+ 		schedule_work(&ht->run_work);
+ 
+ exit:
+ 	if (tbl != old_tbl) {
+ 		hash = head_hashfn(ht, tbl, obj);
+ 		spin_unlock(bucket_lock(tbl, hash));
+ 	}
+ 
+ 	hash = head_hashfn(ht, old_tbl, obj);
+ 	spin_unlock_bh(bucket_lock(old_tbl, hash));
+ 
+ 	rcu_read_unlock();
+ 
+ 	return success;
+ }
+ 
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  /**
 - * rhashtable_insert - insert object into hash table
 + * rhashtable_insert - insert object into hash hash table
   * @ht:		hash table
   * @obj:	pointer to hash head inside object
   *
@@@ -364,33 -511,34 +533,44 @@@ EXPORT_SYMBOL_GPL(rhashtable_insert)
   */
  bool rhashtable_remove(struct rhashtable *ht, struct rhash_head *obj)
  {
 -	struct bucket_table *tbl, *old_tbl;
 -	bool ret;
 +	struct bucket_table *tbl = rht_dereference(ht->tbl, ht);
 +	struct rhash_head __rcu **pprev;
 +	struct rhash_head *he;
 +	u32 h;
  
 -	rcu_read_lock();
 +	ASSERT_RHT_MUTEX(ht);
  
 -	old_tbl = rht_dereference_rcu(ht->tbl, ht);
 -	ret = __rhashtable_remove(ht, old_tbl, obj);
 +	h = head_hashfn(ht, tbl, obj);
  
++<<<<<<< HEAD
 +	pprev = &tbl->buckets[h];
 +	rht_for_each(he, tbl, h) {
 +		if (he != obj) {
 +			pprev = &he->next;
 +			continue;
 +		}
++=======
+ 	/* Because we have already taken (and released) the bucket
+ 	 * lock in old_tbl, if we find that future_tbl is not yet
+ 	 * visible then that guarantees the entry to still be in
+ 	 * old_tbl if it exists.
+ 	 */
+ 	tbl = rht_dereference_rcu(old_tbl->future_tbl, ht) ?: old_tbl;
+ 	if (!ret && old_tbl != tbl)
+ 		ret = __rhashtable_remove(ht, tbl, obj);
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  
 -	if (ret) {
 -		bool no_resize_running = tbl == old_tbl;
 +		RCU_INIT_POINTER(*pprev, he->next);
 +		ht->nelems--;
  
 -		atomic_dec(&ht->nelems);
 -		if (no_resize_running && rht_shrink_below_30(ht, tbl))
 -			schedule_work(&ht->run_work);
 -	}
 +		if (ht->p.shrink_decision &&
 +		    ht->p.shrink_decision(ht, tbl->size))
 +			rhashtable_shrink(ht);
  
 -	rcu_read_unlock();
 +		return true;
 +	}
  
 -	return ret;
 +	return false;
  }
  EXPORT_SYMBOL_GPL(rhashtable_remove);
  
@@@ -443,10 -595,10 +623,14 @@@ EXPORT_SYMBOL_GPL(rhashtable_lookup)
   *
   * Returns the first entry on which the compare function returned true.
   */
 -void *rhashtable_lookup_compare(struct rhashtable *ht, const void *key,
 +void *rhashtable_lookup_compare(const struct rhashtable *ht, const void *key,
  				bool (*compare)(void *, void *), void *arg)
  {
++<<<<<<< HEAD
 +	const struct bucket_table *tbl = rht_dereference_rcu(ht->tbl, ht);
++=======
+ 	const struct bucket_table *tbl;
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  	struct rhash_head *he;
  	u32 hash;
  
@@@ -457,10 -613,265 +641,270 @@@
  		return rht_obj(ht, he);
  	}
  
++<<<<<<< HEAD
++=======
+ 	/* Ensure we see any new tables. */
+ 	smp_rmb();
+ 
+ 	tbl = rht_dereference_rcu(tbl->future_tbl, ht);
+ 	if (unlikely(tbl))
+ 		goto restart;
+ 	rcu_read_unlock();
+ 
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  	return NULL;
  }
  EXPORT_SYMBOL_GPL(rhashtable_lookup_compare);
  
++<<<<<<< HEAD
++=======
+ /**
+  * rhashtable_lookup_insert - lookup and insert object into hash table
+  * @ht:		hash table
+  * @obj:	pointer to hash head inside object
+  *
+  * Locks down the bucket chain in both the old and new table if a resize
+  * is in progress to ensure that writers can't remove from the old table
+  * and can't insert to the new table during the atomic operation of search
+  * and insertion. Searches for duplicates in both the old and new table if
+  * a resize is in progress.
+  *
+  * This lookup function may only be used for fixed key hash table (key_len
+  * parameter set). It will BUG() if used inappropriately.
+  *
+  * It is safe to call this function from atomic context.
+  *
+  * Will trigger an automatic deferred table resizing if the size grows
+  * beyond the watermark indicated by grow_decision() which can be passed
+  * to rhashtable_init().
+  */
+ bool rhashtable_lookup_insert(struct rhashtable *ht, struct rhash_head *obj)
+ {
+ 	struct rhashtable_compare_arg arg = {
+ 		.ht = ht,
+ 		.key = rht_obj(ht, obj) + ht->p.key_offset,
+ 	};
+ 
+ 	BUG_ON(!ht->p.key_len);
+ 
+ 	return rhashtable_lookup_compare_insert(ht, obj, &rhashtable_compare,
+ 						&arg);
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_lookup_insert);
+ 
+ /**
+  * rhashtable_lookup_compare_insert - search and insert object to hash table
+  *                                    with compare function
+  * @ht:		hash table
+  * @obj:	pointer to hash head inside object
+  * @compare:	compare function, must return true on match
+  * @arg:	argument passed on to compare function
+  *
+  * Locks down the bucket chain in both the old and new table if a resize
+  * is in progress to ensure that writers can't remove from the old table
+  * and can't insert to the new table during the atomic operation of search
+  * and insertion. Searches for duplicates in both the old and new table if
+  * a resize is in progress.
+  *
+  * Lookups may occur in parallel with hashtable mutations and resizing.
+  *
+  * Will trigger an automatic deferred table resizing if the size grows
+  * beyond the watermark indicated by grow_decision() which can be passed
+  * to rhashtable_init().
+  */
+ bool rhashtable_lookup_compare_insert(struct rhashtable *ht,
+ 				      struct rhash_head *obj,
+ 				      bool (*compare)(void *, void *),
+ 				      void *arg)
+ {
+ 	BUG_ON(!ht->p.key_len);
+ 
+ 	return __rhashtable_insert(ht, obj, compare, arg);
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_lookup_compare_insert);
+ 
+ /**
+  * rhashtable_walk_init - Initialise an iterator
+  * @ht:		Table to walk over
+  * @iter:	Hash table Iterator
+  *
+  * This function prepares a hash table walk.
+  *
+  * Note that if you restart a walk after rhashtable_walk_stop you
+  * may see the same object twice.  Also, you may miss objects if
+  * there are removals in between rhashtable_walk_stop and the next
+  * call to rhashtable_walk_start.
+  *
+  * For a completely stable walk you should construct your own data
+  * structure outside the hash table.
+  *
+  * This function may sleep so you must not call it from interrupt
+  * context or with spin locks held.
+  *
+  * You must call rhashtable_walk_exit if this function returns
+  * successfully.
+  */
+ int rhashtable_walk_init(struct rhashtable *ht, struct rhashtable_iter *iter)
+ {
+ 	iter->ht = ht;
+ 	iter->p = NULL;
+ 	iter->slot = 0;
+ 	iter->skip = 0;
+ 
+ 	iter->walker = kmalloc(sizeof(*iter->walker), GFP_KERNEL);
+ 	if (!iter->walker)
+ 		return -ENOMEM;
+ 
+ 	mutex_lock(&ht->mutex);
+ 	iter->walker->tbl = rht_dereference(ht->tbl, ht);
+ 	list_add(&iter->walker->list, &iter->walker->tbl->walkers);
+ 	mutex_unlock(&ht->mutex);
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_init);
+ 
+ /**
+  * rhashtable_walk_exit - Free an iterator
+  * @iter:	Hash table Iterator
+  *
+  * This function frees resources allocated by rhashtable_walk_init.
+  */
+ void rhashtable_walk_exit(struct rhashtable_iter *iter)
+ {
+ 	mutex_lock(&iter->ht->mutex);
+ 	if (iter->walker->tbl)
+ 		list_del(&iter->walker->list);
+ 	mutex_unlock(&iter->ht->mutex);
+ 	kfree(iter->walker);
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_exit);
+ 
+ /**
+  * rhashtable_walk_start - Start a hash table walk
+  * @iter:	Hash table iterator
+  *
+  * Start a hash table walk.  Note that we take the RCU lock in all
+  * cases including when we return an error.  So you must always call
+  * rhashtable_walk_stop to clean up.
+  *
+  * Returns zero if successful.
+  *
+  * Returns -EAGAIN if resize event occured.  Note that the iterator
+  * will rewind back to the beginning and you may use it immediately
+  * by calling rhashtable_walk_next.
+  */
+ int rhashtable_walk_start(struct rhashtable_iter *iter)
+ {
+ 	struct rhashtable *ht = iter->ht;
+ 
+ 	mutex_lock(&ht->mutex);
+ 
+ 	if (iter->walker->tbl)
+ 		list_del(&iter->walker->list);
+ 
+ 	rcu_read_lock();
+ 
+ 	mutex_unlock(&ht->mutex);
+ 
+ 	if (!iter->walker->tbl) {
+ 		iter->walker->tbl = rht_dereference_rcu(ht->tbl, ht);
+ 		return -EAGAIN;
+ 	}
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_start);
+ 
+ /**
+  * rhashtable_walk_next - Return the next object and advance the iterator
+  * @iter:	Hash table iterator
+  *
+  * Note that you must call rhashtable_walk_stop when you are finished
+  * with the walk.
+  *
+  * Returns the next object or NULL when the end of the table is reached.
+  *
+  * Returns -EAGAIN if resize event occured.  Note that the iterator
+  * will rewind back to the beginning and you may continue to use it.
+  */
+ void *rhashtable_walk_next(struct rhashtable_iter *iter)
+ {
+ 	struct bucket_table *tbl = iter->walker->tbl;
+ 	struct rhashtable *ht = iter->ht;
+ 	struct rhash_head *p = iter->p;
+ 	void *obj = NULL;
+ 
+ 	if (p) {
+ 		p = rht_dereference_bucket_rcu(p->next, tbl, iter->slot);
+ 		goto next;
+ 	}
+ 
+ 	for (; iter->slot < tbl->size; iter->slot++) {
+ 		int skip = iter->skip;
+ 
+ 		rht_for_each_rcu(p, tbl, iter->slot) {
+ 			if (!skip)
+ 				break;
+ 			skip--;
+ 		}
+ 
+ next:
+ 		if (!rht_is_a_nulls(p)) {
+ 			iter->skip++;
+ 			iter->p = p;
+ 			obj = rht_obj(ht, p);
+ 			goto out;
+ 		}
+ 
+ 		iter->skip = 0;
+ 	}
+ 
+ 	iter->walker->tbl = rht_dereference_rcu(tbl->future_tbl, ht);
+ 	if (iter->walker->tbl) {
+ 		iter->slot = 0;
+ 		iter->skip = 0;
+ 		return ERR_PTR(-EAGAIN);
+ 	}
+ 
+ 	iter->p = NULL;
+ 
+ out:
+ 
+ 	return obj;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_next);
+ 
+ /**
+  * rhashtable_walk_stop - Finish a hash table walk
+  * @iter:	Hash table iterator
+  *
+  * Finish a hash table walk.
+  */
+ void rhashtable_walk_stop(struct rhashtable_iter *iter)
+ {
+ 	struct rhashtable *ht;
+ 	struct bucket_table *tbl = iter->walker->tbl;
+ 
+ 	rcu_read_unlock();
+ 
+ 	if (!tbl)
+ 		return;
+ 
+ 	ht = iter->ht;
+ 
+ 	mutex_lock(&ht->mutex);
+ 	if (tbl->rehash < tbl->size)
+ 		list_add(&iter->walker->list, &tbl->walkers);
+ 	else
+ 		iter->walker->tbl = NULL;
+ 	mutex_unlock(&ht->mutex);
+ 
+ 	iter->p = NULL;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_walk_stop);
+ 
++>>>>>>> c4db8848af6a (rhashtable: Move future_tbl into struct bucket_table)
  static size_t rounded_hashtable_size(struct rhashtable_params *params)
  {
  	return max(roundup_pow_of_two(params->nelem_hint * 4 / 3),
* Unmerged path include/linux/rhashtable.h
* Unmerged path lib/rhashtable.c
