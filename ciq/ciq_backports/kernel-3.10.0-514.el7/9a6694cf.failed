perf/x86/intel/pt: Do not force sync packets on every schedule-in

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Alexander Shishkin <alexander.shishkin@linux.intel.com>
commit 9a6694cfa2390181dec936a17c0d9d21ef7b08d9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/9a6694cf.failed

Currently, the PT driver zeroes out the status register every time before
starting the event. However, all the writable bits are already taken care
of in pt_handle_status() function, except the new PacketByteCnt field,
which in new versions of PT contains the number of packet bytes written
since the last sync (PSB) packet. Zeroing it out before enabling PT forces
a sync packet to be written. This means that, with the existing code, a
sync packet (PSB and PSBEND, 18 bytes in total) will be generated every
time a PT event is scheduled in.

To avoid these unnecessary syncs and save a WRMSR in the fast path, this
patch changes the default behavior to not clear PacketByteCnt field, so
that the sync packets will be generated with the period specified as
"psb_period" attribute config field. This has little impact on the trace
data as the other packets that are normally sent within PSB+ (between PSB
and PSBEND) have their own generation scenarios which do not depend on the
sync packets.

One exception where we do need to force PSB like this when tracing starts,
so that the decoder has a clear sync point in the trace. For this purpose
we aready have hw::itrace_started flag, which we are currently using to
output PERF_RECORD_ITRACE_START. This patch moves setting itrace_started
from perf core to the pmu::start, where it should still be 0 on the very
first run.

	Signed-off-by: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: acme@infradead.org
	Cc: adrian.hunter@intel.com
	Cc: hpa@zytor.com
Link: http://lkml.kernel.org/r/1438264104-16189-1-git-send-email-alexander.shishkin@linux.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 9a6694cfa2390181dec936a17c0d9d21ef7b08d9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel_pt.c
#	kernel/events/core.c
diff --cc kernel/events/core.c
index c71870e24d1d,bdea12924b11..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -5791,6 -6121,42 +5791,45 @@@ static void perf_log_throttle(struct pe
  	perf_output_end(&handle);
  }
  
++<<<<<<< HEAD
++=======
+ static void perf_log_itrace_start(struct perf_event *event)
+ {
+ 	struct perf_output_handle handle;
+ 	struct perf_sample_data sample;
+ 	struct perf_aux_event {
+ 		struct perf_event_header        header;
+ 		u32				pid;
+ 		u32				tid;
+ 	} rec;
+ 	int ret;
+ 
+ 	if (event->parent)
+ 		event = event->parent;
+ 
+ 	if (!(event->pmu->capabilities & PERF_PMU_CAP_ITRACE) ||
+ 	    event->hw.itrace_started)
+ 		return;
+ 
+ 	rec.header.type	= PERF_RECORD_ITRACE_START;
+ 	rec.header.misc	= 0;
+ 	rec.header.size	= sizeof(rec);
+ 	rec.pid	= perf_event_pid(event, current);
+ 	rec.tid	= perf_event_tid(event, current);
+ 
+ 	perf_event_header__init_id(&rec.header, &sample, event);
+ 	ret = perf_output_begin(&handle, event, rec.header.size);
+ 
+ 	if (ret)
+ 		return;
+ 
+ 	perf_output_put(&handle, rec);
+ 	perf_event__output_id_sample(event, &handle, &sample);
+ 
+ 	perf_output_end(&handle);
+ }
+ 
++>>>>>>> 9a6694cfa239 (perf/x86/intel/pt: Do not force sync packets on every schedule-in)
  /*
   * Generic event overflow handling, sampling.
   */
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_pt.c
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_pt.c
* Unmerged path kernel/events/core.c
