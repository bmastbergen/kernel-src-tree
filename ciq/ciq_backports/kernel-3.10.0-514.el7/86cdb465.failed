mm: prepare to remove /proc/sys/vm/hugepages_treat_as_movable

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] prepare to remove /proc/sys/vm/hugepages_treat_as_movable (Tomoaki Nishimura) [1287322]
Rebuild_FUZZ: 96.61%
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit 86cdb465cf3a9d81058b517af05074157fa9dcdd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/86cdb465.failed

Now hugepage migration is enabled, although restricted on pmd-based
hugepages for now (due to lack of testing.) So we should allocate
migratable hugepages from ZONE_MOVABLE if possible.

This patch makes GFP flags in hugepage allocation dependent on migration
support, not only the value of hugepages_treat_as_movable.  It provides no
change on the behavior for architectures which do not support hugepage
migration,

	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Acked-by: Andi Kleen <ak@linux.intel.com>
	Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
	Cc: Hillf Danton <dhillf@gmail.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 86cdb465cf3a9d81058b517af05074157fa9dcdd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 4a192c9e708d,b49579c7f2a5..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -580,6 -604,162 +588,165 @@@ err
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ static void update_and_free_page(struct hstate *h, struct page *page)
+ {
+ 	int i;
+ 
+ 	VM_BUG_ON(h->order >= MAX_ORDER);
+ 
+ 	h->nr_huge_pages--;
+ 	h->nr_huge_pages_node[page_to_nid(page)]--;
+ 	for (i = 0; i < pages_per_huge_page(h); i++) {
+ 		page[i].flags &= ~(1 << PG_locked | 1 << PG_error |
+ 				1 << PG_referenced | 1 << PG_dirty |
+ 				1 << PG_active | 1 << PG_reserved |
+ 				1 << PG_private | 1 << PG_writeback);
+ 	}
+ 	VM_BUG_ON(hugetlb_cgroup_from_page(page));
+ 	set_compound_page_dtor(page, NULL);
+ 	set_page_refcounted(page);
+ 	arch_release_hugepage(page);
+ 	__free_pages(page, huge_page_order(h));
+ }
+ 
+ struct hstate *size_to_hstate(unsigned long size)
+ {
+ 	struct hstate *h;
+ 
+ 	for_each_hstate(h) {
+ 		if (huge_page_size(h) == size)
+ 			return h;
+ 	}
+ 	return NULL;
+ }
+ 
+ static void free_huge_page(struct page *page)
+ {
+ 	/*
+ 	 * Can't pass hstate in here because it is called from the
+ 	 * compound page destructor.
+ 	 */
+ 	struct hstate *h = page_hstate(page);
+ 	int nid = page_to_nid(page);
+ 	struct hugepage_subpool *spool =
+ 		(struct hugepage_subpool *)page_private(page);
+ 	bool restore_reserve;
+ 
+ 	set_page_private(page, 0);
+ 	page->mapping = NULL;
+ 	BUG_ON(page_count(page));
+ 	BUG_ON(page_mapcount(page));
+ 	restore_reserve = PagePrivate(page);
+ 
+ 	spin_lock(&hugetlb_lock);
+ 	hugetlb_cgroup_uncharge_page(hstate_index(h),
+ 				     pages_per_huge_page(h), page);
+ 	if (restore_reserve)
+ 		h->resv_huge_pages++;
+ 
+ 	if (h->surplus_huge_pages_node[nid] && huge_page_order(h) < MAX_ORDER) {
+ 		/* remove the page from active list */
+ 		list_del(&page->lru);
+ 		update_and_free_page(h, page);
+ 		h->surplus_huge_pages--;
+ 		h->surplus_huge_pages_node[nid]--;
+ 	} else {
+ 		arch_clear_hugepage_flags(page);
+ 		enqueue_huge_page(h, page);
+ 	}
+ 	spin_unlock(&hugetlb_lock);
+ 	hugepage_subpool_put_pages(spool, 1);
+ }
+ 
+ static void prep_new_huge_page(struct hstate *h, struct page *page, int nid)
+ {
+ 	INIT_LIST_HEAD(&page->lru);
+ 	set_compound_page_dtor(page, free_huge_page);
+ 	spin_lock(&hugetlb_lock);
+ 	set_hugetlb_cgroup(page, NULL);
+ 	h->nr_huge_pages++;
+ 	h->nr_huge_pages_node[nid]++;
+ 	spin_unlock(&hugetlb_lock);
+ 	put_page(page); /* free it into the hugepage allocator */
+ }
+ 
+ static void prep_compound_gigantic_page(struct page *page, unsigned long order)
+ {
+ 	int i;
+ 	int nr_pages = 1 << order;
+ 	struct page *p = page + 1;
+ 
+ 	/* we rely on prep_new_huge_page to set the destructor */
+ 	set_compound_order(page, order);
+ 	__SetPageHead(page);
+ 	for (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {
+ 		__SetPageTail(p);
+ 		set_page_count(p, 0);
+ 		p->first_page = page;
+ 	}
+ }
+ 
+ /*
+  * PageHuge() only returns true for hugetlbfs pages, but not for normal or
+  * transparent huge pages.  See the PageTransHuge() documentation for more
+  * details.
+  */
+ int PageHuge(struct page *page)
+ {
+ 	compound_page_dtor *dtor;
+ 
+ 	if (!PageCompound(page))
+ 		return 0;
+ 
+ 	page = compound_head(page);
+ 	dtor = get_compound_page_dtor(page);
+ 
+ 	return dtor == free_huge_page;
+ }
+ EXPORT_SYMBOL_GPL(PageHuge);
+ 
+ pgoff_t __basepage_index(struct page *page)
+ {
+ 	struct page *page_head = compound_head(page);
+ 	pgoff_t index = page_index(page_head);
+ 	unsigned long compound_idx;
+ 
+ 	if (!PageHuge(page_head))
+ 		return page_index(page);
+ 
+ 	if (compound_order(page_head) >= MAX_ORDER)
+ 		compound_idx = page_to_pfn(page) - page_to_pfn(page_head);
+ 	else
+ 		compound_idx = page - page_head;
+ 
+ 	return (index << compound_order(page_head)) + compound_idx;
+ }
+ 
+ static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)
+ {
+ 	struct page *page;
+ 
+ 	if (h->order >= MAX_ORDER)
+ 		return NULL;
+ 
+ 	page = alloc_pages_exact_node(nid,
+ 		htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
+ 						__GFP_REPEAT|__GFP_NOWARN,
+ 		huge_page_order(h));
+ 	if (page) {
+ 		if (arch_prepare_hugepage(page)) {
+ 			__free_pages(page, huge_page_order(h));
+ 			return NULL;
+ 		}
+ 		prep_new_huge_page(h, page, nid);
+ 	}
+ 
+ 	return page;
+ }
+ 
++>>>>>>> 86cdb465cf3a (mm: prepare to remove /proc/sys/vm/hugepages_treat_as_movable)
  /*
   * common helper functions for hstate_next_node_to_{alloc|free}.
   * We may have allocated or freed a huge page based on a different
diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt
index fd6472d0a920..1d5823cb65b3 100644
--- a/Documentation/sysctl/vm.txt
+++ b/Documentation/sysctl/vm.txt
@@ -222,17 +222,25 @@ fragmentation index is <= extfrag_threshold. The default value is 500.
 
 hugepages_treat_as_movable
 
-This parameter is only useful when kernelcore= is specified at boot time to
-create ZONE_MOVABLE for pages that may be reclaimed or migrated. Huge pages
-are not movable so are not normally allocated from ZONE_MOVABLE. A non-zero
-value written to hugepages_treat_as_movable allows huge pages to be allocated
-from ZONE_MOVABLE.
-
-Once enabled, the ZONE_MOVABLE is treated as an area of memory the huge
-pages pool can easily grow or shrink within. Assuming that applications are
-not running that mlock() a lot of memory, it is likely the huge pages pool
-can grow to the size of ZONE_MOVABLE by repeatedly entering the desired value
-into nr_hugepages and triggering page reclaim.
+This parameter controls whether we can allocate hugepages from ZONE_MOVABLE
+or not. If set to non-zero, hugepages can be allocated from ZONE_MOVABLE.
+ZONE_MOVABLE is created when kernel boot parameter kernelcore= is specified,
+so this parameter has no effect if used without kernelcore=.
+
+Hugepage migration is now available in some situations which depend on the
+architecture and/or the hugepage size. If a hugepage supports migration,
+allocation from ZONE_MOVABLE is always enabled for the hugepage regardless
+of the value of this parameter.
+IOW, this parameter affects only non-migratable hugepages.
+
+Assuming that hugepages are not migratable in your system, one usecase of
+this parameter is that users can make hugepage pool more extensible by
+enabling the allocation from ZONE_MOVABLE. This is because on ZONE_MOVABLE
+page reclaim/migration/compaction work more and you can get contiguous
+memory more likely. Note that using ZONE_MOVABLE for non-migratable
+hugepages can do harm to other features like memory hotremove (because
+memory hotremove expects that memory blocks on ZONE_MOVABLE are always
+removable,) so it's a trade-off responsible for the users.
 
 ==============================================================
 
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 88c4796ecf48..7a693fd905ba 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1274,7 +1274,7 @@ static struct ctl_table vm_table[] = {
 		.data		= &hugepages_treat_as_movable,
 		.maxlen		= sizeof(int),
 		.mode		= 0644,
-		.proc_handler	= hugetlb_treat_movable_handler,
+		.proc_handler	= proc_dointvec,
 	},
 	{
 		.procname	= "nr_overcommit_hugepages",
* Unmerged path mm/hugetlb.c
