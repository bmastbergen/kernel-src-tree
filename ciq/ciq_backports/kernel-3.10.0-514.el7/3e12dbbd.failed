xfs: fix inode size update overflow in xfs_map_direct()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit 3e12dbbdbd8809f0455920e42fdbf9eddc002651
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/3e12dbbd.failed

Both direct IO and DAX pass an offset and count into get_blocks that
will overflow a s64 variable when an IO goes into the last supported
block in a file (i.e. at offset 2^63 - 1FSB bytes). This can be seen
from the tracing:

xfs_get_blocks_alloc: [...] offset 0x7ffffffffffff000 count 4096
xfs_gbmap_direct:     [...] offset 0x7ffffffffffff000 count 4096
xfs_gbmap_direct_none:[...] offset 0x7ffffffffffff000 count 4096

0x7ffffffffffff000 + 4096 = 0x8000000000000000, and hence that
overflows the s64 offset and we fail to detect the need for a
filesize update and an ioend is not allocated.

This is *mostly* avoided for direct IO because such extending IOs
occur with full block allocation, and so the "IS_UNWRITTEN()" check
still evaluates as true and we get an ioend that way. However, doing
single sector extending IOs to this last block will expose the fact
that file size updates will not occur after the first allocating
direct IO as the overflow will then be exposed.

There is one further complexity: the DAX page fault path also
exposes the same issue in block allocation. However, page faults
cannot extend the file size, so in this case we want to allocate the
block but do not want to allocate an ioend to enable file size
update at IO completion. Hence we now need to distinguish between
the direct IO patch allocation and dax fault path allocation to
avoid leaking ioend structures.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 3e12dbbdbd8809f0455920e42fdbf9eddc002651)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_aops.c
#	fs/xfs/xfs_aops.h
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_aops.c
index 3ffbdb7cbd8f,e747d6ad5d18..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -1260,13 -1228,151 +1260,158 @@@ xfs_vm_releasepage
  	return try_to_free_buffers(page);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * When we map a DIO buffer, we may need to attach an ioend that describes the
+  * type of write IO we are doing. This passes to the completion function the
+  * operations it needs to perform. If the mapping is for an overwrite wholly
+  * within the EOF then we don't need an ioend and so we don't allocate one.
+  * This avoids the unnecessary overhead of allocating and freeing ioends for
+  * workloads that don't require transactions on IO completion.
+  *
+  * If we get multiple mappings in a single IO, we might be mapping different
+  * types. But because the direct IO can only have a single private pointer, we
+  * need to ensure that:
+  *
+  * a) i) the ioend spans the entire region of unwritten mappings; or
+  *    ii) the ioend spans all the mappings that cross or are beyond EOF; and
+  * b) if it contains unwritten extents, it is *permanently* marked as such
+  *
+  * We could do this by chaining ioends like buffered IO does, but we only
+  * actually get one IO completion callback from the direct IO, and that spans
+  * the entire IO regardless of how many mappings and IOs are needed to complete
+  * the DIO. There is only going to be one reference to the ioend and its life
+  * cycle is constrained by the DIO completion code. hence we don't need
+  * reference counting here.
+  *
+  * Note that for DIO, an IO to the highest supported file block offset (i.e.
+  * 2^63 - 1FSB bytes) will result in the offset + count overflowing a signed 64
+  * bit variable. Hence if we see this overflow, we have to assume that the IO is
+  * extending the file size. We won't know for sure until IO completion is run
+  * and the actual max write offset is communicated to the IO completion
+  * routine.
+  *
+  * For DAX page faults, we are preparing to never see unwritten extents here,
+  * nor should we ever extend the inode size. Hence we will soon have nothing to
+  * do here for this case, ensuring we don't have to provide an IO completion
+  * callback to free an ioend that we don't actually need for a fault into the
+  * page at offset (2^63 - 1FSB) bytes.
+  */
+ 
+ static void
+ xfs_map_direct(
+ 	struct inode		*inode,
+ 	struct buffer_head	*bh_result,
+ 	struct xfs_bmbt_irec	*imap,
+ 	xfs_off_t		offset,
+ 	bool			dax_fault)
+ {
+ 	struct xfs_ioend	*ioend;
+ 	xfs_off_t		size = bh_result->b_size;
+ 	int			type;
+ 
+ 	if (ISUNWRITTEN(imap))
+ 		type = XFS_IO_UNWRITTEN;
+ 	else
+ 		type = XFS_IO_OVERWRITE;
+ 
+ 	trace_xfs_gbmap_direct(XFS_I(inode), offset, size, type, imap);
+ 
+ 	/* XXX: preparation for removing unwritten extents in DAX */
+ #if 0
+ 	if (dax_fault) {
+ 		ASSERT(type == XFS_IO_OVERWRITE);
+ 		trace_xfs_gbmap_direct_none(XFS_I(inode), offset, size, type,
+ 					    imap);
+ 		return;
+ 	}
+ #endif
+ 
+ 	if (bh_result->b_private) {
+ 		ioend = bh_result->b_private;
+ 		ASSERT(ioend->io_size > 0);
+ 		ASSERT(offset >= ioend->io_offset);
+ 		if (offset + size > ioend->io_offset + ioend->io_size)
+ 			ioend->io_size = offset - ioend->io_offset + size;
+ 
+ 		if (type == XFS_IO_UNWRITTEN && type != ioend->io_type)
+ 			ioend->io_type = XFS_IO_UNWRITTEN;
+ 
+ 		trace_xfs_gbmap_direct_update(XFS_I(inode), ioend->io_offset,
+ 					      ioend->io_size, ioend->io_type,
+ 					      imap);
+ 	} else if (type == XFS_IO_UNWRITTEN ||
+ 		   offset + size > i_size_read(inode) ||
+ 		   offset + size < 0) {
+ 		ioend = xfs_alloc_ioend(inode, type);
+ 		ioend->io_offset = offset;
+ 		ioend->io_size = size;
+ 
+ 		bh_result->b_private = ioend;
+ 		set_buffer_defer_completion(bh_result);
+ 
+ 		trace_xfs_gbmap_direct_new(XFS_I(inode), offset, size, type,
+ 					   imap);
+ 	} else {
+ 		trace_xfs_gbmap_direct_none(XFS_I(inode), offset, size, type,
+ 					    imap);
+ 	}
+ }
+ 
+ /*
+  * If this is O_DIRECT or the mpage code calling tell them how large the mapping
+  * is, so that we can avoid repeated get_blocks calls.
+  *
+  * If the mapping spans EOF, then we have to break the mapping up as the mapping
+  * for blocks beyond EOF must be marked new so that sub block regions can be
+  * correctly zeroed. We can't do this for mappings within EOF unless the mapping
+  * was just allocated or is unwritten, otherwise the callers would overwrite
+  * existing data with zeros. Hence we have to split the mapping into a range up
+  * to and including EOF, and a second mapping for beyond EOF.
+  */
+ static void
+ xfs_map_trim_size(
+ 	struct inode		*inode,
+ 	sector_t		iblock,
+ 	struct buffer_head	*bh_result,
+ 	struct xfs_bmbt_irec	*imap,
+ 	xfs_off_t		offset,
+ 	ssize_t			size)
+ {
+ 	xfs_off_t		mapping_size;
+ 
+ 	mapping_size = imap->br_startoff + imap->br_blockcount - iblock;
+ 	mapping_size <<= inode->i_blkbits;
+ 
+ 	ASSERT(mapping_size > 0);
+ 	if (mapping_size > size)
+ 		mapping_size = size;
+ 	if (offset < i_size_read(inode) &&
+ 	    offset + mapping_size >= i_size_read(inode)) {
+ 		/* limit mapping to block that spans EOF */
+ 		mapping_size = roundup_64(i_size_read(inode) - offset,
+ 					  1 << inode->i_blkbits);
+ 	}
+ 	if (mapping_size > LONG_MAX)
+ 		mapping_size = LONG_MAX;
+ 
+ 	bh_result->b_size = mapping_size;
+ }
+ 
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  STATIC int
  __xfs_get_blocks(
  	struct inode		*inode,
  	sector_t		iblock,
  	struct buffer_head	*bh_result,
  	int			create,
++<<<<<<< HEAD
 +	int			direct)
++=======
+ 	bool			direct,
+ 	bool			dax_fault)
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  {
  	struct xfs_inode	*ip = XFS_I(inode);
  	struct xfs_mount	*mp = ip->i_mount;
@@@ -1358,19 -1468,25 +1503,26 @@@
  		goto out_unlock;
  	}
  
 -	/* trim mapping down to size requested */
 -	if (direct || size > (1 << inode->i_blkbits))
 -		xfs_map_trim_size(inode, iblock, bh_result,
 -				  &imap, offset, size);
 -
 -	/*
 -	 * For unwritten extents do not report a disk address in the buffered
 -	 * read case (treat as if we're reading into a hole).
 -	 */
  	if (imap.br_startblock != HOLESTARTBLOCK &&
 -	    imap.br_startblock != DELAYSTARTBLOCK &&
 -	    (create || !ISUNWRITTEN(&imap))) {
 -		xfs_map_buffer(inode, bh_result, &imap, offset);
 -		if (ISUNWRITTEN(&imap))
 +	    imap.br_startblock != DELAYSTARTBLOCK) {
 +		/*
 +		 * For unwritten extents do not report a disk address on
 +		 * the read case (treat as if we're reading into a hole).
 +		 */
 +		if (create || !ISUNWRITTEN(&imap))
 +			xfs_map_buffer(inode, bh_result, &imap, offset);
 +		if (create && ISUNWRITTEN(&imap)) {
 +			if (direct)
 +				bh_result->b_private = inode;
  			set_buffer_unwritten(bh_result);
++<<<<<<< HEAD
 +		}
++=======
+ 		/* direct IO needs special help */
+ 		if (create && direct)
+ 			xfs_map_direct(inode, bh_result, &imap, offset,
+ 				       dax_fault);
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  	}
  
  	/*
@@@ -1450,17 -1533,93 +1602,101 @@@ xfs_get_blocks
  	struct buffer_head	*bh_result,
  	int			create)
  {
++<<<<<<< HEAD
 +	return __xfs_get_blocks(inode, iblock, bh_result, create, 0);
++=======
+ 	return __xfs_get_blocks(inode, iblock, bh_result, create, false, false);
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  }
  
 -int
 +STATIC int
  xfs_get_blocks_direct(
  	struct inode		*inode,
  	sector_t		iblock,
  	struct buffer_head	*bh_result,
  	int			create)
  {
++<<<<<<< HEAD
 +	return __xfs_get_blocks(inode, iblock, bh_result, create, 1);
++=======
+ 	return __xfs_get_blocks(inode, iblock, bh_result, create, true, false);
+ }
+ 
+ int
+ xfs_get_blocks_dax_fault(
+ 	struct inode		*inode,
+ 	sector_t		iblock,
+ 	struct buffer_head	*bh_result,
+ 	int			create)
+ {
+ 	return __xfs_get_blocks(inode, iblock, bh_result, create, true, true);
+ }
+ 
+ static void
+ __xfs_end_io_direct_write(
+ 	struct inode		*inode,
+ 	struct xfs_ioend	*ioend,
+ 	loff_t			offset,
+ 	ssize_t			size)
+ {
+ 	struct xfs_mount	*mp = XFS_I(inode)->i_mount;
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp) || ioend->io_error)
+ 		goto out_end_io;
+ 
+ 	/*
+ 	 * dio completion end_io functions are only called on writes if more
+ 	 * than 0 bytes was written.
+ 	 */
+ 	ASSERT(size > 0);
+ 
+ 	/*
+ 	 * The ioend only maps whole blocks, while the IO may be sector aligned.
+ 	 * Hence the ioend offset/size may not match the IO offset/size exactly.
+ 	 * Because we don't map overwrites within EOF into the ioend, the offset
+ 	 * may not match, but only if the endio spans EOF.  Either way, write
+ 	 * the IO sizes into the ioend so that completion processing does the
+ 	 * right thing.
+ 	 */
+ 	ASSERT(offset + size <= ioend->io_offset + ioend->io_size);
+ 	ioend->io_size = size;
+ 	ioend->io_offset = offset;
+ 
+ 	/*
+ 	 * The ioend tells us whether we are doing unwritten extent conversion
+ 	 * or an append transaction that updates the on-disk file size. These
+ 	 * cases are the only cases where we should *potentially* be needing
+ 	 * to update the VFS inode size.
+ 	 *
+ 	 * We need to update the in-core inode size here so that we don't end up
+ 	 * with the on-disk inode size being outside the in-core inode size. We
+ 	 * have no other method of updating EOF for AIO, so always do it here
+ 	 * if necessary.
+ 	 *
+ 	 * We need to lock the test/set EOF update as we can be racing with
+ 	 * other IO completions here to update the EOF. Failing to serialise
+ 	 * here can result in EOF moving backwards and Bad Things Happen when
+ 	 * that occurs.
+ 	 */
+ 	spin_lock(&XFS_I(inode)->i_flags_lock);
+ 	if (offset + size > i_size_read(inode))
+ 		i_size_write(inode, offset + size);
+ 	spin_unlock(&XFS_I(inode)->i_flags_lock);
+ 
+ 	/*
+ 	 * If we are doing an append IO that needs to update the EOF on disk,
+ 	 * do the transaction reserve now so we can use common end io
+ 	 * processing. Stashing the error (if there is one) in the ioend will
+ 	 * result in the ioend processing passing on the error if it is
+ 	 * possible as we can't return it from here.
+ 	 */
+ 	if (ioend->io_type == XFS_IO_OVERWRITE)
+ 		ioend->io_error = xfs_setfilesize_trans_alloc(ioend);
+ 
+ out_end_io:
+ 	xfs_end_io(&ioend->io_work);
+ 	return;
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  }
  
  /*
diff --cc fs/xfs/xfs_aops.h
index c325abb8d61a,d39ba25ccc98..000000000000
--- a/fs/xfs/xfs_aops.h
+++ b/fs/xfs/xfs_aops.h
@@@ -59,7 -53,14 +59,18 @@@ typedef struct xfs_ioend 
  } xfs_ioend_t;
  
  extern const struct address_space_operations xfs_address_space_operations;
++<<<<<<< HEAD
 +extern int xfs_get_blocks(struct inode *, sector_t, struct buffer_head *, int);
++=======
+ 
+ int	xfs_get_blocks(struct inode *inode, sector_t offset,
+ 		       struct buffer_head *map_bh, int create);
+ int	xfs_get_blocks_direct(struct inode *inode, sector_t offset,
+ 			      struct buffer_head *map_bh, int create);
+ int	xfs_get_blocks_dax_fault(struct inode *inode, sector_t offset,
+ 			         struct buffer_head *map_bh, int create);
+ void	xfs_end_io_dax_write(struct buffer_head *bh, int uptodate);
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  
  extern void xfs_count_page_state(struct page *, int *, int *);
  
diff --cc fs/xfs/xfs_file.c
index ff72b4ea0a57,27abe1c92184..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -1562,21 -1493,103 +1562,111 @@@ xfs_filemap_page_mkwrite
  	struct vm_area_struct	*vma,
  	struct vm_fault		*vmf)
  {
 -	struct inode		*inode = file_inode(vma->vm_file);
 +	struct xfs_inode	*ip = XFS_I(vma->vm_file->f_mapping->host);
  	int			ret;
  
 -	trace_xfs_filemap_page_mkwrite(XFS_I(inode));
 +	trace_xfs_filemap_page_mkwrite(ip);
  
 -	sb_start_pagefault(inode->i_sb);
 +	sb_start_pagefault(VFS_I(ip)->i_sb);
  	file_update_time(vma->vm_file);
 -	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 +	xfs_ilock(ip, XFS_MMAPLOCK_SHARED);
  
++<<<<<<< HEAD
 +	ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
++=======
+ 	if (IS_DAX(inode)) {
+ 		ret = __dax_mkwrite(vma, vmf, xfs_get_blocks_dax_fault,
+ 				    xfs_end_io_dax_write);
+ 	} else {
+ 		ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
+ 		ret = block_page_mkwrite_return(ret);
+ 	}
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  
 -	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 -	sb_end_pagefault(inode->i_sb);
 +	xfs_iunlock(ip, XFS_MMAPLOCK_SHARED);
 +	sb_end_pagefault(VFS_I(ip)->i_sb);
  
++<<<<<<< HEAD
 +	return block_page_mkwrite_return(ret);
++=======
+ 	return ret;
+ }
+ 
+ STATIC int
+ xfs_filemap_fault(
+ 	struct vm_area_struct	*vma,
+ 	struct vm_fault		*vmf)
+ {
+ 	struct inode		*inode = file_inode(vma->vm_file);
+ 	int			ret;
+ 
+ 	trace_xfs_filemap_fault(XFS_I(inode));
+ 
+ 	/* DAX can shortcut the normal fault path on write faults! */
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && IS_DAX(inode))
+ 		return xfs_filemap_page_mkwrite(vma, vmf);
+ 
+ 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	if (IS_DAX(inode)) {
+ 		/*
+ 		 * we do not want to trigger unwritten extent conversion on read
+ 		 * faults - that is unnecessary overhead and would also require
+ 		 * changes to xfs_get_blocks_direct() to map unwritten extent
+ 		 * ioend for conversion on read-only mappings.
+ 		 */
+ 		ret = __dax_fault(vma, vmf, xfs_get_blocks_dax_fault, NULL);
+ 	} else
+ 		ret = filemap_fault(vma, vmf);
+ 	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 
+ 	return ret;
+ }
+ 
+ STATIC int
+ xfs_filemap_pmd_fault(
+ 	struct vm_area_struct	*vma,
+ 	unsigned long		addr,
+ 	pmd_t			*pmd,
+ 	unsigned int		flags)
+ {
+ 	struct inode		*inode = file_inode(vma->vm_file);
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	int			ret;
+ 
+ 	if (!IS_DAX(inode))
+ 		return VM_FAULT_FALLBACK;
+ 
+ 	trace_xfs_filemap_pmd_fault(ip);
+ 
+ 	sb_start_pagefault(inode->i_sb);
+ 	file_update_time(vma->vm_file);
+ 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	ret = __dax_pmd_fault(vma, addr, pmd, flags, xfs_get_blocks_dax_fault,
+ 				    xfs_end_io_dax_write);
+ 	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	sb_end_pagefault(inode->i_sb);
+ 
+ 	return ret;
+ }
+ 
+ static const struct vm_operations_struct xfs_file_vm_ops = {
+ 	.fault		= xfs_filemap_fault,
+ 	.pmd_fault	= xfs_filemap_pmd_fault,
+ 	.map_pages	= filemap_map_pages,
+ 	.page_mkwrite	= xfs_filemap_page_mkwrite,
+ };
+ 
+ STATIC int
+ xfs_file_mmap(
+ 	struct file	*filp,
+ 	struct vm_area_struct *vma)
+ {
+ 	file_accessed(filp);
+ 	vma->vm_ops = &xfs_file_vm_ops;
+ 	if (IS_DAX(file_inode(filp)))
+ 		vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
+ 	return 0;
++>>>>>>> 3e12dbbdbd88 (xfs: fix inode size update overflow in xfs_map_direct())
  }
  
  const struct file_operations xfs_file_operations = {
* Unmerged path fs/xfs/xfs_aops.c
* Unmerged path fs/xfs/xfs_aops.h
* Unmerged path fs/xfs/xfs_file.c
