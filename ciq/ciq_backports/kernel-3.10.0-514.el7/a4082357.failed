xfs: rework buffer dispose list tracking

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit a408235726aa82c0358c9ec68124b6f4bc0a79df
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a4082357.failed

In converting the buffer lru lists to use the generic code, the locking
for marking the buffers as on the dispose list was lost.  This results in
confusion in LRU buffer tracking and acocunting, resulting in reference
counts being mucked up and filesystem beig unmountable.

To fix this, introduce an internal buffer spinlock to protect the state
field that holds the dispose list information.  Because there is now
locking needed around xfs_buf_lru_add/del, and they are used in exactly
one place each two lines apart, get rid of the wrappers and code the logic
directly in place.

Further, the LRU emptying code used on unmount is less than optimal.
Convert it to use a dispose list as per a normal shrinker walk, and repeat
the walk that fills the dispose list until the LRU is empty.  Thi avoids
needing to drop and regain the LRU lock for every item being freed, and
allows the same logic as the shrinker isolate call to be used.  Simpler,
easier to understand.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Signed-off-by: Glauber Costa <glommer@openvz.org>
	Cc: "Theodore Ts'o" <tytso@mit.edu>
	Cc: Adrian Hunter <adrian.hunter@intel.com>
	Cc: Al Viro <viro@zeniv.linux.org.uk>
	Cc: Artem Bityutskiy <artem.bityutskiy@linux.intel.com>
	Cc: Arve Hjønnevåg <arve@android.com>
	Cc: Carlos Maiolino <cmaiolino@redhat.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Chuck Lever <chuck.lever@oracle.com>
	Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Gleb Natapov <gleb@redhat.com>
	Cc: Greg Thelen <gthelen@google.com>
	Cc: J. Bruce Fields <bfields@redhat.com>
	Cc: Jan Kara <jack@suse.cz>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: John Stultz <john.stultz@linaro.org>
	Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
	Cc: Kent Overstreet <koverstreet@google.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Marcelo Tosatti <mtosatti@redhat.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Steven Whitehouse <swhiteho@redhat.com>
	Cc: Thomas Hellstrom <thellstrom@vmware.com>
	Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
(cherry picked from commit a408235726aa82c0358c9ec68124b6f4bc0a79df)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_buf.c
#	fs/xfs/xfs_buf.h
diff --cc fs/xfs/xfs_buf.c
index c8156d3de9e4,d46f6a3dc1de..000000000000
--- a/fs/xfs/xfs_buf.c
+++ b/fs/xfs/xfs_buf.c
@@@ -80,54 -81,6 +80,57 @@@ xfs_buf_vmap_len
  }
  
  /*
++<<<<<<< HEAD
 + * xfs_buf_lru_add - add a buffer to the LRU.
 + *
 + * The LRU takes a new reference to the buffer so that it will only be freed
 + * once the shrinker takes the buffer off the LRU.
 + */
 +STATIC void
 +xfs_buf_lru_add(
 +	struct xfs_buf	*bp)
 +{
 +	struct xfs_buftarg *btp = bp->b_target;
 +
 +	spin_lock(&btp->bt_lru_lock);
 +	if (list_empty(&bp->b_lru)) {
 +		atomic_inc(&bp->b_hold);
 +		list_add_tail(&bp->b_lru, &btp->bt_lru);
 +		btp->bt_lru_nr++;
 +		bp->b_lru_flags &= ~_XBF_LRU_DISPOSE;
 +	}
 +	spin_unlock(&btp->bt_lru_lock);
 +}
 +
 +/*
 + * xfs_buf_lru_del - remove a buffer from the LRU
 + *
 + * The unlocked check is safe here because it only occurs when there are not
 + * b_lru_ref counts left on the inode under the pag->pag_buf_lock. it is there
 + * to optimise the shrinker removing the buffer from the LRU and calling
 + * xfs_buf_free(). i.e. it removes an unnecessary round trip on the
 + * bt_lru_lock.
 + */
 +STATIC void
 +xfs_buf_lru_del(
 +	struct xfs_buf	*bp)
 +{
 +	struct xfs_buftarg *btp = bp->b_target;
 +
 +	if (list_empty(&bp->b_lru))
 +		return;
 +
 +	spin_lock(&btp->bt_lru_lock);
 +	if (!list_empty(&bp->b_lru)) {
 +		list_del_init(&bp->b_lru);
 +		btp->bt_lru_nr--;
 +	}
 +	spin_unlock(&btp->bt_lru_lock);
 +}
 +
 +/*
++=======
++>>>>>>> a408235726aa (xfs: rework buffer dispose list tracking)
   * When we mark a buffer stale, we remove the buffer from the LRU and clear the
   * b_lru_ref count so that the buffer is freed immediately when the buffer
   * reference count falls to zero. If the buffer is already on the LRU, we need
@@@ -150,20 -103,14 +153,29 @@@ xfs_buf_stale
  	 */
  	bp->b_flags &= ~_XBF_DELWRI_Q;
  
++<<<<<<< HEAD
 +	atomic_set(&(bp)->b_lru_ref, 0);
 +	if (!list_empty(&bp->b_lru)) {
 +		struct xfs_buftarg *btp = bp->b_target;
++=======
+ 	spin_lock(&bp->b_lock);
+ 	atomic_set(&bp->b_lru_ref, 0);
+ 	if (!(bp->b_state & XFS_BSTATE_DISPOSE) &&
+ 	    (list_lru_del(&bp->b_target->bt_lru, &bp->b_lru)))
+ 		atomic_dec(&bp->b_hold);
++>>>>>>> a408235726aa (xfs: rework buffer dispose list tracking)
  
 +		spin_lock(&btp->bt_lru_lock);
 +		if (!list_empty(&bp->b_lru) &&
 +		    !(bp->b_lru_flags & _XBF_LRU_DISPOSE)) {
 +			list_del_init(&bp->b_lru);
 +			btp->bt_lru_nr--;
 +			atomic_dec(&bp->b_hold);
 +		}
 +		spin_unlock(&btp->bt_lru_lock);
 +	}
  	ASSERT(atomic_read(&bp->b_hold) >= 1);
+ 	spin_unlock(&bp->b_lock);
  }
  
  static int
@@@ -1518,43 -1470,91 +1551,124 @@@ xfs_buf_iomove
   * returned. These buffers will have an elevated hold count, so wait on those
   * while freeing all the buffers only held by the LRU.
   */
 -static enum lru_status
 -xfs_buftarg_wait_rele(
 -	struct list_head	*item,
 -	spinlock_t		*lru_lock,
 -	void			*arg)
 -
 +void
 +xfs_wait_buftarg(
 +	struct xfs_buftarg	*btp)
  {
++<<<<<<< HEAD
 +	struct xfs_buf		*bp;
 +
 +restart:
 +	spin_lock(&btp->bt_lru_lock);
 +	while (!list_empty(&btp->bt_lru)) {
 +		bp = list_first_entry(&btp->bt_lru, struct xfs_buf, b_lru);
 +		if (atomic_read(&bp->b_hold) > 1) {
 +			trace_xfs_buf_wait_buftarg(bp, _RET_IP_);
 +			list_move_tail(&bp->b_lru, &btp->bt_lru);
 +			spin_unlock(&btp->bt_lru_lock);
 +			delay(100);
 +			goto restart;
 +		}
 +		/*
 +		 * clear the LRU reference count so the buffer doesn't get
 +		 * ignored in xfs_buf_rele().
 +		 */
 +		atomic_set(&bp->b_lru_ref, 0);
 +		spin_unlock(&btp->bt_lru_lock);
 +		if (bp->b_flags & XBF_WRITE_FAIL) {
 +			xfs_alert(btp->bt_mount,
 +"Corruption Alert: Buffer at block 0x%llx had permanent write failures!\n"
 +"Please run xfs_repair to determine the extent of the problem.",
 +				(long long)bp->b_bn);
 +		}
 +		xfs_buf_rele(bp);
 +		spin_lock(&btp->bt_lru_lock);
 +	}
 +	spin_unlock(&btp->bt_lru_lock);
 +}
 +
 +int
 +xfs_buftarg_shrink(
++=======
+ 	struct xfs_buf		*bp = container_of(item, struct xfs_buf, b_lru);
+ 	struct list_head	*dispose = arg;
+ 
+ 	if (atomic_read(&bp->b_hold) > 1) {
+ 		/* need to wait, so skip it this pass */
+ 		trace_xfs_buf_wait_buftarg(bp, _RET_IP_);
+ 		return LRU_SKIP;
+ 	}
+ 	if (!spin_trylock(&bp->b_lock))
+ 		return LRU_SKIP;
+ 
+ 	/*
+ 	 * clear the LRU reference count so the buffer doesn't get
+ 	 * ignored in xfs_buf_rele().
+ 	 */
+ 	atomic_set(&bp->b_lru_ref, 0);
+ 	bp->b_state |= XFS_BSTATE_DISPOSE;
+ 	list_move(item, dispose);
+ 	spin_unlock(&bp->b_lock);
+ 	return LRU_REMOVED;
+ }
+ 
+ void
+ xfs_wait_buftarg(
+ 	struct xfs_buftarg	*btp)
+ {
+ 	LIST_HEAD(dispose);
+ 	int loop = 0;
+ 
+ 	/* loop until there is nothing left on the lru list. */
+ 	while (list_lru_count(&btp->bt_lru)) {
+ 		list_lru_walk(&btp->bt_lru, xfs_buftarg_wait_rele,
+ 			      &dispose, LONG_MAX);
+ 
+ 		while (!list_empty(&dispose)) {
+ 			struct xfs_buf *bp;
+ 			bp = list_first_entry(&dispose, struct xfs_buf, b_lru);
+ 			list_del_init(&bp->b_lru);
+ 			xfs_buf_rele(bp);
+ 		}
+ 		if (loop++ != 0)
+ 			delay(100);
+ 	}
+ }
+ 
+ static enum lru_status
+ xfs_buftarg_isolate(
+ 	struct list_head	*item,
+ 	spinlock_t		*lru_lock,
+ 	void			*arg)
+ {
+ 	struct xfs_buf		*bp = container_of(item, struct xfs_buf, b_lru);
+ 	struct list_head	*dispose = arg;
+ 
+ 	/*
+ 	 * we are inverting the lru lock/bp->b_lock here, so use a trylock.
+ 	 * If we fail to get the lock, just skip it.
+ 	 */
+ 	if (!spin_trylock(&bp->b_lock))
+ 		return LRU_SKIP;
+ 	/*
+ 	 * Decrement the b_lru_ref count unless the value is already
+ 	 * zero. If the value is already zero, we need to reclaim the
+ 	 * buffer, otherwise it gets another trip through the LRU.
+ 	 */
+ 	if (!atomic_add_unless(&bp->b_lru_ref, -1, 0)) {
+ 		spin_unlock(&bp->b_lock);
+ 		return LRU_ROTATE;
+ 	}
+ 
+ 	bp->b_state |= XFS_BSTATE_DISPOSE;
+ 	list_move(item, dispose);
+ 	spin_unlock(&bp->b_lock);
+ 	return LRU_REMOVED;
+ }
+ 
+ static unsigned long
+ xfs_buftarg_shrink_scan(
++>>>>>>> a408235726aa (xfs: rework buffer dispose list tracking)
  	struct shrinker		*shrink,
  	struct shrink_control	*sc)
  {
diff --cc fs/xfs/xfs_buf.h
index 5e9d6b7627e0,e65683361017..000000000000
--- a/fs/xfs/xfs_buf.h
+++ b/fs/xfs/xfs_buf.h
@@@ -81,23 -78,13 +80,27 @@@ typedef unsigned int xfs_buf_flags_t
  	{ _XBF_PAGES,		"PAGES" }, \
  	{ _XBF_KMEM,		"KMEM" }, \
  	{ _XBF_DELWRI_Q,	"DELWRI_Q" }, \
- 	{ _XBF_COMPOUND,	"COMPOUND" }, \
- 	{ _XBF_LRU_DISPOSE,	"LRU_DISPOSE" }
+ 	{ _XBF_COMPOUND,	"COMPOUND" }
+ 
+ /*
+  * Internal state flags.
+  */
+ #define XFS_BSTATE_DISPOSE	 (1 << 0)	/* buffer being discarded */
  
 +
 +/*
 + * The xfs_buftarg contains 2 notions of "sector size" -
 + *
 + * 1) The metadata sector size, which is the minimum unit and
 + *    alignment of IO which will be performed by metadata operations.
 + * 2) The device logical sector size
 + *
 + * The first is specified at mkfs time, and is stored on-disk in the
 + * superblock's sb_sectsize.
 + *
 + * The latter is derived from the underlying device, and controls direct IO
 + * alignment constraints.
 + */
  typedef struct xfs_buftarg {
  	dev_t			bt_dev;
  	struct block_device	*bt_bdev;
@@@ -156,9 -139,8 +159,14 @@@ typedef struct xfs_buf 
  	 * bt_lru_lock and not by b_sema
  	 */
  	struct list_head	b_lru;		/* lru list */
++<<<<<<< HEAD
 +	xfs_buf_flags_t		b_lru_flags;	/* internal lru status flags */
 +	spinlock_t		b_lock;		/* internal state lock */
 +	int			b_io_error;	/* internal IO error state */
++=======
+ 	spinlock_t		b_lock;		/* internal state lock */
+ 	unsigned int		b_state;	/* internal state flags */
++>>>>>>> a408235726aa (xfs: rework buffer dispose list tracking)
  	wait_queue_head_t	b_waiters;	/* unpin waiters */
  	struct list_head	b_list;
  	struct xfs_perag	*b_pag;		/* contains rbtree root */
* Unmerged path fs/xfs/xfs_buf.c
* Unmerged path fs/xfs/xfs_buf.h
