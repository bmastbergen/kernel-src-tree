slub: support for bulk free with SLUB freelists

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jesper Dangaard Brouer <brouer@redhat.com>
commit 81084651d73737988355f167065fab8a73574db1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/81084651.failed

Make it possible to free a freelist with several objects by adjusting API
of slab_free() and __slab_free() to have head, tail and an objects counter
(cnt).

Tail being NULL indicate single object free of head object.  This allow
compiler inline constant propagation in slab_free() and
slab_free_freelist_hook() to avoid adding any overhead in case of single
object free.

This allows a freelist with several objects (all within the same
slab-page) to be free'ed using a single locked cmpxchg_double in
__slab_free() and with an unlocked cmpxchg_double in slab_free().

Object debugging on the free path is also extended to handle these
freelists.  When CONFIG_SLUB_DEBUG is enabled it will also detect if
objects don't belong to the same slab-page.

These changes are needed for the next patch to bulk free the detached
freelists it introduces and constructs.

Micro benchmarking showed no performance reduction due to this change,
when debugging is turned off (compiled with CONFIG_SLUB_DEBUG).

	Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Signed-off-by: Alexander Duyck <alexander.h.duyck@redhat.com>
	Acked-by: Christoph Lameter <cl@linux.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 81084651d73737988355f167065fab8a73574db1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 9230da41a30e,d52f0d0ab712..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -1270,11 -1264,110 +1289,112 @@@ static inline void slab_free_hook(struc
  #endif /* CONFIG_SLUB_DEBUG */
  
  /*
++<<<<<<< HEAD
++=======
+  * Hooks for other subsystems that check memory allocations. In a typical
+  * production configuration these hooks all should produce no code at all.
+  */
+ static inline void kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)
+ {
+ 	kmemleak_alloc(ptr, size, 1, flags);
+ 	kasan_kmalloc_large(ptr, size);
+ }
+ 
+ static inline void kfree_hook(const void *x)
+ {
+ 	kmemleak_free(x);
+ 	kasan_kfree_large(x);
+ }
+ 
+ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
+ 						     gfp_t flags)
+ {
+ 	flags &= gfp_allowed_mask;
+ 	lockdep_trace_alloc(flags);
+ 	might_sleep_if(gfpflags_allow_blocking(flags));
+ 
+ 	if (should_failslab(s->object_size, flags, s->flags))
+ 		return NULL;
+ 
+ 	return memcg_kmem_get_cache(s, flags);
+ }
+ 
+ static inline void slab_post_alloc_hook(struct kmem_cache *s,
+ 					gfp_t flags, void *object)
+ {
+ 	flags &= gfp_allowed_mask;
+ 	kmemcheck_slab_alloc(s, flags, object, slab_ksize(s));
+ 	kmemleak_alloc_recursive(object, s->object_size, 1, s->flags, flags);
+ 	memcg_kmem_put_cache(s);
+ 	kasan_slab_alloc(s, object);
+ }
+ 
+ static inline void slab_free_hook(struct kmem_cache *s, void *x)
+ {
+ 	kmemleak_free_recursive(x, s->flags);
+ 
+ 	/*
+ 	 * Trouble is that we may no longer disable interrupts in the fast path
+ 	 * So in order to make the debug calls that expect irqs to be
+ 	 * disabled we need to disable interrupts temporarily.
+ 	 */
+ #if defined(CONFIG_KMEMCHECK) || defined(CONFIG_LOCKDEP)
+ 	{
+ 		unsigned long flags;
+ 
+ 		local_irq_save(flags);
+ 		kmemcheck_slab_free(s, x, s->object_size);
+ 		debug_check_no_locks_freed(x, s->object_size);
+ 		local_irq_restore(flags);
+ 	}
+ #endif
+ 	if (!(s->flags & SLAB_DEBUG_OBJECTS))
+ 		debug_check_no_obj_freed(x, s->object_size);
+ 
+ 	kasan_slab_free(s, x);
+ }
+ 
+ static inline void slab_free_freelist_hook(struct kmem_cache *s,
+ 					   void *head, void *tail)
+ {
+ /*
+  * Compiler cannot detect this function can be removed if slab_free_hook()
+  * evaluates to nothing.  Thus, catch all relevant config debug options here.
+  */
+ #if defined(CONFIG_KMEMCHECK) ||		\
+ 	defined(CONFIG_LOCKDEP)	||		\
+ 	defined(CONFIG_DEBUG_KMEMLEAK) ||	\
+ 	defined(CONFIG_DEBUG_OBJECTS_FREE) ||	\
+ 	defined(CONFIG_KASAN)
+ 
+ 	void *object = head;
+ 	void *tail_obj = tail ? : head;
+ 
+ 	do {
+ 		slab_free_hook(s, object);
+ 	} while ((object != tail_obj) &&
+ 		 (object = get_freepointer(s, object)));
+ #endif
+ }
+ 
+ static void setup_object(struct kmem_cache *s, struct page *page,
+ 				void *object)
+ {
+ 	setup_object_debug(s, page, object);
+ 	if (unlikely(s->ctor)) {
+ 		kasan_unpoison_object_data(s, object);
+ 		s->ctor(object);
+ 		kasan_poison_object_data(s, object);
+ 	}
+ }
+ 
+ /*
++>>>>>>> 81084651d737 (slub: support for bulk free with SLUB freelists)
   * Slab allocation and freeing
   */
 -static inline struct page *alloc_slab_page(struct kmem_cache *s,
 -		gfp_t flags, int node, struct kmem_cache_order_objects oo)
 +static inline struct page *alloc_slab_page(gfp_t flags, int node,
 +					struct kmem_cache_order_objects oo)
  {
 -	struct page *page;
  	int order = oo_order(oo);
  
  	flags |= __GFP_NOTRACK;
@@@ -2514,17 -2650,19 +2636,17 @@@ static void __slab_free(struct kmem_cac
  		}
  		prior = page->freelist;
  		counters = page->counters;
- 		set_freepointer(s, object, prior);
+ 		set_freepointer(s, tail, prior);
  		new.counters = counters;
  		was_frozen = new.frozen;
- 		new.inuse--;
+ 		new.inuse -= cnt;
  		if ((!new.inuse || !prior) && !was_frozen) {
  
 -			if (kmem_cache_has_cpu_partial(s) && !prior) {
 +			if (!kmem_cache_debug(s) && !prior)
  
  				/*
 -				 * Slab was on no list before and will be
 -				 * partially empty
 -				 * We can defer the list move and instead
 -				 * freeze it.
 +				 * Slab was on no list before and will be partially empty
 +				 * We can defer the list move and instead freeze it.
  				 */
  				new.frozen = 1;
  
@@@ -2624,16 -2769,19 +2751,16 @@@ redo
  	 * Determine the currently cpus per cpu slab.
  	 * The cpu may change afterward. However that does not matter since
  	 * data is retrieved via this pointer. If we are on the same cpu
 -	 * during the cmpxchg then the free will succeed.
 +	 * during the cmpxchg then the free will succedd.
  	 */
 -	do {
 -		tid = this_cpu_read(s->cpu_slab->tid);
 -		c = raw_cpu_ptr(s->cpu_slab);
 -	} while (IS_ENABLED(CONFIG_PREEMPT) &&
 -		 unlikely(tid != READ_ONCE(c->tid)));
 +	preempt_disable();
 +	c = __this_cpu_ptr(s->cpu_slab);
  
 -	/* Same with comment on barrier() in slab_alloc_node() */
 -	barrier();
 +	tid = c->tid;
 +	preempt_enable();
  
  	if (likely(page == c->page)) {
- 		set_freepointer(s, object, c->freelist);
+ 		set_freepointer(s, tail_obj, c->freelist);
  
  		if (unlikely(!this_cpu_cmpxchg_double(
  				s->cpu_slab->freelist, s->cpu_slab->tid,
@@@ -2659,6 -2807,111 +2786,114 @@@ void kmem_cache_free(struct kmem_cache 
  }
  EXPORT_SYMBOL(kmem_cache_free);
  
++<<<<<<< HEAD
++=======
+ /* Note that interrupts must be enabled when calling this function. */
+ void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)
+ {
+ 	struct kmem_cache_cpu *c;
+ 	struct page *page;
+ 	int i;
+ 
+ 	local_irq_disable();
+ 	c = this_cpu_ptr(s->cpu_slab);
+ 
+ 	for (i = 0; i < size; i++) {
+ 		void *object = p[i];
+ 
+ 		BUG_ON(!object);
+ 		/* kmem cache debug support */
+ 		s = cache_from_obj(s, object);
+ 		if (unlikely(!s))
+ 			goto exit;
+ 		slab_free_hook(s, object);
+ 
+ 		page = virt_to_head_page(object);
+ 
+ 		if (c->page == page) {
+ 			/* Fastpath: local CPU free */
+ 			set_freepointer(s, object, c->freelist);
+ 			c->freelist = object;
+ 		} else {
+ 			c->tid = next_tid(c->tid);
+ 			local_irq_enable();
+ 			/* Slowpath: overhead locked cmpxchg_double_slab */
+ 			__slab_free(s, page, object, object, 1, _RET_IP_);
+ 			local_irq_disable();
+ 			c = this_cpu_ptr(s->cpu_slab);
+ 		}
+ 	}
+ exit:
+ 	c->tid = next_tid(c->tid);
+ 	local_irq_enable();
+ }
+ EXPORT_SYMBOL(kmem_cache_free_bulk);
+ 
+ /* Note that interrupts must be enabled when calling this function. */
+ bool kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
+ 			   void **p)
+ {
+ 	struct kmem_cache_cpu *c;
+ 	int i;
+ 
+ 	/*
+ 	 * Drain objects in the per cpu slab, while disabling local
+ 	 * IRQs, which protects against PREEMPT and interrupts
+ 	 * handlers invoking normal fastpath.
+ 	 */
+ 	local_irq_disable();
+ 	c = this_cpu_ptr(s->cpu_slab);
+ 
+ 	for (i = 0; i < size; i++) {
+ 		void *object = c->freelist;
+ 
+ 		if (unlikely(!object)) {
+ 			/*
+ 			 * Invoking slow path likely have side-effect
+ 			 * of re-populating per CPU c->freelist
+ 			 */
+ 			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
+ 					    _RET_IP_, c);
+ 			if (unlikely(!p[i]))
+ 				goto error;
+ 
+ 			c = this_cpu_ptr(s->cpu_slab);
+ 			continue; /* goto for-loop */
+ 		}
+ 
+ 		/* kmem_cache debug support */
+ 		s = slab_pre_alloc_hook(s, flags);
+ 		if (unlikely(!s))
+ 			goto error;
+ 
+ 		c->freelist = get_freepointer(s, object);
+ 		p[i] = object;
+ 
+ 		/* kmem_cache debug support */
+ 		slab_post_alloc_hook(s, flags, object);
+ 	}
+ 	c->tid = next_tid(c->tid);
+ 	local_irq_enable();
+ 
+ 	/* Clear memory outside IRQ disabled fastpath loop */
+ 	if (unlikely(flags & __GFP_ZERO)) {
+ 		int j;
+ 
+ 		for (j = 0; j < i; j++)
+ 			memset(p[j], 0, s->object_size);
+ 	}
+ 
+ 	return true;
+ 
+ error:
+ 	__kmem_cache_free_bulk(s, i, p);
+ 	local_irq_enable();
+ 	return false;
+ }
+ EXPORT_SYMBOL(kmem_cache_alloc_bulk);
+ 
+ 
++>>>>>>> 81084651d737 (slub: support for bulk free with SLUB freelists)
  /*
   * Object placement in a slab is made very easy because we always start at
   * offset 0. If we tune the size of the object to the alignment then we can
@@@ -3375,11 -3568,11 +3610,11 @@@ void kfree(const void *x
  	page = virt_to_head_page(x);
  	if (unlikely(!PageSlab(page))) {
  		BUG_ON(!PageCompound(page));
 -		kfree_hook(x);
 -		__free_kmem_pages(page, compound_order(page));
 +		kmemleak_free(x);
 +		__free_memcg_kmem_pages(page, compound_order(page));
  		return;
  	}
- 	slab_free(page->slab_cache, page, object, _RET_IP_);
+ 	slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
  }
  EXPORT_SYMBOL(kfree);
  
* Unmerged path mm/slub.c
