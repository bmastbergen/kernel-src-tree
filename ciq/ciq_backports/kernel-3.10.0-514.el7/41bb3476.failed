mm: save soft-dirty bits on file pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] save soft-dirty bits on file pages (Oleg Nesterov) [1269561]
Rebuild_FUZZ: 94.44%
commit-author Cyrill Gorcunov <gorcunov@gmail.com>
commit 41bb3476b361ef38576cf9d539b19bae2ac93167
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/41bb3476.failed

Andy reported that if file page get reclaimed we lose the soft-dirty bit
if it was there, so save _PAGE_BIT_SOFT_DIRTY bit when page address get
encoded into pte entry.  Thus when #pf happens on such non-present pte
we can restore it back.

	Reported-by: Andy Lutomirski <luto@amacapital.net>
	Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
	Acked-by: Pavel Emelyanov <xemul@parallels.com>
	Cc: Matt Mackall <mpm@selenic.com>
	Cc: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
	Cc: Marcelo Tosatti <mtosatti@redhat.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@gmail.com>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 41bb3476b361ef38576cf9d539b19bae2ac93167)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/pgtable.h
#	arch/x86/include/asm/pgtable_types.h
#	fs/proc/task_mmu.c
#	include/asm-generic/pgtable.h
diff --cc arch/x86/include/asm/pgtable.h
index baa6c1dd0ecb,1c00631164c2..000000000000
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@@ -295,6 -294,56 +295,59 @@@ static inline pmd_t pmd_mknotpresent(pm
  	return pmd_clear_flags(pmd, _PAGE_PRESENT);
  }
  
++<<<<<<< HEAD
++=======
+ static inline int pte_soft_dirty(pte_t pte)
+ {
+ 	return pte_flags(pte) & _PAGE_SOFT_DIRTY;
+ }
+ 
+ static inline int pmd_soft_dirty(pmd_t pmd)
+ {
+ 	return pmd_flags(pmd) & _PAGE_SOFT_DIRTY;
+ }
+ 
+ static inline pte_t pte_mksoft_dirty(pte_t pte)
+ {
+ 	return pte_set_flags(pte, _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pmd_t pmd_mksoft_dirty(pmd_t pmd)
+ {
+ 	return pmd_set_flags(pmd, _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
+ {
+ 	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+ }
+ 
+ static inline int pte_swp_soft_dirty(pte_t pte)
+ {
+ 	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
+ }
+ 
+ static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
+ {
+ 	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+ }
+ 
+ static inline pte_t pte_file_clear_soft_dirty(pte_t pte)
+ {
+ 	return pte_clear_flags(pte, _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pte_t pte_file_mksoft_dirty(pte_t pte)
+ {
+ 	return pte_set_flags(pte, _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline int pte_file_soft_dirty(pte_t pte)
+ {
+ 	return pte_flags(pte) & _PAGE_SOFT_DIRTY;
+ }
+ 
++>>>>>>> 41bb3476b361 (mm: save soft-dirty bits on file pages)
  /*
   * Mask out unsupported bits in a present pgprot.  Non-present pgprots
   * can use those bits for other purposes, so leave them be.
diff --cc arch/x86/include/asm/pgtable_types.h
index 35216aeb100f,f4843e031131..000000000000
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@@ -57,6 -55,33 +57,36 @@@
  #define _PAGE_HIDDEN	(_AT(pteval_t, 0))
  #endif
  
++<<<<<<< HEAD
++=======
+ /*
+  * The same hidden bit is used by kmemcheck, but since kmemcheck
+  * works on kernel pages while soft-dirty engine on user space,
+  * they do not conflict with each other.
+  */
+ 
+ #define _PAGE_BIT_SOFT_DIRTY	_PAGE_BIT_HIDDEN
+ 
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ #define _PAGE_SOFT_DIRTY	(_AT(pteval_t, 1) << _PAGE_BIT_SOFT_DIRTY)
+ #else
+ #define _PAGE_SOFT_DIRTY	(_AT(pteval_t, 0))
+ #endif
+ 
+ /*
+  * Tracking soft dirty bit when a page goes to a swap is tricky.
+  * We need a bit which can be stored in pte _and_ not conflict
+  * with swap entry format. On x86 bits 6 and 7 are *not* involved
+  * into swap entry computation, but bit 6 is used for nonlinear
+  * file mapping, so we borrow bit 7 for soft dirty tracking.
+  */
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ #define _PAGE_SWP_SOFT_DIRTY	_PAGE_PSE
+ #else
+ #define _PAGE_SWP_SOFT_DIRTY	(_AT(pteval_t, 0))
+ #endif
+ 
++>>>>>>> 41bb3476b361 (mm: save soft-dirty bits on file pages)
  #if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)
  #define _PAGE_NX	(_AT(pteval_t, 1) << _PAGE_BIT_NX)
  #else
diff --cc fs/proc/task_mmu.c
index 452c618feb42,a11720767abc..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -703,8 -716,34 +703,36 @@@ enum clear_refs_types 
  
  struct clear_refs_private {
  	struct vm_area_struct *vma;
 -	enum clear_refs_types type;
  };
  
++<<<<<<< HEAD
++=======
+ static inline void clear_soft_dirty(struct vm_area_struct *vma,
+ 		unsigned long addr, pte_t *pte)
+ {
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ 	/*
+ 	 * The soft-dirty tracker uses #PF-s to catch writes
+ 	 * to pages, so write-protect the pte as well. See the
+ 	 * Documentation/vm/soft-dirty.txt for full description
+ 	 * of how soft-dirty works.
+ 	 */
+ 	pte_t ptent = *pte;
+ 
+ 	if (pte_present(ptent)) {
+ 		ptent = pte_wrprotect(ptent);
+ 		ptent = pte_clear_flags(ptent, _PAGE_SOFT_DIRTY);
+ 	} else if (is_swap_pte(ptent)) {
+ 		ptent = pte_swp_clear_soft_dirty(ptent);
+ 	} else if (pte_file(ptent)) {
+ 		ptent = pte_file_clear_soft_dirty(ptent);
+ 	}
+ 
+ 	set_pte_at(vma->vm_mm, addr, pte, ptent);
+ #endif
+ }
+ 
++>>>>>>> 41bb3476b361 (mm: save soft-dirty bits on file pages)
  static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,
  				unsigned long end, struct mm_walk *walk)
  {
diff --cc include/asm-generic/pgtable.h
index 34b45521969b,0807ddf97b05..000000000000
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@@ -393,6 -397,58 +393,61 @@@ static inline void ptep_modify_prot_com
  #define arch_start_context_switch(prev)	do {} while (0)
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifndef CONFIG_HAVE_ARCH_SOFT_DIRTY
+ static inline int pte_soft_dirty(pte_t pte)
+ {
+ 	return 0;
+ }
+ 
+ static inline int pmd_soft_dirty(pmd_t pmd)
+ {
+ 	return 0;
+ }
+ 
+ static inline pte_t pte_mksoft_dirty(pte_t pte)
+ {
+ 	return pte;
+ }
+ 
+ static inline pmd_t pmd_mksoft_dirty(pmd_t pmd)
+ {
+ 	return pmd;
+ }
+ 
+ static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
+ {
+ 	return pte;
+ }
+ 
+ static inline int pte_swp_soft_dirty(pte_t pte)
+ {
+ 	return 0;
+ }
+ 
+ static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
+ {
+ 	return pte;
+ }
+ 
+ static inline pte_t pte_file_clear_soft_dirty(pte_t pte)
+ {
+        return pte;
+ }
+ 
+ static inline pte_t pte_file_mksoft_dirty(pte_t pte)
+ {
+        return pte;
+ }
+ 
+ static inline int pte_file_soft_dirty(pte_t pte)
+ {
+        return 0;
+ }
+ #endif
+ 
++>>>>>>> 41bb3476b361 (mm: save soft-dirty bits on file pages)
  #ifndef __HAVE_PFNMAP_TRACKING
  /*
   * Interfaces that can be used by architecture code to keep track of
diff --git a/arch/x86/include/asm/pgtable-2level.h b/arch/x86/include/asm/pgtable-2level.h
index 7060c7780e06..5316b4e531c5 100644
--- a/arch/x86/include/asm/pgtable-2level.h
+++ b/arch/x86/include/asm/pgtable-2level.h
@@ -60,9 +60,53 @@ static inline pmd_t native_pmdp_get_and_clear(pmd_t *xp)
 #define native_pmdp_get_and_clear(xp) native_local_pmdp_get_and_clear(xp)
 #endif
 
+#ifdef CONFIG_MEM_SOFT_DIRTY
+
+/*
+ * Bits _PAGE_BIT_PRESENT, _PAGE_BIT_FILE, _PAGE_BIT_SOFT_DIRTY and
+ * _PAGE_BIT_PROTNONE are taken, split up the 28 bits of offset
+ * into this range.
+ */
+#define PTE_FILE_MAX_BITS	28
+#define PTE_FILE_SHIFT1		(_PAGE_BIT_PRESENT + 1)
+#define PTE_FILE_SHIFT2		(_PAGE_BIT_FILE + 1)
+#define PTE_FILE_SHIFT3		(_PAGE_BIT_PROTNONE + 1)
+#define PTE_FILE_SHIFT4		(_PAGE_BIT_SOFT_DIRTY + 1)
+#define PTE_FILE_BITS1		(PTE_FILE_SHIFT2 - PTE_FILE_SHIFT1 - 1)
+#define PTE_FILE_BITS2		(PTE_FILE_SHIFT3 - PTE_FILE_SHIFT2 - 1)
+#define PTE_FILE_BITS3		(PTE_FILE_SHIFT4 - PTE_FILE_SHIFT3 - 1)
+
+#define pte_to_pgoff(pte)						\
+	((((pte).pte_low >> (PTE_FILE_SHIFT1))				\
+	  & ((1U << PTE_FILE_BITS1) - 1)))				\
+	+ ((((pte).pte_low >> (PTE_FILE_SHIFT2))			\
+	    & ((1U << PTE_FILE_BITS2) - 1))				\
+	   << (PTE_FILE_BITS1))						\
+	+ ((((pte).pte_low >> (PTE_FILE_SHIFT3))			\
+	    & ((1U << PTE_FILE_BITS3) - 1))				\
+	   << (PTE_FILE_BITS1 + PTE_FILE_BITS2))			\
+	+ ((((pte).pte_low >> (PTE_FILE_SHIFT4)))			\
+	    << (PTE_FILE_BITS1 + PTE_FILE_BITS2 + PTE_FILE_BITS3))
+
+#define pgoff_to_pte(off)						\
+	((pte_t) { .pte_low =						\
+	 ((((off)) & ((1U << PTE_FILE_BITS1) - 1)) << PTE_FILE_SHIFT1)	\
+	 + ((((off) >> PTE_FILE_BITS1)					\
+	     & ((1U << PTE_FILE_BITS2) - 1))				\
+	    << PTE_FILE_SHIFT2)						\
+	 + ((((off) >> (PTE_FILE_BITS1 + PTE_FILE_BITS2))		\
+	     & ((1U << PTE_FILE_BITS3) - 1))				\
+	    << PTE_FILE_SHIFT3)						\
+	 + ((((off) >>							\
+	      (PTE_FILE_BITS1 + PTE_FILE_BITS2 + PTE_FILE_BITS3)))	\
+	    << PTE_FILE_SHIFT4)						\
+	 + _PAGE_FILE })
+
+#else /* CONFIG_MEM_SOFT_DIRTY */
+
 /*
  * Bits _PAGE_BIT_PRESENT, _PAGE_BIT_FILE and _PAGE_BIT_PROTNONE are taken,
- * split up the 29 bits of offset into this range:
+ * split up the 29 bits of offset into this range.
  */
 #define PTE_FILE_MAX_BITS	29
 #define PTE_FILE_SHIFT1		(_PAGE_BIT_PRESENT + 1)
@@ -93,6 +137,8 @@ static inline pmd_t native_pmdp_get_and_clear(pmd_t *xp)
 	    << PTE_FILE_SHIFT3)						\
 	 + _PAGE_FILE })
 
+#endif /* CONFIG_MEM_SOFT_DIRTY */
+
 /* Encode and de-code a swap entry */
 #if _PAGE_BIT_FILE < _PAGE_BIT_PROTNONE
 #define SWP_TYPE_BITS (_PAGE_BIT_FILE - _PAGE_BIT_PRESENT - 1)
diff --git a/arch/x86/include/asm/pgtable-3level.h b/arch/x86/include/asm/pgtable-3level.h
index e91be242ee54..2ead1f1201a8 100644
--- a/arch/x86/include/asm/pgtable-3level.h
+++ b/arch/x86/include/asm/pgtable-3level.h
@@ -192,6 +192,9 @@ static inline pmd_t native_pmdp_get_and_clear(pmd_t *pmdp)
 /*
  * Bits 0, 6 and 7 are taken in the low part of the pte,
  * put the 32 bits of offset into the high part.
+ *
+ * For soft-dirty tracking 11 bit is taken from
+ * the low part of pte as well.
  */
 #define pte_to_pgoff(pte) ((pte).pte_high)
 #define pgoff_to_pte(off)						\
* Unmerged path arch/x86/include/asm/pgtable.h
* Unmerged path arch/x86/include/asm/pgtable_types.h
* Unmerged path fs/proc/task_mmu.c
* Unmerged path include/asm-generic/pgtable.h
diff --git a/mm/fremap.c b/mm/fremap.c
index 8d04ba431b82..0d8f32f425d7 100644
--- a/mm/fremap.c
+++ b/mm/fremap.c
@@ -57,17 +57,22 @@ static int install_file_pte(struct mm_struct *mm, struct vm_area_struct *vma,
 		unsigned long addr, unsigned long pgoff, pgprot_t prot)
 {
 	int err = -ENOMEM;
-	pte_t *pte;
+	pte_t *pte, ptfile;
 	spinlock_t *ptl;
 
 	pte = get_locked_pte(mm, addr, &ptl);
 	if (!pte)
 		goto out;
 
-	if (!pte_none(*pte))
+	ptfile = pgoff_to_pte(pgoff);
+
+	if (!pte_none(*pte)) {
+		if (pte_present(*pte) && pte_soft_dirty(*pte))
+			pte_file_mksoft_dirty(ptfile);
 		zap_pte(mm, vma, addr, pte);
+	}
 
-	set_pte_at(mm, addr, pte, pgoff_to_pte(pgoff));
+	set_pte_at(mm, addr, pte, ptfile);
 	/*
 	 * We don't need to run update_mmu_cache() here because the "file pte"
 	 * being installed by install_file_pte() is not a real pte - it's a
diff --git a/mm/memory.c b/mm/memory.c
index fce51319197b..a16a7df29b69 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1146,9 +1146,12 @@ again:
 				continue;
 			if (unlikely(details) && details->nonlinear_vma
 			    && linear_page_index(details->nonlinear_vma,
-						addr) != page->index)
-				set_pte_at(mm, addr, pte,
-					   pgoff_to_pte(page->index));
+						addr) != page->index) {
+				pte_t ptfile = pgoff_to_pte(page->index);
+				if (pte_soft_dirty(ptent))
+					pte_file_mksoft_dirty(ptfile);
+				set_pte_at(mm, addr, pte, ptfile);
+			}
 			if (PageAnon(page))
 				rss[MM_ANONPAGES]--;
 			else {
@@ -3642,6 +3645,8 @@ static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 		entry = mk_pte(page, vma->vm_page_prot);
 		if (flags & FAULT_FLAG_WRITE)
 			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+		else if (pte_file(orig_pte) && pte_file_soft_dirty(orig_pte))
+			pte_mksoft_dirty(entry);
 		if (anon) {
 			inc_mm_counter_fast(mm, MM_ANONPAGES);
 			page_add_new_anon_rmap(page, vma, address);
diff --git a/mm/rmap.c b/mm/rmap.c
index 8a1f7d7fc267..5d4debafa82f 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -1409,8 +1409,12 @@ static int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,
 		pteval = ptep_clear_flush_notify(vma, address, pte);
 
 		/* If nonlinear, store the file page offset in the pte. */
-		if (page->index != linear_page_index(vma, address))
-			set_pte_at(mm, address, pte, pgoff_to_pte(page->index));
+		if (page->index != linear_page_index(vma, address)) {
+			pte_t ptfile = pgoff_to_pte(page->index);
+			if (pte_soft_dirty(pteval))
+				pte_file_mksoft_dirty(ptfile);
+			set_pte_at(mm, address, pte, ptfile);
+		}
 
 		/* Move the dirty bit to the physical page now the pte is gone. */
 		if (pte_dirty(pteval))
