net/mlx5e: Use napi_alloc_skb for RX SKB allocations

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Use napi_alloc_skb for RX SKB allocations (kamal heib) [1275159 1296272 1296405 1298421 1298422 1298423 1298424 1298425]
Rebuild_FUZZ: 96.00%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit c5adb96f6c4a22aceff2e8220612c5b9239ffeb2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/c5adb96f.failed

Instead of netdev_alloc_skb, we use the napi_alloc_skb function
which is designated to allocate skbuff's for RX in a
channel-specific NAPI instance, and implies the IP packet alignment.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c5adb96f6c4a22aceff2e8220612c5b9239ffeb2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index a33b9e6fa485,303e6cdf9fcd..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -68,6 -93,60 +68,63 @@@
  #define MLX5E_SQ_BF_BUDGET             16
  
  #define MLX5E_NUM_MAIN_GROUPS 9
++<<<<<<< HEAD
++=======
+ 
+ static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
+ {
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return min_t(u16, MLX5E_PARAMS_DEFAULT_MIN_RX_WQES_MPW,
+ 			     wq_size / 2);
+ 	default:
+ 		return min_t(u16, MLX5E_PARAMS_DEFAULT_MIN_RX_WQES,
+ 			     wq_size / 2);
+ 	}
+ }
+ 
+ static inline int mlx5_min_log_rq_size(int wq_type)
+ {
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW;
+ 	default:
+ 		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE;
+ 	}
+ }
+ 
+ static inline int mlx5_max_log_rq_size(int wq_type)
+ {
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW;
+ 	default:
+ 		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE;
+ 	}
+ }
+ 
+ struct mlx5e_tx_wqe {
+ 	struct mlx5_wqe_ctrl_seg ctrl;
+ 	struct mlx5_wqe_eth_seg  eth;
+ };
+ 
+ struct mlx5e_rx_wqe {
+ 	struct mlx5_wqe_srq_next_seg  next;
+ 	struct mlx5_wqe_data_seg      data;
+ };
+ 
+ struct mlx5e_umr_wqe {
+ 	struct mlx5_wqe_ctrl_seg       ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg   uctrl;
+ 	struct mlx5_mkey_seg           mkc;
+ 	struct mlx5_wqe_data_seg       data;
+ };
+ 
+ #ifdef CONFIG_MLX5_CORE_EN_DCB
+ #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
+ #define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
+ #endif
++>>>>>>> c5adb96f6c4a (net/mlx5e: Use napi_alloc_skb for RX SKB allocations)
  
  static const char vport_strings[][ETH_GSTRING_LEN] = {
  	/* vport statistics */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index a8ca30e0bf3d,9b17bc064cc8..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -323,28 -345,49 +323,62 @@@ static int mlx5e_create_rq(struct mlx5e
  	rq->wq.db = &rq->wq.db[MLX5_RCV_DBR];
  
  	wq_sz = mlx5_wq_ll_get_size(&rq->wq);
++<<<<<<< HEAD
 +	rq->skb = kzalloc_node(wq_sz * sizeof(*rq->skb), GFP_KERNEL,
 +			       cpu_to_node(c->cpu));
 +	if (!rq->skb) {
 +		err = -ENOMEM;
 +		goto err_rq_wq_destroy;
++=======
+ 
+ 	switch (priv->params.rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		rq->wqe_info = kzalloc_node(wq_sz * sizeof(*rq->wqe_info),
+ 					    GFP_KERNEL, cpu_to_node(c->cpu));
+ 		if (!rq->wqe_info) {
+ 			err = -ENOMEM;
+ 			goto err_rq_wq_destroy;
+ 		}
+ 		rq->handle_rx_cqe = mlx5e_handle_rx_cqe_mpwrq;
+ 		rq->alloc_wqe = mlx5e_alloc_rx_mpwqe;
+ 
+ 		rq->wqe_sz = MLX5_MPWRQ_NUM_STRIDES * MLX5_MPWRQ_STRIDE_SIZE;
+ 		byte_count = rq->wqe_sz;
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_LINKED_LIST */
+ 		rq->skb = kzalloc_node(wq_sz * sizeof(*rq->skb), GFP_KERNEL,
+ 				       cpu_to_node(c->cpu));
+ 		if (!rq->skb) {
+ 			err = -ENOMEM;
+ 			goto err_rq_wq_destroy;
+ 		}
+ 		rq->handle_rx_cqe = mlx5e_handle_rx_cqe;
+ 		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
+ 
+ 		rq->wqe_sz = (priv->params.lro_en) ?
+ 				priv->params.lro_wqe_sz :
+ 				MLX5E_SW2HW_MTU(priv->netdev->mtu);
+ 		rq->wqe_sz = SKB_DATA_ALIGN(rq->wqe_sz);
+ 		byte_count = rq->wqe_sz;
+ 		byte_count |= MLX5_HW_START_PADDING;
++>>>>>>> c5adb96f6c4a (net/mlx5e: Use napi_alloc_skb for RX SKB allocations)
  	}
  
 +	rq->wqe_sz = (priv->params.lro_en) ? priv->params.lro_wqe_sz :
 +					     MLX5E_SW2HW_MTU(priv->netdev->mtu);
 +	rq->wqe_sz = SKB_DATA_ALIGN(rq->wqe_sz + MLX5E_NET_IP_ALIGN);
 +
  	for (i = 0; i < wq_sz; i++) {
  		struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(&rq->wq, i);
 +		u32 byte_count = rq->wqe_sz - MLX5E_NET_IP_ALIGN;
  
 -		wqe->data.byte_count = cpu_to_be32(byte_count);
 +		wqe->data.lkey       = c->mkey_be;
 +		wqe->data.byte_count =
 +			cpu_to_be32(byte_count | MLX5_HW_START_PADDING);
  	}
  
 -	rq->wq_type = priv->params.rq_wq_type;
  	rq->pdev    = c->pdev;
  	rq->netdev  = c->netdev;
 -	rq->tstamp  = &priv->tstamp;
  	rq->channel = c;
  	rq->ix      = c->ix;
  	rq->priv    = c->priv;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 2dc1f9b26b65,5bdcc0b69f76..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -57,10 -61,9 +57,13 @@@ static inline int mlx5e_alloc_rx_wqe(st
  	if (unlikely(dma_mapping_error(rq->pdev, dma_addr)))
  		goto err_free_skb;
  
- 	skb_reserve(skb, MLX5E_NET_IP_ALIGN);
- 
  	*((dma_addr_t *)skb->cb) = dma_addr;
++<<<<<<< HEAD
 +	wqe->data.addr = cpu_to_be64(dma_addr + MLX5E_NET_IP_ALIGN);
++=======
+ 	wqe->data.addr = cpu_to_be64(dma_addr);
+ 	wqe->data.lkey = rq->mkey_be;
++>>>>>>> c5adb96f6c4a (net/mlx5e: Use napi_alloc_skb for RX SKB allocations)
  
  	rq->skb[ix] = skb;
  
@@@ -215,6 -585,138 +218,141 @@@ static inline void mlx5e_build_rx_skb(s
  	if (cqe_has_vlan(cqe))
  		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
  				       be16_to_cpu(cqe->vlan_info));
++<<<<<<< HEAD
++=======
+ 
+ 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
+ }
+ 
+ static inline void mlx5e_complete_rx_cqe(struct mlx5e_rq *rq,
+ 					 struct mlx5_cqe64 *cqe,
+ 					 u32 cqe_bcnt,
+ 					 struct sk_buff *skb)
+ {
+ 	rq->stats.packets++;
+ 	rq->stats.bytes += cqe_bcnt;
+ 	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ }
+ 
+ void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_rx_wqe *wqe;
+ 	struct sk_buff *skb;
+ 	__be16 wqe_counter_be;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	skb            = rq->skb[wqe_counter];
+ 	prefetch(skb->data);
+ 	rq->skb[wqe_counter] = NULL;
+ 
+ 	dma_unmap_single(rq->pdev,
+ 			 *((dma_addr_t *)skb->cb),
+ 			 rq->wqe_sz,
+ 			 DMA_FROM_DEVICE);
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		dev_kfree_skb(skb);
+ 		goto wq_ll_pop;
+ 	}
+ 
+ 	cqe_bcnt = be32_to_cpu(cqe->byte_cnt);
+ 	skb_put(skb, cqe_bcnt);
+ 
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ wq_ll_pop:
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ static inline void mlx5e_mpwqe_fill_rx_skb(struct mlx5e_rq *rq,
+ 					   struct mlx5_cqe64 *cqe,
+ 					   struct mlx5e_mpw_info *wi,
+ 					   u32 cqe_bcnt,
+ 					   struct sk_buff *skb)
+ {
+ 	u32 consumed_bytes = ALIGN(cqe_bcnt, MLX5_MPWRQ_STRIDE_SIZE);
+ 	u16 stride_ix      = mpwrq_get_cqe_stride_index(cqe);
+ 	u32 wqe_offset     = stride_ix * MLX5_MPWRQ_STRIDE_SIZE;
+ 	u32 head_offset    = wqe_offset & (PAGE_SIZE - 1);
+ 	u32 page_idx       = wqe_offset >> PAGE_SHIFT;
+ 	u32 head_page_idx  = page_idx;
+ 	u16 headlen = min_t(u16, MLX5_MPWRQ_SMALL_PACKET_THRESHOLD, cqe_bcnt);
+ 	u32 frag_offset    = head_offset + headlen;
+ 	u16 byte_cnt       = cqe_bcnt - headlen;
+ 
+ #if (MLX5_MPWRQ_SMALL_PACKET_THRESHOLD >= MLX5_MPWRQ_STRIDE_SIZE)
+ 	if (unlikely(frag_offset >= PAGE_SIZE)) {
+ 		page_idx++;
+ 		frag_offset -= PAGE_SIZE;
+ 	}
+ #endif
+ 	wi->dma_pre_sync(rq->pdev, wi, wqe_offset, consumed_bytes);
+ 
+ 	while (byte_cnt) {
+ 		u32 pg_consumed_bytes =
+ 			min_t(u32, PAGE_SIZE - frag_offset, byte_cnt);
+ 
+ 		wi->add_skb_frag(rq->pdev, skb, wi, page_idx, frag_offset,
+ 				 pg_consumed_bytes);
+ 		byte_cnt -= pg_consumed_bytes;
+ 		frag_offset = 0;
+ 		page_idx++;
+ 	}
+ 	/* copy header */
+ 	wi->copy_skb_header(rq->pdev, skb, wi, head_page_idx, head_offset,
+ 			    headlen);
+ 	/* skb linear part was allocated with headlen and aligned to long */
+ 	skb->tail += headlen;
+ 	skb->len  += headlen;
+ }
+ 
+ void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
+ 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
+ 	struct mlx5e_mpw_info *wi = &rq->wqe_info[wqe_id];
+ 	struct mlx5e_rx_wqe  *wqe = mlx5_wq_ll_get_wqe(&rq->wq, wqe_id);
+ 	struct sk_buff *skb;
+ 	u16 cqe_bcnt;
+ 
+ 	wi->consumed_strides += cstrides;
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	if (unlikely(mpwrq_is_filler_cqe(cqe))) {
+ 		rq->stats.mpwqe_filler++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	skb = napi_alloc_skb(rq->cq.napi,
+ 			     ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
+ 				   sizeof(long)));
+ 	if (unlikely(!skb))
+ 		goto mpwrq_cqe_out;
+ 
+ 	prefetch(skb->data);
+ 	cqe_bcnt = mpwrq_get_cqe_byte_cnt(cqe);
+ 
+ 	mlx5e_mpwqe_fill_rx_skb(rq, cqe, wi, cqe_bcnt, skb);
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ mpwrq_cqe_out:
+ 	if (likely(wi->consumed_strides < MLX5_MPWRQ_NUM_STRIDES))
+ 		return;
+ 
+ 	wi->free_wqe(rq, wi);
+ 	mlx5_wq_ll_pop(&rq->wq, cqe->wqe_id, &wqe->next.next_wqe_index);
++>>>>>>> c5adb96f6c4a (net/mlx5e: Use napi_alloc_skb for RX SKB allocations)
  }
  
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
