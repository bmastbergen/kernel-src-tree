rhashtable: Extend RCU read lock into rhashtable_insert_rehash()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Graf <tgraf@suug.ch>
commit 58be8a583d8d316448bafa5926414cfb83c02dec
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/58be8a58.failed

rhashtable_insert_rehash() requires RCU locks to be held in order
to access ht->tbl and traverse to the last table.

Fixes: ccd57b1bd324 ("rhashtable: Add immediate rehash during insertion")
	Signed-off-by: Thomas Graf <tgraf@suug.ch>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 58be8a583d8d316448bafa5926414cfb83c02dec)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rhashtable.h
diff --cc include/linux/rhashtable.h
index 0839d7b8cd60,5976ab59b88f..000000000000
--- a/include/linux/rhashtable.h
+++ b/include/linux/rhashtable.h
@@@ -261,4 -482,313 +261,316 @@@ void rhashtable_destroy(const struct rh
  	rht_for_each_entry_rcu_continue(tpos, pos, (tbl)->buckets[hash],\
  					tbl, hash, member)
  
++<<<<<<< HEAD
++=======
+ static inline int rhashtable_compare(struct rhashtable_compare_arg *arg,
+ 				     const void *obj)
+ {
+ 	struct rhashtable *ht = arg->ht;
+ 	const char *ptr = obj;
+ 
+ 	return memcmp(ptr + ht->p.key_offset, arg->key, ht->p.key_len);
+ }
+ 
+ /**
+  * rhashtable_lookup_fast - search hash table, inlined version
+  * @ht:		hash table
+  * @key:	the pointer to the key
+  * @params:	hash table parameters
+  *
+  * Computes the hash value for the key and traverses the bucket chain looking
+  * for a entry with an identical key. The first matching entry is returned.
+  *
+  * Returns the first entry on which the compare function returned true.
+  */
+ static inline void *rhashtable_lookup_fast(
+ 	struct rhashtable *ht, const void *key,
+ 	const struct rhashtable_params params)
+ {
+ 	struct rhashtable_compare_arg arg = {
+ 		.ht = ht,
+ 		.key = key,
+ 	};
+ 	const struct bucket_table *tbl;
+ 	struct rhash_head *he;
+ 	unsigned hash;
+ 
+ 	rcu_read_lock();
+ 
+ 	tbl = rht_dereference_rcu(ht->tbl, ht);
+ restart:
+ 	hash = rht_key_hashfn(ht, tbl, key, params);
+ 	rht_for_each_rcu(he, tbl, hash) {
+ 		if (params.obj_cmpfn ?
+ 		    params.obj_cmpfn(&arg, rht_obj(ht, he)) :
+ 		    rhashtable_compare(&arg, rht_obj(ht, he)))
+ 			continue;
+ 		rcu_read_unlock();
+ 		return rht_obj(ht, he);
+ 	}
+ 
+ 	/* Ensure we see any new tables. */
+ 	smp_rmb();
+ 
+ 	tbl = rht_dereference_rcu(tbl->future_tbl, ht);
+ 	if (unlikely(tbl))
+ 		goto restart;
+ 	rcu_read_unlock();
+ 
+ 	return NULL;
+ }
+ 
+ static inline int __rhashtable_insert_fast(
+ 	struct rhashtable *ht, const void *key, struct rhash_head *obj,
+ 	const struct rhashtable_params params)
+ {
+ 	struct rhashtable_compare_arg arg = {
+ 		.ht = ht,
+ 		.key = key,
+ 	};
+ 	struct bucket_table *tbl, *new_tbl;
+ 	struct rhash_head *head;
+ 	spinlock_t *lock;
+ 	unsigned elasticity;
+ 	unsigned hash;
+ 	int err;
+ 
+ restart:
+ 	rcu_read_lock();
+ 
+ 	tbl = rht_dereference_rcu(ht->tbl, ht);
+ 
+ 	/* All insertions must grab the oldest table containing
+ 	 * the hashed bucket that is yet to be rehashed.
+ 	 */
+ 	for (;;) {
+ 		hash = rht_head_hashfn(ht, tbl, obj, params);
+ 		lock = rht_bucket_lock(tbl, hash);
+ 		spin_lock_bh(lock);
+ 
+ 		if (tbl->rehash <= hash)
+ 			break;
+ 
+ 		spin_unlock_bh(lock);
+ 		tbl = rht_dereference_rcu(tbl->future_tbl, ht);
+ 	}
+ 
+ 	new_tbl = rht_dereference_rcu(tbl->future_tbl, ht);
+ 	if (unlikely(new_tbl)) {
+ 		err = rhashtable_insert_slow(ht, key, obj, new_tbl);
+ 		if (err == -EAGAIN)
+ 			goto slow_path;
+ 		goto out;
+ 	}
+ 
+ 	if (unlikely(rht_grow_above_100(ht, tbl))) {
+ slow_path:
+ 		spin_unlock_bh(lock);
+ 		err = rhashtable_insert_rehash(ht);
+ 		rcu_read_unlock();
+ 		if (err)
+ 			return err;
+ 
+ 		goto restart;
+ 	}
+ 
+ 	err = -EEXIST;
+ 	elasticity = ht->elasticity;
+ 	rht_for_each(head, tbl, hash) {
+ 		if (key &&
+ 		    unlikely(!(params.obj_cmpfn ?
+ 			       params.obj_cmpfn(&arg, rht_obj(ht, head)) :
+ 			       rhashtable_compare(&arg, rht_obj(ht, head)))))
+ 			goto out;
+ 		if (!--elasticity)
+ 			goto slow_path;
+ 	}
+ 
+ 	err = 0;
+ 
+ 	head = rht_dereference_bucket(tbl->buckets[hash], tbl, hash);
+ 
+ 	RCU_INIT_POINTER(obj->next, head);
+ 
+ 	rcu_assign_pointer(tbl->buckets[hash], obj);
+ 
+ 	atomic_inc(&ht->nelems);
+ 	if (rht_grow_above_75(ht, tbl))
+ 		schedule_work(&ht->run_work);
+ 
+ out:
+ 	spin_unlock_bh(lock);
+ 	rcu_read_unlock();
+ 
+ 	return err;
+ }
+ 
+ /**
+  * rhashtable_insert_fast - insert object into hash table
+  * @ht:		hash table
+  * @obj:	pointer to hash head inside object
+  * @params:	hash table parameters
+  *
+  * Will take a per bucket spinlock to protect against mutual mutations
+  * on the same bucket. Multiple insertions may occur in parallel unless
+  * they map to the same bucket lock.
+  *
+  * It is safe to call this function from atomic context.
+  *
+  * Will trigger an automatic deferred table resizing if the size grows
+  * beyond the watermark indicated by grow_decision() which can be passed
+  * to rhashtable_init().
+  */
+ static inline int rhashtable_insert_fast(
+ 	struct rhashtable *ht, struct rhash_head *obj,
+ 	const struct rhashtable_params params)
+ {
+ 	return __rhashtable_insert_fast(ht, NULL, obj, params);
+ }
+ 
+ /**
+  * rhashtable_lookup_insert_fast - lookup and insert object into hash table
+  * @ht:		hash table
+  * @obj:	pointer to hash head inside object
+  * @params:	hash table parameters
+  *
+  * Locks down the bucket chain in both the old and new table if a resize
+  * is in progress to ensure that writers can't remove from the old table
+  * and can't insert to the new table during the atomic operation of search
+  * and insertion. Searches for duplicates in both the old and new table if
+  * a resize is in progress.
+  *
+  * This lookup function may only be used for fixed key hash table (key_len
+  * parameter set). It will BUG() if used inappropriately.
+  *
+  * It is safe to call this function from atomic context.
+  *
+  * Will trigger an automatic deferred table resizing if the size grows
+  * beyond the watermark indicated by grow_decision() which can be passed
+  * to rhashtable_init().
+  */
+ static inline int rhashtable_lookup_insert_fast(
+ 	struct rhashtable *ht, struct rhash_head *obj,
+ 	const struct rhashtable_params params)
+ {
+ 	const char *key = rht_obj(ht, obj);
+ 
+ 	BUG_ON(ht->p.obj_hashfn);
+ 
+ 	return __rhashtable_insert_fast(ht, key + ht->p.key_offset, obj,
+ 					params);
+ }
+ 
+ /**
+  * rhashtable_lookup_insert_key - search and insert object to hash table
+  *				  with explicit key
+  * @ht:		hash table
+  * @key:	key
+  * @obj:	pointer to hash head inside object
+  * @params:	hash table parameters
+  *
+  * Locks down the bucket chain in both the old and new table if a resize
+  * is in progress to ensure that writers can't remove from the old table
+  * and can't insert to the new table during the atomic operation of search
+  * and insertion. Searches for duplicates in both the old and new table if
+  * a resize is in progress.
+  *
+  * Lookups may occur in parallel with hashtable mutations and resizing.
+  *
+  * Will trigger an automatic deferred table resizing if the size grows
+  * beyond the watermark indicated by grow_decision() which can be passed
+  * to rhashtable_init().
+  *
+  * Returns zero on success.
+  */
+ static inline int rhashtable_lookup_insert_key(
+ 	struct rhashtable *ht, const void *key, struct rhash_head *obj,
+ 	const struct rhashtable_params params)
+ {
+ 	BUG_ON(!ht->p.obj_hashfn || !key);
+ 
+ 	return __rhashtable_insert_fast(ht, key, obj, params);
+ }
+ 
+ static inline int __rhashtable_remove_fast(
+ 	struct rhashtable *ht, struct bucket_table *tbl,
+ 	struct rhash_head *obj, const struct rhashtable_params params)
+ {
+ 	struct rhash_head __rcu **pprev;
+ 	struct rhash_head *he;
+ 	spinlock_t * lock;
+ 	unsigned hash;
+ 	int err = -ENOENT;
+ 
+ 	hash = rht_head_hashfn(ht, tbl, obj, params);
+ 	lock = rht_bucket_lock(tbl, hash);
+ 
+ 	spin_lock_bh(lock);
+ 
+ 	pprev = &tbl->buckets[hash];
+ 	rht_for_each(he, tbl, hash) {
+ 		if (he != obj) {
+ 			pprev = &he->next;
+ 			continue;
+ 		}
+ 
+ 		rcu_assign_pointer(*pprev, obj->next);
+ 		err = 0;
+ 		break;
+ 	}
+ 
+ 	spin_unlock_bh(lock);
+ 
+ 	return err;
+ }
+ 
+ /**
+  * rhashtable_remove_fast - remove object from hash table
+  * @ht:		hash table
+  * @obj:	pointer to hash head inside object
+  * @params:	hash table parameters
+  *
+  * Since the hash chain is single linked, the removal operation needs to
+  * walk the bucket chain upon removal. The removal operation is thus
+  * considerable slow if the hash table is not correctly sized.
+  *
+  * Will automatically shrink the table via rhashtable_expand() if the
+  * shrink_decision function specified at rhashtable_init() returns true.
+  *
+  * Returns zero on success, -ENOENT if the entry could not be found.
+  */
+ static inline int rhashtable_remove_fast(
+ 	struct rhashtable *ht, struct rhash_head *obj,
+ 	const struct rhashtable_params params)
+ {
+ 	struct bucket_table *tbl;
+ 	int err;
+ 
+ 	rcu_read_lock();
+ 
+ 	tbl = rht_dereference_rcu(ht->tbl, ht);
+ 
+ 	/* Because we have already taken (and released) the bucket
+ 	 * lock in old_tbl, if we find that future_tbl is not yet
+ 	 * visible then that guarantees the entry to still be in
+ 	 * the old tbl if it exists.
+ 	 */
+ 	while ((err = __rhashtable_remove_fast(ht, tbl, obj, params)) &&
+ 	       (tbl = rht_dereference_rcu(tbl->future_tbl, ht)))
+ 		;
+ 
+ 	if (err)
+ 		goto out;
+ 
+ 	atomic_dec(&ht->nelems);
+ 	if (rht_shrink_below_30(ht, tbl))
+ 		schedule_work(&ht->run_work);
+ 
+ out:
+ 	rcu_read_unlock();
+ 
+ 	return err;
+ }
+ 
++>>>>>>> 58be8a583d8d (rhashtable: Extend RCU read lock into rhashtable_insert_rehash())
  #endif /* _LINUX_RHASHTABLE_H */
* Unmerged path include/linux/rhashtable.h
