dm: remove support for stacking dm-mq on .request_fn device(s)

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mike Snitzer <snitzer@redhat.com>
commit c5248f79f39e5254977a3916b2149c3ccffa2722
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/c5248f79.failed

Remove all fiddley code that propped up this support for a blk-mq
request-queue ontop of all .request_fn devices.

Testing has proven this niche request-based dm-mq mode to be buggy, when
testing fault tolerance with DM multipath, and there is no point trying
to preserve it.

Should help improve efficiency of pure dm-mq code and make code
maintenance less delicate.

	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit c5248f79f39e5254977a3916b2149c3ccffa2722)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm.c
diff --cc drivers/md/dm.c
index 511a3a6992f6,26fedd93702e..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -2909,24 -2705,29 +2888,44 @@@ static int dm_init_request_based_blk_mq
  	struct request_queue *q;
  	int err;
  
++<<<<<<< HEAD
 +	memset(&md->tag_set, 0, sizeof(md->tag_set));
 +	md->tag_set.ops = &dm_mq_ops;
 +	md->tag_set.queue_depth = dm_get_blk_mq_queue_depth();
 +	md->tag_set.numa_node = NUMA_NO_NODE;
 +	md->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
 +	md->tag_set.nr_hw_queues = dm_get_blk_mq_nr_hw_queues();
 +	if (md_type == DM_TYPE_REQUEST_BASED) {
 +		/* make the memory for non-blk-mq clone part of the pdu */
 +		md->tag_set.cmd_size = sizeof(struct dm_rq_target_io) + sizeof(struct request);
 +	} else
 +		md->tag_set.cmd_size = sizeof(struct dm_rq_target_io);
 +	md->tag_set.driver_data = md;
++=======
+ 	if (dm_get_md_type(md) == DM_TYPE_REQUEST_BASED) {
+ 		DMERR("request-based dm-mq may only be stacked on blk-mq device(s)");
+ 		return -EINVAL;
+ 	}
+ 
+ 	md->tag_set = kzalloc(sizeof(struct blk_mq_tag_set), GFP_KERNEL);
+ 	if (!md->tag_set)
+ 		return -ENOMEM;
+ 
+ 	md->tag_set->ops = &dm_mq_ops;
+ 	md->tag_set->queue_depth = dm_get_blk_mq_queue_depth();
+ 	md->tag_set->numa_node = NUMA_NO_NODE;
+ 	md->tag_set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+ 	md->tag_set->nr_hw_queues = dm_get_blk_mq_nr_hw_queues();
+ 	md->tag_set->driver_data = md;
+ 
+ 	md->tag_set->cmd_size = sizeof(struct dm_rq_target_io);
++>>>>>>> c5248f79f39e (dm: remove support for stacking dm-mq on .request_fn device(s))
  
 -	err = blk_mq_alloc_tag_set(md->tag_set);
 +	err = blk_mq_alloc_tag_set(&md->tag_set);
  	if (err)
 -		goto out_kfree_tag_set;
 +		return err;
  
 -	q = blk_mq_init_allocated_queue(md->tag_set, md->queue);
 +	q = blk_mq_init_allocated_queue(&md->tag_set, md->queue);
  	if (IS_ERR(q)) {
  		err = PTR_ERR(q);
  		goto out_tag_set;
diff --git a/drivers/md/dm-mpath.c b/drivers/md/dm-mpath.c
index 15016b4ea208..0057227d4cae 100644
--- a/drivers/md/dm-mpath.c
+++ b/drivers/md/dm-mpath.c
@@ -421,7 +421,10 @@ static int __multipath_map(struct dm_target *ti, struct request *clone,
 	spin_unlock_irq(&m->lock);
 
 	if (clone) {
-		/* Old request-based interface: allocated clone is passed in */
+		/*
+		 * Old request-based interface: allocated clone is passed in.
+		 * Used by: .request_fn stacked on .request_fn path(s).
+		 */
 		clone->q = bdev_get_queue(bdev);
 		clone->rq_disk = bdev->bd_disk;
 		clone->cmd_flags |= REQ_FAILFAST_TRANSPORT;
* Unmerged path drivers/md/dm.c
