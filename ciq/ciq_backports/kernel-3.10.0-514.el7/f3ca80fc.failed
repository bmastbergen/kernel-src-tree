nvme: move chardev and sysfs interface to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Christoph Hellwig <hch@lst.de>
commit f3ca80fc11c3af566eacd99cf821c1a48035c63b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f3ca80fc.failed

For this we need to add a proper controller init routine and a list of
all controllers that is in addition to the list of PCIe controllers,
which stays in pci.c.  Note that we remove the sysfs device when the
last reference to a controller is dropped now - the old code would have
kept it around longer, which doesn't make much sense.

This requires a new ->reset_ctrl operation to implement controleller
resets, and a new ->write_reg32 operation that is required to implement
subsystem resets.  We also now store caches copied of the NVMe compliance
version and the flag if a controller is attached to a subsystem or not in
the generic controller structure now.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
[Fixes for pr merge]
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit f3ca80fc11c3af566eacd99cf821c1a48035c63b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
#	drivers/block/nvme-scsi.c
#	drivers/nvme/host/core.c
#	drivers/nvme/host/nvme.h
diff --cc drivers/block/nvme-core.c
index 2a6eb55ad96c,87ad57bcc7ed..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -38,13 -36,13 +38,21 @@@
  #include <linux/ptrace.h>
  #include <linux/sched.h>
  #include <linux/slab.h>
 -#include <linux/t10-pi.h>
  #include <linux/types.h>
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +#include <linux/pr.h>
 +#include <scsi/sg.h>
 +#include <asm-generic/io-64-nonatomic-lo-hi.h>
 +#include <asm/unaligned.h>
 +
 +#define NVME_MINORS		(1U << MINORBITS)
++=======
+ #include <linux/io-64-nonatomic-lo-hi.h>
+ #include <asm/unaligned.h>
+ 
+ #include "nvme.h"
+ 
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  #define NVME_Q_DEPTH		1024
  #define NVME_AQ_DEPTH		256
  #define SQ_SIZE(depth)		(depth * sizeof(struct nvme_command))
@@@ -64,12 -60,6 +72,15 @@@ static unsigned char shutdown_timeout 
  module_param(shutdown_timeout, byte, 0644);
  MODULE_PARM_DESC(shutdown_timeout, "timeout in seconds for controller shutdown");
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static int nvme_major;
 +module_param(nvme_major, int, 0);
 +
 +static int nvme_char_major;
 +module_param(nvme_char_major, int, 0);
 +
++=======
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  static int use_threaded_interrupts;
  module_param(use_threaded_interrupts, int, 0);
  
@@@ -79,11 -72,15 +90,19 @@@ static struct task_struct *nvme_thread
  static struct workqueue_struct *nvme_workq;
  static wait_queue_head_t nvme_kthread_wait;
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static struct class *nvme_class;
 +
 +static void nvme_reset_failed_dev(struct work_struct *ws);
++=======
+ struct nvme_dev;
+ struct nvme_queue;
+ struct nvme_iod;
+ 
+ static int __nvme_reset(struct nvme_dev *dev);
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  static int nvme_reset(struct nvme_dev *dev);
 -static void nvme_process_cq(struct nvme_queue *nvmeq);
 -static void nvme_unmap_data(struct nvme_dev *dev, struct nvme_iod *iod);
 -static void nvme_dead_ctrl(struct nvme_dev *dev);
 +static int nvme_process_cq(struct nvme_queue *nvmeq);
  
  struct async_cmd_info {
  	struct kthread_work work;
@@@ -1562,504 -1496,6 +1581,507 @@@ static int nvme_configure_admin_queue(s
  	return result;
  }
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
 +				unsigned long addr, unsigned length)
 +{
 +	int i, err, count, nents, offset;
 +	struct scatterlist *sg;
 +	struct page **pages;
 +	struct nvme_iod *iod;
 +
 +	if (addr & 3)
 +		return ERR_PTR(-EINVAL);
 +	if (!length || length > INT_MAX - PAGE_SIZE)
 +		return ERR_PTR(-EINVAL);
 +
 +	offset = offset_in_page(addr);
 +	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
 +	pages = kcalloc(count, sizeof(*pages), GFP_KERNEL);
 +	if (!pages)
 +		return ERR_PTR(-ENOMEM);
 +
 +	err = get_user_pages_fast(addr, count, 1, pages);
 +	if (err < count) {
 +		count = err;
 +		err = -EFAULT;
 +		goto put_pages;
 +	}
 +
 +	err = -ENOMEM;
 +	iod = __nvme_alloc_iod(count, length, dev, 0, GFP_KERNEL);
 +	if (!iod)
 +		goto put_pages;
 +
 +	sg = iod->sg;
 +	sg_init_table(sg, count);
 +	for (i = 0; i < count; i++) {
 +		sg_set_page(&sg[i], pages[i],
 +			    min_t(unsigned, length, PAGE_SIZE - offset),
 +			    offset);
 +		length -= (PAGE_SIZE - offset);
 +		offset = 0;
 +	}
 +	sg_mark_end(&sg[i - 1]);
 +	iod->nents = count;
 +
 +	nents = dma_map_sg(&dev->pci_dev->dev, sg, count,
 +				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 +	if (!nents)
 +		goto free_iod;
 +
 +	kfree(pages);
 +	return iod;
 +
 + free_iod:
 +	kfree(iod);
 + put_pages:
 +	for (i = 0; i < count; i++)
 +		put_page(pages[i]);
 +	kfree(pages);
 +	return ERR_PTR(err);
 +}
 +
 +void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
 +			struct nvme_iod *iod)
 +{
 +	int i;
 +
 +	dma_unmap_sg(&dev->pci_dev->dev, iod->sg, iod->nents,
 +				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 +
 +	for (i = 0; i < iod->nents; i++)
 +		put_page(sg_page(&iod->sg[i]));
 +}
 +
 +static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 +{
 +	struct nvme_dev *dev = ns->dev;
 +	struct nvme_user_io io;
 +	struct nvme_command c;
 +	unsigned length, meta_len, prp_len;
 +	int status, write;
 +	struct nvme_iod *iod;
 +	dma_addr_t meta_dma = 0;
 +	void *meta = NULL;
 +
 +	if (copy_from_user(&io, uio, sizeof(io)))
 +		return -EFAULT;
 +	length = (io.nblocks + 1) << ns->lba_shift;
 +	meta_len = (io.nblocks + 1) * ns->ms;
 +
 +	if (meta_len && ((io.metadata & 3) || !io.metadata) && !ns->ext)
 +		return -EINVAL;
 +	else if (meta_len && ns->ext) {
 +		length += meta_len;
 +		meta_len = 0;
 +	}
 +
 +	write = io.opcode & 1;
 +
 +	switch (io.opcode) {
 +	case nvme_cmd_write:
 +	case nvme_cmd_read:
 +	case nvme_cmd_compare:
 +		iod = nvme_map_user_pages(dev, write, io.addr, length);
 +		break;
 +	default:
 +		return -EINVAL;
 +	}
 +
 +	if (IS_ERR(iod))
 +		return PTR_ERR(iod);
 +
 +	prp_len = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
 +	if (length != prp_len) {
 +		status = -ENOMEM;
 +		goto unmap;
 +	}
 +	if (meta_len) {
 +		meta = dma_alloc_coherent(&dev->pci_dev->dev, meta_len,
 +					  &meta_dma, GFP_KERNEL);
 +		if (!meta) {
 +			status = -ENOMEM;
 +			goto unmap;
 +		}
 +		if (write) {
 +			if (copy_from_user(meta, (void __user *)io.metadata,
 +					   meta_len)) {
 +				status = -EFAULT;
 +				goto unmap;
 +			}
 +		}
 +	}
 +
 +	memset(&c, 0, sizeof(c));
 +	c.rw.opcode = io.opcode;
 +	c.rw.flags = io.flags;
 +	c.rw.nsid = cpu_to_le32(ns->ns_id);
 +	c.rw.slba = cpu_to_le64(io.slba);
 +	c.rw.length = cpu_to_le16(io.nblocks);
 +	c.rw.control = cpu_to_le16(io.control);
 +	c.rw.dsmgmt = cpu_to_le32(io.dsmgmt);
 +	c.rw.reftag = cpu_to_le32(io.reftag);
 +	c.rw.apptag = cpu_to_le16(io.apptag);
 +	c.rw.appmask = cpu_to_le16(io.appmask);
 +	c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 +	c.rw.prp2 = cpu_to_le64(iod->first_dma);
 +	c.rw.metadata = cpu_to_le64(meta_dma);
 +	status = nvme_submit_sync_cmd(ns->queue, &c);
 + unmap:
 +	nvme_unmap_user_pages(dev, write, iod);
 +	nvme_free_iod(dev, iod);
 +
 +	if (meta) {
 +		if (status == NVME_SC_SUCCESS && !write) {
 +			if (copy_to_user((void __user *)io.metadata, meta,
 +					 meta_len))
 +				status = -EFAULT;
 +		}
 +		dma_free_coherent(&dev->pci_dev->dev, meta_len, meta, meta_dma);
 +	}
 +	return status;
 +}
 +
 +static int nvme_user_cmd(struct nvme_dev *dev, struct nvme_ns *ns,
 +			struct nvme_passthru_cmd __user *ucmd)
 +{
 +	struct nvme_passthru_cmd cmd;
 +	struct nvme_command c;
 +	int status, length;
 +	struct nvme_iod *uninitialized_var(iod);
 +	unsigned timeout;
 +
 +	if (!capable(CAP_SYS_ADMIN))
 +		return -EACCES;
 +	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
 +		return -EFAULT;
 +
 +	memset(&c, 0, sizeof(c));
 +	c.common.opcode = cmd.opcode;
 +	c.common.flags = cmd.flags;
 +	c.common.nsid = cpu_to_le32(cmd.nsid);
 +	c.common.cdw2[0] = cpu_to_le32(cmd.cdw2);
 +	c.common.cdw2[1] = cpu_to_le32(cmd.cdw3);
 +	c.common.cdw10[0] = cpu_to_le32(cmd.cdw10);
 +	c.common.cdw10[1] = cpu_to_le32(cmd.cdw11);
 +	c.common.cdw10[2] = cpu_to_le32(cmd.cdw12);
 +	c.common.cdw10[3] = cpu_to_le32(cmd.cdw13);
 +	c.common.cdw10[4] = cpu_to_le32(cmd.cdw14);
 +	c.common.cdw10[5] = cpu_to_le32(cmd.cdw15);
 +
 +	length = cmd.data_len;
 +	if (cmd.data_len) {
 +		iod = nvme_map_user_pages(dev, cmd.opcode & 1, cmd.addr,
 +								length);
 +		if (IS_ERR(iod))
 +			return PTR_ERR(iod);
 +		length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
 +		c.common.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 +		c.common.prp2 = cpu_to_le64(iod->first_dma);
 +	}
 +
 +	timeout = cmd.timeout_ms ? msecs_to_jiffies(cmd.timeout_ms) :
 +								ADMIN_TIMEOUT;
 +
 +	if (length != cmd.data_len) {
 +		status = -ENOMEM;
 +		goto out;
 +	}
 +
 +	status = __nvme_submit_sync_cmd(ns ? ns->queue : dev->admin_q, &c,
 +					&cmd.result, timeout);
 +
 +out:
 +	if (cmd.data_len) {
 +		nvme_unmap_user_pages(dev, cmd.opcode & 1, iod);
 +		nvme_free_iod(dev, iod);
 +	}
 +
 +	if ((status >= 0) && copy_to_user(&ucmd->result, &cmd.result,
 +							sizeof(cmd.result)))
 +		status = -EFAULT;
 +
 +	return status;
 +}
 +
 +static int nvme_subsys_reset(struct nvme_dev *dev)
 +{
 +	if (!dev->subsystem)
 +		return -ENOTTY;
 +
 +	writel(0x4E564D65, &dev->bar->nssr); /* "NVMe" */
 +	return 0;
 +}
 +
 +static int nvme_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,
 +							unsigned long arg)
 +{
 +	struct nvme_ns *ns = bdev->bd_disk->private_data;
 +
 +	switch (cmd) {
 +	case NVME_IOCTL_ID:
 +		force_successful_syscall_return();
 +		return ns->ns_id;
 +	case NVME_IOCTL_ADMIN_CMD:
 +		return nvme_user_cmd(ns->dev, NULL, (void __user *)arg);
 +	case NVME_IOCTL_IO_CMD:
 +		return nvme_user_cmd(ns->dev, ns, (void __user *)arg);
 +	case NVME_IOCTL_SUBMIT_IO:
 +		return nvme_submit_io(ns, (void __user *)arg);
 +	case SG_GET_VERSION_NUM:
 +		return nvme_sg_get_version_num((void __user *)arg);
 +	case SG_IO:
 +		return nvme_sg_io(ns, (void __user *)arg);
 +	default:
 +		return -ENOTTY;
 +	}
 +}
 +
 +#ifdef CONFIG_COMPAT
 +static int nvme_compat_ioctl(struct block_device *bdev, fmode_t mode,
 +					unsigned int cmd, unsigned long arg)
 +{
 +	switch (cmd) {
 +	case SG_IO:
 +		return -ENOIOCTLCMD;
 +	}
 +	return nvme_ioctl(bdev, mode, cmd, arg);
 +}
 +#else
 +#define nvme_compat_ioctl	NULL
 +#endif
 +
 +static void nvme_free_dev(struct kref *kref);
 +static void nvme_free_ns(struct kref *kref)
 +{
 +	struct nvme_ns *ns = container_of(kref, struct nvme_ns, kref);
 +
 +	spin_lock(&dev_list_lock);
 +	ns->disk->private_data = NULL;
 +	spin_unlock(&dev_list_lock);
 +
 +	kref_put(&ns->dev->kref, nvme_free_dev);
 +	put_disk(ns->disk);
 +	kfree(ns);
 +}
 +
 +static int nvme_open(struct block_device *bdev, fmode_t mode)
 +{
 +	int ret = 0;
 +	struct nvme_ns *ns;
 +
 +	spin_lock(&dev_list_lock);
 +	ns = bdev->bd_disk->private_data;
 +	if (!ns)
 +		ret = -ENXIO;
 +	else if (!kref_get_unless_zero(&ns->kref))
 +		ret = -ENXIO;
 +	spin_unlock(&dev_list_lock);
 +
 +	return ret;
 +}
 +
 +static void nvme_release(struct gendisk *disk, fmode_t mode)
 +{
 +	struct nvme_ns *ns = disk->private_data;
 +	kref_put(&ns->kref, nvme_free_ns);
 +}
 +
 +static int nvme_getgeo(struct block_device *bd, struct hd_geometry *geo)
 +{
 +	/* some standard values */
 +	geo->heads = 1 << 6;
 +	geo->sectors = 1 << 5;
 +	geo->cylinders = get_capacity(bd->bd_disk) >> 11;
 +	return 0;
 +}
 +
 +static void nvme_config_discard(struct nvme_ns *ns)
 +{
 +	u32 logical_block_size = queue_logical_block_size(ns->queue);
 +	ns->queue->limits.discard_zeroes_data = 0;
 +	ns->queue->limits.discard_alignment = logical_block_size;
 +	ns->queue->limits.discard_granularity = logical_block_size;
 +	ns->queue->limits.max_discard_sectors = 0xffffffff;
 +	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, ns->queue);
 +}
 +
 +static int nvme_revalidate_disk(struct gendisk *disk)
 +{
 +	struct nvme_ns *ns = disk->private_data;
 +	struct nvme_dev *dev = ns->dev;
 +	struct nvme_id_ns *id;
 +	dma_addr_t dma_addr;
 +	u8 lbaf;
 +	u16 old_ms;
 +	unsigned short bs;
 +
 +	id = dma_alloc_coherent(&dev->pci_dev->dev, 4096, &dma_addr,
 +								GFP_KERNEL);
 +	if (!id) {
 +		dev_warn(&dev->pci_dev->dev, "%s: Memory alocation failure\n",
 +								__func__);
 +		return 0;
 +	}
 +
 +	if (nvme_identify(dev, ns->ns_id, 0, dma_addr)) {
 +		dev_warn(&dev->pci_dev->dev,
 +			"identify failed ns:%d, setting capacity to 0\n",
 +			ns->ns_id);
 +		memset(id, 0, sizeof(*id));
 +	}
 +
 +	if (id->ncap == 0) {
 +		dma_free_coherent(&dev->pci_dev->dev, 4096, id, dma_addr);
 +		return -ENODEV;
 +	}
 +
 +	old_ms = ns->ms;
 +	lbaf = id->flbas & NVME_NS_FLBAS_LBA_MASK;
 +	ns->lba_shift = id->lbaf[lbaf].ds;
 +
 +	ns->ms = le16_to_cpu(id->lbaf[lbaf].ms);
 +	ns->ext = ns->ms && (id->flbas & NVME_NS_FLBAS_META_EXT);
 +
 +	/*
 +	 * If identify namespace failed, use default 512 byte block size so
 +	 * block layer can use before failing read/write for 0 capacity.
 +	 */
 +	if (ns->lba_shift == 0)
 +		ns->lba_shift = 9;
 +	bs = 1 << ns->lba_shift;
 +
 +	if (blk_get_integrity(disk) && (ns->ms != old_ms ||
 +				bs != queue_logical_block_size(disk->queue) ||
 +				(ns->ms && ns->ext)))
 +		blk_integrity_unregister(disk);
 +
 +	ns->pi_type = ns->ms == 8 ? id->dps & NVME_NS_DPS_PI_MASK : 0;
 +	blk_queue_logical_block_size(ns->queue, bs);
 +
 +	if (ns->ms && !blk_get_integrity(disk) && (disk->flags & GENHD_FL_UP) &&
 +								!ns->ext)
 +		nvme_init_integrity(ns);
 +
 +	if ((ns->ms && !blk_get_integrity(disk) &&
 +					!(ns->pi_type && ns->ms == 8)))
 +		set_capacity(disk, 0);
 +	else
 +		set_capacity(disk, le64_to_cpup(&id->nsze) << (ns->lba_shift - 9));
 +
 +	if (dev->oncs & NVME_CTRL_ONCS_DSM)
 +		nvme_config_discard(ns);
 +
 +	dma_free_coherent(&dev->pci_dev->dev, 4096, id, dma_addr);
 +	return 0;
 +}
 +
 +static char nvme_pr_type(enum pr_type type)
 +{
 +	switch (type) {
 +	case PR_WRITE_EXCLUSIVE:
 +		return 1;
 +	case PR_EXCLUSIVE_ACCESS:
 +		return 2;
 +	case PR_WRITE_EXCLUSIVE_REG_ONLY:
 +		return 3;
 +	case PR_EXCLUSIVE_ACCESS_REG_ONLY:
 +		return 4;
 +	case PR_WRITE_EXCLUSIVE_ALL_REGS:
 +		return 5;
 +	case PR_EXCLUSIVE_ACCESS_ALL_REGS:
 +		return 6;
 +	default:
 +		return 0;
 +	}
 +};
 +
 +static int nvme_pr_command(struct block_device *bdev, u32 cdw10,
 +				u64 key, u64 sa_key, u8 op)
 +{
 +	struct nvme_ns *ns = bdev->bd_disk->private_data;
 +	struct nvme_command c;
 +	u8 data[16] = { 0, };
 +
 +	put_unaligned_le64(key, &data[0]);
 +	put_unaligned_le64(sa_key, &data[8]);
 +
 +	memset(&c, 0, sizeof(c));
 +	c.common.opcode = op;
 +	c.common.nsid = cpu_to_le32(ns->ns_id);
 +	c.common.cdw10[0] = cpu_to_le32(cdw10);
 +
 +	return nvme_submit_sync_cmd(ns->queue, &c, data, 16);
 +}
 +
 +static int nvme_pr_register(struct block_device *bdev, u64 old,
 +		u64 new, unsigned flags)
 +{
 +	u32 cdw10;
 +
 +	if (flags & ~PR_FL_IGNORE_KEY)
 +		return -EOPNOTSUPP;
 +
 +	cdw10 = old ? 2 : 0;
 +	cdw10 |= (flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0;
 +	cdw10 |= (1 << 30) | (1 << 31); /* PTPL=1 */
 +	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_register);
 +}
 +
 +static int nvme_pr_reserve(struct block_device *bdev, u64 key,
 +		enum pr_type type, unsigned flags)
 +{
 +	u32 cdw10;
 +
 +	if (flags & ~PR_FL_IGNORE_KEY)
 +		return -EOPNOTSUPP;
 +
 +	cdw10 = nvme_pr_type(type) << 8;
 +	cdw10 |= ((flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0);
 +	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_acquire);
 +}
 +
 +static int nvme_pr_preempt(struct block_device *bdev, u64 old, u64 new,
 +		enum pr_type type, bool abort)
 +{
 +	u32 cdw10 = nvme_pr_type(type) << 8 | abort ? 2 : 1;
 +	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_acquire);
 +}
 +
 +static int nvme_pr_clear(struct block_device *bdev, u64 key)
 +{
 +	u32 cdw10 = 1 | (key ? 1 << 3 : 0);
 +	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_register);
 +}
 +
 +static int nvme_pr_release(struct block_device *bdev, u64 key, enum pr_type type)
 +{
 +	u32 cdw10 = nvme_pr_type(type) << 8 | key ? 1 << 3 : 0;
 +	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_release);
 +}
 +
 +static const struct pr_ops nvme_pr_ops = {
 +	.pr_register	= nvme_pr_register,
 +	.pr_reserve	= nvme_pr_reserve,
 +	.pr_release	= nvme_pr_release,
 +	.pr_preempt	= nvme_pr_preempt,
 +	.pr_clear	= nvme_pr_clear,
 +};
 +
 +static const struct block_device_operations nvme_fops = {
 +	.owner		= THIS_MODULE,
 +	.ioctl		= nvme_ioctl,
 +	.compat_ioctl	= nvme_compat_ioctl,
 +	.open		= nvme_open,
 +	.release	= nvme_release,
 +	.getgeo		= nvme_getgeo,
 +	.revalidate_disk= nvme_revalidate_disk,
 +	.pr_ops		= &nvme_pr_ops,
 +};
 +
++=======
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  static int nvme_kthread(void *data)
  {
  	struct nvme_dev *dev, *next;
@@@ -2800,134 -2095,25 +2822,146 @@@ static void nvme_release_prp_pools(stru
  	dma_pool_destroy(dev->prp_small_pool);
  }
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static DEFINE_IDA(nvme_instance_ida);
 +
 +static int nvme_set_instance(struct nvme_dev *dev)
 +{
 +	int instance, error;
 +
 +	do {
 +		if (!ida_pre_get(&nvme_instance_ida, GFP_KERNEL))
 +			return -ENODEV;
 +
 +		spin_lock(&dev_list_lock);
 +		error = ida_get_new(&nvme_instance_ida, &instance);
 +		spin_unlock(&dev_list_lock);
 +	} while (error == -EAGAIN);
 +
 +	if (error)
 +		return -ENODEV;
 +
 +	dev->instance = instance;
 +	return 0;
 +}
 +
 +static void nvme_release_instance(struct nvme_dev *dev)
 +{
 +	spin_lock(&dev_list_lock);
 +	ida_remove(&nvme_instance_ida, dev->instance);
 +	spin_unlock(&dev_list_lock);
 +}
 +
 +static void nvme_free_dev(struct kref *kref)
++=======
+ static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  {
 -	struct nvme_dev *dev = to_nvme_dev(ctrl);
 +	struct nvme_dev *dev = container_of(kref, struct nvme_dev, kref);
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	pci_dev_put(dev->pci_dev);
 +	put_device(dev->device);
 +	nvme_release_instance(dev);
++=======
+ 	put_device(dev->dev);
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  	if (dev->tagset.tags)
  		blk_mq_free_tag_set(&dev->tagset);
 -	if (dev->ctrl.admin_q)
 -		blk_put_queue(dev->ctrl.admin_q);
 +	if (dev->admin_q)
 +		blk_put_queue(dev->admin_q);
  	kfree(dev->queues);
  	kfree(dev->entry);
  	kfree(dev);
  }
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static int nvme_dev_open(struct inode *inode, struct file *f)
 +{
 +	struct nvme_dev *dev;
 +	int instance = iminor(inode);
 +	int ret = -ENODEV;
 +
 +	spin_lock(&dev_list_lock);
 +	list_for_each_entry(dev, &dev_list, node) {
 +		if (dev->instance == instance) {
 +			if (!dev->admin_q) {
 +				ret = -EWOULDBLOCK;
 +				break;
 +			}
 +			if (!kref_get_unless_zero(&dev->kref))
 +				break;
 +			f->private_data = dev;
 +			ret = 0;
 +			break;
 +		}
 +	}
 +	spin_unlock(&dev_list_lock);
 +
 +	return ret;
 +}
 +
 +static int nvme_dev_release(struct inode *inode, struct file *f)
 +{
 +	struct nvme_dev *dev = f->private_data;
 +	kref_put(&dev->kref, nvme_free_dev);
 +	return 0;
 +}
 +
 +static long nvme_dev_ioctl(struct file *f, unsigned int cmd, unsigned long arg)
 +{
 +	struct nvme_dev *dev = f->private_data;
 +	struct nvme_ns *ns;
 +
 +	switch (cmd) {
 +	case NVME_IOCTL_ADMIN_CMD:
 +		return nvme_user_cmd(dev, NULL, (void __user *)arg);
 +	case NVME_IOCTL_IO_CMD:
 +		if (list_empty(&dev->namespaces))
 +			return -ENOTTY;
 +		ns = list_first_entry(&dev->namespaces, struct nvme_ns, list);
 +		return nvme_user_cmd(dev, ns, (void __user *)arg);
 +	case NVME_IOCTL_RESET:
 +		dev_warn(&dev->pci_dev->dev, "resetting controller\n");
 +		return nvme_reset(dev);
 +	case NVME_IOCTL_SUBSYS_RESET:
 +		return nvme_subsys_reset(dev);
 +	default:
 +		return -ENOTTY;
 +	}
 +}
 +
 +static const struct file_operations nvme_dev_fops = {
 +	.owner		= THIS_MODULE,
 +	.open		= nvme_dev_open,
 +	.release	= nvme_dev_release,
 +	.unlocked_ioctl	= nvme_dev_ioctl,
 +	.compat_ioctl	= nvme_dev_ioctl,
 +};
 +
 +static void nvme_set_irq_hints(struct nvme_dev *dev)
 +{
 +	struct nvme_queue *nvmeq;
 +	int i;
 +
 +	for (i = 0; i < dev->online_queues; i++) {
 +		nvmeq = dev->queues[i];
 +
 +		if (!nvmeq->tags || !(*nvmeq->tags))
 +			continue;
 +
 +		irq_set_affinity_hint(dev->entry[nvmeq->cq_vector].vector,
 +					blk_mq_tags_cpumask(*nvmeq->tags));
 +	}
 +}
 +
 +static int nvme_dev_start(struct nvme_dev *dev)
++=======
+ static void nvme_probe_work(struct work_struct *work)
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  {
 -	struct nvme_dev *dev = container_of(work, struct nvme_dev, probe_work);
 -	bool start_thread = false;
  	int result;
 +	bool start_thread = false;
  
  	result = nvme_dev_map(dev);
  	if (result)
@@@ -3085,22 -2260,45 +3119,64 @@@ static int nvme_reset(struct nvme_dev *
  	return ret;
  }
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static ssize_t nvme_sysfs_reset(struct device *dev,
 +				struct device_attribute *attr, const char *buf,
 +				size_t count)
 +{
 +	struct nvme_dev *ndev = dev_get_drvdata(dev);
 +	int ret;
 +
 +	ret = nvme_reset(ndev);
 +	if (ret < 0)
 +		return ret;
 +
 +	return count;
 +}
 +static DEVICE_ATTR(reset_controller, S_IWUSR, NULL, nvme_sysfs_reset);
 +
 +static void nvme_async_probe(struct work_struct *work);
++=======
+ static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
+ {
+ 	*val = readl(to_nvme_dev(ctrl)->bar + off);
+ 	return 0;
+ }
+ 
+ static int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
+ {
+ 	writel(val, to_nvme_dev(ctrl)->bar + off);
+ 	return 0;
+ }
+ 
+ static int nvme_pci_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
+ {
+ 	*val = readq(to_nvme_dev(ctrl)->bar + off);
+ 	return 0;
+ }
+ 
+ static bool nvme_pci_io_incapable(struct nvme_ctrl *ctrl)
+ {
+ 	struct nvme_dev *dev = to_nvme_dev(ctrl);
+ 
+ 	return !dev->bar || dev->online_queues < 2;
+ }
+ 
+ static int nvme_pci_reset_ctrl(struct nvme_ctrl *ctrl)
+ {
+ 	return nvme_reset(to_nvme_dev(ctrl));
+ }
+ 
+ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
+ 	.reg_read32		= nvme_pci_reg_read32,
+ 	.reg_write32		= nvme_pci_reg_write32,
+ 	.reg_read64		= nvme_pci_reg_read64,
+ 	.io_incapable		= nvme_pci_io_incapable,
+ 	.reset_ctrl		= nvme_pci_reset_ctrl,
+ 	.free_ctrl		= nvme_pci_free_ctrl,
+ };
+ 
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
  {
  	int node, result = -ENOMEM;
@@@ -3122,48 -2320,30 +3198,69 @@@
  	if (!dev->queues)
  		goto free;
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	INIT_LIST_HEAD(&dev->namespaces);
 +	INIT_WORK(&dev->reset_work, nvme_reset_failed_dev);
 +	dev->pci_dev = pci_dev_get(pdev);
 +	pci_set_drvdata(pdev, dev);
 +	result = nvme_set_instance(dev);
 +	if (result)
 +		goto put_pci;
 +
 +	result = nvme_setup_prp_pools(dev);
 +	if (result)
 +		goto release;
 +
 +	kref_init(&dev->kref);
 +	dev->device = device_create(nvme_class, &pdev->dev,
 +				MKDEV(nvme_char_major, dev->instance),
 +				dev, "nvme%d", dev->instance);
 +	if (IS_ERR(dev->device)) {
 +		result = PTR_ERR(dev->device);
 +		goto release_pools;
 +	}
 +	get_device(dev->device);
 +	dev_set_drvdata(dev->device, dev);
 +
 +	result = device_create_file(dev->device, &dev_attr_reset_controller);
 +	if (result)
 +		goto put_dev;
 +
 +	INIT_LIST_HEAD(&dev->node);
 +	INIT_WORK(&dev->scan_work, nvme_dev_scan);
 +	INIT_WORK(&dev->probe_work, nvme_async_probe);
 +	schedule_work(&dev->probe_work);
 +	return 0;
 +
 + put_dev:
 +	device_destroy(nvme_class, MKDEV(nvme_char_major, dev->instance));
 +	put_device(dev->device);
++=======
+ 	dev->dev = get_device(&pdev->dev);
+ 	pci_set_drvdata(pdev, dev);
+ 
+ 	INIT_LIST_HEAD(&dev->node);
+ 	INIT_WORK(&dev->scan_work, nvme_dev_scan);
+ 	INIT_WORK(&dev->probe_work, nvme_probe_work);
+ 	INIT_WORK(&dev->reset_work, nvme_reset_work);
+ 
+ 	result = nvme_setup_prp_pools(dev);
+ 	if (result)
+ 		goto put_pci;
+ 
+ 	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ 			id->driver_data);
+ 	if (result)
+ 		goto release_pools;
+ 
+ 	schedule_work(&dev->probe_work);
+ 	return 0;
+ 
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
   release_pools:
  	nvme_release_prp_pools(dev);
-  release:
- 	nvme_release_instance(dev);
   put_pci:
 -	put_device(dev->dev);
 +	pci_dev_put(dev->pci_dev);
   free:
  	kfree(dev->queues);
  	kfree(dev->entry);
@@@ -3207,14 -2379,13 +3304,20 @@@ static void nvme_remove(struct pci_dev 
  	flush_work(&dev->probe_work);
  	flush_work(&dev->reset_work);
  	flush_work(&dev->scan_work);
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	device_remove_file(dev->device, &dev_attr_reset_controller);
 +	nvme_dev_remove(dev);
 +	nvme_dev_shutdown(dev);
 +	nvme_dev_remove_admin(dev);
 +	device_destroy(nvme_class, MKDEV(nvme_char_major, dev->instance));
++=======
+ 	nvme_remove_namespaces(&dev->ctrl);
+ 	nvme_dev_shutdown(dev);
+ 	nvme_dev_remove_admin(dev);
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
  	nvme_free_queues(dev, 0);
 -	nvme_release_cmb(dev);
  	nvme_release_prp_pools(dev);
 -	nvme_put_ctrl(&dev->ctrl);
 +	kref_put(&dev->kref, nvme_free_dev);
  }
  
  /* These functions are yet to be implemented */
@@@ -3292,36 -2460,17 +3395,28 @@@ static int __init nvme_init(void
  	if (!nvme_workq)
  		return -ENOMEM;
  
 -	result = nvme_core_init();
 +	result = register_blkdev(nvme_major, "nvme");
  	if (result < 0)
  		goto kill_workq;
 +	else if (result > 0)
 +		nvme_major = result;
  
- 	result = __register_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme",
- 							&nvme_dev_fops);
- 	if (result < 0)
- 		goto unregister_blkdev;
- 	else if (result > 0)
- 		nvme_char_major = result;
- 
- 	nvme_class = class_create(THIS_MODULE, "nvme");
- 	if (IS_ERR(nvme_class)) {
- 		result = PTR_ERR(nvme_class);
- 		goto unregister_chrdev;
- 	}
- 
  	result = pci_register_driver(&nvme_driver);
  	if (result)
- 		goto destroy_class;
+ 		goto core_exit;
  	return 0;
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 + destroy_class:
 +	class_destroy(nvme_class);
 + unregister_chrdev:
 +	__unregister_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme");
 + unregister_blkdev:
 +	unregister_blkdev(nvme_major, "nvme");
++=======
+  core_exit:
+ 	nvme_core_exit();
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/pci.c
   kill_workq:
  	destroy_workqueue(nvme_workq);
  	return result;
@@@ -3330,10 -2479,8 +3425,8 @@@
  static void __exit nvme_exit(void)
  {
  	pci_unregister_driver(&nvme_driver);
 -	nvme_core_exit();
 +	unregister_blkdev(nvme_major, "nvme");
  	destroy_workqueue(nvme_workq);
- 	class_destroy(nvme_class);
- 	__unregister_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme");
  	BUG_ON(nvme_thread && !IS_ERR(nvme_thread));
  	_nvme_check_size();
  }
diff --cc drivers/block/nvme-scsi.c
index daa0d50b3bfd,e947e298a737..000000000000
--- a/drivers/block/nvme-scsi.c
+++ b/drivers/block/nvme-scsi.c
@@@ -612,81 -599,102 +612,176 @@@ static int nvme_trans_unit_serial_page(
  	return nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
  }
  
++<<<<<<< HEAD:drivers/block/nvme-scsi.c
++=======
+ static int nvme_fill_device_id_eui64(struct nvme_ns *ns, struct sg_io_hdr *hdr,
+ 		u8 *inq_response, int alloc_len)
+ {
+ 	struct nvme_id_ns *id_ns;
+ 	int nvme_sc, res;
+ 	size_t len;
+ 	void *eui;
+ 
+ 	nvme_sc = nvme_identify_ns(ns->ctrl, ns->ns_id, &id_ns);
+ 	res = nvme_trans_status_code(hdr, nvme_sc);
+ 	if (res)
+ 		return res;
+ 
+ 	eui = id_ns->eui64;
+ 	len = sizeof(id_ns->eui64);
+ 
+ 	if (ns->ctrl->vs >= NVME_VS(1, 2)) {
+ 		if (bitmap_empty(eui, len * 8)) {
+ 			eui = id_ns->nguid;
+ 			len = sizeof(id_ns->nguid);
+ 		}
+ 	}
+ 
+ 	if (bitmap_empty(eui, len * 8)) {
+ 		res = -EOPNOTSUPP;
+ 		goto out_free_id;
+ 	}
+ 
+ 	memset(inq_response, 0, alloc_len);
+ 	inq_response[1] = INQ_DEVICE_IDENTIFICATION_PAGE;
+ 	inq_response[3] = 4 + len; /* Page Length */
+ 
+ 	/* Designation Descriptor start */
+ 	inq_response[4] = 0x01;	/* Proto ID=0h | Code set=1h */
+ 	inq_response[5] = 0x02;	/* PIV=0b | Asso=00b | Designator Type=2h */
+ 	inq_response[6] = 0x00;	/* Rsvd */
+ 	inq_response[7] = len;	/* Designator Length */
+ 	memcpy(&inq_response[8], eui, len);
+ 
+ 	res = nvme_trans_copy_to_user(hdr, inq_response, alloc_len);
+ out_free_id:
+ 	kfree(id_ns);
+ 	return res;
+ }
+ 
+ static int nvme_fill_device_id_scsi_string(struct nvme_ns *ns,
+ 		struct sg_io_hdr *hdr, u8 *inq_response, int alloc_len)
+ {
+ 	struct nvme_ctrl *ctrl = ns->ctrl;
+ 	struct nvme_id_ctrl *id_ctrl;
+ 	int nvme_sc, res;
+ 
+ 	if (alloc_len < 72) {
+ 		return nvme_trans_completion(hdr,
+ 				SAM_STAT_CHECK_CONDITION,
+ 				ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
+ 				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
+ 	}
+ 
+ 	nvme_sc = nvme_identify_ctrl(ctrl, &id_ctrl);
+ 	res = nvme_trans_status_code(hdr, nvme_sc);
+ 	if (res)
+ 		return res;
+ 
+ 	memset(inq_response, 0, alloc_len);
+ 	inq_response[1] = INQ_DEVICE_IDENTIFICATION_PAGE;
+ 	inq_response[3] = 0x48;	/* Page Length */
+ 
+ 	/* Designation Descriptor start */
+ 	inq_response[4] = 0x03;	/* Proto ID=0h | Code set=3h */
+ 	inq_response[5] = 0x08;	/* PIV=0b | Asso=00b | Designator Type=8h */
+ 	inq_response[6] = 0x00;	/* Rsvd */
+ 	inq_response[7] = 0x44;	/* Designator Length */
+ 
+ 	sprintf(&inq_response[8], "%04x", le16_to_cpu(id_ctrl->vid));
+ 	memcpy(&inq_response[12], ctrl->model, sizeof(ctrl->model));
+ 	sprintf(&inq_response[52], "%04x", cpu_to_be32(ns->ns_id));
+ 	memcpy(&inq_response[56], ctrl->serial, sizeof(ctrl->serial));
+ 
+ 	res = nvme_trans_copy_to_user(hdr, inq_response, alloc_len);
+ 	kfree(id_ctrl);
+ 	return res;
+ }
+ 
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/scsi.c
  static int nvme_trans_device_id_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 -					u8 *resp, int alloc_len)
 +					u8 *inq_response, int alloc_len)
  {
 +	struct nvme_dev *dev = ns->dev;
 +	dma_addr_t dma_addr;
 +	void *mem;
  	int res;
++<<<<<<< HEAD:drivers/block/nvme-scsi.c
 +	int nvme_sc;
 +	int xfer_len;
 +	__be32 tmp_id = cpu_to_be32(ns->ns_id);
 +
 +	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
 +					&dma_addr, GFP_KERNEL);
 +	if (mem == NULL) {
 +		res = -ENOMEM;
 +		goto out_dma;
++=======
+ 
+ 	if (ns->ctrl->vs >= NVME_VS(1, 1)) {
+ 		res = nvme_fill_device_id_eui64(ns, hdr, resp, alloc_len);
+ 		if (res != -EOPNOTSUPP)
+ 			return res;
++>>>>>>> f3ca80fc11c3 (nvme: move chardev and sysfs interface to common code):drivers/nvme/host/scsi.c
  	}
  
 -	return nvme_fill_device_id_scsi_string(ns, hdr, resp, alloc_len);
 +	memset(inq_response, 0, alloc_len);
 +	inq_response[1] = INQ_DEVICE_IDENTIFICATION_PAGE;    /* Page Code */
 +	if (readl(&dev->bar->vs) >= NVME_VS(1, 1)) {
 +		struct nvme_id_ns *id_ns = mem;
 +		void *eui = id_ns->eui64;
 +		int len = sizeof(id_ns->eui64);
 +
 +		nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
 +		res = nvme_trans_status_code(hdr, nvme_sc);
 +		if (res)
 +			goto out_free;
 +
 +		if (readl(&dev->bar->vs) >= NVME_VS(1, 2)) {
 +			if (bitmap_empty(eui, len * 8)) {
 +				eui = id_ns->nguid;
 +				len = sizeof(id_ns->nguid);
 +			}
 +		}
 +		if (bitmap_empty(eui, len * 8))
 +			goto scsi_string;
 +
 +		inq_response[3] = 4 + len; /* Page Length */
 +		/* Designation Descriptor start */
 +		inq_response[4] = 0x01;    /* Proto ID=0h | Code set=1h */
 +		inq_response[5] = 0x02;    /* PIV=0b | Asso=00b | Designator Type=2h */
 +		inq_response[6] = 0x00;    /* Rsvd */
 +		inq_response[7] = len;     /* Designator Length */
 +		memcpy(&inq_response[8], eui, len);
 +	} else {
 + scsi_string:
 +		if (alloc_len < 72) {
 +			res = nvme_trans_completion(hdr,
 +					SAM_STAT_CHECK_CONDITION,
 +					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
 +					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
 +			goto out_free;
 +		}
 +		inq_response[3] = 0x48;    /* Page Length */
 +		/* Designation Descriptor start */
 +		inq_response[4] = 0x03;    /* Proto ID=0h | Code set=3h */
 +		inq_response[5] = 0x08;    /* PIV=0b | Asso=00b | Designator Type=8h */
 +		inq_response[6] = 0x00;    /* Rsvd */
 +		inq_response[7] = 0x44;    /* Designator Length */
 +
 +		sprintf(&inq_response[8], "%04x", dev->pci_dev->vendor);
 +		memcpy(&inq_response[12], dev->model, sizeof(dev->model));
 +		sprintf(&inq_response[52], "%04x", tmp_id);
 +		memcpy(&inq_response[56], dev->serial, sizeof(dev->serial));
 +	}
 +	xfer_len = alloc_len;
 +	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
 +
 + out_free:
 +	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
 +			  dma_addr);
 + out_dma:
 +	return res;
  }
  
  static int nvme_trans_ext_inq_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
* Unmerged path drivers/block/nvme-core.c
* Unmerged path drivers/block/nvme-scsi.c
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
