net: adjust napi_consume_skb to handle non-NAPI callers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [net] adjust napi_consume_skb to handle non-NAPI callers (Ivan Vecera) [1268334]
Rebuild_FUZZ: 95.24%
commit-author Jesper Dangaard Brouer <brouer@redhat.com>
commit 885eb0a516e4d686849b91c5a1ba25c70b7a6540
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/885eb0a5.failed

Some drivers reuse/share code paths that free SKBs between NAPI
and non-NAPI calls. Adjust napi_consume_skb to handle this
use-case.

Before, calls from netpoll (w/ IRQs disabled) was handled and
indicated with a budget zero indication.  Use the same zero
indication to handle calls not originating from NAPI/softirq.
Simply handled by using dev_consume_skb_any().

This adds an extra branch+call for the netpoll case (checking
in_irq() + irqs_disabled()), but that is okay as this is a slowpath.

	Suggested-by: Alexander Duyck <aduyck@mirantis.com>
	Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 885eb0a516e4d686849b91c5a1ba25c70b7a6540)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skbuff.c
diff --cc net/core/skbuff.c
index fc02ef9734c7,f044f970f1a6..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -831,6 -757,80 +831,83 @@@ void consume_skb(struct sk_buff *skb
  }
  EXPORT_SYMBOL(consume_skb);
  
++<<<<<<< HEAD
++=======
+ void __kfree_skb_flush(void)
+ {
+ 	struct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);
+ 
+ 	/* flush skb_cache if containing objects */
+ 	if (nc->skb_count) {
+ 		kmem_cache_free_bulk(skbuff_head_cache, nc->skb_count,
+ 				     nc->skb_cache);
+ 		nc->skb_count = 0;
+ 	}
+ }
+ 
+ static inline void _kfree_skb_defer(struct sk_buff *skb)
+ {
+ 	struct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);
+ 
+ 	/* drop skb->head and call any destructors for packet */
+ 	skb_release_all(skb);
+ 
+ 	/* record skb to CPU local list */
+ 	nc->skb_cache[nc->skb_count++] = skb;
+ 
+ #ifdef CONFIG_SLUB
+ 	/* SLUB writes into objects when freeing */
+ 	prefetchw(skb);
+ #endif
+ 
+ 	/* flush skb_cache if it is filled */
+ 	if (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {
+ 		kmem_cache_free_bulk(skbuff_head_cache, NAPI_SKB_CACHE_SIZE,
+ 				     nc->skb_cache);
+ 		nc->skb_count = 0;
+ 	}
+ }
+ void __kfree_skb_defer(struct sk_buff *skb)
+ {
+ 	_kfree_skb_defer(skb);
+ }
+ 
+ void napi_consume_skb(struct sk_buff *skb, int budget)
+ {
+ 	if (unlikely(!skb))
+ 		return;
+ 
+ 	/* Zero budget indicate non-NAPI context called us, like netpoll */
+ 	if (unlikely(!budget)) {
+ 		dev_consume_skb_any(skb);
+ 		return;
+ 	}
+ 
+ 	if (likely(atomic_read(&skb->users) == 1))
+ 		smp_rmb();
+ 	else if (likely(!atomic_dec_and_test(&skb->users)))
+ 		return;
+ 	/* if reaching here SKB is ready to free */
+ 	trace_consume_skb(skb);
+ 
+ 	/* if SKB is a clone, don't handle this case */
+ 	if (unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE)) {
+ 		__kfree_skb(skb);
+ 		return;
+ 	}
+ 
+ 	_kfree_skb_defer(skb);
+ }
+ EXPORT_SYMBOL(napi_consume_skb);
+ 
+ /* Make sure a field is enclosed inside headers_start/headers_end section */
+ #define CHECK_SKB_FIELD(field) \
+ 	BUILD_BUG_ON(offsetof(struct sk_buff, field) <		\
+ 		     offsetof(struct sk_buff, headers_start));	\
+ 	BUILD_BUG_ON(offsetof(struct sk_buff, field) >		\
+ 		     offsetof(struct sk_buff, headers_end));	\
+ 
++>>>>>>> 885eb0a516e4 (net: adjust napi_consume_skb to handle non-NAPI callers)
  static void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)
  {
  	new->tstamp		= old->tstamp;
* Unmerged path net/core/skbuff.c
