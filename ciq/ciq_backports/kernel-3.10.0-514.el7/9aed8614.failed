mm/memory.c: don't forget to set softdirty on file mapped fault

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] memory.c: don't forget to set softdirty on file mapped fault (Oleg Nesterov) [1269561]
Rebuild_FUZZ: 97.56%
commit-author Cyrill Gorcunov <gorcunov@gmail.com>
commit 9aed8614af5a05cdaa32a0b78b0f1a424754a958
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/9aed8614.failed

Otherwise we may not notice that pte was softdirty because
pte_mksoft_dirty helper _returns_ new pte but doesn't modify the
argument.

In case if page fault happend on dirty filemapping the newly created pte
may loose softdirty bit thus if a userspace program is tracking memory
changes with help of a memory tracker (CONFIG_MEM_SOFT_DIRTY) it might
miss modification of a memory page (which in worts case may lead to data
inconsistency).

	Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
	Acked-by: Pavel Emelyanov <xemul@parallels.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 9aed8614af5a05cdaa32a0b78b0f1a424754a958)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,7e131325bdf8..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3582,128 -2715,304 +3582,140 @@@ static int __do_fault(struct mm_struct 
  	else
  		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
  
 -	*page = vmf.page;
 -	return ret;
 -}
 +	/*
 +	 * Should we do an early C-O-W break?
 +	 */
 +	page = vmf.page;
 +	if (flags & FAULT_FLAG_WRITE) {
 +		if (!(vma->vm_flags & VM_SHARED)) {
 +			page = cow_page;
 +			anon = 1;
 +			copy_user_highpage(page, vmf.page, address, vma);
 +			__SetPageUptodate(page);
 +		} else {
 +			/*
 +			 * If the page will be shareable, see if the backing
 +			 * address space wants to know that the page is about
 +			 * to become writable
 +			 */
 +			if (vma->vm_ops->page_mkwrite) {
 +				int tmp;
 +
 +				unlock_page(page);
 +				vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
 +				tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
 +				if (unlikely(tmp &
 +					  (VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
 +					ret = tmp;
 +					goto unwritable_page;
 +				}
 +				if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
 +					lock_page(page);
 +					if (!page->mapping) {
 +						ret = 0; /* retry the fault */
 +						unlock_page(page);
 +						goto unwritable_page;
 +					}
 +				} else
 +					VM_BUG_ON_PAGE(!PageLocked(page), page);
 +				page_mkwrite = 1;
 +			}
 +		}
  
 -/**
 - * do_set_pte - setup new PTE entry for given page and add reverse page mapping.
 - *
 - * @vma: virtual memory area
 - * @address: user virtual address
 - * @page: page to map
 - * @pte: pointer to target page table entry
 - * @write: true, if new entry is writable
 - * @anon: true, if it's anonymous page
 - *
 - * Caller must hold page table lock relevant for @pte.
 - *
 - * Target users are page handler itself and implementations of
 - * vm_ops->map_pages.
 - */
 -void do_set_pte(struct vm_area_struct *vma, unsigned long address,
 -		struct page *page, pte_t *pte, bool write, bool anon)
 -{
 -	pte_t entry;
++<<<<<<< HEAD
 +	}
 +
 +	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
  
 +	/*
 +	 * This silly early PAGE_DIRTY setting removes a race
 +	 * due to the bad i386 page protection. But it's valid
 +	 * for other architectures too.
 +	 *
 +	 * Note that if FAULT_FLAG_WRITE is set, we either now have
 +	 * an exclusive copy of the page, or this is a shared mapping,
 +	 * so we can make it writable and dirty to avoid having to
 +	 * handle that later.
 +	 */
 +	/* Only go through if we didn't race with anybody else... */
 +	if (likely(pte_same(*page_table, orig_pte))) {
 +		flush_icache_page(vma, page);
 +		entry = mk_pte(page, vma->vm_page_prot);
 +		if (flags & FAULT_FLAG_WRITE)
 +			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +		if (anon) {
 +			inc_mm_counter_fast(mm, MM_ANONPAGES);
 +			page_add_new_anon_rmap(page, vma, address);
 +		} else {
 +			inc_mm_counter_fast(mm, MM_FILEPAGES);
 +			page_add_file_rmap(page);
 +			if (flags & FAULT_FLAG_WRITE) {
 +				dirty_page = page;
 +				get_page(dirty_page);
 +			}
 +		}
 +		set_pte_at(mm, address, page_table, entry);
 +
 +		/* no need to invalidate: a not-present page won't be cached */
 +		update_mmu_cache(vma, address, page_table);
++=======
+ 	flush_icache_page(vma, page);
+ 	entry = mk_pte(page, vma->vm_page_prot);
+ 	if (write)
+ 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	else if (pte_file(*pte) && pte_file_soft_dirty(*pte))
+ 		entry = pte_mksoft_dirty(entry);
+ 	if (anon) {
+ 		inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
+ 		page_add_new_anon_rmap(page, vma, address);
++>>>>>>> 9aed8614af5a (mm/memory.c: don't forget to set softdirty on file mapped fault)
  	} else {
 -		inc_mm_counter_fast(vma->vm_mm, MM_FILEPAGES);
 -		page_add_file_rmap(page);
 +		if (cow_page)
 +			mem_cgroup_uncharge_page(cow_page);
 +		if (anon)
 +			page_cache_release(page);
 +		else
 +			anon = 1; /* no anon but release faulted_page */
  	}
 -	set_pte_at(vma->vm_mm, address, pte, entry);
 -
 -	/* no need to invalidate: a not-present page won't be cached */
 -	update_mmu_cache(vma, address, pte);
 -}
 -
 -static unsigned long fault_around_bytes = rounddown_pow_of_two(65536);
 -
 -static inline unsigned long fault_around_pages(void)
 -{
 -	return fault_around_bytes >> PAGE_SHIFT;
 -}
 -
 -static inline unsigned long fault_around_mask(void)
 -{
 -	return ~(fault_around_bytes - 1) & PAGE_MASK;
 -}
 -
 -#ifdef CONFIG_DEBUG_FS
 -static int fault_around_bytes_get(void *data, u64 *val)
 -{
 -	*val = fault_around_bytes;
 -	return 0;
 -}
 -
 -/*
 - * fault_around_pages() and fault_around_mask() expects fault_around_bytes
 - * rounded down to nearest page order. It's what do_fault_around() expects to
 - * see.
 - */
 -static int fault_around_bytes_set(void *data, u64 val)
 -{
 -	if (val / PAGE_SIZE > PTRS_PER_PTE)
 -		return -EINVAL;
 -	if (val > PAGE_SIZE)
 -		fault_around_bytes = rounddown_pow_of_two(val);
 -	else
 -		fault_around_bytes = PAGE_SIZE; /* rounddown_pow_of_two(0) is undefined */
 -	return 0;
 -}
 -DEFINE_SIMPLE_ATTRIBUTE(fault_around_bytes_fops,
 -		fault_around_bytes_get, fault_around_bytes_set, "%llu\n");
 -
 -static int __init fault_around_debugfs(void)
 -{
 -	void *ret;
 -
 -	ret = debugfs_create_file("fault_around_bytes", 0644, NULL, NULL,
 -			&fault_around_bytes_fops);
 -	if (!ret)
 -		pr_warn("Failed to create fault_around_bytes in debugfs");
 -	return 0;
 -}
 -late_initcall(fault_around_debugfs);
 -#endif
  
 -/*
 - * do_fault_around() tries to map few pages around the fault address. The hope
 - * is that the pages will be needed soon and this will lower the number of
 - * faults to handle.
 - *
 - * It uses vm_ops->map_pages() to map the pages, which skips the page if it's
 - * not ready to be mapped: not up-to-date, locked, etc.
 - *
 - * This function is called with the page table lock taken. In the split ptlock
 - * case the page table lock only protects only those entries which belong to
 - * the page table corresponding to the fault address.
 - *
 - * This function doesn't cross the VMA boundaries, in order to call map_pages()
 - * only once.
 - *
 - * fault_around_pages() defines how many pages we'll try to map.
 - * do_fault_around() expects it to return a power of two less than or equal to
 - * PTRS_PER_PTE.
 - *
 - * The virtual address of the area that we map is naturally aligned to the
 - * fault_around_pages() value (and therefore to page order).  This way it's
 - * easier to guarantee that we don't cross page table boundaries.
 - */
 -static void do_fault_around(struct vm_area_struct *vma, unsigned long address,
 -		pte_t *pte, pgoff_t pgoff, unsigned int flags)
 -{
 -	unsigned long start_addr;
 -	pgoff_t max_pgoff;
 -	struct vm_fault vmf;
 -	int off;
 -
 -	start_addr = max(address & fault_around_mask(), vma->vm_start);
 -	off = ((address - start_addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
 -	pte -= off;
 -	pgoff -= off;
 -
 -	/*
 -	 *  max_pgoff is either end of page table or end of vma
 -	 *  or fault_around_pages() from pgoff, depending what is nearest.
 -	 */
 -	max_pgoff = pgoff - ((start_addr >> PAGE_SHIFT) & (PTRS_PER_PTE - 1)) +
 -		PTRS_PER_PTE - 1;
 -	max_pgoff = min3(max_pgoff, vma_pages(vma) + vma->vm_pgoff - 1,
 -			pgoff + fault_around_pages() - 1);
 -
 -	/* Check if it makes any sense to call ->map_pages */
 -	while (!pte_none(*pte)) {
 -		if (++pgoff > max_pgoff)
 -			return;
 -		start_addr += PAGE_SIZE;
 -		if (start_addr >= vma->vm_end)
 -			return;
 -		pte++;
 -	}
 +	pte_unmap_unlock(page_table, ptl);
  
 -	vmf.virtual_address = (void __user *) start_addr;
 -	vmf.pte = pte;
 -	vmf.pgoff = pgoff;
 -	vmf.max_pgoff = max_pgoff;
 -	vmf.flags = flags;
 -	vma->vm_ops->map_pages(vma, &vmf);
 -}
 +	if (dirty_page) {
 +		struct address_space *mapping = page->mapping;
 +		int dirtied = 0;
  
 -static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 -		unsigned long address, pmd_t *pmd,
 -		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
 -{
 -	struct page *fault_page;
 -	spinlock_t *ptl;
 -	pte_t *pte;
 -	int ret = 0;
 +		if (set_page_dirty(dirty_page))
 +			dirtied = 1;
 +		unlock_page(dirty_page);
 +		put_page(dirty_page);
 +		if ((dirtied || page_mkwrite) && mapping) {
 +			/*
 +			 * Some device drivers do not set page.mapping but still
 +			 * dirty their pages
 +			 */
 +			balance_dirty_pages_ratelimited(mapping);
 +		}
  
 -	/*
 -	 * Let's call ->map_pages() first and use ->fault() as fallback
 -	 * if page by the offset is not ready to be mapped (cold cache or
 -	 * something).
 -	 */
 -	if (vma->vm_ops->map_pages && !(flags & FAULT_FLAG_NONLINEAR) &&
 -	    fault_around_pages() > 1) {
 -		pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 -		do_fault_around(vma, address, pte, pgoff, flags);
 -		if (!pte_same(*pte, orig_pte))
 -			goto unlock_out;
 -		pte_unmap_unlock(pte, ptl);
 +		/* file_update_time outside page_lock */
 +		if (vma->vm_file && !page_mkwrite)
 +			file_update_time(vma->vm_file);
 +	} else {
 +		unlock_page(vmf.page);
 +		if (anon)
 +			page_cache_release(vmf.page);
  	}
  
 -	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
 -	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 -		return ret;
 -
 -	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 -	if (unlikely(!pte_same(*pte, orig_pte))) {
 -		pte_unmap_unlock(pte, ptl);
 -		unlock_page(fault_page);
 -		page_cache_release(fault_page);
 -		return ret;
 -	}
 -	do_set_pte(vma, address, fault_page, pte, false, false);
 -	unlock_page(fault_page);
 -unlock_out:
 -	pte_unmap_unlock(pte, ptl);
  	return ret;
 -}
  
 -static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 -		unsigned long address, pmd_t *pmd,
 -		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
 -{
 -	struct page *fault_page, *new_page;
 -	spinlock_t *ptl;
 -	pte_t *pte;
 -	int ret;
 -
 -	if (unlikely(anon_vma_prepare(vma)))
 -		return VM_FAULT_OOM;
 -
 -	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
 -	if (!new_page)
 -		return VM_FAULT_OOM;
 -
 -	if (mem_cgroup_charge_anon(new_page, mm, GFP_KERNEL)) {
 -		page_cache_release(new_page);
 -		return VM_FAULT_OOM;
 -	}
 -
 -	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
 -	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 -		goto uncharge_out;
 -
 -	copy_user_highpage(new_page, fault_page, address, vma);
 -	__SetPageUptodate(new_page);
 -
 -	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 -	if (unlikely(!pte_same(*pte, orig_pte))) {
 -		pte_unmap_unlock(pte, ptl);
 -		unlock_page(fault_page);
 -		page_cache_release(fault_page);
 -		goto uncharge_out;
 -	}
 -	do_set_pte(vma, address, new_page, pte, true, true);
 -	pte_unmap_unlock(pte, ptl);
 -	unlock_page(fault_page);
 -	page_cache_release(fault_page);
 +unwritable_page:
 +	page_cache_release(page);
  	return ret;
  uncharge_out:
 -	mem_cgroup_uncharge_page(new_page);
 -	page_cache_release(new_page);
 -	return ret;
 -}
 -
 -static int do_shared_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 -		unsigned long address, pmd_t *pmd,
 -		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
 -{
 -	struct page *fault_page;
 -	struct address_space *mapping;
 -	spinlock_t *ptl;
 -	pte_t *pte;
 -	int dirtied = 0;
 -	int ret, tmp;
 -
 -	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
 -	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 -		return ret;
 -
 -	/*
 -	 * Check if the backing address space wants to know that the page is
 -	 * about to become writable
 -	 */
 -	if (vma->vm_ops->page_mkwrite) {
 -		unlock_page(fault_page);
 -		tmp = do_page_mkwrite(vma, fault_page, address);
 -		if (unlikely(!tmp ||
 -				(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
 -			page_cache_release(fault_page);
 -			return tmp;
 -		}
 -	}
 -
 -	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 -	if (unlikely(!pte_same(*pte, orig_pte))) {
 -		pte_unmap_unlock(pte, ptl);
 -		unlock_page(fault_page);
 -		page_cache_release(fault_page);
 -		return ret;
 +	/* fs's fault handler get error */
 +	if (cow_page) {
 +		mem_cgroup_uncharge_page(cow_page);
 +		page_cache_release(cow_page);
  	}
 -	do_set_pte(vma, address, fault_page, pte, true, false);
 -	pte_unmap_unlock(pte, ptl);
 -
 -	if (set_page_dirty(fault_page))
 -		dirtied = 1;
 -	mapping = fault_page->mapping;
 -	unlock_page(fault_page);
 -	if ((dirtied || vma->vm_ops->page_mkwrite) && mapping) {
 -		/*
 -		 * Some device drivers do not set page.mapping but still
 -		 * dirty their pages
 -		 */
 -		balance_dirty_pages_ratelimited(mapping);
 -	}
 -
 -	/* file_update_time outside page_lock */
 -	if (vma->vm_file && !vma->vm_ops->page_mkwrite)
 -		file_update_time(vma->vm_file);
 -
  	return ret;
  }
  
* Unmerged path mm/memory.c
