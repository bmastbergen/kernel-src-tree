perf/x86/intel/rapl: Make PMU lock raw

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit a208749c642618b6c106874153906b53f8e2ded9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a208749c.failed

This lock is taken in hard interrupt context even on Preempt-RT. Make it raw
so RT does not have to patch it.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andi Kleen <andi.kleen@intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Harish Chegondi <harish.chegondi@intel.com>
	Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Kan Liang <kan.liang@intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Cc: linux-kernel@vger.kernel.org
Link: http://lkml.kernel.org/r/20160222221012.669411833@linutronix.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit a208749c642618b6c106874153906b53f8e2ded9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel_rapl.c
diff --cc arch/x86/kernel/cpu/perf_event_intel_rapl.c
index c019c57572a7,29349990e11c..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel_rapl.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_rapl.c
@@@ -123,15 -120,16 +123,24 @@@ static struct perf_pmu_events_attr even
  };
  
  struct rapl_pmu {
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_rapl.c
 +	spinlock_t	 lock;
 +	int		 n_active; /* number of active events */
 +	struct list_head active_list;
 +	struct pmu	 *pmu; /* pointer to rapl_pmu_class */
 +	ktime_t		 timer_interval; /* in ktime_t unit */
 +	struct hrtimer   hrtimer;
++=======
+ 	raw_spinlock_t		lock;
+ 	int			n_active;
+ 	struct list_head	active_list;
+ 	struct pmu		*pmu;
+ 	ktime_t			timer_interval;
+ 	struct hrtimer		hrtimer;
++>>>>>>> a208749c6426 (perf/x86/intel/rapl: Make PMU lock raw):arch/x86/events/intel/rapl.c
  };
  
 - /* 1/2^hw_unit Joule */
 -static int rapl_hw_unit[NR_RAPL_DOMAINS] __read_mostly;
 +static int rapl_hw_unit[NR_RAPL_DOMAINS] __read_mostly;  /* 1/2^hw_unit Joule */
  static struct pmu rapl_pmu_class;
  static cpumask_t rapl_cpu_mask;
  static int rapl_cntr_mask;
@@@ -226,13 -210,12 +235,13 @@@ static enum hrtimer_restart rapl_hrtime
  	if (!pmu->n_active)
  		return HRTIMER_NORESTART;
  
- 	spin_lock_irqsave(&pmu->lock, flags);
+ 	raw_spin_lock_irqsave(&pmu->lock, flags);
  
 -	list_for_each_entry(event, &pmu->active_list, active_entry)
 +	list_for_each_entry(event, &pmu->active_list, active_entry) {
  		rapl_event_update(event);
 +	}
  
- 	spin_unlock_irqrestore(&pmu->lock, flags);
+ 	raw_spin_unlock_irqrestore(&pmu->lock, flags);
  
  	hrtimer_forward_now(hrtimer, pmu->timer_interval);
  
@@@ -266,12 -249,12 +275,12 @@@ static void __rapl_pmu_event_start(stru
  
  static void rapl_pmu_event_start(struct perf_event *event, int mode)
  {
 -	struct rapl_pmu *pmu = __this_cpu_read(rapl_pmu);
 +	struct rapl_pmu *pmu = __get_cpu_var(rapl_pmu);
  	unsigned long flags;
  
- 	spin_lock_irqsave(&pmu->lock, flags);
+ 	raw_spin_lock_irqsave(&pmu->lock, flags);
  	__rapl_pmu_event_start(pmu, event);
- 	spin_unlock_irqrestore(&pmu->lock, flags);
+ 	raw_spin_unlock_irqrestore(&pmu->lock, flags);
  }
  
  static void rapl_pmu_event_stop(struct perf_event *event, int mode)
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_rapl.c
