mm: meminit: move page initialization into a separate function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] meminit: move page initialization into a separate function (George Beshers) [727269]
Rebuild_FUZZ: 96.67%
commit-author Robin Holt <holt@sgi.com>
commit 1e8ce83cd17fd0f549a7ad145ddd2bfcdd7dfe37
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/1e8ce83c.failed

Currently, memmap_init_zone() has all the smarts for initializing a single
page.  A subset of this is required for parallel page initialisation and
so this patch breaks up the monolithic function in preparation.

	Signed-off-by: Robin Holt <holt@sgi.com>
	Signed-off-by: Nathan Zimmer <nzimmer@sgi.com>
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Nate Zimmer <nzimmer@sgi.com>
	Tested-by: Waiman Long <waiman.long@hp.com>
	Tested-by: Daniel J Blueman <daniel@numascale.com>
	Acked-by: Pekka Enberg <penberg@kernel.org>
	Cc: Robin Holt <robinmholt@gmail.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Waiman Long <waiman.long@hp.com>
	Cc: Scott Norton <scott.norton@hp.com>
	Cc: "Luck, Tony" <tony.luck@intel.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1e8ce83cd17fd0f549a7ad145ddd2bfcdd7dfe37)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index f60ded95bce9,bc5da2cdfc84..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -719,10 -749,73 +719,73 @@@ static void free_one_page(struct zone *
  	spin_unlock(&zone->lock);
  }
  
++<<<<<<< HEAD
++=======
+ static int free_tail_pages_check(struct page *head_page, struct page *page)
+ {
+ 	if (!IS_ENABLED(CONFIG_DEBUG_VM))
+ 		return 0;
+ 	if (unlikely(!PageTail(page))) {
+ 		bad_page(page, "PageTail not set", 0);
+ 		return 1;
+ 	}
+ 	if (unlikely(page->first_page != head_page)) {
+ 		bad_page(page, "first_page not consistent", 0);
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
+ 				unsigned long zone, int nid)
+ {
+ 	struct zone *z = &NODE_DATA(nid)->node_zones[zone];
+ 
+ 	set_page_links(page, zone, nid, pfn);
+ 	mminit_verify_page_links(page, zone, nid, pfn);
+ 	init_page_count(page);
+ 	page_mapcount_reset(page);
+ 	page_cpupid_reset_last(page);
+ 	SetPageReserved(page);
+ 
+ 	/*
+ 	 * Mark the block movable so that blocks are reserved for
+ 	 * movable at startup. This will force kernel allocations
+ 	 * to reserve their blocks rather than leaking throughout
+ 	 * the address space during boot when many long-lived
+ 	 * kernel allocations are made. Later some blocks near
+ 	 * the start are marked MIGRATE_RESERVE by
+ 	 * setup_zone_migrate_reserve()
+ 	 *
+ 	 * bitmap is created for zone's valid pfn range. but memmap
+ 	 * can be created for invalid pages (for alignment)
+ 	 * check here not to call set_pageblock_migratetype() against
+ 	 * pfn out of zone.
+ 	 */
+ 	if ((z->zone_start_pfn <= pfn)
+ 	    && (pfn < zone_end_pfn(z))
+ 	    && !(pfn & (pageblock_nr_pages - 1)))
+ 		set_pageblock_migratetype(page, MIGRATE_MOVABLE);
+ 
+ 	INIT_LIST_HEAD(&page->lru);
+ #ifdef WANT_PAGE_VIRTUAL
+ 	/* The shift won't overflow because ZONE_NORMAL is below 4G. */
+ 	if (!is_highmem_idx(zone))
+ 		set_page_address(page, __va(pfn << PAGE_SHIFT));
+ #endif
+ }
+ 
+ static void __meminit __init_single_pfn(unsigned long pfn, unsigned long zone,
+ 					int nid)
+ {
+ 	return __init_single_page(pfn_to_page(pfn), pfn, zone, nid);
+ }
+ 
++>>>>>>> 1e8ce83cd17f (mm: meminit: move page initialization into a separate function)
  static bool free_pages_prepare(struct page *page, unsigned int order)
  {
 -	bool compound = PageCompound(page);
 -	int i, bad = 0;
 -
 -	VM_BUG_ON_PAGE(PageTail(page), page);
 -	VM_BUG_ON_PAGE(compound && compound_order(page) != order, page);
 +	int i;
 +	int bad = 0;
  
  	trace_mm_page_free(page, order);
  	kmemcheck_free_shadow(page, order);
* Unmerged path mm/page_alloc.c
