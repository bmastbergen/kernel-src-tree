mm: memory-hotplug: enable memory hotplug to handle hugepage

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] memory-hotplug: enable memory hotplug to handle hugepage (Tomoaki Nishimura) [1287322]
Rebuild_FUZZ: 96.55%
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit c8721bbbdd36382de51cd6b7a56322e0acca2414
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/c8721bbb.failed

Until now we can't offline memory blocks which contain hugepages because a
hugepage is considered as an unmovable page.  But now with this patch
series, a hugepage has become movable, so by using hugepage migration we
can offline such memory blocks.

What's different from other users of hugepage migration is that we need to
decompose all the hugepages inside the target memory block into free buddy
pages after hugepage migration, because otherwise free hugepages remaining
in the memory block intervene the memory offlining.  For this reason we
introduce new functions dissolve_free_huge_page() and
dissolve_free_huge_pages().

Other than that, what this patch does is straightforwardly to add hugepage
migration code, that is, adding hugepage code to the functions which scan
over pfn and collect hugepages to be migrated, and adding a hugepage
allocation function to alloc_migrate_target().

As for larger hugepages (1GB for x86_64), it's not easy to do hotremove
over them because it's larger than memory block.  So we now simply leave
it to fail as it is.

[yongjun_wei@trendmicro.com.cn: remove duplicated include]
	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Acked-by: Andi Kleen <ak@linux.intel.com>
	Cc: Hillf Danton <dhillf@gmail.com>
	Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c8721bbbdd36382de51cd6b7a56322e0acca2414)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hugetlb.h
#	mm/hugetlb.c
#	mm/memory_hotplug.c
diff --cc include/linux/hugetlb.h
index f71f104feba3,2e02c4ed1035..000000000000
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@@ -79,9 -66,10 +79,14 @@@ int hugetlb_reserve_pages(struct inode 
  						vm_flags_t vm_flags);
  void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);
  int dequeue_hwpoisoned_huge_page(struct page *page);
 +void free_huge_page(struct page *page);
  bool isolate_huge_page(struct page *page, struct list_head *list);
  void putback_active_hugepage(struct page *page);
++<<<<<<< HEAD
++=======
+ bool is_hugepage_active(struct page *page);
+ void copy_huge_page(struct page *dst, struct page *src);
++>>>>>>> c8721bbbdd36 (mm: memory-hotplug: enable memory hotplug to handle hugepage)
  
  #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE
  pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud);
@@@ -157,6 -139,11 +162,14 @@@ static inline int dequeue_hwpoisoned_hu
  
  #define isolate_huge_page(p, l) false
  #define putback_active_hugepage(p)	do {} while (0)
++<<<<<<< HEAD
++=======
+ #define is_hugepage_active(x)	false
+ static inline void copy_huge_page(struct page *dst, struct page *src)
+ {
+ }
+ 
++>>>>>>> c8721bbbdd36 (mm: memory-hotplug: enable memory hotplug to handle hugepage)
  static inline unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
  		unsigned long address, unsigned long end, pgprot_t newprot)
  {
@@@ -398,24 -379,8 +411,29 @@@ static inline pgoff_t basepage_index(st
  	return __basepage_index(page);
  }
  
++<<<<<<< HEAD
 +static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 +					   struct mm_struct *mm, pte_t *pte)
 +{
 +	if (huge_page_size(h) == PMD_SIZE)
 +		return pmd_lockptr(mm, (pmd_t *) pte);
 +	VM_BUG_ON(huge_page_size(h) == PAGE_SIZE);
 +	return &mm->page_table_lock;
 +}
 +
 +static inline bool hugepages_supported(void)
 +{
 +	/*
 +	 * Some platform decide whether they support huge pages at boot
 +	 * time. On these, such as powerpc, HPAGE_SHIFT is set to 0 when
 +	 * there is no such support
 +	 */
 +	return HPAGE_SHIFT != 0;
 +}
++=======
+ extern void dissolve_free_huge_pages(unsigned long start_pfn,
+ 				     unsigned long end_pfn);
++>>>>>>> c8721bbbdd36 (mm: memory-hotplug: enable memory hotplug to handle hugepage)
  
  #else	/* CONFIG_HUGETLB_PAGE */
  struct hstate {};
@@@ -444,22 -408,7 +462,26 @@@ static inline pgoff_t basepage_index(st
  {
  	return page->index;
  }
++<<<<<<< HEAD
 +
 +static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
 +					   struct mm_struct *mm, pte_t *pte)
 +{
 +	return &mm->page_table_lock;
 +}
++=======
+ #define dissolve_free_huge_pages(s, e)	do {} while (0)
++>>>>>>> c8721bbbdd36 (mm: memory-hotplug: enable memory hotplug to handle hugepage)
  #endif	/* CONFIG_HUGETLB_PAGE */
  
 +static inline spinlock_t *huge_pte_lock(struct hstate *h,
 +					struct mm_struct *mm, pte_t *pte)
 +{
 +	spinlock_t *ptl;
 +
 +	ptl = huge_pte_lockptr(h, mm, pte);
 +	spin_lock(ptl);
 +	return ptl;
 +}
 +
  #endif /* _LINUX_HUGETLB_H */
diff --cc mm/hugetlb.c
index 4a192c9e708d,fb4293b93fd0..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -21,7 -21,7 +21,11 @@@
  #include <linux/rmap.h>
  #include <linux/swap.h>
  #include <linux/swapops.h>
++<<<<<<< HEAD
 +#include <linux/jhash.h>
++=======
+ #include <linux/page-isolation.h>
++>>>>>>> c8721bbbdd36 (mm: memory-hotplug: enable memory hotplug to handle hugepage)
  
  #include <asm/page.h>
  #include <asm/pgtable.h>
diff --cc mm/memory_hotplug.c
index 6133e8a87211,0eb1a1df649d..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -30,8 -30,7 +30,12 @@@
  #include <linux/mm_inline.h>
  #include <linux/firmware-map.h>
  #include <linux/stop_machine.h>
++<<<<<<< HEAD
 +#include <linux/memblock.h>
 +#include <linux/bootmem.h>
++=======
+ #include <linux/hugetlb.h>
++>>>>>>> c8721bbbdd36 (mm: memory-hotplug: enable memory hotplug to handle hugepage)
  
  #include <asm/tlbflush.h>
  
* Unmerged path include/linux/hugetlb.h
* Unmerged path mm/hugetlb.c
* Unmerged path mm/memory_hotplug.c
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index d14b967f0ef3..b7747038629c 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -5981,6 +5981,17 @@ bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
 			continue;
 
 		page = pfn_to_page(check);
+
+		/*
+		 * Hugepages are not in LRU lists, but they're movable.
+		 * We need not scan over tail pages bacause we don't
+		 * handle each tail page individually in migration.
+		 */
+		if (PageHuge(page)) {
+			iter = round_up(iter + 1, 1<<compound_order(page)) - 1;
+			continue;
+		}
+
 		/*
 		 * We can't use page_count without pin a page
 		 * because another CPU can free compound page.
diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 383bdbb98b04..229d66fe7674 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -6,6 +6,7 @@
 #include <linux/page-isolation.h>
 #include <linux/pageblock-flags.h>
 #include <linux/memory.h>
+#include <linux/hugetlb.h>
 #include "internal.h"
 
 int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
@@ -252,6 +253,19 @@ struct page *alloc_migrate_target(struct page *page, unsigned long private,
 {
 	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE;
 
+	/*
+	 * TODO: allocate a destination hugepage from a nearest neighbor node,
+	 * accordance with memory policy of the user process if possible. For
+	 * now as a simple work-around, we use the next node for destination.
+	 */
+	if (PageHuge(page)) {
+		nodemask_t src = nodemask_of_node(page_to_nid(page));
+		nodemask_t dst;
+		nodes_complement(dst, src);
+		return alloc_huge_page_node(page_hstate(compound_head(page)),
+					    next_node(page_to_nid(page), dst));
+	}
+
 	if (PageHighMem(page))
 		gfp_mask |= __GFP_HIGHMEM;
 
