mm: meminit: initialise remaining struct pages in parallel with kswapd

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] meminit: initialise remaining struct pages in parallel with kswapd (George Beshers) [727269]
Rebuild_FUZZ: 97.06%
commit-author Mel Gorman <mgorman@suse.de>
commit 7e18adb4f80bea90d30b62158694d97c31f71d37
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/7e18adb4.failed

Only a subset of struct pages are initialised at the moment.  When this
patch is applied kswapd initialise the remaining struct pages in parallel.

This should boot faster by spreading the work to multiple CPUs and
initialising data that is local to the CPU.  The user-visible effect on
large machines is that free memory will appear to rapidly increase early
in the lifetime of the system until kswapd reports that all memory is
initialised in the kernel log.  Once initialised there should be no other
user-visibile effects.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Nate Zimmer <nzimmer@sgi.com>
	Tested-by: Waiman Long <waiman.long@hp.com>
	Tested-by: Daniel J Blueman <daniel@numascale.com>
	Acked-by: Pekka Enberg <penberg@kernel.org>
	Cc: Robin Holt <robinmholt@gmail.com>
	Cc: Nate Zimmer <nzimmer@sgi.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Waiman Long <waiman.long@hp.com>
	Cc: Scott Norton <scott.norton@hp.com>
	Cc: "Luck, Tony" <tony.luck@intel.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 7e18adb4f80bea90d30b62158694d97c31f71d37)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/internal.h
#	mm/mm_init.c
#	mm/page_alloc.c
diff --cc mm/internal.h
index d07fa9595ecf,71d160437205..000000000000
--- a/mm/internal.h
+++ b/mm/internal.h
@@@ -326,6 -387,30 +326,33 @@@ static inline void mminit_verify_zoneli
  }
  #endif /* CONFIG_DEBUG_MEMORY_INIT */
  
++<<<<<<< HEAD
++=======
+ /*
+  * Deferred struct page initialisation requires init functions that are freed
+  * before kswapd is available. Reuse the memory hotplug section annotation
+  * to mark the required code.
+  *
+  * __defermem_init is code that always exists but is annotated __meminit to
+  * 	avoid section warnings.
+  * __defer_init code gets marked __meminit when deferring struct page
+  *	initialistion but is otherwise in the init section.
+  */
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ #define __defermem_init __meminit
+ #define __defer_init    __meminit
+ 
+ void deferred_init_memmap(int nid);
+ #else
+ #define __defermem_init
+ #define __defer_init __init
+ 
+ static inline void deferred_init_memmap(int nid)
+ {
+ }
+ #endif
+ 
++>>>>>>> 7e18adb4f80b (mm: meminit: initialise remaining struct pages in parallel with kswapd)
  /* mminit_validate_memmodel_limits is independent of CONFIG_DEBUG_MEMORY_INIT */
  #if defined(CONFIG_SPARSEMEM)
  extern void mminit_validate_memmodel_limits(unsigned long *start_pfn,
diff --cc mm/mm_init.c
index 12dd27a66518,28fbf87b20aa..000000000000
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@@ -9,6 -9,9 +9,12 @@@
  #include <linux/init.h>
  #include <linux/kobject.h>
  #include <linux/export.h>
++<<<<<<< HEAD
++=======
+ #include <linux/memory.h>
+ #include <linux/notifier.h>
+ #include <linux/sched.h>
++>>>>>>> 7e18adb4f80b (mm: meminit: initialise remaining struct pages in parallel with kswapd)
  #include "internal.h"
  
  #ifdef CONFIG_DEBUG_MEMORY_INIT
diff --cc mm/page_alloc.c
index 20d353397e7d,c30f5a0535fd..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -229,10 -235,81 +229,84 @@@ EXPORT_SYMBOL(nr_online_nodes)
  
  int page_group_by_mobility_disabled __read_mostly;
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static inline void reset_deferred_meminit(pg_data_t *pgdat)
+ {
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ }
+ 
+ /* Returns true if the struct page for the pfn is uninitialised */
+ static inline bool __defermem_init early_page_uninitialised(unsigned long pfn)
+ {
+ 	int nid = early_pfn_to_nid(pfn);
+ 
+ 	if (pfn >= NODE_DATA(nid)->first_deferred_pfn)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static inline bool early_page_nid_uninitialised(unsigned long pfn, int nid)
+ {
+ 	if (pfn >= NODE_DATA(nid)->first_deferred_pfn)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Returns false when the remaining initialisation should be deferred until
+  * later in the boot cycle when it can be parallelised.
+  */
+ static inline bool update_defer_init(pg_data_t *pgdat,
+ 				unsigned long pfn, unsigned long zone_end,
+ 				unsigned long *nr_initialised)
+ {
+ 	/* Always populate low zones for address-contrained allocations */
+ 	if (zone_end < pgdat_end_pfn(pgdat))
+ 		return true;
+ 
+ 	/* Initialise at least 2G of the highest zone */
+ 	(*nr_initialised)++;
+ 	if (*nr_initialised > (2UL << (30 - PAGE_SHIFT)) &&
+ 	    (pfn & (PAGES_PER_SECTION - 1)) == 0) {
+ 		pgdat->first_deferred_pfn = pfn;
+ 		return false;
+ 	}
+ 
+ 	return true;
+ }
+ #else
+ static inline void reset_deferred_meminit(pg_data_t *pgdat)
+ {
+ }
+ 
+ static inline bool early_page_uninitialised(unsigned long pfn)
+ {
+ 	return false;
+ }
+ 
+ static inline bool early_page_nid_uninitialised(unsigned long pfn, int nid)
+ {
+ 	return false;
+ }
+ 
+ static inline bool update_defer_init(pg_data_t *pgdat,
+ 				unsigned long pfn, unsigned long zone_end,
+ 				unsigned long *nr_initialised)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
++>>>>>>> 7e18adb4f80b (mm: meminit: initialise remaining struct pages in parallel with kswapd)
  void set_pageblock_migratetype(struct page *page, int migratetype)
  {
 -	if (unlikely(page_group_by_mobility_disabled &&
 -		     migratetype < MIGRATE_PCPTYPES))
 +
 +	if (unlikely(page_group_by_mobility_disabled))
  		migratetype = MIGRATE_UNMOVABLE;
  
  	set_pageblock_flags_group(page, (unsigned long)migratetype,
@@@ -719,10 -820,119 +793,119 @@@ static void free_one_page(struct zone *
  	spin_unlock(&zone->lock);
  }
  
++<<<<<<< HEAD
++=======
+ static int free_tail_pages_check(struct page *head_page, struct page *page)
+ {
+ 	if (!IS_ENABLED(CONFIG_DEBUG_VM))
+ 		return 0;
+ 	if (unlikely(!PageTail(page))) {
+ 		bad_page(page, "PageTail not set", 0);
+ 		return 1;
+ 	}
+ 	if (unlikely(page->first_page != head_page)) {
+ 		bad_page(page, "first_page not consistent", 0);
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
+ 				unsigned long zone, int nid)
+ {
+ 	struct zone *z = &NODE_DATA(nid)->node_zones[zone];
+ 
+ 	set_page_links(page, zone, nid, pfn);
+ 	mminit_verify_page_links(page, zone, nid, pfn);
+ 	init_page_count(page);
+ 	page_mapcount_reset(page);
+ 	page_cpupid_reset_last(page);
+ 
+ 	/*
+ 	 * Mark the block movable so that blocks are reserved for
+ 	 * movable at startup. This will force kernel allocations
+ 	 * to reserve their blocks rather than leaking throughout
+ 	 * the address space during boot when many long-lived
+ 	 * kernel allocations are made. Later some blocks near
+ 	 * the start are marked MIGRATE_RESERVE by
+ 	 * setup_zone_migrate_reserve()
+ 	 *
+ 	 * bitmap is created for zone's valid pfn range. but memmap
+ 	 * can be created for invalid pages (for alignment)
+ 	 * check here not to call set_pageblock_migratetype() against
+ 	 * pfn out of zone.
+ 	 */
+ 	if ((z->zone_start_pfn <= pfn)
+ 	    && (pfn < zone_end_pfn(z))
+ 	    && !(pfn & (pageblock_nr_pages - 1)))
+ 		set_pageblock_migratetype(page, MIGRATE_MOVABLE);
+ 
+ 	INIT_LIST_HEAD(&page->lru);
+ #ifdef WANT_PAGE_VIRTUAL
+ 	/* The shift won't overflow because ZONE_NORMAL is below 4G. */
+ 	if (!is_highmem_idx(zone))
+ 		set_page_address(page, __va(pfn << PAGE_SHIFT));
+ #endif
+ }
+ 
+ static void __meminit __init_single_pfn(unsigned long pfn, unsigned long zone,
+ 					int nid)
+ {
+ 	return __init_single_page(pfn_to_page(pfn), pfn, zone, nid);
+ }
+ 
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static void init_reserved_page(unsigned long pfn)
+ {
+ 	pg_data_t *pgdat;
+ 	int nid, zid;
+ 
+ 	if (!early_page_uninitialised(pfn))
+ 		return;
+ 
+ 	nid = early_pfn_to_nid(pfn);
+ 	pgdat = NODE_DATA(nid);
+ 
+ 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
+ 		struct zone *zone = &pgdat->node_zones[zid];
+ 
+ 		if (pfn >= zone->zone_start_pfn && pfn < zone_end_pfn(zone))
+ 			break;
+ 	}
+ 	__init_single_pfn(pfn, zid, nid);
+ }
+ #else
+ static inline void init_reserved_page(unsigned long pfn)
+ {
+ }
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
+ 
+ /*
+  * Initialised pages do not have PageReserved set. This function is
+  * called for each range allocated by the bootmem allocator and
+  * marks the pages PageReserved. The remaining valid pages are later
+  * sent to the buddy page allocator.
+  */
+ void __meminit reserve_bootmem_region(unsigned long start, unsigned long end)
+ {
+ 	unsigned long start_pfn = PFN_DOWN(start);
+ 	unsigned long end_pfn = PFN_UP(end);
+ 
+ 	for (; start_pfn < end_pfn; start_pfn++) {
+ 		if (pfn_valid(start_pfn)) {
+ 			struct page *page = pfn_to_page(start_pfn);
+ 
+ 			init_reserved_page(start_pfn);
+ 			SetPageReserved(page);
+ 		}
+ 	}
+ }
+ 
++>>>>>>> 7e18adb4f80b (mm: meminit: initialise remaining struct pages in parallel with kswapd)
  static bool free_pages_prepare(struct page *page, unsigned int order)
  {
 -	bool compound = PageCompound(page);
 -	int i, bad = 0;
 -
 -	VM_BUG_ON_PAGE(PageTail(page), page);
 -	VM_BUG_ON_PAGE(compound && compound_order(page) != order, page);
 +	int i;
 +	int bad = 0;
  
  	trace_mm_page_free(page, order);
  	kmemcheck_free_shadow(page, order);
@@@ -781,8 -1001,136 +964,139 @@@ void __init __free_pages_bootmem(struc
  	__free_pages(page, order);
  }
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) || \
+ 	defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;
+ 
+ int __meminit early_pfn_to_nid(unsigned long pfn)
+ {
+ 	int nid;
+ 
+ 	/* The system will behave unpredictably otherwise */
+ 	BUG_ON(system_state != SYSTEM_BOOTING);
+ 
+ 	nid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);
+ 	if (nid >= 0)
+ 		return nid;
+ 	/* just returns 0 */
+ 	return 0;
+ }
+ #endif
+ 
+ #ifdef CONFIG_NODES_SPAN_OTHER_NODES
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	int nid;
+ 
+ 	nid = __early_pfn_to_nid(pfn, state);
+ 	if (nid >= 0 && nid != node)
+ 		return false;
+ 	return true;
+ }
+ 
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return meminit_pfn_in_nid(pfn, node, &early_pfnnid_cache);
+ }
+ 
+ #else
+ 
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return true;
+ }
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
+ void __defer_init __free_pages_bootmem(struct page *page, unsigned long pfn,
+ 							unsigned int order)
+ {
+ 	if (early_page_uninitialised(pfn))
+ 		return;
+ 	return __free_pages_boot_core(page, pfn, order);
+ }
+ 
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ /* Initialise remaining memory on a node */
+ void __defermem_init deferred_init_memmap(int nid)
+ {
+ 	struct mminit_pfnnid_cache nid_init_state = { };
+ 	unsigned long start = jiffies;
+ 	unsigned long nr_pages = 0;
+ 	unsigned long walk_start, walk_end;
+ 	int i, zid;
+ 	struct zone *zone;
+ 	pg_data_t *pgdat = NODE_DATA(nid);
+ 	unsigned long first_init_pfn = pgdat->first_deferred_pfn;
+ 
+ 	if (first_init_pfn == ULONG_MAX)
+ 		return;
+ 
+ 	/* Sanity check boundaries */
+ 	BUG_ON(pgdat->first_deferred_pfn < pgdat->node_start_pfn);
+ 	BUG_ON(pgdat->first_deferred_pfn > pgdat_end_pfn(pgdat));
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ 
+ 	/* Only the highest zone is deferred so find it */
+ 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
+ 		zone = pgdat->node_zones + zid;
+ 		if (first_init_pfn < zone_end_pfn(zone))
+ 			break;
+ 	}
+ 
+ 	for_each_mem_pfn_range(i, nid, &walk_start, &walk_end, NULL) {
+ 		unsigned long pfn, end_pfn;
+ 
+ 		end_pfn = min(walk_end, zone_end_pfn(zone));
+ 		pfn = first_init_pfn;
+ 		if (pfn < walk_start)
+ 			pfn = walk_start;
+ 		if (pfn < zone->zone_start_pfn)
+ 			pfn = zone->zone_start_pfn;
+ 
+ 		for (; pfn < end_pfn; pfn++) {
+ 			struct page *page;
+ 
+ 			if (!pfn_valid(pfn))
+ 				continue;
+ 
+ 			if (!meminit_pfn_in_nid(pfn, nid, &nid_init_state))
+ 				continue;
+ 
+ 			if (page->flags) {
+ 				VM_BUG_ON(page_zone(page) != zone);
+ 				continue;
+ 			}
+ 
+ 			__init_single_page(page, pfn, zid, nid);
+ 			__free_pages_boot_core(page, pfn, 0);
+ 			nr_pages++;
+ 			cond_resched();
+ 		}
+ 		first_init_pfn = max(end_pfn, first_init_pfn);
+ 	}
+ 
+ 	/* Sanity check that the next zone really is unpopulated */
+ 	WARN_ON(++zid < MAX_NR_ZONES && populated_zone(++zone));
+ 
+ 	pr_info("kswapd %d initialised %lu pages in %ums\n", nid, nr_pages,
+ 					jiffies_to_msecs(jiffies - start));
+ }
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
+ 
++>>>>>>> 7e18adb4f80b (mm: meminit: initialise remaining struct pages in parallel with kswapd)
  #ifdef CONFIG_CMA
 -/* Free whole pageblock and set its migration type to MIGRATE_CMA. */
 +/* Free whole pageblock and set it's migration type to MIGRATE_CMA. */
  void __init init_cma_reserved_pageblock(struct page *page)
  {
  	unsigned i = pageblock_nr_pages;
* Unmerged path mm/internal.h
* Unmerged path mm/mm_init.c
* Unmerged path mm/page_alloc.c
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 7592127381db..19af7f1aa255 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -3207,7 +3207,7 @@ static void kswapd_try_to_sleep(pg_data_t *pgdat, int order, int classzone_idx)
  * If there are applications that are active memory-allocators
  * (most normal use), this basically shouldn't matter.
  */
-static int kswapd(void *p)
+static int __defermem_init kswapd(void *p)
 {
 	unsigned long order, new_order;
 	unsigned balanced_order;
@@ -3242,6 +3242,8 @@ static int kswapd(void *p)
 	tsk->flags |= PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD;
 	set_freezable();
 
+	deferred_init_memmap(pgdat->node_id);
+
 	order = new_order = 0;
 	balanced_order = 0;
 	classzone_idx = new_classzone_idx = pgdat->nr_zones - 1;
@@ -3401,7 +3403,7 @@ static int cpu_callback(struct notifier_block *nfb, unsigned long action,
  * This kswapd start function will be called by init and node-hot-add.
  * On node-hot-add, kswapd will moved to proper cpus if cpus are hot-added.
  */
-int kswapd_run(int nid)
+int __defermem_init kswapd_run(int nid)
 {
 	pg_data_t *pgdat = NODE_DATA(nid);
 	int ret = 0;
