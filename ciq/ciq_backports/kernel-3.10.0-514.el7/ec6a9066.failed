watchdog: rename watchdog_suspend() and watchdog_resume()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Ulrich Obergfell <uobergfe@redhat.com>
commit ec6a90661a0d6ce1461d05c7a58a0a151154e14a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ec6a9066.failed

Rename watchdog_suspend() to lockup_detector_suspend() and
watchdog_resume() to lockup_detector_resume() to avoid confusion with the
watchdog subsystem and to be consistent with the existing name
lockup_detector_init().

Also provide comment blocks to explain the watchdog_running and
watchdog_suspended variables and their relationship.

	Signed-off-by: Ulrich Obergfell <uobergfe@redhat.com>
	Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
	Cc: Guenter Roeck <linux@roeck-us.net>
	Cc: Don Zickus <dzickus@redhat.com>
	Cc: Ulrich Obergfell <uobergfe@redhat.com>
	Cc: Jiri Olsa <jolsa@kernel.org>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Chris Metcalf <cmetcalf@ezchip.com>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Cc: Ingo Molnar <mingo@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ec6a90661a0d6ce1461d05c7a58a0a151154e14a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel.c
#	include/linux/nmi.h
#	kernel/watchdog.c
diff --cc arch/x86/kernel/cpu/perf_event_intel.c
index d4cfd53179e2,cd9b6d0b10bf..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel.c
+++ b/arch/x86/kernel/cpu/perf_event_intel.c
@@@ -3728,7 -3627,10 +3728,14 @@@ static __init int fixup_ht_bug(void
  		return 0;
  	}
  
++<<<<<<< HEAD
 +	watchdog_nmi_disable_all();
++=======
+ 	if (lockup_detector_suspend() != 0) {
+ 		pr_debug("failed to disable PMU erratum BJ122, BV98, HSD29 workaround\n");
+ 		return 0;
+ 	}
++>>>>>>> ec6a90661a0d (watchdog: rename watchdog_suspend() and watchdog_resume())
  
  	x86_pmu.flags &= ~(PMU_FL_EXCL_CNTRS | PMU_FL_EXCL_ENABLED);
  
@@@ -3736,7 -3638,7 +3743,11 @@@
  	x86_pmu.commit_scheduling = NULL;
  	x86_pmu.stop_scheduling = NULL;
  
++<<<<<<< HEAD
 +	watchdog_nmi_enable_all();
++=======
+ 	lockup_detector_resume();
++>>>>>>> ec6a90661a0d (watchdog: rename watchdog_suspend() and watchdog_resume())
  
  	get_online_cpus();
  
diff --cc include/linux/nmi.h
index 6fe7c9ae7224,a91adf6e02f2..000000000000
--- a/include/linux/nmi.h
+++ b/include/linux/nmi.h
@@@ -82,8 -76,23 +82,28 @@@ extern int proc_soft_watchdog(struct ct
  			      void __user *, size_t *, loff_t *);
  extern int proc_watchdog_thresh(struct ctl_table *, int ,
  				void __user *, size_t *, loff_t *);
++<<<<<<< HEAD
 +extern int proc_dowatchdog(struct ctl_table *, int ,
 +			   void __user *, size_t *, loff_t *);
++=======
+ extern int proc_watchdog_cpumask(struct ctl_table *, int,
+ 				 void __user *, size_t *, loff_t *);
+ extern int lockup_detector_suspend(void);
+ extern void lockup_detector_resume(void);
+ #else
+ static inline int lockup_detector_suspend(void)
+ {
+ 	return 0;
+ }
+ 
+ static inline void lockup_detector_resume(void)
+ {
+ }
+ #endif
+ 
+ #ifdef CONFIG_HAVE_ACPI_APEI_NMI
+ #include <asm/nmi.h>
++>>>>>>> ec6a90661a0d (watchdog: rename watchdog_suspend() and watchdog_resume())
  #endif
  
  #endif
diff --cc kernel/watchdog.c
index 22977eb33006,64ed1c37bd1f..000000000000
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@@ -59,8 -60,33 +59,34 @@@ int __read_mostly sysctl_softlockup_all
  #else
  #define sysctl_softlockup_all_cpu_backtrace 0
  #endif
 -static struct cpumask watchdog_cpumask __read_mostly;
 -unsigned long *watchdog_cpumask_bits = cpumask_bits(&watchdog_cpumask);
  
++<<<<<<< HEAD
++=======
+ /* Helper for online, unparked cpus. */
+ #define for_each_watchdog_cpu(cpu) \
+ 	for_each_cpu_and((cpu), cpu_online_mask, &watchdog_cpumask)
+ 
+ /*
+  * The 'watchdog_running' variable is set to 1 when the watchdog threads
+  * are registered/started and is set to 0 when the watchdog threads are
+  * unregistered/stopped, so it is an indicator whether the threads exist.
+  */
++>>>>>>> ec6a90661a0d (watchdog: rename watchdog_suspend() and watchdog_resume())
  static int __read_mostly watchdog_running;
+ /*
+  * If a subsystem has a need to deactivate the watchdog temporarily, it
+  * can use the suspend/resume interface to achieve this. The content of
+  * the 'watchdog_suspended' variable reflects this state. Existing threads
+  * are parked/unparked by the lockup_detector_{suspend|resume} functions
+  * (see comment blocks pertaining to those functions for further details).
+  *
+  * 'watchdog_suspended' also prevents threads from being registered/started
+  * or unregistered/stopped via parameters in /proc/sys/kernel, so the state
+  * of 'watchdog_running' cannot change while the watchdog is deactivated
+  * temporarily (see related code in 'proc' handlers).
+  */
+ static int __read_mostly watchdog_suspended;
+ 
  static u64 __read_mostly sample_period;
  
  static DEFINE_PER_CPU(unsigned long, watchdog_touch_ts);
@@@ -710,49 -684,57 +736,71 @@@ static void watchdog_unpark_threads(voi
  	put_online_cpus();
  }
  
++<<<<<<< HEAD
 +static void restart_watchdog_hrtimer(void *info)
++=======
+ /*
+  * Suspend the hard and soft lockup detector by parking the watchdog threads.
+  */
+ int lockup_detector_suspend(void)
++>>>>>>> ec6a90661a0d (watchdog: rename watchdog_suspend() and watchdog_resume())
  {
 -	int ret = 0;
 +	struct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);
 +	int ret;
  
 -	mutex_lock(&watchdog_proc_mutex);
  	/*
++<<<<<<< HEAD
 +	 * No need to cancel and restart hrtimer if it is currently executing
 +	 * because it will reprogram itself with the new period now.
 +	 * We should never see it unqueued here because we are running per-cpu
 +	 * with interrupts disabled.
++=======
+ 	 * Multiple suspend requests can be active in parallel (counted by
+ 	 * the 'watchdog_suspended' variable). If the watchdog threads are
+ 	 * running, the first caller takes care that they will be parked.
+ 	 * The state of 'watchdog_running' cannot change while a suspend
+ 	 * request is active (see related code in 'proc' handlers).
++>>>>>>> ec6a90661a0d (watchdog: rename watchdog_suspend() and watchdog_resume())
  	 */
 -	if (watchdog_running && !watchdog_suspended)
 -		ret = watchdog_park_threads();
 -
 -	if (ret == 0)
 -		watchdog_suspended++;
 -
 -	mutex_unlock(&watchdog_proc_mutex);
 -
 -	return ret;
 +	ret = hrtimer_try_to_cancel(hrtimer);
 +	if (ret == 1)
 +		hrtimer_start(hrtimer, ns_to_ktime(sample_period),
 +				HRTIMER_MODE_REL_PINNED);
  }
  
++<<<<<<< HEAD
 +static void update_timers(int cpu)
++=======
+ /*
+  * Resume the hard and soft lockup detector by unparking the watchdog threads.
+  */
+ void lockup_detector_resume(void)
++>>>>>>> ec6a90661a0d (watchdog: rename watchdog_suspend() and watchdog_resume())
  {
 -	mutex_lock(&watchdog_proc_mutex);
 -
 -	watchdog_suspended--;
  	/*
 -	 * The watchdog threads are unparked if they were previously running
 -	 * and if there is no more active suspend request.
 +	 * Make sure that perf event counter will adopt to a new
 +	 * sampling period. Updating the sampling period directly would
 +	 * be much nicer but we do not have an API for that now so
 +	 * let's use a big hammer.
 +	 * Hrtimer will adopt the new period on the next tick but this
 +	 * might be late already so we have to restart the timer as well.
  	 */
 -	if (watchdog_running && !watchdog_suspended)
 -		watchdog_unpark_threads();
 -
 -	mutex_unlock(&watchdog_proc_mutex);
 +	watchdog_nmi_disable(cpu);
 +	smp_call_function_single(cpu, restart_watchdog_hrtimer, NULL, 1);
 +	watchdog_nmi_enable(cpu);
  }
  
 -static void update_watchdog_all_cpus(void)
 +static void update_timers_all_cpus(void)
  {
 -	watchdog_park_threads();
 -	watchdog_unpark_threads();
 +	int cpu;
 +
 +	get_online_cpus();
 +	for_each_online_cpu(cpu)
 +		update_timers(cpu);
 +	put_online_cpus();
  }
  
 -static int watchdog_enable_all_cpus(void)
 +static int watchdog_enable_all_cpus(bool sample_period_changed)
  {
  	int err = 0;
  
* Unmerged path arch/x86/kernel/cpu/perf_event_intel.c
* Unmerged path include/linux/nmi.h
* Unmerged path kernel/watchdog.c
