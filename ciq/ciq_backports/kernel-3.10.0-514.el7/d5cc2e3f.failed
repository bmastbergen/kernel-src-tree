xfs: DIO needs an ioend for writes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit d5cc2e3f968ff60f247fdef15b04fac788ef46d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/d5cc2e3f.failed

Currently we can only tell DIO completion that an IO requires
unwritten extent completion. This is done by a hacky non-null
private pointer passed to Io completion, but the private pointer
does not actually contain any information that is used.

We also need to pass to IO completion the fact that the IO may be
beyond EOF and so a size update transaction needs to be done. This
is currently determined by checks in the io completion, but we need
to determine if this is necessary at block mapping time as we need
to defer the size update transactions to a completion workqueue,
just like unwritten extent conversion.

To do this, first we need to allocate and pass an ioend to to IO
completion. Add this for unwritten extent conversion; we'll do the
EOF updates in the next commit.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit d5cc2e3f968ff60f247fdef15b04fac788ef46d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_aops.c
diff --cc fs/xfs/xfs_aops.c
index bb6024910e57,60d6466d72f6..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -1260,6 -1233,111 +1260,114 @@@ xfs_vm_releasepage
  	return try_to_free_buffers(page);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * When we map a DIO buffer, we need to attach an ioend that describes the type
+  * of write IO we are doing. This passes to the completion function the
+  * operations it needs to perform.
+  *
+  * If we get multiple mappings in a single IO, we might be mapping different
+  * types. But because the direct IO can only have a single private pointer, we
+  * need to ensure that:
+  *
+  * a) the ioend spans the entire region of the IO; and
+  * b) if it contains unwritten extents, it is *permanently* marked as such
+  *
+  * We could do this by chaining ioends like buffered IO does, but we only
+  * actually get one IO completion callback from the direct IO, and that spans
+  * the entire IO regardless of how many mappings and IOs are needed to complete
+  * the DIO. There is only going to be one reference to the ioend and its life
+  * cycle is constrained by the DIO completion code. hence we don't need
+  * reference counting here.
+  */
+ static void
+ xfs_map_direct(
+ 	struct inode		*inode,
+ 	struct buffer_head	*bh_result,
+ 	struct xfs_bmbt_irec	*imap,
+ 	xfs_off_t		offset)
+ {
+ 	struct xfs_ioend	*ioend;
+ 	xfs_off_t		size = bh_result->b_size;
+ 	int			type;
+ 
+ 	if (ISUNWRITTEN(imap))
+ 		type = XFS_IO_UNWRITTEN;
+ 	else
+ 		type = XFS_IO_OVERWRITE;
+ 
+ 	trace_xfs_gbmap_direct(XFS_I(inode), offset, size, type, imap);
+ 
+ 	if (bh_result->b_private) {
+ 		ioend = bh_result->b_private;
+ 		ASSERT(ioend->io_size > 0);
+ 		ASSERT(offset >= ioend->io_offset);
+ 		if (offset + size > ioend->io_offset + ioend->io_size)
+ 			ioend->io_size = offset - ioend->io_offset + size;
+ 
+ 		if (type == XFS_IO_UNWRITTEN && type != ioend->io_type)
+ 			ioend->io_type = XFS_IO_UNWRITTEN;
+ 
+ 		trace_xfs_gbmap_direct_update(XFS_I(inode), ioend->io_offset,
+ 					      ioend->io_size, ioend->io_type,
+ 					      imap);
+ 	} else {
+ 		ioend = xfs_alloc_ioend(inode, type);
+ 		ioend->io_offset = offset;
+ 		ioend->io_size = size;
+ 		bh_result->b_private = ioend;
+ 
+ 		trace_xfs_gbmap_direct_new(XFS_I(inode), offset, size, type,
+ 					   imap);
+ 	}
+ 
+ 	if (ioend->io_type == XFS_IO_UNWRITTEN)
+ 		set_buffer_defer_completion(bh_result);
+ }
+ 
+ 
+ /*
+  * If this is O_DIRECT or the mpage code calling tell them how large the mapping
+  * is, so that we can avoid repeated get_blocks calls.
+  *
+  * If the mapping spans EOF, then we have to break the mapping up as the mapping
+  * for blocks beyond EOF must be marked new so that sub block regions can be
+  * correctly zeroed. We can't do this for mappings within EOF unless the mapping
+  * was just allocated or is unwritten, otherwise the callers would overwrite
+  * existing data with zeros. Hence we have to split the mapping into a range up
+  * to and including EOF, and a second mapping for beyond EOF.
+  */
+ static void
+ xfs_map_trim_size(
+ 	struct inode		*inode,
+ 	sector_t		iblock,
+ 	struct buffer_head	*bh_result,
+ 	struct xfs_bmbt_irec	*imap,
+ 	xfs_off_t		offset,
+ 	ssize_t			size)
+ {
+ 	xfs_off_t		mapping_size;
+ 
+ 	mapping_size = imap->br_startoff + imap->br_blockcount - iblock;
+ 	mapping_size <<= inode->i_blkbits;
+ 
+ 	ASSERT(mapping_size > 0);
+ 	if (mapping_size > size)
+ 		mapping_size = size;
+ 	if (offset < i_size_read(inode) &&
+ 	    offset + mapping_size >= i_size_read(inode)) {
+ 		/* limit mapping to block that spans EOF */
+ 		mapping_size = roundup_64(i_size_read(inode) - offset,
+ 					  1 << inode->i_blkbits);
+ 	}
+ 	if (mapping_size > LONG_MAX)
+ 		mapping_size = LONG_MAX;
+ 
+ 	bh_result->b_size = mapping_size;
+ }
+ 
++>>>>>>> d5cc2e3f968f (xfs: DIO needs an ioend for writes)
  STATIC int
  __xfs_get_blocks(
  	struct inode		*inode,
@@@ -1479,50 -1528,69 +1590,95 @@@ xfs_end_io_direct_write
  	struct kiocb		*iocb,
  	loff_t			offset,
  	ssize_t			size,
 -	void			*private)
 +	void			*private,
 +	int			ret,
 +	bool			is_async)
  {
++<<<<<<< HEAD
 +	struct xfs_ioend	*ioend = iocb->private;
 +	struct xfs_inode	*ip = XFS_I(ioend->io_inode);
 +	unsigned long		flags;
++=======
+ 	struct inode		*inode = file_inode(iocb->ki_filp);
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_ioend	*ioend = private;
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		goto out_destroy_ioend;
+ 
+ 	/*
+ 	 * dio completion end_io functions are only called on writes if more
+ 	 * than 0 bytes was written.
+ 	 */
+ 	ASSERT(size > 0);
+ 
+ 	/*
+ 	 * The ioend only maps whole blocks, while the IO may be sector aligned.
+ 	 * Hence the ioend offset/size may not match the IO offset/size exactly,
+ 	 * but should span it completely. Write the IO sizes into the ioend so
+ 	 * that completion processing does the right thing.
+ 	 */
+ 	ASSERT(size <= ioend->io_size);
+ 	ASSERT(offset >= ioend->io_offset);
+ 	ASSERT(offset + size <= ioend->io_offset + ioend->io_size);
+ 	ioend->io_size = size;
+ 	ioend->io_offset = offset;
++>>>>>>> d5cc2e3f968f (xfs: DIO needs an ioend for writes)
  
  	/*
  	 * While the generic direct I/O code updates the inode size, it does
  	 * so only after the end_io handler is called, which means our
  	 * end_io handler thinks the on-disk size is outside the in-core
  	 * size.  To prevent this just update it a little bit earlier here.
 +	 *
 +	 * We need to lock the test/set EOF update as we can be racing with
 +	 * other IO completions here to update the EOF. Failing to serialise
 +	 * here can result in EOF moving backwards and Bad Things Happen when
 +	 * that occurs.
  	 */
 -	if (offset + size > i_size_read(inode))
 -		i_size_write(inode, offset + size);
 +	spin_lock_irqsave(&ip->i_size_lock, flags);
 +	if (offset + size > i_size_read(ioend->io_inode))
 +		i_size_write(ioend->io_inode, offset + size);
 +	spin_unlock_irqrestore(&ip->i_size_lock, flags);
  
  	/*
 -	 * For direct I/O we do not know if we need to allocate blocks or not,
 -	 * so we can't preallocate an append transaction, as that results in
 -	 * nested reservations and log space deadlocks. Hence allocate the
 -	 * transaction here. While this is sub-optimal and can block IO
 -	 * completion for some time, we're stuck with doing it this way until
 -	 * we can pass the ioend to the direct IO allocation callbacks and
 -	 * avoid nesting that way.
 +	 * blockdev_direct_IO can return an error even after the I/O
 +	 * completion handler was called.  Thus we need to protect
 +	 * against double-freeing.
  	 */
++<<<<<<< HEAD
 +	iocb->private = NULL;
 +
 +	ioend->io_offset = offset;
 +	ioend->io_size = size;
 +	ioend->io_iocb = iocb;
 +	ioend->io_result = ret;
 +	if (private && size > 0)
 +		ioend->io_type = XFS_IO_UNWRITTEN;
++=======
+ 	if (ioend->io_type == XFS_IO_UNWRITTEN) {
+ 		xfs_iomap_write_unwritten(ip, offset, size);
+ 	} else if (offset + size > ip->i_d.di_size) {
+ 		struct xfs_trans	*tp;
+ 		int			error;
+ 
+ 		tp = xfs_trans_alloc(mp, XFS_TRANS_FSYNC_TS);
+ 		error = xfs_trans_reserve(tp, &M_RES(mp)->tr_fsyncts, 0, 0);
+ 		if (error) {
+ 			xfs_trans_cancel(tp, 0);
+ 			goto out_destroy_ioend;
+ 		}
++>>>>>>> d5cc2e3f968f (xfs: DIO needs an ioend for writes)
  
 -		xfs_setfilesize(ip, tp, offset, size);
 +	if (is_async) {
 +		ioend->io_isasync = 1;
 +		xfs_finish_ioend(ioend);
 +	} else {
 +		xfs_finish_ioend_sync(ioend);
  	}
+ out_destroy_ioend:
+ 	xfs_destroy_ioend(ioend);
  }
  
  STATIC ssize_t
* Unmerged path fs/xfs/xfs_aops.c
diff --git a/fs/xfs/xfs_trace.h b/fs/xfs/xfs_trace.h
index 1cc33065983c..38036680c674 100644
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@ -1221,6 +1221,9 @@ DEFINE_IOMAP_EVENT(xfs_map_blocks_found);
 DEFINE_IOMAP_EVENT(xfs_map_blocks_alloc);
 DEFINE_IOMAP_EVENT(xfs_get_blocks_found);
 DEFINE_IOMAP_EVENT(xfs_get_blocks_alloc);
+DEFINE_IOMAP_EVENT(xfs_gbmap_direct);
+DEFINE_IOMAP_EVENT(xfs_gbmap_direct_new);
+DEFINE_IOMAP_EVENT(xfs_gbmap_direct_update);
 
 DECLARE_EVENT_CLASS(xfs_simple_io_class,
 	TP_PROTO(struct xfs_inode *ip, xfs_off_t offset, ssize_t count),
