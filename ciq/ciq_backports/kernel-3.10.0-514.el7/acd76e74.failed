xfs: huge page fault support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Matthew Wilcox <willy@linux.intel.com>
commit acd76e74d80f961553861d9cf49a62cbcf496d28
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/acd76e74.failed

Use DAX to provide support for huge pages.

	Signed-off-by: Matthew Wilcox <willy@linux.intel.com>
	Cc: Hillf Danton <dhillf@gmail.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Theodore Ts'o <tytso@mit.edu>
	Cc: Jan Kara <jack@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit acd76e74d80f961553861d9cf49a62cbcf496d28)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_file.c
index ff72b4ea0a57,e78feb400e22..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -1562,21 -1493,103 +1562,101 @@@ xfs_filemap_page_mkwrite
  	struct vm_area_struct	*vma,
  	struct vm_fault		*vmf)
  {
 -	struct inode		*inode = file_inode(vma->vm_file);
 +	struct xfs_inode	*ip = XFS_I(vma->vm_file->f_mapping->host);
  	int			ret;
  
 -	trace_xfs_filemap_page_mkwrite(XFS_I(inode));
 +	trace_xfs_filemap_page_mkwrite(ip);
  
 -	sb_start_pagefault(inode->i_sb);
 +	sb_start_pagefault(VFS_I(ip)->i_sb);
  	file_update_time(vma->vm_file);
 -	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 +	xfs_ilock(ip, XFS_MMAPLOCK_SHARED);
  
 -	if (IS_DAX(inode)) {
 -		ret = __dax_mkwrite(vma, vmf, xfs_get_blocks_direct,
 -				    xfs_end_io_dax_write);
 -	} else {
 -		ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
 -		ret = block_page_mkwrite_return(ret);
 -	}
 +	ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
  
 -	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
 -	sb_end_pagefault(inode->i_sb);
 +	xfs_iunlock(ip, XFS_MMAPLOCK_SHARED);
 +	sb_end_pagefault(VFS_I(ip)->i_sb);
  
++<<<<<<< HEAD
 +	return block_page_mkwrite_return(ret);
++=======
+ 	return ret;
+ }
+ 
+ STATIC int
+ xfs_filemap_fault(
+ 	struct vm_area_struct	*vma,
+ 	struct vm_fault		*vmf)
+ {
+ 	struct inode		*inode = file_inode(vma->vm_file);
+ 	int			ret;
+ 
+ 	trace_xfs_filemap_fault(XFS_I(inode));
+ 
+ 	/* DAX can shortcut the normal fault path on write faults! */
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && IS_DAX(inode))
+ 		return xfs_filemap_page_mkwrite(vma, vmf);
+ 
+ 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	if (IS_DAX(inode)) {
+ 		/*
+ 		 * we do not want to trigger unwritten extent conversion on read
+ 		 * faults - that is unnecessary overhead and would also require
+ 		 * changes to xfs_get_blocks_direct() to map unwritten extent
+ 		 * ioend for conversion on read-only mappings.
+ 		 */
+ 		ret = __dax_fault(vma, vmf, xfs_get_blocks_direct, NULL);
+ 	} else
+ 		ret = filemap_fault(vma, vmf);
+ 	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 
+ 	return ret;
+ }
+ 
+ STATIC int
+ xfs_filemap_pmd_fault(
+ 	struct vm_area_struct	*vma,
+ 	unsigned long		addr,
+ 	pmd_t			*pmd,
+ 	unsigned int		flags)
+ {
+ 	struct inode		*inode = file_inode(vma->vm_file);
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	int			ret;
+ 
+ 	if (!IS_DAX(inode))
+ 		return VM_FAULT_FALLBACK;
+ 
+ 	trace_xfs_filemap_pmd_fault(ip);
+ 
+ 	sb_start_pagefault(inode->i_sb);
+ 	file_update_time(vma->vm_file);
+ 	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	ret = __dax_pmd_fault(vma, addr, pmd, flags, xfs_get_blocks_direct,
+ 				    xfs_end_io_dax_write);
+ 	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
+ 	sb_end_pagefault(inode->i_sb);
+ 
+ 	return ret;
+ }
+ 
+ static const struct vm_operations_struct xfs_file_vm_ops = {
+ 	.fault		= xfs_filemap_fault,
+ 	.pmd_fault	= xfs_filemap_pmd_fault,
+ 	.map_pages	= filemap_map_pages,
+ 	.page_mkwrite	= xfs_filemap_page_mkwrite,
+ };
+ 
+ STATIC int
+ xfs_file_mmap(
+ 	struct file	*filp,
+ 	struct vm_area_struct *vma)
+ {
+ 	file_accessed(filp);
+ 	vma->vm_ops = &xfs_file_vm_ops;
+ 	if (IS_DAX(file_inode(filp)))
+ 		vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
+ 	return 0;
++>>>>>>> acd76e74d80f (xfs: huge page fault support)
  }
  
  const struct file_operations xfs_file_operations = {
* Unmerged path fs/xfs/xfs_file.c
diff --git a/fs/xfs/xfs_trace.h b/fs/xfs/xfs_trace.h
index dcf226ef842a..dafbc8a8055e 100644
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@ -686,6 +686,7 @@ DEFINE_INODE_EVENT(xfs_inode_clear_eofblocks_tag);
 DEFINE_INODE_EVENT(xfs_inode_free_eofblocks_invalid);
 
 DEFINE_INODE_EVENT(xfs_filemap_fault);
+DEFINE_INODE_EVENT(xfs_filemap_pmd_fault);
 DEFINE_INODE_EVENT(xfs_filemap_page_mkwrite);
 
 DECLARE_EVENT_CLASS(xfs_iref_class,
