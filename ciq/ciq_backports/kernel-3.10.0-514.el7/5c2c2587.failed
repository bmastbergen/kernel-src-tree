mm, dax, pmem: introduce {get|put}_dev_pagemap() for dax-gup

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 5c2c2587b13235bf8b5c9027589f22eff68bdf49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5c2c2587.failed

get_dev_page() enables paths like get_user_pages() to pin a dynamically
mapped pfn-range (devm_memremap_pages()) while the resulting struct page
objects are in use.  Unlike get_page() it may fail if the device is, or
is in the process of being, disabled.  While the initial lookup of the
range may be an expensive list walk, the result is cached to speed up
subsequent lookups which are likely to be in the same mapped range.

devm_memremap_pages() now requires a reference counter to be specified
at init time.  For pmem this means moving request_queue allocation into
pmem_alloc() so the existing queue usage counter can track "device
pages".

ZONE_DEVICE pages always have an elevated count and will never be on an
lru reclaim list.  That space in 'struct page' can be redirected for
other uses, but for safety introduce a poison value that will always
trip __list_add() to assert.  This allows half of the struct list_head
storage to be reclaimed with some assurance to back up the assumption
that the page count never goes to zero and a list_add() is never
attempted.

	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Tested-by: Logan Gunthorpe <logang@deltatee.com>
	Cc: Dave Hansen <dave@sr71.net>
	Cc: Matthew Wilcox <willy@linux.intel.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Alexander Viro <viro@zeniv.linux.org.uk>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5c2c2587b13235bf8b5c9027589f22eff68bdf49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvdimm/pmem.c
#	include/linux/memremap.h
#	kernel/memremap.c
diff --cc kernel/memremap.c
index 26717809cbd2,3eb8944265d5..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -144,3 -149,241 +144,244 @@@ void devm_memunmap(struct device *dev, 
  				devm_memremap_match, addr));
  }
  EXPORT_SYMBOL(devm_memunmap);
++<<<<<<< HEAD
++=======
+ 
+ pfn_t phys_to_pfn_t(dma_addr_t addr, unsigned long flags)
+ {
+ 	return __pfn_to_pfn_t(addr >> PAGE_SHIFT, flags);
+ }
+ EXPORT_SYMBOL(phys_to_pfn_t);
+ 
+ #ifdef CONFIG_ZONE_DEVICE
+ static DEFINE_MUTEX(pgmap_lock);
+ static RADIX_TREE(pgmap_radix, GFP_KERNEL);
+ #define SECTION_MASK ~((1UL << PA_SECTION_SHIFT) - 1)
+ #define SECTION_SIZE (1UL << PA_SECTION_SHIFT)
+ 
+ struct page_map {
+ 	struct resource res;
+ 	struct percpu_ref *ref;
+ 	struct dev_pagemap pgmap;
+ 	struct vmem_altmap altmap;
+ };
+ 
+ static void pgmap_radix_release(struct resource *res)
+ {
+ 	resource_size_t key;
+ 
+ 	mutex_lock(&pgmap_lock);
+ 	for (key = res->start; key <= res->end; key += SECTION_SIZE)
+ 		radix_tree_delete(&pgmap_radix, key >> PA_SECTION_SHIFT);
+ 	mutex_unlock(&pgmap_lock);
+ }
+ 
+ static unsigned long pfn_first(struct page_map *page_map)
+ {
+ 	struct dev_pagemap *pgmap = &page_map->pgmap;
+ 	const struct resource *res = &page_map->res;
+ 	struct vmem_altmap *altmap = pgmap->altmap;
+ 	unsigned long pfn;
+ 
+ 	pfn = res->start >> PAGE_SHIFT;
+ 	if (altmap)
+ 		pfn += vmem_altmap_offset(altmap);
+ 	return pfn;
+ }
+ 
+ static unsigned long pfn_end(struct page_map *page_map)
+ {
+ 	const struct resource *res = &page_map->res;
+ 
+ 	return (res->start + resource_size(res)) >> PAGE_SHIFT;
+ }
+ 
+ #define for_each_device_pfn(pfn, map) \
+ 	for (pfn = pfn_first(map); pfn < pfn_end(map); pfn++)
+ 
+ static void devm_memremap_pages_release(struct device *dev, void *data)
+ {
+ 	struct page_map *page_map = data;
+ 	struct resource *res = &page_map->res;
+ 	resource_size_t align_start, align_size;
+ 	struct dev_pagemap *pgmap = &page_map->pgmap;
+ 
+ 	if (percpu_ref_tryget_live(pgmap->ref)) {
+ 		dev_WARN(dev, "%s: page mapping is still live!\n", __func__);
+ 		percpu_ref_put(pgmap->ref);
+ 	}
+ 
+ 	pgmap_radix_release(res);
+ 
+ 	/* pages are dead and unused, undo the arch mapping */
+ 	align_start = res->start & ~(SECTION_SIZE - 1);
+ 	align_size = ALIGN(resource_size(res), SECTION_SIZE);
+ 	arch_remove_memory(align_start, align_size);
+ 	dev_WARN_ONCE(dev, pgmap->altmap && pgmap->altmap->alloc,
+ 			"%s: failed to free all reserved pages\n", __func__);
+ }
+ 
+ /* assumes rcu_read_lock() held at entry */
+ struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
+ {
+ 	struct page_map *page_map;
+ 
+ 	WARN_ON_ONCE(!rcu_read_lock_held());
+ 
+ 	page_map = radix_tree_lookup(&pgmap_radix, phys >> PA_SECTION_SHIFT);
+ 	return page_map ? &page_map->pgmap : NULL;
+ }
+ 
+ /**
+  * devm_memremap_pages - remap and provide memmap backing for the given resource
+  * @dev: hosting device for @res
+  * @res: "host memory" address range
+  * @ref: a live per-cpu reference count
+  * @altmap: optional descriptor for allocating the memmap from @res
+  *
+  * Notes:
+  * 1/ @ref must be 'live' on entry and 'dead' before devm_memunmap_pages() time
+  *    (or devm release event).
+  *
+  * 2/ @res is expected to be a host memory range that could feasibly be
+  *    treated as a "System RAM" range, i.e. not a device mmio range, but
+  *    this is not enforced.
+  */
+ void *devm_memremap_pages(struct device *dev, struct resource *res,
+ 		struct percpu_ref *ref, struct vmem_altmap *altmap)
+ {
+ 	int is_ram = region_intersects(res->start, resource_size(res),
+ 			"System RAM");
+ 	resource_size_t key, align_start, align_size;
+ 	struct dev_pagemap *pgmap;
+ 	struct page_map *page_map;
+ 	unsigned long pfn;
+ 	int error, nid;
+ 
+ 	if (is_ram == REGION_MIXED) {
+ 		WARN_ONCE(1, "%s attempted on mixed region %pr\n",
+ 				__func__, res);
+ 		return ERR_PTR(-ENXIO);
+ 	}
+ 
+ 	if (is_ram == REGION_INTERSECTS)
+ 		return __va(res->start);
+ 
+ 	if (altmap && !IS_ENABLED(CONFIG_SPARSEMEM_VMEMMAP)) {
+ 		dev_err(dev, "%s: altmap requires CONFIG_SPARSEMEM_VMEMMAP=y\n",
+ 				__func__);
+ 		return ERR_PTR(-ENXIO);
+ 	}
+ 
+ 	if (!ref)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	page_map = devres_alloc_node(devm_memremap_pages_release,
+ 			sizeof(*page_map), GFP_KERNEL, dev_to_node(dev));
+ 	if (!page_map)
+ 		return ERR_PTR(-ENOMEM);
+ 	pgmap = &page_map->pgmap;
+ 
+ 	memcpy(&page_map->res, res, sizeof(*res));
+ 
+ 	pgmap->dev = dev;
+ 	if (altmap) {
+ 		memcpy(&page_map->altmap, altmap, sizeof(*altmap));
+ 		pgmap->altmap = &page_map->altmap;
+ 	}
+ 	pgmap->ref = ref;
+ 	pgmap->res = &page_map->res;
+ 
+ 	mutex_lock(&pgmap_lock);
+ 	error = 0;
+ 	for (key = res->start; key <= res->end; key += SECTION_SIZE) {
+ 		struct dev_pagemap *dup;
+ 
+ 		rcu_read_lock();
+ 		dup = find_dev_pagemap(key);
+ 		rcu_read_unlock();
+ 		if (dup) {
+ 			dev_err(dev, "%s: %pr collides with mapping for %s\n",
+ 					__func__, res, dev_name(dup->dev));
+ 			error = -EBUSY;
+ 			break;
+ 		}
+ 		error = radix_tree_insert(&pgmap_radix, key >> PA_SECTION_SHIFT,
+ 				page_map);
+ 		if (error) {
+ 			dev_err(dev, "%s: failed: %d\n", __func__, error);
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&pgmap_lock);
+ 	if (error)
+ 		goto err_radix;
+ 
+ 	nid = dev_to_node(dev);
+ 	if (nid < 0)
+ 		nid = numa_mem_id();
+ 
+ 	align_start = res->start & ~(SECTION_SIZE - 1);
+ 	align_size = ALIGN(resource_size(res), SECTION_SIZE);
+ 	error = arch_add_memory(nid, align_start, align_size, true);
+ 	if (error)
+ 		goto err_add_memory;
+ 
+ 	for_each_device_pfn(pfn, page_map) {
+ 		struct page *page = pfn_to_page(pfn);
+ 
+ 		/* ZONE_DEVICE pages must never appear on a slab lru */
+ 		list_force_poison(&page->lru);
+ 		page->pgmap = pgmap;
+ 	}
+ 	devres_add(dev, page_map);
+ 	return __va(res->start);
+ 
+  err_add_memory:
+  err_radix:
+ 	pgmap_radix_release(res);
+ 	devres_free(page_map);
+ 	return ERR_PTR(error);
+ }
+ EXPORT_SYMBOL(devm_memremap_pages);
+ 
+ unsigned long vmem_altmap_offset(struct vmem_altmap *altmap)
+ {
+ 	/* number of pfns from base where pfn_to_page() is valid */
+ 	return altmap->reserve + altmap->free;
+ }
+ 
+ void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns)
+ {
+ 	altmap->alloc -= nr_pfns;
+ }
+ 
+ #ifdef CONFIG_SPARSEMEM_VMEMMAP
+ struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
+ {
+ 	/*
+ 	 * 'memmap_start' is the virtual address for the first "struct
+ 	 * page" in this range of the vmemmap array.  In the case of
+ 	 * CONFIG_SPARSE_VMEMMAP a page_to_pfn conversion is simple
+ 	 * pointer arithmetic, so we can perform this to_vmem_altmap()
+ 	 * conversion without concern for the initialization state of
+ 	 * the struct page fields.
+ 	 */
+ 	struct page *page = (struct page *) memmap_start;
+ 	struct dev_pagemap *pgmap;
+ 
+ 	/*
+ 	 * Uncoditionally retrieve a dev_pagemap associated with the
+ 	 * given physical address, this is only for use in the
+ 	 * arch_{add|remove}_memory() for setting up and tearing down
+ 	 * the memmap.
+ 	 */
+ 	rcu_read_lock();
+ 	pgmap = find_dev_pagemap(__pfn_to_phys(page_to_pfn(page)));
+ 	rcu_read_unlock();
+ 
+ 	return pgmap ? pgmap->altmap : NULL;
+ }
+ #endif /* CONFIG_SPARSEMEM_VMEMMAP */
+ #endif /* CONFIG_ZONE_DEVICE */
++>>>>>>> 5c2c2587b132 (mm, dax, pmem: introduce {get|put}_dev_pagemap() for dax-gup)
* Unmerged path drivers/nvdimm/pmem.c
* Unmerged path include/linux/memremap.h
* Unmerged path drivers/nvdimm/pmem.c
diff --git a/include/linux/list.h b/include/linux/list.h
index 0c034fcfc505..c4103ee0d74e 100644
--- a/include/linux/list.h
+++ b/include/linux/list.h
@@ -112,6 +112,17 @@ extern void __list_del_entry(struct list_head *entry);
 extern void list_del(struct list_head *entry);
 #endif
 
+#ifdef CONFIG_DEBUG_LIST
+/*
+ * See devm_memremap_pages() which wants DEBUG_LIST=y to assert if one
+ * of the pages it allocates is ever passed to list_add()
+ */
+extern void list_force_poison(struct list_head *entry);
+#else
+/* fallback to the less strict LIST_POISON* definitions */
+#define list_force_poison list_del
+#endif
+
 /**
  * list_replace - replace old entry by new one
  * @old : the element to be replaced
* Unmerged path include/linux/memremap.h
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 43f423b198b4..ddd76338ff3d 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -124,6 +124,11 @@ struct page {
 		struct list_head lru;	/* Pageout list, eg. active_list
 					 * protected by zone->lru_lock !
 					 */
+		struct dev_pagemap *pgmap; /* ZONE_DEVICE pages are never on an
+					    * lru or handled by a slab
+					    * allocator, this points to the
+					    * hosting device page map.
+					    */
 		struct {		/* slub per cpu partial pages */
 			struct page *next;	/* Next partial slab */
 #ifdef CONFIG_64BIT
* Unmerged path kernel/memremap.c
diff --git a/lib/list_debug.c b/lib/list_debug.c
index c24c2f7e296f..37fea7c0a7a4 100644
--- a/lib/list_debug.c
+++ b/lib/list_debug.c
@@ -12,6 +12,13 @@
 #include <linux/kernel.h>
 #include <linux/rculist.h>
 
+static struct list_head force_poison;
+void list_force_poison(struct list_head *entry)
+{
+	entry->next = &force_poison;
+	entry->prev = &force_poison;
+}
+
 /*
  * Insert a new entry between two known consecutive entries.
  *
@@ -23,6 +30,8 @@ void __list_add(struct list_head *new,
 			      struct list_head *prev,
 			      struct list_head *next)
 {
+	WARN(new->next == &force_poison || new->prev == &force_poison,
+		"list_add attempted on force-poisoned entry\n");
 	WARN(next->prev != prev,
 		"list_add corruption. next->prev should be "
 		"prev (%p), but was %p. (next=%p).\n",
