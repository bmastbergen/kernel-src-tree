crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [crypto] vmx - Adding enable_kernel_vsx() to access VSX instructions (Gustavo Duarte) [1274481]
Rebuild_FUZZ: 93.65%
commit-author Leonidas Da Silva Barbosa <leosilva@linux.vnet.ibm.com>
commit 2d6f0600b2cd755959527230ef5a6fba97bb762a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/2d6f0600.failed

vmx-crypto driver make use of some VSX instructions which are
only available if VSX is enabled. Running in cases where VSX
are not enabled vmx-crypto fails in a VSX exception.

In order to fix this enable_kernel_vsx() was added to turn on
VSX instructions for vmx-crypto.

	Signed-off-by: Leonidas S. Barbosa <leosilva@linux.vnet.ibm.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 2d6f0600b2cd755959527230ef5a6fba97bb762a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/vmx/aes.c
#	drivers/crypto/vmx/aes_cbc.c
#	drivers/crypto/vmx/aes_ctr.c
#	drivers/crypto/vmx/ghash.c
diff --cc drivers/crypto/vmx/aes.c
index a9064e36e7b5,263af709e536..000000000000
--- a/drivers/crypto/vmx/aes.c
+++ b/drivers/crypto/vmx/aes.c
@@@ -73,53 -76,56 +73,92 @@@ static void p8_aes_exit(struct crypto_t
  }
  
  static int p8_aes_setkey(struct crypto_tfm *tfm, const u8 *key,
 -			 unsigned int keylen)
 +    unsigned int keylen)
  {
 -	int ret;
 -	struct p8_aes_ctx *ctx = crypto_tfm_ctx(tfm);
 -
 +    int ret;
 +    struct p8_aes_ctx *ctx = crypto_tfm_ctx(tfm);
 +
++<<<<<<< HEAD
 +    preempt_disable();
 +    pagefault_disable();
 +    enable_kernel_altivec();
 +    ret = aes_p8_set_encrypt_key(key, keylen * 8, &ctx->enc_key);
 +    ret += aes_p8_set_decrypt_key(key, keylen * 8, &ctx->dec_key);
 +    pagefault_enable();
 +    preempt_enable();
++=======
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	enable_kernel_altivec();
+ 	enable_kernel_vsx();
+ 	ret = aes_p8_set_encrypt_key(key, keylen * 8, &ctx->enc_key);
+ 	ret += aes_p8_set_decrypt_key(key, keylen * 8, &ctx->dec_key);
+ 	pagefault_enable();
+ 	preempt_enable();
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  
 -	ret += crypto_cipher_setkey(ctx->fallback, key, keylen);
 -	return ret;
 +    ret += crypto_cipher_setkey(ctx->fallback, key, keylen);
 +    return ret;
  }
  
  static void p8_aes_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)
  {
 -	struct p8_aes_ctx *ctx = crypto_tfm_ctx(tfm);
 +    struct p8_aes_ctx *ctx = crypto_tfm_ctx(tfm);
  
++<<<<<<< HEAD
 +    if (in_interrupt()) {
 +        crypto_cipher_encrypt_one(ctx->fallback, dst, src);
 +    } else {
 +	preempt_disable();
 +        pagefault_disable();
 +        enable_kernel_altivec();
 +        aes_p8_encrypt(src, dst, &ctx->enc_key);
 +        pagefault_enable();
 +	preempt_enable();
 +    }
++=======
+ 	if (in_interrupt()) {
+ 		crypto_cipher_encrypt_one(ctx->fallback, dst, src);
+ 	} else {
+ 		preempt_disable();
+ 		pagefault_disable();
+ 		enable_kernel_altivec();
+ 		enable_kernel_vsx();
+ 		aes_p8_encrypt(src, dst, &ctx->enc_key);
+ 		pagefault_enable();
+ 		preempt_enable();
+ 	}
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  }
  
  static void p8_aes_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)
  {
 -	struct p8_aes_ctx *ctx = crypto_tfm_ctx(tfm);
 +    struct p8_aes_ctx *ctx = crypto_tfm_ctx(tfm);
  
++<<<<<<< HEAD
 +    if (in_interrupt()) {
 +        crypto_cipher_decrypt_one(ctx->fallback, dst, src);
 +    } else {
 +	preempt_disable();
 +        pagefault_disable();
 +        enable_kernel_altivec();
 +        aes_p8_decrypt(src, dst, &ctx->dec_key);
 +        pagefault_enable();
 +	preempt_enable();
 +    }
++=======
+ 	if (in_interrupt()) {
+ 		crypto_cipher_decrypt_one(ctx->fallback, dst, src);
+ 	} else {
+ 		preempt_disable();
+ 		pagefault_disable();
+ 		enable_kernel_altivec();
+ 		enable_kernel_vsx();
+ 		aes_p8_decrypt(src, dst, &ctx->dec_key);
+ 		pagefault_enable();
+ 		preempt_enable();
+ 	}
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  }
  
  struct crypto_alg p8_aes_alg = {
diff --cc drivers/crypto/vmx/aes_cbc.c
index 477284abdd11,0b8fe2ec5315..000000000000
--- a/drivers/crypto/vmx/aes_cbc.c
+++ b/drivers/crypto/vmx/aes_cbc.c
@@@ -74,86 -77,95 +74,119 @@@ static void p8_aes_cbc_exit(struct cryp
  }
  
  static int p8_aes_cbc_setkey(struct crypto_tfm *tfm, const u8 *key,
 -			     unsigned int keylen)
 +    unsigned int keylen)
  {
 -	int ret;
 -	struct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(tfm);
 -
 +    int ret;
 +    struct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(tfm);
 +
++<<<<<<< HEAD
 +    preempt_disable();
 +    pagefault_disable();
 +    enable_kernel_altivec();
 +    ret = aes_p8_set_encrypt_key(key, keylen * 8, &ctx->enc_key);
 +    ret += aes_p8_set_decrypt_key(key, keylen * 8, &ctx->dec_key);
 +    pagefault_enable();
 +    preempt_enable();
++=======
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	enable_kernel_altivec();
+ 	enable_kernel_vsx();
+ 	ret = aes_p8_set_encrypt_key(key, keylen * 8, &ctx->enc_key);
+ 	ret += aes_p8_set_decrypt_key(key, keylen * 8, &ctx->dec_key);
+ 	pagefault_enable();
+ 	preempt_enable();
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  
 -	ret += crypto_blkcipher_setkey(ctx->fallback, key, keylen);
 -	return ret;
 +    ret += crypto_blkcipher_setkey(ctx->fallback, key, keylen);
 +    return ret;
  }
  
  static int p8_aes_cbc_encrypt(struct blkcipher_desc *desc,
 -			      struct scatterlist *dst,
 -			      struct scatterlist *src, unsigned int nbytes)
 +    struct scatterlist *dst, struct scatterlist *src,
 +    unsigned int nbytes)
  {
 -	int ret;
 -	struct blkcipher_walk walk;
 -	struct p8_aes_cbc_ctx *ctx =
 -		crypto_tfm_ctx(crypto_blkcipher_tfm(desc->tfm));
 -	struct blkcipher_desc fallback_desc = {
 -		.tfm = ctx->fallback,
 -		.info = desc->info,
 -		.flags = desc->flags
 -	};
 -
 +    int ret;
 +    struct blkcipher_walk walk;
 +    struct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(
 +            crypto_blkcipher_tfm(desc->tfm));
 +    struct blkcipher_desc fallback_desc = {
 +        .tfm = ctx->fallback,
 +        .info = desc->info,
 +        .flags = desc->flags
 +    };
 +
++<<<<<<< HEAD
 +    if (in_interrupt()) {
 +        ret = crypto_blkcipher_encrypt(&fallback_desc, dst, src, nbytes);
 +    } else {
 +	preempt_disable();
 +        pagefault_disable();
 +        enable_kernel_altivec();
++=======
+ 	if (in_interrupt()) {
+ 		ret = crypto_blkcipher_encrypt(&fallback_desc, dst, src,
+ 					       nbytes);
+ 	} else {
+ 		preempt_disable();
+ 		pagefault_disable();
+ 		enable_kernel_altivec();
+ 		enable_kernel_vsx();
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  
 -		blkcipher_walk_init(&walk, dst, src, nbytes);
 -		ret = blkcipher_walk_virt(desc, &walk);
 -		while ((nbytes = walk.nbytes)) {
 -			aes_p8_cbc_encrypt(walk.src.virt.addr,
 -					   walk.dst.virt.addr,
 -					   nbytes & AES_BLOCK_MASK,
 -					   &ctx->enc_key, walk.iv, 1);
 +	blkcipher_walk_init(&walk, dst, src, nbytes);
 +        ret = blkcipher_walk_virt(desc, &walk);
 +        while ((nbytes = walk.nbytes)) {
 +			aes_p8_cbc_encrypt(walk.src.virt.addr, walk.dst.virt.addr,
 +				nbytes & AES_BLOCK_MASK, &ctx->enc_key, walk.iv, 1);
  			nbytes &= AES_BLOCK_SIZE - 1;
  			ret = blkcipher_walk_done(desc, &walk, nbytes);
 -		}
 -
 -		pagefault_enable();
 -		preempt_enable();
  	}
  
 -	return ret;
 +        pagefault_enable();
 +	preempt_enable();
 +    }
 +
 +    return ret;
  }
  
  static int p8_aes_cbc_decrypt(struct blkcipher_desc *desc,
 -			      struct scatterlist *dst,
 -			      struct scatterlist *src, unsigned int nbytes)
 +    struct scatterlist *dst, struct scatterlist *src,
 +    unsigned int nbytes)
  {
 -	int ret;
 -	struct blkcipher_walk walk;
 -	struct p8_aes_cbc_ctx *ctx =
 -		crypto_tfm_ctx(crypto_blkcipher_tfm(desc->tfm));
 -	struct blkcipher_desc fallback_desc = {
 -		.tfm = ctx->fallback,
 -		.info = desc->info,
 -		.flags = desc->flags
 -	};
 -
 +    int ret;
 +    struct blkcipher_walk walk;
 +    struct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(
 +            crypto_blkcipher_tfm(desc->tfm));
 +    struct blkcipher_desc fallback_desc = {
 +        .tfm = ctx->fallback,
 +        .info = desc->info,
 +        .flags = desc->flags
 +    };
 +
++<<<<<<< HEAD
 +    if (in_interrupt()) {
 +        ret = crypto_blkcipher_decrypt(&fallback_desc, dst, src, nbytes);
 +    } else {
 +	preempt_disable();
 +        pagefault_disable();
 +        enable_kernel_altivec();
++=======
+ 	if (in_interrupt()) {
+ 		ret = crypto_blkcipher_decrypt(&fallback_desc, dst, src,
+ 					       nbytes);
+ 	} else {
+ 		preempt_disable();
+ 		pagefault_disable();
+ 		enable_kernel_altivec();
+ 		enable_kernel_vsx();
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  
 -		blkcipher_walk_init(&walk, dst, src, nbytes);
 -		ret = blkcipher_walk_virt(desc, &walk);
 -		while ((nbytes = walk.nbytes)) {
 -			aes_p8_cbc_encrypt(walk.src.virt.addr,
 -					   walk.dst.virt.addr,
 -					   nbytes & AES_BLOCK_MASK,
 -					   &ctx->dec_key, walk.iv, 0);
 +	blkcipher_walk_init(&walk, dst, src, nbytes);
 +        ret = blkcipher_walk_virt(desc, &walk);
 +        while ((nbytes = walk.nbytes)) {
 +			aes_p8_cbc_encrypt(walk.src.virt.addr, walk.dst.virt.addr,
 +				nbytes & AES_BLOCK_MASK, &ctx->dec_key, walk.iv, 0);
  			nbytes &= AES_BLOCK_SIZE - 1;
  			ret = blkcipher_walk_done(desc, &walk, nbytes);
  		}
diff --cc drivers/crypto/vmx/aes_ctr.c
index 96dbee4bf4a6,1e754ae4e850..000000000000
--- a/drivers/crypto/vmx/aes_ctr.c
+++ b/drivers/crypto/vmx/aes_ctr.c
@@@ -72,76 -75,85 +72,113 @@@ static void p8_aes_ctr_exit(struct cryp
  }
  
  static int p8_aes_ctr_setkey(struct crypto_tfm *tfm, const u8 *key,
 -			     unsigned int keylen)
 +    unsigned int keylen)
  {
 -	int ret;
 -	struct p8_aes_ctr_ctx *ctx = crypto_tfm_ctx(tfm);
 -
 +    int ret;
 +    struct p8_aes_ctr_ctx *ctx = crypto_tfm_ctx(tfm);
 +
++<<<<<<< HEAD
 +    pagefault_disable();
 +    enable_kernel_altivec();
 +    ret = aes_p8_set_encrypt_key(key, keylen * 8, &ctx->enc_key);
 +    pagefault_enable();
++=======
+ 	pagefault_disable();
+ 	enable_kernel_altivec();
+ 	enable_kernel_vsx();
+ 	ret = aes_p8_set_encrypt_key(key, keylen * 8, &ctx->enc_key);
+ 	pagefault_enable();
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  
 -	ret += crypto_blkcipher_setkey(ctx->fallback, key, keylen);
 -	return ret;
 +    ret += crypto_blkcipher_setkey(ctx->fallback, key, keylen);
 +    return ret;
  }
  
  static void p8_aes_ctr_final(struct p8_aes_ctr_ctx *ctx,
 -			     struct blkcipher_walk *walk)
 +                struct blkcipher_walk *walk)
  {
 -	u8 *ctrblk = walk->iv;
 -	u8 keystream[AES_BLOCK_SIZE];
 -	u8 *src = walk->src.virt.addr;
 -	u8 *dst = walk->dst.virt.addr;
 -	unsigned int nbytes = walk->nbytes;
 -
 +    u8 *ctrblk = walk->iv;
 +    u8 keystream[AES_BLOCK_SIZE];
 +    u8 *src = walk->src.virt.addr;
 +    u8 *dst = walk->dst.virt.addr;
 +    unsigned int nbytes = walk->nbytes;
 +
++<<<<<<< HEAD
 +    pagefault_disable();
 +    enable_kernel_altivec();
 +    aes_p8_encrypt(ctrblk, keystream, &ctx->enc_key);
 +    pagefault_enable();
++=======
+ 	pagefault_disable();
+ 	enable_kernel_altivec();
+ 	enable_kernel_vsx();
+ 	aes_p8_encrypt(ctrblk, keystream, &ctx->enc_key);
+ 	pagefault_enable();
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  
 -	crypto_xor(keystream, src, nbytes);
 -	memcpy(dst, keystream, nbytes);
 -	crypto_inc(ctrblk, AES_BLOCK_SIZE);
 +    crypto_xor(keystream, src, nbytes);
 +    memcpy(dst, keystream, nbytes);
 +    crypto_inc(ctrblk, AES_BLOCK_SIZE);
  }
  
  static int p8_aes_ctr_crypt(struct blkcipher_desc *desc,
 -			    struct scatterlist *dst,
 -			    struct scatterlist *src, unsigned int nbytes)
 +    struct scatterlist *dst, struct scatterlist *src,
 +    unsigned int nbytes)
  {
 -	int ret;
 -	struct blkcipher_walk walk;
 -	struct p8_aes_ctr_ctx *ctx =
 -		crypto_tfm_ctx(crypto_blkcipher_tfm(desc->tfm));
 -	struct blkcipher_desc fallback_desc = {
 -		.tfm = ctx->fallback,
 -		.info = desc->info,
 -		.flags = desc->flags
 -	};
 -
 +    int ret;
 +    struct blkcipher_walk walk;
 +    struct p8_aes_ctr_ctx *ctx = crypto_tfm_ctx(
 +            crypto_blkcipher_tfm(desc->tfm));
 +    struct blkcipher_desc fallback_desc = {
 +        .tfm = ctx->fallback,
 +        .info = desc->info,
 +        .flags = desc->flags
 +    };
 +
++<<<<<<< HEAD
 +    if (in_interrupt()) {
 +        ret = crypto_blkcipher_encrypt(&fallback_desc, dst, src, nbytes);
 +    } else {
 +        blkcipher_walk_init(&walk, dst, src, nbytes);
 +        ret = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);
 +        while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
 +            pagefault_disable();
 +            enable_kernel_altivec();
 +            aes_p8_ctr32_encrypt_blocks(walk.src.virt.addr, walk.dst.virt.addr,
 +                (nbytes & AES_BLOCK_MASK)/AES_BLOCK_SIZE, &ctx->enc_key, walk.iv);
 +            pagefault_enable();
++=======
+ 	if (in_interrupt()) {
+ 		ret = crypto_blkcipher_encrypt(&fallback_desc, dst, src,
+ 					       nbytes);
+ 	} else {
+ 		blkcipher_walk_init(&walk, dst, src, nbytes);
+ 		ret = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);
+ 		while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
+ 			pagefault_disable();
+ 			enable_kernel_altivec();
+ 			enable_kernel_vsx();
+ 			aes_p8_ctr32_encrypt_blocks(walk.src.virt.addr,
+ 						    walk.dst.virt.addr,
+ 						    (nbytes &
+ 						     AES_BLOCK_MASK) /
+ 						    AES_BLOCK_SIZE,
+ 						    &ctx->enc_key,
+ 						    walk.iv);
+ 			pagefault_enable();
 -
 -			crypto_inc(walk.iv, AES_BLOCK_SIZE);
 -			nbytes &= AES_BLOCK_SIZE - 1;
 -			ret = blkcipher_walk_done(desc, &walk, nbytes);
 -		}
 -		if (walk.nbytes) {
 -			p8_aes_ctr_final(ctx, &walk);
 -			ret = blkcipher_walk_done(desc, &walk, 0);
 -		}
 -	}
 -
 -	return ret;
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
 +
 +            crypto_inc(walk.iv, AES_BLOCK_SIZE);
 +            nbytes &= AES_BLOCK_SIZE - 1;
 +            ret = blkcipher_walk_done(desc, &walk, nbytes);
 +        }
 +        if (walk.nbytes) {
 +            p8_aes_ctr_final(ctx, &walk);
 +            ret = blkcipher_walk_done(desc, &walk, 0);
 +        }
 +    }
 +
 +    return ret;
  }
  
  struct crypto_alg p8_aes_ctr_alg = {
diff --cc drivers/crypto/vmx/ghash.c
index f255ec4a04d4,2183a2e77641..000000000000
--- a/drivers/crypto/vmx/ghash.c
+++ b/drivers/crypto/vmx/ghash.c
@@@ -107,98 -109,104 +107,181 @@@ static int p8_ghash_init(struct shash_d
  }
  
  static int p8_ghash_setkey(struct crypto_shash *tfm, const u8 *key,
 -			   unsigned int keylen)
 +    unsigned int keylen)
  {
 -	struct p8_ghash_ctx *ctx = crypto_tfm_ctx(crypto_shash_tfm(tfm));
 +    struct p8_ghash_ctx *ctx = crypto_tfm_ctx(crypto_shash_tfm(tfm));
  
 -	if (keylen != GHASH_KEY_LEN)
 -		return -EINVAL;
 +    if (keylen != GHASH_KEY_LEN)
 +        return -EINVAL;
  
++<<<<<<< HEAD
 +    preempt_disable();
 +    pagefault_disable();
 +    enable_kernel_altivec();
 +    enable_kernel_fp();
 +    gcm_init_p8(ctx->htable, (const u64 *) key);
 +    pagefault_enable();
 +    preempt_enable();
 +    return crypto_shash_setkey(ctx->fallback, key, keylen);
++=======
+ 	preempt_disable();
+ 	pagefault_disable();
+ 	enable_kernel_altivec();
+ 	enable_kernel_vsx();
+ 	enable_kernel_fp();
+ 	gcm_init_p8(ctx->htable, (const u64 *) key);
+ 	pagefault_enable();
+ 	preempt_enable();
+ 	return crypto_shash_setkey(ctx->fallback, key, keylen);
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  }
  
  static int p8_ghash_update(struct shash_desc *desc,
 -			   const u8 *src, unsigned int srclen)
 +        const u8 *src, unsigned int srclen)
  {
 -	unsigned int len;
 -	struct p8_ghash_ctx *ctx = crypto_tfm_ctx(crypto_shash_tfm(desc->tfm));
 -	struct p8_ghash_desc_ctx *dctx = shash_desc_ctx(desc);
 +    unsigned int len;
 +    struct p8_ghash_ctx *ctx = crypto_tfm_ctx(crypto_shash_tfm(desc->tfm));
 +    struct p8_ghash_desc_ctx *dctx = shash_desc_ctx(desc);
  
++<<<<<<< HEAD
 +    if (IN_INTERRUPT) {
 +        return crypto_shash_update(&dctx->fallback_desc, src, srclen);
 +    } else {
 +        if (dctx->bytes) {
 +            if (dctx->bytes + srclen < GHASH_DIGEST_SIZE) {
 +                memcpy(dctx->buffer + dctx->bytes, src, srclen);
 +                dctx->bytes += srclen;
 +                return 0;
 +            }
 +            memcpy(dctx->buffer + dctx->bytes, src,
 +                    GHASH_DIGEST_SIZE - dctx->bytes);
 +	    preempt_disable();
 +            pagefault_disable();
 +            enable_kernel_altivec();
 +            enable_kernel_fp();
 +            gcm_ghash_p8(dctx->shash, ctx->htable, dctx->buffer,
 +                    GHASH_DIGEST_SIZE);
 +            pagefault_enable();
 +	    preempt_enable();
 +            src += GHASH_DIGEST_SIZE - dctx->bytes;
 +            srclen -= GHASH_DIGEST_SIZE - dctx->bytes;
 +            dctx->bytes = 0;
 +        }
 +        len = srclen & ~(GHASH_DIGEST_SIZE - 1);
 +        if (len) {
 +	    preempt_disable();
 +            pagefault_disable();
 +            enable_kernel_altivec();
 +            enable_kernel_fp();
 +            gcm_ghash_p8(dctx->shash, ctx->htable, src, len);
 +            pagefault_enable();
 +	    preempt_enable();
 +            src += len;
 +            srclen -= len;
 +        }
 +        if (srclen) {
 +            memcpy(dctx->buffer, src, srclen);
 +            dctx->bytes = srclen;
 +        }
 +        return 0;
 +    }
++=======
+ 	if (IN_INTERRUPT) {
+ 		return crypto_shash_update(&dctx->fallback_desc, src,
+ 					   srclen);
+ 	} else {
+ 		if (dctx->bytes) {
+ 			if (dctx->bytes + srclen < GHASH_DIGEST_SIZE) {
+ 				memcpy(dctx->buffer + dctx->bytes, src,
+ 				       srclen);
+ 				dctx->bytes += srclen;
+ 				return 0;
+ 			}
+ 			memcpy(dctx->buffer + dctx->bytes, src,
+ 			       GHASH_DIGEST_SIZE - dctx->bytes);
+ 			preempt_disable();
+ 			pagefault_disable();
+ 			enable_kernel_altivec();
+ 			enable_kernel_vsx();
+ 			enable_kernel_fp();
+ 			gcm_ghash_p8(dctx->shash, ctx->htable,
+ 				     dctx->buffer, GHASH_DIGEST_SIZE);
+ 			pagefault_enable();
+ 			preempt_enable();
+ 			src += GHASH_DIGEST_SIZE - dctx->bytes;
+ 			srclen -= GHASH_DIGEST_SIZE - dctx->bytes;
+ 			dctx->bytes = 0;
+ 		}
+ 		len = srclen & ~(GHASH_DIGEST_SIZE - 1);
+ 		if (len) {
+ 			preempt_disable();
+ 			pagefault_disable();
+ 			enable_kernel_altivec();
+ 			enable_kernel_vsx();
+ 			enable_kernel_fp();
+ 			gcm_ghash_p8(dctx->shash, ctx->htable, src, len);
+ 			pagefault_enable();
+ 			preempt_enable();
+ 			src += len;
+ 			srclen -= len;
+ 		}
+ 		if (srclen) {
+ 			memcpy(dctx->buffer, src, srclen);
+ 			dctx->bytes = srclen;
+ 		}
+ 		return 0;
+ 	}
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  }
  
  static int p8_ghash_final(struct shash_desc *desc, u8 *out)
  {
 -	int i;
 -	struct p8_ghash_ctx *ctx = crypto_tfm_ctx(crypto_shash_tfm(desc->tfm));
 -	struct p8_ghash_desc_ctx *dctx = shash_desc_ctx(desc);
 +    int i;
 +    struct p8_ghash_ctx *ctx = crypto_tfm_ctx(crypto_shash_tfm(desc->tfm));
 +    struct p8_ghash_desc_ctx *dctx = shash_desc_ctx(desc);
  
++<<<<<<< HEAD
 +    if (IN_INTERRUPT) {
 +        return crypto_shash_final(&dctx->fallback_desc, out);
 +    } else {
 +        if (dctx->bytes) {
 +            for (i = dctx->bytes; i < GHASH_DIGEST_SIZE; i++)
 +                dctx->buffer[i] = 0;
 +	    preempt_disable();
 +            pagefault_disable();
 +            enable_kernel_altivec();
 +            enable_kernel_fp();
 +            gcm_ghash_p8(dctx->shash, ctx->htable, dctx->buffer,
 +                    GHASH_DIGEST_SIZE);
 +            pagefault_enable();
 +	    preempt_enable();
 +            dctx->bytes = 0;
 +        }
 +        memcpy(out, dctx->shash, GHASH_DIGEST_SIZE);
 +        return 0;
 +    }
++=======
+ 	if (IN_INTERRUPT) {
+ 		return crypto_shash_final(&dctx->fallback_desc, out);
+ 	} else {
+ 		if (dctx->bytes) {
+ 			for (i = dctx->bytes; i < GHASH_DIGEST_SIZE; i++)
+ 				dctx->buffer[i] = 0;
+ 			preempt_disable();
+ 			pagefault_disable();
+ 			enable_kernel_altivec();
+ 			enable_kernel_vsx();
+ 			enable_kernel_fp();
+ 			gcm_ghash_p8(dctx->shash, ctx->htable,
+ 				     dctx->buffer, GHASH_DIGEST_SIZE);
+ 			pagefault_enable();
+ 			preempt_enable();
+ 			dctx->bytes = 0;
+ 		}
+ 		memcpy(out, dctx->shash, GHASH_DIGEST_SIZE);
+ 		return 0;
+ 	}
++>>>>>>> 2d6f0600b2cd (crypto: vmx - Adding enable_kernel_vsx() to access VSX instructions)
  }
  
  struct shash_alg p8_ghash_alg = {
* Unmerged path drivers/crypto/vmx/aes.c
* Unmerged path drivers/crypto/vmx/aes_cbc.c
* Unmerged path drivers/crypto/vmx/aes_ctr.c
* Unmerged path drivers/crypto/vmx/ghash.c
