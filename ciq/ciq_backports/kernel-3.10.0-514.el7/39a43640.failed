perf: Fix task context scheduling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 39a4364076921511e212bc42f94fbf062c989576
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/39a43640.failed

There is a very nasty problem wrt disabling the perf task scheduling
hooks.

Currently we {set,clear} ctx->is_active on every
__perf_event_task_sched_{in,out}, _however_ this means that if we
disable these calls we'll have task contexts with ->is_active set that
are not active and 'active' task contexts without ->is_active set.

This can result in event_function_call() looping on the ctx->is_active
condition basically indefinitely.

Resolve this by changing things such that contexts without events do
not set ->is_active like we used to. From this invariant it trivially
follows that if there are no (task) events, every task ctx is inactive
and disabling the context switch hooks is harmless.

This leaves two places that need attention (and already had
accumulated weird and wonderful hacks to work around, without
recognising this actual problem).

Namely:

 - perf_install_in_context() will need to deal with installing events
   in an inactive context, meaning it cannot rely on ctx-is_active for
   its IPIs.

 - perf_remove_from_context() will have to mark a context as inactive
   when it removes the last event.

For specific detail, see the patch/comments.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 39a4364076921511e212bc42f94fbf062c989576)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/events/core.c
diff --cc kernel/events/core.c
index 1a7853e13cec,c27e04655d86..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -2020,16 -2092,15 +2056,28 @@@ static void perf_event_sched_in(struct 
  		ctx_sched_in(ctx, cpuctx, EVENT_FLEXIBLE, task);
  }
  
++<<<<<<< HEAD
 +static void ___perf_install_in_context(void *info)
 +{
 +	struct perf_event *event = info;
 +	struct perf_event_context *ctx = event->ctx;
 +
 +	/*
 +	 * Since the task isn't running, its safe to add the event, us holding
 +	 * the ctx->lock ensures the task won't get scheduled in.
 +	 */
 +	add_event_to_ctx(event, ctx);
++=======
+ static void ctx_resched(struct perf_cpu_context *cpuctx,
+ 			struct perf_event_context *task_ctx)
+ {
+ 	perf_pmu_disable(cpuctx->ctx.pmu);
+ 	if (task_ctx)
+ 		task_ctx_sched_out(cpuctx, task_ctx);
+ 	cpu_ctx_sched_out(cpuctx, EVENT_ALL);
+ 	perf_event_sched_in(cpuctx, task_ctx, current);
+ 	perf_pmu_enable(cpuctx->ctx.pmu);
++>>>>>>> 39a436407692 (perf: Fix task context scheduling)
  }
  
  /*
@@@ -2039,29 -2110,22 +2087,39 @@@
   */
  static int  __perf_install_in_context(void *info)
  {
- 	struct perf_event *event = info;
- 	struct perf_event_context *ctx = event->ctx;
+ 	struct perf_event_context *ctx = info;
  	struct perf_cpu_context *cpuctx = __get_cpu_context(ctx);
  	struct perf_event_context *task_ctx = cpuctx->task_ctx;
- 	struct task_struct *task = current;
  
- 	perf_ctx_lock(cpuctx, task_ctx);
- 	perf_pmu_disable(cpuctx->ctx.pmu);
+ 	if (ctx->task) {
+ 		/*
+ 		 * If we hit the 'wrong' task, we've since scheduled and
+ 		 * everything should be sorted, nothing to do!
+ 		 */
+ 		if (ctx->task != current)
+ 			return 0;
  
++<<<<<<< HEAD
 +	/*
 +	 * If there was an active task_ctx schedule it out.
 +	 */
 +	if (task_ctx)
 +		task_ctx_sched_out(task_ctx);
 +
 +	/*
 +	 * If the context we're installing events in is not the
 +	 * active task_ctx, flip them.
 +	 */
 +	if (ctx->task && task_ctx != ctx) {
 +		if (task_ctx)
 +			raw_spin_unlock(&task_ctx->lock);
 +		raw_spin_lock(&ctx->lock);
++=======
+ 		/*
+ 		 * If task_ctx is set, it had better be to us.
+ 		 */
+ 		WARN_ON_ONCE(cpuctx->task_ctx != ctx && cpuctx->task_ctx);
++>>>>>>> 39a436407692 (perf: Fix task context scheduling)
  		task_ctx = ctx;
  	}
  
* Unmerged path kernel/events/core.c
