net: move netdev_pick_tx and dependencies to net/core/dev.c

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [net] move netdev_pick_tx and dependencies to net/core/dev.c (Ivan Vecera) [1268334]
Rebuild_FUZZ: 95.58%
commit-author Jiri Pirko <jiri@resnulli.us>
commit 638b2a699fd3ec926d6dda2d2bd96e8f1c49e463
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/638b2a69.failed

next to its user. No relation to flow_dissector so it makes no sense to
have it in flow_dissector.c

	Signed-off-by: Jiri Pirko <jiri@resnulli.us>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 638b2a699fd3ec926d6dda2d2bd96e8f1c49e463)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/flow_dissector.c
diff --cc net/core/flow_dissector.c
index 243f397103e5,04f87237bd64..000000000000
--- a/net/core/flow_dissector.c
+++ b/net/core/flow_dissector.c
@@@ -361,85 -431,3 +361,88 @@@ u32 skb_get_poff(const struct sk_buff *
  
  	return __skb_get_poff(skb, skb->data, &keys, skb_headlen(skb));
  }
++<<<<<<< HEAD
 +
 +static inline u16 dev_cap_txqueue(struct net_device *dev, u16 queue_index)
 +{
 +	if (unlikely(queue_index >= dev->real_num_tx_queues)) {
 +		net_warn_ratelimited("%s selects TX queue %d, but real number of TX queues is %d\n",
 +				     dev->name, queue_index,
 +				     dev->real_num_tx_queues);
 +		return 0;
 +	}
 +	return queue_index;
 +}
 +
 +static inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)
 +{
 +#ifdef CONFIG_XPS
 +	struct xps_dev_maps *dev_maps;
 +	struct xps_map *map;
 +	int queue_index = -1;
 +
 +	rcu_read_lock();
 +	dev_maps = rcu_dereference(dev->xps_maps);
 +	if (dev_maps) {
 +		map = rcu_dereference(
 +		    dev_maps->cpu_map[raw_smp_processor_id()]);
 +		if (map) {
 +			if (map->len == 1)
 +				queue_index = map->queues[0];
 +			else
 +				queue_index = map->queues[
 +				    ((u64)skb_get_hash(skb) * map->len) >> 32];
 +
 +			if (unlikely(queue_index >= dev->real_num_tx_queues))
 +				queue_index = -1;
 +		}
 +	}
 +	rcu_read_unlock();
 +
 +	return queue_index;
 +#else
 +	return -1;
 +#endif
 +}
 +
 +u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)
 +{
 +	struct sock *sk = skb->sk;
 +	int queue_index = sk_tx_queue_get(sk);
 +
 +	if (queue_index < 0 || skb->ooo_okay ||
 +	    queue_index >= dev->real_num_tx_queues) {
 +		int new_index = get_xps_queue(dev, skb);
 +		if (new_index < 0)
 +			new_index = skb_tx_hash(dev, skb);
 +
 +		if (queue_index != new_index && sk &&
 +		    rcu_access_pointer(sk->sk_dst_cache))
 +			sk_tx_queue_set(sk, new_index);
 +
 +		queue_index = new_index;
 +	}
 +
 +	return queue_index;
 +}
 +EXPORT_SYMBOL(__netdev_pick_tx);
 +
 +struct netdev_queue *netdev_pick_tx(struct net_device *dev,
 +				    struct sk_buff *skb)
 +{
 +	int queue_index = 0;
 +
 +	if (dev->real_num_tx_queues != 1) {
 +		const struct net_device_ops *ops = dev->netdev_ops;
 +		if (ops->ndo_select_queue)
 +			queue_index = ops->ndo_select_queue(dev, skb);
 +		else
 +			queue_index = __netdev_pick_tx(dev, skb);
 +		queue_index = dev_cap_txqueue(dev, queue_index);
 +	}
 +
 +	skb_set_queue_mapping(skb, queue_index);
 +	return netdev_get_tx_queue(dev, queue_index);
 +}
++=======
++>>>>>>> 638b2a699fd3 (net: move netdev_pick_tx and dependencies to net/core/dev.c)
diff --git a/net/core/dev.c b/net/core/dev.c
index b52e68f2dd45..5d25f3959f2b 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -2768,6 +2768,84 @@ int dev_loopback_xmit(struct sock *sk, struct sk_buff *skb)
 }
 EXPORT_SYMBOL(dev_loopback_xmit);
 
+static inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)
+{
+#ifdef CONFIG_XPS
+	struct xps_dev_maps *dev_maps;
+	struct xps_map *map;
+	int queue_index = -1;
+
+	rcu_read_lock();
+	dev_maps = rcu_dereference(dev->xps_maps);
+	if (dev_maps) {
+		map = rcu_dereference(
+		    dev_maps->cpu_map[skb->sender_cpu - 1]);
+		if (map) {
+			if (map->len == 1)
+				queue_index = map->queues[0];
+			else
+				queue_index = map->queues[reciprocal_scale(skb_get_hash(skb),
+									   map->len)];
+			if (unlikely(queue_index >= dev->real_num_tx_queues))
+				queue_index = -1;
+		}
+	}
+	rcu_read_unlock();
+
+	return queue_index;
+#else
+	return -1;
+#endif
+}
+
+static u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)
+{
+	struct sock *sk = skb->sk;
+	int queue_index = sk_tx_queue_get(sk);
+
+	if (queue_index < 0 || skb->ooo_okay ||
+	    queue_index >= dev->real_num_tx_queues) {
+		int new_index = get_xps_queue(dev, skb);
+		if (new_index < 0)
+			new_index = skb_tx_hash(dev, skb);
+
+		if (queue_index != new_index && sk &&
+		    rcu_access_pointer(sk->sk_dst_cache))
+			sk_tx_queue_set(sk, new_index);
+
+		queue_index = new_index;
+	}
+
+	return queue_index;
+}
+
+struct netdev_queue *netdev_pick_tx(struct net_device *dev,
+				    struct sk_buff *skb,
+				    void *accel_priv)
+{
+	int queue_index = 0;
+
+#ifdef CONFIG_XPS
+	if (skb->sender_cpu == 0)
+		skb->sender_cpu = raw_smp_processor_id() + 1;
+#endif
+
+	if (dev->real_num_tx_queues != 1) {
+		const struct net_device_ops *ops = dev->netdev_ops;
+		if (ops->ndo_select_queue)
+			queue_index = ops->ndo_select_queue(dev, skb, accel_priv,
+							    __netdev_pick_tx);
+		else
+			queue_index = __netdev_pick_tx(dev, skb);
+
+		if (!accel_priv)
+			queue_index = netdev_cap_txqueue(dev, queue_index);
+	}
+
+	skb_set_queue_mapping(skb, queue_index);
+	return netdev_get_tx_queue(dev, queue_index);
+}
+
 /**
  *	dev_queue_xmit - transmit a buffer
  *	@skb: buffer to transmit
* Unmerged path net/core/flow_dissector.c
