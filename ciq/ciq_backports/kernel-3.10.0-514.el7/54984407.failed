net/mlx5e: Add ethtool counter for RX buffer allocation failures

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Add ethtool counter for RX buffer allocation failures (kamal heib) [1275159 1296272 1296405 1298421 1298422 1298423 1298424 1298425]
Rebuild_FUZZ: 96.77%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit 54984407564ef6b35488f52654f828c17b9d6fa8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/54984407.failed

Counts the number of RX buffer allocation failures and shows it
in ethtool statistics.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 54984407564ef6b35488f52654f828c17b9d6fa8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index a33b9e6fa485,6e24e821a1d8..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -105,6 -187,9 +105,12 @@@ static const char vport_strings[][ETH_G
  	"tx_queue_wake",
  	"tx_queue_dropped",
  	"rx_wqe_err",
++<<<<<<< HEAD
++=======
+ 	"rx_mpwqe_filler",
+ 	"rx_mpwqe_frag",
+ 	"rx_buff_alloc_err",
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  };
  
  struct mlx5e_vport_stats {
@@@ -143,8 -231,11 +149,16 @@@
  	u64 tx_queue_wake;
  	u64 tx_queue_dropped;
  	u64 rx_wqe_err;
++<<<<<<< HEAD
 +
 +#define NUM_VPORT_COUNTERS     32
++=======
+ 	u64 rx_mpwqe_filler;
+ 	u64 rx_mpwqe_frag;
+ 	u64 rx_buff_alloc_err;
+ 
+ #define NUM_VPORT_COUNTERS     38
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  };
  
  static const char pport_strings[][ETH_GSTRING_LEN] = {
@@@ -227,7 -328,10 +241,14 @@@ static const char rq_stats_strings[][ET
  	"csum_sw",
  	"lro_packets",
  	"lro_bytes",
++<<<<<<< HEAD
 +	"wqe_err"
++=======
+ 	"wqe_err",
+ 	"mpwqe_filler",
+ 	"mpwqe_frag",
+ 	"buff_alloc_err",
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  };
  
  struct mlx5e_rq_stats {
@@@ -237,7 -342,10 +258,14 @@@
  	u64 lro_packets;
  	u64 lro_bytes;
  	u64 wqe_err;
++<<<<<<< HEAD
 +#define NUM_RQ_STATS 6
++=======
+ 	u64 mpwqe_filler;
+ 	u64 mpwqe_frag;
+ 	u64 buff_alloc_err;
+ #define NUM_RQ_STATS 10
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  };
  
  static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index a8ca30e0bf3d,d485d1e4e100..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -159,6 -178,9 +159,12 @@@ void mlx5e_update_stats(struct mlx5e_pr
  	s->rx_csum_none		= 0;
  	s->rx_csum_sw		= 0;
  	s->rx_wqe_err		= 0;
++<<<<<<< HEAD
++=======
+ 	s->rx_mpwqe_filler	= 0;
+ 	s->rx_mpwqe_frag	= 0;
+ 	s->rx_buff_alloc_err	= 0;
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  	for (i = 0; i < priv->params.num_channels; i++) {
  		rq_stats = &priv->channel[i]->rq.stats;
  
@@@ -169,6 -191,9 +175,12 @@@
  		s->rx_csum_none	+= rq_stats->csum_none;
  		s->rx_csum_sw	+= rq_stats->csum_sw;
  		s->rx_wqe_err   += rq_stats->wqe_err;
++<<<<<<< HEAD
++=======
+ 		s->rx_mpwqe_filler += rq_stats->mpwqe_filler;
+ 		s->rx_mpwqe_frag   += rq_stats->mpwqe_frag;
+ 		s->rx_buff_alloc_err += rq_stats->buff_alloc_err;
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  
  		for (j = 0; j < priv->params.num_tc; j++) {
  			sq_stats = &priv->channel[i]->sq[j].stats;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 2dc1f9b26b65,918b7c7fd74f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -81,9 -447,14 +81,18 @@@ bool mlx5e_post_rx_wqes(struct mlx5e_r
  
  	while (!mlx5_wq_ll_is_full(wq)) {
  		struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
+ 		int err;
  
++<<<<<<< HEAD
 +		if (unlikely(mlx5e_alloc_rx_wqe(rq, wqe, wq->head)))
++=======
+ 		err = rq->alloc_wqe(rq, wqe, wq->head);
+ 		if (unlikely(err)) {
+ 			if (err != -EBUSY)
+ 				rq->stats.buff_alloc_err++;
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  			break;
+ 		}
  
  		mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
  	}
@@@ -215,6 -586,143 +224,146 @@@ static inline void mlx5e_build_rx_skb(s
  	if (cqe_has_vlan(cqe))
  		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
  				       be16_to_cpu(cqe->vlan_info));
++<<<<<<< HEAD
++=======
+ 
+ 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
+ 
+ 	mlx5e_handle_csum(netdev, cqe, rq, skb, !!lro_num_seg);
+ 	skb->protocol = eth_type_trans(skb, netdev);
+ }
+ 
+ static inline void mlx5e_complete_rx_cqe(struct mlx5e_rq *rq,
+ 					 struct mlx5_cqe64 *cqe,
+ 					 u32 cqe_bcnt,
+ 					 struct sk_buff *skb)
+ {
+ 	rq->stats.packets++;
+ 	rq->stats.bytes += cqe_bcnt;
+ 	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ }
+ 
+ void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_rx_wqe *wqe;
+ 	struct sk_buff *skb;
+ 	__be16 wqe_counter_be;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	skb            = rq->skb[wqe_counter];
+ 	prefetch(skb->data);
+ 	rq->skb[wqe_counter] = NULL;
+ 
+ 	dma_unmap_single(rq->pdev,
+ 			 *((dma_addr_t *)skb->cb),
+ 			 rq->wqe_sz,
+ 			 DMA_FROM_DEVICE);
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		dev_kfree_skb(skb);
+ 		goto wq_ll_pop;
+ 	}
+ 
+ 	cqe_bcnt = be32_to_cpu(cqe->byte_cnt);
+ 	skb_put(skb, cqe_bcnt);
+ 
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ wq_ll_pop:
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ static inline void mlx5e_mpwqe_fill_rx_skb(struct mlx5e_rq *rq,
+ 					   struct mlx5_cqe64 *cqe,
+ 					   struct mlx5e_mpw_info *wi,
+ 					   u32 cqe_bcnt,
+ 					   struct sk_buff *skb)
+ {
+ 	u32 consumed_bytes = ALIGN(cqe_bcnt, MLX5_MPWRQ_STRIDE_SIZE);
+ 	u16 stride_ix      = mpwrq_get_cqe_stride_index(cqe);
+ 	u32 wqe_offset     = stride_ix * MLX5_MPWRQ_STRIDE_SIZE;
+ 	u32 head_offset    = wqe_offset & (PAGE_SIZE - 1);
+ 	u32 page_idx       = wqe_offset >> PAGE_SHIFT;
+ 	u32 head_page_idx  = page_idx;
+ 	u16 headlen = min_t(u16, MLX5_MPWRQ_SMALL_PACKET_THRESHOLD, cqe_bcnt);
+ 	u32 frag_offset    = head_offset + headlen;
+ 	u16 byte_cnt       = cqe_bcnt - headlen;
+ 
+ #if (MLX5_MPWRQ_SMALL_PACKET_THRESHOLD >= MLX5_MPWRQ_STRIDE_SIZE)
+ 	if (unlikely(frag_offset >= PAGE_SIZE)) {
+ 		page_idx++;
+ 		frag_offset -= PAGE_SIZE;
+ 	}
+ #endif
+ 	wi->dma_pre_sync(rq->pdev, wi, wqe_offset, consumed_bytes);
+ 
+ 	while (byte_cnt) {
+ 		u32 pg_consumed_bytes =
+ 			min_t(u32, PAGE_SIZE - frag_offset, byte_cnt);
+ 
+ 		wi->add_skb_frag(rq->pdev, skb, wi, page_idx, frag_offset,
+ 				 pg_consumed_bytes);
+ 		byte_cnt -= pg_consumed_bytes;
+ 		frag_offset = 0;
+ 		page_idx++;
+ 	}
+ 	/* copy header */
+ 	wi->copy_skb_header(rq->pdev, skb, wi, head_page_idx, head_offset,
+ 			    headlen);
+ 	/* skb linear part was allocated with headlen and aligned to long */
+ 	skb->tail += headlen;
+ 	skb->len  += headlen;
+ }
+ 
+ void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
+ 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
+ 	struct mlx5e_mpw_info *wi = &rq->wqe_info[wqe_id];
+ 	struct mlx5e_rx_wqe  *wqe = mlx5_wq_ll_get_wqe(&rq->wq, wqe_id);
+ 	struct sk_buff *skb;
+ 	u16 cqe_bcnt;
+ 
+ 	wi->consumed_strides += cstrides;
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	if (unlikely(mpwrq_is_filler_cqe(cqe))) {
+ 		rq->stats.mpwqe_filler++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	skb = napi_alloc_skb(rq->cq.napi,
+ 			     ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
+ 				   sizeof(long)));
+ 	if (unlikely(!skb)) {
+ 		rq->stats.buff_alloc_err++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	prefetch(skb->data);
+ 	cqe_bcnt = mpwrq_get_cqe_byte_cnt(cqe);
+ 
+ 	mlx5e_mpwqe_fill_rx_skb(rq, cqe, wi, cqe_bcnt, skb);
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ mpwrq_cqe_out:
+ 	if (likely(wi->consumed_strides < MLX5_MPWRQ_NUM_STRIDES))
+ 		return;
+ 
+ 	wi->free_wqe(rq, wi);
+ 	mlx5_wq_ll_pop(&rq->wq, cqe->wqe_id, &wqe->next.next_wqe_index);
++>>>>>>> 54984407564e (net/mlx5e: Add ethtool counter for RX buffer allocation failures)
  }
  
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
