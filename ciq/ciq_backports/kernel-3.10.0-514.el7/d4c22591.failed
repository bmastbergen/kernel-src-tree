perf tools: Add stat round event synthesize function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jiri Olsa <jolsa@kernel.org>
commit d4c2259195f538505d2570e78555532372fb4ad2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/d4c22591.failed

Introduce the perf_event__synthesize_stat_round function to
synthesize a 'struct stat_round_event'.

	Signed-off-by: Jiri Olsa <jolsa@kernel.org>
	Tested-by: Kan Liang <kan.liang@intel.com>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
Link: http://lkml.kernel.org/r/1445784728-21732-19-git-send-email-jolsa@kernel.org
[ Renamed 'time' parameter to 'evtime' to fix build on older systems ]
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit d4c2259195f538505d2570e78555532372fb4ad2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/perf/tests/builtin-test.c
#	tools/perf/tests/stat.c
#	tools/perf/tests/tests.h
#	tools/perf/util/event.c
#	tools/perf/util/event.h
diff --cc tools/perf/tests/builtin-test.c
index 2337c002804d,6a3519814492..000000000000
--- a/tools/perf/tests/builtin-test.c
+++ b/tools/perf/tests/builtin-test.c
@@@ -178,6 -158,48 +178,51 @@@ static struct test generic_tests[] = 
  		.func = test__thread_map,
  	},
  	{
++<<<<<<< HEAD
++=======
+ 		.desc = "Test LLVM searching and compiling",
+ 		.func = test__llvm,
+ 		.subtest = {
+ 			.skip_if_fail	= true,
+ 			.get_nr		= test__llvm_subtest_get_nr,
+ 			.get_desc	= test__llvm_subtest_get_desc,
+ 		},
+ 	},
+ 	{
+ 		.desc = "Test topology in session",
+ 		.func = test_session_topology,
+ 	},
+ 	{
+ 		.desc = "Test BPF filter",
+ 		.func = test__bpf,
+ 		.subtest = {
+ 			.skip_if_fail	= true,
+ 			.get_nr		= test__bpf_subtest_get_nr,
+ 			.get_desc	= test__bpf_subtest_get_desc,
+ 		},
+ 	},
+ 	{
+ 		.desc = "Test thread map synthesize",
+ 		.func = test__thread_map_synthesize,
+ 	},
+ 	{
+ 		.desc = "Test cpu map synthesize",
+ 		.func = test__cpu_map_synthesize,
+ 	},
+ 	{
+ 		.desc = "Test stat config synthesize",
+ 		.func = test__synthesize_stat_config,
+ 	},
+ 	{
+ 		.desc = "Test stat synthesize",
+ 		.func = test__synthesize_stat,
+ 	},
+ 	{
+ 		.desc = "Test stat round synthesize",
+ 		.func = test__synthesize_stat_round,
+ 	},
+ 	{
++>>>>>>> d4c2259195f5 (perf tools: Add stat round event synthesize function)
  		.func = NULL,
  	},
  };
diff --cc tools/perf/tests/tests.h
index a08027794fa0,a82ab9c4c0ca..000000000000
--- a/tools/perf/tests/tests.h
+++ b/tools/perf/tests/tests.h
@@@ -30,45 -37,55 +30,95 @@@ struct test 
  };
  
  /* Tests */
++<<<<<<< HEAD
 +int test__vmlinux_matches_kallsyms(void);
 +int test__openat_syscall_event(void);
 +int test__openat_syscall_event_on_all_cpus(void);
 +int test__basic_mmap(void);
 +int test__PERF_RECORD(void);
 +int test__rdpmc(void);
 +int test__perf_evsel__roundtrip_name_test(void);
 +int test__perf_evsel__tp_sched_test(void);
 +int test__syscall_openat_tp_fields(void);
 +int test__pmu(void);
 +int test__attr(void);
 +int test__dso_data(void);
 +int test__dso_data_cache(void);
 +int test__dso_data_reopen(void);
 +int test__parse_events(void);
 +int test__hists_link(void);
 +int test__python_use(void);
 +int test__bp_signal(void);
 +int test__bp_signal_overflow(void);
 +int test__task_exit(void);
 +int test__sw_clock_freq(void);
 +int test__perf_time_to_tsc(void);
 +int test__code_reading(void);
 +int test__sample_parsing(void);
 +int test__keep_tracking(void);
 +int test__parse_no_sample_id_all(void);
 +int test__dwarf_unwind(void);
 +int test__hists_filter(void);
 +int test__mmap_thread_lookup(void);
 +int test__thread_mg_share(void);
 +int test__hists_output(void);
 +int test__hists_cumulate(void);
 +int test__switch_tracking(void);
 +int test__fdarray__filter(void);
 +int test__fdarray__add(void);
 +int test__kmod_path__parse(void);
 +int test__thread_map(void);
++=======
+ int test__vmlinux_matches_kallsyms(int subtest);
+ int test__openat_syscall_event(int subtest);
+ int test__openat_syscall_event_on_all_cpus(int subtest);
+ int test__basic_mmap(int subtest);
+ int test__PERF_RECORD(int subtest);
+ int test__perf_evsel__roundtrip_name_test(int subtest);
+ int test__perf_evsel__tp_sched_test(int subtest);
+ int test__syscall_openat_tp_fields(int subtest);
+ int test__pmu(int subtest);
+ int test__attr(int subtest);
+ int test__dso_data(int subtest);
+ int test__dso_data_cache(int subtest);
+ int test__dso_data_reopen(int subtest);
+ int test__parse_events(int subtest);
+ int test__hists_link(int subtest);
+ int test__python_use(int subtest);
+ int test__bp_signal(int subtest);
+ int test__bp_signal_overflow(int subtest);
+ int test__task_exit(int subtest);
+ int test__sw_clock_freq(int subtest);
+ int test__code_reading(int subtest);
+ int test__sample_parsing(int subtest);
+ int test__keep_tracking(int subtest);
+ int test__parse_no_sample_id_all(int subtest);
+ int test__dwarf_unwind(int subtest);
+ int test__hists_filter(int subtest);
+ int test__mmap_thread_lookup(int subtest);
+ int test__thread_mg_share(int subtest);
+ int test__hists_output(int subtest);
+ int test__hists_cumulate(int subtest);
+ int test__switch_tracking(int subtest);
+ int test__fdarray__filter(int subtest);
+ int test__fdarray__add(int subtest);
+ int test__kmod_path__parse(int subtest);
+ int test__thread_map(int subtest);
+ int test__llvm(int subtest);
+ const char *test__llvm_subtest_get_desc(int subtest);
+ int test__llvm_subtest_get_nr(void);
+ int test__bpf(int subtest);
+ const char *test__bpf_subtest_get_desc(int subtest);
+ int test__bpf_subtest_get_nr(void);
+ int test_session_topology(int subtest);
+ int test__thread_map_synthesize(int subtest);
+ int test__cpu_map_synthesize(int subtest);
+ int test__synthesize_stat_config(int subtest);
+ int test__synthesize_stat(int subtest);
+ int test__synthesize_stat_round(int subtest);
++>>>>>>> d4c2259195f5 (perf tools: Add stat round event synthesize function)
  
 -#if defined(__arm__) || defined(__aarch64__)
 +#if defined(__x86_64__) || defined(__i386__) || defined(__arm__) || defined(__aarch64__)
  #ifdef HAVE_DWARF_UNWIND_SUPPORT
  struct thread;
  struct perf_sample;
diff --cc tools/perf/util/event.c
index 763fbcde76fc,e4c68ba79974..000000000000
--- a/tools/perf/util/event.c
+++ b/tools/perf/util/event.c
@@@ -687,6 -706,274 +687,277 @@@ int perf_event__synthesize_kernel_mmap(
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ int perf_event__synthesize_thread_map2(struct perf_tool *tool,
+ 				      struct thread_map *threads,
+ 				      perf_event__handler_t process,
+ 				      struct machine *machine)
+ {
+ 	union perf_event *event;
+ 	int i, err, size;
+ 
+ 	size  = sizeof(event->thread_map);
+ 	size +=	threads->nr * sizeof(event->thread_map.entries[0]);
+ 
+ 	event = zalloc(size);
+ 	if (!event)
+ 		return -ENOMEM;
+ 
+ 	event->header.type = PERF_RECORD_THREAD_MAP;
+ 	event->header.size = size;
+ 	event->thread_map.nr = threads->nr;
+ 
+ 	for (i = 0; i < threads->nr; i++) {
+ 		struct thread_map_event_entry *entry = &event->thread_map.entries[i];
+ 		char *comm = thread_map__comm(threads, i);
+ 
+ 		if (!comm)
+ 			comm = (char *) "";
+ 
+ 		entry->pid = thread_map__pid(threads, i);
+ 		strncpy((char *) &entry->comm, comm, sizeof(entry->comm));
+ 	}
+ 
+ 	err = process(tool, event, NULL, machine);
+ 
+ 	free(event);
+ 	return err;
+ }
+ 
+ static void synthesize_cpus(struct cpu_map_entries *cpus,
+ 			    struct cpu_map *map)
+ {
+ 	int i;
+ 
+ 	cpus->nr = map->nr;
+ 
+ 	for (i = 0; i < map->nr; i++)
+ 		cpus->cpu[i] = map->map[i];
+ }
+ 
+ static void synthesize_mask(struct cpu_map_mask *mask,
+ 			    struct cpu_map *map, int max)
+ {
+ 	int i;
+ 
+ 	mask->nr = BITS_TO_LONGS(max);
+ 	mask->long_size = sizeof(long);
+ 
+ 	for (i = 0; i < map->nr; i++)
+ 		set_bit(map->map[i], mask->mask);
+ }
+ 
+ static size_t cpus_size(struct cpu_map *map)
+ {
+ 	return sizeof(struct cpu_map_entries) + map->nr * sizeof(u16);
+ }
+ 
+ static size_t mask_size(struct cpu_map *map, int *max)
+ {
+ 	int i;
+ 
+ 	*max = 0;
+ 
+ 	for (i = 0; i < map->nr; i++) {
+ 		/* bit possition of the cpu is + 1 */
+ 		int bit = map->map[i] + 1;
+ 
+ 		if (bit > *max)
+ 			*max = bit;
+ 	}
+ 
+ 	return sizeof(struct cpu_map_mask) + BITS_TO_LONGS(*max) * sizeof(long);
+ }
+ 
+ void *cpu_map_data__alloc(struct cpu_map *map, size_t *size, u16 *type, int *max)
+ {
+ 	size_t size_cpus, size_mask;
+ 	bool is_dummy = cpu_map__empty(map);
+ 
+ 	/*
+ 	 * Both array and mask data have variable size based
+ 	 * on the number of cpus and their actual values.
+ 	 * The size of the 'struct cpu_map_data' is:
+ 	 *
+ 	 *   array = size of 'struct cpu_map_entries' +
+ 	 *           number of cpus * sizeof(u64)
+ 	 *
+ 	 *   mask  = size of 'struct cpu_map_mask' +
+ 	 *           maximum cpu bit converted to size of longs
+ 	 *
+ 	 * and finaly + the size of 'struct cpu_map_data'.
+ 	 */
+ 	size_cpus = cpus_size(map);
+ 	size_mask = mask_size(map, max);
+ 
+ 	if (is_dummy || (size_cpus < size_mask)) {
+ 		*size += size_cpus;
+ 		*type  = PERF_CPU_MAP__CPUS;
+ 	} else {
+ 		*size += size_mask;
+ 		*type  = PERF_CPU_MAP__MASK;
+ 	}
+ 
+ 	*size += sizeof(struct cpu_map_data);
+ 	return zalloc(*size);
+ }
+ 
+ void cpu_map_data__synthesize(struct cpu_map_data *data, struct cpu_map *map,
+ 			      u16 type, int max)
+ {
+ 	data->type = type;
+ 
+ 	switch (type) {
+ 	case PERF_CPU_MAP__CPUS:
+ 		synthesize_cpus((struct cpu_map_entries *) data->data, map);
+ 		break;
+ 	case PERF_CPU_MAP__MASK:
+ 		synthesize_mask((struct cpu_map_mask *) data->data, map, max);
+ 	default:
+ 		break;
+ 	};
+ }
+ 
+ static struct cpu_map_event* cpu_map_event__new(struct cpu_map *map)
+ {
+ 	size_t size = sizeof(struct cpu_map_event);
+ 	struct cpu_map_event *event;
+ 	int max;
+ 	u16 type;
+ 
+ 	event = cpu_map_data__alloc(map, &size, &type, &max);
+ 	if (!event)
+ 		return NULL;
+ 
+ 	event->header.type = PERF_RECORD_CPU_MAP;
+ 	event->header.size = size;
+ 	event->data.type   = type;
+ 
+ 	cpu_map_data__synthesize(&event->data, map, type, max);
+ 	return event;
+ }
+ 
+ int perf_event__synthesize_cpu_map(struct perf_tool *tool,
+ 				   struct cpu_map *map,
+ 				   perf_event__handler_t process,
+ 				   struct machine *machine)
+ {
+ 	struct cpu_map_event *event;
+ 	int err;
+ 
+ 	event = cpu_map_event__new(map);
+ 	if (!event)
+ 		return -ENOMEM;
+ 
+ 	err = process(tool, (union perf_event *) event, NULL, machine);
+ 
+ 	free(event);
+ 	return err;
+ }
+ 
+ int perf_event__synthesize_stat_config(struct perf_tool *tool,
+ 				       struct perf_stat_config *config,
+ 				       perf_event__handler_t process,
+ 				       struct machine *machine)
+ {
+ 	struct stat_config_event *event;
+ 	int size, i = 0, err;
+ 
+ 	size  = sizeof(*event);
+ 	size += (PERF_STAT_CONFIG_TERM__MAX * sizeof(event->data[0]));
+ 
+ 	event = zalloc(size);
+ 	if (!event)
+ 		return -ENOMEM;
+ 
+ 	event->header.type = PERF_RECORD_STAT_CONFIG;
+ 	event->header.size = size;
+ 	event->nr          = PERF_STAT_CONFIG_TERM__MAX;
+ 
+ #define ADD(__term, __val)					\
+ 	event->data[i].tag = PERF_STAT_CONFIG_TERM__##__term;	\
+ 	event->data[i].val = __val;				\
+ 	i++;
+ 
+ 	ADD(AGGR_MODE,	config->aggr_mode)
+ 	ADD(INTERVAL,	config->interval)
+ 	ADD(SCALE,	config->scale)
+ 
+ 	WARN_ONCE(i != PERF_STAT_CONFIG_TERM__MAX,
+ 		  "stat config terms unbalanced\n");
+ #undef ADD
+ 
+ 	err = process(tool, (union perf_event *) event, NULL, machine);
+ 
+ 	free(event);
+ 	return err;
+ }
+ 
+ int perf_event__synthesize_stat(struct perf_tool *tool,
+ 				u32 cpu, u32 thread, u64 id,
+ 				struct perf_counts_values *count,
+ 				perf_event__handler_t process,
+ 				struct machine *machine)
+ {
+ 	struct stat_event event;
+ 
+ 	event.header.type = PERF_RECORD_STAT;
+ 	event.header.size = sizeof(event);
+ 	event.header.misc = 0;
+ 
+ 	event.id        = id;
+ 	event.cpu       = cpu;
+ 	event.thread    = thread;
+ 	event.val       = count->val;
+ 	event.ena       = count->ena;
+ 	event.run       = count->run;
+ 
+ 	return process(tool, (union perf_event *) &event, NULL, machine);
+ }
+ 
+ int perf_event__synthesize_stat_round(struct perf_tool *tool,
+ 				      u64 evtime, u64 type,
+ 				      perf_event__handler_t process,
+ 				      struct machine *machine)
+ {
+ 	struct stat_round_event event;
+ 
+ 	event.header.type = PERF_RECORD_STAT_ROUND;
+ 	event.header.size = sizeof(event);
+ 	event.header.misc = 0;
+ 
+ 	event.time = evtime;
+ 	event.type = type;
+ 
+ 	return process(tool, (union perf_event *) &event, NULL, machine);
+ }
+ 
+ void perf_event__read_stat_config(struct perf_stat_config *config,
+ 				  struct stat_config_event *event)
+ {
+ 	unsigned i;
+ 
+ 	for (i = 0; i < event->nr; i++) {
+ 
+ 		switch (event->data[i].tag) {
+ #define CASE(__term, __val)					\
+ 		case PERF_STAT_CONFIG_TERM__##__term:		\
+ 			config->__val = event->data[i].val;	\
+ 			break;
+ 
+ 		CASE(AGGR_MODE, aggr_mode)
+ 		CASE(SCALE,     scale)
+ 		CASE(INTERVAL,  interval)
+ #undef CASE
+ 		default:
+ 			pr_warning("unknown stat config term %" PRIu64 "\n",
+ 				   event->data[i].tag);
+ 		}
+ 	}
+ }
+ 
++>>>>>>> d4c2259195f5 (perf tools: Add stat round event synthesize function)
  size_t perf_event__fprintf_comm(union perf_event *event, FILE *fp)
  {
  	const char *s;
diff --cc tools/perf/util/event.h
index 9400ef1c1335,1afaa21eeebe..000000000000
--- a/tools/perf/util/event.h
+++ b/tools/perf/util/event.h
@@@ -393,7 -506,21 +393,25 @@@ int perf_event__synthesize_threads(stru
  int perf_event__synthesize_kernel_mmap(struct perf_tool *tool,
  				       perf_event__handler_t process,
  				       struct machine *machine);
++<<<<<<< HEAD
 +
++=======
+ int perf_event__synthesize_stat_config(struct perf_tool *tool,
+ 				       struct perf_stat_config *config,
+ 				       perf_event__handler_t process,
+ 				       struct machine *machine);
+ void perf_event__read_stat_config(struct perf_stat_config *config,
+ 				  struct stat_config_event *event);
+ int perf_event__synthesize_stat(struct perf_tool *tool,
+ 				u32 cpu, u32 thread, u64 id,
+ 				struct perf_counts_values *count,
+ 				perf_event__handler_t process,
+ 				struct machine *machine);
+ int perf_event__synthesize_stat_round(struct perf_tool *tool,
+ 				      u64 time, u64 type,
+ 				      perf_event__handler_t process,
+ 				      struct machine *machine);
++>>>>>>> d4c2259195f5 (perf tools: Add stat round event synthesize function)
  int perf_event__synthesize_modules(struct perf_tool *tool,
  				   perf_event__handler_t process,
  				   struct machine *machine);
* Unmerged path tools/perf/tests/stat.c
* Unmerged path tools/perf/tests/builtin-test.c
* Unmerged path tools/perf/tests/stat.c
* Unmerged path tools/perf/tests/tests.h
* Unmerged path tools/perf/util/event.c
* Unmerged path tools/perf/util/event.h
