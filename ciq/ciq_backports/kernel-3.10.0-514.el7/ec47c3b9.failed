mm: introduce do_cow_fault()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] introduce do_cow_fault() (Eric Sandeen) [1274459]
Rebuild_FUZZ: 92.31%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit ec47c3b9543054f6f255d027100fa8214e637003
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ec47c3b9.failed

Introduce do_cow_fault().  The function does what do_fault() does for
write page faults to private mappings.

Unlike do_fault(), do_read_fault() is relatively clean and
straight-forward.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ec47c3b9543054f6f255d027100fa8214e637003)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,5be13e794a7c..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3508,8 -3286,132 +3508,135 @@@ oom
  	return VM_FAULT_OOM;
  }
  
++<<<<<<< HEAD
++=======
+ static int __do_fault(struct vm_area_struct *vma, unsigned long address,
+ 		pgoff_t pgoff, unsigned int flags, struct page **page)
+ {
+ 	struct vm_fault vmf;
+ 	int ret;
+ 
+ 	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
+ 	vmf.pgoff = pgoff;
+ 	vmf.flags = flags;
+ 	vmf.page = NULL;
+ 
+ 	ret = vma->vm_ops->fault(vma, &vmf);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
+ 
+ 	if (unlikely(PageHWPoison(vmf.page))) {
+ 		if (ret & VM_FAULT_LOCKED)
+ 			unlock_page(vmf.page);
+ 		page_cache_release(vmf.page);
+ 		return VM_FAULT_HWPOISON;
+ 	}
+ 
+ 	if (unlikely(!(ret & VM_FAULT_LOCKED)))
+ 		lock_page(vmf.page);
+ 	else
+ 		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
+ 
+ 	*page = vmf.page;
+ 	return ret;
+ }
+ 
+ static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pmd_t *pmd,
+ 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
+ {
+ 	struct page *fault_page;
+ 	spinlock_t *ptl;
+ 	pte_t entry, *pte;
+ 	int ret;
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
+ 
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		return ret;
+ 	}
+ 
+ 	flush_icache_page(vma, fault_page);
+ 	entry = mk_pte(fault_page, vma->vm_page_prot);
+ 	if (pte_file(orig_pte) && pte_file_soft_dirty(orig_pte))
+ 		pte_mksoft_dirty(entry);
+ 	inc_mm_counter_fast(mm, MM_FILEPAGES);
+ 	page_add_file_rmap(fault_page);
+ 	set_pte_at(mm, address, pte, entry);
+ 
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 	pte_unmap_unlock(pte, ptl);
+ 	unlock_page(fault_page);
+ 
+ 	return ret;
+ }
+ 
+ static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pmd_t *pmd,
+ 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
+ {
+ 	struct page *fault_page, *new_page;
+ 	spinlock_t *ptl;
+ 	pte_t entry, *pte;
+ 	int ret;
+ 
+ 	if (unlikely(anon_vma_prepare(vma)))
+ 		return VM_FAULT_OOM;
+ 
+ 	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
+ 	if (!new_page)
+ 		return VM_FAULT_OOM;
+ 
+ 	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL)) {
+ 		page_cache_release(new_page);
+ 		return VM_FAULT_OOM;
+ 	}
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		goto uncharge_out;
+ 
+ 	copy_user_highpage(new_page, fault_page, address, vma);
+ 	__SetPageUptodate(new_page);
+ 
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		goto uncharge_out;
+ 	}
+ 
+ 	flush_icache_page(vma, new_page);
+ 	entry = mk_pte(new_page, vma->vm_page_prot);
+ 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	inc_mm_counter_fast(mm, MM_ANONPAGES);
+ 	page_add_new_anon_rmap(new_page, vma, address);
+ 	set_pte_at(mm, address, pte, entry);
+ 
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 
+ 	pte_unmap_unlock(pte, ptl);
+ 	unlock_page(fault_page);
+ 	page_cache_release(fault_page);
+ 	return ret;
+ uncharge_out:
+ 	mem_cgroup_uncharge_page(new_page);
+ 	page_cache_release(new_page);
+ 	return ret;
+ }
+ 
++>>>>>>> ec47c3b95430 (mm: introduce do_cow_fault())
  /*
 - * do_fault() tries to create a new page mapping. It aggressively
 + * __do_fault() tries to create a new page mapping. It aggressively
   * tries to share with existing pages, but makes a separate copy if
   * the FAULT_FLAG_WRITE is set in the flags parameter in order to avoid
   * the next page fault.
@@@ -3715,10 -3603,13 +3842,20 @@@ static int do_linear_fault(struct mm_st
  			- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
  
  	pte_unmap(page_table);
++<<<<<<< HEAD
 +	/* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
 +	if (!vma->vm_ops->fault)
 +		return VM_FAULT_SIGBUS;
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++=======
+ 	if (!(flags & FAULT_FLAG_WRITE))
+ 		return do_read_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	if (!(vma->vm_flags & VM_SHARED))
+ 		return do_cow_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	return do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++>>>>>>> ec47c3b95430 (mm: introduce do_cow_fault())
  }
  
  /*
@@@ -3750,10 -3641,16 +3887,20 @@@ static int do_nonlinear_fault(struct mm
  	}
  
  	pgoff = pte_to_pgoff(orig_pte);
++<<<<<<< HEAD
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++=======
+ 	if (!(flags & FAULT_FLAG_WRITE))
+ 		return do_read_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	if (!(vma->vm_flags & VM_SHARED))
+ 		return do_cow_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	return do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++>>>>>>> ec47c3b95430 (mm: introduce do_cow_fault())
  }
  
 -static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 +int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
  				unsigned long addr, int page_nid,
  				int *flags)
  {
* Unmerged path mm/memory.c
