staging/rdma/hfi1: Use rdmavt version of post_send

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [infiniband] rdma/hfi1: Use rdmavt version of post_send (Alex Estrin) [1272062 1273170]
Rebuild_FUZZ: 91.30%
commit-author Dennis Dalessandro <dennis.dalessandro@intel.com>
commit 83693bd146063e6843efafbedf302014511fee25
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/83693bd1.failed

This patch removes the post_send and post_one_send from the hfi1 driver.
The "posting" of sends will be done by rdmavt which will walk a WQE and
queue work. This patch will still provide the capability to schedule that
work as well as kick the progress. These are provided to the rdmavt layer.

	Reviewed-by: Jubin John <jubin.john@intel.com>
	Signed-off-by: Dean Luick <dean.luick@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Edward Mascarenhas <edward.mascarenhas@intel.com>
	Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Ira Weiny <ira.weiny@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 83693bd146063e6843efafbedf302014511fee25)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/driver.c
#	drivers/staging/hfi1/qp.c
#	drivers/staging/hfi1/rc.c
#	drivers/staging/hfi1/ruc.c
#	drivers/staging/hfi1/ud.c
#	drivers/staging/hfi1/verbs.c
#	drivers/staging/hfi1/verbs_mcast.c
#	drivers/staging/rdma/hfi1/verbs.h
diff --cc drivers/staging/hfi1/driver.c
index fca20e92c79b,d57c08f3b69c..000000000000
--- a/drivers/staging/hfi1/driver.c
+++ b/drivers/staging/hfi1/driver.c
@@@ -318,9 -335,9 +318,15 @@@ static void rcv_hdrerr(struct hfi1_ctxt
  			spin_lock_irqsave(&qp->r_lock, flags);
  
  			/* Check for valid receive state. */
++<<<<<<< HEAD:drivers/staging/hfi1/driver.c
 +			if (!(ib_hfi1_state_ops[qp->state] &
 +			      HFI1_PROCESS_RECV_OK)) {
 +				ibp->n_pkt_drops++;
++=======
+ 			if (!(ib_rvt_state_ops[qp->state] &
+ 			      RVT_PROCESS_RECV_OK)) {
+ 				ibp->rvp.n_pkt_drops++;
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/driver.c
  			}
  
  			switch (qp->ibqp.qp_type) {
@@@ -764,17 -781,17 +770,17 @@@ static inline void process_rcv_qp_work(
  	 */
  	list_for_each_entry_safe(qp, nqp, &rcd->qp_wait_list, rspwait) {
  		list_del_init(&qp->rspwait);
 -		if (qp->r_flags & RVT_R_RSP_NAK) {
 -			qp->r_flags &= ~RVT_R_RSP_NAK;
 +		if (qp->r_flags & HFI1_R_RSP_DEFERED_ACK) {
 +			qp->r_flags &= ~HFI1_R_RSP_DEFERED_ACK;
  			hfi1_send_rc_ack(rcd, qp, 0);
  		}
 -		if (qp->r_flags & RVT_R_RSP_SEND) {
 +		if (qp->r_flags & HFI1_R_RSP_SEND) {
  			unsigned long flags;
  
 -			qp->r_flags &= ~RVT_R_RSP_SEND;
 +			qp->r_flags &= ~HFI1_R_RSP_SEND;
  			spin_lock_irqsave(&qp->s_lock, flags);
- 			if (ib_hfi1_state_ops[qp->state] &
- 					HFI1_PROCESS_OR_FLUSH_SEND)
+ 			if (ib_rvt_state_ops[qp->state] &
+ 					RVT_PROCESS_OR_FLUSH_SEND)
  				hfi1_schedule_send(qp);
  			spin_unlock_irqrestore(&qp->s_lock, flags);
  		}
diff --cc drivers/staging/hfi1/qp.c
index 9ffed6e14d8e,ff27f1a24af2..000000000000
--- a/drivers/staging/hfi1/qp.c
+++ b/drivers/staging/hfi1/qp.c
@@@ -413,7 -208,7 +413,11 @@@ static void clear_mr_refs(struct hfi1_q
  
  	if (clr_sends) {
  		while (qp->s_last != qp->s_head) {
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +			struct hfi1_swqe *wqe = get_swqe_ptr(qp, qp->s_last);
++=======
+ 			struct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, qp->s_last);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/qp.c
  			unsigned i;
  
  			for (i = 0; i < wqe->wr.num_sge; i++) {
@@@ -616,7 -411,7 +620,11 @@@ int hfi1_modify_qp(struct ib_qp *ibqp, 
  		   int attr_mask, struct ib_udata *udata)
  {
  	struct hfi1_ibdev *dev = to_idev(ibqp->device);
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +	struct hfi1_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/qp.c
  	struct hfi1_qp_priv *priv = qp->priv;
  	enum ib_qp_state cur_state, new_state;
  	struct ib_event ev;
@@@ -914,7 -710,7 +922,11 @@@ bail
  int hfi1_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
  		  int attr_mask, struct ib_qp_init_attr *init_attr)
  {
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +	struct hfi1_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/qp.c
  
  	attr->qp_state = qp->state;
  	attr->cur_qp_state = attr->qp_state;
@@@ -1280,7 -829,7 +1292,11 @@@ bail
   */
  int hfi1_destroy_qp(struct ib_qp *ibqp)
  {
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
 +	struct hfi1_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/qp.c
  	struct hfi1_ibdev *dev = to_idev(ibqp->device);
  	struct hfi1_qp_priv *priv = qp->priv;
  
@@@ -1721,6 -1163,75 +1737,78 @@@ void qp_comm_est(struct hfi1_qp *qp
  	}
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/qp.c
++=======
+ void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+ 		    gfp_t gfp)
+ {
+ 	struct hfi1_qp_priv *priv;
+ 
+ 	priv = kzalloc(sizeof(*priv), gfp);
+ 	if (!priv)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	priv->owner = qp;
+ 
+ 	priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
+ 	if (!priv->s_hdr) {
+ 		kfree(priv);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+ 	return priv;
+ }
+ 
+ void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	kfree(priv->s_hdr);
+ 	kfree(priv);
+ }
+ 
+ unsigned free_all_qps(struct rvt_dev_info *rdi)
+ {
+ 	struct hfi1_ibdev *verbs_dev = container_of(rdi,
+ 						    struct hfi1_ibdev,
+ 						    rdi);
+ 	struct hfi1_devdata *dd = container_of(verbs_dev,
+ 					       struct hfi1_devdata,
+ 					       verbs_dev);
+ 	int n;
+ 	unsigned qp_inuse = 0;
+ 
+ 	for (n = 0; n < dd->num_pports; n++) {
+ 		struct hfi1_ibport *ibp = &dd->pport[n].ibport_data;
+ 
+ 		if (!hfi1_mcast_tree_empty(ibp))
+ 			qp_inuse++;
+ 		rcu_read_lock();
+ 		if (rcu_dereference(ibp->rvp.qp[0]))
+ 			qp_inuse++;
+ 		if (rcu_dereference(ibp->rvp.qp[1]))
+ 			qp_inuse++;
+ 		rcu_read_unlock();
+ 	}
+ 
+ 	return qp_inuse;
+ }
+ 
+ void notify_qp_reset(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	iowait_init(
+ 		&priv->s_iowait,
+ 		1,
+ 		_hfi1_do_send,
+ 		iowait_sleep,
+ 		iowait_wakeup);
+ 	priv->r_adefered = 0;
+ 	clear_ahg(qp);
+ }
+ 
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/qp.c
  /*
   * Switch to alternate path.
   * The QP s_lock should be held and interrupts disabled.
diff --cc drivers/staging/hfi1/rc.c
index dd57d65aa9b2,a30bf300f5cb..000000000000
--- a/drivers/staging/hfi1/rc.c
+++ b/drivers/staging/hfi1/rc.c
@@@ -794,10 -794,10 +794,14 @@@ queue_ack
   * for the given QP.
   * Called at interrupt level with the QP s_lock held.
   */
 -static void reset_psn(struct rvt_qp *qp, u32 psn)
 +static void reset_psn(struct hfi1_qp *qp, u32 psn)
  {
  	u32 n = qp->s_acked;
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +	struct hfi1_swqe *wqe = get_swqe_ptr(qp, n);
++=======
+ 	struct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, n);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/rc.c
  	u32 opcode;
  
  	qp->s_cur = n;
@@@ -880,9 -880,9 +884,13 @@@ done
   * Back up requester to resend the last un-ACKed request.
   * The QP r_lock and s_lock should be held and interrupts disabled.
   */
 -static void restart_rc(struct rvt_qp *qp, u32 psn, int wait)
 +static void restart_rc(struct hfi1_qp *qp, u32 psn, int wait)
  {
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +	struct hfi1_swqe *wqe = get_swqe_ptr(qp, qp->s_acked);
++=======
+ 	struct rvt_swqe *wqe = rvt_get_swqe_ptr(qp, qp->s_acked);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/rc.c
  	struct hfi1_ibport *ibp;
  
  	if (qp->s_retry == 0) {
@@@ -1017,8 -1017,8 +1025,13 @@@ void hfi1_rc_send_complete(struct hfi1_
  	 */
  	if ((psn & IB_BTH_REQ_ACK) && qp->s_acked != qp->s_tail &&
  	    !(qp->s_flags &
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +		(HFI1_S_TIMER | HFI1_S_WAIT_RNR | HFI1_S_WAIT_PSN)) &&
 +		(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_RECV_OK))
++=======
+ 		(RVT_S_TIMER | RVT_S_WAIT_RNR | RVT_S_WAIT_PSN)) &&
+ 		(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK))
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/rc.c
  		start_timer(qp);
  
  	while (qp->s_last != qp->s_acked) {
diff --cc drivers/staging/hfi1/ruc.c
index c4280b6f47d4,b47e462c26b3..000000000000
--- a/drivers/staging/hfi1/ruc.c
+++ b/drivers/staging/hfi1/ruc.c
@@@ -380,11 -382,11 +380,16 @@@ static void ruc_loopback(struct hfi1_q
  	spin_lock_irqsave(&sqp->s_lock, flags);
  
  	/* Return if we are already busy processing a work request. */
++<<<<<<< HEAD:drivers/staging/hfi1/ruc.c
 +	if ((sqp->s_flags & (HFI1_S_BUSY | HFI1_S_ANY_WAIT)) ||
 +	    !(ib_hfi1_state_ops[sqp->state] & HFI1_PROCESS_OR_FLUSH_SEND))
++=======
+ 	if ((sqp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT)) ||
+ 	    !(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_OR_FLUSH_SEND))
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/ruc.c
  		goto unlock;
  
 -	sqp->s_flags |= RVT_S_BUSY;
 +	sqp->s_flags |= HFI1_S_BUSY;
  
  again:
  	if (sqp->s_last == sqp->s_head)
@@@ -411,9 -413,9 +416,9 @@@
  	}
  	spin_unlock_irqrestore(&sqp->s_lock, flags);
  
- 	if (!qp || !(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_RECV_OK) ||
+ 	if (!qp || !(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) ||
  	    qp->ibqp.qp_type != sqp->ibqp.qp_type) {
 -		ibp->rvp.n_pkt_drops++;
 +		ibp->n_pkt_drops++;
  		/*
  		 * For RC, the requester would timeout and retry so
  		 * shortcut the timeouts and just signal too many retries.
@@@ -592,9 -593,9 +597,9 @@@ rnr_nak
  	if (sqp->s_rnr_retry_cnt < 7)
  		sqp->s_rnr_retry--;
  	spin_lock_irqsave(&sqp->s_lock, flags);
- 	if (!(ib_hfi1_state_ops[sqp->state] & HFI1_PROCESS_RECV_OK))
+ 	if (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_RECV_OK))
  		goto clr_busy;
 -	sqp->s_flags |= RVT_S_WAIT_RNR;
 +	sqp->s_flags |= HFI1_S_WAIT_RNR;
  	sqp->s_timer.function = hfi1_rc_rnr_retry;
  	sqp->s_timer.expires = jiffies +
  		usecs_to_jiffies(ib_hfi1_rnr_table[qp->r_min_rnr_timer]);
@@@ -809,12 -818,10 +822,15 @@@ void _hfi1_do_send(struct work_struct *
   * exhausted.  Only allow one CPU to send a packet per QP (tasklet).
   * Otherwise, two threads could send packets out of order.
   */
- void hfi1_do_send(struct work_struct *work)
+ void hfi1_do_send(struct rvt_qp *qp)
  {
++<<<<<<< HEAD:drivers/staging/hfi1/ruc.c
 +	struct iowait *wait = container_of(work, struct iowait, iowork);
 +	struct hfi1_qp *qp = iowait_to_qp(wait);
++=======
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/ruc.c
  	struct hfi1_pkt_state ps;
 -	int (*make_req)(struct rvt_qp *qp);
 +	int (*make_req)(struct hfi1_qp *qp);
  	unsigned long flags;
  	unsigned long timeout;
  
diff --cc drivers/staging/hfi1/ud.c
index a7f67b0111da,e2cbdc86d1a3..000000000000
--- a/drivers/staging/hfi1/ud.c
+++ b/drivers/staging/hfi1/ud.c
@@@ -93,8 -93,8 +93,13 @@@ static void ud_loopback(struct hfi1_qp 
  			IB_QPT_UD : qp->ibqp.qp_type;
  
  	if (dqptype != sqptype ||
++<<<<<<< HEAD:drivers/staging/hfi1/ud.c
 +	    !(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_RECV_OK)) {
 +		ibp->n_pkt_drops++;
++=======
+ 	    !(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)) {
+ 		ibp->rvp.n_pkt_drops++;
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/ud.c
  		goto drop;
  	}
  
@@@ -290,10 -290,10 +295,10 @@@ int hfi1_make_ud_req(struct hfi1_qp *qp
  			goto bail;
  		/* If DMAs are in progress, we can't flush immediately. */
  		if (atomic_read(&priv->s_iowait.sdma_busy)) {
 -			qp->s_flags |= RVT_S_WAIT_DMA;
 +			qp->s_flags |= HFI1_S_WAIT_DMA;
  			goto bail;
  		}
- 		wqe = get_swqe_ptr(qp, qp->s_last);
+ 		wqe = rvt_get_swqe_ptr(qp, qp->s_last);
  		hfi1_send_complete(qp, wqe, IB_WC_WR_FLUSH_ERR);
  		goto done;
  	}
diff --cc drivers/staging/hfi1/verbs.c
index d228eb7fc4f0,b4cfda482254..000000000000
--- a/drivers/staging/hfi1/verbs.c
+++ b/drivers/staging/hfi1/verbs.c
@@@ -133,38 -133,6 +133,41 @@@ static void verbs_sdma_complete
  #define TXREQ_NAME_LEN 24
  
  /*
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 + * Note that it is OK to post send work requests in the SQE and ERR
 + * states; hfi1_do_send() will process them and generate error
 + * completions as per IB 1.2 C10-96.
 + */
 +const int ib_hfi1_state_ops[IB_QPS_ERR + 1] = {
 +	[IB_QPS_RESET] = 0,
 +	[IB_QPS_INIT] = HFI1_POST_RECV_OK,
 +	[IB_QPS_RTR] = HFI1_POST_RECV_OK | HFI1_PROCESS_RECV_OK,
 +	[IB_QPS_RTS] = HFI1_POST_RECV_OK | HFI1_PROCESS_RECV_OK |
 +	    HFI1_POST_SEND_OK | HFI1_PROCESS_SEND_OK |
 +	    HFI1_PROCESS_NEXT_SEND_OK,
 +	[IB_QPS_SQD] = HFI1_POST_RECV_OK | HFI1_PROCESS_RECV_OK |
 +	    HFI1_POST_SEND_OK | HFI1_PROCESS_SEND_OK,
 +	[IB_QPS_SQE] = HFI1_POST_RECV_OK | HFI1_PROCESS_RECV_OK |
 +	    HFI1_POST_SEND_OK | HFI1_FLUSH_SEND,
 +	[IB_QPS_ERR] = HFI1_POST_RECV_OK | HFI1_FLUSH_RECV |
 +	    HFI1_POST_SEND_OK | HFI1_FLUSH_SEND,
 +};
 +
 +struct hfi1_ucontext {
 +	struct ib_ucontext ibucontext;
 +};
 +
 +static inline struct hfi1_ucontext *to_iucontext(struct ib_ucontext
 +						  *ibucontext)
 +{
 +	return container_of(ibucontext, struct hfi1_ucontext, ibucontext);
 +}
 +
 +static inline void _hfi1_schedule_send(struct hfi1_qp *qp);
 +
 +/*
++=======
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/verbs.c
   * Translate ib_wr_opcode into ib_wc_opcode.
   */
  const enum ib_wc_opcode ib_hfi1_wc_opcode[] = {
@@@ -356,159 -324,6 +359,162 @@@ void hfi1_skip_sge(struct hfi1_sge_stat
  }
  
  /**
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 + * post_one_send - post one RC, UC, or UD send work request
 + * @qp: the QP to post on
 + * @wr: the work request to send
 + */
 +static int post_one_send(struct hfi1_qp *qp, struct ib_send_wr *wr)
 +{
 +	struct hfi1_swqe *wqe;
 +	u32 next;
 +	int i;
 +	int j;
 +	int acc;
 +	struct rvt_lkey_table *rkt;
 +	struct rvt_pd *pd;
 +	struct hfi1_devdata *dd = dd_from_ibdev(qp->ibqp.device);
 +	struct hfi1_pportdata *ppd;
 +	struct hfi1_ibport *ibp;
 +
 +	/* IB spec says that num_sge == 0 is OK. */
 +	if (unlikely(wr->num_sge > qp->s_max_sge))
 +		return -EINVAL;
 +
 +	ppd = &dd->pport[qp->port_num - 1];
 +	ibp = &ppd->ibport_data;
 +
 +	/*
 +	 * Don't allow RDMA reads or atomic operations on UC or
 +	 * undefined operations.
 +	 * Make sure buffer is large enough to hold the result for atomics.
 +	 */
 +	if (wr->opcode == IB_WR_FAST_REG_MR) {
 +		return -EINVAL;
 +	} else if (qp->ibqp.qp_type == IB_QPT_UC) {
 +		if ((unsigned) wr->opcode >= IB_WR_RDMA_READ)
 +			return -EINVAL;
 +	} else if (qp->ibqp.qp_type != IB_QPT_RC) {
 +		/* Check IB_QPT_SMI, IB_QPT_GSI, IB_QPT_UD opcode */
 +		if (wr->opcode != IB_WR_SEND &&
 +		    wr->opcode != IB_WR_SEND_WITH_IMM)
 +			return -EINVAL;
 +		/* Check UD destination address PD */
 +		if (qp->ibqp.pd != wr->wr.ud.ah->pd)
 +			return -EINVAL;
 +	} else if ((unsigned) wr->opcode > IB_WR_ATOMIC_FETCH_AND_ADD)
 +		return -EINVAL;
 +	else if (wr->opcode >= IB_WR_ATOMIC_CMP_AND_SWP &&
 +		   (wr->num_sge == 0 ||
 +		    wr->sg_list[0].length < sizeof(u64) ||
 +		    wr->sg_list[0].addr & (sizeof(u64) - 1)))
 +		return -EINVAL;
 +	else if (wr->opcode >= IB_WR_RDMA_READ && !qp->s_max_rd_atomic)
 +		return -EINVAL;
 +
 +	next = qp->s_head + 1;
 +	if (next >= qp->s_size)
 +		next = 0;
 +	if (next == qp->s_last)
 +		return -ENOMEM;
 +
 +	rkt = &to_idev(qp->ibqp.device)->lk_table;
 +	pd = ibpd_to_rvtpd(qp->ibqp.pd);
 +	wqe = get_swqe_ptr(qp, qp->s_head);
 +	wqe->wr = *wr;
 +	wqe->length = 0;
 +	j = 0;
 +	if (wr->num_sge) {
 +		acc = wr->opcode >= IB_WR_RDMA_READ ?
 +			IB_ACCESS_LOCAL_WRITE : 0;
 +		for (i = 0; i < wr->num_sge; i++) {
 +			u32 length = wr->sg_list[i].length;
 +			int ok;
 +
 +			if (length == 0)
 +				continue;
 +			ok = hfi1_lkey_ok(rkt, pd, &wqe->sg_list[j],
 +					  &wr->sg_list[i], acc);
 +			if (!ok)
 +				goto bail_inval_free;
 +			wqe->length += length;
 +			j++;
 +		}
 +		wqe->wr.num_sge = j;
 +	}
 +	if (qp->ibqp.qp_type == IB_QPT_UC ||
 +	    qp->ibqp.qp_type == IB_QPT_RC) {
 +		if (wqe->length > 0x80000000U)
 +			goto bail_inval_free;
 +	} else {
 +		struct hfi1_ah *ah = to_iah(wr->wr.ud.ah);
 +
 +		atomic_inc(&ah->refcount);
 +	}
 +	wqe->ssn = qp->s_ssn++;
 +	qp->s_head = next;
 +
 +	return 0;
 +
 +bail_inval_free:
 +	/* release mr holds */
 +	while (j) {
 +		struct hfi1_sge *sge = &wqe->sg_list[--j];
 +
 +		hfi1_put_mr(sge->mr);
 +	}
 +	return -EINVAL;
 +}
 +
 +/**
 + * post_send - post a send on a QP
 + * @ibqp: the QP to post the send on
 + * @wr: the list of work requests to post
 + * @bad_wr: the first bad WR is put here
 + *
 + * This may be called from interrupt context.
 + */
 +static int post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 +		     struct ib_send_wr **bad_wr)
 +{
 +	struct hfi1_qp *qp = to_iqp(ibqp);
 +	struct hfi1_qp_priv *priv = qp->priv;
 +	int err = 0;
 +	int call_send;
 +	unsigned long flags;
 +	unsigned nreq = 0;
 +
 +	spin_lock_irqsave(&qp->s_lock, flags);
 +
 +	/* Check that state is OK to post send. */
 +	if (unlikely(!(ib_hfi1_state_ops[qp->state] & HFI1_POST_SEND_OK))) {
 +		spin_unlock_irqrestore(&qp->s_lock, flags);
 +		return -EINVAL;
 +	}
 +
 +	/* sq empty and not list -> call send */
 +	call_send = qp->s_head == qp->s_last && !wr->next;
 +
 +	for (; wr; wr = wr->next) {
 +		err = post_one_send(qp, wr);
 +		if (unlikely(err)) {
 +			*bad_wr = wr;
 +			goto bail;
 +		}
 +		nreq++;
 +	}
 +bail:
 +	spin_unlock_irqrestore(&qp->s_lock, flags);
 +	if (nreq && !call_send)
 +		_hfi1_schedule_send(qp);
 +	if (nreq && call_send)
 +		hfi1_do_send(&priv->s_iowait.iowork);
 +	return err;
 +}
 +
 +/**
++=======
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/verbs.c
   * post_receive - post a receive on a QP
   * @ibqp: the QP to post the receive on
   * @wr: the WR to post
@@@ -519,8 -334,8 +525,13 @@@
  static int post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,
  			struct ib_recv_wr **bad_wr)
  {
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	struct hfi1_qp *qp = to_iqp(ibqp);
 +	struct hfi1_rwq *wq = qp->r_rq.wq;
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
+ 	struct rvt_rwq *wq = qp->r_rq.wq;
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/verbs.c
  	unsigned long flags;
  	int ret;
  
@@@ -737,15 -552,15 +748,15 @@@ static noinline struct verbs_txreq *__g
  	if (!tx) {
  		spin_lock_irqsave(&qp->s_lock, flags);
  		write_seqlock(&dev->iowait_lock);
- 		if (ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_RECV_OK &&
+ 		if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK &&
  		    list_empty(&priv->s_iowait.list)) {
  			dev->n_txwait++;
 -			qp->s_flags |= RVT_S_WAIT_TX;
 +			qp->s_flags |= HFI1_S_WAIT_TX;
  			list_add_tail(&priv->s_iowait.list, &dev->txwait);
 -			trace_hfi1_qpsleep(qp, RVT_S_WAIT_TX);
 +			trace_hfi1_qpsleep(qp, HFI1_S_WAIT_TX);
  			atomic_inc(&qp->refcount);
  		}
 -		qp->s_flags &= ~RVT_S_BUSY;
 +		qp->s_flags &= ~HFI1_S_BUSY;
  		write_sequnlock(&dev->iowait_lock);
  		spin_unlock_irqrestore(&qp->s_lock, flags);
  		tx = ERR_PTR(-EBUSY);
@@@ -1971,25 -1627,23 +1982,25 @@@ int hfi1_register_ib_device(struct hfi1
  	ibdev->modify_qp = hfi1_modify_qp;
  	ibdev->query_qp = hfi1_query_qp;
  	ibdev->destroy_qp = hfi1_destroy_qp;
- 	ibdev->post_send = post_send;
+ 	ibdev->post_send = NULL;
  	ibdev->post_recv = post_receive;
  	ibdev->post_srq_recv = hfi1_post_srq_receive;
 -	ibdev->create_cq = NULL;
 -	ibdev->destroy_cq = NULL;
 -	ibdev->resize_cq = NULL;
 -	ibdev->poll_cq = NULL;
 -	ibdev->req_notify_cq = NULL;
 -	ibdev->get_dma_mr = NULL;
 -	ibdev->reg_user_mr = NULL;
 -	ibdev->dereg_mr = NULL;
 -	ibdev->alloc_mr = NULL;
 -	ibdev->map_mr_sg = NULL;
 -	ibdev->alloc_fmr = NULL;
 -	ibdev->map_phys_fmr = NULL;
 -	ibdev->unmap_fmr = NULL;
 -	ibdev->dealloc_fmr = NULL;
 +	ibdev->create_cq = hfi1_create_cq;
 +	ibdev->destroy_cq = hfi1_destroy_cq;
 +	ibdev->resize_cq = hfi1_resize_cq;
 +	ibdev->poll_cq = hfi1_poll_cq;
 +	ibdev->req_notify_cq = hfi1_req_notify_cq;
 +	ibdev->get_dma_mr = hfi1_get_dma_mr;
 +	ibdev->reg_phys_mr = hfi1_reg_phys_mr;
 +	ibdev->reg_user_mr = hfi1_reg_user_mr;
 +	ibdev->dereg_mr = hfi1_dereg_mr;
 +	ibdev->alloc_mr = hfi1_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = hfi1_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = hfi1_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = hfi1_alloc_fmr;
 +	ibdev->map_phys_fmr = hfi1_map_phys_fmr;
 +	ibdev->unmap_fmr = hfi1_unmap_fmr;
 +	ibdev->dealloc_fmr = hfi1_dealloc_fmr;
  	ibdev->attach_mcast = hfi1_multicast_attach;
  	ibdev->detach_mcast = hfi1_multicast_detach;
  	ibdev->process_mad = hfi1_process_mad;
@@@ -2000,7 -1654,54 +2011,58 @@@
  	strncpy(ibdev->node_desc, init_utsname()->nodename,
  		sizeof(ibdev->node_desc));
  
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	ret = ib_register_device(ibdev, hfi1_create_port_files);
++=======
+ 	/*
+ 	 * Fill in rvt info object.
+ 	 */
+ 	dd->verbs_dev.rdi.driver_f.port_callback = hfi1_create_port_files;
+ 	dd->verbs_dev.rdi.driver_f.get_card_name = get_card_name;
+ 	dd->verbs_dev.rdi.driver_f.get_pci_dev = get_pci_dev;
+ 	dd->verbs_dev.rdi.driver_f.check_ah = hfi1_check_ah;
+ 	dd->verbs_dev.rdi.driver_f.notify_new_ah = hfi1_notify_new_ah;
+ 	/*
+ 	 * Fill in rvt info device attributes.
+ 	 */
+ 	hfi1_fill_device_attr(dd);
+ 
+ 	/* queue pair */
+ 	dd->verbs_dev.rdi.dparms.qp_table_size = hfi1_qp_table_size;
+ 	dd->verbs_dev.rdi.dparms.qpn_start = 0;
+ 	dd->verbs_dev.rdi.dparms.qpn_inc = 1;
+ 	dd->verbs_dev.rdi.dparms.qos_shift = dd->qos_shift;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_start = kdeth_qp << 16;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_end =
+ 	dd->verbs_dev.rdi.dparms.qpn_res_start + 65535;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_alloc = qp_priv_alloc;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_free = qp_priv_free;
+ 	dd->verbs_dev.rdi.driver_f.free_all_qps = free_all_qps;
+ 	dd->verbs_dev.rdi.driver_f.notify_qp_reset = notify_qp_reset;
+ 	dd->verbs_dev.rdi.driver_f.do_send = hfi1_do_send;
+ 	dd->verbs_dev.rdi.driver_f.schedule_send = hfi1_schedule_send;
+ 
+ 	/* completeion queue */
+ 	snprintf(dd->verbs_dev.rdi.dparms.cq_name,
+ 		 sizeof(dd->verbs_dev.rdi.dparms.cq_name),
+ 		 "hfi1_cq%d", dd->unit);
+ 	dd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;
+ 
+ 	/* misc settings */
+ 	dd->verbs_dev.rdi.flags = 0; /* Let rdmavt handle it all */
+ 	dd->verbs_dev.rdi.dparms.lkey_table_size = hfi1_lkey_table_size;
+ 	dd->verbs_dev.rdi.dparms.nports = dd->num_pports;
+ 	dd->verbs_dev.rdi.dparms.npkeys = hfi1_get_npkeys(dd);
+ 
+ 	ppd = dd->pport;
+ 	for (i = 0; i < dd->num_pports; i++, ppd++)
+ 		rvt_init_port(&dd->verbs_dev.rdi,
+ 			      &ppd->ibport_data.rvp,
+ 			      i,
+ 			      ppd->pkeys);
+ 
+ 	ret = rvt_register_device(&dd->verbs_dev.rdi);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/verbs.c
  	if (ret)
  		goto err_reg;
  
diff --cc drivers/staging/hfi1/verbs_mcast.c
index afc6b4c61a1d,175396b8b347..000000000000
--- a/drivers/staging/hfi1/verbs_mcast.c
+++ b/drivers/staging/hfi1/verbs_mcast.c
@@@ -241,7 -241,7 +241,11 @@@ bail
  
  int hfi1_multicast_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
  {
++<<<<<<< HEAD:drivers/staging/hfi1/verbs_mcast.c
 +	struct hfi1_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/verbs_mcast.c
  	struct hfi1_ibdev *dev = to_idev(ibqp->device);
  	struct hfi1_ibport *ibp;
  	struct hfi1_mcast *mcast;
@@@ -299,7 -299,7 +303,11 @@@ bail
  
  int hfi1_multicast_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
  {
++<<<<<<< HEAD:drivers/staging/hfi1/verbs_mcast.c
 +	struct hfi1_qp *qp = to_iqp(ibqp);
++=======
+ 	struct rvt_qp *qp = ibqp_to_rvtqp(ibqp);
++>>>>>>> 83693bd14606 (staging/rdma/hfi1: Use rdmavt version of post_send):drivers/staging/rdma/hfi1/verbs_mcast.c
  	struct hfi1_ibdev *dev = to_idev(ibqp->device);
  	struct hfi1_ibport *ibp = to_iport(ibqp->device, qp->port_num);
  	struct hfi1_mcast *mcast = NULL;
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
* Unmerged path drivers/staging/hfi1/driver.c
* Unmerged path drivers/staging/hfi1/qp.c
* Unmerged path drivers/staging/hfi1/rc.c
* Unmerged path drivers/staging/hfi1/ruc.c
diff --git a/drivers/staging/hfi1/uc.c b/drivers/staging/hfi1/uc.c
index fc90d4f544e4..e35f013fd09a 100644
--- a/drivers/staging/hfi1/uc.c
+++ b/drivers/staging/hfi1/uc.c
@@ -76,8 +76,8 @@ int hfi1_make_uc_req(struct hfi1_qp *qp)
 
 	spin_lock_irqsave(&qp->s_lock, flags);
 
-	if (!(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_SEND_OK)) {
-		if (!(ib_hfi1_state_ops[qp->state] & HFI1_FLUSH_SEND))
+	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_SEND_OK)) {
+		if (!(ib_rvt_state_ops[qp->state] & RVT_FLUSH_SEND))
 			goto bail;
 		/* We are in the error state, flush the work request. */
 		if (qp->s_last == qp->s_head)
@@ -88,7 +88,7 @@ int hfi1_make_uc_req(struct hfi1_qp *qp)
 			goto bail;
 		}
 		clear_ahg(qp);
-		wqe = get_swqe_ptr(qp, qp->s_last);
+		wqe = rvt_get_swqe_ptr(qp, qp->s_last);
 		hfi1_send_complete(qp, wqe, IB_WC_WR_FLUSH_ERR);
 		goto done;
 	}
@@ -98,12 +98,12 @@ int hfi1_make_uc_req(struct hfi1_qp *qp)
 		ohdr = &priv->s_hdr->ibh.u.l.oth;
 
 	/* Get the next send request. */
-	wqe = get_swqe_ptr(qp, qp->s_cur);
+	wqe = rvt_get_swqe_ptr(qp, qp->s_cur);
 	qp->s_wqe = NULL;
 	switch (qp->s_state) {
 	default:
-		if (!(ib_hfi1_state_ops[qp->state] &
-		    HFI1_PROCESS_NEXT_SEND_OK))
+		if (!(ib_rvt_state_ops[qp->state] &
+		    RVT_PROCESS_NEXT_SEND_OK))
 			goto bail;
 		/* Check if send work queue is empty. */
 		if (qp->s_cur == qp->s_head) {
* Unmerged path drivers/staging/hfi1/ud.c
* Unmerged path drivers/staging/hfi1/verbs.c
* Unmerged path drivers/staging/hfi1/verbs_mcast.c
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
