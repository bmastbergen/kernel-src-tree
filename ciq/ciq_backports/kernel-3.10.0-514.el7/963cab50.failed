iw_cxgb4: Adds support for T6 adapter

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Hariprasad S <hariprasad@chelsio.com>
commit 963cab508296a06ed8063c848f32d74f2b4b4c26
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/963cab50.failed

	Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 963cab508296a06ed8063c848f32d74f2b4b4c26)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/cxgb4/cm.c
diff --cc drivers/infiniband/hw/cxgb4/cm.c
index cc2be9e50b5a,c9cffced00ca..000000000000
--- a/drivers/infiniband/hw/cxgb4/cm.c
+++ b/drivers/infiniband/hw/cxgb4/cm.c
@@@ -623,7 -652,28 +619,32 @@@ static int send_connect(struct c4iw_ep 
  				   &ep->com.mapped_local_addr;
  	struct sockaddr_in6 *ra6 = (struct sockaddr_in6 *)
  				   &ep->com.mapped_remote_addr;
++<<<<<<< HEAD
 +	int win;
++=======
+ 	int ret;
+ 	enum chip_type adapter_type = ep->com.dev->rdev.lldi.adapter_type;
+ 	u32 isn = (prandom_u32() & ~7UL) - 1;
+ 
+ 	switch (CHELSIO_CHIP_VERSION(adapter_type)) {
+ 	case CHELSIO_T4:
+ 		sizev4 = sizeof(struct cpl_act_open_req);
+ 		sizev6 = sizeof(struct cpl_act_open_req6);
+ 		break;
+ 	case CHELSIO_T5:
+ 		sizev4 = sizeof(struct cpl_t5_act_open_req);
+ 		sizev6 = sizeof(struct cpl_t5_act_open_req6);
+ 		break;
+ 	case CHELSIO_T6:
+ 		sizev4 = sizeof(struct cpl_t6_act_open_req);
+ 		sizev6 = sizeof(struct cpl_t6_act_open_req6);
+ 		break;
+ 	default:
+ 		pr_err("T%d Chip is not supported\n",
+ 		       CHELSIO_CHIP_VERSION(adapter_type));
+ 		return -EINVAL;
+ 	}
++>>>>>>> 963cab508296 (iw_cxgb4: Adds support for T6 adapter)
  
  	wrlen = (ep->com.remote_addr.ss_family == AF_INET) ?
  			roundup(sizev4, 16) :
@@@ -672,25 -722,56 +693,51 @@@
  		opt2 |= SACK_EN_F;
  	if (wscale && enable_tcp_window_scaling)
  		opt2 |= WND_SCALE_EN_F;
- 	if (is_t5(ep->com.dev->rdev.lldi.adapter_type)) {
+ 	if (CHELSIO_CHIP_VERSION(adapter_type) > CHELSIO_T4) {
+ 		if (peer2peer)
+ 			isn += 4;
+ 
  		opt2 |= T5_OPT_2_VALID_F;
 -		opt2 |= CONG_CNTRL_V(CONG_ALG_TAHOE);
 +		opt2 |= V_CONG_CNTRL(CONG_ALG_TAHOE);
  		opt2 |= T5_ISS_F;
  	}
 -
 -	if (ep->com.remote_addr.ss_family == AF_INET6)
 -		cxgb4_clip_get(ep->com.dev->rdev.lldi.ports[0],
 -			       (const u32 *)&la6->sin6_addr.s6_addr, 1);
 -
  	t4_set_arp_err_handler(skb, ep, act_open_req_arp_failure);
  
- 	if (is_t4(ep->com.dev->rdev.lldi.adapter_type)) {
- 		if (ep->com.remote_addr.ss_family == AF_INET) {
- 			req = (struct cpl_act_open_req *) skb_put(skb, wrlen);
+ 	if (ep->com.remote_addr.ss_family == AF_INET) {
+ 		switch (CHELSIO_CHIP_VERSION(adapter_type)) {
+ 		case CHELSIO_T4:
+ 			req = (struct cpl_act_open_req *)skb_put(skb, wrlen);
  			INIT_TP_WR(req, 0);
- 			OPCODE_TID(req) = cpu_to_be32(
- 					MK_OPCODE_TID(CPL_ACT_OPEN_REQ,
- 					((ep->rss_qid << 14) | ep->atid)));
- 			req->local_port = la->sin_port;
- 			req->peer_port = ra->sin_port;
- 			req->local_ip = la->sin_addr.s_addr;
- 			req->peer_ip = ra->sin_addr.s_addr;
- 			req->opt0 = cpu_to_be64(opt0);
+ 			break;
+ 		case CHELSIO_T5:
+ 			t5req = (struct cpl_t5_act_open_req *)skb_put(skb,
+ 					wrlen);
+ 			INIT_TP_WR(t5req, 0);
+ 			req = (struct cpl_act_open_req *)t5req;
+ 			break;
+ 		case CHELSIO_T6:
+ 			t6req = (struct cpl_t6_act_open_req *)skb_put(skb,
+ 					wrlen);
+ 			INIT_TP_WR(t6req, 0);
+ 			req = (struct cpl_act_open_req *)t6req;
+ 			t5req = (struct cpl_t5_act_open_req *)t6req;
+ 			break;
+ 		default:
+ 			pr_err("T%d Chip is not supported\n",
+ 			       CHELSIO_CHIP_VERSION(adapter_type));
+ 			ret = -EINVAL;
+ 			goto clip_release;
+ 		}
+ 
+ 		OPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_ACT_OPEN_REQ,
+ 					((ep->rss_qid<<14) | ep->atid)));
+ 		req->local_port = la->sin_port;
+ 		req->peer_port = ra->sin_port;
+ 		req->local_ip = la->sin_addr.s_addr;
+ 		req->peer_ip = ra->sin_addr.s_addr;
+ 		req->opt0 = cpu_to_be64(opt0);
+ 
+ 		if (is_t4(ep->com.dev->rdev.lldi.adapter_type)) {
  			req->params = cpu_to_be32(cxgb4_select_ntuple(
  						ep->com.dev->rdev.lldi.ports[0],
  						ep->l2t));
@@@ -774,7 -838,12 +804,16 @@@
  	}
  
  	set_bit(ACT_OPEN_REQ, &ep->com.history);
++<<<<<<< HEAD
 +	return c4iw_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
++=======
+ 	ret = c4iw_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
+ clip_release:
+ 	if (ret && ep->com.remote_addr.ss_family == AF_INET6)
+ 		cxgb4_clip_release(ep->com.dev->rdev.lldi.ports[0],
+ 				   (const u32 *)&la6->sin6_addr.s6_addr, 1);
+ 	return ret;
++>>>>>>> 963cab508296 (iw_cxgb4: Adds support for T6 adapter)
  }
  
  static void send_mpa_req(struct c4iw_ep *ep, struct sk_buff *skb,
@@@ -2210,15 -2300,19 +2253,24 @@@ static void accept_cr(struct c4iw_ep *e
  		const struct tcphdr *tcph;
  		u32 hlen = ntohl(req->hdr_len);
  
++<<<<<<< HEAD
 +		tcph = (const void *)(req + 1) + G_ETH_HDR_LEN(hlen) +
 +			G_IP_HDR_LEN(hlen);
++=======
+ 		if (CHELSIO_CHIP_VERSION(adapter_type) <= CHELSIO_T5)
+ 			tcph = (const void *)(req + 1) + ETH_HDR_LEN_G(hlen) +
+ 				IP_HDR_LEN_G(hlen);
+ 		else
+ 			tcph = (const void *)(req + 1) +
+ 				T6_ETH_HDR_LEN_G(hlen) + T6_IP_HDR_LEN_G(hlen);
++>>>>>>> 963cab508296 (iw_cxgb4: Adds support for T6 adapter)
  		if (tcph->ece && tcph->cwr)
  			opt2 |= CCTRL_ECN_V(1);
  	}
- 	if (is_t5(ep->com.dev->rdev.lldi.adapter_type)) {
+ 	if (CHELSIO_CHIP_VERSION(adapter_type) > CHELSIO_T4) {
  		u32 isn = (prandom_u32() & ~7UL) - 1;
  		opt2 |= T5_OPT_2_VALID_F;
 -		opt2 |= CONG_CNTRL_V(CONG_ALG_TAHOE);
 +		opt2 |= V_CONG_CNTRL(CONG_ALG_TAHOE);
  		opt2 |= T5_ISS_F;
  		rpl5 = (void *)rpl;
  		memset(&rpl5->iss, 0, roundup(sizeof(*rpl5)-sizeof(*rpl), 16));
@@@ -2246,12 -2340,16 +2298,21 @@@ static void reject_cr(struct c4iw_dev *
  	return;
  }
  
- static void get_4tuple(struct cpl_pass_accept_req *req, int *iptype,
- 		       __u8 *local_ip, __u8 *peer_ip,
+ static void get_4tuple(struct cpl_pass_accept_req *req, enum chip_type type,
+ 		       int *iptype, __u8 *local_ip, __u8 *peer_ip,
  		       __be16 *local_port, __be16 *peer_port)
  {
++<<<<<<< HEAD
 +	int eth_len = G_ETH_HDR_LEN(be32_to_cpu(req->hdr_len));
 +	int ip_len = G_IP_HDR_LEN(be32_to_cpu(req->hdr_len));
++=======
+ 	int eth_len = (CHELSIO_CHIP_VERSION(type) <= CHELSIO_T5) ?
+ 		      ETH_HDR_LEN_G(be32_to_cpu(req->hdr_len)) :
+ 		      T6_ETH_HDR_LEN_G(be32_to_cpu(req->hdr_len));
+ 	int ip_len = (CHELSIO_CHIP_VERSION(type) <= CHELSIO_T5) ?
+ 		     IP_HDR_LEN_G(be32_to_cpu(req->hdr_len)) :
+ 		     T6_IP_HDR_LEN_G(be32_to_cpu(req->hdr_len));
++>>>>>>> 963cab508296 (iw_cxgb4: Adds support for T6 adapter)
  	struct iphdr *ip = (struct iphdr *)((u8 *)(req + 1) + eth_len);
  	struct ipv6hdr *ip6 = (struct ipv6hdr *)((u8 *)(req + 1) + eth_len);
  	struct tcphdr *tcp = (struct tcphdr *)
@@@ -3509,22 -3673,30 +3576,49 @@@ static void build_cpl_pass_accept_req(s
  
  	req = (struct cpl_pass_accept_req *)__skb_push(skb, sizeof(*req));
  	memset(req, 0, sizeof(*req));
++<<<<<<< HEAD
 +	req->l2info = cpu_to_be16(V_SYN_INTF(intf) |
 +			 V_SYN_MAC_IDX(RX_MACIDX_G(
 +			 (__force int) htonl(l2info))) |
 +			 F_SYN_XACT_MATCH);
 +	eth_hdr_len = is_t4(dev->rdev.lldi.adapter_type) ?
 +			    RX_ETHHDR_LEN_G((__force int)htonl(l2info)) :
 +			    RX_T5_ETHHDR_LEN_G((__force int)htonl(l2info));
 +	req->hdr_len = cpu_to_be32(V_SYN_RX_CHAN(RX_CHAN_G(
 +					(__force int) htonl(l2info))) |
 +				   V_TCP_HDR_LEN(RX_TCPHDR_LEN_G(
 +					(__force int) htons(hdr_len))) |
 +				   V_IP_HDR_LEN(RX_IPHDR_LEN_G(
 +					(__force int) htons(hdr_len))) |
 +				   V_ETH_HDR_LEN(RX_ETHHDR_LEN_G(eth_hdr_len)));
 +	req->vlan = (__force __be16) vlantag;
 +	req->len = (__force __be16) len;
++=======
+ 	req->l2info = cpu_to_be16(SYN_INTF_V(intf) |
+ 			 SYN_MAC_IDX_V(RX_MACIDX_G(
+ 			 be32_to_cpu(l2info))) |
+ 			 SYN_XACT_MATCH_F);
+ 	type = dev->rdev.lldi.adapter_type;
+ 	tcp_hdr_len = RX_TCPHDR_LEN_G(be16_to_cpu(hdr_len));
+ 	ip_hdr_len = RX_IPHDR_LEN_G(be16_to_cpu(hdr_len));
+ 	req->hdr_len =
+ 		cpu_to_be32(SYN_RX_CHAN_V(RX_CHAN_G(be32_to_cpu(l2info))));
+ 	if (CHELSIO_CHIP_VERSION(type) <= CHELSIO_T5) {
+ 		eth_hdr_len = is_t4(type) ?
+ 				RX_ETHHDR_LEN_G(be32_to_cpu(l2info)) :
+ 				RX_T5_ETHHDR_LEN_G(be32_to_cpu(l2info));
+ 		req->hdr_len |= cpu_to_be32(TCP_HDR_LEN_V(tcp_hdr_len) |
+ 					    IP_HDR_LEN_V(ip_hdr_len) |
+ 					    ETH_HDR_LEN_V(eth_hdr_len));
+ 	} else { /* T6 and later */
+ 		eth_hdr_len = RX_T6_ETHHDR_LEN_G(be32_to_cpu(l2info));
+ 		req->hdr_len |= cpu_to_be32(T6_TCP_HDR_LEN_V(tcp_hdr_len) |
+ 					    T6_IP_HDR_LEN_V(ip_hdr_len) |
+ 					    T6_ETH_HDR_LEN_V(eth_hdr_len));
+ 	}
+ 	req->vlan = vlantag;
+ 	req->len = len;
++>>>>>>> 963cab508296 (iw_cxgb4: Adds support for T6 adapter)
  	req->tos_stid = cpu_to_be32(PASS_OPEN_TID_V(stid) |
  				    PASS_OPEN_TOS_V(tos));
  	req->tcpopt.mss = htons(tmp_opt.mss_clamp);
* Unmerged path drivers/infiniband/hw/cxgb4/cm.c
diff --git a/drivers/infiniband/hw/cxgb4/device.c b/drivers/infiniband/hw/cxgb4/device.c
index 92f60c34a77e..8b5909faf4b8 100644
--- a/drivers/infiniband/hw/cxgb4/device.c
+++ b/drivers/infiniband/hw/cxgb4/device.c
@@ -971,12 +971,12 @@ static struct c4iw_dev *c4iw_alloc(const struct cxgb4_lld_info *infop)
 		devp->rdev.lldi.sge_egrstatuspagesize;
 
 	/*
-	 * For T5 devices, we map all of BAR2 with WC.
+	 * For T5/T6 devices, we map all of BAR2 with WC.
 	 * For T4 devices with onchip qp mem, we map only that part
 	 * of BAR2 with WC.
 	 */
 	devp->rdev.bar2_pa = pci_resource_start(devp->rdev.lldi.pdev, 2);
-	if (is_t5(devp->rdev.lldi.adapter_type)) {
+	if (!is_t4(devp->rdev.lldi.adapter_type)) {
 		devp->rdev.bar2_kva = ioremap_wc(devp->rdev.bar2_pa,
 			pci_resource_len(devp->rdev.lldi.pdev, 2));
 		if (!devp->rdev.bar2_kva) {
@@ -1276,11 +1276,9 @@ static int enable_qp_db(int id, void *p, void *data)
 static void resume_rc_qp(struct c4iw_qp *qp)
 {
 	spin_lock(&qp->lock);
-	t4_ring_sq_db(&qp->wq, qp->wq.sq.wq_pidx_inc,
-		      is_t5(qp->rhp->rdev.lldi.adapter_type), NULL);
+	t4_ring_sq_db(&qp->wq, qp->wq.sq.wq_pidx_inc, NULL);
 	qp->wq.sq.wq_pidx_inc = 0;
-	t4_ring_rq_db(&qp->wq, qp->wq.rq.wq_pidx_inc,
-		      is_t5(qp->rhp->rdev.lldi.adapter_type), NULL);
+	t4_ring_rq_db(&qp->wq, qp->wq.rq.wq_pidx_inc, NULL);
 	qp->wq.rq.wq_pidx_inc = 0;
 	spin_unlock(&qp->lock);
 }
diff --git a/drivers/infiniband/hw/cxgb4/provider.c b/drivers/infiniband/hw/cxgb4/provider.c
index 7746113552e7..df3b3f1ad066 100644
--- a/drivers/infiniband/hw/cxgb4/provider.c
+++ b/drivers/infiniband/hw/cxgb4/provider.c
@@ -209,7 +209,7 @@ static int c4iw_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)
 		if (addr >= rdev->oc_mw_pa)
 			vma->vm_page_prot = t4_pgprot_wc(vma->vm_page_prot);
 		else {
-			if (is_t5(rdev->lldi.adapter_type))
+			if (!is_t4(rdev->lldi.adapter_type))
 				vma->vm_page_prot =
 					t4_pgprot_wc(vma->vm_page_prot);
 			else
diff --git a/drivers/infiniband/hw/cxgb4/qp.c b/drivers/infiniband/hw/cxgb4/qp.c
index 323fa1d872d6..d5cdfcd7906c 100644
--- a/drivers/infiniband/hw/cxgb4/qp.c
+++ b/drivers/infiniband/hw/cxgb4/qp.c
@@ -712,8 +712,7 @@ static int ring_kernel_sq_db(struct c4iw_qp *qhp, u16 inc)
 	spin_lock_irqsave(&qhp->rhp->lock, flags);
 	spin_lock(&qhp->lock);
 	if (qhp->rhp->db_state == NORMAL)
-		t4_ring_sq_db(&qhp->wq, inc,
-			      is_t5(qhp->rhp->rdev.lldi.adapter_type), NULL);
+		t4_ring_sq_db(&qhp->wq, inc, NULL);
 	else {
 		add_to_fc_list(&qhp->rhp->db_fc_list, &qhp->db_fc_entry);
 		qhp->wq.sq.wq_pidx_inc += inc;
@@ -730,8 +729,7 @@ static int ring_kernel_rq_db(struct c4iw_qp *qhp, u16 inc)
 	spin_lock_irqsave(&qhp->rhp->lock, flags);
 	spin_lock(&qhp->lock);
 	if (qhp->rhp->db_state == NORMAL)
-		t4_ring_rq_db(&qhp->wq, inc,
-			      is_t5(qhp->rhp->rdev.lldi.adapter_type), NULL);
+		t4_ring_rq_db(&qhp->wq, inc, NULL);
 	else {
 		add_to_fc_list(&qhp->rhp->db_fc_list, &qhp->db_fc_entry);
 		qhp->wq.rq.wq_pidx_inc += inc;
@@ -817,7 +815,7 @@ int c4iw_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 			fw_opcode = FW_RI_FR_NSMR_WR;
 			swsqe->opcode = FW_RI_FAST_REGISTER;
 			err = build_fastreg(&qhp->wq.sq, wqe, wr, &len16,
-					    is_t5(
+					    !is_t4(
 					    qhp->rhp->rdev.lldi.adapter_type) ?
 					    1 : 0);
 			break;
@@ -860,8 +858,7 @@ int c4iw_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 		idx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);
 	}
 	if (!qhp->rhp->rdev.status_page->db_off) {
-		t4_ring_sq_db(&qhp->wq, idx,
-			      is_t5(qhp->rhp->rdev.lldi.adapter_type), wqe);
+		t4_ring_sq_db(&qhp->wq, idx, wqe);
 		spin_unlock_irqrestore(&qhp->lock, flag);
 	} else {
 		spin_unlock_irqrestore(&qhp->lock, flag);
@@ -934,8 +931,7 @@ int c4iw_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,
 		num_wrs--;
 	}
 	if (!qhp->rhp->rdev.status_page->db_off) {
-		t4_ring_rq_db(&qhp->wq, idx,
-			      is_t5(qhp->rhp->rdev.lldi.adapter_type), wqe);
+		t4_ring_rq_db(&qhp->wq, idx, wqe);
 		spin_unlock_irqrestore(&qhp->lock, flag);
 	} else {
 		spin_unlock_irqrestore(&qhp->lock, flag);
@@ -1875,7 +1871,7 @@ int c4iw_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 	attrs.rq_db_inc = attr->rq_psn;
 	mask |= (attr_mask & IB_QP_SQ_PSN) ? C4IW_QP_ATTR_SQ_DB : 0;
 	mask |= (attr_mask & IB_QP_RQ_PSN) ? C4IW_QP_ATTR_RQ_DB : 0;
-	if (is_t5(to_c4iw_qp(ibqp)->rhp->rdev.lldi.adapter_type) &&
+	if (!is_t4(to_c4iw_qp(ibqp)->rhp->rdev.lldi.adapter_type) &&
 	    (mask & (C4IW_QP_ATTR_SQ_DB|C4IW_QP_ATTR_RQ_DB)))
 		return -EINVAL;
 
diff --git a/drivers/infiniband/hw/cxgb4/t4.h b/drivers/infiniband/hw/cxgb4/t4.h
index 1f446c8ff792..287576c79a23 100644
--- a/drivers/infiniband/hw/cxgb4/t4.h
+++ b/drivers/infiniband/hw/cxgb4/t4.h
@@ -455,8 +455,7 @@ static inline void pio_copy(u64 __iomem *dst, u64 *src)
 	}
 }
 
-static inline void t4_ring_sq_db(struct t4_wq *wq, u16 inc, u8 t5,
-				 union t4_wr *wqe)
+static inline void t4_ring_sq_db(struct t4_wq *wq, u16 inc, union t4_wr *wqe)
 {
 
 	/* Flush host queue memory writes. */
@@ -482,7 +481,7 @@ static inline void t4_ring_sq_db(struct t4_wq *wq, u16 inc, u8 t5,
 	writel(QID_V(wq->sq.qid) | PIDX_V(inc), wq->db);
 }
 
-static inline void t4_ring_rq_db(struct t4_wq *wq, u16 inc, u8 t5,
+static inline void t4_ring_rq_db(struct t4_wq *wq, u16 inc,
 				 union t4_recv_wr *wqe)
 {
 
