nvme: use the block layer for userspace passthrough metadata

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Keith Busch <keith.busch@intel.com>
commit 0b7f1f26f95a51ab11d4dc0adee230212b3cd675
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/0b7f1f26.failed

Use the integrity API to pass through metadata from userspace.  For PI
enabled devices this means that we now validate the reftag, which seems
like an unintentional ommission in the old code.

Thanks to Keith Busch for testing and fixes.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
[Skip metadata setup on admin commands]
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 0b7f1f26f95a51ab11d4dc0adee230212b3cd675)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
#	drivers/nvme/host/core.c
#	drivers/nvme/host/nvme.h
diff --cc drivers/block/nvme-core.c
index 2a6eb55ad96c,aa033f047aaf..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -1562,88 -1633,12 +1562,95 @@@ static int nvme_configure_admin_queue(s
  	return result;
  }
  
 +struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
 +				unsigned long addr, unsigned length)
 +{
 +	int i, err, count, nents, offset;
 +	struct scatterlist *sg;
 +	struct page **pages;
 +	struct nvme_iod *iod;
 +
 +	if (addr & 3)
 +		return ERR_PTR(-EINVAL);
 +	if (!length || length > INT_MAX - PAGE_SIZE)
 +		return ERR_PTR(-EINVAL);
 +
 +	offset = offset_in_page(addr);
 +	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
 +	pages = kcalloc(count, sizeof(*pages), GFP_KERNEL);
 +	if (!pages)
 +		return ERR_PTR(-ENOMEM);
 +
 +	err = get_user_pages_fast(addr, count, 1, pages);
 +	if (err < count) {
 +		count = err;
 +		err = -EFAULT;
 +		goto put_pages;
 +	}
 +
 +	err = -ENOMEM;
 +	iod = __nvme_alloc_iod(count, length, dev, 0, GFP_KERNEL);
 +	if (!iod)
 +		goto put_pages;
 +
 +	sg = iod->sg;
 +	sg_init_table(sg, count);
 +	for (i = 0; i < count; i++) {
 +		sg_set_page(&sg[i], pages[i],
 +			    min_t(unsigned, length, PAGE_SIZE - offset),
 +			    offset);
 +		length -= (PAGE_SIZE - offset);
 +		offset = 0;
 +	}
 +	sg_mark_end(&sg[i - 1]);
 +	iod->nents = count;
 +
 +	nents = dma_map_sg(&dev->pci_dev->dev, sg, count,
 +				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 +	if (!nents)
 +		goto free_iod;
 +
 +	kfree(pages);
 +	return iod;
 +
 + free_iod:
 +	kfree(iod);
 + put_pages:
 +	for (i = 0; i < count; i++)
 +		put_page(pages[i]);
 +	kfree(pages);
 +	return ERR_PTR(err);
 +}
 +
 +void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
 +			struct nvme_iod *iod)
 +{
 +	int i;
 +
 +	dma_unmap_sg(&dev->pci_dev->dev, iod->sg, iod->nents,
 +				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 +
 +	for (i = 0; i < iod->nents; i++)
 +		put_page(sg_page(&iod->sg[i]));
 +}
 +
  static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
  {
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	struct nvme_dev *dev = ns->dev;
 +	struct nvme_user_io io;
 +	struct nvme_command c;
 +	unsigned length, meta_len, prp_len;
 +	int status, write;
 +	struct nvme_iod *iod;
 +	dma_addr_t meta_dma = 0;
 +	void *meta = NULL;
++=======
+ 	struct nvme_user_io io;
+ 	struct nvme_command c;
+ 	unsigned length, meta_len;
+ 	void __user *metadata;
++>>>>>>> 0b7f1f26f95a (nvme: use the block layer for userspace passthrough metadata):drivers/nvme/host/pci.c
  
  	if (copy_from_user(&io, uio, sizeof(io)))
  		return -EFAULT;
@@@ -1669,28 -1652,16 +1676,41 @@@
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	if (IS_ERR(iod))
 +		return PTR_ERR(iod);
 +
 +	prp_len = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
 +	if (length != prp_len) {
 +		status = -ENOMEM;
 +		goto unmap;
 +	}
 +	if (meta_len) {
 +		meta = dma_alloc_coherent(&dev->pci_dev->dev, meta_len,
 +					  &meta_dma, GFP_KERNEL);
 +		if (!meta) {
 +			status = -ENOMEM;
 +			goto unmap;
 +		}
 +		if (write) {
 +			if (copy_from_user(meta, (void __user *)io.metadata,
 +					   meta_len)) {
 +				status = -EFAULT;
 +				goto unmap;
 +			}
 +		}
++=======
+ 	length = (io.nblocks + 1) << ns->lba_shift;
+ 	meta_len = (io.nblocks + 1) * ns->ms;
+ 	metadata = (void __user *)(uintptr_t)io.metadata;
+ 
+ 	if (ns->ext) {
+ 		length += meta_len;
+ 		meta_len = 0;
+ 	} else if (meta_len) {
+ 		if ((io.metadata & 3) || !io.metadata)
+ 			return -EINVAL;
++>>>>>>> 0b7f1f26f95a (nvme: use the block layer for userspace passthrough metadata):drivers/nvme/host/pci.c
  	}
  
  	memset(&c, 0, sizeof(c));
@@@ -1704,26 -1675,13 +1724,33 @@@
  	c.rw.reftag = cpu_to_le32(io.reftag);
  	c.rw.apptag = cpu_to_le16(io.apptag);
  	c.rw.appmask = cpu_to_le16(io.appmask);
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 +	c.rw.prp2 = cpu_to_le64(iod->first_dma);
 +	c.rw.metadata = cpu_to_le64(meta_dma);
 +	status = nvme_submit_sync_cmd(ns->queue, &c);
 + unmap:
 +	nvme_unmap_user_pages(dev, write, iod);
 +	nvme_free_iod(dev, iod);
 +
 +	if (meta) {
 +		if (status == NVME_SC_SUCCESS && !write) {
 +			if (copy_to_user((void __user *)io.metadata, meta,
 +					 meta_len))
 +				status = -EFAULT;
 +		}
 +		dma_free_coherent(&dev->pci_dev->dev, meta_len, meta, meta_dma);
 +	}
 +	return status;
++=======
+ 
+ 	return __nvme_submit_user_cmd(ns->queue, &c,
+ 			(void __user *)(uintptr_t)io.addr, length,
+ 			metadata, meta_len, io.slba, NULL, 0);
++>>>>>>> 0b7f1f26f95a (nvme: use the block layer for userspace passthrough metadata):drivers/nvme/host/pci.c
  }
  
 -static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 +static int nvme_user_cmd(struct nvme_dev *dev, struct nvme_ns *ns,
  			struct nvme_passthru_cmd __user *ucmd)
  {
  	struct nvme_passthru_cmd cmd;
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
* Unmerged path drivers/block/nvme-core.c
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
