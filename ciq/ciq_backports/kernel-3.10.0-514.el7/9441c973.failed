RDS: IB: handle rds_ibdev release case instead of crashing the kernel

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Santosh Shilimkar <santosh.shilimkar@oracle.com>
commit 9441c973e1e0e9885537a3a86020fe8e121e9a98
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/9441c973.failed

Just in case we are still handling the QP receive completion while the
rds_ibdev is released, drop the connection instead of crashing the kernel.

	Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
	Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
(cherry picked from commit 9441c973e1e0e9885537a3a86020fe8e121e9a98)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/rds/ib_cm.c
diff --cc net/rds/ib_cm.c
index c129638ff6f1,2b2370e7f356..000000000000
--- a/net/rds/ib_cm.c
+++ b/net/rds/ib_cm.c
@@@ -207,6 -216,96 +207,99 @@@ static void rds_ib_cq_event_handler(str
  		 event->event, ib_event_msg(event->event), data);
  }
  
++<<<<<<< HEAD
++=======
+ /* Plucking the oldest entry from the ring can be done concurrently with
+  * the thread refilling the ring.  Each ring operation is protected by
+  * spinlocks and the transient state of refilling doesn't change the
+  * recording of which entry is oldest.
+  *
+  * This relies on IB only calling one cq comp_handler for each cq so that
+  * there will only be one caller of rds_recv_incoming() per RDS connection.
+  */
+ static void rds_ib_cq_comp_handler_recv(struct ib_cq *cq, void *context)
+ {
+ 	struct rds_connection *conn = context;
+ 	struct rds_ib_connection *ic = conn->c_transport_data;
+ 
+ 	rdsdebug("conn %p cq %p\n", conn, cq);
+ 
+ 	rds_ib_stats_inc(s_ib_evt_handler_call);
+ 
+ 	tasklet_schedule(&ic->i_recv_tasklet);
+ }
+ 
+ static void poll_cq(struct rds_ib_connection *ic, struct ib_cq *cq,
+ 		    struct ib_wc *wcs,
+ 		    struct rds_ib_ack_state *ack_state)
+ {
+ 	int nr;
+ 	int i;
+ 	struct ib_wc *wc;
+ 
+ 	while ((nr = ib_poll_cq(cq, RDS_IB_WC_MAX, wcs)) > 0) {
+ 		for (i = 0; i < nr; i++) {
+ 			wc = wcs + i;
+ 			rdsdebug("wc wr_id 0x%llx status %u byte_len %u imm_data %u\n",
+ 				 (unsigned long long)wc->wr_id, wc->status,
+ 				 wc->byte_len, be32_to_cpu(wc->ex.imm_data));
+ 
+ 			if (wc->wr_id & RDS_IB_SEND_OP)
+ 				rds_ib_send_cqe_handler(ic, wc);
+ 			else
+ 				rds_ib_recv_cqe_handler(ic, wc, ack_state);
+ 		}
+ 	}
+ }
+ 
+ static void rds_ib_tasklet_fn_send(unsigned long data)
+ {
+ 	struct rds_ib_connection *ic = (struct rds_ib_connection *)data;
+ 	struct rds_connection *conn = ic->conn;
+ 	struct rds_ib_ack_state state;
+ 
+ 	rds_ib_stats_inc(s_ib_tasklet_call);
+ 
+ 	memset(&state, 0, sizeof(state));
+ 	poll_cq(ic, ic->i_send_cq, ic->i_send_wc, &state);
+ 	ib_req_notify_cq(ic->i_send_cq, IB_CQ_NEXT_COMP);
+ 	poll_cq(ic, ic->i_send_cq, ic->i_send_wc, &state);
+ 
+ 	if (rds_conn_up(conn) &&
+ 	    (!test_bit(RDS_LL_SEND_FULL, &conn->c_flags) ||
+ 	    test_bit(0, &conn->c_map_queued)))
+ 		rds_send_xmit(ic->conn);
+ }
+ 
+ static void rds_ib_tasklet_fn_recv(unsigned long data)
+ {
+ 	struct rds_ib_connection *ic = (struct rds_ib_connection *)data;
+ 	struct rds_connection *conn = ic->conn;
+ 	struct rds_ib_device *rds_ibdev = ic->rds_ibdev;
+ 	struct rds_ib_ack_state state;
+ 
+ 	if (!rds_ibdev)
+ 		rds_conn_drop(conn);
+ 
+ 	rds_ib_stats_inc(s_ib_tasklet_call);
+ 
+ 	memset(&state, 0, sizeof(state));
+ 	poll_cq(ic, ic->i_recv_cq, ic->i_recv_wc, &state);
+ 	ib_req_notify_cq(ic->i_recv_cq, IB_CQ_SOLICITED);
+ 	poll_cq(ic, ic->i_recv_cq, ic->i_recv_wc, &state);
+ 
+ 	if (state.ack_next_valid)
+ 		rds_ib_set_ack(ic, state.ack_next, state.ack_required);
+ 	if (state.ack_recv_valid && state.ack_recv > ic->i_ack_recv) {
+ 		rds_send_drop_acked(conn, state.ack_recv, NULL);
+ 		ic->i_ack_recv = state.ack_recv;
+ 	}
+ 
+ 	if (rds_conn_up(conn))
+ 		rds_ib_attempt_ack(ic);
+ }
+ 
++>>>>>>> 9441c973e1e0 (RDS: IB: handle rds_ibdev release case instead of crashing the kernel)
  static void rds_ib_qp_event_handler(struct ib_event *event, void *data)
  {
  	struct rds_connection *conn = data;
* Unmerged path net/rds/ib_cm.c
