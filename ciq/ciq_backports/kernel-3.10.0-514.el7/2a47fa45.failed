ixgbe: enable l2 forwarding acceleration for macvlans

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author John Fastabend <john.r.fastabend@intel.com>
commit 2a47fa45d4dfbc54659d28de311a1f764b296a3c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/2a47fa45.failed

Now that l2 acceleration ops are in place from the prior patch,
enable ixgbe to take advantage of these operations.  Allow it to
allocate queues for a macvlan so that when we transmit a frame,
we can do the switching in hardware inside the ixgbe card, rather
than in software.

	Signed-off-by: John Fastabend <john.r.fastabend@intel.com>
	Signed-off-by: Neil Horman <nhorman@tuxdriver.com>
CC: Andy Gospodarek <andy@greyhouse.net>
CC: "David S. Miller" <davem@davemloft.net>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 2a47fa45d4dfbc54659d28de311a1f764b296a3c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ixgbe/ixgbe.h
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
#	drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe.h
index 86d789ab072e,f38fc0a343a2..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe.h
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
@@@ -293,17 -301,17 +303,31 @@@ enum ixgbe_ring_f_enum 
  	RING_F_ARRAY_SIZE      /* must be last in enum set */
  };
  
++<<<<<<< HEAD
 +#define IXGBE_MAX_RSS_INDICES		16
 +#define IXGBE_MAX_RSS_INDICES_X550	64
 +#define IXGBE_MAX_VMDQ_INDICES		64
 +#define IXGBE_MAX_FDIR_INDICES		63	/* based on q_vector limit */
 +#define IXGBE_MAX_FCOE_INDICES		8
 +#define MAX_RX_QUEUES			(IXGBE_MAX_FDIR_INDICES + 1)
 +#define MAX_TX_QUEUES			(IXGBE_MAX_FDIR_INDICES + 1)
 +#define IXGBE_MAX_L2A_QUEUES		4
 +#define IXGBE_BAD_L2A_QUEUE		3
 +#define IXGBE_MAX_MACVLANS		31
 +#define IXGBE_MAX_DCBMACVLANS		8
++=======
+ #define IXGBE_MAX_RSS_INDICES  16
+ #define IXGBE_MAX_VMDQ_INDICES 64
+ #define IXGBE_MAX_FDIR_INDICES 63	/* based on q_vector limit */
+ #define IXGBE_MAX_FCOE_INDICES  8
+ #define MAX_RX_QUEUES (IXGBE_MAX_FDIR_INDICES + 1)
+ #define MAX_TX_QUEUES (IXGBE_MAX_FDIR_INDICES + 1)
+ #define IXGBE_MAX_L2A_QUEUES 4
+ #define IXGBE_MAX_L2A_QUEUES 4
+ #define IXGBE_BAD_L2A_QUEUE 3
+ #define IXGBE_MAX_MACVLANS	31
+ #define IXGBE_MAX_DCBMACVLANS	8
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
  
  struct ixgbe_ring_feature {
  	u16 limit;	/* upper limit on feature indices */
@@@ -787,32 -783,8 +811,35 @@@ struct ixgbe_adapter 
  
  	u8 default_up;
  	unsigned long fwd_bitmask; /* Bitmask indicating in use pools */
++<<<<<<< HEAD
 +
 +/* maximum number of RETA entries among all devices supported by ixgbe
 + * driver: currently it's x550 device in non-SRIOV mode
 + */
 +#define IXGBE_MAX_RETA_ENTRIES 512
 +	u8 rss_indir_tbl[IXGBE_MAX_RETA_ENTRIES];
 +
 +#define IXGBE_RSS_KEY_SIZE     40  /* size of RSS Hash Key in bytes */
 +	u32 rss_key[IXGBE_RSS_KEY_SIZE / sizeof(u32)];
++=======
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
  };
  
 +static inline u8 ixgbe_max_rss_indices(struct ixgbe_adapter *adapter)
 +{
 +	switch (adapter->hw.mac.type) {
 +	case ixgbe_mac_82598EB:
 +	case ixgbe_mac_82599EB:
 +	case ixgbe_mac_X540:
 +		return IXGBE_MAX_RSS_INDICES;
 +	case ixgbe_mac_X550:
 +	case ixgbe_mac_X550EM_x:
 +		return IXGBE_MAX_RSS_INDICES_X550;
 +	default:
 +		return 0;
 +	}
 +}
 +
  struct ixgbe_fdir_filter {
  	struct hlist_node fdir_node;
  	union ixgbe_atr_input filter;
@@@ -982,5 -956,7 +1009,11 @@@ void ixgbe_ptp_check_pps_event(struct i
  void ixgbe_sriov_reinit(struct ixgbe_adapter *adapter);
  #endif
  
++<<<<<<< HEAD
 +u32 ixgbe_rss_indir_tbl_entries(struct ixgbe_adapter *adapter);
++=======
+ netdev_tx_t ixgbe_xmit_frame_ring(struct sk_buff *skb,
+ 				  struct ixgbe_adapter *adapter,
+ 				  struct ixgbe_ring *tx_ring);
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
  #endif /* _IXGBE_H_ */
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index e464f2e3e2eb,607275de2f1e..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -3850,10 -3687,12 +3857,14 @@@ static void ixgbe_vlan_strip_disable(st
  		break;
  	case ixgbe_mac_82599EB:
  	case ixgbe_mac_X540:
 +	case ixgbe_mac_X550:
 +	case ixgbe_mac_X550EM_x:
  		for (i = 0; i < adapter->num_rx_queues; i++) {
- 			j = adapter->rx_ring[i]->reg_idx;
+ 			struct ixgbe_ring *ring = adapter->rx_ring[i];
+ 
+ 			if (ring->l2_accel_priv)
+ 				continue;
+ 			j = ring->reg_idx;
  			vlnctrl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(j));
  			vlnctrl &= ~IXGBE_RXDCTL_VME;
  			IXGBE_WRITE_REG(hw, IXGBE_RXDCTL(j), vlnctrl);
@@@ -3882,10 -3721,12 +3893,14 @@@ static void ixgbe_vlan_strip_enable(str
  		break;
  	case ixgbe_mac_82599EB:
  	case ixgbe_mac_X540:
 +	case ixgbe_mac_X550:
 +	case ixgbe_mac_X550EM_x:
  		for (i = 0; i < adapter->num_rx_queues; i++) {
- 			j = adapter->rx_ring[i]->reg_idx;
+ 			struct ixgbe_ring *ring = adapter->rx_ring[i];
+ 
+ 			if (ring->l2_accel_priv)
+ 				continue;
+ 			j = ring->reg_idx;
  			vlnctrl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(j));
  			vlnctrl |= IXGBE_RXDCTL_VME;
  			IXGBE_WRITE_REG(hw, IXGBE_RXDCTL(j), vlnctrl);
@@@ -4067,13 -3756,19 +4082,20 @@@ int ixgbe_del_mac_filter(struct ixgbe_a
   *                0 on no addresses written
   *                X on writing X addresses to the RAR table
   **/
 -static int ixgbe_write_uc_addr_list(struct net_device *netdev)
 +static int ixgbe_write_uc_addr_list(struct net_device *netdev, int vfn)
  {
  	struct ixgbe_adapter *adapter = netdev_priv(netdev);
 -	struct ixgbe_hw *hw = &adapter->hw;
 -	unsigned int rar_entries = hw->mac.num_rar_entries - 1;
  	int count = 0;
  
++<<<<<<< HEAD
++=======
+ 	/* In SR-IOV/VMDQ modes significantly less RAR entries are available */
+ 	if (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)
+ 		rar_entries = IXGBE_MAX_PF_MACVLANS - 1;
+ 
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
  	/* return ENOMEM indicating insufficient memory for addresses */
 -	if (netdev_uc_count(netdev) > rar_entries)
 +	if (netdev_uc_count(netdev) > ixgbe_available_rars(adapter))
  		return -ENOMEM;
  
  	if (!netdev_uc_empty(netdev)) {
@@@ -4498,8 -4403,33 +4744,9 @@@ static void ixgbe_configure(struct ixgb
  #endif /* IXGBE_FCOE */
  	ixgbe_configure_tx(adapter);
  	ixgbe_configure_rx(adapter);
+ 	ixgbe_configure_dfwd(adapter);
  }
  
 -static inline bool ixgbe_is_sfp(struct ixgbe_hw *hw)
 -{
 -	switch (hw->phy.type) {
 -	case ixgbe_phy_sfp_avago:
 -	case ixgbe_phy_sfp_ftl:
 -	case ixgbe_phy_sfp_intel:
 -	case ixgbe_phy_sfp_unknown:
 -	case ixgbe_phy_sfp_passive_tyco:
 -	case ixgbe_phy_sfp_passive_unknown:
 -	case ixgbe_phy_sfp_active_unknown:
 -	case ixgbe_phy_sfp_ftl_active:
 -	case ixgbe_phy_qsfp_passive_unknown:
 -	case ixgbe_phy_qsfp_active_unknown:
 -	case ixgbe_phy_qsfp_intel:
 -	case ixgbe_phy_qsfp_unknown:
 -		return true;
 -	case ixgbe_phy_nl:
 -		if (hw->mac.type == ixgbe_mac_82598EB)
 -			return true;
 -	default:
 -		return false;
 -	}
 -}
 -
  /**
   * ixgbe_sfp_link_config - set up SFP+ link
   * @adapter: pointer to private adapter struct
@@@ -4779,56 -4703,6 +5038,59 @@@ void ixgbe_reset(struct ixgbe_adapter *
  }
  
  /**
++<<<<<<< HEAD
 + * ixgbe_clean_rx_ring - Free Rx Buffers per Queue
 + * @rx_ring: ring to free buffers from
 + **/
 +static void ixgbe_clean_rx_ring(struct ixgbe_ring *rx_ring)
 +{
 +	struct device *dev = rx_ring->dev;
 +	unsigned long size;
 +	u16 i;
 +
 +	/* ring already cleared, nothing to do */
 +	if (!rx_ring->rx_buffer_info)
 +		return;
 +
 +	/* Free all the Rx ring sk_buffs */
 +	for (i = 0; i < rx_ring->count; i++) {
 +		struct ixgbe_rx_buffer *rx_buffer = &rx_ring->rx_buffer_info[i];
 +
 +		if (rx_buffer->skb) {
 +			struct sk_buff *skb = rx_buffer->skb;
 +			if (IXGBE_CB(skb)->page_released)
 +				dma_unmap_page(dev,
 +					       IXGBE_CB(skb)->dma,
 +					       ixgbe_rx_bufsz(rx_ring),
 +					       DMA_FROM_DEVICE);
 +			dev_kfree_skb(skb);
 +			rx_buffer->skb = NULL;
 +		}
 +
 +		if (!rx_buffer->page)
 +			continue;
 +
 +		dma_unmap_page(dev, rx_buffer->dma,
 +			       ixgbe_rx_pg_size(rx_ring), DMA_FROM_DEVICE);
 +		__free_pages(rx_buffer->page, ixgbe_rx_pg_order(rx_ring));
 +
 +		rx_buffer->page = NULL;
 +	}
 +
 +	size = sizeof(struct ixgbe_rx_buffer) * rx_ring->count;
 +	memset(rx_ring->rx_buffer_info, 0, size);
 +
 +	/* Zero out the descriptor ring */
 +	memset(rx_ring->desc, 0, rx_ring->size);
 +
 +	rx_ring->next_to_alloc = 0;
 +	rx_ring->next_to_clean = 0;
 +	rx_ring->next_to_use = 0;
 +}
 +
 +/**
++=======
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
   * ixgbe_clean_tx_ring - Free Tx Buffers
   * @tx_ring: ring to be cleaned
   **/
@@@ -4905,6 -4779,9 +5167,12 @@@ void ixgbe_down(struct ixgbe_adapter *a
  {
  	struct net_device *netdev = adapter->netdev;
  	struct ixgbe_hw *hw = &adapter->hw;
++<<<<<<< HEAD
++=======
+ 	struct net_device *upper;
+ 	struct list_head *iter;
+ 	u32 rxctrl;
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
  	int i;
  
  	/* signal that we are down to the interrupt handler */
@@@ -5475,8 -5354,7 +5758,12 @@@ static int ixgbe_change_mtu(struct net_
  static int ixgbe_open(struct net_device *netdev)
  {
  	struct ixgbe_adapter *adapter = netdev_priv(netdev);
++<<<<<<< HEAD
 +	struct ixgbe_hw *hw = &adapter->hw;
 +	int err;
++=======
+ 	int err, queues;
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
  
  	/* disallow open during test */
  	if (test_bit(__IXGBE_TESTING, &adapter->state))
@@@ -7211,10 -6994,15 +7504,11 @@@ static netdev_tx_t __ixgbe_xmit_frame(s
  	 * The minimum packet size for olinfo paylen is 17 so pad the skb
  	 * in order to meet this minimum size requirement.
  	 */
 -	if (unlikely(skb->len < 17)) {
 -		if (skb_pad(skb, 17 - skb->len))
 -			return NETDEV_TX_OK;
 -		skb->len = 17;
 -		skb_set_tail_pointer(skb, 17);
 -	}
 +	if (skb_put_padto(skb, 17))
 +		return NETDEV_TX_OK;
  
- 	tx_ring = adapter->tx_ring[skb->queue_mapping];
+ 	tx_ring = ring ? ring : adapter->tx_ring[skb->queue_mapping];
+ 
  	return ixgbe_xmit_frame_ring(skb, adapter, tx_ring);
  }
  
@@@ -7769,10 -7526,102 +8074,98 @@@ static int ixgbe_ndo_bridge_getlink(str
  	if (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))
  		return 0;
  
 -	if (adapter->flags2 & IXGBE_FLAG2_BRIDGE_MODE_VEB)
 -		mode = BRIDGE_MODE_VEB;
 -	else
 -		mode = BRIDGE_MODE_VEPA;
 -
 -	return ndo_dflt_bridge_getlink(skb, pid, seq, dev, mode);
 +	return ndo_dflt_bridge_getlink(skb, pid, seq, dev,
 +				       adapter->bridge_mode, 0, 0);
  }
  
+ static void *ixgbe_fwd_add(struct net_device *pdev, struct net_device *vdev)
+ {
+ 	struct ixgbe_fwd_adapter *fwd_adapter = NULL;
+ 	struct ixgbe_adapter *adapter = netdev_priv(pdev);
+ 	int pool, err;
+ 
+ 	/* Check for hardware restriction on number of rx/tx queues */
+ 	if (vdev->num_rx_queues != vdev->num_tx_queues ||
+ 	    vdev->num_tx_queues > IXGBE_MAX_L2A_QUEUES ||
+ 	    vdev->num_tx_queues == IXGBE_BAD_L2A_QUEUE) {
+ 		netdev_info(pdev,
+ 			    "%s: Supports RX/TX Queue counts 1,2, and 4\n",
+ 			    pdev->name);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	if (((adapter->flags & IXGBE_FLAG_DCB_ENABLED) &&
+ 	      adapter->num_rx_pools > IXGBE_MAX_DCBMACVLANS - 1) ||
+ 	    (adapter->num_rx_pools > IXGBE_MAX_MACVLANS))
+ 		return ERR_PTR(-EBUSY);
+ 
+ 	fwd_adapter = kcalloc(1, sizeof(struct ixgbe_fwd_adapter), GFP_KERNEL);
+ 	if (!fwd_adapter)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	pool = find_first_zero_bit(&adapter->fwd_bitmask, 32);
+ 	adapter->num_rx_pools++;
+ 	set_bit(pool, &adapter->fwd_bitmask);
+ 
+ 	/* Enable VMDq flag so device will be set in VM mode */
+ 	adapter->flags |= IXGBE_FLAG_VMDQ_ENABLED | IXGBE_FLAG_SRIOV_ENABLED;
+ 	adapter->ring_feature[RING_F_VMDQ].limit = adapter->num_rx_pools;
+ 	adapter->ring_feature[RING_F_RSS].limit = vdev->num_rx_queues;
+ 
+ 	/* Force reinit of ring allocation with VMDQ enabled */
+ 	err = ixgbe_setup_tc(pdev, netdev_get_num_tc(pdev));
+ 	if (err)
+ 		goto fwd_add_err;
+ 	fwd_adapter->pool = pool;
+ 	fwd_adapter->real_adapter = adapter;
+ 	err = ixgbe_fwd_ring_up(vdev, fwd_adapter);
+ 	if (err)
+ 		goto fwd_add_err;
+ 	netif_tx_start_all_queues(vdev);
+ 	return fwd_adapter;
+ fwd_add_err:
+ 	/* unwind counter and free adapter struct */
+ 	netdev_info(pdev,
+ 		    "%s: dfwd hardware acceleration failed\n", vdev->name);
+ 	clear_bit(pool, &adapter->fwd_bitmask);
+ 	adapter->num_rx_pools--;
+ 	kfree(fwd_adapter);
+ 	return ERR_PTR(err);
+ }
+ 
+ static void ixgbe_fwd_del(struct net_device *pdev, void *priv)
+ {
+ 	struct ixgbe_fwd_adapter *fwd_adapter = priv;
+ 	struct ixgbe_adapter *adapter = fwd_adapter->real_adapter;
+ 
+ 	clear_bit(fwd_adapter->pool, &adapter->fwd_bitmask);
+ 	adapter->num_rx_pools--;
+ 
+ 	adapter->ring_feature[RING_F_VMDQ].limit = adapter->num_rx_pools;
+ 	ixgbe_fwd_ring_down(fwd_adapter->netdev, fwd_adapter);
+ 	ixgbe_setup_tc(pdev, netdev_get_num_tc(pdev));
+ 	netdev_dbg(pdev, "pool %i:%i queues %i:%i VSI bitmask %lx\n",
+ 		   fwd_adapter->pool, adapter->num_rx_pools,
+ 		   fwd_adapter->rx_base_queue,
+ 		   fwd_adapter->rx_base_queue + adapter->num_rx_queues_per_pool,
+ 		   adapter->fwd_bitmask);
+ 	kfree(fwd_adapter);
+ }
+ 
+ static netdev_tx_t ixgbe_fwd_xmit(struct sk_buff *skb,
+ 				  struct net_device *dev,
+ 				  void *priv)
+ {
+ 	struct ixgbe_fwd_adapter *fwd_adapter = priv;
+ 	unsigned int queue;
+ 	struct ixgbe_ring *tx_ring;
+ 
+ 	queue = skb->queue_mapping + fwd_adapter->tx_base_queue;
+ 	tx_ring = fwd_adapter->real_adapter->tx_ring[queue];
+ 
+ 	return __ixgbe_xmit_frame(skb, dev, tx_ring);
+ }
+ 
  static const struct net_device_ops ixgbe_netdev_ops = {
  	.ndo_open		= ixgbe_open,
  	.ndo_stop		= ixgbe_close,
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
index d8bd0fc90bf6,a8571e488ea4..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
@@@ -229,12 -223,15 +229,20 @@@ int ixgbe_disable_sriov(struct ixgbe_ad
  	IXGBE_WRITE_FLUSH(hw);
  
  	/* Disable VMDq flag so device will be set in VM mode */
- 	if (adapter->ring_feature[RING_F_VMDQ].limit == 1)
+ 	if (adapter->ring_feature[RING_F_VMDQ].limit == 1) {
  		adapter->flags &= ~IXGBE_FLAG_VMDQ_ENABLED;
- 	adapter->ring_feature[RING_F_VMDQ].offset = 0;
+ 		adapter->flags &= ~IXGBE_FLAG_SRIOV_ENABLED;
+ 		rss = min_t(int, IXGBE_MAX_RSS_INDICES, num_online_cpus());
+ 	} else {
+ 		rss = min_t(int, IXGBE_MAX_L2A_QUEUES, num_online_cpus());
+ 	}
  
++<<<<<<< HEAD
 +	rss = min_t(int, ixgbe_max_rss_indices(adapter),
 +		    num_online_cpus());
++=======
+ 	adapter->ring_feature[RING_F_VMDQ].offset = 0;
++>>>>>>> 2a47fa45d4df (ixgbe: enable l2 forwarding acceleration for macvlans)
  	adapter->ring_feature[RING_F_RSS].limit = rss;
  
  	/* take a breather then clean up driver data */
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe.h
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c
index 14a289533eec..e678a9c10c99 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_lib.c
@@ -501,6 +501,7 @@ static bool ixgbe_set_sriov_queues(struct ixgbe_adapter *adapter)
 #ifdef IXGBE_FCOE
 	u16 fcoe_i = 0;
 #endif
+	bool pools = (find_first_zero_bit(&adapter->fwd_bitmask, 32) > 1);
 
 	/* only proceed if SR-IOV is enabled */
 	if (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))
@@ -513,7 +514,7 @@ static bool ixgbe_set_sriov_queues(struct ixgbe_adapter *adapter)
 	vmdq_i = min_t(u16, IXGBE_MAX_VMDQ_INDICES, vmdq_i);
 
 	/* 64 pool mode with 2 queues per pool */
-	if ((vmdq_i > 32) || (rss_i < 4)) {
+	if ((vmdq_i > 32) || (rss_i < 4) || (vmdq_i > 16 && pools)) {
 		vmdq_m = IXGBE_82599_VMDQ_2Q_MASK;
 		rss_m = IXGBE_RSS_2Q_MASK;
 		rss_i = min_t(u16, rss_i, 2);
@@ -884,7 +885,11 @@ static int ixgbe_alloc_q_vector(struct ixgbe_adapter *adapter,
 
 		/* apply Tx specific ring traits */
 		ring->count = adapter->tx_ring_count;
-		ring->queue_index = txr_idx;
+		if (adapter->num_rx_pools > 1)
+			ring->queue_index =
+				txr_idx % adapter->num_rx_queues_per_pool;
+		else
+			ring->queue_index = txr_idx;
 
 		/* assign ring to adapter */
 		adapter->tx_ring[txr_idx] = ring;
@@ -927,7 +932,11 @@ static int ixgbe_alloc_q_vector(struct ixgbe_adapter *adapter,
 #endif /* IXGBE_FCOE */
 		/* apply Rx specific ring traits */
 		ring->count = adapter->rx_ring_count;
-		ring->queue_index = rxr_idx;
+		if (adapter->num_rx_pools > 1)
+			ring->queue_index =
+				rxr_idx % adapter->num_rx_queues_per_pool;
+		else
+			ring->queue_index = rxr_idx;
 
 		/* assign ring to adapter */
 		adapter->rx_ring[rxr_idx] = ring;
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
