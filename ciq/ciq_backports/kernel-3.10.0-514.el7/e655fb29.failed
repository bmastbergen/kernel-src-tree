mm: introduce do_read_fault()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] introduce do_read_fault() (Eric Sandeen) [1274459]
Rebuild_FUZZ: 92.59%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit e655fb29074a7aa471bfc9f51a0139c6f636a649
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e655fb29.failed

Introduce do_read_fault().  The function does what do_fault() does for
read page faults.

Unlike do_fault(), do_read_fault() is pretty clean and straightforward.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e655fb29074a7aa471bfc9f51a0139c6f636a649)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,56784e9a7151..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3508,8 -3286,76 +3508,79 @@@ oom
  	return VM_FAULT_OOM;
  }
  
++<<<<<<< HEAD
++=======
+ static int __do_fault(struct vm_area_struct *vma, unsigned long address,
+ 		pgoff_t pgoff, unsigned int flags, struct page **page)
+ {
+ 	struct vm_fault vmf;
+ 	int ret;
+ 
+ 	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
+ 	vmf.pgoff = pgoff;
+ 	vmf.flags = flags;
+ 	vmf.page = NULL;
+ 
+ 	ret = vma->vm_ops->fault(vma, &vmf);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
+ 
+ 	if (unlikely(PageHWPoison(vmf.page))) {
+ 		if (ret & VM_FAULT_LOCKED)
+ 			unlock_page(vmf.page);
+ 		page_cache_release(vmf.page);
+ 		return VM_FAULT_HWPOISON;
+ 	}
+ 
+ 	if (unlikely(!(ret & VM_FAULT_LOCKED)))
+ 		lock_page(vmf.page);
+ 	else
+ 		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
+ 
+ 	*page = vmf.page;
+ 	return ret;
+ }
+ 
+ static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pmd_t *pmd,
+ 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
+ {
+ 	struct page *fault_page;
+ 	spinlock_t *ptl;
+ 	pte_t entry, *pte;
+ 	int ret;
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
+ 
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		return ret;
+ 	}
+ 
+ 	flush_icache_page(vma, fault_page);
+ 	entry = mk_pte(fault_page, vma->vm_page_prot);
+ 	if (pte_file(orig_pte) && pte_file_soft_dirty(orig_pte))
+ 		pte_mksoft_dirty(entry);
+ 	inc_mm_counter_fast(mm, MM_FILEPAGES);
+ 	page_add_file_rmap(fault_page);
+ 	set_pte_at(mm, address, pte, entry);
+ 
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 	pte_unmap_unlock(pte, ptl);
+ 	unlock_page(fault_page);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> e655fb29074a (mm: introduce do_read_fault())
  /*
 - * do_fault() tries to create a new page mapping. It aggressively
 + * __do_fault() tries to create a new page mapping. It aggressively
   * tries to share with existing pages, but makes a separate copy if
   * the FAULT_FLAG_WRITE is set in the flags parameter in order to avoid
   * the next page fault.
@@@ -3715,10 -3547,10 +3786,17 @@@ static int do_linear_fault(struct mm_st
  			- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
  
  	pte_unmap(page_table);
++<<<<<<< HEAD
 +	/* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
 +	if (!vma->vm_ops->fault)
 +		return VM_FAULT_SIGBUS;
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++=======
+ 	if (!(flags & FAULT_FLAG_WRITE))
+ 		return do_read_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	return do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++>>>>>>> e655fb29074a (mm: introduce do_read_fault())
  }
  
  /*
@@@ -3750,10 -3582,13 +3828,17 @@@ static int do_nonlinear_fault(struct mm
  	}
  
  	pgoff = pte_to_pgoff(orig_pte);
++<<<<<<< HEAD
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++=======
+ 	if (!(flags & FAULT_FLAG_WRITE))
+ 		return do_read_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	return do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++>>>>>>> e655fb29074a (mm: introduce do_read_fault())
  }
  
 -static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 +int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
  				unsigned long addr, int page_nid,
  				int *flags)
  {
* Unmerged path mm/memory.c
