pmem, memremap: convert to numa aware allocations

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 538ea4aa44737127ce2b5c8511c7349d2abdcf9c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/538ea4aa.failed

Given that pmem ranges come with numa-locality hints, arrange for the
resulting driver objects to be obtained from node-local memory.

	Reviewed-by: Tejun Heo <tj@kernel.org>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 538ea4aa44737127ce2b5c8511c7349d2abdcf9c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvdimm/pmem.c
#	kernel/memremap.c
diff --cc kernel/memremap.c
index 26717809cbd2,3218e8b1fc28..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -123,9 -114,10 +123,10 @@@ void *devm_memremap(struct device *dev
  {
  	void **ptr, *addr;
  
- 	ptr = devres_alloc(devm_memremap_release, sizeof(*ptr), GFP_KERNEL);
+ 	ptr = devres_alloc_node(devm_memremap_release, sizeof(*ptr), GFP_KERNEL,
+ 			dev_to_node(dev));
  	if (!ptr)
 -		return ERR_PTR(-ENOMEM);
 +		return NULL;
  
  	addr = memremap(offset, size, flags);
  	if (addr) {
@@@ -144,3 -136,55 +145,58 @@@ void devm_memunmap(struct device *dev, 
  				devm_memremap_match, addr));
  }
  EXPORT_SYMBOL(devm_memunmap);
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_ZONE_DEVICE
+ struct page_map {
+ 	struct resource res;
+ };
+ 
+ static void devm_memremap_pages_release(struct device *dev, void *res)
+ {
+ 	struct page_map *page_map = res;
+ 
+ 	/* pages are dead and unused, undo the arch mapping */
+ 	arch_remove_memory(page_map->res.start, resource_size(&page_map->res));
+ }
+ 
+ void *devm_memremap_pages(struct device *dev, struct resource *res)
+ {
+ 	int is_ram = region_intersects(res->start, resource_size(res),
+ 			"System RAM");
+ 	struct page_map *page_map;
+ 	int error, nid;
+ 
+ 	if (is_ram == REGION_MIXED) {
+ 		WARN_ONCE(1, "%s attempted on mixed region %pr\n",
+ 				__func__, res);
+ 		return ERR_PTR(-ENXIO);
+ 	}
+ 
+ 	if (is_ram == REGION_INTERSECTS)
+ 		return __va(res->start);
+ 
+ 	page_map = devres_alloc_node(devm_memremap_pages_release,
+ 			sizeof(*page_map), GFP_KERNEL, dev_to_node(dev));
+ 	if (!page_map)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	memcpy(&page_map->res, res, sizeof(*res));
+ 
+ 	nid = dev_to_node(dev);
+ 	if (nid < 0)
+ 		nid = numa_mem_id();
+ 
+ 	error = arch_add_memory(nid, res->start, resource_size(res), true);
+ 	if (error) {
+ 		devres_free(page_map);
+ 		return ERR_PTR(error);
+ 	}
+ 
+ 	devres_add(dev, page_map);
+ 	return __va(res->start);
+ }
+ EXPORT_SYMBOL(devm_memremap_pages);
+ #endif /* CONFIG_ZONE_DEVICE */
++>>>>>>> 538ea4aa4473 (pmem, memremap: convert to numa aware allocations)
* Unmerged path drivers/nvdimm/pmem.c
* Unmerged path drivers/nvdimm/pmem.c
* Unmerged path kernel/memremap.c
