libceph: switch to calc_target(), part 1

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Ilya Dryomov <idryomov@gmail.com>
commit a66dd38309f5d9c66ec9bc7911ff8da8cc37bb9f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a66dd383.failed

Replace __calc_request_pg() and most of __map_request() with
calc_target() and start using req->r_t.

ceph_osdc_build_request() however still encodes base_oid, because it's
called before calc_target() is and target_oid is empty at that point in
time; a printf in osdc_show() also shows base_oid.  This is fixed in
"libceph: switch to calc_target(), part 2".

	Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
(cherry picked from commit a66dd38309f5d9c66ec9bc7911ff8da8cc37bb9f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ceph/osd_client.c
diff --cc net/ceph/osd_client.c
index 4e649b707367,013101598c41..000000000000
--- a/net/ceph/osd_client.c
+++ b/net/ceph/osd_client.c
@@@ -327,7 -350,9 +327,11 @@@ static void ceph_osdc_release_request(s
  	for (which = 0; which < req->r_num_ops; which++)
  		osd_req_op_data_release(req, which);
  
++<<<<<<< HEAD
++=======
+ 	target_destroy(&req->r_t);
++>>>>>>> a66dd38309f5 (libceph: switch to calc_target(), part 1)
  	ceph_put_snap_context(req->r_snapc);
 -
  	if (req->r_mempool)
  		mempool_free(req, req->r_osdc->req_mempool);
  	else if (req->r_num_ops <= CEPH_OSD_SLAB_OPS)
@@@ -394,28 -419,22 +398,32 @@@ struct ceph_osd_request *ceph_osdc_allo
  	INIT_LIST_HEAD(&req->r_req_lru_item);
  	INIT_LIST_HEAD(&req->r_osd_item);
  
++<<<<<<< HEAD
 +	req->r_base_oloc.pool = -1;
 +	req->r_target_oloc.pool = -1;
++=======
+ 	target_init(&req->r_t);
++>>>>>>> a66dd38309f5 (libceph: switch to calc_target(), part 1)
  
 -	dout("%s req %p\n", __func__, req);
 -	return req;
 -}
 -EXPORT_SYMBOL(ceph_osdc_alloc_request);
 -
 -int ceph_osdc_alloc_messages(struct ceph_osd_request *req, gfp_t gfp)
 -{
 -	struct ceph_osd_client *osdc = req->r_osdc;
 -	struct ceph_msg *msg;
 -	int msg_size;
 +	msg_size = OSD_OPREPLY_FRONT_LEN;
 +	if (num_ops > CEPH_OSD_SLAB_OPS) {
 +		/* ceph_osd_op and rval */
 +		msg_size += (num_ops - CEPH_OSD_SLAB_OPS) *
 +			    (sizeof(struct ceph_osd_op) + 4);
 +	}
  
 -	WARN_ON(ceph_oid_empty(&req->r_base_oid));
 +	/* create reply message */
 +	if (use_mempool)
 +		msg = ceph_msgpool_get(&osdc->msgpool_op_reply, 0);
 +	else
 +		msg = ceph_msg_new(CEPH_MSG_OSD_OPREPLY, msg_size,
 +				   gfp_flags, true);
 +	if (!msg) {
 +		ceph_osdc_put_request(req);
 +		return NULL;
 +	}
 +	req->r_reply = msg;
  
 -	/* create request message */
  	msg_size = 4 + 4 + 4; /* client_inc, osdmap_epoch, flags */
  	msg_size += 4 + 4 + 4 + 8; /* mtime, reassert_version */
  	msg_size += 2 + 4 + 8 + 4 + 4; /* oloc */
@@@ -1291,53 -1304,128 +1299,177 @@@ EXPORT_SYMBOL(ceph_osdc_set_request_lin
   *
   * Caller should hold map_sem for read.
   */
++<<<<<<< HEAD
 +static bool __req_should_be_paused(struct ceph_osd_client *osdc,
 +				   struct ceph_osd_request *req)
 +{
 +	bool pauserd = ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSERD);
 +	bool pausewr = ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSEWR) ||
 +		ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_FULL);
 +	return (req->r_flags & CEPH_OSD_FLAG_READ && pauserd) ||
 +		(req->r_flags & CEPH_OSD_FLAG_WRITE && pausewr);
 +}
 +
 +/*
 + * Calculate mapping of a request to a PG.  Takes tiering into account.
 + */
 +static int __calc_request_pg(struct ceph_osdmap *osdmap,
 +			     struct ceph_osd_request *req,
 +			     struct ceph_pg *pg_out)
 +{
 +	bool need_check_tiering;
 +
 +	need_check_tiering = false;
 +	if (req->r_target_oloc.pool == -1) {
 +		req->r_target_oloc = req->r_base_oloc; /* struct */
 +		need_check_tiering = true;
 +	}
 +	if (req->r_target_oid.name_len == 0) {
 +		ceph_oid_copy(&req->r_target_oid, &req->r_base_oid);
++=======
+ static bool target_should_be_paused(struct ceph_osd_client *osdc,
+ 				    const struct ceph_osd_request_target *t,
+ 				    struct ceph_pg_pool_info *pi)
+ {
+ 	bool pauserd = ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSERD);
+ 	bool pausewr = ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_PAUSEWR) ||
+ 		       ceph_osdmap_flag(osdc->osdmap, CEPH_OSDMAP_FULL) ||
+ 		       __pool_full(pi);
+ 
+ 	WARN_ON(pi->id != t->base_oloc.pool);
+ 	return (t->flags & CEPH_OSD_FLAG_READ && pauserd) ||
+ 	       (t->flags & CEPH_OSD_FLAG_WRITE && pausewr);
+ }
+ 
+ enum calc_target_result {
+ 	CALC_TARGET_NO_ACTION = 0,
+ 	CALC_TARGET_NEED_RESEND,
+ 	CALC_TARGET_POOL_DNE,
+ };
+ 
+ static enum calc_target_result calc_target(struct ceph_osd_client *osdc,
+ 					   struct ceph_osd_request_target *t,
+ 					   u32 *last_force_resend,
+ 					   bool any_change)
+ {
+ 	struct ceph_pg_pool_info *pi;
+ 	struct ceph_pg pgid, last_pgid;
+ 	struct ceph_osds up, acting;
+ 	bool force_resend = false;
+ 	bool need_check_tiering = false;
+ 	bool need_resend = false;
+ 	bool sort_bitwise = ceph_osdmap_flag(osdc->osdmap,
+ 					     CEPH_OSDMAP_SORTBITWISE);
+ 	enum calc_target_result ct_res;
+ 	int ret;
+ 
+ 	pi = ceph_pg_pool_by_id(osdc->osdmap, t->base_oloc.pool);
+ 	if (!pi) {
+ 		t->osd = CEPH_HOMELESS_OSD;
+ 		ct_res = CALC_TARGET_POOL_DNE;
+ 		goto out;
+ 	}
+ 
+ 	if (osdc->osdmap->epoch == pi->last_force_request_resend) {
+ 		if (last_force_resend &&
+ 		    *last_force_resend < pi->last_force_request_resend) {
+ 			*last_force_resend = pi->last_force_request_resend;
+ 			force_resend = true;
+ 		} else if (!last_force_resend) {
+ 			force_resend = true;
+ 		}
+ 	}
+ 	if (ceph_oid_empty(&t->target_oid) || force_resend) {
+ 		ceph_oid_copy(&t->target_oid, &t->base_oid);
+ 		need_check_tiering = true;
+ 	}
+ 	if (ceph_oloc_empty(&t->target_oloc) || force_resend) {
+ 		ceph_oloc_copy(&t->target_oloc, &t->base_oloc);
++>>>>>>> a66dd38309f5 (libceph: switch to calc_target(), part 1)
  		need_check_tiering = true;
  	}
  
  	if (need_check_tiering &&
++<<<<<<< HEAD
 +	    (req->r_flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) {
 +		struct ceph_pg_pool_info *pi;
 +
 +		pi = ceph_pg_pool_by_id(osdmap, req->r_target_oloc.pool);
 +		if (pi) {
 +			if ((req->r_flags & CEPH_OSD_FLAG_READ) &&
 +			    pi->read_tier >= 0)
 +				req->r_target_oloc.pool = pi->read_tier;
 +			if ((req->r_flags & CEPH_OSD_FLAG_WRITE) &&
 +			    pi->write_tier >= 0)
 +				req->r_target_oloc.pool = pi->write_tier;
 +		}
 +		/* !pi is caught in ceph_oloc_oid_to_pg() */
 +	}
 +
 +	return ceph_oloc_oid_to_pg(osdmap, &req->r_target_oloc,
 +				   &req->r_target_oid, pg_out);
++=======
+ 	    (t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) {
+ 		if (t->flags & CEPH_OSD_FLAG_READ && pi->read_tier >= 0)
+ 			t->target_oloc.pool = pi->read_tier;
+ 		if (t->flags & CEPH_OSD_FLAG_WRITE && pi->write_tier >= 0)
+ 			t->target_oloc.pool = pi->write_tier;
+ 	}
+ 
+ 	ret = ceph_object_locator_to_pg(osdc->osdmap, &t->target_oid,
+ 					&t->target_oloc, &pgid);
+ 	if (ret) {
+ 		WARN_ON(ret != -ENOENT);
+ 		t->osd = CEPH_HOMELESS_OSD;
+ 		ct_res = CALC_TARGET_POOL_DNE;
+ 		goto out;
+ 	}
+ 	last_pgid.pool = pgid.pool;
+ 	last_pgid.seed = ceph_stable_mod(pgid.seed, t->pg_num, t->pg_num_mask);
+ 
+ 	ceph_pg_to_up_acting_osds(osdc->osdmap, &pgid, &up, &acting);
+ 	if (any_change &&
+ 	    ceph_is_new_interval(&t->acting,
+ 				 &acting,
+ 				 &t->up,
+ 				 &up,
+ 				 t->size,
+ 				 pi->size,
+ 				 t->min_size,
+ 				 pi->min_size,
+ 				 t->pg_num,
+ 				 pi->pg_num,
+ 				 t->sort_bitwise,
+ 				 sort_bitwise,
+ 				 &last_pgid))
+ 		force_resend = true;
+ 
+ 	if (t->paused && !target_should_be_paused(osdc, t, pi)) {
+ 		t->paused = false;
+ 		need_resend = true;
+ 	}
+ 
+ 	if (ceph_pg_compare(&t->pgid, &pgid) ||
+ 	    ceph_osds_changed(&t->acting, &acting, any_change) ||
+ 	    force_resend) {
+ 		t->pgid = pgid; /* struct */
+ 		ceph_osds_copy(&t->acting, &acting);
+ 		ceph_osds_copy(&t->up, &up);
+ 		t->size = pi->size;
+ 		t->min_size = pi->min_size;
+ 		t->pg_num = pi->pg_num;
+ 		t->pg_num_mask = pi->pg_num_mask;
+ 		t->sort_bitwise = sort_bitwise;
+ 
+ 		t->osd = acting.primary;
+ 		need_resend = true;
+ 	}
+ 
+ 	ct_res = need_resend ? CALC_TARGET_NEED_RESEND : CALC_TARGET_NO_ACTION;
+ out:
+ 	dout("%s t %p -> ct_res %d osd %d\n", __func__, t, ct_res, t->osd);
+ 	return ct_res;
++>>>>>>> a66dd38309f5 (libceph: switch to calc_target(), part 1)
  }
  
  static void __enqueue_request(struct ceph_osd_request *req)
@@@ -1814,7 -1890,7 +1926,11 @@@ static void handle_reply(struct ceph_os
  
  		__unregister_request(osdc, req);
  
++<<<<<<< HEAD
 +		req->r_target_oloc = redir.oloc; /* struct */
++=======
+ 		ceph_oloc_copy(&req->r_t.target_oloc, &redir.oloc);
++>>>>>>> a66dd38309f5 (libceph: switch to calc_target(), part 1)
  
  		/*
  		 * Start redirect requests with nofail=true.  If
diff --git a/include/linux/ceph/osd_client.h b/include/linux/ceph/osd_client.h
index cc16ab3e4c14..f22803c78cb2 100644
--- a/include/linux/ceph/osd_client.h
+++ b/include/linux/ceph/osd_client.h
@@ -127,12 +127,13 @@ struct ceph_osd_request {
 	struct list_head r_linger_item;
 	struct list_head r_linger_osd_item;
 	struct ceph_osd *r_osd;
-	struct ceph_pg   r_pgid;
-	int              r_pg_osds[CEPH_PG_MAX_SIZE];
-	int              r_num_pg_osds;
+
+	struct ceph_osd_request_target r_t;
+#define r_base_oid	r_t.base_oid
+#define r_base_oloc	r_t.base_oloc
+#define r_flags		r_t.flags
 
 	struct ceph_msg  *r_request, *r_reply;
-	int               r_flags;     /* any additional flags for the osd */
 	u32               r_sent;      /* >0 if r_request is sending/sent */
 
 	/* request osd ops array  */
@@ -144,7 +145,6 @@ struct ceph_osd_request {
 	__le64           *r_request_pool;
 	void             *r_request_pgid;
 	__le32           *r_request_attempts;
-	bool              r_paused;
 	struct ceph_eversion *r_request_reassert_version;
 
 	int               r_result;
@@ -163,11 +163,6 @@ struct ceph_osd_request {
 	struct inode *r_inode;         	      /* for use by callbacks */
 	void *r_priv;			      /* ditto */
 
-	struct ceph_object_locator r_base_oloc;
-	struct ceph_object_id r_base_oid;
-	struct ceph_object_locator r_target_oloc;
-	struct ceph_object_id r_target_oid;
-
 	u64               r_snapid;
 	unsigned long     r_stamp;            /* send OR check time */
 
diff --git a/net/ceph/debugfs.c b/net/ceph/debugfs.c
index 1633b622f0f7..2e0fdccc45c6 100644
--- a/net/ceph/debugfs.c
+++ b/net/ceph/debugfs.c
@@ -156,7 +156,7 @@ static int osdc_show(struct seq_file *s, void *pp)
 
 		seq_printf(s, "%lld\tosd%d\t%lld.%x\t", req->r_tid,
 			   req->r_osd ? req->r_osd->o_osd : -1,
-			   req->r_pgid.pool, req->r_pgid.seed);
+			   req->r_t.pgid.pool, req->r_t.pgid.seed);
 
 		seq_printf(s, "%.*s", req->r_base_oid.name_len,
 			   req->r_base_oid.name);
* Unmerged path net/ceph/osd_client.c
