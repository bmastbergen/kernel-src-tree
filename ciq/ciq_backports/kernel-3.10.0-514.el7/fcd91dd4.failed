net: add recursion limit to GRO

jira LE-1907
cve CVE-2016-7039
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [net] add recursion limit to GRO (Sabrina Dubroca) [1374191] {CVE-2016-7039}
Rebuild_FUZZ: 91.23%
commit-author Sabrina Dubroca <sd@queasysnail.net>
commit fcd91dd449867c6bfe56a81cabba76b829fd05cd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/fcd91dd4.failed

Currently, GRO can do unlimited recursion through the gro_receive
handlers.  This was fixed for tunneling protocols by limiting tunnel GRO
to one level with encap_mark, but both VLAN and TEB still have this
problem.  Thus, the kernel is vulnerable to a stack overflow, if we
receive a packet composed entirely of VLAN headers.

This patch adds a recursion counter to the GRO layer to prevent stack
overflow.  When a gro_receive function hits the recursion limit, GRO is
aborted for this skb and it is processed normally.  This recursion
counter is put in the GRO CB, but could be turned into a percpu counter
if we run out of space in the CB.

Thanks to Vladimír Beneš <vbenes@redhat.com> for the initial bug report.

Fixes: CVE-2016-7039
Fixes: 9b174d88c257 ("net: Add Transparent Ethernet Bridging GRO support.")
Fixes: 66e5133f19e9 ("vlan: Add GRO support for non hardware accelerated vlan")
	Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
	Reviewed-by: Jiri Benc <jbenc@redhat.com>
	Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Acked-by: Tom Herbert <tom@herbertland.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit fcd91dd449867c6bfe56a81cabba76b829fd05cd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/netdevice.h
#	net/core/dev.c
#	net/ipv4/udp_offload.c
diff --cc include/linux/netdevice.h
index 4c4e05c381ed,91ee3643ccc8..000000000000
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@@ -1829,10 -2160,19 +1829,23 @@@ struct napi_gro_cb 
  #define NAPI_GRO_FREE		  1
  #define NAPI_GRO_FREE_STOLEN_HEAD 2
  
 -	/* Used in foo-over-udp, set in udp[46]_gro_receive */
 -	u8	is_ipv6:1;
 +	/* This is non-zero if the packet may be of the same flow. */
 +	u8	same_flow:1;
  
++<<<<<<< HEAD
 +	/* 7 bit hole */
++=======
+ 	/* Used in GRE, set in fou/gue_gro_receive */
+ 	u8	is_fou:1;
+ 
+ 	/* Used to determine if flush_id can be ignored */
+ 	u8	is_atomic:1;
+ 
+ 	/* Number of gro_receive callbacks this packet already went through */
+ 	u8 recursion_counter:4;
+ 
+ 	/* 1 bit hole */
++>>>>>>> fcd91dd44986 (net: add recursion limit to GRO)
  
  	/* used to support CHECKSUM_COMPLETE for tunneling protocols */
  	__wsum	csum;
diff --cc net/core/dev.c
index 2ce9ccdd4029,dbc871306910..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -3846,7 -4510,10 +3846,14 @@@ static enum gro_result dev_gro_receive(
  		NAPI_GRO_CB(skb)->same_flow = 0;
  		NAPI_GRO_CB(skb)->flush = 0;
  		NAPI_GRO_CB(skb)->free = 0;
++<<<<<<< HEAD
 +		NAPI_GRO_CB(skb)->udp_mark = 0;
++=======
+ 		NAPI_GRO_CB(skb)->encap_mark = 0;
+ 		NAPI_GRO_CB(skb)->recursion_counter = 0;
+ 		NAPI_GRO_CB(skb)->is_fou = 0;
+ 		NAPI_GRO_CB(skb)->is_atomic = 1;
++>>>>>>> fcd91dd44986 (net: add recursion limit to GRO)
  		NAPI_GRO_CB(skb)->gro_remcsum_start = 0;
  
  		/* Setup for GRO checksum validation */
diff --cc net/ipv4/udp_offload.c
index 72c08903d3e9,b2be1d9757ef..000000000000
--- a/net/ipv4/udp_offload.c
+++ b/net/ipv4/udp_offload.c
@@@ -342,9 -295,7 +342,13 @@@ unflush
  
  	skb_gro_pull(skb, sizeof(struct udphdr)); /* pull encapsulating udp header */
  	skb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));
++<<<<<<< HEAD
 +	NAPI_GRO_CB(skb)->proto = uo_priv->offload->ipproto;
 +	pp = uo_priv->offload->callbacks.gro_receive(head, skb,
 +						     uo_priv->offload);
++=======
+ 	pp = call_gro_receive_sk(udp_sk(sk)->gro_receive, sk, head, skb);
++>>>>>>> fcd91dd44986 (net: add recursion limit to GRO)
  
  out_unlock:
  	rcu_read_unlock();
diff --git a/drivers/net/geneve.c b/drivers/net/geneve.c
index 02b7bf0b4bec..3e44df863a7f 100644
--- a/drivers/net/geneve.c
+++ b/drivers/net/geneve.c
@@ -373,7 +373,7 @@ static struct sk_buff **geneve_gro_receive(struct sk_buff **head,
 
 	skb_gro_pull(skb, gh_len);
 	skb_gro_postpull_rcsum(skb, gh, gh_len);
-	pp = ptype->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ptype->callbacks.gro_receive, head, skb);
 	flush = 0;
 
 out_unlock:
diff --git a/drivers/net/vxlan.c b/drivers/net/vxlan.c
index c9c40f07cb6d..72d9226b1112 100644
--- a/drivers/net/vxlan.c
+++ b/drivers/net/vxlan.c
@@ -624,7 +624,7 @@ static struct sk_buff **vxlan_gro_receive(struct sk_buff **head,
 		}
 	}
 
-	pp = eth_gro_receive(head, skb);
+	pp = call_gro_receive(eth_gro_receive, head, skb);
 	flush = 0;
 
 out:
* Unmerged path include/linux/netdevice.h
diff --git a/net/8021q/vlan.c b/net/8021q/vlan.c
index 1d512256f545..7fa34b123fc9 100644
--- a/net/8021q/vlan.c
+++ b/net/8021q/vlan.c
@@ -640,7 +640,7 @@ static struct sk_buff **vlan_gro_receive(struct sk_buff **head,
 
 	skb_gro_pull(skb, sizeof(*vhdr));
 	skb_gro_postpull_rcsum(skb, vhdr, sizeof(*vhdr));
-	pp = ptype->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ptype->callbacks.gro_receive, head, skb);
 
 out_unlock:
 	rcu_read_unlock();
* Unmerged path net/core/dev.c
diff --git a/net/ethernet/eth.c b/net/ethernet/eth.c
index e9efc9e40e1c..ec0002991bb8 100644
--- a/net/ethernet/eth.c
+++ b/net/ethernet/eth.c
@@ -499,7 +499,7 @@ struct sk_buff **eth_gro_receive(struct sk_buff **head,
 
 	skb_gro_pull(skb, sizeof(*eh));
 	skb_gro_postpull_rcsum(skb, eh, sizeof(*eh));
-	pp = ptype->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ptype->callbacks.gro_receive, head, skb);
 
 out_unlock:
 	rcu_read_unlock();
diff --git a/net/ipv4/af_inet.c b/net/ipv4/af_inet.c
index 96cfb62894e8..1f7bd8c47918 100644
--- a/net/ipv4/af_inet.c
+++ b/net/ipv4/af_inet.c
@@ -1379,7 +1379,7 @@ static struct sk_buff **inet_gro_receive(struct sk_buff **head,
 	skb_gro_pull(skb, sizeof(*iph));
 	skb_set_transport_header(skb, skb_gro_offset(skb));
 
-	pp = ops->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ops->callbacks.gro_receive, head, skb);
 
 out_unlock:
 	rcu_read_unlock();
diff --git a/net/ipv4/fou.c b/net/ipv4/fou.c
index 90bfd2d78747..73cf0db78563 100644
--- a/net/ipv4/fou.c
+++ b/net/ipv4/fou.c
@@ -186,7 +186,7 @@ static struct sk_buff **fou_gro_receive(struct sk_buff **head,
 	if (!ops || !ops->callbacks.gro_receive)
 		goto out_unlock;
 
-	pp = ops->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ops->callbacks.gro_receive, head, skb);
 
 out_unlock:
 	rcu_read_unlock();
@@ -343,7 +343,7 @@ static struct sk_buff **gue_gro_receive(struct sk_buff **head,
 	if (WARN_ON(!ops || !ops->callbacks.gro_receive))
 		goto out_unlock;
 
-	pp = ops->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ops->callbacks.gro_receive, head, skb);
 	flush = 0;
 
 out_unlock:
diff --git a/net/ipv4/gre_offload.c b/net/ipv4/gre_offload.c
index bbd163adf449..e22c939e5f07 100644
--- a/net/ipv4/gre_offload.c
+++ b/net/ipv4/gre_offload.c
@@ -211,7 +211,7 @@ static struct sk_buff **gre_gro_receive(struct sk_buff **head,
 	/* Adjusted NAPI_GRO_CB(skb)->csum after skb_gro_pull()*/
 	skb_gro_postpull_rcsum(skb, greh, grehlen);
 
-	pp = ptype->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ptype->callbacks.gro_receive, head, skb);
 	flush = 0;
 
 out_unlock:
* Unmerged path net/ipv4/udp_offload.c
diff --git a/net/ipv6/ip6_offload.c b/net/ipv6/ip6_offload.c
index b8fec4830926..b2328c3360fc 100644
--- a/net/ipv6/ip6_offload.c
+++ b/net/ipv6/ip6_offload.c
@@ -247,7 +247,7 @@ static struct sk_buff **ipv6_gro_receive(struct sk_buff **head,
 
 	skb_gro_postpull_rcsum(skb, iph, nlen);
 
-	pp = ops->callbacks.gro_receive(head, skb);
+	pp = call_gro_receive(ops->callbacks.gro_receive, head, skb);
 
 out_unlock:
 	rcu_read_unlock();
