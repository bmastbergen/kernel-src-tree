mm: ZONE_DEVICE for "device memory"

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] ZONE_DEVICE for "device memory" (Jeff Moyer) [1345801 1348502 1348836 1350149 1350156]
Rebuild_FUZZ: 93.94%
commit-author Dan Williams <dan.j.williams@intel.com>
commit 033fbae988fcb67e5077203512181890848b8e90
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/033fbae9.failed

While pmem is usable as a block device or via DAX mappings to userspace
there are several usage scenarios that can not target pmem due to its
lack of struct page coverage. In preparation for "hot plugging" pmem
into the vmemmap add ZONE_DEVICE as a new zone to tag these pages
separately from the ones that are subject to standard page allocations.
Importantly "device memory" can be removed at will by userspace
unbinding the driver of the device.

Having a separate zone prevents allocation and otherwise marks these
pages that are distinct from typical uniform memory.  Device memory has
different lifetime and performance characteristics than RAM.  However,
since we have run out of ZONES_SHIFT bits this functionality currently
depends on sacrificing ZONE_DMA.

	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Jerome Glisse <j.glisse@gmail.com>
[hch: various simplifications in the arch interface]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 033fbae988fcb67e5077203512181890848b8e90)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/ia64/mm/init.c
#	arch/sh/mm/init.c
#	arch/x86/mm/init_32.c
#	arch/x86/mm/init_64.c
#	mm/Kconfig
diff --cc arch/ia64/mm/init.c
index d1fe4b402601,1841ef69183d..000000000000
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@@ -664,7 -655,8 +664,12 @@@ int arch_add_memory(int nid, u64 start
  
  	pgdat = NODE_DATA(nid);
  
++<<<<<<< HEAD
 +	zone = pgdat->node_zones + ZONE_NORMAL;
++=======
+ 	zone = pgdat->node_zones +
+ 		zone_for_memory(nid, start, size, ZONE_NORMAL, for_device);
++>>>>>>> 033fbae988fc (mm: ZONE_DEVICE for "device memory")
  	ret = __add_pages(nid, zone, start_pfn, nr_pages);
  
  	if (ret)
diff --cc arch/sh/mm/init.c
index 20f9ead650d3,c1490096b863..000000000000
--- a/arch/sh/mm/init.c
+++ b/arch/sh/mm/init.c
@@@ -520,8 -495,10 +520,15 @@@ int arch_add_memory(int nid, u64 start
  	pgdat = NODE_DATA(nid);
  
  	/* We only have ZONE_NORMAL, so this is easy.. */
++<<<<<<< HEAD
 +	ret = __add_pages(nid, pgdat->node_zones + ZONE_NORMAL,
 +				start_pfn, nr_pages);
++=======
+ 	ret = __add_pages(nid, pgdat->node_zones +
+ 			zone_for_memory(nid, start, size, ZONE_NORMAL,
+ 			for_device),
+ 			start_pfn, nr_pages);
++>>>>>>> 033fbae988fc (mm: ZONE_DEVICE for "device memory")
  	if (unlikely(ret))
  		printk("%s: Failed, __add_pages() == %d\n", __func__, ret);
  
diff --cc arch/x86/mm/init_32.c
index 152d0c729562,2a9237d20a70..000000000000
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@@ -845,10 -822,11 +845,15 @@@ void __init mem_init(void
  }
  
  #ifdef CONFIG_MEMORY_HOTPLUG
- int arch_add_memory(int nid, u64 start, u64 size)
+ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
  {
  	struct pglist_data *pgdata = NODE_DATA(nid);
++<<<<<<< HEAD
 +	struct zone *zone = pgdata->node_zones + ZONE_HIGHMEM;
++=======
+ 	struct zone *zone = pgdata->node_zones +
+ 		zone_for_memory(nid, start, size, ZONE_HIGHMEM, for_device);
++>>>>>>> 033fbae988fc (mm: ZONE_DEVICE for "device memory")
  	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  
diff --cc arch/x86/mm/init_64.c
index 79259c2e343b,30564e2752d3..000000000000
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@@ -699,10 -687,11 +699,15 @@@ static void  update_end_of_memory_vars(
   * Memory is added always to NORMAL zone. This means you will never get
   * additional DMA/DMA32 memory.
   */
- int arch_add_memory(int nid, u64 start, u64 size)
+ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
  {
  	struct pglist_data *pgdat = NODE_DATA(nid);
++<<<<<<< HEAD
 +	struct zone *zone = pgdat->node_zones + ZONE_NORMAL;
++=======
+ 	struct zone *zone = pgdat->node_zones +
+ 		zone_for_memory(nid, start, size, ZONE_NORMAL, for_device);
++>>>>>>> 033fbae988fc (mm: ZONE_DEVICE for "device memory")
  	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  	int ret;
diff --cc mm/Kconfig
index 88ac087b6ee9,a0cd086df16b..000000000000
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@@ -572,3 -608,66 +572,69 @@@ config PGTABLE_MAPPIN
  
  	  You can check speed with zsmalloc benchmark:
  	  https://github.com/spartacus06/zsmapbench
++<<<<<<< HEAD
++=======
+ 
+ config ZSMALLOC_STAT
+ 	bool "Export zsmalloc statistics"
+ 	depends on ZSMALLOC
+ 	select DEBUG_FS
+ 	help
+ 	  This option enables code in the zsmalloc to collect various
+ 	  statistics about whats happening in zsmalloc and exports that
+ 	  information to userspace via debugfs.
+ 	  If unsure, say N.
+ 
+ config GENERIC_EARLY_IOREMAP
+ 	bool
+ 
+ config MAX_STACK_SIZE_MB
+ 	int "Maximum user stack size for 32-bit processes (MB)"
+ 	default 80
+ 	range 8 256 if METAG
+ 	range 8 2048
+ 	depends on STACK_GROWSUP && (!64BIT || COMPAT)
+ 	help
+ 	  This is the maximum stack size in Megabytes in the VM layout of 32-bit
+ 	  user processes when the stack grows upwards (currently only on parisc
+ 	  and metag arch). The stack will be located at the highest memory
+ 	  address minus the given value, unless the RLIMIT_STACK hard limit is
+ 	  changed to a smaller value in which case that is used.
+ 
+ 	  A sane initial value is 80 MB.
+ 
+ # For architectures that support deferred memory initialisation
+ config ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	bool
+ 
+ config DEFERRED_STRUCT_PAGE_INIT
+ 	bool "Defer initialisation of struct pages to kswapd"
+ 	default n
+ 	depends on ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	depends on MEMORY_HOTPLUG
+ 	help
+ 	  Ordinarily all struct pages are initialised during early boot in a
+ 	  single thread. On very large machines this can take a considerable
+ 	  amount of time. If this option is set, large machines will bring up
+ 	  a subset of memmap at boot and then initialise the rest in parallel
+ 	  when kswapd starts. This has a potential performance impact on
+ 	  processes running early in the lifetime of the systemm until kswapd
+ 	  finishes the initialisation.
+ 
+ config ZONE_DEVICE
+ 	bool "Device memory (pmem, etc...) hotplug support" if EXPERT
+ 	default !ZONE_DMA
+ 	depends on !ZONE_DMA
+ 	depends on MEMORY_HOTPLUG
+ 	depends on MEMORY_HOTREMOVE
+ 	depends on X86_64 #arch_add_memory() comprehends device memory
+ 
+ 	help
+ 	  Device memory hotplug support allows for establishing pmem,
+ 	  or other device driver discovered memory regions, in the
+ 	  memmap. This allows pfn_to_page() lookups of otherwise
+ 	  "device-physical" addresses which is needed for using a DAX
+ 	  mapping in an O_DIRECT operation, among other things.
+ 
+ 	  If FS_DAX is enabled, then say Y.
++>>>>>>> 033fbae988fc (mm: ZONE_DEVICE for "device memory")
* Unmerged path arch/ia64/mm/init.c
diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e561abe3749a..d1db37486ff2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -114,7 +114,7 @@ int memory_add_physaddr_to_nid(u64 start)
 }
 #endif
 
-int arch_add_memory(int nid, u64 start, u64 size)
+int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 {
 	struct pglist_data *pgdata;
 	struct zone *zone;
@@ -129,7 +129,7 @@ int arch_add_memory(int nid, u64 start, u64 size)
 
 	/* this should work for most non-highmem platforms */
 	zone = pgdata->node_zones +
-		zone_for_memory(nid, start, size, 0);
+		zone_for_memory(nid, start, size, 0, for_device);
 
 	return __add_pages(nid, zone, start_pfn, nr_pages);
 }
diff --git a/arch/s390/mm/init.c b/arch/s390/mm/init.c
index cf410b36b055..90ad7122679c 100644
--- a/arch/s390/mm/init.c
+++ b/arch/s390/mm/init.c
@@ -182,7 +182,7 @@ void __init free_initrd_mem(unsigned long start, unsigned long end)
 #endif
 
 #ifdef CONFIG_MEMORY_HOTPLUG
-int arch_add_memory(int nid, u64 start, u64 size)
+int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 {
 	unsigned long zone_start_pfn, zone_end_pfn, nr_pages;
 	unsigned long start_pfn = PFN_DOWN(start);
* Unmerged path arch/sh/mm/init.c
diff --git a/arch/tile/mm/init.c b/arch/tile/mm/init.c
index 2749515a0547..768f586d8992 100644
--- a/arch/tile/mm/init.c
+++ b/arch/tile/mm/init.c
@@ -921,7 +921,7 @@ void __init mem_init(void)
  * memory to the highmem for now.
  */
 #ifndef CONFIG_NEED_MULTIPLE_NODES
-int arch_add_memory(u64 start, u64 size)
+int arch_add_memory(u64 start, u64 size, bool for_device)
 {
 	struct pglist_data *pgdata = &contig_page_data;
 	struct zone *zone = pgdata->node_zones + MAX_NR_ZONES-1;
* Unmerged path arch/x86/mm/init_32.c
* Unmerged path arch/x86/mm/init_64.c
diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index ebe3e1dfae11..65cf624fe8a9 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -258,8 +258,9 @@ extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
 		void *arg, int (*func)(struct memory_block *, void *));
 extern int mem_online_node(int nid);
 extern int add_memory(int nid, u64 start, u64 size);
-extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default);
-extern int arch_add_memory(int nid, u64 start, u64 size);
+extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
+		bool for_device);
+extern int arch_add_memory(int nid, u64 start, u64 size, bool for_device);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 extern bool is_memblock_offlined(struct memory_block *mem);
 extern void remove_memory(int nid, u64 start, u64 size);
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 2d47b2da44e8..3d1259202086 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -311,7 +311,11 @@ enum zone_type {
 	ZONE_HIGHMEM,
 #endif
 	ZONE_MOVABLE,
+#ifdef CONFIG_ZONE_DEVICE
+	ZONE_DEVICE,
+#endif
 	__MAX_NR_ZONES
+
 };
 
 #ifndef __GENERATING_BOUNDS_H
@@ -821,6 +825,25 @@ static inline bool pgdat_is_empty(pg_data_t *pgdat)
 	return !pgdat->node_start_pfn && !pgdat->node_spanned_pages;
 }
 
+static inline int zone_id(const struct zone *zone)
+{
+	struct pglist_data *pgdat = zone->zone_pgdat;
+
+	return zone - pgdat->node_zones;
+}
+
+#ifdef CONFIG_ZONE_DEVICE
+static inline bool is_dev_zone(const struct zone *zone)
+{
+	return zone_id(zone) == ZONE_DEVICE;
+}
+#else
+static inline bool is_dev_zone(const struct zone *zone)
+{
+	return false;
+}
+#endif
+
 #include <linux/memory_hotplug.h>
 
 extern struct mutex zonelists_mutex;
* Unmerged path mm/Kconfig
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index e7bfdf837815..d0bde83bf934 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -713,7 +713,10 @@ int __remove_pages(struct zone *zone, unsigned long phys_start_pfn,
 
 	start = phys_start_pfn << PAGE_SHIFT;
 	size = nr_pages * PAGE_SIZE;
-	ret = release_mem_region_adjustable(&iomem_resource, start, size);
+
+	/* in the ZONE_DEVICE case device driver owns the memory region */
+	if (!is_dev_zone(zone))
+		ret = release_mem_region_adjustable(&iomem_resource, start, size);
 	if (ret) {
 		resource_size_t endres = start + size - 1;
 
@@ -1148,8 +1151,13 @@ static int should_add_memory_movable(int nid, u64 start, u64 size)
 	return 0;
 }
 
-int zone_for_memory(int nid, u64 start, u64 size, int zone_default)
+int zone_for_memory(int nid, u64 start, u64 size, int zone_default,
+		bool for_device)
 {
+#ifdef CONFIG_ZONE_DEVICE
+	if (for_device)
+		return ZONE_DEVICE;
+#endif
 	if (should_add_memory_movable(nid, start, size))
 		return ZONE_MOVABLE;
 
@@ -1189,7 +1197,7 @@ int __ref add_memory(int nid, u64 start, u64 size)
 	}
 
 	/* call arch's memory hotadd */
-	ret = arch_add_memory(nid, start, size);
+	ret = arch_add_memory(nid, start, size, false);
 
 	if (ret < 0)
 		goto error;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 20d353397e7d..7655a9042aef 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -199,6 +199,9 @@ static char * const zone_names[MAX_NR_ZONES] = {
 	 "HighMem",
 #endif
 	 "Movable",
+#ifdef CONFIG_ZONE_DEVICE
+	 "Device",
+#endif
 };
 
 int min_free_kbytes = 1024;
