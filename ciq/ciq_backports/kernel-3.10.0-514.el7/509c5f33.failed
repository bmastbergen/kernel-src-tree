IB/srp: Prevent mapping failures

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Bart Van Assche <bart.vanassche@sandisk.com>
commit 509c5f33f4f6dc328d96bf4099ef6589739f22d4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/509c5f33.failed

If both max_sectors and the queue_depth are high enough it can
happen that the MR pool is depleted temporarily. This causes
the SRP initiator to report mapping failures. Although the SRP
initiator recovers from such mapping failures, prevent that
this can happen by allocating more memory regions.

Additionally, only enable memory registration if at least two
pages can be registered per memory region.

	Reported-by: Laurence Oberman <loberman@redhat.com>
	Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Tested-by: Laurence Oberman <loberman@redhat.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Sagi Grimberg <sagi@grimberg.me>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 509c5f33f4f6dc328d96bf4099ef6589739f22d4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/ulp/srp/ib_srp.c
diff --cc drivers/infiniband/ulp/srp/ib_srp.c
index eb57f69536ed,6a5ccd4c7e63..000000000000
--- a/drivers/infiniband/ulp/srp/ib_srp.c
+++ b/drivers/infiniband/ulp/srp/ib_srp.c
@@@ -497,8 -468,7 +497,12 @@@ static int srp_create_ch_ib(struct srp_
  	struct ib_qp *qp;
  	struct ib_fmr_pool *fmr_pool = NULL;
  	struct srp_fr_pool *fr_pool = NULL;
++<<<<<<< HEAD
 +	const int m = dev->use_fast_reg ? 3 : 1;
 +	struct ib_cq_init_attr cq_attr = {};
++=======
+ 	const int m = 1 + dev->use_fast_reg * target->mr_per_cmd * 2;
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
  	int ret;
  
  	init_attr = kzalloc(sizeof *init_attr, GFP_KERNEL);
@@@ -1306,8 -1289,25 +1310,26 @@@ static int srp_map_finish_fmr(struct sr
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int srp_map_finish_fr(struct srp_map_state *state,
 +			     struct srp_rdma_ch *ch)
++=======
+ static void srp_reg_mr_err_done(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	srp_handle_qp_err(cq, wc, "FAST REG");
+ }
+ 
+ /*
+  * Map up to sg_nents elements of state->sg where *sg_offset_p is the offset
+  * where to start in the first element. If sg_offset_p != NULL then
+  * *sg_offset_p is updated to the offset in state->sg[retval] of the first
+  * byte that has not yet been mapped.
+  */
+ static int srp_map_finish_fr(struct srp_map_state *state,
+ 			     struct srp_request *req,
+ 			     struct srp_rdma_ch *ch, int sg_nents,
+ 			     unsigned int *sg_offset_p)
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
  {
  	struct srp_target_port *target = ch->target;
  	struct srp_device *dev = target->srp_host->srp_dev;
@@@ -1319,6 -1320,19 +1341,22 @@@
  	if (state->fr.next >= state->fr.end)
  		return -ENOMEM;
  
++<<<<<<< HEAD
++=======
+ 	WARN_ON_ONCE(!dev->use_fast_reg);
+ 
+ 	if (sg_nents == 1 && target->global_mr) {
+ 		unsigned int sg_offset = sg_offset_p ? *sg_offset_p : 0;
+ 
+ 		srp_map_desc(state, sg_dma_address(state->sg) + sg_offset,
+ 			     sg_dma_len(state->sg) - sg_offset,
+ 			     target->global_mr->rkey);
+ 		if (sg_offset_p)
+ 			*sg_offset_p = 0;
+ 		return 1;
+ 	}
+ 
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
  	desc = srp_fr_pool_get(ch->fr_pool);
  	if (!desc)
  		return -ENOMEM;
@@@ -1326,56 -1340,44 +1364,91 @@@
  	rkey = ib_inc_rkey(desc->mr->rkey);
  	ib_update_fast_reg_key(desc->mr, rkey);
  
++<<<<<<< HEAD
 +	memcpy(desc->frpl->page_list, state->pages,
 +	       sizeof(state->pages[0]) * state->npages);
 +
 +	memset(&wr, 0, sizeof(wr));
 +	wr.opcode = IB_WR_FAST_REG_MR;
 +	wr.wr_id = FAST_REG_WR_ID_MASK;
 +	wr.wr.fast_reg.iova_start = state->base_dma_addr;
 +	wr.wr.fast_reg.page_list = desc->frpl;
 +	wr.wr.fast_reg.page_list_len = state->npages;
 +	wr.wr.fast_reg.page_shift = ilog2(dev->mr_page_size);
 +	wr.wr.fast_reg.length = state->dma_len;
 +	wr.wr.fast_reg.access_flags = (IB_ACCESS_LOCAL_WRITE |
 +				       IB_ACCESS_REMOTE_READ |
 +				       IB_ACCESS_REMOTE_WRITE);
 +	wr.wr.fast_reg.rkey = desc->mr->lkey;
++=======
+ 	n = ib_map_mr_sg(desc->mr, state->sg, sg_nents, sg_offset_p,
+ 			 dev->mr_page_size);
+ 	if (unlikely(n < 0)) {
+ 		srp_fr_pool_put(ch->fr_pool, &desc, 1);
+ 		pr_debug("%s: ib_map_mr_sg(%d, %d) returned %d.\n",
+ 			 dev_name(&req->scmnd->device->sdev_gendev), sg_nents,
+ 			 sg_offset_p ? *sg_offset_p : -1, n);
+ 		return n;
+ 	}
+ 
+ 	WARN_ON_ONCE(desc->mr->length == 0);
+ 
+ 	req->reg_cqe.done = srp_reg_mr_err_done;
+ 
+ 	wr.wr.next = NULL;
+ 	wr.wr.opcode = IB_WR_REG_MR;
+ 	wr.wr.wr_cqe = &req->reg_cqe;
+ 	wr.wr.num_sge = 0;
+ 	wr.wr.send_flags = 0;
+ 	wr.mr = desc->mr;
+ 	wr.key = desc->mr->rkey;
+ 	wr.access = (IB_ACCESS_LOCAL_WRITE |
+ 		     IB_ACCESS_REMOTE_READ |
+ 		     IB_ACCESS_REMOTE_WRITE);
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
  
  	*state->fr.next++ = desc;
  	state->nmdesc++;
  
 -	srp_map_desc(state, desc->mr->iova,
 -		     desc->mr->length, desc->mr->rkey);
 +	srp_map_desc(state, state->base_dma_addr, state->dma_len,
 +		     desc->mr->rkey);
  
++<<<<<<< HEAD
 +	return ib_post_send(ch->qp, &wr, &bad_wr);
 +}
++=======
+ 	err = ib_post_send(ch->qp, &wr.wr, &bad_wr);
+ 	if (unlikely(err)) {
+ 		WARN_ON_ONCE(err == -ENOMEM);
+ 		return err;
+ 	}
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
 +
 +static int srp_finish_mapping(struct srp_map_state *state,
 +			      struct srp_rdma_ch *ch)
 +{
 +	struct srp_target_port *target = ch->target;
 +	struct srp_device *dev = target->srp_host->srp_dev;
 +	int ret = 0;
  
 -	return n;
 +	WARN_ON_ONCE(!dev->use_fast_reg && !dev->use_fmr);
 +
 +	if (state->npages == 0)
 +		return 0;
 +
 +	if (state->npages == 1 && target->global_mr)
 +		srp_map_desc(state, state->base_dma_addr, state->dma_len,
 +			     target->global_mr->rkey);
 +	else
 +		ret = dev->use_fast_reg ? srp_map_finish_fr(state, ch) :
 +			srp_map_finish_fmr(state, ch);
 +
 +	if (ret == 0) {
 +		state->npages = 0;
 +		state->dma_len = 0;
 +	}
 +
 +	return ret;
  }
  
  static int srp_map_sg_entry(struct srp_map_state *state,
@@@ -1421,9 -1423,62 +1494,68 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int srp_map_sg(struct srp_map_state *state, struct srp_rdma_ch *ch,
 +		      struct srp_request *req, struct scatterlist *scat,
 +		      int count)
++=======
+ static int srp_map_sg_fmr(struct srp_map_state *state, struct srp_rdma_ch *ch,
+ 			  struct srp_request *req, struct scatterlist *scat,
+ 			  int count)
+ {
+ 	struct scatterlist *sg;
+ 	int i, ret;
+ 
+ 	state->pages = req->map_page;
+ 	state->fmr.next = req->fmr_list;
+ 	state->fmr.end = req->fmr_list + ch->target->mr_per_cmd;
+ 
+ 	for_each_sg(scat, sg, count, i) {
+ 		ret = srp_map_sg_entry(state, ch, sg, i);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	ret = srp_map_finish_fmr(state, ch);
+ 	if (ret)
+ 		return ret;
+ 
+ 	return 0;
+ }
+ 
+ static int srp_map_sg_fr(struct srp_map_state *state, struct srp_rdma_ch *ch,
+ 			 struct srp_request *req, struct scatterlist *scat,
+ 			 int count)
+ {
+ 	unsigned int sg_offset = 0;
+ 
+ 	state->desc = req->indirect_desc;
+ 	state->fr.next = req->fr_list;
+ 	state->fr.end = req->fr_list + ch->target->mr_per_cmd;
+ 	state->sg = scat;
+ 
+ 	if (count == 0)
+ 		return 0;
+ 
+ 	while (count) {
+ 		int i, n;
+ 
+ 		n = srp_map_finish_fr(state, req, ch, count, &sg_offset);
+ 		if (unlikely(n < 0))
+ 			return n;
+ 
+ 		count -= n;
+ 		for (i = 0; i < n; i++)
+ 			state->sg = sg_next(state->sg);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int srp_map_sg_dma(struct srp_map_state *state, struct srp_rdma_ch *ch,
+ 			  struct srp_request *req, struct scatterlist *scat,
+ 			  int count)
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
  {
  	struct srp_target_port *target = ch->target;
  	struct srp_device *dev = target->srp_host->srp_dev;
@@@ -1487,22 -1519,63 +1619,74 @@@ static int srp_map_idb(struct srp_rdma_
  	state.gen.next = next_mr;
  	state.gen.end = end_mr;
  	state.desc = &idb_desc;
 +	state.pages = idb_pages;
 +	state.pages[0] = (req->indirect_dma_addr &
 +			  dev->mr_page_mask);
 +	state.npages = 1;
  	state.base_dma_addr = req->indirect_dma_addr;
  	state.dma_len = idb_len;
++<<<<<<< HEAD
 +	ret = srp_finish_mapping(&state, ch);
 +	if (ret < 0)
 +		goto out;
++=======
+ 
+ 	if (dev->use_fast_reg) {
+ 		state.sg = idb_sg;
+ 		sg_set_buf(idb_sg, req->indirect_desc, idb_len);
+ 		idb_sg->dma_address = req->indirect_dma_addr; /* hack! */
+ #ifdef CONFIG_NEED_SG_DMA_LENGTH
+ 		idb_sg->dma_length = idb_sg->length;	      /* hack^2 */
+ #endif
+ 		ret = srp_map_finish_fr(&state, req, ch, 1, NULL);
+ 		if (ret < 0)
+ 			return ret;
+ 		WARN_ON_ONCE(ret < 1);
+ 	} else if (dev->use_fmr) {
+ 		state.pages = idb_pages;
+ 		state.pages[0] = (req->indirect_dma_addr &
+ 				  dev->mr_page_mask);
+ 		state.npages = 1;
+ 		ret = srp_map_finish_fmr(&state, ch);
+ 		if (ret < 0)
+ 			return ret;
+ 	} else {
+ 		return -EINVAL;
+ 	}
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
  
  	*idb_rkey = idb_desc.key;
  
 -	return 0;
 +out:
 +	return ret;
  }
  
+ #if defined(DYNAMIC_DATA_DEBUG)
+ static void srp_check_mapping(struct srp_map_state *state,
+ 			      struct srp_rdma_ch *ch, struct srp_request *req,
+ 			      struct scatterlist *scat, int count)
+ {
+ 	struct srp_device *dev = ch->target->srp_host->srp_dev;
+ 	struct srp_fr_desc **pfr;
+ 	u64 desc_len = 0, mr_len = 0;
+ 	int i;
+ 
+ 	for (i = 0; i < state->ndesc; i++)
+ 		desc_len += be32_to_cpu(req->indirect_desc[i].len);
+ 	if (dev->use_fast_reg)
+ 		for (i = 0, pfr = req->fr_list; i < state->nmdesc; i++, pfr++)
+ 			mr_len += (*pfr)->mr->length;
+ 	else if (dev->use_fmr)
+ 		for (i = 0; i < state->nmdesc; i++)
+ 			mr_len += be32_to_cpu(req->indirect_desc[i].len);
+ 	if (desc_len != scsi_bufflen(req->scmnd) ||
+ 	    mr_len > scsi_bufflen(req->scmnd))
+ 		pr_err("Inconsistent: scsi len %d <> desc len %lld <> mr len %lld; ndesc %d; nmdesc = %d\n",
+ 		       scsi_bufflen(req->scmnd), desc_len, mr_len,
+ 		       state->ndesc, state->nmdesc);
+ }
+ #endif
+ 
  /**
   * srp_map_data() - map SCSI data buffer onto an SRP request
   * @scmnd: SCSI command to map
@@@ -1578,8 -1651,25 +1762,17 @@@ static int srp_map_data(struct scsi_cmn
  				   target->indirect_size, DMA_TO_DEVICE);
  
  	memset(&state, 0, sizeof(state));
 -	if (dev->use_fast_reg)
 -		ret = srp_map_sg_fr(&state, ch, req, scat, count);
 -	else if (dev->use_fmr)
 -		ret = srp_map_sg_fmr(&state, ch, req, scat, count);
 -	else
 -		ret = srp_map_sg_dma(&state, ch, req, scat, count);
 -	req->nmdesc = state.nmdesc;
 -	if (ret < 0)
 -		goto unmap;
 +	srp_map_sg(&state, ch, req, scat, count);
  
+ #if defined(DYNAMIC_DEBUG)
+ 	{
+ 		DEFINE_DYNAMIC_DEBUG_METADATA(ddm,
+ 			"Memory mapping consistency check");
+ 		if (unlikely(ddm.flags & _DPRINTK_FLAGS_PRINT))
+ 			srp_check_mapping(&state, ch, req, scat, count);
+ 	}
+ #endif
+ 
  	/* We've mapped the request, now pull as much of the indirect
  	 * descriptor table as we can into the command buffer. If this
  	 * target is not using an external indirect table, we are
@@@ -3449,42 -3522,38 +3683,61 @@@ static void srp_add_one(struct ib_devic
  	int mr_page_shift, p;
  	u64 max_pages_per_mr;
  
 -	srp_dev = kmalloc(sizeof *srp_dev, GFP_KERNEL);
 -	if (!srp_dev)
 +	dev_attr = kmalloc(sizeof *dev_attr, GFP_KERNEL);
 +	if (!dev_attr)
  		return;
  
++<<<<<<< HEAD
 +	if (ib_query_device(device, dev_attr)) {
 +		pr_warn("Query device failed for %s\n", device->name);
 +		goto free_attr;
 +	}
 +
 +	srp_dev = kmalloc(sizeof *srp_dev, GFP_KERNEL);
 +	if (!srp_dev)
 +		goto free_attr;
++=======
+ 	/*
+ 	 * Use the smallest page size supported by the HCA, down to a
+ 	 * minimum of 4096 bytes. We're unlikely to build large sglists
+ 	 * out of smaller entries.
+ 	 */
+ 	mr_page_shift		= max(12, ffs(device->attrs.page_size_cap) - 1);
+ 	srp_dev->mr_page_size	= 1 << mr_page_shift;
+ 	srp_dev->mr_page_mask	= ~((u64) srp_dev->mr_page_size - 1);
+ 	max_pages_per_mr	= device->attrs.max_mr_size;
+ 	do_div(max_pages_per_mr, srp_dev->mr_page_size);
+ 	pr_debug("%s: %llu / %u = %llu <> %u\n", __func__,
+ 		 device->attrs.max_mr_size, srp_dev->mr_page_size,
+ 		 max_pages_per_mr, SRP_MAX_PAGES_PER_MR);
+ 	srp_dev->max_pages_per_mr = min_t(u64, SRP_MAX_PAGES_PER_MR,
+ 					  max_pages_per_mr);
++>>>>>>> 509c5f33f4f6 (IB/srp: Prevent mapping failures)
  
  	srp_dev->has_fmr = (device->alloc_fmr && device->dealloc_fmr &&
  			    device->map_phys_fmr && device->unmap_fmr);
 -	srp_dev->has_fr = (device->attrs.device_cap_flags &
 +	srp_dev->has_fr = (dev_attr->device_cap_flags &
  			   IB_DEVICE_MEM_MGT_EXTENSIONS);
- 	if (!srp_dev->has_fmr && !srp_dev->has_fr)
+ 	if (!srp_dev->has_fmr && !srp_dev->has_fr) {
  		dev_warn(&device->dev, "neither FMR nor FR is supported\n");
- 
- 	srp_dev->use_fast_reg = (srp_dev->has_fr &&
- 				 (!srp_dev->has_fmr || prefer_fr));
- 	srp_dev->use_fmr = !srp_dev->use_fast_reg && srp_dev->has_fmr;
+ 	} else if (device->attrs.max_mr_size >= 2 * srp_dev->mr_page_size) {
+ 		srp_dev->use_fast_reg = (srp_dev->has_fr &&
+ 					 (!srp_dev->has_fmr || prefer_fr));
+ 		srp_dev->use_fmr = !srp_dev->use_fast_reg && srp_dev->has_fmr;
+ 	}
  
 +	/*
 +	 * Use the smallest page size supported by the HCA, down to a
 +	 * minimum of 4096 bytes. We're unlikely to build large sglists
 +	 * out of smaller entries.
 +	 */
 +	mr_page_shift		= max(12, ffs(dev_attr->page_size_cap) - 1);
 +	srp_dev->mr_page_size	= 1 << mr_page_shift;
 +	srp_dev->mr_page_mask	= ~((u64) srp_dev->mr_page_size - 1);
 +	max_pages_per_mr	= dev_attr->max_mr_size;
 +	do_div(max_pages_per_mr, srp_dev->mr_page_size);
 +	srp_dev->max_pages_per_mr = min_t(u64, SRP_MAX_PAGES_PER_MR,
 +					  max_pages_per_mr);
  	if (srp_dev->use_fast_reg) {
  		srp_dev->max_pages_per_mr =
  			min_t(u32, srp_dev->max_pages_per_mr,
* Unmerged path drivers/infiniband/ulp/srp/ib_srp.c
diff --git a/drivers/infiniband/ulp/srp/ib_srp.h b/drivers/infiniband/ulp/srp/ib_srp.h
index 148e0bf36803..89e175929609 100644
--- a/drivers/infiniband/ulp/srp/ib_srp.h
+++ b/drivers/infiniband/ulp/srp/ib_srp.h
@@ -207,6 +207,7 @@ struct srp_target_port {
 	unsigned int		scsi_id;
 	unsigned int		sg_tablesize;
 	int			mr_pool_size;
+	int			mr_per_cmd;
 	int			queue_size;
 	int			req_ring_size;
 	int			comp_vector;
