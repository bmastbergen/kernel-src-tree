rhashtable: Add annotation to nested lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit 84ed82b74dcb23d96cee2987612a677ffd2b5470
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/84ed82b7.failed

Commit aa34a6cb0478842452bac58edb50d3ef9e178c92 ("rhashtable:
Add arbitrary rehash function") killed the annotation on the
nested lock which leads to bitching from lockdep.

	Reported-by: Fengguang Wu <fengguang.wu@intel.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 84ed82b74dcb23d96cee2987612a677ffd2b5470)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rhashtable.c
diff --cc lib/rhashtable.c
index 6d0c4774001c,d7f3db57b5d0..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -136,54 -200,99 +136,107 @@@ EXPORT_SYMBOL_GPL(rht_grow_above_75)
   * @ht:		hash table
   * @new_size:	new table size
   */
 -static bool rht_shrink_below_30(const struct rhashtable *ht, size_t new_size)
 +bool rht_shrink_below_30(const struct rhashtable *ht, size_t new_size)
  {
  	/* Shrink table beneath 30% load */
 -	return atomic_read(&ht->nelems) < (new_size * 3 / 10) &&
 -	       (atomic_read(&ht->shift) > ht->p.min_shift);
 +	return ht->nelems < (new_size * 3 / 10);
  }
 +EXPORT_SYMBOL_GPL(rht_shrink_below_30);
  
 -static int rhashtable_rehash_one(struct rhashtable *ht, unsigned old_hash)
 +static void hashtable_chain_unzip(const struct rhashtable *ht,
 +				  const struct bucket_table *new_tbl,
 +				  struct bucket_table *old_tbl, size_t n)
  {
 -	struct bucket_table *new_tbl = rht_dereference(ht->future_tbl, ht);
 -	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
 -	struct rhash_head __rcu **pprev = &old_tbl->buckets[old_hash];
 -	int err = -ENOENT;
 -	struct rhash_head *head, *next, *entry;
 -	spinlock_t *new_bucket_lock;
 -	unsigned new_hash;
 +	struct rhash_head *he, *p, *next;
 +	unsigned int h;
  
 -	rht_for_each(entry, old_tbl, old_hash) {
 -		err = 0;
 -		next = rht_dereference_bucket(entry->next, old_tbl, old_hash);
 +	/* Old bucket empty, no work needed. */
 +	p = rht_dereference(old_tbl->buckets[n], ht);
 +	if (!p)
 +		return;
  
 -		if (rht_is_a_nulls(next))
 +	/* Advance the old bucket pointer one or more times until it
 +	 * reaches a node that doesn't hash to the same bucket as the
 +	 * previous node p. Call the previous node p;
 +	 */
 +	h = head_hashfn(ht, new_tbl, p);
 +	rht_for_each_continue(he, p->next, old_tbl, n) {
 +		if (head_hashfn(ht, new_tbl, he) != h)
  			break;
 +		p = he;
 +	}
 +	RCU_INIT_POINTER(old_tbl->buckets[n], p->next);
  
 -		pprev = &entry->next;
 +	/* Find the subsequent node which does hash to the same
 +	 * bucket as node P, or NULL if no such node exists.
 +	 */
 +	next = NULL;
 +	if (he) {
 +		rht_for_each_continue(he, he->next, old_tbl, n) {
 +			if (head_hashfn(ht, new_tbl, he) == h) {
 +				next = he;
 +				break;
 +			}
 +		}
  	}
  
++<<<<<<< HEAD
 +	/* Set p's next pointer to that subsequent node pointer,
 +	 * bypassing the nodes which do not hash to p's bucket
++=======
+ 	if (err)
+ 		goto out;
+ 
+ 	new_hash = head_hashfn(ht, new_tbl, entry);
+ 
+ 	new_bucket_lock = bucket_lock(new_tbl, new_hash);
+ 
+ 	spin_lock_nested(new_bucket_lock, RHT_LOCK_NESTED);
+ 	head = rht_dereference_bucket(new_tbl->buckets[new_hash],
+ 				      new_tbl, new_hash);
+ 
+ 	if (rht_is_a_nulls(head))
+ 		INIT_RHT_NULLS_HEAD(entry->next, ht, new_hash);
+ 	else
+ 		RCU_INIT_POINTER(entry->next, head);
+ 
+ 	rcu_assign_pointer(new_tbl->buckets[new_hash], entry);
+ 	spin_unlock(new_bucket_lock);
+ 
+ 	rcu_assign_pointer(*pprev, next);
+ 
+ out:
+ 	return err;
+ }
+ 
+ static void rhashtable_rehash_chain(struct rhashtable *ht, unsigned old_hash)
+ {
+ 	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
+ 	spinlock_t *old_bucket_lock;
+ 
+ 	old_bucket_lock = bucket_lock(old_tbl, old_hash);
+ 
+ 	spin_lock_bh(old_bucket_lock);
+ 	while (!rhashtable_rehash_one(ht, old_hash))
+ 		;
+ 	spin_unlock_bh(old_bucket_lock);
+ }
+ 
+ static void rhashtable_rehash(struct rhashtable *ht,
+ 			      struct bucket_table *new_tbl)
+ {
+ 	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
+ 	unsigned old_hash;
+ 
+ 	get_random_bytes(&new_tbl->hash_rnd, sizeof(new_tbl->hash_rnd));
+ 
+ 	/* Make insertions go into the new, empty table right away. Deletions
+ 	 * and lookups will be attempted in both tables until we synchronize.
+ 	 * The synchronize_rcu() guarantees for the new table to be picked up
+ 	 * so no new additions go into the old table while we relink.
++>>>>>>> 84ed82b74dcb (rhashtable: Add annotation to nested lock)
  	 */
 -	rcu_assign_pointer(ht->future_tbl, new_tbl);
 -
 -	for (old_hash = 0; old_hash < old_tbl->size; old_hash++)
 -		rhashtable_rehash_chain(ht, old_hash);
 -
 -	/* Publish the new table pointer. */
 -	rcu_assign_pointer(ht->tbl, new_tbl);
 -
 -	/* Wait for readers. All new readers will see the new
 -	 * table, and thus no references to the old table will
 -	 * remain.
 -	 */
 -	synchronize_rcu();
 -
 -	bucket_table_free(old_tbl);
 +	RCU_INIT_POINTER(p->next, next);
  }
  
  /**
@@@ -319,8 -366,97 +372,100 @@@ int rhashtable_shrink(struct rhashtabl
  }
  EXPORT_SYMBOL_GPL(rhashtable_shrink);
  
++<<<<<<< HEAD
++=======
+ static void rht_deferred_worker(struct work_struct *work)
+ {
+ 	struct rhashtable *ht;
+ 	struct bucket_table *tbl;
+ 	struct rhashtable_walker *walker;
+ 
+ 	ht = container_of(work, struct rhashtable, run_work);
+ 	mutex_lock(&ht->mutex);
+ 	if (ht->being_destroyed)
+ 		goto unlock;
+ 
+ 	tbl = rht_dereference(ht->tbl, ht);
+ 
+ 	list_for_each_entry(walker, &ht->walkers, list)
+ 		walker->resize = true;
+ 
+ 	if (rht_grow_above_75(ht, tbl->size))
+ 		rhashtable_expand(ht);
+ 	else if (rht_shrink_below_30(ht, tbl->size))
+ 		rhashtable_shrink(ht);
+ unlock:
+ 	mutex_unlock(&ht->mutex);
+ }
+ 
+ static bool __rhashtable_insert(struct rhashtable *ht, struct rhash_head *obj,
+ 				bool (*compare)(void *, void *), void *arg)
+ {
+ 	struct bucket_table *tbl, *old_tbl;
+ 	struct rhash_head *head;
+ 	bool no_resize_running;
+ 	unsigned hash;
+ 	bool success = true;
+ 
+ 	rcu_read_lock();
+ 
+ 	old_tbl = rht_dereference_rcu(ht->tbl, ht);
+ 	hash = obj_raw_hashfn(ht, old_tbl, rht_obj(ht, obj));
+ 
+ 	spin_lock_bh(bucket_lock(old_tbl, hash));
+ 
+ 	/* Because we have already taken the bucket lock in old_tbl,
+ 	 * if we find that future_tbl is not yet visible then that
+ 	 * guarantees all other insertions of the same entry will
+ 	 * also grab the bucket lock in old_tbl because until the
+ 	 * rehash completes ht->tbl won't be changed.
+ 	 */
+ 	tbl = rht_dereference_rcu(ht->future_tbl, ht);
+ 	if (tbl != old_tbl) {
+ 		hash = obj_raw_hashfn(ht, tbl, rht_obj(ht, obj));
+ 		spin_lock_nested(bucket_lock(tbl, hash), RHT_LOCK_NESTED);
+ 	}
+ 
+ 	if (compare &&
+ 	    rhashtable_lookup_compare(ht, rht_obj(ht, obj) + ht->p.key_offset,
+ 				      compare, arg)) {
+ 		success = false;
+ 		goto exit;
+ 	}
+ 
+ 	no_resize_running = tbl == old_tbl;
+ 
+ 	hash = rht_bucket_index(tbl, hash);
+ 	head = rht_dereference_bucket(tbl->buckets[hash], tbl, hash);
+ 
+ 	if (rht_is_a_nulls(head))
+ 		INIT_RHT_NULLS_HEAD(obj->next, ht, hash);
+ 	else
+ 		RCU_INIT_POINTER(obj->next, head);
+ 
+ 	rcu_assign_pointer(tbl->buckets[hash], obj);
+ 
+ 	atomic_inc(&ht->nelems);
+ 	if (no_resize_running && rht_grow_above_75(ht, tbl->size))
+ 		schedule_work(&ht->run_work);
+ 
+ exit:
+ 	if (tbl != old_tbl) {
+ 		hash = obj_raw_hashfn(ht, tbl, rht_obj(ht, obj));
+ 		spin_unlock(bucket_lock(tbl, hash));
+ 	}
+ 
+ 	hash = obj_raw_hashfn(ht, old_tbl, rht_obj(ht, obj));
+ 	spin_unlock_bh(bucket_lock(old_tbl, hash));
+ 
+ 	rcu_read_unlock();
+ 
+ 	return success;
+ }
+ 
++>>>>>>> 84ed82b74dcb (rhashtable: Add annotation to nested lock)
  /**
 - * rhashtable_insert - insert object into hash table
 + * rhashtable_insert - insert object into hash hash table
   * @ht:		hash table
   * @obj:	pointer to hash head inside object
   *
* Unmerged path lib/rhashtable.c
