mm: introduce do_shared_fault() and drop do_fault()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] introduce do_shared_fault() and drop do_fault() (Eric Sandeen) [1274459]
Rebuild_FUZZ: 95.92%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit f0c6d4d295e4ea9a47375304420baa38ca279542
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f0c6d4d2.failed

Introduce do_shared_fault().  The function does what do_fault() does for
write faults to shared mappings

Unlike do_fault(), do_shared_fault() is relatively clean and
straight-forward.

Old do_fault() is not needed anymore.  Let it die.

[lliubbo@gmail.com: fix NULL pointer dereference]
	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Bob Liu <bob.liu@oracle.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f0c6d4d295e4ea9a47375304420baa38ca279542)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,d4320e42989d..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2953,7 -2748,7 +2953,11 @@@ reuse
  		 * bit after it clear all dirty ptes, but before a racing
  		 * do_wp_page installs a dirty pte.
  		 *
++<<<<<<< HEAD
 +		 * __do_fault is protected similarly.
++=======
+ 		 * do_shared_fault is protected similarly.
++>>>>>>> f0c6d4d295e4 (mm: introduce do_shared_fault() and drop do_fault())
  		 */
  		if (!page_mkwrite) {
  			wait_on_page_locked(dirty_page);
@@@ -3508,202 -3286,210 +3512,393 @@@ oom
  	return VM_FAULT_OOM;
  }
  
++<<<<<<< HEAD
 +/*
 + * __do_fault() tries to create a new page mapping. It aggressively
 + * tries to share with existing pages, but makes a separate copy if
 + * the FAULT_FLAG_WRITE is set in the flags parameter in order to avoid
 + * the next page fault.
 + *
 + * As this is called only for pages that do not currently exist, we
 + * do not need to flush old virtual caches or the TLB.
 + *
 + * We enter with non-exclusive mmap_sem (to exclude vma changes,
 + * but allow concurrent faults), and pte neither mapped nor locked.
 + * We return with mmap_sem still held, but pte unmapped and unlocked.
 + */
 +static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
++=======
+ static int __do_fault(struct vm_area_struct *vma, unsigned long address,
+ 		pgoff_t pgoff, unsigned int flags, struct page **page)
+ {
+ 	struct vm_fault vmf;
+ 	int ret;
+ 
+ 	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
+ 	vmf.pgoff = pgoff;
+ 	vmf.flags = flags;
+ 	vmf.page = NULL;
+ 
+ 	ret = vma->vm_ops->fault(vma, &vmf);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
+ 
+ 	if (unlikely(PageHWPoison(vmf.page))) {
+ 		if (ret & VM_FAULT_LOCKED)
+ 			unlock_page(vmf.page);
+ 		page_cache_release(vmf.page);
+ 		return VM_FAULT_HWPOISON;
+ 	}
+ 
+ 	if (unlikely(!(ret & VM_FAULT_LOCKED)))
+ 		lock_page(vmf.page);
+ 	else
+ 		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
+ 
+ 	*page = vmf.page;
+ 	return ret;
+ }
+ 
+ static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pmd_t *pmd,
+ 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
+ {
+ 	struct page *fault_page;
+ 	spinlock_t *ptl;
+ 	pte_t entry, *pte;
+ 	int ret;
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
+ 
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		return ret;
+ 	}
+ 
+ 	flush_icache_page(vma, fault_page);
+ 	entry = mk_pte(fault_page, vma->vm_page_prot);
+ 	if (pte_file(orig_pte) && pte_file_soft_dirty(orig_pte))
+ 		pte_mksoft_dirty(entry);
+ 	inc_mm_counter_fast(mm, MM_FILEPAGES);
+ 	page_add_file_rmap(fault_page);
+ 	set_pte_at(mm, address, pte, entry);
+ 
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 	pte_unmap_unlock(pte, ptl);
+ 	unlock_page(fault_page);
+ 
+ 	return ret;
+ }
+ 
+ static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pmd_t *pmd,
+ 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
+ {
+ 	struct page *fault_page, *new_page;
+ 	spinlock_t *ptl;
+ 	pte_t entry, *pte;
+ 	int ret;
+ 
+ 	if (unlikely(anon_vma_prepare(vma)))
+ 		return VM_FAULT_OOM;
+ 
+ 	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
+ 	if (!new_page)
+ 		return VM_FAULT_OOM;
+ 
+ 	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL)) {
+ 		page_cache_release(new_page);
+ 		return VM_FAULT_OOM;
+ 	}
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		goto uncharge_out;
+ 
+ 	copy_user_highpage(new_page, fault_page, address, vma);
+ 	__SetPageUptodate(new_page);
+ 
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		goto uncharge_out;
+ 	}
+ 
+ 	flush_icache_page(vma, new_page);
+ 	entry = mk_pte(new_page, vma->vm_page_prot);
+ 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	inc_mm_counter_fast(mm, MM_ANONPAGES);
+ 	page_add_new_anon_rmap(new_page, vma, address);
+ 	set_pte_at(mm, address, pte, entry);
+ 
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 
+ 	pte_unmap_unlock(pte, ptl);
+ 	unlock_page(fault_page);
+ 	page_cache_release(fault_page);
+ 	return ret;
+ uncharge_out:
+ 	mem_cgroup_uncharge_page(new_page);
+ 	page_cache_release(new_page);
+ 	return ret;
+ }
+ 
+ static int do_shared_fault(struct mm_struct *mm, struct vm_area_struct *vma,
++>>>>>>> f0c6d4d295e4 (mm: introduce do_shared_fault() and drop do_fault())
  		unsigned long address, pmd_t *pmd,
  		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
  {
- 	pte_t *page_table;
+ 	struct page *fault_page;
+ 	struct address_space *mapping;
  	spinlock_t *ptl;
++<<<<<<< HEAD
 +	struct page *page;
 +	struct page *cow_page;
 +	pte_t entry;
 +	int anon = 0;
 +	struct page *dirty_page = NULL;
 +	struct vm_fault vmf;
 +	int ret;
 +	int page_mkwrite = 0;
 +
 +	/*
 +	 * If we do COW later, allocate page befor taking lock_page()
 +	 * on the file cache page. This will reduce lock holding time.
 +	 */
 +	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
 +
 +		if (unlikely(anon_vma_prepare(vma)))
 +			return VM_FAULT_OOM;
 +
 +		cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
 +		if (!cow_page)
 +			return VM_FAULT_OOM;
 +
 +		if (mem_cgroup_newpage_charge(cow_page, mm, GFP_KERNEL)) {
 +			page_cache_release(cow_page);
 +			return VM_FAULT_OOM;
 +		}
 +	} else
 +		cow_page = NULL;
 +
 +	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
 +	vmf.pgoff = pgoff;
 +	vmf.flags = flags;
 +	vmf.page = NULL;
 +
 +	ret = vma->vm_ops->fault(vma, &vmf);
 +	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
 +			    VM_FAULT_RETRY)))
 +		goto uncharge_out;
++=======
+ 	pte_t entry, *pte;
+ 	int dirtied = 0;
+ 	struct vm_fault vmf;
+ 	int ret, tmp;
+ 
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
++>>>>>>> f0c6d4d295e4 (mm: introduce do_shared_fault() and drop do_fault())
 +
 +	if (unlikely(PageHWPoison(vmf.page))) {
 +		if (ret & VM_FAULT_LOCKED)
 +			unlock_page(vmf.page);
 +		ret = VM_FAULT_HWPOISON;
 +		goto uncharge_out;
 +	}
 +
 +	/*
 +	 * For consistency in subsequent calls, make the faulted page always
 +	 * locked.
 +	 */
 +	if (unlikely(!(ret & VM_FAULT_LOCKED)))
 +		lock_page(vmf.page);
 +	else
 +		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
  
  	/*
- 	 * Should we do an early C-O-W break?
+ 	 * Check if the backing address space wants to know that the page is
+ 	 * about to become writable
  	 */
++<<<<<<< HEAD
 +	page = vmf.page;
 +	if (flags & FAULT_FLAG_WRITE) {
 +		if (!(vma->vm_flags & VM_SHARED)) {
 +			page = cow_page;
 +			anon = 1;
 +			copy_user_highpage(page, vmf.page, address, vma);
 +			__SetPageUptodate(page);
 +		} else {
 +			/*
 +			 * If the page will be shareable, see if the backing
 +			 * address space wants to know that the page is about
 +			 * to become writable
 +			 */
 +			if (vma->vm_ops->page_mkwrite) {
 +				int tmp;
 +
 +				unlock_page(page);
 +				vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
 +				tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
 +				if (unlikely(tmp &
 +					  (VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
 +					ret = tmp;
 +					goto unwritable_page;
 +				}
 +				if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
 +					lock_page(page);
 +					if (!page->mapping) {
 +						ret = 0; /* retry the fault */
 +						unlock_page(page);
 +						goto unwritable_page;
 +					}
 +				} else
 +					VM_BUG_ON_PAGE(!PageLocked(page), page);
 +				page_mkwrite = 1;
 +			}
 +		}
++=======
+ 	if (!vma->vm_ops->page_mkwrite)
+ 		goto set_pte;
+ 
+ 	unlock_page(fault_page);
+ 	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
+ 	vmf.pgoff = pgoff;
+ 	vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
+ 	vmf.page = fault_page;
++>>>>>>> f0c6d4d295e4 (mm: introduce do_shared_fault() and drop do_fault())
  
+ 	tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
+ 	if (unlikely(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
+ 		page_cache_release(fault_page);
+ 		return tmp;
  	}
  
++<<<<<<< HEAD
 +	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
 +
 +	/*
 +	 * This silly early PAGE_DIRTY setting removes a race
 +	 * due to the bad i386 page protection. But it's valid
 +	 * for other architectures too.
 +	 *
 +	 * Note that if FAULT_FLAG_WRITE is set, we either now have
 +	 * an exclusive copy of the page, or this is a shared mapping,
 +	 * so we can make it writable and dirty to avoid having to
 +	 * handle that later.
 +	 */
 +	/* Only go through if we didn't race with anybody else... */
 +	if (likely(pte_same(*page_table, orig_pte))) {
 +		flush_icache_page(vma, page);
 +		entry = mk_pte(page, vma->vm_page_prot);
 +		if (flags & FAULT_FLAG_WRITE)
 +			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +		if (anon) {
 +			inc_mm_counter_fast(mm, MM_ANONPAGES);
 +			page_add_new_anon_rmap(page, vma, address);
 +		} else {
 +			inc_mm_counter_fast(mm, MM_FILEPAGES);
 +			page_add_file_rmap(page);
 +			if (flags & FAULT_FLAG_WRITE) {
 +				dirty_page = page;
 +				get_page(dirty_page);
 +			}
 +		}
 +		set_pte_at(mm, address, page_table, entry);
 +
 +		/* no need to invalidate: a not-present page won't be cached */
 +		update_mmu_cache(vma, address, page_table);
 +	} else {
 +		if (cow_page)
 +			mem_cgroup_uncharge_page(cow_page);
 +		if (anon)
 +			page_cache_release(page);
 +		else
 +			anon = 1; /* no anon but release faulted_page */
 +	}
 +
 +	pte_unmap_unlock(page_table, ptl);
 +
 +	if (dirty_page) {
 +		struct address_space *mapping = page->mapping;
 +		int dirtied = 0;
 +
 +		if (set_page_dirty(dirty_page))
 +			dirtied = 1;
 +		unlock_page(dirty_page);
 +		put_page(dirty_page);
 +		if ((dirtied || page_mkwrite) && mapping) {
 +			/*
 +			 * Some device drivers do not set page.mapping but still
 +			 * dirty their pages
 +			 */
 +			balance_dirty_pages_ratelimited(mapping);
 +		}
 +
 +		/* file_update_time outside page_lock */
 +		if (vma->vm_file && !page_mkwrite)
 +			file_update_time(vma->vm_file);
 +	} else {
 +		unlock_page(vmf.page);
 +		if (anon)
 +			page_cache_release(vmf.page);
++=======
+ 	if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
+ 		lock_page(fault_page);
+ 		if (!fault_page->mapping) {
+ 			unlock_page(fault_page);
+ 			page_cache_release(fault_page);
+ 			return 0; /* retry */
+ 		}
+ 	} else
+ 		VM_BUG_ON_PAGE(!PageLocked(fault_page), fault_page);
+ set_pte:
+ 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
+ 	if (unlikely(!pte_same(*pte, orig_pte))) {
+ 		pte_unmap_unlock(pte, ptl);
+ 		unlock_page(fault_page);
+ 		page_cache_release(fault_page);
+ 		return ret;
++>>>>>>> f0c6d4d295e4 (mm: introduce do_shared_fault() and drop do_fault())
  	}
  
- 	return ret;
+ 	flush_icache_page(vma, fault_page);
+ 	entry = mk_pte(fault_page, vma->vm_page_prot);
+ 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	inc_mm_counter_fast(mm, MM_FILEPAGES);
+ 	page_add_file_rmap(fault_page);
+ 	set_pte_at(mm, address, pte, entry);
  
- unwritable_page:
- 	page_cache_release(page);
- 	return ret;
- uncharge_out:
- 	/* fs's fault handler get error */
- 	if (cow_page) {
- 		mem_cgroup_uncharge_page(cow_page);
- 		page_cache_release(cow_page);
+ 	/* no need to invalidate: a not-present page won't be cached */
+ 	update_mmu_cache(vma, address, pte);
+ 	pte_unmap_unlock(pte, ptl);
+ 
+ 	if (set_page_dirty(fault_page))
+ 		dirtied = 1;
+ 	mapping = fault_page->mapping;
+ 	unlock_page(fault_page);
+ 	if ((dirtied || vma->vm_ops->page_mkwrite) && mapping) {
+ 		/*
+ 		 * Some device drivers do not set page.mapping but still
+ 		 * dirty their pages
+ 		 */
+ 		balance_dirty_pages_ratelimited(mapping);
  	}
+ 
+ 	/* file_update_time outside page_lock */
+ 	if (vma->vm_file && !vma->vm_ops->page_mkwrite)
+ 		file_update_time(vma->vm_file);
+ 
  	return ret;
  }
  
@@@ -3715,10 -3501,13 +3910,20 @@@ static int do_linear_fault(struct mm_st
  			- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
  
  	pte_unmap(page_table);
++<<<<<<< HEAD
 +	/* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
 +	if (!vma->vm_ops->fault)
 +		return VM_FAULT_SIGBUS;
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++=======
+ 	if (!(flags & FAULT_FLAG_WRITE))
+ 		return do_read_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	if (!(vma->vm_flags & VM_SHARED))
+ 		return do_cow_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	return do_shared_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++>>>>>>> f0c6d4d295e4 (mm: introduce do_shared_fault() and drop do_fault())
  }
  
  /*
@@@ -3750,10 -3539,16 +3955,20 @@@ static int do_nonlinear_fault(struct mm
  	}
  
  	pgoff = pte_to_pgoff(orig_pte);
++<<<<<<< HEAD
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++=======
+ 	if (!(flags & FAULT_FLAG_WRITE))
+ 		return do_read_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	if (!(vma->vm_flags & VM_SHARED))
+ 		return do_cow_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	return do_shared_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++>>>>>>> f0c6d4d295e4 (mm: introduce do_shared_fault() and drop do_fault())
  }
  
 -static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 +int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
  				unsigned long addr, int page_nid,
  				int *flags)
  {
* Unmerged path mm/memory.c
