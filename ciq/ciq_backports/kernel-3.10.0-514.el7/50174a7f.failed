IB/core: Add interfaces to control VF attributes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Eli Cohen <eli@mellanox.com>
commit 50174a7f2c24d13cdeec435ee1ba70b1e0b1318f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/50174a7f.failed

Following the practice exercised for network devices which allow the PF
net device to configure attributes of its virtual functions, we
introduce the following functions to be used by IPoIB which is the
network driver implementation for IB devices.

ib_set_vf_link_state - set the policy for a VF link. More below.
ib_get_vf_config - read configuration information of a VF
ib_get_vf_stats - read VF statistics
ib_set_vf_guid - set the node or port GUID of a VF

Also add an indication in the device cap flags that indicates that this
IB devices is based on a virtual function.

A VF shares the physical port with the PF and other VFs. When setting
the link state we have three options:

1. Auto - in this mode, the virtual port follows the state of the
   physical port and becomes active only if the physical port's state is
   active. In all other cases it remains in a Down state.
2. Down - sets the state of the virtual port to Down
3. Up - causes the virtual port to transition into Initialize state if
   it was not already in this state. A virtualization aware subnet manager
   can then bring the state of the port into the Active state.

	Signed-off-by: Eli Cohen <eli@mellanox.com>
	Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 50174a7f2c24d13cdeec435ee1ba70b1e0b1318f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/verbs.c
#	include/rdma/ib_verbs.h
diff --cc drivers/infiniband/core/verbs.c
index 6ff33ede8c15,15b8adbf39c0..000000000000
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@@ -1456,3 -1550,316 +1456,319 @@@ int ib_check_mr_status(struct ib_mr *mr
  		mr->device->check_mr_status(mr, check_mask, mr_status) : -ENOSYS;
  }
  EXPORT_SYMBOL(ib_check_mr_status);
++<<<<<<< HEAD
++=======
+ 
+ int ib_set_vf_link_state(struct ib_device *device, int vf, u8 port,
+ 			 int state)
+ {
+ 	if (!device->set_vf_link_state)
+ 		return -ENOSYS;
+ 
+ 	return device->set_vf_link_state(device, vf, port, state);
+ }
+ EXPORT_SYMBOL(ib_set_vf_link_state);
+ 
+ int ib_get_vf_config(struct ib_device *device, int vf, u8 port,
+ 		     struct ifla_vf_info *info)
+ {
+ 	if (!device->get_vf_config)
+ 		return -ENOSYS;
+ 
+ 	return device->get_vf_config(device, vf, port, info);
+ }
+ EXPORT_SYMBOL(ib_get_vf_config);
+ 
+ int ib_get_vf_stats(struct ib_device *device, int vf, u8 port,
+ 		    struct ifla_vf_stats *stats)
+ {
+ 	if (!device->get_vf_stats)
+ 		return -ENOSYS;
+ 
+ 	return device->get_vf_stats(device, vf, port, stats);
+ }
+ EXPORT_SYMBOL(ib_get_vf_stats);
+ 
+ int ib_set_vf_guid(struct ib_device *device, int vf, u8 port, u64 guid,
+ 		   int type)
+ {
+ 	if (!device->set_vf_guid)
+ 		return -ENOSYS;
+ 
+ 	return device->set_vf_guid(device, vf, port, guid, type);
+ }
+ EXPORT_SYMBOL(ib_set_vf_guid);
+ 
+ /**
+  * ib_map_mr_sg() - Map the largest prefix of a dma mapped SG list
+  *     and set it the memory region.
+  * @mr:            memory region
+  * @sg:            dma mapped scatterlist
+  * @sg_nents:      number of entries in sg
+  * @page_size:     page vector desired page size
+  *
+  * Constraints:
+  * - The first sg element is allowed to have an offset.
+  * - Each sg element must be aligned to page_size (or physically
+  *   contiguous to the previous element). In case an sg element has a
+  *   non contiguous offset, the mapping prefix will not include it.
+  * - The last sg element is allowed to have length less than page_size.
+  * - If sg_nents total byte length exceeds the mr max_num_sge * page_size
+  *   then only max_num_sg entries will be mapped.
+  * - If the MR was allocated with type IB_MR_TYPE_SG_GAPS_REG, non of these
+  *   constraints holds and the page_size argument is ignored.
+  *
+  * Returns the number of sg elements that were mapped to the memory region.
+  *
+  * After this completes successfully, the  memory region
+  * is ready for registration.
+  */
+ int ib_map_mr_sg(struct ib_mr *mr,
+ 		 struct scatterlist *sg,
+ 		 int sg_nents,
+ 		 unsigned int page_size)
+ {
+ 	if (unlikely(!mr->device->map_mr_sg))
+ 		return -ENOSYS;
+ 
+ 	mr->page_size = page_size;
+ 
+ 	return mr->device->map_mr_sg(mr, sg, sg_nents);
+ }
+ EXPORT_SYMBOL(ib_map_mr_sg);
+ 
+ /**
+  * ib_sg_to_pages() - Convert the largest prefix of a sg list
+  *     to a page vector
+  * @mr:            memory region
+  * @sgl:           dma mapped scatterlist
+  * @sg_nents:      number of entries in sg
+  * @set_page:      driver page assignment function pointer
+  *
+  * Core service helper for drivers to convert the largest
+  * prefix of given sg list to a page vector. The sg list
+  * prefix converted is the prefix that meet the requirements
+  * of ib_map_mr_sg.
+  *
+  * Returns the number of sg elements that were assigned to
+  * a page vector.
+  */
+ int ib_sg_to_pages(struct ib_mr *mr,
+ 		   struct scatterlist *sgl,
+ 		   int sg_nents,
+ 		   int (*set_page)(struct ib_mr *, u64))
+ {
+ 	struct scatterlist *sg;
+ 	u64 last_end_dma_addr = 0;
+ 	unsigned int last_page_off = 0;
+ 	u64 page_mask = ~((u64)mr->page_size - 1);
+ 	int i, ret;
+ 
+ 	mr->iova = sg_dma_address(&sgl[0]);
+ 	mr->length = 0;
+ 
+ 	for_each_sg(sgl, sg, sg_nents, i) {
+ 		u64 dma_addr = sg_dma_address(sg);
+ 		unsigned int dma_len = sg_dma_len(sg);
+ 		u64 end_dma_addr = dma_addr + dma_len;
+ 		u64 page_addr = dma_addr & page_mask;
+ 
+ 		/*
+ 		 * For the second and later elements, check whether either the
+ 		 * end of element i-1 or the start of element i is not aligned
+ 		 * on a page boundary.
+ 		 */
+ 		if (i && (last_page_off != 0 || page_addr != dma_addr)) {
+ 			/* Stop mapping if there is a gap. */
+ 			if (last_end_dma_addr != dma_addr)
+ 				break;
+ 
+ 			/*
+ 			 * Coalesce this element with the last. If it is small
+ 			 * enough just update mr->length. Otherwise start
+ 			 * mapping from the next page.
+ 			 */
+ 			goto next_page;
+ 		}
+ 
+ 		do {
+ 			ret = set_page(mr, page_addr);
+ 			if (unlikely(ret < 0))
+ 				return i ? : ret;
+ next_page:
+ 			page_addr += mr->page_size;
+ 		} while (page_addr < end_dma_addr);
+ 
+ 		mr->length += dma_len;
+ 		last_end_dma_addr = end_dma_addr;
+ 		last_page_off = end_dma_addr & ~page_mask;
+ 	}
+ 
+ 	return i;
+ }
+ EXPORT_SYMBOL(ib_sg_to_pages);
+ 
+ struct ib_drain_cqe {
+ 	struct ib_cqe cqe;
+ 	struct completion done;
+ };
+ 
+ static void ib_drain_qp_done(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	struct ib_drain_cqe *cqe = container_of(wc->wr_cqe, struct ib_drain_cqe,
+ 						cqe);
+ 
+ 	complete(&cqe->done);
+ }
+ 
+ /*
+  * Post a WR and block until its completion is reaped for the SQ.
+  */
+ static void __ib_drain_sq(struct ib_qp *qp)
+ {
+ 	struct ib_qp_attr attr = { .qp_state = IB_QPS_ERR };
+ 	struct ib_drain_cqe sdrain;
+ 	struct ib_send_wr swr = {}, *bad_swr;
+ 	int ret;
+ 
+ 	if (qp->send_cq->poll_ctx == IB_POLL_DIRECT) {
+ 		WARN_ONCE(qp->send_cq->poll_ctx == IB_POLL_DIRECT,
+ 			  "IB_POLL_DIRECT poll_ctx not supported for drain\n");
+ 		return;
+ 	}
+ 
+ 	swr.wr_cqe = &sdrain.cqe;
+ 	sdrain.cqe.done = ib_drain_qp_done;
+ 	init_completion(&sdrain.done);
+ 
+ 	ret = ib_modify_qp(qp, &attr, IB_QP_STATE);
+ 	if (ret) {
+ 		WARN_ONCE(ret, "failed to drain send queue: %d\n", ret);
+ 		return;
+ 	}
+ 
+ 	ret = ib_post_send(qp, &swr, &bad_swr);
+ 	if (ret) {
+ 		WARN_ONCE(ret, "failed to drain send queue: %d\n", ret);
+ 		return;
+ 	}
+ 
+ 	wait_for_completion(&sdrain.done);
+ }
+ 
+ /*
+  * Post a WR and block until its completion is reaped for the RQ.
+  */
+ static void __ib_drain_rq(struct ib_qp *qp)
+ {
+ 	struct ib_qp_attr attr = { .qp_state = IB_QPS_ERR };
+ 	struct ib_drain_cqe rdrain;
+ 	struct ib_recv_wr rwr = {}, *bad_rwr;
+ 	int ret;
+ 
+ 	if (qp->recv_cq->poll_ctx == IB_POLL_DIRECT) {
+ 		WARN_ONCE(qp->recv_cq->poll_ctx == IB_POLL_DIRECT,
+ 			  "IB_POLL_DIRECT poll_ctx not supported for drain\n");
+ 		return;
+ 	}
+ 
+ 	rwr.wr_cqe = &rdrain.cqe;
+ 	rdrain.cqe.done = ib_drain_qp_done;
+ 	init_completion(&rdrain.done);
+ 
+ 	ret = ib_modify_qp(qp, &attr, IB_QP_STATE);
+ 	if (ret) {
+ 		WARN_ONCE(ret, "failed to drain recv queue: %d\n", ret);
+ 		return;
+ 	}
+ 
+ 	ret = ib_post_recv(qp, &rwr, &bad_rwr);
+ 	if (ret) {
+ 		WARN_ONCE(ret, "failed to drain recv queue: %d\n", ret);
+ 		return;
+ 	}
+ 
+ 	wait_for_completion(&rdrain.done);
+ }
+ 
+ /**
+  * ib_drain_sq() - Block until all SQ CQEs have been consumed by the
+  *		   application.
+  * @qp:            queue pair to drain
+  *
+  * If the device has a provider-specific drain function, then
+  * call that.  Otherwise call the generic drain function
+  * __ib_drain_sq().
+  *
+  * The caller must:
+  *
+  * ensure there is room in the CQ and SQ for the drain work request and
+  * completion.
+  *
+  * allocate the CQ using ib_alloc_cq() and the CQ poll context cannot be
+  * IB_POLL_DIRECT.
+  *
+  * ensure that there are no other contexts that are posting WRs concurrently.
+  * Otherwise the drain is not guaranteed.
+  */
+ void ib_drain_sq(struct ib_qp *qp)
+ {
+ 	if (qp->device->drain_sq)
+ 		qp->device->drain_sq(qp);
+ 	else
+ 		__ib_drain_sq(qp);
+ }
+ EXPORT_SYMBOL(ib_drain_sq);
+ 
+ /**
+  * ib_drain_rq() - Block until all RQ CQEs have been consumed by the
+  *		   application.
+  * @qp:            queue pair to drain
+  *
+  * If the device has a provider-specific drain function, then
+  * call that.  Otherwise call the generic drain function
+  * __ib_drain_rq().
+  *
+  * The caller must:
+  *
+  * ensure there is room in the CQ and RQ for the drain work request and
+  * completion.
+  *
+  * allocate the CQ using ib_alloc_cq() and the CQ poll context cannot be
+  * IB_POLL_DIRECT.
+  *
+  * ensure that there are no other contexts that are posting WRs concurrently.
+  * Otherwise the drain is not guaranteed.
+  */
+ void ib_drain_rq(struct ib_qp *qp)
+ {
+ 	if (qp->device->drain_rq)
+ 		qp->device->drain_rq(qp);
+ 	else
+ 		__ib_drain_rq(qp);
+ }
+ EXPORT_SYMBOL(ib_drain_rq);
+ 
+ /**
+  * ib_drain_qp() - Block until all CQEs have been consumed by the
+  *		   application on both the RQ and SQ.
+  * @qp:            queue pair to drain
+  *
+  * The caller must:
+  *
+  * ensure there is room in the CQ(s), SQ, and RQ for drain work requests
+  * and completions.
+  *
+  * allocate the CQs using ib_alloc_cq() and the CQ poll context cannot be
+  * IB_POLL_DIRECT.
+  *
+  * ensure that there are no other contexts that are posting WRs concurrently.
+  * Otherwise the drain is not guaranteed.
+  */
+ void ib_drain_qp(struct ib_qp *qp)
+ {
+ 	ib_drain_sq(qp);
+ 	ib_drain_rq(qp);
+ }
+ EXPORT_SYMBOL(ib_drain_qp);
++>>>>>>> 50174a7f2c24 (IB/core: Add interfaces to control VF attributes)
diff --cc include/rdma/ib_verbs.h
index 1795c6208763,8a245a7f981a..000000000000
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@@ -51,7 -51,12 +51,8 @@@
  #include <linux/socket.h>
  #include <linux/irq_poll.h>
  #include <uapi/linux/if_ether.h>
 -#include <net/ipv6.h>
 -#include <net/ip.h>
 -#include <linux/string.h>
 -#include <linux/slab.h>
  
+ #include <linux/if_link.h>
  #include <linux/atomic.h>
  #include <linux/mmu_notifier.h>
  #include <asm/uaccess.h>
@@@ -174,6 -218,8 +175,11 @@@ enum ib_device_cap_flags 
  	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
  	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
  	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
++<<<<<<< HEAD
++=======
+ 	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
+ 	IB_DEVICE_VIRTUAL_FUNCTION		= ((u64)1 << 33),
++>>>>>>> 50174a7f2c24 (IB/core: Add interfaces to control VF attributes)
  };
  
  enum ib_signature_prot_cap {
@@@ -1824,6 -1867,16 +1830,19 @@@ struct ib_device 
  	int			   (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
  						      struct ib_mr_status *mr_status);
  	void			   (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
++<<<<<<< HEAD
++=======
+ 	void			   (*drain_rq)(struct ib_qp *qp);
+ 	void			   (*drain_sq)(struct ib_qp *qp);
+ 	int			   (*set_vf_link_state)(struct ib_device *device, int vf, u8 port,
+ 							int state);
+ 	int			   (*get_vf_config)(struct ib_device *device, int vf, u8 port,
+ 						   struct ifla_vf_info *ivf);
+ 	int			   (*get_vf_stats)(struct ib_device *device, int vf, u8 port,
+ 						   struct ifla_vf_stats *stats);
+ 	int			   (*set_vf_guid)(struct ib_device *device, int vf, u8 port, u64 guid,
+ 						  int type);
++>>>>>>> 50174a7f2c24 (IB/core: Add interfaces to control VF attributes)
  
  	struct ib_dma_mapping_ops   *dma_ops;
  
@@@ -2228,8 -2317,18 +2247,17 @@@ static inline bool rdma_cap_roce_gid_ta
  }
  
  int ib_query_gid(struct ib_device *device,
 -		 u8 port_num, int index, union ib_gid *gid,
 -		 struct ib_gid_attr *attr);
 +		 u8 port_num, int index, union ib_gid *gid);
  
+ int ib_set_vf_link_state(struct ib_device *device, int vf, u8 port,
+ 			 int state);
+ int ib_get_vf_config(struct ib_device *device, int vf, u8 port,
+ 		     struct ifla_vf_info *info);
+ int ib_get_vf_stats(struct ib_device *device, int vf, u8 port,
+ 		    struct ifla_vf_stats *stats);
+ int ib_set_vf_guid(struct ib_device *device, int vf, u8 port, u64 guid,
+ 		   int type);
+ 
  int ib_query_pkey(struct ib_device *device,
  		  u8 port_num, u16 index, u16 *pkey);
  
* Unmerged path drivers/infiniband/core/verbs.c
* Unmerged path include/rdma/ib_verbs.h
