IB/core: Fix user mode post wr corruption

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mike Marciniszyn <mike.marciniszyn@intel.com>
commit 1d784b890c0101db2556b981e864c2282a3c1b02
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/1d784b89.failed

Commit e622f2f4ad21 ("IB: split struct ib_send_wr")
introduced a regression for HCAs whose user mode post
sends go through ib_uverbs_post_send().

The code didn't account for the fact that the first sge is
offset by an operation dependent length.  The allocation did,
but the pointer to the destination sge list is computed without
that knowledge.  The sge list copy_from_user() then corrupts
fields in the work request

Store the operation dependent length in a local variable and
compute the sge list copy_from_user() destination using that length.

	Reviewed-by: Ira Weiny <ira.weiny@intel.com>
	Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 1d784b890c0101db2556b981e864c2282a3c1b02)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/uverbs_cmd.c
diff --cc drivers/infiniband/core/uverbs_cmd.c
index 2304560489e5,4cb8e9d9966c..000000000000
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@@ -2475,11 -2482,76 +2476,84 @@@ ssize_t ib_uverbs_post_send(struct ib_u
  			goto out_put;
  		}
  
++<<<<<<< HEAD
 +		next = kmalloc(ALIGN(sizeof *next, sizeof (struct ib_sge)) +
 +			       user_wr->num_sge * sizeof (struct ib_sge),
 +			       GFP_KERNEL);
 +		if (!next) {
 +			ret = -ENOMEM;
++=======
+ 		if (is_ud) {
+ 			struct ib_ud_wr *ud;
+ 
+ 			if (user_wr->opcode != IB_WR_SEND &&
+ 			    user_wr->opcode != IB_WR_SEND_WITH_IMM) {
+ 				ret = -EINVAL;
+ 				goto out_put;
+ 			}
+ 
+ 			next_size = sizeof(*ud);
+ 			ud = alloc_wr(next_size, user_wr->num_sge);
+ 			if (!ud) {
+ 				ret = -ENOMEM;
+ 				goto out_put;
+ 			}
+ 
+ 			ud->ah = idr_read_ah(user_wr->wr.ud.ah, file->ucontext);
+ 			if (!ud->ah) {
+ 				kfree(ud);
+ 				ret = -EINVAL;
+ 				goto out_put;
+ 			}
+ 			ud->remote_qpn = user_wr->wr.ud.remote_qpn;
+ 			ud->remote_qkey = user_wr->wr.ud.remote_qkey;
+ 
+ 			next = &ud->wr;
+ 		} else if (user_wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM ||
+ 			   user_wr->opcode == IB_WR_RDMA_WRITE ||
+ 			   user_wr->opcode == IB_WR_RDMA_READ) {
+ 			struct ib_rdma_wr *rdma;
+ 
+ 			next_size = sizeof(*rdma);
+ 			rdma = alloc_wr(next_size, user_wr->num_sge);
+ 			if (!rdma) {
+ 				ret = -ENOMEM;
+ 				goto out_put;
+ 			}
+ 
+ 			rdma->remote_addr = user_wr->wr.rdma.remote_addr;
+ 			rdma->rkey = user_wr->wr.rdma.rkey;
+ 
+ 			next = &rdma->wr;
+ 		} else if (user_wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP ||
+ 			   user_wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD) {
+ 			struct ib_atomic_wr *atomic;
+ 
+ 			next_size = sizeof(*atomic);
+ 			atomic = alloc_wr(next_size, user_wr->num_sge);
+ 			if (!atomic) {
+ 				ret = -ENOMEM;
+ 				goto out_put;
+ 			}
+ 
+ 			atomic->remote_addr = user_wr->wr.atomic.remote_addr;
+ 			atomic->compare_add = user_wr->wr.atomic.compare_add;
+ 			atomic->swap = user_wr->wr.atomic.swap;
+ 			atomic->rkey = user_wr->wr.atomic.rkey;
+ 
+ 			next = &atomic->wr;
+ 		} else if (user_wr->opcode == IB_WR_SEND ||
+ 			   user_wr->opcode == IB_WR_SEND_WITH_IMM ||
+ 			   user_wr->opcode == IB_WR_SEND_WITH_INV) {
+ 			next_size = sizeof(*next);
+ 			next = alloc_wr(next_size, user_wr->num_sge);
+ 			if (!next) {
+ 				ret = -ENOMEM;
+ 				goto out_put;
+ 			}
+ 		} else {
+ 			ret = -EINVAL;
++>>>>>>> 1d784b890c01 (IB/core: Fix user mode post wr corruption)
  			goto out_put;
  		}
  
@@@ -2495,63 -2575,9 +2569,63 @@@
  		next->opcode     = user_wr->opcode;
  		next->send_flags = user_wr->send_flags;
  
 +		if (is_ud) {
 +			if (next->opcode != IB_WR_SEND &&
 +			    next->opcode != IB_WR_SEND_WITH_IMM) {
 +				ret = -EINVAL;
 +				goto out_put;
 +			}
 +
 +			next->wr.ud.ah = idr_read_ah(user_wr->wr.ud.ah,
 +						     file->ucontext);
 +			if (!next->wr.ud.ah) {
 +				ret = -EINVAL;
 +				goto out_put;
 +			}
 +			next->wr.ud.remote_qpn  = user_wr->wr.ud.remote_qpn;
 +			next->wr.ud.remote_qkey = user_wr->wr.ud.remote_qkey;
 +			if (next->opcode == IB_WR_SEND_WITH_IMM)
 +				next->ex.imm_data =
 +					(__be32 __force) user_wr->ex.imm_data;
 +		} else {
 +			switch (next->opcode) {
 +			case IB_WR_RDMA_WRITE_WITH_IMM:
 +				next->ex.imm_data =
 +					(__be32 __force) user_wr->ex.imm_data;
 +			case IB_WR_RDMA_WRITE:
 +			case IB_WR_RDMA_READ:
 +				next->wr.rdma.remote_addr =
 +					user_wr->wr.rdma.remote_addr;
 +				next->wr.rdma.rkey        =
 +					user_wr->wr.rdma.rkey;
 +				break;
 +			case IB_WR_SEND_WITH_IMM:
 +				next->ex.imm_data =
 +					(__be32 __force) user_wr->ex.imm_data;
 +				break;
 +			case IB_WR_SEND_WITH_INV:
 +				next->ex.invalidate_rkey =
 +					user_wr->ex.invalidate_rkey;
 +				break;
 +			case IB_WR_ATOMIC_CMP_AND_SWP:
 +			case IB_WR_ATOMIC_FETCH_AND_ADD:
 +				next->wr.atomic.remote_addr =
 +					user_wr->wr.atomic.remote_addr;
 +				next->wr.atomic.compare_add =
 +					user_wr->wr.atomic.compare_add;
 +				next->wr.atomic.swap = user_wr->wr.atomic.swap;
 +				next->wr.atomic.rkey = user_wr->wr.atomic.rkey;
 +			case IB_WR_SEND:
 +				break;
 +			default:
 +				ret = -EINVAL;
 +				goto out_put;
 +			}
 +		}
 +
  		if (next->num_sge) {
  			next->sg_list = (void *) next +
- 				ALIGN(sizeof *next, sizeof (struct ib_sge));
+ 				ALIGN(next_size, sizeof(struct ib_sge));
  			if (copy_from_user(next->sg_list,
  					   buf + sizeof cmd +
  					   cmd.wr_count * cmd.wqe_size +
* Unmerged path drivers/infiniband/core/uverbs_cmd.c
