rhashtable: Do not schedule more than one rehash if we can't grow further

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Thomas Graf <tgraf@suug.ch>
commit a87b9ebf1709687ff213091d0fdb4254b1564803
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a87b9ebf.failed

The current code currently only stops inserting rehashes into the
chain when no resizes are currently scheduled. As long as resizes
are scheduled and while inserting above the utilization watermark,
more and more rehashes will be scheduled.

This lead to a perfect DoS storm with thousands of rehashes
scheduled which lead to thousands of spinlocks to be taken
sequentially.

Instead, only allow either a series of resizes or a single rehash.
Drop any further rehashes and return -EBUSY.

Fixes: ccd57b1bd324 ("rhashtable: Add immediate rehash during insertion")
	Signed-off-by: Thomas Graf <tgraf@suug.ch>
	Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a87b9ebf1709687ff213091d0fdb4254b1564803)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rhashtable.c
diff --cc lib/rhashtable.c
index 6d0c4774001c,b28df4019ade..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -392,79 -389,281 +392,160 @@@ bool rhashtable_remove(struct rhashtabl
  
  	return false;
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(rhashtable_remove);
++=======
+ 
+ int rhashtable_insert_rehash(struct rhashtable *ht)
+ {
+ 	struct bucket_table *old_tbl;
+ 	struct bucket_table *new_tbl;
+ 	struct bucket_table *tbl;
+ 	unsigned int size;
+ 	int err;
+ 
+ 	old_tbl = rht_dereference_rcu(ht->tbl, ht);
+ 	tbl = rhashtable_last_table(ht, old_tbl);
+ 
+ 	size = tbl->size;
+ 
+ 	if (rht_grow_above_75(ht, tbl))
+ 		size *= 2;
+ 	/* Do not schedule more than one rehash */
+ 	else if (old_tbl != tbl)
+ 		return -EBUSY;
+ 
+ 	new_tbl = bucket_table_alloc(ht, size, GFP_ATOMIC);
+ 	if (new_tbl == NULL) {
+ 		/* Schedule async resize/rehash to try allocation
+ 		 * non-atomic context.
+ 		 */
+ 		schedule_work(&ht->run_work);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	err = rhashtable_rehash_attach(ht, tbl, new_tbl);
+ 	if (err) {
+ 		bucket_table_free(new_tbl);
+ 		if (err == -EEXIST)
+ 			err = 0;
+ 	} else
+ 		schedule_work(&ht->run_work);
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_insert_rehash);
+ 
+ int rhashtable_insert_slow(struct rhashtable *ht, const void *key,
+ 			   struct rhash_head *obj,
+ 			   struct bucket_table *tbl)
+ {
+ 	struct rhash_head *head;
+ 	unsigned int hash;
+ 	int err;
+ 
+ 	tbl = rhashtable_last_table(ht, tbl);
+ 	hash = head_hashfn(ht, tbl, obj);
+ 	spin_lock_nested(rht_bucket_lock(tbl, hash), SINGLE_DEPTH_NESTING);
+ 
+ 	err = -EEXIST;
+ 	if (key && rhashtable_lookup_fast(ht, key, ht->p))
+ 		goto exit;
+ 
+ 	err = -EAGAIN;
+ 	if (rhashtable_check_elasticity(ht, tbl, hash) ||
+ 	    rht_grow_above_100(ht, tbl))
+ 		goto exit;
+ 
+ 	err = 0;
+ 
+ 	head = rht_dereference_bucket(tbl->buckets[hash], tbl, hash);
+ 
+ 	RCU_INIT_POINTER(obj->next, head);
+ 
+ 	rcu_assign_pointer(tbl->buckets[hash], obj);
+ 
+ 	atomic_inc(&ht->nelems);
+ 
+ exit:
+ 	spin_unlock(rht_bucket_lock(tbl, hash));
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL_GPL(rhashtable_insert_slow);
++>>>>>>> a87b9ebf1709 (rhashtable: Do not schedule more than one rehash if we can't grow further)
  
  /**
 - * rhashtable_walk_init - Initialise an iterator
 - * @ht:		Table to walk over
 - * @iter:	Hash table Iterator
 - *
 - * This function prepares a hash table walk.
 - *
 - * Note that if you restart a walk after rhashtable_walk_stop you
 - * may see the same object twice.  Also, you may miss objects if
 - * there are removals in between rhashtable_walk_stop and the next
 - * call to rhashtable_walk_start.
 - *
 - * For a completely stable walk you should construct your own data
 - * structure outside the hash table.
 - *
 - * This function may sleep so you must not call it from interrupt
 - * context or with spin locks held.
 - *
 - * You must call rhashtable_walk_exit if this function returns
 - * successfully.
 - */
 -int rhashtable_walk_init(struct rhashtable *ht, struct rhashtable_iter *iter)
 -{
 -	iter->ht = ht;
 -	iter->p = NULL;
 -	iter->slot = 0;
 -	iter->skip = 0;
 -
 -	iter->walker = kmalloc(sizeof(*iter->walker), GFP_KERNEL);
 -	if (!iter->walker)
 -		return -ENOMEM;
 -
 -	mutex_lock(&ht->mutex);
 -	iter->walker->tbl = rht_dereference(ht->tbl, ht);
 -	list_add(&iter->walker->list, &iter->walker->tbl->walkers);
 -	mutex_unlock(&ht->mutex);
 -
 -	return 0;
 -}
 -EXPORT_SYMBOL_GPL(rhashtable_walk_init);
 -
 -/**
 - * rhashtable_walk_exit - Free an iterator
 - * @iter:	Hash table Iterator
 - *
 - * This function frees resources allocated by rhashtable_walk_init.
 - */
 -void rhashtable_walk_exit(struct rhashtable_iter *iter)
 -{
 -	mutex_lock(&iter->ht->mutex);
 -	if (iter->walker->tbl)
 -		list_del(&iter->walker->list);
 -	mutex_unlock(&iter->ht->mutex);
 -	kfree(iter->walker);
 -}
 -EXPORT_SYMBOL_GPL(rhashtable_walk_exit);
 -
 -/**
 - * rhashtable_walk_start - Start a hash table walk
 - * @iter:	Hash table iterator
 + * rhashtable_lookup - lookup key in hash table
 + * @ht:		hash table
 + * @key:	pointer to key
   *
 - * Start a hash table walk.  Note that we take the RCU lock in all
 - * cases including when we return an error.  So you must always call
 - * rhashtable_walk_stop to clean up.
 + * Computes the hash value for the key and traverses the bucket chain looking
 + * for a entry with an identical key. The first matching entry is returned.
   *
 - * Returns zero if successful.
 + * This lookup function may only be used for fixed key hash table (key_len
 + * paramter set). It will BUG() if used inappropriately.
   *
 - * Returns -EAGAIN if resize event occured.  Note that the iterator
 - * will rewind back to the beginning and you may use it immediately
 - * by calling rhashtable_walk_next.
 + * Lookups may occur in parallel with hash mutations as long as the lookup is
 + * guarded by rcu_read_lock(). The caller must take care of this.
   */
 -int rhashtable_walk_start(struct rhashtable_iter *iter)
 -	__acquires(RCU)
 +void *rhashtable_lookup(const struct rhashtable *ht, const void *key)
  {
 -	struct rhashtable *ht = iter->ht;
 -
 -	mutex_lock(&ht->mutex);
 -
 -	if (iter->walker->tbl)
 -		list_del(&iter->walker->list);
 -
 -	rcu_read_lock();
 -
 -	mutex_unlock(&ht->mutex);
 -
 -	if (!iter->walker->tbl) {
 -		iter->walker->tbl = rht_dereference_rcu(ht->tbl, ht);
 -		return -EAGAIN;
 +	const struct bucket_table *tbl = rht_dereference_rcu(ht->tbl, ht);
 +	struct rhash_head *he;
 +	u32 h;
 +
 +	BUG_ON(!ht->p.key_len);
 +
 +	h = key_hashfn(ht, key, ht->p.key_len);
 +	rht_for_each_rcu(he, tbl, h) {
 +		if (memcmp(rht_obj(ht, he) + ht->p.key_offset, key,
 +			   ht->p.key_len))
 +			continue;
 +		return rht_obj(ht, he);
  	}
  
 -	return 0;
 +	return NULL;
  }
 -EXPORT_SYMBOL_GPL(rhashtable_walk_start);
 +EXPORT_SYMBOL_GPL(rhashtable_lookup);
  
  /**
 - * rhashtable_walk_next - Return the next object and advance the iterator
 - * @iter:	Hash table iterator
 + * rhashtable_lookup_compare - search hash table with compare function
 + * @ht:		hash table
 + * @key:	the pointer to the key
 + * @compare:	compare function, must return true on match
 + * @arg:	argument passed on to compare function
   *
 - * Note that you must call rhashtable_walk_stop when you are finished
 - * with the walk.
 + * Traverses the bucket chain behind the provided hash value and calls the
 + * specified compare function for each entry.
   *
 - * Returns the next object or NULL when the end of the table is reached.
 + * Lookups may occur in parallel with hash mutations as long as the lookup is
 + * guarded by rcu_read_lock(). The caller must take care of this.
   *
 - * Returns -EAGAIN if resize event occured.  Note that the iterator
 - * will rewind back to the beginning and you may continue to use it.
 + * Returns the first entry on which the compare function returned true.
   */
 -void *rhashtable_walk_next(struct rhashtable_iter *iter)
 +void *rhashtable_lookup_compare(const struct rhashtable *ht, const void *key,
 +				bool (*compare)(void *, void *), void *arg)
  {
 -	struct bucket_table *tbl = iter->walker->tbl;
 -	struct rhashtable *ht = iter->ht;
 -	struct rhash_head *p = iter->p;
 -	void *obj = NULL;
 -
 -	if (p) {
 -		p = rht_dereference_bucket_rcu(p->next, tbl, iter->slot);
 -		goto next;
 +	const struct bucket_table *tbl = rht_dereference_rcu(ht->tbl, ht);
 +	struct rhash_head *he;
 +	u32 hash;
 +
 +	hash = key_hashfn(ht, key, ht->p.key_len);
 +	rht_for_each_rcu(he, tbl, hash) {
 +		if (!compare(rht_obj(ht, he), arg))
 +			continue;
 +		return rht_obj(ht, he);
  	}
  
 -	for (; iter->slot < tbl->size; iter->slot++) {
 -		int skip = iter->skip;
 -
 -		rht_for_each_rcu(p, tbl, iter->slot) {
 -			if (!skip)
 -				break;
 -			skip--;
 -		}
 -
 -next:
 -		if (!rht_is_a_nulls(p)) {
 -			iter->skip++;
 -			iter->p = p;
 -			obj = rht_obj(ht, p);
 -			goto out;
 -		}
 -
 -		iter->skip = 0;
 -	}
 -
 -	/* Ensure we see any new tables. */
 -	smp_rmb();
 -
 -	iter->walker->tbl = rht_dereference_rcu(tbl->future_tbl, ht);
 -	if (iter->walker->tbl) {
 -		iter->slot = 0;
 -		iter->skip = 0;
 -		return ERR_PTR(-EAGAIN);
 -	}
 -
 -	iter->p = NULL;
 -
 -out:
 -
 -	return obj;
 +	return NULL;
  }
 -EXPORT_SYMBOL_GPL(rhashtable_walk_next);
 +EXPORT_SYMBOL_GPL(rhashtable_lookup_compare);
  
 -/**
 - * rhashtable_walk_stop - Finish a hash table walk
 - * @iter:	Hash table iterator
 - *
 - * Finish a hash table walk.
 - */
 -void rhashtable_walk_stop(struct rhashtable_iter *iter)
 -	__releases(RCU)
 -{
 -	struct rhashtable *ht;
 -	struct bucket_table *tbl = iter->walker->tbl;
 -
 -	if (!tbl)
 -		goto out;
 -
 -	ht = iter->ht;
 -
 -	spin_lock(&ht->lock);
 -	if (tbl->rehash < tbl->size)
 -		list_add(&iter->walker->list, &tbl->walkers);
 -	else
 -		iter->walker->tbl = NULL;
 -	spin_unlock(&ht->lock);
 -
 -	iter->p = NULL;
 -
 -out:
 -	rcu_read_unlock();
 -}
 -EXPORT_SYMBOL_GPL(rhashtable_walk_stop);
 -
 -static size_t rounded_hashtable_size(const struct rhashtable_params *params)
 +static size_t rounded_hashtable_size(struct rhashtable_params *params)
  {
  	return max(roundup_pow_of_two(params->nelem_hint * 4 / 3),
 -		   (unsigned long)params->min_size);
 -}
 -
 -static u32 rhashtable_jhash2(const void *key, u32 length, u32 seed)
 -{
 -	return jhash2(key, length, seed);
 +		   1UL << params->min_shift);
  }
  
  /**
* Unmerged path lib/rhashtable.c
