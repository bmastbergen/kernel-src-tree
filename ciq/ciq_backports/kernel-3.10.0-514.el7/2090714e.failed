gre: build header correctly for collect metadata tunnels

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jiri Benc <jbenc@redhat.com>
commit 2090714e1d6e80979dd6926be22b0de9ca432273
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/2090714e.failed

In ipgre (i.e. not gretap) + collect metadata mode, the skb was assumed to
contain Ethernet header and was encapsulated as ETH_P_TEB. This is not the
case, the interface is ARPHRD_IPGRE and the protocol to be used for
encapsulation is skb->protocol.

Fixes: 2e15ea390e6f4 ("ip_gre: Add support to collect tunnel metadata.")
	Signed-off-by: Jiri Benc <jbenc@redhat.com>
	Acked-by: Pravin B Shelar <pshelar@ovn.org>
	Reviewed-by: Simon Horman <simon.horman@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 2090714e1d6e80979dd6926be22b0de9ca432273)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/ip_gre.c
diff --cc net/ipv4/ip_gre.c
index 215bbcd478d6,f973e0a58993..000000000000
--- a/net/ipv4/ip_gre.c
+++ b/net/ipv4/ip_gre.c
@@@ -246,12 -499,128 +246,133 @@@ static void __gre_xmit(struct sk_buff *
  	ip_tunnel_xmit(skb, dev, tnl_params, tnl_params->protocol);
  }
  
++<<<<<<< HEAD
++=======
+ static struct sk_buff *gre_handle_offloads(struct sk_buff *skb,
+ 					   bool csum)
+ {
+ 	return iptunnel_handle_offloads(skb, csum ? SKB_GSO_GRE_CSUM : SKB_GSO_GRE);
+ }
+ 
+ static struct rtable *gre_get_rt(struct sk_buff *skb,
+ 				 struct net_device *dev,
+ 				 struct flowi4 *fl,
+ 				 const struct ip_tunnel_key *key)
+ {
+ 	struct net *net = dev_net(dev);
+ 
+ 	memset(fl, 0, sizeof(*fl));
+ 	fl->daddr = key->u.ipv4.dst;
+ 	fl->saddr = key->u.ipv4.src;
+ 	fl->flowi4_tos = RT_TOS(key->tos);
+ 	fl->flowi4_mark = skb->mark;
+ 	fl->flowi4_proto = IPPROTO_GRE;
+ 
+ 	return ip_route_output_key(net, fl);
+ }
+ 
+ static void gre_fb_xmit(struct sk_buff *skb, struct net_device *dev,
+ 			__be16 proto)
+ {
+ 	struct ip_tunnel_info *tun_info;
+ 	const struct ip_tunnel_key *key;
+ 	struct rtable *rt = NULL;
+ 	struct flowi4 fl;
+ 	int min_headroom;
+ 	int tunnel_hlen;
+ 	__be16 df, flags;
+ 	bool use_cache;
+ 	int err;
+ 
+ 	tun_info = skb_tunnel_info(skb);
+ 	if (unlikely(!tun_info || !(tun_info->mode & IP_TUNNEL_INFO_TX) ||
+ 		     ip_tunnel_info_af(tun_info) != AF_INET))
+ 		goto err_free_skb;
+ 
+ 	key = &tun_info->key;
+ 	use_cache = ip_tunnel_dst_cache_usable(skb, tun_info);
+ 	if (use_cache)
+ 		rt = dst_cache_get_ip4(&tun_info->dst_cache, &fl.saddr);
+ 	if (!rt) {
+ 		rt = gre_get_rt(skb, dev, &fl, key);
+ 		if (IS_ERR(rt))
+ 				goto err_free_skb;
+ 		if (use_cache)
+ 			dst_cache_set_ip4(&tun_info->dst_cache, &rt->dst,
+ 					  fl.saddr);
+ 	}
+ 
+ 	tunnel_hlen = ip_gre_calc_hlen(key->tun_flags);
+ 
+ 	min_headroom = LL_RESERVED_SPACE(rt->dst.dev) + rt->dst.header_len
+ 			+ tunnel_hlen + sizeof(struct iphdr);
+ 	if (skb_headroom(skb) < min_headroom || skb_header_cloned(skb)) {
+ 		int head_delta = SKB_DATA_ALIGN(min_headroom -
+ 						skb_headroom(skb) +
+ 						16);
+ 		err = pskb_expand_head(skb, max_t(int, head_delta, 0),
+ 				       0, GFP_ATOMIC);
+ 		if (unlikely(err))
+ 			goto err_free_rt;
+ 	}
+ 
+ 	/* Push Tunnel header. */
+ 	skb = gre_handle_offloads(skb, !!(tun_info->key.tun_flags & TUNNEL_CSUM));
+ 	if (IS_ERR(skb)) {
+ 		skb = NULL;
+ 		goto err_free_rt;
+ 	}
+ 
+ 	flags = tun_info->key.tun_flags & (TUNNEL_CSUM | TUNNEL_KEY);
+ 	build_header(skb, tunnel_hlen, flags, proto,
+ 		     tunnel_id_to_key(tun_info->key.tun_id), 0);
+ 
+ 	df = key->tun_flags & TUNNEL_DONT_FRAGMENT ?  htons(IP_DF) : 0;
+ 
+ 	iptunnel_xmit(skb->sk, rt, skb, fl.saddr, key->u.ipv4.dst, IPPROTO_GRE,
+ 		      key->tos, key->ttl, df, false);
+ 	return;
+ 
+ err_free_rt:
+ 	ip_rt_put(rt);
+ err_free_skb:
+ 	kfree_skb(skb);
+ 	dev->stats.tx_dropped++;
+ }
+ 
+ static int gre_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)
+ {
+ 	struct ip_tunnel_info *info = skb_tunnel_info(skb);
+ 	struct rtable *rt;
+ 	struct flowi4 fl4;
+ 
+ 	if (ip_tunnel_info_af(info) != AF_INET)
+ 		return -EINVAL;
+ 
+ 	rt = gre_get_rt(skb, dev, &fl4, &info->key);
+ 	if (IS_ERR(rt))
+ 		return PTR_ERR(rt);
+ 
+ 	ip_rt_put(rt);
+ 	info->key.u.ipv4.src = fl4.saddr;
+ 	return 0;
+ }
+ 
++>>>>>>> 2090714e1d6e (gre: build header correctly for collect metadata tunnels)
  static netdev_tx_t ipgre_xmit(struct sk_buff *skb,
  			      struct net_device *dev)
  {
  	struct ip_tunnel *tunnel = netdev_priv(dev);
  	const struct iphdr *tnl_params;
  
++<<<<<<< HEAD
++=======
+ 	if (tunnel->collect_md) {
+ 		gre_fb_xmit(skb, dev, skb->protocol);
+ 		return NETDEV_TX_OK;
+ 	}
+ 
++>>>>>>> 2090714e1d6e (gre: build header correctly for collect metadata tunnels)
  	if (dev->header_ops) {
  		/* Need space for new headers */
  		if (skb_cow_head(skb, dev->needed_headroom -
@@@ -292,6 -660,11 +413,14 @@@ static netdev_tx_t gre_tap_xmit(struct 
  {
  	struct ip_tunnel *tunnel = netdev_priv(dev);
  
++<<<<<<< HEAD
++=======
+ 	if (tunnel->collect_md) {
+ 		gre_fb_xmit(skb, dev, htons(ETH_P_TEB));
+ 		return NETDEV_TX_OK;
+ 	}
+ 
++>>>>>>> 2090714e1d6e (gre: build header correctly for collect metadata tunnels)
  	skb = gre_handle_offloads(skb, !!(tunnel->parms.o_flags&TUNNEL_CSUM));
  	if (IS_ERR(skb))
  		goto out;
* Unmerged path net/ipv4/ip_gre.c
