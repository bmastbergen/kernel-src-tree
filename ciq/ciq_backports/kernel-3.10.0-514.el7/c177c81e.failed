hugetlb: restrict hugepage_migration_support() to x86_64

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit c177c81e09e517bbf75b67762cdab1b83aba6976
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/c177c81e.failed

Currently hugepage migration is available for all archs which support
pmd-level hugepage, but testing is done only for x86_64 and there're
bugs for other archs.  So to avoid breaking such archs, this patch
limits the availability strictly to x86_64 until developers of other
archs get interested in enabling this feature.

Simply disabling hugepage migration on non-x86_64 archs is not enough to
fix the reported problem where sys_move_pages() hits the BUG_ON() in
follow_page(FOLL_GET), so let's fix this by checking if hugepage
migration is supported in vma_migratable().

	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Reported-by: Michael Ellerman <mpe@ellerman.id.au>
	Tested-by: Michael Ellerman <mpe@ellerman.id.au>
	Acked-by: Hugh Dickins <hughd@google.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Russell King <rmk@arm.linux.org.uk>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: James Hogan <james.hogan@imgtec.com>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: David Miller <davem@davemloft.net>
	Cc: <stable@vger.kernel.org>	[3.12+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c177c81e09e517bbf75b67762cdab1b83aba6976)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/mm/hugetlbpage.c
#	arch/arm64/mm/hugetlbpage.c
#	arch/x86/mm/hugetlbpage.c
#	include/linux/hugetlb.h
diff --cc arch/x86/mm/hugetlbpage.c
index e4afb30191c1,8b977ebf9388..000000000000
--- a/arch/x86/mm/hugetlbpage.c
+++ b/arch/x86/mm/hugetlbpage.c
@@@ -223,7 -58,6 +223,10 @@@ follow_huge_pmd(struct mm_struct *mm, u
  {
  	return NULL;
  }
++<<<<<<< HEAD
 +
++=======
++>>>>>>> c177c81e09e5 (hugetlb: restrict hugepage_migration_support() to x86_64)
  #else
  
  struct page *
@@@ -241,36 -75,9 +244,39 @@@ int pud_huge(pud_t pud
  {
  	return !!(pud_val(pud) & _PAGE_PSE);
  }
++<<<<<<< HEAD
 +
 +struct page *
 +follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 +		pmd_t *pmd, int write)
 +{
 +	struct page *page;
 +
 +	page = pte_page(*(pte_t *)pmd);
 +	if (page)
 +		page += ((address & ~PMD_MASK) >> PAGE_SHIFT);
 +	return page;
 +}
 +
 +struct page *
 +follow_huge_pud(struct mm_struct *mm, unsigned long address,
 +		pud_t *pud, int write)
 +{
 +	struct page *page;
 +
 +	page = pte_page(*(pte_t *)pud);
 +	if (page)
 +		page += ((address & ~PUD_MASK) >> PAGE_SHIFT);
 +	return page;
 +}
 +
++=======
++>>>>>>> c177c81e09e5 (hugetlb: restrict hugepage_migration_support() to x86_64)
  #endif
  
 -#ifdef CONFIG_HUGETLB_PAGE
 +/* x86_64 also uses this file */
 +
 +#ifdef HAVE_ARCH_HUGETLB_UNMAPPED_AREA
  static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,
  		unsigned long addr, unsigned long len,
  		unsigned long pgoff, unsigned long flags)
diff --cc include/linux/hugetlb.h
index f71f104feba3,d0bad1a8b0bd..000000000000
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@@ -398,6 -390,17 +398,20 @@@ static inline pgoff_t basepage_index(st
  	return __basepage_index(page);
  }
  
++<<<<<<< HEAD
++=======
+ extern void dissolve_free_huge_pages(unsigned long start_pfn,
+ 				     unsigned long end_pfn);
+ static inline int hugepage_migration_support(struct hstate *h)
+ {
+ #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
+ 	return huge_page_shift(h) == PMD_SHIFT;
+ #else
+ 	return 0;
+ #endif
+ }
+ 
++>>>>>>> c177c81e09e5 (hugetlb: restrict hugepage_migration_support() to x86_64)
  static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
  					   struct mm_struct *mm, pte_t *pte)
  {
@@@ -444,6 -447,8 +458,11 @@@ static inline pgoff_t basepage_index(st
  {
  	return page->index;
  }
++<<<<<<< HEAD
++=======
+ #define dissolve_free_huge_pages(s, e)	do {} while (0)
+ #define hugepage_migration_support(h)	0
++>>>>>>> c177c81e09e5 (hugetlb: restrict hugepage_migration_support() to x86_64)
  
  static inline spinlock_t *huge_pte_lockptr(struct hstate *h,
  					   struct mm_struct *mm, pte_t *pte)
* Unmerged path arch/arm/mm/hugetlbpage.c
* Unmerged path arch/arm64/mm/hugetlbpage.c
* Unmerged path arch/arm/mm/hugetlbpage.c
* Unmerged path arch/arm64/mm/hugetlbpage.c
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 3fae2a63357c..68c7343012d2 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1916,6 +1916,10 @@ config ARCH_ENABLE_SPLIT_PMD_PTLOCK
 	def_bool y
 	depends on X86_64 || X86_PAE
 
+config ARCH_ENABLE_HUGEPAGE_MIGRATION
+	def_bool y
+	depends on X86_64 && HUGETLB_PAGE && MIGRATION
+
 menu "Power management and ACPI options"
 
 config ARCH_HIBERNATION_HEADER
* Unmerged path arch/x86/mm/hugetlbpage.c
* Unmerged path include/linux/hugetlb.h
diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 78fbe9ac5c1b..343045589075 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -176,6 +176,12 @@ static inline int vma_migratable(struct vm_area_struct *vma)
 {
 	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
 		return 0;
+
+#ifndef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
+	if (vma->vm_flags & VM_HUGETLB)
+		return 0;
+#endif
+
 	/*
 	 * Migration allocates pages in the highest zone. If we cannot
 	 * do so then migration (at least from node to node) is not
diff --git a/mm/Kconfig b/mm/Kconfig
index 88ac087b6ee9..fbbb076a1a6d 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -263,6 +263,9 @@ config MIGRATION
 	  pages as migration can relocate pages to satisfy a huge page
 	  allocation instead of reclaiming.
 
+config ARCH_ENABLE_HUGEPAGE_MIGRATION
+	boolean
+
 config PHYS_ADDR_T_64BIT
 	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT
 
