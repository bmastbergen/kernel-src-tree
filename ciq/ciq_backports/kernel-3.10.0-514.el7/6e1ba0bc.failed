xfs: add DAX IO path support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit 6e1ba0bcb84b3f97616feb07c27f974509ba57be
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/6e1ba0bc.failed

DAX does not do buffered IO (can't buffer direct access!) and hence
all read/write IO is vectored through the direct IO path.  Hence we
need to add the DAX IO path callouts to the direct IO
infrastructure.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 6e1ba0bcb84b3f97616feb07c27f974509ba57be)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_aops.c
diff --cc fs/xfs/xfs_aops.c
index 3ffbdb7cbd8f,e5e9fc23f230..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -1479,97 -1602,96 +1479,128 @@@ xfs_end_io_direct_write
  	struct kiocb		*iocb,
  	loff_t			offset,
  	ssize_t			size,
 -	void			*private)
 -{
 -	struct inode		*inode = file_inode(iocb->ki_filp);
 -	struct xfs_ioend	*ioend = private;
 -
 -	trace_xfs_gbmap_direct_endio(XFS_I(inode), offset, size,
 -				     ioend ? ioend->io_type : 0, NULL);
 -
 -	if (!ioend) {
 -		ASSERT(offset + size <= i_size_read(inode));
 -		return;
 -	}
 -
 -	__xfs_end_io_direct_write(inode, ioend, offset, size);
 -}
 -
 -/*
 - * For DAX we need a mapping buffer callback for unwritten extent conversion
 - * when page faults allocate blocks and then zero them. Note that in this
 - * case the mapping indicated by the ioend may extend beyond EOF. We most
 - * definitely do not want to extend EOF here, so we trim back the ioend size to
 - * EOF.
 - */
 -#ifdef CONFIG_FS_DAX
 -void
 -xfs_end_io_dax_write(
 -	struct buffer_head	*bh,
 -	int			uptodate)
 +	void			*private,
 +	int			ret,
 +	bool			is_async)
  {
 -	struct xfs_ioend	*ioend = bh->b_private;
 -	struct inode		*inode = ioend->io_inode;
 -	ssize_t			size = ioend->io_size;
 -
 -	ASSERT(IS_DAX(ioend->io_inode));
 +	struct xfs_ioend	*ioend = iocb->private;
 +	struct xfs_inode	*ip = XFS_I(ioend->io_inode);
 +	unsigned long		flags;
  
 -	/* if there was an error zeroing, then don't convert it */
 -	if (!uptodate)
 -		ioend->io_error = -EIO;
 +	/*
 +	 * While the generic direct I/O code updates the inode size, it does
 +	 * so only after the end_io handler is called, which means our
 +	 * end_io handler thinks the on-disk size is outside the in-core
 +	 * size.  To prevent this just update it a little bit earlier here.
 +	 *
 +	 * We need to lock the test/set EOF update as we can be racing with
 +	 * other IO completions here to update the EOF. Failing to serialise
 +	 * here can result in EOF moving backwards and Bad Things Happen when
 +	 * that occurs.
 +	 */
 +	spin_lock_irqsave(&ip->i_size_lock, flags);
 +	if (offset + size > i_size_read(ioend->io_inode))
 +		i_size_write(ioend->io_inode, offset + size);
 +	spin_unlock_irqrestore(&ip->i_size_lock, flags);
  
  	/*
 -	 * Trim update to EOF, so we don't extend EOF during unwritten extent
 -	 * conversion of partial EOF blocks.
 +	 * blockdev_direct_IO can return an error even after the I/O
 +	 * completion handler was called.  Thus we need to protect
 +	 * against double-freeing.
  	 */
 -	spin_lock(&XFS_I(inode)->i_flags_lock);
 -	if (ioend->io_offset + size > i_size_read(inode))
 -		size = i_size_read(inode) - ioend->io_offset;
 -	spin_unlock(&XFS_I(inode)->i_flags_lock);
 +	iocb->private = NULL;
  
 -	__xfs_end_io_direct_write(inode, ioend, ioend->io_offset, size);
 +	ioend->io_offset = offset;
 +	ioend->io_size = size;
 +	ioend->io_iocb = iocb;
 +	ioend->io_result = ret;
 +	if (private && size > 0)
 +		ioend->io_type = XFS_IO_UNWRITTEN;
  
 +	if (is_async) {
 +		ioend->io_isasync = 1;
 +		xfs_finish_ioend(ioend);
 +	} else {
 +		xfs_finish_ioend_sync(ioend);
 +	}
  }
 -#else
 -void xfs_end_io_dax_write(struct buffer_head *bh, int uptodate) { }
 -#endif
  
+ static inline ssize_t
+ xfs_vm_do_dio(
+ 	struct inode		*inode,
+ 	struct kiocb		*iocb,
+ 	struct iov_iter		*iter,
+ 	loff_t			offset,
+ 	void			(*endio)(struct kiocb	*iocb,
+ 					 loff_t		offset,
+ 					 ssize_t	size,
+ 					 void		*private),
+ 	int			flags)
+ {
+ 	struct block_device	*bdev;
+ 
+ 	if (IS_DAX(inode))
+ 		return dax_do_io(iocb, inode, iter, offset,
+ 				 xfs_get_blocks_direct, endio, 0);
+ 
+ 	bdev = xfs_find_bdev_for_inode(inode);
+ 	return  __blockdev_direct_IO(iocb, inode, bdev, iter, offset,
+ 				     xfs_get_blocks_direct, endio, NULL, flags);
+ }
+ 
  STATIC ssize_t
  xfs_vm_direct_IO(
 +	int			rw,
  	struct kiocb		*iocb,
 -	struct iov_iter		*iter,
 -	loff_t			offset)
 +	const struct iovec	*iov,
 +	loff_t			offset,
 +	unsigned long		nr_segs)
  {
  	struct inode		*inode = iocb->ki_filp->f_mapping->host;
++<<<<<<< HEAD
 +	struct block_device	*bdev = xfs_find_bdev_for_inode(inode);
 +	struct xfs_ioend	*ioend = NULL;
 +	ssize_t			ret;
 +
 +	if (rw & WRITE) {
 +		size_t size = iov_length(iov, nr_segs);
 +
 +		/*
 +		 * We cannot preallocate a size update transaction here as we
 +		 * don't know whether allocation is necessary or not. Hence we
 +		 * can only tell IO completion that one is necessary if we are
 +		 * not doing unwritten extent conversion.
 +		 */
 +		iocb->private = ioend = xfs_alloc_ioend(inode, XFS_IO_DIRECT);
 +		if (offset + size > XFS_I(inode)->i_d.di_size)
 +			ioend->io_isdirect = 1;
 +
 +		ret = __blockdev_direct_IO(rw, iocb, inode, bdev, iov,
 +					    offset, nr_segs,
 +					    xfs_get_blocks_direct,
 +					    xfs_end_io_direct_write, NULL,
 +					    DIO_ASYNC_EXTEND);
 +		if (ret != -EIOCBQUEUED && iocb->private)
 +			goto out_destroy_ioend;
 +	} else {
 +		ret = __blockdev_direct_IO(rw, iocb, inode, bdev, iov,
 +					    offset, nr_segs,
 +					    xfs_get_blocks_direct,
 +					    NULL, NULL, 0);
 +	}
 +
 +	return ret;
 +
 +out_destroy_ioend:
 +	xfs_destroy_ioend(ioend);
 +	return ret;
++=======
+ 
+ 	if (iov_iter_rw(iter) == WRITE)
+ 		return xfs_vm_do_dio(inode, iocb, iter, offset,
+ 				     xfs_end_io_direct_write, DIO_ASYNC_EXTEND);
+ 	return xfs_vm_do_dio(inode, iocb, iter, offset, NULL, 0);
++>>>>>>> 6e1ba0bcb84b (xfs: add DAX IO path support)
  }
  
  /*
* Unmerged path fs/xfs/xfs_aops.c
