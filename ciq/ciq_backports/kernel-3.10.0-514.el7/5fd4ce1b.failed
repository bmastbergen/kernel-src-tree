nvme: move nvme_{enable,disable,shutdown}_ctrl to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [block] nvme: move nvme_enable,disable,shutdown_ctrl to common code (David Milburn) [1288601]
Rebuild_FUZZ: 98.33%
commit-author Christoph Hellwig <hch@lst.de>
commit 5fd4ce1b005bd6ede913763f65efae9af6f7f386
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5fd4ce1b.failed

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 5fd4ce1b005bd6ede913763f65efae9af6f7f386)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
#	drivers/nvme/host/core.c
#	drivers/nvme/host/nvme.h
diff --cc drivers/block/nvme-core.c
index 2a6eb55ad96c,ccb315101a5e..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -49,10 -52,8 +49,13 @@@
  #define NVME_AQ_DEPTH		256
  #define SQ_SIZE(depth)		(depth * sizeof(struct nvme_command))
  #define CQ_SIZE(depth)		(depth * sizeof(struct nvme_completion))
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +#define ADMIN_TIMEOUT		(admin_timeout * HZ)
 +#define SHUTDOWN_TIMEOUT	(shutdown_timeout * HZ)
++=======
++>>>>>>> 5fd4ce1b005b (nvme: move nvme_{enable,disable,shutdown}_ctrl to common code):drivers/nvme/host/pci.c
  
 -unsigned char admin_timeout = 60;
 +static unsigned char admin_timeout = 60;
  module_param(admin_timeout, byte, 0644);
  MODULE_PARM_DESC(admin_timeout, "timeout in seconds for admin commands");
  
@@@ -95,6 -105,46 +98,49 @@@ struct async_cmd_info 
  };
  
  /*
++<<<<<<< HEAD:drivers/block/nvme-core.c
++=======
+  * Represents an NVM Express device.  Each nvme_dev is a PCI function.
+  */
+ struct nvme_dev {
+ 	struct list_head node;
+ 	struct nvme_queue **queues;
+ 	struct blk_mq_tag_set tagset;
+ 	struct blk_mq_tag_set admin_tagset;
+ 	u32 __iomem *dbs;
+ 	struct device *dev;
+ 	struct dma_pool *prp_page_pool;
+ 	struct dma_pool *prp_small_pool;
+ 	unsigned queue_count;
+ 	unsigned online_queues;
+ 	unsigned max_qid;
+ 	int q_depth;
+ 	u32 db_stride;
+ 	struct msix_entry *entry;
+ 	void __iomem *bar;
+ 	struct list_head namespaces;
+ 	struct device *device;
+ 	struct work_struct reset_work;
+ 	struct work_struct probe_work;
+ 	struct work_struct scan_work;
+ 	bool subsystem;
+ 	u32 max_hw_sectors;
+ 	u32 stripe_size;
+ 	void __iomem *cmb;
+ 	dma_addr_t cmb_dma_addr;
+ 	u64 cmb_size;
+ 	u32 cmbsz;
+ 
+ 	struct nvme_ctrl ctrl;
+ };
+ 
+ static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
+ {
+ 	return container_of(ctrl, struct nvme_dev, ctrl);
+ }
+ 
+ /*
++>>>>>>> 5fd4ce1b005b (nvme: move nvme_{enable,disable,shutdown}_ctrl to common code):drivers/nvme/host/pci.c
   * An NVM Express queue.  Each device has at least two (one for admin
   * commands and one for I/O commands).
   */
@@@ -450,9 -523,9 +497,9 @@@ static struct nvme_iod *nvme_alloc_iod(
  				(unsigned long) rq, gfp);
  }
  
 -static void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod)
 +void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod)
  {
- 	const int last_prp = dev->page_size / 8 - 1;
+ 	const int last_prp = dev->ctrl.page_size / 8 - 1;
  	int i;
  	__le64 **list = iod_list(iod);
  	dma_addr_t prp_dma = iod->first_dma;
@@@ -1262,6 -1269,48 +1309,51 @@@ static void nvme_disable_queue(struct n
  	spin_unlock_irq(&nvmeq->q_lock);
  }
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
++=======
+ static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
+ 				int entry_size)
+ {
+ 	int q_depth = dev->q_depth;
+ 	unsigned q_size_aligned = roundup(q_depth * entry_size,
+ 					  dev->ctrl.page_size);
+ 
+ 	if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+ 		u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+ 		mem_per_q = round_down(mem_per_q, dev->ctrl.page_size);
+ 		q_depth = div_u64(mem_per_q, entry_size);
+ 
+ 		/*
+ 		 * Ensure the reduced q_depth is above some threshold where it
+ 		 * would be better to map queues in system memory with the
+ 		 * original depth
+ 		 */
+ 		if (q_depth < 64)
+ 			return -ENOMEM;
+ 	}
+ 
+ 	return q_depth;
+ }
+ 
+ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
+ 				int qid, int depth)
+ {
+ 	if (qid && dev->cmb && use_cmb_sqes && NVME_CMB_SQS(dev->cmbsz)) {
+ 		unsigned offset = (qid - 1) * roundup(SQ_SIZE(depth),
+ 						      dev->ctrl.page_size);
+ 		nvmeq->sq_dma_addr = dev->cmb_dma_addr + offset;
+ 		nvmeq->sq_cmds_io = dev->cmb + offset;
+ 	} else {
+ 		nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(depth),
+ 					&nvmeq->sq_dma_addr, GFP_KERNEL);
+ 		if (!nvmeq->sq_cmds)
+ 			return -ENOMEM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 5fd4ce1b005b (nvme: move nvme_{enable,disable,shutdown}_ctrl to common code):drivers/nvme/host/pci.c
  static struct nvme_queue *nvme_alloc_queue(struct nvme_dev *dev, int qid,
  							int depth)
  {
@@@ -1361,79 -1406,8 +1453,82 @@@ static int nvme_create_queue(struct nvm
  	return result;
  }
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static int nvme_wait_ready(struct nvme_dev *dev, u64 cap, bool enabled)
 +{
 +	unsigned long timeout;
 +	u32 bit = enabled ? NVME_CSTS_RDY : 0;
 +
 +	timeout = ((NVME_CAP_TIMEOUT(cap) + 1) * HZ / 2) + jiffies;
 +
 +	while ((readl(&dev->bar->csts) & NVME_CSTS_RDY) != bit) {
 +		msleep(100);
 +		if (fatal_signal_pending(current))
 +			return -EINTR;
 +		if (time_after(jiffies, timeout)) {
 +			dev_err(&dev->pci_dev->dev,
 +				"Device not ready; aborting %s\n", enabled ?
 +						"initialisation" : "reset");
 +			return -ENODEV;
 +		}
 +	}
 +
 +	return 0;
 +}
 +
 +/*
 + * If the device has been passed off to us in an enabled state, just clear
 + * the enabled bit.  The spec says we should set the 'shutdown notification
 + * bits', but doing so may cause the device to complete commands to the
 + * admin queue ... and we don't know what memory that might be pointing at!
 + */
 +static int nvme_disable_ctrl(struct nvme_dev *dev, u64 cap)
 +{
 +	dev->ctrl_config &= ~NVME_CC_SHN_MASK;
 +	dev->ctrl_config &= ~NVME_CC_ENABLE;
 +	writel(dev->ctrl_config, &dev->bar->cc);
 +
 +	return nvme_wait_ready(dev, cap, false);
 +}
 +
 +static int nvme_enable_ctrl(struct nvme_dev *dev, u64 cap)
 +{
 +	dev->ctrl_config &= ~NVME_CC_SHN_MASK;
 +	dev->ctrl_config |= NVME_CC_ENABLE;
 +	writel(dev->ctrl_config, &dev->bar->cc);
 +
 +	return nvme_wait_ready(dev, cap, true);
 +}
 +
 +static int nvme_shutdown_ctrl(struct nvme_dev *dev)
 +{
 +	unsigned long timeout;
 +
 +	dev->ctrl_config &= ~NVME_CC_SHN_MASK;
 +	dev->ctrl_config |= NVME_CC_SHN_NORMAL;
 +
 +	writel(dev->ctrl_config, &dev->bar->cc);
 +
 +	timeout = SHUTDOWN_TIMEOUT + jiffies;
 +	while ((readl(&dev->bar->csts) & NVME_CSTS_SHST_MASK) !=
 +							NVME_CSTS_SHST_CMPLT) {
 +		msleep(100);
 +		if (fatal_signal_pending(current))
 +			return -EINTR;
 +		if (time_after(jiffies, timeout)) {
 +			dev_err(&dev->pci_dev->dev,
 +				"Device shutdown incomplete; abort shutdown\n");
 +			return -ENODEV;
 +		}
 +	}
 +
 +	return 0;
 +}
 +
++=======
++>>>>>>> 5fd4ce1b005b (nvme: move nvme_{enable,disable,shutdown}_ctrl to common code):drivers/nvme/host/pci.c
  static struct blk_mq_ops nvme_mq_admin_ops = {
 -	.queue_rq	= nvme_queue_rq,
 +	.queue_rq	= nvme_admin_queue_rq,
  	.map_queue	= blk_mq_map_queue,
  	.init_hctx	= nvme_admin_init_hctx,
  	.exit_hctx      = nvme_admin_exit_hctx,
@@@ -1492,34 -1467,17 +1587,34 @@@ static int nvme_configure_admin_queue(s
  {
  	int result;
  	u32 aqa;
 -	u64 cap = lo_hi_readq(dev->bar + NVME_REG_CAP);
 +	u64 cap = readq(&dev->bar->cap);
  	struct nvme_queue *nvmeq;
 +	unsigned page_shift = PAGE_SHIFT;
 +	unsigned dev_page_min = NVME_CAP_MPSMIN(cap) + 12;
 +	unsigned dev_page_max = NVME_CAP_MPSMAX(cap) + 12;
 +
 +	if (page_shift < dev_page_min) {
 +		dev_err(&dev->pci_dev->dev,
 +				"Minimum device page size (%u) too large for "
 +				"host (%u)\n", 1 << dev_page_min,
 +				1 << page_shift);
 +		return -ENODEV;
 +	}
 +	if (page_shift > dev_page_max) {
 +		dev_info(&dev->pci_dev->dev,
 +				"Device maximum page size (%u) smaller than "
 +				"host (%u); enabling work-around\n",
 +				1 << dev_page_max, 1 << page_shift);
 +		page_shift = dev_page_max;
 +	}
  
 -	dev->subsystem = readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 1) ?
 +	dev->subsystem = readl(&dev->bar->vs) >= NVME_VS(1, 1) ?
  						NVME_CAP_NSSRC(cap) : 0;
  
 -	if (dev->subsystem &&
 -	    (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
 -		writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
 +	if (dev->subsystem && (readl(&dev->bar->csts) & NVME_CSTS_NSSRO))
 +		writel(NVME_CSTS_NSSRO, &dev->bar->csts);
  
- 	result = nvme_disable_ctrl(dev, cap);
+ 	result = nvme_disable_ctrl(&dev->ctrl, cap);
  	if (result < 0)
  		return result;
  
@@@ -1533,18 -1491,11 +1628,18 @@@
  	aqa = nvmeq->q_depth - 1;
  	aqa |= aqa << 16;
  
 -	writel(aqa, dev->bar + NVME_REG_AQA);
 -	lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
 -	lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
 +	dev->page_size = 1 << page_shift;
 +
 +	dev->ctrl_config = NVME_CC_CSS_NVM;
 +	dev->ctrl_config |= (page_shift - 12) << NVME_CC_MPS_SHIFT;
 +	dev->ctrl_config |= NVME_CC_ARB_RR | NVME_CC_SHN_NONE;
 +	dev->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
 +
 +	writel(aqa, &dev->bar->aqa);
 +	writeq(nvmeq->sq_dma_addr, &dev->bar->asq);
 +	writeq(nvmeq->cq_dma_addr, &dev->bar->acq);
  
- 	result = nvme_enable_ctrl(dev, cap);
+ 	result = nvme_enable_ctrl(&dev->ctrl, cap);
  	if (result)
  		goto free_nvmeq;
  
@@@ -2142,8 -1599,9 +2237,12 @@@ static void nvme_alloc_ns(struct nvme_d
  	}
  	if (dev->stripe_size)
  		blk_queue_chunk_sectors(ns->queue, dev->stripe_size >> 9);
 -	if (dev->ctrl.vwc & NVME_CTRL_VWC_PRESENT)
 +	if (dev->vwc & NVME_CTRL_VWC_PRESENT)
  		blk_queue_flush(ns->queue, REQ_FLUSH | REQ_FUA);
++<<<<<<< HEAD:drivers/block/nvme-core.c
++=======
+ 	blk_queue_virt_boundary(ns->queue, dev->ctrl.page_size - 1);
++>>>>>>> 5fd4ce1b005b (nvme: move nvme_{enable,disable,shutdown}_ctrl to common code):drivers/nvme/host/pci.c
  
  	disk->major = nvme_major;
  	disk->first_minor = 0;
@@@ -2574,7 -2089,8 +2673,12 @@@ static void nvme_wait_dq(struct nvme_de
  			 * queues than admin tags.
  			 */
  			set_current_state(TASK_RUNNING);
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +			nvme_disable_ctrl(dev, readq(&dev->bar->cap));
++=======
+ 			nvme_disable_ctrl(&dev->ctrl,
+ 				lo_hi_readq(dev->bar + NVME_REG_CAP));
++>>>>>>> 5fd4ce1b005b (nvme: move nvme_{enable,disable,shutdown}_ctrl to common code):drivers/nvme/host/pci.c
  			nvme_clear_queue(dev->queues[0]);
  			flush_kthread_worker(dq->worker);
  			nvme_disable_queue(dev, 0);
@@@ -3100,7 -2585,24 +3204,28 @@@ static ssize_t nvme_sysfs_reset(struct 
  }
  static DEVICE_ATTR(reset_controller, S_IWUSR, NULL, nvme_sysfs_reset);
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static void nvme_async_probe(struct work_struct *work);
++=======
+ static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
+ {
+ 	*val = readl(to_nvme_dev(ctrl)->bar + off);
+ 	return 0;
+ }
+ 
+ static int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
+ {
+ 	writel(val, to_nvme_dev(ctrl)->bar + off);
+ 	return 0;
+ }
+ 
+ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
+ 	.reg_read32		= nvme_pci_reg_read32,
+ 	.reg_write32		= nvme_pci_reg_write32,
+ 	.free_ctrl		= nvme_pci_free_ctrl,
+ };
+ 
++>>>>>>> 5fd4ce1b005b (nvme: move nvme_{enable,disable,shutdown}_ctrl to common code):drivers/nvme/host/pci.c
  static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
  {
  	int node, result = -ENOMEM;
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
* Unmerged path drivers/block/nvme-core.c
* Unmerged path drivers/nvme/host/core.c
* Unmerged path drivers/nvme/host/nvme.h
