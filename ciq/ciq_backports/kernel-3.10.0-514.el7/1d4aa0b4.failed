crypto: vmx - Fixing AES-CTR counter bug

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [crypto] vmx - Fixing AES-CTR counter bug (Gustavo Duarte) [1274481]
Rebuild_FUZZ: 88.89%
commit-author Leonidas Da Silva Barbosa <leosilva@linux.vnet.ibm.com>
commit 1d4aa0b4c1816e8ca92a6aadb0d8f6b43c56c0d0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/1d4aa0b4.failed

AES-CTR is using a counter 8bytes-8bytes what miss match with
kernel specs.

In the previous code a vadduwm was done to increment counter.
Replacing this for a vadduqm now considering both cases counter
8-8 bytes and full 16bytes.

	Cc: stable@vger.kernel.org
	Signed-off-by: Leonidas S Barbosa <leosilva@linux.vnet.ibm.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 1d4aa0b4c1816e8ca92a6aadb0d8f6b43c56c0d0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/vmx/aes_ctr.c
diff --cc drivers/crypto/vmx/aes_ctr.c
index 96dbee4bf4a6,ee1306cd8f59..000000000000
--- a/drivers/crypto/vmx/aes_ctr.c
+++ b/drivers/crypto/vmx/aes_ctr.c
@@@ -106,42 -111,55 +106,72 @@@ static void p8_aes_ctr_final(struct p8_
  }
  
  static int p8_aes_ctr_crypt(struct blkcipher_desc *desc,
 -			    struct scatterlist *dst,
 -			    struct scatterlist *src, unsigned int nbytes)
 +    struct scatterlist *dst, struct scatterlist *src,
 +    unsigned int nbytes)
  {
++<<<<<<< HEAD
 +    int ret;
 +    struct blkcipher_walk walk;
 +    struct p8_aes_ctr_ctx *ctx = crypto_tfm_ctx(
 +            crypto_blkcipher_tfm(desc->tfm));
 +    struct blkcipher_desc fallback_desc = {
 +        .tfm = ctx->fallback,
 +        .info = desc->info,
 +        .flags = desc->flags
 +    };
++=======
+ 	int ret;
+ 	u64 inc;
+ 	struct blkcipher_walk walk;
+ 	struct p8_aes_ctr_ctx *ctx =
+ 		crypto_tfm_ctx(crypto_blkcipher_tfm(desc->tfm));
+ 	struct blkcipher_desc fallback_desc = {
+ 		.tfm = ctx->fallback,
+ 		.info = desc->info,
+ 		.flags = desc->flags
+ 	};
 -
 -	if (in_interrupt()) {
 -		ret = crypto_blkcipher_encrypt(&fallback_desc, dst, src,
 -					       nbytes);
 -	} else {
 -		blkcipher_walk_init(&walk, dst, src, nbytes);
 -		ret = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);
 -		while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
 -			pagefault_disable();
 -			enable_kernel_altivec();
 -			enable_kernel_vsx();
 -			aes_p8_ctr32_encrypt_blocks(walk.src.virt.addr,
 -						    walk.dst.virt.addr,
 -						    (nbytes &
 -						     AES_BLOCK_MASK) /
 -						    AES_BLOCK_SIZE,
 -						    &ctx->enc_key,
 -						    walk.iv);
 -			pagefault_enable();
 -
++>>>>>>> 1d4aa0b4c181 (crypto: vmx - Fixing AES-CTR counter bug)
 +
 +    if (in_interrupt()) {
 +        ret = crypto_blkcipher_encrypt(&fallback_desc, dst, src, nbytes);
 +    } else {
 +        blkcipher_walk_init(&walk, dst, src, nbytes);
 +        ret = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);
 +        while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
 +            pagefault_disable();
 +            enable_kernel_altivec();
 +            aes_p8_ctr32_encrypt_blocks(walk.src.virt.addr, walk.dst.virt.addr,
 +                (nbytes & AES_BLOCK_MASK)/AES_BLOCK_SIZE, &ctx->enc_key, walk.iv);
 +            pagefault_enable();
 +
++<<<<<<< HEAD
 +            crypto_inc(walk.iv, AES_BLOCK_SIZE);
 +            nbytes &= AES_BLOCK_SIZE - 1;
 +            ret = blkcipher_walk_done(desc, &walk, nbytes);
 +        }
 +        if (walk.nbytes) {
 +            p8_aes_ctr_final(ctx, &walk);
 +            ret = blkcipher_walk_done(desc, &walk, 0);
 +        }
 +    }
++=======
+ 			/* We need to update IV mostly for last bytes/round */
+ 			inc = (nbytes & AES_BLOCK_MASK) / AES_BLOCK_SIZE;
+ 			if (inc > 0)
+ 				while (inc--)
+ 					crypto_inc(walk.iv, AES_BLOCK_SIZE);
+ 
+ 			nbytes &= AES_BLOCK_SIZE - 1;
+ 			ret = blkcipher_walk_done(desc, &walk, nbytes);
+ 		}
+ 		if (walk.nbytes) {
+ 			p8_aes_ctr_final(ctx, &walk);
+ 			ret = blkcipher_walk_done(desc, &walk, 0);
+ 		}
+ 	}
++>>>>>>> 1d4aa0b4c181 (crypto: vmx - Fixing AES-CTR counter bug)
  
 -	return ret;
 +    return ret;
  }
  
  struct crypto_alg p8_aes_ctr_alg = {
* Unmerged path drivers/crypto/vmx/aes_ctr.c
diff --git a/drivers/crypto/vmx/aesp8-ppc.pl b/drivers/crypto/vmx/aesp8-ppc.pl
index 3ee8979e7625..e9390ff05ba5 100755
--- a/drivers/crypto/vmx/aesp8-ppc.pl
+++ b/drivers/crypto/vmx/aesp8-ppc.pl
@@ -1447,28 +1447,28 @@ Load_ctr32_enc_key:
 	?vperm		v31,v31,$out0,$keyperm
 	lvx		v25,$x10,$key_		# pre-load round[2]
 
-	vadduwm		$two,$one,$one
+	vadduqm		$two,$one,$one
 	subi		$inp,$inp,15		# undo "caller"
 	$SHL		$len,$len,4
 
-	vadduwm		$out1,$ivec,$one	# counter values ...
-	vadduwm		$out2,$ivec,$two
+	vadduqm		$out1,$ivec,$one	# counter values ...
+	vadduqm		$out2,$ivec,$two
 	vxor		$out0,$ivec,$rndkey0	# ... xored with rndkey[0]
 	 le?li		$idx,8
-	vadduwm		$out3,$out1,$two
+	vadduqm		$out3,$out1,$two
 	vxor		$out1,$out1,$rndkey0
 	 le?lvsl	$inpperm,0,$idx
-	vadduwm		$out4,$out2,$two
+	vadduqm		$out4,$out2,$two
 	vxor		$out2,$out2,$rndkey0
 	 le?vspltisb	$tmp,0x0f
-	vadduwm		$out5,$out3,$two
+	vadduqm		$out5,$out3,$two
 	vxor		$out3,$out3,$rndkey0
 	 le?vxor	$inpperm,$inpperm,$tmp	# transform for lvx_u/stvx_u
-	vadduwm		$out6,$out4,$two
+	vadduqm		$out6,$out4,$two
 	vxor		$out4,$out4,$rndkey0
-	vadduwm		$out7,$out5,$two
+	vadduqm		$out7,$out5,$two
 	vxor		$out5,$out5,$rndkey0
-	vadduwm		$ivec,$out6,$two	# next counter value
+	vadduqm		$ivec,$out6,$two	# next counter value
 	vxor		$out6,$out6,$rndkey0
 	vxor		$out7,$out7,$rndkey0
 
@@ -1604,27 +1604,27 @@ Loop_ctr32_enc8x_middle:
 
 	vcipherlast	$in0,$out0,$in0
 	vcipherlast	$in1,$out1,$in1
-	 vadduwm	$out1,$ivec,$one	# counter values ...
+	 vadduqm	$out1,$ivec,$one	# counter values ...
 	vcipherlast	$in2,$out2,$in2
-	 vadduwm	$out2,$ivec,$two
+	 vadduqm	$out2,$ivec,$two
 	 vxor		$out0,$ivec,$rndkey0	# ... xored with rndkey[0]
 	vcipherlast	$in3,$out3,$in3
-	 vadduwm	$out3,$out1,$two
+	 vadduqm	$out3,$out1,$two
 	 vxor		$out1,$out1,$rndkey0
 	vcipherlast	$in4,$out4,$in4
-	 vadduwm	$out4,$out2,$two
+	 vadduqm	$out4,$out2,$two
 	 vxor		$out2,$out2,$rndkey0
 	vcipherlast	$in5,$out5,$in5
-	 vadduwm	$out5,$out3,$two
+	 vadduqm	$out5,$out3,$two
 	 vxor		$out3,$out3,$rndkey0
 	vcipherlast	$in6,$out6,$in6
-	 vadduwm	$out6,$out4,$two
+	 vadduqm	$out6,$out4,$two
 	 vxor		$out4,$out4,$rndkey0
 	vcipherlast	$in7,$out7,$in7
-	 vadduwm	$out7,$out5,$two
+	 vadduqm	$out7,$out5,$two
 	 vxor		$out5,$out5,$rndkey0
 	le?vperm	$in0,$in0,$in0,$inpperm
-	 vadduwm	$ivec,$out6,$two	# next counter value
+	 vadduqm	$ivec,$out6,$two	# next counter value
 	 vxor		$out6,$out6,$rndkey0
 	le?vperm	$in1,$in1,$in1,$inpperm
 	 vxor		$out7,$out7,$rndkey0
