ceph: set i_head_snapc when getting CEPH_CAP_FILE_WR reference

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Yan, Zheng <zyan@redhat.com>
commit 5dda377cf0a6bd43f64a3c1efb670d7c668e7b29
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5dda377c.failed

In most cases that snap context is needed, we are holding
reference of CEPH_CAP_FILE_WR. So we can set ceph inode's
i_head_snapc when getting the CEPH_CAP_FILE_WR reference,
and make codes get snap context from i_head_snapc. This makes
the code simpler.

Another benefit of this change is that we can handle snap
notification more elegantly. Especially when snap context
is updated while someone else is doing write. The old queue
cap_snap code may set cap_snap's context to ether the old
context or the new snap context, depending on if i_head_snapc
is set. The new queue capp_snap code always set cap_snap's
context to the old snap context.

	Signed-off-by: Yan, Zheng <zyan@redhat.com>
(cherry picked from commit 5dda377cf0a6bd43f64a3c1efb670d7c668e7b29)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ceph/caps.c
#	fs/ceph/file.c
#	fs/ceph/snap.c
diff --cc fs/ceph/caps.c
index abbec56e2313,feb8ec92f1b4..000000000000
--- a/fs/ceph/caps.c
+++ b/fs/ceph/caps.c
@@@ -2229,50 -2268,70 +2266,85 @@@ static void check_max_size(struct inod
  int ceph_get_caps(struct ceph_inode_info *ci, int need, int want,
  		  loff_t endoff, int *got, struct page **pinned_page)
  {
- 	int _got, check_max, ret, err = 0;
+ 	int _got, ret, err = 0;
  
++<<<<<<< HEAD
 +retry:
 +	if (endoff > 0)
 +		check_max_size(&ci->vfs_inode, endoff);
 +	_got = 0;
 +	check_max = 0;
 +	ret = wait_event_interruptible(ci->i_cap_wq,
 +				try_get_cap_refs(ci, need, want, endoff,
 +						 &_got, &check_max, &err));
 +	if (err)
 +		ret = err;
 +	if (ret < 0)
 +		return ret;
++=======
+ 	ret = ceph_pool_perm_check(ci, need);
+ 	if (ret < 0)
+ 		return ret;
  
- 	if (check_max)
- 		goto retry;
+ 	while (true) {
+ 		if (endoff > 0)
+ 			check_max_size(&ci->vfs_inode, endoff);
++>>>>>>> 5dda377cf0a6 (ceph: set i_head_snapc when getting CEPH_CAP_FILE_WR reference)
  
- 	if (ci->i_inline_version != CEPH_INLINE_NONE &&
- 	    (_got & (CEPH_CAP_FILE_CACHE|CEPH_CAP_FILE_LAZYIO)) &&
- 	    i_size_read(&ci->vfs_inode) > 0) {
- 		struct page *page = find_get_page(ci->vfs_inode.i_mapping, 0);
- 		if (page) {
- 			if (PageUptodate(page)) {
- 				*pinned_page = page;
- 				goto out;
- 			}
- 			page_cache_release(page);
- 		}
- 		/*
- 		 * drop cap refs first because getattr while holding
- 		 * caps refs can cause deadlock.
- 		 */
- 		ceph_put_cap_refs(ci, _got);
+ 		err = 0;
  		_got = 0;
+ 		ret = try_get_cap_refs(ci, need, want, endoff,
+ 				       false, &_got, &err);
+ 		if (ret) {
+ 			if (err == -EAGAIN)
+ 				continue;
+ 			if (err < 0)
+ 				return err;
+ 		} else {
+ 			ret = wait_event_interruptible(ci->i_cap_wq,
+ 					try_get_cap_refs(ci, need, want, endoff,
+ 							 true, &_got, &err));
+ 			if (err == -EAGAIN)
+ 				continue;
+ 			if (err < 0)
+ 				ret = err;
+ 			if (ret < 0)
+ 				return ret;
+ 		}
  
- 		/* getattr request will bring inline data into page cache */
- 		ret = __ceph_do_getattr(&ci->vfs_inode, NULL,
- 					CEPH_STAT_CAP_INLINE_DATA, true);
- 		if (ret < 0)
- 			return ret;
- 		goto retry;
+ 		if (ci->i_inline_version != CEPH_INLINE_NONE &&
+ 		    (_got & (CEPH_CAP_FILE_CACHE|CEPH_CAP_FILE_LAZYIO)) &&
+ 		    i_size_read(&ci->vfs_inode) > 0) {
+ 			struct page *page =
+ 				find_get_page(ci->vfs_inode.i_mapping, 0);
+ 			if (page) {
+ 				if (PageUptodate(page)) {
+ 					*pinned_page = page;
+ 					break;
+ 				}
+ 				page_cache_release(page);
+ 			}
+ 			/*
+ 			 * drop cap refs first because getattr while
+ 			 * holding * caps refs can cause deadlock.
+ 			 */
+ 			ceph_put_cap_refs(ci, _got);
+ 			_got = 0;
+ 
+ 			/*
+ 			 * getattr request will bring inline data into
+ 			 * page cache
+ 			 */
+ 			ret = __ceph_do_getattr(&ci->vfs_inode, NULL,
+ 						CEPH_STAT_CAP_INLINE_DATA,
+ 						true);
+ 			if (ret < 0)
+ 				return ret;
+ 			continue;
+ 		}
+ 		break;
  	}
- out:
+ 
  	*got = _got;
  	return 0;
  }
diff --cc fs/ceph/file.c
index 28243f329f48,0a76a370d798..000000000000
--- a/fs/ceph/file.c
+++ b/fs/ceph/file.c
@@@ -532,8 -557,8 +532,13 @@@ static void ceph_sync_write_unsafe(stru
   * objects, rollback on failure, etc.)
   */
  static ssize_t
++<<<<<<< HEAD
 +ceph_sync_direct_write(struct kiocb *iocb, const struct iovec *iov,
 +		       unsigned long nr_segs, size_t count)
++=======
+ ceph_sync_direct_write(struct kiocb *iocb, struct iov_iter *from, loff_t pos,
+ 		       struct ceph_snap_context *snapc)
++>>>>>>> 5dda377cf0a6 (ceph: set i_head_snapc when getting CEPH_CAP_FILE_WR reference)
  {
  	struct file *file = iocb->ki_filp;
  	struct inode *inode = file_inode(file);
@@@ -573,15 -595,11 +577,14 @@@
  		CEPH_OSD_FLAG_ONDISK |
  		CEPH_OSD_FLAG_WRITE;
  
 -	while (iov_iter_count(from) > 0) {
 -		u64 len = iov_iter_single_seg_count(from);
 -		size_t start;
 -		ssize_t n;
 +	iov_iter_init(&i, iov, nr_segs, count, 0);
 +
 +	while (iov_iter_count(&i) > 0) {
 +		void __user *data = i.iov->iov_base + i.iov_offset;
 +		u64 len = i.iov->iov_len - i.iov_offset;
 +
 +		page_align = (unsigned long)data & ~PAGE_MASK;
  
- 		snapc = ci->i_snap_realm->cached_context;
  		vino = ceph_vino(inode);
  		req = ceph_osdc_new_request(&fsc->client->osdc, &ci->i_layout,
  					    vino, pos, &len, 0,
@@@ -655,8 -672,9 +658,14 @@@ out
   * correct atomic write, we should e.g. take write locks on all
   * objects, rollback on failure, etc.)
   */
++<<<<<<< HEAD
 +static ssize_t ceph_sync_write(struct kiocb *iocb, const struct iovec *iov,
 +			       unsigned long nr_segs, size_t count)
++=======
+ static ssize_t
+ ceph_sync_write(struct kiocb *iocb, struct iov_iter *from, loff_t pos,
+ 		struct ceph_snap_context *snapc)
++>>>>>>> 5dda377cf0a6 (ceph: set i_head_snapc when getting CEPH_CAP_FILE_WR reference)
  {
  	struct file *file = iocb->ki_filp;
  	struct inode *inode = file_inode(file);
@@@ -1046,13 -993,31 +1053,41 @@@ retry_snap
  	     inode, ceph_vinop(inode), pos, count, ceph_cap_string(got));
  
  	if ((got & (CEPH_CAP_FILE_BUFFER|CEPH_CAP_FILE_LAZYIO)) == 0 ||
++<<<<<<< HEAD
 +	    (file->f_flags & O_DIRECT) || (fi->flags & CEPH_F_SYNC)) {
 +		mutex_unlock(&inode->i_mutex);
 +		if (file->f_flags & O_DIRECT)
 +			written = ceph_sync_direct_write(iocb, iov,
 +							 nr_segs, count);
 +		else
 +			written = ceph_sync_write(iocb, iov, nr_segs, count);
++=======
+ 	    (iocb->ki_flags & IOCB_DIRECT) || (fi->flags & CEPH_F_SYNC)) {
+ 		struct ceph_snap_context *snapc;
+ 		struct iov_iter data;
+ 		mutex_unlock(&inode->i_mutex);
+ 
+ 		spin_lock(&ci->i_ceph_lock);
+ 		if (__ceph_have_pending_cap_snap(ci)) {
+ 			struct ceph_cap_snap *capsnap =
+ 					list_last_entry(&ci->i_cap_snaps,
+ 							struct ceph_cap_snap,
+ 							ci_item);
+ 			snapc = ceph_get_snap_context(capsnap->context);
+ 		} else {
+ 			BUG_ON(!ci->i_head_snapc);
+ 			snapc = ceph_get_snap_context(ci->i_head_snapc);
+ 		}
+ 		spin_unlock(&ci->i_ceph_lock);
+ 
+ 		/* we might need to revert back to that point */
+ 		data = *from;
+ 		if (iocb->ki_flags & IOCB_DIRECT)
+ 			written = ceph_sync_direct_write(iocb, &data, pos,
+ 							 snapc);
+ 		else
+ 			written = ceph_sync_write(iocb, &data, pos, snapc);
++>>>>>>> 5dda377cf0a6 (ceph: set i_head_snapc when getting CEPH_CAP_FILE_WR reference)
  		if (written == -EOLDSNAPC) {
  			dout("aio_write %p %llx.%llx %llu~%u"
  				"got EOLDSNAPC, retrying\n",
@@@ -1061,7 -1026,11 +1096,13 @@@
  			mutex_lock(&inode->i_mutex);
  			goto retry_snap;
  		}
++<<<<<<< HEAD
++=======
+ 		if (written > 0)
+ 			iov_iter_advance(from, written);
+ 		ceph_put_snap_context(snapc);
++>>>>>>> 5dda377cf0a6 (ceph: set i_head_snapc when getting CEPH_CAP_FILE_WR reference)
  	} else {
 -		loff_t old_size = inode->i_size;
  		/*
  		 * No need to acquire the i_truncate_mutex. Because
  		 * the MDS revokes Fwb caps before sending truncate
diff --cc fs/ceph/snap.c
index a97e39f09ba6,5bfdab9a465e..000000000000
--- a/fs/ceph/snap.c
+++ b/fs/ceph/snap.c
@@@ -481,76 -484,79 +484,84 @@@ void ceph_queue_cap_snap(struct ceph_in
  		   writes in progress now were started before the previous
  		   cap_snap.  lucky us. */
  		dout("queue_cap_snap %p already pending\n", inode);
++<<<<<<< HEAD
 +		kfree(capsnap);
 +	} else if (ci->i_snap_realm->cached_context == empty_snapc) {
++=======
+ 		goto update_snapc;
+ 	}
+ 	if (ci->i_snap_realm->cached_context == ceph_empty_snapc) {
++>>>>>>> 5dda377cf0a6 (ceph: set i_head_snapc when getting CEPH_CAP_FILE_WR reference)
  		dout("queue_cap_snap %p empty snapc\n", inode);
- 		kfree(capsnap);
- 	} else if (dirty & (CEPH_CAP_AUTH_EXCL|CEPH_CAP_XATTR_EXCL|
- 			    CEPH_CAP_FILE_EXCL|CEPH_CAP_FILE_WR)) {
- 		struct ceph_snap_context *snapc = ci->i_head_snapc;
- 
- 		/*
- 		 * if we are a sync write, we may need to go to the snaprealm
- 		 * to get the current snapc.
- 		 */
- 		if (!snapc)
- 			snapc = ci->i_snap_realm->cached_context;
- 
- 		dout("queue_cap_snap %p cap_snap %p queuing under %p %s\n",
- 		     inode, capsnap, snapc, ceph_cap_string(dirty));
- 		ihold(inode);
- 
- 		atomic_set(&capsnap->nref, 1);
- 		capsnap->ci = ci;
- 		INIT_LIST_HEAD(&capsnap->ci_item);
- 		INIT_LIST_HEAD(&capsnap->flushing_item);
- 
- 		capsnap->follows = snapc->seq;
- 		capsnap->issued = __ceph_caps_issued(ci, NULL);
- 		capsnap->dirty = dirty;
- 
- 		capsnap->mode = inode->i_mode;
- 		capsnap->uid = inode->i_uid;
- 		capsnap->gid = inode->i_gid;
- 
- 		if (dirty & CEPH_CAP_XATTR_EXCL) {
- 			__ceph_build_xattrs_blob(ci);
- 			capsnap->xattr_blob =
- 				ceph_buffer_get(ci->i_xattrs.blob);
- 			capsnap->xattr_version = ci->i_xattrs.version;
- 		} else {
- 			capsnap->xattr_blob = NULL;
- 			capsnap->xattr_version = 0;
- 		}
+ 		goto update_snapc;
+ 	}
+ 	if (!(dirty & (CEPH_CAP_AUTH_EXCL|CEPH_CAP_XATTR_EXCL|
+ 		       CEPH_CAP_FILE_EXCL|CEPH_CAP_FILE_WR))) {
+ 		dout("queue_cap_snap %p nothing dirty|writing\n", inode);
+ 		goto update_snapc;
+ 	}
  
- 		capsnap->inline_data = ci->i_inline_version != CEPH_INLINE_NONE;
+ 	BUG_ON(!old_snapc);
  
- 		/* dirty page count moved from _head to this cap_snap;
- 		   all subsequent writes page dirties occur _after_ this
- 		   snapshot. */
- 		capsnap->dirty_pages = ci->i_wrbuffer_ref_head;
- 		ci->i_wrbuffer_ref_head = 0;
- 		capsnap->context = snapc;
- 		ci->i_head_snapc =
- 			ceph_get_snap_context(ci->i_snap_realm->cached_context);
- 		dout(" new snapc is %p\n", ci->i_head_snapc);
- 		list_add_tail(&capsnap->ci_item, &ci->i_cap_snaps);
- 
- 		if (used & CEPH_CAP_FILE_WR) {
- 			dout("queue_cap_snap %p cap_snap %p snapc %p"
- 			     " seq %llu used WR, now pending\n", inode,
- 			     capsnap, snapc, snapc->seq);
- 			capsnap->writing = 1;
- 		} else {
- 			/* note mtime, size NOW. */
- 			__ceph_finish_cap_snap(ci, capsnap);
- 		}
+ 	dout("queue_cap_snap %p cap_snap %p queuing under %p %s\n",
+ 	     inode, capsnap, old_snapc, ceph_cap_string(dirty));
+ 	ihold(inode);
+ 
+ 	atomic_set(&capsnap->nref, 1);
+ 	capsnap->ci = ci;
+ 	INIT_LIST_HEAD(&capsnap->ci_item);
+ 	INIT_LIST_HEAD(&capsnap->flushing_item);
+ 
+ 	capsnap->follows = old_snapc->seq;
+ 	capsnap->issued = __ceph_caps_issued(ci, NULL);
+ 	capsnap->dirty = dirty;
+ 
+ 	capsnap->mode = inode->i_mode;
+ 	capsnap->uid = inode->i_uid;
+ 	capsnap->gid = inode->i_gid;
+ 
+ 	if (dirty & CEPH_CAP_XATTR_EXCL) {
+ 		__ceph_build_xattrs_blob(ci);
+ 		capsnap->xattr_blob =
+ 			ceph_buffer_get(ci->i_xattrs.blob);
+ 		capsnap->xattr_version = ci->i_xattrs.version;
  	} else {
- 		dout("queue_cap_snap %p nothing dirty|writing\n", inode);
- 		kfree(capsnap);
+ 		capsnap->xattr_blob = NULL;
+ 		capsnap->xattr_version = 0;
+ 	}
+ 
+ 	capsnap->inline_data = ci->i_inline_version != CEPH_INLINE_NONE;
+ 
+ 	/* dirty page count moved from _head to this cap_snap;
+ 	   all subsequent writes page dirties occur _after_ this
+ 	   snapshot. */
+ 	capsnap->dirty_pages = ci->i_wrbuffer_ref_head;
+ 	ci->i_wrbuffer_ref_head = 0;
+ 	capsnap->context = old_snapc;
+ 	list_add_tail(&capsnap->ci_item, &ci->i_cap_snaps);
+ 	old_snapc = NULL;
+ 
+ 	if (used & CEPH_CAP_FILE_WR) {
+ 		dout("queue_cap_snap %p cap_snap %p snapc %p"
+ 		     " seq %llu used WR, now pending\n", inode,
+ 		     capsnap, old_snapc, old_snapc->seq);
+ 		capsnap->writing = 1;
+ 	} else {
+ 		/* note mtime, size NOW. */
+ 		__ceph_finish_cap_snap(ci, capsnap);
  	}
+ 	capsnap = NULL;
  
+ update_snapc:
+ 	if (ci->i_head_snapc) {
+ 		ci->i_head_snapc = ceph_get_snap_context(
+ 				ci->i_snap_realm->cached_context);
+ 		dout(" new snapc is %p\n", ci->i_head_snapc);
+ 	}
  	spin_unlock(&ci->i_ceph_lock);
+ 
+ 	kfree(capsnap);
+ 	ceph_put_snap_context(old_snapc);
  }
  
  /*
diff --git a/fs/ceph/addr.c b/fs/ceph/addr.c
index 74bb35f71b62..9be6b70b5769 100644
--- a/fs/ceph/addr.c
+++ b/fs/ceph/addr.c
@@ -86,17 +86,21 @@ static int ceph_set_page_dirty(struct page *page)
 	inode = mapping->host;
 	ci = ceph_inode(inode);
 
-	/*
-	 * Note that we're grabbing a snapc ref here without holding
-	 * any locks!
-	 */
-	snapc = ceph_get_snap_context(ci->i_snap_realm->cached_context);
-
 	/* dirty the head */
 	spin_lock(&ci->i_ceph_lock);
-	if (ci->i_head_snapc == NULL)
-		ci->i_head_snapc = ceph_get_snap_context(snapc);
-	++ci->i_wrbuffer_ref_head;
+	BUG_ON(ci->i_wr_ref == 0); // caller should hold Fw reference
+	if (__ceph_have_pending_cap_snap(ci)) {
+		struct ceph_cap_snap *capsnap =
+				list_last_entry(&ci->i_cap_snaps,
+						struct ceph_cap_snap,
+						ci_item);
+		snapc = ceph_get_snap_context(capsnap->context);
+		capsnap->dirty_pages++;
+	} else {
+		BUG_ON(!ci->i_head_snapc);
+		snapc = ceph_get_snap_context(ci->i_head_snapc);
+		++ci->i_wrbuffer_ref_head;
+	}
 	if (ci->i_wrbuffer_ref == 0)
 		ihold(inode);
 	++ci->i_wrbuffer_ref;
@@ -1006,7 +1010,6 @@ static int ceph_update_writeable_page(struct file *file,
 {
 	struct inode *inode = file_inode(file);
 	struct ceph_inode_info *ci = ceph_inode(inode);
-	struct ceph_mds_client *mdsc = ceph_inode_to_client(inode)->mdsc;
 	loff_t page_off = pos & PAGE_CACHE_MASK;
 	int pos_in_page = pos & ~PAGE_CACHE_MASK;
 	int end_in_page = pos_in_page + len;
@@ -1018,10 +1021,6 @@ retry_locked:
 	/* writepages currently holds page lock, but if we change that later, */
 	wait_on_page_writeback(page);
 
-	/* check snap context */
-	BUG_ON(!ci->i_snap_realm);
-	down_read(&mdsc->snap_rwsem);
-	BUG_ON(!ci->i_snap_realm->cached_context);
 	snapc = page_snap_context(page);
 	if (snapc && snapc != ci->i_head_snapc) {
 		/*
@@ -1029,7 +1028,6 @@ retry_locked:
 		 * context!  is it writeable now?
 		 */
 		oldest = get_oldest_context(inode, NULL);
-		up_read(&mdsc->snap_rwsem);
 
 		if (snapc->seq > oldest->seq) {
 			ceph_put_snap_context(oldest);
@@ -1086,7 +1084,6 @@ retry_locked:
 	}
 
 	/* we need to read it. */
-	up_read(&mdsc->snap_rwsem);
 	r = readpage_nounlock(file, page);
 	if (r < 0)
 		goto fail_nosnap;
@@ -1131,16 +1128,13 @@ static int ceph_write_begin(struct file *file, struct address_space *mapping,
 
 /*
  * we don't do anything in here that simple_write_end doesn't do
- * except adjust dirty page accounting and drop read lock on
- * mdsc->snap_rwsem.
+ * except adjust dirty page accounting
  */
 static int ceph_write_end(struct file *file, struct address_space *mapping,
 			  loff_t pos, unsigned len, unsigned copied,
 			  struct page *page, void *fsdata)
 {
 	struct inode *inode = file_inode(file);
-	struct ceph_fs_client *fsc = ceph_inode_to_client(inode);
-	struct ceph_mds_client *mdsc = fsc->mdsc;
 	unsigned from = pos & (PAGE_CACHE_SIZE - 1);
 	int check_cap = 0;
 
@@ -1162,7 +1156,6 @@ static int ceph_write_end(struct file *file, struct address_space *mapping,
 	set_page_dirty(page);
 
 	unlock_page(page);
-	up_read(&mdsc->snap_rwsem);
 	page_cache_release(page);
 
 	if (check_cap)
@@ -1289,7 +1282,6 @@ static int ceph_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
 	struct inode *inode = file_inode(vma->vm_file);
 	struct ceph_inode_info *ci = ceph_inode(inode);
 	struct ceph_file_info *fi = vma->vm_file->private_data;
-	struct ceph_mds_client *mdsc = ceph_inode_to_client(inode)->mdsc;
 	struct page *page = vmf->page;
 	loff_t off = page_offset(page);
 	loff_t size = i_size_read(inode);
@@ -1348,7 +1340,6 @@ static int ceph_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
 	if (ret == 0) {
 		/* success.  we'll keep the page locked. */
 		set_page_dirty(page);
-		up_read(&mdsc->snap_rwsem);
 		ret = VM_FAULT_LOCKED;
 	} else {
 		if (ret == -ENOMEM)
* Unmerged path fs/ceph/caps.c
* Unmerged path fs/ceph/file.c
* Unmerged path fs/ceph/snap.c
