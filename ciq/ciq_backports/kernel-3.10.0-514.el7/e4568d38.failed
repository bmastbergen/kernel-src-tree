mm, meminit: always return a valid node from early_pfn_to_nid

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] meminit: always return a valid node from early_pfn_to_nid (Koki Sanagi) [1359649]
Rebuild_FUZZ: 96.61%
commit-author Mel Gorman <mgorman@techsingularity.net>
commit e4568d3803852d00effd41dcdd489e726b998879
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e4568d38.failed

early_pfn_to_nid can return node 0 if a PFN is invalid on machines that
has no node 0.  A machine with only node 1 was observed to crash with
the following message:

   BUG: unable to handle kernel paging request at 000000000002a3c8
   PGD 0
   Modules linked in:
   Hardware name: Supermicro H8DSP-8/H8DSP-8, BIOS 080011  06/30/2006
   task: ffffffff81c0d500 ti: ffffffff81c00000 task.ti: ffffffff81c00000
   RIP: reserve_bootmem_region+0x6a/0xef
   CR2: 000000000002a3c8 CR3: 0000000001c06000 CR4: 00000000000006b0
   Call Trace:
      free_all_bootmem+0x4b/0x12a
      mem_init+0x70/0xa3
      start_kernel+0x25b/0x49b

The problem is that early_page_uninitialised uses the early_pfn_to_nid
helper which returns node 0 for invalid PFNs.  No caller of
early_pfn_to_nid cares except early_page_uninitialised.  This patch has
early_pfn_to_nid always return a valid node.

Link: http://lkml.kernel.org/r/1468008031-3848-3-git-send-email-mgorman@techsingularity.net
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Acked-by: David Rientjes <rientjes@google.com>
	Cc: <stable@vger.kernel.org>	[4.2+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e4568d3803852d00effd41dcdd489e726b998879)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index 20d353397e7d,5d013526bd0a..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -781,8 -1260,313 +781,316 @@@ void __init __free_pages_bootmem(struc
  	__free_pages(page, order);
  }
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) || \
+ 	defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
+ 
+ static struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;
+ 
+ int __meminit early_pfn_to_nid(unsigned long pfn)
+ {
+ 	static DEFINE_SPINLOCK(early_pfn_lock);
+ 	int nid;
+ 
+ 	spin_lock(&early_pfn_lock);
+ 	nid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);
+ 	if (nid < 0)
+ 		nid = first_online_node;
+ 	spin_unlock(&early_pfn_lock);
+ 
+ 	return nid;
+ }
+ #endif
+ 
+ #ifdef CONFIG_NODES_SPAN_OTHER_NODES
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	int nid;
+ 
+ 	nid = __early_pfn_to_nid(pfn, state);
+ 	if (nid >= 0 && nid != node)
+ 		return false;
+ 	return true;
+ }
+ 
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return meminit_pfn_in_nid(pfn, node, &early_pfnnid_cache);
+ }
+ 
+ #else
+ 
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return true;
+ }
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
+ void __init __free_pages_bootmem(struct page *page, unsigned long pfn,
+ 							unsigned int order)
+ {
+ 	if (early_page_uninitialised(pfn))
+ 		return;
+ 	return __free_pages_boot_core(page, order);
+ }
+ 
+ /*
+  * Check that the whole (or subset of) a pageblock given by the interval of
+  * [start_pfn, end_pfn) is valid and within the same zone, before scanning it
+  * with the migration of free compaction scanner. The scanners then need to
+  * use only pfn_valid_within() check for arches that allow holes within
+  * pageblocks.
+  *
+  * Return struct page pointer of start_pfn, or NULL if checks were not passed.
+  *
+  * It's possible on some configurations to have a setup like node0 node1 node0
+  * i.e. it's possible that all pages within a zones range of pages do not
+  * belong to a single zone. We assume that a border between node0 and node1
+  * can occur within a single pageblock, but not a node0 node1 node0
+  * interleaving within a single pageblock. It is therefore sufficient to check
+  * the first and last page of a pageblock and avoid checking each individual
+  * page in a pageblock.
+  */
+ struct page *__pageblock_pfn_to_page(unsigned long start_pfn,
+ 				     unsigned long end_pfn, struct zone *zone)
+ {
+ 	struct page *start_page;
+ 	struct page *end_page;
+ 
+ 	/* end_pfn is one past the range we are checking */
+ 	end_pfn--;
+ 
+ 	if (!pfn_valid(start_pfn) || !pfn_valid(end_pfn))
+ 		return NULL;
+ 
+ 	start_page = pfn_to_page(start_pfn);
+ 
+ 	if (page_zone(start_page) != zone)
+ 		return NULL;
+ 
+ 	end_page = pfn_to_page(end_pfn);
+ 
+ 	/* This gives a shorter code than deriving page_zone(end_page) */
+ 	if (page_zone_id(start_page) != page_zone_id(end_page))
+ 		return NULL;
+ 
+ 	return start_page;
+ }
+ 
+ void set_zone_contiguous(struct zone *zone)
+ {
+ 	unsigned long block_start_pfn = zone->zone_start_pfn;
+ 	unsigned long block_end_pfn;
+ 
+ 	block_end_pfn = ALIGN(block_start_pfn + 1, pageblock_nr_pages);
+ 	for (; block_start_pfn < zone_end_pfn(zone);
+ 			block_start_pfn = block_end_pfn,
+ 			 block_end_pfn += pageblock_nr_pages) {
+ 
+ 		block_end_pfn = min(block_end_pfn, zone_end_pfn(zone));
+ 
+ 		if (!__pageblock_pfn_to_page(block_start_pfn,
+ 					     block_end_pfn, zone))
+ 			return;
+ 	}
+ 
+ 	/* We confirm that there is no hole */
+ 	zone->contiguous = true;
+ }
+ 
+ void clear_zone_contiguous(struct zone *zone)
+ {
+ 	zone->contiguous = false;
+ }
+ 
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static void __init deferred_free_range(struct page *page,
+ 					unsigned long pfn, int nr_pages)
+ {
+ 	int i;
+ 
+ 	if (!page)
+ 		return;
+ 
+ 	/* Free a large naturally-aligned chunk if possible */
+ 	if (nr_pages == MAX_ORDER_NR_PAGES &&
+ 	    (pfn & (MAX_ORDER_NR_PAGES-1)) == 0) {
+ 		set_pageblock_migratetype(page, MIGRATE_MOVABLE);
+ 		__free_pages_boot_core(page, MAX_ORDER-1);
+ 		return;
+ 	}
+ 
+ 	for (i = 0; i < nr_pages; i++, page++)
+ 		__free_pages_boot_core(page, 0);
+ }
+ 
+ /* Completion tracking for deferred_init_memmap() threads */
+ static atomic_t pgdat_init_n_undone __initdata;
+ static __initdata DECLARE_COMPLETION(pgdat_init_all_done_comp);
+ 
+ static inline void __init pgdat_init_report_one_done(void)
+ {
+ 	if (atomic_dec_and_test(&pgdat_init_n_undone))
+ 		complete(&pgdat_init_all_done_comp);
+ }
+ 
+ /* Initialise remaining memory on a node */
+ static int __init deferred_init_memmap(void *data)
+ {
+ 	pg_data_t *pgdat = data;
+ 	int nid = pgdat->node_id;
+ 	struct mminit_pfnnid_cache nid_init_state = { };
+ 	unsigned long start = jiffies;
+ 	unsigned long nr_pages = 0;
+ 	unsigned long walk_start, walk_end;
+ 	int i, zid;
+ 	struct zone *zone;
+ 	unsigned long first_init_pfn = pgdat->first_deferred_pfn;
+ 	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
+ 
+ 	if (first_init_pfn == ULONG_MAX) {
+ 		pgdat_init_report_one_done();
+ 		return 0;
+ 	}
+ 
+ 	/* Bind memory initialisation thread to a local node if possible */
+ 	if (!cpumask_empty(cpumask))
+ 		set_cpus_allowed_ptr(current, cpumask);
+ 
+ 	/* Sanity check boundaries */
+ 	BUG_ON(pgdat->first_deferred_pfn < pgdat->node_start_pfn);
+ 	BUG_ON(pgdat->first_deferred_pfn > pgdat_end_pfn(pgdat));
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ 
+ 	/* Only the highest zone is deferred so find it */
+ 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
+ 		zone = pgdat->node_zones + zid;
+ 		if (first_init_pfn < zone_end_pfn(zone))
+ 			break;
+ 	}
+ 
+ 	for_each_mem_pfn_range(i, nid, &walk_start, &walk_end, NULL) {
+ 		unsigned long pfn, end_pfn;
+ 		struct page *page = NULL;
+ 		struct page *free_base_page = NULL;
+ 		unsigned long free_base_pfn = 0;
+ 		int nr_to_free = 0;
+ 
+ 		end_pfn = min(walk_end, zone_end_pfn(zone));
+ 		pfn = first_init_pfn;
+ 		if (pfn < walk_start)
+ 			pfn = walk_start;
+ 		if (pfn < zone->zone_start_pfn)
+ 			pfn = zone->zone_start_pfn;
+ 
+ 		for (; pfn < end_pfn; pfn++) {
+ 			if (!pfn_valid_within(pfn))
+ 				goto free_range;
+ 
+ 			/*
+ 			 * Ensure pfn_valid is checked every
+ 			 * MAX_ORDER_NR_PAGES for memory holes
+ 			 */
+ 			if ((pfn & (MAX_ORDER_NR_PAGES - 1)) == 0) {
+ 				if (!pfn_valid(pfn)) {
+ 					page = NULL;
+ 					goto free_range;
+ 				}
+ 			}
+ 
+ 			if (!meminit_pfn_in_nid(pfn, nid, &nid_init_state)) {
+ 				page = NULL;
+ 				goto free_range;
+ 			}
+ 
+ 			/* Minimise pfn page lookups and scheduler checks */
+ 			if (page && (pfn & (MAX_ORDER_NR_PAGES - 1)) != 0) {
+ 				page++;
+ 			} else {
+ 				nr_pages += nr_to_free;
+ 				deferred_free_range(free_base_page,
+ 						free_base_pfn, nr_to_free);
+ 				free_base_page = NULL;
+ 				free_base_pfn = nr_to_free = 0;
+ 
+ 				page = pfn_to_page(pfn);
+ 				cond_resched();
+ 			}
+ 
+ 			if (page->flags) {
+ 				VM_BUG_ON(page_zone(page) != zone);
+ 				goto free_range;
+ 			}
+ 
+ 			__init_single_page(page, pfn, zid, nid);
+ 			if (!free_base_page) {
+ 				free_base_page = page;
+ 				free_base_pfn = pfn;
+ 				nr_to_free = 0;
+ 			}
+ 			nr_to_free++;
+ 
+ 			/* Where possible, batch up pages for a single free */
+ 			continue;
+ free_range:
+ 			/* Free the current block of pages to allocator */
+ 			nr_pages += nr_to_free;
+ 			deferred_free_range(free_base_page, free_base_pfn,
+ 								nr_to_free);
+ 			free_base_page = NULL;
+ 			free_base_pfn = nr_to_free = 0;
+ 		}
+ 
+ 		first_init_pfn = max(end_pfn, first_init_pfn);
+ 	}
+ 
+ 	/* Sanity check that the next zone really is unpopulated */
+ 	WARN_ON(++zid < MAX_NR_ZONES && populated_zone(++zone));
+ 
+ 	pr_info("node %d initialised, %lu pages in %ums\n", nid, nr_pages,
+ 					jiffies_to_msecs(jiffies - start));
+ 
+ 	pgdat_init_report_one_done();
+ 	return 0;
+ }
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
+ 
+ void __init page_alloc_init_late(void)
+ {
+ 	struct zone *zone;
+ 
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ 	int nid;
+ 
+ 	/* There will be num_node_state(N_MEMORY) threads */
+ 	atomic_set(&pgdat_init_n_undone, num_node_state(N_MEMORY));
+ 	for_each_node_state(nid, N_MEMORY) {
+ 		kthread_run(deferred_init_memmap, NODE_DATA(nid), "pgdatinit%d", nid);
+ 	}
+ 
+ 	/* Block until all are initialised */
+ 	wait_for_completion(&pgdat_init_all_done_comp);
+ 
+ 	/* Reinit limits that are based on free pages after the kernel is up */
+ 	files_maxfiles_init();
+ #endif
+ 
+ 	for_each_populated_zone(zone)
+ 		set_zone_contiguous(zone);
+ }
+ 
++>>>>>>> e4568d380385 (mm, meminit: always return a valid node from early_pfn_to_nid)
  #ifdef CONFIG_CMA
 -/* Free whole pageblock and set its migration type to MIGRATE_CMA. */
 +/* Free whole pageblock and set it's migration type to MIGRATE_CMA. */
  void __init init_cma_reserved_pageblock(struct page *page)
  {
  	unsigned i = pageblock_nr_pages;
* Unmerged path mm/page_alloc.c
