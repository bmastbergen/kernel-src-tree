sched/fair: Allow calculate_imbalance() to move idle cpus

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [kernel] sched: Allow calculate_imbalance() to move idle cpus (Jiri Olsa) [1211784]
Rebuild_FUZZ: 95.41%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 9a5d9ba6a3631d55c358fe1bdbaa162a97471a05
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/9a5d9ba6.failed

Allow calculate_imbalance() to 'create' idle cpus in the busiest group
if there are idle cpus in the local group.

	Suggested-by: Rik van Riel <riel@redhat.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Acked-by: Vincent Guittot <vincent.guittot@linaro.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
Link: http://lkml.kernel.org/r/20140729152705.GX12054@laptop.lan
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 9a5d9ba6a3631d55c358fe1bdbaa162a97471a05)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 961e580cbff7,df1ed176c7b7..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -6002,17 -6273,16 +6002,25 @@@ static inline void calculate_imbalance(
  		return fix_small_imbalance(env, sds);
  	}
  
++<<<<<<< HEAD
 +	if (busiest->sum_nr_running > busiest->group_capacity_factor) {
 +		/*
 +		 * Don't want to pull so many tasks that a group would go idle.
 +		 * Except of course for the group_imb case, since then we might
 +		 * have to drop below capacity to reach cpu-load equilibrium.
 +		 */
++=======
+ 	/*
+ 	 * If there aren't any idle cpus, avoid creating some.
+ 	 */
+ 	if (busiest->group_type == group_overloaded &&
+ 	    local->group_type   == group_overloaded) {
++>>>>>>> 9a5d9ba6a363 (sched/fair: Allow calculate_imbalance() to move idle cpus)
  		load_above_capacity =
 -			(busiest->sum_nr_running - busiest->group_capacity_factor);
 +			(busiest->sum_nr_running - busiest->group_capacity);
  
 -		load_above_capacity *= (SCHED_LOAD_SCALE * SCHED_CAPACITY_SCALE);
 -		load_above_capacity /= busiest->group_capacity;
 +		load_above_capacity *= (SCHED_LOAD_SCALE * SCHED_POWER_SCALE);
 +		load_above_capacity /= busiest->group_power;
  	}
  
  	/*
* Unmerged path kernel/sched/fair.c
