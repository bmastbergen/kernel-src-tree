mm: hugetlb: fix copy_hugetlb_page_range()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] hugetlb: fix copy_hugetlb_page_range() (Tomoaki Nishimura) [1287322]
Rebuild_FUZZ: 95.00%
commit-author Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
commit 0253d634e0803a8376a0d88efee0bf523d8673f9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/0253d634.failed

Commit 4a705fef9862 ("hugetlb: fix copy_hugetlb_page_range() to handle
migration/hwpoisoned entry") changed the order of
huge_ptep_set_wrprotect() and huge_ptep_get(), which leads to breakage
in some workloads like hugepage-backed heap allocation via libhugetlbfs.
This patch fixes it.

The test program for the problem is shown below:

  $ cat heap.c
  #include <unistd.h>
  #include <stdlib.h>
  #include <string.h>

  #define HPS 0x200000

  int main() {
  	int i;
  	char *p = malloc(HPS);
  	memset(p, '1', HPS);
  	for (i = 0; i < 5; i++) {
  		if (!fork()) {
  			memset(p, '2', HPS);
  			p = malloc(HPS);
  			memset(p, '3', HPS);
  			free(p);
  			return 0;
  		}
  	}
  	sleep(1);
  	free(p);
  	return 0;
  }

  $ export HUGETLB_MORECORE=yes ; export HUGETLB_NO_PREFAULT= ; hugectl --heap ./heap

Fixes 4a705fef9862 ("hugetlb: fix copy_hugetlb_page_range() to handle
migration/hwpoisoned entry"), so is applicable to -stable kernels which
include it.

	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Reported-by: Guillaume Morin <guillaume@morinfr.org>
	Suggested-by: Guillaume Morin <guillaume@morinfr.org>
	Acked-by: Hugh Dickins <hughd@google.com>
	Cc: <stable@vger.kernel.org>	[2.6.37+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0253d634e0803a8376a0d88efee0bf523d8673f9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 4a192c9e708d,9221c02ed9e2..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -2566,12 -2584,26 +2566,15 @@@ int copy_hugetlb_page_range(struct mm_s
  		dst_ptl = huge_pte_lock(h, dst, dst_pte);
  		src_ptl = huge_pte_lockptr(h, src, src_pte);
  		spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
 -		entry = huge_ptep_get(src_pte);
 -		if (huge_pte_none(entry)) { /* skip none entry */
 -			;
 -		} else if (unlikely(is_hugetlb_entry_migration(entry) ||
 -				    is_hugetlb_entry_hwpoisoned(entry))) {
 -			swp_entry_t swp_entry = pte_to_swp_entry(entry);
 -
 -			if (is_write_migration_entry(swp_entry) && cow) {
 -				/*
 -				 * COW mappings require pages in both
 -				 * parent and child to be set to read.
 -				 */
 -				make_migration_entry_read(&swp_entry);
 -				entry = swp_entry_to_pte(swp_entry);
 -				set_huge_pte_at(src, addr, src_pte, entry);
 -			}
 -			set_huge_pte_at(dst, addr, dst_pte, entry);
 -		} else {
 -			if (cow)
 +		if (!huge_pte_none(huge_ptep_get(src_pte))) {
 +			if (cow) {
  				huge_ptep_set_wrprotect(src, addr, src_pte);
++<<<<<<< HEAD
 +				mmu_notifier_invalidate_range(src, mmun_start,
 +							      mmun_end);
 +			}
++=======
++>>>>>>> 0253d634e080 (mm: hugetlb: fix copy_hugetlb_page_range())
  			entry = huge_ptep_get(src_pte);
  			ptepage = pte_page(entry);
  			get_page(ptepage);
* Unmerged path mm/hugetlb.c
