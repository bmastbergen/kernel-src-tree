lib: fix data race in rhashtable_rehash_one

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [lib] rhashtable: fix data race in rhashtable_rehash_one (Phil Sutter) [1238749]
Rebuild_FUZZ: 88.17%
commit-author Dmitriy Vyukov <dvyukov@google.com>
commit 7def0f952eccdd0edb3c504f4dab35ee0d3aba1f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/7def0f95.failed

rhashtable_rehash_one() uses complex logic to update entry->next field,
after INIT_RHT_NULLS_HEAD and NULLS_MARKER expansion:

entry->next = 1 | ((base + off) << 1)

This can be compiled along the lines of:

entry->next = base + off
entry->next <<= 1
entry->next |= 1

Which will break concurrent readers.

NULLS value recomputation is not needed here, so just remove
the complex logic.

The data race was found with KernelThreadSanitizer (KTSAN).

	Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
	Acked-by: Eric Dumazet <edumazet@google.com>
	Acked-by: Thomas Graf <tgraf@suug.ch>
	Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 7def0f952eccdd0edb3c504f4dab35ee0d3aba1f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/rhashtable.c
diff --cc lib/rhashtable.c
index 15ef938f4f67,a54ff8949f91..000000000000
--- a/lib/rhashtable.c
+++ b/lib/rhashtable.c
@@@ -115,76 -130,144 +115,131 @@@ static struct bucket_table *bucket_tabl
  	return tbl;
  }
  
 -static struct bucket_table *rhashtable_last_table(struct rhashtable *ht,
 -						  struct bucket_table *tbl)
 +static void bucket_table_free(const struct bucket_table *tbl)
  {
 -	struct bucket_table *new_tbl;
 +	kvfree(tbl);
 +}
  
 -	do {
 -		new_tbl = tbl;
 -		tbl = rht_dereference_rcu(tbl->future_tbl, ht);
 -	} while (tbl);
 +/**
 + * rht_grow_above_75 - returns true if nelems > 0.75 * table-size
 + * @ht:		hash table
 + * @new_size:	new table size
 + */
 +bool rht_grow_above_75(const struct rhashtable *ht, size_t new_size)
 +{
 +	/* Expand table when exceeding 75% load */
 +	return ht->nelems > (new_size / 4 * 3);
 +}
 +EXPORT_SYMBOL_GPL(rht_grow_above_75);
  
 -	return new_tbl;
 +/**
 + * rht_shrink_below_30 - returns true if nelems < 0.3 * table-size
 + * @ht:		hash table
 + * @new_size:	new table size
 + */
 +bool rht_shrink_below_30(const struct rhashtable *ht, size_t new_size)
 +{
 +	/* Shrink table beneath 30% load */
 +	return ht->nelems < (new_size * 3 / 10);
  }
 +EXPORT_SYMBOL_GPL(rht_shrink_below_30);
  
 -static int rhashtable_rehash_one(struct rhashtable *ht, unsigned int old_hash)
 +static void hashtable_chain_unzip(const struct rhashtable *ht,
 +				  const struct bucket_table *new_tbl,
 +				  struct bucket_table *old_tbl, size_t n)
  {
 -	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
 -	struct bucket_table *new_tbl = rhashtable_last_table(ht,
 -		rht_dereference_rcu(old_tbl->future_tbl, ht));
 -	struct rhash_head __rcu **pprev = &old_tbl->buckets[old_hash];
 -	int err = -ENOENT;
 -	struct rhash_head *head, *next, *entry;
 -	spinlock_t *new_bucket_lock;
 -	unsigned int new_hash;
 -
 -	rht_for_each(entry, old_tbl, old_hash) {
 -		err = 0;
 -		next = rht_dereference_bucket(entry->next, old_tbl, old_hash);
 -
 -		if (rht_is_a_nulls(next))
 +	struct rhash_head *he, *p, *next;
 +	unsigned int h;
 +
 +	/* Old bucket empty, no work needed. */
 +	p = rht_dereference(old_tbl->buckets[n], ht);
 +	if (!p)
 +		return;
 +
 +	/* Advance the old bucket pointer one or more times until it
 +	 * reaches a node that doesn't hash to the same bucket as the
 +	 * previous node p. Call the previous node p;
 +	 */
 +	h = head_hashfn(ht, new_tbl, p);
 +	rht_for_each_continue(he, p->next, old_tbl, n) {
 +		if (head_hashfn(ht, new_tbl, he) != h)
  			break;
 +		p = he;
 +	}
 +	RCU_INIT_POINTER(old_tbl->buckets[n], p->next);
  
 -		pprev = &entry->next;
 +	/* Find the subsequent node which does hash to the same
 +	 * bucket as node P, or NULL if no such node exists.
 +	 */
 +	next = NULL;
 +	if (he) {
 +		rht_for_each_continue(he, he->next, old_tbl, n) {
 +			if (head_hashfn(ht, new_tbl, he) == h) {
 +				next = he;
 +				break;
 +			}
 +		}
  	}
  
++<<<<<<< HEAD
 +	/* Set p's next pointer to that subsequent node pointer,
 +	 * bypassing the nodes which do not hash to p's bucket
++=======
+ 	if (err)
+ 		goto out;
+ 
+ 	new_hash = head_hashfn(ht, new_tbl, entry);
+ 
+ 	new_bucket_lock = rht_bucket_lock(new_tbl, new_hash);
+ 
+ 	spin_lock_nested(new_bucket_lock, SINGLE_DEPTH_NESTING);
+ 	head = rht_dereference_bucket(new_tbl->buckets[new_hash],
+ 				      new_tbl, new_hash);
+ 
+ 	RCU_INIT_POINTER(entry->next, head);
+ 
+ 	rcu_assign_pointer(new_tbl->buckets[new_hash], entry);
+ 	spin_unlock(new_bucket_lock);
+ 
+ 	rcu_assign_pointer(*pprev, next);
+ 
+ out:
+ 	return err;
+ }
+ 
+ static void rhashtable_rehash_chain(struct rhashtable *ht,
+ 				    unsigned int old_hash)
+ {
+ 	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
+ 	spinlock_t *old_bucket_lock;
+ 
+ 	old_bucket_lock = rht_bucket_lock(old_tbl, old_hash);
+ 
+ 	spin_lock_bh(old_bucket_lock);
+ 	while (!rhashtable_rehash_one(ht, old_hash))
+ 		;
+ 	old_tbl->rehash++;
+ 	spin_unlock_bh(old_bucket_lock);
+ }
+ 
+ static int rhashtable_rehash_attach(struct rhashtable *ht,
+ 				    struct bucket_table *old_tbl,
+ 				    struct bucket_table *new_tbl)
+ {
+ 	/* Protect future_tbl using the first bucket lock. */
+ 	spin_lock_bh(old_tbl->locks);
+ 
+ 	/* Did somebody beat us to it? */
+ 	if (rcu_access_pointer(old_tbl->future_tbl)) {
+ 		spin_unlock_bh(old_tbl->locks);
+ 		return -EEXIST;
+ 	}
+ 
+ 	/* Make insertions go into the new, empty table right away. Deletions
+ 	 * and lookups will be attempted in both tables until we synchronize.
++>>>>>>> 7def0f952ecc (lib: fix data race in rhashtable_rehash_one)
  	 */
 -	rcu_assign_pointer(old_tbl->future_tbl, new_tbl);
 -
 -	/* Ensure the new table is visible to readers. */
 -	smp_wmb();
 -
 -	spin_unlock_bh(old_tbl->locks);
 -
 -	return 0;
 -}
 -
 -static int rhashtable_rehash_table(struct rhashtable *ht)
 -{
 -	struct bucket_table *old_tbl = rht_dereference(ht->tbl, ht);
 -	struct bucket_table *new_tbl;
 -	struct rhashtable_walker *walker;
 -	unsigned int old_hash;
 -
 -	new_tbl = rht_dereference(old_tbl->future_tbl, ht);
 -	if (!new_tbl)
 -		return 0;
 -
 -	for (old_hash = 0; old_hash < old_tbl->size; old_hash++)
 -		rhashtable_rehash_chain(ht, old_hash);
 -
 -	/* Publish the new table pointer. */
 -	rcu_assign_pointer(ht->tbl, new_tbl);
 -
 -	spin_lock(&ht->lock);
 -	list_for_each_entry(walker, &old_tbl->walkers, list)
 -		walker->tbl = NULL;
 -	spin_unlock(&ht->lock);
 -
 -	/* Wait for readers. All new readers will see the new
 -	 * table, and thus no references to the old table will
 -	 * remain.
 -	 */
 -	call_rcu(&old_tbl->rcu, bucket_table_free_rcu);
 -
 -	return rht_dereference(new_tbl->future_tbl, ht) ? -EAGAIN : 0;
 +	RCU_INIT_POINTER(p->next, next);
  }
  
  /**
* Unmerged path lib/rhashtable.c
