slub: add missing kmem cgroup support to kmem_cache_free_bulk

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Jesper Dangaard Brouer <brouer@redhat.com>
commit 033745189b1bae3fc931beeaf48604ee7c259309
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/03374518.failed

Initial implementation missed support for kmem cgroup support in
kmem_cache_free_bulk() call, add this.

If CONFIG_MEMCG_KMEM is not enabled, the compiler should be smart enough
to not add any asm code.

Incoming bulk free objects can belong to different kmem cgroups, and
object free call can happen at a later point outside memcg context.  Thus,
we need to keep the orig kmem_cache, to correctly verify if a memcg object
match against its "root_cache" (s->memcg_params.root_cache).

	Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
	Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 033745189b1bae3fc931beeaf48604ee7c259309)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 9230da41a30e,34847044dfe5..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -2659,6 -2814,161 +2659,164 @@@ void kmem_cache_free(struct kmem_cache 
  }
  EXPORT_SYMBOL(kmem_cache_free);
  
++<<<<<<< HEAD
++=======
+ struct detached_freelist {
+ 	struct page *page;
+ 	void *tail;
+ 	void *freelist;
+ 	int cnt;
+ };
+ 
+ /*
+  * This function progressively scans the array with free objects (with
+  * a limited look ahead) and extract objects belonging to the same
+  * page.  It builds a detached freelist directly within the given
+  * page/objects.  This can happen without any need for
+  * synchronization, because the objects are owned by running process.
+  * The freelist is build up as a single linked list in the objects.
+  * The idea is, that this detached freelist can then be bulk
+  * transferred to the real freelist(s), but only requiring a single
+  * synchronization primitive.  Look ahead in the array is limited due
+  * to performance reasons.
+  */
+ static int build_detached_freelist(struct kmem_cache *s, size_t size,
+ 				   void **p, struct detached_freelist *df)
+ {
+ 	size_t first_skipped_index = 0;
+ 	int lookahead = 3;
+ 	void *object;
+ 
+ 	/* Always re-init detached_freelist */
+ 	df->page = NULL;
+ 
+ 	do {
+ 		object = p[--size];
+ 	} while (!object && size);
+ 
+ 	if (!object)
+ 		return 0;
+ 
+ 	/* Start new detached freelist */
+ 	set_freepointer(s, object, NULL);
+ 	df->page = virt_to_head_page(object);
+ 	df->tail = object;
+ 	df->freelist = object;
+ 	p[size] = NULL; /* mark object processed */
+ 	df->cnt = 1;
+ 
+ 	while (size) {
+ 		object = p[--size];
+ 		if (!object)
+ 			continue; /* Skip processed objects */
+ 
+ 		/* df->page is always set at this point */
+ 		if (df->page == virt_to_head_page(object)) {
+ 			/* Opportunity build freelist */
+ 			set_freepointer(s, object, df->freelist);
+ 			df->freelist = object;
+ 			df->cnt++;
+ 			p[size] = NULL; /* mark object processed */
+ 
+ 			continue;
+ 		}
+ 
+ 		/* Limit look ahead search */
+ 		if (!--lookahead)
+ 			break;
+ 
+ 		if (!first_skipped_index)
+ 			first_skipped_index = size + 1;
+ 	}
+ 
+ 	return first_skipped_index;
+ }
+ 
+ 
+ /* Note that interrupts must be enabled when calling this function. */
+ void kmem_cache_free_bulk(struct kmem_cache *orig_s, size_t size, void **p)
+ {
+ 	if (WARN_ON(!size))
+ 		return;
+ 
+ 	do {
+ 		struct detached_freelist df;
+ 		struct kmem_cache *s;
+ 
+ 		/* Support for memcg */
+ 		s = cache_from_obj(orig_s, p[size - 1]);
+ 
+ 		size = build_detached_freelist(s, size, p, &df);
+ 		if (unlikely(!df.page))
+ 			continue;
+ 
+ 		slab_free(s, df.page, df.freelist, df.tail, df.cnt, _RET_IP_);
+ 	} while (likely(size));
+ }
+ EXPORT_SYMBOL(kmem_cache_free_bulk);
+ 
+ /* Note that interrupts must be enabled when calling this function. */
+ bool kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
+ 			   void **p)
+ {
+ 	struct kmem_cache_cpu *c;
+ 	int i;
+ 
+ 	/* memcg and kmem_cache debug support */
+ 	s = slab_pre_alloc_hook(s, flags);
+ 	if (unlikely(!s))
+ 		return false;
+ 	/*
+ 	 * Drain objects in the per cpu slab, while disabling local
+ 	 * IRQs, which protects against PREEMPT and interrupts
+ 	 * handlers invoking normal fastpath.
+ 	 */
+ 	local_irq_disable();
+ 	c = this_cpu_ptr(s->cpu_slab);
+ 
+ 	for (i = 0; i < size; i++) {
+ 		void *object = c->freelist;
+ 
+ 		if (unlikely(!object)) {
+ 			/*
+ 			 * Invoking slow path likely have side-effect
+ 			 * of re-populating per CPU c->freelist
+ 			 */
+ 			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
+ 					    _RET_IP_, c);
+ 			if (unlikely(!p[i]))
+ 				goto error;
+ 
+ 			c = this_cpu_ptr(s->cpu_slab);
+ 			continue; /* goto for-loop */
+ 		}
+ 		c->freelist = get_freepointer(s, object);
+ 		p[i] = object;
+ 	}
+ 	c->tid = next_tid(c->tid);
+ 	local_irq_enable();
+ 
+ 	/* Clear memory outside IRQ disabled fastpath loop */
+ 	if (unlikely(flags & __GFP_ZERO)) {
+ 		int j;
+ 
+ 		for (j = 0; j < i; j++)
+ 			memset(p[j], 0, s->object_size);
+ 	}
+ 
+ 	/* memcg and kmem_cache debug support */
+ 	slab_post_alloc_hook(s, flags, size, p);
+ 	return true;
+ error:
+ 	local_irq_enable();
+ 	slab_post_alloc_hook(s, flags, i, p);
+ 	__kmem_cache_free_bulk(s, i, p);
+ 	return false;
+ }
+ EXPORT_SYMBOL(kmem_cache_alloc_bulk);
+ 
+ 
++>>>>>>> 033745189b1b (slub: add missing kmem cgroup support to kmem_cache_free_bulk)
  /*
   * Object placement in a slab is made very easy because we always start at
   * offset 0. If we tune the size of the object to the alignment then we can
* Unmerged path mm/slub.c
