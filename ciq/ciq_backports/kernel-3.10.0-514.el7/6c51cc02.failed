powercap/intel_rapl: Add support for Kabylake

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [powercap] intel_rapl: Add support for Kabylake (David Arcari) [1310935]
Rebuild_FUZZ: 88.89%
commit-author Jacob Pan <jacob.jun.pan@linux.intel.com>
commit 6c51cc0203de25aeaff9d0236d6c2b497be93e3b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/6c51cc02.failed

Kabylake is similar to Skylake in terms of RAPL.

	Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 6c51cc0203de25aeaff9d0236d6c2b497be93e3b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/powercap/intel_rapl.c
diff --cc drivers/powercap/intel_rapl.c
index 37cf01bfb059,470bb622fd8c..000000000000
--- a/drivers/powercap/intel_rapl.c
+++ b/drivers/powercap/intel_rapl.c
@@@ -943,25 -940,169 +943,188 @@@ static void package_power_limit_irq_res
  	else
  		l &= ~PACKAGE_THERM_INT_PLN_ENABLE;
  
 -	wrmsr_safe(MSR_IA32_PACKAGE_THERM_INTERRUPT, l, h);
 +	wrmsr_on_cpu(cpu, MSR_IA32_PACKAGE_THERM_INTERRUPT, l, h);
  }
  
++<<<<<<< HEAD
 +static const struct x86_cpu_id rapl_ids[] = {
 +	{ X86_VENDOR_INTEL, 6, 0x2a},/* Sandy Bridge */
 +	{ X86_VENDOR_INTEL, 6, 0x2d},/* Sandy Bridge EP */
 +	{ X86_VENDOR_INTEL, 6, 0x37},/* Valleyview */
 +	{ X86_VENDOR_INTEL, 6, 0x3a},/* Ivy Bridge */
 +	{ X86_VENDOR_INTEL, 6, 0x3c},/* Haswell */
 +	{ X86_VENDOR_INTEL, 6, 0x3d},/* Broadwell */
 +	{ X86_VENDOR_INTEL, 6, 0x3f},/* Haswell */
 +	{ X86_VENDOR_INTEL, 6, 0x4f},/* Broadwell servers */
 +	{ X86_VENDOR_INTEL, 6, 0x45},/* Haswell ULT */
 +	{ X86_VENDOR_INTEL, 6, 0x47},/* Broadwell-H */
 +	{ X86_VENDOR_INTEL, 6, 0x4E},/* Skylake */
 +	{ X86_VENDOR_INTEL, 6, 0x56},/* Future Xeon */
 +	{ X86_VENDOR_INTEL, 6, 0x57},/* Knights Landing */
 +	{ X86_VENDOR_INTEL, 6, 0x5E},/* Skylake-H/S */
 +	/* TODO: Add more CPU IDs after testing */
++=======
+ /* restore per package power limit interrupt enable state */
+ static void package_power_limit_irq_restore(struct rapl_package *rp)
+ {
+ 	if (!boot_cpu_has(X86_FEATURE_PTS) || !boot_cpu_has(X86_FEATURE_PLN))
+ 		return;
+ 
+ 	/* irq enable state not saved, nothing to restore */
+ 	if (!(rp->power_limit_irq & PACKAGE_PLN_INT_SAVED))
+ 		return;
+ 
+ 	smp_call_function_single(rp->lead_cpu, power_limit_irq_restore_cpu, rp, 1);
+ }
+ 
+ static void set_floor_freq_default(struct rapl_domain *rd, bool mode)
+ {
+ 	int nr_powerlimit = find_nr_power_limit(rd);
+ 
+ 	/* always enable clamp such that p-state can go below OS requested
+ 	 * range. power capping priority over guranteed frequency.
+ 	 */
+ 	rapl_write_data_raw(rd, PL1_CLAMP, mode);
+ 
+ 	/* some domains have pl2 */
+ 	if (nr_powerlimit > 1) {
+ 		rapl_write_data_raw(rd, PL2_ENABLE, mode);
+ 		rapl_write_data_raw(rd, PL2_CLAMP, mode);
+ 	}
+ }
+ 
+ static void set_floor_freq_atom(struct rapl_domain *rd, bool enable)
+ {
+ 	static u32 power_ctrl_orig_val;
+ 	u32 mdata;
+ 
+ 	if (!rapl_defaults->floor_freq_reg_addr) {
+ 		pr_err("Invalid floor frequency config register\n");
+ 		return;
+ 	}
+ 
+ 	if (!power_ctrl_orig_val)
+ 		iosf_mbi_read(BT_MBI_UNIT_PMC, MBI_CR_READ,
+ 			      rapl_defaults->floor_freq_reg_addr,
+ 			      &power_ctrl_orig_val);
+ 	mdata = power_ctrl_orig_val;
+ 	if (enable) {
+ 		mdata &= ~(0x7f << 8);
+ 		mdata |= 1 << 8;
+ 	}
+ 	iosf_mbi_write(BT_MBI_UNIT_PMC, MBI_CR_WRITE,
+ 		       rapl_defaults->floor_freq_reg_addr, mdata);
+ }
+ 
+ static u64 rapl_compute_time_window_core(struct rapl_package *rp, u64 value,
+ 					bool to_raw)
+ {
+ 	u64 f, y; /* fraction and exp. used for time unit */
+ 
+ 	/*
+ 	 * Special processing based on 2^Y*(1+F/4), refer
+ 	 * to Intel Software Developer's manual Vol.3B: CH 14.9.3.
+ 	 */
+ 	if (!to_raw) {
+ 		f = (value & 0x60) >> 5;
+ 		y = value & 0x1f;
+ 		value = (1 << y) * (4 + f) * rp->time_unit / 4;
+ 	} else {
+ 		do_div(value, rp->time_unit);
+ 		y = ilog2(value);
+ 		f = div64_u64(4 * (value - (1 << y)), 1 << y);
+ 		value = (y & 0x1f) | ((f & 0x3) << 5);
+ 	}
+ 	return value;
+ }
+ 
+ static u64 rapl_compute_time_window_atom(struct rapl_package *rp, u64 value,
+ 					bool to_raw)
+ {
+ 	/*
+ 	 * Atom time unit encoding is straight forward val * time_unit,
+ 	 * where time_unit is default to 1 sec. Never 0.
+ 	 */
+ 	if (!to_raw)
+ 		return (value) ? value *= rp->time_unit : rp->time_unit;
+ 	else
+ 		value = div64_u64(value, rp->time_unit);
+ 
+ 	return value;
+ }
+ 
+ static const struct rapl_defaults rapl_defaults_core = {
+ 	.floor_freq_reg_addr = 0,
+ 	.check_unit = rapl_check_unit_core,
+ 	.set_floor_freq = set_floor_freq_default,
+ 	.compute_time_window = rapl_compute_time_window_core,
+ };
+ 
+ static const struct rapl_defaults rapl_defaults_hsw_server = {
+ 	.check_unit = rapl_check_unit_core,
+ 	.set_floor_freq = set_floor_freq_default,
+ 	.compute_time_window = rapl_compute_time_window_core,
+ 	.dram_domain_energy_unit = 15300,
+ };
+ 
+ static const struct rapl_defaults rapl_defaults_byt = {
+ 	.floor_freq_reg_addr = IOSF_CPU_POWER_BUDGET_CTL_BYT,
+ 	.check_unit = rapl_check_unit_atom,
+ 	.set_floor_freq = set_floor_freq_atom,
+ 	.compute_time_window = rapl_compute_time_window_atom,
+ };
+ 
+ static const struct rapl_defaults rapl_defaults_tng = {
+ 	.floor_freq_reg_addr = IOSF_CPU_POWER_BUDGET_CTL_TNG,
+ 	.check_unit = rapl_check_unit_atom,
+ 	.set_floor_freq = set_floor_freq_atom,
+ 	.compute_time_window = rapl_compute_time_window_atom,
+ };
+ 
+ static const struct rapl_defaults rapl_defaults_ann = {
+ 	.floor_freq_reg_addr = 0,
+ 	.check_unit = rapl_check_unit_atom,
+ 	.set_floor_freq = NULL,
+ 	.compute_time_window = rapl_compute_time_window_atom,
+ };
+ 
+ static const struct rapl_defaults rapl_defaults_cht = {
+ 	.floor_freq_reg_addr = 0,
+ 	.check_unit = rapl_check_unit_atom,
+ 	.set_floor_freq = NULL,
+ 	.compute_time_window = rapl_compute_time_window_atom,
+ };
+ 
+ #define RAPL_CPU(_model, _ops) {			\
+ 		.vendor = X86_VENDOR_INTEL,		\
+ 		.family = 6,				\
+ 		.model = _model,			\
+ 		.driver_data = (kernel_ulong_t)&_ops,	\
+ 		}
+ 
+ static const struct x86_cpu_id rapl_ids[] __initconst = {
+ 	RAPL_CPU(0x2a, rapl_defaults_core),/* Sandy Bridge */
+ 	RAPL_CPU(0x2d, rapl_defaults_core),/* Sandy Bridge EP */
+ 	RAPL_CPU(0x37, rapl_defaults_byt),/* Valleyview */
+ 	RAPL_CPU(0x3a, rapl_defaults_core),/* Ivy Bridge */
+ 	RAPL_CPU(0x3c, rapl_defaults_core),/* Haswell */
+ 	RAPL_CPU(0x3d, rapl_defaults_core),/* Broadwell */
+ 	RAPL_CPU(0x3f, rapl_defaults_hsw_server),/* Haswell servers */
+ 	RAPL_CPU(0x4f, rapl_defaults_hsw_server),/* Broadwell servers */
+ 	RAPL_CPU(0x45, rapl_defaults_core),/* Haswell ULT */
+ 	RAPL_CPU(0x46, rapl_defaults_core),/* Haswell */
+ 	RAPL_CPU(0x47, rapl_defaults_core),/* Broadwell-H */
+ 	RAPL_CPU(0x4E, rapl_defaults_core),/* Skylake */
+ 	RAPL_CPU(0x4C, rapl_defaults_cht),/* Braswell/Cherryview */
+ 	RAPL_CPU(0x4A, rapl_defaults_tng),/* Tangier */
+ 	RAPL_CPU(0x56, rapl_defaults_core),/* Future Xeon */
+ 	RAPL_CPU(0x5A, rapl_defaults_ann),/* Annidale */
+ 	RAPL_CPU(0X5C, rapl_defaults_core),/* Broxton */
+ 	RAPL_CPU(0x5E, rapl_defaults_core),/* Skylake-H/S */
+ 	RAPL_CPU(0x57, rapl_defaults_hsw_server),/* Knights Landing */
+ 	RAPL_CPU(0x8E, rapl_defaults_core),/* Kabylake */
+ 	RAPL_CPU(0x9E, rapl_defaults_core),/* Kabylake */
++>>>>>>> 6c51cc0203de (powercap/intel_rapl: Add support for Kabylake)
  	{}
  };
  MODULE_DEVICE_TABLE(x86cpu, rapl_ids);
* Unmerged path drivers/powercap/intel_rapl.c
