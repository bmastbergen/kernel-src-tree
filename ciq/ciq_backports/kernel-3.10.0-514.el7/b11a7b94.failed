mm: exclude ZONE_DEVICE from GFP_ZONE_TABLE

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] exclude ZONE_DEVICE from GFP_ZONE_TABLE (Jeff Moyer) [1271957 1278621 1343736 1349587 1349702 1353614 1355792]
Rebuild_FUZZ: 95.12%
commit-author Dan Williams <dan.j.williams@intel.com>
commit b11a7b94100cba5ec926a181894c2897a22651b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/b11a7b94.failed

ZONE_DEVICE (merged in 4.3) and ZONE_CMA (proposed) are examples of new
mm zones that are bumping up against the current maximum limit of 4
zones, i.e.  2 bits in page->flags for the GFP_ZONE_TABLE.

The GFP_ZONE_TABLE poses an interesting constraint since
include/linux/gfp.h gets included by the 32-bit portion of a 64-bit
build.  We need to be careful to only build the table for zones that
have a corresponding gfp_t flag.  GFP_ZONES_SHIFT is introduced for this
purpose.  This patch does not attempt to solve the problem of adding a
new zone that also has a corresponding GFP_ flag.

Vlastimil points out that ZONE_DEVICE, by depending on x86_64 and
SPARSEMEM_VMEMMAP implies that SECTIONS_WIDTH is zero.  In other words
even though ZONE_DEVICE does not fit in GFP_ZONE_TABLE it is free to
consume another bit in page->flags (expand ZONES_WIDTH) with room to
spare.

Link: https://bugzilla.kernel.org/show_bug.cgi?id=110931
Fixes: 033fbae988fc ("mm: ZONE_DEVICE for "device memory"")
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Reported-by: Mark <markk@clara.co.uk>
	Reported-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b11a7b94100cba5ec926a181894c2897a22651b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/Kconfig
diff --cc mm/Kconfig
index 88ac087b6ee9,520dae6e6ed0..000000000000
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@@ -572,3 -589,80 +572,83 @@@ config PGTABLE_MAPPIN
  
  	  You can check speed with zsmalloc benchmark:
  	  https://github.com/spartacus06/zsmapbench
++<<<<<<< HEAD
++=======
+ 
+ config ZSMALLOC_STAT
+ 	bool "Export zsmalloc statistics"
+ 	depends on ZSMALLOC
+ 	select DEBUG_FS
+ 	help
+ 	  This option enables code in the zsmalloc to collect various
+ 	  statistics about whats happening in zsmalloc and exports that
+ 	  information to userspace via debugfs.
+ 	  If unsure, say N.
+ 
+ config GENERIC_EARLY_IOREMAP
+ 	bool
+ 
+ config MAX_STACK_SIZE_MB
+ 	int "Maximum user stack size for 32-bit processes (MB)"
+ 	default 80
+ 	range 8 256 if METAG
+ 	range 8 2048
+ 	depends on STACK_GROWSUP && (!64BIT || COMPAT)
+ 	help
+ 	  This is the maximum stack size in Megabytes in the VM layout of 32-bit
+ 	  user processes when the stack grows upwards (currently only on parisc
+ 	  and metag arch). The stack will be located at the highest memory
+ 	  address minus the given value, unless the RLIMIT_STACK hard limit is
+ 	  changed to a smaller value in which case that is used.
+ 
+ 	  A sane initial value is 80 MB.
+ 
+ # For architectures that support deferred memory initialisation
+ config ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	bool
+ 
+ config DEFERRED_STRUCT_PAGE_INIT
+ 	bool "Defer initialisation of struct pages to kthreads"
+ 	default n
+ 	depends on ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	depends on MEMORY_HOTPLUG
+ 	help
+ 	  Ordinarily all struct pages are initialised during early boot in a
+ 	  single thread. On very large machines this can take a considerable
+ 	  amount of time. If this option is set, large machines will bring up
+ 	  a subset of memmap at boot and then initialise the rest in parallel
+ 	  by starting one-off "pgdatinitX" kernel thread for each node X. This
+ 	  has a potential performance impact on processes running early in the
+ 	  lifetime of the system until these kthreads finish the
+ 	  initialisation.
+ 
+ config IDLE_PAGE_TRACKING
+ 	bool "Enable idle page tracking"
+ 	depends on SYSFS && MMU
+ 	select PAGE_EXTENSION if !64BIT
+ 	help
+ 	  This feature allows to estimate the amount of user pages that have
+ 	  not been touched during a given period of time. This information can
+ 	  be useful to tune memory cgroup limits and/or for job placement
+ 	  within a compute cluster.
+ 
+ 	  See Documentation/vm/idle_page_tracking.txt for more details.
+ 
+ config ZONE_DEVICE
+ 	bool "Device memory (pmem, etc...) hotplug support" if EXPERT
+ 	depends on MEMORY_HOTPLUG
+ 	depends on MEMORY_HOTREMOVE
+ 	depends on X86_64 #arch_add_memory() comprehends device memory
+ 
+ 	help
+ 	  Device memory hotplug support allows for establishing pmem,
+ 	  or other device driver discovered memory regions, in the
+ 	  memmap. This allows pfn_to_page() lookups of otherwise
+ 	  "device-physical" addresses which is needed for using a DAX
+ 	  mapping in an O_DIRECT operation, among other things.
+ 
+ 	  If FS_DAX is enabled, then say Y.
+ 
+ config FRAME_VECTOR
+ 	bool
++>>>>>>> b11a7b94100c (mm: exclude ZONE_DEVICE from GFP_ZONE_TABLE)
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index 07dd33b21a0b..c4e73670ef2f 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -219,22 +219,29 @@ static inline int allocflags_to_migratetype(gfp_t gfp_flags)
  *       0xe    => BAD (MOVABLE+DMA32+HIGHMEM)
  *       0xf    => BAD (MOVABLE+DMA32+HIGHMEM+DMA)
  *
- * ZONES_SHIFT must be <= 2 on 32 bit platforms.
+ * GFP_ZONES_SHIFT must be <= 2 on 32 bit platforms.
  */
 
-#if 16 * ZONES_SHIFT > BITS_PER_LONG
-#error ZONES_SHIFT too large to create GFP_ZONE_TABLE integer
+#if defined(CONFIG_ZONE_DEVICE) && (MAX_NR_ZONES-1) <= 4
+/* ZONE_DEVICE is not a valid GFP zone specifier */
+#define GFP_ZONES_SHIFT 2
+#else
+#define GFP_ZONES_SHIFT ZONES_SHIFT
+#endif
+
+#if 16 * GFP_ZONES_SHIFT > BITS_PER_LONG
+#error GFP_ZONES_SHIFT too large to create GFP_ZONE_TABLE integer
 #endif
 
 #define GFP_ZONE_TABLE ( \
-	(ZONE_NORMAL << 0 * ZONES_SHIFT)				      \
-	| (OPT_ZONE_DMA << ___GFP_DMA * ZONES_SHIFT)			      \
-	| (OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * ZONES_SHIFT)		      \
-	| (OPT_ZONE_DMA32 << ___GFP_DMA32 * ZONES_SHIFT)		      \
-	| (ZONE_NORMAL << ___GFP_MOVABLE * ZONES_SHIFT)			      \
-	| (OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * ZONES_SHIFT)	      \
-	| (ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * ZONES_SHIFT)   \
-	| (OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * ZONES_SHIFT)   \
+	(ZONE_NORMAL << 0 * GFP_ZONES_SHIFT)				       \
+	| (OPT_ZONE_DMA << ___GFP_DMA * GFP_ZONES_SHIFT)		       \
+	| (OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * GFP_ZONES_SHIFT)	       \
+	| (OPT_ZONE_DMA32 << ___GFP_DMA32 * GFP_ZONES_SHIFT)		       \
+	| (ZONE_NORMAL << ___GFP_MOVABLE * GFP_ZONES_SHIFT)		       \
+	| (OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * GFP_ZONES_SHIFT)    \
+	| (ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * GFP_ZONES_SHIFT)\
+	| (OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * GFP_ZONES_SHIFT)\
 )
 
 /*
@@ -259,8 +266,8 @@ static inline enum zone_type gfp_zone(gfp_t flags)
 	enum zone_type z;
 	int bit = (__force int) (flags & GFP_ZONEMASK);
 
-	z = (GFP_ZONE_TABLE >> (bit * ZONES_SHIFT)) &
-					 ((1 << ZONES_SHIFT) - 1);
+	z = (GFP_ZONE_TABLE >> (bit * GFP_ZONES_SHIFT)) &
+					 ((1 << GFP_ZONES_SHIFT) - 1);
 	VM_BUG_ON((GFP_ZONE_BAD >> bit) & 1);
 	return z;
 }
diff --git a/include/linux/page-flags-layout.h b/include/linux/page-flags-layout.h
index da523661500a..77b078c103b2 100644
--- a/include/linux/page-flags-layout.h
+++ b/include/linux/page-flags-layout.h
@@ -17,6 +17,8 @@
 #define ZONES_SHIFT 1
 #elif MAX_NR_ZONES <= 4
 #define ZONES_SHIFT 2
+#elif MAX_NR_ZONES <= 8
+#define ZONES_SHIFT 3
 #else
 #error ZONES_SHIFT -- too many zones configured adjust calculation
 #endif
* Unmerged path mm/Kconfig
