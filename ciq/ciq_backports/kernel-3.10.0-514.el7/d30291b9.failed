libceph: variable-sized ceph_object_id

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Ilya Dryomov <idryomov@gmail.com>
commit d30291b985d1854565d7f2c82a4457869d5265e8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/d30291b9.failed

Currently ceph_object_id can hold object names of up to 100
(CEPH_MAX_OID_NAME_LEN) characters.  This is enough for all use cases,
expect one - long rbd image names:

- a format 1 header is named "<imgname>.rbd"
- an object that points to a format 2 header is named "rbd_id.<imgname>"

We operate on these potentially long-named objects during rbd map, and,
for format 1 images, during header refresh.  (A format 2 header name is
a small system-generated string.)

Lift this 100 character limit by making ceph_object_id be able to point
to an externally-allocated string.  Apart from being able to work with
almost arbitrarily-long named objects, this allows us to reduce the
size of ceph_object_id from >100 bytes to 64 bytes.

	Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
(cherry picked from commit d30291b985d1854565d7f2c82a4457869d5265e8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/ceph/addr.c
#	fs/ceph/file.c
#	net/ceph/osd_client.c
diff --cc fs/ceph/addr.c
index 5265e35034fa,6f28dd9bacb2..000000000000
--- a/fs/ceph/addr.c
+++ b/fs/ceph/addr.c
@@@ -1670,3 -1694,212 +1670,215 @@@ int ceph_mmap(struct file *file, struc
  	vma->vm_ops = &ceph_vmops;
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ enum {
+ 	POOL_READ	= 1,
+ 	POOL_WRITE	= 2,
+ };
+ 
+ static int __ceph_pool_perm_get(struct ceph_inode_info *ci, u32 pool)
+ {
+ 	struct ceph_fs_client *fsc = ceph_inode_to_client(&ci->vfs_inode);
+ 	struct ceph_mds_client *mdsc = fsc->mdsc;
+ 	struct ceph_osd_request *rd_req = NULL, *wr_req = NULL;
+ 	struct rb_node **p, *parent;
+ 	struct ceph_pool_perm *perm;
+ 	struct page **pages;
+ 	int err = 0, err2 = 0, have = 0;
+ 
+ 	down_read(&mdsc->pool_perm_rwsem);
+ 	p = &mdsc->pool_perm_tree.rb_node;
+ 	while (*p) {
+ 		perm = rb_entry(*p, struct ceph_pool_perm, node);
+ 		if (pool < perm->pool)
+ 			p = &(*p)->rb_left;
+ 		else if (pool > perm->pool)
+ 			p = &(*p)->rb_right;
+ 		else {
+ 			have = perm->perm;
+ 			break;
+ 		}
+ 	}
+ 	up_read(&mdsc->pool_perm_rwsem);
+ 	if (*p)
+ 		goto out;
+ 
+ 	dout("__ceph_pool_perm_get pool %u no perm cached\n", pool);
+ 
+ 	down_write(&mdsc->pool_perm_rwsem);
+ 	parent = NULL;
+ 	while (*p) {
+ 		parent = *p;
+ 		perm = rb_entry(parent, struct ceph_pool_perm, node);
+ 		if (pool < perm->pool)
+ 			p = &(*p)->rb_left;
+ 		else if (pool > perm->pool)
+ 			p = &(*p)->rb_right;
+ 		else {
+ 			have = perm->perm;
+ 			break;
+ 		}
+ 	}
+ 	if (*p) {
+ 		up_write(&mdsc->pool_perm_rwsem);
+ 		goto out;
+ 	}
+ 
+ 	rd_req = ceph_osdc_alloc_request(&fsc->client->osdc, NULL,
+ 					 1, false, GFP_NOFS);
+ 	if (!rd_req) {
+ 		err = -ENOMEM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	rd_req->r_flags = CEPH_OSD_FLAG_READ;
+ 	osd_req_op_init(rd_req, 0, CEPH_OSD_OP_STAT, 0);
+ 	rd_req->r_base_oloc.pool = pool;
+ 	ceph_oid_printf(&rd_req->r_base_oid, "%llx.00000000", ci->i_vino.ino);
+ 
+ 	err = ceph_osdc_alloc_messages(rd_req, GFP_NOFS);
+ 	if (err)
+ 		goto out_unlock;
+ 
+ 	wr_req = ceph_osdc_alloc_request(&fsc->client->osdc, NULL,
+ 					 1, false, GFP_NOFS);
+ 	if (!wr_req) {
+ 		err = -ENOMEM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	wr_req->r_flags = CEPH_OSD_FLAG_WRITE |
+ 			  CEPH_OSD_FLAG_ACK | CEPH_OSD_FLAG_ONDISK;
+ 	osd_req_op_init(wr_req, 0, CEPH_OSD_OP_CREATE, CEPH_OSD_OP_FLAG_EXCL);
+ 	wr_req->r_base_oloc.pool = pool;
+ 	ceph_oid_copy(&wr_req->r_base_oid, &rd_req->r_base_oid);
+ 
+ 	err = ceph_osdc_alloc_messages(wr_req, GFP_NOFS);
+ 	if (err)
+ 		goto out_unlock;
+ 
+ 	/* one page should be large enough for STAT data */
+ 	pages = ceph_alloc_page_vector(1, GFP_KERNEL);
+ 	if (IS_ERR(pages)) {
+ 		err = PTR_ERR(pages);
+ 		goto out_unlock;
+ 	}
+ 
+ 	osd_req_op_raw_data_in_pages(rd_req, 0, pages, PAGE_SIZE,
+ 				     0, false, true);
+ 	ceph_osdc_build_request(rd_req, 0, NULL, CEPH_NOSNAP,
+ 				&ci->vfs_inode.i_mtime);
+ 	err = ceph_osdc_start_request(&fsc->client->osdc, rd_req, false);
+ 
+ 	ceph_osdc_build_request(wr_req, 0, NULL, CEPH_NOSNAP,
+ 				&ci->vfs_inode.i_mtime);
+ 	err2 = ceph_osdc_start_request(&fsc->client->osdc, wr_req, false);
+ 
+ 	if (!err)
+ 		err = ceph_osdc_wait_request(&fsc->client->osdc, rd_req);
+ 	if (!err2)
+ 		err2 = ceph_osdc_wait_request(&fsc->client->osdc, wr_req);
+ 
+ 	if (err >= 0 || err == -ENOENT)
+ 		have |= POOL_READ;
+ 	else if (err != -EPERM)
+ 		goto out_unlock;
+ 
+ 	if (err2 == 0 || err2 == -EEXIST)
+ 		have |= POOL_WRITE;
+ 	else if (err2 != -EPERM) {
+ 		err = err2;
+ 		goto out_unlock;
+ 	}
+ 
+ 	perm = kmalloc(sizeof(*perm), GFP_NOFS);
+ 	if (!perm) {
+ 		err = -ENOMEM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	perm->pool = pool;
+ 	perm->perm = have;
+ 	rb_link_node(&perm->node, parent, p);
+ 	rb_insert_color(&perm->node, &mdsc->pool_perm_tree);
+ 	err = 0;
+ out_unlock:
+ 	up_write(&mdsc->pool_perm_rwsem);
+ 
+ 	ceph_osdc_put_request(rd_req);
+ 	ceph_osdc_put_request(wr_req);
+ out:
+ 	if (!err)
+ 		err = have;
+ 	dout("__ceph_pool_perm_get pool %u result = %d\n", pool, err);
+ 	return err;
+ }
+ 
+ int ceph_pool_perm_check(struct ceph_inode_info *ci, int need)
+ {
+ 	u32 pool;
+ 	int ret, flags;
+ 
+ 	/* does not support pool namespace yet */
+ 	if (ci->i_pool_ns_len)
+ 		return -EIO;
+ 
+ 	if (ceph_test_mount_opt(ceph_inode_to_client(&ci->vfs_inode),
+ 				NOPOOLPERM))
+ 		return 0;
+ 
+ 	spin_lock(&ci->i_ceph_lock);
+ 	flags = ci->i_ceph_flags;
+ 	pool = ceph_file_layout_pg_pool(ci->i_layout);
+ 	spin_unlock(&ci->i_ceph_lock);
+ check:
+ 	if (flags & CEPH_I_POOL_PERM) {
+ 		if ((need & CEPH_CAP_FILE_RD) && !(flags & CEPH_I_POOL_RD)) {
+ 			dout("ceph_pool_perm_check pool %u no read perm\n",
+ 			     pool);
+ 			return -EPERM;
+ 		}
+ 		if ((need & CEPH_CAP_FILE_WR) && !(flags & CEPH_I_POOL_WR)) {
+ 			dout("ceph_pool_perm_check pool %u no write perm\n",
+ 			     pool);
+ 			return -EPERM;
+ 		}
+ 		return 0;
+ 	}
+ 
+ 	ret = __ceph_pool_perm_get(ci, pool);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	flags = CEPH_I_POOL_PERM;
+ 	if (ret & POOL_READ)
+ 		flags |= CEPH_I_POOL_RD;
+ 	if (ret & POOL_WRITE)
+ 		flags |= CEPH_I_POOL_WR;
+ 
+ 	spin_lock(&ci->i_ceph_lock);
+ 	if (pool == ceph_file_layout_pg_pool(ci->i_layout)) {
+ 		ci->i_ceph_flags = flags;
+         } else {
+ 		pool = ceph_file_layout_pg_pool(ci->i_layout);
+ 		flags = ci->i_ceph_flags;
+ 	}
+ 	spin_unlock(&ci->i_ceph_lock);
+ 	goto check;
+ }
+ 
+ void ceph_pool_perm_destroy(struct ceph_mds_client *mdsc)
+ {
+ 	struct ceph_pool_perm *perm;
+ 	struct rb_node *n;
+ 
+ 	while (!RB_EMPTY_ROOT(&mdsc->pool_perm_tree)) {
+ 		n = rb_first(&mdsc->pool_perm_tree);
+ 		perm = rb_entry(n, struct ceph_pool_perm, node);
+ 		rb_erase(n, &mdsc->pool_perm_tree);
+ 		kfree(perm);
+ 	}
+ }
++>>>>>>> d30291b985d1 (libceph: variable-sized ceph_object_id)
diff --cc fs/ceph/file.c
index 0c3070bb755c,9d470397e249..000000000000
--- a/fs/ceph/file.c
+++ b/fs/ceph/file.c
@@@ -491,6 -554,199 +491,202 @@@ static ssize_t ceph_sync_read(struct ki
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ struct ceph_aio_request {
+ 	struct kiocb *iocb;
+ 	size_t total_len;
+ 	int write;
+ 	int error;
+ 	struct list_head osd_reqs;
+ 	unsigned num_reqs;
+ 	atomic_t pending_reqs;
+ 	struct timespec mtime;
+ 	struct ceph_cap_flush *prealloc_cf;
+ };
+ 
+ struct ceph_aio_work {
+ 	struct work_struct work;
+ 	struct ceph_osd_request *req;
+ };
+ 
+ static void ceph_aio_retry_work(struct work_struct *work);
+ 
+ static void ceph_aio_complete(struct inode *inode,
+ 			      struct ceph_aio_request *aio_req)
+ {
+ 	struct ceph_inode_info *ci = ceph_inode(inode);
+ 	int ret;
+ 
+ 	if (!atomic_dec_and_test(&aio_req->pending_reqs))
+ 		return;
+ 
+ 	ret = aio_req->error;
+ 	if (!ret)
+ 		ret = aio_req->total_len;
+ 
+ 	dout("ceph_aio_complete %p rc %d\n", inode, ret);
+ 
+ 	if (ret >= 0 && aio_req->write) {
+ 		int dirty;
+ 
+ 		loff_t endoff = aio_req->iocb->ki_pos + aio_req->total_len;
+ 		if (endoff > i_size_read(inode)) {
+ 			if (ceph_inode_set_size(inode, endoff))
+ 				ceph_check_caps(ci, CHECK_CAPS_AUTHONLY, NULL);
+ 		}
+ 
+ 		spin_lock(&ci->i_ceph_lock);
+ 		ci->i_inline_version = CEPH_INLINE_NONE;
+ 		dirty = __ceph_mark_dirty_caps(ci, CEPH_CAP_FILE_WR,
+ 					       &aio_req->prealloc_cf);
+ 		spin_unlock(&ci->i_ceph_lock);
+ 		if (dirty)
+ 			__mark_inode_dirty(inode, dirty);
+ 
+ 	}
+ 
+ 	ceph_put_cap_refs(ci, (aio_req->write ? CEPH_CAP_FILE_WR :
+ 						CEPH_CAP_FILE_RD));
+ 
+ 	aio_req->iocb->ki_complete(aio_req->iocb, ret, 0);
+ 
+ 	ceph_free_cap_flush(aio_req->prealloc_cf);
+ 	kfree(aio_req);
+ }
+ 
+ static void ceph_aio_complete_req(struct ceph_osd_request *req,
+ 				  struct ceph_msg *msg)
+ {
+ 	int rc = req->r_result;
+ 	struct inode *inode = req->r_inode;
+ 	struct ceph_aio_request *aio_req = req->r_priv;
+ 	struct ceph_osd_data *osd_data = osd_req_op_extent_osd_data(req, 0);
+ 	int num_pages = calc_pages_for((u64)osd_data->alignment,
+ 				       osd_data->length);
+ 
+ 	dout("ceph_aio_complete_req %p rc %d bytes %llu\n",
+ 	     inode, rc, osd_data->length);
+ 
+ 	if (rc == -EOLDSNAPC) {
+ 		struct ceph_aio_work *aio_work;
+ 		BUG_ON(!aio_req->write);
+ 
+ 		aio_work = kmalloc(sizeof(*aio_work), GFP_NOFS);
+ 		if (aio_work) {
+ 			INIT_WORK(&aio_work->work, ceph_aio_retry_work);
+ 			aio_work->req = req;
+ 			queue_work(ceph_inode_to_client(inode)->wb_wq,
+ 				   &aio_work->work);
+ 			return;
+ 		}
+ 		rc = -ENOMEM;
+ 	} else if (!aio_req->write) {
+ 		if (rc == -ENOENT)
+ 			rc = 0;
+ 		if (rc >= 0 && osd_data->length > rc) {
+ 			int zoff = osd_data->alignment + rc;
+ 			int zlen = osd_data->length - rc;
+ 			/*
+ 			 * If read is satisfied by single OSD request,
+ 			 * it can pass EOF. Otherwise read is within
+ 			 * i_size.
+ 			 */
+ 			if (aio_req->num_reqs == 1) {
+ 				loff_t i_size = i_size_read(inode);
+ 				loff_t endoff = aio_req->iocb->ki_pos + rc;
+ 				if (endoff < i_size)
+ 					zlen = min_t(size_t, zlen,
+ 						     i_size - endoff);
+ 				aio_req->total_len = rc + zlen;
+ 			}
+ 
+ 			if (zlen > 0)
+ 				ceph_zero_page_vector_range(zoff, zlen,
+ 							    osd_data->pages);
+ 		}
+ 	}
+ 
+ 	ceph_put_page_vector(osd_data->pages, num_pages, false);
+ 	ceph_osdc_put_request(req);
+ 
+ 	if (rc < 0)
+ 		cmpxchg(&aio_req->error, 0, rc);
+ 
+ 	ceph_aio_complete(inode, aio_req);
+ 	return;
+ }
+ 
+ static void ceph_aio_retry_work(struct work_struct *work)
+ {
+ 	struct ceph_aio_work *aio_work =
+ 		container_of(work, struct ceph_aio_work, work);
+ 	struct ceph_osd_request *orig_req = aio_work->req;
+ 	struct ceph_aio_request *aio_req = orig_req->r_priv;
+ 	struct inode *inode = orig_req->r_inode;
+ 	struct ceph_inode_info *ci = ceph_inode(inode);
+ 	struct ceph_snap_context *snapc;
+ 	struct ceph_osd_request *req;
+ 	int ret;
+ 
+ 	spin_lock(&ci->i_ceph_lock);
+ 	if (__ceph_have_pending_cap_snap(ci)) {
+ 		struct ceph_cap_snap *capsnap =
+ 			list_last_entry(&ci->i_cap_snaps,
+ 					struct ceph_cap_snap,
+ 					ci_item);
+ 		snapc = ceph_get_snap_context(capsnap->context);
+ 	} else {
+ 		BUG_ON(!ci->i_head_snapc);
+ 		snapc = ceph_get_snap_context(ci->i_head_snapc);
+ 	}
+ 	spin_unlock(&ci->i_ceph_lock);
+ 
+ 	req = ceph_osdc_alloc_request(orig_req->r_osdc, snapc, 2,
+ 			false, GFP_NOFS);
+ 	if (!req) {
+ 		ret = -ENOMEM;
+ 		req = orig_req;
+ 		goto out;
+ 	}
+ 
+ 	req->r_flags =	CEPH_OSD_FLAG_ORDERSNAP |
+ 			CEPH_OSD_FLAG_ONDISK |
+ 			CEPH_OSD_FLAG_WRITE;
+ 	req->r_base_oloc = orig_req->r_base_oloc;
+ 	ceph_oid_copy(&req->r_base_oid, &orig_req->r_base_oid);
+ 
+ 	ret = ceph_osdc_alloc_messages(req, GFP_NOFS);
+ 	if (ret) {
+ 		ceph_osdc_put_request(req);
+ 		req = orig_req;
+ 		goto out;
+ 	}
+ 
+ 	req->r_ops[0] = orig_req->r_ops[0];
+ 	osd_req_op_init(req, 1, CEPH_OSD_OP_STARTSYNC, 0);
+ 
+ 	ceph_osdc_build_request(req, req->r_ops[0].extent.offset,
+ 				snapc, CEPH_NOSNAP, &aio_req->mtime);
+ 
+ 	ceph_osdc_put_request(orig_req);
+ 
+ 	req->r_callback = ceph_aio_complete_req;
+ 	req->r_inode = inode;
+ 	req->r_priv = aio_req;
+ 
+ 	ret = ceph_osdc_start_request(req->r_osdc, req, false);
+ out:
+ 	if (ret < 0) {
+ 		req->r_result = ret;
+ 		ceph_aio_complete_req(req, NULL);
+ 	}
+ 
+ 	ceph_put_snap_context(snapc);
+ 	kfree(aio_work);
+ }
+ 
++>>>>>>> d30291b985d1 (libceph: variable-sized ceph_object_id)
  /*
   * Write commit request unsafe callback, called to tell us when a
   * request is unsafe (that is, in flight--has been handed to the
diff --cc net/ceph/osd_client.c
index f75717edb624,95910aed8e2e..000000000000
--- a/net/ceph/osd_client.c
+++ b/net/ceph/osd_client.c
@@@ -402,28 -404,25 +405,36 @@@ struct ceph_osd_request *ceph_osdc_allo
  	INIT_LIST_HEAD(&req->r_req_lru_item);
  	INIT_LIST_HEAD(&req->r_osd_item);
  
+ 	ceph_oid_init(&req->r_base_oid);
  	req->r_base_oloc.pool = -1;
+ 	ceph_oid_init(&req->r_target_oid);
  	req->r_target_oloc.pool = -1;
  
 -	dout("%s req %p\n", __func__, req);
 -	return req;
 -}
 -EXPORT_SYMBOL(ceph_osdc_alloc_request);
 +	msg_size = OSD_OPREPLY_FRONT_LEN;
 +	if (num_ops > CEPH_OSD_SLAB_OPS) {
 +		/* ceph_osd_op and rval */
 +		msg_size += (num_ops - CEPH_OSD_SLAB_OPS) *
 +			    (sizeof(struct ceph_osd_op) + 4);
 +	}
  
 -int ceph_osdc_alloc_messages(struct ceph_osd_request *req, gfp_t gfp)
 -{
 -	struct ceph_osd_client *osdc = req->r_osdc;
 -	struct ceph_msg *msg;
 -	int msg_size;
 +	/* create reply message */
 +	if (use_mempool)
 +		msg = ceph_msgpool_get(&osdc->msgpool_op_reply, 0);
 +	else
 +		msg = ceph_msg_new(CEPH_MSG_OSD_OPREPLY, msg_size,
 +				   gfp_flags, true);
 +	if (!msg) {
 +		ceph_osdc_put_request(req);
 +		return NULL;
 +	}
 +	req->r_reply = msg;
  
++<<<<<<< HEAD
++=======
+ 	WARN_ON(ceph_oid_empty(&req->r_base_oid));
+ 
+ 	/* create request message */
++>>>>>>> d30291b985d1 (libceph: variable-sized ceph_object_id)
  	msg_size = 4 + 4 + 4; /* client_inc, osdmap_epoch, flags */
  	msg_size += 4 + 4 + 4 + 8; /* mtime, reassert_version */
  	msg_size += 2 + 4 + 8 + 4 + 4; /* oloc */
@@@ -857,12 -866,17 +868,9 @@@ struct ceph_osd_request *ceph_osdc_new_
  	}
  
  	req->r_base_oloc.pool = ceph_file_layout_pg_pool(*layout);
- 
- 	snprintf(req->r_base_oid.name, sizeof(req->r_base_oid.name),
- 		 "%llx.%08llx", vino.ino, objnum);
- 	req->r_base_oid.name_len = strlen(req->r_base_oid.name);
+ 	ceph_oid_printf(&req->r_base_oid, "%llx.%08llx", vino.ino, objnum);
  
 -	r = ceph_osdc_alloc_messages(req, GFP_NOFS);
 -	if (r)
 -		goto fail;
 -
  	return req;
 -
 -fail:
 -	ceph_osdc_put_request(req);
 -	return ERR_PTR(r);
  }
  EXPORT_SYMBOL(ceph_osdc_new_request);
  
diff --git a/drivers/block/rbd.c b/drivers/block/rbd.c
index 37889e614644..9fae3d2a73c4 100644
--- a/drivers/block/rbd.c
+++ b/drivers/block/rbd.c
@@ -2009,7 +2009,9 @@ static struct ceph_osd_request *rbd_osd_req_create(
 	osd_req->r_priv = obj_request;
 
 	osd_req->r_base_oloc.pool = ceph_file_layout_pg_pool(rbd_dev->layout);
-	ceph_oid_set_name(&osd_req->r_base_oid, obj_request->object_name);
+	if (ceph_oid_aprintf(&osd_req->r_base_oid, GFP_NOIO, "%s",
+			     obj_request->object_name))
+		goto fail;
 
 	return osd_req;
 }
@@ -2054,7 +2056,9 @@ rbd_osd_req_create_copyup(struct rbd_obj_request *obj_request)
 	osd_req->r_priv = obj_request;
 
 	osd_req->r_base_oloc.pool = ceph_file_layout_pg_pool(rbd_dev->layout);
-	ceph_oid_set_name(&osd_req->r_base_oid, obj_request->object_name);
+	if (ceph_oid_aprintf(&osd_req->r_base_oid, GFP_NOIO, "%s",
+			     obj_request->object_name))
+		goto fail;
 
 	return osd_req;
 }
* Unmerged path fs/ceph/addr.c
* Unmerged path fs/ceph/file.c
diff --git a/fs/ceph/ioctl.c b/fs/ceph/ioctl.c
index f851d8d70158..db296709784a 100644
--- a/fs/ceph/ioctl.c
+++ b/fs/ceph/ioctl.c
@@ -213,7 +213,7 @@ static long ceph_ioctl_get_dataloc(struct file *file, void __user *arg)
 		 ceph_ino(inode), dl.object_no);
 
 	oloc.pool = ceph_file_layout_pg_pool(ci->i_layout);
-	ceph_oid_set_name(&oid, dl.object_name);
+	ceph_oid_printf(&oid, "%s", dl.object_name);
 
 	r = ceph_oloc_oid_to_pg(osdc->osdmap, &oloc, &oid, &pgid);
 	if (r < 0) {
diff --git a/include/linux/ceph/osdmap.h b/include/linux/ceph/osdmap.h
index e55c08bc3a96..777a29412706 100644
--- a/include/linux/ceph/osdmap.h
+++ b/include/linux/ceph/osdmap.h
@@ -64,11 +64,47 @@ struct ceph_object_locator {
  */
 #define CEPH_MAX_OID_NAME_LEN 100
 
+/*
+ * 51-char inline_name is long enough for all cephfs and all but one
+ * rbd requests: <imgname> in "<imgname>.rbd"/"rbd_id.<imgname>" can be
+ * arbitrarily long (~PAGE_SIZE).  It's done once during rbd map; all
+ * other rbd requests fit into inline_name.
+ *
+ * Makes ceph_object_id 64 bytes on 64-bit.
+ */
+#define CEPH_OID_INLINE_LEN 52
+
+/*
+ * Both inline and external buffers have space for a NUL-terminator,
+ * which is carried around.  It's not required though - RADOS object
+ * names don't have to be NUL-terminated and may contain NULs.
+ */
 struct ceph_object_id {
-	char name[CEPH_MAX_OID_NAME_LEN];
+	char *name;
+	char inline_name[CEPH_OID_INLINE_LEN];
 	int name_len;
 };
 
+static inline void ceph_oid_init(struct ceph_object_id *oid)
+{
+	oid->name = oid->inline_name;
+	oid->name_len = 0;
+}
+
+static inline bool ceph_oid_empty(const struct ceph_object_id *oid)
+{
+	return oid->name == oid->inline_name && !oid->name_len;
+}
+
+void ceph_oid_copy(struct ceph_object_id *dest,
+		   const struct ceph_object_id *src);
+__printf(2, 3)
+void ceph_oid_printf(struct ceph_object_id *oid, const char *fmt, ...);
+__printf(3, 4)
+int ceph_oid_aprintf(struct ceph_object_id *oid, gfp_t gfp,
+		     const char *fmt, ...);
+void ceph_oid_destroy(struct ceph_object_id *oid);
+
 struct ceph_pg_mapping {
 	struct rb_node node;
 	struct ceph_pg pgid;
@@ -113,30 +149,6 @@ struct ceph_osdmap {
 	int crush_scratch_ary[CEPH_PG_MAX_SIZE * 3];
 };
 
-static inline void ceph_oid_set_name(struct ceph_object_id *oid,
-				     const char *name)
-{
-	int len;
-
-	len = strlen(name);
-	if (len > sizeof(oid->name)) {
-		WARN(1, "ceph_oid_set_name '%s' len %d vs %zu, truncating\n",
-		     name, len, sizeof(oid->name));
-		len = sizeof(oid->name);
-	}
-
-	memcpy(oid->name, name, len);
-	oid->name_len = len;
-}
-
-static inline void ceph_oid_copy(struct ceph_object_id *dest,
-				 struct ceph_object_id *src)
-{
-	BUG_ON(src->name_len > sizeof(dest->name));
-	memcpy(dest->name, src->name, src->name_len);
-	dest->name_len = src->name_len;
-}
-
 static inline int ceph_osd_exists(struct ceph_osdmap *map, int osd)
 {
 	return osd >= 0 && osd < map->max_osd &&
diff --git a/net/ceph/debugfs.c b/net/ceph/debugfs.c
index 593dc2eabcc8..6bfb7573f5f7 100644
--- a/net/ceph/debugfs.c
+++ b/net/ceph/debugfs.c
@@ -156,7 +156,7 @@ static int osdc_show(struct seq_file *s, void *pp)
 			   req->r_osd ? req->r_osd->o_osd : -1,
 			   req->r_pgid.pool, req->r_pgid.seed);
 
-		seq_printf(s, "%.*s", req->r_base_oid.name_len,
+		seq_printf(s, "%*pE", req->r_base_oid.name_len,
 			   req->r_base_oid.name);
 
 		if (req->r_reassert_version.epoch)
* Unmerged path net/ceph/osd_client.c
diff --git a/net/ceph/osdmap.c b/net/ceph/osdmap.c
index 243574c8cf33..4668b871ca47 100644
--- a/net/ceph/osdmap.c
+++ b/net/ceph/osdmap.c
@@ -1381,8 +1381,99 @@ bad:
 	return ERR_PTR(err);
 }
 
+void ceph_oid_copy(struct ceph_object_id *dest,
+		   const struct ceph_object_id *src)
+{
+	WARN_ON(!ceph_oid_empty(dest));
+
+	if (src->name != src->inline_name) {
+		/* very rare, see ceph_object_id definition */
+		dest->name = kmalloc(src->name_len + 1,
+				     GFP_NOIO | __GFP_NOFAIL);
+	}
 
+	memcpy(dest->name, src->name, src->name_len + 1);
+	dest->name_len = src->name_len;
+}
+EXPORT_SYMBOL(ceph_oid_copy);
 
+static __printf(2, 0)
+int oid_printf_vargs(struct ceph_object_id *oid, const char *fmt, va_list ap)
+{
+	int len;
+
+	WARN_ON(!ceph_oid_empty(oid));
+
+	len = vsnprintf(oid->inline_name, sizeof(oid->inline_name), fmt, ap);
+	if (len >= sizeof(oid->inline_name))
+		return len;
+
+	oid->name_len = len;
+	return 0;
+}
+
+/*
+ * If oid doesn't fit into inline buffer, BUG.
+ */
+void ceph_oid_printf(struct ceph_object_id *oid, const char *fmt, ...)
+{
+	va_list ap;
+
+	va_start(ap, fmt);
+	BUG_ON(oid_printf_vargs(oid, fmt, ap));
+	va_end(ap);
+}
+EXPORT_SYMBOL(ceph_oid_printf);
+
+static __printf(3, 0)
+int oid_aprintf_vargs(struct ceph_object_id *oid, gfp_t gfp,
+		      const char *fmt, va_list ap)
+{
+	va_list aq;
+	int len;
+
+	va_copy(aq, ap);
+	len = oid_printf_vargs(oid, fmt, aq);
+	va_end(aq);
+
+	if (len) {
+		char *external_name;
+
+		external_name = kmalloc(len + 1, gfp);
+		if (!external_name)
+			return -ENOMEM;
+
+		oid->name = external_name;
+		WARN_ON(vsnprintf(oid->name, len + 1, fmt, ap) != len);
+		oid->name_len = len;
+	}
+
+	return 0;
+}
+
+/*
+ * If oid doesn't fit into inline buffer, allocate.
+ */
+int ceph_oid_aprintf(struct ceph_object_id *oid, gfp_t gfp,
+		     const char *fmt, ...)
+{
+	va_list ap;
+	int ret;
+
+	va_start(ap, fmt);
+	ret = oid_aprintf_vargs(oid, gfp, fmt, ap);
+	va_end(ap);
+
+	return ret;
+}
+EXPORT_SYMBOL(ceph_oid_aprintf);
+
+void ceph_oid_destroy(struct ceph_object_id *oid)
+{
+	if (oid->name != oid->inline_name)
+		kfree(oid->name);
+}
+EXPORT_SYMBOL(ceph_oid_destroy);
 
 /*
  * calculate file layout from given offset, length.
@@ -1474,7 +1565,7 @@ int ceph_oloc_oid_to_pg(struct ceph_osdmap *osdmap,
 	pg_out->seed = ceph_str_hash(pi->object_hash, oid->name,
 				     oid->name_len);
 
-	dout("%s '%.*s' pgid %llu.%x\n", __func__, oid->name_len, oid->name,
+	dout("%s %*pE pgid %llu.%x\n", __func__, oid->name_len, oid->name,
 	     pg_out->pool, pg_out->seed);
 	return 0;
 }
