time: Add timerkeeper::tkr_raw

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 4a4ad80d32cea69ee93bd4589f24dc478804cd80
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/4a4ad80d.failed

Introduce tkr_raw and make use of it.

  base_raw -> tkr_raw.base
  clock->{mult,shift} -> tkr_raw.{mult.shift}

Kill timekeeping_get_ns_raw() in favour of
timekeeping_get_ns(&tkr_raw), this removes all mono_raw special
casing.

Duplicate the updates to tkr_mono.cycle_last into tkr_raw.cycle_last,
both need the same value.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: John Stultz <john.stultz@linaro.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20150319093400.422589590@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 4a4ad80d32cea69ee93bd4589f24dc478804cd80)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/timekeeper_internal.h
#	kernel/time/timekeeping.c
diff --cc include/linux/timekeeper_internal.h
index a44b704da541,fb86963859c7..000000000000
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@@ -10,68 -10,88 +10,149 @@@
  #include <linux/jiffies.h>
  #include <linux/time.h>
  
++<<<<<<< HEAD
 +/* Structure holding internal timekeeping values. */
 +struct timekeeper {
 +	/* Current clocksource used for timekeeping. */
 +	struct clocksource	*clock;
 +	/* NTP adjusted clock multiplier */
 +	u32			mult;
 +	/* The shift value of the current clocksource. */
 +	u32			shift;
 +	/* Number of clock cycles in one NTP interval. */
++=======
+ /**
+  * struct tk_read_base - base structure for timekeeping readout
+  * @clock:	Current clocksource used for timekeeping.
+  * @read:	Read function of @clock
+  * @mask:	Bitmask for two's complement subtraction of non 64bit clocks
+  * @cycle_last: @clock cycle value at last update
+  * @mult:	(NTP adjusted) multiplier for scaled math conversion
+  * @shift:	Shift value for scaled math conversion
+  * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+  * @base:	ktime_t (nanoseconds) base time for readout
+  *
+  * This struct has size 56 byte on 64 bit. Together with a seqcount it
+  * occupies a single 64byte cache line.
+  *
+  * The struct is separate from struct timekeeper as it is also used
+  * for a fast NMI safe accessors.
+  */
+ struct tk_read_base {
+ 	struct clocksource	*clock;
+ 	cycle_t			(*read)(struct clocksource *cs);
+ 	cycle_t			mask;
+ 	cycle_t			cycle_last;
+ 	u32			mult;
+ 	u32			shift;
+ 	u64			xtime_nsec;
+ 	ktime_t			base;
+ };
+ 
+ /**
+  * struct timekeeper - Structure holding internal timekeeping values.
+  * @tkr_mono:		The readout base structure for CLOCK_MONOTONIC
+  * @tkr_raw:		The readout base structure for CLOCK_MONOTONIC_RAW
+  * @xtime_sec:		Current CLOCK_REALTIME time in seconds
+  * @ktime_sec:		Current CLOCK_MONOTONIC time in seconds
+  * @wall_to_monotonic:	CLOCK_REALTIME to CLOCK_MONOTONIC offset
+  * @offs_real:		Offset clock monotonic -> clock realtime
+  * @offs_boot:		Offset clock monotonic -> clock boottime
+  * @offs_tai:		Offset clock monotonic -> clock tai
+  * @tai_offset:		The current UTC to TAI offset in seconds
+  * @raw_time:		Monotonic raw base time in timespec64 format
+  * @cycle_interval:	Number of clock cycles in one NTP interval
+  * @xtime_interval:	Number of clock shifted nano seconds in one NTP
+  *			interval.
+  * @xtime_remainder:	Shifted nano seconds left over when rounding
+  *			@cycle_interval
+  * @raw_interval:	Raw nano seconds accumulated per NTP interval.
+  * @ntp_error:		Difference between accumulated time and NTP time in ntp
+  *			shifted nano seconds.
+  * @ntp_error_shift:	Shift conversion between clock shifted nano seconds and
+  *			ntp shifted nano seconds.
+  *
+  * Note: For timespec(64) based interfaces wall_to_monotonic is what
+  * we need to add to xtime (or xtime corrected for sub jiffie times)
+  * to get to monotonic time.  Monotonic is pegged at zero at system
+  * boot time, so wall_to_monotonic will be negative, however, we will
+  * ALWAYS keep the tv_nsec part positive so we can use the usual
+  * normalization.
+  *
+  * wall_to_monotonic is moved after resume from suspend for the
+  * monotonic time not to jump. We need to add total_sleep_time to
+  * wall_to_monotonic to get the real boot based time offset.
+  *
+  * wall_to_monotonic is no longer the boot time, getboottime must be
+  * used instead.
+  */
+ struct timekeeper {
+ 	struct tk_read_base	tkr_mono;
+ 	struct tk_read_base	tkr_raw;
+ 	u64			xtime_sec;
+ 	unsigned long		ktime_sec;
+ 	struct timespec64	wall_to_monotonic;
+ 	ktime_t			offs_real;
+ 	ktime_t			offs_boot;
+ 	ktime_t			offs_tai;
+ 	s32			tai_offset;
+ 	struct timespec64	raw_time;
+ 
+ 	/* The following members are for timekeeping internal use */
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  	cycle_t			cycle_interval;
 +	/* Last cycle value (also stored in clock->cycle_last) */
 +	cycle_t			cycle_last;
 +	/* Number of clock shifted nano seconds in one NTP interval. */
  	u64			xtime_interval;
 +	/* shifted nano seconds left over when rounding cycle_interval */
  	s64			xtime_remainder;
 +	/* Raw nano seconds accumulated per NTP interval. */
  	u32			raw_interval;
 +
 +	/* Current CLOCK_REALTIME time in seconds */
 +	u64			xtime_sec;
 +	/* Clock shifted nano seconds */
 +	u64			xtime_nsec;
 +
 +	/* Monotonic base time */
 +	ktime_t                 base_mono;
 +
 +	/* Difference between accumulated time and NTP time in ntp
 +	 * shifted nano seconds. */
 +	s64			ntp_error;
 +	/* Shift conversion between clock shifted nano seconds and
 +	 * ntp shifted nano seconds. */
 +	u32			ntp_error_shift;
 +
 +	/*
 +	 * wall_to_monotonic is what we need to add to xtime (or xtime corrected
 +	 * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
 +	 * at zero at system boot time, so wall_to_monotonic will be negative,
 +	 * however, we will ALWAYS keep the tv_nsec part positive so we can use
 +	 * the usual normalization.
 +	 *
 +	 * wall_to_monotonic is moved after resume from suspend for the
 +	 * monotonic time not to jump. We need to add total_sleep_time to
 +	 * wall_to_monotonic to get the real boot based time offset.
 +	 *
 +	 * - wall_to_monotonic is no longer the boot time, getboottime must be
 +	 * used instead.
 +	 */
 +	struct timespec64	wall_to_monotonic;
 +	/* Offset clock monotonic -> clock realtime */
 +	ktime_t			offs_real;
 +	/* time spent in suspend */
 +	struct timespec64	total_sleep_time;
 +	/* Offset clock monotonic -> clock boottime */
 +	ktime_t			offs_boot;
 +	/* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
 +	struct timespec64	raw_time;
 +	/* The current UTC to TAI offset in seconds */
 +	s32			tai_offset;
 +	/* Offset clock monotonic -> clock tai */
 +	ktime_t			offs_tai;
 +
  	/* The ntp_tick_length() value currently being used.
  	 * This cached copy ensures we consistently apply the tick
  	 * length for an entire tick, as ntp_tick_length may change
diff --cc kernel/time/timekeeping.c
index 7251ed9665bf,cbb612ee813f..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -105,10 -246,17 +105,15 @@@ static void tk_setup_internals(struct t
  	u64 tmp, ntpinterval;
  	struct clocksource *old_clock;
  
 -	old_clock = tk->tkr_mono.clock;
 -	tk->tkr_mono.clock = clock;
 -	tk->tkr_mono.read = clock->read;
 -	tk->tkr_mono.mask = clock->mask;
 -	tk->tkr_mono.cycle_last = tk->tkr_mono.read(clock);
 +	old_clock = tk->clock;
 +	tk->clock = clock;
 +	tk->cycle_last = clock->cycle_last = clock->read(clock);
  
+ 	tk->tkr_raw.clock = clock;
+ 	tk->tkr_raw.read = clock->read;
+ 	tk->tkr_raw.mask = clock->mask;
+ 	tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+ 
  	/* Do the ns -> cycle conversion first, using original mult */
  	tmp = NTP_INTERVAL_LENGTH;
  	tmp <<= clock->shift;
@@@ -131,11 -279,14 +136,18 @@@
  	if (old_clock) {
  		int shift_change = clock->shift - old_clock->shift;
  		if (shift_change < 0)
 -			tk->tkr_mono.xtime_nsec >>= -shift_change;
 +			tk->xtime_nsec >>= -shift_change;
  		else
 -			tk->tkr_mono.xtime_nsec <<= shift_change;
 +			tk->xtime_nsec <<= shift_change;
  	}
++<<<<<<< HEAD
 +	tk->shift = clock->shift;
++=======
+ 	tk->tkr_raw.xtime_nsec = 0;
+ 
+ 	tk->tkr_mono.shift = clock->shift;
+ 	tk->tkr_raw.shift = clock->shift;
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  
  	tk->ntp_error = 0;
  	tk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;
@@@ -146,65 -297,203 +158,242 @@@
  	 * active clocksource. These value will be adjusted via NTP
  	 * to counteract clock drifting.
  	 */
++<<<<<<< HEAD
 +	tk->mult = clock->mult;
++=======
+ 	tk->tkr_mono.mult = clock->mult;
+ 	tk->tkr_raw.mult = clock->mult;
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  	tk->ntp_err_mult = 0;
  }
  
  /* Timekeeper helper functions. */
  
  #ifdef CONFIG_ARCH_USES_GETTIMEOFFSET
 -static u32 default_arch_gettimeoffset(void) { return 0; }
 -u32 (*arch_gettimeoffset)(void) = default_arch_gettimeoffset;
 +u32 (*arch_gettimeoffset)(void);
 +
 +u32 get_arch_timeoffset(void)
 +{
 +	if (likely(arch_gettimeoffset))
 +		return arch_gettimeoffset();
 +	return 0;
 +}
  #else
 -static inline u32 arch_gettimeoffset(void) { return 0; }
 +static inline u32 get_arch_timeoffset(void) { return 0; }
  #endif
  
 -static inline s64 timekeeping_get_ns(struct tk_read_base *tkr)
 +static inline s64 timekeeping_get_ns(struct timekeeper *tk)
 +{
 +	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
 +	s64 nsec;
 +
 +	/* read clocksource: */
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +
 +	/* calculate the delta since the last update_wall_time: */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
 +
 +	nsec = delta * tk->mult + tk->xtime_nsec;
 +	nsec >>= tk->shift;
 +
 +	/* If arch requires, add in get_arch_timeoffset() */
 +	return nsec + get_arch_timeoffset();
 +}
 +
++<<<<<<< HEAD
 +static inline s64 timekeeping_get_ns_raw(struct timekeeper *tk)
  {
 -	cycle_t delta;
 +	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
  	s64 nsec;
  
 -	delta = timekeeping_get_delta(tkr);
 +	/* read clocksource: */
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +
 +	/* calculate the delta since the last update_wall_time: */
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
  
 -	nsec = delta * tkr->mult + tkr->xtime_nsec;
 -	nsec >>= tkr->shift;
 +	/* convert delta to nanoseconds. */
 +	nsec = clocksource_cyc2ns(delta, clock->mult, clock->shift);
  
  	/* If arch requires, add in get_arch_timeoffset() */
 -	return nsec + arch_gettimeoffset();
 +	return nsec + get_arch_timeoffset();
  }
  
++=======
+ /**
+  * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.
+  * @tkr: Timekeeping readout base from which we take the update
+  *
+  * We want to use this from any context including NMI and tracing /
+  * instrumenting the timekeeping code itself.
+  *
+  * So we handle this differently than the other timekeeping accessor
+  * functions which retry when the sequence count has changed. The
+  * update side does:
+  *
+  * smp_wmb();	<- Ensure that the last base[1] update is visible
+  * tkf->seq++;
+  * smp_wmb();	<- Ensure that the seqcount update is visible
+  * update(tkf->base[0], tkr);
+  * smp_wmb();	<- Ensure that the base[0] update is visible
+  * tkf->seq++;
+  * smp_wmb();	<- Ensure that the seqcount update is visible
+  * update(tkf->base[1], tkr);
+  *
+  * The reader side does:
+  *
+  * do {
+  *	seq = tkf->seq;
+  *	smp_rmb();
+  *	idx = seq & 0x01;
+  *	now = now(tkf->base[idx]);
+  *	smp_rmb();
+  * } while (seq != tkf->seq)
+  *
+  * As long as we update base[0] readers are forced off to
+  * base[1]. Once base[0] is updated readers are redirected to base[0]
+  * and the base[1] update takes place.
+  *
+  * So if a NMI hits the update of base[0] then it will use base[1]
+  * which is still consistent. In the worst case this can result is a
+  * slightly wrong timestamp (a few nanoseconds). See
+  * @ktime_get_mono_fast_ns.
+  */
+ static void update_fast_timekeeper(struct tk_read_base *tkr)
+ {
+ 	struct tk_read_base *base = tk_fast_mono.base;
+ 
+ 	/* Force readers off to base[1] */
+ 	raw_write_seqcount_latch(&tk_fast_mono.seq);
+ 
+ 	/* Update base[0] */
+ 	memcpy(base, tkr, sizeof(*base));
+ 
+ 	/* Force readers back to base[0] */
+ 	raw_write_seqcount_latch(&tk_fast_mono.seq);
+ 
+ 	/* Update base[1] */
+ 	memcpy(base + 1, base, sizeof(*base));
+ }
+ 
+ /**
+  * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic
+  *
+  * This timestamp is not guaranteed to be monotonic across an update.
+  * The timestamp is calculated by:
+  *
+  *	now = base_mono + clock_delta * slope
+  *
+  * So if the update lowers the slope, readers who are forced to the
+  * not yet updated second array are still using the old steeper slope.
+  *
+  * tmono
+  * ^
+  * |    o  n
+  * |   o n
+  * |  u
+  * | o
+  * |o
+  * |12345678---> reader order
+  *
+  * o = old slope
+  * u = update
+  * n = new slope
+  *
+  * So reader 6 will observe time going backwards versus reader 5.
+  *
+  * While other CPUs are likely to be able observe that, the only way
+  * for a CPU local observation is when an NMI hits in the middle of
+  * the update. Timestamps taken from that NMI context might be ahead
+  * of the following timestamps. Callers need to be aware of that and
+  * deal with it.
+  */
+ u64 notrace ktime_get_mono_fast_ns(void)
+ {
+ 	struct tk_read_base *tkr;
+ 	unsigned int seq;
+ 	u64 now;
+ 
+ 	do {
+ 		seq = raw_read_seqcount(&tk_fast_mono.seq);
+ 		tkr = tk_fast_mono.base + (seq & 0x01);
+ 		now = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);
+ 
+ 	} while (read_seqcount_retry(&tk_fast_mono.seq, seq));
+ 	return now;
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);
+ 
+ /* Suspend-time cycles value for halted fast timekeeper. */
+ static cycle_t cycles_at_suspend;
+ 
+ static cycle_t dummy_clock_read(struct clocksource *cs)
+ {
+ 	return cycles_at_suspend;
+ }
+ 
+ /**
+  * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.
+  * @tk: Timekeeper to snapshot.
+  *
+  * It generally is unsafe to access the clocksource after timekeeping has been
+  * suspended, so take a snapshot of the readout base of @tk and use it as the
+  * fast timekeeper's readout base while suspended.  It will return the same
+  * number of cycles every time until timekeeping is resumed at which time the
+  * proper readout base for the fast timekeeper will be restored automatically.
+  */
+ static void halt_fast_timekeeper(struct timekeeper *tk)
+ {
+ 	static struct tk_read_base tkr_dummy;
+ 	struct tk_read_base *tkr = &tk->tkr_mono;
+ 
+ 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
+ 	cycles_at_suspend = tkr->read(tkr->clock);
+ 	tkr_dummy.read = dummy_clock_read;
+ 	update_fast_timekeeper(&tkr_dummy);
+ }
+ 
+ #ifdef CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 
+ static inline void update_vsyscall(struct timekeeper *tk)
+ {
+ 	struct timespec xt, wm;
+ 
+ 	xt = timespec64_to_timespec(tk_xtime(tk));
+ 	wm = timespec64_to_timespec(tk->wall_to_monotonic);
+ 	update_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,
+ 			    tk->tkr_mono.cycle_last);
+ }
+ 
+ static inline void old_vsyscall_fixup(struct timekeeper *tk)
+ {
+ 	s64 remainder;
+ 
+ 	/*
+ 	* Store only full nanoseconds into xtime_nsec after rounding
+ 	* it up and add the remainder to the error difference.
+ 	* XXX - This is necessary to avoid small 1ns inconsistnecies caused
+ 	* by truncating the remainder in vsyscalls. However, it causes
+ 	* additional work to be done in timekeeping_adjust(). Once
+ 	* the vsyscall implementations are converted to use xtime_nsec
+ 	* (shifted nanoseconds), and CONFIG_GENERIC_TIME_VSYSCALL_OLD
+ 	* users are removed, this can be killed.
+ 	*/
+ 	remainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);
+ 	tk->tkr_mono.xtime_nsec -= remainder;
+ 	tk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;
+ 	tk->ntp_error += remainder << tk->ntp_error_shift;
+ 	tk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;
+ }
+ #else
+ #define old_vsyscall_fixup(tk)
+ #endif
+ 
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
  
  static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
@@@ -272,10 -551,22 +461,29 @@@ static inline void tk_update_ktime_data
  	 *	nsec = base_mono + now();
  	 * ==> base_mono = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec
  	 */
++<<<<<<< HEAD
 +	nsec = (s64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
 +	nsec *= NSEC_PER_SEC;
 +	nsec += tk->wall_to_monotonic.tv_nsec;
 +	tk->base_mono = ns_to_ktime(nsec);
++=======
+ 	seconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
+ 	nsec = (u32) tk->wall_to_monotonic.tv_nsec;
+ 	tk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);
+ 
+ 	/* Update the monotonic raw base */
+ 	tk->tkr_raw.base = timespec64_to_ktime(tk->raw_time);
+ 
+ 	/*
+ 	 * The sum of the nanoseconds portions of xtime and
+ 	 * wall_to_monotonic can be greater/equal one second. Take
+ 	 * this into account before updating tk->ktime_sec.
+ 	 */
+ 	nsec += (u32)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ 	if (nsec >= NSEC_PER_SEC)
+ 		seconds++;
+ 	tk->ktime_sec = seconds;
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  }
  
  /* must hold timekeeper_lock */
@@@ -308,19 -598,19 +516,26 @@@ static void timekeeping_update(struct t
   */
  static void timekeeping_forward_now(struct timekeeper *tk)
  {
 -	struct clocksource *clock = tk->tkr_mono.clock;
  	cycle_t cycle_now, delta;
 +	struct clocksource *clock;
  	s64 nsec;
  
++<<<<<<< HEAD
 +	clock = tk->clock;
 +	cycle_now = clock->read(clock);
 +	delta = clocksource_delta(cycle_now, clock->cycle_last, clock->mask);
 +	tk->cycle_last = clock->cycle_last = cycle_now;
++=======
+ 	cycle_now = tk->tkr_mono.read(clock);
+ 	delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
+ 	tk->tkr_mono.cycle_last = cycle_now;
+ 	tk->tkr_raw.cycle_last  = cycle_now;
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  
 -	tk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;
 +	tk->xtime_nsec += delta * tk->mult;
  
  	/* If arch requires, add in get_arch_timeoffset() */
 -	tk->tkr_mono.xtime_nsec += (u64)arch_gettimeoffset() << tk->tkr_mono.shift;
 +	tk->xtime_nsec += (u64)get_arch_timeoffset() << tk->shift;
  
  	tk_normalize_xtime(tk);
  
@@@ -396,6 -684,74 +611,77 @@@ ktime_t ktime_get(void
  }
  EXPORT_SYMBOL_GPL(ktime_get);
  
++<<<<<<< HEAD
++=======
+ static ktime_t *offsets[TK_OFFS_MAX] = {
+ 	[TK_OFFS_REAL]	= &tk_core.timekeeper.offs_real,
+ 	[TK_OFFS_BOOT]	= &tk_core.timekeeper.offs_boot,
+ 	[TK_OFFS_TAI]	= &tk_core.timekeeper.offs_tai,
+ };
+ 
+ ktime_t ktime_get_with_offset(enum tk_offsets offs)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base, *offset = offsets[offs];
+ 	s64 nsecs;
+ 
+ 	WARN_ON(timekeeping_suspended);
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = ktime_add(tk->tkr_mono.base, *offset);
+ 		nsecs = timekeeping_get_ns(&tk->tkr_mono);
+ 
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ 
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_with_offset);
+ 
+ /**
+  * ktime_mono_to_any() - convert mononotic time to any other time
+  * @tmono:	time to convert.
+  * @offs:	which offset to use
+  */
+ ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)
+ {
+ 	ktime_t *offset = offsets[offs];
+ 	unsigned long seq;
+ 	ktime_t tconv;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		tconv = ktime_add(tmono, *offset);
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return tconv;
+ }
+ EXPORT_SYMBOL_GPL(ktime_mono_to_any);
+ 
+ /**
+  * ktime_get_raw - Returns the raw monotonic time in ktime_t format
+  */
+ ktime_t ktime_get_raw(void)
+ {
+ 	struct timekeeper *tk = &tk_core.timekeeper;
+ 	unsigned int seq;
+ 	ktime_t base;
+ 	s64 nsecs;
+ 
+ 	do {
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		base = tk->tkr_raw.base;
+ 		nsecs = timekeeping_get_ns(&tk->tkr_raw);
+ 
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
+ 
+ 	return ktime_add_ns(base, nsecs);
+ }
+ EXPORT_SYMBOL_GPL(ktime_get_raw);
+ 
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  /**
   * ktime_get_ts64 - get the monotonic clock in timespec64 format
   * @ts:		pointer to timespec variable
@@@ -499,10 -857,10 +785,15 @@@ void getnstime_raw_and_real(struct time
  		ts_real->tv_sec = tk->xtime_sec;
  		ts_real->tv_nsec = 0;
  
++<<<<<<< HEAD
 +		nsecs_raw = timekeeping_get_ns_raw(tk);
 +		nsecs_real = timekeeping_get_ns(tk);
++=======
+ 		nsecs_raw  = timekeeping_get_ns(&tk->tkr_raw);
+ 		nsecs_real = timekeeping_get_ns(&tk->tkr_mono);
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	timespec_add_ns(ts_raw, nsecs_raw);
  	timespec_add_ns(ts_real, nsecs_real);
@@@ -748,16 -1090,17 +1039,21 @@@ void getrawmonotonic(struct timespec *t
  	s64 nsecs;
  
  	do {
++<<<<<<< HEAD
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +		nsecs = timekeeping_get_ns_raw(tk);
++=======
+ 		seq = read_seqcount_begin(&tk_core.seq);
+ 		nsecs = timekeeping_get_ns(&tk->tkr_raw);
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  		ts64 = tk->raw_time;
  
 -	} while (read_seqcount_retry(&tk_core.seq, seq));
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
  	timespec64_add_ns(&ts64, nsecs);
 -	*ts = ts64;
 +	*ts = timespec64_to_timespec(ts64);
  }
 -EXPORT_SYMBOL(getrawmonotonic64);
 -
 +EXPORT_SYMBOL(getrawmonotonic);
  
  /**
   * timekeeping_valid_for_hres - Check if timekeeping is suitable for hres
@@@ -1021,7 -1360,9 +1317,13 @@@ static void timekeeping_resume(void
  		__timekeeping_inject_sleeptime(tk, &ts_delta);
  
  	/* Re-base the last cycle value */
++<<<<<<< HEAD
 +	tk->cycle_last = clock->cycle_last = cycle_now;
++=======
+ 	tk->tkr_mono.cycle_last = cycle_now;
+ 	tk->tkr_raw.cycle_last  = cycle_now;
+ 
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  	tk->ntp_error = 0;
  	timekeeping_suspended = 0;
  	timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
@@@ -1325,9 -1676,10 +1627,14 @@@ static cycle_t logarithmic_accumulation
  
  	/* Accumulate one shifted interval */
  	offset -= interval;
++<<<<<<< HEAD
 +	tk->cycle_last += interval;
++=======
+ 	tk->tkr_mono.cycle_last += interval;
+ 	tk->tkr_raw.cycle_last  += interval;
++>>>>>>> 4a4ad80d32ce (time: Add timerkeeper::tkr_raw)
  
 -	tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
 +	tk->xtime_nsec += tk->xtime_interval << shift;
  	*clock_set |= accumulate_nsecs_to_secs(tk);
  
  	/* Accumulate raw time */
* Unmerged path include/linux/timekeeper_internal.h
* Unmerged path kernel/time/timekeeping.c
