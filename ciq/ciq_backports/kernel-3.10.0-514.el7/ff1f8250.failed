Btrfs: fix race between block group creation and their cache writeout

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Filipe Manana <fdmanana@suse.com>
commit ff1f8250a9e47c1032eb5c86ababff461a11f3a0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ff1f8250.failed

So creating a block group has 2 distinct phases:

Phase 1 - creates the btrfs_block_group_cache item and adds it to the
rbtree fs_info->block_group_cache_tree and to the corresponding list
space_info->block_groups[];

Phase 2 - adds the block group item to the extent tree and corresponding
items to the chunk tree.

The first phase adds the block_group_cache_item to a list of pending block
groups in the transaction handle, and phase 2 happens when
btrfs_end_transaction() is called against the transaction handle.

It happens that once phase 1 completes, other concurrent tasks that use
their own transaction handle, but points to the same running transaction
(struct btrfs_trans_handle->transaction), can use this block group for
space allocations and therefore mark it dirty. Dirty block groups are
tracked in a list belonging to the currently running transaction (struct
btrfs_transaction) and not in the transaction handle (btrfs_trans_handle).

This is a problem because once a task calls btrfs_commit_transaction(),
it calls btrfs_start_dirty_block_groups() which will see all dirty block
groups and attempt to start their writeout, including those that are
still attached to the transaction handle of some concurrent task that
hasn't called btrfs_end_transaction() yet - which means those block
groups haven't gone through phase 2 yet and therefore when
write_one_cache_group() is called, it won't find the block group items
in the extent tree and abort the current transaction with -ENOENT,
turning the fs into readonly mode and require a remount.

Fix this by ignoring -ENOENT when looking for block group items in the
extent tree when we attempt to start the writeout of the block group
caches outside the critical section of the transaction commit. We will
try again later during the critical section and if there we still don't
find the block group item in the extent tree, we then abort the current
transaction.

This issue happened twice, once while running fstests btrfs/067 and once
for btrfs/078, which produced the following trace:

[ 3278.703014] WARNING: CPU: 7 PID: 18499 at fs/btrfs/super.c:260 __btrfs_abort_transaction+0x52/0x114 [btrfs]()
[ 3278.707329] BTRFS: Transaction aborted (error -2)
(...)
[ 3278.731555] Call Trace:
[ 3278.732396]  [<ffffffff8142fa46>] dump_stack+0x4f/0x7b
[ 3278.733860]  [<ffffffff8108b6a2>] ? console_unlock+0x361/0x3ad
[ 3278.735312]  [<ffffffff81045ea5>] warn_slowpath_common+0xa1/0xbb
[ 3278.736874]  [<ffffffffa03ada6d>] ? __btrfs_abort_transaction+0x52/0x114 [btrfs]
[ 3278.738302]  [<ffffffff81045f05>] warn_slowpath_fmt+0x46/0x48
[ 3278.739520]  [<ffffffffa03ada6d>] __btrfs_abort_transaction+0x52/0x114 [btrfs]
[ 3278.741222]  [<ffffffffa03b9e56>] write_one_cache_group+0xae/0xbf [btrfs]
[ 3278.742797]  [<ffffffffa03c487b>] btrfs_start_dirty_block_groups+0x170/0x2b2 [btrfs]
[ 3278.744492]  [<ffffffffa03d309c>] btrfs_commit_transaction+0x130/0x9c9 [btrfs]
[ 3278.746084]  [<ffffffff8107d33d>] ? trace_hardirqs_on+0xd/0xf
[ 3278.747249]  [<ffffffffa03e5660>] btrfs_sync_file+0x313/0x387 [btrfs]
[ 3278.748744]  [<ffffffff8117acad>] vfs_fsync_range+0x95/0xa4
[ 3278.749958]  [<ffffffff81435b54>] ? ret_from_sys_call+0x1d/0x58
[ 3278.751218]  [<ffffffff8117acd8>] vfs_fsync+0x1c/0x1e
[ 3278.754197]  [<ffffffff8117ae54>] do_fsync+0x34/0x4e
[ 3278.755192]  [<ffffffff8117b07c>] SyS_fsync+0x10/0x14
[ 3278.756236]  [<ffffffff81435b32>] system_call_fastpath+0x12/0x17
[ 3278.757366] ---[ end trace 9a4d4df4969709aa ]---

Fixes: 1bbc621ef284 ("Btrfs: allow block group cache writeout
                      outside critical section in commit")

	Signed-off-by: Filipe Manana <fdmanana@suse.com>
	Signed-off-by: Chris Mason <clm@fb.com>
(cherry picked from commit ff1f8250a9e47c1032eb5c86ababff461a11f3a0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/btrfs/extent-tree.c
diff --cc fs/btrfs/extent-tree.c
index be2ff97f08ac,7effed6f2fa6..000000000000
--- a/fs/btrfs/extent-tree.c
+++ b/fs/btrfs/extent-tree.c
@@@ -3349,6 -3379,178 +3347,181 @@@ int btrfs_setup_space_cache(struct btrf
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * transaction commit does final block group cache writeback during a
+  * critical section where nothing is allowed to change the FS.  This is
+  * required in order for the cache to actually match the block group,
+  * but can introduce a lot of latency into the commit.
+  *
+  * So, btrfs_start_dirty_block_groups is here to kick off block group
+  * cache IO.  There's a chance we'll have to redo some of it if the
+  * block group changes again during the commit, but it greatly reduces
+  * the commit latency by getting rid of the easy block groups while
+  * we're still allowing others to join the commit.
+  */
+ int btrfs_start_dirty_block_groups(struct btrfs_trans_handle *trans,
+ 				   struct btrfs_root *root)
+ {
+ 	struct btrfs_block_group_cache *cache;
+ 	struct btrfs_transaction *cur_trans = trans->transaction;
+ 	int ret = 0;
+ 	int should_put;
+ 	struct btrfs_path *path = NULL;
+ 	LIST_HEAD(dirty);
+ 	struct list_head *io = &cur_trans->io_bgs;
+ 	int num_started = 0;
+ 	int loops = 0;
+ 
+ 	spin_lock(&cur_trans->dirty_bgs_lock);
+ 	if (list_empty(&cur_trans->dirty_bgs)) {
+ 		spin_unlock(&cur_trans->dirty_bgs_lock);
+ 		return 0;
+ 	}
+ 	list_splice_init(&cur_trans->dirty_bgs, &dirty);
+ 	spin_unlock(&cur_trans->dirty_bgs_lock);
+ 
+ again:
+ 	/*
+ 	 * make sure all the block groups on our dirty list actually
+ 	 * exist
+ 	 */
+ 	btrfs_create_pending_block_groups(trans, root);
+ 
+ 	if (!path) {
+ 		path = btrfs_alloc_path();
+ 		if (!path)
+ 			return -ENOMEM;
+ 	}
+ 
+ 	/*
+ 	 * cache_write_mutex is here only to save us from balance or automatic
+ 	 * removal of empty block groups deleting this block group while we are
+ 	 * writing out the cache
+ 	 */
+ 	mutex_lock(&trans->transaction->cache_write_mutex);
+ 	while (!list_empty(&dirty)) {
+ 		cache = list_first_entry(&dirty,
+ 					 struct btrfs_block_group_cache,
+ 					 dirty_list);
+ 		/*
+ 		 * this can happen if something re-dirties a block
+ 		 * group that is already under IO.  Just wait for it to
+ 		 * finish and then do it all again
+ 		 */
+ 		if (!list_empty(&cache->io_list)) {
+ 			list_del_init(&cache->io_list);
+ 			btrfs_wait_cache_io(root, trans, cache,
+ 					    &cache->io_ctl, path,
+ 					    cache->key.objectid);
+ 			btrfs_put_block_group(cache);
+ 		}
+ 
+ 
+ 		/*
+ 		 * btrfs_wait_cache_io uses the cache->dirty_list to decide
+ 		 * if it should update the cache_state.  Don't delete
+ 		 * until after we wait.
+ 		 *
+ 		 * Since we're not running in the commit critical section
+ 		 * we need the dirty_bgs_lock to protect from update_block_group
+ 		 */
+ 		spin_lock(&cur_trans->dirty_bgs_lock);
+ 		list_del_init(&cache->dirty_list);
+ 		spin_unlock(&cur_trans->dirty_bgs_lock);
+ 
+ 		should_put = 1;
+ 
+ 		cache_save_setup(cache, trans, path);
+ 
+ 		if (cache->disk_cache_state == BTRFS_DC_SETUP) {
+ 			cache->io_ctl.inode = NULL;
+ 			ret = btrfs_write_out_cache(root, trans, cache, path);
+ 			if (ret == 0 && cache->io_ctl.inode) {
+ 				num_started++;
+ 				should_put = 0;
+ 
+ 				/*
+ 				 * the cache_write_mutex is protecting
+ 				 * the io_list
+ 				 */
+ 				list_add_tail(&cache->io_list, io);
+ 			} else {
+ 				/*
+ 				 * if we failed to write the cache, the
+ 				 * generation will be bad and life goes on
+ 				 */
+ 				ret = 0;
+ 			}
+ 		}
+ 		if (!ret) {
+ 			ret = write_one_cache_group(trans, root, path, cache);
+ 			/*
+ 			 * Our block group might still be attached to the list
+ 			 * of new block groups in the transaction handle of some
+ 			 * other task (struct btrfs_trans_handle->new_bgs). This
+ 			 * means its block group item isn't yet in the extent
+ 			 * tree. If this happens ignore the error, as we will
+ 			 * try again later in the critical section of the
+ 			 * transaction commit.
+ 			 */
+ 			if (ret == -ENOENT) {
+ 				ret = 0;
+ 				spin_lock(&cur_trans->dirty_bgs_lock);
+ 				if (list_empty(&cache->dirty_list)) {
+ 					list_add_tail(&cache->dirty_list,
+ 						      &cur_trans->dirty_bgs);
+ 					btrfs_get_block_group(cache);
+ 				}
+ 				spin_unlock(&cur_trans->dirty_bgs_lock);
+ 			} else if (ret) {
+ 				btrfs_abort_transaction(trans, root, ret);
+ 			}
+ 		}
+ 
+ 		/* if its not on the io list, we need to put the block group */
+ 		if (should_put)
+ 			btrfs_put_block_group(cache);
+ 
+ 		if (ret)
+ 			break;
+ 
+ 		/*
+ 		 * Avoid blocking other tasks for too long. It might even save
+ 		 * us from writing caches for block groups that are going to be
+ 		 * removed.
+ 		 */
+ 		mutex_unlock(&trans->transaction->cache_write_mutex);
+ 		mutex_lock(&trans->transaction->cache_write_mutex);
+ 	}
+ 	mutex_unlock(&trans->transaction->cache_write_mutex);
+ 
+ 	/*
+ 	 * go through delayed refs for all the stuff we've just kicked off
+ 	 * and then loop back (just once)
+ 	 */
+ 	ret = btrfs_run_delayed_refs(trans, root, 0);
+ 	if (!ret && loops == 0) {
+ 		loops++;
+ 		spin_lock(&cur_trans->dirty_bgs_lock);
+ 		list_splice_init(&cur_trans->dirty_bgs, &dirty);
+ 		/*
+ 		 * dirty_bgs_lock protects us from concurrent block group
+ 		 * deletes too (not just cache_write_mutex).
+ 		 */
+ 		if (!list_empty(&dirty)) {
+ 			spin_unlock(&cur_trans->dirty_bgs_lock);
+ 			goto again;
+ 		}
+ 		spin_unlock(&cur_trans->dirty_bgs_lock);
+ 	}
+ 
+ 	btrfs_free_path(path);
+ 	return ret;
+ }
+ 
++>>>>>>> ff1f8250a9e4 (Btrfs: fix race between block group creation and their cache writeout)
  int btrfs_write_dirty_block_groups(struct btrfs_trans_handle *trans,
  				   struct btrfs_root *root)
  {
* Unmerged path fs/btrfs/extent-tree.c
