perf: Optimize perf_sched_events() usage

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 25432ae96a9889774a05bf5f0f6fd8dbcdec5e72
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/25432ae9.failed

It doesn't make sense to take up-to _4_ references on
perf_sched_events() per event, avoid doing this.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 25432ae96a9889774a05bf5f0f6fd8dbcdec5e72)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/events/core.c
diff --cc kernel/events/core.c
index d78664f10b9a,935aefd16354..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -3447,9 -3506,16 +3449,19 @@@ static void unaccount_event(struct perf
  		atomic_dec(&nr_task_events);
  	if (event->attr.freq)
  		atomic_dec(&nr_freq_events);
++<<<<<<< HEAD
++=======
+ 	if (event->attr.context_switch) {
+ 		dec = true;
+ 		atomic_dec(&nr_switch_events);
+ 	}
++>>>>>>> 25432ae96a98 (perf: Optimize perf_sched_events() usage)
  	if (is_cgroup_event(event))
- 		static_key_slow_dec_deferred(&perf_sched_events);
+ 		dec = true;
  	if (has_branch_stack(event))
+ 		dec = true;
+ 
+ 	if (dec)
  		static_key_slow_dec_deferred(&perf_sched_events);
  
  	unaccount_event_cpu(event, event->cpu);
@@@ -7243,9 -7745,16 +7257,19 @@@ static void account_event(struct perf_e
  		if (atomic_inc_return(&nr_freq_events) == 1)
  			tick_nohz_full_kick_all();
  	}
++<<<<<<< HEAD
++=======
+ 	if (event->attr.context_switch) {
+ 		atomic_inc(&nr_switch_events);
+ 		inc = true;
+ 	}
++>>>>>>> 25432ae96a98 (perf: Optimize perf_sched_events() usage)
  	if (has_branch_stack(event))
- 		static_key_slow_inc(&perf_sched_events.key);
+ 		inc = true;
  	if (is_cgroup_event(event))
+ 		inc = true;
+ 
+ 	if (inc)
  		static_key_slow_inc(&perf_sched_events.key);
  
  	account_event_cpu(event, event->cpu);
* Unmerged path kernel/events/core.c
