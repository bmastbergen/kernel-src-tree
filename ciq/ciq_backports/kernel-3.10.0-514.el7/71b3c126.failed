x86/mm: Add barriers and document switch_mm()-vs-flush synchronization

jira LE-1907
cve CVE-2016-2069
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] mm: Add barriers and document switch_mm()-vs-flush synchronization (Rafael Aquini) [1332602] {CVE-2016-2069}
Rebuild_FUZZ: 97.06%
commit-author Andy Lutomirski <luto@kernel.org>
commit 71b3c126e61177eb693423f2e18a1914205b165e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/71b3c126.failed

When switch_mm() activates a new PGD, it also sets a bit that
tells other CPUs that the PGD is in use so that TLB flush IPIs
will be sent.  In order for that to work correctly, the bit
needs to be visible prior to loading the PGD and therefore
starting to fill the local TLB.

Document all the barriers that make this work correctly and add
a couple that were missing.

	Signed-off-by: Andy Lutomirski <luto@kernel.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: linux-mm@kvack.org
	Cc: stable@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 71b3c126e61177eb693423f2e18a1914205b165e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mmu_context.h
#	arch/x86/mm/tlb.c
diff --cc arch/x86/include/asm/mmu_context.h
index be12c534fd59,1edc9cd198b8..000000000000
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@@ -42,8 -116,35 +42,38 @@@ static inline void switch_mm(struct mm_
  #endif
  		cpumask_set_cpu(cpu, mm_cpumask(next));
  
- 		/* Re-load page tables */
+ 		/*
+ 		 * Re-load page tables.
+ 		 *
+ 		 * This logic has an ordering constraint:
+ 		 *
+ 		 *  CPU 0: Write to a PTE for 'next'
+ 		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.
+ 		 *  CPU 1: set bit 1 in next's mm_cpumask
+ 		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)
+ 		 *
+ 		 * We need to prevent an outcome in which CPU 1 observes
+ 		 * the new PTE value and CPU 0 observes bit 1 clear in
+ 		 * mm_cpumask.  (If that occurs, then the IPI will never
+ 		 * be sent, and CPU 0's TLB will contain a stale entry.)
+ 		 *
+ 		 * The bad outcome can occur if either CPU's load is
+ 		 * reordered before that CPU's store, so both CPUs much
+ 		 * execute full barriers to prevent this from happening.
+ 		 *
+ 		 * Thus, switch_mm needs a full barrier between the
+ 		 * store to mm_cpumask and any operation that could load
+ 		 * from next->pgd.  This barrier synchronizes with
+ 		 * remote TLB flushers.  Fortunately, load_cr3 is
+ 		 * serializing and thus acts as a full barrier.
+ 		 *
+ 		 */
  		load_cr3(next->pgd);
++<<<<<<< HEAD
++=======
+ 
+ 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
++>>>>>>> 71b3c126e611 (x86/mm: Add barriers and document switch_mm()-vs-flush synchronization)
  
  		/* Stop flush ipis for the previous mm */
  		cpumask_clear_cpu(cpu, mm_cpumask(prev));
@@@ -69,9 -187,15 +100,13 @@@
  			 * We were in lazy tlb mode and leave_mm disabled
  			 * tlb flush IPI delivery. We must reload CR3
  			 * to make sure to use no freed page tables.
+ 			 *
+ 			 * As above, this is a barrier that forces
+ 			 * TLB repopulation to be ordered after the
+ 			 * store to mm_cpumask.
  			 */
  			load_cr3(next->pgd);
 -			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 -			load_mm_cr4(next);
 -			load_mm_ldt(next);
 +			load_LDT_nolock(&next->context);
  		}
  	}
  #endif
diff --cc arch/x86/mm/tlb.c
index 2571f372ea2e,8f4cc3dfac32..000000000000
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@@ -149,7 -160,12 +149,16 @@@ void flush_tlb_current_task(void
  
  	preempt_disable();
  
++<<<<<<< HEAD
 +	local_flush_tlb();
++=======
+ 	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
+ 
+ 	/* This is an implicit full barrier that synchronizes with switch_mm. */
+ 	local_flush_tlb();
+ 
+ 	trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);
++>>>>>>> 71b3c126e611 (x86/mm: Add barriers and document switch_mm()-vs-flush synchronization)
  	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)
  		flush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);
  	preempt_enable();
@@@ -158,14 -174,29 +167,18 @@@
  void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
  				unsigned long end, unsigned long vmflag)
  {
 +	bool need_flush_others_all = true;
  	unsigned long addr;
 -	/* do a global flush by default */
 -	unsigned long base_pages_to_flush = TLB_FLUSH_ALL;
 +	unsigned act_entries, tlb_entries = 0;
 +	unsigned long nr_base_pages;
  
  	preempt_disable();
- 	if (current->active_mm != mm)
+ 	if (current->active_mm != mm) {
+ 		/* Synchronize with switch_mm. */
+ 		smp_mb();
+ 
  		goto out;
+ 	}
  
  	if (!current->mm) {
  		leave_mm(smp_processor_id());
@@@ -172,34 -207,27 +189,44 @@@
  		goto out;
  	}
  
 -	if ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))
 -		base_pages_to_flush = (end - start) >> PAGE_SHIFT;
 +	if (end == TLB_FLUSH_ALL || tlb_flushall_shift == -1
 +					|| vmflag & VM_HUGETLB) {
 +		local_flush_tlb();
 +		goto out;
 +	}
  
++<<<<<<< HEAD
 +	/* In modern CPU, last level tlb used for both data/ins */
 +	if (vmflag & VM_EXEC)
 +		tlb_entries = tlb_lli_4k[ENTRIES];
 +	else
 +		tlb_entries = tlb_lld_4k[ENTRIES];
 +
 +	/* Assume all of TLB entries was occupied by this task */
 +	act_entries = tlb_entries >> tlb_flushall_shift;
 +	act_entries = mm->total_vm > act_entries ? act_entries : mm->total_vm;
 +	nr_base_pages = (end - start) >> PAGE_SHIFT;
 +
 +	/* tlb_flushall_shift is on balance point, details in commit log */
 +	if (nr_base_pages > act_entries) {
++=======
+ 	/*
+ 	 * Both branches below are implicit full barriers (MOV to CR or
+ 	 * INVLPG) that synchronize with switch_mm.
+ 	 */
+ 	if (base_pages_to_flush > tlb_single_page_flush_ceiling) {
+ 		base_pages_to_flush = TLB_FLUSH_ALL;
+ 		count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
++>>>>>>> 71b3c126e611 (x86/mm: Add barriers and document switch_mm()-vs-flush synchronization)
  		local_flush_tlb();
  	} else {
 +		need_flush_others_all = false;
  		/* flush range by one by one 'invlpg' */
 -		for (addr = start; addr < end;	addr += PAGE_SIZE) {
 -			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 +		for (addr = start; addr < end;	addr += PAGE_SIZE)
  			__flush_tlb_single(addr);
 -		}
  	}
 -	trace_tlb_flush(TLB_LOCAL_MM_SHOOTDOWN, base_pages_to_flush);
  out:
 -	if (base_pages_to_flush == TLB_FLUSH_ALL) {
 +	if (need_flush_others_all) {
  		start = 0UL;
  		end = TLB_FLUSH_ALL;
  	}
* Unmerged path arch/x86/include/asm/mmu_context.h
* Unmerged path arch/x86/mm/tlb.c
