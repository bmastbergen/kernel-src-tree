block: Replace SG_GAPS with new queue limits mask

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [block] Replace SG_GAPS with new queue limits mask (David Milburn) [1283326]
Rebuild_FUZZ: 92.31%
commit-author Keith Busch <keith.busch@intel.com>
commit 03100aada96f0645bbcb89aea24c01f02d0ef1fa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/03100aad.failed

The SG_GAPS queue flag caused checks for bio vector alignment against
PAGE_SIZE, but the device may have different constraints. This patch
adds a queue limits so a driver with such constraints can set to allow
requests that would have been unnecessarily split. The new gaps check
takes the request_queue as a parameter to simplify the logic around
invoking this function.

This new limit makes the queue flag redundant, so removing it and
all usage. Device-mappers will inherit the correct settings through
blk_stack_limits().

	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 03100aada96f0645bbcb89aea24c01f02d0ef1fa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-merge.c
#	include/linux/bio.h
#	include/linux/blkdev.h
diff --cc block/blk-merge.c
index e9133160f261,0e0d9fd01c40..000000000000
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@@ -9,6 -9,140 +9,143 @@@
  
  #include "blk.h"
  
++<<<<<<< HEAD
++=======
+ static struct bio *blk_bio_discard_split(struct request_queue *q,
+ 					 struct bio *bio,
+ 					 struct bio_set *bs)
+ {
+ 	unsigned int max_discard_sectors, granularity;
+ 	int alignment;
+ 	sector_t tmp;
+ 	unsigned split_sectors;
+ 
+ 	/* Zero-sector (unknown) and one-sector granularities are the same.  */
+ 	granularity = max(q->limits.discard_granularity >> 9, 1U);
+ 
+ 	max_discard_sectors = min(q->limits.max_discard_sectors, UINT_MAX >> 9);
+ 	max_discard_sectors -= max_discard_sectors % granularity;
+ 
+ 	if (unlikely(!max_discard_sectors)) {
+ 		/* XXX: warn */
+ 		return NULL;
+ 	}
+ 
+ 	if (bio_sectors(bio) <= max_discard_sectors)
+ 		return NULL;
+ 
+ 	split_sectors = max_discard_sectors;
+ 
+ 	/*
+ 	 * If the next starting sector would be misaligned, stop the discard at
+ 	 * the previous aligned sector.
+ 	 */
+ 	alignment = (q->limits.discard_alignment >> 9) % granularity;
+ 
+ 	tmp = bio->bi_iter.bi_sector + split_sectors - alignment;
+ 	tmp = sector_div(tmp, granularity);
+ 
+ 	if (split_sectors > tmp)
+ 		split_sectors -= tmp;
+ 
+ 	return bio_split(bio, split_sectors, GFP_NOIO, bs);
+ }
+ 
+ static struct bio *blk_bio_write_same_split(struct request_queue *q,
+ 					    struct bio *bio,
+ 					    struct bio_set *bs)
+ {
+ 	if (!q->limits.max_write_same_sectors)
+ 		return NULL;
+ 
+ 	if (bio_sectors(bio) <= q->limits.max_write_same_sectors)
+ 		return NULL;
+ 
+ 	return bio_split(bio, q->limits.max_write_same_sectors, GFP_NOIO, bs);
+ }
+ 
+ static struct bio *blk_bio_segment_split(struct request_queue *q,
+ 					 struct bio *bio,
+ 					 struct bio_set *bs)
+ {
+ 	struct bio *split;
+ 	struct bio_vec bv, bvprv;
+ 	struct bvec_iter iter;
+ 	unsigned seg_size = 0, nsegs = 0, sectors = 0;
+ 	int prev = 0;
+ 
+ 	bio_for_each_segment(bv, bio, iter) {
+ 		sectors += bv.bv_len >> 9;
+ 
+ 		if (sectors > queue_max_sectors(q))
+ 			goto split;
+ 
+ 		/*
+ 		 * If the queue doesn't support SG gaps and adding this
+ 		 * offset would create a gap, disallow it.
+ 		 */
+ 		if (prev && bvec_gap_to_prev(q, &bvprv, bv.bv_offset))
+ 			goto split;
+ 
+ 		if (prev && blk_queue_cluster(q)) {
+ 			if (seg_size + bv.bv_len > queue_max_segment_size(q))
+ 				goto new_segment;
+ 			if (!BIOVEC_PHYS_MERGEABLE(&bvprv, &bv))
+ 				goto new_segment;
+ 			if (!BIOVEC_SEG_BOUNDARY(q, &bvprv, &bv))
+ 				goto new_segment;
+ 
+ 			seg_size += bv.bv_len;
+ 			bvprv = bv;
+ 			prev = 1;
+ 			continue;
+ 		}
+ new_segment:
+ 		if (nsegs == queue_max_segments(q))
+ 			goto split;
+ 
+ 		nsegs++;
+ 		bvprv = bv;
+ 		prev = 1;
+ 		seg_size = bv.bv_len;
+ 	}
+ 
+ 	return NULL;
+ split:
+ 	split = bio_clone_bioset(bio, GFP_NOIO, bs);
+ 
+ 	split->bi_iter.bi_size -= iter.bi_size;
+ 	bio->bi_iter = iter;
+ 
+ 	if (bio_integrity(bio)) {
+ 		bio_integrity_advance(bio, split->bi_iter.bi_size);
+ 		bio_integrity_trim(split, 0, bio_sectors(split));
+ 	}
+ 
+ 	return split;
+ }
+ 
+ void blk_queue_split(struct request_queue *q, struct bio **bio,
+ 		     struct bio_set *bs)
+ {
+ 	struct bio *split;
+ 
+ 	if ((*bio)->bi_rw & REQ_DISCARD)
+ 		split = blk_bio_discard_split(q, *bio, bs);
+ 	else if ((*bio)->bi_rw & REQ_WRITE_SAME)
+ 		split = blk_bio_write_same_split(q, *bio, bs);
+ 	else
+ 		split = blk_bio_segment_split(q, *bio, q->bio_split);
+ 
+ 	if (split) {
+ 		bio_chain(split, *bio);
+ 		generic_make_request(*bio);
+ 		*bio = split;
+ 	}
+ }
+ EXPORT_SYMBOL(blk_queue_split);
+ 
++>>>>>>> 03100aada96f (block: Replace SG_GAPS with new queue limits mask)
  static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
  					     struct bio *bio,
  					     bool no_sg_merge)
diff --cc include/linux/bio.h
index 09dfccd12025,b9b6e046b52e..000000000000
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@@ -126,42 -185,85 +126,45 @@@ static inline void *bio_data(struct bi
  	(((addr1) | (mask)) == (((addr2) - 1) | (mask)))
  #define BIOVEC_SEG_BOUNDARY(q, b1, b2) \
  	__BIO_SEG_BOUNDARY(bvec_to_phys((b1)), bvec_to_phys((b2)) + (b2)->bv_len, queue_segment_boundary((q)))
 +#define BIO_SEG_BOUNDARY(q, b1, b2) \
 +	BIOVEC_SEG_BOUNDARY((q), __BVEC_END((b1)), __BVEC_START((b2)))
  
  /*
 - * drivers should _never_ use the all version - the bio may have been split
 - * before it got to the driver and the driver won't own all of it
++<<<<<<< HEAD
 + * Check if adding a bio_vec after bprv with offset would create a gap in
 + * the SG list. Most drivers don't care about this, but some do.
   */
 -#define bio_for_each_segment_all(bvl, bio, i)				\
 -	for (i = 0, bvl = (bio)->bi_io_vec; i < (bio)->bi_vcnt; i++, bvl++)
 -
 -static inline void bvec_iter_advance(struct bio_vec *bv, struct bvec_iter *iter,
 -				     unsigned bytes)
 +static inline bool bvec_gap_to_prev(struct bio_vec *bprv, unsigned int offset)
  {
 -	WARN_ONCE(bytes > iter->bi_size,
 -		  "Attempted to advance past end of bvec iter\n");
 -
 -	while (bytes) {
 -		unsigned len = min(bytes, bvec_iter_len(bv, *iter));
 -
 -		bytes -= len;
 -		iter->bi_size -= len;
 -		iter->bi_bvec_done += len;
 -
 -		if (iter->bi_bvec_done == __bvec_iter_bvec(bv, *iter)->bv_len) {
 -			iter->bi_bvec_done = 0;
 -			iter->bi_idx++;
 -		}
 -	}
 -}
 -
 -#define for_each_bvec(bvl, bio_vec, iter, start)			\
 -	for (iter = (start);						\
 -	     (iter).bi_size &&						\
 -		((bvl = bvec_iter_bvec((bio_vec), (iter))), 1);	\
 -	     bvec_iter_advance((bio_vec), &(iter), (bvl).bv_len))
 -
 -
 -static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 -				    unsigned bytes)
 -{
 -	iter->bi_sector += bytes >> 9;
 -
 -	if (bio->bi_rw & BIO_NO_ADVANCE_ITER_MASK)
 -		iter->bi_size -= bytes;
 -	else
 -		bvec_iter_advance(bio->bi_io_vec, iter, bytes);
 +	return offset || ((bprv->bv_offset + bprv->bv_len) & (PAGE_SIZE - 1));
  }
  
 -#define __bio_for_each_segment(bvl, bio, iter, start)			\
 -	for (iter = (start);						\
 -	     (iter).bi_size &&						\
 -		((bvl = bio_iter_iovec((bio), (iter))), 1);		\
 -	     bio_advance_iter((bio), &(iter), (bvl).bv_len))
 +#define bio_io_error(bio) bio_endio((bio), -EIO)
  
 -#define bio_for_each_segment(bvl, bio, iter)				\
 -	__bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
 -
 -#define bio_iter_last(bvec, iter) ((iter).bi_size == (bvec).bv_len)
 -
 -static inline unsigned bio_segments(struct bio *bio)
 -{
 -	unsigned segs = 0;
 -	struct bio_vec bv;
 -	struct bvec_iter iter;
 -
 -	/*
 -	 * We special case discard/write same, because they interpret bi_size
 -	 * differently:
 -	 */
 -
 -	if (bio->bi_rw & REQ_DISCARD)
 -		return 1;
 -
 -	if (bio->bi_rw & REQ_WRITE_SAME)
 -		return 1;
 +/*
 + * drivers should not use the __ version unless they _really_ know what
 + * they're doing
 + */
 +#define __bio_for_each_segment(bvl, bio, i, start_idx)			\
 +	for (bvl = bio_iovec_idx((bio), (start_idx)), i = (start_idx);	\
 +	     i < (bio)->bi_vcnt;					\
 +	     bvl++, i++)
  
 -	bio_for_each_segment(bv, bio, iter)
 -		segs++;
 +/*
++=======
++>>>>>>> 03100aada96f (block: Replace SG_GAPS with new queue limits mask)
 + * drivers should _never_ use the all version - the bio may have been split
 + * before it got to the driver and the driver won't own all of it
 + */
 +#define bio_for_each_segment_all(bvl, bio, i)				\
 +	for (i = 0;							\
 +	     bvl = bio_iovec_idx((bio), (i)), i < (bio)->bi_vcnt;	\
 +	     i++)
  
 -	return segs;
 -}
 +#define bio_for_each_segment(bvl, bio, i)				\
 +	for (i = (bio)->bi_idx;						\
 +	     bvl = bio_iovec_idx((bio), (i)), i < (bio)->bi_vcnt;	\
 +	     i++)
  
  /*
   * get a reference to a bio, so it won't disappear. the intended use is
diff --cc include/linux/blkdev.h
index 0e72d45d3caf,a622f270f09e..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -295,8 -250,10 +295,9 @@@ struct blk_queue_tag 
  struct queue_limits {
  	unsigned long		bounce_pfn;
  	unsigned long		seg_boundary_mask;
+ 	unsigned long		virt_boundary_mask;
  
  	unsigned int		max_hw_sectors;
 -	unsigned int		chunk_sectors;
  	unsigned int		max_sectors;
  	unsigned int		max_segment_size;
  	unsigned int		physical_block_size;
@@@ -543,9 -479,7 +544,13 @@@ struct request_queue 
  #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
  #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
  #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
++<<<<<<< HEAD
 +#define QUEUE_FLAG_UNPRIV_SGIO 21	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NO_SG_MERGE 22	/* don't attempt to merge SG segments*/
 +#define QUEUE_FLAG_SG_GAPS     23	/* queue doesn't support SG gaps */
++=======
+ #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
++>>>>>>> 03100aada96f (block: Replace SG_GAPS with new queue limits mask)
  
  #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
  				 (1 << QUEUE_FLAG_STACKABLE)	|	\
@@@ -1070,9 -981,9 +1075,10 @@@ extern int blk_queue_dma_drain(struct r
  			       void *buf, unsigned int size);
  extern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);
  extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
+ extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
  extern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);
  extern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);
 +extern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);
  extern void blk_queue_dma_alignment(struct request_queue *, int);
  extern void blk_queue_update_dma_alignment(struct request_queue *, int);
  extern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);
* Unmerged path block/blk-merge.c
diff --git a/block/blk-settings.c b/block/blk-settings.c
index d6b328ac9170..76f0a9f457c4 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -111,6 +111,7 @@ void blk_set_default_limits(struct queue_limits *lim)
 	lim->max_segments = BLK_MAX_SEGMENTS;
 	lim->max_integrity_segments = 0;
 	lim->seg_boundary_mask = BLK_SEG_BOUNDARY_MASK;
+	lim->virt_boundary_mask = 0;
 	lim->max_segment_size = BLK_MAX_SEGMENT_SIZE;
 	lim->max_sectors = lim->max_hw_sectors = BLK_SAFE_MAX_SECTORS;
 	lim->chunk_sectors = 0;
@@ -552,6 +553,8 @@ int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 
 	t->seg_boundary_mask = min_not_zero(t->seg_boundary_mask,
 					    b->seg_boundary_mask);
+	t->virt_boundary_mask = min_not_zero(t->virt_boundary_mask,
+					    b->virt_boundary_mask);
 
 	t->max_segments = min_not_zero(t->max_segments, b->max_segments);
 	t->max_integrity_segments = min_not_zero(t->max_integrity_segments,
@@ -785,6 +788,17 @@ void blk_queue_segment_boundary(struct request_queue *q, unsigned long mask)
 }
 EXPORT_SYMBOL(blk_queue_segment_boundary);
 
+/**
+ * blk_queue_virt_boundary - set boundary rules for bio merging
+ * @q:  the request queue for the device
+ * @mask:  the memory boundary mask
+ **/
+void blk_queue_virt_boundary(struct request_queue *q, unsigned long mask)
+{
+	q->limits.virt_boundary_mask = mask;
+}
+EXPORT_SYMBOL(blk_queue_virt_boundary);
+
 /**
  * blk_queue_dma_alignment - set dma length and memory alignment
  * @q:     the request queue for the device
diff --git a/drivers/block/nvme-core.c b/drivers/block/nvme-core.c
index 2a6eb55ad96c..8604ae319e4a 100644
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@ -2120,7 +2120,6 @@ static void nvme_alloc_ns(struct nvme_dev *dev, unsigned nsid)
 		goto out_free_ns;
 	queue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, ns->queue);
 	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, ns->queue);
-	queue_flag_set_unlocked(QUEUE_FLAG_SG_GAPS, ns->queue);
 	ns->dev = dev;
 	ns->queue->queuedata = ns;
 
@@ -2144,6 +2143,7 @@ static void nvme_alloc_ns(struct nvme_dev *dev, unsigned nsid)
 		blk_queue_chunk_sectors(ns->queue, dev->stripe_size >> 9);
 	if (dev->vwc & NVME_CTRL_VWC_PRESENT)
 		blk_queue_flush(ns->queue, REQ_FLUSH | REQ_FUA);
+	blk_queue_virt_boundary(ns->queue, dev->page_size - 1);
 
 	disk->major = nvme_major;
 	disk->first_minor = 0;
diff --git a/drivers/md/dm-table.c b/drivers/md/dm-table.c
index e77ca7d91cfe..80d56a967752 100644
--- a/drivers/md/dm-table.c
+++ b/drivers/md/dm-table.c
@@ -1424,14 +1424,6 @@ static int queue_supports_sg_merge(struct dm_target *ti, struct dm_dev *dev,
 	return q && !test_bit(QUEUE_FLAG_NO_SG_MERGE, &q->queue_flags);
 }
 
-static int queue_supports_sg_gaps(struct dm_target *ti, struct dm_dev *dev,
-				  sector_t start, sector_t len, void *data)
-{
-	struct request_queue *q = bdev_get_queue(dev->bdev);
-
-	return q && !test_bit(QUEUE_FLAG_SG_GAPS, &q->queue_flags);
-}
-
 static bool dm_table_all_devices_attribute(struct dm_table *t,
 					   iterate_devices_callout_fn func)
 {
@@ -1552,11 +1544,6 @@ void dm_table_set_restrictions(struct dm_table *t, struct request_queue *q,
 	else
 		queue_flag_set_unlocked(QUEUE_FLAG_NO_SG_MERGE, q);
 
-	if (dm_table_all_devices_attribute(t, queue_supports_sg_gaps))
-		queue_flag_clear_unlocked(QUEUE_FLAG_SG_GAPS, q);
-	else
-		queue_flag_set_unlocked(QUEUE_FLAG_SG_GAPS, q);
-
 	dm_table_set_integrity(t);
 
 	/*
diff --git a/fs/bio.c b/fs/bio.c
index 746d996a349a..9132619b2c46 100644
--- a/fs/bio.c
+++ b/fs/bio.c
@@ -709,8 +709,7 @@ static int __bio_add_page(struct request_queue *q, struct bio *bio, struct page
 		 * If the queue doesn't support SG gaps and adding this
 		 * offset would create a gap, disallow it.
 		 */
-		if (q->queue_flags & (1 << QUEUE_FLAG_SG_GAPS) &&
-		    bvec_gap_to_prev(prev, offset))
+		if (bvec_gap_to_prev(q, prev, offset))
 			return 0;
 	}
 
* Unmerged path include/linux/bio.h
* Unmerged path include/linux/blkdev.h
