x86/asm/tsc: Add rdtsc_ordered() and use it in trivial call sites

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] tsc: Add rdtsc_ordered() and use it in trivial call sites (Prarit Bhargava) [1302325]
Rebuild_FUZZ: 93.44%
commit-author Andy Lutomirski <luto@kernel.org>
commit 03b9730b769fc4d87e40f6104f4c5b2e43889f19
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/03b9730b.failed

rdtsc_barrier(); rdtsc() is an unnecessary mouthful and requires
more thought than should be necessary. Add an rdtsc_ordered()
helper and replace the trivial call sites with it.

This should not change generated code. The duplication of the
fence asm is temporary.

	Signed-off-by: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Huang Rui <ray.huang@amd.com>
	Cc: John Stultz <john.stultz@linaro.org>
	Cc: Len Brown <lenb@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: kvm ML <kvm@vger.kernel.org>
Link: http://lkml.kernel.org/r/dddbf98a2af53312e9aa73a5a2b1622fe5d6f52b.1434501121.git.luto@kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 03b9730b769fc4d87e40f6104f4c5b2e43889f19)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/trace_clock.c
#	arch/x86/kvm/x86.c
#	arch/x86/lib/delay.c
#	arch/x86/vdso/vclock_gettime.c
diff --cc arch/x86/kernel/trace_clock.c
index 25b993729f9b,80bb24d9b880..000000000000
--- a/arch/x86/kernel/trace_clock.c
+++ b/arch/x86/kernel/trace_clock.c
@@@ -12,10 -12,5 +12,14 @@@
   */
  u64 notrace trace_clock_x86_tsc(void)
  {
++<<<<<<< HEAD
 +	u64 ret;
 +
 +	rdtsc_barrier();
 +	rdtscll(ret);
 +
 +	return ret;
++=======
+ 	return rdtsc_ordered();
++>>>>>>> 03b9730b769f (x86/asm/tsc: Add rdtsc_ordered() and use it in trivial call sites)
  }
diff --cc arch/x86/kvm/x86.c
index 0df755714887,8d73ec8a2364..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1417,20 -1444,8 +1417,25 @@@ EXPORT_SYMBOL_GPL(kvm_write_tsc)
  
  static cycle_t read_tsc(void)
  {
++<<<<<<< HEAD
 +	cycle_t ret;
 +	u64 last;
 +
 +	/*
 +	 * Empirically, a fence (of type that depends on the CPU)
 +	 * before rdtsc is enough to ensure that rdtsc is ordered
 +	 * with respect to loads.  The various CPU manuals are unclear
 +	 * as to whether rdtsc can be reordered with later loads,
 +	 * but no one has ever seen it happen.
 +	 */
 +	rdtsc_barrier();
 +	ret = (cycle_t)vget_cycles();
 +
 +	last = pvclock_gtod_data.clock.cycle_last;
++=======
+ 	cycle_t ret = (cycle_t)rdtsc_ordered();
+ 	u64 last = pvclock_gtod_data.clock.cycle_last;
++>>>>>>> 03b9730b769f (x86/asm/tsc: Add rdtsc_ordered() and use it in trivial call sites)
  
  	if (likely(ret >= last))
  		return ret;
diff --cc arch/x86/lib/delay.c
index e2dfb7e87c55,4453d52a143d..000000000000
--- a/arch/x86/lib/delay.c
+++ b/arch/x86/lib/delay.c
@@@ -54,11 -54,9 +54,17 @@@ static void delay_tsc(unsigned long __l
  
  	preempt_disable();
  	cpu = smp_processor_id();
++<<<<<<< HEAD
 +	rdtsc_barrier();
 +	bclock = native_read_tsc();
 +	for (;;) {
 +		rdtsc_barrier();
 +		now = native_read_tsc();
++=======
+ 	bclock = rdtsc_ordered();
+ 	for (;;) {
+ 		now = rdtsc_ordered();
++>>>>>>> 03b9730b769f (x86/asm/tsc: Add rdtsc_ordered() and use it in trivial call sites)
  		if ((now - bclock) >= loops)
  			break;
  
@@@ -79,8 -77,7 +85,12 @@@
  		if (unlikely(cpu != smp_processor_id())) {
  			loops -= (now - bclock);
  			cpu = smp_processor_id();
++<<<<<<< HEAD
 +			rdtsc_barrier();
 +			bclock = native_read_tsc();
++=======
+ 			bclock = rdtsc_ordered();
++>>>>>>> 03b9730b769f (x86/asm/tsc: Add rdtsc_ordered() and use it in trivial call sites)
  		}
  	}
  	preempt_enable();
diff --cc arch/x86/vdso/vclock_gettime.c
index 72074d528400,ca94fa649251..000000000000
--- a/arch/x86/vdso/vclock_gettime.c
+++ b/arch/x86/vdso/vclock_gettime.c
@@@ -150,6 -162,36 +150,39 @@@ notrace static long vdso_fallback_gtod(
  	return ret;
  }
  
++<<<<<<< HEAD:arch/x86/vdso/vclock_gettime.c
++=======
+ #ifdef CONFIG_PARAVIRT_CLOCK
+ 
+ static notrace cycle_t vread_pvclock(int *mode)
+ {
+ 	*mode = VCLOCK_NONE;
+ 	return 0;
+ }
+ #endif
+ 
+ #endif
+ 
+ notrace static cycle_t vread_tsc(void)
+ {
+ 	cycle_t ret = (cycle_t)rdtsc_ordered();
+ 	u64 last = gtod->cycle_last;
+ 
+ 	if (likely(ret >= last))
+ 		return ret;
+ 
+ 	/*
+ 	 * GCC likes to generate cmov here, but this branch is extremely
+ 	 * predictable (it's just a funciton of time and the likely is
+ 	 * very likely) and there's a data dependence, so force GCC
+ 	 * to generate a branch instead.  I don't barrier() because
+ 	 * we don't actually need a barrier, and if this function
+ 	 * ever gets inlined it will generate worse code.
+ 	 */
+ 	asm volatile ("");
+ 	return last;
+ }
++>>>>>>> 03b9730b769f (x86/asm/tsc: Add rdtsc_ordered() and use it in trivial call sites):arch/x86/entry/vdso/vclock_gettime.c
  
  notrace static inline u64 vgetsns(int *mode)
  {
diff --git a/arch/x86/include/asm/msr.h b/arch/x86/include/asm/msr.h
index de36f22eb0b9..914280089e45 100644
--- a/arch/x86/include/asm/msr.h
+++ b/arch/x86/include/asm/msr.h
@@ -119,6 +119,32 @@ static __always_inline unsigned long long __native_read_tsc(void)
 	return EAX_EDX_VAL(val, low, high);
 }
 
+/**
+ * rdtsc_ordered() - read the current TSC in program order
+ *
+ * rdtsc_ordered() returns the result of RDTSC as a 64-bit integer.
+ * It is ordered like a load to a global in-memory counter.  It should
+ * be impossible to observe non-monotonic rdtsc_unordered() behavior
+ * across multiple CPUs as long as the TSC is synced.
+ */
+static __always_inline unsigned long long rdtsc_ordered(void)
+{
+	/*
+	 * The RDTSC instruction is not ordered relative to memory
+	 * access.  The Intel SDM and the AMD APM are both vague on this
+	 * point, but empirically an RDTSC instruction can be
+	 * speculatively executed before prior loads.  An RDTSC
+	 * immediately after an appropriate barrier appears to be
+	 * ordered as a normal load, that is, it provides the same
+	 * ordering guarantees as reading from a global memory location
+	 * that some other imaginary CPU is updating continuously with a
+	 * time stamp.
+	 */
+	alternative_2("", "mfence", X86_FEATURE_MFENCE_RDTSC,
+			  "lfence", X86_FEATURE_LFENCE_RDTSC);
+	return rdtsc();
+}
+
 static inline unsigned long long native_read_pmc(int counter)
 {
 	DECLARE_ARGS(val, low, high);
* Unmerged path arch/x86/kernel/trace_clock.c
* Unmerged path arch/x86/kvm/x86.c
* Unmerged path arch/x86/lib/delay.c
* Unmerged path arch/x86/vdso/vclock_gettime.c
