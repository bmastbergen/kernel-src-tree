sched/balancing: Fix cfs_rq->task_h_load calculation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [kernel] sched: Fix cfs_rq->task_h_load calculation (Jiri Olsa) [1211784]
Rebuild_FUZZ: 89.36%
commit-author Vladimir Davydov <vdavydov@parallels.com>
commit 7e3115ef5149fc502e3a2e80719dba54a8e7409d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/7e3115ef.failed

Patch a003a2 (sched: Consider runnable load average in move_tasks())
sets all top-level cfs_rqs' h_load to rq->avg.load_avg_contrib, which is
always 0. This mistype leads to all tasks having weight 0 when load
balancing in a cpu-cgroup enabled setup. There obviously should be sum
of weights of all runnable tasks there instead. Fix it.

	Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
	Reviewed-by: Paul Turner <pjt@google.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1379173186-11944-1-git-send-email-vdavydov@parallels.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 7e3115ef5149fc502e3a2e80719dba54a8e7409d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index e5b7e88448c9,7c70201fbc61..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5250,37 -4223,37 +5250,53 @@@ static void update_blocked_averages(in
   * This needs to be done in a top-down fashion because the load of a child
   * group is a fraction of its parents load.
   */
 -static void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)
 +static int tg_load_down(struct task_group *tg, void *data)
  {
 -	struct rq *rq = rq_of(cfs_rq);
 -	struct sched_entity *se = cfs_rq->tg->se[cpu_of(rq)];
 -	unsigned long now = jiffies;
  	unsigned long load;
 +	long cpu = (long)data;
 +
 +	if (!tg->parent) {
 +		load = cpu_rq(cpu)->load.weight;
 +	} else {
 +		load = tg->parent->cfs_rq[cpu]->h_load;
 +		load *= tg->se[cpu]->load.weight;
 +		load /= tg->parent->cfs_rq[cpu]->load.weight + 1;
 +	}
 +
 +	tg->cfs_rq[cpu]->h_load = load;
 +
 +	return 0;
 +}
 +
 +static void update_h_load(long cpu)
 +{
 +	struct rq *rq = cpu_rq(cpu);
 +	unsigned long now = jiffies;
  
 -	if (cfs_rq->last_h_load_update == now)
 +	if (rq->h_load_throttle == now)
  		return;
  
 -	cfs_rq->h_load_next = NULL;
 -	for_each_sched_entity(se) {
 -		cfs_rq = cfs_rq_of(se);
 -		cfs_rq->h_load_next = se;
 -		if (cfs_rq->last_h_load_update == now)
 -			break;
 -	}
 +	rq->h_load_throttle = now;
  
++<<<<<<< HEAD
 +	rcu_read_lock();
 +	walk_tg_tree(tg_load_down, tg_nop, (void *)cpu);
 +	rcu_read_unlock();
++=======
+ 	if (!se) {
+ 		cfs_rq->h_load = cfs_rq->runnable_load_avg;
+ 		cfs_rq->last_h_load_update = now;
+ 	}
+ 
+ 	while ((se = cfs_rq->h_load_next) != NULL) {
+ 		load = cfs_rq->h_load;
+ 		load = div64_ul(load * se->avg.load_avg_contrib,
+ 				cfs_rq->runnable_load_avg + 1);
+ 		cfs_rq = group_cfs_rq(se);
+ 		cfs_rq->h_load = load;
+ 		cfs_rq->last_h_load_update = now;
+ 	}
++>>>>>>> 7e3115ef5149 (sched/balancing: Fix cfs_rq->task_h_load calculation)
  }
  
  static unsigned long task_h_load(struct task_struct *p)
* Unmerged path kernel/sched/fair.c
