mm: save soft-dirty bits on swapped pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] save soft-dirty bits on swapped pages (Oleg Nesterov) [1269561]
Rebuild_FUZZ: 94.87%
commit-author Cyrill Gorcunov <gorcunov@gmail.com>
commit 179ef71cbc085252e3fe6b8159263a7ed1d88ea4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/179ef71c.failed

Andy Lutomirski reported that if a page with _PAGE_SOFT_DIRTY bit set
get swapped out, the bit is getting lost and no longer available when
pte read back.

To resolve this we introduce _PTE_SWP_SOFT_DIRTY bit which is saved in
pte entry for the page being swapped out.  When such page is to be read
back from a swap cache we check for bit presence and if it's there we
clear it and restore the former _PAGE_SOFT_DIRTY bit back.

One of the problem was to find a place in pte entry where we can save
the _PTE_SWP_SOFT_DIRTY bit while page is in swap.  The _PAGE_PSE was
chosen for that, it doesn't intersect with swap entry format stored in
pte.

	Reported-by: Andy Lutomirski <luto@amacapital.net>
	Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
	Acked-by: Pavel Emelyanov <xemul@parallels.com>
	Cc: Matt Mackall <mpm@selenic.com>
	Cc: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
	Cc: Marcelo Tosatti <mtosatti@redhat.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@gmail.com>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Reviewed-by: Minchan Kim <minchan@kernel.org>
	Reviewed-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 179ef71cbc085252e3fe6b8159263a7ed1d88ea4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/pgtable.h
#	arch/x86/include/asm/pgtable_types.h
#	fs/proc/task_mmu.c
#	include/asm-generic/pgtable.h
diff --cc arch/x86/include/asm/pgtable.h
index baa6c1dd0ecb,bd0518a7f197..000000000000
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@@ -295,6 -294,41 +295,44 @@@ static inline pmd_t pmd_mknotpresent(pm
  	return pmd_clear_flags(pmd, _PAGE_PRESENT);
  }
  
++<<<<<<< HEAD
++=======
+ static inline int pte_soft_dirty(pte_t pte)
+ {
+ 	return pte_flags(pte) & _PAGE_SOFT_DIRTY;
+ }
+ 
+ static inline int pmd_soft_dirty(pmd_t pmd)
+ {
+ 	return pmd_flags(pmd) & _PAGE_SOFT_DIRTY;
+ }
+ 
+ static inline pte_t pte_mksoft_dirty(pte_t pte)
+ {
+ 	return pte_set_flags(pte, _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pmd_t pmd_mksoft_dirty(pmd_t pmd)
+ {
+ 	return pmd_set_flags(pmd, _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
+ {
+ 	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+ }
+ 
+ static inline int pte_swp_soft_dirty(pte_t pte)
+ {
+ 	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
+ }
+ 
+ static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
+ {
+ 	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+ }
+ 
++>>>>>>> 179ef71cbc08 (mm: save soft-dirty bits on swapped pages)
  /*
   * Mask out unsupported bits in a present pgprot.  Non-present pgprots
   * can use those bits for other purposes, so leave them be.
diff --cc arch/x86/include/asm/pgtable_types.h
index 35216aeb100f,5e8442f178f9..000000000000
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@@ -57,6 -55,31 +57,34 @@@
  #define _PAGE_HIDDEN	(_AT(pteval_t, 0))
  #endif
  
++<<<<<<< HEAD
++=======
+ /*
+  * The same hidden bit is used by kmemcheck, but since kmemcheck
+  * works on kernel pages while soft-dirty engine on user space,
+  * they do not conflict with each other.
+  */
+ 
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ #define _PAGE_SOFT_DIRTY	(_AT(pteval_t, 1) << _PAGE_BIT_HIDDEN)
+ #else
+ #define _PAGE_SOFT_DIRTY	(_AT(pteval_t, 0))
+ #endif
+ 
+ /*
+  * Tracking soft dirty bit when a page goes to a swap is tricky.
+  * We need a bit which can be stored in pte _and_ not conflict
+  * with swap entry format. On x86 bits 6 and 7 are *not* involved
+  * into swap entry computation, but bit 6 is used for nonlinear
+  * file mapping, so we borrow bit 7 for soft dirty tracking.
+  */
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ #define _PAGE_SWP_SOFT_DIRTY	_PAGE_PSE
+ #else
+ #define _PAGE_SWP_SOFT_DIRTY	(_AT(pteval_t, 0))
+ #endif
+ 
++>>>>>>> 179ef71cbc08 (mm: save soft-dirty bits on swapped pages)
  #if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)
  #define _PAGE_NX	(_AT(pteval_t, 1) << _PAGE_BIT_NX)
  #else
diff --cc fs/proc/task_mmu.c
index 452c618feb42,e2d9bdce5e7e..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -703,8 -716,32 +703,34 @@@ enum clear_refs_types 
  
  struct clear_refs_private {
  	struct vm_area_struct *vma;
 -	enum clear_refs_types type;
  };
  
++<<<<<<< HEAD
++=======
+ static inline void clear_soft_dirty(struct vm_area_struct *vma,
+ 		unsigned long addr, pte_t *pte)
+ {
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ 	/*
+ 	 * The soft-dirty tracker uses #PF-s to catch writes
+ 	 * to pages, so write-protect the pte as well. See the
+ 	 * Documentation/vm/soft-dirty.txt for full description
+ 	 * of how soft-dirty works.
+ 	 */
+ 	pte_t ptent = *pte;
+ 
+ 	if (pte_present(ptent)) {
+ 		ptent = pte_wrprotect(ptent);
+ 		ptent = pte_clear_flags(ptent, _PAGE_SOFT_DIRTY);
+ 	} else if (is_swap_pte(ptent)) {
+ 		ptent = pte_swp_clear_soft_dirty(ptent);
+ 	}
+ 
+ 	set_pte_at(vma->vm_mm, addr, pte, ptent);
+ #endif
+ }
+ 
++>>>>>>> 179ef71cbc08 (mm: save soft-dirty bits on swapped pages)
  static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,
  				unsigned long end, struct mm_walk *walk)
  {
@@@ -721,9 -758,15 +747,18 @@@
  	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
  	for (; addr != end; pte++, addr += PAGE_SIZE) {
  		ptent = *pte;
+ 
++<<<<<<< HEAD
++=======
+ 		if (cp->type == CLEAR_REFS_SOFT_DIRTY) {
+ 			clear_soft_dirty(vma, addr, pte);
+ 			continue;
+ 		}
+ 
  		if (!pte_present(ptent))
  			continue;
  
++>>>>>>> 179ef71cbc08 (mm: save soft-dirty bits on swapped pages)
  		page = vm_normal_page(vma, addr, ptent);
  		if (!page)
  			continue;
diff --cc include/asm-generic/pgtable.h
index 34b45521969b,2a7e0d10ad9a..000000000000
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@@ -393,6 -397,43 +393,46 @@@ static inline void ptep_modify_prot_com
  #define arch_start_context_switch(prev)	do {} while (0)
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifndef CONFIG_HAVE_ARCH_SOFT_DIRTY
+ static inline int pte_soft_dirty(pte_t pte)
+ {
+ 	return 0;
+ }
+ 
+ static inline int pmd_soft_dirty(pmd_t pmd)
+ {
+ 	return 0;
+ }
+ 
+ static inline pte_t pte_mksoft_dirty(pte_t pte)
+ {
+ 	return pte;
+ }
+ 
+ static inline pmd_t pmd_mksoft_dirty(pmd_t pmd)
+ {
+ 	return pmd;
+ }
+ 
+ static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
+ {
+ 	return pte;
+ }
+ 
+ static inline int pte_swp_soft_dirty(pte_t pte)
+ {
+ 	return 0;
+ }
+ 
+ static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
+ {
+ 	return pte;
+ }
+ #endif
+ 
++>>>>>>> 179ef71cbc08 (mm: save soft-dirty bits on swapped pages)
  #ifndef __HAVE_PFNMAP_TRACKING
  /*
   * Interfaces that can be used by architecture code to keep track of
* Unmerged path arch/x86/include/asm/pgtable.h
* Unmerged path arch/x86/include/asm/pgtable_types.h
* Unmerged path fs/proc/task_mmu.c
* Unmerged path include/asm-generic/pgtable.h
diff --git a/include/linux/swapops.h b/include/linux/swapops.h
index dff14c873903..c0f75261a728 100644
--- a/include/linux/swapops.h
+++ b/include/linux/swapops.h
@@ -67,6 +67,8 @@ static inline swp_entry_t pte_to_swp_entry(pte_t pte)
 	swp_entry_t arch_entry;
 
 	BUG_ON(pte_file(pte));
+	if (pte_swp_soft_dirty(pte))
+		pte = pte_swp_clear_soft_dirty(pte);
 	arch_entry = __pte_to_swp_entry(pte);
 	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
 }
diff --git a/mm/memory.c b/mm/memory.c
index fce51319197b..503e78b36004 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3330,6 +3330,8 @@ static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		exclusive = 1;
 	}
 	flush_icache_page(vma, page);
+	if (pte_swp_soft_dirty(orig_pte))
+		pte = pte_mksoft_dirty(pte);
 	set_pte_at(mm, address, page_table, pte);
 	if (page == swapcache)
 		do_page_add_anon_rmap(page, vma, address, exclusive);
diff --git a/mm/rmap.c b/mm/rmap.c
index 8a1f7d7fc267..90328d53cc9e 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -1234,6 +1234,7 @@ int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 			   swp_entry_to_pte(make_hwpoison_entry(page)));
 	} else if (PageAnon(page)) {
 		swp_entry_t entry = { .val = page_private(page) };
+		pte_t swp_pte;
 
 		if (PageSwapCache(page)) {
 			/*
@@ -1262,7 +1263,10 @@ int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 			BUG_ON(TTU_ACTION(flags) != TTU_MIGRATION);
 			entry = make_migration_entry(page, pte_write(pteval));
 		}
-		set_pte_at(mm, address, pte, swp_entry_to_pte(entry));
+		swp_pte = swp_entry_to_pte(entry);
+		if (pte_soft_dirty(pteval))
+			swp_pte = pte_swp_mksoft_dirty(swp_pte);
+		set_pte_at(mm, address, pte, swp_pte);
 		BUG_ON(pte_file(*pte));
 	} else if (IS_ENABLED(CONFIG_MIGRATION) &&
 		   (TTU_ACTION(flags) == TTU_MIGRATION)) {
diff --git a/mm/swapfile.c b/mm/swapfile.c
index 7c328d1c43b7..9cb1769b2bc1 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -870,6 +870,21 @@ unsigned int count_swap_pages(int type, int free)
 }
 #endif /* CONFIG_HIBERNATION */
 
+static inline int maybe_same_pte(pte_t pte, pte_t swp_pte)
+{
+#ifdef CONFIG_MEM_SOFT_DIRTY
+	/*
+	 * When pte keeps soft dirty bit the pte generated
+	 * from swap entry does not has it, still it's same
+	 * pte from logical point of view.
+	 */
+	pte_t swp_pte_dirty = pte_swp_mksoft_dirty(swp_pte);
+	return pte_same(pte, swp_pte) || pte_same(pte, swp_pte_dirty);
+#else
+	return pte_same(pte, swp_pte);
+#endif
+}
+
 /*
  * No need to decide whether this PTE shares the swap entry with others,
  * just let do_wp_page work it out if a write is requested later - to
@@ -896,7 +911,7 @@ static int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,
 	}
 
 	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
-	if (unlikely(!pte_same(*pte, swp_entry_to_pte(entry)))) {
+	if (unlikely(!maybe_same_pte(*pte, swp_entry_to_pte(entry)))) {
 		mem_cgroup_cancel_charge_swapin(memcg);
 		ret = 0;
 		goto out;
@@ -951,7 +966,7 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 		 * swapoff spends a _lot_ of time in this loop!
 		 * Test inline before going to call unuse_pte.
 		 */
-		if (unlikely(pte_same(*pte, swp_pte))) {
+		if (unlikely(maybe_same_pte(*pte, swp_pte))) {
 			pte_unmap(pte);
 			ret = unuse_pte(vma, pmd, addr, entry, page);
 			if (ret)
