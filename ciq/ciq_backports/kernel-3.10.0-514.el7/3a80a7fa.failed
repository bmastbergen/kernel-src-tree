mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set (George Beshers) [727269]
Rebuild_FUZZ: 97.75%
commit-author Mel Gorman <mgorman@suse.de>
commit 3a80a7fa7989fbb6aa56bb6ad31811b62cf99e60
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/3a80a7fa.failed

This patch initalises all low memory struct pages and 2G of the highest
zone on each node during memory initialisation if
CONFIG_DEFERRED_STRUCT_PAGE_INIT is set.  That config option cannot be set
but will be available in a later patch.  Parallel initialisation of struct
page depends on some features from memory hotplug and it is necessary to
alter alter section annotations.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Nate Zimmer <nzimmer@sgi.com>
	Tested-by: Waiman Long <waiman.long@hp.com>
	Tested-by: Daniel J Blueman <daniel@numascale.com>
	Acked-by: Pekka Enberg <penberg@kernel.org>
	Cc: Robin Holt <robinmholt@gmail.com>
	Cc: Nate Zimmer <nzimmer@sgi.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Waiman Long <waiman.long@hp.com>
	Cc: Scott Norton <scott.norton@hp.com>
	Cc: "Luck, Tony" <tony.luck@intel.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3a80a7fa7989fbb6aa56bb6ad31811b62cf99e60)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmzone.h
#	mm/Kconfig
#	mm/page_alloc.c
diff --cc include/linux/mmzone.h
index 2d47b2da44e8,754c25966a0a..000000000000
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@@ -791,12 -763,13 +791,22 @@@ typedef struct pglist_data 
  	unsigned long numabalancing_migrate_nr_pages;
  #endif
  
++<<<<<<< HEAD
 +	/* reserved for Red Hat */
 +	RH_KABI_RESERVE(1)
 +	RH_KABI_RESERVE(2)
 +	RH_KABI_RESERVE(3)
 +	RH_KABI_RESERVE(4)
 +
++=======
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ 	/*
+ 	 * If memory initialisation on large machines is deferred then this
+ 	 * is the first PFN that needs to be initialised.
+ 	 */
+ 	unsigned long first_deferred_pfn;
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
++>>>>>>> 3a80a7fa7989 (mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set)
  } pg_data_t;
  
  #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
diff --cc mm/Kconfig
index 88ac087b6ee9,e79de2bd12cd..000000000000
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@@ -572,3 -608,49 +572,52 @@@ config PGTABLE_MAPPIN
  
  	  You can check speed with zsmalloc benchmark:
  	  https://github.com/spartacus06/zsmapbench
++<<<<<<< HEAD
++=======
+ 
+ config ZSMALLOC_STAT
+ 	bool "Export zsmalloc statistics"
+ 	depends on ZSMALLOC
+ 	select DEBUG_FS
+ 	help
+ 	  This option enables code in the zsmalloc to collect various
+ 	  statistics about whats happening in zsmalloc and exports that
+ 	  information to userspace via debugfs.
+ 	  If unsure, say N.
+ 
+ config GENERIC_EARLY_IOREMAP
+ 	bool
+ 
+ config MAX_STACK_SIZE_MB
+ 	int "Maximum user stack size for 32-bit processes (MB)"
+ 	default 80
+ 	range 8 256 if METAG
+ 	range 8 2048
+ 	depends on STACK_GROWSUP && (!64BIT || COMPAT)
+ 	help
+ 	  This is the maximum stack size in Megabytes in the VM layout of 32-bit
+ 	  user processes when the stack grows upwards (currently only on parisc
+ 	  and metag arch). The stack will be located at the highest memory
+ 	  address minus the given value, unless the RLIMIT_STACK hard limit is
+ 	  changed to a smaller value in which case that is used.
+ 
+ 	  A sane initial value is 80 MB.
+ 
+ # For architectures that support deferred memory initialisation
+ config ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	bool
+ 
+ config DEFERRED_STRUCT_PAGE_INIT
+ 	bool "Defer initialisation of struct pages to kswapd"
+ 	default n
+ 	depends on ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	depends on MEMORY_HOTPLUG
+ 	help
+ 	  Ordinarily all struct pages are initialised during early boot in a
+ 	  single thread. On very large machines this can take a considerable
+ 	  amount of time. If this option is set, large machines will bring up
+ 	  a subset of memmap at boot and then initialise the rest in parallel
+ 	  when kswapd starts. This has a potential performance impact on
+ 	  processes running early in the lifetime of the systemm until kswapd
+ 	  finishes the initialisation.
++>>>>>>> 3a80a7fa7989 (mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set)
diff --cc mm/page_alloc.c
index 20d353397e7d,7af45b2e8870..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -229,10 -235,68 +229,68 @@@ EXPORT_SYMBOL(nr_online_nodes)
  
  int page_group_by_mobility_disabled __read_mostly;
  
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static inline void reset_deferred_meminit(pg_data_t *pgdat)
+ {
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ }
+ 
+ /* Returns true if the struct page for the pfn is uninitialised */
+ static inline bool __defermem_init early_page_uninitialised(unsigned long pfn)
+ {
+ 	int nid = early_pfn_to_nid(pfn);
+ 
+ 	if (pfn >= NODE_DATA(nid)->first_deferred_pfn)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Returns false when the remaining initialisation should be deferred until
+  * later in the boot cycle when it can be parallelised.
+  */
+ static inline bool update_defer_init(pg_data_t *pgdat,
+ 				unsigned long pfn, unsigned long zone_end,
+ 				unsigned long *nr_initialised)
+ {
+ 	/* Always populate low zones for address-contrained allocations */
+ 	if (zone_end < pgdat_end_pfn(pgdat))
+ 		return true;
+ 
+ 	/* Initialise at least 2G of the highest zone */
+ 	(*nr_initialised)++;
+ 	if (*nr_initialised > (2UL << (30 - PAGE_SHIFT)) &&
+ 	    (pfn & (PAGES_PER_SECTION - 1)) == 0) {
+ 		pgdat->first_deferred_pfn = pfn;
+ 		return false;
+ 	}
+ 
+ 	return true;
+ }
+ #else
+ static inline void reset_deferred_meminit(pg_data_t *pgdat)
+ {
+ }
+ 
+ static inline bool early_page_uninitialised(unsigned long pfn)
+ {
+ 	return false;
+ }
+ 
+ static inline bool update_defer_init(pg_data_t *pgdat,
+ 				unsigned long pfn, unsigned long zone_end,
+ 				unsigned long *nr_initialised)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
  void set_pageblock_migratetype(struct page *page, int migratetype)
  {
 -	if (unlikely(page_group_by_mobility_disabled &&
 -		     migratetype < MIGRATE_PCPTYPES))
 +
 +	if (unlikely(page_group_by_mobility_disabled))
  		migratetype = MIGRATE_UNMOVABLE;
  
  	set_pageblock_flags_group(page, (unsigned long)migratetype,
@@@ -761,17 -936,16 +819,22 @@@ static void __free_pages_ok(struct pag
  	local_irq_restore(flags);
  }
  
++<<<<<<< HEAD
 +void __init __free_pages_bootmem(struct page *page, unsigned int order)
++=======
+ static void __defer_init __free_pages_boot_core(struct page *page,
+ 					unsigned long pfn, unsigned int order)
++>>>>>>> 3a80a7fa7989 (mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set)
  {
  	unsigned int nr_pages = 1 << order;
 -	struct page *p = page;
  	unsigned int loop;
  
 -	prefetchw(p);
 -	for (loop = 0; loop < (nr_pages - 1); loop++, p++) {
 -		prefetchw(p + 1);
 +	prefetchw(page);
 +	for (loop = 0; loop < nr_pages; loop++) {
 +		struct page *p = &page[loop];
 +
 +		if (loop + 1 < nr_pages)
 +			prefetchw(p + 1);
  		__ClearPageReserved(p);
  		set_page_count(p, 0);
  	}
@@@ -781,8 -957,68 +844,71 @@@
  	__free_pages(page, order);
  }
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) || \
+ 	defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;
+ 
+ int __meminit early_pfn_to_nid(unsigned long pfn)
+ {
+ 	int nid;
+ 
+ 	/* The system will behave unpredictably otherwise */
+ 	BUG_ON(system_state != SYSTEM_BOOTING);
+ 
+ 	nid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);
+ 	if (nid >= 0)
+ 		return nid;
+ 	/* just returns 0 */
+ 	return 0;
+ }
+ #endif
+ 
+ #ifdef CONFIG_NODES_SPAN_OTHER_NODES
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	int nid;
+ 
+ 	nid = __early_pfn_to_nid(pfn, state);
+ 	if (nid >= 0 && nid != node)
+ 		return false;
+ 	return true;
+ }
+ 
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return meminit_pfn_in_nid(pfn, node, &early_pfnnid_cache);
+ }
+ 
+ #else
+ 
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return true;
+ }
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
+ void __defer_init __free_pages_bootmem(struct page *page, unsigned long pfn,
+ 							unsigned int order)
+ {
+ 	if (early_page_uninitialised(pfn))
+ 		return;
+ 	return __free_pages_boot_core(page, pfn, order);
+ }
+ 
++>>>>>>> 3a80a7fa7989 (mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set)
  #ifdef CONFIG_CMA
 -/* Free whole pageblock and set its migration type to MIGRATE_CMA. */
 +/* Free whole pageblock and set it's migration type to MIGRATE_CMA. */
  void __init init_cma_reserved_pageblock(struct page *page)
  {
  	unsigned i = pageblock_nr_pages;
@@@ -4006,7 -4391,7 +4132,11 @@@ static void setup_zone_migrate_reserve(
  void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
  		unsigned long start_pfn, enum memmap_context context)
  {
++<<<<<<< HEAD
 +	struct page *page;
++=======
+ 	pg_data_t *pgdat = NODE_DATA(nid);
++>>>>>>> 3a80a7fa7989 (mm: meminit: initialise a subset of struct pages if CONFIG_DEFERRED_STRUCT_PAGE_INIT is set)
  	unsigned long end_pfn = start_pfn + size;
  	unsigned long pfn;
  	struct zone *z;
@@@ -4026,39 -4412,11 +4157,42 @@@
  				continue;
  			if (!early_pfn_in_nid(pfn, nid))
  				continue;
+ 			if (!update_defer_init(pgdat, pfn, end_pfn,
+ 						&nr_initialised))
+ 				break;
  		}
 -		__init_single_pfn(pfn, zone, nid);
 +		page = pfn_to_page(pfn);
 +		set_page_links(page, zone, nid, pfn);
 +		mminit_verify_page_links(page, zone, nid, pfn);
 +		init_page_count(page);
 +		page_mapcount_reset(page);
 +		page_cpupid_reset_last(page);
 +		SetPageReserved(page);
 +		/*
 +		 * Mark the block movable so that blocks are reserved for
 +		 * movable at startup. This will force kernel allocations
 +		 * to reserve their blocks rather than leaking throughout
 +		 * the address space during boot when many long-lived
 +		 * kernel allocations are made. Later some blocks near
 +		 * the start are marked MIGRATE_RESERVE by
 +		 * setup_zone_migrate_reserve()
 +		 *
 +		 * bitmap is created for zone's valid pfn range. but memmap
 +		 * can be created for invalid pages (for alignment)
 +		 * check here not to call set_pageblock_migratetype() against
 +		 * pfn out of zone.
 +		 */
 +		if ((z->zone_start_pfn <= pfn)
 +		    && (pfn < zone_end_pfn(z))
 +		    && !(pfn & (pageblock_nr_pages - 1)))
 +			set_pageblock_migratetype(page, MIGRATE_MOVABLE);
 +
 +		INIT_LIST_HEAD(&page->lru);
 +#ifdef WANT_PAGE_VIRTUAL
 +		/* The shift won't overflow because ZONE_NORMAL is below 4G. */
 +		if (!is_highmem_idx(zone))
 +			set_page_address(page, __va(pfn << PAGE_SHIFT));
 +#endif
  	}
  }
  
@@@ -4826,9 -5215,16 +4960,10 @@@ void __paginginit free_area_init_node(i
  	/* pg_data_t should be reset to zero when it's allocated */
  	WARN_ON(pgdat->nr_zones || pgdat->classzone_idx);
  
+ 	reset_deferred_meminit(pgdat);
  	pgdat->node_id = nid;
  	pgdat->node_start_pfn = node_start_pfn;
 -#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 -	get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
 -	pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,
 -		(u64)start_pfn << PAGE_SHIFT, ((u64)end_pfn << PAGE_SHIFT) - 1);
 -#endif
 -	calculate_node_totalpages(pgdat, start_pfn, end_pfn,
 -				  zones_size, zholes_size);
 +	calculate_node_totalpages(pgdat, zones_size, zholes_size);
  
  	alloc_node_mem_map(pgdat);
  #ifdef CONFIG_FLAT_NODE_MEM_MAP
diff --git a/drivers/base/node.c b/drivers/base/node.c
index 2d6e698d4eba..e53bd8a17b27 100644
--- a/drivers/base/node.c
+++ b/drivers/base/node.c
@@ -373,12 +373,16 @@ int unregister_cpu_under_node(unsigned int cpu, unsigned int nid)
 #ifdef CONFIG_MEMORY_HOTPLUG_SPARSE
 #define page_initialized(page)  (page->lru.next)
 
-static int get_nid_for_pfn(unsigned long pfn)
+static int __init_refok get_nid_for_pfn(unsigned long pfn)
 {
 	struct page *page;
 
 	if (!pfn_valid_within(pfn))
 		return -1;
+#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+	if (system_state == SYSTEM_BOOTING)
+		return early_pfn_to_nid(pfn);
+#endif
 	page = pfn_to_page(pfn);
 	if (!page_initialized(page))
 		return -1;
* Unmerged path include/linux/mmzone.h
* Unmerged path mm/Kconfig
diff --git a/mm/internal.h b/mm/internal.h
index d07fa9595ecf..5507b609fd9d 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -326,6 +326,24 @@ static inline void mminit_verify_zonelist(void)
 }
 #endif /* CONFIG_DEBUG_MEMORY_INIT */
 
+/*
+ * Deferred struct page initialisation requires init functions that are freed
+ * before kswapd is available. Reuse the memory hotplug section annotation
+ * to mark the required code.
+ *
+ * __defermem_init is code that always exists but is annotated __meminit to
+ * 	avoid section warnings.
+ * __defer_init code gets marked __meminit when deferring struct page
+ *	initialistion but is otherwise in the init section.
+ */
+#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+#define __defermem_init __meminit
+#define __defer_init    __meminit
+#else
+#define __defermem_init
+#define __defer_init __init
+#endif
+
 /* mminit_validate_memmodel_limits is independent of CONFIG_DEBUG_MEMORY_INIT */
 #if defined(CONFIG_SPARSEMEM)
 extern void mminit_validate_memmodel_limits(unsigned long *start_pfn,
* Unmerged path mm/page_alloc.c
