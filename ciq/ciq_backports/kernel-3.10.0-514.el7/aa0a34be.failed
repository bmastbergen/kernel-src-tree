hv_netvsc: Implement partial copy into send buffer

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Haiyang Zhang <haiyangz@microsoft.com>
commit aa0a34be68290aa9aa071c0691fb8b6edda38358
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/aa0a34be.failed

If remaining space in a send buffer slot is too small for the whole message,
we only copy the RNDIS header and PPI data into send buffer, so we can batch
one more packet each time. It reduces the vmbus per-message overhead.

	Signed-off-by: Haiyang Zhang <haiyangz@microsoft.com>
	Reviewed-by: K. Y. Srinivasan <kys@microsoft.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit aa0a34be68290aa9aa071c0691fb8b6edda38358)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/hyperv/hyperv_net.h
#	drivers/net/hyperv/netvsc.c
#	drivers/net/hyperv/netvsc_drv.c
diff --cc drivers/net/hyperv/hyperv_net.h
index 3362765215d6,a10b31664709..000000000000
--- a/drivers/net/hyperv/hyperv_net.h
+++ b/drivers/net/hyperv/hyperv_net.h
@@@ -129,8 -128,12 +129,14 @@@ struct ndis_tcp_ip_checksum_info
  struct hv_netvsc_packet {
  	/* Bookkeeping stuff */
  	u32 status;
 -	bool part_of_skb;
  
  	bool is_data_pkt;
++<<<<<<< HEAD
++=======
+ 	bool xmit_more; /* from skb */
+ 	bool cp_partial; /* partial copy into send buffer */
+ 
++>>>>>>> aa0a34be6829 (hv_netvsc: Implement partial copy into send buffer)
  	u16 vlan_tci;
  
  	u16 q_idx;
diff --cc drivers/net/hyperv/netvsc.c
index f6702b01e754,2e8ad0636b46..000000000000
--- a/drivers/net/hyperv/netvsc.c
+++ b/drivers/net/hyperv/netvsc.c
@@@ -686,16 -691,30 +686,31 @@@ static u32 netvsc_get_next_send_section
  	return ret_val;
  }
  
 -static u32 netvsc_copy_to_send_buf(struct netvsc_device *net_device,
 -				   unsigned int section_index,
 -				   u32 pend_size,
 -				   struct hv_netvsc_packet *packet)
 +u32 netvsc_copy_to_send_buf(struct netvsc_device *net_device,
 +			    unsigned int section_index,
 +			    struct hv_netvsc_packet *packet)
  {
  	char *start = net_device->send_buf;
 -	char *dest = start + (section_index * net_device->send_section_size)
 -		     + pend_size;
 +	char *dest = (start + (section_index * net_device->send_section_size));
  	int i;
  	u32 msg_size = 0;
++<<<<<<< HEAD
++=======
+ 	u32 padding = 0;
+ 	u32 remain = packet->total_data_buflen % net_device->pkt_align;
+ 	u32 page_count = packet->cp_partial ? packet->rmsg_pgcnt :
+ 		packet->page_buf_cnt;
+ 
+ 	/* Add padding */
+ 	if (packet->is_data_pkt && packet->xmit_more && remain &&
+ 	    !packet->cp_partial) {
+ 		padding = net_device->pkt_align - remain;
+ 		packet->rndis_msg->msg_len += padding;
+ 		packet->total_data_buflen += padding;
+ 	}
++>>>>>>> aa0a34be6829 (hv_netvsc: Implement partial copy into send buffer)
  
- 	for (i = 0; i < packet->page_buf_cnt; i++) {
+ 	for (i = 0; i < page_count; i++) {
  		char *src = phys_to_virt(packet->page_buf[i].pfn << PAGE_SHIFT);
  		u32 offset = packet->page_buf[i].offset;
  		u32 len = packet->page_buf[i].len;
@@@ -707,53 -726,40 +722,58 @@@
  	return msg_size;
  }
  
 -static inline int netvsc_send_pkt(
 -	struct hv_netvsc_packet *packet,
 -	struct netvsc_device *net_device)
 +int netvsc_send(struct hv_device *device,
 +			struct hv_netvsc_packet *packet)
  {
 -	struct nvsp_message nvmsg;
 -	struct vmbus_channel *out_channel = packet->channel;
 -	u16 q_idx = packet->q_idx;
 -	struct net_device *ndev = net_device->ndev;
 +	struct netvsc_device *net_device;
 +	int ret = 0;
 +	struct nvsp_message sendMessage;
 +	struct net_device *ndev;
 +	struct vmbus_channel *out_channel = NULL;
  	u64 req_id;
++<<<<<<< HEAD
 +	unsigned int section_index = NETVSC_INVALID_INDEX;
 +	u32 msg_size = 0;
 +	struct sk_buff *skb = NULL;
 +	u16 q_idx = packet->q_idx;
++=======
+ 	int ret;
+ 	struct hv_page_buffer *pgbuf;
++>>>>>>> aa0a34be6829 (hv_netvsc: Implement partial copy into send buffer)
 +
  
 -	nvmsg.hdr.msg_type = NVSP_MSG1_TYPE_SEND_RNDIS_PKT;
 +	net_device = get_outbound_net_device(device);
 +	if (!net_device)
 +		return -ENODEV;
 +	ndev = net_device->ndev;
 +
 +	sendMessage.hdr.msg_type = NVSP_MSG1_TYPE_SEND_RNDIS_PKT;
  	if (packet->is_data_pkt) {
  		/* 0 is RMC_DATA; */
 -		nvmsg.msg.v1_msg.send_rndis_pkt.channel_type = 0;
 +		sendMessage.msg.v1_msg.send_rndis_pkt.channel_type = 0;
  	} else {
  		/* 1 is RMC_CONTROL; */
 -		nvmsg.msg.v1_msg.send_rndis_pkt.channel_type = 1;
 +		sendMessage.msg.v1_msg.send_rndis_pkt.channel_type = 1;
  	}
  
 -	nvmsg.msg.v1_msg.send_rndis_pkt.send_buf_section_index =
 -		packet->send_buf_index;
 -	if (packet->send_buf_index == NETVSC_INVALID_INDEX)
 -		nvmsg.msg.v1_msg.send_rndis_pkt.send_buf_section_size = 0;
 -	else
 -		nvmsg.msg.v1_msg.send_rndis_pkt.send_buf_section_size =
 -			packet->total_data_buflen;
 +	/* Attempt to send via sendbuf */
 +	if (packet->total_data_buflen < net_device->send_section_size) {
 +		section_index = netvsc_get_next_send_section(net_device);
 +		if (section_index != NETVSC_INVALID_INDEX) {
 +			msg_size = netvsc_copy_to_send_buf(net_device,
 +							   section_index,
 +							   packet);
 +			skb = (struct sk_buff *)
 +			      (unsigned long)packet->send_completion_tid;
 +			packet->page_buf_cnt = 0;
 +		}
 +	}
 +	packet->send_buf_index = section_index;
 +
 +
 +	sendMessage.msg.v1_msg.send_rndis_pkt.send_buf_section_index =
 +		section_index;
 +	sendMessage.msg.v1_msg.send_rndis_pkt.send_buf_section_size = msg_size;
  
  	if (packet->send_completion)
  		req_id = (ulong)packet;
@@@ -769,10 -770,12 +789,12 @@@
  		return -ENODEV;
  
  	if (packet->page_buf_cnt) {
+ 		pgbuf = packet->cp_partial ? packet->page_buf +
+ 			packet->rmsg_pgcnt : packet->page_buf;
  		ret = vmbus_sendpacket_pagebuffer(out_channel,
- 						  packet->page_buf,
+ 						  pgbuf,
  						  packet->page_buf_cnt,
 -						  &nvmsg,
 +						  &sendMessage,
  						  sizeof(struct nvsp_message),
  						  req_id);
  	} else {
@@@ -810,6 -814,121 +832,124 @@@
  			   packet, ret);
  	}
  
++<<<<<<< HEAD
++=======
+ 	return ret;
+ }
+ 
+ int netvsc_send(struct hv_device *device,
+ 		struct hv_netvsc_packet *packet)
+ {
+ 	struct netvsc_device *net_device;
+ 	int ret = 0, m_ret = 0;
+ 	struct vmbus_channel *out_channel;
+ 	u16 q_idx = packet->q_idx;
+ 	u32 pktlen = packet->total_data_buflen, msd_len = 0;
+ 	unsigned int section_index = NETVSC_INVALID_INDEX;
+ 	struct sk_buff *skb = NULL;
+ 	unsigned long flag;
+ 	struct multi_send_data *msdp;
+ 	struct hv_netvsc_packet *msd_send = NULL, *cur_send = NULL;
+ 	bool try_batch;
+ 
+ 	net_device = get_outbound_net_device(device);
+ 	if (!net_device)
+ 		return -ENODEV;
+ 
+ 	out_channel = net_device->chn_table[q_idx];
+ 	if (!out_channel) {
+ 		out_channel = device->channel;
+ 		q_idx = 0;
+ 		packet->q_idx = 0;
+ 	}
+ 	packet->channel = out_channel;
+ 	packet->send_buf_index = NETVSC_INVALID_INDEX;
+ 	packet->cp_partial = false;
+ 
+ 	msdp = &net_device->msd[q_idx];
+ 
+ 	/* batch packets in send buffer if possible */
+ 	spin_lock_irqsave(&msdp->lock, flag);
+ 	if (msdp->pkt)
+ 		msd_len = msdp->pkt->total_data_buflen;
+ 
+ 	try_batch = packet->is_data_pkt && msd_len > 0 && msdp->count <
+ 		    net_device->max_pkt;
+ 
+ 	if (try_batch && msd_len + pktlen + net_device->pkt_align <
+ 	    net_device->send_section_size) {
+ 		section_index = msdp->pkt->send_buf_index;
+ 
+ 	} else if (try_batch && msd_len + packet->rmsg_size <
+ 		   net_device->send_section_size) {
+ 		section_index = msdp->pkt->send_buf_index;
+ 		packet->cp_partial = true;
+ 
+ 	} else if (packet->is_data_pkt && pktlen + net_device->pkt_align <
+ 		   net_device->send_section_size) {
+ 		section_index = netvsc_get_next_send_section(net_device);
+ 		if (section_index != NETVSC_INVALID_INDEX) {
+ 				msd_send = msdp->pkt;
+ 				msdp->pkt = NULL;
+ 				msdp->count = 0;
+ 				msd_len = 0;
+ 		}
+ 	}
+ 
+ 	if (section_index != NETVSC_INVALID_INDEX) {
+ 		netvsc_copy_to_send_buf(net_device,
+ 					section_index, msd_len,
+ 					packet);
+ 
+ 		packet->send_buf_index = section_index;
+ 
+ 		if (packet->cp_partial) {
+ 			packet->page_buf_cnt -= packet->rmsg_pgcnt;
+ 			packet->total_data_buflen = msd_len + packet->rmsg_size;
+ 		} else {
+ 			packet->page_buf_cnt = 0;
+ 			packet->total_data_buflen += msd_len;
+ 			if (!packet->part_of_skb) {
+ 				skb = (struct sk_buff *)(unsigned long)packet->
+ 				       send_completion_tid;
+ 				packet->send_completion_tid = 0;
+ 			}
+ 		}
+ 
+ 		if (msdp->pkt)
+ 			netvsc_xmit_completion(msdp->pkt);
+ 
+ 		if (packet->xmit_more && !packet->cp_partial) {
+ 			msdp->pkt = packet;
+ 			msdp->count++;
+ 		} else {
+ 			cur_send = packet;
+ 			msdp->pkt = NULL;
+ 			msdp->count = 0;
+ 		}
+ 	} else {
+ 		msd_send = msdp->pkt;
+ 		msdp->pkt = NULL;
+ 		msdp->count = 0;
+ 		cur_send = packet;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&msdp->lock, flag);
+ 
+ 	if (msd_send) {
+ 		m_ret = netvsc_send_pkt(msd_send, net_device);
+ 
+ 		if (m_ret != 0) {
+ 			netvsc_free_send_slot(net_device,
+ 					      msd_send->send_buf_index);
+ 			netvsc_xmit_completion(msd_send);
+ 		}
+ 	}
+ 
+ 	if (cur_send)
+ 		ret = netvsc_send_pkt(cur_send, net_device);
+ 
++>>>>>>> aa0a34be6829 (hv_netvsc: Implement partial copy into send buffer)
  	if (ret != 0) {
  		if (section_index != NETVSC_INVALID_INDEX)
  			netvsc_free_send_slot(net_device, section_index);
diff --cc drivers/net/hyperv/netvsc_drv.c
index f6bc78f79972,a3a9d3898a6e..000000000000
--- a/drivers/net/hyperv/netvsc_drv.c
+++ b/drivers/net/hyperv/netvsc_drv.c
@@@ -554,7 -582,7 +558,11 @@@ do_send
  	rndis_msg->msg_len += rndis_msg_size;
  	packet->total_data_buflen = rndis_msg->msg_len;
  	packet->page_buf_cnt = init_page_array(rndis_msg, rndis_msg_size,
++<<<<<<< HEAD
 +					skb, &packet->page_buf[0]);
++=======
+ 					       skb, packet);
++>>>>>>> aa0a34be6829 (hv_netvsc: Implement partial copy into send buffer)
  
  	ret = netvsc_send(net_device_ctx->device_ctx, packet);
  
* Unmerged path drivers/net/hyperv/hyperv_net.h
* Unmerged path drivers/net/hyperv/netvsc.c
* Unmerged path drivers/net/hyperv/netvsc_drv.c
