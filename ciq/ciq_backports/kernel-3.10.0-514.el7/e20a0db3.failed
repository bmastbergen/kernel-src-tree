net/mlx5e: Delay skb->data access

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Delay skb->data access (kamal heib) [1275159 1296272 1296405 1298421 1298422 1298423 1298424 1298425]
Rebuild_FUZZ: 93.55%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit e20a0db30454a07f03f3a34a79e9f35881cfaa9d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e20a0db3.failed

Move mlx5e_handle_csum and eth_type_trans to the end of
mlx5e_build_rx_skb to gain some more time before accessing
skb->data, to reduce cache misses.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e20a0db30454a07f03f3a34a79e9f35881cfaa9d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 2dc1f9b26b65,ee5fa16aafd1..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -203,9 -570,8 +203,14 @@@ static inline void mlx5e_build_rx_skb(s
  		rq->stats.lro_bytes += cqe_bcnt;
  	}
  
++<<<<<<< HEAD
 +	mlx5e_handle_csum(netdev, cqe, rq, skb);
 +
 +	skb->protocol = eth_type_trans(skb, netdev);
++=======
+ 	if (unlikely(mlx5e_rx_hw_stamp(tstamp)))
+ 		mlx5e_fill_hwstamp(tstamp, get_cqe_ts(cqe), skb_hwtstamps(skb));
++>>>>>>> e20a0db30454 (net/mlx5e: Delay skb->data access)
  
  	skb_record_rx_queue(skb, rq->ix);
  
@@@ -215,6 -581,141 +220,144 @@@
  	if (cqe_has_vlan(cqe))
  		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
  				       be16_to_cpu(cqe->vlan_info));
++<<<<<<< HEAD
++=======
+ 
+ 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
+ 
+ 	mlx5e_handle_csum(netdev, cqe, rq, skb, !!lro_num_seg);
+ 	skb->protocol = eth_type_trans(skb, netdev);
+ }
+ 
+ static inline void mlx5e_complete_rx_cqe(struct mlx5e_rq *rq,
+ 					 struct mlx5_cqe64 *cqe,
+ 					 u32 cqe_bcnt,
+ 					 struct sk_buff *skb)
+ {
+ 	rq->stats.packets++;
+ 	rq->stats.bytes += cqe_bcnt;
+ 	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ }
+ 
+ void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_rx_wqe *wqe;
+ 	struct sk_buff *skb;
+ 	__be16 wqe_counter_be;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	skb            = rq->skb[wqe_counter];
+ 	prefetch(skb->data);
+ 	rq->skb[wqe_counter] = NULL;
+ 
+ 	dma_unmap_single(rq->pdev,
+ 			 *((dma_addr_t *)skb->cb),
+ 			 rq->wqe_sz,
+ 			 DMA_FROM_DEVICE);
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		dev_kfree_skb(skb);
+ 		goto wq_ll_pop;
+ 	}
+ 
+ 	cqe_bcnt = be32_to_cpu(cqe->byte_cnt);
+ 	skb_put(skb, cqe_bcnt);
+ 
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ wq_ll_pop:
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ static inline void mlx5e_mpwqe_fill_rx_skb(struct mlx5e_rq *rq,
+ 					   struct mlx5_cqe64 *cqe,
+ 					   struct mlx5e_mpw_info *wi,
+ 					   u32 cqe_bcnt,
+ 					   struct sk_buff *skb)
+ {
+ 	u32 consumed_bytes = ALIGN(cqe_bcnt, MLX5_MPWRQ_STRIDE_SIZE);
+ 	u16 stride_ix      = mpwrq_get_cqe_stride_index(cqe);
+ 	u32 wqe_offset     = stride_ix * MLX5_MPWRQ_STRIDE_SIZE;
+ 	u32 head_offset    = wqe_offset & (PAGE_SIZE - 1);
+ 	u32 page_idx       = wqe_offset >> PAGE_SHIFT;
+ 	u32 head_page_idx  = page_idx;
+ 	u16 headlen = min_t(u16, MLX5_MPWRQ_SMALL_PACKET_THRESHOLD, cqe_bcnt);
+ 	u32 frag_offset    = head_offset + headlen;
+ 	u16 byte_cnt       = cqe_bcnt - headlen;
+ 
+ #if (MLX5_MPWRQ_SMALL_PACKET_THRESHOLD >= MLX5_MPWRQ_STRIDE_SIZE)
+ 	if (unlikely(frag_offset >= PAGE_SIZE)) {
+ 		page_idx++;
+ 		frag_offset -= PAGE_SIZE;
+ 	}
+ #endif
+ 	wi->dma_pre_sync(rq->pdev, wi, wqe_offset, consumed_bytes);
+ 
+ 	while (byte_cnt) {
+ 		u32 pg_consumed_bytes =
+ 			min_t(u32, PAGE_SIZE - frag_offset, byte_cnt);
+ 
+ 		wi->add_skb_frag(rq->pdev, skb, wi, page_idx, frag_offset,
+ 				 pg_consumed_bytes);
+ 		byte_cnt -= pg_consumed_bytes;
+ 		frag_offset = 0;
+ 		page_idx++;
+ 	}
+ 	/* copy header */
+ 	wi->copy_skb_header(rq->pdev, skb, wi, head_page_idx, head_offset,
+ 			    headlen);
+ 	/* skb linear part was allocated with headlen and aligned to long */
+ 	skb->tail += headlen;
+ 	skb->len  += headlen;
+ }
+ 
+ void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
+ 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
+ 	struct mlx5e_mpw_info *wi = &rq->wqe_info[wqe_id];
+ 	struct mlx5e_rx_wqe  *wqe = mlx5_wq_ll_get_wqe(&rq->wq, wqe_id);
+ 	struct sk_buff *skb;
+ 	u16 cqe_bcnt;
+ 
+ 	wi->consumed_strides += cstrides;
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	if (unlikely(mpwrq_is_filler_cqe(cqe))) {
+ 		rq->stats.mpwqe_filler++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	skb = napi_alloc_skb(rq->cq.napi,
+ 			     ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
+ 				   sizeof(long)));
+ 	if (unlikely(!skb))
+ 		goto mpwrq_cqe_out;
+ 
+ 	prefetch(skb->data);
+ 	cqe_bcnt = mpwrq_get_cqe_byte_cnt(cqe);
+ 
+ 	mlx5e_mpwqe_fill_rx_skb(rq, cqe, wi, cqe_bcnt, skb);
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ mpwrq_cqe_out:
+ 	if (likely(wi->consumed_strides < MLX5_MPWRQ_NUM_STRIDES))
+ 		return;
+ 
+ 	wi->free_wqe(rq, wi);
+ 	mlx5_wq_ll_pop(&rq->wq, cqe->wqe_id, &wqe->next.next_wqe_index);
++>>>>>>> e20a0db30454 (net/mlx5e: Delay skb->data access)
  }
  
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
