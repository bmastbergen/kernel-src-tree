mm: meminit: free pages in large chunks where possible

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] meminit: free pages in large chunks where possible (George Beshers) [727269]
Rebuild_FUZZ: 96.15%
commit-author Mel Gorman <mgorman@suse.de>
commit a4de83dd3377eb43ad95387cc16c27a11aae2feb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a4de83dd.failed

Parallel struct page frees pages one at a time. Try free pages as single
large pages where possible.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Nate Zimmer <nzimmer@sgi.com>
	Tested-by: Waiman Long <waiman.long@hp.com>
	Tested-by: Daniel J Blueman <daniel@numascale.com>
	Acked-by: Pekka Enberg <penberg@kernel.org>
	Cc: Robin Holt <robinmholt@gmail.com>
	Cc: Nate Zimmer <nzimmer@sgi.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Waiman Long <waiman.long@hp.com>
	Cc: Scott Norton <scott.norton@hp.com>
	Cc: "Luck, Tony" <tony.luck@intel.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a4de83dd3377eb43ad95387cc16c27a11aae2feb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index 20d353397e7d,e3f00f622f28..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -781,8 -1001,198 +781,201 @@@ void __init __free_pages_bootmem(struc
  	__free_pages(page, order);
  }
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) || \
+ 	defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;
+ 
+ int __meminit early_pfn_to_nid(unsigned long pfn)
+ {
+ 	int nid;
+ 
+ 	/* The system will behave unpredictably otherwise */
+ 	BUG_ON(system_state != SYSTEM_BOOTING);
+ 
+ 	nid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);
+ 	if (nid >= 0)
+ 		return nid;
+ 	/* just returns 0 */
+ 	return 0;
+ }
+ #endif
+ 
+ #ifdef CONFIG_NODES_SPAN_OTHER_NODES
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	int nid;
+ 
+ 	nid = __early_pfn_to_nid(pfn, state);
+ 	if (nid >= 0 && nid != node)
+ 		return false;
+ 	return true;
+ }
+ 
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return meminit_pfn_in_nid(pfn, node, &early_pfnnid_cache);
+ }
+ 
+ #else
+ 
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return true;
+ }
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
+ void __defer_init __free_pages_bootmem(struct page *page, unsigned long pfn,
+ 							unsigned int order)
+ {
+ 	if (early_page_uninitialised(pfn))
+ 		return;
+ 	return __free_pages_boot_core(page, pfn, order);
+ }
+ 
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static void __defermem_init deferred_free_range(struct page *page,
+ 					unsigned long pfn, int nr_pages)
+ {
+ 	int i;
+ 
+ 	if (!page)
+ 		return;
+ 
+ 	/* Free a large naturally-aligned chunk if possible */
+ 	if (nr_pages == MAX_ORDER_NR_PAGES &&
+ 	    (pfn & (MAX_ORDER_NR_PAGES-1)) == 0) {
+ 		__free_pages_boot_core(page, pfn, MAX_ORDER-1);
+ 		return;
+ 	}
+ 
+ 	for (i = 0; i < nr_pages; i++, page++, pfn++)
+ 		__free_pages_boot_core(page, pfn, 0);
+ }
+ 
+ /* Initialise remaining memory on a node */
+ void __defermem_init deferred_init_memmap(int nid)
+ {
+ 	struct mminit_pfnnid_cache nid_init_state = { };
+ 	unsigned long start = jiffies;
+ 	unsigned long nr_pages = 0;
+ 	unsigned long walk_start, walk_end;
+ 	int i, zid;
+ 	struct zone *zone;
+ 	pg_data_t *pgdat = NODE_DATA(nid);
+ 	unsigned long first_init_pfn = pgdat->first_deferred_pfn;
+ 
+ 	if (first_init_pfn == ULONG_MAX)
+ 		return;
+ 
+ 	/* Sanity check boundaries */
+ 	BUG_ON(pgdat->first_deferred_pfn < pgdat->node_start_pfn);
+ 	BUG_ON(pgdat->first_deferred_pfn > pgdat_end_pfn(pgdat));
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ 
+ 	/* Only the highest zone is deferred so find it */
+ 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
+ 		zone = pgdat->node_zones + zid;
+ 		if (first_init_pfn < zone_end_pfn(zone))
+ 			break;
+ 	}
+ 
+ 	for_each_mem_pfn_range(i, nid, &walk_start, &walk_end, NULL) {
+ 		unsigned long pfn, end_pfn;
+ 		struct page *page = NULL;
+ 		struct page *free_base_page = NULL;
+ 		unsigned long free_base_pfn = 0;
+ 		int nr_to_free = 0;
+ 
+ 		end_pfn = min(walk_end, zone_end_pfn(zone));
+ 		pfn = first_init_pfn;
+ 		if (pfn < walk_start)
+ 			pfn = walk_start;
+ 		if (pfn < zone->zone_start_pfn)
+ 			pfn = zone->zone_start_pfn;
+ 
+ 		for (; pfn < end_pfn; pfn++) {
+ 			if (!pfn_valid_within(pfn))
+ 				goto free_range;
+ 
+ 			/*
+ 			 * Ensure pfn_valid is checked every
+ 			 * MAX_ORDER_NR_PAGES for memory holes
+ 			 */
+ 			if ((pfn & (MAX_ORDER_NR_PAGES - 1)) == 0) {
+ 				if (!pfn_valid(pfn)) {
+ 					page = NULL;
+ 					goto free_range;
+ 				}
+ 			}
+ 
+ 			if (!meminit_pfn_in_nid(pfn, nid, &nid_init_state)) {
+ 				page = NULL;
+ 				goto free_range;
+ 			}
+ 
+ 			/* Minimise pfn page lookups and scheduler checks */
+ 			if (page && (pfn & (MAX_ORDER_NR_PAGES - 1)) != 0) {
+ 				page++;
+ 			} else {
+ 				nr_pages += nr_to_free;
+ 				deferred_free_range(free_base_page,
+ 						free_base_pfn, nr_to_free);
+ 				free_base_page = NULL;
+ 				free_base_pfn = nr_to_free = 0;
+ 
+ 				page = pfn_to_page(pfn);
+ 				cond_resched();
+ 			}
+ 
+ 			if (page->flags) {
+ 				VM_BUG_ON(page_zone(page) != zone);
+ 				goto free_range;
+ 			}
+ 
+ 			__init_single_page(page, pfn, zid, nid);
+ 			if (!free_base_page) {
+ 				free_base_page = page;
+ 				free_base_pfn = pfn;
+ 				nr_to_free = 0;
+ 			}
+ 			nr_to_free++;
+ 
+ 			/* Where possible, batch up pages for a single free */
+ 			continue;
+ free_range:
+ 			/* Free the current block of pages to allocator */
+ 			nr_pages += nr_to_free;
+ 			deferred_free_range(free_base_page, free_base_pfn,
+ 								nr_to_free);
+ 			free_base_page = NULL;
+ 			free_base_pfn = nr_to_free = 0;
+ 		}
+ 
+ 		first_init_pfn = max(end_pfn, first_init_pfn);
+ 	}
+ 
+ 	/* Sanity check that the next zone really is unpopulated */
+ 	WARN_ON(++zid < MAX_NR_ZONES && populated_zone(++zone));
+ 
+ 	pr_info("kswapd %d initialised %lu pages in %ums\n", nid, nr_pages,
+ 					jiffies_to_msecs(jiffies - start));
+ }
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
+ 
++>>>>>>> a4de83dd3377 (mm: meminit: free pages in large chunks where possible)
  #ifdef CONFIG_CMA
 -/* Free whole pageblock and set its migration type to MIGRATE_CMA. */
 +/* Free whole pageblock and set it's migration type to MIGRATE_CMA. */
  void __init init_cma_reserved_pageblock(struct page *page)
  {
  	unsigned i = pageblock_nr_pages;
* Unmerged path mm/page_alloc.c
