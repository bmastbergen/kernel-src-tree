perf/x86/mbm: Add support for MBM counter overflow handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Vikas Shivappa <vikas.shivappa@linux.intel.com>
commit e7ee3e8cb550ce43752ae1d1b190d6b5c4150a43
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/e7ee3e8c.failed

This patch adds a per package timer which periodically updates the
memory bandwidth counters for the events that are currently active.

Current patch has a periodic timer every 1s since the SDM guarantees
that the counter will not overflow in 1s but this time can be definitely
improved by calibrating on the system. The overflow is really a function
of the max memory b/w that the socket can support, max counter value and
scaling factor.

	Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Tony Luck <tony.luck@intel.com>
	Acked-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: David Ahern <dsahern@gmail.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Stephane Eranian <eranian@google.com>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Cc: fenghua.yu@intel.com
	Cc: h.peter.anvin@intel.com
	Cc: ravi.v.shankar@intel.com
	Cc: vikas.shivappa@intel.com
Link: http://lkml.kernel.org/r/013b756c5006b1c4ca411f3ecf43ed52f19fbf87.1457723885.git.tony.luck@intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit e7ee3e8cb550ce43752ae1d1b190d6b5c4150a43)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel_cqm.c
diff --cc arch/x86/kernel/cpu/perf_event_intel_cqm.c
index b6e1ef0c0cab,380d62da8108..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel_cqm.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_cqm.c
@@@ -17,10 -21,26 +21,15 @@@
  
  static u32 cqm_max_rmid = -1;
  static unsigned int cqm_l3_scale; /* supposedly cacheline size */
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_cqm.c
++=======
+ static bool cqm_enabled, mbm_enabled;
+ unsigned int mbm_socket_max;
++>>>>>>> e7ee3e8cb550 (perf/x86/mbm: Add support for MBM counter overflow handling):arch/x86/events/intel/cqm.c
  
 -/**
 - * struct intel_pqr_state - State cache for the PQR MSR
 - * @rmid:		The cached Resource Monitoring ID
 - * @closid:		The cached Class Of Service ID
 - * @rmid_usecnt:	The usage counter for rmid
 - *
 - * The upper 32 bits of MSR_IA32_PQR_ASSOC contain closid and the
 - * lower 10 bits rmid. The update to MSR_IA32_PQR_ASSOC always
 - * contains both parts, so we need to cache them.
 - *
 - * The cache also helps to avoid pointless updates if the value does
 - * not change.
 - */
 -struct intel_pqr_state {
 +struct intel_cqm_state {
  	u32			rmid;
 -	u32			closid;
 -	int			rmid_usecnt;
 +	int			cnt;
  };
  
  /*
@@@ -29,7 -49,26 +38,30 @@@
   * (intel_cqm_event_start and intel_cqm_event_stop) are called with
   * interrupts disabled, which is sufficient for the protection.
   */
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_cqm.c
 +static DEFINE_PER_CPU(struct intel_cqm_state, cqm_state);
++=======
+ static DEFINE_PER_CPU(struct intel_pqr_state, pqr_state);
+ static struct hrtimer *mbm_timers;
+ /**
+  * struct sample - mbm event's (local or total) data
+  * @total_bytes    #bytes since we began monitoring
+  * @prev_msr       previous value of MSR
+  */
+ struct sample {
+ 	u64	total_bytes;
+ 	u64	prev_msr;
+ };
+ 
+ /*
+  * samples profiled for total memory bandwidth type events
+  */
+ static struct sample *mbm_total;
+ /*
+  * samples profiled for local memory bandwidth type events
+  */
+ static struct sample *mbm_local;
++>>>>>>> e7ee3e8cb550 (perf/x86/mbm: Add support for MBM counter overflow handling):arch/x86/events/intel/cqm.c
  
  #define pkg_id	topology_physical_package_id(smp_processor_id())
  /*
@@@ -1451,6 -1630,57 +1593,60 @@@ static const struct x86_cpu_id intel_cq
  	{}
  };
  
++<<<<<<< HEAD:arch/x86/kernel/cpu/perf_event_intel_cqm.c
++=======
+ static void mbm_cleanup(void)
+ {
+ 	if (!mbm_enabled)
+ 		return;
+ 
+ 	kfree(mbm_local);
+ 	kfree(mbm_total);
+ 	mbm_enabled = false;
+ }
+ 
+ static const struct x86_cpu_id intel_mbm_local_match[] = {
+ 	{ .vendor = X86_VENDOR_INTEL, .feature = X86_FEATURE_CQM_MBM_LOCAL },
+ 	{}
+ };
+ 
+ static const struct x86_cpu_id intel_mbm_total_match[] = {
+ 	{ .vendor = X86_VENDOR_INTEL, .feature = X86_FEATURE_CQM_MBM_TOTAL },
+ 	{}
+ };
+ 
+ static int intel_mbm_init(void)
+ {
+ 	int ret = 0, array_size, maxid = cqm_max_rmid + 1;
+ 
+ 	mbm_socket_max = topology_max_packages();
+ 	array_size = sizeof(struct sample) * maxid * mbm_socket_max;
+ 	mbm_local = kmalloc(array_size, GFP_KERNEL);
+ 	if (!mbm_local)
+ 		return -ENOMEM;
+ 
+ 	mbm_total = kmalloc(array_size, GFP_KERNEL);
+ 	if (!mbm_total) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	array_size = sizeof(struct hrtimer) * mbm_socket_max;
+ 	mbm_timers = kmalloc(array_size, GFP_KERNEL);
+ 	if (!mbm_timers) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 	mbm_hrtimer_init();
+ 
+ out:
+ 	if (ret)
+ 		mbm_cleanup();
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> e7ee3e8cb550 (perf/x86/mbm: Add support for MBM counter overflow handling):arch/x86/events/intel/cqm.c
  static int __init intel_cqm_init(void)
  {
  	char *str = NULL, scale[20];
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_cqm.c
