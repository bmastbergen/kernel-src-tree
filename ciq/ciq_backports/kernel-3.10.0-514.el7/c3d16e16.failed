mm: migration: do not lose soft dirty bit if page is in migration state

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] migration: do not lose soft dirty bit if page is in migration state (Oleg Nesterov) [1269561]
Rebuild_FUZZ: 97.10%
commit-author Cyrill Gorcunov <gorcunov@gmail.com>
commit c3d16e16522fe3fe8759735850a0676da18f4b1d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/c3d16e16.failed

If page migration is turned on in config and the page is migrating, we
may lose the soft dirty bit.  If fork and mprotect are called on
migrating pages (once migration is complete) pages do not obtain the
soft dirty bit in the correspond pte entries.  Fix it adding an
appropriate test on swap entries.

	Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
	Cc: Pavel Emelyanov <xemul@parallels.com>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Matt Mackall <mpm@selenic.com>
	Cc: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
	Cc: Marcelo Tosatti <mtosatti@redhat.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@gmail.com>
	Cc: Stephen Rothwell <sfr@canb.auug.org.au>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Mel Gorman <mel@csn.ul.ie>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c3d16e16522fe3fe8759735850a0676da18f4b1d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mprotect.c
diff --cc mm/mprotect.c
index 468917529124,a3af058f68e4..000000000000
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@@ -119,11 -100,12 +120,18 @@@ static unsigned long change_pte_range(s
  				 * just be safe and disable write
  				 */
  				make_migration_entry_read(&entry);
++<<<<<<< HEAD
 +				set_pte_at(mm, addr, pte,
 +					swp_entry_to_pte(entry));
 +
 +				pages++;
++=======
+ 				newpte = swp_entry_to_pte(entry);
+ 				if (pte_swp_soft_dirty(oldpte))
+ 					newpte = pte_swp_mksoft_dirty(newpte);
+ 				set_pte_at(mm, addr, pte, newpte);
++>>>>>>> c3d16e16522f (mm: migration: do not lose soft dirty bit if page is in migration state)
  			}
 -			pages++;
  		}
  	} while (pte++, addr += PAGE_SIZE, addr != end);
  	arch_leave_lazy_mmu_mode();
diff --git a/mm/memory.c b/mm/memory.c
index fce51319197b..b6f98d2a03a2 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -868,6 +868,8 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 					 */
 					make_migration_entry_read(&entry);
 					pte = swp_entry_to_pte(entry);
+					if (pte_swp_soft_dirty(*src_pte))
+						pte = pte_swp_mksoft_dirty(pte);
 					set_pte_at(src_mm, addr, src_pte, pte);
 				}
 			}
diff --git a/mm/migrate.c b/mm/migrate.c
index c1313d07c550..5042eb7ab5cc 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -160,6 +160,8 @@ static int remove_migration_pte(struct page *new, struct vm_area_struct *vma,
 
 	get_page(new);
 	pte = pte_mkold(mk_pte(new, vma->vm_page_prot));
+	if (pte_swp_soft_dirty(*ptep))
+		pte = pte_mksoft_dirty(pte);
 	if (is_write_migration_entry(entry))
 		pte = pte_mkwrite(pte);
 #ifdef CONFIG_HUGETLB_PAGE
* Unmerged path mm/mprotect.c
