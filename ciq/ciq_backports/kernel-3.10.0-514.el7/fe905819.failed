drm/i915/kbl: Add WaDisableLSQCROPERFforOCL

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [drm] i915/kbl: Add WaDisableLSQCROPERFforOCL (Rob Clark) [1348329 1349064]
Rebuild_FUZZ: 95.12%
commit-author Mika Kuoppala <mika.kuoppala@linux.intel.com>
commit fe90581987cd5fadd2942f59f8511bcb39fdec34
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/fe905819.failed

Extend the scope of this workaround, already used in skl,
to also take effect in kbl.

v2: Fix KBL_REVID_E0 (Matthew)

References: HSD#2132677
	Cc: Matthew Auld <matthew.william.auld@gmail.com>
	Signed-off-by: Mika Kuoppala <mika.kuoppala@intel.com>
	Reviewed-by: Matthew Auld <matthew.auld@intel.com>
Link: http://patchwork.freedesktop.org/patch/msgid/1465309159-30531-12-git-send-email-mika.kuoppala@intel.com
(cherry picked from commit fe90581987cd5fadd2942f59f8511bcb39fdec34)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/intel_lrc.c
#	drivers/gpu/drm/i915/intel_ringbuffer.c
diff --cc drivers/gpu/drm/i915/intel_lrc.c
index 424e62197787,cc41b6717a5d..000000000000
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@@ -1049,116 -1332,148 +1049,128 @@@ static int logical_ring_prepare(struct 
  	return 0;
  }
  
 -static void lrc_destroy_wa_ctx_obj(struct intel_engine_cs *engine)
 -{
 -	if (engine->wa_ctx.obj) {
 -		i915_gem_object_ggtt_unpin(engine->wa_ctx.obj);
 -		drm_gem_object_unreference(&engine->wa_ctx.obj->base);
 -		engine->wa_ctx.obj = NULL;
 -	}
 -}
 -
 -static int intel_init_workaround_bb(struct intel_engine_cs *engine)
 +/**
 + * intel_logical_ring_begin() - prepare the logical ringbuffer to accept some commands
 + *
 + * @ringbuf: Logical ringbuffer.
 + * @num_dwords: number of DWORDs that we plan to write to the ringbuffer.
 + *
 + * The ringbuffer might not be ready to accept the commands right away (maybe it needs to
 + * be wrapped, or wait a bit for the tail to be updated). This function takes care of that
 + * and also preallocates a request (every workload submission is still mediated through
 + * requests, same as it did with legacy ringbuffer submission).
 + *
 + * Return: non-zero if the ringbuffer is not ready to be written to.
 + */
 +int intel_logical_ring_begin(struct intel_ringbuffer *ringbuf,
 +			     struct intel_context *ctx, int num_dwords)
  {
 +	struct intel_engine_cs *ring = ringbuf->ring;
 +	struct drm_device *dev = ring->dev;
 +	struct drm_i915_private *dev_priv = dev->dev_private;
  	int ret;
 -	uint32_t *batch;
 -	uint32_t offset;
 -	struct page *page;
 -	struct i915_ctx_workarounds *wa_ctx = &engine->wa_ctx;
 -
 -	WARN_ON(engine->id != RCS);
 -
 -	/* update this when WA for higher Gen are added */
 -	if (INTEL_GEN(engine->i915) > 9) {
 -		DRM_ERROR("WA batch buffer is not initialized for Gen%d\n",
 -			  INTEL_GEN(engine->i915));
 -		return 0;
 -	}
  
 -	/* some WA perform writes to scratch page, ensure it is valid */
 -	if (engine->scratch.obj == NULL) {
 -		DRM_ERROR("scratch page not allocated for %s\n", engine->name);
 -		return -EINVAL;
 -	}
 -
 -	ret = lrc_setup_wa_ctx_obj(engine, PAGE_SIZE);
 -	if (ret) {
 -		DRM_DEBUG_DRIVER("Failed to setup context WA page: %d\n", ret);
 +	ret = i915_gem_check_wedge(&dev_priv->gpu_error,
 +				   dev_priv->mm.interruptible);
 +	if (ret)
  		return ret;
 -	}
 -
 -	page = i915_gem_object_get_dirty_page(wa_ctx->obj, 0);
 -	batch = kmap_atomic(page);
 -	offset = 0;
 -
 -	if (IS_GEN8(engine->i915)) {
 -		ret = gen8_init_indirectctx_bb(engine,
 -					       &wa_ctx->indirect_ctx,
 -					       batch,
 -					       &offset);
 -		if (ret)
 -			goto out;
 -
 -		ret = gen8_init_perctx_bb(engine,
 -					  &wa_ctx->per_ctx,
 -					  batch,
 -					  &offset);
 -		if (ret)
 -			goto out;
 -	} else if (IS_GEN9(engine->i915)) {
 -		ret = gen9_init_indirectctx_bb(engine,
 -					       &wa_ctx->indirect_ctx,
 -					       batch,
 -					       &offset);
 -		if (ret)
 -			goto out;
  
 -		ret = gen9_init_perctx_bb(engine,
 -					  &wa_ctx->per_ctx,
 -					  batch,
 -					  &offset);
 -		if (ret)
 -			goto out;
 -	}
 +	ret = logical_ring_prepare(ringbuf, ctx, num_dwords * sizeof(uint32_t));
 +	if (ret)
 +		return ret;
  
 -out:
 -	kunmap_atomic(batch);
 +	/* Preallocate the olr before touching the ring */
 +	ret = logical_ring_alloc_request(ring, ctx);
  	if (ret)
 -		lrc_destroy_wa_ctx_obj(engine);
 +		return ret;
  
 -	return ret;
 +	ringbuf->space -= num_dwords * sizeof(uint32_t);
 +	return 0;
  }
  
 -static void lrc_init_hws(struct intel_engine_cs *engine)
 +static int intel_logical_ring_workarounds_emit(struct intel_engine_cs *ring,
 +					       struct intel_context *ctx)
  {
 -	struct drm_i915_private *dev_priv = engine->i915;
 +	int ret, i;
 +	struct intel_ringbuffer *ringbuf = ctx->engine[ring->id].ringbuf;
 +	struct drm_device *dev = ring->dev;
 +	struct drm_i915_private *dev_priv = dev->dev_private;
 +	struct i915_workarounds *w = &dev_priv->workarounds;
  
 -	I915_WRITE(RING_HWS_PGA(engine->mmio_base),
 -		   (u32)engine->status_page.gfx_addr);
 -	POSTING_READ(RING_HWS_PGA(engine->mmio_base));
 -}
 +	if (WARN_ON_ONCE(w->count == 0))
 +		return 0;
  
 -static int gen8_init_common_ring(struct intel_engine_cs *engine)
 -{
 -	struct drm_i915_private *dev_priv = engine->i915;
 -	unsigned int next_context_status_buffer_hw;
 +	ring->gpu_caches_dirty = true;
 +	ret = logical_ring_flush_all_caches(ringbuf, ctx);
 +	if (ret)
 +		return ret;
 +
 +	ret = intel_logical_ring_begin(ringbuf, ctx, w->count * 2 + 2);
 +	if (ret)
 +		return ret;
 +
 +	intel_logical_ring_emit(ringbuf, MI_LOAD_REGISTER_IMM(w->count));
 +	for (i = 0; i < w->count; i++) {
 +		intel_logical_ring_emit(ringbuf, w->reg[i].addr);
 +		intel_logical_ring_emit(ringbuf, w->reg[i].value);
 +	}
 +	intel_logical_ring_emit(ringbuf, MI_NOOP);
  
 -	lrc_init_hws(engine);
 +	intel_logical_ring_advance(ringbuf);
  
 -	I915_WRITE_IMR(engine,
 -		       ~(engine->irq_enable_mask | engine->irq_keep_mask));
 -	I915_WRITE(RING_HWSTAM(engine->mmio_base), 0xffffffff);
 +	ring->gpu_caches_dirty = true;
 +	ret = logical_ring_flush_all_caches(ringbuf, ctx);
 +	if (ret)
 +		return ret;
  
 -	I915_WRITE(RING_MODE_GEN7(engine),
 -		   _MASKED_BIT_DISABLE(GFX_REPLAY_MODE) |
 -		   _MASKED_BIT_ENABLE(GFX_RUN_LIST_ENABLE));
 -	POSTING_READ(RING_MODE_GEN7(engine));
 +	return 0;
 +}
  
 -	/*
 -	 * Instead of resetting the Context Status Buffer (CSB) read pointer to
 -	 * zero, we need to read the write pointer from hardware and use its
 -	 * value because "this register is power context save restored".
 -	 * Effectively, these states have been observed:
 -	 *
 -	 *      | Suspend-to-idle (freeze) | Suspend-to-RAM (mem) |
 -	 * BDW  | CSB regs not reset       | CSB regs reset       |
 -	 * CHT  | CSB regs not reset       | CSB regs not reset   |
 -	 * SKL  |         ?                |         ?            |
 -	 * BXT  |         ?                |         ?            |
 -	 */
 -	next_context_status_buffer_hw =
 -		GEN8_CSB_WRITE_PTR(I915_READ(RING_CONTEXT_STATUS_PTR(engine)));
 +static int gen8_init_common_ring(struct intel_engine_cs *ring)
 +{
 +	struct drm_device *dev = ring->dev;
 +	struct drm_i915_private *dev_priv = dev->dev_private;
  
++<<<<<<< HEAD
 +	I915_WRITE_IMR(ring, ~(ring->irq_enable_mask | ring->irq_keep_mask));
 +	I915_WRITE(RING_HWSTAM(ring->mmio_base), 0xffffffff);
++=======
+ 	/*
 -	 * When the CSB registers are reset (also after power-up / gpu reset),
 -	 * CSB write pointer is set to all 1's, which is not valid, use '5' in
 -	 * this special case, so the first element read is CSB[0].
++	 * WaDisableLSQCROPERFforOCL:skl,kbl
++	 * This WA is implemented in skl_init_clock_gating() but since
++	 * this batch updates GEN8_L3SQCREG4 with default value we need to
++	 * set this bit here to retain the WA during flush.
+ 	 */
 -	if (next_context_status_buffer_hw == GEN8_CSB_PTR_MASK)
 -		next_context_status_buffer_hw = (GEN8_CSB_ENTRIES - 1);
++	if (IS_SKL_REVID(engine->i915, 0, SKL_REVID_E0) ||
++	    IS_KBL_REVID(engine->i915, 0, KBL_REVID_E0))
++		l3sqc4_flush |= GEN8_LQSC_RO_PERF_DIS;
++>>>>>>> fe90581987cd (drm/i915/kbl: Add WaDisableLSQCROPERFforOCL)
  
 -	engine->next_context_status_buffer = next_context_status_buffer_hw;
 -	DRM_DEBUG_DRIVER("Execlists enabled for %s\n", engine->name);
 +	if (ring->status_page.obj) {
 +		I915_WRITE(RING_HWS_PGA(ring->mmio_base),
 +			   (u32)ring->status_page.gfx_addr);
 +		POSTING_READ(RING_HWS_PGA(ring->mmio_base));
 +	}
 +
 +	I915_WRITE(RING_MODE_GEN7(ring),
 +		   _MASKED_BIT_DISABLE(GFX_REPLAY_MODE) |
 +		   _MASKED_BIT_ENABLE(GFX_RUN_LIST_ENABLE));
 +	POSTING_READ(RING_MODE_GEN7(ring));
 +	ring->next_context_status_buffer = 0;
 +	DRM_DEBUG_DRIVER("Execlists enabled for %s\n", ring->name);
  
 -	intel_engine_init_hangcheck(engine);
 +	memset(&ring->hangcheck, 0, sizeof(ring->hangcheck));
  
 -	return intel_mocs_init_engine(engine);
 +	return 0;
  }
  
 -static int gen8_init_render_ring(struct intel_engine_cs *engine)
 +static int gen8_init_render_ring(struct intel_engine_cs *ring)
  {
 -	struct drm_i915_private *dev_priv = engine->i915;
 +	struct drm_device *dev = ring->dev;
 +	struct drm_i915_private *dev_priv = dev->dev_private;
  	int ret;
  
 -	ret = gen8_init_common_ring(engine);
 +	ret = gen8_init_common_ring(ring);
  	if (ret)
  		return ret;
  
diff --cc drivers/gpu/drm/i915/intel_ringbuffer.c
index 005b5e04de4d,50379e863ae3..000000000000
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@@ -1024,28 -1116,133 +1024,118 @@@ static int skl_init_workarounds(struct 
  				  HDC_FENCE_DEST_SLM_DISABLE |
  				  HDC_BARRIER_PERFORMANCE_DISABLE);
  
 -	/* WaDisableSbeCacheDispatchPortSharing:skl */
 -	if (IS_SKL_REVID(dev_priv, 0, SKL_REVID_F0))
 -		WA_SET_BIT_MASKED(
 -			GEN7_HALF_SLICE_CHICKEN1,
 -			GEN7_SBE_SS_CACHE_DISPATCH_PORT_SHARING_DISABLE);
 -
 -	/* WaDisableGafsUnitClkGating:skl */
 -	WA_SET_BIT(GEN7_UCGCTL4, GEN8_EU_GAUNIT_CLOCK_GATE_DISABLE);
 -
 -	/* WaDisableLSQCROPERFforOCL:skl */
 -	ret = wa_ring_whitelist_reg(engine, GEN8_L3SQCREG4);
 -	if (ret)
 -		return ret;
 -
 -	return skl_tune_iz_hashing(engine);
 +	return skl_tune_iz_hashing(ring);
  }
  
 -static int bxt_init_workarounds(struct intel_engine_cs *engine)
 +int init_workarounds_ring(struct intel_engine_cs *ring)
  {
 -	struct drm_i915_private *dev_priv = engine->i915;
 -	int ret;
 +	struct drm_device *dev = ring->dev;
 +	struct drm_i915_private *dev_priv = dev->dev_private;
  
++<<<<<<< HEAD
 +	WARN_ON(ring->id != RCS);
++=======
+ 	ret = gen9_init_workarounds(engine);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* WaStoreMultiplePTEenable:bxt */
+ 	/* This is a requirement according to Hardware specification */
+ 	if (IS_BXT_REVID(dev_priv, 0, BXT_REVID_A1))
+ 		I915_WRITE(TILECTL, I915_READ(TILECTL) | TILECTL_TLBPF);
+ 
+ 	/* WaSetClckGatingDisableMedia:bxt */
+ 	if (IS_BXT_REVID(dev_priv, 0, BXT_REVID_A1)) {
+ 		I915_WRITE(GEN7_MISCCPCTL, (I915_READ(GEN7_MISCCPCTL) &
+ 					    ~GEN8_DOP_CLOCK_GATE_MEDIA_ENABLE));
+ 	}
+ 
+ 	/* WaDisableThreadStallDopClockGating:bxt */
+ 	WA_SET_BIT_MASKED(GEN8_ROW_CHICKEN,
+ 			  STALL_DOP_GATING_DISABLE);
+ 
+ 	/* WaDisableSbeCacheDispatchPortSharing:bxt */
+ 	if (IS_BXT_REVID(dev_priv, 0, BXT_REVID_B0)) {
+ 		WA_SET_BIT_MASKED(
+ 			GEN7_HALF_SLICE_CHICKEN1,
+ 			GEN7_SBE_SS_CACHE_DISPATCH_PORT_SHARING_DISABLE);
+ 	}
+ 
+ 	/* WaDisableObjectLevelPreemptionForTrifanOrPolygon:bxt */
+ 	/* WaDisableObjectLevelPreemptionForInstancedDraw:bxt */
+ 	/* WaDisableObjectLevelPreemtionForInstanceId:bxt */
+ 	/* WaDisableLSQCROPERFforOCL:bxt */
+ 	if (IS_BXT_REVID(dev_priv, 0, BXT_REVID_A1)) {
+ 		ret = wa_ring_whitelist_reg(engine, GEN9_CS_DEBUG_MODE1);
+ 		if (ret)
+ 			return ret;
+ 
+ 		ret = wa_ring_whitelist_reg(engine, GEN8_L3SQCREG4);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	/* WaProgramL3SqcReg1DefaultForPerf:bxt */
+ 	if (IS_BXT_REVID(dev_priv, BXT_REVID_B0, REVID_FOREVER))
+ 		I915_WRITE(GEN8_L3SQCREG1, L3_GENERAL_PRIO_CREDITS(62) |
+ 					   L3_HIGH_PRIO_CREDITS(2));
+ 
+ 	return 0;
+ }
+ 
+ static int kbl_init_workarounds(struct intel_engine_cs *engine)
+ {
+ 	struct drm_i915_private *dev_priv = engine->i915;
+ 	int ret;
+ 
+ 	ret = gen9_init_workarounds(engine);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* WaEnableGapsTsvCreditFix:kbl */
+ 	I915_WRITE(GEN8_GARBCNTL, (I915_READ(GEN8_GARBCNTL) |
+ 				   GEN9_GAPS_TSV_CREDIT_DISABLE));
+ 
+ 	/* WaDisableFenceDestinationToSLM:kbl (pre-prod) */
+ 	if (IS_KBL_REVID(dev_priv, KBL_REVID_A0, KBL_REVID_A0))
+ 		WA_SET_BIT_MASKED(HDC_CHICKEN0,
+ 				  HDC_FENCE_DEST_SLM_DISABLE);
+ 
+ 	/* GEN8_L3SQCREG4 has a dependency with WA batch so any new changes
+ 	 * involving this register should also be added to WA batch as required.
+ 	 */
+ 	if (IS_KBL_REVID(dev_priv, 0, KBL_REVID_E0))
+ 		/* WaDisableLSQCROPERFforOCL:kbl */
+ 		I915_WRITE(GEN8_L3SQCREG4, I915_READ(GEN8_L3SQCREG4) |
+ 			   GEN8_LQSC_RO_PERF_DIS);
+ 
+ 	/* WaDisableLSQCROPERFforOCL:kbl */
+ 	ret = wa_ring_whitelist_reg(engine, GEN8_L3SQCREG4);
+ 	if (ret)
+ 		return ret;
+ 
+ 	return 0;
+ }
+ 
+ int init_workarounds_ring(struct intel_engine_cs *engine)
+ {
+ 	struct drm_i915_private *dev_priv = engine->i915;
+ 
+ 	WARN_ON(engine->id != RCS);
++>>>>>>> fe90581987cd (drm/i915/kbl: Add WaDisableLSQCROPERFforOCL)
  
  	dev_priv->workarounds.count = 0;
 -	dev_priv->workarounds.hw_whitelist_count[RCS] = 0;
 -
 -	if (IS_BROADWELL(dev_priv))
 -		return bdw_init_workarounds(engine);
 -
 -	if (IS_CHERRYVIEW(dev_priv))
 -		return chv_init_workarounds(engine);
  
 -	if (IS_SKYLAKE(dev_priv))
 -		return skl_init_workarounds(engine);
 +	if (IS_BROADWELL(dev))
 +		return bdw_init_workarounds(ring);
  
 -	if (IS_BROXTON(dev_priv))
 -		return bxt_init_workarounds(engine);
 +	if (IS_CHERRYVIEW(dev))
 +		return chv_init_workarounds(ring);
  
 -	if (IS_KABYLAKE(dev_priv))
 -		return kbl_init_workarounds(engine);
 +	if (IS_SKYLAKE(dev))
 +		return skl_init_workarounds(ring);
 +	else if (IS_GEN9(dev))
 +		return gen9_init_workarounds(ring);
  
  	return 0;
  }
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index f691be6f9546..fa87e0b3cb1f 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -2334,6 +2334,9 @@ struct drm_i915_cmd_table {
 
 #define KBL_REVID_A0		0x0
 #define KBL_REVID_B0		0x1
+#define KBL_REVID_C0		0x2
+#define KBL_REVID_D0		0x3
+#define KBL_REVID_E0		0x4
 
 #define IS_KBL_REVID(p, since, until) \
 	(IS_KABYLAKE(p) && IS_REVID(p, since, until))
* Unmerged path drivers/gpu/drm/i915/intel_lrc.c
* Unmerged path drivers/gpu/drm/i915/intel_ringbuffer.c
