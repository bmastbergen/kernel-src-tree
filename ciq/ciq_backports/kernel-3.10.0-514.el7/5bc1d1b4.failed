sctp: remove macros sctp_bh_[un]lock_sock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author wangweidong <wangweidong1@huawei.com>
commit 5bc1d1b4a261a865cbde65b1561748df5b9c724b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/5bc1d1b4.failed

Redefined bh_[un]lock_sock to sctp_bh[un]lock_sock for user
space friendly code which we haven't use in years, so removing them.

	Signed-off-by: Wang Weidong <wangweidong1@huawei.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 5bc1d1b4a261a865cbde65b1561748df5b9c724b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/sctp/sctp.h
#	net/sctp/protocol.c
#	net/sctp/socket.c
diff --cc include/net/sctp/sctp.h
index 45ea11fe19f1,a3353f45ef94..000000000000
--- a/include/net/sctp/sctp.h
+++ b/include/net/sctp/sctp.h
@@@ -209,20 -170,6 +209,23 @@@ extern struct kmem_cache *sctp_bucket_c
   *  Section:  Macros, externs, and inlines
   */
  
++<<<<<<< HEAD
 +/* spin lock wrappers. */
 +#define sctp_local_bh_disable() local_bh_disable()
 +#define sctp_local_bh_enable()  local_bh_enable()
 +#define sctp_spin_lock(lock)    spin_lock(lock)
 +#define sctp_spin_unlock(lock)  spin_unlock(lock)
 +#define sctp_write_lock(lock)   write_lock(lock)
 +#define sctp_write_unlock(lock) write_unlock(lock)
 +#define sctp_read_lock(lock)    read_lock(lock)
 +#define sctp_read_unlock(lock)  read_unlock(lock)
 +
 +/* sock lock wrappers. */
 +#define sctp_bh_lock_sock(sk)    bh_lock_sock(sk)
 +#define sctp_bh_unlock_sock(sk)  bh_unlock_sock(sk)
 +
++=======
++>>>>>>> 5bc1d1b4a261 (sctp: remove macros sctp_bh_[un]lock_sock)
  /* SCTP SNMP MIB stats handlers */
  #define SCTP_INC_STATS(net, field)      SNMP_INC_STATS((net)->sctp.sctp_statistics, field)
  #define SCTP_INC_STATS_BH(net, field)   SNMP_INC_STATS_BH((net)->sctp.sctp_statistics, field)
diff --cc net/sctp/protocol.c
index e1a5277351de,4e1d0fcb028e..000000000000
--- a/net/sctp/protocol.c
+++ b/net/sctp/protocol.c
@@@ -664,10 -634,10 +664,15 @@@ static void sctp_addr_wq_timeout_handle
  			/* ignore bound-specific endpoints */
  			if (!sctp_is_ep_boundall(sk))
  				continue;
- 			sctp_bh_lock_sock(sk);
+ 			bh_lock_sock(sk);
  			if (sctp_asconf_mgmt(sp, addrw) < 0)
++<<<<<<< HEAD
 +				SCTP_DEBUG_PRINTK("sctp_addrwq_timo_handler: sctp_asconf_mgmt failed\n");
 +			sctp_bh_unlock_sock(sk);
++=======
+ 				pr_debug("%s: sctp_asconf_mgmt failed\n", __func__);
+ 			bh_unlock_sock(sk);
++>>>>>>> 5bc1d1b4a261 (sctp: remove macros sctp_bh_[un]lock_sock)
  		}
  #if IS_ENABLED(CONFIG_IPV6)
  free_next:
diff --cc net/sctp/socket.c
index f1f5a4712c6f,9e91d6e5df63..000000000000
--- a/net/sctp/socket.c
+++ b/net/sctp/socket.c
@@@ -1548,11 -1509,9 +1548,16 @@@ SCTP_STATIC void sctp_close(struct soc
  
  	/* Supposedly, no process has access to the socket, but
  	 * the net layers still may.
 +	 * Also, sctp_destroy_sock() needs to be called with addr_wq_lock
 +	 * held and that should be grabbed before socket lock.
  	 */
++<<<<<<< HEAD
 +	spin_lock_bh(&net->sctp.addr_wq_lock);
 +	sctp_bh_lock_sock(sk);
++=======
+ 	local_bh_disable();
+ 	bh_lock_sock(sk);
++>>>>>>> 5bc1d1b4a261 (sctp: remove macros sctp_bh_[un]lock_sock)
  
  	/* Hold the sock, since sk_common_release() will put sock_put()
  	 * and we have just a little more cleanup.
@@@ -1560,8 -1519,8 +1565,13 @@@
  	sock_hold(sk);
  	sk_common_release(sk);
  
++<<<<<<< HEAD
 +	sctp_bh_unlock_sock(sk);
 +	spin_unlock_bh(&net->sctp.addr_wq_lock);
++=======
+ 	bh_unlock_sock(sk);
+ 	local_bh_enable();
++>>>>>>> 5bc1d1b4a261 (sctp: remove macros sctp_bh_[un]lock_sock)
  
  	sock_put(sk);
  
* Unmerged path include/net/sctp/sctp.h
diff --git a/net/sctp/input.c b/net/sctp/input.c
index f5c23efe96a7..7bd46b4f14c0 100644
--- a/net/sctp/input.c
+++ b/net/sctp/input.c
@@ -247,7 +247,7 @@ int sctp_rcv(struct sk_buff *skb)
 	 * bottom halves on this lock, but a user may be in the lock too,
 	 * so check if it is busy.
 	 */
-	sctp_bh_lock_sock(sk);
+	bh_lock_sock(sk);
 
 	if (sk != rcvr->sk) {
 		/* Our cached sk is different from the rcvr->sk.  This is
@@ -257,14 +257,14 @@ int sctp_rcv(struct sk_buff *skb)
 		 * be doing something with the new socket.  Switch our veiw
 		 * of the current sk.
 		 */
-		sctp_bh_unlock_sock(sk);
+		bh_unlock_sock(sk);
 		sk = rcvr->sk;
-		sctp_bh_lock_sock(sk);
+		bh_lock_sock(sk);
 	}
 
 	if (sock_owned_by_user(sk)) {
 		if (sctp_add_backlog(sk, skb)) {
-			sctp_bh_unlock_sock(sk);
+			bh_unlock_sock(sk);
 			sctp_chunk_free(chunk);
 			skb = NULL; /* sctp_chunk_free already freed the skb */
 			goto discard_release;
@@ -275,7 +275,7 @@ int sctp_rcv(struct sk_buff *skb)
 		sctp_inq_push(&chunk->rcvr->inqueue, chunk);
 	}
 
-	sctp_bh_unlock_sock(sk);
+	bh_unlock_sock(sk);
 
 	/* Release the asoc/ep ref we took in the lookup calls. */
 	if (asoc)
@@ -336,7 +336,7 @@ int sctp_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 		 */
 
 		sk = rcvr->sk;
-		sctp_bh_lock_sock(sk);
+		bh_lock_sock(sk);
 
 		if (sock_owned_by_user(sk)) {
 			if (sk_add_backlog(sk, skb, sk->sk_rcvbuf))
@@ -346,7 +346,7 @@ int sctp_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 		} else
 			sctp_inq_push(inqueue, chunk);
 
-		sctp_bh_unlock_sock(sk);
+		bh_unlock_sock(sk);
 
 		/* If the chunk was backloged again, don't drop refs */
 		if (backloged)
@@ -531,7 +531,7 @@ struct sock *sctp_err_lookup(struct net *net, int family, struct sk_buff *skb,
 		goto out;
 	}
 
-	sctp_bh_lock_sock(sk);
+	bh_lock_sock(sk);
 
 	/* If too many ICMPs get dropped on busy
 	 * servers this needs to be solved differently.
@@ -551,7 +551,7 @@ out:
 /* Common cleanup code for icmp/icmpv6 error handler. */
 void sctp_err_finish(struct sock *sk, struct sctp_association *asoc)
 {
-	sctp_bh_unlock_sock(sk);
+	bh_unlock_sock(sk);
 	sctp_association_put(asoc);
 }
 
* Unmerged path net/sctp/protocol.c
diff --git a/net/sctp/sm_sideeffect.c b/net/sctp/sm_sideeffect.c
index 8663a0d28fe7..20a6df69c790 100644
--- a/net/sctp/sm_sideeffect.c
+++ b/net/sctp/sm_sideeffect.c
@@ -259,7 +259,7 @@ void sctp_generate_t3_rtx_event(unsigned long peer)
 
 	/* Check whether a task is in the sock.  */
 
-	sctp_bh_lock_sock(asoc->base.sk);
+	bh_lock_sock(asoc->base.sk);
 	if (sock_owned_by_user(asoc->base.sk)) {
 		SCTP_DEBUG_PRINTK("%s:Sock is busy.\n", __func__);
 
@@ -286,7 +286,7 @@ void sctp_generate_t3_rtx_event(unsigned long peer)
 		asoc->base.sk->sk_err = -error;
 
 out_unlock:
-	sctp_bh_unlock_sock(asoc->base.sk);
+	bh_unlock_sock(asoc->base.sk);
 	sctp_transport_put(transport);
 }
 
@@ -299,7 +299,7 @@ static void sctp_generate_timeout_event(struct sctp_association *asoc,
 	struct net *net = sock_net(asoc->base.sk);
 	int error = 0;
 
-	sctp_bh_lock_sock(asoc->base.sk);
+	bh_lock_sock(asoc->base.sk);
 	if (sock_owned_by_user(asoc->base.sk)) {
 		SCTP_DEBUG_PRINTK("%s:Sock is busy: timer %d\n",
 				  __func__,
@@ -327,7 +327,7 @@ static void sctp_generate_timeout_event(struct sctp_association *asoc,
 		asoc->base.sk->sk_err = -error;
 
 out_unlock:
-	sctp_bh_unlock_sock(asoc->base.sk);
+	bh_unlock_sock(asoc->base.sk);
 	sctp_association_put(asoc);
 }
 
@@ -379,7 +379,7 @@ void sctp_generate_heartbeat_event(unsigned long data)
 	struct sctp_association *asoc = transport->asoc;
 	struct net *net = sock_net(asoc->base.sk);
 
-	sctp_bh_lock_sock(asoc->base.sk);
+	bh_lock_sock(asoc->base.sk);
 	if (sock_owned_by_user(asoc->base.sk)) {
 		SCTP_DEBUG_PRINTK("%s:Sock is busy.\n", __func__);
 
@@ -404,7 +404,7 @@ void sctp_generate_heartbeat_event(unsigned long data)
 		asoc->base.sk->sk_err = -error;
 
 out_unlock:
-	sctp_bh_unlock_sock(asoc->base.sk);
+	bh_unlock_sock(asoc->base.sk);
 	sctp_transport_put(transport);
 }
 
@@ -417,7 +417,7 @@ void sctp_generate_proto_unreach_event(unsigned long data)
 	struct sctp_association *asoc = transport->asoc;
 	struct net *net = sock_net(asoc->base.sk);
 
-	sctp_bh_lock_sock(asoc->base.sk);
+	bh_lock_sock(asoc->base.sk);
 	if (sock_owned_by_user(asoc->base.sk)) {
 		SCTP_DEBUG_PRINTK("%s:Sock is busy.\n", __func__);
 
@@ -439,7 +439,7 @@ void sctp_generate_proto_unreach_event(unsigned long data)
 		   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);
 
 out_unlock:
-	sctp_bh_unlock_sock(asoc->base.sk);
+	bh_unlock_sock(asoc->base.sk);
 	sctp_association_put(asoc);
 }
 
* Unmerged path net/sctp/socket.c
