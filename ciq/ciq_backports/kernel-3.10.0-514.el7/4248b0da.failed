fs, file table: reinit files_stat.max_files after deferred memory initialisation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] reinit files_stat.max_files after deferred memory initialisation (George Beshers) [727269]
Rebuild_FUZZ: 88.89%
commit-author Mel Gorman <mgorman@suse.de>
commit 4248b0da460839e30eaaad78992b9a1dd3e63e21
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/4248b0da.failed

Dave Hansen reported the following;

	My laptop has been behaving strangely with 4.2-rc2.  Once I log
	in to my X session, I start getting all kinds of strange errors
	from applications and see this in my dmesg:

        	VFS: file-max limit 8192 reached

The problem is that the file-max is calculated before memory is fully
initialised and miscalculates how much memory the kernel is using.  This
patch recalculates file-max after deferred memory initialisation.  Note
that using memory hotplug infrastructure would not have avoided this
problem as the value is not recalculated after memory hot-add.

4.1:             files_stat.max_files = 6582781
4.2-rc2:         files_stat.max_files = 8192
4.2-rc2 patched: files_stat.max_files = 6562467

Small differences with the patch applied and 4.1 but not enough to matter.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reported-by: Dave Hansen <dave.hansen@intel.com>
	Cc: Nicolai Stange <nicstange@gmail.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Alex Ng <alexng@microsoft.com>
	Cc: Fengguang Wu <fengguang.wu@intel.com>
	Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4248b0da460839e30eaaad78992b9a1dd3e63e21)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/file_table.c
#	mm/page_alloc.c
diff --cc fs/file_table.c
index 52ef2a338a0e,ad17e05ebf95..000000000000
--- a/fs/file_table.c
+++ b/fs/file_table.c
@@@ -346,20 -309,24 +347,36 @@@ void put_filp(struct file *file
  	}
  }
  
- void __init files_init(unsigned long mempages)
+ void __init files_init(void)
  { 
- 	unsigned long n;
- 
  	filp_cachep = kmem_cache_create("filp", sizeof(struct file), 0,
  			SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);
++<<<<<<< HEAD
 +
 +	/*
 +	 * One file with associated inode and dcache is very roughly 1K.
 +	 * Per default don't use more than 10% of our memory for files. 
 +	 */ 
 +
 +	n = (mempages * (PAGE_SIZE / 1024)) / 10;
 +	files_stat.max_files = max_t(unsigned long, n, NR_FILE);
 +	files_defer_init();
++=======
++>>>>>>> 4248b0da4608 (fs, file table: reinit files_stat.max_files after deferred memory initialisation)
  	percpu_counter_init(&nr_files, 0, GFP_KERNEL);
+ }
+ 
+ /*
+  * One file with associated inode and dcache is very roughly 1K. Per default
+  * do not use more than 10% of our memory for files.
+  */
+ void __init files_maxfiles_init(void)
+ {
+ 	unsigned long n;
+ 	unsigned long memreserve = (totalram_pages - nr_free_pages()) * 3/2;
+ 
+ 	memreserve = min(memreserve, totalram_pages - 1);
+ 	n = ((totalram_pages - memreserve) * (PAGE_SIZE / 1024)) / 10;
+ 
+ 	files_stat.max_files = max_t(unsigned long, n, NR_FILE);
  } 
diff --cc mm/page_alloc.c
index 20d353397e7d,cb61f44eb3fc..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -781,8 -978,237 +781,240 @@@ void __init __free_pages_bootmem(struc
  	__free_pages(page, order);
  }
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID) || \
+ 	defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP)
+ 
+ static struct mminit_pfnnid_cache early_pfnnid_cache __meminitdata;
+ 
+ int __meminit early_pfn_to_nid(unsigned long pfn)
+ {
+ 	static DEFINE_SPINLOCK(early_pfn_lock);
+ 	int nid;
+ 
+ 	spin_lock(&early_pfn_lock);
+ 	nid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);
+ 	if (nid < 0)
+ 		nid = 0;
+ 	spin_unlock(&early_pfn_lock);
+ 
+ 	return nid;
+ }
+ #endif
+ 
+ #ifdef CONFIG_NODES_SPAN_OTHER_NODES
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	int nid;
+ 
+ 	nid = __early_pfn_to_nid(pfn, state);
+ 	if (nid >= 0 && nid != node)
+ 		return false;
+ 	return true;
+ }
+ 
+ /* Only safe to use early in boot when initialisation is single-threaded */
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return meminit_pfn_in_nid(pfn, node, &early_pfnnid_cache);
+ }
+ 
+ #else
+ 
+ static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
+ {
+ 	return true;
+ }
+ static inline bool __meminit meminit_pfn_in_nid(unsigned long pfn, int node,
+ 					struct mminit_pfnnid_cache *state)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ 
+ void __init __free_pages_bootmem(struct page *page, unsigned long pfn,
+ 							unsigned int order)
+ {
+ 	if (early_page_uninitialised(pfn))
+ 		return;
+ 	return __free_pages_boot_core(page, pfn, order);
+ }
+ 
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static void __init deferred_free_range(struct page *page,
+ 					unsigned long pfn, int nr_pages)
+ {
+ 	int i;
+ 
+ 	if (!page)
+ 		return;
+ 
+ 	/* Free a large naturally-aligned chunk if possible */
+ 	if (nr_pages == MAX_ORDER_NR_PAGES &&
+ 	    (pfn & (MAX_ORDER_NR_PAGES-1)) == 0) {
+ 		set_pageblock_migratetype(page, MIGRATE_MOVABLE);
+ 		__free_pages_boot_core(page, pfn, MAX_ORDER-1);
+ 		return;
+ 	}
+ 
+ 	for (i = 0; i < nr_pages; i++, page++, pfn++)
+ 		__free_pages_boot_core(page, pfn, 0);
+ }
+ 
+ /* Completion tracking for deferred_init_memmap() threads */
+ static atomic_t pgdat_init_n_undone __initdata;
+ static __initdata DECLARE_COMPLETION(pgdat_init_all_done_comp);
+ 
+ static inline void __init pgdat_init_report_one_done(void)
+ {
+ 	if (atomic_dec_and_test(&pgdat_init_n_undone))
+ 		complete(&pgdat_init_all_done_comp);
+ }
+ 
+ /* Initialise remaining memory on a node */
+ static int __init deferred_init_memmap(void *data)
+ {
+ 	pg_data_t *pgdat = data;
+ 	int nid = pgdat->node_id;
+ 	struct mminit_pfnnid_cache nid_init_state = { };
+ 	unsigned long start = jiffies;
+ 	unsigned long nr_pages = 0;
+ 	unsigned long walk_start, walk_end;
+ 	int i, zid;
+ 	struct zone *zone;
+ 	unsigned long first_init_pfn = pgdat->first_deferred_pfn;
+ 	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
+ 
+ 	if (first_init_pfn == ULONG_MAX) {
+ 		pgdat_init_report_one_done();
+ 		return 0;
+ 	}
+ 
+ 	/* Bind memory initialisation thread to a local node if possible */
+ 	if (!cpumask_empty(cpumask))
+ 		set_cpus_allowed_ptr(current, cpumask);
+ 
+ 	/* Sanity check boundaries */
+ 	BUG_ON(pgdat->first_deferred_pfn < pgdat->node_start_pfn);
+ 	BUG_ON(pgdat->first_deferred_pfn > pgdat_end_pfn(pgdat));
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ 
+ 	/* Only the highest zone is deferred so find it */
+ 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
+ 		zone = pgdat->node_zones + zid;
+ 		if (first_init_pfn < zone_end_pfn(zone))
+ 			break;
+ 	}
+ 
+ 	for_each_mem_pfn_range(i, nid, &walk_start, &walk_end, NULL) {
+ 		unsigned long pfn, end_pfn;
+ 		struct page *page = NULL;
+ 		struct page *free_base_page = NULL;
+ 		unsigned long free_base_pfn = 0;
+ 		int nr_to_free = 0;
+ 
+ 		end_pfn = min(walk_end, zone_end_pfn(zone));
+ 		pfn = first_init_pfn;
+ 		if (pfn < walk_start)
+ 			pfn = walk_start;
+ 		if (pfn < zone->zone_start_pfn)
+ 			pfn = zone->zone_start_pfn;
+ 
+ 		for (; pfn < end_pfn; pfn++) {
+ 			if (!pfn_valid_within(pfn))
+ 				goto free_range;
+ 
+ 			/*
+ 			 * Ensure pfn_valid is checked every
+ 			 * MAX_ORDER_NR_PAGES for memory holes
+ 			 */
+ 			if ((pfn & (MAX_ORDER_NR_PAGES - 1)) == 0) {
+ 				if (!pfn_valid(pfn)) {
+ 					page = NULL;
+ 					goto free_range;
+ 				}
+ 			}
+ 
+ 			if (!meminit_pfn_in_nid(pfn, nid, &nid_init_state)) {
+ 				page = NULL;
+ 				goto free_range;
+ 			}
+ 
+ 			/* Minimise pfn page lookups and scheduler checks */
+ 			if (page && (pfn & (MAX_ORDER_NR_PAGES - 1)) != 0) {
+ 				page++;
+ 			} else {
+ 				nr_pages += nr_to_free;
+ 				deferred_free_range(free_base_page,
+ 						free_base_pfn, nr_to_free);
+ 				free_base_page = NULL;
+ 				free_base_pfn = nr_to_free = 0;
+ 
+ 				page = pfn_to_page(pfn);
+ 				cond_resched();
+ 			}
+ 
+ 			if (page->flags) {
+ 				VM_BUG_ON(page_zone(page) != zone);
+ 				goto free_range;
+ 			}
+ 
+ 			__init_single_page(page, pfn, zid, nid);
+ 			if (!free_base_page) {
+ 				free_base_page = page;
+ 				free_base_pfn = pfn;
+ 				nr_to_free = 0;
+ 			}
+ 			nr_to_free++;
+ 
+ 			/* Where possible, batch up pages for a single free */
+ 			continue;
+ free_range:
+ 			/* Free the current block of pages to allocator */
+ 			nr_pages += nr_to_free;
+ 			deferred_free_range(free_base_page, free_base_pfn,
+ 								nr_to_free);
+ 			free_base_page = NULL;
+ 			free_base_pfn = nr_to_free = 0;
+ 		}
+ 
+ 		first_init_pfn = max(end_pfn, first_init_pfn);
+ 	}
+ 
+ 	/* Sanity check that the next zone really is unpopulated */
+ 	WARN_ON(++zid < MAX_NR_ZONES && populated_zone(++zone));
+ 
+ 	pr_info("node %d initialised, %lu pages in %ums\n", nid, nr_pages,
+ 					jiffies_to_msecs(jiffies - start));
+ 
+ 	pgdat_init_report_one_done();
+ 	return 0;
+ }
+ 
+ void __init page_alloc_init_late(void)
+ {
+ 	int nid;
+ 
+ 	/* There will be num_node_state(N_MEMORY) threads */
+ 	atomic_set(&pgdat_init_n_undone, num_node_state(N_MEMORY));
+ 	for_each_node_state(nid, N_MEMORY) {
+ 		kthread_run(deferred_init_memmap, NODE_DATA(nid), "pgdatinit%d", nid);
+ 	}
+ 
+ 	/* Block until all are initialised */
+ 	wait_for_completion(&pgdat_init_all_done_comp);
+ 
+ 	/* Reinit limits that are based on free pages after the kernel is up */
+ 	files_maxfiles_init();
+ }
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
+ 
++>>>>>>> 4248b0da4608 (fs, file table: reinit files_stat.max_files after deferred memory initialisation)
  #ifdef CONFIG_CMA
 -/* Free whole pageblock and set its migration type to MIGRATE_CMA. */
 +/* Free whole pageblock and set it's migration type to MIGRATE_CMA. */
  void __init init_cma_reserved_pageblock(struct page *page)
  {
  	unsigned i = pageblock_nr_pages;
diff --git a/fs/dcache.c b/fs/dcache.c
index 94451775d8ad..79bda59f3b4a 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -3290,22 +3290,15 @@ void __init vfs_caches_init_early(void)
 	inode_init_early();
 }
 
-void __init vfs_caches_init(unsigned long mempages)
+void __init vfs_caches_init(void)
 {
-	unsigned long reserve;
-
-	/* Base hash sizes on available memory, with a reserve equal to
-           150% of current kernel size */
-
-	reserve = min((mempages - nr_free_pages()) * 3/2, mempages - 1);
-	mempages -= reserve;
-
 	names_cachep = kmem_cache_create("names_cache", PATH_MAX, 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
 
 	dcache_init();
 	inode_init();
-	files_init(mempages);
+	files_init();
+	files_maxfiles_init();
 	mnt_init();
 	bdev_cache_init();
 	chrdev_init();
* Unmerged path fs/file_table.c
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 37d4ac39152e..2826ef56889b 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -49,7 +49,8 @@ struct seq_file;
 
 extern void __init inode_init(void);
 extern void __init inode_init_early(void);
-extern void __init files_init(unsigned long);
+extern void __init files_init(void);
+extern void __init files_maxfiles_init(void);
 
 extern struct files_stat_struct files_stat;
 extern unsigned long get_max_files(void);
@@ -2295,7 +2296,7 @@ extern int ioctl_preallocate(struct file *filp, void __user *argp);
 
 /* fs/dcache.c */
 extern void __init vfs_caches_init_early(void);
-extern void __init vfs_caches_init(unsigned long);
+extern void __init vfs_caches_init(void);
 
 extern struct kmem_cache *names_cachep;
 
diff --git a/init/main.c b/init/main.c
index 8678d709aacc..9909e1769757 100644
--- a/init/main.c
+++ b/init/main.c
@@ -617,7 +617,7 @@ asmlinkage void __init start_kernel(void)
 	key_init();
 	security_init();
 	dbg_late_init();
-	vfs_caches_init(totalram_pages);
+	vfs_caches_init();
 	signals_init();
 	/* rootfs populating might need page-writeback */
 	page_writeback_init();
* Unmerged path mm/page_alloc.c
