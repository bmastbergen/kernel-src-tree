libnvdimm, pmem: 'struct page' for pmem

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 32ab0a3f51701cb37ab960635254d5f84ec3de0a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/32ab0a3f.failed

Enable the pmem driver to handle PFN device instances.  Attaching a pmem
namespace to a pfn device triggers the driver to allocate and initialize
struct page entries for pmem.  Memory capacity for this allocation comes
exclusively from RAM for now which is suitable for low PMEM to RAM
ratios.  This mechanism will be expanded later for setting an "allocate
from PMEM" policy.

	Cc: Boaz Harrosh <boaz@plexistor.com>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 32ab0a3f51701cb37ab960635254d5f84ec3de0a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/pmem.c
#	drivers/nvdimm/Kconfig
#	drivers/nvdimm/nd.h
#	drivers/nvdimm/pfn_devs.c
#	tools/testing/nvdimm/Kbuild
#	tools/testing/nvdimm/test/iomap.c
diff --cc drivers/block/pmem.c
index 0069d751af83,2f885e5d9c36..000000000000
--- a/drivers/block/pmem.c
+++ b/drivers/block/pmem.c
@@@ -21,9 -21,13 +21,12 @@@
  #include <linux/init.h>
  #include <linux/platform_device.h>
  #include <linux/module.h>
+ #include <linux/memory_hotplug.h>
  #include <linux/moduleparam.h>
+ #include <linux/vmalloc.h>
  #include <linux/slab.h>
 -#include <linux/pmem.h>
  #include <linux/nd.h>
+ #include "pfn.h"
  #include "nd.h"
  
  struct pmem_device {
@@@ -32,7 -37,9 +36,13 @@@
  
  	/* One contiguous memory region per device */
  	phys_addr_t		phys_addr;
++<<<<<<< HEAD:drivers/block/pmem.c
 +	void			*virt_addr;
++=======
+ 	/* when non-zero this device is hosting a 'pfn' instance */
+ 	phys_addr_t		data_offset;
+ 	void __pmem		*virt_addr;
++>>>>>>> 32ab0a3f5170 (libnvdimm, pmem: 'struct page' for pmem):drivers/nvdimm/pmem.c
  	size_t			size;
  };
  
@@@ -43,10 -50,11 +53,15 @@@ static void pmem_do_bvec(struct pmem_de
  			sector_t sector)
  {
  	void *mem = kmap_atomic(page);
++<<<<<<< HEAD:drivers/block/pmem.c
 +	size_t pmem_off = sector << 9;
++=======
+ 	phys_addr_t pmem_off = sector * 512 + pmem->data_offset;
+ 	void __pmem *pmem_addr = pmem->virt_addr + pmem_off;
++>>>>>>> 32ab0a3f5170 (libnvdimm, pmem: 'struct page' for pmem):drivers/nvdimm/pmem.c
  
  	if (rw == READ) {
 -		memcpy_from_pmem(mem + off, pmem_addr, len);
 +		memcpy(mem + off, pmem->virt_addr + pmem_off, len);
  		flush_dcache_page(page);
  	} else {
  		flush_dcache_page(page);
@@@ -81,14 -98,22 +96,21 @@@ static int pmem_rw_page(struct block_de
  }
  
  static long pmem_direct_access(struct block_device *bdev, sector_t sector,
 -		      void __pmem **kaddr, unsigned long *pfn)
 +			      void **kaddr, unsigned long *pfn, long size)
  {
  	struct pmem_device *pmem = bdev->bd_disk->private_data;
- 	size_t offset = sector << 9;
- 
- 	if (!pmem)
- 		return -ENODEV;
+ 	resource_size_t offset = sector * 512 + pmem->data_offset;
+ 	resource_size_t size;
+ 
+ 	if (pmem->data_offset) {
+ 		/*
+ 		 * Limit the direct_access() size to what is covered by
+ 		 * the memmap
+ 		 */
+ 		size = (pmem->size - offset) & ~ND_PFN_MASK;
+ 	} else
+ 		size = pmem->size - offset;
  
 -	/* FIXME convert DAX to comprehend that this mapping has a lifetime */
  	*kaddr = pmem->virt_addr + offset;
  	*pfn = (pmem->phys_addr + offset) >> PAGE_SHIFT;
  
@@@ -115,22 -138,37 +137,44 @@@ static struct pmem_device *pmem_alloc(s
  
  	pmem->phys_addr = res->start;
  	pmem->size = resource_size(res);
 -	if (!arch_has_wmb_pmem())
 -		dev_warn(dev, "unable to guarantee persistence of writes\n");
  
 -	if (!devm_request_mem_region(dev, pmem->phys_addr, pmem->size,
 -			dev_name(dev))) {
 +	err = -EINVAL;
 +	if (!request_mem_region(pmem->phys_addr, pmem->size, "pmem")) {
  		dev_warn(dev, "could not reserve region [0x%pa:0x%zx]\n",
  				&pmem->phys_addr, pmem->size);
 -		return ERR_PTR(-EBUSY);
 +		goto out_free_dev;
  	}
  
 -	pmem->virt_addr = memremap_pmem(dev, pmem->phys_addr, pmem->size);
 +	/*
 +	 * Map the memory as non-cachable, as we can't write back the contents
 +	 * of the CPU caches in case of a crash.
 +	 */
 +	err = -ENOMEM;
 +	pmem->virt_addr = ioremap_nocache(pmem->phys_addr, pmem->size);
  	if (!pmem->virt_addr)
++<<<<<<< HEAD:drivers/block/pmem.c
 +		goto out_release_region;
++=======
+ 		return ERR_PTR(-ENXIO);
+ 
+ 	return pmem;
+ }
+ 
+ static void pmem_detach_disk(struct pmem_device *pmem)
+ {
+ 	if (!pmem->pmem_disk)
+ 		return;
+ 
+ 	del_gendisk(pmem->pmem_disk);
+ 	put_disk(pmem->pmem_disk);
+ 	blk_cleanup_queue(pmem->pmem_queue);
+ }
+ 
+ static int pmem_attach_disk(struct device *dev,
+ 		struct nd_namespace_common *ndns, struct pmem_device *pmem)
+ {
+ 	struct gendisk *disk;
++>>>>>>> 32ab0a3f5170 (libnvdimm, pmem: 'struct page' for pmem):drivers/nvdimm/pmem.c
  
  	pmem->pmem_queue = blk_alloc_queue(GFP_KERNEL);
  	if (!pmem->pmem_queue)
@@@ -152,37 -192,185 +196,191 @@@
  	disk->private_data	= pmem;
  	disk->queue		= pmem->pmem_queue;
  	disk->flags		= GENHD_FL_EXT_DEVT;
++<<<<<<< HEAD:drivers/block/pmem.c
 +	sprintf(disk->disk_name, "pmem%d", id);
 +	disk->driverfs_dev = dev;
 +	set_capacity(disk, pmem->size >> 9);
++=======
+ 	nvdimm_namespace_disk_name(ndns, disk->disk_name);
+ 	disk->driverfs_dev = dev;
+ 	set_capacity(disk, (pmem->size - pmem->data_offset) / 512);
++>>>>>>> 32ab0a3f5170 (libnvdimm, pmem: 'struct page' for pmem):drivers/nvdimm/pmem.c
  	pmem->pmem_disk = disk;
  
  	add_disk(disk);
 -	revalidate_disk(disk);
  
 -	return 0;
 +	return pmem;
 +
 +out_free_queue:
 +	blk_cleanup_queue(pmem->pmem_queue);
 +out_unmap:
 +	iounmap(pmem->virt_addr);
 +out_release_region:
 +	release_mem_region(pmem->phys_addr, pmem->size);
 +out_free_dev:
 +	kfree(pmem);
 +out:
 +	return ERR_PTR(err);
  }
  
 -static int pmem_rw_bytes(struct nd_namespace_common *ndns,
 -		resource_size_t offset, void *buf, size_t size, int rw)
 +static void pmem_free(struct pmem_device *pmem)
  {
 -	struct pmem_device *pmem = dev_get_drvdata(ndns->claim);
 -
 -	if (unlikely(offset + size > pmem->size)) {
 -		dev_WARN_ONCE(&ndns->dev, 1, "request out of range\n");
 -		return -EFAULT;
 -	}
 -
 -	if (rw == READ)
 -		memcpy_from_pmem(buf, pmem->virt_addr + offset, size);
 -	else {
 -		memcpy_to_pmem(pmem->virt_addr + offset, buf, size);
 -		wmb_pmem();
 -	}
 -
 -	return 0;
 +	del_gendisk(pmem->pmem_disk);
 +	put_disk(pmem->pmem_disk);
 +	blk_cleanup_queue(pmem->pmem_queue);
 +	iounmap(pmem->virt_addr);
 +	release_mem_region(pmem->phys_addr, pmem->size);
 +	kfree(pmem);
  }
  
+ static int nd_pfn_init(struct nd_pfn *nd_pfn)
+ {
+ 	struct nd_pfn_sb *pfn_sb = kzalloc(sizeof(*pfn_sb), GFP_KERNEL);
+ 	struct pmem_device *pmem = dev_get_drvdata(&nd_pfn->dev);
+ 	struct nd_namespace_common *ndns = nd_pfn->ndns;
+ 	struct nd_region *nd_region;
+ 	unsigned long npfns;
+ 	phys_addr_t offset;
+ 	u64 checksum;
+ 	int rc;
+ 
+ 	if (!pfn_sb)
+ 		return -ENOMEM;
+ 
+ 	nd_pfn->pfn_sb = pfn_sb;
+ 	rc = nd_pfn_validate(nd_pfn);
+ 	if (rc == 0 || rc == -EBUSY)
+ 		return rc;
+ 
+ 	/* section alignment for simple hotplug */
+ 	if (nvdimm_namespace_capacity(ndns) < ND_PFN_ALIGN
+ 			|| pmem->phys_addr & ND_PFN_MASK)
+ 		return -ENODEV;
+ 
+ 	nd_region = to_nd_region(nd_pfn->dev.parent);
+ 	if (nd_region->ro) {
+ 		dev_info(&nd_pfn->dev,
+ 				"%s is read-only, unable to init metadata\n",
+ 				dev_name(&nd_region->dev));
+ 		goto err;
+ 	}
+ 
+ 	memset(pfn_sb, 0, sizeof(*pfn_sb));
+ 	npfns = (pmem->size - SZ_8K) / SZ_4K;
+ 	/*
+ 	 * Note, we use 64 here for the standard size of struct page,
+ 	 * debugging options may cause it to be larger in which case the
+ 	 * implementation will limit the pfns advertised through
+ 	 * ->direct_access() to those that are included in the memmap.
+ 	 */
+ 	if (nd_pfn->mode == PFN_MODE_PMEM)
+ 		offset = ALIGN(SZ_8K + 64 * npfns, PMD_SIZE);
+ 	else if (nd_pfn->mode == PFN_MODE_RAM)
+ 		offset = SZ_8K;
+ 	else
+ 		goto err;
+ 
+ 	npfns = (pmem->size - offset) / SZ_4K;
+ 	pfn_sb->mode = cpu_to_le32(nd_pfn->mode);
+ 	pfn_sb->dataoff = cpu_to_le64(offset);
+ 	pfn_sb->npfns = cpu_to_le64(npfns);
+ 	memcpy(pfn_sb->signature, PFN_SIG, PFN_SIG_LEN);
+ 	memcpy(pfn_sb->uuid, nd_pfn->uuid, 16);
+ 	pfn_sb->version_major = cpu_to_le16(1);
+ 	checksum = nd_sb_checksum((struct nd_gen_sb *) pfn_sb);
+ 	pfn_sb->checksum = cpu_to_le64(checksum);
+ 
+ 	rc = nvdimm_write_bytes(ndns, SZ_4K, pfn_sb, sizeof(*pfn_sb));
+ 	if (rc)
+ 		goto err;
+ 
+ 	return 0;
+  err:
+ 	nd_pfn->pfn_sb = NULL;
+ 	kfree(pfn_sb);
+ 	return -ENXIO;
+ }
+ 
+ static int nvdimm_namespace_detach_pfn(struct nd_namespace_common *ndns)
+ {
+ 	struct nd_pfn *nd_pfn = to_nd_pfn(ndns->claim);
+ 	struct pmem_device *pmem;
+ 
+ 	/* free pmem disk */
+ 	pmem = dev_get_drvdata(&nd_pfn->dev);
+ 	pmem_detach_disk(pmem);
+ 
+ 	/* release nd_pfn resources */
+ 	kfree(nd_pfn->pfn_sb);
+ 	nd_pfn->pfn_sb = NULL;
+ 
+ 	return 0;
+ }
+ 
+ static int nvdimm_namespace_attach_pfn(struct nd_namespace_common *ndns)
+ {
+ 	struct nd_namespace_io *nsio = to_nd_namespace_io(&ndns->dev);
+ 	struct nd_pfn *nd_pfn = to_nd_pfn(ndns->claim);
+ 	struct device *dev = &nd_pfn->dev;
+ 	struct vmem_altmap *altmap;
+ 	struct nd_region *nd_region;
+ 	struct nd_pfn_sb *pfn_sb;
+ 	struct pmem_device *pmem;
+ 	phys_addr_t offset;
+ 	int rc;
+ 
+ 	if (!nd_pfn->uuid || !nd_pfn->ndns)
+ 		return -ENODEV;
+ 
+ 	nd_region = to_nd_region(dev->parent);
+ 	rc = nd_pfn_init(nd_pfn);
+ 	if (rc)
+ 		return rc;
+ 
+ 	if (PAGE_SIZE != SZ_4K) {
+ 		dev_err(dev, "only supported on systems with 4K PAGE_SIZE\n");
+ 		return -ENXIO;
+ 	}
+ 	if (nsio->res.start & ND_PFN_MASK) {
+ 		dev_err(dev, "%s not memory hotplug section aligned\n",
+ 				dev_name(&ndns->dev));
+ 		return -ENXIO;
+ 	}
+ 
+ 	pfn_sb = nd_pfn->pfn_sb;
+ 	offset = le64_to_cpu(pfn_sb->dataoff);
+ 	nd_pfn->mode = le32_to_cpu(nd_pfn->pfn_sb->mode);
+ 	if (nd_pfn->mode == PFN_MODE_RAM) {
+ 		if (offset != SZ_8K)
+ 			return -EINVAL;
+ 		nd_pfn->npfns = le64_to_cpu(pfn_sb->npfns);
+ 		altmap = NULL;
+ 	} else {
+ 		rc = -ENXIO;
+ 		goto err;
+ 	}
+ 
+ 	/* establish pfn range for lookup, and switch to direct map */
+ 	pmem = dev_get_drvdata(dev);
+ 	memunmap_pmem(dev, pmem->virt_addr);
+ 	pmem->virt_addr = (void __pmem *)devm_memremap_pages(dev, &nsio->res);
+ 	if (IS_ERR(pmem->virt_addr)) {
+ 		rc = PTR_ERR(pmem->virt_addr);
+ 		goto err;
+ 	}
+ 
+ 	/* attach pmem disk in "pfn-mode" */
+ 	pmem->data_offset = offset;
+ 	rc = pmem_attach_disk(dev, ndns, pmem);
+ 	if (rc)
+ 		goto err;
+ 
+ 	return rc;
+  err:
+ 	nvdimm_namespace_detach_pfn(ndns);
+ 	return rc;
+ }
+ 
  static int nd_pmem_probe(struct device *dev)
  {
  	struct nd_region *nd_region = to_nd_region(dev->parent);
@@@ -193,16 -387,40 +391,47 @@@
  	if (IS_ERR(pmem))
  		return PTR_ERR(pmem);
  
+ 	pmem->ndns = ndns;
  	dev_set_drvdata(dev, pmem);
 -	ndns->rw_bytes = pmem_rw_bytes;
  
++<<<<<<< HEAD:drivers/block/pmem.c
 +	return 0;
++=======
+ 	if (is_nd_btt(dev))
+ 		return nvdimm_namespace_attach_btt(ndns);
+ 
+ 	if (is_nd_pfn(dev))
+ 		return nvdimm_namespace_attach_pfn(ndns);
+ 
+ 	if (nd_btt_probe(ndns, pmem) == 0) {
+ 		/* we'll come back as btt-pmem */
+ 		return -ENXIO;
+ 	}
+ 
+ 	if (nd_pfn_probe(ndns, pmem) == 0) {
+ 		/* we'll come back as pfn-pmem */
+ 		return -ENXIO;
+ 	}
+ 
+ 	return pmem_attach_disk(dev, ndns, pmem);
++>>>>>>> 32ab0a3f5170 (libnvdimm, pmem: 'struct page' for pmem):drivers/nvdimm/pmem.c
  }
  
  static int nd_pmem_remove(struct device *dev)
  {
  	struct pmem_device *pmem = dev_get_drvdata(dev);
  
++<<<<<<< HEAD:drivers/block/pmem.c
 +	pmem_free(pmem);
++=======
+ 	if (is_nd_btt(dev))
+ 		nvdimm_namespace_detach_btt(pmem->ndns);
+ 	else if (is_nd_pfn(dev))
+ 		nvdimm_namespace_detach_pfn(pmem->ndns);
+ 	else
+ 		pmem_detach_disk(pmem);
+ 
++>>>>>>> 32ab0a3f5170 (libnvdimm, pmem: 'struct page' for pmem):drivers/nvdimm/pmem.c
  	return 0;
  }
  
* Unmerged path drivers/nvdimm/Kconfig
* Unmerged path drivers/nvdimm/nd.h
* Unmerged path drivers/nvdimm/pfn_devs.c
* Unmerged path tools/testing/nvdimm/Kbuild
* Unmerged path tools/testing/nvdimm/test/iomap.c
* Unmerged path drivers/block/pmem.c
* Unmerged path drivers/nvdimm/Kconfig
* Unmerged path drivers/nvdimm/nd.h
* Unmerged path drivers/nvdimm/pfn_devs.c
* Unmerged path tools/testing/nvdimm/Kbuild
* Unmerged path tools/testing/nvdimm/test/iomap.c
