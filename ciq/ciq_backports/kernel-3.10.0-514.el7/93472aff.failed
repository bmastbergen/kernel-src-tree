perf/x86: Fix 'active_events' imbalance

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] perf: Fix 'active_events' imbalance (Jiri Olsa) [1312571]
Rebuild_FUZZ: 94.59%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 93472aff802fd7b61f2209335207e9bd793012f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/93472aff.failed

Commit 1b7b938f1817 ("perf/x86/intel: Fix PMI handling for Intel PT") conditionally
increments active_events in x86_add_exclusive() but unconditionally decrements in
x86_del_exclusive().

These extra decrements can lead to the situation where
active_events is zero and thus the PMI handler is 'disabled'
while we have active events on the PMU generating PMIs.

This leads to a truckload of:

  Uhhuh. NMI received for unknown reason 21 on CPU 28.
  Do you have a strange power saving mode enabled?
  Dazed and confused, but trying to continue

messages and generally messes up perf.

Remove the condition on the increment, double increment balanced
by a double decrement is perfectly fine.

Restructure the code a little bit to make the unconditional inc
a bit more natural.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: alexander.shishkin@linux.intel.com
	Cc: brgerst@gmail.com
	Cc: dvlasenk@redhat.com
	Cc: luto@amacapital.net
	Cc: oleg@redhat.com
Fixes: 1b7b938f1817 ("perf/x86/intel: Fix PMI handling for Intel PT")
Link: http://lkml.kernel.org/r/20150624144750.GJ18673@twins.programming.kicks-ass.net
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 93472aff802fd7b61f2209335207e9bd793012f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event.c
diff --cc arch/x86/kernel/cpu/perf_event.c
index f1131c0efdf8,3658de47900f..000000000000
--- a/arch/x86/kernel/cpu/perf_event.c
+++ b/arch/x86/kernel/cpu/perf_event.c
@@@ -326,22 -357,24 +326,38 @@@ set_ext_hw_attr(struct hw_perf_event *h
   */
  int x86_add_exclusive(unsigned int what)
  {
- 	int ret = -EBUSY, i;
+ 	int i;
  
++<<<<<<< HEAD
 +	if (atomic_inc_not_zero(&x86_pmu.lbr_exclusive[what]))
 +		return 0;
 +
 +	mutex_lock(&pmc_reserve_mutex);
 +	for (i = 0; i < ARRAY_SIZE(x86_pmu.lbr_exclusive); i++)
 +		if (i != what && atomic_read(&x86_pmu.lbr_exclusive[i]))
 +			goto out;
++=======
+ 	if (!atomic_inc_not_zero(&x86_pmu.lbr_exclusive[what])) {
+ 		mutex_lock(&pmc_reserve_mutex);
+ 		for (i = 0; i < ARRAY_SIZE(x86_pmu.lbr_exclusive); i++) {
+ 			if (i != what && atomic_read(&x86_pmu.lbr_exclusive[i]))
+ 				goto fail_unlock;
+ 		}
+ 		atomic_inc(&x86_pmu.lbr_exclusive[what]);
+ 		mutex_unlock(&pmc_reserve_mutex);
+ 	}
++>>>>>>> 93472aff802f (perf/x86: Fix 'active_events' imbalance)
  
- 	atomic_inc(&x86_pmu.lbr_exclusive[what]);
- 	ret = 0;
+ 	atomic_inc(&active_events);
+ 	return 0;
  
- out:
+ fail_unlock:
  	mutex_unlock(&pmc_reserve_mutex);
++<<<<<<< HEAD
 +	return ret;
++=======
+ 	return -EBUSY;
++>>>>>>> 93472aff802f (perf/x86: Fix 'active_events' imbalance)
  }
  
  void x86_del_exclusive(unsigned int what)
* Unmerged path arch/x86/kernel/cpu/perf_event.c
