x86/asm/tsc: Add rdtscll() merge helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [x86] tsc: Add rdtscll() merge helper (Mitsuhiro Tanino) [1372398]
Rebuild_FUZZ: 88.57%
commit-author Ingo Molnar <mingo@kernel.org>
commit 99770737ca7e3ebc14e66460a69b7032de9421e1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/99770737.failed

Some in-flight code makes use of the old rdtscll() (now removed), provide a wrapper
for a kernel cycle to smooth the transition to rdtsc().

( We use the safest variant, rdtsc_ordered(), which has barriers - this adds another
  incentive to remove the wrapper in the future. )

	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Huang Rui <ray.huang@amd.com>
	Cc: John Stultz <john.stultz@linaro.org>
	Cc: Len Brown <lenb@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: kvm ML <kvm@vger.kernel.org>
Link: http://lkml.kernel.org/r/dddbf98a2af53312e9aa73a5a2b1622fe5d6f52b.1434501121.git.luto@kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 99770737ca7e3ebc14e66460a69b7032de9421e1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/msr.h
diff --cc arch/x86/include/asm/msr.h
index 441ecf83d81a,54e9f088919d..000000000000
--- a/arch/x86/include/asm/msr.h
+++ b/arch/x86/include/asm/msr.h
@@@ -120,6 -126,35 +120,38 @@@ static __always_inline unsigned long lo
  	return EAX_EDX_VAL(val, low, high);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * rdtsc_ordered() - read the current TSC in program order
+  *
+  * rdtsc_ordered() returns the result of RDTSC as a 64-bit integer.
+  * It is ordered like a load to a global in-memory counter.  It should
+  * be impossible to observe non-monotonic rdtsc_unordered() behavior
+  * across multiple CPUs as long as the TSC is synced.
+  */
+ static __always_inline unsigned long long rdtsc_ordered(void)
+ {
+ 	/*
+ 	 * The RDTSC instruction is not ordered relative to memory
+ 	 * access.  The Intel SDM and the AMD APM are both vague on this
+ 	 * point, but empirically an RDTSC instruction can be
+ 	 * speculatively executed before prior loads.  An RDTSC
+ 	 * immediately after an appropriate barrier appears to be
+ 	 * ordered as a normal load, that is, it provides the same
+ 	 * ordering guarantees as reading from a global memory location
+ 	 * that some other imaginary CPU is updating continuously with a
+ 	 * time stamp.
+ 	 */
+ 	alternative_2("", "mfence", X86_FEATURE_MFENCE_RDTSC,
+ 			  "lfence", X86_FEATURE_LFENCE_RDTSC);
+ 	return rdtsc();
+ }
+ 
+ /* Deprecated, keep it for a cycle for easier merging: */
+ #define rdtscll(now)	do { (now) = rdtsc_ordered(); } while (0)
+ 
++>>>>>>> 99770737ca7e (x86/asm/tsc: Add rdtscll() merge helper)
  static inline unsigned long long native_read_pmc(int counter)
  {
  	DECLARE_ARGS(val, low, high);
* Unmerged path arch/x86/include/asm/msr.h
