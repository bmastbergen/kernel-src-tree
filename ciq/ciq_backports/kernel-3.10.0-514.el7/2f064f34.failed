mm: make page pfmemalloc check more robust

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] make page pfmemalloc check more robust (Ivan Vecera) [1268334]
Rebuild_FUZZ: 95.00%
commit-author Michal Hocko <mhocko@suse.com>
commit 2f064f3485cd29633ad1b3cfb00cc519509a3d72
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/2f064f34.failed

Commit c48a11c7ad26 ("netvm: propagate page->pfmemalloc to skb") added
checks for page->pfmemalloc to __skb_fill_page_desc():

        if (page->pfmemalloc && !page->mapping)
                skb->pfmemalloc = true;

It assumes page->mapping == NULL implies that page->pfmemalloc can be
trusted.  However, __delete_from_page_cache() can set set page->mapping
to NULL and leave page->index value alone.  Due to being in union, a
non-zero page->index will be interpreted as true page->pfmemalloc.

So the assumption is invalid if the networking code can see such a page.
And it seems it can.  We have encountered this with a NFS over loopback
setup when such a page is attached to a new skbuf.  There is no copying
going on in this case so the page confuses __skb_fill_page_desc which
interprets the index as pfmemalloc flag and the network stack drops
packets that have been allocated using the reserves unless they are to
be queued on sockets handling the swapping which is the case here and
that leads to hangs when the nfs client waits for a response from the
server which has been dropped and thus never arrive.

The struct page is already heavily packed so rather than finding another
hole to put it in, let's do a trick instead.  We can reuse the index
again but define it to an impossible value (-1UL).  This is the page
index so it should never see the value that large.  Replace all direct
users of page->pfmemalloc by page_is_pfmemalloc which will hide this
nastiness from unspoiled eyes.

The information will get lost if somebody wants to use page->index
obviously but that was the case before and the original code expected
that the information should be persisted somewhere else if that is
really needed (e.g.  what SLAB and SLUB do).

[akpm@linux-foundation.org: fix blooper in slub]
Fixes: c48a11c7ad26 ("netvm: propagate page->pfmemalloc to skb")
	Signed-off-by: Michal Hocko <mhocko@suse.com>
	Debugged-by: Vlastimil Babka <vbabka@suse.com>
	Debugged-by: Jiri Bohac <jbohac@suse.com>
	Cc: Eric Dumazet <eric.dumazet@gmail.com>
	Cc: David Miller <davem@davemloft.net>
	Acked-by: Mel Gorman <mgorman@suse.de>
	Cc: <stable@vger.kernel.org>	[3.6+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 2f064f3485cd29633ad1b3cfb00cc519509a3d72)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm_types.h
#	mm/page_alloc.c
#	mm/slab.c
#	net/core/skbuff.c
diff --cc include/linux/mm_types.h
index 43f423b198b4,15549578d559..000000000000
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@@ -57,19 -62,7 +57,23 @@@ struct page 
  	struct {
  		union {
  			pgoff_t index;		/* Our offset within mapping. */
++<<<<<<< HEAD
 +			void *freelist;		/* slub/slob first free object */
 +			bool pfmemalloc;	/* If set by the page allocator,
 +						 * ALLOC_NO_WATERMARKS was set
 +						 * and the low watermark was not
 +						 * met implying that the system
 +						 * is under some pressure. The
 +						 * caller should try ensure
 +						 * this page is only used to
 +						 * free other pages.
 +						 */
 +#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && USE_SPLIT_PMD_PTLOCKS
 +		pgtable_t pmd_huge_pte; /* protected by page->ptl */
 +#endif
++=======
+ 			void *freelist;		/* sl[aou]b first free object */
++>>>>>>> 2f064f3485cd (mm: make page pfmemalloc check more robust)
  		};
  
  		union {
diff --cc mm/page_alloc.c
index f60ded95bce9,5b5240b7f642..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -900,13 -1340,18 +900,25 @@@ static int prep_new_page(struct page *p
  	if (order && (gfp_flags & __GFP_COMP))
  		prep_compound_page(page, order);
  
 -	set_page_owner(page, order, gfp_flags);
 -
  	/*
++<<<<<<< HEAD
 +	 * Make sure the caller of get_page_unless_zero() will see the
 +	 * fully initialized page. Say, to ensure that compound_lock()
 +	 * can't race with the non-atomic __SetPage*() above.
 +	 */
 +	smp_wmb();
 +	set_page_refcounted(page);
++=======
+ 	 * page is set pfmemalloc when ALLOC_NO_WATERMARKS was necessary to
+ 	 * allocate the page. The expectation is that the caller is taking
+ 	 * steps that will free more memory. The caller should avoid the page
+ 	 * being used for !PFMEMALLOC purposes.
+ 	 */
+ 	if (alloc_flags & ALLOC_NO_WATERMARKS)
+ 		set_page_pfmemalloc(page);
+ 	else
+ 		clear_page_pfmemalloc(page);
++>>>>>>> 2f064f3485cd (mm: make page pfmemalloc check more robust)
  
  	return 0;
  }
@@@ -2810,17 -3294,139 +2822,115 @@@ void free_pages(unsigned long addr, uns
  EXPORT_SYMBOL(free_pages);
  
  /*
 - * Page Fragment:
 - *  An arbitrary-length arbitrary-offset area of memory which resides
 - *  within a 0 or higher order page.  Multiple fragments within that page
 - *  are individually refcounted, in the page's reference counter.
 + * __free_memcg_kmem_pages and free_memcg_kmem_pages will free
 + * pages allocated with __GFP_KMEMCG.
   *
++<<<<<<< HEAD
 + * Those pages are accounted to a particular memcg, embedded in the
 + * corresponding page_cgroup. To avoid adding a hit in the allocator to search
 + * for that information only to find out that it is NULL for users who have no
 + * interest in that whatsoever, we provide these functions.
++=======
+  * The page_frag functions below provide a simple allocation framework for
+  * page fragments.  This is used by the network stack and network device
+  * drivers to provide a backing region of memory for use as either an
+  * sk_buff->head, or to be used in the "frags" portion of skb_shared_info.
+  */
+ static struct page *__page_frag_refill(struct page_frag_cache *nc,
+ 				       gfp_t gfp_mask)
+ {
+ 	struct page *page = NULL;
+ 	gfp_t gfp = gfp_mask;
+ 
+ #if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)
+ 	gfp_mask |= __GFP_COMP | __GFP_NOWARN | __GFP_NORETRY |
+ 		    __GFP_NOMEMALLOC;
+ 	page = alloc_pages_node(NUMA_NO_NODE, gfp_mask,
+ 				PAGE_FRAG_CACHE_MAX_ORDER);
+ 	nc->size = page ? PAGE_FRAG_CACHE_MAX_SIZE : PAGE_SIZE;
+ #endif
+ 	if (unlikely(!page))
+ 		page = alloc_pages_node(NUMA_NO_NODE, gfp, 0);
+ 
+ 	nc->va = page ? page_address(page) : NULL;
+ 
+ 	return page;
+ }
+ 
+ void *__alloc_page_frag(struct page_frag_cache *nc,
+ 			unsigned int fragsz, gfp_t gfp_mask)
+ {
+ 	unsigned int size = PAGE_SIZE;
+ 	struct page *page;
+ 	int offset;
+ 
+ 	if (unlikely(!nc->va)) {
+ refill:
+ 		page = __page_frag_refill(nc, gfp_mask);
+ 		if (!page)
+ 			return NULL;
+ 
+ #if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)
+ 		/* if size can vary use size else just use PAGE_SIZE */
+ 		size = nc->size;
+ #endif
+ 		/* Even if we own the page, we do not use atomic_set().
+ 		 * This would break get_page_unless_zero() users.
+ 		 */
+ 		atomic_add(size - 1, &page->_count);
+ 
+ 		/* reset page count bias and offset to start of new frag */
+ 		nc->pfmemalloc = page_is_pfmemalloc(page);
+ 		nc->pagecnt_bias = size;
+ 		nc->offset = size;
+ 	}
+ 
+ 	offset = nc->offset - fragsz;
+ 	if (unlikely(offset < 0)) {
+ 		page = virt_to_page(nc->va);
+ 
+ 		if (!atomic_sub_and_test(nc->pagecnt_bias, &page->_count))
+ 			goto refill;
+ 
+ #if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)
+ 		/* if size can vary use size else just use PAGE_SIZE */
+ 		size = nc->size;
+ #endif
+ 		/* OK, page count is 0, we can safely set it */
+ 		atomic_set(&page->_count, size);
+ 
+ 		/* reset page count bias and offset to start of new frag */
+ 		nc->pagecnt_bias = size;
+ 		offset = size - fragsz;
+ 	}
+ 
+ 	nc->pagecnt_bias--;
+ 	nc->offset = offset;
+ 
+ 	return nc->va + offset;
+ }
+ EXPORT_SYMBOL(__alloc_page_frag);
+ 
+ /*
+  * Frees a page fragment allocated out of either a compound or order 0 page.
+  */
 -void __free_page_frag(void *addr)
 -{
 -	struct page *page = virt_to_head_page(addr);
 -
 -	if (unlikely(put_page_testzero(page)))
 -		__free_pages_ok(page, compound_order(page));
 -}
 -EXPORT_SYMBOL(__free_page_frag);
 -
 -/*
 - * alloc_kmem_pages charges newly allocated pages to the kmem resource counter
 - * of the current memory cgroup.
 - *
 - * It should be used when the caller would like to use kmalloc, but since the
 - * allocation is large, it has to fall back to the page allocator.
 - */
 -struct page *alloc_kmem_pages(gfp_t gfp_mask, unsigned int order)
 -{
 -	struct page *page;
 -	struct mem_cgroup *memcg = NULL;
 -
 -	if (!memcg_kmem_newpage_charge(gfp_mask, &memcg, order))
 -		return NULL;
 -	page = alloc_pages(gfp_mask, order);
 -	memcg_kmem_commit_charge(page, memcg, order);
 -	return page;
 -}
 -
 -struct page *alloc_kmem_pages_node(int nid, gfp_t gfp_mask, unsigned int order)
++void __free_page_frag(void *addr)
+ {
 -	struct page *page;
 -	struct mem_cgroup *memcg = NULL;
++	struct page *page = virt_to_head_page(addr);
+ 
 -	if (!memcg_kmem_newpage_charge(gfp_mask, &memcg, order))
 -		return NULL;
 -	page = alloc_pages_node(nid, gfp_mask, order);
 -	memcg_kmem_commit_charge(page, memcg, order);
 -	return page;
++	if (unlikely(put_page_testzero(page)))
++		__free_pages_ok(page, compound_order(page));
+ }
++EXPORT_SYMBOL(__free_page_frag);
+ 
+ /*
 - * __free_kmem_pages and free_kmem_pages will free pages allocated with
 - * alloc_kmem_pages.
++ * alloc_kmem_pages charges newly allocated pages to the kmem resource counter
++ * of the current memory cgroup.
++>>>>>>> 2f064f3485cd (mm: make page pfmemalloc check more robust)
 + *
 + * The caller knows better which flags it relies on.
   */
 -void __free_kmem_pages(struct page *page, unsigned int order)
 +void __free_memcg_kmem_pages(struct page *page, unsigned int order)
  {
  	memcg_kmem_uncharge_pages(page, order);
  	__free_pages(page, order);
diff --cc mm/slab.c
index 2343cb092452,bbd0b47dc6a9..000000000000
--- a/mm/slab.c
+++ b/mm/slab.c
@@@ -1766,13 -1613,9 +1766,19 @@@ static void *kmem_getpages(struct kmem_
  	else
  		add_zone_page_state(page_zone(page),
  			NR_SLAB_UNRECLAIMABLE, nr_pages);
++<<<<<<< HEAD
 +	for (i = 0; i < nr_pages; i++) {
 +		__SetPageSlab(page + i);
 +
 +		if (page->pfmemalloc)
 +			SetPageSlabPfmemalloc(page + i);
 +	}
 +	memcg_bind_pages(cachep, cachep->gfporder);
++=======
+ 	__SetPageSlab(page);
+ 	if (page_is_pfmemalloc(page))
+ 		SetPageSlabPfmemalloc(page);
++>>>>>>> 2f064f3485cd (mm: make page pfmemalloc check more robust)
  
  	if (kmemcheck_enabled && !(cachep->flags & SLAB_NOTRACK)) {
  		kmemcheck_alloc_shadow(page, cachep->gfporder, flags, nodeid);
diff --cc net/core/skbuff.c
index d9282bd8c9a4,7b84330e5d30..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -356,87 -328,27 +356,107 @@@ struct sk_buff *build_skb(void *data, u
  
  	return skb;
  }
++<<<<<<< HEAD
++=======
+ 
+ /* build_skb() is wrapper over __build_skb(), that specifically
+  * takes care of skb->head and skb->pfmemalloc
+  * This means that if @frag_size is not zero, then @data must be backed
+  * by a page fragment, not kmalloc() or vmalloc()
+  */
+ struct sk_buff *build_skb(void *data, unsigned int frag_size)
+ {
+ 	struct sk_buff *skb = __build_skb(data, frag_size);
+ 
+ 	if (skb && frag_size) {
+ 		skb->head_frag = 1;
+ 		if (page_is_pfmemalloc(virt_to_head_page(data)))
+ 			skb->pfmemalloc = 1;
+ 	}
+ 	return skb;
+ }
++>>>>>>> 2f064f3485cd (mm: make page pfmemalloc check more robust)
  EXPORT_SYMBOL(build_skb);
  
 -static DEFINE_PER_CPU(struct page_frag_cache, netdev_alloc_cache);
 -static DEFINE_PER_CPU(struct page_frag_cache, napi_alloc_cache);
 +struct netdev_alloc_cache {
 +	struct page_frag	frag;
 +	/* we maintain a pagecount bias, so that we dont dirty cache line
 +	 * containing page->_count every time we allocate a fragment.
 +	 */
 +	unsigned int		pagecnt_bias;
 +};
 +static DEFINE_PER_CPU(struct netdev_alloc_cache, netdev_alloc_cache);
 +static DEFINE_PER_CPU(struct netdev_alloc_cache, napi_alloc_cache);
 +
 +static struct page *__page_frag_refill(struct netdev_alloc_cache *nc,
 +				       gfp_t gfp_mask)
 +{
 +	const unsigned int order = NETDEV_FRAG_PAGE_MAX_ORDER;
 +	struct page *page = NULL;
 +	gfp_t gfp = gfp_mask;
 +
 +	if (order) {
 +		gfp_mask |= __GFP_COMP | __GFP_NOWARN | __GFP_NORETRY;
 +		page = alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);
 +		nc->frag.size = PAGE_SIZE << (page ? order : 0);
 +	}
 +
 +	if (unlikely(!page))
 +		page = alloc_pages_node(NUMA_NO_NODE, gfp, 0);
 +
 +	nc->frag.page = page;
 +
 +	return page;
 +}
 +
 +static void *__alloc_page_frag(struct netdev_alloc_cache __percpu *cache,
 +			       unsigned int fragsz, gfp_t gfp_mask)
 +{
 +	struct netdev_alloc_cache *nc = this_cpu_ptr(cache);
 +	struct page *page = nc->frag.page;
 +	unsigned int size;
 +	int offset;
 +
 +	if (unlikely(!page)) {
 +refill:
 +		page = __page_frag_refill(nc, gfp_mask);
 +		if (!page)
 +			return NULL;
 +
 +		/* if size can vary use frag.size else just use PAGE_SIZE */
 +		size = NETDEV_FRAG_PAGE_MAX_ORDER ? nc->frag.size : PAGE_SIZE;
 +
 +		/* Even if we own the page, we do not use atomic_set().
 +		 * This would break get_page_unless_zero() users.
 +		 */
 +		atomic_add(size - 1, &page->_count);
 +
 +		/* reset page count bias and offset to start of new frag */
 +		nc->pagecnt_bias = size;
 +		nc->frag.offset = size;
 +	}
 +
 +	offset = nc->frag.offset - fragsz;
 +	if (unlikely(offset < 0)) {
 +		if (!atomic_sub_and_test(nc->pagecnt_bias, &page->_count))
 +			goto refill;
 +
 +		/* if size can vary use frag.size else just use PAGE_SIZE */
 +		size = NETDEV_FRAG_PAGE_MAX_ORDER ? nc->frag.size : PAGE_SIZE;
 +
 +		/* OK, page count is 0, we can safely set it */
 +		atomic_set(&page->_count, size);
 +
 +		/* reset page count bias and offset to start of new frag */
 +		nc->pagecnt_bias = size;
 +		offset = size - fragsz;
 +	}
 +
 +	nc->pagecnt_bias--;
 +	nc->frag.offset = offset;
 +
 +	return page_address(page) + offset;
 +}
  
  static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
  {
diff --git a/drivers/net/ethernet/intel/fm10k/fm10k_main.c b/drivers/net/ethernet/intel/fm10k/fm10k_main.c
index 9a55e7bf1c88..0c48c6af4b3b 100644
--- a/drivers/net/ethernet/intel/fm10k/fm10k_main.c
+++ b/drivers/net/ethernet/intel/fm10k/fm10k_main.c
@@ -216,7 +216,7 @@ static void fm10k_reuse_rx_page(struct fm10k_ring *rx_ring,
 
 static inline bool fm10k_page_is_reserved(struct page *page)
 {
-	return (page_to_nid(page) != numa_mem_id()) || page->pfmemalloc;
+	return (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);
 }
 
 static bool fm10k_can_reuse_rx_page(struct fm10k_rx_buffer *rx_buffer,
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index 8e35a37c7917..6ba967bd34d4 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -6574,7 +6574,7 @@ static void igb_reuse_rx_page(struct igb_ring *rx_ring,
 
 static inline bool igb_page_is_reserved(struct page *page)
 {
-	return (page_to_nid(page) != numa_mem_id()) || page->pfmemalloc;
+	return (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);
 }
 
 static bool igb_can_reuse_rx_page(struct igb_rx_buffer *rx_buffer,
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index e464f2e3e2eb..a74cbf2a8443 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -1812,7 +1812,7 @@ static void ixgbe_reuse_rx_page(struct ixgbe_ring *rx_ring,
 
 static inline bool ixgbe_page_is_reserved(struct page *page)
 {
-	return (page_to_nid(page) != numa_mem_id()) || page->pfmemalloc;
+	return (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);
 }
 
 /**
diff --git a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 5edcc4fd4be3..c2e8aee78a7c 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@ -748,7 +748,7 @@ static void ixgbevf_reuse_rx_page(struct ixgbevf_ring *rx_ring,
 
 static inline bool ixgbevf_page_is_reserved(struct page *page)
 {
-	return (page_to_nid(page) != numa_mem_id()) || page->pfmemalloc;
+	return (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);
 }
 
 /**
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 77beacdc1090..8db56a8b68d2 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -994,6 +994,34 @@ static inline int page_mapped(struct page *page)
 	return atomic_read(&(page)->_mapcount) >= 0;
 }
 
+/*
+ * Return true only if the page has been allocated with
+ * ALLOC_NO_WATERMARKS and the low watermark was not
+ * met implying that the system is under some pressure.
+ */
+static inline bool page_is_pfmemalloc(struct page *page)
+{
+	/*
+	 * Page index cannot be this large so this must be
+	 * a pfmemalloc page.
+	 */
+	return page->index == -1UL;
+}
+
+/*
+ * Only to be called by the page allocator on a freshly allocated
+ * page.
+ */
+static inline void set_page_pfmemalloc(struct page *page)
+{
+	page->index = -1UL;
+}
+
+static inline void clear_page_pfmemalloc(struct page *page)
+{
+	page->index = 0;
+}
+
 /*
  * Different kinds of faults, as returned by handle_mm_fault().
  * Used to decide whether a process gets delivered SIGBUS or
* Unmerged path include/linux/mm_types.h
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 3247ee7f6870..96534cb336bc 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -1560,20 +1560,16 @@ static inline void __skb_fill_page_desc(struct sk_buff *skb, int i,
 	skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
 
 	/*
-	 * Propagate page->pfmemalloc to the skb if we can. The problem is
-	 * that not all callers have unique ownership of the page. If
-	 * pfmemalloc is set, we check the mapping as a mapping implies
-	 * page->index is set (index and pfmemalloc share space).
-	 * If it's a valid mapping, we cannot use page->pfmemalloc but we
-	 * do not lose pfmemalloc information as the pages would not be
-	 * allocated using __GFP_MEMALLOC.
+	 * Propagate page pfmemalloc to the skb if we can. The problem is
+	 * that not all callers have unique ownership of the page but rely
+	 * on page_is_pfmemalloc doing the right thing(tm).
 	 */
 	frag->page.p		  = page;
 	frag->page_offset	  = off;
 	skb_frag_size_set(frag, size);
 
 	page = compound_head(page);
-	if (page->pfmemalloc && !page->mapping)
+	if (page_is_pfmemalloc(page))
 		skb->pfmemalloc	= true;
 }
 
@@ -2251,7 +2247,7 @@ static inline struct page *__skb_alloc_page(gfp_t gfp_mask,
 static inline void skb_propagate_pfmemalloc(struct page *page,
 					     struct sk_buff *skb)
 {
-	if (page && page->pfmemalloc)
+	if (page_is_pfmemalloc(page))
 		skb->pfmemalloc = true;
 }
 
* Unmerged path mm/page_alloc.c
* Unmerged path mm/slab.c
diff --git a/mm/slub.c b/mm/slub.c
index 9bb60eec298f..f799c0ea8239 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1375,7 +1375,7 @@ static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
 	memcg_bind_pages(s, order);
 	page->slab_cache = s;
 	__SetPageSlab(page);
-	if (page->pfmemalloc)
+	if (page_is_pfmemalloc(page))
 		SetPageSlabPfmemalloc(page);
 
 	start = page_address(page);
* Unmerged path net/core/skbuff.c
