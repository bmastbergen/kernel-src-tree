net/mlx5e: Add fragmented memory support for RX multi packet WQE

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Add fragmented memory support for RX multi packet WQE (kamal heib) [1275159 1296272 1296405 1298421 1298422 1298423 1298424 1298425]
Rebuild_FUZZ: 96.77%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit bc77b240b3c57236cdcc08d64ca390655d3a16ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/bc77b240.failed

If the allocation of a linear (physically continuous) MPWQE fails,
we allocate a fragmented MPWQE.

This is implemented via device's UMR (User Memory Registration)
which allows to register multiple memory fragments into ConnectX
hardware as a continuous buffer.
UMR registration is an asynchronous operation and is done via
ICO SQs.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit bc77b240b3c57236cdcc08d64ca390655d3a16ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index a33b9e6fa485,c99fdff74c97..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -53,6 -57,26 +53,29 @@@
  #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
  #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
  
++<<<<<<< HEAD
++=======
+ #define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x1
+ #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x4
+ #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
+ 
+ #define MLX5_MPWRQ_LOG_NUM_STRIDES		11 /* >= 9, HW restriction */
+ #define MLX5_MPWRQ_LOG_STRIDE_SIZE		6  /* >= 6, HW restriction */
+ #define MLX5_MPWRQ_NUM_STRIDES			BIT(MLX5_MPWRQ_LOG_NUM_STRIDES)
+ #define MLX5_MPWRQ_STRIDE_SIZE			BIT(MLX5_MPWRQ_LOG_STRIDE_SIZE)
+ #define MLX5_MPWRQ_LOG_WQE_SZ			(MLX5_MPWRQ_LOG_NUM_STRIDES +\
+ 						 MLX5_MPWRQ_LOG_STRIDE_SIZE)
+ #define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
+ 				    MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT : 0)
+ #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
+ #define MLX5_MPWRQ_STRIDES_PER_PAGE		(MLX5_MPWRQ_NUM_STRIDES >> \
+ 						 MLX5_MPWRQ_WQE_PAGE_ORDER)
+ #define MLX5_CHANNEL_MAX_NUM_MTTS (ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8) * \
+ 				   BIT(MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW))
+ #define MLX5_UMR_ALIGN				(2048)
+ #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
+ 
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
  #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC      0x10
  #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS      0x20
@@@ -68,6 -93,61 +91,64 @@@
  #define MLX5E_SQ_BF_BUDGET             16
  
  #define MLX5E_NUM_MAIN_GROUPS 9
++<<<<<<< HEAD
++=======
+ #define MLX5E_NET_IP_ALIGN 2
+ 
+ static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
+ {
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return min_t(u16, MLX5E_PARAMS_DEFAULT_MIN_RX_WQES_MPW,
+ 			     wq_size / 2);
+ 	default:
+ 		return min_t(u16, MLX5E_PARAMS_DEFAULT_MIN_RX_WQES,
+ 			     wq_size / 2);
+ 	}
+ }
+ 
+ static inline int mlx5_min_log_rq_size(int wq_type)
+ {
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW;
+ 	default:
+ 		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE;
+ 	}
+ }
+ 
+ static inline int mlx5_max_log_rq_size(int wq_type)
+ {
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW;
+ 	default:
+ 		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE;
+ 	}
+ }
+ 
+ struct mlx5e_tx_wqe {
+ 	struct mlx5_wqe_ctrl_seg ctrl;
+ 	struct mlx5_wqe_eth_seg  eth;
+ };
+ 
+ struct mlx5e_rx_wqe {
+ 	struct mlx5_wqe_srq_next_seg  next;
+ 	struct mlx5_wqe_data_seg      data;
+ };
+ 
+ struct mlx5e_umr_wqe {
+ 	struct mlx5_wqe_ctrl_seg       ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg   uctrl;
+ 	struct mlx5_mkey_seg           mkc;
+ 	struct mlx5_wqe_data_seg       data;
+ };
+ 
+ #ifdef CONFIG_MLX5_CORE_EN_DCB
+ #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
+ #define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
+ #endif
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  
  static const char vport_strings[][ETH_GSTRING_LEN] = {
  	/* vport statistics */
@@@ -105,6 -188,8 +186,11 @@@
  	"tx_queue_wake",
  	"tx_queue_dropped",
  	"rx_wqe_err",
++<<<<<<< HEAD
++=======
+ 	"rx_mpwqe_filler",
+ 	"rx_mpwqe_frag",
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  };
  
  struct mlx5e_vport_stats {
@@@ -143,8 -231,10 +229,15 @@@
  	u64 tx_queue_wake;
  	u64 tx_queue_dropped;
  	u64 rx_wqe_err;
++<<<<<<< HEAD
 +
 +#define NUM_VPORT_COUNTERS     32
++=======
+ 	u64 rx_mpwqe_filler;
+ 	u64 rx_mpwqe_frag;
+ 
+ #define NUM_VPORT_COUNTERS     37
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  };
  
  static const char pport_strings[][ETH_GSTRING_LEN] = {
@@@ -227,7 -327,9 +320,13 @@@ static const char rq_stats_strings[][ET
  	"csum_sw",
  	"lro_packets",
  	"lro_bytes",
++<<<<<<< HEAD
 +	"wqe_err"
++=======
+ 	"wqe_err",
+ 	"mpwqe_filler",
+ 	"mpwqe_frag",
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  };
  
  struct mlx5e_rq_stats {
@@@ -237,7 -340,9 +336,13 @@@
  	u64 lro_packets;
  	u64 lro_bytes;
  	u64 wqe_err;
++<<<<<<< HEAD
 +#define NUM_RQ_STATS 6
++=======
+ 	u64 mpwqe_filler;
+ 	u64 mpwqe_frag;
+ #define NUM_RQ_STATS 9
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  };
  
  static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
@@@ -305,11 -438,25 +411,31 @@@ struct mlx5e_cq 
  	struct mlx5_wq_ctrl        wq_ctrl;
  } ____cacheline_aligned_in_smp;
  
++<<<<<<< HEAD
++=======
+ struct mlx5e_rq;
+ typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
+ 				       struct mlx5_cqe64 *cqe);
+ typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
+ 				  u16 ix);
+ 
+ struct mlx5e_dma_info {
+ 	struct page	*page;
+ 	dma_addr_t	addr;
+ };
+ 
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  struct mlx5e_rq {
  	/* data path */
  	struct mlx5_wq_ll      wq;
  	u32                    wqe_sz;
  	struct sk_buff       **skb;
++<<<<<<< HEAD
++=======
+ 	struct mlx5e_mpw_info *wqe_info;
+ 	__be32                 mkey_be;
+ 	__be32                 umr_mkey_be;
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  
  	struct device         *pdev;
  	struct net_device     *netdev;
@@@ -326,7 -477,37 +452,41 @@@
  	struct mlx5e_priv     *priv;
  } ____cacheline_aligned_in_smp;
  
++<<<<<<< HEAD
 +struct mlx5e_tx_skb_cb {
++=======
+ struct mlx5e_umr_dma_info {
+ 	__be64                *mtt;
+ 	__be64                *mtt_no_align;
+ 	dma_addr_t             mtt_addr;
+ 	struct mlx5e_dma_info *dma_info;
+ };
+ 
+ struct mlx5e_mpw_info {
+ 	union {
+ 		struct mlx5e_dma_info     dma_info;
+ 		struct mlx5e_umr_dma_info umr;
+ 	};
+ 	u16 consumed_strides;
+ 	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
+ 
+ 	void (*dma_pre_sync)(struct device *pdev,
+ 			     struct mlx5e_mpw_info *wi,
+ 			     u32 wqe_offset, u32 len);
+ 	void (*add_skb_frag)(struct device *pdev,
+ 			     struct sk_buff *skb,
+ 			     struct mlx5e_mpw_info *wi,
+ 			     u32 page_idx, u32 frag_offset, u32 len);
+ 	void (*copy_skb_header)(struct device *pdev,
+ 				struct sk_buff *skb,
+ 				struct mlx5e_mpw_info *wi,
+ 				u32 page_idx, u32 offset,
+ 				u32 headlen);
+ 	void (*free_wqe)(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
+ };
+ 
+ struct mlx5e_tx_wqe_info {
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  	u32 num_bytes;
  	u8  num_wqebbs;
  	u8  num_dma;
@@@ -497,7 -698,8 +657,12 @@@ struct mlx5e_priv 
  	struct mlx5_uar            cq_uar;
  	u32                        pdn;
  	u32                        tdn;
++<<<<<<< HEAD
 +	struct mlx5_core_mr        mr;
++=======
+ 	struct mlx5_core_mkey      mkey;
+ 	struct mlx5_core_mkey      umr_mkey;
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  	struct mlx5e_rq            drop_rq;
  
  	struct mlx5e_channel     **channel;
@@@ -571,9 -764,29 +736,29 @@@ netdev_tx_t mlx5e_xmit(struct sk_buff *
  void mlx5e_completion_event(struct mlx5_core_cq *mcq);
  void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
  int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 -bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 +bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq);
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 -
 -void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 -void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
  bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
++<<<<<<< HEAD
++=======
+ int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
+ int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
+ void mlx5e_post_rx_fragmented_mpwqe(struct mlx5e_rq *rq);
+ void mlx5e_complete_rx_linear_mpwqe(struct mlx5e_rq *rq,
+ 				    struct mlx5_cqe64 *cqe,
+ 				    u16 byte_cnt,
+ 				    struct mlx5e_mpw_info *wi,
+ 				    struct sk_buff *skb);
+ void mlx5e_complete_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
+ 					struct mlx5_cqe64 *cqe,
+ 					u16 byte_cnt,
+ 					struct mlx5e_mpw_info *wi,
+ 					struct sk_buff *skb);
+ void mlx5e_free_rx_linear_mpwqe(struct mlx5e_rq *rq,
+ 				struct mlx5e_mpw_info *wi);
+ void mlx5e_free_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
+ 				    struct mlx5e_mpw_info *wi);
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
  
  void mlx5e_update_stats(struct mlx5e_priv *priv);
@@@ -611,16 -833,12 +796,25 @@@ static inline void mlx5e_tx_notify_hw(s
  	 * doorbell
  	 */
  	wmb();
++<<<<<<< HEAD
 +
 +	if (bf_sz) {
 +		__iowrite64_copy(sq->uar_bf_map + ofst, &wqe->ctrl, bf_sz);
 +
 +		/* flush the write-combining mapped buffer */
 +		wmb();
 +
 +	} else {
 +		mlx5_write64((__be32 *)&wqe->ctrl, sq->uar_map + ofst, NULL);
 +	}
++=======
+ 	if (bf_sz)
+ 		__iowrite64_copy(sq->uar_map + ofst, ctrl, bf_sz);
+ 	else
+ 		mlx5_write64((__be32 *)ctrl, sq->uar_map + ofst, NULL);
+ 	/* flush the write-combining mapped buffer */
+ 	wmb();
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  
  	sq->bf_offset ^= sq->bf_buf_size;
  }
@@@ -639,7 -857,17 +833,12 @@@ static inline int mlx5e_get_max_num_cha
  		     MLX5E_MAX_NUM_CHANNELS);
  }
  
+ static inline int mlx5e_get_mtt_octw(int npages)
+ {
+ 	return ALIGN(npages, 8) / 2;
+ }
+ 
  extern const struct ethtool_ops mlx5e_ethtool_ops;
 -#ifdef CONFIG_MLX5_CORE_EN_DCB
 -extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
 -int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
 -#endif
 -
  u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
  
  #endif /* __MLX5_EN_H__ */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index a8ca30e0bf3d,942829e6d8ba..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -159,6 -178,8 +159,11 @@@ void mlx5e_update_stats(struct mlx5e_pr
  	s->rx_csum_none		= 0;
  	s->rx_csum_sw		= 0;
  	s->rx_wqe_err		= 0;
++<<<<<<< HEAD
++=======
+ 	s->rx_mpwqe_filler	= 0;
+ 	s->rx_mpwqe_frag	= 0;
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  	for (i = 0; i < priv->params.num_channels; i++) {
  		rq_stats = &priv->channel[i]->rq.stats;
  
@@@ -169,6 -190,8 +174,11 @@@
  		s->rx_csum_none	+= rq_stats->csum_none;
  		s->rx_csum_sw	+= rq_stats->csum_sw;
  		s->rx_wqe_err   += rq_stats->wqe_err;
++<<<<<<< HEAD
++=======
+ 		s->rx_mpwqe_filler += rq_stats->mpwqe_filler;
+ 		s->rx_mpwqe_frag   += rq_stats->mpwqe_frag;
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  
  		for (j = 0; j < priv->params.num_tc; j++) {
  			sq_stats = &priv->channel[i]->sq[j].stats;
@@@ -323,28 -345,49 +333,32 @@@ static int mlx5e_create_rq(struct mlx5e
  	rq->wq.db = &rq->wq.db[MLX5_RCV_DBR];
  
  	wq_sz = mlx5_wq_ll_get_size(&rq->wq);
 -
 -	switch (priv->params.rq_wq_type) {
 -	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
 -		rq->wqe_info = kzalloc_node(wq_sz * sizeof(*rq->wqe_info),
 -					    GFP_KERNEL, cpu_to_node(c->cpu));
 -		if (!rq->wqe_info) {
 -			err = -ENOMEM;
 -			goto err_rq_wq_destroy;
 -		}
 -		rq->handle_rx_cqe = mlx5e_handle_rx_cqe_mpwrq;
 -		rq->alloc_wqe = mlx5e_alloc_rx_mpwqe;
 -
 -		rq->wqe_sz = MLX5_MPWRQ_NUM_STRIDES * MLX5_MPWRQ_STRIDE_SIZE;
 -		byte_count = rq->wqe_sz;
 -		break;
 -	default: /* MLX5_WQ_TYPE_LINKED_LIST */
 -		rq->skb = kzalloc_node(wq_sz * sizeof(*rq->skb), GFP_KERNEL,
 -				       cpu_to_node(c->cpu));
 -		if (!rq->skb) {
 -			err = -ENOMEM;
 -			goto err_rq_wq_destroy;
 -		}
 -		rq->handle_rx_cqe = mlx5e_handle_rx_cqe;
 -		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
 -
 -		rq->wqe_sz = (priv->params.lro_en) ?
 -				priv->params.lro_wqe_sz :
 -				MLX5E_SW2HW_MTU(priv->netdev->mtu);
 -		rq->wqe_sz = SKB_DATA_ALIGN(rq->wqe_sz + MLX5E_NET_IP_ALIGN);
 -		byte_count = rq->wqe_sz - MLX5E_NET_IP_ALIGN;
 -		byte_count |= MLX5_HW_START_PADDING;
 +	rq->skb = kzalloc_node(wq_sz * sizeof(*rq->skb), GFP_KERNEL,
 +			       cpu_to_node(c->cpu));
 +	if (!rq->skb) {
 +		err = -ENOMEM;
 +		goto err_rq_wq_destroy;
  	}
  
 +	rq->wqe_sz = (priv->params.lro_en) ? priv->params.lro_wqe_sz :
 +					     MLX5E_SW2HW_MTU(priv->netdev->mtu);
 +	rq->wqe_sz = SKB_DATA_ALIGN(rq->wqe_sz + MLX5E_NET_IP_ALIGN);
 +
  	for (i = 0; i < wq_sz; i++) {
  		struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(&rq->wq, i);
 +		u32 byte_count = rq->wqe_sz - MLX5E_NET_IP_ALIGN;
  
++<<<<<<< HEAD
 +		wqe->data.lkey       = c->mkey_be;
 +		wqe->data.byte_count =
 +			cpu_to_be32(byte_count | MLX5_HW_START_PADDING);
++=======
+ 		wqe->data.byte_count = cpu_to_be32(byte_count);
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  	}
  
 -	rq->wq_type = priv->params.rq_wq_type;
  	rq->pdev    = c->pdev;
  	rq->netdev  = c->netdev;
 -	rq->tstamp  = &priv->tstamp;
  	rq->channel = c;
  	rq->ix      = c->ix;
  	rq->priv    = c->priv;
@@@ -2165,8 -2477,21 +2189,26 @@@ static void mlx5e_build_netdev_priv(str
  
  	priv->params.log_sq_size           =
  		MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
++<<<<<<< HEAD
 +	priv->params.log_rq_size           =
 +		MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
++=======
+ 	priv->params.rq_wq_type = mlx5e_check_fragmented_striding_rq_cap(mdev) ?
+ 		MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ :
+ 		MLX5_WQ_TYPE_LINKED_LIST;
+ 
+ 	switch (priv->params.rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		priv->params.log_rq_size = MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW;
+ 		priv->params.lro_en = true;
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_LINKED_LIST */
+ 		priv->params.log_rq_size = MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
+ 	}
+ 
+ 	priv->params.min_rx_wqes = mlx5_min_rx_wqes(priv->params.rq_wq_type,
+ 					    BIT(priv->params.log_rq_size));
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  	priv->params.rx_cq_moderation_usec =
  		MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC;
  	priv->params.rx_cq_moderation_pkts =
@@@ -2289,6 -2630,61 +2331,64 @@@ static int mlx5e_create_mkey(struct mlx
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static void mlx5e_create_q_counter(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	int err;
+ 
+ 	err = mlx5_core_alloc_q_counter(mdev, &priv->q_counter);
+ 	if (err) {
+ 		mlx5_core_warn(mdev, "alloc queue counter failed, %d\n", err);
+ 		priv->q_counter = 0;
+ 	}
+ }
+ 
+ static void mlx5e_destroy_q_counter(struct mlx5e_priv *priv)
+ {
+ 	if (!priv->q_counter)
+ 		return;
+ 
+ 	mlx5_core_dealloc_q_counter(priv->mdev, priv->q_counter);
+ }
+ 
+ static int mlx5e_create_umr_mkey(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	struct mlx5_create_mkey_mbox_in *in;
+ 	struct mlx5_mkey_seg *mkc;
+ 	int inlen = sizeof(*in);
+ 	u64 npages =
+ 		mlx5e_get_max_num_channels(mdev) * MLX5_CHANNEL_MAX_NUM_MTTS;
+ 	int err;
+ 
+ 	in = mlx5_vzalloc(inlen);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	mkc = &in->seg;
+ 	mkc->status = MLX5_MKEY_STATUS_FREE;
+ 	mkc->flags = MLX5_PERM_UMR_EN |
+ 		     MLX5_PERM_LOCAL_READ |
+ 		     MLX5_PERM_LOCAL_WRITE |
+ 		     MLX5_ACCESS_MODE_MTT;
+ 
+ 	mkc->qpn_mkey7_0 = cpu_to_be32(0xffffff << 8);
+ 	mkc->flags_pd = cpu_to_be32(priv->pdn);
+ 	mkc->len = cpu_to_be64(npages << PAGE_SHIFT);
+ 	mkc->xlt_oct_size = cpu_to_be32(mlx5e_get_mtt_octw(npages));
+ 	mkc->log2_page_size = PAGE_SHIFT;
+ 
+ 	err = mlx5_core_create_mkey(mdev, &priv->umr_mkey, in, inlen, NULL,
+ 				    NULL, NULL);
+ 
+ 	kvfree(in);
+ 
+ 	return err;
+ }
+ 
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  static void *mlx5e_create_netdev(struct mlx5_core_dev *mdev)
  {
  	struct net_device *netdev;
@@@ -2403,8 -2826,11 +2509,11 @@@ err_close_drop_rq
  err_destroy_tises:
  	mlx5e_destroy_tises(priv);
  
+ err_destroy_umr_mkey:
+ 	mlx5_core_destroy_mkey(mdev, &priv->umr_mkey);
+ 
  err_destroy_mkey:
 -	mlx5_core_destroy_mkey(mdev, &priv->mkey);
 +	mlx5_core_destroy_mkey(mdev, &priv->mr);
  
  err_dealloc_transport_domain:
  	mlx5_core_dealloc_transport_domain(mdev, priv->tdn);
@@@ -2438,7 -2867,8 +2547,12 @@@ static void mlx5e_destroy_netdev(struc
  	mlx5e_destroy_rqt(priv, MLX5E_INDIRECTION_RQT);
  	mlx5e_close_drop_rq(priv);
  	mlx5e_destroy_tises(priv);
++<<<<<<< HEAD
 +	mlx5_core_destroy_mkey(priv->mdev, &priv->mr);
++=======
+ 	mlx5_core_destroy_mkey(priv->mdev, &priv->umr_mkey);
+ 	mlx5_core_destroy_mkey(priv->mdev, &priv->mkey);
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  	mlx5_core_dealloc_transport_domain(priv->mdev, priv->tdn);
  	mlx5_core_dealloc_pd(priv->mdev, priv->pdn);
  	mlx5_unmap_free_uar(priv->mdev, &priv->cq_uar);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 2dc1f9b26b65,d71919ccf912..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -72,6 -77,369 +73,372 @@@ err_free_skb
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
++=======
+ static inline void
+ mlx5e_dma_pre_sync_linear_mpwqe(struct device *pdev,
+ 				struct mlx5e_mpw_info *wi,
+ 				u32 wqe_offset, u32 len)
+ {
+ 	dma_sync_single_for_cpu(pdev, wi->dma_info.addr + wqe_offset,
+ 				len, DMA_FROM_DEVICE);
+ }
+ 
+ static inline void
+ mlx5e_dma_pre_sync_fragmented_mpwqe(struct device *pdev,
+ 				    struct mlx5e_mpw_info *wi,
+ 				    u32 wqe_offset, u32 len)
+ {
+ 	/* No dma pre sync for fragmented MPWQE */
+ }
+ 
+ static inline void
+ mlx5e_add_skb_frag_linear_mpwqe(struct device *pdev,
+ 				struct sk_buff *skb,
+ 				struct mlx5e_mpw_info *wi,
+ 				u32 page_idx, u32 frag_offset,
+ 				u32 len)
+ {
+ 	unsigned int truesize =	ALIGN(len, MLX5_MPWRQ_STRIDE_SIZE);
+ 
+ 	wi->skbs_frags[page_idx]++;
+ 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+ 			&wi->dma_info.page[page_idx], frag_offset,
+ 			len, truesize);
+ }
+ 
+ static inline void
+ mlx5e_add_skb_frag_fragmented_mpwqe(struct device *pdev,
+ 				    struct sk_buff *skb,
+ 				    struct mlx5e_mpw_info *wi,
+ 				    u32 page_idx, u32 frag_offset,
+ 				    u32 len)
+ {
+ 	unsigned int truesize =	ALIGN(len, MLX5_MPWRQ_STRIDE_SIZE);
+ 
+ 	dma_sync_single_for_cpu(pdev,
+ 				wi->umr.dma_info[page_idx].addr + frag_offset,
+ 				len, DMA_FROM_DEVICE);
+ 	wi->skbs_frags[page_idx]++;
+ 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+ 			wi->umr.dma_info[page_idx].page, frag_offset,
+ 			len, truesize);
+ }
+ 
+ static inline void
+ mlx5e_copy_skb_header_linear_mpwqe(struct device *pdev,
+ 				   struct sk_buff *skb,
+ 				   struct mlx5e_mpw_info *wi,
+ 				   u32 page_idx, u32 offset,
+ 				   u32 headlen)
+ {
+ 	struct page *page = &wi->dma_info.page[page_idx];
+ 
+ 	skb_copy_to_linear_data(skb, page_address(page) + offset,
+ 				ALIGN(headlen, sizeof(long)));
+ }
+ 
+ static inline void
+ mlx5e_copy_skb_header_fragmented_mpwqe(struct device *pdev,
+ 				       struct sk_buff *skb,
+ 				       struct mlx5e_mpw_info *wi,
+ 				       u32 page_idx, u32 offset,
+ 				       u32 headlen)
+ {
+ 	u16 headlen_pg = min_t(u32, headlen, PAGE_SIZE - offset);
+ 	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[page_idx];
+ 	unsigned int len;
+ 
+ 	 /* Aligning len to sizeof(long) optimizes memcpy performance */
+ 	len = ALIGN(headlen_pg, sizeof(long));
+ 	dma_sync_single_for_cpu(pdev, dma_info->addr + offset, len,
+ 				DMA_FROM_DEVICE);
+ 	skb_copy_to_linear_data_offset(skb, 0,
+ 				       page_address(dma_info->page) + offset,
+ 				       len);
+ #if (MLX5_MPWRQ_SMALL_PACKET_THRESHOLD >= MLX5_MPWRQ_STRIDE_SIZE)
+ 	if (unlikely(offset + headlen > PAGE_SIZE)) {
+ 		dma_info++;
+ 		headlen_pg = len;
+ 		len = ALIGN(headlen - headlen_pg, sizeof(long));
+ 		dma_sync_single_for_cpu(pdev, dma_info->addr, len,
+ 					DMA_FROM_DEVICE);
+ 		skb_copy_to_linear_data_offset(skb, headlen_pg,
+ 					       page_address(dma_info->page),
+ 					       len);
+ 	}
+ #endif
+ }
+ 
+ static u16 mlx5e_get_wqe_mtt_offset(u16 rq_ix, u16 wqe_ix)
+ {
+ 	return rq_ix * MLX5_CHANNEL_MAX_NUM_MTTS +
+ 		wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
+ }
+ 
+ static void mlx5e_build_umr_wqe(struct mlx5e_rq *rq,
+ 				struct mlx5e_sq *sq,
+ 				struct mlx5e_umr_wqe *wqe,
+ 				u16 ix)
+ {
+ 	struct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;
+ 	struct mlx5_wqe_data_seg      *dseg = &wqe->data;
+ 	struct mlx5e_mpw_info *wi = &rq->wqe_info[ix];
+ 	u8 ds_cnt = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS);
+ 	u16 umr_wqe_mtt_offset = mlx5e_get_wqe_mtt_offset(rq->ix, ix);
+ 
+ 	memset(wqe, 0, sizeof(*wqe));
+ 	cseg->opmod_idx_opcode =
+ 		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
+ 			    MLX5_OPCODE_UMR);
+ 	cseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |
+ 				      ds_cnt);
+ 	cseg->fm_ce_se  = MLX5_WQE_CTRL_CQ_UPDATE;
+ 	cseg->imm       = rq->umr_mkey_be;
+ 
+ 	ucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN;
+ 	ucseg->klm_octowords =
+ 		cpu_to_be16(mlx5e_get_mtt_octw(MLX5_MPWRQ_PAGES_PER_WQE));
+ 	ucseg->bsf_octowords =
+ 		cpu_to_be16(mlx5e_get_mtt_octw(umr_wqe_mtt_offset));
+ 	ucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ 
+ 	dseg->lkey = sq->mkey_be;
+ 	dseg->addr = cpu_to_be64(wi->umr.mtt_addr);
+ }
+ 
+ static void mlx5e_post_umr_wqe(struct mlx5e_rq *rq, u16 ix)
+ {
+ 	struct mlx5e_sq *sq = &rq->channel->icosq;
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5e_umr_wqe *wqe;
+ 	u8 num_wqebbs = DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_BB);
+ 	u16 pi;
+ 
+ 	/* fill sq edge with nops to avoid wqe wrap around */
+ 	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
+ 		sq->ico_wqe_info[pi].opcode = MLX5_OPCODE_NOP;
+ 		sq->ico_wqe_info[pi].num_wqebbs = 1;
+ 		mlx5e_send_nop(sq, true);
+ 	}
+ 
+ 	wqe = mlx5_wq_cyc_get_wqe(wq, pi);
+ 	mlx5e_build_umr_wqe(rq, sq, wqe, ix);
+ 	sq->ico_wqe_info[pi].opcode = MLX5_OPCODE_UMR;
+ 	sq->ico_wqe_info[pi].num_wqebbs = num_wqebbs;
+ 	sq->pc += num_wqebbs;
+ 	mlx5e_tx_notify_hw(sq, &wqe->ctrl, 0);
+ }
+ 
+ static inline int mlx5e_get_wqe_mtt_sz(void)
+ {
+ 	/* UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
+ 	 * To avoid copying garbage after the mtt array, we allocate
+ 	 * a little more.
+ 	 */
+ 	return ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(__be64),
+ 		     MLX5_UMR_MTT_ALIGNMENT);
+ }
+ 
+ static int mlx5e_alloc_and_map_page(struct mlx5e_rq *rq,
+ 				    struct mlx5e_mpw_info *wi,
+ 				    int i)
+ {
+ 	struct page *page;
+ 
+ 	page = dev_alloc_page();
+ 	if (unlikely(!page))
+ 		return -ENOMEM;
+ 
+ 	wi->umr.dma_info[i].page = page;
+ 	wi->umr.dma_info[i].addr = dma_map_page(rq->pdev, page, 0, PAGE_SIZE,
+ 						PCI_DMA_FROMDEVICE);
+ 	if (unlikely(dma_mapping_error(rq->pdev, wi->umr.dma_info[i].addr))) {
+ 		put_page(page);
+ 		return -ENOMEM;
+ 	}
+ 	wi->umr.mtt[i] = cpu_to_be64(wi->umr.dma_info[i].addr | MLX5_EN_WR);
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5e_alloc_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
+ 					   struct mlx5e_rx_wqe *wqe,
+ 					   u16 ix)
+ {
+ 	struct mlx5e_mpw_info *wi = &rq->wqe_info[ix];
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	u32 dma_offset = mlx5e_get_wqe_mtt_offset(rq->ix, ix) << PAGE_SHIFT;
+ 	int i;
+ 
+ 	wi->umr.dma_info = kmalloc(sizeof(*wi->umr.dma_info) *
+ 				   MLX5_MPWRQ_PAGES_PER_WQE,
+ 				   GFP_ATOMIC);
+ 	if (unlikely(!wi->umr.dma_info))
+ 		goto err_out;
+ 
+ 	/* We allocate more than mtt_sz as we will align the pointer */
+ 	wi->umr.mtt_no_align = kzalloc(mtt_sz + MLX5_UMR_ALIGN - 1,
+ 				       GFP_ATOMIC);
+ 	if (unlikely(!wi->umr.mtt_no_align))
+ 		goto err_free_umr;
+ 
+ 	wi->umr.mtt = PTR_ALIGN(wi->umr.mtt_no_align, MLX5_UMR_ALIGN);
+ 	wi->umr.mtt_addr = dma_map_single(rq->pdev, wi->umr.mtt, mtt_sz,
+ 					  PCI_DMA_TODEVICE);
+ 	if (unlikely(dma_mapping_error(rq->pdev, wi->umr.mtt_addr)))
+ 		goto err_free_mtt;
+ 
+ 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
+ 		if (unlikely(mlx5e_alloc_and_map_page(rq, wi, i)))
+ 			goto err_unmap;
+ 		atomic_add(MLX5_MPWRQ_STRIDES_PER_PAGE,
+ 			   &wi->umr.dma_info[i].page->_count);
+ 		wi->skbs_frags[i] = 0;
+ 	}
+ 
+ 	wi->consumed_strides = 0;
+ 	wi->dma_pre_sync = mlx5e_dma_pre_sync_fragmented_mpwqe;
+ 	wi->add_skb_frag = mlx5e_add_skb_frag_fragmented_mpwqe;
+ 	wi->copy_skb_header = mlx5e_copy_skb_header_fragmented_mpwqe;
+ 	wi->free_wqe     = mlx5e_free_rx_fragmented_mpwqe;
+ 	wqe->data.lkey = rq->umr_mkey_be;
+ 	wqe->data.addr = cpu_to_be64(dma_offset);
+ 
+ 	return 0;
+ 
+ err_unmap:
+ 	while (--i >= 0) {
+ 		dma_unmap_page(rq->pdev, wi->umr.dma_info[i].addr, PAGE_SIZE,
+ 			       PCI_DMA_FROMDEVICE);
+ 		atomic_sub(MLX5_MPWRQ_STRIDES_PER_PAGE,
+ 			   &wi->umr.dma_info[i].page->_count);
+ 		put_page(wi->umr.dma_info[i].page);
+ 	}
+ 	dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz, PCI_DMA_TODEVICE);
+ 
+ err_free_mtt:
+ 	kfree(wi->umr.mtt_no_align);
+ 
+ err_free_umr:
+ 	kfree(wi->umr.dma_info);
+ 
+ err_out:
+ 	return -ENOMEM;
+ }
+ 
+ void mlx5e_free_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
+ 				    struct mlx5e_mpw_info *wi)
+ {
+ 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
+ 	int i;
+ 
+ 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
+ 		dma_unmap_page(rq->pdev, wi->umr.dma_info[i].addr, PAGE_SIZE,
+ 			       PCI_DMA_FROMDEVICE);
+ 		atomic_sub(MLX5_MPWRQ_STRIDES_PER_PAGE - wi->skbs_frags[i],
+ 			   &wi->umr.dma_info[i].page->_count);
+ 		put_page(wi->umr.dma_info[i].page);
+ 	}
+ 	dma_unmap_single(rq->pdev, wi->umr.mtt_addr, mtt_sz, PCI_DMA_TODEVICE);
+ 	kfree(wi->umr.mtt_no_align);
+ 	kfree(wi->umr.dma_info);
+ }
+ 
+ void mlx5e_post_rx_fragmented_mpwqe(struct mlx5e_rq *rq)
+ {
+ 	struct mlx5_wq_ll *wq = &rq->wq;
+ 	struct mlx5e_rx_wqe *wqe = mlx5_wq_ll_get_wqe(wq, wq->head);
+ 
+ 	clear_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state);
+ 	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
+ 	rq->stats.mpwqe_frag++;
+ 
+ 	/* ensure wqes are visible to device before updating doorbell record */
+ 	dma_wmb();
+ 
+ 	mlx5_wq_ll_update_db_record(wq);
+ }
+ 
+ static int mlx5e_alloc_rx_linear_mpwqe(struct mlx5e_rq *rq,
+ 				       struct mlx5e_rx_wqe *wqe,
+ 				       u16 ix)
+ {
+ 	struct mlx5e_mpw_info *wi = &rq->wqe_info[ix];
+ 	gfp_t gfp_mask;
+ 	int i;
+ 
+ 	gfp_mask = GFP_ATOMIC | __GFP_COLD | __GFP_MEMALLOC;
+ 	wi->dma_info.page = alloc_pages_node(NUMA_NO_NODE, gfp_mask,
+ 					     MLX5_MPWRQ_WQE_PAGE_ORDER);
+ 	if (unlikely(!wi->dma_info.page))
+ 		return -ENOMEM;
+ 
+ 	wi->dma_info.addr = dma_map_page(rq->pdev, wi->dma_info.page, 0,
+ 					 rq->wqe_sz, PCI_DMA_FROMDEVICE);
+ 	if (unlikely(dma_mapping_error(rq->pdev, wi->dma_info.addr))) {
+ 		put_page(wi->dma_info.page);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	/* We split the high-order page into order-0 ones and manage their
+ 	 * reference counter to minimize the memory held by small skb fragments
+ 	 */
+ 	split_page(wi->dma_info.page, MLX5_MPWRQ_WQE_PAGE_ORDER);
+ 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
+ 		atomic_add(MLX5_MPWRQ_STRIDES_PER_PAGE,
+ 			   &wi->dma_info.page[i]._count);
+ 		wi->skbs_frags[i] = 0;
+ 	}
+ 
+ 	wi->consumed_strides = 0;
+ 	wi->dma_pre_sync = mlx5e_dma_pre_sync_linear_mpwqe;
+ 	wi->add_skb_frag = mlx5e_add_skb_frag_linear_mpwqe;
+ 	wi->copy_skb_header = mlx5e_copy_skb_header_linear_mpwqe;
+ 	wi->free_wqe     = mlx5e_free_rx_linear_mpwqe;
+ 	wqe->data.lkey = rq->mkey_be;
+ 	wqe->data.addr = cpu_to_be64(wi->dma_info.addr);
+ 
+ 	return 0;
+ }
+ 
+ void mlx5e_free_rx_linear_mpwqe(struct mlx5e_rq *rq,
+ 				struct mlx5e_mpw_info *wi)
+ {
+ 	int i;
+ 
+ 	dma_unmap_page(rq->pdev, wi->dma_info.addr, rq->wqe_sz,
+ 		       PCI_DMA_FROMDEVICE);
+ 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
+ 		atomic_sub(MLX5_MPWRQ_STRIDES_PER_PAGE - wi->skbs_frags[i],
+ 			   &wi->dma_info.page[i]._count);
+ 		put_page(&wi->dma_info.page[i]);
+ 	}
+ }
+ 
+ int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix)
+ {
+ 	int err;
+ 
+ 	err = mlx5e_alloc_rx_linear_mpwqe(rq, wqe, ix);
+ 	if (unlikely(err)) {
+ 		err = mlx5e_alloc_rx_fragmented_mpwqe(rq, wqe, ix);
+ 		if (unlikely(err))
+ 			return err;
+ 		set_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state);
+ 		mlx5e_post_umr_wqe(rq, ix);
+ 		return -EBUSY;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ #define RQ_CANNOT_POST(rq) \
+ 		(!test_bit(MLX5E_RQ_STATE_POST_WQES_ENABLE, &rq->state) || \
+ 		 test_bit(MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS, &rq->state))
+ 
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq)
  {
  	struct mlx5_wq_ll *wq = &rq->wq;
@@@ -215,6 -587,138 +582,141 @@@ static inline void mlx5e_build_rx_skb(s
  	if (cqe_has_vlan(cqe))
  		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
  				       be16_to_cpu(cqe->vlan_info));
++<<<<<<< HEAD
++=======
+ 
+ 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
+ }
+ 
+ static inline void mlx5e_complete_rx_cqe(struct mlx5e_rq *rq,
+ 					 struct mlx5_cqe64 *cqe,
+ 					 u32 cqe_bcnt,
+ 					 struct sk_buff *skb)
+ {
+ 	rq->stats.packets++;
+ 	rq->stats.bytes += cqe_bcnt;
+ 	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ }
+ 
+ void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_rx_wqe *wqe;
+ 	struct sk_buff *skb;
+ 	__be16 wqe_counter_be;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	skb            = rq->skb[wqe_counter];
+ 	prefetch(skb->data);
+ 	rq->skb[wqe_counter] = NULL;
+ 
+ 	dma_unmap_single(rq->pdev,
+ 			 *((dma_addr_t *)skb->cb),
+ 			 rq->wqe_sz,
+ 			 DMA_FROM_DEVICE);
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		dev_kfree_skb(skb);
+ 		goto wq_ll_pop;
+ 	}
+ 
+ 	cqe_bcnt = be32_to_cpu(cqe->byte_cnt);
+ 	skb_put(skb, cqe_bcnt);
+ 
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ wq_ll_pop:
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ static inline void mlx5e_mpwqe_fill_rx_skb(struct mlx5e_rq *rq,
+ 					   struct mlx5_cqe64 *cqe,
+ 					   struct mlx5e_mpw_info *wi,
+ 					   u32 cqe_bcnt,
+ 					   struct sk_buff *skb)
+ {
+ 	u32 consumed_bytes = ALIGN(cqe_bcnt, MLX5_MPWRQ_STRIDE_SIZE);
+ 	u16 stride_ix      = mpwrq_get_cqe_stride_index(cqe);
+ 	u32 wqe_offset     = stride_ix * MLX5_MPWRQ_STRIDE_SIZE;
+ 	u32 head_offset    = wqe_offset & (PAGE_SIZE - 1);
+ 	u32 page_idx       = wqe_offset >> PAGE_SHIFT;
+ 	u32 head_page_idx  = page_idx;
+ 	u16 headlen = min_t(u16, MLX5_MPWRQ_SMALL_PACKET_THRESHOLD, cqe_bcnt);
+ 	u32 frag_offset    = head_offset + headlen;
+ 	u16 byte_cnt       = cqe_bcnt - headlen;
+ 
+ #if (MLX5_MPWRQ_SMALL_PACKET_THRESHOLD >= MLX5_MPWRQ_STRIDE_SIZE)
+ 	if (unlikely(frag_offset >= PAGE_SIZE)) {
+ 		page_idx++;
+ 		frag_offset -= PAGE_SIZE;
+ 	}
+ #endif
+ 	wi->dma_pre_sync(rq->pdev, wi, wqe_offset, consumed_bytes);
+ 
+ 	while (byte_cnt) {
+ 		u32 pg_consumed_bytes =
+ 			min_t(u32, PAGE_SIZE - frag_offset, byte_cnt);
+ 
+ 		wi->add_skb_frag(rq->pdev, skb, wi, page_idx, frag_offset,
+ 				 pg_consumed_bytes);
+ 		byte_cnt -= pg_consumed_bytes;
+ 		frag_offset = 0;
+ 		page_idx++;
+ 	}
+ 	/* copy header */
+ 	wi->copy_skb_header(rq->pdev, skb, wi, head_page_idx, head_offset,
+ 			    headlen);
+ 	/* skb linear part was allocated with headlen and aligned to long */
+ 	skb->tail += headlen;
+ 	skb->len  += headlen;
+ }
+ 
+ void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
+ 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
+ 	struct mlx5e_mpw_info *wi = &rq->wqe_info[wqe_id];
+ 	struct mlx5e_rx_wqe  *wqe = mlx5_wq_ll_get_wqe(&rq->wq, wqe_id);
+ 	struct sk_buff *skb;
+ 	u16 cqe_bcnt;
+ 
+ 	wi->consumed_strides += cstrides;
+ 
+ 	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
+ 		rq->stats.wqe_err++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	if (unlikely(mpwrq_is_filler_cqe(cqe))) {
+ 		rq->stats.mpwqe_filler++;
+ 		goto mpwrq_cqe_out;
+ 	}
+ 
+ 	skb = netdev_alloc_skb(rq->netdev,
+ 			       ALIGN(MLX5_MPWRQ_SMALL_PACKET_THRESHOLD,
+ 				     sizeof(long)));
+ 	if (unlikely(!skb))
+ 		goto mpwrq_cqe_out;
+ 
+ 	prefetch(skb->data);
+ 	cqe_bcnt = mpwrq_get_cqe_byte_cnt(cqe);
+ 
+ 	mlx5e_mpwqe_fill_rx_skb(rq, cqe, wi, cqe_bcnt, skb);
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 
+ mpwrq_cqe_out:
+ 	if (likely(wi->consumed_strides < MLX5_MPWRQ_NUM_STRIDES))
+ 		return;
+ 
+ 	wi->free_wqe(rq, wi);
+ 	mlx5_wq_ll_pop(&rq->wq, cqe->wqe_id, &wqe->next.next_wqe_index);
++>>>>>>> bc77b240b3c5 (net/mlx5e: Add fragmented memory support for RX multi packet WQE)
  }
  
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 8d9e16c03057..5d458cc80710 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -58,7 +58,7 @@ void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw)
 
 	if (notify_hw) {
 		cseg->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
-		mlx5e_tx_notify_hw(sq, wqe, 0);
+		mlx5e_tx_notify_hw(sq, &wqe->ctrl, 0);
 	}
 }
 
@@ -275,7 +275,7 @@ static netdev_tx_t mlx5e_sq_xmit(struct mlx5e_sq *sq, struct sk_buff *skb)
 			bf_sz = MLX5E_TX_SKB_CB(skb)->num_wqebbs << 3;
 
 		cseg->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
-		mlx5e_tx_notify_hw(sq, wqe, bf_sz);
+		mlx5e_tx_notify_hw(sq, &wqe->ctrl, bf_sz);
 	}
 
 	/* fill sq edge with nops to avoid wqe wrap around */
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
index 500dcd4eddc2..882e42e652bd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -84,6 +84,9 @@ static void mlx5e_poll_ico_cq(struct mlx5e_cq *cq)
 		switch (icowi->opcode) {
 		case MLX5_OPCODE_NOP:
 			break;
+		case MLX5_OPCODE_UMR:
+			mlx5e_post_rx_fragmented_mpwqe(&sq->channel->rq);
+			break;
 		default:
 			WARN_ONCE(true,
 				  "mlx5e: Bad OPCODE in ICOSQ WQE info: 0x%x\n",
