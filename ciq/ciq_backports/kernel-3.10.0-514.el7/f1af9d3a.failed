mm/memblock: Do some refactoring, enhance API

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] memblock: Do some refactoring, enhance API (Yasuaki Ishimatsu) [1270209]
Rebuild_FUZZ: 96.55%
commit-author Philipp Hachtmann <phacht@linux.vnet.ibm.com>
commit f1af9d3af308145478749194346f11efad1134b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f1af9d3a.failed

Refactor the memblock code and extend the memblock API to make it
more flexible. With the extended API it is simple to define and
work with additional memory lists.

The static functions memblock_add_region and __memblock_remove are
renamed to memblock_add_range and meblock_remove_range and added to
the memblock API.

The __next_free_mem_range and __next_free_mem_range_rev functions
are replaced with calls to the more generic list walkers
__next_mem_range and __next_mem_range_rev.

To walk an arbitrary memory list two new macros for_each_mem_range
and for_each_mem_range_rev are added. These new macros are used
to define for_each_free_mem_range and for_each_free_mem_range_reverse.

	Signed-off-by: Philipp Hachtmann <phacht@linux.vnet.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit f1af9d3af308145478749194346f11efad1134b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memblock.c
diff --cc mm/memblock.c
index 9ed67f4de3ab,9edd0928daab..000000000000
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@@ -761,36 -783,57 +764,73 @@@ int __init_memblock memblock_clear_hotp
   * As both region arrays are sorted, the function advances the two indices
   * in lockstep and returns each intersection.
   */
- void __init_memblock __next_free_mem_range(u64 *idx, int nid,
- 					   phys_addr_t *out_start,
- 					   phys_addr_t *out_end, int *out_nid)
+ void __init_memblock __next_mem_range(u64 *idx, int nid,
+ 				      struct memblock_type *type_a,
+ 				      struct memblock_type *type_b,
+ 				      phys_addr_t *out_start,
+ 				      phys_addr_t *out_end, int *out_nid)
  {
++<<<<<<< HEAD
 +	struct memblock_type *mem = &memblock.memory;
 +	struct memblock_type *rsv = &memblock.reserved;
 +	int mi = *idx & 0xffffffff;
 +	int ri = *idx >> 32;
 +	bool check_node = (nid != NUMA_NO_NODE) && (nid != MAX_NUMNODES);
 +
 +	if (nid == MAX_NUMNODES)
 +		pr_warn_once("%s: Usage of MAX_NUMNODES is depricated. Use NUMA_NO_NODE instead\n",
 +			     __func__);
++=======
+ 	int idx_a = *idx & 0xffffffff;
+ 	int idx_b = *idx >> 32;
+ 
+ 	if (WARN_ONCE(nid == MAX_NUMNODES,
+ 	"Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
+ 		nid = NUMA_NO_NODE;
++>>>>>>> f1af9d3af308 (mm/memblock: Do some refactoring, enhance API)
+ 
+ 	for (; idx_a < type_a->cnt; idx_a++) {
+ 		struct memblock_region *m = &type_a->regions[idx_a];
  
- 	for ( ; mi < mem->cnt; mi++) {
- 		struct memblock_region *m = &mem->regions[mi];
  		phys_addr_t m_start = m->base;
  		phys_addr_t m_end = m->base + m->size;
+ 		int	    m_nid = memblock_get_region_node(m);
  
  		/* only memory regions are associated with nodes, check it */
++<<<<<<< HEAD
 +		if (check_node && nid != memblock_get_region_node(m))
++=======
+ 		if (nid != NUMA_NO_NODE && nid != m_nid)
++>>>>>>> f1af9d3af308 (mm/memblock: Do some refactoring, enhance API)
  			continue;
  
- 		/* scan areas before each reservation for intersection */
- 		for ( ; ri < rsv->cnt + 1; ri++) {
- 			struct memblock_region *r = &rsv->regions[ri];
- 			phys_addr_t r_start = ri ? r[-1].base + r[-1].size : 0;
- 			phys_addr_t r_end = ri < rsv->cnt ? r->base : ULLONG_MAX;
+ 		if (!type_b) {
+ 			if (out_start)
+ 				*out_start = m_start;
+ 			if (out_end)
+ 				*out_end = m_end;
+ 			if (out_nid)
+ 				*out_nid = m_nid;
+ 			idx_a++;
+ 			*idx = (u32)idx_a | (u64)idx_b << 32;
+ 			return;
+ 		}
+ 
+ 		/* scan areas before each reservation */
+ 		for (; idx_b < type_b->cnt + 1; idx_b++) {
+ 			struct memblock_region *r;
+ 			phys_addr_t r_start;
+ 			phys_addr_t r_end;
+ 
+ 			r = &type_b->regions[idx_b];
+ 			r_start = idx_b ? r[-1].base + r[-1].size : 0;
+ 			r_end = idx_b < type_b->cnt ?
+ 				r->base : ULLONG_MAX;
  
- 			/* if ri advanced past mi, break out to advance mi */
+ 			/*
+ 			 * if idx_b advanced past idx_a,
+ 			 * break out to advance idx_a
+ 			 */
  			if (r_start >= m_end)
  				break;
  			/* if the two regions intersect, we're done */
@@@ -827,39 -877,34 +874,47 @@@
   * @out_end: ptr to phys_addr_t for end address of the range, can be %NULL
   * @out_nid: ptr to int for nid of the range, can be %NULL
   *
-  * Reverse of __next_free_mem_range().
-  *
-  * Linux kernel cannot migrate pages used by itself. Memory hotplug users won't
-  * be able to hot-remove hotpluggable memory used by the kernel. So this
-  * function skip hotpluggable regions if needed when allocating memory for the
-  * kernel.
+  * Reverse of __next_mem_range().
   */
- void __init_memblock __next_free_mem_range_rev(u64 *idx, int nid,
- 					   phys_addr_t *out_start,
- 					   phys_addr_t *out_end, int *out_nid)
+ void __init_memblock __next_mem_range_rev(u64 *idx, int nid,
+ 					  struct memblock_type *type_a,
+ 					  struct memblock_type *type_b,
+ 					  phys_addr_t *out_start,
+ 					  phys_addr_t *out_end, int *out_nid)
  {
++<<<<<<< HEAD
 +	struct memblock_type *mem = &memblock.memory;
 +	struct memblock_type *rsv = &memblock.reserved;
 +	int mi = *idx & 0xffffffff;
 +	int ri = *idx >> 32;
 +	bool check_node = (nid != NUMA_NO_NODE) && (nid != MAX_NUMNODES);
++=======
+ 	int idx_a = *idx & 0xffffffff;
+ 	int idx_b = *idx >> 32;
++>>>>>>> f1af9d3af308 (mm/memblock: Do some refactoring, enhance API)
  
 -	if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
 -		nid = NUMA_NO_NODE;
 +	if (nid == MAX_NUMNODES)
 +		pr_warn_once("%s: Usage of MAX_NUMNODES is depricated. Use NUMA_NO_NODE instead\n",
 +			     __func__);
  
  	if (*idx == (u64)ULLONG_MAX) {
- 		mi = mem->cnt - 1;
- 		ri = rsv->cnt;
+ 		idx_a = type_a->cnt - 1;
+ 		idx_b = type_b->cnt;
  	}
  
- 	for ( ; mi >= 0; mi--) {
- 		struct memblock_region *m = &mem->regions[mi];
+ 	for (; idx_a >= 0; idx_a--) {
+ 		struct memblock_region *m = &type_a->regions[idx_a];
+ 
  		phys_addr_t m_start = m->base;
  		phys_addr_t m_end = m->base + m->size;
+ 		int m_nid = memblock_get_region_node(m);
  
  		/* only memory regions are associated with nodes, check it */
++<<<<<<< HEAD
 +		if (check_node && nid != memblock_get_region_node(m))
++=======
+ 		if (nid != NUMA_NO_NODE && nid != m_nid)
++>>>>>>> f1af9d3af308 (mm/memblock: Do some refactoring, enhance API)
  			continue;
  
  		/* skip hotpluggable memory regions if needed */
@@@ -1016,6 -1077,207 +1090,210 @@@ phys_addr_t __init memblock_alloc_try_n
  	return memblock_alloc_base(size, align, MEMBLOCK_ALLOC_ACCESSIBLE);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * memblock_virt_alloc_internal - allocate boot memory block
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region to allocate (phys address)
+  * @max_addr: the upper bound of the memory region to allocate (phys address)
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * The @min_addr limit is dropped if it can not be satisfied and the allocation
+  * will fall back to memory below @min_addr. Also, allocation may fall back
+  * to any node in the system if the specified node can not
+  * hold the requested memory.
+  *
+  * The allocation is performed from memory region limited by
+  * memblock.current_limit if @max_addr == %BOOTMEM_ALLOC_ACCESSIBLE.
+  *
+  * The memory block is aligned on SMP_CACHE_BYTES if @align == 0.
+  *
+  * The phys address of allocated boot memory block is converted to virtual and
+  * allocated memory is reset to 0.
+  *
+  * In addition, function sets the min_count to 0 using kmemleak_alloc for
+  * allocated boot memory block, so that it is never reported as leaks.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ static void * __init memblock_virt_alloc_internal(
+ 				phys_addr_t size, phys_addr_t align,
+ 				phys_addr_t min_addr, phys_addr_t max_addr,
+ 				int nid)
+ {
+ 	phys_addr_t alloc;
+ 	void *ptr;
+ 
+ 	if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
+ 		nid = NUMA_NO_NODE;
+ 
+ 	/*
+ 	 * Detect any accidental use of these APIs after slab is ready, as at
+ 	 * this moment memblock may be deinitialized already and its
+ 	 * internal data may be destroyed (after execution of free_all_bootmem)
+ 	 */
+ 	if (WARN_ON_ONCE(slab_is_available()))
+ 		return kzalloc_node(size, GFP_NOWAIT, nid);
+ 
+ 	if (!align)
+ 		align = SMP_CACHE_BYTES;
+ 
+ 	if (max_addr > memblock.current_limit)
+ 		max_addr = memblock.current_limit;
+ 
+ again:
+ 	alloc = memblock_find_in_range_node(size, align, min_addr, max_addr,
+ 					    nid);
+ 	if (alloc)
+ 		goto done;
+ 
+ 	if (nid != NUMA_NO_NODE) {
+ 		alloc = memblock_find_in_range_node(size, align, min_addr,
+ 						    max_addr,  NUMA_NO_NODE);
+ 		if (alloc)
+ 			goto done;
+ 	}
+ 
+ 	if (min_addr) {
+ 		min_addr = 0;
+ 		goto again;
+ 	} else {
+ 		goto error;
+ 	}
+ 
+ done:
+ 	memblock_reserve(alloc, size);
+ 	ptr = phys_to_virt(alloc);
+ 	memset(ptr, 0, size);
+ 
+ 	/*
+ 	 * The min_count is set to 0 so that bootmem allocated blocks
+ 	 * are never reported as leaks. This is because many of these blocks
+ 	 * are only referred via the physical address which is not
+ 	 * looked up by kmemleak.
+ 	 */
+ 	kmemleak_alloc(ptr, size, 0, 0);
+ 
+ 	return ptr;
+ 
+ error:
+ 	return NULL;
+ }
+ 
+ /**
+  * memblock_virt_alloc_try_nid_nopanic - allocate boot memory block
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region from where the allocation
+  *	  is preferred (phys address)
+  * @max_addr: the upper bound of the memory region from where the allocation
+  *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+  *	      allocate only from memory limited by memblock.current_limit value
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * Public version of _memblock_virt_alloc_try_nid_nopanic() which provides
+  * additional debug information (including caller info), if enabled.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ void * __init memblock_virt_alloc_try_nid_nopanic(
+ 				phys_addr_t size, phys_addr_t align,
+ 				phys_addr_t min_addr, phys_addr_t max_addr,
+ 				int nid)
+ {
+ 	memblock_dbg("%s: %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx %pF\n",
+ 		     __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 		     (u64)max_addr, (void *)_RET_IP_);
+ 	return memblock_virt_alloc_internal(size, align, min_addr,
+ 					     max_addr, nid);
+ }
+ 
+ /**
+  * memblock_virt_alloc_try_nid - allocate boot memory block with panicking
+  * @size: size of memory block to be allocated in bytes
+  * @align: alignment of the region and block's size
+  * @min_addr: the lower bound of the memory region from where the allocation
+  *	  is preferred (phys address)
+  * @max_addr: the upper bound of the memory region from where the allocation
+  *	      is preferred (phys address), or %BOOTMEM_ALLOC_ACCESSIBLE to
+  *	      allocate only from memory limited by memblock.current_limit value
+  * @nid: nid of the free area to find, %NUMA_NO_NODE for any node
+  *
+  * Public panicking version of _memblock_virt_alloc_try_nid_nopanic()
+  * which provides debug information (including caller info), if enabled,
+  * and panics if the request can not be satisfied.
+  *
+  * RETURNS:
+  * Virtual address of allocated memory block on success, NULL on failure.
+  */
+ void * __init memblock_virt_alloc_try_nid(
+ 			phys_addr_t size, phys_addr_t align,
+ 			phys_addr_t min_addr, phys_addr_t max_addr,
+ 			int nid)
+ {
+ 	void *ptr;
+ 
+ 	memblock_dbg("%s: %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx %pF\n",
+ 		     __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 		     (u64)max_addr, (void *)_RET_IP_);
+ 	ptr = memblock_virt_alloc_internal(size, align,
+ 					   min_addr, max_addr, nid);
+ 	if (ptr)
+ 		return ptr;
+ 
+ 	panic("%s: Failed to allocate %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx\n",
+ 	      __func__, (u64)size, (u64)align, nid, (u64)min_addr,
+ 	      (u64)max_addr);
+ 	return NULL;
+ }
+ 
+ /**
+  * __memblock_free_early - free boot memory block
+  * @base: phys starting address of the  boot memory block
+  * @size: size of the boot memory block in bytes
+  *
+  * Free boot memory block previously allocated by memblock_virt_alloc_xx() API.
+  * The freeing memory will not be released to the buddy allocator.
+  */
+ void __init __memblock_free_early(phys_addr_t base, phys_addr_t size)
+ {
+ 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
+ 		     __func__, (u64)base, (u64)base + size - 1,
+ 		     (void *)_RET_IP_);
+ 	kmemleak_free_part(__va(base), size);
+ 	memblock_remove_range(&memblock.reserved, base, size);
+ }
+ 
+ /*
+  * __memblock_free_late - free bootmem block pages directly to buddy allocator
+  * @addr: phys starting address of the  boot memory block
+  * @size: size of the boot memory block in bytes
+  *
+  * This is only useful when the bootmem allocator has already been torn
+  * down, but we are still initializing the system.  Pages are released directly
+  * to the buddy allocator, no bootmem metadata is updated because it is gone.
+  */
+ void __init __memblock_free_late(phys_addr_t base, phys_addr_t size)
+ {
+ 	u64 cursor, end;
+ 
+ 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
+ 		     __func__, (u64)base, (u64)base + size - 1,
+ 		     (void *)_RET_IP_);
+ 	kmemleak_free_part(__va(base), size);
+ 	cursor = PFN_UP(base);
+ 	end = PFN_DOWN(base + size);
+ 
+ 	for (; cursor < end; cursor++) {
+ 		__free_pages_bootmem(pfn_to_page(cursor), 0);
+ 		totalram_pages++;
+ 	}
+ }
++>>>>>>> f1af9d3af308 (mm/memblock: Do some refactoring, enhance API)
  
  /*
   * Remaining API functions
diff --git a/include/linux/memblock.h b/include/linux/memblock.h
index 39082db5d5cd..620a4a590117 100644
--- a/include/linux/memblock.h
+++ b/include/linux/memblock.h
@@ -70,6 +70,63 @@ int memblock_reserve(phys_addr_t base, phys_addr_t size);
 void memblock_trim_memory(phys_addr_t align);
 int memblock_mark_hotplug(phys_addr_t base, phys_addr_t size);
 int memblock_clear_hotplug(phys_addr_t base, phys_addr_t size);
+
+/* Low level functions */
+int memblock_add_range(struct memblock_type *type,
+		       phys_addr_t base, phys_addr_t size,
+		       int nid, unsigned long flags);
+
+int memblock_remove_range(struct memblock_type *type,
+			  phys_addr_t base,
+			  phys_addr_t size);
+
+void __next_mem_range(u64 *idx, int nid, struct memblock_type *type_a,
+		      struct memblock_type *type_b, phys_addr_t *out_start,
+		      phys_addr_t *out_end, int *out_nid);
+
+void __next_mem_range_rev(u64 *idx, int nid, struct memblock_type *type_a,
+			  struct memblock_type *type_b, phys_addr_t *out_start,
+			  phys_addr_t *out_end, int *out_nid);
+
+/**
+ * for_each_mem_range - iterate through memblock areas from type_a and not
+ * included in type_b. Or just type_a if type_b is NULL.
+ * @i: u64 used as loop variable
+ * @type_a: ptr to memblock_type to iterate
+ * @type_b: ptr to memblock_type which excludes from the iteration
+ * @nid: node selector, %NUMA_NO_NODE for all nodes
+ * @p_start: ptr to phys_addr_t for start address of the range, can be %NULL
+ * @p_end: ptr to phys_addr_t for end address of the range, can be %NULL
+ * @p_nid: ptr to int for nid of the range, can be %NULL
+ */
+#define for_each_mem_range(i, type_a, type_b, nid,			\
+			   p_start, p_end, p_nid)			\
+	for (i = 0, __next_mem_range(&i, nid, type_a, type_b,		\
+				     p_start, p_end, p_nid);		\
+	     i != (u64)ULLONG_MAX;					\
+	     __next_mem_range(&i, nid, type_a, type_b,			\
+			      p_start, p_end, p_nid))
+
+/**
+ * for_each_mem_range_rev - reverse iterate through memblock areas from
+ * type_a and not included in type_b. Or just type_a if type_b is NULL.
+ * @i: u64 used as loop variable
+ * @type_a: ptr to memblock_type to iterate
+ * @type_b: ptr to memblock_type which excludes from the iteration
+ * @nid: node selector, %NUMA_NO_NODE for all nodes
+ * @p_start: ptr to phys_addr_t for start address of the range, can be %NULL
+ * @p_end: ptr to phys_addr_t for end address of the range, can be %NULL
+ * @p_nid: ptr to int for nid of the range, can be %NULL
+ */
+#define for_each_mem_range_rev(i, type_a, type_b, nid,			\
+			       p_start, p_end, p_nid)			\
+	for (i = (u64)ULLONG_MAX,					\
+		     __next_mem_range_rev(&i, nid, type_a, type_b,	\
+					 p_start, p_end, p_nid);	\
+	     i != (u64)ULLONG_MAX;					\
+	     __next_mem_range_rev(&i, nid, type_a, type_b,		\
+				  p_start, p_end, p_nid))
+
 #ifdef CONFIG_MOVABLE_NODE
 static inline bool memblock_is_hotpluggable(struct memblock_region *m)
 {
@@ -110,9 +167,6 @@ void __next_mem_pfn_range(int *idx, int nid, unsigned long *out_start_pfn,
 	     i >= 0; __next_mem_pfn_range(&i, nid, p_start, p_end, p_nid))
 #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
 
-void __next_free_mem_range(u64 *idx, int nid, phys_addr_t *out_start,
-			   phys_addr_t *out_end, int *out_nid);
-
 /**
  * for_each_free_mem_range - iterate through free memblock areas
  * @i: u64 used as loop variable
@@ -125,13 +179,8 @@ void __next_free_mem_range(u64 *idx, int nid, phys_addr_t *out_start,
  * soon as memblock is initialized.
  */
 #define for_each_free_mem_range(i, nid, p_start, p_end, p_nid)		\
-	for (i = 0,							\
-	     __next_free_mem_range(&i, nid, p_start, p_end, p_nid);	\
-	     i != (u64)ULLONG_MAX;					\
-	     __next_free_mem_range(&i, nid, p_start, p_end, p_nid))
-
-void __next_free_mem_range_rev(u64 *idx, int nid, phys_addr_t *out_start,
-			       phys_addr_t *out_end, int *out_nid);
+	for_each_mem_range(i, &memblock.memory, &memblock.reserved,	\
+			   nid, p_start, p_end, p_nid)
 
 /**
  * for_each_free_mem_range_reverse - rev-iterate through free memblock areas
@@ -145,10 +194,8 @@ void __next_free_mem_range_rev(u64 *idx, int nid, phys_addr_t *out_start,
  * order.  Available as soon as memblock is initialized.
  */
 #define for_each_free_mem_range_reverse(i, nid, p_start, p_end, p_nid)	\
-	for (i = (u64)ULLONG_MAX,					\
-	     __next_free_mem_range_rev(&i, nid, p_start, p_end, p_nid);	\
-	     i != (u64)ULLONG_MAX;					\
-	     __next_free_mem_range_rev(&i, nid, p_start, p_end, p_nid))
+	for_each_mem_range_rev(i, &memblock.memory, &memblock.reserved,	\
+			       nid, p_start, p_end, p_nid)
 
 static inline void memblock_set_region_flags(struct memblock_region *r,
 					     unsigned long flags)
* Unmerged path mm/memblock.c
