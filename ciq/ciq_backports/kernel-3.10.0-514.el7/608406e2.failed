KVM: nVMX: Enable nested virtual interrupt delivery

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Wincy Van <fanwenyi0529@gmail.com>
commit 608406e290ca31d8f217cb765ee50152b41a7c9c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/608406e2.failed

With virtual interrupt delivery, the hardware lets KVM use a more
efficient mechanism for interrupt injection. This is an important feature
for nested VMX, because it reduces vmexits substantially and they are
much more expensive with nested virtualization.  This is especially
important for throughput-bound scenarios.

	Signed-off-by: Wincy Van <fanwenyi0529@gmail.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 608406e290ca31d8f217cb765ee50152b41a7c9c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 8ebb2b95d0d8,955eff21f14a..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -218,6 -219,11 +218,14 @@@ struct __packed vmcs12 
  	u64 virtual_apic_page_addr;
  	u64 apic_access_addr;
  	u64 ept_pointer;
++<<<<<<< HEAD
++=======
+ 	u64 eoi_exit_bitmap0;
+ 	u64 eoi_exit_bitmap1;
+ 	u64 eoi_exit_bitmap2;
+ 	u64 eoi_exit_bitmap3;
+ 	u64 xss_exit_bitmap;
++>>>>>>> 608406e290ca (KVM: nVMX: Enable nested virtual interrupt delivery)
  	u64 guest_physical_address;
  	u64 vmcs_link_pointer;
  	u64 guest_ia32_debugctl;
@@@ -624,6 -649,11 +634,14 @@@ static const unsigned short vmcs_field_
  	FIELD64(VIRTUAL_APIC_PAGE_ADDR, virtual_apic_page_addr),
  	FIELD64(APIC_ACCESS_ADDR, apic_access_addr),
  	FIELD64(EPT_POINTER, ept_pointer),
++<<<<<<< HEAD
++=======
+ 	FIELD64(EOI_EXIT_BITMAP0, eoi_exit_bitmap0),
+ 	FIELD64(EOI_EXIT_BITMAP1, eoi_exit_bitmap1),
+ 	FIELD64(EOI_EXIT_BITMAP2, eoi_exit_bitmap2),
+ 	FIELD64(EOI_EXIT_BITMAP3, eoi_exit_bitmap3),
+ 	FIELD64(XSS_EXIT_BITMAP, xss_exit_bitmap),
++>>>>>>> 608406e290ca (KVM: nVMX: Enable nested virtual interrupt delivery)
  	FIELD64(GUEST_PHYSICAL_ADDRESS, guest_physical_address),
  	FIELD64(VMCS_LINK_POINTER, vmcs_link_pointer),
  	FIELD64(GUEST_IA32_DEBUGCTL, guest_ia32_debugctl),
@@@ -1106,6 -1136,27 +1124,30 @@@ static inline int nested_cpu_has_ept(st
  	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_EPT);
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool nested_cpu_has_xsaves(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES) &&
+ 		vmx_xsaves_supported();
+ }
+ 
+ static inline bool nested_cpu_has_virt_x2apic_mode(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
+ }
+ 
+ static inline bool nested_cpu_has_apic_reg_virt(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_APIC_REGISTER_VIRT);
+ }
+ 
+ static inline bool nested_cpu_has_vid(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+ }
+ 
++>>>>>>> 608406e290ca (KVM: nVMX: Enable nested virtual interrupt delivery)
  static inline bool is_exception(u32 intr_info)
  {
  	return (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))
@@@ -2389,11 -2444,16 +2431,19 @@@ static __init void nested_vmx_setup_ctl
  
  	/* secondary cpu-based controls */
  	rdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,
 -		vmx->nested.nested_vmx_secondary_ctls_low,
 -		vmx->nested.nested_vmx_secondary_ctls_high);
 -	vmx->nested.nested_vmx_secondary_ctls_low = 0;
 -	vmx->nested.nested_vmx_secondary_ctls_high &=
 +		nested_vmx_secondary_ctls_low, nested_vmx_secondary_ctls_high);
 +	nested_vmx_secondary_ctls_low = 0;
 +	nested_vmx_secondary_ctls_high &=
  		SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |
++<<<<<<< HEAD
 +		SECONDARY_EXEC_WBINVD_EXITING;
++=======
+ 		SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
+ 		SECONDARY_EXEC_APIC_REGISTER_VIRT |
+ 		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+ 		SECONDARY_EXEC_WBINVD_EXITING |
+ 		SECONDARY_EXEC_XSAVES;
++>>>>>>> 608406e290ca (KVM: nVMX: Enable nested virtual interrupt delivery)
  
  	if (enable_ept) {
  		/* nested EPT: emulate EPT also to L1 */
@@@ -7294,6 -7456,10 +7344,13 @@@ static bool nested_vmx_exit_handled(str
  	case EXIT_REASON_APIC_ACCESS:
  		return nested_cpu_has2(vmcs12,
  			SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);
++<<<<<<< HEAD
++=======
+ 	case EXIT_REASON_APIC_WRITE:
+ 	case EXIT_REASON_EOI_INDUCED:
+ 		/* apic_write and eoi_induced should exit unconditionally. */
+ 		return 1;
++>>>>>>> 608406e290ca (KVM: nVMX: Enable nested virtual interrupt delivery)
  	case EXIT_REASON_EPT_VIOLATION:
  		/*
  		 * L0 always deals with the EPT violation. If nested EPT is
@@@ -8593,7 -8619,111 +8650,115 @@@ static int nested_vmx_check_msr_bitmap_
  static inline bool nested_vmx_merge_msr_bitmap(struct kvm_vcpu *vcpu,
  					       struct vmcs12 *vmcs12)
  {
++<<<<<<< HEAD
 +	return false;
++=======
+ 	int msr;
+ 	struct page *page;
+ 	unsigned long *msr_bitmap;
+ 
+ 	if (!nested_cpu_has_virt_x2apic_mode(vmcs12))
+ 		return false;
+ 
+ 	page = nested_get_page(vcpu, vmcs12->msr_bitmap);
+ 	if (!page) {
+ 		WARN_ON(1);
+ 		return false;
+ 	}
+ 	msr_bitmap = (unsigned long *)kmap(page);
+ 	if (!msr_bitmap) {
+ 		nested_release_page_clean(page);
+ 		WARN_ON(1);
+ 		return false;
+ 	}
+ 
+ 	if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+ 		if (nested_cpu_has_apic_reg_virt(vmcs12))
+ 			for (msr = 0x800; msr <= 0x8ff; msr++)
+ 				nested_vmx_disable_intercept_for_msr(
+ 					msr_bitmap,
+ 					vmx_msr_bitmap_nested,
+ 					msr, MSR_TYPE_R);
+ 		/* TPR is allowed */
+ 		nested_vmx_disable_intercept_for_msr(msr_bitmap,
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_TASKPRI >> 4),
+ 				MSR_TYPE_R | MSR_TYPE_W);
+ 		if (nested_cpu_has_vid(vmcs12)) {
+ 			/* EOI and self-IPI are allowed */
+ 			nested_vmx_disable_intercept_for_msr(
+ 				msr_bitmap,
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_EOI >> 4),
+ 				MSR_TYPE_W);
+ 			nested_vmx_disable_intercept_for_msr(
+ 				msr_bitmap,
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_SELF_IPI >> 4),
+ 				MSR_TYPE_W);
+ 		}
+ 	} else {
+ 		/*
+ 		 * Enable reading intercept of all the x2apic
+ 		 * MSRs. We should not rely on vmcs12 to do any
+ 		 * optimizations here, it may have been modified
+ 		 * by L1.
+ 		 */
+ 		for (msr = 0x800; msr <= 0x8ff; msr++)
+ 			__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				msr,
+ 				MSR_TYPE_R);
+ 
+ 		__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_TASKPRI >> 4),
+ 				MSR_TYPE_W);
+ 		__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_EOI >> 4),
+ 				MSR_TYPE_W);
+ 		__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_SELF_IPI >> 4),
+ 				MSR_TYPE_W);
+ 	}
+ 	kunmap(page);
+ 	nested_release_page_clean(page);
+ 
+ 	return true;
+ }
+ 
+ static int nested_vmx_check_apicv_controls(struct kvm_vcpu *vcpu,
+ 					   struct vmcs12 *vmcs12)
+ {
+ 	if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ 	    !nested_cpu_has_apic_reg_virt(vmcs12) &&
+ 	    !nested_cpu_has_vid(vmcs12))
+ 		return 0;
+ 
+ 	/*
+ 	 * If virtualize x2apic mode is enabled,
+ 	 * virtualize apic access must be disabled.
+ 	 */
+ 	if (nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ 	    nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * If virtual interrupt delivery is enabled,
+ 	 * we must exit on external interrupts.
+ 	 */
+ 	if (nested_cpu_has_vid(vmcs12) &&
+ 	   !nested_exit_on_intr(vcpu))
+ 		return -EINVAL;
+ 
+ 	/* tpr shadow is needed by all apicv features. */
+ 	if (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))
+ 		return -EINVAL;
+ 
+ 	return 0;
++>>>>>>> 608406e290ca (KVM: nVMX: Enable nested virtual interrupt delivery)
  }
  
  static int nested_vmx_check_msr_switch(struct kvm_vcpu *vcpu,
* Unmerged path arch/x86/kvm/vmx.c
