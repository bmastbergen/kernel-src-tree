Drivers: hv: vmbus: Introduce functions for estimating room in the ring buffer

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [hv] vmbus: Introduce functions for estimating room in the ring buffer (Vitaly Kuznetsov) [1339684]
Rebuild_FUZZ: 90.91%
commit-author K. Y. Srinivasan <kys@microsoft.com>
commit a6341f000024cdf1ec14dc26743a409a17378db5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/a6341f00.failed

Introduce separate functions for estimating how much can be read from
and written to the ring buffer.

	Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
	Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
(cherry picked from commit a6341f000024cdf1ec14dc26743a409a17378db5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/hv/ring_buffer.c
diff --cc drivers/hv/ring_buffer.c
index 6361d124f67d,544362c6a9ca..000000000000
--- a/drivers/hv/ring_buffer.c
+++ b/drivers/hv/ring_buffer.c
@@@ -103,31 -99,32 +99,40 @@@ static bool hv_need_to_signal(u32 old_w
   *    there is room for the producer to send the pending packet.
   */
  
 -static bool hv_need_to_signal_on_read(struct hv_ring_buffer_info *rbi)
 +static bool hv_need_to_signal_on_read(u32 old_rd,
 +					 struct hv_ring_buffer_info *rbi)
  {
 +	u32 prev_write_sz;
  	u32 cur_write_sz;
++<<<<<<< HEAD
 +	u32 r_size;
 +	u32 write_loc = rbi->ring_buffer->write_index;
 +	u32 read_loc = rbi->ring_buffer->read_index;
 +	u32 pending_sz = rbi->ring_buffer->pending_send_sz;
++=======
+ 	u32 pending_sz;
++>>>>>>> a6341f000024 (Drivers: hv: vmbus: Introduce functions for estimating room in the ring buffer)
  
  	/*
 -	 * Issue a full memory barrier before making the signaling decision.
 -	 * Here is the reason for having this barrier:
 -	 * If the reading of the pend_sz (in this function)
 -	 * were to be reordered and read before we commit the new read
 -	 * index (in the calling function)  we could
 -	 * have a problem. If the host were to set the pending_sz after we
 -	 * have sampled pending_sz and go to sleep before we commit the
 -	 * read index, we could miss sending the interrupt. Issue a full
 -	 * memory barrier to address this.
 +	 * If the other end is not blocked on write don't bother.
  	 */
++<<<<<<< HEAD
++=======
+ 	mb();
+ 
+ 	pending_sz = rbi->ring_buffer->pending_send_sz;
+ 	/* If the other end is not blocked on write don't bother. */
++>>>>>>> a6341f000024 (Drivers: hv: vmbus: Introduce functions for estimating room in the ring buffer)
  	if (pending_sz == 0)
  		return false;
  
- 	r_size = rbi->ring_datasize;
- 	cur_write_sz = write_loc >= read_loc ? r_size - (write_loc - read_loc) :
- 			read_loc - write_loc;
+ 	cur_write_sz = hv_get_bytes_to_write(rbi);
  
 -	if (cur_write_sz >= pending_sz)
 +	prev_write_sz = write_loc >= old_rd ? r_size - (write_loc - old_rd) :
 +			old_rd - write_loc;
 +
 +
 +	if ((prev_write_sz < pending_sz) && (cur_write_sz >= pending_sz))
  		return true;
  
  	return false;
@@@ -410,18 -334,19 +414,16 @@@ int hv_ringbuffer_write(struct hv_ring_
  
  	totalbytes_towrite += sizeof(u64);
  
 -	if (lock)
 -		spin_lock_irqsave(&outring_info->ring_lock, flags);
 +	spin_lock_irqsave(&outring_info->ring_lock, flags);
  
- 	hv_get_ringbuffer_availbytes(outring_info,
- 				&bytes_avail_toread,
- 				&bytes_avail_towrite);
+ 	bytes_avail_towrite = hv_get_bytes_to_write(outring_info);
  
 -	/*
 -	 * If there is only room for the packet, assume it is full.
 -	 * Otherwise, the next time around, we think the ring buffer
 -	 * is empty since the read index == write index.
 -	 */
 +
 +	/* If there is only room for the packet, assume it is full. */
 +	/* Otherwise, the next time around, we think the ring buffer */
 +	/* is empty since the read index == write index */
  	if (bytes_avail_towrite <= totalbytes_towrite) {
 -		if (lock)
 -			spin_unlock_irqrestore(&outring_info->ring_lock, flags);
 +		spin_unlock_irqrestore(&outring_info->ring_lock, flags);
  		return -EAGAIN;
  	}
  
@@@ -458,61 -384,10 +460,60 @@@
  	return 0;
  }
  
 -int hv_ringbuffer_read(struct hv_ring_buffer_info *inring_info,
 -		       void *buffer, u32 buflen, u32 *buffer_actual_len,
 -		       u64 *requestid, bool *signal, bool raw)
 +
 +/*
 + *
 + * hv_ringbuffer_peek()
 + *
 + * Read without advancing the read index
 + *
 + */
 +int hv_ringbuffer_peek(struct hv_ring_buffer_info *Inring_info,
 +		   void *Buffer, u32 buflen)
 +{
 +	u32 bytes_avail_towrite;
 +	u32 bytes_avail_toread;
 +	u32 next_read_location = 0;
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&Inring_info->ring_lock, flags);
 +
 +	hv_get_ringbuffer_availbytes(Inring_info,
 +				&bytes_avail_toread,
 +				&bytes_avail_towrite);
 +
 +	/* Make sure there is something to read */
 +	if (bytes_avail_toread < buflen) {
 +
 +		spin_unlock_irqrestore(&Inring_info->ring_lock, flags);
 +
 +		return -EAGAIN;
 +	}
 +
 +	/* Convert to byte offset */
 +	next_read_location = hv_get_next_read_location(Inring_info);
 +
 +	next_read_location = hv_copyfrom_ringbuffer(Inring_info,
 +						Buffer,
 +						buflen,
 +						next_read_location);
 +
 +	spin_unlock_irqrestore(&Inring_info->ring_lock, flags);
 +
 +	return 0;
 +}
 +
 +
 +/*
 + *
 + * hv_ringbuffer_read()
 + *
 + * Read and advance the read index
 + *
 + */
 +int hv_ringbuffer_read(struct hv_ring_buffer_info *inring_info, void *buffer,
 +		   u32 buflen, u32 offset, bool *signal)
  {
- 	u32 bytes_avail_towrite;
  	u32 bytes_avail_toread;
  	u32 next_read_location = 0;
  	u64 prev_indices = 0;
@@@ -522,20 -399,35 +523,24 @@@
  	if (buflen <= 0)
  		return -EINVAL;
  
 +	spin_lock_irqsave(&inring_info->ring_lock, flags);
 +
++<<<<<<< HEAD
 +	hv_get_ringbuffer_availbytes(inring_info,
 +				&bytes_avail_toread,
 +				&bytes_avail_towrite);
  
 -	*buffer_actual_len = 0;
 -	*requestid = 0;
 +	old_read = bytes_avail_toread;
  
++=======
+ 	bytes_avail_toread = hv_get_bytes_to_read(inring_info);
++>>>>>>> a6341f000024 (Drivers: hv: vmbus: Introduce functions for estimating room in the ring buffer)
  	/* Make sure there is something to read */
 -	if (bytes_avail_toread < sizeof(desc)) {
 -		/*
 -		 * No error is set when there is even no header, drivers are
 -		 * supposed to analyze buffer_actual_len.
 -		 */
 -		return ret;
 -	}
 -
 -	next_read_location = hv_get_next_read_location(inring_info);
 -	next_read_location = hv_copyfrom_ringbuffer(inring_info, &desc,
 -						    sizeof(desc),
 -						    next_read_location);
 -
 -	offset = raw ? 0 : (desc.offset8 << 3);
 -	packetlen = (desc.len8 << 3) - offset;
 -	*buffer_actual_len = packetlen;
 -	*requestid = desc.trans_id;
 +	if (bytes_avail_toread < buflen) {
 +		spin_unlock_irqrestore(&inring_info->ring_lock, flags);
  
 -	if (bytes_avail_toread < packetlen + offset)
  		return -EAGAIN;
 -
 -	if (packetlen > buflen)
 -		return -ENOBUFS;
 +	}
  
  	next_read_location =
  		hv_get_next_readlocation_withoffset(inring_info, offset);
* Unmerged path drivers/hv/ring_buffer.c
diff --git a/include/linux/hyperv.h b/include/linux/hyperv.h
index 50e20e4283f6..9a42d9057d37 100644
--- a/include/linux/hyperv.h
+++ b/include/linux/hyperv.h
@@ -151,6 +151,33 @@ hv_get_ringbuffer_availbytes(struct hv_ring_buffer_info *rbi,
 	*read = dsize - *write;
 }
 
+static inline u32 hv_get_bytes_to_read(struct hv_ring_buffer_info *rbi)
+{
+	u32 read_loc, write_loc, dsize, read;
+
+	dsize = rbi->ring_datasize;
+	read_loc = rbi->ring_buffer->read_index;
+	write_loc = READ_ONCE(rbi->ring_buffer->write_index);
+
+	read = write_loc >= read_loc ? (write_loc - read_loc) :
+		(dsize - read_loc) + write_loc;
+
+	return read;
+}
+
+static inline u32 hv_get_bytes_to_write(struct hv_ring_buffer_info *rbi)
+{
+	u32 read_loc, write_loc, dsize, write;
+
+	dsize = rbi->ring_datasize;
+	read_loc = READ_ONCE(rbi->ring_buffer->read_index);
+	write_loc = rbi->ring_buffer->write_index;
+
+	write = write_loc >= read_loc ? dsize - (write_loc - read_loc) :
+		read_loc - write_loc;
+	return write;
+}
+
 /*
  * VMBUS version is 32 bit entity broken up into
  * two 16 bit quantities: major_number. minor_number.
