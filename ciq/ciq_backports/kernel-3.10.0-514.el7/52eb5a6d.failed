KVM: x86: For the symbols used locally only should be static type

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Xiubo Li <lixiubo@cmss.chinamobile.com>
commit 52eb5a6d576b5bca14797a4085abdd68ad8c0b3f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/52eb5a6d.failed

This patch fix the following sparse warnings:

for arch/x86/kvm/x86.c:
warning: symbol 'emulator_read_write' was not declared. Should it be static?
warning: symbol 'emulator_write_emulated' was not declared. Should it be static?
warning: symbol 'emulator_get_dr' was not declared. Should it be static?
warning: symbol 'emulator_set_dr' was not declared. Should it be static?

for arch/x86/kvm/pmu.c:
warning: symbol 'fixed_pmc_events' was not declared. Should it be static?

	Signed-off-by: Xiubo Li <lixiubo@cmss.chinamobile.com>
	Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
(cherry picked from commit 52eb5a6d576b5bca14797a4085abdd68ad8c0b3f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/pmu.c
diff --cc arch/x86/kvm/pmu.c
index 31aa2c85dc97,29fbf9dfdc54..000000000000
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@@ -20,39 -19,88 +20,46 @@@
  #include "x86.h"
  #include "cpuid.h"
  #include "lapic.h"
 +#include "pmu.h"
 +
 +/* NOTE:
 + * - Each perf counter is defined as "struct kvm_pmc";
 + * - There are two types of perf counters: general purpose (gp) and fixed.
 + *   gp counters are stored in gp_counters[] and fixed counters are stored
 + *   in fixed_counters[] respectively. Both of them are part of "struct
 + *   kvm_pmu";
 + * - pmu.c understands the difference between gp counters and fixed counters.
 + *   However AMD doesn't support fixed-counters;
 + * - There are three types of index to access perf counters (PMC):
 + *     1. MSR (named msr): For example Intel has MSR_IA32_PERFCTRn and AMD
 + *        has MSR_K7_PERFCTRn.
 + *     2. MSR Index (named idx): This normally is used by RDPMC instruction.
 + *        For instance AMD RDPMC instruction uses 0000_0003h in ECX to access
 + *        C001_0007h (MSR_K7_PERCTR3). Intel has a similar mechanism, except
 + *        that it also supports fixed counters. idx can be used to as index to
 + *        gp and fixed counters.
 + *     3. Global PMC Index (named pmc): pmc is an index specific to PMU
 + *        code. Each pmc, stored in kvm_pmc.idx field, is unique across
 + *        all perf counters (both gp and fixed). The mapping relationship
 + *        between pmc and perf counters is as the following:
 + *        * Intel: [0 .. INTEL_PMC_MAX_GENERIC-1] <=> gp counters
 + *                 [INTEL_PMC_IDX_FIXED .. INTEL_PMC_IDX_FIXED + 2] <=> fixed
 + *        * AMD:   [0 .. AMD64_NUM_COUNTERS-1] <=> gp counters
 + */
  
 -static struct kvm_arch_event_perf_mapping {
 -	u8 eventsel;
 -	u8 unit_mask;
 -	unsigned event_type;
 -	bool inexact;
 -} arch_events[] = {
 -	/* Index must match CPUID 0x0A.EBX bit vector */
 -	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
 -	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
 -	[2] = { 0x3c, 0x01, PERF_COUNT_HW_BUS_CYCLES  },
 -	[3] = { 0x2e, 0x4f, PERF_COUNT_HW_CACHE_REFERENCES },
 -	[4] = { 0x2e, 0x41, PERF_COUNT_HW_CACHE_MISSES },
 -	[5] = { 0xc4, 0x00, PERF_COUNT_HW_BRANCH_INSTRUCTIONS },
 -	[6] = { 0xc5, 0x00, PERF_COUNT_HW_BRANCH_MISSES },
 -	[7] = { 0x00, 0x30, PERF_COUNT_HW_REF_CPU_CYCLES },
 -};
 -
++<<<<<<< HEAD
 +static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
++=======
+ /* mapping between fixed pmc index and arch_events array */
+ static int fixed_pmc_events[] = {1, 0, 7};
+ 
+ static bool pmc_is_gp(struct kvm_pmc *pmc)
++>>>>>>> 52eb5a6d576b (KVM: x86: For the symbols used locally only should be static type)
  {
 -	return pmc->type == KVM_PMC_GP;
 -}
 -
 -static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 -{
 -	struct kvm_pmu *pmu = &pmc->vcpu->arch.pmu;
 +	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
 +	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);
  
 -	return pmu->counter_bitmask[pmc->type];
 -}
 -
 -static inline bool pmc_enabled(struct kvm_pmc *pmc)
 -{
 -	struct kvm_pmu *pmu = &pmc->vcpu->arch.pmu;
 -	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
 -}
 -
 -static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 -					 u32 base)
 -{
 -	if (msr >= base && msr < base + pmu->nr_arch_gp_counters)
 -		return &pmu->gp_counters[msr - base];
 -	return NULL;
 -}
 -
 -static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 -{
 -	int base = MSR_CORE_PERF_FIXED_CTR0;
 -	if (msr >= base && msr < base + pmu->nr_arch_fixed_counters)
 -		return &pmu->fixed_counters[msr - base];
 -	return NULL;
 -}
 -
 -static inline struct kvm_pmc *get_fixed_pmc_idx(struct kvm_pmu *pmu, int idx)
 -{
 -	return get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + idx);
 -}
 -
 -static struct kvm_pmc *global_idx_to_pmc(struct kvm_pmu *pmu, int idx)
 -{
 -	if (idx < INTEL_PMC_IDX_FIXED)
 -		return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + idx, MSR_P6_EVNTSEL0);
 -	else
 -		return get_fixed_pmc_idx(pmu, idx - INTEL_PMC_IDX_FIXED);
 -}
 -
 -void kvm_deliver_pmi(struct kvm_vcpu *vcpu)
 -{
 -	if (vcpu->arch.apic)
 -		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 -}
 -
 -static void trigger_pmi(struct irq_work *irq_work)
 -{
 -	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu,
 -			irq_work);
 -	struct kvm_vcpu *vcpu = container_of(pmu, struct kvm_vcpu,
 -			arch.pmu);
 -
 -	kvm_deliver_pmi(vcpu);
 +	kvm_pmu_deliver_pmi(vcpu);
  }
  
  static void kvm_perf_overflow(struct perf_event *perf_event,
* Unmerged path arch/x86/kvm/pmu.c
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index de26fcfbe9d5..d7f3c9de0487 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -2905,7 +2905,8 @@ static int rdpmc_interception(struct vcpu_svm *svm)
 	return 1;
 }
 
-bool check_selective_cr0_intercepted(struct vcpu_svm *svm, unsigned long val)
+static bool check_selective_cr0_intercepted(struct vcpu_svm *svm,
+					    unsigned long val)
 {
 	unsigned long cr0 = svm->vcpu.arch.cr0;
 	bool ret = false;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c93c135f9a5a..29c8e13c4cd5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -4645,7 +4645,8 @@ mmio:
 	return X86EMUL_CONTINUE;
 }
 
-int emulator_read_write(struct x86_emulate_ctxt *ctxt, unsigned long addr,
+static int emulator_read_write(struct x86_emulate_ctxt *ctxt,
+			unsigned long addr,
 			void *val, unsigned int bytes,
 			struct x86_exception *exception,
 			const struct read_write_emulator_ops *ops)
@@ -4706,7 +4707,7 @@ static int emulator_read_emulated(struct x86_emulate_ctxt *ctxt,
 				   exception, &read_emultor);
 }
 
-int emulator_write_emulated(struct x86_emulate_ctxt *ctxt,
+static int emulator_write_emulated(struct x86_emulate_ctxt *ctxt,
 			    unsigned long addr,
 			    const void *val,
 			    unsigned int bytes,
@@ -4905,12 +4906,14 @@ static void emulator_wbinvd(struct x86_emulate_ctxt *ctxt)
 	kvm_emulate_wbinvd_noskip(emul_to_vcpu(ctxt));
 }
 
-int emulator_get_dr(struct x86_emulate_ctxt *ctxt, int dr, unsigned long *dest)
+static int emulator_get_dr(struct x86_emulate_ctxt *ctxt, int dr,
+			   unsigned long *dest)
 {
 	return kvm_get_dr(emul_to_vcpu(ctxt), dr, dest);
 }
 
-int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr, unsigned long value)
+static int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,
+			   unsigned long value)
 {
 
 	return __kvm_set_dr(emul_to_vcpu(ctxt), dr, value);
