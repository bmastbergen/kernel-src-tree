nvme: move nvme_setup_flush and nvme_setup_rw to common code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 22944e9981db1e496d983298fd420a8c6b758c80
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/22944e99.failed

And mark them inline so that we don't slow down the I/O submission path by
having to turn it into a forced out of line call.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 22944e9981db1e496d983298fd420a8c6b758c80)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
#	drivers/nvme/host/nvme.h
diff --cc drivers/block/nvme-core.c
index 2a6eb55ad96c,c2d2b8a1a4de..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -654,70 -860,12 +654,76 @@@ static void nvme_submit_discard(struct 
  	cmnd->dsm.prp1 = cpu_to_le64(iod->first_dma);
  	cmnd->dsm.nr = 0;
  	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
 -	return BLK_MQ_RQ_QUEUE_OK;
 +
 +	if (++nvmeq->sq_tail == nvmeq->q_depth)
 +		nvmeq->sq_tail = 0;
 +	writel(nvmeq->sq_tail, nvmeq->q_db);
 +}
 +
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static void nvme_submit_flush(struct nvme_queue *nvmeq, struct nvme_ns *ns,
 +								int cmdid)
 +{
 +	struct nvme_command *cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
 +
 +	memset(cmnd, 0, sizeof(*cmnd));
 +	cmnd->common.opcode = nvme_cmd_flush;
 +	cmnd->common.command_id = cmdid;
 +	cmnd->common.nsid = cpu_to_le32(ns->ns_id);
 +
 +	if (++nvmeq->sq_tail == nvmeq->q_depth)
 +		nvmeq->sq_tail = 0;
 +	writel(nvmeq->sq_tail, nvmeq->q_db);
 +}
 +
 +static int nvme_submit_iod(struct nvme_queue *nvmeq, struct nvme_iod *iod,
 +							struct nvme_ns *ns)
 +{
 +	struct request *req = iod_get_private(iod);
 +	struct nvme_command *cmnd;
 +	u16 control = 0;
 +	u32 dsmgmt = 0;
 +
 +	if (req->cmd_flags & REQ_FUA)
 +		control |= NVME_RW_FUA;
 +	if (req->cmd_flags & (REQ_FAILFAST_DEV | REQ_RAHEAD))
 +		control |= NVME_RW_LR;
 +
 +	if (req->cmd_flags & REQ_RAHEAD)
 +		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
 +
 +	cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
 +	memset(cmnd, 0, sizeof(*cmnd));
 +
 +	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
 +	cmnd->rw.command_id = req->tag;
 +	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
 +	cmnd->rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 +	cmnd->rw.prp2 = cpu_to_le64(iod->first_dma);
 +	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
 +	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
 +
 +	if (blk_integrity_rq(req))
 +		cmnd->rw.metadata = cpu_to_le64(sg_dma_address(iod->meta_sg));
 +	else if (ns->ms)
 +		control |= NVME_RW_PRINFO_PRACT;
 +
 +	cmnd->rw.control = cpu_to_le16(control);
 +	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
 +
 +	if (++nvmeq->sq_tail == nvmeq->q_depth)
 +		nvmeq->sq_tail = 0;
 +	writel(nvmeq->sq_tail, nvmeq->q_db);
 +
 +	return 0;
 +
  }
  
++=======
+ /*
+  * NOTE: ns is NULL when called on the admin queue.
+  */
++>>>>>>> 22944e9981db (nvme: move nvme_setup_flush and nvme_setup_rw to common code):drivers/nvme/host/pci.c
  static int nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
  			 const struct blk_mq_queue_data *bd)
  {
* Unmerged path drivers/nvme/host/nvme.h
* Unmerged path drivers/block/nvme-core.c
* Unmerged path drivers/nvme/host/nvme.h
