perf: Fix scaling vs. perf_event_enable_on_exec()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 7fce250915efca0f8f51dddee3ae89bf30d86ca5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/7fce2509.failed

The recent commit 3e349507d12d ("perf: Fix perf_enable_on_exec() event
scheduling") caused this by moving task_ctx_sched_out() from before
__perf_event_mask_enable() to after it.

The overlooked consequence of that change is that task_ctx_sched_out()
would update the ctx time fields, and now __perf_event_mask_enable()
uses stale time.

In order to fix this, explicitly stop our context's time before
enabling the event(s).

	Reported-by: Oleg Nesterov <oleg@redhat.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: dvyukov@google.com
	Cc: eranian@google.com
	Cc: panand@redhat.com
	Cc: sasha.levin@oracle.com
	Cc: vince@deater.net
Fixes: 3e349507d12d ("perf: Fix perf_enable_on_exec() event scheduling")
Link: http://lkml.kernel.org/r/20160224174948.159242158@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 7fce250915efca0f8f51dddee3ae89bf30d86ca5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/events/core.c
diff --cc kernel/events/core.c
index 4091a178da37,d0030886c402..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -3055,36 -3141,21 +3055,44 @@@ static void perf_event_enable_on_exec(i
  	if (!ctx || !ctx->nr_events)
  		goto out;
  
++<<<<<<< HEAD
 +	/*
 +	 * We must ctxsw out cgroup events to avoid conflict
 +	 * when invoking perf_task_event_sched_in() later on
 +	 * in this function. Otherwise we end up trying to
 +	 * ctxswin cgroup events which are already scheduled
 +	 * in.
 +	 */
 +	perf_cgroup_sched_out(current, NULL);
 +
 +	raw_spin_lock(&ctx->lock);
 +	task_ctx_sched_out(ctx);
 +
 +	list_for_each_entry(event, &ctx->event_list, event_entry) {
 +		ret = event_enable_on_exec(event, ctx);
 +		if (ret)
 +			enabled = 1;
 +	}
++=======
+ 	cpuctx = __get_cpu_context(ctx);
+ 	perf_ctx_lock(cpuctx, ctx);
+ 	ctx_sched_out(ctx, cpuctx, EVENT_TIME);
+ 	list_for_each_entry(event, &ctx->event_list, event_entry)
+ 		enabled |= event_enable_on_exec(event, ctx);
++>>>>>>> 7fce250915ef (perf: Fix scaling vs. perf_event_enable_on_exec())
  
  	/*
 -	 * Unclone and reschedule this context if we enabled any event.
 +	 * Unclone this context if we enabled any event.
  	 */
 -	if (enabled) {
 +	if (enabled)
  		clone_ctx = unclone_ctx(ctx);
 -		ctx_resched(cpuctx, ctx);
 -	}
 -	perf_ctx_unlock(cpuctx, ctx);
  
 +	raw_spin_unlock(&ctx->lock);
 +
 +	/*
 +	 * Also calls ctxswin for cgroup events, if any:
 +	 */
 +	perf_event_context_sched_in(ctx, ctx->task);
  out:
  	local_irq_restore(flags);
  
* Unmerged path kernel/events/core.c
