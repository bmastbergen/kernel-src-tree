mm: do_fault(): extract to call vm_ops->do_fault() to separate function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] do_fault(): extract to call vm_ops->do_fault() to separate function (Eric Sandeen) [1274459]
Rebuild_FUZZ: 97.10%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit 7eae74af32d2048d27c38bad1c767a8f3ce4ddb6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/7eae74af.failed

Extract code to vm_ops->do_fault() and basic error handling to separate
function.  The code will be reused.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 7eae74af32d2048d27c38bad1c767a8f3ce4ddb6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,af76397c2c54..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3508,8 -3286,39 +3508,39 @@@ oom
  	return VM_FAULT_OOM;
  }
  
+ static int __do_fault(struct vm_area_struct *vma, unsigned long address,
+ 		pgoff_t pgoff, unsigned int flags, struct page **page)
+ {
+ 	struct vm_fault vmf;
+ 	int ret;
+ 
+ 	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
+ 	vmf.pgoff = pgoff;
+ 	vmf.flags = flags;
+ 	vmf.page = NULL;
+ 
+ 	ret = vma->vm_ops->fault(vma, &vmf);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+ 		return ret;
+ 
+ 	if (unlikely(PageHWPoison(vmf.page))) {
+ 		if (ret & VM_FAULT_LOCKED)
+ 			unlock_page(vmf.page);
+ 		page_cache_release(vmf.page);
+ 		return VM_FAULT_HWPOISON;
+ 	}
+ 
+ 	if (unlikely(!(ret & VM_FAULT_LOCKED)))
+ 		lock_page(vmf.page);
+ 	else
+ 		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
+ 
+ 	*page = vmf.page;
+ 	return ret;
+ }
+ 
  /*
 - * do_fault() tries to create a new page mapping. It aggressively
 + * __do_fault() tries to create a new page mapping. It aggressively
   * tries to share with existing pages, but makes a separate copy if
   * the FAULT_FLAG_WRITE is set in the flags parameter in order to avoid
   * the next page fault.
@@@ -3556,32 -3364,10 +3586,29 @@@ static int __do_fault(struct mm_struct 
  	} else
  		cow_page = NULL;
  
- 	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
- 	vmf.pgoff = pgoff;
- 	vmf.flags = flags;
- 	vmf.page = NULL;
- 
- 	ret = vma->vm_ops->fault(vma, &vmf);
- 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
- 			    VM_FAULT_RETRY)))
+ 	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
  		goto uncharge_out;
  
++<<<<<<< HEAD
 +	if (unlikely(PageHWPoison(vmf.page))) {
 +		if (ret & VM_FAULT_LOCKED)
 +			unlock_page(vmf.page);
 +		ret = VM_FAULT_HWPOISON;
 +		goto uncharge_out;
 +	}
 +
 +	/*
 +	 * For consistency in subsequent calls, make the faulted page always
 +	 * locked.
 +	 */
 +	if (unlikely(!(ret & VM_FAULT_LOCKED)))
 +		lock_page(vmf.page);
 +	else
 +		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
 +
++=======
++>>>>>>> 7eae74af32d2 (mm: do_fault(): extract to call vm_ops->do_fault() to separate function)
  	/*
  	 * Should we do an early C-O-W break?
  	 */
* Unmerged path mm/memory.c
