perf: Add PERF_RECORD_SWITCH to indicate context switches

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Adrian Hunter <adrian.hunter@intel.com>
commit 45ac1403f564f411c6a383a2448688ba8dd705a4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/45ac1403.failed

There are already two events for context switches, namely the tracepoint
sched:sched_switch and the software event context_switches.
Unfortunately neither are suitable for use by non-privileged users for
the purpose of synchronizing hardware trace data (e.g. Intel PT) to the
context switch.

Tracepoints are no good at all for non-privileged users because they
need either CAP_SYS_ADMIN or /proc/sys/kernel/perf_event_paranoid <= -1.

On the other hand, kernel software events need either CAP_SYS_ADMIN or
/proc/sys/kernel/perf_event_paranoid <= 1.

Now many distributions do default perf_event_paranoid to 1 making
context_switches a contender, except it has another problem (which is
also shared with sched:sched_switch) which is that it happens before
perf schedules events out instead of after perf schedules events in.
Whereas a privileged user can see all the events anyway, a
non-privileged user only sees events for their own processes, in other
words they see when their process was scheduled out not when it was
scheduled in. That presents two problems to use the event:

1. the information comes too late, so tools have to look ahead in the
   event stream to find out what the current state is

2. if they are unlucky tracing might have stopped before the
   context-switches event is recorded.

This new PERF_RECORD_SWITCH event does not have those problems
and it also has a couple of other small advantages.

It is easier to use because it is an auxiliary event (like mmap, comm
and task events) which can be enabled by setting a single bit. It is
smaller than sched:sched_switch and easier to parse.

To make the event useful for privileged users also, if the
context is cpu-wide then the event record will be
PERF_RECORD_SWITCH_CPU_WIDE which is the same as
PERF_RECORD_SWITCH except it also provides the next or
previous pid/tid.

	Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Tested-by: Jiri Olsa <jolsa@redhat.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
	Cc: Pawel Moll <pawel.moll@arm.com>
	Cc: Stephane Eranian <eranian@google.com>
Link: http://lkml.kernel.org/r/1437471846-26995-2-git-send-email-adrian.hunter@intel.com
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit 45ac1403f564f411c6a383a2448688ba8dd705a4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/perf_event.h
#	kernel/events/core.c
diff --cc include/uapi/linux/perf_event.h
index 31428ba83f05,022d0acf7df0..000000000000
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@@ -325,13 -328,10 +325,19 @@@ struct perf_event_attr 
  				exclude_callchain_kernel : 1, /* exclude kernel callchains */
  				exclude_callchain_user   : 1, /* exclude user callchains */
  				mmap2          :  1, /* include mmap with inode data     */
 +
 +#ifdef __GENKSYMS__
 +				__reserved_1   : 40;
 +#else
  				comm_exec      :  1, /* flag comm events that are due to an exec */
++<<<<<<< HEAD
 +				__reserved_1   : 39;
 +#endif
++=======
+ 				use_clockid    :  1, /* use @clockid for time fields */
+ 				context_switch :  1, /* context switch data */
+ 				__reserved_1   : 37;
++>>>>>>> 45ac1403f564 (perf: Add PERF_RECORD_SWITCH to indicate context switches)
  
  	union {
  		__u32		wakeup_events;	  /* wakeup every n events */
@@@ -772,6 -798,55 +780,58 @@@ enum perf_event_type 
  	 */
  	PERF_RECORD_AUX				= 11,
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Indicates that instruction trace has started
+ 	 *
+ 	 * struct {
+ 	 *	struct perf_event_header	header;
+ 	 *	u32				pid;
+ 	 *	u32				tid;
+ 	 * };
+ 	 */
+ 	PERF_RECORD_ITRACE_START		= 12,
+ 
+ 	/*
+ 	 * Records the dropped/lost sample number.
+ 	 *
+ 	 * struct {
+ 	 *	struct perf_event_header	header;
+ 	 *
+ 	 *	u64				lost;
+ 	 *	struct sample_id		sample_id;
+ 	 * };
+ 	 */
+ 	PERF_RECORD_LOST_SAMPLES		= 13,
+ 
+ 	/*
+ 	 * Records a context switch in or out (flagged by
+ 	 * PERF_RECORD_MISC_SWITCH_OUT). See also
+ 	 * PERF_RECORD_SWITCH_CPU_WIDE.
+ 	 *
+ 	 * struct {
+ 	 *	struct perf_event_header	header;
+ 	 *	struct sample_id		sample_id;
+ 	 * };
+ 	 */
+ 	PERF_RECORD_SWITCH			= 14,
+ 
+ 	/*
+ 	 * CPU-wide version of PERF_RECORD_SWITCH with next_prev_pid and
+ 	 * next_prev_tid that are the next (switching out) or previous
+ 	 * (switching in) pid/tid.
+ 	 *
+ 	 * struct {
+ 	 *	struct perf_event_header	header;
+ 	 *	u32				next_prev_pid;
+ 	 *	u32				next_prev_tid;
+ 	 *	struct sample_id		sample_id;
+ 	 * };
+ 	 */
+ 	PERF_RECORD_SWITCH_CPU_WIDE		= 15,
+ 
++>>>>>>> 45ac1403f564 (perf: Add PERF_RECORD_SWITCH to indicate context switches)
  	PERF_RECORD_MAX,			/* non-ABI */
  };
  
diff --cc kernel/events/core.c
index c71870e24d1d,ce21143c0d9e..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -2825,9 -2835,12 +2832,12 @@@ void __perf_event_task_sched_in(struct 
  	 * to check if we have to switch in PMU state.
  	 * cgroup event are system-wide mode only
  	 */
 -	if (atomic_read(this_cpu_ptr(&perf_cgroup_events)))
 +	if (atomic_read(&__get_cpu_var(perf_cgroup_events)))
  		perf_cgroup_sched_in(prev, task);
  
+ 	if (atomic_read(&nr_switch_events))
+ 		perf_event_switch(task, prev, true);
+ 
  	if (__this_cpu_read(perf_sched_cb_usages))
  		perf_pmu_sched_task(prev, task, true);
  }
@@@ -5751,6 -5963,124 +5765,127 @@@ void perf_event_aux_event(struct perf_e
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Lost/dropped samples logging
+  */
+ void perf_log_lost_samples(struct perf_event *event, u64 lost)
+ {
+ 	struct perf_output_handle handle;
+ 	struct perf_sample_data sample;
+ 	int ret;
+ 
+ 	struct {
+ 		struct perf_event_header	header;
+ 		u64				lost;
+ 	} lost_samples_event = {
+ 		.header = {
+ 			.type = PERF_RECORD_LOST_SAMPLES,
+ 			.misc = 0,
+ 			.size = sizeof(lost_samples_event),
+ 		},
+ 		.lost		= lost,
+ 	};
+ 
+ 	perf_event_header__init_id(&lost_samples_event.header, &sample, event);
+ 
+ 	ret = perf_output_begin(&handle, event,
+ 				lost_samples_event.header.size);
+ 	if (ret)
+ 		return;
+ 
+ 	perf_output_put(&handle, lost_samples_event);
+ 	perf_event__output_id_sample(event, &handle, &sample);
+ 	perf_output_end(&handle);
+ }
+ 
+ /*
+  * context_switch tracking
+  */
+ 
+ struct perf_switch_event {
+ 	struct task_struct	*task;
+ 	struct task_struct	*next_prev;
+ 
+ 	struct {
+ 		struct perf_event_header	header;
+ 		u32				next_prev_pid;
+ 		u32				next_prev_tid;
+ 	} event_id;
+ };
+ 
+ static int perf_event_switch_match(struct perf_event *event)
+ {
+ 	return event->attr.context_switch;
+ }
+ 
+ static void perf_event_switch_output(struct perf_event *event, void *data)
+ {
+ 	struct perf_switch_event *se = data;
+ 	struct perf_output_handle handle;
+ 	struct perf_sample_data sample;
+ 	int ret;
+ 
+ 	if (!perf_event_switch_match(event))
+ 		return;
+ 
+ 	/* Only CPU-wide events are allowed to see next/prev pid/tid */
+ 	if (event->ctx->task) {
+ 		se->event_id.header.type = PERF_RECORD_SWITCH;
+ 		se->event_id.header.size = sizeof(se->event_id.header);
+ 	} else {
+ 		se->event_id.header.type = PERF_RECORD_SWITCH_CPU_WIDE;
+ 		se->event_id.header.size = sizeof(se->event_id);
+ 		se->event_id.next_prev_pid =
+ 					perf_event_pid(event, se->next_prev);
+ 		se->event_id.next_prev_tid =
+ 					perf_event_tid(event, se->next_prev);
+ 	}
+ 
+ 	perf_event_header__init_id(&se->event_id.header, &sample, event);
+ 
+ 	ret = perf_output_begin(&handle, event, se->event_id.header.size);
+ 	if (ret)
+ 		return;
+ 
+ 	if (event->ctx->task)
+ 		perf_output_put(&handle, se->event_id.header);
+ 	else
+ 		perf_output_put(&handle, se->event_id);
+ 
+ 	perf_event__output_id_sample(event, &handle, &sample);
+ 
+ 	perf_output_end(&handle);
+ }
+ 
+ static void perf_event_switch(struct task_struct *task,
+ 			      struct task_struct *next_prev, bool sched_in)
+ {
+ 	struct perf_switch_event switch_event;
+ 
+ 	/* N.B. caller checks nr_switch_events != 0 */
+ 
+ 	switch_event = (struct perf_switch_event){
+ 		.task		= task,
+ 		.next_prev	= next_prev,
+ 		.event_id	= {
+ 			.header = {
+ 				/* .type */
+ 				.misc = sched_in ? 0 : PERF_RECORD_MISC_SWITCH_OUT,
+ 				/* .size */
+ 			},
+ 			/* .next_prev_pid */
+ 			/* .next_prev_tid */
+ 		},
+ 	};
+ 
+ 	perf_event_aux(perf_event_switch_output,
+ 		       &switch_event,
+ 		       NULL);
+ }
+ 
+ /*
++>>>>>>> 45ac1403f564 (perf: Add PERF_RECORD_SWITCH to indicate context switches)
   * IRQ throttle logging
   */
  
* Unmerged path include/uapi/linux/perf_event.h
* Unmerged path kernel/events/core.c
