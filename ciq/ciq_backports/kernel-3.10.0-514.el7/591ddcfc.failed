dm: allow immutable request-based targets to use blk-mq pdu

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mike Snitzer <snitzer@redhat.com>
commit 591ddcfc4bfad28e096787b1159942124d49cd1e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/591ddcfc.failed

This will allow DM multipath to use a portion of the blk-mq pdu space
for target data (e.g. struct dm_mpath_io).

	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 591ddcfc4bfad28e096787b1159942124d49cd1e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm.c
diff --cc drivers/md/dm.c
index 511a3a6992f6,92c2fee413b6..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -224,8 -223,9 +224,14 @@@ struct mapped_device 
  	ktime_t last_rq_start_time;
  
  	/* for blk-mq request-based DM support */
++<<<<<<< HEAD
 +	struct blk_mq_tag_set tag_set;
 +	bool use_blk_mq;
++=======
+ 	struct blk_mq_tag_set *tag_set;
+ 	bool use_blk_mq:1;
+ 	bool init_tio_pdu:1;
++>>>>>>> 591ddcfc4bfa (dm: allow immutable request-based targets to use blk-mq pdu)
  };
  
  #ifdef CONFIG_DM_MQ_DEFAULT
@@@ -2903,30 -2718,40 +2922,57 @@@ static struct blk_mq_ops dm_mq_ops = 
  	.init_request = dm_mq_init_request,
  };
  
++<<<<<<< HEAD
 +static int dm_init_request_based_blk_mq_queue(struct mapped_device *md)
++=======
+ static int dm_mq_init_request_queue(struct mapped_device *md,
+ 				    struct dm_target *immutable_tgt)
++>>>>>>> 591ddcfc4bfa (dm: allow immutable request-based targets to use blk-mq pdu)
  {
 +	unsigned md_type = dm_get_md_type(md);
  	struct request_queue *q;
  	int err;
  
 -	if (dm_get_md_type(md) == DM_TYPE_REQUEST_BASED) {
 -		DMERR("request-based dm-mq may only be stacked on blk-mq device(s)");
 -		return -EINVAL;
 -	}
 -
 +	memset(&md->tag_set, 0, sizeof(md->tag_set));
 +	md->tag_set.ops = &dm_mq_ops;
 +	md->tag_set.queue_depth = dm_get_blk_mq_queue_depth();
 +	md->tag_set.numa_node = NUMA_NO_NODE;
 +	md->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
 +	md->tag_set.nr_hw_queues = dm_get_blk_mq_nr_hw_queues();
 +	if (md_type == DM_TYPE_REQUEST_BASED) {
 +		/* make the memory for non-blk-mq clone part of the pdu */
 +		md->tag_set.cmd_size = sizeof(struct dm_rq_target_io) + sizeof(struct request);
 +	} else
 +		md->tag_set.cmd_size = sizeof(struct dm_rq_target_io);
 +	md->tag_set.driver_data = md;
 +
++<<<<<<< HEAD
 +	err = blk_mq_alloc_tag_set(&md->tag_set);
++=======
+ 	md->tag_set = kzalloc(sizeof(struct blk_mq_tag_set), GFP_KERNEL);
+ 	if (!md->tag_set)
+ 		return -ENOMEM;
+ 
+ 	md->tag_set->ops = &dm_mq_ops;
+ 	md->tag_set->queue_depth = dm_get_blk_mq_queue_depth();
+ 	md->tag_set->numa_node = NUMA_NO_NODE;
+ 	md->tag_set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+ 	md->tag_set->nr_hw_queues = dm_get_blk_mq_nr_hw_queues();
+ 	md->tag_set->driver_data = md;
+ 
+ 	md->tag_set->cmd_size = sizeof(struct dm_rq_target_io);
+ 	if (immutable_tgt && immutable_tgt->per_io_data_size) {
+ 		/* any target-specific per-io data is immediately after the tio */
+ 		md->tag_set->cmd_size += immutable_tgt->per_io_data_size;
+ 		md->init_tio_pdu = true;
+ 	}
+ 
+ 	err = blk_mq_alloc_tag_set(md->tag_set);
++>>>>>>> 591ddcfc4bfa (dm: allow immutable request-based targets to use blk-mq pdu)
  	if (err)
 -		goto out_kfree_tag_set;
 +		return err;
  
 -	q = blk_mq_init_allocated_queue(md->tag_set, md->queue);
 +	q = blk_mq_init_allocated_queue(&md->tag_set, md->queue);
  	if (IS_ERR(q)) {
  		err = PTR_ERR(q);
  		goto out_tag_set;
@@@ -2972,9 -2797,9 +3018,13 @@@ int dm_setup_md_queue(struct mapped_dev
  		}
  		break;
  	case DM_TYPE_MQ_REQUEST_BASED:
++<<<<<<< HEAD
 +		r = dm_init_request_based_blk_mq_queue(md);
++=======
+ 		r = dm_mq_init_request_queue(md, dm_table_get_immutable_target(t));
++>>>>>>> 591ddcfc4bfa (dm: allow immutable request-based targets to use blk-mq pdu)
  		if (r) {
 -			DMERR("Cannot initialize queue for request-based dm-mq mapped device");
 +			DMWARN("Cannot initialize queue for request-based blk-mq mapped device");
  			return r;
  		}
  		break;
@@@ -3703,8 -3525,7 +3753,12 @@@ struct dm_md_mempools *dm_alloc_md_memp
  		if (!pool_size)
  			pool_size = dm_get_reserved_rq_based_ios();
  		front_pad = offsetof(struct dm_rq_clone_bio_info, clone);
++<<<<<<< HEAD
 +		/* per_bio_data_size is not used. See __bind_mempools(). */
 +		WARN_ON(per_bio_data_size != 0);
++=======
+ 		/* per_io_data_size is used for blk-mq pdu at queue allocation */
++>>>>>>> 591ddcfc4bfa (dm: allow immutable request-based targets to use blk-mq pdu)
  		break;
  	default:
  		BUG();
diff --git a/drivers/md/dm-ioctl.c b/drivers/md/dm-ioctl.c
index bf685db7625f..7d4b4dc2e78d 100644
--- a/drivers/md/dm-ioctl.c
+++ b/drivers/md/dm-ioctl.c
@@ -1304,7 +1304,7 @@ static int table_load(struct dm_ioctl *param, size_t param_size)
 		dm_set_md_type(md, dm_table_get_type(t));
 
 		/* setup md->queue to reflect md's type (may block) */
-		r = dm_setup_md_queue(md);
+		r = dm_setup_md_queue(md, t);
 		if (r) {
 			DMWARN("unable to set up device queue for new table.");
 			goto err_unlock_md_type;
* Unmerged path drivers/md/dm.c
diff --git a/drivers/md/dm.h b/drivers/md/dm.h
index a04cbed7c17f..1bf243cc15a7 100644
--- a/drivers/md/dm.h
+++ b/drivers/md/dm.h
@@ -87,7 +87,7 @@ void dm_set_md_type(struct mapped_device *md, unsigned type);
 unsigned dm_get_md_type(struct mapped_device *md);
 struct target_type *dm_get_immutable_target_type(struct mapped_device *md);
 
-int dm_setup_md_queue(struct mapped_device *md);
+int dm_setup_md_queue(struct mapped_device *md, struct dm_table *t);
 
 /*
  * To check the return value from dm_table_find_target().
