KVM: nVMX: Enable nested posted interrupt processing

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Wincy Van <fanwenyi0529@gmail.com>
commit 705699a139948a671cd66b915e8095c95fdf44d9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/705699a1.failed

If vcpu has a interrupt in vmx non-root mode, injecting that interrupt
requires a vmexit.  With posted interrupt processing, the vmexit
is not needed, and interrupts are fully taken care of by hardware.
In nested vmx, this feature avoids much more vmexits than non-nested vmx.

When L1 asks L0 to deliver L1's posted interrupt vector, and the target
VCPU is in non-root mode, we use a physical ipi to deliver POSTED_INTR_NV
to the target vCPU.  Using POSTED_INTR_NV avoids unexpected interrupts
if a concurrent vmexit happens and L1's vector is different with L0's.
The IPI triggers posted interrupt processing in the target physical CPU.

In case the target vCPU was not in guest mode, complete the posted
interrupt delivery on the next entry to L2.

	Signed-off-by: Wincy Van <fanwenyi0529@gmail.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 705699a139948a671cd66b915e8095c95fdf44d9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 8ebb2b95d0d8,6e112472b0b3..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -217,7 -218,13 +217,8 @@@ struct __packed vmcs12 
  	u64 tsc_offset;
  	u64 virtual_apic_page_addr;
  	u64 apic_access_addr;
+ 	u64 posted_intr_desc_addr;
  	u64 ept_pointer;
 -	u64 eoi_exit_bitmap0;
 -	u64 eoi_exit_bitmap1;
 -	u64 eoi_exit_bitmap2;
 -	u64 eoi_exit_bitmap3;
 -	u64 xss_exit_bitmap;
  	u64 guest_physical_address;
  	u64 vmcs_link_pointer;
  	u64 guest_ia32_debugctl;
@@@ -623,7 -655,13 +630,8 @@@ static const unsigned short vmcs_field_
  	FIELD64(TSC_OFFSET, tsc_offset),
  	FIELD64(VIRTUAL_APIC_PAGE_ADDR, virtual_apic_page_addr),
  	FIELD64(APIC_ACCESS_ADDR, apic_access_addr),
+ 	FIELD64(POSTED_INTR_DESC_ADDR, posted_intr_desc_addr),
  	FIELD64(EPT_POINTER, ept_pointer),
 -	FIELD64(EOI_EXIT_BITMAP0, eoi_exit_bitmap0),
 -	FIELD64(EOI_EXIT_BITMAP1, eoi_exit_bitmap1),
 -	FIELD64(EOI_EXIT_BITMAP2, eoi_exit_bitmap2),
 -	FIELD64(EOI_EXIT_BITMAP3, eoi_exit_bitmap3),
 -	FIELD64(XSS_EXIT_BITMAP, xss_exit_bitmap),
  	FIELD64(GUEST_PHYSICAL_ADDRESS, guest_physical_address),
  	FIELD64(VMCS_LINK_POINTER, vmcs_link_pointer),
  	FIELD64(GUEST_IA32_DEBUGCTL, guest_ia32_debugctl),
@@@ -769,6 -807,8 +777,11 @@@ static u64 construct_eptp(unsigned lon
  static void kvm_cpu_vmxon(u64 addr);
  static void kvm_cpu_vmxoff(void);
  static bool vmx_mpx_supported(void);
++<<<<<<< HEAD
++=======
+ static bool vmx_xsaves_supported(void);
+ static int vmx_vm_has_apicv(struct kvm *kvm);
++>>>>>>> 705699a13994 (KVM: nVMX: Enable nested posted interrupt processing)
  static int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr);
  static void vmx_set_segment(struct kvm_vcpu *vcpu,
  			    struct kvm_segment *var, int seg);
@@@ -1106,6 -1145,32 +1119,35 @@@ static inline int nested_cpu_has_ept(st
  	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_EPT);
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool nested_cpu_has_xsaves(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES) &&
+ 		vmx_xsaves_supported();
+ }
+ 
+ static inline bool nested_cpu_has_virt_x2apic_mode(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
+ }
+ 
+ static inline bool nested_cpu_has_apic_reg_virt(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_APIC_REGISTER_VIRT);
+ }
+ 
+ static inline bool nested_cpu_has_vid(struct vmcs12 *vmcs12)
+ {
+ 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+ }
+ 
+ static inline bool nested_cpu_has_posted_intr(struct vmcs12 *vmcs12)
+ {
+ 	return vmcs12->pin_based_vm_exec_control & PIN_BASED_POSTED_INTR;
+ }
+ 
++>>>>>>> 705699a13994 (KVM: nVMX: Enable nested posted interrupt processing)
  static inline bool is_exception(u32 intr_info)
  {
  	return (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))
@@@ -2310,12 -2363,20 +2352,15 @@@ static __init void nested_vmx_setup_ctl
  
  	/* pin-based controls */
  	rdmsr(MSR_IA32_VMX_PINBASED_CTLS,
 -		vmx->nested.nested_vmx_pinbased_ctls_low,
 -		vmx->nested.nested_vmx_pinbased_ctls_high);
 -	vmx->nested.nested_vmx_pinbased_ctls_low |=
 -		PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;
 -	vmx->nested.nested_vmx_pinbased_ctls_high &=
 -		PIN_BASED_EXT_INTR_MASK |
 -		PIN_BASED_NMI_EXITING |
 -		PIN_BASED_VIRTUAL_NMIS;
 -	vmx->nested.nested_vmx_pinbased_ctls_high |=
 -		PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR |
 +	      nested_vmx_pinbased_ctls_low, nested_vmx_pinbased_ctls_high);
 +	nested_vmx_pinbased_ctls_low |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;
 +	nested_vmx_pinbased_ctls_high &= PIN_BASED_EXT_INTR_MASK |
 +		PIN_BASED_NMI_EXITING | PIN_BASED_VIRTUAL_NMIS;
 +	nested_vmx_pinbased_ctls_high |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR |
  		PIN_BASED_VMX_PREEMPTION_TIMER;
+ 	if (vmx_vm_has_apicv(vmx->vcpu.kvm))
+ 		vmx->nested.nested_vmx_pinbased_ctls_high |=
+ 			PIN_BASED_POSTED_INTR;
  
  	/* exit controls */
  	rdmsr(MSR_IA32_VMX_EXIT_CTLS,
@@@ -8295,6 -8403,10 +8409,13 @@@ static struct kvm_vcpu *vmx_create_vcpu
  			goto free_vmcs;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (nested)
+ 		nested_vmx_setup_ctls_msrs(vmx);
+ 
+ 	vmx->nested.posted_intr_nv = -1;
++>>>>>>> 705699a13994 (KVM: nVMX: Enable nested posted interrupt processing)
  	vmx->nested.current_vmptr = -1ull;
  	vmx->nested.current_vmcs12 = NULL;
  
@@@ -8593,7 -8731,123 +8739,127 @@@ static int nested_vmx_check_msr_bitmap_
  static inline bool nested_vmx_merge_msr_bitmap(struct kvm_vcpu *vcpu,
  					       struct vmcs12 *vmcs12)
  {
++<<<<<<< HEAD
 +	return false;
++=======
+ 	int msr;
+ 	struct page *page;
+ 	unsigned long *msr_bitmap;
+ 
+ 	if (!nested_cpu_has_virt_x2apic_mode(vmcs12))
+ 		return false;
+ 
+ 	page = nested_get_page(vcpu, vmcs12->msr_bitmap);
+ 	if (!page) {
+ 		WARN_ON(1);
+ 		return false;
+ 	}
+ 	msr_bitmap = (unsigned long *)kmap(page);
+ 	if (!msr_bitmap) {
+ 		nested_release_page_clean(page);
+ 		WARN_ON(1);
+ 		return false;
+ 	}
+ 
+ 	if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+ 		if (nested_cpu_has_apic_reg_virt(vmcs12))
+ 			for (msr = 0x800; msr <= 0x8ff; msr++)
+ 				nested_vmx_disable_intercept_for_msr(
+ 					msr_bitmap,
+ 					vmx_msr_bitmap_nested,
+ 					msr, MSR_TYPE_R);
+ 		/* TPR is allowed */
+ 		nested_vmx_disable_intercept_for_msr(msr_bitmap,
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_TASKPRI >> 4),
+ 				MSR_TYPE_R | MSR_TYPE_W);
+ 		if (nested_cpu_has_vid(vmcs12)) {
+ 			/* EOI and self-IPI are allowed */
+ 			nested_vmx_disable_intercept_for_msr(
+ 				msr_bitmap,
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_EOI >> 4),
+ 				MSR_TYPE_W);
+ 			nested_vmx_disable_intercept_for_msr(
+ 				msr_bitmap,
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_SELF_IPI >> 4),
+ 				MSR_TYPE_W);
+ 		}
+ 	} else {
+ 		/*
+ 		 * Enable reading intercept of all the x2apic
+ 		 * MSRs. We should not rely on vmcs12 to do any
+ 		 * optimizations here, it may have been modified
+ 		 * by L1.
+ 		 */
+ 		for (msr = 0x800; msr <= 0x8ff; msr++)
+ 			__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				msr,
+ 				MSR_TYPE_R);
+ 
+ 		__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_TASKPRI >> 4),
+ 				MSR_TYPE_W);
+ 		__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_EOI >> 4),
+ 				MSR_TYPE_W);
+ 		__vmx_enable_intercept_for_msr(
+ 				vmx_msr_bitmap_nested,
+ 				APIC_BASE_MSR + (APIC_SELF_IPI >> 4),
+ 				MSR_TYPE_W);
+ 	}
+ 	kunmap(page);
+ 	nested_release_page_clean(page);
+ 
+ 	return true;
+ }
+ 
+ static int nested_vmx_check_apicv_controls(struct kvm_vcpu *vcpu,
+ 					   struct vmcs12 *vmcs12)
+ {
+ 	if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ 	    !nested_cpu_has_apic_reg_virt(vmcs12) &&
+ 	    !nested_cpu_has_vid(vmcs12) &&
+ 	    !nested_cpu_has_posted_intr(vmcs12))
+ 		return 0;
+ 
+ 	/*
+ 	 * If virtualize x2apic mode is enabled,
+ 	 * virtualize apic access must be disabled.
+ 	 */
+ 	if (nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ 	    nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * If virtual interrupt delivery is enabled,
+ 	 * we must exit on external interrupts.
+ 	 */
+ 	if (nested_cpu_has_vid(vmcs12) &&
+ 	   !nested_exit_on_intr(vcpu))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * bits 15:8 should be zero in posted_intr_nv,
+ 	 * the descriptor address has been already checked
+ 	 * in nested_get_vmcs12_pages.
+ 	 */
+ 	if (nested_cpu_has_posted_intr(vmcs12) &&
+ 	   (!nested_cpu_has_vid(vmcs12) ||
+ 	    !nested_exit_intr_ack_set(vcpu) ||
+ 	    vmcs12->posted_intr_nv & 0xff00))
+ 		return -EINVAL;
+ 
+ 	/* tpr shadow is needed by all apicv features. */
+ 	if (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))
+ 		return -EINVAL;
+ 
+ 	return 0;
++>>>>>>> 705699a13994 (KVM: nVMX: Enable nested posted interrupt processing)
  }
  
  static int nested_vmx_check_msr_switch(struct kvm_vcpu *vcpu,
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 07284ef82f0a..24dcbbc4007f 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -337,19 +337,26 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
-void kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir)
+void __kvm_apic_update_irr(u32 *pir, void *regs)
 {
 	u32 i, pir_val;
-	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	for (i = 0; i <= 7; i++) {
 		pir_val = xchg(&pir[i], 0);
 		if (pir_val)
-			*((u32 *)(apic->regs + APIC_IRR + i * 0x10)) |= pir_val;
+			*((u32 *)(regs + APIC_IRR + i * 0x10)) |= pir_val;
 	}
 
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 }
+EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
+
+void kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir)
+{
+	struct kvm_lapic *apic = vcpu->arch.apic;
+
+	__kvm_apic_update_irr(pir, apic->regs);
+}
 EXPORT_SYMBOL_GPL(kvm_apic_update_irr);
 
 static inline void apic_set_irr(int vec, struct kvm_lapic *apic)
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 91aa7916931c..11d43faaf274 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -58,6 +58,7 @@ u64 kvm_lapic_get_base(struct kvm_vcpu *vcpu);
 void kvm_apic_set_version(struct kvm_vcpu *vcpu);
 
 void kvm_apic_update_tmr(struct kvm_vcpu *vcpu, u32 *tmr);
+void __kvm_apic_update_irr(u32 *pir, void *regs);
 void kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir);
 int kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 dest);
 int kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda);
* Unmerged path arch/x86/kvm/vmx.c
