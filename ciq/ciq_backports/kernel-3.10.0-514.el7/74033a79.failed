mm: meminit: remove mminit_verify_page_links

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] meminit: remove mminit_verify_page_links (George Beshers) [727269]
Rebuild_FUZZ: 95.24%
commit-author Mel Gorman <mgorman@suse.de>
commit 74033a798f5a5db368126ee6f690111cf019bf7a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/74033a79.failed

mminit_verify_page_links() is an extremely paranoid check that was
introduced when memory initialisation was being heavily reworked.
Profiles indicated that up to 10% of parallel memory initialisation was
spent on checking this for every page.  The cost could be reduced but in
practice this check only found problems very early during the
initialisation rewrite and has found nothing since.  This patch removes an
expensive unnecessary check.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Nate Zimmer <nzimmer@sgi.com>
	Tested-by: Waiman Long <waiman.long@hp.com>
	Tested-by: Daniel J Blueman <daniel@numascale.com>
	Acked-by: Pekka Enberg <penberg@kernel.org>
	Cc: Robin Holt <robinmholt@gmail.com>
	Cc: Nate Zimmer <nzimmer@sgi.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Waiman Long <waiman.long@hp.com>
	Cc: Scott Norton <scott.norton@hp.com>
	Cc: "Luck, Tony" <tony.luck@intel.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 74033a798f5a5db368126ee6f690111cf019bf7a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index 20d353397e7d,5a38e39b30d1..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -719,10 -820,97 +719,97 @@@ static void free_one_page(struct zone *
  	spin_unlock(&zone->lock);
  }
  
++<<<<<<< HEAD
++=======
+ static int free_tail_pages_check(struct page *head_page, struct page *page)
+ {
+ 	if (!IS_ENABLED(CONFIG_DEBUG_VM))
+ 		return 0;
+ 	if (unlikely(!PageTail(page))) {
+ 		bad_page(page, "PageTail not set", 0);
+ 		return 1;
+ 	}
+ 	if (unlikely(page->first_page != head_page)) {
+ 		bad_page(page, "first_page not consistent", 0);
+ 		return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
+ 				unsigned long zone, int nid)
+ {
+ 	set_page_links(page, zone, nid, pfn);
+ 	init_page_count(page);
+ 	page_mapcount_reset(page);
+ 	page_cpupid_reset_last(page);
+ 
+ 	INIT_LIST_HEAD(&page->lru);
+ #ifdef WANT_PAGE_VIRTUAL
+ 	/* The shift won't overflow because ZONE_NORMAL is below 4G. */
+ 	if (!is_highmem_idx(zone))
+ 		set_page_address(page, __va(pfn << PAGE_SHIFT));
+ #endif
+ }
+ 
+ static void __meminit __init_single_pfn(unsigned long pfn, unsigned long zone,
+ 					int nid)
+ {
+ 	return __init_single_page(pfn_to_page(pfn), pfn, zone, nid);
+ }
+ 
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static void init_reserved_page(unsigned long pfn)
+ {
+ 	pg_data_t *pgdat;
+ 	int nid, zid;
+ 
+ 	if (!early_page_uninitialised(pfn))
+ 		return;
+ 
+ 	nid = early_pfn_to_nid(pfn);
+ 	pgdat = NODE_DATA(nid);
+ 
+ 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
+ 		struct zone *zone = &pgdat->node_zones[zid];
+ 
+ 		if (pfn >= zone->zone_start_pfn && pfn < zone_end_pfn(zone))
+ 			break;
+ 	}
+ 	__init_single_pfn(pfn, zid, nid);
+ }
+ #else
+ static inline void init_reserved_page(unsigned long pfn)
+ {
+ }
+ #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
+ 
+ /*
+  * Initialised pages do not have PageReserved set. This function is
+  * called for each range allocated by the bootmem allocator and
+  * marks the pages PageReserved. The remaining valid pages are later
+  * sent to the buddy page allocator.
+  */
+ void __meminit reserve_bootmem_region(unsigned long start, unsigned long end)
+ {
+ 	unsigned long start_pfn = PFN_DOWN(start);
+ 	unsigned long end_pfn = PFN_UP(end);
+ 
+ 	for (; start_pfn < end_pfn; start_pfn++) {
+ 		if (pfn_valid(start_pfn)) {
+ 			struct page *page = pfn_to_page(start_pfn);
+ 
+ 			init_reserved_page(start_pfn);
+ 			SetPageReserved(page);
+ 		}
+ 	}
+ }
+ 
++>>>>>>> 74033a798f5a (mm: meminit: remove mminit_verify_page_links)
  static bool free_pages_prepare(struct page *page, unsigned int order)
  {
 -	bool compound = PageCompound(page);
 -	int i, bad = 0;
 -
 -	VM_BUG_ON_PAGE(PageTail(page), page);
 -	VM_BUG_ON_PAGE(compound && compound_order(page) != order, page);
 +	int i;
 +	int bad = 0;
  
  	trace_mm_page_free(page, order);
  	kmemcheck_free_shadow(page, order);
diff --git a/mm/internal.h b/mm/internal.h
index d07fa9595ecf..34b70b6bf382 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -301,10 +301,7 @@ do { \
 } while (0)
 
 extern void mminit_verify_pageflags_layout(void);
-extern void mminit_verify_page_links(struct page *page,
-		enum zone_type zone, unsigned long nid, unsigned long pfn);
 extern void mminit_verify_zonelist(void);
-
 #else
 
 static inline void mminit_dprintk(enum mminit_level level,
@@ -316,11 +313,6 @@ static inline void mminit_verify_pageflags_layout(void)
 {
 }
 
-static inline void mminit_verify_page_links(struct page *page,
-		enum zone_type zone, unsigned long nid, unsigned long pfn)
-{
-}
-
 static inline void mminit_verify_zonelist(void)
 {
 }
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 12dd27a66518..fb0b5cb97792 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -128,14 +128,6 @@ void __init mminit_verify_pageflags_layout(void)
 	BUG_ON(or_mask != add_mask);
 }
 
-void __meminit mminit_verify_page_links(struct page *page, enum zone_type zone,
-			unsigned long nid, unsigned long pfn)
-{
-	BUG_ON(page_to_nid(page) != nid);
-	BUG_ON(page_zonenum(page) != zone);
-	BUG_ON(page_to_pfn(page) != pfn);
-}
-
 static __init int set_mminit_loglevel(char *str)
 {
 	get_option(&str, &mminit_loglevel);
* Unmerged path mm/page_alloc.c
