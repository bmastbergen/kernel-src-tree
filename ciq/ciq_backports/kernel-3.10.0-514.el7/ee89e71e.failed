kernel/watchdog.c: avoid race between lockup detector suspend/resume and CPU hotplug

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [kernel] watchdog: avoid race between lockup detector suspend/resume and CPU hotplug (David Arcari) [1290573]
Rebuild_FUZZ: 94.34%
commit-author Ulrich Obergfell <uobergfe@redhat.com>
commit ee89e71eb091d3ef8ca2be8bd4ec77ccfa91334c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ee89e71e.failed

The lockup detector suspend/resume interface that was introduced by
commit 8c073d27d7ad ("watchdog: introduce watchdog_suspend() and
watchdog_resume()") does not protect itself against races with CPU
hotplug.  Hence, theoretically it is possible that a new watchdog thread
is started on a hotplugged CPU while the lockup detector is suspended,
and the thread could thus interfere unexpectedly with the code that
requested to suspend the lockup detector.

Avoid the race by calling

  get_online_cpus() in lockup_detector_suspend()
  put_online_cpus() in lockup_detector_resume()

	Signed-off-by: Ulrich Obergfell <uobergfe@redhat.com>
	Acked-by: Don Zickus <dzickus@redhat.com>
	Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
	Cc: Ulrich Obergfell <uobergfe@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ee89e71eb091d3ef8ca2be8bd4ec77ccfa91334c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/watchdog.c
diff --cc kernel/watchdog.c
index 830c2c875378,7357842da933..000000000000
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@@ -715,49 -712,71 +715,104 @@@ static void watchdog_unpark_threads(voi
  	put_online_cpus();
  }
  
++<<<<<<< HEAD
 +static void restart_watchdog_hrtimer(void *info)
++=======
+ /*
+  * Suspend the hard and soft lockup detector by parking the watchdog threads.
+  */
+ int lockup_detector_suspend(void)
+ {
+ 	int ret = 0;
+ 
+ 	get_online_cpus();
+ 	mutex_lock(&watchdog_proc_mutex);
+ 	/*
+ 	 * Multiple suspend requests can be active in parallel (counted by
+ 	 * the 'watchdog_suspended' variable). If the watchdog threads are
+ 	 * running, the first caller takes care that they will be parked.
+ 	 * The state of 'watchdog_running' cannot change while a suspend
+ 	 * request is active (see related code in 'proc' handlers).
+ 	 */
+ 	if (watchdog_running && !watchdog_suspended)
+ 		ret = watchdog_park_threads();
+ 
+ 	if (ret == 0)
+ 		watchdog_suspended++;
+ 	else {
+ 		watchdog_disable_all_cpus();
+ 		pr_err("Failed to suspend lockup detectors, disabled\n");
+ 		watchdog_enabled = 0;
+ 	}
+ 
+ 	mutex_unlock(&watchdog_proc_mutex);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Resume the hard and soft lockup detector by unparking the watchdog threads.
+  */
+ void lockup_detector_resume(void)
+ {
+ 	mutex_lock(&watchdog_proc_mutex);
+ 
+ 	watchdog_suspended--;
+ 	/*
+ 	 * The watchdog threads are unparked if they were previously running
+ 	 * and if there is no more active suspend request.
+ 	 */
+ 	if (watchdog_running && !watchdog_suspended)
+ 		watchdog_unpark_threads();
+ 
+ 	mutex_unlock(&watchdog_proc_mutex);
+ 	put_online_cpus();
+ }
+ 
+ static int update_watchdog_all_cpus(void)
++>>>>>>> ee89e71eb091 (kernel/watchdog.c: avoid race between lockup detector suspend/resume and CPU hotplug)
  {
 +	struct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);
  	int ret;
  
 -	ret = watchdog_park_threads();
 -	if (ret)
 -		return ret;
 +	/*
 +	 * No need to cancel and restart hrtimer if it is currently executing
 +	 * because it will reprogram itself with the new period now.
 +	 * We should never see it unqueued here because we are running per-cpu
 +	 * with interrupts disabled.
 +	 */
 +	ret = hrtimer_try_to_cancel(hrtimer);
 +	if (ret == 1)
 +		hrtimer_start(hrtimer, ns_to_ktime(sample_period),
 +				HRTIMER_MODE_REL_PINNED);
 +}
  
 -	watchdog_unpark_threads();
 +static void update_timers(int cpu)
 +{
 +	/*
 +	 * Make sure that perf event counter will adopt to a new
 +	 * sampling period. Updating the sampling period directly would
 +	 * be much nicer but we do not have an API for that now so
 +	 * let's use a big hammer.
 +	 * Hrtimer will adopt the new period on the next tick but this
 +	 * might be late already so we have to restart the timer as well.
 +	 */
 +	watchdog_nmi_disable(cpu);
 +	smp_call_function_single(cpu, restart_watchdog_hrtimer, NULL, 1);
 +	watchdog_nmi_enable(cpu);
 +}
  
 -	return 0;
 +static void update_timers_all_cpus(void)
 +{
 +	int cpu;
 +
 +	get_online_cpus();
 +	for_each_online_cpu(cpu)
 +		update_timers(cpu);
 +	put_online_cpus();
  }
  
 -static int watchdog_enable_all_cpus(void)
 +static int watchdog_enable_all_cpus(bool sample_period_changed)
  {
  	int err = 0;
  
* Unmerged path kernel/watchdog.c
