staging/rdma/hfi1: Adaptive PIO for short messages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [infiniband] rdma/hfi1: Adaptive PIO for short messages (Alex Estrin) [1272062 1273170]
Rebuild_FUZZ: 91.30%
commit-author Mike Marciniszyn <mike.marciniszyn@intel.com>
commit 14553ca11039732bcba3c160a26d702dbe71dd49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/14553ca1.failed

The change requires a new pio_busy field in the iowait structure to
track the number of outstanding pios.  The new counter together
with the sdma counter serve as the basis for a packet by packet decision
as to which egress mechanism to use.  Since packets given to different
egress mechanisms are not ordered, this scheme will preserve the order.

The iowait drain/wait mechanisms are extended for a pio case.  An
additional qp wait flag is added for the PIO drain wait case.

Currently the only pio wait is for buffers, so the no_bufs_available()
routine name is changed to pio_wait() and a third argument is passed
with one of the two pio wait flags to generalize the routine.  A module
parameter is added to hold a configurable threshold. For now, the
module parameter is zero.

A heuristic routine is added to return the func pointer of the proper
egress routine to use.

The heuristic is as follows:
- SMI always uses pio
- GSI,UD qps <= threshold use pio
- UD qps > threadhold use sdma
  o No coordination with sdma is required because order is not required
    and this qp pio count is not maintained for UD
- RC/UC ONLY packets <= threshold chose as follows:
  o If sdmas pending, use SDMA
  o Otherwise use pio and enable the pio tracking count at
    the time the pio buffer is allocated
- RC/UC ONLY packets > threshold use SDMA
  o If pio's are pending the pio_wait with the new wait flag is
    called to delay for pios to drain

The threshold is potentially reduced by the QP's mtu.

The sc_buffer_alloc() has two additional args (a callback, a void *)
which are exploited by the RC/UC cases to pass a new complete routine
and a qp *.

When the shadow ring completes the credit associated with a packet,
the new complete routine is called.  The verbs_pio_complete() will then
decrement the busy count and trigger any drain waiters in qp destroy
or reset.

	Reviewed-by: Jubin John <jubin.john@intel.com>
	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 14553ca11039732bcba3c160a26d702dbe71dd49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/hfi.h
#	drivers/staging/hfi1/iowait.h
#	drivers/staging/hfi1/pio.c
#	drivers/staging/hfi1/rc.c
#	drivers/staging/hfi1/uc.c
#	drivers/staging/hfi1/ud.c
#	drivers/staging/hfi1/user_sdma.h
#	drivers/staging/hfi1/verbs.c
#	drivers/staging/rdma/hfi1/qp.c
#	drivers/staging/rdma/hfi1/verbs.h
diff --cc drivers/staging/hfi1/hfi.h
index 5b1e71911aff,43d48613d48e..000000000000
--- a/drivers/staging/hfi1/hfi.h
+++ b/drivers/staging/hfi1/hfi.h
@@@ -1110,10 -1122,8 +1111,15 @@@ struct hfi1_devdata 
  	 * Handlers for outgoing data so that snoop/capture does not
  	 * have to have its hooks in the send path
  	 */
++<<<<<<< HEAD:drivers/staging/hfi1/hfi.h
 +	int (*process_pio_send)(struct hfi1_qp *qp, struct hfi1_pkt_state *ps,
 +				u64 pbc);
 +	int (*process_dma_send)(struct hfi1_qp *qp, struct hfi1_pkt_state *ps,
 +				u64 pbc);
++=======
+ 	send_routine process_pio_send;
+ 	send_routine process_dma_send;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/hfi.h
  	void (*pio_inline_send)(struct hfi1_devdata *dd, struct pio_buf *pbuf,
  				u64 pbc, const void *from, size_t count);
  
diff --cc drivers/staging/hfi1/iowait.h
index e8ba5606d08d,b5eb1e0a5aa2..000000000000
--- a/drivers/staging/hfi1/iowait.h
+++ b/drivers/staging/hfi1/iowait.h
@@@ -54,6 -54,8 +54,11 @@@
  #include <linux/workqueue.h>
  #include <linux/sched.h>
  
++<<<<<<< HEAD:drivers/staging/hfi1/iowait.h
++=======
+ #include "sdma_txreq.h"
+ 
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/iowait.h
  /*
   * typedef (*restart_t)() - restart callback
   * @work: pointer to work structure
@@@ -183,6 -272,26 +275,7 @@@ static inline int iowait_pio_dec(struc
  static inline void iowait_drain_wakeup(struct iowait *wait)
  {
  	wake_up(&wait->wait_dma);
+ 	wake_up(&wait->wait_pio);
  }
  
 -/**
 - * iowait_get_txhead() - get packet off of iowait list
 - *
 - * @wait wait struture
 - */
 -static inline struct sdma_txreq *iowait_get_txhead(struct iowait *wait)
 -{
 -	struct sdma_txreq *tx = NULL;
 -
 -	if (!list_empty(&wait->tx_head)) {
 -		tx = list_first_entry(
 -			&wait->tx_head,
 -			struct sdma_txreq,
 -			list);
 -		list_del_init(&tx->list);
 -	}
 -	return tx;
 -}
 -
  #endif
diff --cc drivers/staging/hfi1/pio.c
index 25d65f9a0b94,f5aab0ed39d7..000000000000
--- a/drivers/staging/hfi1/pio.c
+++ b/drivers/staging/hfi1/pio.c
@@@ -1564,7 -1564,8 +1564,12 @@@ full
  	write_sequnlock_irqrestore(&dev->iowait_lock, flags);
  
  	for (i = 0; i < n; i++)
++<<<<<<< HEAD:drivers/staging/hfi1/pio.c
 +		hfi1_qp_wakeup(qps[i], HFI1_S_WAIT_PIO);
++=======
+ 		hfi1_qp_wakeup(qps[i],
+ 			       RVT_S_WAIT_PIO | RVT_S_WAIT_PIO_DRAIN);
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/pio.c
  }
  
  /* translate a send credit update to a bit code of reasons */
diff --cc drivers/staging/hfi1/rc.c
index dd57d65aa9b2,443fda8df380..000000000000
--- a/drivers/staging/hfi1/rc.c
+++ b/drivers/staging/hfi1/rc.c
@@@ -58,9 -60,140 +58,135 @@@
  /* cut down ridiculously long IB macro names */
  #define OP(x) IB_OPCODE_RC_##x
  
 -/**
 - * hfi1_add_retry_timer - add/start a retry timer
 - * @qp - the QP
 - *
 - * add a retry timer on the QP
 - */
 -static inline void hfi1_add_retry_timer(struct rvt_qp *qp)
 -{
 -	struct ib_qp *ibqp = &qp->ibqp;
 -	struct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);
 +static void rc_timeout(unsigned long arg);
  
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +static u32 restart_sge(struct hfi1_sge_state *ss, struct hfi1_swqe *wqe,
++=======
+ 	qp->s_flags |= RVT_S_TIMER;
+ 	/* 4.096 usec. * (1 << qp->timeout) */
+ 	qp->s_timer.expires = jiffies + qp->timeout_jiffies +
+ 			      rdi->busy_jiffies;
+ 	add_timer(&qp->s_timer);
+ }
+ 
+ /**
+  * hfi1_add_rnr_timer - add/start an rnr timer
+  * @qp - the QP
+  * @to - timeout in usecs
+  *
+  * add an rnr timer on the QP
+  */
+ void hfi1_add_rnr_timer(struct rvt_qp *qp, u32 to)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	qp->s_flags |= RVT_S_WAIT_RNR;
+ 	qp->s_timer.expires = jiffies + usecs_to_jiffies(to);
+ 	add_timer(&priv->s_rnr_timer);
+ }
+ 
+ /**
+  * hfi1_mod_retry_timer - mod a retry timer
+  * @qp - the QP
+  *
+  * Modify a potentially already running retry
+  * timer
+  */
+ static inline void hfi1_mod_retry_timer(struct rvt_qp *qp)
+ {
+ 	struct ib_qp *ibqp = &qp->ibqp;
+ 	struct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);
+ 
+ 	qp->s_flags |= RVT_S_TIMER;
+ 	/* 4.096 usec. * (1 << qp->timeout) */
+ 	mod_timer(&qp->s_timer, jiffies + qp->timeout_jiffies +
+ 		  rdi->busy_jiffies);
+ }
+ 
+ /**
+  * hfi1_stop_retry_timer - stop a retry timer
+  * @qp - the QP
+  *
+  * stop a retry timer and return if the timer
+  * had been pending.
+  */
+ static inline int hfi1_stop_retry_timer(struct rvt_qp *qp)
+ {
+ 	int rval = 0;
+ 
+ 	/* Remove QP from retry */
+ 	if (qp->s_flags & RVT_S_TIMER) {
+ 		qp->s_flags &= ~RVT_S_TIMER;
+ 		rval = del_timer(&qp->s_timer);
+ 	}
+ 	return rval;
+ }
+ 
+ /**
+  * hfi1_stop_rc_timers - stop all timers
+  * @qp - the QP
+  *
+  * stop any pending timers
+  */
+ void hfi1_stop_rc_timers(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	/* Remove QP from all timers */
+ 	if (qp->s_flags & (RVT_S_TIMER | RVT_S_WAIT_RNR)) {
+ 		qp->s_flags &= ~(RVT_S_TIMER | RVT_S_WAIT_RNR);
+ 		del_timer(&qp->s_timer);
+ 		del_timer(&priv->s_rnr_timer);
+ 	}
+ }
+ 
+ /**
+  * hfi1_stop_rnr_timer - stop an rnr timer
+  * @qp - the QP
+  *
+  * stop an rnr timer and return if the timer
+  * had been pending.
+  */
+ static inline int hfi1_stop_rnr_timer(struct rvt_qp *qp)
+ {
+ 	int rval = 0;
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	/* Remove QP from rnr timer */
+ 	if (qp->s_flags & RVT_S_WAIT_RNR) {
+ 		qp->s_flags &= ~RVT_S_WAIT_RNR;
+ 		rval = del_timer(&priv->s_rnr_timer);
+ 	}
+ 	return rval;
+ }
+ 
+ /**
+  * hfi1_del_timers_sync - wait for any timeout routines to exit
+  * @qp - the QP
+  */
+ void hfi1_del_timers_sync(struct rvt_qp *qp)
+ {
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	del_timer_sync(&qp->s_timer);
+ 	del_timer_sync(&priv->s_rnr_timer);
+ }
+ 
+ /* only opcode mask for adaptive pio */
+ const u32 rc_only_opcode =
+ 	BIT(OP(SEND_ONLY) & 0x1f) |
+ 	BIT(OP(SEND_ONLY_WITH_IMMEDIATE & 0x1f)) |
+ 	BIT(OP(RDMA_WRITE_ONLY & 0x1f)) |
+ 	BIT(OP(RDMA_WRITE_ONLY_WITH_IMMEDIATE & 0x1f)) |
+ 	BIT(OP(RDMA_READ_REQUEST & 0x1f)) |
+ 	BIT(OP(ACKNOWLEDGE & 0x1f)) |
+ 	BIT(OP(ATOMIC_ACKNOWLEDGE & 0x1f)) |
+ 	BIT(OP(COMPARE_SWAP & 0x1f)) |
+ 	BIT(OP(FETCH_ADD & 0x1f));
+ 
+ static u32 restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe,
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/rc.c
  		       u32 psn, u32 pmtu)
  {
  	u32 len;
@@@ -103,9 -228,11 +229,14 @@@ static int make_rc_ack(struct hfi1_ibde
  	u32 bth0;
  	u32 bth2;
  	int middle = 0;
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
++=======
+ 	u32 pmtu = qp->pmtu;
+ 	struct hfi1_qp_priv *priv = qp->priv;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/rc.c
  
  	/* Don't send an ACK if we aren't supposed to. */
 -	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK))
 +	if (!(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_RECV_OK))
  		goto bail;
  
  	/* header size in 32-bit words LRH+BTH = (8+12)/4. */
@@@ -234,8 -361,11 +365,14 @@@ normal
  	}
  	qp->s_rdma_ack_cnt++;
  	qp->s_hdrwords = hwords;
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
++=======
+ 	/* pbc */
+ 	ps->s_txreq->hdr_dwords = hwords + 2;
+ 	ps->s_txreq->sde = priv->s_sde;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/rc.c
  	qp->s_cur_size = len;
 -	hfi1_make_ruc_header(qp, ohdr, bth0, bth2, middle, ps);
 +	hfi1_make_ruc_header(qp, ohdr, bth0, bth2, middle);
  	return 1;
  
  bail:
@@@ -276,30 -406,29 +413,35 @@@ int hfi1_make_rc_req(struct hfi1_qp *qp
  	int middle = 0;
  	int delta;
  
 -	ps->s_txreq = get_txreq(ps->dev, qp);
 -	if (IS_ERR(ps->s_txreq))
 -		goto bail_no_tx;
 -
 -	ohdr = &ps->s_txreq->phdr.hdr.u.oth;
 +	ohdr = &priv->s_hdr->ibh.u.oth;
  	if (qp->remote_ah_attr.ah_flags & IB_AH_GRH)
 -		ohdr = &ps->s_txreq->phdr.hdr.u.l.oth;
 +		ohdr = &priv->s_hdr->ibh.u.l.oth;
 +
 +	/*
 +	 * The lock is needed to synchronize between the sending tasklet,
 +	 * the receive interrupt handler, and timeout re-sends.
 +	 */
 +	spin_lock_irqsave(&qp->s_lock, flags);
  
  	/* Sending responses has higher priority over sending requests. */
 -	if ((qp->s_flags & RVT_S_RESP_PENDING) &&
 -	    make_rc_ack(dev, qp, ohdr, ps))
 -		return 1;
 +	if ((qp->s_flags & HFI1_S_RESP_PENDING) &&
 +	    make_rc_ack(dev, qp, ohdr, pmtu))
 +		goto done;
  
 -	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_SEND_OK)) {
 -		if (!(ib_rvt_state_ops[qp->state] & RVT_FLUSH_SEND))
 +	if (!(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_SEND_OK)) {
 +		if (!(ib_hfi1_state_ops[qp->state] & HFI1_FLUSH_SEND))
  			goto bail;
  		/* We are in the error state, flush the work request. */
 -		smp_read_barrier_depends(); /* see post_one_send() */
 -		if (qp->s_last == ACCESS_ONCE(qp->s_head))
 +		if (qp->s_last == qp->s_head)
  			goto bail;
  		/* If DMAs are in progress, we can't flush immediately. */
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
 +		if (atomic_read(&priv->s_iowait.sdma_busy)) {
 +			qp->s_flags |= HFI1_S_WAIT_DMA;
++=======
+ 		if (iowait_sdma_pending(&priv->s_iowait)) {
+ 			qp->s_flags |= RVT_S_WAIT_DMA;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/rc.c
  			goto bail;
  		}
  		clear_ahg(qp);
@@@ -656,6 -766,9 +798,12 @@@
  	}
  	qp->s_len -= len;
  	qp->s_hdrwords = hwords;
++<<<<<<< HEAD:drivers/staging/hfi1/rc.c
++=======
+ 	/* pbc */
+ 	ps->s_txreq->hdr_dwords = hwords + 2;
+ 	ps->s_txreq->sde = priv->s_sde;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/rc.c
  	qp->s_cur_sge = ss;
  	qp->s_cur_size = len;
  	hfi1_make_ruc_header(
diff --cc drivers/staging/hfi1/uc.c
index fc90d4f544e4,e58ec15dd892..000000000000
--- a/drivers/staging/hfi1/uc.c
+++ b/drivers/staging/hfi1/uc.c
@@@ -71,20 -79,22 +78,25 @@@ int hfi1_make_uc_req(struct hfi1_qp *qp
  	u32 bth0 = 0;
  	u32 len;
  	u32 pmtu = qp->pmtu;
 +	int ret = 0;
  	int middle = 0;
  
 -	ps->s_txreq = get_txreq(ps->dev, qp);
 -	if (IS_ERR(ps->s_txreq))
 -		goto bail_no_tx;
 +	spin_lock_irqsave(&qp->s_lock, flags);
  
 -	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_SEND_OK)) {
 -		if (!(ib_rvt_state_ops[qp->state] & RVT_FLUSH_SEND))
 +	if (!(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_SEND_OK)) {
 +		if (!(ib_hfi1_state_ops[qp->state] & HFI1_FLUSH_SEND))
  			goto bail;
  		/* We are in the error state, flush the work request. */
 -		smp_read_barrier_depends(); /* see post_one_send() */
 -		if (qp->s_last == ACCESS_ONCE(qp->s_head))
 +		if (qp->s_last == qp->s_head)
  			goto bail;
  		/* If DMAs are in progress, we can't flush immediately. */
++<<<<<<< HEAD:drivers/staging/hfi1/uc.c
 +		if (atomic_read(&priv->s_iowait.sdma_busy)) {
 +			qp->s_flags |= HFI1_S_WAIT_DMA;
++=======
+ 		if (iowait_sdma_pending(&priv->s_iowait)) {
+ 			qp->s_flags |= RVT_S_WAIT_DMA;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/uc.c
  			goto bail;
  		}
  		clear_ahg(qp);
@@@ -232,6 -242,9 +244,12 @@@
  	}
  	qp->s_len -= len;
  	qp->s_hdrwords = hwords;
++<<<<<<< HEAD:drivers/staging/hfi1/uc.c
++=======
+ 	/* pbc */
+ 	ps->s_txreq->hdr_dwords = qp->s_hdrwords + 2;
+ 	ps->s_txreq->sde = priv->s_sde;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/uc.c
  	qp->s_cur_sge = &qp->s_sge;
  	qp->s_cur_size = len;
  	hfi1_make_ruc_header(qp, ohdr, bth0 | (qp->s_state << 24),
diff --cc drivers/staging/hfi1/ud.c
index a7f67b0111da,da4e465ae846..000000000000
--- a/drivers/staging/hfi1/ud.c
+++ b/drivers/staging/hfi1/ud.c
@@@ -280,28 -282,33 +280,33 @@@ int hfi1_make_ud_req(struct hfi1_qp *qp
  	int next_cur;
  	u8 sc5;
  
 -	ps->s_txreq = get_txreq(ps->dev, qp);
 -	if (IS_ERR(ps->s_txreq))
 -		goto bail_no_tx;
 +	spin_lock_irqsave(&qp->s_lock, flags);
  
 -	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_NEXT_SEND_OK)) {
 -		if (!(ib_rvt_state_ops[qp->state] & RVT_FLUSH_SEND))
 +	if (!(ib_hfi1_state_ops[qp->state] & HFI1_PROCESS_NEXT_SEND_OK)) {
 +		if (!(ib_hfi1_state_ops[qp->state] & HFI1_FLUSH_SEND))
  			goto bail;
  		/* We are in the error state, flush the work request. */
 -		smp_read_barrier_depends(); /* see post_one_send */
 -		if (qp->s_last == ACCESS_ONCE(qp->s_head))
 +		if (qp->s_last == qp->s_head)
  			goto bail;
  		/* If DMAs are in progress, we can't flush immediately. */
++<<<<<<< HEAD:drivers/staging/hfi1/ud.c
 +		if (atomic_read(&priv->s_iowait.sdma_busy)) {
 +			qp->s_flags |= HFI1_S_WAIT_DMA;
++=======
+ 		if (iowait_sdma_pending(&priv->s_iowait)) {
+ 			qp->s_flags |= RVT_S_WAIT_DMA;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/ud.c
  			goto bail;
  		}
 -		wqe = rvt_get_swqe_ptr(qp, qp->s_last);
 +		wqe = get_swqe_ptr(qp, qp->s_last);
  		hfi1_send_complete(qp, wqe, IB_WC_WR_FLUSH_ERR);
 -		goto done_free_tx;
 +		goto done;
  	}
  
 -	/* see post_one_send() */
 -	smp_read_barrier_depends();
 -	if (qp->s_cur == ACCESS_ONCE(qp->s_head))
 +	if (qp->s_cur == qp->s_head)
  		goto bail;
  
 -	wqe = rvt_get_swqe_ptr(qp, qp->s_cur);
 +	wqe = get_swqe_ptr(qp, qp->s_cur);
  	next_cur = qp->s_cur + 1;
  	if (next_cur >= qp->s_size)
  		next_cur = 0;
@@@ -323,8 -331,8 +328,13 @@@
  			 * Instead of waiting, we could queue a
  			 * zero length descriptor so we get a callback.
  			 */
++<<<<<<< HEAD:drivers/staging/hfi1/ud.c
 +			if (atomic_read(&priv->s_iowait.sdma_busy)) {
 +				qp->s_flags |= HFI1_S_WAIT_DMA;
++=======
+ 			if (iowait_sdma_pending(&priv->s_iowait)) {
+ 				qp->s_flags |= RVT_S_WAIT_DMA;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/ud.c
  				goto bail;
  			}
  			qp->s_cur = next_cur;
diff --cc drivers/staging/hfi1/user_sdma.h
index 7ebbc4634989,1cf69b2fe4a5..000000000000
--- a/drivers/staging/hfi1/user_sdma.h
+++ b/drivers/staging/hfi1/user_sdma.h
@@@ -47,45 -44,73 +47,91 @@@
   * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
   *
   */
 +#include <linux/device.h>
 +#include <linux/wait.h>
  
 -#ifndef HFI1_VERBS_TXREQ_H
 -#define HFI1_VERBS_TXREQ_H
 +#include "common.h"
 +#include "iowait.h"
  
 -#include <linux/types.h>
 -#include <linux/slab.h>
 +#define EXP_TID_TIDLEN_MASK   0x7FFULL
 +#define EXP_TID_TIDLEN_SHIFT  0
 +#define EXP_TID_TIDCTRL_MASK  0x3ULL
 +#define EXP_TID_TIDCTRL_SHIFT 20
 +#define EXP_TID_TIDIDX_MASK   0x7FFULL
 +#define EXP_TID_TIDIDX_SHIFT  22
 +#define EXP_TID_GET(tid, field)	\
 +	(((tid) >> EXP_TID_TID##field##_SHIFT) & EXP_TID_TID##field##_MASK)
  
 -#include "verbs.h"
 -#include "sdma_txreq.h"
 -#include "iowait.h"
 +extern uint extended_psn;
  
 -struct verbs_txreq {
 -	struct hfi1_pio_header	phdr;
 -	struct sdma_txreq       txreq;
 -	struct rvt_qp           *qp;
 -	struct rvt_swqe         *wqe;
 -	struct rvt_mregion	*mr;
 -	struct rvt_sge_state    *ss;
 -	struct sdma_engine     *sde;
 -	struct send_context     *psc;
 -	u16                     hdr_dwords;
 +struct hfi1_user_sdma_pkt_q {
 +	struct list_head list;
 +	unsigned ctxt;
 +	unsigned subctxt;
 +	u16 n_max_reqs;
 +	atomic_t n_reqs;
 +	u16 reqidx;
 +	struct hfi1_devdata *dd;
 +	struct kmem_cache *txreq_cache;
 +	struct user_sdma_request *reqs;
 +	struct iowait busy;
 +	unsigned state;
 +	wait_queue_head_t wait;
 +	unsigned long unpinned;
  };
  
 -struct hfi1_ibdev;
 -struct verbs_txreq *__get_txreq(struct hfi1_ibdev *dev,
 -				struct rvt_qp *qp);
 +struct hfi1_user_sdma_comp_q {
 +	u16 nentries;
 +	struct hfi1_sdma_comp_entry *comps;
 +};
  
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.h
 +int hfi1_user_sdma_alloc_queues(struct hfi1_ctxtdata *, struct file *);
 +int hfi1_user_sdma_free_queues(struct hfi1_filedata *);
 +int hfi1_user_sdma_process_request(struct file *, struct iovec *, unsigned long,
 +				   unsigned long *);
++=======
+ static inline struct verbs_txreq *get_txreq(struct hfi1_ibdev *dev,
+ 					    struct rvt_qp *qp)
+ {
+ 	struct verbs_txreq *tx;
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	tx = kmem_cache_alloc(dev->verbs_txreq_cache, GFP_ATOMIC);
+ 	if (unlikely(!tx)) {
+ 		/* call slow path to get the lock */
+ 		tx = __get_txreq(dev, qp);
+ 		if (IS_ERR(tx))
+ 			return tx;
+ 	}
+ 	tx->qp = qp;
+ 	tx->mr = NULL;
+ 	tx->sde = priv->s_sde;
+ 	tx->psc = priv->s_sendcontext;
+ 	/* so that we can test if the sdma decriptors are there */
+ 	tx->txreq.num_desc = 0;
+ 	return tx;
+ }
+ 
+ static inline struct sdma_txreq *get_sdma_txreq(struct verbs_txreq *tx)
+ {
+ 	return &tx->txreq;
+ }
+ 
+ static inline struct verbs_txreq *get_waiting_verbs_txreq(struct rvt_qp *qp)
+ {
+ 	struct sdma_txreq *stx;
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	stx = iowait_get_txhead(&priv->s_iowait);
+ 	if (stx)
+ 		return container_of(stx, struct verbs_txreq, txreq);
+ 	return NULL;
+ }
+ 
+ void hfi1_put_txreq(struct verbs_txreq *tx);
+ int verbs_txreq_init(struct hfi1_ibdev *dev);
+ void verbs_txreq_exit(struct hfi1_ibdev *dev);
+ 
+ #endif                         /* HFI1_VERBS_TXREQ_H */
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs_txreq.h
diff --cc drivers/staging/hfi1/verbs.c
index d228eb7fc4f0,d900374abe70..000000000000
--- a/drivers/staging/hfi1/verbs.c
+++ b/drivers/staging/hfi1/verbs.c
@@@ -1070,7 -751,10 +1079,14 @@@ bail_tx
   * If we are now in the error state, return zero to flush the
   * send work request.
   */
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +static int no_bufs_available(struct hfi1_qp *qp, struct send_context *sc)
++=======
+ static int pio_wait(struct rvt_qp *qp,
+ 		    struct send_context *sc,
+ 		    struct hfi1_pkt_state *ps,
+ 		    u32 flag)
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs.c
  {
  	struct hfi1_qp_priv *priv = qp->priv;
  	struct hfi1_devdata *dd = sc->dd;
@@@ -1091,11 -777,13 +1107,17 @@@
  			struct hfi1_ibdev *dev = &dd->verbs_dev;
  			int was_empty;
  
+ 			dev->n_piowait += !!(flag & RVT_S_WAIT_PIO);
+ 			dev->n_piodrain += !!(flag & RVT_S_WAIT_PIO_DRAIN);
  			dev->n_piowait++;
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +			qp->s_flags |= HFI1_S_WAIT_PIO;
++=======
+ 			qp->s_flags |= flag;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs.c
  			was_empty = list_empty(&sc->piowait);
  			list_add_tail(&priv->s_iowait.list, &sc->piowait);
 -			trace_hfi1_qpsleep(qp, RVT_S_WAIT_PIO);
 +			trace_hfi1_qpsleep(qp, HFI1_S_WAIT_PIO);
  			atomic_inc(&qp->refcount);
  			/* counting: only call wantpiobuf_intr if first user */
  			if (was_empty)
@@@ -1121,7 -809,16 +1143,20 @@@ struct send_context *qp_to_send_context
  	return dd->vld[vl].sc;
  }
  
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +int hfi1_verbs_send_pio(struct hfi1_qp *qp, struct hfi1_pkt_state *ps,
++=======
+ static void verbs_pio_complete(void *arg, int code)
+ {
+ 	struct rvt_qp *qp = (struct rvt_qp *)arg;
+ 	struct hfi1_qp_priv *priv = qp->priv;
+ 
+ 	if (iowait_pio_dec(&priv->s_iowait))
+ 		iowait_drain_wakeup(&priv->s_iowait);
+ }
+ 
+ int hfi1_verbs_send_pio(struct rvt_qp *qp, struct hfi1_pkt_state *ps,
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs.c
  			u64 pbc)
  {
  	struct hfi1_qp_priv *priv = qp->priv;
@@@ -1139,6 -835,18 +1174,21 @@@
  	struct send_context *sc;
  	struct pio_buf *pbuf;
  	int wc_status = IB_WC_SUCCESS;
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
++=======
+ 	int ret = 0;
+ 	pio_release_cb cb = NULL;
+ 
+ 	/* only RC/UC use complete */
+ 	switch (qp->ibqp.qp_type) {
+ 	case IB_QPT_RC:
+ 	case IB_QPT_UC:
+ 		cb = verbs_pio_complete;
+ 		break;
+ 	default:
+ 		break;
+ 	}
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs.c
  
  	/* vl15 special case taken care of in ud.c */
  	sc5 = priv->s_sc;
@@@ -1173,7 -887,12 +1227,16 @@@
  			 * so lets continue to queue the request.
  			 */
  			hfi1_cdbg(PIO, "alloc failed. state active, queuing");
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +			return no_bufs_available(qp, sc);
++=======
+ 			ret = pio_wait(qp, sc, ps, RVT_S_WAIT_PIO);
+ 			if (!ret)
+ 				/* txreq not queued - free */
+ 				goto bail;
+ 			/* tx consumed in wait */
+ 			return ret;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs.c
  		}
  	}
  
@@@ -1307,27 -1069,16 +1412,25 @@@ static inline send_routine get_send_rou
   * @ps: the state of the packet to send
   *
   * Return zero if packet is sent or queued OK.
 - * Return non-zero and clear qp->s_flags RVT_S_BUSY otherwise.
 + * Return non-zero and clear qp->s_flags HFI1_S_BUSY otherwise.
   */
 -int hfi1_verbs_send(struct rvt_qp *qp, struct hfi1_pkt_state *ps)
 +int hfi1_verbs_send(struct hfi1_qp *qp, struct hfi1_pkt_state *ps)
  {
  	struct hfi1_devdata *dd = dd_from_ibdev(qp->ibqp.device);
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	struct hfi1_qp_priv *priv = qp->priv;
 +	struct ahg_ib_header *ahdr = priv->s_hdr;
++=======
+ 	send_routine sr;
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs.c
  	int ret;
- 	int pio = 0;
- 	unsigned long flags = 0;
- 
- 	/*
- 	 * VL15 packets (IB_QPT_SMI) will always use PIO, so we
- 	 * can defer SDMA restart until link goes ACTIVE without
- 	 * worrying about just how we got there.
- 	 */
- 	if ((qp->ibqp.qp_type == IB_QPT_SMI) ||
- 	    !(dd->flags & HFI1_HAS_SEND_DMA))
- 		pio = 1;
  
++<<<<<<< HEAD:drivers/staging/hfi1/verbs.c
 +	ret = egress_pkey_check(dd->pport, &ahdr->ibh, qp);
++=======
+ 	sr = get_send_routine(qp, &ps->s_txreq->phdr.hdr);
+ 	ret = egress_pkey_check(dd->pport, &ps->s_txreq->phdr.hdr, qp);
++>>>>>>> 14553ca11039 (staging/rdma/hfi1: Adaptive PIO for short messages):drivers/staging/rdma/hfi1/verbs.c
  	if (unlikely(ret)) {
  		/*
  		 * The value we are returning here does not get propagated to
@@@ -1346,71 -1099,52 +1451,58 @@@
  		}
  		return -EINVAL;
  	}
- 
- 	if (pio) {
- 		ret = dd->process_pio_send(qp, ps, 0);
- 	} else {
- #ifdef CONFIG_SDMA_VERBOSITY
- 		dd_dev_err(dd, "CONFIG SDMA %s:%d %s()\n",
- 			   slashstrip(__FILE__), __LINE__, __func__);
- 		dd_dev_err(dd, "SDMA hdrwords = %u, len = %u\n", qp->s_hdrwords,
- 			   qp->s_cur_size);
- #endif
- 		ret = dd->process_dma_send(qp, ps, 0);
- 	}
- 
- 	return ret;
+ 	return sr(qp, ps, 0);
  }
  
 -/**
 - * hfi1_fill_device_attr - Fill in rvt dev info device attributes.
 - * @dd: the device data structure
 - */
 -static void hfi1_fill_device_attr(struct hfi1_devdata *dd)
 +static int query_device(struct ib_device *ibdev,
 +			struct ib_device_attr *props,
 +			struct ib_udata *uhw)
  {
 -	struct rvt_dev_info *rdi = &dd->verbs_dev.rdi;
 -
 -	memset(&rdi->dparms.props, 0, sizeof(rdi->dparms.props));
 -
 -	rdi->dparms.props.device_cap_flags = IB_DEVICE_BAD_PKEY_CNTR |
 -			IB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT |
 -			IB_DEVICE_SYS_IMAGE_GUID | IB_DEVICE_RC_RNR_NAK_GEN |
 -			IB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE;
 -	rdi->dparms.props.page_size_cap = PAGE_SIZE;
 -	rdi->dparms.props.vendor_id = dd->oui1 << 16 | dd->oui2 << 8 | dd->oui3;
 -	rdi->dparms.props.vendor_part_id = dd->pcidev->device;
 -	rdi->dparms.props.hw_ver = dd->minrev;
 -	rdi->dparms.props.sys_image_guid = ib_hfi1_sys_image_guid;
 -	rdi->dparms.props.max_mr_size = ~0ULL;
 -	rdi->dparms.props.max_qp = hfi1_max_qps;
 -	rdi->dparms.props.max_qp_wr = hfi1_max_qp_wrs;
 -	rdi->dparms.props.max_sge = hfi1_max_sges;
 -	rdi->dparms.props.max_sge_rd = hfi1_max_sges;
 -	rdi->dparms.props.max_cq = hfi1_max_cqs;
 -	rdi->dparms.props.max_ah = hfi1_max_ahs;
 -	rdi->dparms.props.max_cqe = hfi1_max_cqes;
 -	rdi->dparms.props.max_mr = rdi->lkey_table.max;
 -	rdi->dparms.props.max_fmr = rdi->lkey_table.max;
 -	rdi->dparms.props.max_map_per_fmr = 32767;
 -	rdi->dparms.props.max_pd = hfi1_max_pds;
 -	rdi->dparms.props.max_qp_rd_atom = HFI1_MAX_RDMA_ATOMIC;
 -	rdi->dparms.props.max_qp_init_rd_atom = 255;
 -	rdi->dparms.props.max_srq = hfi1_max_srqs;
 -	rdi->dparms.props.max_srq_wr = hfi1_max_srq_wrs;
 -	rdi->dparms.props.max_srq_sge = hfi1_max_srq_sges;
 -	rdi->dparms.props.atomic_cap = IB_ATOMIC_GLOB;
 -	rdi->dparms.props.max_pkeys = hfi1_get_npkeys(dd);
 -	rdi->dparms.props.max_mcast_grp = hfi1_max_mcast_grps;
 -	rdi->dparms.props.max_mcast_qp_attach = hfi1_max_mcast_qp_attached;
 -	rdi->dparms.props.max_total_mcast_qp_attach =
 -					rdi->dparms.props.max_mcast_qp_attach *
 -					rdi->dparms.props.max_mcast_grp;
 +	struct hfi1_devdata *dd = dd_from_ibdev(ibdev);
 +	struct hfi1_ibdev *dev = to_idev(ibdev);
 +
 +	if (uhw->inlen || uhw->outlen)
 +		return -EINVAL;
 +	memset(props, 0, sizeof(*props));
 +
 +	props->device_cap_flags = IB_DEVICE_BAD_PKEY_CNTR |
 +		IB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT |
 +		IB_DEVICE_SYS_IMAGE_GUID | IB_DEVICE_RC_RNR_NAK_GEN |
 +		IB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE;
 +
 +	props->page_size_cap = PAGE_SIZE;
 +	props->vendor_id =
 +		dd->oui1 << 16 | dd->oui2 << 8 | dd->oui3;
 +	props->vendor_part_id = dd->pcidev->device;
 +	props->hw_ver = dd->minrev;
 +	props->sys_image_guid = ib_hfi1_sys_image_guid;
 +	props->max_mr_size = ~0ULL;
 +	props->max_qp = hfi1_max_qps;
 +	props->max_qp_wr = hfi1_max_qp_wrs;
 +	props->max_sge = hfi1_max_sges;
 +	props->max_sge_rd = hfi1_max_sges;
 +	props->max_cq = hfi1_max_cqs;
 +	props->max_ah = hfi1_max_ahs;
 +	props->max_cqe = hfi1_max_cqes;
 +	props->max_mr = dev->lk_table.max;
 +	props->max_fmr = dev->lk_table.max;
 +	props->max_map_per_fmr = 32767;
 +	props->max_pd = dev->rdi.dparms.props.max_pd;
 +	props->max_qp_rd_atom = HFI1_MAX_RDMA_ATOMIC;
 +	props->max_qp_init_rd_atom = 255;
 +	/* props->max_res_rd_atom */
 +	props->max_srq = hfi1_max_srqs;
 +	props->max_srq_wr = hfi1_max_srq_wrs;
 +	props->max_srq_sge = hfi1_max_srq_sges;
 +	/* props->local_ca_ack_delay */
 +	props->atomic_cap = IB_ATOMIC_GLOB;
 +	props->max_pkeys = hfi1_get_npkeys(dd);
 +	props->max_mcast_grp = hfi1_max_mcast_grps;
 +	props->max_mcast_qp_attach = hfi1_max_mcast_qp_attached;
 +	props->max_total_mcast_qp_attach = props->max_mcast_qp_attach *
 +		props->max_mcast_grp;
 +
 +	return 0;
  }
  
  static inline u16 opa_speed_to_ib(u16 in)
* Unmerged path drivers/staging/rdma/hfi1/qp.c
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
diff --git a/drivers/staging/hfi1/chip.c b/drivers/staging/hfi1/chip.c
index ab8a02990dfd..5739b5be2bf3 100644
--- a/drivers/staging/hfi1/chip.c
+++ b/drivers/staging/hfi1/chip.c
@@ -1585,6 +1585,14 @@ static u64 access_sw_pio_wait(const struct cntr_entry *entry,
 	return dd->verbs_dev.n_piowait;
 }
 
+static u64 access_sw_pio_drain(const struct cntr_entry *entry,
+			       void *context, int vl, int mode, u64 data)
+{
+	struct hfi1_devdata *dd = (struct hfi1_devdata *)context;
+
+	return dd->verbs_dev.n_piodrain;
+}
+
 static u64 access_sw_vtx_wait(const struct cntr_entry *entry,
 			      void *context, int vl, int mode, u64 data)
 {
@@ -4125,6 +4133,8 @@ static struct cntr_entry dev_cntrs[DEV_CNTR_LAST] = {
 			    access_sw_vtx_wait),
 [C_SW_PIO_WAIT] = CNTR_ELEM("PioWait", 0, 0, CNTR_NORMAL,
 			    access_sw_pio_wait),
+[C_SW_PIO_DRAIN] = CNTR_ELEM("PioDrain", 0, 0, CNTR_NORMAL,
+			    access_sw_pio_drain),
 [C_SW_KMEM_WAIT] = CNTR_ELEM("KmemWait", 0, 0, CNTR_NORMAL,
 			    access_sw_kmem_wait),
 [C_SW_SEND_SCHED] = CNTR_ELEM("SendSched", 0, 0, CNTR_NORMAL,
diff --git a/drivers/staging/hfi1/chip.h b/drivers/staging/hfi1/chip.h
index e9dd145941a9..d0df1a48c72f 100644
--- a/drivers/staging/hfi1/chip.h
+++ b/drivers/staging/hfi1/chip.h
@@ -788,6 +788,7 @@ enum {
 	C_SW_CPU_RCV_LIM,
 	C_SW_VTX_WAIT,
 	C_SW_PIO_WAIT,
+	C_SW_PIO_DRAIN,
 	C_SW_KMEM_WAIT,
 	C_SW_SEND_SCHED,
 	C_SDMA_DESC_FETCHED_CNT,
* Unmerged path drivers/staging/hfi1/hfi.h
* Unmerged path drivers/staging/hfi1/iowait.h
* Unmerged path drivers/staging/hfi1/pio.c
* Unmerged path drivers/staging/hfi1/rc.c
diff --git a/drivers/staging/hfi1/sdma.c b/drivers/staging/hfi1/sdma.c
index 8a57bc183ce2..42510c58661f 100644
--- a/drivers/staging/hfi1/sdma.c
+++ b/drivers/staging/hfi1/sdma.c
@@ -410,7 +410,7 @@ static void sdma_flush(struct sdma_engine *sde)
 #endif
 		sdma_txclean(sde->dd, txp);
 		if (wait)
-			drained = atomic_dec_and_test(&wait->sdma_busy);
+			drained = iowait_sdma_dec(wait);
 		if (txp->complete)
 			(*txp->complete)(txp, SDMA_TXREQ_S_ABORTED, drained);
 		if (wait && drained)
@@ -584,7 +584,7 @@ static void sdma_flush_descq(struct sdma_engine *sde)
 			/* remove from list */
 			sde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;
 			if (wait)
-				drained = atomic_dec_and_test(&wait->sdma_busy);
+				drained = iowait_sdma_dec(wait);
 #ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER
 			trace_hfi1_sdma_out_sn(sde, txp->sn);
 			if (WARN_ON_ONCE(sde->head_sn != txp->sn))
@@ -1500,7 +1500,7 @@ retry:
 			/* remove from list */
 			sde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;
 			if (wait)
-				drained = atomic_dec_and_test(&wait->sdma_busy);
+				drained = iowait_sdma_dec(wait);
 #ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER
 			trace_hfi1_sdma_out_sn(sde, txp->sn);
 			if (WARN_ON_ONCE(sde->head_sn != txp->sn))
@@ -2094,14 +2094,14 @@ retry:
 		goto nodesc;
 	tail = submit_tx(sde, tx);
 	if (wait)
-		atomic_inc(&wait->sdma_busy);
+		iowait_sdma_inc(wait);
 	sdma_update_tail(sde, tail);
 unlock:
 	spin_unlock_irqrestore(&sde->tail_lock, flags);
 	return ret;
 unlock_noconn:
 	if (wait)
-		atomic_inc(&wait->sdma_busy);
+		iowait_sdma_inc(wait);
 	tx->next_descq_idx = 0;
 #ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER
 	tx->sn = sde->tail_sn++;
@@ -2183,7 +2183,7 @@ retry:
 	}
 update_tail:
 	if (wait)
-		atomic_add(count, &wait->sdma_busy);
+		iowait_sdma_add(wait, count);
 	if (tail != INVALID_TAIL)
 		sdma_update_tail(sde, tail);
 	spin_unlock_irqrestore(&sde->tail_lock, flags);
@@ -2194,7 +2194,7 @@ unlock_noconn:
 		tx->wait = wait;
 		list_del_init(&tx->list);
 		if (wait)
-			atomic_inc(&wait->sdma_busy);
+			iowait_sdma_inc(wait);
 		tx->next_descq_idx = 0;
 #ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER
 		tx->sn = sde->tail_sn++;
* Unmerged path drivers/staging/hfi1/uc.c
* Unmerged path drivers/staging/hfi1/ud.c
* Unmerged path drivers/staging/hfi1/user_sdma.h
* Unmerged path drivers/staging/hfi1/verbs.c
* Unmerged path drivers/staging/rdma/hfi1/qp.c
* Unmerged path drivers/staging/rdma/hfi1/verbs.h
diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 1084635190f7..b1b70486c52f 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -82,6 +82,7 @@
  * RVT_S_WAIT_DMA - waiting for send DMA queue to drain before generating
  *                  next send completion entry not via send DMA
  * RVT_S_WAIT_PIO - waiting for a send buffer to be available
+ * RVT_S_WAIT_PIO_DRAIN - waiting for a qp to drain pio packets
  * RVT_S_WAIT_TX - waiting for a struct verbs_txreq to be available
  * RVT_S_WAIT_DMA_DESC - waiting for DMA descriptors to be available
  * RVT_S_WAIT_KMEM - waiting for kernel memory to be available
@@ -101,16 +102,17 @@
 #define RVT_S_WAIT_SSN_CREDIT	0x0100
 #define RVT_S_WAIT_DMA		0x0200
 #define RVT_S_WAIT_PIO		0x0400
-#define RVT_S_WAIT_TX		0x0800
-#define RVT_S_WAIT_DMA_DESC	0x1000
-#define RVT_S_WAIT_KMEM		0x2000
-#define RVT_S_WAIT_PSN		0x4000
-#define RVT_S_WAIT_ACK		0x8000
-#define RVT_S_SEND_ONE		0x10000
-#define RVT_S_UNLIMITED_CREDIT	0x20000
-#define RVT_S_AHG_VALID		0x40000
-#define RVT_S_AHG_CLEAR		0x80000
-#define RVT_S_ECN		0x100000
+#define RVT_S_WAIT_PIO_DRAIN    0x0800
+#define RVT_S_WAIT_TX		0x1000
+#define RVT_S_WAIT_DMA_DESC	0x2000
+#define RVT_S_WAIT_KMEM		0x4000
+#define RVT_S_WAIT_PSN		0x8000
+#define RVT_S_WAIT_ACK		0x10000
+#define RVT_S_SEND_ONE		0x20000
+#define RVT_S_UNLIMITED_CREDIT	0x40000
+#define RVT_S_AHG_VALID		0x80000
+#define RVT_S_AHG_CLEAR		0x100000
+#define RVT_S_ECN		0x200000
 
 /*
  * Wait flags that would prevent any packet type from being sent.
