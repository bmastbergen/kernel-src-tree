IB/qib: Remove modify queue pair code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit 70696ea75b0b9d2cb220a09ea19d72a49f501d8e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/70696ea7.failed

Modify queue pair functionality in rdmavt will be used instead.
Remove ancillary functions which are being used by modify QP code.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 70696ea75b0b9d2cb220a09ea19d72a49f501d8e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/qib/qib_qp.c
#	drivers/infiniband/hw/qib/qib_sdma.c
#	drivers/infiniband/hw/qib/qib_uc.c
#	drivers/infiniband/hw/qib/qib_ud.c
#	drivers/infiniband/hw/qib/qib_verbs.c
#	drivers/infiniband/hw/qib/qib_verbs.h
diff --cc drivers/infiniband/hw/qib/qib_qp.c
index cf1dd6e9d434,cee4aa35ab70..000000000000
--- a/drivers/infiniband/hw/qib/qib_qp.c
+++ b/drivers/infiniband/hw/qib/qib_qp.c
@@@ -201,47 -209,11 +201,53 @@@ bail
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void free_qpn(struct qib_qpn_table *qpt, u32 qpn)
 +{
 +	struct qpn_map *map;
 +
 +	map = qpt->map + qpn / BITS_PER_PAGE;
 +	if (map->page)
 +		clear_bit(qpn & BITS_PER_PAGE_MASK, map->page);
 +}
 +
++=======
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  static inline unsigned qpn_hash(struct qib_ibdev *dev, u32 qpn)
  {
 -	return hash_32(qpn, dev->rdi.qp_dev->qp_table_bits);
 +	return jhash_1word(qpn, dev->qp_rnd) &
 +		(dev->qp_table_size - 1);
 +}
 +
++<<<<<<< HEAD
 +
 +/*
 + * Put the QP into the hash table.
 + * The hash table holds a reference to the QP.
 + */
 +static void insert_qp(struct qib_ibdev *dev, struct qib_qp *qp)
 +{
 +	struct qib_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);
 +	unsigned long flags;
 +	unsigned n = qpn_hash(dev, qp->ibqp.qp_num);
 +
 +	atomic_inc(&qp->refcount);
 +	spin_lock_irqsave(&dev->qpt_lock, flags);
 +
 +	if (qp->ibqp.qp_num == 0)
 +		rcu_assign_pointer(ibp->qp0, qp);
 +	else if (qp->ibqp.qp_num == 1)
 +		rcu_assign_pointer(ibp->qp1, qp);
 +	else {
 +		qp->next = dev->qp_table[n];
 +		rcu_assign_pointer(dev->qp_table[n], qp);
 +	}
 +
 +	spin_unlock_irqrestore(&dev->qpt_lock, flags);
  }
  
++=======
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  /*
   * Remove the QP from the table so it can't be found asynchronously by
   * the receive interrupt routine.
@@@ -329,181 -284,29 +335,188 @@@ unsigned qib_free_all_qps(struct qib_de
  	return qp_inuse;
  }
  
 -void notify_qp_reset(struct rvt_qp *qp)
 +/**
 + * qib_lookup_qpn - return the QP with the given QPN
 + * @qpt: the QP table
 + * @qpn: the QP number to look up
 + *
 + * The caller is responsible for decrementing the QP reference count
 + * when done.
 + */
 +struct qib_qp *qib_lookup_qpn(struct qib_ibport *ibp, u32 qpn)
  {
 -	struct qib_qp_priv *priv = qp->priv;
 +	struct qib_qp *qp = NULL;
 +
 +	rcu_read_lock();
 +	if (unlikely(qpn <= 1)) {
 +		if (qpn == 0)
 +			qp = rcu_dereference(ibp->qp0);
 +		else
 +			qp = rcu_dereference(ibp->qp1);
 +		if (qp)
 +			atomic_inc(&qp->refcount);
 +	} else {
 +		struct qib_ibdev *dev = &ppd_from_ibp(ibp)->dd->verbs_dev;
 +		unsigned n = qpn_hash(dev, qpn);
 +
 +		for (qp = rcu_dereference(dev->qp_table[n]); qp;
 +			qp = rcu_dereference(qp->next))
 +			if (qp->ibqp.qp_num == qpn) {
 +				atomic_inc(&qp->refcount);
 +				break;
 +			}
 +	}
 +	rcu_read_unlock();
 +	return qp;
 +}
  
++<<<<<<< HEAD
 +/**
 + * qib_reset_qp - initialize the QP state to the reset state
 + * @qp: the QP to reset
 + * @type: the QP type
 + */
 +static void qib_reset_qp(struct qib_qp *qp, enum ib_qp_type type)
 +{
 +	struct qib_qp_priv *priv = qp->priv;
 +	qp->remote_qpn = 0;
 +	qp->qkey = 0;
 +	qp->qp_access_flags = 0;
  	atomic_set(&priv->s_dma_busy, 0);
 +	qp->s_flags &= QIB_S_SIGNAL_REQ_WR;
 +	qp->s_hdrwords = 0;
 +	qp->s_wqe = NULL;
 +	qp->s_draining = 0;
 +	qp->s_next_psn = 0;
 +	qp->s_last_psn = 0;
 +	qp->s_sending_psn = 0;
 +	qp->s_sending_hpsn = 0;
 +	qp->s_psn = 0;
 +	qp->r_psn = 0;
 +	qp->r_msn = 0;
 +	if (type == IB_QPT_RC) {
 +		qp->s_state = IB_OPCODE_RC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_RC_SEND_LAST;
 +	} else {
 +		qp->s_state = IB_OPCODE_UC_SEND_LAST;
 +		qp->r_state = IB_OPCODE_UC_SEND_LAST;
 +	}
 +	qp->s_ack_state = IB_OPCODE_RC_ACKNOWLEDGE;
 +	qp->r_nak_state = 0;
 +	qp->r_aflags = 0;
 +	qp->r_flags = 0;
 +	qp->s_head = 0;
 +	qp->s_tail = 0;
 +	qp->s_cur = 0;
 +	qp->s_acked = 0;
 +	qp->s_last = 0;
 +	qp->s_ssn = 1;
 +	qp->s_lsn = 0;
 +	qp->s_mig_state = IB_MIG_MIGRATED;
 +	memset(qp->s_ack_queue, 0, sizeof(qp->s_ack_queue));
 +	qp->r_head_ack_queue = 0;
 +	qp->s_tail_ack_queue = 0;
 +	qp->s_num_rd_atomic = 0;
 +	if (qp->r_rq.wq) {
 +		qp->r_rq.wq->head = 0;
 +		qp->r_rq.wq->tail = 0;
 +	}
 +	qp->r_sge.num_sge = 0;
 +}
 +
 +static void clear_mr_refs(struct qib_qp *qp, int clr_sends)
 +{
 +	unsigned n;
 +
 +	if (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))
 +		qib_put_ss(&qp->s_rdma_read_sge);
 +
 +	qib_put_ss(&qp->r_sge);
 +
 +	if (clr_sends) {
 +		while (qp->s_last != qp->s_head) {
 +			struct qib_swqe *wqe = get_swqe_ptr(qp, qp->s_last);
 +			unsigned i;
 +
 +			for (i = 0; i < wqe->wr.num_sge; i++) {
 +				struct qib_sge *sge = &wqe->sg_list[i];
 +
 +				qib_put_mr(sge->mr);
 +			}
 +			if (qp->ibqp.qp_type == IB_QPT_UD ||
 +			    qp->ibqp.qp_type == IB_QPT_SMI ||
 +			    qp->ibqp.qp_type == IB_QPT_GSI)
 +				atomic_dec(&to_iah(wqe->wr.wr.ud.ah)->refcount);
 +			if (++qp->s_last >= qp->s_size)
 +				qp->s_last = 0;
 +		}
 +		if (qp->s_rdma_mr) {
 +			qib_put_mr(qp->s_rdma_mr);
 +			qp->s_rdma_mr = NULL;
 +		}
 +	}
 +
 +	if (qp->ibqp.qp_type != IB_QPT_RC)
 +		return;
 +
 +	for (n = 0; n < ARRAY_SIZE(qp->s_ack_queue); n++) {
 +		struct qib_ack_entry *e = &qp->s_ack_queue[n];
 +
 +		if (e->opcode == IB_OPCODE_RC_RDMA_READ_REQUEST &&
 +		    e->rdma_sge.mr) {
 +			qib_put_mr(e->rdma_sge.mr);
 +			e->rdma_sge.mr = NULL;
 +		}
 +	}
  }
  
 +/**
 + * qib_error_qp - put a QP into the error state
 + * @qp: the QP to put into the error state
 + * @err: the receive completion error to signal if a RWQE is active
 + *
 + * Flushes both send and receive work queues.
 + * Returns true if last WQE event should be generated.
 + * The QP r_lock and s_lock should be held and interrupts disabled.
 + * If we are already in error state, just return.
 + */
 +int qib_error_qp(struct qib_qp *qp, enum ib_wc_status err)
 +{
 +	struct qib_qp_priv *priv = qp->priv;
 +	struct qib_ibdev *dev = to_idev(qp->ibqp.device);
 +	struct ib_wc wc;
 +	int ret = 0;
 +
 +	if (qp->state == IB_QPS_ERR || qp->state == IB_QPS_RESET)
 +		goto bail;
 +
 +	qp->state = IB_QPS_ERR;
 +
 +	if (qp->s_flags & (QIB_S_TIMER | QIB_S_WAIT_RNR)) {
 +		qp->s_flags &= ~(QIB_S_TIMER | QIB_S_WAIT_RNR);
 +		del_timer(&qp->s_timer);
 +	}
 +
 +	if (qp->s_flags & QIB_S_ANY_WAIT_SEND)
 +		qp->s_flags &= ~QIB_S_ANY_WAIT_SEND;
++=======
+ void notify_error_qp(struct rvt_qp *qp)
+ {
+ 	struct qib_qp_priv *priv = qp->priv;
+ 	struct qib_ibdev *dev = to_idev(qp->ibqp.device);
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  
 -	spin_lock(&dev->rdi.pending_lock);
 -	if (!list_empty(&priv->iowait) && !(qp->s_flags & RVT_S_BUSY)) {
 -		qp->s_flags &= ~RVT_S_ANY_WAIT_IO;
 +	spin_lock(&dev->pending_lock);
 +	if (!list_empty(&priv->iowait) && !(qp->s_flags & QIB_S_BUSY)) {
 +		qp->s_flags &= ~QIB_S_ANY_WAIT_IO;
  		list_del_init(&priv->iowait);
  	}
 -	spin_unlock(&dev->rdi.pending_lock);
 +	spin_unlock(&dev->pending_lock);
  
 -	if (!(qp->s_flags & RVT_S_BUSY)) {
 +	if (!(qp->s_flags & QIB_S_BUSY)) {
  		qp->s_hdrwords = 0;
  		if (qp->s_rdma_mr) {
 -			rvt_put_mr(qp->s_rdma_mr);
 +			qib_put_mr(qp->s_rdma_mr);
  			qp->s_rdma_mr = NULL;
  		}
  		if (priv->s_tx) {
@@@ -511,402 -314,62 +524,322 @@@
  			priv->s_tx = NULL;
  		}
  	}
++<<<<<<< HEAD
 +
 +	/* Schedule the sending tasklet to drain the send work queue. */
 +	if (qp->s_last != qp->s_head)
 +		qib_schedule_send(qp);
 +
 +	clear_mr_refs(qp, 0);
 +
 +	memset(&wc, 0, sizeof(wc));
 +	wc.qp = &qp->ibqp;
 +	wc.opcode = IB_WC_RECV;
 +
 +	if (test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags)) {
 +		wc.wr_id = qp->r_wr_id;
 +		wc.status = err;
 +		qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
 +	}
 +	wc.status = IB_WC_WR_FLUSH_ERR;
 +
 +	if (qp->r_rq.wq) {
 +		struct qib_rwq *wq;
 +		u32 head;
 +		u32 tail;
 +
 +		spin_lock(&qp->r_rq.lock);
 +
 +		/* sanity check pointers before trusting them */
 +		wq = qp->r_rq.wq;
 +		head = wq->head;
 +		if (head >= qp->r_rq.size)
 +			head = 0;
 +		tail = wq->tail;
 +		if (tail >= qp->r_rq.size)
 +			tail = 0;
 +		while (tail != head) {
 +			wc.wr_id = get_rwqe_ptr(&qp->r_rq, tail)->wr_id;
 +			if (++tail >= qp->r_rq.size)
 +				tail = 0;
 +			qib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);
 +		}
 +		wq->tail = tail;
 +
 +		spin_unlock(&qp->r_rq.lock);
 +	} else if (qp->ibqp.event_handler)
 +		ret = 1;
 +
 +bail:
 +	return ret;
++=======
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  }
  
- /**
-  * qib_modify_qp - modify the attributes of a queue pair
-  * @ibqp: the queue pair who's attributes we're modifying
-  * @attr: the new attributes
-  * @attr_mask: the mask of attributes to modify
-  * @udata: user data for libibverbs.so
-  *
-  * Returns 0 on success, otherwise returns an errno.
-  */
- int qib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
- 		  int attr_mask, struct ib_udata *udata)
+ static int mtu_to_enum(u32 mtu)
  {
++<<<<<<< HEAD
 +	struct qib_ibdev *dev = to_idev(ibqp->device);
 +	struct qib_qp *qp = to_iqp(ibqp);
 +	struct qib_qp_priv *priv = qp->priv;
 +	enum ib_qp_state cur_state, new_state;
 +	struct ib_event ev;
 +	int lastwqe = 0;
 +	int mig = 0;
 +	int ret;
 +	u32 pmtu = 0; /* for gcc warning only */
 +
 +	spin_lock_irq(&qp->r_lock);
 +	spin_lock(&qp->s_lock);
 +
 +	cur_state = attr_mask & IB_QP_CUR_STATE ?
 +		attr->cur_qp_state : qp->state;
 +	new_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;
 +
 +	if (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type,
 +				attr_mask, IB_LINK_LAYER_UNSPECIFIED))
 +		goto inval;
 +
 +	if (attr_mask & IB_QP_AV) {
 +		if (attr->ah_attr.dlid >= QIB_MULTICAST_LID_BASE)
 +			goto inval;
 +		if (qib_check_ah(qp->ibqp.device, &attr->ah_attr))
 +			goto inval;
 +	}
 +
 +	if (attr_mask & IB_QP_ALT_PATH) {
 +		if (attr->alt_ah_attr.dlid >= QIB_MULTICAST_LID_BASE)
 +			goto inval;
 +		if (qib_check_ah(qp->ibqp.device, &attr->alt_ah_attr))
 +			goto inval;
 +		if (attr->alt_pkey_index >= qib_get_npkeys(dd_from_dev(dev)))
 +			goto inval;
 +	}
 +
 +	if (attr_mask & IB_QP_PKEY_INDEX)
 +		if (attr->pkey_index >= qib_get_npkeys(dd_from_dev(dev)))
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_MIN_RNR_TIMER)
 +		if (attr->min_rnr_timer > 31)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_PORT)
 +		if (qp->ibqp.qp_type == IB_QPT_SMI ||
 +		    qp->ibqp.qp_type == IB_QPT_GSI ||
 +		    attr->port_num == 0 ||
 +		    attr->port_num > ibqp->device->phys_port_cnt)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_DEST_QPN)
 +		if (attr->dest_qp_num > QIB_QPN_MASK)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_RETRY_CNT)
 +		if (attr->retry_cnt > 7)
 +			goto inval;
 +
 +	if (attr_mask & IB_QP_RNR_RETRY)
 +		if (attr->rnr_retry > 7)
 +			goto inval;
 +
 +	/*
 +	 * Don't allow invalid path_mtu values.  OK to set greater
 +	 * than the active mtu (or even the max_cap, if we have tuned
 +	 * that to a small mtu.  We'll set qp->path_mtu
 +	 * to the lesser of requested attribute mtu and active,
 +	 * for packetizing messages.
 +	 * Note that the QP port has to be set in INIT and MTU in RTR.
 +	 */
 +	if (attr_mask & IB_QP_PATH_MTU) {
 +		struct qib_devdata *dd = dd_from_dev(dev);
 +		int mtu, pidx = qp->port_num - 1;
 +
 +		mtu = ib_mtu_enum_to_int(attr->path_mtu);
 +		if (mtu == -1)
 +			goto inval;
 +		if (mtu > dd->pport[pidx].ibmtu) {
 +			switch (dd->pport[pidx].ibmtu) {
 +			case 4096:
 +				pmtu = IB_MTU_4096;
 +				break;
 +			case 2048:
 +				pmtu = IB_MTU_2048;
 +				break;
 +			case 1024:
 +				pmtu = IB_MTU_1024;
 +				break;
 +			case 512:
 +				pmtu = IB_MTU_512;
 +				break;
 +			case 256:
 +				pmtu = IB_MTU_256;
 +				break;
 +			default:
 +				pmtu = IB_MTU_2048;
 +			}
 +		} else
 +			pmtu = attr->path_mtu;
 +	}
 +
 +	if (attr_mask & IB_QP_PATH_MIG_STATE) {
 +		if (attr->path_mig_state == IB_MIG_REARM) {
 +			if (qp->s_mig_state == IB_MIG_ARMED)
 +				goto inval;
 +			if (new_state != IB_QPS_RTS)
 +				goto inval;
 +		} else if (attr->path_mig_state == IB_MIG_MIGRATED) {
 +			if (qp->s_mig_state == IB_MIG_REARM)
 +				goto inval;
 +			if (new_state != IB_QPS_RTS && new_state != IB_QPS_SQD)
 +				goto inval;
 +			if (qp->s_mig_state == IB_MIG_ARMED)
 +				mig = 1;
 +		} else
 +			goto inval;
 +	}
 +
 +	if (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)
 +		if (attr->max_dest_rd_atomic > QIB_MAX_RDMA_ATOMIC)
 +			goto inval;
 +
 +	switch (new_state) {
 +	case IB_QPS_RESET:
 +		if (qp->state != IB_QPS_RESET) {
 +			qp->state = IB_QPS_RESET;
 +			spin_lock(&dev->pending_lock);
 +			if (!list_empty(&priv->iowait))
 +				list_del_init(&priv->iowait);
 +			spin_unlock(&dev->pending_lock);
 +			qp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);
 +			spin_unlock(&qp->s_lock);
 +			spin_unlock_irq(&qp->r_lock);
 +			/* Stop the sending work queue and retry timer */
 +			cancel_work_sync(&priv->s_work);
 +			del_timer_sync(&qp->s_timer);
 +			wait_event(priv->wait_dma,
 +				   !atomic_read(&priv->s_dma_busy));
 +			if (priv->s_tx) {
 +				qib_put_txreq(priv->s_tx);
 +				priv->s_tx = NULL;
 +			}
 +			remove_qp(dev, qp);
 +			wait_event(qp->wait, !atomic_read(&qp->refcount));
 +			spin_lock_irq(&qp->r_lock);
 +			spin_lock(&qp->s_lock);
 +			clear_mr_refs(qp, 1);
 +			qib_reset_qp(qp, ibqp->qp_type);
 +		}
 +		break;
 +
 +	case IB_QPS_RTR:
 +		/* Allow event to retrigger if QP set to RTR more than once */
 +		qp->r_flags &= ~QIB_R_COMM_EST;
 +		qp->state = new_state;
- 		break;
++=======
+ 	int enum_mtu;
  
- 	case IB_QPS_SQD:
- 		qp->s_draining = qp->s_last != qp->s_cur;
- 		qp->state = new_state;
+ 	switch (mtu) {
+ 	case 4096:
+ 		enum_mtu = IB_MTU_4096;
  		break;
- 
- 	case IB_QPS_SQE:
- 		if (qp->ibqp.qp_type == IB_QPT_RC)
- 			goto inval;
- 		qp->state = new_state;
+ 	case 2048:
+ 		enum_mtu = IB_MTU_2048;
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  		break;
- 
- 	case IB_QPS_ERR:
- 		lastwqe = qib_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+ 	case 1024:
+ 		enum_mtu = IB_MTU_1024;
  		break;
- 
- 	default:
- 		qp->state = new_state;
+ 	case 512:
+ 		enum_mtu = IB_MTU_512;
  		break;
+ 	case 256:
+ 		enum_mtu = IB_MTU_256;
+ 		break;
+ 	default:
+ 		enum_mtu = IB_MTU_2048;
  	}
+ 	return enum_mtu;
+ }
  
- 	if (attr_mask & IB_QP_PKEY_INDEX)
- 		qp->s_pkey_index = attr->pkey_index;
- 
- 	if (attr_mask & IB_QP_PORT)
- 		qp->port_num = attr->port_num;
- 
- 	if (attr_mask & IB_QP_DEST_QPN)
- 		qp->remote_qpn = attr->dest_qp_num;
- 
- 	if (attr_mask & IB_QP_SQ_PSN) {
- 		qp->s_next_psn = attr->sq_psn & QIB_PSN_MASK;
- 		qp->s_psn = qp->s_next_psn;
- 		qp->s_sending_psn = qp->s_next_psn;
- 		qp->s_last_psn = qp->s_next_psn - 1;
- 		qp->s_sending_hpsn = qp->s_last_psn;
- 	}
- 
- 	if (attr_mask & IB_QP_RQ_PSN)
- 		qp->r_psn = attr->rq_psn & QIB_PSN_MASK;
- 
- 	if (attr_mask & IB_QP_ACCESS_FLAGS)
- 		qp->qp_access_flags = attr->qp_access_flags;
- 
- 	if (attr_mask & IB_QP_AV) {
- 		qp->remote_ah_attr = attr->ah_attr;
- 		qp->s_srate = attr->ah_attr.static_rate;
- 	}
- 
- 	if (attr_mask & IB_QP_ALT_PATH) {
- 		qp->alt_ah_attr = attr->alt_ah_attr;
- 		qp->s_alt_pkey_index = attr->alt_pkey_index;
- 	}
- 
- 	if (attr_mask & IB_QP_PATH_MIG_STATE) {
- 		qp->s_mig_state = attr->path_mig_state;
- 		if (mig) {
- 			qp->remote_ah_attr = qp->alt_ah_attr;
- 			qp->port_num = qp->alt_ah_attr.port_num;
- 			qp->s_pkey_index = qp->s_alt_pkey_index;
- 		}
- 	}
- 
- 	if (attr_mask & IB_QP_PATH_MTU) {
- 		qp->path_mtu = pmtu;
- 		qp->pmtu = ib_mtu_enum_to_int(pmtu);
- 	}
- 
- 	if (attr_mask & IB_QP_RETRY_CNT) {
- 		qp->s_retry_cnt = attr->retry_cnt;
- 		qp->s_retry = attr->retry_cnt;
- 	}
- 
- 	if (attr_mask & IB_QP_RNR_RETRY) {
- 		qp->s_rnr_retry_cnt = attr->rnr_retry;
- 		qp->s_rnr_retry = attr->rnr_retry;
- 	}
- 
- 	if (attr_mask & IB_QP_MIN_RNR_TIMER)
- 		qp->r_min_rnr_timer = attr->min_rnr_timer;
- 
- 	if (attr_mask & IB_QP_TIMEOUT) {
- 		qp->timeout = attr->timeout;
- 		qp->timeout_jiffies =
- 			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
- 				1000UL);
- 	}
- 
- 	if (attr_mask & IB_QP_QKEY)
- 		qp->qkey = attr->qkey;
- 
- 	if (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)
- 		qp->r_max_rd_atomic = attr->max_dest_rd_atomic;
- 
- 	if (attr_mask & IB_QP_MAX_QP_RD_ATOMIC)
- 		qp->s_max_rd_atomic = attr->max_rd_atomic;
- 
- 	spin_unlock(&qp->s_lock);
- 	spin_unlock_irq(&qp->r_lock);
- 
- 	if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT)
- 		insert_qp(dev, qp);
- 
- 	if (lastwqe) {
- 		ev.device = qp->ibqp.device;
- 		ev.element.qp = &qp->ibqp;
- 		ev.event = IB_EVENT_QP_LAST_WQE_REACHED;
- 		qp->ibqp.event_handler(&ev, qp->ibqp.qp_context);
- 	}
- 	if (mig) {
- 		ev.device = qp->ibqp.device;
- 		ev.element.qp = &qp->ibqp;
- 		ev.event = IB_EVENT_PATH_MIG;
- 		qp->ibqp.event_handler(&ev, qp->ibqp.qp_context);
- 	}
- 	ret = 0;
- 	goto bail;
+ int get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+ 		       struct ib_qp_attr *attr)
+ {
+ 	int mtu, pmtu, pidx = qp->port_num - 1;
+ 	struct qib_ibdev *verbs_dev = container_of(rdi, struct qib_ibdev, rdi);
+ 	struct qib_devdata *dd = container_of(verbs_dev, struct qib_devdata,
+ 					      verbs_dev);
+ 	mtu = ib_mtu_enum_to_int(attr->path_mtu);
+ 	if (mtu == -1)
+ 		return -EINVAL;
+ 
+ 	if (mtu > dd->pport[pidx].ibmtu)
+ 		pmtu = mtu_to_enum(dd->pport[pidx].ibmtu);
+ 	else
+ 		pmtu = attr->path_mtu;
+ 	return pmtu;
+ }
  
- inval:
- 	spin_unlock(&qp->s_lock);
- 	spin_unlock_irq(&qp->r_lock);
- 	ret = -EINVAL;
+ int mtu_to_path_mtu(u32 mtu)
+ {
+ 	return mtu_to_enum(mtu);
+ }
  
- bail:
- 	return ret;
+ u32 mtu_from_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp, u32 pmtu)
+ {
+ 	return ib_mtu_enum_to_int(pmtu);
  }
  
 +int qib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		 int attr_mask, struct ib_qp_init_attr *init_attr)
 +{
 +	struct qib_qp *qp = to_iqp(ibqp);
 +
 +	attr->qp_state = qp->state;
 +	attr->cur_qp_state = attr->qp_state;
 +	attr->path_mtu = qp->path_mtu;
 +	attr->path_mig_state = qp->s_mig_state;
 +	attr->qkey = qp->qkey;
 +	attr->rq_psn = qp->r_psn & QIB_PSN_MASK;
 +	attr->sq_psn = qp->s_next_psn & QIB_PSN_MASK;
 +	attr->dest_qp_num = qp->remote_qpn;
 +	attr->qp_access_flags = qp->qp_access_flags;
 +	attr->cap.max_send_wr = qp->s_size - 1;
 +	attr->cap.max_recv_wr = qp->ibqp.srq ? 0 : qp->r_rq.size - 1;
 +	attr->cap.max_send_sge = qp->s_max_sge;
 +	attr->cap.max_recv_sge = qp->r_rq.max_sge;
 +	attr->cap.max_inline_data = 0;
 +	attr->ah_attr = qp->remote_ah_attr;
 +	attr->alt_ah_attr = qp->alt_ah_attr;
 +	attr->pkey_index = qp->s_pkey_index;
 +	attr->alt_pkey_index = qp->s_alt_pkey_index;
 +	attr->en_sqd_async_notify = 0;
 +	attr->sq_draining = qp->s_draining;
 +	attr->max_rd_atomic = qp->s_max_rd_atomic;
 +	attr->max_dest_rd_atomic = qp->r_max_rd_atomic;
 +	attr->min_rnr_timer = qp->r_min_rnr_timer;
 +	attr->port_num = qp->port_num;
 +	attr->timeout = qp->timeout;
 +	attr->retry_cnt = qp->s_retry_cnt;
 +	attr->rnr_retry = qp->s_rnr_retry_cnt;
 +	attr->alt_port_num = qp->alt_ah_attr.port_num;
 +	attr->alt_timeout = qp->alt_timeout;
 +
 +	init_attr->event_handler = qp->ibqp.event_handler;
 +	init_attr->qp_context = qp->ibqp.qp_context;
 +	init_attr->send_cq = qp->ibqp.send_cq;
 +	init_attr->recv_cq = qp->ibqp.recv_cq;
 +	init_attr->srq = qp->ibqp.srq;
 +	init_attr->cap = attr->cap;
 +	if (qp->s_flags & QIB_S_SIGNAL_REQ_WR)
 +		init_attr->sq_sig_type = IB_SIGNAL_REQ_WR;
 +	else
 +		init_attr->sq_sig_type = IB_SIGNAL_ALL_WR;
 +	init_attr->qp_type = qp->ibqp.qp_type;
 +	init_attr->port_num = qp->port_num;
 +	return 0;
 +}
 +
  /**
   * qib_compute_aeth - compute the AETH (syndrome + MSN)
   * @qp: the queue pair to compute the AETH for
@@@ -967,248 -430,64 +900,277 @@@ __be32 qib_compute_aeth(struct qib_qp *
  	return cpu_to_be32(aeth);
  }
  
 -void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp, gfp_t gfp)
 +/**
 + * qib_create_qp - create a queue pair for a device
 + * @ibpd: the protection domain who's device we create the queue pair for
 + * @init_attr: the attributes of the queue pair
 + * @udata: user data for libibverbs.so
 + *
 + * Returns the queue pair on success, otherwise returns an errno.
 + *
 + * Called by the ib_create_qp() core verbs function.
 + */
 +struct ib_qp *qib_create_qp(struct ib_pd *ibpd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata)
  {
 +	struct qib_qp *qp;
 +	int err;
 +	struct qib_swqe *swq = NULL;
 +	struct qib_ibdev *dev;
 +	struct qib_devdata *dd;
 +	size_t sz;
 +	size_t sg_list_sz;
 +	struct ib_qp *ret;
 +	gfp_t gfp;
  	struct qib_qp_priv *priv;
  
 -	priv = kzalloc(sizeof(*priv), gfp);
 -	if (!priv)
 -		return ERR_PTR(-ENOMEM);
 -	priv->owner = qp;
 +	if (init_attr->cap.max_send_sge > ib_qib_max_sges ||
 +	    init_attr->cap.max_send_wr > ib_qib_max_qp_wrs ||
 +	    init_attr->create_flags & ~(IB_QP_CREATE_USE_GFP_NOIO))
 +		return ERR_PTR(-EINVAL);
 +
 +	/* GFP_NOIO is applicable in RC QPs only */
 +	if (init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO &&
 +	    init_attr->qp_type != IB_QPT_RC)
 +		return ERR_PTR(-EINVAL);
 +
 +	gfp = init_attr->create_flags & IB_QP_CREATE_USE_GFP_NOIO ?
 +			GFP_NOIO : GFP_KERNEL;
 +
 +	/* Check receive queue parameters if no SRQ is specified. */
 +	if (!init_attr->srq) {
 +		if (init_attr->cap.max_recv_sge > ib_qib_max_sges ||
 +		    init_attr->cap.max_recv_wr > ib_qib_max_qp_wrs) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +		if (init_attr->cap.max_send_sge +
 +		    init_attr->cap.max_send_wr +
 +		    init_attr->cap.max_recv_sge +
 +		    init_attr->cap.max_recv_wr == 0) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +	}
 +
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_SMI:
 +	case IB_QPT_GSI:
 +		if (init_attr->port_num == 0 ||
 +		    init_attr->port_num > ibpd->device->phys_port_cnt) {
 +			ret = ERR_PTR(-EINVAL);
 +			goto bail;
 +		}
 +	case IB_QPT_UC:
 +	case IB_QPT_RC:
 +	case IB_QPT_UD:
 +		sz = sizeof(struct qib_sge) *
 +			init_attr->cap.max_send_sge +
 +			sizeof(struct qib_swqe);
 +		swq = __vmalloc((init_attr->cap.max_send_wr + 1) * sz,
 +				gfp, PAGE_KERNEL);
 +		if (swq == NULL) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail;
 +		}
 +		sz = sizeof(*qp);
 +		sg_list_sz = 0;
 +		if (init_attr->srq) {
 +			struct qib_srq *srq = to_isrq(init_attr->srq);
 +
 +			if (srq->rq.max_sge > 1)
 +				sg_list_sz = sizeof(*qp->r_sg_list) *
 +					(srq->rq.max_sge - 1);
 +		} else if (init_attr->cap.max_recv_sge > 1)
 +			sg_list_sz = sizeof(*qp->r_sg_list) *
 +				(init_attr->cap.max_recv_sge - 1);
 +		qp = kzalloc(sz + sg_list_sz, gfp);
 +		if (!qp) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_swq;
 +		}
 +		RCU_INIT_POINTER(qp->next, NULL);
 +		priv = kzalloc(sizeof(*priv), gfp);
 +		if (!priv) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp_hdr;
 +		}
 +		priv->owner = qp;
 +		priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 +		if (!priv->s_hdr) {
 +			ret = ERR_PTR(-ENOMEM);
 +			goto bail_qp;
 +		}
 +		qp->priv = priv;
 +		qp->timeout_jiffies =
 +			usecs_to_jiffies((4096UL * (1UL << qp->timeout)) /
 +				1000UL);
 +		if (init_attr->srq)
 +			sz = 0;
 +		else {
 +			qp->r_rq.size = init_attr->cap.max_recv_wr + 1;
 +			qp->r_rq.max_sge = init_attr->cap.max_recv_sge;
 +			sz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +
 +				sizeof(struct qib_rwqe);
 +			if (gfp != GFP_NOIO)
 +				qp->r_rq.wq = vmalloc_user(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz);
 +			else
 +				qp->r_rq.wq = __vmalloc(
 +						sizeof(struct qib_rwq) +
 +						qp->r_rq.size * sz,
 +						gfp, PAGE_KERNEL);
 +
 +			if (!qp->r_rq.wq) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_qp;
 +			}
 +		}
 +
 +		/*
 +		 * ib_create_qp() will initialize qp->ibqp
 +		 * except for qp->ibqp.qp_num.
 +		 */
 +		spin_lock_init(&qp->r_lock);
 +		spin_lock_init(&qp->s_lock);
 +		spin_lock_init(&qp->r_rq.lock);
 +		atomic_set(&qp->refcount, 0);
 +		init_waitqueue_head(&qp->wait);
 +		init_waitqueue_head(&priv->wait_dma);
 +		init_timer(&qp->s_timer);
 +		qp->s_timer.data = (unsigned long)qp;
 +		INIT_WORK(&priv->s_work, qib_do_send);
 +		INIT_LIST_HEAD(&priv->iowait);
 +		INIT_LIST_HEAD(&qp->rspwait);
 +		qp->state = IB_QPS_RESET;
 +		qp->s_wq = swq;
 +		qp->s_size = init_attr->cap.max_send_wr + 1;
 +		qp->s_max_sge = init_attr->cap.max_send_sge;
 +		if (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)
 +			qp->s_flags = QIB_S_SIGNAL_REQ_WR;
 +		dev = to_idev(ibpd->device);
 +		dd = dd_from_dev(dev);
 +		err = alloc_qpn(dd, &dev->qpn_table, init_attr->qp_type,
 +				init_attr->port_num, gfp);
 +		if (err < 0) {
 +			ret = ERR_PTR(err);
 +			vfree(qp->r_rq.wq);
 +			goto bail_qp;
 +		}
 +		qp->ibqp.qp_num = err;
 +		qp->port_num = init_attr->port_num;
 +		qib_reset_qp(qp, init_attr->qp_type);
 +		break;
  
 -	priv->s_hdr = kzalloc(sizeof(*priv->s_hdr), gfp);
 -	if (!priv->s_hdr) {
 -		kfree(priv);
 -		return ERR_PTR(-ENOMEM);
 +	default:
 +		/* Don't support raw QPs */
 +		ret = ERR_PTR(-ENOSYS);
 +		goto bail;
  	}
 -	init_waitqueue_head(&priv->wait_dma);
 -	INIT_WORK(&priv->s_work, _qib_do_send);
 -	INIT_LIST_HEAD(&priv->iowait);
  
 -	return priv;
 -}
 +	init_attr->cap.max_inline_data = 0;
  
 -void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp)
 -{
 -	struct qib_qp_priv *priv = qp->priv;
 +	/*
 +	 * Return the address of the RWQ as the offset to mmap.
 +	 * See qib_mmap() for details.
 +	 */
 +	if (udata && udata->outlen >= sizeof(__u64)) {
 +		if (!qp->r_rq.wq) {
 +			__u64 offset = 0;
 +
 +			err = ib_copy_to_udata(udata, &offset,
 +					       sizeof(offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		} else {
 +			u32 s = sizeof(struct qib_rwq) + qp->r_rq.size * sz;
 +
 +			qp->ip = qib_create_mmap_info(dev, s,
 +						      ibpd->uobject->context,
 +						      qp->r_rq.wq);
 +			if (!qp->ip) {
 +				ret = ERR_PTR(-ENOMEM);
 +				goto bail_ip;
 +			}
  
 +			err = ib_copy_to_udata(udata, &(qp->ip->offset),
 +					       sizeof(qp->ip->offset));
 +			if (err) {
 +				ret = ERR_PTR(err);
 +				goto bail_ip;
 +			}
 +		}
 +	}
 +
 +	spin_lock(&dev->n_qps_lock);
 +	if (dev->n_qps_allocated == ib_qib_max_qps) {
 +		spin_unlock(&dev->n_qps_lock);
 +		ret = ERR_PTR(-ENOMEM);
 +		goto bail_ip;
 +	}
 +
 +	dev->n_qps_allocated++;
 +	spin_unlock(&dev->n_qps_lock);
 +
 +	if (qp->ip) {
 +		spin_lock_irq(&dev->pending_lock);
 +		list_add(&qp->ip->pending_mmaps, &dev->pending_mmaps);
 +		spin_unlock_irq(&dev->pending_lock);
 +	}
 +
 +	ret = &qp->ibqp;
 +	goto bail;
 +
 +bail_ip:
 +	if (qp->ip)
 +		kref_put(&qp->ip->ref, qib_release_mmap_info);
 +	else
 +		vfree(qp->r_rq.wq);
 +	free_qpn(&dev->qpn_table, qp->ibqp.qp_num);
 +bail_qp:
  	kfree(priv->s_hdr);
  	kfree(priv);
 +bail_qp_hdr:
 +	kfree(qp);
 +bail_swq:
 +	vfree(swq);
 +bail:
 +	return ret;
  }
  
+ void stop_send_queue(struct rvt_qp *qp)
+ {
+ 	struct qib_qp_priv *priv = qp->priv;
+ 
+ 	cancel_work_sync(&priv->s_work);
+ }
+ 
+ void quiesce_qp(struct rvt_qp *qp)
+ {
+ 	struct qib_qp_priv *priv = qp->priv;
+ 
+ 	wait_event(priv->wait_dma, !atomic_read(&priv->s_dma_busy));
+ 	if (priv->s_tx) {
+ 		qib_put_txreq(priv->s_tx);
+ 		priv->s_tx = NULL;
+ 	}
+ }
+ 
+ void flush_qp_waiters(struct rvt_qp *qp)
+ {
+ 	struct qib_qp_priv *priv = qp->priv;
+ 	struct qib_ibdev *dev = to_idev(qp->ibqp.device);
+ 
+ 	spin_lock(&dev->rdi.pending_lock);
+ 	if (!list_empty(&priv->iowait))
+ 		list_del_init(&priv->iowait);
+ 	spin_unlock(&dev->rdi.pending_lock);
+ }
+ 
  /**
   * qib_destroy_qp - destroy a queue pair
   * @ibqp: the queue pair to destroy
@@@ -1248,13 -527,11 +1210,18 @@@ int qib_destroy_qp(struct ib_qp *ibqp
  		spin_unlock_irq(&qp->s_lock);
  
  	/* all user's cleaned up, mark it available */
++<<<<<<< HEAD
 +	free_qpn(&dev->qpn_table, qp->ibqp.qp_num);
 +	spin_lock(&dev->n_qps_lock);
 +	dev->n_qps_allocated--;
 +	spin_unlock(&dev->n_qps_lock);
++=======
+ 	rvt_free_qpn(&dev->rdi.qp_dev->qpn_table, qp->ibqp.qp_num);
+ 	rvt_dec_qp_cnt(&dev->rdi);
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  
  	if (qp->ip)
 -		kref_put(&qp->ip->ref, rvt_release_mmap_info);
 +		kref_put(&qp->ip->ref, qib_release_mmap_info);
  	else
  		vfree(qp->r_rq.wq);
  	vfree(qp->s_wq);
diff --cc drivers/infiniband/hw/qib/qib_sdma.c
index ac4fcad97505,891873b38a1e..000000000000
--- a/drivers/infiniband/hw/qib/qib_sdma.c
+++ b/drivers/infiniband/hw/qib/qib_sdma.c
@@@ -672,8 -672,8 +672,13 @@@ unmap
  	spin_lock(&qp->s_lock);
  	if (qp->ibqp.qp_type == IB_QPT_RC) {
  		/* XXX what about error sending RDMA read responses? */
++<<<<<<< HEAD
 +		if (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK)
 +			qib_error_qp(qp, IB_WC_GENERAL_ERR);
++=======
+ 		if (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)
+ 			rvt_error_qp(qp, IB_WC_GENERAL_ERR);
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  	} else if (qp->s_wqe)
  		qib_send_complete(qp, qp->s_wqe, IB_WC_GENERAL_ERR);
  	spin_unlock(&qp->s_lock);
diff --cc drivers/infiniband/hw/qib/qib_uc.c
index bab9aeb5dd9e,1b2fc69855b2..000000000000
--- a/drivers/infiniband/hw/qib/qib_uc.c
+++ b/drivers/infiniband/hw/qib/qib_uc.c
@@@ -279,10 -279,10 +279,10 @@@ void qib_uc_rcv(struct qib_ibport *ibp
  inv:
  		if (qp->r_state == OP(SEND_FIRST) ||
  		    qp->r_state == OP(SEND_MIDDLE)) {
 -			set_bit(RVT_R_REWIND_SGE, &qp->r_aflags);
 +			set_bit(QIB_R_REWIND_SGE, &qp->r_aflags);
  			qp->r_sge.num_sge = 0;
  		} else
- 			qib_put_ss(&qp->r_sge);
+ 			rvt_put_ss(&qp->r_sge);
  		qp->r_state = OP(SEND_LAST);
  		switch (opcode) {
  		case OP(SEND_FIRST):
@@@ -484,8 -484,8 +484,13 @@@ rdma_last_imm
  		tlen -= (hdrsize + pad + 4);
  		if (unlikely(tlen + qp->r_rcv_len != qp->r_len))
  			goto drop;
++<<<<<<< HEAD
 +		if (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))
 +			qib_put_ss(&qp->s_rdma_read_sge);
++=======
+ 		if (test_and_clear_bit(RVT_R_REWIND_SGE, &qp->r_aflags))
+ 			rvt_put_ss(&qp->s_rdma_read_sge);
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  		else {
  			ret = qib_get_rwqe(qp, 1);
  			if (ret < 0)
diff --cc drivers/infiniband/hw/qib/qib_ud.c
index 75faa5bd8dd6,fe4917272b89..000000000000
--- a/drivers/infiniband/hw/qib/qib_ud.c
+++ b/drivers/infiniband/hw/qib/qib_ud.c
@@@ -201,8 -206,8 +201,13 @@@ static void qib_ud_loopback(struct qib_
  		}
  		length -= len;
  	}
++<<<<<<< HEAD
 +	qib_put_ss(&qp->r_sge);
 +	if (!test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags))
++=======
+ 	rvt_put_ss(&qp->r_sge);
+ 	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  		goto bail_unlock;
  	wc.wr_id = qp->r_wr_id;
  	wc.status = IB_WC_SUCCESS;
@@@ -561,8 -565,8 +566,13 @@@ void qib_ud_rcv(struct qib_ibport *ibp
  	} else
  		qib_skip_sge(&qp->r_sge, sizeof(struct ib_grh), 1);
  	qib_copy_sge(&qp->r_sge, data, wc.byte_len - sizeof(struct ib_grh), 1);
++<<<<<<< HEAD
 +	qib_put_ss(&qp->r_sge);
 +	if (!test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags))
++=======
+ 	rvt_put_ss(&qp->r_sge);
+ 	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  		return;
  	wc.wr_id = qp->r_wr_id;
  	wc.status = IB_WC_SUCCESS;
diff --cc drivers/infiniband/hw/qib/qib_verbs.c
index c4417a1f33be,187f150b1ff7..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.c
+++ b/drivers/infiniband/hw/qib/qib_verbs.c
@@@ -655,10 -411,10 +655,10 @@@ void qib_ib_rcv(struct qib_ctxtdata *rc
  #endif
  
  	/* Get the destination QP number. */
- 	qp_num = be32_to_cpu(ohdr->bth[1]) & QIB_QPN_MASK;
+ 	qp_num = be32_to_cpu(ohdr->bth[1]) & RVT_QPN_MASK;
  	if (qp_num == QIB_MULTICAST_QPN) {
 -		struct rvt_mcast *mcast;
 -		struct rvt_mcast_qp *p;
 +		struct qib_mcast *mcast;
 +		struct qib_mcast_qp *p;
  
  		if (lnh != QIB_LRH_GRH)
  			goto drop;
@@@ -2109,13 -1644,6 +2109,16 @@@ int qib_register_ib_device(struct qib_d
  		init_ibport(ppd + i);
  
  	/* Only need to initialize non-zero fields. */
++<<<<<<< HEAD
 +	spin_lock_init(&dev->qpt_lock);
 +	spin_lock_init(&dev->n_pds_lock);
 +	spin_lock_init(&dev->n_ahs_lock);
 +	spin_lock_init(&dev->n_cqs_lock);
 +	spin_lock_init(&dev->n_qps_lock);
 +	spin_lock_init(&dev->n_srqs_lock);
 +	spin_lock_init(&dev->n_mcast_grps_lock);
++=======
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  	init_timer(&dev->mem_timer);
  	dev->mem_timer.function = mem_timer;
  	dev->mem_timer.data = (unsigned long) dev;
@@@ -2230,48 -1696,9 +2233,51 @@@
  	ibdev->modify_device = qib_modify_device;
  	ibdev->query_port = qib_query_port;
  	ibdev->modify_port = qib_modify_port;
 +	ibdev->query_pkey = qib_query_pkey;
  	ibdev->query_gid = qib_query_gid;
++<<<<<<< HEAD
 +	ibdev->alloc_ucontext = qib_alloc_ucontext;
 +	ibdev->dealloc_ucontext = qib_dealloc_ucontext;
 +	ibdev->alloc_pd = qib_alloc_pd;
 +	ibdev->dealloc_pd = qib_dealloc_pd;
 +	ibdev->create_ah = qib_create_ah;
 +	ibdev->destroy_ah = qib_destroy_ah;
 +	ibdev->modify_ah = qib_modify_ah;
 +	ibdev->query_ah = qib_query_ah;
 +	ibdev->create_srq = qib_create_srq;
 +	ibdev->modify_srq = qib_modify_srq;
 +	ibdev->query_srq = qib_query_srq;
 +	ibdev->destroy_srq = qib_destroy_srq;
 +	ibdev->create_qp = qib_create_qp;
 +	ibdev->modify_qp = qib_modify_qp;
 +	ibdev->query_qp = qib_query_qp;
++=======
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  	ibdev->destroy_qp = qib_destroy_qp;
 +	ibdev->post_send = qib_post_send;
 +	ibdev->post_recv = qib_post_receive;
 +	ibdev->post_srq_recv = qib_post_srq_receive;
 +	ibdev->create_cq = qib_create_cq;
 +	ibdev->destroy_cq = qib_destroy_cq;
 +	ibdev->resize_cq = qib_resize_cq;
 +	ibdev->poll_cq = qib_poll_cq;
 +	ibdev->req_notify_cq = qib_req_notify_cq;
 +	ibdev->get_dma_mr = qib_get_dma_mr;
 +	ibdev->reg_phys_mr = qib_reg_phys_mr;
 +	ibdev->reg_user_mr = qib_reg_user_mr;
 +	ibdev->dereg_mr = qib_dereg_mr;
 +	ibdev->alloc_mr = qib_alloc_mr;
 +	ibdev->alloc_fast_reg_page_list = qib_alloc_fast_reg_page_list;
 +	ibdev->free_fast_reg_page_list = qib_free_fast_reg_page_list;
 +	ibdev->alloc_fmr = qib_alloc_fmr;
 +	ibdev->map_phys_fmr = qib_map_phys_fmr;
 +	ibdev->unmap_fmr = qib_unmap_fmr;
 +	ibdev->dealloc_fmr = qib_dealloc_fmr;
 +	ibdev->attach_mcast = qib_multicast_attach;
 +	ibdev->detach_mcast = qib_multicast_detach;
  	ibdev->process_mad = qib_process_mad;
 +	ibdev->mmap = qib_mmap;
 +	ibdev->dma_ops = NULL;
  	ibdev->get_port_immutable = qib_port_immutable;
  
  	snprintf(ibdev->node_desc, sizeof(ibdev->node_desc),
@@@ -2283,11 -1710,51 +2289,59 @@@
  	dd->verbs_dev.rdi.driver_f.port_callback = qib_create_port_files;
  	dd->verbs_dev.rdi.driver_f.get_card_name = qib_get_card_name;
  	dd->verbs_dev.rdi.driver_f.get_pci_dev = qib_get_pci_dev;
++<<<<<<< HEAD
 +	dd->verbs_dev.rdi.dparms.props.max_pd = ib_qib_max_pds;
 +	dd->verbs_dev.rdi.flags = (RVT_FLAG_MR_INIT_DRIVER |
 +				   RVT_FLAG_QP_INIT_DRIVER |
 +				   RVT_FLAG_CQ_INIT_DRIVER);
 +
++=======
+ 	dd->verbs_dev.rdi.driver_f.check_ah = qib_check_ah;
+ 	dd->verbs_dev.rdi.driver_f.notify_new_ah = qib_notify_new_ah;
+ 	dd->verbs_dev.rdi.driver_f.alloc_qpn = alloc_qpn;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_alloc = qp_priv_alloc;
+ 	dd->verbs_dev.rdi.driver_f.qp_priv_free = qp_priv_free;
+ 	dd->verbs_dev.rdi.driver_f.free_all_qps = qib_free_all_qps;
+ 	dd->verbs_dev.rdi.driver_f.notify_qp_reset = notify_qp_reset;
+ 	dd->verbs_dev.rdi.driver_f.do_send = qib_do_send;
+ 	dd->verbs_dev.rdi.driver_f.schedule_send = qib_schedule_send;
+ 	dd->verbs_dev.rdi.driver_f.quiesce_qp = quiesce_qp;
+ 	dd->verbs_dev.rdi.driver_f.stop_send_queue = stop_send_queue;
+ 	dd->verbs_dev.rdi.driver_f.flush_qp_waiters = flush_qp_waiters;
+ 	dd->verbs_dev.rdi.driver_f.notify_error_qp = notify_error_qp;
+ 	dd->verbs_dev.rdi.driver_f.mtu_to_path_mtu = mtu_to_path_mtu;
+ 	dd->verbs_dev.rdi.driver_f.mtu_from_qp = mtu_from_qp;
+ 	dd->verbs_dev.rdi.driver_f.get_pmtu_from_attr = get_pmtu_from_attr;
+ 
+ 	dd->verbs_dev.rdi.dparms.max_rdma_atomic = QIB_MAX_RDMA_ATOMIC;
+ 	dd->verbs_dev.rdi.dparms.lkey_table_size = qib_lkey_table_size;
+ 	dd->verbs_dev.rdi.dparms.qp_table_size = ib_qib_qp_table_size;
+ 	dd->verbs_dev.rdi.dparms.qpn_start = 1;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_start = QIB_KD_QP;
+ 	dd->verbs_dev.rdi.dparms.qpn_res_end = QIB_KD_QP; /* Reserve one QP */
+ 	dd->verbs_dev.rdi.dparms.qpn_inc = 1;
+ 	dd->verbs_dev.rdi.dparms.qos_shift = 1;
+ 	dd->verbs_dev.rdi.dparms.psn_mask = QIB_PSN_MASK;
+ 	dd->verbs_dev.rdi.dparms.psn_shift = QIB_PSN_SHIFT;
+ 	dd->verbs_dev.rdi.dparms.psn_modify_mask = QIB_PSN_MASK;
+ 	dd->verbs_dev.rdi.dparms.nports = dd->num_pports;
+ 	dd->verbs_dev.rdi.dparms.npkeys = qib_get_npkeys(dd);
+ 	dd->verbs_dev.rdi.dparms.node = dd->assigned_node_id;
+ 	snprintf(dd->verbs_dev.rdi.dparms.cq_name,
+ 		 sizeof(dd->verbs_dev.rdi.dparms.cq_name),
+ 		 "qib_cq%d", dd->unit);
+ 
+ 	qib_fill_device_attr(dd);
+ 
+ 	ppd = dd->pport;
+ 	for (i = 0; i < dd->num_pports; i++, ppd++) {
+ 		ctxt = ppd->hw_pidx;
+ 		rvt_init_port(&dd->verbs_dev.rdi,
+ 			      &ppd->ibport_data.rvp,
+ 			      i,
+ 			      dd->rcd[ctxt]->pkeys);
+ 	}
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  
  	ret = rvt_register_device(&dd->verbs_dev.rdi);
  	if (ret)
diff --cc drivers/infiniband/hw/qib/qib_verbs.h
index ca366073af4f,0487d62b9cdd..000000000000
--- a/drivers/infiniband/hw/qib/qib_verbs.h
+++ b/drivers/infiniband/hw/qib/qib_verbs.h
@@@ -54,10 -55,8 +54,11 @@@ struct qib_verbs_txreq
  
  #define QIB_MAX_RDMA_ATOMIC     16
  #define QIB_GUIDS_PER_PORT	5
+ #define QIB_PSN_SHIFT		8
  
 +#define QPN_MAX                 (1 << 24)
 +#define QPNMAP_ENTRIES          (QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)
 +
  /*
   * Increment this value if any changes that break userspace ABI
   * compatibility are made.
@@@ -422,252 -196,11 +423,255 @@@ struct qib_qp_priv 
  	struct qib_verbs_txreq *s_tx;
  	struct work_struct s_work;
  	wait_queue_head_t wait_dma;
 -	struct rvt_qp *owner;
 +	struct qib_qp *owner;
 +};
 +
 +/*
 + * Variables prefixed with s_ are for the requester (sender).
 + * Variables prefixed with r_ are for the responder (receiver).
 + * Variables prefixed with ack_ are for responder replies.
 + *
 + * Common variables are protected by both r_rq.lock and s_lock in that order
 + * which only happens in modify_qp() or changing the QP 'state'.
 + */
 +struct qib_qp {
 +	struct ib_qp ibqp;
 +	struct qib_qp_priv *priv;
 +	/* read mostly fields above and below */
 +	struct ib_ah_attr remote_ah_attr;
 +	struct ib_ah_attr alt_ah_attr;
 +	struct qib_qp __rcu *next;            /* link list for QPN hash table */
 +	struct qib_swqe *s_wq;  /* send work queue */
 +	struct qib_mmap_info *ip;
 +	unsigned long timeout_jiffies;  /* computed from timeout */
 +
 +	enum ib_mtu path_mtu;
 +	u32 remote_qpn;
 +	u32 pmtu;		/* decoded from path_mtu */
 +	u32 qkey;               /* QKEY for this QP (for UD or RD) */
 +	u32 s_size;             /* send work queue size */
 +	u32 s_rnr_timeout;      /* number of milliseconds for RNR timeout */
 +
 +	u8 state;               /* QP state */
 +	u8 qp_access_flags;
 +	u8 alt_timeout;         /* Alternate path timeout for this QP */
 +	u8 timeout;             /* Timeout for this QP */
 +	u8 s_srate;
 +	u8 s_mig_state;
 +	u8 port_num;
 +	u8 s_pkey_index;        /* PKEY index to use */
 +	u8 s_alt_pkey_index;    /* Alternate path PKEY index to use */
 +	u8 r_max_rd_atomic;     /* max number of RDMA read/atomic to receive */
 +	u8 s_max_rd_atomic;     /* max number of RDMA read/atomic to send */
 +	u8 s_retry_cnt;         /* number of times to retry */
 +	u8 s_rnr_retry_cnt;
 +	u8 r_min_rnr_timer;     /* retry timeout value for RNR NAKs */
 +	u8 s_max_sge;           /* size of s_wq->sg_list */
 +	u8 s_draining;
 +
 +	/* start of read/write fields */
 +
 +	atomic_t refcount ____cacheline_aligned_in_smp;
 +	wait_queue_head_t wait;
 +
 +
 +	struct qib_ack_entry s_ack_queue[QIB_MAX_RDMA_ATOMIC + 1]
 +		____cacheline_aligned_in_smp;
 +	struct qib_sge_state s_rdma_read_sge;
 +
 +	spinlock_t r_lock ____cacheline_aligned_in_smp;      /* used for APM */
 +	unsigned long r_aflags;
 +	u64 r_wr_id;            /* ID for current receive WQE */
 +	u32 r_ack_psn;          /* PSN for next ACK or atomic ACK */
 +	u32 r_len;              /* total length of r_sge */
 +	u32 r_rcv_len;          /* receive data len processed */
 +	u32 r_psn;              /* expected rcv packet sequence number */
 +	u32 r_msn;              /* message sequence number */
 +
 +	u8 r_state;             /* opcode of last packet received */
 +	u8 r_flags;
 +	u8 r_head_ack_queue;    /* index into s_ack_queue[] */
 +
 +	struct list_head rspwait;       /* link for waititing to respond */
 +
 +	struct qib_sge_state r_sge;     /* current receive data */
 +	struct qib_rq r_rq;             /* receive work queue */
 +
 +	spinlock_t s_lock ____cacheline_aligned_in_smp;
 +	struct qib_sge_state *s_cur_sge;
 +	u32 s_flags;
 +
 +	struct qib_swqe *s_wqe;
 +	struct qib_sge_state s_sge;     /* current send request data */
 +	struct qib_mregion *s_rdma_mr;
 +
 +	u32 s_cur_size;         /* size of send packet in bytes */
 +	u32 s_len;              /* total length of s_sge */
 +	u32 s_rdma_read_len;    /* total length of s_rdma_read_sge */
 +	u32 s_next_psn;         /* PSN for next request */
 +	u32 s_last_psn;         /* last response PSN processed */
 +	u32 s_sending_psn;      /* lowest PSN that is being sent */
 +	u32 s_sending_hpsn;     /* highest PSN that is being sent */
 +	u32 s_psn;              /* current packet sequence number */
 +	u32 s_ack_rdma_psn;     /* PSN for sending RDMA read responses */
 +	u32 s_ack_psn;          /* PSN for acking sends and RDMA writes */
 +	u32 s_head;             /* new entries added here */
 +	u32 s_tail;             /* next entry to process */
 +	u32 s_cur;              /* current work queue entry */
 +	u32 s_acked;            /* last un-ACK'ed entry */
 +	u32 s_last;             /* last completed entry */
 +	u32 s_ssn;              /* SSN of tail entry */
 +	u32 s_lsn;              /* limit sequence number (credit) */
 +	u16 s_hdrwords;         /* size of s_hdr in 32 bit words */
 +	u16 s_rdma_ack_cnt;
 +	u8 s_state;             /* opcode of last packet sent */
 +	u8 s_ack_state;         /* opcode of packet to ACK */
 +	u8 s_nak_state;         /* non-zero if NAK is pending */
 +	u8 r_nak_state;         /* non-zero if NAK is pending */
 +	u8 s_retry;             /* requester retry counter */
 +	u8 s_rnr_retry;         /* requester RNR retry counter */
 +	u8 s_num_rd_atomic;     /* number of RDMA read/atomic pending */
 +	u8 s_tail_ack_queue;    /* index into s_ack_queue[] */
 +
 +	struct qib_sge_state s_ack_rdma_sge;
 +	struct timer_list s_timer;
 +
 +	struct qib_sge r_sg_list[0] /* verified SGEs */
 +		____cacheline_aligned_in_smp;
  };
  
 +/*
 + * Atomic bit definitions for r_aflags.
 + */
 +#define QIB_R_WRID_VALID        0
 +#define QIB_R_REWIND_SGE        1
 +
 +/*
 + * Bit definitions for r_flags.
 + */
 +#define QIB_R_REUSE_SGE 0x01
 +#define QIB_R_RDMAR_SEQ 0x02
 +#define QIB_R_RSP_NAK   0x04
 +#define QIB_R_RSP_SEND  0x08
 +#define QIB_R_COMM_EST  0x10
 +
 +/*
 + * Bit definitions for s_flags.
 + *
 + * QIB_S_SIGNAL_REQ_WR - set if QP send WRs contain completion signaled
 + * QIB_S_BUSY - send tasklet is processing the QP
 + * QIB_S_TIMER - the RC retry timer is active
 + * QIB_S_ACK_PENDING - an ACK is waiting to be sent after RDMA read/atomics
 + * QIB_S_WAIT_FENCE - waiting for all prior RDMA read or atomic SWQEs
 + *                         before processing the next SWQE
 + * QIB_S_WAIT_RDMAR - waiting for a RDMA read or atomic SWQE to complete
 + *                         before processing the next SWQE
 + * QIB_S_WAIT_RNR - waiting for RNR timeout
 + * QIB_S_WAIT_SSN_CREDIT - waiting for RC credits to process next SWQE
 + * QIB_S_WAIT_DMA - waiting for send DMA queue to drain before generating
 + *                  next send completion entry not via send DMA
 + * QIB_S_WAIT_PIO - waiting for a send buffer to be available
 + * QIB_S_WAIT_TX - waiting for a struct qib_verbs_txreq to be available
 + * QIB_S_WAIT_DMA_DESC - waiting for DMA descriptors to be available
 + * QIB_S_WAIT_KMEM - waiting for kernel memory to be available
 + * QIB_S_WAIT_PSN - waiting for a packet to exit the send DMA queue
 + * QIB_S_WAIT_ACK - waiting for an ACK packet before sending more requests
 + * QIB_S_SEND_ONE - send one packet, request ACK, then wait for ACK
 + */
 +#define QIB_S_SIGNAL_REQ_WR	0x0001
 +#define QIB_S_BUSY		0x0002
 +#define QIB_S_TIMER		0x0004
 +#define QIB_S_RESP_PENDING	0x0008
 +#define QIB_S_ACK_PENDING	0x0010
 +#define QIB_S_WAIT_FENCE	0x0020
 +#define QIB_S_WAIT_RDMAR	0x0040
 +#define QIB_S_WAIT_RNR		0x0080
 +#define QIB_S_WAIT_SSN_CREDIT	0x0100
 +#define QIB_S_WAIT_DMA		0x0200
 +#define QIB_S_WAIT_PIO		0x0400
 +#define QIB_S_WAIT_TX		0x0800
 +#define QIB_S_WAIT_DMA_DESC	0x1000
 +#define QIB_S_WAIT_KMEM		0x2000
 +#define QIB_S_WAIT_PSN		0x4000
 +#define QIB_S_WAIT_ACK		0x8000
 +#define QIB_S_SEND_ONE		0x10000
 +#define QIB_S_UNLIMITED_CREDIT	0x20000
 +
 +/*
 + * Wait flags that would prevent any packet type from being sent.
 + */
 +#define QIB_S_ANY_WAIT_IO (QIB_S_WAIT_PIO | QIB_S_WAIT_TX | \
 +	QIB_S_WAIT_DMA_DESC | QIB_S_WAIT_KMEM)
 +
 +/*
 + * Wait flags that would prevent send work requests from making progress.
 + */
 +#define QIB_S_ANY_WAIT_SEND (QIB_S_WAIT_FENCE | QIB_S_WAIT_RDMAR | \
 +	QIB_S_WAIT_RNR | QIB_S_WAIT_SSN_CREDIT | QIB_S_WAIT_DMA | \
 +	QIB_S_WAIT_PSN | QIB_S_WAIT_ACK)
 +
 +#define QIB_S_ANY_WAIT (QIB_S_ANY_WAIT_IO | QIB_S_ANY_WAIT_SEND)
 +
  #define QIB_PSN_CREDIT  16
  
++<<<<<<< HEAD
 +/*
 + * Since struct qib_swqe is not a fixed size, we can't simply index into
 + * struct qib_qp.s_wq.  This function does the array index computation.
 + */
 +static inline struct qib_swqe *get_swqe_ptr(struct qib_qp *qp,
 +					      unsigned n)
 +{
 +	return (struct qib_swqe *)((char *)qp->s_wq +
 +				     (sizeof(struct qib_swqe) +
 +				      qp->s_max_sge *
 +				      sizeof(struct qib_sge)) * n);
 +}
 +
 +/*
 + * Since struct qib_rwqe is not a fixed size, we can't simply index into
 + * struct qib_rwq.wq.  This function does the array index computation.
 + */
 +static inline struct qib_rwqe *get_rwqe_ptr(struct qib_rq *rq, unsigned n)
 +{
 +	return (struct qib_rwqe *)
 +		((char *) rq->wq->wq +
 +		 (sizeof(struct qib_rwqe) +
 +		  rq->max_sge * sizeof(struct ib_sge)) * n);
 +}
 +
 +/*
 + * QPN-map pages start out as NULL, they get allocated upon
 + * first use and are never deallocated. This way,
 + * large bitmaps are not allocated unless large numbers of QPs are used.
 + */
 +struct qpn_map {
 +	void *page;
 +};
 +
 +struct qib_qpn_table {
 +	spinlock_t lock; /* protect changes in this struct */
 +	unsigned flags;         /* flags for QP0/1 allocated for each port */
 +	u32 last;               /* last QP number allocated */
 +	u32 nmaps;              /* size of the map table */
 +	u16 limit;
 +	u16 mask;
 +	/* bit map of free QP numbers other than 0/1 */
 +	struct qpn_map map[QPNMAP_ENTRIES];
 +};
 +
 +#define MAX_LKEY_TABLE_BITS 23
 +
 +struct qib_lkey_table {
 +	spinlock_t lock; /* protect changes in this struct */
 +	u32 next;               /* next unused index (speeds search) */
 +	u32 gen;                /* generation count */
 +	u32 max;                /* size of the table */
 +	struct qib_mregion __rcu **table;
 +};
 +
++=======
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  struct qib_opcode_stats {
  	u64 n_packets;          /* number of packets */
  	u64 n_bytes;            /* total number of bytes */
@@@ -782,18 -257,6 +786,21 @@@ struct qib_ibdev 
  	u32 n_piowait;
  	u32 n_txwait;
  
++<<<<<<< HEAD
 +	u32 n_pds_allocated;    /* number of PDs allocated for device */
 +	spinlock_t n_pds_lock;
 +	u32 n_ahs_allocated;    /* number of AHs allocated for device */
 +	spinlock_t n_ahs_lock;
 +	u32 n_cqs_allocated;    /* number of CQs allocated for device */
 +	spinlock_t n_cqs_lock;
 +	u32 n_qps_allocated;    /* number of QPs allocated for device */
 +	spinlock_t n_qps_lock;
 +	u32 n_srqs_allocated;   /* number of SRQs allocated for device */
 +	spinlock_t n_srqs_lock;
 +	u32 n_mcast_grps_allocated; /* number of mcast groups allocated */
 +	spinlock_t n_mcast_grps_lock;
++=======
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  #ifdef CONFIG_DEBUG_FS
  	/* per HCA debugfs */
  	struct dentry *qib_ibdev_dbg;
@@@ -929,19 -348,15 +936,31 @@@ struct ib_qp *qib_create_qp(struct ib_p
  
  int qib_destroy_qp(struct ib_qp *ibqp);
  
++<<<<<<< HEAD
 +int qib_error_qp(struct qib_qp *qp, enum ib_wc_status err);
 +
 +int qib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		  int attr_mask, struct ib_udata *udata);
 +
 +int qib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 +		 int attr_mask, struct ib_qp_init_attr *init_attr);
 +
 +unsigned qib_free_all_qps(struct qib_devdata *dd);
 +
 +void qib_init_qpn_table(struct qib_devdata *dd, struct qib_qpn_table *qpt);
 +
 +void qib_free_qpn_table(struct qib_qpn_table *qpt);
++=======
+ /*
+  * Functions provided by qib driver for rdmavt to use
+  */
+ unsigned qib_free_all_qps(struct rvt_dev_info *rdi);
+ void *qp_priv_alloc(struct rvt_dev_info *rdi, struct rvt_qp *qp, gfp_t gfp);
+ void qp_priv_free(struct rvt_dev_info *rdi, struct rvt_qp *qp);
+ void notify_qp_reset(struct rvt_qp *qp);
+ int alloc_qpn(struct rvt_dev_info *rdi, struct rvt_qpn_table *qpt,
+ 	      enum ib_qp_type type, u8 port, gfp_t gfp);
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
  
  #ifdef CONFIG_DEBUG_FS
  
@@@ -1076,39 -409,12 +1095,43 @@@ static inline void qib_get_mr(struct qi
  
  void mr_rcu_callback(struct rcu_head *list);
  
++<<<<<<< HEAD
 +static inline void qib_put_mr(struct qib_mregion *mr)
 +{
 +	if (unlikely(atomic_dec_and_test(&mr->refcount)))
 +		call_rcu(&mr->list, mr_rcu_callback);
 +}
 +
 +static inline void qib_put_ss(struct qib_sge_state *ss)
 +{
 +	while (ss->num_sge) {
 +		qib_put_mr(ss->sge.mr);
 +		if (--ss->num_sge)
 +			ss->sge = *ss->sg_list++;
 +	}
 +}
 +
++=======
+ int qib_get_rwqe(struct rvt_qp *qp, int wr_id_only);
++>>>>>>> 70696ea75b0b (IB/qib: Remove modify queue pair code)
 +
 +void qib_release_mmap_info(struct kref *ref);
  
 -void qib_migrate_qp(struct rvt_qp *qp);
 +struct qib_mmap_info *qib_create_mmap_info(struct qib_ibdev *dev, u32 size,
 +					   struct ib_ucontext *context,
 +					   void *obj);
 +
 +void qib_update_mmap_info(struct qib_ibdev *dev, struct qib_mmap_info *ip,
 +			  u32 size, void *obj);
 +
 +int qib_mmap(struct ib_ucontext *context, struct vm_area_struct *vma);
 +
 +int qib_get_rwqe(struct qib_qp *qp, int wr_id_only);
 +
 +void qib_migrate_qp(struct qib_qp *qp);
  
  int qib_ruc_check_hdr(struct qib_ibport *ibp, struct qib_ib_header *hdr,
 -		      int has_grh, struct rvt_qp *qp, u32 bth0);
 +		      int has_grh, struct qib_qp *qp, u32 bth0);
  
  u32 qib_make_grh(struct qib_ibport *ibp, struct ib_grh *hdr,
  		 struct ib_global_route *grh, u32 hwords, u32 nwords);
diff --git a/drivers/infiniband/hw/qib/qib.h b/drivers/infiniband/hw/qib/qib.h
index 2c9672d7da79..d3e9fa6e4e13 100644
--- a/drivers/infiniband/hw/qib/qib.h
+++ b/drivers/infiniband/hw/qib/qib.h
@@ -1544,4 +1544,14 @@ struct qib_hwerror_msgs {
 void qib_format_hwerrors(u64 hwerrs,
 			 const struct qib_hwerror_msgs *hwerrmsgs,
 			 size_t nhwerrmsgs, char *msg, size_t lmsg);
+
+void stop_send_queue(struct rvt_qp *qp);
+void quiesce_qp(struct rvt_qp *qp);
+void flush_qp_waiters(struct rvt_qp *qp);
+int mtu_to_path_mtu(u32 mtu);
+u32 mtu_from_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp, u32 pmtu);
+void notify_error_qp(struct rvt_qp *qp);
+int get_pmtu_from_attr(struct rvt_dev_info *rdi, struct rvt_qp *qp,
+		       struct ib_qp_attr *attr);
+
 #endif                          /* _QIB_KERNEL_H */
diff --git a/drivers/infiniband/hw/qib/qib_common.h b/drivers/infiniband/hw/qib/qib_common.h
index 1d87ec011fa3..1d6e63eb1146 100644
--- a/drivers/infiniband/hw/qib/qib_common.h
+++ b/drivers/infiniband/hw/qib/qib_common.h
@@ -747,7 +747,6 @@ struct qib_tid_session_member {
 #define QIB_AETH_CREDIT_INVAL 0x1F
 #define QIB_PSN_MASK 0xFFFFFF
 #define QIB_MSN_MASK 0xFFFFFF
-#define QIB_QPN_MASK 0xFFFFFF
 #define QIB_EAGER_TID_ID QLOGIC_IB_I_TID_MASK
 #define QIB_MULTICAST_QPN 0xFFFFFF
 
diff --git a/drivers/infiniband/hw/qib/qib_driver.c b/drivers/infiniband/hw/qib/qib_driver.c
index 323352f4bb60..f2037a0ac9ac 100644
--- a/drivers/infiniband/hw/qib/qib_driver.c
+++ b/drivers/infiniband/hw/qib/qib_driver.c
@@ -362,7 +362,7 @@ static u32 qib_rcv_hdrerr(struct qib_ctxtdata *rcd, struct qib_pportdata *ppd,
 		psn = be32_to_cpu(ohdr->bth[2]);
 
 		/* Get the destination QP number. */
-		qp_num = be32_to_cpu(ohdr->bth[1]) & QIB_QPN_MASK;
+		qp_num = be32_to_cpu(ohdr->bth[1]) & RVT_QPN_MASK;
 		if (qp_num != QIB_MULTICAST_QPN) {
 			int ruc_res;
 
* Unmerged path drivers/infiniband/hw/qib/qib_qp.c
diff --git a/drivers/infiniband/hw/qib/qib_rc.c b/drivers/infiniband/hw/qib/qib_rc.c
index c23ede5294da..51dda9aefe5c 100644
--- a/drivers/infiniband/hw/qib/qib_rc.c
+++ b/drivers/infiniband/hw/qib/qib_rc.c
@@ -877,7 +877,7 @@ static void qib_restart_rc(struct qib_qp *qp, u32 psn, int wait)
 			qp->s_retry = qp->s_retry_cnt;
 		} else if (qp->s_last == qp->s_acked) {
 			qib_send_complete(qp, wqe, IB_WC_RETRY_EXC_ERR);
-			qib_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+			rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);
 			return;
 		} else /* XXX need to handle delayed completion */
 			return;
@@ -1322,7 +1322,7 @@ static int do_rc_ack(struct qib_qp *qp, u32 aeth, u32 psn, int opcode,
 class_b:
 			if (qp->s_last == qp->s_acked) {
 				qib_send_complete(qp, wqe, status);
-				qib_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+				rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);
 			}
 			break;
 
@@ -1597,7 +1597,7 @@ ack_len_err:
 ack_err:
 	if (qp->s_last == qp->s_acked) {
 		qib_send_complete(qp, wqe, status);
-		qib_error_qp(qp, IB_WC_WR_FLUSH_ERR);
+		rvt_error_qp(qp, IB_WC_WR_FLUSH_ERR);
 	}
 ack_done:
 	spin_unlock_irqrestore(&qp->s_lock, flags);
@@ -1823,7 +1823,7 @@ void qib_rc_error(struct qib_qp *qp, enum ib_wc_status err)
 	int lastwqe;
 
 	spin_lock_irqsave(&qp->s_lock, flags);
-	lastwqe = qib_error_qp(qp, err);
+	lastwqe = rvt_error_qp(qp, err);
 	spin_unlock_irqrestore(&qp->s_lock, flags);
 
 	if (lastwqe) {
@@ -2025,7 +2025,7 @@ send_last:
 		if (unlikely(wc.byte_len > qp->r_len))
 			goto nack_inv;
 		qib_copy_sge(&qp->r_sge, data, tlen, 1);
-		qib_put_ss(&qp->r_sge);
+		rvt_put_ss(&qp->r_sge);
 		qp->r_msn++;
 		if (!test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags))
 			break;
diff --git a/drivers/infiniband/hw/qib/qib_ruc.c b/drivers/infiniband/hw/qib/qib_ruc.c
index e9132f7a68b0..8bb902c96915 100644
--- a/drivers/infiniband/hw/qib/qib_ruc.c
+++ b/drivers/infiniband/hw/qib/qib_ruc.c
@@ -174,7 +174,7 @@ int qib_get_rwqe(struct qib_qp *qp, int wr_id_only)
 	}
 	/* Make sure entry is read after head index is read. */
 	smp_rmb();
-	wqe = get_rwqe_ptr(rq, tail);
+	wqe = rvt_get_rwqe_ptr(rq, tail);
 	/*
 	 * Even though we update the tail index in memory, the verbs
 	 * consumer is not supposed to post more entries until a
@@ -543,7 +543,7 @@ again:
 		sqp->s_len -= len;
 	}
 	if (release)
-		qib_put_ss(&qp->r_sge);
+		rvt_put_ss(&qp->r_sge);
 
 	if (!test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags))
 		goto send_comp;
@@ -618,7 +618,7 @@ serr:
 	spin_lock_irqsave(&sqp->s_lock, flags);
 	qib_send_complete(sqp, wqe, send_status);
 	if (sqp->ibqp.qp_type == IB_QPT_RC) {
-		int lastwqe = qib_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
+		int lastwqe = rvt_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
 
 		sqp->s_flags &= ~QIB_S_BUSY;
 		spin_unlock_irqrestore(&sqp->s_lock, flags);
* Unmerged path drivers/infiniband/hw/qib/qib_sdma.c
* Unmerged path drivers/infiniband/hw/qib/qib_uc.c
* Unmerged path drivers/infiniband/hw/qib/qib_ud.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.c
* Unmerged path drivers/infiniband/hw/qib/qib_verbs.h
