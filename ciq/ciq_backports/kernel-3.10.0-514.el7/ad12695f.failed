ksm: add cond_resched() to the rmap_walks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Andrea Arcangeli <aarcange@redhat.com>
commit ad12695f177c3403a64348b42718faf9727fe358
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/ad12695f.failed

While at it add it to the file and anon walks too.

	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Acked-by: Hugh Dickins <hughd@google.com>
	Cc: Petr Holasek <pholasek@redhat.com>
	Acked-by: Davidlohr Bueso <dbueso@suse.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ad12695f177c3403a64348b42718faf9727fe358)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/rmap.c
diff --cc mm/rmap.c
index aa83980b82fe,78a692827a63..000000000000
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@@ -1698,9 -1608,17 +1698,19 @@@ static int rmap_walk_anon(struct page *
  	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
  		struct vm_area_struct *vma = avc->vma;
  		unsigned long address = vma_address(page, vma);
++<<<<<<< HEAD
 +		ret = rmap_one(page, vma, address, arg);
++=======
+ 
+ 		cond_resched();
+ 
+ 		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
+ 			continue;
+ 
+ 		ret = rwc->rmap_one(page, vma, address, rwc->arg);
++>>>>>>> ad12695f177c (ksm: add cond_resched() to the rmap_walks)
  		if (ret != SWAP_AGAIN)
  			break;
 -		if (rwc->done && rwc->done(page))
 -			break;
  	}
  	anon_vma_unlock_read(anon_vma);
  	return ret;
@@@ -1714,21 -1644,36 +1724,31 @@@ static int rmap_walk_file(struct page *
  	struct vm_area_struct *vma;
  	int ret = SWAP_AGAIN;
  
 -	/*
 -	 * The page lock not only makes sure that page->mapping cannot
 -	 * suddenly be NULLified by truncation, it makes sure that the
 -	 * structure at mapping cannot be freed and reused yet,
 -	 * so we can safely take mapping->i_mmap_rwsem.
 -	 */
 -	VM_BUG_ON_PAGE(!PageLocked(page), page);
 -
  	if (!mapping)
  		return ret;
 -
 -	pgoff = page_to_pgoff(page);
 -	i_mmap_lock_read(mapping);
 +	mutex_lock(&mapping->i_mmap_mutex);
  	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
  		unsigned long address = vma_address(page, vma);
++<<<<<<< HEAD
 +		ret = rmap_one(page, vma, address, arg);
++=======
+ 
+ 		cond_resched();
+ 
+ 		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
+ 			continue;
+ 
+ 		ret = rwc->rmap_one(page, vma, address, rwc->arg);
++>>>>>>> ad12695f177c (ksm: add cond_resched() to the rmap_walks)
  		if (ret != SWAP_AGAIN)
 -			goto done;
 -		if (rwc->done && rwc->done(page))
 -			goto done;
 +			break;
  	}
 -
 -done:
 -	i_mmap_unlock_read(mapping);
 +	/*
 +	 * No nonlinear handling: being always shared, nonlinear vmas
 +	 * never contain migration ptes.  Decide what to do about this
 +	 * limitation to linear when we need rmap_walk() on nonlinear.
 +	 */
 +	mutex_unlock(&mapping->i_mmap_mutex);
  	return ret;
  }
  
diff --git a/mm/ksm.c b/mm/ksm.c
index 7ee5b1c104e0..d0bcf65ca4b9 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -2022,9 +2022,11 @@ again:
 		struct anon_vma_chain *vmac;
 		struct vm_area_struct *vma;
 
+		cond_resched();
 		anon_vma_lock_read(anon_vma);
 		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
 					       0, ULONG_MAX) {
+			cond_resched();
 			vma = vmac->vma;
 			if (rmap_item->address < vma->vm_start ||
 			    rmap_item->address >= vma->vm_end)
* Unmerged path mm/rmap.c
