sched/dl: Use dl_bw_of() under rcu_read_lock_sched()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [kernel] sched/deadline: Use dl_bw_of() under rcu_read_lock_sched() (Xunlei Pang) [1298387]
Rebuild_FUZZ: 94.55%
commit-author Kirill Tkhai <ktkhai@parallels.com>
commit f10e00f4bf360c36edbe6bf18a6c75b171cbe012
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/f10e00f4.failed

rq->rd is freed using call_rcu_sched(), so rcu_read_lock() to access it
is not enough. We should use either rcu_read_lock_sched() or preempt_disable().

	Reported-by: Sasha Levin <sasha.levin@oracle.com>
	Suggested-by: Peter Zijlstra <peterz@infradead.org>
	Signed-off-by: Kirill Tkhai <ktkhai@parallels.com
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Fixes: 66339c31bc39 "sched: Use dl_bw_of() under RCU read lock"
Link: http://lkml.kernel.org/r/1412065417.20287.24.camel@tkhai
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit f10e00f4bf360c36edbe6bf18a6c75b171cbe012)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index f8654b1100de,c84bdc098656..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -5616,13 -5262,36 +5616,42 @@@ static int sched_cpu_active(struct noti
  static int sched_cpu_inactive(struct notifier_block *nfb,
  					unsigned long action, void *hcpu)
  {
++<<<<<<< HEAD
 +	switch (action & ~CPU_TASKS_FROZEN) {
 +	case CPU_DOWN_PREPARE:
 +		set_cpu_active((long)hcpu, false);
++=======
+ 	unsigned long flags;
+ 	long cpu = (long)hcpu;
+ 	struct dl_bw *dl_b;
+ 
+ 	switch (action & ~CPU_TASKS_FROZEN) {
+ 	case CPU_DOWN_PREPARE:
+ 		set_cpu_active(cpu, false);
+ 
+ 		/* explicitly allow suspend */
+ 		if (!(action & CPU_TASKS_FROZEN)) {
+ 			bool overflow;
+ 			int cpus;
+ 
+ 			rcu_read_lock_sched();
+ 			dl_b = dl_bw_of(cpu);
+ 
+ 			raw_spin_lock_irqsave(&dl_b->lock, flags);
+ 			cpus = dl_bw_cpus(cpu);
+ 			overflow = __dl_overflow(dl_b, cpus, 0, 0);
+ 			raw_spin_unlock_irqrestore(&dl_b->lock, flags);
+ 
+ 			rcu_read_unlock_sched();
+ 
+ 			if (overflow)
+ 				return notifier_from_errno(-EBUSY);
+ 		}
++>>>>>>> f10e00f4bf36 (sched/dl: Use dl_bw_of() under rcu_read_lock_sched())
  		return NOTIFY_OK;
 +	default:
 +		return NOTIFY_DONE;
  	}
 -
 -	return NOTIFY_DONE;
  }
  
  static int __init migration_init(void)
@@@ -7988,11 -7643,93 +8017,91 @@@ static int sched_rt_global_constraints(
  	}
  	raw_spin_unlock_irqrestore(&def_rt_bandwidth.rt_runtime_lock, flags);
  
++<<<<<<< HEAD
++=======
+ 	return ret;
+ }
+ #endif /* CONFIG_RT_GROUP_SCHED */
+ 
+ static int sched_dl_global_constraints(void)
+ {
+ 	u64 runtime = global_rt_runtime();
+ 	u64 period = global_rt_period();
+ 	u64 new_bw = to_ratio(period, runtime);
+ 	struct dl_bw *dl_b;
+ 	int cpu, ret = 0;
+ 	unsigned long flags;
+ 
+ 	/*
+ 	 * Here we want to check the bandwidth not being set to some
+ 	 * value smaller than the currently allocated bandwidth in
+ 	 * any of the root_domains.
+ 	 *
+ 	 * FIXME: Cycling on all the CPUs is overdoing, but simpler than
+ 	 * cycling on root_domains... Discussion on different/better
+ 	 * solutions is welcome!
+ 	 */
+ 	for_each_possible_cpu(cpu) {
+ 		rcu_read_lock_sched();
+ 		dl_b = dl_bw_of(cpu);
+ 
+ 		raw_spin_lock_irqsave(&dl_b->lock, flags);
+ 		if (new_bw < dl_b->total_bw)
+ 			ret = -EBUSY;
+ 		raw_spin_unlock_irqrestore(&dl_b->lock, flags);
+ 
+ 		rcu_read_unlock_sched();
+ 
+ 		if (ret)
+ 			break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void sched_dl_do_global(void)
+ {
+ 	u64 new_bw = -1;
+ 	struct dl_bw *dl_b;
+ 	int cpu;
+ 	unsigned long flags;
+ 
+ 	def_dl_bandwidth.dl_period = global_rt_period();
+ 	def_dl_bandwidth.dl_runtime = global_rt_runtime();
+ 
+ 	if (global_rt_runtime() != RUNTIME_INF)
+ 		new_bw = to_ratio(global_rt_period(), global_rt_runtime());
+ 
+ 	/*
+ 	 * FIXME: As above...
+ 	 */
+ 	for_each_possible_cpu(cpu) {
+ 		rcu_read_lock_sched();
+ 		dl_b = dl_bw_of(cpu);
+ 
+ 		raw_spin_lock_irqsave(&dl_b->lock, flags);
+ 		dl_b->bw = new_bw;
+ 		raw_spin_unlock_irqrestore(&dl_b->lock, flags);
+ 
+ 		rcu_read_unlock_sched();
+ 	}
+ }
+ 
+ static int sched_rt_global_validate(void)
+ {
+ 	if (sysctl_sched_rt_period <= 0)
+ 		return -EINVAL;
+ 
+ 	if ((sysctl_sched_rt_runtime != RUNTIME_INF) &&
+ 		(sysctl_sched_rt_runtime > sysctl_sched_rt_period))
+ 		return -EINVAL;
+ 
++>>>>>>> f10e00f4bf36 (sched/dl: Use dl_bw_of() under rcu_read_lock_sched())
  	return 0;
  }
 +#endif /* CONFIG_RT_GROUP_SCHED */
  
 -static void sched_rt_do_global(void)
 -{
 -	def_rt_bandwidth.rt_runtime = global_rt_runtime();
 -	def_rt_bandwidth.rt_period = ns_to_ktime(global_rt_period());
 -}
 -
 -int sched_rt_handler(struct ctl_table *table, int write,
 +int sched_rr_handler(struct ctl_table *table, int write,
  		void __user *buffer, size_t *lenp,
  		loff_t *ppos)
  {
* Unmerged path kernel/sched/core.c
