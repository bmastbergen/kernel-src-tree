net: Always untag vlan-tagged traffic on input.

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [net] Always untag vlan-tagged traffic on input (Lance Richardson) [1247264]
Rebuild_FUZZ: 93.18%
commit-author Vlad Yasevich <vyasevic@redhat.com>
commit 0d5501c1c828fb97d02af50aa9d2b1a5498b94e4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/0d5501c1.failed

Currently the functionality to untag traffic on input resides
as part of the vlan module and is build only when VLAN support
is enabled in the kernel.  When VLAN is disabled, the function
vlan_untag() turns into a stub and doesn't really untag the
packets.  This seems to create an interesting interaction
between VMs supporting checksum offloading and some network drivers.

There are some drivers that do not allow the user to change
tx-vlan-offload feature of the driver.  These drivers also seem
to assume that any VLAN-tagged traffic they transmit will
have the vlan information in the vlan_tci and not in the vlan
header already in the skb.  When transmitting skbs that already
have tagged data with partial checksum set, the checksum doesn't
appear to be updated correctly by the card thus resulting in a
failure to establish TCP connections.

The following is a packet trace taken on the receiver where a
sender is a VM with a VLAN configued.  The host VM is running on
doest not have VLAN support and the outging interface on the
host is tg3:
10:12:43.503055 52:54:00:ae:42:3f > 28:d2:44:7d:c2:de, ethertype 802.1Q
(0x8100), length 78: vlan 100, p 0, ethertype IPv4, (tos 0x0, ttl 64, id 27243,
offset 0, flags [DF], proto TCP (6), length 60)
    10.0.100.1.58545 > 10.0.100.10.ircu-2: Flags [S], cksum 0xdc39 (incorrect
-> 0x48d9), seq 1069378582, win 29200, options [mss 1460,sackOK,TS val
4294837885 ecr 0,nop,wscale 7], length 0
10:12:44.505556 52:54:00:ae:42:3f > 28:d2:44:7d:c2:de, ethertype 802.1Q
(0x8100), length 78: vlan 100, p 0, ethertype IPv4, (tos 0x0, ttl 64, id 27244,
offset 0, flags [DF], proto TCP (6), length 60)
    10.0.100.1.58545 > 10.0.100.10.ircu-2: Flags [S], cksum 0xdc39 (incorrect
-> 0x44ee), seq 1069378582, win 29200, options [mss 1460,sackOK,TS val
4294838888 ecr 0,nop,wscale 7], length 0

This connection finally times out.

I've only access to the TG3 hardware in this configuration thus have
only tested this with TG3 driver.  There are a lot of other drivers
that do not permit user changes to vlan acceleration features, and
I don't know if they all suffere from a similar issue.

The patch attempt to fix this another way.  It moves the vlan header
stipping code out of the vlan module and always builds it into the
kernel network core.  This way, even if vlan is not supported on
a virtualizatoin host, the virtual machines running on top of such
host will still work with VLANs enabled.

CC: Patrick McHardy <kaber@trash.net>
CC: Nithin Nayak Sujir <nsujir@broadcom.com>
CC: Michael Chan <mchan@broadcom.com>
CC: Jiri Pirko <jiri@resnulli.us>
	Signed-off-by: Vladislav Yasevich <vyasevic@redhat.com>
	Acked-by: Jiri Pirko <jiri@resnulli.us>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 0d5501c1c828fb97d02af50aa9d2b1a5498b94e4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/skbuff.h
#	net/8021q/vlan_core.c
#	net/core/skbuff.c
diff --cc include/linux/skbuff.h
index 89bcfe8d4b01,abde271c18ae..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -2702,9 -2555,7 +2702,13 @@@ int skb_shift(struct sk_buff *tgt, stru
  void skb_scrub_packet(struct sk_buff *skb, bool xnet);
  unsigned int skb_gso_transport_seglen(const struct sk_buff *skb);
  struct sk_buff *skb_segment(struct sk_buff *skb, netdev_features_t features);
++<<<<<<< HEAD
 +int skb_ensure_writable(struct sk_buff *skb, int write_len);
 +int skb_vlan_pop(struct sk_buff *skb);
 +int skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci);
++=======
+ struct sk_buff *skb_vlan_untag(struct sk_buff *skb);
++>>>>>>> 0d5501c1c828 (net: Always untag vlan-tagged traffic on input.)
  
  struct skb_checksum_ops {
  	__wsum (*update)(const void *mem, int len, __wsum wsum);
diff --cc net/8021q/vlan_core.c
index 1fe4f0fb04d1,90cc2bdd4064..000000000000
--- a/net/8021q/vlan_core.c
+++ b/net/8021q/vlan_core.c
@@@ -112,56 -112,6 +112,59 @@@ __be16 vlan_dev_vlan_proto(const struc
  }
  EXPORT_SYMBOL(vlan_dev_vlan_proto);
  
++<<<<<<< HEAD
 +static struct sk_buff *vlan_reorder_header(struct sk_buff *skb)
 +{
 +	if (skb_cow(skb, skb_headroom(skb)) < 0)
 +		return NULL;
 +	memmove(skb->data - ETH_HLEN, skb->data - VLAN_ETH_HLEN, 2 * ETH_ALEN);
 +	skb->mac_header += VLAN_HLEN;
 +	return skb;
 +}
 +
 +struct sk_buff *vlan_untag(struct sk_buff *skb)
 +{
 +	struct vlan_hdr *vhdr;
 +	u16 vlan_tci;
 +
 +	if (unlikely(skb_vlan_tag_present(skb))) {
 +		/* vlan_tci is already set-up so leave this for another time */
 +		return skb;
 +	}
 +
 +	skb = skb_share_check(skb, GFP_ATOMIC);
 +	if (unlikely(!skb))
 +		goto err_free;
 +
 +	if (unlikely(!pskb_may_pull(skb, VLAN_HLEN)))
 +		goto err_free;
 +
 +	vhdr = (struct vlan_hdr *) skb->data;
 +	vlan_tci = ntohs(vhdr->h_vlan_TCI);
 +	__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);
 +
 +	skb_pull_rcsum(skb, VLAN_HLEN);
 +	vlan_set_encap_proto(skb, vhdr);
 +
 +	skb = vlan_reorder_header(skb);
 +	if (unlikely(!skb))
 +		goto err_free;
 +
 +	skb_reset_network_header(skb);
 +	skb_reset_transport_header(skb);
 +	skb_reset_mac_len(skb);
 +
 +	return skb;
 +
 +err_free:
 +	kfree_skb(skb);
 +	return NULL;
 +}
 +EXPORT_SYMBOL(vlan_untag);
 +
 +
++=======
++>>>>>>> 0d5501c1c828 (net: Always untag vlan-tagged traffic on input.)
  /*
   * vlan info and vid list
   */
diff --cc net/core/skbuff.c
index 4b9a3369cb53,163b673f9e62..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -3865,109 -3975,54 +3865,163 @@@ unsigned int skb_gso_transport_seglen(c
  }
  EXPORT_SYMBOL_GPL(skb_gso_transport_seglen);
  
++<<<<<<< HEAD
 +int skb_ensure_writable(struct sk_buff *skb, int write_len)
 +{
 +	if (!pskb_may_pull(skb, write_len))
 +		return -ENOMEM;
 +
 +	if (!skb_cloned(skb) || skb_clone_writable(skb, write_len))
 +		return 0;
 +
 +	return pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
 +}
 +EXPORT_SYMBOL(skb_ensure_writable);
 +
 +/* remove VLAN header from packet and update csum accordingly. */
 +static int __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci)
 +{
 +	struct vlan_hdr *vhdr;
 +	unsigned int offset = skb->data - skb_mac_header(skb);
 +	int err;
 +
 +	__skb_push(skb, offset);
 +	err = skb_ensure_writable(skb, VLAN_ETH_HLEN);
 +	if (unlikely(err))
 +		goto pull;
 +
 +	skb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);
 +
 +	vhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);
 +	*vlan_tci = ntohs(vhdr->h_vlan_TCI);
 +
 +	memmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);
 +	__skb_pull(skb, VLAN_HLEN);
 +
 +	vlan_set_encap_proto(skb, vhdr);
 +	skb->mac_header += VLAN_HLEN;
 +
 +	if (skb_network_offset(skb) < ETH_HLEN)
 +		skb_set_network_header(skb, ETH_HLEN);
 +
 +	skb_reset_mac_len(skb);
 +pull:
 +	__skb_pull(skb, offset);
 +
 +	return err;
 +}
 +
 +int skb_vlan_pop(struct sk_buff *skb)
 +{
 +	u16 vlan_tci;
 +	__be16 vlan_proto;
 +	int err;
 +
 +	if (likely(skb_vlan_tag_present(skb))) {
 +		skb->vlan_tci = 0;
 +	} else {
 +		if (unlikely((skb->protocol != htons(ETH_P_8021Q) &&
 +			      skb->protocol != htons(ETH_P_8021AD)) ||
 +			     skb->len < VLAN_ETH_HLEN))
 +			return 0;
 +
 +		err = __skb_vlan_pop(skb, &vlan_tci);
 +		if (err)
 +			return err;
 +	}
 +	/* move next vlan tag to hw accel tag */
 +	if (likely((skb->protocol != htons(ETH_P_8021Q) &&
 +		    skb->protocol != htons(ETH_P_8021AD)) ||
 +		   skb->len < VLAN_ETH_HLEN))
 +		return 0;
 +
 +	vlan_proto = skb->protocol;
 +	err = __skb_vlan_pop(skb, &vlan_tci);
 +	if (unlikely(err))
 +		return err;
 +
 +	__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);
 +	return 0;
 +}
 +EXPORT_SYMBOL(skb_vlan_pop);
 +
 +int skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci)
 +{
 +	if (skb_vlan_tag_present(skb)) {
 +		unsigned int offset = skb->data - skb_mac_header(skb);
 +		int err;
 +
 +		/* __vlan_insert_tag expect skb->data pointing to mac header.
 +		 * So change skb->data before calling it and change back to
 +		 * original position later
 +		 */
 +		__skb_push(skb, offset);
 +		err = __vlan_insert_tag(skb, skb->vlan_proto,
 +					skb_vlan_tag_get(skb));
 +		if (err)
 +			return err;
 +		skb->protocol = skb->vlan_proto;
 +		skb->mac_len += VLAN_HLEN;
 +		__skb_pull(skb, offset);
 +
 +		if (skb->ip_summed == CHECKSUM_COMPLETE)
 +			skb->csum = csum_add(skb->csum, csum_partial(skb->data
 +					+ (2 * ETH_ALEN), VLAN_HLEN, 0));
 +	}
 +	__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);
 +	return 0;
 +}
 +EXPORT_SYMBOL(skb_vlan_push);
++=======
+ static struct sk_buff *skb_reorder_vlan_header(struct sk_buff *skb)
+ {
+ 	if (skb_cow(skb, skb_headroom(skb)) < 0) {
+ 		kfree_skb(skb);
+ 		return NULL;
+ 	}
+ 
+ 	memmove(skb->data - ETH_HLEN, skb->data - VLAN_ETH_HLEN, 2 * ETH_ALEN);
+ 	skb->mac_header += VLAN_HLEN;
+ 	return skb;
+ }
+ 
+ struct sk_buff *skb_vlan_untag(struct sk_buff *skb)
+ {
+ 	struct vlan_hdr *vhdr;
+ 	u16 vlan_tci;
+ 
+ 	if (unlikely(vlan_tx_tag_present(skb))) {
+ 		/* vlan_tci is already set-up so leave this for another time */
+ 		return skb;
+ 	}
+ 
+ 	skb = skb_share_check(skb, GFP_ATOMIC);
+ 	if (unlikely(!skb))
+ 		goto err_free;
+ 
+ 	if (unlikely(!pskb_may_pull(skb, VLAN_HLEN)))
+ 		goto err_free;
+ 
+ 	vhdr = (struct vlan_hdr *)skb->data;
+ 	vlan_tci = ntohs(vhdr->h_vlan_TCI);
+ 	__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);
+ 
+ 	skb_pull_rcsum(skb, VLAN_HLEN);
+ 	vlan_set_encap_proto(skb, vhdr);
+ 
+ 	skb = skb_reorder_vlan_header(skb);
+ 	if (unlikely(!skb))
+ 		goto err_free;
+ 
+ 	skb_reset_network_header(skb);
+ 	skb_reset_transport_header(skb);
+ 	skb_reset_mac_len(skb);
+ 
+ 	return skb;
+ 
+ err_free:
+ 	kfree_skb(skb);
+ 	return NULL;
+ }
+ EXPORT_SYMBOL(skb_vlan_untag);
++>>>>>>> 0d5501c1c828 (net: Always untag vlan-tagged traffic on input.)
diff --git a/include/linux/if_vlan.h b/include/linux/if_vlan.h
index 089213ca80b7..27f8f3cdf6ec 100644
--- a/include/linux/if_vlan.h
+++ b/include/linux/if_vlan.h
@@ -186,7 +186,6 @@ vlan_dev_get_egress_qos_mask(struct net_device *dev, u32 skprio)
 }
 
 extern bool vlan_do_receive(struct sk_buff **skb);
-extern struct sk_buff *vlan_untag(struct sk_buff *skb);
 
 extern int vlan_vid_add(struct net_device *dev, __be16 proto, u16 vid);
 extern void vlan_vid_del(struct net_device *dev, __be16 proto, u16 vid);
@@ -234,11 +233,6 @@ static inline bool vlan_do_receive(struct sk_buff **skb)
 	return false;
 }
 
-static inline struct sk_buff *vlan_untag(struct sk_buff *skb)
-{
-	return skb;
-}
-
 static inline int vlan_vid_add(struct net_device *dev, __be16 proto, u16 vid)
 {
 	return 0;
* Unmerged path include/linux/skbuff.h
* Unmerged path net/8021q/vlan_core.c
diff --git a/net/bridge/br_vlan.c b/net/bridge/br_vlan.c
index b08164360340..a07ee96dfd77 100644
--- a/net/bridge/br_vlan.c
+++ b/net/bridge/br_vlan.c
@@ -187,7 +187,7 @@ bool br_allowed_ingress(struct net_bridge *br, struct net_port_vlans *v,
 	 */
 	if (unlikely(!skb_vlan_tag_present(skb) &&
 		     skb->protocol == proto)) {
-		skb = vlan_untag(skb);
+		skb = skb_vlan_untag(skb);
 		if (unlikely(!skb))
 			return false;
 	}
diff --git a/net/core/dev.c b/net/core/dev.c
index f3c2ad673f66..385ac56dcb63 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -3475,7 +3475,7 @@ another_round:
 
 	if (skb->protocol == cpu_to_be16(ETH_P_8021Q) ||
 	    skb->protocol == cpu_to_be16(ETH_P_8021AD)) {
-		skb = vlan_untag(skb);
+		skb = skb_vlan_untag(skb);
 		if (unlikely(!skb))
 			goto out;
 	}
* Unmerged path net/core/skbuff.c
