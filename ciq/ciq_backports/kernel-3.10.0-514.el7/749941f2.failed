nvme: only ignore hardware errors in nvme_create_io_queues

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 749941f2365db8198b5d75c83a575ee6e55bf03b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/749941f2.failed

Half initialized queues due to kernel error returns or timeout are still a
good reason to give up on initializing a controller.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Acked-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 749941f2365db8198b5d75c83a575ee6e55bf03b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index 2a6eb55ad96c,1f92b328522a..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -2105,117 -1537,85 +2105,143 @@@ static int nvme_kthread(void *data
  	return 0;
  }
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +static void nvme_alloc_ns(struct nvme_dev *dev, unsigned nsid)
 +{
 +	struct nvme_ns *ns;
 +	struct gendisk *disk;
 +	int node = dev_to_node(&dev->pci_dev->dev);
 +
 +	ns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);
 +	if (!ns)
 +		return;
 +
 +	ns->queue = blk_mq_init_queue(&dev->tagset);
 +	if (IS_ERR(ns->queue))
 +		goto out_free_ns;
 +	queue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, ns->queue);
 +	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, ns->queue);
 +	queue_flag_set_unlocked(QUEUE_FLAG_SG_GAPS, ns->queue);
 +	ns->dev = dev;
 +	ns->queue->queuedata = ns;
 +
 +	disk = alloc_disk_node(0, node);
 +	if (!disk)
 +		goto out_free_queue;
 +
 +	kref_init(&ns->kref);
 +	ns->ns_id = nsid;
 +	ns->disk = disk;
 +	ns->lba_shift = 9; /* set to a default value for 512 until disk is validated */
 +	list_add_tail(&ns->list, &dev->namespaces);
 +
 +	blk_queue_logical_block_size(ns->queue, 1 << ns->lba_shift);
 +	if (dev->max_hw_sectors) {
 +		blk_queue_max_hw_sectors(ns->queue, dev->max_hw_sectors);
 +		blk_queue_max_segments(ns->queue,
 +			(dev->max_hw_sectors / (dev->page_size >> 9)) + 1);
 +	}
 +	if (dev->stripe_size)
 +		blk_queue_chunk_sectors(ns->queue, dev->stripe_size >> 9);
 +	if (dev->vwc & NVME_CTRL_VWC_PRESENT)
 +		blk_queue_flush(ns->queue, REQ_FLUSH | REQ_FUA);
 +
 +	disk->major = nvme_major;
 +	disk->first_minor = 0;
 +	disk->fops = &nvme_fops;
 +	disk->private_data = ns;
 +	disk->queue = ns->queue;
 +	disk->driverfs_dev = dev->device;
 +	disk->flags = GENHD_FL_EXT_DEVT;
 +	sprintf(disk->disk_name, "nvme%dn%d", dev->instance, nsid);
 +
 +	/*
 +	 * Initialize capacity to 0 until we establish the namespace format and
 +	 * setup integrity extentions if necessary. The revalidate_disk after
 +	 * add_disk allows the driver to register with integrity if the format
 +	 * requires it.
 +	 */
 +	set_capacity(disk, 0);
 +	if (nvme_revalidate_disk(ns->disk))
 +		goto out_free_disk;
 +
 +	kref_get(&dev->kref);
 +	add_disk(ns->disk);
 +	if (ns->ms) {
 +		struct block_device *bd = bdget_disk(ns->disk, 0);
 +		if (!bd)
 +			return;
 +		if (blkdev_get(bd, FMODE_READ, NULL)) {
 +			bdput(bd);
 +			return;
 +		}
 +		blkdev_reread_part(bd);
 +		blkdev_put(bd, FMODE_READ);
 +	}
 +	return;
 + out_free_disk:
 +	kfree(disk);
 +	list_del(&ns->list);
 + out_free_queue:
 +	blk_cleanup_queue(ns->queue);
 + out_free_ns:
 +	kfree(ns);
 +}
 +
 +static void nvme_create_io_queues(struct nvme_dev *dev)
++=======
+ static int nvme_create_io_queues(struct nvme_dev *dev)
++>>>>>>> 749941f2365d (nvme: only ignore hardware errors in nvme_create_io_queues):drivers/nvme/host/pci.c
  {
  	unsigned i;
+ 	int ret = 0;
  
- 	for (i = dev->queue_count; i <= dev->max_qid; i++)
- 		if (!nvme_alloc_queue(dev, i, dev->q_depth))
+ 	for (i = dev->queue_count; i <= dev->max_qid; i++) {
+ 		if (!nvme_alloc_queue(dev, i, dev->q_depth)) {
+ 			ret = -ENOMEM;
  			break;
+ 		}
+ 	}
  
++<<<<<<< HEAD:drivers/block/nvme-core.c
 +	for (i = dev->online_queues; i <= dev->queue_count - 1; i++)
 +		if (nvme_create_queue(dev->queues[i], i))
 +			break;
++=======
+ 	for (i = dev->online_queues; i <= dev->queue_count - 1; i++) {
+ 		ret = nvme_create_queue(dev->queues[i], i);
+ 		if (ret) {
+ 			nvme_free_queues(dev, i);
+ 			break;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Ignore failing Create SQ/CQ commands, we can continue with less
+ 	 * than the desired aount of queues, and even a controller without
+ 	 * I/O queues an still be used to issue admin commands.  This might
+ 	 * be useful to upgrade a buggy firmware for example.
+ 	 */
+ 	return ret >= 0 ? 0 : ret;
++>>>>>>> 749941f2365d (nvme: only ignore hardware errors in nvme_create_io_queues):drivers/nvme/host/pci.c
  }
  
 -static void __iomem *nvme_map_cmb(struct nvme_dev *dev)
 +static int set_queue_count(struct nvme_dev *dev, int count)
  {
 -	u64 szu, size, offset;
 -	u32 cmbloc;
 -	resource_size_t bar_size;
 -	struct pci_dev *pdev = to_pci_dev(dev->dev);
 -	void __iomem *cmb;
 -	dma_addr_t dma_addr;
 -
 -	if (!use_cmb_sqes)
 -		return NULL;
 -
 -	dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);
 -	if (!(NVME_CMB_SZ(dev->cmbsz)))
 -		return NULL;
 -
 -	cmbloc = readl(dev->bar + NVME_REG_CMBLOC);
 -
 -	szu = (u64)1 << (12 + 4 * NVME_CMB_SZU(dev->cmbsz));
 -	size = szu * NVME_CMB_SZ(dev->cmbsz);
 -	offset = szu * NVME_CMB_OFST(cmbloc);
 -	bar_size = pci_resource_len(pdev, NVME_CMB_BIR(cmbloc));
 -
 -	if (offset > bar_size)
 -		return NULL;
 -
 -	/*
 -	 * Controllers may support a CMB size larger than their BAR,
 -	 * for example, due to being behind a bridge. Reduce the CMB to
 -	 * the reported size of the BAR
 -	 */
 -	if (size > bar_size - offset)
 -		size = bar_size - offset;
 -
 -	dma_addr = pci_resource_start(pdev, NVME_CMB_BIR(cmbloc)) + offset;
 -	cmb = ioremap_wc(dma_addr, size);
 -	if (!cmb)
 -		return NULL;
 -
 -	dev->cmb_dma_addr = dma_addr;
 -	dev->cmb_size = size;
 -	return cmb;
 -}
 -
 -static inline void nvme_release_cmb(struct nvme_dev *dev)
 -{
 -	if (dev->cmb) {
 -		iounmap(dev->cmb);
 -		dev->cmb = NULL;
 +	int status;
 +	u32 result;
 +	u32 q_count = (count - 1) | ((count - 1) << 16);
 +
 +	status = nvme_set_features(dev, NVME_FEAT_NUM_QUEUES, q_count, 0,
 +								&result);
 +	if (status < 0)
 +		return status;
 +	if (status > 0) {
 +		dev_err(&dev->pci_dev->dev, "Could not set queue count (%d)\n",
 +									status);
 +		return 0;
  	}
 +	return min(result & 0xffff, result >> 16) + 1;
  }
  
  static size_t db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
* Unmerged path drivers/block/nvme-core.c
