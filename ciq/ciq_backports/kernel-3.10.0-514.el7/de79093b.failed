IB/hfi1: Correctly compute node interval

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
commit-author Mitko Haralanov <mitko.haralanov@intel.com>
commit de79093b284888faedb826d8ecd326e5b6843d88
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/de79093b.failed

The computation of the interval of an interval RB node
was incorrect leading to data corruption due to the RB
search algorithm not properly finding the all RB nodes
in an MMU invalidation interval.

The problem stemmed from the fact that the beginning
address of the node's range was being aligned to a page
boundary. For certain buffer sizes, this would lead to
a end address calculation that was off by 1 page.

An important aspect of keeping the RB same is also
updating the node's range in the case it's being extended.

	Reviewed-by: Dean Luick <dean.luick@intel.com>
	Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit de79093b284888faedb826d8ecd326e5b6843d88)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/staging/hfi1/user_sdma.c
#	drivers/staging/rdma/hfi1/mmu_rb.c
diff --cc drivers/staging/hfi1/user_sdma.c
index 6967deb7956a,d1645d98a43d..000000000000
--- a/drivers/staging/hfi1/user_sdma.c
+++ b/drivers/staging/hfi1/user_sdma.c
@@@ -1036,40 -1030,134 +1036,114 @@@ static inline int num_user_pages(const 
  	return 1 + ((epage - spage) >> PAGE_SHIFT);
  }
  
 -/* Caller must hold pq->evict_lock */
 -static u32 sdma_cache_evict(struct hfi1_user_sdma_pkt_q *pq, u32 npages)
 -{
 -	u32 cleared = 0;
 -	struct sdma_mmu_node *node, *ptr;
 -
 -	list_for_each_entry_safe_reverse(node, ptr, &pq->evict, list) {
 -		/* Make sure that no one is still using the node. */
 -		if (!atomic_read(&node->refcount)) {
 -			/*
 -			 * Need to use the page count now as the remove callback
 -			 * will free the node.
 -			 */
 -			cleared += node->npages;
 -			spin_unlock(&pq->evict_lock);
 -			hfi1_mmu_rb_remove(&pq->sdma_rb_root, &node->rb);
 -			spin_lock(&pq->evict_lock);
 -			if (cleared >= npages)
 -				break;
 -		}
 -	}
 -	return cleared;
 -}
 -
  static int pin_vector_pages(struct user_sdma_request *req,
  			    struct user_sdma_iovec *iovec) {
++<<<<<<< HEAD:drivers/staging/hfi1/user_sdma.c
 +	int pinned, npages;
 +
 +	npages = num_user_pages(&iovec->iov);
 +	iovec->pages = kcalloc(npages, sizeof(*iovec->pages), GFP_KERNEL);
 +	if (!iovec->pages) {
 +		SDMA_DBG(req, "Failed page array alloc");
 +		return -ENOMEM;
++=======
+ 	int ret = 0, pinned, npages, cleared;
+ 	struct page **pages;
+ 	struct hfi1_user_sdma_pkt_q *pq = req->pq;
+ 	struct sdma_mmu_node *node = NULL;
+ 	struct mmu_rb_node *rb_node;
+ 
+ 	rb_node = hfi1_mmu_rb_search(&pq->sdma_rb_root,
+ 				     (unsigned long)iovec->iov.iov_base,
+ 				     iovec->iov.iov_len);
+ 	if (rb_node && !IS_ERR(rb_node))
+ 		node = container_of(rb_node, struct sdma_mmu_node, rb);
+ 	else
+ 		rb_node = NULL;
+ 
+ 	if (!node) {
+ 		node = kzalloc(sizeof(*node), GFP_KERNEL);
+ 		if (!node)
+ 			return -ENOMEM;
+ 
+ 		node->rb.addr = (unsigned long)iovec->iov.iov_base;
+ 		node->pq = pq;
+ 		atomic_set(&node->refcount, 0);
+ 		INIT_LIST_HEAD(&node->list);
+ 	}
+ 
+ 	npages = num_user_pages(&iovec->iov);
+ 	if (node->npages < npages) {
+ 		pages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);
+ 		if (!pages) {
+ 			SDMA_DBG(req, "Failed page array alloc");
+ 			ret = -ENOMEM;
+ 			goto bail;
+ 		}
+ 		memcpy(pages, node->pages, node->npages * sizeof(*pages));
+ 
+ 		npages -= node->npages;
+ retry:
+ 		if (!hfi1_can_pin_pages(pq->dd, pq->n_locked, npages)) {
+ 			spin_lock(&pq->evict_lock);
+ 			cleared = sdma_cache_evict(pq, npages);
+ 			spin_unlock(&pq->evict_lock);
+ 			if (cleared >= npages)
+ 				goto retry;
+ 		}
+ 		pinned = hfi1_acquire_user_pages(
+ 			((unsigned long)iovec->iov.iov_base +
+ 			 (node->npages * PAGE_SIZE)), npages, 0,
+ 			pages + node->npages);
+ 		if (pinned < 0) {
+ 			kfree(pages);
+ 			ret = pinned;
+ 			goto bail;
+ 		}
+ 		if (pinned != npages) {
+ 			unpin_vector_pages(current->mm, pages, node->npages,
+ 					   pinned);
+ 			ret = -EFAULT;
+ 			goto bail;
+ 		}
+ 		kfree(node->pages);
+ 		node->rb.len = iovec->iov.iov_len;
+ 		node->pages = pages;
+ 		node->npages += pinned;
+ 		npages = node->npages;
+ 		spin_lock(&pq->evict_lock);
+ 		if (!rb_node)
+ 			list_add(&node->list, &pq->evict);
+ 		else
+ 			list_move(&node->list, &pq->evict);
+ 		pq->n_locked += pinned;
+ 		spin_unlock(&pq->evict_lock);
++>>>>>>> de79093b2848 (IB/hfi1: Correctly compute node interval):drivers/staging/rdma/hfi1/user_sdma.c
  	}
 -	iovec->pages = node->pages;
 -	iovec->npages = npages;
  
 -	if (!rb_node) {
 -		ret = hfi1_mmu_rb_insert(&req->pq->sdma_rb_root, &node->rb);
 -		if (ret) {
 -			spin_lock(&pq->evict_lock);
 -			if (!list_empty(&node->list))
 -				list_del(&node->list);
 -			pq->n_locked -= node->npages;
 -			spin_unlock(&pq->evict_lock);
 -			unpin_vector_pages(current->mm, node->pages, 0,
 -					   node->npages);
 -			goto bail;
 -		}
 -	} else {
 -		atomic_inc(&node->refcount);
 +	pinned = hfi1_acquire_user_pages((unsigned long)iovec->iov.iov_base,
 +					 npages, 0, iovec->pages);
 +
 +	if (pinned < 0)
 +		return pinned;
 +
 +	iovec->npages = pinned;
 +	if (pinned != npages) {
 +		SDMA_DBG(req, "Failed to pin pages (%d/%u)", pinned, npages);
 +		unpin_vector_pages(iovec);
 +		return -EFAULT;
  	}
  	return 0;
 -bail:
 -	if (!rb_node)
 -		kfree(node);
 -	return ret;
  }
  
 -static void unpin_vector_pages(struct mm_struct *mm, struct page **pages,
 -			       unsigned start, unsigned npages)
 +static void unpin_vector_pages(struct user_sdma_iovec *iovec)
  {
 -	hfi1_release_user_pages(mm, pages + start, npages, 0);
 -	kfree(pages);
 +	hfi1_release_user_pages(iovec->pages, iovec->npages, 0);
 +
 +	kfree(iovec->pages);
 +	iovec->pages = NULL;
 +	iovec->npages = 0;
 +	iovec->offset = 0;
  }
  
  static int check_header_template(struct user_sdma_request *req,
* Unmerged path drivers/staging/rdma/hfi1/mmu_rb.c
* Unmerged path drivers/staging/hfi1/user_sdma.c
* Unmerged path drivers/staging/rdma/hfi1/mmu_rb.c
