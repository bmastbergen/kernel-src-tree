mm: refactor do_wp_page handling of shared vma into a function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-514.el7
Rebuild_CHGLOG: - [mm] refactor do_wp_page handling of shared vma into a function (Eric Sandeen) [1274459]
Rebuild_FUZZ: 96.67%
commit-author Shachar Raindel <raindel@mellanox.com>
commit 93e478d4c36ecaf15b942988b8272102d661d44e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-514.el7/93e478d4.failed

The do_wp_page function is extremely long.  Extract the logic for
handling a page belonging to a shared vma into a function of its own.

This helps the readability of the code, without doing any functional
change in it.

	Signed-off-by: Shachar Raindel <raindel@mellanox.com>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Rik van Riel <riel@redhat.com>
	Acked-by: Andi Kleen <ak@linux.intel.com>
	Acked-by: Haggai Eran <haggaie@mellanox.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Peter Feiner <pfeiner@google.com>
	Cc: Michel Lespinasse <walken@google.com>
	Reviewed-by: Michal Hocko <mhocko@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 93e478d4c36ecaf15b942988b8272102d661d44e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index fce51319197b,ac20b2a6a0c3..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3097,10 -2179,144 +3097,149 @@@ oom
  	if (old_page)
  		page_cache_release(old_page);
  	return VM_FAULT_OOM;
 -}
  
++<<<<<<< HEAD
 +unwritable_page:
 +	page_cache_release(old_page);
 +	return ret;
++=======
+ static int wp_page_shared(struct mm_struct *mm, struct vm_area_struct *vma,
+ 			  unsigned long address, pte_t *page_table,
+ 			  pmd_t *pmd, spinlock_t *ptl, pte_t orig_pte,
+ 			  struct page *old_page)
+ 	__releases(ptl)
+ {
+ 	int page_mkwrite = 0;
+ 
+ 	page_cache_get(old_page);
+ 
+ 	/*
+ 	 * Only catch write-faults on shared writable pages,
+ 	 * read-only shared pages can get COWed by
+ 	 * get_user_pages(.write=1, .force=1).
+ 	 */
+ 	if (vma->vm_ops && vma->vm_ops->page_mkwrite) {
+ 		int tmp;
+ 
+ 		pte_unmap_unlock(page_table, ptl);
+ 		tmp = do_page_mkwrite(vma, old_page, address);
+ 		if (unlikely(!tmp || (tmp &
+ 				      (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
+ 			page_cache_release(old_page);
+ 			return tmp;
+ 		}
+ 		/*
+ 		 * Since we dropped the lock we need to revalidate
+ 		 * the PTE as someone else may have changed it.  If
+ 		 * they did, we just return, as we can count on the
+ 		 * MMU to tell us if they didn't also make it writable.
+ 		 */
+ 		page_table = pte_offset_map_lock(mm, pmd, address,
+ 						 &ptl);
+ 		if (!pte_same(*page_table, orig_pte)) {
+ 			unlock_page(old_page);
+ 			pte_unmap_unlock(page_table, ptl);
+ 			page_cache_release(old_page);
+ 			return 0;
+ 		}
+ 		page_mkwrite = 1;
+ 	}
+ 
+ 	return wp_page_reuse(mm, vma, address, page_table, ptl,
+ 			     orig_pte, old_page, page_mkwrite, 1);
+ }
+ 
+ /*
+  * This routine handles present pages, when users try to write
+  * to a shared page. It is done by copying the page to a new address
+  * and decrementing the shared-page counter for the old page.
+  *
+  * Note that this routine assumes that the protection checks have been
+  * done by the caller (the low-level page fault routine in most cases).
+  * Thus we can safely just mark it writable once we've done any necessary
+  * COW.
+  *
+  * We also mark the page dirty at this point even though the page will
+  * change only once the write actually happens. This avoids a few races,
+  * and potentially makes it more efficient.
+  *
+  * We enter with non-exclusive mmap_sem (to exclude vma changes,
+  * but allow concurrent faults), with pte both mapped and locked.
+  * We return with mmap_sem still held, but pte unmapped and unlocked.
+  */
+ static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
+ 		unsigned long address, pte_t *page_table, pmd_t *pmd,
+ 		spinlock_t *ptl, pte_t orig_pte)
+ 	__releases(ptl)
+ {
+ 	struct page *old_page;
+ 
+ 	old_page = vm_normal_page(vma, address, orig_pte);
+ 	if (!old_page) {
+ 		/*
+ 		 * VM_MIXEDMAP !pfn_valid() case, or VM_SOFTDIRTY clear on a
+ 		 * VM_PFNMAP VMA.
+ 		 *
+ 		 * We should not cow pages in a shared writeable mapping.
+ 		 * Just mark the pages writable as we can't do any dirty
+ 		 * accounting on raw pfn maps.
+ 		 */
+ 		if ((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
+ 				     (VM_WRITE|VM_SHARED))
+ 			return wp_page_reuse(mm, vma, address, page_table, ptl,
+ 					     orig_pte, old_page, 0, 0);
+ 
+ 		pte_unmap_unlock(page_table, ptl);
+ 		return wp_page_copy(mm, vma, address, page_table, pmd,
+ 				    orig_pte, old_page);
+ 	}
+ 
+ 	/*
+ 	 * Take out anonymous pages first, anonymous shared vmas are
+ 	 * not dirty accountable.
+ 	 */
+ 	if (PageAnon(old_page) && !PageKsm(old_page)) {
+ 		if (!trylock_page(old_page)) {
+ 			page_cache_get(old_page);
+ 			pte_unmap_unlock(page_table, ptl);
+ 			lock_page(old_page);
+ 			page_table = pte_offset_map_lock(mm, pmd, address,
+ 							 &ptl);
+ 			if (!pte_same(*page_table, orig_pte)) {
+ 				unlock_page(old_page);
+ 				pte_unmap_unlock(page_table, ptl);
+ 				page_cache_release(old_page);
+ 				return 0;
+ 			}
+ 			page_cache_release(old_page);
+ 		}
+ 		if (reuse_swap_page(old_page)) {
+ 			/*
+ 			 * The page is all ours.  Move it to our anon_vma so
+ 			 * the rmap code will not search our parent or siblings.
+ 			 * Protected against the rmap code by the page lock.
+ 			 */
+ 			page_move_anon_rmap(old_page, vma, address);
+ 			unlock_page(old_page);
+ 			return wp_page_reuse(mm, vma, address, page_table, ptl,
+ 					     orig_pte, old_page, 0, 0);
+ 		}
+ 		unlock_page(old_page);
+ 	} else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
+ 					(VM_WRITE|VM_SHARED))) {
+ 		return wp_page_shared(mm, vma, address, page_table, pmd,
+ 				      ptl, orig_pte, old_page);
+ 	}
+ 
+ 	/*
+ 	 * Ok, we need to copy. Oh, well..
+ 	 */
+ 	page_cache_get(old_page);
+ 
+ 	pte_unmap_unlock(page_table, ptl);
+ 	return wp_page_copy(mm, vma, address, page_table, pmd,
+ 			    orig_pte, old_page);
++>>>>>>> 93e478d4c36e (mm: refactor do_wp_page handling of shared vma into a function)
  }
  
  static void unmap_mapping_range_vma(struct vm_area_struct *vma,
* Unmerged path mm/memory.c
