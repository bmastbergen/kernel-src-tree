memcg, kmem: do not fail __GFP_NOFAIL charges

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.62.1.el7
Rebuild_CHGLOG: - mm: memcg: do not fail __GFP_NOFAIL charges (Rafael Aquini) [2054345]
Rebuild_FUZZ: 90.91%
commit-author Michal Hocko <mhocko@suse.com>
commit e55d9d9bfb69405bd7615c0f8d229d8fafb3e9b8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.62.1.el7/e55d9d9b.failed

Thomas has noticed the following NULL ptr dereference when using cgroup
v1 kmem limit:
BUG: unable to handle kernel NULL pointer dereference at 0000000000000008
PGD 0
P4D 0
Oops: 0000 [#1] PREEMPT SMP PTI
CPU: 3 PID: 16923 Comm: gtk-update-icon Not tainted 4.19.51 #42
Hardware name: Gigabyte Technology Co., Ltd. Z97X-Gaming G1/Z97X-Gaming G1, BIOS F9 07/31/2015
RIP: 0010:create_empty_buffers+0x24/0x100
Code: cd 0f 1f 44 00 00 0f 1f 44 00 00 41 54 49 89 d4 ba 01 00 00 00 55 53 48 89 fb e8 97 fe ff ff 48 89 c5 48 89 c2 eb 03 48 89 ca <48> 8b 4a 08 4c 09 22 48 85 c9 75 f1 48 89 6a 08 48 8b 43 18 48 8d
RSP: 0018:ffff927ac1b37bf8 EFLAGS: 00010286
RAX: 0000000000000000 RBX: fffff2d4429fd740 RCX: 0000000100097149
RDX: 0000000000000000 RSI: 0000000000000082 RDI: ffff9075a99fbe00
RBP: 0000000000000000 R08: fffff2d440949cc8 R09: 00000000000960c0
R10: 0000000000000002 R11: 0000000000000000 R12: 0000000000000000
R13: ffff907601f18360 R14: 0000000000002000 R15: 0000000000001000
FS:  00007fb55b288bc0(0000) GS:ffff90761f8c0000(0000) knlGS:0000000000000000
CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
CR2: 0000000000000008 CR3: 000000007aebc002 CR4: 00000000001606e0
Call Trace:
 create_page_buffers+0x4d/0x60
 __block_write_begin_int+0x8e/0x5a0
 ? ext4_inode_attach_jinode.part.82+0xb0/0xb0
 ? jbd2__journal_start+0xd7/0x1f0
 ext4_da_write_begin+0x112/0x3d0
 generic_perform_write+0xf1/0x1b0
 ? file_update_time+0x70/0x140
 __generic_file_write_iter+0x141/0x1a0
 ext4_file_write_iter+0xef/0x3b0
 __vfs_write+0x17e/0x1e0
 vfs_write+0xa5/0x1a0
 ksys_write+0x57/0xd0
 do_syscall_64+0x55/0x160
 entry_SYSCALL_64_after_hwframe+0x44/0xa9

Tetsuo then noticed that this is because the __memcg_kmem_charge_memcg
fails __GFP_NOFAIL charge when the kmem limit is reached.  This is a wrong
behavior because nofail allocations are not allowed to fail.  Normal
charge path simply forces the charge even if that means to cross the
limit.  Kmem accounting should be doing the same.

Link: http://lkml.kernel.org/r/20190906125608.32129-1-mhocko@kernel.org
	Signed-off-by: Michal Hocko <mhocko@suse.com>
	Reported-by: Thomas Lindroth <thomas.lindroth@gmail.com>
	Debugged-by: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Thomas Lindroth <thomas.lindroth@gmail.com>
	Cc: Shakeel Butt <shakeelb@google.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e55d9d9bfb69405bd7615c0f8d229d8fafb3e9b8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index 3c799fa22ce9,c313c49074ca..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -3479,500 -3225,353 +3479,539 @@@ static void memcg_create_cache_work_fun
  }
  
  /*
 - * Test whether @memcg has children, dead or alive.  Note that this
 - * function doesn't care whether @memcg has use_hierarchy enabled and
 - * returns %true if there are child csses according to the cgroup
 - * hierarchy.  Testing use_hierarchy is the caller's responsiblity.
 + * Enqueue the creation of a per-memcg kmem_cache.
   */
 -static inline bool memcg_has_children(struct mem_cgroup *memcg)
 +static void __memcg_create_cache_enqueue(struct mem_cgroup *memcg,
 +					 struct kmem_cache *cachep)
  {
 -	bool ret;
 +	struct create_work *cw;
  
 -	rcu_read_lock();
 -	ret = css_next_child(NULL, &memcg->css);
 -	rcu_read_unlock();
 -	return ret;
 +	cw = kmalloc(sizeof(struct create_work), GFP_NOWAIT);
 +	if (cw == NULL) {
 +		css_put(&memcg->css);
 +		return;
 +	}
 +
 +	cw->memcg = memcg;
 +	cw->cachep = cachep;
 +
 +	INIT_WORK(&cw->work, memcg_create_cache_work_func);
 +	schedule_work(&cw->work);
  }
  
 +static void memcg_create_cache_enqueue(struct mem_cgroup *memcg,
 +				       struct kmem_cache *cachep)
 +{
 +	/*
 +	 * We need to stop accounting when we kmalloc, because if the
 +	 * corresponding kmalloc cache is not yet created, the first allocation
 +	 * in __memcg_create_cache_enqueue will recurse.
 +	 *
 +	 * However, it is better to enclose the whole function. Depending on
 +	 * the debugging options enabled, INIT_WORK(), for instance, can
 +	 * trigger an allocation. This too, will make us recurse. Because at
 +	 * this point we can't allow ourselves back into memcg_kmem_get_cache,
 +	 * the safest choice is to do it like this, wrapping the whole function.
 +	 */
 +	memcg_stop_kmem_account();
 +	__memcg_create_cache_enqueue(memcg, cachep);
 +	memcg_resume_kmem_account();
 +}
  /*
 - * Reclaims as many pages from the given memcg as possible.
 + * Return the kmem_cache we're supposed to use for a slab allocation.
 + * We try to use the current memcg's version of the cache.
   *
 - * Caller is responsible for holding css reference for memcg.
 + * If the cache does not exist yet, if we are the first user of it,
 + * we either create it immediately, if possible, or create it asynchronously
 + * in a workqueue.
 + * In the latter case, we will let the current allocation go through with
 + * the original cache.
 + *
 + * Can't be called in interrupt context or from kernel threads.
 + * This function needs to be called with rcu_read_lock() held.
   */
 -static int mem_cgroup_force_empty(struct mem_cgroup *memcg)
 +struct kmem_cache *__memcg_kmem_get_cache(struct kmem_cache *cachep,
 +					  gfp_t gfp)
  {
 -	int nr_retries = MEM_CGROUP_RECLAIM_RETRIES;
 -
 -	/* we call try-to-free pages for make this cgroup empty */
 -	lru_add_drain_all();
 +	struct mem_cgroup *memcg;
 +	struct kmem_cache *memcg_cachep;
  
 -	drain_all_stock(memcg);
 +	VM_BUG_ON(!cachep->memcg_params);
 +	VM_BUG_ON(!cachep->memcg_params->is_root_cache);
  
 -	/* try to free all pages in this cgroup */
 -	while (nr_retries && page_counter_read(&memcg->memory)) {
 -		int progress;
 +	if (cachep->flags & SLAB_ACCOUNT)
 +		gfp |= __GFP_ACCOUNT;
  
 -		if (signal_pending(current))
 -			return -EINTR;
 +	if (!(gfp & __GFP_ACCOUNT))
 +		return cachep;
  
 -		progress = try_to_free_mem_cgroup_pages(memcg, 1,
 -							GFP_KERNEL, true);
 -		if (!progress) {
 -			nr_retries--;
 -			/* maybe some writeback is necessary */
 -			congestion_wait(BLK_RW_ASYNC, HZ/10);
 -		}
 +	if (!current->mm || current->memcg_kmem_skip_account)
 +		return cachep;
  
 -	}
 +	rcu_read_lock();
 +	memcg = mem_cgroup_from_task(rcu_dereference(current->mm->owner));
  
 -	return 0;
 -}
 +	if (!memcg_can_account_kmem(memcg))
 +		goto out;
  
 -static ssize_t mem_cgroup_force_empty_write(struct kernfs_open_file *of,
 -					    char *buf, size_t nbytes,
 -					    loff_t off)
 -{
 -	struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
 +	memcg_cachep = cache_from_memcg_idx(cachep, memcg_cache_id(memcg));
 +	if (likely(memcg_cachep)) {
 +		cachep = memcg_cachep;
 +		goto out;
 +	}
  
 -	if (mem_cgroup_is_root(memcg))
 -		return -EINVAL;
 -	return mem_cgroup_force_empty(memcg) ?: nbytes;
 -}
 +	/* The corresponding put will be done in the workqueue. */
 +	if (!css_tryget(&memcg->css))
 +		goto out;
 +	rcu_read_unlock();
  
 -static u64 mem_cgroup_hierarchy_read(struct cgroup_subsys_state *css,
 -				     struct cftype *cft)
 -{
 -	return mem_cgroup_from_css(css)->use_hierarchy;
 +	/*
 +	 * If we are in a safe context (can wait, and not in interrupt
 +	 * context), we could be be predictable and return right away.
 +	 * This would guarantee that the allocation being performed
 +	 * already belongs in the new cache.
 +	 *
 +	 * However, there are some clashes that can arrive from locking.
 +	 * For instance, because we acquire the slab_mutex while doing
 +	 * kmem_cache_dup, this means no further allocation could happen
 +	 * with the slab_mutex held.
 +	 *
 +	 * Also, because cache creation issue get_online_cpus(), this
 +	 * creates a lock chain: memcg_slab_mutex -> cpu_hotplug_mutex,
 +	 * that ends up reversed during cpu hotplug. (cpuset allocates
 +	 * a bunch of GFP_KERNEL memory during cpuup). Due to all that,
 +	 * better to defer everything.
 +	 */
 +	memcg_create_cache_enqueue(memcg, cachep);
 +	return cachep;
 +out:
 +	rcu_read_unlock();
 +	return cachep;
  }
 +EXPORT_SYMBOL(__memcg_kmem_get_cache);
  
 -static int mem_cgroup_hierarchy_write(struct cgroup_subsys_state *css,
 -				      struct cftype *cft, u64 val)
 +/*
 + * We need to verify if the allocation against current->mm->owner's memcg is
 + * possible for the given order. But the page is not allocated yet, so we'll
 + * need a further commit step to do the final arrangements.
 + *
++<<<<<<< HEAD
 + * It is possible for the task to switch cgroups in this mean time, so at
 + * commit time, we can't rely on task conversion any longer.  We'll then use
 + * the handle argument to return to the caller which cgroup we should commit
 + * against. We could also return the memcg directly and avoid the pointer
 + * passing, but a boolean return value gives better semantics considering
 + * the compiled-out case as well.
++=======
++ * Returns 0 on success, an error code on failure.
++ */
++int __memcg_kmem_charge_memcg(struct page *page, gfp_t gfp, int order,
++			    struct mem_cgroup *memcg)
+ {
 -	int retval = 0;
 -	struct mem_cgroup *memcg = mem_cgroup_from_css(css);
 -	struct mem_cgroup *parent_memcg = mem_cgroup_from_css(memcg->css.parent);
++	unsigned int nr_pages = 1 << order;
++	struct page_counter *counter;
++	int ret;
+ 
 -	if (memcg->use_hierarchy == val)
 -		return 0;
++	ret = try_charge(memcg, gfp, nr_pages);
++	if (ret)
++		return ret;
++
++	if (!cgroup_subsys_on_dfl(memory_cgrp_subsys) &&
++	    !page_counter_try_charge(&memcg->kmem, nr_pages, &counter)) {
++
++		/*
++		 * Enforce __GFP_NOFAIL allocation because callers are not
++		 * prepared to see failures and likely do not have any failure
++		 * handling code.
++		 */
++		if (gfp & __GFP_NOFAIL) {
++			page_counter_charge(&memcg->kmem, nr_pages);
++			return 0;
++		}
++		cancel_charge(memcg, nr_pages);
++		return -ENOMEM;
++	}
++	return 0;
++}
++
++/**
++ * __memcg_kmem_charge: charge a kmem page to the current memory cgroup
++ * @page: page to charge
++ * @gfp: reclaim mode
++ * @order: allocation order
++>>>>>>> e55d9d9bfb69 (memcg, kmem: do not fail __GFP_NOFAIL charges)
 + *
 + * Returning true means the allocation is possible.
 + */
 +bool
 +__memcg_kmem_newpage_charge(gfp_t gfp, struct mem_cgroup **_memcg, int order)
 +{
 +	struct mem_cgroup *memcg;
 +	int ret;
 +
 +	*_memcg = NULL;
  
  	/*
 -	 * If parent's use_hierarchy is set, we can't make any modifications
 -	 * in the child subtrees. If it is unset, then the change can
 -	 * occur, provided the current cgroup has no children.
 +	 * Disabling accounting is only relevant for some specific memcg
 +	 * internal allocations. Therefore we would initially not have such
 +	 * check here, since direct calls to the page allocator that are marked
 +	 * with GFP_KMEMCG only happen outside memcg core. We are mostly
 +	 * concerned with cache allocations, and by having this test at
 +	 * memcg_kmem_get_cache, we are already able to relay the allocation to
 +	 * the root cache and bypass the memcg cache altogether.
  	 *
 -	 * For the root cgroup, parent_mem is NULL, we allow value to be
 -	 * set if there are no children.
 +	 * There is one exception, though: the SLUB allocator does not create
 +	 * large order caches, but rather service large kmallocs directly from
 +	 * the page allocator. Therefore, the following sequence when backed by
 +	 * the SLUB allocator:
 +	 *
 +	 * 	memcg_stop_kmem_account();
 +	 * 	kmalloc(<large_number>)
 +	 * 	memcg_resume_kmem_account();
 +	 *
 +	 * would effectively ignore the fact that we should skip accounting,
 +	 * since it will drive us directly to this function without passing
 +	 * through the cache selector memcg_kmem_get_cache. Such large
 +	 * allocations are extremely rare but can happen, for instance, for the
 +	 * cache arrays. We bring this test here.
  	 */
 -	if ((!parent_memcg || !parent_memcg->use_hierarchy) &&
 -				(val == 1 || val == 0)) {
 -		if (!memcg_has_children(memcg))
 -			memcg->use_hierarchy = val;
 -		else
 -			retval = -EBUSY;
 -	} else
 -		retval = -EINVAL;
 +	if (!current->mm || current->memcg_kmem_skip_account)
 +		return true;
  
 -	return retval;
 -}
 +	memcg = try_get_mem_cgroup_from_mm(current->mm);
  
 -static unsigned long mem_cgroup_usage(struct mem_cgroup *memcg, bool swap)
 -{
 -	unsigned long val;
 +	/*
 +	 * very rare case described in mem_cgroup_from_task. Unfortunately there
 +	 * isn't much we can do without complicating this too much, and it would
 +	 * be gfp-dependent anyway. Just let it go
 +	 */
 +	if (unlikely(!memcg))
 +		return true;
  
 -	if (mem_cgroup_is_root(memcg)) {
 -		val = memcg_page_state(memcg, MEMCG_CACHE) +
 -			memcg_page_state(memcg, MEMCG_RSS);
 -		if (swap)
 -			val += memcg_page_state(memcg, MEMCG_SWAP);
 -	} else {
 -		if (!swap)
 -			val = page_counter_read(&memcg->memory);
 -		else
 -			val = page_counter_read(&memcg->memsw);
 +	if (!memcg_can_account_kmem(memcg)) {
 +		css_put(&memcg->css);
 +		return true;
  	}
 -	return val;
 -}
  
 -enum {
 -	RES_USAGE,
 -	RES_LIMIT,
 -	RES_MAX_USAGE,
 -	RES_FAILCNT,
 -	RES_SOFT_LIMIT,
 -};
 +	ret = memcg_charge_kmem(memcg, gfp, 1 << order);
 +	if (!ret)
 +		*_memcg = memcg;
 +
 +	css_put(&memcg->css);
 +	return (ret == 0);
 +}
  
 -static u64 mem_cgroup_read_u64(struct cgroup_subsys_state *css,
 -			       struct cftype *cft)
 +void __memcg_kmem_commit_charge(struct page *page, struct mem_cgroup *memcg,
 +			      int order)
  {
 -	struct mem_cgroup *memcg = mem_cgroup_from_css(css);
 -	struct page_counter *counter;
 +	struct page_cgroup *pc;
  
 -	switch (MEMFILE_TYPE(cft->private)) {
 -	case _MEM:
 -		counter = &memcg->memory;
 -		break;
 -	case _MEMSWAP:
 -		counter = &memcg->memsw;
 -		break;
 -	case _KMEM:
 -		counter = &memcg->kmem;
 -		break;
 -	case _TCP:
 -		counter = &memcg->tcpmem;
 -		break;
 -	default:
 -		BUG();
 -	}
 +	VM_BUG_ON(mem_cgroup_is_root(memcg));
  
 -	switch (MEMFILE_ATTR(cft->private)) {
 -	case RES_USAGE:
 -		if (counter == &memcg->memory)
 -			return (u64)mem_cgroup_usage(memcg, false) * PAGE_SIZE;
 -		if (counter == &memcg->memsw)
 -			return (u64)mem_cgroup_usage(memcg, true) * PAGE_SIZE;
 -		return (u64)page_counter_read(counter) * PAGE_SIZE;
 -	case RES_LIMIT:
 -		return (u64)counter->max * PAGE_SIZE;
 -	case RES_MAX_USAGE:
 -		return (u64)counter->watermark * PAGE_SIZE;
 -	case RES_FAILCNT:
 -		return counter->failcnt;
 -	case RES_SOFT_LIMIT:
 -		return (u64)memcg->soft_limit * PAGE_SIZE;
 -	default:
 -		BUG();
 +	/* The page allocation failed. Revert */
 +	if (!page) {
 +		memcg_uncharge_kmem(memcg, 1 << order);
 +		return;
  	}
 +
 +	pc = lookup_page_cgroup(page);
 +	lock_page_cgroup(pc);
 +	pc->mem_cgroup = memcg;
 +	SetPageCgroupUsed(pc);
 +	unlock_page_cgroup(pc);
  }
  
 -static void memcg_flush_percpu_vmstats(struct mem_cgroup *memcg, bool slab_only)
 +void __memcg_kmem_uncharge_pages(struct page *page, int order)
  {
 -	unsigned long stat[MEMCG_NR_STAT];
 -	struct mem_cgroup *mi;
 -	int node, cpu, i;
 -	int min_idx, max_idx;
 +	struct mem_cgroup *memcg = NULL;
 +	struct page_cgroup *pc;
  
 -	if (slab_only) {
 -		min_idx = NR_SLAB_RECLAIMABLE;
 -		max_idx = NR_SLAB_UNRECLAIMABLE;
 -	} else {
 -		min_idx = 0;
 -		max_idx = MEMCG_NR_STAT;
 -	}
  
 -	for (i = min_idx; i < max_idx; i++)
 -		stat[i] = 0;
 +	pc = lookup_page_cgroup(page);
 +	/*
 +	 * Fast unlocked return. Theoretically might have changed, have to
 +	 * check again after locking.
 +	 */
 +	if (!PageCgroupUsed(pc))
 +		return;
  
 -	for_each_online_cpu(cpu)
 -		for (i = min_idx; i < max_idx; i++)
 -			stat[i] += per_cpu(memcg->vmstats_percpu->stat[i], cpu);
 +	lock_page_cgroup(pc);
 +	if (PageCgroupUsed(pc)) {
 +		memcg = pc->mem_cgroup;
 +		ClearPageCgroupUsed(pc);
 +	}
 +	unlock_page_cgroup(pc);
  
 -	for (mi = memcg; mi; mi = parent_mem_cgroup(mi))
 -		for (i = min_idx; i < max_idx; i++)
 -			atomic_long_add(stat[i], &mi->vmstats[i]);
 +	/*
 +	 * We trust that only if there is a memcg associated with the page, it
 +	 * is a valid allocation
 +	 */
 +	if (!memcg)
 +		return;
  
 -	if (!slab_only)
 -		max_idx = NR_VM_NODE_STAT_ITEMS;
 +	VM_BUG_ON_PAGE(mem_cgroup_is_root(memcg), page);
 +	memcg_uncharge_kmem(memcg, 1 << order);
 +}
 +#else
 +static inline void mem_cgroup_destroy_all_caches(struct mem_cgroup *memcg)
 +{
 +}
 +#endif /* CONFIG_MEMCG_KMEM */
  
 -	for_each_node(node) {
 -		struct mem_cgroup_per_node *pn = memcg->nodeinfo[node];
 -		struct mem_cgroup_per_node *pi;
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
  
 -		for (i = min_idx; i < max_idx; i++)
 -			stat[i] = 0;
 +#define PCGF_NOCOPY_AT_SPLIT (1 << PCG_LOCK | 1 << PCG_MIGRATION)
 +/*
 + * Because tail pages are not marked as "used", set it. We're under
 + * zone->lru_lock, 'splitting on pmd' and compound_lock.
 + * charge/uncharge will be never happen and move_account() is done under
 + * compound_lock(), so we don't have to take care of races.
 + */
 +void mem_cgroup_split_huge_fixup(struct page *head)
 +{
 +	struct page_cgroup *head_pc = lookup_page_cgroup(head);
 +	struct page_cgroup *pc;
 +	struct mem_cgroup *memcg;
 +	int i;
  
 -		for_each_online_cpu(cpu)
 -			for (i = min_idx; i < max_idx; i++)
 -				stat[i] += per_cpu(
 -					pn->lruvec_stat_cpu->count[i], cpu);
 +	if (mem_cgroup_disabled())
 +		return;
  
 -		for (pi = pn; pi; pi = parent_nodeinfo(pi, node))
 -			for (i = min_idx; i < max_idx; i++)
 -				atomic_long_add(stat[i], &pi->lruvec_stat[i]);
 +	memcg = head_pc->mem_cgroup;
 +	for (i = 1; i < HPAGE_PMD_NR; i++) {
 +		pc = head_pc + i;
 +		pc->mem_cgroup = memcg;
 +		smp_wmb();/* see __commit_charge() */
 +		pc->flags = head_pc->flags & ~PCGF_NOCOPY_AT_SPLIT;
  	}
 +	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_RSS_HUGE],
 +		       HPAGE_PMD_NR);
  }
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  
 -static void memcg_flush_percpu_vmevents(struct mem_cgroup *memcg)
 +/**
 + * mem_cgroup_move_account - move account of the page
 + * @page: the page
 + * @nr_pages: number of regular pages (>1 for huge pages)
 + * @pc:	page_cgroup of the page.
 + * @from: mem_cgroup which the page is moved from.
 + * @to:	mem_cgroup which the page is moved to. @from != @to.
 + *
 + * The caller must confirm following.
 + * - page is not on LRU (isolate_page() is useful.)
 + * - compound_lock is held when nr_pages > 1
 + *
 + * This function doesn't do "charge" to new cgroup and doesn't do "uncharge"
 + * from old cgroup.
 + */
 +static int mem_cgroup_move_account(struct page *page,
 +				   unsigned int nr_pages,
 +				   struct page_cgroup *pc,
 +				   struct mem_cgroup *from,
 +				   struct mem_cgroup *to)
  {
 -	unsigned long events[NR_VM_EVENT_ITEMS];
 -	struct mem_cgroup *mi;
 -	int cpu, i;
 -
 -	for (i = 0; i < NR_VM_EVENT_ITEMS; i++)
 -		events[i] = 0;
 -
 -	for_each_online_cpu(cpu)
 -		for (i = 0; i < NR_VM_EVENT_ITEMS; i++)
 -			events[i] += per_cpu(memcg->vmstats_percpu->events[i],
 -					     cpu);
 +	unsigned long flags;
 +	int ret;
 +	bool anon = PageAnon(page);
  
 -	for (mi = memcg; mi; mi = parent_mem_cgroup(mi))
 -		for (i = 0; i < NR_VM_EVENT_ITEMS; i++)
 -			atomic_long_add(events[i], &mi->vmevents[i]);
 -}
 +	VM_BUG_ON(from == to);
 +	VM_BUG_ON_PAGE(PageLRU(page), page);
 +	/*
 +	 * The page is isolated from LRU. So, collapse function
 +	 * will not handle this page. But page splitting can happen.
 +	 * Do this check under compound_page_lock(). The caller should
 +	 * hold it.
 +	 */
 +	ret = -EBUSY;
 +	if (nr_pages > 1 && !PageTransHuge(page))
 +		goto out;
  
 -#ifdef CONFIG_MEMCG_KMEM
 -static int memcg_online_kmem(struct mem_cgroup *memcg)
 -{
 -	int memcg_id;
 +	lock_page_cgroup(pc);
  
 -	if (cgroup_memory_nokmem)
 -		return 0;
 +	ret = -EINVAL;
 +	if (!PageCgroupUsed(pc) || pc->mem_cgroup != from)
 +		goto unlock;
  
 -	BUG_ON(memcg->kmemcg_id >= 0);
 -	BUG_ON(memcg->kmem_state);
 +	move_lock_mem_cgroup(from, &flags);
  
 -	memcg_id = memcg_alloc_cache_id();
 -	if (memcg_id < 0)
 -		return memcg_id;
 +	if (!anon && page_mapped(page)) {
 +		/* Update mapped_file data for mem_cgroup */
 +		preempt_disable();
 +		__this_cpu_dec(from->stat->count[MEM_CGROUP_STAT_FILE_MAPPED]);
 +		__this_cpu_inc(to->stat->count[MEM_CGROUP_STAT_FILE_MAPPED]);
 +		preempt_enable();
 +	}
 +	mem_cgroup_charge_statistics(from, page, anon, -nr_pages);
  
 -	static_branch_inc(&memcg_kmem_enabled_key);
 +	/* caller should have done css_get */
 +	pc->mem_cgroup = to;
 +	mem_cgroup_charge_statistics(to, page, anon, nr_pages);
 +	move_unlock_mem_cgroup(from, &flags);
 +	ret = 0;
 +unlock:
 +	unlock_page_cgroup(pc);
  	/*
 -	 * A memory cgroup is considered kmem-online as soon as it gets
 -	 * kmemcg_id. Setting the id after enabling static branching will
 -	 * guarantee no one starts accounting before all call sites are
 -	 * patched.
 +	 * check events
  	 */
 -	memcg->kmemcg_id = memcg_id;
 -	memcg->kmem_state = KMEM_ONLINE;
 -	INIT_LIST_HEAD(&memcg->kmem_caches);
 -
 -	return 0;
 +	memcg_check_events(to, page);
 +	memcg_check_events(from, page);
 +out:
 +	return ret;
  }
  
 -static void memcg_offline_kmem(struct mem_cgroup *memcg)
 +/**
 + * mem_cgroup_move_parent - moves page to the parent group
 + * @page: the page to move
 + * @pc: page_cgroup of the page
 + * @child: page's cgroup
 + *
 + * move charges to its parent or the root cgroup if the group has no
 + * parent (aka use_hierarchy==0).
 + * Although this might fail (get_page_unless_zero, isolate_lru_page or
 + * mem_cgroup_move_account fails) the failure is always temporary and
 + * it signals a race with a page removal/uncharge or migration. In the
 + * first case the page is on the way out and it will vanish from the LRU
 + * on the next attempt and the call should be retried later.
 + * Isolation from the LRU fails only if page has been isolated from
 + * the LRU since we looked at it and that usually means either global
 + * reclaim or migration going on. The page will either get back to the
 + * LRU or vanish.
 + * Finaly mem_cgroup_move_account fails only if the page got uncharged
 + * (!PageCgroupUsed) or moved to a different group. The page will
 + * disappear in the next attempt.
 + */
 +static int mem_cgroup_move_parent(struct page *page,
 +				  struct page_cgroup *pc,
 +				  struct mem_cgroup *child)
  {
 -	struct cgroup_subsys_state *css;
 -	struct mem_cgroup *parent, *child;
 -	int kmemcg_id;
 -
 -	if (memcg->kmem_state != KMEM_ONLINE)
 -		return;
 -	/*
 -	 * Clear the online state before clearing memcg_caches array
 -	 * entries. The slab_mutex in memcg_deactivate_kmem_caches()
 -	 * guarantees that no cache will be created for this cgroup
 -	 * after we are done (see memcg_create_kmem_cache()).
 -	 */
 -	memcg->kmem_state = KMEM_ALLOCATED;
 +	struct mem_cgroup *parent;
 +	unsigned int nr_pages;
 +	unsigned long uninitialized_var(flags);
 +	int ret;
  
 -	parent = parent_mem_cgroup(memcg);
 -	if (!parent)
 -		parent = root_mem_cgroup;
 +	VM_BUG_ON(mem_cgroup_is_root(child));
  
 -	/*
 -	 * Deactivate and reparent kmem_caches. Then flush percpu
 -	 * slab statistics to have precise values at the parent and
 -	 * all ancestor levels. It's required to keep slab stats
 -	 * accurate after the reparenting of kmem_caches.
 -	 */
 -	memcg_deactivate_kmem_caches(memcg, parent);
 -	memcg_flush_percpu_vmstats(memcg, true);
 +	ret = -EBUSY;
 +	if (!get_page_unless_zero(page))
 +		goto out;
 +	if (isolate_lru_page(page))
 +		goto put;
  
 -	kmemcg_id = memcg->kmemcg_id;
 -	BUG_ON(kmemcg_id < 0);
 +	nr_pages = hpage_nr_pages(page);
  
 +	parent = parent_mem_cgroup(child);
  	/*
 -	 * Change kmemcg_id of this cgroup and all its descendants to the
 -	 * parent's id, and then move all entries from this cgroup's list_lrus
 -	 * to ones of the parent. After we have finished, all list_lrus
 -	 * corresponding to this cgroup are guaranteed to remain empty. The
 -	 * ordering is imposed by list_lru_node->lock taken by
 -	 * memcg_drain_all_list_lrus().
 +	 * If no parent, move charges to root cgroup.
  	 */
 -	rcu_read_lock(); /* can be called from css_free w/o cgroup_mutex */
 -	css_for_each_descendant_pre(css, &memcg->css) {
 -		child = mem_cgroup_from_css(css);
 -		BUG_ON(child->kmemcg_id != kmemcg_id);
 -		child->kmemcg_id = parent->kmemcg_id;
 -		if (!memcg->use_hierarchy)
 -			break;
 +	if (!parent)
 +		parent = root_mem_cgroup;
 +
 +	if (nr_pages > 1) {
 +		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 +		flags = compound_lock_irqsave(page);
  	}
 -	rcu_read_unlock();
  
 -	memcg_drain_all_list_lrus(kmemcg_id, parent);
 +	ret = mem_cgroup_move_account(page, nr_pages,
 +				pc, child, parent);
 +	if (!ret) {
 +		/* Take charge off the local counters */
 +		page_counter_cancel(&child->memory, nr_pages);
 +		if (do_swap_account)
 +			page_counter_cancel(&child->memsw, nr_pages);
 +	}
  
 -	memcg_free_cache_id(kmemcg_id);
 +	if (nr_pages > 1)
 +		compound_unlock_irqrestore(page, flags);
 +	putback_lru_page(page);
 +put:
 +	put_page(page);
 +out:
 +	return ret;
  }
  
 -static void memcg_free_kmem(struct mem_cgroup *memcg)
 +/*
 + * Charge the memory controller for page usage.
 + * Return
 + * 0 if the charge was successful
 + * < 0 if the cgroup is over its limit
 + */
 +static int mem_cgroup_charge_common(struct page *page, struct mm_struct *mm,
 +				gfp_t gfp_mask, enum charge_type ctype)
  {
 -	/* css_alloc() failed, offlining didn't happen */
 -	if (unlikely(memcg->kmem_state == KMEM_ONLINE))
 -		memcg_offline_kmem(memcg);
 +	struct mem_cgroup *memcg = NULL;
 +	unsigned int nr_pages = 1;
 +	bool oom = true;
 +	int ret;
  
 -	if (memcg->kmem_state == KMEM_ALLOCATED) {
 -		WARN_ON(!list_empty(&memcg->kmem_caches));
 -		static_branch_dec(&memcg_kmem_enabled_key);
 +	if (PageTransHuge(page)) {
 +		nr_pages <<= compound_order(page);
 +		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 +		/*
 +		 * Never OOM-kill a process for a huge page.  The
 +		 * fault handler will fall back to regular pages.
 +		 */
 +		oom = false;
  	}
 -}
 -#else
 -static int memcg_online_kmem(struct mem_cgroup *memcg)
 -{
 +
 +	ret = __mem_cgroup_try_charge(mm, gfp_mask, nr_pages, &memcg, oom);
 +	if (ret == -ENOMEM)
 +		return ret;
 +	__mem_cgroup_commit_charge(memcg, page, nr_pages, ctype, false);
  	return 0;
  }
 -static void memcg_offline_kmem(struct mem_cgroup *memcg)
 -{
 -}
 -static void memcg_free_kmem(struct mem_cgroup *memcg)
 +
 +int mem_cgroup_newpage_charge(struct page *page,
 +			      struct mm_struct *mm, gfp_t gfp_mask)
  {
 +	if (mem_cgroup_disabled())
 +		return 0;
 +	VM_BUG_ON_PAGE(page_mapped(page), page);
 +	VM_BUG_ON_PAGE(page->mapping && !PageAnon(page), page);
 +	VM_BUG_ON(!mm);
 +	return mem_cgroup_charge_common(page, mm, gfp_mask,
 +					MEM_CGROUP_CHARGE_TYPE_ANON);
  }
 -#endif /* CONFIG_MEMCG_KMEM */
  
 -static int memcg_update_kmem_max(struct mem_cgroup *memcg,
 -				 unsigned long max)
 +/*
 + * While swap-in, try_charge -> commit or cancel, the page is locked.
 + * And when try_charge() successfully returns, one refcnt to memcg without
 + * struct page_cgroup is acquired. This refcnt will be consumed by
 + * "commit()" or removed by "cancel()"
 + */
 +static int __mem_cgroup_try_charge_swapin(struct mm_struct *mm,
 +					  struct page *page,
 +					  gfp_t mask,
 +					  struct mem_cgroup **memcgp)
  {
 +	struct mem_cgroup *memcg;
 +	struct page_cgroup *pc;
  	int ret;
  
 -	mutex_lock(&memcg_max_mutex);
 -	ret = page_counter_set_max(&memcg->kmem, max);
 -	mutex_unlock(&memcg_max_mutex);
 +	pc = lookup_page_cgroup(page);
 +	/*
 +	 * Every swap fault against a single page tries to charge the
 +	 * page, bail as early as possible.  shmem_unuse() encounters
 +	 * already charged pages, too.  The USED bit is protected by
 +	 * the page lock, which serializes swap cache removal, which
 +	 * in turn serializes uncharging.
 +	 */
 +	if (PageCgroupUsed(pc))
 +		return 0;
 +	if (!do_swap_account)
 +		goto charge_cur_mm;
 +	memcg = try_get_mem_cgroup_from_page(page);
 +	if (!memcg)
 +		goto charge_cur_mm;
 +	*memcgp = memcg;
 +	ret = __mem_cgroup_try_charge(NULL, mask, 1, memcgp, true);
 +	css_put(&memcg->css);
 +	if (ret == -EINTR)
 +		ret = 0;
 +	return ret;
 +charge_cur_mm:
 +	ret = __mem_cgroup_try_charge(mm, mask, 1, memcgp, true);
 +	if (ret == -EINTR)
 +		ret = 0;
  	return ret;
  }
  
* Unmerged path mm/memcontrol.c
