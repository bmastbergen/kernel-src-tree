dm table: fix iterate_devices based device capability checks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.62.1.el7
commit-author Jeffle Xu <jefflexu@linux.alibaba.com>
commit a4c8dd9c2d0987cf542a2a0c42684c9c6d78a04e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.62.1.el7/a4c8dd9c.failed

According to the definition of dm_iterate_devices_fn:
 * This function must iterate through each section of device used by the
 * target until it encounters a non-zero return code, which it then returns.
 * Returns zero if no callout returned non-zero.

For some target type (e.g. dm-stripe), one call of iterate_devices() may
iterate multiple underlying devices internally, in which case a non-zero
return code returned by iterate_devices_callout_fn will stop the iteration
in advance. No iterate_devices_callout_fn should return non-zero unless
device iteration should stop.

Rename dm_table_requires_stable_pages() to dm_table_any_dev_attr() and
elevate it for reuse to stop iterating (and return non-zero) on the
first device that causes iterate_devices_callout_fn to return non-zero.
Use dm_table_any_dev_attr() to properly iterate through devices.

Rename device_is_nonrot() to device_is_rotational() and invert logic
accordingly to fix improper disposition.

Fixes: c3c4555edd10 ("dm table: clear add_random unless all devices have it set")
Fixes: 4693c9668fdc ("dm table: propagate non rotational flag")
	Cc: stable@vger.kernel.org
	Signed-off-by: Jeffle Xu <jefflexu@linux.alibaba.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit a4c8dd9c2d0987cf542a2a0c42684c9c6d78a04e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-table.c
diff --cc drivers/md/dm-table.c
index 857711dc7745,3e211e64f348..000000000000
--- a/drivers/md/dm-table.c
+++ b/drivers/md/dm-table.c
@@@ -1530,8 -1651,8 +1570,9 @@@ static int device_is_not_random(struct 
  	return q && !blk_queue_add_random(q);
  }
  
 -static int device_not_write_same_capable(struct dm_target *ti, struct dm_dev *dev,
 -					 sector_t start, sector_t len, void *data)
++<<<<<<< HEAD
 +static int queue_supports_sg_merge(struct dm_target *ti, struct dm_dev *dev,
 +				   sector_t start, sector_t len, void *data)
  {
  	struct request_queue *q = bdev_get_queue(dev->bdev);
  
@@@ -1555,8 -1678,35 +1596,10 @@@ static bool dm_table_all_devices_attrib
  	return true;
  }
  
 -static int device_not_write_zeroes_capable(struct dm_target *ti, struct dm_dev *dev,
 -					   sector_t start, sector_t len, void *data)
 -{
 -	struct request_queue *q = bdev_get_queue(dev->bdev);
 -
 -	return q && !q->limits.max_write_zeroes_sectors;
 -}
 -
 -static bool dm_table_supports_write_zeroes(struct dm_table *t)
 -{
 -	struct dm_target *ti;
 -	unsigned i = 0;
 -
 -	while (i < dm_table_get_num_targets(t)) {
 -		ti = dm_table_get_target(t, i++);
 -
 -		if (!ti->num_write_zeroes_bios)
 -			return false;
 -
 -		if (!ti->type->iterate_devices ||
 -		    ti->type->iterate_devices(ti, device_not_write_zeroes_capable, NULL))
 -			return false;
 -	}
 -
 -	return true;
 -}
 -
 -static int device_not_nowait_capable(struct dm_target *ti, struct dm_dev *dev,
 -				     sector_t start, sector_t len, void *data)
++=======
++>>>>>>> a4c8dd9c2d09 (dm table: fix iterate_devices based device capability checks)
 +static int device_not_write_same_capable(struct dm_target *ti, struct dm_dev *dev,
 +					 sector_t start, sector_t len, void *data)
  {
  	struct request_queue *q = bdev_get_queue(dev->bdev);
  
@@@ -1621,30 -1799,9 +1664,9 @@@ static int device_requires_stable_pages
  {
  	struct request_queue *q = bdev_get_queue(dev->bdev);
  
 -	return q && blk_queue_stable_writes(q);
 +	return q && bdi_cap_stable_pages_required(&q->backing_dev_info);
  }
  
- /*
-  * If any underlying device requires stable pages, a table must require
-  * them as well.  Only targets that support iterate_devices are considered:
-  * don't want error, zero, etc to require stable pages.
-  */
- static bool dm_table_requires_stable_pages(struct dm_table *t)
- {
- 	struct dm_target *ti;
- 	unsigned i;
- 
- 	for (i = 0; i < dm_table_get_num_targets(t); i++) {
- 		ti = dm_table_get_target(t, i);
- 
- 		if (ti->type->iterate_devices &&
- 		    ti->type->iterate_devices(ti, device_requires_stable_pages, NULL))
- 			return true;
- 	}
- 
- 	return false;
- }
- 
  void dm_table_set_restrictions(struct dm_table *t, struct request_queue *q,
  			       struct queue_limits *limits)
  {
@@@ -1683,14 -1850,11 +1705,21 @@@
  	if (dm_table_supports_dax_write_cache(t))
  		dax_write_cache(t->md->dax_dev, true);
  
 +	if (!dm_table_discard_zeroes_data(t))
 +		q->limits.discard_zeroes_data = 0;
 +
  	/* Ensure that all underlying devices are non-rotational. */
++<<<<<<< HEAD
 +	if (dm_table_all_devices_attribute(t, device_is_nonrot))
 +		queue_flag_set_unlocked(QUEUE_FLAG_NONROT, q);
 +	else
 +		queue_flag_clear_unlocked(QUEUE_FLAG_NONROT, q);
++=======
+ 	if (dm_table_any_dev_attr(t, device_is_rotational))
+ 		blk_queue_flag_clear(QUEUE_FLAG_NONROT, q);
+ 	else
+ 		blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
++>>>>>>> a4c8dd9c2d09 (dm table: fix iterate_devices based device capability checks)
  
  	if (!dm_table_supports_write_same(t))
  		q->limits.max_write_same_sectors = 0;
@@@ -1705,11 -1866,14 +1734,19 @@@
  	/*
  	 * Some devices don't use blk_integrity but still want stable pages
  	 * because they do their own checksumming.
+ 	 * If any underlying device requires stable pages, a table must require
+ 	 * them as well.  Only targets that support iterate_devices are considered:
+ 	 * don't want error, zero, etc to require stable pages.
  	 */
++<<<<<<< HEAD
 +	if (dm_table_requires_stable_pages(t))
 +		q->backing_dev_info.capabilities |= BDI_CAP_STABLE_WRITES;
++=======
+ 	if (dm_table_any_dev_attr(t, device_requires_stable_pages))
+ 		blk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, q);
++>>>>>>> a4c8dd9c2d09 (dm table: fix iterate_devices based device capability checks)
  	else
 -		blk_queue_flag_clear(QUEUE_FLAG_STABLE_WRITES, q);
 +		q->backing_dev_info.capabilities &= ~BDI_CAP_STABLE_WRITES;
  
  	/*
  	 * Determine whether or not this queue's I/O timings contribute
@@@ -1717,21 -1881,22 +1754,26 @@@
  	 * Clear QUEUE_FLAG_ADD_RANDOM if any underlying device does not
  	 * have it set.
  	 */
++<<<<<<< HEAD
 +	if (blk_queue_add_random(q) && dm_table_all_devices_attribute(t, device_is_not_random))
 +		queue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, q);
++=======
+ 	if (blk_queue_add_random(q) && dm_table_any_dev_attr(t, device_is_not_random))
+ 		blk_queue_flag_clear(QUEUE_FLAG_ADD_RANDOM, q);
++>>>>>>> a4c8dd9c2d09 (dm table: fix iterate_devices based device capability checks)
  
  	/*
 -	 * For a zoned target, the number of zones should be updated for the
 -	 * correct value to be exposed in sysfs queue/nr_zones. For a BIO based
 -	 * target, this is all that is needed.
 +	 * QUEUE_FLAG_STACKABLE must be set after all queue settings are
 +	 * visible to other CPUs because, once the flag is set, incoming bios
 +	 * are processed by request-based dm, which refers to the queue
 +	 * settings.
 +	 * Until the flag set, bios are passed to bio-based dm and queued to
 +	 * md->deferred where queue settings are not needed yet.
 +	 * Those bios are passed to request-based dm at the resume time.
  	 */
 -#ifdef CONFIG_BLK_DEV_ZONED
 -	if (blk_queue_is_zoned(q)) {
 -		WARN_ON_ONCE(queue_is_mq(q));
 -		q->nr_zones = blkdev_nr_zones(t->md->disk);
 -	}
 -#endif
 -
 -	blk_queue_update_readahead(q);
 +	smp_mb();
 +	if (dm_table_request_based(t))
 +		queue_flag_set_unlocked(QUEUE_FLAG_STACKABLE, q);
  }
  
  unsigned int dm_table_get_num_targets(struct dm_table *t)
* Unmerged path drivers/md/dm-table.c
