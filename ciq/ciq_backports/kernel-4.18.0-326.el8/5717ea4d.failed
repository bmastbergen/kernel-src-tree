xfs: rework xfs_iflush_cluster() dirty inode iteration

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Dave Chinner <dchinner@redhat.com>
commit 5717ea4d527acbec9300cb083b100dd0003ac777
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/5717ea4d.failed

Now that we have all the dirty inodes attached to the cluster
buffer, we don't actually have to do radix tree lookups to find
them. Sure, the radix tree is efficient, but walking a linked list
of just the dirty inodes attached to the buffer is much better.

We are also no longer dependent on having a locked inode passed into
the function to determine where to start the lookup. This means we
can drop it from the function call and treat all inodes the same.

We also make xfs_iflush_cluster skip inodes marked with
XFS_IRECLAIM. This we avoid races with inodes that reclaim is
actively referencing or are being re-initialised by inode lookup. If
they are actually dirty, they'll get written by a future cluster
flush....

We also add a shutdown check after obtaining the flush lock so that
we catch inodes that are dirty in memory and may have inconsistent
state due to the shutdown in progress. We abort these inodes
directly and so they remove themselves directly from the buffer list
and the AIL rather than having to wait for the buffer to be failed
and callbacks run to be processed correctly.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
(cherry picked from commit 5717ea4d527acbec9300cb083b100dd0003ac777)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_inode.c
#	fs/xfs/xfs_inode.h
#	fs/xfs/xfs_inode_item.c
diff --cc fs/xfs/xfs_inode.c
index bd37c3b531db,ece3622f6d28..000000000000
--- a/fs/xfs/xfs_inode.c
+++ b/fs/xfs/xfs_inode.c
@@@ -3457,374 -3449,274 +3457,613 @@@ out_release_wip
  	return error;
  }
  
++<<<<<<< HEAD
 +STATIC int
- xfs_iflush_cluster(
++=======
+ static int
+ xfs_iflush(
  	struct xfs_inode	*ip,
  	struct xfs_buf		*bp)
  {
+ 	struct xfs_inode_log_item *iip = ip->i_itemp;
+ 	struct xfs_dinode	*dip;
  	struct xfs_mount	*mp = ip->i_mount;
- 	struct xfs_perag	*pag;
- 	unsigned long		first_index, mask;
- 	int			cilist_size;
- 	struct xfs_inode	**cilist;
- 	struct xfs_inode	*cip;
- 	struct xfs_ino_geometry	*igeo = M_IGEO(mp);
- 	int			error = 0;
- 	int			nr_found;
+ 	int			error;
+ 
+ 	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL|XFS_ILOCK_SHARED));
+ 	ASSERT(xfs_isiflocked(ip));
+ 	ASSERT(ip->i_df.if_format != XFS_DINODE_FMT_BTREE ||
+ 	       ip->i_df.if_nextents > XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK));
+ 	ASSERT(iip->ili_item.li_buf == bp);
+ 
+ 	dip = xfs_buf_offset(bp, ip->i_imap.im_boffset);
+ 
+ 	/*
+ 	 * We don't flush the inode if any of the following checks fail, but we
+ 	 * do still update the log item and attach to the backing buffer as if
+ 	 * the flush happened. This is a formality to facilitate predictable
+ 	 * error handling as the caller will shutdown and fail the buffer.
+ 	 */
+ 	error = -EFSCORRUPTED;
+ 	if (XFS_TEST_ERROR(dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC),
+ 			       mp, XFS_ERRTAG_IFLUSH_1)) {
+ 		xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
+ 			"%s: Bad inode %Lu magic number 0x%x, ptr "PTR_FMT,
+ 			__func__, ip->i_ino, be16_to_cpu(dip->di_magic), dip);
+ 		goto flush_out;
+ 	}
+ 	if (S_ISREG(VFS_I(ip)->i_mode)) {
+ 		if (XFS_TEST_ERROR(
+ 		    ip->i_df.if_format != XFS_DINODE_FMT_EXTENTS &&
+ 		    ip->i_df.if_format != XFS_DINODE_FMT_BTREE,
+ 		    mp, XFS_ERRTAG_IFLUSH_3)) {
+ 			xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
+ 				"%s: Bad regular inode %Lu, ptr "PTR_FMT,
+ 				__func__, ip->i_ino, ip);
+ 			goto flush_out;
+ 		}
+ 	} else if (S_ISDIR(VFS_I(ip)->i_mode)) {
+ 		if (XFS_TEST_ERROR(
+ 		    ip->i_df.if_format != XFS_DINODE_FMT_EXTENTS &&
+ 		    ip->i_df.if_format != XFS_DINODE_FMT_BTREE &&
+ 		    ip->i_df.if_format != XFS_DINODE_FMT_LOCAL,
+ 		    mp, XFS_ERRTAG_IFLUSH_4)) {
+ 			xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
+ 				"%s: Bad directory inode %Lu, ptr "PTR_FMT,
+ 				__func__, ip->i_ino, ip);
+ 			goto flush_out;
+ 		}
+ 	}
+ 	if (XFS_TEST_ERROR(ip->i_df.if_nextents + xfs_ifork_nextents(ip->i_afp) >
+ 				ip->i_d.di_nblocks, mp, XFS_ERRTAG_IFLUSH_5)) {
+ 		xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
+ 			"%s: detected corrupt incore inode %Lu, "
+ 			"total extents = %d, nblocks = %Ld, ptr "PTR_FMT,
+ 			__func__, ip->i_ino,
+ 			ip->i_df.if_nextents + xfs_ifork_nextents(ip->i_afp),
+ 			ip->i_d.di_nblocks, ip);
+ 		goto flush_out;
+ 	}
+ 	if (XFS_TEST_ERROR(ip->i_d.di_forkoff > mp->m_sb.sb_inodesize,
+ 				mp, XFS_ERRTAG_IFLUSH_6)) {
+ 		xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
+ 			"%s: bad inode %Lu, forkoff 0x%x, ptr "PTR_FMT,
+ 			__func__, ip->i_ino, ip->i_d.di_forkoff, ip);
+ 		goto flush_out;
+ 	}
+ 
+ 	/*
+ 	 * Inode item log recovery for v2 inodes are dependent on the
+ 	 * di_flushiter count for correct sequencing. We bump the flush
+ 	 * iteration count so we can detect flushes which postdate a log record
+ 	 * during recovery. This is redundant as we now log every change and
+ 	 * hence this can't happen but we need to still do it to ensure
+ 	 * backwards compatibility with old kernels that predate logging all
+ 	 * inode changes.
+ 	 */
+ 	if (!xfs_sb_version_has_v3inode(&mp->m_sb))
+ 		ip->i_d.di_flushiter++;
+ 
+ 	/*
+ 	 * If there are inline format data / attr forks attached to this inode,
+ 	 * make sure they are not corrupt.
+ 	 */
+ 	if (ip->i_df.if_format == XFS_DINODE_FMT_LOCAL &&
+ 	    xfs_ifork_verify_local_data(ip))
+ 		goto flush_out;
+ 	if (ip->i_afp && ip->i_afp->if_format == XFS_DINODE_FMT_LOCAL &&
+ 	    xfs_ifork_verify_local_attr(ip))
+ 		goto flush_out;
+ 
+ 	/*
+ 	 * Copy the dirty parts of the inode into the on-disk inode.  We always
+ 	 * copy out the core of the inode, because if the inode is dirty at all
+ 	 * the core must be.
+ 	 */
+ 	xfs_inode_to_disk(ip, dip, iip->ili_item.li_lsn);
+ 
+ 	/* Wrap, we never let the log put out DI_MAX_FLUSH */
+ 	if (ip->i_d.di_flushiter == DI_MAX_FLUSH)
+ 		ip->i_d.di_flushiter = 0;
+ 
+ 	xfs_iflush_fork(ip, dip, iip, XFS_DATA_FORK);
+ 	if (XFS_IFORK_Q(ip))
+ 		xfs_iflush_fork(ip, dip, iip, XFS_ATTR_FORK);
+ 	xfs_inobp_check(mp, bp);
+ 
+ 	/*
+ 	 * We've recorded everything logged in the inode, so we'd like to clear
+ 	 * the ili_fields bits so we don't log and flush things unnecessarily.
+ 	 * However, we can't stop logging all this information until the data
+ 	 * we've copied into the disk buffer is written to disk.  If we did we
+ 	 * might overwrite the copy of the inode in the log with all the data
+ 	 * after re-logging only part of it, and in the face of a crash we
+ 	 * wouldn't have all the data we need to recover.
+ 	 *
+ 	 * What we do is move the bits to the ili_last_fields field.  When
+ 	 * logging the inode, these bits are moved back to the ili_fields field.
+ 	 * In the xfs_iflush_done() routine we clear ili_last_fields, since we
+ 	 * know that the information those bits represent is permanently on
+ 	 * disk.  As long as the flush completes before the inode is logged
+ 	 * again, then both ili_fields and ili_last_fields will be cleared.
+ 	 */
+ 	error = 0;
+ flush_out:
+ 	spin_lock(&iip->ili_lock);
+ 	iip->ili_last_fields = iip->ili_fields;
+ 	iip->ili_fields = 0;
+ 	iip->ili_fsync_fields = 0;
+ 	spin_unlock(&iip->ili_lock);
+ 
+ 	/*
+ 	 * Store the current LSN of the inode so that we can tell whether the
+ 	 * item has moved in the AIL from xfs_iflush_done().
+ 	 */
+ 	xfs_trans_ail_copy_lsn(mp->m_ail, &iip->ili_flush_lsn,
+ 				&iip->ili_item.li_lsn);
+ 
+ 	/* generate the checksum. */
+ 	xfs_dinode_calc_crc(mp, dip);
+ 	return error;
+ }
+ 
+ /*
+  * Non-blocking flush of dirty inode metadata into the backing buffer.
+  *
+  * The caller must have a reference to the inode and hold the cluster buffer
+  * locked. The function will walk across all the inodes on the cluster buffer it
+  * can find and lock without blocking, and flush them to the cluster buffer.
+  *
+  * On successful flushing of at least one inode, the caller must write out the
+  * buffer and release it. If no inodes are flushed, -EAGAIN will be returned and
+  * the caller needs to release the buffer. On failure, the filesystem will be
+  * shut down, the buffer will have been unlocked and released, and EFSCORRUPTED
+  * will be returned.
+  */
+ int
++>>>>>>> 5717ea4d527a (xfs: rework xfs_iflush_cluster() dirty inode iteration)
+ xfs_iflush_cluster(
+ 	struct xfs_buf		*bp)
+ {
+ 	struct xfs_mount	*mp = bp->b_mount;
+ 	struct xfs_log_item	*lip, *n;
+ 	struct xfs_inode	*ip;
+ 	struct xfs_inode_log_item *iip;
  	int			clcount = 0;
- 	int			i;
+ 	int			error = 0;
  
++<<<<<<< HEAD
 +	pag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));
 +
 +	cilist_size = igeo->inodes_per_cluster * sizeof(struct xfs_inode *);
 +	cilist = kmem_alloc(cilist_size, KM_MAYFAIL|KM_NOFS);
 +	if (!cilist)
 +		goto out_put;
 +
 +	mask = ~(igeo->inodes_per_cluster - 1);
 +	first_index = XFS_INO_TO_AGINO(mp, ip->i_ino) & mask;
 +	rcu_read_lock();
 +	/* really need a gang lookup range call here */
 +	nr_found = radix_tree_gang_lookup(&pag->pag_ici_root, (void**)cilist,
 +					first_index, igeo->inodes_per_cluster);
 +	if (nr_found == 0)
 +		goto out_free;
 +
 +	for (i = 0; i < nr_found; i++) {
 +		cip = cilist[i];
 +		if (cip == ip)
 +			continue;
++=======
+ 	/*
+ 	 * We must use the safe variant here as on shutdown xfs_iflush_abort()
+ 	 * can remove itself from the list.
+ 	 */
+ 	list_for_each_entry_safe(lip, n, &bp->b_li_list, li_bio_list) {
+ 		iip = (struct xfs_inode_log_item *)lip;
+ 		ip = iip->ili_inode;
++>>>>>>> 5717ea4d527a (xfs: rework xfs_iflush_cluster() dirty inode iteration)
+ 
+ 		/*
+ 		 * Quick and dirty check to avoid locks if possible.
+ 		 */
+ 		if (__xfs_iflags_test(ip, XFS_IRECLAIM | XFS_IFLOCK))
+ 			continue;
+ 		if (xfs_ipincount(ip))
+ 			continue;
+ 
+ 		/*
+ 		 * The inode is still attached to the buffer, which means it is
+ 		 * dirty but reclaim might try to grab it. Check carefully for
+ 		 * that, and grab the ilock while still holding the i_flags_lock
+ 		 * to guarantee reclaim will not be able to reclaim this inode
+ 		 * once we drop the i_flags_lock.
+ 		 */
+ 		spin_lock(&ip->i_flags_lock);
+ 		ASSERT(!__xfs_iflags_test(ip, XFS_ISTALE));
+ 		if (__xfs_iflags_test(ip, XFS_IRECLAIM | XFS_IFLOCK)) {
+ 			spin_unlock(&ip->i_flags_lock);
+ 			continue;
+ 		}
+ 
+ 		/*
+ 		 * ILOCK will pin the inode against reclaim and prevent
+ 		 * concurrent transactions modifying the inode while we are
+ 		 * flushing the inode.
+ 		 */
+ 		if (!xfs_ilock_nowait(ip, XFS_ILOCK_SHARED)) {
+ 			spin_unlock(&ip->i_flags_lock);
+ 			continue;
+ 		}
+ 		spin_unlock(&ip->i_flags_lock);
  
  		/*
- 		 * because this is an RCU protected lookup, we could find a
- 		 * recently freed or even reallocated inode during the lookup.
- 		 * We need to check under the i_flags_lock for a valid inode
- 		 * here. Skip it if it is not valid or the wrong inode.
+ 		 * Skip inodes that are already flush locked as they have
+ 		 * already been written to the buffer.
  		 */
- 		spin_lock(&cip->i_flags_lock);
- 		if (!cip->i_ino ||
- 		    __xfs_iflags_test(cip, XFS_ISTALE)) {
- 			spin_unlock(&cip->i_flags_lock);
+ 		if (!xfs_iflock_nowait(ip)) {
+ 			xfs_iunlock(ip, XFS_ILOCK_SHARED);
  			continue;
  		}
  
  		/*
- 		 * Once we fall off the end of the cluster, no point checking
- 		 * any more inodes in the list because they will also all be
- 		 * outside the cluster.
+ 		 * Abort flushing this inode if we are shut down because the
+ 		 * inode may not currently be in the AIL. This can occur when
+ 		 * log I/O failure unpins the inode without inserting into the
+ 		 * AIL, leaving a dirty/unpinned inode attached to the buffer
+ 		 * that otherwise looks like it should be flushed.
  		 */
- 		if ((XFS_INO_TO_AGINO(mp, cip->i_ino) & mask) != first_index) {
- 			spin_unlock(&cip->i_flags_lock);
+ 		if (XFS_FORCED_SHUTDOWN(mp)) {
+ 			xfs_iunpin_wait(ip);
+ 			/* xfs_iflush_abort() drops the flush lock */
+ 			xfs_iflush_abort(ip);
+ 			xfs_iunlock(ip, XFS_ILOCK_SHARED);
+ 			error = -EIO;
+ 			continue;
+ 		}
+ 
+ 		/* don't block waiting on a log force to unpin dirty inodes */
+ 		if (xfs_ipincount(ip)) {
+ 			xfs_ifunlock(ip);
+ 			xfs_iunlock(ip, XFS_ILOCK_SHARED);
+ 			continue;
+ 		}
+ 
+ 		if (!xfs_inode_clean(ip))
+ 			error = xfs_iflush(ip, bp);
+ 		else
+ 			xfs_ifunlock(ip);
+ 		xfs_iunlock(ip, XFS_ILOCK_SHARED);
+ 		if (error)
  			break;
++<<<<<<< HEAD
 +		}
 +		spin_unlock(&cip->i_flags_lock);
 +
 +		/*
 +		 * Do an un-protected check to see if the inode is dirty and
 +		 * is a candidate for flushing.  These checks will be repeated
 +		 * later after the appropriate locks are acquired.
 +		 */
 +		if (xfs_inode_clean(cip) && xfs_ipincount(cip) == 0)
 +			continue;
 +
 +		/*
 +		 * Try to get locks.  If any are unavailable or it is pinned,
 +		 * then this inode cannot be flushed and is skipped.
 +		 */
 +
 +		if (!xfs_ilock_nowait(cip, XFS_ILOCK_SHARED))
 +			continue;
 +		if (!xfs_iflock_nowait(cip)) {
 +			xfs_iunlock(cip, XFS_ILOCK_SHARED);
 +			continue;
 +		}
 +		if (xfs_ipincount(cip)) {
 +			xfs_ifunlock(cip);
 +			xfs_iunlock(cip, XFS_ILOCK_SHARED);
 +			continue;
 +		}
 +
 +
 +		/*
 +		 * Check the inode number again, just to be certain we are not
 +		 * racing with freeing in xfs_reclaim_inode(). See the comments
 +		 * in that function for more information as to why the initial
 +		 * check is not sufficient.
 +		 */
 +		if (!cip->i_ino) {
 +			xfs_ifunlock(cip);
 +			xfs_iunlock(cip, XFS_ILOCK_SHARED);
 +			continue;
 +		}
 +
 +		/*
 +		 * arriving here means that this inode can be flushed.  First
 +		 * re-check that it's dirty before flushing.
 +		 */
 +		if (!xfs_inode_clean(cip)) {
 +			error = xfs_iflush_int(cip, bp);
 +			if (error) {
 +				xfs_iunlock(cip, XFS_ILOCK_SHARED);
 +				goto out_free;
 +			}
 +			clcount++;
 +		} else {
 +			xfs_ifunlock(cip);
 +		}
 +		xfs_iunlock(cip, XFS_ILOCK_SHARED);
 +	}
 +
 +	if (clcount) {
 +		XFS_STATS_INC(mp, xs_icluster_flushcnt);
 +		XFS_STATS_ADD(mp, xs_icluster_flushinode, clcount);
 +	}
 +
 +out_free:
 +	rcu_read_unlock();
 +	kmem_free(cilist);
 +out_put:
 +	xfs_perag_put(pag);
 +	return error;
 +}
 +
 +/*
 + * Flush dirty inode metadata into the backing buffer.
 + *
 + * The caller must have the inode lock and the inode flush lock held.  The
 + * inode lock will still be held upon return to the caller, and the inode
 + * flush lock will be released after the inode has reached the disk.
 + *
 + * The caller must write out the buffer returned in *bpp and release it.
 + */
 +int
 +xfs_iflush(
 +	struct xfs_inode	*ip,
 +	struct xfs_buf		**bpp)
 +{
 +	struct xfs_mount	*mp = ip->i_mount;
 +	struct xfs_buf		*bp = NULL;
 +	struct xfs_dinode	*dip;
 +	int			error;
 +
 +	XFS_STATS_INC(mp, xs_iflush_count);
 +
 +	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL|XFS_ILOCK_SHARED));
 +	ASSERT(xfs_isiflocked(ip));
 +	ASSERT(ip->i_d.di_format != XFS_DINODE_FMT_BTREE ||
 +	       ip->i_d.di_nextents > XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK));
 +
 +	*bpp = NULL;
 +
 +	xfs_iunpin_wait(ip);
 +
 +	/*
 +	 * For stale inodes we cannot rely on the backing buffer remaining
 +	 * stale in cache for the remaining life of the stale inode and so
 +	 * xfs_imap_to_bp() below may give us a buffer that no longer contains
 +	 * inodes below. We have to check this after ensuring the inode is
 +	 * unpinned so that it is safe to reclaim the stale inode after the
 +	 * flush call.
 +	 */
 +	if (xfs_iflags_test(ip, XFS_ISTALE)) {
 +		xfs_ifunlock(ip);
 +		return 0;
 +	}
 +
 +	/*
 +	 * Get the buffer containing the on-disk inode. We are doing a try-lock
 +	 * operation here, so we may get an EAGAIN error. In that case, return
 +	 * leaving the inode dirty.
 +	 *
 +	 * If we get any other error, we effectively have a corruption situation
 +	 * and we cannot flush the inode. Abort the flush and shut down.
 +	 */
 +	error = xfs_imap_to_bp(mp, NULL, &ip->i_imap, &dip, &bp, XBF_TRYLOCK);
 +	if (error == -EAGAIN) {
 +		xfs_ifunlock(ip);
 +		return error;
 +	}
 +	if (error)
 +		goto abort;
 +
 +	/*
 +	 * If the buffer is pinned then push on the log now so we won't
 +	 * get stuck waiting in the write for too long.
 +	 */
 +	if (xfs_buf_ispinned(bp))
 +		xfs_log_force(mp, 0);
 +
 +	/*
 +	 * Flush the provided inode then attempt to gather others from the
 +	 * cluster into the write.
 +	 *
 +	 * Note: Once we attempt to flush an inode, we must run buffer
 +	 * completion callbacks on any failure. If this fails, simulate an I/O
 +	 * failure on the buffer and shut down.
 +	 */
 +	error = xfs_iflush_int(ip, bp);
 +	if (!error)
 +		error = xfs_iflush_cluster(ip, bp);
 +	if (error) {
 +		bp->b_flags |= XBF_ASYNC;
 +		xfs_buf_ioend_fail(bp);
 +		goto shutdown;
 +	}
 +
 +	*bpp = bp;
 +	return 0;
 +
 +abort:
 +	xfs_iflush_abort(ip);
 +shutdown:
 +	xfs_force_shutdown(mp, SHUTDOWN_CORRUPT_INCORE);
 +	return error;
 +}
 +
 +STATIC int
 +xfs_iflush_int(
 +	struct xfs_inode	*ip,
 +	struct xfs_buf		*bp)
 +{
 +	struct xfs_inode_log_item *iip = ip->i_itemp;
 +	struct xfs_dinode	*dip;
 +	struct xfs_mount	*mp = ip->i_mount;
 +	int			error;
 +
 +	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL|XFS_ILOCK_SHARED));
 +	ASSERT(xfs_isiflocked(ip));
 +	ASSERT(ip->i_d.di_format != XFS_DINODE_FMT_BTREE ||
 +	       ip->i_d.di_nextents > XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK));
 +	ASSERT(iip != NULL && iip->ili_fields != 0);
 +
 +	dip = xfs_buf_offset(bp, ip->i_imap.im_boffset);
 +
 +	/*
 +	 * We don't flush the inode if any of the following checks fail, but we
 +	 * do still update the log item and attach to the backing buffer as if
 +	 * the flush happened. This is a formality to facilitate predictable
 +	 * error handling as the caller will shutdown and fail the buffer.
 +	 */
 +	error = -EFSCORRUPTED;
 +	if (XFS_TEST_ERROR(dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC),
 +			       mp, XFS_ERRTAG_IFLUSH_1)) {
 +		xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
 +			"%s: Bad inode %Lu magic number 0x%x, ptr "PTR_FMT,
 +			__func__, ip->i_ino, be16_to_cpu(dip->di_magic), dip);
 +		goto flush_out;
 +	}
 +	if (S_ISREG(VFS_I(ip)->i_mode)) {
 +		if (XFS_TEST_ERROR(
 +		    (ip->i_d.di_format != XFS_DINODE_FMT_EXTENTS) &&
 +		    (ip->i_d.di_format != XFS_DINODE_FMT_BTREE),
 +		    mp, XFS_ERRTAG_IFLUSH_3)) {
 +			xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
 +				"%s: Bad regular inode %Lu, ptr "PTR_FMT,
 +				__func__, ip->i_ino, ip);
 +			goto flush_out;
 +		}
 +	} else if (S_ISDIR(VFS_I(ip)->i_mode)) {
 +		if (XFS_TEST_ERROR(
 +		    (ip->i_d.di_format != XFS_DINODE_FMT_EXTENTS) &&
 +		    (ip->i_d.di_format != XFS_DINODE_FMT_BTREE) &&
 +		    (ip->i_d.di_format != XFS_DINODE_FMT_LOCAL),
 +		    mp, XFS_ERRTAG_IFLUSH_4)) {
 +			xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
 +				"%s: Bad directory inode %Lu, ptr "PTR_FMT,
 +				__func__, ip->i_ino, ip);
 +			goto flush_out;
 +		}
 +	}
 +	if (XFS_TEST_ERROR(ip->i_d.di_nextents + ip->i_d.di_anextents >
 +				ip->i_d.di_nblocks, mp, XFS_ERRTAG_IFLUSH_5)) {
 +		xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
 +			"%s: detected corrupt incore inode %Lu, "
 +			"total extents = %d, nblocks = %Ld, ptr "PTR_FMT,
 +			__func__, ip->i_ino,
 +			ip->i_d.di_nextents + ip->i_d.di_anextents,
 +			ip->i_d.di_nblocks, ip);
 +		goto flush_out;
 +	}
 +	if (XFS_TEST_ERROR(ip->i_d.di_forkoff > mp->m_sb.sb_inodesize,
 +				mp, XFS_ERRTAG_IFLUSH_6)) {
 +		xfs_alert_tag(mp, XFS_PTAG_IFLUSH,
 +			"%s: bad inode %Lu, forkoff 0x%x, ptr "PTR_FMT,
 +			__func__, ip->i_ino, ip->i_d.di_forkoff, ip);
 +		goto flush_out;
 +	}
 +
 +	/*
 +	 * Inode item log recovery for v2 inodes are dependent on the
 +	 * di_flushiter count for correct sequencing. We bump the flush
 +	 * iteration count so we can detect flushes which postdate a log record
 +	 * during recovery. This is redundant as we now log every change and
 +	 * hence this can't happen but we need to still do it to ensure
 +	 * backwards compatibility with old kernels that predate logging all
 +	 * inode changes.
 +	 */
 +	if (!xfs_sb_version_has_v3inode(&mp->m_sb))
 +		ip->i_d.di_flushiter++;
 +
 +	/*
 +	 * If there are inline format data / attr forks attached to this inode,
 +	 * make sure they are not corrupt.
 +	 */
 +	if (ip->i_d.di_format == XFS_DINODE_FMT_LOCAL &&
 +	    xfs_ifork_verify_local_data(ip))
 +		goto flush_out;
 +	if (ip->i_d.di_aformat == XFS_DINODE_FMT_LOCAL &&
 +	    xfs_ifork_verify_local_attr(ip))
 +		goto flush_out;
 +
 +	/*
 +	 * Copy the dirty parts of the inode into the on-disk inode.  We always
 +	 * copy out the core of the inode, because if the inode is dirty at all
 +	 * the core must be.
 +	 */
 +	xfs_inode_to_disk(ip, dip, iip->ili_item.li_lsn);
 +
 +	/* Wrap, we never let the log put out DI_MAX_FLUSH */
 +	if (ip->i_d.di_flushiter == DI_MAX_FLUSH)
 +		ip->i_d.di_flushiter = 0;
 +
 +	xfs_iflush_fork(ip, dip, iip, XFS_DATA_FORK);
 +	if (XFS_IFORK_Q(ip))
 +		xfs_iflush_fork(ip, dip, iip, XFS_ATTR_FORK);
 +	xfs_inobp_check(mp, bp);
 +
 +	/*
 +	 * We've recorded everything logged in the inode, so we'd like to clear
 +	 * the ili_fields bits so we don't log and flush things unnecessarily.
 +	 * However, we can't stop logging all this information until the data
 +	 * we've copied into the disk buffer is written to disk.  If we did we
 +	 * might overwrite the copy of the inode in the log with all the data
 +	 * after re-logging only part of it, and in the face of a crash we
 +	 * wouldn't have all the data we need to recover.
 +	 *
 +	 * What we do is move the bits to the ili_last_fields field.  When
 +	 * logging the inode, these bits are moved back to the ili_fields field.
 +	 * In the xfs_iflush_done() routine we clear ili_last_fields, since we
 +	 * know that the information those bits represent is permanently on
 +	 * disk.  As long as the flush completes before the inode is logged
 +	 * again, then both ili_fields and ili_last_fields will be cleared.
 +	 */
 +	error = 0;
 +flush_out:
 +	spin_lock(&iip->ili_lock);
 +	iip->ili_last_fields = iip->ili_fields;
 +	iip->ili_fields = 0;
 +	iip->ili_fsync_fields = 0;
 +	spin_unlock(&iip->ili_lock);
 +
 +	/*
 +	 * Store the current LSN of the inode so that we can tell whether the
 +	 * item has moved in the AIL from xfs_iflush_done().
 +	 */
 +	xfs_trans_ail_copy_lsn(mp->m_ail, &iip->ili_flush_lsn,
 +				&iip->ili_item.li_lsn);
 +
 +	/* generate the checksum. */
 +	xfs_dinode_calc_crc(mp, dip);
 +	return error;
++=======
+ 		clcount++;
+ 	}
+ 
+ 	if (error) {
+ 		bp->b_flags |= XBF_ASYNC;
+ 		xfs_buf_ioend_fail(bp);
+ 		xfs_force_shutdown(mp, SHUTDOWN_CORRUPT_INCORE);
+ 		return error;
+ 	}
+ 
+ 	if (!clcount)
+ 		return -EAGAIN;
+ 
+ 	XFS_STATS_INC(mp, xs_icluster_flushcnt);
+ 	XFS_STATS_ADD(mp, xs_icluster_flushinode, clcount);
+ 	return 0;
+ 
++>>>>>>> 5717ea4d527a (xfs: rework xfs_iflush_cluster() dirty inode iteration)
  }
  
  /* Release an inode. */
diff --cc fs/xfs/xfs_inode.h
index f9a8b26c3625,e9a8bb184d1f..000000000000
--- a/fs/xfs/xfs_inode.h
+++ b/fs/xfs/xfs_inode.h
@@@ -429,7 -426,7 +429,11 @@@ int		xfs_log_force_inode(struct xfs_ino
  void		xfs_iunpin_wait(xfs_inode_t *);
  #define xfs_ipincount(ip)	((unsigned int) atomic_read(&ip->i_pincount))
  
++<<<<<<< HEAD
 +int		xfs_iflush(struct xfs_inode *, struct xfs_buf **);
++=======
+ int		xfs_iflush_cluster(struct xfs_buf *);
++>>>>>>> 5717ea4d527a (xfs: rework xfs_iflush_cluster() dirty inode iteration)
  void		xfs_lock_two_inodes(struct xfs_inode *ip0, uint ip0_mode,
  				struct xfs_inode *ip1, uint ip1_mode);
  
diff --cc fs/xfs/xfs_inode_item.c
index 92cf5780c28a,4e7fce8d4f7c..000000000000
--- a/fs/xfs/xfs_inode_item.c
+++ b/fs/xfs/xfs_inode_item.c
@@@ -485,53 -485,44 +485,68 @@@ xfs_inode_item_push
  	uint			rval = XFS_ITEM_SUCCESS;
  	int			error;
  
 -	ASSERT(iip->ili_item.li_buf);
 -
 -	if (xfs_ipincount(ip) > 0 || xfs_buf_ispinned(bp) ||
 -	    (ip->i_flags & XFS_ISTALE))
 +	if (xfs_ipincount(ip) > 0)
  		return XFS_ITEM_PINNED;
  
 -	/* If the inode is already flush locked, we're already flushing. */
 -	if (xfs_isiflocked(ip))
 -		return XFS_ITEM_FLUSHING;
 -
 -	if (!xfs_buf_trylock(bp))
 +	if (!xfs_ilock_nowait(ip, XFS_ILOCK_SHARED))
  		return XFS_ITEM_LOCKED;
  
 -	spin_unlock(&lip->li_ailp->ail_lock);
 +	/*
 +	 * Re-check the pincount now that we stabilized the value by
 +	 * taking the ilock.
 +	 */
 +	if (xfs_ipincount(ip) > 0) {
 +		rval = XFS_ITEM_PINNED;
 +		goto out_unlock;
 +	}
  
  	/*
 -	 * We need to hold a reference for flushing the cluster buffer as it may
 -	 * fail the buffer without IO submission. In which case, we better get a
 -	 * reference for that completion because otherwise we don't get a
 -	 * reference for IO until we queue the buffer for delwri submission.
 +	 * Stale inode items should force out the iclog.
  	 */
++<<<<<<< HEAD
 +	if (ip->i_flags & XFS_ISTALE) {
 +		rval = XFS_ITEM_PINNED;
 +		goto out_unlock;
 +	}
 +
 +	/*
 +	 * Someone else is already flushing the inode.  Nothing we can do
 +	 * here but wait for the flush to finish and remove the item from
 +	 * the AIL.
 +	 */
 +	if (!xfs_iflock_nowait(ip)) {
 +		rval = XFS_ITEM_FLUSHING;
 +		goto out_unlock;
 +	}
 +
 +	ASSERT(iip->ili_fields != 0 || XFS_FORCED_SHUTDOWN(ip->i_mount));
 +	spin_unlock(&lip->li_ailp->ail_lock);
 +
 +	error = xfs_iflush(ip, &bp);
++=======
+ 	xfs_buf_hold(bp);
+ 	error = xfs_iflush_cluster(bp);
++>>>>>>> 5717ea4d527a (xfs: rework xfs_iflush_cluster() dirty inode iteration)
  	if (!error) {
  		if (!xfs_buf_delwri_queue(bp, buffer_list))
  			rval = XFS_ITEM_FLUSHING;
  		xfs_buf_relse(bp);
++<<<<<<< HEAD
 +	} else if (error == -EAGAIN)
++=======
+ 	} else {
+ 		/*
+ 		 * Release the buffer if we were unable to flush anything. On
+ 		 * any other error, the buffer has already been released.
+ 		 */
+ 		if (error == -EAGAIN)
+ 			xfs_buf_relse(bp);
++>>>>>>> 5717ea4d527a (xfs: rework xfs_iflush_cluster() dirty inode iteration)
  		rval = XFS_ITEM_LOCKED;
 -	}
  
  	spin_lock(&lip->li_ailp->ail_lock);
 +out_unlock:
 +	xfs_iunlock(ip, XFS_ILOCK_SHARED);
  	return rval;
  }
  
* Unmerged path fs/xfs/xfs_inode.c
* Unmerged path fs/xfs/xfs_inode.h
* Unmerged path fs/xfs/xfs_inode_item.c
