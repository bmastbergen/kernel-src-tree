RDMA/mlx5: Use mlx5_umem_find_best_quantized_pgoff() for QP

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit a59b7b05efc827929c2aa46fc2fe561982bd19fc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/a59b7b05.failed

Delete custom logic in the QP in favor of more general variant.

Link: https://lore.kernel.org/r/20201115114311.136250-5-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit a59b7b05efc827929c2aa46fc2fe561982bd19fc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mem.c
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/mem.c
index 13de3d2edd34,ad9830eeb4b6..000000000000
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@@ -165,33 -107,50 +165,36 @@@ void __mlx5_ib_populate_pas(struct mlx5
  	}
  }
  
 -/*
 - * Compute the page shift and page_offset for mailboxes that use a quantized
 - * page_offset. The granulatity of the page offset scales according to page
 - * size.
 - */
 -unsigned long __mlx5_umem_find_best_quantized_pgoff(
 -	struct ib_umem *umem, unsigned long pgsz_bitmap,
 -	unsigned int page_offset_bits, u64 pgoff_bitmask, unsigned int scale,
 -	unsigned int *page_offset_quantized)
 +void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			  int page_shift, __be64 *pas, int access_flags)
  {
 -	const u64 page_offset_mask = (1 << page_offset_bits) - 1;
 -	unsigned long page_size;
 -	u64 page_offset;
 -
 -	page_size = ib_umem_find_best_pgoff(umem, pgsz_bitmap, pgoff_bitmask);
 -	if (!page_size)
 -		return 0;
 -
 -	/*
 -	 * page size is the largest possible page size.
 -	 *
 -	 * Reduce the page_size, and thus the page_offset and quanta, until the
 -	 * page_offset fits into the mailbox field. Once page_size < scale this
 -	 * loop is guaranteed to terminate.
 -	 */
 -	page_offset = ib_umem_dma_offset(umem, page_size);
 -	while (page_offset & ~(u64)(page_offset_mask * (page_size / scale))) {
 -		page_size /= 2;
 -		page_offset = ib_umem_dma_offset(umem, page_size);
 -	}
 -
 -	/*
 -	 * The address is not aligned, or otherwise cannot be represented by the
 -	 * page_offset.
 -	 */
 -	if (!(pgsz_bitmap & page_size))
 -		return 0;
 -
 -	*page_offset_quantized =
 -		(unsigned long)page_offset / (page_size / scale);
 -	if (WARN_ON(*page_offset_quantized > page_offset_mask))
 -		return 0;
 -	return page_size;
 +	return __mlx5_ib_populate_pas(dev, umem, page_shift, 0,
 +				      ib_umem_num_dma_blocks(umem, PAGE_SIZE),
 +				      pas, access_flags);
 +}
++<<<<<<< HEAD
 +int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset)
 +{
 +	u64 page_size;
 +	u64 page_mask;
 +	u64 off_size;
 +	u64 off_mask;
 +	u64 buf_off;
 +
 +	page_size = (u64)1 << page_shift;
 +	page_mask = page_size - 1;
 +	buf_off = addr & page_mask;
 +	off_size = page_size >> 6;
 +	off_mask = off_size - 1;
 +
 +	if (buf_off & off_mask)
 +		return -EINVAL;
 +
 +	*offset = buf_off >> ilog2(off_size);
 +	return 0;
  }
++=======
++>>>>>>> a59b7b05efc8 (RDMA/mlx5: Use mlx5_umem_find_best_quantized_pgoff() for QP)
  
  #define WR_ID_BF 0xBF
  #define WR_ID_END 0xBAD
diff --cc drivers/infiniband/hw/mlx5/qp.c
index dfd5c890ce96,1ff156cfb5b1..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -778,39 -778,6 +778,42 @@@ int bfregn_to_uar_index(struct mlx5_ib_
  	return bfregi->sys_pages[index_of_sys_page] + offset;
  }
  
++<<<<<<< HEAD
 +static int mlx5_ib_umem_get(struct mlx5_ib_dev *dev, struct ib_udata *udata,
 +			    unsigned long addr, size_t size,
 +			    struct ib_umem **umem, int *npages, int *page_shift,
 +			    int *ncont, u32 *offset)
 +{
 +	int err;
 +
 +	*umem = ib_umem_get(udata, addr, size, 0);
 +	if (IS_ERR(*umem)) {
 +		mlx5_ib_dbg(dev, "umem_get failed\n");
 +		return PTR_ERR(*umem);
 +	}
 +
 +	mlx5_ib_cont_pages(*umem, addr, 0, npages, page_shift, ncont, NULL);
 +
 +	err = mlx5_ib_get_buf_offset(addr, *page_shift, offset);
 +	if (err) {
 +		mlx5_ib_warn(dev, "bad offset\n");
 +		goto err_umem;
 +	}
 +
 +	mlx5_ib_dbg(dev, "addr 0x%lx, size %zu, npages %d, page_shift %d, ncont %d, offset %d\n",
 +		    addr, size, *npages, *page_shift, *ncont, *offset);
 +
 +	return 0;
 +
 +err_umem:
 +	ib_umem_release(*umem);
 +	*umem = NULL;
 +
 +	return err;
 +}
 +
++=======
++>>>>>>> a59b7b05efc8 (RDMA/mlx5: Use mlx5_umem_find_best_quantized_pgoff() for QP)
  static void destroy_user_rq(struct mlx5_ib_dev *dev, struct ib_pd *pd,
  			    struct mlx5_ib_rwq *rwq, struct ib_udata *udata)
  {
@@@ -896,10 -864,9 +899,14 @@@ static int _create_user_qp(struct mlx5_
  {
  	struct mlx5_ib_ucontext *context;
  	struct mlx5_ib_ubuffer *ubuffer = &base->ubuffer;
- 	int page_shift = 0;
+ 	unsigned int page_offset_quantized = 0;
+ 	unsigned long page_size = 0;
  	int uar_index = 0;
++<<<<<<< HEAD
 +	int npages;
 +	u32 offset = 0;
++=======
++>>>>>>> a59b7b05efc8 (RDMA/mlx5: Use mlx5_umem_find_best_quantized_pgoff() for QP)
  	int bfregn;
  	int ncont = 0;
  	__be64 *pas;
@@@ -950,11 -917,21 +957,29 @@@
  
  	if (ucmd->buf_addr && ubuffer->buf_size) {
  		ubuffer->buf_addr = ucmd->buf_addr;
++<<<<<<< HEAD
 +		err = mlx5_ib_umem_get(dev, udata, ubuffer->buf_addr,
 +				       ubuffer->buf_size, &ubuffer->umem,
 +				       &npages, &page_shift, &ncont, &offset);
 +		if (err)
 +			goto err_bfreg;
++=======
+ 		ubuffer->umem = ib_umem_get(&dev->ib_dev, ubuffer->buf_addr,
+ 					    ubuffer->buf_size, 0);
+ 		if (IS_ERR(ubuffer->umem)) {
+ 			err = PTR_ERR(ubuffer->umem);
+ 			goto err_bfreg;
+ 		}
+ 		page_size = mlx5_umem_find_best_quantized_pgoff(
+ 			ubuffer->umem, qpc, log_page_size,
+ 			MLX5_ADAPTER_PAGE_SHIFT, page_offset, 64,
+ 			&page_offset_quantized);
+ 		if (!page_size) {
+ 			err = -EINVAL;
+ 			goto err_umem;
+ 		}
+ 		ncont = ib_umem_num_dma_blocks(ubuffer->umem, page_size);
++>>>>>>> a59b7b05efc8 (RDMA/mlx5: Use mlx5_umem_find_best_quantized_pgoff() for QP)
  	} else {
  		ubuffer->umem = NULL;
  	}
@@@ -969,15 -946,14 +994,21 @@@
  
  	uid = (attr->qp_type != IB_QPT_XRC_INI) ? to_mpd(pd)->uid : 0;
  	MLX5_SET(create_qp_in, *in, uid, uid);
++<<<<<<< HEAD
 +	pas = (__be64 *)MLX5_ADDR_OF(create_qp_in, *in, pas);
 +	if (ubuffer->umem)
 +		mlx5_ib_populate_pas(dev, ubuffer->umem, page_shift, pas, 0);
 +
++=======
++>>>>>>> a59b7b05efc8 (RDMA/mlx5: Use mlx5_umem_find_best_quantized_pgoff() for QP)
  	qpc = MLX5_ADDR_OF(create_qp_in, *in, qpc);
- 
- 	MLX5_SET(qpc, qpc, log_page_size, page_shift - MLX5_ADAPTER_PAGE_SHIFT);
- 	MLX5_SET(qpc, qpc, page_offset, offset);
- 
+ 	pas = (__be64 *)MLX5_ADDR_OF(create_qp_in, *in, pas);
+ 	if (ubuffer->umem) {
+ 		mlx5_ib_populate_pas(ubuffer->umem, page_size, pas, 0);
+ 		MLX5_SET(qpc, qpc, log_page_size,
+ 			 order_base_2(page_size) - MLX5_ADAPTER_PAGE_SHIFT);
+ 		MLX5_SET(qpc, qpc, page_offset, page_offset_quantized);
+ 	}
  	MLX5_SET(qpc, qpc, uar_page, uar_index);
  	if (bfregn != MLX5_IB_INVALID_BFREG)
  		resp->bfreg_index = adjust_bfregn(dev, &context->bfregi, bfregn);
* Unmerged path drivers/infiniband/hw/mlx5/mem.c
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index adddf68c9489..6c7e25f212dc 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1212,7 +1212,6 @@ int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			size_t *out_mad_size, u16 *out_mad_pkey_index);
 int mlx5_ib_alloc_xrcd(struct ib_xrcd *xrcd, struct ib_udata *udata);
 int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd, struct ib_udata *udata);
-int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset);
 int mlx5_query_ext_port_caps(struct mlx5_ib_dev *dev, u8 port);
 int mlx5_query_mad_ifc_smp_attr_node_info(struct ib_device *ibdev,
 					  struct ib_smp *out_mad);
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
