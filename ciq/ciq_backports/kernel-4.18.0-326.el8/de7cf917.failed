dma-mapping: add new {alloc,free}_noncoherent dma_map_ops methods

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit de7cf917768f438aae6d2f4e9bced3739f15f5b6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/de7cf917.failed

This will allow IOMMU drivers to allocate non-contigous memory and
return a vmapped virtual address.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit de7cf917768f438aae6d2f4e9bced3739f15f5b6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-mapping.h
#	kernel/dma/mapping.c
diff --cc include/linux/dma-mapping.h
index ba0c1c793dd8,7c77cd6f3604..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -83,6 -69,16 +83,19 @@@ struct dma_map_ops 
  	void (*free)(struct device *dev, size_t size,
  			      void *vaddr, dma_addr_t dma_handle,
  			      unsigned long attrs);
++<<<<<<< HEAD
++=======
+ 	struct page *(*alloc_pages)(struct device *dev, size_t size,
+ 			dma_addr_t *dma_handle, enum dma_data_direction dir,
+ 			gfp_t gfp);
+ 	void (*free_pages)(struct device *dev, size_t size, struct page *vaddr,
+ 			dma_addr_t dma_handle, enum dma_data_direction dir);
+ 	void* (*alloc_noncoherent)(struct device *dev, size_t size,
+ 			dma_addr_t *dma_handle, enum dma_data_direction dir,
+ 			gfp_t gfp);
+ 	void (*free_noncoherent)(struct device *dev, size_t size, void *vaddr,
+ 			dma_addr_t dma_handle, enum dma_data_direction dir);
++>>>>>>> de7cf917768f (dma-mapping: add new {alloc,free}_noncoherent dma_map_ops methods)
  	int (*mmap)(struct device *, struct vm_area_struct *,
  			  void *, dma_addr_t, size_t,
  			  unsigned long attrs);
diff --cc kernel/dma/mapping.c
index 0597cdaee492,9669550656a0..000000000000
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@@ -528,6 -470,86 +528,89 @@@ void dma_free_attrs(struct device *dev
  }
  EXPORT_SYMBOL(dma_free_attrs);
  
++<<<<<<< HEAD
++=======
+ struct page *dma_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 	struct page *page;
+ 
+ 	if (WARN_ON_ONCE(!dev->coherent_dma_mask))
+ 		return NULL;
+ 	if (WARN_ON_ONCE(gfp & (__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM)))
+ 		return NULL;
+ 
+ 	size = PAGE_ALIGN(size);
+ 	if (dma_alloc_direct(dev, ops))
+ 		page = dma_direct_alloc_pages(dev, size, dma_handle, dir, gfp);
+ 	else if (ops->alloc_pages)
+ 		page = ops->alloc_pages(dev, size, dma_handle, dir, gfp);
+ 	else
+ 		return NULL;
+ 
+ 	debug_dma_map_page(dev, page, 0, size, dir, *dma_handle);
+ 
+ 	return page;
+ }
+ EXPORT_SYMBOL_GPL(dma_alloc_pages);
+ 
+ void dma_free_pages(struct device *dev, size_t size, struct page *page,
+ 		dma_addr_t dma_handle, enum dma_data_direction dir)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	size = PAGE_ALIGN(size);
+ 	debug_dma_unmap_page(dev, dma_handle, size, dir);
+ 
+ 	if (dma_alloc_direct(dev, ops))
+ 		dma_direct_free_pages(dev, size, page, dma_handle, dir);
+ 	else if (ops->free_pages)
+ 		ops->free_pages(dev, size, page, dma_handle, dir);
+ }
+ EXPORT_SYMBOL_GPL(dma_free_pages);
+ 
+ void *dma_alloc_noncoherent(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 	void *vaddr;
+ 
+ 	if (!ops || !ops->alloc_noncoherent) {
+ 		struct page *page;
+ 
+ 		page = dma_alloc_pages(dev, size, dma_handle, dir, gfp);
+ 		if (!page)
+ 			return NULL;
+ 		return page_address(page);
+ 	}
+ 
+ 	size = PAGE_ALIGN(size);
+ 	vaddr = ops->alloc_noncoherent(dev, size, dma_handle, dir, gfp);
+ 	if (vaddr)
+ 		debug_dma_map_page(dev, virt_to_page(vaddr), 0, size, dir,
+ 				   *dma_handle);
+ 	return vaddr;
+ }
+ EXPORT_SYMBOL_GPL(dma_alloc_noncoherent);
+ 
+ void dma_free_noncoherent(struct device *dev, size_t size, void *vaddr,
+ 		dma_addr_t dma_handle, enum dma_data_direction dir)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (!ops || !ops->free_noncoherent) {
+ 		dma_free_pages(dev, size, virt_to_page(vaddr), dma_handle, dir);
+ 		return;
+ 	}
+ 
+ 	size = PAGE_ALIGN(size);
+ 	debug_dma_unmap_page(dev, dma_handle, size, dir);
+ 	ops->free_noncoherent(dev, size, vaddr, dma_handle, dir);
+ }
+ EXPORT_SYMBOL_GPL(dma_free_noncoherent);
+ 
++>>>>>>> de7cf917768f (dma-mapping: add new {alloc,free}_noncoherent dma_map_ops methods)
  int dma_supported(struct device *dev, u64 mask)
  {
  	const struct dma_map_ops *ops = get_dma_ops(dev);
* Unmerged path include/linux/dma-mapping.h
* Unmerged path kernel/dma/mapping.c
