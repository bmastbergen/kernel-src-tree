dma-mapping: add a dma_mmap_pages helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit eedb0b12d091a21909b5e84d9f3e5e649305bd12
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/eedb0b12.failed

Add a helper to map memory allocated using dma_alloc_pages into
a user address space, similar to the dma_alloc_attrs function for
coherent allocations.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Tomasz Figa <tfiga@chromium.org>
	Tested-by: Ricardo Ribalda <ribalda@chromium.org>
(cherry picked from commit eedb0b12d091a21909b5e84d9f3e5e649305bd12)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/core-api/dma-api.rst
#	include/linux/dma-mapping.h
#	kernel/dma/mapping.c
diff --cc Documentation/core-api/dma-api.rst
index f7c065be590b,157a474ae544..000000000000
--- a/Documentation/core-api/dma-api.rst
+++ b/Documentation/core-api/dma-api.rst
@@@ -600,9 -560,39 +600,45 @@@ reused
  			dma_addr_t dma_handle, enum dma_data_direction dir)
  
  Free a region of memory previously allocated using dma_alloc_pages().
++<<<<<<< HEAD
 +dev, size and dma_handle and dir must all be the same as those passed into
 +dma_alloc_noncoherent().  page must be the pointer returned by
 +dma_alloc_pages().
++=======
+ dev, size, dma_handle and dir must all be the same as those passed into
+ dma_alloc_pages().  page must be the pointer returned by dma_alloc_pages().
+ 
+ ::
+ 
+ 	int
+ 	dma_mmap_pages(struct device *dev, struct vm_area_struct *vma,
+ 		       size_t size, struct page *page)
+ 
+ Map an allocation returned from dma_alloc_pages() into a user address space.
+ dev and size must be the same as those passed into dma_alloc_pages().
+ page must be the pointer returned by dma_alloc_pages().
+ 
+ ::
+ 
+ 	void *
+ 	dma_alloc_noncoherent(struct device *dev, size_t size,
+ 			dma_addr_t *dma_handle, enum dma_data_direction dir,
+ 			gfp_t gfp)
+ 
+ This routine is a convenient wrapper around dma_alloc_pages that returns the
+ kernel virtual address for the allocated memory instead of the page structure.
+ 
+ ::
+ 
+ 	void
+ 	dma_free_noncoherent(struct device *dev, size_t size, void *cpu_addr,
+ 			dma_addr_t dma_handle, enum dma_data_direction dir)
+ 
+ Free a region of memory previously allocated using dma_alloc_noncoherent().
+ dev, size, dma_handle and dir must all be the same as those passed into
+ dma_alloc_noncoherent().  cpu_addr must be the virtual address returned by
+ dma_alloc_noncoherent().
++>>>>>>> eedb0b12d091 (dma-mapping: add a dma_mmap_pages helper)
  
  ::
  
diff --cc include/linux/dma-mapping.h
index ba0c1c793dd8,2b8dce756e1f..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -392,6 -259,13 +392,16 @@@ static inline unsigned long dma_get_mer
  }
  #endif /* CONFIG_HAS_DMA */
  
++<<<<<<< HEAD
++=======
+ struct page *dma_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp);
+ void dma_free_pages(struct device *dev, size_t size, struct page *page,
+ 		dma_addr_t dma_handle, enum dma_data_direction dir);
+ int dma_mmap_pages(struct device *dev, struct vm_area_struct *vma,
+ 		size_t size, struct page *page);
+ 
++>>>>>>> eedb0b12d091 (dma-mapping: add a dma_mmap_pages helper)
  static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
  		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)
  {
diff --cc kernel/dma/mapping.c
index 0597cdaee492,9ce86c77651c..000000000000
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@@ -528,6 -477,59 +528,62 @@@ void dma_free_attrs(struct device *dev
  }
  EXPORT_SYMBOL(dma_free_attrs);
  
++<<<<<<< HEAD
++=======
+ struct page *dma_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 	struct page *page;
+ 
+ 	if (WARN_ON_ONCE(!dev->coherent_dma_mask))
+ 		return NULL;
+ 	if (WARN_ON_ONCE(gfp & (__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM)))
+ 		return NULL;
+ 
+ 	size = PAGE_ALIGN(size);
+ 	if (dma_alloc_direct(dev, ops))
+ 		page = dma_direct_alloc_pages(dev, size, dma_handle, dir, gfp);
+ 	else if (ops->alloc_pages)
+ 		page = ops->alloc_pages(dev, size, dma_handle, dir, gfp);
+ 	else
+ 		return NULL;
+ 
+ 	debug_dma_map_page(dev, page, 0, size, dir, *dma_handle);
+ 
+ 	return page;
+ }
+ EXPORT_SYMBOL_GPL(dma_alloc_pages);
+ 
+ void dma_free_pages(struct device *dev, size_t size, struct page *page,
+ 		dma_addr_t dma_handle, enum dma_data_direction dir)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	size = PAGE_ALIGN(size);
+ 	debug_dma_unmap_page(dev, dma_handle, size, dir);
+ 
+ 	if (dma_alloc_direct(dev, ops))
+ 		dma_direct_free_pages(dev, size, page, dma_handle, dir);
+ 	else if (ops->free_pages)
+ 		ops->free_pages(dev, size, page, dma_handle, dir);
+ }
+ EXPORT_SYMBOL_GPL(dma_free_pages);
+ 
+ int dma_mmap_pages(struct device *dev, struct vm_area_struct *vma,
+ 		size_t size, struct page *page)
+ {
+ 	unsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 
+ 	if (vma->vm_pgoff >= count || vma_pages(vma) > count - vma->vm_pgoff)
+ 		return -ENXIO;
+ 	return remap_pfn_range(vma, vma->vm_start,
+ 			       page_to_pfn(page) + vma->vm_pgoff,
+ 			       vma_pages(vma) << PAGE_SHIFT, vma->vm_page_prot);
+ }
+ EXPORT_SYMBOL_GPL(dma_mmap_pages);
+ 
++>>>>>>> eedb0b12d091 (dma-mapping: add a dma_mmap_pages helper)
  int dma_supported(struct device *dev, u64 mask)
  {
  	const struct dma_map_ops *ops = get_dma_ops(dev);
* Unmerged path Documentation/core-api/dma-api.rst
* Unmerged path include/linux/dma-mapping.h
* Unmerged path kernel/dma/mapping.c
