dma-direct: factor out a dma_direct_alloc_from_pool helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 5b138c534fda57c2ebc1e6de72578aa1d70788a6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/5b138c53.failed

This ensures dma_direct_alloc_pages will use the right gfp mask, as
well as keeping the code for that common between the two allocators.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 5b138c534fda57c2ebc1e6de72578aa1d70788a6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index b992d9926af3,eddcb70251c9..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -291,6 -299,61 +298,64 @@@ void dma_direct_free(struct device *dev
  	dma_free_contiguous(dev, dma_direct_to_page(dev, dma_addr), size);
  }
  
++<<<<<<< HEAD
++=======
+ struct page *dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)
+ {
+ 	struct page *page;
+ 	void *ret;
+ 
+ 	if (dma_should_alloc_from_pool(dev, gfp, 0))
+ 		return dma_direct_alloc_from_pool(dev, size, dma_handle, gfp);
+ 
+ 	page = __dma_direct_alloc_pages(dev, size, gfp);
+ 	if (!page)
+ 		return NULL;
+ 	if (PageHighMem(page)) {
+ 		/*
+ 		 * Depending on the cma= arguments and per-arch setup
+ 		 * dma_alloc_contiguous could return highmem pages.
+ 		 * Without remapping there is no way to return them here,
+ 		 * so log an error and fail.
+ 		 */
+ 		dev_info(dev, "Rejecting highmem page from CMA.\n");
+ 		goto out_free_pages;
+ 	}
+ 
+ 	ret = page_address(page);
+ 	if (force_dma_unencrypted(dev)) {
+ 		if (set_memory_decrypted((unsigned long)ret,
+ 				1 << get_order(size)))
+ 			goto out_free_pages;
+ 	}
+ 	memset(ret, 0, size);
+ 	*dma_handle = phys_to_dma_direct(dev, page_to_phys(page));
+ 	return page;
+ out_free_pages:
+ 	dma_free_contiguous(dev, page, size);
+ 	return NULL;
+ }
+ 
+ void dma_direct_free_pages(struct device *dev, size_t size,
+ 		struct page *page, dma_addr_t dma_addr,
+ 		enum dma_data_direction dir)
+ {
+ 	unsigned int page_order = get_order(size);
+ 	void *vaddr = page_address(page);
+ 
+ 	/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */
+ 	if (dma_should_free_from_pool(dev, 0) &&
+ 	    dma_free_from_pool(dev, vaddr, size))
+ 		return;
+ 
+ 	if (force_dma_unencrypted(dev))
+ 		set_memory_encrypted((unsigned long)vaddr, 1 << page_order);
+ 
+ 	dma_free_contiguous(dev, page, size);
+ }
+ 
++>>>>>>> 5b138c534fda (dma-direct: factor out a dma_direct_alloc_from_pool helper)
  #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
      defined(CONFIG_SWIOTLB)
  void dma_direct_sync_sg_for_device(struct device *dev,
* Unmerged path kernel/dma/direct.c
