mm/hmm: do not set pfns when returning an error code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 53bfe17ff88faaadf024956e7cb2b295fae7744b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/53bfe17f.failed

Most places that return an error code, like -EFAULT, do not set
HMM_PFN_ERROR, only two places do this.

Resolve this inconsistency by never setting the pfns on an error
exit. This doesn't seem like a worthwhile thing to do anyhow.

If for some reason it becomes important, it makes more sense to directly
return the address of the failing page rather than have the caller scan
for the HMM_PFN_ERROR.

No caller inspects the pnfs output array if hmm_range_fault() fails.

Link: https://lore.kernel.org/r/20200327200021.29372-9-jgg@ziepe.ca
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 53bfe17ff88faaadf024956e7cb2b295fae7744b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index 3233a7437881,9b6a8a26a1fa..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -352,37 -73,33 +352,54 @@@ static int hmm_pfns_bad(unsigned long a
   * This function will be called whenever pmd_none() or pte_none() returns true,
   * or whenever there is no page directory covering the virtual address range.
   */
 -static int hmm_vma_fault(unsigned long addr, unsigned long end,
 -			 unsigned int required_fault, struct mm_walk *walk)
 +static int hmm_vma_walk_hole_(unsigned long addr, unsigned long end,
 +			      bool fault, bool write_fault,
 +			      struct mm_walk *walk)
  {
  	struct hmm_vma_walk *hmm_vma_walk = walk->private;
++<<<<<<< HEAD
 +	struct hmm_range *range = hmm_vma_walk->range;
 +	uint64_t *pfns = range->pfns;
 +	unsigned long i, page_size;
++=======
+ 	struct vm_area_struct *vma = walk->vma;
+ 	unsigned int fault_flags = FAULT_FLAG_REMOTE;
++>>>>>>> 53bfe17ff88f (mm/hmm: do not set pfns when returning an error code)
  
 -	WARN_ON_ONCE(!required_fault);
  	hmm_vma_walk->last = addr;
 -
 +	page_size = hmm_range_page_size(range);
 +	i = (addr - range->start) >> range->page_shift;
 +
++<<<<<<< HEAD
 +	for (; addr < end; addr += page_size, i++) {
 +		pfns[i] = range->values[HMM_PFN_NONE];
 +		if (fault || write_fault) {
 +			int ret;
++=======
+ 	if (!vma)
+ 		return -EFAULT;
++>>>>>>> 53bfe17ff88f (mm/hmm: do not set pfns when returning an error code)
  
 -	if (required_fault & HMM_NEED_WRITE_FAULT) {
 -		if (!(vma->vm_flags & VM_WRITE))
 -			return -EPERM;
 -		fault_flags |= FAULT_FLAG_WRITE;
 +			ret = hmm_vma_do_fault(walk, addr, write_fault,
 +					       &pfns[i]);
 +			if (ret != -EBUSY)
 +				return ret;
 +		}
  	}
  
++<<<<<<< HEAD
 +	return (fault || write_fault) ? -EBUSY : 0;
++=======
+ 	for (; addr < end; addr += PAGE_SIZE)
+ 		if (handle_mm_fault(vma, addr, fault_flags) & VM_FAULT_ERROR)
+ 			return -EFAULT;
+ 	return -EBUSY;
++>>>>>>> 53bfe17ff88f (mm/hmm: do not set pfns when returning an error code)
  }
  
 -static unsigned int hmm_pte_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
 -				       uint64_t pfns, uint64_t cpu_flags)
 +static inline void hmm_pte_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
 +				      uint64_t pfns, uint64_t cpu_flags,
 +				      bool *fault, bool *write_fault)
  {
  	struct hmm_range *range = hmm_vma_walk->range;
  
@@@ -926,114 -546,40 +942,119 @@@ static const struct mm_walk_ops hmm_wal
  	.pmd_entry	= hmm_vma_walk_pmd,
  	.pte_hole	= hmm_vma_walk_hole,
  	.hugetlb_entry	= hmm_vma_walk_hugetlb_entry,
 -	.test_walk	= hmm_vma_walk_test,
  };
  
 -/**
 - * hmm_range_fault - try to fault some address in a virtual address range
 - * @range:	argument structure
 +/*
 + * hmm_range_snapshot() - snapshot CPU page table for a range
 + * @range: range
 + * Return: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 + *          permission (for instance asking for write and range is read only),
 + *          -EBUSY if you need to retry, -EFAULT invalid (ie either no valid
 + *          vma or it is illegal to access that range), number of valid pages
 + *          in range->pfns[] (from range start address).
   *
 - * Return: the number of valid pages in range->pfns[] (from range start
 - * address), which may be zero.  On error one of the following status codes
 - * can be returned:
 + * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 + * validity is tracked by range struct. See in include/linux/hmm.h for example
 + * on how to use.
 + */
 +long hmm_range_snapshot(struct hmm_range *range)
 +{
 +	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 +	unsigned long start = range->start, end;
 +	struct hmm_vma_walk hmm_vma_walk;
 +	struct hmm *hmm = range->hmm;
 +	struct vm_area_struct *vma;
 +
 +	lockdep_assert_held(&hmm->mm->mmap_sem);
 +	do {
 +		/* If range is no longer valid force retry. */
 +		if (!range->valid)
 +			return -EBUSY;
 +
 +		vma = find_vma(hmm->mm, start);
 +		if (vma == NULL || (vma->vm_flags & device_vma))
 +			return -EFAULT;
 +
 +		if (is_vm_hugetlb_page(vma)) {
 +			if (huge_page_shift(hstate_vma(vma)) !=
 +				    range->page_shift &&
 +			    range->page_shift != PAGE_SHIFT)
 +				return -EINVAL;
 +		} else {
 +			if (range->page_shift != PAGE_SHIFT)
 +				return -EINVAL;
 +		}
 +
 +		if (!(vma->vm_flags & VM_READ)) {
 +			/*
 +			 * If vma do not allow read access, then assume that it
 +			 * does not allow write access, either. HMM does not
 +			 * support architecture that allow write without read.
 +			 */
 +			hmm_pfns_clear(range, range->pfns,
 +				range->start, range->end);
 +			return -EPERM;
 +		}
 +
 +		range->vma = vma;
 +		hmm_vma_walk.pgmap = NULL;
 +		hmm_vma_walk.last = start;
 +		hmm_vma_walk.fault = false;
 +		hmm_vma_walk.range = range;
 +		end = min(range->end, vma->vm_end);
 +
 +		walk_page_range(vma->vm_mm, 0, end, &hmm_walk_ops,
 +				&hmm_vma_walk);
 +		start = end;
 +	} while (start < range->end);
 +
 +	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +}
 +EXPORT_SYMBOL(hmm_range_snapshot);
 +
 +/*
 + * hmm_range_fault() - try to fault some address in a virtual address range
 + * @range: range being faulted
 + * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
 + * Return: number of valid pages in range->pfns[] (from range start
 + *          address). This may be zero. If the return value is negative,
 + *          then one of the following values may be returned:
 + *
 + *           -EINVAL  invalid arguments or mm or virtual address are in an
 + *                    invalid vma (for instance device file vma).
 + *           -ENOMEM: Out of memory.
 + *           -EPERM:  Invalid permission (for instance asking for write and
 + *                    range is read only).
 + *           -EAGAIN: If you need to retry and mmap_sem was drop. This can only
 + *                    happens if block argument is false.
 + *           -EBUSY:  If the the range is being invalidated and you should wait
 + *                    for invalidation to finish.
 + *           -EFAULT: Invalid (ie either no valid vma or it is illegal to access
 + *                    that range), number of valid pages in range->pfns[] (from
 + *                    range start address).
   *
 - * -EINVAL:	Invalid arguments or mm or virtual address is in an invalid vma
 - *		(e.g., device file vma).
 - * -ENOMEM:	Out of memory.
 - * -EPERM:	Invalid permission (e.g., asking for write and range is read
 - *		only).
 - * -EBUSY:	The range has been invalidated and the caller needs to wait for
 - *		the invalidation to finish.
 - * -EFAULT:     A page was requested to be valid and could not be made valid
 - *              ie it has no backing VMA or it is illegal to access
++<<<<<<< HEAD
 + * This is similar to a regular CPU page fault except that it will not trigger
 + * any memory migration if the memory being faulted is not accessible by CPUs
 + * and caller does not ask for migration.
   *
 + * On error, for one virtual address in the range, the function will mark the
 + * corresponding HMM pfn entry with an error flag.
++=======
+  * This is similar to get_user_pages(), except that it can read the page tables
+  * without mutating them (ie causing faults).
++>>>>>>> 53bfe17ff88f (mm/hmm: do not set pfns when returning an error code)
   */
 -long hmm_range_fault(struct hmm_range *range)
 +long hmm_range_fault(struct hmm_range *range, bool block)
  {
 -	struct hmm_vma_walk hmm_vma_walk = {
 -		.range = range,
 -		.last = range->start,
 -	};
 -	struct mm_struct *mm = range->notifier->mm;
 +	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 +	unsigned long start = range->start, end;
 +	struct hmm_vma_walk hmm_vma_walk;
 +	struct hmm *hmm = range->hmm;
 +	struct vm_area_struct *vma;
  	int ret;
  
 -	lockdep_assert_held(&mm->mmap_sem);
 +	lockdep_assert_held(&hmm->mm->mmap_sem);
  
  	do {
  		/* If range is no longer valid force retry. */
* Unmerged path mm/hmm.c
