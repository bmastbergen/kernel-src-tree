mmap locking API: convert nested write lock sites

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Michel Lespinasse <walken@google.com>
commit aaa2cc56c1cd757efec88a4978ffce4cbf884352
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/aaa2cc56.failed

Add API for nested write locks and convert the few call sites doing that.

	Signed-off-by: Michel Lespinasse <walken@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
	Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
	Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Davidlohr Bueso <dbueso@suse.de>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Liam Howlett <Liam.Howlett@oracle.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ying Han <yinghan@google.com>
Link: http://lkml.kernel.org/r/20200520052908.204642-7-walken@google.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit aaa2cc56c1cd757efec88a4978ffce4cbf884352)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/um/include/asm/mmu_context.h
diff --cc arch/um/include/asm/mmu_context.h
index 9f4b4bb78120,17ddd4edf875..000000000000
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@@ -52,9 -48,9 +53,13 @@@ static inline void activate_mm(struct m
  	 * when the new ->mm is used for the first time.
  	 */
  	__switch_mm(&new->context.id);
++<<<<<<< HEAD
 +	down_write(&new->mmap_sem);
++=======
+ 	mmap_write_lock_nested(new, SINGLE_DEPTH_NESTING);
++>>>>>>> aaa2cc56c1cd (mmap locking API: convert nested write lock sites)
  	uml_setup_stubs(new);
 -	mmap_write_unlock(new);
 +	up_write(&new->mmap_sem);
  }
  
  static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next, 
* Unmerged path arch/um/include/asm/mmu_context.h
diff --git a/include/linux/mmap_lock.h b/include/linux/mmap_lock.h
index 424a43c06c0d..d1826ce42f00 100644
--- a/include/linux/mmap_lock.h
+++ b/include/linux/mmap_lock.h
@@ -11,6 +11,11 @@ static inline void mmap_write_lock(struct mm_struct *mm)
 	down_write(&mm->mmap_sem);
 }
 
+static inline void mmap_write_lock_nested(struct mm_struct *mm, int subclass)
+{
+	down_write_nested(&mm->mmap_sem, subclass);
+}
+
 static inline int mmap_write_lock_killable(struct mm_struct *mm)
 {
 	return down_write_killable(&mm->mmap_sem);
diff --git a/kernel/fork.c b/kernel/fork.c
index 41396f9e4c67..a6d8b8512148 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -470,7 +470,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	/*
 	 * Not linked in yet - no deadlock potential:
 	 */
-	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
+	mmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING);
 
 	/* No ordering required: file already has been exposed. */
 	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
