RDMA/mlx5: Zero out ODP related items in the mlx5_ib_mr

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit a639e66703ee45745dc4057c7c2013ed9e1963a7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/a639e667.failed

All of the ODP code assumes when it calls mlx5_mr_cache_alloc() the ODP
related fields are zero'd. This is true if the MR was just allocated, but
if the MR is recycled through the cache then the values are never zero'd.

This causes a bug in the odp_stats, they don't reset when the MR is
reallocated, also is_odp_implicit is never 0'd.

So we can use memset on a block of the mlx5_ib_mr reorganize the structure
to put all the data that can be zero'd by the cache at the end.

It is organized as an anonymous struct because the next patch will make
this a union.

Delete the unused smr_info. Don't set the kernel only desc_size on the
user path. No longer any need to zero mr->parent before freeing it, the
memset() will get it now.

Fixes: a3de94e3d61e ("IB/mlx5: Introduce ODP diagnostic counters")
Link: https://lore.kernel.org/r/20210304120745.1090751-2-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit a639e66703ee45745dc4057c7c2013ed9e1963a7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index de9c5a944683,b085c02b53d0..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -587,49 -649,69 +582,105 @@@ struct mlx5_ib_dm 
  	atomic64_add(value, &((mr)->odp_stats.counter_name))
  
  struct mlx5_ib_mr {
++<<<<<<< HEAD
 +	struct ib_mr		ibmr;
 +	void			*descs;
 +	dma_addr_t		desc_map;
 +	int			ndescs;
 +	int			data_length;
 +	int			meta_ndescs;
 +	int			meta_length;
 +	int			max_descs;
 +	int			desc_size;
 +	int			access_mode;
 +	struct mlx5_core_mkey	mmkey;
 +	struct ib_umem	       *umem;
 +	struct mlx5_shared_mr_info	*smr_info;
 +	struct list_head	list;
 +	struct mlx5_cache_ent  *cache_ent;
 +	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
 +	struct mlx5_core_sig_ctx    *sig;
 +	void			*descs_alloc;
 +	int			access_flags; /* Needed for rereg MR */
++=======
+ 	struct ib_mr ibmr;
+ 	struct mlx5_core_mkey mmkey;
++>>>>>>> a639e66703ee (RDMA/mlx5: Zero out ODP related items in the mlx5_ib_mr)
  
- 	struct mlx5_ib_mr      *parent;
- 	/* Needed for IB_MR_TYPE_INTEGRITY */
- 	struct mlx5_ib_mr      *pi_mr;
- 	struct mlx5_ib_mr      *klm_mr;
- 	struct mlx5_ib_mr      *mtt_mr;
- 	u64			data_iova;
- 	u64			pi_iova;
+ 	/* User MR data */
+ 	struct mlx5_cache_ent *cache_ent;
+ 	struct ib_umem *umem;
  
++<<<<<<< HEAD
 +	/* For ODP and implicit */
 +	atomic_t		num_deferred_work;
 +	wait_queue_head_t       q_deferred_work;
 +	struct xarray		implicit_children;
 +	union {
 +		struct rcu_head rcu;
 +		struct list_head elm;
 +		struct work_struct work;
 +	} odp_destroy;
 +	struct ib_odp_counters	odp_stats;
 +	bool			is_odp_implicit;
- 
- 	struct mlx5_async_work  cb_work;
++=======
+ 	/* This is zero'd when the MR is allocated */
+ 	struct {
+ 		/* Used only while the MR is in the cache */
+ 		struct {
+ 			u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
+ 			struct mlx5_async_work cb_work;
+ 			/* Cache list element */
+ 			struct list_head list;
+ 		};
++>>>>>>> a639e66703ee (RDMA/mlx5: Zero out ODP related items in the mlx5_ib_mr)
+ 
+ 		/* Used only by kernel MRs (umem == NULL) */
+ 		struct {
+ 			void *descs;
+ 			void *descs_alloc;
+ 			dma_addr_t desc_map;
+ 			int max_descs;
+ 			int ndescs;
+ 			int desc_size;
+ 			int access_mode;
+ 
+ 			/* For Kernel IB_MR_TYPE_INTEGRITY */
+ 			struct mlx5_core_sig_ctx *sig;
+ 			struct mlx5_ib_mr *pi_mr;
+ 			struct mlx5_ib_mr *klm_mr;
+ 			struct mlx5_ib_mr *mtt_mr;
+ 			u64 data_iova;
+ 			u64 pi_iova;
+ 			int meta_ndescs;
+ 			int meta_length;
+ 			int data_length;
+ 		};
+ 
+ 		/* Used only by User MRs (umem != NULL) */
+ 		struct {
+ 			unsigned int page_shift;
+ 			/* Current access_flags */
+ 			int access_flags;
+ 
+ 			/* For User ODP */
+ 			struct mlx5_ib_mr *parent;
+ 			struct xarray implicit_children;
+ 			union {
+ 				struct work_struct work;
+ 			} odp_destroy;
+ 			struct ib_odp_counters odp_stats;
+ 			bool is_odp_implicit;
+ 		};
+ 	};
  };
  
+ /* Zero the fields in the mr that are variant depending on usage */
+ static inline void mlx5_clear_mr(struct mlx5_ib_mr *mr)
+ {
+ 	memset(mr->out, 0, sizeof(*mr) - offsetof(struct mlx5_ib_mr, out));
+ }
+ 
  static inline bool is_odp_mr(struct mlx5_ib_mr *mr)
  {
  	return IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) && mr->umem &&
diff --cc drivers/infiniband/hw/mlx5/mr.c
index d1c1a16be30f,ea8f068a6da3..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -1007,11 -993,11 +1007,16 @@@ alloc_mr_from_cache(struct ib_pd *pd, s
  
  	mr->ibmr.pd = pd;
  	mr->umem = umem;
++<<<<<<< HEAD
 +	mr->access_flags = access_flags;
 +	mr->desc_size = sizeof(struct mlx5_mtt);
 +	mr->mmkey.iova = virt_addr;
 +	mr->mmkey.size = len;
++=======
+ 	mr->mmkey.iova = iova;
+ 	mr->mmkey.size = umem->length;
++>>>>>>> a639e66703ee (RDMA/mlx5: Zero out ODP related items in the mlx5_ib_mr)
  	mr->mmkey.pd = to_mpd(pd)->pdn;
 -	mr->page_shift = order_base_2(page_size);
 -	set_mr_fields(dev, mr, umem->length, access_flags);
  
  	return mr;
  }
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
diff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c
index cb9cf1cb236b..5f503375f6a1 100644
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@ -233,7 +233,6 @@ static void free_implicit_child_mr(struct mlx5_ib_mr *mr, bool need_imr_xlt)
 
 	dma_fence_odp_mr(mr);
 
-	mr->parent = NULL;
 	mlx5_mr_cache_free(mr_to_mdev(mr), mr);
 	ib_umem_odp_release(odp);
 	if (atomic_dec_and_test(&imr->num_deferred_work))
