xfs: xfs_iflush() is no longer necessary

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Dave Chinner <dchinner@redhat.com>
commit 90c60e16401248a4900f3f9387f563d0178dcf34
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/90c60e16.failed

Now we have a cached buffer on inode log items, we don't need
to do buffer lookups when flushing inodes anymore - all we need
to do is lock the buffer and we are ready to go.

This largely gets rid of the need for xfs_iflush(), which is
essentially just a mechanism to look up the buffer and flush the
inode to it. Instead, we can just call xfs_iflush_cluster() with a
few modifications to ensure it also flushes the inode we already
hold locked.

This allows the AIL inode item pushing to be almost entirely
non-blocking in XFS - we won't block unless memory allocation
for the cluster inode lookup blocks or the block device queues are
full.

Writeback during inode reclaim becomes a little more complex because
we now have to lock the buffer ourselves, but otherwise this change
is largely a functional no-op that removes a whole lot of code.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
(cherry picked from commit 90c60e16401248a4900f3f9387f563d0178dcf34)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_inode.c
diff --cc fs/xfs/xfs_inode.c
index bd37c3b531db,4a9539048639..000000000000
--- a/fs/xfs/xfs_inode.c
+++ b/fs/xfs/xfs_inode.c
@@@ -3584,86 -3586,6 +3593,89 @@@ out_free
  	kmem_free(cilist);
  out_put:
  	xfs_perag_put(pag);
++<<<<<<< HEAD
 +	return error;
 +}
 +
 +/*
 + * Flush dirty inode metadata into the backing buffer.
 + *
 + * The caller must have the inode lock and the inode flush lock held.  The
 + * inode lock will still be held upon return to the caller, and the inode
 + * flush lock will be released after the inode has reached the disk.
 + *
 + * The caller must write out the buffer returned in *bpp and release it.
 + */
 +int
 +xfs_iflush(
 +	struct xfs_inode	*ip,
 +	struct xfs_buf		**bpp)
 +{
 +	struct xfs_mount	*mp = ip->i_mount;
 +	struct xfs_buf		*bp = NULL;
 +	struct xfs_dinode	*dip;
 +	int			error;
 +
 +	XFS_STATS_INC(mp, xs_iflush_count);
 +
 +	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL|XFS_ILOCK_SHARED));
 +	ASSERT(xfs_isiflocked(ip));
 +	ASSERT(ip->i_d.di_format != XFS_DINODE_FMT_BTREE ||
 +	       ip->i_d.di_nextents > XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK));
 +
 +	*bpp = NULL;
 +
 +	xfs_iunpin_wait(ip);
 +
 +	/*
 +	 * For stale inodes we cannot rely on the backing buffer remaining
 +	 * stale in cache for the remaining life of the stale inode and so
 +	 * xfs_imap_to_bp() below may give us a buffer that no longer contains
 +	 * inodes below. We have to check this after ensuring the inode is
 +	 * unpinned so that it is safe to reclaim the stale inode after the
 +	 * flush call.
 +	 */
 +	if (xfs_iflags_test(ip, XFS_ISTALE)) {
 +		xfs_ifunlock(ip);
 +		return 0;
 +	}
 +
 +	/*
 +	 * Get the buffer containing the on-disk inode. We are doing a try-lock
 +	 * operation here, so we may get an EAGAIN error. In that case, return
 +	 * leaving the inode dirty.
 +	 *
 +	 * If we get any other error, we effectively have a corruption situation
 +	 * and we cannot flush the inode. Abort the flush and shut down.
 +	 */
 +	error = xfs_imap_to_bp(mp, NULL, &ip->i_imap, &dip, &bp, XBF_TRYLOCK);
 +	if (error == -EAGAIN) {
 +		xfs_ifunlock(ip);
 +		return error;
 +	}
 +	if (error)
 +		goto abort;
 +
 +	/*
 +	 * If the buffer is pinned then push on the log now so we won't
 +	 * get stuck waiting in the write for too long.
 +	 */
 +	if (xfs_buf_ispinned(bp))
 +		xfs_log_force(mp, 0);
 +
 +	/*
 +	 * Flush the provided inode then attempt to gather others from the
 +	 * cluster into the write.
 +	 *
 +	 * Note: Once we attempt to flush an inode, we must run buffer
 +	 * completion callbacks on any failure. If this fails, simulate an I/O
 +	 * failure on the buffer and shut down.
 +	 */
 +	error = xfs_iflush_int(ip, bp);
 +	if (!error)
 +		error = xfs_iflush_cluster(ip, bp);
++=======
++>>>>>>> 90c60e164012 (xfs: xfs_iflush() is no longer necessary)
  	if (error) {
  		bp->b_flags |= XBF_ASYNC;
  		xfs_buf_ioend_fail(bp);
@@@ -3692,9 -3606,9 +3696,15 @@@ xfs_iflush_int
  
  	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL|XFS_ILOCK_SHARED));
  	ASSERT(xfs_isiflocked(ip));
++<<<<<<< HEAD
 +	ASSERT(ip->i_d.di_format != XFS_DINODE_FMT_BTREE ||
 +	       ip->i_d.di_nextents > XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK));
 +	ASSERT(iip != NULL && iip->ili_fields != 0);
++=======
+ 	ASSERT(ip->i_df.if_format != XFS_DINODE_FMT_BTREE ||
+ 	       ip->i_df.if_nextents > XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK));
+ 	ASSERT(iip->ili_item.li_buf == bp);
++>>>>>>> 90c60e164012 (xfs: xfs_iflush() is no longer necessary)
  
  	dip = xfs_buf_offset(bp, ip->i_imap.im_boffset);
  
* Unmerged path fs/xfs/xfs_inode.c
diff --git a/fs/xfs/xfs_inode.h b/fs/xfs/xfs_inode.h
index f9a8b26c3625..afbae9520a9b 100644
--- a/fs/xfs/xfs_inode.h
+++ b/fs/xfs/xfs_inode.h
@@ -429,7 +429,7 @@ int		xfs_log_force_inode(struct xfs_inode *ip);
 void		xfs_iunpin_wait(xfs_inode_t *);
 #define xfs_ipincount(ip)	((unsigned int) atomic_read(&ip->i_pincount))
 
-int		xfs_iflush(struct xfs_inode *, struct xfs_buf **);
+int		xfs_iflush_cluster(struct xfs_inode *, struct xfs_buf *);
 void		xfs_lock_two_inodes(struct xfs_inode *ip0, uint ip0_mode,
 				struct xfs_inode *ip1, uint ip1_mode);
 
diff --git a/fs/xfs/xfs_inode_item.c b/fs/xfs/xfs_inode_item.c
index 92cf5780c28a..0fcf5b7344a6 100644
--- a/fs/xfs/xfs_inode_item.c
+++ b/fs/xfs/xfs_inode_item.c
@@ -485,53 +485,38 @@ xfs_inode_item_push(
 	uint			rval = XFS_ITEM_SUCCESS;
 	int			error;
 
-	if (xfs_ipincount(ip) > 0)
+	ASSERT(iip->ili_item.li_buf);
+
+	if (xfs_ipincount(ip) > 0 || xfs_buf_ispinned(bp) ||
+	    (ip->i_flags & XFS_ISTALE))
 		return XFS_ITEM_PINNED;
 
-	if (!xfs_ilock_nowait(ip, XFS_ILOCK_SHARED))
-		return XFS_ITEM_LOCKED;
+	/* If the inode is already flush locked, we're already flushing. */
+	if (xfs_isiflocked(ip))
+		return XFS_ITEM_FLUSHING;
 
-	/*
-	 * Re-check the pincount now that we stabilized the value by
-	 * taking the ilock.
-	 */
-	if (xfs_ipincount(ip) > 0) {
-		rval = XFS_ITEM_PINNED;
-		goto out_unlock;
-	}
+	if (!xfs_buf_trylock(bp))
+		return XFS_ITEM_LOCKED;
 
-	/*
-	 * Stale inode items should force out the iclog.
-	 */
-	if (ip->i_flags & XFS_ISTALE) {
-		rval = XFS_ITEM_PINNED;
-		goto out_unlock;
-	}
+	spin_unlock(&lip->li_ailp->ail_lock);
 
 	/*
-	 * Someone else is already flushing the inode.  Nothing we can do
-	 * here but wait for the flush to finish and remove the item from
-	 * the AIL.
+	 * We need to hold a reference for flushing the cluster buffer as it may
+	 * fail the buffer without IO submission. In which case, we better get a
+	 * reference for that completion because otherwise we don't get a
+	 * reference for IO until we queue the buffer for delwri submission.
 	 */
-	if (!xfs_iflock_nowait(ip)) {
-		rval = XFS_ITEM_FLUSHING;
-		goto out_unlock;
-	}
-
-	ASSERT(iip->ili_fields != 0 || XFS_FORCED_SHUTDOWN(ip->i_mount));
-	spin_unlock(&lip->li_ailp->ail_lock);
-
-	error = xfs_iflush(ip, &bp);
+	xfs_buf_hold(bp);
+	error = xfs_iflush_cluster(ip, bp);
 	if (!error) {
 		if (!xfs_buf_delwri_queue(bp, buffer_list))
 			rval = XFS_ITEM_FLUSHING;
 		xfs_buf_relse(bp);
-	} else if (error == -EAGAIN)
+	} else {
 		rval = XFS_ITEM_LOCKED;
+	}
 
 	spin_lock(&lip->li_ailp->ail_lock);
-out_unlock:
-	xfs_iunlock(ip, XFS_ILOCK_SHARED);
 	return rval;
 }
 
