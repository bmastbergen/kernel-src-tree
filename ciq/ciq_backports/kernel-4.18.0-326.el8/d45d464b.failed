mm/hmm: merge hmm_range_snapshot into hmm_range_fault

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit d45d464b118f428229d91769c8a3cc1e2e0bb4d5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/d45d464b.failed

Add a HMM_FAULT_SNAPSHOT flag so that hmm_range_snapshot can be merged
into the almost identical hmm_range_fault function.

Link: https://lore.kernel.org/r/20190726005650.2566-5-rcampbell@nvidia.com
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit d45d464b118f428229d91769c8a3cc1e2e0bb4d5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/vm/hmm.rst
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc Documentation/vm/hmm.rst
index b4b84a2cecd8,ddcb5ca8b296..000000000000
--- a/Documentation/vm/hmm.rst
+++ b/Documentation/vm/hmm.rst
@@@ -187,16 -192,15 +187,21 @@@ read only, or fully unmap, ...). The de
  the driver callback returns.
  
  When the device driver wants to populate a range of virtual addresses, it can
- use either::
+ use::
  
-   long hmm_range_snapshot(struct hmm_range *range);
-   long hmm_range_fault(struct hmm_range *range, bool block);
+   long hmm_range_fault(struct hmm_range *range, unsigned int flags);
  
- The first one (hmm_range_snapshot()) will only fetch present CPU page table
+ With the HMM_RANGE_SNAPSHOT flag, it will only fetch present CPU page table
  entries and will not trigger a page fault on missing or non-present entries.
++<<<<<<< HEAD
 +The second one does trigger a page fault on missing or read-only entry if the
 +write parameter is true. Page faults use the generic mm page fault code path
 +just like a CPU page fault.
++=======
+ Without that flag, it does trigger a page fault on missing or read-only entries
+ if write access is requested (see below). Page faults use the generic mm page
+ fault code path just like a CPU page fault.
++>>>>>>> d45d464b118f (mm/hmm: merge hmm_range_snapshot into hmm_range_fault)
  
  Both functions copy CPU page table entries into their pfns array argument. Each
  entry in that array corresponds to an address in the virtual range. HMM
diff --cc include/linux/hmm.h
index 86b23fe09dc9,90dc5944b1bc..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -483,8 -407,17 +483,22 @@@ int hmm_range_register(struct hmm_rang
  		       unsigned long end,
  		       unsigned page_shift);
  void hmm_range_unregister(struct hmm_range *range);
++<<<<<<< HEAD
 +long hmm_range_snapshot(struct hmm_range *range);
 +long hmm_range_fault(struct hmm_range *range, bool block);
++=======
+ 
+ /*
+  * Retry fault if non-blocking, drop mmap_sem and return -EAGAIN in that case.
+  */
+ #define HMM_FAULT_ALLOW_RETRY		(1 << 0)
+ 
+ /* Don't fault in missing PTEs, just snapshot the current state. */
+ #define HMM_FAULT_SNAPSHOT		(1 << 1)
+ 
+ long hmm_range_fault(struct hmm_range *range, unsigned int flags);
+ 
++>>>>>>> d45d464b118f (mm/hmm: merge hmm_range_snapshot into hmm_range_fault)
  long hmm_range_dma_map(struct hmm_range *range,
  		       struct device *device,
  		       dma_addr_t *daddrs,
diff --cc mm/hmm.c
index 44f8c4b225fa,1bc014cddd78..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -297,8 -280,7 +297,12 @@@ struct hmm_vma_walk 
  	struct hmm_range	*range;
  	struct dev_pagemap	*pgmap;
  	unsigned long		last;
++<<<<<<< HEAD
 +	bool			fault;
 +	bool			block;
++=======
+ 	unsigned int		flags;
++>>>>>>> d45d464b118f (mm/hmm: merge hmm_range_snapshot into hmm_range_fault)
  };
  
  static int hmm_vma_do_fault(struct mm_walk *walk, unsigned long addr,
@@@ -972,101 -935,26 +976,108 @@@ void hmm_range_unregister(struct hmm_ra
  }
  EXPORT_SYMBOL(hmm_range_unregister);
  
++<<<<<<< HEAD
 +static const struct mm_walk_ops hmm_walk_ops = {
 +	.pud_entry	= hmm_vma_walk_pud,
 +	.pmd_entry	= hmm_vma_walk_pmd,
 +	.pte_hole	= hmm_vma_walk_hole,
 +	.hugetlb_entry	= hmm_vma_walk_hugetlb_entry,
 +};
 +
 +/*
 + * hmm_range_snapshot() - snapshot CPU page table for a range
 + * @range: range
 + * Return: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 + *          permission (for instance asking for write and range is read only),
 + *          -EBUSY if you need to retry, -EFAULT invalid (ie either no valid
 + *          vma or it is illegal to access that range), number of valid pages
 + *          in range->pfns[] (from range start address).
 + *
 + * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 + * validity is tracked by range struct. See in include/linux/hmm.h for example
 + * on how to use.
 + */
 +long hmm_range_snapshot(struct hmm_range *range)
 +{
 +	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 +	unsigned long start = range->start, end;
 +	struct hmm_vma_walk hmm_vma_walk;
 +	struct hmm *hmm = range->hmm;
 +	struct vm_area_struct *vma;
 +
 +	lockdep_assert_held(&hmm->mm->mmap_sem);
 +	do {
 +		/* If range is no longer valid force retry. */
 +		if (!range->valid)
 +			return -EBUSY;
 +
 +		vma = find_vma(hmm->mm, start);
 +		if (vma == NULL || (vma->vm_flags & device_vma))
 +			return -EFAULT;
 +
 +		if (is_vm_hugetlb_page(vma)) {
 +			if (huge_page_shift(hstate_vma(vma)) !=
 +				    range->page_shift &&
 +			    range->page_shift != PAGE_SHIFT)
 +				return -EINVAL;
 +		} else {
 +			if (range->page_shift != PAGE_SHIFT)
 +				return -EINVAL;
 +		}
 +
 +		if (!(vma->vm_flags & VM_READ)) {
 +			/*
 +			 * If vma do not allow read access, then assume that it
 +			 * does not allow write access, either. HMM does not
 +			 * support architecture that allow write without read.
 +			 */
 +			hmm_pfns_clear(range, range->pfns,
 +				range->start, range->end);
 +			return -EPERM;
 +		}
 +
 +		range->vma = vma;
 +		hmm_vma_walk.pgmap = NULL;
 +		hmm_vma_walk.last = start;
 +		hmm_vma_walk.fault = false;
 +		hmm_vma_walk.range = range;
 +		end = min(range->end, vma->vm_end);
 +
 +		walk_page_range(vma->vm_mm, 0, end, &hmm_walk_ops,
 +				&hmm_vma_walk);
 +		start = end;
 +	} while (start < range->end);
 +
 +	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +}
 +EXPORT_SYMBOL(hmm_range_snapshot);
 +
 +/*
 + * hmm_range_fault() - try to fault some address in a virtual address range
 + * @range: range being faulted
 + * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
 + * Return: number of valid pages in range->pfns[] (from range start
 + *          address). This may be zero. If the return value is negative,
 + *          then one of the following values may be returned:
++=======
+ /**
+  * hmm_range_fault - try to fault some address in a virtual address range
+  * @range:	range being faulted
+  * @flags:	HMM_FAULT_* flags
++>>>>>>> d45d464b118f (mm/hmm: merge hmm_range_snapshot into hmm_range_fault)
   *
 - * Return: the number of valid pages in range->pfns[] (from range start
 - * address), which may be zero.  On error one of the following status codes
 - * can be returned:
 - *
 - * -EINVAL:	Invalid arguments or mm or virtual address is in an invalid vma
 - *		(e.g., device file vma).
 - * -ENOMEM:	Out of memory.
 - * -EPERM:	Invalid permission (e.g., asking for write and range is read
 - *		only).
 - * -EAGAIN:	A page fault needs to be retried and mmap_sem was dropped.
 - * -EBUSY:	The range has been invalidated and the caller needs to wait for
 - *		the invalidation to finish.
 - * -EFAULT:	Invalid (i.e., either no valid vma or it is illegal to access
 - *		that range) number of valid pages in range->pfns[] (from
 - *              range start address).
 + *           -EINVAL  invalid arguments or mm or virtual address are in an
 + *                    invalid vma (for instance device file vma).
 + *           -ENOMEM: Out of memory.
 + *           -EPERM:  Invalid permission (for instance asking for write and
 + *                    range is read only).
 + *           -EAGAIN: If you need to retry and mmap_sem was drop. This can only
 + *                    happens if block argument is false.
 + *           -EBUSY:  If the the range is being invalidated and you should wait
 + *                    for invalidation to finish.
 + *           -EFAULT: Invalid (ie either no valid vma or it is illegal to access
 + *                    that range), number of valid pages in range->pfns[] (from
 + *                    range start address).
   *
   * This is similar to a regular CPU page fault except that it will not trigger
   * any memory migration if the memory being faulted is not accessible by CPUs
@@@ -1119,17 -1008,23 +1130,21 @@@ long hmm_range_fault(struct hmm_range *
  		range->vma = vma;
  		hmm_vma_walk.pgmap = NULL;
  		hmm_vma_walk.last = start;
++<<<<<<< HEAD
 +		hmm_vma_walk.fault = true;
 +		hmm_vma_walk.block = block;
++=======
+ 		hmm_vma_walk.flags = flags;
++>>>>>>> d45d464b118f (mm/hmm: merge hmm_range_snapshot into hmm_range_fault)
  		hmm_vma_walk.range = range;
 -		mm_walk.private = &hmm_vma_walk;
  		end = min(range->end, vma->vm_end);
  
 -		mm_walk.vma = vma;
 -		mm_walk.mm = vma->vm_mm;
 -		mm_walk.pte_entry = NULL;
 -		mm_walk.test_walk = NULL;
 -		mm_walk.hugetlb_entry = NULL;
 -		mm_walk.pud_entry = hmm_vma_walk_pud;
 -		mm_walk.pmd_entry = hmm_vma_walk_pmd;
 -		mm_walk.pte_hole = hmm_vma_walk_hole;
 -		mm_walk.hugetlb_entry = hmm_vma_walk_hugetlb_entry;
 +		walk_page_range(vma->vm_mm, start, end, &hmm_walk_ops,
 +				&hmm_vma_walk);
  
  		do {
 -			ret = walk_page_range(start, end, &mm_walk);
 +			ret = walk_page_range(vma->vm_mm, start, end,
 +					&hmm_walk_ops, &hmm_vma_walk);
  			start = hmm_vma_walk.last;
  
  			/* Keep trying while the range is valid. */
* Unmerged path Documentation/vm/hmm.rst
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
