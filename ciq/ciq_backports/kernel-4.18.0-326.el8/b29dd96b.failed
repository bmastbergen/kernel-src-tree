bpf, x86: Fix BPF_FETCH atomic and/or/xor with r0 as src

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Brendan Jackman <jackmanb@google.com>
commit b29dd96b905f3dd543f4ca729447286adf934dd6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/b29dd96b.failed

This code generates a CMPXCHG loop in order to implement atomic_fetch
bitwise operations. Because CMPXCHG is hard-coded to use rax (which
holds the BPF r0 value), it saves the _real_ r0 value into the
internal "ax" temporary register and restores it once the loop is
complete.

In the middle of the loop, the actual bitwise operation is performed
using src_reg. The bug occurs when src_reg is r0: as described above,
r0 has been clobbered and the real r0 value is in the ax register.

Therefore, perform this operation on the ax register instead, when
src_reg is r0.

Fixes: 981f94c3e921 ("bpf: Add bitwise atomic instructions")
	Signed-off-by: Brendan Jackman <jackmanb@google.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: KP Singh <kpsingh@kernel.org>
Link: https://lore.kernel.org/bpf/20210216125307.1406237-1-jackmanb@google.com
(cherry picked from commit b29dd96b905f3dd543f4ca729447286adf934dd6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
diff --cc arch/x86/net/bpf_jit_comp.c
index dd4d8265af59,6926d0ca6c71..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -1254,18 -1342,60 +1254,75 @@@ st:			if (is_imm8(insn->off)
  			}
  			break;
  
++<<<<<<< HEAD
 +			/* STX XADD: lock *(u32*)(dst_reg + off) += src_reg */
 +		case BPF_STX | BPF_XADD | BPF_W:
 +			/* Emit 'lock add dword ptr [rax + off], eax' */
 +			if (is_ereg(dst_reg) || is_ereg(src_reg))
 +				EMIT3(0xF0, add_2mod(0x40, dst_reg, src_reg), 0x01);
 +			else
 +				EMIT2(0xF0, 0x01);
 +			goto xadd;
 +		case BPF_STX | BPF_XADD | BPF_DW:
 +			EMIT3(0xF0, add_2mod(0x48, dst_reg, src_reg), 0x01);
 +xadd:
 +			emit_modrm_dstoff(&prog, dst_reg, src_reg, insn->off);
++=======
+ 		case BPF_STX | BPF_ATOMIC | BPF_W:
+ 		case BPF_STX | BPF_ATOMIC | BPF_DW:
+ 			if (insn->imm == (BPF_AND | BPF_FETCH) ||
+ 			    insn->imm == (BPF_OR | BPF_FETCH) ||
+ 			    insn->imm == (BPF_XOR | BPF_FETCH)) {
+ 				u8 *branch_target;
+ 				bool is64 = BPF_SIZE(insn->code) == BPF_DW;
+ 				u32 real_src_reg = src_reg;
+ 
+ 				/*
+ 				 * Can't be implemented with a single x86 insn.
+ 				 * Need to do a CMPXCHG loop.
+ 				 */
+ 
+ 				/* Will need RAX as a CMPXCHG operand so save R0 */
+ 				emit_mov_reg(&prog, true, BPF_REG_AX, BPF_REG_0);
+ 				if (src_reg == BPF_REG_0)
+ 					real_src_reg = BPF_REG_AX;
+ 
+ 				branch_target = prog;
+ 				/* Load old value */
+ 				emit_ldx(&prog, BPF_SIZE(insn->code),
+ 					 BPF_REG_0, dst_reg, insn->off);
+ 				/*
+ 				 * Perform the (commutative) operation locally,
+ 				 * put the result in the AUX_REG.
+ 				 */
+ 				emit_mov_reg(&prog, is64, AUX_REG, BPF_REG_0);
+ 				maybe_emit_mod(&prog, AUX_REG, real_src_reg, is64);
+ 				EMIT2(simple_alu_opcodes[BPF_OP(insn->imm)],
+ 				      add_2reg(0xC0, AUX_REG, real_src_reg));
+ 				/* Attempt to swap in new value */
+ 				err = emit_atomic(&prog, BPF_CMPXCHG,
+ 						  dst_reg, AUX_REG, insn->off,
+ 						  BPF_SIZE(insn->code));
+ 				if (WARN_ON(err))
+ 					return err;
+ 				/*
+ 				 * ZF tells us whether we won the race. If it's
+ 				 * cleared we need to try again.
+ 				 */
+ 				EMIT2(X86_JNE, -(prog - branch_target) - 2);
+ 				/* Return the pre-modification value */
+ 				emit_mov_reg(&prog, is64, real_src_reg, BPF_REG_0);
+ 				/* Restore R0 after clobbering RAX */
+ 				emit_mov_reg(&prog, true, BPF_REG_0, BPF_REG_AX);
+ 				break;
+ 
+ 			}
+ 
+ 			err = emit_atomic(&prog, insn->imm, dst_reg, src_reg,
+ 						  insn->off, BPF_SIZE(insn->code));
+ 			if (err)
+ 				return err;
++>>>>>>> b29dd96b905f (bpf, x86: Fix BPF_FETCH atomic and/or/xor with r0 as src)
  			break;
  
  			/* call */
* Unmerged path arch/x86/net/bpf_jit_comp.c
diff --git a/tools/testing/selftests/bpf/verifier/atomic_and.c b/tools/testing/selftests/bpf/verifier/atomic_and.c
index 600bc5e0f143..c5a4aa2b9396 100644
--- a/tools/testing/selftests/bpf/verifier/atomic_and.c
+++ b/tools/testing/selftests/bpf/verifier/atomic_and.c
@@ -75,3 +75,26 @@
 	},
 	.result = ACCEPT,
 },
+{
+	"BPF_ATOMIC_AND with fetch - r0 as source reg",
+	.insns = {
+		/* val = 0x110; */
+		BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0x110),
+		/* old = atomic_fetch_and(&val, 0x011); */
+		BPF_MOV64_IMM(BPF_REG_0, 0x011),
+		BPF_ATOMIC_OP(BPF_DW, BPF_AND | BPF_FETCH, BPF_REG_10, BPF_REG_0, -8),
+		/* if (old != 0x110) exit(3); */
+		BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0x110, 2),
+		BPF_MOV64_IMM(BPF_REG_0, 3),
+		BPF_EXIT_INSN(),
+		/* if (val != 0x010) exit(2); */
+		BPF_LDX_MEM(BPF_DW, BPF_REG_1, BPF_REG_10, -8),
+		BPF_JMP_IMM(BPF_JEQ, BPF_REG_1, 0x010, 2),
+		BPF_MOV64_IMM(BPF_REG_1, 2),
+		BPF_EXIT_INSN(),
+		/* exit(0); */
+		BPF_MOV64_IMM(BPF_REG_0, 0),
+		BPF_EXIT_INSN(),
+	},
+	.result = ACCEPT,
+},
