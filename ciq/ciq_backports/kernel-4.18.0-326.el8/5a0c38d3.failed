mm: merge hmm_vma_do_fault into into hmm_vma_walk_hole_

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 5a0c38d307afde167175602aa52ca4e6c5c42c44
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/5a0c38d3.failed

There is no good reason for this split, as it just obsfucates the flow.

Link: https://lore.kernel.org/r/20200316135310.899364-6-hch@lst.de
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 5a0c38d307afde167175602aa52ca4e6c5c42c44)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index 0031a5d7b75b,a5f4f8010965..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -297,40 -30,12 +297,45 @@@ struct hmm_vma_walk 
  	struct hmm_range	*range;
  	struct dev_pagemap	*pgmap;
  	unsigned long		last;
 -	unsigned int		flags;
 +	bool			fault;
 +	bool			block;
  };
  
++<<<<<<< HEAD
 +static int hmm_vma_do_fault(struct mm_walk *walk, unsigned long addr,
 +			    bool write_fault, uint64_t *pfn)
 +{
 +	unsigned int flags = FAULT_FLAG_REMOTE;
 +	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 +	struct hmm_range *range = hmm_vma_walk->range;
 +	struct vm_area_struct *vma = walk->vma;
 +	vm_fault_t ret;
 +
 +	flags |= hmm_vma_walk->block ? 0 : FAULT_FLAG_ALLOW_RETRY;
 +	flags |= write_fault ? FAULT_FLAG_WRITE : 0;
 +	ret = handle_mm_fault(vma, addr, flags);
 +	if (ret & VM_FAULT_RETRY) {
 +		/* Note, handle_mm_fault did up_read(&mm->mmap_sem)) */
 +		return -EAGAIN;
 +	}
 +	if (ret & VM_FAULT_ERROR) {
 +		*pfn = range->values[HMM_PFN_ERROR];
 +		return -EFAULT;
 +	}
 +
 +	return -EBUSY;
 +}
 +
 +static int hmm_pfns_bad(unsigned long addr,
 +			unsigned long end,
 +			struct mm_walk *walk)
++=======
+ static int hmm_pfns_fill(unsigned long addr, unsigned long end,
+ 		struct hmm_range *range, enum hmm_pfn_value_e value)
++>>>>>>> 5a0c38d307af (mm: merge hmm_vma_do_fault into into hmm_vma_walk_hole_)
  {
 +	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 +	struct hmm_range *range = hmm_vma_walk->range;
  	uint64_t *pfns = range->pfns;
  	unsigned long i;
  
@@@ -359,26 -64,32 +364,53 @@@ static int hmm_vma_walk_hole_(unsigned 
  {
  	struct hmm_vma_walk *hmm_vma_walk = walk->private;
  	struct hmm_range *range = hmm_vma_walk->range;
+ 	struct vm_area_struct *vma = walk->vma;
  	uint64_t *pfns = range->pfns;
++<<<<<<< HEAD
 +	unsigned long i, page_size;
++=======
+ 	unsigned long i = (addr - range->start) >> PAGE_SHIFT;
+ 	unsigned int fault_flags = FAULT_FLAG_REMOTE;
++>>>>>>> 5a0c38d307af (mm: merge hmm_vma_do_fault into into hmm_vma_walk_hole_)
  
 -	WARN_ON_ONCE(!fault && !write_fault);
  	hmm_vma_walk->last = addr;
 +	page_size = hmm_range_page_size(range);
 +	i = (addr - range->start) >> range->page_shift;
 +
++<<<<<<< HEAD
 +	for (; addr < end; addr += page_size, i++) {
 +		pfns[i] = range->values[HMM_PFN_NONE];
 +		if (fault || write_fault) {
 +			int ret;
 +
 +			ret = hmm_vma_do_fault(walk, addr, write_fault,
 +					       &pfns[i]);
 +			if (ret != -EBUSY)
 +				return ret;
 +		}
 +	}
  
 +	return (fault || write_fault) ? -EBUSY : 0;
++=======
+ 	if (!vma)
+ 		goto out_error;
+ 
+ 	if (write_fault) {
+ 		if (!(vma->vm_flags & VM_WRITE))
+ 			return -EPERM;
+ 		fault_flags |= FAULT_FLAG_WRITE;
+ 	}
+ 
+ 	for (; addr < end; addr += PAGE_SIZE, i++)
+ 		if (handle_mm_fault(vma, addr, fault_flags) & VM_FAULT_ERROR)
+ 			goto out_error;
+ 
+ 	return -EBUSY;
+ 
+ out_error:
+ 	pfns[i] = range->values[HMM_PFN_ERROR];
+ 	return -EFAULT;
++>>>>>>> 5a0c38d307af (mm: merge hmm_vma_do_fault into into hmm_vma_walk_hole_)
  }
  
  static inline void hmm_pte_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
* Unmerged path mm/hmm.c
