workqueue: Convert the pool::lock and wq_mayday_lock to raw_spinlock_t

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Sebastian Andrzej Siewior <bigeasy@linutronix.de>
commit a9b8a985294debae00f6c087dfec8c384d30a3b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/a9b8a985.failed

The workqueue code has it's internal spinlocks (pool::lock), which
are acquired on most workqueue operations. These spinlocks are
converted to 'sleeping' spinlocks on a RT-kernel.

Workqueue functions can be invoked from contexts which are truly atomic
even on a PREEMPT_RT enabled kernel. Taking sleeping locks from such
contexts is forbidden.

The pool::lock hold times are bound and the code sections are
relatively short, which allows to convert pool::lock and as a
consequence wq_mayday_lock to raw spinlocks which are truly spinning
locks even on a PREEMPT_RT kernel.

With the previous conversion of the manager waitqueue to a simple
waitqueue workqueues are now fully RT compliant.

	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Reviewed-by: Lai Jiangshan <jiangshanlai@gmail.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit a9b8a985294debae00f6c087dfec8c384d30a3b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/workqueue.c
diff --cc kernel/workqueue.c
index 96b73779681e,82f85f5d81a8..000000000000
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@@ -4371,28 -4382,28 +4371,41 @@@ void destroy_workqueue(struct workqueue
  		kfree(rescuer);
  	}
  
 -	/*
 -	 * Sanity checks - grab all the locks so that we wait for all
 -	 * in-flight operations which may do put_pwq().
 -	 */
 -	mutex_lock(&wq_pool_mutex);
 +	/* sanity checks */
  	mutex_lock(&wq->mutex);
  	for_each_pwq(pwq, wq) {
++<<<<<<< HEAD
 +		int i;
 +
 +		for (i = 0; i < WORK_NR_COLORS; i++) {
 +			if (WARN_ON(pwq->nr_in_flight[i])) {
 +				mutex_unlock(&wq->mutex);
 +				show_workqueue_state();
 +				return;
 +			}
 +		}
 +
 +		if (WARN_ON((pwq != wq->dfl_pwq) && (pwq->refcnt > 1)) ||
 +		    WARN_ON(pwq->nr_active) ||
 +		    WARN_ON(!list_empty(&pwq->delayed_works))) {
++=======
+ 		raw_spin_lock_irq(&pwq->pool->lock);
+ 		if (WARN_ON(pwq_busy(pwq))) {
+ 			pr_warn("%s: %s has the following busy pwq\n",
+ 				__func__, wq->name);
+ 			show_pwq(pwq);
+ 			raw_spin_unlock_irq(&pwq->pool->lock);
++>>>>>>> a9b8a985294d (workqueue: Convert the pool::lock and wq_mayday_lock to raw_spinlock_t)
  			mutex_unlock(&wq->mutex);
 -			mutex_unlock(&wq_pool_mutex);
  			show_workqueue_state();
  			return;
  		}
++<<<<<<< HEAD
++=======
+ 		raw_spin_unlock_irq(&pwq->pool->lock);
++>>>>>>> a9b8a985294d (workqueue: Convert the pool::lock and wq_mayday_lock to raw_spinlock_t)
  	}
  	mutex_unlock(&wq->mutex);
 -	mutex_unlock(&wq_pool_mutex);
  
  	/*
  	 * wq list is used to freeze wq, remove from list after
* Unmerged path kernel/workqueue.c
