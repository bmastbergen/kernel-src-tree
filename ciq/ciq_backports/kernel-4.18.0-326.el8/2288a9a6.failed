mm/hmm: return -EFAULT when setting HMM_PFN_ERROR on requested valid pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 2288a9a68175cec9f91afb52948ba585b690774b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/2288a9a6.failed

hmm_range_fault() should never return 0 if the caller requested a valid
page, but the pfns output for that page would be HMM_PFN_ERROR.

hmm_pte_need_fault() must always be called before setting HMM_PFN_ERROR to
detect if the page is in faulting mode or not.

Fix two cases in hmm_vma_walk_pmd() and reorganize some of the duplicated
code.

Fixes: d08faca018c4 ("mm/hmm: properly handle migration pmd")
Fixes: da4c3c735ea4 ("mm/hmm/mirror: helper to snapshot CPU page table")
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 2288a9a68175cec9f91afb52948ba585b690774b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index db8bd4cd8f85,3a03fcff2668..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -664,8 -392,15 +658,20 @@@ again
  			return -EBUSY;
  		}
  		return hmm_pfns_fill(start, end, range, HMM_PFN_NONE);
++<<<<<<< HEAD
 +	} else if (!pmd_present(pmd))
 +		return hmm_pfns_bad(start, end, walk);
++=======
+ 	}
+ 
+ 	if (!pmd_present(pmd)) {
+ 		hmm_range_need_fault(hmm_vma_walk, pfns, npages, 0, &fault,
+ 				     &write_fault);
+ 		if (fault || write_fault)
+ 			return -EFAULT;
+ 		return hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
+ 	}
++>>>>>>> 2288a9a68175 (mm/hmm: return -EFAULT when setting HMM_PFN_ERROR on requested valid pages)
  
  	if (pmd_devmap(pmd) || pmd_trans_huge(pmd)) {
  		/*
@@@ -692,15 -426,19 +697,24 @@@
  	 * entry pointing to pte directory or it is a bad pmd that will not
  	 * recover.
  	 */
++<<<<<<< HEAD
 +	if (pmd_bad(pmd))
 +		return hmm_pfns_bad(start, end, walk);
++=======
+ 	if (pmd_bad(pmd)) {
+ 		hmm_range_need_fault(hmm_vma_walk, pfns, npages, 0, &fault,
+ 				     &write_fault);
+ 		if (fault || write_fault)
+ 			return -EFAULT;
+ 		return hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
+ 	}
++>>>>>>> 2288a9a68175 (mm/hmm: return -EFAULT when setting HMM_PFN_ERROR on requested valid pages)
  
  	ptep = pte_offset_map(pmdp, addr);
- 	i = (addr - range->start) >> PAGE_SHIFT;
- 	for (; addr < end; addr += PAGE_SIZE, ptep++, i++) {
+ 	for (; addr < end; addr += PAGE_SIZE, ptep++, pfns++) {
  		int r;
  
- 		r = hmm_vma_handle_pte(walk, addr, end, pmdp, ptep, &pfns[i]);
+ 		r = hmm_vma_handle_pte(walk, addr, end, pmdp, ptep, pfns);
  		if (r) {
  			/* hmm_vma_handle_pte() did pte_unmap() */
  			hmm_vma_walk->last = addr;
* Unmerged path mm/hmm.c
