dma-direct: simplify the DMA_ATTR_NO_KERNEL_MAPPING handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 849facea92fa68d9292f9b06d7c4ee9e7a06b8dc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/849facea.failed

Use and entirely separate code path for the DMA_ATTR_NO_KERNEL_MAPPING
path.  This avoids any confusion about the ret type, and avoids lots of
attr checks and helpers that can be significantly simplified now.

It also ensures that common handling is applied to architetures still
using the arch alloc/free hooks.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 849facea92fa68d9292f9b06d7c4ee9e7a06b8dc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-map-ops.h
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index b992d9926af3,b92d08e65999..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -70,43 -67,14 +70,10 @@@ static gfp_t dma_direct_optimal_gfp_mas
  
  static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)
  {
 -	dma_addr_t dma_addr = phys_to_dma_direct(dev, phys);
 -
 -	if (dma_addr == DMA_MAPPING_ERROR)
 -		return false;
 -	return dma_addr + size - 1 <=
 -		min_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);
 +	return phys_to_dma_direct(dev, phys) + size - 1 <=
 +			min_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);
  }
  
- /*
-  * Decrypting memory is allowed to block, so if this device requires
-  * unencrypted memory it must come from atomic pools.
-  */
- static inline bool dma_should_alloc_from_pool(struct device *dev, gfp_t gfp,
- 					      unsigned long attrs)
- {
- 	if (!IS_ENABLED(CONFIG_DMA_COHERENT_POOL))
- 		return false;
- 	if (gfpflags_allow_blocking(gfp))
- 		return false;
- 	if (force_dma_unencrypted(dev))
- 		return true;
- 	if (!IS_ENABLED(CONFIG_DMA_DIRECT_REMAP))
- 		return false;
- 	if (dma_alloc_need_uncached(dev, attrs))
- 		return true;
- 	return false;
- }
- 
- static inline bool dma_should_free_from_pool(struct device *dev,
- 					     unsigned long attrs)
- {
- 	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL))
- 		return true;
- 	if ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&
- 	    !force_dma_unencrypted(dev))
- 		return false;
- 	if (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP))
- 		return true;
- 	return false;
- }
- 
  static struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
  		gfp_t gfp)
  {
@@@ -162,35 -141,41 +124,55 @@@ void *dma_direct_alloc(struct device *d
  	if (attrs & DMA_ATTR_NO_WARN)
  		gfp |= __GFP_NOWARN;
  
++<<<<<<< HEAD
 +	if (dma_should_alloc_from_pool(dev, gfp, attrs)) {
 +		u64 phys_mask;
 +
 +		gfp |= dma_direct_optimal_gfp_mask(dev, dev->coherent_dma_mask,
 +				&phys_mask);
 +		page = dma_alloc_from_pool(dev, size, &ret, gfp,
 +				dma_coherent_ok);
 +		if (!page)
 +			return NULL;
 +		goto done;
 +	}
- 
- 	/* we always manually zero the memory once we are done */
- 	page = __dma_direct_alloc_pages(dev, size, gfp & ~__GFP_ZERO);
- 	if (!page)
- 		return NULL;
- 
++=======
  	if ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&
  	    !force_dma_unencrypted(dev)) {
+ 		page = __dma_direct_alloc_pages(dev, size, gfp & ~__GFP_ZERO);
+ 		if (!page)
+ 			return NULL;
  		/* remove any dirty cache lines on the kernel alias */
  		if (!PageHighMem(page))
  			arch_dma_prep_coherent(page, size);
+ 		*dma_handle = phys_to_dma_direct(dev, page_to_phys(page));
  		/* return the page pointer as the opaque cookie */
- 		ret = page;
- 		goto done;
+ 		return page;
  	}
  
+ 	if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&
+ 	    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
+ 	    !dev_is_dma_coherent(dev))
+ 		return arch_dma_alloc(dev, size, dma_handle, gfp, attrs);
+ 
+ 	/*
+ 	 * Remapping or decrypting memory may block. If either is required and
+ 	 * we can't block, allocate the memory from the atomic pools.
+ 	 */
+ 	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&
+ 	    !gfpflags_allow_blocking(gfp) &&
+ 	    (force_dma_unencrypted(dev) ||
+ 	     (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) && !dev_is_dma_coherent(dev))))
+ 		return dma_direct_alloc_from_pool(dev, size, dma_handle, gfp);
++>>>>>>> 849facea92fa (dma-direct: simplify the DMA_ATTR_NO_KERNEL_MAPPING handling)
+ 
+ 	/* we always manually zero the memory once we are done */
+ 	page = __dma_direct_alloc_pages(dev, size, gfp & ~__GFP_ZERO);
+ 	if (!page)
+ 		return NULL;
+ 
  	if ((IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
- 	     dma_alloc_need_uncached(dev, attrs)) ||
+ 	     !dev_is_dma_coherent(dev)) ||
  	    (IS_ENABLED(CONFIG_DMA_REMAP) && PageHighMem(page))) {
  		/* remove any dirty cache lines on the kernel alias */
  		arch_dma_prep_coherent(page, size);
@@@ -291,6 -276,62 +273,65 @@@ void dma_direct_free(struct device *dev
  	dma_free_contiguous(dev, dma_direct_to_page(dev, dma_addr), size);
  }
  
++<<<<<<< HEAD
++=======
+ struct page *dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)
+ {
+ 	struct page *page;
+ 	void *ret;
+ 
+ 	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&
+ 	    force_dma_unencrypted(dev) && !gfpflags_allow_blocking(gfp))
+ 		return dma_direct_alloc_from_pool(dev, size, dma_handle, gfp);
+ 
+ 	page = __dma_direct_alloc_pages(dev, size, gfp);
+ 	if (!page)
+ 		return NULL;
+ 	if (PageHighMem(page)) {
+ 		/*
+ 		 * Depending on the cma= arguments and per-arch setup
+ 		 * dma_alloc_contiguous could return highmem pages.
+ 		 * Without remapping there is no way to return them here,
+ 		 * so log an error and fail.
+ 		 */
+ 		dev_info(dev, "Rejecting highmem page from CMA.\n");
+ 		goto out_free_pages;
+ 	}
+ 
+ 	ret = page_address(page);
+ 	if (force_dma_unencrypted(dev)) {
+ 		if (set_memory_decrypted((unsigned long)ret,
+ 				1 << get_order(size)))
+ 			goto out_free_pages;
+ 	}
+ 	memset(ret, 0, size);
+ 	*dma_handle = phys_to_dma_direct(dev, page_to_phys(page));
+ 	return page;
+ out_free_pages:
+ 	dma_free_contiguous(dev, page, size);
+ 	return NULL;
+ }
+ 
+ void dma_direct_free_pages(struct device *dev, size_t size,
+ 		struct page *page, dma_addr_t dma_addr,
+ 		enum dma_data_direction dir)
+ {
+ 	unsigned int page_order = get_order(size);
+ 	void *vaddr = page_address(page);
+ 
+ 	/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */
+ 	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&
+ 	    dma_free_from_pool(dev, vaddr, size))
+ 		return;
+ 
+ 	if (force_dma_unencrypted(dev))
+ 		set_memory_encrypted((unsigned long)vaddr, 1 << page_order);
+ 
+ 	dma_free_contiguous(dev, page, size);
+ }
+ 
++>>>>>>> 849facea92fa (dma-direct: simplify the DMA_ATTR_NO_KERNEL_MAPPING handling)
  #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
      defined(CONFIG_SWIOTLB)
  void dma_direct_sync_sg_for_device(struct device *dev,
* Unmerged path include/linux/dma-map-ops.h
* Unmerged path include/linux/dma-map-ops.h
* Unmerged path kernel/dma/direct.c
