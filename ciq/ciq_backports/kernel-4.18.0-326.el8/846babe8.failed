mm/hmm: do not unconditionally set pfns when returning EBUSY

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 846babe85efdda49feba5b169668333dcf3edf25
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/846babe8.failed

In hmm_vma_handle_pte() and hmm_vma_walk_hugetlb_entry() if fault happens
then -EBUSY will be returned and the pfns input flags will have been
destroyed.

For hmm_vma_handle_pte() set HMM_PFN_NONE only on the success returns that
don't otherwise store to pfns.

For hmm_vma_walk_hugetlb_entry() all exit paths already set pfns, so
remove the redundant store.

Fixes: 2aee09d8c116 ("mm/hmm: change hmm_vma_fault() to allow write fault on page basis")
Link: https://lore.kernel.org/r/20200327200021.29372-8-jgg@ziepe.ca
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 846babe85efdda49feba5b169668333dcf3edf25)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index 3233a7437881,660e8db44811..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -521,14 -250,11 +521,18 @@@ static int hmm_vma_handle_pte(struct mm
  	pte_t pte = *ptep;
  	uint64_t orig_pfn = *pfn;
  
++<<<<<<< HEAD
 +	*pfn = range->values[HMM_PFN_NONE];
 +	fault = write_fault = false;
 +
++=======
++>>>>>>> 846babe85efd (mm/hmm: do not unconditionally set pfns when returning EBUSY)
  	if (pte_none(pte)) {
 -		required_fault = hmm_pte_need_fault(hmm_vma_walk, orig_pfn, 0);
 -		if (required_fault)
 +		hmm_pte_need_fault(hmm_vma_walk, orig_pfn, 0,
 +				   &fault, &write_fault);
 +		if (fault || write_fault)
  			goto fault;
+ 		*pfn = range->values[HMM_PFN_NONE];
  		return 0;
  	}
  
@@@ -554,10 -274,11 +558,17 @@@
  			return 0;
  		}
  
++<<<<<<< HEAD
 +		hmm_pte_need_fault(hmm_vma_walk, orig_pfn, 0, &fault,
 +				   &write_fault);
 +		if (!fault && !write_fault)
++=======
+ 		required_fault = hmm_pte_need_fault(hmm_vma_walk, orig_pfn, 0);
+ 		if (!required_fault) {
+ 			*pfn = range->values[HMM_PFN_NONE];
++>>>>>>> 846babe85efd (mm/hmm: do not unconditionally set pfns when returning EBUSY)
  			return 0;
+ 		}
  
  		if (!non_swap_entry(entry))
  			goto fault;
@@@ -796,20 -494,17 +807,19 @@@ static int hmm_vma_walk_hugetlb_entry(p
  	ptl = huge_pte_lock(hstate_vma(vma), walk->mm, pte);
  	entry = huge_ptep_get(pte);
  
 -	i = (start - range->start) >> PAGE_SHIFT;
 +	i = (start - range->start) >> range->page_shift;
  	orig_pfn = range->pfns[i];
- 	range->pfns[i] = range->values[HMM_PFN_NONE];
  	cpu_flags = pte_to_hmm_pfn_flags(range, entry);
 -	required_fault = hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags);
 -	if (required_fault) {
 -		spin_unlock(ptl);
 -		return hmm_vma_fault(addr, end, required_fault, walk);
 +	fault = write_fault = false;
 +	hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags,
 +			   &fault, &write_fault);
 +	if (fault || write_fault) {
 +		ret = -ENOENT;
 +		goto unlock;
  	}
  
 -	pfn = pte_pfn(entry) + ((start & ~hmask) >> PAGE_SHIFT);
 -	for (; addr < end; addr += PAGE_SIZE, i++, pfn++)
 +	pfn = pte_pfn(entry) + ((start & mask) >> range->page_shift);
 +	for (; addr < end; addr += size, i++, pfn += pfn_inc)
  		range->pfns[i] = hmm_device_entry_from_pfn(range, pfn) |
  				 cpu_flags;
  	hmm_vma_walk->last = end;
* Unmerged path mm/hmm.c
