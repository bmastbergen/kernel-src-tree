RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit 878f7b31c3a7f3e48c6601ea373b8688e7e308e0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/878f7b31.failed

Since devx uses the new rdma_for_each_block() to fill the PAS it can also
use ib_umem_find_best_pgsz().

However, the umem constructionin devx is complicated, the umem must still
respect all the HW limits such as page_offset_quantized and the IOVA
alignment.

Since we don't know what the user intends to use the umem for we have to
limit it to PAGE_SIZE.

There are users trying to mix umem's with mkeys so this makes them work
reliably, at least for an identity IOVA, by ensuring the IOVA matches the
selected page size.

Last user of mlx5_ib_get_buf_offset() so it can also be removed.

Fixes: aeae94579caf ("IB/mlx5: Add DEVX support for memory registration")
Link: https://lore.kernel.org/r/20201115114311.136250-7-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 878f7b31c3a7f3e48c6601ea373b8688e7e308e0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/devx.c
#	drivers/infiniband/hw/mlx5/mem.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --cc drivers/infiniband/hw/mlx5/devx.c
index ed09a0921724,7c3eefba6197..000000000000
--- a/drivers/infiniband/hw/mlx5/devx.c
+++ b/drivers/infiniband/hw/mlx5/devx.c
@@@ -93,11 -93,8 +93,14 @@@ struct devx_async_event_file 
  struct devx_umem {
  	struct mlx5_core_dev		*mdev;
  	struct ib_umem			*umem;
++<<<<<<< HEAD
 +	u32				page_offset;
 +	int				page_shift;
 +	int				ncont;
++=======
++>>>>>>> 878f7b31c3a7 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx)
  	u32				dinlen;
 -	u32				dinbox[MLX5_ST_SZ_DW(general_obj_in_cmd_hdr)];
 +	u32				dinbox[MLX5_ST_SZ_DW(destroy_umem_in)];
  };
  
  struct devx_umem_reg_cmd {
@@@ -2152,9 -2054,7 +2155,8 @@@ static int devx_umem_get(struct mlx5_ib
  	u64 addr;
  	size_t size;
  	u32 access;
 +	int npages;
  	int err;
- 	u32 page_mask;
  
  	if (uverbs_copy_from(&addr, attrs, MLX5_IB_ATTR_DEVX_UMEM_REG_ADDR) ||
  	    uverbs_copy_from(&size, attrs, MLX5_IB_ATTR_DEVX_UMEM_REG_LEN))
@@@ -2172,22 -2072,9 +2174,25 @@@
  	if (err)
  		return err;
  
 -	obj->umem = ib_umem_get(&dev->ib_dev, addr, size, access);
 +	obj->umem = ib_umem_get(&attrs->driver_udata, addr, size, access);
  	if (IS_ERR(obj->umem))
  		return PTR_ERR(obj->umem);
++<<<<<<< HEAD
 +
 +	mlx5_ib_cont_pages(obj->umem, obj->umem->address,
 +			   MLX5_MKEY_PAGE_SHIFT_MASK, &npages,
 +			   &obj->page_shift, &obj->ncont, NULL);
 +
 +	if (!npages) {
 +		ib_umem_release(obj->umem);
 +		return -EINVAL;
 +	}
 +
 +	page_mask = (1 << obj->page_shift) - 1;
 +	obj->page_offset = obj->umem->address & page_mask;
 +
++=======
++>>>>>>> 878f7b31c3a7 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx)
  	return 0;
  }
  
@@@ -2195,30 -2083,47 +2201,71 @@@ static int devx_umem_reg_cmd_alloc(stru
  				   struct devx_umem *obj,
  				   struct devx_umem_reg_cmd *cmd)
  {
++<<<<<<< HEAD
 +	cmd->inlen = MLX5_ST_SZ_BYTES(create_umem_in) +
 +		    (MLX5_ST_SZ_BYTES(mtt) * obj->ncont);
 +	cmd->in = uverbs_zalloc(attrs, cmd->inlen);
 +	return PTR_ERR_OR_ZERO(cmd->in);
 +}
 +
 +static void devx_umem_reg_cmd_build(struct mlx5_ib_dev *dev,
 +				    struct devx_umem *obj,
 +				    struct devx_umem_reg_cmd *cmd)
 +{
 +	void *umem;
++=======
+ 	unsigned int page_size;
++>>>>>>> 878f7b31c3a7 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx)
  	__be64 *mtt;
+ 	void *umem;
+ 
+ 	/*
+ 	 * We don't know what the user intends to use this umem for, but the HW
+ 	 * restrictions must be met. MR, doorbell records, QP, WQ and CQ all
+ 	 * have different requirements. Since we have no idea how to sort this
+ 	 * out, only support PAGE_SIZE with the expectation that userspace will
+ 	 * provide the necessary alignments inside the known PAGE_SIZE and that
+ 	 * FW will check everything.
+ 	 */
+ 	page_size = ib_umem_find_best_pgoff(
+ 		obj->umem, PAGE_SIZE,
+ 		__mlx5_page_offset_to_bitmask(__mlx5_bit_sz(umem, page_offset),
+ 					      0));
+ 	if (!page_size)
+ 		return -EINVAL;
+ 
+ 	cmd->inlen = MLX5_ST_SZ_BYTES(create_umem_in) +
+ 		     (MLX5_ST_SZ_BYTES(mtt) *
+ 		      ib_umem_num_dma_blocks(obj->umem, page_size));
+ 	cmd->in = uverbs_zalloc(attrs, cmd->inlen);
+ 	if (!cmd->in)
+ 		return PTR_ERR(cmd->in);
  
  	umem = MLX5_ADDR_OF(create_umem_in, cmd->in, umem);
  	mtt = (__be64 *)MLX5_ADDR_OF(umem, umem, mtt);
  
  	MLX5_SET(create_umem_in, cmd->in, opcode, MLX5_CMD_OP_CREATE_UMEM);
++<<<<<<< HEAD
 +	MLX5_SET64(umem, umem, num_of_mtt, obj->ncont);
 +	MLX5_SET(umem, umem, log_page_size, obj->page_shift -
 +					    MLX5_ADAPTER_PAGE_SHIFT);
 +	MLX5_SET(umem, umem, page_offset, obj->page_offset);
 +	mlx5_ib_populate_pas(dev, obj->umem, obj->page_shift, mtt,
 +			     (obj->umem->writable ? MLX5_IB_MTT_WRITE : 0) |
 +			     MLX5_IB_MTT_READ);
++=======
+ 	MLX5_SET64(umem, umem, num_of_mtt,
+ 		   ib_umem_num_dma_blocks(obj->umem, page_size));
+ 	MLX5_SET(umem, umem, log_page_size,
+ 		 order_base_2(page_size) - MLX5_ADAPTER_PAGE_SHIFT);
+ 	MLX5_SET(umem, umem, page_offset,
+ 		 ib_umem_dma_offset(obj->umem, page_size));
+ 
+ 	mlx5_ib_populate_pas(obj->umem, page_size, mtt,
+ 			     (obj->umem->writable ? MLX5_IB_MTT_WRITE : 0) |
+ 				     MLX5_IB_MTT_READ);
+ 	return 0;
++>>>>>>> 878f7b31c3a7 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx)
  }
  
  static int UVERBS_HANDLER(MLX5_IB_METHOD_DEVX_UMEM_REG)(
diff --cc drivers/infiniband/hw/mlx5/mem.c
index 13de3d2edd34,f8ec5156d8e9..000000000000
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@@ -36,161 -36,65 +36,164 @@@
  #include "mlx5_ib.h"
  #include <linux/jiffies.h>
  
 -/*
 - * Fill in a physical address list. ib_umem_num_dma_blocks() entries will be
 - * filled in the pas array.
++<<<<<<< HEAD
 +/* @umem: umem object to scan
 + * @addr: ib virtual address requested by the user
 + * @max_page_shift: high limit for page_shift - 0 means no limit
 + * @count: number of PAGE_SIZE pages covered by umem
 + * @shift: page shift for the compound pages found in the region
 + * @ncont: number of compund pages
 + * @order: log2 of the number of compound pages
   */
 -void mlx5_ib_populate_pas(struct ib_umem *umem, size_t page_size, __be64 *pas,
 -			  u64 access_flags)
 +void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr,
 +			unsigned long max_page_shift,
 +			int *count, int *shift,
 +			int *ncont, int *order)
  {
 -	struct ib_block_iter biter;
 +	unsigned long tmp;
 +	unsigned long m;
 +	u64 base = ~0, p = 0;
 +	u64 len, pfn;
 +	int i = 0;
 +	struct scatterlist *sg;
 +	int entry;
 +
 +	addr = addr >> PAGE_SHIFT;
 +	tmp = (unsigned long)addr;
 +	m = find_first_bit(&tmp, BITS_PER_LONG);
 +	if (max_page_shift)
 +		m = min_t(unsigned long, max_page_shift - PAGE_SHIFT, m);
 +
 +	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {
 +		len = sg_dma_len(sg) >> PAGE_SHIFT;
 +		pfn = sg_dma_address(sg) >> PAGE_SHIFT;
 +		if (base + p != pfn) {
 +			/* If either the offset or the new
 +			 * base are unaligned update m
 +			 */
 +			tmp = (unsigned long)(pfn | p);
 +			if (!IS_ALIGNED(tmp, 1 << m))
 +				m = find_first_bit(&tmp, BITS_PER_LONG);
 +
 +			base = pfn;
 +			p = 0;
 +		}
 +
 +		p += len;
 +		i += len;
 +	}
  
 -	rdma_umem_for_each_dma_block (umem, &biter, page_size) {
 -		*pas = cpu_to_be64(rdma_block_iter_dma_address(&biter) |
 -				   access_flags);
 -		pas++;
 +	if (i) {
 +		m = min_t(unsigned long, ilog2(roundup_pow_of_two(i)), m);
 +
 +		if (order)
 +			*order = ilog2(roundup_pow_of_two(i) >> m);
 +
 +		*ncont = DIV_ROUND_UP(i, (1 << m));
 +	} else {
 +		m  = 0;
 +
 +		if (order)
 +			*order = 0;
 +
 +		*ncont = 0;
  	}
 +	*shift = PAGE_SHIFT + m;
 +	*count = i;
  }
  
++=======
++>>>>>>> 878f7b31c3a7 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx)
  /*
 - * Compute the page shift and page_offset for mailboxes that use a quantized
 - * page_offset. The granulatity of the page offset scales according to page
 - * size.
 + * Populate the given array with bus addresses from the umem.
 + *
 + * dev - mlx5_ib device
 + * umem - umem to use to fill the pages
 + * page_shift - determines the page size used in the resulting array
 + * offset - offset into the umem to start from,
 + *          only implemented for ODP umems
 + * num_pages - total number of pages to fill
 + * pas - bus addresses array to fill
 + * access_flags - access flags to set on all present pages.
 +		  use enum mlx5_ib_mtt_access_flags for this.
   */
 -unsigned long __mlx5_umem_find_best_quantized_pgoff(
 -	struct ib_umem *umem, unsigned long pgsz_bitmap,
 -	unsigned int page_offset_bits, u64 pgoff_bitmask, unsigned int scale,
 -	unsigned int *page_offset_quantized)
 +void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			    int page_shift, size_t offset, size_t num_pages,
 +			    __be64 *pas, int access_flags)
  {
 -	const u64 page_offset_mask = (1 << page_offset_bits) - 1;
 -	unsigned long page_size;
 -	u64 page_offset;
 -
 -	page_size = ib_umem_find_best_pgoff(umem, pgsz_bitmap, pgoff_bitmask);
 -	if (!page_size)
 -		return 0;
 -
 -	/*
 -	 * page size is the largest possible page size.
 -	 *
 -	 * Reduce the page_size, and thus the page_offset and quanta, until the
 -	 * page_offset fits into the mailbox field. Once page_size < scale this
 -	 * loop is guaranteed to terminate.
 -	 */
 -	page_offset = ib_umem_dma_offset(umem, page_size);
 -	while (page_offset & ~(u64)(page_offset_mask * (page_size / scale))) {
 -		page_size /= 2;
 -		page_offset = ib_umem_dma_offset(umem, page_size);
 +	int shift = page_shift - PAGE_SHIFT;
 +	int mask = (1 << shift) - 1;
 +	int i, k, idx;
 +	u64 cur = 0;
 +	u64 base;
 +	int len;
 +	struct scatterlist *sg;
 +	int entry;
 +
 +	i = 0;
 +	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {
 +		len = sg_dma_len(sg) >> PAGE_SHIFT;
 +		base = sg_dma_address(sg);
 +
 +		/* Skip elements below offset */
 +		if (i + len < offset << shift) {
 +			i += len;
 +			continue;
 +		}
 +
 +		/* Skip pages below offset */
 +		if (i < offset << shift) {
 +			k = (offset << shift) - i;
 +			i = offset << shift;
 +		} else {
 +			k = 0;
 +		}
 +
 +		for (; k < len; k++) {
 +			if (!(i & mask)) {
 +				cur = base + (k << PAGE_SHIFT);
 +				cur |= access_flags;
 +				idx = (i >> shift) - offset;
 +
 +				pas[idx] = cpu_to_be64(cur);
 +				mlx5_ib_dbg(dev, "pas[%d] 0x%llx\n",
 +					    i >> shift, be64_to_cpu(pas[idx]));
 +			}
 +			i++;
 +
 +			/* Stop after num_pages reached */
 +			if (i >> shift >= offset + num_pages)
 +				return;
 +		}
  	}
 +}
 +
 +void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			  int page_shift, __be64 *pas, int access_flags)
 +{
 +	return __mlx5_ib_populate_pas(dev, umem, page_shift, 0,
 +				      ib_umem_num_dma_blocks(umem, PAGE_SIZE),
 +				      pas, access_flags);
 +}
 +int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset)
 +{
 +	u64 page_size;
 +	u64 page_mask;
 +	u64 off_size;
 +	u64 off_mask;
 +	u64 buf_off;
 +
 +	page_size = (u64)1 << page_shift;
 +	page_mask = page_size - 1;
 +	buf_off = addr & page_mask;
 +	off_size = page_size >> 6;
 +	off_mask = off_size - 1;
 +
 +	if (buf_off & off_mask)
 +		return -EINVAL;
  
 -	/*
 -	 * The address is not aligned, or otherwise cannot be represented by the
 -	 * page_offset.
 -	 */
 -	if (!(pgsz_bitmap & page_size))
 -		return 0;
 -
 -	*page_offset_quantized =
 -		(unsigned long)page_offset / (page_size / scale);
 -	if (WARN_ON(*page_offset_quantized > page_offset_mask))
 -		return 0;
 -	return page_size;
 +	*offset = buf_off >> ilog2(off_size);
 +	return 0;
  }
  
  #define WR_ID_BF 0xBF
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index adddf68c9489,718e59fce006..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -40,7 -40,73 +40,77 @@@
  #define MLX5_IB_DEFAULT_UIDX 0xffffff
  #define MLX5_USER_ASSIGNED_UIDX_MASK __mlx5_mask(qpc, user_index)
  
++<<<<<<< HEAD
 +#define MLX5_MKEY_PAGE_SHIFT_MASK __mlx5_mask(mkc, log_page_size)
++=======
+ static __always_inline unsigned long
+ __mlx5_log_page_size_to_bitmap(unsigned int log_pgsz_bits,
+ 			       unsigned int pgsz_shift)
+ {
+ 	unsigned int largest_pg_shift =
+ 		min_t(unsigned long, (1ULL << log_pgsz_bits) - 1 + pgsz_shift,
+ 		      BITS_PER_LONG - 1);
+ 
+ 	/*
+ 	 * Despite a command allowing it, the device does not support lower than
+ 	 * 4k page size.
+ 	 */
+ 	pgsz_shift = max_t(unsigned int, MLX5_ADAPTER_PAGE_SHIFT, pgsz_shift);
+ 	return GENMASK(largest_pg_shift, pgsz_shift);
+ }
+ 
+ /*
+  * For mkc users, instead of a page_offset the command has a start_iova which
+  * specifies both the page_offset and the on-the-wire IOVA
+  */
+ #define mlx5_umem_find_best_pgsz(umem, typ, log_pgsz_fld, pgsz_shift, iova)    \
+ 	ib_umem_find_best_pgsz(umem,                                           \
+ 			       __mlx5_log_page_size_to_bitmap(                 \
+ 				       __mlx5_bit_sz(typ, log_pgsz_fld),       \
+ 				       pgsz_shift),                            \
+ 			       iova)
+ 
+ static __always_inline unsigned long
+ __mlx5_page_offset_to_bitmask(unsigned int page_offset_bits,
+ 			      unsigned int offset_shift)
+ {
+ 	unsigned int largest_offset_shift =
+ 		min_t(unsigned long, page_offset_bits - 1 + offset_shift,
+ 		      BITS_PER_LONG - 1);
+ 
+ 	return GENMASK(largest_offset_shift, offset_shift);
+ }
+ 
+ /*
+  * QP/CQ/WQ/etc type commands take a page offset that satisifies:
+  *   page_offset_quantized * (page_size/scale) = page_offset
+  * Which restricts allowed page sizes to ones that satisify the above.
+  */
+ unsigned long __mlx5_umem_find_best_quantized_pgoff(
+ 	struct ib_umem *umem, unsigned long pgsz_bitmap,
+ 	unsigned int page_offset_bits, u64 pgoff_bitmask, unsigned int scale,
+ 	unsigned int *page_offset_quantized);
+ #define mlx5_umem_find_best_quantized_pgoff(umem, typ, log_pgsz_fld,           \
+ 					    pgsz_shift, page_offset_fld,       \
+ 					    scale, page_offset_quantized)      \
+ 	__mlx5_umem_find_best_quantized_pgoff(                                 \
+ 		umem,                                                          \
+ 		__mlx5_log_page_size_to_bitmap(                                \
+ 			__mlx5_bit_sz(typ, log_pgsz_fld), pgsz_shift),         \
+ 		__mlx5_bit_sz(typ, page_offset_fld),                           \
+ 		GENMASK(31, order_base_2(scale)), scale,                       \
+ 		page_offset_quantized)
+ 
+ #define mlx5_umem_find_best_cq_quantized_pgoff(umem, typ, log_pgsz_fld,        \
+ 					       pgsz_shift, page_offset_fld,    \
+ 					       scale, page_offset_quantized)   \
+ 	__mlx5_umem_find_best_quantized_pgoff(                                 \
+ 		umem,                                                          \
+ 		__mlx5_log_page_size_to_bitmap(                                \
+ 			__mlx5_bit_sz(typ, log_pgsz_fld), pgsz_shift),         \
+ 		__mlx5_bit_sz(typ, page_offset_fld), 0, scale,                 \
+ 		page_offset_quantized)
++>>>>>>> 878f7b31c3a7 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx)
  
  enum {
  	MLX5_IB_MMAP_OFFSET_START = 9,
@@@ -1232,15 -1294,8 +1302,20 @@@ int mlx5_query_mad_ifc_port(struct ib_d
  			    struct ib_port_attr *props);
  int mlx5_ib_query_port(struct ib_device *ibdev, u8 port,
  		       struct ib_port_attr *props);
++<<<<<<< HEAD
 +void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr,
 +			unsigned long max_page_shift,
 +			int *count, int *shift,
 +			int *ncont, int *order);
 +void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			    int page_shift, size_t offset, size_t num_pages,
 +			    __be64 *pas, int access_flags);
 +void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			  int page_shift, __be64 *pas, int access_flags);
++=======
+ void mlx5_ib_populate_pas(struct ib_umem *umem, size_t page_size, __be64 *pas,
+ 			  u64 access_flags);
++>>>>>>> 878f7b31c3a7 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for devx)
  void mlx5_ib_copy_pas(u64 *old, u64 *new, int step, int num);
  int mlx5_ib_get_cqe_size(struct ib_cq *ibcq);
  int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
* Unmerged path drivers/infiniband/hw/mlx5/devx.c
* Unmerged path drivers/infiniband/hw/mlx5/mem.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
