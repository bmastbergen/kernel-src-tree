dma-direct check for highmem pages in dma_direct_alloc_pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 08a89c28304ae74e4c7422f784359e41a37e3e7c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/08a89c28.failed

Check for highmem pages from CMA, just like in the dma_direct_alloc path.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 08a89c28304ae74e4c7422f784359e41a37e3e7c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index b992d9926af3,f5ecadd4e1c1..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -291,6 -292,67 +291,70 @@@ void dma_direct_free(struct device *dev
  	dma_free_contiguous(dev, dma_direct_to_page(dev, dma_addr), size);
  }
  
++<<<<<<< HEAD
++=======
+ struct page *dma_direct_alloc_pages(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)
+ {
+ 	struct page *page;
+ 	void *ret;
+ 
+ 	if (dma_should_alloc_from_pool(dev, gfp, 0)) {
+ 		page = dma_alloc_from_pool(dev, size, &ret, gfp,
+ 				dma_coherent_ok);
+ 		if (!page)
+ 			return NULL;
+ 		goto done;
+ 	}
+ 
+ 	page = __dma_direct_alloc_pages(dev, size, gfp);
+ 	if (!page)
+ 		return NULL;
+ 	if (PageHighMem(page)) {
+ 		/*
+ 		 * Depending on the cma= arguments and per-arch setup
+ 		 * dma_alloc_contiguous could return highmem pages.
+ 		 * Without remapping there is no way to return them here,
+ 		 * so log an error and fail.
+ 		 */
+ 		dev_info(dev, "Rejecting highmem page from CMA.\n");
+ 		goto out_free_pages;
+ 	}
+ 
+ 	ret = page_address(page);
+ 	if (force_dma_unencrypted(dev)) {
+ 		if (set_memory_decrypted((unsigned long)ret,
+ 				1 << get_order(size)))
+ 			goto out_free_pages;
+ 	}
+ 	memset(ret, 0, size);
+ done:
+ 	*dma_handle = phys_to_dma_direct(dev, page_to_phys(page));
+ 	return page;
+ out_free_pages:
+ 	dma_free_contiguous(dev, page, size);
+ 	return NULL;
+ }
+ 
+ void dma_direct_free_pages(struct device *dev, size_t size,
+ 		struct page *page, dma_addr_t dma_addr,
+ 		enum dma_data_direction dir)
+ {
+ 	unsigned int page_order = get_order(size);
+ 	void *vaddr = page_address(page);
+ 
+ 	/* If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */
+ 	if (dma_should_free_from_pool(dev, 0) &&
+ 	    dma_free_from_pool(dev, vaddr, size))
+ 		return;
+ 
+ 	if (force_dma_unencrypted(dev))
+ 		set_memory_encrypted((unsigned long)vaddr, 1 << page_order);
+ 
+ 	dma_free_contiguous(dev, page, size);
+ }
+ 
++>>>>>>> 08a89c28304a (dma-direct check for highmem pages in dma_direct_alloc_pages)
  #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
      defined(CONFIG_SWIOTLB)
  void dma_direct_sync_sg_for_device(struct device *dev,
* Unmerged path kernel/dma/direct.c
