scsi: sbitmap: Maintain allocation round_robin in sbitmap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Ming Lei <ming.lei@redhat.com>
commit efe1f3a1d5833c0ddd61ee50dbef8908f65a0a5e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/efe1f3a1.failed

Currently the allocation round_robin info is maintained by sbitmap_queue.

However, bit allocation really belongs to sbitmap. Move it there.

Link: https://lore.kernel.org/r/20210122023317.687987-3-ming.lei@redhat.com
	Cc: Omar Sandoval <osandov@fb.com>
	Cc: Kashyap Desai <kashyap.desai@broadcom.com>
	Cc: Sumanesh Samanta <sumanesh.samanta@broadcom.com>
	Cc: Ewan D. Milne <emilne@redhat.com>
	Cc: Hannes Reinecke <hare@suse.de>
	Cc: virtualization@lists.linux-foundation.org
	Tested-by: Sumanesh Samanta <sumanesh.samanta@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.de>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit efe1f3a1d5833c0ddd61ee50dbef8908f65a0a5e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vhost/scsi.c
diff --cc drivers/vhost/scsi.c
index 227271ef91e4,3412a3ad28e6..000000000000
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@@ -569,9 -613,8 +569,13 @@@ vhost_scsi_get_tag(struct vhost_virtque
  		pr_err("Unable to locate active struct vhost_scsi_nexus\n");
  		return ERR_PTR(-EIO);
  	}
 +	se_sess = tv_nexus->tvn_se_sess;
  
++<<<<<<< HEAD
 +	tag = sbitmap_queue_get(&se_sess->sess_tag_pool, &cpu);
++=======
+ 	tag = sbitmap_get(&svq->scsi_tags, 0);
++>>>>>>> efe1f3a1d583 (scsi: sbitmap: Maintain allocation round_robin in sbitmap)
  	if (tag < 0) {
  		pr_err("Unable to obtain tag for vhost_scsi_cmd\n");
  		return ERR_PTR(-ENOMEM);
@@@ -1137,6 -1478,83 +1141,86 @@@ static void vhost_scsi_flush(struct vho
  		wait_for_completion(&old_inflight[i]->comp);
  }
  
++<<<<<<< HEAD
++=======
+ static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
+ {
+ 	struct vhost_scsi_virtqueue *svq = container_of(vq,
+ 					struct vhost_scsi_virtqueue, vq);
+ 	struct vhost_scsi_cmd *tv_cmd;
+ 	unsigned int i;
+ 
+ 	if (!svq->scsi_cmds)
+ 		return;
+ 
+ 	for (i = 0; i < svq->max_cmds; i++) {
+ 		tv_cmd = &svq->scsi_cmds[i];
+ 
+ 		kfree(tv_cmd->tvc_sgl);
+ 		kfree(tv_cmd->tvc_prot_sgl);
+ 		kfree(tv_cmd->tvc_upages);
+ 	}
+ 
+ 	sbitmap_free(&svq->scsi_tags);
+ 	kfree(svq->scsi_cmds);
+ 	svq->scsi_cmds = NULL;
+ }
+ 
+ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
+ {
+ 	struct vhost_scsi_virtqueue *svq = container_of(vq,
+ 					struct vhost_scsi_virtqueue, vq);
+ 	struct vhost_scsi_cmd *tv_cmd;
+ 	unsigned int i;
+ 
+ 	if (svq->scsi_cmds)
+ 		return 0;
+ 
+ 	if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+ 			      NUMA_NO_NODE, false))
+ 		return -ENOMEM;
+ 	svq->max_cmds = max_cmds;
+ 
+ 	svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+ 	if (!svq->scsi_cmds) {
+ 		sbitmap_free(&svq->scsi_tags);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for (i = 0; i < max_cmds; i++) {
+ 		tv_cmd = &svq->scsi_cmds[i];
+ 
+ 		tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+ 					  sizeof(struct scatterlist),
+ 					  GFP_KERNEL);
+ 		if (!tv_cmd->tvc_sgl) {
+ 			pr_err("Unable to allocate tv_cmd->tvc_sgl\n");
+ 			goto out;
+ 		}
+ 
+ 		tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+ 					     sizeof(struct page *),
+ 					     GFP_KERNEL);
+ 		if (!tv_cmd->tvc_upages) {
+ 			pr_err("Unable to allocate tv_cmd->tvc_upages\n");
+ 			goto out;
+ 		}
+ 
+ 		tv_cmd->tvc_prot_sgl = kcalloc(VHOST_SCSI_PREALLOC_PROT_SGLS,
+ 					       sizeof(struct scatterlist),
+ 					       GFP_KERNEL);
+ 		if (!tv_cmd->tvc_prot_sgl) {
+ 			pr_err("Unable to allocate tv_cmd->tvc_prot_sgl\n");
+ 			goto out;
+ 		}
+ 	}
+ 	return 0;
+ out:
+ 	vhost_scsi_destroy_vq_cmds(vq);
+ 	return -ENOMEM;
+ }
+ 
++>>>>>>> efe1f3a1d583 (scsi: sbitmap: Maintain allocation round_robin in sbitmap)
  /*
   * Called from vhost_scsi_ioctl() context to walk the list of available
   * vhost_scsi_tpg with an active struct vhost_scsi_nexus
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 2c7e4bee30bc..819e69fb2fd3 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -2800,7 +2800,7 @@ blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		goto free_cpumask;
 
 	if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
-				gfp, node))
+				gfp, node, false))
 		goto free_ctxs;
 	hctx->nr_ctx = 0;
 
diff --git a/block/kyber-iosched.c b/block/kyber-iosched.c
index 338c991ee47e..66444bf96d94 100644
--- a/block/kyber-iosched.c
+++ b/block/kyber-iosched.c
@@ -487,7 +487,8 @@ static int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 
 	for (i = 0; i < KYBER_NUM_DOMAINS; i++) {
 		if (sbitmap_init_node(&khd->kcq_map[i], hctx->nr_ctx,
-				      ilog2(8), GFP_KERNEL, hctx->numa_node)) {
+				      ilog2(8), GFP_KERNEL, hctx->numa_node,
+				      false)) {
 			while (--i >= 0)
 				sbitmap_free(&khd->kcq_map[i]);
 			goto err_kcqs;
* Unmerged path drivers/vhost/scsi.c
diff --git a/include/linux/sbitmap.h b/include/linux/sbitmap.h
index 0d021ecff388..b03a0db75b77 100644
--- a/include/linux/sbitmap.h
+++ b/include/linux/sbitmap.h
@@ -72,6 +72,11 @@ struct sbitmap {
 	 */
 	unsigned int map_nr;
 
+	/**
+	 * @round_robin: Allocate bits in strict round-robin order.
+	 */
+	bool round_robin;
+
 	/**
 	 * @map: Allocated bitmap.
 	 */
@@ -140,11 +145,6 @@ struct sbitmap_queue {
 	 */
 	atomic_t ws_active;
 
-	/**
-	 * @round_robin: Allocate bits in strict round-robin order.
-	 */
-	bool round_robin;
-
 	/**
 	 * @min_shallow_depth: The minimum shallow depth which may be passed to
 	 * sbitmap_queue_get_shallow() or __sbitmap_queue_get_shallow().
@@ -160,11 +160,14 @@ struct sbitmap_queue {
  *         given, a good default is chosen.
  * @flags: Allocation flags.
  * @node: Memory node to allocate on.
+ * @round_robin: If true, be stricter about allocation order; always allocate
+ *               starting from the last allocated bit. This is less efficient
+ *               than the default behavior (false).
  *
  * Return: Zero on success or negative errno on failure.
  */
 int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
-		      gfp_t flags, int node);
+		      gfp_t flags, int node, bool round_robin);
 
 /**
  * sbitmap_free() - Free memory used by a &struct sbitmap.
@@ -190,15 +193,12 @@ void sbitmap_resize(struct sbitmap *sb, unsigned int depth);
  * sbitmap_get() - Try to allocate a free bit from a &struct sbitmap.
  * @sb: Bitmap to allocate from.
  * @alloc_hint: Hint for where to start searching for a free bit.
- * @round_robin: If true, be stricter about allocation order; always allocate
- *               starting from the last allocated bit. This is less efficient
- *               than the default behavior (false).
  *
  * This operation provides acquire barrier semantics if it succeeds.
  *
  * Return: Non-negative allocated bit number if successful, -1 otherwise.
  */
-int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin);
+int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint);
 
 /**
  * sbitmap_get_shallow() - Try to allocate a free bit from a &struct sbitmap,
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index d69d338697cf..68215e09662f 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -44,7 +44,7 @@ static inline bool sbitmap_deferred_clear(struct sbitmap_word *map)
 }
 
 int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
-		      gfp_t flags, int node)
+		      gfp_t flags, int node, bool round_robin)
 {
 	unsigned int bits_per_word;
 	unsigned int i;
@@ -69,6 +69,7 @@ int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 	sb->shift = shift;
 	sb->depth = depth;
 	sb->map_nr = DIV_ROUND_UP(sb->depth, bits_per_word);
+	sb->round_robin = round_robin;
 
 	if (depth == 0) {
 		sb->map = NULL;
@@ -138,14 +139,14 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 }
 
 static int sbitmap_find_bit_in_index(struct sbitmap *sb, int index,
-				     unsigned int alloc_hint, bool round_robin)
+				     unsigned int alloc_hint)
 {
 	struct sbitmap_word *map = &sb->map[index];
 	int nr;
 
 	do {
 		nr = __sbitmap_get_word(&map->word, map->depth, alloc_hint,
-					!round_robin);
+					!sb->round_robin);
 		if (nr != -1)
 			break;
 		if (!sbitmap_deferred_clear(map))
@@ -155,7 +156,7 @@ static int sbitmap_find_bit_in_index(struct sbitmap *sb, int index,
 	return nr;
 }
 
-int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
+int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint)
 {
 	unsigned int i, index;
 	int nr = -1;
@@ -167,14 +168,13 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 	 * alloc_hint to find the right word index. No point in looping
 	 * twice in find_next_zero_bit() for that case.
 	 */
-	if (round_robin)
+	if (sb->round_robin)
 		alloc_hint = SB_NR_TO_BIT(sb, alloc_hint);
 	else
 		alloc_hint = 0;
 
 	for (i = 0; i < sb->map_nr; i++) {
-		nr = sbitmap_find_bit_in_index(sb, index, alloc_hint,
-						round_robin);
+		nr = sbitmap_find_bit_in_index(sb, index, alloc_hint);
 		if (nr != -1) {
 			nr += index << sb->shift;
 			break;
@@ -359,7 +359,8 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 	int ret;
 	int i;
 
-	ret = sbitmap_init_node(&sbq->sb, depth, shift, flags, node);
+	ret = sbitmap_init_node(&sbq->sb, depth, shift, flags, node,
+				round_robin);
 	if (ret)
 		return ret;
 
@@ -391,7 +392,6 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 		atomic_set(&sbq->ws[i].wait_cnt, sbq->wake_batch);
 	}
 
-	sbq->round_robin = round_robin;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_init_node);
@@ -433,12 +433,12 @@ int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 		hint = depth ? prandom_u32() % depth : 0;
 		this_cpu_write(*sbq->alloc_hint, hint);
 	}
-	nr = sbitmap_get(&sbq->sb, hint, sbq->round_robin);
+	nr = sbitmap_get(&sbq->sb, hint);
 
 	if (nr == -1) {
 		/* If the map is full, a hint won't do us much good. */
 		this_cpu_write(*sbq->alloc_hint, 0);
-	} else if (nr == hint || unlikely(sbq->round_robin)) {
+	} else if (nr == hint || unlikely(sbq->sb.round_robin)) {
 		/* Only update the hint if we used it. */
 		hint = nr + 1;
 		if (hint >= depth - 1)
@@ -469,7 +469,7 @@ int __sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
 	if (nr == -1) {
 		/* If the map is full, a hint won't do us much good. */
 		this_cpu_write(*sbq->alloc_hint, 0);
-	} else if (nr == hint || unlikely(sbq->round_robin)) {
+	} else if (nr == hint || unlikely(sbq->sb.round_robin)) {
 		/* Only update the hint if we used it. */
 		hint = nr + 1;
 		if (hint >= depth - 1)
@@ -585,7 +585,7 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 	smp_mb__after_atomic();
 	sbitmap_queue_wake_up(sbq);
 
-	if (likely(!sbq->round_robin && nr < sbq->sb.depth))
+	if (likely(!sbq->sb.round_robin && nr < sbq->sb.depth))
 		*per_cpu_ptr(sbq->alloc_hint, cpu) = nr;
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
@@ -642,7 +642,7 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 	}
 	seq_puts(m, "}\n");
 
-	seq_printf(m, "round_robin=%d\n", sbq->round_robin);
+	seq_printf(m, "round_robin=%d\n", sbq->sb.round_robin);
 	seq_printf(m, "min_shallow_depth=%u\n", sbq->min_shallow_depth);
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);
