RDMA/mlx5: mlx5_umem_find_best_quantized_pgoff() for CQ

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit c08fbdc57741026a440d01593e09e11b60b3e210
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/c08fbdc5.failed

This fixes a bug where the page_offset was not being considered when
building a CQ. The HW specification says it 'must be zero', so use
a variant of mlx5_umem_find_best_quantized_pgoff() with a 0 pgoff_bitmask
to force this result.

Fixes: e126ba97dba9 ("mlx5: Add driver for Mellanox Connect-IB adapters")
Link: https://lore.kernel.org/r/20201115114311.136250-6-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit c08fbdc57741026a440d01593e09e11b60b3e210)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/cq.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --cc drivers/infiniband/hw/mlx5/cq.c
index 97c9edcdf214,eb92cefffd77..000000000000
--- a/drivers/infiniband/hw/mlx5/cq.c
+++ b/drivers/infiniband/hw/mlx5/cq.c
@@@ -707,10 -707,10 +707,11 @@@ static int create_cq_user(struct mlx5_i
  			  int *cqe_size, int *index, int *inlen)
  {
  	struct mlx5_ib_create_cq ucmd = {};
+ 	unsigned long page_size;
+ 	unsigned int page_offset_quantized;
  	size_t ucmdlen;
- 	int page_shift;
  	__be64 *pas;
 +	int npages;
  	int ncont;
  	void *cqc;
  	int err;
@@@ -746,10 -754,12 +755,19 @@@
  	if (err)
  		goto err_umem;
  
++<<<<<<< HEAD
 +	mlx5_ib_cont_pages(cq->buf.umem, ucmd.buf_addr, 0, &npages, &page_shift,
 +			   &ncont, NULL);
 +	mlx5_ib_dbg(dev, "addr 0x%llx, size %u, npages %d, page_shift %d, ncont %d\n",
 +		    ucmd.buf_addr, entries * ucmd.cqe_size, npages, page_shift, ncont);
++=======
+ 	ncont = ib_umem_num_dma_blocks(cq->buf.umem, page_size);
+ 	mlx5_ib_dbg(
+ 		dev,
+ 		"addr 0x%llx, size %u, npages %zu, page_size %lu, ncont %d\n",
+ 		ucmd.buf_addr, entries * ucmd.cqe_size,
+ 		ib_umem_num_pages(cq->buf.umem), page_size, ncont);
++>>>>>>> c08fbdc57741 (RDMA/mlx5: mlx5_umem_find_best_quantized_pgoff() for CQ)
  
  	*inlen = MLX5_ST_SZ_BYTES(create_cq_in) +
  		 MLX5_FLD_SZ_BYTES(create_cq_in, pas[0]) * ncont;
@@@ -760,7 -770,7 +778,11 @@@
  	}
  
  	pas = (__be64 *)MLX5_ADDR_OF(create_cq_in, *cqb, pas);
++<<<<<<< HEAD
 +	mlx5_ib_populate_pas(dev, cq->buf.umem, page_shift, pas, 0);
++=======
+ 	mlx5_ib_populate_pas(cq->buf.umem, page_size, pas, 0);
++>>>>>>> c08fbdc57741 (RDMA/mlx5: mlx5_umem_find_best_quantized_pgoff() for CQ)
  
  	cqc = MLX5_ADDR_OF(create_cq_in, *cqb, cq_context);
  	MLX5_SET(cqc, cqc, log_page_size,
@@@ -1128,8 -1139,8 +1151,13 @@@ int mlx5_ib_modify_cq(struct ib_cq *cq
  }
  
  static int resize_user(struct mlx5_ib_dev *dev, struct mlx5_ib_cq *cq,
++<<<<<<< HEAD
 +		       int entries, struct ib_udata *udata, int *npas,
 +		       int *page_shift, int *cqe_size)
++=======
+ 		       int entries, struct ib_udata *udata,
+ 		       int *cqe_size)
++>>>>>>> c08fbdc57741 (RDMA/mlx5: mlx5_umem_find_best_quantized_pgoff() for CQ)
  {
  	struct mlx5_ib_resize_cq ucmd;
  	struct ib_umem *umem;
@@@ -1155,9 -1165,6 +1183,12 @@@
  		return err;
  	}
  
++<<<<<<< HEAD
 +	mlx5_ib_cont_pages(umem, ucmd.buf_addr, 0, &npages, page_shift,
 +			   npas, NULL);
 +
++=======
++>>>>>>> c08fbdc57741 (RDMA/mlx5: mlx5_umem_find_best_quantized_pgoff() for CQ)
  	cq->resize_umem = umem;
  	*cqe_size = ucmd.cqe_size;
  
@@@ -1250,9 -1257,10 +1281,10 @@@ int mlx5_ib_resize_cq(struct ib_cq *ibc
  	int err;
  	int npas;
  	__be64 *pas;
- 	int page_shift;
+ 	unsigned int page_offset_quantized = 0;
+ 	unsigned int page_shift;
  	int inlen;
 -	int cqe_size;
 +	int uninitialized_var(cqe_size);
  	unsigned long flags;
  
  	if (!MLX5_CAP_GEN(dev->mdev, cq_resize)) {
@@@ -1277,22 -1285,34 +1309,41 @@@
  
  	mutex_lock(&cq->resize_mutex);
  	if (udata) {
++<<<<<<< HEAD
 +		err = resize_user(dev, cq, entries, udata, &npas, &page_shift,
 +				  &cqe_size);
++=======
+ 		unsigned long page_size;
+ 
+ 		err = resize_user(dev, cq, entries, udata, &cqe_size);
+ 		if (err)
+ 			goto ex;
+ 
+ 		page_size = mlx5_umem_find_best_cq_quantized_pgoff(
+ 			cq->resize_umem, cqc, log_page_size,
+ 			MLX5_ADAPTER_PAGE_SHIFT, page_offset, 64,
+ 			&page_offset_quantized);
+ 		if (!page_size) {
+ 			err = -EINVAL;
+ 			goto ex_resize;
+ 		}
+ 		npas = ib_umem_num_dma_blocks(cq->resize_umem, page_size);
+ 		page_shift = order_base_2(page_size);
++>>>>>>> c08fbdc57741 (RDMA/mlx5: mlx5_umem_find_best_quantized_pgoff() for CQ)
  	} else {
 -		struct mlx5_frag_buf *frag_buf;
 -
  		cqe_size = 64;
  		err = resize_kernel(dev, cq, entries, cqe_size);
 -		if (err)
 -			goto ex;
 -		frag_buf = &cq->resize_buf->frag_buf;
 -		npas = frag_buf->npages;
 -		page_shift = frag_buf->page_shift;
 +		if (!err) {
 +			struct mlx5_frag_buf *frag_buf = &cq->resize_buf->frag_buf;
 +
 +			npas = frag_buf->npages;
 +			page_shift = frag_buf->page_shift;
 +		}
  	}
  
 +	if (err)
 +		goto ex;
 +
  	inlen = MLX5_ST_SZ_BYTES(modify_cq_in) +
  		MLX5_FLD_SZ_BYTES(modify_cq_in, pas[0]) * npas;
  
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index adddf68c9489,3098a7e0d186..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -42,6 -42,74 +42,77 @@@
  
  #define MLX5_MKEY_PAGE_SHIFT_MASK __mlx5_mask(mkc, log_page_size)
  
++<<<<<<< HEAD
++=======
+ static __always_inline unsigned long
+ __mlx5_log_page_size_to_bitmap(unsigned int log_pgsz_bits,
+ 			       unsigned int pgsz_shift)
+ {
+ 	unsigned int largest_pg_shift =
+ 		min_t(unsigned long, (1ULL << log_pgsz_bits) - 1 + pgsz_shift,
+ 		      BITS_PER_LONG - 1);
+ 
+ 	/*
+ 	 * Despite a command allowing it, the device does not support lower than
+ 	 * 4k page size.
+ 	 */
+ 	pgsz_shift = max_t(unsigned int, MLX5_ADAPTER_PAGE_SHIFT, pgsz_shift);
+ 	return GENMASK(largest_pg_shift, pgsz_shift);
+ }
+ 
+ /*
+  * For mkc users, instead of a page_offset the command has a start_iova which
+  * specifies both the page_offset and the on-the-wire IOVA
+  */
+ #define mlx5_umem_find_best_pgsz(umem, typ, log_pgsz_fld, pgsz_shift, iova)    \
+ 	ib_umem_find_best_pgsz(umem,                                           \
+ 			       __mlx5_log_page_size_to_bitmap(                 \
+ 				       __mlx5_bit_sz(typ, log_pgsz_fld),       \
+ 				       pgsz_shift),                            \
+ 			       iova)
+ 
+ static __always_inline unsigned long
+ __mlx5_page_offset_to_bitmask(unsigned int page_offset_bits,
+ 			      unsigned int offset_shift)
+ {
+ 	unsigned int largest_offset_shift =
+ 		min_t(unsigned long, page_offset_bits - 1 + offset_shift,
+ 		      BITS_PER_LONG - 1);
+ 
+ 	return GENMASK(largest_offset_shift, offset_shift);
+ }
+ 
+ /*
+  * QP/CQ/WQ/etc type commands take a page offset that satisifies:
+  *   page_offset_quantized * (page_size/scale) = page_offset
+  * Which restricts allowed page sizes to ones that satisify the above.
+  */
+ unsigned long __mlx5_umem_find_best_quantized_pgoff(
+ 	struct ib_umem *umem, unsigned long pgsz_bitmap,
+ 	unsigned int page_offset_bits, u64 pgoff_bitmask, unsigned int scale,
+ 	unsigned int *page_offset_quantized);
+ #define mlx5_umem_find_best_quantized_pgoff(umem, typ, log_pgsz_fld,           \
+ 					    pgsz_shift, page_offset_fld,       \
+ 					    scale, page_offset_quantized)      \
+ 	__mlx5_umem_find_best_quantized_pgoff(                                 \
+ 		umem,                                                          \
+ 		__mlx5_log_page_size_to_bitmap(                                \
+ 			__mlx5_bit_sz(typ, log_pgsz_fld), pgsz_shift),         \
+ 		__mlx5_bit_sz(typ, page_offset_fld),                           \
+ 		GENMASK(31, order_base_2(scale)), scale,                       \
+ 		page_offset_quantized)
+ 
+ #define mlx5_umem_find_best_cq_quantized_pgoff(umem, typ, log_pgsz_fld,        \
+ 					       pgsz_shift, page_offset_fld,    \
+ 					       scale, page_offset_quantized)   \
+ 	__mlx5_umem_find_best_quantized_pgoff(                                 \
+ 		umem,                                                          \
+ 		__mlx5_log_page_size_to_bitmap(                                \
+ 			__mlx5_bit_sz(typ, log_pgsz_fld), pgsz_shift),         \
+ 		__mlx5_bit_sz(typ, page_offset_fld), 0, scale,                 \
+ 		page_offset_quantized)
+ 
++>>>>>>> c08fbdc57741 (RDMA/mlx5: mlx5_umem_find_best_quantized_pgoff() for CQ)
  enum {
  	MLX5_IB_MMAP_OFFSET_START = 9,
  	MLX5_IB_MMAP_OFFSET_END = 255,
* Unmerged path drivers/infiniband/hw/mlx5/cq.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
