iommu/io-pgtable: Remove tlb_flush_leaf

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit fefe8527a1e0e0014946c6b5b3b2e40cb32bb5d3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/fefe8527.failed

The only user of tlb_flush_leaf is a particularly hairy corner of the
Arm short-descriptor code, which wants a synchronous invalidation to
minimise the races inherent in trying to split a large page mapping.
This is already far enough into "here be dragons" territory that no
sensible caller should ever hit it, and thus it really doesn't need
optimising. Although using tlb_flush_walk there may technically be
more heavyweight than needed, it does the job and saves everyone else
having to carry around useless baggage.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Reviewed-by: Steven Price <steven.price@arm.com>
Link: https://lore.kernel.org/r/9844ab0c5cb3da8b2f89c6c2da16941910702b41.1606324115.git.robin.murphy@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit fefe8527a1e0e0014946c6b5b3b2e40cb32bb5d3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/msm/msm_iommu.c
#	drivers/gpu/drm/panfrost/panfrost_mmu.c
#	drivers/iommu/mtk_iommu.c
diff --cc drivers/gpu/drm/msm/msm_iommu.c
index b23d33622f37,50d881794758..000000000000
--- a/drivers/gpu/drm/msm/msm_iommu.c
+++ b/drivers/gpu/drm/msm/msm_iommu.c
@@@ -21,9 -12,201 +21,202 @@@
  struct msm_iommu {
  	struct msm_mmu base;
  	struct iommu_domain *domain;
 -	atomic_t pagetables;
  };
 -
  #define to_msm_iommu(x) container_of(x, struct msm_iommu, base)
  
++<<<<<<< HEAD
++=======
+ struct msm_iommu_pagetable {
+ 	struct msm_mmu base;
+ 	struct msm_mmu *parent;
+ 	struct io_pgtable_ops *pgtbl_ops;
+ 	phys_addr_t ttbr;
+ 	u32 asid;
+ };
+ static struct msm_iommu_pagetable *to_pagetable(struct msm_mmu *mmu)
+ {
+ 	return container_of(mmu, struct msm_iommu_pagetable, base);
+ }
+ 
+ static int msm_iommu_pagetable_unmap(struct msm_mmu *mmu, u64 iova,
+ 		size_t size)
+ {
+ 	struct msm_iommu_pagetable *pagetable = to_pagetable(mmu);
+ 	struct io_pgtable_ops *ops = pagetable->pgtbl_ops;
+ 	size_t unmapped = 0;
+ 
+ 	/* Unmap the block one page at a time */
+ 	while (size) {
+ 		unmapped += ops->unmap(ops, iova, 4096, NULL);
+ 		iova += 4096;
+ 		size -= 4096;
+ 	}
+ 
+ 	iommu_flush_iotlb_all(to_msm_iommu(pagetable->parent)->domain);
+ 
+ 	return (unmapped == size) ? 0 : -EINVAL;
+ }
+ 
+ static int msm_iommu_pagetable_map(struct msm_mmu *mmu, u64 iova,
+ 		struct sg_table *sgt, size_t len, int prot)
+ {
+ 	struct msm_iommu_pagetable *pagetable = to_pagetable(mmu);
+ 	struct io_pgtable_ops *ops = pagetable->pgtbl_ops;
+ 	struct scatterlist *sg;
+ 	size_t mapped = 0;
+ 	u64 addr = iova;
+ 	unsigned int i;
+ 
+ 	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
+ 		size_t size = sg->length;
+ 		phys_addr_t phys = sg_phys(sg);
+ 
+ 		/* Map the block one page at a time */
+ 		while (size) {
+ 			if (ops->map(ops, addr, phys, 4096, prot, GFP_KERNEL)) {
+ 				msm_iommu_pagetable_unmap(mmu, iova, mapped);
+ 				return -EINVAL;
+ 			}
+ 
+ 			phys += 4096;
+ 			addr += 4096;
+ 			size -= 4096;
+ 			mapped += 4096;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void msm_iommu_pagetable_destroy(struct msm_mmu *mmu)
+ {
+ 	struct msm_iommu_pagetable *pagetable = to_pagetable(mmu);
+ 	struct msm_iommu *iommu = to_msm_iommu(pagetable->parent);
+ 	struct adreno_smmu_priv *adreno_smmu =
+ 		dev_get_drvdata(pagetable->parent->dev);
+ 
+ 	/*
+ 	 * If this is the last attached pagetable for the parent,
+ 	 * disable TTBR0 in the arm-smmu driver
+ 	 */
+ 	if (atomic_dec_return(&iommu->pagetables) == 0)
+ 		adreno_smmu->set_ttbr0_cfg(adreno_smmu->cookie, NULL);
+ 
+ 	free_io_pgtable_ops(pagetable->pgtbl_ops);
+ 	kfree(pagetable);
+ }
+ 
+ int msm_iommu_pagetable_params(struct msm_mmu *mmu,
+ 		phys_addr_t *ttbr, int *asid)
+ {
+ 	struct msm_iommu_pagetable *pagetable;
+ 
+ 	if (mmu->type != MSM_MMU_IOMMU_PAGETABLE)
+ 		return -EINVAL;
+ 
+ 	pagetable = to_pagetable(mmu);
+ 
+ 	if (ttbr)
+ 		*ttbr = pagetable->ttbr;
+ 
+ 	if (asid)
+ 		*asid = pagetable->asid;
+ 
+ 	return 0;
+ }
+ 
+ static const struct msm_mmu_funcs pagetable_funcs = {
+ 		.map = msm_iommu_pagetable_map,
+ 		.unmap = msm_iommu_pagetable_unmap,
+ 		.destroy = msm_iommu_pagetable_destroy,
+ };
+ 
+ static void msm_iommu_tlb_flush_all(void *cookie)
+ {
+ }
+ 
+ static void msm_iommu_tlb_flush_walk(unsigned long iova, size_t size,
+ 		size_t granule, void *cookie)
+ {
+ }
+ 
+ static void msm_iommu_tlb_add_page(struct iommu_iotlb_gather *gather,
+ 		unsigned long iova, size_t granule, void *cookie)
+ {
+ }
+ 
+ static const struct iommu_flush_ops null_tlb_ops = {
+ 	.tlb_flush_all = msm_iommu_tlb_flush_all,
+ 	.tlb_flush_walk = msm_iommu_tlb_flush_walk,
+ 	.tlb_add_page = msm_iommu_tlb_add_page,
+ };
+ 
+ struct msm_mmu *msm_iommu_pagetable_create(struct msm_mmu *parent)
+ {
+ 	struct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(parent->dev);
+ 	struct msm_iommu *iommu = to_msm_iommu(parent);
+ 	struct msm_iommu_pagetable *pagetable;
+ 	const struct io_pgtable_cfg *ttbr1_cfg = NULL;
+ 	struct io_pgtable_cfg ttbr0_cfg;
+ 	int ret;
+ 
+ 	/* Get the pagetable configuration from the domain */
+ 	if (adreno_smmu->cookie)
+ 		ttbr1_cfg = adreno_smmu->get_ttbr1_cfg(adreno_smmu->cookie);
+ 	if (!ttbr1_cfg)
+ 		return ERR_PTR(-ENODEV);
+ 
+ 	pagetable = kzalloc(sizeof(*pagetable), GFP_KERNEL);
+ 	if (!pagetable)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	msm_mmu_init(&pagetable->base, parent->dev, &pagetable_funcs,
+ 		MSM_MMU_IOMMU_PAGETABLE);
+ 
+ 	/* Clone the TTBR1 cfg as starting point for TTBR0 cfg: */
+ 	ttbr0_cfg = *ttbr1_cfg;
+ 
+ 	/* The incoming cfg will have the TTBR1 quirk enabled */
+ 	ttbr0_cfg.quirks &= ~IO_PGTABLE_QUIRK_ARM_TTBR1;
+ 	ttbr0_cfg.tlb = &null_tlb_ops;
+ 
+ 	pagetable->pgtbl_ops = alloc_io_pgtable_ops(ARM_64_LPAE_S1,
+ 		&ttbr0_cfg, iommu->domain);
+ 
+ 	if (!pagetable->pgtbl_ops) {
+ 		kfree(pagetable);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+ 	/*
+ 	 * If this is the first pagetable that we've allocated, send it back to
+ 	 * the arm-smmu driver as a trigger to set up TTBR0
+ 	 */
+ 	if (atomic_inc_return(&iommu->pagetables) == 1) {
+ 		ret = adreno_smmu->set_ttbr0_cfg(adreno_smmu->cookie, &ttbr0_cfg);
+ 		if (ret) {
+ 			free_io_pgtable_ops(pagetable->pgtbl_ops);
+ 			kfree(pagetable);
+ 			return ERR_PTR(ret);
+ 		}
+ 	}
+ 
+ 	/* Needed later for TLB flush */
+ 	pagetable->parent = parent;
+ 	pagetable->ttbr = ttbr0_cfg.arm_lpae_s1_cfg.ttbr;
+ 
+ 	/*
+ 	 * TODO we would like each set of page tables to have a unique ASID
+ 	 * to optimize TLB invalidation.  But iommu_flush_iotlb_all() will
+ 	 * end up flushing the ASID used for TTBR1 pagetables, which is not
+ 	 * what we want.  So for now just use the same ASID as TTBR1.
+ 	 */
+ 	pagetable->asid = 0;
+ 
+ 	return &pagetable->base;
+ }
+ 
++>>>>>>> fefe8527a1e0 (iommu/io-pgtable: Remove tlb_flush_leaf)
  static int msm_fault_handler(struct iommu_domain *domain, struct device *dev,
  		unsigned long iova, int flags, void *arg)
  {
diff --cc drivers/iommu/mtk_iommu.c
index 38aceac865e2,8e56cec532e7..000000000000
--- a/drivers/iommu/mtk_iommu.c
+++ b/drivers/iommu/mtk_iommu.c
@@@ -219,8 -239,7 +219,12 @@@ static void mtk_iommu_tlb_flush_page_no
  
  static const struct iommu_flush_ops mtk_iommu_flush_ops = {
  	.tlb_flush_all = mtk_iommu_tlb_flush_all,
++<<<<<<< HEAD
 +	.tlb_flush_walk = mtk_iommu_tlb_flush_walk,
 +	.tlb_flush_leaf = mtk_iommu_tlb_flush_leaf,
++=======
+ 	.tlb_flush_walk = mtk_iommu_tlb_flush_range_sync,
++>>>>>>> fefe8527a1e0 (iommu/io-pgtable: Remove tlb_flush_leaf)
  	.tlb_add_page = mtk_iommu_tlb_flush_page_nosync,
  };
  
* Unmerged path drivers/gpu/drm/panfrost/panfrost_mmu.c
* Unmerged path drivers/gpu/drm/msm/msm_iommu.c
* Unmerged path drivers/gpu/drm/panfrost/panfrost_mmu.c
diff --git a/drivers/iommu/arm-smmu-v3.c b/drivers/iommu/arm-smmu-v3.c
index d64028588d13..af89d674e287 100644
--- a/drivers/iommu/arm-smmu-v3.c
+++ b/drivers/iommu/arm-smmu-v3.c
@@ -2376,16 +2376,9 @@ static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
 	arm_smmu_tlb_inv_range(iova, size, granule, false, cookie);
 }
 
-static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
-				  size_t granule, void *cookie)
-{
-	arm_smmu_tlb_inv_range(iova, size, granule, true, cookie);
-}
-
 static const struct iommu_flush_ops arm_smmu_flush_ops = {
 	.tlb_flush_all	= arm_smmu_tlb_inv_context,
 	.tlb_flush_walk = arm_smmu_tlb_inv_walk,
-	.tlb_flush_leaf = arm_smmu_tlb_inv_leaf,
 	.tlb_add_page	= arm_smmu_tlb_inv_page_nosync,
 };
 
diff --git a/drivers/iommu/arm-smmu.c b/drivers/iommu/arm-smmu.c
index 1a351b39e689..04a38096351c 100644
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@ -387,14 +387,6 @@ static void arm_smmu_tlb_inv_walk_s1(unsigned long iova, size_t size,
 	arm_smmu_tlb_sync_context(cookie);
 }
 
-static void arm_smmu_tlb_inv_leaf_s1(unsigned long iova, size_t size,
-				     size_t granule, void *cookie)
-{
-	arm_smmu_tlb_inv_range_s1(iova, size, granule, cookie,
-				  ARM_SMMU_CB_S1_TLBIVAL);
-	arm_smmu_tlb_sync_context(cookie);
-}
-
 static void arm_smmu_tlb_add_page_s1(struct iommu_iotlb_gather *gather,
 				     unsigned long iova, size_t granule,
 				     void *cookie)
@@ -411,14 +403,6 @@ static void arm_smmu_tlb_inv_walk_s2(unsigned long iova, size_t size,
 	arm_smmu_tlb_sync_context(cookie);
 }
 
-static void arm_smmu_tlb_inv_leaf_s2(unsigned long iova, size_t size,
-				     size_t granule, void *cookie)
-{
-	arm_smmu_tlb_inv_range_s2(iova, size, granule, cookie,
-				  ARM_SMMU_CB_S2_TLBIIPAS2L);
-	arm_smmu_tlb_sync_context(cookie);
-}
-
 static void arm_smmu_tlb_add_page_s2(struct iommu_iotlb_gather *gather,
 				     unsigned long iova, size_t granule,
 				     void *cookie)
@@ -427,8 +411,8 @@ static void arm_smmu_tlb_add_page_s2(struct iommu_iotlb_gather *gather,
 				  ARM_SMMU_CB_S2_TLBIIPAS2L);
 }
 
-static void arm_smmu_tlb_inv_any_s2_v1(unsigned long iova, size_t size,
-				       size_t granule, void *cookie)
+static void arm_smmu_tlb_inv_walk_s2_v1(unsigned long iova, size_t size,
+					size_t granule, void *cookie)
 {
 	arm_smmu_tlb_inv_context_s2(cookie);
 }
@@ -455,21 +439,18 @@ static void arm_smmu_tlb_add_page_s2_v1(struct iommu_iotlb_gather *gather,
 static const struct iommu_flush_ops arm_smmu_s1_tlb_ops = {
 	.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
 	.tlb_flush_walk	= arm_smmu_tlb_inv_walk_s1,
-	.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf_s1,
 	.tlb_add_page	= arm_smmu_tlb_add_page_s1,
 };
 
 static const struct iommu_flush_ops arm_smmu_s2_tlb_ops_v2 = {
 	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 	.tlb_flush_walk	= arm_smmu_tlb_inv_walk_s2,
-	.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf_s2,
 	.tlb_add_page	= arm_smmu_tlb_add_page_s2,
 };
 
 static const struct iommu_flush_ops arm_smmu_s2_tlb_ops_v1 = {
 	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
-	.tlb_flush_walk	= arm_smmu_tlb_inv_any_s2_v1,
-	.tlb_flush_leaf	= arm_smmu_tlb_inv_any_s2_v1,
+	.tlb_flush_walk	= arm_smmu_tlb_inv_walk_s2_v1,
 	.tlb_add_page	= arm_smmu_tlb_add_page_s2_v1,
 };
 
diff --git a/drivers/iommu/io-pgtable-arm-v7s.c b/drivers/iommu/io-pgtable-arm-v7s.c
index d9e1d25bdd4b..6d917539fbb9 100644
--- a/drivers/iommu/io-pgtable-arm-v7s.c
+++ b/drivers/iommu/io-pgtable-arm-v7s.c
@@ -572,7 +572,7 @@ static arm_v7s_iopte arm_v7s_split_cont(struct arm_v7s_io_pgtable *data,
 	__arm_v7s_pte_sync(ptep, ARM_V7S_CONT_PAGES, &iop->cfg);
 
 	size *= ARM_V7S_CONT_PAGES;
-	io_pgtable_tlb_flush_leaf(iop, iova, size, size);
+	io_pgtable_tlb_flush_walk(iop, iova, size, size);
 	return pte;
 }
 
@@ -851,7 +851,6 @@ static void __init dummy_tlb_add_page(struct iommu_iotlb_gather *gather,
 static const struct iommu_flush_ops dummy_tlb_ops __initconst = {
 	.tlb_flush_all	= dummy_tlb_flush_all,
 	.tlb_flush_walk	= dummy_tlb_flush,
-	.tlb_flush_leaf	= dummy_tlb_flush,
 	.tlb_add_page	= dummy_tlb_add_page,
 };
 
diff --git a/drivers/iommu/io-pgtable-arm.c b/drivers/iommu/io-pgtable-arm.c
index 7966fbc51add..2c620b5958a4 100644
--- a/drivers/iommu/io-pgtable-arm.c
+++ b/drivers/iommu/io-pgtable-arm.c
@@ -1127,7 +1127,6 @@ static void __init dummy_tlb_add_page(struct iommu_iotlb_gather *gather,
 static const struct iommu_flush_ops dummy_tlb_ops __initconst = {
 	.tlb_flush_all	= dummy_tlb_flush_all,
 	.tlb_flush_walk	= dummy_tlb_flush,
-	.tlb_flush_leaf	= dummy_tlb_flush,
 	.tlb_add_page	= dummy_tlb_add_page,
 };
 
diff --git a/drivers/iommu/ipmmu-vmsa.c b/drivers/iommu/ipmmu-vmsa.c
index 5a8da82fc7ca..dcd96542bc49 100644
--- a/drivers/iommu/ipmmu-vmsa.c
+++ b/drivers/iommu/ipmmu-vmsa.c
@@ -358,7 +358,6 @@ static void ipmmu_tlb_flush(unsigned long iova, size_t size,
 static const struct iommu_flush_ops ipmmu_flush_ops = {
 	.tlb_flush_all = ipmmu_tlb_flush_all,
 	.tlb_flush_walk = ipmmu_tlb_flush,
-	.tlb_flush_leaf = ipmmu_tlb_flush,
 };
 
 /* -----------------------------------------------------------------------------
diff --git a/drivers/iommu/msm_iommu.c b/drivers/iommu/msm_iommu.c
index d8227caba891..3be264a8c3ef 100644
--- a/drivers/iommu/msm_iommu.c
+++ b/drivers/iommu/msm_iommu.c
@@ -185,12 +185,6 @@ static void __flush_iotlb_walk(unsigned long iova, size_t size,
 	__flush_iotlb_range(iova, size, granule, false, cookie);
 }
 
-static void __flush_iotlb_leaf(unsigned long iova, size_t size,
-			       size_t granule, void *cookie)
-{
-	__flush_iotlb_range(iova, size, granule, true, cookie);
-}
-
 static void __flush_iotlb_page(struct iommu_iotlb_gather *gather,
 			       unsigned long iova, size_t granule, void *cookie)
 {
@@ -200,7 +194,6 @@ static void __flush_iotlb_page(struct iommu_iotlb_gather *gather,
 static const struct iommu_flush_ops msm_iommu_flush_ops = {
 	.tlb_flush_all = __flush_iotlb,
 	.tlb_flush_walk = __flush_iotlb_walk,
-	.tlb_flush_leaf = __flush_iotlb_leaf,
 	.tlb_add_page = __flush_iotlb_page,
 };
 
* Unmerged path drivers/iommu/mtk_iommu.c
diff --git a/drivers/iommu/qcom_iommu.c b/drivers/iommu/qcom_iommu.c
index 93b1bb1c380d..9a53035cb7e5 100644
--- a/drivers/iommu/qcom_iommu.c
+++ b/drivers/iommu/qcom_iommu.c
@@ -183,13 +183,6 @@ static void qcom_iommu_tlb_flush_walk(unsigned long iova, size_t size,
 	qcom_iommu_tlb_sync(cookie);
 }
 
-static void qcom_iommu_tlb_flush_leaf(unsigned long iova, size_t size,
-				      size_t granule, void *cookie)
-{
-	qcom_iommu_tlb_inv_range_nosync(iova, size, granule, true, cookie);
-	qcom_iommu_tlb_sync(cookie);
-}
-
 static void qcom_iommu_tlb_add_page(struct iommu_iotlb_gather *gather,
 				    unsigned long iova, size_t granule,
 				    void *cookie)
@@ -200,7 +193,6 @@ static void qcom_iommu_tlb_add_page(struct iommu_iotlb_gather *gather,
 static const struct iommu_flush_ops qcom_flush_ops = {
 	.tlb_flush_all	= qcom_iommu_tlb_inv_context,
 	.tlb_flush_walk = qcom_iommu_tlb_flush_walk,
-	.tlb_flush_leaf = qcom_iommu_tlb_flush_leaf,
 	.tlb_add_page	= qcom_iommu_tlb_add_page,
 };
 
diff --git a/include/linux/io-pgtable.h b/include/linux/io-pgtable.h
index a0d871fdf49a..7791ebf8346f 100644
--- a/include/linux/io-pgtable.h
+++ b/include/linux/io-pgtable.h
@@ -25,8 +25,6 @@ enum io_pgtable_fmt {
  * @tlb_flush_walk: Synchronously invalidate all intermediate TLB state
  *                  (sometimes referred to as the "walk cache") for a virtual
  *                  address range.
- * @tlb_flush_leaf: Synchronously invalidate all leaf TLB state for a virtual
- *                  address range.
  * @tlb_add_page:   Optional callback to queue up leaf TLB invalidation for a
  *                  single page.  IOMMUs that cannot batch TLB invalidation
  *                  operations efficiently will typically issue them here, but
@@ -40,8 +38,6 @@ struct iommu_flush_ops {
 	void (*tlb_flush_all)(void *cookie);
 	void (*tlb_flush_walk)(unsigned long iova, size_t size, size_t granule,
 			       void *cookie);
-	void (*tlb_flush_leaf)(unsigned long iova, size_t size, size_t granule,
-			       void *cookie);
 	void (*tlb_add_page)(struct iommu_iotlb_gather *gather,
 			     unsigned long iova, size_t granule, void *cookie);
 };
@@ -225,13 +221,6 @@ io_pgtable_tlb_flush_walk(struct io_pgtable *iop, unsigned long iova,
 	iop->cfg.tlb->tlb_flush_walk(iova, size, granule, iop->cookie);
 }
 
-static inline void
-io_pgtable_tlb_flush_leaf(struct io_pgtable *iop, unsigned long iova,
-			  size_t size, size_t granule)
-{
-	iop->cfg.tlb->tlb_flush_leaf(iova, size, granule, iop->cookie);
-}
-
 static inline void
 io_pgtable_tlb_add_page(struct io_pgtable *iop,
 			struct iommu_iotlb_gather * gather, unsigned long iova,
