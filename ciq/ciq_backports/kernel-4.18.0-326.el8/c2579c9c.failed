mm/hmm: add missing call to hmm_range_need_fault() before returning EFAULT

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit c2579c9c4add3110b2ce81198f8a6bbb5055cfda
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/c2579c9c.failed

All return paths that do EFAULT must call hmm_range_need_fault() to
determine if the user requires this page to be valid.

If the page cannot be made valid if the user later requires it, due to vma
flags in this case, then the return should be HMM_PFN_ERROR.

Fixes: a3e0d41c2b1f ("mm/hmm: improve driver API to work and wait over a range")
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit c2579c9c4add3110b2ce81198f8a6bbb5055cfda)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index 5bd2314869ec,a16b8e360e4c..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -872,71 -591,47 +872,103 @@@ unlock
  		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
  
  	return ret;
 +#else /* CONFIG_HUGETLB_PAGE */
 +	return -EINVAL;
 +#endif
  }
 -#else
 -#define hmm_vma_walk_hugetlb_entry NULL
 -#endif /* CONFIG_HUGETLB_PAGE */
  
 -static int hmm_vma_walk_test(unsigned long start, unsigned long end,
 -			     struct mm_walk *walk)
 +static void hmm_pfns_clear(struct hmm_range *range,
 +			   uint64_t *pfns,
 +			   unsigned long addr,
 +			   unsigned long end)
  {
 -	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 -	struct hmm_range *range = hmm_vma_walk->range;
 -	struct vm_area_struct *vma = walk->vma;
 +	for (; addr < end; addr += PAGE_SIZE, pfns++)
 +		*pfns = range->values[HMM_PFN_NONE];
 +}
  
++<<<<<<< HEAD
 +/*
 + * hmm_range_register() - start tracking change to CPU page table over a range
 + * @range: range
 + * @mm: the mm struct for the range of virtual address
 + * @start: start virtual address (inclusive)
 + * @end: end virtual address (exclusive)
 + * @page_shift: expect page shift for the range
 + * Return: 0 on success, -EFAULT if the address space is no longer valid
 + *
 + * Track updates to the CPU page table see include/linux/hmm.h
 + */
 +int hmm_range_register(struct hmm_range *range,
 +		       struct hmm_mirror *mirror,
 +		       unsigned long start,
 +		       unsigned long end,
 +		       unsigned page_shift)
 +{
 +	unsigned long mask = ((1UL << page_shift) - 1UL);
 +	struct hmm *hmm = mirror->hmm;
 +	unsigned long flags;
 +
 +	range->valid = false;
 +	range->hmm = NULL;
 +
 +	if ((start & mask) || (end & mask))
 +		return -EINVAL;
 +	if (start >= end)
 +		return -EINVAL;
 +
 +	range->page_shift = page_shift;
 +	range->start = start;
 +	range->end = end;
 +
 +	/* Prevent hmm_release() from running while the range is valid */
 +	if (!mmget_not_zero(hmm->mm))
 +		return -EFAULT;
 +
 +	/* Initialize range to track CPU page table updates. */
 +	spin_lock_irqsave(&hmm->ranges_lock, flags);
 +
 +	range->hmm = hmm;
 +	kref_get(&hmm->kref);
 +	list_add(&range->list, &hmm->ranges);
 +
 +	/*
 +	 * If there are any concurrent notifiers we have to wait for them for
 +	 * the range to be valid (see hmm_range_wait_until_valid()).
 +	 */
 +	if (!hmm->notifiers)
 +		range->valid = true;
 +	spin_unlock_irqrestore(&hmm->ranges_lock, flags);
++=======
+ 	/*
+ 	 * Skip vma ranges that don't have struct page backing them or map I/O
+ 	 * devices directly.
+ 	 *
+ 	 * If the vma does not allow read access, then assume that it does not
+ 	 * allow write access either. HMM does not support architectures that
+ 	 * allow write without read.
+ 	 */
+ 	if ((vma->vm_flags & (VM_IO | VM_PFNMAP | VM_MIXEDMAP)) ||
+ 	    !(vma->vm_flags & VM_READ)) {
+ 		bool fault, write_fault;
+ 
+ 		/*
+ 		 * Check to see if a fault is requested for any page in the
+ 		 * range.
+ 		 */
+ 		hmm_range_need_fault(hmm_vma_walk, range->pfns +
+ 					((start - range->start) >> PAGE_SHIFT),
+ 					(end - start) >> PAGE_SHIFT,
+ 					0, &fault, &write_fault);
+ 		if (fault || write_fault)
+ 			return -EFAULT;
+ 
+ 		hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
+ 		hmm_vma_walk->last = end;
+ 
+ 		/* Skip this vma and continue processing the next vma. */
+ 		return 1;
+ 	}
++>>>>>>> c2579c9c4add (mm/hmm: add missing call to hmm_range_need_fault() before returning EFAULT)
  
  	return 0;
  }
* Unmerged path mm/hmm.c
