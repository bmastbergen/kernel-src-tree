net/mlx5e: TC, Parse sample action

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Chris Mi <cmi@nvidia.com>
commit 41c2fd949803a5ff8acfed1d81c0bbd62d8f660d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/41c2fd94.failed

Parse TC sample action and save sample parameters in flow attribute
data structure.

	Signed-off-by: Chris Mi <cmi@nvidia.com>
	Reviewed-by: Oz Shlomo <ozsh@nvidia.com>
	Reviewed-by: Mark Bloch <mbloch@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 41c2fd949803a5ff8acfed1d81c0bbd62d8f660d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en/tc_priv.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index 8d4d2686ea65,85782d12ffb2..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -1386,385 -1449,55 +1387,386 @@@ static void mlx5e_tc_del_fdb_flow(struc
  	if (mlx5_flow_has_geneve_opt(flow))
  		mlx5_geneve_tlv_option_del(priv->mdev->geneve);
  
 -	mlx5_eswitch_del_vlan_action(esw, attr);
 +	mlx5_eswitch_del_vlan_action(esw, attr);
 +
 +	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++)
 +		if (attr->esw_attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP) {
 +			mlx5e_detach_encap(priv, flow, out_index);
 +			kfree(attr->parse_attr->tun_info[out_index]);
 +		}
 +	kvfree(attr->parse_attr);
 +
 +	mlx5_tc_ct_match_del(priv, &flow->attr->ct_attr);
 +
 +	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
 +		mlx5e_detach_mod_hdr(priv, flow);
 +
 +	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_COUNT)
 +		mlx5_fc_destroy(attr->esw_attr->counter_dev, attr->counter);
 +
 +	if (flow_flag_test(flow, L3_TO_L2_DECAP))
 +		mlx5e_detach_decap(priv, flow);
 +
++	kfree(flow->attr->esw_attr->sample);
 +	kfree(flow->attr);
 +}
 +
 +void mlx5e_tc_encap_flows_add(struct mlx5e_priv *priv,
 +			      struct mlx5e_encap_entry *e,
 +			      struct list_head *flow_list)
 +{
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct mlx5_esw_flow_attr *esw_attr;
 +	struct mlx5_flow_handle *rule;
 +	struct mlx5_flow_attr *attr;
 +	struct mlx5_flow_spec *spec;
 +	struct mlx5e_tc_flow *flow;
 +	int err;
 +
 +	e->pkt_reformat = mlx5_packet_reformat_alloc(priv->mdev,
 +						     e->reformat_type,
 +						     e->encap_size, e->encap_header,
 +						     MLX5_FLOW_NAMESPACE_FDB);
 +	if (IS_ERR(e->pkt_reformat)) {
 +		mlx5_core_warn(priv->mdev, "Failed to offload cached encapsulation header, %lu\n",
 +			       PTR_ERR(e->pkt_reformat));
 +		return;
 +	}
 +	e->flags |= MLX5_ENCAP_ENTRY_VALID;
 +	mlx5e_rep_queue_neigh_stats_work(priv);
 +
 +	list_for_each_entry(flow, flow_list, tmp_list) {
 +		bool all_flow_encaps_valid = true;
 +		int i;
 +
 +		if (!mlx5e_is_offloaded_flow(flow))
 +			continue;
 +		attr = flow->attr;
 +		esw_attr = attr->esw_attr;
 +		spec = &attr->parse_attr->spec;
 +
 +		esw_attr->dests[flow->tmp_efi_index].pkt_reformat = e->pkt_reformat;
 +		esw_attr->dests[flow->tmp_efi_index].flags |= MLX5_ESW_DEST_ENCAP_VALID;
 +		/* Flow can be associated with multiple encap entries.
 +		 * Before offloading the flow verify that all of them have
 +		 * a valid neighbour.
 +		 */
 +		for (i = 0; i < MLX5_MAX_FLOW_FWD_VPORTS; i++) {
 +			if (!(esw_attr->dests[i].flags & MLX5_ESW_DEST_ENCAP))
 +				continue;
 +			if (!(esw_attr->dests[i].flags & MLX5_ESW_DEST_ENCAP_VALID)) {
 +				all_flow_encaps_valid = false;
 +				break;
 +			}
 +		}
 +		/* Do not offload flows with unresolved neighbors */
 +		if (!all_flow_encaps_valid)
 +			continue;
 +		/* update from slow path rule to encap rule */
 +		rule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, attr);
 +		if (IS_ERR(rule)) {
 +			err = PTR_ERR(rule);
 +			mlx5_core_warn(priv->mdev, "Failed to update cached encapsulation flow, %d\n",
 +				       err);
 +			continue;
 +		}
 +
 +		mlx5e_tc_unoffload_from_slow_path(esw, flow);
 +		flow->rule[0] = rule;
 +		/* was unset when slow path rule removed */
 +		flow_flag_set(flow, OFFLOADED);
 +	}
 +}
 +
 +void mlx5e_tc_encap_flows_del(struct mlx5e_priv *priv,
 +			      struct mlx5e_encap_entry *e,
 +			      struct list_head *flow_list)
 +{
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct mlx5_esw_flow_attr *esw_attr;
 +	struct mlx5_flow_handle *rule;
 +	struct mlx5_flow_attr *attr;
 +	struct mlx5_flow_spec *spec;
 +	struct mlx5e_tc_flow *flow;
 +	int err;
 +
 +	list_for_each_entry(flow, flow_list, tmp_list) {
 +		if (!mlx5e_is_offloaded_flow(flow))
 +			continue;
 +		attr = flow->attr;
 +		esw_attr = attr->esw_attr;
 +		spec = &attr->parse_attr->spec;
 +
 +		/* update from encap rule to slow path rule */
 +		rule = mlx5e_tc_offload_to_slow_path(esw, flow, spec);
 +		/* mark the flow's encap dest as non-valid */
 +		esw_attr->dests[flow->tmp_efi_index].flags &= ~MLX5_ESW_DEST_ENCAP_VALID;
 +
 +		if (IS_ERR(rule)) {
 +			err = PTR_ERR(rule);
 +			mlx5_core_warn(priv->mdev, "Failed to update slow path (encap) flow, %d\n",
 +				       err);
 +			continue;
 +		}
 +
 +		mlx5e_tc_unoffload_fdb_rules(esw, flow, attr);
 +		flow->rule[0] = rule;
 +		/* was unset when fast path rule removed */
 +		flow_flag_set(flow, OFFLOADED);
 +	}
 +
 +	/* we know that the encap is valid */
 +	e->flags &= ~MLX5_ENCAP_ENTRY_VALID;
 +	mlx5_packet_reformat_dealloc(priv->mdev, e->pkt_reformat);
 +}
 +
 +static struct mlx5_fc *mlx5e_tc_get_counter(struct mlx5e_tc_flow *flow)
 +{
 +	return flow->attr->counter;
 +}
 +
 +/* Takes reference to all flows attached to encap and adds the flows to
 + * flow_list using 'tmp_list' list_head in mlx5e_tc_flow.
 + */
 +void mlx5e_take_all_encap_flows(struct mlx5e_encap_entry *e, struct list_head *flow_list)
 +{
 +	struct encap_flow_item *efi;
 +	struct mlx5e_tc_flow *flow;
 +
 +	list_for_each_entry(efi, &e->flows, list) {
 +		flow = container_of(efi, struct mlx5e_tc_flow, encaps[efi->index]);
 +		if (IS_ERR(mlx5e_flow_get(flow)))
 +			continue;
 +		wait_for_completion(&flow->init_done);
 +
 +		flow->tmp_efi_index = efi->index;
 +		list_add(&flow->tmp_list, flow_list);
 +	}
 +}
 +
 +/* Iterate over tmp_list of flows attached to flow_list head. */
 +void mlx5e_put_encap_flow_list(struct mlx5e_priv *priv, struct list_head *flow_list)
 +{
 +	struct mlx5e_tc_flow *flow, *tmp;
 +
 +	list_for_each_entry_safe(flow, tmp, flow_list, tmp_list)
 +		mlx5e_flow_put(priv, flow);
 +}
 +
 +static struct mlx5e_encap_entry *
 +mlx5e_get_next_valid_encap(struct mlx5e_neigh_hash_entry *nhe,
 +			   struct mlx5e_encap_entry *e)
 +{
 +	struct mlx5e_encap_entry *next = NULL;
 +
 +retry:
 +	rcu_read_lock();
 +
 +	/* find encap with non-zero reference counter value */
 +	for (next = e ?
 +		     list_next_or_null_rcu(&nhe->encap_list,
 +					   &e->encap_list,
 +					   struct mlx5e_encap_entry,
 +					   encap_list) :
 +		     list_first_or_null_rcu(&nhe->encap_list,
 +					    struct mlx5e_encap_entry,
 +					    encap_list);
 +	     next;
 +	     next = list_next_or_null_rcu(&nhe->encap_list,
 +					  &next->encap_list,
 +					  struct mlx5e_encap_entry,
 +					  encap_list))
 +		if (mlx5e_encap_take(next))
 +			break;
 +
 +	rcu_read_unlock();
 +
 +	/* release starting encap */
 +	if (e)
 +		mlx5e_encap_put(netdev_priv(e->out_dev), e);
 +	if (!next)
 +		return next;
 +
 +	/* wait for encap to be fully initialized */
 +	wait_for_completion(&next->res_ready);
 +	/* continue searching if encap entry is not in valid state after completion */
 +	if (!(next->flags & MLX5_ENCAP_ENTRY_VALID)) {
 +		e = next;
 +		goto retry;
 +	}
 +
 +	return next;
 +}
 +
 +void mlx5e_tc_update_neigh_used_value(struct mlx5e_neigh_hash_entry *nhe)
 +{
 +	struct mlx5e_neigh *m_neigh = &nhe->m_neigh;
 +	struct mlx5e_encap_entry *e = NULL;
 +	struct mlx5e_tc_flow *flow;
 +	struct mlx5_fc *counter;
 +	struct neigh_table *tbl;
 +	bool neigh_used = false;
 +	struct neighbour *n;
 +	u64 lastuse;
 +
 +	if (m_neigh->family == AF_INET)
 +		tbl = &arp_tbl;
 +#if IS_ENABLED(CONFIG_IPV6)
 +	else if (m_neigh->family == AF_INET6)
 +		tbl = ipv6_stub->nd_tbl;
 +#endif
 +	else
 +		return;
 +
 +	/* mlx5e_get_next_valid_encap() releases previous encap before returning
 +	 * next one.
 +	 */
 +	while ((e = mlx5e_get_next_valid_encap(nhe, e)) != NULL) {
 +		struct mlx5e_priv *priv = netdev_priv(e->out_dev);
 +		struct encap_flow_item *efi, *tmp;
 +		struct mlx5_eswitch *esw;
 +		LIST_HEAD(flow_list);
 +
 +		esw = priv->mdev->priv.eswitch;
 +		mutex_lock(&esw->offloads.encap_tbl_lock);
 +		list_for_each_entry_safe(efi, tmp, &e->flows, list) {
 +			flow = container_of(efi, struct mlx5e_tc_flow,
 +					    encaps[efi->index]);
 +			if (IS_ERR(mlx5e_flow_get(flow)))
 +				continue;
 +			list_add(&flow->tmp_list, &flow_list);
 +
 +			if (mlx5e_is_offloaded_flow(flow)) {
 +				counter = mlx5e_tc_get_counter(flow);
 +				lastuse = mlx5_fc_query_lastuse(counter);
 +				if (time_after((unsigned long)lastuse, nhe->reported_lastuse)) {
 +					neigh_used = true;
 +					break;
 +				}
 +			}
 +		}
 +		mutex_unlock(&esw->offloads.encap_tbl_lock);
 +
 +		mlx5e_put_encap_flow_list(priv, &flow_list);
 +		if (neigh_used) {
 +			/* release current encap before breaking the loop */
 +			mlx5e_encap_put(priv, e);
 +			break;
 +		}
 +	}
 +
 +	trace_mlx5e_tc_update_neigh_used_value(nhe, neigh_used);
 +
 +	if (neigh_used) {
 +		nhe->reported_lastuse = jiffies;
 +
 +		/* find the relevant neigh according to the cached device and
 +		 * dst ip pair
 +		 */
 +		n = neigh_lookup(tbl, &m_neigh->dst_ip, m_neigh->dev);
 +		if (!n)
 +			return;
 +
 +		neigh_event_send(n, NULL);
 +		neigh_release(n);
 +	}
 +}
 +
 +static void mlx5e_encap_dealloc(struct mlx5e_priv *priv, struct mlx5e_encap_entry *e)
 +{
 +	WARN_ON(!list_empty(&e->flows));
 +
 +	if (e->compl_result > 0) {
 +		mlx5e_rep_encap_entry_detach(netdev_priv(e->out_dev), e);
 +
 +		if (e->flags & MLX5_ENCAP_ENTRY_VALID)
 +			mlx5_packet_reformat_dealloc(priv->mdev, e->pkt_reformat);
 +	}
 +
 +	kfree(e->tun_info);
 +	kfree(e->encap_header);
 +	kfree_rcu(e, rcu);
 +}
 +
 +static void mlx5e_decap_dealloc(struct mlx5e_priv *priv,
 +				struct mlx5e_decap_entry *d)
 +{
 +	WARN_ON(!list_empty(&d->flows));
 +
 +	if (!d->compl_result)
 +		mlx5_packet_reformat_dealloc(priv->mdev, d->pkt_reformat);
  
 -	if (flow->decap_route)
 -		mlx5e_detach_decap_route(priv, flow);
 +	kfree_rcu(d, rcu);
 +}
  
 -	for (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++) {
 -		if (esw_attr->dests[out_index].flags &
 -		    MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)
 -			vf_tun = true;
 -		if (esw_attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP) {
 -			mlx5e_detach_encap(priv, flow, out_index);
 -			kfree(attr->parse_attr->tun_info[out_index]);
 -		}
 -	}
 +void mlx5e_encap_put(struct mlx5e_priv *priv, struct mlx5e_encap_entry *e)
 +{
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  
 -	mlx5_tc_ct_match_del(get_ct_priv(priv), &flow->attr->ct_attr);
 +	if (!refcount_dec_and_mutex_lock(&e->refcnt, &esw->offloads.encap_tbl_lock))
 +		return;
 +	hash_del_rcu(&e->encap_hlist);
 +	mutex_unlock(&esw->offloads.encap_tbl_lock);
  
 -	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR) {
 -		dealloc_mod_hdr_actions(&attr->parse_attr->mod_hdr_acts);
 -		if (vf_tun && attr->modify_hdr)
 -			mlx5_modify_header_dealloc(priv->mdev, attr->modify_hdr);
 -		else
 -			mlx5e_detach_mod_hdr(priv, flow);
 -	}
 -	kvfree(attr->parse_attr);
 -	kvfree(attr->esw_attr->rx_tun_attr);
 +	mlx5e_encap_dealloc(priv, e);
 +}
  
 -	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_COUNT)
 -		mlx5_fc_destroy(esw_attr->counter_dev, attr->counter);
 +static void mlx5e_decap_put(struct mlx5e_priv *priv, struct mlx5e_decap_entry *d)
 +{
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
  
 -	if (flow_flag_test(flow, L3_TO_L2_DECAP))
 -		mlx5e_detach_decap(priv, flow);
 +	if (!refcount_dec_and_mutex_lock(&d->refcnt, &esw->offloads.decap_tbl_lock))
 +		return;
 +	hash_del_rcu(&d->hlist);
 +	mutex_unlock(&esw->offloads.decap_tbl_lock);
  
 -	kfree(flow->attr->esw_attr->sample);
 -	kfree(flow->attr);
 +	mlx5e_decap_dealloc(priv, d);
  }
  
 -struct mlx5_fc *mlx5e_tc_get_counter(struct mlx5e_tc_flow *flow)
 +static void mlx5e_detach_encap(struct mlx5e_priv *priv,
 +			       struct mlx5e_tc_flow *flow, int out_index)
  {
 -	return flow->attr->counter;
 +	struct mlx5e_encap_entry *e = flow->encaps[out_index].e;
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +
 +	/* flow wasn't fully initialized */
 +	if (!e)
 +		return;
 +
 +	mutex_lock(&esw->offloads.encap_tbl_lock);
 +	list_del(&flow->encaps[out_index].list);
 +	flow->encaps[out_index].e = NULL;
 +	if (!refcount_dec_and_test(&e->refcnt)) {
 +		mutex_unlock(&esw->offloads.encap_tbl_lock);
 +		return;
 +	}
 +	hash_del_rcu(&e->encap_hlist);
 +	mutex_unlock(&esw->offloads.encap_tbl_lock);
 +
 +	mlx5e_encap_dealloc(priv, e);
  }
  
 -/* Iterate over tmp_list of flows attached to flow_list head. */
 -void mlx5e_put_flow_list(struct mlx5e_priv *priv, struct list_head *flow_list)
 +static void mlx5e_detach_decap(struct mlx5e_priv *priv,
 +			       struct mlx5e_tc_flow *flow)
  {
 -	struct mlx5e_tc_flow *flow, *tmp;
 +	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
 +	struct mlx5e_decap_entry *d = flow->decap_reformat;
  
 -	list_for_each_entry_safe(flow, tmp, flow_list, tmp_list)
 -		mlx5e_flow_put(priv, flow);
 +	if (!d)
 +		return;
 +
 +	mutex_lock(&esw->offloads.decap_tbl_lock);
 +	list_del(&flow->l3_to_l2_reformat);
 +	flow->decap_reformat = NULL;
 +
 +	if (!refcount_dec_and_test(&d->refcnt)) {
 +		mutex_unlock(&esw->offloads.decap_tbl_lock);
 +		return;
 +	}
 +	hash_del_rcu(&d->hlist);
 +	mutex_unlock(&esw->offloads.decap_tbl_lock);
 +
 +	mlx5e_decap_dealloc(priv, d);
  }
  
  static void __mlx5e_tc_del_fdb_peer_flow(struct mlx5e_tc_flow *flow)
@@@ -4351,12 -3884,28 +4354,31 @@@ static int parse_tc_fdb_actions(struct 
  			attr->dest_chain = act->chain_index;
  			break;
  		case FLOW_ACTION_CT:
++<<<<<<< HEAD
 +			err = mlx5_tc_ct_parse_action(priv, attr, act, extack);
++=======
+ 			if (flow_flag_test(flow, SAMPLE)) {
+ 				NL_SET_ERR_MSG_MOD(extack, "Sample action with connection tracking is not supported");
+ 				return -EOPNOTSUPP;
+ 			}
+ 			err = mlx5_tc_ct_parse_action(get_ct_priv(priv), attr, act, extack);
++>>>>>>> 41c2fd949803 (net/mlx5e: TC, Parse sample action)
  			if (err)
  				return err;
  
  			flow_flag_set(flow, CT);
 -			esw_attr->split_count = esw_attr->out_count;
  			break;
+ 		case FLOW_ACTION_SAMPLE:
+ 			if (flow_flag_test(flow, CT)) {
+ 				NL_SET_ERR_MSG_MOD(extack, "Sample action with connection tracking is not supported");
+ 				return -EOPNOTSUPP;
+ 			}
+ 			sample.rate = act->sample.rate;
+ 			sample.group_num = act->sample.psample_group->group_num;
+ 			if (act->sample.truncate)
+ 				sample.trunc_size = act->sample.trunc_size;
+ 			flow_flag_set(flow, SAMPLE);
+ 			break;
  		default:
  			NL_SET_ERR_MSG_MOD(extack, "The offload action is not supported");
  			return -EOPNOTSUPP;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/tc_priv.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/tc_priv.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/esw/sample.h b/drivers/net/ethernet/mellanox/mlx5/core/esw/sample.h
new file mode 100644
index 000000000000..35a5e6dddcd0
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/esw/sample.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB */
+/* Copyright (c) 2021 Mellanox Technologies. */
+
+#ifndef __MLX5_EN_TC_SAMPLE_H__
+#define __MLX5_EN_TC_SAMPLE_H__
+
+struct mlx5_sample_attr {
+	u32 group_num;
+	u32 rate;
+	u32 trunc_size;
+};
+
+#endif /* __MLX5_EN_TC_SAMPLE_H__ */
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
index 451410599bb1..4f9c738a50e0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h
@@ -45,6 +45,7 @@
 #include "lib/fs_chains.h"
 #include "sf/sf.h"
 #include "en/tc_ct.h"
+#include "esw/sample.h"
 
 #ifdef CONFIG_MLX5_ESWITCH
 
@@ -419,6 +420,7 @@ struct mlx5_esw_flow_attr {
 	} dests[MLX5_MAX_FLOW_FWD_VPORTS];
 	struct mlx5_rx_tun_attr *rx_tun_attr;
 	struct mlx5_pkt_reformat *decap_pkt_reformat;
+	struct mlx5_sample_attr *sample;
 };
 
 int mlx5_devlink_eswitch_mode_set(struct devlink *devlink, u16 mode,
