x86/kvm/svm: Move guest enter/exit into .noinstr.text

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 135961e0a7d555fc8f1d7c89ad44a94dffa5dcd8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/135961e0.failed

Move the functions which are inside the RCU off region into the
non-instrumentable text section.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
	Acked-by: Peter Zijlstra <peterz@infradead.org>
	Acked-by: Paolo Bonzini <pbonzini@redhat.com>

Message-Id: <20200708195322.144607767@linutronix.de>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 135961e0a7d555fc8f1d7c89ad44a94dffa5dcd8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/svm.c
diff --cc arch/x86/kvm/svm/svm.c
index c96b24a58317,71500e865c94..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -3702,12 -3341,66 +3702,72 @@@ static fastpath_t svm_exit_handlers_fas
  	return EXIT_FASTPATH_NONE;
  }
  
++<<<<<<< HEAD
 +static fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
++=======
+ void __svm_vcpu_run(unsigned long vmcb_pa, unsigned long *regs);
+ 
+ static noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu,
+ 					struct vcpu_svm *svm)
+ {
+ 	/*
+ 	 * VMENTER enables interrupts (host state), but the kernel state is
+ 	 * interrupts disabled when this is invoked. Also tell RCU about
+ 	 * it. This is the same logic as for exit_to_user_mode().
+ 	 *
+ 	 * This ensures that e.g. latency analysis on the host observes
+ 	 * guest mode as interrupt enabled.
+ 	 *
+ 	 * guest_enter_irqoff() informs context tracking about the
+ 	 * transition to guest mode and if enabled adjusts RCU state
+ 	 * accordingly.
+ 	 */
+ 	instrumentation_begin();
+ 	trace_hardirqs_on_prepare();
+ 	lockdep_hardirqs_on_prepare(CALLER_ADDR0);
+ 	instrumentation_end();
+ 
+ 	guest_enter_irqoff();
+ 	lockdep_hardirqs_on(CALLER_ADDR0);
+ 
+ 	__svm_vcpu_run(svm->vmcb_pa, (unsigned long *)&svm->vcpu.arch.regs);
+ 
+ #ifdef CONFIG_X86_64
+ 	wrmsrl(MSR_GS_BASE, svm->host.gs_base);
+ #else
+ 	loadsegment(fs, svm->host.fs);
+ #ifndef CONFIG_X86_32_LAZY_GS
+ 	loadsegment(gs, svm->host.gs);
+ #endif
+ #endif
+ 
+ 	/*
+ 	 * VMEXIT disables interrupts (host state), but tracing and lockdep
+ 	 * have them in state 'on' as recorded before entering guest mode.
+ 	 * Same as enter_from_user_mode().
+ 	 *
+ 	 * guest_exit_irqoff() restores host context and reinstates RCU if
+ 	 * enabled and required.
+ 	 *
+ 	 * This needs to be done before the below as native_read_msr()
+ 	 * contains a tracepoint and x86_spec_ctrl_restore_host() calls
+ 	 * into world and some more.
+ 	 */
+ 	lockdep_hardirqs_off(CALLER_ADDR0);
+ 	guest_exit_irqoff();
+ 
+ 	instrumentation_begin();
+ 	trace_hardirqs_off_finish();
+ 	instrumentation_end();
+ }
+ 
+ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
++>>>>>>> 135961e0a7d5 (x86/kvm/svm: Move guest enter/exit into .noinstr.text)
  {
 -	fastpath_t exit_fastpath;
  	struct vcpu_svm *svm = to_svm(vcpu);
 +	unsigned long vmcb_pa = svm->current_vmcb->pa;
 +
 +	trace_kvm_entry(vcpu);
  
  	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
  	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
@@@ -3759,26 -3450,9 +3819,30 @@@
  	 * is no need to worry about the conditional branch over the wrmsr
  	 * being speculatively taken.
  	 */
 -	x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
 +	if (!static_cpu_has(X86_FEATURE_V_SPEC_CTRL))
 +		x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
 +
++<<<<<<< HEAD
 +	if (sev_es_guest(vcpu->kvm)) {
 +		__svm_sev_es_vcpu_run(vmcb_pa);
 +	} else {
 +		struct svm_cpu_data *sd = per_cpu(svm_data, vcpu->cpu);
 +
 +		/*
 +		 * Use a single vmcb (vmcb01 because it's always valid) for
 +		 * context switching guest state via VMLOAD/VMSAVE, that way
 +		 * the state doesn't need to be copied between vmcb01 and
 +		 * vmcb02 when switching vmcbs for nested virtualization.
 +		 */
 +		vmload(svm->vmcb01.pa);
 +		__svm_vcpu_run(vmcb_pa, (unsigned long *)&vcpu->arch.regs);
 +		vmsave(svm->vmcb01.pa);
  
 +		vmload(__sme_page_pa(sd->save_area));
 +	}
++=======
+ 	svm_vcpu_enter_exit(vcpu, svm);
++>>>>>>> 135961e0a7d5 (x86/kvm/svm: Move guest enter/exit into .noinstr.text)
  
  	/*
  	 * We do not use IBRS in the kernel. If this vCPU has used the
* Unmerged path arch/x86/kvm/svm/svm.c
diff --git a/arch/x86/kvm/svm/vmenter.S b/arch/x86/kvm/svm/vmenter.S
index 38a3a87fc5b6..4fa17df123cd 100644
--- a/arch/x86/kvm/svm/vmenter.S
+++ b/arch/x86/kvm/svm/vmenter.S
@@ -27,7 +27,7 @@
 #define VCPU_R15	__VCPU_REGS_R15 * WORD_SIZE
 #endif
 
-	.text
+.section .noinstr.text, "ax"
 
 /**
  * __svm_vcpu_run - Run a vCPU via a transition to SVM guest mode
