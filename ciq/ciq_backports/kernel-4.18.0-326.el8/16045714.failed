KVM: x86: Defer vtime accounting 'til after IRQ handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Wanpeng Li <wanpengli@tencent.com>
commit 160457140187c5fb127b844e5a85f87f00a01b14
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/16045714.failed

Defer the call to account guest time until after servicing any IRQ(s)
that happened in the guest or immediately after VM-Exit.  Tick-based
accounting of vCPU time relies on PF_VCPU being set when the tick IRQ
handler runs, and IRQs are blocked throughout the main sequence of
vcpu_enter_guest(), including the call into vendor code to actually
enter and exit the guest.

This fixes a bug where reported guest time remains '0', even when
running an infinite loop in the guest:

  https://bugzilla.kernel.org/show_bug.cgi?id=209831

Fixes: 87fa7f3e98a131 ("x86/kvm: Move context tracking where it belongs")
	Suggested-by: Thomas Gleixner <tglx@linutronix.de>
Co-developed-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/r/20210505002735.1684165-4-seanjc@google.com
(cherry picked from commit 160457140187c5fb127b844e5a85f87f00a01b14)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/svm/svm.c
index c96b24a58317,c400def6220b..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -3707,6 -3710,68 +3707,71 @@@ static fastpath_t svm_vcpu_run(struct k
  	struct vcpu_svm *svm = to_svm(vcpu);
  	unsigned long vmcb_pa = svm->current_vmcb->pa;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * VMENTER enables interrupts (host state), but the kernel state is
+ 	 * interrupts disabled when this is invoked. Also tell RCU about
+ 	 * it. This is the same logic as for exit_to_user_mode().
+ 	 *
+ 	 * This ensures that e.g. latency analysis on the host observes
+ 	 * guest mode as interrupt enabled.
+ 	 *
+ 	 * guest_enter_irqoff() informs context tracking about the
+ 	 * transition to guest mode and if enabled adjusts RCU state
+ 	 * accordingly.
+ 	 */
+ 	instrumentation_begin();
+ 	trace_hardirqs_on_prepare();
+ 	lockdep_hardirqs_on_prepare(CALLER_ADDR0);
+ 	instrumentation_end();
+ 
+ 	guest_enter_irqoff();
+ 	lockdep_hardirqs_on(CALLER_ADDR0);
+ 
+ 	if (sev_es_guest(vcpu->kvm)) {
+ 		__svm_sev_es_vcpu_run(vmcb_pa);
+ 	} else {
+ 		struct svm_cpu_data *sd = per_cpu(svm_data, vcpu->cpu);
+ 
+ 		/*
+ 		 * Use a single vmcb (vmcb01 because it's always valid) for
+ 		 * context switching guest state via VMLOAD/VMSAVE, that way
+ 		 * the state doesn't need to be copied between vmcb01 and
+ 		 * vmcb02 when switching vmcbs for nested virtualization.
+ 		 */
+ 		vmload(svm->vmcb01.pa);
+ 		__svm_vcpu_run(vmcb_pa, (unsigned long *)&vcpu->arch.regs);
+ 		vmsave(svm->vmcb01.pa);
+ 
+ 		vmload(__sme_page_pa(sd->save_area));
+ 	}
+ 
+ 	/*
+ 	 * VMEXIT disables interrupts (host state), but tracing and lockdep
+ 	 * have them in state 'on' as recorded before entering guest mode.
+ 	 * Same as enter_from_user_mode().
+ 	 *
+ 	 * context_tracking_guest_exit() restores host context and reinstates
+ 	 * RCU if enabled and required.
+ 	 *
+ 	 * This needs to be done before the below as native_read_msr()
+ 	 * contains a tracepoint and x86_spec_ctrl_restore_host() calls
+ 	 * into world and some more.
+ 	 */
+ 	lockdep_hardirqs_off(CALLER_ADDR0);
+ 	context_tracking_guest_exit();
+ 
+ 	instrumentation_begin();
+ 	trace_hardirqs_off_finish();
+ 	instrumentation_end();
+ }
+ 
+ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
++>>>>>>> 160457140187 (KVM: x86: Defer vtime accounting 'til after IRQ handling)
  	trace_kvm_entry(vcpu);
  
  	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
diff --cc arch/x86/kvm/vmx/vmx.c
index a165c3a9d655,e108fb47855b..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -6598,9 -6661,65 +6598,69 @@@ static fastpath_t vmx_exit_handlers_fas
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
+ 					struct vcpu_vmx *vmx)
+ {
+ 	/*
+ 	 * VMENTER enables interrupts (host state), but the kernel state is
+ 	 * interrupts disabled when this is invoked. Also tell RCU about
+ 	 * it. This is the same logic as for exit_to_user_mode().
+ 	 *
+ 	 * This ensures that e.g. latency analysis on the host observes
+ 	 * guest mode as interrupt enabled.
+ 	 *
+ 	 * guest_enter_irqoff() informs context tracking about the
+ 	 * transition to guest mode and if enabled adjusts RCU state
+ 	 * accordingly.
+ 	 */
+ 	instrumentation_begin();
+ 	trace_hardirqs_on_prepare();
+ 	lockdep_hardirqs_on_prepare(CALLER_ADDR0);
+ 	instrumentation_end();
+ 
+ 	guest_enter_irqoff();
+ 	lockdep_hardirqs_on(CALLER_ADDR0);
+ 
+ 	/* L1D Flush includes CPU buffer clear to mitigate MDS */
+ 	if (static_branch_unlikely(&vmx_l1d_should_flush))
+ 		vmx_l1d_flush(vcpu);
+ 	else if (static_branch_unlikely(&mds_user_clear))
+ 		mds_clear_cpu_buffers();
+ 
+ 	if (vcpu->arch.cr2 != native_read_cr2())
+ 		native_write_cr2(vcpu->arch.cr2);
+ 
+ 	vmx->fail = __vmx_vcpu_run(vmx, (unsigned long *)&vcpu->arch.regs,
+ 				   vmx->loaded_vmcs->launched);
+ 
+ 	vcpu->arch.cr2 = native_read_cr2();
+ 
+ 	/*
+ 	 * VMEXIT disables interrupts (host state), but tracing and lockdep
+ 	 * have them in state 'on' as recorded before entering guest mode.
+ 	 * Same as enter_from_user_mode().
+ 	 *
+ 	 * context_tracking_guest_exit() restores host context and reinstates
+ 	 * RCU if enabled and required.
+ 	 *
+ 	 * This needs to be done before the below as native_read_msr()
+ 	 * contains a tracepoint and x86_spec_ctrl_restore_host() calls
+ 	 * into world and some more.
+ 	 */
+ 	lockdep_hardirqs_off(CALLER_ADDR0);
+ 	context_tracking_guest_exit();
+ 
+ 	instrumentation_begin();
+ 	trace_hardirqs_off_finish();
+ 	instrumentation_end();
+ }
+ 
++>>>>>>> 160457140187 (KVM: x86: Defer vtime accounting 'til after IRQ handling)
  static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
  {
 +	fastpath_t exit_fastpath;
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
  	unsigned long cr3, cr4;
  
diff --cc arch/x86/kvm/x86.c
index 1085ad8f4583,6eda2834fc05..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -9272,7 -9315,15 +9272,19 @@@ static int vcpu_enter_guest(struct kvm_
  	local_irq_disable();
  	kvm_after_interrupt(vcpu);
  
++<<<<<<< HEAD
 +	guest_exit_irqoff();
++=======
+ 	/*
+ 	 * Wait until after servicing IRQs to account guest time so that any
+ 	 * ticks that occurred while running the guest are properly accounted
+ 	 * to the guest.  Waiting until IRQs are enabled degrades the accuracy
+ 	 * of accounting via context tracking, but the loss of accuracy is
+ 	 * acceptable for all known use cases.
+ 	 */
+ 	vtime_account_guest_exit();
+ 
++>>>>>>> 160457140187 (KVM: x86: Defer vtime accounting 'til after IRQ handling)
  	if (lapic_in_kernel(vcpu)) {
  		s64 delta = vcpu->arch.apic->lapic_timer.advance_expire_delta;
  		if (delta != S64_MIN) {
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/x86.c
