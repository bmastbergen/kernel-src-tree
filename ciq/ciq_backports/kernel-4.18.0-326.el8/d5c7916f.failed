RDMA/mlx5: Use ib_umem_find_best_pgsz() for mkc's

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit d5c7916fe4613e9128b0f877f7e2dd0c85f5d2d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/d5c7916f.failed

Now that all the PAS arrays or UMR XLT's for mkcs are filled using
rdma_for_each_block() we can use the common ib_umem_find_best_pgsz()
algorithm.

Link: https://lore.kernel.org/r/20201026132314.1336717-6-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit d5c7916fe4613e9128b0f877f7e2dd0c85f5d2d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mr.c
index d1c1a16be30f,b091d84ba435..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -982,15 -957,20 +982,25 @@@ static struct mlx5_cache_ent *mr_cache_
  	return &cache->ent[order];
  }
  
 -static struct mlx5_ib_mr *alloc_mr_from_cache(struct ib_pd *pd,
 -					      struct ib_umem *umem, u64 iova,
 -					      int access_flags)
 +static struct mlx5_ib_mr *
 +alloc_mr_from_cache(struct ib_pd *pd, struct ib_umem *umem, u64 virt_addr,
 +		    u64 len, int npages, int page_shift, unsigned int order,
 +		    int access_flags)
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 -	struct mlx5_cache_ent *ent;
 +	struct mlx5_cache_ent *ent = mr_cache_ent_from_order(dev, order);
  	struct mlx5_ib_mr *mr;
++<<<<<<< HEAD
 +
++=======
+ 	unsigned int page_size;
+ 
+ 	page_size = mlx5_umem_find_best_pgsz(umem, mkc, log_page_size, 0, iova);
+ 	if (WARN_ON(!page_size))
+ 		return ERR_PTR(-EINVAL);
+ 	ent = mr_cache_ent_from_order(
+ 		dev, order_base_2(ib_umem_num_dma_blocks(umem, page_size)));
++>>>>>>> d5c7916fe461 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for mkc's)
  	if (!ent)
  		return ERR_PTR(-E2BIG);
  
@@@ -1009,9 -989,10 +1019,13 @@@
  	mr->umem = umem;
  	mr->access_flags = access_flags;
  	mr->desc_size = sizeof(struct mlx5_mtt);
 -	mr->mmkey.iova = iova;
 -	mr->mmkey.size = umem->length;
 +	mr->mmkey.iova = virt_addr;
 +	mr->mmkey.size = len;
  	mr->mmkey.pd = to_mpd(pd)->pdn;
++<<<<<<< HEAD
++=======
+ 	mr->page_shift = order_base_2(page_size);
++>>>>>>> d5c7916fe461 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for mkc's)
  
  	return mr;
  }
@@@ -1240,12 -1278,11 +1254,13 @@@ int mlx5_ib_update_xlt(struct mlx5_ib_m
   * Else, the given ibmr will be used.
   */
  static struct mlx5_ib_mr *reg_create(struct ib_mr *ibmr, struct ib_pd *pd,
 -				     struct ib_umem *umem, u64 iova,
 -				     int access_flags, bool populate)
 +				     u64 virt_addr, u64 length,
 +				     struct ib_umem *umem, int npages,
 +				     int page_shift, int access_flags,
 +				     bool populate)
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	unsigned int page_size;
  	struct mlx5_ib_mr *mr;
  	__be64 *pas;
  	void *mkc;
@@@ -1263,7 -1306,8 +1284,12 @@@
  
  	inlen = MLX5_ST_SZ_BYTES(create_mkey_in);
  	if (populate)
++<<<<<<< HEAD
 +		inlen += sizeof(*pas) * roundup(npages, 2);
++=======
+ 		inlen += sizeof(*pas) *
+ 			 roundup(ib_umem_num_dma_blocks(umem, page_size), 2);
++>>>>>>> d5c7916fe461 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for mkc's)
  	in = kvzalloc(inlen, GFP_KERNEL);
  	if (!in) {
  		err = -ENOMEM;
@@@ -1275,7 -1319,7 +1301,11 @@@
  			err = -EINVAL;
  			goto err_2;
  		}
++<<<<<<< HEAD
 +		mlx5_ib_populate_pas(dev, umem, page_shift, pas,
++=======
+ 		mlx5_ib_populate_pas(umem, 1UL << mr->page_shift, pas,
++>>>>>>> d5c7916fe461 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for mkc's)
  				     pg_cap ? MLX5_IB_MTT_PRESENT : 0);
  	}
  
@@@ -1290,14 -1334,14 +1320,22 @@@
  	MLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_MTT);
  	MLX5_SET(mkc, mkc, umr_en, 1);
  
 -	MLX5_SET64(mkc, mkc, len, umem->length);
 +	MLX5_SET64(mkc, mkc, len, length);
  	MLX5_SET(mkc, mkc, bsf_octword_size, 0);
  	MLX5_SET(mkc, mkc, translations_octword_size,
++<<<<<<< HEAD
 +		 get_octo_len(virt_addr, length, page_shift));
 +	MLX5_SET(mkc, mkc, log_page_size, page_shift);
 +	if (populate) {
 +		MLX5_SET(create_mkey_in, in, translations_octword_actual_size,
 +			 get_octo_len(virt_addr, length, page_shift));
++=======
+ 		 get_octo_len(iova, umem->length, mr->page_shift));
+ 	MLX5_SET(mkc, mkc, log_page_size, mr->page_shift);
+ 	if (populate) {
+ 		MLX5_SET(create_mkey_in, in, translations_octword_actual_size,
+ 			 get_octo_len(iova, umem->length, mr->page_shift));
++>>>>>>> d5c7916fe461 (RDMA/mlx5: Use ib_umem_find_best_pgsz() for mkc's)
  	}
  
  	err = mlx5_ib_create_mkey(dev, &mr->mmkey, in, inlen);
diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c
index 64b33b1a2d9b..e642fd4f1880 100644
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@ -84,6 +84,15 @@ unsigned long ib_umem_find_best_pgsz(struct ib_umem *umem,
 	dma_addr_t mask;
 	int i;
 
+	if (umem->is_odp) {
+		unsigned int page_size = BIT(to_ib_umem_odp(umem)->page_shift);
+
+		/* ODP must always be self consistent. */
+		if (!(pgsz_bitmap & page_size))
+			return 0;
+		return page_size;
+	}
+
 	/* rdma_for_each_block() has a bug if the page size is smaller than the
 	 * page size used to build the umem. For now prevent smaller page sizes
 	 * from being returned.
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index adddf68c9489..1a6e0eafe707 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -42,6 +42,33 @@
 
 #define MLX5_MKEY_PAGE_SHIFT_MASK __mlx5_mask(mkc, log_page_size)
 
+static __always_inline unsigned long
+__mlx5_log_page_size_to_bitmap(unsigned int log_pgsz_bits,
+			       unsigned int pgsz_shift)
+{
+	unsigned int largest_pg_shift =
+		min_t(unsigned long, (1ULL << log_pgsz_bits) - 1 + pgsz_shift,
+		      BITS_PER_LONG - 1);
+
+	/*
+	 * Despite a command allowing it, the device does not support lower than
+	 * 4k page size.
+	 */
+	pgsz_shift = max_t(unsigned int, MLX5_ADAPTER_PAGE_SHIFT, pgsz_shift);
+	return GENMASK(largest_pg_shift, pgsz_shift);
+}
+
+/*
+ * For mkc users, instead of a page_offset the command has a start_iova which
+ * specifies both the page_offset and the on-the-wire IOVA
+ */
+#define mlx5_umem_find_best_pgsz(umem, typ, log_pgsz_fld, pgsz_shift, iova)    \
+	ib_umem_find_best_pgsz(umem,                                           \
+			       __mlx5_log_page_size_to_bitmap(                 \
+				       __mlx5_bit_sz(typ, log_pgsz_fld),       \
+				       pgsz_shift),                            \
+			       iova)
+
 enum {
 	MLX5_IB_MMAP_OFFSET_START = 9,
 	MLX5_IB_MMAP_OFFSET_END = 255,
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
