net/mlx5e: VF tunnel RX traffic offloading

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Vlad Buslov <vladbu@nvidia.com>
commit a508728a4c8bfaf15839d5b23c19bf6b9908d43d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/a508728a.failed

When tunnel endpoint is on VF the encapsulated RX traffic is exposed on the
representor of the VF without any further processing of rules installed on
the VF. Detect such case by checking if the device returned by route lookup
in decap rule handling code is a mlx5 VF and handle it with new redirection
tables API.

Example TC rules for VF tunnel traffic:

1. Rule that encapsulates the tunneled flow and redirects packets from
source VF rep to tunnel device:

$ tc -s filter show dev enp8s0f0_1 ingress
filter protocol ip pref 4 flower chain 0
filter protocol ip pref 4 flower chain 0 handle 0x1
  dst_mac 0a:40:bd:30:89:99
  src_mac ca:2e:a7:3f:f5:0f
  eth_type ipv4
  ip_tos 0/0x3
  ip_flags nofrag
  in_hw in_hw_count 1
        action order 1: tunnel_key  set
        src_ip 7.7.7.5
        dst_ip 7.7.7.1
        key_id 98
        dst_port 4789
        nocsum
        ttl 64 pipe
         index 1 ref 1 bind 1 installed 411 sec used 411 sec
        Action statistics:
        Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0
        no_percpu
        used_hw_stats delayed

        action order 2: mirred (Egress Redirect to device vxlan_sys_4789) stolen
        index 1 ref 1 bind 1 installed 411 sec used 0 sec
        Action statistics:
        Sent 5615833 bytes 4028 pkt (dropped 0, overlimits 0 requeues 0)
        Sent software 0 bytes 0 pkt
        Sent hardware 5615833 bytes 4028 pkt
        backlog 0b 0p requeues 0
        cookie bb406d45d343bf7ade9690ae80c7cba4
        no_percpu
        used_hw_stats delayed

2. Rule that redirects from tunnel device to UL rep:

$ tc -s filter show dev vxlan_sys_4789 ingress
filter protocol ip pref 4 flower chain 0
filter protocol ip pref 4 flower chain 0 handle 0x1
  dst_mac ca:2e:a7:3f:f5:0f
  src_mac 0a:40:bd:30:89:99
  eth_type ipv4
  enc_dst_ip 7.7.7.5
  enc_src_ip 7.7.7.1
  enc_key_id 98
  enc_dst_port 4789
  enc_tos 0
  ip_flags nofrag
  in_hw in_hw_count 1
        action order 1: tunnel_key  unset pipe
         index 2 ref 1 bind 1 installed 434 sec used 434 sec
        Action statistics:
        Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0
        used_hw_stats delayed

        action order 2: mirred (Egress Redirect to device enp8s0f0_1) stolen
        index 4 ref 1 bind 1 installed 434 sec used 0 sec
        Action statistics:
        Sent 129936 bytes 1082 pkt (dropped 0, overlimits 0 requeues 0)
        Sent software 0 bytes 0 pkt
        Sent hardware 129936 bytes 1082 pkt
        backlog 0b 0p requeues 0
        cookie ac17cf398c4c69e4a5b2f7aabd1b88ff
        no_percpu
        used_hw_stats delayed

Co-developed-by: Dmytro Linkin <dlinkin@nvidia.com>
	Signed-off-by: Dmytro Linkin <dlinkin@nvidia.com>
	Signed-off-by: Vlad Buslov <vladbu@nvidia.com>
	Reviewed-by: Roi Dayan <roid@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit a508728a4c8bfaf15839d5b23c19bf6b9908d43d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
#	drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
index 00c2a6fd4471,098f3efa5d4d..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@@ -1248,6 -1320,43 +1248,46 @@@ static void remove_unready_flow(struct 
  	mutex_unlock(&uplink_priv->unready_flows_lock);
  }
  
++<<<<<<< HEAD
++=======
+ static bool same_hw_devs(struct mlx5e_priv *priv, struct mlx5e_priv *peer_priv);
+ 
+ bool mlx5e_tc_is_vf_tunnel(struct net_device *out_dev, struct net_device *route_dev)
+ {
+ 	struct mlx5_core_dev *out_mdev, *route_mdev;
+ 	struct mlx5e_priv *out_priv, *route_priv;
+ 
+ 	out_priv = netdev_priv(out_dev);
+ 	out_mdev = out_priv->mdev;
+ 	route_priv = netdev_priv(route_dev);
+ 	route_mdev = route_priv->mdev;
+ 
+ 	if (out_mdev->coredev_type != MLX5_COREDEV_PF ||
+ 	    route_mdev->coredev_type != MLX5_COREDEV_VF)
+ 		return false;
+ 
+ 	return same_hw_devs(out_priv, route_priv);
+ }
+ 
+ int mlx5e_tc_query_route_vport(struct net_device *out_dev, struct net_device *route_dev, u16 *vport)
+ {
+ 	struct mlx5e_priv *out_priv, *route_priv;
+ 	struct mlx5_core_dev *route_mdev;
+ 	struct mlx5_eswitch *esw;
+ 	u16 vhca_id;
+ 	int err;
+ 
+ 	out_priv = netdev_priv(out_dev);
+ 	esw = out_priv->mdev->priv.eswitch;
+ 	route_priv = netdev_priv(route_dev);
+ 	route_mdev = route_priv->mdev;
+ 
+ 	vhca_id = MLX5_CAP_GEN(route_mdev, vhca_id);
+ 	err = mlx5_eswitch_vhca_id_to_vport(esw, vhca_id, vport);
+ 	return err;
+ }
+ 
++>>>>>>> a508728a4c8b (net/mlx5e: VF tunnel RX traffic offloading)
  static int
  mlx5e_tc_add_fdb_flow(struct mlx5e_priv *priv,
  		      struct mlx5e_tc_flow *flow,
@@@ -1394,8 -1503,9 +1434,9 @@@ static void mlx5e_tc_del_fdb_flow(struc
  			kfree(attr->parse_attr->tun_info[out_index]);
  		}
  	kvfree(attr->parse_attr);
+ 	kvfree(attr->esw_attr->rx_tun_attr);
  
 -	mlx5_tc_ct_match_del(get_ct_priv(priv), &flow->attr->ct_attr);
 +	mlx5_tc_ct_match_del(priv, &flow->attr->ct_attr);
  
  	if (attr->action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)
  		mlx5e_detach_mod_hdr(priv, flow);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
index 00b8693ad3dc,ee0029192504..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
@@@ -238,6 -248,19 +238,22 @@@ void mlx5e_del_offloaded_nic_rule(struc
  				  struct mlx5_flow_handle *rule,
  				  struct mlx5_flow_attr *attr);
  
++<<<<<<< HEAD
++=======
+ struct mlx5_flow_handle *
+ mlx5_tc_rule_insert(struct mlx5e_priv *priv,
+ 		    struct mlx5_flow_spec *spec,
+ 		    struct mlx5_flow_attr *attr);
+ void
+ mlx5_tc_rule_delete(struct mlx5e_priv *priv,
+ 		    struct mlx5_flow_handle *rule,
+ 		    struct mlx5_flow_attr *attr);
+ 
+ bool mlx5e_tc_is_vf_tunnel(struct net_device *out_dev, struct net_device *route_dev);
+ int mlx5e_tc_query_route_vport(struct net_device *out_dev, struct net_device *route_dev,
+ 			       u16 *vport);
+ 
++>>>>>>> a508728a4c8b (net/mlx5e: VF tunnel RX traffic offloading)
  #else /* CONFIG_MLX5_CLS_ACT */
  static inline int  mlx5e_tc_nic_init(struct mlx5e_priv *priv) { return 0; }
  static inline void mlx5e_tc_nic_cleanup(struct mlx5e_priv *priv) {}
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index 1aa743021f64,a44728595420..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@@ -339,6 -373,131 +374,134 @@@ esw_setup_chain_dest(struct mlx5_flow_d
  	return  0;
  }
  
++<<<<<<< HEAD
++=======
+ static void esw_put_dest_tables_loop(struct mlx5_eswitch *esw, struct mlx5_flow_attr *attr,
+ 				     int from, int to)
+ {
+ 	struct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;
+ 	struct mlx5_fs_chains *chains = esw_chains(esw);
+ 	int i;
+ 
+ 	for (i = from; i < to; i++)
+ 		if (esw_attr->dests[i].flags & MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)
+ 			mlx5_chains_put_table(chains, 0, 1, 0);
+ 		else if (mlx5_esw_indir_table_needed(esw, attr, esw_attr->dests[i].rep->vport,
+ 						     esw_attr->dests[i].mdev))
+ 			mlx5_esw_indir_table_put(esw, attr, esw_attr->dests[i].rep->vport,
+ 						 false);
+ }
+ 
+ static bool
+ esw_is_chain_src_port_rewrite(struct mlx5_eswitch *esw, struct mlx5_esw_flow_attr *esw_attr)
+ {
+ 	int i;
+ 
+ 	for (i = esw_attr->split_count; i < esw_attr->out_count; i++)
+ 		if (esw_attr->dests[i].flags & MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)
+ 			return true;
+ 	return false;
+ }
+ 
+ static int
+ esw_setup_chain_src_port_rewrite(struct mlx5_flow_destination *dest,
+ 				 struct mlx5_flow_act *flow_act,
+ 				 struct mlx5_eswitch *esw,
+ 				 struct mlx5_fs_chains *chains,
+ 				 struct mlx5_flow_attr *attr,
+ 				 int *i)
+ {
+ 	struct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;
+ 	int j, err;
+ 
+ 	if (!(attr->flags & MLX5_ESW_ATTR_FLAG_SRC_REWRITE))
+ 		return -EOPNOTSUPP;
+ 
+ 	for (j = esw_attr->split_count; j < esw_attr->out_count; j++, (*i)++) {
+ 		err = esw_setup_chain_dest(dest, flow_act, chains, attr->dest_chain, 1, 0, *i);
+ 		if (err)
+ 			goto err_setup_chain;
+ 		flow_act->action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;
+ 		flow_act->pkt_reformat = esw_attr->dests[j].pkt_reformat;
+ 	}
+ 	return 0;
+ 
+ err_setup_chain:
+ 	esw_put_dest_tables_loop(esw, attr, esw_attr->split_count, j);
+ 	return err;
+ }
+ 
+ static void esw_cleanup_chain_src_port_rewrite(struct mlx5_eswitch *esw,
+ 					       struct mlx5_flow_attr *attr)
+ {
+ 	struct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;
+ 
+ 	esw_put_dest_tables_loop(esw, attr, esw_attr->split_count, esw_attr->out_count);
+ }
+ 
+ static bool
+ esw_is_indir_table(struct mlx5_eswitch *esw, struct mlx5_flow_attr *attr)
+ {
+ 	struct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;
+ 	int i;
+ 
+ 	for (i = esw_attr->split_count; i < esw_attr->out_count; i++)
+ 		if (mlx5_esw_indir_table_needed(esw, attr, esw_attr->dests[i].rep->vport,
+ 						esw_attr->dests[i].mdev))
+ 			return true;
+ 	return false;
+ }
+ 
+ static int
+ esw_setup_indir_table(struct mlx5_flow_destination *dest,
+ 		      struct mlx5_flow_act *flow_act,
+ 		      struct mlx5_eswitch *esw,
+ 		      struct mlx5_flow_attr *attr,
+ 		      struct mlx5_flow_spec *spec,
+ 		      bool ignore_flow_lvl,
+ 		      int *i)
+ {
+ 	struct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;
+ 	int j, err;
+ 
+ 	if (!(attr->flags & MLX5_ESW_ATTR_FLAG_SRC_REWRITE))
+ 		return -EOPNOTSUPP;
+ 
+ 	for (j = esw_attr->split_count; j < esw_attr->out_count; j++, (*i)++) {
+ 		if (ignore_flow_lvl)
+ 			flow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;
+ 		dest[*i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
+ 
+ 		dest[*i].ft = mlx5_esw_indir_table_get(esw, attr, spec,
+ 						       esw_attr->dests[j].rep->vport, false);
+ 		if (IS_ERR(dest[*i].ft)) {
+ 			err = PTR_ERR(dest[*i].ft);
+ 			goto err_indir_tbl_get;
+ 		}
+ 	}
+ 
+ 	if (mlx5_esw_indir_table_decap_vport(attr)) {
+ 		err = esw_setup_decap_indir(esw, attr, spec);
+ 		if (err)
+ 			goto err_indir_tbl_get;
+ 	}
+ 
+ 	return 0;
+ 
+ err_indir_tbl_get:
+ 	esw_put_dest_tables_loop(esw, attr, esw_attr->split_count, j);
+ 	return err;
+ }
+ 
+ static void esw_cleanup_indir_table(struct mlx5_eswitch *esw, struct mlx5_flow_attr *attr)
+ {
+ 	struct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;
+ 
+ 	esw_put_dest_tables_loop(esw, attr, esw_attr->split_count, esw_attr->out_count);
+ 	esw_cleanup_decap_indir(esw, attr);
+ }
+ 
++>>>>>>> a508728a4c8b (net/mlx5e: VF tunnel RX traffic offloading)
  static void
  esw_cleanup_chain_dest(struct mlx5_fs_chains *chains, u32 chain, u32 prio, u32 level)
  {
@@@ -389,8 -549,13 +552,8 @@@ esw_setup_dests(struct mlx5_flow_destin
  	struct mlx5_fs_chains *chains = esw_chains(esw);
  	int err = 0;
  
 -	if (!mlx5_eswitch_termtbl_required(esw, attr, flow_act, spec) &&
 -	    MLX5_CAP_GEN(esw_attr->in_mdev, reg_c_preserve) &&
 -	    mlx5_eswitch_vport_match_metadata_enabled(esw))
 -		attr->flags |= MLX5_ESW_ATTR_FLAG_SRC_REWRITE;
 -
  	if (attr->dest_ft) {
- 		esw_setup_ft_dest(dest, flow_act, attr, *i);
+ 		esw_setup_ft_dest(dest, flow_act, esw, attr, spec, *i);
  		(*i)++;
  	} else if (attr->flags & MLX5_ESW_ATTR_FLAG_SLOW_PATH) {
  		esw_setup_slow_path_dest(dest, flow_act, chains, *i);
@@@ -399,6 -564,10 +562,13 @@@
  		err = esw_setup_chain_dest(dest, flow_act, chains, attr->dest_chain,
  					   1, 0, *i);
  		(*i)++;
++<<<<<<< HEAD
++=======
+ 	} else if (esw_is_indir_table(esw, attr)) {
+ 		err = esw_setup_indir_table(dest, flow_act, esw, attr, spec, true, i);
+ 	} else if (esw_is_chain_src_port_rewrite(esw, esw_attr)) {
+ 		err = esw_setup_chain_src_port_rewrite(dest, flow_act, esw, chains, attr, i);
++>>>>>>> a508728a4c8b (net/mlx5e: VF tunnel RX traffic offloading)
  	} else {
  		*i = esw_setup_vport_dests(dest, flow_act, esw, esw_attr, *i);
  	}
@@@ -410,10 -579,19 +580,23 @@@ static voi
  esw_cleanup_dests(struct mlx5_eswitch *esw,
  		  struct mlx5_flow_attr *attr)
  {
 -	struct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;
  	struct mlx5_fs_chains *chains = esw_chains(esw);
  
++<<<<<<< HEAD
 +	if (!(attr->flags & MLX5_ESW_ATTR_FLAG_SLOW_PATH) && attr->dest_chain)
 +		esw_cleanup_chain_dest(chains, attr->dest_chain, 1, 0);
++=======
+ 	if (attr->dest_ft) {
+ 		esw_cleanup_decap_indir(esw, attr);
+ 	} else if (!(attr->flags & MLX5_ESW_ATTR_FLAG_SLOW_PATH)) {
+ 		if (attr->dest_chain)
+ 			esw_cleanup_chain_dest(chains, attr->dest_chain, 1, 0);
+ 		else if (esw_is_indir_table(esw, attr))
+ 			esw_cleanup_indir_table(esw, attr);
+ 		else if (esw_is_chain_src_port_rewrite(esw, esw_attr))
+ 			esw_cleanup_chain_src_port_rewrite(esw, attr);
+ 	}
++>>>>>>> a508728a4c8b (net/mlx5e: VF tunnel RX traffic offloading)
  }
  
  struct mlx5_flow_handle *
@@@ -556,8 -734,20 +739,25 @@@ mlx5_eswitch_add_fwd_rule(struct mlx5_e
  	}
  
  	flow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;
++<<<<<<< HEAD
 +	for (i = 0; i < esw_attr->split_count; i++)
 +		esw_setup_vport_dest(dest, &flow_act, esw, esw_attr, i, i, false);
++=======
+ 	for (i = 0; i < esw_attr->split_count; i++) {
+ 		if (esw_is_indir_table(esw, attr))
+ 			err = esw_setup_indir_table(dest, &flow_act, esw, attr, spec, false, &i);
+ 		else if (esw_is_chain_src_port_rewrite(esw, esw_attr))
+ 			err = esw_setup_chain_src_port_rewrite(dest, &flow_act, esw, chains, attr,
+ 							       &i);
+ 		else
+ 			esw_setup_vport_dest(dest, &flow_act, esw, esw_attr, i, i, false);
+ 
+ 		if (err) {
+ 			rule = ERR_PTR(err);
+ 			goto err_chain_src_rewrite;
+ 		}
+ 	}
++>>>>>>> a508728a4c8b (net/mlx5e: VF tunnel RX traffic offloading)
  	dest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
  	dest[i].ft = fwd_fdb;
  	i++;
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
index b2322fbe0687..d832a2cd5894 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.c
@@ -466,6 +466,57 @@ int mlx5e_tc_tun_create_header_ipv6(struct mlx5e_priv *priv,
 }
 #endif
 
+int mlx5e_tc_tun_route_lookup(struct mlx5e_priv *priv,
+			      struct mlx5_flow_spec *spec,
+			      struct mlx5_flow_attr *flow_attr)
+{
+	struct mlx5_esw_flow_attr *esw_attr = flow_attr->esw_attr;
+	TC_TUN_ROUTE_ATTR_INIT(attr);
+	u16 vport_num;
+	int err = 0;
+
+	if (flow_attr->ip_version == 4) {
+		/* Addresses are swapped for decap */
+		attr.fl.fl4.saddr = esw_attr->rx_tun_attr->dst_ip.v4;
+		attr.fl.fl4.daddr = esw_attr->rx_tun_attr->src_ip.v4;
+		err = mlx5e_route_lookup_ipv4_get(priv, priv->netdev, &attr);
+	}
+#if IS_ENABLED(CONFIG_INET) && IS_ENABLED(CONFIG_IPV6)
+	else if (flow_attr->ip_version == 6) {
+		/* Addresses are swapped for decap */
+		attr.fl.fl6.saddr = esw_attr->rx_tun_attr->dst_ip.v6;
+		attr.fl.fl6.daddr = esw_attr->rx_tun_attr->src_ip.v6;
+		err = mlx5e_route_lookup_ipv6_get(priv, priv->netdev, &attr);
+	}
+#endif
+	else
+		return 0;
+
+	if (err)
+		return err;
+
+	if (attr.route_dev->netdev_ops != &mlx5e_netdev_ops ||
+	    !mlx5e_tc_is_vf_tunnel(attr.out_dev, attr.route_dev))
+		goto out;
+
+	err = mlx5e_tc_query_route_vport(attr.out_dev, attr.route_dev, &vport_num);
+	if (err)
+		goto out;
+
+	esw_attr->rx_tun_attr->vni = MLX5_GET(fte_match_param, spec->match_value,
+					      misc_parameters.vxlan_vni);
+	esw_attr->rx_tun_attr->decap_vport = vport_num;
+
+out:
+	if (flow_attr->ip_version == 4)
+		mlx5e_route_lookup_ipv4_put(&attr);
+#if IS_ENABLED(CONFIG_INET) && IS_ENABLED(CONFIG_IPV6)
+	else if (flow_attr->ip_version == 6)
+		mlx5e_route_lookup_ipv6_put(&attr);
+#endif
+	return err;
+}
+
 bool mlx5e_tc_tun_device_to_offload(struct mlx5e_priv *priv,
 				    struct net_device *netdev)
 {
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.h b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.h
index 704359df6095..9d6ee9405eaf 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun.h
@@ -70,6 +70,9 @@ mlx5e_tc_tun_create_header_ipv6(struct mlx5e_priv *priv,
 				struct net_device *mirred_dev,
 				struct mlx5e_encap_entry *e) { return -EOPNOTSUPP; }
 #endif
+int mlx5e_tc_tun_route_lookup(struct mlx5e_priv *priv,
+			      struct mlx5_flow_spec *spec,
+			      struct mlx5_flow_attr *attr);
 
 bool mlx5e_tc_tun_device_to_offload(struct mlx5e_priv *priv,
 				    struct net_device *netdev);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tc.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
