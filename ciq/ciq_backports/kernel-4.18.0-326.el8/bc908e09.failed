KVM: x86: Consolidate guest enter/exit logic to common helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Sean Christopherson <seanjc@google.com>
commit bc908e091b3264672889162733020048901021fb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/bc908e09.failed

Move the enter/exit logic in {svm,vmx}_vcpu_enter_exit() to common
helpers.  Opportunistically update the somewhat stale comment about the
updates needing to occur immediately after VM-Exit.

No functional change intended.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lore.kernel.org/r/20210505002735.1684165-9-seanjc@google.com

(cherry picked from commit bc908e091b3264672889162733020048901021fb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/svm/svm.c
index c96b24a58317,b649f92287a2..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -3707,6 -3710,33 +3707,36 @@@ static fastpath_t svm_vcpu_run(struct k
  	struct vcpu_svm *svm = to_svm(vcpu);
  	unsigned long vmcb_pa = svm->current_vmcb->pa;
  
++<<<<<<< HEAD
++=======
+ 	kvm_guest_enter_irqoff();
+ 
+ 	if (sev_es_guest(vcpu->kvm)) {
+ 		__svm_sev_es_vcpu_run(vmcb_pa);
+ 	} else {
+ 		struct svm_cpu_data *sd = per_cpu(svm_data, vcpu->cpu);
+ 
+ 		/*
+ 		 * Use a single vmcb (vmcb01 because it's always valid) for
+ 		 * context switching guest state via VMLOAD/VMSAVE, that way
+ 		 * the state doesn't need to be copied between vmcb01 and
+ 		 * vmcb02 when switching vmcbs for nested virtualization.
+ 		 */
+ 		vmload(svm->vmcb01.pa);
+ 		__svm_vcpu_run(vmcb_pa, (unsigned long *)&vcpu->arch.regs);
+ 		vmsave(svm->vmcb01.pa);
+ 
+ 		vmload(__sme_page_pa(sd->save_area));
+ 	}
+ 
+ 	kvm_guest_exit_irqoff();
+ }
+ 
+ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_svm *svm = to_svm(vcpu);
+ 
++>>>>>>> bc908e091b32 (KVM: x86: Consolidate guest enter/exit logic to common helpers)
  	trace_kvm_entry(vcpu);
  
  	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
diff --cc arch/x86/kvm/vmx/vmx.c
index a165c3a9d655,d000cddbd734..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -6598,9 -6661,30 +6598,34 @@@ static fastpath_t vmx_exit_handlers_fas
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
+ 					struct vcpu_vmx *vmx)
+ {
+ 	kvm_guest_enter_irqoff();
+ 
+ 	/* L1D Flush includes CPU buffer clear to mitigate MDS */
+ 	if (static_branch_unlikely(&vmx_l1d_should_flush))
+ 		vmx_l1d_flush(vcpu);
+ 	else if (static_branch_unlikely(&mds_user_clear))
+ 		mds_clear_cpu_buffers();
+ 
+ 	if (vcpu->arch.cr2 != native_read_cr2())
+ 		native_write_cr2(vcpu->arch.cr2);
+ 
+ 	vmx->fail = __vmx_vcpu_run(vmx, (unsigned long *)&vcpu->arch.regs,
+ 				   vmx->loaded_vmcs->launched);
+ 
+ 	vcpu->arch.cr2 = native_read_cr2();
+ 
+ 	kvm_guest_exit_irqoff();
+ }
+ 
++>>>>>>> bc908e091b32 (KVM: x86: Consolidate guest enter/exit logic to common helpers)
  static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
  {
 +	fastpath_t exit_fastpath;
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
  	unsigned long cr3, cr4;
  
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/vmx/vmx.c
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 65d9c551171a..2f981e319795 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -8,6 +8,51 @@
 #include "kvm_cache_regs.h"
 #include "kvm_emulate.h"
 
+static __always_inline void kvm_guest_enter_irqoff(void)
+{
+	/*
+	 * VMENTER enables interrupts (host state), but the kernel state is
+	 * interrupts disabled when this is invoked. Also tell RCU about
+	 * it. This is the same logic as for exit_to_user_mode().
+	 *
+	 * This ensures that e.g. latency analysis on the host observes
+	 * guest mode as interrupt enabled.
+	 *
+	 * guest_enter_irqoff() informs context tracking about the
+	 * transition to guest mode and if enabled adjusts RCU state
+	 * accordingly.
+	 */
+	instrumentation_begin();
+	trace_hardirqs_on_prepare();
+	lockdep_hardirqs_on_prepare(CALLER_ADDR0);
+	instrumentation_end();
+
+	guest_enter_irqoff();
+	lockdep_hardirqs_on(CALLER_ADDR0);
+}
+
+static __always_inline void kvm_guest_exit_irqoff(void)
+{
+	/*
+	 * VMEXIT disables interrupts (host state), but tracing and lockdep
+	 * have them in state 'on' as recorded before entering guest mode.
+	 * Same as enter_from_user_mode().
+	 *
+	 * context_tracking_guest_exit() restores host context and reinstates
+	 * RCU if enabled and required.
+	 *
+	 * This needs to be done immediately after VM-Exit, before any code
+	 * that might contain tracepoints or call out to the greater world,
+	 * e.g. before x86_spec_ctrl_restore_host().
+	 */
+	lockdep_hardirqs_off(CALLER_ADDR0);
+	context_tracking_guest_exit();
+
+	instrumentation_begin();
+	trace_hardirqs_off_finish();
+	instrumentation_end();
+}
+
 #define KVM_NESTED_VMENTER_CONSISTENCY_CHECK(consistency_check)		\
 ({									\
 	bool failed = (consistency_check);				\
