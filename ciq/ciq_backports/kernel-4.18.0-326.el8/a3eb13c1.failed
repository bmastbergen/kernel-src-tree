mm/hmm: return the fault type from hmm_pte_need_fault()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit a3eb13c1579ba97d79fbbc98bc5b1296a3688a25
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/a3eb13c1.failed

Using two bools instead of flags return is not necessary and leads to
bugs. Returning a value is easier for the compiler to check and easier to
pass around the code flow.

Convert the two bools into flags and push the change to all callers.

Link: https://lore.kernel.org/r/20200327200021.29372-3-jgg@ziepe.ca
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit a3eb13c1579ba97d79fbbc98bc5b1296a3688a25)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index 3233a7437881,d208ddd35106..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -296,40 -29,18 +296,51 @@@ EXPORT_SYMBOL(hmm_mirror_unregister)
  struct hmm_vma_walk {
  	struct hmm_range	*range;
  	unsigned long		last;
 -	unsigned int		flags;
 +	bool			fault;
 +	bool			block;
  };
  
++<<<<<<< HEAD
 +static int hmm_vma_do_fault(struct mm_walk *walk, unsigned long addr,
 +			    bool write_fault, uint64_t *pfn)
++=======
+ enum {
+ 	HMM_NEED_FAULT = 1 << 0,
+ 	HMM_NEED_WRITE_FAULT = 1 << 1,
+ 	HMM_NEED_ALL_BITS = HMM_NEED_FAULT | HMM_NEED_WRITE_FAULT,
+ };
+ 
+ static int hmm_pfns_fill(unsigned long addr, unsigned long end,
+ 		struct hmm_range *range, enum hmm_pfn_value_e value)
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  {
 +	unsigned int flags = FAULT_FLAG_REMOTE;
 +	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 +	struct hmm_range *range = hmm_vma_walk->range;
 +	struct vm_area_struct *vma = walk->vma;
 +	vm_fault_t ret;
 +
 +	flags |= hmm_vma_walk->block ? 0 : FAULT_FLAG_ALLOW_RETRY;
 +	flags |= write_fault ? FAULT_FLAG_WRITE : 0;
 +	ret = handle_mm_fault(vma, addr, flags);
 +	if (ret & VM_FAULT_RETRY) {
 +		/* Note, handle_mm_fault did up_read(&mm->mmap_sem)) */
 +		return -EAGAIN;
 +	}
 +	if (ret & VM_FAULT_ERROR) {
 +		*pfn = range->values[HMM_PFN_ERROR];
 +		return -EFAULT;
 +	}
 +
 +	return -EBUSY;
 +}
 +
 +static int hmm_pfns_bad(unsigned long addr,
 +			unsigned long end,
 +			struct mm_walk *walk)
 +{
 +	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 +	struct hmm_range *range = hmm_vma_walk->range;
  	uint64_t *pfns = range->pfns;
  	unsigned long i;
  
@@@ -341,53 -52,56 +352,72 @@@
  }
  
  /*
 - * hmm_vma_fault() - fault in a range lacking valid pmd or pte(s)
 + * hmm_vma_walk_hole_() - handle a range lacking valid pmd or pte(s)
   * @addr: range virtual start address (inclusive)
   * @end: range virtual end address (exclusive)
-  * @fault: should we fault or not ?
-  * @write_fault: write fault ?
+  * @required_fault: HMM_NEED_* flags
   * @walk: mm_walk structure
 - * Return: -EBUSY after page fault, or page fault error
 + * Return: 0 on success, -EBUSY after page fault, or page fault error
   *
   * This function will be called whenever pmd_none() or pte_none() returns true,
   * or whenever there is no page directory covering the virtual address range.
   */
++<<<<<<< HEAD
 +static int hmm_vma_walk_hole_(unsigned long addr, unsigned long end,
 +			      bool fault, bool write_fault,
 +			      struct mm_walk *walk)
++=======
+ static int hmm_vma_fault(unsigned long addr, unsigned long end,
+ 			 unsigned int required_fault, struct mm_walk *walk)
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  {
  	struct hmm_vma_walk *hmm_vma_walk = walk->private;
  	struct hmm_range *range = hmm_vma_walk->range;
 -	struct vm_area_struct *vma = walk->vma;
  	uint64_t *pfns = range->pfns;
 -	unsigned long i = (addr - range->start) >> PAGE_SHIFT;
 -	unsigned int fault_flags = FAULT_FLAG_REMOTE;
 +	unsigned long i, page_size;
  
++<<<<<<< HEAD
++=======
+ 	WARN_ON_ONCE(!required_fault);
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  	hmm_vma_walk->last = addr;
 -
 -	if (!vma)
 -		goto out_error;
 -
 +	page_size = hmm_range_page_size(range);
 +	i = (addr - range->start) >> range->page_shift;
 +
 +	for (; addr < end; addr += page_size, i++) {
 +		pfns[i] = range->values[HMM_PFN_NONE];
 +		if (fault || write_fault) {
 +			int ret;
 +
++<<<<<<< HEAD
 +			ret = hmm_vma_do_fault(walk, addr, write_fault,
 +					       &pfns[i]);
 +			if (ret != -EBUSY)
 +				return ret;
 +		}
++=======
+ 	if (required_fault & HMM_NEED_WRITE_FAULT) {
+ 		if (!(vma->vm_flags & VM_WRITE))
+ 			return -EPERM;
+ 		fault_flags |= FAULT_FLAG_WRITE;
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  	}
  
 -	for (; addr < end; addr += PAGE_SIZE, i++)
 -		if (handle_mm_fault(vma, addr, fault_flags) & VM_FAULT_ERROR)
 -			goto out_error;
 -
 -	return -EBUSY;
 -
 -out_error:
 -	pfns[i] = range->values[HMM_PFN_ERROR];
 -	return -EFAULT;
 +	return (fault || write_fault) ? -EBUSY : 0;
  }
  
- static inline void hmm_pte_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
- 				      uint64_t pfns, uint64_t cpu_flags,
- 				      bool *fault, bool *write_fault)
+ static unsigned int hmm_pte_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
+ 				       uint64_t pfns, uint64_t cpu_flags)
  {
  	struct hmm_range *range = hmm_vma_walk->range;
  
++<<<<<<< HEAD
 +	if (!hmm_vma_walk->fault)
 +		return;
++=======
+ 	if (hmm_vma_walk->flags & HMM_FAULT_SNAPSHOT)
+ 		return 0;
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  
  	/*
  	 * So we not only consider the individual per page request we also
@@@ -403,46 -117,37 +433,57 @@@
  
  	/* We aren't ask to do anything ... */
  	if (!(pfns & range->flags[HMM_PFN_VALID]))
++<<<<<<< HEAD
 +		return;
 +	/* If this is device memory then only fault if explicitly requested */
 +	if ((cpu_flags & range->flags[HMM_PFN_DEVICE_PRIVATE])) {
 +		/* Do we fault on device memory ? */
 +		if (pfns & range->flags[HMM_PFN_DEVICE_PRIVATE]) {
 +			*write_fault = pfns & range->flags[HMM_PFN_WRITE];
 +			*fault = true;
 +		}
 +		return;
 +	}
++=======
+ 		return 0;
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  
- 	/* If CPU page table is not valid then we need to fault */
- 	*fault = !(cpu_flags & range->flags[HMM_PFN_VALID]);
  	/* Need to write fault ? */
  	if ((pfns & range->flags[HMM_PFN_WRITE]) &&
- 	    !(cpu_flags & range->flags[HMM_PFN_WRITE])) {
- 		*write_fault = true;
- 		*fault = true;
- 	}
+ 	    !(cpu_flags & range->flags[HMM_PFN_WRITE]))
+ 		return HMM_NEED_FAULT | HMM_NEED_WRITE_FAULT;
+ 
+ 	/* If CPU page table is not valid then we need to fault */
+ 	if (!(cpu_flags & range->flags[HMM_PFN_VALID]))
+ 		return HMM_NEED_FAULT;
+ 	return 0;
  }
  
- static void hmm_range_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
- 				 const uint64_t *pfns, unsigned long npages,
- 				 uint64_t cpu_flags, bool *fault,
- 				 bool *write_fault)
+ static unsigned int
+ hmm_range_need_fault(const struct hmm_vma_walk *hmm_vma_walk,
+ 		     const uint64_t *pfns, unsigned long npages,
+ 		     uint64_t cpu_flags)
  {
+ 	unsigned int required_fault = 0;
  	unsigned long i;
  
++<<<<<<< HEAD
 +	if (!hmm_vma_walk->fault) {
 +		*fault = *write_fault = false;
 +		return;
 +	}
++=======
+ 	if (hmm_vma_walk->flags & HMM_FAULT_SNAPSHOT)
+ 		return 0;
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  
- 	*fault = *write_fault = false;
  	for (i = 0; i < npages; ++i) {
- 		hmm_pte_need_fault(hmm_vma_walk, pfns[i], cpu_flags,
- 				   fault, write_fault);
- 		if ((*write_fault))
- 			return;
+ 		required_fault |=
+ 			hmm_pte_need_fault(hmm_vma_walk, pfns[i], cpu_flags);
+ 		if (required_fault == HMM_NEED_ALL_BITS)
+ 			return required_fault;
  	}
+ 	return required_fault;
  }
  
  static int hmm_vma_walk_hole(unsigned long addr, unsigned long end,
@@@ -457,9 -162,11 +498,17 @@@
  	i = (addr - range->start) >> PAGE_SHIFT;
  	npages = (end - addr) >> PAGE_SHIFT;
  	pfns = &range->pfns[i];
++<<<<<<< HEAD
 +	hmm_range_need_fault(hmm_vma_walk, pfns, npages,
 +			     0, &fault, &write_fault);
 +	return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
++=======
+ 	required_fault = hmm_range_need_fault(hmm_vma_walk, pfns, npages, 0);
+ 	if (required_fault)
+ 		return hmm_vma_fault(addr, end, required_fault, walk);
+ 	hmm_vma_walk->last = addr;
+ 	return hmm_pfns_fill(addr, end, range, HMM_PFN_NONE);
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  }
  
  static inline uint64_t pmd_to_hmm_pfn_flags(struct hmm_range *range, pmd_t pmd)
@@@ -483,11 -190,10 +532,18 @@@ static int hmm_vma_handle_pmd(struct mm
  
  	npages = (end - addr) >> PAGE_SHIFT;
  	cpu_flags = pmd_to_hmm_pfn_flags(range, pmd);
++<<<<<<< HEAD
 +	hmm_range_need_fault(hmm_vma_walk, pfns, npages, cpu_flags,
 +			     &fault, &write_fault);
 +
 +	if (fault || write_fault)
 +		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
++=======
+ 	required_fault =
+ 		hmm_range_need_fault(hmm_vma_walk, pfns, npages, cpu_flags);
+ 	if (required_fault)
+ 		return hmm_vma_fault(addr, end, required_fault, walk);
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  
  	pfn = pmd_pfn(pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
  	for (i = 0; addr < end; addr += PAGE_SIZE, i++, pfn++)
@@@ -602,7 -303,7 +651,11 @@@ static int hmm_vma_handle_pte(struct mm
  fault:
  	pte_unmap(ptep);
  	/* Fault any virtual address we were asked to fault */
++<<<<<<< HEAD
 +	return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
++=======
+ 	return hmm_vma_fault(addr, end, required_fault, walk);
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  }
  
  static int hmm_vma_walk_pmd(pmd_t *pmdp,
@@@ -612,8 -313,9 +665,14 @@@
  {
  	struct hmm_vma_walk *hmm_vma_walk = walk->private;
  	struct hmm_range *range = hmm_vma_walk->range;
++<<<<<<< HEAD
 +	uint64_t *pfns = range->pfns;
 +	unsigned long addr = start, i;
++=======
+ 	uint64_t *pfns = &range->pfns[(start - range->start) >> PAGE_SHIFT];
+ 	unsigned long npages = (end - start) >> PAGE_SHIFT;
+ 	unsigned long addr = start;
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  	pte_t *ptep;
  	pmd_t pmd;
  
@@@ -623,24 -325,19 +682,38 @@@ again
  		return hmm_vma_walk_hole(start, end, -1, walk);
  
  	if (thp_migration_supported() && is_pmd_migration_entry(pmd)) {
++<<<<<<< HEAD
 +		bool fault, write_fault;
 +		unsigned long npages;
 +		uint64_t *pfns;
 +
 +		i = (addr - range->start) >> PAGE_SHIFT;
 +		npages = (end - addr) >> PAGE_SHIFT;
 +		pfns = &range->pfns[i];
 +
 +		hmm_range_need_fault(hmm_vma_walk, pfns, npages,
 +				     0, &fault, &write_fault);
 +		if (fault || write_fault) {
++=======
+ 		if (hmm_range_need_fault(hmm_vma_walk, pfns, npages, 0)) {
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  			hmm_vma_walk->last = addr;
  			pmd_migration_entry_wait(walk->mm, pmdp);
  			return -EBUSY;
  		}
  		return hmm_pfns_fill(start, end, range, HMM_PFN_NONE);
++<<<<<<< HEAD
 +	} else if (!pmd_present(pmd))
 +		return hmm_pfns_bad(start, end, walk);
++=======
+ 	}
+ 
+ 	if (!pmd_present(pmd)) {
+ 		if (hmm_range_need_fault(hmm_vma_walk, pfns, npages, 0))
+ 			return -EFAULT;
+ 		return hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
+ 	}
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  
  	if (pmd_devmap(pmd) || pmd_trans_huge(pmd)) {
  		/*
@@@ -667,15 -363,17 +740,23 @@@
  	 * entry pointing to pte directory or it is a bad pmd that will not
  	 * recover.
  	 */
++<<<<<<< HEAD
 +	if (pmd_bad(pmd))
 +		return hmm_pfns_bad(start, end, walk);
++=======
+ 	if (pmd_bad(pmd)) {
+ 		if (hmm_range_need_fault(hmm_vma_walk, pfns, npages, 0))
+ 			return -EFAULT;
+ 		return hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
+ 	}
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  
  	ptep = pte_offset_map(pmdp, addr);
 -	for (; addr < end; addr += PAGE_SIZE, ptep++, pfns++) {
 +	i = (addr - range->start) >> PAGE_SHIFT;
 +	for (; addr < end; addr += PAGE_SIZE, ptep++, i++) {
  		int r;
  
 -		r = hmm_vma_handle_pte(walk, addr, end, pmdp, ptep, pfns);
 +		r = hmm_vma_handle_pte(walk, addr, end, pmdp, ptep, &pfns[i]);
  		if (r) {
  			/* hmm_vma_handle_pte() did pte_unmap() */
  			hmm_vma_walk->last = addr;
@@@ -736,12 -434,11 +817,16 @@@ static int hmm_vma_walk_pud(pud_t *pudp
  		pfns = &range->pfns[i];
  
  		cpu_flags = pud_to_hmm_pfn_flags(range, pud);
- 		hmm_range_need_fault(hmm_vma_walk, pfns, npages,
- 				     cpu_flags, &fault, &write_fault);
- 		if (fault || write_fault) {
+ 		required_fault = hmm_range_need_fault(hmm_vma_walk, pfns,
+ 						      npages, cpu_flags);
+ 		if (required_fault) {
  			spin_unlock(ptl);
++<<<<<<< HEAD
 +			return hmm_vma_walk_hole_(addr, end, fault, write_fault,
 +						  walk);
++=======
+ 			return hmm_vma_fault(addr, end, required_fault, walk);
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  		}
  
  		pfn = pud_pfn(pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
@@@ -772,26 -469,10 +857,26 @@@ static int hmm_vma_walk_hugetlb_entry(p
  	struct hmm_vma_walk *hmm_vma_walk = walk->private;
  	struct hmm_range *range = hmm_vma_walk->range;
  	struct vm_area_struct *vma = walk->vma;
 +	struct hstate *h = hstate_vma(vma);
  	uint64_t orig_pfn, cpu_flags;
- 	bool fault, write_fault;
+ 	unsigned int required_fault;
  	spinlock_t *ptl;
  	pte_t entry;
 +	int ret = 0;
 +
 +	size = huge_page_size(h);
 +	mask = size - 1;
 +	if (range->page_shift != PAGE_SHIFT) {
 +		/* Make sure we are looking at a full page. */
 +		if (start & mask)
 +			return -EINVAL;
 +		if (end < (start + size))
 +			return -EINVAL;
 +		pfn_inc = size >> PAGE_SHIFT;
 +	} else {
 +		pfn_inc = 1;
 +		size = PAGE_SIZE;
 +	}
  
  	ptl = huge_pte_lock(hstate_vma(vma), walk->mm, pte);
  	entry = huge_ptep_get(pte);
@@@ -800,126 -481,58 +885,161 @@@
  	orig_pfn = range->pfns[i];
  	range->pfns[i] = range->values[HMM_PFN_NONE];
  	cpu_flags = pte_to_hmm_pfn_flags(range, entry);
++<<<<<<< HEAD
 +	fault = write_fault = false;
 +	hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags,
 +			   &fault, &write_fault);
 +	if (fault || write_fault) {
 +		ret = -ENOENT;
 +		goto unlock;
++=======
+ 	required_fault = hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags);
+ 	if (required_fault) {
+ 		spin_unlock(ptl);
+ 		return hmm_vma_fault(addr, end, required_fault, walk);
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
  	}
  
 -	pfn = pte_pfn(entry) + ((start & ~hmask) >> PAGE_SHIFT);
 -	for (; addr < end; addr += PAGE_SIZE, i++, pfn++)
 +	pfn = pte_pfn(entry) + ((start & mask) >> range->page_shift);
 +	for (; addr < end; addr += size, i++, pfn += pfn_inc)
  		range->pfns[i] = hmm_device_entry_from_pfn(range, pfn) |
  				 cpu_flags;
  	hmm_vma_walk->last = end;
 +
 +unlock:
  	spin_unlock(ptl);
 -	return 0;
 +
 +	if (ret == -ENOENT)
 +		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
 +
 +	return ret;
 +#else /* CONFIG_HUGETLB_PAGE */
 +	return -EINVAL;
 +#endif
  }
 -#else
 -#define hmm_vma_walk_hugetlb_entry NULL
 -#endif /* CONFIG_HUGETLB_PAGE */
  
 -static int hmm_vma_walk_test(unsigned long start, unsigned long end,
 -			     struct mm_walk *walk)
 +static void hmm_pfns_clear(struct hmm_range *range,
 +			   uint64_t *pfns,
 +			   unsigned long addr,
 +			   unsigned long end)
  {
 -	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 -	struct hmm_range *range = hmm_vma_walk->range;
 -	struct vm_area_struct *vma = walk->vma;
 +	for (; addr < end; addr += PAGE_SIZE, pfns++)
 +		*pfns = range->values[HMM_PFN_NONE];
 +}
 +
 +/*
 + * hmm_range_register() - start tracking change to CPU page table over a range
 + * @range: range
 + * @mm: the mm struct for the range of virtual address
 + * @start: start virtual address (inclusive)
 + * @end: end virtual address (exclusive)
 + * @page_shift: expect page shift for the range
 + * Return: 0 on success, -EFAULT if the address space is no longer valid
 + *
 + * Track updates to the CPU page table see include/linux/hmm.h
 + */
 +int hmm_range_register(struct hmm_range *range,
 +		       struct hmm_mirror *mirror,
 +		       unsigned long start,
 +		       unsigned long end,
 +		       unsigned page_shift)
 +{
 +	unsigned long mask = ((1UL << page_shift) - 1UL);
 +	struct hmm *hmm = mirror->hmm;
 +	unsigned long flags;
 +
 +	range->valid = false;
 +	range->hmm = NULL;
 +
 +	if ((start & mask) || (end & mask))
 +		return -EINVAL;
 +	if (start >= end)
 +		return -EINVAL;
 +
 +	range->page_shift = page_shift;
 +	range->start = start;
 +	range->end = end;
 +
 +	/* Prevent hmm_release() from running while the range is valid */
 +	if (!mmget_not_zero(hmm->mm))
 +		return -EFAULT;
 +
 +	/* Initialize range to track CPU page table updates. */
 +	spin_lock_irqsave(&hmm->ranges_lock, flags);
 +
 +	range->hmm = hmm;
 +	kref_get(&hmm->kref);
 +	list_add(&range->list, &hmm->ranges);
  
+ 	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP | VM_MIXEDMAP)) &&
+ 	    vma->vm_flags & VM_READ)
+ 		return 0;
+ 
  	/*
++<<<<<<< HEAD
 +	 * If there are any concurrent notifiers we have to wait for them for
 +	 * the range to be valid (see hmm_range_wait_until_valid()).
 +	 */
 +	if (!hmm->notifiers)
 +		range->valid = true;
 +	spin_unlock_irqrestore(&hmm->ranges_lock, flags);
 +
 +	return 0;
++=======
+ 	 * vma ranges that don't have struct page backing them or map I/O
+ 	 * devices directly cannot be handled by hmm_range_fault().
+ 	 *
+ 	 * If the vma does not allow read access, then assume that it does not
+ 	 * allow write access either. HMM does not support architectures that
+ 	 * allow write without read.
+ 	 *
+ 	 * If a fault is requested for an unsupported range then it is a hard
+ 	 * failure.
+ 	 */
+ 	if (hmm_range_need_fault(hmm_vma_walk,
+ 				 range->pfns +
+ 					 ((start - range->start) >> PAGE_SHIFT),
+ 				 (end - start) >> PAGE_SHIFT, 0))
+ 		return -EFAULT;
+ 
+ 	hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
+ 	hmm_vma_walk->last = end;
+ 
+ 	/* Skip this vma and continue processing the next vma. */
+ 	return 1;
++>>>>>>> a3eb13c1579b (mm/hmm: return the fault type from hmm_pte_need_fault())
 +}
 +EXPORT_SYMBOL(hmm_range_register);
 +
 +/*
 + * hmm_range_unregister() - stop tracking change to CPU page table over a range
 + * @range: range
 + *
 + * Range struct is used to track updates to the CPU page table after a call to
 + * hmm_range_register(). See include/linux/hmm.h for how to use it.
 + */
 +void hmm_range_unregister(struct hmm_range *range)
 +{
 +	struct hmm *hmm = range->hmm;
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&hmm->ranges_lock, flags);
 +	list_del_init(&range->list);
 +	spin_unlock_irqrestore(&hmm->ranges_lock, flags);
 +
 +	/* Drop reference taken by hmm_range_register() */
 +	mmput(hmm->mm);
 +	hmm_put(hmm);
 +
 +	/*
 +	 * The range is now invalid and the ref on the hmm is dropped, so
 +	 * poison the pointer.  Leave other fields in place, for the caller's
 +	 * use.
 +	 */
 +	range->valid = false;
 +	memset(&range->hmm, POISON_INUSE, sizeof(range->hmm));
  }
 +EXPORT_SYMBOL(hmm_range_unregister);
  
  static const struct mm_walk_ops hmm_walk_ops = {
  	.pud_entry	= hmm_vma_walk_pud,
* Unmerged path mm/hmm.c
