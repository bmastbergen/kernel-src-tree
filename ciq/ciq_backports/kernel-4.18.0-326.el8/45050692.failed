mm/hmm: simplify hmm_vma_walk_hugetlb_entry()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 45050692dec83a67c0325535aae984f56560e3a9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/45050692.failed

Remove the rather confusing goto label and just handle the fault case
directly in the branch checking for it.

Link: https://lore.kernel.org/r/20200316135310.899364-4-hch@lst.de
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 45050692dec83a67c0325535aae984f56560e3a9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hmm.c
diff --cc mm/hmm.c
index 0031a5d7b75b,d13dedfb4781..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -823,22 -559,7 +823,21 @@@ static int hmm_vma_walk_hugetlb_entry(p
  	bool fault, write_fault;
  	spinlock_t *ptl;
  	pte_t entry;
- 	int ret = 0;
  
 +	size = huge_page_size(h);
 +	mask = size - 1;
 +	if (range->page_shift != PAGE_SHIFT) {
 +		/* Make sure we are looking at a full page. */
 +		if (start & mask)
 +			return -EINVAL;
 +		if (end < (start + size))
 +			return -EINVAL;
 +		pfn_inc = size >> PAGE_SHIFT;
 +	} else {
 +		pfn_inc = 1;
 +		size = PAGE_SIZE;
 +	}
 +
  	ptl = huge_pte_lock(hstate_vma(vma), walk->mm, pte);
  	entry = huge_ptep_get(pte);
  
@@@ -850,122 -571,61 +849,124 @@@
  	hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags,
  			   &fault, &write_fault);
  	if (fault || write_fault) {
- 		ret = -ENOENT;
- 		goto unlock;
+ 		spin_unlock(ptl);
+ 		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
  	}
  
 -	pfn = pte_pfn(entry) + ((start & ~hmask) >> PAGE_SHIFT);
 -	for (; addr < end; addr += PAGE_SIZE, i++, pfn++)
 +	pfn = pte_pfn(entry) + ((start & mask) >> range->page_shift);
 +	for (; addr < end; addr += size, i++, pfn += pfn_inc)
  		range->pfns[i] = hmm_device_entry_from_pfn(range, pfn) |
  				 cpu_flags;
  	hmm_vma_walk->last = end;
- 
- unlock:
  	spin_unlock(ptl);
++<<<<<<< HEAD
 +
 +	if (ret == -ENOENT)
 +		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
 +
 +	return ret;
 +#else /* CONFIG_HUGETLB_PAGE */
 +	return -EINVAL;
 +#endif
++=======
+ 	return 0;
++>>>>>>> 45050692dec8 (mm/hmm: simplify hmm_vma_walk_hugetlb_entry())
  }
 -#else
 -#define hmm_vma_walk_hugetlb_entry NULL
 -#endif /* CONFIG_HUGETLB_PAGE */
  
 -static int hmm_vma_walk_test(unsigned long start, unsigned long end,
 -			     struct mm_walk *walk)
 +static void hmm_pfns_clear(struct hmm_range *range,
 +			   uint64_t *pfns,
 +			   unsigned long addr,
 +			   unsigned long end)
  {
 -	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 -	struct hmm_range *range = hmm_vma_walk->range;
 -	struct vm_area_struct *vma = walk->vma;
 +	for (; addr < end; addr += PAGE_SIZE, pfns++)
 +		*pfns = range->values[HMM_PFN_NONE];
 +}
 +
 +/*
 + * hmm_range_register() - start tracking change to CPU page table over a range
 + * @range: range
 + * @mm: the mm struct for the range of virtual address
 + * @start: start virtual address (inclusive)
 + * @end: end virtual address (exclusive)
 + * @page_shift: expect page shift for the range
 + * Return: 0 on success, -EFAULT if the address space is no longer valid
 + *
 + * Track updates to the CPU page table see include/linux/hmm.h
 + */
 +int hmm_range_register(struct hmm_range *range,
 +		       struct hmm_mirror *mirror,
 +		       unsigned long start,
 +		       unsigned long end,
 +		       unsigned page_shift)
 +{
 +	unsigned long mask = ((1UL << page_shift) - 1UL);
 +	struct hmm *hmm = mirror->hmm;
 +	unsigned long flags;
 +
 +	range->valid = false;
 +	range->hmm = NULL;
 +
 +	if ((start & mask) || (end & mask))
 +		return -EINVAL;
 +	if (start >= end)
 +		return -EINVAL;
 +
 +	range->page_shift = page_shift;
 +	range->start = start;
 +	range->end = end;
 +
 +	/* Prevent hmm_release() from running while the range is valid */
 +	if (!mmget_not_zero(hmm->mm))
 +		return -EFAULT;
 +
 +	/* Initialize range to track CPU page table updates. */
 +	spin_lock_irqsave(&hmm->ranges_lock, flags);
 +
 +	range->hmm = hmm;
 +	kref_get(&hmm->kref);
 +	list_add(&range->list, &hmm->ranges);
  
  	/*
 -	 * Skip vma ranges that don't have struct page backing them or map I/O
 -	 * devices directly.
 -	 *
 -	 * If the vma does not allow read access, then assume that it does not
 -	 * allow write access either. HMM does not support architectures that
 -	 * allow write without read.
 +	 * If there are any concurrent notifiers we have to wait for them for
 +	 * the range to be valid (see hmm_range_wait_until_valid()).
  	 */
 -	if ((vma->vm_flags & (VM_IO | VM_PFNMAP | VM_MIXEDMAP)) ||
 -	    !(vma->vm_flags & VM_READ)) {
 -		bool fault, write_fault;
 +	if (!hmm->notifiers)
 +		range->valid = true;
 +	spin_unlock_irqrestore(&hmm->ranges_lock, flags);
  
 -		/*
 -		 * Check to see if a fault is requested for any page in the
 -		 * range.
 -		 */
 -		hmm_range_need_fault(hmm_vma_walk, range->pfns +
 -					((start - range->start) >> PAGE_SHIFT),
 -					(end - start) >> PAGE_SHIFT,
 -					0, &fault, &write_fault);
 -		if (fault || write_fault)
 -			return -EFAULT;
 +	return 0;
 +}
 +EXPORT_SYMBOL(hmm_range_register);
  
 -		hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
 -		hmm_vma_walk->last = end;
 +/*
 + * hmm_range_unregister() - stop tracking change to CPU page table over a range
 + * @range: range
 + *
 + * Range struct is used to track updates to the CPU page table after a call to
 + * hmm_range_register(). See include/linux/hmm.h for how to use it.
 + */
 +void hmm_range_unregister(struct hmm_range *range)
 +{
 +	struct hmm *hmm = range->hmm;
 +	unsigned long flags;
  
 -		/* Skip this vma and continue processing the next vma. */
 -		return 1;
 -	}
 +	spin_lock_irqsave(&hmm->ranges_lock, flags);
 +	list_del_init(&range->list);
 +	spin_unlock_irqrestore(&hmm->ranges_lock, flags);
  
 -	return 0;
 +	/* Drop reference taken by hmm_range_register() */
 +	mmput(hmm->mm);
 +	hmm_put(hmm);
 +
 +	/*
 +	 * The range is now invalid and the ref on the hmm is dropped, so
 +	 * poison the pointer.  Leave other fields in place, for the caller's
 +	 * use.
 +	 */
 +	range->valid = false;
 +	memset(&range->hmm, POISON_INUSE, sizeof(range->hmm));
  }
 +EXPORT_SYMBOL(hmm_range_unregister);
  
  static const struct mm_walk_ops hmm_walk_ops = {
  	.pud_entry	= hmm_vma_walk_pud,
* Unmerged path mm/hmm.c
