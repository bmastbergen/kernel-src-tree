net/mlx5e: Disable TLS device offload in kdump mode

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Alaa Hleihel <alaa@nvidia.com>
commit 39e8cc6d757af7ee5edf5826102c95e3f5bb374b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/39e8cc6d.failed

Under kdump environment we want to use the smallest possible amount
of resources, that includes setting SQ size to minimum.
However, when running on a device that supports TLS device offload,
then the SQ stop room becomes larger than with non-capable device and
requires increasing the SQ size.

Since TLS device offload is not necessary in kdump mode, disable it to
reduce the memory requirements for capable devices.

With this change, the needed SQ stop room size drops by 33.

	Signed-off-by: Alaa Hleihel <alaa@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 39e8cc6d757af7ee5edf5826102c95e3f5bb374b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en/params.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_rxtx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_stats.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/params.c
index 36381a2ed5a5,150c8e82c738..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
@@@ -186,3 -198,491 +186,494 @@@ int mlx5e_validate_params(struct mlx5e_
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static struct dim_cq_moder mlx5e_get_def_tx_moderation(u8 cq_period_mode)
+ {
+ 	struct dim_cq_moder moder = {};
+ 
+ 	moder.cq_period_mode = cq_period_mode;
+ 	moder.pkts = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS;
+ 	moder.usec = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC;
+ 	if (cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE)
+ 		moder.usec = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC_FROM_CQE;
+ 
+ 	return moder;
+ }
+ 
+ static struct dim_cq_moder mlx5e_get_def_rx_moderation(u8 cq_period_mode)
+ {
+ 	struct dim_cq_moder moder = {};
+ 
+ 	moder.cq_period_mode = cq_period_mode;
+ 	moder.pkts = MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS;
+ 	moder.usec = MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC;
+ 	if (cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE)
+ 		moder.usec = MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC_FROM_CQE;
+ 
+ 	return moder;
+ }
+ 
+ static u8 mlx5_to_net_dim_cq_period_mode(u8 cq_period_mode)
+ {
+ 	return cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE ?
+ 		DIM_CQ_PERIOD_MODE_START_FROM_CQE :
+ 		DIM_CQ_PERIOD_MODE_START_FROM_EQE;
+ }
+ 
+ void mlx5e_reset_tx_moderation(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	if (params->tx_dim_enabled) {
+ 		u8 dim_period_mode = mlx5_to_net_dim_cq_period_mode(cq_period_mode);
+ 
+ 		params->tx_cq_moderation = net_dim_get_def_tx_moderation(dim_period_mode);
+ 	} else {
+ 		params->tx_cq_moderation = mlx5e_get_def_tx_moderation(cq_period_mode);
+ 	}
+ }
+ 
+ void mlx5e_reset_rx_moderation(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	if (params->rx_dim_enabled) {
+ 		u8 dim_period_mode = mlx5_to_net_dim_cq_period_mode(cq_period_mode);
+ 
+ 		params->rx_cq_moderation = net_dim_get_def_rx_moderation(dim_period_mode);
+ 	} else {
+ 		params->rx_cq_moderation = mlx5e_get_def_rx_moderation(cq_period_mode);
+ 	}
+ }
+ 
+ void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	mlx5e_reset_tx_moderation(params, cq_period_mode);
+ 	MLX5E_SET_PFLAG(params, MLX5E_PFLAG_TX_CQE_BASED_MODER,
+ 			params->tx_cq_moderation.cq_period_mode ==
+ 				MLX5_CQ_PERIOD_MODE_START_FROM_CQE);
+ }
+ 
+ void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	mlx5e_reset_rx_moderation(params, cq_period_mode);
+ 	MLX5E_SET_PFLAG(params, MLX5E_PFLAG_RX_CQE_BASED_MODER,
+ 			params->rx_cq_moderation.cq_period_mode ==
+ 				MLX5_CQ_PERIOD_MODE_START_FROM_CQE);
+ }
+ 
+ bool slow_pci_heuristic(struct mlx5_core_dev *mdev)
+ {
+ 	u32 link_speed = 0;
+ 	u32 pci_bw = 0;
+ 
+ 	mlx5e_port_max_linkspeed(mdev, &link_speed);
+ 	pci_bw = pcie_bandwidth_available(mdev->pdev, NULL, NULL, NULL);
+ 	mlx5_core_dbg_once(mdev, "Max link speed = %d, PCI BW = %d\n",
+ 			   link_speed, pci_bw);
+ 
+ #define MLX5E_SLOW_PCI_RATIO (2)
+ 
+ 	return link_speed && pci_bw &&
+ 		link_speed > MLX5E_SLOW_PCI_RATIO * pci_bw;
+ }
+ 
+ bool mlx5e_striding_rq_possible(struct mlx5_core_dev *mdev,
+ 				struct mlx5e_params *params)
+ {
+ 	if (!mlx5e_check_fragmented_striding_rq_cap(mdev))
+ 		return false;
+ 
+ 	if (mlx5_fpga_is_ipsec_device(mdev))
+ 		return false;
+ 
+ 	if (params->xdp_prog) {
+ 		/* XSK params are not considered here. If striding RQ is in use,
+ 		 * and an XSK is being opened, mlx5e_rx_mpwqe_is_linear_skb will
+ 		 * be called with the known XSK params.
+ 		 */
+ 		if (!mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL))
+ 			return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
+ 			       struct mlx5e_params *params)
+ {
+ 	params->log_rq_mtu_frames = is_kdump_kernel() ?
+ 		MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE :
+ 		MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
+ 
+ 	mlx5_core_info(mdev, "MLX5E: StrdRq(%d) RqSz(%ld) StrdSz(%ld) RxCqeCmprss(%d)\n",
+ 		       params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ,
+ 		       params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ ?
+ 		       BIT(mlx5e_mpwqe_get_log_rq_size(params, NULL)) :
+ 		       BIT(params->log_rq_mtu_frames),
+ 		       BIT(mlx5e_mpwqe_get_log_stride_size(mdev, params, NULL)),
+ 		       MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS));
+ }
+ 
+ void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
+ {
+ 	params->rq_wq_type = mlx5e_striding_rq_possible(mdev, params) &&
+ 		MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_STRIDING_RQ) ?
+ 		MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ :
+ 		MLX5_WQ_TYPE_CYCLIC;
+ }
+ 
+ void mlx5e_build_rq_params(struct mlx5_core_dev *mdev,
+ 			   struct mlx5e_params *params)
+ {
+ 	/* Prefer Striding RQ, unless any of the following holds:
+ 	 * - Striding RQ configuration is not possible/supported.
+ 	 * - Slow PCI heuristic.
+ 	 * - Legacy RQ would use linear SKB while Striding RQ would use non-linear.
+ 	 *
+ 	 * No XSK params: checking the availability of striding RQ in general.
+ 	 */
+ 	if (!slow_pci_heuristic(mdev) &&
+ 	    mlx5e_striding_rq_possible(mdev, params) &&
+ 	    (mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL) ||
+ 	     !mlx5e_rx_is_linear_skb(params, NULL)))
+ 		MLX5E_SET_PFLAG(params, MLX5E_PFLAG_RX_STRIDING_RQ, true);
+ 	mlx5e_set_rq_type(mdev, params);
+ 	mlx5e_init_rq_type_params(mdev, params);
+ }
+ 
+ /* Build queue parameters */
+ 
+ void mlx5e_build_create_cq_param(struct mlx5e_create_cq_param *ccp, struct mlx5e_channel *c)
+ {
+ 	*ccp = (struct mlx5e_create_cq_param) {
+ 		.napi = &c->napi,
+ 		.ch_stats = c->stats,
+ 		.node = cpu_to_node(c->cpu),
+ 		.ix = c->ix,
+ 	};
+ }
+ 
+ #define DEFAULT_FRAG_SIZE (2048)
+ 
+ static void mlx5e_build_rq_frags_info(struct mlx5_core_dev *mdev,
+ 				      struct mlx5e_params *params,
+ 				      struct mlx5e_xsk_param *xsk,
+ 				      struct mlx5e_rq_frags_info *info)
+ {
+ 	u32 byte_count = MLX5E_SW2HW_MTU(params, params->sw_mtu);
+ 	int frag_size_max = DEFAULT_FRAG_SIZE;
+ 	u32 buf_size = 0;
+ 	int i;
+ 
+ 	if (mlx5_fpga_is_ipsec_device(mdev))
+ 		byte_count += MLX5E_METADATA_ETHER_LEN;
+ 
+ 	if (mlx5e_rx_is_linear_skb(params, xsk)) {
+ 		int frag_stride;
+ 
+ 		frag_stride = mlx5e_rx_get_linear_frag_sz(params, xsk);
+ 		frag_stride = roundup_pow_of_two(frag_stride);
+ 
+ 		info->arr[0].frag_size = byte_count;
+ 		info->arr[0].frag_stride = frag_stride;
+ 		info->num_frags = 1;
+ 		info->wqe_bulk = PAGE_SIZE / frag_stride;
+ 		goto out;
+ 	}
+ 
+ 	if (byte_count > PAGE_SIZE +
+ 	    (MLX5E_MAX_RX_FRAGS - 1) * frag_size_max)
+ 		frag_size_max = PAGE_SIZE;
+ 
+ 	i = 0;
+ 	while (buf_size < byte_count) {
+ 		int frag_size = byte_count - buf_size;
+ 
+ 		if (i < MLX5E_MAX_RX_FRAGS - 1)
+ 			frag_size = min(frag_size, frag_size_max);
+ 
+ 		info->arr[i].frag_size = frag_size;
+ 		info->arr[i].frag_stride = roundup_pow_of_two(frag_size);
+ 
+ 		buf_size += frag_size;
+ 		i++;
+ 	}
+ 	info->num_frags = i;
+ 	/* number of different wqes sharing a page */
+ 	info->wqe_bulk = 1 + (info->num_frags % 2);
+ 
+ out:
+ 	info->wqe_bulk = max_t(u8, info->wqe_bulk, 8);
+ 	info->log_num_frags = order_base_2(info->num_frags);
+ }
+ 
+ static u8 mlx5e_get_rqwq_log_stride(u8 wq_type, int ndsegs)
+ {
+ 	int sz = sizeof(struct mlx5_wqe_data_seg) * ndsegs;
+ 
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		sz += sizeof(struct mlx5e_rx_wqe_ll);
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		sz += sizeof(struct mlx5e_rx_wqe_cyc);
+ 	}
+ 
+ 	return order_base_2(sz);
+ }
+ 
+ static void mlx5e_build_common_cq_param(struct mlx5_core_dev *mdev,
+ 					struct mlx5e_cq_param *param)
+ {
+ 	void *cqc = param->cqc;
+ 
+ 	MLX5_SET(cqc, cqc, uar_page, mdev->priv.uar->index);
+ 	if (MLX5_CAP_GEN(mdev, cqe_128_always) && cache_line_size() >= 128)
+ 		MLX5_SET(cqc, cqc, cqe_sz, CQE_STRIDE_128_PAD);
+ }
+ 
+ static void mlx5e_build_rx_cq_param(struct mlx5_core_dev *mdev,
+ 				    struct mlx5e_params *params,
+ 				    struct mlx5e_xsk_param *xsk,
+ 				    struct mlx5e_cq_param *param)
+ {
+ 	bool hw_stridx = false;
+ 	void *cqc = param->cqc;
+ 	u8 log_cq_size;
+ 
+ 	switch (params->rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		log_cq_size = mlx5e_mpwqe_get_log_rq_size(params, xsk) +
+ 			mlx5e_mpwqe_get_log_num_strides(mdev, params, xsk);
+ 		hw_stridx = MLX5_CAP_GEN(mdev, mini_cqe_resp_stride_index);
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		log_cq_size = params->log_rq_mtu_frames;
+ 	}
+ 
+ 	MLX5_SET(cqc, cqc, log_cq_size, log_cq_size);
+ 	if (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS)) {
+ 		MLX5_SET(cqc, cqc, mini_cqe_res_format, hw_stridx ?
+ 			 MLX5_CQE_FORMAT_CSUM_STRIDX : MLX5_CQE_FORMAT_CSUM);
+ 		MLX5_SET(cqc, cqc, cqe_comp_en, 1);
+ 	}
+ 
+ 	mlx5e_build_common_cq_param(mdev, param);
+ 	param->cq_period_mode = params->rx_cq_moderation.cq_period_mode;
+ }
+ 
+ int mlx5e_build_rq_param(struct mlx5_core_dev *mdev,
+ 			 struct mlx5e_params *params,
+ 			 struct mlx5e_xsk_param *xsk,
+ 			 u16 q_counter,
+ 			 struct mlx5e_rq_param *param)
+ {
+ 	void *rqc = param->rqc;
+ 	void *wq = MLX5_ADDR_OF(rqc, rqc, wq);
+ 	int ndsegs = 1;
+ 
+ 	switch (params->rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ: {
+ 		u8 log_wqe_num_of_strides = mlx5e_mpwqe_get_log_num_strides(mdev, params, xsk);
+ 		u8 log_wqe_stride_size = mlx5e_mpwqe_get_log_stride_size(mdev, params, xsk);
+ 
+ 		if (!mlx5e_verify_rx_mpwqe_strides(mdev, log_wqe_stride_size,
+ 						   log_wqe_num_of_strides)) {
+ 			mlx5_core_err(mdev,
+ 				      "Bad RX MPWQE params: log_stride_size %u, log_num_strides %u\n",
+ 				      log_wqe_stride_size, log_wqe_num_of_strides);
+ 			return -EINVAL;
+ 		}
+ 
+ 		MLX5_SET(wq, wq, log_wqe_num_of_strides,
+ 			 log_wqe_num_of_strides - MLX5_MPWQE_LOG_NUM_STRIDES_BASE);
+ 		MLX5_SET(wq, wq, log_wqe_stride_size,
+ 			 log_wqe_stride_size - MLX5_MPWQE_LOG_STRIDE_SZ_BASE);
+ 		MLX5_SET(wq, wq, log_wq_sz, mlx5e_mpwqe_get_log_rq_size(params, xsk));
+ 		break;
+ 	}
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		MLX5_SET(wq, wq, log_wq_sz, params->log_rq_mtu_frames);
+ 		mlx5e_build_rq_frags_info(mdev, params, xsk, &param->frags_info);
+ 		ndsegs = param->frags_info.num_frags;
+ 	}
+ 
+ 	MLX5_SET(wq, wq, wq_type,          params->rq_wq_type);
+ 	MLX5_SET(wq, wq, end_padding_mode, MLX5_WQ_END_PAD_MODE_ALIGN);
+ 	MLX5_SET(wq, wq, log_wq_stride,
+ 		 mlx5e_get_rqwq_log_stride(params->rq_wq_type, ndsegs));
+ 	MLX5_SET(wq, wq, pd,               mdev->mlx5e_res.hw_objs.pdn);
+ 	MLX5_SET(rqc, rqc, counter_set_id, q_counter);
+ 	MLX5_SET(rqc, rqc, vsd,            params->vlan_strip_disable);
+ 	MLX5_SET(rqc, rqc, scatter_fcs,    params->scatter_fcs_en);
+ 
+ 	param->wq.buf_numa_node = dev_to_node(mlx5_core_dma_dev(mdev));
+ 	mlx5e_build_rx_cq_param(mdev, params, xsk, &param->cqp);
+ 
+ 	return 0;
+ }
+ 
+ void mlx5e_build_drop_rq_param(struct mlx5_core_dev *mdev,
+ 			       u16 q_counter,
+ 			       struct mlx5e_rq_param *param)
+ {
+ 	void *rqc = param->rqc;
+ 	void *wq = MLX5_ADDR_OF(rqc, rqc, wq);
+ 
+ 	MLX5_SET(wq, wq, wq_type, MLX5_WQ_TYPE_CYCLIC);
+ 	MLX5_SET(wq, wq, log_wq_stride,
+ 		 mlx5e_get_rqwq_log_stride(MLX5_WQ_TYPE_CYCLIC, 1));
+ 	MLX5_SET(rqc, rqc, counter_set_id, q_counter);
+ 
+ 	param->wq.buf_numa_node = dev_to_node(mlx5_core_dma_dev(mdev));
+ }
+ 
+ void mlx5e_build_tx_cq_param(struct mlx5_core_dev *mdev,
+ 			     struct mlx5e_params *params,
+ 			     struct mlx5e_cq_param *param)
+ {
+ 	void *cqc = param->cqc;
+ 
+ 	MLX5_SET(cqc, cqc, log_cq_size, params->log_sq_size);
+ 
+ 	mlx5e_build_common_cq_param(mdev, param);
+ 	param->cq_period_mode = params->tx_cq_moderation.cq_period_mode;
+ }
+ 
+ void mlx5e_build_sq_param_common(struct mlx5_core_dev *mdev,
+ 				 struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	MLX5_SET(wq, wq, log_wq_stride, ilog2(MLX5_SEND_WQE_BB));
+ 	MLX5_SET(wq, wq, pd,            mdev->mlx5e_res.hw_objs.pdn);
+ 
+ 	param->wq.buf_numa_node = dev_to_node(mlx5_core_dma_dev(mdev));
+ }
+ 
+ void mlx5e_build_sq_param(struct mlx5_core_dev *mdev,
+ 			  struct mlx5e_params *params,
+ 			  struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 	bool allow_swp;
+ 
+ 	allow_swp = mlx5_geneve_tx_allowed(mdev) ||
+ 		    !!MLX5_IPSEC_DEV(mdev);
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
+ 	MLX5_SET(sqc, sqc, allow_swp, allow_swp);
+ 	param->is_mpw = MLX5E_GET_PFLAG(params, MLX5E_PFLAG_SKB_TX_MPWQE);
+ 	param->stop_room = mlx5e_calc_sq_stop_room(mdev, params);
+ 	mlx5e_build_tx_cq_param(mdev, params, &param->cqp);
+ }
+ 
+ static void mlx5e_build_ico_cq_param(struct mlx5_core_dev *mdev,
+ 				     u8 log_wq_size,
+ 				     struct mlx5e_cq_param *param)
+ {
+ 	void *cqc = param->cqc;
+ 
+ 	MLX5_SET(cqc, cqc, log_cq_size, log_wq_size);
+ 
+ 	mlx5e_build_common_cq_param(mdev, param);
+ 
+ 	param->cq_period_mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;
+ }
+ 
+ static u8 mlx5e_get_rq_log_wq_sz(void *rqc)
+ {
+ 	void *wq = MLX5_ADDR_OF(rqc, rqc, wq);
+ 
+ 	return MLX5_GET(wq, wq, log_wq_sz);
+ }
+ 
+ static u8 mlx5e_build_icosq_log_wq_sz(struct mlx5e_params *params,
+ 				      struct mlx5e_rq_param *rqp)
+ {
+ 	switch (params->rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return max_t(u8, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE,
+ 			     order_base_2(MLX5E_UMR_WQEBBS) +
+ 			     mlx5e_get_rq_log_wq_sz(rqp->rqc));
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
+ 	}
+ }
+ 
+ static u8 mlx5e_build_async_icosq_log_wq_sz(struct mlx5_core_dev *mdev)
+ {
+ 	if (mlx5e_accel_is_ktls_rx(mdev))
+ 		return MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
+ 
+ 	return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
+ }
+ 
+ static void mlx5e_build_icosq_param(struct mlx5_core_dev *mdev,
+ 				    u8 log_wq_size,
+ 				    struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 
+ 	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
+ 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(mdev, reg_umr_sq));
+ 	mlx5e_build_ico_cq_param(mdev, log_wq_size, &param->cqp);
+ }
+ 
+ static void mlx5e_build_async_icosq_param(struct mlx5_core_dev *mdev,
+ 					  u8 log_wq_size,
+ 					  struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 	param->stop_room = mlx5e_stop_room_for_wqe(1); /* for XSK NOP */
+ 	param->is_tls = mlx5e_accel_is_ktls_rx(mdev);
+ 	if (param->is_tls)
+ 		param->stop_room += mlx5e_stop_room_for_wqe(1); /* for TLS RX resync NOP */
+ 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(mdev, reg_umr_sq));
+ 	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
+ 	mlx5e_build_ico_cq_param(mdev, log_wq_size, &param->cqp);
+ }
+ 
+ void mlx5e_build_xdpsq_param(struct mlx5_core_dev *mdev,
+ 			     struct mlx5e_params *params,
+ 			     struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
+ 	param->is_mpw = MLX5E_GET_PFLAG(params, MLX5E_PFLAG_XDP_TX_MPWQE);
+ 	mlx5e_build_tx_cq_param(mdev, params, &param->cqp);
+ }
+ 
+ int mlx5e_build_channel_param(struct mlx5_core_dev *mdev,
+ 			      struct mlx5e_params *params,
+ 			      u16 q_counter,
+ 			      struct mlx5e_channel_param *cparam)
+ {
+ 	u8 icosq_log_wq_sz, async_icosq_log_wq_sz;
+ 	int err;
+ 
+ 	err = mlx5e_build_rq_param(mdev, params, NULL, q_counter, &cparam->rq);
+ 	if (err)
+ 		return err;
+ 
+ 	icosq_log_wq_sz = mlx5e_build_icosq_log_wq_sz(params, &cparam->rq);
+ 	async_icosq_log_wq_sz = mlx5e_build_async_icosq_log_wq_sz(mdev);
+ 
+ 	mlx5e_build_sq_param(mdev, params, &cparam->txq_sq);
+ 	mlx5e_build_xdpsq_param(mdev, params, &cparam->xdp_sq);
+ 	mlx5e_build_icosq_param(mdev, icosq_log_wq_sz, &cparam->icosq);
+ 	mlx5e_build_async_icosq_param(mdev, async_icosq_log_wq_sz, &cparam->async_icosq);
+ 
+ 	return 0;
+ }
++>>>>>>> 39e8cc6d757a (net/mlx5e: Disable TLS device offload in kdump mode)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
index baa58b62e8df,5833deb2354c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
@@@ -12,6 -12,28 +12,31 @@@ void mlx5e_ktls_build_netdev(struct mlx
  int mlx5e_ktls_init_rx(struct mlx5e_priv *priv);
  void mlx5e_ktls_cleanup_rx(struct mlx5e_priv *priv);
  int mlx5e_ktls_set_feature_rx(struct net_device *netdev, bool enable);
++<<<<<<< HEAD
++=======
+ struct mlx5e_ktls_resync_resp *
+ mlx5e_ktls_rx_resync_create_resp_list(void);
+ void mlx5e_ktls_rx_resync_destroy_resp_list(struct mlx5e_ktls_resync_resp *resp_list);
+ 
+ static inline bool mlx5e_accel_is_ktls_tx(struct mlx5_core_dev *mdev)
+ {
+ 	return !is_kdump_kernel() &&
+ 		mlx5_accel_is_ktls_tx(mdev);
+ }
+ 
+ static inline bool mlx5e_accel_is_ktls_rx(struct mlx5_core_dev *mdev)
+ {
+ 	return !is_kdump_kernel() &&
+ 		mlx5_accel_is_ktls_rx(mdev);
+ }
+ 
+ static inline bool mlx5e_accel_is_ktls_device(struct mlx5_core_dev *mdev)
+ {
+ 	return !is_kdump_kernel() &&
+ 		mlx5_accel_is_ktls_device(mdev);
+ }
+ 
++>>>>>>> 39e8cc6d757a (net/mlx5e: Disable TLS device offload in kdump mode)
  #else
  
  static inline void mlx5e_ktls_build_netdev(struct mlx5e_priv *priv)
@@@ -33,6 -55,19 +58,22 @@@ static inline int mlx5e_ktls_set_featur
  	return -EOPNOTSUPP;
  }
  
++<<<<<<< HEAD
++=======
+ static inline struct mlx5e_ktls_resync_resp *
+ mlx5e_ktls_rx_resync_create_resp_list(void)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void
+ mlx5e_ktls_rx_resync_destroy_resp_list(struct mlx5e_ktls_resync_resp *resp_list) {}
+ 
+ static inline bool mlx5e_accel_is_ktls_tx(struct mlx5_core_dev *mdev) { return false; }
+ static inline bool mlx5e_accel_is_ktls_rx(struct mlx5_core_dev *mdev) { return false; }
+ static inline bool mlx5e_accel_is_ktls_device(struct mlx5_core_dev *mdev) { return false; }
+ 
++>>>>>>> 39e8cc6d757a (net/mlx5e: Disable TLS device offload in kdump mode)
  #endif
  
  #endif /* __MLX5E_TLS_H__ */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_rxtx.c
index 119cabf4a428,7a700f913582..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_rxtx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_rxtx.c
@@@ -273,7 -273,7 +273,11 @@@ bool mlx5e_tls_handle_tx_skb(struct net
  	if (WARN_ON_ONCE(tls_ctx->netdev != netdev))
  		goto err_out;
  
++<<<<<<< HEAD
 +	if (mlx5_accel_is_ktls_tx(sq->channel->mdev))
++=======
+ 	if (mlx5e_accel_is_ktls_tx(sq->mdev))
++>>>>>>> 39e8cc6d757a (net/mlx5e: Disable TLS device offload in kdump mode)
  		return mlx5e_ktls_handle_tx_skb(tls_ctx, sq, skb, datalen, state);
  
  	/* FPGA */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_stats.c
index b949b9a7538b,ffc84f9b41b0..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_stats.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_stats.c
@@@ -48,19 -48,28 +48,33 @@@ static const struct counter_desc mlx5e_
  #define MLX5E_READ_CTR_ATOMIC64(ptr, dsc, i) \
  	atomic64_read((atomic64_t *)((char *)(ptr) + (dsc)[i].offset))
  
 -static const struct counter_desc *get_tls_atomic_stats(struct mlx5e_priv *priv)
 +#define NUM_TLS_SW_COUNTERS ARRAY_SIZE(mlx5e_tls_sw_stats_desc)
 +
 +static bool is_tls_atomic_stats(struct mlx5e_priv *priv)
  {
++<<<<<<< HEAD
 +	return priv->tls && !mlx5_accel_is_ktls_device(priv->mdev);
++=======
+ 	if (!priv->tls)
+ 		return NULL;
+ 	if (mlx5e_accel_is_ktls_device(priv->mdev))
+ 		return mlx5e_ktls_sw_stats_desc;
+ 	return mlx5e_tls_sw_stats_desc;
++>>>>>>> 39e8cc6d757a (net/mlx5e: Disable TLS device offload in kdump mode)
  }
  
  int mlx5e_tls_get_count(struct mlx5e_priv *priv)
  {
 -	if (!priv->tls)
 +	if (!is_tls_atomic_stats(priv))
  		return 0;
++<<<<<<< HEAD
 +
 +	return NUM_TLS_SW_COUNTERS;
++=======
+ 	if (mlx5e_accel_is_ktls_device(priv->mdev))
+ 		return ARRAY_SIZE(mlx5e_ktls_sw_stats_desc);
+ 	return ARRAY_SIZE(mlx5e_tls_sw_stats_desc);
++>>>>>>> 39e8cc6d757a (net/mlx5e: Disable TLS device offload in kdump mode)
  }
  
  int mlx5e_tls_get_strings(struct mlx5e_priv *priv, uint8_t *data)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 237d6c6a22d1,0d59639f8ac0..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -873,14 -857,14 +873,19 @@@ int mlx5e_open_rq(struct mlx5e_channel 
  	if (err)
  		goto err_destroy_rq;
  
++<<<<<<< HEAD
 +	if (mlx5e_is_tls_on(c->priv) && !mlx5_accel_is_ktls_device(c->mdev))
 +		__set_bit(MLX5E_RQ_STATE_FPGA_TLS, &c->rq.state); /* must be FPGA */
++=======
+ 	if (mlx5e_is_tls_on(rq->priv) && !mlx5e_accel_is_ktls_device(mdev))
+ 		__set_bit(MLX5E_RQ_STATE_FPGA_TLS, &rq->state); /* must be FPGA */
++>>>>>>> 39e8cc6d757a (net/mlx5e: Disable TLS device offload in kdump mode)
  
 -	if (MLX5_CAP_ETH(mdev, cqe_checksum_full))
 -		__set_bit(MLX5E_RQ_STATE_CSUM_FULL, &rq->state);
 +	if (MLX5_CAP_ETH(c->mdev, cqe_checksum_full))
 +		__set_bit(MLX5E_RQ_STATE_CSUM_FULL, &c->rq.state);
  
  	if (params->rx_dim_enabled)
 -		__set_bit(MLX5E_RQ_STATE_AM, &rq->state);
 +		__set_bit(MLX5E_RQ_STATE_AM, &c->rq.state);
  
  	/* We disable csum_complete when XDP is enabled since
  	 * XDP programs might manipulate packets which will render
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/params.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.c b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.c
index 95293ee0d38d..d93aadbf10da 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.c
@@ -59,12 +59,15 @@ void mlx5e_ktls_build_netdev(struct mlx5e_priv *priv)
 	struct net_device *netdev = priv->netdev;
 	struct mlx5_core_dev *mdev = priv->mdev;
 
-	if (mlx5_accel_is_ktls_tx(mdev)) {
+	if (!mlx5e_accel_is_ktls_tx(mdev) && !mlx5e_accel_is_ktls_rx(mdev))
+		return;
+
+	if (mlx5e_accel_is_ktls_tx(mdev)) {
 		netdev->hw_features |= NETIF_F_HW_TLS_TX;
 		netdev->features    |= NETIF_F_HW_TLS_TX;
 	}
 
-	if (mlx5_accel_is_ktls_rx(mdev))
+	if (mlx5e_accel_is_ktls_rx(mdev))
 		netdev->hw_features |= NETIF_F_HW_TLS_RX;
 
 	netdev->tlsdev_ops = &mlx5e_ktls_ops;
@@ -89,7 +92,7 @@ int mlx5e_ktls_init_rx(struct mlx5e_priv *priv)
 {
 	int err;
 
-	if (!mlx5_accel_is_ktls_rx(priv->mdev))
+	if (!mlx5e_accel_is_ktls_rx(priv->mdev))
 		return 0;
 
 	priv->tls->rx_wq = create_singlethread_workqueue("mlx5e_tls_rx");
@@ -109,7 +112,7 @@ int mlx5e_ktls_init_rx(struct mlx5e_priv *priv)
 
 void mlx5e_ktls_cleanup_rx(struct mlx5e_priv *priv)
 {
-	if (!mlx5_accel_is_ktls_rx(priv->mdev))
+	if (!mlx5e_accel_is_ktls_rx(priv->mdev))
 		return;
 
 	if (priv->netdev->features & NETIF_F_HW_TLS_RX)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
index d16def68ecff..af9332ae341a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
@@ -22,10 +22,13 @@ mlx5e_ktls_dumps_num_wqes(struct mlx5e_params *params, unsigned int nfrags,
 	return nfrags + DIV_ROUND_UP(sync_len, MLX5E_SW2HW_MTU(params, params->sw_mtu));
 }
 
-u16 mlx5e_ktls_get_stop_room(struct mlx5e_params *params)
+u16 mlx5e_ktls_get_stop_room(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
 {
 	u16 num_dumps, stop_room = 0;
 
+	if (!mlx5e_accel_is_ktls_tx(mdev))
+		return 0;
+
 	num_dumps = mlx5e_ktls_dumps_num_wqes(params, MAX_SKB_FRAGS, TLS_MAX_PAYLOAD_SIZE);
 
 	stop_room += mlx5e_stop_room_for_wqe(MLX5E_TLS_SET_STATIC_PARAMS_WQEBBS);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h
index ee04e916fa21..e8596dfc4f0d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h
@@ -14,7 +14,7 @@ struct mlx5e_accel_tx_tls_state {
 	u32 tls_tisn;
 };
 
-u16 mlx5e_ktls_get_stop_room(struct mlx5e_params *params);
+u16 mlx5e_ktls_get_stop_room(struct mlx5_core_dev *mdev, struct mlx5e_params *params);
 
 bool mlx5e_ktls_handle_tx_skb(struct tls_context *tls_ctx, struct mlx5e_txqsq *sq,
 			      struct sk_buff *skb, int datalen,
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.c b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.c
index d6b21b899dbc..b8fc863aa68d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.c
@@ -192,13 +192,13 @@ void mlx5e_tls_build_netdev(struct mlx5e_priv *priv)
 	struct net_device *netdev = priv->netdev;
 	u32 caps;
 
-	if (mlx5_accel_is_ktls_device(priv->mdev)) {
+	if (mlx5e_accel_is_ktls_device(priv->mdev)) {
 		mlx5e_ktls_build_netdev(priv);
 		return;
 	}
 
 	/* FPGA */
-	if (!mlx5_accel_is_tls_device(priv->mdev))
+	if (!mlx5e_accel_is_tls_device(priv->mdev))
 		return;
 
 	caps = mlx5_accel_tls_device_caps(priv->mdev);
@@ -224,7 +224,7 @@ int mlx5e_tls_init(struct mlx5e_priv *priv)
 {
 	struct mlx5e_tls *tls;
 
-	if (!mlx5_accel_is_tls_device(priv->mdev))
+	if (!mlx5e_accel_is_tls_device(priv->mdev))
 		return 0;
 
 	tls = kzalloc(sizeof(*tls), GFP_KERNEL);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.h
index bd270a85c804..84e21660cd08 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls.h
@@ -100,11 +100,18 @@ int mlx5e_tls_get_count(struct mlx5e_priv *priv);
 int mlx5e_tls_get_strings(struct mlx5e_priv *priv, uint8_t *data);
 int mlx5e_tls_get_stats(struct mlx5e_priv *priv, u64 *data);
 
+static inline bool mlx5e_accel_is_tls_device(struct mlx5_core_dev *mdev)
+{
+	return !is_kdump_kernel() &&
+		mlx5_accel_is_tls_device(mdev);
+}
+
 #else
 
 static inline void mlx5e_tls_build_netdev(struct mlx5e_priv *priv)
 {
-	if (mlx5_accel_is_ktls_device(priv->mdev))
+	if (!is_kdump_kernel() &&
+	    mlx5_accel_is_ktls_device(priv->mdev))
 		mlx5e_ktls_build_netdev(priv);
 }
 
@@ -114,6 +121,7 @@ static inline void mlx5e_tls_cleanup(struct mlx5e_priv *priv) { }
 static inline int mlx5e_tls_get_count(struct mlx5e_priv *priv) { return 0; }
 static inline int mlx5e_tls_get_strings(struct mlx5e_priv *priv, uint8_t *data) { return 0; }
 static inline int mlx5e_tls_get_stats(struct mlx5e_priv *priv, u64 *data) { return 0; }
+static inline bool mlx5e_accel_is_tls_device(struct mlx5_core_dev *mdev) { return false; }
 
 #endif
 
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_rxtx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/tls_stats.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
