dma-iommu: refactor iommu_dma_alloc_remap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 8230ce9a4e206fa1be17d66245f87cae2935d7d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/8230ce9a.failed

Split out a new helper that only allocates a sg_table worth of
memory without mapping it into contiguous kernel address space.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Tomasz Figa <tfiga@chromium.org>
	Tested-by: Ricardo Ribalda <ribalda@chromium.org>
(cherry picked from commit 8230ce9a4e206fa1be17d66245f87cae2935d7d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/dma-iommu.c
diff --cc drivers/iommu/dma-iommu.c
index fe135a263f73,ec1abad156db..000000000000
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@@ -640,36 -650,22 +640,41 @@@ static struct page **__iommu_dma_alloc_
  	return pages;
  }
  
++<<<<<<< HEAD
 +/**
 + * iommu_dma_alloc_remap - Allocate and map a buffer contiguous in IOVA space
 + * @dev: Device to allocate memory for. Must be a real device
 + *	 attached to an iommu_dma_domain
 + * @size: Size of buffer in bytes
 + * @dma_handle: Out argument for allocated DMA handle
 + * @gfp: Allocation flags
 + * @attrs: DMA attributes for this allocation
 + *
 + * If @size is less than PAGE_SIZE, then a full CPU page will be allocated,
++=======
+ /*
+  * If size is less than PAGE_SIZE, then a full CPU page will be allocated,
++>>>>>>> 8230ce9a4e20 (dma-iommu: refactor iommu_dma_alloc_remap)
   * but an IOMMU which supports smaller pages might not map the whole thing.
-  *
-  * Return: Mapped virtual address, or NULL on failure.
   */
++<<<<<<< HEAD
 +static void *iommu_dma_alloc_remap(struct device *dev, size_t size,
 +		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
++=======
+ static struct page **__iommu_dma_alloc_noncontiguous(struct device *dev,
+ 		size_t size, struct sg_table *sgt, gfp_t gfp, pgprot_t prot,
+ 		unsigned long attrs)
++>>>>>>> 8230ce9a4e20 (dma-iommu: refactor iommu_dma_alloc_remap)
  {
  	struct iommu_domain *domain = iommu_get_dma_domain(dev);
  	struct iommu_dma_cookie *cookie = domain->iova_cookie;
  	struct iova_domain *iovad = &cookie->iovad;
  	bool coherent = dev_is_dma_coherent(dev);
  	int ioprot = dma_info_to_prot(DMA_BIDIRECTIONAL, coherent, attrs);
 +	pgprot_t prot = dma_pgprot(dev, PAGE_KERNEL, attrs);
  	unsigned int count, min_size, alloc_sizes = domain->pgsize_bitmap;
  	struct page **pages;
- 	struct sg_table sgt;
  	dma_addr_t iova;
- 	void *vaddr;
- 
- 	*dma_handle = DMA_MAPPING_ERROR;
  
  	if (static_branch_unlikely(&iommu_deferred_attach_enabled) &&
  	    iommu_deferred_attach(dev, domain))
@@@ -711,6 -707,32 +716,32 @@@
  			< size)
  		goto out_free_sg;
  
+ 	sgt->sgl->dma_address = iova;
+ 	return pages;
+ 
+ out_free_sg:
+ 	sg_free_table(sgt);
+ out_free_iova:
 -	iommu_dma_free_iova(cookie, iova, size, NULL);
++	iommu_dma_free_iova(cookie, iova, size);
+ out_free_pages:
+ 	__iommu_dma_free_pages(pages, count);
+ 	return NULL;
+ }
+ 
+ static void *iommu_dma_alloc_remap(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp, pgprot_t prot,
+ 		unsigned long attrs)
+ {
+ 	struct page **pages;
+ 	struct sg_table sgt;
+ 	void *vaddr;
+ 
+ 	pages = __iommu_dma_alloc_noncontiguous(dev, size, &sgt, gfp, prot,
+ 						attrs);
+ 	if (!pages)
+ 		return NULL;
+ 	*dma_handle = sgt.sgl->dma_address;
+ 	sg_free_table(&sgt);
  	vaddr = dma_common_pages_remap(pages, size, prot,
  			__builtin_return_address(0));
  	if (!vaddr)
* Unmerged path drivers/iommu/dma-iommu.c
