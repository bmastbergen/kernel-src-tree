mm/hmm: remove hmm_range vma

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Ralph Campbell <rcampbell@nvidia.com>
commit cc374377a19d2a49d693997b62dc3a6f5fac6d61
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/cc374377.failed

Since hmm_range_fault() doesn't use the struct hmm_range vma field, remove
it.

Link: https://lore.kernel.org/r/20190726005650.2566-8-rcampbell@nvidia.com
	Suggested-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit cc374377a19d2a49d693997b62dc3a6f5fac6d61)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/nouveau/nouveau_svm.c
diff --cc drivers/gpu/drm/nouveau/nouveau_svm.c
index 4f69e4c3dafd,a74530b5a523..000000000000
--- a/drivers/gpu/drm/nouveau/nouveau_svm.c
+++ b/drivers/gpu/drm/nouveau/nouveau_svm.c
@@@ -471,138 -475,45 +471,166 @@@ nouveau_svm_fault_cache(struct nouveau_
  		fault->inst, fault->addr, fault->access);
  }
  
 -static inline bool
 -nouveau_range_done(struct hmm_range *range)
 +struct svm_notifier {
 +	struct mmu_interval_notifier notifier;
 +	struct nouveau_svmm *svmm;
 +};
 +
 +static bool nouveau_svm_range_invalidate(struct mmu_interval_notifier *mni,
 +					 const struct mmu_notifier_range *range,
 +					 unsigned long cur_seq)
  {
 -	bool ret = hmm_range_valid(range);
 +	struct svm_notifier *sn =
 +		container_of(mni, struct svm_notifier, notifier);
  
 -	hmm_range_unregister(range);
 -	return ret;
 +	/*
 +	 * serializes the update to mni->invalidate_seq done by caller and
 +	 * prevents invalidation of the PTE from progressing while HW is being
 +	 * programmed. This is very hacky and only works because the normal
 +	 * notifier that does invalidation is always called after the range
 +	 * notifier.
 +	 */
 +	if (mmu_notifier_range_blockable(range))
 +		mutex_lock(&sn->svmm->mutex);
 +	else if (!mutex_trylock(&sn->svmm->mutex))
 +		return false;
 +	mmu_interval_set_seq(mni, cur_seq);
 +	mutex_unlock(&sn->svmm->mutex);
 +	return true;
  }
  
 -static int
 -nouveau_range_fault(struct hmm_mirror *mirror, struct hmm_range *range)
 +static const struct mmu_interval_notifier_ops nouveau_svm_mni_ops = {
 +	.invalidate = nouveau_svm_range_invalidate,
 +};
 +
 +static void nouveau_hmm_convert_pfn(struct nouveau_drm *drm,
 +				    struct hmm_range *range,
 +				    struct nouveau_pfnmap_args *args)
 +{
 +	struct page *page;
 +
++<<<<<<< HEAD
 +	/*
 +	 * The address prepared here is passed through nvif_object_ioctl()
 +	 * to an eventual DMA map in something like gp100_vmm_pgt_pfn()
 +	 *
 +	 * This is all just encoding the internal hmm representation into a
 +	 * different nouveau internal representation.
 +	 */
 +	if (!(range->hmm_pfns[0] & HMM_PFN_VALID)) {
 +		args->p.phys[0] = 0;
 +		return;
 +	}
 +
 +	page = hmm_pfn_to_page(range->hmm_pfns[0]);
 +	/*
 +	 * Only map compound pages to the GPU if the CPU is also mapping the
 +	 * page as a compound page. Otherwise, the PTE protections might not be
 +	 * consistent (e.g., CPU only maps part of a compound page).
 +	 * Note that the underlying page might still be larger than the
 +	 * CPU mapping (e.g., a PUD sized compound page partially mapped with
 +	 * a PMD sized page table entry).
 +	 */
 +	if (hmm_pfn_to_map_order(range->hmm_pfns[0])) {
 +		unsigned long addr = args->p.addr;
 +
 +		args->p.page = hmm_pfn_to_map_order(range->hmm_pfns[0]) +
 +				PAGE_SHIFT;
 +		args->p.size = 1UL << args->p.page;
 +		args->p.addr &= ~(args->p.size - 1);
 +		page -= (addr - args->p.addr) >> PAGE_SHIFT;
 +	}
 +	if (is_device_private_page(page))
 +		args->p.phys[0] = nouveau_dmem_page_addr(page) |
 +				NVIF_VMM_PFNMAP_V0_V |
 +				NVIF_VMM_PFNMAP_V0_VRAM;
 +	else
 +		args->p.phys[0] = page_to_phys(page) |
 +				NVIF_VMM_PFNMAP_V0_V |
 +				NVIF_VMM_PFNMAP_V0_HOST;
 +	if (range->hmm_pfns[0] & HMM_PFN_WRITE)
 +		args->p.phys[0] |= NVIF_VMM_PFNMAP_V0_W;
 +}
 +
 +static int nouveau_range_fault(struct nouveau_svmm *svmm,
 +			       struct nouveau_drm *drm,
 +			       struct nouveau_pfnmap_args *args, u32 size,
 +			       unsigned long hmm_flags,
 +			       struct svm_notifier *notifier)
  {
 -	long ret;
 +	unsigned long timeout =
 +		jiffies + msecs_to_jiffies(HMM_RANGE_DEFAULT_TIMEOUT);
 +	/* Have HMM fault pages within the fault window to the GPU. */
 +	unsigned long hmm_pfns[1];
 +	struct hmm_range range = {
 +		.notifier = &notifier->notifier,
 +		.start = notifier->notifier.interval_tree.start,
 +		.end = notifier->notifier.interval_tree.last + 1,
 +		.default_flags = hmm_flags,
 +		.hmm_pfns = hmm_pfns,
 +		.dev_private_owner = drm->dev,
 +	};
 +	struct mm_struct *mm = notifier->notifier.mm;
 +	int ret;
  
 +	while (true) {
 +		if (time_after(jiffies, timeout))
 +			return -EBUSY;
 +
 +		range.notifier_seq = mmu_interval_read_begin(range.notifier);
 +		mmap_read_lock(mm);
 +		ret = hmm_range_fault(&range);
 +		mmap_read_unlock(mm);
 +		if (ret) {
 +			if (ret == -EBUSY)
 +				continue;
 +			return ret;
 +		}
 +
 +		mutex_lock(&svmm->mutex);
 +		if (mmu_interval_read_retry(range.notifier,
 +					    range.notifier_seq)) {
 +			mutex_unlock(&svmm->mutex);
 +			continue;
 +		}
 +		break;
 +	}
 +
 +	nouveau_hmm_convert_pfn(drm, &range, args);
 +
 +	svmm->vmm->vmm.object.client->super = true;
 +	ret = nvif_object_ioctl(&svmm->vmm->vmm.object, args, size, NULL);
 +	svmm->vmm->vmm.object.client->super = false;
 +	mutex_unlock(&svmm->mutex);
 +
 +	return ret;
++=======
+ 	range->default_flags = 0;
+ 	range->pfn_flags_mask = -1UL;
+ 
+ 	ret = hmm_range_register(range, mirror,
+ 				 range->start, range->end,
+ 				 PAGE_SHIFT);
+ 	if (ret) {
+ 		up_read(&range->hmm->mm->mmap_sem);
+ 		return (int)ret;
+ 	}
+ 
+ 	if (!hmm_range_wait_until_valid(range, HMM_RANGE_DEFAULT_TIMEOUT)) {
+ 		up_read(&range->hmm->mm->mmap_sem);
+ 		return -EBUSY;
+ 	}
+ 
+ 	ret = hmm_range_fault(range, 0);
+ 	if (ret <= 0) {
+ 		if (ret == 0)
+ 			ret = -EBUSY;
+ 		up_read(&range->hmm->mm->mmap_sem);
+ 		hmm_range_unregister(range);
+ 		return ret;
+ 	}
+ 	return 0;
++>>>>>>> cc374377a19d (mm/hmm: remove hmm_range vma)
  }
  
  static int
@@@ -741,29 -656,76 +769,72 @@@ nouveau_svm_fault(struct nvif_notify *n
  			 *
  			 * ie. WRITE faults appear first, thus any handling of
  			 * pending READ faults will already be satisfied.
 +			 * But if a large page is mapped, make sure subsequent
 +			 * fault addresses have sufficient access permission.
  			 */
 -			while (++fn < buffer->fault_nr &&
 -			       buffer->fault[fn]->svmm == svmm &&
 -			       buffer->fault[fn    ]->addr ==
 -			       buffer->fault[fn - 1]->addr);
 -
 -			/* If the next fault is outside the window, or all GPU
 -			 * faults have been dealt with, we're done here.
 -			 */
 -			if (fn >= buffer->fault_nr ||
 -			    buffer->fault[fn]->svmm != svmm ||
 -			    buffer->fault[fn]->addr >= limit)
 +			if (buffer->fault[fn]->svmm != svmm ||
 +			    buffer->fault[fn]->addr >= limit ||
 +			    (buffer->fault[fi]->access == 0 /* READ. */ &&
 +			     !(args.phys[0] & NVIF_VMM_PFNMAP_V0_V)) ||
 +			    (buffer->fault[fi]->access != 0 /* READ. */ &&
 +			     buffer->fault[fi]->access != 3 /* PREFETCH. */ &&
 +			     !(args.phys[0] & NVIF_VMM_PFNMAP_V0_W)))
  				break;
 -
 -			/* Fill in the gap between this fault and the next. */
 -			fill = (buffer->fault[fn    ]->addr -
 -				buffer->fault[fn - 1]->addr) >> PAGE_SHIFT;
 -			while (--fill)
 -				args.phys[pi++] = NVIF_VMM_PFNMAP_V0_NONE;
  		}
  
 -		SVMM_DBG(svmm, "wndw %016llx-%016llx covering %d fault(s)",
 -			 args.i.p.addr,
 -			 args.i.p.addr + args.i.p.size, fn - fi);
 +		/* If handling failed completely, cancel all faults. */
 +		if (ret) {
 +			while (fi < fn) {
 +				struct nouveau_svm_fault *fault =
 +					buffer->fault[fi++];
  
++<<<<<<< HEAD
++=======
+ 		/* Have HMM fault pages within the fault window to the GPU. */
+ 		range.start = args.i.p.addr;
+ 		range.end = args.i.p.addr + args.i.p.size;
+ 		range.pfns = args.phys;
+ 		range.flags = nouveau_svm_pfn_flags;
+ 		range.values = nouveau_svm_pfn_values;
+ 		range.pfn_shift = NVIF_VMM_PFNMAP_V0_ADDR_SHIFT;
+ again:
+ 		ret = nouveau_range_fault(&svmm->mirror, &range);
+ 		if (ret == 0) {
+ 			mutex_lock(&svmm->mutex);
+ 			if (!nouveau_range_done(&range)) {
+ 				mutex_unlock(&svmm->mutex);
+ 				goto again;
+ 			}
+ 
+ 			nouveau_dmem_convert_pfn(svm->drm, &range);
+ 
+ 			svmm->vmm->vmm.object.client->super = true;
+ 			ret = nvif_object_ioctl(&svmm->vmm->vmm.object,
+ 						&args, sizeof(args.i) +
+ 						pi * sizeof(args.phys[0]),
+ 						NULL);
+ 			svmm->vmm->vmm.object.client->super = false;
+ 			mutex_unlock(&svmm->mutex);
+ 			up_read(&svmm->mm->mmap_sem);
+ 		}
+ 
+ 		/* Cancel any faults in the window whose pages didn't manage
+ 		 * to keep their valid bit, or stay writeable when required.
+ 		 *
+ 		 * If handling failed completely, cancel all faults.
+ 		 */
+ 		while (fi < fn) {
+ 			struct nouveau_svm_fault *fault = buffer->fault[fi++];
+ 			pi = (fault->addr - range.start) >> PAGE_SHIFT;
+ 			if (ret ||
+ 			     !(range.pfns[pi] & NVIF_VMM_PFNMAP_V0_V) ||
+ 			    (!(range.pfns[pi] & NVIF_VMM_PFNMAP_V0_W) &&
+ 			     fault->access != 0 && fault->access != 3)) {
++>>>>>>> cc374377a19d (mm/hmm: remove hmm_range vma)
  				nouveau_svm_fault_cancel_fault(svm, fault);
 -				continue;
  			}
 +		} else
  			replay++;
 -		}
  	}
  
  	/* Issue fault replay to the GPU. */
* Unmerged path drivers/gpu/drm/nouveau/nouveau_svm.c
diff --git a/include/linux/hmm.h b/include/linux/hmm.h
index 86b23fe09dc9..027a43bb1b2e 100644
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@ -182,7 +182,6 @@ enum hmm_pfn_value_e {
  */
 struct hmm_range {
 	struct hmm		*hmm;
-	struct vm_area_struct	*vma;
 	struct list_head	list;
 	unsigned long		start;
 	unsigned long		end;
diff --git a/mm/hmm.c b/mm/hmm.c
index 582130a96799..4761d08c82fa 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -1113,7 +1113,6 @@ long hmm_range_fault(struct hmm_range *range, bool block)
 			return -EPERM;
 		}
 
-		range->vma = vma;
 		hmm_vma_walk.pgmap = NULL;
 		hmm_vma_walk.last = start;
 		hmm_vma_walk.fault = true;
