bpf: Pull out a macro for interpreting atomic ALU operations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Brendan Jackman <jackmanb@google.com>
commit 462910670e4ac91509829c5549bd0227668176fb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/46291067.failed

Since the atomic operations that are added in subsequent commits are
all isomorphic with BPF_ADD, pull out a macro to avoid the
interpreter becoming dominated by lines of atomic-related code.

Note that this sacrificies interpreter performance (combining
STX_ATOMIC_W and STX_ATOMIC_DW into single switch case means that we
need an extra conditional branch to differentiate them) in favour of
compact and (relatively!) simple C code.

	Signed-off-by: Brendan Jackman <jackmanb@google.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Yonghong Song <yhs@fb.com>
Link: https://lore.kernel.org/bpf/20210114181751.768687-9-jackmanb@google.com
(cherry picked from commit 462910670e4ac91509829c5549bd0227668176fb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/core.c
diff --cc kernel/bpf/core.c
index a915516dd706,8669e685825f..000000000000
--- a/kernel/bpf/core.c
+++ b/kernel/bpf/core.c
@@@ -1631,13 -1618,56 +1631,66 @@@ out
  	LDX_PROBE(DW, 8)
  #undef LDX_PROBE
  
++<<<<<<< HEAD
 +	STX_XADD_W: /* lock xadd *(u32 *)(dst_reg + off16) += src_reg */
 +		atomic_add((u32) SRC, (atomic_t *)(unsigned long)
 +			   (DST + insn->off));
 +		CONT;
 +	STX_XADD_DW: /* lock xadd *(u64 *)(dst_reg + off16) += src_reg */
 +		atomic64_add((u64) SRC, (atomic64_t *)(unsigned long)
 +			     (DST + insn->off));
++=======
+ #define ATOMIC_ALU_OP(BOP, KOP)						\
+ 		case BOP:						\
+ 			if (BPF_SIZE(insn->code) == BPF_W)		\
+ 				atomic_##KOP((u32) SRC, (atomic_t *)(unsigned long) \
+ 					     (DST + insn->off));	\
+ 			else						\
+ 				atomic64_##KOP((u64) SRC, (atomic64_t *)(unsigned long) \
+ 					       (DST + insn->off));	\
+ 			break;						\
+ 		case BOP | BPF_FETCH:					\
+ 			if (BPF_SIZE(insn->code) == BPF_W)		\
+ 				SRC = (u32) atomic_fetch_##KOP(		\
+ 					(u32) SRC,			\
+ 					(atomic_t *)(unsigned long) (DST + insn->off)); \
+ 			else						\
+ 				SRC = (u64) atomic64_fetch_##KOP(	\
+ 					(u64) SRC,			\
+ 					(atomic64_t *)(unsigned long) (DST + insn->off)); \
+ 			break;
+ 
+ 	STX_ATOMIC_DW:
+ 	STX_ATOMIC_W:
+ 		switch (IMM) {
+ 		ATOMIC_ALU_OP(BPF_ADD, add)
+ #undef ATOMIC_ALU_OP
+ 
+ 		case BPF_XCHG:
+ 			if (BPF_SIZE(insn->code) == BPF_W)
+ 				SRC = (u32) atomic_xchg(
+ 					(atomic_t *)(unsigned long) (DST + insn->off),
+ 					(u32) SRC);
+ 			else
+ 				SRC = (u64) atomic64_xchg(
+ 					(atomic64_t *)(unsigned long) (DST + insn->off),
+ 					(u64) SRC);
+ 			break;
+ 		case BPF_CMPXCHG:
+ 			if (BPF_SIZE(insn->code) == BPF_W)
+ 				BPF_R0 = (u32) atomic_cmpxchg(
+ 					(atomic_t *)(unsigned long) (DST + insn->off),
+ 					(u32) BPF_R0, (u32) SRC);
+ 			else
+ 				BPF_R0 = (u64) atomic64_cmpxchg(
+ 					(atomic64_t *)(unsigned long) (DST + insn->off),
+ 					(u64) BPF_R0, (u64) SRC);
+ 			break;
+ 
+ 		default:
+ 			goto default_label;
+ 		}
++>>>>>>> 462910670e4a (bpf: Pull out a macro for interpreting atomic ALU operations)
  		CONT;
  
  	default_label:
* Unmerged path kernel/bpf/core.c
