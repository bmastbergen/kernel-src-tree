net/mlx5e: kTLS, Add resiliency to RX resync failures

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Tariq Toukan <tariqt@nvidia.com>
commit e9ce991bce5bacf71641bd0f72f4b7c589529f40
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/e9ce991b.failed

When the TLS logic finds a tcp seq match for a kTLS RX resync
request, it calls the driver callback function mlx5e_ktls_resync()
to handle it and communicate it to the device.

Errors might occur during mlx5e_ktls_resync(), however, they are not
reported to the stack. Moreover, there is no error handling in the
stack for these errors.

In this patch, the driver obtains responsibility on errors handling,
adding queue and retry mechanisms to these resyncs.

We maintain a linked list of resync matches, and try posting them
to the async ICOSQ in the NAPI context.

Only possible failure that demands driver handling is ICOSQ being full.
By relying on the NAPI mechanism, we make sure that the entries in list
will be handled when ICOSQ completions arrive and make some room
available.

	Signed-off-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit e9ce991bce5bacf71641bd0f72f4b7c589529f40)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en/params.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/params.c
index 36381a2ed5a5,f6ba568e00be..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
@@@ -186,3 -188,474 +186,477 @@@ int mlx5e_validate_params(struct mlx5e_
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static struct dim_cq_moder mlx5e_get_def_tx_moderation(u8 cq_period_mode)
+ {
+ 	struct dim_cq_moder moder;
+ 
+ 	moder.cq_period_mode = cq_period_mode;
+ 	moder.pkts = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS;
+ 	moder.usec = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC;
+ 	if (cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE)
+ 		moder.usec = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC_FROM_CQE;
+ 
+ 	return moder;
+ }
+ 
+ static struct dim_cq_moder mlx5e_get_def_rx_moderation(u8 cq_period_mode)
+ {
+ 	struct dim_cq_moder moder;
+ 
+ 	moder.cq_period_mode = cq_period_mode;
+ 	moder.pkts = MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS;
+ 	moder.usec = MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC;
+ 	if (cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE)
+ 		moder.usec = MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC_FROM_CQE;
+ 
+ 	return moder;
+ }
+ 
+ static u8 mlx5_to_net_dim_cq_period_mode(u8 cq_period_mode)
+ {
+ 	return cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE ?
+ 		DIM_CQ_PERIOD_MODE_START_FROM_CQE :
+ 		DIM_CQ_PERIOD_MODE_START_FROM_EQE;
+ }
+ 
+ void mlx5e_reset_tx_moderation(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	if (params->tx_dim_enabled) {
+ 		u8 dim_period_mode = mlx5_to_net_dim_cq_period_mode(cq_period_mode);
+ 
+ 		params->tx_cq_moderation = net_dim_get_def_tx_moderation(dim_period_mode);
+ 	} else {
+ 		params->tx_cq_moderation = mlx5e_get_def_tx_moderation(cq_period_mode);
+ 	}
+ }
+ 
+ void mlx5e_reset_rx_moderation(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	if (params->rx_dim_enabled) {
+ 		u8 dim_period_mode = mlx5_to_net_dim_cq_period_mode(cq_period_mode);
+ 
+ 		params->rx_cq_moderation = net_dim_get_def_rx_moderation(dim_period_mode);
+ 	} else {
+ 		params->rx_cq_moderation = mlx5e_get_def_rx_moderation(cq_period_mode);
+ 	}
+ }
+ 
+ void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	mlx5e_reset_tx_moderation(params, cq_period_mode);
+ 	MLX5E_SET_PFLAG(params, MLX5E_PFLAG_TX_CQE_BASED_MODER,
+ 			params->tx_cq_moderation.cq_period_mode ==
+ 				MLX5_CQ_PERIOD_MODE_START_FROM_CQE);
+ }
+ 
+ void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params, u8 cq_period_mode)
+ {
+ 	mlx5e_reset_rx_moderation(params, cq_period_mode);
+ 	MLX5E_SET_PFLAG(params, MLX5E_PFLAG_RX_CQE_BASED_MODER,
+ 			params->rx_cq_moderation.cq_period_mode ==
+ 				MLX5_CQ_PERIOD_MODE_START_FROM_CQE);
+ }
+ 
+ bool slow_pci_heuristic(struct mlx5_core_dev *mdev)
+ {
+ 	u32 link_speed = 0;
+ 	u32 pci_bw = 0;
+ 
+ 	mlx5e_port_max_linkspeed(mdev, &link_speed);
+ 	pci_bw = pcie_bandwidth_available(mdev->pdev, NULL, NULL, NULL);
+ 	mlx5_core_dbg_once(mdev, "Max link speed = %d, PCI BW = %d\n",
+ 			   link_speed, pci_bw);
+ 
+ #define MLX5E_SLOW_PCI_RATIO (2)
+ 
+ 	return link_speed && pci_bw &&
+ 		link_speed > MLX5E_SLOW_PCI_RATIO * pci_bw;
+ }
+ 
+ bool mlx5e_striding_rq_possible(struct mlx5_core_dev *mdev,
+ 				struct mlx5e_params *params)
+ {
+ 	if (!mlx5e_check_fragmented_striding_rq_cap(mdev))
+ 		return false;
+ 
+ 	if (MLX5_IPSEC_DEV(mdev))
+ 		return false;
+ 
+ 	if (params->xdp_prog) {
+ 		/* XSK params are not considered here. If striding RQ is in use,
+ 		 * and an XSK is being opened, mlx5e_rx_mpwqe_is_linear_skb will
+ 		 * be called with the known XSK params.
+ 		 */
+ 		if (!mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL))
+ 			return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
+ 			       struct mlx5e_params *params)
+ {
+ 	params->log_rq_mtu_frames = is_kdump_kernel() ?
+ 		MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE :
+ 		MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
+ 
+ 	mlx5_core_info(mdev, "MLX5E: StrdRq(%d) RqSz(%ld) StrdSz(%ld) RxCqeCmprss(%d)\n",
+ 		       params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ,
+ 		       params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ ?
+ 		       BIT(mlx5e_mpwqe_get_log_rq_size(params, NULL)) :
+ 		       BIT(params->log_rq_mtu_frames),
+ 		       BIT(mlx5e_mpwqe_get_log_stride_size(mdev, params, NULL)),
+ 		       MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS));
+ }
+ 
+ void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
+ {
+ 	params->rq_wq_type = mlx5e_striding_rq_possible(mdev, params) &&
+ 		MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_STRIDING_RQ) ?
+ 		MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ :
+ 		MLX5_WQ_TYPE_CYCLIC;
+ }
+ 
+ void mlx5e_build_rq_params(struct mlx5_core_dev *mdev,
+ 			   struct mlx5e_params *params)
+ {
+ 	/* Prefer Striding RQ, unless any of the following holds:
+ 	 * - Striding RQ configuration is not possible/supported.
+ 	 * - Slow PCI heuristic.
+ 	 * - Legacy RQ would use linear SKB while Striding RQ would use non-linear.
+ 	 *
+ 	 * No XSK params: checking the availability of striding RQ in general.
+ 	 */
+ 	if (!slow_pci_heuristic(mdev) &&
+ 	    mlx5e_striding_rq_possible(mdev, params) &&
+ 	    (mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL) ||
+ 	     !mlx5e_rx_is_linear_skb(params, NULL)))
+ 		MLX5E_SET_PFLAG(params, MLX5E_PFLAG_RX_STRIDING_RQ, true);
+ 	mlx5e_set_rq_type(mdev, params);
+ 	mlx5e_init_rq_type_params(mdev, params);
+ }
+ 
+ /* Build queue parameters */
+ 
+ void mlx5e_build_create_cq_param(struct mlx5e_create_cq_param *ccp, struct mlx5e_channel *c)
+ {
+ 	*ccp = (struct mlx5e_create_cq_param) {
+ 		.napi = &c->napi,
+ 		.ch_stats = c->stats,
+ 		.node = cpu_to_node(c->cpu),
+ 		.ix = c->ix,
+ 	};
+ }
+ 
+ #define DEFAULT_FRAG_SIZE (2048)
+ 
+ static void mlx5e_build_rq_frags_info(struct mlx5_core_dev *mdev,
+ 				      struct mlx5e_params *params,
+ 				      struct mlx5e_xsk_param *xsk,
+ 				      struct mlx5e_rq_frags_info *info)
+ {
+ 	u32 byte_count = MLX5E_SW2HW_MTU(params, params->sw_mtu);
+ 	int frag_size_max = DEFAULT_FRAG_SIZE;
+ 	u32 buf_size = 0;
+ 	int i;
+ 
+ 	if (MLX5_IPSEC_DEV(mdev))
+ 		byte_count += MLX5E_METADATA_ETHER_LEN;
+ 
+ 	if (mlx5e_rx_is_linear_skb(params, xsk)) {
+ 		int frag_stride;
+ 
+ 		frag_stride = mlx5e_rx_get_linear_frag_sz(params, xsk);
+ 		frag_stride = roundup_pow_of_two(frag_stride);
+ 
+ 		info->arr[0].frag_size = byte_count;
+ 		info->arr[0].frag_stride = frag_stride;
+ 		info->num_frags = 1;
+ 		info->wqe_bulk = PAGE_SIZE / frag_stride;
+ 		goto out;
+ 	}
+ 
+ 	if (byte_count > PAGE_SIZE +
+ 	    (MLX5E_MAX_RX_FRAGS - 1) * frag_size_max)
+ 		frag_size_max = PAGE_SIZE;
+ 
+ 	i = 0;
+ 	while (buf_size < byte_count) {
+ 		int frag_size = byte_count - buf_size;
+ 
+ 		if (i < MLX5E_MAX_RX_FRAGS - 1)
+ 			frag_size = min(frag_size, frag_size_max);
+ 
+ 		info->arr[i].frag_size = frag_size;
+ 		info->arr[i].frag_stride = roundup_pow_of_two(frag_size);
+ 
+ 		buf_size += frag_size;
+ 		i++;
+ 	}
+ 	info->num_frags = i;
+ 	/* number of different wqes sharing a page */
+ 	info->wqe_bulk = 1 + (info->num_frags % 2);
+ 
+ out:
+ 	info->wqe_bulk = max_t(u8, info->wqe_bulk, 8);
+ 	info->log_num_frags = order_base_2(info->num_frags);
+ }
+ 
+ static u8 mlx5e_get_rqwq_log_stride(u8 wq_type, int ndsegs)
+ {
+ 	int sz = sizeof(struct mlx5_wqe_data_seg) * ndsegs;
+ 
+ 	switch (wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		sz += sizeof(struct mlx5e_rx_wqe_ll);
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		sz += sizeof(struct mlx5e_rx_wqe_cyc);
+ 	}
+ 
+ 	return order_base_2(sz);
+ }
+ 
+ static void mlx5e_build_common_cq_param(struct mlx5_core_dev *mdev,
+ 					struct mlx5e_cq_param *param)
+ {
+ 	void *cqc = param->cqc;
+ 
+ 	MLX5_SET(cqc, cqc, uar_page, mdev->priv.uar->index);
+ 	if (MLX5_CAP_GEN(mdev, cqe_128_always) && cache_line_size() >= 128)
+ 		MLX5_SET(cqc, cqc, cqe_sz, CQE_STRIDE_128_PAD);
+ }
+ 
+ static void mlx5e_build_rx_cq_param(struct mlx5_core_dev *mdev,
+ 				    struct mlx5e_params *params,
+ 				    struct mlx5e_xsk_param *xsk,
+ 				    struct mlx5e_cq_param *param)
+ {
+ 	bool hw_stridx = false;
+ 	void *cqc = param->cqc;
+ 	u8 log_cq_size;
+ 
+ 	switch (params->rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		log_cq_size = mlx5e_mpwqe_get_log_rq_size(params, xsk) +
+ 			mlx5e_mpwqe_get_log_num_strides(mdev, params, xsk);
+ 		hw_stridx = MLX5_CAP_GEN(mdev, mini_cqe_resp_stride_index);
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		log_cq_size = params->log_rq_mtu_frames;
+ 	}
+ 
+ 	MLX5_SET(cqc, cqc, log_cq_size, log_cq_size);
+ 	if (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS)) {
+ 		MLX5_SET(cqc, cqc, mini_cqe_res_format, hw_stridx ?
+ 			 MLX5_CQE_FORMAT_CSUM_STRIDX : MLX5_CQE_FORMAT_CSUM);
+ 		MLX5_SET(cqc, cqc, cqe_comp_en, 1);
+ 	}
+ 
+ 	mlx5e_build_common_cq_param(mdev, param);
+ 	param->cq_period_mode = params->rx_cq_moderation.cq_period_mode;
+ }
+ 
+ void mlx5e_build_rq_param(struct mlx5_core_dev *mdev,
+ 			  struct mlx5e_params *params,
+ 			  struct mlx5e_xsk_param *xsk,
+ 			  u16 q_counter,
+ 			  struct mlx5e_rq_param *param)
+ {
+ 	void *rqc = param->rqc;
+ 	void *wq = MLX5_ADDR_OF(rqc, rqc, wq);
+ 	int ndsegs = 1;
+ 
+ 	switch (params->rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		MLX5_SET(wq, wq, log_wqe_num_of_strides,
+ 			 mlx5e_mpwqe_get_log_num_strides(mdev, params, xsk) -
+ 			 MLX5_MPWQE_LOG_NUM_STRIDES_BASE);
+ 		MLX5_SET(wq, wq, log_wqe_stride_size,
+ 			 mlx5e_mpwqe_get_log_stride_size(mdev, params, xsk) -
+ 			 MLX5_MPWQE_LOG_STRIDE_SZ_BASE);
+ 		MLX5_SET(wq, wq, log_wq_sz, mlx5e_mpwqe_get_log_rq_size(params, xsk));
+ 		break;
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		MLX5_SET(wq, wq, log_wq_sz, params->log_rq_mtu_frames);
+ 		mlx5e_build_rq_frags_info(mdev, params, xsk, &param->frags_info);
+ 		ndsegs = param->frags_info.num_frags;
+ 	}
+ 
+ 	MLX5_SET(wq, wq, wq_type,          params->rq_wq_type);
+ 	MLX5_SET(wq, wq, end_padding_mode, MLX5_WQ_END_PAD_MODE_ALIGN);
+ 	MLX5_SET(wq, wq, log_wq_stride,
+ 		 mlx5e_get_rqwq_log_stride(params->rq_wq_type, ndsegs));
+ 	MLX5_SET(wq, wq, pd,               mdev->mlx5e_res.hw_objs.pdn);
+ 	MLX5_SET(rqc, rqc, counter_set_id, q_counter);
+ 	MLX5_SET(rqc, rqc, vsd,            params->vlan_strip_disable);
+ 	MLX5_SET(rqc, rqc, scatter_fcs,    params->scatter_fcs_en);
+ 
+ 	param->wq.buf_numa_node = dev_to_node(mlx5_core_dma_dev(mdev));
+ 	mlx5e_build_rx_cq_param(mdev, params, xsk, &param->cqp);
+ }
+ 
+ void mlx5e_build_drop_rq_param(struct mlx5_core_dev *mdev,
+ 			       u16 q_counter,
+ 			       struct mlx5e_rq_param *param)
+ {
+ 	void *rqc = param->rqc;
+ 	void *wq = MLX5_ADDR_OF(rqc, rqc, wq);
+ 
+ 	MLX5_SET(wq, wq, wq_type, MLX5_WQ_TYPE_CYCLIC);
+ 	MLX5_SET(wq, wq, log_wq_stride,
+ 		 mlx5e_get_rqwq_log_stride(MLX5_WQ_TYPE_CYCLIC, 1));
+ 	MLX5_SET(rqc, rqc, counter_set_id, q_counter);
+ 
+ 	param->wq.buf_numa_node = dev_to_node(mlx5_core_dma_dev(mdev));
+ }
+ 
+ void mlx5e_build_tx_cq_param(struct mlx5_core_dev *mdev,
+ 			     struct mlx5e_params *params,
+ 			     struct mlx5e_cq_param *param)
+ {
+ 	void *cqc = param->cqc;
+ 
+ 	MLX5_SET(cqc, cqc, log_cq_size, params->log_sq_size);
+ 
+ 	mlx5e_build_common_cq_param(mdev, param);
+ 	param->cq_period_mode = params->tx_cq_moderation.cq_period_mode;
+ }
+ 
+ void mlx5e_build_sq_param_common(struct mlx5_core_dev *mdev,
+ 				 struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	MLX5_SET(wq, wq, log_wq_stride, ilog2(MLX5_SEND_WQE_BB));
+ 	MLX5_SET(wq, wq, pd,            mdev->mlx5e_res.hw_objs.pdn);
+ 
+ 	param->wq.buf_numa_node = dev_to_node(mlx5_core_dma_dev(mdev));
+ }
+ 
+ void mlx5e_build_sq_param(struct mlx5_core_dev *mdev,
+ 			  struct mlx5e_params *params,
+ 			  struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 	bool allow_swp;
+ 
+ 	allow_swp = mlx5_geneve_tx_allowed(mdev) ||
+ 		    !!MLX5_IPSEC_DEV(mdev);
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
+ 	MLX5_SET(sqc, sqc, allow_swp, allow_swp);
+ 	param->is_mpw = MLX5E_GET_PFLAG(params, MLX5E_PFLAG_SKB_TX_MPWQE);
+ 	param->stop_room = mlx5e_calc_sq_stop_room(mdev, params);
+ 	mlx5e_build_tx_cq_param(mdev, params, &param->cqp);
+ }
+ 
+ static void mlx5e_build_ico_cq_param(struct mlx5_core_dev *mdev,
+ 				     u8 log_wq_size,
+ 				     struct mlx5e_cq_param *param)
+ {
+ 	void *cqc = param->cqc;
+ 
+ 	MLX5_SET(cqc, cqc, log_cq_size, log_wq_size);
+ 
+ 	mlx5e_build_common_cq_param(mdev, param);
+ 
+ 	param->cq_period_mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;
+ }
+ 
+ static u8 mlx5e_get_rq_log_wq_sz(void *rqc)
+ {
+ 	void *wq = MLX5_ADDR_OF(rqc, rqc, wq);
+ 
+ 	return MLX5_GET(wq, wq, log_wq_sz);
+ }
+ 
+ static u8 mlx5e_build_icosq_log_wq_sz(struct mlx5e_params *params,
+ 				      struct mlx5e_rq_param *rqp)
+ {
+ 	switch (params->rq_wq_type) {
+ 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+ 		return max_t(u8, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE,
+ 			     order_base_2(MLX5E_UMR_WQEBBS) +
+ 			     mlx5e_get_rq_log_wq_sz(rqp->rqc));
+ 	default: /* MLX5_WQ_TYPE_CYCLIC */
+ 		return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
+ 	}
+ }
+ 
+ static u8 mlx5e_build_async_icosq_log_wq_sz(struct mlx5_core_dev *mdev)
+ {
+ 	if (mlx5_accel_is_ktls_rx(mdev))
+ 		return MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
+ 
+ 	return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
+ }
+ 
+ static void mlx5e_build_icosq_param(struct mlx5_core_dev *mdev,
+ 				    u8 log_wq_size,
+ 				    struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 
+ 	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
+ 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(mdev, reg_umr_sq));
+ 	mlx5e_build_ico_cq_param(mdev, log_wq_size, &param->cqp);
+ }
+ 
+ static void mlx5e_build_async_icosq_param(struct mlx5_core_dev *mdev,
+ 					  u8 log_wq_size,
+ 					  struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 	param->stop_room = mlx5e_stop_room_for_wqe(1); /* for XSK NOP */
+ 	param->is_tls = mlx5_accel_is_ktls_rx(mdev);
+ 	if (param->is_tls)
+ 		param->stop_room += mlx5e_stop_room_for_wqe(1); /* for TLS RX resync NOP */
+ 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(mdev, reg_umr_sq));
+ 	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
+ 	mlx5e_build_ico_cq_param(mdev, log_wq_size, &param->cqp);
+ }
+ 
+ void mlx5e_build_xdpsq_param(struct mlx5_core_dev *mdev,
+ 			     struct mlx5e_params *params,
+ 			     struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
+ 
+ 	mlx5e_build_sq_param_common(mdev, param);
+ 	MLX5_SET(wq, wq, log_wq_sz, params->log_sq_size);
+ 	param->is_mpw = MLX5E_GET_PFLAG(params, MLX5E_PFLAG_XDP_TX_MPWQE);
+ 	mlx5e_build_tx_cq_param(mdev, params, &param->cqp);
+ }
+ 
+ void mlx5e_build_channel_param(struct mlx5_core_dev *mdev,
+ 			       struct mlx5e_params *params,
+ 			       u16 q_counter,
+ 			       struct mlx5e_channel_param *cparam)
+ {
+ 	u8 icosq_log_wq_sz, async_icosq_log_wq_sz;
+ 
+ 	mlx5e_build_rq_param(mdev, params, NULL, q_counter, &cparam->rq);
+ 
+ 	icosq_log_wq_sz = mlx5e_build_icosq_log_wq_sz(params, &cparam->rq);
+ 	async_icosq_log_wq_sz = mlx5e_build_async_icosq_log_wq_sz(mdev);
+ 
+ 	mlx5e_build_sq_param(mdev, params, &cparam->txq_sq);
+ 	mlx5e_build_xdpsq_param(mdev, params, &cparam->xdp_sq);
+ 	mlx5e_build_icosq_param(mdev, icosq_log_wq_sz, &cparam->icosq);
+ 	mlx5e_build_async_icosq_param(mdev, async_icosq_log_wq_sz, &cparam->async_icosq);
+ }
++>>>>>>> e9ce991bce5b (net/mlx5e: kTLS, Add resiliency to RX resync failures)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c
index bdec56008da9,4e58fade7a60..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c
@@@ -357,33 -385,32 +384,48 @@@ static void resync_init(struct mlx5e_kt
  /* Function can be called with the refcount being either elevated or not.
   * It does not affect the refcount.
   */
- static int resync_handle_seq_match(struct mlx5e_ktls_offload_context_rx *priv_rx,
- 				   struct mlx5e_channel *c)
+ static void resync_handle_seq_match(struct mlx5e_ktls_offload_context_rx *priv_rx,
+ 				    struct mlx5e_channel *c)
  {
  	struct tls12_crypto_info_aes_gcm_128 *info = &priv_rx->crypto_info;
- 	struct mlx5_wqe_ctrl_seg *cseg;
+ 	struct mlx5e_ktls_resync_resp *ktls_resync;
  	struct mlx5e_icosq *sq;
- 	int err;
+ 	bool trigger_poll;
  
  	memcpy(info->rec_seq, &priv_rx->resync.sw_rcd_sn_be, sizeof(info->rec_seq));
- 	err = 0;
  
  	sq = &c->async_icosq;
- 	spin_lock_bh(&c->async_icosq_lock);
+ 	ktls_resync = sq->ktls_resync;
  
++<<<<<<< HEAD
 +	cseg = post_static_params(sq, priv_rx);
 +	if (IS_ERR(cseg)) {
 +		priv_rx->stats->tls_resync_res_skip++;
 +		err = PTR_ERR(cseg);
 +		goto unlock;
 +	}
 +	/* Do not increment priv_rx refcnt, CQE handling is empty */
 +	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
 +	priv_rx->stats->tls_resync_res_ok++;
 +unlock:
 +	spin_unlock_bh(&c->async_icosq_lock);
 +
 +	return err;
++=======
+ 	spin_lock_bh(&ktls_resync->lock);
+ 	list_add_tail(&priv_rx->list, &ktls_resync->list);
+ 	trigger_poll = !test_and_set_bit(MLX5E_SQ_STATE_PENDING_TLS_RX_RESYNC, &sq->state);
+ 	spin_unlock_bh(&ktls_resync->lock);
+ 
+ 	if (!trigger_poll)
+ 		return;
+ 
+ 	if (!napi_if_scheduled_mark_missed(&c->napi)) {
+ 		spin_lock_bh(&c->async_icosq_lock);
+ 		mlx5e_trigger_irq(sq);
+ 		spin_unlock_bh(&c->async_icosq_lock);
+ 	}
++>>>>>>> e9ce991bce5b (net/mlx5e: kTLS, Add resiliency to RX resync failures)
  }
  
  /* Function can be called with the refcount being either elevated or not.
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9b2636b23a6c..234516878a77 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -318,6 +318,7 @@ enum {
 	MLX5E_SQ_STATE_AM,
 	MLX5E_SQ_STATE_VLAN_NEED_L2_INLINE,
 	MLX5E_SQ_STATE_PENDING_XSK_TX,
+	MLX5E_SQ_STATE_PENDING_TLS_RX_RESYNC,
 };
 
 struct mlx5e_tx_mpwqe {
@@ -484,6 +485,8 @@ struct mlx5e_xdpsq {
 	struct mlx5e_channel      *channel;
 } ____cacheline_aligned_in_smp;
 
+struct mlx5e_ktls_resync_resp;
+
 struct mlx5e_icosq {
 	/* data path */
 	u16                        cc;
@@ -503,6 +506,7 @@ struct mlx5e_icosq {
 	u32                        sqn;
 	u16                        reserved_room;
 	unsigned long              state;
+	struct mlx5e_ktls_resync_resp *ktls_resync;
 
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/params.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/params.h b/drivers/net/ethernet/mellanox/mlx5/core/en/params.h
index 187007ad3349..20d0eb48f515 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/params.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/params.h
@@ -30,6 +30,7 @@ struct mlx5e_sq_param {
 	u32                        sqc[MLX5_ST_SZ_DW(sqc)];
 	struct mlx5_wq_param       wq;
 	bool                       is_mpw;
+	bool                       is_tls;
 	u16                        stop_room;
 };
 
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
index baa58b62e8df..aaa579bf9a39 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls.h
@@ -12,6 +12,9 @@ void mlx5e_ktls_build_netdev(struct mlx5e_priv *priv);
 int mlx5e_ktls_init_rx(struct mlx5e_priv *priv);
 void mlx5e_ktls_cleanup_rx(struct mlx5e_priv *priv);
 int mlx5e_ktls_set_feature_rx(struct net_device *netdev, bool enable);
+struct mlx5e_ktls_resync_resp *
+mlx5e_ktls_rx_resync_create_resp_list(void);
+void mlx5e_ktls_rx_resync_destroy_resp_list(struct mlx5e_ktls_resync_resp *resp_list);
 #else
 
 static inline void mlx5e_ktls_build_netdev(struct mlx5e_priv *priv)
@@ -33,6 +36,14 @@ static inline int mlx5e_ktls_set_feature_rx(struct net_device *netdev, bool enab
 	return -EOPNOTSUPP;
 }
 
+static inline struct mlx5e_ktls_resync_resp *
+mlx5e_ktls_rx_resync_create_resp_list(void)
+{
+	return ERR_PTR(-EOPNOTSUPP);
+}
+
+static inline void
+mlx5e_ktls_rx_resync_destroy_resp_list(struct mlx5e_ktls_resync_resp *resp_list) {}
 #endif
 
 #endif /* __MLX5E_TLS_H__ */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h
index ee04e916fa21..8f79335057dc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_txrx.h
@@ -40,6 +40,14 @@ mlx5e_ktls_tx_try_handle_resync_dump_comp(struct mlx5e_txqsq *sq,
 	}
 	return false;
 }
+
+bool mlx5e_ktls_rx_handle_resync_list(struct mlx5e_channel *c, int budget);
+
+static inline bool
+mlx5e_ktls_rx_pending_resync_list(struct mlx5e_channel *c, int budget)
+{
+	return budget && test_bit(MLX5E_SQ_STATE_PENDING_TLS_RX_RESYNC, &c->async_icosq.state);
+}
 #else
 static inline bool
 mlx5e_ktls_tx_try_handle_resync_dump_comp(struct mlx5e_txqsq *sq,
@@ -49,6 +57,18 @@ mlx5e_ktls_tx_try_handle_resync_dump_comp(struct mlx5e_txqsq *sq,
 	return false;
 }
 
+static inline bool
+mlx5e_ktls_rx_handle_resync_list(struct mlx5e_channel *c, int budget)
+{
+	return false;
+}
+
+static inline bool
+mlx5e_ktls_rx_pending_resync_list(struct mlx5e_channel *c, int budget)
+{
+	return false;
+}
+
 #endif /* CONFIG_MLX5_EN_TLS */
 
 #endif /* __MLX5E_TLS_TXRX_H__ */
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 237d6c6a22d1..c93596931e2b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -1414,8 +1414,17 @@ int mlx5e_open_icosq(struct mlx5e_channel *c, struct mlx5e_params *params,
 	if (err)
 		goto err_free_icosq;
 
+	if (param->is_tls) {
+		sq->ktls_resync = mlx5e_ktls_rx_resync_create_resp_list();
+		if (IS_ERR(sq->ktls_resync)) {
+			err = PTR_ERR(sq->ktls_resync);
+			goto err_destroy_icosq;
+		}
+	}
 	return 0;
 
+err_destroy_icosq:
+	mlx5e_destroy_sq(c->mdev, sq->sqn);
 err_free_icosq:
 	mlx5e_free_icosq(sq);
 
@@ -1437,6 +1446,8 @@ void mlx5e_close_icosq(struct mlx5e_icosq *sq)
 {
 	struct mlx5e_channel *c = sq->channel;
 
+	if (sq->ktls_resync)
+		mlx5e_ktls_rx_resync_destroy_resp_list(sq->ktls_resync);
 	mlx5e_destroy_sq(c->mdev, sq->sqn);
 	mlx5e_free_icosq_descs(sq);
 	mlx5e_free_icosq(sq);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
index ebfb47a09128..65238dcb1426 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
@@ -187,6 +187,7 @@ static const struct counter_desc sw_stats_desc[] = {
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_resync_req_end) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_resync_req_skip) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_resync_res_ok) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_resync_res_retry) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_resync_res_skip) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_tls_err) },
 #endif
@@ -349,6 +350,7 @@ static void mlx5e_stats_grp_sw_update_stats_rq_stats(struct mlx5e_sw_stats *s,
 	s->rx_tls_resync_req_end      += rq_stats->tls_resync_req_end;
 	s->rx_tls_resync_req_skip     += rq_stats->tls_resync_req_skip;
 	s->rx_tls_resync_res_ok       += rq_stats->tls_resync_res_ok;
+	s->rx_tls_resync_res_retry    += rq_stats->tls_resync_res_retry;
 	s->rx_tls_resync_res_skip     += rq_stats->tls_resync_res_skip;
 	s->rx_tls_err                 += rq_stats->tls_err;
 #endif
@@ -1590,6 +1592,7 @@ static const struct counter_desc rq_stats_desc[] = {
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_resync_req_end) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_resync_req_skip) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_resync_res_ok) },
+	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_resync_res_retry) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_resync_res_skip) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, tls_err) },
 #endif
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
index 162daaadb0d8..460351e49b93 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h
@@ -203,6 +203,7 @@ struct mlx5e_sw_stats {
 	u64 rx_tls_resync_req_end;
 	u64 rx_tls_resync_req_skip;
 	u64 rx_tls_resync_res_ok;
+	u64 rx_tls_resync_res_retry;
 	u64 rx_tls_resync_res_skip;
 	u64 rx_tls_err;
 #endif
@@ -335,6 +336,7 @@ struct mlx5e_rq_stats {
 	u64 tls_resync_req_end;
 	u64 tls_resync_req_skip;
 	u64 tls_resync_res_ok;
+	u64 tls_resync_res_retry;
 	u64 tls_resync_res_skip;
 	u64 tls_err;
 #endif
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
index 793e313dcb8b..0d208b827df2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -36,6 +36,7 @@
 #include "en/xdp.h"
 #include "en/xsk/rx.h"
 #include "en/xsk/tx.h"
+#include "en_accel/ktls_txrx.h"
 
 static inline bool mlx5e_channel_no_affinity_change(struct mlx5e_channel *c)
 {
@@ -155,6 +156,10 @@ int mlx5e_napi_poll(struct napi_struct *napi, int budget)
 		 */
 		clear_bit(MLX5E_SQ_STATE_PENDING_XSK_TX, &c->async_icosq.state);
 
+	/* Keep after async ICOSQ CQ poll */
+	if (unlikely(mlx5e_ktls_rx_pending_resync_list(c, budget)))
+		busy |= mlx5e_ktls_rx_handle_resync_list(c, budget);
+
 	busy |= INDIRECT_CALL_2(rq->post_wqes,
 				mlx5e_post_rx_mpwqes,
 				mlx5e_post_rx_wqes,
