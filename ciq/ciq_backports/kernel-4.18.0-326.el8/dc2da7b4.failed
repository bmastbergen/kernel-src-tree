mm: memmap defer init doesn't work as expected

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Baoquan He <bhe@redhat.com>
commit dc2da7b45ffe954a0090f5d0310ed7b0b37d2bd2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/dc2da7b4.failed

VMware observed a performance regression during memmap init on their
platform, and bisected to commit 73a6e474cb376 ("mm: memmap_init:
iterate over memblock regions rather that check each PFN") causing it.

Before the commit:

  [0.033176] Normal zone: 1445888 pages used for memmap
  [0.033176] Normal zone: 89391104 pages, LIFO batch:63
  [0.035851] ACPI: PM-Timer IO Port: 0x448

With commit

  [0.026874] Normal zone: 1445888 pages used for memmap
  [0.026875] Normal zone: 89391104 pages, LIFO batch:63
  [2.028450] ACPI: PM-Timer IO Port: 0x448

The root cause is the current memmap defer init doesn't work as expected.

Before, memmap_init_zone() was used to do memmap init of one whole zone,
to initialize all low zones of one numa node, but defer memmap init of
the last zone in that numa node.  However, since commit 73a6e474cb376,
function memmap_init() is adapted to iterater over memblock regions
inside one zone, then call memmap_init_zone() to do memmap init for each
region.

E.g, on VMware's system, the memory layout is as below, there are two
memory regions in node 2.  The current code will mistakenly initialize the
whole 1st region [mem 0xab00000000-0xfcffffffff], then do memmap defer to
iniatialize only one memmory section on the 2nd region [mem
0x10000000000-0x1033fffffff].  In fact, we only expect to see that there's
only one memory section's memmap initialized.  That's why more time is
costed at the time.

[    0.008842] ACPI: SRAT: Node 0 PXM 0 [mem 0x00000000-0x0009ffff]
[    0.008842] ACPI: SRAT: Node 0 PXM 0 [mem 0x00100000-0xbfffffff]
[    0.008843] ACPI: SRAT: Node 0 PXM 0 [mem 0x100000000-0x55ffffffff]
[    0.008844] ACPI: SRAT: Node 1 PXM 1 [mem 0x5600000000-0xaaffffffff]
[    0.008844] ACPI: SRAT: Node 2 PXM 2 [mem 0xab00000000-0xfcffffffff]
[    0.008845] ACPI: SRAT: Node 2 PXM 2 [mem 0x10000000000-0x1033fffffff]

Now, let's add a parameter 'zone_end_pfn' to memmap_init_zone() to pass
down the real zone end pfn so that defer_init() can use it to judge
whether defer need be taken in zone wide.

Link: https://lkml.kernel.org/r/20201223080811.16211-1-bhe@redhat.com
Link: https://lkml.kernel.org/r/20201223080811.16211-2-bhe@redhat.com
Fixes: commit 73a6e474cb376 ("mm: memmap_init: iterate over memblock regions rather that check each PFN")
	Signed-off-by: Baoquan He <bhe@redhat.com>
	Reported-by: Rahul Gopakumar <gopakumarr@vmware.com>
	Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit dc2da7b45ffe954a0090f5d0310ed7b0b37d2bd2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/ia64/mm/init.c
#	include/linux/mm.h
#	mm/memory_hotplug.c
#	mm/page_alloc.c
diff --cc arch/ia64/mm/init.c
index 5ab64d9d3462,e76386a3479e..000000000000
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@@ -497,8 -536,8 +497,13 @@@ virtual_memmap_init(u64 start, u64 end
  
  	if (map_start < map_end)
  		memmap_init_zone((unsigned long)(map_end - map_start),
++<<<<<<< HEAD
 +				 args->nid, args->zone, page_to_pfn(map_start),
 +				 MEMINIT_EARLY, NULL);
++=======
+ 				 args->nid, args->zone, page_to_pfn(map_start), page_to_pfn(map_end),
+ 				 MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);
++>>>>>>> dc2da7b45ffe (mm: memmap defer init doesn't work as expected)
  	return 0;
  }
  
@@@ -507,8 -546,8 +512,13 @@@ memmap_init (unsigned long size, int ni
  	     unsigned long start_pfn)
  {
  	if (!vmem_map) {
++<<<<<<< HEAD
 +		memmap_init_zone(size, nid, zone, start_pfn,
 +				 MEMINIT_EARLY, NULL);
++=======
+ 		memmap_init_zone(size, nid, zone, start_pfn, start_pfn + size,
+ 				 MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);
++>>>>>>> dc2da7b45ffe (mm: memmap defer init doesn't work as expected)
  	} else {
  		struct page *start;
  		struct memmap_init_callback_data args;
diff --cc include/linux/mm.h
index be5eb0ce5da1,ecdf8a8cd6ae..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -2331,8 -2439,9 +2331,14 @@@ extern int __meminit __early_pfn_to_nid
  #endif
  
  extern void set_dma_reserve(unsigned long new_dma_reserve);
++<<<<<<< HEAD
 +extern void memmap_init_zone(unsigned long, int, unsigned long, unsigned long,
 +		enum meminit_context, struct vmem_altmap *);
++=======
+ extern void memmap_init_zone(unsigned long, int, unsigned long,
+ 		unsigned long, unsigned long, enum meminit_context,
+ 		struct vmem_altmap *, int migratetype);
++>>>>>>> dc2da7b45ffe (mm: memmap defer init doesn't work as expected)
  extern void setup_per_zone_wmarks(void);
  extern int __meminit init_per_zone_wmark_min(void);
  extern void mem_init(void);
diff --cc mm/memory_hotplug.c
index 3f7b739a6e53,f9d57b9be8c7..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -694,8 -713,8 +694,13 @@@ void __ref move_pfn_range_to_zone(struc
  	 * expects the zone spans the pfn range. All the pages in the range
  	 * are reserved so nobody should be touching them so we should be safe
  	 */
++<<<<<<< HEAD
 +	memmap_init_zone(nr_pages, nid, zone_idx(zone), start_pfn,
 +			 MEMINIT_HOTPLUG, altmap);
++=======
+ 	memmap_init_zone(nr_pages, nid, zone_idx(zone), start_pfn, 0,
+ 			 MEMINIT_HOTPLUG, altmap, migratetype);
++>>>>>>> dc2da7b45ffe (mm: memmap defer init doesn't work as expected)
  
  	set_zone_contiguous(zone);
  }
diff --cc mm/page_alloc.c
index e2eef4a004f9,bdbec4c98173..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -6010,10 -6112,15 +6012,16 @@@ overlap_memmap_init(unsigned long zone
   * Initially all pages are reserved - free ones are freed
   * up by memblock_free_all() once the early boot process is
   * done. Non-atomic initialization, single-pass.
 - *
 - * All aligned pageblocks are initialized to the specified migratetype
 - * (usually MIGRATE_MOVABLE). Besides setting the migratetype, no related
 - * zone stats (e.g., nr_isolate_pageblock) are touched.
   */
  void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
++<<<<<<< HEAD
 +		unsigned long start_pfn, enum meminit_context context,
 +		struct vmem_altmap *altmap)
++=======
+ 		unsigned long start_pfn, unsigned long zone_end_pfn,
+ 		enum meminit_context context,
+ 		struct vmem_altmap *altmap, int migratetype)
++>>>>>>> dc2da7b45ffe (mm: memmap defer init doesn't work as expected)
  {
  	unsigned long pfn, end_pfn = start_pfn + size;
  	struct page *page;
@@@ -6163,8 -6268,8 +6171,13 @@@ void __meminit __weak memmap_init(unsig
  
  		if (end_pfn > start_pfn) {
  			size = end_pfn - start_pfn;
++<<<<<<< HEAD
 +			memmap_init_zone(size, nid, zone, start_pfn,
 +					 MEMINIT_EARLY, NULL);
++=======
+ 			memmap_init_zone(size, nid, zone, start_pfn, range_end_pfn,
+ 					 MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);
++>>>>>>> dc2da7b45ffe (mm: memmap defer init doesn't work as expected)
  		}
  	}
  }
* Unmerged path arch/ia64/mm/init.c
* Unmerged path include/linux/mm.h
* Unmerged path mm/memory_hotplug.c
* Unmerged path mm/page_alloc.c
