bpf: Add bitwise atomic instructions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Brendan Jackman <jackmanb@google.com>
commit 981f94c3e92146705baf97fb417a5ed1ab1a79a5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/981f94c3.failed

This adds instructions for

atomic[64]_[fetch_]and
atomic[64]_[fetch_]or
atomic[64]_[fetch_]xor

All these operations are isomorphic enough to implement with the same
verifier, interpreter, and x86 JIT code, hence being a single commit.

The main interesting thing here is that x86 doesn't directly support
the fetch_ version these operations, so we need to generate a CMPXCHG
loop in the JIT. This requires the use of two temporary registers,
IIUC it's safe to use BPF_REG_AX and x86's AUX_REG for this purpose.

	Signed-off-by: Brendan Jackman <jackmanb@google.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Yonghong Song <yhs@fb.com>
Link: https://lore.kernel.org/bpf/20210114181751.768687-10-jackmanb@google.com
(cherry picked from commit 981f94c3e92146705baf97fb417a5ed1ab1a79a5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
#	include/linux/filter.h
#	kernel/bpf/core.c
#	kernel/bpf/disasm.c
#	kernel/bpf/verifier.c
#	tools/include/linux/filter.h
diff --cc arch/x86/net/bpf_jit_comp.c
index dd4d8265af59,1d4d50199293..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -799,6 -795,49 +799,52 @@@ static void emit_stx(u8 **pprog, u32 si
  	*pprog = prog;
  }
  
++<<<<<<< HEAD
++=======
+ static int emit_atomic(u8 **pprog, u8 atomic_op,
+ 		       u32 dst_reg, u32 src_reg, s16 off, u8 bpf_size)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	EMIT1(0xF0); /* lock prefix */
+ 
+ 	maybe_emit_mod(&prog, dst_reg, src_reg, bpf_size == BPF_DW);
+ 
+ 	/* emit opcode */
+ 	switch (atomic_op) {
+ 	case BPF_ADD:
+ 	case BPF_SUB:
+ 	case BPF_AND:
+ 	case BPF_OR:
+ 	case BPF_XOR:
+ 		/* lock *(u32/u64*)(dst_reg + off) <op>= src_reg */
+ 		EMIT1(simple_alu_opcodes[atomic_op]);
+ 		break;
+ 	case BPF_ADD | BPF_FETCH:
+ 		/* src_reg = atomic_fetch_add(dst_reg + off, src_reg); */
+ 		EMIT2(0x0F, 0xC1);
+ 		break;
+ 	case BPF_XCHG:
+ 		/* src_reg = atomic_xchg(dst_reg + off, src_reg); */
+ 		EMIT1(0x87);
+ 		break;
+ 	case BPF_CMPXCHG:
+ 		/* r0 = atomic_cmpxchg(dst_reg + off, r0, src_reg); */
+ 		EMIT2(0x0F, 0xB1);
+ 		break;
+ 	default:
+ 		pr_err("bpf_jit: unknown atomic opcode %02x\n", atomic_op);
+ 		return -EFAULT;
+ 	}
+ 
+ 	emit_insn_suffix(&prog, dst_reg, src_reg, off);
+ 
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  static bool ex_handler_bpf(const struct exception_table_entry *x,
  			   struct pt_regs *regs, int trapnr,
  			   unsigned long error_code, unsigned long fault_addr)
@@@ -1254,18 -1294,56 +1300,71 @@@ st:			if (is_imm8(insn->off)
  			}
  			break;
  
++<<<<<<< HEAD
 +			/* STX XADD: lock *(u32*)(dst_reg + off) += src_reg */
 +		case BPF_STX | BPF_XADD | BPF_W:
 +			/* Emit 'lock add dword ptr [rax + off], eax' */
 +			if (is_ereg(dst_reg) || is_ereg(src_reg))
 +				EMIT3(0xF0, add_2mod(0x40, dst_reg, src_reg), 0x01);
 +			else
 +				EMIT2(0xF0, 0x01);
 +			goto xadd;
 +		case BPF_STX | BPF_XADD | BPF_DW:
 +			EMIT3(0xF0, add_2mod(0x48, dst_reg, src_reg), 0x01);
 +xadd:
 +			emit_modrm_dstoff(&prog, dst_reg, src_reg, insn->off);
++=======
+ 		case BPF_STX | BPF_ATOMIC | BPF_W:
+ 		case BPF_STX | BPF_ATOMIC | BPF_DW:
+ 			if (insn->imm == (BPF_AND | BPF_FETCH) ||
+ 			    insn->imm == (BPF_OR | BPF_FETCH) ||
+ 			    insn->imm == (BPF_XOR | BPF_FETCH)) {
+ 				u8 *branch_target;
+ 				bool is64 = BPF_SIZE(insn->code) == BPF_DW;
+ 
+ 				/*
+ 				 * Can't be implemented with a single x86 insn.
+ 				 * Need to do a CMPXCHG loop.
+ 				 */
+ 
+ 				/* Will need RAX as a CMPXCHG operand so save R0 */
+ 				emit_mov_reg(&prog, true, BPF_REG_AX, BPF_REG_0);
+ 				branch_target = prog;
+ 				/* Load old value */
+ 				emit_ldx(&prog, BPF_SIZE(insn->code),
+ 					 BPF_REG_0, dst_reg, insn->off);
+ 				/*
+ 				 * Perform the (commutative) operation locally,
+ 				 * put the result in the AUX_REG.
+ 				 */
+ 				emit_mov_reg(&prog, is64, AUX_REG, BPF_REG_0);
+ 				maybe_emit_mod(&prog, AUX_REG, src_reg, is64);
+ 				EMIT2(simple_alu_opcodes[BPF_OP(insn->imm)],
+ 				      add_2reg(0xC0, AUX_REG, src_reg));
+ 				/* Attempt to swap in new value */
+ 				err = emit_atomic(&prog, BPF_CMPXCHG,
+ 						  dst_reg, AUX_REG, insn->off,
+ 						  BPF_SIZE(insn->code));
+ 				if (WARN_ON(err))
+ 					return err;
+ 				/*
+ 				 * ZF tells us whether we won the race. If it's
+ 				 * cleared we need to try again.
+ 				 */
+ 				EMIT2(X86_JNE, -(prog - branch_target) - 2);
+ 				/* Return the pre-modification value */
+ 				emit_mov_reg(&prog, is64, src_reg, BPF_REG_0);
+ 				/* Restore R0 after clobbering RAX */
+ 				emit_mov_reg(&prog, true, BPF_REG_0, BPF_REG_AX);
+ 				break;
+ 
+ 			}
+ 
+ 			err = emit_atomic(&prog, insn->imm, dst_reg, src_reg,
+ 						  insn->off, BPF_SIZE(insn->code));
+ 			if (err)
+ 				return err;
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  			break;
  
  			/* call */
diff --cc include/linux/filter.h
index 3267b6630b89,7fdce5407214..000000000000
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@@ -259,11 -259,25 +259,30 @@@ static inline bool insn_is_zext(const s
  		.off   = OFF,					\
  		.imm   = 0 })
  
 +/* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
  
++<<<<<<< HEAD
 +#define BPF_STX_XADD(SIZE, DST, SRC, OFF)			\
++=======
+ /*
+  * Atomic operations:
+  *
+  *   BPF_ADD                  *(uint *) (dst_reg + off16) += src_reg
+  *   BPF_AND                  *(uint *) (dst_reg + off16) &= src_reg
+  *   BPF_OR                   *(uint *) (dst_reg + off16) |= src_reg
+  *   BPF_XOR                  *(uint *) (dst_reg + off16) ^= src_reg
+  *   BPF_ADD | BPF_FETCH      src_reg = atomic_fetch_add(dst_reg + off16, src_reg);
+  *   BPF_AND | BPF_FETCH      src_reg = atomic_fetch_and(dst_reg + off16, src_reg);
+  *   BPF_OR | BPF_FETCH       src_reg = atomic_fetch_or(dst_reg + off16, src_reg);
+  *   BPF_XOR | BPF_FETCH      src_reg = atomic_fetch_xor(dst_reg + off16, src_reg);
+  *   BPF_XCHG                 src_reg = atomic_xchg(dst_reg + off16, src_reg)
+  *   BPF_CMPXCHG              r0 = atomic_cmpxchg(dst_reg + off16, r0, src_reg)
+  */
+ 
+ #define BPF_ATOMIC_OP(SIZE, OP, DST, SRC, OFF)			\
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  	((struct bpf_insn) {					\
 -		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_ATOMIC,	\
 +		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_XADD,	\
  		.dst_reg = DST,					\
  		.src_reg = SRC,					\
  		.off   = OFF,					\
diff --cc kernel/bpf/core.c
index a915516dd706,5bbd4884ff7a..000000000000
--- a/kernel/bpf/core.c
+++ b/kernel/bpf/core.c
@@@ -1631,13 -1618,59 +1631,69 @@@ out
  	LDX_PROBE(DW, 8)
  #undef LDX_PROBE
  
++<<<<<<< HEAD
 +	STX_XADD_W: /* lock xadd *(u32 *)(dst_reg + off16) += src_reg */
 +		atomic_add((u32) SRC, (atomic_t *)(unsigned long)
 +			   (DST + insn->off));
 +		CONT;
 +	STX_XADD_DW: /* lock xadd *(u64 *)(dst_reg + off16) += src_reg */
 +		atomic64_add((u64) SRC, (atomic64_t *)(unsigned long)
 +			     (DST + insn->off));
++=======
+ #define ATOMIC_ALU_OP(BOP, KOP)						\
+ 		case BOP:						\
+ 			if (BPF_SIZE(insn->code) == BPF_W)		\
+ 				atomic_##KOP((u32) SRC, (atomic_t *)(unsigned long) \
+ 					     (DST + insn->off));	\
+ 			else						\
+ 				atomic64_##KOP((u64) SRC, (atomic64_t *)(unsigned long) \
+ 					       (DST + insn->off));	\
+ 			break;						\
+ 		case BOP | BPF_FETCH:					\
+ 			if (BPF_SIZE(insn->code) == BPF_W)		\
+ 				SRC = (u32) atomic_fetch_##KOP(		\
+ 					(u32) SRC,			\
+ 					(atomic_t *)(unsigned long) (DST + insn->off)); \
+ 			else						\
+ 				SRC = (u64) atomic64_fetch_##KOP(	\
+ 					(u64) SRC,			\
+ 					(atomic64_t *)(unsigned long) (DST + insn->off)); \
+ 			break;
+ 
+ 	STX_ATOMIC_DW:
+ 	STX_ATOMIC_W:
+ 		switch (IMM) {
+ 		ATOMIC_ALU_OP(BPF_ADD, add)
+ 		ATOMIC_ALU_OP(BPF_AND, and)
+ 		ATOMIC_ALU_OP(BPF_OR, or)
+ 		ATOMIC_ALU_OP(BPF_XOR, xor)
+ #undef ATOMIC_ALU_OP
+ 
+ 		case BPF_XCHG:
+ 			if (BPF_SIZE(insn->code) == BPF_W)
+ 				SRC = (u32) atomic_xchg(
+ 					(atomic_t *)(unsigned long) (DST + insn->off),
+ 					(u32) SRC);
+ 			else
+ 				SRC = (u64) atomic64_xchg(
+ 					(atomic64_t *)(unsigned long) (DST + insn->off),
+ 					(u64) SRC);
+ 			break;
+ 		case BPF_CMPXCHG:
+ 			if (BPF_SIZE(insn->code) == BPF_W)
+ 				BPF_R0 = (u32) atomic_cmpxchg(
+ 					(atomic_t *)(unsigned long) (DST + insn->off),
+ 					(u32) BPF_R0, (u32) SRC);
+ 			else
+ 				BPF_R0 = (u64) atomic64_cmpxchg(
+ 					(atomic64_t *)(unsigned long) (DST + insn->off),
+ 					(u64) BPF_R0, (u64) SRC);
+ 			break;
+ 
+ 		default:
+ 			goto default_label;
+ 		}
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  		CONT;
  
  	default_label:
diff --cc kernel/bpf/disasm.c
index d9ce383c0f9c,19ff8fed7f4b..000000000000
--- a/kernel/bpf/disasm.c
+++ b/kernel/bpf/disasm.c
@@@ -161,14 -160,44 +168,52 @@@ void print_bpf_insn(const struct bpf_in
  				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
  				insn->dst_reg,
  				insn->off, insn->src_reg);
++<<<<<<< HEAD
 +		else if (BPF_MODE(insn->code) == BPF_XADD)
 +			verbose(cbs->private_data, "(%02x) lock *(%s *)(r%d %+d) += r%d\n",
++=======
+ 		else if (BPF_MODE(insn->code) == BPF_ATOMIC &&
+ 			 (insn->imm == BPF_ADD || insn->imm == BPF_ADD ||
+ 			  insn->imm == BPF_OR || insn->imm == BPF_XOR)) {
+ 			verbose(cbs->private_data, "(%02x) lock *(%s *)(r%d %+d) %s r%d\n",
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  				insn->code,
  				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
  				insn->dst_reg, insn->off,
+ 				bpf_alu_string[BPF_OP(insn->imm) >> 4],
  				insn->src_reg);
++<<<<<<< HEAD
 +		else
++=======
+ 		} else if (BPF_MODE(insn->code) == BPF_ATOMIC &&
+ 			   (insn->imm == (BPF_ADD | BPF_FETCH) ||
+ 			    insn->imm == (BPF_AND | BPF_FETCH) ||
+ 			    insn->imm == (BPF_OR | BPF_FETCH) ||
+ 			    insn->imm == (BPF_XOR | BPF_FETCH))) {
+ 			verbose(cbs->private_data, "(%02x) r%d = atomic%s_fetch_%s((%s *)(r%d %+d), r%d)\n",
+ 				insn->code, insn->src_reg,
+ 				BPF_SIZE(insn->code) == BPF_DW ? "64" : "",
+ 				bpf_atomic_alu_string[BPF_OP(insn->imm) >> 4],
+ 				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
+ 				insn->dst_reg, insn->off, insn->src_reg);
+ 		} else if (BPF_MODE(insn->code) == BPF_ATOMIC &&
+ 			   insn->imm == BPF_CMPXCHG) {
+ 			verbose(cbs->private_data, "(%02x) r0 = atomic%s_cmpxchg((%s *)(r%d %+d), r0, r%d)\n",
+ 				insn->code,
+ 				BPF_SIZE(insn->code) == BPF_DW ? "64" : "",
+ 				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
+ 				insn->dst_reg, insn->off,
+ 				insn->src_reg);
+ 		} else if (BPF_MODE(insn->code) == BPF_ATOMIC &&
+ 			   insn->imm == BPF_XCHG) {
+ 			verbose(cbs->private_data, "(%02x) r%d = atomic%s_xchg((%s *)(r%d %+d), r%d)\n",
+ 				insn->code, insn->src_reg,
+ 				BPF_SIZE(insn->code) == BPF_DW ? "64" : "",
+ 				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
+ 				insn->dst_reg, insn->off, insn->src_reg);
+ 		} else {
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  			verbose(cbs->private_data, "BUG_%02x\n", insn->code);
 -		}
  	} else if (class == BPF_ST) {
  		if (BPF_MODE(insn->code) != BPF_MEM) {
  			verbose(cbs->private_data, "BUG_st_%02x\n", insn->code);
diff --cc kernel/bpf/verifier.c
index 628ac51f9efd,0f82d5d46e2c..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -3602,13 -3604,30 +3602,35 @@@ static int check_mem_access(struct bpf_
  	return err;
  }
  
 -static int check_atomic(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)
 +static int check_xadd(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)
  {
 -	int load_reg;
  	int err;
  
++<<<<<<< HEAD
 +	if ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||
 +	    insn->imm != 0) {
 +		verbose(env, "BPF_XADD uses reserved fields\n");
++=======
+ 	switch (insn->imm) {
+ 	case BPF_ADD:
+ 	case BPF_ADD | BPF_FETCH:
+ 	case BPF_AND:
+ 	case BPF_AND | BPF_FETCH:
+ 	case BPF_OR:
+ 	case BPF_OR | BPF_FETCH:
+ 	case BPF_XOR:
+ 	case BPF_XOR | BPF_FETCH:
+ 	case BPF_XCHG:
+ 	case BPF_CMPXCHG:
+ 		break;
+ 	default:
+ 		verbose(env, "BPF_ATOMIC uses invalid atomic opcode %02x\n", insn->imm);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) {
+ 		verbose(env, "invalid atomic operand size\n");
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  		return -EINVAL;
  	}
  
diff --cc tools/include/linux/filter.h
index ca28b6ab8db7,736bdeccdfe4..000000000000
--- a/tools/include/linux/filter.h
+++ b/tools/include/linux/filter.h
@@@ -169,11 -169,24 +169,28 @@@
  		.off   = OFF,					\
  		.imm   = 0 })
  
++<<<<<<< HEAD
 +/* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
++=======
+ /*
+  * Atomic operations:
+  *
+  *   BPF_ADD                  *(uint *) (dst_reg + off16) += src_reg
+  *   BPF_AND                  *(uint *) (dst_reg + off16) &= src_reg
+  *   BPF_OR                   *(uint *) (dst_reg + off16) |= src_reg
+  *   BPF_XOR                  *(uint *) (dst_reg + off16) ^= src_reg
+  *   BPF_ADD | BPF_FETCH      src_reg = atomic_fetch_add(dst_reg + off16, src_reg);
+  *   BPF_AND | BPF_FETCH      src_reg = atomic_fetch_and(dst_reg + off16, src_reg);
+  *   BPF_OR | BPF_FETCH       src_reg = atomic_fetch_or(dst_reg + off16, src_reg);
+  *   BPF_XOR | BPF_FETCH      src_reg = atomic_fetch_xor(dst_reg + off16, src_reg);
+  *   BPF_XCHG                 src_reg = atomic_xchg(dst_reg + off16, src_reg)
+  *   BPF_CMPXCHG              r0 = atomic_cmpxchg(dst_reg + off16, r0, src_reg)
+  */
++>>>>>>> 981f94c3e921 (bpf: Add bitwise atomic instructions)
  
 -#define BPF_ATOMIC_OP(SIZE, OP, DST, SRC, OFF)			\
 +#define BPF_STX_XADD(SIZE, DST, SRC, OFF)			\
  	((struct bpf_insn) {					\
 -		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_ATOMIC,	\
 +		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_XADD,	\
  		.dst_reg = DST,					\
  		.src_reg = SRC,					\
  		.off   = OFF,					\
* Unmerged path arch/x86/net/bpf_jit_comp.c
* Unmerged path include/linux/filter.h
* Unmerged path kernel/bpf/core.c
* Unmerged path kernel/bpf/disasm.c
* Unmerged path kernel/bpf/verifier.c
* Unmerged path tools/include/linux/filter.h
