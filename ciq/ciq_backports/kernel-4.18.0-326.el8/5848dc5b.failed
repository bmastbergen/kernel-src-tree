dma-debug: remove debug_dma_assert_idle() function

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Linus Torvalds <torvalds@linux-foundation.org>
commit 5848dc5b1b76d83599dcec1b39f188a7cbdca7e2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/5848dc5b.failed

This remoes the code from the COW path to call debug_dma_assert_idle(),
which was added many years ago.

Google shows that it hasn't caught anything in the 6+ years we've had it
apart from a false positive, and Hugh just noticed how it had a very
unfortunate spinlock serialization in the COW path.

He fixed that issue the previous commit (a85ffd59bd36: "dma-debug: fix
debug_dma_assert_idle(), use rcu_read_lock()"), but let's see if anybody
even notices when we remove this function entirely.

NOTE! We keep the dma tracking infrastructure that was added by the
commit that introduced it.  Partly to make it easier to resurrect this
debug code if we ever deside to, and partly because that tracking by pfn
and offset looks quite reasonable.

The problem with this debug code was simply that it was expensive and
didn't seem worth it, not that it was wrong per se.

	Acked-by: Dan Williams <dan.j.williams@intel.com>
	Acked-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5848dc5b1b76d83599dcec1b39f188a7cbdca7e2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/Kconfig
#	mm/memory.c
diff --cc kernel/dma/Kconfig
index 351e3cb0b8ed,5732b2b3ef17..000000000000
--- a/kernel/dma/Kconfig
+++ b/kernel/dma/Kconfig
@@@ -91,3 -99,111 +91,114 @@@ config DMA_DIRECT_REMA
  	bool
  	select DMA_REMAP
  	select DMA_COHERENT_POOL
++<<<<<<< HEAD
++=======
+ 
+ config DMA_CMA
+ 	bool "DMA Contiguous Memory Allocator"
+ 	depends on HAVE_DMA_CONTIGUOUS && CMA
+ 	help
+ 	  This enables the Contiguous Memory Allocator which allows drivers
+ 	  to allocate big physically-contiguous blocks of memory for use with
+ 	  hardware components that do not support I/O map nor scatter-gather.
+ 
+ 	  You can disable CMA by specifying "cma=0" on the kernel's command
+ 	  line.
+ 
+ 	  For more information see <include/linux/dma-contiguous.h>.
+ 	  If unsure, say "n".
+ 
+ if  DMA_CMA
+ comment "Default contiguous memory area size:"
+ 
+ config CMA_SIZE_MBYTES
+ 	int "Size in Mega Bytes"
+ 	depends on !CMA_SIZE_SEL_PERCENTAGE
+ 	default 0 if X86
+ 	default 16
+ 	help
+ 	  Defines the size (in MiB) of the default memory area for Contiguous
+ 	  Memory Allocator.  If the size of 0 is selected, CMA is disabled by
+ 	  default, but it can be enabled by passing cma=size[MG] to the kernel.
+ 
+ 
+ config CMA_SIZE_PERCENTAGE
+ 	int "Percentage of total memory"
+ 	depends on !CMA_SIZE_SEL_MBYTES
+ 	default 0 if X86
+ 	default 10
+ 	help
+ 	  Defines the size of the default memory area for Contiguous Memory
+ 	  Allocator as a percentage of the total memory in the system.
+ 	  If 0 percent is selected, CMA is disabled by default, but it can be
+ 	  enabled by passing cma=size[MG] to the kernel.
+ 
+ choice
+ 	prompt "Selected region size"
+ 	default CMA_SIZE_SEL_MBYTES
+ 
+ config CMA_SIZE_SEL_MBYTES
+ 	bool "Use mega bytes value only"
+ 
+ config CMA_SIZE_SEL_PERCENTAGE
+ 	bool "Use percentage value only"
+ 
+ config CMA_SIZE_SEL_MIN
+ 	bool "Use lower value (minimum)"
+ 
+ config CMA_SIZE_SEL_MAX
+ 	bool "Use higher value (maximum)"
+ 
+ endchoice
+ 
+ config CMA_ALIGNMENT
+ 	int "Maximum PAGE_SIZE order of alignment for contiguous buffers"
+ 	range 4 12
+ 	default 8
+ 	help
+ 	  DMA mapping framework by default aligns all buffers to the smallest
+ 	  PAGE_SIZE order which is greater than or equal to the requested buffer
+ 	  size. This works well for buffers up to a few hundreds kilobytes, but
+ 	  for larger buffers it just a memory waste. With this parameter you can
+ 	  specify the maximum PAGE_SIZE order for contiguous buffers. Larger
+ 	  buffers will be aligned only to this specified order. The order is
+ 	  expressed as a power of two multiplied by the PAGE_SIZE.
+ 
+ 	  For example, if your system defaults to 4KiB pages, the order value
+ 	  of 8 means that the buffers will be aligned up to 1MiB only.
+ 
+ 	  If unsure, leave the default value "8".
+ 
+ endif
+ 
+ config DMA_API_DEBUG
+ 	bool "Enable debugging of DMA-API usage"
+ 	select NEED_DMA_MAP_STATE
+ 	help
+ 	  Enable this option to debug the use of the DMA API by device drivers.
+ 	  With this option you will be able to detect common bugs in device
+ 	  drivers like double-freeing of DMA mappings or freeing mappings that
+ 	  were never allocated.
+ 
+ 	  This option causes a performance degradation.  Use only if you want to
+ 	  debug device drivers and dma interactions.
+ 
+ 	  If unsure, say N.
+ 
+ config DMA_API_DEBUG_SG
+ 	bool "Debug DMA scatter-gather usage"
+ 	default y
+ 	depends on DMA_API_DEBUG
+ 	help
+ 	  Perform extra checking that callers of dma_map_sg() have respected the
+ 	  appropriate segment length/boundary limits for the given device when
+ 	  preparing DMA scatterlists.
+ 
+ 	  This is particularly likely to have been overlooked in cases where the
+ 	  dma_map_sg() API is used for general bulk mapping of pages rather than
+ 	  preparing literal scatter-gather descriptors, where there is a risk of
+ 	  unexpected behaviour from DMA API implementations if the scatterlist
+ 	  is technically out-of-spec.
+ 
+ 	  If unsure, say N.
++>>>>>>> 5848dc5b1b76 (dma-debug: remove debug_dma_assert_idle() function)
diff --cc mm/memory.c
index af1aa66a977c,d3c3bbd65a7e..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2408,9 -2400,21 +2408,24 @@@ static inline int pte_unmap_same(struc
  	return same;
  }
  
 -static inline bool cow_user_page(struct page *dst, struct page *src,
 -				 struct vm_fault *vmf)
 +static inline void cow_user_page(struct page *dst, struct page *src, unsigned long va, struct vm_area_struct *vma)
  {
++<<<<<<< HEAD
 +	debug_dma_assert_idle(src);
++=======
+ 	bool ret;
+ 	void *kaddr;
+ 	void __user *uaddr;
+ 	bool locked = false;
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	unsigned long addr = vmf->address;
+ 
+ 	if (likely(src)) {
+ 		copy_user_highpage(dst, src, addr, vma);
+ 		return true;
+ 	}
++>>>>>>> 5848dc5b1b76 (dma-debug: remove debug_dma_assert_idle() function)
  
  	/*
  	 * If the source page was a PFN mapping, we don't have
diff --git a/include/linux/dma-debug.h b/include/linux/dma-debug.h
index cb422cbe587d..ddbe92e3a69a 100644
--- a/include/linux/dma-debug.h
+++ b/include/linux/dma-debug.h
@@ -79,8 +79,6 @@ extern void debug_dma_sync_sg_for_device(struct device *dev,
 
 extern void debug_dma_dump_mappings(struct device *dev);
 
-extern void debug_dma_assert_idle(struct page *page);
-
 #else /* CONFIG_DMA_API_DEBUG */
 
 static inline void dma_debug_add_bus(struct bus_type *bus)
@@ -169,10 +167,6 @@ static inline void debug_dma_dump_mappings(struct device *dev)
 {
 }
 
-static inline void debug_dma_assert_idle(struct page *page)
-{
-}
-
 #endif /* CONFIG_DMA_API_DEBUG */
 
 #endif /* __DMA_DEBUG_H */
* Unmerged path kernel/dma/Kconfig
diff --git a/kernel/dma/debug.c b/kernel/dma/debug.c
index c431b5aeefae..5f73b2a0d350 100644
--- a/kernel/dma/debug.c
+++ b/kernel/dma/debug.c
@@ -460,9 +460,6 @@ void debug_dma_dump_mappings(struct device *dev)
  * dma_active_cacheline entry to track per event.  dma_map_sg(), on the
  * other hand, consumes a single dma_debug_entry, but inserts 'nents'
  * entries into the tree.
- *
- * At any time debug_dma_assert_idle() can be called to trigger a
- * warning if any cachelines in the given page are in the active set.
  */
 static RADIX_TREE(dma_active_cacheline, GFP_NOWAIT);
 static DEFINE_SPINLOCK(radix_lock);
@@ -509,10 +506,7 @@ static void active_cacheline_inc_overlap(phys_addr_t cln)
 	overlap = active_cacheline_set_overlap(cln, ++overlap);
 
 	/* If we overflowed the overlap counter then we're potentially
-	 * leaking dma-mappings.  Otherwise, if maps and unmaps are
-	 * balanced then this overflow may cause false negatives in
-	 * debug_dma_assert_idle() as the cacheline may be marked idle
-	 * prematurely.
+	 * leaking dma-mappings.
 	 */
 	WARN_ONCE(overlap > ACTIVE_CACHELINE_MAX_OVERLAP,
 		  pr_fmt("exceeded %d overlapping mappings of cacheline %pa\n"),
@@ -567,44 +561,6 @@ static void active_cacheline_remove(struct dma_debug_entry *entry)
 	spin_unlock_irqrestore(&radix_lock, flags);
 }
 
-/**
- * debug_dma_assert_idle() - assert that a page is not undergoing dma
- * @page: page to lookup in the dma_active_cacheline tree
- *
- * Place a call to this routine in cases where the cpu touching the page
- * before the dma completes (page is dma_unmapped) will lead to data
- * corruption.
- */
-void debug_dma_assert_idle(struct page *page)
-{
-	struct dma_debug_entry *entry;
-	unsigned long pfn;
-	phys_addr_t cln;
-
-	if (dma_debug_disabled())
-		return;
-
-	if (!page)
-		return;
-
-	pfn = page_to_pfn(page);
-	cln = (phys_addr_t) pfn << CACHELINE_PER_PAGE_SHIFT;
-
-	rcu_read_lock();
-	if (!radix_tree_gang_lookup(&dma_active_cacheline, (void **) &entry,
-				    cln, 1) || entry->pfn != pfn)
-		entry = NULL;
-	rcu_read_unlock();
-
-	if (!entry)
-		return;
-
-	cln = to_cacheline_number(entry);
-	err_printk(entry->dev, entry,
-		   "cpu touching an active dma mapped cacheline [cln=%pa]\n",
-		   &cln);
-}
-
 /*
  * Wrapper function for adding an entry to the hash.
  * This function takes care of locking itself.
* Unmerged path mm/memory.c
