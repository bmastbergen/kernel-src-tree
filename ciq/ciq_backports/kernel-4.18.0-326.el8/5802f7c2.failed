SUNRPC: When expanding the buffer, we may need grow the sparse pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Trond Myklebust <trond.myklebust@hammerspace.com>
commit 5802f7c2a6b876b2810e3e9f26d719961f12e251
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/5802f7c2.failed

If we're shifting the page data to the right, and this happens to be a
sparse page array, then we may need to allocate new pages in order to
receive the data.

	Reported-by: "Mkrtchyan, Tigran" <tigran.mkrtchyan@desy.de>
	Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>
(cherry picked from commit 5802f7c2a6b876b2810e3e9f26d719961f12e251)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xdr.c
diff --cc net/sunrpc/xdr.c
index 09eff12df7e6,60d4442c5273..000000000000
--- a/net/sunrpc/xdr.c
+++ b/net/sunrpc/xdr.c
@@@ -515,6 -478,303 +515,306 @@@ _zero_pages(struct page **pages, size_
  	} while ((len -= zero) != 0);
  }
  
++<<<<<<< HEAD
++=======
+ static unsigned int xdr_buf_pages_fill_sparse(const struct xdr_buf *buf,
+ 					      unsigned int buflen, gfp_t gfp)
+ {
+ 	unsigned int i, npages, pagelen;
+ 
+ 	if (!(buf->flags & XDRBUF_SPARSE_PAGES))
+ 		return buflen;
+ 	if (buflen <= buf->head->iov_len)
+ 		return buflen;
+ 	pagelen = buflen - buf->head->iov_len;
+ 	if (pagelen > buf->page_len)
+ 		pagelen = buf->page_len;
+ 	npages = (pagelen + buf->page_base + PAGE_SIZE - 1) >> PAGE_SHIFT;
+ 	for (i = 0; i < npages; i++) {
+ 		if (!buf->pages[i])
+ 			continue;
+ 		buf->pages[i] = alloc_page(gfp);
+ 		if (likely(buf->pages[i]))
+ 			continue;
+ 		buflen -= pagelen;
+ 		pagelen = i << PAGE_SHIFT;
+ 		if (pagelen > buf->page_base)
+ 			buflen += pagelen - buf->page_base;
+ 		break;
+ 	}
+ 	return buflen;
+ }
+ 
+ static void xdr_buf_try_expand(struct xdr_buf *buf, unsigned int len)
+ {
+ 	struct kvec *head = buf->head;
+ 	struct kvec *tail = buf->tail;
+ 	unsigned int sum = head->iov_len + buf->page_len + tail->iov_len;
+ 	unsigned int free_space, newlen;
+ 
+ 	if (sum > buf->len) {
+ 		free_space = min_t(unsigned int, sum - buf->len, len);
+ 		newlen = xdr_buf_pages_fill_sparse(buf, buf->len + free_space,
+ 						   GFP_KERNEL);
+ 		free_space = newlen - buf->len;
+ 		buf->len = newlen;
+ 		len -= free_space;
+ 		if (!len)
+ 			return;
+ 	}
+ 
+ 	if (buf->buflen > sum) {
+ 		/* Expand the tail buffer */
+ 		free_space = min_t(unsigned int, buf->buflen - sum, len);
+ 		tail->iov_len += free_space;
+ 		buf->len += free_space;
+ 	}
+ }
+ 
+ static void xdr_buf_tail_copy_right(const struct xdr_buf *buf,
+ 				    unsigned int base, unsigned int len,
+ 				    unsigned int shift)
+ {
+ 	const struct kvec *tail = buf->tail;
+ 	unsigned int to = base + shift;
+ 
+ 	if (to >= tail->iov_len)
+ 		return;
+ 	if (len + to > tail->iov_len)
+ 		len = tail->iov_len - to;
+ 	memmove(tail->iov_base + to, tail->iov_base + base, len);
+ }
+ 
+ static void xdr_buf_pages_copy_right(const struct xdr_buf *buf,
+ 				     unsigned int base, unsigned int len,
+ 				     unsigned int shift)
+ {
+ 	const struct kvec *tail = buf->tail;
+ 	unsigned int to = base + shift;
+ 	unsigned int pglen = 0;
+ 	unsigned int talen = 0, tato = 0;
+ 
+ 	if (base >= buf->page_len)
+ 		return;
+ 	if (len > buf->page_len - base)
+ 		len = buf->page_len - base;
+ 	if (to >= buf->page_len) {
+ 		tato = to - buf->page_len;
+ 		if (tail->iov_len >= len + tato)
+ 			talen = len;
+ 		else if (tail->iov_len > tato)
+ 			talen = tail->iov_len - tato;
+ 	} else if (len + to >= buf->page_len) {
+ 		pglen = buf->page_len - to;
+ 		talen = len - pglen;
+ 		if (talen > tail->iov_len)
+ 			talen = tail->iov_len;
+ 	} else
+ 		pglen = len;
+ 
+ 	_copy_from_pages(tail->iov_base + tato, buf->pages,
+ 			 buf->page_base + base + pglen, talen);
+ 	_shift_data_right_pages(buf->pages, buf->page_base + to,
+ 				buf->page_base + base, pglen);
+ }
+ 
+ static void xdr_buf_head_copy_right(const struct xdr_buf *buf,
+ 				    unsigned int base, unsigned int len,
+ 				    unsigned int shift)
+ {
+ 	const struct kvec *head = buf->head;
+ 	const struct kvec *tail = buf->tail;
+ 	unsigned int to = base + shift;
+ 	unsigned int pglen = 0, pgto = 0;
+ 	unsigned int talen = 0, tato = 0;
+ 
+ 	if (base >= head->iov_len)
+ 		return;
+ 	if (len > head->iov_len - base)
+ 		len = head->iov_len - base;
+ 	if (to >= buf->page_len + head->iov_len) {
+ 		tato = to - buf->page_len - head->iov_len;
+ 		talen = len;
+ 	} else if (to >= head->iov_len) {
+ 		pgto = to - head->iov_len;
+ 		pglen = len;
+ 		if (pgto + pglen > buf->page_len) {
+ 			talen = pgto + pglen - buf->page_len;
+ 			pglen -= talen;
+ 		}
+ 	} else {
+ 		pglen = len - to;
+ 		if (pglen > buf->page_len) {
+ 			talen = pglen - buf->page_len;
+ 			pglen = buf->page_len;
+ 		}
+ 	}
+ 
+ 	len -= talen;
+ 	base += len;
+ 	if (talen + tato > tail->iov_len)
+ 		talen = tail->iov_len > tato ? tail->iov_len - tato : 0;
+ 	memcpy(tail->iov_base + tato, head->iov_base + base, talen);
+ 
+ 	len -= pglen;
+ 	base -= pglen;
+ 	_copy_to_pages(buf->pages, buf->page_base + pgto, head->iov_base + base,
+ 		       pglen);
+ 
+ 	base -= len;
+ 	memmove(head->iov_base + to, head->iov_base + base, len);
+ }
+ 
+ static void xdr_buf_tail_shift_right(const struct xdr_buf *buf,
+ 				     unsigned int base, unsigned int len,
+ 				     unsigned int shift)
+ {
+ 	const struct kvec *tail = buf->tail;
+ 
+ 	if (base >= tail->iov_len || !shift || !len)
+ 		return;
+ 	xdr_buf_tail_copy_right(buf, base, len, shift);
+ }
+ 
+ static void xdr_buf_pages_shift_right(const struct xdr_buf *buf,
+ 				      unsigned int base, unsigned int len,
+ 				      unsigned int shift)
+ {
+ 	if (!shift || !len)
+ 		return;
+ 	if (base >= buf->page_len) {
+ 		xdr_buf_tail_shift_right(buf, base - buf->page_len, len, shift);
+ 		return;
+ 	}
+ 	if (base + len > buf->page_len)
+ 		xdr_buf_tail_shift_right(buf, 0, base + len - buf->page_len,
+ 					 shift);
+ 	xdr_buf_pages_copy_right(buf, base, len, shift);
+ }
+ 
+ static void xdr_buf_head_shift_right(const struct xdr_buf *buf,
+ 				     unsigned int base, unsigned int len,
+ 				     unsigned int shift)
+ {
+ 	const struct kvec *head = buf->head;
+ 
+ 	if (!shift)
+ 		return;
+ 	if (base >= head->iov_len) {
+ 		xdr_buf_pages_shift_right(buf, head->iov_len - base, len,
+ 					  shift);
+ 		return;
+ 	}
+ 	if (base + len > head->iov_len)
+ 		xdr_buf_pages_shift_right(buf, 0, base + len - head->iov_len,
+ 					  shift);
+ 	xdr_buf_head_copy_right(buf, base, len, shift);
+ }
+ 
+ static void xdr_buf_tail_copy_left(const struct xdr_buf *buf, unsigned int base,
+ 				   unsigned int len, unsigned int shift)
+ {
+ 	const struct kvec *tail = buf->tail;
+ 
+ 	if (base >= tail->iov_len)
+ 		return;
+ 	if (len > tail->iov_len - base)
+ 		len = tail->iov_len - base;
+ 	/* Shift data into head */
+ 	if (shift > buf->page_len + base) {
+ 		const struct kvec *head = buf->head;
+ 		unsigned int hdto =
+ 			head->iov_len + buf->page_len + base - shift;
+ 		unsigned int hdlen = len;
+ 
+ 		if (WARN_ONCE(shift > head->iov_len + buf->page_len + base,
+ 			      "SUNRPC: Misaligned data.\n"))
+ 			return;
+ 		if (hdto + hdlen > head->iov_len)
+ 			hdlen = head->iov_len - hdto;
+ 		memcpy(head->iov_base + hdto, tail->iov_base + base, hdlen);
+ 		base += hdlen;
+ 		len -= hdlen;
+ 		if (!len)
+ 			return;
+ 	}
+ 	/* Shift data into pages */
+ 	if (shift > base) {
+ 		unsigned int pgto = buf->page_len + base - shift;
+ 		unsigned int pglen = len;
+ 
+ 		if (pgto + pglen > buf->page_len)
+ 			pglen = buf->page_len - pgto;
+ 		_copy_to_pages(buf->pages, buf->page_base + pgto,
+ 			       tail->iov_base + base, pglen);
+ 		base += pglen;
+ 		len -= pglen;
+ 		if (!len)
+ 			return;
+ 	}
+ 	memmove(tail->iov_base + base - shift, tail->iov_base + base, len);
+ }
+ 
+ static void xdr_buf_pages_copy_left(const struct xdr_buf *buf,
+ 				    unsigned int base, unsigned int len,
+ 				    unsigned int shift)
+ {
+ 	unsigned int pgto;
+ 
+ 	if (base >= buf->page_len)
+ 		return;
+ 	if (len > buf->page_len - base)
+ 		len = buf->page_len - base;
+ 	/* Shift data into head */
+ 	if (shift > base) {
+ 		const struct kvec *head = buf->head;
+ 		unsigned int hdto = head->iov_len + base - shift;
+ 		unsigned int hdlen = len;
+ 
+ 		if (WARN_ONCE(shift > head->iov_len + base,
+ 			      "SUNRPC: Misaligned data.\n"))
+ 			return;
+ 		if (hdto + hdlen > head->iov_len)
+ 			hdlen = head->iov_len - hdto;
+ 		_copy_from_pages(head->iov_base + hdto, buf->pages,
+ 				 buf->page_base + base, hdlen);
+ 		base += hdlen;
+ 		len -= hdlen;
+ 		if (!len)
+ 			return;
+ 	}
+ 	pgto = base - shift;
+ 	_shift_data_left_pages(buf->pages, buf->page_base + pgto,
+ 			       buf->page_base + base, len);
+ }
+ 
+ static void xdr_buf_tail_shift_left(const struct xdr_buf *buf,
+ 				    unsigned int base, unsigned int len,
+ 				    unsigned int shift)
+ {
+ 	if (!shift || !len)
+ 		return;
+ 	xdr_buf_tail_copy_left(buf, base, len, shift);
+ }
+ 
+ static void xdr_buf_pages_shift_left(const struct xdr_buf *buf,
+ 				     unsigned int base, unsigned int len,
+ 				     unsigned int shift)
+ {
+ 	if (!shift || !len)
+ 		return;
+ 	if (base >= buf->page_len) {
+ 		xdr_buf_tail_shift_left(buf, base - buf->page_len, len, shift);
+ 		return;
+ 	}
+ 	xdr_buf_pages_copy_left(buf, base, len, shift);
+ 	len += base;
+ 	if (len <= buf->page_len)
+ 		return;
+ 	xdr_buf_tail_copy_left(buf, 0, len - buf->page_len, shift);
+ }
+ 
++>>>>>>> 5802f7c2a6b8 (SUNRPC: When expanding the buffer, we may need grow the sparse pages)
  /**
   * xdr_shrink_bufhead
   * @buf: xdr_buf
* Unmerged path net/sunrpc/xdr.c
