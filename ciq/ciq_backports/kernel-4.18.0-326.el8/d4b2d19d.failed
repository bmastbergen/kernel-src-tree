RDMA/mlx5: Silence the overflow warning while building offset mask

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Leon Romanovsky <leonro@nvidia.com>
commit d4b2d19dc53ecb5ef4fe79cc2d4b7ae3413b2604
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/d4b2d19d.failed

Coverity reports "Potentially overflowing expression ..." warning, which
is correct thing to complain from the compiler point of view, but this is
not possible in the current code. Still, this is a small error as there
are some future situations that might need to use a 32 bit offset. Use ULL
so the calculation works up to 63.

Fixes: b045db62f6f6 ("RDMA/mlx5: Use ib_umem_find_best_pgoff() for SRQ")
Link: https://lore.kernel.org/r/20201125061704.6580-1-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit d4b2d19dc53ecb5ef4fe79cc2d4b7ae3413b2604)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mem.c
diff --cc drivers/infiniband/hw/mlx5/mem.c
index 13de3d2edd34,844545064c9e..000000000000
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@@ -102,95 -53,48 +102,101 @@@ void mlx5_ib_cont_pages(struct ib_umem 
  }
  
  /*
 - * Compute the page shift and page_offset for mailboxes that use a quantized
 - * page_offset. The granulatity of the page offset scales according to page
 - * size.
 + * Populate the given array with bus addresses from the umem.
 + *
 + * dev - mlx5_ib device
 + * umem - umem to use to fill the pages
 + * page_shift - determines the page size used in the resulting array
 + * offset - offset into the umem to start from,
 + *          only implemented for ODP umems
 + * num_pages - total number of pages to fill
 + * pas - bus addresses array to fill
 + * access_flags - access flags to set on all present pages.
 +		  use enum mlx5_ib_mtt_access_flags for this.
   */
 -unsigned long __mlx5_umem_find_best_quantized_pgoff(
 -	struct ib_umem *umem, unsigned long pgsz_bitmap,
 -	unsigned int page_offset_bits, u64 pgoff_bitmask, unsigned int scale,
 -	unsigned int *page_offset_quantized)
 +void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			    int page_shift, size_t offset, size_t num_pages,
 +			    __be64 *pas, int access_flags)
  {
++<<<<<<< HEAD
 +	int shift = page_shift - PAGE_SHIFT;
 +	int mask = (1 << shift) - 1;
 +	int i, k, idx;
 +	u64 cur = 0;
 +	u64 base;
 +	int len;
 +	struct scatterlist *sg;
 +	int entry;
++=======
+ 	const u64 page_offset_mask = (1UL << page_offset_bits) - 1;
+ 	unsigned long page_size;
+ 	u64 page_offset;
++>>>>>>> d4b2d19dc53e (RDMA/mlx5: Silence the overflow warning while building offset mask)
 +
 +	i = 0;
 +	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {
 +		len = sg_dma_len(sg) >> PAGE_SHIFT;
 +		base = sg_dma_address(sg);
  
 -	page_size = ib_umem_find_best_pgoff(umem, pgsz_bitmap, pgoff_bitmask);
 -	if (!page_size)
 -		return 0;
 -
 -	/*
 -	 * page size is the largest possible page size.
 -	 *
 -	 * Reduce the page_size, and thus the page_offset and quanta, until the
 -	 * page_offset fits into the mailbox field. Once page_size < scale this
 -	 * loop is guaranteed to terminate.
 -	 */
 -	page_offset = ib_umem_dma_offset(umem, page_size);
 -	while (page_offset & ~(u64)(page_offset_mask * (page_size / scale))) {
 -		page_size /= 2;
 -		page_offset = ib_umem_dma_offset(umem, page_size);
 +		/* Skip elements below offset */
 +		if (i + len < offset << shift) {
 +			i += len;
 +			continue;
 +		}
 +
 +		/* Skip pages below offset */
 +		if (i < offset << shift) {
 +			k = (offset << shift) - i;
 +			i = offset << shift;
 +		} else {
 +			k = 0;
 +		}
 +
 +		for (; k < len; k++) {
 +			if (!(i & mask)) {
 +				cur = base + (k << PAGE_SHIFT);
 +				cur |= access_flags;
 +				idx = (i >> shift) - offset;
 +
 +				pas[idx] = cpu_to_be64(cur);
 +				mlx5_ib_dbg(dev, "pas[%d] 0x%llx\n",
 +					    i >> shift, be64_to_cpu(pas[idx]));
 +			}
 +			i++;
 +
 +			/* Stop after num_pages reached */
 +			if (i >> shift >= offset + num_pages)
 +				return;
 +		}
  	}
 +}
 +
 +void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			  int page_shift, __be64 *pas, int access_flags)
 +{
 +	return __mlx5_ib_populate_pas(dev, umem, page_shift, 0,
 +				      ib_umem_num_dma_blocks(umem, PAGE_SIZE),
 +				      pas, access_flags);
 +}
 +int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset)
 +{
 +	u64 page_size;
 +	u64 page_mask;
 +	u64 off_size;
 +	u64 off_mask;
 +	u64 buf_off;
 +
 +	page_size = (u64)1 << page_shift;
 +	page_mask = page_size - 1;
 +	buf_off = addr & page_mask;
 +	off_size = page_size >> 6;
 +	off_mask = off_size - 1;
 +
 +	if (buf_off & off_mask)
 +		return -EINVAL;
  
 -	/*
 -	 * The address is not aligned, or otherwise cannot be represented by the
 -	 * page_offset.
 -	 */
 -	if (!(pgsz_bitmap & page_size))
 -		return 0;
 -
 -	*page_offset_quantized =
 -		(unsigned long)page_offset / (page_size / scale);
 -	if (WARN_ON(*page_offset_quantized > page_offset_mask))
 -		return 0;
 -	return page_size;
 +	*offset = buf_off >> ilog2(off_size);
 +	return 0;
  }
  
  #define WR_ID_BF 0xBF
* Unmerged path drivers/infiniband/hw/mlx5/mem.c
