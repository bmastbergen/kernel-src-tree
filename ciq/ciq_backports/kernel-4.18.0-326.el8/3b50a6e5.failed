mm/hmm: provide the page mapping order in hmm_range_fault()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Ralph Campbell <rcampbell@nvidia.com>
commit 3b50a6e536d2d843857ffe5f923eff7be4222afe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/3b50a6e5.failed

hmm_range_fault() returns an array of page frame numbers and flags for how
the pages are mapped in the requested process' page tables. The PFN can be
used to get the struct page with hmm_pfn_to_page() and the page size order
can be determined with compound_order(page).

However, if the page is larger than order 0 (PAGE_SIZE), there is no
indication that a compound page is mapped by the CPU using a larger page
size. Without this information, the caller can't safely use a large device
PTE to map the compound page because the CPU might be using smaller PTEs
with different read/write permissions.

Add a new function hmm_pfn_to_map_order() to return the mapping size order
so that callers know the pages are being mapped with consistent
permissions and a large device page table mapping can be used if one is
available.

This will allow devices to optimize mapping the page into HW by avoiding
or batching work for huge pages. For instance the dma_map can be done with
a high order directly.

Link: https://lore.kernel.org/r/20200701225352.9649-3-rcampbell@nvidia.com
	Signed-off-by: Ralph Campbell <rcampbell@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 3b50a6e536d2d843857ffe5f923eff7be4222afe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc include/linux/hmm.h
index 6a8157d67186,866a0fa104c4..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -79,90 -18,69 +79,124 @@@
  #include <linux/completion.h>
  #include <linux/mmu_notifier.h>
  
 +
  /*
 - * On output:
 - * 0             - The page is faultable and a future call with 
 - *                 HMM_PFN_REQ_FAULT could succeed.
 - * HMM_PFN_VALID - the pfn field points to a valid PFN. This PFN is at
 - *                 least readable. If dev_private_owner is !NULL then this could
 - *                 point at a DEVICE_PRIVATE page.
 - * HMM_PFN_WRITE - if the page memory can be written to (requires HMM_PFN_VALID)
 - * HMM_PFN_ERROR - accessing the pfn is impossible and the device should
 - *                 fail. ie poisoned memory, special pages, no vma, etc
 + * We need this because from rhel 8.0 to rhel 8.1 this struct definition was
 + * hidden in a C file. Now we need to expose it in this header file and thus
 + * to silence KABI check we need to gard it with GENKSYMS this is ok because
 + * HMM is never going to be a whitelist KABI as it would impede other back-
 + * port.
 + */
++<<<<<<< HEAD
 +#ifndef __GENKSYMS__
 +/*
 + * struct hmm - HMM per mm struct
 + *
 + * @mm: mm struct this HMM struct is bound to
 + * @lock: lock protecting ranges list
 + * @ranges: list of range being snapshotted
 + * @mirrors: list of mirrors for this mm
 + * @mmu_notifier: mmu notifier to track updates to CPU page table
 + * @mirrors_sem: read/write semaphore protecting the mirrors list
 + * @wq: wait queue for user waiting on a range invalidation
 + * @notifiers: count of active mmu notifiers
 + */
 +struct hmm {
 +	struct mm_struct	*mm;
 +	struct kref		kref;
 +	spinlock_t		ranges_lock;
 +	struct list_head	ranges;
 +	struct list_head	mirrors;
 +	struct mmu_notifier	mmu_notifier;
 +	struct rw_semaphore	mirrors_sem;
 +	wait_queue_head_t	wq;
 +	struct rcu_head		rcu;
 +	long			notifiers;
 +};
 +#endif
 +
 +/*
 + * hmm_pfn_flag_e - HMM flag enums
   *
 - * On input:
 - * 0                 - Return the current state of the page, do not fault it.
 - * HMM_PFN_REQ_FAULT - The output must have HMM_PFN_VALID or hmm_range_fault()
 - *                     will fail
 - * HMM_PFN_REQ_WRITE - The output must have HMM_PFN_WRITE or hmm_range_fault()
 - *                     will fail. Must be combined with HMM_PFN_REQ_FAULT.
 + * Flags:
 + * HMM_PFN_VALID: pfn is valid. It has, at least, read permission.
 + * HMM_PFN_WRITE: CPU page table has write permission set
 + * HMM_PFN_DEVICE_PRIVATE: private device memory (ZONE_DEVICE)
 + *
 + * The driver provides a flags array for mapping page protections to device
 + * PTE bits. If the driver valid bit for an entry is bit 3,
 + * i.e., (entry & (1 << 3)), then the driver must provide
 + * an array in hmm_range.flags with hmm_range.flags[HMM_PFN_VALID] == 1 << 3.
 + * Same logic apply to all flags. This is the same idea as vm_page_prot in vma
 + * except that this is per device driver rather than per architecture.
   */
 +enum hmm_pfn_flag_e {
 +	HMM_PFN_VALID = 0,
 +	HMM_PFN_WRITE,
 +	HMM_PFN_DEVICE_PRIVATE,
 +	HMM_PFN_FLAG_MAX
++=======
+ enum hmm_pfn_flags {
+ 	/* Output fields and flags */
+ 	HMM_PFN_VALID = 1UL << (BITS_PER_LONG - 1),
+ 	HMM_PFN_WRITE = 1UL << (BITS_PER_LONG - 2),
+ 	HMM_PFN_ERROR = 1UL << (BITS_PER_LONG - 3),
+ 	HMM_PFN_ORDER_SHIFT = (BITS_PER_LONG - 8),
+ 
+ 	/* Input flags */
+ 	HMM_PFN_REQ_FAULT = HMM_PFN_VALID,
+ 	HMM_PFN_REQ_WRITE = HMM_PFN_WRITE,
+ 
+ 	HMM_PFN_FLAGS = 0xFFUL << HMM_PFN_ORDER_SHIFT,
++>>>>>>> 3b50a6e536d2 (mm/hmm: provide the page mapping order in hmm_range_fault())
  };
  
  /*
 - * hmm_pfn_to_page() - return struct page pointed to by a device entry
 + * hmm_pfn_value_e - HMM pfn special value
   *
 - * This must be called under the caller 'user_lock' after a successful
 - * mmu_interval_read_begin(). The caller must have tested for HMM_PFN_VALID
 - * already.
 + * Flags:
 + * HMM_PFN_ERROR: corresponding CPU page table entry points to poisoned memory
 + * HMM_PFN_NONE: corresponding CPU page table entry is pte_none()
 + * HMM_PFN_SPECIAL: corresponding CPU page table entry is special; i.e., the
 + *      result of vm_insert_pfn() or vm_insert_page(). Therefore, it should not
 + *      be mirrored by a device, because the entry will never have HMM_PFN_VALID
 + *      set and the pfn value is undefined.
 + *
 + * Driver provides values for none entry, error entry, and special entry.
 + * Driver can alias (i.e., use same value) error and special, but
 + * it should not alias none with error or special.
 + *
 + * HMM pfn value returned by hmm_vma_get_pfns() or hmm_vma_fault() will be:
 + * hmm_range.values[HMM_PFN_ERROR] if CPU page table entry is poisonous,
 + * hmm_range.values[HMM_PFN_NONE] if there is no CPU page table entry,
 + * hmm_range.values[HMM_PFN_SPECIAL] if CPU page table entry is a special one
   */
 -static inline struct page *hmm_pfn_to_page(unsigned long hmm_pfn)
 -{
 -	return pfn_to_page(hmm_pfn & ~HMM_PFN_FLAGS);
 -}
 +enum hmm_pfn_value_e {
 +	HMM_PFN_ERROR,
 +	HMM_PFN_NONE,
 +	HMM_PFN_SPECIAL,
 +	HMM_PFN_VALUE_MAX
 +};
  
+ /*
+  * hmm_pfn_to_map_order() - return the CPU mapping size order
+  *
+  * This is optionally useful to optimize processing of the pfn result
+  * array. It indicates that the page starts at the order aligned VA and is
+  * 1<<order bytes long.  Every pfn within an high order page will have the
+  * same pfn flags, both access protections and the map_order.  The caller must
+  * be careful with edge cases as the start and end VA of the given page may
+  * extend past the range used with hmm_range_fault().
+  *
+  * This must be called under the caller 'user_lock' after a successful
+  * mmu_interval_read_begin(). The caller must have tested for HMM_PFN_VALID
+  * already.
+  */
+ static inline unsigned int hmm_pfn_to_map_order(unsigned long hmm_pfn)
+ {
+ 	return (hmm_pfn >> HMM_PFN_ORDER_SHIFT) & 0x1F;
+ }
+ 
  /*
   * struct hmm_range - track invalidation lock on virtual address range
   *
diff --cc mm/hmm.c
index 3233a7437881,0809baee49d0..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -456,19 -152,32 +456,35 @@@ static int hmm_vma_walk_hole(unsigned l
  
  	i = (addr - range->start) >> PAGE_SHIFT;
  	npages = (end - addr) >> PAGE_SHIFT;
 -	hmm_pfns = &range->hmm_pfns[i];
 -	required_fault =
 -		hmm_range_need_fault(hmm_vma_walk, hmm_pfns, npages, 0);
 -	if (!walk->vma) {
 -		if (required_fault)
 -			return -EFAULT;
 -		return hmm_pfns_fill(addr, end, range, HMM_PFN_ERROR);
 -	}
 -	if (required_fault)
 -		return hmm_vma_fault(addr, end, required_fault, walk);
 -	return hmm_pfns_fill(addr, end, range, 0);
 +	pfns = &range->pfns[i];
 +	hmm_range_need_fault(hmm_vma_walk, pfns, npages,
 +			     0, &fault, &write_fault);
 +	return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
  }
  
++<<<<<<< HEAD
 +static inline uint64_t pmd_to_hmm_pfn_flags(struct hmm_range *range, pmd_t pmd)
 +{
 +	if (pmd_protnone(pmd))
 +		return 0;
 +	return pmd_write(pmd) ? range->flags[HMM_PFN_VALID] |
 +				range->flags[HMM_PFN_WRITE] :
 +				range->flags[HMM_PFN_VALID];
++=======
+ static inline unsigned long hmm_pfn_flags_order(unsigned long order)
+ {
+ 	return order << HMM_PFN_ORDER_SHIFT;
+ }
+ 
+ static inline unsigned long pmd_to_hmm_pfn_flags(struct hmm_range *range,
+ 						 pmd_t pmd)
+ {
+ 	if (pmd_protnone(pmd))
+ 		return 0;
+ 	return (pmd_write(pmd) ? (HMM_PFN_VALID | HMM_PFN_WRITE) :
+ 				 HMM_PFN_VALID) |
+ 	       hmm_pfn_flags_order(PMD_SHIFT - PAGE_SHIFT);
++>>>>>>> 3b50a6e536d2 (mm/hmm: provide the page mapping order in hmm_range_fault())
  }
  
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
@@@ -694,9 -396,9 +710,15 @@@ static inline uint64_t pud_to_hmm_pfn_f
  {
  	if (!pud_present(pud))
  		return 0;
++<<<<<<< HEAD
 +	return pud_write(pud) ? range->flags[HMM_PFN_VALID] |
 +				range->flags[HMM_PFN_WRITE] :
 +				range->flags[HMM_PFN_VALID];
++=======
+ 	return (pud_write(pud) ? (HMM_PFN_VALID | HMM_PFN_WRITE) :
+ 				 HMM_PFN_VALID) |
+ 	       hmm_pfn_flags_order(PUD_SHIFT - PAGE_SHIFT);
++>>>>>>> 3b50a6e536d2 (mm/hmm: provide the page mapping order in hmm_range_fault())
  }
  
  static int hmm_vma_walk_pud(pud_t *pudp, unsigned long start, unsigned long end,
@@@ -796,130 -481,61 +818,142 @@@ static int hmm_vma_walk_hugetlb_entry(p
  	ptl = huge_pte_lock(hstate_vma(vma), walk->mm, pte);
  	entry = huge_ptep_get(pte);
  
++<<<<<<< HEAD
 +	i = (start - range->start) >> range->page_shift;
 +	orig_pfn = range->pfns[i];
 +	range->pfns[i] = range->values[HMM_PFN_NONE];
 +	cpu_flags = pte_to_hmm_pfn_flags(range, entry);
 +	fault = write_fault = false;
 +	hmm_pte_need_fault(hmm_vma_walk, orig_pfn, cpu_flags,
 +			   &fault, &write_fault);
 +	if (fault || write_fault) {
 +		ret = -ENOENT;
 +		goto unlock;
++=======
+ 	i = (start - range->start) >> PAGE_SHIFT;
+ 	pfn_req_flags = range->hmm_pfns[i];
+ 	cpu_flags = pte_to_hmm_pfn_flags(range, entry) |
+ 		    hmm_pfn_flags_order(huge_page_order(hstate_vma(vma)));
+ 	required_fault =
+ 		hmm_pte_need_fault(hmm_vma_walk, pfn_req_flags, cpu_flags);
+ 	if (required_fault) {
+ 		spin_unlock(ptl);
+ 		return hmm_vma_fault(addr, end, required_fault, walk);
++>>>>>>> 3b50a6e536d2 (mm/hmm: provide the page mapping order in hmm_range_fault())
  	}
  
 -	pfn = pte_pfn(entry) + ((start & ~hmask) >> PAGE_SHIFT);
 -	for (; addr < end; addr += PAGE_SIZE, i++, pfn++)
 -		range->hmm_pfns[i] = pfn | cpu_flags;
 +	pfn = pte_pfn(entry) + ((start & mask) >> range->page_shift);
 +	for (; addr < end; addr += size, i++, pfn += pfn_inc)
 +		range->pfns[i] = hmm_device_entry_from_pfn(range, pfn) |
 +				 cpu_flags;
 +	hmm_vma_walk->last = end;
  
 +unlock:
  	spin_unlock(ptl);
 -	return 0;
 +
 +	if (ret == -ENOENT)
 +		return hmm_vma_walk_hole_(addr, end, fault, write_fault, walk);
 +
 +	return ret;
 +#else /* CONFIG_HUGETLB_PAGE */
 +	return -EINVAL;
 +#endif
  }
 -#else
 -#define hmm_vma_walk_hugetlb_entry NULL
 -#endif /* CONFIG_HUGETLB_PAGE */
  
 -static int hmm_vma_walk_test(unsigned long start, unsigned long end,
 -			     struct mm_walk *walk)
 +static void hmm_pfns_clear(struct hmm_range *range,
 +			   uint64_t *pfns,
 +			   unsigned long addr,
 +			   unsigned long end)
  {
 -	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 -	struct hmm_range *range = hmm_vma_walk->range;
 -	struct vm_area_struct *vma = walk->vma;
 +	for (; addr < end; addr += PAGE_SIZE, pfns++)
 +		*pfns = range->values[HMM_PFN_NONE];
 +}
  
 -	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP | VM_MIXEDMAP)) &&
 -	    vma->vm_flags & VM_READ)
 -		return 0;
 +/*
 + * hmm_range_register() - start tracking change to CPU page table over a range
 + * @range: range
 + * @mm: the mm struct for the range of virtual address
 + * @start: start virtual address (inclusive)
 + * @end: end virtual address (exclusive)
 + * @page_shift: expect page shift for the range
 + * Return: 0 on success, -EFAULT if the address space is no longer valid
 + *
 + * Track updates to the CPU page table see include/linux/hmm.h
 + */
 +int hmm_range_register(struct hmm_range *range,
 +		       struct hmm_mirror *mirror,
 +		       unsigned long start,
 +		       unsigned long end,
 +		       unsigned page_shift)
 +{
 +	unsigned long mask = ((1UL << page_shift) - 1UL);
 +	struct hmm *hmm = mirror->hmm;
 +	unsigned long flags;
 +
 +	range->valid = false;
 +	range->hmm = NULL;
 +
 +	if ((start & mask) || (end & mask))
 +		return -EINVAL;
 +	if (start >= end)
 +		return -EINVAL;
 +
 +	range->page_shift = page_shift;
 +	range->start = start;
 +	range->end = end;
 +
 +	/* Prevent hmm_release() from running while the range is valid */
 +	if (!mmget_not_zero(hmm->mm))
 +		return -EFAULT;
 +
 +	/* Initialize range to track CPU page table updates. */
 +	spin_lock_irqsave(&hmm->ranges_lock, flags);
 +
 +	range->hmm = hmm;
 +	kref_get(&hmm->kref);
 +	list_add(&range->list, &hmm->ranges);
  
  	/*
 -	 * vma ranges that don't have struct page backing them or map I/O
 -	 * devices directly cannot be handled by hmm_range_fault().
 -	 *
 -	 * If the vma does not allow read access, then assume that it does not
 -	 * allow write access either. HMM does not support architectures that
 -	 * allow write without read.
 -	 *
 -	 * If a fault is requested for an unsupported range then it is a hard
 -	 * failure.
 +	 * If there are any concurrent notifiers we have to wait for them for
 +	 * the range to be valid (see hmm_range_wait_until_valid()).
  	 */
 -	if (hmm_range_need_fault(hmm_vma_walk,
 -				 range->hmm_pfns +
 -					 ((start - range->start) >> PAGE_SHIFT),
 -				 (end - start) >> PAGE_SHIFT, 0))
 -		return -EFAULT;
 +	if (!hmm->notifiers)
 +		range->valid = true;
 +	spin_unlock_irqrestore(&hmm->ranges_lock, flags);
  
 -	hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
 +	return 0;
 +}
 +EXPORT_SYMBOL(hmm_range_register);
  
 -	/* Skip this vma and continue processing the next vma. */
 -	return 1;
 +/*
 + * hmm_range_unregister() - stop tracking change to CPU page table over a range
 + * @range: range
 + *
 + * Range struct is used to track updates to the CPU page table after a call to
 + * hmm_range_register(). See include/linux/hmm.h for how to use it.
 + */
 +void hmm_range_unregister(struct hmm_range *range)
 +{
 +	struct hmm *hmm = range->hmm;
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&hmm->ranges_lock, flags);
 +	list_del_init(&range->list);
 +	spin_unlock_irqrestore(&hmm->ranges_lock, flags);
 +
 +	/* Drop reference taken by hmm_range_register() */
 +	mmput(hmm->mm);
 +	hmm_put(hmm);
 +
 +	/*
 +	 * The range is now invalid and the ref on the hmm is dropped, so
 +	 * poison the pointer.  Leave other fields in place, for the caller's
 +	 * use.
 +	 */
 +	range->valid = false;
 +	memset(&range->hmm, POISON_INUSE, sizeof(range->hmm));
  }
 +EXPORT_SYMBOL(hmm_range_unregister);
  
  static const struct mm_walk_ops hmm_walk_ops = {
  	.pud_entry	= hmm_vma_walk_pud,
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
