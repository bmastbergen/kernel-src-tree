RDMA/mlx5: Cleanup the synchronize_srcu() from the ODP flow

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Yishai Hadas <yishaih@nvidia.com>
commit db72438c9319cfd37e3c237a7754ca862ae12d63
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/db72438c.failed

Cleanup the synchronize_srcu() from the ODP flow as it was found to be a
very heavy time consumer as part of dereg_mr.

For example de-registration of 10000 ODP MRs each with size of 2M hugepage
took 19.6 sec comparing de-registration of same number of non ODP MRs that
took 172 ms.

The new locking scheme uses the wait_event() mechanism which follows the
use count of the MR instead of using synchronize_srcu().

By that change, the time required for the above test took 95 ms which is
even better than the non ODP flow.

Once fully dropped the srcu usage, had to come with a lock to protect the
XA access.

As part of using the above mechanism we could also clean the
num_deferred_work stuff and follow the use count instead.

Link: https://lore.kernel.org/r/20210202071309.2057998-1-leon@kernel.org
	Signed-off-by: Yishai Hadas <yishaih@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit db72438c9319cfd37e3c237a7754ca862ae12d63)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/mr.c
index d1c1a16be30f,db05b0e0a8d7..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -1465,72 -1536,130 +1466,167 @@@ struct ib_mr *mlx5_ib_reg_user_mr(struc
  		return &mr->ibmr;
  	}
  
 -	/* ODP requires xlt update via umr to work. */
 -	if (!mlx5_ib_can_load_pas_with_umr(dev, length))
 -		return ERR_PTR(-EINVAL);
 +	err = mr_umem_get(dev, udata, start, length, access_flags, &umem,
 +			  &npages, &page_shift, &ncont, &order);
  
 -	odp = ib_umem_odp_get(&dev->ib_dev, start, length, access_flags,
 -			      &mlx5_mn_ops);
 -	if (IS_ERR(odp))
 -		return ERR_CAST(odp);
 +	if (err < 0)
 +		return ERR_PTR(err);
  
 -	mr = alloc_cacheable_mr(pd, &odp->umem, iova, access_flags);
 -	if (IS_ERR(mr)) {
 -		ib_umem_release(&odp->umem);
 -		return ERR_CAST(mr);
 +	if (xlt_with_umr) {
 +		mr = alloc_mr_from_cache(pd, umem, virt_addr, length, ncont,
 +					 page_shift, order, access_flags);
 +		if (IS_ERR(mr))
 +			mr = NULL;
  	}
  
++<<<<<<< HEAD
 +	if (!mr) {
 +		mutex_lock(&dev->slow_path_mutex);
 +		mr = reg_create(NULL, pd, virt_addr, length, umem, ncont,
 +				page_shift, access_flags, !xlt_with_umr);
 +		mutex_unlock(&dev->slow_path_mutex);
++=======
+ 	odp->private = mr;
+ 	err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 
+ 	err = mlx5_ib_init_odp_mr(mr);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 	return &mr->ibmr;
+ 
+ err_dereg_mr:
+ 	dereg_mr(dev, mr);
+ 	return ERR_PTR(err);
+ }
+ 
+ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
+ 				  u64 iova, int access_flags,
+ 				  struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct ib_umem *umem;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	mlx5_ib_dbg(dev, "start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\n",
+ 		    start, iova, length, access_flags);
+ 
+ 	if (access_flags & IB_ACCESS_ON_DEMAND)
+ 		return create_user_odp_mr(pd, start, length, iova, access_flags,
+ 					  udata);
+ 	umem = ib_umem_get(&dev->ib_dev, start, length, access_flags);
+ 	if (IS_ERR(umem))
+ 		return ERR_CAST(umem);
+ 	return create_real_mr(pd, umem, iova, access_flags);
+ }
+ 
+ static void mlx5_ib_dmabuf_invalidate_cb(struct dma_buf_attachment *attach)
+ {
+ 	struct ib_umem_dmabuf *umem_dmabuf = attach->importer_priv;
+ 	struct mlx5_ib_mr *mr = umem_dmabuf->private;
+ 
+ 	dma_resv_assert_held(umem_dmabuf->attach->dmabuf->resv);
+ 
+ 	if (!umem_dmabuf->sgt)
+ 		return;
+ 
+ 	mlx5_ib_update_mr_pas(mr, MLX5_IB_UPD_XLT_ZAP);
+ 	ib_umem_dmabuf_unmap_pages(umem_dmabuf);
+ }
+ 
+ static struct dma_buf_attach_ops mlx5_ib_dmabuf_attach_ops = {
+ 	.allow_peer2peer = 1,
+ 	.move_notify = mlx5_ib_dmabuf_invalidate_cb,
+ };
+ 
+ struct ib_mr *mlx5_ib_reg_user_mr_dmabuf(struct ib_pd *pd, u64 offset,
+ 					 u64 length, u64 virt_addr,
+ 					 int fd, int access_flags,
+ 					 struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct mlx5_ib_mr *mr = NULL;
+ 	struct ib_umem_dmabuf *umem_dmabuf;
+ 	int err;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM) ||
+ 	    !IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	mlx5_ib_dbg(dev,
+ 		    "offset 0x%llx, virt_addr 0x%llx, length 0x%llx, fd %d, access_flags 0x%x\n",
+ 		    offset, virt_addr, length, fd, access_flags);
+ 
+ 	/* dmabuf requires xlt update via umr to work. */
+ 	if (!mlx5_ib_can_load_pas_with_umr(dev, length))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	umem_dmabuf = ib_umem_dmabuf_get(&dev->ib_dev, offset, length, fd,
+ 					 access_flags,
+ 					 &mlx5_ib_dmabuf_attach_ops);
+ 	if (IS_ERR(umem_dmabuf)) {
+ 		mlx5_ib_dbg(dev, "umem_dmabuf get failed (%ld)\n",
+ 			    PTR_ERR(umem_dmabuf));
+ 		return ERR_CAST(umem_dmabuf);
++>>>>>>> db72438c9319 (RDMA/mlx5: Cleanup the synchronize_srcu() from the ODP flow)
  	}
  
 -	mr = alloc_cacheable_mr(pd, &umem_dmabuf->umem, virt_addr,
 -				access_flags);
  	if (IS_ERR(mr)) {
 -		ib_umem_release(&umem_dmabuf->umem);
 -		return ERR_CAST(mr);
 +		err = PTR_ERR(mr);
 +		goto error;
  	}
  
  	mlx5_ib_dbg(dev, "mkey 0x%x\n", mr->mmkey.key);
  
 +	mr->umem = umem;
  	atomic_add(ib_umem_num_pages(mr->umem), &dev->mdev->priv.reg_pages);
++<<<<<<< HEAD
 +	set_mr_fields(dev, mr, length, access_flags);
 +
 +	if (xlt_with_umr) {
 +		/*
 +		 * If the MR was created with reg_create then it will be
 +		 * configured properly but left disabled. It is safe to go ahead
 +		 * and configure it again via UMR while enabling it.
 +		 */
 +		int update_xlt_flags = MLX5_IB_UPD_XLT_ENABLE;
 +
 +		if (access_flags & IB_ACCESS_ON_DEMAND)
 +			update_xlt_flags |= MLX5_IB_UPD_XLT_ZAP;
 +
 +		err = mlx5_ib_update_xlt(mr, 0, ncont, page_shift,
 +					 update_xlt_flags);
 +		if (err) {
 +			dereg_mr(dev, mr);
 +			return ERR_PTR(err);
 +		}
 +	}
 +
 +	if (is_odp_mr(mr)) {
 +		to_ib_umem_odp(mr->umem)->private = mr;
 +		init_waitqueue_head(&mr->q_deferred_work);
 +		atomic_set(&mr->num_deferred_work, 0);
 +		err = xa_err(xa_store(&dev->odp_mkeys,
 +				      mlx5_base_mkey(mr->mmkey.key), &mr->mmkey,
 +				      GFP_KERNEL));
 +		if (err) {
 +			dereg_mr(dev, mr);
 +			return ERR_PTR(err);
 +		}
 +	}
++=======
+ 	umem_dmabuf->private = mr;
+ 	err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
+ 	if (err)
+ 		goto err_dereg_mr;
++>>>>>>> db72438c9319 (RDMA/mlx5: Cleanup the synchronize_srcu() from the ODP flow)
  
 -	err = mlx5_ib_init_dmabuf_mr(mr);
 -	if (err)
 -		goto err_dereg_mr;
  	return &mr->ibmr;
 -
 -err_dereg_mr:
 -	dereg_mr(dev, mr);
 +error:
 +	ib_umem_release(umem);
  	return ERR_PTR(err);
  }
  
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 4e1bcb432959,374698186662..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -264,26 -248,22 +246,19 @@@ static void destroy_unused_implicit_chi
  	unsigned long idx = ib_umem_start(odp) >> MLX5_IMR_MTT_SHIFT;
  	struct mlx5_ib_mr *imr = mr->parent;
  
- 	xa_lock(&imr->implicit_children);
- 	/*
- 	 * This can race with mlx5_ib_free_implicit_mr(), the first one to
- 	 * reach the xa lock wins the race and destroys the MR.
- 	 */
- 	if (__xa_cmpxchg(&imr->implicit_children, idx, mr, NULL, GFP_ATOMIC) !=
- 	    mr)
- 		goto out_unlock;
+ 	if (!refcount_inc_not_zero(&imr->mmkey.usecount))
+ 		return;
  
- 	atomic_inc(&imr->num_deferred_work);
- 	call_srcu(&mr_to_mdev(mr)->odp_srcu, &mr->odp_destroy.rcu,
- 		  free_implicit_child_mr_rcu);
+ 	xa_erase(&imr->implicit_children, idx);
  
- out_unlock:
- 	xa_unlock(&imr->implicit_children);
+ 	/* Freeing a MR is a sleeping operation, so bounce to a work queue */
+ 	INIT_WORK(&mr->odp_destroy.work, free_implicit_child_mr_work);
+ 	queue_work(system_unbound_wq, &mr->odp_destroy.work);
  }
  
 -static bool mlx5_ib_invalidate_range(struct mmu_interval_notifier *mni,
 -				     const struct mmu_notifier_range *range,
 -				     unsigned long cur_seq)
 +void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
 +			      unsigned long end)
  {
 -	struct ib_umem_odp *umem_odp =
 -		container_of(mni, struct ib_umem_odp, notifier);
  	struct mlx5_ib_mr *mr;
  	const u64 umr_block_mask = (MLX5_UMR_MTT_ALIGNMENT /
  				    sizeof(struct mlx5_mtt)) - 1;
@@@ -541,12 -542,9 +523,16 @@@ struct mlx5_ib_mr *mlx5_ib_alloc_implic
  	imr->ibmr.rkey = imr->mmkey.key;
  	imr->ibmr.device = &dev->ib_dev;
  	imr->umem = &umem_odp->umem;
++<<<<<<< HEAD
 +	atomic_set(&imr->num_deferred_work, 0);
 +	init_waitqueue_head(&imr->q_deferred_work);
++=======
+ 	imr->is_odp_implicit = true;
++>>>>>>> db72438c9319 (RDMA/mlx5: Cleanup the synchronize_srcu() from the ODP flow)
  	xa_init(&imr->implicit_children);
  
 +	imr->is_odp_implicit = true;
 +
  	err = mlx5_ib_update_xlt(imr, 0,
  				 mlx5_imr_ksm_entries,
  				 MLX5_KSM_PAGE_SHIFT,
@@@ -654,7 -624,37 +612,38 @@@ void mlx5_ib_fence_odp_mr(struct mlx5_i
  	dma_fence_odp_mr(mr);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * mlx5_ib_fence_dmabuf_mr - Stop all access to the dmabuf MR
+  * @mr: to fence
+  *
+  * On return no parallel threads will be touching this MR and no DMA will be
+  * active.
+  */
+ void mlx5_ib_fence_dmabuf_mr(struct mlx5_ib_mr *mr)
+ {
+ 	struct ib_umem_dmabuf *umem_dmabuf = to_ib_umem_dmabuf(mr->umem);
+ 
+ 	/* Prevent new page faults and prefetch requests from succeeding */
+ 	xa_erase(&mr_to_mdev(mr)->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
+ 
+ 	mlx5r_deref_wait_odp_mkey(&mr->mmkey);
+ 
+ 	dma_resv_lock(umem_dmabuf->attach->dmabuf->resv, NULL);
+ 	mlx5_mr_cache_invalidate(mr);
+ 	umem_dmabuf->private = NULL;
+ 	ib_umem_dmabuf_unmap_pages(umem_dmabuf);
+ 	dma_resv_unlock(umem_dmabuf->attach->dmabuf->resv);
+ 
+ 	if (!mr->cache_ent) {
+ 		mlx5_core_destroy_mkey(mr_to_mdev(mr)->mdev, &mr->mmkey);
+ 		WARN_ON(mr->descs);
+ 	}
+ }
+ 
++>>>>>>> db72438c9319 (RDMA/mlx5: Cleanup the synchronize_srcu() from the ODP flow)
  #define MLX5_PF_FLAGS_DOWNGRADE BIT(1)
 -#define MLX5_PF_FLAGS_SNAPSHOT BIT(2)
 -#define MLX5_PF_FLAGS_ENABLE BIT(3)
  static int pagefault_real_mr(struct mlx5_ib_mr *mr, struct ib_umem_odp *odp,
  			     u64 user_va, size_t bcnt, u32 *bytes_mapped,
  			     u32 flags)
@@@ -1747,27 -1796,30 +1748,44 @@@ get_prefetchable_mr(struct ib_pd *pd, e
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
  	struct mlx5_core_mkey *mmkey;
++<<<<<<< HEAD
 +	struct ib_umem_odp *odp;
 +	struct mlx5_ib_mr *mr;
 +
 +	lockdep_assert_held(&dev->odp_srcu);
++=======
+ 	struct mlx5_ib_mr *mr = NULL;
++>>>>>>> db72438c9319 (RDMA/mlx5: Cleanup the synchronize_srcu() from the ODP flow)
  
+ 	xa_lock(&dev->odp_mkeys);
  	mmkey = xa_load(&dev->odp_mkeys, mlx5_base_mkey(lkey));
  	if (!mmkey || mmkey->key != lkey || mmkey->type != MLX5_MKEY_MR)
- 		return NULL;
+ 		goto end;
  
  	mr = container_of(mmkey, struct mlx5_ib_mr, mmkey);
  
- 	if (mr->ibmr.pd != pd)
- 		return NULL;
+ 	if (mr->ibmr.pd != pd) {
+ 		mr = NULL;
+ 		goto end;
+ 	}
  
 +	odp = to_ib_umem_odp(mr->umem);
 +
  	/* prefetch with write-access must be supported by the MR */
  	if (advice == IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH_WRITE &&
++<<<<<<< HEAD
 +	    !odp->umem.writable)
 +		return NULL;
++=======
+ 	    !mr->umem->writable) {
+ 		mr = NULL;
+ 		goto end;
+ 	}
++>>>>>>> db72438c9319 (RDMA/mlx5: Cleanup the synchronize_srcu() from the ODP flow)
  
+ 	refcount_inc(&mmkey->usecount);
+ end:
+ 	xa_unlock(&dev->odp_mkeys);
  	return mr;
  }
  
diff --git a/drivers/infiniband/hw/mlx5/devx.c b/drivers/infiniband/hw/mlx5/devx.c
index ed09a0921724..c3a11cafa40a 100644
--- a/drivers/infiniband/hw/mlx5/devx.c
+++ b/drivers/infiniband/hw/mlx5/devx.c
@@ -1313,9 +1313,9 @@ static int devx_handle_mkey_indirect(struct devx_obj *obj,
 	mkey->size = MLX5_GET64(mkc, mkc, len);
 	mkey->pd = MLX5_GET(mkc, mkc, pd);
 	devx_mr->ndescs = MLX5_GET(mkc, mkc, translations_octword_size);
+	init_waitqueue_head(&mkey->wait);
 
-	return xa_err(xa_store(&dev->odp_mkeys, mlx5_base_mkey(mkey->key), mkey,
-			       GFP_KERNEL));
+	return mlx5r_store_odp_mkey(dev, mkey);
 }
 
 static int devx_handle_mkey_create(struct mlx5_ib_dev *dev,
@@ -1388,16 +1388,15 @@ static int devx_obj_cleanup(struct ib_uobject *uobject,
 	int ret;
 
 	dev = mlx5_udata_to_mdev(&attrs->driver_udata);
-	if (obj->flags & DEVX_OBJ_FLAGS_INDIRECT_MKEY) {
+	if (obj->flags & DEVX_OBJ_FLAGS_INDIRECT_MKEY &&
+	    xa_erase(&obj->ib_dev->odp_mkeys,
+		     mlx5_base_mkey(obj->devx_mr.mmkey.key)))
 		/*
 		 * The pagefault_single_data_segment() does commands against
 		 * the mmkey, we must wait for that to stop before freeing the
 		 * mkey, as another allocation could get the same mkey #.
 		 */
-		xa_erase(&obj->ib_dev->odp_mkeys,
-			 mlx5_base_mkey(obj->devx_mr.mmkey.key));
-		synchronize_srcu(&dev->odp_srcu);
-	}
+		mlx5r_deref_wait_odp_mkey(&obj->devx_mr.mmkey);
 
 	if (obj->flags & DEVX_OBJ_FLAGS_DCT)
 		ret = mlx5_core_destroy_dct(obj->ib_dev, &obj->core_dct);
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index 4db2ee46f8f2..a42f0e49d45c 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -3912,7 +3912,6 @@ static void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev)
 {
 	mlx5_ib_cleanup_multiport_master(dev);
 	WARN_ON(!xa_empty(&dev->odp_mkeys));
-	cleanup_srcu_struct(&dev->odp_srcu);
 	mutex_destroy(&dev->cap_mask_mutex);
 	WARN_ON(!xa_empty(&dev->sig_mrs));
 	WARN_ON(!bitmap_empty(dev->dm.memic_alloc_pages, MLX5_MAX_MEMIC_PAGES));
@@ -3968,10 +3967,6 @@ static int mlx5_ib_stage_init_init(struct mlx5_ib_dev *dev)
 	dev->ib_dev.dev.parent		= mdev->device;
 	dev->ib_dev.lag_flags		= RDMA_LAG_FLAGS_HASH_ALL_SLAVES;
 
-	err = init_srcu_struct(&dev->odp_srcu);
-	if (err)
-		goto err_mp;
-
 	mutex_init(&dev->cap_mask_mutex);
 	INIT_LIST_HEAD(&dev->qp_list);
 	spin_lock_init(&dev->reset_flow_resource_lock);
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index de9c5a944683..0d7be31b1016 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -616,11 +616,8 @@ struct mlx5_ib_mr {
 	u64			pi_iova;
 
 	/* For ODP and implicit */
-	atomic_t		num_deferred_work;
-	wait_queue_head_t       q_deferred_work;
 	struct xarray		implicit_children;
 	union {
-		struct rcu_head rcu;
 		struct list_head elm;
 		struct work_struct work;
 	} odp_destroy;
@@ -995,11 +992,6 @@ struct mlx5_ib_dev {
 	u64			odp_max_size;
 	struct mlx5_ib_pf_eq	odp_pf_eq;
 
-	/*
-	 * Sleepable RCU that prevents destruction of MRs while they are still
-	 * being used by a page fault handler.
-	 */
-	struct srcu_struct      odp_srcu;
 	struct xarray		odp_mkeys;
 
 	u32			null_mkey;
@@ -1521,6 +1513,29 @@ static inline bool mlx5_ib_can_reconfig_with_umr(struct mlx5_ib_dev *dev,
 	return true;
 }
 
+static inline int mlx5r_store_odp_mkey(struct mlx5_ib_dev *dev,
+				       struct mlx5_core_mkey *mmkey)
+{
+	refcount_set(&mmkey->usecount, 1);
+
+	return xa_err(xa_store(&dev->odp_mkeys, mlx5_base_mkey(mmkey->key),
+			       mmkey, GFP_KERNEL));
+}
+
+/* deref an mkey that can participate in ODP flow */
+static inline void mlx5r_deref_odp_mkey(struct mlx5_core_mkey *mmkey)
+{
+	if (refcount_dec_and_test(&mmkey->usecount))
+		wake_up(&mmkey->wait);
+}
+
+/* deref an mkey that can participate in ODP flow and wait for relese */
+static inline void mlx5r_deref_wait_odp_mkey(struct mlx5_core_mkey *mmkey)
+{
+	mlx5r_deref_odp_mkey(mmkey);
+	wait_event(mmkey->wait, refcount_read(&mmkey->usecount) == 0);
+}
+
 int mlx5_ib_test_wc(struct mlx5_ib_dev *dev);
 
 static inline bool mlx5_ib_lag_should_assign_affinity(struct mlx5_ib_dev *dev)
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/mr.c b/drivers/net/ethernet/mellanox/mlx5/core/mr.c
index 9eb51f06d3ae..50af84e76fb6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/mr.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mr.c
@@ -56,6 +56,7 @@ int mlx5_core_create_mkey(struct mlx5_core_dev *dev,
 	mkey->size = MLX5_GET64(mkc, mkc, len);
 	mkey->key |= mlx5_idx_to_mkey(mkey_index);
 	mkey->pd = MLX5_GET(mkc, mkc, pd);
+	init_waitqueue_head(&mkey->wait);
 
 	mlx5_core_dbg(dev, "out 0x%x, mkey 0x%x\n", mkey_index, mkey->key);
 	return 0;
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index 8df7f0ec948b..064ca1f40206 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -366,6 +366,8 @@ struct mlx5_core_mkey {
 	u32			key;
 	u32			pd;
 	u32			type;
+	struct wait_queue_head wait;
+	refcount_t usecount;
 };
 
 #define MLX5_24BIT_MASK		((1 << 24) - 1)
