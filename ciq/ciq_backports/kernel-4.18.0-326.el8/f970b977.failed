mm/hmm: remove unused code and tidy comments

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit f970b977e068aa54e6eaf916a964a0abaf028afe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/f970b977.failed

Delete several functions that are never called, fix some desync between
comments and structure content, toss the now out of date top of file
header, and move one function only used by hmm.c into hmm.c

Link: https://lore.kernel.org/r/20200327200021.29372-4-jgg@ziepe.ca
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit f970b977e068aa54e6eaf916a964a0abaf028afe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/hmm.h
#	mm/hmm.c
diff --cc include/linux/hmm.h
index 6a8157d67186,daee6508a3f6..000000000000
--- a/include/linux/hmm.h
+++ b/include/linux/hmm.h
@@@ -1,69 -1,10 +1,19 @@@
  /*
   * Copyright 2013 Red Hat Inc.
   *
 + * This program is free software; you can redistribute it and/or modify
 + * it under the terms of the GNU General Public License as published by
 + * the Free Software Foundation; either version 2 of the License, or
 + * (at your option) any later version.
 + *
 + * This program is distributed in the hope that it will be useful,
 + * but WITHOUT ANY WARRANTY; without even the implied warranty of
 + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 + * GNU General Public License for more details.
 + *
   * Authors: Jérôme Glisse <jglisse@redhat.com>
-  */
- /*
-  * Heterogeneous Memory Management (HMM)
-  *
-  * See Documentation/vm/hmm.rst for reasons and overview of what HMM is and it
-  * is for. Here we focus on the HMM API description, with some explanation of
-  * the underlying implementation.
-  *
-  * Short description: HMM provides a set of helpers to share a virtual address
-  * space between CPU and a device, so that the device can access any valid
-  * address of the process (while still obeying memory protection). HMM also
-  * provides helpers to migrate process memory to device memory, and back. Each
-  * set of functionality (address space mirroring, and migration to and from
-  * device memory) can be used independently of the other.
-  *
-  *
-  * HMM address space mirroring API:
-  *
-  * Use HMM address space mirroring if you want to mirror a range of the CPU
-  * page tables of a process into a device page table. Here, "mirror" means "keep
-  * synchronized". Prerequisites: the device must provide the ability to write-
-  * protect its page tables (at PAGE_SIZE granularity), and must be able to
-  * recover from the resulting potential page faults.
-  *
-  * HMM guarantees that at any point in time, a given virtual address points to
-  * either the same memory in both CPU and device page tables (that is: CPU and
-  * device page tables each point to the same pages), or that one page table (CPU
-  * or device) points to no entry, while the other still points to the old page
-  * for the address. The latter case happens when the CPU page table update
-  * happens first, and then the update is mirrored over to the device page table.
-  * This does not cause any issue, because the CPU page table cannot start
-  * pointing to a new page until the device page table is invalidated.
-  *
-  * HMM uses mmu_notifiers to monitor the CPU page tables, and forwards any
-  * updates to each device driver that has registered a mirror. It also provides
-  * some API calls to help with taking a snapshot of the CPU page table, and to
-  * synchronize with any updates that might happen concurrently.
-  *
-  *
-  * HMM migration to and from device memory:
-  *
-  * HMM provides a set of helpers to hotplug device memory as ZONE_DEVICE, with
-  * a new MEMORY_DEVICE_PRIVATE type. This provides a struct page for each page
-  * of the device memory, and allows the device driver to manage its memory
-  * using those struct pages. Having struct pages for device memory makes
-  * migration easier. Because that memory is not addressable by the CPU it must
-  * never be pinned to the device; in other words, any CPU page fault can always
-  * cause the device memory to be migrated (copied/moved) back to regular memory.
   *
-  * A new migrate helper (migrate_vma()) has been added (see mm/migrate.c) that
-  * allows use of a device DMA engine to perform the copy operation between
-  * regular system memory and device memory.
+  * See Documentation/vm/hmm.rst for reasons and overview of what HMM is.
   */
  #ifndef LINUX_HMM_H
  #define LINUX_HMM_H
@@@ -166,9 -68,8 +116,14 @@@ enum hmm_pfn_value_e 
  /*
   * struct hmm_range - track invalidation lock on virtual address range
   *
++<<<<<<< HEAD
 + * @hmm: the core HMM structure this range is active against
 + * @vma: the vm area struct for the range
 + * @list: all range lock are on a list
++=======
+  * @notifier: a mmu_interval_notifier that includes the start/end
+  * @notifier_seq: result of mmu_interval_read_begin()
++>>>>>>> f970b977e068 (mm/hmm: remove unused code and tidy comments)
   * @start: range virtual start address (inclusive)
   * @end: range virtual end address (exclusive)
   * @pfns: array of pfns (big enough for the range)
@@@ -176,14 -77,12 +131,19 @@@
   * @values: pfn value for some special case (none, special, error, ...)
   * @default_flags: default flags for the range (write, read, ... see hmm doc)
   * @pfn_flags_mask: allows to mask pfn flags so that only default_flags matter
++<<<<<<< HEAD
 + * @page_shift: device virtual address shift value (should be >= PAGE_SHIFT)
 + * @pfn_shifts: pfn shift value (should be <= PAGE_SHIFT)
 + * @valid: pfns array did not change since it has been fill by an HMM function
++=======
+  * @pfn_shift: pfn shift value (should be <= PAGE_SHIFT)
+  * @dev_private_owner: owner of device private pages
++>>>>>>> f970b977e068 (mm/hmm: remove unused code and tidy comments)
   */
  struct hmm_range {
 -	struct mmu_interval_notifier *notifier;
 -	unsigned long		notifier_seq;
 +	struct hmm		*hmm;
 +	struct vm_area_struct	*vma;
 +	struct list_head	list;
  	unsigned long		start;
  	unsigned long		end;
  	uint64_t		*pfns;
@@@ -262,217 -117,8 +222,222 @@@ static inline struct page *hmm_device_e
  	return pfn_to_page(entry >> range->pfn_shift);
  }
  
++<<<<<<< HEAD
 +/*
 + * hmm_device_entry_to_pfn() - return pfn value store in a device entry
 + * @range: range use to decode device entry value
 + * @entry: device entry to extract pfn from
 + * Return: pfn value if device entry is valid, -1UL otherwise
 + */
 +static inline unsigned long
 +hmm_device_entry_to_pfn(const struct hmm_range *range, uint64_t pfn)
 +{
 +	if (pfn == range->values[HMM_PFN_NONE])
 +		return -1UL;
 +	if (pfn == range->values[HMM_PFN_ERROR])
 +		return -1UL;
 +	if (pfn == range->values[HMM_PFN_SPECIAL])
 +		return -1UL;
 +	if (!(pfn & range->flags[HMM_PFN_VALID]))
 +		return -1UL;
 +	return (pfn >> range->pfn_shift);
 +}
 +
 +/*
 + * hmm_device_entry_from_page() - create a valid device entry for a page
 + * @range: range use to encode HMM pfn value
 + * @page: page for which to create the device entry
 + * Return: valid device entry for the page
 + */
 +static inline uint64_t hmm_device_entry_from_page(const struct hmm_range *range,
 +						  struct page *page)
 +{
 +	return (page_to_pfn(page) << range->pfn_shift) |
 +		range->flags[HMM_PFN_VALID];
 +}
 +
 +/*
 + * hmm_device_entry_from_pfn() - create a valid device entry value from pfn
 + * @range: range use to encode HMM pfn value
 + * @pfn: pfn value for which to create the device entry
 + * Return: valid device entry for the pfn
 + */
 +static inline uint64_t hmm_device_entry_from_pfn(const struct hmm_range *range,
 +						 unsigned long pfn)
 +{
 +	return (pfn << range->pfn_shift) |
 +		range->flags[HMM_PFN_VALID];
 +}
 +
 +/*
 + * Old API:
 + * hmm_pfn_to_page()
 + * hmm_pfn_to_pfn()
 + * hmm_pfn_from_page()
 + * hmm_pfn_from_pfn()
 + *
 + * This are the OLD API please use new API, it is here to avoid cross-tree
 + * merge painfullness ie we convert things to new API in stages.
 + */
 +static inline struct page *hmm_pfn_to_page(const struct hmm_range *range,
 +					   uint64_t pfn)
 +{
 +	return hmm_device_entry_to_page(range, pfn);
 +}
 +
 +static inline unsigned long hmm_pfn_to_pfn(const struct hmm_range *range,
 +					   uint64_t pfn)
 +{
 +	return hmm_device_entry_to_pfn(range, pfn);
 +}
 +
 +static inline uint64_t hmm_pfn_from_page(const struct hmm_range *range,
 +					 struct page *page)
 +{
 +	return hmm_device_entry_from_page(range, page);
 +}
 +
 +static inline uint64_t hmm_pfn_from_pfn(const struct hmm_range *range,
 +					unsigned long pfn)
 +{
 +	return hmm_device_entry_from_pfn(range, pfn);
 +}
 +
 +
 +
 +#if IS_ENABLED(CONFIG_HMM_MIRROR)
 +/*
 + * Mirroring: how to synchronize device page table with CPU page table.
 + *
 + * A device driver that is participating in HMM mirroring must always
 + * synchronize with CPU page table updates. For this, device drivers can either
 + * directly use mmu_notifier APIs or they can use the hmm_mirror API. Device
 + * drivers can decide to register one mirror per device per process, or just
 + * one mirror per process for a group of devices. The pattern is:
 + *
 + *      int device_bind_address_space(..., struct mm_struct *mm, ...)
 + *      {
 + *          struct device_address_space *das;
 + *
 + *          // Device driver specific initialization, and allocation of das
 + *          // which contains an hmm_mirror struct as one of its fields.
 + *          ...
 + *
 + *          ret = hmm_mirror_register(&das->mirror, mm, &device_mirror_ops);
 + *          if (ret) {
 + *              // Cleanup on error
 + *              return ret;
 + *          }
 + *
 + *          // Other device driver specific initialization
 + *          ...
 + *      }
 + *
 + * Once an hmm_mirror is registered for an address space, the device driver
 + * will get callbacks through sync_cpu_device_pagetables() operation (see
 + * hmm_mirror_ops struct).
 + *
 + * Device driver must not free the struct containing the hmm_mirror struct
 + * before calling hmm_mirror_unregister(). The expected usage is to do that when
 + * the device driver is unbinding from an address space.
 + *
 + *
 + *      void device_unbind_address_space(struct device_address_space *das)
 + *      {
 + *          // Device driver specific cleanup
 + *          ...
 + *
 + *          hmm_mirror_unregister(&das->mirror);
 + *
 + *          // Other device driver specific cleanup, and now das can be freed
 + *          ...
 + *      }
 + */
 +
 +struct hmm_mirror;
 +
 +/*
 + * enum hmm_update_event - type of update
 + * @HMM_UPDATE_INVALIDATE: invalidate range (no indication as to why)
 + */
 +enum hmm_update_event {
 +	HMM_UPDATE_INVALIDATE,
 +};
 +
 +/*
 + * struct hmm_update - HMM update information for callback
 + *
 + * @start: virtual start address of the range to update
 + * @end: virtual end address of the range to update
 + * @event: event triggering the update (what is happening)
 + * @blockable: can the callback block/sleep ?
 + */
 +struct hmm_update {
 +	unsigned long start;
 +	unsigned long end;
 +	enum hmm_update_event event;
 +	bool blockable;
 +};
 +
 +/*
 + * struct hmm_mirror_ops - HMM mirror device operations callback
 + *
 + * @update: callback to update range on a device
 + */
 +struct hmm_mirror_ops {
 +	/* release() - release hmm_mirror
 +	 *
 +	 * @mirror: pointer to struct hmm_mirror
 +	 *
 +	 * This is called when the mm_struct is being released.
 +	 * The callback should make sure no references to the mirror occur
 +	 * after the callback returns.
 +	 */
 +	void (*release)(struct hmm_mirror *mirror);
 +
 +	/* sync_cpu_device_pagetables() - synchronize page tables
 +	 *
 +	 * @mirror: pointer to struct hmm_mirror
 +	 * @update: update information (see struct hmm_update)
 +	 * Return: -EAGAIN if update.blockable false and callback need to
 +	 *          block, 0 otherwise.
 +	 *
 +	 * This callback ultimately originates from mmu_notifiers when the CPU
 +	 * page table is updated. The device driver must update its page table
 +	 * in response to this callback. The update argument tells what action
 +	 * to perform.
 +	 *
 +	 * The device driver must not return from this callback until the device
 +	 * page tables are completely updated (TLBs flushed, etc); this is a
 +	 * synchronous call.
 +	 */
 +	int (*sync_cpu_device_pagetables)(struct hmm_mirror *mirror,
 +					  const struct hmm_update *update);
 +};
 +
 +/*
 + * struct hmm_mirror - mirror struct for a device driver
 + *
 + * @hmm: pointer to struct hmm (which is unique per mm_struct)
 + * @ops: device driver callback for HMM mirror operations
 + * @list: for list of mirrors of a given mm
 + *
 + * Each address space (mm_struct) being mirrored by a device must register one
 + * instance of an hmm_mirror struct with HMM. HMM will track the list of all
 + * mirrors for each mm_struct.
 + */
 +struct hmm_mirror {
 +	struct hmm			*hmm;
 +	const struct hmm_mirror_ops	*ops;
 +	struct list_head		list;
 +};
 +
 +int hmm_mirror_register(struct hmm_mirror *mirror, struct mm_struct *mm);
 +void hmm_mirror_unregister(struct hmm_mirror *mirror);
++=======
+ /* Don't fault in missing PTEs, just snapshot the current state. */
+ #define HMM_FAULT_SNAPSHOT		(1 << 1)
++>>>>>>> f970b977e068 (mm/hmm: remove unused code and tidy comments)
  
  /*
   * Please see Documentation/vm/hmm.rst for how to use the range API.
diff --cc mm/hmm.c
index 3233a7437881,136de474221d..000000000000
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@@ -296,40 -29,30 +296,63 @@@ EXPORT_SYMBOL(hmm_mirror_unregister)
  struct hmm_vma_walk {
  	struct hmm_range	*range;
  	unsigned long		last;
 -	unsigned int		flags;
 +	bool			fault;
 +	bool			block;
  };
  
++<<<<<<< HEAD
 +static int hmm_vma_do_fault(struct mm_walk *walk, unsigned long addr,
 +			    bool write_fault, uint64_t *pfn)
++=======
+ enum {
+ 	HMM_NEED_FAULT = 1 << 0,
+ 	HMM_NEED_WRITE_FAULT = 1 << 1,
+ 	HMM_NEED_ALL_BITS = HMM_NEED_FAULT | HMM_NEED_WRITE_FAULT,
+ };
+ 
+ /*
+  * hmm_device_entry_from_pfn() - create a valid device entry value from pfn
+  * @range: range use to encode HMM pfn value
+  * @pfn: pfn value for which to create the device entry
+  * Return: valid device entry for the pfn
+  */
+ static uint64_t hmm_device_entry_from_pfn(const struct hmm_range *range,
+ 					  unsigned long pfn)
+ {
+ 	return (pfn << range->pfn_shift) | range->flags[HMM_PFN_VALID];
+ }
+ 
+ static int hmm_pfns_fill(unsigned long addr, unsigned long end,
+ 		struct hmm_range *range, enum hmm_pfn_value_e value)
++>>>>>>> f970b977e068 (mm/hmm: remove unused code and tidy comments)
  {
 +	unsigned int flags = FAULT_FLAG_REMOTE;
 +	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 +	struct hmm_range *range = hmm_vma_walk->range;
 +	struct vm_area_struct *vma = walk->vma;
 +	vm_fault_t ret;
 +
 +	flags |= hmm_vma_walk->block ? 0 : FAULT_FLAG_ALLOW_RETRY;
 +	flags |= write_fault ? FAULT_FLAG_WRITE : 0;
 +	ret = handle_mm_fault(vma, addr, flags);
 +	if (ret & VM_FAULT_RETRY) {
 +		/* Note, handle_mm_fault did up_read(&mm->mmap_sem)) */
 +		return -EAGAIN;
 +	}
 +	if (ret & VM_FAULT_ERROR) {
 +		*pfn = range->values[HMM_PFN_ERROR];
 +		return -EFAULT;
 +	}
 +
 +	return -EBUSY;
 +}
 +
 +static int hmm_pfns_bad(unsigned long addr,
 +			unsigned long end,
 +			struct mm_walk *walk)
 +{
 +	struct hmm_vma_walk *hmm_vma_walk = walk->private;
 +	struct hmm_range *range = hmm_vma_walk->range;
  	uint64_t *pfns = range->pfns;
  	unsigned long i;
  
@@@ -926,100 -551,30 +949,118 @@@ static const struct mm_walk_ops hmm_wal
  	.pmd_entry	= hmm_vma_walk_pmd,
  	.pte_hole	= hmm_vma_walk_hole,
  	.hugetlb_entry	= hmm_vma_walk_hugetlb_entry,
 -	.test_walk	= hmm_vma_walk_test,
  };
  
++<<<<<<< HEAD
 +/*
 + * hmm_range_snapshot() - snapshot CPU page table for a range
 + * @range: range
 + * Return: -EINVAL if invalid argument, -ENOMEM out of memory, -EPERM invalid
 + *          permission (for instance asking for write and range is read only),
 + *          -EBUSY if you need to retry, -EFAULT invalid (ie either no valid
 + *          vma or it is illegal to access that range), number of valid pages
 + *          in range->pfns[] (from range start address).
++=======
+ /**
+  * hmm_range_fault - try to fault some address in a virtual address range
+  * @range:	argument structure
+  * @flags:	HMM_FAULT_* flags
++>>>>>>> f970b977e068 (mm/hmm: remove unused code and tidy comments)
   *
 - * Return: the number of valid pages in range->pfns[] (from range start
 - * address), which may be zero.  On error one of the following status codes
 - * can be returned:
 + * This snapshots the CPU page table for a range of virtual addresses. Snapshot
 + * validity is tracked by range struct. See in include/linux/hmm.h for example
 + * on how to use.
 + */
 +long hmm_range_snapshot(struct hmm_range *range)
 +{
 +	const unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;
 +	unsigned long start = range->start, end;
 +	struct hmm_vma_walk hmm_vma_walk;
 +	struct hmm *hmm = range->hmm;
 +	struct vm_area_struct *vma;
 +
 +	lockdep_assert_held(&hmm->mm->mmap_sem);
 +	do {
 +		/* If range is no longer valid force retry. */
 +		if (!range->valid)
 +			return -EBUSY;
 +
 +		vma = find_vma(hmm->mm, start);
 +		if (vma == NULL || (vma->vm_flags & device_vma))
 +			return -EFAULT;
 +
 +		if (is_vm_hugetlb_page(vma)) {
 +			if (huge_page_shift(hstate_vma(vma)) !=
 +				    range->page_shift &&
 +			    range->page_shift != PAGE_SHIFT)
 +				return -EINVAL;
 +		} else {
 +			if (range->page_shift != PAGE_SHIFT)
 +				return -EINVAL;
 +		}
 +
 +		if (!(vma->vm_flags & VM_READ)) {
 +			/*
 +			 * If vma do not allow read access, then assume that it
 +			 * does not allow write access, either. HMM does not
 +			 * support architecture that allow write without read.
 +			 */
 +			hmm_pfns_clear(range, range->pfns,
 +				range->start, range->end);
 +			return -EPERM;
 +		}
 +
 +		range->vma = vma;
 +		hmm_vma_walk.pgmap = NULL;
 +		hmm_vma_walk.last = start;
 +		hmm_vma_walk.fault = false;
 +		hmm_vma_walk.range = range;
 +		end = min(range->end, vma->vm_end);
 +
 +		walk_page_range(vma->vm_mm, 0, end, &hmm_walk_ops,
 +				&hmm_vma_walk);
 +		start = end;
 +	} while (start < range->end);
 +
 +	return (hmm_vma_walk.last - range->start) >> PAGE_SHIFT;
 +}
 +EXPORT_SYMBOL(hmm_range_snapshot);
 +
 +/*
 + * hmm_range_fault() - try to fault some address in a virtual address range
 + * @range: range being faulted
 + * @block: allow blocking on fault (if true it sleeps and do not drop mmap_sem)
 + * Return: number of valid pages in range->pfns[] (from range start
 + *          address). This may be zero. If the return value is negative,
 + *          then one of the following values may be returned:
   *
++<<<<<<< HEAD
 + *           -EINVAL  invalid arguments or mm or virtual address are in an
 + *                    invalid vma (for instance device file vma).
 + *           -ENOMEM: Out of memory.
 + *           -EPERM:  Invalid permission (for instance asking for write and
 + *                    range is read only).
 + *           -EAGAIN: If you need to retry and mmap_sem was drop. This can only
 + *                    happens if block argument is false.
 + *           -EBUSY:  If the the range is being invalidated and you should wait
 + *                    for invalidation to finish.
 + *           -EFAULT: Invalid (ie either no valid vma or it is illegal to access
 + *                    that range), number of valid pages in range->pfns[] (from
 + *                    range start address).
++=======
+  * -EINVAL:	Invalid arguments or mm or virtual address is in an invalid vma
+  *		(e.g., device file vma).
+  * -ENOMEM:	Out of memory.
+  * -EPERM:	Invalid permission (e.g., asking for write and range is read
+  *		only).
+  * -EBUSY:	The range has been invalidated and the caller needs to wait for
+  *		the invalidation to finish.
+  * -EFAULT:     A page was requested to be valid and could not be made valid
+  *              ie it has no backing VMA or it is illegal to access
++>>>>>>> f970b977e068 (mm/hmm: remove unused code and tidy comments)
   *
-  * This is similar to a regular CPU page fault except that it will not trigger
-  * any memory migration if the memory being faulted is not accessible by CPUs
-  * and caller does not ask for migration.
+  * This is similar to get_user_pages(), except that it can read the page tables
+  * without mutating them (ie causing faults).
   *
   * On error, for one virtual address in the range, the function will mark the
   * corresponding HMM pfn entry with an error flag.
* Unmerged path include/linux/hmm.h
* Unmerged path mm/hmm.c
