mm: cleanup the gfp_mask handling in __vmalloc_area_node

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Christoph Hellwig <hch@lst.de>
commit f255935b976729dbd8ddd079b96ddb6ecb1895bc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/f255935b.failed

Patch series "two small vmalloc cleanups".

This patch (of 2):

__vmalloc_area_node currently has four different gfp_t variables to
just express this simple logic:

 - use the passed in mask, plus __GFP_NOWARN and __GFP_HIGHMEM (if
   suitable) for the underlying page allocation
 - use just the reclaim flags from the passed in mask plus __GFP_ZERO
   for allocating the page array

Simplify this down to just use the pre-existing nested_gfp as-is for
the page array allocation, and just the passed in gfp_mask for the
page allocation, after conditionally ORing __GFP_HIGHMEM into it.  This
also makes the allocation warning a little more correct.

Also initialize two variables at the time of declaration while touching
this area.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Uladzislau Rezki (Sony) <urezki@gmail.com>
Link: https://lkml.kernel.org/r/20201002124035.1539300-1-hch@lst.de
Link: https://lkml.kernel.org/r/20201002124035.1539300-2-hch@lst.de
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f255935b976729dbd8ddd079b96ddb6ecb1895bc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmalloc.c
diff --cc mm/vmalloc.c
index c367ad5c9850,acd11d3b8667..000000000000
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@@ -2368,22 -2461,19 +2368,25 @@@ static void *__vmalloc_node(unsigned lo
  static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,
  				 pgprot_t prot, int node)
  {
- 	struct page **pages;
- 	unsigned int nr_pages, array_size, i;
  	const gfp_t nested_gfp = (gfp_mask & GFP_RECLAIM_MASK) | __GFP_ZERO;
- 	const gfp_t alloc_mask = gfp_mask | __GFP_NOWARN;
- 	const gfp_t highmem_mask = (gfp_mask & (GFP_DMA | GFP_DMA32)) ?
- 					0 :
- 					__GFP_HIGHMEM;
+ 	unsigned int nr_pages = get_vm_area_size(area) >> PAGE_SHIFT;
+ 	unsigned int array_size = nr_pages * sizeof(struct page *), i;
+ 	struct page **pages;
  
- 	nr_pages = get_vm_area_size(area) >> PAGE_SHIFT;
- 	array_size = (nr_pages * sizeof(struct page *));
+ 	gfp_mask |= __GFP_NOWARN;
+ 	if (!(gfp_mask & (GFP_DMA | GFP_DMA32)))
+ 		gfp_mask |= __GFP_HIGHMEM;
  
 +	area->nr_pages = nr_pages;
  	/* Please note that the recursion is strictly bounded. */
  	if (array_size > PAGE_SIZE) {
++<<<<<<< HEAD
 +		pages = __vmalloc_node(array_size, 1, nested_gfp|highmem_mask,
 +				PAGE_KERNEL, node, area->caller);
++=======
+ 		pages = __vmalloc_node(array_size, 1, nested_gfp, node,
+ 					area->caller);
++>>>>>>> f255935b9767 (mm: cleanup the gfp_mask handling in __vmalloc_area_node)
  	} else {
  		pages = kmalloc_node(array_size, nested_gfp, node);
  	}
@@@ -2398,13 -2491,14 +2401,13 @@@
  		struct page *page;
  
  		if (node == NUMA_NO_NODE)
- 			page = alloc_page(alloc_mask|highmem_mask);
+ 			page = alloc_page(gfp_mask);
  		else
- 			page = alloc_pages_node(node, alloc_mask|highmem_mask, 0);
+ 			page = alloc_pages_node(node, gfp_mask, 0);
  
  		if (unlikely(!page)) {
 -			/* Successfully allocated i pages, free them in __vfree() */
 +			/* Successfully allocated i pages, free them in __vunmap() */
  			area->nr_pages = i;
 -			atomic_long_add(area->nr_pages, &nr_vmalloc_pages);
  			goto fail;
  		}
  		area->pages[i] = page;
* Unmerged path mm/vmalloc.c
