mmap locking API: convert mmap_sem API comments

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-326.el8
commit-author Michel Lespinasse <walken@google.com>
commit 3e4e28c5a8f01ee4174d639e36ed155ade489a6f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-326.el8/3e4e28c5.failed

Convert comments that reference old mmap_sem APIs to reference
corresponding new mmap locking APIs instead.

	Signed-off-by: Michel Lespinasse <walken@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
	Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Laurent Dufour <ldufour@linux.ibm.com>
	Cc: Liam Howlett <Liam.Howlett@oracle.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ying Han <yinghan@google.com>
Link: http://lkml.kernel.org/r/20200520052908.204642-12-walken@google.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3e4e28c5a8f01ee4174d639e36ed155ade489a6f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/vm/hmm.rst
#	drivers/android/binder_alloc.c
diff --cc Documentation/vm/hmm.rst
index b4b84a2cecd8,6f9e000757fa..000000000000
--- a/Documentation/vm/hmm.rst
+++ b/Documentation/vm/hmm.rst
@@@ -211,41 -181,29 +211,55 @@@ respect in order to keep things properl
        struct hmm_range range;
        ...
  
 -      range.notifier = &interval_sub;
        range.start = ...;
        range.end = ...;
 -      range.hmm_pfns = ...;
 -
 -      if (!mmget_not_zero(interval_sub->notifier.mm))
 -          return -EFAULT;
 +      range.pfns = ...;
 +      range.flags = ...;
 +      range.values = ...;
 +      range.pfn_shift = ...;
 +      hmm_range_register(&range);
 +
 +      /*
 +       * Just wait for range to be valid, safe to ignore return value as we
 +       * will use the return value of hmm_range_snapshot() below under the
 +       * mmap_sem to ascertain the validity of the range.
 +       */
 +      hmm_range_wait_until_valid(&range, TIMEOUT_IN_MSEC);
  
   again:
++<<<<<<< HEAD
 +      down_read(&mm->mmap_sem);
 +      ret = hmm_range_snapshot(&range);
 +      if (ret) {
 +          up_read(&mm->mmap_sem);
 +          if (ret == -EBUSY) {
 +            /*
 +             * No need to check hmm_range_wait_until_valid() return value
 +             * on retry we will get proper error with hmm_range_snapshot()
 +             */
 +            hmm_range_wait_until_valid(&range, TIMEOUT_IN_MSEC);
 +            goto again;
 +          }
 +          hmm_mirror_unregister(&range);
 +          return ret;
 +      }
++=======
+       range.notifier_seq = mmu_interval_read_begin(&interval_sub);
+       mmap_read_lock(mm);
+       ret = hmm_range_fault(&range);
+       if (ret) {
+           mmap_read_unlock(mm);
+           if (ret == -EBUSY)
+                  goto again;
+           return ret;
+       }
+       mmap_read_unlock(mm);
+ 
++>>>>>>> 3e4e28c5a8f0 (mmap locking API: convert mmap_sem API comments)
        take_lock(driver->update);
 -      if (mmu_interval_read_retry(&ni, range.notifier_seq) {
 +      if (!range.valid) {
            release_lock(driver->update);
 +          up_read(&mm->mmap_sem);
            goto again;
        }
  
diff --cc drivers/android/binder_alloc.c
index 2628806c64a2,42c672f1584e..000000000000
--- a/drivers/android/binder_alloc.c
+++ b/drivers/android/binder_alloc.c
@@@ -924,14 -928,13 +924,24 @@@ enum lru_status binder_alloc_free_page(
  
  	index = page - alloc->pages;
  	page_addr = (uintptr_t)alloc->buffer + index * PAGE_SIZE;
++<<<<<<< HEAD
 +	vma = alloc->vma;
 +	if (vma) {
 +		if (!mmget_not_zero(alloc->vma_vm_mm))
 +			goto err_mmget;
 +		mm = alloc->vma_vm_mm;
 +		if (!down_write_trylock(&mm->mmap_sem))
 +			goto err_down_write_mmap_sem_failed;
 +	}
++=======
+ 
+ 	mm = alloc->vma_vm_mm;
+ 	if (!mmget_not_zero(mm))
+ 		goto err_mmget;
+ 	if (!mmap_read_trylock(mm))
+ 		goto err_mmap_read_lock_failed;
+ 	vma = binder_alloc_get_vma(alloc);
++>>>>>>> 3e4e28c5a8f0 (mmap locking API: convert mmap_sem API comments)
  
  	list_lru_isolate(lru, item);
  	spin_unlock(lock);
@@@ -961,7 -960,7 +971,11 @@@
  	mutex_unlock(&alloc->mutex);
  	return LRU_REMOVED_RETRY;
  
++<<<<<<< HEAD
 +err_down_write_mmap_sem_failed:
++=======
+ err_mmap_read_lock_failed:
++>>>>>>> 3e4e28c5a8f0 (mmap locking API: convert mmap_sem API comments)
  	mmput_async(mm);
  err_mmget:
  err_page_already_freed:
* Unmerged path Documentation/vm/hmm.rst
diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c
index d73dc473fbb9..3389cb7b3036 100644
--- a/arch/alpha/mm/fault.c
+++ b/arch/alpha/mm/fault.c
@@ -171,7 +171,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,
 		if (fault & VM_FAULT_RETRY) {
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 
-			 /* No need to up_read(&mm->mmap_sem) as we would
+			 /* No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c
index a9d55ad8d67b..bcd466db2cf6 100644
--- a/arch/ia64/mm/fault.c
+++ b/arch/ia64/mm/fault.c
@@ -192,7 +192,7 @@ ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			flags |= FAULT_FLAG_TRIED;
 
-			 /* No need to up_read(&mm->mmap_sem) as we would
+			 /* No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c
index 9b6163c05a75..9097910ab55c 100644
--- a/arch/m68k/mm/fault.c
+++ b/arch/m68k/mm/fault.c
@@ -168,7 +168,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,
 			flags |= FAULT_FLAG_TRIED;
 
 			/*
-			 * No need to up_read(&mm->mmap_sem) as we would
+			 * No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c
index 202ad6a494f5..22c77cea32c5 100644
--- a/arch/microblaze/mm/fault.c
+++ b/arch/microblaze/mm/fault.c
@@ -240,7 +240,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,
 			flags |= FAULT_FLAG_TRIED;
 
 			/*
-			 * No need to up_read(&mm->mmap_sem) as we would
+			 * No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index 73d8a0f0b810..3aa4e6e211df 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -182,7 +182,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,
 			flags |= FAULT_FLAG_TRIED;
 
 			/*
-			 * No need to up_read(&mm->mmap_sem) as we would
+			 * No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/nds32/mm/fault.c b/arch/nds32/mm/fault.c
index b740534b152c..aca513f9c846 100644
--- a/arch/nds32/mm/fault.c
+++ b/arch/nds32/mm/fault.c
@@ -240,7 +240,7 @@ void do_page_fault(unsigned long entry, unsigned long addr,
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			flags |= FAULT_FLAG_TRIED;
 
-			/* No need to up_read(&mm->mmap_sem) as we would
+			/* No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/nios2/mm/fault.c b/arch/nios2/mm/fault.c
index 24fd84cf6006..95b98ad4f75c 100644
--- a/arch/nios2/mm/fault.c
+++ b/arch/nios2/mm/fault.c
@@ -164,7 +164,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,
 			flags |= FAULT_FLAG_TRIED;
 
 			/*
-			 * No need to up_read(&mm->mmap_sem) as we would
+			 * No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/openrisc/mm/fault.c b/arch/openrisc/mm/fault.c
index dc4dbafc1d83..72f9c612f603 100644
--- a/arch/openrisc/mm/fault.c
+++ b/arch/openrisc/mm/fault.c
@@ -188,7 +188,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			flags |= FAULT_FLAG_TRIED;
 
-			 /* No need to up_read(&mm->mmap_sem) as we would
+			 /* No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c
index c8e8b7c05558..59ed428ca110 100644
--- a/arch/parisc/mm/fault.c
+++ b/arch/parisc/mm/fault.c
@@ -330,7 +330,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 
 			/*
-			 * No need to up_read(&mm->mmap_sem) as we would
+			 * No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c
index 88401d5125bc..ad12b5312956 100644
--- a/arch/riscv/mm/fault.c
+++ b/arch/riscv/mm/fault.c
@@ -162,7 +162,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 			flags |= FAULT_FLAG_TRIED;
 
 			/*
-			 * No need to up_read(&mm->mmap_sem) as we would
+			 * No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c
index 6defd2c6d9b1..1ab6dca5cf5e 100644
--- a/arch/sh/mm/fault.c
+++ b/arch/sh/mm/fault.c
@@ -502,7 +502,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,
 			flags |= FAULT_FLAG_TRIED;
 
 			/*
-			 * No need to up_read(&mm->mmap_sem) as we would
+			 * No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c
index b0440b0edd97..7999912180fb 100644
--- a/arch/sparc/mm/fault_32.c
+++ b/arch/sparc/mm/fault_32.c
@@ -264,7 +264,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			flags |= FAULT_FLAG_TRIED;
 
-			/* No need to up_read(&mm->mmap_sem) as we would
+			/* No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c
index 8f8a604c1300..9116ab2ea39b 100644
--- a/arch/sparc/mm/fault_64.c
+++ b/arch/sparc/mm/fault_64.c
@@ -462,7 +462,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			flags |= FAULT_FLAG_TRIED;
 
-			/* No need to up_read(&mm->mmap_sem) as we would
+			/* No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c
index 2ab0e0dcd166..8b0bc7889e3e 100644
--- a/arch/xtensa/mm/fault.c
+++ b/arch/xtensa/mm/fault.c
@@ -131,7 +131,7 @@ void do_page_fault(struct pt_regs *regs)
 			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			flags |= FAULT_FLAG_TRIED;
 
-			 /* No need to up_read(&mm->mmap_sem) as we would
+			 /* No need to mmap_read_unlock(mm) as we would
 			 * have already released it in __lock_page_or_retry
 			 * in mm/filemap.c.
 			 */
* Unmerged path drivers/android/binder_alloc.c
diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index df250e567adc..480ed9f64791 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -186,7 +186,7 @@ static int hugetlbfs_file_mmap(struct file *file, struct vm_area_struct *vma)
 }
 
 /*
- * Called under down_write(mmap_sem).
+ * Called under mmap_write_lock(mm).
  */
 
 #ifndef HAVE_ARCH_HUGETLB_UNMAPPED_AREA
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index e74ff77e1791..7135879b560c 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -1234,7 +1234,7 @@ static __always_inline void wake_userfault(struct userfaultfd_ctx *ctx,
 	/*
 	 * To be sure waitqueue_active() is not reordered by the CPU
 	 * before the pagetable update, use an explicit SMP memory
-	 * barrier here. PT lock release or up_read(mmap_sem) still
+	 * barrier here. PT lock release or mmap_read_unlock(mm) still
 	 * have release semantics that can allow the
 	 * waitqueue_active() to be reordered before the pte update.
 	 */
diff --git a/mm/filemap.c b/mm/filemap.c
index 7802a4d3e44b..f50c9bb5ea80 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -1408,7 +1408,7 @@ EXPORT_SYMBOL_GPL(__lock_page_killable);
  * Return values:
  * 1 - page is locked; mmap_sem is still held.
  * 0 - page is not locked.
- *     mmap_sem has been released (up_read()), unless flags had both
+ *     mmap_lock has been released (mmap_read_unlock(), unless flags had both
  *     FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_RETRY_NOWAIT set, in
  *     which case mmap_sem is still held.
  *
diff --git a/mm/gup.c b/mm/gup.c
index 2007cab8de67..f3fc08ea0cfc 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1898,19 +1898,19 @@ EXPORT_SYMBOL(get_user_pages);
 /**
  * get_user_pages_locked() is suitable to replace the form:
  *
- *      down_read(&mm->mmap_sem);
+ *      mmap_read_lock(mm);
  *      do_something()
  *      get_user_pages(tsk, mm, ..., pages, NULL);
- *      up_read(&mm->mmap_sem);
+ *      mmap_read_unlock(mm);
  *
  *  to:
  *
  *      int locked = 1;
- *      down_read(&mm->mmap_sem);
+ *      mmap_read_lock(mm);
  *      do_something()
  *      get_user_pages_locked(tsk, mm, ..., pages, &locked);
  *      if (locked)
- *          up_read(&mm->mmap_sem);
+ *          mmap_read_unlock(mm);
  *
  * @start:      starting user address
  * @nr_pages:   number of pages from start to pin
@@ -1949,9 +1949,9 @@ EXPORT_SYMBOL(get_user_pages_locked);
 /*
  * get_user_pages_unlocked() is suitable to replace the form:
  *
- *      down_read(&mm->mmap_sem);
+ *      mmap_read_lock(mm);
  *      get_user_pages(tsk, mm, ..., pages, NULL);
- *      up_read(&mm->mmap_sem);
+ *      mmap_read_unlock(mm);
  *
  *  with:
  *
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 97e3c030e205..28ed8f5e58b7 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1804,9 +1804,9 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 		goto unlock;
 
 	/*
-	 * In case prot_numa, we are under down_read(mmap_sem). It's critical
+	 * In case prot_numa, we are under mmap_read_lock(mm). It's critical
 	 * to not clear pmd intermittently to avoid race with MADV_DONTNEED
-	 * which is also under down_read(mmap_sem):
+	 * which is also under mmap_read_lock(mm):
 	 *
 	 *	CPU0:				CPU1:
 	 *				change_huge_pmd(prot_numa=1)
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index 4a149596ca6a..b287904eb278 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -1491,7 +1491,7 @@ static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)
 		/*
 		 * Check vma->anon_vma to exclude MAP_PRIVATE mappings that
 		 * got written to. These VMAs are likely not worth investing
-		 * down_write(mmap_sem) as PMD-mapping is likely to be split
+		 * mmap_write_lock(mm) as PMD-mapping is likely to be split
 		 * later.
 		 *
 		 * Not that vma->anon_vma check is racy: it can be set up after
diff --git a/mm/ksm.c b/mm/ksm.c
index b36dcf466627..4296ba74e48f 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -2336,7 +2336,7 @@ static struct rmap_item *scan_get_next_rmap_item(struct page **page)
 	} else {
 		up_read(&mm->mmap_sem);
 		/*
-		 * up_read(&mm->mmap_sem) first because after
+		 * mmap_read_unlock(mm) first because after
 		 * spin_unlock(&ksm_mmlist_lock) run, the "mm" may
 		 * already have been freed under us by __ksm_exit()
 		 * because the "mm_slot" is still hashed and
diff --git a/mm/memory.c b/mm/memory.c
index 2d81ca6d26e3..26996ac561ac 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3071,10 +3071,10 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	 * pte_offset_map() on pmds where a huge pmd might be created
 	 * from a different thread.
 	 *
-	 * pte_alloc_map() is safe to use under down_write(mmap_sem) or when
+	 * pte_alloc_map() is safe to use under mmap_write_lock(mm) or when
 	 * parallel threads are excluded by other means.
 	 *
-	 * Here we only have down_read(mmap_sem).
+	 * Here we only have mmap_read_lock(mm).
 	 */
 	if (pte_alloc(vma->vm_mm, vmf->pmd))
 		return VM_FAULT_OOM;
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 52c843347420..c34f9e406920 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -2110,7 +2110,7 @@ static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,
  *
  * 	This function allocates a page from the kernel page pool and applies
  *	a NUMA policy associated with the VMA or the current process.
- *	When VMA is not NULL caller must hold down_read on the mmap_sem of the
+ *	When VMA is not NULL caller must read-lock the mmap_lock of the
  *	mm_struct of the VMA to prevent it from going away. Should be used for
  *	all allocations for pages that will be mapped into user space. Returns
  *	NULL when no page can be allocated.
diff --git a/mm/migrate.c b/mm/migrate.c
index dd185639b857..1d29f44a1bed 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -2818,10 +2818,10 @@ static void migrate_vma_insert_page(struct migrate_vma *migrate,
 	 * pte_offset_map() on pmds where a huge pmd might be created
 	 * from a different thread.
 	 *
-	 * pte_alloc_map() is safe to use under down_write(mmap_sem) or when
+	 * pte_alloc_map() is safe to use under mmap_write_lock(mm) or when
 	 * parallel threads are excluded by other means.
 	 *
-	 * Here we only have down_read(mmap_sem).
+	 * Here we only have mmap_read_lock(mm).
 	 */
 	if (pte_alloc(mm, pmdp))
 		goto abort;
diff --git a/mm/mmap.c b/mm/mmap.c
index f1a83e0aa395..c1afddfd3140 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1380,7 +1380,7 @@ static inline bool file_mmap_ok(struct file *file, struct inode *inode,
 }
 
 /*
- * The caller must hold down_write(&current->mm->mmap_sem).
+ * The caller must write-lock current->mm->mmap_lock.
  */
 unsigned long do_mmap(struct file *file, unsigned long addr,
 			unsigned long len, unsigned long prot,
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index dc3c83ecb2f0..255e64be1e3f 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -563,8 +563,8 @@ static bool oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)
 	/*
 	 * MMF_OOM_SKIP is set by exit_mmap when the OOM reaper can't
 	 * work on the mm anymore. The check for MMF_OOM_SKIP must run
-	 * under mmap_sem for reading because it serializes against the
-	 * down_write();up_write() cycle in exit_mmap().
+	 * under mmap_lock for reading because it serializes against the
+	 * mmap_write_lock();mmap_write_unlock() cycle in exit_mmap().
 	 */
 	if (test_bit(MMF_OOM_SKIP, &mm->flags)) {
 		up_read(&mm->mmap_sem);
@@ -593,7 +593,7 @@ static void oom_reap_task(struct task_struct *tsk)
 	int attempts = 0;
 	struct mm_struct *mm = tsk->signal->oom_mm;
 
-	/* Retry the down_read_trylock(mmap_sem) a few times */
+	/* Retry the mmap_read_trylock(mm) a few times */
 	while (attempts++ < MAX_OOM_REAP_RETRIES && !oom_reap_task_mm(tsk, mm))
 		schedule_timeout_idle(HZ/10);
 
@@ -611,7 +611,7 @@ static void oom_reap_task(struct task_struct *tsk)
 
 	/*
 	 * Hide this mm from OOM killer because it has been either reaped or
-	 * somebody can't call up_write(mmap_sem).
+	 * somebody can't call mmap_write_unlock(mm).
 	 */
 	set_bit(MMF_OOM_SKIP, &mm->flags);
 
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 5426b63bed77..73bccf79aa43 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -1754,7 +1754,7 @@ int tcp_mmap(struct file *file, struct socket *sock,
 		return -EPERM;
 	vma->vm_flags &= ~(VM_MAYWRITE | VM_MAYEXEC);
 
-	/* Instruct vm_insert_page() to not down_read(mmap_sem) */
+	/* Instruct vm_insert_page() to not mmap_read_lock(mm) */
 	vma->vm_flags |= VM_MIXEDMAP;
 
 	vma->vm_ops = &tcp_vm_ops;
