mm,numa: reorganize change_pmd_range()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] numa: reorganize change_pmd_range (Rik van Riel) [1066655]
Rebuild_FUZZ: 92.96%
commit-author Rik van Riel <riel@redhat.com>
commit 88a9ab6e3dfb5b10168130c255c6102c925343ab
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/88a9ab6e.failed

Reorganize the order of ifs in change_pmd_range a little, in preparation
for the next patch.

[akpm@linux-foundation.org: fix indenting, per David]
	Signed-off-by: Rik van Riel <riel@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Reported-by: Xing Gang <gang.xing@hp.com>
	Tested-by: Chegu Vinod <chegu_vinod@hp.com>
	Acked-by: David Rientjes <rientjes@google.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 88a9ab6e3dfb5b10168130c255c6102c925343ab)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mprotect.c
diff --cc mm/mprotect.c
index ac7046362258,79cb51866e02..000000000000
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@@ -145,7 -115,11 +145,9 @@@ static inline unsigned long change_pmd_
  
  	pmd = pmd_offset(pud, addr);
  	do {
 -		unsigned long this_pages;
 -
  		next = pmd_addr_end(addr, end);
+ 		if (!pmd_trans_huge(*pmd) && pmd_none_or_clear_bad(pmd))
+ 			continue;
  		if (pmd_trans_huge(*pmd)) {
  			if (next - addr != HPAGE_PMD_SIZE)
  				split_huge_page_pmd(vma, addr, pmd);
@@@ -160,23 -135,16 +162,30 @@@
  					continue;
  				}
  			}
- 			/* fall through */
+ 			/* fall through, the trans huge pmd just split */
  		}
++<<<<<<< HEAD
 +		if (pmd_none_or_clear_bad(pmd))
 +			continue;
 +		pages += change_pte_range(vma, pmd, addr, next, newprot,
 +				 dirty_accountable, prot_numa, &all_same_node);
 +
 +		/*
 +		 * If we are changing protections for NUMA hinting faults then
 +		 * set pmd_numa if the examined pages were all on the same
 +		 * node. This allows a regular PMD to be handled as one fault
 +		 * and effectively batches the taking of the PTL
 +		 */
 +		if (prot_numa && all_same_node)
 +			change_pmd_protnuma(vma->vm_mm, addr, pmd);
++=======
+ 		VM_BUG_ON(pmd_trans_huge(*pmd));
+ 		this_pages = change_pte_range(vma, pmd, addr, next, newprot,
+ 				 dirty_accountable, prot_numa);
+ 		pages += this_pages;
++>>>>>>> 88a9ab6e3dfb (mm,numa: reorganize change_pmd_range())
  	} while (pmd++, addr = next, addr != end);
  
 -	if (nr_huge_updates)
 -		count_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);
  	return pages;
  }
  
* Unmerged path mm/mprotect.c
