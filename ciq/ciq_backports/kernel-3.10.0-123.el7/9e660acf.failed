blk-throttle: make blk_throtl_bio() ready for hierarchy

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Tejun Heo <tj@kernel.org>
commit 9e660acffcd1b5adc4ec1ffba0cbb584f86b8907
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/9e660acf.failed

Currently, blk_throtl_bio() issues the passed in bio directly if it's
within limits of its associated tg (throtl_grp).  This behavior
becomes incorrect with hierarchy support as the bio should be
accounted to and throttled by the ancestor throtl_grps too.

This patch makes the direct issue path of blk_throtl_bio() to loop
until it reaches the top-level service_queue or gets throttled.  If
the former, the bio can be issued directly; otherwise, it gets queued
at the first layer it was above limits.

As tg->parent_sq is always the top-level service queue currently, this
patch in itself doesn't make any behavior differences.

	Signed-off-by: Tejun Heo <tj@kernel.org>
	Acked-by: Vivek Goyal <vgoyal@redhat.com>
(cherry picked from commit 9e660acffcd1b5adc4ec1ffba0cbb584f86b8907)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-throttle.c
diff --cc block/blk-throttle.c
index e65e45a33372,52321a42cd78..000000000000
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@@ -1123,25 -1263,38 +1127,43 @@@ bool blk_throtl_bio(struct request_queu
  		 * So keep on trimming slice even if bio is not queued.
  		 */
  		throtl_trim_slice(tg, rw);
- 		goto out_unlock;
+ 
+ 		/*
+ 		 * @bio passed through this layer without being throttled.
+ 		 * Climb up the ladder.  If we''re already at the top, it
+ 		 * can be executed directly.
+ 		 */
+ 		sq = sq->parent_sq;
+ 		tg = sq_to_tg(sq);
+ 		if (!tg)
+ 			goto out_unlock;
  	}
  
++<<<<<<< HEAD
 +queue_bio:
 +	throtl_log_tg(tg, "[%c] bio. bdisp=%llu sz=%u bps=%llu"
 +			" iodisp=%u iops=%u queued=%d/%d",
 +			rw == READ ? 'R' : 'W',
 +			tg->bytes_disp[rw], bio->bi_size, tg->bps[rw],
 +			tg->io_disp[rw], tg->iops[rw],
 +			sq->nr_queued[READ], sq->nr_queued[WRITE]);
++=======
+ 	/* out-of-limit, queue to @tg */
+ 	throtl_log(sq, "[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d",
+ 		   rw == READ ? 'R' : 'W',
+ 		   tg->bytes_disp[rw], bio->bi_size, tg->bps[rw],
+ 		   tg->io_disp[rw], tg->iops[rw],
+ 		   sq->nr_queued[READ], sq->nr_queued[WRITE]);
++>>>>>>> 9e660acffcd1 (blk-throttle: make blk_throtl_bio() ready for hierarchy)
  
  	bio_associate_current(bio);
 -	throtl_add_bio_tg(bio, tg);
 +	throtl_add_bio_tg(bio, tg, &q->td->service_queue);
  	throttled = true;
  
 -	/*
 -	 * Update @tg's dispatch time and force schedule dispatch if @tg
 -	 * was empty before @bio.  The forced scheduling isn't likely to
 -	 * cause undue delay as @bio is likely to be dispatched directly if
 -	 * its @tg's disptime is not in the future.
 -	 */
 +	/* update @tg's dispatch time if @tg was empty before @bio */
  	if (tg->flags & THROTL_TG_WAS_EMPTY) {
 -		tg_update_disptime(tg);
 -		throtl_schedule_next_dispatch(tg->service_queue.parent_sq, true);
 +		tg_update_disptime(tg, &td->service_queue);
 +		throtl_schedule_next_dispatch(td);
  	}
  
  out_unlock:
* Unmerged path block/blk-throttle.c
