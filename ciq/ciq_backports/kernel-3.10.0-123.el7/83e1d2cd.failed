sched/numa: Use group fault statistics in numa placement

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Mel Gorman <mgorman@suse.de>
commit 83e1d2cd9eabec5164afea295ff06b941ae8e4a9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/83e1d2cd.failed

This patch uses the fraction of faults on a particular node for both task
and group, to figure out the best node to place a task.  If the task and
group statistics disagree on what the preferred node should be then a full
rescan will select the node with the best combined weight.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-50-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 83e1d2cd9eabec5164afea295ff06b941ae8e4a9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 98cee68da024,4c40e13310e9..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,13 -879,361 +839,366 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Once a preferred node is selected the scheduler balancer will prefer moving
+  * a task to that node for sysctl_numa_balancing_settle_count number of PTE
+  * scans. This will give the process the chance to accumulate more faults on
+  * the preferred node but still allow the scheduler to move the task again if
+  * the nodes CPUs are overloaded.
+  */
+ unsigned int sysctl_numa_balancing_settle_count __read_mostly = 4;
+ 
+ struct numa_group {
+ 	atomic_t refcount;
+ 
+ 	spinlock_t lock; /* nr_tasks, tasks */
+ 	int nr_tasks;
+ 	pid_t gid;
+ 	struct list_head task_list;
+ 
+ 	struct rcu_head rcu;
+ 	atomic_long_t total_faults;
+ 	atomic_long_t faults[0];
+ };
+ 
+ pid_t task_numa_group_id(struct task_struct *p)
+ {
+ 	return p->numa_group ? p->numa_group->gid : 0;
+ }
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static inline unsigned long group_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	return atomic_long_read(&p->numa_group->faults[2*nid]) +
+ 	       atomic_long_read(&p->numa_group->faults[2*nid+1]);
+ }
+ 
+ /*
+  * These return the fraction of accesses done by a particular task, or
+  * task group, on a particular numa node.  The group weight is given a
+  * larger multiplier, in order to group tasks together that are almost
+  * evenly spread out between numa nodes.
+  */
+ static inline unsigned long task_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	total_faults = p->total_numa_faults;
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * task_faults(p, nid) / total_faults;
+ }
+ 
+ static inline unsigned long group_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	total_faults = atomic_long_read(&p->numa_group->total_faults);
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1200 * group_faults(p, nid) / total_faults;
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ /* Cached statistics for all CPUs within a node */
+ struct numa_stats {
+ 	unsigned long nr_running;
+ 	unsigned long load;
+ 
+ 	/* Total compute capacity of CPUs on a node */
+ 	unsigned long power;
+ 
+ 	/* Approximate capacity in terms of runnable tasks on a node */
+ 	unsigned long capacity;
+ 	int has_capacity;
+ };
+ 
+ /*
+  * XXX borrowed from update_sg_lb_stats
+  */
+ static void update_numa_stats(struct numa_stats *ns, int nid)
+ {
+ 	int cpu;
+ 
+ 	memset(ns, 0, sizeof(*ns));
+ 	for_each_cpu(cpu, cpumask_of_node(nid)) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 
+ 		ns->nr_running += rq->nr_running;
+ 		ns->load += weighted_cpuload(cpu);
+ 		ns->power += power_of(cpu);
+ 	}
+ 
+ 	ns->load = (ns->load * SCHED_POWER_SCALE) / ns->power;
+ 	ns->capacity = DIV_ROUND_CLOSEST(ns->power, SCHED_POWER_SCALE);
+ 	ns->has_capacity = (ns->nr_running < ns->capacity);
+ }
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	int imbalance_pct, idx;
+ 
+ 	struct task_struct *best_task;
+ 	long best_imp;
+ 	int best_cpu;
+ };
+ 
+ static void task_numa_assign(struct task_numa_env *env,
+ 			     struct task_struct *p, long imp)
+ {
+ 	if (env->best_task)
+ 		put_task_struct(env->best_task);
+ 	if (p)
+ 		get_task_struct(p);
+ 
+ 	env->best_task = p;
+ 	env->best_imp = imp;
+ 	env->best_cpu = env->dst_cpu;
+ }
+ 
+ /*
+  * This checks if the overall compute and NUMA accesses of the system would
+  * be improved if the source tasks was migrated to the target dst_cpu taking
+  * into account that it might be best if task running on the dst_cpu should
+  * be exchanged with the source task
+  */
+ static void task_numa_compare(struct task_numa_env *env, long imp)
+ {
+ 	struct rq *src_rq = cpu_rq(env->src_cpu);
+ 	struct rq *dst_rq = cpu_rq(env->dst_cpu);
+ 	struct task_struct *cur;
+ 	long dst_load, src_load;
+ 	long load;
+ 
+ 	rcu_read_lock();
+ 	cur = ACCESS_ONCE(dst_rq->curr);
+ 	if (cur->pid == 0) /* idle */
+ 		cur = NULL;
+ 
+ 	/*
+ 	 * "imp" is the fault differential for the source task between the
+ 	 * source and destination node. Calculate the total differential for
+ 	 * the source task and potential destination task. The more negative
+ 	 * the value is, the more rmeote accesses that would be expected to
+ 	 * be incurred if the tasks were swapped.
+ 	 */
+ 	if (cur) {
+ 		/* Skip this swap candidate if cannot move to the source cpu */
+ 		if (!cpumask_test_cpu(env->src_cpu, tsk_cpus_allowed(cur)))
+ 			goto unlock;
+ 
+ 		imp += task_weight(cur, env->src_nid) +
+ 		       group_weight(cur, env->src_nid) -
+ 		       task_weight(cur, env->dst_nid) -
+ 		       group_weight(cur, env->dst_nid);
+ 	}
+ 
+ 	if (imp < env->best_imp)
+ 		goto unlock;
+ 
+ 	if (!cur) {
+ 		/* Is there capacity at our destination? */
+ 		if (env->src_stats.has_capacity &&
+ 		    !env->dst_stats.has_capacity)
+ 			goto unlock;
+ 
+ 		goto balance;
+ 	}
+ 
+ 	/* Balance doesn't matter much if we're running a task per cpu */
+ 	if (src_rq->nr_running == 1 && dst_rq->nr_running == 1)
+ 		goto assign;
+ 
+ 	/*
+ 	 * In the overloaded case, try and keep the load balanced.
+ 	 */
+ balance:
+ 	dst_load = env->dst_stats.load;
+ 	src_load = env->src_stats.load;
+ 
+ 	/* XXX missing power terms */
+ 	load = task_h_load(env->p);
+ 	dst_load += load;
+ 	src_load -= load;
+ 
+ 	if (cur) {
+ 		load = task_h_load(cur);
+ 		dst_load -= load;
+ 		src_load += load;
+ 	}
+ 
+ 	/* make src_load the smaller */
+ 	if (dst_load < src_load)
+ 		swap(dst_load, src_load);
+ 
+ 	if (src_load * env->imbalance_pct < dst_load * 100)
+ 		goto unlock;
+ 
+ assign:
+ 	task_numa_assign(env, cur, imp);
+ unlock:
+ 	rcu_read_unlock();
+ }
+ 
+ static void task_numa_find_cpu(struct task_numa_env *env, long imp)
+ {
+ 	int cpu;
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
+ 		/* Skip this CPU if the source task cannot migrate */
+ 		if (!cpumask_test_cpu(cpu, tsk_cpus_allowed(env->p)))
+ 			continue;
+ 
+ 		env->dst_cpu = cpu;
+ 		task_numa_compare(env, imp);
+ 	}
+ }
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = cpu_to_node(task_cpu(p)),
+ 
+ 		.imbalance_pct = 112,
+ 
+ 		.best_task = NULL,
+ 		.best_imp = 0,
+ 		.best_cpu = -1
+ 	};
+ 	struct sched_domain *sd;
+ 	unsigned long weight;
+ 	int nid, ret;
+ 	long imp;
+ 
+ 	/*
+ 	 * Pick the lowest SD_NUMA domain, as that would have the smallest
+ 	 * imbalance and would be the first to start moving tasks about.
+ 	 *
+ 	 * And we want to avoid any moving of tasks about, as that would create
+ 	 * random movement of tasks -- counter the numa conditions we're trying
+ 	 * to satisfy here.
+ 	 */
+ 	rcu_read_lock();
+ 	sd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));
+ 	env.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;
+ 	rcu_read_unlock();
+ 
+ 	weight = task_weight(p, env.src_nid) + group_weight(p, env.src_nid);
+ 	update_numa_stats(&env.src_stats, env.src_nid);
+ 	env.dst_nid = p->numa_preferred_nid;
+ 	imp = task_weight(p, env.dst_nid) + group_weight(p, env.dst_nid) - weight;
+ 	update_numa_stats(&env.dst_stats, env.dst_nid);
+ 
+ 	/* If the preferred nid has capacity, try to use it. */
+ 	if (env.dst_stats.has_capacity)
+ 		task_numa_find_cpu(&env, imp);
+ 
+ 	/* No space available on the preferred nid. Look elsewhere. */
+ 	if (env.best_cpu == -1) {
+ 		for_each_online_node(nid) {
+ 			if (nid == env.src_nid || nid == p->numa_preferred_nid)
+ 				continue;
+ 
+ 			/* Only consider nodes where both task and groups benefit */
+ 			imp = task_weight(p, nid) + group_weight(p, nid) - weight;
+ 			if (imp < 0)
+ 				continue;
+ 
+ 			env.dst_nid = nid;
+ 			update_numa_stats(&env.dst_stats, env.dst_nid);
+ 			task_numa_find_cpu(&env, imp);
+ 		}
+ 	}
+ 
+ 	/* No better CPU than the current one was found. */
+ 	if (env.best_cpu == -1)
+ 		return -EAGAIN;
+ 
+ 	if (env.best_task == NULL) {
+ 		int ret = migrate_task_to(p, env.best_cpu);
+ 		return ret;
+ 	}
+ 
+ 	ret = migrate_swap(p, env.best_task);
+ 	put_task_struct(env.best_task);
+ 	return ret;
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* Success if task is already running on preferred CPU */
+ 	p->numa_migrate_retry = 0;
+ 	if (cpu_to_node(task_cpu(p)) == p->numa_preferred_nid) {
+ 		/*
+ 		 * If migration is temporarily disabled due to a task migration
+ 		 * then re-enable it now as the task is running on its
+ 		 * preferred node and memory should migrate locally
+ 		 */
+ 		if (!p->numa_migrate_seq)
+ 			p->numa_migrate_seq++;
+ 		return;
+ 	}
+ 
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1))
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	if (task_numa_migrate(p) != 0)
+ 		p->numa_migrate_retry = jiffies + HZ*5;
+ }
+ 
++>>>>>>> 83e1d2cd9eab (sched/numa: Use group fault statistics in numa placement)
  static void task_numa_placement(struct task_struct *p)
  {
- 	int seq, nid, max_nid = -1;
- 	unsigned long max_faults = 0;
+ 	int seq, nid, max_nid = -1, max_group_nid = -1;
+ 	unsigned long max_faults = 0, max_group_faults = 0;
  
 +	if (!p->mm)	/* for example, ksmd faulting in a user's mm */
 +		return;
  	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
  	if (p->numa_scan_seq == seq)
  		return;
@@@ -854,23 -1243,197 +1207,209 @@@
  
  	/* Find the node with the highest number of faults */
  	for_each_online_node(nid) {
++<<<<<<< HEAD
 +		unsigned long faults;
 +
 +		/* Decay existing window and copy faults since last scan */
 +		p->numa_faults[nid] >>= 1;
 +		p->numa_faults[nid] += p->numa_faults_buffer[nid];
 +		p->numa_faults_buffer[nid] = 0;
++=======
+ 		unsigned long faults = 0, group_faults = 0;
+ 		int priv, i;
+ 
+ 		for (priv = 0; priv < 2; priv++) {
+ 			long diff;
+ 
+ 			i = task_faults_idx(nid, priv);
+ 			diff = -p->numa_faults[i];
+ 
+ 			/* Decay existing window, copy faults since last scan */
+ 			p->numa_faults[i] >>= 1;
+ 			p->numa_faults[i] += p->numa_faults_buffer[i];
+ 			p->numa_faults_buffer[i] = 0;
+ 
+ 			faults += p->numa_faults[i];
+ 			diff += p->numa_faults[i];
+ 			p->total_numa_faults += diff;
+ 			if (p->numa_group) {
+ 				/* safe because we can only change our own group */
+ 				atomic_long_add(diff, &p->numa_group->faults[i]);
+ 				atomic_long_add(diff, &p->numa_group->total_faults);
+ 				group_faults += atomic_long_read(&p->numa_group->faults[i]);
+ 			}
+ 		}
++>>>>>>> 83e1d2cd9eab (sched/numa: Use group fault statistics in numa placement)
  
 +		faults = p->numa_faults[nid];
  		if (faults > max_faults) {
  			max_faults = faults;
  			max_nid = nid;
  		}
+ 
+ 		if (group_faults > max_group_faults) {
+ 			max_group_faults = group_faults;
+ 			max_group_nid = nid;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * If the preferred task and group nids are different,
+ 	 * iterate over the nodes again to find the best place.
+ 	 */
+ 	if (p->numa_group && max_nid != max_group_nid) {
+ 		unsigned long weight, max_weight = 0;
+ 
+ 		for_each_online_node(nid) {
+ 			weight = task_weight(p, nid) + group_weight(p, nid);
+ 			if (weight > max_weight) {
+ 				max_weight = weight;
+ 				max_nid = nid;
+ 			}
+ 		}
  	}
  
 -	/* Preferred node as the node with the most faults */
 -	if (max_faults && max_nid != p->numa_preferred_nid) {
 -		/* Update the preferred nid and migrate task if possible */
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
  		p->numa_preferred_nid = max_nid;
++<<<<<<< HEAD
++=======
+ 		p->numa_migrate_seq = 1;
+ 		numa_migrate_preferred(p);
+ 	}
+ }
+ 
+ static inline int get_numa_group(struct numa_group *grp)
+ {
+ 	return atomic_inc_not_zero(&grp->refcount);
+ }
+ 
+ static inline void put_numa_group(struct numa_group *grp)
+ {
+ 	if (atomic_dec_and_test(&grp->refcount))
+ 		kfree_rcu(grp, rcu);
+ }
+ 
+ static void double_lock(spinlock_t *l1, spinlock_t *l2)
+ {
+ 	if (l1 > l2)
+ 		swap(l1, l2);
+ 
+ 	spin_lock(l1);
+ 	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+ }
+ 
+ static void task_numa_group(struct task_struct *p, int cpupid)
+ {
+ 	struct numa_group *grp, *my_grp;
+ 	struct task_struct *tsk;
+ 	bool join = false;
+ 	int cpu = cpupid_to_cpu(cpupid);
+ 	int i;
+ 
+ 	if (unlikely(!p->numa_group)) {
+ 		unsigned int size = sizeof(struct numa_group) +
+ 				    2*nr_node_ids*sizeof(atomic_long_t);
+ 
+ 		grp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);
+ 		if (!grp)
+ 			return;
+ 
+ 		atomic_set(&grp->refcount, 1);
+ 		spin_lock_init(&grp->lock);
+ 		INIT_LIST_HEAD(&grp->task_list);
+ 		grp->gid = p->pid;
+ 
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_set(&grp->faults[i], p->numa_faults[i]);
+ 
+ 		atomic_long_set(&grp->total_faults, p->total_numa_faults);
+ 
+ 		list_add(&p->numa_entry, &grp->task_list);
+ 		grp->nr_tasks++;
+ 		rcu_assign_pointer(p->numa_group, grp);
+ 	}
+ 
+ 	rcu_read_lock();
+ 	tsk = ACCESS_ONCE(cpu_rq(cpu)->curr);
+ 
+ 	if (!cpupid_match_pid(tsk, cpupid))
+ 		goto unlock;
+ 
+ 	grp = rcu_dereference(tsk->numa_group);
+ 	if (!grp)
+ 		goto unlock;
+ 
+ 	my_grp = p->numa_group;
+ 	if (grp == my_grp)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Only join the other group if its bigger; if we're the bigger group,
+ 	 * the other task will join us.
+ 	 */
+ 	if (my_grp->nr_tasks > grp->nr_tasks)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Tie-break on the grp address.
+ 	 */
+ 	if (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)
+ 		goto unlock;
+ 
+ 	if (!get_numa_group(grp))
+ 		goto unlock;
+ 
+ 	join = true;
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 
+ 	if (!join)
+ 		return;
+ 
+ 	for (i = 0; i < 2*nr_node_ids; i++) {
+ 		atomic_long_sub(p->numa_faults[i], &my_grp->faults[i]);
+ 		atomic_long_add(p->numa_faults[i], &grp->faults[i]);
+ 	}
+ 	atomic_long_sub(p->total_numa_faults, &my_grp->total_faults);
+ 	atomic_long_add(p->total_numa_faults, &grp->total_faults);
+ 
+ 	double_lock(&my_grp->lock, &grp->lock);
+ 
+ 	list_move(&p->numa_entry, &grp->task_list);
+ 	my_grp->nr_tasks--;
+ 	grp->nr_tasks++;
+ 
+ 	spin_unlock(&my_grp->lock);
+ 	spin_unlock(&grp->lock);
+ 
+ 	rcu_assign_pointer(p->numa_group, grp);
+ 
+ 	put_numa_group(my_grp);
+ }
+ 
+ void task_numa_free(struct task_struct *p)
+ {
+ 	struct numa_group *grp = p->numa_group;
+ 	int i;
+ 
+ 	if (grp) {
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_sub(p->numa_faults[i], &grp->faults[i]);
+ 
+ 		atomic_long_sub(p->total_numa_faults, &grp->total_faults);
+ 
+ 		spin_lock(&grp->lock);
+ 		list_del(&p->numa_entry);
+ 		grp->nr_tasks--;
+ 		spin_unlock(&grp->lock);
+ 		rcu_assign_pointer(p->numa_group, NULL);
+ 		put_numa_group(grp);
+ 	}
+ 
+ 	kfree(p->numa_faults);
++>>>>>>> 83e1d2cd9eab (sched/numa: Use group fault statistics in numa placement)
  }
  
  /*
@@@ -893,7 -1462,20 +1432,24 @@@ void task_numa_fault(int node, int page
  			return;
  
  		BUG_ON(p->numa_faults_buffer);
++<<<<<<< HEAD
 +		p->numa_faults_buffer = p->numa_faults + nr_node_ids;
++=======
+ 		p->numa_faults_buffer = p->numa_faults + (2 * nr_node_ids);
+ 		p->total_numa_faults = 0;
+ 	}
+ 
+ 	/*
+ 	 * First accesses are treated as private, otherwise consider accesses
+ 	 * to be private if the accessing pid has not changed
+ 	 */
+ 	if (unlikely(last_cpupid == (-1 & LAST_CPUPID_MASK))) {
+ 		priv = 1;
+ 	} else {
+ 		priv = cpupid_match_pid(p, last_cpupid);
+ 		if (!priv && !(flags & TNF_NO_GROUP))
+ 			task_numa_group(p, last_cpupid);
++>>>>>>> 83e1d2cd9eab (sched/numa: Use group fault statistics in numa placement)
  	}
  
  	/*
@@@ -4054,6 -4637,80 +4610,83 @@@ task_hot(struct task_struct *p, u64 now
  	return delta < (s64)sysctl_sched_migration_cost;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NUMA_BALANCING
+ /* Returns true if the destination node has incurred more faults */
+ static bool migrate_improves_locality(struct task_struct *p, struct lb_env *env)
+ {
+ 	int src_nid, dst_nid;
+ 
+ 	if (!sched_feat(NUMA_FAVOUR_HIGHER) || !p->numa_faults ||
+ 	    !(env->sd->flags & SD_NUMA)) {
+ 		return false;
+ 	}
+ 
+ 	src_nid = cpu_to_node(env->src_cpu);
+ 	dst_nid = cpu_to_node(env->dst_cpu);
+ 
+ 	if (src_nid == dst_nid)
+ 		return false;
+ 
+ 	/* Always encourage migration to the preferred node. */
+ 	if (dst_nid == p->numa_preferred_nid)
+ 		return true;
+ 
+ 	/* After the task has settled, check if the new node is better. */
+ 	if (p->numa_migrate_seq >= sysctl_numa_balancing_settle_count &&
+ 			task_weight(p, dst_nid) + group_weight(p, dst_nid) >
+ 			task_weight(p, src_nid) + group_weight(p, src_nid))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ 
+ static bool migrate_degrades_locality(struct task_struct *p, struct lb_env *env)
+ {
+ 	int src_nid, dst_nid;
+ 
+ 	if (!sched_feat(NUMA) || !sched_feat(NUMA_RESIST_LOWER))
+ 		return false;
+ 
+ 	if (!p->numa_faults || !(env->sd->flags & SD_NUMA))
+ 		return false;
+ 
+ 	src_nid = cpu_to_node(env->src_cpu);
+ 	dst_nid = cpu_to_node(env->dst_cpu);
+ 
+ 	if (src_nid == dst_nid)
+ 		return false;
+ 
+ 	/* Migrating away from the preferred node is always bad. */
+ 	if (src_nid == p->numa_preferred_nid)
+ 		return true;
+ 
+ 	/* After the task has settled, check if the new node is worse. */
+ 	if (p->numa_migrate_seq >= sysctl_numa_balancing_settle_count &&
+ 			task_weight(p, dst_nid) + group_weight(p, dst_nid) <
+ 			task_weight(p, src_nid) + group_weight(p, src_nid))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ #else
+ static inline bool migrate_improves_locality(struct task_struct *p,
+ 					     struct lb_env *env)
+ {
+ 	return false;
+ }
+ 
+ static inline bool migrate_degrades_locality(struct task_struct *p,
+ 					     struct lb_env *env)
+ {
+ 	return false;
+ }
+ #endif
+ 
++>>>>>>> 83e1d2cd9eab (sched/numa: Use group fault statistics in numa placement)
  /*
   * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
   */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 664a046b12fb..1ae097d0fbd2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1351,6 +1351,7 @@ struct task_struct {
 	 * The values remain static for the duration of a PTE scan
 	 */
 	unsigned long *numa_faults;
+	unsigned long total_numa_faults;
 
 	/*
 	 * numa_faults_buffer records faults per node during the current
* Unmerged path kernel/sched/fair.c
