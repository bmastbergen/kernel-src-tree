sched/numa: Avoid migrating tasks that are placed on their preferred node

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 0ec8aa00f2b4dc457836ef4e2662b02483e94fb7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/0ec8aa00.failed

This patch classifies scheduler domains and runqueues into types depending
the number of tasks that are about their NUMA placement and the number
that are currently running on their preferred node. The types are

regular: There are tasks running that do not care about their NUMA
	placement.

remote: There are tasks running that care about their placement but are
	currently running on a node remote to their ideal placement

all: No distinction

To implement this the patch tracks the number of tasks that are optimally
NUMA placed (rq->nr_preferred_running) and the number of tasks running
that care about their placement (nr_numa_running). The load balancer
uses this information to avoid migrating idea placed NUMA tasks as long
as better options for load balancing exists. For example, it will not
consider balancing between a group whose tasks are all perfectly placed
and a group with remote tasks.

	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Link: http://lkml.kernel.org/r/1381141781-10992-56-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 0ec8aa00f2b4dc457836ef4e2662b02483e94fb7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/fair.c
#	kernel/sched/sched.h
diff --cc kernel/sched/core.c
index 2902fa55d246,8cfd51f62241..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -4932,6 -4451,54 +4932,57 @@@ fail
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NUMA_BALANCING
+ /* Migrate current task p to target_cpu */
+ int migrate_task_to(struct task_struct *p, int target_cpu)
+ {
+ 	struct migration_arg arg = { p, target_cpu };
+ 	int curr_cpu = task_cpu(p);
+ 
+ 	if (curr_cpu == target_cpu)
+ 		return 0;
+ 
+ 	if (!cpumask_test_cpu(target_cpu, tsk_cpus_allowed(p)))
+ 		return -EINVAL;
+ 
+ 	/* TODO: This is not properly updating schedstats */
+ 
+ 	return stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);
+ }
+ 
+ /*
+  * Requeue a task on a given node and accurately track the number of NUMA
+  * tasks on the runqueues
+  */
+ void sched_setnuma(struct task_struct *p, int nid)
+ {
+ 	struct rq *rq;
+ 	unsigned long flags;
+ 	bool on_rq, running;
+ 
+ 	rq = task_rq_lock(p, &flags);
+ 	on_rq = p->on_rq;
+ 	running = task_current(rq, p);
+ 
+ 	if (on_rq)
+ 		dequeue_task(rq, p, 0);
+ 	if (running)
+ 		p->sched_class->put_prev_task(rq, p);
+ 
+ 	p->numa_preferred_nid = nid;
+ 	p->numa_migrate_seq = 1;
+ 
+ 	if (running)
+ 		p->sched_class->set_curr_task(rq);
+ 	if (on_rq)
+ 		enqueue_task(rq, p, 0);
+ 	task_rq_unlock(rq, p, &flags);
+ }
+ #endif
+ 
++>>>>>>> 0ec8aa00f2b4 (sched/numa: Avoid migrating tasks that are placed on their preferred node)
  /*
   * migration_cpu_stop - this will be executed by a highprio stopper thread
   * and performs thread migration by bumping thread off CPU then
diff --cc kernel/sched/fair.c
index 98cee68da024,5166b9b1af70..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,13 -879,409 +839,413 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Once a preferred node is selected the scheduler balancer will prefer moving
+  * a task to that node for sysctl_numa_balancing_settle_count number of PTE
+  * scans. This will give the process the chance to accumulate more faults on
+  * the preferred node but still allow the scheduler to move the task again if
+  * the nodes CPUs are overloaded.
+  */
+ unsigned int sysctl_numa_balancing_settle_count __read_mostly = 4;
+ 
+ static void account_numa_enqueue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running += (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ static void account_numa_dequeue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running -= (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ struct numa_group {
+ 	atomic_t refcount;
+ 
+ 	spinlock_t lock; /* nr_tasks, tasks */
+ 	int nr_tasks;
+ 	pid_t gid;
+ 	struct list_head task_list;
+ 
+ 	struct rcu_head rcu;
+ 	atomic_long_t total_faults;
+ 	atomic_long_t faults[0];
+ };
+ 
+ pid_t task_numa_group_id(struct task_struct *p)
+ {
+ 	return p->numa_group ? p->numa_group->gid : 0;
+ }
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static inline unsigned long group_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	return atomic_long_read(&p->numa_group->faults[2*nid]) +
+ 	       atomic_long_read(&p->numa_group->faults[2*nid+1]);
+ }
+ 
+ /*
+  * These return the fraction of accesses done by a particular task, or
+  * task group, on a particular numa node.  The group weight is given a
+  * larger multiplier, in order to group tasks together that are almost
+  * evenly spread out between numa nodes.
+  */
+ static inline unsigned long task_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	total_faults = p->total_numa_faults;
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * task_faults(p, nid) / total_faults;
+ }
+ 
+ static inline unsigned long group_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	total_faults = atomic_long_read(&p->numa_group->total_faults);
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * group_faults(p, nid) / total_faults;
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ /* Cached statistics for all CPUs within a node */
+ struct numa_stats {
+ 	unsigned long nr_running;
+ 	unsigned long load;
+ 
+ 	/* Total compute capacity of CPUs on a node */
+ 	unsigned long power;
+ 
+ 	/* Approximate capacity in terms of runnable tasks on a node */
+ 	unsigned long capacity;
+ 	int has_capacity;
+ };
+ 
+ /*
+  * XXX borrowed from update_sg_lb_stats
+  */
+ static void update_numa_stats(struct numa_stats *ns, int nid)
+ {
+ 	int cpu;
+ 
+ 	memset(ns, 0, sizeof(*ns));
+ 	for_each_cpu(cpu, cpumask_of_node(nid)) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 
+ 		ns->nr_running += rq->nr_running;
+ 		ns->load += weighted_cpuload(cpu);
+ 		ns->power += power_of(cpu);
+ 	}
+ 
+ 	ns->load = (ns->load * SCHED_POWER_SCALE) / ns->power;
+ 	ns->capacity = DIV_ROUND_CLOSEST(ns->power, SCHED_POWER_SCALE);
+ 	ns->has_capacity = (ns->nr_running < ns->capacity);
+ }
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	int imbalance_pct, idx;
+ 
+ 	struct task_struct *best_task;
+ 	long best_imp;
+ 	int best_cpu;
+ };
+ 
+ static void task_numa_assign(struct task_numa_env *env,
+ 			     struct task_struct *p, long imp)
+ {
+ 	if (env->best_task)
+ 		put_task_struct(env->best_task);
+ 	if (p)
+ 		get_task_struct(p);
+ 
+ 	env->best_task = p;
+ 	env->best_imp = imp;
+ 	env->best_cpu = env->dst_cpu;
+ }
+ 
+ /*
+  * This checks if the overall compute and NUMA accesses of the system would
+  * be improved if the source tasks was migrated to the target dst_cpu taking
+  * into account that it might be best if task running on the dst_cpu should
+  * be exchanged with the source task
+  */
+ static void task_numa_compare(struct task_numa_env *env,
+ 			      long taskimp, long groupimp)
+ {
+ 	struct rq *src_rq = cpu_rq(env->src_cpu);
+ 	struct rq *dst_rq = cpu_rq(env->dst_cpu);
+ 	struct task_struct *cur;
+ 	long dst_load, src_load;
+ 	long load;
+ 	long imp = (groupimp > 0) ? groupimp : taskimp;
+ 
+ 	rcu_read_lock();
+ 	cur = ACCESS_ONCE(dst_rq->curr);
+ 	if (cur->pid == 0) /* idle */
+ 		cur = NULL;
+ 
+ 	/*
+ 	 * "imp" is the fault differential for the source task between the
+ 	 * source and destination node. Calculate the total differential for
+ 	 * the source task and potential destination task. The more negative
+ 	 * the value is, the more rmeote accesses that would be expected to
+ 	 * be incurred if the tasks were swapped.
+ 	 */
+ 	if (cur) {
+ 		/* Skip this swap candidate if cannot move to the source cpu */
+ 		if (!cpumask_test_cpu(env->src_cpu, tsk_cpus_allowed(cur)))
+ 			goto unlock;
+ 
+ 		/*
+ 		 * If dst and source tasks are in the same NUMA group, or not
+ 		 * in any group then look only at task weights.
+ 		 */
+ 		if (cur->numa_group == env->p->numa_group) {
+ 			imp = taskimp + task_weight(cur, env->src_nid) -
+ 			      task_weight(cur, env->dst_nid);
+ 			/*
+ 			 * Add some hysteresis to prevent swapping the
+ 			 * tasks within a group over tiny differences.
+ 			 */
+ 			if (cur->numa_group)
+ 				imp -= imp/16;
+ 		} else {
+ 			/*
+ 			 * Compare the group weights. If a task is all by
+ 			 * itself (not part of a group), use the task weight
+ 			 * instead.
+ 			 */
+ 			if (env->p->numa_group)
+ 				imp = groupimp;
+ 			else
+ 				imp = taskimp;
+ 
+ 			if (cur->numa_group)
+ 				imp += group_weight(cur, env->src_nid) -
+ 				       group_weight(cur, env->dst_nid);
+ 			else
+ 				imp += task_weight(cur, env->src_nid) -
+ 				       task_weight(cur, env->dst_nid);
+ 		}
+ 	}
+ 
+ 	if (imp < env->best_imp)
+ 		goto unlock;
+ 
+ 	if (!cur) {
+ 		/* Is there capacity at our destination? */
+ 		if (env->src_stats.has_capacity &&
+ 		    !env->dst_stats.has_capacity)
+ 			goto unlock;
+ 
+ 		goto balance;
+ 	}
+ 
+ 	/* Balance doesn't matter much if we're running a task per cpu */
+ 	if (src_rq->nr_running == 1 && dst_rq->nr_running == 1)
+ 		goto assign;
+ 
+ 	/*
+ 	 * In the overloaded case, try and keep the load balanced.
+ 	 */
+ balance:
+ 	dst_load = env->dst_stats.load;
+ 	src_load = env->src_stats.load;
+ 
+ 	/* XXX missing power terms */
+ 	load = task_h_load(env->p);
+ 	dst_load += load;
+ 	src_load -= load;
+ 
+ 	if (cur) {
+ 		load = task_h_load(cur);
+ 		dst_load -= load;
+ 		src_load += load;
+ 	}
+ 
+ 	/* make src_load the smaller */
+ 	if (dst_load < src_load)
+ 		swap(dst_load, src_load);
+ 
+ 	if (src_load * env->imbalance_pct < dst_load * 100)
+ 		goto unlock;
+ 
+ assign:
+ 	task_numa_assign(env, cur, imp);
+ unlock:
+ 	rcu_read_unlock();
+ }
+ 
+ static void task_numa_find_cpu(struct task_numa_env *env,
+ 				long taskimp, long groupimp)
+ {
+ 	int cpu;
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
+ 		/* Skip this CPU if the source task cannot migrate */
+ 		if (!cpumask_test_cpu(cpu, tsk_cpus_allowed(env->p)))
+ 			continue;
+ 
+ 		env->dst_cpu = cpu;
+ 		task_numa_compare(env, taskimp, groupimp);
+ 	}
+ }
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = task_node(p),
+ 
+ 		.imbalance_pct = 112,
+ 
+ 		.best_task = NULL,
+ 		.best_imp = 0,
+ 		.best_cpu = -1
+ 	};
+ 	struct sched_domain *sd;
+ 	unsigned long taskweight, groupweight;
+ 	int nid, ret;
+ 	long taskimp, groupimp;
+ 
+ 	/*
+ 	 * Pick the lowest SD_NUMA domain, as that would have the smallest
+ 	 * imbalance and would be the first to start moving tasks about.
+ 	 *
+ 	 * And we want to avoid any moving of tasks about, as that would create
+ 	 * random movement of tasks -- counter the numa conditions we're trying
+ 	 * to satisfy here.
+ 	 */
+ 	rcu_read_lock();
+ 	sd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));
+ 	env.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;
+ 	rcu_read_unlock();
+ 
+ 	taskweight = task_weight(p, env.src_nid);
+ 	groupweight = group_weight(p, env.src_nid);
+ 	update_numa_stats(&env.src_stats, env.src_nid);
+ 	env.dst_nid = p->numa_preferred_nid;
+ 	taskimp = task_weight(p, env.dst_nid) - taskweight;
+ 	groupimp = group_weight(p, env.dst_nid) - groupweight;
+ 	update_numa_stats(&env.dst_stats, env.dst_nid);
+ 
+ 	/* If the preferred nid has capacity, try to use it. */
+ 	if (env.dst_stats.has_capacity)
+ 		task_numa_find_cpu(&env, taskimp, groupimp);
+ 
+ 	/* No space available on the preferred nid. Look elsewhere. */
+ 	if (env.best_cpu == -1) {
+ 		for_each_online_node(nid) {
+ 			if (nid == env.src_nid || nid == p->numa_preferred_nid)
+ 				continue;
+ 
+ 			/* Only consider nodes where both task and groups benefit */
+ 			taskimp = task_weight(p, nid) - taskweight;
+ 			groupimp = group_weight(p, nid) - groupweight;
+ 			if (taskimp < 0 && groupimp < 0)
+ 				continue;
+ 
+ 			env.dst_nid = nid;
+ 			update_numa_stats(&env.dst_stats, env.dst_nid);
+ 			task_numa_find_cpu(&env, taskimp, groupimp);
+ 		}
+ 	}
+ 
+ 	/* No better CPU than the current one was found. */
+ 	if (env.best_cpu == -1)
+ 		return -EAGAIN;
+ 
+ 	sched_setnuma(p, env.dst_nid);
+ 
+ 	if (env.best_task == NULL) {
+ 		int ret = migrate_task_to(p, env.best_cpu);
+ 		return ret;
+ 	}
+ 
+ 	ret = migrate_swap(p, env.best_task);
+ 	put_task_struct(env.best_task);
+ 	return ret;
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* Success if task is already running on preferred CPU */
+ 	p->numa_migrate_retry = 0;
+ 	if (cpu_to_node(task_cpu(p)) == p->numa_preferred_nid) {
+ 		/*
+ 		 * If migration is temporarily disabled due to a task migration
+ 		 * then re-enable it now as the task is running on its
+ 		 * preferred node and memory should migrate locally
+ 		 */
+ 		if (!p->numa_migrate_seq)
+ 			p->numa_migrate_seq++;
+ 		return;
+ 	}
+ 
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1))
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	if (task_numa_migrate(p) != 0)
+ 		p->numa_migrate_retry = jiffies + HZ*5;
+ }
+ 
++>>>>>>> 0ec8aa00f2b4 (sched/numa: Avoid migrating tasks that are placed on their preferred node)
  static void task_numa_placement(struct task_struct *p)
  {
 -	int seq, nid, max_nid = -1, max_group_nid = -1;
 -	unsigned long max_faults = 0, max_group_faults = 0;
 -	spinlock_t *group_lock = NULL;
 +	int seq, nid, max_nid = -1;
 +	unsigned long max_faults = 0;
  
 +	if (!p->mm)	/* for example, ksmd faulting in a user's mm */
 +		return;
  	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
  	if (p->numa_scan_seq == seq)
  		return;
@@@ -866,11 -1326,174 +1266,175 @@@
  			max_faults = faults;
  			max_nid = nid;
  		}
 -
 -		if (group_faults > max_group_faults) {
 -			max_group_faults = group_faults;
 -			max_group_nid = nid;
 -		}
  	}
  
++<<<<<<< HEAD
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
 +		p->numa_preferred_nid = max_nid;
++=======
+ 	if (p->numa_group) {
+ 		/*
+ 		 * If the preferred task and group nids are different,
+ 		 * iterate over the nodes again to find the best place.
+ 		 */
+ 		if (max_nid != max_group_nid) {
+ 			unsigned long weight, max_weight = 0;
+ 
+ 			for_each_online_node(nid) {
+ 				weight = task_weight(p, nid) + group_weight(p, nid);
+ 				if (weight > max_weight) {
+ 					max_weight = weight;
+ 					max_nid = nid;
+ 				}
+ 			}
+ 		}
+ 
+ 		spin_unlock(group_lock);
+ 	}
+ 
+ 	/* Preferred node as the node with the most faults */
+ 	if (max_faults && max_nid != p->numa_preferred_nid) {
+ 		/* Update the preferred nid and migrate task if possible */
+ 		sched_setnuma(p, max_nid);
+ 		numa_migrate_preferred(p);
+ 	}
+ }
+ 
+ static inline int get_numa_group(struct numa_group *grp)
+ {
+ 	return atomic_inc_not_zero(&grp->refcount);
+ }
+ 
+ static inline void put_numa_group(struct numa_group *grp)
+ {
+ 	if (atomic_dec_and_test(&grp->refcount))
+ 		kfree_rcu(grp, rcu);
+ }
+ 
+ static void double_lock(spinlock_t *l1, spinlock_t *l2)
+ {
+ 	if (l1 > l2)
+ 		swap(l1, l2);
+ 
+ 	spin_lock(l1);
+ 	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+ }
+ 
+ static void task_numa_group(struct task_struct *p, int cpupid)
+ {
+ 	struct numa_group *grp, *my_grp;
+ 	struct task_struct *tsk;
+ 	bool join = false;
+ 	int cpu = cpupid_to_cpu(cpupid);
+ 	int i;
+ 
+ 	if (unlikely(!p->numa_group)) {
+ 		unsigned int size = sizeof(struct numa_group) +
+ 				    2*nr_node_ids*sizeof(atomic_long_t);
+ 
+ 		grp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);
+ 		if (!grp)
+ 			return;
+ 
+ 		atomic_set(&grp->refcount, 1);
+ 		spin_lock_init(&grp->lock);
+ 		INIT_LIST_HEAD(&grp->task_list);
+ 		grp->gid = p->pid;
+ 
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_set(&grp->faults[i], p->numa_faults[i]);
+ 
+ 		atomic_long_set(&grp->total_faults, p->total_numa_faults);
+ 
+ 		list_add(&p->numa_entry, &grp->task_list);
+ 		grp->nr_tasks++;
+ 		rcu_assign_pointer(p->numa_group, grp);
+ 	}
+ 
+ 	rcu_read_lock();
+ 	tsk = ACCESS_ONCE(cpu_rq(cpu)->curr);
+ 
+ 	if (!cpupid_match_pid(tsk, cpupid))
+ 		goto unlock;
+ 
+ 	grp = rcu_dereference(tsk->numa_group);
+ 	if (!grp)
+ 		goto unlock;
+ 
+ 	my_grp = p->numa_group;
+ 	if (grp == my_grp)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Only join the other group if its bigger; if we're the bigger group,
+ 	 * the other task will join us.
+ 	 */
+ 	if (my_grp->nr_tasks > grp->nr_tasks)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Tie-break on the grp address.
+ 	 */
+ 	if (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)
+ 		goto unlock;
+ 
+ 	if (!get_numa_group(grp))
+ 		goto unlock;
+ 
+ 	join = true;
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 
+ 	if (!join)
+ 		return;
+ 
+ 	for (i = 0; i < 2*nr_node_ids; i++) {
+ 		atomic_long_sub(p->numa_faults[i], &my_grp->faults[i]);
+ 		atomic_long_add(p->numa_faults[i], &grp->faults[i]);
+ 	}
+ 	atomic_long_sub(p->total_numa_faults, &my_grp->total_faults);
+ 	atomic_long_add(p->total_numa_faults, &grp->total_faults);
+ 
+ 	double_lock(&my_grp->lock, &grp->lock);
+ 
+ 	list_move(&p->numa_entry, &grp->task_list);
+ 	my_grp->nr_tasks--;
+ 	grp->nr_tasks++;
+ 
+ 	spin_unlock(&my_grp->lock);
+ 	spin_unlock(&grp->lock);
+ 
+ 	rcu_assign_pointer(p->numa_group, grp);
+ 
+ 	put_numa_group(my_grp);
+ }
+ 
+ void task_numa_free(struct task_struct *p)
+ {
+ 	struct numa_group *grp = p->numa_group;
+ 	int i;
+ 	void *numa_faults = p->numa_faults;
+ 
+ 	if (grp) {
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_sub(p->numa_faults[i], &grp->faults[i]);
+ 
+ 		atomic_long_sub(p->total_numa_faults, &grp->total_faults);
+ 
+ 		spin_lock(&grp->lock);
+ 		list_del(&p->numa_entry);
+ 		grp->nr_tasks--;
+ 		spin_unlock(&grp->lock);
+ 		rcu_assign_pointer(p->numa_group, NULL);
+ 		put_numa_group(grp);
+ 	}
+ 
+ 	p->numa_faults = NULL;
+ 	p->numa_faults_buffer = NULL;
+ 	kfree(numa_faults);
++>>>>>>> 0ec8aa00f2b4 (sched/numa: Avoid migrating tasks that are placed on their preferred node)
  }
  
  /*
@@@ -3983,9 -4632,12 +4561,11 @@@ static bool yield_to_task_fair(struct r
  
  static unsigned long __read_mostly max_load_balance_interval = HZ/10;
  
+ enum fbq_type { regular, remote, all };
+ 
  #define LBF_ALL_PINNED	0x01
  #define LBF_NEED_BREAK	0x02
 -#define LBF_DST_PINNED  0x04
 -#define LBF_SOME_PINNED	0x08
 +#define LBF_SOME_PINNED 0x04
  
  struct lb_env {
  	struct sched_domain	*sd;
@@@ -4397,15 -5114,55 +4979,19 @@@ struct sd_lb_stats 
  struct sg_lb_stats {
  	unsigned long avg_load; /*Avg load across the CPUs of the group */
  	unsigned long group_load; /* Total load over the CPUs of the group */
 +	unsigned long sum_nr_running; /* Nr tasks running in the group */
  	unsigned long sum_weighted_load; /* Weighted load of group's tasks */
 -	unsigned long load_per_task;
 -	unsigned long group_power;
 -	unsigned int sum_nr_running; /* Nr tasks running in the group */
 -	unsigned int group_capacity;
 -	unsigned int idle_cpus;
 -	unsigned int group_weight;
 +	unsigned long group_capacity;
 +	unsigned long idle_cpus;
 +	unsigned long group_weight;
  	int group_imb; /* Is there an imbalance in the group ? */
  	int group_has_capacity; /* Is there extra capacity in the group? */
+ #ifdef CONFIG_NUMA_BALANCING
+ 	unsigned int nr_numa_running;
+ 	unsigned int nr_preferred_running;
+ #endif
  };
  
 -/*
 - * sd_lb_stats - Structure to store the statistics of a sched_domain
 - *		 during load balancing.
 - */
 -struct sd_lb_stats {
 -	struct sched_group *busiest;	/* Busiest group in this sd */
 -	struct sched_group *local;	/* Local group in this sd */
 -	unsigned long total_load;	/* Total load of all groups in sd */
 -	unsigned long total_pwr;	/* Total power of all groups in sd */
 -	unsigned long avg_load;	/* Average load across all groups in sd */
 -
 -	struct sg_lb_stats busiest_stat;/* Statistics of the busiest group */
 -	struct sg_lb_stats local_stat;	/* Statistics of the local group */
 -};
 -
 -static inline void init_sd_lb_stats(struct sd_lb_stats *sds)
 -{
 -	/*
 -	 * Skimp on the clearing to avoid duplicate work. We can avoid clearing
 -	 * local_stat because update_sg_lb_stats() does a full clear/assignment.
 -	 * We must however clear busiest_stat::avg_load because
 -	 * update_sd_pick_busiest() reads this before assignment.
 -	 */
 -	*sds = (struct sd_lb_stats){
 -		.busiest = NULL,
 -		.local = NULL,
 -		.total_load = 0UL,
 -		.total_pwr = 0UL,
 -		.busiest_stat = {
 -			.avg_load = 0UL,
 -		},
 -	};
 -}
 -
  /**
   * get_sd_load_idx - Obtain the load index for a given sched domain.
   * @sd: The sched_domain whose load_idx is to be obtained.
@@@ -4747,8 -5549,7 +5367,12 @@@ static inline enum fbq_type fbq_classif
   * @balance: Should we balance.
   * @sds: variable to hold the statistics for this sched_domain.
   */
++<<<<<<< HEAD
 +static inline void update_sd_lb_stats(struct lb_env *env,
 +					int *balance, struct sd_lb_stats *sds)
++=======
+ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)
++>>>>>>> 0ec8aa00f2b4 (sched/numa: Avoid migrating tasks that are placed on their preferred node)
  {
  	struct sched_domain *child = env->sd->child;
  	struct sched_group *sg = env->sd->groups;
@@@ -4783,30 -5590,25 +5407,33 @@@
  		 * heaviest group when it is already under-utilized (possible
  		 * with a large weight task outweighs the tasks on the system).
  		 */
 -		if (prefer_sibling && sds->local &&
 -		    sds->local_stat.group_has_capacity)
 -			sgs->group_capacity = min(sgs->group_capacity, 1U);
 +		if (prefer_sibling && !local_group && sds->this_has_capacity)
 +			sgs.group_capacity = min(sgs.group_capacity, 1UL);
  
 -		if (update_sd_pick_busiest(env, sds, sg, sgs)) {
 +		if (local_group) {
 +			sds->this_load = sgs.avg_load;
 +			sds->this = sg;
 +			sds->this_nr_running = sgs.sum_nr_running;
 +			sds->this_load_per_task = sgs.sum_weighted_load;
 +			sds->this_has_capacity = sgs.group_has_capacity;
 +			sds->this_idle_cpus = sgs.idle_cpus;
 +		} else if (update_sd_pick_busiest(env, sds, sg, &sgs)) {
 +			sds->max_load = sgs.avg_load;
  			sds->busiest = sg;
 -			sds->busiest_stat = *sgs;
 +			sds->busiest_nr_running = sgs.sum_nr_running;
 +			sds->busiest_idle_cpus = sgs.idle_cpus;
 +			sds->busiest_group_capacity = sgs.group_capacity;
 +			sds->busiest_load_per_task = sgs.sum_weighted_load;
 +			sds->busiest_has_capacity = sgs.group_has_capacity;
 +			sds->busiest_group_weight = sgs.group_weight;
 +			sds->group_imb = sgs.group_imb;
  		}
  
 -next_group:
 -		/* Now, start updating sd_lb_stats */
 -		sds->total_load += sgs->group_load;
 -		sds->total_pwr += sgs->group_power;
 -
  		sg = sg->next;
  	} while (sg != env->sd->groups);
+ 
+ 	if (env->sd->flags & SD_NUMA)
+ 		env->fbq_type = fbq_classify_group(&sds->busiest_stat);
  }
  
  /**
@@@ -5103,22 -5908,43 +5730,58 @@@ static struct rq *find_busiest_queue(st
  				     struct sched_group *group)
  {
  	struct rq *busiest = NULL, *rq;
 -	unsigned long busiest_load = 0, busiest_power = 1;
 +	unsigned long max_load = 0;
  	int i;
  
++<<<<<<< HEAD
 +	for_each_cpu(i, sched_group_cpus(group)) {
 +		unsigned long power = power_of(i);
 +		unsigned long capacity = DIV_ROUND_CLOSEST(power,
 +							   SCHED_POWER_SCALE);
 +		unsigned long wl;
++=======
+ 	for_each_cpu_and(i, sched_group_cpus(group), env->cpus) {
+ 		unsigned long power, capacity, wl;
+ 		enum fbq_type rt;
++>>>>>>> 0ec8aa00f2b4 (sched/numa: Avoid migrating tasks that are placed on their preferred node)
+ 
+ 		rq = cpu_rq(i);
+ 		rt = fbq_classify_rq(rq);
+ 
+ 		/*
+ 		 * We classify groups/runqueues into three groups:
+ 		 *  - regular: there are !numa tasks
+ 		 *  - remote:  there are numa tasks that run on the 'wrong' node
+ 		 *  - all:     there is no distinction
+ 		 *
+ 		 * In order to avoid migrating ideally placed numa tasks,
+ 		 * ignore those when there's better options.
+ 		 *
+ 		 * If we ignore the actual busiest queue to migrate another
+ 		 * task, the next balance pass can still reduce the busiest
+ 		 * queue by moving tasks around inside the node.
+ 		 *
+ 		 * If we cannot move enough load due to this classification
+ 		 * the next pass will adjust the group classification and
+ 		 * allow migration of more tasks.
+ 		 *
+ 		 * Both cases only affect the total convergence complexity.
+ 		 */
+ 		if (rt > env->fbq_type)
+ 			continue;
  
+ 		power = power_of(i);
+ 		capacity = DIV_ROUND_CLOSEST(power, SCHED_POWER_SCALE);
  		if (!capacity)
  			capacity = fix_small_capacity(env->sd, group);
  
++<<<<<<< HEAD
 +		if (!cpumask_test_cpu(i, env->cpus))
 +			continue;
 +
 +		rq = cpu_rq(i);
++=======
++>>>>>>> 0ec8aa00f2b4 (sched/numa: Avoid migrating tasks that are placed on their preferred node)
  		wl = weighted_cpuload(i);
  
  		/*
diff --cc kernel/sched/sched.h
index 5abdf358588f,d69cb325c27e..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -552,14 -561,9 +556,20 @@@ static inline u64 rq_clock_task(struct 
  }
  
  #ifdef CONFIG_NUMA_BALANCING
++<<<<<<< HEAD
 +static inline void task_numa_free(struct task_struct *p)
 +{
 +	kfree(p->numa_faults);
 +}
 +#else /* CONFIG_NUMA_BALANCING */
 +static inline void task_numa_free(struct task_struct *p)
 +{
 +}
++=======
+ extern void sched_setnuma(struct task_struct *p, int node);
+ extern int migrate_task_to(struct task_struct *p, int cpu);
+ extern int migrate_swap(struct task_struct *, struct task_struct *);
++>>>>>>> 0ec8aa00f2b4 (sched/numa: Avoid migrating tasks that are placed on their preferred node)
  #endif /* CONFIG_NUMA_BALANCING */
  
  #ifdef CONFIG_SMP
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/sched.h
