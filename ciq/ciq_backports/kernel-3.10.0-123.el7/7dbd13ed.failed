sched/numa: Prevent parallel updates to group stats during placement

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Mel Gorman <mgorman@suse.de>
commit 7dbd13ed06513b047216a7ffc718bad9df0660f1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/7dbd13ed.failed

Having multiple tasks in a group go through task_numa_placement
simultaneously can lead to a task picking a wrong node to run on, because
the group stats may be in the middle of an update. This patch avoids
parallel updates by holding the numa_group lock during placement
decisions.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-52-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 7dbd13ed06513b047216a7ffc718bad9df0660f1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 98cee68da024,147349987bfe..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,38 -879,575 +839,76 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
 -/*
 - * Once a preferred node is selected the scheduler balancer will prefer moving
 - * a task to that node for sysctl_numa_balancing_settle_count number of PTE
 - * scans. This will give the process the chance to accumulate more faults on
 - * the preferred node but still allow the scheduler to move the task again if
 - * the nodes CPUs are overloaded.
 - */
 -unsigned int sysctl_numa_balancing_settle_count __read_mostly = 4;
 -
 -struct numa_group {
 -	atomic_t refcount;
 -
 -	spinlock_t lock; /* nr_tasks, tasks */
 -	int nr_tasks;
 -	pid_t gid;
 -	struct list_head task_list;
 -
 -	struct rcu_head rcu;
 -	atomic_long_t total_faults;
 -	atomic_long_t faults[0];
 -};
 -
 -pid_t task_numa_group_id(struct task_struct *p)
 -{
 -	return p->numa_group ? p->numa_group->gid : 0;
 -}
 -
 -static inline int task_faults_idx(int nid, int priv)
 -{
 -	return 2 * nid + priv;
 -}
 -
 -static inline unsigned long task_faults(struct task_struct *p, int nid)
 -{
 -	if (!p->numa_faults)
 -		return 0;
 -
 -	return p->numa_faults[task_faults_idx(nid, 0)] +
 -		p->numa_faults[task_faults_idx(nid, 1)];
 -}
 -
 -static inline unsigned long group_faults(struct task_struct *p, int nid)
 -{
 -	if (!p->numa_group)
 -		return 0;
 -
 -	return atomic_long_read(&p->numa_group->faults[2*nid]) +
 -	       atomic_long_read(&p->numa_group->faults[2*nid+1]);
 -}
 -
 -/*
 - * These return the fraction of accesses done by a particular task, or
 - * task group, on a particular numa node.  The group weight is given a
 - * larger multiplier, in order to group tasks together that are almost
 - * evenly spread out between numa nodes.
 - */
 -static inline unsigned long task_weight(struct task_struct *p, int nid)
 -{
 -	unsigned long total_faults;
 -
 -	if (!p->numa_faults)
 -		return 0;
 -
 -	total_faults = p->total_numa_faults;
 -
 -	if (!total_faults)
 -		return 0;
 -
 -	return 1000 * task_faults(p, nid) / total_faults;
 -}
 -
 -static inline unsigned long group_weight(struct task_struct *p, int nid)
 -{
 -	unsigned long total_faults;
 -
 -	if (!p->numa_group)
 -		return 0;
 -
 -	total_faults = atomic_long_read(&p->numa_group->total_faults);
 -
 -	if (!total_faults)
 -		return 0;
 -
 -	return 1200 * group_faults(p, nid) / total_faults;
 -}
 -
 -static unsigned long weighted_cpuload(const int cpu);
 -static unsigned long source_load(int cpu, int type);
 -static unsigned long target_load(int cpu, int type);
 -static unsigned long power_of(int cpu);
 -static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
 -
 -/* Cached statistics for all CPUs within a node */
 -struct numa_stats {
 -	unsigned long nr_running;
 -	unsigned long load;
 -
 -	/* Total compute capacity of CPUs on a node */
 -	unsigned long power;
 -
 -	/* Approximate capacity in terms of runnable tasks on a node */
 -	unsigned long capacity;
 -	int has_capacity;
 -};
 -
 -/*
 - * XXX borrowed from update_sg_lb_stats
 - */
 -static void update_numa_stats(struct numa_stats *ns, int nid)
 -{
 -	int cpu;
 -
 -	memset(ns, 0, sizeof(*ns));
 -	for_each_cpu(cpu, cpumask_of_node(nid)) {
 -		struct rq *rq = cpu_rq(cpu);
 -
 -		ns->nr_running += rq->nr_running;
 -		ns->load += weighted_cpuload(cpu);
 -		ns->power += power_of(cpu);
 -	}
 -
 -	ns->load = (ns->load * SCHED_POWER_SCALE) / ns->power;
 -	ns->capacity = DIV_ROUND_CLOSEST(ns->power, SCHED_POWER_SCALE);
 -	ns->has_capacity = (ns->nr_running < ns->capacity);
 -}
 -
 -struct task_numa_env {
 -	struct task_struct *p;
 -
 -	int src_cpu, src_nid;
 -	int dst_cpu, dst_nid;
 -
 -	struct numa_stats src_stats, dst_stats;
 -
 -	int imbalance_pct, idx;
 -
 -	struct task_struct *best_task;
 -	long best_imp;
 -	int best_cpu;
 -};
 -
 -static void task_numa_assign(struct task_numa_env *env,
 -			     struct task_struct *p, long imp)
 -{
 -	if (env->best_task)
 -		put_task_struct(env->best_task);
 -	if (p)
 -		get_task_struct(p);
 -
 -	env->best_task = p;
 -	env->best_imp = imp;
 -	env->best_cpu = env->dst_cpu;
 -}
 -
 -/*
 - * This checks if the overall compute and NUMA accesses of the system would
 - * be improved if the source tasks was migrated to the target dst_cpu taking
 - * into account that it might be best if task running on the dst_cpu should
 - * be exchanged with the source task
 - */
 -static void task_numa_compare(struct task_numa_env *env, long imp)
 -{
 -	struct rq *src_rq = cpu_rq(env->src_cpu);
 -	struct rq *dst_rq = cpu_rq(env->dst_cpu);
 -	struct task_struct *cur;
 -	long dst_load, src_load;
 -	long load;
 -
 -	rcu_read_lock();
 -	cur = ACCESS_ONCE(dst_rq->curr);
 -	if (cur->pid == 0) /* idle */
 -		cur = NULL;
 -
 -	/*
 -	 * "imp" is the fault differential for the source task between the
 -	 * source and destination node. Calculate the total differential for
 -	 * the source task and potential destination task. The more negative
 -	 * the value is, the more rmeote accesses that would be expected to
 -	 * be incurred if the tasks were swapped.
 -	 */
 -	if (cur) {
 -		/* Skip this swap candidate if cannot move to the source cpu */
 -		if (!cpumask_test_cpu(env->src_cpu, tsk_cpus_allowed(cur)))
 -			goto unlock;
 -
 -		imp += task_weight(cur, env->src_nid) +
 -		       group_weight(cur, env->src_nid) -
 -		       task_weight(cur, env->dst_nid) -
 -		       group_weight(cur, env->dst_nid);
 -	}
 -
 -	if (imp < env->best_imp)
 -		goto unlock;
 -
 -	if (!cur) {
 -		/* Is there capacity at our destination? */
 -		if (env->src_stats.has_capacity &&
 -		    !env->dst_stats.has_capacity)
 -			goto unlock;
 -
 -		goto balance;
 -	}
 -
 -	/* Balance doesn't matter much if we're running a task per cpu */
 -	if (src_rq->nr_running == 1 && dst_rq->nr_running == 1)
 -		goto assign;
 -
 -	/*
 -	 * In the overloaded case, try and keep the load balanced.
 -	 */
 -balance:
 -	dst_load = env->dst_stats.load;
 -	src_load = env->src_stats.load;
 -
 -	/* XXX missing power terms */
 -	load = task_h_load(env->p);
 -	dst_load += load;
 -	src_load -= load;
 -
 -	if (cur) {
 -		load = task_h_load(cur);
 -		dst_load -= load;
 -		src_load += load;
 -	}
 -
 -	/* make src_load the smaller */
 -	if (dst_load < src_load)
 -		swap(dst_load, src_load);
 -
 -	if (src_load * env->imbalance_pct < dst_load * 100)
 -		goto unlock;
 -
 -assign:
 -	task_numa_assign(env, cur, imp);
 -unlock:
 -	rcu_read_unlock();
 -}
 -
 -static void task_numa_find_cpu(struct task_numa_env *env, long imp)
 -{
 -	int cpu;
 -
 -	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
 -		/* Skip this CPU if the source task cannot migrate */
 -		if (!cpumask_test_cpu(cpu, tsk_cpus_allowed(env->p)))
 -			continue;
 -
 -		env->dst_cpu = cpu;
 -		task_numa_compare(env, imp);
 -	}
 -}
 -
 -static int task_numa_migrate(struct task_struct *p)
 -{
 -	struct task_numa_env env = {
 -		.p = p,
 -
 -		.src_cpu = task_cpu(p),
 -		.src_nid = cpu_to_node(task_cpu(p)),
 -
 -		.imbalance_pct = 112,
 -
 -		.best_task = NULL,
 -		.best_imp = 0,
 -		.best_cpu = -1
 -	};
 -	struct sched_domain *sd;
 -	unsigned long weight;
 -	int nid, ret;
 -	long imp;
 -
 -	/*
 -	 * Pick the lowest SD_NUMA domain, as that would have the smallest
 -	 * imbalance and would be the first to start moving tasks about.
 -	 *
 -	 * And we want to avoid any moving of tasks about, as that would create
 -	 * random movement of tasks -- counter the numa conditions we're trying
 -	 * to satisfy here.
 -	 */
 -	rcu_read_lock();
 -	sd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));
 -	env.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;
 -	rcu_read_unlock();
 -
 -	weight = task_weight(p, env.src_nid) + group_weight(p, env.src_nid);
 -	update_numa_stats(&env.src_stats, env.src_nid);
 -	env.dst_nid = p->numa_preferred_nid;
 -	imp = task_weight(p, env.dst_nid) + group_weight(p, env.dst_nid) - weight;
 -	update_numa_stats(&env.dst_stats, env.dst_nid);
 -
 -	/* If the preferred nid has capacity, try to use it. */
 -	if (env.dst_stats.has_capacity)
 -		task_numa_find_cpu(&env, imp);
 -
 -	/* No space available on the preferred nid. Look elsewhere. */
 -	if (env.best_cpu == -1) {
 -		for_each_online_node(nid) {
 -			if (nid == env.src_nid || nid == p->numa_preferred_nid)
 -				continue;
 -
 -			/* Only consider nodes where both task and groups benefit */
 -			imp = task_weight(p, nid) + group_weight(p, nid) - weight;
 -			if (imp < 0)
 -				continue;
 -
 -			env.dst_nid = nid;
 -			update_numa_stats(&env.dst_stats, env.dst_nid);
 -			task_numa_find_cpu(&env, imp);
 -		}
 -	}
 -
 -	/* No better CPU than the current one was found. */
 -	if (env.best_cpu == -1)
 -		return -EAGAIN;
 -
 -	if (env.best_task == NULL) {
 -		int ret = migrate_task_to(p, env.best_cpu);
 -		return ret;
 -	}
 -
 -	ret = migrate_swap(p, env.best_task);
 -	put_task_struct(env.best_task);
 -	return ret;
 -}
 -
 -/* Attempt to migrate a task to a CPU on the preferred node. */
 -static void numa_migrate_preferred(struct task_struct *p)
 -{
 -	/* Success if task is already running on preferred CPU */
 -	p->numa_migrate_retry = 0;
 -	if (cpu_to_node(task_cpu(p)) == p->numa_preferred_nid) {
 -		/*
 -		 * If migration is temporarily disabled due to a task migration
 -		 * then re-enable it now as the task is running on its
 -		 * preferred node and memory should migrate locally
 -		 */
 -		if (!p->numa_migrate_seq)
 -			p->numa_migrate_seq++;
 -		return;
 -	}
 -
 -	/* This task has no NUMA fault statistics yet */
 -	if (unlikely(p->numa_preferred_nid == -1))
 -		return;
 -
 -	/* Otherwise, try migrate to a CPU on the preferred node */
 -	if (task_numa_migrate(p) != 0)
 -		p->numa_migrate_retry = jiffies + HZ*5;
 -}
 -
  static void task_numa_placement(struct task_struct *p)
  {
++<<<<<<< HEAD
 +	int seq, nid, max_nid = -1;
 +	unsigned long max_faults = 0;
++=======
+ 	int seq, nid, max_nid = -1, max_group_nid = -1;
+ 	unsigned long max_faults = 0, max_group_faults = 0;
+ 	spinlock_t *group_lock = NULL;
++>>>>>>> 7dbd13ed0651 (sched/numa: Prevent parallel updates to group stats during placement)
  
 +	if (!p->mm)	/* for example, ksmd faulting in a user's mm */
 +		return;
  	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
  	if (p->numa_scan_seq == seq)
  		return;
  	p->numa_scan_seq = seq;
 -	p->numa_migrate_seq++;
  	p->numa_scan_period_max = task_scan_max(p);
  
+ 	/* If the task is part of a group prevent parallel updates to group stats */
+ 	if (p->numa_group) {
+ 		group_lock = &p->numa_group->lock;
+ 		spin_lock(group_lock);
+ 	}
+ 
  	/* Find the node with the highest number of faults */
  	for_each_online_node(nid) {
 -		unsigned long faults = 0, group_faults = 0;
 -		int priv, i;
 -
 -		for (priv = 0; priv < 2; priv++) {
 -			long diff;
 -
 -			i = task_faults_idx(nid, priv);
 -			diff = -p->numa_faults[i];
 -
 -			/* Decay existing window, copy faults since last scan */
 -			p->numa_faults[i] >>= 1;
 -			p->numa_faults[i] += p->numa_faults_buffer[i];
 -			p->numa_faults_buffer[i] = 0;
 -
 -			faults += p->numa_faults[i];
 -			diff += p->numa_faults[i];
 -			p->total_numa_faults += diff;
 -			if (p->numa_group) {
 -				/* safe because we can only change our own group */
 -				atomic_long_add(diff, &p->numa_group->faults[i]);
 -				atomic_long_add(diff, &p->numa_group->total_faults);
 -				group_faults += atomic_long_read(&p->numa_group->faults[i]);
 -			}
 -		}
 +		unsigned long faults;
  
 +		/* Decay existing window and copy faults since last scan */
 +		p->numa_faults[nid] >>= 1;
 +		p->numa_faults[nid] += p->numa_faults_buffer[nid];
 +		p->numa_faults_buffer[nid] = 0;
 +
 +		faults = p->numa_faults[nid];
  		if (faults > max_faults) {
  			max_faults = faults;
  			max_nid = nid;
  		}
 -
 -		if (group_faults > max_group_faults) {
 -			max_group_faults = group_faults;
 -			max_group_nid = nid;
 -		}
  	}
  
++<<<<<<< HEAD
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
++=======
+ 	if (p->numa_group) {
+ 		/*
+ 		 * If the preferred task and group nids are different,
+ 		 * iterate over the nodes again to find the best place.
+ 		 */
+ 		if (max_nid != max_group_nid) {
+ 			unsigned long weight, max_weight = 0;
+ 
+ 			for_each_online_node(nid) {
+ 				weight = task_weight(p, nid) + group_weight(p, nid);
+ 				if (weight > max_weight) {
+ 					max_weight = weight;
+ 					max_nid = nid;
+ 				}
+ 			}
+ 		}
+ 
+ 		spin_unlock(group_lock);
+ 	}
+ 
+ 	/* Preferred node as the node with the most faults */
+ 	if (max_faults && max_nid != p->numa_preferred_nid) {
+ 		/* Update the preferred nid and migrate task if possible */
++>>>>>>> 7dbd13ed0651 (sched/numa: Prevent parallel updates to group stats during placement)
  		p->numa_preferred_nid = max_nid;
 -		p->numa_migrate_seq = 1;
 -		numa_migrate_preferred(p);
 -	}
 -}
 -
 -static inline int get_numa_group(struct numa_group *grp)
 -{
 -	return atomic_inc_not_zero(&grp->refcount);
 -}
 -
 -static inline void put_numa_group(struct numa_group *grp)
 -{
 -	if (atomic_dec_and_test(&grp->refcount))
 -		kfree_rcu(grp, rcu);
 -}
 -
 -static void double_lock(spinlock_t *l1, spinlock_t *l2)
 -{
 -	if (l1 > l2)
 -		swap(l1, l2);
 -
 -	spin_lock(l1);
 -	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
 -}
 -
 -static void task_numa_group(struct task_struct *p, int cpupid)
 -{
 -	struct numa_group *grp, *my_grp;
 -	struct task_struct *tsk;
 -	bool join = false;
 -	int cpu = cpupid_to_cpu(cpupid);
 -	int i;
 -
 -	if (unlikely(!p->numa_group)) {
 -		unsigned int size = sizeof(struct numa_group) +
 -				    2*nr_node_ids*sizeof(atomic_long_t);
 -
 -		grp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);
 -		if (!grp)
 -			return;
 -
 -		atomic_set(&grp->refcount, 1);
 -		spin_lock_init(&grp->lock);
 -		INIT_LIST_HEAD(&grp->task_list);
 -		grp->gid = p->pid;
 -
 -		for (i = 0; i < 2*nr_node_ids; i++)
 -			atomic_long_set(&grp->faults[i], p->numa_faults[i]);
 -
 -		atomic_long_set(&grp->total_faults, p->total_numa_faults);
 -
 -		list_add(&p->numa_entry, &grp->task_list);
 -		grp->nr_tasks++;
 -		rcu_assign_pointer(p->numa_group, grp);
 -	}
 -
 -	rcu_read_lock();
 -	tsk = ACCESS_ONCE(cpu_rq(cpu)->curr);
 -
 -	if (!cpupid_match_pid(tsk, cpupid))
 -		goto unlock;
 -
 -	grp = rcu_dereference(tsk->numa_group);
 -	if (!grp)
 -		goto unlock;
 -
 -	my_grp = p->numa_group;
 -	if (grp == my_grp)
 -		goto unlock;
 -
 -	/*
 -	 * Only join the other group if its bigger; if we're the bigger group,
 -	 * the other task will join us.
 -	 */
 -	if (my_grp->nr_tasks > grp->nr_tasks)
 -		goto unlock;
 -
 -	/*
 -	 * Tie-break on the grp address.
 -	 */
 -	if (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)
 -		goto unlock;
 -
 -	if (!get_numa_group(grp))
 -		goto unlock;
 -
 -	join = true;
 -
 -unlock:
 -	rcu_read_unlock();
 -
 -	if (!join)
 -		return;
 -
 -	for (i = 0; i < 2*nr_node_ids; i++) {
 -		atomic_long_sub(p->numa_faults[i], &my_grp->faults[i]);
 -		atomic_long_add(p->numa_faults[i], &grp->faults[i]);
 -	}
 -	atomic_long_sub(p->total_numa_faults, &my_grp->total_faults);
 -	atomic_long_add(p->total_numa_faults, &grp->total_faults);
 -
 -	double_lock(&my_grp->lock, &grp->lock);
 -
 -	list_move(&p->numa_entry, &grp->task_list);
 -	my_grp->nr_tasks--;
 -	grp->nr_tasks++;
 -
 -	spin_unlock(&my_grp->lock);
 -	spin_unlock(&grp->lock);
 -
 -	rcu_assign_pointer(p->numa_group, grp);
 -
 -	put_numa_group(my_grp);
 -}
 -
 -void task_numa_free(struct task_struct *p)
 -{
 -	struct numa_group *grp = p->numa_group;
 -	int i;
 -	void *numa_faults = p->numa_faults;
 -
 -	if (grp) {
 -		for (i = 0; i < 2*nr_node_ids; i++)
 -			atomic_long_sub(p->numa_faults[i], &grp->faults[i]);
 -
 -		atomic_long_sub(p->total_numa_faults, &grp->total_faults);
 -
 -		spin_lock(&grp->lock);
 -		list_del(&p->numa_entry);
 -		grp->nr_tasks--;
 -		spin_unlock(&grp->lock);
 -		rcu_assign_pointer(p->numa_group, NULL);
 -		put_numa_group(grp);
 -	}
 -
 -	p->numa_faults = NULL;
 -	p->numa_faults_buffer = NULL;
 -	kfree(numa_faults);
  }
  
  /*
* Unmerged path kernel/sched/fair.c
