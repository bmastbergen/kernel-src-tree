mm: numa: Change page last {nid,pid} into {cpu,pid}

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] numa: Change page last {nid, pid} into {cpu, pid} (Rik van Riel) [683513]
Rebuild_FUZZ: 94.00%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 90572890d202527c366aa9489b32404e88a7c020
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/90572890.failed

Change the per page last fault tracking to use cpu,pid instead of
nid,pid. This will allow us to try and lookup the alternate task more
easily. Note that even though it is the cpu that is store in the page
flags that the mpol_misplaced decision is still based on the node.

	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Link: http://lkml.kernel.org/r/1381141781-10992-43-git-send-email-mgorman@suse.de
[ Fixed build failure on 32-bit systems. ]
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 90572890d202527c366aa9489b32404e88a7c020)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	include/linux/mm_types.h
#	include/linux/page-flags-layout.h
#	kernel/sched/fair.c
#	mm/huge_memory.c
#	mm/memory.c
#	mm/mempolicy.c
#	mm/migrate.c
#	mm/mm_init.c
#	mm/mmzone.c
#	mm/mprotect.c
#	mm/page_alloc.c
diff --cc include/linux/mm.h
index 958e9efd02a7,ce464cd4777e..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -585,11 -581,11 +585,19 @@@ static inline pte_t maybe_mkwrite(pte_
   * sets it, so none of the operations on it need to be atomic.
   */
  
++<<<<<<< HEAD
 +/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_NID] | ... | FLAGS | */
 +#define SECTIONS_PGOFF		((sizeof(unsigned long)*8) - SECTIONS_WIDTH)
 +#define NODES_PGOFF		(SECTIONS_PGOFF - NODES_WIDTH)
 +#define ZONES_PGOFF		(NODES_PGOFF - ZONES_WIDTH)
 +#define LAST_NID_PGOFF		(ZONES_PGOFF - LAST_NID_WIDTH)
++=======
+ /* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */
+ #define SECTIONS_PGOFF		((sizeof(unsigned long)*8) - SECTIONS_WIDTH)
+ #define NODES_PGOFF		(SECTIONS_PGOFF - NODES_WIDTH)
+ #define ZONES_PGOFF		(NODES_PGOFF - ZONES_WIDTH)
+ #define LAST_CPUPID_PGOFF	(ZONES_PGOFF - LAST_CPUPID_WIDTH)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  /*
   * Define the bit shifts to access each section.  For non-existent
@@@ -599,7 -595,7 +607,11 @@@
  #define SECTIONS_PGSHIFT	(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))
  #define NODES_PGSHIFT		(NODES_PGOFF * (NODES_WIDTH != 0))
  #define ZONES_PGSHIFT		(ZONES_PGOFF * (ZONES_WIDTH != 0))
++<<<<<<< HEAD
 +#define LAST_NID_PGSHIFT	(LAST_NID_PGOFF * (LAST_NID_WIDTH != 0))
++=======
+ #define LAST_CPUPID_PGSHIFT	(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  /* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
  #ifdef NODE_NOT_IN_PAGE_FLAGS
@@@ -621,7 -617,7 +633,11 @@@
  #define ZONES_MASK		((1UL << ZONES_WIDTH) - 1)
  #define NODES_MASK		((1UL << NODES_WIDTH) - 1)
  #define SECTIONS_MASK		((1UL << SECTIONS_WIDTH) - 1)
++<<<<<<< HEAD
 +#define LAST_NID_MASK		((1UL << LAST_NID_WIDTH) - 1)
++=======
+ #define LAST_CPUPID_MASK	((1UL << LAST_CPUPID_WIDTH) - 1)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  #define ZONEID_MASK		((1UL << ZONEID_SHIFT) - 1)
  
  static inline enum zone_type page_zonenum(const struct page *page)
@@@ -665,51 -661,106 +681,149 @@@ static inline int page_to_nid(const str
  #endif
  
  #ifdef CONFIG_NUMA_BALANCING
++<<<<<<< HEAD
 +#ifdef LAST_NID_NOT_IN_PAGE_FLAGS
 +static inline int page_nid_xchg_last(struct page *page, int nid)
 +{
 +	return xchg(&page->_last_nid, nid);
 +}
 +
 +static inline int page_nid_last(struct page *page)
 +{
 +	return page->_last_nid;
 +}
 +static inline void page_nid_reset_last(struct page *page)
 +{
 +	page->_last_nid = -1;
 +}
 +#else
 +static inline int page_nid_last(struct page *page)
 +{
 +	return (page->flags >> LAST_NID_PGSHIFT) & LAST_NID_MASK;
 +}
 +
 +extern int page_nid_xchg_last(struct page *page, int nid);
 +
 +static inline void page_nid_reset_last(struct page *page)
 +{
 +	int nid = (1 << LAST_NID_SHIFT) - 1;
 +
 +	page->flags &= ~(LAST_NID_MASK << LAST_NID_PGSHIFT);
 +	page->flags |= (nid & LAST_NID_MASK) << LAST_NID_PGSHIFT;
 +}
 +#endif /* LAST_NID_NOT_IN_PAGE_FLAGS */
 +#else
 +static inline int page_nid_xchg_last(struct page *page, int nid)
++=======
+ static inline int cpu_pid_to_cpupid(int cpu, int pid)
+ {
+ 	return ((cpu & LAST__CPU_MASK) << LAST__PID_SHIFT) | (pid & LAST__PID_MASK);
+ }
+ 
+ static inline int cpupid_to_pid(int cpupid)
+ {
+ 	return cpupid & LAST__PID_MASK;
+ }
+ 
+ static inline int cpupid_to_cpu(int cpupid)
+ {
+ 	return (cpupid >> LAST__PID_SHIFT) & LAST__CPU_MASK;
+ }
+ 
+ static inline int cpupid_to_nid(int cpupid)
+ {
+ 	return cpu_to_node(cpupid_to_cpu(cpupid));
+ }
+ 
+ static inline bool cpupid_pid_unset(int cpupid)
+ {
+ 	return cpupid_to_pid(cpupid) == (-1 & LAST__PID_MASK);
+ }
+ 
+ static inline bool cpupid_cpu_unset(int cpupid)
+ {
+ 	return cpupid_to_cpu(cpupid) == (-1 & LAST__CPU_MASK);
+ }
+ 
+ #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
+ static inline int page_cpupid_xchg_last(struct page *page, int cpupid)
+ {
+ 	return xchg(&page->_last_cpupid, cpupid);
+ }
+ 
+ static inline int page_cpupid_last(struct page *page)
+ {
+ 	return page->_last_cpupid;
+ }
+ static inline void page_cpupid_reset_last(struct page *page)
  {
- 	return page_to_nid(page);
+ 	page->_last_cpupid = -1;
  }
+ #else
+ static inline int page_cpupid_last(struct page *page)
+ {
+ 	return (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;
+ }
+ 
+ extern int page_cpupid_xchg_last(struct page *page, int cpupid);
  
+ static inline void page_cpupid_reset_last(struct page *page)
+ {
+ 	int cpupid = (1 << LAST_CPUPID_SHIFT) - 1;
+ 
+ 	page->flags &= ~(LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT);
+ 	page->flags |= (cpupid & LAST_CPUPID_MASK) << LAST_CPUPID_PGSHIFT;
+ }
+ #endif /* LAST_CPUPID_NOT_IN_PAGE_FLAGS */
+ #else /* !CONFIG_NUMA_BALANCING */
+ static inline int page_cpupid_xchg_last(struct page *page, int cpupid)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
+ {
+ 	return page_to_nid(page); /* XXX */
+ }
+ 
++<<<<<<< HEAD
 +static inline int page_nid_last(struct page *page)
++=======
+ static inline int page_cpupid_last(struct page *page)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  {
- 	return page_to_nid(page);
+ 	return page_to_nid(page); /* XXX */
  }
  
++<<<<<<< HEAD
 +static inline void page_nid_reset_last(struct page *page)
++=======
+ static inline int cpupid_to_nid(int cpupid)
  {
+ 	return -1;
  }
- #endif
+ 
+ static inline int cpupid_to_pid(int cpupid)
+ {
+ 	return -1;
+ }
+ 
+ static inline int cpupid_to_cpu(int cpupid)
+ {
+ 	return -1;
+ }
+ 
+ static inline int cpu_pid_to_cpupid(int nid, int pid)
+ {
+ 	return -1;
+ }
+ 
+ static inline bool cpupid_pid_unset(int cpupid)
+ {
+ 	return 1;
+ }
+ 
+ static inline void page_cpupid_reset_last(struct page *page)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
+ {
+ }
+ #endif /* CONFIG_NUMA_BALANCING */
  
  static inline struct zone *page_zone(const struct page *page)
  {
diff --cc include/linux/mm_types.h
index ace9a5f01c64,a30f9ca66557..000000000000
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@@ -174,8 -174,8 +174,13 @@@ struct page 
  	void *shadow;
  #endif
  
++<<<<<<< HEAD
 +#ifdef LAST_NID_NOT_IN_PAGE_FLAGS
 +	int _last_nid;
++=======
+ #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
+ 	int _last_cpupid;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  #endif
  }
  /*
diff --cc include/linux/page-flags-layout.h
index 93506a114034,da523661500a..000000000000
--- a/include/linux/page-flags-layout.h
+++ b/include/linux/page-flags-layout.h
@@@ -38,10 -38,10 +38,17 @@@
   * The last is when there is insufficient space in page->flags and a separate
   * lookup is necessary.
   *
++<<<<<<< HEAD
 + * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE |          ... | FLAGS |
 + *         " plus space for last_nid: |       NODE     | ZONE | LAST_NID ... | FLAGS |
 + * classic sparse with space for node:| SECTION | NODE | ZONE |          ... | FLAGS |
 + *         " plus space for last_nid: | SECTION | NODE | ZONE | LAST_NID ... | FLAGS |
++=======
+  * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE |             ... | FLAGS |
+  *      " plus space for last_cpupid: |       NODE     | ZONE | LAST_CPUPID ... | FLAGS |
+  * classic sparse with space for node:| SECTION | NODE | ZONE |             ... | FLAGS |
+  *      " plus space for last_cpupid: | SECTION | NODE | ZONE | LAST_CPUPID ... | FLAGS |
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
   * classic sparse no space for node:  | SECTION |     ZONE    | ... | FLAGS |
   */
  #if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)
@@@ -62,15 -62,21 +69,33 @@@
  #endif
  
  #ifdef CONFIG_NUMA_BALANCING
++<<<<<<< HEAD
 +#define LAST_NID_SHIFT NODES_SHIFT
 +#else
 +#define LAST_NID_SHIFT 0
 +#endif
 +
 +#if SECTIONS_WIDTH+ZONES_WIDTH+NODES_SHIFT+LAST_NID_SHIFT <= BITS_PER_LONG - NR_PAGEFLAGS
 +#define LAST_NID_WIDTH LAST_NID_SHIFT
 +#else
 +#define LAST_NID_WIDTH 0
++=======
+ #define LAST__PID_SHIFT 8
+ #define LAST__PID_MASK  ((1 << LAST__PID_SHIFT)-1)
+ 
+ #define LAST__CPU_SHIFT NR_CPUS_BITS
+ #define LAST__CPU_MASK  ((1 << LAST__CPU_SHIFT)-1)
+ 
+ #define LAST_CPUPID_SHIFT (LAST__PID_SHIFT+LAST__CPU_SHIFT)
+ #else
+ #define LAST_CPUPID_SHIFT 0
+ #endif
+ 
+ #if SECTIONS_WIDTH+ZONES_WIDTH+NODES_SHIFT+LAST_CPUPID_SHIFT <= BITS_PER_LONG - NR_PAGEFLAGS
+ #define LAST_CPUPID_WIDTH LAST_CPUPID_SHIFT
+ #else
+ #define LAST_CPUPID_WIDTH 0
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  #endif
  
  /*
@@@ -81,8 -87,8 +106,13 @@@
  #define NODE_NOT_IN_PAGE_FLAGS
  #endif
  
++<<<<<<< HEAD
 +#if defined(CONFIG_NUMA_BALANCING) && LAST_NID_WIDTH == 0
 +#define LAST_NID_NOT_IN_PAGE_FLAGS
++=======
+ #if defined(CONFIG_NUMA_BALANCING) && LAST_CPUPID_WIDTH == 0
+ #define LAST_CPUPID_NOT_IN_PAGE_FLAGS
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  #endif
  
  #endif /* _LINUX_PAGE_FLAGS_LAYOUT */
diff --cc kernel/sched/fair.c
index 98cee68da024,dbe0f628efa3..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -876,16 -1210,30 +876,36 @@@ static void task_numa_placement(struct 
  /*
   * Got a PROT_NONE fault for a page on @node.
   */
++<<<<<<< HEAD
 +void task_numa_fault(int node, int pages, bool migrated)
++=======
+ void task_numa_fault(int last_cpupid, int node, int pages, bool migrated)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  {
  	struct task_struct *p = current;
 -	int priv;
  
  	if (!numabalancing_enabled)
  		return;
  
++<<<<<<< HEAD
++=======
+ 	/* for example, ksmd faulting in a user's mm */
+ 	if (!p->mm)
+ 		return;
+ 
+ 	/*
+ 	 * First accesses are treated as private, otherwise consider accesses
+ 	 * to be private if the accessing pid has not changed
+ 	 */
+ 	if (!cpupid_pid_unset(last_cpupid))
+ 		priv = ((p->pid & LAST__PID_MASK) == cpupid_to_pid(last_cpupid));
+ 	else
+ 		priv = 1;
+ 
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	/* Allocate buffer to track faults on a per-node basis */
  	if (unlikely(!p->numa_faults)) {
 -		int size = sizeof(*p->numa_faults) * 2 * nr_node_ids;
 +		int size = sizeof(*p->numa_faults) * nr_node_ids;
  
  		/* numa_faults and numa_faults_buffer share the allocation */
  		p->numa_faults = kzalloc(size * 2, GFP_KERNEL|__GFP_NOWARN);
diff --cc mm/huge_memory.c
index ba9f65152a91,becf92ca54f3..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1294,7 -1282,7 +1294,11 @@@ int do_huge_pmd_numa_page(struct mm_str
  	struct page *page;
  	unsigned long haddr = addr & HPAGE_PMD_MASK;
  	int page_nid = -1, this_nid = numa_node_id();
++<<<<<<< HEAD
 +	int target_nid;
++=======
+ 	int target_nid, last_cpupid = -1;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	bool page_locked;
  	bool migrated = false;
  
@@@ -1305,6 -1293,7 +1309,10 @@@
  	page = pmd_page(pmd);
  	BUG_ON(is_huge_zero_page(page));
  	page_nid = page_to_nid(page);
++<<<<<<< HEAD
++=======
+ 	last_cpupid = page_cpupid_last(page);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	count_vm_numa_event(NUMA_HINT_FAULTS);
  	if (page_nid == this_nid)
  		count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
@@@ -1373,7 -1362,7 +1381,11 @@@ out
  		page_unlock_anon_vma_read(anon_vma);
  
  	if (page_nid != -1)
++<<<<<<< HEAD
 +		task_numa_fault(page_nid, HPAGE_PMD_NR, migrated);
++=======
+ 		task_numa_fault(last_cpupid, page_nid, HPAGE_PMD_NR, migrated);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  	return 0;
  }
@@@ -1691,7 -1682,7 +1703,11 @@@ static void __split_huge_page_refcount(
  		page_tail->mapping = page->mapping;
  
  		page_tail->index = page->index + i;
++<<<<<<< HEAD
 +		page_nid_xchg_last(page_tail, page_nid_last(page));
++=======
+ 		page_cpupid_xchg_last(page_tail, page_cpupid_last(page));
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  		BUG_ON(!PageAnon(page_tail));
  		BUG_ON(!PageUptodate(page_tail));
diff --cc mm/memory.c
index 6285e7df615f,5162e6d0d652..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -69,8 -69,8 +69,13 @@@
  
  #include "internal.h"
  
++<<<<<<< HEAD
 +#ifdef LAST_NID_NOT_IN_PAGE_FLAGS
 +#warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_nid.
++=======
+ #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
+ #warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  #endif
  
  #ifndef CONFIG_NEED_MULTIPLE_NODES
@@@ -3533,6 -3536,7 +3538,10 @@@ int do_numa_page(struct mm_struct *mm, 
  	struct page *page = NULL;
  	spinlock_t *ptl;
  	int page_nid = -1;
++<<<<<<< HEAD
++=======
+ 	int last_cpupid;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	int target_nid;
  	bool migrated = false;
  
@@@ -3563,6 -3567,7 +3572,10 @@@
  	}
  	BUG_ON(is_zero_pfn(page_to_pfn(page)));
  
++<<<<<<< HEAD
++=======
+ 	last_cpupid = page_cpupid_last(page);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	page_nid = page_to_nid(page);
  	target_nid = numa_migrate_prep(page, vma, addr, page_nid);
  	pte_unmap_unlock(ptep, ptl);
@@@ -3578,7 -3583,7 +3591,11 @@@
  
  out:
  	if (page_nid != -1)
++<<<<<<< HEAD
 +		task_numa_fault(page_nid, 1, migrated);
++=======
+ 		task_numa_fault(last_cpupid, page_nid, 1, migrated);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	return 0;
  }
  
@@@ -3593,6 -3598,7 +3610,10 @@@ static int do_pmd_numa_page(struct mm_s
  	unsigned long offset;
  	spinlock_t *ptl;
  	bool numa = false;
++<<<<<<< HEAD
++=======
+ 	int last_cpupid;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  	spin_lock(&mm->page_table_lock);
  	pmd = *pmdp;
@@@ -3637,6 -3643,7 +3658,10 @@@
  		if (unlikely(!page))
  			continue;
  
++<<<<<<< HEAD
++=======
+ 		last_cpupid = page_cpupid_last(page);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  		page_nid = page_to_nid(page);
  		target_nid = numa_migrate_prep(page, vma, addr, page_nid);
  		pte_unmap_unlock(pte, ptl);
@@@ -3649,7 -3656,7 +3674,11 @@@
  		}
  
  		if (page_nid != -1)
++<<<<<<< HEAD
 +			task_numa_fault(page_nid, 1, migrated);
++=======
+ 			task_numa_fault(last_cpupid, page_nid, 1, migrated);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  		pte = pte_offset_map_lock(mm, pmdp, addr, &ptl);
  	}
diff --cc mm/mempolicy.c
index f05aa4c2c73c,a5867ef24bda..000000000000
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@@ -2312,9 -2374,11 +2314,17 @@@ int mpol_misplaced(struct page *page, s
  
  	/* Migrate the page towards the node whose CPU is referencing it */
  	if (pol->flags & MPOL_F_MORON) {
++<<<<<<< HEAD
 +		int last_nid;
 +
 +		polnid = numa_node_id();
++=======
+ 		int last_cpupid;
+ 		int this_cpupid;
+ 
+ 		polnid = thisnid;
+ 		this_cpupid = cpu_pid_to_cpupid(thiscpu, current->pid);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  		/*
  		 * Multi-stage node selection is used in conjunction
@@@ -2337,9 -2401,21 +2347,27 @@@
  		 * it less likely we act on an unlikely task<->page
  		 * relation.
  		 */
++<<<<<<< HEAD
 +		last_nid = page_nid_xchg_last(page, polnid);
 +		if (last_nid != polnid)
 +			goto out;
++=======
+ 		last_cpupid = page_cpupid_xchg_last(page, this_cpupid);
+ 		if (!cpupid_pid_unset(last_cpupid) && cpupid_to_nid(last_cpupid) != thisnid)
+ 			goto out;
+ 
+ #ifdef CONFIG_NUMA_BALANCING
+ 		/*
+ 		 * If the scheduler has just moved us away from our
+ 		 * preferred node, do not bother migrating pages yet.
+ 		 * This way a short and temporary process migration will
+ 		 * not cause excessive memory migration.
+ 		 */
+ 		if (thisnid != current->numa_preferred_nid &&
+ 				!current->numa_migrate_seq)
+ 			goto out;
+ #endif
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	}
  
  	if (curnid != polnid)
diff --cc mm/migrate.c
index 1698b7393558,ff537749d3b4..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -1495,7 -1498,7 +1495,11 @@@ static struct page *alloc_misplaced_dst
  					  __GFP_NOWARN) &
  					 ~GFP_IOFS, 0);
  	if (newpage)
++<<<<<<< HEAD
 +		page_nid_xchg_last(newpage, page_nid_last(page));
++=======
+ 		page_cpupid_xchg_last(newpage, page_cpupid_last(page));
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  	return newpage;
  }
@@@ -1672,7 -1675,7 +1676,11 @@@ int migrate_misplaced_transhuge_page(st
  	if (!new_page)
  		goto out_fail;
  
++<<<<<<< HEAD
 +	page_nid_xchg_last(new_page, page_nid_last(page));
++=======
+ 	page_cpupid_xchg_last(new_page, page_cpupid_last(page));
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  	isolated = numamigrate_isolate_page(pgdat, page);
  	if (!isolated) {
diff --cc mm/mm_init.c
index c280a02ea11e,68562e92d50c..000000000000
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@@ -69,26 -71,26 +69,49 @@@ void __init mminit_verify_pageflags_lay
  	unsigned long or_mask, add_mask;
  
  	shift = 8 * sizeof(unsigned long);
++<<<<<<< HEAD
 +	width = shift - SECTIONS_WIDTH - NODES_WIDTH - ZONES_WIDTH - LAST_NID_SHIFT;
 +	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_widths",
 +		"Section %d Node %d Zone %d Lastnid %d Flags %d\n",
 +		SECTIONS_WIDTH,
 +		NODES_WIDTH,
 +		ZONES_WIDTH,
 +		LAST_NID_WIDTH,
 +		NR_PAGEFLAGS);
 +	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_shifts",
 +		"Section %d Node %d Zone %d Lastnid %d\n",
 +		SECTIONS_SHIFT,
 +		NODES_SHIFT,
 +		ZONES_SHIFT,
 +		LAST_NID_SHIFT);
 +	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_pgshifts",
 +		"Section %lu Node %lu Zone %lu Lastnid %lu\n",
 +		(unsigned long)SECTIONS_PGSHIFT,
 +		(unsigned long)NODES_PGSHIFT,
 +		(unsigned long)ZONES_PGSHIFT,
 +		(unsigned long)LAST_NID_PGSHIFT);
++=======
+ 	width = shift - SECTIONS_WIDTH - NODES_WIDTH - ZONES_WIDTH - LAST_CPUPID_SHIFT;
+ 	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_widths",
+ 		"Section %d Node %d Zone %d Lastcpupid %d Flags %d\n",
+ 		SECTIONS_WIDTH,
+ 		NODES_WIDTH,
+ 		ZONES_WIDTH,
+ 		LAST_CPUPID_WIDTH,
+ 		NR_PAGEFLAGS);
+ 	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_shifts",
+ 		"Section %d Node %d Zone %d Lastcpupid %d\n",
+ 		SECTIONS_SHIFT,
+ 		NODES_SHIFT,
+ 		ZONES_SHIFT,
+ 		LAST_CPUPID_SHIFT);
+ 	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_pgshifts",
+ 		"Section %lu Node %lu Zone %lu Lastcpupid %lu\n",
+ 		(unsigned long)SECTIONS_PGSHIFT,
+ 		(unsigned long)NODES_PGSHIFT,
+ 		(unsigned long)ZONES_PGSHIFT,
+ 		(unsigned long)LAST_CPUPID_PGSHIFT);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_nodezoneid",
  		"Node/Zone ID: %lu -> %lu\n",
  		(unsigned long)(ZONEID_PGOFF + ZONEID_SHIFT),
@@@ -100,9 -102,9 +123,15 @@@
  	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_nodeflags",
  		"Node not in page flags");
  #endif
++<<<<<<< HEAD
 +#ifdef LAST_NID_NOT_IN_PAGE_FLAGS
 +	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_nodeflags",
 +		"Last nid not in page flags");
++=======
+ #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
+ 	mminit_dprintk(MMINIT_TRACE, "pageflags_layout_nodeflags",
+ 		"Last cpupid not in page flags");
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  #endif
  
  	if (SECTIONS_WIDTH) {
diff --cc mm/mmzone.c
index 2ac0afbd68f3,bf34fb8556db..000000000000
--- a/mm/mmzone.c
+++ b/mm/mmzone.c
@@@ -97,20 -97,20 +97,38 @@@ void lruvec_init(struct lruvec *lruvec
  		INIT_LIST_HEAD(&lruvec->lists[lru]);
  }
  
++<<<<<<< HEAD
 +#if defined(CONFIG_NUMA_BALANCING) && !defined(LAST_NID_NOT_IN_PAGE_FLAGS)
 +int page_nid_xchg_last(struct page *page, int nid)
 +{
 +	unsigned long old_flags, flags;
 +	int last_nid;
 +
 +	do {
 +		old_flags = flags = page->flags;
 +		last_nid = page_nid_last(page);
 +
 +		flags &= ~(LAST_NID_MASK << LAST_NID_PGSHIFT);
 +		flags |= (nid & LAST_NID_MASK) << LAST_NID_PGSHIFT;
 +	} while (unlikely(cmpxchg(&page->flags, old_flags, flags) != old_flags));
 +
 +	return last_nid;
++=======
+ #if defined(CONFIG_NUMA_BALANCING) && !defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS)
+ int page_cpupid_xchg_last(struct page *page, int cpupid)
+ {
+ 	unsigned long old_flags, flags;
+ 	int last_cpupid;
+ 
+ 	do {
+ 		old_flags = flags = page->flags;
+ 		last_cpupid = page_cpupid_last(page);
+ 
+ 		flags &= ~(LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT);
+ 		flags |= (cpupid & LAST_CPUPID_MASK) << LAST_CPUPID_PGSHIFT;
+ 	} while (unlikely(cmpxchg(&page->flags, old_flags, flags) != old_flags));
+ 
+ 	return last_cpupid;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  }
  #endif
diff --cc mm/mprotect.c
index 41e02923fcd9,9a74855f1241..000000000000
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@@ -37,14 -37,15 +37,24 @@@ static inline pgprot_t pgprot_modify(pg
  
  static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
  		unsigned long addr, unsigned long end, pgprot_t newprot,
++<<<<<<< HEAD
 +		int dirty_accountable, int prot_numa, bool *ret_all_same_node)
++=======
+ 		int dirty_accountable, int prot_numa, bool *ret_all_same_cpupid)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  {
  	struct mm_struct *mm = vma->vm_mm;
  	pte_t *pte, oldpte;
  	spinlock_t *ptl;
  	unsigned long pages = 0;
++<<<<<<< HEAD
 +	bool all_same_node = true;
 +	int last_nid = -1;
++=======
+ 	bool all_same_cpupid = true;
+ 	int last_cpu = -1;
+ 	int last_pid = -1;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  	pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
  	arch_enter_lazy_mmu_mode();
@@@ -63,11 -64,18 +73,26 @@@
  
  				page = vm_normal_page(vma, addr, oldpte);
  				if (page) {
++<<<<<<< HEAD
 +					int this_nid = page_to_nid(page);
 +					if (last_nid == -1)
 +						last_nid = this_nid;
 +					if (last_nid != this_nid)
 +						all_same_node = false;
++=======
+ 					int cpupid = page_cpupid_last(page);
+ 					int this_cpu = cpupid_to_cpu(cpupid);
+ 					int this_pid = cpupid_to_pid(cpupid);
+ 
+ 					if (last_cpu == -1)
+ 						last_cpu = this_cpu;
+ 					if (last_pid == -1)
+ 						last_pid = this_pid;
+ 					if (last_cpu != this_cpu ||
+ 					    last_pid != this_pid) {
+ 						all_same_cpupid = false;
+ 					}
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  					if (!pte_numa(oldpte)) {
  						ptent = pte_mknuma(ptent);
@@@ -107,7 -115,7 +132,11 @@@
  	arch_leave_lazy_mmu_mode();
  	pte_unmap_unlock(pte - 1, ptl);
  
++<<<<<<< HEAD
 +	*ret_all_same_node = all_same_node;
++=======
+ 	*ret_all_same_cpupid = all_same_cpupid;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	return pages;
  }
  
@@@ -134,7 -142,7 +163,11 @@@ static inline unsigned long change_pmd_
  	pmd_t *pmd;
  	unsigned long next;
  	unsigned long pages = 0;
++<<<<<<< HEAD
 +	bool all_same_node;
++=======
+ 	bool all_same_cpupid;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  	pmd = pmd_offset(pud, addr);
  	do {
@@@ -157,8 -167,9 +190,14 @@@
  		}
  		if (pmd_none_or_clear_bad(pmd))
  			continue;
++<<<<<<< HEAD
 +		pages += change_pte_range(vma, pmd, addr, next, newprot,
 +				 dirty_accountable, prot_numa, &all_same_node);
++=======
+ 		this_pages = change_pte_range(vma, pmd, addr, next, newprot,
+ 				 dirty_accountable, prot_numa, &all_same_cpupid);
+ 		pages += this_pages;
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  
  		/*
  		 * If we are changing protections for NUMA hinting faults then
@@@ -166,7 -177,7 +205,11 @@@
  		 * node. This allows a regular PMD to be handled as one fault
  		 * and effectively batches the taking of the PTL
  		 */
++<<<<<<< HEAD
 +		if (prot_numa && all_same_node)
++=======
+ 		if (prot_numa && this_pages && all_same_cpupid)
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  			change_pmd_protnuma(vma->vm_mm, addr, pmd);
  	} while (pmd++, addr = next, addr != end);
  
diff --cc mm/page_alloc.c
index c3edb624fccf,73d812f16dde..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -614,7 -626,7 +614,11 @@@ static inline int free_pages_check(stru
  		bad_page(page);
  		return 1;
  	}
++<<<<<<< HEAD
 +	page_nid_reset_last(page);
++=======
+ 	page_cpupid_reset_last(page);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  	if (page->flags & PAGE_FLAGS_CHECK_AT_PREP)
  		page->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
  	return 0;
@@@ -3938,7 -4015,7 +3942,11 @@@ void __meminit memmap_init_zone(unsigne
  		mminit_verify_page_links(page, zone, nid, pfn);
  		init_page_count(page);
  		page_mapcount_reset(page);
++<<<<<<< HEAD
 +		page_nid_reset_last(page);
++=======
+ 		page_cpupid_reset_last(page);
++>>>>>>> 90572890d202 (mm: numa: Change page last {nid,pid} into {cpu,pid})
  		SetPageReserved(page);
  		/*
  		 * Mark the block movable so that blocks are reserved for
* Unmerged path include/linux/mm.h
* Unmerged path include/linux/mm_types.h
* Unmerged path include/linux/page-flags-layout.h
diff --git a/kernel/bounds.c b/kernel/bounds.c
index 0c9b862292b2..e8ca97b5c386 100644
--- a/kernel/bounds.c
+++ b/kernel/bounds.c
@@ -10,6 +10,7 @@
 #include <linux/mmzone.h>
 #include <linux/kbuild.h>
 #include <linux/page_cgroup.h>
+#include <linux/log2.h>
 
 void foo(void)
 {
@@ -17,5 +18,8 @@ void foo(void)
 	DEFINE(NR_PAGEFLAGS, __NR_PAGEFLAGS);
 	DEFINE(MAX_NR_ZONES, __MAX_NR_ZONES);
 	DEFINE(NR_PCG_FLAGS, __NR_PCG_FLAGS);
+#ifdef CONFIG_SMP
+	DEFINE(NR_CPUS_BITS, ilog2(CONFIG_NR_CPUS));
+#endif
 	/* End of constants */
 }
* Unmerged path kernel/sched/fair.c
* Unmerged path mm/huge_memory.c
* Unmerged path mm/memory.c
* Unmerged path mm/mempolicy.c
* Unmerged path mm/migrate.c
* Unmerged path mm/mm_init.c
* Unmerged path mm/mmzone.c
* Unmerged path mm/mprotect.c
* Unmerged path mm/page_alloc.c
