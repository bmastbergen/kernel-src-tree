sched: Fix endless sync_sched/rcu() loop inside _cpu_down()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Michael wang <wangyun@linux.vnet.ibm.com>
commit 106dd5afde3cd10db7e1370b6ddc77f0b2496a75
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/106dd5af.failed

Commit 6acce3ef8:

	sched: Remove get_online_cpus() usage

tries to do sync_sched/rcu() inside _cpu_down() but triggers:

	INFO: task swapper/0:1 blocked for more than 120 seconds.
	...
	[<ffffffff811263dc>] synchronize_rcu+0x2c/0x30
	[<ffffffff81d1bd82>] _cpu_down+0x2b2/0x340
	...

It was caused by that in the rcu boost case we rely on smpboot thread to
finish the rcu callback, which has already been parked before sync in here
and leads to the endless sync_sched/rcu().

This patch exchanges the sequence of smpboot_park_threads() and
sync_sched/rcu() to fix the bug.

	Reported-by: Fengguang Wu <fengguang.wu@intel.com>
	Tested-by: Fengguang Wu <fengguang.wu@intel.com>
	Signed-off-by: Michael Wang <wangyun@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/5282EDC0.6060003@linux.vnet.ibm.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 106dd5afde3cd10db7e1370b6ddc77f0b2496a75)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cpu.c
diff --cc kernel/cpu.c
index 198a38883e64,2227b58734a7..000000000000
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@@ -309,8 -306,28 +309,31 @@@ static int __ref _cpu_down(unsigned in
  				__func__, cpu);
  		goto out_release;
  	}
+ 
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * By now we've cleared cpu_active_mask, wait for all preempt-disabled
+ 	 * and RCU users of this state to go away such that all new such users
+ 	 * will observe it.
+ 	 *
+ 	 * For CONFIG_PREEMPT we have preemptible RCU and its sync_rcu() might
+ 	 * not imply sync_sched(), so explicitly call both.
+ 	 *
+ 	 * Do sync before park smpboot threads to take care the rcu boost case.
+ 	 */
+ #ifdef CONFIG_PREEMPT
+ 	synchronize_sched();
+ #endif
+ 	synchronize_rcu();
+ 
  	smpboot_park_threads(cpu);
  
+ 	/*
+ 	 * So now all preempt/rcu users must observe !cpu_active().
+ 	 */
+ 
++>>>>>>> 106dd5afde3c (sched: Fix endless sync_sched/rcu() loop inside _cpu_down())
  	err = __stop_machine(take_cpu_down, &tcd_param, cpumask_of(cpu));
  	if (err) {
  		/* CPU didn't die: tell everyone.  Can't complain. */
* Unmerged path kernel/cpu.c
