sched/numa: Normalize faults_cpu stats and weigh by CPU use

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Rik van Riel <riel@redhat.com>
commit 7e2703e6099609adc93679c4d45cd6247f565971
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/7e2703e6.failed

Tracing the code that decides the active nodes has made it abundantly clear
that the naive implementation of the faults_from code has issues.

Specifically, the garbage collector in some workloads will access orders
of magnitudes more memory than the threads that do all the active work.
This resulted in the node with the garbage collector being marked the only
active node in the group.

This issue is avoided if we weigh the statistics by CPU use of each task in
the numa group, instead of by how many faults each thread has occurred.

To achieve this, we normalize the number of faults to the fraction of faults
that occurred on each node, and then multiply that fraction by the fraction
of CPU time the task has used since the last time task_numa_placement was
invoked.

This way the nodes in the active node mask will be the ones where the tasks
from the numa group are most actively running, and the influence of eg. the
garbage collector and other do-little threads is properly minimized.

On a 4 node system, using CPU use statistics calculated over a longer interval
results in about 1% fewer page migrations with two 32-warehouse specjbb runs
on a 4 node system, and about 5% fewer page migrations, as well as 1% better
throughput, with two 8-warehouse specjbb runs, as compared with the shorter
term statistics kept by the scheduler.

	Signed-off-by: Rik van Riel <riel@redhat.com>
	Acked-by: Mel Gorman <mgorman@suse.de>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Chegu Vinod <chegu_vinod@hp.com>
Link: http://lkml.kernel.org/r/1390860228-21539-7-git-send-email-riel@redhat.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 7e2703e6099609adc93679c4d45cd6247f565971)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/fair.c
diff --cc kernel/sched/core.c
index 79ad0cf7cc11,a561c9e8e382..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -1651,11 -1742,15 +1651,21 @@@ static void __sched_fork(unsigned long 
  
  	p->node_stamp = 0ULL;
  	p->numa_scan_seq = p->mm ? p->mm->numa_scan_seq : 0;
 +	p->numa_migrate_seq = p->mm ? p->mm->numa_scan_seq - 1 : 0;
  	p->numa_scan_period = sysctl_numa_balancing_scan_delay;
  	p->numa_work.next = &p->numa_work;
++<<<<<<< HEAD
 +	p->numa_faults = NULL;
 +	p->numa_faults_buffer = NULL;
++=======
+ 	p->numa_faults_memory = NULL;
+ 	p->numa_faults_buffer_memory = NULL;
+ 	p->last_task_numa_placement = 0;
+ 	p->last_sum_exec_runtime = 0;
+ 
+ 	INIT_LIST_HEAD(&p->numa_entry);
+ 	p->numa_group = NULL;
++>>>>>>> 7e2703e60996 (sched/numa: Normalize faults_cpu stats and weigh by CPU use)
  #endif /* CONFIG_NUMA_BALANCING */
  }
  
diff --cc kernel/sched/fair.c
index c33e42bc435b,8fc3a8234817..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,29 -864,691 +839,710 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
 +static void task_numa_placement(struct task_struct *p)
 +{
 +	int seq, nid, max_nid = -1;
 +	unsigned long max_faults = 0;
++=======
+ static void account_numa_enqueue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running += (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ static void account_numa_dequeue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running -= (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ struct numa_group {
+ 	atomic_t refcount;
+ 
+ 	spinlock_t lock; /* nr_tasks, tasks */
+ 	int nr_tasks;
+ 	pid_t gid;
+ 	struct list_head task_list;
+ 
+ 	struct rcu_head rcu;
+ 	nodemask_t active_nodes;
+ 	unsigned long total_faults;
+ 	/*
+ 	 * Faults_cpu is used to decide whether memory should move
+ 	 * towards the CPU. As a consequence, these stats are weighted
+ 	 * more by CPU use than by memory faults.
+ 	 */
+ 	unsigned long *faults_cpu;
+ 	unsigned long faults[0];
+ };
+ 
+ pid_t task_numa_group_id(struct task_struct *p)
+ {
+ 	return p->numa_group ? p->numa_group->gid : 0;
+ }
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults_memory)
+ 		return 0;
+ 
+ 	return p->numa_faults_memory[task_faults_idx(nid, 0)] +
+ 		p->numa_faults_memory[task_faults_idx(nid, 1)];
+ }
+ 
+ static inline unsigned long group_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	return p->numa_group->faults[task_faults_idx(nid, 0)] +
+ 		p->numa_group->faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static inline unsigned long group_faults_cpu(struct numa_group *group, int nid)
+ {
+ 	return group->faults_cpu[task_faults_idx(nid, 0)] +
+ 		group->faults_cpu[task_faults_idx(nid, 1)];
+ }
+ 
+ /*
+  * These return the fraction of accesses done by a particular task, or
+  * task group, on a particular numa node.  The group weight is given a
+  * larger multiplier, in order to group tasks together that are almost
+  * evenly spread out between numa nodes.
+  */
+ static inline unsigned long task_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_faults_memory)
+ 		return 0;
+ 
+ 	total_faults = p->total_numa_faults;
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * task_faults(p, nid) / total_faults;
+ }
+ 
+ static inline unsigned long group_weight(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group || !p->numa_group->total_faults)
+ 		return 0;
+ 
+ 	return 1000 * group_faults(p, nid) / p->numa_group->total_faults;
+ }
+ 
+ bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
+ 				int src_nid, int dst_cpu)
+ {
+ 	struct numa_group *ng = p->numa_group;
+ 	int dst_nid = cpu_to_node(dst_cpu);
+ 	int last_cpupid, this_cpupid;
+ 
+ 	this_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);
+ 
+ 	/*
+ 	 * Multi-stage node selection is used in conjunction with a periodic
+ 	 * migration fault to build a temporal task<->page relation. By using
+ 	 * a two-stage filter we remove short/unlikely relations.
+ 	 *
+ 	 * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate
+ 	 * a task's usage of a particular page (n_p) per total usage of this
+ 	 * page (n_t) (in a given time-span) to a probability.
+ 	 *
+ 	 * Our periodic faults will sample this probability and getting the
+ 	 * same result twice in a row, given these samples are fully
+ 	 * independent, is then given by P(n)^2, provided our sample period
+ 	 * is sufficiently short compared to the usage pattern.
+ 	 *
+ 	 * This quadric squishes small probabilities, making it less likely we
+ 	 * act on an unlikely task<->page relation.
+ 	 */
+ 	last_cpupid = page_cpupid_xchg_last(page, this_cpupid);
+ 	if (!cpupid_pid_unset(last_cpupid) &&
+ 				cpupid_to_nid(last_cpupid) != dst_nid)
+ 		return false;
+ 
+ 	/* Always allow migrate on private faults */
+ 	if (cpupid_match_pid(p, last_cpupid))
+ 		return true;
+ 
+ 	/* A shared fault, but p->numa_group has not been set up yet. */
+ 	if (!ng)
+ 		return true;
+ 
+ 	/*
+ 	 * Do not migrate if the destination is not a node that
+ 	 * is actively used by this numa group.
+ 	 */
+ 	if (!node_isset(dst_nid, ng->active_nodes))
+ 		return false;
+ 
+ 	/*
+ 	 * Source is a node that is not actively used by this
+ 	 * numa group, while the destination is. Migrate.
+ 	 */
+ 	if (!node_isset(src_nid, ng->active_nodes))
+ 		return true;
+ 
+ 	/*
+ 	 * Both source and destination are nodes in active
+ 	 * use by this numa group. Maximize memory bandwidth
+ 	 * by migrating from more heavily used groups, to less
+ 	 * heavily used ones, spreading the load around.
+ 	 * Use a 1/4 hysteresis to avoid spurious page movement.
+ 	 */
+ 	return group_faults(p, dst_nid) < (group_faults(p, src_nid) * 3 / 4);
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ /* Cached statistics for all CPUs within a node */
+ struct numa_stats {
+ 	unsigned long nr_running;
+ 	unsigned long load;
+ 
+ 	/* Total compute capacity of CPUs on a node */
+ 	unsigned long power;
+ 
+ 	/* Approximate capacity in terms of runnable tasks on a node */
+ 	unsigned long capacity;
+ 	int has_capacity;
+ };
+ 
+ /*
+  * XXX borrowed from update_sg_lb_stats
+  */
+ static void update_numa_stats(struct numa_stats *ns, int nid)
+ {
+ 	int cpu, cpus = 0;
+ 
+ 	memset(ns, 0, sizeof(*ns));
+ 	for_each_cpu(cpu, cpumask_of_node(nid)) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 
+ 		ns->nr_running += rq->nr_running;
+ 		ns->load += weighted_cpuload(cpu);
+ 		ns->power += power_of(cpu);
+ 
+ 		cpus++;
+ 	}
+ 
+ 	/*
+ 	 * If we raced with hotplug and there are no CPUs left in our mask
+ 	 * the @ns structure is NULL'ed and task_numa_compare() will
+ 	 * not find this node attractive.
+ 	 *
+ 	 * We'll either bail at !has_capacity, or we'll detect a huge imbalance
+ 	 * and bail there.
+ 	 */
+ 	if (!cpus)
+ 		return;
+ 
+ 	ns->load = (ns->load * SCHED_POWER_SCALE) / ns->power;
+ 	ns->capacity = DIV_ROUND_CLOSEST(ns->power, SCHED_POWER_SCALE);
+ 	ns->has_capacity = (ns->nr_running < ns->capacity);
+ }
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	int imbalance_pct;
+ 
+ 	struct task_struct *best_task;
+ 	long best_imp;
+ 	int best_cpu;
+ };
+ 
+ static void task_numa_assign(struct task_numa_env *env,
+ 			     struct task_struct *p, long imp)
+ {
+ 	if (env->best_task)
+ 		put_task_struct(env->best_task);
+ 	if (p)
+ 		get_task_struct(p);
+ 
+ 	env->best_task = p;
+ 	env->best_imp = imp;
+ 	env->best_cpu = env->dst_cpu;
+ }
+ 
+ /*
+  * This checks if the overall compute and NUMA accesses of the system would
+  * be improved if the source tasks was migrated to the target dst_cpu taking
+  * into account that it might be best if task running on the dst_cpu should
+  * be exchanged with the source task
+  */
+ static void task_numa_compare(struct task_numa_env *env,
+ 			      long taskimp, long groupimp)
+ {
+ 	struct rq *src_rq = cpu_rq(env->src_cpu);
+ 	struct rq *dst_rq = cpu_rq(env->dst_cpu);
+ 	struct task_struct *cur;
+ 	long dst_load, src_load;
+ 	long load;
+ 	long imp = (groupimp > 0) ? groupimp : taskimp;
+ 
+ 	rcu_read_lock();
+ 	cur = ACCESS_ONCE(dst_rq->curr);
+ 	if (cur->pid == 0) /* idle */
+ 		cur = NULL;
+ 
+ 	/*
+ 	 * "imp" is the fault differential for the source task between the
+ 	 * source and destination node. Calculate the total differential for
+ 	 * the source task and potential destination task. The more negative
+ 	 * the value is, the more rmeote accesses that would be expected to
+ 	 * be incurred if the tasks were swapped.
+ 	 */
+ 	if (cur) {
+ 		/* Skip this swap candidate if cannot move to the source cpu */
+ 		if (!cpumask_test_cpu(env->src_cpu, tsk_cpus_allowed(cur)))
+ 			goto unlock;
+ 
+ 		/*
+ 		 * If dst and source tasks are in the same NUMA group, or not
+ 		 * in any group then look only at task weights.
+ 		 */
+ 		if (cur->numa_group == env->p->numa_group) {
+ 			imp = taskimp + task_weight(cur, env->src_nid) -
+ 			      task_weight(cur, env->dst_nid);
+ 			/*
+ 			 * Add some hysteresis to prevent swapping the
+ 			 * tasks within a group over tiny differences.
+ 			 */
+ 			if (cur->numa_group)
+ 				imp -= imp/16;
+ 		} else {
+ 			/*
+ 			 * Compare the group weights. If a task is all by
+ 			 * itself (not part of a group), use the task weight
+ 			 * instead.
+ 			 */
+ 			if (env->p->numa_group)
+ 				imp = groupimp;
+ 			else
+ 				imp = taskimp;
+ 
+ 			if (cur->numa_group)
+ 				imp += group_weight(cur, env->src_nid) -
+ 				       group_weight(cur, env->dst_nid);
+ 			else
+ 				imp += task_weight(cur, env->src_nid) -
+ 				       task_weight(cur, env->dst_nid);
+ 		}
+ 	}
+ 
+ 	if (imp < env->best_imp)
+ 		goto unlock;
+ 
+ 	if (!cur) {
+ 		/* Is there capacity at our destination? */
+ 		if (env->src_stats.has_capacity &&
+ 		    !env->dst_stats.has_capacity)
+ 			goto unlock;
+ 
+ 		goto balance;
+ 	}
+ 
+ 	/* Balance doesn't matter much if we're running a task per cpu */
+ 	if (src_rq->nr_running == 1 && dst_rq->nr_running == 1)
+ 		goto assign;
+ 
+ 	/*
+ 	 * In the overloaded case, try and keep the load balanced.
+ 	 */
+ balance:
+ 	dst_load = env->dst_stats.load;
+ 	src_load = env->src_stats.load;
+ 
+ 	/* XXX missing power terms */
+ 	load = task_h_load(env->p);
+ 	dst_load += load;
+ 	src_load -= load;
+ 
+ 	if (cur) {
+ 		load = task_h_load(cur);
+ 		dst_load -= load;
+ 		src_load += load;
+ 	}
+ 
+ 	/* make src_load the smaller */
+ 	if (dst_load < src_load)
+ 		swap(dst_load, src_load);
+ 
+ 	if (src_load * env->imbalance_pct < dst_load * 100)
+ 		goto unlock;
+ 
+ assign:
+ 	task_numa_assign(env, cur, imp);
+ unlock:
+ 	rcu_read_unlock();
+ }
+ 
+ static void task_numa_find_cpu(struct task_numa_env *env,
+ 				long taskimp, long groupimp)
+ {
+ 	int cpu;
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
+ 		/* Skip this CPU if the source task cannot migrate */
+ 		if (!cpumask_test_cpu(cpu, tsk_cpus_allowed(env->p)))
+ 			continue;
+ 
+ 		env->dst_cpu = cpu;
+ 		task_numa_compare(env, taskimp, groupimp);
+ 	}
+ }
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = task_node(p),
+ 
+ 		.imbalance_pct = 112,
+ 
+ 		.best_task = NULL,
+ 		.best_imp = 0,
+ 		.best_cpu = -1
+ 	};
+ 	struct sched_domain *sd;
+ 	unsigned long taskweight, groupweight;
+ 	int nid, ret;
+ 	long taskimp, groupimp;
+ 
+ 	/*
+ 	 * Pick the lowest SD_NUMA domain, as that would have the smallest
+ 	 * imbalance and would be the first to start moving tasks about.
+ 	 *
+ 	 * And we want to avoid any moving of tasks about, as that would create
+ 	 * random movement of tasks -- counter the numa conditions we're trying
+ 	 * to satisfy here.
+ 	 */
+ 	rcu_read_lock();
+ 	sd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));
+ 	if (sd)
+ 		env.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;
+ 	rcu_read_unlock();
+ 
+ 	/*
+ 	 * Cpusets can break the scheduler domain tree into smaller
+ 	 * balance domains, some of which do not cross NUMA boundaries.
+ 	 * Tasks that are "trapped" in such domains cannot be migrated
+ 	 * elsewhere, so there is no point in (re)trying.
+ 	 */
+ 	if (unlikely(!sd)) {
+ 		p->numa_preferred_nid = task_node(p);
+ 		return -EINVAL;
+ 	}
+ 
+ 	taskweight = task_weight(p, env.src_nid);
+ 	groupweight = group_weight(p, env.src_nid);
+ 	update_numa_stats(&env.src_stats, env.src_nid);
+ 	env.dst_nid = p->numa_preferred_nid;
+ 	taskimp = task_weight(p, env.dst_nid) - taskweight;
+ 	groupimp = group_weight(p, env.dst_nid) - groupweight;
+ 	update_numa_stats(&env.dst_stats, env.dst_nid);
+ 
+ 	/* If the preferred nid has capacity, try to use it. */
+ 	if (env.dst_stats.has_capacity)
+ 		task_numa_find_cpu(&env, taskimp, groupimp);
+ 
+ 	/* No space available on the preferred nid. Look elsewhere. */
+ 	if (env.best_cpu == -1) {
+ 		for_each_online_node(nid) {
+ 			if (nid == env.src_nid || nid == p->numa_preferred_nid)
+ 				continue;
+ 
+ 			/* Only consider nodes where both task and groups benefit */
+ 			taskimp = task_weight(p, nid) - taskweight;
+ 			groupimp = group_weight(p, nid) - groupweight;
+ 			if (taskimp < 0 && groupimp < 0)
+ 				continue;
+ 
+ 			env.dst_nid = nid;
+ 			update_numa_stats(&env.dst_stats, env.dst_nid);
+ 			task_numa_find_cpu(&env, taskimp, groupimp);
+ 		}
+ 	}
+ 
+ 	/* No better CPU than the current one was found. */
+ 	if (env.best_cpu == -1)
+ 		return -EAGAIN;
+ 
+ 	sched_setnuma(p, env.dst_nid);
+ 
+ 	/*
+ 	 * Reset the scan period if the task is being rescheduled on an
+ 	 * alternative node to recheck if the tasks is now properly placed.
+ 	 */
+ 	p->numa_scan_period = task_scan_min(p);
+ 
+ 	if (env.best_task == NULL) {
+ 		int ret = migrate_task_to(p, env.best_cpu);
+ 		return ret;
+ 	}
+ 
+ 	ret = migrate_swap(p, env.best_task);
+ 	put_task_struct(env.best_task);
+ 	return ret;
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults_memory))
+ 		return;
+ 
+ 	/* Periodically retry migrating the task to the preferred node */
+ 	p->numa_migrate_retry = jiffies + HZ;
+ 
+ 	/* Success if task is already running on preferred CPU */
+ 	if (task_node(p) == p->numa_preferred_nid)
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	task_numa_migrate(p);
+ }
+ 
+ /*
+  * Find the nodes on which the workload is actively running. We do this by
+  * tracking the nodes from which NUMA hinting faults are triggered. This can
+  * be different from the set of nodes where the workload's memory is currently
+  * located.
+  *
+  * The bitmask is used to make smarter decisions on when to do NUMA page
+  * migrations, To prevent flip-flopping, and excessive page migrations, nodes
+  * are added when they cause over 6/16 of the maximum number of faults, but
+  * only removed when they drop below 3/16.
+  */
+ static void update_numa_active_node_mask(struct numa_group *numa_group)
+ {
+ 	unsigned long faults, max_faults = 0;
+ 	int nid;
+ 
+ 	for_each_online_node(nid) {
+ 		faults = group_faults_cpu(numa_group, nid);
+ 		if (faults > max_faults)
+ 			max_faults = faults;
+ 	}
+ 
+ 	for_each_online_node(nid) {
+ 		faults = group_faults_cpu(numa_group, nid);
+ 		if (!node_isset(nid, numa_group->active_nodes)) {
+ 			if (faults > max_faults * 6 / 16)
+ 				node_set(nid, numa_group->active_nodes);
+ 		} else if (faults < max_faults * 3 / 16)
+ 			node_clear(nid, numa_group->active_nodes);
+ 	}
+ }
+ 
+ /*
+  * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS
+  * increments. The more local the fault statistics are, the higher the scan
+  * period will be for the next scan window. If local/remote ratio is below
+  * NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS) the
+  * scan period will decrease
+  */
+ #define NUMA_PERIOD_SLOTS 10
+ #define NUMA_PERIOD_THRESHOLD 3
+ 
+ /*
+  * Increase the scan period (slow down scanning) if the majority of
+  * our memory is already on our local node, or if the majority of
+  * the page accesses are shared with other processes.
+  * Otherwise, decrease the scan period.
+  */
+ static void update_task_scan_period(struct task_struct *p,
+ 			unsigned long shared, unsigned long private)
+ {
+ 	unsigned int period_slot;
+ 	int ratio;
+ 	int diff;
+ 
+ 	unsigned long remote = p->numa_faults_locality[0];
+ 	unsigned long local = p->numa_faults_locality[1];
+ 
+ 	/*
+ 	 * If there were no record hinting faults then either the task is
+ 	 * completely idle or all activity is areas that are not of interest
+ 	 * to automatic numa balancing. Scan slower
+ 	 */
+ 	if (local + shared == 0) {
+ 		p->numa_scan_period = min(p->numa_scan_period_max,
+ 			p->numa_scan_period << 1);
+ 
+ 		p->mm->numa_next_scan = jiffies +
+ 			msecs_to_jiffies(p->numa_scan_period);
+ 
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * Prepare to scale scan period relative to the current period.
+ 	 *	 == NUMA_PERIOD_THRESHOLD scan period stays the same
+ 	 *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)
+ 	 *	 >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)
+ 	 */
+ 	period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+ 	ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);
+ 	if (ratio >= NUMA_PERIOD_THRESHOLD) {
+ 		int slot = ratio - NUMA_PERIOD_THRESHOLD;
+ 		if (!slot)
+ 			slot = 1;
+ 		diff = slot * period_slot;
+ 	} else {
+ 		diff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;
+ 
+ 		/*
+ 		 * Scale scan rate increases based on sharing. There is an
+ 		 * inverse relationship between the degree of sharing and
+ 		 * the adjustment made to the scanning period. Broadly
+ 		 * speaking the intent is that there is little point
+ 		 * scanning faster if shared accesses dominate as it may
+ 		 * simply bounce migrations uselessly
+ 		 */
+ 		ratio = DIV_ROUND_UP(private * NUMA_PERIOD_SLOTS, (private + shared));
+ 		diff = (diff * ratio) / NUMA_PERIOD_SLOTS;
+ 	}
+ 
+ 	p->numa_scan_period = clamp(p->numa_scan_period + diff,
+ 			task_scan_min(p), task_scan_max(p));
+ 	memset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));
+ }
+ 
+ /*
+  * Get the fraction of time the task has been running since the last
+  * NUMA placement cycle. The scheduler keeps similar statistics, but
+  * decays those on a 32ms period, which is orders of magnitude off
+  * from the dozens-of-seconds NUMA balancing period. Use the scheduler
+  * stats only if the task is so new there are no NUMA statistics yet.
+  */
+ static u64 numa_get_avg_runtime(struct task_struct *p, u64 *period)
+ {
+ 	u64 runtime, delta, now;
+ 	/* Use the start of this time slice to avoid calculations. */
+ 	now = p->se.exec_start;
+ 	runtime = p->se.sum_exec_runtime;
+ 
+ 	if (p->last_task_numa_placement) {
+ 		delta = runtime - p->last_sum_exec_runtime;
+ 		*period = now - p->last_task_numa_placement;
+ 	} else {
+ 		delta = p->se.avg.runnable_avg_sum;
+ 		*period = p->se.avg.runnable_avg_period;
+ 	}
+ 
+ 	p->last_sum_exec_runtime = runtime;
+ 	p->last_task_numa_placement = now;
+ 
+ 	return delta;
+ }
+ 
+ static void task_numa_placement(struct task_struct *p)
+ {
+ 	int seq, nid, max_nid = -1, max_group_nid = -1;
+ 	unsigned long max_faults = 0, max_group_faults = 0;
+ 	unsigned long fault_types[2] = { 0, 0 };
+ 	unsigned long total_faults;
+ 	u64 runtime, period;
+ 	spinlock_t *group_lock = NULL;
++>>>>>>> 7e2703e60996 (sched/numa: Normalize faults_cpu stats and weigh by CPU use)
  
 +	if (!p->mm)	/* for example, ksmd faulting in a user's mm */
 +		return;
  	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
  	if (p->numa_scan_seq == seq)
  		return;
  	p->numa_scan_seq = seq;
  	p->numa_scan_period_max = task_scan_max(p);
  
++<<<<<<< HEAD
++=======
+ 	total_faults = p->numa_faults_locality[0] +
+ 		       p->numa_faults_locality[1];
+ 	runtime = numa_get_avg_runtime(p, &period);
+ 
+ 	/* If the task is part of a group prevent parallel updates to group stats */
+ 	if (p->numa_group) {
+ 		group_lock = &p->numa_group->lock;
+ 		spin_lock(group_lock);
+ 	}
+ 
++>>>>>>> 7e2703e60996 (sched/numa: Normalize faults_cpu stats and weigh by CPU use)
  	/* Find the node with the highest number of faults */
  	for_each_online_node(nid) {
 -		unsigned long faults = 0, group_faults = 0;
 -		int priv, i;
 -
 +		unsigned long faults;
 +
++<<<<<<< HEAD
 +		/* Decay existing window and copy faults since last scan */
 +		p->numa_faults[nid] >>= 1;
 +		p->numa_faults[nid] += p->numa_faults_buffer[nid];
 +		p->numa_faults_buffer[nid] = 0;
++=======
+ 		for (priv = 0; priv < 2; priv++) {
+ 			long diff, f_diff, f_weight;
+ 
+ 			i = task_faults_idx(nid, priv);
+ 			diff = -p->numa_faults_memory[i];
+ 			f_diff = -p->numa_faults_cpu[i];
+ 
+ 			/* Decay existing window, copy faults since last scan */
+ 			p->numa_faults_memory[i] >>= 1;
+ 			p->numa_faults_memory[i] += p->numa_faults_buffer_memory[i];
+ 			fault_types[priv] += p->numa_faults_buffer_memory[i];
+ 			p->numa_faults_buffer_memory[i] = 0;
+ 
+ 			/*
+ 			 * Normalize the faults_from, so all tasks in a group
+ 			 * count according to CPU use, instead of by the raw
+ 			 * number of faults. Tasks with little runtime have
+ 			 * little over-all impact on throughput, and thus their
+ 			 * faults are less important.
+ 			 */
+ 			f_weight = div64_u64(runtime << 16, period + 1);
+ 			f_weight = (f_weight * p->numa_faults_buffer_cpu[i]) /
+ 				   (total_faults + 1);
+ 			p->numa_faults_cpu[i] >>= 1;
+ 			p->numa_faults_cpu[i] += f_weight;
+ 			p->numa_faults_buffer_cpu[i] = 0;
+ 
+ 			faults += p->numa_faults_memory[i];
+ 			diff += p->numa_faults_memory[i];
+ 			f_diff += p->numa_faults_cpu[i];
+ 			p->total_numa_faults += diff;
+ 			if (p->numa_group) {
+ 				/* safe because we can only change our own group */
+ 				p->numa_group->faults[i] += diff;
+ 				p->numa_group->faults_cpu[i] += f_diff;
+ 				p->numa_group->total_faults += diff;
+ 				group_faults += p->numa_group->faults[i];
+ 			}
+ 		}
++>>>>>>> 7e2703e60996 (sched/numa: Normalize faults_cpu stats and weigh by CPU use)
  
 +		faults = p->numa_faults[nid];
  		if (faults > max_faults) {
  			max_faults = faults;
  			max_nid = nid;
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 933a94b9c8f9..996f42c7e5df 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1351,6 +1351,8 @@ struct task_struct {
 	unsigned int numa_scan_period;
 	unsigned int numa_scan_period_max;
 	u64 node_stamp;			/* migration stamp  */
+	u64 last_task_numa_placement;
+	u64 last_sum_exec_runtime;
 	struct callback_head numa_work;
 
 	/*
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/fair.c
