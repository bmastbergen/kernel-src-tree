sched/numa: Adjust scan rate in task_numa_placement

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Rik van Riel <riel@redhat.com>
commit 04bb2f9475054298f0c67a89ca92cade42d3fe5e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/04bb2f94.failed

Adjust numa_scan_period in task_numa_placement, depending on how much
useful work the numa code can do. The more local faults there are in a
given scan window the longer the period (and hence the slower the scan rate)
during the next window. If there are excessive shared faults then the scan
period will decrease with the amount of scaling depending on whether the
ratio of shared/private faults. If the preferred node changes then the
scan rate is reset to recheck if the task is properly placed.

	Signed-off-by: Rik van Riel <riel@redhat.com>
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-59-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 04bb2f9475054298f0c67a89ca92cade42d3fe5e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched.h
#	kernel/sched/fair.c
diff --cc include/linux/sched.h
index 664a046b12fb,2292f6c1596f..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -1359,7 -1365,16 +1359,15 @@@ struct task_struct 
  	 */
  	unsigned long *numa_faults_buffer;
  
+ 	/*
+ 	 * numa_faults_locality tracks if faults recorded during the last
+ 	 * scan window were remote/local. The task scan period is adapted
+ 	 * based on the locality of the faults with different weights
+ 	 * depending on whether they were shared or private faults
+ 	 */
+ 	unsigned long numa_faults_locality[2];
+ 
  	int numa_preferred_nid;
 -	unsigned long numa_pages_migrated;
  #endif /* CONFIG_NUMA_BALANCING */
  
  	struct rcu_head rcu;
@@@ -1441,13 -1460,25 +1449,21 @@@
  /* Future-safe accessor for struct task_struct's cpus_allowed. */
  #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
  
++<<<<<<< HEAD
++=======
+ #define TNF_MIGRATED	0x01
+ #define TNF_NO_GROUP	0x02
+ #define TNF_SHARED	0x04
+ #define TNF_FAULT_LOCAL	0x08
+ 
++>>>>>>> 04bb2f947505 (sched/numa: Adjust scan rate in task_numa_placement)
  #ifdef CONFIG_NUMA_BALANCING
 -extern void task_numa_fault(int last_node, int node, int pages, int flags);
 -extern pid_t task_numa_group_id(struct task_struct *p);
 +extern void task_numa_fault(int node, int pages, bool migrated);
  extern void set_numabalancing_state(bool enabled);
 -extern void task_numa_free(struct task_struct *p);
  #else
 -static inline void task_numa_fault(int last_node, int node, int pages,
 -				   int flags)
 +static inline void task_numa_fault(int node, int pages, bool migrated)
  {
  }
 -static inline pid_t task_numa_group_id(struct task_struct *p)
 -{
 -	return 0;
 -}
  static inline void set_numabalancing_state(bool enabled)
  {
  }
diff --cc kernel/sched/fair.c
index 98cee68da024,66237ff8b01e..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,38 -879,716 +839,726 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
 +static void task_numa_placement(struct task_struct *p)
 +{
 +	int seq, nid, max_nid = -1;
 +	unsigned long max_faults = 0;
++=======
+ /*
+  * Once a preferred node is selected the scheduler balancer will prefer moving
+  * a task to that node for sysctl_numa_balancing_settle_count number of PTE
+  * scans. This will give the process the chance to accumulate more faults on
+  * the preferred node but still allow the scheduler to move the task again if
+  * the nodes CPUs are overloaded.
+  */
+ unsigned int sysctl_numa_balancing_settle_count __read_mostly = 4;
+ 
+ static void account_numa_enqueue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running += (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ static void account_numa_dequeue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running -= (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ struct numa_group {
+ 	atomic_t refcount;
+ 
+ 	spinlock_t lock; /* nr_tasks, tasks */
+ 	int nr_tasks;
+ 	pid_t gid;
+ 	struct list_head task_list;
+ 
+ 	struct rcu_head rcu;
+ 	atomic_long_t total_faults;
+ 	atomic_long_t faults[0];
+ };
+ 
+ pid_t task_numa_group_id(struct task_struct *p)
+ {
+ 	return p->numa_group ? p->numa_group->gid : 0;
+ }
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static inline unsigned long group_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	return atomic_long_read(&p->numa_group->faults[2*nid]) +
+ 	       atomic_long_read(&p->numa_group->faults[2*nid+1]);
+ }
+ 
+ /*
+  * These return the fraction of accesses done by a particular task, or
+  * task group, on a particular numa node.  The group weight is given a
+  * larger multiplier, in order to group tasks together that are almost
+  * evenly spread out between numa nodes.
+  */
+ static inline unsigned long task_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	total_faults = p->total_numa_faults;
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * task_faults(p, nid) / total_faults;
+ }
+ 
+ static inline unsigned long group_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	total_faults = atomic_long_read(&p->numa_group->total_faults);
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * group_faults(p, nid) / total_faults;
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ /* Cached statistics for all CPUs within a node */
+ struct numa_stats {
+ 	unsigned long nr_running;
+ 	unsigned long load;
+ 
+ 	/* Total compute capacity of CPUs on a node */
+ 	unsigned long power;
+ 
+ 	/* Approximate capacity in terms of runnable tasks on a node */
+ 	unsigned long capacity;
+ 	int has_capacity;
+ };
+ 
+ /*
+  * XXX borrowed from update_sg_lb_stats
+  */
+ static void update_numa_stats(struct numa_stats *ns, int nid)
+ {
+ 	int cpu;
+ 
+ 	memset(ns, 0, sizeof(*ns));
+ 	for_each_cpu(cpu, cpumask_of_node(nid)) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 
+ 		ns->nr_running += rq->nr_running;
+ 		ns->load += weighted_cpuload(cpu);
+ 		ns->power += power_of(cpu);
+ 	}
+ 
+ 	ns->load = (ns->load * SCHED_POWER_SCALE) / ns->power;
+ 	ns->capacity = DIV_ROUND_CLOSEST(ns->power, SCHED_POWER_SCALE);
+ 	ns->has_capacity = (ns->nr_running < ns->capacity);
+ }
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	int imbalance_pct, idx;
+ 
+ 	struct task_struct *best_task;
+ 	long best_imp;
+ 	int best_cpu;
+ };
+ 
+ static void task_numa_assign(struct task_numa_env *env,
+ 			     struct task_struct *p, long imp)
+ {
+ 	if (env->best_task)
+ 		put_task_struct(env->best_task);
+ 	if (p)
+ 		get_task_struct(p);
+ 
+ 	env->best_task = p;
+ 	env->best_imp = imp;
+ 	env->best_cpu = env->dst_cpu;
+ }
+ 
+ /*
+  * This checks if the overall compute and NUMA accesses of the system would
+  * be improved if the source tasks was migrated to the target dst_cpu taking
+  * into account that it might be best if task running on the dst_cpu should
+  * be exchanged with the source task
+  */
+ static void task_numa_compare(struct task_numa_env *env,
+ 			      long taskimp, long groupimp)
+ {
+ 	struct rq *src_rq = cpu_rq(env->src_cpu);
+ 	struct rq *dst_rq = cpu_rq(env->dst_cpu);
+ 	struct task_struct *cur;
+ 	long dst_load, src_load;
+ 	long load;
+ 	long imp = (groupimp > 0) ? groupimp : taskimp;
+ 
+ 	rcu_read_lock();
+ 	cur = ACCESS_ONCE(dst_rq->curr);
+ 	if (cur->pid == 0) /* idle */
+ 		cur = NULL;
+ 
+ 	/*
+ 	 * "imp" is the fault differential for the source task between the
+ 	 * source and destination node. Calculate the total differential for
+ 	 * the source task and potential destination task. The more negative
+ 	 * the value is, the more rmeote accesses that would be expected to
+ 	 * be incurred if the tasks were swapped.
+ 	 */
+ 	if (cur) {
+ 		/* Skip this swap candidate if cannot move to the source cpu */
+ 		if (!cpumask_test_cpu(env->src_cpu, tsk_cpus_allowed(cur)))
+ 			goto unlock;
+ 
+ 		/*
+ 		 * If dst and source tasks are in the same NUMA group, or not
+ 		 * in any group then look only at task weights.
+ 		 */
+ 		if (cur->numa_group == env->p->numa_group) {
+ 			imp = taskimp + task_weight(cur, env->src_nid) -
+ 			      task_weight(cur, env->dst_nid);
+ 			/*
+ 			 * Add some hysteresis to prevent swapping the
+ 			 * tasks within a group over tiny differences.
+ 			 */
+ 			if (cur->numa_group)
+ 				imp -= imp/16;
+ 		} else {
+ 			/*
+ 			 * Compare the group weights. If a task is all by
+ 			 * itself (not part of a group), use the task weight
+ 			 * instead.
+ 			 */
+ 			if (env->p->numa_group)
+ 				imp = groupimp;
+ 			else
+ 				imp = taskimp;
+ 
+ 			if (cur->numa_group)
+ 				imp += group_weight(cur, env->src_nid) -
+ 				       group_weight(cur, env->dst_nid);
+ 			else
+ 				imp += task_weight(cur, env->src_nid) -
+ 				       task_weight(cur, env->dst_nid);
+ 		}
+ 	}
+ 
+ 	if (imp < env->best_imp)
+ 		goto unlock;
+ 
+ 	if (!cur) {
+ 		/* Is there capacity at our destination? */
+ 		if (env->src_stats.has_capacity &&
+ 		    !env->dst_stats.has_capacity)
+ 			goto unlock;
+ 
+ 		goto balance;
+ 	}
+ 
+ 	/* Balance doesn't matter much if we're running a task per cpu */
+ 	if (src_rq->nr_running == 1 && dst_rq->nr_running == 1)
+ 		goto assign;
+ 
+ 	/*
+ 	 * In the overloaded case, try and keep the load balanced.
+ 	 */
+ balance:
+ 	dst_load = env->dst_stats.load;
+ 	src_load = env->src_stats.load;
+ 
+ 	/* XXX missing power terms */
+ 	load = task_h_load(env->p);
+ 	dst_load += load;
+ 	src_load -= load;
+ 
+ 	if (cur) {
+ 		load = task_h_load(cur);
+ 		dst_load -= load;
+ 		src_load += load;
+ 	}
+ 
+ 	/* make src_load the smaller */
+ 	if (dst_load < src_load)
+ 		swap(dst_load, src_load);
+ 
+ 	if (src_load * env->imbalance_pct < dst_load * 100)
+ 		goto unlock;
+ 
+ assign:
+ 	task_numa_assign(env, cur, imp);
+ unlock:
+ 	rcu_read_unlock();
+ }
+ 
+ static void task_numa_find_cpu(struct task_numa_env *env,
+ 				long taskimp, long groupimp)
+ {
+ 	int cpu;
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
+ 		/* Skip this CPU if the source task cannot migrate */
+ 		if (!cpumask_test_cpu(cpu, tsk_cpus_allowed(env->p)))
+ 			continue;
+ 
+ 		env->dst_cpu = cpu;
+ 		task_numa_compare(env, taskimp, groupimp);
+ 	}
+ }
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = task_node(p),
+ 
+ 		.imbalance_pct = 112,
+ 
+ 		.best_task = NULL,
+ 		.best_imp = 0,
+ 		.best_cpu = -1
+ 	};
+ 	struct sched_domain *sd;
+ 	unsigned long taskweight, groupweight;
+ 	int nid, ret;
+ 	long taskimp, groupimp;
+ 
+ 	/*
+ 	 * Pick the lowest SD_NUMA domain, as that would have the smallest
+ 	 * imbalance and would be the first to start moving tasks about.
+ 	 *
+ 	 * And we want to avoid any moving of tasks about, as that would create
+ 	 * random movement of tasks -- counter the numa conditions we're trying
+ 	 * to satisfy here.
+ 	 */
+ 	rcu_read_lock();
+ 	sd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));
+ 	env.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;
+ 	rcu_read_unlock();
+ 
+ 	taskweight = task_weight(p, env.src_nid);
+ 	groupweight = group_weight(p, env.src_nid);
+ 	update_numa_stats(&env.src_stats, env.src_nid);
+ 	env.dst_nid = p->numa_preferred_nid;
+ 	taskimp = task_weight(p, env.dst_nid) - taskweight;
+ 	groupimp = group_weight(p, env.dst_nid) - groupweight;
+ 	update_numa_stats(&env.dst_stats, env.dst_nid);
+ 
+ 	/* If the preferred nid has capacity, try to use it. */
+ 	if (env.dst_stats.has_capacity)
+ 		task_numa_find_cpu(&env, taskimp, groupimp);
+ 
+ 	/* No space available on the preferred nid. Look elsewhere. */
+ 	if (env.best_cpu == -1) {
+ 		for_each_online_node(nid) {
+ 			if (nid == env.src_nid || nid == p->numa_preferred_nid)
+ 				continue;
+ 
+ 			/* Only consider nodes where both task and groups benefit */
+ 			taskimp = task_weight(p, nid) - taskweight;
+ 			groupimp = group_weight(p, nid) - groupweight;
+ 			if (taskimp < 0 && groupimp < 0)
+ 				continue;
+ 
+ 			env.dst_nid = nid;
+ 			update_numa_stats(&env.dst_stats, env.dst_nid);
+ 			task_numa_find_cpu(&env, taskimp, groupimp);
+ 		}
+ 	}
+ 
+ 	/* No better CPU than the current one was found. */
+ 	if (env.best_cpu == -1)
+ 		return -EAGAIN;
+ 
+ 	sched_setnuma(p, env.dst_nid);
+ 
+ 	/*
+ 	 * Reset the scan period if the task is being rescheduled on an
+ 	 * alternative node to recheck if the tasks is now properly placed.
+ 	 */
+ 	p->numa_scan_period = task_scan_min(p);
+ 
+ 	if (env.best_task == NULL) {
+ 		int ret = migrate_task_to(p, env.best_cpu);
+ 		return ret;
+ 	}
+ 
+ 	ret = migrate_swap(p, env.best_task);
+ 	put_task_struct(env.best_task);
+ 	return ret;
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* Success if task is already running on preferred CPU */
+ 	p->numa_migrate_retry = 0;
+ 	if (cpu_to_node(task_cpu(p)) == p->numa_preferred_nid) {
+ 		/*
+ 		 * If migration is temporarily disabled due to a task migration
+ 		 * then re-enable it now as the task is running on its
+ 		 * preferred node and memory should migrate locally
+ 		 */
+ 		if (!p->numa_migrate_seq)
+ 			p->numa_migrate_seq++;
+ 		return;
+ 	}
+ 
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1))
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	if (task_numa_migrate(p) != 0)
+ 		p->numa_migrate_retry = jiffies + HZ*5;
+ }
+ 
+ /*
+  * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS
+  * increments. The more local the fault statistics are, the higher the scan
+  * period will be for the next scan window. If local/remote ratio is below
+  * NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS) the
+  * scan period will decrease
+  */
+ #define NUMA_PERIOD_SLOTS 10
+ #define NUMA_PERIOD_THRESHOLD 3
+ 
+ /*
+  * Increase the scan period (slow down scanning) if the majority of
+  * our memory is already on our local node, or if the majority of
+  * the page accesses are shared with other processes.
+  * Otherwise, decrease the scan period.
+  */
+ static void update_task_scan_period(struct task_struct *p,
+ 			unsigned long shared, unsigned long private)
+ {
+ 	unsigned int period_slot;
+ 	int ratio;
+ 	int diff;
+ 
+ 	unsigned long remote = p->numa_faults_locality[0];
+ 	unsigned long local = p->numa_faults_locality[1];
+ 
+ 	/*
+ 	 * If there were no record hinting faults then either the task is
+ 	 * completely idle or all activity is areas that are not of interest
+ 	 * to automatic numa balancing. Scan slower
+ 	 */
+ 	if (local + shared == 0) {
+ 		p->numa_scan_period = min(p->numa_scan_period_max,
+ 			p->numa_scan_period << 1);
+ 
+ 		p->mm->numa_next_scan = jiffies +
+ 			msecs_to_jiffies(p->numa_scan_period);
+ 
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * Prepare to scale scan period relative to the current period.
+ 	 *	 == NUMA_PERIOD_THRESHOLD scan period stays the same
+ 	 *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)
+ 	 *	 >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)
+ 	 */
+ 	period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+ 	ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);
+ 	if (ratio >= NUMA_PERIOD_THRESHOLD) {
+ 		int slot = ratio - NUMA_PERIOD_THRESHOLD;
+ 		if (!slot)
+ 			slot = 1;
+ 		diff = slot * period_slot;
+ 	} else {
+ 		diff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;
+ 
+ 		/*
+ 		 * Scale scan rate increases based on sharing. There is an
+ 		 * inverse relationship between the degree of sharing and
+ 		 * the adjustment made to the scanning period. Broadly
+ 		 * speaking the intent is that there is little point
+ 		 * scanning faster if shared accesses dominate as it may
+ 		 * simply bounce migrations uselessly
+ 		 */
+ 		period_slot = DIV_ROUND_UP(diff, NUMA_PERIOD_SLOTS);
+ 		ratio = DIV_ROUND_UP(private * NUMA_PERIOD_SLOTS, (private + shared));
+ 		diff = (diff * ratio) / NUMA_PERIOD_SLOTS;
+ 	}
+ 
+ 	p->numa_scan_period = clamp(p->numa_scan_period + diff,
+ 			task_scan_min(p), task_scan_max(p));
+ 	memset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));
+ }
+ 
+ static void task_numa_placement(struct task_struct *p)
+ {
+ 	int seq, nid, max_nid = -1, max_group_nid = -1;
+ 	unsigned long max_faults = 0, max_group_faults = 0;
+ 	unsigned long fault_types[2] = { 0, 0 };
+ 	spinlock_t *group_lock = NULL;
++>>>>>>> 04bb2f947505 (sched/numa: Adjust scan rate in task_numa_placement)
  
 +	if (!p->mm)	/* for example, ksmd faulting in a user's mm */
 +		return;
  	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
  	if (p->numa_scan_seq == seq)
  		return;
  	p->numa_scan_seq = seq;
 -	p->numa_migrate_seq++;
  	p->numa_scan_period_max = task_scan_max(p);
  
 -	/* If the task is part of a group prevent parallel updates to group stats */
 -	if (p->numa_group) {
 -		group_lock = &p->numa_group->lock;
 -		spin_lock(group_lock);
 -	}
 -
  	/* Find the node with the highest number of faults */
  	for_each_online_node(nid) {
 -		unsigned long faults = 0, group_faults = 0;
 -		int priv, i;
 -
 +		unsigned long faults;
 +
++<<<<<<< HEAD
 +		/* Decay existing window and copy faults since last scan */
 +		p->numa_faults[nid] >>= 1;
 +		p->numa_faults[nid] += p->numa_faults_buffer[nid];
 +		p->numa_faults_buffer[nid] = 0;
++=======
+ 		for (priv = 0; priv < 2; priv++) {
+ 			long diff;
+ 
+ 			i = task_faults_idx(nid, priv);
+ 			diff = -p->numa_faults[i];
+ 
+ 			/* Decay existing window, copy faults since last scan */
+ 			p->numa_faults[i] >>= 1;
+ 			p->numa_faults[i] += p->numa_faults_buffer[i];
+ 			fault_types[priv] += p->numa_faults_buffer[i];
+ 			p->numa_faults_buffer[i] = 0;
+ 
+ 			faults += p->numa_faults[i];
+ 			diff += p->numa_faults[i];
+ 			p->total_numa_faults += diff;
+ 			if (p->numa_group) {
+ 				/* safe because we can only change our own group */
+ 				atomic_long_add(diff, &p->numa_group->faults[i]);
+ 				atomic_long_add(diff, &p->numa_group->total_faults);
+ 				group_faults += atomic_long_read(&p->numa_group->faults[i]);
+ 			}
+ 		}
++>>>>>>> 04bb2f947505 (sched/numa: Adjust scan rate in task_numa_placement)
  
 +		faults = p->numa_faults[nid];
  		if (faults > max_faults) {
  			max_faults = faults;
  			max_nid = nid;
  		}
 -
 -		if (group_faults > max_group_faults) {
 -			max_group_faults = group_faults;
 -			max_group_nid = nid;
 -		}
  	}
  
++<<<<<<< HEAD
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
 +		p->numa_preferred_nid = max_nid;
++=======
+ 	update_task_scan_period(p, fault_types[0], fault_types[1]);
+ 
+ 	if (p->numa_group) {
+ 		/*
+ 		 * If the preferred task and group nids are different,
+ 		 * iterate over the nodes again to find the best place.
+ 		 */
+ 		if (max_nid != max_group_nid) {
+ 			unsigned long weight, max_weight = 0;
+ 
+ 			for_each_online_node(nid) {
+ 				weight = task_weight(p, nid) + group_weight(p, nid);
+ 				if (weight > max_weight) {
+ 					max_weight = weight;
+ 					max_nid = nid;
+ 				}
+ 			}
+ 		}
+ 
+ 		spin_unlock(group_lock);
+ 	}
+ 
+ 	/* Preferred node as the node with the most faults */
+ 	if (max_faults && max_nid != p->numa_preferred_nid) {
+ 		/* Update the preferred nid and migrate task if possible */
+ 		sched_setnuma(p, max_nid);
+ 		numa_migrate_preferred(p);
+ 	}
+ }
+ 
+ static inline int get_numa_group(struct numa_group *grp)
+ {
+ 	return atomic_inc_not_zero(&grp->refcount);
+ }
+ 
+ static inline void put_numa_group(struct numa_group *grp)
+ {
+ 	if (atomic_dec_and_test(&grp->refcount))
+ 		kfree_rcu(grp, rcu);
+ }
+ 
+ static void double_lock(spinlock_t *l1, spinlock_t *l2)
+ {
+ 	if (l1 > l2)
+ 		swap(l1, l2);
+ 
+ 	spin_lock(l1);
+ 	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+ }
+ 
+ static void task_numa_group(struct task_struct *p, int cpupid, int flags,
+ 			int *priv)
+ {
+ 	struct numa_group *grp, *my_grp;
+ 	struct task_struct *tsk;
+ 	bool join = false;
+ 	int cpu = cpupid_to_cpu(cpupid);
+ 	int i;
+ 
+ 	if (unlikely(!p->numa_group)) {
+ 		unsigned int size = sizeof(struct numa_group) +
+ 				    2*nr_node_ids*sizeof(atomic_long_t);
+ 
+ 		grp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);
+ 		if (!grp)
+ 			return;
+ 
+ 		atomic_set(&grp->refcount, 1);
+ 		spin_lock_init(&grp->lock);
+ 		INIT_LIST_HEAD(&grp->task_list);
+ 		grp->gid = p->pid;
+ 
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_set(&grp->faults[i], p->numa_faults[i]);
+ 
+ 		atomic_long_set(&grp->total_faults, p->total_numa_faults);
+ 
+ 		list_add(&p->numa_entry, &grp->task_list);
+ 		grp->nr_tasks++;
+ 		rcu_assign_pointer(p->numa_group, grp);
+ 	}
+ 
+ 	rcu_read_lock();
+ 	tsk = ACCESS_ONCE(cpu_rq(cpu)->curr);
+ 
+ 	if (!cpupid_match_pid(tsk, cpupid))
+ 		goto unlock;
+ 
+ 	grp = rcu_dereference(tsk->numa_group);
+ 	if (!grp)
+ 		goto unlock;
+ 
+ 	my_grp = p->numa_group;
+ 	if (grp == my_grp)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Only join the other group if its bigger; if we're the bigger group,
+ 	 * the other task will join us.
+ 	 */
+ 	if (my_grp->nr_tasks > grp->nr_tasks)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Tie-break on the grp address.
+ 	 */
+ 	if (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)
+ 		goto unlock;
+ 
+ 	/* Always join threads in the same process. */
+ 	if (tsk->mm == current->mm)
+ 		join = true;
+ 
+ 	/* Simple filter to avoid false positives due to PID collisions */
+ 	if (flags & TNF_SHARED)
+ 		join = true;
+ 
+ 	/* Update priv based on whether false sharing was detected */
+ 	*priv = !join;
+ 
+ 	if (join && !get_numa_group(grp))
+ 		join = false;
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 
+ 	if (!join)
+ 		return;
+ 
+ 	for (i = 0; i < 2*nr_node_ids; i++) {
+ 		atomic_long_sub(p->numa_faults[i], &my_grp->faults[i]);
+ 		atomic_long_add(p->numa_faults[i], &grp->faults[i]);
+ 	}
+ 	atomic_long_sub(p->total_numa_faults, &my_grp->total_faults);
+ 	atomic_long_add(p->total_numa_faults, &grp->total_faults);
+ 
+ 	double_lock(&my_grp->lock, &grp->lock);
+ 
+ 	list_move(&p->numa_entry, &grp->task_list);
+ 	my_grp->nr_tasks--;
+ 	grp->nr_tasks++;
+ 
+ 	spin_unlock(&my_grp->lock);
+ 	spin_unlock(&grp->lock);
+ 
+ 	rcu_assign_pointer(p->numa_group, grp);
+ 
+ 	put_numa_group(my_grp);
+ }
+ 
+ void task_numa_free(struct task_struct *p)
+ {
+ 	struct numa_group *grp = p->numa_group;
+ 	int i;
+ 	void *numa_faults = p->numa_faults;
+ 
+ 	if (grp) {
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_sub(p->numa_faults[i], &grp->faults[i]);
+ 
+ 		atomic_long_sub(p->total_numa_faults, &grp->total_faults);
+ 
+ 		spin_lock(&grp->lock);
+ 		list_del(&p->numa_entry);
+ 		grp->nr_tasks--;
+ 		spin_unlock(&grp->lock);
+ 		rcu_assign_pointer(p->numa_group, NULL);
+ 		put_numa_group(grp);
+ 	}
+ 
+ 	p->numa_faults = NULL;
+ 	p->numa_faults_buffer = NULL;
+ 	kfree(numa_faults);
++>>>>>>> 04bb2f947505 (sched/numa: Adjust scan rate in task_numa_placement)
  }
  
  /*
@@@ -893,25 -1621,34 +1581,42 @@@ void task_numa_fault(int node, int page
  			return;
  
  		BUG_ON(p->numa_faults_buffer);
++<<<<<<< HEAD
 +		p->numa_faults_buffer = p->numa_faults + nr_node_ids;
++=======
+ 		p->numa_faults_buffer = p->numa_faults + (2 * nr_node_ids);
+ 		p->total_numa_faults = 0;
+ 		memset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));
  	}
  
  	/*
- 	 * If pages are properly placed (did not migrate) then scan slower.
- 	 * This is reset periodically in case of phase changes
+ 	 * First accesses are treated as private, otherwise consider accesses
+ 	 * to be private if the accessing pid has not changed
  	 */
- 	if (!migrated) {
- 		/* Initialise if necessary */
- 		if (!p->numa_scan_period_max)
- 			p->numa_scan_period_max = task_scan_max(p);
- 
- 		p->numa_scan_period = min(p->numa_scan_period_max,
- 			p->numa_scan_period + 10);
+ 	if (unlikely(last_cpupid == (-1 & LAST_CPUPID_MASK))) {
+ 		priv = 1;
+ 	} else {
+ 		priv = cpupid_match_pid(p, last_cpupid);
+ 		if (!priv && !(flags & TNF_NO_GROUP))
+ 			task_numa_group(p, last_cpupid, flags, &priv);
++>>>>>>> 04bb2f947505 (sched/numa: Adjust scan rate in task_numa_placement)
  	}
  
  	task_numa_placement(p);
  
++<<<<<<< HEAD
 +	p->numa_faults_buffer[node] += pages;
++=======
+ 	/* Retry task to preferred node migration if it previously failed */
+ 	if (p->numa_migrate_retry && time_after(jiffies, p->numa_migrate_retry))
+ 		numa_migrate_preferred(p);
+ 
+ 	if (migrated)
+ 		p->numa_pages_migrated += pages;
+ 
+ 	p->numa_faults_buffer[task_faults_idx(node, priv)] += pages;
+ 	p->numa_faults_locality[!!(flags & TNF_FAULT_LOCAL)] += pages;
++>>>>>>> 04bb2f947505 (sched/numa: Adjust scan rate in task_numa_placement)
  }
  
  static void reset_ptenuma_scan(struct task_struct *p)
* Unmerged path include/linux/sched.h
* Unmerged path kernel/sched/fair.c
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index ba9f65152a91..60ff8d1ae6ed 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1306,8 +1306,10 @@ int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	BUG_ON(is_huge_zero_page(page));
 	page_nid = page_to_nid(page);
 	count_vm_numa_event(NUMA_HINT_FAULTS);
-	if (page_nid == this_nid)
+	if (page_nid == this_nid) {
 		count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
+		flags |= TNF_FAULT_LOCAL;
+	}
 
 	/*
 	 * Acquire the page lock to serialise THP migrations but avoid dropping
diff --git a/mm/memory.c b/mm/memory.c
index 6285e7df615f..6bfa41f193f3 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3516,13 +3516,16 @@ static int do_nonlinear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 }
 
 int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
-				unsigned long addr, int page_nid)
+				unsigned long addr, int page_nid,
+				int *flags)
 {
 	get_page(page);
 
 	count_vm_numa_event(NUMA_HINT_FAULTS);
-	if (page_nid == numa_node_id())
+	if (page_nid == numa_node_id()) {
 		count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
+		*flags |= TNF_FAULT_LOCAL;
+	}
 
 	return mpol_misplaced(page, vma, addr);
 }
@@ -3564,7 +3567,7 @@ int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	BUG_ON(is_zero_pfn(page_to_pfn(page)));
 
 	page_nid = page_to_nid(page);
-	target_nid = numa_migrate_prep(page, vma, addr, page_nid);
+	target_nid = numa_migrate_prep(page, vma, addr, page_nid, &flags);
 	pte_unmap_unlock(ptep, ptl);
 	if (target_nid == -1) {
 		put_page(page);
