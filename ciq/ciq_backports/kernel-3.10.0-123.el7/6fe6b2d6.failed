sched/numa: Do not migrate memory immediately after switching node

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Rik van Riel <riel@redhat.com>
commit 6fe6b2d6dabf392aceb3ad3a5e859b46a04465c6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/6fe6b2d6.failed

The load balancer can move tasks between nodes and does not take NUMA
locality into account. With automatic NUMA balancing this may result in the
tasks working set being migrated to the new node. However, as the fault
buffer will still store faults from the old node the schduler may decide to
reset the preferred node and migrate the task back resulting in more
migrations.

The ideal would be that the scheduler did not migrate tasks with a heavy
memory footprint but this may result nodes being overloaded. We could
also discard the fault information on task migration but this would still
cause all the tasks working set to be migrated. This patch simply avoids
migrating the memory for a short time after a task is migrated.

	Signed-off-by: Rik van Riel <riel@redhat.com>
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-31-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 6fe6b2d6dabf392aceb3ad3a5e859b46a04465c6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/fair.c
diff --cc kernel/sched/core.c
index e7a70e97bb75,9060a7f4e9ed..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -1640,7 -1631,7 +1640,11 @@@ static void __sched_fork(struct task_st
  
  	p->node_stamp = 0ULL;
  	p->numa_scan_seq = p->mm ? p->mm->numa_scan_seq : 0;
++<<<<<<< HEAD
 +	p->numa_migrate_seq = p->mm ? p->mm->numa_scan_seq - 1 : 0;
++=======
+ 	p->numa_migrate_seq = 1;
++>>>>>>> 6fe6b2d6dabf (sched/numa: Do not migrate memory immediately after switching node)
  	p->numa_scan_period = sysctl_numa_balancing_scan_delay;
  	p->numa_preferred_nid = -1;
  	p->numa_work.next = &p->numa_work;
diff --cc kernel/sched/fair.c
index ee90b28e4ef6,61ec0d4765b9..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,6 -877,54 +839,57 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Once a preferred node is selected the scheduler balancer will prefer moving
+  * a task to that node for sysctl_numa_balancing_settle_count number of PTE
+  * scans. This will give the process the chance to accumulate more faults on
+  * the preferred node but still allow the scheduler to move the task again if
+  * the nodes CPUs are overloaded.
+  */
+ unsigned int sysctl_numa_balancing_settle_count __read_mostly = 4;
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ 
+ 
+ static int
+ find_idlest_cpu_node(int this_cpu, int nid)
+ {
+ 	unsigned long load, min_load = ULONG_MAX;
+ 	int i, idlest_cpu = this_cpu;
+ 
+ 	BUG_ON(cpu_to_node(this_cpu) == nid);
+ 
+ 	rcu_read_lock();
+ 	for_each_cpu(i, cpumask_of_node(nid)) {
+ 		load = weighted_cpuload(i);
+ 
+ 		if (load < min_load) {
+ 			min_load = load;
+ 			idlest_cpu = i;
+ 		}
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	return idlest_cpu;
+ }
+ 
++>>>>>>> 6fe6b2d6dabf (sched/numa: Do not migrate memory immediately after switching node)
  static void task_numa_placement(struct task_struct *p)
  {
  	int seq, nid, max_nid = -1;
@@@ -868,9 -959,30 +919,15 @@@
  		}
  	}
  
 -	/*
 -	 * Record the preferred node as the node with the most faults,
 -	 * requeue the task to be running on the idlest CPU on the
 -	 * preferred node and reset the scanning rate to recheck
 -	 * the working set placement.
 -	 */
 -	if (max_faults && max_nid != p->numa_preferred_nid) {
 -		int preferred_cpu;
 -
 -		/*
 -		 * If the task is not on the preferred node then find the most
 -		 * idle CPU to migrate to.
 -		 */
 -		preferred_cpu = task_cpu(p);
 -		if (cpu_to_node(preferred_cpu) != max_nid) {
 -			preferred_cpu = find_idlest_cpu_node(preferred_cpu,
 -							     max_nid);
 -		}
 -
 -		/* Update the preferred nid and migrate task if possible */
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
  		p->numa_preferred_nid = max_nid;
++<<<<<<< HEAD
++=======
+ 		p->numa_migrate_seq = 1;
+ 		migrate_task_to(p, preferred_cpu);
+ 	}
++>>>>>>> 6fe6b2d6dabf (sched/numa: Do not migrate memory immediately after switching node)
  }
  
  /*
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/fair.c
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 74310017296e..83b86b1db48c 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -2316,6 +2316,18 @@ int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long
 		last_nid = page_nid_xchg_last(page, polnid);
 		if (last_nid != polnid)
 			goto out;
+
+#ifdef CONFIG_NUMA_BALANCING
+		/*
+		 * If the scheduler has just moved us away from our
+		 * preferred node, do not bother migrating pages yet.
+		 * This way a short and temporary process migration will
+		 * not cause excessive memory migration.
+		 */
+		if (polnid != current->numa_preferred_nid &&
+				!current->numa_migrate_seq)
+			goto out;
+#endif
 	}
 
 	if (curnid != polnid)
