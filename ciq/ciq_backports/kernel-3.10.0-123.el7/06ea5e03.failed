sched/numa: Increment numa_migrate_seq when task runs in correct location

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Rik van Riel <riel@redhat.com>
commit 06ea5e035b4e66cc77790457a89fc7e368060c4b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/06ea5e03.failed

When a task is already running on its preferred node, increment
numa_migrate_seq to indicate that the task is settled if migration is
temporarily disabled, and memory should migrate towards it.

	Signed-off-by: Rik van Riel <riel@redhat.com>
[ Only increment migrate_seq if migration temporarily disabled. ]
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-35-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 06ea5e035b4e66cc77790457a89fc7e368060c4b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 70f4aef1e8ed,de9b4d8eb853..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,6 -877,165 +839,168 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Once a preferred node is selected the scheduler balancer will prefer moving
+  * a task to that node for sysctl_numa_balancing_settle_count number of PTE
+  * scans. This will give the process the chance to accumulate more faults on
+  * the preferred node but still allow the scheduler to move the task again if
+  * the nodes CPUs are overloaded.
+  */
+ unsigned int sysctl_numa_balancing_settle_count __read_mostly = 4;
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ struct numa_stats {
+ 	unsigned long load;
+ 	s64 eff_load;
+ 	unsigned long faults;
+ };
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	unsigned long best_load;
+ 	int best_cpu;
+ };
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	int node_cpu = cpumask_first(cpumask_of_node(p->numa_preferred_nid));
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = cpu_to_node(task_cpu(p)),
+ 		.dst_cpu = node_cpu,
+ 		.dst_nid = p->numa_preferred_nid,
+ 		.best_load = ULONG_MAX,
+ 		.best_cpu = task_cpu(p),
+ 	};
+ 	struct sched_domain *sd;
+ 	int cpu;
+ 	struct task_group *tg = task_group(p);
+ 	unsigned long weight;
+ 	bool balanced;
+ 	int imbalance_pct, idx = -1;
+ 
+ 	/*
+ 	 * Find the lowest common scheduling domain covering the nodes of both
+ 	 * the CPU the task is currently running on and the target NUMA node.
+ 	 */
+ 	rcu_read_lock();
+ 	for_each_domain(env.src_cpu, sd) {
+ 		if (cpumask_test_cpu(node_cpu, sched_domain_span(sd))) {
+ 			/*
+ 			 * busy_idx is used for the load decision as it is the
+ 			 * same index used by the regular load balancer for an
+ 			 * active cpu.
+ 			 */
+ 			idx = sd->busy_idx;
+ 			imbalance_pct = sd->imbalance_pct;
+ 			break;
+ 		}
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	if (WARN_ON_ONCE(idx == -1))
+ 		return 0;
+ 
+ 	/*
+ 	 * XXX the below is mostly nicked from wake_affine(); we should
+ 	 * see about sharing a bit if at all possible; also it might want
+ 	 * some per entity weight love.
+ 	 */
+ 	weight = p->se.load.weight;
+ 	env.src_stats.load = source_load(env.src_cpu, idx);
+ 	env.src_stats.eff_load = 100 + (imbalance_pct - 100) / 2;
+ 	env.src_stats.eff_load *= power_of(env.src_cpu);
+ 	env.src_stats.eff_load *= env.src_stats.load + effective_load(tg, env.src_cpu, -weight, -weight);
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env.dst_nid)) {
+ 		env.dst_cpu = cpu;
+ 		env.dst_stats.load = target_load(cpu, idx);
+ 
+ 		/* If the CPU is idle, use it */
+ 		if (!env.dst_stats.load) {
+ 			env.best_cpu = cpu;
+ 			goto migrate;
+ 		}
+ 
+ 		/* Otherwise check the target CPU load */
+ 		env.dst_stats.eff_load = 100;
+ 		env.dst_stats.eff_load *= power_of(cpu);
+ 		env.dst_stats.eff_load *= env.dst_stats.load + effective_load(tg, cpu, weight, weight);
+ 
+ 		/*
+ 		 * Destination is considered balanced if the destination CPU is
+ 		 * less loaded than the source CPU. Unfortunately there is a
+ 		 * risk that a task running on a lightly loaded CPU will not
+ 		 * migrate to its preferred node due to load imbalances.
+ 		 */
+ 		balanced = (env.dst_stats.eff_load <= env.src_stats.eff_load);
+ 		if (!balanced)
+ 			continue;
+ 
+ 		if (env.dst_stats.eff_load < env.best_load) {
+ 			env.best_load = env.dst_stats.eff_load;
+ 			env.best_cpu = cpu;
+ 		}
+ 	}
+ 
+ migrate:
+ 	return migrate_task_to(p, env.best_cpu);
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* Success if task is already running on preferred CPU */
+ 	p->numa_migrate_retry = 0;
+ 	if (cpu_to_node(task_cpu(p)) == p->numa_preferred_nid) {
+ 		/*
+ 		 * If migration is temporarily disabled due to a task migration
+ 		 * then re-enable it now as the task is running on its
+ 		 * preferred node and memory should migrate locally
+ 		 */
+ 		if (!p->numa_migrate_seq)
+ 			p->numa_migrate_seq++;
+ 		return;
+ 	}
+ 
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1))
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	if (task_numa_migrate(p) != 0)
+ 		p->numa_migrate_retry = jiffies + HZ*5;
+ }
+ 
++>>>>>>> 06ea5e035b4e (sched/numa: Increment numa_migrate_seq when task runs in correct location)
  static void task_numa_placement(struct task_struct *p)
  {
  	int seq, nid, max_nid = -1;
* Unmerged path kernel/sched/fair.c
