sched/numa: Add infrastructure for split shared/private accounting of NUMA hinting faults

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [kernel] sched/numa: Add infrastructure for split shared/ private accounting of NUMA hinting faults (Rik van Riel) [683513]
Rebuild_FUZZ: 99.44%
commit-author Mel Gorman <mgorman@suse.de>
commit ac8e895bd260cb8bb19ade6a3abd44e7abe9a01d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/ac8e895b.failed

Ideally it would be possible to distinguish between NUMA hinting faults
that are private to a task and those that are shared.  This patch prepares
infrastructure for separately accounting shared and private faults by
allocating the necessary buffers and passing in relevant information. For
now, all faults are treated as private and detection will be introduced
later.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-26-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit ac8e895bd260cb8bb19ade6a3abd44e7abe9a01d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index b231594d7817,89eeb89fd99a..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,6 -877,54 +839,57 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Once a preferred node is selected the scheduler balancer will prefer moving
+  * a task to that node for sysctl_numa_balancing_settle_count number of PTE
+  * scans. This will give the process the chance to accumulate more faults on
+  * the preferred node but still allow the scheduler to move the task again if
+  * the nodes CPUs are overloaded.
+  */
+ unsigned int sysctl_numa_balancing_settle_count __read_mostly = 3;
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ 
+ 
+ static int
+ find_idlest_cpu_node(int this_cpu, int nid)
+ {
+ 	unsigned long load, min_load = ULONG_MAX;
+ 	int i, idlest_cpu = this_cpu;
+ 
+ 	BUG_ON(cpu_to_node(this_cpu) == nid);
+ 
+ 	rcu_read_lock();
+ 	for_each_cpu(i, cpumask_of_node(nid)) {
+ 		load = weighted_cpuload(i);
+ 
+ 		if (load < min_load) {
+ 			min_load = load;
+ 			idlest_cpu = i;
+ 		}
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	return idlest_cpu;
+ }
+ 
++>>>>>>> ac8e895bd260 (sched/numa: Add infrastructure for split shared/private accounting of NUMA hinting faults)
  static void task_numa_placement(struct task_struct *p)
  {
  	int seq, nid, max_nid = -1;
@@@ -4048,6 -4151,69 +4109,72 @@@ task_hot(struct task_struct *p, u64 now
  	return delta < (s64)sysctl_sched_migration_cost;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NUMA_BALANCING
+ /* Returns true if the destination node has incurred more faults */
+ static bool migrate_improves_locality(struct task_struct *p, struct lb_env *env)
+ {
+ 	int src_nid, dst_nid;
+ 
+ 	if (!sched_feat(NUMA_FAVOUR_HIGHER) || !p->numa_faults ||
+ 	    !(env->sd->flags & SD_NUMA)) {
+ 		return false;
+ 	}
+ 
+ 	src_nid = cpu_to_node(env->src_cpu);
+ 	dst_nid = cpu_to_node(env->dst_cpu);
+ 
+ 	if (src_nid == dst_nid ||
+ 	    p->numa_migrate_seq >= sysctl_numa_balancing_settle_count)
+ 		return false;
+ 
+ 	if (dst_nid == p->numa_preferred_nid ||
+ 	    task_faults(p, dst_nid) > task_faults(p, src_nid))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ 
+ static bool migrate_degrades_locality(struct task_struct *p, struct lb_env *env)
+ {
+ 	int src_nid, dst_nid;
+ 
+ 	if (!sched_feat(NUMA) || !sched_feat(NUMA_RESIST_LOWER))
+ 		return false;
+ 
+ 	if (!p->numa_faults || !(env->sd->flags & SD_NUMA))
+ 		return false;
+ 
+ 	src_nid = cpu_to_node(env->src_cpu);
+ 	dst_nid = cpu_to_node(env->dst_cpu);
+ 
+ 	if (src_nid == dst_nid ||
+ 	    p->numa_migrate_seq >= sysctl_numa_balancing_settle_count)
+ 		return false;
+ 
+ 	if (task_faults(p, dst_nid) < task_faults(p, src_nid))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ #else
+ static inline bool migrate_improves_locality(struct task_struct *p,
+ 					     struct lb_env *env)
+ {
+ 	return false;
+ }
+ 
+ static inline bool migrate_degrades_locality(struct task_struct *p,
+ 					     struct lb_env *env)
+ {
+ 	return false;
+ }
+ #endif
+ 
++>>>>>>> ac8e895bd260 (sched/numa: Add infrastructure for split shared/private accounting of NUMA hinting faults)
  /*
   * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
   */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index dcb716d68b2f..30b7a523eb8c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1442,10 +1442,11 @@ struct task_struct {
 #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
 
 #ifdef CONFIG_NUMA_BALANCING
-extern void task_numa_fault(int node, int pages, bool migrated);
+extern void task_numa_fault(int last_node, int node, int pages, bool migrated);
 extern void set_numabalancing_state(bool enabled);
 #else
-static inline void task_numa_fault(int node, int pages, bool migrated)
+static inline void task_numa_fault(int last_node, int node, int pages,
+				   bool migrated)
 {
 }
 static inline void set_numabalancing_state(bool enabled)
* Unmerged path kernel/sched/fair.c
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 02ffbc679b3c..b18c11e5d121 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1294,7 +1294,7 @@ int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	struct page *page;
 	unsigned long haddr = addr & HPAGE_PMD_MASK;
 	int page_nid = -1, this_nid = numa_node_id();
-	int target_nid;
+	int target_nid, last_nid = -1;
 	bool page_locked;
 	bool migrated = false;
 
@@ -1305,6 +1305,7 @@ int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	page = pmd_page(pmd);
 	BUG_ON(is_huge_zero_page(page));
 	page_nid = page_to_nid(page);
+	last_nid = page_nid_last(page);
 	count_vm_numa_event(NUMA_HINT_FAULTS);
 	if (page_nid == this_nid)
 		count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
@@ -1373,7 +1374,7 @@ out:
 		page_unlock_anon_vma_read(anon_vma);
 
 	if (page_nid != -1)
-		task_numa_fault(page_nid, HPAGE_PMD_NR, migrated);
+		task_numa_fault(last_nid, page_nid, HPAGE_PMD_NR, migrated);
 
 	return 0;
 }
diff --git a/mm/memory.c b/mm/memory.c
index b11763dcea8d..8a5b15aa2033 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3533,6 +3533,7 @@ int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	struct page *page = NULL;
 	spinlock_t *ptl;
 	int page_nid = -1;
+	int last_nid;
 	int target_nid;
 	bool migrated = false;
 
@@ -3563,6 +3564,7 @@ int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	}
 	BUG_ON(is_zero_pfn(page_to_pfn(page)));
 
+	last_nid = page_nid_last(page);
 	page_nid = page_to_nid(page);
 	target_nid = numa_migrate_prep(page, vma, addr, page_nid);
 	pte_unmap_unlock(ptep, ptl);
@@ -3578,7 +3580,7 @@ int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 
 out:
 	if (page_nid != -1)
-		task_numa_fault(page_nid, 1, migrated);
+		task_numa_fault(last_nid, page_nid, 1, migrated);
 	return 0;
 }
 
@@ -3593,6 +3595,7 @@ static int do_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	unsigned long offset;
 	spinlock_t *ptl;
 	bool numa = false;
+	int last_nid;
 
 	spin_lock(&mm->page_table_lock);
 	pmd = *pmdp;
@@ -3640,6 +3643,7 @@ static int do_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		if (unlikely(page_mapcount(page) != 1))
 			continue;
 
+		last_nid = page_nid_last(page);
 		page_nid = page_to_nid(page);
 		target_nid = numa_migrate_prep(page, vma, addr, page_nid);
 		pte_unmap_unlock(pte, ptl);
@@ -3652,7 +3656,7 @@ static int do_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		}
 
 		if (page_nid != -1)
-			task_numa_fault(page_nid, 1, migrated);
+			task_numa_fault(last_nid, page_nid, 1, migrated);
 
 		pte = pte_offset_map_lock(mm, pmdp, addr, &ptl);
 	}
