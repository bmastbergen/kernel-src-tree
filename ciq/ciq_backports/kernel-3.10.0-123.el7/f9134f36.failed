perf/x86/intel: Add mem-loads/stores support for Haswell

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [kernel] perf_event_intel: Add mem-loads/stores support for Haswell (Prarit Bhargava) [829878]
Rebuild_FUZZ: 89.47%
commit-author Andi Kleen <ak@linux.intel.com>
commit f9134f36aed59ab55c0ab1a4618dd455f15aef5f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/f9134f36.failed

mem-loads is basically the same as Sandy Bridge,
but we use a separate string for changes later.

Haswell doesn't support the full precise store mode,
so we emulate it using the "DataLA" facility.
This allows to do everything, but for data sources we
can only detect L1 hit or not.

There is no explicit enable bit anymore, so we have
to tie it to a perf internal only flag.

The address is supported for all memory related PEBS
events with DataLA. Instead of only logging for the
load and store events we allow logging it for all
(it will be simply 0 if the current event does not
support it)

	Signed-off-by: Andi Kleen <ak@linux.intel.com>
	Cc: Andi Kleen <ak@linux.jf.intel.com>
	Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
Link: http://lkml.kernel.org/r/1371515812-9646-7-git-send-email-andi@firstfloor.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit f9134f36aed59ab55c0ab1a4618dd455f15aef5f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel_ds.c
diff --cc arch/x86/kernel/cpu/perf_event_intel_ds.c
index 2a63d1307804,ed3e5533ce33..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel_ds.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_ds.c
@@@ -564,6 -577,42 +577,45 @@@ struct event_constraint intel_ivb_pebs_
          EVENT_CONSTRAINT_END
  };
  
++<<<<<<< HEAD
++=======
+ struct event_constraint intel_hsw_pebs_event_constraints[] = {
+ 	INTEL_UEVENT_CONSTRAINT(0x01c0, 0x2), /* INST_RETIRED.PRECDIST */
+ 	INTEL_PST_HSW_CONSTRAINT(0x01c2, 0xf), /* UOPS_RETIRED.ALL */
+ 	INTEL_UEVENT_CONSTRAINT(0x02c2, 0xf), /* UOPS_RETIRED.RETIRE_SLOTS */
+ 	INTEL_EVENT_CONSTRAINT(0xc4, 0xf),    /* BR_INST_RETIRED.* */
+ 	INTEL_UEVENT_CONSTRAINT(0x01c5, 0xf), /* BR_MISP_RETIRED.CONDITIONAL */
+ 	INTEL_UEVENT_CONSTRAINT(0x04c5, 0xf), /* BR_MISP_RETIRED.ALL_BRANCHES */
+ 	INTEL_UEVENT_CONSTRAINT(0x20c5, 0xf), /* BR_MISP_RETIRED.NEAR_TAKEN */
+ 	INTEL_PLD_CONSTRAINT(0x01cd, 0x8),    /* MEM_TRANS_RETIRED.* */
+ 	/* MEM_UOPS_RETIRED.STLB_MISS_LOADS */
+ 	INTEL_UEVENT_CONSTRAINT(0x11d0, 0xf),
+ 	/* MEM_UOPS_RETIRED.STLB_MISS_STORES */
+ 	INTEL_UEVENT_CONSTRAINT(0x12d0, 0xf),
+ 	INTEL_UEVENT_CONSTRAINT(0x21d0, 0xf), /* MEM_UOPS_RETIRED.LOCK_LOADS */
+ 	INTEL_UEVENT_CONSTRAINT(0x41d0, 0xf), /* MEM_UOPS_RETIRED.SPLIT_LOADS */
+ 	/* MEM_UOPS_RETIRED.SPLIT_STORES */
+ 	INTEL_UEVENT_CONSTRAINT(0x42d0, 0xf),
+ 	INTEL_UEVENT_CONSTRAINT(0x81d0, 0xf), /* MEM_UOPS_RETIRED.ALL_LOADS */
+ 	INTEL_PST_HSW_CONSTRAINT(0x82d0, 0xf), /* MEM_UOPS_RETIRED.ALL_STORES */
+ 	INTEL_UEVENT_CONSTRAINT(0x01d1, 0xf), /* MEM_LOAD_UOPS_RETIRED.L1_HIT */
+ 	INTEL_UEVENT_CONSTRAINT(0x02d1, 0xf), /* MEM_LOAD_UOPS_RETIRED.L2_HIT */
+ 	INTEL_UEVENT_CONSTRAINT(0x04d1, 0xf), /* MEM_LOAD_UOPS_RETIRED.L3_HIT */
+ 	/* MEM_LOAD_UOPS_RETIRED.HIT_LFB */
+ 	INTEL_UEVENT_CONSTRAINT(0x40d1, 0xf),
+ 	/* MEM_LOAD_UOPS_LLC_HIT_RETIRED.XSNP_MISS */
+ 	INTEL_UEVENT_CONSTRAINT(0x01d2, 0xf),
+ 	/* MEM_LOAD_UOPS_LLC_HIT_RETIRED.XSNP_HIT */
+ 	INTEL_UEVENT_CONSTRAINT(0x02d2, 0xf),
+ 	/* MEM_LOAD_UOPS_LLC_MISS_RETIRED.LOCAL_DRAM */
+ 	INTEL_UEVENT_CONSTRAINT(0x01d3, 0xf),
+ 	INTEL_UEVENT_CONSTRAINT(0x04c8, 0xf), /* HLE_RETIRED.Abort */
+ 	INTEL_UEVENT_CONSTRAINT(0x04c9, 0xf), /* RTM_RETIRED.Abort */
+ 
+ 	EVENT_CONSTRAINT_END
+ };
+ 
++>>>>>>> f9134f36aed5 (perf/x86/intel: Add mem-loads/stores support for Haswell)
  struct event_constraint *intel_pebs_constraints(struct perf_event *event)
  {
  	struct event_constraint *c;
diff --git a/arch/x86/kernel/cpu/perf_event.h b/arch/x86/kernel/cpu/perf_event.h
index 172e7708e913..42f93dcae539 100644
--- a/arch/x86/kernel/cpu/perf_event.h
+++ b/arch/x86/kernel/cpu/perf_event.h
@@ -67,6 +67,7 @@ struct event_constraint {
  */
 #define PERF_X86_EVENT_PEBS_LDLAT	0x1 /* ld+ldlat data address sampling */
 #define PERF_X86_EVENT_PEBS_ST		0x2 /* st data address sampling */
+#define PERF_X86_EVENT_PEBS_ST_HSW	0x4 /* haswell style st data sampling */
 
 struct amd_nb {
 	int nb_id;  /* NorthBridge id */
@@ -250,6 +251,11 @@ struct cpu_hw_events {
 	__EVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK, \
 			  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_ST)
 
+/* DataLA version of store sampling without extra enable bit. */
+#define INTEL_PST_HSW_CONSTRAINT(c, n)	\
+	__EVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK, \
+			  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_ST_HSW)
+
 #define EVENT_CONSTRAINT_END		\
 	EVENT_CONSTRAINT(0, 0, 0)
 
diff --git a/arch/x86/kernel/cpu/perf_event_intel.c b/arch/x86/kernel/cpu/perf_event_intel.c
index 0034cadcd115..b4dc0bf5ac95 100644
--- a/arch/x86/kernel/cpu/perf_event_intel.c
+++ b/arch/x86/kernel/cpu/perf_event_intel.c
@@ -2031,6 +2031,15 @@ static __init void intel_nehalem_quirk(void)
 	}
 }
 
+EVENT_ATTR_STR(mem-loads,      mem_ld_hsw,     "event=0xcd,umask=0x1,ldlat=3");
+EVENT_ATTR_STR(mem-stores,     mem_st_hsw,     "event=0xd0,umask=0x82")
+
+static struct attribute *hsw_events_attrs[] = {
+	EVENT_PTR(mem_ld_hsw),
+	EVENT_PTR(mem_st_hsw),
+	NULL
+};
+
 __init int intel_pmu_init(void)
 {
 	union cpuid10_edx edx;
@@ -2273,6 +2282,7 @@ __init int intel_pmu_init(void)
 
 		x86_pmu.hw_config = hsw_hw_config;
 		x86_pmu.get_event_constraints = hsw_get_event_constraints;
+		x86_pmu.cpu_events = hsw_events_attrs;
 		pr_cont("Haswell events, ");
 		break;
 
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_ds.c
