sched/numa: Call task_numa_free() from do_execve()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [kernel] sched/numa: Call task_numa_free() from do_execve () (Rik van Riel) [683513]
Rebuild_FUZZ: 99.01%
commit-author Rik van Riel <riel@redhat.com>
commit 82727018b0d33d188e9916bcf76f18387484cb04
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/82727018.failed

It is possible for a task in a numa group to call exec, and
have the new (unrelated) executable inherit the numa group
association from its former self.

This has the potential to break numa grouping, and is trivial
to fix.

	Signed-off-by: Rik van Riel <riel@redhat.com>
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-51-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 82727018b0d33d188e9916bcf76f18387484cb04)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
#	kernel/sched/sched.h
diff --cc kernel/sched/fair.c
index 98cee68da024,c4df2de6ca4a..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -866,11 -1272,171 +866,152 @@@ static void task_numa_placement(struct 
  			max_faults = faults;
  			max_nid = nid;
  		}
 -
 -		if (group_faults > max_group_faults) {
 -			max_group_faults = group_faults;
 -			max_group_nid = nid;
 -		}
  	}
  
 -	/*
 -	 * If the preferred task and group nids are different,
 -	 * iterate over the nodes again to find the best place.
 -	 */
 -	if (p->numa_group && max_nid != max_group_nid) {
 -		unsigned long weight, max_weight = 0;
 -
 -		for_each_online_node(nid) {
 -			weight = task_weight(p, nid) + group_weight(p, nid);
 -			if (weight > max_weight) {
 -				max_weight = weight;
 -				max_nid = nid;
 -			}
 -		}
 -	}
 -
 -	/* Preferred node as the node with the most faults */
 -	if (max_faults && max_nid != p->numa_preferred_nid) {
 -		/* Update the preferred nid and migrate task if possible */
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
  		p->numa_preferred_nid = max_nid;
++<<<<<<< HEAD
++=======
+ 		p->numa_migrate_seq = 1;
+ 		numa_migrate_preferred(p);
+ 	}
+ }
+ 
+ static inline int get_numa_group(struct numa_group *grp)
+ {
+ 	return atomic_inc_not_zero(&grp->refcount);
+ }
+ 
+ static inline void put_numa_group(struct numa_group *grp)
+ {
+ 	if (atomic_dec_and_test(&grp->refcount))
+ 		kfree_rcu(grp, rcu);
+ }
+ 
+ static void double_lock(spinlock_t *l1, spinlock_t *l2)
+ {
+ 	if (l1 > l2)
+ 		swap(l1, l2);
+ 
+ 	spin_lock(l1);
+ 	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+ }
+ 
+ static void task_numa_group(struct task_struct *p, int cpupid)
+ {
+ 	struct numa_group *grp, *my_grp;
+ 	struct task_struct *tsk;
+ 	bool join = false;
+ 	int cpu = cpupid_to_cpu(cpupid);
+ 	int i;
+ 
+ 	if (unlikely(!p->numa_group)) {
+ 		unsigned int size = sizeof(struct numa_group) +
+ 				    2*nr_node_ids*sizeof(atomic_long_t);
+ 
+ 		grp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);
+ 		if (!grp)
+ 			return;
+ 
+ 		atomic_set(&grp->refcount, 1);
+ 		spin_lock_init(&grp->lock);
+ 		INIT_LIST_HEAD(&grp->task_list);
+ 		grp->gid = p->pid;
+ 
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_set(&grp->faults[i], p->numa_faults[i]);
+ 
+ 		atomic_long_set(&grp->total_faults, p->total_numa_faults);
+ 
+ 		list_add(&p->numa_entry, &grp->task_list);
+ 		grp->nr_tasks++;
+ 		rcu_assign_pointer(p->numa_group, grp);
+ 	}
+ 
+ 	rcu_read_lock();
+ 	tsk = ACCESS_ONCE(cpu_rq(cpu)->curr);
+ 
+ 	if (!cpupid_match_pid(tsk, cpupid))
+ 		goto unlock;
+ 
+ 	grp = rcu_dereference(tsk->numa_group);
+ 	if (!grp)
+ 		goto unlock;
+ 
+ 	my_grp = p->numa_group;
+ 	if (grp == my_grp)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Only join the other group if its bigger; if we're the bigger group,
+ 	 * the other task will join us.
+ 	 */
+ 	if (my_grp->nr_tasks > grp->nr_tasks)
+ 		goto unlock;
+ 
+ 	/*
+ 	 * Tie-break on the grp address.
+ 	 */
+ 	if (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)
+ 		goto unlock;
+ 
+ 	if (!get_numa_group(grp))
+ 		goto unlock;
+ 
+ 	join = true;
+ 
+ unlock:
+ 	rcu_read_unlock();
+ 
+ 	if (!join)
+ 		return;
+ 
+ 	for (i = 0; i < 2*nr_node_ids; i++) {
+ 		atomic_long_sub(p->numa_faults[i], &my_grp->faults[i]);
+ 		atomic_long_add(p->numa_faults[i], &grp->faults[i]);
+ 	}
+ 	atomic_long_sub(p->total_numa_faults, &my_grp->total_faults);
+ 	atomic_long_add(p->total_numa_faults, &grp->total_faults);
+ 
+ 	double_lock(&my_grp->lock, &grp->lock);
+ 
+ 	list_move(&p->numa_entry, &grp->task_list);
+ 	my_grp->nr_tasks--;
+ 	grp->nr_tasks++;
+ 
+ 	spin_unlock(&my_grp->lock);
+ 	spin_unlock(&grp->lock);
+ 
+ 	rcu_assign_pointer(p->numa_group, grp);
+ 
+ 	put_numa_group(my_grp);
+ }
+ 
+ void task_numa_free(struct task_struct *p)
+ {
+ 	struct numa_group *grp = p->numa_group;
+ 	int i;
+ 	void *numa_faults = p->numa_faults;
+ 
+ 	if (grp) {
+ 		for (i = 0; i < 2*nr_node_ids; i++)
+ 			atomic_long_sub(p->numa_faults[i], &grp->faults[i]);
+ 
+ 		atomic_long_sub(p->total_numa_faults, &grp->total_faults);
+ 
+ 		spin_lock(&grp->lock);
+ 		list_del(&p->numa_entry);
+ 		grp->nr_tasks--;
+ 		spin_unlock(&grp->lock);
+ 		rcu_assign_pointer(p->numa_group, NULL);
+ 		put_numa_group(grp);
+ 	}
+ 
+ 	p->numa_faults = NULL;
+ 	p->numa_faults_buffer = NULL;
+ 	kfree(numa_faults);
++>>>>>>> 82727018b0d3 (sched/numa: Call task_numa_free() from do_execve())
  }
  
  /*
@@@ -883,9 -1451,17 +1024,20 @@@ void task_numa_fault(int node, int page
  	if (!numabalancing_enabled)
  		return;
  
++<<<<<<< HEAD
++=======
+ 	/* for example, ksmd faulting in a user's mm */
+ 	if (!p->mm)
+ 		return;
+ 
+ 	/* Do not worry about placement if exiting */
+ 	if (p->state == TASK_DEAD)
+ 		return;
+ 
++>>>>>>> 82727018b0d3 (sched/numa: Call task_numa_free() from do_execve())
  	/* Allocate buffer to track faults on a per-node basis */
  	if (unlikely(!p->numa_faults)) {
 -		int size = sizeof(*p->numa_faults) * 2 * nr_node_ids;
 +		int size = sizeof(*p->numa_faults) * nr_node_ids;
  
  		/* numa_faults and numa_faults_buffer share the allocation */
  		p->numa_faults = kzalloc(size * 2, GFP_KERNEL|__GFP_NOWARN);
diff --cc kernel/sched/sched.h
index 5abdf358588f,eeb1923812a1..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -552,14 -557,8 +552,19 @@@ static inline u64 rq_clock_task(struct 
  }
  
  #ifdef CONFIG_NUMA_BALANCING
++<<<<<<< HEAD
 +static inline void task_numa_free(struct task_struct *p)
 +{
 +	kfree(p->numa_faults);
 +}
 +#else /* CONFIG_NUMA_BALANCING */
 +static inline void task_numa_free(struct task_struct *p)
 +{
 +}
++=======
+ extern int migrate_task_to(struct task_struct *p, int cpu);
+ extern int migrate_swap(struct task_struct *, struct task_struct *);
++>>>>>>> 82727018b0d3 (sched/numa: Call task_numa_free() from do_execve())
  #endif /* CONFIG_NUMA_BALANCING */
  
  #ifdef CONFIG_SMP
diff --git a/fs/exec.c b/fs/exec.c
index ffd7a813ad3d..a6da73b68f94 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -1548,6 +1548,7 @@ static int do_execve_common(const char *filename,
 	current->fs->in_exec = 0;
 	current->in_execve = 0;
 	acct_update_integrals(current);
+	task_numa_free(current);
 	free_bprm(bprm);
 	if (displaced)
 		put_files_struct(displaced);
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 664a046b12fb..76f4022cf676 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1444,6 +1444,7 @@ struct task_struct {
 #ifdef CONFIG_NUMA_BALANCING
 extern void task_numa_fault(int node, int pages, bool migrated);
 extern void set_numabalancing_state(bool enabled);
+extern void task_numa_free(struct task_struct *p);
 #else
 static inline void task_numa_fault(int node, int pages, bool migrated)
 {
@@ -1451,6 +1452,9 @@ static inline void task_numa_fault(int node, int pages, bool migrated)
 static inline void set_numabalancing_state(bool enabled)
 {
 }
+static inline void task_numa_free(struct task_struct *p)
+{
+}
 #endif
 
 static inline struct pid *task_pid(struct task_struct *task)
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/sched.h
