powerpc: Provide for giveup_fpu/altivec to save state in alternate location

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [powerpc] Provide for giveup_fpu/altivec to save state in alternate location (Steve Best) [1052083]
Rebuild_FUZZ: 93.62%
commit-author Paul Mackerras <paulus@samba.org>
commit 18461960cbf50bf345ef0667d45d5f64de8fb893
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/18461960.failed

This provides a facility which is intended for use by KVM, where the
contents of the FP/VSX and VMX (Altivec) registers can be saved away
to somewhere other than the thread_struct when kernel code wants to
use floating point or VMX instructions.  This is done by providing a
pointer in the thread_struct to indicate where the state should be
saved to.  The giveup_fpu() and giveup_altivec() functions test these
pointers and save state to the indicated location if they are non-NULL.
Note that the MSR_FP/VEC bits in task->thread.regs->msr are still used
to indicate whether the CPU register state is live, even when an
alternate save location is being used.

This also provides load_fp_state() and load_vr_state() functions, which
load up FP/VSX and VMX state from memory into the CPU registers, and
corresponding store_fp_state() and store_vr_state() functions, which
store FP/VSX and VMX state into memory from the CPU registers.

	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
(cherry picked from commit 18461960cbf50bf345ef0667d45d5f64de8fb893)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/processor.h
#	arch/powerpc/kernel/asm-offsets.c
#	arch/powerpc/kernel/fpu.S
#	arch/powerpc/kernel/ppc_ksyms.c
#	arch/powerpc/kernel/process.c
#	arch/powerpc/kernel/vector.S
diff --cc arch/powerpc/include/asm/processor.h
index 419e7125cce2,ea88e7bd4a34..000000000000
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@@ -199,13 -210,8 +199,18 @@@ struct thread_struct 
  	unsigned long	dvc2;
  #endif
  #endif
++<<<<<<< HEAD
 +	/* FP and VSX 0-31 register set */
 +	double		fpr[32][TS_FPRWIDTH];
 +	struct {
 +
 +		unsigned int pad;
 +		unsigned int val;	/* Floating point status */
 +	} fpscr;
++=======
+ 	struct thread_fp_state	fp_state;
+ 	struct thread_fp_state	*fp_save_area;
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
  	int		fpexc_mode;	/* floating-point exception mode */
  	unsigned int	align_ctl;	/* alignment handling control */
  #ifdef CONFIG_PPC64
@@@ -223,10 -229,8 +228,15 @@@
  	struct arch_hw_breakpoint hw_brk; /* info on the hardware breakpoint */
  	unsigned long	trap_nr;	/* last trap # on this thread */
  #ifdef CONFIG_ALTIVEC
++<<<<<<< HEAD
 +	/* Complete AltiVec register set */
 +	vector128	vr[32] __attribute__((aligned(16)));
 +	/* AltiVec status */
 +	vector128	vscr __attribute__((aligned(16)));
++=======
+ 	struct thread_vr_state vr_state;
+ 	struct thread_vr_state *vr_save_area;
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
  	unsigned long	vrsave;
  	int		used_vr;	/* set if process has used altivec */
  #endif /* CONFIG_ALTIVEC */
diff --cc arch/powerpc/kernel/asm-offsets.c
index 302886b77de2,6278edddc3f8..000000000000
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@@ -89,16 -90,17 +89,25 @@@ int main(void
  	DEFINE(THREAD_NORMSAVES, offsetof(struct thread_struct, normsave[0]));
  #endif
  	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
++<<<<<<< HEAD
 +	DEFINE(THREAD_FPR0, offsetof(struct thread_struct, fpr[0]));
 +	DEFINE(THREAD_FPSCR, offsetof(struct thread_struct, fpscr));
 +#ifdef CONFIG_ALTIVEC
 +	DEFINE(THREAD_VR0, offsetof(struct thread_struct, vr[0]));
++=======
+ 	DEFINE(THREAD_FPSTATE, offsetof(struct thread_struct, fp_state));
+ 	DEFINE(THREAD_FPSAVEAREA, offsetof(struct thread_struct, fp_save_area));
+ 	DEFINE(FPSTATE_FPSCR, offsetof(struct thread_fp_state, fpscr));
+ #ifdef CONFIG_ALTIVEC
+ 	DEFINE(THREAD_VRSTATE, offsetof(struct thread_struct, vr_state));
+ 	DEFINE(THREAD_VRSAVEAREA, offsetof(struct thread_struct, vr_save_area));
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
  	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
 +	DEFINE(THREAD_VSCR, offsetof(struct thread_struct, vscr));
  	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
 -	DEFINE(VRSTATE_VSCR, offsetof(struct thread_vr_state, vscr));
  #endif /* CONFIG_ALTIVEC */
  #ifdef CONFIG_VSX
 +	DEFINE(THREAD_VSR0, offsetof(struct thread_struct, fpr));
  	DEFINE(THREAD_USED_VSR, offsetof(struct thread_struct, used_vsr));
  #endif /* CONFIG_VSX */
  #ifdef CONFIG_PPC64
diff --cc arch/powerpc/kernel/fpu.S
index caeaabf11a2f,4dca05e91e95..000000000000
--- a/arch/powerpc/kernel/fpu.S
+++ b/arch/powerpc/kernel/fpu.S
@@@ -206,11 -192,15 +226,20 @@@ END_FTR_SECTION_IFSET(CPU_FTR_VSX
  	PPC_LCMPI	0,r3,0
  	beqlr-				/* if no previous owner, done */
  	addi	r3,r3,THREAD	        /* want THREAD of task */
+ 	PPC_LL	r6,THREAD_FPSAVEAREA(r3)
  	PPC_LL	r5,PT_REGS(r3)
++<<<<<<< HEAD
 +	PPC_LCMPI	0,r5,0
 +	SAVE_32FPVSRS(0, R4 ,R3)
++=======
+ 	PPC_LCMPI	0,r6,0
+ 	bne	2f
+ 	addi	r6,r3,THREAD_FPSTATE
+ 2:	PPC_LCMPI	0,r5,0
+ 	SAVE_32FPVSRS(0, R4, R6)
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
  	mffs	fr0
 -	stfd	fr0,FPSTATE_FPSCR(r6)
 +	stfd	fr0,THREAD_FPSCR(r3)
  	beq	1f
  	PPC_LL	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
  	li	r3,MSR_FP|MSR_FE0|MSR_FE1
diff --cc arch/powerpc/kernel/ppc_ksyms.c
index c29666586998,56a4bec1b11a..000000000000
--- a/arch/powerpc/kernel/ppc_ksyms.c
+++ b/arch/powerpc/kernel/ppc_ksyms.c
@@@ -96,9 -96,15 +96,17 @@@ EXPORT_SYMBOL(pci_dram_offset)
  
  EXPORT_SYMBOL(start_thread);
  
 -#ifdef CONFIG_PPC_FPU
  EXPORT_SYMBOL(giveup_fpu);
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL(load_fp_state);
+ EXPORT_SYMBOL(store_fp_state);
+ #endif
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
  #ifdef CONFIG_ALTIVEC
  EXPORT_SYMBOL(giveup_altivec);
+ EXPORT_SYMBOL(load_vr_state);
+ EXPORT_SYMBOL(store_vr_state);
  #endif /* CONFIG_ALTIVEC */
  #ifdef CONFIG_VSX
  EXPORT_SYMBOL(giveup_vsx);
diff --cc arch/powerpc/kernel/process.c
index 076d1242507a,8649a3d629e1..000000000000
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@@ -1096,12 -1118,12 +1101,21 @@@ void start_thread(struct pt_regs *regs
  #ifdef CONFIG_VSX
  	current->thread.used_vsr = 0;
  #endif
++<<<<<<< HEAD
 +	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
 +	current->thread.fpscr.val = 0;
 +#ifdef CONFIG_ALTIVEC
 +	memset(current->thread.vr, 0, sizeof(current->thread.vr));
 +	memset(&current->thread.vscr, 0, sizeof(current->thread.vscr));
 +	current->thread.vscr.u[3] = 0x00010000; /* Java mode disabled */
++=======
+ 	memset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));
+ 	current->thread.fp_save_area = NULL;
+ #ifdef CONFIG_ALTIVEC
+ 	memset(&current->thread.vr_state, 0, sizeof(current->thread.vr_state));
+ 	current->thread.vr_state.vscr.u[3] = 0x00010000; /* Java mode disabled */
+ 	current->thread.vr_save_area = NULL;
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
  	current->thread.vrsave = 0;
  	current->thread.used_vr = 0;
  #endif /* CONFIG_ALTIVEC */
diff --cc arch/powerpc/kernel/vector.S
index 9e20999aaef2,eacda4eea2d7..000000000000
--- a/arch/powerpc/kernel/vector.S
+++ b/arch/powerpc/kernel/vector.S
@@@ -59,7 -37,28 +59,32 @@@ _GLOBAL(do_load_up_transact_altivec
  #endif
  
  /*
++<<<<<<< HEAD
 + * load_up_altivec(unused, unused, tsk)
++=======
+  * Load state from memory into VMX registers including VSCR.
+  * Assumes the caller has enabled VMX in the MSR.
+  */
+ _GLOBAL(load_vr_state)
+ 	li	r4,VRSTATE_VSCR
+ 	lvx	vr0,r4,r3
+ 	mtvscr	vr0
+ 	REST_32VRS(0,r4,r3)
+ 	blr
+ 
+ /*
+  * Store VMX state into memory, including VSCR.
+  * Assumes the caller has enabled VMX in the MSR.
+  */
+ _GLOBAL(store_vr_state)
+ 	SAVE_32VRS(0, r4, r3)
+ 	mfvscr	vr0
+ 	li	r4, VRSTATE_VSCR
+ 	stvx	vr0, r4, r3
+ 	blr
+ 
+ /*
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
   * Disable VMX for the task which had it previously,
   * and save its vector registers in its thread_struct.
   * Enables the VMX for use in the kernel on return.
@@@ -165,12 -166,16 +190,22 @@@ _GLOBAL(giveup_altivec
  	PPC_LCMPI	0,r3,0
  	beqlr				/* if no previous owner, done */
  	addi	r3,r3,THREAD		/* want THREAD of task */
++<<<<<<< HEAD
 +	PPC_LL	r5,PT_REGS(r3)
 +	PPC_LCMPI	0,r5,0
 +	SAVE_32VRS(0,r4,r3)
++=======
+ 	PPC_LL	r7,THREAD_VRSAVEAREA(r3)
+ 	PPC_LL	r5,PT_REGS(r3)
+ 	PPC_LCMPI	0,r7,0
+ 	bne	2f
+ 	addi	r7,r3,THREAD_VRSTATE
+ 2:	PPC_LCMPI	0,r5,0
+ 	SAVE_32VRS(0,r4,r7)
++>>>>>>> 18461960cbf5 (powerpc: Provide for giveup_fpu/altivec to save state in alternate location)
  	mfvscr	vr0
 -	li	r4,VRSTATE_VSCR
 -	stvx	vr0,r4,r7
 +	li	r4,THREAD_VSCR
 +	stvx	vr0,r4,r3
  	beq	1f
  	PPC_LL	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
  #ifdef CONFIG_VSX
* Unmerged path arch/powerpc/include/asm/processor.h
* Unmerged path arch/powerpc/kernel/asm-offsets.c
* Unmerged path arch/powerpc/kernel/fpu.S
* Unmerged path arch/powerpc/kernel/ppc_ksyms.c
* Unmerged path arch/powerpc/kernel/process.c
* Unmerged path arch/powerpc/kernel/vector.S
