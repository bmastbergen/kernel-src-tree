mm + fs: prepare for non-page entries in page cache radix trees

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] prepare for non-page entries in page cache radix trees (Johannes Weiner) [1062372]
Rebuild_FUZZ: 92.31%
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 0cd6144aadd2afd19d1aca880153530c52957604
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/0cd6144a.failed

shmem mappings already contain exceptional entries where swap slot
information is remembered.

To be able to store eviction information for regular page cache, prepare
every site dealing with the radix trees directly to handle entries other
than pages.

The common lookup functions will filter out non-page entries and return
NULL for page cache holes, just as before.  But provide a raw version of
the API which returns non-page entries as well, and switch shmem over to
use it.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Reviewed-by: Minchan Kim <minchan@kernel.org>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Bob Liu <bob.liu@oracle.com>
	Cc: Christoph Hellwig <hch@infradead.org>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Greg Thelen <gthelen@google.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jan Kara <jack@suse.cz>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Cc: Luigi Semenzato <semenzato@google.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Metin Doslu <metin@citusdata.com>
	Cc: Michel Lespinasse <walken@google.com>
	Cc: Ozgun Erdogan <ozgun@citusdata.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Roman Gushchin <klamm@yandex-team.ru>
	Cc: Ryan Mallon <rmallon@gmail.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0cd6144aadd2afd19d1aca880153530c52957604)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/filemap.c
#	mm/truncate.c
diff --cc mm/filemap.c
index 844f5d8d25f6,efc63876477f..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -467,32 -490,33 +490,57 @@@ int add_to_page_cache_locked(struct pag
  	error = mem_cgroup_cache_charge(page, current->mm,
  					gfp_mask & GFP_RECLAIM_MASK);
  	if (error)
 -		return error;
 +		goto out;
 +
 +	error = radix_tree_preload(gfp_mask & ~__GFP_HIGHMEM);
 +	if (error == 0) {
 +		page_cache_get(page);
 +		page->mapping = mapping;
 +		page->index = offset;
  
 -	error = radix_tree_maybe_preload(gfp_mask & ~__GFP_HIGHMEM);
 -	if (error) {
 +		spin_lock_irq(&mapping->tree_lock);
 +		error = radix_tree_insert(&mapping->page_tree, offset, page);
 +		if (likely(!error)) {
 +			mapping->nrpages++;
 +			__inc_zone_page_state(page, NR_FILE_PAGES);
 +			spin_unlock_irq(&mapping->tree_lock);
 +			trace_mm_filemap_add_to_page_cache(page);
 +		} else {
 +			page->mapping = NULL;
 +			/* Leave page->index set: truncation relies upon it */
 +			spin_unlock_irq(&mapping->tree_lock);
 +			mem_cgroup_uncharge_cache_page(page);
 +			page_cache_release(page);
 +		}
 +		radix_tree_preload_end();
 +	} else
  		mem_cgroup_uncharge_cache_page(page);
++<<<<<<< HEAD
 +out:
++=======
+ 		return error;
+ 	}
+ 
+ 	page_cache_get(page);
+ 	page->mapping = mapping;
+ 	page->index = offset;
+ 
+ 	spin_lock_irq(&mapping->tree_lock);
+ 	error = page_cache_tree_insert(mapping, page);
+ 	radix_tree_preload_end();
+ 	if (unlikely(error))
+ 		goto err_insert;
+ 	__inc_zone_page_state(page, NR_FILE_PAGES);
+ 	spin_unlock_irq(&mapping->tree_lock);
+ 	trace_mm_filemap_add_to_page_cache(page);
+ 	return 0;
+ err_insert:
+ 	page->mapping = NULL;
+ 	/* Leave page->index set: truncation relies upon it */
+ 	spin_unlock_irq(&mapping->tree_lock);
+ 	mem_cgroup_uncharge_cache_page(page);
+ 	page_cache_release(page);
++>>>>>>> 0cd6144aadd2 (mm + fs: prepare for non-page entries in page cache radix trees)
  	return error;
  }
  EXPORT_SYMBOL(add_to_page_cache_locked);
diff --cc mm/truncate.c
index c75b736e54b7,2e84fe59190b..000000000000
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@@ -204,12 -219,14 +220,23 @@@ int invalidate_inode_page(struct page *
  void truncate_inode_pages_range(struct address_space *mapping,
  				loff_t lstart, loff_t lend)
  {
++<<<<<<< HEAD
 +	const pgoff_t start = (lstart + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;
 +	const unsigned partial = lstart & (PAGE_CACHE_SIZE - 1);
 +	struct pagevec pvec;
 +	pgoff_t index;
 +	pgoff_t end;
 +	int i;
++=======
+ 	pgoff_t		start;		/* inclusive */
+ 	pgoff_t		end;		/* exclusive */
+ 	unsigned int	partial_start;	/* inclusive */
+ 	unsigned int	partial_end;	/* exclusive */
+ 	struct pagevec	pvec;
+ 	pgoff_t		indices[PAGEVEC_SIZE];
+ 	pgoff_t		index;
+ 	int		i;
++>>>>>>> 0cd6144aadd2 (mm + fs: prepare for non-page entries in page cache radix trees)
  
  	cleancache_invalidate_inode(mapping);
  	if (mapping->nrpages == 0)
@@@ -220,17 -255,23 +247,33 @@@
  
  	pagevec_init(&pvec, 0);
  	index = start;
++<<<<<<< HEAD
 +	while (index <= end && pagevec_lookup(&pvec, mapping, index,
 +			min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {
++=======
+ 	while (index < end && pagevec_lookup_entries(&pvec, mapping, index,
+ 			min(end - index, (pgoff_t)PAGEVEC_SIZE),
+ 			indices)) {
++>>>>>>> 0cd6144aadd2 (mm + fs: prepare for non-page entries in page cache radix trees)
  		mem_cgroup_uncharge_start();
  		for (i = 0; i < pagevec_count(&pvec); i++) {
  			struct page *page = pvec.pages[i];
  
  			/* We rely upon deletion not changing page->index */
++<<<<<<< HEAD
 +			index = page->index;
 +			if (index > end)
++=======
+ 			index = indices[i];
+ 			if (index >= end)
++>>>>>>> 0cd6144aadd2 (mm + fs: prepare for non-page entries in page cache radix trees)
  				break;
  
+ 			if (radix_tree_exceptional_entry(page)) {
+ 				clear_exceptional_entry(mapping, index, page);
+ 				continue;
+ 			}
+ 
  			if (!trylock_page(page))
  				continue;
  			WARN_ON(page->index != index);
@@@ -260,14 -331,16 +304,25 @@@
  	index = start;
  	for ( ; ; ) {
  		cond_resched();
++<<<<<<< HEAD
 +		if (!pagevec_lookup(&pvec, mapping, index,
 +			min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {
++=======
+ 		if (!pagevec_lookup_entries(&pvec, mapping, index,
+ 			min(end - index, (pgoff_t)PAGEVEC_SIZE),
+ 			indices)) {
++>>>>>>> 0cd6144aadd2 (mm + fs: prepare for non-page entries in page cache radix trees)
  			if (index == start)
  				break;
  			index = start;
  			continue;
  		}
++<<<<<<< HEAD
 +		if (index == start && pvec.pages[0]->index > end) {
++=======
+ 		if (index == start && indices[0] >= end) {
+ 			pagevec_remove_exceptionals(&pvec);
++>>>>>>> 0cd6144aadd2 (mm + fs: prepare for non-page entries in page cache radix trees)
  			pagevec_release(&pvec);
  			break;
  		}
@@@ -276,10 -349,15 +331,20 @@@
  			struct page *page = pvec.pages[i];
  
  			/* We rely upon deletion not changing page->index */
++<<<<<<< HEAD
 +			index = page->index;
 +			if (index > end)
++=======
+ 			index = indices[i];
+ 			if (index >= end)
++>>>>>>> 0cd6144aadd2 (mm + fs: prepare for non-page entries in page cache radix trees)
  				break;
  
+ 			if (radix_tree_exceptional_entry(page)) {
+ 				clear_exceptional_entry(mapping, index, page);
+ 				continue;
+ 			}
+ 
  			lock_page(page);
  			WARN_ON(page->index != index);
  			wait_on_page_writeback(page);
diff --git a/fs/btrfs/compression.c b/fs/btrfs/compression.c
index 6aad98cb343f..c88316587900 100644
--- a/fs/btrfs/compression.c
+++ b/fs/btrfs/compression.c
@@ -474,7 +474,7 @@ static noinline int add_ra_bio_pages(struct inode *inode,
 		rcu_read_lock();
 		page = radix_tree_lookup(&mapping->page_tree, pg_index);
 		rcu_read_unlock();
-		if (page) {
+		if (page && !radix_tree_exceptional_entry(page)) {
 			misses++;
 			if (misses > 4)
 				break;
diff --git a/include/linux/mm.h b/include/linux/mm.h
index c562a7f66a87..4275499d760f 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -895,6 +895,14 @@ extern void show_free_areas(unsigned int flags);
 extern bool skip_free_areas_node(unsigned int flags, int nid);
 
 int shmem_zero_setup(struct vm_area_struct *);
+#ifdef CONFIG_SHMEM
+bool shmem_mapping(struct address_space *mapping);
+#else
+static inline bool shmem_mapping(struct address_space *mapping)
+{
+	return false;
+}
+#endif
 
 extern int can_do_mlock(void);
 extern int user_shm_lock(size_t, struct user_struct *);
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index c73130c607c4..76aaeaad37be 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -248,12 +248,15 @@ pgoff_t page_cache_next_hole(struct address_space *mapping,
 pgoff_t page_cache_prev_hole(struct address_space *mapping,
 			     pgoff_t index, unsigned long max_scan);
 
-extern struct page * find_get_page(struct address_space *mapping,
-				pgoff_t index);
-extern struct page * find_lock_page(struct address_space *mapping,
-				pgoff_t index);
-extern struct page * find_or_create_page(struct address_space *mapping,
-				pgoff_t index, gfp_t gfp_mask);
+struct page *find_get_entry(struct address_space *mapping, pgoff_t offset);
+struct page *find_get_page(struct address_space *mapping, pgoff_t offset);
+struct page *find_lock_entry(struct address_space *mapping, pgoff_t offset);
+struct page *find_lock_page(struct address_space *mapping, pgoff_t offset);
+struct page *find_or_create_page(struct address_space *mapping, pgoff_t index,
+				 gfp_t gfp_mask);
+unsigned find_get_entries(struct address_space *mapping, pgoff_t start,
+			  unsigned int nr_entries, struct page **entries,
+			  pgoff_t *indices);
 unsigned find_get_pages(struct address_space *mapping, pgoff_t start,
 			unsigned int nr_pages, struct page **pages);
 unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t start,
diff --git a/include/linux/pagevec.h b/include/linux/pagevec.h
index e4dbfab37729..b45d391b4540 100644
--- a/include/linux/pagevec.h
+++ b/include/linux/pagevec.h
@@ -22,6 +22,11 @@ struct pagevec {
 
 void __pagevec_release(struct pagevec *pvec);
 void __pagevec_lru_add(struct pagevec *pvec);
+unsigned pagevec_lookup_entries(struct pagevec *pvec,
+				struct address_space *mapping,
+				pgoff_t start, unsigned nr_entries,
+				pgoff_t *indices);
+void pagevec_remove_exceptionals(struct pagevec *pvec);
 unsigned pagevec_lookup(struct pagevec *pvec, struct address_space *mapping,
 		pgoff_t start, unsigned nr_pages);
 unsigned pagevec_lookup_tag(struct pagevec *pvec,
diff --git a/include/linux/shmem_fs.h b/include/linux/shmem_fs.h
index 30aa0dc60d75..deb49609cd36 100644
--- a/include/linux/shmem_fs.h
+++ b/include/linux/shmem_fs.h
@@ -49,6 +49,7 @@ extern struct file *shmem_file_setup(const char *name,
 					loff_t size, unsigned long flags);
 extern int shmem_zero_setup(struct vm_area_struct *);
 extern int shmem_lock(struct file *file, int lock, struct user_struct *user);
+extern bool shmem_mapping(struct address_space *mapping);
 extern void shmem_unlock_mapping(struct address_space *mapping);
 extern struct page *shmem_read_mapping_page_gfp(struct address_space *mapping,
 					pgoff_t index, gfp_t gfp_mask);
* Unmerged path mm/filemap.c
diff --git a/mm/mincore.c b/mm/mincore.c
index da2be56a7b8f..06cb81005c77 100644
--- a/mm/mincore.c
+++ b/mm/mincore.c
@@ -70,13 +70,21 @@ static unsigned char mincore_page(struct address_space *mapping, pgoff_t pgoff)
 	 * any other file mapping (ie. marked !present and faulted in with
 	 * tmpfs's .fault). So swapped out tmpfs mappings are tested here.
 	 */
-	page = find_get_page(mapping, pgoff);
 #ifdef CONFIG_SWAP
-	/* shmem/tmpfs may return swap: account for swapcache page too. */
-	if (radix_tree_exceptional_entry(page)) {
-		swp_entry_t swap = radix_to_swp_entry(page);
-		page = find_get_page(swap_address_space(swap), swap.val);
-	}
+	if (shmem_mapping(mapping)) {
+		page = find_get_entry(mapping, pgoff);
+		/*
+		 * shmem/tmpfs may return swap: account for swapcache
+		 * page too.
+		 */
+		if (radix_tree_exceptional_entry(page)) {
+			swp_entry_t swp = radix_to_swp_entry(page);
+			page = find_get_page(swap_address_space(swp), swp.val);
+		}
+	} else
+		page = find_get_page(mapping, pgoff);
+#else
+	page = find_get_page(mapping, pgoff);
 #endif
 	if (page) {
 		present = PageUptodate(page);
diff --git a/mm/readahead.c b/mm/readahead.c
index a5ae672dceba..26aa987cdac7 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -179,7 +179,7 @@ __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 		rcu_read_lock();
 		page = radix_tree_lookup(&mapping->page_tree, page_offset);
 		rcu_read_unlock();
-		if (page)
+		if (page && !radix_tree_exceptional_entry(page))
 			continue;
 
 		page = page_cache_alloc_readahead(mapping);
diff --git a/mm/shmem.c b/mm/shmem.c
index 5e6a8422658b..aba3f0a2532f 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -330,56 +330,6 @@ static void shmem_delete_from_page_cache(struct page *page, void *radswap)
 	BUG_ON(error);
 }
 
-/*
- * Like find_get_pages, but collecting swap entries as well as pages.
- */
-static unsigned shmem_find_get_pages_and_swap(struct address_space *mapping,
-					pgoff_t start, unsigned int nr_pages,
-					struct page **pages, pgoff_t *indices)
-{
-	void **slot;
-	unsigned int ret = 0;
-	struct radix_tree_iter iter;
-
-	if (!nr_pages)
-		return 0;
-
-	rcu_read_lock();
-restart:
-	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
-		struct page *page;
-repeat:
-		page = radix_tree_deref_slot(slot);
-		if (unlikely(!page))
-			continue;
-		if (radix_tree_exception(page)) {
-			if (radix_tree_deref_retry(page))
-				goto restart;
-			/*
-			 * Otherwise, we must be storing a swap entry
-			 * here as an exceptional entry: so return it
-			 * without attempting to raise page count.
-			 */
-			goto export;
-		}
-		if (!page_cache_get_speculative(page))
-			goto repeat;
-
-		/* Has the page moved? */
-		if (unlikely(page != *slot)) {
-			page_cache_release(page);
-			goto repeat;
-		}
-export:
-		indices[ret] = iter.index;
-		pages[ret] = page;
-		if (++ret == nr_pages)
-			break;
-	}
-	rcu_read_unlock();
-	return ret;
-}
-
 /*
  * Remove swap entry from radix tree, free the swap and its page cache.
  */
@@ -396,21 +346,6 @@ static int shmem_free_swap(struct address_space *mapping,
 	return error;
 }
 
-/*
- * Pagevec may contain swap entries, so shuffle up pages before releasing.
- */
-static void shmem_deswap_pagevec(struct pagevec *pvec)
-{
-	int i, j;
-
-	for (i = 0, j = 0; i < pagevec_count(pvec); i++) {
-		struct page *page = pvec->pages[i];
-		if (!radix_tree_exceptional_entry(page))
-			pvec->pages[j++] = page;
-	}
-	pvec->nr = j;
-}
-
 /*
  * SysV IPC SHM_UNLOCK restore Unevictable pages to their evictable lists.
  */
@@ -429,12 +364,12 @@ void shmem_unlock_mapping(struct address_space *mapping)
 		 * Avoid pagevec_lookup(): find_get_pages() returns 0 as if it
 		 * has finished, if it hits a row of PAGEVEC_SIZE swap entries.
 		 */
-		pvec.nr = shmem_find_get_pages_and_swap(mapping, index,
-					PAGEVEC_SIZE, pvec.pages, indices);
+		pvec.nr = find_get_entries(mapping, index,
+					   PAGEVEC_SIZE, pvec.pages, indices);
 		if (!pvec.nr)
 			break;
 		index = indices[pvec.nr - 1] + 1;
-		shmem_deswap_pagevec(&pvec);
+		pagevec_remove_exceptionals(&pvec);
 		check_move_unevictable_pages(pvec.pages, pvec.nr);
 		pagevec_release(&pvec);
 		cond_resched();
@@ -466,9 +401,9 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 	pagevec_init(&pvec, 0);
 	index = start;
 	while (index < end) {
-		pvec.nr = shmem_find_get_pages_and_swap(mapping, index,
-				min(end - index, (pgoff_t)PAGEVEC_SIZE),
-							pvec.pages, indices);
+		pvec.nr = find_get_entries(mapping, index,
+			min(end - index, (pgoff_t)PAGEVEC_SIZE),
+			pvec.pages, indices);
 		if (!pvec.nr)
 			break;
 		mem_cgroup_uncharge_start();
@@ -497,7 +432,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 			}
 			unlock_page(page);
 		}
-		shmem_deswap_pagevec(&pvec);
+		pagevec_remove_exceptionals(&pvec);
 		pagevec_release(&pvec);
 		mem_cgroup_uncharge_end();
 		cond_resched();
@@ -535,9 +470,10 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 	index = start;
 	for ( ; ; ) {
 		cond_resched();
-		pvec.nr = shmem_find_get_pages_and_swap(mapping, index,
+
+		pvec.nr = find_get_entries(mapping, index,
 				min(end - index, (pgoff_t)PAGEVEC_SIZE),
-							pvec.pages, indices);
+				pvec.pages, indices);
 		if (!pvec.nr) {
 			if (index == start || unfalloc)
 				break;
@@ -545,7 +481,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 			continue;
 		}
 		if ((index == start || unfalloc) && indices[0] >= end) {
-			shmem_deswap_pagevec(&pvec);
+			pagevec_remove_exceptionals(&pvec);
 			pagevec_release(&pvec);
 			break;
 		}
@@ -574,7 +510,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 			}
 			unlock_page(page);
 		}
-		shmem_deswap_pagevec(&pvec);
+		pagevec_remove_exceptionals(&pvec);
 		pagevec_release(&pvec);
 		mem_cgroup_uncharge_end();
 		index++;
@@ -1082,7 +1018,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 		return -EFBIG;
 repeat:
 	swap.val = 0;
-	page = find_lock_page(mapping, index);
+	page = find_lock_entry(mapping, index);
 	if (radix_tree_exceptional_entry(page)) {
 		swap = radix_to_swp_entry(page);
 		page = NULL;
@@ -1419,6 +1355,11 @@ static struct inode *shmem_get_inode(struct super_block *sb, const struct inode
 	return inode;
 }
 
+bool shmem_mapping(struct address_space *mapping)
+{
+	return mapping->backing_dev_info == &shmem_backing_dev_info;
+}
+
 #ifdef CONFIG_TMPFS
 static const struct inode_operations shmem_symlink_inode_operations;
 static const struct inode_operations shmem_short_symlink_operations;
@@ -1731,7 +1672,7 @@ static pgoff_t shmem_seek_hole_data(struct address_space *mapping,
 	pagevec_init(&pvec, 0);
 	pvec.nr = 1;		/* start small: we may be there already */
 	while (!done) {
-		pvec.nr = shmem_find_get_pages_and_swap(mapping, index,
+		pvec.nr = find_get_entries(mapping, index,
 					pvec.nr, pvec.pages, indices);
 		if (!pvec.nr) {
 			if (whence == SEEK_DATA)
@@ -1758,7 +1699,7 @@ static pgoff_t shmem_seek_hole_data(struct address_space *mapping,
 				break;
 			}
 		}
-		shmem_deswap_pagevec(&pvec);
+		pagevec_remove_exceptionals(&pvec);
 		pagevec_release(&pvec);
 		pvec.nr = PAGEVEC_SIZE;
 		cond_resched();
diff --git a/mm/swap.c b/mm/swap.c
index 62b78a6e224f..c54cedf67646 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -831,6 +831,57 @@ void __pagevec_lru_add(struct pagevec *pvec)
 }
 EXPORT_SYMBOL(__pagevec_lru_add);
 
+/**
+ * pagevec_lookup_entries - gang pagecache lookup
+ * @pvec:	Where the resulting entries are placed
+ * @mapping:	The address_space to search
+ * @start:	The starting entry index
+ * @nr_entries:	The maximum number of entries
+ * @indices:	The cache indices corresponding to the entries in @pvec
+ *
+ * pagevec_lookup_entries() will search for and return a group of up
+ * to @nr_entries pages and shadow entries in the mapping.  All
+ * entries are placed in @pvec.  pagevec_lookup_entries() takes a
+ * reference against actual pages in @pvec.
+ *
+ * The search returns a group of mapping-contiguous entries with
+ * ascending indexes.  There may be holes in the indices due to
+ * not-present entries.
+ *
+ * pagevec_lookup_entries() returns the number of entries which were
+ * found.
+ */
+unsigned pagevec_lookup_entries(struct pagevec *pvec,
+				struct address_space *mapping,
+				pgoff_t start, unsigned nr_pages,
+				pgoff_t *indices)
+{
+	pvec->nr = find_get_entries(mapping, start, nr_pages,
+				    pvec->pages, indices);
+	return pagevec_count(pvec);
+}
+
+/**
+ * pagevec_remove_exceptionals - pagevec exceptionals pruning
+ * @pvec:	The pagevec to prune
+ *
+ * pagevec_lookup_entries() fills both pages and exceptional radix
+ * tree entries into the pagevec.  This function prunes all
+ * exceptionals from @pvec without leaving holes, so that it can be
+ * passed on to page-only pagevec operations.
+ */
+void pagevec_remove_exceptionals(struct pagevec *pvec)
+{
+	int i, j;
+
+	for (i = 0, j = 0; i < pagevec_count(pvec); i++) {
+		struct page *page = pvec->pages[i];
+		if (!radix_tree_exceptional_entry(page))
+			pvec->pages[j++] = page;
+	}
+	pvec->nr = j;
+}
+
 /**
  * pagevec_lookup - gang pagecache lookup
  * @pvec:	Where the resulting pages are placed
* Unmerged path mm/truncate.c
