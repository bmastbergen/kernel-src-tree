x86/mm: Clean up inconsistencies when flushing TLB ranges

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] Clean up inconsistencies when flushing TLB ranges (Rik van Riel) [1058886]
Rebuild_FUZZ: 92.45%
commit-author Mel Gorman <mgorman@suse.de>
commit 15aa368255f249df0b2af630c9487bb5471bd7da
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/15aa3682.failed

NR_TLB_LOCAL_FLUSH_ALL is not always accounted for correctly and
the comparison with total_vm is done before taking
tlb_flushall_shift into account.  Clean it up.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Davidlohr Bueso <davidlohr@hp.com>
	Reviewed-by: Alex Shi <alex.shi@linaro.org>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Hugh Dickins <hughd@google.com>
Link: http://lkml.kernel.org/n/tip-Iz5gcahrgskIldvukulzi0hh@git.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 15aa368255f249df0b2af630c9487bb5471bd7da)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/tlb.c
diff --cc arch/x86/mm/tlb.c
index 282375f13c7e,5176526ddd59..000000000000
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@@ -207,20 -211,22 +208,30 @@@ void flush_tlb_mm_range(struct mm_struc
  		tlb_entries = tlb_lli_4k[ENTRIES];
  	else
  		tlb_entries = tlb_lld_4k[ENTRIES];
+ 
  	/* Assume all of TLB entries was occupied by this task */
- 	act_entries = mm->total_vm > tlb_entries ? tlb_entries : mm->total_vm;
+ 	act_entries = tlb_entries >> tlb_flushall_shift;
+ 	act_entries = mm->total_vm > act_entries ? act_entries : mm->total_vm;
+ 	nr_base_pages = (end - start) >> PAGE_SHIFT;
  
  	/* tlb_flushall_shift is on balance point, details in commit log */
++<<<<<<< HEAD
 +	if ((end - start) >> PAGE_SHIFT > act_entries >> tlb_flushall_shift)
 +		local_flush_tlb();
 +	else {
 +		if (has_large_page(mm, start, end)) {
 +			local_flush_tlb();
 +			goto flush_all;
 +		}
++=======
+ 	if (nr_base_pages > act_entries || has_large_page(mm, start, end)) {
+ 		count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
+ 		local_flush_tlb();
+ 	} else {
++>>>>>>> 15aa368255f2 (x86/mm: Clean up inconsistencies when flushing TLB ranges)
  		/* flush range by one by one 'invlpg' */
 -		for (addr = start; addr < end;	addr += PAGE_SIZE) {
 -			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 +		for (addr = start; addr < end;	addr += PAGE_SIZE)
  			__flush_tlb_single(addr);
 -		}
  
  		if (cpumask_any_but(mm_cpumask(mm),
  				smp_processor_id()) < nr_cpu_ids)
* Unmerged path arch/x86/mm/tlb.c
