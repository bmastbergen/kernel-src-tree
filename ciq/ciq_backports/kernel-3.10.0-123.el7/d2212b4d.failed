lockref: allow relaxed cmpxchg64 variant for lockless updates

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Will Deacon <will.deacon@arm.com>
commit d2212b4dce596fee83e5c523400bf084f4cc816c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/d2212b4d.failed

The 64-bit cmpxchg operation on the lockref is ordered by virtue of
hazarding between the cmpxchg operation and the reference count
manipulation. On weakly ordered memory architectures (such as ARM), it
can be of great benefit to omit the barrier instructions where they are
not needed.

This patch moves the lockless lockref code over to a cmpxchg64_relaxed
operation, which doesn't provide barrier semantics. If the operation
isn't defined, we simply #define it as the usual 64-bit cmpxchg macro.

	Cc: Waiman Long <Waiman.Long@hp.com>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d2212b4dce596fee83e5c523400bf084f4cc816c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/lockref.c
diff --cc lib/lockref.c
index 01ba8088f424,e294ae445c9a..000000000000
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@@ -1,6 -1,43 +1,46 @@@
  #include <linux/export.h>
  #include <linux/lockref.h>
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_CMPXCHG_LOCKREF
+ 
+ /*
+  * Allow weakly-ordered memory architectures to provide barrier-less
+  * cmpxchg semantics for lockref updates.
+  */
+ #ifndef cmpxchg64_relaxed
+ # define cmpxchg64_relaxed cmpxchg64
+ #endif
+ 
+ /*
+  * Note that the "cmpxchg()" reloads the "old" value for the
+  * failure case.
+  */
+ #define CMPXCHG_LOOP(CODE, SUCCESS) do {					\
+ 	struct lockref old;							\
+ 	BUILD_BUG_ON(sizeof(old) != 8);						\
+ 	old.lock_count = ACCESS_ONCE(lockref->lock_count);			\
+ 	while (likely(arch_spin_value_unlocked(old.lock.rlock.raw_lock))) {  	\
+ 		struct lockref new = old, prev = old;				\
+ 		CODE								\
+ 		old.lock_count = cmpxchg64_relaxed(&lockref->lock_count,	\
+ 						   old.lock_count,		\
+ 						   new.lock_count);		\
+ 		if (likely(old.lock_count == prev.lock_count)) {		\
+ 			SUCCESS;						\
+ 		}								\
+ 		cpu_relax();							\
+ 	}									\
+ } while (0)
+ 
+ #else
+ 
+ #define CMPXCHG_LOOP(CODE, SUCCESS) do { } while (0)
+ 
+ #endif
+ 
++>>>>>>> d2212b4dce59 (lockref: allow relaxed cmpxchg64 variant for lockless updates)
  /**
   * lockref_get - Increments reference count unconditionally
   * @lockref: pointer to lockref structure
* Unmerged path lib/lockref.c
