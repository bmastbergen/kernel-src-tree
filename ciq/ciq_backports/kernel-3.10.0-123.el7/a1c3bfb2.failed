mm/page-writeback.c: do not count anon pages as dirtyable memory

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] page-writeback: do not count anon pages as dirtyable memory (Johannes Weiner) [832482]
Rebuild_FUZZ: 95.93%
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit a1c3bfb2f67ef766de03f1f56bdfff9c8595ab14
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/a1c3bfb2.failed

The VM is currently heavily tuned to avoid swapping.  Whether that is
good or bad is a separate discussion, but as long as the VM won't swap
to make room for dirty cache, we can not consider anonymous pages when
calculating the amount of dirtyable memory, the baseline to which
dirty_background_ratio and dirty_ratio are applied.

A simple workload that occupies a significant size (40+%, depending on
memory layout, storage speeds etc.) of memory with anon/tmpfs pages and
uses the remainder for a streaming writer demonstrates this problem.  In
that case, the actual cache pages are a small fraction of what is
considered dirtyable overall, which results in an relatively large
portion of the cache pages to be dirtied.  As kswapd starts rotating
these, random tasks enter direct reclaim and stall on IO.

Only consider free pages and file pages dirtyable.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Reported-by: Tejun Heo <tj@kernel.org>
	Tested-by: Tejun Heo <tj@kernel.org>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Wu Fengguang <fengguang.wu@intel.com>
	Reviewed-by: Michal Hocko <mhocko@suse.cz>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a1c3bfb2f67ef766de03f1f56bdfff9c8595ab14)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/vmstat.h
#	mm/internal.h
#	mm/vmscan.c
diff --cc include/linux/vmstat.h
index 51d3ba1005fd,a67b38415768..000000000000
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@@ -142,9 -142,6 +142,12 @@@ static inline unsigned long zone_page_s
  	return x;
  }
  
++<<<<<<< HEAD
 +extern unsigned long global_reclaimable_pages(void);
 +extern unsigned long zone_reclaimable_pages(struct zone *zone);
 +
++=======
++>>>>>>> a1c3bfb2f67e (mm/page-writeback.c: do not count anon pages as dirtyable memory)
  #ifdef CONFIG_NUMA
  /*
   * Determine the per node value of a stat item. This function
diff --cc mm/internal.h
index 8562de0a5197,29e1e761f9eb..000000000000
--- a/mm/internal.h
+++ b/mm/internal.h
@@@ -90,6 -83,7 +90,10 @@@ extern unsigned long highest_memmap_pfn
   */
  extern int isolate_lru_page(struct page *page);
  extern void putback_lru_page(struct page *page);
++<<<<<<< HEAD
++=======
+ extern bool zone_reclaimable(struct zone *zone);
++>>>>>>> a1c3bfb2f67e (mm/page-writeback.c: do not count anon pages as dirtyable memory)
  
  /*
   * in mm/rmap.c:
diff --cc mm/vmscan.c
index 65cbae5b85c4,a9c74b409681..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -147,6 -147,25 +147,28 @@@ static bool global_reclaim(struct scan_
  }
  #endif
  
++<<<<<<< HEAD
++=======
+ static unsigned long zone_reclaimable_pages(struct zone *zone)
+ {
+ 	int nr;
+ 
+ 	nr = zone_page_state(zone, NR_ACTIVE_FILE) +
+ 	     zone_page_state(zone, NR_INACTIVE_FILE);
+ 
+ 	if (get_nr_swap_pages() > 0)
+ 		nr += zone_page_state(zone, NR_ACTIVE_ANON) +
+ 		      zone_page_state(zone, NR_INACTIVE_ANON);
+ 
+ 	return nr;
+ }
+ 
+ bool zone_reclaimable(struct zone *zone)
+ {
+ 	return zone->pages_scanned < zone_reclaimable_pages(zone) * 6;
+ }
+ 
++>>>>>>> a1c3bfb2f67e (mm/page-writeback.c: do not count anon pages as dirtyable memory)
  static unsigned long get_lru_size(struct lruvec *lruvec, enum lru_list lru)
  {
  	if (!mem_cgroup_disabled())
@@@ -3246,41 -3315,6 +3268,44 @@@ void wakeup_kswapd(struct zone *zone, i
  	wake_up_interruptible(&pgdat->kswapd_wait);
  }
  
++<<<<<<< HEAD
 +/*
 + * The reclaimable count would be mostly accurate.
 + * The less reclaimable pages may be
 + * - mlocked pages, which will be moved to unevictable list when encountered
 + * - mapped pages, which may require several travels to be reclaimed
 + * - dirty pages, which is not "instantly" reclaimable
 + */
 +unsigned long global_reclaimable_pages(void)
 +{
 +	int nr;
 +
 +	nr = global_page_state(NR_ACTIVE_FILE) +
 +	     global_page_state(NR_INACTIVE_FILE);
 +
 +	if (get_nr_swap_pages() > 0)
 +		nr += global_page_state(NR_ACTIVE_ANON) +
 +		      global_page_state(NR_INACTIVE_ANON);
 +
 +	return nr;
 +}
 +
 +unsigned long zone_reclaimable_pages(struct zone *zone)
 +{
 +	int nr;
 +
 +	nr = zone_page_state(zone, NR_ACTIVE_FILE) +
 +	     zone_page_state(zone, NR_INACTIVE_FILE);
 +
 +	if (get_nr_swap_pages() > 0)
 +		nr += zone_page_state(zone, NR_ACTIVE_ANON) +
 +		      zone_page_state(zone, NR_INACTIVE_ANON);
 +
 +	return nr;
 +}
 +
++=======
++>>>>>>> a1c3bfb2f67e (mm/page-writeback.c: do not count anon pages as dirtyable memory)
  #ifdef CONFIG_HIBERNATION
  /*
   * Try to free `nr_to_reclaim' of memory, system-wide, and return the number of
* Unmerged path include/linux/vmstat.h
* Unmerged path mm/internal.h
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 3e7885c4e99c..8098aa4304f4 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -202,7 +202,8 @@ static unsigned long zone_dirtyable_memory(struct zone *zone)
 	nr_pages = zone_page_state(zone, NR_FREE_PAGES);
 	nr_pages -= min(nr_pages, zone->dirty_balance_reserve);
 
-	nr_pages += zone_reclaimable_pages(zone);
+	nr_pages += zone_page_state(zone, NR_INACTIVE_FILE);
+	nr_pages += zone_page_state(zone, NR_ACTIVE_FILE);
 
 	return nr_pages;
 }
@@ -255,7 +256,8 @@ static unsigned long global_dirtyable_memory(void)
 	x = global_page_state(NR_FREE_PAGES);
 	x -= min(x, dirty_balance_reserve);
 
-	x += global_reclaimable_pages();
+	x += global_page_state(NR_INACTIVE_FILE);
+	x += global_page_state(NR_ACTIVE_FILE);
 
 	if (!vm_highmem_is_dirtyable)
 		x -= highmem_dirtyable_memory(x);
* Unmerged path mm/vmscan.c
