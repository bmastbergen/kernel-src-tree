mm: convert the rest to new page table lock api

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] convert the rest to new page table lock api (Rik van Riel) [1058896]
Rebuild_FUZZ: 95.56%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit c4088ebdca64c9a2e34a38177d2249805ede1f4b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/c4088ebd.failed

Only trivial cases left. Let's convert them altogether.

	Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Tested-by: Alex Thorlton <athorlton@sgi.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: "Eric W . Biederman" <ebiederm@xmission.com>
	Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
	Cc: Al Viro <viro@zeniv.linux.org.uk>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Dave Jones <davej@redhat.com>
	Cc: David Howells <dhowells@redhat.com>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Kees Cook <keescook@chromium.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Michael Kerrisk <mtk.manpages@gmail.com>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Robin Holt <robinmholt@gmail.com>
	Cc: Sedat Dilek <sedat.dilek@gmail.com>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Hugh Dickins <hughd@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c4088ebdca64c9a2e34a38177d2249805ede1f4b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index 2b345c81f273,bccd5a628ea6..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -753,14 -760,7 +754,18 @@@ static inline struct page *alloc_hugepa
  			       HPAGE_PMD_ORDER, vma, haddr, nd);
  }
  
++<<<<<<< HEAD
 +#ifndef CONFIG_NUMA
 +static inline struct page *alloc_hugepage(int defrag)
 +{
 +	return alloc_pages(alloc_hugepage_gfpmask(defrag, 0),
 +			   HPAGE_PMD_ORDER);
 +}
 +#endif
 +
++=======
+ /* Caller must hold page table lock. */
++>>>>>>> c4088ebdca64 (mm: convert the rest to new page table lock api)
  static bool set_huge_zero_page(pgtable_t pgtable, struct mm_struct *mm,
  		struct vm_area_struct *vma, unsigned long haddr, pmd_t *pmd,
  		struct page *zero_page)
@@@ -783,77 -783,58 +788,104 @@@ int do_huge_pmd_anonymous_page(struct m
  {
  	struct page *page;
  	unsigned long haddr = address & HPAGE_PMD_MASK;
 +	pte_t *pte;
  
++<<<<<<< HEAD
 +	if (haddr >= vma->vm_start && haddr + HPAGE_PMD_SIZE <= vma->vm_end) {
 +		if (unlikely(anon_vma_prepare(vma)))
++=======
+ 	if (haddr < vma->vm_start || haddr + HPAGE_PMD_SIZE > vma->vm_end)
+ 		return VM_FAULT_FALLBACK;
+ 	if (unlikely(anon_vma_prepare(vma)))
+ 		return VM_FAULT_OOM;
+ 	if (unlikely(khugepaged_enter(vma)))
+ 		return VM_FAULT_OOM;
+ 	if (!(flags & FAULT_FLAG_WRITE) &&
+ 			transparent_hugepage_use_zero_page()) {
+ 		spinlock_t *ptl;
+ 		pgtable_t pgtable;
+ 		struct page *zero_page;
+ 		bool set;
+ 		pgtable = pte_alloc_one(mm, haddr);
+ 		if (unlikely(!pgtable))
++>>>>>>> c4088ebdca64 (mm: convert the rest to new page table lock api)
  			return VM_FAULT_OOM;
 -		zero_page = get_huge_zero_page();
 -		if (unlikely(!zero_page)) {
 -			pte_free(mm, pgtable);
 +		if (unlikely(khugepaged_enter(vma)))
 +			return VM_FAULT_OOM;
 +		if (!(flags & FAULT_FLAG_WRITE) &&
 +				transparent_hugepage_use_zero_page()) {
 +			pgtable_t pgtable;
 +			struct page *zero_page;
 +			bool set;
 +			pgtable = pte_alloc_one(mm, haddr);
 +			if (unlikely(!pgtable))
 +				return VM_FAULT_OOM;
 +			zero_page = get_huge_zero_page();
 +			if (unlikely(!zero_page)) {
 +				pte_free(mm, pgtable);
 +				count_vm_event(THP_FAULT_FALLBACK);
 +				goto out;
 +			}
 +			spin_lock(&mm->page_table_lock);
 +			set = set_huge_zero_page(pgtable, mm, vma, haddr, pmd,
 +					zero_page);
 +			spin_unlock(&mm->page_table_lock);
 +			if (!set) {
 +				pte_free(mm, pgtable);
 +				put_huge_zero_page();
 +			}
 +			return 0;
 +		}
 +		page = alloc_hugepage_vma(transparent_hugepage_defrag(vma),
 +					  vma, haddr, numa_node_id(), 0);
 +		if (unlikely(!page)) {
  			count_vm_event(THP_FAULT_FALLBACK);
 -			return VM_FAULT_FALLBACK;
 +			goto out;
  		}
++<<<<<<< HEAD
 +		count_vm_event(THP_FAULT_ALLOC);
 +		if (unlikely(mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))) {
 +			put_page(page);
 +			goto out;
++=======
+ 		ptl = pmd_lock(mm, pmd);
+ 		set = set_huge_zero_page(pgtable, mm, vma, haddr, pmd,
+ 				zero_page);
+ 		spin_unlock(ptl);
+ 		if (!set) {
+ 			pte_free(mm, pgtable);
+ 			put_huge_zero_page();
++>>>>>>> c4088ebdca64 (mm: convert the rest to new page table lock api)
 +		}
 +		if (unlikely(__do_huge_pmd_anonymous_page(mm, vma, haddr, pmd,
 +							  page))) {
 +			mem_cgroup_uncharge_page(page);
 +			put_page(page);
 +			goto out;
  		}
 +
  		return 0;
  	}
 -	page = alloc_hugepage_vma(transparent_hugepage_defrag(vma),
 -			vma, haddr, numa_node_id(), 0);
 -	if (unlikely(!page)) {
 -		count_vm_event(THP_FAULT_FALLBACK);
 -		return VM_FAULT_FALLBACK;
 -	}
 -	if (unlikely(mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))) {
 -		put_page(page);
 -		count_vm_event(THP_FAULT_FALLBACK);
 -		return VM_FAULT_FALLBACK;
 -	}
 -	if (unlikely(__do_huge_pmd_anonymous_page(mm, vma, haddr, pmd, page))) {
 -		mem_cgroup_uncharge_page(page);
 -		put_page(page);
 -		count_vm_event(THP_FAULT_FALLBACK);
 -		return VM_FAULT_FALLBACK;
 -	}
 -
 -	count_vm_event(THP_FAULT_ALLOC);
 -	return 0;
 +out:
 +	/*
 +	 * Use __pte_alloc instead of pte_alloc_map, because we can't
 +	 * run pte_offset_map on the pmd, if an huge pmd could
 +	 * materialize from under us from a different thread.
 +	 */
 +	if (unlikely(pmd_none(*pmd)) &&
 +	    unlikely(__pte_alloc(mm, vma, pmd, address)))
 +		return VM_FAULT_OOM;
 +	/* if an huge pmd materialized from under us just retry later */
 +	if (unlikely(pmd_trans_huge(*pmd)))
 +		return 0;
 +	/*
 +	 * A regular pmd is established and it can't morph into a huge pmd
 +	 * from under us anymore at this point because we hold the mmap_sem
 +	 * read mode and khugepaged takes it in write mode. So now it's
 +	 * safe to run pte_offset_map().
 +	 */
 +	pte = pte_offset_map(pmd, address);
 +	return handle_pte_fault(mm, vma, address, pte, pmd, flags);
  }
  
  int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
@@@ -1294,11 -1286,12 +1334,11 @@@ int do_huge_pmd_numa_page(struct mm_str
  	struct page *page;
  	unsigned long haddr = addr & HPAGE_PMD_MASK;
  	int page_nid = -1, this_nid = numa_node_id();
 -	int target_nid, last_cpupid = -1;
 +	int target_nid;
  	bool page_locked;
  	bool migrated = false;
 -	int flags = 0;
  
- 	spin_lock(&mm->page_table_lock);
+ 	ptl = pmd_lock(mm, pmdp);
  	if (unlikely(!pmd_same(pmd, *pmdp)))
  		goto out_unlock;
  
@@@ -1358,11 -1355,13 +1398,11 @@@
  	 * Migrate the THP to the requested node, returns with page unlocked
  	 * and pmd_numa cleared.
  	 */
- 	spin_unlock(&mm->page_table_lock);
+ 	spin_unlock(ptl);
  	migrated = migrate_misplaced_transhuge_page(mm, vma,
  				pmdp, pmd, addr, page, target_nid);
 -	if (migrated) {
 -		flags |= TNF_MIGRATED;
 +	if (migrated)
  		page_nid = target_nid;
 -	}
  
  	goto out;
  clear_pmdnuma:
@@@ -2772,10 -2827,11 +2813,10 @@@ void __split_huge_page_pmd(struct vm_ar
  
  	mmun_start = haddr;
  	mmun_end   = haddr + HPAGE_PMD_SIZE;
 -again:
  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
- 	spin_lock(&mm->page_table_lock);
+ 	ptl = pmd_lock(mm, pmd);
  	if (unlikely(!pmd_trans_huge(*pmd))) {
- 		spin_unlock(&mm->page_table_lock);
+ 		spin_unlock(ptl);
  		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
  		return;
  	}
* Unmerged path mm/huge_memory.c
diff --git a/mm/memory.c b/mm/memory.c
index c855ff4ff439..75a57222ad13 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -579,6 +579,7 @@ void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
 int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
 		pmd_t *pmd, unsigned long address)
 {
+	spinlock_t *ptl;
 	pgtable_t new = pte_alloc_one(mm, address);
 	int wait_split_huge_page;
 	if (!new)
@@ -599,7 +600,7 @@ int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
 	 */
 	smp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */
 
-	spin_lock(&mm->page_table_lock);
+	ptl = pmd_lock(mm, pmd);
 	wait_split_huge_page = 0;
 	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
 		atomic_long_inc(&mm->nr_ptes);
@@ -607,7 +608,7 @@ int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
 		new = NULL;
 	} else if (unlikely(pmd_trans_splitting(*pmd)))
 		wait_split_huge_page = 1;
-	spin_unlock(&mm->page_table_lock);
+	spin_unlock(ptl);
 	if (new)
 		pte_free(mm, new);
 	if (wait_split_huge_page)
@@ -1517,20 +1518,20 @@ struct page *follow_page_mask(struct vm_area_struct *vma,
 			split_huge_page_pmd(vma, address, pmd);
 			goto split_fallthrough;
 		}
-		spin_lock(&mm->page_table_lock);
+		ptl = pmd_lock(mm, pmd);
 		if (likely(pmd_trans_huge(*pmd))) {
 			if (unlikely(pmd_trans_splitting(*pmd))) {
-				spin_unlock(&mm->page_table_lock);
+				spin_unlock(ptl);
 				wait_split_huge_page(vma->anon_vma, pmd);
 			} else {
 				page = follow_trans_huge_pmd(vma, address,
 							     pmd, flags);
-				spin_unlock(&mm->page_table_lock);
+				spin_unlock(ptl);
 				*page_mask = HPAGE_PMD_NR - 1;
 				goto out;
 			}
 		} else
-			spin_unlock(&mm->page_table_lock);
+			spin_unlock(ptl);
 		/* fall through */
 	}
 split_fallthrough:
diff --git a/mm/migrate.c b/mm/migrate.c
index 40c5b97e9161..27aebe5c816d 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1668,6 +1668,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 				unsigned long address,
 				struct page *page, int node)
 {
+	spinlock_t *ptl;
 	unsigned long haddr = address & HPAGE_PMD_MASK;
 	pg_data_t *pgdat = NODE_DATA(node);
 	int isolated = 0;
@@ -1710,9 +1711,9 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 	WARN_ON(PageLRU(new_page));
 
 	/* Recheck the target PMD */
-	spin_lock(&mm->page_table_lock);
+	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_same(*pmd, entry))) {
-		spin_unlock(&mm->page_table_lock);
+		spin_unlock(ptl);
 
 		/* Reverse changes made by migrate_page_copy() */
 		if (TestClearPageActive(new_page))
@@ -1758,7 +1759,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 	 * before it's fully transferred to the new page.
 	 */
 	mem_cgroup_end_migration(memcg, page, new_page, true);
-	spin_unlock(&mm->page_table_lock);
+	spin_unlock(ptl);
 
 	unlock_page(new_page);
 	unlock_page(page);
diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c
index 91ec39be2582..b178c2e5d1a9 100644
--- a/mm/pgtable-generic.c
+++ b/mm/pgtable-generic.c
@@ -128,7 +128,7 @@ void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
 void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				pgtable_t pgtable)
 {
-	assert_spin_locked(&mm->page_table_lock);
+	assert_spin_locked(pmd_lockptr(mm, pmdp));
 
 	/* FIFO */
 	if (!pmd_huge_pte(mm, pmdp))
@@ -147,7 +147,7 @@ pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)
 {
 	pgtable_t pgtable;
 
-	assert_spin_locked(&mm->page_table_lock);
+	assert_spin_locked(pmd_lockptr(mm, pmdp));
 
 	/* FIFO */
 	pgtable = pmd_huge_pte(mm, pmdp);
