xfs: swap extents operations for CRC filesystems

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit 21b5c9784bceb8b8e0095f87355f3b138ebac2d0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/21b5c978.failed

For CRC enabled filesystems, we can't just swap inode forks from one
inode to another when defragmenting a file - the blocks in the inode
fork bmap btree contain pointers back to the owner inode. Hence if
we are to swap the inode forks we have to atomically modify every
block in the btree during the transaction.

We are doing an entire fork swap here, so we could create a new
transaction item type that indicates we are changing the owner of a
certain structure from one value to another. If we combine this with
ordered buffer logging to modify all the buffers in the tree, then
we can change the buffers in the tree without needing log space for
the operation. However, this then requires log recovery to perform
the modification of the owner information of the objects/structures
in question.

This does introduce some interesting ordering details into recovery:
we have to make sure that the owner change replay occurs after the
change that moves the objects is made, not before. Hence we can't
use a separate log item for this as we have no guarantee of strict
ordering between multiple items in the log due to the relogging
action of asynchronous transaction commits. Hence there is no
"generic" method we can use for changing the ownership of arbitrary
metadata structures.

For inode forks, however, there is a simple method of communicating
that the fork contents need the owner rewritten - we can pass a
inode log format flag for the fork for the transaction that does a
fork swap. This flag will then follow the inode fork through
relogging actions so when the swap actually gets replayed the
ownership can be changed immediately by log recovery.  So that gives
us a simple method of "whole fork" exchange between two inodes.

This is relatively simple to implement, so it makes sense to do this
as an initial implementation to support xfs_fsr on CRC enabled
filesytems in the same manner as we do on existing filesystems. This
commit introduces the swapext driven functionality, the recovery
functionality will be in a separate patch.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Mark Tinguely <tinguely@sgi.com>
	Signed-off-by: Ben Myers <bpm@sgi.com>
(cherry picked from commit 21b5c9784bceb8b8e0095f87355f3b138ebac2d0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_bmap_util.c
diff --cc fs/xfs/xfs_bmap_util.c
index d4d53f34241d,ad8a91d2e011..000000000000
--- a/fs/xfs/xfs_bmap_util.c
+++ b/fs/xfs/xfs_bmap_util.c
@@@ -832,3 -835,1216 +832,1219 @@@ next_block
  
  	return error;
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Test whether it is appropriate to check an inode for and free post EOF
+  * blocks. The 'force' parameter determines whether we should also consider
+  * regular files that are marked preallocated or append-only.
+  */
+ bool
+ xfs_can_free_eofblocks(struct xfs_inode *ip, bool force)
+ {
+ 	/* prealloc/delalloc exists only on regular files */
+ 	if (!S_ISREG(ip->i_d.di_mode))
+ 		return false;
+ 
+ 	/*
+ 	 * Zero sized files with no cached pages and delalloc blocks will not
+ 	 * have speculative prealloc/delalloc blocks to remove.
+ 	 */
+ 	if (VFS_I(ip)->i_size == 0 &&
+ 	    VN_CACHED(VFS_I(ip)) == 0 &&
+ 	    ip->i_delayed_blks == 0)
+ 		return false;
+ 
+ 	/* If we haven't read in the extent list, then don't do it now. */
+ 	if (!(ip->i_df.if_flags & XFS_IFEXTENTS))
+ 		return false;
+ 
+ 	/*
+ 	 * Do not free real preallocated or append-only files unless the file
+ 	 * has delalloc blocks and we are forced to remove them.
+ 	 */
+ 	if (ip->i_d.di_flags & (XFS_DIFLAG_PREALLOC | XFS_DIFLAG_APPEND))
+ 		if (!force || ip->i_delayed_blks == 0)
+ 			return false;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * This is called by xfs_inactive to free any blocks beyond eof
+  * when the link count isn't zero and by xfs_dm_punch_hole() when
+  * punching a hole to EOF.
+  */
+ int
+ xfs_free_eofblocks(
+ 	xfs_mount_t	*mp,
+ 	xfs_inode_t	*ip,
+ 	bool		need_iolock)
+ {
+ 	xfs_trans_t	*tp;
+ 	int		error;
+ 	xfs_fileoff_t	end_fsb;
+ 	xfs_fileoff_t	last_fsb;
+ 	xfs_filblks_t	map_len;
+ 	int		nimaps;
+ 	xfs_bmbt_irec_t	imap;
+ 
+ 	/*
+ 	 * Figure out if there are any blocks beyond the end
+ 	 * of the file.  If not, then there is nothing to do.
+ 	 */
+ 	end_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)XFS_ISIZE(ip));
+ 	last_fsb = XFS_B_TO_FSB(mp, mp->m_super->s_maxbytes);
+ 	if (last_fsb <= end_fsb)
+ 		return 0;
+ 	map_len = last_fsb - end_fsb;
+ 
+ 	nimaps = 1;
+ 	xfs_ilock(ip, XFS_ILOCK_SHARED);
+ 	error = xfs_bmapi_read(ip, end_fsb, map_len, &imap, &nimaps, 0);
+ 	xfs_iunlock(ip, XFS_ILOCK_SHARED);
+ 
+ 	if (!error && (nimaps != 0) &&
+ 	    (imap.br_startblock != HOLESTARTBLOCK ||
+ 	     ip->i_delayed_blks)) {
+ 		/*
+ 		 * Attach the dquots to the inode up front.
+ 		 */
+ 		error = xfs_qm_dqattach(ip, 0);
+ 		if (error)
+ 			return error;
+ 
+ 		/*
+ 		 * There are blocks after the end of file.
+ 		 * Free them up now by truncating the file to
+ 		 * its current size.
+ 		 */
+ 		tp = xfs_trans_alloc(mp, XFS_TRANS_INACTIVE);
+ 
+ 		if (need_iolock) {
+ 			if (!xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL)) {
+ 				xfs_trans_cancel(tp, 0);
+ 				return EAGAIN;
+ 			}
+ 		}
+ 
+ 		error = xfs_trans_reserve(tp, &M_RES(mp)->tr_itruncate, 0, 0);
+ 		if (error) {
+ 			ASSERT(XFS_FORCED_SHUTDOWN(mp));
+ 			xfs_trans_cancel(tp, 0);
+ 			if (need_iolock)
+ 				xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+ 			return error;
+ 		}
+ 
+ 		xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 		xfs_trans_ijoin(tp, ip, 0);
+ 
+ 		/*
+ 		 * Do not update the on-disk file size.  If we update the
+ 		 * on-disk file size and then the system crashes before the
+ 		 * contents of the file are flushed to disk then the files
+ 		 * may be full of holes (ie NULL files bug).
+ 		 */
+ 		error = xfs_itruncate_extents(&tp, ip, XFS_DATA_FORK,
+ 					      XFS_ISIZE(ip));
+ 		if (error) {
+ 			/*
+ 			 * If we get an error at this point we simply don't
+ 			 * bother truncating the file.
+ 			 */
+ 			xfs_trans_cancel(tp,
+ 					 (XFS_TRANS_RELEASE_LOG_RES |
+ 					  XFS_TRANS_ABORT));
+ 		} else {
+ 			error = xfs_trans_commit(tp,
+ 						XFS_TRANS_RELEASE_LOG_RES);
+ 			if (!error)
+ 				xfs_inode_clear_eofblocks_tag(ip);
+ 		}
+ 
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 		if (need_iolock)
+ 			xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+ 	}
+ 	return error;
+ }
+ 
+ /*
+  * xfs_alloc_file_space()
+  *      This routine allocates disk space for the given file.
+  *
+  *	If alloc_type == 0, this request is for an ALLOCSP type
+  *	request which will change the file size.  In this case, no
+  *	DMAPI event will be generated by the call.  A TRUNCATE event
+  *	will be generated later by xfs_setattr.
+  *
+  *	If alloc_type != 0, this request is for a RESVSP type
+  *	request, and a DMAPI DM_EVENT_WRITE will be generated if the
+  *	lower block boundary byte address is less than the file's
+  *	length.
+  *
+  * RETURNS:
+  *       0 on success
+  *      errno on error
+  *
+  */
+ STATIC int
+ xfs_alloc_file_space(
+ 	xfs_inode_t		*ip,
+ 	xfs_off_t		offset,
+ 	xfs_off_t		len,
+ 	int			alloc_type,
+ 	int			attr_flags)
+ {
+ 	xfs_mount_t		*mp = ip->i_mount;
+ 	xfs_off_t		count;
+ 	xfs_filblks_t		allocated_fsb;
+ 	xfs_filblks_t		allocatesize_fsb;
+ 	xfs_extlen_t		extsz, temp;
+ 	xfs_fileoff_t		startoffset_fsb;
+ 	xfs_fsblock_t		firstfsb;
+ 	int			nimaps;
+ 	int			quota_flag;
+ 	int			rt;
+ 	xfs_trans_t		*tp;
+ 	xfs_bmbt_irec_t		imaps[1], *imapp;
+ 	xfs_bmap_free_t		free_list;
+ 	uint			qblocks, resblks, resrtextents;
+ 	int			committed;
+ 	int			error;
+ 
+ 	trace_xfs_alloc_file_space(ip);
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		return XFS_ERROR(EIO);
+ 
+ 	error = xfs_qm_dqattach(ip, 0);
+ 	if (error)
+ 		return error;
+ 
+ 	if (len <= 0)
+ 		return XFS_ERROR(EINVAL);
+ 
+ 	rt = XFS_IS_REALTIME_INODE(ip);
+ 	extsz = xfs_get_extsz_hint(ip);
+ 
+ 	count = len;
+ 	imapp = &imaps[0];
+ 	nimaps = 1;
+ 	startoffset_fsb	= XFS_B_TO_FSBT(mp, offset);
+ 	allocatesize_fsb = XFS_B_TO_FSB(mp, count);
+ 
+ 	/*
+ 	 * Allocate file space until done or until there is an error
+ 	 */
+ 	while (allocatesize_fsb && !error) {
+ 		xfs_fileoff_t	s, e;
+ 
+ 		/*
+ 		 * Determine space reservations for data/realtime.
+ 		 */
+ 		if (unlikely(extsz)) {
+ 			s = startoffset_fsb;
+ 			do_div(s, extsz);
+ 			s *= extsz;
+ 			e = startoffset_fsb + allocatesize_fsb;
+ 			if ((temp = do_mod(startoffset_fsb, extsz)))
+ 				e += temp;
+ 			if ((temp = do_mod(e, extsz)))
+ 				e += extsz - temp;
+ 		} else {
+ 			s = 0;
+ 			e = allocatesize_fsb;
+ 		}
+ 
+ 		/*
+ 		 * The transaction reservation is limited to a 32-bit block
+ 		 * count, hence we need to limit the number of blocks we are
+ 		 * trying to reserve to avoid an overflow. We can't allocate
+ 		 * more than @nimaps extents, and an extent is limited on disk
+ 		 * to MAXEXTLEN (21 bits), so use that to enforce the limit.
+ 		 */
+ 		resblks = min_t(xfs_fileoff_t, (e - s), (MAXEXTLEN * nimaps));
+ 		if (unlikely(rt)) {
+ 			resrtextents = qblocks = resblks;
+ 			resrtextents /= mp->m_sb.sb_rextsize;
+ 			resblks = XFS_DIOSTRAT_SPACE_RES(mp, 0);
+ 			quota_flag = XFS_QMOPT_RES_RTBLKS;
+ 		} else {
+ 			resrtextents = 0;
+ 			resblks = qblocks = XFS_DIOSTRAT_SPACE_RES(mp, resblks);
+ 			quota_flag = XFS_QMOPT_RES_REGBLKS;
+ 		}
+ 
+ 		/*
+ 		 * Allocate and setup the transaction.
+ 		 */
+ 		tp = xfs_trans_alloc(mp, XFS_TRANS_DIOSTRAT);
+ 		error = xfs_trans_reserve(tp, &M_RES(mp)->tr_write,
+ 					  resblks, resrtextents);
+ 		/*
+ 		 * Check for running out of space
+ 		 */
+ 		if (error) {
+ 			/*
+ 			 * Free the transaction structure.
+ 			 */
+ 			ASSERT(error == ENOSPC || XFS_FORCED_SHUTDOWN(mp));
+ 			xfs_trans_cancel(tp, 0);
+ 			break;
+ 		}
+ 		xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 		error = xfs_trans_reserve_quota_nblks(tp, ip, qblocks,
+ 						      0, quota_flag);
+ 		if (error)
+ 			goto error1;
+ 
+ 		xfs_trans_ijoin(tp, ip, 0);
+ 
+ 		xfs_bmap_init(&free_list, &firstfsb);
+ 		error = xfs_bmapi_write(tp, ip, startoffset_fsb,
+ 					allocatesize_fsb, alloc_type, &firstfsb,
+ 					0, imapp, &nimaps, &free_list);
+ 		if (error) {
+ 			goto error0;
+ 		}
+ 
+ 		/*
+ 		 * Complete the transaction
+ 		 */
+ 		error = xfs_bmap_finish(&tp, &free_list, &committed);
+ 		if (error) {
+ 			goto error0;
+ 		}
+ 
+ 		error = xfs_trans_commit(tp, XFS_TRANS_RELEASE_LOG_RES);
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 		if (error) {
+ 			break;
+ 		}
+ 
+ 		allocated_fsb = imapp->br_blockcount;
+ 
+ 		if (nimaps == 0) {
+ 			error = XFS_ERROR(ENOSPC);
+ 			break;
+ 		}
+ 
+ 		startoffset_fsb += allocated_fsb;
+ 		allocatesize_fsb -= allocated_fsb;
+ 	}
+ 
+ 	return error;
+ 
+ error0:	/* Cancel bmap, unlock inode, unreserve quota blocks, cancel trans */
+ 	xfs_bmap_cancel(&free_list);
+ 	xfs_trans_unreserve_quota_nblks(tp, ip, (long)qblocks, 0, quota_flag);
+ 
+ error1:	/* Just cancel transaction */
+ 	xfs_trans_cancel(tp, XFS_TRANS_RELEASE_LOG_RES | XFS_TRANS_ABORT);
+ 	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	return error;
+ }
+ 
+ /*
+  * Zero file bytes between startoff and endoff inclusive.
+  * The iolock is held exclusive and no blocks are buffered.
+  *
+  * This function is used by xfs_free_file_space() to zero
+  * partial blocks when the range to free is not block aligned.
+  * When unreserving space with boundaries that are not block
+  * aligned we round up the start and round down the end
+  * boundaries and then use this function to zero the parts of
+  * the blocks that got dropped during the rounding.
+  */
+ STATIC int
+ xfs_zero_remaining_bytes(
+ 	xfs_inode_t		*ip,
+ 	xfs_off_t		startoff,
+ 	xfs_off_t		endoff)
+ {
+ 	xfs_bmbt_irec_t		imap;
+ 	xfs_fileoff_t		offset_fsb;
+ 	xfs_off_t		lastoffset;
+ 	xfs_off_t		offset;
+ 	xfs_buf_t		*bp;
+ 	xfs_mount_t		*mp = ip->i_mount;
+ 	int			nimap;
+ 	int			error = 0;
+ 
+ 	/*
+ 	 * Avoid doing I/O beyond eof - it's not necessary
+ 	 * since nothing can read beyond eof.  The space will
+ 	 * be zeroed when the file is extended anyway.
+ 	 */
+ 	if (startoff >= XFS_ISIZE(ip))
+ 		return 0;
+ 
+ 	if (endoff > XFS_ISIZE(ip))
+ 		endoff = XFS_ISIZE(ip);
+ 
+ 	bp = xfs_buf_get_uncached(XFS_IS_REALTIME_INODE(ip) ?
+ 					mp->m_rtdev_targp : mp->m_ddev_targp,
+ 				  BTOBB(mp->m_sb.sb_blocksize), 0);
+ 	if (!bp)
+ 		return XFS_ERROR(ENOMEM);
+ 
+ 	xfs_buf_unlock(bp);
+ 
+ 	for (offset = startoff; offset <= endoff; offset = lastoffset + 1) {
+ 		offset_fsb = XFS_B_TO_FSBT(mp, offset);
+ 		nimap = 1;
+ 		error = xfs_bmapi_read(ip, offset_fsb, 1, &imap, &nimap, 0);
+ 		if (error || nimap < 1)
+ 			break;
+ 		ASSERT(imap.br_blockcount >= 1);
+ 		ASSERT(imap.br_startoff == offset_fsb);
+ 		lastoffset = XFS_FSB_TO_B(mp, imap.br_startoff + 1) - 1;
+ 		if (lastoffset > endoff)
+ 			lastoffset = endoff;
+ 		if (imap.br_startblock == HOLESTARTBLOCK)
+ 			continue;
+ 		ASSERT(imap.br_startblock != DELAYSTARTBLOCK);
+ 		if (imap.br_state == XFS_EXT_UNWRITTEN)
+ 			continue;
+ 		XFS_BUF_UNDONE(bp);
+ 		XFS_BUF_UNWRITE(bp);
+ 		XFS_BUF_READ(bp);
+ 		XFS_BUF_SET_ADDR(bp, xfs_fsb_to_db(ip, imap.br_startblock));
+ 		xfsbdstrat(mp, bp);
+ 		error = xfs_buf_iowait(bp);
+ 		if (error) {
+ 			xfs_buf_ioerror_alert(bp,
+ 					"xfs_zero_remaining_bytes(read)");
+ 			break;
+ 		}
+ 		memset(bp->b_addr +
+ 			(offset - XFS_FSB_TO_B(mp, imap.br_startoff)),
+ 		      0, lastoffset - offset + 1);
+ 		XFS_BUF_UNDONE(bp);
+ 		XFS_BUF_UNREAD(bp);
+ 		XFS_BUF_WRITE(bp);
+ 		xfsbdstrat(mp, bp);
+ 		error = xfs_buf_iowait(bp);
+ 		if (error) {
+ 			xfs_buf_ioerror_alert(bp,
+ 					"xfs_zero_remaining_bytes(write)");
+ 			break;
+ 		}
+ 	}
+ 	xfs_buf_free(bp);
+ 	return error;
+ }
+ 
+ /*
+  * xfs_free_file_space()
+  *      This routine frees disk space for the given file.
+  *
+  *	This routine is only called by xfs_change_file_space
+  *	for an UNRESVSP type call.
+  *
+  * RETURNS:
+  *       0 on success
+  *      errno on error
+  *
+  */
+ STATIC int
+ xfs_free_file_space(
+ 	xfs_inode_t		*ip,
+ 	xfs_off_t		offset,
+ 	xfs_off_t		len,
+ 	int			attr_flags)
+ {
+ 	int			committed;
+ 	int			done;
+ 	xfs_fileoff_t		endoffset_fsb;
+ 	int			error;
+ 	xfs_fsblock_t		firstfsb;
+ 	xfs_bmap_free_t		free_list;
+ 	xfs_bmbt_irec_t		imap;
+ 	xfs_off_t		ioffset;
+ 	xfs_extlen_t		mod=0;
+ 	xfs_mount_t		*mp;
+ 	int			nimap;
+ 	uint			resblks;
+ 	xfs_off_t		rounding;
+ 	int			rt;
+ 	xfs_fileoff_t		startoffset_fsb;
+ 	xfs_trans_t		*tp;
+ 	int			need_iolock = 1;
+ 
+ 	mp = ip->i_mount;
+ 
+ 	trace_xfs_free_file_space(ip);
+ 
+ 	error = xfs_qm_dqattach(ip, 0);
+ 	if (error)
+ 		return error;
+ 
+ 	error = 0;
+ 	if (len <= 0)	/* if nothing being freed */
+ 		return error;
+ 	rt = XFS_IS_REALTIME_INODE(ip);
+ 	startoffset_fsb	= XFS_B_TO_FSB(mp, offset);
+ 	endoffset_fsb = XFS_B_TO_FSBT(mp, offset + len);
+ 
+ 	if (attr_flags & XFS_ATTR_NOLOCK)
+ 		need_iolock = 0;
+ 	if (need_iolock) {
+ 		xfs_ilock(ip, XFS_IOLOCK_EXCL);
+ 		/* wait for the completion of any pending DIOs */
+ 		inode_dio_wait(VFS_I(ip));
+ 	}
+ 
+ 	rounding = max_t(xfs_off_t, 1 << mp->m_sb.sb_blocklog, PAGE_CACHE_SIZE);
+ 	ioffset = offset & ~(rounding - 1);
+ 	error = -filemap_write_and_wait_range(VFS_I(ip)->i_mapping,
+ 					      ioffset, -1);
+ 	if (error)
+ 		goto out_unlock_iolock;
+ 	truncate_pagecache_range(VFS_I(ip), ioffset, -1);
+ 
+ 	/*
+ 	 * Need to zero the stuff we're not freeing, on disk.
+ 	 * If it's a realtime file & can't use unwritten extents then we
+ 	 * actually need to zero the extent edges.  Otherwise xfs_bunmapi
+ 	 * will take care of it for us.
+ 	 */
+ 	if (rt && !xfs_sb_version_hasextflgbit(&mp->m_sb)) {
+ 		nimap = 1;
+ 		error = xfs_bmapi_read(ip, startoffset_fsb, 1,
+ 					&imap, &nimap, 0);
+ 		if (error)
+ 			goto out_unlock_iolock;
+ 		ASSERT(nimap == 0 || nimap == 1);
+ 		if (nimap && imap.br_startblock != HOLESTARTBLOCK) {
+ 			xfs_daddr_t	block;
+ 
+ 			ASSERT(imap.br_startblock != DELAYSTARTBLOCK);
+ 			block = imap.br_startblock;
+ 			mod = do_div(block, mp->m_sb.sb_rextsize);
+ 			if (mod)
+ 				startoffset_fsb += mp->m_sb.sb_rextsize - mod;
+ 		}
+ 		nimap = 1;
+ 		error = xfs_bmapi_read(ip, endoffset_fsb - 1, 1,
+ 					&imap, &nimap, 0);
+ 		if (error)
+ 			goto out_unlock_iolock;
+ 		ASSERT(nimap == 0 || nimap == 1);
+ 		if (nimap && imap.br_startblock != HOLESTARTBLOCK) {
+ 			ASSERT(imap.br_startblock != DELAYSTARTBLOCK);
+ 			mod++;
+ 			if (mod && (mod != mp->m_sb.sb_rextsize))
+ 				endoffset_fsb -= mod;
+ 		}
+ 	}
+ 	if ((done = (endoffset_fsb <= startoffset_fsb)))
+ 		/*
+ 		 * One contiguous piece to clear
+ 		 */
+ 		error = xfs_zero_remaining_bytes(ip, offset, offset + len - 1);
+ 	else {
+ 		/*
+ 		 * Some full blocks, possibly two pieces to clear
+ 		 */
+ 		if (offset < XFS_FSB_TO_B(mp, startoffset_fsb))
+ 			error = xfs_zero_remaining_bytes(ip, offset,
+ 				XFS_FSB_TO_B(mp, startoffset_fsb) - 1);
+ 		if (!error &&
+ 		    XFS_FSB_TO_B(mp, endoffset_fsb) < offset + len)
+ 			error = xfs_zero_remaining_bytes(ip,
+ 				XFS_FSB_TO_B(mp, endoffset_fsb),
+ 				offset + len - 1);
+ 	}
+ 
+ 	/*
+ 	 * free file space until done or until there is an error
+ 	 */
+ 	resblks = XFS_DIOSTRAT_SPACE_RES(mp, 0);
+ 	while (!error && !done) {
+ 
+ 		/*
+ 		 * allocate and setup the transaction. Allow this
+ 		 * transaction to dip into the reserve blocks to ensure
+ 		 * the freeing of the space succeeds at ENOSPC.
+ 		 */
+ 		tp = xfs_trans_alloc(mp, XFS_TRANS_DIOSTRAT);
+ 		tp->t_flags |= XFS_TRANS_RESERVE;
+ 		error = xfs_trans_reserve(tp, &M_RES(mp)->tr_write, resblks, 0);
+ 
+ 		/*
+ 		 * check for running out of space
+ 		 */
+ 		if (error) {
+ 			/*
+ 			 * Free the transaction structure.
+ 			 */
+ 			ASSERT(error == ENOSPC || XFS_FORCED_SHUTDOWN(mp));
+ 			xfs_trans_cancel(tp, 0);
+ 			break;
+ 		}
+ 		xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 		error = xfs_trans_reserve_quota(tp, mp,
+ 				ip->i_udquot, ip->i_gdquot, ip->i_pdquot,
+ 				resblks, 0, XFS_QMOPT_RES_REGBLKS);
+ 		if (error)
+ 			goto error1;
+ 
+ 		xfs_trans_ijoin(tp, ip, 0);
+ 
+ 		/*
+ 		 * issue the bunmapi() call to free the blocks
+ 		 */
+ 		xfs_bmap_init(&free_list, &firstfsb);
+ 		error = xfs_bunmapi(tp, ip, startoffset_fsb,
+ 				  endoffset_fsb - startoffset_fsb,
+ 				  0, 2, &firstfsb, &free_list, &done);
+ 		if (error) {
+ 			goto error0;
+ 		}
+ 
+ 		/*
+ 		 * complete the transaction
+ 		 */
+ 		error = xfs_bmap_finish(&tp, &free_list, &committed);
+ 		if (error) {
+ 			goto error0;
+ 		}
+ 
+ 		error = xfs_trans_commit(tp, XFS_TRANS_RELEASE_LOG_RES);
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	}
+ 
+  out_unlock_iolock:
+ 	if (need_iolock)
+ 		xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+ 	return error;
+ 
+  error0:
+ 	xfs_bmap_cancel(&free_list);
+  error1:
+ 	xfs_trans_cancel(tp, XFS_TRANS_RELEASE_LOG_RES | XFS_TRANS_ABORT);
+ 	xfs_iunlock(ip, need_iolock ? (XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL) :
+ 		    XFS_ILOCK_EXCL);
+ 	return error;
+ }
+ 
+ 
+ STATIC int
+ xfs_zero_file_space(
+ 	struct xfs_inode	*ip,
+ 	xfs_off_t		offset,
+ 	xfs_off_t		len,
+ 	int			attr_flags)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	uint			granularity;
+ 	xfs_off_t		start_boundary;
+ 	xfs_off_t		end_boundary;
+ 	int			error;
+ 
+ 	granularity = max_t(uint, 1 << mp->m_sb.sb_blocklog, PAGE_CACHE_SIZE);
+ 
+ 	/*
+ 	 * Round the range of extents we are going to convert inwards.  If the
+ 	 * offset is aligned, then it doesn't get changed so we zero from the
+ 	 * start of the block offset points to.
+ 	 */
+ 	start_boundary = round_up(offset, granularity);
+ 	end_boundary = round_down(offset + len, granularity);
+ 
+ 	ASSERT(start_boundary >= offset);
+ 	ASSERT(end_boundary <= offset + len);
+ 
+ 	if (!(attr_flags & XFS_ATTR_NOLOCK))
+ 		xfs_ilock(ip, XFS_IOLOCK_EXCL);
+ 
+ 	if (start_boundary < end_boundary - 1) {
+ 		/* punch out the page cache over the conversion range */
+ 		truncate_pagecache_range(VFS_I(ip), start_boundary,
+ 					 end_boundary - 1);
+ 		/* convert the blocks */
+ 		error = xfs_alloc_file_space(ip, start_boundary,
+ 					end_boundary - start_boundary - 1,
+ 					XFS_BMAPI_PREALLOC | XFS_BMAPI_CONVERT,
+ 					attr_flags);
+ 		if (error)
+ 			goto out_unlock;
+ 
+ 		/* We've handled the interior of the range, now for the edges */
+ 		if (start_boundary != offset)
+ 			error = xfs_iozero(ip, offset, start_boundary - offset);
+ 		if (error)
+ 			goto out_unlock;
+ 
+ 		if (end_boundary != offset + len)
+ 			error = xfs_iozero(ip, end_boundary,
+ 					   offset + len - end_boundary);
+ 
+ 	} else {
+ 		/*
+ 		 * It's either a sub-granularity range or the range spanned lies
+ 		 * partially across two adjacent blocks.
+ 		 */
+ 		error = xfs_iozero(ip, offset, len);
+ 	}
+ 
+ out_unlock:
+ 	if (!(attr_flags & XFS_ATTR_NOLOCK))
+ 		xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+ 	return error;
+ 
+ }
+ 
+ /*
+  * xfs_change_file_space()
+  *      This routine allocates or frees disk space for the given file.
+  *      The user specified parameters are checked for alignment and size
+  *      limitations.
+  *
+  * RETURNS:
+  *       0 on success
+  *      errno on error
+  *
+  */
+ int
+ xfs_change_file_space(
+ 	xfs_inode_t	*ip,
+ 	int		cmd,
+ 	xfs_flock64_t	*bf,
+ 	xfs_off_t	offset,
+ 	int		attr_flags)
+ {
+ 	xfs_mount_t	*mp = ip->i_mount;
+ 	int		clrprealloc;
+ 	int		error;
+ 	xfs_fsize_t	fsize;
+ 	int		setprealloc;
+ 	xfs_off_t	startoffset;
+ 	xfs_trans_t	*tp;
+ 	struct iattr	iattr;
+ 
+ 	if (!S_ISREG(ip->i_d.di_mode))
+ 		return XFS_ERROR(EINVAL);
+ 
+ 	switch (bf->l_whence) {
+ 	case 0: /*SEEK_SET*/
+ 		break;
+ 	case 1: /*SEEK_CUR*/
+ 		bf->l_start += offset;
+ 		break;
+ 	case 2: /*SEEK_END*/
+ 		bf->l_start += XFS_ISIZE(ip);
+ 		break;
+ 	default:
+ 		return XFS_ERROR(EINVAL);
+ 	}
+ 
+ 	/*
+ 	 * length of <= 0 for resv/unresv/zero is invalid.  length for
+ 	 * alloc/free is ignored completely and we have no idea what userspace
+ 	 * might have set it to, so set it to zero to allow range
+ 	 * checks to pass.
+ 	 */
+ 	switch (cmd) {
+ 	case XFS_IOC_ZERO_RANGE:
+ 	case XFS_IOC_RESVSP:
+ 	case XFS_IOC_RESVSP64:
+ 	case XFS_IOC_UNRESVSP:
+ 	case XFS_IOC_UNRESVSP64:
+ 		if (bf->l_len <= 0)
+ 			return XFS_ERROR(EINVAL);
+ 		break;
+ 	default:
+ 		bf->l_len = 0;
+ 		break;
+ 	}
+ 
+ 	if (bf->l_start < 0 ||
+ 	    bf->l_start > mp->m_super->s_maxbytes ||
+ 	    bf->l_start + bf->l_len < 0 ||
+ 	    bf->l_start + bf->l_len >= mp->m_super->s_maxbytes)
+ 		return XFS_ERROR(EINVAL);
+ 
+ 	bf->l_whence = 0;
+ 
+ 	startoffset = bf->l_start;
+ 	fsize = XFS_ISIZE(ip);
+ 
+ 	setprealloc = clrprealloc = 0;
+ 	switch (cmd) {
+ 	case XFS_IOC_ZERO_RANGE:
+ 		error = xfs_zero_file_space(ip, startoffset, bf->l_len,
+ 						attr_flags);
+ 		if (error)
+ 			return error;
+ 		setprealloc = 1;
+ 		break;
+ 
+ 	case XFS_IOC_RESVSP:
+ 	case XFS_IOC_RESVSP64:
+ 		error = xfs_alloc_file_space(ip, startoffset, bf->l_len,
+ 						XFS_BMAPI_PREALLOC, attr_flags);
+ 		if (error)
+ 			return error;
+ 		setprealloc = 1;
+ 		break;
+ 
+ 	case XFS_IOC_UNRESVSP:
+ 	case XFS_IOC_UNRESVSP64:
+ 		if ((error = xfs_free_file_space(ip, startoffset, bf->l_len,
+ 								attr_flags)))
+ 			return error;
+ 		break;
+ 
+ 	case XFS_IOC_ALLOCSP:
+ 	case XFS_IOC_ALLOCSP64:
+ 	case XFS_IOC_FREESP:
+ 	case XFS_IOC_FREESP64:
+ 		/*
+ 		 * These operations actually do IO when extending the file, but
+ 		 * the allocation is done seperately to the zeroing that is
+ 		 * done. This set of operations need to be serialised against
+ 		 * other IO operations, such as truncate and buffered IO. We
+ 		 * need to take the IOLOCK here to serialise the allocation and
+ 		 * zeroing IO to prevent other IOLOCK holders (e.g. getbmap,
+ 		 * truncate, direct IO) from racing against the transient
+ 		 * allocated but not written state we can have here.
+ 		 */
+ 		xfs_ilock(ip, XFS_IOLOCK_EXCL);
+ 		if (startoffset > fsize) {
+ 			error = xfs_alloc_file_space(ip, fsize,
+ 					startoffset - fsize, 0,
+ 					attr_flags | XFS_ATTR_NOLOCK);
+ 			if (error) {
+ 				xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+ 				break;
+ 			}
+ 		}
+ 
+ 		iattr.ia_valid = ATTR_SIZE;
+ 		iattr.ia_size = startoffset;
+ 
+ 		error = xfs_setattr_size(ip, &iattr,
+ 					 attr_flags | XFS_ATTR_NOLOCK);
+ 		xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+ 
+ 		if (error)
+ 			return error;
+ 
+ 		clrprealloc = 1;
+ 		break;
+ 
+ 	default:
+ 		ASSERT(0);
+ 		return XFS_ERROR(EINVAL);
+ 	}
+ 
+ 	/*
+ 	 * update the inode timestamp, mode, and prealloc flag bits
+ 	 */
+ 	tp = xfs_trans_alloc(mp, XFS_TRANS_WRITEID);
+ 	error = xfs_trans_reserve(tp, &M_RES(mp)->tr_writeid, 0, 0);
+ 	if (error) {
+ 		xfs_trans_cancel(tp, 0);
+ 		return error;
+ 	}
+ 
+ 	xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 	xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
+ 
+ 	if ((attr_flags & XFS_ATTR_DMI) == 0) {
+ 		ip->i_d.di_mode &= ~S_ISUID;
+ 
+ 		/*
+ 		 * Note that we don't have to worry about mandatory
+ 		 * file locking being disabled here because we only
+ 		 * clear the S_ISGID bit if the Group execute bit is
+ 		 * on, but if it was on then mandatory locking wouldn't
+ 		 * have been enabled.
+ 		 */
+ 		if (ip->i_d.di_mode & S_IXGRP)
+ 			ip->i_d.di_mode &= ~S_ISGID;
+ 
+ 		xfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_MOD | XFS_ICHGTIME_CHG);
+ 	}
+ 	if (setprealloc)
+ 		ip->i_d.di_flags |= XFS_DIFLAG_PREALLOC;
+ 	else if (clrprealloc)
+ 		ip->i_d.di_flags &= ~XFS_DIFLAG_PREALLOC;
+ 
+ 	xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);
+ 	if (attr_flags & XFS_ATTR_SYNC)
+ 		xfs_trans_set_sync(tp);
+ 	return xfs_trans_commit(tp, 0);
+ }
+ 
+ /*
+  * We need to check that the format of the data fork in the temporary inode is
+  * valid for the target inode before doing the swap. This is not a problem with
+  * attr1 because of the fixed fork offset, but attr2 has a dynamically sized
+  * data fork depending on the space the attribute fork is taking so we can get
+  * invalid formats on the target inode.
+  *
+  * E.g. target has space for 7 extents in extent format, temp inode only has
+  * space for 6.  If we defragment down to 7 extents, then the tmp format is a
+  * btree, but when swapped it needs to be in extent format. Hence we can't just
+  * blindly swap data forks on attr2 filesystems.
+  *
+  * Note that we check the swap in both directions so that we don't end up with
+  * a corrupt temporary inode, either.
+  *
+  * Note that fixing the way xfs_fsr sets up the attribute fork in the source
+  * inode will prevent this situation from occurring, so all we do here is
+  * reject and log the attempt. basically we are putting the responsibility on
+  * userspace to get this right.
+  */
+ static int
+ xfs_swap_extents_check_format(
+ 	xfs_inode_t	*ip,	/* target inode */
+ 	xfs_inode_t	*tip)	/* tmp inode */
+ {
+ 
+ 	/* Should never get a local format */
+ 	if (ip->i_d.di_format == XFS_DINODE_FMT_LOCAL ||
+ 	    tip->i_d.di_format == XFS_DINODE_FMT_LOCAL)
+ 		return EINVAL;
+ 
+ 	/*
+ 	 * if the target inode has less extents that then temporary inode then
+ 	 * why did userspace call us?
+ 	 */
+ 	if (ip->i_d.di_nextents < tip->i_d.di_nextents)
+ 		return EINVAL;
+ 
+ 	/*
+ 	 * if the target inode is in extent form and the temp inode is in btree
+ 	 * form then we will end up with the target inode in the wrong format
+ 	 * as we already know there are less extents in the temp inode.
+ 	 */
+ 	if (ip->i_d.di_format == XFS_DINODE_FMT_EXTENTS &&
+ 	    tip->i_d.di_format == XFS_DINODE_FMT_BTREE)
+ 		return EINVAL;
+ 
+ 	/* Check temp in extent form to max in target */
+ 	if (tip->i_d.di_format == XFS_DINODE_FMT_EXTENTS &&
+ 	    XFS_IFORK_NEXTENTS(tip, XFS_DATA_FORK) >
+ 			XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK))
+ 		return EINVAL;
+ 
+ 	/* Check target in extent form to max in temp */
+ 	if (ip->i_d.di_format == XFS_DINODE_FMT_EXTENTS &&
+ 	    XFS_IFORK_NEXTENTS(ip, XFS_DATA_FORK) >
+ 			XFS_IFORK_MAXEXT(tip, XFS_DATA_FORK))
+ 		return EINVAL;
+ 
+ 	/*
+ 	 * If we are in a btree format, check that the temp root block will fit
+ 	 * in the target and that it has enough extents to be in btree format
+ 	 * in the target.
+ 	 *
+ 	 * Note that we have to be careful to allow btree->extent conversions
+ 	 * (a common defrag case) which will occur when the temp inode is in
+ 	 * extent format...
+ 	 */
+ 	if (tip->i_d.di_format == XFS_DINODE_FMT_BTREE) {
+ 		if (XFS_IFORK_BOFF(ip) &&
+ 		    XFS_BMAP_BMDR_SPACE(tip->i_df.if_broot) > XFS_IFORK_BOFF(ip))
+ 			return EINVAL;
+ 		if (XFS_IFORK_NEXTENTS(tip, XFS_DATA_FORK) <=
+ 		    XFS_IFORK_MAXEXT(ip, XFS_DATA_FORK))
+ 			return EINVAL;
+ 	}
+ 
+ 	/* Reciprocal target->temp btree format checks */
+ 	if (ip->i_d.di_format == XFS_DINODE_FMT_BTREE) {
+ 		if (XFS_IFORK_BOFF(tip) &&
+ 		    XFS_BMAP_BMDR_SPACE(ip->i_df.if_broot) > XFS_IFORK_BOFF(tip))
+ 			return EINVAL;
+ 		if (XFS_IFORK_NEXTENTS(ip, XFS_DATA_FORK) <=
+ 		    XFS_IFORK_MAXEXT(tip, XFS_DATA_FORK))
+ 			return EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int
+ xfs_swap_extents(
+ 	xfs_inode_t	*ip,	/* target inode */
+ 	xfs_inode_t	*tip,	/* tmp inode */
+ 	xfs_swapext_t	*sxp)
+ {
+ 	xfs_mount_t	*mp = ip->i_mount;
+ 	xfs_trans_t	*tp;
+ 	xfs_bstat_t	*sbp = &sxp->sx_stat;
+ 	xfs_ifork_t	*tempifp, *ifp, *tifp;
+ 	int		src_log_flags, target_log_flags;
+ 	int		error = 0;
+ 	int		aforkblks = 0;
+ 	int		taforkblks = 0;
+ 	__uint64_t	tmp;
+ 
+ 	tempifp = kmem_alloc(sizeof(xfs_ifork_t), KM_MAYFAIL);
+ 	if (!tempifp) {
+ 		error = XFS_ERROR(ENOMEM);
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * we have to do two separate lock calls here to keep lockdep
+ 	 * happy. If we try to get all the locks in one call, lock will
+ 	 * report false positives when we drop the ILOCK and regain them
+ 	 * below.
+ 	 */
+ 	xfs_lock_two_inodes(ip, tip, XFS_IOLOCK_EXCL);
+ 	xfs_lock_two_inodes(ip, tip, XFS_ILOCK_EXCL);
+ 
+ 	/* Verify that both files have the same format */
+ 	if ((ip->i_d.di_mode & S_IFMT) != (tip->i_d.di_mode & S_IFMT)) {
+ 		error = XFS_ERROR(EINVAL);
+ 		goto out_unlock;
+ 	}
+ 
+ 	/* Verify both files are either real-time or non-realtime */
+ 	if (XFS_IS_REALTIME_INODE(ip) != XFS_IS_REALTIME_INODE(tip)) {
+ 		error = XFS_ERROR(EINVAL);
+ 		goto out_unlock;
+ 	}
+ 
+ 	error = -filemap_write_and_wait(VFS_I(tip)->i_mapping);
+ 	if (error)
+ 		goto out_unlock;
+ 	truncate_pagecache_range(VFS_I(tip), 0, -1);
+ 
+ 	/* Verify O_DIRECT for ftmp */
+ 	if (VN_CACHED(VFS_I(tip)) != 0) {
+ 		error = XFS_ERROR(EINVAL);
+ 		goto out_unlock;
+ 	}
+ 
+ 	/* Verify all data are being swapped */
+ 	if (sxp->sx_offset != 0 ||
+ 	    sxp->sx_length != ip->i_d.di_size ||
+ 	    sxp->sx_length != tip->i_d.di_size) {
+ 		error = XFS_ERROR(EFAULT);
+ 		goto out_unlock;
+ 	}
+ 
+ 	trace_xfs_swap_extent_before(ip, 0);
+ 	trace_xfs_swap_extent_before(tip, 1);
+ 
+ 	/* check inode formats now that data is flushed */
+ 	error = xfs_swap_extents_check_format(ip, tip);
+ 	if (error) {
+ 		xfs_notice(mp,
+ 		    "%s: inode 0x%llx format is incompatible for exchanging.",
+ 				__func__, ip->i_ino);
+ 		goto out_unlock;
+ 	}
+ 
+ 	/*
+ 	 * Compare the current change & modify times with that
+ 	 * passed in.  If they differ, we abort this swap.
+ 	 * This is the mechanism used to ensure the calling
+ 	 * process that the file was not changed out from
+ 	 * under it.
+ 	 */
+ 	if ((sbp->bs_ctime.tv_sec != VFS_I(ip)->i_ctime.tv_sec) ||
+ 	    (sbp->bs_ctime.tv_nsec != VFS_I(ip)->i_ctime.tv_nsec) ||
+ 	    (sbp->bs_mtime.tv_sec != VFS_I(ip)->i_mtime.tv_sec) ||
+ 	    (sbp->bs_mtime.tv_nsec != VFS_I(ip)->i_mtime.tv_nsec)) {
+ 		error = XFS_ERROR(EBUSY);
+ 		goto out_unlock;
+ 	}
+ 
+ 	/* We need to fail if the file is memory mapped.  Once we have tossed
+ 	 * all existing pages, the page fault will have no option
+ 	 * but to go to the filesystem for pages. By making the page fault call
+ 	 * vop_read (or write in the case of autogrow) they block on the iolock
+ 	 * until we have switched the extents.
+ 	 */
+ 	if (VN_MAPPED(VFS_I(ip))) {
+ 		error = XFS_ERROR(EBUSY);
+ 		goto out_unlock;
+ 	}
+ 
+ 	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	xfs_iunlock(tip, XFS_ILOCK_EXCL);
+ 
+ 	/*
+ 	 * There is a race condition here since we gave up the
+ 	 * ilock.  However, the data fork will not change since
+ 	 * we have the iolock (locked for truncation too) so we
+ 	 * are safe.  We don't really care if non-io related
+ 	 * fields change.
+ 	 */
+ 	truncate_pagecache_range(VFS_I(ip), 0, -1);
+ 
+ 	tp = xfs_trans_alloc(mp, XFS_TRANS_SWAPEXT);
+ 	error = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);
+ 	if (error) {
+ 		xfs_iunlock(ip,  XFS_IOLOCK_EXCL);
+ 		xfs_iunlock(tip, XFS_IOLOCK_EXCL);
+ 		xfs_trans_cancel(tp, 0);
+ 		goto out;
+ 	}
+ 	xfs_lock_two_inodes(ip, tip, XFS_ILOCK_EXCL);
+ 
+ 	/*
+ 	 * Count the number of extended attribute blocks
+ 	 */
+ 	if ( ((XFS_IFORK_Q(ip) != 0) && (ip->i_d.di_anextents > 0)) &&
+ 	     (ip->i_d.di_aformat != XFS_DINODE_FMT_LOCAL)) {
+ 		error = xfs_bmap_count_blocks(tp, ip, XFS_ATTR_FORK, &aforkblks);
+ 		if (error)
+ 			goto out_trans_cancel;
+ 	}
+ 	if ( ((XFS_IFORK_Q(tip) != 0) && (tip->i_d.di_anextents > 0)) &&
+ 	     (tip->i_d.di_aformat != XFS_DINODE_FMT_LOCAL)) {
+ 		error = xfs_bmap_count_blocks(tp, tip, XFS_ATTR_FORK,
+ 			&taforkblks);
+ 		if (error)
+ 			goto out_trans_cancel;
+ 	}
+ 
+ 	xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL);
+ 	xfs_trans_ijoin(tp, tip, XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL);
+ 
+ 	/*
+ 	 * Before we've swapped the forks, lets set the owners of the forks
+ 	 * appropriately. We have to do this as we are demand paging the btree
+ 	 * buffers, and so the validation done on read will expect the owner
+ 	 * field to be correctly set. Once we change the owners, we can swap the
+ 	 * inode forks.
+ 	 *
+ 	 * Note the trickiness in setting the log flags - we set the owner log
+ 	 * flag on the opposite inode (i.e. the inode we are setting the new
+ 	 * owner to be) because once we swap the forks and log that, log
+ 	 * recovery is going to see the fork as owned by the swapped inode,
+ 	 * not the pre-swapped inodes.
+ 	 */
+ 	src_log_flags = XFS_ILOG_CORE;
+ 	target_log_flags = XFS_ILOG_CORE;
+ 	if (ip->i_d.di_version == 3 &&
+ 	    ip->i_d.di_format == XFS_DINODE_FMT_BTREE) {
+ 		target_log_flags |= XFS_ILOG_OWNER;
+ 		error = xfs_bmbt_change_owner(tp, ip, XFS_DATA_FORK, tip->i_ino);
+ 		if (error)
+ 			goto out_trans_cancel;
+ 	}
+ 
+ 	if (tip->i_d.di_version == 3 &&
+ 	    tip->i_d.di_format == XFS_DINODE_FMT_BTREE) {
+ 		src_log_flags |= XFS_ILOG_OWNER;
+ 		error = xfs_bmbt_change_owner(tp, tip, XFS_DATA_FORK, ip->i_ino);
+ 		if (error)
+ 			goto out_trans_cancel;
+ 	}
+ 
+ 	/*
+ 	 * Swap the data forks of the inodes
+ 	 */
+ 	ifp = &ip->i_df;
+ 	tifp = &tip->i_df;
+ 	*tempifp = *ifp;	/* struct copy */
+ 	*ifp = *tifp;		/* struct copy */
+ 	*tifp = *tempifp;	/* struct copy */
+ 
+ 	/*
+ 	 * Fix the on-disk inode values
+ 	 */
+ 	tmp = (__uint64_t)ip->i_d.di_nblocks;
+ 	ip->i_d.di_nblocks = tip->i_d.di_nblocks - taforkblks + aforkblks;
+ 	tip->i_d.di_nblocks = tmp + taforkblks - aforkblks;
+ 
+ 	tmp = (__uint64_t) ip->i_d.di_nextents;
+ 	ip->i_d.di_nextents = tip->i_d.di_nextents;
+ 	tip->i_d.di_nextents = tmp;
+ 
+ 	tmp = (__uint64_t) ip->i_d.di_format;
+ 	ip->i_d.di_format = tip->i_d.di_format;
+ 	tip->i_d.di_format = tmp;
+ 
+ 	/*
+ 	 * The extents in the source inode could still contain speculative
+ 	 * preallocation beyond EOF (e.g. the file is open but not modified
+ 	 * while defrag is in progress). In that case, we need to copy over the
+ 	 * number of delalloc blocks the data fork in the source inode is
+ 	 * tracking beyond EOF so that when the fork is truncated away when the
+ 	 * temporary inode is unlinked we don't underrun the i_delayed_blks
+ 	 * counter on that inode.
+ 	 */
+ 	ASSERT(tip->i_delayed_blks == 0);
+ 	tip->i_delayed_blks = ip->i_delayed_blks;
+ 	ip->i_delayed_blks = 0;
+ 
+ 	switch (ip->i_d.di_format) {
+ 	case XFS_DINODE_FMT_EXTENTS:
+ 		/* If the extents fit in the inode, fix the
+ 		 * pointer.  Otherwise it's already NULL or
+ 		 * pointing to the extent.
+ 		 */
+ 		if (ip->i_d.di_nextents <= XFS_INLINE_EXTS) {
+ 			ifp->if_u1.if_extents =
+ 				ifp->if_u2.if_inline_ext;
+ 		}
+ 		src_log_flags |= XFS_ILOG_DEXT;
+ 		break;
+ 	case XFS_DINODE_FMT_BTREE:
+ 		ASSERT(ip->i_d.di_version < 3 ||
+ 		       (src_log_flags & XFS_ILOG_OWNER));
+ 		src_log_flags |= XFS_ILOG_DBROOT;
+ 		break;
+ 	}
+ 
+ 	switch (tip->i_d.di_format) {
+ 	case XFS_DINODE_FMT_EXTENTS:
+ 		/* If the extents fit in the inode, fix the
+ 		 * pointer.  Otherwise it's already NULL or
+ 		 * pointing to the extent.
+ 		 */
+ 		if (tip->i_d.di_nextents <= XFS_INLINE_EXTS) {
+ 			tifp->if_u1.if_extents =
+ 				tifp->if_u2.if_inline_ext;
+ 		}
+ 		target_log_flags |= XFS_ILOG_DEXT;
+ 		break;
+ 	case XFS_DINODE_FMT_BTREE:
+ 		target_log_flags |= XFS_ILOG_DBROOT;
+ 		ASSERT(tip->i_d.di_version < 3 ||
+ 		       (target_log_flags & XFS_ILOG_OWNER));
+ 		break;
+ 	}
+ 
+ 	xfs_trans_log_inode(tp, ip,  src_log_flags);
+ 	xfs_trans_log_inode(tp, tip, target_log_flags);
+ 
+ 	/*
+ 	 * If this is a synchronous mount, make sure that the
+ 	 * transaction goes to disk before returning to the user.
+ 	 */
+ 	if (mp->m_flags & XFS_MOUNT_WSYNC)
+ 		xfs_trans_set_sync(tp);
+ 
+ 	error = xfs_trans_commit(tp, 0);
+ 
+ 	trace_xfs_swap_extent_after(ip, 0);
+ 	trace_xfs_swap_extent_after(tip, 1);
+ out:
+ 	kmem_free(tempifp);
+ 	return error;
+ 
+ out_unlock:
+ 	xfs_iunlock(ip,  XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL);
+ 	xfs_iunlock(tip, XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL);
+ 	goto out;
+ 
+ out_trans_cancel:
+ 	xfs_trans_cancel(tp, 0);
+ 	goto out_unlock;
+ }
++>>>>>>> 21b5c9784bce (xfs: swap extents operations for CRC filesystems)
diff --git a/fs/xfs/xfs_bmap_btree.c b/fs/xfs/xfs_bmap_btree.c
index cf3bc76710c3..aa2eadd41bab 100644
--- a/fs/xfs/xfs_bmap_btree.c
+++ b/fs/xfs/xfs_bmap_btree.c
@@ -925,3 +925,37 @@ xfs_bmdr_maxrecs(
 		return blocklen / sizeof(xfs_bmdr_rec_t);
 	return blocklen / (sizeof(xfs_bmdr_key_t) + sizeof(xfs_bmdr_ptr_t));
 }
+
+/*
+ * Change the owner of a btree format fork fo the inode passed in. Change it to
+ * the owner of that is passed in so that we can change owners before or after
+ * we switch forks between inodes. The operation that the caller is doing will
+ * determine whether is needs to change owner before or after the switch.
+ *
+ * For demand paged modification, the fork switch should be done after reading
+ * in all the blocks, modifying them and pinning them in the transaction. For
+ * modification when the buffers are already pinned in memory, the fork switch
+ * can be done before changing the owner as we won't need to validate the owner
+ * until the btree buffers are unpinned and writes can occur again.
+ */
+int
+xfs_bmbt_change_owner(
+	struct xfs_trans	*tp,
+	struct xfs_inode	*ip,
+	int			whichfork,
+	xfs_ino_t		new_owner)
+{
+	struct xfs_btree_cur	*cur;
+	int			error;
+
+	if (whichfork == XFS_DATA_FORK)
+		ASSERT(ip->i_d.di_format = XFS_DINODE_FMT_BTREE);
+	else
+		ASSERT(ip->i_d.di_aformat = XFS_DINODE_FMT_BTREE);
+
+	cur = xfs_bmbt_init_cursor(ip->i_mount, tp, ip, whichfork);
+	error = xfs_btree_change_owner(cur, new_owner);
+	xfs_btree_del_cursor(cur, error ? XFS_BTREE_ERROR : XFS_BTREE_NOERROR);
+	return error;
+}
+
diff --git a/fs/xfs/xfs_bmap_btree.h b/fs/xfs/xfs_bmap_btree.h
index 1b726d626941..bceac7affa27 100644
--- a/fs/xfs/xfs_bmap_btree.h
+++ b/fs/xfs/xfs_bmap_btree.h
@@ -236,6 +236,9 @@ extern int xfs_bmbt_get_maxrecs(struct xfs_btree_cur *, int level);
 extern int xfs_bmdr_maxrecs(struct xfs_mount *, int blocklen, int leaf);
 extern int xfs_bmbt_maxrecs(struct xfs_mount *, int blocklen, int leaf);
 
+extern int xfs_bmbt_change_owner(struct xfs_trans *tp, struct xfs_inode *ip,
+				 int whichfork, xfs_ino_t new_owner);
+
 extern struct xfs_btree_cur *xfs_bmbt_init_cursor(struct xfs_mount *,
 		struct xfs_trans *, struct xfs_inode *, int);
 
* Unmerged path fs/xfs/xfs_bmap_util.c
diff --git a/fs/xfs/xfs_btree.c b/fs/xfs/xfs_btree.c
index 7a2b4da3c0db..047573f02702 100644
--- a/fs/xfs/xfs_btree.c
+++ b/fs/xfs/xfs_btree.c
@@ -855,6 +855,41 @@ xfs_btree_readahead(
 	return xfs_btree_readahead_sblock(cur, lr, block);
 }
 
+STATIC xfs_daddr_t
+xfs_btree_ptr_to_daddr(
+	struct xfs_btree_cur	*cur,
+	union xfs_btree_ptr	*ptr)
+{
+	if (cur->bc_flags & XFS_BTREE_LONG_PTRS) {
+		ASSERT(ptr->l != cpu_to_be64(NULLDFSBNO));
+
+		return XFS_FSB_TO_DADDR(cur->bc_mp, be64_to_cpu(ptr->l));
+	} else {
+		ASSERT(cur->bc_private.a.agno != NULLAGNUMBER);
+		ASSERT(ptr->s != cpu_to_be32(NULLAGBLOCK));
+
+		return XFS_AGB_TO_DADDR(cur->bc_mp, cur->bc_private.a.agno,
+					be32_to_cpu(ptr->s));
+	}
+}
+
+/*
+ * Readahead @count btree blocks at the given @ptr location.
+ *
+ * We don't need to care about long or short form btrees here as we have a
+ * method of converting the ptr directly to a daddr available to us.
+ */
+STATIC void
+xfs_btree_readahead_ptr(
+	struct xfs_btree_cur	*cur,
+	union xfs_btree_ptr	*ptr,
+	xfs_extlen_t		count)
+{
+	xfs_buf_readahead(cur->bc_mp->m_ddev_targp,
+			  xfs_btree_ptr_to_daddr(cur, ptr),
+			  cur->bc_mp->m_bsize * count, cur->bc_ops->buf_ops);
+}
+
 /*
  * Set the buffer for level "lev" in the cursor to bp, releasing
  * any previous buffer.
@@ -1073,24 +1108,6 @@ xfs_btree_buf_to_ptr(
 	}
 }
 
-STATIC xfs_daddr_t
-xfs_btree_ptr_to_daddr(
-	struct xfs_btree_cur	*cur,
-	union xfs_btree_ptr	*ptr)
-{
-	if (cur->bc_flags & XFS_BTREE_LONG_PTRS) {
-		ASSERT(ptr->l != cpu_to_be64(NULLDFSBNO));
-
-		return XFS_FSB_TO_DADDR(cur->bc_mp, be64_to_cpu(ptr->l));
-	} else {
-		ASSERT(cur->bc_private.a.agno != NULLAGNUMBER);
-		ASSERT(ptr->s != cpu_to_be32(NULLAGBLOCK));
-
-		return XFS_AGB_TO_DADDR(cur->bc_mp, cur->bc_private.a.agno,
-					be32_to_cpu(ptr->s));
-	}
-}
-
 STATIC void
 xfs_btree_set_refs(
 	struct xfs_btree_cur	*cur,
@@ -3869,3 +3886,112 @@ xfs_btree_get_rec(
 	*stat = 1;
 	return 0;
 }
+
+/*
+ * Change the owner of a btree.
+ *
+ * The mechanism we use here is ordered buffer logging. Because we don't know
+ * how many buffers were are going to need to modify, we don't really want to
+ * have to make transaction reservations for the worst case of every buffer in a
+ * full size btree as that may be more space that we can fit in the log....
+ *
+ * We do the btree walk in the most optimal manner possible - we have sibling
+ * pointers so we can just walk all the blocks on each level from left to right
+ * in a single pass, and then move to the next level and do the same. We can
+ * also do readahead on the sibling pointers to get IO moving more quickly,
+ * though for slow disks this is unlikely to make much difference to performance
+ * as the amount of CPU work we have to do before moving to the next block is
+ * relatively small.
+ *
+ * For each btree block that we load, modify the owner appropriately, set the
+ * buffer as an ordered buffer and log it appropriately. We need to ensure that
+ * we mark the region we change dirty so that if the buffer is relogged in
+ * a subsequent transaction the changes we make here as an ordered buffer are
+ * correctly relogged in that transaction.
+ */
+static int
+xfs_btree_block_change_owner(
+	struct xfs_btree_cur	*cur,
+	int			level,
+	__uint64_t		new_owner)
+{
+	struct xfs_btree_block	*block;
+	struct xfs_buf		*bp;
+	union xfs_btree_ptr     rptr;
+
+	/* do right sibling readahead */
+	xfs_btree_readahead(cur, level, XFS_BTCUR_RIGHTRA);
+
+	/* modify the owner */
+	block = xfs_btree_get_block(cur, level, &bp);
+	if (cur->bc_flags & XFS_BTREE_LONG_PTRS)
+		block->bb_u.l.bb_owner = cpu_to_be64(new_owner);
+	else
+		block->bb_u.s.bb_owner = cpu_to_be32(new_owner);
+
+	/*
+	 * Log owner change as an ordered buffer. If the block is a root block
+	 * hosted in an inode, we might not have a buffer pointer here and we
+	 * shouldn't attempt to log the change as the information is already
+	 * held in the inode and discarded when the root block is formatted into
+	 * the on-disk inode fork. We still change it, though, so everything is
+	 * consistent in memory.
+	 */
+	if (bp) {
+		xfs_trans_ordered_buf(cur->bc_tp, bp);
+		xfs_btree_log_block(cur, bp, XFS_BB_OWNER);
+	} else {
+		ASSERT(cur->bc_flags & XFS_BTREE_ROOT_IN_INODE);
+		ASSERT(level == cur->bc_nlevels - 1);
+	}
+
+	/* now read rh sibling block for next iteration */
+	xfs_btree_get_sibling(cur, block, &rptr, XFS_BB_RIGHTSIB);
+	if (xfs_btree_ptr_is_null(cur, &rptr))
+		return ENOENT;
+
+	return xfs_btree_lookup_get_block(cur, level, &rptr, &block);
+}
+
+int
+xfs_btree_change_owner(
+	struct xfs_btree_cur	*cur,
+	__uint64_t		new_owner)
+{
+	union xfs_btree_ptr     lptr;
+	int			level;
+	struct xfs_btree_block	*block = NULL;
+	int			error = 0;
+
+	cur->bc_ops->init_ptr_from_cur(cur, &lptr);
+
+	/* for each level */
+	for (level = cur->bc_nlevels - 1; level >= 0; level--) {
+		/* grab the left hand block */
+		error = xfs_btree_lookup_get_block(cur, level, &lptr, &block);
+		if (error)
+			return error;
+
+		/* readahead the left most block for the next level down */
+		if (level > 0) {
+			union xfs_btree_ptr     *ptr;
+
+			ptr = xfs_btree_ptr_addr(cur, 1, block);
+			xfs_btree_readahead_ptr(cur, ptr, 1);
+
+			/* save for the next iteration of the loop */
+			lptr = *ptr;
+		}
+
+		/* for each buffer in the level */
+		do {
+			error = xfs_btree_block_change_owner(cur, level,
+							     new_owner);
+		} while (!error);
+
+		if (error != ENOENT)
+			return error;
+	}
+
+	return 0;
+}
diff --git a/fs/xfs/xfs_btree.h b/fs/xfs/xfs_btree.h
index c8473c7ef45e..544b209e0256 100644
--- a/fs/xfs/xfs_btree.h
+++ b/fs/xfs/xfs_btree.h
@@ -121,15 +121,18 @@ union xfs_btree_rec {
 /*
  * For logging record fields.
  */
-#define	XFS_BB_MAGIC		0x01
-#define	XFS_BB_LEVEL		0x02
-#define	XFS_BB_NUMRECS		0x04
-#define	XFS_BB_LEFTSIB		0x08
-#define	XFS_BB_RIGHTSIB		0x10
-#define	XFS_BB_BLKNO		0x20
+#define	XFS_BB_MAGIC		(1 << 0)
+#define	XFS_BB_LEVEL		(1 << 1)
+#define	XFS_BB_NUMRECS		(1 << 2)
+#define	XFS_BB_LEFTSIB		(1 << 3)
+#define	XFS_BB_RIGHTSIB		(1 << 4)
+#define	XFS_BB_BLKNO		(1 << 5)
+#define	XFS_BB_LSN		(1 << 6)
+#define	XFS_BB_UUID		(1 << 7)
+#define	XFS_BB_OWNER		(1 << 8)
 #define	XFS_BB_NUM_BITS		5
 #define	XFS_BB_ALL_BITS		((1 << XFS_BB_NUM_BITS) - 1)
-#define	XFS_BB_NUM_BITS_CRC	8
+#define	XFS_BB_NUM_BITS_CRC	9
 #define	XFS_BB_ALL_BITS_CRC	((1 << XFS_BB_NUM_BITS_CRC) - 1)
 
 /*
@@ -442,6 +445,7 @@ int xfs_btree_new_iroot(struct xfs_btree_cur *, int *, int *);
 int xfs_btree_insert(struct xfs_btree_cur *, int *);
 int xfs_btree_delete(struct xfs_btree_cur *, int *);
 int xfs_btree_get_rec(struct xfs_btree_cur *, union xfs_btree_rec **, int *);
+int xfs_btree_change_owner(struct xfs_btree_cur *cur, __uint64_t new_owner);
 
 /*
  * btree block CRC helpers
diff --git a/fs/xfs/xfs_log_format.h b/fs/xfs/xfs_log_format.h
index 31e3a06c4644..08a6fbe03bb6 100644
--- a/fs/xfs/xfs_log_format.h
+++ b/fs/xfs/xfs_log_format.h
@@ -474,6 +474,7 @@ typedef struct xfs_inode_log_format_64 {
 #define	XFS_ILOG_ADATA	0x040	/* log i_af.if_data */
 #define	XFS_ILOG_AEXT	0x080	/* log i_af.if_extents */
 #define	XFS_ILOG_ABROOT	0x100	/* log i_af.i_broot */
+#define XFS_ILOG_OWNER	0x200	/* change the extent tree owner on replay */
 
 
 /*
