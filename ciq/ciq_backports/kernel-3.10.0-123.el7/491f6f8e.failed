lockref: use arch_mutex_cpu_relax() in CMPXCHG_LOOP()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Heiko Carstens <heiko.carstens@de.ibm.com>
commit 491f6f8e5fd9a57aaf03b6d6e3e153f1c27d8a46
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/491f6f8e.failed

Make use of arch_mutex_cpu_relax() so architectures can override the
default cpu_relax() semantics.
This is especially useful for s390, where cpu_relax() means that we
yield() the current (virtual) cpu and therefore is very expensive,
and would contradict the whole purpose of the lockless cmpxchg loop.

	Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
(cherry picked from commit 491f6f8e5fd9a57aaf03b6d6e3e153f1c27d8a46)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/lockref.c
diff --cc lib/lockref.c
index 01ba8088f424,6f9d434c1521..000000000000
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@@ -1,6 -1,51 +1,54 @@@
  #include <linux/export.h>
  #include <linux/lockref.h>
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_CMPXCHG_LOCKREF
+ 
+ /*
+  * Allow weakly-ordered memory architectures to provide barrier-less
+  * cmpxchg semantics for lockref updates.
+  */
+ #ifndef cmpxchg64_relaxed
+ # define cmpxchg64_relaxed cmpxchg64
+ #endif
+ 
+ /*
+  * Allow architectures to override the default cpu_relax() within CMPXCHG_LOOP.
+  * This is useful for architectures with an expensive cpu_relax().
+  */
+ #ifndef arch_mutex_cpu_relax
+ # define arch_mutex_cpu_relax() cpu_relax()
+ #endif
+ 
+ /*
+  * Note that the "cmpxchg()" reloads the "old" value for the
+  * failure case.
+  */
+ #define CMPXCHG_LOOP(CODE, SUCCESS) do {					\
+ 	struct lockref old;							\
+ 	BUILD_BUG_ON(sizeof(old) != 8);						\
+ 	old.lock_count = ACCESS_ONCE(lockref->lock_count);			\
+ 	while (likely(arch_spin_value_unlocked(old.lock.rlock.raw_lock))) {  	\
+ 		struct lockref new = old, prev = old;				\
+ 		CODE								\
+ 		old.lock_count = cmpxchg64_relaxed(&lockref->lock_count,	\
+ 						   old.lock_count,		\
+ 						   new.lock_count);		\
+ 		if (likely(old.lock_count == prev.lock_count)) {		\
+ 			SUCCESS;						\
+ 		}								\
+ 		arch_mutex_cpu_relax();						\
+ 	}									\
+ } while (0)
+ 
+ #else
+ 
+ #define CMPXCHG_LOOP(CODE, SUCCESS) do { } while (0)
+ 
+ #endif
+ 
++>>>>>>> 491f6f8e5fd9 (lockref: use arch_mutex_cpu_relax() in CMPXCHG_LOOP())
  /**
   * lockref_get - Increments reference count unconditionally
   * @lockref: pointer to lockref structure
* Unmerged path lib/lockref.c
