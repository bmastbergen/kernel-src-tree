blk-throttle: make tg_dispatch_one_bio() ready for hierarchy

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Tejun Heo <tj@kernel.org>
commit 6bc9c2b464fb89eab705da87aa4284171d942369
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/6bc9c2b4.failed

tg_dispatch_one_bio() currently assumes that the parent_sq is the top
level one and the bio being dispatched is ready to be issued; however,
this assumption will be wrong with proper hierarchy support.  This
patch makes the following changes to make tg_dispatch_on_bio() ready
for hiearchy.

* throtl_data->nr_queued[] is incremented in blk_throtl_bio() instead
  of throtl_add_bio_tg() so that throtl_add_bio_tg() can be used to
  transfer a bio from a child tg to its parent.

* tg_dispatch_one_bio() is updated to distinguish whether its parent
  is another throtl_grp or the throtl_data.  If former, the bio is
  transferred to the parent throtl_grp using throtl_add_bio_tg().  If
  latter, the bio is ready to be issued and put on the top-level
  service_queue's bio_lists[] and throtl_data->nr_queued is
  decremented.

As all throtl_grps currently have the top level service_queue as their
->parent_sq, this patch in itself doesn't make any behavior
difference.

	Signed-off-by: Tejun Heo <tj@kernel.org>
	Acked-by: Vivek Goyal <vgoyal@redhat.com>
(cherry picked from commit 6bc9c2b464fb89eab705da87aa4284171d942369)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-throttle.c
diff --cc block/blk-throttle.c
index e65e45a33372,04202617fda5..000000000000
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@@ -726,12 -824,10 +726,16 @@@ static void throtl_add_bio_tg(struct bi
  	/* Take a bio reference on tg */
  	blkg_get(tg_to_blkg(tg));
  	sq->nr_queued[rw]++;
++<<<<<<< HEAD
 +	tg->td->nr_queued[rw]++;
 +	throtl_enqueue_tg(tg, parent_sq);
++=======
+ 	throtl_enqueue_tg(tg);
++>>>>>>> 6bc9c2b464fb (blk-throttle: make tg_dispatch_one_bio() ready for hierarchy)
  }
  
 -static void tg_update_disptime(struct throtl_grp *tg)
 +static void tg_update_disptime(struct throtl_grp *tg,
 +			       struct throtl_service_queue *parent_sq)
  {
  	struct throtl_service_queue *sq = &tg->service_queue;
  	unsigned long read_wait = -1, write_wait = -1, min_wait = -1, disptime;
@@@ -755,28 -851,40 +759,46 @@@
  	tg->flags &= ~THROTL_TG_WAS_EMPTY;
  }
  
 -static void tg_dispatch_one_bio(struct throtl_grp *tg, bool rw)
 +static void tg_dispatch_one_bio(struct throtl_grp *tg, bool rw,
 +				struct bio_list *bl)
  {
  	struct throtl_service_queue *sq = &tg->service_queue;
+ 	struct throtl_service_queue *parent_sq = sq->parent_sq;
+ 	struct throtl_grp *parent_tg = sq_to_tg(parent_sq);
  	struct bio *bio;
  
  	bio = bio_list_pop(&sq->bio_lists[rw]);
  	sq->nr_queued[rw]--;
- 	/* Drop bio reference on blkg */
- 	blkg_put(tg_to_blkg(tg));
- 
- 	BUG_ON(tg->td->nr_queued[rw] <= 0);
- 	tg->td->nr_queued[rw]--;
  
  	throtl_charge_bio(tg, bio);
++<<<<<<< HEAD
 +	bio_list_add(bl, bio);
 +	bio->bi_rw |= REQ_THROTTLED;
++=======
+ 
+ 	/*
+ 	 * If our parent is another tg, we just need to transfer @bio to
+ 	 * the parent using throtl_add_bio_tg().  If our parent is
+ 	 * @td->service_queue, @bio is ready to be issued.  Put it on its
+ 	 * bio_lists[] and decrease total number queued.  The caller is
+ 	 * responsible for issuing these bios.
+ 	 */
+ 	if (parent_tg) {
+ 		throtl_add_bio_tg(bio, parent_tg);
+ 	} else {
+ 		bio_list_add(&parent_sq->bio_lists[rw], bio);
+ 		BUG_ON(tg->td->nr_queued[rw] <= 0);
+ 		tg->td->nr_queued[rw]--;
+ 	}
++>>>>>>> 6bc9c2b464fb (blk-throttle: make tg_dispatch_one_bio() ready for hierarchy)
  
  	throtl_trim_slice(tg, rw);
+ 
+ 	/* @bio is transferred to parent, drop its blkg reference */
+ 	blkg_put(tg_to_blkg(tg));
  }
  
 -static int throtl_dispatch_tg(struct throtl_grp *tg)
 +static int throtl_dispatch_tg(struct throtl_grp *tg, struct bio_list *bl)
  {
  	struct throtl_service_queue *sq = &tg->service_queue;
  	unsigned int nr_reads = 0, nr_writes = 0;
@@@ -1123,25 -1276,39 +1145,30 @@@ bool blk_throtl_bio(struct request_queu
  		 * So keep on trimming slice even if bio is not queued.
  		 */
  		throtl_trim_slice(tg, rw);
 -
 -		/*
 -		 * @bio passed through this layer without being throttled.
 -		 * Climb up the ladder.  If we''re already at the top, it
 -		 * can be executed directly.
 -		 */
 -		sq = sq->parent_sq;
 -		tg = sq_to_tg(sq);
 -		if (!tg)
 -			goto out_unlock;
 +		goto out_unlock;
  	}
  
 -	/* out-of-limit, queue to @tg */
 -	throtl_log(sq, "[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d",
 -		   rw == READ ? 'R' : 'W',
 -		   tg->bytes_disp[rw], bio->bi_size, tg->bps[rw],
 -		   tg->io_disp[rw], tg->iops[rw],
 -		   sq->nr_queued[READ], sq->nr_queued[WRITE]);
 +queue_bio:
 +	throtl_log_tg(tg, "[%c] bio. bdisp=%llu sz=%u bps=%llu"
 +			" iodisp=%u iops=%u queued=%d/%d",
 +			rw == READ ? 'R' : 'W',
 +			tg->bytes_disp[rw], bio->bi_size, tg->bps[rw],
 +			tg->io_disp[rw], tg->iops[rw],
 +			sq->nr_queued[READ], sq->nr_queued[WRITE]);
  
  	bio_associate_current(bio);
++<<<<<<< HEAD
 +	throtl_add_bio_tg(bio, tg, &q->td->service_queue);
++=======
+ 	tg->td->nr_queued[rw]++;
+ 	throtl_add_bio_tg(bio, tg);
++>>>>>>> 6bc9c2b464fb (blk-throttle: make tg_dispatch_one_bio() ready for hierarchy)
  	throttled = true;
  
 -	/*
 -	 * Update @tg's dispatch time and force schedule dispatch if @tg
 -	 * was empty before @bio.  The forced scheduling isn't likely to
 -	 * cause undue delay as @bio is likely to be dispatched directly if
 -	 * its @tg's disptime is not in the future.
 -	 */
 +	/* update @tg's dispatch time if @tg was empty before @bio */
  	if (tg->flags & THROTL_TG_WAS_EMPTY) {
 -		tg_update_disptime(tg);
 -		throtl_schedule_next_dispatch(tg->service_queue.parent_sq, true);
 +		tg_update_disptime(tg, &td->service_queue);
 +		throtl_schedule_next_dispatch(td);
  	}
  
  out_unlock:
* Unmerged path block/blk-throttle.c
