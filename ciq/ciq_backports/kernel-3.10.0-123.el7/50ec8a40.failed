sched/numa: Track from which nodes NUMA faults are triggered

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Rik van Riel <riel@redhat.com>
commit 50ec8a401fed6d246ab65e6011d61ac91c34af70
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/50ec8a40.failed

Track which nodes NUMA faults are triggered from, in other words
the CPUs on which the NUMA faults happened. This uses a similar
mechanism to what is used to track the memory involved in numa faults.

The next patches use this to build up a bitmap of which nodes a
workload is actively running on.

	Signed-off-by: Rik van Riel <riel@redhat.com>
	Acked-by: Mel Gorman <mgorman@suse.de>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Chegu Vinod <chegu_vinod@hp.com>
Link: http://lkml.kernel.org/r/1390860228-21539-4-git-send-email-riel@redhat.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 50ec8a401fed6d246ab65e6011d61ac91c34af70)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched.h
#	kernel/sched/fair.c
diff --cc include/linux/sched.h
index 933a94b9c8f9,5fb0cfb43ecf..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -1362,12 -1474,27 +1362,31 @@@ struct task_struct 
  
  	/*
  	 * numa_faults_buffer records faults per node during the current
 -	 * scan window. When the scan completes, the counts in
 -	 * numa_faults_memory decay and these values are copied.
 +	 * scan window. When the scan completes, the counts in numa_faults
 +	 * decay and these values are copied.
  	 */
 -	unsigned long *numa_faults_buffer_memory;
 +	unsigned long *numa_faults_buffer;
  
++<<<<<<< HEAD
 +	int numa_preferred_nid;
++=======
+ 	/*
+ 	 * Track the nodes the process was running on when a NUMA hinting
+ 	 * fault was incurred.
+ 	 */
+ 	unsigned long *numa_faults_cpu;
+ 	unsigned long *numa_faults_buffer_cpu;
+ 
+ 	/*
+ 	 * numa_faults_locality tracks if faults recorded during the last
+ 	 * scan window were remote/local. The task scan period is adapted
+ 	 * based on the locality of the faults with different weights
+ 	 * depending on whether they were shared or private faults
+ 	 */
+ 	unsigned long numa_faults_locality[2];
+ 
+ 	unsigned long numa_pages_migrated;
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  #endif /* CONFIG_NUMA_BALANCING */
  
  	struct rcu_head rcu;
@@@ -1449,13 -1579,25 +1468,17 @@@
  /* Future-safe accessor for struct task_struct's cpus_allowed. */
  #define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
  
 -#define TNF_MIGRATED	0x01
 -#define TNF_NO_GROUP	0x02
 -#define TNF_SHARED	0x04
 -#define TNF_FAULT_LOCAL	0x08
 -
  #ifdef CONFIG_NUMA_BALANCING
 -extern void task_numa_fault(int last_node, int node, int pages, int flags);
 -extern pid_t task_numa_group_id(struct task_struct *p);
 +extern void task_numa_fault(int node, int pages, bool migrated);
  extern void set_numabalancing_state(bool enabled);
++<<<<<<< HEAD
++=======
+ extern void task_numa_free(struct task_struct *p);
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  #else
 -static inline void task_numa_fault(int last_node, int node, int pages,
 -				   int flags)
 +static inline void task_numa_fault(int node, int pages, bool migrated)
  {
  }
 -static inline pid_t task_numa_group_id(struct task_struct *p)
 -{
 -	return 0;
 -}
  static inline void set_numabalancing_state(bool enabled)
  {
  }
diff --cc kernel/sched/fair.c
index c33e42bc435b,4841aaff7394..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,61 -864,760 +839,770 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ static void account_numa_enqueue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running += (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ static void account_numa_dequeue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running -= (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ struct numa_group {
+ 	atomic_t refcount;
+ 
+ 	spinlock_t lock; /* nr_tasks, tasks */
+ 	int nr_tasks;
+ 	pid_t gid;
+ 	struct list_head task_list;
+ 
+ 	struct rcu_head rcu;
+ 	unsigned long total_faults;
+ 	unsigned long *faults_cpu;
+ 	unsigned long faults[0];
+ };
+ 
+ pid_t task_numa_group_id(struct task_struct *p)
+ {
+ 	return p->numa_group ? p->numa_group->gid : 0;
+ }
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults_memory)
+ 		return 0;
+ 
+ 	return p->numa_faults_memory[task_faults_idx(nid, 0)] +
+ 		p->numa_faults_memory[task_faults_idx(nid, 1)];
+ }
+ 
+ static inline unsigned long group_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	return p->numa_group->faults[task_faults_idx(nid, 0)] +
+ 		p->numa_group->faults[task_faults_idx(nid, 1)];
+ }
+ 
+ /*
+  * These return the fraction of accesses done by a particular task, or
+  * task group, on a particular numa node.  The group weight is given a
+  * larger multiplier, in order to group tasks together that are almost
+  * evenly spread out between numa nodes.
+  */
+ static inline unsigned long task_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_faults_memory)
+ 		return 0;
+ 
+ 	total_faults = p->total_numa_faults;
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * task_faults(p, nid) / total_faults;
+ }
+ 
+ static inline unsigned long group_weight(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group || !p->numa_group->total_faults)
+ 		return 0;
+ 
+ 	return 1000 * group_faults(p, nid) / p->numa_group->total_faults;
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ /* Cached statistics for all CPUs within a node */
+ struct numa_stats {
+ 	unsigned long nr_running;
+ 	unsigned long load;
+ 
+ 	/* Total compute capacity of CPUs on a node */
+ 	unsigned long power;
+ 
+ 	/* Approximate capacity in terms of runnable tasks on a node */
+ 	unsigned long capacity;
+ 	int has_capacity;
+ };
+ 
+ /*
+  * XXX borrowed from update_sg_lb_stats
+  */
+ static void update_numa_stats(struct numa_stats *ns, int nid)
+ {
+ 	int cpu, cpus = 0;
+ 
+ 	memset(ns, 0, sizeof(*ns));
+ 	for_each_cpu(cpu, cpumask_of_node(nid)) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 
+ 		ns->nr_running += rq->nr_running;
+ 		ns->load += weighted_cpuload(cpu);
+ 		ns->power += power_of(cpu);
+ 
+ 		cpus++;
+ 	}
+ 
+ 	/*
+ 	 * If we raced with hotplug and there are no CPUs left in our mask
+ 	 * the @ns structure is NULL'ed and task_numa_compare() will
+ 	 * not find this node attractive.
+ 	 *
+ 	 * We'll either bail at !has_capacity, or we'll detect a huge imbalance
+ 	 * and bail there.
+ 	 */
+ 	if (!cpus)
+ 		return;
+ 
+ 	ns->load = (ns->load * SCHED_POWER_SCALE) / ns->power;
+ 	ns->capacity = DIV_ROUND_CLOSEST(ns->power, SCHED_POWER_SCALE);
+ 	ns->has_capacity = (ns->nr_running < ns->capacity);
+ }
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	int imbalance_pct;
+ 
+ 	struct task_struct *best_task;
+ 	long best_imp;
+ 	int best_cpu;
+ };
+ 
+ static void task_numa_assign(struct task_numa_env *env,
+ 			     struct task_struct *p, long imp)
+ {
+ 	if (env->best_task)
+ 		put_task_struct(env->best_task);
+ 	if (p)
+ 		get_task_struct(p);
+ 
+ 	env->best_task = p;
+ 	env->best_imp = imp;
+ 	env->best_cpu = env->dst_cpu;
+ }
+ 
+ /*
+  * This checks if the overall compute and NUMA accesses of the system would
+  * be improved if the source tasks was migrated to the target dst_cpu taking
+  * into account that it might be best if task running on the dst_cpu should
+  * be exchanged with the source task
+  */
+ static void task_numa_compare(struct task_numa_env *env,
+ 			      long taskimp, long groupimp)
+ {
+ 	struct rq *src_rq = cpu_rq(env->src_cpu);
+ 	struct rq *dst_rq = cpu_rq(env->dst_cpu);
+ 	struct task_struct *cur;
+ 	long dst_load, src_load;
+ 	long load;
+ 	long imp = (groupimp > 0) ? groupimp : taskimp;
+ 
+ 	rcu_read_lock();
+ 	cur = ACCESS_ONCE(dst_rq->curr);
+ 	if (cur->pid == 0) /* idle */
+ 		cur = NULL;
+ 
+ 	/*
+ 	 * "imp" is the fault differential for the source task between the
+ 	 * source and destination node. Calculate the total differential for
+ 	 * the source task and potential destination task. The more negative
+ 	 * the value is, the more rmeote accesses that would be expected to
+ 	 * be incurred if the tasks were swapped.
+ 	 */
+ 	if (cur) {
+ 		/* Skip this swap candidate if cannot move to the source cpu */
+ 		if (!cpumask_test_cpu(env->src_cpu, tsk_cpus_allowed(cur)))
+ 			goto unlock;
+ 
+ 		/*
+ 		 * If dst and source tasks are in the same NUMA group, or not
+ 		 * in any group then look only at task weights.
+ 		 */
+ 		if (cur->numa_group == env->p->numa_group) {
+ 			imp = taskimp + task_weight(cur, env->src_nid) -
+ 			      task_weight(cur, env->dst_nid);
+ 			/*
+ 			 * Add some hysteresis to prevent swapping the
+ 			 * tasks within a group over tiny differences.
+ 			 */
+ 			if (cur->numa_group)
+ 				imp -= imp/16;
+ 		} else {
+ 			/*
+ 			 * Compare the group weights. If a task is all by
+ 			 * itself (not part of a group), use the task weight
+ 			 * instead.
+ 			 */
+ 			if (env->p->numa_group)
+ 				imp = groupimp;
+ 			else
+ 				imp = taskimp;
+ 
+ 			if (cur->numa_group)
+ 				imp += group_weight(cur, env->src_nid) -
+ 				       group_weight(cur, env->dst_nid);
+ 			else
+ 				imp += task_weight(cur, env->src_nid) -
+ 				       task_weight(cur, env->dst_nid);
+ 		}
+ 	}
+ 
+ 	if (imp < env->best_imp)
+ 		goto unlock;
+ 
+ 	if (!cur) {
+ 		/* Is there capacity at our destination? */
+ 		if (env->src_stats.has_capacity &&
+ 		    !env->dst_stats.has_capacity)
+ 			goto unlock;
+ 
+ 		goto balance;
+ 	}
+ 
+ 	/* Balance doesn't matter much if we're running a task per cpu */
+ 	if (src_rq->nr_running == 1 && dst_rq->nr_running == 1)
+ 		goto assign;
+ 
+ 	/*
+ 	 * In the overloaded case, try and keep the load balanced.
+ 	 */
+ balance:
+ 	dst_load = env->dst_stats.load;
+ 	src_load = env->src_stats.load;
+ 
+ 	/* XXX missing power terms */
+ 	load = task_h_load(env->p);
+ 	dst_load += load;
+ 	src_load -= load;
+ 
+ 	if (cur) {
+ 		load = task_h_load(cur);
+ 		dst_load -= load;
+ 		src_load += load;
+ 	}
+ 
+ 	/* make src_load the smaller */
+ 	if (dst_load < src_load)
+ 		swap(dst_load, src_load);
+ 
+ 	if (src_load * env->imbalance_pct < dst_load * 100)
+ 		goto unlock;
+ 
+ assign:
+ 	task_numa_assign(env, cur, imp);
+ unlock:
+ 	rcu_read_unlock();
+ }
+ 
+ static void task_numa_find_cpu(struct task_numa_env *env,
+ 				long taskimp, long groupimp)
+ {
+ 	int cpu;
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
+ 		/* Skip this CPU if the source task cannot migrate */
+ 		if (!cpumask_test_cpu(cpu, tsk_cpus_allowed(env->p)))
+ 			continue;
+ 
+ 		env->dst_cpu = cpu;
+ 		task_numa_compare(env, taskimp, groupimp);
+ 	}
+ }
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = task_node(p),
+ 
+ 		.imbalance_pct = 112,
+ 
+ 		.best_task = NULL,
+ 		.best_imp = 0,
+ 		.best_cpu = -1
+ 	};
+ 	struct sched_domain *sd;
+ 	unsigned long taskweight, groupweight;
+ 	int nid, ret;
+ 	long taskimp, groupimp;
+ 
+ 	/*
+ 	 * Pick the lowest SD_NUMA domain, as that would have the smallest
+ 	 * imbalance and would be the first to start moving tasks about.
+ 	 *
+ 	 * And we want to avoid any moving of tasks about, as that would create
+ 	 * random movement of tasks -- counter the numa conditions we're trying
+ 	 * to satisfy here.
+ 	 */
+ 	rcu_read_lock();
+ 	sd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));
+ 	if (sd)
+ 		env.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;
+ 	rcu_read_unlock();
+ 
+ 	/*
+ 	 * Cpusets can break the scheduler domain tree into smaller
+ 	 * balance domains, some of which do not cross NUMA boundaries.
+ 	 * Tasks that are "trapped" in such domains cannot be migrated
+ 	 * elsewhere, so there is no point in (re)trying.
+ 	 */
+ 	if (unlikely(!sd)) {
+ 		p->numa_preferred_nid = task_node(p);
+ 		return -EINVAL;
+ 	}
+ 
+ 	taskweight = task_weight(p, env.src_nid);
+ 	groupweight = group_weight(p, env.src_nid);
+ 	update_numa_stats(&env.src_stats, env.src_nid);
+ 	env.dst_nid = p->numa_preferred_nid;
+ 	taskimp = task_weight(p, env.dst_nid) - taskweight;
+ 	groupimp = group_weight(p, env.dst_nid) - groupweight;
+ 	update_numa_stats(&env.dst_stats, env.dst_nid);
+ 
+ 	/* If the preferred nid has capacity, try to use it. */
+ 	if (env.dst_stats.has_capacity)
+ 		task_numa_find_cpu(&env, taskimp, groupimp);
+ 
+ 	/* No space available on the preferred nid. Look elsewhere. */
+ 	if (env.best_cpu == -1) {
+ 		for_each_online_node(nid) {
+ 			if (nid == env.src_nid || nid == p->numa_preferred_nid)
+ 				continue;
+ 
+ 			/* Only consider nodes where both task and groups benefit */
+ 			taskimp = task_weight(p, nid) - taskweight;
+ 			groupimp = group_weight(p, nid) - groupweight;
+ 			if (taskimp < 0 && groupimp < 0)
+ 				continue;
+ 
+ 			env.dst_nid = nid;
+ 			update_numa_stats(&env.dst_stats, env.dst_nid);
+ 			task_numa_find_cpu(&env, taskimp, groupimp);
+ 		}
+ 	}
+ 
+ 	/* No better CPU than the current one was found. */
+ 	if (env.best_cpu == -1)
+ 		return -EAGAIN;
+ 
+ 	sched_setnuma(p, env.dst_nid);
+ 
+ 	/*
+ 	 * Reset the scan period if the task is being rescheduled on an
+ 	 * alternative node to recheck if the tasks is now properly placed.
+ 	 */
+ 	p->numa_scan_period = task_scan_min(p);
+ 
+ 	if (env.best_task == NULL) {
+ 		int ret = migrate_task_to(p, env.best_cpu);
+ 		return ret;
+ 	}
+ 
+ 	ret = migrate_swap(p, env.best_task);
+ 	put_task_struct(env.best_task);
+ 	return ret;
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults_memory))
+ 		return;
+ 
+ 	/* Periodically retry migrating the task to the preferred node */
+ 	p->numa_migrate_retry = jiffies + HZ;
+ 
+ 	/* Success if task is already running on preferred CPU */
+ 	if (task_node(p) == p->numa_preferred_nid)
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	task_numa_migrate(p);
+ }
+ 
+ /*
+  * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS
+  * increments. The more local the fault statistics are, the higher the scan
+  * period will be for the next scan window. If local/remote ratio is below
+  * NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS) the
+  * scan period will decrease
+  */
+ #define NUMA_PERIOD_SLOTS 10
+ #define NUMA_PERIOD_THRESHOLD 3
+ 
+ /*
+  * Increase the scan period (slow down scanning) if the majority of
+  * our memory is already on our local node, or if the majority of
+  * the page accesses are shared with other processes.
+  * Otherwise, decrease the scan period.
+  */
+ static void update_task_scan_period(struct task_struct *p,
+ 			unsigned long shared, unsigned long private)
+ {
+ 	unsigned int period_slot;
+ 	int ratio;
+ 	int diff;
+ 
+ 	unsigned long remote = p->numa_faults_locality[0];
+ 	unsigned long local = p->numa_faults_locality[1];
+ 
+ 	/*
+ 	 * If there were no record hinting faults then either the task is
+ 	 * completely idle or all activity is areas that are not of interest
+ 	 * to automatic numa balancing. Scan slower
+ 	 */
+ 	if (local + shared == 0) {
+ 		p->numa_scan_period = min(p->numa_scan_period_max,
+ 			p->numa_scan_period << 1);
+ 
+ 		p->mm->numa_next_scan = jiffies +
+ 			msecs_to_jiffies(p->numa_scan_period);
+ 
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * Prepare to scale scan period relative to the current period.
+ 	 *	 == NUMA_PERIOD_THRESHOLD scan period stays the same
+ 	 *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)
+ 	 *	 >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)
+ 	 */
+ 	period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+ 	ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);
+ 	if (ratio >= NUMA_PERIOD_THRESHOLD) {
+ 		int slot = ratio - NUMA_PERIOD_THRESHOLD;
+ 		if (!slot)
+ 			slot = 1;
+ 		diff = slot * period_slot;
+ 	} else {
+ 		diff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;
+ 
+ 		/*
+ 		 * Scale scan rate increases based on sharing. There is an
+ 		 * inverse relationship between the degree of sharing and
+ 		 * the adjustment made to the scanning period. Broadly
+ 		 * speaking the intent is that there is little point
+ 		 * scanning faster if shared accesses dominate as it may
+ 		 * simply bounce migrations uselessly
+ 		 */
+ 		ratio = DIV_ROUND_UP(private * NUMA_PERIOD_SLOTS, (private + shared));
+ 		diff = (diff * ratio) / NUMA_PERIOD_SLOTS;
+ 	}
+ 
+ 	p->numa_scan_period = clamp(p->numa_scan_period + diff,
+ 			task_scan_min(p), task_scan_max(p));
+ 	memset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));
+ }
+ 
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  static void task_numa_placement(struct task_struct *p)
  {
 -	int seq, nid, max_nid = -1, max_group_nid = -1;
 -	unsigned long max_faults = 0, max_group_faults = 0;
 -	unsigned long fault_types[2] = { 0, 0 };
 -	spinlock_t *group_lock = NULL;
 +	int seq, nid, max_nid = -1;
 +	unsigned long max_faults = 0;
  
 +	if (!p->mm)	/* for example, ksmd faulting in a user's mm */
 +		return;
  	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
  	if (p->numa_scan_seq == seq)
  		return;
  	p->numa_scan_seq = seq;
  	p->numa_scan_period_max = task_scan_max(p);
  
 -	/* If the task is part of a group prevent parallel updates to group stats */
 -	if (p->numa_group) {
 -		group_lock = &p->numa_group->lock;
 -		spin_lock(group_lock);
 -	}
 -
  	/* Find the node with the highest number of faults */
  	for_each_online_node(nid) {
 -		unsigned long faults = 0, group_faults = 0;
 -		int priv, i;
 -
 +		unsigned long faults;
 +
++<<<<<<< HEAD
 +		/* Decay existing window and copy faults since last scan */
 +		p->numa_faults[nid] >>= 1;
 +		p->numa_faults[nid] += p->numa_faults_buffer[nid];
 +		p->numa_faults_buffer[nid] = 0;
++=======
+ 		for (priv = 0; priv < 2; priv++) {
+ 			long diff, f_diff;
+ 
+ 			i = task_faults_idx(nid, priv);
+ 			diff = -p->numa_faults_memory[i];
+ 			f_diff = -p->numa_faults_cpu[i];
+ 
+ 			/* Decay existing window, copy faults since last scan */
+ 			p->numa_faults_memory[i] >>= 1;
+ 			p->numa_faults_memory[i] += p->numa_faults_buffer_memory[i];
+ 			fault_types[priv] += p->numa_faults_buffer_memory[i];
+ 			p->numa_faults_buffer_memory[i] = 0;
+ 
+ 			p->numa_faults_cpu[i] >>= 1;
+ 			p->numa_faults_cpu[i] += p->numa_faults_buffer_cpu[i];
+ 			p->numa_faults_buffer_cpu[i] = 0;
+ 
+ 			faults += p->numa_faults_memory[i];
+ 			diff += p->numa_faults_memory[i];
+ 			f_diff += p->numa_faults_cpu[i];
+ 			p->total_numa_faults += diff;
+ 			if (p->numa_group) {
+ 				/* safe because we can only change our own group */
+ 				p->numa_group->faults[i] += diff;
+ 				p->numa_group->faults_cpu[i] += f_diff;
+ 				p->numa_group->total_faults += diff;
+ 				group_faults += p->numa_group->faults[i];
+ 			}
+ 		}
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  
 +		faults = p->numa_faults[nid];
  		if (faults > max_faults) {
  			max_faults = faults;
  			max_nid = nid;
  		}
 -
 -		if (group_faults > max_group_faults) {
 -			max_group_faults = group_faults;
 -			max_group_nid = nid;
 -		}
  	}
  
++<<<<<<< HEAD
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
 +		p->numa_preferred_nid = max_nid;
++=======
+ 	update_task_scan_period(p, fault_types[0], fault_types[1]);
+ 
+ 	if (p->numa_group) {
+ 		/*
+ 		 * If the preferred task and group nids are different,
+ 		 * iterate over the nodes again to find the best place.
+ 		 */
+ 		if (max_nid != max_group_nid) {
+ 			unsigned long weight, max_weight = 0;
+ 
+ 			for_each_online_node(nid) {
+ 				weight = task_weight(p, nid) + group_weight(p, nid);
+ 				if (weight > max_weight) {
+ 					max_weight = weight;
+ 					max_nid = nid;
+ 				}
+ 			}
+ 		}
+ 
+ 		spin_unlock(group_lock);
+ 	}
+ 
+ 	/* Preferred node as the node with the most faults */
+ 	if (max_faults && max_nid != p->numa_preferred_nid) {
+ 		/* Update the preferred nid and migrate task if possible */
+ 		sched_setnuma(p, max_nid);
+ 		numa_migrate_preferred(p);
+ 	}
+ }
+ 
+ static inline int get_numa_group(struct numa_group *grp)
+ {
+ 	return atomic_inc_not_zero(&grp->refcount);
+ }
+ 
+ static inline void put_numa_group(struct numa_group *grp)
+ {
+ 	if (atomic_dec_and_test(&grp->refcount))
+ 		kfree_rcu(grp, rcu);
+ }
+ 
+ static void task_numa_group(struct task_struct *p, int cpupid, int flags,
+ 			int *priv)
+ {
+ 	struct numa_group *grp, *my_grp;
+ 	struct task_struct *tsk;
+ 	bool join = false;
+ 	int cpu = cpupid_to_cpu(cpupid);
+ 	int i;
+ 
+ 	if (unlikely(!p->numa_group)) {
+ 		unsigned int size = sizeof(struct numa_group) +
+ 				    4*nr_node_ids*sizeof(unsigned long);
+ 
+ 		grp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);
+ 		if (!grp)
+ 			return;
+ 
+ 		atomic_set(&grp->refcount, 1);
+ 		spin_lock_init(&grp->lock);
+ 		INIT_LIST_HEAD(&grp->task_list);
+ 		grp->gid = p->pid;
+ 		/* Second half of the array tracks nids where faults happen */
+ 		grp->faults_cpu = grp->faults + 2 * nr_node_ids;
+ 
+ 		for (i = 0; i < 4*nr_node_ids; i++)
+ 			grp->faults[i] = p->numa_faults_memory[i];
+ 
+ 		grp->total_faults = p->total_numa_faults;
+ 
+ 		list_add(&p->numa_entry, &grp->task_list);
+ 		grp->nr_tasks++;
+ 		rcu_assign_pointer(p->numa_group, grp);
+ 	}
+ 
+ 	rcu_read_lock();
+ 	tsk = ACCESS_ONCE(cpu_rq(cpu)->curr);
+ 
+ 	if (!cpupid_match_pid(tsk, cpupid))
+ 		goto no_join;
+ 
+ 	grp = rcu_dereference(tsk->numa_group);
+ 	if (!grp)
+ 		goto no_join;
+ 
+ 	my_grp = p->numa_group;
+ 	if (grp == my_grp)
+ 		goto no_join;
+ 
+ 	/*
+ 	 * Only join the other group if its bigger; if we're the bigger group,
+ 	 * the other task will join us.
+ 	 */
+ 	if (my_grp->nr_tasks > grp->nr_tasks)
+ 		goto no_join;
+ 
+ 	/*
+ 	 * Tie-break on the grp address.
+ 	 */
+ 	if (my_grp->nr_tasks == grp->nr_tasks && my_grp > grp)
+ 		goto no_join;
+ 
+ 	/* Always join threads in the same process. */
+ 	if (tsk->mm == current->mm)
+ 		join = true;
+ 
+ 	/* Simple filter to avoid false positives due to PID collisions */
+ 	if (flags & TNF_SHARED)
+ 		join = true;
+ 
+ 	/* Update priv based on whether false sharing was detected */
+ 	*priv = !join;
+ 
+ 	if (join && !get_numa_group(grp))
+ 		goto no_join;
+ 
+ 	rcu_read_unlock();
+ 
+ 	if (!join)
+ 		return;
+ 
+ 	double_lock(&my_grp->lock, &grp->lock);
+ 
+ 	for (i = 0; i < 4*nr_node_ids; i++) {
+ 		my_grp->faults[i] -= p->numa_faults_memory[i];
+ 		grp->faults[i] += p->numa_faults_memory[i];
+ 	}
+ 	my_grp->total_faults -= p->total_numa_faults;
+ 	grp->total_faults += p->total_numa_faults;
+ 
+ 	list_move(&p->numa_entry, &grp->task_list);
+ 	my_grp->nr_tasks--;
+ 	grp->nr_tasks++;
+ 
+ 	spin_unlock(&my_grp->lock);
+ 	spin_unlock(&grp->lock);
+ 
+ 	rcu_assign_pointer(p->numa_group, grp);
+ 
+ 	put_numa_group(my_grp);
+ 	return;
+ 
+ no_join:
+ 	rcu_read_unlock();
+ 	return;
+ }
+ 
+ void task_numa_free(struct task_struct *p)
+ {
+ 	struct numa_group *grp = p->numa_group;
+ 	int i;
+ 	void *numa_faults = p->numa_faults_memory;
+ 
+ 	if (grp) {
+ 		spin_lock(&grp->lock);
+ 		for (i = 0; i < 4*nr_node_ids; i++)
+ 			grp->faults[i] -= p->numa_faults_memory[i];
+ 		grp->total_faults -= p->total_numa_faults;
+ 
+ 		list_del(&p->numa_entry);
+ 		grp->nr_tasks--;
+ 		spin_unlock(&grp->lock);
+ 		rcu_assign_pointer(p->numa_group, NULL);
+ 		put_numa_group(grp);
+ 	}
+ 
+ 	p->numa_faults_memory = NULL;
+ 	p->numa_faults_buffer_memory = NULL;
+ 	p->numa_faults_cpu= NULL;
+ 	p->numa_faults_buffer_cpu = NULL;
+ 	kfree(numa_faults);
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  }
  
  /*
   * Got a PROT_NONE fault for a page on @node.
   */
 -void task_numa_fault(int last_cpupid, int node, int pages, int flags)
 +void task_numa_fault(int node, int pages, bool migrated)
  {
  	struct task_struct *p = current;
++<<<<<<< HEAD
++=======
+ 	bool migrated = flags & TNF_MIGRATED;
+ 	int this_node = task_node(current);
+ 	int priv;
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  
  	if (!numabalancing_enabled)
  		return;
  
 -	/* for example, ksmd faulting in a user's mm */
 -	if (!p->mm)
 -		return;
 -
 -	/* Do not worry about placement if exiting */
 -	if (p->state == TASK_DEAD)
 -		return;
 -
  	/* Allocate buffer to track faults on a per-node basis */
++<<<<<<< HEAD
 +	if (unlikely(!p->numa_faults)) {
 +		int size = sizeof(*p->numa_faults) * nr_node_ids;
++=======
+ 	if (unlikely(!p->numa_faults_memory)) {
+ 		int size = sizeof(*p->numa_faults_memory) * 4 * nr_node_ids;
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  
  		/* numa_faults and numa_faults_buffer share the allocation */
 -		p->numa_faults_memory = kzalloc(size * 2, GFP_KERNEL|__GFP_NOWARN);
 -		if (!p->numa_faults_memory)
 +		p->numa_faults = kzalloc(size * 2, GFP_KERNEL|__GFP_NOWARN);
 +		if (!p->numa_faults)
  			return;
  
++<<<<<<< HEAD
 +		BUG_ON(p->numa_faults_buffer);
 +		p->numa_faults_buffer = p->numa_faults + nr_node_ids;
++=======
+ 		BUG_ON(p->numa_faults_buffer_memory);
+ 		p->numa_faults_cpu = p->numa_faults_memory + (2 * nr_node_ids);
+ 		p->numa_faults_buffer_memory = p->numa_faults_memory + (4 * nr_node_ids);
+ 		p->numa_faults_buffer_cpu = p->numa_faults_memory + (6 * nr_node_ids);
+ 		p->total_numa_faults = 0;
+ 		memset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  	}
  
  	/*
@@@ -911,7 -1634,19 +1620,23 @@@
  
  	task_numa_placement(p);
  
++<<<<<<< HEAD
 +	p->numa_faults_buffer[node] += pages;
++=======
+ 	/*
+ 	 * Retry task to preferred node migration periodically, in case it
+ 	 * case it previously failed, or the scheduler moved us.
+ 	 */
+ 	if (time_after(jiffies, p->numa_migrate_retry))
+ 		numa_migrate_preferred(p);
+ 
+ 	if (migrated)
+ 		p->numa_pages_migrated += pages;
+ 
+ 	p->numa_faults_buffer_memory[task_faults_idx(node, priv)] += pages;
+ 	p->numa_faults_buffer_cpu[task_faults_idx(this_node, priv)] += pages;
+ 	p->numa_faults_locality[!!(flags & TNF_FAULT_LOCAL)] += pages;
++>>>>>>> 50ec8a401fed (sched/numa: Track from which nodes NUMA faults are triggered)
  }
  
  static void reset_ptenuma_scan(struct task_struct *p)
* Unmerged path include/linux/sched.h
* Unmerged path kernel/sched/fair.c
