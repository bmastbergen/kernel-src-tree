sched/numa: Remove the numa_balancing_scan_period_reset sysctl

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Mel Gorman <mgorman@suse.de>
commit 930aa174fcc8b0efaad102fd80f677b92f35eaa2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/930aa174.failed

With scan rate adaptions based on whether the workload has properly
converged or not there should be no need for the scan period reset
hammer. Get rid of it.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-60-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 930aa174fcc8b0efaad102fd80f677b92f35eaa2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/sysctl/kernel.txt
#	kernel/sched/fair.c
diff --cc Documentation/sysctl/kernel.txt
index 77932153d642,84f17800f8b5..000000000000
--- a/Documentation/sysctl/kernel.txt
+++ b/Documentation/sysctl/kernel.txt
@@@ -373,8 -374,8 +373,13 @@@ guarantee. If the target workload is al
  feature should be disabled. Otherwise, if the system overhead from the
  feature is too high then the rate the kernel samples for NUMA hinting
  faults may be controlled by the numa_balancing_scan_period_min_ms,
++<<<<<<< HEAD
 +numa_balancing_scan_delay_ms, numa_balancing_scan_period_reset,
 +numa_balancing_scan_period_max_ms and numa_balancing_scan_size_mb sysctls.
++=======
+ numa_balancing_scan_delay_ms, numa_balancing_scan_period_max_ms,
+ numa_balancing_scan_size_mb and numa_balancing_settle_count sysctls.
++>>>>>>> 930aa174fcc8 (sched/numa: Remove the numa_balancing_scan_period_reset sysctl)
  
  ==============================================================
  
@@@ -416,8 -416,10 +420,15 @@@ rate for each task
  numa_balancing_scan_size_mb is how many megabytes worth of pages are
  scanned for a given scan.
  
++<<<<<<< HEAD
 +numa_balancing_scan_period_reset is a blunt instrument that controls how
 +often a tasks scan delay is reset to detect sudden changes in task behaviour.
++=======
+ numa_balancing_settle_count is how many scan periods must complete before
+ the schedule balancer stops pushing the task towards a preferred node. This
+ gives the scheduler a chance to place the task on an alternative node if the
+ preferred node is overloaded.
++>>>>>>> 930aa174fcc8 (sched/numa: Remove the numa_balancing_scan_period_reset sysctl)
  
  ==============================================================
  
diff --cc kernel/sched/fair.c
index 98cee68da024,da6fa22be000..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -948,42 -1684,9 +947,45 @@@ void task_numa_work(struct callback_hea
  	if (p->flags & PF_EXITING)
  		return;
  
- 	if (!mm->numa_next_reset || !mm->numa_next_scan) {
+ 	if (!mm->numa_next_scan) {
  		mm->numa_next_scan = now +
  			msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
++<<<<<<< HEAD
 +		mm->numa_next_reset = now +
 +			msecs_to_jiffies(sysctl_numa_balancing_scan_period_reset);
 +	}
 +
 +	/*
 +	 * We do not care about task placement until a task runs on a node
 +	 * other than the first one used by the address space. This is
 +	 * largely because migrations are driven by what CPU the task
 +	 * is running on. If it's never scheduled on another node, it'll
 +	 * not migrate so why bother trapping the fault.
 +	 */
 +	if (mm->first_nid == NUMA_PTE_SCAN_INIT)
 +		mm->first_nid = numa_node_id();
 +	if (mm->first_nid != NUMA_PTE_SCAN_ACTIVE) {
 +		/* Are we running on a new node yet? */
 +		if (numa_node_id() == mm->first_nid &&
 +		    !sched_feat_numa(NUMA_FORCE))
 +			return;
 +
 +		mm->first_nid = NUMA_PTE_SCAN_ACTIVE;
 +	}
 +
 +	/*
 +	 * Reset the scan period if enough time has gone by. Objective is that
 +	 * scanning will be reduced if pages are properly placed. As tasks
 +	 * can enter different phases this needs to be re-examined. Lacking
 +	 * proper tracking of reference behaviour, this blunt hammer is used.
 +	 */
 +	migrate = mm->numa_next_reset;
 +	if (time_after(now, migrate)) {
 +		p->numa_scan_period = task_scan_min(p);
 +		next_scan = now + msecs_to_jiffies(sysctl_numa_balancing_scan_period_reset);
 +		xchg(&mm->numa_next_reset, next_scan);
++=======
++>>>>>>> 930aa174fcc8 (sched/numa: Remove the numa_balancing_scan_period_reset sysctl)
  	}
  
  	/*
* Unmerged path Documentation/sysctl/kernel.txt
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index ace9a5f01c64..de7096409f44 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -421,9 +421,6 @@ struct mm_struct {
 	 */
 	unsigned long numa_next_scan;
 
-	/* numa_next_reset is when the PTE scanner period will be reset */
-	unsigned long numa_next_reset;
-
 	/* Restart point for scanning and setting pte_numa */
 	unsigned long numa_scan_offset;
 
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index bf8086b2506e..10d16c4fbe89 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -47,7 +47,6 @@ extern enum sched_tunable_scaling sysctl_sched_tunable_scaling;
 extern unsigned int sysctl_numa_balancing_scan_delay;
 extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
-extern unsigned int sysctl_numa_balancing_scan_period_reset;
 extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_numa_balancing_settle_count;
 
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 2902fa55d246..5bba0c1e2d5b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1634,7 +1634,6 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 #ifdef CONFIG_NUMA_BALANCING
 	if (p->mm && atomic_read(&p->mm->mm_users) == 1) {
 		p->mm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
-		p->mm->numa_next_reset = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_period_reset);
 		p->mm->numa_scan_seq = 0;
 	}
 
* Unmerged path kernel/sched/fair.c
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index d93ad6b7164b..86205e7baea1 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -371,13 +371,6 @@ static struct ctl_table kern_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
 	},
-	{
-		.procname	= "numa_balancing_scan_period_reset",
-		.data		= &sysctl_numa_balancing_scan_period_reset,
-		.maxlen		= sizeof(unsigned int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
 	{
 		.procname	= "numa_balancing_scan_period_max_ms",
 		.data		= &sysctl_numa_balancing_scan_period_max,
