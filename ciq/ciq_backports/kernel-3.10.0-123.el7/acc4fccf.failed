IB/mlx4: Make sure GID index 0 is always occupied

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [infiniband] mlx4: Make sure GID index 0 is always occupied (Doug Ledford) [1061729]
Rebuild_FUZZ: 96.84%
commit-author Moni Shoua <monis@mellanox.co.il>
commit acc4fccf4eff5b29e545995b75de77e60ea44aae
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/acc4fccf.failed

Make sure that for Ethernet ports, the port GID table index 0 is always
occupied with a default GID of the relevant IPv6 link-local adderss.

This provides better user experience for legacy applications that don't use
the RDMA CM and were working on index 0 prior to the IP addressing change.

Also, as GIDs are generated from IP addresses of the network devices that
are associated with the port, it's basically possible that the GID table
will be empty if no IP address was assigned.  This doesn't comply with the
IB spec section 4.1.1 "GID usage and properties".

	Signed-off-by: Moni Shoua <monis@mellanox.co.il>
	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Roland Dreier <roland@purestorage.com>
(cherry picked from commit acc4fccf4eff5b29e545995b75de77e60ea44aae)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx4/main.c
diff --cc drivers/infiniband/hw/mlx4/main.c
index 3163ef43b43e,b99d0ecc6a89..000000000000
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@@ -1362,7 -1357,8 +1362,12 @@@ static struct device_attribute *mlx4_cl
  	&dev_attr_board_id
  };
  
++<<<<<<< HEAD
 +static void mlx4_addrconf_ifid_eui48(u8 *eui, u16 vlan_id, struct net_device *dev)
++=======
+ static void mlx4_addrconf_ifid_eui48(u8 *eui, u16 vlan_id,
+ 				     struct net_device *dev)
++>>>>>>> acc4fccf4eff (IB/mlx4: Make sure GID index 0 is always occupied)
  {
  	memcpy(eui, dev->dev_addr, 3);
  	memcpy(eui + 5, dev->dev_addr + 3, 3);
@@@ -1407,125 -1401,335 +1412,386 @@@ static void update_gids_task(struct wor
  	kfree(gw);
  }
  
++<<<<<<< HEAD
 +static int update_ipv6_gids(struct mlx4_ib_dev *dev, int port, int clear)
++=======
+ static void reset_gids_task(struct work_struct *work)
+ {
+ 	struct update_gid_work *gw =
+ 			container_of(work, struct update_gid_work, work);
+ 	struct mlx4_cmd_mailbox *mailbox;
+ 	union ib_gid *gids;
+ 	int err;
+ 	int i;
+ 	struct mlx4_dev	*dev = gw->dev->dev;
+ 
+ 	mailbox = mlx4_alloc_cmd_mailbox(dev);
+ 	if (IS_ERR(mailbox)) {
+ 		pr_warn("reset gid table failed\n");
+ 		goto free;
+ 	}
+ 
+ 	gids = mailbox->buf;
+ 	memcpy(gids, gw->gids, sizeof(gw->gids));
+ 
+ 	for (i = 1; i < gw->dev->num_ports + 1; i++) {
+ 		if (mlx4_ib_port_link_layer(&gw->dev->ib_dev, i) ==
+ 					    IB_LINK_LAYER_ETHERNET) {
+ 			err = mlx4_cmd(dev, mailbox->dma,
+ 				       MLX4_SET_PORT_GID_TABLE << 8 | i,
+ 				       1, MLX4_CMD_SET_PORT,
+ 				       MLX4_CMD_TIME_CLASS_B,
+ 				       MLX4_CMD_WRAPPED);
+ 			if (err)
+ 				pr_warn(KERN_WARNING
+ 					"set port %d command failed\n", i);
+ 		}
+ 	}
+ 
+ 	mlx4_free_cmd_mailbox(dev, mailbox);
+ free:
+ 	kfree(gw);
+ }
+ 
+ static int update_gid_table(struct mlx4_ib_dev *dev, int port,
+ 			    union ib_gid *gid, int clear,
+ 			    int default_gid)
++>>>>>>> acc4fccf4eff (IB/mlx4: Make sure GID index 0 is always occupied)
  {
 +	struct net_device *ndev = dev->iboe.netdevs[port - 1];
  	struct update_gid_work *work;
 +	struct net_device *tmp;
  	int i;
 +	u8 *hits;
 +	int ret;
 +	union ib_gid gid;
 +	int free;
 +	int found;
  	int need_update = 0;
 -	int free = -1;
 -	int found = -1;
 -	int max_gids;
 +	u16 vid;
  
++<<<<<<< HEAD
 +	work = kzalloc(sizeof *work, GFP_ATOMIC);
 +	if (!work)
 +		return -ENOMEM;
 +
 +	hits = kzalloc(128, GFP_ATOMIC);
 +	if (!hits) {
 +		ret = -ENOMEM;
 +		goto out;
 +	}
 +
 +	rcu_read_lock();
 +	for_each_netdev_rcu(&init_net, tmp) {
 +		if (ndev && (tmp == ndev || rdma_vlan_dev_real_dev(tmp) == ndev)) {
 +			gid.global.subnet_prefix = cpu_to_be64(0xfe80000000000000LL);
 +			vid = rdma_vlan_dev_vlan_id(tmp);
 +			mlx4_addrconf_ifid_eui48(&gid.raw[8], vid, ndev);
 +			found = 0;
 +			free = -1;
 +			for (i = 0; i < 128; ++i) {
 +				if (free < 0 &&
 +				    !memcmp(&dev->iboe.gid_table[port - 1][i], &zgid, sizeof zgid))
 +					free = i;
 +				if (!memcmp(&dev->iboe.gid_table[port - 1][i], &gid, sizeof gid)) {
 +					hits[i] = 1;
 +					found = 1;
 +					break;
 +				}
 +			}
 +
 +			if (!found) {
 +				if (tmp == ndev &&
 +				    (memcmp(&dev->iboe.gid_table[port - 1][0],
 +					    &gid, sizeof gid) ||
 +				     !memcmp(&dev->iboe.gid_table[port - 1][0],
 +					     &zgid, sizeof gid))) {
 +					dev->iboe.gid_table[port - 1][0] = gid;
 +					++need_update;
 +					hits[0] = 1;
 +				} else if (free >= 0) {
 +					dev->iboe.gid_table[port - 1][free] = gid;
 +					hits[free] = 1;
 +					++need_update;
 +				}
++=======
+ 	if (default_gid) {
+ 		free = 0;
+ 	} else {
+ 		max_gids = dev->dev->caps.gid_table_len[port];
+ 		for (i = 1; i < max_gids; ++i) {
+ 			if (!memcmp(&dev->iboe.gid_table[port - 1][i], gid,
+ 				    sizeof(*gid)))
+ 				found = i;
+ 
+ 			if (clear) {
+ 				if (found >= 0) {
+ 					need_update = 1;
+ 					dev->iboe.gid_table[port - 1][found] =
+ 						zgid;
+ 					break;
+ 				}
+ 			} else {
+ 				if (found >= 0)
+ 					break;
+ 
+ 				if (free < 0 &&
+ 				    !memcmp(&dev->iboe.gid_table[port - 1][i],
+ 					    &zgid, sizeof(*gid)))
+ 					free = i;
++>>>>>>> acc4fccf4eff (IB/mlx4: Make sure GID index 0 is always occupied)
  			}
  		}
  	}
 +	rcu_read_unlock();
  
 -	if (found == -1 && !clear && free >= 0) {
 -		dev->iboe.gid_table[port - 1][free] = *gid;
 -		need_update = 1;
 -	}
 +	for (i = 0; i < 128; ++i)
 +		if (!hits[i]) {
 +			if (memcmp(&dev->iboe.gid_table[port - 1][i], &zgid, sizeof zgid))
 +				++need_update;
 +			dev->iboe.gid_table[port - 1][i] = zgid;
 +		}
  
 -	if (!need_update)
 -		return 0;
 +	if (need_update) {
 +		memcpy(work->gids, dev->iboe.gid_table[port - 1], sizeof work->gids);
 +		INIT_WORK(&work->work, update_gids_task);
 +		work->port = port;
 +		work->dev = dev;
 +		queue_work(wq, &work->work);
 +	} else
 +		kfree(work);
  
 -	work = kzalloc(sizeof(*work), GFP_ATOMIC);
 -	if (!work)
 -		return -ENOMEM;
 +	kfree(hits);
 +	return 0;
  
 -	memcpy(work->gids, dev->iboe.gid_table[port - 1], sizeof(work->gids));
 -	INIT_WORK(&work->work, update_gids_task);
 -	work->port = port;
 -	work->dev = dev;
 -	queue_work(wq, &work->work);
 +out:
 +	kfree(work);
 +	return ret;
 +}
  
 -	return 0;
 +static void handle_en_event(struct mlx4_ib_dev *dev, int port, unsigned long event)
 +{
 +	switch (event) {
 +	case NETDEV_UP:
 +	case NETDEV_CHANGEADDR:
 +		update_ipv6_gids(dev, port, 0);
 +		break;
 +
 +	case NETDEV_DOWN:
 +		update_ipv6_gids(dev, port, 1);
 +		dev->iboe.netdevs[port - 1] = NULL;
 +	}
  }
  
++<<<<<<< HEAD
 +static void netdev_added(struct mlx4_ib_dev *dev, int port)
++=======
+ static void mlx4_make_default_gid(struct  net_device *dev, union ib_gid *gid)
+ {
+ 	gid->global.subnet_prefix = cpu_to_be64(0xfe80000000000000LL);
+ 	mlx4_addrconf_ifid_eui48(&gid->raw[8], 0xffff, dev);
+ }
+ 
+ 
+ static int reset_gid_table(struct mlx4_ib_dev *dev)
++>>>>>>> acc4fccf4eff (IB/mlx4: Make sure GID index 0 is always occupied)
  {
 -	struct update_gid_work *work;
 -
 -
 -	work = kzalloc(sizeof(*work), GFP_ATOMIC);
 -	if (!work)
 -		return -ENOMEM;
 -	memset(dev->iboe.gid_table, 0, sizeof(dev->iboe.gid_table));
 -	memset(work->gids, 0, sizeof(work->gids));
 -	INIT_WORK(&work->work, reset_gids_task);
 -	work->dev = dev;
 -	queue_work(wq, &work->work);
 -	return 0;
 +	update_ipv6_gids(dev, port, 0);
  }
  
 -static int mlx4_ib_addr_event(int event, struct net_device *event_netdev,
 -			      struct mlx4_ib_dev *ibdev, union ib_gid *gid)
 +static void netdev_removed(struct mlx4_ib_dev *dev, int port)
  {
++<<<<<<< HEAD
 +	update_ipv6_gids(dev, port, 1);
++=======
+ 	struct mlx4_ib_iboe *iboe;
+ 	int port = 0;
+ 	struct net_device *real_dev = rdma_vlan_dev_real_dev(event_netdev) ?
+ 				rdma_vlan_dev_real_dev(event_netdev) :
+ 				event_netdev;
+ 	union ib_gid default_gid;
+ 
+ 	mlx4_make_default_gid(real_dev, &default_gid);
+ 
+ 	if (!memcmp(gid, &default_gid, sizeof(*gid)))
+ 		return 0;
+ 
+ 	if (event != NETDEV_DOWN && event != NETDEV_UP)
+ 		return 0;
+ 
+ 	if ((real_dev != event_netdev) &&
+ 	    (event == NETDEV_DOWN) &&
+ 	    rdma_link_local_addr((struct in6_addr *)gid))
+ 		return 0;
+ 
+ 	iboe = &ibdev->iboe;
+ 	spin_lock(&iboe->lock);
+ 
+ 	for (port = 1; port <= MLX4_MAX_PORTS; ++port)
+ 		if ((netif_is_bond_master(real_dev) &&
+ 		     (real_dev == iboe->masters[port - 1])) ||
+ 		     (!netif_is_bond_master(real_dev) &&
+ 		     (real_dev == iboe->netdevs[port - 1])))
+ 			update_gid_table(ibdev, port, gid,
+ 					 event == NETDEV_DOWN, 0);
+ 
+ 	spin_unlock(&iboe->lock);
+ 	return 0;
+ 
++>>>>>>> acc4fccf4eff (IB/mlx4: Make sure GID index 0 is always occupied)
  }
  
 -static u8 mlx4_ib_get_dev_port(struct net_device *dev,
 -			       struct mlx4_ib_dev *ibdev)
 -{
 -	u8 port = 0;
 -	struct mlx4_ib_iboe *iboe;
 -	struct net_device *real_dev = rdma_vlan_dev_real_dev(dev) ?
 -				rdma_vlan_dev_real_dev(dev) : dev;
 -
 -	iboe = &ibdev->iboe;
 -	spin_lock(&iboe->lock);
 -
 -	for (port = 1; port <= MLX4_MAX_PORTS; ++port)
 -		if ((netif_is_bond_master(real_dev) &&
 -		     (real_dev == iboe->masters[port - 1])) ||
 -		     (!netif_is_bond_master(real_dev) &&
 -		     (real_dev == iboe->netdevs[port - 1])))
 -			break;
 -
 -	spin_unlock(&iboe->lock);
 -
 -	if ((port == 0) || (port > MLX4_MAX_PORTS))
 -		return 0;
 -	else
 -		return port;
 -}
 -
 -static int mlx4_ib_inet_event(struct notifier_block *this, unsigned long event,
 +static int mlx4_ib_netdev_event(struct notifier_block *this, unsigned long event,
  				void *ptr)
  {
 +	struct net_device *dev = ptr;
  	struct mlx4_ib_dev *ibdev;
++<<<<<<< HEAD
 +	struct net_device *oldnd;
 +	struct mlx4_ib_iboe *iboe;
 +	int port;
 +
++=======
+ 	struct in_ifaddr *ifa = ptr;
+ 	union ib_gid gid;
+ 	struct net_device *event_netdev = ifa->ifa_dev->dev;
+ 
+ 	ipv6_addr_set_v4mapped(ifa->ifa_address, (struct in6_addr *)&gid);
+ 
+ 	ibdev = container_of(this, struct mlx4_ib_dev, iboe.nb_inet);
+ 
+ 	mlx4_ib_addr_event(event, event_netdev, ibdev, &gid);
+ 	return NOTIFY_DONE;
+ }
+ 
+ #if IS_ENABLED(CONFIG_IPV6)
+ static int mlx4_ib_inet6_event(struct notifier_block *this, unsigned long event,
+ 				void *ptr)
+ {
+ 	struct mlx4_ib_dev *ibdev;
+ 	struct inet6_ifaddr *ifa = ptr;
+ 	union  ib_gid *gid = (union ib_gid *)&ifa->addr;
+ 	struct net_device *event_netdev = ifa->idev->dev;
+ 
+ 	ibdev = container_of(this, struct mlx4_ib_dev, iboe.nb_inet6);
+ 
+ 	mlx4_ib_addr_event(event, event_netdev, ibdev, gid);
+ 	return NOTIFY_DONE;
+ }
+ #endif
+ 
+ static void mlx4_ib_get_dev_addr(struct net_device *dev,
+ 				 struct mlx4_ib_dev *ibdev, u8 port)
+ {
+ 	struct in_device *in_dev;
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	struct inet6_dev *in6_dev;
+ 	union ib_gid  *pgid;
+ 	struct inet6_ifaddr *ifp;
+ #endif
+ 	union ib_gid gid;
+ 
+ 
+ 	if ((port == 0) || (port > MLX4_MAX_PORTS))
+ 		return;
+ 
+ 	/* IPv4 gids */
+ 	in_dev = in_dev_get(dev);
+ 	if (in_dev) {
+ 		for_ifa(in_dev) {
+ 			/*ifa->ifa_address;*/
+ 			ipv6_addr_set_v4mapped(ifa->ifa_address,
+ 					       (struct in6_addr *)&gid);
+ 			update_gid_table(ibdev, port, &gid, 0, 0);
+ 		}
+ 		endfor_ifa(in_dev);
+ 		in_dev_put(in_dev);
+ 	}
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	/* IPv6 gids */
+ 	in6_dev = in6_dev_get(dev);
+ 	if (in6_dev) {
+ 		read_lock_bh(&in6_dev->lock);
+ 		list_for_each_entry(ifp, &in6_dev->addr_list, if_list) {
+ 			pgid = (union ib_gid *)&ifp->addr;
+ 			update_gid_table(ibdev, port, pgid, 0, 0);
+ 		}
+ 		read_unlock_bh(&in6_dev->lock);
+ 		in6_dev_put(in6_dev);
+ 	}
+ #endif
+ }
+ 
+ static void mlx4_ib_set_default_gid(struct mlx4_ib_dev *ibdev,
+ 				 struct  net_device *dev, u8 port)
+ {
+ 	union ib_gid gid;
+ 	mlx4_make_default_gid(dev, &gid);
+ 	update_gid_table(ibdev, port, &gid, 0, 1);
+ }
+ 
+ static int mlx4_ib_init_gid_table(struct mlx4_ib_dev *ibdev)
+ {
+ 	struct	net_device *dev;
+ 
+ 	if (reset_gid_table(ibdev))
+ 		return -1;
+ 
+ 	read_lock(&dev_base_lock);
+ 
+ 	for_each_netdev(&init_net, dev) {
+ 		u8 port = mlx4_ib_get_dev_port(dev, ibdev);
+ 		if (port)
+ 			mlx4_ib_get_dev_addr(dev, ibdev, port);
+ 	}
+ 
+ 	read_unlock(&dev_base_lock);
+ 
+ 	return 0;
+ }
+ 
+ static void mlx4_ib_scan_netdevs(struct mlx4_ib_dev *ibdev)
+ {
+ 	struct mlx4_ib_iboe *iboe;
+ 	int port;
+ 
+ 	iboe = &ibdev->iboe;
+ 
+ 	spin_lock(&iboe->lock);
+ 	mlx4_foreach_ib_transport_port(port, ibdev->dev) {
+ 		struct net_device *old_master = iboe->masters[port - 1];
+ 		struct net_device *curr_master;
+ 		iboe->netdevs[port - 1] =
+ 			mlx4_get_protocol_dev(ibdev->dev, MLX4_PROT_ETH, port);
+ 		if (iboe->netdevs[port - 1])
+ 			mlx4_ib_set_default_gid(ibdev,
+ 						iboe->netdevs[port - 1], port);
+ 		if (iboe->netdevs[port - 1] &&
+ 		    netif_is_bond_slave(iboe->netdevs[port - 1])) {
+ 			rtnl_lock();
+ 			iboe->masters[port - 1] = netdev_master_upper_dev_get(
+ 				iboe->netdevs[port - 1]);
+ 			rtnl_unlock();
+ 		}
+ 		curr_master = iboe->masters[port - 1];
+ 
+ 		/* if bonding is used it is possible that we add it to masters
+ 		    only after IP address is assigned to the net bonding
+ 		    interface */
+ 		if (curr_master && (old_master != curr_master))
+ 			mlx4_ib_get_dev_addr(curr_master, ibdev, port);
+ 	}
+ 
+ 	spin_unlock(&iboe->lock);
+ }
+ 
+ static int mlx4_ib_netdev_event(struct notifier_block *this,
+ 				unsigned long event, void *ptr)
+ {
+ 	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+ 	struct mlx4_ib_dev *ibdev;
+ 
++>>>>>>> acc4fccf4eff (IB/mlx4: Make sure GID index 0 is always occupied)
  	if (!net_eq(dev_net(dev), &init_net))
  		return NOTIFY_DONE;
  
* Unmerged path drivers/infiniband/hw/mlx4/main.c
