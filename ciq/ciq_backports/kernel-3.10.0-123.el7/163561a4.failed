net/mlx4_en: Datapath structures are allocated per NUMA node

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [ethernet] mlx4: Datapath structures are allocated per NUMA node (Amir Vadai) [1030563 1030565 1030568 1030570 1030571 1030573 1030575]
Rebuild_FUZZ: 93.81%
commit-author Eugenia Emantayev <eugenia@mellanox.com>
commit 163561a4e2f8af44e96453bc10c7a4f9bcc736e1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/163561a4.failed

For each RX/TX ring and its CQ, allocation is done on a NUMA node that
corresponds to the core that the data structure should operate on.
The assumption is that the core number is reflected by the ring index.
The affected allocations are the ring/CQ data structures,
the TX/RX info and the shared HW/SW buffer.
For TX rings, each core has rings of all UPs.

	Signed-off-by: Yevgeny Petrilin <yevgenyp@mellanox.com>
	Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
	Reviewed-by: Hadar Hen Zion <hadarh@mellanox.com>
	Signed-off-by: Amir Vadai <amirv@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 163561a4e2f8af44e96453bc10c7a4f9bcc736e1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx4/en_cq.c
#	drivers/net/ethernet/mellanox/mlx4/en_rx.c
#	drivers/net/ethernet/mellanox/mlx4/en_tx.c
#	drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
diff --cc drivers/net/ethernet/mellanox/mlx4/en_cq.c
index 1e6c594d6d04,3a098cc4d349..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_cq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_cq.c
@@@ -44,12 -44,23 +44,30 @@@ static void mlx4_en_cq_event(struct mlx
  
  
  int mlx4_en_create_cq(struct mlx4_en_priv *priv,
++<<<<<<< HEAD
 +		      struct mlx4_en_cq *cq,
 +		      int entries, int ring, enum cq_type mode)
++=======
+ 		      struct mlx4_en_cq **pcq,
+ 		      int entries, int ring, enum cq_type mode,
+ 		      int node)
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  {
  	struct mlx4_en_dev *mdev = priv->mdev;
 -	struct mlx4_en_cq *cq;
  	int err;
  
++<<<<<<< HEAD
++=======
+ 	cq = kzalloc_node(sizeof(*cq), GFP_KERNEL, node);
+ 	if (!cq) {
+ 		cq = kzalloc(sizeof(*cq), GFP_KERNEL);
+ 		if (!cq) {
+ 			en_err(priv, "Failed to allocate CQ structure\n");
+ 			return -ENOMEM;
+ 		}
+ 	}
+ 
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  	cq->size = entries;
  	cq->buf_size = cq->size * mdev->dev->caps.cqe_size;
  
@@@ -57,10 -68,15 +75,15 @@@
  	cq->is_tx = mode;
  	spin_lock_init(&cq->lock);
  
+ 	/* Allocate HW buffers on provided NUMA node.
+ 	 * dev->numa_node is used in mtt range allocation flow.
+ 	 */
+ 	set_dev_node(&mdev->dev->pdev->dev, node);
  	err = mlx4_alloc_hwq_res(mdev->dev, &cq->wqres,
  				cq->buf_size, 2 * PAGE_SIZE);
+ 	set_dev_node(&mdev->dev->pdev->dev, mdev->dev->numa_node);
  	if (err)
 -		goto err_cq;
 +		return err;
  
  	err = mlx4_en_map_buffer(&cq->wqres.buf);
  	if (err)
diff --cc drivers/net/ethernet/mellanox/mlx4/en_rx.c
index a964fbbb897f,07a1d0fbae47..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
@@@ -317,12 -319,23 +317,29 @@@ static void mlx4_en_free_rx_buf(struct 
  }
  
  int mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,
++<<<<<<< HEAD
 +			   struct mlx4_en_rx_ring *ring, u32 size, u16 stride)
++=======
+ 			   struct mlx4_en_rx_ring **pring,
+ 			   u32 size, u16 stride, int node)
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  {
  	struct mlx4_en_dev *mdev = priv->mdev;
 -	struct mlx4_en_rx_ring *ring;
  	int err = -ENOMEM;
  	int tmp;
  
++<<<<<<< HEAD
++=======
+ 	ring = kzalloc_node(sizeof(*ring), GFP_KERNEL, node);
+ 	if (!ring) {
+ 		ring = kzalloc(sizeof(*ring), GFP_KERNEL);
+ 		if (!ring) {
+ 			en_err(priv, "Failed to allocate RX ring structure\n");
+ 			return -ENOMEM;
+ 		}
+ 	}
+ 
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  	ring->prod = 0;
  	ring->cons = 0;
  	ring->size = size;
@@@ -333,17 -346,25 +350,31 @@@
  
  	tmp = size * roundup_pow_of_two(MLX4_EN_MAX_RX_FRAGS *
  					sizeof(struct mlx4_en_rx_alloc));
++<<<<<<< HEAD
 +	ring->rx_info = vmalloc(tmp);
 +	if (!ring->rx_info)
 +		return -ENOMEM;
++=======
+ 	ring->rx_info = vmalloc_node(tmp, node);
+ 	if (!ring->rx_info) {
+ 		ring->rx_info = vmalloc(tmp);
+ 		if (!ring->rx_info) {
+ 			err = -ENOMEM;
+ 			goto err_ring;
+ 		}
+ 	}
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  
  	en_dbg(DRV, priv, "Allocated rx_info ring at addr:%p size:%d\n",
  		 ring->rx_info, tmp);
  
+ 	/* Allocate HW buffers on provided NUMA node */
+ 	set_dev_node(&mdev->dev->pdev->dev, node);
  	err = mlx4_alloc_hwq_res(mdev->dev, &ring->wqres,
  				 ring->buf_size, 2 * PAGE_SIZE);
+ 	set_dev_node(&mdev->dev->pdev->dev, mdev->dev->numa_node);
  	if (err)
 -		goto err_info;
 +		goto err_ring;
  
  	err = mlx4_en_map_buffer(&ring->wqres.buf);
  	if (err) {
diff --cc drivers/net/ethernet/mellanox/mlx4/en_tx.c
index 43b8faafe2af,f54ebd5a1702..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
@@@ -54,13 -54,23 +54,30 @@@ module_param_named(inline_thold, inline
  MODULE_PARM_DESC(inline_thold, "threshold for using inline data");
  
  int mlx4_en_create_tx_ring(struct mlx4_en_priv *priv,
++<<<<<<< HEAD
 +			   struct mlx4_en_tx_ring *ring, int qpn, u32 size,
 +			   u16 stride)
++=======
+ 			   struct mlx4_en_tx_ring **pring, int qpn, u32 size,
+ 			   u16 stride, int node)
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  {
  	struct mlx4_en_dev *mdev = priv->mdev;
 -	struct mlx4_en_tx_ring *ring;
  	int tmp;
  	int err;
  
++<<<<<<< HEAD
++=======
+ 	ring = kzalloc_node(sizeof(*ring), GFP_KERNEL, node);
+ 	if (!ring) {
+ 		ring = kzalloc(sizeof(*ring), GFP_KERNEL);
+ 		if (!ring) {
+ 			en_err(priv, "Failed allocating TX ring\n");
+ 			return -ENOMEM;
+ 		}
+ 	}
+ 
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  	ring->size = size;
  	ring->size_mask = size - 1;
  	ring->stride = stride;
@@@ -68,17 -78,25 +85,36 @@@
  	inline_thold = min(inline_thold, MAX_INLINE);
  
  	tmp = size * sizeof(struct mlx4_en_tx_info);
++<<<<<<< HEAD
 +	ring->tx_info = vmalloc(tmp);
 +	if (!ring->tx_info)
 +		return -ENOMEM;
++=======
+ 	ring->tx_info = vmalloc_node(tmp, node);
+ 	if (!ring->tx_info) {
+ 		ring->tx_info = vmalloc(tmp);
+ 		if (!ring->tx_info) {
+ 			err = -ENOMEM;
+ 			goto err_ring;
+ 		}
+ 	}
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  
  	en_dbg(DRV, priv, "Allocated tx_info ring at addr:%p size:%d\n",
  		 ring->tx_info, tmp);
  
- 	ring->bounce_buf = kmalloc(MAX_DESC_SIZE, GFP_KERNEL);
+ 	ring->bounce_buf = kmalloc_node(MAX_DESC_SIZE, GFP_KERNEL, node);
  	if (!ring->bounce_buf) {
++<<<<<<< HEAD
 +		err = -ENOMEM;
 +		goto err_tx;
++=======
+ 		ring->bounce_buf = kmalloc(MAX_DESC_SIZE, GFP_KERNEL);
+ 		if (!ring->bounce_buf) {
+ 			err = -ENOMEM;
+ 			goto err_info;
+ 		}
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  	}
  	ring->buf_size = ALIGN(size * ring->stride, MLX4_EN_PAGE_SIZE);
  
diff --cc drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
index 49b90487ac48,f3758de59c05..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
@@@ -577,9 -704,9 +577,15 @@@ void mlx4_en_stop_port(struct net_devic
  void mlx4_en_free_resources(struct mlx4_en_priv *priv);
  int mlx4_en_alloc_resources(struct mlx4_en_priv *priv);
  
++<<<<<<< HEAD
 +int mlx4_en_create_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq,
 +		      int entries, int ring, enum cq_type mode);
 +void mlx4_en_destroy_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
++=======
+ int mlx4_en_create_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq **pcq,
+ 		      int entries, int ring, enum cq_type mode, int node);
+ void mlx4_en_destroy_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq **pcq);
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  int mlx4_en_activate_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq,
  			int cq_idx);
  void mlx4_en_deactivate_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
@@@ -590,9 -717,11 +596,17 @@@ void mlx4_en_tx_irq(struct mlx4_cq *mcq
  u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb);
  netdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev);
  
++<<<<<<< HEAD
 +int mlx4_en_create_tx_ring(struct mlx4_en_priv *priv, struct mlx4_en_tx_ring *ring,
 +			   int qpn, u32 size, u16 stride);
 +void mlx4_en_destroy_tx_ring(struct mlx4_en_priv *priv, struct mlx4_en_tx_ring *ring);
++=======
+ int mlx4_en_create_tx_ring(struct mlx4_en_priv *priv,
+ 			   struct mlx4_en_tx_ring **pring,
+ 			   int qpn, u32 size, u16 stride, int node);
+ void mlx4_en_destroy_tx_ring(struct mlx4_en_priv *priv,
+ 			     struct mlx4_en_tx_ring **pring);
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  int mlx4_en_activate_tx_ring(struct mlx4_en_priv *priv,
  			     struct mlx4_en_tx_ring *ring,
  			     int cq, int user_prio);
@@@ -600,10 -729,10 +614,15 @@@ void mlx4_en_deactivate_tx_ring(struct 
  				struct mlx4_en_tx_ring *ring);
  
  int mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,
++<<<<<<< HEAD
 +			   struct mlx4_en_rx_ring *ring,
 +			   u32 size, u16 stride);
++=======
+ 			   struct mlx4_en_rx_ring **pring,
+ 			   u32 size, u16 stride, int node);
++>>>>>>> 163561a4e2f8 (net/mlx4_en: Datapath structures are allocated per NUMA node)
  void mlx4_en_destroy_rx_ring(struct mlx4_en_priv *priv,
 -			     struct mlx4_en_rx_ring **pring,
 +			     struct mlx4_en_rx_ring *ring,
  			     u32 size, u16 stride);
  int mlx4_en_activate_rx_rings(struct mlx4_en_priv *priv);
  void mlx4_en_deactivate_rx_ring(struct mlx4_en_priv *priv,
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_cq.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/en_netdev.c b/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
index 39ca6f97fa10..1c1f4a15b72b 100644
--- a/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
@@ -1856,6 +1856,7 @@ int mlx4_en_alloc_resources(struct mlx4_en_priv *priv)
 	struct mlx4_en_port_profile *prof = priv->prof;
 	int i;
 	int err;
+	int node;
 
 	err = mlx4_qp_reserve_range(priv->mdev->dev, priv->tx_ring_num, 256, &priv->base_tx_qpn);
 	if (err) {
@@ -1865,23 +1866,26 @@ int mlx4_en_alloc_resources(struct mlx4_en_priv *priv)
 
 	/* Create tx Rings */
 	for (i = 0; i < priv->tx_ring_num; i++) {
+		node = cpu_to_node(i % num_online_cpus());
 		if (mlx4_en_create_cq(priv, &priv->tx_cq[i],
-				      prof->tx_ring_size, i, TX))
+				      prof->tx_ring_size, i, TX, node))
 			goto err;
 
 		if (mlx4_en_create_tx_ring(priv, &priv->tx_ring[i], priv->base_tx_qpn + i,
-					   prof->tx_ring_size, TXBB_SIZE))
+					   prof->tx_ring_size, TXBB_SIZE, node))
 			goto err;
 	}
 
 	/* Create rx Rings */
 	for (i = 0; i < priv->rx_ring_num; i++) {
+		node = cpu_to_node(i % num_online_cpus());
 		if (mlx4_en_create_cq(priv, &priv->rx_cq[i],
-				      prof->rx_ring_size, i, RX))
+				      prof->rx_ring_size, i, RX, node))
 			goto err;
 
 		if (mlx4_en_create_rx_ring(priv, &priv->rx_ring[i],
-					   prof->rx_ring_size, priv->stride))
+					   prof->rx_ring_size, priv->stride,
+					   node))
 			goto err;
 	}
 
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_tx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
diff --git a/drivers/net/ethernet/mellanox/mlx4/pd.c b/drivers/net/ethernet/mellanox/mlx4/pd.c
index 00f223acada7..84cfb40bf451 100644
--- a/drivers/net/ethernet/mellanox/mlx4/pd.c
+++ b/drivers/net/ethernet/mellanox/mlx4/pd.c
@@ -168,7 +168,7 @@ void mlx4_uar_free(struct mlx4_dev *dev, struct mlx4_uar *uar)
 }
 EXPORT_SYMBOL_GPL(mlx4_uar_free);
 
-int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf)
+int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf, int node)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_uar *uar;
@@ -186,10 +186,13 @@ int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf)
 			err = -ENOMEM;
 			goto out;
 		}
-		uar = kmalloc(sizeof *uar, GFP_KERNEL);
+		uar = kmalloc_node(sizeof(*uar), GFP_KERNEL, node);
 		if (!uar) {
-			err = -ENOMEM;
-			goto out;
+			uar = kmalloc(sizeof(*uar), GFP_KERNEL);
+			if (!uar) {
+				err = -ENOMEM;
+				goto out;
+			}
 		}
 		err = mlx4_uar_alloc(dev, uar);
 		if (err)
diff --git a/include/linux/mlx4/device.h b/include/linux/mlx4/device.h
index 59da4f816fde..bf4653e49818 100644
--- a/include/linux/mlx4/device.h
+++ b/include/linux/mlx4/device.h
@@ -834,7 +834,7 @@ void mlx4_xrcd_free(struct mlx4_dev *dev, u32 xrcdn);
 
 int mlx4_uar_alloc(struct mlx4_dev *dev, struct mlx4_uar *uar);
 void mlx4_uar_free(struct mlx4_dev *dev, struct mlx4_uar *uar);
-int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf);
+int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf, int node);
 void mlx4_bf_free(struct mlx4_dev *dev, struct mlx4_bf *bf);
 
 int mlx4_mtt_init(struct mlx4_dev *dev, int npages, int page_shift,
