blk-throttle: make blk_throtl_drain() ready for hierarchy

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Tejun Heo <tj@kernel.org>
commit 2a12f0dcdad1ba7c0e53bbff8e5f6d0ee7a29882
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/2a12f0dc.failed

The current blk_throtl_drain() assumes that all active throtl_grps are
queued on throtl_data->service_queue, which won't be true once
hierarchy support is implemented.

This patch makes blk_throtl_drain() perform post-order walk of the
blkg hierarchy draining each associated throtl_grp, which guarantees
that all bios will eventually be pushed to the top-level service_queue
in throtl_data.

	Signed-off-by: Tejun Heo <tj@kernel.org>
	Acked-by: Vivek Goyal <vgoyal@redhat.com>
(cherry picked from commit 2a12f0dcdad1ba7c0e53bbff8e5f6d0ee7a29882)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-throttle.c
diff --cc block/blk-throttle.c
index e65e45a33372,8c6e13359781..000000000000
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@@ -1162,29 -1331,35 +1184,59 @@@ void blk_throtl_drain(struct request_qu
  	__releases(q->queue_lock) __acquires(q->queue_lock)
  {
  	struct throtl_data *td = q->td;
++<<<<<<< HEAD
 +	struct throtl_service_queue *parent_sq = &td->service_queue;
 +	struct throtl_grp *tg;
 +	struct bio_list bl;
++=======
+ 	struct blkcg_gq *blkg;
+ 	struct cgroup *pos_cgrp;
++>>>>>>> 2a12f0dcdad1 (blk-throttle: make blk_throtl_drain() ready for hierarchy)
  	struct bio *bio;
 -	int rw;
  
  	queue_lockdep_assert_held(q);
+ 	rcu_read_lock();
  
++<<<<<<< HEAD
 +	bio_list_init(&bl);
 +
 +	while ((tg = throtl_rb_first(parent_sq))) {
 +		struct throtl_service_queue *sq = &tg->service_queue;
 +
 +		throtl_dequeue_tg(tg, parent_sq);
 +
 +		while ((bio = bio_list_peek(&sq->bio_lists[READ])))
 +			tg_dispatch_one_bio(tg, bio_data_dir(bio), &bl);
 +		while ((bio = bio_list_peek(&sq->bio_lists[WRITE])))
 +			tg_dispatch_one_bio(tg, bio_data_dir(bio), &bl);
 +	}
 +	spin_unlock_irq(q->queue_lock);
 +
 +	while ((bio = bio_list_pop(&bl)))
 +		generic_make_request(bio);
++=======
+ 	/*
+ 	 * Drain each tg while doing post-order walk on the blkg tree, so
+ 	 * that all bios are propagated to td->service_queue.  It'd be
+ 	 * better to walk service_queue tree directly but blkg walk is
+ 	 * easier.
+ 	 */
+ 	blkg_for_each_descendant_post(blkg, pos_cgrp, td->queue->root_blkg)
+ 		tg_drain_bios(&blkg_to_tg(blkg)->service_queue);
+ 
+ 	tg_drain_bios(&td_root_tg(td)->service_queue);
+ 
+ 	/* finally, transfer bios from top-level tg's into the td */
+ 	tg_drain_bios(&td->service_queue);
+ 
+ 	rcu_read_unlock();
+ 	spin_unlock_irq(q->queue_lock);
+ 
+ 	/* all bios now should be in td->service_queue, issue them */
+ 	for (rw = READ; rw <= WRITE; rw++)
+ 		while ((bio = bio_list_pop(&td->service_queue.bio_lists[rw])))
+ 			generic_make_request(bio);
++>>>>>>> 2a12f0dcdad1 (blk-throttle: make blk_throtl_drain() ready for hierarchy)
  
  	spin_lock_irq(q->queue_lock);
  }
* Unmerged path block/blk-throttle.c
