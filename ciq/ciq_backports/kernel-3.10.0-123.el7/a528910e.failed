mm: thrash detection-based file cache sizing

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] thrash detection-based file cache sizing (Johannes Weiner) [1062372]
Rebuild_FUZZ: 95.24%
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit a528910e12ec7ee203095eb1711468a66b9b60b0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/a528910e.failed

The VM maintains cached filesystem pages on two types of lists.  One
list holds the pages recently faulted into the cache, the other list
holds pages that have been referenced repeatedly on that first list.
The idea is to prefer reclaiming young pages over those that have shown
to benefit from caching in the past.  We call the recently usedbut
ultimately was not significantly better than a FIFO policy and still
thrashed cache based on eviction speed, rather than actual demand for
cache.

This patch solves one half of the problem by decoupling the ability to
detect working set changes from the inactive list size.  By maintaining
a history of recently evicted file pages it can detect frequently used
pages with an arbitrarily small inactive list size, and subsequently
apply pressure on the active list based on actual demand for cache, not
just overall eviction speed.

Every zone maintains a counter that tracks inactive list aging speed.
When a page is evicted, a snapshot of this counter is stored in the
now-empty page cache radix tree slot.  On refault, the minimum access
distance of the page can be assessed, to evaluate whether the page
should be part of the active list or not.

This fixes the VM's blindness towards working set changes in excess of
the inactive list.  And it's the foundation to further improve the
protection ability and reduce the minimum inactive list size of 50%.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Reviewed-by: Minchan Kim <minchan@kernel.org>
	Reviewed-by: Bob Liu <bob.liu@oracle.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Christoph Hellwig <hch@infradead.org>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Greg Thelen <gthelen@google.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jan Kara <jack@suse.cz>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Cc: Luigi Semenzato <semenzato@google.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Metin Doslu <metin@citusdata.com>
	Cc: Michel Lespinasse <walken@google.com>
	Cc: Ozgun Erdogan <ozgun@citusdata.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Roman Gushchin <klamm@yandex-team.ru>
	Cc: Ryan Mallon <rmallon@gmail.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a528910e12ec7ee203095eb1711468a66b9b60b0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/Makefile
#	mm/filemap.c
#	mm/vmscan.c
diff --cc mm/Makefile
index 72c5acb9345f,cdd741519ee0..000000000000
--- a/mm/Makefile
+++ b/mm/Makefile
@@@ -17,7 -17,7 +17,11 @@@ obj-y			:= filemap.o mempool.o oom_kill
  			   util.o mmzone.o vmstat.o backing-dev.o \
  			   mm_init.o mmu_context.o percpu.o slab_common.o \
  			   compaction.o balloon_compaction.o \
++<<<<<<< HEAD
 +			   interval_tree.o $(mmu-y)
++=======
+ 			   interval_tree.o list_lru.o workingset.o $(mmu-y)
++>>>>>>> a528910e12ec (mm: thrash detection-based file cache sizing)
  
  obj-y += init-mm.o
  
diff --cc mm/filemap.c
index 844f5d8d25f6,a603c4d7d3c9..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -446,18 -468,36 +446,51 @@@ int replace_page_cache_page(struct pag
  }
  EXPORT_SYMBOL_GPL(replace_page_cache_page);
  
++<<<<<<< HEAD
 +/**
 + * add_to_page_cache_locked - add a locked page to the pagecache
 + * @page:	page to add
 + * @mapping:	the page's address_space
 + * @offset:	page index
 + * @gfp_mask:	page allocation mode
 + *
 + * This function is used to add a page to the pagecache. It must be locked.
 + * This function does not add the page to the LRU.  The caller must do that.
 + */
 +int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
 +		pgoff_t offset, gfp_t gfp_mask)
++=======
+ static int page_cache_tree_insert(struct address_space *mapping,
+ 				  struct page *page, void **shadowp)
+ {
+ 	void **slot;
+ 	int error;
+ 
+ 	slot = radix_tree_lookup_slot(&mapping->page_tree, page->index);
+ 	if (slot) {
+ 		void *p;
+ 
+ 		p = radix_tree_deref_slot_protected(slot, &mapping->tree_lock);
+ 		if (!radix_tree_exceptional_entry(p))
+ 			return -EEXIST;
+ 		radix_tree_replace_slot(slot, page);
+ 		mapping->nrshadows--;
+ 		mapping->nrpages++;
+ 		if (shadowp)
+ 			*shadowp = p;
+ 		return 0;
+ 	}
+ 	error = radix_tree_insert(&mapping->page_tree, page->index, page);
+ 	if (!error)
+ 		mapping->nrpages++;
+ 	return error;
+ }
+ 
+ static int __add_to_page_cache_locked(struct page *page,
+ 				      struct address_space *mapping,
+ 				      pgoff_t offset, gfp_t gfp_mask,
+ 				      void **shadowp)
++>>>>>>> a528910e12ec (mm: thrash detection-based file cache sizing)
  {
  	int error;
  
@@@ -467,34 -507,52 +500,76 @@@
  	error = mem_cgroup_cache_charge(page, current->mm,
  					gfp_mask & GFP_RECLAIM_MASK);
  	if (error)
 -		return error;
 +		goto out;
  
 -	error = radix_tree_maybe_preload(gfp_mask & ~__GFP_HIGHMEM);
 -	if (error) {
 +	error = radix_tree_preload(gfp_mask & ~__GFP_HIGHMEM);
 +	if (error == 0) {
 +		page_cache_get(page);
 +		page->mapping = mapping;
 +		page->index = offset;
 +
 +		spin_lock_irq(&mapping->tree_lock);
 +		error = radix_tree_insert(&mapping->page_tree, offset, page);
 +		if (likely(!error)) {
 +			mapping->nrpages++;
 +			__inc_zone_page_state(page, NR_FILE_PAGES);
 +			spin_unlock_irq(&mapping->tree_lock);
 +			trace_mm_filemap_add_to_page_cache(page);
 +		} else {
 +			page->mapping = NULL;
 +			/* Leave page->index set: truncation relies upon it */
 +			spin_unlock_irq(&mapping->tree_lock);
 +			mem_cgroup_uncharge_cache_page(page);
 +			page_cache_release(page);
 +		}
 +		radix_tree_preload_end();
 +	} else
  		mem_cgroup_uncharge_cache_page(page);
++<<<<<<< HEAD
 +out:
++=======
+ 		return error;
+ 	}
+ 
+ 	page_cache_get(page);
+ 	page->mapping = mapping;
+ 	page->index = offset;
+ 
+ 	spin_lock_irq(&mapping->tree_lock);
+ 	error = page_cache_tree_insert(mapping, page, shadowp);
+ 	radix_tree_preload_end();
+ 	if (unlikely(error))
+ 		goto err_insert;
+ 	__inc_zone_page_state(page, NR_FILE_PAGES);
+ 	spin_unlock_irq(&mapping->tree_lock);
+ 	trace_mm_filemap_add_to_page_cache(page);
+ 	return 0;
+ err_insert:
+ 	page->mapping = NULL;
+ 	/* Leave page->index set: truncation relies upon it */
+ 	spin_unlock_irq(&mapping->tree_lock);
+ 	mem_cgroup_uncharge_cache_page(page);
+ 	page_cache_release(page);
++>>>>>>> a528910e12ec (mm: thrash detection-based file cache sizing)
  	return error;
  }
+ 
+ /**
+  * add_to_page_cache_locked - add a locked page to the pagecache
+  * @page:	page to add
+  * @mapping:	the page's address_space
+  * @offset:	page index
+  * @gfp_mask:	page allocation mode
+  *
+  * This function is used to add a page to the pagecache. It must be locked.
+  * This function does not add the page to the LRU.  The caller must do that.
+  */
+ int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
+ 		pgoff_t offset, gfp_t gfp_mask)
+ {
+ 	return __add_to_page_cache_locked(page, mapping, offset,
+ 					  gfp_mask, NULL);
+ }
  EXPORT_SYMBOL(add_to_page_cache_locked);
  
  int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
diff --cc mm/vmscan.c
index 65cbae5b85c4,1f56a80a7c41..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -497,10 -570,23 +498,28 @@@ static int __remove_mapping(struct addr
  		swapcache_free(swap, page);
  	} else {
  		void (*freepage)(struct page *);
+ 		void *shadow = NULL;
  
  		freepage = mapping->a_ops->freepage;
++<<<<<<< HEAD
 +
 +		__delete_from_page_cache(page);
++=======
+ 		/*
+ 		 * Remember a shadow entry for reclaimed file cache in
+ 		 * order to detect refaults, thus thrashing, later on.
+ 		 *
+ 		 * But don't store shadows in an address space that is
+ 		 * already exiting.  This is not just an optizimation,
+ 		 * inode reclaim needs to empty out the radix tree or
+ 		 * the nodes are lost.  Don't plant shadows behind its
+ 		 * back.
+ 		 */
+ 		if (reclaimed && page_is_file_cache(page) &&
+ 		    !mapping_exiting(mapping))
+ 			shadow = workingset_eviction(mapping, page);
+ 		__delete_from_page_cache(page, shadow);
++>>>>>>> a528910e12ec (mm: thrash detection-based file cache sizing)
  		spin_unlock_irq(&mapping->tree_lock);
  		mem_cgroup_uncharge_cache_page(page);
  
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 493d0844aa75..8bf0de2070c0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -142,6 +142,8 @@ enum zone_stat_item {
 	NUMA_LOCAL,		/* allocation from local node */
 	NUMA_OTHER,		/* allocation from other node */
 #endif
+	WORKINGSET_REFAULT,
+	WORKINGSET_ACTIVATE,
 	NR_ANON_TRANSPARENT_HUGEPAGES,
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
@@ -393,6 +395,9 @@ struct zone {
 	spinlock_t		lru_lock;
 	struct lruvec		lruvec;
 
+	/* Evictions & activations on the inactive file list */
+	atomic_long_t		inactive_age;
+
 	unsigned long		pages_scanned;	   /* since last reclaim */
 	unsigned long		flags;		   /* zone flags, see below */
 
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 85d74373002c..bd61f3916242 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -222,6 +222,11 @@ struct swap_list_t {
 	int next;	/* swapfile to be used next */
 };
 
+/* linux/mm/workingset.c */
+void *workingset_eviction(struct address_space *mapping, struct page *page);
+bool workingset_refault(void *shadow);
+void workingset_activation(struct page *page);
+
 /* linux/mm/page_alloc.c */
 extern unsigned long totalram_pages;
 extern unsigned long totalreserve_pages;
* Unmerged path mm/Makefile
* Unmerged path mm/filemap.c
diff --git a/mm/swap.c b/mm/swap.c
index 62b78a6e224f..da28de6fab85 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -482,6 +482,8 @@ void mark_page_accessed(struct page *page)
 		else
 			__lru_cache_activate_page(page);
 		ClearPageReferenced(page);
+		if (page_is_file_cache(page))
+			workingset_activation(page);
 	} else if (!PageReferenced(page)) {
 		SetPageReferenced(page);
 	}
* Unmerged path mm/vmscan.c
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 4a69ef011d72..1494d4017e7c 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -738,6 +738,8 @@ const char * const vmstat_text[] = {
 	"numa_local",
 	"numa_other",
 #endif
+	"workingset_refault",
+	"workingset_activate",
 	"nr_anon_transparent_hugepages",
 	"nr_free_cma",
 	"nr_dirty_threshold",
diff --git a/mm/workingset.c b/mm/workingset.c
new file mode 100644
index 000000000000..8a6c7cff4923
--- /dev/null
+++ b/mm/workingset.c
@@ -0,0 +1,253 @@
+/*
+ * Workingset detection
+ *
+ * Copyright (C) 2013 Red Hat, Inc., Johannes Weiner
+ */
+
+#include <linux/memcontrol.h>
+#include <linux/writeback.h>
+#include <linux/pagemap.h>
+#include <linux/atomic.h>
+#include <linux/module.h>
+#include <linux/swap.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+
+/*
+ *		Double CLOCK lists
+ *
+ * Per zone, two clock lists are maintained for file pages: the
+ * inactive and the active list.  Freshly faulted pages start out at
+ * the head of the inactive list and page reclaim scans pages from the
+ * tail.  Pages that are accessed multiple times on the inactive list
+ * are promoted to the active list, to protect them from reclaim,
+ * whereas active pages are demoted to the inactive list when the
+ * active list grows too big.
+ *
+ *   fault ------------------------+
+ *                                 |
+ *              +--------------+   |            +-------------+
+ *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+
+ *              +--------------+                +-------------+    |
+ *                     |                                           |
+ *                     +-------------- promotion ------------------+
+ *
+ *
+ *		Access frequency and refault distance
+ *
+ * A workload is thrashing when its pages are frequently used but they
+ * are evicted from the inactive list every time before another access
+ * would have promoted them to the active list.
+ *
+ * In cases where the average access distance between thrashing pages
+ * is bigger than the size of memory there is nothing that can be
+ * done - the thrashing set could never fit into memory under any
+ * circumstance.
+ *
+ * However, the average access distance could be bigger than the
+ * inactive list, yet smaller than the size of memory.  In this case,
+ * the set could fit into memory if it weren't for the currently
+ * active pages - which may be used more, hopefully less frequently:
+ *
+ *      +-memory available to cache-+
+ *      |                           |
+ *      +-inactive------+-active----+
+ *  a b | c d e f g h i | J K L M N |
+ *      +---------------+-----------+
+ *
+ * It is prohibitively expensive to accurately track access frequency
+ * of pages.  But a reasonable approximation can be made to measure
+ * thrashing on the inactive list, after which refaulting pages can be
+ * activated optimistically to compete with the existing active pages.
+ *
+ * Approximating inactive page access frequency - Observations:
+ *
+ * 1. When a page is accessed for the first time, it is added to the
+ *    head of the inactive list, slides every existing inactive page
+ *    towards the tail by one slot, and pushes the current tail page
+ *    out of memory.
+ *
+ * 2. When a page is accessed for the second time, it is promoted to
+ *    the active list, shrinking the inactive list by one slot.  This
+ *    also slides all inactive pages that were faulted into the cache
+ *    more recently than the activated page towards the tail of the
+ *    inactive list.
+ *
+ * Thus:
+ *
+ * 1. The sum of evictions and activations between any two points in
+ *    time indicate the minimum number of inactive pages accessed in
+ *    between.
+ *
+ * 2. Moving one inactive page N page slots towards the tail of the
+ *    list requires at least N inactive page accesses.
+ *
+ * Combining these:
+ *
+ * 1. When a page is finally evicted from memory, the number of
+ *    inactive pages accessed while the page was in cache is at least
+ *    the number of page slots on the inactive list.
+ *
+ * 2. In addition, measuring the sum of evictions and activations (E)
+ *    at the time of a page's eviction, and comparing it to another
+ *    reading (R) at the time the page faults back into memory tells
+ *    the minimum number of accesses while the page was not cached.
+ *    This is called the refault distance.
+ *
+ * Because the first access of the page was the fault and the second
+ * access the refault, we combine the in-cache distance with the
+ * out-of-cache distance to get the complete minimum access distance
+ * of this page:
+ *
+ *      NR_inactive + (R - E)
+ *
+ * And knowing the minimum access distance of a page, we can easily
+ * tell if the page would be able to stay in cache assuming all page
+ * slots in the cache were available:
+ *
+ *   NR_inactive + (R - E) <= NR_inactive + NR_active
+ *
+ * which can be further simplified to
+ *
+ *   (R - E) <= NR_active
+ *
+ * Put into words, the refault distance (out-of-cache) can be seen as
+ * a deficit in inactive list space (in-cache).  If the inactive list
+ * had (R - E) more page slots, the page would not have been evicted
+ * in between accesses, but activated instead.  And on a full system,
+ * the only thing eating into inactive list space is active pages.
+ *
+ *
+ *		Activating refaulting pages
+ *
+ * All that is known about the active list is that the pages have been
+ * accessed more than once in the past.  This means that at any given
+ * time there is actually a good chance that pages on the active list
+ * are no longer in active use.
+ *
+ * So when a refault distance of (R - E) is observed and there are at
+ * least (R - E) active pages, the refaulting page is activated
+ * optimistically in the hope that (R - E) active pages are actually
+ * used less frequently than the refaulting page - or even not used at
+ * all anymore.
+ *
+ * If this is wrong and demotion kicks in, the pages which are truly
+ * used more frequently will be reactivated while the less frequently
+ * used once will be evicted from memory.
+ *
+ * But if this is right, the stale pages will be pushed out of memory
+ * and the used pages get to stay in cache.
+ *
+ *
+ *		Implementation
+ *
+ * For each zone's file LRU lists, a counter for inactive evictions
+ * and activations is maintained (zone->inactive_age).
+ *
+ * On eviction, a snapshot of this counter (along with some bits to
+ * identify the zone) is stored in the now empty page cache radix tree
+ * slot of the evicted page.  This is called a shadow entry.
+ *
+ * On cache misses for which there are shadow entries, an eligible
+ * refault distance will immediately activate the refaulting page.
+ */
+
+static void *pack_shadow(unsigned long eviction, struct zone *zone)
+{
+	eviction = (eviction << NODES_SHIFT) | zone_to_nid(zone);
+	eviction = (eviction << ZONES_SHIFT) | zone_idx(zone);
+	eviction = (eviction << RADIX_TREE_EXCEPTIONAL_SHIFT);
+
+	return (void *)(eviction | RADIX_TREE_EXCEPTIONAL_ENTRY);
+}
+
+static void unpack_shadow(void *shadow,
+			  struct zone **zone,
+			  unsigned long *distance)
+{
+	unsigned long entry = (unsigned long)shadow;
+	unsigned long eviction;
+	unsigned long refault;
+	unsigned long mask;
+	int zid, nid;
+
+	entry >>= RADIX_TREE_EXCEPTIONAL_SHIFT;
+	zid = entry & ((1UL << ZONES_SHIFT) - 1);
+	entry >>= ZONES_SHIFT;
+	nid = entry & ((1UL << NODES_SHIFT) - 1);
+	entry >>= NODES_SHIFT;
+	eviction = entry;
+
+	*zone = NODE_DATA(nid)->node_zones + zid;
+
+	refault = atomic_long_read(&(*zone)->inactive_age);
+	mask = ~0UL >> (NODES_SHIFT + ZONES_SHIFT +
+			RADIX_TREE_EXCEPTIONAL_SHIFT);
+	/*
+	 * The unsigned subtraction here gives an accurate distance
+	 * across inactive_age overflows in most cases.
+	 *
+	 * There is a special case: usually, shadow entries have a
+	 * short lifetime and are either refaulted or reclaimed along
+	 * with the inode before they get too old.  But it is not
+	 * impossible for the inactive_age to lap a shadow entry in
+	 * the field, which can then can result in a false small
+	 * refault distance, leading to a false activation should this
+	 * old entry actually refault again.  However, earlier kernels
+	 * used to deactivate unconditionally with *every* reclaim
+	 * invocation for the longest time, so the occasional
+	 * inappropriate activation leading to pressure on the active
+	 * list is not a problem.
+	 */
+	*distance = (refault - eviction) & mask;
+}
+
+/**
+ * workingset_eviction - note the eviction of a page from memory
+ * @mapping: address space the page was backing
+ * @page: the page being evicted
+ *
+ * Returns a shadow entry to be stored in @mapping->page_tree in place
+ * of the evicted @page so that a later refault can be detected.
+ */
+void *workingset_eviction(struct address_space *mapping, struct page *page)
+{
+	struct zone *zone = page_zone(page);
+	unsigned long eviction;
+
+	eviction = atomic_long_inc_return(&zone->inactive_age);
+	return pack_shadow(eviction, zone);
+}
+
+/**
+ * workingset_refault - evaluate the refault of a previously evicted page
+ * @shadow: shadow entry of the evicted page
+ *
+ * Calculates and evaluates the refault distance of the previously
+ * evicted page in the context of the zone it was allocated in.
+ *
+ * Returns %true if the page should be activated, %false otherwise.
+ */
+bool workingset_refault(void *shadow)
+{
+	unsigned long refault_distance;
+	struct zone *zone;
+
+	unpack_shadow(shadow, &zone, &refault_distance);
+	inc_zone_state(zone, WORKINGSET_REFAULT);
+
+	if (refault_distance <= zone_page_state(zone, NR_ACTIVE_FILE)) {
+		inc_zone_state(zone, WORKINGSET_ACTIVATE);
+		return true;
+	}
+	return false;
+}
+
+/**
+ * workingset_activation - note a page activation
+ * @page: page that is being activated
+ */
+void workingset_activation(struct page *page)
+{
+	atomic_long_inc(&page_zone(page)->inactive_age);
+}
