mm: numa: avoid unnecessary disruption of NUMA hinting during migration

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] numa: Avoid unnecessary disruption of NUMA hinting during migration (Rik van Riel) [1040200]
Rebuild_FUZZ: 97.10%
commit-author Mel Gorman <mgorman@suse.de>
commit de466bd628e8d663fdf3f791bc8db318ee85c714
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/de466bd6.failed

do_huge_pmd_numa_page() handles the case where there is parallel THP
migration.  However, by the time it is checked the NUMA hinting
information has already been disrupted.  This patch adds an earlier
check with some helpers.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Alex Thorlton <athorlton@sgi.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit de466bd628e8d663fdf3f791bc8db318ee85c714)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index 45ae57e74824,7de1bf85f683..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -899,10 -882,14 +899,14 @@@ int copy_huge_pmd(struct mm_struct *dst
  		ret = 0;
  		goto out_unlock;
  	}
+ 
+ 	/* mmap_sem prevents this happening but warn if that changes */
+ 	WARN_ON(pmd_trans_migrating(pmd));
+ 
  	if (unlikely(pmd_trans_splitting(pmd))) {
  		/* split huge page running from under us */
 -		spin_unlock(src_ptl);
 -		spin_unlock(dst_ptl);
 +		spin_unlock(&src_mm->page_table_lock);
 +		spin_unlock(&dst_mm->page_table_lock);
  		pte_free(dst_mm, pgtable);
  
  		wait_split_huge_page(vma->anon_vma, src_pmd); /* src_vma */
@@@ -1319,13 -1342,11 +1334,19 @@@ int do_huge_pmd_numa_page(struct mm_str
  		/* If the page was locked, there are no parallel migrations */
  		if (page_locked)
  			goto clear_pmdnuma;
 -	}
  
++<<<<<<< HEAD
 +		/*
 +		 * Otherwise wait for potential migrations and retry. We do
 +		 * relock and check_same as the page may no longer be mapped.
 +		 * As the fault is being retried, do not account for it.
 +		 */
 +		spin_unlock(&mm->page_table_lock);
++=======
+ 	/* Migration could have started since the pmd_trans_migrating check */
+ 	if (!page_locked) {
+ 		spin_unlock(ptl);
++>>>>>>> de466bd628e8 (mm: numa: avoid unnecessary disruption of NUMA hinting during migration)
  		wait_on_page_locked(page);
  		page_nid = -1;
  		goto out;
diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index e7e26af15a80..442e5c022f90 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -92,10 +92,19 @@ static inline int migrate_huge_page_move_mapping(struct address_space *mapping,
 #endif /* CONFIG_MIGRATION */
 
 #ifdef CONFIG_NUMA_BALANCING
+extern bool pmd_trans_migrating(pmd_t pmd);
+extern void wait_migrate_huge_page(struct anon_vma *anon_vma, pmd_t *pmd);
 extern int migrate_misplaced_page(struct page *page,
 				  struct vm_area_struct *vma, int node);
 extern bool migrate_ratelimited(int node);
 #else
+static inline bool pmd_trans_migrating(pmd_t pmd)
+{
+	return false;
+}
+static inline void wait_migrate_huge_page(struct anon_vma *anon_vma, pmd_t *pmd)
+{
+}
 static inline int migrate_misplaced_page(struct page *page,
 					 struct vm_area_struct *vma, int node)
 {
* Unmerged path mm/huge_memory.c
diff --git a/mm/migrate.c b/mm/migrate.c
index c034cc70e496..f51e095ba0f0 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1600,6 +1600,18 @@ int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)
 	return 1;
 }
 
+bool pmd_trans_migrating(pmd_t pmd)
+{
+	struct page *page = pmd_page(pmd);
+	return PageLocked(page);
+}
+
+void wait_migrate_huge_page(struct anon_vma *anon_vma, pmd_t *pmd)
+{
+	struct page *page = pmd_page(*pmd);
+	wait_on_page_locked(page);
+}
+
 /*
  * Attempt to migrate a misplaced page to the specified destination
  * node. Caller is expected to have an elevated reference count on
