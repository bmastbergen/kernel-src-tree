net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [net] rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL (Michal Schmidt) [1038631]
Rebuild_FUZZ: 95.65%
commit-author Cong Wang <amwang@redhat.com>
commit e0d1095ae3405404d247afb00233ef837d58da83
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/e0d1095a.failed

Eliezer renames several *ll_poll to *busy_poll, but forgets
CONFIG_NET_LL_RX_POLL, so in case of confusion, rename it too.

	Cc: Eliezer Tamir <eliezer.tamir@linux.intel.com>
	Cc: David S. Miller <davem@davemloft.net>
	Signed-off-by: Cong Wang <amwang@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e0d1095ae3405404d247afb00233ef837d58da83)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/sysctl/net.txt
#	drivers/net/ethernet/broadcom/bnx2x/bnx2x.h
#	drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
#	drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
#	drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
#	drivers/net/ethernet/mellanox/mlx4/en_netdev.c
#	drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
#	include/linux/netdevice.h
#	include/linux/skbuff.h
#	include/net/busy_poll.h
#	include/net/sock.h
#	net/Kconfig
#	net/core/skbuff.c
#	net/core/sock.c
#	net/core/sysctl_net_core.c
#	net/socket.c
diff --cc Documentation/sysctl/net.txt
index 98335b7a5337,d569f2a424d5..000000000000
--- a/Documentation/sysctl/net.txt
+++ b/Documentation/sysctl/net.txt
@@@ -50,6 -50,30 +50,33 @@@ The maximum number of packets that kern
  it's a Per-CPU variable.
  Default: 64
  
++<<<<<<< HEAD
++=======
+ busy_read
+ ----------------
+ Low latency busy poll timeout for socket reads. (needs CONFIG_NET_RX_BUSY_POLL)
+ Approximate time in us to busy loop waiting for packets on the device queue.
+ This sets the default value of the SO_BUSY_POLL socket option.
+ Can be set or overridden per socket by setting socket option SO_BUSY_POLL,
+ which is the preferred method of enabling. If you need to enable the feature
+ globally via sysctl, a value of 50 is recommended.
+ Will increase power usage.
+ Default: 0 (off)
+ 
+ busy_poll
+ ----------------
+ Low latency busy poll timeout for poll and select. (needs CONFIG_NET_RX_BUSY_POLL)
+ Approximate time in us to busy loop waiting for events.
+ Recommended value depends on the number of sockets you poll on.
+ For several sockets 50, for several hundreds 100.
+ For more than that you probably want to use epoll.
+ Note that only sockets with SO_BUSY_POLL set will be busy polled,
+ so you want to either selectively set SO_BUSY_POLL on those sockets or set
+ sysctl.net.busy_read globally.
+ Will increase power usage.
+ Default: 0 (off)
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  rmem_default
  ------------
  
diff --cc drivers/net/ethernet/broadcom/bnx2x/bnx2x.h
index c90495437e8c,d80e34b8285f..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x.h
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x.h
@@@ -485,6 -485,21 +485,24 @@@ struct bnx2x_fastpath 
  	struct bnx2x		*bp; /* parent */
  
  	struct napi_struct	napi;
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	unsigned int state;
+ #define BNX2X_FP_STATE_IDLE		      0
+ #define BNX2X_FP_STATE_NAPI		(1 << 0)    /* NAPI owns this FP */
+ #define BNX2X_FP_STATE_POLL		(1 << 1)    /* poll owns this FP */
+ #define BNX2X_FP_STATE_NAPI_YIELD	(1 << 2)    /* NAPI yielded this FP */
+ #define BNX2X_FP_STATE_POLL_YIELD	(1 << 3)    /* poll yielded this FP */
+ #define BNX2X_FP_YIELD	(BNX2X_FP_STATE_NAPI_YIELD | BNX2X_FP_STATE_POLL_YIELD)
+ #define BNX2X_FP_LOCKED	(BNX2X_FP_STATE_NAPI | BNX2X_FP_STATE_POLL)
+ #define BNX2X_FP_USER_PEND (BNX2X_FP_STATE_POLL | BNX2X_FP_STATE_POLL_YIELD)
+ 	/* protect state */
+ 	spinlock_t lock;
+ #endif /* CONFIG_NET_RX_BUSY_POLL */
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	union host_hc_status_block	status_blk;
  	/* chip independent shortcuts into sb structure */
  	__le16			*sb_index_values;
@@@ -557,6 -572,116 +575,119 @@@
  #define bnx2x_fp_stats(bp, fp)	(&((bp)->fp_stats[(fp)->index]))
  #define bnx2x_fp_qstats(bp, fp)	(&((bp)->fp_stats[(fp)->index].eth_q_stats))
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ static inline void bnx2x_fp_init_lock(struct bnx2x_fastpath *fp)
+ {
+ 	spin_lock_init(&fp->lock);
+ 	fp->state = BNX2X_FP_STATE_IDLE;
+ }
+ 
+ /* called from the device poll routine to get ownership of a FP */
+ static inline bool bnx2x_fp_lock_napi(struct bnx2x_fastpath *fp)
+ {
+ 	bool rc = true;
+ 
+ 	spin_lock(&fp->lock);
+ 	if (fp->state & BNX2X_FP_LOCKED) {
+ 		WARN_ON(fp->state & BNX2X_FP_STATE_NAPI);
+ 		fp->state |= BNX2X_FP_STATE_NAPI_YIELD;
+ 		rc = false;
+ 	} else {
+ 		/* we don't care if someone yielded */
+ 		fp->state = BNX2X_FP_STATE_NAPI;
+ 	}
+ 	spin_unlock(&fp->lock);
+ 	return rc;
+ }
+ 
+ /* returns true is someone tried to get the FP while napi had it */
+ static inline bool bnx2x_fp_unlock_napi(struct bnx2x_fastpath *fp)
+ {
+ 	bool rc = false;
+ 
+ 	spin_lock(&fp->lock);
+ 	WARN_ON(fp->state &
+ 		(BNX2X_FP_STATE_POLL | BNX2X_FP_STATE_NAPI_YIELD));
+ 
+ 	if (fp->state & BNX2X_FP_STATE_POLL_YIELD)
+ 		rc = true;
+ 	fp->state = BNX2X_FP_STATE_IDLE;
+ 	spin_unlock(&fp->lock);
+ 	return rc;
+ }
+ 
+ /* called from bnx2x_low_latency_poll() */
+ static inline bool bnx2x_fp_lock_poll(struct bnx2x_fastpath *fp)
+ {
+ 	bool rc = true;
+ 
+ 	spin_lock_bh(&fp->lock);
+ 	if ((fp->state & BNX2X_FP_LOCKED)) {
+ 		fp->state |= BNX2X_FP_STATE_POLL_YIELD;
+ 		rc = false;
+ 	} else {
+ 		/* preserve yield marks */
+ 		fp->state |= BNX2X_FP_STATE_POLL;
+ 	}
+ 	spin_unlock_bh(&fp->lock);
+ 	return rc;
+ }
+ 
+ /* returns true if someone tried to get the FP while it was locked */
+ static inline bool bnx2x_fp_unlock_poll(struct bnx2x_fastpath *fp)
+ {
+ 	bool rc = false;
+ 
+ 	spin_lock_bh(&fp->lock);
+ 	WARN_ON(fp->state & BNX2X_FP_STATE_NAPI);
+ 
+ 	if (fp->state & BNX2X_FP_STATE_POLL_YIELD)
+ 		rc = true;
+ 	fp->state = BNX2X_FP_STATE_IDLE;
+ 	spin_unlock_bh(&fp->lock);
+ 	return rc;
+ }
+ 
+ /* true if a socket is polling, even if it did not get the lock */
+ static inline bool bnx2x_fp_ll_polling(struct bnx2x_fastpath *fp)
+ {
+ 	WARN_ON(!(fp->state & BNX2X_FP_LOCKED));
+ 	return fp->state & BNX2X_FP_USER_PEND;
+ }
+ #else
+ static inline void bnx2x_fp_init_lock(struct bnx2x_fastpath *fp)
+ {
+ }
+ 
+ static inline bool bnx2x_fp_lock_napi(struct bnx2x_fastpath *fp)
+ {
+ 	return true;
+ }
+ 
+ static inline bool bnx2x_fp_unlock_napi(struct bnx2x_fastpath *fp)
+ {
+ 	return false;
+ }
+ 
+ static inline bool bnx2x_fp_lock_poll(struct bnx2x_fastpath *fp)
+ {
+ 	return false;
+ }
+ 
+ static inline bool bnx2x_fp_unlock_poll(struct bnx2x_fastpath *fp)
+ {
+ 	return false;
+ }
+ 
+ static inline bool bnx2x_fp_ll_polling(struct bnx2x_fastpath *fp)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_NET_RX_BUSY_POLL */
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  /* Use 2500 as a mini-jumbo MTU for FCoE */
  #define BNX2X_FCOE_MINI_JUMBO_MTU	2500
  
diff --cc drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
index 97b889e636ed,f2d1ff10054b..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
@@@ -3116,6 -3117,32 +3116,35 @@@ int bnx2x_poll(struct napi_struct *napi
  	return work_done;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ /* must be called with local_bh_disable()d */
+ int bnx2x_low_latency_recv(struct napi_struct *napi)
+ {
+ 	struct bnx2x_fastpath *fp = container_of(napi, struct bnx2x_fastpath,
+ 						 napi);
+ 	struct bnx2x *bp = fp->bp;
+ 	int found = 0;
+ 
+ 	if ((bp->state == BNX2X_STATE_CLOSED) ||
+ 	    (bp->state == BNX2X_STATE_ERROR) ||
+ 	    (bp->flags & (TPA_ENABLE_FLAG | GRO_ENABLE_FLAG)))
+ 		return LL_FLUSH_FAILED;
+ 
+ 	if (!bnx2x_fp_lock_poll(fp))
+ 		return LL_FLUSH_BUSY;
+ 
+ 	if (bnx2x_has_rx_work(fp))
+ 		found = bnx2x_rx_int(fp, 4);
+ 
+ 	bnx2x_fp_unlock_poll(fp);
+ 
+ 	return found;
+ }
+ #endif
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  /* we split the first BD into headers and data BDs
   * to ease the pain of our fellow microcode engineers
   * we use one mapping for both BDs
diff --cc drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
index 8ee0cd7dde10,e06186c305d8..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
@@@ -12108,6 -12025,10 +12108,13 @@@ static const struct net_device_ops bnx2
  #ifdef NETDEV_FCOE_WWNN
  	.ndo_fcoe_get_wwn	= bnx2x_fcoe_get_wwn,
  #endif
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	.ndo_busy_poll		= bnx2x_low_latency_recv,
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  };
  
  static int bnx2x_set_coherency_mask(struct bnx2x *bp)
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index bf9431d8c87b,be4b1fb3d0d2..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -7317,8 -7227,8 +7317,13 @@@ static const struct net_device_ops ixgb
  #ifdef CONFIG_NET_POLL_CONTROLLER
  	.ndo_poll_controller	= ixgbe_netpoll,
  #endif
++<<<<<<< HEAD
 +#ifdef CONFIG_NET_LL_RX_POLL
 +	.ndo_ll_poll		= ixgbe_low_latency_recv,
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	.ndo_busy_poll		= ixgbe_low_latency_recv,
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  #endif
  #ifdef IXGBE_FCOE
  	.ndo_fcoe_ddp_setup = ixgbe_fcoe_ddp_get,
diff --cc drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
index c9e6b62dd000,a28cd801a236..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
@@@ -222,7 -222,12 +222,16 @@@ static int mlx4_en_get_sset_count(struc
  	switch (sset) {
  	case ETH_SS_STATS:
  		return (priv->stats_bitmap ? bit_count : NUM_ALL_STATS) +
++<<<<<<< HEAD
 +			(priv->tx_ring_num + priv->rx_ring_num) * 2;
++=======
+ 			(priv->tx_ring_num * 2) +
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 			(priv->rx_ring_num * 5);
+ #else
+ 			(priv->rx_ring_num * 2);
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	case ETH_SS_TEST:
  		return MLX4_EN_NUM_SELF_TEST - !(priv->mdev->dev->caps.flags
  					& MLX4_DEV_CAP_FLAG_UC_LOOPBACK) * 2;
@@@ -271,6 -276,11 +280,14 @@@ static void mlx4_en_get_ethtool_stats(s
  	for (i = 0; i < priv->rx_ring_num; i++) {
  		data[index++] = priv->rx_ring[i].packets;
  		data[index++] = priv->rx_ring[i].bytes;
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 		data[index++] = priv->rx_ring[i].yields;
+ 		data[index++] = priv->rx_ring[i].misses;
+ 		data[index++] = priv->rx_ring[i].cleaned;
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	}
  	spin_unlock_bh(&priv->stats_lock);
  
@@@ -334,6 -344,14 +351,17 @@@ static void mlx4_en_get_strings(struct 
  				"rx%d_packets", i);
  			sprintf(data + (index++) * ETH_GSTRING_LEN,
  				"rx%d_bytes", i);
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 			sprintf(data + (index++) * ETH_GSTRING_LEN,
+ 				"rx%d_napi_yield", i);
+ 			sprintf(data + (index++) * ETH_GSTRING_LEN,
+ 				"rx%d_misses", i);
+ 			sprintf(data + (index++) * ETH_GSTRING_LEN,
+ 				"rx%d_cleaned", i);
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  		}
  		break;
  	}
diff --cc drivers/net/ethernet/mellanox/mlx4/en_netdev.c
index dd7fede06f39,fa37b7a61213..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_netdev.c
@@@ -67,6 -68,34 +67,37 @@@ int mlx4_en_setup_tc(struct net_device 
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ /* must be called with local_bh_disable()d */
+ static int mlx4_en_low_latency_recv(struct napi_struct *napi)
+ {
+ 	struct mlx4_en_cq *cq = container_of(napi, struct mlx4_en_cq, napi);
+ 	struct net_device *dev = cq->dev;
+ 	struct mlx4_en_priv *priv = netdev_priv(dev);
+ 	struct mlx4_en_rx_ring *rx_ring = &priv->rx_ring[cq->ring];
+ 	int done;
+ 
+ 	if (!priv->port_up)
+ 		return LL_FLUSH_FAILED;
+ 
+ 	if (!mlx4_en_cq_lock_poll(cq))
+ 		return LL_FLUSH_BUSY;
+ 
+ 	done = mlx4_en_process_rx_cq(dev, cq, 4);
+ 	if (likely(done))
+ 		rx_ring->cleaned += done;
+ 	else
+ 		rx_ring->misses++;
+ 
+ 	mlx4_en_cq_unlock_poll(cq);
+ 
+ 	return done;
+ }
+ #endif	/* CONFIG_NET_RX_BUSY_POLL */
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  #ifdef CONFIG_RFS_ACCEL
  
  struct mlx4_en_filter {
@@@ -2100,6 -2140,9 +2131,12 @@@ static const struct net_device_ops mlx4
  #ifdef CONFIG_RFS_ACCEL
  	.ndo_rx_flow_steer	= mlx4_en_filter_rfs,
  #endif
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	.ndo_busy_poll		= mlx4_en_low_latency_recv,
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  };
  
  static const struct net_device_ops mlx4_netdev_ops_master = {
diff --cc drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
index 43887177fc9a,5e0aa569306a..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
@@@ -292,6 -292,11 +292,14 @@@ struct mlx4_en_rx_ring 
  	void *rx_info;
  	unsigned long bytes;
  	unsigned long packets;
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	unsigned long yields;
+ 	unsigned long misses;
+ 	unsigned long cleaned;
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	unsigned long csum_ok;
  	unsigned long csum_none;
  	int hwtstamp_rx_filter;
@@@ -312,6 -317,19 +320,22 @@@ struct mlx4_en_cq 
  	u16 moder_cnt;
  	struct mlx4_cqe *buf;
  #define MLX4_EN_OPCODE_ERROR	0x1e
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	unsigned int state;
+ #define MLX4_EN_CQ_STATE_IDLE        0
+ #define MLX4_EN_CQ_STATE_NAPI     1    /* NAPI owns this CQ */
+ #define MLX4_EN_CQ_STATE_POLL     2    /* poll owns this CQ */
+ #define MLX4_CQ_LOCKED (MLX4_EN_CQ_STATE_NAPI | MLX4_EN_CQ_STATE_POLL)
+ #define MLX4_EN_CQ_STATE_NAPI_YIELD  4    /* NAPI yielded this CQ */
+ #define MLX4_EN_CQ_STATE_POLL_YIELD  8    /* poll yielded this CQ */
+ #define CQ_YIELD (MLX4_EN_CQ_STATE_NAPI_YIELD | MLX4_EN_CQ_STATE_POLL_YIELD)
+ #define CQ_USER_PEND (MLX4_EN_CQ_STATE_POLL | MLX4_EN_CQ_STATE_POLL_YIELD)
+ 	spinlock_t poll_lock; /* protects from LLS/napi conflicts */
+ #endif  /* CONFIG_NET_RX_BUSY_POLL */
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  };
  
  struct mlx4_en_port_profile {
@@@ -562,6 -580,115 +586,118 @@@ struct mlx4_mac_entry 
  	struct rcu_head rcu;
  };
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ static inline void mlx4_en_cq_init_lock(struct mlx4_en_cq *cq)
+ {
+ 	spin_lock_init(&cq->poll_lock);
+ 	cq->state = MLX4_EN_CQ_STATE_IDLE;
+ }
+ 
+ /* called from the device poll rutine to get ownership of a cq */
+ static inline bool mlx4_en_cq_lock_napi(struct mlx4_en_cq *cq)
+ {
+ 	int rc = true;
+ 	spin_lock(&cq->poll_lock);
+ 	if (cq->state & MLX4_CQ_LOCKED) {
+ 		WARN_ON(cq->state & MLX4_EN_CQ_STATE_NAPI);
+ 		cq->state |= MLX4_EN_CQ_STATE_NAPI_YIELD;
+ 		rc = false;
+ 	} else
+ 		/* we don't care if someone yielded */
+ 		cq->state = MLX4_EN_CQ_STATE_NAPI;
+ 	spin_unlock(&cq->poll_lock);
+ 	return rc;
+ }
+ 
+ /* returns true is someone tried to get the cq while napi had it */
+ static inline bool mlx4_en_cq_unlock_napi(struct mlx4_en_cq *cq)
+ {
+ 	int rc = false;
+ 	spin_lock(&cq->poll_lock);
+ 	WARN_ON(cq->state & (MLX4_EN_CQ_STATE_POLL |
+ 			       MLX4_EN_CQ_STATE_NAPI_YIELD));
+ 
+ 	if (cq->state & MLX4_EN_CQ_STATE_POLL_YIELD)
+ 		rc = true;
+ 	cq->state = MLX4_EN_CQ_STATE_IDLE;
+ 	spin_unlock(&cq->poll_lock);
+ 	return rc;
+ }
+ 
+ /* called from mlx4_en_low_latency_poll() */
+ static inline bool mlx4_en_cq_lock_poll(struct mlx4_en_cq *cq)
+ {
+ 	int rc = true;
+ 	spin_lock_bh(&cq->poll_lock);
+ 	if ((cq->state & MLX4_CQ_LOCKED)) {
+ 		struct net_device *dev = cq->dev;
+ 		struct mlx4_en_priv *priv = netdev_priv(dev);
+ 		struct mlx4_en_rx_ring *rx_ring = &priv->rx_ring[cq->ring];
+ 
+ 		cq->state |= MLX4_EN_CQ_STATE_POLL_YIELD;
+ 		rc = false;
+ 		rx_ring->yields++;
+ 	} else
+ 		/* preserve yield marks */
+ 		cq->state |= MLX4_EN_CQ_STATE_POLL;
+ 	spin_unlock_bh(&cq->poll_lock);
+ 	return rc;
+ }
+ 
+ /* returns true if someone tried to get the cq while it was locked */
+ static inline bool mlx4_en_cq_unlock_poll(struct mlx4_en_cq *cq)
+ {
+ 	int rc = false;
+ 	spin_lock_bh(&cq->poll_lock);
+ 	WARN_ON(cq->state & (MLX4_EN_CQ_STATE_NAPI));
+ 
+ 	if (cq->state & MLX4_EN_CQ_STATE_POLL_YIELD)
+ 		rc = true;
+ 	cq->state = MLX4_EN_CQ_STATE_IDLE;
+ 	spin_unlock_bh(&cq->poll_lock);
+ 	return rc;
+ }
+ 
+ /* true if a socket is polling, even if it did not get the lock */
+ static inline bool mlx4_en_cq_ll_polling(struct mlx4_en_cq *cq)
+ {
+ 	WARN_ON(!(cq->state & MLX4_CQ_LOCKED));
+ 	return cq->state & CQ_USER_PEND;
+ }
+ #else
+ static inline void mlx4_en_cq_init_lock(struct mlx4_en_cq *cq)
+ {
+ }
+ 
+ static inline bool mlx4_en_cq_lock_napi(struct mlx4_en_cq *cq)
+ {
+ 	return true;
+ }
+ 
+ static inline bool mlx4_en_cq_unlock_napi(struct mlx4_en_cq *cq)
+ {
+ 	return false;
+ }
+ 
+ static inline bool mlx4_en_cq_lock_poll(struct mlx4_en_cq *cq)
+ {
+ 	return false;
+ }
+ 
+ static inline bool mlx4_en_cq_unlock_poll(struct mlx4_en_cq *cq)
+ {
+ 	return false;
+ }
+ 
+ static inline bool mlx4_en_cq_ll_polling(struct mlx4_en_cq *cq)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_NET_RX_BUSY_POLL */
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  #define MLX4_EN_WOL_DO_MODIFY (1ULL << 63)
  
  void mlx4_en_update_loopback_state(struct net_device *dev,
diff --cc include/linux/netdevice.h
index 1b74e1693ca3,9a4156845e93..000000000000
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@@ -989,6 -973,9 +989,12 @@@ struct net_device_ops 
  						     gfp_t gfp);
  	void			(*ndo_netpoll_cleanup)(struct net_device *dev);
  #endif
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	int			(*ndo_busy_poll)(struct napi_struct *dev);
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	int			(*ndo_set_vf_mac)(struct net_device *dev,
  						  int queue, u8 *mac);
  	int			(*ndo_set_vf_vlan)(struct net_device *dev,
diff --cc include/linux/skbuff.h
index 1783a7488172,3b71a4e83642..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -500,8 -501,11 +500,16 @@@ struct sk_buff 
  	/* 7/9 bit hole (depending on ndisc_nodetype presence) */
  	kmemcheck_bitfield_end(flags2);
  
++<<<<<<< HEAD
 +#ifdef CONFIG_NET_DMA
 +	dma_cookie_t		dma_cookie;
++=======
+ #if defined CONFIG_NET_DMA || defined CONFIG_NET_RX_BUSY_POLL
+ 	union {
+ 		unsigned int	napi_id;
+ 		dma_cookie_t	dma_cookie;
+ 	};
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  #endif
  #ifdef CONFIG_NETWORK_SECMARK
  	__u32			secmark;
diff --cc include/net/sock.h
index 66772cf8c3c5,31d5cfbb51ec..000000000000
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@@ -325,6 -327,10 +325,13 @@@ struct sock 
  #ifdef CONFIG_RPS
  	__u32			sk_rxhash;
  #endif
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	unsigned int		sk_napi_id;
+ 	unsigned int		sk_ll_usec;
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	atomic_t		sk_drops;
  	int			sk_rcvbuf;
  
diff --cc net/Kconfig
index d32a7fe2758c,2b406608a1a4..000000000000
--- a/net/Kconfig
+++ b/net/Kconfig
@@@ -243,6 -244,10 +243,13 @@@ config NETPRIO_CGROU
  	  Cgroup subsystem for use in assigning processes to network priorities on
  	  a per-interface basis
  
++<<<<<<< HEAD
++=======
+ config NET_RX_BUSY_POLL
+ 	boolean
+ 	default y
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  config BQL
  	boolean
  	depends on SYSFS
diff --cc net/core/skbuff.c
index 1aacd2cef5db,2c3d0f53d198..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -738,6 -739,10 +738,13 @@@ static void __copy_skb_header(struct sk
  	new->vlan_tci		= old->vlan_tci;
  
  	skb_copy_secmark(new, old);
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	new->napi_id	= old->napi_id;
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  }
  
  /*
diff --cc net/core/sock.c
index d6d024cfaaaf,2c097c5a35dd..000000000000
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@@ -898,6 -900,19 +898,22 @@@ set_rcvbuf
  		sock_valbool_flag(sk, SOCK_SELECT_ERR_QUEUE, valbool);
  		break;
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	case SO_BUSY_POLL:
+ 		/* allow unprivileged users to decrease the value */
+ 		if ((val > sk->sk_ll_usec) && !capable(CAP_NET_ADMIN))
+ 			ret = -EPERM;
+ 		else {
+ 			if (val < 0)
+ 				ret = -EINVAL;
+ 			else
+ 				sk->sk_ll_usec = val;
+ 		}
+ 		break;
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	default:
  		ret = -ENOPROTOOPT;
  		break;
@@@ -1155,6 -1170,12 +1171,15 @@@ int sock_getsockopt(struct socket *sock
  		v.val = sock_flag(sk, SOCK_SELECT_ERR_QUEUE);
  		break;
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	case SO_BUSY_POLL:
+ 		v.val = sk->sk_ll_usec;
+ 		break;
+ #endif
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	default:
  		return -ENOPROTOOPT;
  	}
@@@ -2271,6 -2292,11 +2296,14 @@@ void sock_init_data(struct socket *sock
  
  	sk->sk_stamp = ktime_set(-1L, 0);
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	sk->sk_napi_id		=	0;
+ 	sk->sk_ll_usec		=	sysctl_net_busy_read;
+ #endif
+ 
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  	/*
  	 * Before updating sk_refcnt, we must commit prior changes to memory
  	 * (Documentation/RCU/rculist_nulls.txt for details)
diff --cc net/core/sysctl_net_core.c
index cfdb46ab3a7f,b59b6804fd98..000000000000
--- a/net/core/sysctl_net_core.c
+++ b/net/core/sysctl_net_core.c
@@@ -180,6 -284,37 +180,40 @@@ static struct ctl_table net_core_table[
  		.proc_handler	= rps_sock_flow_sysctl
  	},
  #endif
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NET_FLOW_LIMIT
+ 	{
+ 		.procname	= "flow_limit_cpu_bitmap",
+ 		.mode		= 0644,
+ 		.proc_handler	= flow_limit_cpu_sysctl
+ 	},
+ 	{
+ 		.procname	= "flow_limit_table_len",
+ 		.data		= &netdev_flow_limit_table_len,
+ 		.maxlen		= sizeof(int),
+ 		.mode		= 0644,
+ 		.proc_handler	= flow_limit_table_len_sysctl
+ 	},
+ #endif /* CONFIG_NET_FLOW_LIMIT */
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 	{
+ 		.procname	= "busy_poll",
+ 		.data		= &sysctl_net_busy_poll,
+ 		.maxlen		= sizeof(unsigned int),
+ 		.mode		= 0644,
+ 		.proc_handler	= proc_dointvec
+ 	},
+ 	{
+ 		.procname	= "busy_read",
+ 		.data		= &sysctl_net_busy_read,
+ 		.maxlen		= sizeof(unsigned int),
+ 		.mode		= 0644,
+ 		.proc_handler	= proc_dointvec
+ 	},
+ #
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  #endif /* CONFIG_NET */
  	{
  		.procname	= "netdev_budget",
diff --cc net/socket.c
index 4ca1526db756,b2d7c629eeb9..000000000000
--- a/net/socket.c
+++ b/net/socket.c
@@@ -104,6 -104,12 +104,15 @@@
  #include <linux/route.h>
  #include <linux/sockios.h>
  #include <linux/atalk.h>
++<<<<<<< HEAD
++=======
+ #include <net/busy_poll.h>
+ 
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ unsigned int sysctl_net_busy_read __read_mostly;
+ unsigned int sysctl_net_busy_poll __read_mostly;
+ #endif
++>>>>>>> e0d1095ae340 (net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL)
  
  static int sock_no_open(struct inode *irrelevant, struct file *dontcare);
  static ssize_t sock_aio_read(struct kiocb *iocb, const struct iovec *iov,
* Unmerged path include/net/busy_poll.h
* Unmerged path Documentation/sysctl/net.txt
* Unmerged path drivers/net/ethernet/broadcom/bnx2x/bnx2x.h
* Unmerged path drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
* Unmerged path drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe.h b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
index ec6aab3a8021..37ca4409559a 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe.h
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
@@ -54,7 +54,7 @@
 
 #include <net/ll_poll.h>
 
-#ifdef CONFIG_NET_LL_RX_POLL
+#ifdef CONFIG_NET_RX_BUSY_POLL
 #define LL_EXTENDED_STATS
 #endif
 /* common prefix used by pr_<> macros */
@@ -366,7 +366,7 @@ struct ixgbe_q_vector {
 	struct rcu_head rcu;	/* to avoid race with update stats on free */
 	char name[IFNAMSIZ + 9];
 
-#ifdef CONFIG_NET_LL_RX_POLL
+#ifdef CONFIG_NET_RX_BUSY_POLL
 	unsigned int state;
 #define IXGBE_QV_STATE_IDLE        0
 #define IXGBE_QV_STATE_NAPI	   1    /* NAPI owns this QV */
@@ -377,12 +377,12 @@ struct ixgbe_q_vector {
 #define IXGBE_QV_YIELD (IXGBE_QV_STATE_NAPI_YIELD | IXGBE_QV_STATE_POLL_YIELD)
 #define IXGBE_QV_USER_PEND (IXGBE_QV_STATE_POLL | IXGBE_QV_STATE_POLL_YIELD)
 	spinlock_t lock;
-#endif  /* CONFIG_NET_LL_RX_POLL */
+#endif  /* CONFIG_NET_RX_BUSY_POLL */
 
 	/* for dynamic allocation of rings associated with this q_vector */
 	struct ixgbe_ring ring[0] ____cacheline_internodealigned_in_smp;
 };
-#ifdef CONFIG_NET_LL_RX_POLL
+#ifdef CONFIG_NET_RX_BUSY_POLL
 static inline void ixgbe_qv_init_lock(struct ixgbe_q_vector *q_vector)
 {
 
@@ -462,7 +462,7 @@ static inline bool ixgbe_qv_ll_polling(struct ixgbe_q_vector *q_vector)
 	WARN_ON(!(q_vector->state & IXGBE_QV_LOCKED));
 	return q_vector->state & IXGBE_QV_USER_PEND;
 }
-#else /* CONFIG_NET_LL_RX_POLL */
+#else /* CONFIG_NET_RX_BUSY_POLL */
 static inline void ixgbe_qv_init_lock(struct ixgbe_q_vector *q_vector)
 {
 }
@@ -491,7 +491,7 @@ static inline bool ixgbe_qv_ll_polling(struct ixgbe_q_vector *q_vector)
 {
 	return false;
 }
-#endif /* CONFIG_NET_LL_RX_POLL */
+#endif /* CONFIG_NET_RX_BUSY_POLL */
 
 #ifdef CONFIG_IXGBE_HWMON
 
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_ethtool.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_netdev.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
* Unmerged path include/linux/netdevice.h
* Unmerged path include/linux/skbuff.h
* Unmerged path include/net/busy_poll.h
* Unmerged path include/net/sock.h
* Unmerged path net/Kconfig
* Unmerged path net/core/skbuff.c
* Unmerged path net/core/sock.c
* Unmerged path net/core/sysctl_net_core.c
* Unmerged path net/socket.c
