sched/numa: Resist moving tasks towards nodes with fewer hinting faults

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Mel Gorman <mgorman@suse.de>
commit 7a0f308337d11fd5caa9f845c6d08cc5d6067988
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/7a0f3083.failed

Just as "sched: Favour moving tasks towards the preferred node" favours
moving tasks towards nodes with a higher number of recorded NUMA hinting
faults, this patch resists moving tasks towards nodes with lower faults.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-24-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 7a0f308337d11fd5caa9f845c6d08cc5d6067988)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
#	kernel/sched/features.h
diff --cc kernel/sched/fair.c
index b231594d7817,89431248d33d..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -4048,6 -4083,69 +4048,72 @@@ task_hot(struct task_struct *p, u64 now
  	return delta < (s64)sysctl_sched_migration_cost;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NUMA_BALANCING
+ /* Returns true if the destination node has incurred more faults */
+ static bool migrate_improves_locality(struct task_struct *p, struct lb_env *env)
+ {
+ 	int src_nid, dst_nid;
+ 
+ 	if (!sched_feat(NUMA_FAVOUR_HIGHER) || !p->numa_faults ||
+ 	    !(env->sd->flags & SD_NUMA)) {
+ 		return false;
+ 	}
+ 
+ 	src_nid = cpu_to_node(env->src_cpu);
+ 	dst_nid = cpu_to_node(env->dst_cpu);
+ 
+ 	if (src_nid == dst_nid ||
+ 	    p->numa_migrate_seq >= sysctl_numa_balancing_settle_count)
+ 		return false;
+ 
+ 	if (dst_nid == p->numa_preferred_nid ||
+ 	    p->numa_faults[dst_nid] > p->numa_faults[src_nid])
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ 
+ static bool migrate_degrades_locality(struct task_struct *p, struct lb_env *env)
+ {
+ 	int src_nid, dst_nid;
+ 
+ 	if (!sched_feat(NUMA) || !sched_feat(NUMA_RESIST_LOWER))
+ 		return false;
+ 
+ 	if (!p->numa_faults || !(env->sd->flags & SD_NUMA))
+ 		return false;
+ 
+ 	src_nid = cpu_to_node(env->src_cpu);
+ 	dst_nid = cpu_to_node(env->dst_cpu);
+ 
+ 	if (src_nid == dst_nid ||
+ 	    p->numa_migrate_seq >= sysctl_numa_balancing_settle_count)
+ 		return false;
+ 
+ 	if (p->numa_faults[dst_nid] < p->numa_faults[src_nid])
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ #else
+ static inline bool migrate_improves_locality(struct task_struct *p,
+ 					     struct lb_env *env)
+ {
+ 	return false;
+ }
+ 
+ static inline bool migrate_degrades_locality(struct task_struct *p,
+ 					     struct lb_env *env)
+ {
+ 	return false;
+ }
+ #endif
+ 
++>>>>>>> 7a0f308337d1 (sched/numa: Resist moving tasks towards nodes with fewer hinting faults)
  /*
   * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
   */
@@@ -4103,11 -4203,24 +4169,27 @@@ int can_migrate_task(struct task_struc
  
  	/*
  	 * Aggressive migration if:
 -	 * 1) destination numa is preferred
 -	 * 2) task is cache cold, or
 -	 * 3) too many balance attempts have failed.
 +	 * 1) task is cache cold, or
 +	 * 2) too many balance attempts have failed.
  	 */
 +
  	tsk_cache_hot = task_hot(p, rq_clock_task(env->src_rq), env->sd);
++<<<<<<< HEAD
++=======
+ 	if (!tsk_cache_hot)
+ 		tsk_cache_hot = migrate_degrades_locality(p, env);
+ 
+ 	if (migrate_improves_locality(p, env)) {
+ #ifdef CONFIG_SCHEDSTATS
+ 		if (tsk_cache_hot) {
+ 			schedstat_inc(env->sd, lb_hot_gained[env->idle]);
+ 			schedstat_inc(p, se.statistics.nr_forced_migrations);
+ 		}
+ #endif
+ 		return 1;
+ 	}
+ 
++>>>>>>> 7a0f308337d1 (sched/numa: Resist moving tasks towards nodes with fewer hinting faults)
  	if (!tsk_cache_hot ||
  		env->sd->nr_balance_failed > env->sd->cache_nice_tries) {
  
diff --cc kernel/sched/features.h
index 99399f8e4799,5716929a2e3a..000000000000
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@@ -68,5 -67,19 +68,23 @@@ SCHED_FEAT(LB_MIN, false
   */
  #ifdef CONFIG_NUMA_BALANCING
  SCHED_FEAT(NUMA,	false)
++<<<<<<< HEAD
 +SCHED_FEAT(NUMA_FORCE,	false)
++=======
+ 
+ /*
+  * NUMA_FAVOUR_HIGHER will favor moving tasks towards nodes where a
+  * higher number of hinting faults are recorded during active load
+  * balancing.
+  */
+ SCHED_FEAT(NUMA_FAVOUR_HIGHER, true)
+ 
+ /*
+  * NUMA_RESIST_LOWER will resist moving tasks towards nodes where a
+  * lower number of hinting faults have been recorded. As this has
+  * the potential to prevent a task ever migrating to a new node
+  * due to CPU overload it is disabled by default.
+  */
+ SCHED_FEAT(NUMA_RESIST_LOWER, false)
++>>>>>>> 7a0f308337d1 (sched/numa: Resist moving tasks towards nodes with fewer hinting faults)
  #endif
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/features.h
