sched: add tracepoints related to NUMA task migration

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [sched] Add tracepoints related to NUMA task migration (Rik van Riel) [1040200]
Rebuild_FUZZ: 92.93%
commit-author Mel Gorman <mgorman@suse.de>
commit 286549dcaf4f128cb04f0ad56dfb677d7d19b500
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/286549dc.failed

This patch adds three tracepoints
 o trace_sched_move_numa	when a task is moved to a node
 o trace_sched_swap_numa	when a task is swapped with another task
 o trace_sched_stick_numa	when a numa-related migration fails

The tracepoints allow the NUMA scheduler activity to be monitored and the
following high-level metrics can be calculated

 o NUMA migrated stuck	 nr trace_sched_stick_numa
 o NUMA migrated idle	 nr trace_sched_move_numa
 o NUMA migrated swapped nr trace_sched_swap_numa
 o NUMA local swapped	 trace_sched_swap_numa src_nid == dst_nid (should never happen)
 o NUMA remote swapped	 trace_sched_swap_numa src_nid != dst_nid (should == NUMA migrated swapped)
 o NUMA group swapped	 trace_sched_swap_numa src_ngid == dst_ngid
			 Maybe a small number of these are acceptable
			 but a high number would be a major surprise.
			 It would be even worse if bounces are frequent.
 o NUMA avg task migs.	 Average number of migrations for tasks
 o NUMA stddev task mig	 Self-explanatory
 o NUMA max task migs.	 Maximum number of migrations for a single task

In general the intent of the tracepoints is to help diagnose problems
where automatic NUMA balancing appears to be doing an excessive amount
of useless work.

[akpm@linux-foundation.org: remove semicolon-after-if, repair coding-style]
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Alex Thorlton <athorlton@sgi.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 286549dcaf4f128cb04f0ad56dfb677d7d19b500)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/trace/events/sched.h
#	kernel/sched/core.c
#	kernel/sched/fair.c
diff --cc include/trace/events/sched.h
index e5586caff67a,67e1bbf83695..000000000000
--- a/include/trace/events/sched.h
+++ b/include/trace/events/sched.h
@@@ -430,6 -424,112 +430,115 @@@ TRACE_EVENT(sched_pi_setprio
  			__entry->oldprio, __entry->newprio)
  );
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_DETECT_HUNG_TASK
+ TRACE_EVENT(sched_process_hang,
+ 	TP_PROTO(struct task_struct *tsk),
+ 	TP_ARGS(tsk),
+ 
+ 	TP_STRUCT__entry(
+ 		__array( char,	comm,	TASK_COMM_LEN	)
+ 		__field( pid_t,	pid			)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		memcpy(__entry->comm, tsk->comm, TASK_COMM_LEN);
+ 		__entry->pid = tsk->pid;
+ 	),
+ 
+ 	TP_printk("comm=%s pid=%d", __entry->comm, __entry->pid)
+ );
+ #endif /* CONFIG_DETECT_HUNG_TASK */
+ 
+ DECLARE_EVENT_CLASS(sched_move_task_template,
+ 
+ 	TP_PROTO(struct task_struct *tsk, int src_cpu, int dst_cpu),
+ 
+ 	TP_ARGS(tsk, src_cpu, dst_cpu),
+ 
+ 	TP_STRUCT__entry(
+ 		__field( pid_t,	pid			)
+ 		__field( pid_t,	tgid			)
+ 		__field( pid_t,	ngid			)
+ 		__field( int,	src_cpu			)
+ 		__field( int,	src_nid			)
+ 		__field( int,	dst_cpu			)
+ 		__field( int,	dst_nid			)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__entry->pid		= task_pid_nr(tsk);
+ 		__entry->tgid		= task_tgid_nr(tsk);
+ 		__entry->ngid		= task_numa_group_id(tsk);
+ 		__entry->src_cpu	= src_cpu;
+ 		__entry->src_nid	= cpu_to_node(src_cpu);
+ 		__entry->dst_cpu	= dst_cpu;
+ 		__entry->dst_nid	= cpu_to_node(dst_cpu);
+ 	),
+ 
+ 	TP_printk("pid=%d tgid=%d ngid=%d src_cpu=%d src_nid=%d dst_cpu=%d dst_nid=%d",
+ 			__entry->pid, __entry->tgid, __entry->ngid,
+ 			__entry->src_cpu, __entry->src_nid,
+ 			__entry->dst_cpu, __entry->dst_nid)
+ );
+ 
+ /*
+  * Tracks migration of tasks from one runqueue to another. Can be used to
+  * detect if automatic NUMA balancing is bouncing between nodes
+  */
+ DEFINE_EVENT(sched_move_task_template, sched_move_numa,
+ 	TP_PROTO(struct task_struct *tsk, int src_cpu, int dst_cpu),
+ 
+ 	TP_ARGS(tsk, src_cpu, dst_cpu)
+ );
+ 
+ DEFINE_EVENT(sched_move_task_template, sched_stick_numa,
+ 	TP_PROTO(struct task_struct *tsk, int src_cpu, int dst_cpu),
+ 
+ 	TP_ARGS(tsk, src_cpu, dst_cpu)
+ );
+ 
+ TRACE_EVENT(sched_swap_numa,
+ 
+ 	TP_PROTO(struct task_struct *src_tsk, int src_cpu,
+ 		 struct task_struct *dst_tsk, int dst_cpu),
+ 
+ 	TP_ARGS(src_tsk, src_cpu, dst_tsk, dst_cpu),
+ 
+ 	TP_STRUCT__entry(
+ 		__field( pid_t,	src_pid			)
+ 		__field( pid_t,	src_tgid		)
+ 		__field( pid_t,	src_ngid		)
+ 		__field( int,	src_cpu			)
+ 		__field( int,	src_nid			)
+ 		__field( pid_t,	dst_pid			)
+ 		__field( pid_t,	dst_tgid		)
+ 		__field( pid_t,	dst_ngid		)
+ 		__field( int,	dst_cpu			)
+ 		__field( int,	dst_nid			)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__entry->src_pid	= task_pid_nr(src_tsk);
+ 		__entry->src_tgid	= task_tgid_nr(src_tsk);
+ 		__entry->src_ngid	= task_numa_group_id(src_tsk);
+ 		__entry->src_cpu	= src_cpu;
+ 		__entry->src_nid	= cpu_to_node(src_cpu);
+ 		__entry->dst_pid	= task_pid_nr(dst_tsk);
+ 		__entry->dst_tgid	= task_tgid_nr(dst_tsk);
+ 		__entry->dst_ngid	= task_numa_group_id(dst_tsk);
+ 		__entry->dst_cpu	= dst_cpu;
+ 		__entry->dst_nid	= cpu_to_node(dst_cpu);
+ 	),
+ 
+ 	TP_printk("src_pid=%d src_tgid=%d src_ngid=%d src_cpu=%d src_nid=%d dst_pid=%d dst_tgid=%d dst_ngid=%d dst_cpu=%d dst_nid=%d",
+ 			__entry->src_pid, __entry->src_tgid, __entry->src_ngid,
+ 			__entry->src_cpu, __entry->src_nid,
+ 			__entry->dst_pid, __entry->dst_tgid, __entry->dst_ngid,
+ 			__entry->dst_cpu, __entry->dst_nid)
+ );
++>>>>>>> 286549dcaf4f (sched: add tracepoints related to NUMA task migration)
  #endif /* _TRACE_SCHED_H */
  
  /* This part must be outside protection */
diff --cc kernel/sched/core.c
index f7886e693a9b,5ae36cc11fe5..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -1030,6 -1013,108 +1030,111 @@@ void set_task_cpu(struct task_struct *p
  	__set_task_cpu(p, new_cpu);
  }
  
++<<<<<<< HEAD
++=======
+ static void __migrate_swap_task(struct task_struct *p, int cpu)
+ {
+ 	if (p->on_rq) {
+ 		struct rq *src_rq, *dst_rq;
+ 
+ 		src_rq = task_rq(p);
+ 		dst_rq = cpu_rq(cpu);
+ 
+ 		deactivate_task(src_rq, p, 0);
+ 		set_task_cpu(p, cpu);
+ 		activate_task(dst_rq, p, 0);
+ 		check_preempt_curr(dst_rq, p, 0);
+ 	} else {
+ 		/*
+ 		 * Task isn't running anymore; make it appear like we migrated
+ 		 * it before it went to sleep. This means on wakeup we make the
+ 		 * previous cpu our targer instead of where it really is.
+ 		 */
+ 		p->wake_cpu = cpu;
+ 	}
+ }
+ 
+ struct migration_swap_arg {
+ 	struct task_struct *src_task, *dst_task;
+ 	int src_cpu, dst_cpu;
+ };
+ 
+ static int migrate_swap_stop(void *data)
+ {
+ 	struct migration_swap_arg *arg = data;
+ 	struct rq *src_rq, *dst_rq;
+ 	int ret = -EAGAIN;
+ 
+ 	src_rq = cpu_rq(arg->src_cpu);
+ 	dst_rq = cpu_rq(arg->dst_cpu);
+ 
+ 	double_raw_lock(&arg->src_task->pi_lock,
+ 			&arg->dst_task->pi_lock);
+ 	double_rq_lock(src_rq, dst_rq);
+ 	if (task_cpu(arg->dst_task) != arg->dst_cpu)
+ 		goto unlock;
+ 
+ 	if (task_cpu(arg->src_task) != arg->src_cpu)
+ 		goto unlock;
+ 
+ 	if (!cpumask_test_cpu(arg->dst_cpu, tsk_cpus_allowed(arg->src_task)))
+ 		goto unlock;
+ 
+ 	if (!cpumask_test_cpu(arg->src_cpu, tsk_cpus_allowed(arg->dst_task)))
+ 		goto unlock;
+ 
+ 	__migrate_swap_task(arg->src_task, arg->dst_cpu);
+ 	__migrate_swap_task(arg->dst_task, arg->src_cpu);
+ 
+ 	ret = 0;
+ 
+ unlock:
+ 	double_rq_unlock(src_rq, dst_rq);
+ 	raw_spin_unlock(&arg->dst_task->pi_lock);
+ 	raw_spin_unlock(&arg->src_task->pi_lock);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * Cross migrate two tasks
+  */
+ int migrate_swap(struct task_struct *cur, struct task_struct *p)
+ {
+ 	struct migration_swap_arg arg;
+ 	int ret = -EINVAL;
+ 
+ 	arg = (struct migration_swap_arg){
+ 		.src_task = cur,
+ 		.src_cpu = task_cpu(cur),
+ 		.dst_task = p,
+ 		.dst_cpu = task_cpu(p),
+ 	};
+ 
+ 	if (arg.src_cpu == arg.dst_cpu)
+ 		goto out;
+ 
+ 	/*
+ 	 * These three tests are all lockless; this is OK since all of them
+ 	 * will be re-checked with proper locks held further down the line.
+ 	 */
+ 	if (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))
+ 		goto out;
+ 
+ 	if (!cpumask_test_cpu(arg.dst_cpu, tsk_cpus_allowed(arg.src_task)))
+ 		goto out;
+ 
+ 	if (!cpumask_test_cpu(arg.src_cpu, tsk_cpus_allowed(arg.dst_task)))
+ 		goto out;
+ 
+ 	trace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);
+ 	ret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);
+ 
+ out:
+ 	return ret;
+ }
+ 
++>>>>>>> 286549dcaf4f (sched: add tracepoints related to NUMA task migration)
  struct migration_arg {
  	struct task_struct *task;
  	int dest_cpu;
@@@ -4944,6 -4589,54 +5049,57 @@@ fail
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_NUMA_BALANCING
+ /* Migrate current task p to target_cpu */
+ int migrate_task_to(struct task_struct *p, int target_cpu)
+ {
+ 	struct migration_arg arg = { p, target_cpu };
+ 	int curr_cpu = task_cpu(p);
+ 
+ 	if (curr_cpu == target_cpu)
+ 		return 0;
+ 
+ 	if (!cpumask_test_cpu(target_cpu, tsk_cpus_allowed(p)))
+ 		return -EINVAL;
+ 
+ 	/* TODO: This is not properly updating schedstats */
+ 
+ 	trace_sched_move_numa(p, curr_cpu, target_cpu);
+ 	return stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);
+ }
+ 
+ /*
+  * Requeue a task on a given node and accurately track the number of NUMA
+  * tasks on the runqueues
+  */
+ void sched_setnuma(struct task_struct *p, int nid)
+ {
+ 	struct rq *rq;
+ 	unsigned long flags;
+ 	bool on_rq, running;
+ 
+ 	rq = task_rq_lock(p, &flags);
+ 	on_rq = p->on_rq;
+ 	running = task_current(rq, p);
+ 
+ 	if (on_rq)
+ 		dequeue_task(rq, p, 0);
+ 	if (running)
+ 		p->sched_class->put_prev_task(rq, p);
+ 
+ 	p->numa_preferred_nid = nid;
+ 
+ 	if (running)
+ 		p->sched_class->set_curr_task(rq);
+ 	if (on_rq)
+ 		enqueue_task(rq, p, 0);
+ 	task_rq_unlock(rq, p, &flags);
+ }
+ #endif
+ 
++>>>>>>> 286549dcaf4f (sched: add tracepoints related to NUMA task migration)
  /*
   * migration_cpu_stop - this will be executed by a highprio stopper thread
   * and performs thread migration by bumping thread off CPU then
diff --cc kernel/sched/fair.c
index 66ca9c969dec,867b0a4b0893..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,13 -872,496 +839,499 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ static void account_numa_enqueue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running += (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ static void account_numa_dequeue(struct rq *rq, struct task_struct *p)
+ {
+ 	rq->nr_numa_running -= (p->numa_preferred_nid != -1);
+ 	rq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));
+ }
+ 
+ struct numa_group {
+ 	atomic_t refcount;
+ 
+ 	spinlock_t lock; /* nr_tasks, tasks */
+ 	int nr_tasks;
+ 	pid_t gid;
+ 	struct list_head task_list;
+ 
+ 	struct rcu_head rcu;
+ 	unsigned long total_faults;
+ 	unsigned long faults[0];
+ };
+ 
+ pid_t task_numa_group_id(struct task_struct *p)
+ {
+ 	return p->numa_group ? p->numa_group->gid : 0;
+ }
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static inline unsigned long group_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group)
+ 		return 0;
+ 
+ 	return p->numa_group->faults[task_faults_idx(nid, 0)] +
+ 		p->numa_group->faults[task_faults_idx(nid, 1)];
+ }
+ 
+ /*
+  * These return the fraction of accesses done by a particular task, or
+  * task group, on a particular numa node.  The group weight is given a
+  * larger multiplier, in order to group tasks together that are almost
+  * evenly spread out between numa nodes.
+  */
+ static inline unsigned long task_weight(struct task_struct *p, int nid)
+ {
+ 	unsigned long total_faults;
+ 
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	total_faults = p->total_numa_faults;
+ 
+ 	if (!total_faults)
+ 		return 0;
+ 
+ 	return 1000 * task_faults(p, nid) / total_faults;
+ }
+ 
+ static inline unsigned long group_weight(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_group || !p->numa_group->total_faults)
+ 		return 0;
+ 
+ 	return 1000 * group_faults(p, nid) / p->numa_group->total_faults;
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ /* Cached statistics for all CPUs within a node */
+ struct numa_stats {
+ 	unsigned long nr_running;
+ 	unsigned long load;
+ 
+ 	/* Total compute capacity of CPUs on a node */
+ 	unsigned long power;
+ 
+ 	/* Approximate capacity in terms of runnable tasks on a node */
+ 	unsigned long capacity;
+ 	int has_capacity;
+ };
+ 
+ /*
+  * XXX borrowed from update_sg_lb_stats
+  */
+ static void update_numa_stats(struct numa_stats *ns, int nid)
+ {
+ 	int cpu, cpus = 0;
+ 
+ 	memset(ns, 0, sizeof(*ns));
+ 	for_each_cpu(cpu, cpumask_of_node(nid)) {
+ 		struct rq *rq = cpu_rq(cpu);
+ 
+ 		ns->nr_running += rq->nr_running;
+ 		ns->load += weighted_cpuload(cpu);
+ 		ns->power += power_of(cpu);
+ 
+ 		cpus++;
+ 	}
+ 
+ 	/*
+ 	 * If we raced with hotplug and there are no CPUs left in our mask
+ 	 * the @ns structure is NULL'ed and task_numa_compare() will
+ 	 * not find this node attractive.
+ 	 *
+ 	 * We'll either bail at !has_capacity, or we'll detect a huge imbalance
+ 	 * and bail there.
+ 	 */
+ 	if (!cpus)
+ 		return;
+ 
+ 	ns->load = (ns->load * SCHED_POWER_SCALE) / ns->power;
+ 	ns->capacity = DIV_ROUND_CLOSEST(ns->power, SCHED_POWER_SCALE);
+ 	ns->has_capacity = (ns->nr_running < ns->capacity);
+ }
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	int imbalance_pct;
+ 
+ 	struct task_struct *best_task;
+ 	long best_imp;
+ 	int best_cpu;
+ };
+ 
+ static void task_numa_assign(struct task_numa_env *env,
+ 			     struct task_struct *p, long imp)
+ {
+ 	if (env->best_task)
+ 		put_task_struct(env->best_task);
+ 	if (p)
+ 		get_task_struct(p);
+ 
+ 	env->best_task = p;
+ 	env->best_imp = imp;
+ 	env->best_cpu = env->dst_cpu;
+ }
+ 
+ /*
+  * This checks if the overall compute and NUMA accesses of the system would
+  * be improved if the source tasks was migrated to the target dst_cpu taking
+  * into account that it might be best if task running on the dst_cpu should
+  * be exchanged with the source task
+  */
+ static void task_numa_compare(struct task_numa_env *env,
+ 			      long taskimp, long groupimp)
+ {
+ 	struct rq *src_rq = cpu_rq(env->src_cpu);
+ 	struct rq *dst_rq = cpu_rq(env->dst_cpu);
+ 	struct task_struct *cur;
+ 	long dst_load, src_load;
+ 	long load;
+ 	long imp = (groupimp > 0) ? groupimp : taskimp;
+ 
+ 	rcu_read_lock();
+ 	cur = ACCESS_ONCE(dst_rq->curr);
+ 	if (cur->pid == 0) /* idle */
+ 		cur = NULL;
+ 
+ 	/*
+ 	 * "imp" is the fault differential for the source task between the
+ 	 * source and destination node. Calculate the total differential for
+ 	 * the source task and potential destination task. The more negative
+ 	 * the value is, the more rmeote accesses that would be expected to
+ 	 * be incurred if the tasks were swapped.
+ 	 */
+ 	if (cur) {
+ 		/* Skip this swap candidate if cannot move to the source cpu */
+ 		if (!cpumask_test_cpu(env->src_cpu, tsk_cpus_allowed(cur)))
+ 			goto unlock;
+ 
+ 		/*
+ 		 * If dst and source tasks are in the same NUMA group, or not
+ 		 * in any group then look only at task weights.
+ 		 */
+ 		if (cur->numa_group == env->p->numa_group) {
+ 			imp = taskimp + task_weight(cur, env->src_nid) -
+ 			      task_weight(cur, env->dst_nid);
+ 			/*
+ 			 * Add some hysteresis to prevent swapping the
+ 			 * tasks within a group over tiny differences.
+ 			 */
+ 			if (cur->numa_group)
+ 				imp -= imp/16;
+ 		} else {
+ 			/*
+ 			 * Compare the group weights. If a task is all by
+ 			 * itself (not part of a group), use the task weight
+ 			 * instead.
+ 			 */
+ 			if (env->p->numa_group)
+ 				imp = groupimp;
+ 			else
+ 				imp = taskimp;
+ 
+ 			if (cur->numa_group)
+ 				imp += group_weight(cur, env->src_nid) -
+ 				       group_weight(cur, env->dst_nid);
+ 			else
+ 				imp += task_weight(cur, env->src_nid) -
+ 				       task_weight(cur, env->dst_nid);
+ 		}
+ 	}
+ 
+ 	if (imp < env->best_imp)
+ 		goto unlock;
+ 
+ 	if (!cur) {
+ 		/* Is there capacity at our destination? */
+ 		if (env->src_stats.has_capacity &&
+ 		    !env->dst_stats.has_capacity)
+ 			goto unlock;
+ 
+ 		goto balance;
+ 	}
+ 
+ 	/* Balance doesn't matter much if we're running a task per cpu */
+ 	if (src_rq->nr_running == 1 && dst_rq->nr_running == 1)
+ 		goto assign;
+ 
+ 	/*
+ 	 * In the overloaded case, try and keep the load balanced.
+ 	 */
+ balance:
+ 	dst_load = env->dst_stats.load;
+ 	src_load = env->src_stats.load;
+ 
+ 	/* XXX missing power terms */
+ 	load = task_h_load(env->p);
+ 	dst_load += load;
+ 	src_load -= load;
+ 
+ 	if (cur) {
+ 		load = task_h_load(cur);
+ 		dst_load -= load;
+ 		src_load += load;
+ 	}
+ 
+ 	/* make src_load the smaller */
+ 	if (dst_load < src_load)
+ 		swap(dst_load, src_load);
+ 
+ 	if (src_load * env->imbalance_pct < dst_load * 100)
+ 		goto unlock;
+ 
+ assign:
+ 	task_numa_assign(env, cur, imp);
+ unlock:
+ 	rcu_read_unlock();
+ }
+ 
+ static void task_numa_find_cpu(struct task_numa_env *env,
+ 				long taskimp, long groupimp)
+ {
+ 	int cpu;
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
+ 		/* Skip this CPU if the source task cannot migrate */
+ 		if (!cpumask_test_cpu(cpu, tsk_cpus_allowed(env->p)))
+ 			continue;
+ 
+ 		env->dst_cpu = cpu;
+ 		task_numa_compare(env, taskimp, groupimp);
+ 	}
+ }
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = task_node(p),
+ 
+ 		.imbalance_pct = 112,
+ 
+ 		.best_task = NULL,
+ 		.best_imp = 0,
+ 		.best_cpu = -1
+ 	};
+ 	struct sched_domain *sd;
+ 	unsigned long taskweight, groupweight;
+ 	int nid, ret;
+ 	long taskimp, groupimp;
+ 
+ 	/*
+ 	 * Pick the lowest SD_NUMA domain, as that would have the smallest
+ 	 * imbalance and would be the first to start moving tasks about.
+ 	 *
+ 	 * And we want to avoid any moving of tasks about, as that would create
+ 	 * random movement of tasks -- counter the numa conditions we're trying
+ 	 * to satisfy here.
+ 	 */
+ 	rcu_read_lock();
+ 	sd = rcu_dereference(per_cpu(sd_numa, env.src_cpu));
+ 	if (sd)
+ 		env.imbalance_pct = 100 + (sd->imbalance_pct - 100) / 2;
+ 	rcu_read_unlock();
+ 
+ 	/*
+ 	 * Cpusets can break the scheduler domain tree into smaller
+ 	 * balance domains, some of which do not cross NUMA boundaries.
+ 	 * Tasks that are "trapped" in such domains cannot be migrated
+ 	 * elsewhere, so there is no point in (re)trying.
+ 	 */
+ 	if (unlikely(!sd)) {
+ 		p->numa_preferred_nid = task_node(p);
+ 		return -EINVAL;
+ 	}
+ 
+ 	taskweight = task_weight(p, env.src_nid);
+ 	groupweight = group_weight(p, env.src_nid);
+ 	update_numa_stats(&env.src_stats, env.src_nid);
+ 	env.dst_nid = p->numa_preferred_nid;
+ 	taskimp = task_weight(p, env.dst_nid) - taskweight;
+ 	groupimp = group_weight(p, env.dst_nid) - groupweight;
+ 	update_numa_stats(&env.dst_stats, env.dst_nid);
+ 
+ 	/* If the preferred nid has capacity, try to use it. */
+ 	if (env.dst_stats.has_capacity)
+ 		task_numa_find_cpu(&env, taskimp, groupimp);
+ 
+ 	/* No space available on the preferred nid. Look elsewhere. */
+ 	if (env.best_cpu == -1) {
+ 		for_each_online_node(nid) {
+ 			if (nid == env.src_nid || nid == p->numa_preferred_nid)
+ 				continue;
+ 
+ 			/* Only consider nodes where both task and groups benefit */
+ 			taskimp = task_weight(p, nid) - taskweight;
+ 			groupimp = group_weight(p, nid) - groupweight;
+ 			if (taskimp < 0 && groupimp < 0)
+ 				continue;
+ 
+ 			env.dst_nid = nid;
+ 			update_numa_stats(&env.dst_stats, env.dst_nid);
+ 			task_numa_find_cpu(&env, taskimp, groupimp);
+ 		}
+ 	}
+ 
+ 	/* No better CPU than the current one was found. */
+ 	if (env.best_cpu == -1)
+ 		return -EAGAIN;
+ 
+ 	sched_setnuma(p, env.dst_nid);
+ 
+ 	/*
+ 	 * Reset the scan period if the task is being rescheduled on an
+ 	 * alternative node to recheck if the tasks is now properly placed.
+ 	 */
+ 	p->numa_scan_period = task_scan_min(p);
+ 
+ 	if (env.best_task == NULL) {
+ 		ret = migrate_task_to(p, env.best_cpu);
+ 		if (ret != 0)
+ 			trace_sched_stick_numa(p, env.src_cpu, env.best_cpu);
+ 		return ret;
+ 	}
+ 
+ 	ret = migrate_swap(p, env.best_task);
+ 	if (ret != 0)
+ 		trace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task));
+ 	put_task_struct(env.best_task);
+ 	return ret;
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1 || !p->numa_faults))
+ 		return;
+ 
+ 	/* Periodically retry migrating the task to the preferred node */
+ 	p->numa_migrate_retry = jiffies + HZ;
+ 
+ 	/* Success if task is already running on preferred CPU */
+ 	if (task_node(p) == p->numa_preferred_nid)
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	task_numa_migrate(p);
+ }
+ 
+ /*
+  * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS
+  * increments. The more local the fault statistics are, the higher the scan
+  * period will be for the next scan window. If local/remote ratio is below
+  * NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS) the
+  * scan period will decrease
+  */
+ #define NUMA_PERIOD_SLOTS 10
+ #define NUMA_PERIOD_THRESHOLD 3
+ 
+ /*
+  * Increase the scan period (slow down scanning) if the majority of
+  * our memory is already on our local node, or if the majority of
+  * the page accesses are shared with other processes.
+  * Otherwise, decrease the scan period.
+  */
+ static void update_task_scan_period(struct task_struct *p,
+ 			unsigned long shared, unsigned long private)
+ {
+ 	unsigned int period_slot;
+ 	int ratio;
+ 	int diff;
+ 
+ 	unsigned long remote = p->numa_faults_locality[0];
+ 	unsigned long local = p->numa_faults_locality[1];
+ 
+ 	/*
+ 	 * If there were no record hinting faults then either the task is
+ 	 * completely idle or all activity is areas that are not of interest
+ 	 * to automatic numa balancing. Scan slower
+ 	 */
+ 	if (local + shared == 0) {
+ 		p->numa_scan_period = min(p->numa_scan_period_max,
+ 			p->numa_scan_period << 1);
+ 
+ 		p->mm->numa_next_scan = jiffies +
+ 			msecs_to_jiffies(p->numa_scan_period);
+ 
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * Prepare to scale scan period relative to the current period.
+ 	 *	 == NUMA_PERIOD_THRESHOLD scan period stays the same
+ 	 *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)
+ 	 *	 >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)
+ 	 */
+ 	period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+ 	ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote);
+ 	if (ratio >= NUMA_PERIOD_THRESHOLD) {
+ 		int slot = ratio - NUMA_PERIOD_THRESHOLD;
+ 		if (!slot)
+ 			slot = 1;
+ 		diff = slot * period_slot;
+ 	} else {
+ 		diff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot;
+ 
+ 		/*
+ 		 * Scale scan rate increases based on sharing. There is an
+ 		 * inverse relationship between the degree of sharing and
+ 		 * the adjustment made to the scanning period. Broadly
+ 		 * speaking the intent is that there is little point
+ 		 * scanning faster if shared accesses dominate as it may
+ 		 * simply bounce migrations uselessly
+ 		 */
+ 		ratio = DIV_ROUND_UP(private * NUMA_PERIOD_SLOTS, (private + shared));
+ 		diff = (diff * ratio) / NUMA_PERIOD_SLOTS;
+ 	}
+ 
+ 	p->numa_scan_period = clamp(p->numa_scan_period + diff,
+ 			task_scan_min(p), task_scan_max(p));
+ 	memset(p->numa_faults_locality, 0, sizeof(p->numa_faults_locality));
+ }
+ 
++>>>>>>> 286549dcaf4f (sched: add tracepoints related to NUMA task migration)
  static void task_numa_placement(struct task_struct *p)
  {
 -	int seq, nid, max_nid = -1, max_group_nid = -1;
 -	unsigned long max_faults = 0, max_group_faults = 0;
 -	unsigned long fault_types[2] = { 0, 0 };
 -	spinlock_t *group_lock = NULL;
 +	int seq, nid, max_nid = -1;
 +	unsigned long max_faults = 0;
  
 +	if (!p->mm)	/* for example, ksmd faulting in a user's mm */
 +		return;
  	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
  	if (p->numa_scan_seq == seq)
  		return;
* Unmerged path include/trace/events/sched.h
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/fair.c
