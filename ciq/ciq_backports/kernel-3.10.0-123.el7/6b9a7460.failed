sched/numa: Retry migration of tasks to CPU on a preferred node

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
commit-author Mel Gorman <mgorman@suse.de>
commit 6b9a7460b6baf6c77fc3d23d927ddfc3f3f05bf3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/6b9a7460.failed

When a preferred node is selected for a tasks there is an attempt to migrate
the task to a CPU there. This may fail in which case the task will only
migrate if the active load balancer takes action. This may never happen if
the conditions are not right. This patch will check at NUMA hinting fault
time if another attempt should be made to migrate the task. It will only
make an attempt once every five seconds.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/r/1381141781-10992-34-git-send-email-mgorman@suse.de
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 6b9a7460b6baf6c77fc3d23d927ddfc3f3f05bf3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 70f4aef1e8ed,f84ac3fb581b..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -839,6 -877,157 +839,160 @@@ static unsigned int task_scan_max(struc
  	return max(smin, smax);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Once a preferred node is selected the scheduler balancer will prefer moving
+  * a task to that node for sysctl_numa_balancing_settle_count number of PTE
+  * scans. This will give the process the chance to accumulate more faults on
+  * the preferred node but still allow the scheduler to move the task again if
+  * the nodes CPUs are overloaded.
+  */
+ unsigned int sysctl_numa_balancing_settle_count __read_mostly = 4;
+ 
+ static inline int task_faults_idx(int nid, int priv)
+ {
+ 	return 2 * nid + priv;
+ }
+ 
+ static inline unsigned long task_faults(struct task_struct *p, int nid)
+ {
+ 	if (!p->numa_faults)
+ 		return 0;
+ 
+ 	return p->numa_faults[task_faults_idx(nid, 0)] +
+ 		p->numa_faults[task_faults_idx(nid, 1)];
+ }
+ 
+ static unsigned long weighted_cpuload(const int cpu);
+ static unsigned long source_load(int cpu, int type);
+ static unsigned long target_load(int cpu, int type);
+ static unsigned long power_of(int cpu);
+ static long effective_load(struct task_group *tg, int cpu, long wl, long wg);
+ 
+ struct numa_stats {
+ 	unsigned long load;
+ 	s64 eff_load;
+ 	unsigned long faults;
+ };
+ 
+ struct task_numa_env {
+ 	struct task_struct *p;
+ 
+ 	int src_cpu, src_nid;
+ 	int dst_cpu, dst_nid;
+ 
+ 	struct numa_stats src_stats, dst_stats;
+ 
+ 	unsigned long best_load;
+ 	int best_cpu;
+ };
+ 
+ static int task_numa_migrate(struct task_struct *p)
+ {
+ 	int node_cpu = cpumask_first(cpumask_of_node(p->numa_preferred_nid));
+ 	struct task_numa_env env = {
+ 		.p = p,
+ 		.src_cpu = task_cpu(p),
+ 		.src_nid = cpu_to_node(task_cpu(p)),
+ 		.dst_cpu = node_cpu,
+ 		.dst_nid = p->numa_preferred_nid,
+ 		.best_load = ULONG_MAX,
+ 		.best_cpu = task_cpu(p),
+ 	};
+ 	struct sched_domain *sd;
+ 	int cpu;
+ 	struct task_group *tg = task_group(p);
+ 	unsigned long weight;
+ 	bool balanced;
+ 	int imbalance_pct, idx = -1;
+ 
+ 	/*
+ 	 * Find the lowest common scheduling domain covering the nodes of both
+ 	 * the CPU the task is currently running on and the target NUMA node.
+ 	 */
+ 	rcu_read_lock();
+ 	for_each_domain(env.src_cpu, sd) {
+ 		if (cpumask_test_cpu(node_cpu, sched_domain_span(sd))) {
+ 			/*
+ 			 * busy_idx is used for the load decision as it is the
+ 			 * same index used by the regular load balancer for an
+ 			 * active cpu.
+ 			 */
+ 			idx = sd->busy_idx;
+ 			imbalance_pct = sd->imbalance_pct;
+ 			break;
+ 		}
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	if (WARN_ON_ONCE(idx == -1))
+ 		return 0;
+ 
+ 	/*
+ 	 * XXX the below is mostly nicked from wake_affine(); we should
+ 	 * see about sharing a bit if at all possible; also it might want
+ 	 * some per entity weight love.
+ 	 */
+ 	weight = p->se.load.weight;
+ 	env.src_stats.load = source_load(env.src_cpu, idx);
+ 	env.src_stats.eff_load = 100 + (imbalance_pct - 100) / 2;
+ 	env.src_stats.eff_load *= power_of(env.src_cpu);
+ 	env.src_stats.eff_load *= env.src_stats.load + effective_load(tg, env.src_cpu, -weight, -weight);
+ 
+ 	for_each_cpu(cpu, cpumask_of_node(env.dst_nid)) {
+ 		env.dst_cpu = cpu;
+ 		env.dst_stats.load = target_load(cpu, idx);
+ 
+ 		/* If the CPU is idle, use it */
+ 		if (!env.dst_stats.load) {
+ 			env.best_cpu = cpu;
+ 			goto migrate;
+ 		}
+ 
+ 		/* Otherwise check the target CPU load */
+ 		env.dst_stats.eff_load = 100;
+ 		env.dst_stats.eff_load *= power_of(cpu);
+ 		env.dst_stats.eff_load *= env.dst_stats.load + effective_load(tg, cpu, weight, weight);
+ 
+ 		/*
+ 		 * Destination is considered balanced if the destination CPU is
+ 		 * less loaded than the source CPU. Unfortunately there is a
+ 		 * risk that a task running on a lightly loaded CPU will not
+ 		 * migrate to its preferred node due to load imbalances.
+ 		 */
+ 		balanced = (env.dst_stats.eff_load <= env.src_stats.eff_load);
+ 		if (!balanced)
+ 			continue;
+ 
+ 		if (env.dst_stats.eff_load < env.best_load) {
+ 			env.best_load = env.dst_stats.eff_load;
+ 			env.best_cpu = cpu;
+ 		}
+ 	}
+ 
+ migrate:
+ 	return migrate_task_to(p, env.best_cpu);
+ }
+ 
+ /* Attempt to migrate a task to a CPU on the preferred node. */
+ static void numa_migrate_preferred(struct task_struct *p)
+ {
+ 	/* Success if task is already running on preferred CPU */
+ 	p->numa_migrate_retry = 0;
+ 	if (cpu_to_node(task_cpu(p)) == p->numa_preferred_nid)
+ 		return;
+ 
+ 	/* This task has no NUMA fault statistics yet */
+ 	if (unlikely(p->numa_preferred_nid == -1))
+ 		return;
+ 
+ 	/* Otherwise, try migrate to a CPU on the preferred node */
+ 	if (task_numa_migrate(p) != 0)
+ 		p->numa_migrate_retry = jiffies + HZ*5;
+ }
+ 
++>>>>>>> 6b9a7460b6ba (sched/numa: Retry migration of tasks to CPU on a preferred node)
  static void task_numa_placement(struct task_struct *p)
  {
  	int seq, nid, max_nid = -1;
@@@ -868,9 -1062,13 +1022,19 @@@
  		}
  	}
  
++<<<<<<< HEAD
 +	/* Update the tasks preferred node if necessary */
 +	if (max_faults && max_nid != p->numa_preferred_nid)
 +		p->numa_preferred_nid = max_nid;
++=======
+ 	/* Preferred node as the node with the most faults */
+ 	if (max_faults && max_nid != p->numa_preferred_nid) {
+ 		/* Update the preferred nid and migrate task if possible */
+ 		p->numa_preferred_nid = max_nid;
+ 		p->numa_migrate_seq = 1;
+ 		numa_migrate_preferred(p);
+ 	}
++>>>>>>> 6b9a7460b6ba (sched/numa: Retry migration of tasks to CPU on a preferred node)
  }
  
  /*
@@@ -911,7 -1123,11 +1075,15 @@@ void task_numa_fault(int node, int page
  
  	task_numa_placement(p);
  
++<<<<<<< HEAD
 +	p->numa_faults_buffer[node] += pages;
++=======
+ 	/* Retry task to preferred node migration if it previously failed */
+ 	if (p->numa_migrate_retry && time_after(jiffies, p->numa_migrate_retry))
+ 		numa_migrate_preferred(p);
+ 
+ 	p->numa_faults_buffer[task_faults_idx(node, priv)] += pages;
++>>>>>>> 6b9a7460b6ba (sched/numa: Retry migration of tasks to CPU on a preferred node)
  }
  
  static void reset_ptenuma_scan(struct task_struct *p)
diff --git a/include/linux/sched.h b/include/linux/sched.h
index dcb716d68b2f..22ebe5d2918c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1342,6 +1342,7 @@ struct task_struct {
 	int numa_migrate_seq;
 	unsigned int numa_scan_period;
 	unsigned int numa_scan_period_max;
+	unsigned long numa_migrate_retry;
 	u64 node_stamp;			/* migration stamp  */
 	struct callback_head numa_work;
 
* Unmerged path kernel/sched/fair.c
