mm: create a separate slab for page->ptl allocation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] create a separate slab for page->ptl allocation (Rik van Riel) [1058896]
Rebuild_FUZZ: 95.92%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit ea1e7ed33708c7a760419ff9ded0a6cb90586a50
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/ea1e7ed3.failed

If DEBUG_SPINLOCK and DEBUG_LOCK_ALLOC are enabled spinlock_t on x86_64
is 72 bytes.  For page->ptl they will be allocated from kmalloc-96 slab,
so we loose 24 on each.  An average system can easily allocate few tens
thousands of page->ptl and overhead is significant.

Let's create a separate slab for page->ptl allocation to solve this.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ingo Molnar <mingo@elte.hu>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ea1e7ed33708c7a760419ff9ded0a6cb90586a50)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	mm/memory.c
diff --cc include/linux/mm.h
index f57f68add466,0548eb201e05..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -1235,18 -1317,24 +1235,32 @@@ static inline pmd_t *pmd_alloc(struct m
  #endif /* CONFIG_MMU && !__ARCH_HAS_4LEVEL_HACK */
  
  #if USE_SPLIT_PTE_PTLOCKS
++<<<<<<< HEAD
 +bool __ptlock_alloc(struct page *page);
 +void __ptlock_free(struct page *page);
++=======
+ #if BLOATED_SPINLOCKS
+ void __init ptlock_cache_init(void);
+ extern bool ptlock_alloc(struct page *page);
+ extern void ptlock_free(struct page *page);
+ 
+ static inline spinlock_t *ptlock_ptr(struct page *page)
+ {
+ 	return page->ptl;
+ }
+ #else /* BLOATED_SPINLOCKS */
+ static inline void ptlock_cache_init(void) {}
++>>>>>>> ea1e7ed33708 (mm: create a separate slab for page->ptl allocation)
  static inline bool ptlock_alloc(struct page *page)
  {
 +	if (sizeof(spinlock_t) > sizeof(page->ptl))
 +		return __ptlock_alloc(page);
  	return true;
  }
 -
  static inline void ptlock_free(struct page *page)
  {
 +	if (sizeof(spinlock_t) > sizeof(page->ptl))
 +		__ptlock_free(page);
  }
  
  static inline spinlock_t *ptlock_ptr(struct page *page)
diff --cc mm/memory.c
index c8aa666d98dc,0409e8f43fa0..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -4287,8 -4271,15 +4287,20 @@@ void copy_user_huge_page(struct page *d
  }
  #endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */
  
++<<<<<<< HEAD
 +#if USE_SPLIT_PTE_PTLOCKS
 +bool __ptlock_alloc(struct page *page)
++=======
+ #if USE_SPLIT_PTE_PTLOCKS && BLOATED_SPINLOCKS
+ static struct kmem_cache *page_ptl_cachep;
+ void __init ptlock_cache_init(void)
+ {
+ 	page_ptl_cachep = kmem_cache_create("page->ptl", sizeof(spinlock_t), 0,
+ 			SLAB_PANIC, NULL);
+ }
+ 
+ bool ptlock_alloc(struct page *page)
++>>>>>>> ea1e7ed33708 (mm: create a separate slab for page->ptl allocation)
  {
  	spinlock_t *ptl;
  
* Unmerged path include/linux/mm.h
diff --git a/init/main.c b/init/main.c
index ec549581d732..838859113c7e 100644
--- a/init/main.c
+++ b/init/main.c
@@ -464,7 +464,7 @@ static void __init mm_init(void)
 	mem_init();
 	kmem_cache_init();
 	percpu_init_late();
-	pgtable_cache_init();
+	pgtable_init();
 	vmalloc_init();
 }
 
* Unmerged path mm/memory.c
