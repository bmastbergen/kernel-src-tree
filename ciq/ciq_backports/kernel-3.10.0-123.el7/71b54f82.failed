x86/mm: Eliminate redundant page table walk during TLB range flushing

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-123.el7
Rebuild_CHGLOG: - [mm] Eliminate redundant page table walk during TLB range flushing (Rik van Riel) [1058886]
Rebuild_FUZZ: 93.85%
commit-author Mel Gorman <mgorman@suse.de>
commit 71b54f8263860a37dd9f50f81880a9d681fd9c10
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-123.el7/71b54f82.failed

When choosing between doing an address space or ranged flush,
the x86 implementation of flush_tlb_mm_range takes into account
whether there are any large pages in the range.  A per-page
flush typically requires fewer entries than would covered by a
single large page and the check is redundant.

There is one potential exception.  THP migration flushes single
THP entries and it conceivably would benefit from flushing a
single entry instead of the mm.  However, this flush is after a
THP allocation, copy and page table update potentially with any
other threads serialised behind it.  In comparison to that, the
flush is noise.  It makes more sense to optimise balancing to
require fewer flushes than to optimise the flush itself.

This patch deletes the redundant huge page check.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Tested-by: Davidlohr Bueso <davidlohr@hp.com>
	Reviewed-by: Rik van Riel <riel@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Alex Shi <alex.shi@linaro.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/n/tip-sgei1drpOcburujPsfh6ovmo@git.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 71b54f8263860a37dd9f50f81880a9d681fd9c10)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/tlb.c
diff --cc arch/x86/mm/tlb.c
index 282375f13c7e,dd8dda167a24..000000000000
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@@ -207,20 -185,22 +181,25 @@@ void flush_tlb_mm_range(struct mm_struc
  		tlb_entries = tlb_lli_4k[ENTRIES];
  	else
  		tlb_entries = tlb_lld_4k[ENTRIES];
 -
  	/* Assume all of TLB entries was occupied by this task */
 -	act_entries = tlb_entries >> tlb_flushall_shift;
 -	act_entries = mm->total_vm > act_entries ? act_entries : mm->total_vm;
 -	nr_base_pages = (end - start) >> PAGE_SHIFT;
 +	act_entries = mm->total_vm > tlb_entries ? tlb_entries : mm->total_vm;
  
  	/* tlb_flushall_shift is on balance point, details in commit log */
++<<<<<<< HEAD
 +	if ((end - start) >> PAGE_SHIFT > act_entries >> tlb_flushall_shift)
++=======
+ 	if (nr_base_pages > act_entries) {
+ 		count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
++>>>>>>> 71b54f826386 (x86/mm: Eliminate redundant page table walk during TLB range flushing)
  		local_flush_tlb();
 -	} else {
 +	else {
 +		if (has_large_page(mm, start, end)) {
 +			local_flush_tlb();
 +			goto flush_all;
 +		}
  		/* flush range by one by one 'invlpg' */
 -		for (addr = start; addr < end;	addr += PAGE_SIZE) {
 -			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 +		for (addr = start; addr < end;	addr += PAGE_SIZE)
  			__flush_tlb_single(addr);
 -		}
  
  		if (cpumask_any_but(mm_cpumask(mm),
  				smp_processor_id()) < nr_cpu_ids)
* Unmerged path arch/x86/mm/tlb.c
