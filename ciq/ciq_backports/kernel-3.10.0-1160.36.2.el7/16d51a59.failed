sched/fair: Don't free p->numa_faults with concurrent readers

jira LE-1907
cve CVE-2019-20934
Rebuild_History Non-Buildable kernel-3.10.0-1160.36.2.el7
commit-author Jann Horn <jannh@google.com>
commit 16d51a590a8ce3befb1308e0e7ab77f3b661af33
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.36.2.el7/16d51a59.failed

When going through execve(), zero out the NUMA fault statistics instead of
freeing them.

During execve, the task is reachable through procfs and the scheduler. A
concurrent /proc/*/sched reader can read data from a freed ->numa_faults
allocation (confirmed by KASAN) and write it back to userspace.
I believe that it would also be possible for a use-after-free read to occur
through a race between a NUMA fault and execve(): task_numa_fault() can
lead to task_numa_compare(), which invokes task_weight() on the currently
running task of a different CPU.

Another way to fix this would be to make ->numa_faults RCU-managed or add
extra locking, but it seems easier to wipe the NUMA fault statistics on
execve.

	Signed-off-by: Jann Horn <jannh@google.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Petr Mladek <pmladek@suse.com>
	Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Will Deacon <will@kernel.org>
Fixes: 82727018b0d3 ("sched/numa: Call task_numa_free() from do_execve()")
Link: https://lkml.kernel.org/r/20190716152047.14424-1-jannh@google.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 16d51a590a8ce3befb1308e0e7ab77f3b661af33)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched/numa_balancing.h
#	kernel/fork.c
#	kernel/sched/fair.c
diff --cc kernel/fork.c
index 8d5da83434bd,2852d0e76ea3..000000000000
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@@ -594,6 -665,280 +594,283 @@@ __cacheline_aligned_in_smp DEFINE_SPINL
  #define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
  #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
  
++<<<<<<< HEAD
++=======
+ /*
+  * Called when the last reference to the mm
+  * is dropped: either by a lazy thread or by
+  * mmput. Free the page directory and the mm.
+  */
+ void __mmdrop(struct mm_struct *mm)
+ {
+ 	BUG_ON(mm == &init_mm);
+ 	WARN_ON_ONCE(mm == current->mm);
+ 	WARN_ON_ONCE(mm == current->active_mm);
+ 	mm_free_pgd(mm);
+ 	destroy_context(mm);
+ 	mmu_notifier_mm_destroy(mm);
+ 	check_mm(mm);
+ 	put_user_ns(mm->user_ns);
+ 	free_mm(mm);
+ }
+ EXPORT_SYMBOL_GPL(__mmdrop);
+ 
+ static void mmdrop_async_fn(struct work_struct *work)
+ {
+ 	struct mm_struct *mm;
+ 
+ 	mm = container_of(work, struct mm_struct, async_put_work);
+ 	__mmdrop(mm);
+ }
+ 
+ static void mmdrop_async(struct mm_struct *mm)
+ {
+ 	if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
+ 		INIT_WORK(&mm->async_put_work, mmdrop_async_fn);
+ 		schedule_work(&mm->async_put_work);
+ 	}
+ }
+ 
+ static inline void free_signal_struct(struct signal_struct *sig)
+ {
+ 	taskstats_tgid_free(sig);
+ 	sched_autogroup_exit(sig);
+ 	/*
+ 	 * __mmdrop is not safe to call from softirq context on x86 due to
+ 	 * pgd_dtor so postpone it to the async context
+ 	 */
+ 	if (sig->oom_mm)
+ 		mmdrop_async(sig->oom_mm);
+ 	kmem_cache_free(signal_cachep, sig);
+ }
+ 
+ static inline void put_signal_struct(struct signal_struct *sig)
+ {
+ 	if (refcount_dec_and_test(&sig->sigcnt))
+ 		free_signal_struct(sig);
+ }
+ 
+ void __put_task_struct(struct task_struct *tsk)
+ {
+ 	WARN_ON(!tsk->exit_state);
+ 	WARN_ON(refcount_read(&tsk->usage));
+ 	WARN_ON(tsk == current);
+ 
+ 	cgroup_free(tsk);
+ 	task_numa_free(tsk, true);
+ 	security_task_free(tsk);
+ 	exit_creds(tsk);
+ 	delayacct_tsk_free(tsk);
+ 	put_signal_struct(tsk->signal);
+ 
+ 	if (!profile_handoff_task(tsk))
+ 		free_task(tsk);
+ }
+ EXPORT_SYMBOL_GPL(__put_task_struct);
+ 
+ void __init __weak arch_task_cache_init(void) { }
+ 
+ /*
+  * set_max_threads
+  */
+ static void set_max_threads(unsigned int max_threads_suggested)
+ {
+ 	u64 threads;
+ 	unsigned long nr_pages = totalram_pages();
+ 
+ 	/*
+ 	 * The number of threads shall be limited such that the thread
+ 	 * structures may only consume a small part of the available memory.
+ 	 */
+ 	if (fls64(nr_pages) + fls64(PAGE_SIZE) > 64)
+ 		threads = MAX_THREADS;
+ 	else
+ 		threads = div64_u64((u64) nr_pages * (u64) PAGE_SIZE,
+ 				    (u64) THREAD_SIZE * 8UL);
+ 
+ 	if (threads > max_threads_suggested)
+ 		threads = max_threads_suggested;
+ 
+ 	max_threads = clamp_t(u64, threads, MIN_THREADS, MAX_THREADS);
+ }
+ 
+ #ifdef CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT
+ /* Initialized by the architecture: */
+ int arch_task_struct_size __read_mostly;
+ #endif
+ 
+ static void task_struct_whitelist(unsigned long *offset, unsigned long *size)
+ {
+ 	/* Fetch thread_struct whitelist for the architecture. */
+ 	arch_thread_struct_whitelist(offset, size);
+ 
+ 	/*
+ 	 * Handle zero-sized whitelist or empty thread_struct, otherwise
+ 	 * adjust offset to position of thread_struct in task_struct.
+ 	 */
+ 	if (unlikely(*size == 0))
+ 		*offset = 0;
+ 	else
+ 		*offset += offsetof(struct task_struct, thread);
+ }
+ 
+ void __init fork_init(void)
+ {
+ 	int i;
+ #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
+ #ifndef ARCH_MIN_TASKALIGN
+ #define ARCH_MIN_TASKALIGN	0
+ #endif
+ 	int align = max_t(int, L1_CACHE_BYTES, ARCH_MIN_TASKALIGN);
+ 	unsigned long useroffset, usersize;
+ 
+ 	/* create a slab on which task_structs can be allocated */
+ 	task_struct_whitelist(&useroffset, &usersize);
+ 	task_struct_cachep = kmem_cache_create_usercopy("task_struct",
+ 			arch_task_struct_size, align,
+ 			SLAB_PANIC|SLAB_ACCOUNT,
+ 			useroffset, usersize, NULL);
+ #endif
+ 
+ 	/* do the arch specific task caches init */
+ 	arch_task_cache_init();
+ 
+ 	set_max_threads(MAX_THREADS);
+ 
+ 	init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;
+ 	init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;
+ 	init_task.signal->rlim[RLIMIT_SIGPENDING] =
+ 		init_task.signal->rlim[RLIMIT_NPROC];
+ 
+ 	for (i = 0; i < UCOUNT_COUNTS; i++) {
+ 		init_user_ns.ucount_max[i] = max_threads/2;
+ 	}
+ 
+ #ifdef CONFIG_VMAP_STACK
+ 	cpuhp_setup_state(CPUHP_BP_PREPARE_DYN, "fork:vm_stack_cache",
+ 			  NULL, free_vm_stack_cache);
+ #endif
+ 
+ 	lockdep_init_task(&init_task);
+ 	uprobes_init();
+ }
+ 
+ int __weak arch_dup_task_struct(struct task_struct *dst,
+ 					       struct task_struct *src)
+ {
+ 	*dst = *src;
+ 	return 0;
+ }
+ 
+ void set_task_stack_end_magic(struct task_struct *tsk)
+ {
+ 	unsigned long *stackend;
+ 
+ 	stackend = end_of_stack(tsk);
+ 	*stackend = STACK_END_MAGIC;	/* for overflow detection */
+ }
+ 
+ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
+ {
+ 	struct task_struct *tsk;
+ 	unsigned long *stack;
+ 	struct vm_struct *stack_vm_area __maybe_unused;
+ 	int err;
+ 
+ 	if (node == NUMA_NO_NODE)
+ 		node = tsk_fork_get_node(orig);
+ 	tsk = alloc_task_struct_node(node);
+ 	if (!tsk)
+ 		return NULL;
+ 
+ 	stack = alloc_thread_stack_node(tsk, node);
+ 	if (!stack)
+ 		goto free_tsk;
+ 
+ 	if (memcg_charge_kernel_stack(tsk))
+ 		goto free_stack;
+ 
+ 	stack_vm_area = task_stack_vm_area(tsk);
+ 
+ 	err = arch_dup_task_struct(tsk, orig);
+ 
+ 	/*
+ 	 * arch_dup_task_struct() clobbers the stack-related fields.  Make
+ 	 * sure they're properly initialized before using any stack-related
+ 	 * functions again.
+ 	 */
+ 	tsk->stack = stack;
+ #ifdef CONFIG_VMAP_STACK
+ 	tsk->stack_vm_area = stack_vm_area;
+ #endif
+ #ifdef CONFIG_THREAD_INFO_IN_TASK
+ 	refcount_set(&tsk->stack_refcount, 1);
+ #endif
+ 
+ 	if (err)
+ 		goto free_stack;
+ 
+ #ifdef CONFIG_SECCOMP
+ 	/*
+ 	 * We must handle setting up seccomp filters once we're under
+ 	 * the sighand lock in case orig has changed between now and
+ 	 * then. Until then, filter must be NULL to avoid messing up
+ 	 * the usage counts on the error path calling free_task.
+ 	 */
+ 	tsk->seccomp.filter = NULL;
+ #endif
+ 
+ 	setup_thread_stack(tsk, orig);
+ 	clear_user_return_notifier(tsk);
+ 	clear_tsk_need_resched(tsk);
+ 	set_task_stack_end_magic(tsk);
+ 
+ #ifdef CONFIG_STACKPROTECTOR
+ 	tsk->stack_canary = get_random_canary();
+ #endif
+ 	if (orig->cpus_ptr == &orig->cpus_mask)
+ 		tsk->cpus_ptr = &tsk->cpus_mask;
+ 
+ 	/*
+ 	 * One for us, one for whoever does the "release_task()" (usually
+ 	 * parent)
+ 	 */
+ 	refcount_set(&tsk->usage, 2);
+ #ifdef CONFIG_BLK_DEV_IO_TRACE
+ 	tsk->btrace_seq = 0;
+ #endif
+ 	tsk->splice_pipe = NULL;
+ 	tsk->task_frag.page = NULL;
+ 	tsk->wake_q.next = NULL;
+ 
+ 	account_kernel_stack(tsk, 1);
+ 
+ 	kcov_task_init(tsk);
+ 
+ #ifdef CONFIG_FAULT_INJECTION
+ 	tsk->fail_nth = 0;
+ #endif
+ 
+ #ifdef CONFIG_BLK_CGROUP
+ 	tsk->throttle_queue = NULL;
+ 	tsk->use_memdelay = 0;
+ #endif
+ 
+ #ifdef CONFIG_MEMCG
+ 	tsk->active_memcg = NULL;
+ #endif
+ 	return tsk;
+ 
+ free_stack:
+ 	free_thread_stack(tsk);
+ free_tsk:
+ 	free_task_struct(tsk);
+ 	return NULL;
+ }
+ 
+ __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
+ 
++>>>>>>> 16d51a590a8c (sched/fair: Don't free p->numa_faults with concurrent readers)
  static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;
  
  static int __init coredump_filter_setup(char *s)
diff --cc kernel/sched/fair.c
index 5530682fcc16,6adb0e0f5feb..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -1744,32 -2346,50 +1744,58 @@@ unlock
  	rcu_assign_pointer(p->numa_group, grp);
  
  	put_numa_group(my_grp);
 -	return;
 -
 -no_join:
 -	rcu_read_unlock();
 -	return;
  }
  
- void task_numa_free(struct task_struct *p)
+ /*
+  * Get rid of NUMA staticstics associated with a task (either current or dead).
+  * If @final is set, the task is dead and has reached refcount zero, so we can
+  * safely free all relevant data structures. Otherwise, there might be
+  * concurrent reads from places like load balancing and procfs, and we should
+  * reset the data back to default state without freeing ->numa_faults.
+  */
+ void task_numa_free(struct task_struct *p, bool final)
  {
  	struct numa_group *grp = p->numa_group;
++<<<<<<< HEAD
++=======
+ 	unsigned long *numa_faults = p->numa_faults;
+ 	unsigned long flags;
++>>>>>>> 16d51a590a8c (sched/fair: Don't free p->numa_faults with concurrent readers)
  	int i;
 +	void *numa_faults = p->numa_faults_memory;
  
+ 	if (!numa_faults)
+ 		return;
+ 
  	if (grp) {
 -		spin_lock_irqsave(&grp->lock, flags);
 +		spin_lock_irq(&grp->lock);
  		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)
 -			grp->faults[i] -= p->numa_faults[i];
 +			grp->faults[i] -= p->numa_faults_memory[i];
  		grp->total_faults -= p->total_numa_faults;
  
 +		list_del(&p->numa_entry);
  		grp->nr_tasks--;
 -		spin_unlock_irqrestore(&grp->lock, flags);
 -		RCU_INIT_POINTER(p->numa_group, NULL);
 +		spin_unlock_irq(&grp->lock);
 +		rcu_assign_pointer(p->numa_group, NULL);
  		put_numa_group(grp);
  	}
  
++<<<<<<< HEAD
 +	p->numa_faults_memory = NULL;
 +	p->numa_faults_buffer_memory = NULL;
 +	p->numa_faults_cpu= NULL;
 +	p->numa_faults_buffer_cpu = NULL;
 +	kfree(numa_faults);
++=======
+ 	if (final) {
+ 		p->numa_faults = NULL;
+ 		kfree(numa_faults);
+ 	} else {
+ 		p->total_numa_faults = 0;
+ 		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)
+ 			numa_faults[i] = 0;
+ 	}
++>>>>>>> 16d51a590a8c (sched/fair: Don't free p->numa_faults with concurrent readers)
  }
  
  /*
* Unmerged path include/linux/sched/numa_balancing.h
diff --git a/fs/exec.c b/fs/exec.c
index 3af835afbb9d..df4a88b2bc3c 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -1624,7 +1624,7 @@ static int do_execve_common(struct filename *filename,
 	current->in_execve = 0;
 	membarrier_execve(current);
 	acct_update_integrals(current);
-	task_numa_free(current);
+	task_numa_free(current, false);
 	free_bprm(bprm);
 	putname(filename);
 	if (displaced)
* Unmerged path include/linux/sched/numa_balancing.h
* Unmerged path kernel/fork.c
* Unmerged path kernel/sched/fair.c
