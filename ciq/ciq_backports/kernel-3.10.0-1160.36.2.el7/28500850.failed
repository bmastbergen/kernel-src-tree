blk-mq: always allow reserved allocation in hctx_may_queue

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1160.36.2.el7
commit-author Ming Lei <ming.lei@redhat.com>
commit 285008501c65a3fcee05d2c2c26cbf629ceff2f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.36.2.el7/28500850.failed

NVMe shares tagset between fabric queue and admin queue or between
connect_q and NS queue, so hctx_may_queue() can be called to allocate
request for these queues.

Tags can be reserved in these tagset. Before error recovery, there is
often lots of in-flight requests which can't be completed, and new
reserved request may be needed in error recovery path. However,
hctx_may_queue() can always return false because there is too many
in-flight requests which can't be completed during error handling.
Finally, nothing can proceed.

Fix this issue by always allowing reserved tag allocation in
hctx_may_queue(). This is reasonable because reserved tags are supposed
to always be available.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Hannes Reinecke <hare@suse.de>
	Cc: David Milburn <dmilburn@redhat.com>
	Cc: Ewan D. Milne <emilne@redhat.com>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 285008501c65a3fcee05d2c2c26cbf629ceff2f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-tag.c
#	block/blk-mq.c
diff --cc block/blk-mq-tag.c
index 94cc174738c0,aacf10decdbd..000000000000
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@@ -94,9 -76,10 +94,16 @@@ static inline bool hctx_may_queue(struc
  static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
  			    struct sbitmap_queue *bt)
  {
++<<<<<<< HEAD
 +	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 +	    !hctx_may_queue(data->hctx, bt))
 +		return -1;
++=======
+ 	if (!data->q->elevator && !(data->flags & BLK_MQ_REQ_RESERVED) &&
+ 			!hctx_may_queue(data->hctx, bt))
+ 		return BLK_MQ_NO_TAG;
+ 
++>>>>>>> 285008501c65 (blk-mq: always allow reserved allocation in hctx_may_queue)
  	if (data->shallow_depth)
  		return __sbitmap_queue_get_shallow(bt, data->shallow_depth);
  	else
diff --cc block/blk-mq.c
index 08520adbac8a,e04b759add75..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1046,37 -1094,47 +1046,54 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
 -static bool __blk_mq_get_driver_tag(struct request *rq)
 +bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 +			   bool wait)
  {
 -	struct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;
 -	unsigned int tag_offset = rq->mq_hctx->tags->nr_reserved_tags;
 -	int tag;
 +	struct blk_mq_alloc_data data = {
 +		.q = rq->q,
 +		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
 +		.flags = wait ? 0 : BLK_MQ_REQ_NOWAIT,
 +	};
 +
 +	if (rq->tag != -1)
 +		goto done;
 +
++<<<<<<< HEAD
 +	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq_aux(rq)->internal_tag))
 +		data.flags |= BLK_MQ_REQ_RESERVED;
  
 -	blk_mq_tag_busy(rq->mq_hctx);
 +	rq->tag = blk_mq_get_tag(&data);
 +	if (rq->tag >= 0) {
 +		if (blk_mq_tag_busy(data.hctx)) {
 +			rq->cmd_flags |= REQ_MQ_INFLIGHT;
 +			atomic_inc(&data.hctx->nr_active);
 +		}
 +		data.hctx->tags->rqs[rq->tag] = rq;
 +	}
  
 +done:
 +	if (hctx)
 +		*hctx = data.hctx;
 +	return rq->tag != -1;
++=======
+ 	if (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {
+ 		bt = rq->mq_hctx->tags->breserved_tags;
+ 		tag_offset = 0;
+ 	} else {
+ 		if (!hctx_may_queue(rq->mq_hctx, bt))
+ 			return false;
+ 	}
+ 
+ 	tag = __sbitmap_queue_get(bt);
+ 	if (tag == BLK_MQ_NO_TAG)
+ 		return false;
+ 
+ 	rq->tag = tag + tag_offset;
+ 	return true;
++>>>>>>> 285008501c65 (blk-mq: always allow reserved allocation in hctx_may_queue)
  }
  
 -static bool blk_mq_get_driver_tag(struct request *rq)
 -{
 -	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
 -
 -	if (rq->tag == BLK_MQ_NO_TAG && !__blk_mq_get_driver_tag(rq))
 -		return false;
 -
 -	if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
 -			!(rq->rq_flags & RQF_MQ_INFLIGHT)) {
 -		rq->rq_flags |= RQF_MQ_INFLIGHT;
 -		__blk_mq_inc_active_requests(hctx);
 -	}
 -	hctx->tags->rqs[rq->tag] = rq;
 -	return true;
 -}
 -
 -static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
 +static int blk_mq_dispatch_wake(wait_queue_t *wait, unsigned mode,
  				int flags, void *key)
  {
  	struct blk_mq_hw_ctx *hctx;
* Unmerged path block/blk-mq-tag.c
* Unmerged path block/blk-mq.c
