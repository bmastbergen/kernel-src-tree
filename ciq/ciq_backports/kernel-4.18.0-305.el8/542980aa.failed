radix tree test: Convert multiorder tests to XArray

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-305.el8
commit-author Matthew Wilcox <willy@infradead.org>
commit 542980aa9318edcfb68aa7bf6eacf2814dc137dd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-305.el8/542980aa.failed

This is the last remaining user of the multiorder functionality of the
radix tree.  Test the XArray instead.

	Signed-off-by: Matthew Wilcox <willy@infradead.org>
(cherry picked from commit 542980aa9318edcfb68aa7bf6eacf2814dc137dd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/radix-tree/multiorder.c
diff --cc tools/testing/radix-tree/multiorder.c
index 98bc8b3f1928,ff27a74d9762..000000000000
--- a/tools/testing/radix-tree/multiorder.c
+++ b/tools/testing/radix-tree/multiorder.c
@@@ -156,262 -154,19 +154,264 @@@ void multiorder_tagged_iteration(struc
  		}
  	}
  
- 	assert(tag_tagged_items(&tree, 1, ~0UL, MT_NUM_ENTRIES * 2, XA_MARK_1,
+ 	assert(tag_tagged_items(xa, 1, ULONG_MAX, MT_NUM_ENTRIES * 2, XA_MARK_1,
  				XA_MARK_0) == TAG_ENTRIES);
  	i = 0;
- 	radix_tree_for_each_tagged(slot, &tree, &iter, 0, 0) {
- 		assert(iter.index == tag_index[i]);
+ 	xas_set(&xas, 0);
+ 	xas_for_each_marked(&xas, item, ULONG_MAX, XA_MARK_0) {
+ 		assert(xas.xa_index == tag_index[i]);
  		i++;
  	}
+ 	assert(i == TAG_ENTRIES);
  
- 	item_kill_tree(&tree);
+ 	item_kill_tree(xa);
  }
  
 +/*
 + * Basic join checks: make sure we can't find an entry in the tree after
 + * a larger entry has replaced it
 + */
 +static void multiorder_join1(unsigned long index,
 +				unsigned order1, unsigned order2)
 +{
 +	unsigned long loc;
 +	void *item, *item2 = item_create(index + 1, order1);
 +	RADIX_TREE(tree, GFP_KERNEL);
 +
 +	item_insert_order(&tree, index, order2);
 +	item = radix_tree_lookup(&tree, index);
 +	radix_tree_join(&tree, index + 1, order1, item2);
 +	loc = find_item(&tree, item);
 +	if (loc == -1)
 +		free(item);
 +	item = radix_tree_lookup(&tree, index + 1);
 +	assert(item == item2);
 +	item_kill_tree(&tree);
 +}
 +
 +/*
 + * Check that the accounting of value entries is handled correctly
 + * by joining a value entry to a normal pointer.
 + */
 +static void multiorder_join2(unsigned order1, unsigned order2)
 +{
 +	RADIX_TREE(tree, GFP_KERNEL);
 +	struct radix_tree_node *node;
 +	void *item1 = item_create(0, order1);
 +	void *item2;
 +
 +	item_insert_order(&tree, 0, order2);
 +	radix_tree_insert(&tree, 1 << order2, xa_mk_value(5));
 +	item2 = __radix_tree_lookup(&tree, 1 << order2, &node, NULL);
 +	assert(item2 == xa_mk_value(5));
 +	assert(node->nr_values == 1);
 +
 +	item2 = radix_tree_lookup(&tree, 0);
 +	free(item2);
 +
 +	radix_tree_join(&tree, 0, order1, item1);
 +	item2 = __radix_tree_lookup(&tree, 1 << order2, &node, NULL);
 +	assert(item2 == item1);
 +	assert(node->nr_values == 0);
 +	item_kill_tree(&tree);
 +}
 +
 +/*
 + * This test revealed an accounting bug for value entries at one point.
 + * Nodes were being freed back into the pool with an elevated exception count
 + * by radix_tree_join() and then radix_tree_split() was failing to zero the
 + * count of value entries.
 + */
 +static void multiorder_join3(unsigned int order)
 +{
 +	RADIX_TREE(tree, GFP_KERNEL);
 +	struct radix_tree_node *node;
 +	void **slot;
 +	struct radix_tree_iter iter;
 +	unsigned long i;
 +
 +	for (i = 0; i < (1 << order); i++) {
 +		radix_tree_insert(&tree, i, xa_mk_value(5));
 +	}
 +
 +	radix_tree_join(&tree, 0, order, xa_mk_value(7));
 +	rcu_barrier();
 +
 +	radix_tree_split(&tree, 0, 0);
 +
 +	radix_tree_for_each_slot(slot, &tree, &iter, 0) {
 +		radix_tree_iter_replace(&tree, &iter, slot, xa_mk_value(5));
 +	}
 +
 +	__radix_tree_lookup(&tree, 0, &node, NULL);
 +	assert(node->nr_values == node->count);
 +
 +	item_kill_tree(&tree);
 +}
 +
 +static void multiorder_join(void)
 +{
 +	int i, j, idx;
 +
 +	for (idx = 0; idx < 1024; idx = idx * 2 + 3) {
 +		for (i = 1; i < 15; i++) {
 +			for (j = 0; j < i; j++) {
 +				multiorder_join1(idx, i, j);
 +			}
 +		}
 +	}
 +
 +	for (i = 1; i < 15; i++) {
 +		for (j = 0; j < i; j++) {
 +			multiorder_join2(i, j);
 +		}
 +	}
 +
 +	for (i = 3; i < 10; i++) {
 +		multiorder_join3(i);
 +	}
 +}
 +
 +static void check_mem(unsigned old_order, unsigned new_order, unsigned alloc)
 +{
 +	struct radix_tree_preload *rtp = &radix_tree_preloads;
 +	if (rtp->nr != 0)
 +		printv(2, "split(%u %u) remaining %u\n", old_order, new_order,
 +							rtp->nr);
 +	/*
 +	 * Can't check for equality here as some nodes may have been
 +	 * RCU-freed while we ran.  But we should never finish with more
 +	 * nodes allocated since they should have all been preloaded.
 +	 */
 +	if (nr_allocated > alloc)
 +		printv(2, "split(%u %u) allocated %u %u\n", old_order, new_order,
 +							alloc, nr_allocated);
 +}
 +
 +static void __multiorder_split(int old_order, int new_order)
 +{
 +	RADIX_TREE(tree, GFP_ATOMIC);
 +	void **slot;
 +	struct radix_tree_iter iter;
 +	unsigned alloc;
 +	struct item *item;
 +
 +	radix_tree_preload(GFP_KERNEL);
 +	assert(item_insert_order(&tree, 0, old_order) == 0);
 +	radix_tree_preload_end();
 +
 +	/* Wipe out the preloaded cache or it'll confuse check_mem() */
 +	radix_tree_cpu_dead(0);
 +
 +	item = radix_tree_tag_set(&tree, 0, 2);
 +
 +	radix_tree_split_preload(old_order, new_order, GFP_KERNEL);
 +	alloc = nr_allocated;
 +	radix_tree_split(&tree, 0, new_order);
 +	check_mem(old_order, new_order, alloc);
 +	radix_tree_for_each_slot(slot, &tree, &iter, 0) {
 +		radix_tree_iter_replace(&tree, &iter, slot,
 +					item_create(iter.index, new_order));
 +	}
 +	radix_tree_preload_end();
 +
 +	item_kill_tree(&tree);
 +	free(item);
 +}
 +
 +static void __multiorder_split2(int old_order, int new_order)
 +{
 +	RADIX_TREE(tree, GFP_KERNEL);
 +	void **slot;
 +	struct radix_tree_iter iter;
 +	struct radix_tree_node *node;
 +	void *item;
 +
 +	__radix_tree_insert(&tree, 0, old_order, xa_mk_value(5));
 +
 +	item = __radix_tree_lookup(&tree, 0, &node, NULL);
 +	assert(item == xa_mk_value(5));
 +	assert(node->nr_values > 0);
 +
 +	radix_tree_split(&tree, 0, new_order);
 +	radix_tree_for_each_slot(slot, &tree, &iter, 0) {
 +		radix_tree_iter_replace(&tree, &iter, slot,
 +					item_create(iter.index, new_order));
 +	}
 +
 +	item = __radix_tree_lookup(&tree, 0, &node, NULL);
 +	assert(item != xa_mk_value(5));
 +	assert(node->nr_values == 0);
 +
 +	item_kill_tree(&tree);
 +}
 +
 +static void __multiorder_split3(int old_order, int new_order)
 +{
 +	RADIX_TREE(tree, GFP_KERNEL);
 +	void **slot;
 +	struct radix_tree_iter iter;
 +	struct radix_tree_node *node;
 +	void *item;
 +
 +	__radix_tree_insert(&tree, 0, old_order, xa_mk_value(5));
 +
 +	item = __radix_tree_lookup(&tree, 0, &node, NULL);
 +	assert(item == xa_mk_value(5));
 +	assert(node->nr_values > 0);
 +
 +	radix_tree_split(&tree, 0, new_order);
 +	radix_tree_for_each_slot(slot, &tree, &iter, 0) {
 +		radix_tree_iter_replace(&tree, &iter, slot, xa_mk_value(7));
 +	}
 +
 +	item = __radix_tree_lookup(&tree, 0, &node, NULL);
 +	assert(item == xa_mk_value(7));
 +	assert(node->nr_values > 0);
 +
 +	item_kill_tree(&tree);
 +
 +	__radix_tree_insert(&tree, 0, old_order, xa_mk_value(5));
 +
 +	item = __radix_tree_lookup(&tree, 0, &node, NULL);
 +	assert(item == xa_mk_value(5));
 +	assert(node->nr_values > 0);
 +
 +	radix_tree_split(&tree, 0, new_order);
 +	radix_tree_for_each_slot(slot, &tree, &iter, 0) {
 +		if (iter.index == (1 << new_order))
 +			radix_tree_iter_replace(&tree, &iter, slot,
 +						xa_mk_value(7));
 +		else
 +			radix_tree_iter_replace(&tree, &iter, slot, NULL);
 +	}
 +
 +	item = __radix_tree_lookup(&tree, 1 << new_order, &node, NULL);
 +	assert(item == xa_mk_value(7));
 +	assert(node->count == node->nr_values);
 +	do {
 +		node = node->parent;
 +		if (!node)
 +			break;
 +		assert(node->count == 1);
 +		assert(node->nr_values == 0);
 +	} while (1);
 +
 +	item_kill_tree(&tree);
 +}
 +
 +static void multiorder_split(void)
 +{
 +	int i, j;
 +
 +	for (i = 3; i < 11; i++)
 +		for (j = 0; j < i; j++) {
 +			__multiorder_split(i, j);
 +			__multiorder_split2(i, j);
 +			__multiorder_split3(i, j);
 +		}
 +}
 +
  bool stop_iteration = false;
  
  static void *creator_func(void *ptr)
@@@ -470,16 -216,16 +461,24 @@@ static void multiorder_iteration_race(s
  	for (i = 0; i < num_threads; i++)
  		pthread_join(worker_thread[i], NULL);
  
- 	item_kill_tree(&tree);
+ 	item_kill_tree(xa);
  }
  
+ static DEFINE_XARRAY(array);
+ 
  void multiorder_checks(void)
  {
++<<<<<<< HEAD
 +	multiorder_iteration();
 +	multiorder_tagged_iteration();
 +	multiorder_join();
 +	multiorder_split();
 +	multiorder_iteration_race();
++=======
+ 	multiorder_iteration(&array);
+ 	multiorder_tagged_iteration(&array);
+ 	multiorder_iteration_race(&array);
++>>>>>>> 542980aa9318 (radix tree test: Convert multiorder tests to XArray)
  
  	radix_tree_cpu_dead(0);
  }
* Unmerged path tools/testing/radix-tree/multiorder.c
