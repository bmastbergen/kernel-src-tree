rcu: Support reclaim for head-less object

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Uladzislau Rezki (Sony) <urezki@gmail.com>
commit 3042f83f19bec2e0cd356f72b39e4d816e8cd5ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/3042f83f.failed

Update the kvfree_call_rcu() function with head-less support.
This allows RCU to reclaim objects without an embedded rcu_head.

tree-RCU:
We introduce two chains of arrays to store SLAB-backed and vmalloc
pointers, each.  Storage in either of these arrays does not require
embedding an rcu_head within the object.

Maintaining the arrays may become impossible due to high memory
pressure. For such cases there is an emergency path. Objects with
rcu_head inside are just queued on a backup rcu_head list. Later on
that list is drained. As for the head-less variant, as the current
context can sleep, the following emergency measures are applied:
   a) Synchronously wait until a grace period has elapsed.
   b) Call kvfree().

tiny-RCU:
For double argument calls, there are no new changes in behavior. For
single argument call, kvfree() is directly inlined on the current
stack after a synchronize_rcu() call. Note that for tiny-RCU, any
call to synchronize_rcu() is actually a quiescent state, therefore
it does nothing.

	Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
	Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
	Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
Co-developed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
(cherry picked from commit 3042f83f19bec2e0cd356f72b39e4d816e8cd5ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/rcutiny.h
#	kernel/rcu/tree.c
diff --cc include/linux/rcutiny.h
index 296f9116796b,5cc9637cac16..000000000000
--- a/include/linux/rcutiny.h
+++ b/include/linux/rcutiny.h
@@@ -47,14 -34,25 +47,34 @@@ static inline void synchronize_rcu_expe
  	synchronize_rcu();
  }
  
++<<<<<<< HEAD
 +static inline void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
 +{
 +	call_rcu(head, func);
 +}
 +
 +static inline void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func)
++=======
+ /*
+  * Add one more declaration of kvfree() here. It is
+  * not so straight forward to just include <linux/mm.h>
+  * where it is defined due to getting many compile
+  * errors caused by that include.
+  */
+ extern void kvfree(const void *addr);
+ 
+ static inline void kvfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
++>>>>>>> 3042f83f19be (rcu: Support reclaim for head-less object)
  {
- 	call_rcu(head, func);
+ 	if (head) {
+ 		call_rcu(head, func);
+ 		return;
+ 	}
+ 
+ 	// kvfree_rcu(one_arg) call.
+ 	might_sleep();
+ 	synchronize_rcu();
+ 	kvfree((void *) func);
  }
  
  void rcu_qs(void);
diff --cc kernel/rcu/tree.c
index 4aa7b6bbcde6,01f29e4500ba..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -2810,55 -3282,131 +2810,163 @@@ static void kfree_rcu_monitor(struct wo
  	if (krcp->monitor_todo)
  		kfree_rcu_drain_unlock(krcp, flags);
  	else
++<<<<<<< HEAD
 +		spin_unlock_irqrestore(&krcp->lock, flags);
++=======
+ 		raw_spin_unlock_irqrestore(&krcp->lock, flags);
+ }
+ 
+ static inline bool
+ kvfree_call_rcu_add_ptr_to_bulk(struct kfree_rcu_cpu *krcp, void *ptr)
+ {
+ 	struct kvfree_rcu_bulk_data *bnode;
+ 	int idx;
+ 
+ 	if (unlikely(!krcp->initialized))
+ 		return false;
+ 
+ 	lockdep_assert_held(&krcp->lock);
+ 	idx = !!is_vmalloc_addr(ptr);
+ 
+ 	/* Check if a new block is required. */
+ 	if (!krcp->bkvhead[idx] ||
+ 			krcp->bkvhead[idx]->nr_records == KVFREE_BULK_MAX_ENTR) {
+ 		bnode = get_cached_bnode(krcp);
+ 		if (!bnode) {
+ 			/*
+ 			 * To keep this path working on raw non-preemptible
+ 			 * sections, prevent the optional entry into the
+ 			 * allocator as it uses sleeping locks. In fact, even
+ 			 * if the caller of kfree_rcu() is preemptible, this
+ 			 * path still is not, as krcp->lock is a raw spinlock.
+ 			 * With additional page pre-allocation in the works,
+ 			 * hitting this return is going to be much less likely.
+ 			 */
+ 			if (IS_ENABLED(CONFIG_PREEMPT_RT))
+ 				return false;
+ 
+ 			/*
+ 			 * NOTE: For one argument of kvfree_rcu() we can
+ 			 * drop the lock and get the page in sleepable
+ 			 * context. That would allow to maintain an array
+ 			 * for the CONFIG_PREEMPT_RT as well if no cached
+ 			 * pages are available.
+ 			 */
+ 			bnode = (struct kvfree_rcu_bulk_data *)
+ 				__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
+ 		}
+ 
+ 		/* Switch to emergency path. */
+ 		if (unlikely(!bnode))
+ 			return false;
+ 
+ 		/* Initialize the new block. */
+ 		bnode->nr_records = 0;
+ 		bnode->next = krcp->bkvhead[idx];
+ 
+ 		/* Attach it to the head. */
+ 		krcp->bkvhead[idx] = bnode;
+ 	}
+ 
+ 	/* Finally insert. */
+ 	krcp->bkvhead[idx]->records
+ 		[krcp->bkvhead[idx]->nr_records++] = ptr;
+ 
+ 	return true;
++>>>>>>> 3042f83f19be (rcu: Support reclaim for head-less object)
 +}
 +
 +/*
 + * This version of kfree_call_rcu does not do batching of kfree_rcu() requests.
 + * Used only by rcuperf torture test for comparison with kfree_rcu_batch().
 + */
 +void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func)
 +{
 +	__call_rcu(head, func, 1);
  }
 +EXPORT_SYMBOL_GPL(kfree_call_rcu_nobatch);
  
  /*
 - * Queue a request for lazy invocation of appropriate free routine after a
 - * grace period. Please note there are three paths are maintained, two are the
 - * main ones that use array of pointers interface and third one is emergency
 - * one, that is used only when the main path can not be maintained temporary,
 - * due to memory pressure.
 + * Queue a request for lazy invocation of kfree() after a grace period.
 + *
 + * Each kfree_call_rcu() request is added to a batch. The batch will be drained
 + * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch
 + * will be kfree'd in workqueue context. This allows us to:
 + *
 + * 1.	Batch requests together to reduce the number of grace periods during
 + *	heavy kfree_rcu() load.
   *
 - * Each kvfree_call_rcu() request is added to a batch. The batch will be drained
 - * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch will
 - * be free'd in workqueue context. This allows us to: batch requests together to
 - * reduce the number of grace periods during heavy kfree_rcu()/kvfree_rcu() load.
 + * 2.	It makes it possible to use kfree_bulk() on a large number of
 + *	kfree_rcu() requests thus reducing cache misses and the per-object
 + *	overhead of kfree().
   */
 -void kvfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
 +void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
  {
  	unsigned long flags;
  	struct kfree_rcu_cpu *krcp;
++<<<<<<< HEAD
 +
 +	head->func = func;
 +
 +	local_irq_save(flags);	// For safely calling this_cpu_ptr().
 +	krcp = this_cpu_ptr(&krc);
 +	if (krcp->initialized)
 +		spin_lock(&krcp->lock);
++=======
+ 	bool success;
+ 	void *ptr;
+ 
+ 	if (head) {
+ 		ptr = (void *) head - (unsigned long) func;
+ 	} else {
+ 		/*
+ 		 * Please note there is a limitation for the head-less
+ 		 * variant, that is why there is a clear rule for such
+ 		 * objects: it can be used from might_sleep() context
+ 		 * only. For other places please embed an rcu_head to
+ 		 * your data.
+ 		 */
+ 		might_sleep();
+ 		ptr = (unsigned long *) func;
+ 	}
+ 
+ 	krcp = krc_this_cpu_lock(&flags);
++>>>>>>> 3042f83f19be (rcu: Support reclaim for head-less object)
  
  	// Queue the object but don't yet schedule the batch.
 -	if (debug_rcu_head_queue(ptr)) {
 +	if (debug_rcu_head_queue(head)) {
  		// Probable double kfree_rcu(), just leak.
  		WARN_ONCE(1, "%s(): Double-freed call. rcu_head %p\n",
  			  __func__, head);
+ 
+ 		// Mark as success and leave.
+ 		success = true;
  		goto unlock_return;
  	}
++<<<<<<< HEAD
 +	head->func = func;
 +	head->next = krcp->head;
 +	krcp->head = head;
++=======
+ 
+ 	/*
+ 	 * Under high memory pressure GFP_NOWAIT can fail,
+ 	 * in that case the emergency path is maintained.
+ 	 */
+ 	success = kvfree_call_rcu_add_ptr_to_bulk(krcp, ptr);
+ 	if (!success) {
+ 		if (head == NULL)
+ 			// Inline if kvfree_rcu(one_arg) call.
+ 			goto unlock_return;
+ 
+ 		head->func = func;
+ 		head->next = krcp->head;
+ 		krcp->head = head;
+ 		success = true;
+ 	}
+ 
+ 	WRITE_ONCE(krcp->count, krcp->count + 1);
++>>>>>>> 3042f83f19be (rcu: Support reclaim for head-less object)
  
  	// Set timer to drain after KFREE_DRAIN_JIFFIES.
  	if (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&
@@@ -2868,11 -3416,70 +2976,26 @@@
  	}
  
  unlock_return:
++<<<<<<< HEAD
 +	if (krcp->initialized)
 +		spin_unlock(&krcp->lock);
 +	local_irq_restore(flags);
++=======
+ 	krc_this_cpu_unlock(krcp, flags);
+ 
+ 	/*
+ 	 * Inline kvfree() after synchronize_rcu(). We can do
+ 	 * it from might_sleep() context only, so the current
+ 	 * CPU can pass the QS state.
+ 	 */
+ 	if (!success) {
+ 		debug_rcu_head_unqueue((struct rcu_head *) ptr);
+ 		synchronize_rcu();
+ 		kvfree(ptr);
+ 	}
++>>>>>>> 3042f83f19be (rcu: Support reclaim for head-less object)
  }
 -EXPORT_SYMBOL_GPL(kvfree_call_rcu);
 -
 -static unsigned long
 -kfree_rcu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
 -{
 -	int cpu;
 -	unsigned long count = 0;
 -
 -	/* Snapshot count of all CPUs */
 -	for_each_online_cpu(cpu) {
 -		struct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);
 -
 -		count += READ_ONCE(krcp->count);
 -	}
 -
 -	return count;
 -}
 -
 -static unsigned long
 -kfree_rcu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
 -{
 -	int cpu, freed = 0;
 -	unsigned long flags;
 -
 -	for_each_online_cpu(cpu) {
 -		int count;
 -		struct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);
 -
 -		count = krcp->count;
 -		raw_spin_lock_irqsave(&krcp->lock, flags);
 -		if (krcp->monitor_todo)
 -			kfree_rcu_drain_unlock(krcp, flags);
 -		else
 -			raw_spin_unlock_irqrestore(&krcp->lock, flags);
 -
 -		sc->nr_to_scan -= count;
 -		freed += count;
 -
 -		if (sc->nr_to_scan <= 0)
 -			break;
 -	}
 -
 -	return freed;
 -}
 -
 -static struct shrinker kfree_rcu_shrinker = {
 -	.count_objects = kfree_rcu_shrink_count,
 -	.scan_objects = kfree_rcu_shrink_scan,
 -	.batch = 0,
 -	.seeks = DEFAULT_SEEKS,
 -};
 +EXPORT_SYMBOL_GPL(kfree_call_rcu);
  
  void __init kfree_rcu_scheduler_running(void)
  {
* Unmerged path include/linux/rcutiny.h
* Unmerged path kernel/rcu/tree.c
