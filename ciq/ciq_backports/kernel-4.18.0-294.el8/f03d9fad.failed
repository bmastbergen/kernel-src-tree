RDMA/core: Add weak ordering dma attr to dma mapping

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Michael Guralnik <michaelgur@mellanox.com>
commit f03d9fadfe13a78ee28fec320d43f7b37574adcb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f03d9fad.failed

For memory regions registered with IB_ACCESS_RELAXED_ORDERING will be dma
mapped with the DMA_ATTR_WEAK_ORDERING.

This will allow reads and writes to the mapping to be weakly ordered, such
change can enhance performance on some supporting architectures.

Link: https://lore.kernel.org/r/20200212073559.684139-1-leon@kernel.org
	Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit f03d9fadfe13a78ee28fec320d43f7b37574adcb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem.c
diff --cc drivers/infiniband/core/umem.c
index 378698f5b8ce,82455a1392f1..000000000000
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@@ -287,10 -279,12 +288,19 @@@ struct ib_umem *ib_umem_get(struct ib_u
  
  	sg_mark_end(sg);
  
++<<<<<<< HEAD
 +	umem->nmap = ib_dma_map_sg(context->device,
 +				  umem->sg_head.sgl,
 +				  umem->sg_nents,
 +				  DMA_BIDIRECTIONAL);
++=======
+ 	if (access & IB_ACCESS_RELAXED_ORDERING)
+ 		dma_attr |= DMA_ATTR_WEAK_ORDERING;
+ 
+ 	umem->nmap =
+ 		ib_dma_map_sg_attrs(device, umem->sg_head.sgl, umem->sg_nents,
+ 				    DMA_BIDIRECTIONAL, dma_attr);
++>>>>>>> f03d9fadfe13 (RDMA/core: Add weak ordering dma attr to dma mapping)
  
  	if (!umem->nmap) {
  		ret = -ENOMEM;
* Unmerged path drivers/infiniband/core/umem.c
