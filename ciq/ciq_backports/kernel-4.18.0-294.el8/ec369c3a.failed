mptcp: do not queue excessive data on subflows

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Paolo Abeni <pabeni@redhat.com>
commit ec369c3a337fe075a7bd4da88d163d44c62ccbb1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/ec369c3a.failed

The current packet scheduler can enqueue up to sndbuf
data on each subflow. If the send buffer is large and
the subflows are not symmetric, this could lead to
suboptimal aggregate bandwidth utilization.

Limit the amount of queued data to the maximum send
window.

	Reviewed-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit ec369c3a337fe075a7bd4da88d163d44c62ccbb1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/mptcp/protocol.c
diff --cc net/mptcp/protocol.c
index 509aa48ee70d,e741201acc98..000000000000
--- a/net/mptcp/protocol.c
+++ b/net/mptcp/protocol.c
@@@ -886,42 -1357,209 +886,137 @@@ static void mptcp_nospace(struct mptcp_
  
  static struct sock *mptcp_subflow_get_send(struct mptcp_sock *msk)
  {
 -	struct subflow_send_info send_info[2];
  	struct mptcp_subflow_context *subflow;
 -	int i, nr_active = 0;
 -	struct sock *ssk;
 -	u64 ratio;
 -	u32 pace;
 -
 -	sock_owned_by_me((struct sock *)msk);
 +	struct sock *sk = (struct sock *)msk;
 +	struct sock *backup = NULL;
 +	bool free;
  
 -	if (__mptcp_check_fallback(msk)) {
 -		if (!msk->first)
 -			return NULL;
 -		return sk_stream_memory_free(msk->first) ? msk->first : NULL;
 -	}
 +	sock_owned_by_me(sk);
  
 -	/* re-use last subflow, if the burst allow that */
 -	if (msk->last_snd && msk->snd_burst > 0 &&
 -	    sk_stream_memory_free(msk->last_snd) &&
 -	    mptcp_subflow_active(mptcp_subflow_ctx(msk->last_snd)))
 -		return msk->last_snd;
 +	if (!mptcp_ext_cache_refill(msk))
 +		return NULL;
  
 -	/* pick the subflow with the lower wmem/wspace ratio */
 -	for (i = 0; i < 2; ++i) {
 -		send_info[i].ssk = NULL;
 -		send_info[i].ratio = -1;
 -	}
  	mptcp_for_each_subflow(msk, subflow) {
 -		ssk =  mptcp_subflow_tcp_sock(subflow);
 -		if (!mptcp_subflow_active(subflow))
 -			continue;
 +		struct sock *ssk = mptcp_subflow_tcp_sock(subflow);
  
++<<<<<<< HEAD
 +		free = sk_stream_is_writeable(subflow->tcp_sock);
 +		if (!free) {
 +			mptcp_nospace(msk);
 +			return NULL;
++=======
+ 		nr_active += !subflow->backup;
+ 		if (!sk_stream_memory_free(subflow->tcp_sock) || !tcp_sk(ssk)->snd_wnd)
+ 			continue;
+ 
+ 		pace = READ_ONCE(ssk->sk_pacing_rate);
+ 		if (!pace)
+ 			continue;
+ 
+ 		ratio = div_u64((u64)READ_ONCE(ssk->sk_wmem_queued) << 32,
+ 				pace);
+ 		if (ratio < send_info[subflow->backup].ratio) {
+ 			send_info[subflow->backup].ssk = ssk;
+ 			send_info[subflow->backup].ratio = ratio;
++>>>>>>> ec369c3a337f (mptcp: do not queue excessive data on subflows)
  		}
 -	}
  
 -	pr_debug("msk=%p nr_active=%d ssk=%p:%lld backup=%p:%lld",
 -		 msk, nr_active, send_info[0].ssk, send_info[0].ratio,
 -		 send_info[1].ssk, send_info[1].ratio);
 +		if (subflow->backup) {
 +			if (!backup)
 +				backup = ssk;
  
++<<<<<<< HEAD
 +			continue;
++=======
+ 	/* pick the best backup if no other subflow is active */
+ 	if (!nr_active)
+ 		send_info[0].ssk = send_info[1].ssk;
+ 
+ 	if (send_info[0].ssk) {
+ 		msk->last_snd = send_info[0].ssk;
+ 		msk->snd_burst = min_t(int, MPTCP_SEND_BURST_SIZE,
+ 				       tcp_sk(msk->last_snd)->snd_wnd);
+ 		return msk->last_snd;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static void mptcp_push_release(struct sock *sk, struct sock *ssk,
+ 			       struct mptcp_sendmsg_info *info)
+ {
+ 	mptcp_set_timeout(sk, ssk);
+ 	tcp_push(ssk, 0, info->mss_now, tcp_sk(ssk)->nonagle, info->size_goal);
+ 	release_sock(ssk);
+ }
+ 
+ static void mptcp_push_pending(struct sock *sk, unsigned int flags)
+ {
+ 	struct sock *prev_ssk = NULL, *ssk = NULL;
+ 	struct mptcp_sock *msk = mptcp_sk(sk);
+ 	struct mptcp_sendmsg_info info = {
+ 				.flags = flags,
+ 	};
+ 	struct mptcp_data_frag *dfrag;
+ 	int len, copied = 0;
+ 
+ 	while ((dfrag = mptcp_send_head(sk))) {
+ 		info.sent = dfrag->already_sent;
+ 		info.limit = dfrag->data_len;
+ 		len = dfrag->data_len - dfrag->already_sent;
+ 		while (len > 0) {
+ 			int ret = 0;
+ 
+ 			prev_ssk = ssk;
+ 			__mptcp_flush_join_list(msk);
+ 			ssk = mptcp_subflow_get_send(msk);
+ 
+ 			/* try to keep the subflow socket lock across
+ 			 * consecutive xmit on the same socket
+ 			 */
+ 			if (ssk != prev_ssk && prev_ssk)
+ 				mptcp_push_release(sk, prev_ssk, &info);
+ 			if (!ssk)
+ 				goto out;
+ 
+ 			if (ssk != prev_ssk || !prev_ssk)
+ 				lock_sock(ssk);
+ 
+ 			/* keep it simple and always provide a new skb for the
+ 			 * subflow, even if we will not use it when collapsing
+ 			 * on the pending one
+ 			 */
+ 			if (!mptcp_alloc_tx_skb(sk, ssk)) {
+ 				mptcp_push_release(sk, ssk, &info);
+ 				goto out;
+ 			}
+ 
+ 			ret = mptcp_sendmsg_frag(sk, ssk, dfrag, &info);
+ 			if (ret <= 0) {
+ 				mptcp_push_release(sk, ssk, &info);
+ 				goto out;
+ 			}
+ 
+ 			info.sent += ret;
+ 			dfrag->already_sent += ret;
+ 			msk->snd_nxt += ret;
+ 			msk->snd_burst -= ret;
+ 			msk->tx_pending_data -= ret;
+ 			copied += ret;
+ 			len -= ret;
++>>>>>>> ec369c3a337f (mptcp: do not queue excessive data on subflows)
  		}
 -		WRITE_ONCE(msk->first_pending, mptcp_send_next(sk));
 -	}
 -
 -	/* at this point we held the socket lock for the last subflow we used */
 -	if (ssk)
 -		mptcp_push_release(sk, ssk, &info);
 -
 -out:
 -	if (copied) {
 -		/* start the timer, if it's not pending */
 -		if (!mptcp_timer_pending(sk))
 -			mptcp_reset_timer(sk);
 -		__mptcp_check_send_data_fin(sk);
 -	}
 -}
 -
 -static void __mptcp_subflow_push_pending(struct sock *sk, struct sock *ssk)
 -{
 -	struct mptcp_sock *msk = mptcp_sk(sk);
 -	struct mptcp_sendmsg_info info;
 -	struct mptcp_data_frag *dfrag;
 -	int len, copied = 0;
 -
 -	info.flags = 0;
 -	while ((dfrag = mptcp_send_head(sk))) {
 -		info.sent = dfrag->already_sent;
 -		info.limit = dfrag->data_len;
 -		len = dfrag->data_len - dfrag->already_sent;
 -		while (len > 0) {
 -			int ret = 0;
  
 -			if (unlikely(mptcp_must_reclaim_memory(sk, ssk))) {
 -				__mptcp_update_wmem(sk);
 -				sk_mem_reclaim_partial(sk);
 -			}
 -			if (!__mptcp_alloc_tx_skb(sk, ssk, GFP_ATOMIC))
 -				goto out;
 -
 -			ret = mptcp_sendmsg_frag(sk, ssk, dfrag, &info);
 -			if (ret <= 0)
 -				goto out;
 -
 -			info.sent += ret;
 -			dfrag->already_sent += ret;
 -			msk->snd_nxt += ret;
 -			msk->snd_burst -= ret;
 -			msk->tx_pending_data -= ret;
 -			copied += ret;
 -			len -= ret;
 -		}
 -		WRITE_ONCE(msk->first_pending, mptcp_send_next(sk));
 +		return ssk;
  	}
  
 -out:
 -	/* __mptcp_alloc_tx_skb could have released some wmem and we are
 -	 * not going to flush it via release_sock()
 -	 */
 -	__mptcp_update_wmem(sk);
 -	if (copied) {
 -		mptcp_set_timeout(sk, ssk);
 -		tcp_push(ssk, 0, info.mss_now, tcp_sk(ssk)->nonagle,
 -			 info.size_goal);
 -		if (msk->snd_data_fin_enable &&
 -		    msk->snd_nxt + 1 == msk->write_seq)
 -			mptcp_schedule_work(sk);
 -	}
 +	return backup;
  }
  
 -static void mptcp_set_nospace(struct sock *sk)
 +static void ssk_check_wmem(struct mptcp_sock *msk)
  {
 -	/* enable autotune */
 -	set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
 -
 -	/* will be cleared on avail space */
 -	set_bit(MPTCP_NOSPACE, &mptcp_sk(sk)->flags);
 +	if (unlikely(!mptcp_is_writeable(msk)))
 +		mptcp_nospace(msk);
  }
  
  static int mptcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
* Unmerged path net/mptcp/protocol.c
