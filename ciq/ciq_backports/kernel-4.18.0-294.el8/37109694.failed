mm: migrate: remove unused mode argument

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Keith Busch <keith.busch@intel.com>
commit 371096949f0ad3950b06729989bd27de51b8c5f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/37109694.failed

migrate_page_move_mapping() doesn't use the mode argument.  Remove it
and update callers accordingly.

Link: http://lkml.kernel.org/r/20190508210301.8472-1-keith.busch@intel.com
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Reviewed-by: Zi Yan <ziy@nvidia.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 371096949f0ad3950b06729989bd27de51b8c5f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/aio.c
#	fs/f2fs/data.c
#	fs/iomap.c
#	fs/ubifs/file.c
#	include/linux/migrate.h
#	mm/migrate.c
diff --cc fs/aio.c
index 64d7c2f91509,8b3aa2739906..000000000000
--- a/fs/aio.c
+++ b/fs/aio.c
@@@ -394,7 -425,7 +394,11 @@@ static int aio_migratepage(struct addre
  	BUG_ON(PageWriteback(old));
  	get_page(new);
  
++<<<<<<< HEAD
 +	rc = migrate_page_move_mapping(mapping, new, old, NULL, mode, 1);
++=======
+ 	rc = migrate_page_move_mapping(mapping, new, old, 1);
++>>>>>>> 371096949f0a (mm: migrate: remove unused mode argument)
  	if (rc != MIGRATEPAGE_SUCCESS) {
  		put_page(new);
  		goto out_unlock;
diff --cc fs/f2fs/data.c
index 8f931d699287,abbf14e9bd72..000000000000
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@@ -2544,14 -2916,10 +2544,18 @@@ int f2fs_migrate_page(struct address_sp
  			return -EAGAIN;
  	}
  
 -	/* one extra reference was held for atomic_write page */
 -	extra_count = atomic_written ? 1 : 0;
 +	/*
 +	 * A reference is expected if PagePrivate set when move mapping,
 +	 * however F2FS breaks this for maintaining dirty page counts when
 +	 * truncating pages. So here adjusting the 'extra_count' make it work.
 +	 */
 +	extra_count = (atomic_written ? 1 : 0) - page_has_private(page);
  	rc = migrate_page_move_mapping(mapping, newpage,
++<<<<<<< HEAD
 +				page, NULL, mode, extra_count);
++=======
+ 				page, extra_count);
++>>>>>>> 371096949f0a (mm: migrate: remove unused mode argument)
  	if (rc != MIGRATEPAGE_SUCCESS) {
  		if (atomic_written)
  			mutex_unlock(&fi->inmem_lock);
diff --cc fs/ubifs/file.c
index 79fcb1ae51c6,400970d740bb..000000000000
--- a/fs/ubifs/file.c
+++ b/fs/ubifs/file.c
@@@ -1475,7 -1470,7 +1475,11 @@@ static int ubifs_migrate_page(struct ad
  {
  	int rc;
  
++<<<<<<< HEAD
 +	rc = migrate_page_move_mapping(mapping, newpage, page, NULL, mode, 0);
++=======
+ 	rc = migrate_page_move_mapping(mapping, newpage, page, 0);
++>>>>>>> 371096949f0a (mm: migrate: remove unused mode argument)
  	if (rc != MIGRATEPAGE_SUCCESS)
  		return rc;
  
diff --cc include/linux/migrate.h
index 938271a0b951,7f04754c7f2b..000000000000
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@@ -77,9 -77,7 +77,13 @@@ extern void migrate_page_copy(struct pa
  extern int migrate_huge_page_move_mapping(struct address_space *mapping,
  				  struct page *newpage, struct page *page);
  extern int migrate_page_move_mapping(struct address_space *mapping,
++<<<<<<< HEAD
 +		struct page *newpage, struct page *page,
 +		struct buffer_head *head, enum migrate_mode mode,
 +		int extra_count);
++=======
+ 		struct page *newpage, struct page *page, int extra_count);
++>>>>>>> 371096949f0a (mm: migrate: remove unused mode argument)
  #else
  
  static inline void putback_movable_pages(struct list_head *l) {}
diff --cc mm/migrate.c
index 5628f1102c6a,8992741f10aa..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -446,13 -394,11 +446,17 @@@ static int expected_page_refs(struct ad
   * 3 for pages with a mapping and PagePrivate/PagePrivate2 set.
   */
  int migrate_page_move_mapping(struct address_space *mapping,
++<<<<<<< HEAD
 +		struct page *newpage, struct page *page,
 +		struct buffer_head *head, enum migrate_mode mode,
 +		int extra_count)
++=======
+ 		struct page *newpage, struct page *page, int extra_count)
++>>>>>>> 371096949f0a (mm: migrate: remove unused mode argument)
  {
 -	XA_STATE(xas, &mapping->i_pages, page_index(page));
  	struct zone *oldzone, *newzone;
  	int dirty;
 +	void **pslot;
  	int expected_count = expected_page_refs(mapping, page) + extra_count;
  
  	if (!mapping) {
@@@ -761,7 -680,7 +765,11 @@@ int migrate_page(struct address_space *
  
  	BUG_ON(PageWriteback(page));	/* Writeback must be complete */
  
++<<<<<<< HEAD
 +	rc = migrate_page_move_mapping(mapping, newpage, page, NULL, mode, 0);
++=======
+ 	rc = migrate_page_move_mapping(mapping, newpage, page, 0);
++>>>>>>> 371096949f0a (mm: migrate: remove unused mode argument)
  
  	if (rc != MIGRATEPAGE_SUCCESS)
  		return rc;
@@@ -789,20 -743,45 +797,47 @@@ int buffer_migrate_page(struct address_
  	if (!page_has_buffers(page))
  		return migrate_page(mapping, newpage, page, mode);
  
 -	/* Check whether page does not have extra refs before we do more work */
 -	expected_count = expected_page_refs(mapping, page);
 -	if (page_count(page) != expected_count)
 -		return -EAGAIN;
 -
  	head = page_buffers(page);
 -	if (!buffer_migrate_lock_buffers(head, mode))
 -		return -EAGAIN;
  
 -	if (check_refs) {
 -		bool busy;
 -		bool invalidated = false;
 +	rc = migrate_page_move_mapping(mapping, newpage, page, head, mode, 0);
  
++<<<<<<< HEAD
++=======
+ recheck_buffers:
+ 		busy = false;
+ 		spin_lock(&mapping->private_lock);
+ 		bh = head;
+ 		do {
+ 			if (atomic_read(&bh->b_count)) {
+ 				busy = true;
+ 				break;
+ 			}
+ 			bh = bh->b_this_page;
+ 		} while (bh != head);
+ 		spin_unlock(&mapping->private_lock);
+ 		if (busy) {
+ 			if (invalidated) {
+ 				rc = -EAGAIN;
+ 				goto unlock_buffers;
+ 			}
+ 			invalidate_bh_lrus();
+ 			invalidated = true;
+ 			goto recheck_buffers;
+ 		}
+ 	}
+ 
+ 	rc = migrate_page_move_mapping(mapping, newpage, page, 0);
++>>>>>>> 371096949f0a (mm: migrate: remove unused mode argument)
  	if (rc != MIGRATEPAGE_SUCCESS)
 -		goto unlock_buffers;
 +		return rc;
 +
 +	/*
 +	 * In the async case, migrate_page_move_mapping locked the buffers
 +	 * with an IRQ-safe spinlock held. In the sync case, the buffers
 +	 * need to be locked now
 +	 */
 +	if (mode != MIGRATE_ASYNC)
 +		BUG_ON(!buffer_migrate_lock_buffers(head, mode));
  
  	ClearPagePrivate(page);
  	set_page_private(newpage, page_private(page));
* Unmerged path fs/iomap.c
* Unmerged path fs/aio.c
* Unmerged path fs/f2fs/data.c
* Unmerged path fs/iomap.c
* Unmerged path fs/ubifs/file.c
* Unmerged path include/linux/migrate.h
* Unmerged path mm/migrate.c
