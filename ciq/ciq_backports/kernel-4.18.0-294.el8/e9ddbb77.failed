bpf: Introduce SK_LOOKUP program type with a dedicated attach point

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Jakub Sitnicki <jakub@cloudflare.com>
commit e9ddbb7707ff5891616240026062b8c1e29864ca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/e9ddbb77.failed

Add a new program type BPF_PROG_TYPE_SK_LOOKUP with a dedicated attach type
BPF_SK_LOOKUP. The new program kind is to be invoked by the transport layer
when looking up a listening socket for a new connection request for
connection oriented protocols, or when looking up an unconnected socket for
a packet for connection-less protocols.

When called, SK_LOOKUP BPF program can select a socket that will receive
the packet. This serves as a mechanism to overcome the limits of what
bind() API allows to express. Two use-cases driving this work are:

 (1) steer packets destined to an IP range, on fixed port to a socket

     192.0.2.0/24, port 80 -> NGINX socket

 (2) steer packets destined to an IP address, on any port to a socket

     198.51.100.1, any port -> L7 proxy socket

In its run-time context program receives information about the packet that
triggered the socket lookup. Namely IP version, L4 protocol identifier, and
address 4-tuple. Context can be further extended to include ingress
interface identifier.

To select a socket BPF program fetches it from a map holding socket
references, like SOCKMAP or SOCKHASH, and calls bpf_sk_assign(ctx, sk, ...)
helper to record the selection. Transport layer then uses the selected
socket as a result of socket lookup.

In its basic form, SK_LOOKUP acts as a filter and hence must return either
SK_PASS or SK_DROP. If the program returns with SK_PASS, transport should
look for a socket to receive the packet, or use the one selected by the
program if available, while SK_DROP informs the transport layer that the
lookup should fail.

This patch only enables the user to attach an SK_LOOKUP program to a
network namespace. Subsequent patches hook it up to run on local delivery
path in ipv4 and ipv6 stacks.

	Suggested-by: Marek Majkowski <marek@cloudflare.com>
	Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200717103536.397595-3-jakub@cloudflare.com
(cherry picked from commit e9ddbb7707ff5891616240026062b8c1e29864ca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf-netns.h
#	include/uapi/linux/bpf.h
#	kernel/bpf/net_namespace.c
#	kernel/bpf/syscall.c
diff --cc include/uapi/linux/bpf.h
index f92a8fa8cc76,54d0c886e3ba..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -189,7 -189,7 +189,11 @@@ enum bpf_prog_type 
  	BPF_PROG_TYPE_STRUCT_OPS,
  	BPF_PROG_TYPE_EXT,
  	BPF_PROG_TYPE_LSM,
++<<<<<<< HEAD
 +#endif /* __GENKSYMS__ */
++=======
+ 	BPF_PROG_TYPE_SK_LOOKUP,
++>>>>>>> e9ddbb7707ff (bpf: Introduce SK_LOOKUP program type with a dedicated attach point)
  };
  
  enum bpf_attach_type {
@@@ -222,7 -221,15 +226,19 @@@
  	BPF_TRACE_FEXIT,
  	BPF_MODIFY_RETURN,
  	BPF_LSM_MAC,
++<<<<<<< HEAD
 +#endif /* __GENKSYMS__ */
++=======
+ 	BPF_TRACE_ITER,
+ 	BPF_CGROUP_INET4_GETPEERNAME,
+ 	BPF_CGROUP_INET6_GETPEERNAME,
+ 	BPF_CGROUP_INET4_GETSOCKNAME,
+ 	BPF_CGROUP_INET6_GETSOCKNAME,
+ 	BPF_XDP_DEVMAP,
+ 	BPF_CGROUP_INET_SOCK_RELEASE,
+ 	BPF_XDP_CPUMAP,
+ 	BPF_SK_LOOKUP,
++>>>>>>> e9ddbb7707ff (bpf: Introduce SK_LOOKUP program type with a dedicated attach point)
  	__MAX_BPF_ATTACH_TYPE
  };
  
@@@ -3054,8 -3069,12 +3070,12 @@@ union bpf_attr 
   * 	Return
   * 		The id is returned or 0 in case the id could not be retrieved.
   *
 - * long bpf_sk_assign(struct sk_buff *skb, struct bpf_sock *sk, u64 flags)
 + * int bpf_sk_assign(struct sk_buff *skb, struct bpf_sock *sk, u64 flags)
   *	Description
+  *		Helper is overloaded depending on BPF program type. This
+  *		description applies to **BPF_PROG_TYPE_SCHED_CLS** and
+  *		**BPF_PROG_TYPE_SCHED_ACT** programs.
+  *
   *		Assign the *sk* to the *skb*. When combined with appropriate
   *		routing configuration to receive the packet towards the socket,
   *		will cause *skb* to be delivered to the specified socket.
@@@ -3367,6 -3640,35 +3437,38 @@@ enum 
  	BPF_F_GET_BRANCH_RECORDS_SIZE	= (1ULL << 0),
  };
  
++<<<<<<< HEAD
++=======
+ /* BPF_FUNC_bpf_ringbuf_commit, BPF_FUNC_bpf_ringbuf_discard, and
+  * BPF_FUNC_bpf_ringbuf_output flags.
+  */
+ enum {
+ 	BPF_RB_NO_WAKEUP		= (1ULL << 0),
+ 	BPF_RB_FORCE_WAKEUP		= (1ULL << 1),
+ };
+ 
+ /* BPF_FUNC_bpf_ringbuf_query flags */
+ enum {
+ 	BPF_RB_AVAIL_DATA = 0,
+ 	BPF_RB_RING_SIZE = 1,
+ 	BPF_RB_CONS_POS = 2,
+ 	BPF_RB_PROD_POS = 3,
+ };
+ 
+ /* BPF ring buffer constants */
+ enum {
+ 	BPF_RINGBUF_BUSY_BIT		= (1U << 31),
+ 	BPF_RINGBUF_DISCARD_BIT		= (1U << 30),
+ 	BPF_RINGBUF_HDR_SZ		= 8,
+ };
+ 
+ /* BPF_FUNC_sk_assign flags in bpf_sk_lookup context. */
+ enum {
+ 	BPF_SK_LOOKUP_F_REPLACE		= (1ULL << 0),
+ 	BPF_SK_LOOKUP_F_NO_REUSEPORT	= (1ULL << 1),
+ };
+ 
++>>>>>>> e9ddbb7707ff (bpf: Introduce SK_LOOKUP program type with a dedicated attach point)
  /* Mode for BPF_FUNC_skb_adjust_room helper. */
  enum bpf_adj_room_mode {
  	BPF_ADJ_ROOM_NET,
diff --cc kernel/bpf/syscall.c
index 9186e511feb3,d07417d17712..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -2712,8 -2760,14 +2716,9 @@@ static int bpf_prog_attach_check_attach
  	case BPF_PROG_TYPE_CGROUP_SOCK:
  	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
  	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
+ 	case BPF_PROG_TYPE_SK_LOOKUP:
  		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
  	case BPF_PROG_TYPE_CGROUP_SKB:
 -		if (!capable(CAP_NET_ADMIN))
 -			/* cg-skb progs can be loaded by unpriv user.
 -			 * check permissions at attach time.
 -			 */
 -			return -EPERM;
  		return prog->enforce_expected_attach_type &&
  			prog->expected_attach_type != attach_type ?
  			-EINVAL : 0;
@@@ -2761,6 -2820,10 +2766,13 @@@ attach_type_to_prog_type(enum bpf_attac
  	case BPF_CGROUP_GETSOCKOPT:
  	case BPF_CGROUP_SETSOCKOPT:
  		return BPF_PROG_TYPE_CGROUP_SOCKOPT;
++<<<<<<< HEAD
++=======
+ 	case BPF_TRACE_ITER:
+ 		return BPF_PROG_TYPE_TRACING;
+ 	case BPF_SK_LOOKUP:
+ 		return BPF_PROG_TYPE_SK_LOOKUP;
++>>>>>>> e9ddbb7707ff (bpf: Introduce SK_LOOKUP program type with a dedicated attach point)
  	default:
  		return BPF_PROG_TYPE_UNSPEC;
  	}
@@@ -2898,7 -2960,8 +2910,12 @@@ static int bpf_prog_query(const union b
  	case BPF_LIRC_MODE2:
  		return lirc_prog_query(attr, uattr);
  	case BPF_FLOW_DISSECTOR:
++<<<<<<< HEAD
 +		return skb_flow_dissector_prog_query(attr, uattr);
++=======
+ 	case BPF_SK_LOOKUP:
+ 		return netns_bpf_prog_query(attr, uattr);
++>>>>>>> e9ddbb7707ff (bpf: Introduce SK_LOOKUP program type with a dedicated attach point)
  	default:
  		return -EINVAL;
  	}
@@@ -3830,6 -3895,13 +3847,16 @@@ static int link_create(union bpf_attr *
  	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
  		ret = cgroup_bpf_link_attach(attr, prog);
  		break;
++<<<<<<< HEAD
++=======
+ 	case BPF_PROG_TYPE_TRACING:
+ 		ret = tracing_bpf_link_attach(attr, prog);
+ 		break;
+ 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
+ 	case BPF_PROG_TYPE_SK_LOOKUP:
+ 		ret = netns_bpf_link_create(attr, prog);
+ 		break;
++>>>>>>> e9ddbb7707ff (bpf: Introduce SK_LOOKUP program type with a dedicated attach point)
  	default:
  		ret = -EINVAL;
  	}
* Unmerged path include/linux/bpf-netns.h
* Unmerged path kernel/bpf/net_namespace.c
* Unmerged path include/linux/bpf-netns.h
diff --git a/include/linux/bpf.h b/include/linux/bpf.h
index 531af1e44e56..78fae23046ef 100644
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -253,6 +253,7 @@ enum bpf_arg_type {
 	ARG_PTR_TO_INT,		/* pointer to int */
 	ARG_PTR_TO_LONG,	/* pointer to long */
 	ARG_PTR_TO_SOCKET,	/* pointer to bpf_sock (fullsock) */
+	ARG_PTR_TO_SOCKET_OR_NULL,	/* pointer to bpf_sock (fullsock) or NULL */
 	ARG_PTR_TO_BTF_ID,	/* pointer to in-kernel struct */
 };
 
diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index 8345cdf553b8..08c2aef674ac 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -64,6 +64,8 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LIRC_MODE2, lirc_mode2,
 #ifdef CONFIG_INET
 BPF_PROG_TYPE(BPF_PROG_TYPE_SK_REUSEPORT, sk_reuseport,
 	      struct sk_reuseport_md, struct sk_reuseport_kern)
+BPF_PROG_TYPE(BPF_PROG_TYPE_SK_LOOKUP, sk_lookup,
+	      struct bpf_sk_lookup, struct bpf_sk_lookup_kern)
 #endif
 #if defined(CONFIG_BPF_JIT)
 BPF_PROG_TYPE(BPF_PROG_TYPE_STRUCT_OPS, bpf_struct_ops,
diff --git a/include/linux/filter.h b/include/linux/filter.h
index dc3bd64e26c3..7211b7330a2d 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1287,4 +1287,21 @@ struct bpf_sockopt_kern {
 	s32		retval;
 };
 
+struct bpf_sk_lookup_kern {
+	u16		family;
+	u16		protocol;
+	struct {
+		__be32 saddr;
+		__be32 daddr;
+	} v4;
+	struct {
+		const struct in6_addr *saddr;
+		const struct in6_addr *daddr;
+	} v6;
+	__be16		sport;
+	u16		dport;
+	struct sock	*selected_sk;
+	bool		no_reuseport;
+};
+
 #endif /* __LINUX_FILTER_H__ */
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/bpf/net_namespace.c
* Unmerged path kernel/bpf/syscall.c
diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c
index b8aec186e303..4f6d695217a6 100644
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -3764,10 +3764,14 @@ static int check_func_arg(struct bpf_verifier_env *env, u32 regno,
 			}
 			meta->ref_obj_id = reg->ref_obj_id;
 		}
-	} else if (arg_type == ARG_PTR_TO_SOCKET) {
+	} else if (arg_type == ARG_PTR_TO_SOCKET ||
+		   arg_type == ARG_PTR_TO_SOCKET_OR_NULL) {
 		expected_type = PTR_TO_SOCKET;
-		if (type != expected_type)
-			goto err_type;
+		if (!(register_is_null(reg) &&
+		      arg_type == ARG_PTR_TO_SOCKET_OR_NULL)) {
+			if (type != expected_type)
+				goto err_type;
+		}
 	} else if (arg_type == ARG_PTR_TO_BTF_ID) {
 		expected_type = PTR_TO_BTF_ID;
 		if (type != expected_type)
@@ -7174,6 +7178,9 @@ static int check_return_code(struct bpf_verifier_env *env)
 			return -ENOTSUPP;
 		}
 		break;
+	case BPF_PROG_TYPE_SK_LOOKUP:
+		range = tnum_range(SK_DROP, SK_PASS);
+		break;
 	case BPF_PROG_TYPE_EXT:
 		/* freplace program can return anything as its return value
 		 * depends on the to-be-replaced kernel func or bpf program.
diff --git a/net/core/filter.c b/net/core/filter.c
index 76aa4a2037db..3128cb5f3867 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -9106,6 +9106,186 @@ const struct bpf_verifier_ops sk_reuseport_verifier_ops = {
 
 const struct bpf_prog_ops sk_reuseport_prog_ops = {
 };
+
+BPF_CALL_3(bpf_sk_lookup_assign, struct bpf_sk_lookup_kern *, ctx,
+	   struct sock *, sk, u64, flags)
+{
+	if (unlikely(flags & ~(BPF_SK_LOOKUP_F_REPLACE |
+			       BPF_SK_LOOKUP_F_NO_REUSEPORT)))
+		return -EINVAL;
+	if (unlikely(sk && sk_is_refcounted(sk)))
+		return -ESOCKTNOSUPPORT; /* reject non-RCU freed sockets */
+	if (unlikely(sk && sk->sk_state == TCP_ESTABLISHED))
+		return -ESOCKTNOSUPPORT; /* reject connected sockets */
+
+	/* Check if socket is suitable for packet L3/L4 protocol */
+	if (sk && sk->sk_protocol != ctx->protocol)
+		return -EPROTOTYPE;
+	if (sk && sk->sk_family != ctx->family &&
+	    (sk->sk_family == AF_INET || ipv6_only_sock(sk)))
+		return -EAFNOSUPPORT;
+
+	if (ctx->selected_sk && !(flags & BPF_SK_LOOKUP_F_REPLACE))
+		return -EEXIST;
+
+	/* Select socket as lookup result */
+	ctx->selected_sk = sk;
+	ctx->no_reuseport = flags & BPF_SK_LOOKUP_F_NO_REUSEPORT;
+	return 0;
+}
+
+static const struct bpf_func_proto bpf_sk_lookup_assign_proto = {
+	.func		= bpf_sk_lookup_assign,
+	.gpl_only	= false,
+	.ret_type	= RET_INTEGER,
+	.arg1_type	= ARG_PTR_TO_CTX,
+	.arg2_type	= ARG_PTR_TO_SOCKET_OR_NULL,
+	.arg3_type	= ARG_ANYTHING,
+};
+
+static const struct bpf_func_proto *
+sk_lookup_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
+{
+	switch (func_id) {
+	case BPF_FUNC_perf_event_output:
+		return &bpf_event_output_data_proto;
+	case BPF_FUNC_sk_assign:
+		return &bpf_sk_lookup_assign_proto;
+	case BPF_FUNC_sk_release:
+		return &bpf_sk_release_proto;
+	default:
+		return bpf_base_func_proto(func_id);
+	}
+}
+
+static bool sk_lookup_is_valid_access(int off, int size,
+				      enum bpf_access_type type,
+				      const struct bpf_prog *prog,
+				      struct bpf_insn_access_aux *info)
+{
+	if (off < 0 || off >= sizeof(struct bpf_sk_lookup))
+		return false;
+	if (off % size != 0)
+		return false;
+	if (type != BPF_READ)
+		return false;
+
+	switch (off) {
+	case offsetof(struct bpf_sk_lookup, sk):
+		info->reg_type = PTR_TO_SOCKET_OR_NULL;
+		return size == sizeof(__u64);
+
+	case bpf_ctx_range(struct bpf_sk_lookup, family):
+	case bpf_ctx_range(struct bpf_sk_lookup, protocol):
+	case bpf_ctx_range(struct bpf_sk_lookup, remote_ip4):
+	case bpf_ctx_range(struct bpf_sk_lookup, local_ip4):
+	case bpf_ctx_range_till(struct bpf_sk_lookup, remote_ip6[0], remote_ip6[3]):
+	case bpf_ctx_range_till(struct bpf_sk_lookup, local_ip6[0], local_ip6[3]):
+	case bpf_ctx_range(struct bpf_sk_lookup, remote_port):
+	case bpf_ctx_range(struct bpf_sk_lookup, local_port):
+		bpf_ctx_record_field_size(info, sizeof(__u32));
+		return bpf_ctx_narrow_access_ok(off, size, sizeof(__u32));
+
+	default:
+		return false;
+	}
+}
+
+static u32 sk_lookup_convert_ctx_access(enum bpf_access_type type,
+					const struct bpf_insn *si,
+					struct bpf_insn *insn_buf,
+					struct bpf_prog *prog,
+					u32 *target_size)
+{
+	struct bpf_insn *insn = insn_buf;
+
+	switch (si->off) {
+	case offsetof(struct bpf_sk_lookup, sk):
+		*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg, si->src_reg,
+				      offsetof(struct bpf_sk_lookup_kern, selected_sk));
+		break;
+
+	case offsetof(struct bpf_sk_lookup, family):
+		*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,
+				      bpf_target_off(struct bpf_sk_lookup_kern,
+						     family, 2, target_size));
+		break;
+
+	case offsetof(struct bpf_sk_lookup, protocol):
+		*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,
+				      bpf_target_off(struct bpf_sk_lookup_kern,
+						     protocol, 2, target_size));
+		break;
+
+	case offsetof(struct bpf_sk_lookup, remote_ip4):
+		*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,
+				      bpf_target_off(struct bpf_sk_lookup_kern,
+						     v4.saddr, 4, target_size));
+		break;
+
+	case offsetof(struct bpf_sk_lookup, local_ip4):
+		*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,
+				      bpf_target_off(struct bpf_sk_lookup_kern,
+						     v4.daddr, 4, target_size));
+		break;
+
+	case bpf_ctx_range_till(struct bpf_sk_lookup,
+				remote_ip6[0], remote_ip6[3]): {
+#if IS_ENABLED(CONFIG_IPV6)
+		int off = si->off;
+
+		off -= offsetof(struct bpf_sk_lookup, remote_ip6[0]);
+		off += bpf_target_off(struct in6_addr, s6_addr32[0], 4, target_size);
+		*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg, si->src_reg,
+				      offsetof(struct bpf_sk_lookup_kern, v6.saddr));
+		*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);
+		*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg, off);
+#else
+		*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);
+#endif
+		break;
+	}
+	case bpf_ctx_range_till(struct bpf_sk_lookup,
+				local_ip6[0], local_ip6[3]): {
+#if IS_ENABLED(CONFIG_IPV6)
+		int off = si->off;
+
+		off -= offsetof(struct bpf_sk_lookup, local_ip6[0]);
+		off += bpf_target_off(struct in6_addr, s6_addr32[0], 4, target_size);
+		*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg, si->src_reg,
+				      offsetof(struct bpf_sk_lookup_kern, v6.daddr));
+		*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);
+		*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg, off);
+#else
+		*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);
+#endif
+		break;
+	}
+	case offsetof(struct bpf_sk_lookup, remote_port):
+		*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,
+				      bpf_target_off(struct bpf_sk_lookup_kern,
+						     sport, 2, target_size));
+		break;
+
+	case offsetof(struct bpf_sk_lookup, local_port):
+		*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,
+				      bpf_target_off(struct bpf_sk_lookup_kern,
+						     dport, 2, target_size));
+		break;
+	}
+
+	return insn - insn_buf;
+}
+
+const struct bpf_prog_ops sk_lookup_prog_ops = {
+};
+
+const struct bpf_verifier_ops sk_lookup_verifier_ops = {
+	.get_func_proto		= sk_lookup_func_proto,
+	.is_valid_access	= sk_lookup_is_valid_access,
+	.convert_ctx_access	= sk_lookup_convert_ctx_access,
+};
+
 #endif /* CONFIG_INET */
 
 DEFINE_BPF_DISPATCHER(xdp)
diff --git a/scripts/bpf_helpers_doc.py b/scripts/bpf_helpers_doc.py
index 91fa668fa860..c6758f89e55f 100755
--- a/scripts/bpf_helpers_doc.py
+++ b/scripts/bpf_helpers_doc.py
@@ -404,6 +404,7 @@ class PrinterHelpers(Printer):
 
     type_fwds = [
             'struct bpf_fib_lookup',
+            'struct bpf_sk_lookup',
             'struct bpf_perf_event_data',
             'struct bpf_perf_event_value',
             'struct bpf_pidns_info',
@@ -444,6 +445,7 @@ class PrinterHelpers(Printer):
             'struct bpf_perf_event_data',
             'struct bpf_perf_event_value',
             'struct bpf_pidns_info',
+            'struct bpf_sk_lookup',
             'struct bpf_sock',
             'struct bpf_sock_addr',
             'struct bpf_sock_ops',
@@ -475,6 +477,11 @@ class PrinterHelpers(Printer):
             'struct sk_msg_buff': 'struct sk_msg_md',
             'struct xdp_buff': 'struct xdp_md',
     }
+    # Helpers overloaded for different context types.
+    overloaded_helpers = [
+        'bpf_get_socket_cookie',
+        'bpf_sk_assign',
+    ]
 
     def print_header(self):
         header = '''\
@@ -531,7 +538,7 @@ class PrinterHelpers(Printer):
         for i, a in enumerate(proto['args']):
             t = a['type']
             n = a['name']
-            if proto['name'] == 'bpf_get_socket_cookie' and i == 0:
+            if proto['name'] in self.overloaded_helpers and i == 0:
                     t = 'void'
                     n = 'ctx'
             one_arg = '{}{}'.format(comma, self.map_type(t))
