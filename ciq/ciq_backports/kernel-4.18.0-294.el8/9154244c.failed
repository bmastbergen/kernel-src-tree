rcu/tree: Add a shrinker to prevent OOM due to kfree_rcu() batching

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Joel Fernandes (Google) <joel@joelfernandes.org>
commit 9154244c1ab6c9db4f1f25ac8f73bd46dba64287
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/9154244c.failed

To reduce grace periods and improve kfree() performance, we have done
batching recently dramatically bringing down the number of grace periods
while giving us the ability to use kfree_bulk() for efficient kfree'ing.

However, this has increased the likelihood of OOM condition under heavy
kfree_rcu() flood on small memory systems. This patch introduces a
shrinker which starts grace periods right away if the system is under
memory pressure due to existence of objects that have still not started
a grace period.

With this patch, I do not observe an OOM anymore on a system with 512MB
RAM and 8 CPUs, with the following rcuperf options:

rcuperf.kfree_loops=20000 rcuperf.kfree_alloc_num=8000
rcuperf.kfree_rcu_test=1 rcuperf.kfree_mult=2

Otherwise it easily OOMs with the above parameters.

NOTE:
1. On systems with no memory pressure, the patch has no effect as intended.
2. In the future, we can use this same mechanism to prevent grace periods
   from happening even more, by relying on shrinkers carefully.

	Cc: urezki@gmail.com
	Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
(cherry picked from commit 9154244c1ab6c9db4f1f25ac8f73bd46dba64287)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index 4aa7b6bbcde6,e299cd0ddd97..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -2757,26 -2909,50 +2759,64 @@@ static void kfree_rcu_work(struct work_
   */
  static inline bool queue_kfree_rcu_work(struct kfree_rcu_cpu *krcp)
  {
 -	struct kfree_rcu_cpu_work *krwp;
 -	bool queued = false;
  	int i;
 +	struct kfree_rcu_cpu_work *krwp = NULL;
  
  	lockdep_assert_held(&krcp->lock);
++<<<<<<< HEAD
 +	for (i = 0; i < KFREE_N_BATCHES; i++)
 +		if (!krcp->krw_arr[i].head_free) {
 +			krwp = &(krcp->krw_arr[i]);
 +			break;
++=======
+ 
+ 	for (i = 0; i < KFREE_N_BATCHES; i++) {
+ 		krwp = &(krcp->krw_arr[i]);
+ 
+ 		/*
+ 		 * Try to detach bhead or head and attach it over any
+ 		 * available corresponding free channel. It can be that
+ 		 * a previous RCU batch is in progress, it means that
+ 		 * immediately to queue another one is not possible so
+ 		 * return false to tell caller to retry.
+ 		 */
+ 		if ((krcp->bhead && !krwp->bhead_free) ||
+ 				(krcp->head && !krwp->head_free)) {
+ 			/* Channel 1. */
+ 			if (!krwp->bhead_free) {
+ 				krwp->bhead_free = krcp->bhead;
+ 				krcp->bhead = NULL;
+ 			}
+ 
+ 			/* Channel 2. */
+ 			if (!krwp->head_free) {
+ 				krwp->head_free = krcp->head;
+ 				krcp->head = NULL;
+ 			}
+ 
+ 			krcp->count = 0;
+ 
+ 			/*
+ 			 * One work is per one batch, so there are two "free channels",
+ 			 * "bhead_free" and "head_free" the batch can handle. It can be
+ 			 * that the work is in the pending state when two channels have
+ 			 * been detached following each other, one by one.
+ 			 */
+ 			queue_rcu_work(system_wq, &krwp->rcu_work);
+ 			queued = true;
++>>>>>>> 9154244c1ab6 (rcu/tree: Add a shrinker to prevent OOM due to kfree_rcu() batching)
  		}
 -	}
  
 -	return queued;
 +	// If a previous RCU batch is in progress, we cannot immediately
 +	// queue another one, so return false to tell caller to retry.
 +	if (!krwp)
 +		return false;
 +
 +	krwp->head_free = krcp->head;
 +	krcp->head = NULL;
 +	INIT_RCU_WORK(&krwp->rcu_work, kfree_rcu_work);
 +	queue_rcu_work(system_wq, &krwp->rcu_work);
 +	return true;
  }
  
  static inline void kfree_rcu_drain_unlock(struct kfree_rcu_cpu *krcp,
@@@ -2856,10 -3066,19 +2896,12 @@@ void kfree_call_rcu(struct rcu_head *he
  			  __func__, head);
  		goto unlock_return;
  	}
 -
 -	/*
 -	 * Under high memory pressure GFP_NOWAIT can fail,
 -	 * in that case the emergency path is maintained.
 -	 */
 -	if (unlikely(!kfree_call_rcu_add_ptr_to_bulk(krcp, head, func))) {
 -		head->func = func;
 -		head->next = krcp->head;
 -		krcp->head = head;
 -	}
 +	head->func = func;
 +	head->next = krcp->head;
 +	krcp->head = head;
  
+ 	krcp->count++;
+ 
  	// Set timer to drain after KFREE_DRAIN_JIFFIES.
  	if (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&
  	    !krcp->monitor_todo) {
* Unmerged path kernel/rcu/tree.c
