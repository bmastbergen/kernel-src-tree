mm: shrinker: make shrinker not depend on memcg kmem

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yang Shi <yang.shi@linux.alibaba.com>
commit 0a432dcbeb32edcd211a5d8f7847d0da7642a8b4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/0a432dcb.failed

Currently shrinker is just allocated and can work when memcg kmem is
enabled.  But, THP deferred split shrinker is not slab shrinker, it
doesn't make too much sense to have such shrinker depend on memcg kmem.
It should be able to reclaim THP even though memcg kmem is disabled.

Introduce a new shrinker flag, SHRINKER_NONSLAB, for non-slab shrinker.
When memcg kmem is disabled, just such shrinkers can be called in
shrinking memcg slab.

[yang.shi@linux.alibaba.com: add comment]
  Link: http://lkml.kernel.org/r/1566496227-84952-4-git-send-email-yang.shi@linux.alibaba.com
Link: http://lkml.kernel.org/r/1565144277-36240-4-git-send-email-yang.shi@linux.alibaba.com
	Signed-off-by: Yang Shi <yang.shi@linux.alibaba.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Reviewed-by: Kirill Tkhai <ktkhai@virtuozzo.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Shakeel Butt <shakeelb@google.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Qian Cai <cai@lca.pw>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0a432dcbeb32edcd211a5d8f7847d0da7642a8b4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	include/linux/shrinker.h
#	mm/vmscan.c
diff --cc include/linux/memcontrol.h
index b3eb17a7bec8,a3c0a639c824..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -131,11 -126,10 +131,16 @@@ struct mem_cgroup_per_node 
  
  	unsigned long		lru_zone_size[MAX_NR_ZONES][NR_LRU_LISTS];
  
 -	struct mem_cgroup_reclaim_iter	iter[DEF_PRIORITY + 1];
 +	RH_KABI_REPLACE_SPLIT(struct mem_cgroup_reclaim_iter iter[DEF_PRIORITY + 1],
 +			      struct mem_cgroup_reclaim_iter iter,
 +			      /* Legacy local VM stats */
 +			      struct lruvec_stat __percpu *lruvec_stat_local)
  
++<<<<<<< HEAD
++=======
+ 	struct memcg_shrinker_map __rcu	*shrinker_map;
+ 
++>>>>>>> 0a432dcbeb32 (mm: shrinker: make shrinker not depend on memcg kmem)
  	struct rb_node		tree_node;	/* RB tree node */
  	unsigned long		usage_in_excess;/* Set to the value by which */
  						/* the soft limit is exceeded*/
@@@ -1402,16 -1399,9 +1417,19 @@@ static inline int memcg_cache_id(struc
  	return memcg ? memcg->kmemcg_id : -1;
  }
  
++<<<<<<< HEAD
 +extern int memcg_expand_shrinker_maps(int new_id);
 +
 +extern void memcg_set_shrinker_bit(struct mem_cgroup *memcg,
 +				   int nid, int shrinker_id);
 +struct mem_cgroup *mem_cgroup_from_obj(void *p);
 +
++=======
++>>>>>>> 0a432dcbeb32 (mm: shrinker: make shrinker not depend on memcg kmem)
  #else
  
 -static inline int memcg_kmem_charge(struct page *page, gfp_t gfp, int order)
 +static inline int memcg_kmem_charge_page(struct page *page, gfp_t gfp,
 +					 int order)
  {
  	return 0;
  }
@@@ -1451,14 -1440,6 +1469,17 @@@ static inline void memcg_put_cache_ids(
  {
  }
  
++<<<<<<< HEAD
 +static inline void memcg_set_shrinker_bit(struct mem_cgroup *memcg,
 +					  int nid, int shrinker_id) { }
 +
 +static inline struct mem_cgroup *mem_cgroup_from_obj(void *p)
 +{
 +       return NULL;
 +}
 +
++=======
++>>>>>>> 0a432dcbeb32 (mm: shrinker: make shrinker not depend on memcg kmem)
  #endif /* CONFIG_MEMCG_KMEM */
  
  #endif /* _LINUX_MEMCONTROL_H */
diff --cc include/linux/shrinker.h
index 9b381a5da9be,0f80123650e2..000000000000
--- a/include/linux/shrinker.h
+++ b/include/linux/shrinker.h
@@@ -71,6 -69,10 +71,13 @@@ struct shrinker 
  
  	/* These are for internal use */
  	struct list_head list;
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_MEMCG
+ 	/* ID in shrinker_idr */
+ 	int id;
+ #endif
++>>>>>>> 0a432dcbeb32 (mm: shrinker: make shrinker not depend on memcg kmem)
  	/* objs pending delete, per node */
  	atomic_long_t *nr_deferred;
  };
diff --cc mm/vmscan.c
index f901bd0da864,4911754c93b7..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -227,18 -238,7 +238,16 @@@ static void unregister_memcg_shrinker(s
  	idr_remove(&shrinker_idr, id);
  	up_write(&shrinker_rwsem);
  }
- #else /* CONFIG_MEMCG_KMEM */
- static int prealloc_memcg_shrinker(struct shrinker *shrinker)
- {
- 	return 0;
- }
  
++<<<<<<< HEAD
 +static void unregister_memcg_shrinker(struct shrinker *shrinker)
 +{
 +}
 +#endif /* CONFIG_MEMCG_KMEM */
 +
 +#ifdef CONFIG_MEMCG
++=======
++>>>>>>> 0a432dcbeb32 (mm: shrinker: make shrinker not depend on memcg kmem)
  static bool global_reclaim(struct scan_control *sc)
  {
  	return !sc->target_mem_cgroup;
* Unmerged path include/linux/memcontrol.h
* Unmerged path include/linux/shrinker.h
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 48fa67e8a272..e43240a17ce9 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -313,6 +313,7 @@ DEFINE_STATIC_KEY_FALSE(memcg_kmem_enabled_key);
 EXPORT_SYMBOL(memcg_kmem_enabled_key);
 
 struct workqueue_struct *memcg_kmem_cache_wq;
+#endif
 
 static int memcg_shrinker_map_size;
 static DEFINE_MUTEX(memcg_shrinker_map_mutex);
@@ -438,14 +439,6 @@ void memcg_set_shrinker_bit(struct mem_cgroup *memcg, int nid, int shrinker_id)
 	}
 }
 
-#else /* CONFIG_MEMCG_KMEM */
-static int memcg_alloc_shrinker_maps(struct mem_cgroup *memcg)
-{
-	return 0;
-}
-static void memcg_free_shrinker_maps(struct mem_cgroup *memcg) { }
-#endif /* CONFIG_MEMCG_KMEM */
-
 /**
  * mem_cgroup_css_from_page - css of the memcg associated with a page
  * @page: page of interest
* Unmerged path mm/vmscan.c
