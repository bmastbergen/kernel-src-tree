mm/khugepaged: minor reorderings in collapse_shmem()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Hugh Dickins <hughd@google.com>
commit 042a30824871fa3149b0127009074b75cc25863c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/042a3082.failed

Several cleanups in collapse_shmem(): most of which probably do not
really matter, beyond doing things in a more familiar and reassuring
order.  Simplify the failure gotos in the main loop, and on success
update stats while interrupts still disabled from the last iteration.

Link: http://lkml.kernel.org/r/alpine.LSU.2.11.1811261526400.2275@eggly.anvils
Fixes: f3f0e1d2150b2 ("khugepaged: add support of collapse for tmpfs/shmem pages")
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: <stable@vger.kernel.org>	[4.8+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 042a30824871fa3149b0127009074b75cc25863c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
diff --cc mm/khugepaged.c
index 180728fbfcba,9d4e9ff1af95..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1333,59 -1329,70 +1333,95 @@@ static void collapse_shmem(struct mm_st
  		goto out;
  	}
  
+ 	__SetPageLocked(new_page);
+ 	__SetPageSwapBacked(new_page);
  	new_page->index = start;
  	new_page->mapping = mapping;
- 	__SetPageSwapBacked(new_page);
- 	__SetPageLocked(new_page);
  	BUG_ON(!page_ref_freeze(new_page, 1));
  
 +
  	/*
 -	 * At this point the new_page is 'frozen' (page_count() is zero),
 -	 * locked and not up-to-date. It's safe to insert it into the page
 -	 * cache, because nobody would be able to map it or use it in other
 -	 * way until we unfreeze it.
 +	 * At this point the new_page is 'frozen' (page_count() is zero), locked
 +	 * and not up-to-date. It's safe to insert it into radix tree, because
 +	 * nobody would be able to map it or use it in other way until we
 +	 * unfreeze it.
  	 */
  
 -	/* This will be less messy when we use multi-index entries */
 -	do {
 -		xas_lock_irq(&xas);
 -		xas_create_range(&xas);
 -		if (!xas_error(&xas))
 +	index = start;
 +	xa_lock_irq(&mapping->i_pages);
 +	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start) {
 +		int n = min(iter.index, end) - index;
 +
 +		/*
 +		 * Handle holes in the radix tree: charge it from shmem and
 +		 * insert relevant subpage of new_page into the radix-tree.
 +		 */
 +		if (n && !shmem_charge(mapping->host, n)) {
 +			result = SCAN_FAIL;
  			break;
++<<<<<<< HEAD
 +		}
 +		nr_none += n;
 +		for (; index < min(iter.index, end); index++) {
 +			radix_tree_insert(&mapping->i_pages, index,
 +					new_page + (index % HPAGE_PMD_NR));
++=======
+ 		xas_unlock_irq(&xas);
+ 		if (!xas_nomem(&xas, GFP_KERNEL))
+ 			goto out;
+ 	} while (1);
+ 
+ 	xas_set(&xas, start);
+ 	for (index = start; index < end; index++) {
+ 		struct page *page = xas_next(&xas);
+ 
+ 		VM_BUG_ON(index != xas.xa_index);
+ 		if (!page) {
+ 			/*
+ 			 * Stop if extent has been truncated or hole-punched,
+ 			 * and is now completely empty.
+ 			 */
+ 			if (index == start) {
+ 				if (!xas_next_entry(&xas, end - 1)) {
+ 					result = SCAN_TRUNCATED;
+ 					goto xa_locked;
+ 				}
+ 				xas_set(&xas, index);
+ 			}
+ 			if (!shmem_charge(mapping->host, 1)) {
+ 				result = SCAN_FAIL;
+ 				goto xa_locked;
+ 			}
+ 			xas_store(&xas, new_page + (index % HPAGE_PMD_NR));
+ 			nr_none++;
+ 			continue;
++>>>>>>> 042a30824871 (mm/khugepaged: minor reorderings in collapse_shmem())
  		}
  
 +		/* We are done. */
 +		if (index >= end)
 +			break;
 +
 +		page = radix_tree_deref_slot_protected(slot,
 +				&mapping->i_pages.xa_lock);
  		if (xa_is_value(page) || !PageUptodate(page)) {
 -			xas_unlock_irq(&xas);
 +			xa_unlock_irq(&mapping->i_pages);
  			/* swap in or instantiate fallocated page */
  			if (shmem_getpage(mapping->host, index, &page,
  						SGP_NOHUGE)) {
  				result = SCAN_FAIL;
 -				goto xa_unlocked;
 +				goto tree_unlocked;
  			}
++<<<<<<< HEAD
 +			xa_lock_irq(&mapping->i_pages);
++=======
++>>>>>>> 042a30824871 (mm/khugepaged: minor reorderings in collapse_shmem())
  		} else if (trylock_page(page)) {
  			get_page(page);
+ 			xas_unlock_irq(&xas);
  		} else {
  			result = SCAN_PAGE_LOCK;
- 			break;
+ 			goto xa_locked;
  		}
  
  		/*
@@@ -1400,7 -1407,6 +1436,10 @@@
  			result = SCAN_TRUNCATED;
  			goto out_unlock;
  		}
++<<<<<<< HEAD
 +		xa_unlock_irq(&mapping->i_pages);
++=======
++>>>>>>> 042a30824871 (mm/khugepaged: minor reorderings in collapse_shmem())
  
  		if (isolate_lru_page(page)) {
  			result = SCAN_DEL_PAGE_LRU;
@@@ -1435,56 -1442,32 +1476,76 @@@
  		list_add_tail(&page->lru, &pagelist);
  
  		/* Finally, replace with the new page. */
 -		xas_store(&xas, new_page + (index % HPAGE_PMD_NR));
 +		radix_tree_replace_slot(&mapping->i_pages, slot,
 +				new_page + (index % HPAGE_PMD_NR));
 +
 +		slot = radix_tree_iter_resume(slot, &iter);
 +		index++;
  		continue;
++<<<<<<< HEAD
 +out_lru:
 +		xa_unlock_irq(&mapping->i_pages);
 +		putback_lru_page(page);
 +out_isolate_failed:
 +		unlock_page(page);
 +		put_page(page);
 +		goto tree_unlocked;
++=======
++>>>>>>> 042a30824871 (mm/khugepaged: minor reorderings in collapse_shmem())
  out_unlock:
  		unlock_page(page);
  		put_page(page);
- 		break;
+ 		goto xa_unlocked;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Handle hole in radix tree at the end of the range.
 +	 * This code only triggers if there's nothing in radix tree
 +	 * beyond 'end'.
 +	 */
 +	if (result == SCAN_SUCCEED && index < end) {
 +		int n = end - index;
 +
 +		if (!shmem_charge(mapping->host, n)) {
 +			result = SCAN_FAIL;
 +			goto tree_locked;
 +		}
 +
 +		for (; index < end; index++) {
 +			radix_tree_insert(&mapping->i_pages, index,
 +					new_page + (index % HPAGE_PMD_NR));
 +		}
 +		nr_none += n;
 +	}
 +
 +tree_locked:
 +	xa_unlock_irq(&mapping->i_pages);
 +tree_unlocked:
 +
 +	if (result == SCAN_SUCCEED) {
 +		unsigned long flags;
 +		struct zone *zone = page_zone(new_page);
++=======
+ 	__inc_node_page_state(new_page, NR_SHMEM_THPS);
+ 	if (nr_none) {
+ 		struct zone *zone = page_zone(new_page);
+ 
+ 		__mod_node_page_state(zone->zone_pgdat, NR_FILE_PAGES, nr_none);
+ 		__mod_node_page_state(zone->zone_pgdat, NR_SHMEM, nr_none);
+ 	}
+ 
+ xa_locked:
+ 	xas_unlock_irq(&xas);
+ xa_unlocked:
+ 
+ 	if (result == SCAN_SUCCEED) {
+ 		struct page *page, *tmp;
++>>>>>>> 042a30824871 (mm/khugepaged: minor reorderings in collapse_shmem())
  
  		/*
 -		 * Replacing old pages with new one has succeeded, now we
 -		 * need to copy the content and free the old pages.
 +		 * Replacing old pages with new one has succeed, now we need to
 +		 * copy the content and free old pages.
  		 */
  		index = start;
  		list_for_each_entry_safe(page, tmp, &pagelist, lru) {
@@@ -1508,29 -1491,17 +1569,35 @@@
  			index++;
  		}
  
++<<<<<<< HEAD
 +		local_irq_save(flags);
 +		__inc_node_page_state(new_page, NR_SHMEM_THPS);
 +		if (nr_none) {
 +			__mod_node_page_state(zone->zone_pgdat, NR_FILE_PAGES, nr_none);
 +			__mod_node_page_state(zone->zone_pgdat, NR_SHMEM, nr_none);
 +		}
 +		local_irq_restore(flags);
 +
 +		/*
 +		 * Remove pte page tables, so we can re-faulti
 +		 * the page as huge.
 +		 */
 +		retract_page_tables(mapping, start);
 +
++=======
++>>>>>>> 042a30824871 (mm/khugepaged: minor reorderings in collapse_shmem())
  		/* Everything is ready, let's unfreeze the new_page */
- 		set_page_dirty(new_page);
  		SetPageUptodate(new_page);
  		page_ref_unfreeze(new_page, HPAGE_PMD_NR);
+ 		set_page_dirty(new_page);
  		mem_cgroup_commit_charge(new_page, memcg, false, true);
 +		count_memcg_events(memcg, THP_COLLAPSE_ALLOC, 1);
  		lru_cache_add_anon(new_page);
- 		unlock_page(new_page);
  
+ 		/*
+ 		 * Remove pte page tables, so we can re-fault the page as huge.
+ 		 */
+ 		retract_page_tables(mapping, start);
  		*hpage = NULL;
  
  		khugepaged_pages_collapsed++;
@@@ -1558,15 -1531,15 +1625,24 @@@
  			/* Unfreeze the page. */
  			list_del(&page->lru);
  			page_ref_unfreeze(page, 2);
++<<<<<<< HEAD
 +			radix_tree_replace_slot(&mapping->i_pages, slot, page);
 +			slot = radix_tree_iter_resume(slot, &iter);
 +			xa_unlock_irq(&mapping->i_pages);
 +			putback_lru_page(page);
 +			unlock_page(page);
 +			xa_lock_irq(&mapping->i_pages);
++=======
+ 			xas_store(&xas, page);
+ 			xas_pause(&xas);
+ 			xas_unlock_irq(&xas);
+ 			unlock_page(page);
+ 			putback_lru_page(page);
+ 			xas_lock_irq(&xas);
++>>>>>>> 042a30824871 (mm/khugepaged: minor reorderings in collapse_shmem())
  		}
  		VM_BUG_ON(nr_none);
 -		xas_unlock_irq(&xas);
 +		xa_unlock_irq(&mapping->i_pages);
  
  		/* Unfreeze new_page, caller would take care about freeing it */
  		page_ref_unfreeze(new_page, 1);
* Unmerged path mm/khugepaged.c
