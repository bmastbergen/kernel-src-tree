KVM: arm64: Use common KVM implementation of MMU memory caches

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit c1a33aebe91d49c958df1648b2a84db96c403075
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/c1a33aeb.failed

Move to the common MMU memory cache implementation now that the common
code and arm64's existing code are semantically compatible.

No functional change intended.

	Cc: Marc Zyngier <maz@kernel.org>
	Suggested-by: Christoffer Dall <christoffer.dall@arm.com>
	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Message-Id: <20200703023545.8771-19-sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c1a33aebe91d49c958df1648b2a84db96c403075)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/Kbuild
#	arch/arm64/include/asm/kvm_host.h
#	arch/arm64/kvm/mmu.c
diff --cc arch/arm64/include/asm/Kbuild
index d4021a199e12,ff9cbb631212..000000000000
--- a/arch/arm64/include/asm/Kbuild
+++ b/arch/arm64/include/asm/Kbuild
@@@ -1,29 -1,8 +1,32 @@@
 -# SPDX-License-Identifier: GPL-2.0
 +generic-y += bugs.h
 +generic-y += delay.h
 +generic-y += div64.h
 +generic-y += dma.h
 +generic-y += dma-contiguous.h
  generic-y += early_ioremap.h
++<<<<<<< HEAD
 +generic-y += emergency-restart.h
 +generic-y += hw_irq.h
 +generic-y += irq_regs.h
 +generic-y += kdebug.h
 +generic-y += kmap_types.h
 +generic-y += local.h
++=======
++>>>>>>> c1a33aebe91d (KVM: arm64: Use common KVM implementation of MMU memory caches)
  generic-y += local64.h
  generic-y += mcs_spinlock.h
 +generic-y += mm-arch-hooks.h
 +generic-y += mmiowb.h
 +generic-y += msi.h
  generic-y += qrwlock.h
  generic-y += qspinlock.h
 +generic-y += segment.h
 +generic-y += serial.h
  generic-y += set_memory.h
 +generic-y += sizes.h
 +generic-y += switch_to.h
 +generic-y += trace_clock.h
 +generic-y += unaligned.h
  generic-y += user.h
 +generic-y += vga.h
 +generic-y += xor.h
diff --cc arch/arm64/include/asm/kvm_host.h
index 9d7ed798fdbc,23d1f41548f5..000000000000
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@@ -108,17 -97,6 +108,20 @@@ struct kvm_arch 
  	bool return_nisv_io_abort_to_user;
  };
  
++<<<<<<< HEAD
 +#define KVM_NR_MEM_OBJS     40
 +
 +/*
 + * We don't want allocation failures within the mmu code, so we preallocate
 + * enough memory for a single page fault in a cache.
 + */
 +struct kvm_mmu_memory_cache {
 +	int nobjs;
 +	void *objects[KVM_NR_MEM_OBJS];
 +};
 +
++=======
++>>>>>>> c1a33aebe91d (KVM: arm64: Use common KVM implementation of MMU memory caches)
  struct kvm_vcpu_fault_info {
  	u32 esr_el2;		/* Hyp Syndrom Register */
  	u64 far_el2;		/* Hyp Fault Address Register */
diff --cc arch/arm64/kvm/mmu.c
index e122989f9bf3,ba66e9a9bd3c..000000000000
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@@ -136,45 -124,22 +136,48 @@@ static void stage2_dissolve_pud(struct 
  	put_page(virt_to_page(pudp));
  }
  
 -static void clear_stage2_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)
++<<<<<<< HEAD
 +static int mmu_topup_memory_cache(struct kvm_mmu_memory_cache *cache,
 +				  int min, int max)
  {
 -	p4d_t *p4d_table __maybe_unused = stage2_p4d_offset(kvm, pgd, 0UL);
 -	stage2_pgd_clear(kvm, pgd);
 -	kvm_tlb_flush_vmid_ipa(kvm, addr);
 -	stage2_p4d_free(kvm, p4d_table);
 -	put_page(virt_to_page(pgd));
 +	void *page;
 +
 +	BUG_ON(max > KVM_NR_MEM_OBJS);
 +	if (cache->nobjs >= min)
 +		return 0;
 +	while (cache->nobjs < max) {
 +		page = (void *)__get_free_page(PGALLOC_GFP);
 +		if (!page)
 +			return -ENOMEM;
 +		cache->objects[cache->nobjs++] = page;
 +	}
 +	return 0;
 +}
 +
 +static void mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc)
 +{
 +	while (mc->nobjs)
 +		free_page((unsigned long)mc->objects[--mc->nobjs]);
 +}
 +
 +static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 +{
 +	void *p;
 +
 +	BUG_ON(!mc || !mc->nobjs);
 +	p = mc->objects[--mc->nobjs];
 +	return p;
  }
  
 -static void clear_stage2_p4d_entry(struct kvm *kvm, p4d_t *p4d, phys_addr_t addr)
++=======
++>>>>>>> c1a33aebe91d (KVM: arm64: Use common KVM implementation of MMU memory caches)
 +static void clear_stage2_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)
  {
 -	pud_t *pud_table __maybe_unused = stage2_pud_offset(kvm, p4d, 0);
 -	stage2_p4d_clear(kvm, p4d);
 +	pud_t *pud_table __maybe_unused = stage2_pud_offset(kvm, pgd, 0UL);
 +	stage2_pgd_clear(kvm, pgd);
  	kvm_tlb_flush_vmid_ipa(kvm, addr);
  	stage2_pud_free(kvm, pud_table);
 -	put_page(virt_to_page(p4d));
 +	put_page(virt_to_page(pgd));
  }
  
  static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)
@@@ -1037,12 -1100,30 +1040,39 @@@ static pud_t *stage2_get_pud(struct kv
  	if (stage2_pgd_none(kvm, *pgd)) {
  		if (!cache)
  			return NULL;
++<<<<<<< HEAD
 +		pud = mmu_memory_cache_alloc(cache);
 +		stage2_pgd_populate(kvm, pgd, pud);
 +		get_page(virt_to_page(pgd));
 +	}
 +
 +	return stage2_pud_offset(kvm, pgd, addr);
++=======
+ 		p4d = kvm_mmu_memory_cache_alloc(cache);
+ 		stage2_pgd_populate(kvm, pgd, p4d);
+ 		get_page(virt_to_page(pgd));
+ 	}
+ 
+ 	return stage2_p4d_offset(kvm, pgd, addr);
+ }
+ 
+ static pud_t *stage2_get_pud(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
+ 			     phys_addr_t addr)
+ {
+ 	p4d_t *p4d;
+ 	pud_t *pud;
+ 
+ 	p4d = stage2_get_p4d(kvm, cache, addr);
+ 	if (stage2_p4d_none(kvm, *p4d)) {
+ 		if (!cache)
+ 			return NULL;
+ 		pud = kvm_mmu_memory_cache_alloc(cache);
+ 		stage2_p4d_populate(kvm, p4d, pud);
+ 		get_page(virt_to_page(p4d));
+ 	}
+ 
+ 	return stage2_pud_offset(kvm, p4d, addr);
++>>>>>>> c1a33aebe91d (KVM: arm64: Use common KVM implementation of MMU memory caches)
  }
  
  static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
@@@ -1356,7 -1437,7 +1386,11 @@@ int kvm_phys_addr_ioremap(struct kvm *k
  	phys_addr_t addr, end;
  	int ret = 0;
  	unsigned long pfn;
++<<<<<<< HEAD
 +	struct kvm_mmu_memory_cache cache = { 0, };
++=======
+ 	struct kvm_mmu_memory_cache cache = { 0, __GFP_ZERO, NULL, };
++>>>>>>> c1a33aebe91d (KVM: arm64: Use common KVM implementation of MMU memory caches)
  
  	end = (guest_ipa + size + PAGE_SIZE - 1) & PAGE_MASK;
  	pfn = __phys_to_pfn(pa);
@@@ -1367,9 -1448,8 +1401,14 @@@
  		if (writable)
  			pte = kvm_s2pte_mkwrite(pte);
  
++<<<<<<< HEAD
 +		ret = mmu_topup_memory_cache(&cache,
 +					     kvm_mmu_cache_min_pages(kvm),
 +					     KVM_NR_MEM_OBJS);
++=======
+ 		ret = kvm_mmu_topup_memory_cache(&cache,
+ 						 kvm_mmu_cache_min_pages(kvm));
++>>>>>>> c1a33aebe91d (KVM: arm64: Use common KVM implementation of MMU memory caches)
  		if (ret)
  			goto out;
  		spin_lock(&kvm->mmu_lock);
@@@ -1747,11 -1846,10 +1786,15 @@@ static int user_mem_abort(struct kvm_vc
  	if (vma_pagesize == PMD_SIZE ||
  	    (vma_pagesize == PUD_SIZE && kvm_stage2_has_pmd(kvm)))
  		gfn = (fault_ipa & huge_page_mask(hstate_vma(vma))) >> PAGE_SHIFT;
 -	mmap_read_unlock(current->mm);
 +	up_read(&current->mm->mmap_sem);
  
  	/* We need minimum second+third level pages */
++<<<<<<< HEAD
 +	ret = mmu_topup_memory_cache(memcache, kvm_mmu_cache_min_pages(kvm),
 +				     KVM_NR_MEM_OBJS);
++=======
+ 	ret = kvm_mmu_topup_memory_cache(memcache, kvm_mmu_cache_min_pages(kvm));
++>>>>>>> c1a33aebe91d (KVM: arm64: Use common KVM implementation of MMU memory caches)
  	if (ret)
  		return ret;
  
* Unmerged path arch/arm64/include/asm/Kbuild
* Unmerged path arch/arm64/include/asm/kvm_host.h
diff --git a/arch/arm64/include/asm/kvm_types.h b/arch/arm64/include/asm/kvm_types.h
new file mode 100644
index 000000000000..9a126b9e2d7c
--- /dev/null
+++ b/arch/arm64/include/asm/kvm_types.h
@@ -0,0 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_ARM64_KVM_TYPES_H
+#define _ASM_ARM64_KVM_TYPES_H
+
+#define KVM_ARCH_NR_OBJS_PER_MEMORY_CACHE 40
+
+#endif /* _ASM_ARM64_KVM_TYPES_H */
+
* Unmerged path arch/arm64/kvm/mmu.c
