kasan: fix crashes on access to memory mapped by vm_map_ram()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Andrey Ryabinin <aryabinin@virtuozzo.com>
commit d98c9e83b5e7ca78175df1b13ac4a6d460d3962d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/d98c9e83.failed

With CONFIG_KASAN_VMALLOC=y any use of memory obtained via vm_map_ram()
will crash because there is no shadow backing that memory.

Instead of sprinkling additional kasan_populate_vmalloc() calls all over
the vmalloc code, move it into alloc_vmap_area(). This will fix
vm_map_ram() and simplify the code a bit.

[aryabinin@virtuozzo.com: v2]
  Link: http://lkml.kernel.org/r/20191205095942.1761-1-aryabinin@virtuozzo.comLink: http://lkml.kernel.org/r/20191204204534.32202-1-aryabinin@virtuozzo.com
Fixes: 3c5c3cfb9ef4 ("kasan: support backing vmalloc space with real shadow memory")
	Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Reported-by: Dmitry Vyukov <dvyukov@google.com>
	Reviewed-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
	Cc: Daniel Axtens <dja@axtens.net>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Daniel Axtens <dja@axtens.net>
	Cc: Qian Cai <cai@lca.pw>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d98c9e83b5e7ca78175df1b13ac4a6d460d3962d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/kasan.h
#	mm/kasan/common.c
#	mm/vmalloc.c
diff --cc include/linux/kasan.h
index 1e5ac58e377c,e18fe54969e9..000000000000
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@@ -159,4 -168,64 +159,67 @@@ static inline size_t kasan_metadata_siz
  
  #endif /* CONFIG_KASAN */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_KASAN_GENERIC
+ 
+ #define KASAN_SHADOW_INIT 0
+ 
+ void kasan_cache_shrink(struct kmem_cache *cache);
+ void kasan_cache_shutdown(struct kmem_cache *cache);
+ 
+ #else /* CONFIG_KASAN_GENERIC */
+ 
+ static inline void kasan_cache_shrink(struct kmem_cache *cache) {}
+ static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
+ 
+ #endif /* CONFIG_KASAN_GENERIC */
+ 
+ #ifdef CONFIG_KASAN_SW_TAGS
+ 
+ #define KASAN_SHADOW_INIT 0xFF
+ 
+ void kasan_init_tags(void);
+ 
+ void *kasan_reset_tag(const void *addr);
+ 
+ void kasan_report(unsigned long addr, size_t size,
+ 		bool is_write, unsigned long ip);
+ 
+ #else /* CONFIG_KASAN_SW_TAGS */
+ 
+ static inline void kasan_init_tags(void) { }
+ 
+ static inline void *kasan_reset_tag(const void *addr)
+ {
+ 	return (void *)addr;
+ }
+ 
+ #endif /* CONFIG_KASAN_SW_TAGS */
+ 
+ #ifdef CONFIG_KASAN_VMALLOC
+ int kasan_populate_vmalloc(unsigned long addr, unsigned long size);
+ void kasan_poison_vmalloc(const void *start, unsigned long size);
+ void kasan_unpoison_vmalloc(const void *start, unsigned long size);
+ void kasan_release_vmalloc(unsigned long start, unsigned long end,
+ 			   unsigned long free_region_start,
+ 			   unsigned long free_region_end);
+ #else
+ static inline int kasan_populate_vmalloc(unsigned long start,
+ 					unsigned long size)
+ {
+ 	return 0;
+ }
+ 
+ static inline void kasan_poison_vmalloc(const void *start, unsigned long size)
+ { }
+ static inline void kasan_unpoison_vmalloc(const void *start, unsigned long size)
+ { }
+ static inline void kasan_release_vmalloc(unsigned long start,
+ 					 unsigned long end,
+ 					 unsigned long free_region_start,
+ 					 unsigned long free_region_end) {}
+ #endif
+ 
++>>>>>>> d98c9e83b5e7 (kasan: fix crashes on access to memory mapped by vm_map_ram())
  #endif /* LINUX_KASAN_H */
diff --cc mm/vmalloc.c
index c82a0db1aefc,6e865cea846c..000000000000
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@@ -363,42 -430,657 +363,62 @@@ static struct vmap_area *__find_vmap_ar
  	return NULL;
  }
  
 -/*
 - * This function returns back addresses of parent node
 - * and its left or right link for further processing.
 - */
 -static __always_inline struct rb_node **
 -find_va_links(struct vmap_area *va,
 -	struct rb_root *root, struct rb_node *from,
 -	struct rb_node **parent)
 -{
 -	struct vmap_area *tmp_va;
 -	struct rb_node **link;
 -
 -	if (root) {
 -		link = &root->rb_node;
 -		if (unlikely(!*link)) {
 -			*parent = NULL;
 -			return link;
 -		}
 -	} else {
 -		link = &from;
 -	}
 -
 -	/*
 -	 * Go to the bottom of the tree. When we hit the last point
 -	 * we end up with parent rb_node and correct direction, i name
 -	 * it link, where the new va->rb_node will be attached to.
 -	 */
 -	do {
 -		tmp_va = rb_entry(*link, struct vmap_area, rb_node);
 -
 -		/*
 -		 * During the traversal we also do some sanity check.
 -		 * Trigger the BUG() if there are sides(left/right)
 -		 * or full overlaps.
 -		 */
 -		if (va->va_start < tmp_va->va_end &&
 -				va->va_end <= tmp_va->va_start)
 -			link = &(*link)->rb_left;
 -		else if (va->va_end > tmp_va->va_start &&
 -				va->va_start >= tmp_va->va_end)
 -			link = &(*link)->rb_right;
 -		else
 -			BUG();
 -	} while (*link);
 -
 -	*parent = &tmp_va->rb_node;
 -	return link;
 -}
 -
 -static __always_inline struct list_head *
 -get_va_next_sibling(struct rb_node *parent, struct rb_node **link)
 -{
 -	struct list_head *list;
 -
 -	if (unlikely(!parent))
 -		/*
 -		 * The red-black tree where we try to find VA neighbors
 -		 * before merging or inserting is empty, i.e. it means
 -		 * there is no free vmap space. Normally it does not
 -		 * happen but we handle this case anyway.
 -		 */
 -		return NULL;
 -
 -	list = &rb_entry(parent, struct vmap_area, rb_node)->list;
 -	return (&parent->rb_right == link ? list->next : list);
 -}
 -
 -static __always_inline void
 -link_va(struct vmap_area *va, struct rb_root *root,
 -	struct rb_node *parent, struct rb_node **link, struct list_head *head)
 -{
 -	/*
 -	 * VA is still not in the list, but we can
 -	 * identify its future previous list_head node.
 -	 */
 -	if (likely(parent)) {
 -		head = &rb_entry(parent, struct vmap_area, rb_node)->list;
 -		if (&parent->rb_right != link)
 -			head = head->prev;
 -	}
 -
 -	/* Insert to the rb-tree */
 -	rb_link_node(&va->rb_node, parent, link);
 -	if (root == &free_vmap_area_root) {
 -		/*
 -		 * Some explanation here. Just perform simple insertion
 -		 * to the tree. We do not set va->subtree_max_size to
 -		 * its current size before calling rb_insert_augmented().
 -		 * It is because of we populate the tree from the bottom
 -		 * to parent levels when the node _is_ in the tree.
 -		 *
 -		 * Therefore we set subtree_max_size to zero after insertion,
 -		 * to let __augment_tree_propagate_from() puts everything to
 -		 * the correct order later on.
 -		 */
 -		rb_insert_augmented(&va->rb_node,
 -			root, &free_vmap_area_rb_augment_cb);
 -		va->subtree_max_size = 0;
 -	} else {
 -		rb_insert_color(&va->rb_node, root);
 -	}
 -
 -	/* Address-sort this list */
 -	list_add(&va->list, head);
 -}
 -
 -static __always_inline void
 -unlink_va(struct vmap_area *va, struct rb_root *root)
 -{
 -	if (WARN_ON(RB_EMPTY_NODE(&va->rb_node)))
 -		return;
 -
 -	if (root == &free_vmap_area_root)
 -		rb_erase_augmented(&va->rb_node,
 -			root, &free_vmap_area_rb_augment_cb);
 -	else
 -		rb_erase(&va->rb_node, root);
 -
 -	list_del(&va->list);
 -	RB_CLEAR_NODE(&va->rb_node);
 -}
 -
 -#if DEBUG_AUGMENT_PROPAGATE_CHECK
 -static void
 -augment_tree_propagate_check(struct rb_node *n)
 -{
 -	struct vmap_area *va;
 -	struct rb_node *node;
 -	unsigned long size;
 -	bool found = false;
 -
 -	if (n == NULL)
 -		return;
 -
 -	va = rb_entry(n, struct vmap_area, rb_node);
 -	size = va->subtree_max_size;
 -	node = n;
 -
 -	while (node) {
 -		va = rb_entry(node, struct vmap_area, rb_node);
 -
 -		if (get_subtree_max_size(node->rb_left) == size) {
 -			node = node->rb_left;
 -		} else {
 -			if (va_size(va) == size) {
 -				found = true;
 -				break;
 -			}
 -
 -			node = node->rb_right;
 -		}
 -	}
 -
 -	if (!found) {
 -		va = rb_entry(n, struct vmap_area, rb_node);
 -		pr_emerg("tree is corrupted: %lu, %lu\n",
 -			va_size(va), va->subtree_max_size);
 -	}
 -
 -	augment_tree_propagate_check(n->rb_left);
 -	augment_tree_propagate_check(n->rb_right);
 -}
 -#endif
 -
 -/*
 - * This function populates subtree_max_size from bottom to upper
 - * levels starting from VA point. The propagation must be done
 - * when VA size is modified by changing its va_start/va_end. Or
 - * in case of newly inserting of VA to the tree.
 - *
 - * It means that __augment_tree_propagate_from() must be called:
 - * - After VA has been inserted to the tree(free path);
 - * - After VA has been shrunk(allocation path);
 - * - After VA has been increased(merging path).
 - *
 - * Please note that, it does not mean that upper parent nodes
 - * and their subtree_max_size are recalculated all the time up
 - * to the root node.
 - *
 - *       4--8
 - *        /\
 - *       /  \
 - *      /    \
 - *    2--2  8--8
 - *
 - * For example if we modify the node 4, shrinking it to 2, then
 - * no any modification is required. If we shrink the node 2 to 1
 - * its subtree_max_size is updated only, and set to 1. If we shrink
 - * the node 8 to 6, then its subtree_max_size is set to 6 and parent
 - * node becomes 4--6.
 - */
 -static __always_inline void
 -augment_tree_propagate_from(struct vmap_area *va)
 -{
 -	struct rb_node *node = &va->rb_node;
 -	unsigned long new_va_sub_max_size;
 -
 -	while (node) {
 -		va = rb_entry(node, struct vmap_area, rb_node);
 -		new_va_sub_max_size = compute_subtree_max_size(va);
 -
 -		/*
 -		 * If the newly calculated maximum available size of the
 -		 * subtree is equal to the current one, then it means that
 -		 * the tree is propagated correctly. So we have to stop at
 -		 * this point to save cycles.
 -		 */
 -		if (va->subtree_max_size == new_va_sub_max_size)
 -			break;
 -
 -		va->subtree_max_size = new_va_sub_max_size;
 -		node = rb_parent(&va->rb_node);
 -	}
 -
 -#if DEBUG_AUGMENT_PROPAGATE_CHECK
 -	augment_tree_propagate_check(free_vmap_area_root.rb_node);
 -#endif
 -}
 -
 -static void
 -insert_vmap_area(struct vmap_area *va,
 -	struct rb_root *root, struct list_head *head)
 -{
 -	struct rb_node **link;
 -	struct rb_node *parent;
 -
 -	link = find_va_links(va, root, NULL, &parent);
 -	link_va(va, root, parent, link, head);
 -}
 -
 -static void
 -insert_vmap_area_augment(struct vmap_area *va,
 -	struct rb_node *from, struct rb_root *root,
 -	struct list_head *head)
 -{
 -	struct rb_node **link;
 -	struct rb_node *parent;
 -
 -	if (from)
 -		link = find_va_links(va, NULL, from, &parent);
 -	else
 -		link = find_va_links(va, root, NULL, &parent);
 -
 -	link_va(va, root, parent, link, head);
 -	augment_tree_propagate_from(va);
 -}
 -
 -/*
 - * Merge de-allocated chunk of VA memory with previous
 - * and next free blocks. If coalesce is not done a new
 - * free area is inserted. If VA has been merged, it is
 - * freed.
 - */
 -static __always_inline struct vmap_area *
 -merge_or_add_vmap_area(struct vmap_area *va,
 -	struct rb_root *root, struct list_head *head)
 -{
 -	struct vmap_area *sibling;
 -	struct list_head *next;
 -	struct rb_node **link;
 -	struct rb_node *parent;
 -	bool merged = false;
 -
 -	/*
 -	 * Find a place in the tree where VA potentially will be
 -	 * inserted, unless it is merged with its sibling/siblings.
 -	 */
 -	link = find_va_links(va, root, NULL, &parent);
 -
 -	/*
 -	 * Get next node of VA to check if merging can be done.
 -	 */
 -	next = get_va_next_sibling(parent, link);
 -	if (unlikely(next == NULL))
 -		goto insert;
 -
 -	/*
 -	 * start            end
 -	 * |                |
 -	 * |<------VA------>|<-----Next----->|
 -	 *                  |                |
 -	 *                  start            end
 -	 */
 -	if (next != head) {
 -		sibling = list_entry(next, struct vmap_area, list);
 -		if (sibling->va_start == va->va_end) {
 -			sibling->va_start = va->va_start;
 -
 -			/* Check and update the tree if needed. */
 -			augment_tree_propagate_from(sibling);
 -
 -			/* Free vmap_area object. */
 -			kmem_cache_free(vmap_area_cachep, va);
 -
 -			/* Point to the new merged area. */
 -			va = sibling;
 -			merged = true;
 -		}
 -	}
 -
 -	/*
 -	 * start            end
 -	 * |                |
 -	 * |<-----Prev----->|<------VA------>|
 -	 *                  |                |
 -	 *                  start            end
 -	 */
 -	if (next->prev != head) {
 -		sibling = list_entry(next->prev, struct vmap_area, list);
 -		if (sibling->va_end == va->va_start) {
 -			sibling->va_end = va->va_end;
 -
 -			/* Check and update the tree if needed. */
 -			augment_tree_propagate_from(sibling);
 -
 -			if (merged)
 -				unlink_va(va, root);
 -
 -			/* Free vmap_area object. */
 -			kmem_cache_free(vmap_area_cachep, va);
 -
 -			/* Point to the new merged area. */
 -			va = sibling;
 -			merged = true;
 -		}
 -	}
 -
 -insert:
 -	if (!merged) {
 -		link_va(va, root, parent, link, head);
 -		augment_tree_propagate_from(va);
 -	}
 -
 -	return va;
 -}
 -
 -static __always_inline bool
 -is_within_this_va(struct vmap_area *va, unsigned long size,
 -	unsigned long align, unsigned long vstart)
 -{
 -	unsigned long nva_start_addr;
 -
 -	if (va->va_start > vstart)
 -		nva_start_addr = ALIGN(va->va_start, align);
 -	else
 -		nva_start_addr = ALIGN(vstart, align);
 -
 -	/* Can be overflowed due to big size or alignment. */
 -	if (nva_start_addr + size < nva_start_addr ||
 -			nva_start_addr < vstart)
 -		return false;
 -
 -	return (nva_start_addr + size <= va->va_end);
 -}
 -
 -/*
 - * Find the first free block(lowest start address) in the tree,
 - * that will accomplish the request corresponding to passing
 - * parameters.
 - */
 -static __always_inline struct vmap_area *
 -find_vmap_lowest_match(unsigned long size,
 -	unsigned long align, unsigned long vstart)
 -{
 -	struct vmap_area *va;
 -	struct rb_node *node;
 -	unsigned long length;
 -
 -	/* Start from the root. */
 -	node = free_vmap_area_root.rb_node;
 -
 -	/* Adjust the search size for alignment overhead. */
 -	length = size + align - 1;
 -
 -	while (node) {
 -		va = rb_entry(node, struct vmap_area, rb_node);
 -
 -		if (get_subtree_max_size(node->rb_left) >= length &&
 -				vstart < va->va_start) {
 -			node = node->rb_left;
 -		} else {
 -			if (is_within_this_va(va, size, align, vstart))
 -				return va;
 -
 -			/*
 -			 * Does not make sense to go deeper towards the right
 -			 * sub-tree if it does not have a free block that is
 -			 * equal or bigger to the requested search length.
 -			 */
 -			if (get_subtree_max_size(node->rb_right) >= length) {
 -				node = node->rb_right;
 -				continue;
 -			}
 -
 -			/*
 -			 * OK. We roll back and find the first right sub-tree,
 -			 * that will satisfy the search criteria. It can happen
 -			 * only once due to "vstart" restriction.
 -			 */
 -			while ((node = rb_parent(node))) {
 -				va = rb_entry(node, struct vmap_area, rb_node);
 -				if (is_within_this_va(va, size, align, vstart))
 -					return va;
 -
 -				if (get_subtree_max_size(node->rb_right) >= length &&
 -						vstart <= va->va_start) {
 -					node = node->rb_right;
 -					break;
 -				}
 -			}
 -		}
 -	}
 -
 -	return NULL;
 -}
 -
 -#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK
 -#include <linux/random.h>
 -
 -static struct vmap_area *
 -find_vmap_lowest_linear_match(unsigned long size,
 -	unsigned long align, unsigned long vstart)
 +static void __insert_vmap_area(struct vmap_area *va)
  {
 -	struct vmap_area *va;
 -
 -	list_for_each_entry(va, &free_vmap_area_list, list) {
 -		if (!is_within_this_va(va, size, align, vstart))
 -			continue;
 +	struct rb_node **p = &vmap_area_root.rb_node;
 +	struct rb_node *parent = NULL;
 +	struct rb_node *tmp;
  
 -		return va;
 -	}
 +	while (*p) {
 +		struct vmap_area *tmp_va;
  
 -	return NULL;
 -}
 -
 -static void
 -find_vmap_lowest_match_check(unsigned long size)
 -{
 -	struct vmap_area *va_1, *va_2;
 -	unsigned long vstart;
 -	unsigned int rnd;
 -
 -	get_random_bytes(&rnd, sizeof(rnd));
 -	vstart = VMALLOC_START + rnd;
 -
 -	va_1 = find_vmap_lowest_match(size, 1, vstart);
 -	va_2 = find_vmap_lowest_linear_match(size, 1, vstart);
 -
 -	if (va_1 != va_2)
 -		pr_emerg("not lowest: t: 0x%p, l: 0x%p, v: 0x%lx\n",
 -			va_1, va_2, vstart);
 -}
 -#endif
 -
 -enum fit_type {
 -	NOTHING_FIT = 0,
 -	FL_FIT_TYPE = 1,	/* full fit */
 -	LE_FIT_TYPE = 2,	/* left edge fit */
 -	RE_FIT_TYPE = 3,	/* right edge fit */
 -	NE_FIT_TYPE = 4		/* no edge fit */
 -};
 -
 -static __always_inline enum fit_type
 -classify_va_fit_type(struct vmap_area *va,
 -	unsigned long nva_start_addr, unsigned long size)
 -{
 -	enum fit_type type;
 -
 -	/* Check if it is within VA. */
 -	if (nva_start_addr < va->va_start ||
 -			nva_start_addr + size > va->va_end)
 -		return NOTHING_FIT;
 -
 -	/* Now classify. */
 -	if (va->va_start == nva_start_addr) {
 -		if (va->va_end == nva_start_addr + size)
 -			type = FL_FIT_TYPE;
 +		parent = *p;
 +		tmp_va = rb_entry(parent, struct vmap_area, rb_node);
 +		if (va->va_start < tmp_va->va_end)
 +			p = &(*p)->rb_left;
 +		else if (va->va_end > tmp_va->va_start)
 +			p = &(*p)->rb_right;
  		else
 -			type = LE_FIT_TYPE;
 -	} else if (va->va_end == nva_start_addr + size) {
 -		type = RE_FIT_TYPE;
 -	} else {
 -		type = NE_FIT_TYPE;
 -	}
 -
 -	return type;
 -}
 -
 -static __always_inline int
 -adjust_va_to_fit_type(struct vmap_area *va,
 -	unsigned long nva_start_addr, unsigned long size,
 -	enum fit_type type)
 -{
 -	struct vmap_area *lva = NULL;
 -
 -	if (type == FL_FIT_TYPE) {
 -		/*
 -		 * No need to split VA, it fully fits.
 -		 *
 -		 * |               |
 -		 * V      NVA      V
 -		 * |---------------|
 -		 */
 -		unlink_va(va, &free_vmap_area_root);
 -		kmem_cache_free(vmap_area_cachep, va);
 -	} else if (type == LE_FIT_TYPE) {
 -		/*
 -		 * Split left edge of fit VA.
 -		 *
 -		 * |       |
 -		 * V  NVA  V   R
 -		 * |-------|-------|
 -		 */
 -		va->va_start += size;
 -	} else if (type == RE_FIT_TYPE) {
 -		/*
 -		 * Split right edge of fit VA.
 -		 *
 -		 *         |       |
 -		 *     L   V  NVA  V
 -		 * |-------|-------|
 -		 */
 -		va->va_end = nva_start_addr;
 -	} else if (type == NE_FIT_TYPE) {
 -		/*
 -		 * Split no edge of fit VA.
 -		 *
 -		 *     |       |
 -		 *   L V  NVA  V R
 -		 * |---|-------|---|
 -		 */
 -		lva = __this_cpu_xchg(ne_fit_preload_node, NULL);
 -		if (unlikely(!lva)) {
 -			/*
 -			 * For percpu allocator we do not do any pre-allocation
 -			 * and leave it as it is. The reason is it most likely
 -			 * never ends up with NE_FIT_TYPE splitting. In case of
 -			 * percpu allocations offsets and sizes are aligned to
 -			 * fixed align request, i.e. RE_FIT_TYPE and FL_FIT_TYPE
 -			 * are its main fitting cases.
 -			 *
 -			 * There are a few exceptions though, as an example it is
 -			 * a first allocation (early boot up) when we have "one"
 -			 * big free space that has to be split.
 -			 *
 -			 * Also we can hit this path in case of regular "vmap"
 -			 * allocations, if "this" current CPU was not preloaded.
 -			 * See the comment in alloc_vmap_area() why. If so, then
 -			 * GFP_NOWAIT is used instead to get an extra object for
 -			 * split purpose. That is rare and most time does not
 -			 * occur.
 -			 *
 -			 * What happens if an allocation gets failed. Basically,
 -			 * an "overflow" path is triggered to purge lazily freed
 -			 * areas to free some memory, then, the "retry" path is
 -			 * triggered to repeat one more time. See more details
 -			 * in alloc_vmap_area() function.
 -			 */
 -			lva = kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);
 -			if (!lva)
 -				return -1;
 -		}
 -
 -		/*
 -		 * Build the remainder.
 -		 */
 -		lva->va_start = va->va_start;
 -		lva->va_end = nva_start_addr;
 -
 -		/*
 -		 * Shrink this VA to remaining size.
 -		 */
 -		va->va_start = nva_start_addr + size;
 -	} else {
 -		return -1;
 +			BUG();
  	}
  
 -	if (type != FL_FIT_TYPE) {
 -		augment_tree_propagate_from(va);
 +	rb_link_node(&va->rb_node, parent, p);
 +	rb_insert_color(&va->rb_node, &vmap_area_root);
  
 -		if (lva)	/* type == NE_FIT_TYPE */
 -			insert_vmap_area_augment(lva, &va->rb_node,
 -				&free_vmap_area_root, &free_vmap_area_list);
 -	}
 -
 -	return 0;
 +	/* address-sort this list */
 +	tmp = rb_prev(&va->rb_node);
 +	if (tmp) {
 +		struct vmap_area *prev;
 +		prev = rb_entry(tmp, struct vmap_area, rb_node);
 +		list_add_rcu(&va->list, &prev->list);
 +	} else
 +		list_add_rcu(&va->list, &vmap_area_list);
  }
  
 -/*
 - * Returns a start address of the newly allocated area, if success.
 - * Otherwise a vend is returned that indicates failure.
 - */
 -static __always_inline unsigned long
 -__alloc_vmap_area(unsigned long size, unsigned long align,
 -	unsigned long vstart, unsigned long vend)
 -{
 -	unsigned long nva_start_addr;
 -	struct vmap_area *va;
 -	enum fit_type type;
 -	int ret;
 -
 -	va = find_vmap_lowest_match(size, align, vstart);
 -	if (unlikely(!va))
 -		return vend;
 -
 -	if (va->va_start > vstart)
 -		nva_start_addr = ALIGN(va->va_start, align);
 -	else
 -		nva_start_addr = ALIGN(vstart, align);
 -
 -	/* Check the "vend" restriction. */
 -	if (nva_start_addr + size > vend)
 -		return vend;
 -
 -	/* Classify what we have found. */
 -	type = classify_va_fit_type(va, nva_start_addr, size);
 -	if (WARN_ON_ONCE(type == NOTHING_FIT))
 -		return vend;
 -
 -	/* Update the free vmap_area. */
 -	ret = adjust_va_to_fit_type(va, nva_start_addr, size, type);
 -	if (ret)
 -		return vend;
 -
 -#if DEBUG_AUGMENT_LOWEST_MATCH_CHECK
 -	find_vmap_lowest_match_check(size);
 -#endif
 +static void purge_vmap_area_lazy(void);
  
 -	return nva_start_addr;
 -}
 +static BLOCKING_NOTIFIER_HEAD(vmap_notify_list);
  
+ /*
+  * Free a region of KVA allocated by alloc_vmap_area
+  */
+ static void free_vmap_area(struct vmap_area *va)
+ {
+ 	/*
+ 	 * Remove from the busy tree/list.
+ 	 */
+ 	spin_lock(&vmap_area_lock);
+ 	unlink_va(va, &vmap_area_root);
+ 	spin_unlock(&vmap_area_lock);
+ 
+ 	/*
+ 	 * Insert/Merge it back to the free tree/list.
+ 	 */
+ 	spin_lock(&free_vmap_area_lock);
+ 	merge_or_add_vmap_area(va, &free_vmap_area_root, &free_vmap_area_list);
+ 	spin_unlock(&free_vmap_area_lock);
+ }
+ 
  /*
   * Allocate a region of KVA of the specified size and alignment, within the
   * vstart and vend.
@@@ -408,11 -1090,10 +428,15 @@@ static struct vmap_area *alloc_vmap_are
  				unsigned long vstart, unsigned long vend,
  				int node, gfp_t gfp_mask)
  {
 -	struct vmap_area *va, *pva;
 +	struct vmap_area *va;
 +	struct rb_node *n;
  	unsigned long addr;
  	int purged = 0;
++<<<<<<< HEAD
 +	struct vmap_area *first;
++=======
+ 	int ret;
++>>>>>>> d98c9e83b5e7 (kasan: fix crashes on access to memory mapped by vm_map_ram())
  
  	BUG_ON(!size);
  	BUG_ON(offset_in_page(size));
@@@ -507,9 -1158,11 +531,17 @@@ found
  
  	va->va_start = addr;
  	va->va_end = addr + size;
++<<<<<<< HEAD
 +	va->flags = 0;
 +	__insert_vmap_area(va);
 +	free_vmap_cache = &va->rb_node;
++=======
+ 	va->vm = NULL;
+ 
+ 
+ 	spin_lock(&vmap_area_lock);
+ 	insert_vmap_area(va, &vmap_area_root, &vmap_area_list);
++>>>>>>> d98c9e83b5e7 (kasan: fix crashes on access to memory mapped by vm_map_ram())
  	spin_unlock(&vmap_area_lock);
  
  	BUG_ON(!IS_ALIGNED(va->va_start, align));
@@@ -538,68 -1196,24 +576,71 @@@ overflow
  	if (!(gfp_mask & __GFP_NOWARN) && printk_ratelimit())
  		pr_warn("vmap allocation for size %lu failed: use vmalloc=<size> to increase size\n",
  			size);
 +	kfree(va);
 +	return ERR_PTR(-EBUSY);
 +}
 +
 +int register_vmap_purge_notifier(struct notifier_block *nb)
 +{
 +	return blocking_notifier_chain_register(&vmap_notify_list, nb);
 +}
 +EXPORT_SYMBOL_GPL(register_vmap_purge_notifier);
 +
 +int unregister_vmap_purge_notifier(struct notifier_block *nb)
 +{
 +	return blocking_notifier_chain_unregister(&vmap_notify_list, nb);
 +}
 +EXPORT_SYMBOL_GPL(unregister_vmap_purge_notifier);
 +
 +static void __free_vmap_area(struct vmap_area *va)
 +{
 +	BUG_ON(RB_EMPTY_NODE(&va->rb_node));
 +
 +	if (free_vmap_cache) {
 +		if (va->va_end < cached_vstart) {
 +			free_vmap_cache = NULL;
 +		} else {
 +			struct vmap_area *cache;
 +			cache = rb_entry(free_vmap_cache, struct vmap_area, rb_node);
 +			if (va->va_start <= cache->va_start) {
 +				free_vmap_cache = rb_prev(&va->rb_node);
 +				/*
 +				 * We don't try to update cached_hole_size or
 +				 * cached_align, but it won't go very wrong.
 +				 */
 +			}
 +		}
 +	}
 +	rb_erase(&va->rb_node, &vmap_area_root);
 +	RB_CLEAR_NODE(&va->rb_node);
 +	list_del_rcu(&va->list);
  
 -	kmem_cache_free(vmap_area_cachep, va);
 -	return ERR_PTR(-EBUSY);
 -}
 +	/*
 +	 * Track the highest possible candidate for pcpu area
 +	 * allocation.  Areas outside of vmalloc area can be returned
 +	 * here too, consider only end addresses which fall inside
 +	 * vmalloc area proper.
 +	 */
 +	if (va->va_end > VMALLOC_START && va->va_end <= VMALLOC_END)
 +		vmap_area_pcpu_hole = max(vmap_area_pcpu_hole, va->va_end);
  
 -int register_vmap_purge_notifier(struct notifier_block *nb)
 -{
 -	return blocking_notifier_chain_register(&vmap_notify_list, nb);
 +	kfree_rcu(va, rcu_head);
  }
 -EXPORT_SYMBOL_GPL(register_vmap_purge_notifier);
  
 -int unregister_vmap_purge_notifier(struct notifier_block *nb)
 +/*
++<<<<<<< HEAD
 + * Free a region of KVA allocated by alloc_vmap_area
 + */
 +static void free_vmap_area(struct vmap_area *va)
  {
 -	return blocking_notifier_chain_unregister(&vmap_notify_list, nb);
 +	spin_lock(&vmap_area_lock);
 +	__free_vmap_area(va);
 +	spin_unlock(&vmap_area_lock);
  }
 -EXPORT_SYMBOL_GPL(unregister_vmap_purge_notifier);
  
  /*
++=======
++>>>>>>> d98c9e83b5e7 (kasan: fix crashes on access to memory mapped by vm_map_ram())
   * Clear the pagetable entries of a given vmap_area
   */
  static void unmap_vmap_area(struct vmap_area *va)
@@@ -1421,8 -2112,10 +1468,13 @@@ static struct vm_struct *__get_vm_area_
  		return NULL;
  	}
  
+ 	kasan_unpoison_vmalloc((void *)va->va_start, requested_size);
+ 
++<<<<<<< HEAD
++=======
  	setup_vmalloc_vm(area, va, flags, caller);
  
++>>>>>>> d98c9e83b5e7 (kasan: fix crashes on access to memory mapped by vm_map_ram())
  	return area;
  }
  
@@@ -1594,6 -2293,8 +1646,11 @@@ static void __vunmap(const void *addr, 
  	debug_check_no_locks_freed(area->addr, get_vm_area_size(area));
  	debug_check_no_obj_freed(area->addr, get_vm_area_size(area));
  
++<<<<<<< HEAD
++=======
+ 	kasan_poison_vmalloc(area->addr, area->size);
+ 
++>>>>>>> d98c9e83b5e7 (kasan: fix crashes on access to memory mapped by vm_map_ram())
  	vm_remove_mappings(area, deallocate_pages);
  
  	if (deallocate_pages) {
@@@ -1815,10 -2535,10 +1872,10 @@@ void *__vmalloc_node_range(unsigned lon
  	unsigned long real_size = size;
  
  	size = PAGE_ALIGN(size);
 -	if (!size || (size >> PAGE_SHIFT) > totalram_pages())
 +	if (!size || (size >> PAGE_SHIFT) > totalram_pages)
  		goto fail;
  
- 	area = __get_vm_area_node(size, align, VM_ALLOC | VM_UNINITIALIZED |
+ 	area = __get_vm_area_node(real_size, align, VM_ALLOC | VM_UNINITIALIZED |
  				vm_flags, start, end, node, gfp_mask, caller);
  	if (!area)
  		goto fail;
@@@ -2696,28 -3383,57 +2753,37 @@@ retry
  		area = (area + nr_vms - 1) % nr_vms;
  		if (area == term_area)
  			break;
 -
  		start = offsets[area];
  		end = start + sizes[area];
 -		va = pvm_find_va_enclose_addr(base + end);
 +		pvm_find_next_prev(base + end, &next, &prev);
  	}
 -
 +found:
  	/* we've found a fitting base, insert all va's */
  	for (area = 0; area < nr_vms; area++) {
 -		int ret;
 -
 -		start = base + offsets[area];
 -		size = sizes[area];
 -
 -		va = pvm_find_va_enclose_addr(start);
 -		if (WARN_ON_ONCE(va == NULL))
 -			/* It is a BUG(), but trigger recovery instead. */
 -			goto recovery;
 +		struct vmap_area *va = vas[area];
  
 -		type = classify_va_fit_type(va, start, size);
 -		if (WARN_ON_ONCE(type == NOTHING_FIT))
 -			/* It is a BUG(), but trigger recovery instead. */
 -			goto recovery;
 -
 -		ret = adjust_va_to_fit_type(va, start, size, type);
 -		if (unlikely(ret))
 -			goto recovery;
 -
 -		/* Allocated area. */
 -		va = vas[area];
 -		va->va_start = start;
 -		va->va_end = start + size;
 +		va->va_start = base + offsets[area];
 +		va->va_end = va->va_start + sizes[area];
 +		__insert_vmap_area(va);
  	}
  
 -	spin_unlock(&free_vmap_area_lock);
 -
 -	/* insert all vm's */
 -	spin_lock(&vmap_area_lock);
 -	for (area = 0; area < nr_vms; area++) {
 -		insert_vmap_area(vas[area], &vmap_area_root, &vmap_area_list);
 +	vmap_area_pcpu_hole = base + offsets[last_area];
  
 -		setup_vmalloc_vm_locked(vms[area], vas[area], VM_ALLOC,
 -				 pcpu_get_vm_areas);
 -	}
  	spin_unlock(&vmap_area_lock);
  
++<<<<<<< HEAD
 +	/* insert all vm's */
 +	for (area = 0; area < nr_vms; area++)
 +		setup_vmalloc_vm(vms[area], vas[area], VM_ALLOC,
 +				 pcpu_get_vm_areas);
++=======
+ 	/* populate the shadow space outside of the lock */
+ 	for (area = 0; area < nr_vms; area++) {
+ 		/* assume success here */
+ 		kasan_populate_vmalloc(vas[area]->va_start, sizes[area]);
+ 		kasan_unpoison_vmalloc((void *)vms[area]->addr, sizes[area]);
+ 	}
++>>>>>>> d98c9e83b5e7 (kasan: fix crashes on access to memory mapped by vm_map_ram())
  
  	kfree(vas);
  	return vms;
* Unmerged path mm/kasan/common.c
* Unmerged path include/linux/kasan.h
* Unmerged path mm/kasan/common.c
* Unmerged path mm/vmalloc.c
