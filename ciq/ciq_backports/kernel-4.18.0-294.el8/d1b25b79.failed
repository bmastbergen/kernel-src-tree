qede: add .ndo_xdp_xmit() and XDP_REDIRECT support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Alexander Lobakin <alobakin@marvell.com>
commit d1b25b79e162b23ec1bbdfb13bda7154b1f46cfb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/d1b25b79.failed

Add XDP_REDIRECT case handling and the corresponding NDO to support
redirecting XDP frames. This also includes registering driver memory
model (currently order-0 page mode) in BPF subsystem.
The total number of XDP queues is usually 1:1 with Rx ones.

	Signed-off-by: Alexander Lobakin <alobakin@marvell.com>
	Signed-off-by: Igor Russkikh <irusskikh@marvell.com>
	Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit d1b25b79e162b23ec1bbdfb13bda7154b1f46cfb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/qlogic/qede/qede.h
#	drivers/net/ethernet/qlogic/qede/qede_fp.c
diff --cc drivers/net/ethernet/qlogic/qede/qede.h
index 2f666109bc6a,803c1fcca8ad..000000000000
--- a/drivers/net/ethernet/qlogic/qede/qede.h
+++ b/drivers/net/ethernet/qlogic/qede/qede.h
@@@ -197,10 -199,12 +197,17 @@@ struct qede_dev 
  	u8				fp_num_rx;
  	u16				req_queues;
  	u16				num_queues;
++<<<<<<< HEAD
 +#define QEDE_QUEUE_CNT(edev)	((edev)->num_queues)
 +#define QEDE_RSS_COUNT(edev)	((edev)->num_queues - (edev)->fp_num_tx)
++=======
+ 	u16				total_xdp_queues;
+ 
+ #define QEDE_QUEUE_CNT(edev)		((edev)->num_queues)
+ #define QEDE_RSS_COUNT(edev)		((edev)->num_queues - (edev)->fp_num_tx)
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  #define QEDE_RX_QUEUE_IDX(edev, i)	(i)
 -#define QEDE_TSS_COUNT(edev)		((edev)->num_queues - (edev)->fp_num_rx)
 +#define QEDE_TSS_COUNT(edev)	((edev)->num_queues - (edev)->fp_num_rx)
  
  	struct qed_int_info		int_info;
  
@@@ -371,29 -381,34 +378,45 @@@ struct sw_tx_bd 
  };
  
  struct sw_tx_xdp {
++<<<<<<< HEAD
 +	struct page *page;
 +	dma_addr_t mapping;
++=======
+ 	struct page			*page;
+ 	struct xdp_frame		*xdpf;
+ 	dma_addr_t			mapping;
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  };
  
  struct qede_tx_queue {
 -	u8				is_xdp;
 -	bool				is_legacy;
 -	u16				sw_tx_cons;
 -	u16				sw_tx_prod;
 -	u16				num_tx_buffers; /* Slowpath only */
 +	u8 is_xdp;
 +	bool is_legacy;
 +	u16 sw_tx_cons;
 +	u16 sw_tx_prod;
 +	u16 num_tx_buffers; /* Slowpath only */
  
 -	u64				xmit_pkts;
 -	u64				stopped_cnt;
 -	u64				tx_mem_alloc_err;
 +	u64 xmit_pkts;
 +	u64 stopped_cnt;
 +	u64 tx_mem_alloc_err;
  
 -	__le16				*hw_cons_ptr;
 +	__le16 *hw_cons_ptr;
  
  	/* Needed for the mapping of packets */
 -	struct device			*dev;
 +	struct device *dev;
  
++<<<<<<< HEAD
 +	void __iomem *doorbell_addr;
 +	union db_prod tx_db;
 +	int index; /* Slowpath only */
++=======
+ 	void __iomem			*doorbell_addr;
+ 	union db_prod			tx_db;
+ 
+ 	/* Spinlock for XDP queues in case of XDP_REDIRECT */
+ 	spinlock_t			xdp_tx_lock;
+ 
+ 	int				index; /* Slowpath only */
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  #define QEDE_TXQ_XDP_TO_IDX(edev, txq)	((txq)->index - \
  					 QEDE_MAX_TSS_CNT(edev))
  #define QEDE_TXQ_IDX_TO_XDP(edev, idx)	((idx) + QEDE_MAX_TSS_CNT(edev))
@@@ -434,22 -449,27 +457,44 @@@
  #define BD_UNMAP_LEN(bd)		(le16_to_cpu((bd)->nbytes))
  
  struct qede_fastpath {
 -	struct qede_dev			*edev;
 -
 +	struct qede_dev	*edev;
 +#define QEDE_FASTPATH_TX	BIT(0)
 +#define QEDE_FASTPATH_RX	BIT(1)
 +#define QEDE_FASTPATH_XDP	BIT(2)
 +#define QEDE_FASTPATH_COMBINED	(QEDE_FASTPATH_TX | QEDE_FASTPATH_RX)
 +	u8			type;
 +	u8			id;
 +	u8			xdp_xmit;
 +	struct napi_struct	napi;
 +	struct qed_sb_info	*sb_info;
 +	struct qede_rx_queue	*rxq;
 +	struct qede_tx_queue	*txq;
 +	struct qede_tx_queue	*xdp_tx;
 +
++<<<<<<< HEAD
 +#define VEC_NAME_SIZE  (FIELD_SIZEOF(struct net_device, name) + 8)
 +	char	name[VEC_NAME_SIZE];
++=======
+ 	u8				type;
+ #define QEDE_FASTPATH_TX		BIT(0)
+ #define QEDE_FASTPATH_RX		BIT(1)
+ #define QEDE_FASTPATH_XDP		BIT(2)
+ #define QEDE_FASTPATH_COMBINED		(QEDE_FASTPATH_TX | QEDE_FASTPATH_RX)
+ 
+ 	u8				id;
+ 
+ 	u8				xdp_xmit;
+ #define QEDE_XDP_TX			BIT(0)
+ #define QEDE_XDP_REDIRECT		BIT(1)
+ 
+ 	struct napi_struct		napi;
+ 	struct qed_sb_info		*sb_info;
+ 	struct qede_rx_queue		*rxq;
+ 	struct qede_tx_queue		*txq;
+ 	struct qede_tx_queue		*xdp_tx;
+ 
+ 	char				name[IFNAMSIZ + 8];
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  };
  
  /* Debug print definitions */
@@@ -502,9 -522,10 +547,11 @@@ struct qede_reload_args 
  
  /* Datapath functions definition */
  netdev_tx_t qede_start_xmit(struct sk_buff *skb, struct net_device *ndev);
+ int qede_xdp_transmit(struct net_device *dev, int n_frames,
+ 		      struct xdp_frame **frames, u32 flags);
  u16 qede_select_queue(struct net_device *dev, struct sk_buff *skb,
 -		      struct net_device *sb_dev);
 +		      struct net_device *sb_dev,
 +		      select_queue_fallback_t fallback);
  netdev_features_t qede_features_check(struct sk_buff *skb,
  				      struct net_device *dev,
  				      netdev_features_t features);
diff --cc drivers/net/ethernet/qlogic/qede/qede_fp.c
index 0778a22f64ba,a2494bf85007..000000000000
--- a/drivers/net/ethernet/qlogic/qede/qede_fp.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_fp.c
@@@ -301,48 -302,38 +301,60 @@@ static inline void qede_update_tx_produ
  	wmb();
  }
  
++<<<<<<< HEAD
 +static int qede_xdp_xmit(struct qede_dev *edev, struct qede_fastpath *fp,
 +			 struct sw_rx_data *metadata, u16 padding, u16 length)
++=======
+ static int qede_xdp_xmit(struct qede_tx_queue *txq, dma_addr_t dma, u16 pad,
+ 			 u16 len, struct page *page, struct xdp_frame *xdpf)
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  {
 -	struct eth_tx_1st_bd *bd;
 -	struct sw_tx_xdp *xdp;
 +	struct qede_tx_queue *txq = fp->xdp_tx;
 +	struct eth_tx_1st_bd *first_bd;
 +	u16 idx = txq->sw_tx_prod;
  	u16 val;
  
 -	if (unlikely(qed_chain_get_elem_used(&txq->tx_pbl) >=
 -		     txq->num_tx_buffers)) {
 +	if (!qed_chain_get_elem_left(&txq->tx_pbl)) {
  		txq->stopped_cnt++;
  		return -ENOMEM;
  	}
  
 -	bd = qed_chain_produce(&txq->tx_pbl);
 -	bd->data.nbds = 1;
 -	bd->data.bd_flags.bitfields = BIT(ETH_TX_1ST_BD_FLAGS_START_BD_SHIFT);
 +	first_bd = (struct eth_tx_1st_bd *)qed_chain_produce(&txq->tx_pbl);
  
 -	val = (len & ETH_TX_DATA_1ST_BD_PKT_LEN_MASK) <<
 +	memset(first_bd, 0, sizeof(*first_bd));
 +	first_bd->data.bd_flags.bitfields =
 +	    BIT(ETH_TX_1ST_BD_FLAGS_START_BD_SHIFT);
 +
 +	val = (length & ETH_TX_DATA_1ST_BD_PKT_LEN_MASK) <<
  	       ETH_TX_DATA_1ST_BD_PKT_LEN_SHIFT;
  
 -	bd->data.bitfields = cpu_to_le16(val);
 +	first_bd->data.bitfields |= cpu_to_le16(val);
 +	first_bd->data.nbds = 1;
  
  	/* We can safely ignore the offset, as it's 0 for XDP */
 -	BD_SET_UNMAP_ADDR_LEN(bd, dma + pad, len);
 +	BD_SET_UNMAP_ADDR_LEN(first_bd, metadata->mapping + padding, length);
  
++<<<<<<< HEAD
 +	/* Synchronize the buffer back to device, as program [probably]
 +	 * has changed it.
 +	 */
 +	dma_sync_single_for_device(&edev->pdev->dev,
 +				   metadata->mapping + padding,
 +				   length, PCI_DMA_TODEVICE);
++=======
+ 	xdp = txq->sw_tx_ring.xdp + txq->sw_tx_prod;
+ 	xdp->mapping = dma;
+ 	xdp->page = page;
+ 	xdp->xdpf = xdpf;
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  
 +	txq->sw_tx_ring.xdp[idx].page = metadata->data;
 +	txq->sw_tx_ring.xdp[idx].mapping = metadata->mapping;
  	txq->sw_tx_prod = (txq->sw_tx_prod + 1) % txq->num_tx_buffers;
  
 +	/* Mark the fastpath for future XDP doorbell */
 +	fp->xdp_xmit = 1;
 +
  	return 0;
  }
  
@@@ -361,20 -405,31 +426,45 @@@ int qede_txq_has_work(struct qede_tx_qu
  
  static void qede_xdp_tx_int(struct qede_dev *edev, struct qede_tx_queue *txq)
  {
++<<<<<<< HEAD
 +	u16 hw_bd_cons, idx;
++=======
+ 	struct sw_tx_xdp *xdp_info, *xdp_arr = txq->sw_tx_ring.xdp;
+ 	struct device *dev = &edev->pdev->dev;
+ 	struct xdp_frame *xdpf;
+ 	u16 hw_bd_cons;
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  
  	hw_bd_cons = le16_to_cpu(*txq->hw_cons_ptr);
  	barrier();
  
  	while (hw_bd_cons != qed_chain_get_cons_idx(&txq->tx_pbl)) {
++<<<<<<< HEAD
++=======
+ 		xdp_info = xdp_arr + txq->sw_tx_cons;
+ 		xdpf = xdp_info->xdpf;
+ 
+ 		if (xdpf) {
+ 			dma_unmap_single(dev, xdp_info->mapping, xdpf->len,
+ 					 DMA_TO_DEVICE);
+ 			xdp_return_frame(xdpf);
+ 
+ 			xdp_info->xdpf = NULL;
+ 		} else {
+ 			dma_unmap_page(dev, xdp_info->mapping, PAGE_SIZE,
+ 				       DMA_BIDIRECTIONAL);
+ 			__free_page(xdp_info->page);
+ 		}
+ 
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  		qed_chain_consume(&txq->tx_pbl);
 +		idx = txq->sw_tx_cons;
 +
 +		dma_unmap_page(&edev->pdev->dev,
 +			       txq->sw_tx_ring.xdp[idx].mapping,
 +			       PAGE_SIZE, DMA_BIDIRECTIONAL);
 +		__free_page(txq->sw_tx_ring.xdp[idx].page);
 +
  		txq->sw_tx_cons = (txq->sw_tx_cons + 1) % txq->num_tx_buffers;
  		txq->xmit_pkts++;
  	}
@@@ -1072,23 -1128,49 +1162,54 @@@ static bool qede_rx_xdp(struct qede_de
  		/* Now if there's a transmission problem, we'd still have to
  		 * throw current buffer, as replacement was already allocated.
  		 */
++<<<<<<< HEAD
 +		if (qede_xdp_xmit(edev, fp, bd, *data_offset, *len)) {
 +			dma_unmap_page(rxq->dev, bd->mapping,
 +				       PAGE_SIZE, DMA_BIDIRECTIONAL);
++=======
+ 		if (unlikely(qede_xdp_xmit(fp->xdp_tx, bd->mapping,
+ 					   *data_offset, *len, bd->data,
+ 					   NULL))) {
+ 			dma_unmap_page(rxq->dev, bd->mapping, PAGE_SIZE,
+ 				       rxq->data_direction);
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  			__free_page(bd->data);
 -
  			trace_xdp_exception(edev->ndev, prog, act);
 -		} else {
 -			dma_sync_single_for_device(rxq->dev,
 -						   bd->mapping + *data_offset,
 -						   *len, rxq->data_direction);
 -			fp->xdp_xmit |= QEDE_XDP_TX;
  		}
  
  		/* Regardless, we've consumed an Rx BD */
  		qede_rx_bd_ring_consume(rxq);
++<<<<<<< HEAD
 +		return false;
 +
++=======
+ 		break;
+ 	case XDP_REDIRECT:
+ 		/* We need the replacement buffer before transmit. */
+ 		if (unlikely(qede_alloc_rx_buffer(rxq, true))) {
+ 			qede_recycle_rx_bd_ring(rxq, 1);
+ 
+ 			trace_xdp_exception(edev->ndev, prog, act);
+ 			break;
+ 		}
+ 
+ 		dma_unmap_page(rxq->dev, bd->mapping, PAGE_SIZE,
+ 			       rxq->data_direction);
+ 
+ 		if (unlikely(xdp_do_redirect(edev->ndev, &xdp, prog)))
+ 			DP_NOTICE(edev, "Failed to redirect the packet\n");
+ 		else
+ 			fp->xdp_xmit |= QEDE_XDP_REDIRECT;
+ 
+ 		qede_rx_bd_ring_consume(rxq);
+ 		break;
++>>>>>>> d1b25b79e162 (qede: add .ndo_xdp_xmit() and XDP_REDIRECT support)
  	default:
  		bpf_warn_invalid_xdp_action(act);
 -		fallthrough;
 +		/* Fall through */
  	case XDP_ABORTED:
  		trace_xdp_exception(edev->ndev, prog, act);
 -		fallthrough;
 +		/* Fall through */
  	case XDP_DROP:
  		qede_recycle_rx_bd_ring(rxq, cqe->bd_num);
  	}
* Unmerged path drivers/net/ethernet/qlogic/qede/qede.h
* Unmerged path drivers/net/ethernet/qlogic/qede/qede_fp.c
diff --git a/drivers/net/ethernet/qlogic/qede/qede_main.c b/drivers/net/ethernet/qlogic/qede/qede_main.c
index d21af772aca7..b6a5c51200a5 100644
--- a/drivers/net/ethernet/qlogic/qede/qede_main.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_main.c
@@ -671,6 +671,7 @@ static const struct net_device_ops qede_netdev_ops = {
 #ifdef CONFIG_RFS_ACCEL
 	.ndo_rx_flow_steer	= qede_rx_flow_steer,
 #endif
+	.ndo_xdp_xmit		= qede_xdp_transmit,
 	.ndo_setup_tc		= qede_setup_tc_offload,
 };
 
@@ -711,6 +712,7 @@ static const struct net_device_ops qede_netdev_vf_xdp_ops = {
 	.ndo_udp_tunnel_del	= udp_tunnel_nic_del_port,
 	.ndo_features_check	= qede_features_check,
 	.ndo_bpf		= qede_xdp,
+	.ndo_xdp_xmit		= qede_xdp_transmit,
 };
 
 /* -------------------------------------------------------------------------
@@ -1711,6 +1713,7 @@ static void qede_init_fp(struct qede_dev *edev)
 {
 	int queue_id, rxq_index = 0, txq_index = 0;
 	struct qede_fastpath *fp;
+	bool init_xdp = false;
 
 	for_each_queue(queue_id) {
 		fp = &edev->fp_array[queue_id];
@@ -1722,6 +1725,9 @@ static void qede_init_fp(struct qede_dev *edev)
 			fp->xdp_tx->index = QEDE_TXQ_IDX_TO_XDP(edev,
 								rxq_index);
 			fp->xdp_tx->is_xdp = 1;
+
+			spin_lock_init(&fp->xdp_tx->xdp_tx_lock);
+			init_xdp = true;
 		}
 
 		if (fp->type & QEDE_FASTPATH_RX) {
@@ -1737,6 +1743,13 @@ static void qede_init_fp(struct qede_dev *edev)
 			/* Driver have no error path from here */
 			WARN_ON(xdp_rxq_info_reg(&fp->rxq->xdp_rxq, edev->ndev,
 						 fp->rxq->rxq_id) < 0);
+
+			if (xdp_rxq_info_reg_mem_model(&fp->rxq->xdp_rxq,
+						       MEM_TYPE_PAGE_ORDER0,
+						       NULL)) {
+				DP_NOTICE(edev,
+					  "Failed to register XDP memory model\n");
+			}
 		}
 
 		if (fp->type & QEDE_FASTPATH_TX) {
@@ -1762,6 +1775,11 @@ static void qede_init_fp(struct qede_dev *edev)
 		snprintf(fp->name, sizeof(fp->name), "%s-fp-%d",
 			 edev->ndev->name, queue_id);
 	}
+
+	if (init_xdp) {
+		edev->total_xdp_queues = QEDE_RSS_COUNT(edev);
+		DP_INFO(edev, "Total XDP queues: %u\n", edev->total_xdp_queues);
+	}
 }
 
 static int qede_set_real_num_queues(struct qede_dev *edev)
