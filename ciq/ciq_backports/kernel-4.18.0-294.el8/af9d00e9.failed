powerpc/mm/radix: Create separate mappings for hot-plugged memory

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
commit af9d00e93a4f062c5f160325d7b8f33336f6744e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/af9d00e9.failed

To enable memory unplug without splitting kernel page table
mapping, we force the max mapping size to the LMB size. LMB
size is the unit in which hypervisor will do memory add/remove
operation.

Pseries systems supports max LMB size of 256MB. Hence on pseries,
we now end up mapping memory with 2M page size instead of 1G. To improve
that we want hypervisor to hint the kernel about the hotplug
memory range. That was added that as part of

commit b6eca183e23e ("powerpc/kernel: Enables memory
hot-remove after reboot on pseries guests")

But PowerVM doesn't provide that hint yet. Once we get PowerVM
updated, we can then force the 2M mapping only to hot-pluggable
memory region using memblock_is_hotpluggable(). Till then
let's depend on LMB size for finding the mapping page size
for linear range.

With this change KVM guest will also be doing linear mapping with
2M page size.

The actual TLB benefit of mapping guest page table entries with
hugepage size can only be materialized if the partition scoped
entries are also using the same or higher page size. A guest using
1G hugetlbfs backing guest memory can have a performance impact with
the above change.

	Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
[mpe: Fold in fix from Aneesh spotted by lkp@intel.com]
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20200709131925.922266-5-aneesh.kumar@linux.ibm.com
(cherry picked from commit af9d00e93a4f062c5f160325d7b8f33336f6744e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/mm/book3s64/radix_pgtable.c
diff --cc arch/powerpc/mm/book3s64/radix_pgtable.c
index ea0a0f917c21,c5bf2ef73c36..000000000000
--- a/arch/powerpc/mm/book3s64/radix_pgtable.c
+++ b/arch/powerpc/mm/book3s64/radix_pgtable.c
@@@ -16,10 -13,10 +16,14 @@@
  #include <linux/memblock.h>
  #include <linux/of_fdt.h>
  #include <linux/mm.h>
 -#include <linux/hugetlb.h>
  #include <linux/string_helpers.h>
++<<<<<<< HEAD
 +#include <linux/stop_machine.h>
++=======
+ #include <linux/memory.h>
++>>>>>>> af9d00e93a4f (powerpc/mm/radix: Create separate mappings for hot-plugged memory)
  
 +#include <asm/pgtable.h>
  #include <asm/pgalloc.h>
  #include <asm/mmu_context.h>
  #include <asm/dma.h>
@@@ -35,20 -34,8 +39,21 @@@
  
  unsigned int mmu_pid_bits;
  unsigned int mmu_base_pid;
+ unsigned int radix_mem_block_size __ro_after_init;
  
 +static int native_register_process_table(unsigned long base, unsigned long pg_sz,
 +					 unsigned long table_size)
 +{
 +	unsigned long patb0, patb1;
 +
 +	patb0 = be64_to_cpu(partition_tb[0].patb0);
 +	patb1 = base | table_size | PATB_GR;
 +
 +	mmu_partition_table_set_entry(0, patb0, patb1);
 +
 +	return 0;
 +}
 +
  static __ref void *early_alloc_pgtable(unsigned long size, int nid,
  			unsigned long region_start, unsigned long region_end)
  {
@@@ -258,52 -267,36 +263,63 @@@ static inline void __meminit print_mapp
  
  static int __meminit create_physical_mapping(unsigned long start,
  					     unsigned long end,
++<<<<<<< HEAD
 +					     int nid)
++=======
+ 					     unsigned long max_mapping_size,
+ 					     int nid, pgprot_t _prot)
++>>>>>>> af9d00e93a4f (powerpc/mm/radix: Create separate mappings for hot-plugged memory)
  {
  	unsigned long vaddr, addr, mapping_size = 0;
 -	bool prev_exec, exec = false;
  	pgprot_t prot;
 -	int psize;
 +	unsigned long max_mapping_size;
 +#ifdef CONFIG_STRICT_KERNEL_RWX
 +	int split_text_mapping = 1;
 +#else
 +	int split_text_mapping = 0;
 +#endif
  
 -	start = ALIGN(start, PAGE_SIZE);
 +	start = _ALIGN_UP(start, PAGE_SIZE);
  	for (addr = start; addr < end; addr += mapping_size) {
  		unsigned long gap, previous_size;
  		int rc;
  
++<<<<<<< HEAD
 +		gap = end - addr;
++=======
+ 		gap = next_boundary(addr, end) - addr;
+ 		if (gap > max_mapping_size)
+ 			gap = max_mapping_size;
++>>>>>>> af9d00e93a4f (powerpc/mm/radix: Create separate mappings for hot-plugged memory)
  		previous_size = mapping_size;
 -		prev_exec = exec;
 +		max_mapping_size = PUD_SIZE;
  
 +retry:
  		if (IS_ALIGNED(addr, PUD_SIZE) && gap >= PUD_SIZE &&
 -		    mmu_psize_defs[MMU_PAGE_1G].shift) {
 +		    mmu_psize_defs[MMU_PAGE_1G].shift &&
 +		    PUD_SIZE <= max_mapping_size)
  			mapping_size = PUD_SIZE;
 -			psize = MMU_PAGE_1G;
 -		} else if (IS_ALIGNED(addr, PMD_SIZE) && gap >= PMD_SIZE &&
 -			   mmu_psize_defs[MMU_PAGE_2M].shift) {
 +		else if (IS_ALIGNED(addr, PMD_SIZE) && gap >= PMD_SIZE &&
 +			 mmu_psize_defs[MMU_PAGE_2M].shift)
  			mapping_size = PMD_SIZE;
 -			psize = MMU_PAGE_2M;
 -		} else {
 +		else
  			mapping_size = PAGE_SIZE;
 -			psize = mmu_virtual_psize;
 +
 +		if (split_text_mapping && (mapping_size == PUD_SIZE) &&
 +			(addr <= __pa_symbol(__init_begin)) &&
 +			(addr + mapping_size) >= __pa_symbol(_stext)) {
 +			max_mapping_size = PMD_SIZE;
 +			goto retry;
 +		}
 +
 +		if (split_text_mapping && (mapping_size == PMD_SIZE) &&
 +		    (addr <= __pa_symbol(__init_begin)) &&
 +		    (addr + mapping_size) >= __pa_symbol(_stext))
 +			mapping_size = PAGE_SIZE;
 +
 +		if (mapping_size != previous_size) {
 +			print_mapping(start, addr, previous_size);
 +			start = addr;
  		}
  
  		vaddr = (unsigned long)__va(addr);
@@@ -339,9 -343,16 +356,14 @@@ void __init radix_init_pgtable(void
  		 * page tables will be allocated within the range. No
  		 * need or a node (which we don't have yet).
  		 */
 -
 -		if ((reg->base + reg->size) >= RADIX_VMALLOC_START) {
 -			pr_warn("Outside the supported range\n");
 -			continue;
 -		}
 -
  		WARN_ON(create_physical_mapping(reg->base,
  						reg->base + reg->size,
++<<<<<<< HEAD
 +						-1));
++=======
+ 						radix_mem_block_size,
+ 						-1, PAGE_KERNEL));
++>>>>>>> af9d00e93a4f (powerpc/mm/radix: Create separate mappings for hot-plugged memory)
  	}
  
  	/* Find out how many PID bits are supported */
@@@ -889,9 -914,17 +972,19 @@@ static void __meminit remove_pagetable(
  	radix__flush_tlb_kernel_range(start, end);
  }
  
 -int __meminit radix__create_section_mapping(unsigned long start,
 -					    unsigned long end, int nid,
 -					    pgprot_t prot)
 +int __meminit radix__create_section_mapping(unsigned long start, unsigned long end, int nid)
  {
++<<<<<<< HEAD
 +	return create_physical_mapping(start, end, nid);
++=======
+ 	if (end >= RADIX_VMALLOC_START) {
+ 		pr_warn("Outside the supported range\n");
+ 		return -1;
+ 	}
+ 
+ 	return create_physical_mapping(__pa(start), __pa(end),
+ 				       radix_mem_block_size, nid, prot);
++>>>>>>> af9d00e93a4f (powerpc/mm/radix: Create separate mappings for hot-plugged memory)
  }
  
  int __meminit radix__remove_section_mapping(unsigned long start, unsigned long end)
diff --git a/arch/powerpc/include/asm/book3s/64/mmu.h b/arch/powerpc/include/asm/book3s/64/mmu.h
index 9c8c669a6b6a..170bf68a1a83 100644
--- a/arch/powerpc/include/asm/book3s/64/mmu.h
+++ b/arch/powerpc/include/asm/book3s/64/mmu.h
@@ -66,6 +66,11 @@ extern unsigned int mmu_pid_bits;
 /* Base PID to allocate from */
 extern unsigned int mmu_base_pid;
 
+/*
+ * memory block size used with radix translation.
+ */
+extern unsigned int __ro_after_init radix_mem_block_size;
+
 #define PRTB_SIZE_SHIFT	(mmu_pid_bits + 4)
 #define PRTB_ENTRIES	(1ul << mmu_pid_bits)
 
* Unmerged path arch/powerpc/mm/book3s64/radix_pgtable.c
diff --git a/arch/powerpc/platforms/powernv/setup.c b/arch/powerpc/platforms/powernv/setup.c
index 781940f55fd7..75fe7e14ca2a 100644
--- a/arch/powerpc/platforms/powernv/setup.c
+++ b/arch/powerpc/platforms/powernv/setup.c
@@ -388,7 +388,15 @@ static void pnv_kexec_cpu_down(int crash_shutdown, int secondary)
 #ifdef CONFIG_MEMORY_HOTPLUG_SPARSE
 static unsigned long pnv_memory_block_size(void)
 {
-	return 256UL * 1024 * 1024;
+	/*
+	 * We map the kernel linear region with 1GB large pages on radix. For
+	 * memory hot unplug to work our memory block size must be at least
+	 * this size.
+	 */
+	if (radix_enabled())
+		return radix_mem_block_size;
+	else
+		return 256UL * 1024 * 1024;
 }
 #endif
 
