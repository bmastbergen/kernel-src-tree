mm: only count actual rotations as LRU reclaim cost

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 264e90cc07f177adec17ee7cc154ddaa132f0b2d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/264e90cc.failed

When shrinking the active file list we rotate referenced pages only when
they're in an executable mapping.  The others get deactivated.  When it
comes to balancing scan pressure, though, we count all referenced pages as
rotated, even the deactivated ones.  Yet they do not carry the same cost
to the system: the deactivated page *might* refault later on, but the
deactivation is tangible progress toward freeing pages; rotations on the
other hand cost time and effort without getting any closer to freeing
memory.

Don't treat both events as equal.  The following patch will hook up LRU
balancing to cache and anon refaults, which are a much more concrete cost
signal for reclaiming one list over the other.  Thus, remove the maybe-IO
cost bias from page references, and only note the CPU cost for actual
rotations that prevent the pages from getting reclaimed.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Rik van Riel <riel@surriel.com>
Link: http://lkml.kernel.org/r/20200520232525.798933-11-hannes@cmpxchg.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 264e90cc07f177adec17ee7cc154ddaa132f0b2d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmscan.c
diff --cc mm/vmscan.c
index 709a0e80e054,3c89eac629f3..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -2106,7 -2063,8 +2105,12 @@@ static void shrink_active_list(unsigne
  			 * IO, plus JVM can create lots of anon VM_EXEC pages,
  			 * so we ignore them here.
  			 */
++<<<<<<< HEAD
 +			if ((vm_flags & VM_EXEC) && page_is_file_cache(page)) {
++=======
+ 			if ((vm_flags & VM_EXEC) && page_is_file_lru(page)) {
+ 				nr_rotated += hpage_nr_pages(page);
++>>>>>>> 264e90cc07f1 (mm: only count actual rotations as LRU reclaim cost)
  				list_add(&page->lru, &l_active);
  				continue;
  			}
@@@ -2122,15 -2080,13 +2126,13 @@@
  	 */
  	spin_lock_irq(&pgdat->lru_lock);
  	/*
- 	 * Count referenced pages from currently used mappings as rotated,
- 	 * even though only some of them are actually re-activated.  This
- 	 * helps balance scan pressure between file and anonymous pages in
- 	 * get_scan_count.
+ 	 * Rotating pages costs CPU without actually
+ 	 * progressing toward the reclaim goal.
  	 */
 -	lru_note_cost(lruvec, file, nr_rotated);
 +	reclaim_stat->recent_rotated[file] += nr_rotated;
  
 -	nr_activate = move_pages_to_lru(lruvec, &l_active);
 -	nr_deactivate = move_pages_to_lru(lruvec, &l_inactive);
 +	nr_activate = move_active_pages_to_lru(lruvec, &l_active, lru);
 +	nr_deactivate = move_active_pages_to_lru(lruvec, &l_inactive, lru - LRU_ACTIVE);
  	/* Keep all free pages in l_active list */
  	list_splice(&l_inactive, &l_active);
  
* Unmerged path mm/vmscan.c
