x86/exceptions: Split debug IST stack

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 2a594d4ccf3f10f80b77d71bd3dad10813ac0137
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/2a594d4c.failed

The debug IST stack is actually two separate debug stacks to handle #DB
recursion. This is required because the CPU starts always at top of stack
on exception entry, which means on #DB recursion the second #DB would
overwrite the stack of the first.

The low level entry code therefore adjusts the top of stack on entry so a
secondary #DB starts from a different stack page. But the stack pages are
adjacent without a guard page between them.

Split the debug stack into 3 stacks which are separated by guard pages. The
3rd stack is never mapped into the cpu_entry_area and is only there to
catch triple #DB nesting:

      --- top of DB_stack	<- Initial stack
      --- end of DB_stack
      	  guard page

      --- top of DB1_stack	<- Top of stack after entering first #DB
      --- end of DB1_stack
      	  guard page

      --- top of DB2_stack	<- Top of stack after entering second #DB
      --- end of DB2_stack
      	  guard page

If DB2 would not act as the final guard hole, a second #DB would point the
top of #DB stack to the stack below #DB1 which would be valid and not catch
the not so desired triple nesting.

The backing store does not allocate any memory for DB2 and its guard page
as it is not going to be mapped into the cpu_entry_area.

 - Adjust the low level entry code so it adjusts top of #DB with the offset
   between the stacks instead of exception stack size.

 - Make the dumpstack code aware of the new stacks.

 - Adjust the in_debug_stack() implementation and move it into the NMI code
   where it belongs. As this is NMI hotpath code, it just checks the full
   area between top of DB_stack and bottom of DB1_stack without checking
   for the guard page. That's correct because the NMI cannot hit a
   stackpointer pointing to the guard page between DB and DB1 stack.  Even
   if it would, then the NMI operation still is unaffected, but the resume
   of the debug exception on the topmost DB stack will crash by touching
   the guard page.

  [ bp: Make exception_stack_names static const char * const ]

	Suggested-by: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Baoquan He <bhe@redhat.com>
	Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Dominik Brodowski <linux@dominikbrodowski.net>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Joerg Roedel <jroedel@suse.de>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: linux-doc@vger.kernel.org
	Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Qian Cai <cai@lca.pw>
	Cc: Sean Christopherson <sean.j.christopherson@intel.com>
	Cc: x86-ml <x86@kernel.org>
Link: https://lkml.kernel.org/r/20190414160145.439944544@linutronix.de
(cherry picked from commit 2a594d4ccf3f10f80b77d71bd3dad10813ac0137)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/x86/kernel-stacks
#	arch/x86/entry/entry_64.S
#	arch/x86/include/asm/cpu_entry_area.h
#	arch/x86/kernel/cpu/common.c
#	arch/x86/kernel/dumpstack_64.c
#	arch/x86/mm/cpu_entry_area.c
diff --cc Documentation/x86/kernel-stacks
index 9a0aa4d3a866,d1bfb0b95ee0..000000000000
--- a/Documentation/x86/kernel-stacks
+++ b/Documentation/x86/kernel-stacks
@@@ -76,7 -76,7 +76,11 @@@ The currently assigned IST stacks are :
    middle of switching stacks.  Using IST for NMI events avoids making
    assumptions about the previous state of the kernel stack.
  
++<<<<<<< HEAD
 +* DEBUG_STACK.  DEBUG_STKSZ
++=======
+ * ESTACK_DB.  EXCEPTION_STKSZ (PAGE_SIZE).
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  
    Used for hardware debug interrupts (interrupt 1) and for software
    debug interrupts (INT3).
@@@ -86,7 -86,12 +90,16 @@@
    avoids making assumptions about the previous state of the kernel
    stack.
  
++<<<<<<< HEAD
 +* MCE_STACK.  EXCEPTION_STKSZ (PAGE_SIZE).
++=======
+   To handle nested #DB correctly there exist two instances of DB stacks. On
+   #DB entry the IST stackpointer for #DB is switched to the second instance
+   so a nested #DB starts from a clean stack. The nested #DB switches
+   the IST stackpointer to a guard hole to catch triple nesting.
+ 
+ * ESTACK_MCE.  EXCEPTION_STKSZ (PAGE_SIZE).
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  
    Used for interrupt 18 - Machine Check Exception (#MC).
  
diff --cc arch/x86/entry/entry_64.S
index 00e63c77d5d4,ee649f1f279e..000000000000
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@@ -907,7 -879,7 +907,11 @@@ apicinterrupt IRQ_WORK_VECTOR			irq_wor
   * @paranoid == 2 is special: the stub will never switch stacks.  This is for
   * #DF: if the thread stack is somehow unusable, we'll still get a useful OOPS.
   */
++<<<<<<< HEAD
 +.macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1 create_gap=0
++=======
+ .macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1 ist_offset=0
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  ENTRY(\sym)
  	UNWIND_HINT_IRET_REGS offset=\has_error_code*8
  
@@@ -1175,8 -1129,8 +1179,13 @@@ apicinterrupt3 HYPERV_STIMER0_VECTOR 
  	hv_stimer0_callback_vector hv_stimer0_vector_handler
  #endif /* CONFIG_HYPERV */
  
++<<<<<<< HEAD
 +idtentry debug			do_debug		has_error_code=0	paranoid=1 shift_ist=DEBUG_STACK
 +idtentry int3			do_int3			has_error_code=0	create_gap=1
++=======
+ idtentry debug			do_debug		has_error_code=0	paranoid=1 shift_ist=IST_INDEX_DB ist_offset=DB_STACK_OFFSET
+ idtentry int3			do_int3			has_error_code=0
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  idtentry stack_segment		do_stack_segment	has_error_code=1
  
  #ifdef CONFIG_XEN_PV
diff --cc arch/x86/include/asm/cpu_entry_area.h
index 29c706415443,cff3f3f3bfe0..000000000000
--- a/arch/x86/include/asm/cpu_entry_area.h
+++ b/arch/x86/include/asm/cpu_entry_area.h
@@@ -7,6 -7,64 +7,67 @@@
  #include <asm/processor.h>
  #include <asm/intel_ds.h>
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_64
+ 
+ /* Macro to enforce the same ordering and stack sizes */
+ #define ESTACKS_MEMBERS(guardsize, db2_holesize)\
+ 	char	DF_stack_guard[guardsize];	\
+ 	char	DF_stack[EXCEPTION_STKSZ];	\
+ 	char	NMI_stack_guard[guardsize];	\
+ 	char	NMI_stack[EXCEPTION_STKSZ];	\
+ 	char	DB2_stack_guard[guardsize];	\
+ 	char	DB2_stack[db2_holesize];	\
+ 	char	DB1_stack_guard[guardsize];	\
+ 	char	DB1_stack[EXCEPTION_STKSZ];	\
+ 	char	DB_stack_guard[guardsize];	\
+ 	char	DB_stack[EXCEPTION_STKSZ];	\
+ 	char	MCE_stack_guard[guardsize];	\
+ 	char	MCE_stack[EXCEPTION_STKSZ];	\
+ 	char	IST_top_guard[guardsize];	\
+ 
+ /* The exception stacks' physical storage. No guard pages required */
+ struct exception_stacks {
+ 	ESTACKS_MEMBERS(0, 0)
+ };
+ 
+ /* The effective cpu entry area mapping with guard pages. */
+ struct cea_exception_stacks {
+ 	ESTACKS_MEMBERS(PAGE_SIZE, EXCEPTION_STKSZ)
+ };
+ 
+ /*
+  * The exception stack ordering in [cea_]exception_stacks
+  */
+ enum exception_stack_ordering {
+ 	ESTACK_DF,
+ 	ESTACK_NMI,
+ 	ESTACK_DB2,
+ 	ESTACK_DB1,
+ 	ESTACK_DB,
+ 	ESTACK_MCE,
+ 	N_EXCEPTION_STACKS
+ };
+ 
+ #define CEA_ESTACK_SIZE(st)					\
+ 	sizeof(((struct cea_exception_stacks *)0)->st## _stack)
+ 
+ #define CEA_ESTACK_BOT(ceastp, st)				\
+ 	((unsigned long)&(ceastp)->st## _stack)
+ 
+ #define CEA_ESTACK_TOP(ceastp, st)				\
+ 	(CEA_ESTACK_BOT(ceastp, st) + CEA_ESTACK_SIZE(st))
+ 
+ #define CEA_ESTACK_OFFS(st)					\
+ 	offsetof(struct cea_exception_stacks, st## _stack)
+ 
+ #define CEA_ESTACK_PAGES					\
+ 	(sizeof(struct cea_exception_stacks) / PAGE_SIZE)
+ 
+ #endif
+ 
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  /*
   * cpu_entry_area is a percpu region that contains things needed by the CPU
   * and early entry/exit code.  Real types aren't used for all fields here
diff --cc arch/x86/kernel/cpu/common.c
index 180f821f5d4d,88cab45707a9..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -1836,16 -1720,11 +1826,24 @@@ void cpu_init(void
  	/*
  	 * set up and load the per-CPU TSS
  	 */
++<<<<<<< HEAD
 +	if (!oist->ist[0]) {
 +		char *estacks = get_cpu_entry_area(cpu)->exception_stacks;
 +
 +		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
 +			estacks += exception_stack_sizes[v];
 +			oist->ist[v] = t->x86_tss.ist[v] =
 +					(unsigned long)estacks;
 +			if (v == DEBUG_STACK-1)
 +				per_cpu(debug_stack_addr, cpu) = (unsigned long)estacks;
 +		}
++=======
+ 	if (!t->x86_tss.ist[0]) {
+ 		t->x86_tss.ist[IST_INDEX_DF] = __this_cpu_ist_top_va(DF);
+ 		t->x86_tss.ist[IST_INDEX_NMI] = __this_cpu_ist_top_va(NMI);
+ 		t->x86_tss.ist[IST_INDEX_DB] = __this_cpu_ist_top_va(DB);
+ 		t->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  	}
  
  	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
diff --cc arch/x86/kernel/dumpstack_64.c
index 16a780db77dc,fca97bd3d8ae..000000000000
--- a/arch/x86/kernel/dumpstack_64.c
+++ b/arch/x86/kernel/dumpstack_64.c
@@@ -16,18 -16,16 +16,28 @@@
  #include <linux/bug.h>
  #include <linux/nmi.h>
  
 -#include <asm/cpu_entry_area.h>
  #include <asm/stacktrace.h>
  
++<<<<<<< HEAD
 +static char *exception_stack_names[N_EXCEPTION_STACKS] = {
 +		[ DOUBLEFAULT_STACK-1	]	= "#DF",
 +		[ NMI_STACK-1		]	= "NMI",
 +		[ DEBUG_STACK-1		]	= "#DB",
 +		[ MCE_STACK-1		]	= "#MC",
 +};
 +
 +static unsigned long exception_stack_sizes[N_EXCEPTION_STACKS] = {
 +	[0 ... N_EXCEPTION_STACKS - 1]		= EXCEPTION_STKSZ,
 +	[DEBUG_STACK - 1]			= DEBUG_STKSZ
++=======
+ static const char * const exception_stack_names[] = {
+ 		[ ESTACK_DF	]	= "#DF",
+ 		[ ESTACK_NMI	]	= "NMI",
+ 		[ ESTACK_DB2	]	= "#DB2",
+ 		[ ESTACK_DB1	]	= "#DB1",
+ 		[ ESTACK_DB	]	= "#DB",
+ 		[ ESTACK_MCE	]	= "#MC",
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  };
  
  const char *stack_type_name(enum stack_type type)
@@@ -52,20 -50,41 +62,42 @@@
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ struct estack_layout {
+ 	unsigned int	begin;
+ 	unsigned int	end;
+ };
+ 
+ #define	ESTACK_ENTRY(x)	{						  \
+ 	.begin	= offsetof(struct cea_exception_stacks, x## _stack),	  \
+ 	.end	= offsetof(struct cea_exception_stacks, x## _stack_guard) \
+ 	}
+ 
+ static const struct estack_layout layout[] = {
+ 	[ ESTACK_DF	]	= ESTACK_ENTRY(DF),
+ 	[ ESTACK_NMI	]	= ESTACK_ENTRY(NMI),
+ 	[ ESTACK_DB2	]	= { .begin = 0, .end = 0},
+ 	[ ESTACK_DB1	]	= ESTACK_ENTRY(DB1),
+ 	[ ESTACK_DB	]	= ESTACK_ENTRY(DB),
+ 	[ ESTACK_MCE	]	= ESTACK_ENTRY(MCE),
+ };
+ 
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  static bool in_exception_stack(unsigned long *stack, struct stack_info *info)
  {
 -	unsigned long estacks, begin, end, stk = (unsigned long)stack;
 +	unsigned long *begin, *end;
  	struct pt_regs *regs;
 -	unsigned int k;
 +	unsigned k;
  
- 	BUILD_BUG_ON(N_EXCEPTION_STACKS != 4);
+ 	BUILD_BUG_ON(N_EXCEPTION_STACKS != 6);
  
 -	estacks = (unsigned long)__this_cpu_read(cea_exception_stacks);
 -
  	for (k = 0; k < N_EXCEPTION_STACKS; k++) {
 -		begin = estacks + layout[k].begin;
 -		end   = estacks + layout[k].end;
 +		end   = (unsigned long *)raw_cpu_ptr(&orig_ist)->ist[k];
 +		begin = end - (exception_stack_sizes[k] / sizeof(long));
  		regs  = (struct pt_regs *)end - 1;
  
 -		if (stk < begin || stk >= end)
 +		if (stack < begin || stack >= end)
  			continue;
  
  		info->type	= STACK_TYPE_EXCEPTION + k;
diff --cc arch/x86/mm/cpu_entry_area.c
index 26fa2a5a8715,752ad11d6868..000000000000
--- a/arch/x86/mm/cpu_entry_area.c
+++ b/arch/x86/mm/cpu_entry_area.c
@@@ -78,9 -78,43 +78,45 @@@ static void __init percpu_setup_debug_s
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_64
+ 
+ #define cea_map_stack(name) do {					\
+ 	npages = sizeof(estacks->name## _stack) / PAGE_SIZE;		\
+ 	cea_map_percpu_pages(cea->estacks.name## _stack,		\
+ 			estacks->name## _stack, npages, PAGE_KERNEL);	\
+ 	} while (0)
+ 
+ static void __init percpu_setup_exception_stacks(unsigned int cpu)
+ {
+ 	struct exception_stacks *estacks = per_cpu_ptr(&exception_stacks, cpu);
+ 	struct cpu_entry_area *cea = get_cpu_entry_area(cpu);
+ 	unsigned int npages;
+ 
+ 	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);
+ 
+ 	per_cpu(cea_exception_stacks, cpu) = &cea->estacks;
+ 
+ 	/*
+ 	 * The exceptions stack mappings in the per cpu area are protected
+ 	 * by guard pages so each stack must be mapped separately. DB2 is
+ 	 * not mapped; it just exists to catch triple nesting of #DB.
+ 	 */
+ 	cea_map_stack(DF);
+ 	cea_map_stack(NMI);
+ 	cea_map_stack(DB1);
+ 	cea_map_stack(DB);
+ 	cea_map_stack(MCE);
+ }
+ #else
+ static inline void percpu_setup_exception_stacks(unsigned int cpu) {}
+ #endif
+ 
++>>>>>>> 2a594d4ccf3f (x86/exceptions: Split debug IST stack)
  /* Setup the fixmap mappings only once per-processor */
 -static void __init setup_cpu_entry_area(unsigned int cpu)
 +static void __init setup_cpu_entry_area(int cpu)
  {
 -	struct cpu_entry_area *cea = get_cpu_entry_area(cpu);
  #ifdef CONFIG_X86_64
  	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */
  	pgprot_t gdt_prot = PAGE_KERNEL_RO;
* Unmerged path Documentation/x86/kernel-stacks
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/include/asm/cpu_entry_area.h
diff --git a/arch/x86/include/asm/debugreg.h b/arch/x86/include/asm/debugreg.h
index 4505ac2735ad..79a87962254c 100644
--- a/arch/x86/include/asm/debugreg.h
+++ b/arch/x86/include/asm/debugreg.h
@@ -104,11 +104,9 @@ static inline void debug_stack_usage_dec(void)
 {
 	__this_cpu_dec(debug_stack_usage);
 }
-int is_debug_stack(unsigned long addr);
 void debug_stack_set_zero(void);
 void debug_stack_reset(void);
 #else /* !X86_64 */
-static inline int is_debug_stack(unsigned long addr) { return 0; }
 static inline void debug_stack_set_zero(void) { }
 static inline void debug_stack_reset(void) { }
 static inline void debug_stack_usage_inc(void) { }
diff --git a/arch/x86/include/asm/page_64_types.h b/arch/x86/include/asm/page_64_types.h
index 24fbc1126554..e8b34d756edc 100644
--- a/arch/x86/include/asm/page_64_types.h
+++ b/arch/x86/include/asm/page_64_types.h
@@ -18,9 +18,6 @@
 #define EXCEPTION_STACK_ORDER (0 + KASAN_STACK_ORDER)
 #define EXCEPTION_STKSZ (PAGE_SIZE << EXCEPTION_STACK_ORDER)
 
-#define DEBUG_STACK_ORDER (EXCEPTION_STACK_ORDER + 1)
-#define DEBUG_STKSZ (PAGE_SIZE << DEBUG_STACK_ORDER)
-
 #define IRQ_STACK_ORDER (2 + KASAN_STACK_ORDER)
 #define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)
 
diff --git a/arch/x86/kernel/asm-offsets_64.c b/arch/x86/kernel/asm-offsets_64.c
index 3b9405e7ba2b..0a70c2a7bd07 100644
--- a/arch/x86/kernel/asm-offsets_64.c
+++ b/arch/x86/kernel/asm-offsets_64.c
@@ -65,6 +65,8 @@ int main(void)
 #undef ENTRY
 
 	OFFSET(TSS_ist, tss_struct, x86_tss.ist);
+	DEFINE(DB_STACK_OFFSET, offsetof(struct cea_exception_stacks, DB_stack) -
+	       offsetof(struct cea_exception_stacks, DB1_stack));
 	BLANK();
 
 #ifdef CONFIG_STACKPROTECTOR
* Unmerged path arch/x86/kernel/cpu/common.c
* Unmerged path arch/x86/kernel/dumpstack_64.c
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index 086cf1d1d71d..05b09896cfaf 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -21,13 +21,14 @@
 #include <linux/ratelimit.h>
 #include <linux/slab.h>
 #include <linux/export.h>
+#include <linux/atomic.h>
 #include <linux/sched/clock.h>
 
 #if defined(CONFIG_EDAC)
 #include <linux/edac.h>
 #endif
 
-#include <linux/atomic.h>
+#include <asm/cpu_entry_area.h>
 #include <asm/traps.h>
 #include <asm/mach_traps.h>
 #include <asm/nmi.h>
@@ -488,6 +489,23 @@ static DEFINE_PER_CPU(unsigned long, nmi_cr2);
  * switch back to the original IDT.
  */
 static DEFINE_PER_CPU(int, update_debug_stack);
+
+static bool notrace is_debug_stack(unsigned long addr)
+{
+	struct cea_exception_stacks *cs = __this_cpu_read(cea_exception_stacks);
+	unsigned long top = CEA_ESTACK_TOP(cs, DB);
+	unsigned long bot = CEA_ESTACK_BOT(cs, DB1);
+
+	if (__this_cpu_read(debug_stack_usage))
+		return true;
+	/*
+	 * Note, this covers the guard page between DB and DB1 as well to
+	 * avoid two checks. But by all means @addr can never point into
+	 * the guard page.
+	 */
+	return addr >= bot && addr < top;
+}
+NOKPROBE_SYMBOL(is_debug_stack);
 #endif
 
 dotraplinkage notrace void
* Unmerged path arch/x86/mm/cpu_entry_area.c
