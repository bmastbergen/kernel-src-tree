x86/exceptions: Enable IST guard pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 1bdb67e5aa2d5d43c48cb7d93393fcba276c9e71
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/1bdb67e5.failed

All usage sites which expected that the exception stacks in the CPU entry
area are mapped linearly are fixed up. Enable guard pages between the
IST stacks.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Sean Christopherson <sean.j.christopherson@intel.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: x86-ml <x86@kernel.org>
Link: https://lkml.kernel.org/r/20190414160145.349862042@linutronix.de
(cherry picked from commit 1bdb67e5aa2d5d43c48cb7d93393fcba276c9e71)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpu_entry_area.h
diff --cc arch/x86/include/asm/cpu_entry_area.h
index 29c706415443,9c96406e6d2b..000000000000
--- a/arch/x86/include/asm/cpu_entry_area.h
+++ b/arch/x86/include/asm/cpu_entry_area.h
@@@ -7,6 -7,58 +7,61 @@@
  #include <asm/processor.h>
  #include <asm/intel_ds.h>
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_64
+ 
+ /* Macro to enforce the same ordering and stack sizes */
+ #define ESTACKS_MEMBERS(guardsize)		\
+ 	char	DF_stack_guard[guardsize];	\
+ 	char	DF_stack[EXCEPTION_STKSZ];	\
+ 	char	NMI_stack_guard[guardsize];	\
+ 	char	NMI_stack[EXCEPTION_STKSZ];	\
+ 	char	DB_stack_guard[guardsize];	\
+ 	char	DB_stack[DEBUG_STKSZ];		\
+ 	char	MCE_stack_guard[guardsize];	\
+ 	char	MCE_stack[EXCEPTION_STKSZ];	\
+ 	char	IST_top_guard[guardsize];	\
+ 
+ /* The exception stacks' physical storage. No guard pages required */
+ struct exception_stacks {
+ 	ESTACKS_MEMBERS(0)
+ };
+ 
+ /* The effective cpu entry area mapping with guard pages. */
+ struct cea_exception_stacks {
+ 	ESTACKS_MEMBERS(PAGE_SIZE)
+ };
+ 
+ /*
+  * The exception stack ordering in [cea_]exception_stacks
+  */
+ enum exception_stack_ordering {
+ 	ESTACK_DF,
+ 	ESTACK_NMI,
+ 	ESTACK_DB,
+ 	ESTACK_MCE,
+ 	N_EXCEPTION_STACKS
+ };
+ 
+ #define CEA_ESTACK_SIZE(st)					\
+ 	sizeof(((struct cea_exception_stacks *)0)->st## _stack)
+ 
+ #define CEA_ESTACK_BOT(ceastp, st)				\
+ 	((unsigned long)&(ceastp)->st## _stack)
+ 
+ #define CEA_ESTACK_TOP(ceastp, st)				\
+ 	(CEA_ESTACK_BOT(ceastp, st) + CEA_ESTACK_SIZE(st))
+ 
+ #define CEA_ESTACK_OFFS(st)					\
+ 	offsetof(struct cea_exception_stacks, st## _stack)
+ 
+ #define CEA_ESTACK_PAGES					\
+ 	(sizeof(struct cea_exception_stacks) / PAGE_SIZE)
+ 
+ #endif
+ 
++>>>>>>> 1bdb67e5aa2d (x86/exceptions: Enable IST guard pages)
  /*
   * cpu_entry_area is a percpu region that contains things needed by the CPU
   * and early entry/exit code.  Real types aren't used for all fields here
* Unmerged path arch/x86/include/asm/cpu_entry_area.h
