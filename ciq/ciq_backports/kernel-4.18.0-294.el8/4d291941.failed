rcu/tree: Skip entry into the page allocator for PREEMPT_RT

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Joel Fernandes (Google) <joel@joelfernandes.org>
commit 4d2919411867848fab78c7cb13139e17ad8b85bc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/4d291941.failed

To keep the kfree_rcu() code working in purely atomic sections on RT,
such as non-threaded IRQ handlers and raw spinlock sections, avoid
calling into the page allocator which uses sleeping locks on RT.

In fact, even if the  caller is preemptible, the kfree_rcu() code is
not, as the krcp->lock is a raw spinlock.

Calling into the page allocator is optional and avoiding it should be
Ok, especially with the page pre-allocation support in future patches.
Such pre-allocation would further avoid the a need for a dynamically
allocated page in the first place.

	Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Reviewed-by: Uladzislau Rezki <urezki@gmail.com>
Co-developed-by: Uladzislau Rezki <urezki@gmail.com>
	Signed-off-by: Uladzislau Rezki <urezki@gmail.com>
	Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
	Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
(cherry picked from commit 4d2919411867848fab78c7cb13139e17ad8b85bc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index 4aa7b6bbcde6,e0425faf3b3b..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -2810,32 -3181,80 +2810,96 @@@ static void kfree_rcu_monitor(struct wo
  	if (krcp->monitor_todo)
  		kfree_rcu_drain_unlock(krcp, flags);
  	else
++<<<<<<< HEAD
 +		spin_unlock_irqrestore(&krcp->lock, flags);
++=======
+ 		raw_spin_unlock_irqrestore(&krcp->lock, flags);
+ }
+ 
+ static inline bool
+ kfree_call_rcu_add_ptr_to_bulk(struct kfree_rcu_cpu *krcp,
+ 	struct rcu_head *head, rcu_callback_t func)
+ {
+ 	struct kfree_rcu_bulk_data *bnode;
+ 
+ 	if (unlikely(!krcp->initialized))
+ 		return false;
+ 
+ 	lockdep_assert_held(&krcp->lock);
+ 
+ 	/* Check if a new block is required. */
+ 	if (!krcp->bhead ||
+ 			krcp->bhead->nr_records == KFREE_BULK_MAX_ENTR) {
+ 		bnode = xchg(&krcp->bcached, NULL);
+ 		if (!bnode) {
+ 			WARN_ON_ONCE(sizeof(struct kfree_rcu_bulk_data) > PAGE_SIZE);
+ 
+ 			/*
+ 			 * To keep this path working on raw non-preemptible
+ 			 * sections, prevent the optional entry into the
+ 			 * allocator as it uses sleeping locks. In fact, even
+ 			 * if the caller of kfree_rcu() is preemptible, this
+ 			 * path still is not, as krcp->lock is a raw spinlock.
+ 			 * With additional page pre-allocation in the works,
+ 			 * hitting this return is going to be much less likely.
+ 			 */
+ 			if (IS_ENABLED(CONFIG_PREEMPT_RT))
+ 				return false;
+ 
+ 			bnode = (struct kfree_rcu_bulk_data *)
+ 				__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
+ 		}
+ 
+ 		/* Switch to emergency path. */
+ 		if (unlikely(!bnode))
+ 			return false;
+ 
+ 		/* Initialize the new block. */
+ 		bnode->nr_records = 0;
+ 		bnode->next = krcp->bhead;
+ 		bnode->head_free_debug = NULL;
+ 
+ 		/* Attach it to the head. */
+ 		krcp->bhead = bnode;
+ 	}
+ 
+ #ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD
+ 	head->func = func;
+ 	head->next = krcp->bhead->head_free_debug;
+ 	krcp->bhead->head_free_debug = head;
+ #endif
+ 
+ 	/* Finally insert. */
+ 	krcp->bhead->records[krcp->bhead->nr_records++] =
+ 		(void *) head - (unsigned long) func;
+ 
+ 	return true;
++>>>>>>> 4d2919411867 (rcu/tree: Skip entry into the page allocator for PREEMPT_RT)
 +}
 +
 +/*
 + * This version of kfree_call_rcu does not do batching of kfree_rcu() requests.
 + * Used only by rcuperf torture test for comparison with kfree_rcu_batch().
 + */
 +void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func)
 +{
 +	__call_rcu(head, func, 1);
  }
 +EXPORT_SYMBOL_GPL(kfree_call_rcu_nobatch);
  
  /*
 - * Queue a request for lazy invocation of kfree_bulk()/kfree() after a grace
 - * period. Please note there are two paths are maintained, one is the main one
 - * that uses kfree_bulk() interface and second one is emergency one, that is
 - * used only when the main path can not be maintained temporary, due to memory
 - * pressure.
 + * Queue a request for lazy invocation of kfree() after a grace period.
   *
   * Each kfree_call_rcu() request is added to a batch. The batch will be drained
 - * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch will
 - * be free'd in workqueue context. This allows us to: batch requests together to
 - * reduce the number of grace periods during heavy kfree_rcu() load.
 + * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch
 + * will be kfree'd in workqueue context. This allows us to:
 + *
 + * 1.	Batch requests together to reduce the number of grace periods during
 + *	heavy kfree_rcu() load.
 + *
 + * 2.	It makes it possible to use kfree_bulk() on a large number of
 + *	kfree_rcu() requests thus reducing cache misses and the per-object
 + *	overhead of kfree().
   */
  void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
  {
* Unmerged path kernel/rcu/tree.c
