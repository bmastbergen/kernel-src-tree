mm: drop mmap_sem before calling balance_dirty_pages() in write fault

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 89b15332af7c0312a41e50846819ca6613b58b4c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/89b15332.failed

One of our services is observing hanging ps/top/etc under heavy write
IO, and the task states show this is an mmap_sem priority inversion:

A write fault is holding the mmap_sem in read-mode and waiting for
(heavily cgroup-limited) IO in balance_dirty_pages():

    balance_dirty_pages+0x724/0x905
    balance_dirty_pages_ratelimited+0x254/0x390
    fault_dirty_shared_page.isra.96+0x4a/0x90
    do_wp_page+0x33e/0x400
    __handle_mm_fault+0x6f0/0xfa0
    handle_mm_fault+0xe4/0x200
    __do_page_fault+0x22b/0x4a0
    page_fault+0x45/0x50

Somebody tries to change the address space, contending for the mmap_sem in
write-mode:

    call_rwsem_down_write_failed_killable+0x13/0x20
    do_mprotect_pkey+0xa8/0x330
    SyS_mprotect+0xf/0x20
    do_syscall_64+0x5b/0x100
    entry_SYSCALL_64_after_hwframe+0x3d/0xa2

The waiting writer locks out all subsequent readers to avoid lock
starvation, and several threads can be seen hanging like this:

    call_rwsem_down_read_failed+0x14/0x30
    proc_pid_cmdline_read+0xa0/0x480
    __vfs_read+0x23/0x140
    vfs_read+0x87/0x130
    SyS_read+0x42/0x90
    do_syscall_64+0x5b/0x100
    entry_SYSCALL_64_after_hwframe+0x3d/0xa2

To fix this, do what we do for cache read faults already: drop the
mmap_sem before calling into anything IO bound, in this case the
balance_dirty_pages() function, and return VM_FAULT_RETRY.

Link: http://lkml.kernel.org/r/20190924194238.GA29030@cmpxchg.org
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Josef Bacik <josef@toxicpanda.com>
	Cc: Hillf Danton <hdanton@sina.com>
	Cc: Hugh Dickins <hughd@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 89b15332af7c0312a41e50846819ca6613b58b4c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/filemap.c
diff --cc mm/filemap.c
index c5561788ab4c,bf6aa30be58d..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -2375,46 -2328,59 +2375,55 @@@ out
  EXPORT_SYMBOL(generic_file_read_iter);
  
  #ifdef CONFIG_MMU
++<<<<<<< HEAD
 +/**
 + * page_cache_read - adds requested page to the page cache if not already there
 + * @file:	file to read
 + * @offset:	page index
 + * @gfp_mask:	memory allocation flags
++=======
+ #define MMAP_LOTSAMISS  (100)
+ /*
+  * lock_page_maybe_drop_mmap - lock the page, possibly dropping the mmap_sem
+  * @vmf - the vm_fault for this fault.
+  * @page - the page to lock.
+  * @fpin - the pointer to the file we may pin (or is already pinned).
++>>>>>>> 89b15332af7c (mm: drop mmap_sem before calling balance_dirty_pages() in write fault)
   *
 - * This works similar to lock_page_or_retry in that it can drop the mmap_sem.
 - * It differs in that it actually returns the page locked if it returns 1 and 0
 - * if it couldn't lock the page.  If we did have to drop the mmap_sem then fpin
 - * will point to the pinned file and needs to be fput()'ed at a later point.
 + * This adds the requested page to the page cache if it isn't already there,
 + * and schedules an I/O to read in its contents from disk.
   */
 -static int lock_page_maybe_drop_mmap(struct vm_fault *vmf, struct page *page,
 -				     struct file **fpin)
 +static int page_cache_read(struct file *file, pgoff_t offset, gfp_t gfp_mask)
  {
 -	if (trylock_page(page))
 -		return 1;
 +	struct address_space *mapping = file->f_mapping;
 +	struct page *page;
 +	int ret;
  
 -	/*
 -	 * NOTE! This will make us return with VM_FAULT_RETRY, but with
 -	 * the mmap_sem still held. That's how FAULT_FLAG_RETRY_NOWAIT
 -	 * is supposed to work. We have way too many special cases..
 -	 */
 -	if (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)
 -		return 0;
 +	do {
 +		page = __page_cache_alloc(gfp_mask);
 +		if (!page)
 +			return -ENOMEM;
  
 -	*fpin = maybe_unlock_mmap_for_io(vmf, *fpin);
 -	if (vmf->flags & FAULT_FLAG_KILLABLE) {
 -		if (__lock_page_killable(page)) {
 -			/*
 -			 * We didn't have the right flags to drop the mmap_sem,
 -			 * but all fault_handlers only check for fatal signals
 -			 * if we return VM_FAULT_RETRY, so we need to drop the
 -			 * mmap_sem here and return 0 if we don't have a fpin.
 -			 */
 -			if (*fpin == NULL)
 -				up_read(&vmf->vma->vm_mm->mmap_sem);
 -			return 0;
 -		}
 -	} else
 -		__lock_page(page);
 -	return 1;
 +		ret = add_to_page_cache_lru(page, mapping, offset, gfp_mask);
 +		if (ret == 0)
 +			ret = mapping->a_ops->readpage(file, page);
 +		else if (ret == -EEXIST)
 +			ret = 0; /* losing race to add is OK */
 +
 +		put_page(page);
 +
 +	} while (ret == AOP_TRUNCATED_PAGE);
 +
 +	return ret;
  }
  
 +#define MMAP_LOTSAMISS  (100)
  
  /*
 - * Synchronous readahead happens when we don't even find a page in the page
 - * cache at all.  We don't want to perform IO under the mmap sem, so if we have
 - * to drop the mmap sem we return the file that was pinned in order for us to do
 - * that.  If we didn't pin a file then we return NULL.  The file that is
 - * returned needs to be fput()'ed when we're done with it.
 + * Synchronous readahead happens when we don't even find
 + * a page in the page cache at all.
   */
 -static struct file *do_sync_mmap_readahead(struct vm_fault *vmf)
 +static void do_sync_mmap_readahead(struct vm_fault *vmf)
  {
  	struct file *file = vmf->vma->vm_file;
  	struct file_ra_state *ra = &file->f_ra;
* Unmerged path mm/filemap.c
diff --git a/mm/internal.h b/mm/internal.h
index 344bbc72ed9a..a71d9c49b9c0 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -355,6 +355,27 @@ vma_address(struct page *page, struct vm_area_struct *vma)
 	return max(start, vma->vm_start);
 }
 
+static inline struct file *maybe_unlock_mmap_for_io(struct vm_fault *vmf,
+						    struct file *fpin)
+{
+	int flags = vmf->flags;
+
+	if (fpin)
+		return fpin;
+
+	/*
+	 * FAULT_FLAG_RETRY_NOWAIT means we don't want to wait on page locks or
+	 * anything, so we only pin the file and drop the mmap_sem if only
+	 * FAULT_FLAG_ALLOW_RETRY is set.
+	 */
+	if ((flags & (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT)) ==
+	    FAULT_FLAG_ALLOW_RETRY) {
+		fpin = get_file(vmf->vma->vm_file);
+		up_read(&vmf->vma->vm_mm->mmap_sem);
+	}
+	return fpin;
+}
+
 #else /* !CONFIG_MMU */
 static inline void clear_page_mlock(struct page *page) { }
 static inline void mlock_vma_page(struct page *page) { }
diff --git a/mm/memory.c b/mm/memory.c
index 583eb7e0dd7f..f4adbd098b36 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2507,10 +2507,11 @@ static vm_fault_t do_page_mkwrite(struct vm_fault *vmf)
  *
  * The function expects the page to be locked and unlocks it.
  */
-static void fault_dirty_shared_page(struct vm_area_struct *vma,
-				    struct page *page)
+static vm_fault_t fault_dirty_shared_page(struct vm_fault *vmf)
 {
+	struct vm_area_struct *vma = vmf->vma;
 	struct address_space *mapping;
+	struct page *page = vmf->page;
 	bool dirtied;
 	bool page_mkwrite = vma->vm_ops && vma->vm_ops->page_mkwrite;
 
@@ -2525,16 +2526,30 @@ static void fault_dirty_shared_page(struct vm_area_struct *vma,
 	mapping = page_rmapping(page);
 	unlock_page(page);
 
+	if (!page_mkwrite)
+		file_update_time(vma->vm_file);
+
+	/*
+	 * Throttle page dirtying rate down to writeback speed.
+	 *
+	 * mapping may be NULL here because some device drivers do not
+	 * set page.mapping but still dirty their pages
+	 *
+	 * Drop the mmap_sem before waiting on IO, if we can. The file
+	 * is pinning the mapping, as per above.
+	 */
 	if ((dirtied || page_mkwrite) && mapping) {
-		/*
-		 * Some device drivers do not set page.mapping
-		 * but still dirty their pages
-		 */
+		struct file *fpin;
+
+		fpin = maybe_unlock_mmap_for_io(vmf, NULL);
 		balance_dirty_pages_ratelimited(mapping);
+		if (fpin) {
+			fput(fpin);
+			return VM_FAULT_RETRY;
+		}
 	}
 
-	if (!page_mkwrite)
-		file_update_time(vma->vm_file);
+	return 0;
 }
 
 /*
@@ -2774,6 +2789,7 @@ static vm_fault_t wp_page_shared(struct vm_fault *vmf)
 	__releases(vmf->ptl)
 {
 	struct vm_area_struct *vma = vmf->vma;
+	vm_fault_t ret = VM_FAULT_WRITE;
 
 	get_page(vmf->page);
 
@@ -2797,10 +2813,10 @@ static vm_fault_t wp_page_shared(struct vm_fault *vmf)
 		wp_page_reuse(vmf);
 		lock_page(vmf->page);
 	}
-	fault_dirty_shared_page(vma, vmf->page);
+	ret |= fault_dirty_shared_page(vmf);
 	put_page(vmf->page);
 
-	return VM_FAULT_WRITE;
+	return ret;
 }
 
 /*
@@ -3847,7 +3863,7 @@ static vm_fault_t do_shared_fault(struct vm_fault *vmf)
 		return ret;
 	}
 
-	fault_dirty_shared_page(vma, vmf->page);
+	ret |= fault_dirty_shared_page(vmf);
 	return ret;
 }
 
