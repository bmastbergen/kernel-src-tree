RDMA/mlx5: Don't rely on FW to set zeros in ECE response

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit 92cd667c0e8a67de024134be0a6f0bdb320606a8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/92cd667c.failed

The FW returns zeros in case feature is not enabled, but it is better to
have the capability check and ensure that returned result is cleared.

Fixes: 3e09a427ae7a ("RDMA/mlx5: Get ECE options from FW during create QP")
Link: https://lore.kernel.org/r/20200602125548.172654-3-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 92cd667c0e8a67de024134be0a6f0bdb320606a8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 68ab87769d36,18135f908971..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -1960,15 -1837,95 +1960,93 @@@ static int get_atomic_mode(struct mlx5_
  	return atomic_mode;
  }
  
 -static int create_xrc_tgt_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 -			     struct mlx5_create_qp_params *params)
 +static inline bool check_flags_mask(uint64_t input, uint64_t supported)
  {
++<<<<<<< HEAD
 +	return (input & ~supported) == 0;
++=======
+ 	struct mlx5_ib_create_qp *ucmd = params->ucmd;
+ 	struct ib_qp_init_attr *attr = params->attr;
+ 	u32 uidx = params->uidx;
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	u32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_qp_base *base;
+ 	unsigned long flags;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	mutex_init(&qp->mutex);
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	in = kvzalloc(inlen, GFP_KERNEL);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	if (MLX5_CAP_GEN(mdev, ece_support))
+ 		MLX5_SET(create_qp_in, in, ece, ucmd->ece_options);
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, MLX5_QP_ST_XRC);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 	MLX5_SET(qpc, qpc, pd, to_mpd(devr->p0)->pdn);
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 	if (qp->flags & IB_QP_CREATE_CROSS_CHANNEL)
+ 		MLX5_SET(qpc, qpc, cd_master, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_SEND)
+ 		MLX5_SET(qpc, qpc, cd_slave_send, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_RECV)
+ 		MLX5_SET(qpc, qpc, cd_slave_receive, 1);
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, MLX5_SRQ_RQ);
+ 	MLX5_SET(qpc, qpc, no_sq, 1);
+ 	MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, cqn_snd, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, srqn_rmpn_xrqn, to_msrq(devr->s0)->msrq.srqn);
+ 	MLX5_SET(qpc, qpc, xrcd, to_mxrcd(attr->xrcd)->xrcdn);
+ 	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
+ 
+ 	/* 0xffffff means we ask to work with cqe version 0 */
+ 	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
+ 		MLX5_SET(qpc, qpc, user_index, uidx);
+ 
+ 	if (qp->flags & IB_QP_CREATE_PCI_WRITE_END_PADDING) {
+ 		MLX5_SET(qpc, qpc, end_padding_mode,
+ 			 MLX5_WQ_END_PAD_MODE_ALIGN);
+ 		/* Special case to clean flag */
+ 		qp->flags &= ~IB_QP_CREATE_PCI_WRITE_END_PADDING;
+ 	}
+ 
+ 	base = &qp->trans_qp.base;
+ 	err = mlx5_qpc_create_qp(dev, &base->mqp, in, inlen, out);
+ 	kvfree(in);
+ 	if (err)
+ 		return err;
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 	if (MLX5_CAP_GEN(mdev, ece_support))
+ 		params->resp.ece_options = MLX5_GET(create_qp_out, out, ece);
+ 
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	qp->trans_qp.xrcdn = to_mxrcd(attr->xrcd)->xrcdn;
+ 	return 0;
++>>>>>>> 92cd667c0e8a (RDMA/mlx5: Don't rely on FW to set zeros in ECE response)
  }
  
 -static int create_user_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 -			  struct mlx5_ib_qp *qp,
 -			  struct mlx5_create_qp_params *params)
 +static int create_qp_common(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata, struct mlx5_ib_qp *qp)
  {
 -	struct ib_qp_init_attr *init_attr = params->attr;
 -	struct mlx5_ib_create_qp *ucmd = params->ucmd;
 -	u32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};
 -	struct ib_udata *udata = params->udata;
 -	u32 uidx = params->uidx;
  	struct mlx5_ib_resources *devr = &dev->devr;
  	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
  	struct mlx5_core_dev *mdev = dev->mdev;
@@@ -2305,21 -2069,149 +2383,137 @@@
  	}
  
  	if (init_attr->qp_type == IB_QPT_RAW_PACKET ||
 -	    qp->flags & IB_QP_CREATE_SOURCE_QPN) {
 -		qp->raw_packet_qp.sq.ubuffer.buf_addr = ucmd->sq_buf_addr;
 +	    qp->flags & MLX5_IB_QP_UNDERLAY) {
 +		qp->raw_packet_qp.sq.ubuffer.buf_addr = ucmd.sq_buf_addr;
  		raw_packet_qp_copy_info(qp, &qp->raw_packet_qp);
  		err = create_raw_packet_qp(dev, qp, in, inlen, pd, udata,
++<<<<<<< HEAD
 +					   &resp);
++=======
+ 					   &params->resp);
+ 	} else
+ 		err = mlx5_qpc_create_qp(dev, &base->mqp, in, inlen, out);
+ 
+ 	kvfree(in);
+ 	if (err)
+ 		goto err_create;
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 	if (MLX5_CAP_GEN(mdev, ece_support))
+ 		params->resp.ece_options = MLX5_GET(create_qp_out, out, ece);
+ 
+ 	get_cqs(qp->type, init_attr->send_cq, init_attr->recv_cq,
+ 		&send_cq, &recv_cq);
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	mlx5_ib_lock_cqs(send_cq, recv_cq);
+ 	/* Maintain device to QPs access, needed for further handling via reset
+ 	 * flow
+ 	 */
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	/* Maintain CQ to QPs access, needed for further handling via reset flow
+ 	 */
+ 	if (send_cq)
+ 		list_add_tail(&qp->cq_send_list, &send_cq->list_send_qp);
+ 	if (recv_cq)
+ 		list_add_tail(&qp->cq_recv_list, &recv_cq->list_recv_qp);
+ 	mlx5_ib_unlock_cqs(send_cq, recv_cq);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	return 0;
+ 
+ err_create:
+ 	destroy_qp(dev, qp, base, udata);
+ 	return err;
+ }
+ 
+ static int create_kernel_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 			    struct mlx5_ib_qp *qp,
+ 			    struct mlx5_create_qp_params *params)
+ {
+ 	struct ib_qp_init_attr *attr = params->attr;
+ 	u32 uidx = params->uidx;
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	u32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_cq *send_cq;
+ 	struct mlx5_ib_cq *recv_cq;
+ 	unsigned long flags;
+ 	struct mlx5_ib_qp_base *base;
+ 	int mlx5_st;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	mutex_init(&qp->mutex);
+ 	spin_lock_init(&qp->sq.lock);
+ 	spin_lock_init(&qp->rq.lock);
+ 
+ 	mlx5_st = to_mlx5_st(qp->type);
+ 	if (mlx5_st < 0)
+ 		return -EINVAL;
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	base = &qp->trans_qp.base;
+ 
+ 	qp->has_rq = qp_has_rq(attr);
+ 	err = set_rq_size(dev, &attr->cap, qp->has_rq, qp, NULL);
+ 	if (err) {
+ 		mlx5_ib_dbg(dev, "err %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	err = _create_kernel_qp(dev, attr, qp, &in, &inlen, base);
+ 	if (err)
+ 		return err;
+ 
+ 	if (is_sqp(attr->qp_type))
+ 		qp->port = attr->port_num;
+ 
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, mlx5_st);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 
+ 	if (attr->qp_type != MLX5_IB_QPT_REG_UMR)
+ 		MLX5_SET(qpc, qpc, pd, to_mpd(pd ? pd : devr->p0)->pdn);
+ 	else
+ 		MLX5_SET(qpc, qpc, latency_sensitive, 1);
+ 
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 
+ 	if (qp->rq.wqe_cnt) {
+ 		MLX5_SET(qpc, qpc, log_rq_stride, qp->rq.wqe_shift - 4);
+ 		MLX5_SET(qpc, qpc, log_rq_size, ilog2(qp->rq.wqe_cnt));
+ 	}
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, get_rx_type(qp, attr));
+ 
+ 	if (qp->sq.wqe_cnt)
+ 		MLX5_SET(qpc, qpc, log_sq_size, ilog2(qp->sq.wqe_cnt));
+ 	else
+ 		MLX5_SET(qpc, qpc, no_sq, 1);
+ 
+ 	if (attr->srq) {
+ 		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x0)->xrcdn);
+ 		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
+ 			 to_msrq(attr->srq)->msrq.srqn);
++>>>>>>> 92cd667c0e8a (RDMA/mlx5: Don't rely on FW to set zeros in ECE response)
  	} else {
 -		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x1)->xrcdn);
 -		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
 -			 to_msrq(devr->s1)->msrq.srqn);
 +		err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
  	}
  
 -	if (attr->send_cq)
 -		MLX5_SET(qpc, qpc, cqn_snd, to_mcq(attr->send_cq)->mcq.cqn);
 -
 -	if (attr->recv_cq)
 -		MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(attr->recv_cq)->mcq.cqn);
 -
 -	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
 -
 -	/* 0xffffff means we ask to work with cqe version 0 */
 -	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
 -		MLX5_SET(qpc, qpc, user_index, uidx);
 -
 -	/* we use IB_QP_CREATE_IPOIB_UD_LSO to indicates ipoib qp */
 -	if (qp->flags & IB_QP_CREATE_IPOIB_UD_LSO)
 -		MLX5_SET(qpc, qpc, ulp_stateless_offload_mode, 1);
 +	if (err) {
 +		mlx5_ib_dbg(dev, "create qp failed\n");
 +		goto err_create;
 +	}
  
 -	err = mlx5_qpc_create_qp(dev, &base->mqp, in, inlen, out);
  	kvfree(in);
 -	if (err)
 -		goto err_create;
  
  	base->container_mibqp = qp;
  	base->mqp.event = mlx5_ib_qp_event;
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
