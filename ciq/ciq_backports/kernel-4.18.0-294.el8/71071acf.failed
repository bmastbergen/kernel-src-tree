KVM: arm64: hyp: Use ctxt_sys_reg/__vcpu_sys_reg instead of raw sys_regs access

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Marc Zyngier <maz@kernel.org>
commit 71071acfd392c51d9c0b286067033534b59f6be4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/71071acf.failed

Switch the hypervisor code to using ctxt_sys_reg/__vcpu_sys_reg instead
of raw sys_regs accesses. No intended functionnal change.

	Signed-off-by: Marc Zyngier <maz@kernel.org>
(cherry picked from commit 71071acfd392c51d9c0b286067033534b59f6be4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kvm/hyp/include/hyp/switch.h
#	arch/arm64/kvm/hyp/nvhe/switch.c
#	arch/arm64/kvm/hyp/sysreg-sr.c
diff --cc arch/arm64/kvm/hyp/sysreg-sr.c
index a4eba45f8075,37ef3e2cdbef..000000000000
--- a/arch/arm64/kvm/hyp/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/sysreg-sr.c
@@@ -23,48 -15,36 +23,48 @@@
  #include <asm/kvm_emulate.h>
  #include <asm/kvm_hyp.h>
  
 -static inline void __sysreg_save_common_state(struct kvm_cpu_context *ctxt)
 +/*
 + * Non-VHE: Both host and guest must save everything.
 + *
 + * VHE: Host and guest must save mdscr_el1 and sp_el0 (and the PC and
 + * pstate, which are handled as part of the el2 return state) on every
 + * switch (sp_el0 is being dealt with in the assembly code).
 + * tpidr_el0 and tpidrro_el0 only need to be switched when going
 + * to host userspace or a different VCPU.  EL1 registers only need to be
 + * switched when potentially going to run a different VCPU.  The latter two
 + * classes are handled as part of kvm_arch_vcpu_load and kvm_arch_vcpu_put.
 + */
 +
 +static void __hyp_text __sysreg_save_common_state(struct kvm_cpu_context *ctxt)
  {
- 	ctxt->sys_regs[MDSCR_EL1]	= read_sysreg(mdscr_el1);
+ 	ctxt_sys_reg(ctxt, MDSCR_EL1)	= read_sysreg(mdscr_el1);
  }
  
 -static inline void __sysreg_save_user_state(struct kvm_cpu_context *ctxt)
 +static void __hyp_text __sysreg_save_user_state(struct kvm_cpu_context *ctxt)
  {
- 	ctxt->sys_regs[TPIDR_EL0]	= read_sysreg(tpidr_el0);
- 	ctxt->sys_regs[TPIDRRO_EL0]	= read_sysreg(tpidrro_el0);
+ 	ctxt_sys_reg(ctxt, TPIDR_EL0)	= read_sysreg(tpidr_el0);
+ 	ctxt_sys_reg(ctxt, TPIDRRO_EL0)	= read_sysreg(tpidrro_el0);
  }
  
 -static inline void __sysreg_save_el1_state(struct kvm_cpu_context *ctxt)
 +static void __hyp_text __sysreg_save_el1_state(struct kvm_cpu_context *ctxt)
  {
- 	ctxt->sys_regs[CSSELR_EL1]	= read_sysreg(csselr_el1);
- 	ctxt->sys_regs[SCTLR_EL1]	= read_sysreg_el1(SYS_SCTLR);
- 	ctxt->sys_regs[CPACR_EL1]	= read_sysreg_el1(SYS_CPACR);
- 	ctxt->sys_regs[TTBR0_EL1]	= read_sysreg_el1(SYS_TTBR0);
- 	ctxt->sys_regs[TTBR1_EL1]	= read_sysreg_el1(SYS_TTBR1);
- 	ctxt->sys_regs[TCR_EL1]		= read_sysreg_el1(SYS_TCR);
- 	ctxt->sys_regs[ESR_EL1]		= read_sysreg_el1(SYS_ESR);
- 	ctxt->sys_regs[AFSR0_EL1]	= read_sysreg_el1(SYS_AFSR0);
- 	ctxt->sys_regs[AFSR1_EL1]	= read_sysreg_el1(SYS_AFSR1);
- 	ctxt->sys_regs[FAR_EL1]		= read_sysreg_el1(SYS_FAR);
- 	ctxt->sys_regs[MAIR_EL1]	= read_sysreg_el1(SYS_MAIR);
- 	ctxt->sys_regs[VBAR_EL1]	= read_sysreg_el1(SYS_VBAR);
- 	ctxt->sys_regs[CONTEXTIDR_EL1]	= read_sysreg_el1(SYS_CONTEXTIDR);
- 	ctxt->sys_regs[AMAIR_EL1]	= read_sysreg_el1(SYS_AMAIR);
- 	ctxt->sys_regs[CNTKCTL_EL1]	= read_sysreg_el1(SYS_CNTKCTL);
- 	ctxt->sys_regs[PAR_EL1]		= read_sysreg(par_el1);
- 	ctxt->sys_regs[TPIDR_EL1]	= read_sysreg(tpidr_el1);
+ 	ctxt_sys_reg(ctxt, CSSELR_EL1)	= read_sysreg(csselr_el1);
+ 	ctxt_sys_reg(ctxt, SCTLR_EL1)	= read_sysreg_el1(SYS_SCTLR);
+ 	ctxt_sys_reg(ctxt, CPACR_EL1)	= read_sysreg_el1(SYS_CPACR);
+ 	ctxt_sys_reg(ctxt, TTBR0_EL1)	= read_sysreg_el1(SYS_TTBR0);
+ 	ctxt_sys_reg(ctxt, TTBR1_EL1)	= read_sysreg_el1(SYS_TTBR1);
+ 	ctxt_sys_reg(ctxt, TCR_EL1)	= read_sysreg_el1(SYS_TCR);
+ 	ctxt_sys_reg(ctxt, ESR_EL1)	= read_sysreg_el1(SYS_ESR);
+ 	ctxt_sys_reg(ctxt, AFSR0_EL1)	= read_sysreg_el1(SYS_AFSR0);
+ 	ctxt_sys_reg(ctxt, AFSR1_EL1)	= read_sysreg_el1(SYS_AFSR1);
+ 	ctxt_sys_reg(ctxt, FAR_EL1)	= read_sysreg_el1(SYS_FAR);
+ 	ctxt_sys_reg(ctxt, MAIR_EL1)	= read_sysreg_el1(SYS_MAIR);
+ 	ctxt_sys_reg(ctxt, VBAR_EL1)	= read_sysreg_el1(SYS_VBAR);
+ 	ctxt_sys_reg(ctxt, CONTEXTIDR_EL1) = read_sysreg_el1(SYS_CONTEXTIDR);
+ 	ctxt_sys_reg(ctxt, AMAIR_EL1)	= read_sysreg_el1(SYS_AMAIR);
+ 	ctxt_sys_reg(ctxt, CNTKCTL_EL1)	= read_sysreg_el1(SYS_CNTKCTL);
+ 	ctxt_sys_reg(ctxt, PAR_EL1)	= read_sysreg(par_el1);
+ 	ctxt_sys_reg(ctxt, TPIDR_EL1)	= read_sysreg(tpidr_el1);
  
  	ctxt->gp_regs.sp_el1		= read_sysreg(sp_el1);
  	ctxt->gp_regs.elr_el1		= read_sysreg_el1(SYS_ELR);
@@@ -76,80 -56,59 +76,91 @@@ static void __hyp_text __sysreg_save_el
  	ctxt->gp_regs.regs.pc		= read_sysreg_el2(SYS_ELR);
  	ctxt->gp_regs.regs.pstate	= read_sysreg_el2(SYS_SPSR);
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +	if (cpus_have_const_cap(ARM64_HAS_RAS_EXTN))
 +		ctxt->sys_regs[DISR_EL1] = read_sysreg_s(SYS_VDISR_EL2);
++=======
+ 	if (cpus_have_final_cap(ARM64_HAS_RAS_EXTN))
+ 		ctxt_sys_reg(ctxt, DISR_EL1) = read_sysreg_s(SYS_VDISR_EL2);
++>>>>>>> 71071acfd392 (KVM: arm64: hyp: Use ctxt_sys_reg/__vcpu_sys_reg instead of raw sys_regs access):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
 +}
 +
 +void __hyp_text __sysreg_save_state_nvhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_save_el1_state(ctxt);
 +	__sysreg_save_common_state(ctxt);
 +	__sysreg_save_user_state(ctxt);
 +	__sysreg_save_el2_return_state(ctxt);
 +}
 +
 +void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_save_common_state(ctxt);
 +}
 +NOKPROBE_SYMBOL(sysreg_save_host_state_vhe);
 +
 +void sysreg_save_guest_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_save_common_state(ctxt);
 +	__sysreg_save_el2_return_state(ctxt);
  }
 +NOKPROBE_SYMBOL(sysreg_save_guest_state_vhe);
  
 -static inline void __sysreg_restore_common_state(struct kvm_cpu_context *ctxt)
 +static void __hyp_text __sysreg_restore_common_state(struct kvm_cpu_context *ctxt)
  {
- 	write_sysreg(ctxt->sys_regs[MDSCR_EL1],	  mdscr_el1);
+ 	write_sysreg(ctxt_sys_reg(ctxt, MDSCR_EL1),  mdscr_el1);
  }
  
 -static inline void __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
 +static void __hyp_text __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
  {
- 	write_sysreg(ctxt->sys_regs[TPIDR_EL0],		tpidr_el0);
- 	write_sysreg(ctxt->sys_regs[TPIDRRO_EL0],	tpidrro_el0);
+ 	write_sysreg(ctxt_sys_reg(ctxt, TPIDR_EL0),	tpidr_el0);
+ 	write_sysreg(ctxt_sys_reg(ctxt, TPIDRRO_EL0),	tpidrro_el0);
  }
  
 -static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
 +static void __hyp_text __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
  {
- 	write_sysreg(ctxt->sys_regs[MPIDR_EL1],		vmpidr_el2);
- 	write_sysreg(ctxt->sys_regs[CSSELR_EL1],	csselr_el1);
+ 	write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1),	vmpidr_el2);
+ 	write_sysreg(ctxt_sys_reg(ctxt, CSSELR_EL1),	csselr_el1);
  
  	if (has_vhe() ||
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +	    !cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 +		write_sysreg_el1(ctxt->sys_regs[SCTLR_EL1],	SYS_SCTLR);
 +		write_sysreg_el1(ctxt->sys_regs[TCR_EL1],	SYS_TCR);
++=======
+ 	    !cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
+ 		write_sysreg_el1(ctxt_sys_reg(ctxt, SCTLR_EL1),	SYS_SCTLR);
+ 		write_sysreg_el1(ctxt_sys_reg(ctxt, TCR_EL1),	SYS_TCR);
++>>>>>>> 71071acfd392 (KVM: arm64: hyp: Use ctxt_sys_reg/__vcpu_sys_reg instead of raw sys_regs access):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  	} else	if (!ctxt->__hyp_running_vcpu) {
  		/*
  		 * Must only be done for guest registers, hence the context
  		 * test. We're coming from the host, so SCTLR.M is already
 -		 * set. Pairs with nVHE's __activate_traps().
 +		 * set. Pairs with __activate_traps_nvhe().
  		 */
- 		write_sysreg_el1((ctxt->sys_regs[TCR_EL1] |
+ 		write_sysreg_el1((ctxt_sys_reg(ctxt, TCR_EL1) |
  				  TCR_EPD1_MASK | TCR_EPD0_MASK),
  				 SYS_TCR);
  		isb();
  	}
  
- 	write_sysreg_el1(ctxt->sys_regs[CPACR_EL1],	SYS_CPACR);
- 	write_sysreg_el1(ctxt->sys_regs[TTBR0_EL1],	SYS_TTBR0);
- 	write_sysreg_el1(ctxt->sys_regs[TTBR1_EL1],	SYS_TTBR1);
- 	write_sysreg_el1(ctxt->sys_regs[ESR_EL1],	SYS_ESR);
- 	write_sysreg_el1(ctxt->sys_regs[AFSR0_EL1],	SYS_AFSR0);
- 	write_sysreg_el1(ctxt->sys_regs[AFSR1_EL1],	SYS_AFSR1);
- 	write_sysreg_el1(ctxt->sys_regs[FAR_EL1],	SYS_FAR);
- 	write_sysreg_el1(ctxt->sys_regs[MAIR_EL1],	SYS_MAIR);
- 	write_sysreg_el1(ctxt->sys_regs[VBAR_EL1],	SYS_VBAR);
- 	write_sysreg_el1(ctxt->sys_regs[CONTEXTIDR_EL1],SYS_CONTEXTIDR);
- 	write_sysreg_el1(ctxt->sys_regs[AMAIR_EL1],	SYS_AMAIR);
- 	write_sysreg_el1(ctxt->sys_regs[CNTKCTL_EL1],	SYS_CNTKCTL);
- 	write_sysreg(ctxt->sys_regs[PAR_EL1],		par_el1);
- 	write_sysreg(ctxt->sys_regs[TPIDR_EL1],		tpidr_el1);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, CPACR_EL1),	SYS_CPACR);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR0_EL1),	SYS_TTBR0);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR1_EL1),	SYS_TTBR1);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, ESR_EL1),	SYS_ESR);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, AFSR0_EL1),	SYS_AFSR0);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, AFSR1_EL1),	SYS_AFSR1);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, FAR_EL1),	SYS_FAR);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, MAIR_EL1),	SYS_MAIR);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, VBAR_EL1),	SYS_VBAR);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, CONTEXTIDR_EL1), SYS_CONTEXTIDR);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, AMAIR_EL1),	SYS_AMAIR);
+ 	write_sysreg_el1(ctxt_sys_reg(ctxt, CNTKCTL_EL1), SYS_CNTKCTL);
+ 	write_sysreg(ctxt_sys_reg(ctxt, PAR_EL1),	par_el1);
+ 	write_sysreg(ctxt_sys_reg(ctxt, TPIDR_EL1),	tpidr_el1);
  
  	if (!has_vhe() &&
 -	    cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT) &&
 +	    cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT) &&
  	    ctxt->__hyp_running_vcpu) {
  		/*
  		 * Must only be done for host registers, hence the context
@@@ -194,34 -152,13 +205,39 @@@ __sysreg_restore_el2_return_state(struc
  	write_sysreg_el2(ctxt->gp_regs.regs.pc,		SYS_ELR);
  	write_sysreg_el2(pstate,			SYS_SPSR);
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +	if (cpus_have_const_cap(ARM64_HAS_RAS_EXTN))
 +		write_sysreg_s(ctxt->sys_regs[DISR_EL1], SYS_VDISR_EL2);
++=======
+ 	if (cpus_have_final_cap(ARM64_HAS_RAS_EXTN))
+ 		write_sysreg_s(ctxt_sys_reg(ctxt, DISR_EL1), SYS_VDISR_EL2);
++>>>>>>> 71071acfd392 (KVM: arm64: hyp: Use ctxt_sys_reg/__vcpu_sys_reg instead of raw sys_regs access):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
 +}
 +
 +void __hyp_text __sysreg_restore_state_nvhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_restore_el1_state(ctxt);
 +	__sysreg_restore_common_state(ctxt);
 +	__sysreg_restore_user_state(ctxt);
 +	__sysreg_restore_el2_return_state(ctxt);
 +}
 +
 +void sysreg_restore_host_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_restore_common_state(ctxt);
 +}
 +NOKPROBE_SYMBOL(sysreg_restore_host_state_vhe);
 +
 +void sysreg_restore_guest_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_restore_common_state(ctxt);
 +	__sysreg_restore_el2_return_state(ctxt);
  }
 +NOKPROBE_SYMBOL(sysreg_restore_guest_state_vhe);
  
 -static inline void __sysreg32_save_state(struct kvm_vcpu *vcpu)
 +void __hyp_text __sysreg32_save_state(struct kvm_vcpu *vcpu)
  {
- 	u64 *spsr, *sysreg;
+ 	u64 *spsr;
  
  	if (!vcpu_el1_is_32bit(vcpu))
  		return;
@@@ -234,16 -170,16 +249,16 @@@
  	spsr[KVM_SPSR_IRQ] = read_sysreg(spsr_irq);
  	spsr[KVM_SPSR_FIQ] = read_sysreg(spsr_fiq);
  
- 	sysreg[DACR32_EL2] = read_sysreg(dacr32_el2);
- 	sysreg[IFSR32_EL2] = read_sysreg(ifsr32_el2);
+ 	__vcpu_sys_reg(vcpu, DACR32_EL2) = read_sysreg(dacr32_el2);
+ 	__vcpu_sys_reg(vcpu, IFSR32_EL2) = read_sysreg(ifsr32_el2);
  
  	if (has_vhe() || vcpu->arch.flags & KVM_ARM64_DEBUG_DIRTY)
- 		sysreg[DBGVCR32_EL2] = read_sysreg(dbgvcr32_el2);
+ 		__vcpu_sys_reg(vcpu, DBGVCR32_EL2) = read_sysreg(dbgvcr32_el2);
  }
  
 -static inline void __sysreg32_restore_state(struct kvm_vcpu *vcpu)
 +void __hyp_text __sysreg32_restore_state(struct kvm_vcpu *vcpu)
  {
- 	u64 *spsr, *sysreg;
+ 	u64 *spsr;
  
  	if (!vcpu_el1_is_32bit(vcpu))
  		return;
@@@ -256,89 -191,11 +270,89 @@@
  	write_sysreg(spsr[KVM_SPSR_IRQ], spsr_irq);
  	write_sysreg(spsr[KVM_SPSR_FIQ], spsr_fiq);
  
- 	write_sysreg(sysreg[DACR32_EL2], dacr32_el2);
- 	write_sysreg(sysreg[IFSR32_EL2], ifsr32_el2);
+ 	write_sysreg(__vcpu_sys_reg(vcpu, DACR32_EL2), dacr32_el2);
+ 	write_sysreg(__vcpu_sys_reg(vcpu, IFSR32_EL2), ifsr32_el2);
  
  	if (has_vhe() || vcpu->arch.flags & KVM_ARM64_DEBUG_DIRTY)
- 		write_sysreg(sysreg[DBGVCR32_EL2], dbgvcr32_el2);
+ 		write_sysreg(__vcpu_sys_reg(vcpu, DBGVCR32_EL2), dbgvcr32_el2);
  }
  
 -#endif /* __ARM64_KVM_HYP_SYSREG_SR_H__ */
 +/**
 + * kvm_vcpu_load_sysregs - Load guest system registers to the physical CPU
 + *
 + * @vcpu: The VCPU pointer
 + *
 + * Load system registers that do not affect the host's execution, for
 + * example EL1 system registers on a VHE system where the host kernel
 + * runs at EL2.  This function is called from KVM's vcpu_load() function
 + * and loading system register state early avoids having to load them on
 + * every entry to the VM.
 + */
 +void kvm_vcpu_load_sysregs(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
 +	struct kvm_cpu_context *host_ctxt;
 +
 +	if (!has_vhe())
 +		return;
 +
 +	host_ctxt = &__hyp_this_cpu_ptr(kvm_host_data)->host_ctxt;
 +	__sysreg_save_user_state(host_ctxt);
 +
 +	/*
 +	 * Load guest EL1 and user state
 +	 *
 +	 * We must restore the 32-bit state before the sysregs, thanks
 +	 * to erratum #852523 (Cortex-A57) or #853709 (Cortex-A72).
 +	 */
 +	__sysreg32_restore_state(vcpu);
 +	__sysreg_restore_user_state(guest_ctxt);
 +	__sysreg_restore_el1_state(guest_ctxt);
 +
 +	vcpu->arch.sysregs_loaded_on_cpu = true;
 +
 +	activate_traps_vhe_load(vcpu);
 +}
 +
 +/**
 + * kvm_vcpu_put_sysregs - Restore host system registers to the physical CPU
 + *
 + * @vcpu: The VCPU pointer
 + *
 + * Save guest system registers that do not affect the host's execution, for
 + * example EL1 system registers on a VHE system where the host kernel
 + * runs at EL2.  This function is called from KVM's vcpu_put() function
 + * and deferring saving system register state until we're no longer running the
 + * VCPU avoids having to save them on every exit from the VM.
 + */
 +void kvm_vcpu_put_sysregs(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
 +	struct kvm_cpu_context *host_ctxt;
 +
 +	if (!has_vhe())
 +		return;
 +
 +	host_ctxt = &__hyp_this_cpu_ptr(kvm_host_data)->host_ctxt;
 +	deactivate_traps_vhe_put();
 +
 +	__sysreg_save_el1_state(guest_ctxt);
 +	__sysreg_save_user_state(guest_ctxt);
 +	__sysreg32_save_state(vcpu);
 +
 +	/* Restore host user state */
 +	__sysreg_restore_user_state(host_ctxt);
 +
 +	vcpu->arch.sysregs_loaded_on_cpu = false;
 +}
 +
 +void __hyp_text __kvm_enable_ssbs(void)
 +{
 +	u64 tmp;
 +
 +	asm volatile(
 +	"mrs	%0, sctlr_el2\n"
 +	"orr	%0, %0, %1\n"
 +	"msr	sctlr_el2, %0"
 +	: "=&r" (tmp) : "L" (SCTLR_ELx_DSSBS));
 +}
* Unmerged path arch/arm64/kvm/hyp/include/hyp/switch.h
* Unmerged path arch/arm64/kvm/hyp/nvhe/switch.c
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index d8c4d368d7b0..022bcba1a932 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -546,7 +546,7 @@ DECLARE_PER_CPU(kvm_host_data_t, kvm_host_data);
 static inline void kvm_init_host_cpu_context(struct kvm_cpu_context *cpu_ctxt)
 {
 	/* The host's MPIDR is immutable, so let's set it up at boot time */
-	cpu_ctxt->sys_regs[MPIDR_EL1] = read_cpuid_mpidr();
+	ctxt_sys_reg(cpu_ctxt, MPIDR_EL1) = read_cpuid_mpidr();
 }
 
 static inline bool kvm_arch_requires_vhe(void)
diff --git a/arch/arm64/kvm/hyp/debug-sr.c b/arch/arm64/kvm/hyp/debug-sr.c
index d825b78b3f10..5560e304194a 100644
--- a/arch/arm64/kvm/hyp/debug-sr.c
+++ b/arch/arm64/kvm/hyp/debug-sr.c
@@ -156,7 +156,7 @@ static void __hyp_text __debug_save_state(struct kvm_vcpu *vcpu,
 	save_debug(dbg->dbg_wcr, dbgwcr, wrps);
 	save_debug(dbg->dbg_wvr, dbgwvr, wrps);
 
-	ctxt->sys_regs[MDCCINT_EL1] = read_sysreg(mdccint_el1);
+	ctxt_sys_reg(ctxt, MDCCINT_EL1) = read_sysreg(mdccint_el1);
 }
 
 static void __hyp_text __debug_restore_state(struct kvm_vcpu *vcpu,
@@ -176,7 +176,7 @@ static void __hyp_text __debug_restore_state(struct kvm_vcpu *vcpu,
 	restore_debug(dbg->dbg_wcr, dbgwcr, wrps);
 	restore_debug(dbg->dbg_wvr, dbgwvr, wrps);
 
-	write_sysreg(ctxt->sys_regs[MDCCINT_EL1], mdccint_el1);
+	write_sysreg(ctxt_sys_reg(ctxt, MDCCINT_EL1), mdccint_el1);
 }
 
 void __hyp_text __debug_switch_to_guest(struct kvm_vcpu *vcpu)
* Unmerged path arch/arm64/kvm/hyp/include/hyp/switch.h
* Unmerged path arch/arm64/kvm/hyp/nvhe/switch.c
* Unmerged path arch/arm64/kvm/hyp/sysreg-sr.c
