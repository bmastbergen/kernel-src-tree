dma-remap: separate DMA atomic pools from direct remap code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author David Rientjes <rientjes@google.com>
commit e860c299ac0d738b44ff91693f11e63080a29698
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/e860c299.failed

DMA atomic pools will be needed beyond only CONFIG_DMA_DIRECT_REMAP so
separate them out into their own file.

This also adds a new Kconfig option that can be subsequently used for
options, such as CONFIG_AMD_MEM_ENCRYPT, that will utilize the coherent
pools but do not have a dependency on direct remapping.

For this patch alone, there is no functional change introduced.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: David Rientjes <rientjes@google.com>
[hch: fixup copyrights and remove unused includes]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit e860c299ac0d738b44ff91693f11e63080a29698)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/Kconfig
diff --cc kernel/dma/Kconfig
index b1048cb7a930,d006668c0027..000000000000
--- a/kernel/dma/Kconfig
+++ b/kernel/dma/Kconfig
@@@ -75,6 -79,123 +75,126 @@@ config DMA_REMA
  	select DMA_NONCOHERENT_MMAP
  	bool
  
- config DMA_DIRECT_REMAP
+ config DMA_COHERENT_POOL
  	bool
  	select DMA_REMAP
++<<<<<<< HEAD
++=======
+ 
+ config DMA_DIRECT_REMAP
+ 	bool
+ 	select DMA_COHERENT_POOL
+ 
+ config DMA_CMA
+ 	bool "DMA Contiguous Memory Allocator"
+ 	depends on HAVE_DMA_CONTIGUOUS && CMA
+ 	help
+ 	  This enables the Contiguous Memory Allocator which allows drivers
+ 	  to allocate big physically-contiguous blocks of memory for use with
+ 	  hardware components that do not support I/O map nor scatter-gather.
+ 
+ 	  You can disable CMA by specifying "cma=0" on the kernel's command
+ 	  line.
+ 
+ 	  For more information see <include/linux/dma-contiguous.h>.
+ 	  If unsure, say "n".
+ 
+ if  DMA_CMA
+ comment "Default contiguous memory area size:"
+ 
+ config CMA_SIZE_MBYTES
+ 	int "Size in Mega Bytes"
+ 	depends on !CMA_SIZE_SEL_PERCENTAGE
+ 	default 0 if X86
+ 	default 16
+ 	help
+ 	  Defines the size (in MiB) of the default memory area for Contiguous
+ 	  Memory Allocator.  If the size of 0 is selected, CMA is disabled by
+ 	  default, but it can be enabled by passing cma=size[MG] to the kernel.
+ 
+ 
+ config CMA_SIZE_PERCENTAGE
+ 	int "Percentage of total memory"
+ 	depends on !CMA_SIZE_SEL_MBYTES
+ 	default 0 if X86
+ 	default 10
+ 	help
+ 	  Defines the size of the default memory area for Contiguous Memory
+ 	  Allocator as a percentage of the total memory in the system.
+ 	  If 0 percent is selected, CMA is disabled by default, but it can be
+ 	  enabled by passing cma=size[MG] to the kernel.
+ 
+ choice
+ 	prompt "Selected region size"
+ 	default CMA_SIZE_SEL_MBYTES
+ 
+ config CMA_SIZE_SEL_MBYTES
+ 	bool "Use mega bytes value only"
+ 
+ config CMA_SIZE_SEL_PERCENTAGE
+ 	bool "Use percentage value only"
+ 
+ config CMA_SIZE_SEL_MIN
+ 	bool "Use lower value (minimum)"
+ 
+ config CMA_SIZE_SEL_MAX
+ 	bool "Use higher value (maximum)"
+ 
+ endchoice
+ 
+ config CMA_ALIGNMENT
+ 	int "Maximum PAGE_SIZE order of alignment for contiguous buffers"
+ 	range 4 12
+ 	default 8
+ 	help
+ 	  DMA mapping framework by default aligns all buffers to the smallest
+ 	  PAGE_SIZE order which is greater than or equal to the requested buffer
+ 	  size. This works well for buffers up to a few hundreds kilobytes, but
+ 	  for larger buffers it just a memory waste. With this parameter you can
+ 	  specify the maximum PAGE_SIZE order for contiguous buffers. Larger
+ 	  buffers will be aligned only to this specified order. The order is
+ 	  expressed as a power of two multiplied by the PAGE_SIZE.
+ 
+ 	  For example, if your system defaults to 4KiB pages, the order value
+ 	  of 8 means that the buffers will be aligned up to 1MiB only.
+ 
+ 	  If unsure, leave the default value "8".
+ 
+ endif
+ 
+ config DMA_API_DEBUG
+ 	bool "Enable debugging of DMA-API usage"
+ 	select NEED_DMA_MAP_STATE
+ 	help
+ 	  Enable this option to debug the use of the DMA API by device drivers.
+ 	  With this option you will be able to detect common bugs in device
+ 	  drivers like double-freeing of DMA mappings or freeing mappings that
+ 	  were never allocated.
+ 
+ 	  This also attempts to catch cases where a page owned by DMA is
+ 	  accessed by the cpu in a way that could cause data corruption.  For
+ 	  example, this enables cow_user_page() to check that the source page is
+ 	  not undergoing DMA.
+ 
+ 	  This option causes a performance degradation.  Use only if you want to
+ 	  debug device drivers and dma interactions.
+ 
+ 	  If unsure, say N.
+ 
+ config DMA_API_DEBUG_SG
+ 	bool "Debug DMA scatter-gather usage"
+ 	default y
+ 	depends on DMA_API_DEBUG
+ 	help
+ 	  Perform extra checking that callers of dma_map_sg() have respected the
+ 	  appropriate segment length/boundary limits for the given device when
+ 	  preparing DMA scatterlists.
+ 
+ 	  This is particularly likely to have been overlooked in cases where the
+ 	  dma_map_sg() API is used for general bulk mapping of pages rather than
+ 	  preparing literal scatter-gather descriptors, where there is a risk of
+ 	  unexpected behaviour from DMA API implementations if the scatterlist
+ 	  is technically out-of-spec.
+ 
+ 	  If unsure, say N.
++>>>>>>> e860c299ac0d (dma-remap: separate DMA atomic pools from direct remap code)
* Unmerged path kernel/dma/Kconfig
diff --git a/kernel/dma/Makefile b/kernel/dma/Makefile
index 72ff6e46aa86..a779ec153210 100644
--- a/kernel/dma/Makefile
+++ b/kernel/dma/Makefile
@@ -6,4 +6,5 @@ obj-$(CONFIG_HAVE_GENERIC_DMA_COHERENT) += coherent.o
 obj-$(CONFIG_DMA_VIRT_OPS)		+= virt.o
 obj-$(CONFIG_DMA_API_DEBUG)		+= debug.o
 obj-$(CONFIG_SWIOTLB)			+= swiotlb.o
+obj-$(CONFIG_DMA_COHERENT_POOL)		+= pool.o
 obj-$(CONFIG_DMA_REMAP)			+= remap.o
diff --git a/kernel/dma/pool.c b/kernel/dma/pool.c
new file mode 100644
index 000000000000..3df5d9d39922
--- /dev/null
+++ b/kernel/dma/pool.c
@@ -0,0 +1,123 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ * Copyright (C) 2020 Google LLC
+ */
+#include <linux/dma-direct.h>
+#include <linux/dma-noncoherent.h>
+#include <linux/dma-contiguous.h>
+#include <linux/init.h>
+#include <linux/genalloc.h>
+#include <linux/slab.h>
+
+static struct gen_pool *atomic_pool __ro_after_init;
+
+#define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K
+static size_t atomic_pool_size __initdata = DEFAULT_DMA_COHERENT_POOL_SIZE;
+
+static int __init early_coherent_pool(char *p)
+{
+	atomic_pool_size = memparse(p, &p);
+	return 0;
+}
+early_param("coherent_pool", early_coherent_pool);
+
+static gfp_t dma_atomic_pool_gfp(void)
+{
+	if (IS_ENABLED(CONFIG_ZONE_DMA))
+		return GFP_DMA;
+	if (IS_ENABLED(CONFIG_ZONE_DMA32))
+		return GFP_DMA32;
+	return GFP_KERNEL;
+}
+
+static int __init dma_atomic_pool_init(void)
+{
+	unsigned int pool_size_order = get_order(atomic_pool_size);
+	unsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;
+	struct page *page;
+	void *addr;
+	int ret;
+
+	if (dev_get_cma_area(NULL))
+		page = dma_alloc_from_contiguous(NULL, nr_pages,
+						 pool_size_order, false);
+	else
+		page = alloc_pages(dma_atomic_pool_gfp(), pool_size_order);
+	if (!page)
+		goto out;
+
+	arch_dma_prep_coherent(page, atomic_pool_size);
+
+	atomic_pool = gen_pool_create(PAGE_SHIFT, -1);
+	if (!atomic_pool)
+		goto free_page;
+
+	addr = dma_common_contiguous_remap(page, atomic_pool_size,
+					   pgprot_dmacoherent(PAGE_KERNEL),
+					   __builtin_return_address(0));
+	if (!addr)
+		goto destroy_genpool;
+
+	ret = gen_pool_add_virt(atomic_pool, (unsigned long)addr,
+				page_to_phys(page), atomic_pool_size, -1);
+	if (ret)
+		goto remove_mapping;
+	gen_pool_set_algo(atomic_pool, gen_pool_first_fit_order_align, NULL);
+
+	pr_info("DMA: preallocated %zu KiB pool for atomic allocations\n",
+		atomic_pool_size / 1024);
+	return 0;
+
+remove_mapping:
+	dma_common_free_remap(addr, atomic_pool_size);
+destroy_genpool:
+	gen_pool_destroy(atomic_pool);
+	atomic_pool = NULL;
+free_page:
+	if (!dma_release_from_contiguous(NULL, page, nr_pages))
+		__free_pages(page, pool_size_order);
+out:
+	pr_err("DMA: failed to allocate %zu KiB pool for atomic coherent allocation\n",
+		atomic_pool_size / 1024);
+	return -ENOMEM;
+}
+postcore_initcall(dma_atomic_pool_init);
+
+bool dma_in_atomic_pool(void *start, size_t size)
+{
+	if (unlikely(!atomic_pool))
+		return false;
+
+	return gen_pool_has_addr(atomic_pool, (unsigned long)start, size);
+}
+
+void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)
+{
+	unsigned long val;
+	void *ptr = NULL;
+
+	if (!atomic_pool) {
+		WARN(1, "coherent pool not initialised!\n");
+		return NULL;
+	}
+
+	val = gen_pool_alloc(atomic_pool, size);
+	if (val) {
+		phys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);
+
+		*ret_page = pfn_to_page(__phys_to_pfn(phys));
+		ptr = (void *)val;
+		memset(ptr, 0, size);
+	}
+
+	return ptr;
+}
+
+bool dma_free_from_pool(void *start, size_t size)
+{
+	if (!dma_in_atomic_pool(start, size))
+		return false;
+	gen_pool_free(atomic_pool, (unsigned long)start, size);
+	return true;
+}
diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 914ff5a58dd5..e739a6eea6e7 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -1,13 +1,8 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
- * Copyright (C) 2012 ARM Ltd.
  * Copyright (c) 2014 The Linux Foundation
  */
-#include <linux/dma-direct.h>
-#include <linux/dma-noncoherent.h>
-#include <linux/dma-contiguous.h>
-#include <linux/init.h>
-#include <linux/genalloc.h>
+#include <linux/dma-mapping.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 
@@ -73,117 +68,3 @@ void dma_common_free_remap(void *cpu_addr, size_t size)
 	unmap_kernel_range((unsigned long)cpu_addr, PAGE_ALIGN(size));
 	vunmap(cpu_addr);
 }
-
-#ifdef CONFIG_DMA_DIRECT_REMAP
-static struct gen_pool *atomic_pool __ro_after_init;
-
-#define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K
-static size_t atomic_pool_size __initdata = DEFAULT_DMA_COHERENT_POOL_SIZE;
-
-static int __init early_coherent_pool(char *p)
-{
-	atomic_pool_size = memparse(p, &p);
-	return 0;
-}
-early_param("coherent_pool", early_coherent_pool);
-
-static gfp_t dma_atomic_pool_gfp(void)
-{
-	if (IS_ENABLED(CONFIG_ZONE_DMA))
-		return GFP_DMA;
-	if (IS_ENABLED(CONFIG_ZONE_DMA32))
-		return GFP_DMA32;
-	return GFP_KERNEL;
-}
-
-static int __init dma_atomic_pool_init(void)
-{
-	unsigned int pool_size_order = get_order(atomic_pool_size);
-	unsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;
-	struct page *page;
-	void *addr;
-	int ret;
-
-	if (dev_get_cma_area(NULL))
-		page = dma_alloc_from_contiguous(NULL, nr_pages,
-						 pool_size_order, false);
-	else
-		page = alloc_pages(dma_atomic_pool_gfp(), pool_size_order);
-	if (!page)
-		goto out;
-
-	arch_dma_prep_coherent(page, atomic_pool_size);
-
-	atomic_pool = gen_pool_create(PAGE_SHIFT, -1);
-	if (!atomic_pool)
-		goto free_page;
-
-	addr = dma_common_contiguous_remap(page, atomic_pool_size,
-					   pgprot_dmacoherent(PAGE_KERNEL),
-					   __builtin_return_address(0));
-	if (!addr)
-		goto destroy_genpool;
-
-	ret = gen_pool_add_virt(atomic_pool, (unsigned long)addr,
-				page_to_phys(page), atomic_pool_size, -1);
-	if (ret)
-		goto remove_mapping;
-	gen_pool_set_algo(atomic_pool, gen_pool_first_fit_order_align, NULL);
-
-	pr_info("DMA: preallocated %zu KiB pool for atomic allocations\n",
-		atomic_pool_size / 1024);
-	return 0;
-
-remove_mapping:
-	dma_common_free_remap(addr, atomic_pool_size);
-destroy_genpool:
-	gen_pool_destroy(atomic_pool);
-	atomic_pool = NULL;
-free_page:
-	if (!dma_release_from_contiguous(NULL, page, nr_pages))
-		__free_pages(page, pool_size_order);
-out:
-	pr_err("DMA: failed to allocate %zu KiB pool for atomic coherent allocation\n",
-		atomic_pool_size / 1024);
-	return -ENOMEM;
-}
-postcore_initcall(dma_atomic_pool_init);
-
-bool dma_in_atomic_pool(void *start, size_t size)
-{
-	if (unlikely(!atomic_pool))
-		return false;
-
-	return gen_pool_has_addr(atomic_pool, (unsigned long)start, size);
-}
-
-void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)
-{
-	unsigned long val;
-	void *ptr = NULL;
-
-	if (!atomic_pool) {
-		WARN(1, "coherent pool not initialised!\n");
-		return NULL;
-	}
-
-	val = gen_pool_alloc(atomic_pool, size);
-	if (val) {
-		phys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);
-
-		*ret_page = pfn_to_page(__phys_to_pfn(phys));
-		ptr = (void *)val;
-		memset(ptr, 0, size);
-	}
-
-	return ptr;
-}
-
-bool dma_free_from_pool(void *start, size_t size)
-{
-	if (!dma_in_atomic_pool(start, size))
-		return false;
-	gen_pool_free(atomic_pool, (unsigned long)start, size);
-	return true;
-}
-#endif /* CONFIG_DMA_DIRECT_REMAP */
