iommu/arm-smmu: Get rid of weird "atomic" write

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit 6100576284e9df139477817e1a207b5fae1a3df8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/61005762.failed

The smmu_write_atomic_lq oddity made some sense when the context
format was effectively tied to CONFIG_64BIT, but these days it's
simpler to just pick an explicit access size based on the format
for the one-and-a-half times we actually care.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 6100576284e9df139477817e1a207b5fae1a3df8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm-smmu.c
diff --cc drivers/iommu/arm-smmu.c
index 6374a0f18d9a,24b4de1a4185..000000000000
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@@ -93,19 -83,8 +93,8 @@@
  		((smmu->options & ARM_SMMU_OPT_SECURE_CFG_ACCESS)	\
  			? 0x400 : 0))
  
- /*
-  * Some 64-bit registers only make sense to write atomically, but in such
-  * cases all the data relevant to AArch32 formats lies within the lower word,
-  * therefore this actually makes more sense than it might first appear.
-  */
- #ifdef CONFIG_64BIT
- #define smmu_write_atomic_lq		writeq_relaxed
- #else
- #define smmu_write_atomic_lq		writel_relaxed
- #endif
- 
  /* Translation context bank */
 -#define ARM_SMMU_CB(smmu, n)	((smmu)->base + (((smmu)->numpage + (n)) << (smmu)->pgshift))
 +#define ARM_SMMU_CB(smmu, n)	((smmu)->cb_base + ((n) << (smmu)->pgshift))
  
  #define MSI_IOVA_BASE			0x8000000
  #define MSI_IOVA_LENGTH			0x100000
@@@ -544,6 -509,27 +533,30 @@@ static void arm_smmu_tlb_inv_range_nosy
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void arm_smmu_tlb_inv_range_s2(unsigned long iova, size_t size,
+ 				      size_t granule, bool leaf, void *cookie)
+ {
+ 	struct arm_smmu_domain *smmu_domain = cookie;
+ 	struct arm_smmu_device *smmu = smmu_domain->smmu;
+ 	void __iomem *reg = ARM_SMMU_CB(smmu, smmu_domain->cfg.cbndx);
+ 
+ 	if (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
+ 		wmb();
+ 
+ 	reg += leaf ? ARM_SMMU_CB_S2_TLBIIPAS2L : ARM_SMMU_CB_S2_TLBIIPAS2;
+ 	iova >>= 12;
+ 	do {
+ 		if (smmu_domain->cfg.fmt == ARM_SMMU_CTX_FMT_AARCH64)
+ 			writeq_relaxed(iova, reg);
+ 		else
+ 			writel_relaxed(iova, reg);
+ 		iova += granule >> 12;
+ 	} while (size -= granule);
+ }
+ 
++>>>>>>> 6100576284e9 (iommu/arm-smmu: Get rid of weird "atomic" write)
  /*
   * On MMU-401 at least, the cost of firing off multiple TLBIVMIDs appears
   * almost negligible, but the benefit of getting the first one in as far ahead
* Unmerged path drivers/iommu/arm-smmu.c
