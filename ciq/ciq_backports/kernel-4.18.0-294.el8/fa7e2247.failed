dma-direct: make uncached_kernel_address more general

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Christoph Hellwig <hch@lst.de>
commit fa7e2247c5729f990c7456fe09f3af99c8f2571b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/fa7e2247.failed

Rename the symbol to arch_dma_set_uncached, and pass a size to it as
well as allow an error return.  That will allow reusing this hook for
in-place pagetable remapping.

As the in-place remap doesn't always require an explicit cache flush,
also detangle ARCH_HAS_DMA_PREP_COHERENT from ARCH_HAS_DMA_SET_UNCACHED.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Robin Murphy <robin.murphy@arm.com>
(cherry picked from commit fa7e2247c5729f990c7456fe09f3af99c8f2571b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/Kconfig
#	arch/microblaze/Kconfig
#	arch/microblaze/mm/consistent.c
#	arch/mips/Kconfig
#	arch/mips/mm/dma-noncoherent.c
#	arch/nios2/Kconfig
#	arch/nios2/mm/dma-mapping.c
#	arch/xtensa/Kconfig
#	arch/xtensa/kernel/pci-dma.c
#	include/linux/dma-noncoherent.h
diff --cc arch/Kconfig
index ce2392831931,090cfe0c82a7..000000000000
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@@ -245,11 -248,11 +245,16 @@@ config ARCH_HAS_SET_DIRECT_MA
  	bool
  
  #
++<<<<<<< HEAD
 +# Select if arch has an uncached kernel segment and provides the
 +# uncached_kernel_address / cached_kernel_address symbols to use it
++=======
+ # Select if the architecture provides the arch_dma_set_uncached symbol to
+ # either provide an uncached segement alias for a DMA allocation, or
+ # to remap the page tables in place.
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
  #
- config ARCH_HAS_UNCACHED_SEGMENT
- 	select ARCH_HAS_DMA_PREP_COHERENT
+ config ARCH_HAS_DMA_SET_UNCACHED
  	bool
  
  # Select if arch init_task must go in the __init_task_data section
diff --cc arch/microblaze/Kconfig
index 4bf3ba3244df,9606c244b5b8..000000000000
--- a/arch/microblaze/Kconfig
+++ b/arch/microblaze/Kconfig
@@@ -1,10 -1,17 +1,16 @@@
 -# SPDX-License-Identifier: GPL-2.0-only
  config MICROBLAZE
  	def_bool y
 -	select ARCH_32BIT_OFF_T
 -	select ARCH_NO_SWAP
 -	select ARCH_HAS_BINFMT_FLAT if !MMU
 -	select ARCH_HAS_DMA_PREP_COHERENT
  	select ARCH_HAS_GCOV_PROFILE_ALL
++<<<<<<< HEAD
++=======
+ 	select ARCH_HAS_SYNC_DMA_FOR_CPU
+ 	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
+ 	select ARCH_HAS_DMA_SET_UNCACHED if !MMU
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
  	select ARCH_MIGHT_HAVE_PC_PARPORT
 +	select ARCH_NO_COHERENT_DMA_MMAP if !MMU
  	select ARCH_WANT_IPC_PARSE_VERSION
 -	select BUILDTIME_TABLE_SORT
 +	select BUILDTIME_EXTABLE_SORT
  	select TIMER_OF
  	select CLONE_BACKWARDS3
  	select COMMON_CLK
diff --cc arch/microblaze/mm/consistent.c
index b0ac81828e6a,e09b66e43cb6..000000000000
--- a/arch/microblaze/mm/consistent.c
+++ b/arch/microblaze/mm/consistent.c
@@@ -3,264 -4,49 +3,274 @@@
   * Copyright (C) 2010 Michal Simek <monstr@monstr.eu>
   * Copyright (C) 2010 PetaLogix
   * Copyright (C) 2005 John Williams <jwilliams@itee.uq.edu.au>
 + *
 + * Based on PowerPC version derived from arch/arm/mm/consistent.c
 + * Copyright (C) 2001 Dan Malek (dmalek@jlc.net)
 + * Copyright (C) 2000 Russell King
 + *
 + * This program is free software; you can redistribute it and/or modify
 + * it under the terms of the GNU General Public License version 2 as
 + * published by the Free Software Foundation.
   */
  
 +#include <linux/export.h>
 +#include <linux/signal.h>
 +#include <linux/sched.h>
  #include <linux/kernel.h>
 +#include <linux/errno.h>
  #include <linux/string.h>
  #include <linux/types.h>
 +#include <linux/ptrace.h>
 +#include <linux/mman.h>
  #include <linux/mm.h>
 +#include <linux/swap.h>
 +#include <linux/stddef.h>
 +#include <linux/vmalloc.h>
  #include <linux/init.h>
 -#include <linux/dma-noncoherent.h>
 -#include <asm/cpuinfo.h>
 -#include <asm/cacheflush.h>
 -
 -void arch_dma_prep_coherent(struct page *page, size_t size)
 -{
 -	phys_addr_t paddr = page_to_phys(page);
 +#include <linux/delay.h>
 +#include <linux/memblock.h>
 +#include <linux/highmem.h>
 +#include <linux/pci.h>
 +#include <linux/interrupt.h>
 +#include <linux/gfp.h>
  
 -	flush_dcache_range(paddr, paddr + size);
 -}
 +#include <asm/pgalloc.h>
 +#include <linux/io.h>
 +#include <linux/hardirq.h>
 +#include <linux/mmu_context.h>
 +#include <asm/mmu.h>
 +#include <linux/uaccess.h>
 +#include <asm/pgtable.h>
 +#include <asm/cpuinfo.h>
 +#include <asm/tlbflush.h>
  
  #ifndef CONFIG_MMU
 +/* I have to use dcache values because I can't relate on ram size */
 +# define UNCACHED_SHADOW_MASK (cpuinfo.dcache_high - cpuinfo.dcache_base + 1)
 +#endif
 +
  /*
 - * Consistent memory allocators. Used for DMA devices that want to share
 - * uncached memory with the processor core.  My crufty no-MMU approach is
 - * simple.  In the HW platform we can optionally mirror the DDR up above the
 - * processor cacheable region.  So, memory accessed in this mirror region will
 - * not be cached.  It's alloced from the same pool as normal memory, but the
 - * handle we return is shifted up into the uncached region.  This will no doubt
 - * cause big problems if memory allocated here is not also freed properly. -- JW
 - *
 - * I have to use dcache values because I can't relate on ram size:
 + * Consistent memory allocators. Used for DMA devices that want to
 + * share uncached memory with the processor core.
 + * My crufty no-MMU approach is simple. In the HW platform we can optionally
 + * mirror the DDR up above the processor cacheable region.  So, memory accessed
 + * in this mirror region will not be cached.  It's alloced from the same
 + * pool as normal memory, but the handle we return is shifted up into the
 + * uncached region.  This will no doubt cause big problems if memory allocated
 + * here is not also freed properly. -- JW
   */
++<<<<<<< HEAD
 +void *consistent_alloc(gfp_t gfp, size_t size, dma_addr_t *dma_handle)
++=======
+ #ifdef CONFIG_XILINX_UNCACHED_SHADOW
+ #define UNCACHED_SHADOW_MASK (cpuinfo.dcache_high - cpuinfo.dcache_base + 1)
+ #else
+ #define UNCACHED_SHADOW_MASK 0
+ #endif /* CONFIG_XILINX_UNCACHED_SHADOW */
+ 
+ void *arch_dma_set_uncached(void *ptr, size_t size)
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
  {
 -	unsigned long addr = (unsigned long)ptr;
 +	unsigned long order, vaddr;
 +	void *ret;
 +	unsigned int i, err = 0;
 +	struct page *page, *end;
 +
 +#ifdef CONFIG_MMU
 +	phys_addr_t pa;
 +	struct vm_struct *area;
 +	unsigned long va;
 +#endif
 +
 +	if (in_interrupt())
 +		BUG();
 +
 +	/* Only allocate page size areas. */
 +	size = PAGE_ALIGN(size);
 +	order = get_order(size);
  
 -	addr |= UNCACHED_SHADOW_MASK;
 -	if (addr > cpuinfo.dcache_base && addr < cpuinfo.dcache_high)
 +	vaddr = __get_free_pages(gfp, order);
 +	if (!vaddr)
 +		return NULL;
 +
 +	/*
 +	 * we need to ensure that there are no cachelines in use,
 +	 * or worse dirty in this area.
 +	 */
 +	flush_dcache_range(virt_to_phys((void *)vaddr),
 +					virt_to_phys((void *)vaddr) + size);
 +
 +#ifndef CONFIG_MMU
 +	ret = (void *)vaddr;
 +	/*
 +	 * Here's the magic!  Note if the uncached shadow is not implemented,
 +	 * it's up to the calling code to also test that condition and make
 +	 * other arranegments, such as manually flushing the cache and so on.
 +	 */
 +# ifdef CONFIG_XILINX_UNCACHED_SHADOW
 +	ret = (void *)((unsigned) ret | UNCACHED_SHADOW_MASK);
 +# endif
 +	if ((unsigned int)ret > cpuinfo.dcache_base &&
 +				(unsigned int)ret < cpuinfo.dcache_high)
  		pr_warn("ERROR: Your cache coherent area is CACHED!!!\n");
 -	return (void *)addr;
 +
 +	/* dma_handle is same as physical (shadowed) address */
 +	*dma_handle = (dma_addr_t)ret;
 +#else
 +	/* Allocate some common virtual space to map the new pages. */
 +	area = get_vm_area(size, VM_ALLOC);
 +	if (!area) {
 +		free_pages(vaddr, order);
 +		return NULL;
 +	}
 +	va = (unsigned long) area->addr;
 +	ret = (void *)va;
 +
 +	/* This gives us the real physical address of the first page. */
 +	*dma_handle = pa = __virt_to_phys(vaddr);
 +#endif
 +
 +	/*
 +	 * free wasted pages.  We skip the first page since we know
 +	 * that it will have count = 1 and won't require freeing.
 +	 * We also mark the pages in use as reserved so that
 +	 * remap_page_range works.
 +	 */
 +	page = virt_to_page(vaddr);
 +	end = page + (1 << order);
 +
 +	split_page(page, order);
 +
 +	for (i = 0; i < size && err == 0; i += PAGE_SIZE) {
 +#ifdef CONFIG_MMU
 +		/* MS: This is the whole magic - use cache inhibit pages */
 +		err = map_page(va + i, pa + i, _PAGE_KERNEL | _PAGE_NO_CACHE);
 +#endif
 +
 +		SetPageReserved(page);
 +		page++;
 +	}
 +
 +	/* Free the otherwise unused pages. */
 +	while (page < end) {
 +		__free_page(page);
 +		page++;
 +	}
 +
 +	if (err) {
 +		free_pages(vaddr, order);
 +		return NULL;
 +	}
 +
 +	return ret;
 +}
 +EXPORT_SYMBOL(consistent_alloc);
 +
 +#ifdef CONFIG_MMU
 +static pte_t *consistent_virt_to_pte(void *vaddr)
 +{
 +	unsigned long addr = (unsigned long)vaddr;
 +
 +	return pte_offset_kernel(pmd_offset(pgd_offset_k(addr), addr), addr);
 +}
 +
 +unsigned long consistent_virt_to_pfn(void *vaddr)
 +{
 +	pte_t *ptep = consistent_virt_to_pte(vaddr);
 +
 +	if (pte_none(*ptep) || !pte_present(*ptep))
 +		return 0;
 +
 +	return pte_pfn(*ptep);
 +}
 +#endif
 +
 +/*
 + * free page(s) as defined by the above mapping.
 + */
 +void consistent_free(size_t size, void *vaddr)
 +{
 +	struct page *page;
 +
 +	if (in_interrupt())
 +		BUG();
 +
 +	size = PAGE_ALIGN(size);
 +
 +#ifndef CONFIG_MMU
 +	/* Clear SHADOW_MASK bit in address, and free as per usual */
 +# ifdef CONFIG_XILINX_UNCACHED_SHADOW
 +	vaddr = (void *)((unsigned)vaddr & ~UNCACHED_SHADOW_MASK);
 +# endif
 +	page = virt_to_page(vaddr);
 +
 +	do {
 +		__free_reserved_page(page);
 +		page++;
 +	} while (size -= PAGE_SIZE);
 +#else
 +	do {
 +		pte_t *ptep = consistent_virt_to_pte(vaddr);
 +		unsigned long pfn;
 +
 +		if (!pte_none(*ptep) && pte_present(*ptep)) {
 +			pfn = pte_pfn(*ptep);
 +			pte_clear(&init_mm, (unsigned int)vaddr, ptep);
 +			if (pfn_valid(pfn)) {
 +				page = pfn_to_page(pfn);
 +				__free_reserved_page(page);
 +			}
 +		}
 +		vaddr += PAGE_SIZE;
 +	} while (size -= PAGE_SIZE);
 +
 +	/* flush tlb */
 +	flush_tlb_all();
 +#endif
 +}
 +EXPORT_SYMBOL(consistent_free);
 +
 +/*
 + * make an area consistent.
 + */
 +void consistent_sync(void *vaddr, size_t size, int direction)
 +{
 +	unsigned long start;
 +	unsigned long end;
 +
 +	start = (unsigned long)vaddr;
 +
 +	/* Convert start address back down to unshadowed memory region */
 +#ifdef CONFIG_XILINX_UNCACHED_SHADOW
 +	start &= ~UNCACHED_SHADOW_MASK;
 +#endif
 +	end = start + size;
 +
 +	switch (direction) {
 +	case PCI_DMA_NONE:
 +		BUG();
 +	case PCI_DMA_FROMDEVICE:	/* invalidate only */
 +		invalidate_dcache_range(start, end);
 +		break;
 +	case PCI_DMA_TODEVICE:		/* writeback only */
 +		flush_dcache_range(start, end);
 +		break;
 +	case PCI_DMA_BIDIRECTIONAL:	/* writeback and invalidate */
 +		flush_dcache_range(start, end);
 +		break;
 +	}
 +}
 +EXPORT_SYMBOL(consistent_sync);
 +
 +/*
 + * consistent_sync_page makes memory consistent. identical
 + * to consistent_sync, but takes a struct page instead of a
 + * virtual address
 + */
 +void consistent_sync_page(struct page *page, unsigned long offset,
 +	size_t size, int direction)
 +{
 +	unsigned long start = (unsigned long)page_address(page) + offset;
 +	consistent_sync((void *)start, size, direction);
  }
 -#endif /* CONFIG_MMU */
 +EXPORT_SYMBOL(consistent_sync_page);
diff --cc arch/mips/Kconfig
index 8ef87c03cf44,489185db501e..000000000000
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@@ -1115,6 -1179,19 +1115,22 @@@ config DMA_COHEREN
  
  config DMA_NONCOHERENT
  	bool
++<<<<<<< HEAD
++=======
+ 	#
+ 	# MIPS allows mixing "slightly different" Cacheability and Coherency
+ 	# Attribute bits.  It is believed that the uncached access through
+ 	# KSEG1 and the implementation specific "uncached accelerated" used
+ 	# by pgprot_writcombine can be mixed, and the latter sometimes provides
+ 	# significant advantages.
+ 	#
+ 	select ARCH_HAS_DMA_WRITE_COMBINE
+ 	select ARCH_HAS_DMA_PREP_COHERENT
+ 	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
+ 	select ARCH_HAS_DMA_SET_UNCACHED
+ 	select DMA_NONCOHERENT_MMAP
+ 	select DMA_NONCOHERENT_CACHE_SYNC
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
  	select NEED_DMA_MAP_STATE
  
  config SYS_HAS_EARLY_PRINTK
diff --cc arch/nios2/Kconfig
index c6ff28b77c86,2fc4ed210b5f..000000000000
--- a/arch/nios2/Kconfig
+++ b/arch/nios2/Kconfig
@@@ -1,6 -1,12 +1,15 @@@
  # SPDX-License-Identifier: GPL-2.0
  config NIOS2
  	def_bool y
++<<<<<<< HEAD
++=======
+ 	select ARCH_32BIT_OFF_T
+ 	select ARCH_HAS_DMA_PREP_COHERENT
+ 	select ARCH_HAS_SYNC_DMA_FOR_CPU
+ 	select ARCH_HAS_SYNC_DMA_FOR_DEVICE
+ 	select ARCH_HAS_DMA_SET_UNCACHED
+ 	select ARCH_NO_SWAP
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
  	select TIMER_OF
  	select GENERIC_ATOMIC64
  	select GENERIC_CLOCKEVENTS
diff --cc arch/nios2/mm/dma-mapping.c
index 4be815519dd4,fd887d5f3f9a..000000000000
--- a/arch/nios2/mm/dma-mapping.c
+++ b/arch/nios2/mm/dma-mapping.c
@@@ -58,147 -60,18 +58,151 @@@ static inline void __dma_sync_for_cpu(v
  	}
  }
  
 -void arch_dma_prep_coherent(struct page *page, size_t size)
 +static void *nios2_dma_alloc(struct device *dev, size_t size,
 +		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
  {
 -	unsigned long start = (unsigned long)page_address(page);
 +	void *ret;
 +
 +	/* optimized page clearing */
 +	gfp |= __GFP_ZERO;
 +
 +	if (dev == NULL || (dev->coherent_dma_mask < 0xffffffff))
 +		gfp |= GFP_DMA;
 +
 +	ret = (void *) __get_free_pages(gfp, get_order(size));
 +	if (ret != NULL) {
 +		*dma_handle = virt_to_phys(ret);
 +		flush_dcache_range((unsigned long) ret,
 +			(unsigned long) ret + size);
 +		ret = UNCAC_ADDR(ret);
 +	}
  
 -	flush_dcache_range(start, start + size);
 +	return ret;
  }
  
++<<<<<<< HEAD
 +static void nios2_dma_free(struct device *dev, size_t size, void *vaddr,
 +		dma_addr_t dma_handle, unsigned long attrs)
++=======
+ void *arch_dma_set_uncached(void *ptr, size_t size)
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
 +{
 +	unsigned long addr = (unsigned long) CAC_ADDR((unsigned long) vaddr);
 +
 +	free_pages(addr, get_order(size));
 +}
 +
 +static int nios2_dma_map_sg(struct device *dev, struct scatterlist *sg,
 +		int nents, enum dma_data_direction direction,
 +		unsigned long attrs)
 +{
 +	int i;
 +
 +	for_each_sg(sg, sg, nents, i) {
 +		void *addr = sg_virt(sg);
 +
 +		if (!addr)
 +			continue;
 +
 +		sg->dma_address = sg_phys(sg);
 +
 +		if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 +			continue;
 +
 +		__dma_sync_for_device(addr, sg->length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +static dma_addr_t nios2_dma_map_page(struct device *dev, struct page *page,
 +			unsigned long offset, size_t size,
 +			enum dma_data_direction direction,
 +			unsigned long attrs)
 +{
 +	void *addr = page_address(page) + offset;
 +
 +	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
 +		__dma_sync_for_device(addr, size, direction);
 +
 +	return page_to_phys(page) + offset;
 +}
 +
 +static void nios2_dma_unmap_page(struct device *dev, dma_addr_t dma_address,
 +		size_t size, enum dma_data_direction direction,
 +		unsigned long attrs)
 +{
 +	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
 +		__dma_sync_for_cpu(phys_to_virt(dma_address), size, direction);
 +}
 +
 +static void nios2_dma_unmap_sg(struct device *dev, struct scatterlist *sg,
 +		int nhwentries, enum dma_data_direction direction,
 +		unsigned long attrs)
  {
 -	unsigned long addr = (unsigned long)ptr;
 +	void *addr;
 +	int i;
 +
 +	if (direction == DMA_TO_DEVICE)
 +		return;
  
 -	addr |= CONFIG_NIOS2_IO_REGION_BASE;
 +	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 +		return;
  
 -	return (void *)ptr;
 +	for_each_sg(sg, sg, nhwentries, i) {
 +		addr = sg_virt(sg);
 +		if (addr)
 +			__dma_sync_for_cpu(addr, sg->length, direction);
 +	}
 +}
 +
 +static void nios2_dma_sync_single_for_cpu(struct device *dev,
 +		dma_addr_t dma_handle, size_t size,
 +		enum dma_data_direction direction)
 +{
 +	__dma_sync_for_cpu(phys_to_virt(dma_handle), size, direction);
  }
 +
 +static void nios2_dma_sync_single_for_device(struct device *dev,
 +		dma_addr_t dma_handle, size_t size,
 +		enum dma_data_direction direction)
 +{
 +	__dma_sync_for_device(phys_to_virt(dma_handle), size, direction);
 +}
 +
 +static void nios2_dma_sync_sg_for_cpu(struct device *dev,
 +		struct scatterlist *sg, int nelems,
 +		enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	/* Make sure that gcc doesn't leave the empty loop body.  */
 +	for_each_sg(sg, sg, nelems, i)
 +		__dma_sync_for_cpu(sg_virt(sg), sg->length, direction);
 +}
 +
 +static void nios2_dma_sync_sg_for_device(struct device *dev,
 +		struct scatterlist *sg, int nelems,
 +		enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	/* Make sure that gcc doesn't leave the empty loop body.  */
 +	for_each_sg(sg, sg, nelems, i)
 +		__dma_sync_for_device(sg_virt(sg), sg->length, direction);
 +
 +}
 +
 +const struct dma_map_ops nios2_dma_ops = {
 +	.alloc			= nios2_dma_alloc,
 +	.free			= nios2_dma_free,
 +	.map_page		= nios2_dma_map_page,
 +	.unmap_page		= nios2_dma_unmap_page,
 +	.map_sg			= nios2_dma_map_sg,
 +	.unmap_sg		= nios2_dma_unmap_sg,
 +	.sync_single_for_device	= nios2_dma_sync_single_for_device,
 +	.sync_single_for_cpu	= nios2_dma_sync_single_for_cpu,
 +	.sync_sg_for_cpu	= nios2_dma_sync_sg_for_cpu,
 +	.sync_sg_for_device	= nios2_dma_sync_sg_for_device,
 +};
 +EXPORT_SYMBOL(nios2_dma_ops);
diff --cc arch/xtensa/Kconfig
index 05be209ece4d,de229424b659..000000000000
--- a/arch/xtensa/Kconfig
+++ b/arch/xtensa/Kconfig
@@@ -1,15 -1,20 +1,26 @@@
  # SPDX-License-Identifier: GPL-2.0
 +config ZONE_DMA
 +	def_bool y
 +
  config XTENSA
  	def_bool y
++<<<<<<< HEAD
 +	select ARCH_NO_COHERENT_DMA_MMAP if !MMU
++=======
+ 	select ARCH_32BIT_OFF_T
+ 	select ARCH_HAS_BINFMT_FLAT if !MMU
+ 	select ARCH_HAS_DMA_PREP_COHERENT if MMU
+ 	select ARCH_HAS_SYNC_DMA_FOR_CPU if MMU
+ 	select ARCH_HAS_SYNC_DMA_FOR_DEVICE if MMU
+ 	select ARCH_HAS_DMA_SET_UNCACHED if MMU
+ 	select ARCH_USE_QUEUED_RWLOCKS
+ 	select ARCH_USE_QUEUED_SPINLOCKS
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
  	select ARCH_WANT_FRAME_POINTERS
  	select ARCH_WANT_IPC_PARSE_VERSION
 -	select BUILDTIME_TABLE_SORT
 +	select BUILDTIME_EXTABLE_SORT
  	select CLONE_BACKWARDS
  	select COMMON_CLK
 -	select DMA_REMAP if MMU
  	select GENERIC_ATOMIC64
  	select GENERIC_CLOCKEVENTS
  	select GENERIC_IRQ_SHOW
diff --cc arch/xtensa/kernel/pci-dma.c
index a02dc563d290,17c4384f8495..000000000000
--- a/arch/xtensa/kernel/pci-dma.c
+++ b/arch/xtensa/kernel/pci-dma.c
@@@ -115,149 -87,13 +115,154 @@@ static void xtensa_sync_sg_for_device(s
  }
  
  /*
 - * Memory caching is platform-dependent in noMMU xtensa configurations.
 - * This function should be implemented in platform code in order to enable
 - * coherent DMA memory operations when CONFIG_MMU is not enabled.
 + * Note: We assume that the full memory space is always mapped to 'kseg'
 + *	 Otherwise we have to use page attributes (not implemented).
   */
++<<<<<<< HEAD
 +
 +static void *xtensa_dma_alloc(struct device *dev, size_t size,
 +			      dma_addr_t *handle, gfp_t flag,
 +			      unsigned long attrs)
++=======
+ #ifdef CONFIG_MMU
+ void *arch_dma_set_uncached(void *p, size_t size)
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
 +{
 +	unsigned long ret;
 +	unsigned long uncached;
 +	unsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;
 +	struct page *page = NULL;
 +
 +	/* ignore region speicifiers */
 +
 +	flag &= ~(__GFP_DMA | __GFP_HIGHMEM);
 +
 +	if (dev == NULL || (dev->coherent_dma_mask < 0xffffffff))
 +		flag |= GFP_DMA;
 +
 +	if (gfpflags_allow_blocking(flag))
 +		page = dma_alloc_from_contiguous(dev, count, get_order(size),
 +						 flag & __GFP_NOWARN);
 +
 +	if (!page)
 +		page = alloc_pages(flag, get_order(size));
 +
 +	if (!page)
 +		return NULL;
 +
 +	*handle = phys_to_dma(dev, page_to_phys(page));
 +
 +#ifdef CONFIG_MMU
 +	if (PageHighMem(page)) {
 +		void *p;
 +
 +		p = dma_common_contiguous_remap(page, size, VM_MAP,
 +						pgprot_noncached(PAGE_KERNEL),
 +						__builtin_return_address(0));
 +		if (!p) {
 +			if (!dma_release_from_contiguous(dev, page, count))
 +				__free_pages(page, get_order(size));
 +		}
 +		return p;
 +	}
 +#endif
 +	ret = (unsigned long)page_address(page);
 +	BUG_ON(ret < XCHAL_KSEG_CACHED_VADDR ||
 +	       ret > XCHAL_KSEG_CACHED_VADDR + XCHAL_KSEG_SIZE - 1);
 +
 +	uncached = ret + XCHAL_KSEG_BYPASS_VADDR - XCHAL_KSEG_CACHED_VADDR;
 +	__invalidate_dcache_range(ret, size);
 +
 +	return (void *)uncached;
 +}
 +
 +static void xtensa_dma_free(struct device *dev, size_t size, void *vaddr,
 +			    dma_addr_t dma_handle, unsigned long attrs)
  {
 -	return p + XCHAL_KSEG_BYPASS_VADDR - XCHAL_KSEG_CACHED_VADDR;
 +	unsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;
 +	unsigned long addr = (unsigned long)vaddr;
 +	struct page *page;
 +
 +	if (addr >= XCHAL_KSEG_BYPASS_VADDR &&
 +	    addr - XCHAL_KSEG_BYPASS_VADDR < XCHAL_KSEG_SIZE) {
 +		addr += XCHAL_KSEG_CACHED_VADDR - XCHAL_KSEG_BYPASS_VADDR;
 +		page = virt_to_page(addr);
 +	} else {
 +#ifdef CONFIG_MMU
 +		dma_common_free_remap(vaddr, size, VM_MAP);
 +#endif
 +		page = pfn_to_page(PHYS_PFN(dma_to_phys(dev, dma_handle)));
 +	}
 +
 +	if (!dma_release_from_contiguous(dev, page, count))
 +		__free_pages(page, get_order(size));
 +}
 +
 +static dma_addr_t xtensa_map_page(struct device *dev, struct page *page,
 +				  unsigned long offset, size_t size,
 +				  enum dma_data_direction dir,
 +				  unsigned long attrs)
 +{
 +	dma_addr_t dma_handle = page_to_phys(page) + offset;
 +
 +	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
 +		xtensa_sync_single_for_device(dev, dma_handle, size, dir);
 +
 +	return dma_handle;
 +}
 +
 +static void xtensa_unmap_page(struct device *dev, dma_addr_t dma_handle,
 +			      size_t size, enum dma_data_direction dir,
 +			      unsigned long attrs)
 +{
 +	if (!(attrs & DMA_ATTR_SKIP_CPU_SYNC))
 +		xtensa_sync_single_for_cpu(dev, dma_handle, size, dir);
 +}
 +
 +static int xtensa_map_sg(struct device *dev, struct scatterlist *sg,
 +			 int nents, enum dma_data_direction dir,
 +			 unsigned long attrs)
 +{
 +	struct scatterlist *s;
 +	int i;
 +
 +	for_each_sg(sg, s, nents, i) {
 +		s->dma_address = xtensa_map_page(dev, sg_page(s), s->offset,
 +						 s->length, dir, attrs);
 +	}
 +	return nents;
  }
 -#endif /* CONFIG_MMU */
 +
 +static void xtensa_unmap_sg(struct device *dev,
 +			    struct scatterlist *sg, int nents,
 +			    enum dma_data_direction dir,
 +			    unsigned long attrs)
 +{
 +	struct scatterlist *s;
 +	int i;
 +
 +	for_each_sg(sg, s, nents, i) {
 +		xtensa_unmap_page(dev, sg_dma_address(s),
 +				  sg_dma_len(s), dir, attrs);
 +	}
 +}
 +
 +int xtensa_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +const struct dma_map_ops xtensa_dma_map_ops = {
 +	.alloc = xtensa_dma_alloc,
 +	.free = xtensa_dma_free,
 +	.map_page = xtensa_map_page,
 +	.unmap_page = xtensa_unmap_page,
 +	.map_sg = xtensa_map_sg,
 +	.unmap_sg = xtensa_unmap_sg,
 +	.sync_single_for_cpu = xtensa_sync_single_for_cpu,
 +	.sync_single_for_device = xtensa_sync_single_for_device,
 +	.sync_sg_for_cpu = xtensa_sync_sg_for_cpu,
 +	.sync_sg_for_device = xtensa_sync_sg_for_device,
 +	.mapping_error = xtensa_dma_mapping_error,
 +};
 +EXPORT_SYMBOL(xtensa_dma_map_ops);
diff --cc include/linux/dma-noncoherent.h
index ecd8259e2fe3,1a4039506673..000000000000
--- a/include/linux/dma-noncoherent.h
+++ b/include/linux/dma-noncoherent.h
@@@ -108,7 -108,6 +108,11 @@@ static inline void arch_dma_prep_cohere
  }
  #endif /* CONFIG_ARCH_HAS_DMA_PREP_COHERENT */
  
++<<<<<<< HEAD
 +void *uncached_kernel_address(void *addr);
 +void *cached_kernel_address(void *addr);
++=======
+ void *arch_dma_set_uncached(void *addr, size_t size);
++>>>>>>> fa7e2247c572 (dma-direct: make uncached_kernel_address more general)
  
  #endif /* _LINUX_DMA_NONCOHERENT_H */
* Unmerged path arch/mips/mm/dma-noncoherent.c
* Unmerged path arch/Kconfig
* Unmerged path arch/microblaze/Kconfig
* Unmerged path arch/microblaze/mm/consistent.c
* Unmerged path arch/mips/Kconfig
* Unmerged path arch/mips/mm/dma-noncoherent.c
* Unmerged path arch/nios2/Kconfig
* Unmerged path arch/nios2/mm/dma-mapping.c
* Unmerged path arch/xtensa/Kconfig
* Unmerged path arch/xtensa/kernel/pci-dma.c
* Unmerged path include/linux/dma-noncoherent.h
diff --git a/kernel/dma/direct.c b/kernel/dma/direct.c
index 78ee188d0def..6c0c1ff45c7a 100644
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@ -180,10 +180,12 @@ void *dma_direct_alloc_pages(struct device *dev, size_t size,
 
 	memset(ret, 0, size);
 
-	if (IS_ENABLED(CONFIG_ARCH_HAS_UNCACHED_SEGMENT) &&
+	if (IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&
 	    dma_alloc_need_uncached(dev, attrs)) {
 		arch_dma_prep_coherent(page, size);
-		ret = uncached_kernel_address(ret);
+		ret = arch_dma_set_uncached(ret, size);
+		if (IS_ERR(ret))
+			goto out_free_pages;
 	}
 done:
 	if (force_dma_unencrypted(dev))
@@ -224,7 +226,7 @@ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 void *dma_direct_alloc(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)
 {
-	if (!IS_ENABLED(CONFIG_ARCH_HAS_UNCACHED_SEGMENT) &&
+	if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&
 	    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
 	    dma_alloc_need_uncached(dev, attrs))
 		return arch_dma_alloc(dev, size, dma_handle, gfp, attrs);
@@ -234,7 +236,7 @@ void *dma_direct_alloc(struct device *dev, size_t size,
 void dma_direct_free(struct device *dev, size_t size,
 		void *cpu_addr, dma_addr_t dma_addr, unsigned long attrs)
 {
-	if (!IS_ENABLED(CONFIG_ARCH_HAS_UNCACHED_SEGMENT) &&
+	if (!IS_ENABLED(CONFIG_ARCH_HAS_DMA_SET_UNCACHED) &&
 	    !IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
 	    dma_alloc_need_uncached(dev, attrs))
 		arch_dma_free(dev, size, cpu_addr, dma_addr, attrs);
