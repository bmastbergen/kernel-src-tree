tcp: optimize tcp internal pacing

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Eric Dumazet <edumazet@google.com>
commit 864e5c090749448e879e86bec06ee396aa2c19c5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/864e5c09.failed

When TCP implements its own pacing (when no fq packet scheduler is used),
it is arming high resolution timer after a packet is sent.

But in many cases (like TCP_RR kind of workloads), this high resolution
timer expires before the application attempts to write the following
packet. This overhead also happens when the flow is ACK clocked and
cwnd limited instead of being limited by the pacing rate.

This leads to extra overhead (high number of IRQ)

Now tcp_wstamp_ns is reserved for the pacing timer only
(after commit "tcp: do not change tcp_wstamp_ns in tcp_mstamp_refresh"),
we can setup the timer only when a packet is about to be sent,
and if tcp_wstamp_ns is in the future.

This leads to a ~10% performance increase in TCP_RR workloads.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 864e5c090749448e879e86bec06ee396aa2c19c5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_output.c
diff --cc net/ipv4/tcp_output.c
index 42d3de51dbfe,d212e4cbc689..000000000000
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@@ -1059,28 -975,28 +1059,52 @@@ enum hrtimer_restart tcp_pace_kick(stru
  	return HRTIMER_NORESTART;
  }
  
++<<<<<<< HEAD
 +static void tcp_internal_pacing(struct sock *sk, const struct sk_buff *skb)
 +{
 +	u64 len_ns;
 +	u32 rate;
 +
 +	if (!tcp_needs_internal_pacing(sk))
 +		return;
 +	rate = sk->sk_pacing_rate;
 +	if (!rate || rate == ~0U)
 +		return;
 +
 +	len_ns = (u64)skb->len * NSEC_PER_SEC;
 +	do_div(len_ns, rate);
 +	hrtimer_start(&tcp_sk(sk)->pacing_timer,
 +		      ktime_add_ns(ktime_get_tai_ns(), len_ns),
 +		      HRTIMER_MODE_ABS_PINNED_SOFT);
 +	sock_hold(sk);
 +}
 +
 +static void tcp_update_skb_after_send(struct tcp_sock *tp, struct sk_buff *skb)
++=======
+ static void tcp_update_skb_after_send(struct sock *sk, struct sk_buff *skb,
+ 				      u64 prior_wstamp)
++>>>>>>> 864e5c090749 (tcp: optimize tcp internal pacing)
  {
 -	struct tcp_sock *tp = tcp_sk(sk);
 -
  	skb->skb_mstamp_ns = tp->tcp_wstamp_ns;
++<<<<<<< HEAD
++=======
+ 	if (sk->sk_pacing_status != SK_PACING_NONE) {
+ 		unsigned long rate = sk->sk_pacing_rate;
+ 
+ 		/* Original sch_fq does not pace first 10 MSS
+ 		 * Note that tp->data_segs_out overflows after 2^32 packets,
+ 		 * this is a minor annoyance.
+ 		 */
+ 		if (rate != ~0UL && rate && tp->data_segs_out >= 10) {
+ 			u64 len_ns = div64_ul((u64)skb->len * NSEC_PER_SEC, rate);
+ 			u64 credit = tp->tcp_wstamp_ns - prior_wstamp;
+ 
+ 			/* take into account OS jitter */
+ 			len_ns -= min_t(u64, len_ns / 2, credit);
+ 			tp->tcp_wstamp_ns += len_ns;
+ 		}
+ 	}
++>>>>>>> 864e5c090749 (tcp: optimize tcp internal pacing)
  	list_move_tail(&skb->tcp_tsorted_anchor, &tp->tsorted_sent_queue);
  }
  
* Unmerged path net/ipv4/tcp_output.c
