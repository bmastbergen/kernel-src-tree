powerpc/smp: Create coregroup domain

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Srikar Dronamraju <srikar@linux.vnet.ibm.com>
commit 72730bfc2a2b91a525f38dfc830f598bdb95f216
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/72730bfc.failed

Add percpu coregroup maps and masks to create coregroup domain.
If a coregroup doesn't exist, the coregroup domain will be degenerated
in favour of SMT/CACHE domain. Do note this patch is only creating stubs
for cpu_to_coregroup_id. The actual cpu_to_coregroup_id implementation
would be in a subsequent patch.

	Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20200810071834.92514-10-srikar@linux.vnet.ibm.com
(cherry picked from commit 72730bfc2a2b91a525f38dfc830f598bdb95f216)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/topology.h
#	arch/powerpc/mm/numa.c
diff --cc arch/powerpc/include/asm/topology.h
index e2e1ccd4a18d,6609174918ab..000000000000
--- a/arch/powerpc/include/asm/topology.h
+++ b/arch/powerpc/include/asm/topology.h
@@@ -93,37 -87,23 +93,53 @@@ static inline int cpu_distance(__be32 *
  #endif /* CONFIG_NUMA */
  
  #if defined(CONFIG_NUMA) && defined(CONFIG_PPC_SPLPAR)
 +extern int start_topology_update(void);
 +extern int stop_topology_update(void);
 +extern int prrn_is_enabled(void);
  extern int find_and_online_cpu_nid(int cpu);
++<<<<<<< HEAD
 +extern int timed_topology_update(int nsecs);
 +extern void __init shared_proc_topology_init(void);
++=======
+ extern int cpu_to_coregroup_id(int cpu);
++>>>>>>> 72730bfc2a2b (powerpc/smp: Create coregroup domain)
  #else
 +static inline int start_topology_update(void)
 +{
 +	return 0;
 +}
 +static inline int stop_topology_update(void)
 +{
 +	return 0;
 +}
 +static inline int prrn_is_enabled(void)
 +{
 +	return 0;
 +}
  static inline int find_and_online_cpu_nid(int cpu)
  {
  	return 0;
  }
 +static inline int timed_topology_update(int nsecs)
 +{
 +	return 0;
 +}
  
++<<<<<<< HEAD
 +#ifdef CONFIG_SMP
 +static inline void shared_proc_topology_init(void) {}
 +#endif
++=======
+ static inline int cpu_to_coregroup_id(int cpu)
+ {
+ #ifdef CONFIG_SMP
+ 	return cpu_to_core_id(cpu);
+ #else
+ 	return 0;
+ #endif
+ }
+ 
++>>>>>>> 72730bfc2a2b (powerpc/smp: Create coregroup domain)
  #endif /* CONFIG_NUMA && CONFIG_PPC_SPLPAR */
  
  #include <asm-generic/topology.h>
diff --cc arch/powerpc/mm/numa.c
index 521195e99be3,dfebca905acb..000000000000
--- a/arch/powerpc/mm/numa.c
+++ b/arch/powerpc/mm/numa.c
@@@ -1271,389 -1245,13 +1271,397 @@@ int find_and_online_cpu_nid(int cpu
  	return new_nid;
  }
  
++<<<<<<< HEAD
 +/*
 + * Update the CPU maps and sysfs entries for a single CPU when its NUMA
 + * characteristics change. This function doesn't perform any locking and is
 + * only safe to call from stop_machine().
 + */
 +static int update_cpu_topology(void *data)
 +{
 +	struct topology_update_data *update;
 +	unsigned long cpu;
 +
 +	if (!data)
 +		return -EINVAL;
 +
 +	cpu = smp_processor_id();
 +
 +	for (update = data; update; update = update->next) {
 +		int new_nid = update->new_nid;
 +		if (cpu != update->cpu)
 +			continue;
 +
 +		unmap_cpu_from_node(cpu);
 +		map_cpu_to_node(cpu, new_nid);
 +		set_cpu_numa_node(cpu, new_nid);
 +		set_cpu_numa_mem(cpu, local_memory_node(new_nid));
 +		vdso_getcpu_init();
 +	}
 +
 +	return 0;
 +}
 +
 +static int update_lookup_table(void *data)
 +{
 +	struct topology_update_data *update;
 +
 +	if (!data)
 +		return -EINVAL;
 +
 +	/*
 +	 * Upon topology update, the numa-cpu lookup table needs to be updated
 +	 * for all threads in the core, including offline CPUs, to ensure that
 +	 * future hotplug operations respect the cpu-to-node associativity
 +	 * properly.
 +	 */
 +	for (update = data; update; update = update->next) {
 +		int nid, base, j;
 +
 +		nid = update->new_nid;
 +		base = cpu_first_thread_sibling(update->cpu);
 +
 +		for (j = 0; j < threads_per_core; j++) {
 +			update_numa_cpu_lookup_table(base + j, nid);
 +		}
 +	}
 +
 +	return 0;
 +}
 +
 +/*
 + * Update the node maps and sysfs entries for each cpu whose home node
 + * has changed. Returns 1 when the topology has changed, and 0 otherwise.
 + *
 + * cpus_locked says whether we already hold cpu_hotplug_lock.
 + */
 +int numa_update_cpu_topology(bool cpus_locked)
 +{
 +	unsigned int cpu, sibling, changed = 0;
 +	struct topology_update_data *updates, *ud;
 +	cpumask_t updated_cpus;
 +	struct device *dev;
 +	int weight, new_nid, i = 0;
 +
 +	if (!prrn_enabled && !vphn_enabled && topology_inited)
 +		return 0;
 +
 +	weight = cpumask_weight(&cpu_associativity_changes_mask);
 +	if (!weight)
 +		return 0;
 +
 +	updates = kcalloc(weight, sizeof(*updates), GFP_KERNEL);
 +	if (!updates)
 +		return 0;
 +
 +	cpumask_clear(&updated_cpus);
 +
 +	for_each_cpu(cpu, &cpu_associativity_changes_mask) {
 +		/*
 +		 * If siblings aren't flagged for changes, updates list
 +		 * will be too short. Skip on this update and set for next
 +		 * update.
 +		 */
 +		if (!cpumask_subset(cpu_sibling_mask(cpu),
 +					&cpu_associativity_changes_mask)) {
 +			pr_info("Sibling bits not set for associativity "
 +					"change, cpu%d\n", cpu);
 +			cpumask_or(&cpu_associativity_changes_mask,
 +					&cpu_associativity_changes_mask,
 +					cpu_sibling_mask(cpu));
 +			cpu = cpu_last_thread_sibling(cpu);
 +			continue;
 +		}
 +
 +		new_nid = find_and_online_cpu_nid(cpu);
 +
 +		if (new_nid == numa_cpu_lookup_table[cpu]) {
 +			cpumask_andnot(&cpu_associativity_changes_mask,
 +					&cpu_associativity_changes_mask,
 +					cpu_sibling_mask(cpu));
 +			dbg("Assoc chg gives same node %d for cpu%d\n",
 +					new_nid, cpu);
 +			cpu = cpu_last_thread_sibling(cpu);
 +			continue;
 +		}
 +
 +		for_each_cpu(sibling, cpu_sibling_mask(cpu)) {
 +			ud = &updates[i++];
 +			ud->next = &updates[i];
 +			ud->cpu = sibling;
 +			ud->new_nid = new_nid;
 +			ud->old_nid = numa_cpu_lookup_table[sibling];
 +			cpumask_set_cpu(sibling, &updated_cpus);
 +		}
 +		cpu = cpu_last_thread_sibling(cpu);
 +	}
 +
 +	/*
 +	 * Prevent processing of 'updates' from overflowing array
 +	 * where last entry filled in a 'next' pointer.
 +	 */
 +	if (i)
 +		updates[i-1].next = NULL;
 +
 +	pr_debug("Topology update for the following CPUs:\n");
 +	if (cpumask_weight(&updated_cpus)) {
 +		for (ud = &updates[0]; ud; ud = ud->next) {
 +			pr_debug("cpu %d moving from node %d "
 +					  "to %d\n", ud->cpu,
 +					  ud->old_nid, ud->new_nid);
 +		}
 +	}
 +
 +	/*
 +	 * In cases where we have nothing to update (because the updates list
 +	 * is too short or because the new topology is same as the old one),
 +	 * skip invoking update_cpu_topology() via stop-machine(). This is
 +	 * necessary (and not just a fast-path optimization) since stop-machine
 +	 * can end up electing a random CPU to run update_cpu_topology(), and
 +	 * thus trick us into setting up incorrect cpu-node mappings (since
 +	 * 'updates' is kzalloc()'ed).
 +	 *
 +	 * And for the similar reason, we will skip all the following updating.
 +	 */
 +	if (!cpumask_weight(&updated_cpus))
 +		goto out;
 +
 +	if (cpus_locked)
 +		stop_machine_cpuslocked(update_cpu_topology, &updates[0],
 +					&updated_cpus);
 +	else
 +		stop_machine(update_cpu_topology, &updates[0], &updated_cpus);
 +
 +	/*
 +	 * Update the numa-cpu lookup table with the new mappings, even for
 +	 * offline CPUs. It is best to perform this update from the stop-
 +	 * machine context.
 +	 */
 +	if (cpus_locked)
 +		stop_machine_cpuslocked(update_lookup_table, &updates[0],
 +					cpumask_of(raw_smp_processor_id()));
 +	else
 +		stop_machine(update_lookup_table, &updates[0],
 +			     cpumask_of(raw_smp_processor_id()));
 +
 +	for (ud = &updates[0]; ud; ud = ud->next) {
 +		unregister_cpu_under_node(ud->cpu, ud->old_nid);
 +		register_cpu_under_node(ud->cpu, ud->new_nid);
 +
 +		dev = get_cpu_device(ud->cpu);
 +		if (dev)
 +			kobject_uevent(&dev->kobj, KOBJ_CHANGE);
 +		cpumask_clear_cpu(ud->cpu, &cpu_associativity_changes_mask);
 +		changed = 1;
 +	}
 +
 +out:
 +	kfree(updates);
 +	return changed;
 +}
 +
 +int arch_update_cpu_topology(void)
 +{
 +	return numa_update_cpu_topology(true);
 +}
 +
 +static void topology_work_fn(struct work_struct *work)
 +{
 +	rebuild_sched_domains();
 +}
 +static DECLARE_WORK(topology_work, topology_work_fn);
 +
 +static void topology_schedule_update(void)
 +{
 +	schedule_work(&topology_work);
 +}
 +
 +static void topology_timer_fn(struct timer_list *unused)
 +{
 +	if (prrn_enabled && cpumask_weight(&cpu_associativity_changes_mask))
 +		topology_schedule_update();
 +	else if (vphn_enabled) {
 +		if (update_cpu_associativity_changes_mask() > 0)
 +			topology_schedule_update();
 +		reset_topology_timer();
 +	}
 +}
 +static struct timer_list topology_timer;
 +
 +static void reset_topology_timer(void)
 +{
 +	if (vphn_enabled)
 +		mod_timer(&topology_timer, jiffies + topology_timer_secs * HZ);
 +}
 +
 +#ifdef CONFIG_SMP
 +
 +static int dt_update_callback(struct notifier_block *nb,
 +				unsigned long action, void *data)
 +{
 +	struct of_reconfig_data *update = data;
 +	int rc = NOTIFY_DONE;
 +
 +	switch (action) {
 +	case OF_RECONFIG_UPDATE_PROPERTY:
 +		if (!of_prop_cmp(update->dn->type, "cpu") &&
 +		    !of_prop_cmp(update->prop->name, "ibm,associativity")) {
 +			u32 core_id;
 +			of_property_read_u32(update->dn, "reg", &core_id);
 +			rc = dlpar_cpu_readd(core_id);
 +			rc = NOTIFY_OK;
 +		}
 +		break;
 +	}
 +
 +	return rc;
 +}
 +
 +static struct notifier_block dt_update_nb = {
 +	.notifier_call = dt_update_callback,
 +};
 +
 +#endif
 +
 +/*
 + * Start polling for associativity changes.
 + */
 +int start_topology_update(void)
 +{
 +	int rc = 0;
 +
 +	if (!topology_updates_enabled)
 +		return 0;
 +
 +	if (firmware_has_feature(FW_FEATURE_PRRN)) {
 +		if (!prrn_enabled) {
 +			prrn_enabled = 1;
 +#ifdef CONFIG_SMP
 +			rc = of_reconfig_notifier_register(&dt_update_nb);
 +#endif
 +		}
 +	}
 +	if (firmware_has_feature(FW_FEATURE_VPHN) &&
 +		   lppaca_shared_proc(get_lppaca())) {
 +		if (!vphn_enabled) {
 +			vphn_enabled = 1;
 +			setup_cpu_associativity_change_counters();
 +			timer_setup(&topology_timer, topology_timer_fn,
 +				    TIMER_DEFERRABLE);
 +			reset_topology_timer();
 +		}
 +	}
 +
 +	pr_info("Starting topology update%s%s\n",
 +		(prrn_enabled ? " prrn_enabled" : ""),
 +		(vphn_enabled ? " vphn_enabled" : ""));
 +
 +	return rc;
 +}
 +
 +/*
 + * Disable polling for VPHN associativity changes.
 + */
 +int stop_topology_update(void)
 +{
 +	int rc = 0;
 +
 +	if (!topology_updates_enabled)
 +		return 0;
 +
 +	if (prrn_enabled) {
 +		prrn_enabled = 0;
 +#ifdef CONFIG_SMP
 +		rc = of_reconfig_notifier_unregister(&dt_update_nb);
 +#endif
 +	}
 +	if (vphn_enabled) {
 +		vphn_enabled = 0;
 +		rc = del_timer_sync(&topology_timer);
 +	}
 +
 +	pr_info("Stopping topology update\n");
 +
 +	return rc;
 +}
 +
 +int prrn_is_enabled(void)
 +{
 +	return prrn_enabled;
 +}
 +
 +void __init shared_proc_topology_init(void)
 +{
 +	if (lppaca_shared_proc(get_lppaca())) {
 +		bitmap_fill(cpumask_bits(&cpu_associativity_changes_mask),
 +			    nr_cpumask_bits);
 +		numa_update_cpu_topology(false);
 +	}
 +}
 +
 +static int topology_read(struct seq_file *file, void *v)
 +{
 +	if (vphn_enabled || prrn_enabled)
 +		seq_puts(file, "on\n");
 +	else
 +		seq_puts(file, "off\n");
 +
 +	return 0;
 +}
 +
 +static int topology_open(struct inode *inode, struct file *file)
 +{
 +	return single_open(file, topology_read, NULL);
 +}
 +
 +static ssize_t topology_write(struct file *file, const char __user *buf,
 +			      size_t count, loff_t *off)
 +{
 +	char kbuf[4]; /* "on" or "off" plus null. */
 +	int read_len;
 +
 +	read_len = count < 3 ? count : 3;
 +	if (copy_from_user(kbuf, buf, read_len))
 +		return -EINVAL;
 +
 +	kbuf[read_len] = '\0';
 +
 +	if (!strncmp(kbuf, "on", 2)) {
 +		topology_updates_enabled = true;
 +		start_topology_update();
 +	} else if (!strncmp(kbuf, "off", 3)) {
 +		stop_topology_update();
 +		topology_updates_enabled = false;
 +	} else
 +		return -EINVAL;
 +
 +	return count;
 +}
 +
 +static const struct file_operations topology_ops = {
 +	.read = seq_read,
 +	.write = topology_write,
 +	.open = topology_open,
 +	.release = single_release
 +};
 +
++=======
+ int cpu_to_coregroup_id(int cpu)
+ {
+ 	return cpu_to_core_id(cpu);
+ }
+ 
++>>>>>>> 72730bfc2a2b (powerpc/smp: Create coregroup domain)
  static int topology_update_init(void)
  {
 +	start_topology_update();
 +
 +	if (vphn_enabled)
 +		topology_schedule_update();
 +
 +	if (!proc_create("powerpc/topology_updates", 0644, NULL, &topology_ops))
 +		return -ENOMEM;
 +
  	topology_inited = 1;
  	return 0;
  }
* Unmerged path arch/powerpc/include/asm/topology.h
diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c
index 210555f6a379..95cacfa2d8d3 100644
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@ -83,12 +83,22 @@ DEFINE_PER_CPU(cpumask_var_t, cpu_sibling_map);
 DEFINE_PER_CPU(cpumask_var_t, cpu_smallcore_map);
 DEFINE_PER_CPU(cpumask_var_t, cpu_l2_cache_map);
 DEFINE_PER_CPU(cpumask_var_t, cpu_core_map);
+DEFINE_PER_CPU(cpumask_var_t, cpu_coregroup_map);
 
 EXPORT_PER_CPU_SYMBOL(cpu_sibling_map);
 EXPORT_PER_CPU_SYMBOL(cpu_l2_cache_map);
 EXPORT_PER_CPU_SYMBOL(cpu_core_map);
 EXPORT_SYMBOL_GPL(has_big_cores);
 
+enum {
+#ifdef CONFIG_SCHED_SMT
+	smt_idx,
+#endif
+	cache_idx,
+	mc_idx,
+	die_idx,
+};
+
 #define MAX_THREAD_LIST_SIZE	8
 #define THREAD_GROUP_SHARE_L1   1
 struct thread_groups {
@@ -863,11 +873,27 @@ static const struct cpumask *smallcore_smt_mask(int cpu)
 }
 #endif
 
+static struct cpumask *cpu_coregroup_mask(int cpu)
+{
+	return per_cpu(cpu_coregroup_map, cpu);
+}
+
+static bool has_coregroup_support(void)
+{
+	return coregroup_enabled;
+}
+
+static const struct cpumask *cpu_mc_mask(int cpu)
+{
+	return cpu_coregroup_mask(cpu);
+}
+
 static struct sched_domain_topology_level powerpc_topology[] = {
 #ifdef CONFIG_SCHED_SMT
 	{ cpu_smt_mask, powerpc_smt_flags, SD_INIT_NAME(SMT) },
 #endif
 	{ shared_cache_mask, powerpc_shared_cache_flags, SD_INIT_NAME(CACHE) },
+	{ cpu_mc_mask, SD_INIT_NAME(MC) },
 	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
 	{ NULL, },
 };
@@ -914,6 +940,10 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 					GFP_KERNEL, cpu_to_node(cpu));
 		zalloc_cpumask_var_node(&per_cpu(cpu_core_map, cpu),
 					GFP_KERNEL, cpu_to_node(cpu));
+		if (has_coregroup_support())
+			zalloc_cpumask_var_node(&per_cpu(cpu_coregroup_map, cpu),
+						GFP_KERNEL, cpu_to_node(cpu));
+
 #ifdef CONFIG_NEED_MULTIPLE_NODES
 		/*
 		 * numa_node_id() works after this.
@@ -931,6 +961,9 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	cpumask_set_cpu(boot_cpuid, cpu_l2_cache_mask(boot_cpuid));
 	cpumask_set_cpu(boot_cpuid, cpu_core_mask(boot_cpuid));
 
+	if (has_coregroup_support())
+		cpumask_set_cpu(boot_cpuid, cpu_coregroup_mask(boot_cpuid));
+
 	init_big_cores();
 	if (has_big_cores) {
 		cpumask_set_cpu(boot_cpuid,
@@ -1236,6 +1269,8 @@ static void remove_cpu_from_masks(int cpu)
 		set_cpus_unrelated(cpu, i, cpu_sibling_mask);
 		if (has_big_cores)
 			set_cpus_unrelated(cpu, i, cpu_smallcore_mask);
+		if (has_coregroup_support())
+			set_cpus_unrelated(cpu, i, cpu_coregroup_mask);
 	}
 }
 #endif
@@ -1300,6 +1335,20 @@ static void add_cpu_to_masks(int cpu)
 	add_cpu_to_smallcore_masks(cpu);
 	update_mask_by_l2(cpu, cpu_l2_cache_mask);
 
+	if (has_coregroup_support()) {
+		int coregroup_id = cpu_to_coregroup_id(cpu);
+
+		cpumask_set_cpu(cpu, cpu_coregroup_mask(cpu));
+		for_each_cpu_and(i, cpu_online_mask, cpu_cpu_mask(cpu)) {
+			int fcpu = cpu_first_thread_sibling(i);
+
+			if (fcpu == first_thread)
+				set_cpus_related(cpu, i, cpu_coregroup_mask);
+			else if (coregroup_id == cpu_to_coregroup_id(i))
+				set_cpus_related(cpu, i, cpu_coregroup_mask);
+		}
+	}
+
 	if (pkg_id == -1) {
 		struct cpumask *(*mask)(int) = cpu_sibling_mask;
 
@@ -1394,9 +1443,12 @@ static void fixup_topology(void)
 #ifdef CONFIG_SCHED_SMT
 	if (has_big_cores) {
 		pr_info("Big cores detected but using small core scheduling\n");
-		powerpc_topology[0].mask = smallcore_smt_mask;
+		powerpc_topology[smt_idx].mask = smallcore_smt_mask;
 	}
 #endif
+
+	if (!has_coregroup_support())
+		powerpc_topology[mc_idx].mask = powerpc_topology[cache_idx].mask;
 }
 
 void __init smp_cpus_done(unsigned int max_cpus)
* Unmerged path arch/powerpc/mm/numa.c
