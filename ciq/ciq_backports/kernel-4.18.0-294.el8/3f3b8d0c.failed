iommu/arm-smmu: Remove .tlb_inv_range indirection

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit 3f3b8d0c9c1838271543df9e655032117a663f88
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/3f3b8d0c.failed

Fill in 'native' iommu_flush_ops callbacks for all the
arm_smmu_flush_ops variants, and clear up the remains of the previous
.tlb_inv_range abstraction.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 3f3b8d0c9c1838271543df9e655032117a663f88)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm-smmu.c
#	drivers/iommu/arm-smmu.h
diff --cc drivers/iommu/arm-smmu.c
index c913cdd695bd,78292e8e31a5..000000000000
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@@ -526,113 -311,141 +526,167 @@@ static void arm_smmu_tlb_inv_context_s2
  	arm_smmu_tlb_sync_global(smmu);
  }
  
++<<<<<<< HEAD
 +static void arm_smmu_tlb_inv_range_nosync(unsigned long iova, size_t size,
 +					  size_t granule, bool leaf, void *cookie)
++=======
+ static void arm_smmu_tlb_inv_range_s1(unsigned long iova, size_t size,
+ 				      size_t granule, void *cookie, bool leaf)
++>>>>>>> 3f3b8d0c9c18 (iommu/arm-smmu: Remove .tlb_inv_range indirection)
  {
  	struct arm_smmu_domain *smmu_domain = cookie;
 -	struct arm_smmu_device *smmu = smmu_domain->smmu;
  	struct arm_smmu_cfg *cfg = &smmu_domain->cfg;
 -	int reg, idx = cfg->cbndx;
 +	bool stage1 = cfg->cbar != CBAR_TYPE_S2_TRANS;
 +	void __iomem *reg = ARM_SMMU_CB(smmu_domain->smmu, cfg->cbndx);
  
 -	if (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
 +	if (smmu_domain->smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
  		wmb();
  
 -	reg = leaf ? ARM_SMMU_CB_S1_TLBIVAL : ARM_SMMU_CB_S1_TLBIVA;
 -
 -	if (cfg->fmt != ARM_SMMU_CTX_FMT_AARCH64) {
 -		iova = (iova >> 12) << 12;
 -		iova |= cfg->asid;
 -		do {
 -			arm_smmu_cb_write(smmu, idx, reg, iova);
 -			iova += granule;
 -		} while (size -= granule);
 +	if (stage1) {
 +		reg += leaf ? ARM_SMMU_CB_S1_TLBIVAL : ARM_SMMU_CB_S1_TLBIVA;
 +
 +		if (cfg->fmt != ARM_SMMU_CTX_FMT_AARCH64) {
 +			iova = (iova >> 12) << 12;
 +			iova |= cfg->asid;
 +			do {
 +				writel_relaxed(iova, reg);
 +				iova += granule;
 +			} while (size -= granule);
 +		} else {
 +			iova >>= 12;
 +			iova |= (u64)cfg->asid << 48;
 +			do {
 +				writeq_relaxed(iova, reg);
 +				iova += granule >> 12;
 +			} while (size -= granule);
 +		}
  	} else {
 +		reg += leaf ? ARM_SMMU_CB_S2_TLBIIPAS2L :
 +			      ARM_SMMU_CB_S2_TLBIIPAS2;
  		iova >>= 12;
 -		iova |= (u64)cfg->asid << 48;
  		do {
 -			arm_smmu_cb_writeq(smmu, idx, reg, iova);
 +			smmu_write_atomic_lq(iova, reg);
  			iova += granule >> 12;
  		} while (size -= granule);
  	}
  }
  
- /*
-  * On MMU-401 at least, the cost of firing off multiple TLBIVMIDs appears
-  * almost negligible, but the benefit of getting the first one in as far ahead
-  * of the sync as possible is significant, hence we don't just make this a
-  * no-op and set .tlb_sync to arm_smmu_tlb_inv_context_s2() as you might think.
-  */
- static void arm_smmu_tlb_inv_vmid_nosync(unsigned long iova, size_t size,
- 					 size_t granule, bool leaf, void *cookie)
++<<<<<<< HEAD
++=======
+ static void arm_smmu_tlb_inv_range_s2(unsigned long iova, size_t size,
+ 				      size_t granule, void *cookie, bool leaf)
  {
  	struct arm_smmu_domain *smmu_domain = cookie;
- 	void __iomem *base = ARM_SMMU_GR0(smmu_domain->smmu);
+ 	struct arm_smmu_device *smmu = smmu_domain->smmu;
+ 	int reg, idx = smmu_domain->cfg.cbndx;
  
- 	if (smmu_domain->smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
+ 	if (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
  		wmb();
  
- 	writel_relaxed(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
+ 	reg = leaf ? ARM_SMMU_CB_S2_TLBIIPAS2L : ARM_SMMU_CB_S2_TLBIIPAS2;
+ 	iova >>= 12;
+ 	do {
+ 		if (smmu_domain->cfg.fmt == ARM_SMMU_CTX_FMT_AARCH64)
+ 			arm_smmu_cb_writeq(smmu, idx, reg, iova);
+ 		else
+ 			arm_smmu_cb_write(smmu, idx, reg, iova);
+ 		iova += granule >> 12;
+ 	} while (size -= granule);
  }
  
- static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
- 				  size_t granule, void *cookie)
+ static void arm_smmu_tlb_inv_walk_s1(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
  {
- 	struct arm_smmu_domain *smmu_domain = cookie;
- 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 	arm_smmu_tlb_inv_range_s1(iova, size, granule, cookie, false);
+ 	arm_smmu_tlb_sync_context(cookie);
+ }
  
- 	ops->tlb_inv_range(iova, size, granule, false, cookie);
- 	ops->tlb_sync(cookie);
+ static void arm_smmu_tlb_inv_leaf_s1(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
+ {
+ 	arm_smmu_tlb_inv_range_s1(iova, size, granule, cookie, true);
+ 	arm_smmu_tlb_sync_context(cookie);
  }
  
- static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
- 				  size_t granule, void *cookie)
+ static void arm_smmu_tlb_add_page_s1(struct iommu_iotlb_gather *gather,
+ 				     unsigned long iova, size_t granule,
+ 				     void *cookie)
  {
- 	struct arm_smmu_domain *smmu_domain = cookie;
- 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
+ 	arm_smmu_tlb_inv_range_s1(iova, granule, granule, cookie, true);
+ }
+ 
+ static void arm_smmu_tlb_inv_walk_s2(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
+ {
+ 	arm_smmu_tlb_inv_range_s2(iova, size, granule, cookie, false);
+ 	arm_smmu_tlb_sync_context(cookie);
+ }
+ 
+ static void arm_smmu_tlb_inv_leaf_s2(unsigned long iova, size_t size,
+ 				     size_t granule, void *cookie)
+ {
+ 	arm_smmu_tlb_inv_range_s2(iova, size, granule, cookie, true);
+ 	arm_smmu_tlb_sync_context(cookie);
+ }
  
- 	ops->tlb_inv_range(iova, size, granule, true, cookie);
- 	ops->tlb_sync(cookie);
+ static void arm_smmu_tlb_add_page_s2(struct iommu_iotlb_gather *gather,
+ 				     unsigned long iova, size_t granule,
+ 				     void *cookie)
+ {
+ 	arm_smmu_tlb_inv_range_s2(iova, granule, granule, cookie, true);
  }
  
- static void arm_smmu_tlb_add_page(struct iommu_iotlb_gather *gather,
- 				  unsigned long iova, size_t granule,
- 				  void *cookie)
+ static void arm_smmu_tlb_inv_any_s2_v1(unsigned long iova, size_t size,
+ 				       size_t granule, void *cookie)
+ {
+ 	arm_smmu_tlb_inv_context_s2(cookie);
+ }
++>>>>>>> 3f3b8d0c9c18 (iommu/arm-smmu: Remove .tlb_inv_range indirection)
+ /*
+  * On MMU-401 at least, the cost of firing off multiple TLBIVMIDs appears
+  * almost negligible, but the benefit of getting the first one in as far ahead
+  * of the sync as possible is significant, hence we don't just make this a
+  * no-op and call arm_smmu_tlb_inv_context_s2() from .iotlb_sync as you might
+  * think.
+  */
+ static void arm_smmu_tlb_add_page_s2_v1(struct iommu_iotlb_gather *gather,
+ 					unsigned long iova, size_t granule,
+ 					void *cookie)
  {
  	struct arm_smmu_domain *smmu_domain = cookie;
- 	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
 -	struct arm_smmu_device *smmu = smmu_domain->smmu;
++	void __iomem *base = ARM_SMMU_GR0(smmu_domain->smmu);
  
- 	ops->tlb_inv_range(iova, granule, granule, true, cookie);
 -	if (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
++	if (smmu_domain->smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
+ 		wmb();
+ 
 -	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_TLBIVMID, smmu_domain->cfg.vmid);
++	writel_relaxed(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
  }
  
  static const struct arm_smmu_flush_ops arm_smmu_s1_tlb_ops = {
  	.tlb = {
  		.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
- 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
- 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
- 		.tlb_add_page	= arm_smmu_tlb_add_page,
+ 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk_s1,
+ 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf_s1,
+ 		.tlb_add_page	= arm_smmu_tlb_add_page_s1,
  	},
++<<<<<<< HEAD
 +	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
++=======
++>>>>>>> 3f3b8d0c9c18 (iommu/arm-smmu: Remove .tlb_inv_range indirection)
  	.tlb_sync		= arm_smmu_tlb_sync_context,
  };
  
  static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v2 = {
  	.tlb = {
  		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
- 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
- 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
- 		.tlb_add_page	= arm_smmu_tlb_add_page,
+ 		.tlb_flush_walk	= arm_smmu_tlb_inv_walk_s2,
+ 		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf_s2,
+ 		.tlb_add_page	= arm_smmu_tlb_add_page_s2,
  	},
++<<<<<<< HEAD
 +	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
++=======
++>>>>>>> 3f3b8d0c9c18 (iommu/arm-smmu: Remove .tlb_inv_range indirection)
  	.tlb_sync		= arm_smmu_tlb_sync_context,
  };
  
diff --cc drivers/iommu/arm-smmu.h
index 671b3a337fea,6edd35ca983c..000000000000
--- a/drivers/iommu/arm-smmu.h
+++ b/drivers/iommu/arm-smmu.h
@@@ -216,4 -203,198 +216,201 @@@ enum arm_smmu_cbar_type 
  #define ARM_SMMU_CB_ATSR		0x8f0
  #define ATSR_ACTIVE			BIT(0)
  
++<<<<<<< HEAD
++=======
+ 
+ /* Maximum number of context banks per SMMU */
+ #define ARM_SMMU_MAX_CBS		128
+ 
+ 
+ /* Shared driver definitions */
+ enum arm_smmu_arch_version {
+ 	ARM_SMMU_V1,
+ 	ARM_SMMU_V1_64K,
+ 	ARM_SMMU_V2,
+ };
+ 
+ enum arm_smmu_implementation {
+ 	GENERIC_SMMU,
+ 	ARM_MMU500,
+ 	CAVIUM_SMMUV2,
+ 	QCOM_SMMUV2,
+ };
+ 
+ struct arm_smmu_device {
+ 	struct device			*dev;
+ 
+ 	void __iomem			*base;
+ 	unsigned int			numpage;
+ 	unsigned int			pgshift;
+ 
+ #define ARM_SMMU_FEAT_COHERENT_WALK	(1 << 0)
+ #define ARM_SMMU_FEAT_STREAM_MATCH	(1 << 1)
+ #define ARM_SMMU_FEAT_TRANS_S1		(1 << 2)
+ #define ARM_SMMU_FEAT_TRANS_S2		(1 << 3)
+ #define ARM_SMMU_FEAT_TRANS_NESTED	(1 << 4)
+ #define ARM_SMMU_FEAT_TRANS_OPS		(1 << 5)
+ #define ARM_SMMU_FEAT_VMID16		(1 << 6)
+ #define ARM_SMMU_FEAT_FMT_AARCH64_4K	(1 << 7)
+ #define ARM_SMMU_FEAT_FMT_AARCH64_16K	(1 << 8)
+ #define ARM_SMMU_FEAT_FMT_AARCH64_64K	(1 << 9)
+ #define ARM_SMMU_FEAT_FMT_AARCH32_L	(1 << 10)
+ #define ARM_SMMU_FEAT_FMT_AARCH32_S	(1 << 11)
+ #define ARM_SMMU_FEAT_EXIDS		(1 << 12)
+ 	u32				features;
+ 
+ 	enum arm_smmu_arch_version	version;
+ 	enum arm_smmu_implementation	model;
+ 	const struct arm_smmu_impl	*impl;
+ 
+ 	u32				num_context_banks;
+ 	u32				num_s2_context_banks;
+ 	DECLARE_BITMAP(context_map, ARM_SMMU_MAX_CBS);
+ 	struct arm_smmu_cb		*cbs;
+ 	atomic_t			irptndx;
+ 
+ 	u32				num_mapping_groups;
+ 	u16				streamid_mask;
+ 	u16				smr_mask_mask;
+ 	struct arm_smmu_smr		*smrs;
+ 	struct arm_smmu_s2cr		*s2crs;
+ 	struct mutex			stream_map_mutex;
+ 
+ 	unsigned long			va_size;
+ 	unsigned long			ipa_size;
+ 	unsigned long			pa_size;
+ 	unsigned long			pgsize_bitmap;
+ 
+ 	u32				num_global_irqs;
+ 	u32				num_context_irqs;
+ 	unsigned int			*irqs;
+ 	struct clk_bulk_data		*clks;
+ 	int				num_clks;
+ 
+ 	spinlock_t			global_sync_lock;
+ 
+ 	/* IOMMU core code handle */
+ 	struct iommu_device		iommu;
+ };
+ 
+ enum arm_smmu_context_fmt {
+ 	ARM_SMMU_CTX_FMT_NONE,
+ 	ARM_SMMU_CTX_FMT_AARCH64,
+ 	ARM_SMMU_CTX_FMT_AARCH32_L,
+ 	ARM_SMMU_CTX_FMT_AARCH32_S,
+ };
+ 
+ struct arm_smmu_cfg {
+ 	u8				cbndx;
+ 	u8				irptndx;
+ 	union {
+ 		u16			asid;
+ 		u16			vmid;
+ 	};
+ 	enum arm_smmu_cbar_type		cbar;
+ 	enum arm_smmu_context_fmt	fmt;
+ };
+ #define INVALID_IRPTNDX			0xff
+ 
+ enum arm_smmu_domain_stage {
+ 	ARM_SMMU_DOMAIN_S1 = 0,
+ 	ARM_SMMU_DOMAIN_S2,
+ 	ARM_SMMU_DOMAIN_NESTED,
+ 	ARM_SMMU_DOMAIN_BYPASS,
+ };
+ 
+ struct arm_smmu_flush_ops {
+ 	struct iommu_flush_ops		tlb;
+ 	void (*tlb_sync)(void *cookie);
+ };
+ 
+ struct arm_smmu_domain {
+ 	struct arm_smmu_device		*smmu;
+ 	struct io_pgtable_ops		*pgtbl_ops;
+ 	const struct arm_smmu_flush_ops	*flush_ops;
+ 	struct arm_smmu_cfg		cfg;
+ 	enum arm_smmu_domain_stage	stage;
+ 	bool				non_strict;
+ 	struct mutex			init_mutex; /* Protects smmu pointer */
+ 	spinlock_t			cb_lock; /* Serialises ATS1* ops and TLB syncs */
+ 	struct iommu_domain		domain;
+ };
+ 
+ 
+ /* Implementation details, yay! */
+ struct arm_smmu_impl {
+ 	u32 (*read_reg)(struct arm_smmu_device *smmu, int page, int offset);
+ 	void (*write_reg)(struct arm_smmu_device *smmu, int page, int offset,
+ 			  u32 val);
+ 	u64 (*read_reg64)(struct arm_smmu_device *smmu, int page, int offset);
+ 	void (*write_reg64)(struct arm_smmu_device *smmu, int page, int offset,
+ 			    u64 val);
+ 	int (*cfg_probe)(struct arm_smmu_device *smmu);
+ 	int (*reset)(struct arm_smmu_device *smmu);
+ 	int (*init_context)(struct arm_smmu_domain *smmu_domain);
+ };
+ 
+ static inline void __iomem *arm_smmu_page(struct arm_smmu_device *smmu, int n)
+ {
+ 	return smmu->base + (n << smmu->pgshift);
+ }
+ 
+ static inline u32 arm_smmu_readl(struct arm_smmu_device *smmu, int page, int offset)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->read_reg))
+ 		return smmu->impl->read_reg(smmu, page, offset);
+ 	return readl_relaxed(arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ static inline void arm_smmu_writel(struct arm_smmu_device *smmu, int page,
+ 				   int offset, u32 val)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->write_reg))
+ 		smmu->impl->write_reg(smmu, page, offset, val);
+ 	else
+ 		writel_relaxed(val, arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ static inline u64 arm_smmu_readq(struct arm_smmu_device *smmu, int page, int offset)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->read_reg64))
+ 		return smmu->impl->read_reg64(smmu, page, offset);
+ 	return readq_relaxed(arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ static inline void arm_smmu_writeq(struct arm_smmu_device *smmu, int page,
+ 				   int offset, u64 val)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->write_reg64))
+ 		smmu->impl->write_reg64(smmu, page, offset, val);
+ 	else
+ 		writeq_relaxed(val, arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ #define ARM_SMMU_GR0		0
+ #define ARM_SMMU_GR1		1
+ #define ARM_SMMU_CB(s, n)	((s)->numpage + (n))
+ 
+ #define arm_smmu_gr0_read(s, o)		\
+ 	arm_smmu_readl((s), ARM_SMMU_GR0, (o))
+ #define arm_smmu_gr0_write(s, o, v)	\
+ 	arm_smmu_writel((s), ARM_SMMU_GR0, (o), (v))
+ 
+ #define arm_smmu_gr1_read(s, o)		\
+ 	arm_smmu_readl((s), ARM_SMMU_GR1, (o))
+ #define arm_smmu_gr1_write(s, o, v)	\
+ 	arm_smmu_writel((s), ARM_SMMU_GR1, (o), (v))
+ 
+ #define arm_smmu_cb_read(s, n, o)	\
+ 	arm_smmu_readl((s), ARM_SMMU_CB((s), (n)), (o))
+ #define arm_smmu_cb_write(s, n, o, v)	\
+ 	arm_smmu_writel((s), ARM_SMMU_CB((s), (n)), (o), (v))
+ #define arm_smmu_cb_readq(s, n, o)	\
+ 	arm_smmu_readq((s), ARM_SMMU_CB((s), (n)), (o))
+ #define arm_smmu_cb_writeq(s, n, o, v)	\
+ 	arm_smmu_writeq((s), ARM_SMMU_CB((s), (n)), (o), (v))
+ 
+ struct arm_smmu_device *arm_smmu_impl_init(struct arm_smmu_device *smmu);
+ 
++>>>>>>> 3f3b8d0c9c18 (iommu/arm-smmu: Remove .tlb_inv_range indirection)
  #endif /* _ARM_SMMU_H */
* Unmerged path drivers/iommu/arm-smmu.c
* Unmerged path drivers/iommu/arm-smmu.h
