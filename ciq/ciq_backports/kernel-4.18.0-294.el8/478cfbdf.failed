bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yonghong Song <yhs@fb.com>
commit 478cfbdf5f13dfe09cfd0b1cbac821f5e27f6108
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/478cfbdf.failed

Three more helpers are added to cast a sock_common pointer to
an tcp_sock, tcp_timewait_sock or a tcp_request_sock for
tracing programs.

	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Martin KaFai Lau <kafai@fb.com>
Link: https://lore.kernel.org/bpf/20200623230811.3988277-1-yhs@fb.com
(cherry picked from commit 478cfbdf5f13dfe09cfd0b1cbac821f5e27f6108)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	include/uapi/linux/bpf.h
#	kernel/trace/bpf_trace.c
#	net/core/filter.c
#	scripts/bpf_helpers_doc.py
#	tools/include/uapi/linux/bpf.h
diff --cc include/linux/bpf.h
index 531af1e44e56,c23998cf6699..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -1593,6 -1644,15 +1593,18 @@@ extern const struct bpf_func_proto bpf_
  extern const struct bpf_func_proto bpf_jiffies64_proto;
  extern const struct bpf_func_proto bpf_get_ns_current_pid_tgid_proto;
  extern const struct bpf_func_proto bpf_event_output_data_proto;
++<<<<<<< HEAD
++=======
+ extern const struct bpf_func_proto bpf_ringbuf_output_proto;
+ extern const struct bpf_func_proto bpf_ringbuf_reserve_proto;
+ extern const struct bpf_func_proto bpf_ringbuf_submit_proto;
+ extern const struct bpf_func_proto bpf_ringbuf_discard_proto;
+ extern const struct bpf_func_proto bpf_ringbuf_query_proto;
+ extern const struct bpf_func_proto bpf_skc_to_tcp6_sock_proto;
+ extern const struct bpf_func_proto bpf_skc_to_tcp_sock_proto;
+ extern const struct bpf_func_proto bpf_skc_to_tcp_timewait_sock_proto;
+ extern const struct bpf_func_proto bpf_skc_to_tcp_request_sock_proto;
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
  
  const struct bpf_func_proto *bpf_tracing_func_proto(
  	enum bpf_func_id func_id, const struct bpf_prog *prog);
diff --cc include/uapi/linux/bpf.h
index af3754a78dbd,b9412ab275f3..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -3127,6 -3138,147 +3127,150 @@@ union bpf_attr 
   * 		0 on success, or a negative error in case of failure:
   *
   *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
++<<<<<<< HEAD
++=======
+  *
+  * u64 bpf_sk_cgroup_id(struct bpf_sock *sk)
+  *	Description
+  *		Return the cgroup v2 id of the socket *sk*.
+  *
+  *		*sk* must be a non-**NULL** pointer to a full socket, e.g. one
+  *		returned from **bpf_sk_lookup_xxx**\ (),
+  *		**bpf_sk_fullsock**\ (), etc. The format of returned id is
+  *		same as in **bpf_skb_cgroup_id**\ ().
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		the **CONFIG_SOCK_CGROUP_DATA** configuration option.
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * u64 bpf_sk_ancestor_cgroup_id(struct bpf_sock *sk, int ancestor_level)
+  *	Description
+  *		Return id of cgroup v2 that is ancestor of cgroup associated
+  *		with the *sk* at the *ancestor_level*.  The root cgroup is at
+  *		*ancestor_level* zero and each step down the hierarchy
+  *		increments the level. If *ancestor_level* == level of cgroup
+  *		associated with *sk*, then return value will be same as that
+  *		of **bpf_sk_cgroup_id**\ ().
+  *
+  *		The helper is useful to implement policies based on cgroups
+  *		that are upper in hierarchy than immediate cgroup associated
+  *		with *sk*.
+  *
+  *		The format of returned id and helper limitations are same as in
+  *		**bpf_sk_cgroup_id**\ ().
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * int bpf_ringbuf_output(void *ringbuf, void *data, u64 size, u64 flags)
+  * 	Description
+  * 		Copy *size* bytes from *data* into a ring buffer *ringbuf*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		0, on success;
+  * 		< 0, on error.
+  *
+  * void *bpf_ringbuf_reserve(void *ringbuf, u64 size, u64 flags)
+  * 	Description
+  * 		Reserve *size* bytes of payload in a ring buffer *ringbuf*.
+  * 	Return
+  * 		Valid pointer with *size* bytes of memory available; NULL,
+  * 		otherwise.
+  *
+  * void bpf_ringbuf_submit(void *data, u64 flags)
+  * 	Description
+  * 		Submit reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * void bpf_ringbuf_discard(void *data, u64 flags)
+  * 	Description
+  * 		Discard reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * u64 bpf_ringbuf_query(void *ringbuf, u64 flags)
+  *	Description
+  *		Query various characteristics of provided ring buffer. What
+  *		exactly is queries is determined by *flags*:
+  *		  - BPF_RB_AVAIL_DATA - amount of data not yet consumed;
+  *		  - BPF_RB_RING_SIZE - the size of ring buffer;
+  *		  - BPF_RB_CONS_POS - consumer position (can wrap around);
+  *		  - BPF_RB_PROD_POS - producer(s) position (can wrap around);
+  *		Data returned is just a momentary snapshots of actual values
+  *		and could be inaccurate, so this facility should be used to
+  *		power heuristics and for reporting, not to make 100% correct
+  *		calculation.
+  *	Return
+  *		Requested value, or 0, if flags are not recognized.
+  *
+  * long bpf_csum_level(struct sk_buff *skb, u64 level)
+  * 	Description
+  * 		Change the skbs checksum level by one layer up or down, or
+  * 		reset it entirely to none in order to have the stack perform
+  * 		checksum validation. The level is applicable to the following
+  * 		protocols: TCP, UDP, GRE, SCTP, FCOE. For example, a decap of
+  * 		| ETH | IP | UDP | GUE | IP | TCP | into | ETH | IP | TCP |
+  * 		through **bpf_skb_adjust_room**\ () helper with passing in
+  * 		**BPF_F_ADJ_ROOM_NO_CSUM_RESET** flag would require one	call
+  * 		to **bpf_csum_level**\ () with **BPF_CSUM_LEVEL_DEC** since
+  * 		the UDP header is removed. Similarly, an encap of the latter
+  * 		into the former could be accompanied by a helper call to
+  * 		**bpf_csum_level**\ () with **BPF_CSUM_LEVEL_INC** if the
+  * 		skb is still intended to be processed in higher layers of the
+  * 		stack instead of just egressing at tc.
+  *
+  * 		There are three supported level settings at this time:
+  *
+  * 		* **BPF_CSUM_LEVEL_INC**: Increases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_DEC**: Decreases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_RESET**: Resets skb->csum_level to 0 and
+  * 		  sets CHECKSUM_NONE to force checksum validation by the stack.
+  * 		* **BPF_CSUM_LEVEL_QUERY**: No-op, returns the current
+  * 		  skb->csum_level.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure. In the
+  * 		case of **BPF_CSUM_LEVEL_QUERY**, the current skb->csum_level
+  * 		is returned or the error code -EACCES in case the skb is not
+  * 		subject to CHECKSUM_UNNECESSARY.
+  *
+  * struct tcp6_sock *bpf_skc_to_tcp6_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_sock *bpf_skc_to_tcp_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_timewait_sock *bpf_skc_to_tcp_timewait_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_timewait_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_request_sock *bpf_skc_to_tcp_request_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_request_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3256,7 -3408,19 +3400,23 @@@
  	FN(sk_assign),			\
  	FN(ktime_get_boot_ns),		\
  	FN(seq_printf),			\
++<<<<<<< HEAD
 +	FN(seq_write),
++=======
+ 	FN(seq_write),			\
+ 	FN(sk_cgroup_id),		\
+ 	FN(sk_ancestor_cgroup_id),	\
+ 	FN(ringbuf_output),		\
+ 	FN(ringbuf_reserve),		\
+ 	FN(ringbuf_submit),		\
+ 	FN(ringbuf_discard),		\
+ 	FN(ringbuf_query),		\
+ 	FN(csum_level),			\
+ 	FN(skc_to_tcp6_sock),		\
+ 	FN(skc_to_tcp_sock),		\
+ 	FN(skc_to_tcp_timewait_sock),	\
+ 	FN(skc_to_tcp_request_sock),
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --cc kernel/trace/bpf_trace.c
index 29a70cbf26d7,48d935b0d87c..000000000000
--- a/kernel/trace/bpf_trace.c
+++ b/kernel/trace/bpf_trace.c
@@@ -1459,6 -1515,14 +1459,17 @@@ tracing_prog_func_proto(enum bpf_func_i
  		return &bpf_skb_output_proto;
  	case BPF_FUNC_xdp_output:
  		return &bpf_xdp_output_proto;
++<<<<<<< HEAD
++=======
+ 	case BPF_FUNC_skc_to_tcp6_sock:
+ 		return &bpf_skc_to_tcp6_sock_proto;
+ 	case BPF_FUNC_skc_to_tcp_sock:
+ 		return &bpf_skc_to_tcp_sock_proto;
+ 	case BPF_FUNC_skc_to_tcp_timewait_sock:
+ 		return &bpf_skc_to_tcp_timewait_sock_proto;
+ 	case BPF_FUNC_skc_to_tcp_request_sock:
+ 		return &bpf_skc_to_tcp_request_sock_proto;
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
  #endif
  	case BPF_FUNC_seq_printf:
  		return prog->expected_attach_type == BPF_TRACE_ITER ?
diff --cc net/core/filter.c
index 76aa4a2037db,0b4e5aed7e20..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -76,9 -74,8 +76,10 @@@
  #include <net/lwtunnel.h>
  #include <net/ipv6_stubs.h>
  #include <net/bpf_sk_storage.h>
+ #include <net/transp_v6.h>
  
 +#include <linux/rh_features.h>
 +
  /**
   *	sk_filter_trim_cap - run a packet through a socket filter
   *	@sk: sock associated with &sk_buff
@@@ -9114,3 -9227,145 +9115,148 @@@ void bpf_prog_change_xdp(struct bpf_pro
  {
  	bpf_dispatcher_change_prog(BPF_DISPATCHER_PTR(xdp), prev_prog, prog);
  }
++<<<<<<< HEAD
++=======
+ 
+ /* Define a list of socket types which can be the argument for
+  * skc_to_*_sock() helpers. All these sockets should have
+  * sock_common as the first argument in its memory layout.
+  */
+ #define BTF_SOCK_TYPE_xxx \
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_INET, "inet_sock")			\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_INET_CONN, "inet_connection_sock")	\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_INET_REQ, "inet_request_sock")	\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_INET_TW, "inet_timewait_sock")	\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_REQ, "request_sock")		\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_SOCK, "sock")			\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_SOCK_COMMON, "sock_common")		\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_TCP, "tcp_sock")			\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_TCP_REQ, "tcp_request_sock")	\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_TCP_TW, "tcp_timewait_sock")	\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_TCP6, "tcp6_sock")			\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_UDP, "udp_sock")			\
+ 	BTF_SOCK_TYPE(BTF_SOCK_TYPE_UDP6, "udp6_sock")
+ 
+ enum {
+ #define BTF_SOCK_TYPE(name, str) name,
+ BTF_SOCK_TYPE_xxx
+ #undef BTF_SOCK_TYPE
+ MAX_BTF_SOCK_TYPE,
+ };
+ 
+ static int btf_sock_ids[MAX_BTF_SOCK_TYPE];
+ 
+ #ifdef CONFIG_BPF_SYSCALL
+ static const char *bpf_sock_types[] = {
+ #define BTF_SOCK_TYPE(name, str) str,
+ BTF_SOCK_TYPE_xxx
+ #undef BTF_SOCK_TYPE
+ };
+ 
+ void init_btf_sock_ids(struct btf *btf)
+ {
+ 	int i, btf_id;
+ 
+ 	for (i = 0; i < MAX_BTF_SOCK_TYPE; i++) {
+ 		btf_id = btf_find_by_name_kind(btf, bpf_sock_types[i],
+ 					       BTF_KIND_STRUCT);
+ 		if (btf_id > 0)
+ 			btf_sock_ids[i] = btf_id;
+ 	}
+ }
+ #endif
+ 
+ static bool check_arg_btf_id(u32 btf_id, u32 arg)
+ {
+ 	int i;
+ 
+ 	/* only one argument, no need to check arg */
+ 	for (i = 0; i < MAX_BTF_SOCK_TYPE; i++)
+ 		if (btf_sock_ids[i] == btf_id)
+ 			return true;
+ 	return false;
+ }
+ 
+ BPF_CALL_1(bpf_skc_to_tcp6_sock, struct sock *, sk)
+ {
+ 	/* tcp6_sock type is not generated in dwarf and hence btf,
+ 	 * trigger an explicit type generation here.
+ 	 */
+ 	BTF_TYPE_EMIT(struct tcp6_sock);
+ 	if (sk_fullsock(sk) && sk->sk_protocol == IPPROTO_TCP &&
+ 	    sk->sk_family == AF_INET6)
+ 		return (unsigned long)sk;
+ 
+ 	return (unsigned long)NULL;
+ }
+ 
+ const struct bpf_func_proto bpf_skc_to_tcp6_sock_proto = {
+ 	.func			= bpf_skc_to_tcp6_sock,
+ 	.gpl_only		= false,
+ 	.ret_type		= RET_PTR_TO_BTF_ID_OR_NULL,
+ 	.arg1_type		= ARG_PTR_TO_BTF_ID,
+ 	.check_btf_id		= check_arg_btf_id,
+ 	.ret_btf_id		= &btf_sock_ids[BTF_SOCK_TYPE_TCP6],
+ };
+ 
+ BPF_CALL_1(bpf_skc_to_tcp_sock, struct sock *, sk)
+ {
+ 	if (sk_fullsock(sk) && sk->sk_protocol == IPPROTO_TCP)
+ 		return (unsigned long)sk;
+ 
+ 	return (unsigned long)NULL;
+ }
+ 
+ const struct bpf_func_proto bpf_skc_to_tcp_sock_proto = {
+ 	.func			= bpf_skc_to_tcp_sock,
+ 	.gpl_only		= false,
+ 	.ret_type		= RET_PTR_TO_BTF_ID_OR_NULL,
+ 	.arg1_type		= ARG_PTR_TO_BTF_ID,
+ 	.check_btf_id		= check_arg_btf_id,
+ 	.ret_btf_id		= &btf_sock_ids[BTF_SOCK_TYPE_TCP],
+ };
+ 
+ BPF_CALL_1(bpf_skc_to_tcp_timewait_sock, struct sock *, sk)
+ {
+ 	if (sk->sk_prot == &tcp_prot && sk->sk_state == TCP_TIME_WAIT)
+ 		return (unsigned long)sk;
+ 
+ #if IS_BUILTIN(CONFIG_IPV6)
+ 	if (sk->sk_prot == &tcpv6_prot && sk->sk_state == TCP_TIME_WAIT)
+ 		return (unsigned long)sk;
+ #endif
+ 
+ 	return (unsigned long)NULL;
+ }
+ 
+ const struct bpf_func_proto bpf_skc_to_tcp_timewait_sock_proto = {
+ 	.func			= bpf_skc_to_tcp_timewait_sock,
+ 	.gpl_only		= false,
+ 	.ret_type		= RET_PTR_TO_BTF_ID_OR_NULL,
+ 	.arg1_type		= ARG_PTR_TO_BTF_ID,
+ 	.check_btf_id		= check_arg_btf_id,
+ 	.ret_btf_id		= &btf_sock_ids[BTF_SOCK_TYPE_TCP_TW],
+ };
+ 
+ BPF_CALL_1(bpf_skc_to_tcp_request_sock, struct sock *, sk)
+ {
+ 	if (sk->sk_prot == &tcp_prot  && sk->sk_state == TCP_NEW_SYN_RECV)
+ 		return (unsigned long)sk;
+ 
+ #if IS_BUILTIN(CONFIG_IPV6)
+ 	if (sk->sk_prot == &tcpv6_prot && sk->sk_state == TCP_NEW_SYN_RECV)
+ 		return (unsigned long)sk;
+ #endif
+ 
+ 	return (unsigned long)NULL;
+ }
+ 
+ const struct bpf_func_proto bpf_skc_to_tcp_request_sock_proto = {
+ 	.func			= bpf_skc_to_tcp_request_sock,
+ 	.gpl_only		= false,
+ 	.ret_type		= RET_PTR_TO_BTF_ID_OR_NULL,
+ 	.arg1_type		= ARG_PTR_TO_BTF_ID,
+ 	.check_btf_id		= check_arg_btf_id,
+ 	.ret_btf_id		= &btf_sock_ids[BTF_SOCK_TYPE_TCP_REQ],
+ };
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
diff --cc scripts/bpf_helpers_doc.py
index 91fa668fa860,d886657c6aaa..000000000000
--- a/scripts/bpf_helpers_doc.py
+++ b/scripts/bpf_helpers_doc.py
@@@ -421,6 -421,10 +421,13 @@@ class PrinterHelpers(Printer)
              'struct sockaddr',
              'struct tcphdr',
              'struct seq_file',
++<<<<<<< HEAD
++=======
+             'struct tcp6_sock',
+             'struct tcp_sock',
+             'struct tcp_timewait_sock',
+             'struct tcp_request_sock',
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
  
              'struct __sk_buff',
              'struct sk_msg_md',
@@@ -458,6 -462,10 +465,13 @@@
              'struct sockaddr',
              'struct tcphdr',
              'struct seq_file',
++<<<<<<< HEAD
++=======
+             'struct tcp6_sock',
+             'struct tcp_sock',
+             'struct tcp_timewait_sock',
+             'struct tcp_request_sock',
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
      }
      mapped_types = {
              'u8': '__u8',
diff --cc tools/include/uapi/linux/bpf.h
index b34454b6dad0,b9412ab275f3..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -3081,25 -3116,169 +3081,170 @@@ union bpf_attr 
   *		valid address but requiring a major memory fault. If reading kernel memory
   *		fails, the string for **%s** will be an empty string, and the ip
   *		address for **%p{i,I}{4,6}** will be 0. Not returning error to
 - *		bpf program is consistent with what **bpf_trace_printk**\ () does for now.
 + *		bpf program is consistent with what bpf_trace_printk() does for now.
   * 	Return
 - * 		0 on success, or a negative error in case of failure:
 - *
 - *		**-EBUSY** if per-CPU memory copy buffer is busy, can try again
 - *		by returning 1 from bpf program.
 - *
 - *		**-EINVAL** if arguments are invalid, or if *fmt* is invalid/unsupported.
 + * 		0 on success, or a negative errno in case of failure.
   *
 - *		**-E2BIG** if *fmt* contains too many format specifiers.
 + *		* **-EBUSY**		Percpu memory copy buffer is busy, can try again
 + *					by returning 1 from bpf program.
 + *		* **-EINVAL**		Invalid arguments, or invalid/unsupported formats.
 + *		* **-E2BIG**		Too many format specifiers.
 + *		* **-EOVERFLOW**	Overflow happens, the same object will be tried again.
   *
 - *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
 - *
 - * long bpf_seq_write(struct seq_file *m, const void *data, u32 len)
 + * int bpf_seq_write(struct seq_file *m, const void *data, u32 len)
   * 	Description
 - * 		**bpf_seq_write**\ () uses seq_file **seq_write**\ () to write the data.
 + * 		seq_write uses seq_file seq_write() to write the data.
   * 		The *m* represents the seq_file. The *data* and *len* represent the
 - * 		data to write in bytes.
 + *		data to write in bytes.
   * 	Return
 - * 		0 on success, or a negative error in case of failure:
 + * 		0 on success, or a negative errno in case of failure.
   *
++<<<<<<< HEAD
 + *		* **-EOVERFLOW**	Overflow happens, the same object will be tried again.
++=======
+  *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
+  *
+  * u64 bpf_sk_cgroup_id(struct bpf_sock *sk)
+  *	Description
+  *		Return the cgroup v2 id of the socket *sk*.
+  *
+  *		*sk* must be a non-**NULL** pointer to a full socket, e.g. one
+  *		returned from **bpf_sk_lookup_xxx**\ (),
+  *		**bpf_sk_fullsock**\ (), etc. The format of returned id is
+  *		same as in **bpf_skb_cgroup_id**\ ().
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		the **CONFIG_SOCK_CGROUP_DATA** configuration option.
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * u64 bpf_sk_ancestor_cgroup_id(struct bpf_sock *sk, int ancestor_level)
+  *	Description
+  *		Return id of cgroup v2 that is ancestor of cgroup associated
+  *		with the *sk* at the *ancestor_level*.  The root cgroup is at
+  *		*ancestor_level* zero and each step down the hierarchy
+  *		increments the level. If *ancestor_level* == level of cgroup
+  *		associated with *sk*, then return value will be same as that
+  *		of **bpf_sk_cgroup_id**\ ().
+  *
+  *		The helper is useful to implement policies based on cgroups
+  *		that are upper in hierarchy than immediate cgroup associated
+  *		with *sk*.
+  *
+  *		The format of returned id and helper limitations are same as in
+  *		**bpf_sk_cgroup_id**\ ().
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * int bpf_ringbuf_output(void *ringbuf, void *data, u64 size, u64 flags)
+  * 	Description
+  * 		Copy *size* bytes from *data* into a ring buffer *ringbuf*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		0, on success;
+  * 		< 0, on error.
+  *
+  * void *bpf_ringbuf_reserve(void *ringbuf, u64 size, u64 flags)
+  * 	Description
+  * 		Reserve *size* bytes of payload in a ring buffer *ringbuf*.
+  * 	Return
+  * 		Valid pointer with *size* bytes of memory available; NULL,
+  * 		otherwise.
+  *
+  * void bpf_ringbuf_submit(void *data, u64 flags)
+  * 	Description
+  * 		Submit reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * void bpf_ringbuf_discard(void *data, u64 flags)
+  * 	Description
+  * 		Discard reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * u64 bpf_ringbuf_query(void *ringbuf, u64 flags)
+  *	Description
+  *		Query various characteristics of provided ring buffer. What
+  *		exactly is queries is determined by *flags*:
+  *		  - BPF_RB_AVAIL_DATA - amount of data not yet consumed;
+  *		  - BPF_RB_RING_SIZE - the size of ring buffer;
+  *		  - BPF_RB_CONS_POS - consumer position (can wrap around);
+  *		  - BPF_RB_PROD_POS - producer(s) position (can wrap around);
+  *		Data returned is just a momentary snapshots of actual values
+  *		and could be inaccurate, so this facility should be used to
+  *		power heuristics and for reporting, not to make 100% correct
+  *		calculation.
+  *	Return
+  *		Requested value, or 0, if flags are not recognized.
+  *
+  * long bpf_csum_level(struct sk_buff *skb, u64 level)
+  * 	Description
+  * 		Change the skbs checksum level by one layer up or down, or
+  * 		reset it entirely to none in order to have the stack perform
+  * 		checksum validation. The level is applicable to the following
+  * 		protocols: TCP, UDP, GRE, SCTP, FCOE. For example, a decap of
+  * 		| ETH | IP | UDP | GUE | IP | TCP | into | ETH | IP | TCP |
+  * 		through **bpf_skb_adjust_room**\ () helper with passing in
+  * 		**BPF_F_ADJ_ROOM_NO_CSUM_RESET** flag would require one	call
+  * 		to **bpf_csum_level**\ () with **BPF_CSUM_LEVEL_DEC** since
+  * 		the UDP header is removed. Similarly, an encap of the latter
+  * 		into the former could be accompanied by a helper call to
+  * 		**bpf_csum_level**\ () with **BPF_CSUM_LEVEL_INC** if the
+  * 		skb is still intended to be processed in higher layers of the
+  * 		stack instead of just egressing at tc.
+  *
+  * 		There are three supported level settings at this time:
+  *
+  * 		* **BPF_CSUM_LEVEL_INC**: Increases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_DEC**: Decreases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_RESET**: Resets skb->csum_level to 0 and
+  * 		  sets CHECKSUM_NONE to force checksum validation by the stack.
+  * 		* **BPF_CSUM_LEVEL_QUERY**: No-op, returns the current
+  * 		  skb->csum_level.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure. In the
+  * 		case of **BPF_CSUM_LEVEL_QUERY**, the current skb->csum_level
+  * 		is returned or the error code -EACCES in case the skb is not
+  * 		subject to CHECKSUM_UNNECESSARY.
+  *
+  * struct tcp6_sock *bpf_skc_to_tcp6_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_sock *bpf_skc_to_tcp_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_timewait_sock *bpf_skc_to_tcp_timewait_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_timewait_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_request_sock *bpf_skc_to_tcp_request_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_request_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3229,7 -3408,19 +3374,23 @@@
  	FN(sk_assign),			\
  	FN(ktime_get_boot_ns),		\
  	FN(seq_printf),			\
++<<<<<<< HEAD
 +	FN(seq_write),
++=======
+ 	FN(seq_write),			\
+ 	FN(sk_cgroup_id),		\
+ 	FN(sk_ancestor_cgroup_id),	\
+ 	FN(ringbuf_output),		\
+ 	FN(ringbuf_reserve),		\
+ 	FN(ringbuf_submit),		\
+ 	FN(ringbuf_discard),		\
+ 	FN(ringbuf_query),		\
+ 	FN(csum_level),			\
+ 	FN(skc_to_tcp6_sock),		\
+ 	FN(skc_to_tcp_sock),		\
+ 	FN(skc_to_tcp_timewait_sock),	\
+ 	FN(skc_to_tcp_request_sock),
++>>>>>>> 478cfbdf5f13 (bpf: Add bpf_skc_to_{tcp, tcp_timewait, tcp_request}_sock() helpers)
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
* Unmerged path include/linux/bpf.h
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path kernel/trace/bpf_trace.c
* Unmerged path net/core/filter.c
* Unmerged path scripts/bpf_helpers_doc.py
* Unmerged path tools/include/uapi/linux/bpf.h
