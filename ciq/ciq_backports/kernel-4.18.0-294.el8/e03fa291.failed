KVM: arm64: Duplicate hyp/tlb.c for VHE/nVHE

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author David Brazdil <dbrazdil@google.com>
commit e03fa29164ec1db1a81dc0168d0017a9e0366c7c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/e03fa291.failed

tlb.c contains code for flushing the TLB, with code shared between VHE/nVHE.
Because common code is small, duplicate tlb.c and specialize each copy for
VHE/nVHE.

	Signed-off-by: David Brazdil <dbrazdil@google.com>
	Signed-off-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20200625131420.71444-9-dbrazdil@google.com
(cherry picked from commit e03fa29164ec1db1a81dc0168d0017a9e0366c7c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/image-vars.h
#	arch/arm64/kvm/hyp/Makefile
#	arch/arm64/kvm/hyp/nvhe/Makefile
#	arch/arm64/kvm/hyp/vhe/Makefile
#	arch/arm64/kvm/tlb.c
diff --cc arch/arm64/kernel/image-vars.h
index 25a2a9b479c2,f029f3ea7ffe..000000000000
--- a/arch/arm64/kernel/image-vars.h
+++ b/arch/arm64/kernel/image-vars.h
@@@ -48,4 -50,74 +48,76 @@@ __efistub_screen_info		= screen_info
  
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_KVM
+ 
+ /*
+  * KVM nVHE code has its own symbol namespace prefixed with __kvm_nvhe_, to
+  * separate it from the kernel proper. The following symbols are legally
+  * accessed by it, therefore provide aliases to make them linkable.
+  * Do not include symbols which may not be safely accessed under hypervisor
+  * memory mappings.
+  */
+ 
+ #define KVM_NVHE_ALIAS(sym) __kvm_nvhe_##sym = sym;
+ 
+ /* Symbols defined in debug-sr.c (not yet compiled with nVHE build rules). */
+ KVM_NVHE_ALIAS(__kvm_get_mdcr_el2);
+ 
+ /* Symbols defined in entry.S (not yet compiled with nVHE build rules). */
+ KVM_NVHE_ALIAS(__guest_exit);
+ KVM_NVHE_ALIAS(abort_guest_exit_end);
+ KVM_NVHE_ALIAS(abort_guest_exit_start);
+ 
+ /* Symbols defined in switch.c (not yet compiled with nVHE build rules). */
+ KVM_NVHE_ALIAS(__kvm_vcpu_run_nvhe);
+ KVM_NVHE_ALIAS(hyp_panic);
+ 
+ /* Symbols defined in sysreg-sr.c (not yet compiled with nVHE build rules). */
+ KVM_NVHE_ALIAS(__kvm_enable_ssbs);
+ 
+ /* Symbols defined in timer-sr.c (not yet compiled with nVHE build rules). */
+ KVM_NVHE_ALIAS(__kvm_timer_set_cntvoff);
+ 
+ /* Symbols defined in vgic-v3-sr.c (not yet compiled with nVHE build rules). */
+ KVM_NVHE_ALIAS(__vgic_v3_get_ich_vtr_el2);
+ KVM_NVHE_ALIAS(__vgic_v3_init_lrs);
+ KVM_NVHE_ALIAS(__vgic_v3_read_vmcr);
+ KVM_NVHE_ALIAS(__vgic_v3_restore_aprs);
+ KVM_NVHE_ALIAS(__vgic_v3_save_aprs);
+ KVM_NVHE_ALIAS(__vgic_v3_write_vmcr);
+ 
+ /* Alternative callbacks for init-time patching of nVHE hyp code. */
+ KVM_NVHE_ALIAS(arm64_enable_wa2_handling);
+ KVM_NVHE_ALIAS(kvm_patch_vector_branch);
+ KVM_NVHE_ALIAS(kvm_update_va_mask);
+ 
+ /* Global kernel state accessed by nVHE hyp code. */
+ KVM_NVHE_ALIAS(arm64_ssbd_callback_required);
+ KVM_NVHE_ALIAS(kvm_host_data);
+ 
+ /* Kernel constant needed to compute idmap addresses. */
+ KVM_NVHE_ALIAS(kimage_voffset);
+ 
+ /* Kernel symbols used to call panic() from nVHE hyp code (via ERET). */
+ KVM_NVHE_ALIAS(panic);
+ 
+ /* Vectors installed by hyp-init on reset HVC. */
+ KVM_NVHE_ALIAS(__hyp_stub_vectors);
+ 
+ /* IDMAP TCR_EL1.T0SZ as computed by the EL1 init code */
+ KVM_NVHE_ALIAS(idmap_t0sz);
+ 
+ /* Kernel symbol used by icache_is_vpipt(). */
+ KVM_NVHE_ALIAS(__icache_flags);
+ 
+ /* Kernel symbols needed for cpus_have_final/const_caps checks. */
+ KVM_NVHE_ALIAS(arm64_const_caps_ready);
+ KVM_NVHE_ALIAS(cpu_hwcap_keys);
+ KVM_NVHE_ALIAS(cpu_hwcaps);
+ 
+ #endif /* CONFIG_KVM */
+ 
++>>>>>>> e03fa29164ec (KVM: arm64: Duplicate hyp/tlb.c for VHE/nVHE)
  #endif /* __ARM64_KERNEL_IMAGE_VARS_H */
diff --cc arch/arm64/kvm/hyp/Makefile
index 9e1beab1b440,87d3cce2b26e..000000000000
--- a/arch/arm64/kvm/hyp/Makefile
+++ b/arch/arm64/kvm/hyp/Makefile
@@@ -3,12 -3,18 +3,16 @@@
  # Makefile for Kernel-based Virtual Machine module, HYP part
  #
  
 -incdir := $(srctree)/$(src)/include
 -subdir-asflags-y := -I$(incdir)
 -subdir-ccflags-y := -I$(incdir)				\
 -		    -fno-stack-protector		\
 -		    -DDISABLE_BRANCH_PROFILING		\
 -		    $(DISABLE_STACKLEAK_PLUGIN)
 +ccflags-y += -fno-stack-protector -DDISABLE_BRANCH_PROFILING
  
 -obj-$(CONFIG_KVM) += hyp.o vhe/ nvhe/
 -obj-$(CONFIG_KVM_INDIRECT_VECTORS) += smccc_wa.o
 +obj-$(CONFIG_KVM) += hyp.o
  
  hyp-y := vgic-v3-sr.o timer-sr.o aarch32.o vgic-v2-cpuif-proxy.o sysreg-sr.o \
++<<<<<<< HEAD
 +	 debug-sr.o entry.o switch.o fpsimd.o tlb.o hyp-entry.o
++=======
+ 	 debug-sr.o entry.o switch.o fpsimd.o
++>>>>>>> e03fa29164ec (KVM: arm64: Duplicate hyp/tlb.c for VHE/nVHE)
  
  # KVM code is run at a different exception code with a different map, so
  # compiler instrumentation that inserts callbacks or checks into the code may
diff --cc arch/arm64/kvm/tlb.c
index d82b0d7000d0,deb48c8c00ee..000000000000
--- a/arch/arm64/kvm/tlb.c
+++ b/arch/arm64/kvm/tlb.c
@@@ -1,80 -1,21 +1,81 @@@
  /*
   * Copyright (C) 2015 - ARM Ltd
   * Author: Marc Zyngier <marc.zyngier@arm.com>
 + *
 + * This program is free software; you can redistribute it and/or modify
 + * it under the terms of the GNU General Public License version 2 as
 + * published by the Free Software Foundation.
 + *
 + * This program is distributed in the hope that it will be useful,
 + * but WITHOUT ANY WARRANTY; without even the implied warranty of
 + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 + * GNU General Public License for more details.
 + *
 + * You should have received a copy of the GNU General Public License
 + * along with this program.  If not, see <http://www.gnu.org/licenses/>.
   */
  
- #include <linux/irqflags.h>
- 
  #include <asm/kvm_hyp.h>
  #include <asm/kvm_mmu.h>
  #include <asm/tlbflush.h>
  
  struct tlb_inv_context {
- 	unsigned long	flags;
  	u64		tcr;
- 	u64		sctlr;
  };
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/tlb.c
 +static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm *kvm,
 +						 struct tlb_inv_context *cxt)
 +{
 +	u64 val;
 +
 +	local_irq_save(cxt->flags);
 +
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 +		/*
 +		 * For CPUs that are affected by ARM errata 1165522 or 1530923,
 +		 * we cannot trust stage-1 to be in a correct state at that
 +		 * point. Since we do not want to force a full load of the
 +		 * vcpu state, we prevent the EL1 page-table walker to
 +		 * allocate new TLBs. This is done by setting the EPD bits
 +		 * in the TCR_EL1 register. We also need to prevent it to
 +		 * allocate IPA->PA walks, so we enable the S1 MMU...
 +		 */
 +		val = cxt->tcr = read_sysreg_el1(SYS_TCR);
 +		val |= TCR_EPD1_MASK | TCR_EPD0_MASK;
 +		write_sysreg_el1(val, SYS_TCR);
 +		val = cxt->sctlr = read_sysreg_el1(SYS_SCTLR);
 +		val |= SCTLR_ELx_M;
 +		write_sysreg_el1(val, SYS_SCTLR);
 +	}
 +
 +	/*
 +	 * With VHE enabled, we have HCR_EL2.{E2H,TGE} = {1,1}, and
 +	 * most TLB operations target EL2/EL0. In order to affect the
 +	 * guest TLBs (EL1/EL0), we need to change one of these two
 +	 * bits. Changing E2H is impossible (goodbye TTBR1_EL2), so
 +	 * let's flip TGE before executing the TLB operation.
 +	 *
 +	 * ARM erratum 1165522 requires some special handling (again),
 +	 * as we need to make sure both stages of translation are in
 +	 * place before clearing TGE. __load_guest_stage2() already
 +	 * has an ISB in order to deal with this.
 +	 */
 +	__load_guest_stage2(kvm);
 +	val = read_sysreg(hcr_el2);
 +	val &= ~HCR_TGE;
 +	write_sysreg(val, hcr_el2);
 +	isb();
 +}
 +
 +static void __hyp_text __tlb_switch_to_guest_nvhe(struct kvm *kvm,
 +						  struct tlb_inv_context *cxt)
++=======
+ static void __hyp_text __tlb_switch_to_guest(struct kvm *kvm,
+ 					     struct tlb_inv_context *cxt)
++>>>>>>> e03fa29164ec (KVM: arm64: Duplicate hyp/tlb.c for VHE/nVHE):arch/arm64/kvm/hyp/nvhe/tlb.c
  {
 -	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
  		u64 val;
  
  		/*
@@@ -95,37 -36,8 +96,42 @@@
  	asm(ALTERNATIVE("isb", "nop", ARM64_WORKAROUND_SPECULATIVE_AT));
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/tlb.c
 +static void __hyp_text __tlb_switch_to_guest(struct kvm *kvm,
 +					     struct tlb_inv_context *cxt)
 +{
 +	if (has_vhe())
 +		__tlb_switch_to_guest_vhe(kvm, cxt);
 +	else
 +		__tlb_switch_to_guest_nvhe(kvm, cxt);
 +}
 +
 +static void __hyp_text __tlb_switch_to_host_vhe(struct kvm *kvm,
 +						struct tlb_inv_context *cxt)
 +{
 +	/*
 +	 * We're done with the TLB operation, let's restore the host's
 +	 * view of HCR_EL2.
 +	 */
 +	write_sysreg(0, vttbr_el2);
 +	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 +	isb();
 +
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 +		/* Restore the registers to what they were */
 +		write_sysreg_el1(cxt->tcr, SYS_TCR);
 +		write_sysreg_el1(cxt->sctlr, SYS_SCTLR);
 +	}
 +
 +	local_irq_restore(cxt->flags);
 +}
 +
 +static void __hyp_text __tlb_switch_to_host_nvhe(struct kvm *kvm,
 +						 struct tlb_inv_context *cxt)
++=======
+ static void __hyp_text __tlb_switch_to_host(struct kvm *kvm,
+ 					    struct tlb_inv_context *cxt)
++>>>>>>> e03fa29164ec (KVM: arm64: Duplicate hyp/tlb.c for VHE/nVHE):arch/arm64/kvm/hyp/nvhe/tlb.c
  {
  	write_sysreg(0, vttbr_el2);
  
* Unmerged path arch/arm64/kvm/hyp/nvhe/Makefile
* Unmerged path arch/arm64/kvm/hyp/vhe/Makefile
* Unmerged path arch/arm64/kernel/image-vars.h
* Unmerged path arch/arm64/kvm/hyp/Makefile
* Unmerged path arch/arm64/kvm/hyp/nvhe/Makefile
diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c
deleted file mode 100644
index d82b0d7000d0..000000000000
--- a/arch/arm64/kvm/hyp/tlb.c
+++ /dev/null
@@ -1,253 +0,0 @@
-/*
- * Copyright (C) 2015 - ARM Ltd
- * Author: Marc Zyngier <marc.zyngier@arm.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
- */
-
-#include <linux/irqflags.h>
-
-#include <asm/kvm_hyp.h>
-#include <asm/kvm_mmu.h>
-#include <asm/tlbflush.h>
-
-struct tlb_inv_context {
-	unsigned long	flags;
-	u64		tcr;
-	u64		sctlr;
-};
-
-static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm *kvm,
-						 struct tlb_inv_context *cxt)
-{
-	u64 val;
-
-	local_irq_save(cxt->flags);
-
-	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
-		/*
-		 * For CPUs that are affected by ARM errata 1165522 or 1530923,
-		 * we cannot trust stage-1 to be in a correct state at that
-		 * point. Since we do not want to force a full load of the
-		 * vcpu state, we prevent the EL1 page-table walker to
-		 * allocate new TLBs. This is done by setting the EPD bits
-		 * in the TCR_EL1 register. We also need to prevent it to
-		 * allocate IPA->PA walks, so we enable the S1 MMU...
-		 */
-		val = cxt->tcr = read_sysreg_el1(SYS_TCR);
-		val |= TCR_EPD1_MASK | TCR_EPD0_MASK;
-		write_sysreg_el1(val, SYS_TCR);
-		val = cxt->sctlr = read_sysreg_el1(SYS_SCTLR);
-		val |= SCTLR_ELx_M;
-		write_sysreg_el1(val, SYS_SCTLR);
-	}
-
-	/*
-	 * With VHE enabled, we have HCR_EL2.{E2H,TGE} = {1,1}, and
-	 * most TLB operations target EL2/EL0. In order to affect the
-	 * guest TLBs (EL1/EL0), we need to change one of these two
-	 * bits. Changing E2H is impossible (goodbye TTBR1_EL2), so
-	 * let's flip TGE before executing the TLB operation.
-	 *
-	 * ARM erratum 1165522 requires some special handling (again),
-	 * as we need to make sure both stages of translation are in
-	 * place before clearing TGE. __load_guest_stage2() already
-	 * has an ISB in order to deal with this.
-	 */
-	__load_guest_stage2(kvm);
-	val = read_sysreg(hcr_el2);
-	val &= ~HCR_TGE;
-	write_sysreg(val, hcr_el2);
-	isb();
-}
-
-static void __hyp_text __tlb_switch_to_guest_nvhe(struct kvm *kvm,
-						  struct tlb_inv_context *cxt)
-{
-	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
-		u64 val;
-
-		/*
-		 * For CPUs that are affected by ARM 1319367, we need to
-		 * avoid a host Stage-1 walk while we have the guest's
-		 * VMID set in the VTTBR in order to invalidate TLBs.
-		 * We're guaranteed that the S1 MMU is enabled, so we can
-		 * simply set the EPD bits to avoid any further TLB fill.
-		 */
-		val = cxt->tcr = read_sysreg_el1(SYS_TCR);
-		val |= TCR_EPD1_MASK | TCR_EPD0_MASK;
-		write_sysreg_el1(val, SYS_TCR);
-		isb();
-	}
-
-	/* __load_guest_stage2() includes an ISB for the workaround. */
-	__load_guest_stage2(kvm);
-	asm(ALTERNATIVE("isb", "nop", ARM64_WORKAROUND_SPECULATIVE_AT));
-}
-
-static void __hyp_text __tlb_switch_to_guest(struct kvm *kvm,
-					     struct tlb_inv_context *cxt)
-{
-	if (has_vhe())
-		__tlb_switch_to_guest_vhe(kvm, cxt);
-	else
-		__tlb_switch_to_guest_nvhe(kvm, cxt);
-}
-
-static void __hyp_text __tlb_switch_to_host_vhe(struct kvm *kvm,
-						struct tlb_inv_context *cxt)
-{
-	/*
-	 * We're done with the TLB operation, let's restore the host's
-	 * view of HCR_EL2.
-	 */
-	write_sysreg(0, vttbr_el2);
-	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
-	isb();
-
-	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
-		/* Restore the registers to what they were */
-		write_sysreg_el1(cxt->tcr, SYS_TCR);
-		write_sysreg_el1(cxt->sctlr, SYS_SCTLR);
-	}
-
-	local_irq_restore(cxt->flags);
-}
-
-static void __hyp_text __tlb_switch_to_host_nvhe(struct kvm *kvm,
-						 struct tlb_inv_context *cxt)
-{
-	write_sysreg(0, vttbr_el2);
-
-	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
-		/* Ensure write of the host VMID */
-		isb();
-		/* Restore the host's TCR_EL1 */
-		write_sysreg_el1(cxt->tcr, SYS_TCR);
-	}
-}
-
-static void __hyp_text __tlb_switch_to_host(struct kvm *kvm,
-					    struct tlb_inv_context *cxt)
-{
-	if (has_vhe())
-		__tlb_switch_to_host_vhe(kvm, cxt);
-	else
-		__tlb_switch_to_host_nvhe(kvm, cxt);
-}
-
-void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
-{
-	struct tlb_inv_context cxt;
-
-	dsb(ishst);
-
-	/* Switch to requested VMID */
-	kvm = kern_hyp_va(kvm);
-	__tlb_switch_to_guest(kvm, &cxt);
-
-	/*
-	 * We could do so much better if we had the VA as well.
-	 * Instead, we invalidate Stage-2 for this IPA, and the
-	 * whole of Stage-1. Weep...
-	 */
-	ipa >>= 12;
-	__tlbi(ipas2e1is, ipa);
-
-	/*
-	 * We have to ensure completion of the invalidation at Stage-2,
-	 * since a table walk on another CPU could refill a TLB with a
-	 * complete (S1 + S2) walk based on the old Stage-2 mapping if
-	 * the Stage-1 invalidation happened first.
-	 */
-	dsb(ish);
-	__tlbi(vmalle1is);
-	dsb(ish);
-	isb();
-
-	/*
-	 * If the host is running at EL1 and we have a VPIPT I-cache,
-	 * then we must perform I-cache maintenance at EL2 in order for
-	 * it to have an effect on the guest. Since the guest cannot hit
-	 * I-cache lines allocated with a different VMID, we don't need
-	 * to worry about junk out of guest reset (we nuke the I-cache on
-	 * VMID rollover), but we do need to be careful when remapping
-	 * executable pages for the same guest. This can happen when KSM
-	 * takes a CoW fault on an executable page, copies the page into
-	 * a page that was previously mapped in the guest and then needs
-	 * to invalidate the guest view of the I-cache for that page
-	 * from EL1. To solve this, we invalidate the entire I-cache when
-	 * unmapping a page from a guest if we have a VPIPT I-cache but
-	 * the host is running at EL1. As above, we could do better if
-	 * we had the VA.
-	 *
-	 * The moral of this story is: if you have a VPIPT I-cache, then
-	 * you should be running with VHE enabled.
-	 */
-	if (!has_vhe() && icache_is_vpipt())
-		__flush_icache_all();
-
-	__tlb_switch_to_host(kvm, &cxt);
-}
-
-void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)
-{
-	struct tlb_inv_context cxt;
-
-	dsb(ishst);
-
-	/* Switch to requested VMID */
-	kvm = kern_hyp_va(kvm);
-	__tlb_switch_to_guest(kvm, &cxt);
-
-	__tlbi(vmalls12e1is);
-	dsb(ish);
-	isb();
-
-	__tlb_switch_to_host(kvm, &cxt);
-}
-
-void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu)
-{
-	struct kvm *kvm = kern_hyp_va(kern_hyp_va(vcpu)->kvm);
-	struct tlb_inv_context cxt;
-
-	/* Switch to requested VMID */
-	__tlb_switch_to_guest(kvm, &cxt);
-
-	__tlbi(vmalle1);
-	dsb(nsh);
-	isb();
-
-	__tlb_switch_to_host(kvm, &cxt);
-}
-
-void __hyp_text __kvm_flush_vm_context(void)
-{
-	dsb(ishst);
-	__tlbi(alle1is);
-
-	/*
-	 * VIPT and PIPT caches are not affected by VMID, so no maintenance
-	 * is necessary across a VMID rollover.
-	 *
-	 * VPIPT caches constrain lookup and maintenance to the active VMID,
-	 * so we need to invalidate lines with a stale VMID to avoid an ABA
-	 * race after multiple rollovers.
-	 *
-	 */
-	if (icache_is_vpipt())
-		asm volatile("ic ialluis");
-
-	dsb(ish);
-}
* Unmerged path arch/arm64/kvm/hyp/vhe/Makefile
diff --git a/arch/arm64/kvm/hyp/vhe/tlb.c b/arch/arm64/kvm/hyp/vhe/tlb.c
new file mode 100644
index 000000000000..b275101e9c9c
--- /dev/null
+++ b/arch/arm64/kvm/hyp/vhe/tlb.c
@@ -0,0 +1,161 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2015 - ARM Ltd
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ */
+
+#include <linux/irqflags.h>
+
+#include <asm/kvm_hyp.h>
+#include <asm/kvm_mmu.h>
+#include <asm/tlbflush.h>
+
+struct tlb_inv_context {
+	unsigned long	flags;
+	u64		tcr;
+	u64		sctlr;
+};
+
+static void __tlb_switch_to_guest(struct kvm *kvm, struct tlb_inv_context *cxt)
+{
+	u64 val;
+
+	local_irq_save(cxt->flags);
+
+	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
+		/*
+		 * For CPUs that are affected by ARM errata 1165522 or 1530923,
+		 * we cannot trust stage-1 to be in a correct state at that
+		 * point. Since we do not want to force a full load of the
+		 * vcpu state, we prevent the EL1 page-table walker to
+		 * allocate new TLBs. This is done by setting the EPD bits
+		 * in the TCR_EL1 register. We also need to prevent it to
+		 * allocate IPA->PA walks, so we enable the S1 MMU...
+		 */
+		val = cxt->tcr = read_sysreg_el1(SYS_TCR);
+		val |= TCR_EPD1_MASK | TCR_EPD0_MASK;
+		write_sysreg_el1(val, SYS_TCR);
+		val = cxt->sctlr = read_sysreg_el1(SYS_SCTLR);
+		val |= SCTLR_ELx_M;
+		write_sysreg_el1(val, SYS_SCTLR);
+	}
+
+	/*
+	 * With VHE enabled, we have HCR_EL2.{E2H,TGE} = {1,1}, and
+	 * most TLB operations target EL2/EL0. In order to affect the
+	 * guest TLBs (EL1/EL0), we need to change one of these two
+	 * bits. Changing E2H is impossible (goodbye TTBR1_EL2), so
+	 * let's flip TGE before executing the TLB operation.
+	 *
+	 * ARM erratum 1165522 requires some special handling (again),
+	 * as we need to make sure both stages of translation are in
+	 * place before clearing TGE. __load_guest_stage2() already
+	 * has an ISB in order to deal with this.
+	 */
+	__load_guest_stage2(kvm);
+	val = read_sysreg(hcr_el2);
+	val &= ~HCR_TGE;
+	write_sysreg(val, hcr_el2);
+	isb();
+}
+
+static void __tlb_switch_to_host(struct kvm *kvm, struct tlb_inv_context *cxt)
+{
+	/*
+	 * We're done with the TLB operation, let's restore the host's
+	 * view of HCR_EL2.
+	 */
+	write_sysreg(0, vttbr_el2);
+	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
+	isb();
+
+	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
+		/* Restore the registers to what they were */
+		write_sysreg_el1(cxt->tcr, SYS_TCR);
+		write_sysreg_el1(cxt->sctlr, SYS_SCTLR);
+	}
+
+	local_irq_restore(cxt->flags);
+}
+
+void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
+{
+	struct tlb_inv_context cxt;
+
+	dsb(ishst);
+
+	/* Switch to requested VMID */
+	__tlb_switch_to_guest(kvm, &cxt);
+
+	/*
+	 * We could do so much better if we had the VA as well.
+	 * Instead, we invalidate Stage-2 for this IPA, and the
+	 * whole of Stage-1. Weep...
+	 */
+	ipa >>= 12;
+	__tlbi(ipas2e1is, ipa);
+
+	/*
+	 * We have to ensure completion of the invalidation at Stage-2,
+	 * since a table walk on another CPU could refill a TLB with a
+	 * complete (S1 + S2) walk based on the old Stage-2 mapping if
+	 * the Stage-1 invalidation happened first.
+	 */
+	dsb(ish);
+	__tlbi(vmalle1is);
+	dsb(ish);
+	isb();
+
+	__tlb_switch_to_host(kvm, &cxt);
+}
+
+void __kvm_tlb_flush_vmid(struct kvm *kvm)
+{
+	struct tlb_inv_context cxt;
+
+	dsb(ishst);
+
+	/* Switch to requested VMID */
+	__tlb_switch_to_guest(kvm, &cxt);
+
+	__tlbi(vmalls12e1is);
+	dsb(ish);
+	isb();
+
+	__tlb_switch_to_host(kvm, &cxt);
+}
+
+void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu)
+{
+	struct kvm *kvm = vcpu->kvm;
+	struct tlb_inv_context cxt;
+
+	/* Switch to requested VMID */
+	__tlb_switch_to_guest(kvm, &cxt);
+
+	__tlbi(vmalle1);
+	dsb(nsh);
+	isb();
+
+	__tlb_switch_to_host(kvm, &cxt);
+}
+
+void __kvm_flush_vm_context(void)
+{
+	dsb(ishst);
+	__tlbi(alle1is);
+
+	/*
+	 * VIPT and PIPT caches are not affected by VMID, so no maintenance
+	 * is necessary across a VMID rollover.
+	 *
+	 * VPIPT caches constrain lookup and maintenance to the active VMID,
+	 * so we need to invalidate lines with a stale VMID to avoid an ABA
+	 * race after multiple rollovers.
+	 *
+	 */
+	if (icache_is_vpipt())
+		asm volatile("ic ialluis");
+
+	dsb(ish);
+}
* Unmerged path arch/arm64/kvm/tlb.c
