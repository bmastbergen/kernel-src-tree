bpf: Implement bpf iterator for hash maps

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yonghong Song <yhs@fb.com>
commit d6c4503cc29638f328e1a6e6fefbdbda401c28fc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/d6c4503c.failed

The bpf iterators for hash, percpu hash, lru hash
and lru percpu hash are implemented. During link time,
bpf_iter_reg->check_target() will check map type
and ensure the program access key/value region is
within the map defined key/value size limit.

For percpu hash and lru hash maps, the bpf program
will receive values for all cpus. The map element
bpf iterator infrastructure will prepare value
properly before passing the value pointer to the
bpf program.

This patch set supports readonly map keys and
read/write map values. It does not support deleting
map elements, e.g., from hash tables. If there is
a user case for this, the following mechanism can
be used to support map deletion for hashtab, etc.
  - permit a new bpf program return value, e.g., 2,
    to let bpf iterator know the map element should
    be removed.
  - since bucket lock is taken, the map element will be
    queued.
  - once bucket lock is released after all elements under
    this bucket are traversed, all to-be-deleted map
    elements can be deleted.

	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200723184114.590470-1-yhs@fb.com
(cherry picked from commit d6c4503cc29638f328e1a6e6fefbdbda401c28fc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/hashtab.c
#	kernel/bpf/map_iter.c
diff --cc kernel/bpf/hashtab.c
index 496ea743bae8,024276787055..000000000000
--- a/kernel/bpf/hashtab.c
+++ b/kernel/bpf/hashtab.c
@@@ -1628,6 -1612,197 +1628,200 @@@ htab_lru_map_lookup_and_delete_batch(st
  						  true, false);
  }
  
++<<<<<<< HEAD
++=======
+ struct bpf_iter_seq_hash_map_info {
+ 	struct bpf_map *map;
+ 	struct bpf_htab *htab;
+ 	void *percpu_value_buf; // non-zero means percpu hash
+ 	unsigned long flags;
+ 	u32 bucket_id;
+ 	u32 skip_elems;
+ };
+ 
+ static struct htab_elem *
+ bpf_hash_map_seq_find_next(struct bpf_iter_seq_hash_map_info *info,
+ 			   struct htab_elem *prev_elem)
+ {
+ 	const struct bpf_htab *htab = info->htab;
+ 	unsigned long flags = info->flags;
+ 	u32 skip_elems = info->skip_elems;
+ 	u32 bucket_id = info->bucket_id;
+ 	struct hlist_nulls_head *head;
+ 	struct hlist_nulls_node *n;
+ 	struct htab_elem *elem;
+ 	struct bucket *b;
+ 	u32 i, count;
+ 
+ 	if (bucket_id >= htab->n_buckets)
+ 		return NULL;
+ 
+ 	/* try to find next elem in the same bucket */
+ 	if (prev_elem) {
+ 		/* no update/deletion on this bucket, prev_elem should be still valid
+ 		 * and we won't skip elements.
+ 		 */
+ 		n = rcu_dereference_raw(hlist_nulls_next_rcu(&prev_elem->hash_node));
+ 		elem = hlist_nulls_entry_safe(n, struct htab_elem, hash_node);
+ 		if (elem)
+ 			return elem;
+ 
+ 		/* not found, unlock and go to the next bucket */
+ 		b = &htab->buckets[bucket_id++];
+ 		htab_unlock_bucket(htab, b, flags);
+ 		skip_elems = 0;
+ 	}
+ 
+ 	for (i = bucket_id; i < htab->n_buckets; i++) {
+ 		b = &htab->buckets[i];
+ 		flags = htab_lock_bucket(htab, b);
+ 
+ 		count = 0;
+ 		head = &b->head;
+ 		hlist_nulls_for_each_entry_rcu(elem, n, head, hash_node) {
+ 			if (count >= skip_elems) {
+ 				info->flags = flags;
+ 				info->bucket_id = i;
+ 				info->skip_elems = count;
+ 				return elem;
+ 			}
+ 			count++;
+ 		}
+ 
+ 		htab_unlock_bucket(htab, b, flags);
+ 		skip_elems = 0;
+ 	}
+ 
+ 	info->bucket_id = i;
+ 	info->skip_elems = 0;
+ 	return NULL;
+ }
+ 
+ static void *bpf_hash_map_seq_start(struct seq_file *seq, loff_t *pos)
+ {
+ 	struct bpf_iter_seq_hash_map_info *info = seq->private;
+ 	struct htab_elem *elem;
+ 
+ 	elem = bpf_hash_map_seq_find_next(info, NULL);
+ 	if (!elem)
+ 		return NULL;
+ 
+ 	if (*pos == 0)
+ 		++*pos;
+ 	return elem;
+ }
+ 
+ static void *bpf_hash_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+ {
+ 	struct bpf_iter_seq_hash_map_info *info = seq->private;
+ 
+ 	++*pos;
+ 	++info->skip_elems;
+ 	return bpf_hash_map_seq_find_next(info, v);
+ }
+ 
+ static int __bpf_hash_map_seq_show(struct seq_file *seq, struct htab_elem *elem)
+ {
+ 	struct bpf_iter_seq_hash_map_info *info = seq->private;
+ 	u32 roundup_key_size, roundup_value_size;
+ 	struct bpf_iter__bpf_map_elem ctx = {};
+ 	struct bpf_map *map = info->map;
+ 	struct bpf_iter_meta meta;
+ 	int ret = 0, off = 0, cpu;
+ 	struct bpf_prog *prog;
+ 	void __percpu *pptr;
+ 
+ 	meta.seq = seq;
+ 	prog = bpf_iter_get_info(&meta, elem == NULL);
+ 	if (prog) {
+ 		ctx.meta = &meta;
+ 		ctx.map = info->map;
+ 		if (elem) {
+ 			roundup_key_size = round_up(map->key_size, 8);
+ 			ctx.key = elem->key;
+ 			if (!info->percpu_value_buf) {
+ 				ctx.value = elem->key + roundup_key_size;
+ 			} else {
+ 				roundup_value_size = round_up(map->value_size, 8);
+ 				pptr = htab_elem_get_ptr(elem, map->key_size);
+ 				for_each_possible_cpu(cpu) {
+ 					bpf_long_memcpy(info->percpu_value_buf + off,
+ 							per_cpu_ptr(pptr, cpu),
+ 							roundup_value_size);
+ 					off += roundup_value_size;
+ 				}
+ 				ctx.value = info->percpu_value_buf;
+ 			}
+ 		}
+ 		ret = bpf_iter_run_prog(prog, &ctx);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int bpf_hash_map_seq_show(struct seq_file *seq, void *v)
+ {
+ 	return __bpf_hash_map_seq_show(seq, v);
+ }
+ 
+ static void bpf_hash_map_seq_stop(struct seq_file *seq, void *v)
+ {
+ 	struct bpf_iter_seq_hash_map_info *info = seq->private;
+ 
+ 	if (!v)
+ 		(void)__bpf_hash_map_seq_show(seq, NULL);
+ 	else
+ 		htab_unlock_bucket(info->htab,
+ 				   &info->htab->buckets[info->bucket_id],
+ 				   info->flags);
+ }
+ 
+ static int bpf_iter_init_hash_map(void *priv_data,
+ 				  struct bpf_iter_aux_info *aux)
+ {
+ 	struct bpf_iter_seq_hash_map_info *seq_info = priv_data;
+ 	struct bpf_map *map = aux->map;
+ 	void *value_buf;
+ 	u32 buf_size;
+ 
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+ 		buf_size = round_up(map->value_size, 8) * num_possible_cpus();
+ 		value_buf = kmalloc(buf_size, GFP_USER | __GFP_NOWARN);
+ 		if (!value_buf)
+ 			return -ENOMEM;
+ 
+ 		seq_info->percpu_value_buf = value_buf;
+ 	}
+ 
+ 	seq_info->map = map;
+ 	seq_info->htab = container_of(map, struct bpf_htab, map);
+ 	return 0;
+ }
+ 
+ static void bpf_iter_fini_hash_map(void *priv_data)
+ {
+ 	struct bpf_iter_seq_hash_map_info *seq_info = priv_data;
+ 
+ 	kfree(seq_info->percpu_value_buf);
+ }
+ 
+ static const struct seq_operations bpf_hash_map_seq_ops = {
+ 	.start	= bpf_hash_map_seq_start,
+ 	.next	= bpf_hash_map_seq_next,
+ 	.stop	= bpf_hash_map_seq_stop,
+ 	.show	= bpf_hash_map_seq_show,
+ };
+ 
+ static const struct bpf_iter_seq_info iter_seq_info = {
+ 	.seq_ops		= &bpf_hash_map_seq_ops,
+ 	.init_seq_private	= bpf_iter_init_hash_map,
+ 	.fini_seq_private	= bpf_iter_fini_hash_map,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_hash_map_info),
+ };
+ 
+ static int htab_map_btf_id;
++>>>>>>> d6c4503cc296 (bpf: Implement bpf iterator for hash maps)
  const struct bpf_map_ops htab_map_ops = {
  	.map_alloc_check = htab_map_alloc_check,
  	.map_alloc = htab_map_alloc,
@@@ -1639,8 -1814,12 +1833,14 @@@
  	.map_gen_lookup = htab_map_gen_lookup,
  	.map_seq_show_elem = htab_map_seq_show_elem,
  	BATCH_OPS(htab),
++<<<<<<< HEAD
++=======
+ 	.map_btf_name = "bpf_htab",
+ 	.map_btf_id = &htab_map_btf_id,
+ 	.iter_seq_info = &iter_seq_info,
++>>>>>>> d6c4503cc296 (bpf: Implement bpf iterator for hash maps)
  };
  
 -static int htab_lru_map_btf_id;
  const struct bpf_map_ops htab_lru_map_ops = {
  	.map_alloc_check = htab_map_alloc_check,
  	.map_alloc = htab_map_alloc,
@@@ -1653,6 -1832,9 +1853,12 @@@
  	.map_gen_lookup = htab_lru_map_gen_lookup,
  	.map_seq_show_elem = htab_map_seq_show_elem,
  	BATCH_OPS(htab_lru),
++<<<<<<< HEAD
++=======
+ 	.map_btf_name = "bpf_htab",
+ 	.map_btf_id = &htab_lru_map_btf_id,
+ 	.iter_seq_info = &iter_seq_info,
++>>>>>>> d6c4503cc296 (bpf: Implement bpf iterator for hash maps)
  };
  
  /* Called from eBPF program */
@@@ -1767,8 -1950,12 +1973,14 @@@ const struct bpf_map_ops htab_percpu_ma
  	.map_delete_elem = htab_map_delete_elem,
  	.map_seq_show_elem = htab_percpu_map_seq_show_elem,
  	BATCH_OPS(htab_percpu),
++<<<<<<< HEAD
++=======
+ 	.map_btf_name = "bpf_htab",
+ 	.map_btf_id = &htab_percpu_map_btf_id,
+ 	.iter_seq_info = &iter_seq_info,
++>>>>>>> d6c4503cc296 (bpf: Implement bpf iterator for hash maps)
  };
  
 -static int htab_lru_percpu_map_btf_id;
  const struct bpf_map_ops htab_lru_percpu_map_ops = {
  	.map_alloc_check = htab_map_alloc_check,
  	.map_alloc = htab_map_alloc,
@@@ -1779,6 -1966,9 +1991,12 @@@
  	.map_delete_elem = htab_lru_map_delete_elem,
  	.map_seq_show_elem = htab_percpu_map_seq_show_elem,
  	BATCH_OPS(htab_lru_percpu),
++<<<<<<< HEAD
++=======
+ 	.map_btf_name = "bpf_htab",
+ 	.map_btf_id = &htab_lru_percpu_map_btf_id,
+ 	.iter_seq_info = &iter_seq_info,
++>>>>>>> d6c4503cc296 (bpf: Implement bpf iterator for hash maps)
  };
  
  static int fd_htab_map_alloc_check(union bpf_attr *attr)
diff --cc kernel/bpf/map_iter.c
index af759897e5bd,bcb68b55bf65..000000000000
--- a/kernel/bpf/map_iter.c
+++ b/kernel/bpf/map_iter.c
@@@ -77,17 -78,80 +77,84 @@@ static const struct seq_operations bpf_
  	.show	= bpf_map_seq_show,
  };
  
++<<<<<<< HEAD
++=======
+ BTF_ID_LIST(btf_bpf_map_id)
+ BTF_ID(struct, bpf_map)
+ 
+ static const struct bpf_iter_seq_info bpf_map_seq_info = {
+ 	.seq_ops		= &bpf_map_seq_ops,
+ 	.init_seq_private	= NULL,
+ 	.fini_seq_private	= NULL,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
+ };
+ 
+ static struct bpf_iter_reg bpf_map_reg_info = {
+ 	.target			= "bpf_map",
+ 	.ctx_arg_info_size	= 1,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__bpf_map, map),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ 	.seq_info		= &bpf_map_seq_info,
+ };
+ 
+ static int bpf_iter_check_map(struct bpf_prog *prog,
+ 			      struct bpf_iter_aux_info *aux)
+ {
+ 	u32 key_acc_size, value_acc_size, key_size, value_size;
+ 	struct bpf_map *map = aux->map;
+ 	bool is_percpu = false;
+ 
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH)
+ 		is_percpu = true;
+ 	else if (map->map_type != BPF_MAP_TYPE_HASH &&
+ 		 map->map_type != BPF_MAP_TYPE_LRU_HASH)
+ 		return -EINVAL;
+ 
+ 	key_acc_size = prog->aux->max_rdonly_access;
+ 	value_acc_size = prog->aux->max_rdwr_access;
+ 	key_size = map->key_size;
+ 	if (!is_percpu)
+ 		value_size = map->value_size;
+ 	else
+ 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
+ 
+ 	if (key_acc_size > key_size || value_acc_size > value_size)
+ 		return -EACCES;
+ 
+ 	return 0;
+ }
+ 
+ DEFINE_BPF_ITER_FUNC(bpf_map_elem, struct bpf_iter_meta *meta,
+ 		     struct bpf_map *map, void *key, void *value)
+ 
+ static const struct bpf_iter_reg bpf_map_elem_reg_info = {
+ 	.target			= "bpf_map_elem",
+ 	.check_target		= bpf_iter_check_map,
+ 	.req_linfo		= BPF_ITER_LINK_MAP_FD,
+ 	.ctx_arg_info_size	= 2,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__bpf_map_elem, key),
+ 		  PTR_TO_RDONLY_BUF_OR_NULL },
+ 		{ offsetof(struct bpf_iter__bpf_map_elem, value),
+ 		  PTR_TO_RDWR_BUF_OR_NULL },
+ 	},
+ };
+ 
++>>>>>>> d6c4503cc296 (bpf: Implement bpf iterator for hash maps)
  static int __init bpf_map_iter_init(void)
  {
 -	int ret;
 -
 -	bpf_map_reg_info.ctx_arg_info[0].btf_id = *btf_bpf_map_id;
 -	ret = bpf_iter_reg_target(&bpf_map_reg_info);
 -	if (ret)
 -		return ret;
 -
 -	return bpf_iter_reg_target(&bpf_map_elem_reg_info);
 +	struct bpf_iter_reg reg_info = {
 +		.target			= "bpf_map",
 +		.seq_ops		= &bpf_map_seq_ops,
 +		.init_seq_private	= NULL,
 +		.fini_seq_private	= NULL,
 +		.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
 +	};
 +
 +	return bpf_iter_reg_target(&reg_info);
  }
  
  late_initcall(bpf_map_iter_init);
* Unmerged path kernel/bpf/hashtab.c
* Unmerged path kernel/bpf/map_iter.c
