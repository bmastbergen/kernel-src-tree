x86/boot/compressed/64: Check SEV encryption in 64-bit boot-path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
Rebuild_CHGLOG: - [x86] boot/compressed/64: Check SEV encryption in 64-bit boot-path (Vitaly Kuznetsov) [1868080]
Rebuild_FUZZ: 96.77%
commit-author Joerg Roedel <jroedel@suse.de>
commit 86ce43f7dde81562f58b24b426cef068bd9f7595
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/86ce43f7.failed

Check whether the hypervisor reported the correct C-bit when running as
an SEV guest. Using a wrong C-bit position could be used to leak
sensitive data from the guest to the hypervisor.

The check function is in a separate file:

  arch/x86/kernel/sev_verify_cbit.S

so that it can be re-used in the running kernel image.

 [ bp: Massage. ]

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
Link: https://lkml.kernel.org/r/20201028164659.27002-4-joro@8bytes.org
(cherry picked from commit 86ce43f7dde81562f58b24b426cef068bd9f7595)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/ident_map_64.c
#	arch/x86/boot/compressed/mem_encrypt.S
#	arch/x86/boot/compressed/misc.h
diff --cc arch/x86/boot/compressed/mem_encrypt.S
index a480356e0ed8,aa561795efd1..000000000000
--- a/arch/x86/boot/compressed/mem_encrypt.S
+++ b/arch/x86/boot/compressed/mem_encrypt.S
@@@ -68,10 -65,13 +68,17 @@@ ENTRY(get_sev_encryption_bit
  #endif	/* CONFIG_AMD_MEM_ENCRYPT */
  
  	ret
 -SYM_FUNC_END(get_sev_encryption_bit)
 +ENDPROC(get_sev_encryption_bit)
  
  	.code64
++<<<<<<< HEAD
 +ENTRY(set_sev_encryption_mask)
++=======
+ 
+ #include "../../kernel/sev_verify_cbit.S"
+ 
+ SYM_FUNC_START(set_sev_encryption_mask)
++>>>>>>> 86ce43f7dde8 (x86/boot/compressed/64: Check SEV encryption in 64-bit boot-path)
  #ifdef CONFIG_AMD_MEM_ENCRYPT
  	push	%rbp
  	push	%rdx
@@@ -99,6 -112,7 +106,12 @@@ ENDPROC(set_sev_encryption_mask
  
  #ifdef CONFIG_AMD_MEM_ENCRYPT
  	.balign	8
++<<<<<<< HEAD
 +GLOBAL(sme_me_mask)
 +	.quad	0
++=======
+ SYM_DATA(sme_me_mask,		.quad 0)
+ SYM_DATA(sev_status,		.quad 0)
+ SYM_DATA(sev_check_data,	.quad 0)
++>>>>>>> 86ce43f7dde8 (x86/boot/compressed/64: Check SEV encryption in 64-bit boot-path)
  #endif
diff --cc arch/x86/boot/compressed/misc.h
index d55a1ca4f95b,d9a631c5973c..000000000000
--- a/arch/x86/boot/compressed/misc.h
+++ b/arch/x86/boot/compressed/misc.h
@@@ -133,4 -142,23 +133,26 @@@ int count_immovable_mem_regions(void)
  static inline int count_immovable_mem_regions(void) { return 0; }
  #endif
  
++<<<<<<< HEAD
++=======
+ /* ident_map_64.c */
+ #ifdef CONFIG_X86_5LEVEL
+ extern unsigned int __pgtable_l5_enabled, pgdir_shift, ptrs_per_p4d;
+ #endif
+ 
+ /* Used by PAGE_KERN* macros: */
+ extern pteval_t __default_kernel_pte_mask;
+ 
+ /* idt_64.c */
+ extern gate_desc boot_idt[BOOT_IDT_ENTRIES];
+ extern struct desc_ptr boot_idt_desc;
+ 
+ /* IDT Entry Points */
+ void boot_page_fault(void);
+ void boot_stage1_vc(void);
+ void boot_stage2_vc(void);
+ 
+ unsigned long sev_verify_cbit(unsigned long cr3);
+ 
++>>>>>>> 86ce43f7dde8 (x86/boot/compressed/64: Check SEV encryption in 64-bit boot-path)
  #endif /* BOOT_COMPRESSED_MISC_H */
* Unmerged path arch/x86/boot/compressed/ident_map_64.c
* Unmerged path arch/x86/boot/compressed/ident_map_64.c
* Unmerged path arch/x86/boot/compressed/mem_encrypt.S
* Unmerged path arch/x86/boot/compressed/misc.h
diff --git a/arch/x86/kernel/sev_verify_cbit.S b/arch/x86/kernel/sev_verify_cbit.S
new file mode 100644
index 000000000000..ee04941a6546
--- /dev/null
+++ b/arch/x86/kernel/sev_verify_cbit.S
@@ -0,0 +1,89 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ *	sev_verify_cbit.S - Code for verification of the C-bit position reported
+ *			    by the Hypervisor when running with SEV enabled.
+ *
+ *	Copyright (c) 2020  Joerg Roedel (jroedel@suse.de)
+ *
+ * sev_verify_cbit() is called before switching to a new long-mode page-table
+ * at boot.
+ *
+ * Verify that the C-bit position is correct by writing a random value to
+ * an encrypted memory location while on the current page-table. Then it
+ * switches to the new page-table to verify the memory content is still the
+ * same. After that it switches back to the current page-table and when the
+ * check succeeded it returns. If the check failed the code invalidates the
+ * stack pointer and goes into a hlt loop. The stack-pointer is invalidated to
+ * make sure no interrupt or exception can get the CPU out of the hlt loop.
+ *
+ * New page-table pointer is expected in %rdi (first parameter)
+ *
+ */
+SYM_FUNC_START(sev_verify_cbit)
+#ifdef CONFIG_AMD_MEM_ENCRYPT
+	/* First check if a C-bit was detected */
+	movq	sme_me_mask(%rip), %rsi
+	testq	%rsi, %rsi
+	jz	3f
+
+	/* sme_me_mask != 0 could mean SME or SEV - Check also for SEV */
+	movq	sev_status(%rip), %rsi
+	testq	%rsi, %rsi
+	jz	3f
+
+	/* Save CR4 in %rsi */
+	movq	%cr4, %rsi
+
+	/* Disable Global Pages */
+	movq	%rsi, %rdx
+	andq	$(~X86_CR4_PGE), %rdx
+	movq	%rdx, %cr4
+
+	/*
+	 * Verified that running under SEV - now get a random value using
+	 * RDRAND. This instruction is mandatory when running as an SEV guest.
+	 *
+	 * Don't bail out of the loop if RDRAND returns errors. It is better to
+	 * prevent forward progress than to work with a non-random value here.
+	 */
+1:	rdrand	%rdx
+	jnc	1b
+
+	/* Store value to memory and keep it in %rdx */
+	movq	%rdx, sev_check_data(%rip)
+
+	/* Backup current %cr3 value to restore it later */
+	movq	%cr3, %rcx
+
+	/* Switch to new %cr3 - This might unmap the stack */
+	movq	%rdi, %cr3
+
+	/*
+	 * Compare value in %rdx with memory location. If C-bit is incorrect
+	 * this would read the encrypted data and make the check fail.
+	 */
+	cmpq	%rdx, sev_check_data(%rip)
+
+	/* Restore old %cr3 */
+	movq	%rcx, %cr3
+
+	/* Restore previous CR4 */
+	movq	%rsi, %cr4
+
+	/* Check CMPQ result */
+	je	3f
+
+	/*
+	 * The check failed, prevent any forward progress to prevent ROP
+	 * attacks, invalidate the stack and go into a hlt loop.
+	 */
+	xorq	%rsp, %rsp
+	subq	$0x1000, %rsp
+2:	hlt
+	jmp 2b
+3:
+#endif
+	/* Return page-table pointer */
+	movq	%rdi, %rax
+	ret
+SYM_FUNC_END(sev_verify_cbit)
