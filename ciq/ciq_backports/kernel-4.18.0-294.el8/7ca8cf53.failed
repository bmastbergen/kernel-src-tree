locking/atomic: Move ATOMIC_INIT into linux/types.h

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit 7ca8cf5347f720b07a0b32a924b768f5710547e7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/7ca8cf53.failed

This patch moves ATOMIC_INIT from asm/atomic.h into linux/types.h.
This allows users of atomic_t to use ATOMIC_INIT without having to
include atomic.h as that way may lead to header loops.

	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Waiman Long <longman@redhat.com>
Link: https://lkml.kernel.org/r/20200729123105.GB7047@gondor.apana.org.au
(cherry picked from commit 7ca8cf5347f720b07a0b32a924b768f5710547e7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/atomic.h
#	arch/mips/include/asm/atomic.h
#	arch/riscv/include/asm/atomic.h
diff --cc arch/arm64/include/asm/atomic.h
index 9bca54dda75c,015ddffaf6ca..000000000000
--- a/arch/arm64/include/asm/atomic.h
+++ b/arch/arm64/include/asm/atomic.h
@@@ -24,142 -13,216 +24,147 @@@
  #include <linux/types.h>
  
  #include <asm/barrier.h>
 -#include <asm/cmpxchg.h>
  #include <asm/lse.h>
  
 -#define ATOMIC_OP(op)							\
 -static inline void arch_##op(int i, atomic_t *v)			\
 -{									\
 -	__lse_ll_sc_body(op, i, v);					\
 -}
 -
 -ATOMIC_OP(atomic_andnot)
 -ATOMIC_OP(atomic_or)
 -ATOMIC_OP(atomic_xor)
 -ATOMIC_OP(atomic_add)
 -ATOMIC_OP(atomic_and)
 -ATOMIC_OP(atomic_sub)
 -
 -#undef ATOMIC_OP
 -
 -#define ATOMIC_FETCH_OP(name, op)					\
 -static inline int arch_##op##name(int i, atomic_t *v)			\
 -{									\
 -	return __lse_ll_sc_body(op##name, i, v);			\
 -}
 -
 -#define ATOMIC_FETCH_OPS(op)						\
 -	ATOMIC_FETCH_OP(_relaxed, op)					\
 -	ATOMIC_FETCH_OP(_acquire, op)					\
 -	ATOMIC_FETCH_OP(_release, op)					\
 -	ATOMIC_FETCH_OP(        , op)
 -
 -ATOMIC_FETCH_OPS(atomic_fetch_andnot)
 -ATOMIC_FETCH_OPS(atomic_fetch_or)
 -ATOMIC_FETCH_OPS(atomic_fetch_xor)
 -ATOMIC_FETCH_OPS(atomic_fetch_add)
 -ATOMIC_FETCH_OPS(atomic_fetch_and)
 -ATOMIC_FETCH_OPS(atomic_fetch_sub)
 -ATOMIC_FETCH_OPS(atomic_add_return)
 -ATOMIC_FETCH_OPS(atomic_sub_return)
 -
 -#undef ATOMIC_FETCH_OP
 -#undef ATOMIC_FETCH_OPS
 -
 -#define ATOMIC64_OP(op)							\
 -static inline void arch_##op(long i, atomic64_t *v)			\
 -{									\
 -	__lse_ll_sc_body(op, i, v);					\
 -}
 -
 -ATOMIC64_OP(atomic64_andnot)
 -ATOMIC64_OP(atomic64_or)
 -ATOMIC64_OP(atomic64_xor)
 -ATOMIC64_OP(atomic64_add)
 -ATOMIC64_OP(atomic64_and)
 -ATOMIC64_OP(atomic64_sub)
 -
 -#undef ATOMIC64_OP
 -
 -#define ATOMIC64_FETCH_OP(name, op)					\
 -static inline long arch_##op##name(long i, atomic64_t *v)		\
 -{									\
 -	return __lse_ll_sc_body(op##name, i, v);			\
 -}
 -
 -#define ATOMIC64_FETCH_OPS(op)						\
 -	ATOMIC64_FETCH_OP(_relaxed, op)					\
 -	ATOMIC64_FETCH_OP(_acquire, op)					\
 -	ATOMIC64_FETCH_OP(_release, op)					\
 -	ATOMIC64_FETCH_OP(        , op)
 -
 -ATOMIC64_FETCH_OPS(atomic64_fetch_andnot)
 -ATOMIC64_FETCH_OPS(atomic64_fetch_or)
 -ATOMIC64_FETCH_OPS(atomic64_fetch_xor)
 -ATOMIC64_FETCH_OPS(atomic64_fetch_add)
 -ATOMIC64_FETCH_OPS(atomic64_fetch_and)
 -ATOMIC64_FETCH_OPS(atomic64_fetch_sub)
 -ATOMIC64_FETCH_OPS(atomic64_add_return)
 -ATOMIC64_FETCH_OPS(atomic64_sub_return)
 -
 -#undef ATOMIC64_FETCH_OP
 -#undef ATOMIC64_FETCH_OPS
 -
 -static inline long arch_atomic64_dec_if_positive(atomic64_t *v)
 -{
 -	return __lse_ll_sc_body(atomic64_dec_if_positive, v);
 -}
 +#ifdef __KERNEL__
 +
 +#define __ARM64_IN_ATOMIC_IMPL
 +
 +#if defined(CONFIG_ARM64_LSE_ATOMICS) && defined(CONFIG_AS_LSE)
 +#include <asm/atomic_lse.h>
 +#else
 +#include <asm/atomic_ll_sc.h>
 +#endif
  
 +#undef __ARM64_IN_ATOMIC_IMPL
 +
 +#include <asm/cmpxchg.h>
 +
++<<<<<<< HEAD
 +#define ATOMIC_INIT(i)	{ (i) }
 +
 +#define atomic_read(v)			READ_ONCE((v)->counter)
 +#define atomic_set(v, i)		WRITE_ONCE(((v)->counter), (i))
++=======
+ #define arch_atomic_read(v)			__READ_ONCE((v)->counter)
+ #define arch_atomic_set(v, i)			__WRITE_ONCE(((v)->counter), (i))
 -
 -#define arch_atomic_add_return_relaxed		arch_atomic_add_return_relaxed
 -#define arch_atomic_add_return_acquire		arch_atomic_add_return_acquire
 -#define arch_atomic_add_return_release		arch_atomic_add_return_release
 -#define arch_atomic_add_return			arch_atomic_add_return
 -
 -#define arch_atomic_sub_return_relaxed		arch_atomic_sub_return_relaxed
 -#define arch_atomic_sub_return_acquire		arch_atomic_sub_return_acquire
 -#define arch_atomic_sub_return_release		arch_atomic_sub_return_release
 -#define arch_atomic_sub_return			arch_atomic_sub_return
 -
 -#define arch_atomic_fetch_add_relaxed		arch_atomic_fetch_add_relaxed
 -#define arch_atomic_fetch_add_acquire		arch_atomic_fetch_add_acquire
 -#define arch_atomic_fetch_add_release		arch_atomic_fetch_add_release
 -#define arch_atomic_fetch_add			arch_atomic_fetch_add
 -
 -#define arch_atomic_fetch_sub_relaxed		arch_atomic_fetch_sub_relaxed
 -#define arch_atomic_fetch_sub_acquire		arch_atomic_fetch_sub_acquire
 -#define arch_atomic_fetch_sub_release		arch_atomic_fetch_sub_release
 -#define arch_atomic_fetch_sub			arch_atomic_fetch_sub
 -
 -#define arch_atomic_fetch_and_relaxed		arch_atomic_fetch_and_relaxed
 -#define arch_atomic_fetch_and_acquire		arch_atomic_fetch_and_acquire
 -#define arch_atomic_fetch_and_release		arch_atomic_fetch_and_release
 -#define arch_atomic_fetch_and			arch_atomic_fetch_and
 -
 -#define arch_atomic_fetch_andnot_relaxed	arch_atomic_fetch_andnot_relaxed
 -#define arch_atomic_fetch_andnot_acquire	arch_atomic_fetch_andnot_acquire
 -#define arch_atomic_fetch_andnot_release	arch_atomic_fetch_andnot_release
 -#define arch_atomic_fetch_andnot		arch_atomic_fetch_andnot
 -
 -#define arch_atomic_fetch_or_relaxed		arch_atomic_fetch_or_relaxed
 -#define arch_atomic_fetch_or_acquire		arch_atomic_fetch_or_acquire
 -#define arch_atomic_fetch_or_release		arch_atomic_fetch_or_release
 -#define arch_atomic_fetch_or			arch_atomic_fetch_or
 -
 -#define arch_atomic_fetch_xor_relaxed		arch_atomic_fetch_xor_relaxed
 -#define arch_atomic_fetch_xor_acquire		arch_atomic_fetch_xor_acquire
 -#define arch_atomic_fetch_xor_release		arch_atomic_fetch_xor_release
 -#define arch_atomic_fetch_xor			arch_atomic_fetch_xor
 -
 -#define arch_atomic_xchg_relaxed(v, new) \
 -	arch_xchg_relaxed(&((v)->counter), (new))
 -#define arch_atomic_xchg_acquire(v, new) \
 -	arch_xchg_acquire(&((v)->counter), (new))
 -#define arch_atomic_xchg_release(v, new) \
 -	arch_xchg_release(&((v)->counter), (new))
 -#define arch_atomic_xchg(v, new) \
 -	arch_xchg(&((v)->counter), (new))
 -
 -#define arch_atomic_cmpxchg_relaxed(v, old, new) \
 -	arch_cmpxchg_relaxed(&((v)->counter), (old), (new))
 -#define arch_atomic_cmpxchg_acquire(v, old, new) \
 -	arch_cmpxchg_acquire(&((v)->counter), (old), (new))
 -#define arch_atomic_cmpxchg_release(v, old, new) \
 -	arch_cmpxchg_release(&((v)->counter), (old), (new))
 -#define arch_atomic_cmpxchg(v, old, new) \
 -	arch_cmpxchg(&((v)->counter), (old), (new))
 -
 -#define arch_atomic_andnot			arch_atomic_andnot
++>>>>>>> 7ca8cf5347f7 (locking/atomic: Move ATOMIC_INIT into linux/types.h)
 +
 +#define atomic_add_return_relaxed	atomic_add_return_relaxed
 +#define atomic_add_return_acquire	atomic_add_return_acquire
 +#define atomic_add_return_release	atomic_add_return_release
 +#define atomic_add_return		atomic_add_return
 +
 +#define atomic_sub_return_relaxed	atomic_sub_return_relaxed
 +#define atomic_sub_return_acquire	atomic_sub_return_acquire
 +#define atomic_sub_return_release	atomic_sub_return_release
 +#define atomic_sub_return		atomic_sub_return
 +
 +#define atomic_fetch_add_relaxed	atomic_fetch_add_relaxed
 +#define atomic_fetch_add_acquire	atomic_fetch_add_acquire
 +#define atomic_fetch_add_release	atomic_fetch_add_release
 +#define atomic_fetch_add		atomic_fetch_add
 +
 +#define atomic_fetch_sub_relaxed	atomic_fetch_sub_relaxed
 +#define atomic_fetch_sub_acquire	atomic_fetch_sub_acquire
 +#define atomic_fetch_sub_release	atomic_fetch_sub_release
 +#define atomic_fetch_sub		atomic_fetch_sub
 +
 +#define atomic_fetch_and_relaxed	atomic_fetch_and_relaxed
 +#define atomic_fetch_and_acquire	atomic_fetch_and_acquire
 +#define atomic_fetch_and_release	atomic_fetch_and_release
 +#define atomic_fetch_and		atomic_fetch_and
 +
 +#define atomic_fetch_andnot_relaxed	atomic_fetch_andnot_relaxed
 +#define atomic_fetch_andnot_acquire	atomic_fetch_andnot_acquire
 +#define atomic_fetch_andnot_release	atomic_fetch_andnot_release
 +#define atomic_fetch_andnot		atomic_fetch_andnot
 +
 +#define atomic_fetch_or_relaxed		atomic_fetch_or_relaxed
 +#define atomic_fetch_or_acquire		atomic_fetch_or_acquire
 +#define atomic_fetch_or_release		atomic_fetch_or_release
 +#define atomic_fetch_or			atomic_fetch_or
 +
 +#define atomic_fetch_xor_relaxed	atomic_fetch_xor_relaxed
 +#define atomic_fetch_xor_acquire	atomic_fetch_xor_acquire
 +#define atomic_fetch_xor_release	atomic_fetch_xor_release
 +#define atomic_fetch_xor		atomic_fetch_xor
 +
 +#define atomic_xchg_relaxed(v, new)	xchg_relaxed(&((v)->counter), (new))
 +#define atomic_xchg_acquire(v, new)	xchg_acquire(&((v)->counter), (new))
 +#define atomic_xchg_release(v, new)	xchg_release(&((v)->counter), (new))
 +#define atomic_xchg(v, new)		xchg(&((v)->counter), (new))
 +
 +#define atomic_cmpxchg_relaxed(v, old, new)				\
 +	cmpxchg_relaxed(&((v)->counter), (old), (new))
 +#define atomic_cmpxchg_acquire(v, old, new)				\
 +	cmpxchg_acquire(&((v)->counter), (old), (new))
 +#define atomic_cmpxchg_release(v, old, new)				\
 +	cmpxchg_release(&((v)->counter), (old), (new))
 +#define atomic_cmpxchg(v, old, new)	cmpxchg(&((v)->counter), (old), (new))
 +
 +#define atomic_andnot			atomic_andnot
  
  /*
 - * 64-bit arch_atomic operations.
 + * 64-bit atomic operations.
   */
 -#define ATOMIC64_INIT				ATOMIC_INIT
 -#define arch_atomic64_read			arch_atomic_read
 -#define arch_atomic64_set			arch_atomic_set
 -
 -#define arch_atomic64_add_return_relaxed	arch_atomic64_add_return_relaxed
 -#define arch_atomic64_add_return_acquire	arch_atomic64_add_return_acquire
 -#define arch_atomic64_add_return_release	arch_atomic64_add_return_release
 -#define arch_atomic64_add_return		arch_atomic64_add_return
 -
 -#define arch_atomic64_sub_return_relaxed	arch_atomic64_sub_return_relaxed
 -#define arch_atomic64_sub_return_acquire	arch_atomic64_sub_return_acquire
 -#define arch_atomic64_sub_return_release	arch_atomic64_sub_return_release
 -#define arch_atomic64_sub_return		arch_atomic64_sub_return
 -
 -#define arch_atomic64_fetch_add_relaxed		arch_atomic64_fetch_add_relaxed
 -#define arch_atomic64_fetch_add_acquire		arch_atomic64_fetch_add_acquire
 -#define arch_atomic64_fetch_add_release		arch_atomic64_fetch_add_release
 -#define arch_atomic64_fetch_add			arch_atomic64_fetch_add
 -
 -#define arch_atomic64_fetch_sub_relaxed		arch_atomic64_fetch_sub_relaxed
 -#define arch_atomic64_fetch_sub_acquire		arch_atomic64_fetch_sub_acquire
 -#define arch_atomic64_fetch_sub_release		arch_atomic64_fetch_sub_release
 -#define arch_atomic64_fetch_sub			arch_atomic64_fetch_sub
 -
 -#define arch_atomic64_fetch_and_relaxed		arch_atomic64_fetch_and_relaxed
 -#define arch_atomic64_fetch_and_acquire		arch_atomic64_fetch_and_acquire
 -#define arch_atomic64_fetch_and_release		arch_atomic64_fetch_and_release
 -#define arch_atomic64_fetch_and			arch_atomic64_fetch_and
 -
 -#define arch_atomic64_fetch_andnot_relaxed	arch_atomic64_fetch_andnot_relaxed
 -#define arch_atomic64_fetch_andnot_acquire	arch_atomic64_fetch_andnot_acquire
 -#define arch_atomic64_fetch_andnot_release	arch_atomic64_fetch_andnot_release
 -#define arch_atomic64_fetch_andnot		arch_atomic64_fetch_andnot
 -
 -#define arch_atomic64_fetch_or_relaxed		arch_atomic64_fetch_or_relaxed
 -#define arch_atomic64_fetch_or_acquire		arch_atomic64_fetch_or_acquire
 -#define arch_atomic64_fetch_or_release		arch_atomic64_fetch_or_release
 -#define arch_atomic64_fetch_or			arch_atomic64_fetch_or
 -
 -#define arch_atomic64_fetch_xor_relaxed		arch_atomic64_fetch_xor_relaxed
 -#define arch_atomic64_fetch_xor_acquire		arch_atomic64_fetch_xor_acquire
 -#define arch_atomic64_fetch_xor_release		arch_atomic64_fetch_xor_release
 -#define arch_atomic64_fetch_xor			arch_atomic64_fetch_xor
 -
 -#define arch_atomic64_xchg_relaxed		arch_atomic_xchg_relaxed
 -#define arch_atomic64_xchg_acquire		arch_atomic_xchg_acquire
 -#define arch_atomic64_xchg_release		arch_atomic_xchg_release
 -#define arch_atomic64_xchg			arch_atomic_xchg
 -
 -#define arch_atomic64_cmpxchg_relaxed		arch_atomic_cmpxchg_relaxed
 -#define arch_atomic64_cmpxchg_acquire		arch_atomic_cmpxchg_acquire
 -#define arch_atomic64_cmpxchg_release		arch_atomic_cmpxchg_release
 -#define arch_atomic64_cmpxchg			arch_atomic_cmpxchg
 -
 -#define arch_atomic64_andnot			arch_atomic64_andnot
 -
 -#define arch_atomic64_dec_if_positive		arch_atomic64_dec_if_positive
 -
 -#define ARCH_ATOMIC
 -
 -#endif /* __ASM_ATOMIC_H */
 +#define ATOMIC64_INIT			ATOMIC_INIT
 +#define atomic64_read			atomic_read
 +#define atomic64_set			atomic_set
 +
 +#define atomic64_add_return_relaxed	atomic64_add_return_relaxed
 +#define atomic64_add_return_acquire	atomic64_add_return_acquire
 +#define atomic64_add_return_release	atomic64_add_return_release
 +#define atomic64_add_return		atomic64_add_return
 +
 +#define atomic64_sub_return_relaxed	atomic64_sub_return_relaxed
 +#define atomic64_sub_return_acquire	atomic64_sub_return_acquire
 +#define atomic64_sub_return_release	atomic64_sub_return_release
 +#define atomic64_sub_return		atomic64_sub_return
 +
 +#define atomic64_fetch_add_relaxed	atomic64_fetch_add_relaxed
 +#define atomic64_fetch_add_acquire	atomic64_fetch_add_acquire
 +#define atomic64_fetch_add_release	atomic64_fetch_add_release
 +#define atomic64_fetch_add		atomic64_fetch_add
 +
 +#define atomic64_fetch_sub_relaxed	atomic64_fetch_sub_relaxed
 +#define atomic64_fetch_sub_acquire	atomic64_fetch_sub_acquire
 +#define atomic64_fetch_sub_release	atomic64_fetch_sub_release
 +#define atomic64_fetch_sub		atomic64_fetch_sub
 +
 +#define atomic64_fetch_and_relaxed	atomic64_fetch_and_relaxed
 +#define atomic64_fetch_and_acquire	atomic64_fetch_and_acquire
 +#define atomic64_fetch_and_release	atomic64_fetch_and_release
 +#define atomic64_fetch_and		atomic64_fetch_and
 +
 +#define atomic64_fetch_andnot_relaxed	atomic64_fetch_andnot_relaxed
 +#define atomic64_fetch_andnot_acquire	atomic64_fetch_andnot_acquire
 +#define atomic64_fetch_andnot_release	atomic64_fetch_andnot_release
 +#define atomic64_fetch_andnot		atomic64_fetch_andnot
 +
 +#define atomic64_fetch_or_relaxed	atomic64_fetch_or_relaxed
 +#define atomic64_fetch_or_acquire	atomic64_fetch_or_acquire
 +#define atomic64_fetch_or_release	atomic64_fetch_or_release
 +#define atomic64_fetch_or		atomic64_fetch_or
 +
 +#define atomic64_fetch_xor_relaxed	atomic64_fetch_xor_relaxed
 +#define atomic64_fetch_xor_acquire	atomic64_fetch_xor_acquire
 +#define atomic64_fetch_xor_release	atomic64_fetch_xor_release
 +#define atomic64_fetch_xor		atomic64_fetch_xor
 +
 +#define atomic64_xchg_relaxed		atomic_xchg_relaxed
 +#define atomic64_xchg_acquire		atomic_xchg_acquire
 +#define atomic64_xchg_release		atomic_xchg_release
 +#define atomic64_xchg			atomic_xchg
 +
 +#define atomic64_cmpxchg_relaxed	atomic_cmpxchg_relaxed
 +#define atomic64_cmpxchg_acquire	atomic_cmpxchg_acquire
 +#define atomic64_cmpxchg_release	atomic_cmpxchg_release
 +#define atomic64_cmpxchg		atomic_cmpxchg
 +
 +#define atomic64_andnot			atomic64_andnot
 +
 +#define atomic64_dec_if_positive	atomic64_dec_if_positive
 +
 +#endif
 +#endif
diff --cc arch/mips/include/asm/atomic.h
index 79be687de4ab,f904084fcb1f..000000000000
--- a/arch/mips/include/asm/atomic.h
+++ b/arch/mips/include/asm/atomic.h
@@@ -20,166 -20,138 +20,201 @@@
  #include <asm/compiler.h>
  #include <asm/cpu-features.h>
  #include <asm/cmpxchg.h>
 -#include <asm/llsc.h>
 -#include <asm/sync.h>
  #include <asm/war.h>
  
 -#define ATOMIC_OPS(pfx, type)						\
 -static __always_inline type pfx##_read(const pfx##_t *v)		\
 -{									\
 -	return READ_ONCE(v->counter);					\
 -}									\
 -									\
 -static __always_inline void pfx##_set(pfx##_t *v, type i)		\
 -{									\
 -	WRITE_ONCE(v->counter, i);					\
 -}									\
 -									\
 -static __always_inline type pfx##_cmpxchg(pfx##_t *v, type o, type n)	\
 -{									\
 -	return cmpxchg(&v->counter, o, n);				\
 -}									\
 -									\
 -static __always_inline type pfx##_xchg(pfx##_t *v, type n)		\
 -{									\
 -	return xchg(&v->counter, n);					\
 +#define ATOMIC_INIT(i)	  { (i) }
 +
 +/*
 + * atomic_read - read atomic variable
 + * @v: pointer of type atomic_t
 + *
 + * Atomically reads the value of @v.
 + */
 +#define atomic_read(v)		READ_ONCE((v)->counter)
 +
 +/*
 + * atomic_set - set atomic variable
 + * @v: pointer of type atomic_t
 + * @i: required value
 + *
 + * Atomically sets the value of @v to @i.
 + */
 +#define atomic_set(v, i)	WRITE_ONCE((v)->counter, (i))
 +
 +#define ATOMIC_OP(op, c_op, asm_op)					      \
 +static __inline__ void atomic_##op(int i, atomic_t * v)			      \
 +{									      \
 +	if (kernel_uses_llsc && R10000_LLSC_WAR) {			      \
 +		int temp;						      \
 +									      \
 +		__asm__ __volatile__(					      \
 +		"	.set	arch=r4000				\n"   \
 +		"1:	ll	%0, %1		# atomic_" #op "	\n"   \
 +		"	" #asm_op " %0, %2				\n"   \
 +		"	sc	%0, %1					\n"   \
 +		"	beqzl	%0, 1b					\n"   \
 +		"	.set	mips0					\n"   \
 +		: "=&r" (temp), "+" GCC_OFF_SMALL_ASM() (v->counter)	      \
 +		: "Ir" (i));						      \
 +	} else if (kernel_uses_llsc) {					      \
 +		int temp;						      \
 +									      \
 +		do {							      \
 +			__asm__ __volatile__(				      \
 +			"	.set	"MIPS_ISA_LEVEL"		\n"   \
 +			"	ll	%0, %1		# atomic_" #op "\n"   \
 +			"	" #asm_op " %0, %2			\n"   \
 +			"	sc	%0, %1				\n"   \
 +			"	.set	mips0				\n"   \
 +			: "=&r" (temp), "+" GCC_OFF_SMALL_ASM() (v->counter)  \
 +			: "Ir" (i));					      \
 +		} while (unlikely(!temp));				      \
 +	} else {							      \
 +		unsigned long flags;					      \
 +									      \
 +		raw_local_irq_save(flags);				      \
 +		v->counter c_op i;					      \
 +		raw_local_irq_restore(flags);				      \
 +	}								      \
  }
  
++<<<<<<< HEAD
 +#define ATOMIC_OP_RETURN(op, c_op, asm_op)				      \
 +static __inline__ int atomic_##op##_return_relaxed(int i, atomic_t * v)	      \
 +{									      \
 +	int result;							      \
 +									      \
 +	if (kernel_uses_llsc && R10000_LLSC_WAR) {			      \
 +		int temp;						      \
 +									      \
 +		__asm__ __volatile__(					      \
 +		"	.set	arch=r4000				\n"   \
 +		"1:	ll	%1, %2		# atomic_" #op "_return	\n"   \
 +		"	" #asm_op " %0, %1, %3				\n"   \
 +		"	sc	%0, %2					\n"   \
 +		"	beqzl	%0, 1b					\n"   \
 +		"	" #asm_op " %0, %1, %3				\n"   \
 +		"	.set	mips0					\n"   \
 +		: "=&r" (result), "=&r" (temp),				      \
 +		  "+" GCC_OFF_SMALL_ASM() (v->counter)			      \
 +		: "Ir" (i));						      \
 +	} else if (kernel_uses_llsc) {					      \
 +		int temp;						      \
 +									      \
 +		do {							      \
 +			__asm__ __volatile__(				      \
 +			"	.set	"MIPS_ISA_LEVEL"		\n"   \
 +			"	ll	%1, %2	# atomic_" #op "_return	\n"   \
 +			"	" #asm_op " %0, %1, %3			\n"   \
 +			"	sc	%0, %2				\n"   \
 +			"	.set	mips0				\n"   \
 +			: "=&r" (result), "=&r" (temp),			      \
 +			  "+" GCC_OFF_SMALL_ASM() (v->counter)		      \
 +			: "Ir" (i));					      \
 +		} while (unlikely(!result));				      \
 +									      \
 +		result = temp; result c_op i;				      \
 +	} else {							      \
 +		unsigned long flags;					      \
 +									      \
 +		raw_local_irq_save(flags);				      \
 +		result = v->counter;					      \
 +		result c_op i;						      \
 +		v->counter = result;					      \
 +		raw_local_irq_restore(flags);				      \
 +	}								      \
 +									      \
 +	return result;							      \
++=======
+ ATOMIC_OPS(atomic, int)
+ 
+ #ifdef CONFIG_64BIT
+ # define ATOMIC64_INIT(i)	{ (i) }
+ ATOMIC_OPS(atomic64, s64)
+ #endif
+ 
+ #define ATOMIC_OP(pfx, op, type, c_op, asm_op, ll, sc)			\
+ static __inline__ void pfx##_##op(type i, pfx##_t * v)			\
+ {									\
+ 	type temp;							\
+ 									\
+ 	if (!kernel_uses_llsc) {					\
+ 		unsigned long flags;					\
+ 									\
+ 		raw_local_irq_save(flags);				\
+ 		v->counter c_op i;					\
+ 		raw_local_irq_restore(flags);				\
+ 		return;							\
+ 	}								\
+ 									\
+ 	__asm__ __volatile__(						\
+ 	"	.set	push					\n"	\
+ 	"	.set	" MIPS_ISA_LEVEL "			\n"	\
+ 	"	" __SYNC(full, loongson3_war) "			\n"	\
+ 	"1:	" #ll "	%0, %1		# " #pfx "_" #op "	\n"	\
+ 	"	" #asm_op " %0, %2				\n"	\
+ 	"	" #sc "	%0, %1					\n"	\
+ 	"\t" __SC_BEQZ "%0, 1b					\n"	\
+ 	"	.set	pop					\n"	\
+ 	: "=&r" (temp), "+" GCC_OFF_SMALL_ASM() (v->counter)		\
+ 	: "Ir" (i) : __LLSC_CLOBBER);					\
++>>>>>>> 7ca8cf5347f7 (locking/atomic: Move ATOMIC_INIT into linux/types.h)
  }
  
 -#define ATOMIC_OP_RETURN(pfx, op, type, c_op, asm_op, ll, sc)		\
 -static __inline__ type pfx##_##op##_return_relaxed(type i, pfx##_t * v)	\
 -{									\
 -	type temp, result;						\
 -									\
 -	if (!kernel_uses_llsc) {					\
 -		unsigned long flags;					\
 -									\
 -		raw_local_irq_save(flags);				\
 -		result = v->counter;					\
 -		result c_op i;						\
 -		v->counter = result;					\
 -		raw_local_irq_restore(flags);				\
 -		return result;						\
 -	}								\
 -									\
 -	__asm__ __volatile__(						\
 -	"	.set	push					\n"	\
 -	"	.set	" MIPS_ISA_LEVEL "			\n"	\
 -	"	" __SYNC(full, loongson3_war) "			\n"	\
 -	"1:	" #ll "	%1, %2		# " #pfx "_" #op "_return\n"	\
 -	"	" #asm_op " %0, %1, %3				\n"	\
 -	"	" #sc "	%0, %2					\n"	\
 -	"\t" __SC_BEQZ "%0, 1b					\n"	\
 -	"	" #asm_op " %0, %1, %3				\n"	\
 -	"	.set	pop					\n"	\
 -	: "=&r" (result), "=&r" (temp),					\
 -	  "+" GCC_OFF_SMALL_ASM() (v->counter)				\
 -	: "Ir" (i) : __LLSC_CLOBBER);					\
 -									\
 -	return result;							\
 -}
 -
 -#define ATOMIC_FETCH_OP(pfx, op, type, c_op, asm_op, ll, sc)		\
 -static __inline__ type pfx##_fetch_##op##_relaxed(type i, pfx##_t * v)	\
 -{									\
 -	int temp, result;						\
 -									\
 -	if (!kernel_uses_llsc) {					\
 -		unsigned long flags;					\
 -									\
 -		raw_local_irq_save(flags);				\
 -		result = v->counter;					\
 -		v->counter c_op i;					\
 -		raw_local_irq_restore(flags);				\
 -		return result;						\
 -	}								\
 -									\
 -	__asm__ __volatile__(						\
 -	"	.set	push					\n"	\
 -	"	.set	" MIPS_ISA_LEVEL "			\n"	\
 -	"	" __SYNC(full, loongson3_war) "			\n"	\
 -	"1:	" #ll "	%1, %2		# " #pfx "_fetch_" #op "\n"	\
 -	"	" #asm_op " %0, %1, %3				\n"	\
 -	"	" #sc "	%0, %2					\n"	\
 -	"\t" __SC_BEQZ "%0, 1b					\n"	\
 -	"	.set	pop					\n"	\
 -	"	move	%0, %1					\n"	\
 -	: "=&r" (result), "=&r" (temp),					\
 -	  "+" GCC_OFF_SMALL_ASM() (v->counter)				\
 -	: "Ir" (i) : __LLSC_CLOBBER);					\
 -									\
 -	return result;							\
 +#define ATOMIC_FETCH_OP(op, c_op, asm_op)				      \
 +static __inline__ int atomic_fetch_##op##_relaxed(int i, atomic_t * v)	      \
 +{									      \
 +	int result;							      \
 +									      \
 +	if (kernel_uses_llsc && R10000_LLSC_WAR) {			      \
 +		int temp;						      \
 +									      \
 +		__asm__ __volatile__(					      \
 +		"	.set	arch=r4000				\n"   \
 +		"1:	ll	%1, %2		# atomic_fetch_" #op "	\n"   \
 +		"	" #asm_op " %0, %1, %3				\n"   \
 +		"	sc	%0, %2					\n"   \
 +		"	beqzl	%0, 1b					\n"   \
 +		"	move	%0, %1					\n"   \
 +		"	.set	mips0					\n"   \
 +		: "=&r" (result), "=&r" (temp),				      \
 +		  "+" GCC_OFF_SMALL_ASM() (v->counter)			      \
 +		: "Ir" (i));						      \
 +	} else if (kernel_uses_llsc) {					      \
 +		int temp;						      \
 +									      \
 +		do {							      \
 +			__asm__ __volatile__(				      \
 +			"	.set	"MIPS_ISA_LEVEL"		\n"   \
 +			"	ll	%1, %2	# atomic_fetch_" #op "	\n"   \
 +			"	" #asm_op " %0, %1, %3			\n"   \
 +			"	sc	%0, %2				\n"   \
 +			"	.set	mips0				\n"   \
 +			: "=&r" (result), "=&r" (temp),			      \
 +			  "+" GCC_OFF_SMALL_ASM() (v->counter)		      \
 +			: "Ir" (i));					      \
 +		} while (unlikely(!result));				      \
 +									      \
 +		result = temp;						      \
 +	} else {							      \
 +		unsigned long flags;					      \
 +									      \
 +		raw_local_irq_save(flags);				      \
 +		result = v->counter;					      \
 +		v->counter c_op i;					      \
 +		raw_local_irq_restore(flags);				      \
 +	}								      \
 +									      \
 +	return result;							      \
  }
  
 -#undef ATOMIC_OPS
 -#define ATOMIC_OPS(pfx, op, type, c_op, asm_op, ll, sc)			\
 -	ATOMIC_OP(pfx, op, type, c_op, asm_op, ll, sc)			\
 -	ATOMIC_OP_RETURN(pfx, op, type, c_op, asm_op, ll, sc)		\
 -	ATOMIC_FETCH_OP(pfx, op, type, c_op, asm_op, ll, sc)
 +#define ATOMIC_OPS(op, c_op, asm_op)					      \
 +	ATOMIC_OP(op, c_op, asm_op)					      \
 +	ATOMIC_OP_RETURN(op, c_op, asm_op)				      \
 +	ATOMIC_FETCH_OP(op, c_op, asm_op)
  
 -ATOMIC_OPS(atomic, add, int, +=, addu, ll, sc)
 -ATOMIC_OPS(atomic, sub, int, -=, subu, ll, sc)
 +ATOMIC_OPS(add, +=, addu)
 +ATOMIC_OPS(sub, -=, subu)
  
  #define atomic_add_return_relaxed	atomic_add_return_relaxed
  #define atomic_sub_return_relaxed	atomic_sub_return_relaxed
diff --cc arch/riscv/include/asm/atomic.h
index 191a79f2027b,400a8c8b6de7..000000000000
--- a/arch/riscv/include/asm/atomic.h
+++ b/arch/riscv/include/asm/atomic.h
@@@ -23,20 -19,11 +23,25 @@@
  #include <asm/cmpxchg.h>
  #include <asm/barrier.h>
  
++<<<<<<< HEAD
 +#define ATOMIC_INIT(i)	{ (i) }
 +
 +#define __atomic_op_acquire(op, args...)				\
 +({									\
 +	typeof(op##_relaxed(args)) __ret  = op##_relaxed(args);		\
 +	__asm__ __volatile__(RISCV_ACQUIRE_BARRIER "" ::: "memory");	\
 +	__ret;								\
 +})
++=======
+ #define __atomic_acquire_fence()					\
+ 	__asm__ __volatile__(RISCV_ACQUIRE_BARRIER "" ::: "memory")
++>>>>>>> 7ca8cf5347f7 (locking/atomic: Move ATOMIC_INIT into linux/types.h)
  
 -#define __atomic_release_fence()					\
 -	__asm__ __volatile__(RISCV_RELEASE_BARRIER "" ::: "memory");
 +#define __atomic_op_release(op, args...)				\
 +({									\
 +	__asm__ __volatile__(RISCV_RELEASE_BARRIER "" ::: "memory");	\
 +	op##_relaxed(args);						\
 +})
  
  static __always_inline int atomic_read(const atomic_t *v)
  {
diff --git a/arch/alpha/include/asm/atomic.h b/arch/alpha/include/asm/atomic.h
index d76218e3d740..5c4d9a9a25e6 100644
--- a/arch/alpha/include/asm/atomic.h
+++ b/arch/alpha/include/asm/atomic.h
@@ -24,7 +24,6 @@
 #define __atomic_op_acquire(op, args...)	op##_relaxed(args)
 #define __atomic_op_fence			__atomic_op_release
 
-#define ATOMIC_INIT(i)		{ (i) }
 #define ATOMIC64_INIT(i)	{ (i) }
 
 #define atomic_read(v)		READ_ONCE((v)->counter)
diff --git a/arch/arc/include/asm/atomic.h b/arch/arc/include/asm/atomic.h
index 6437bb5b30f4..16c4e52d83a9 100644
--- a/arch/arc/include/asm/atomic.h
+++ b/arch/arc/include/asm/atomic.h
@@ -17,8 +17,6 @@
 #include <asm/barrier.h>
 #include <asm/smp.h>
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 #ifndef CONFIG_ARC_PLAT_EZNPS
 
 #define atomic_read(v)  READ_ONCE((v)->counter)
diff --git a/arch/arm/include/asm/atomic.h b/arch/arm/include/asm/atomic.h
index f74756641410..e77bfcfd75ab 100644
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@ -18,8 +18,6 @@
 #include <asm/barrier.h>
 #include <asm/cmpxchg.h>
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 #ifdef __KERNEL__
 
 /*
* Unmerged path arch/arm64/include/asm/atomic.h
diff --git a/arch/h8300/include/asm/atomic.h b/arch/h8300/include/asm/atomic.h
index 532f3409f751..4509a9124907 100644
--- a/arch/h8300/include/asm/atomic.h
+++ b/arch/h8300/include/asm/atomic.h
@@ -10,8 +10,6 @@
  * resource counting etc..
  */
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 #define atomic_read(v)		READ_ONCE((v)->counter)
 #define atomic_set(v, i)	WRITE_ONCE(((v)->counter), (i))
 
diff --git a/arch/hexagon/include/asm/atomic.h b/arch/hexagon/include/asm/atomic.h
index 311b9894ccc8..6f180719c791 100644
--- a/arch/hexagon/include/asm/atomic.h
+++ b/arch/hexagon/include/asm/atomic.h
@@ -26,8 +26,6 @@
 #include <asm/cmpxchg.h>
 #include <asm/barrier.h>
 
-#define ATOMIC_INIT(i)		{ (i) }
-
 /*  Normal writes in our arch don't clear lock reservations  */
 
 static inline void atomic_set(atomic_t *v, int new)
diff --git a/arch/ia64/include/asm/atomic.h b/arch/ia64/include/asm/atomic.h
index 206530d0751b..813ca8fb9b4e 100644
--- a/arch/ia64/include/asm/atomic.h
+++ b/arch/ia64/include/asm/atomic.h
@@ -19,7 +19,6 @@
 #include <asm/barrier.h>
 
 
-#define ATOMIC_INIT(i)		{ (i) }
 #define ATOMIC64_INIT(i)	{ (i) }
 
 #define atomic_read(v)		READ_ONCE((v)->counter)
diff --git a/arch/m68k/include/asm/atomic.h b/arch/m68k/include/asm/atomic.h
index 47228b0d4163..756c5cc58f94 100644
--- a/arch/m68k/include/asm/atomic.h
+++ b/arch/m68k/include/asm/atomic.h
@@ -16,8 +16,6 @@
  * We do not have SMP m68k systems, so we don't have to deal with that.
  */
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 #define atomic_read(v)		READ_ONCE((v)->counter)
 #define atomic_set(v, i)	WRITE_ONCE(((v)->counter), (i))
 
* Unmerged path arch/mips/include/asm/atomic.h
diff --git a/arch/parisc/include/asm/atomic.h b/arch/parisc/include/asm/atomic.h
index 118953d41763..f960e2f32b1b 100644
--- a/arch/parisc/include/asm/atomic.h
+++ b/arch/parisc/include/asm/atomic.h
@@ -136,8 +136,6 @@ ATOMIC_OPS(xor, ^=)
 #undef ATOMIC_OP_RETURN
 #undef ATOMIC_OP
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 #ifdef CONFIG_64BIT
 
 #define ATOMIC64_INIT(i) { (i) }
diff --git a/arch/powerpc/include/asm/atomic.h b/arch/powerpc/include/asm/atomic.h
index a0156cb43d1f..efc102e32137 100644
--- a/arch/powerpc/include/asm/atomic.h
+++ b/arch/powerpc/include/asm/atomic.h
@@ -11,8 +11,6 @@
 #include <asm/cmpxchg.h>
 #include <asm/barrier.h>
 
-#define ATOMIC_INIT(i)		{ (i) }
-
 /*
  * Since *_return_relaxed and {cmp}xchg_relaxed are implemented with
  * a "bne-" instruction at the end, so an isync is enough as a acquire barrier
* Unmerged path arch/riscv/include/asm/atomic.h
diff --git a/arch/s390/include/asm/atomic.h b/arch/s390/include/asm/atomic.h
index fd20ab5d4cf7..3ef059c38808 100644
--- a/arch/s390/include/asm/atomic.h
+++ b/arch/s390/include/asm/atomic.h
@@ -15,8 +15,6 @@
 #include <asm/barrier.h>
 #include <asm/cmpxchg.h>
 
-#define ATOMIC_INIT(i)  { (i) }
-
 static inline int atomic_read(const atomic_t *v)
 {
 	int c;
diff --git a/arch/sh/include/asm/atomic.h b/arch/sh/include/asm/atomic.h
index f37b95a80232..7c2a8a703b9a 100644
--- a/arch/sh/include/asm/atomic.h
+++ b/arch/sh/include/asm/atomic.h
@@ -19,8 +19,6 @@
 #include <asm/cmpxchg.h>
 #include <asm/barrier.h>
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 #define atomic_read(v)		READ_ONCE((v)->counter)
 #define atomic_set(v,i)		WRITE_ONCE((v)->counter, (i))
 
diff --git a/arch/sparc/include/asm/atomic_32.h b/arch/sparc/include/asm/atomic_32.h
index 94c930f0bc62..efad5532f169 100644
--- a/arch/sparc/include/asm/atomic_32.h
+++ b/arch/sparc/include/asm/atomic_32.h
@@ -18,8 +18,6 @@
 #include <asm/barrier.h>
 #include <asm-generic/atomic64.h>
 
-#define ATOMIC_INIT(i)  { (i) }
-
 int atomic_add_return(int, atomic_t *);
 int atomic_fetch_add(int, atomic_t *);
 int atomic_fetch_and(int, atomic_t *);
diff --git a/arch/sparc/include/asm/atomic_64.h b/arch/sparc/include/asm/atomic_64.h
index 6963482c81d8..f138111fd312 100644
--- a/arch/sparc/include/asm/atomic_64.h
+++ b/arch/sparc/include/asm/atomic_64.h
@@ -12,7 +12,6 @@
 #include <asm/cmpxchg.h>
 #include <asm/barrier.h>
 
-#define ATOMIC_INIT(i)		{ (i) }
 #define ATOMIC64_INIT(i)	{ (i) }
 
 #define atomic_read(v)		READ_ONCE((v)->counter)
diff --git a/arch/x86/include/asm/atomic.h b/arch/x86/include/asm/atomic.h
index 61b8f726b6dc..cf86bbcf5e9b 100644
--- a/arch/x86/include/asm/atomic.h
+++ b/arch/x86/include/asm/atomic.h
@@ -14,8 +14,6 @@
  * resource counting etc..
  */
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 /**
  * arch_atomic_read - read atomic variable
  * @v: pointer of type atomic_t
diff --git a/arch/xtensa/include/asm/atomic.h b/arch/xtensa/include/asm/atomic.h
index 7de0149e1cf7..7da79026fa12 100644
--- a/arch/xtensa/include/asm/atomic.h
+++ b/arch/xtensa/include/asm/atomic.h
@@ -21,8 +21,6 @@
 #include <asm/cmpxchg.h>
 #include <asm/barrier.h>
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 /*
  * This Xtensa implementation assumes that the right mechanism
  * for exclusion is for locking interrupts to level EXCM_LEVEL.
diff --git a/include/asm-generic/atomic.h b/include/asm-generic/atomic.h
index 13324aa828eb..9aceb38c1e5b 100644
--- a/include/asm-generic/atomic.h
+++ b/include/asm-generic/atomic.h
@@ -163,8 +163,6 @@ ATOMIC_OP(xor, ^)
  * resource counting etc..
  */
 
-#define ATOMIC_INIT(i)	{ (i) }
-
 /**
  * atomic_read - read atomic variable
  * @v: pointer of type atomic_t
diff --git a/include/linux/types.h b/include/linux/types.h
index 3327032f9224..f2f01c4c3db9 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -178,6 +178,8 @@ typedef struct {
 	int counter;
 } atomic_t;
 
+#define ATOMIC_INIT(i) { (i) }
+
 #ifdef CONFIG_64BIT
 typedef struct {
 	long counter;
