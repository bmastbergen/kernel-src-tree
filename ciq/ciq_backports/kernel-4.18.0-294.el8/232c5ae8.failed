iommu/arm-smmu: Implement iommu_ops->def_domain_type call-back

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
commit 232c5ae8a3614f112712d43e1dbbd8dd6f8453c1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/232c5ae8.failed

Implement the new def_domain_type call-back for the ARM
SMMU driver. We need this to support requesting the domain
type by the client devices.

	Signed-off-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
	Reviewed-by: Robin Murphy <robin.murphy@arm.com>
Link: https://lore.kernel.org/r/28c5d101cc4ac29aff3553ecec7cf256d0907ed7.1587407458.git.saiprakash.ranjan@codeaurora.org
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 232c5ae8a3614f112712d43e1dbbd8dd6f8453c1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm-smmu.h
diff --cc drivers/iommu/arm-smmu.h
index 671b3a337fea,d172c024be61..000000000000
--- a/drivers/iommu/arm-smmu.h
+++ b/drivers/iommu/arm-smmu.h
@@@ -214,6 -230,228 +214,232 @@@ enum arm_smmu_cbar_type 
  #define ARM_SMMU_CB_ATS1PR		0x800
  
  #define ARM_SMMU_CB_ATSR		0x8f0
++<<<<<<< HEAD
 +#define ATSR_ACTIVE			BIT(0)
++=======
+ #define ARM_SMMU_ATSR_ACTIVE		BIT(0)
+ 
+ 
+ /* Maximum number of context banks per SMMU */
+ #define ARM_SMMU_MAX_CBS		128
+ 
+ 
+ /* Shared driver definitions */
+ enum arm_smmu_arch_version {
+ 	ARM_SMMU_V1,
+ 	ARM_SMMU_V1_64K,
+ 	ARM_SMMU_V2,
+ };
+ 
+ enum arm_smmu_implementation {
+ 	GENERIC_SMMU,
+ 	ARM_MMU500,
+ 	CAVIUM_SMMUV2,
+ 	QCOM_SMMUV2,
+ };
+ 
+ struct arm_smmu_device {
+ 	struct device			*dev;
+ 
+ 	void __iomem			*base;
+ 	unsigned int			numpage;
+ 	unsigned int			pgshift;
+ 
+ #define ARM_SMMU_FEAT_COHERENT_WALK	(1 << 0)
+ #define ARM_SMMU_FEAT_STREAM_MATCH	(1 << 1)
+ #define ARM_SMMU_FEAT_TRANS_S1		(1 << 2)
+ #define ARM_SMMU_FEAT_TRANS_S2		(1 << 3)
+ #define ARM_SMMU_FEAT_TRANS_NESTED	(1 << 4)
+ #define ARM_SMMU_FEAT_TRANS_OPS		(1 << 5)
+ #define ARM_SMMU_FEAT_VMID16		(1 << 6)
+ #define ARM_SMMU_FEAT_FMT_AARCH64_4K	(1 << 7)
+ #define ARM_SMMU_FEAT_FMT_AARCH64_16K	(1 << 8)
+ #define ARM_SMMU_FEAT_FMT_AARCH64_64K	(1 << 9)
+ #define ARM_SMMU_FEAT_FMT_AARCH32_L	(1 << 10)
+ #define ARM_SMMU_FEAT_FMT_AARCH32_S	(1 << 11)
+ #define ARM_SMMU_FEAT_EXIDS		(1 << 12)
+ 	u32				features;
+ 
+ 	enum arm_smmu_arch_version	version;
+ 	enum arm_smmu_implementation	model;
+ 	const struct arm_smmu_impl	*impl;
+ 
+ 	u32				num_context_banks;
+ 	u32				num_s2_context_banks;
+ 	DECLARE_BITMAP(context_map, ARM_SMMU_MAX_CBS);
+ 	struct arm_smmu_cb		*cbs;
+ 	atomic_t			irptndx;
+ 
+ 	u32				num_mapping_groups;
+ 	u16				streamid_mask;
+ 	u16				smr_mask_mask;
+ 	struct arm_smmu_smr		*smrs;
+ 	struct arm_smmu_s2cr		*s2crs;
+ 	struct mutex			stream_map_mutex;
+ 
+ 	unsigned long			va_size;
+ 	unsigned long			ipa_size;
+ 	unsigned long			pa_size;
+ 	unsigned long			pgsize_bitmap;
+ 
+ 	u32				num_global_irqs;
+ 	u32				num_context_irqs;
+ 	unsigned int			*irqs;
+ 	struct clk_bulk_data		*clks;
+ 	int				num_clks;
+ 
+ 	spinlock_t			global_sync_lock;
+ 
+ 	/* IOMMU core code handle */
+ 	struct iommu_device		iommu;
+ };
+ 
+ enum arm_smmu_context_fmt {
+ 	ARM_SMMU_CTX_FMT_NONE,
+ 	ARM_SMMU_CTX_FMT_AARCH64,
+ 	ARM_SMMU_CTX_FMT_AARCH32_L,
+ 	ARM_SMMU_CTX_FMT_AARCH32_S,
+ };
+ 
+ struct arm_smmu_cfg {
+ 	u8				cbndx;
+ 	u8				irptndx;
+ 	union {
+ 		u16			asid;
+ 		u16			vmid;
+ 	};
+ 	enum arm_smmu_cbar_type		cbar;
+ 	enum arm_smmu_context_fmt	fmt;
+ };
+ #define ARM_SMMU_INVALID_IRPTNDX	0xff
+ 
+ enum arm_smmu_domain_stage {
+ 	ARM_SMMU_DOMAIN_S1 = 0,
+ 	ARM_SMMU_DOMAIN_S2,
+ 	ARM_SMMU_DOMAIN_NESTED,
+ 	ARM_SMMU_DOMAIN_BYPASS,
+ };
+ 
+ struct arm_smmu_domain {
+ 	struct arm_smmu_device		*smmu;
+ 	struct io_pgtable_ops		*pgtbl_ops;
+ 	const struct iommu_flush_ops	*flush_ops;
+ 	struct arm_smmu_cfg		cfg;
+ 	enum arm_smmu_domain_stage	stage;
+ 	bool				non_strict;
+ 	struct mutex			init_mutex; /* Protects smmu pointer */
+ 	spinlock_t			cb_lock; /* Serialises ATS1* ops and TLB syncs */
+ 	struct iommu_domain		domain;
+ };
+ 
+ static inline u32 arm_smmu_lpae_tcr(struct io_pgtable_cfg *cfg)
+ {
+ 	return ARM_SMMU_TCR_EPD1 |
+ 	       FIELD_PREP(ARM_SMMU_TCR_TG0, cfg->arm_lpae_s1_cfg.tcr.tg) |
+ 	       FIELD_PREP(ARM_SMMU_TCR_SH0, cfg->arm_lpae_s1_cfg.tcr.sh) |
+ 	       FIELD_PREP(ARM_SMMU_TCR_ORGN0, cfg->arm_lpae_s1_cfg.tcr.orgn) |
+ 	       FIELD_PREP(ARM_SMMU_TCR_IRGN0, cfg->arm_lpae_s1_cfg.tcr.irgn) |
+ 	       FIELD_PREP(ARM_SMMU_TCR_T0SZ, cfg->arm_lpae_s1_cfg.tcr.tsz);
+ }
+ 
+ static inline u32 arm_smmu_lpae_tcr2(struct io_pgtable_cfg *cfg)
+ {
+ 	return FIELD_PREP(ARM_SMMU_TCR2_PASIZE, cfg->arm_lpae_s1_cfg.tcr.ips) |
+ 	       FIELD_PREP(ARM_SMMU_TCR2_SEP, ARM_SMMU_TCR2_SEP_UPSTREAM);
+ }
+ 
+ static inline u32 arm_smmu_lpae_vtcr(struct io_pgtable_cfg *cfg)
+ {
+ 	return ARM_SMMU_VTCR_RES1 |
+ 	       FIELD_PREP(ARM_SMMU_VTCR_PS, cfg->arm_lpae_s2_cfg.vtcr.ps) |
+ 	       FIELD_PREP(ARM_SMMU_VTCR_TG0, cfg->arm_lpae_s2_cfg.vtcr.tg) |
+ 	       FIELD_PREP(ARM_SMMU_VTCR_SH0, cfg->arm_lpae_s2_cfg.vtcr.sh) |
+ 	       FIELD_PREP(ARM_SMMU_VTCR_ORGN0, cfg->arm_lpae_s2_cfg.vtcr.orgn) |
+ 	       FIELD_PREP(ARM_SMMU_VTCR_IRGN0, cfg->arm_lpae_s2_cfg.vtcr.irgn) |
+ 	       FIELD_PREP(ARM_SMMU_VTCR_SL0, cfg->arm_lpae_s2_cfg.vtcr.sl) |
+ 	       FIELD_PREP(ARM_SMMU_VTCR_T0SZ, cfg->arm_lpae_s2_cfg.vtcr.tsz);
+ }
+ 
+ /* Implementation details, yay! */
+ struct arm_smmu_impl {
+ 	u32 (*read_reg)(struct arm_smmu_device *smmu, int page, int offset);
+ 	void (*write_reg)(struct arm_smmu_device *smmu, int page, int offset,
+ 			  u32 val);
+ 	u64 (*read_reg64)(struct arm_smmu_device *smmu, int page, int offset);
+ 	void (*write_reg64)(struct arm_smmu_device *smmu, int page, int offset,
+ 			    u64 val);
+ 	int (*cfg_probe)(struct arm_smmu_device *smmu);
+ 	int (*reset)(struct arm_smmu_device *smmu);
+ 	int (*init_context)(struct arm_smmu_domain *smmu_domain);
+ 	void (*tlb_sync)(struct arm_smmu_device *smmu, int page, int sync,
+ 			 int status);
+ 	int (*def_domain_type)(struct device *dev);
+ };
+ 
+ static inline void __iomem *arm_smmu_page(struct arm_smmu_device *smmu, int n)
+ {
+ 	return smmu->base + (n << smmu->pgshift);
+ }
+ 
+ static inline u32 arm_smmu_readl(struct arm_smmu_device *smmu, int page, int offset)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->read_reg))
+ 		return smmu->impl->read_reg(smmu, page, offset);
+ 	return readl_relaxed(arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ static inline void arm_smmu_writel(struct arm_smmu_device *smmu, int page,
+ 				   int offset, u32 val)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->write_reg))
+ 		smmu->impl->write_reg(smmu, page, offset, val);
+ 	else
+ 		writel_relaxed(val, arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ static inline u64 arm_smmu_readq(struct arm_smmu_device *smmu, int page, int offset)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->read_reg64))
+ 		return smmu->impl->read_reg64(smmu, page, offset);
+ 	return readq_relaxed(arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ static inline void arm_smmu_writeq(struct arm_smmu_device *smmu, int page,
+ 				   int offset, u64 val)
+ {
+ 	if (smmu->impl && unlikely(smmu->impl->write_reg64))
+ 		smmu->impl->write_reg64(smmu, page, offset, val);
+ 	else
+ 		writeq_relaxed(val, arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ #define ARM_SMMU_GR0		0
+ #define ARM_SMMU_GR1		1
+ #define ARM_SMMU_CB(s, n)	((s)->numpage + (n))
+ 
+ #define arm_smmu_gr0_read(s, o)		\
+ 	arm_smmu_readl((s), ARM_SMMU_GR0, (o))
+ #define arm_smmu_gr0_write(s, o, v)	\
+ 	arm_smmu_writel((s), ARM_SMMU_GR0, (o), (v))
+ 
+ #define arm_smmu_gr1_read(s, o)		\
+ 	arm_smmu_readl((s), ARM_SMMU_GR1, (o))
+ #define arm_smmu_gr1_write(s, o, v)	\
+ 	arm_smmu_writel((s), ARM_SMMU_GR1, (o), (v))
+ 
+ #define arm_smmu_cb_read(s, n, o)	\
+ 	arm_smmu_readl((s), ARM_SMMU_CB((s), (n)), (o))
+ #define arm_smmu_cb_write(s, n, o, v)	\
+ 	arm_smmu_writel((s), ARM_SMMU_CB((s), (n)), (o), (v))
+ #define arm_smmu_cb_readq(s, n, o)	\
+ 	arm_smmu_readq((s), ARM_SMMU_CB((s), (n)), (o))
+ #define arm_smmu_cb_writeq(s, n, o, v)	\
+ 	arm_smmu_writeq((s), ARM_SMMU_CB((s), (n)), (o), (v))
+ 
+ struct arm_smmu_device *arm_smmu_impl_init(struct arm_smmu_device *smmu);
+ struct arm_smmu_device *qcom_smmu_impl_init(struct arm_smmu_device *smmu);
+ 
+ int arm_mmu500_reset(struct arm_smmu_device *smmu);
++>>>>>>> 232c5ae8a361 (iommu/arm-smmu: Implement iommu_ops->def_domain_type call-back)
  
  #endif /* _ARM_SMMU_H */
diff --git a/drivers/iommu/arm-smmu.c b/drivers/iommu/arm-smmu.c
index a8f7c91a9ac6..7d76c40e3c18 100644
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@ -1762,6 +1762,17 @@ static void arm_smmu_get_resv_regions(struct device *dev,
 	iommu_dma_get_resv_regions(dev, head);
 }
 
+static int arm_smmu_def_domain_type(struct device *dev)
+{
+	struct arm_smmu_master_cfg *cfg = dev_iommu_priv_get(dev);
+	const struct arm_smmu_impl *impl = cfg->smmu->impl;
+
+	if (impl && impl->def_domain_type)
+		return impl->def_domain_type(dev);
+
+	return 0;
+}
+
 static struct iommu_ops arm_smmu_ops = {
 	.capable		= arm_smmu_capable,
 	.domain_alloc		= arm_smmu_domain_alloc,
@@ -1780,6 +1791,7 @@ static struct iommu_ops arm_smmu_ops = {
 	.of_xlate		= arm_smmu_of_xlate,
 	.get_resv_regions	= arm_smmu_get_resv_regions,
 	.put_resv_regions	= generic_iommu_put_resv_regions,
+	.def_domain_type	= arm_smmu_def_domain_type,
 	.pgsize_bitmap		= -1UL, /* Restricted during device attach */
 };
 
* Unmerged path drivers/iommu/arm-smmu.h
