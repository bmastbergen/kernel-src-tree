mm: memcontrol: delete unused lrucare handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit d9eb1ea2bf8734afd8ec7d995270437a7242f82b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/d9eb1ea2.failed

Swapin faults were the last event to charge pages after they had already
been put on the LRU list.  Now that we charge directly on swapin, the
lrucare portion of the charge code is unused.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Alex Shi <alex.shi@linux.alibaba.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Roman Gushchin <guro@fb.com>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Shakeel Butt <shakeelb@google.com>
Link: http://lkml.kernel.org/r/20200508183105.225460-19-hannes@cmpxchg.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d9eb1ea2bf8734afd8ec7d995270437a7242f82b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	kernel/events/uprobes.c
#	mm/filemap.c
#	mm/huge_memory.c
#	mm/khugepaged.c
#	mm/memcontrol.c
#	mm/memory.c
#	mm/migrate.c
#	mm/shmem.c
#	mm/swap_state.c
#	mm/userfaultfd.c
diff --cc include/linux/memcontrol.h
index aaaa5a5dc184,d5bf3b5bfe6d..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -372,16 -355,8 +372,21 @@@ static inline unsigned long mem_cgroup_
  enum mem_cgroup_protection mem_cgroup_protected(struct mem_cgroup *root,
  						struct mem_cgroup *memcg);
  
++<<<<<<< HEAD
 +int mem_cgroup_try_charge(struct page *page, struct mm_struct *mm,
 +			  gfp_t gfp_mask, struct mem_cgroup **memcgp,
 +			  bool compound);
 +int mem_cgroup_try_charge_delay(struct page *page, struct mm_struct *mm,
 +			  gfp_t gfp_mask, struct mem_cgroup **memcgp,
 +			  bool compound);
 +void mem_cgroup_commit_charge(struct page *page, struct mem_cgroup *memcg,
 +			      bool lrucare, bool compound);
 +void mem_cgroup_cancel_charge(struct page *page, struct mem_cgroup *memcg,
 +		bool compound);
++=======
+ int mem_cgroup_charge(struct page *page, struct mm_struct *mm, gfp_t gfp_mask);
+ 
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  void mem_cgroup_uncharge(struct page *page);
  void mem_cgroup_uncharge_list(struct list_head *page_list);
  
@@@ -848,22 -837,9 +853,27 @@@ static inline enum mem_cgroup_protectio
  	return MEMCG_PROT_NONE;
  }
  
++<<<<<<< HEAD
 +static inline int mem_cgroup_try_charge(struct page *page, struct mm_struct *mm,
 +					gfp_t gfp_mask,
 +					struct mem_cgroup **memcgp,
 +					bool compound)
++=======
+ static inline int mem_cgroup_charge(struct page *page, struct mm_struct *mm,
+ 				    gfp_t gfp_mask)
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
 +{
 +	*memcgp = NULL;
 +	return 0;
 +}
 +
 +static inline int mem_cgroup_try_charge_delay(struct page *page,
 +					      struct mm_struct *mm,
 +					      gfp_t gfp_mask,
 +					      struct mem_cgroup **memcgp,
 +					      bool compound)
  {
 +	*memcgp = NULL;
  	return 0;
  }
  
diff --cc kernel/events/uprobes.c
index 053c0240152f,eddc8db96027..000000000000
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@@ -160,16 -161,13 +160,20 @@@ static int __replace_page(struct vm_are
  		.address = addr,
  	};
  	int err;
 -	struct mmu_notifier_range range;
 +	/* For mmu_notifiers */
 +	const unsigned long mmun_start = addr;
 +	const unsigned long mmun_end   = addr + PAGE_SIZE;
 +	struct mem_cgroup *memcg;
  
 -	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, mm, addr,
 -				addr + PAGE_SIZE);
 +	VM_BUG_ON_PAGE(PageTransHuge(old_page), old_page);
  
  	if (new_page) {
++<<<<<<< HEAD
 +		err = mem_cgroup_try_charge(new_page, vma->vm_mm, GFP_KERNEL,
 +					    &memcg, false);
++=======
+ 		err = mem_cgroup_charge(new_page, vma->vm_mm, GFP_KERNEL);
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		if (err)
  			return err;
  	}
diff --cc mm/filemap.c
index efe054d0678b,455990621989..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -843,6 -844,12 +843,15 @@@ static int __add_to_page_cache_locked(s
  	page->mapping = mapping;
  	page->index = offset;
  
++<<<<<<< HEAD
++=======
+ 	if (!huge) {
+ 		error = mem_cgroup_charge(page, current->mm, gfp_mask);
+ 		if (error)
+ 			goto error;
+ 	}
+ 
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  	do {
  		xas_lock_irq(&xas);
  		old = xas_load(&xas);
diff --cc mm/huge_memory.c
index 4033c78cc361,6df182a18d2c..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -556,7 -593,7 +556,11 @@@ static vm_fault_t __do_huge_pmd_anonymo
  
  	VM_BUG_ON_PAGE(!PageCompound(page), page);
  
++<<<<<<< HEAD
 +	if (mem_cgroup_try_charge_delay(page, vma->vm_mm, gfp, &memcg, true)) {
++=======
+ 	if (mem_cgroup_charge(page, vma->vm_mm, gfp)) {
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		put_page(page);
  		count_vm_event(THP_FAULT_FALLBACK);
  		count_vm_event(THP_FAULT_FALLBACK_CHARGE);
diff --cc mm/khugepaged.c
index 0af263adde1a,f29038c485e0..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -985,7 -1059,7 +985,11 @@@ static void collapse_huge_page(struct m
  		goto out_nolock;
  	}
  
++<<<<<<< HEAD
 +	if (unlikely(mem_cgroup_try_charge(new_page, mm, gfp, &memcg, true))) {
++=======
+ 	if (unlikely(mem_cgroup_charge(new_page, mm, gfp))) {
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		result = SCAN_CGROUP_CHARGE_FAIL;
  		goto out_nolock;
  	}
@@@ -1361,7 -1632,7 +1365,11 @@@ static void collapse_file(struct mm_str
  		goto out;
  	}
  
++<<<<<<< HEAD
 +	if (unlikely(mem_cgroup_try_charge(new_page, mm, gfp, &memcg, true))) {
++=======
+ 	if (unlikely(mem_cgroup_charge(new_page, mm, gfp))) {
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		result = SCAN_CGROUP_CHARGE_FAIL;
  		goto out;
  	}
diff --cc mm/memcontrol.c
index e0a80d1ff817,316a84025090..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -2685,52 -2653,11 +2685,47 @@@ static void cancel_charge(struct mem_cg
  
  	css_put_many(&memcg->css, nr_pages);
  }
 -#endif
  
- static void lock_page_lru(struct page *page, int *isolated)
+ static void commit_charge(struct page *page, struct mem_cgroup *memcg)
  {
++<<<<<<< HEAD
 +	struct zone *zone = page_zone(page);
 +
 +	spin_lock_irq(zone_lru_lock(zone));
 +	if (PageLRU(page)) {
 +		struct lruvec *lruvec;
 +
 +		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
 +		ClearPageLRU(page);
 +		del_page_from_lru_list(page, lruvec, page_lru(page));
 +		*isolated = 1;
 +	} else
 +		*isolated = 0;
 +}
 +
 +static void unlock_page_lru(struct page *page, int isolated)
 +{
 +	struct zone *zone = page_zone(page);
 +
 +	if (isolated) {
 +		struct lruvec *lruvec;
 +
 +		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
 +		VM_BUG_ON_PAGE(PageLRU(page), page);
 +		SetPageLRU(page);
 +		add_page_to_lru_list(page, lruvec, page_lru(page));
 +	}
 +	spin_unlock_irq(zone_lru_lock(zone));
 +}
 +
 +static void commit_charge(struct page *page, struct mem_cgroup *memcg,
 +			  bool lrucare)
 +{
 +	int isolated;
 +
++=======
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  	VM_BUG_ON_PAGE(page->mem_cgroup, page);
- 
- 	/*
- 	 * In some cases, SwapCache and FUSE(splice_buf->radixtree), the page
- 	 * may already be on some other mem_cgroup's LRU.  Take care of it.
- 	 */
- 	if (lrucare)
- 		lock_page_lru(page, &isolated);
- 
  	/*
  	 * Nobody should be changing or seriously looking at
  	 * page->mem_cgroup at this point:
@@@ -6300,25 -6458,16 +6292,32 @@@ out
   * @page: page to charge
   * @mm: mm context of the victim
   * @gfp_mask: reclaim mode
++<<<<<<< HEAD
 + * @memcgp: charged memcg return
 + * @compound: charge the page as compound or small page
++=======
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
   *
   * Try to charge @page to the memcg that @mm belongs to, reclaiming
   * pages according to @gfp_mask if necessary.
   *
 - * Returns 0 on success. Otherwise, an error code is returned.
 + * Returns 0 on success, with *@memcgp pointing to the charged memcg.
 + * Otherwise, an error code is returned.
 + *
 + * After page->mapping has been set up, the caller must finalize the
 + * charge with mem_cgroup_commit_charge().  Or abort the transaction
 + * with mem_cgroup_cancel_charge() in case page instantiation fails.
   */
++<<<<<<< HEAD
 +int mem_cgroup_try_charge(struct page *page, struct mm_struct *mm,
 +			  gfp_t gfp_mask, struct mem_cgroup **memcgp,
 +			  bool compound)
++=======
+ int mem_cgroup_charge(struct page *page, struct mm_struct *mm, gfp_t gfp_mask)
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  {
 -	unsigned int nr_pages = hpage_nr_pages(page);
  	struct mem_cgroup *memcg = NULL;
 +	unsigned int nr_pages = compound ? hpage_nr_pages(page) : 1;
  	int ret = 0;
  
  	if (mem_cgroup_disabled())
@@@ -6351,65 -6500,13 +6350,65 @@@
  		memcg = get_mem_cgroup_from_mm(mm);
  
  	ret = try_charge(memcg, gfp_mask, nr_pages);
 -	if (ret)
 -		goto out_put;
 +
 +	css_put(&memcg->css);
 +out:
 +	*memcgp = memcg;
 +	return ret;
 +}
 +
 +int mem_cgroup_try_charge_delay(struct page *page, struct mm_struct *mm,
 +			  gfp_t gfp_mask, struct mem_cgroup **memcgp,
 +			  bool compound)
 +{
 +	struct mem_cgroup *memcg;
 +	int ret;
 +
 +	ret = mem_cgroup_try_charge(page, mm, gfp_mask, memcgp, compound);
 +	memcg = *memcgp;
 +	mem_cgroup_throttle_swaprate(memcg, page_to_nid(page), gfp_mask);
 +	return ret;
 +}
 +
 +/**
 + * mem_cgroup_commit_charge - commit a page charge
 + * @page: page to charge
 + * @memcg: memcg to charge the page to
 + * @lrucare: page might be on LRU already
 + * @compound: charge the page as compound or small page
 + *
 + * Finalize a charge transaction started by mem_cgroup_try_charge(),
 + * after page->mapping has been set up.  This must happen atomically
 + * as part of the page instantiation, i.e. under the page table lock
 + * for anonymous pages, under the page lock for page and swap cache.
 + *
 + * In addition, the page must not be on the LRU during the commit, to
 + * prevent racing with task migration.  If it might be, use @lrucare.
 + *
 + * Use mem_cgroup_cancel_charge() to cancel the transaction instead.
 + */
 +void mem_cgroup_commit_charge(struct page *page, struct mem_cgroup *memcg,
 +			      bool lrucare, bool compound)
 +{
 +	unsigned int nr_pages = compound ? hpage_nr_pages(page) : 1;
 +
 +	VM_BUG_ON_PAGE(!page->mapping, page);
 +	VM_BUG_ON_PAGE(PageLRU(page) && !lrucare, page);
 +
 +	if (mem_cgroup_disabled())
 +		return;
 +	/*
 +	 * Swap faults will attempt to charge the same page multiple
 +	 * times.  But reuse_swap_page() might have removed the page
 +	 * from swapcache already, so we can't check PageSwapCache().
 +	 */
 +	if (!memcg)
 +		return;
  
- 	commit_charge(page, memcg, lrucare);
+ 	commit_charge(page, memcg);
  
  	local_irq_disable();
 -	mem_cgroup_charge_statistics(memcg, page, nr_pages);
 +	mem_cgroup_charge_statistics(memcg, page, compound, nr_pages);
  	memcg_check_events(memcg, page);
  	local_irq_enable();
  
@@@ -6647,11 -6704,10 +6646,11 @@@ void mem_cgroup_migrate(struct page *ol
  		page_counter_charge(&memcg->memsw, nr_pages);
  	css_get_many(&memcg->css, nr_pages);
  
- 	commit_charge(newpage, memcg, false);
+ 	commit_charge(newpage, memcg);
  
  	local_irq_save(flags);
 -	mem_cgroup_charge_statistics(memcg, newpage, nr_pages);
 +	mem_cgroup_charge_statistics(memcg, newpage, PageTransHuge(newpage),
 +			nr_pages);
  	memcg_check_events(memcg, newpage);
  	local_irq_restore(flags);
  }
diff --cc mm/memory.c
index 583eb7e0dd7f,d50d8b498af5..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2608,11 -2660,24 +2608,15 @@@ static vm_fault_t wp_page_copy(struct v
  				vmf->address);
  		if (!new_page)
  			goto oom;
 -
 -		if (!cow_user_page(new_page, old_page, vmf)) {
 -			/*
 -			 * COW failed, if the fault was solved by other,
 -			 * it's fine. If not, userspace would re-fault on
 -			 * the same address and we will handle the fault
 -			 * from the second attempt.
 -			 */
 -			put_page(new_page);
 -			if (old_page)
 -				put_page(old_page);
 -			return 0;
 -		}
 +		cow_user_page(new_page, old_page, vmf->address, vma);
  	}
  
++<<<<<<< HEAD
 +	if (mem_cgroup_try_charge_delay(new_page, mm, GFP_KERNEL, &memcg, false))
++=======
+ 	if (mem_cgroup_charge(new_page, mm, GFP_KERNEL))
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		goto oom_free_new;
 -	cgroup_throttle_swaprate(new_page, GFP_KERNEL);
  
  	__SetPageUptodate(new_page);
  
@@@ -3045,6 -3128,17 +3049,18 @@@ vm_fault_t do_swap_page(struct vm_faul
  				__SetPageLocked(page);
  				__SetPageSwapBacked(page);
  				set_page_private(page, entry.val);
++<<<<<<< HEAD
++=======
+ 
+ 				/* Tell memcg to use swap ownership records */
+ 				SetPageSwapCache(page);
+ 				err = mem_cgroup_charge(page, vma->vm_mm,
+ 							GFP_KERNEL);
+ 				ClearPageSwapCache(page);
+ 				if (err)
+ 					goto out_page;
+ 
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  				lru_cache_add_anon(page);
  				swap_readpage(page, true);
  			}
@@@ -3268,9 -3358,9 +3284,13 @@@ static vm_fault_t do_anonymous_page(str
  	if (!page)
  		goto oom;
  
++<<<<<<< HEAD
 +	if (mem_cgroup_try_charge_delay(page, vma->vm_mm, GFP_KERNEL, &memcg,
 +					false))
++=======
+ 	if (mem_cgroup_charge(page, vma->vm_mm, GFP_KERNEL))
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		goto oom_free_page;
 -	cgroup_throttle_swaprate(page, GFP_KERNEL);
  
  	/*
  	 * The memory barrier inside __SetPageUptodate makes sure that
@@@ -3789,8 -3854,7 +3809,12 @@@ static vm_fault_t do_cow_fault(struct v
  	if (!vmf->cow_page)
  		return VM_FAULT_OOM;
  
++<<<<<<< HEAD
 +	if (mem_cgroup_try_charge_delay(vmf->cow_page, vma->vm_mm, GFP_KERNEL,
 +				&vmf->memcg, false)) {
++=======
+ 	if (mem_cgroup_charge(vmf->cow_page, vma->vm_mm, GFP_KERNEL)) {
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		put_page(vmf->cow_page);
  		return VM_FAULT_OOM;
  	}
diff --cc mm/migrate.c
index 60059875287d,7bfd0962149e..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -2737,7 -2786,7 +2737,11 @@@ static void migrate_vma_insert_page(str
  
  	if (unlikely(anon_vma_prepare(vma)))
  		goto abort;
++<<<<<<< HEAD
 +	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL, &memcg, false))
++=======
+ 	if (mem_cgroup_charge(page, vma->vm_mm, GFP_KERNEL))
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		goto abort;
  
  	/*
diff --cc mm/shmem.c
index 10f03436a6bf,e83de27ce8f4..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -615,6 -623,18 +615,21 @@@ static int shmem_add_to_page_cache(stru
  	page->mapping = mapping;
  	page->index = index;
  
++<<<<<<< HEAD
++=======
+ 	if (!PageSwapCache(page)) {
+ 		error = mem_cgroup_charge(page, charge_mm, gfp);
+ 		if (error) {
+ 			if (PageTransHuge(page)) {
+ 				count_vm_event(THP_FILE_FALLBACK);
+ 				count_vm_event(THP_FILE_FALLBACK_CHARGE);
+ 			}
+ 			goto error;
+ 		}
+ 	}
+ 	cgroup_throttle_swaprate(page, gfp);
+ 
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  	do {
  		void *entry;
  		xas_lock_irq(&xas);
diff --cc mm/swap_state.c
index e2aded84261e,ab0462819a5b..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -430,47 -405,51 +430,78 @@@ struct page *__read_swap_cache_async(sw
  		 * Swap entry may have been freed since our caller observed it.
  		 */
  		err = swapcache_prepare(entry);
 -		if (!err)
 +		if (err == -EEXIST) {
 +			radix_tree_preload_end();
 +			/*
 +			 * We might race against get_swap_page() and stumble
 +			 * across a SWAP_HAS_CACHE swap_map entry whose page
 +			 * has not been brought into the swapcache yet.
 +			 */
 +			cond_resched();
 +			continue;
 +		}
 +		if (err) {		/* swp entry is obsolete ? */
 +			radix_tree_preload_end();
  			break;
 +		}
  
 -		put_page(page);
 -		if (err != -EEXIST)
 -			return NULL;
 -
 +		/* May fail (-ENOMEM) if radix-tree node allocation failed. */
 +		__SetPageLocked(new_page);
 +		__SetPageSwapBacked(new_page);
 +		err = __add_to_swap_cache(new_page, entry);
 +		if (likely(!err)) {
 +			radix_tree_preload_end();
 +			/*
 +			 * Initiate read into locked page and return.
 +			 */
 +			SetPageWorkingset(new_page);
 +			lru_cache_add_anon(new_page);
 +			*new_page_allocated = true;
 +			return new_page;
 +		}
 +		radix_tree_preload_end();
 +		__ClearPageLocked(new_page);
  		/*
 -		 * We might race against __delete_from_swap_cache(), and
 -		 * stumble across a swap_map entry whose SWAP_HAS_CACHE
 -		 * has not yet been cleared.  Or race against another
 -		 * __read_swap_cache_async(), which has set SWAP_HAS_CACHE
 -		 * in swap_map, but not yet added its page to swap cache.
 +		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
 +		 * clear SWAP_HAS_CACHE flag.
  		 */
 -		cond_resched();
 -	}
 -
 +		put_swap_page(new_page, entry);
 +	} while (err != -ENOMEM);
 +
++<<<<<<< HEAD
 +	if (new_page)
 +		put_page(new_page);
 +	return found_page;
++=======
+ 	/*
+ 	 * The swap entry is ours to swap in. Prepare the new page.
+ 	 */
+ 
+ 	__SetPageLocked(page);
+ 	__SetPageSwapBacked(page);
+ 
+ 	/* May fail (-ENOMEM) if XArray node allocation failed. */
+ 	if (add_to_swap_cache(page, entry, gfp_mask & GFP_KERNEL)) {
+ 		put_swap_page(page, entry);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	if (mem_cgroup_charge(page, NULL, gfp_mask)) {
+ 		delete_from_swap_cache(page);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	/* Caller will initiate read into locked page */
+ 	SetPageWorkingset(page);
+ 	lru_cache_add_anon(page);
+ 	*new_page_allocated = true;
+ 	return page;
+ 
+ fail_unlock:
+ 	unlock_page(page);
+ 	put_page(page);
+ 	return NULL;
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  }
  
  /*
diff --cc mm/userfaultfd.c
index 7529d3fcc899,7f5194046b01..000000000000
--- a/mm/userfaultfd.c
+++ b/mm/userfaultfd.c
@@@ -68,12 -96,16 +68,16 @@@ static int mcopy_atomic_pte(struct mm_s
  	__SetPageUptodate(page);
  
  	ret = -ENOMEM;
++<<<<<<< HEAD
 +	if (mem_cgroup_try_charge(page, dst_mm, GFP_KERNEL, &memcg, false))
++=======
+ 	if (mem_cgroup_charge(page, dst_mm, GFP_KERNEL))
++>>>>>>> d9eb1ea2bf87 (mm: memcontrol: delete unused lrucare handling)
  		goto out_release;
  
 -	_dst_pte = pte_mkdirty(mk_pte(page, dst_vma->vm_page_prot));
 -	if (dst_vma->vm_flags & VM_WRITE) {
 -		if (wp_copy)
 -			_dst_pte = pte_mkuffd_wp(_dst_pte);
 -		else
 -			_dst_pte = pte_mkwrite(_dst_pte);
 -	}
 +	_dst_pte = mk_pte(page, dst_vma->vm_page_prot);
 +	if (dst_vma->vm_flags & VM_WRITE)
 +		_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));
  
  	dst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);
  	if (dst_vma->vm_file) {
* Unmerged path include/linux/memcontrol.h
* Unmerged path kernel/events/uprobes.c
* Unmerged path mm/filemap.c
* Unmerged path mm/huge_memory.c
* Unmerged path mm/khugepaged.c
* Unmerged path mm/memcontrol.c
* Unmerged path mm/memory.c
* Unmerged path mm/migrate.c
* Unmerged path mm/shmem.c
* Unmerged path mm/swap_state.c
* Unmerged path mm/userfaultfd.c
