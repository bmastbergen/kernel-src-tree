mm: vmscan: enforce inactive:active ratio at the reclaim root

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit b91ac374346ba206cfd568bb0ab830af6b205cfd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/b91ac374.failed

We split the LRU lists into inactive and an active parts to maximize
workingset protection while allowing just enough inactive cache space to
faciltate readahead and writeback for one-off file accesses (e.g.  a
linear scan through a file, or logging); or just enough inactive anon to
maintain recent reference information when reclaim needs to swap.

With cgroups and their nested LRU lists, we currently don't do this
correctly.  While recursive cgroup reclaim establishes a relative LRU
order among the pages of all involved cgroups, inactive:active size
decisions are done on a per-cgroup level.  As a result, we'll reclaim a
cgroup's workingset when it doesn't have cold pages, even when one of its
siblings has plenty of it that should be reclaimed first.

For example: workload A has 50M worth of hot cache but doesn't do any
one-off file accesses; meanwhile, parallel workload B scans files and
rarely accesses the same page twice.

If these workloads were to run in an uncgrouped system, A would be
protected from the high rate of cache faults from B.  But if they were put
in parallel cgroups for memory accounting purposes, B's fast cache fault
rate would push out the hot cache pages of A.  This is unexpected and
undesirable - the "scan resistance" of the page cache is broken.

This patch moves inactive:active size balancing decisions to the root of
reclaim - the same level where the LRU order is established.

It does this by looking at the recursive size of the inactive and the
active file sets of the cgroup subtree at the beginning of the reclaim
cycle, and then making a decision - scan or skip active pages - that
applies throughout the entire run and to every cgroup involved.

With that in place, in the test above, the VM will recognize that there
are plenty of inactive pages in the combined cache set of workloads A and
B and prefer the one-off cache in B over the hot pages in A.  The scan
resistance of the cache is restored.

[cai@lca.pw: fix some -Wenum-conversion warnings]
  Link: http://lkml.kernel.org/r/1573848697-29262-1-git-send-email-cai@lca.pw
Link: http://lkml.kernel.org/r/20191107205334.158354-4-hannes@cmpxchg.org
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Reviewed-by: Suren Baghdasaryan <surenb@google.com>
	Reviewed-by: Shakeel Butt <shakeelb@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b91ac374346ba206cfd568bb0ab830af6b205cfd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmscan.c
diff --cc mm/vmscan.c
index 707c1839b196,23273293532b..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -101,6 -108,12 +108,15 @@@ struct scan_control 
  	/* One of the zones is ready for compaction */
  	unsigned int compaction_ready:1;
  
++<<<<<<< HEAD
++=======
+ 	/* There is easily reclaimable cold cache in the current node */
+ 	unsigned int cache_trim_mode:1;
+ 
+ 	/* The file pages on the current node are dangerously low */
+ 	unsigned int file_is_tiny:1;
+ 
++>>>>>>> b91ac374346b (mm: vmscan: enforce inactive:active ratio at the reclaim root)
  	/* Allocation order */
  	s8 order;
  
@@@ -2359,45 -2285,11 +2355,53 @@@ static void get_scan_count(struct lruve
  	}
  
  	/*
++<<<<<<< HEAD
 +	 * Prevent the reclaimer from falling into the cache trap: as
 +	 * cache pages start out inactive, every cache fault will tip
 +	 * the scan balance towards the file LRU.  And as the file LRU
 +	 * shrinks, so does the window for rotation from references.
 +	 * This means we have a runaway feedback loop where a tiny
 +	 * thrashing file LRU becomes infinitely more attractive than
 +	 * anon pages.  Try to detect this based on file LRU size.
 +	 */
 +	if (global_reclaim(sc)) {
 +		unsigned long pgdatfile;
 +		unsigned long pgdatfree;
 +		int z;
 +		unsigned long total_high_wmark = 0;
 +
 +		pgdatfree = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
 +		pgdatfile = node_page_state(pgdat, NR_ACTIVE_FILE) +
 +			   node_page_state(pgdat, NR_INACTIVE_FILE);
 +
 +		for (z = 0; z < MAX_NR_ZONES; z++) {
 +			struct zone *zone = &pgdat->node_zones[z];
 +			if (!managed_zone(zone))
 +				continue;
 +
 +			total_high_wmark += high_wmark_pages(zone);
 +		}
 +
 +		if (unlikely(pgdatfile + pgdatfree <= total_high_wmark)) {
 +			/*
 +			 * Force SCAN_ANON if there are enough inactive
 +			 * anonymous pages on the LRU in eligible zones.
 +			 * Otherwise, the small LRU gets thrashed.
 +			 */
 +			if (!inactive_list_is_low(lruvec, false, sc, false) &&
 +			    lruvec_lru_size(lruvec, LRU_INACTIVE_ANON, sc->reclaim_idx)
 +					>> sc->priority) {
 +				scan_balance = SCAN_ANON;
 +				goto out;
 +			}
 +		}
++=======
+ 	 * If the system is almost out of file pages, force-scan anon.
+ 	 */
+ 	if (sc->file_is_tiny) {
+ 		scan_balance = SCAN_ANON;
+ 		goto out;
++>>>>>>> b91ac374346b (mm: vmscan: enforce inactive:active ratio at the reclaim root)
  	}
  
  	/*
@@@ -2769,143 -2699,172 +2767,213 @@@ static bool shrink_node(pg_data_t *pgda
  {
  	struct reclaim_state *reclaim_state = current->reclaim_state;
  	unsigned long nr_reclaimed, nr_scanned;
 -	struct lruvec *target_lruvec;
  	bool reclaimable = false;
+ 	unsigned long file;
  
 -	target_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);
 +	do {
 +		struct mem_cgroup *root = sc->target_mem_cgroup;
 +		struct mem_cgroup *memcg;
  
 -again:
 -	memset(&sc->nr, 0, sizeof(sc->nr));
 +		memset(&sc->nr, 0, sizeof(sc->nr));
  
 -	nr_reclaimed = sc->nr_reclaimed;
 -	nr_scanned = sc->nr_scanned;
 +		nr_reclaimed = sc->nr_reclaimed;
 +		nr_scanned = sc->nr_scanned;
  
++<<<<<<< HEAD
 +		memcg = mem_cgroup_iter(root, NULL, NULL);
 +		do {
 +			unsigned long reclaimed;
 +			unsigned long scanned;
++=======
+ 	/*
+ 	 * Target desirable inactive:active list ratios for the anon
+ 	 * and file LRU lists.
+ 	 */
+ 	if (!sc->force_deactivate) {
+ 		unsigned long refaults;
+ 
+ 		if (inactive_is_low(target_lruvec, LRU_INACTIVE_ANON))
+ 			sc->may_deactivate |= DEACTIVATE_ANON;
+ 		else
+ 			sc->may_deactivate &= ~DEACTIVATE_ANON;
+ 
+ 		/*
+ 		 * When refaults are being observed, it means a new
+ 		 * workingset is being established. Deactivate to get
+ 		 * rid of any stale active pages quickly.
+ 		 */
+ 		refaults = lruvec_page_state(target_lruvec,
+ 					     WORKINGSET_ACTIVATE);
+ 		if (refaults != target_lruvec->refaults ||
+ 		    inactive_is_low(target_lruvec, LRU_INACTIVE_FILE))
+ 			sc->may_deactivate |= DEACTIVATE_FILE;
+ 		else
+ 			sc->may_deactivate &= ~DEACTIVATE_FILE;
+ 	} else
+ 		sc->may_deactivate = DEACTIVATE_ANON | DEACTIVATE_FILE;
+ 
+ 	/*
+ 	 * If we have plenty of inactive file pages that aren't
+ 	 * thrashing, try to reclaim those first before touching
+ 	 * anonymous pages.
+ 	 */
+ 	file = lruvec_page_state(target_lruvec, NR_INACTIVE_FILE);
+ 	if (file >> sc->priority && !(sc->may_deactivate & DEACTIVATE_FILE))
+ 		sc->cache_trim_mode = 1;
+ 	else
+ 		sc->cache_trim_mode = 0;
+ 
+ 	/*
+ 	 * Prevent the reclaimer from falling into the cache trap: as
+ 	 * cache pages start out inactive, every cache fault will tip
+ 	 * the scan balance towards the file LRU.  And as the file LRU
+ 	 * shrinks, so does the window for rotation from references.
+ 	 * This means we have a runaway feedback loop where a tiny
+ 	 * thrashing file LRU becomes infinitely more attractive than
+ 	 * anon pages.  Try to detect this based on file LRU size.
+ 	 */
+ 	if (!cgroup_reclaim(sc)) {
+ 		unsigned long total_high_wmark = 0;
+ 		unsigned long free, anon;
+ 		int z;
++>>>>>>> b91ac374346b (mm: vmscan: enforce inactive:active ratio at the reclaim root)
  
 -		free = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
 -		file = node_page_state(pgdat, NR_ACTIVE_FILE) +
 -			   node_page_state(pgdat, NR_INACTIVE_FILE);
 -
 -		for (z = 0; z < MAX_NR_ZONES; z++) {
 -			struct zone *zone = &pgdat->node_zones[z];
 -			if (!managed_zone(zone))
 +			switch (mem_cgroup_protected(root, memcg)) {
 +			case MEMCG_PROT_MIN:
 +				/*
 +				 * Hard protection.
 +				 * If there is no reclaimable memory, OOM.
 +				 */
  				continue;
 +			case MEMCG_PROT_LOW:
 +				/*
 +				 * Soft protection.
 +				 * Respect the protection only as long as
 +				 * there is an unprotected supply
 +				 * of reclaimable memory from other cgroups.
 +				 */
 +				if (!sc->memcg_low_reclaim) {
 +					sc->memcg_low_skipped = 1;
 +					continue;
 +				}
 +				memcg_memory_event(memcg, MEMCG_LOW);
 +				break;
 +			case MEMCG_PROT_NONE:
 +				/*
 +				 * All protection thresholds breached. We may
 +				 * still choose to vary the scan pressure
 +				 * applied based on by how much the cgroup in
 +				 * question has exceeded its protection
 +				 * thresholds (see get_scan_count).
 +				 */
 +				break;
 +			}
  
 -			total_high_wmark += high_wmark_pages(zone);
 +			reclaimed = sc->nr_reclaimed;
 +			scanned = sc->nr_scanned;
 +			shrink_node_memcg(pgdat, memcg, sc);
 +
 +			shrink_slab(sc->gfp_mask, pgdat->node_id, memcg,
 +					sc->priority);
 +
 +			/* Record the group's reclaim efficiency */
 +			vmpressure(sc->gfp_mask, memcg, false,
 +				   sc->nr_scanned - scanned,
 +				   sc->nr_reclaimed - reclaimed);
 +
 +		} while ((memcg = mem_cgroup_iter(root, memcg, NULL)));
 +
 +		if (reclaim_state) {
 +			sc->nr_reclaimed += reclaim_state->reclaimed_slab;
 +			reclaim_state->reclaimed_slab = 0;
  		}
  
++<<<<<<< HEAD
 +		/* Record the subtree's reclaim efficiency */
 +		vmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,
 +			   sc->nr_scanned - nr_scanned,
 +			   sc->nr_reclaimed - nr_reclaimed);
++=======
+ 		/*
+ 		 * Consider anon: if that's low too, this isn't a
+ 		 * runaway file reclaim problem, but rather just
+ 		 * extreme pressure. Reclaim as per usual then.
+ 		 */
+ 		anon = node_page_state(pgdat, NR_INACTIVE_ANON);
+ 
+ 		sc->file_is_tiny =
+ 			file + free <= total_high_wmark &&
+ 			!(sc->may_deactivate & DEACTIVATE_ANON) &&
+ 			anon >> sc->priority;
+ 	}
++>>>>>>> b91ac374346b (mm: vmscan: enforce inactive:active ratio at the reclaim root)
  
 -	shrink_node_memcgs(pgdat, sc);
 +		if (sc->nr_reclaimed - nr_reclaimed)
 +			reclaimable = true;
  
 -	if (reclaim_state) {
 -		sc->nr_reclaimed += reclaim_state->reclaimed_slab;
 -		reclaim_state->reclaimed_slab = 0;
 -	}
 +		if (current_is_kswapd()) {
 +			/*
 +			 * If reclaim is isolating dirty pages under writeback,
 +			 * it implies that the long-lived page allocation rate
 +			 * is exceeding the page laundering rate. Either the
 +			 * global limits are not being effective at throttling
 +			 * processes due to the page distribution throughout
 +			 * zones or there is heavy usage of a slow backing
 +			 * device. The only option is to throttle from reclaim
 +			 * context which is not ideal as there is no guarantee
 +			 * the dirtying process is throttled in the same way
 +			 * balance_dirty_pages() manages.
 +			 *
 +			 * Once a node is flagged PGDAT_WRITEBACK, kswapd will
 +			 * count the number of pages under pages flagged for
 +			 * immediate reclaim and stall if any are encountered
 +			 * in the nr_immediate check below.
 +			 */
 +			if (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)
 +				set_bit(PGDAT_WRITEBACK, &pgdat->flags);
  
 -	/* Record the subtree's reclaim efficiency */
 -	vmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,
 -		   sc->nr_scanned - nr_scanned,
 -		   sc->nr_reclaimed - nr_reclaimed);
 +			/*
 +			 * Tag a node as congested if all the dirty pages
 +			 * scanned were backed by a congested BDI and
 +			 * wait_iff_congested will stall.
 +			 */
 +			if (sc->nr.dirty && sc->nr.dirty == sc->nr.congested)
 +				set_bit(PGDAT_CONGESTED, &pgdat->flags);
  
 -	if (sc->nr_reclaimed - nr_reclaimed)
 -		reclaimable = true;
 +			/* Allow kswapd to start writing pages during reclaim.*/
 +			if (sc->nr.unqueued_dirty == sc->nr.file_taken)
 +				set_bit(PGDAT_DIRTY, &pgdat->flags);
 +
 +			/*
 +			 * If kswapd scans pages marked marked for immediate
 +			 * reclaim and under writeback (nr_immediate), it
 +			 * implies that pages are cycling through the LRU
 +			 * faster than they are written so also forcibly stall.
 +			 */
 +			if (sc->nr.immediate)
 +				congestion_wait(BLK_RW_ASYNC, HZ/10);
 +		}
  
 -	if (current_is_kswapd()) {
  		/*
 -		 * If reclaim is isolating dirty pages under writeback,
 -		 * it implies that the long-lived page allocation rate
 -		 * is exceeding the page laundering rate. Either the
 -		 * global limits are not being effective at throttling
 -		 * processes due to the page distribution throughout
 -		 * zones or there is heavy usage of a slow backing
 -		 * device. The only option is to throttle from reclaim
 -		 * context which is not ideal as there is no guarantee
 -		 * the dirtying process is throttled in the same way
 -		 * balance_dirty_pages() manages.
 -		 *
 -		 * Once a node is flagged PGDAT_WRITEBACK, kswapd will
 -		 * count the number of pages under pages flagged for
 -		 * immediate reclaim and stall if any are encountered
 -		 * in the nr_immediate check below.
 +		 * Legacy memcg will stall in page writeback so avoid forcibly
 +		 * stalling in wait_iff_congested().
  		 */
 -		if (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)
 -			set_bit(PGDAT_WRITEBACK, &pgdat->flags);
 -
 -		/* Allow kswapd to start writing pages during reclaim.*/
 -		if (sc->nr.unqueued_dirty == sc->nr.file_taken)
 -			set_bit(PGDAT_DIRTY, &pgdat->flags);
 +		if (!global_reclaim(sc) && sane_reclaim(sc) &&
 +		    sc->nr.dirty && sc->nr.dirty == sc->nr.congested)
 +			set_memcg_congestion(pgdat, root, true);
  
  		/*
 -		 * If kswapd scans pages marked marked for immediate
 -		 * reclaim and under writeback (nr_immediate), it
 -		 * implies that pages are cycling through the LRU
 -		 * faster than they are written so also forcibly stall.
 +		 * Stall direct reclaim for IO completions if underlying BDIs
 +		 * and node is congested. Allow kswapd to continue until it
 +		 * starts encountering unqueued dirty pages or cycling through
 +		 * the LRU too quickly.
  		 */
 -		if (sc->nr.immediate)
 -			congestion_wait(BLK_RW_ASYNC, HZ/10);
 -	}
 -
 -	/*
 -	 * Tag a node/memcg as congested if all the dirty pages
 -	 * scanned were backed by a congested BDI and
 -	 * wait_iff_congested will stall.
 -	 *
 -	 * Legacy memcg will stall in page writeback so avoid forcibly
 -	 * stalling in wait_iff_congested().
 -	 */
 -	if ((current_is_kswapd() ||
 -	     (cgroup_reclaim(sc) && writeback_throttling_sane(sc))) &&
 -	    sc->nr.dirty && sc->nr.dirty == sc->nr.congested)
 -		set_bit(LRUVEC_CONGESTED, &target_lruvec->flags);
 -
 -	/*
 -	 * Stall direct reclaim for IO completions if underlying BDIs
 -	 * and node is congested. Allow kswapd to continue until it
 -	 * starts encountering unqueued dirty pages or cycling through
 -	 * the LRU too quickly.
 -	 */
 -	if (!current_is_kswapd() && current_may_throttle() &&
 -	    !sc->hibernation_mode &&
 -	    test_bit(LRUVEC_CONGESTED, &target_lruvec->flags))
 -		wait_iff_congested(BLK_RW_ASYNC, HZ/10);
 +		if (!sc->hibernation_mode && !current_is_kswapd() &&
 +		   current_may_throttle() && pgdat_memcg_congested(pgdat, root))
 +			wait_iff_congested(BLK_RW_ASYNC, HZ/10);
  
 -	if (should_continue_reclaim(pgdat, sc->nr_reclaimed - nr_reclaimed,
 -				    sc))
 -		goto again;
 +	} while (should_continue_reclaim(pgdat, sc->nr_reclaimed - nr_reclaimed,
 +					 sc));
  
  	/*
  	 * Kswapd gives up on balancing particular nodes after too
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c7d5827d7de4..4608b73232ea 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -293,12 +293,12 @@ enum lru_list {
 
 #define for_each_evictable_lru(lru) for (lru = 0; lru <= LRU_ACTIVE_FILE; lru++)
 
-static inline int is_file_lru(enum lru_list lru)
+static inline bool is_file_lru(enum lru_list lru)
 {
 	return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);
 }
 
-static inline int is_active_lru(enum lru_list lru)
+static inline bool is_active_lru(enum lru_list lru)
 {
 	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
 }
* Unmerged path mm/vmscan.c
