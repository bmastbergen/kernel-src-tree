rcu/tree: Move kfree_rcu_cpu locking/unlocking to separate functions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Uladzislau Rezki (Sony) <urezki@gmail.com>
commit 952371d6fc0bc360d1d5780f86bb355836117ca2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/952371d6.failed

Introduce helpers to lock and unlock per-cpu "kfree_rcu_cpu"
structures. That will make kfree_call_rcu() more readable
and prevent programming errors.

	Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
	Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
(cherry picked from commit 952371d6fc0bc360d1d5780f86bb355836117ca2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index 4aa7b6bbcde6,368bdc441ffb..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -2720,9 -3024,41 +2720,44 @@@ struct kfree_rcu_cpu 
  
  static DEFINE_PER_CPU(struct kfree_rcu_cpu, krc);
  
++<<<<<<< HEAD
++=======
+ static __always_inline void
+ debug_rcu_bhead_unqueue(struct kfree_rcu_bulk_data *bhead)
+ {
+ #ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD
+ 	int i;
+ 
+ 	for (i = 0; i < bhead->nr_records; i++)
+ 		debug_rcu_head_unqueue((struct rcu_head *)(bhead->records[i]));
+ #endif
+ }
+ 
+ static inline struct kfree_rcu_cpu *
+ krc_this_cpu_lock(unsigned long *flags)
+ {
+ 	struct kfree_rcu_cpu *krcp;
+ 
+ 	local_irq_save(*flags);	// For safely calling this_cpu_ptr().
+ 	krcp = this_cpu_ptr(&krc);
+ 	if (likely(krcp->initialized))
+ 		raw_spin_lock(&krcp->lock);
+ 
+ 	return krcp;
+ }
+ 
+ static inline void
+ krc_this_cpu_unlock(struct kfree_rcu_cpu *krcp, unsigned long flags)
+ {
+ 	if (likely(krcp->initialized))
+ 		raw_spin_unlock(&krcp->lock);
+ 	local_irq_restore(flags);
+ }
+ 
++>>>>>>> 952371d6fc0b (rcu/tree: Move kfree_rcu_cpu locking/unlocking to separate functions)
  /*
   * This function is invoked in workqueue context after a grace period.
 - * It frees all the objects queued on ->bhead_free or ->head_free.
 + * It frees all the objects queued on ->head_free.
   */
  static void kfree_rcu_work(struct work_struct *work)
  {
@@@ -2841,16 -3279,13 +2876,21 @@@ void kfree_call_rcu(struct rcu_head *he
  {
  	unsigned long flags;
  	struct kfree_rcu_cpu *krcp;
 -	void *ptr;
  
 +	head->func = func;
 +
++<<<<<<< HEAD
 +	local_irq_save(flags);	// For safely calling this_cpu_ptr().
 +	krcp = this_cpu_ptr(&krc);
 +	if (krcp->initialized)
 +		spin_lock(&krcp->lock);
++=======
+ 	krcp = krc_this_cpu_lock(&flags);
+ 	ptr = (void *)head - (unsigned long)func;
++>>>>>>> 952371d6fc0b (rcu/tree: Move kfree_rcu_cpu locking/unlocking to separate functions)
  
  	// Queue the object but don't yet schedule the batch.
 -	if (debug_rcu_head_queue(ptr)) {
 +	if (debug_rcu_head_queue(head)) {
  		// Probable double kfree_rcu(), just leak.
  		WARN_ONCE(1, "%s(): Double-freed call. rcu_head %p\n",
  			  __func__, head);
@@@ -2868,9 -3312,7 +2908,13 @@@
  	}
  
  unlock_return:
++<<<<<<< HEAD
 +	if (krcp->initialized)
 +		spin_unlock(&krcp->lock);
 +	local_irq_restore(flags);
++=======
+ 	krc_this_cpu_unlock(krcp, flags);
++>>>>>>> 952371d6fc0b (rcu/tree: Move kfree_rcu_cpu locking/unlocking to separate functions)
  }
  EXPORT_SYMBOL_GPL(kfree_call_rcu);
  
* Unmerged path kernel/rcu/tree.c
