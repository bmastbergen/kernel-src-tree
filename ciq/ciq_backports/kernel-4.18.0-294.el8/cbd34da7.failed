mm: move the powerpc hugepd code to mm/gup.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Christoph Hellwig <hch@lst.de>
commit cbd34da7dc9afd521e0bea5e7d12701f4a9da7c7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/cbd34da7.failed

While only powerpc supports the hugepd case, the code is pretty generic
and I'd like to keep all GUP internals in one place.

Link: http://lkml.kernel.org/r/20190625143715.1689-15-hch@lst.de
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Cc: Andrey Konovalov <andreyknvl@google.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: David Miller <davem@davemloft.net>
	Cc: James Hogan <jhogan@kernel.org>
	Cc: Jason Gunthorpe <jgg@mellanox.com>
	Cc: Khalid Aziz <khalid.aziz@oracle.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Nicholas Piggin <npiggin@gmail.com>
	Cc: Paul Burton <paul.burton@mips.com>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit cbd34da7dc9afd521e0bea5e7d12701f4a9da7c7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/Kconfig
#	arch/powerpc/mm/hugetlbpage.c
#	mm/Kconfig
diff --cc arch/powerpc/Kconfig
index f95f7924e00e,24a41f919309..000000000000
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@@ -123,7 -124,8 +123,12 @@@ config PP
  	select ARCH_HAS_ELF_RANDOMIZE
  	select ARCH_HAS_FORTIFY_SOURCE
  	select ARCH_HAS_GCOV_PROFILE_ALL
++<<<<<<< HEAD
 +	select ARCH_HAS_MEMREMAP_COMPAT_ALIGN
++=======
+ 	select ARCH_HAS_KCOV
+ 	select ARCH_HAS_HUGEPD			if HUGETLB_PAGE
++>>>>>>> cbd34da7dc9a (mm: move the powerpc hugepd code to mm/gup.c)
  	select ARCH_HAS_MMIOWB			if PPC64
  	select ARCH_HAS_PHYS_TO_DMA
  	select ARCH_HAS_PMEM_API                if PPC64
diff --cc arch/powerpc/mm/hugetlbpage.c
index 2ee76f7b28cf,51716c11d0fb..000000000000
--- a/arch/powerpc/mm/hugetlbpage.c
+++ b/arch/powerpc/mm/hugetlbpage.c
@@@ -533,30 -511,6 +533,33 @@@ retry
  	return page;
  }
  
++<<<<<<< HEAD
 +static unsigned long hugepte_addr_end(unsigned long addr, unsigned long end,
 +				      unsigned long sz)
 +{
 +	unsigned long __boundary = (addr + sz) & ~(sz-1);
 +	return (__boundary - 1 < end - 1) ? __boundary : end;
 +}
 +
 +int gup_huge_pd(hugepd_t hugepd, unsigned long addr, unsigned pdshift,
 +		unsigned long end, int write, struct page **pages, int *nr)
 +{
 +	pte_t *ptep;
 +	unsigned long sz = 1UL << hugepd_shift(hugepd);
 +	unsigned long next;
 +
 +	ptep = hugepte_offset(hugepd, addr, pdshift);
 +	do {
 +		next = hugepte_addr_end(addr, end, sz);
 +		if (!gup_hugepte(ptep, sz, addr, end, write, pages, nr))
 +			return 0;
 +	} while (ptep++, addr = next, addr != end);
 +
 +	return 1;
 +}
 +
++=======
++>>>>>>> cbd34da7dc9a (mm: move the powerpc hugepd code to mm/gup.c)
  #ifdef CONFIG_PPC_MM_SLICES
  unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
  					unsigned long len, unsigned long pgoff,
@@@ -755,152 -658,3 +758,155 @@@ void flush_dcache_icache_hugepage(struc
  		}
  	}
  }
++<<<<<<< HEAD
 +
 +#endif /* CONFIG_HUGETLB_PAGE */
 +
 +/*
 + * We have 4 cases for pgds and pmds:
 + * (1) invalid (all zeroes)
 + * (2) pointer to next table, as normal; bottom 6 bits == 0
 + * (3) leaf pte for huge page _PAGE_PTE set
 + * (4) hugepd pointer, _PAGE_PTE = 0 and bits [2..6] indicate size of table
 + *
 + * So long as we atomically load page table pointers we are safe against teardown,
 + * we can follow the address down to the the page and take a ref on it.
 + * This function need to be called with interrupts disabled. We use this variant
 + * when we have MSR[EE] = 0 but the paca->irq_soft_mask = IRQS_ENABLED
 + */
 +pte_t *__find_linux_pte(pgd_t *pgdir, unsigned long ea,
 +			bool *is_thp, unsigned *hpage_shift)
 +{
 +	pgd_t pgd, *pgdp;
 +	pud_t pud, *pudp;
 +	pmd_t pmd, *pmdp;
 +	pte_t *ret_pte;
 +	hugepd_t *hpdp = NULL;
 +	unsigned pdshift = PGDIR_SHIFT;
 +
 +	if (hpage_shift)
 +		*hpage_shift = 0;
 +
 +	if (is_thp)
 +		*is_thp = false;
 +
 +	pgdp = pgdir + pgd_index(ea);
 +	pgd  = READ_ONCE(*pgdp);
 +	/*
 +	 * Always operate on the local stack value. This make sure the
 +	 * value don't get updated by a parallel THP split/collapse,
 +	 * page fault or a page unmap. The return pte_t * is still not
 +	 * stable. So should be checked there for above conditions.
 +	 */
 +	if (pgd_none(pgd))
 +		return NULL;
 +	else if (pgd_huge(pgd)) {
 +		ret_pte = (pte_t *) pgdp;
 +		goto out;
 +	} else if (is_hugepd(__hugepd(pgd_val(pgd))))
 +		hpdp = (hugepd_t *)&pgd;
 +	else {
 +		/*
 +		 * Even if we end up with an unmap, the pgtable will not
 +		 * be freed, because we do an rcu free and here we are
 +		 * irq disabled
 +		 */
 +		pdshift = PUD_SHIFT;
 +		pudp = pud_offset(&pgd, ea);
 +		pud  = READ_ONCE(*pudp);
 +
 +		if (pud_none(pud))
 +			return NULL;
 +		else if (pud_huge(pud)) {
 +			ret_pte = (pte_t *) pudp;
 +			goto out;
 +		} else if (is_hugepd(__hugepd(pud_val(pud))))
 +			hpdp = (hugepd_t *)&pud;
 +		else {
 +			pdshift = PMD_SHIFT;
 +			pmdp = pmd_offset(&pud, ea);
 +			pmd  = READ_ONCE(*pmdp);
 +			/*
 +			 * A hugepage collapse is captured by pmd_none, because
 +			 * it mark the pmd none and do a hpte invalidate.
 +			 */
 +			if (pmd_none(pmd))
 +				return NULL;
 +
 +			if (pmd_trans_huge(pmd) || pmd_devmap(pmd)) {
 +				if (is_thp)
 +					*is_thp = true;
 +				ret_pte = (pte_t *) pmdp;
 +				goto out;
 +			}
 +
 +			if (pmd_huge(pmd)) {
 +				ret_pte = (pte_t *) pmdp;
 +				goto out;
 +			} else if (is_hugepd(__hugepd(pmd_val(pmd))))
 +				hpdp = (hugepd_t *)&pmd;
 +			else
 +				return pte_offset_kernel(&pmd, ea);
 +		}
 +	}
 +	if (!hpdp)
 +		return NULL;
 +
 +	ret_pte = hugepte_offset(*hpdp, ea, pdshift);
 +	pdshift = hugepd_shift(*hpdp);
 +out:
 +	if (hpage_shift)
 +		*hpage_shift = pdshift;
 +	return ret_pte;
 +}
 +EXPORT_SYMBOL_GPL(__find_linux_pte);
 +
 +int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,
 +		unsigned long end, int write, struct page **pages, int *nr)
 +{
 +	unsigned long pte_end;
 +	struct page *head, *page;
 +	pte_t pte;
 +	int refs;
 +
 +	pte_end = (addr + sz) & ~(sz-1);
 +	if (pte_end < end)
 +		end = pte_end;
 +
 +	pte = READ_ONCE(*ptep);
 +
 +	if (!pte_access_permitted(pte, write))
 +		return 0;
 +
 +	/* hugepages are never "special" */
 +	VM_BUG_ON(!pfn_valid(pte_pfn(pte)));
 +
 +	refs = 0;
 +	head = pte_page(pte);
 +
 +	page = head + ((addr & (sz-1)) >> PAGE_SHIFT);
 +	do {
 +		VM_BUG_ON(compound_head(page) != head);
 +		pages[*nr] = page;
 +		(*nr)++;
 +		page++;
 +		refs++;
 +	} while (addr += PAGE_SIZE, addr != end);
 +
 +	if (!page_cache_add_speculative(head, refs)) {
 +		*nr -= refs;
 +		return 0;
 +	}
 +
 +	if (unlikely(pte_val(pte) != pte_val(*ptep))) {
 +		/* Could be optimized better */
 +		*nr -= refs;
 +		while (refs--)
 +			put_page(head);
 +		return 0;
 +	}
 +
 +	return 1;
 +}
++=======
++>>>>>>> cbd34da7dc9a (mm: move the powerpc hugepd code to mm/gup.c)
diff --cc mm/Kconfig
index ba2bdef1230d,0b4352557dd5..000000000000
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@@ -744,5 -766,17 +744,19 @@@ config GUP_BENCHMAR
  config ARCH_HAS_PTE_SPECIAL
  	bool
  
++<<<<<<< HEAD
 +config MAPPING_DIRTY_HELPERS
 +        bool
++=======
+ #
+ # Some architectures require a special hugepage directory format that is
+ # required to support multiple hugepage sizes. For example a4fe3ce76
+ # "powerpc/mm: Allow more flexible layouts for hugepage pagetables"
+ # introduced it on powerpc.  This allows for a more flexible hugepage
+ # pagetable layouts.
+ #
+ config ARCH_HAS_HUGEPD
+ 	bool
+ 
+ endmenu
++>>>>>>> cbd34da7dc9a (mm: move the powerpc hugepd code to mm/gup.c)
* Unmerged path arch/powerpc/Kconfig
* Unmerged path arch/powerpc/mm/hugetlbpage.c
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 0d2e735b77d3..33ccc59197dd 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -16,29 +16,11 @@ struct user_struct;
 struct mmu_gather;
 
 #ifndef is_hugepd
-/*
- * Some architectures requires a hugepage directory format that is
- * required to support multiple hugepage sizes. For example
- * a4fe3ce76 "powerpc/mm: Allow more flexible layouts for hugepage pagetables"
- * introduced the same on powerpc. This allows for a more flexible hugepage
- * pagetable layout.
- */
 typedef struct { unsigned long pd; } hugepd_t;
 #define is_hugepd(hugepd) (0)
 #define __hugepd(x) ((hugepd_t) { (x) })
-static inline int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
-			      unsigned pdshift, unsigned long end,
-			      int write, struct page **pages, int *nr)
-{
-	return 0;
-}
-#else
-extern int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
-		       unsigned pdshift, unsigned long end,
-		       int write, struct page **pages, int *nr);
 #endif
 
-
 #ifdef CONFIG_HUGETLB_PAGE
 
 #include <linux/mempolicy.h>
* Unmerged path mm/Kconfig
diff --git a/mm/gup.c b/mm/gup.c
index f1c484f79152..ec5950c4a217 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1883,6 +1883,88 @@ static int __gup_device_huge_pud(pud_t pud, pud_t *pudp, unsigned long addr,
 }
 #endif
 
+#ifdef CONFIG_ARCH_HAS_HUGEPD
+static unsigned long hugepte_addr_end(unsigned long addr, unsigned long end,
+				      unsigned long sz)
+{
+	unsigned long __boundary = (addr + sz) & ~(sz-1);
+	return (__boundary - 1 < end - 1) ? __boundary : end;
+}
+
+static int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,
+		       unsigned long end, int write, struct page **pages, int *nr)
+{
+	unsigned long pte_end;
+	struct page *head, *page;
+	pte_t pte;
+	int refs;
+
+	pte_end = (addr + sz) & ~(sz-1);
+	if (pte_end < end)
+		end = pte_end;
+
+	pte = READ_ONCE(*ptep);
+
+	if (!pte_access_permitted(pte, write))
+		return 0;
+
+	/* hugepages are never "special" */
+	VM_BUG_ON(!pfn_valid(pte_pfn(pte)));
+
+	refs = 0;
+	head = pte_page(pte);
+
+	page = head + ((addr & (sz-1)) >> PAGE_SHIFT);
+	do {
+		VM_BUG_ON(compound_head(page) != head);
+		pages[*nr] = page;
+		(*nr)++;
+		page++;
+		refs++;
+	} while (addr += PAGE_SIZE, addr != end);
+
+	if (!page_cache_add_speculative(head, refs)) {
+		*nr -= refs;
+		return 0;
+	}
+
+	if (unlikely(pte_val(pte) != pte_val(*ptep))) {
+		/* Could be optimized better */
+		*nr -= refs;
+		while (refs--)
+			put_page(head);
+		return 0;
+	}
+
+	return 1;
+}
+
+static int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
+		unsigned int pdshift, unsigned long end, int write,
+		struct page **pages, int *nr)
+{
+	pte_t *ptep;
+	unsigned long sz = 1UL << hugepd_shift(hugepd);
+	unsigned long next;
+
+	ptep = hugepte_offset(hugepd, addr, pdshift);
+	do {
+		next = hugepte_addr_end(addr, end, sz);
+		if (!gup_hugepte(ptep, sz, addr, end, write, pages, nr))
+			return 0;
+	} while (ptep++, addr = next, addr != end);
+
+	return 1;
+}
+#else
+static inline int gup_huge_pd(hugepd_t hugepd, unsigned long addr,
+		unsigned pdshift, unsigned long end, int write,
+		struct page **pages, int *nr)
+{
+	return 0;
+}
+#endif /* CONFIG_ARCH_HAS_HUGEPD */
+
 static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
 		unsigned long end, unsigned int flags, struct page **pages, int *nr)
 {
