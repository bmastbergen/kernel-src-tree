RDMA/mlx5: Group all create QP parameters to simplify in-kernel interfaces

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit f78d358cec9088ed77b5129c44f858cdfdb1e8c9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f78d358c.failed

The amount of parameters passed in and out between internal mlx5
create QP functions is too large to easily follow the flow. Change
it by grouping all create QP parameter into one structure.

Link: https://lore.kernel.org/r/20200427154636.381474-31-leon@kernel.org
	Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit f78d358cec9088ed77b5129c44f858cdfdb1e8c9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 08f1eef60c1f,3807e1687cb2..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -1632,11 -1610,22 +1632,29 @@@ static void destroy_rss_raw_qp_tir(stru
  			     to_mpd(qp->ibqp.pd)->uid);
  }
  
++<<<<<<< HEAD
 +static int create_rss_raw_qp_tir(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 +				 struct ib_pd *pd,
 +				 struct ib_qp_init_attr *init_attr,
 +				 struct ib_udata *udata)
++=======
+ struct mlx5_create_qp_params {
+ 	struct ib_udata *udata;
+ 	size_t inlen;
+ 	void *ucmd;
+ 	u8 is_rss_raw : 1;
+ 	struct ib_qp_init_attr *attr;
+ 	u32 uidx;
+ };
+ 
+ static int create_rss_raw_qp_tir(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 				 struct mlx5_ib_qp *qp,
+ 				 struct mlx5_create_qp_params *params)
++>>>>>>> f78d358cec90 (RDMA/mlx5: Group all create QP parameters to simplify in-kernel interfaces)
  {
+ 	struct ib_qp_init_attr *init_attr = params->attr;
+ 	struct mlx5_ib_create_qp_rss *ucmd = params->ucmd;
+ 	struct ib_udata *udata = params->udata;
  	struct mlx5_ib_ucontext *mucontext = rdma_udata_to_drv_context(
  		udata, struct mlx5_ib_ucontext, ibucontext);
  	struct mlx5_ib_create_qp_resp resp = {};
@@@ -1651,17 -1640,10 +1669,22 @@@
  	u32 outer_l4;
  	size_t min_resp_len;
  	u32 tdn = mucontext->tdn;
 +	struct mlx5_ib_create_qp_rss ucmd = {};
 +	size_t required_cmd_sz;
  	u8 lb_flag = 0;
  
++<<<<<<< HEAD
 +	if (init_attr->qp_type != IB_QPT_RAW_PACKET)
 +		return -EOPNOTSUPP;
 +
 +	if (init_attr->create_flags || init_attr->send_cq)
 +		return -EINVAL;
 +
 +	min_resp_len = offsetof(typeof(resp), bfreg_index) + sizeof(resp.bfreg_index);
++=======
+ 	min_resp_len =
+ 		offsetof(typeof(resp), bfreg_index) + sizeof(resp.bfreg_index);
++>>>>>>> f78d358cec90 (RDMA/mlx5: Group all create QP parameters to simplify in-kernel interfaces)
  	if (udata->outlen < min_resp_len)
  		return -EINVAL;
  
@@@ -1960,15 -1920,90 +1983,101 @@@ static int get_atomic_mode(struct mlx5_
  	return atomic_mode;
  }
  
++<<<<<<< HEAD
 +static inline bool check_flags_mask(uint64_t input, uint64_t supported)
 +{
 +	return (input & ~supported) == 0;
 +}
 +
 +static int create_qp_common(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata, struct mlx5_ib_qp *qp)
++=======
+ static int create_xrc_tgt_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
+ 			     struct mlx5_create_qp_params *params)
  {
+ 	struct ib_qp_init_attr *attr = params->attr;
+ 	struct ib_udata *udata = params->udata;
+ 	u32 uidx = params->uidx;
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_qp_base *base;
+ 	unsigned long flags;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	mutex_init(&qp->mutex);
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	in = kvzalloc(inlen, GFP_KERNEL);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, MLX5_QP_ST_XRC);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 	MLX5_SET(qpc, qpc, pd, to_mpd(devr->p0)->pdn);
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 	if (qp->flags & IB_QP_CREATE_CROSS_CHANNEL)
+ 		MLX5_SET(qpc, qpc, cd_master, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_SEND)
+ 		MLX5_SET(qpc, qpc, cd_slave_send, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_RECV)
+ 		MLX5_SET(qpc, qpc, cd_slave_receive, 1);
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, MLX5_SRQ_RQ);
+ 	MLX5_SET(qpc, qpc, no_sq, 1);
+ 	MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, cqn_snd, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, srqn_rmpn_xrqn, to_msrq(devr->s0)->msrq.srqn);
+ 	MLX5_SET(qpc, qpc, xrcd, to_mxrcd(attr->xrcd)->xrcdn);
+ 	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
+ 
+ 	/* 0xffffff means we ask to work with cqe version 0 */
+ 	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
+ 		MLX5_SET(qpc, qpc, user_index, uidx);
+ 
+ 	if (qp->flags & IB_QP_CREATE_PCI_WRITE_END_PADDING) {
+ 		MLX5_SET(qpc, qpc, end_padding_mode,
+ 			 MLX5_WQ_END_PAD_MODE_ALIGN);
+ 		/* Special case to clean flag */
+ 		qp->flags &= ~IB_QP_CREATE_PCI_WRITE_END_PADDING;
+ 	}
+ 
+ 	base = &qp->trans_qp.base;
+ 	err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
+ 	kvfree(in);
+ 	if (err) {
+ 		destroy_qp(dev, qp, base, udata);
+ 		return err;
+ 	}
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	return 0;
+ }
+ 
+ static int create_user_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 			  struct mlx5_ib_qp *qp,
+ 			  struct mlx5_create_qp_params *params)
++>>>>>>> f78d358cec90 (RDMA/mlx5: Group all create QP parameters to simplify in-kernel interfaces)
+ {
+ 	struct ib_qp_init_attr *init_attr = params->attr;
+ 	struct mlx5_ib_create_qp *ucmd = params->ucmd;
+ 	struct ib_udata *udata = params->udata;
+ 	u32 uidx = params->uidx;
  	struct mlx5_ib_resources *devr = &dev->devr;
  	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
  	struct mlx5_core_dev *mdev = dev->mdev;
@@@ -2310,16 -2152,141 +2419,128 @@@
  		raw_packet_qp_copy_info(qp, &qp->raw_packet_qp);
  		err = create_raw_packet_qp(dev, qp, in, inlen, pd, udata,
  					   &resp);
++<<<<<<< HEAD
++=======
+ 	} else
+ 		err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
+ 
+ 	kvfree(in);
+ 	if (err)
+ 		goto err_create;
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 
+ 	get_cqs(qp->type, init_attr->send_cq, init_attr->recv_cq,
+ 		&send_cq, &recv_cq);
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	mlx5_ib_lock_cqs(send_cq, recv_cq);
+ 	/* Maintain device to QPs access, needed for further handling via reset
+ 	 * flow
+ 	 */
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	/* Maintain CQ to QPs access, needed for further handling via reset flow
+ 	 */
+ 	if (send_cq)
+ 		list_add_tail(&qp->cq_send_list, &send_cq->list_send_qp);
+ 	if (recv_cq)
+ 		list_add_tail(&qp->cq_recv_list, &recv_cq->list_recv_qp);
+ 	mlx5_ib_unlock_cqs(send_cq, recv_cq);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	return 0;
+ 
+ err_create:
+ 	destroy_qp(dev, qp, base, udata);
+ 	return err;
+ }
+ 
+ static int create_kernel_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 			    struct mlx5_ib_qp *qp,
+ 			    struct mlx5_create_qp_params *params)
+ {
+ 	struct ib_qp_init_attr *attr = params->attr;
+ 	u32 uidx = params->uidx;
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_cq *send_cq;
+ 	struct mlx5_ib_cq *recv_cq;
+ 	unsigned long flags;
+ 	struct mlx5_ib_qp_base *base;
+ 	int mlx5_st;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	mutex_init(&qp->mutex);
+ 	spin_lock_init(&qp->sq.lock);
+ 	spin_lock_init(&qp->rq.lock);
+ 
+ 	mlx5_st = to_mlx5_st(qp->type);
+ 	if (mlx5_st < 0)
+ 		return -EINVAL;
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	base = &qp->trans_qp.base;
+ 
+ 	qp->has_rq = qp_has_rq(attr);
+ 	err = set_rq_size(dev, &attr->cap, qp->has_rq, qp, NULL);
+ 	if (err) {
+ 		mlx5_ib_dbg(dev, "err %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	err = _create_kernel_qp(dev, attr, qp, &in, &inlen, base);
+ 	if (err)
+ 		return err;
+ 
+ 	if (is_sqp(attr->qp_type))
+ 		qp->port = attr->port_num;
+ 
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, mlx5_st);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 
+ 	if (attr->qp_type != MLX5_IB_QPT_REG_UMR)
+ 		MLX5_SET(qpc, qpc, pd, to_mpd(pd ? pd : devr->p0)->pdn);
+ 	else
+ 		MLX5_SET(qpc, qpc, latency_sensitive, 1);
+ 
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 
+ 	if (qp->rq.wqe_cnt) {
+ 		MLX5_SET(qpc, qpc, log_rq_stride, qp->rq.wqe_shift - 4);
+ 		MLX5_SET(qpc, qpc, log_rq_size, ilog2(qp->rq.wqe_cnt));
+ 	}
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, get_rx_type(qp, attr));
+ 
+ 	if (qp->sq.wqe_cnt)
+ 		MLX5_SET(qpc, qpc, log_sq_size, ilog2(qp->sq.wqe_cnt));
+ 	else
+ 		MLX5_SET(qpc, qpc, no_sq, 1);
+ 
+ 	if (attr->srq) {
+ 		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x0)->xrcdn);
+ 		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
+ 			 to_msrq(attr->srq)->msrq.srqn);
++>>>>>>> f78d358cec90 (RDMA/mlx5: Group all create QP parameters to simplify in-kernel interfaces)
  	} else {
 -		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x1)->xrcdn);
 -		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
 -			 to_msrq(devr->s1)->msrq.srqn);
 +		err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
  	}
  
 -	if (attr->send_cq)
 -		MLX5_SET(qpc, qpc, cqn_snd, to_mcq(attr->send_cq)->mcq.cqn);
 -
 -	if (attr->recv_cq)
 -		MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(attr->recv_cq)->mcq.cqn);
 -
 -	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
 -
 -	/* 0xffffff means we ask to work with cqe version 0 */
 -	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
 -		MLX5_SET(qpc, qpc, user_index, uidx);
 -
 -	/* we use IB_QP_CREATE_IPOIB_UD_LSO to indicates ipoib qp */
 -	if (qp->flags & IB_QP_CREATE_IPOIB_UD_LSO)
 -		MLX5_SET(qpc, qpc, ulp_stateless_offload_mode, 1);
 +	if (err) {
 +		mlx5_ib_dbg(dev, "create qp failed\n");
 +		goto err_create;
 +	}
  
 -	err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
  	kvfree(in);
 -	if (err)
 -		goto err_create;
  
  	base->container_mibqp = qp;
  	base->mqp.event = mlx5_ib_qp_event;
@@@ -2522,73 -2481,20 +2743,82 @@@ static void destroy_qp_common(struct ml
  				     base->mqp.qpn);
  	}
  
 -	destroy_qp(dev, qp, base, udata);
 +	if (udata)
 +		destroy_qp_user(dev, &get_pd(qp)->ibpd, qp, base, udata);
 +	else
 +		destroy_qp_kernel(dev, qp);
 +}
 +
++<<<<<<< HEAD
 +static const char *ib_qp_type_str(enum ib_qp_type type)
 +{
 +	switch (type) {
 +	case IB_QPT_SMI:
 +		return "IB_QPT_SMI";
 +	case IB_QPT_GSI:
 +		return "IB_QPT_GSI";
 +	case IB_QPT_RC:
 +		return "IB_QPT_RC";
 +	case IB_QPT_UC:
 +		return "IB_QPT_UC";
 +	case IB_QPT_UD:
 +		return "IB_QPT_UD";
 +	case IB_QPT_RAW_IPV6:
 +		return "IB_QPT_RAW_IPV6";
 +	case IB_QPT_RAW_ETHERTYPE:
 +		return "IB_QPT_RAW_ETHERTYPE";
 +	case IB_QPT_XRC_INI:
 +		return "IB_QPT_XRC_INI";
 +	case IB_QPT_XRC_TGT:
 +		return "IB_QPT_XRC_TGT";
 +	case IB_QPT_RAW_PACKET:
 +		return "IB_QPT_RAW_PACKET";
 +	case MLX5_IB_QPT_REG_UMR:
 +		return "MLX5_IB_QPT_REG_UMR";
 +	case IB_QPT_DRIVER:
 +		return "IB_QPT_DRIVER";
 +	case IB_QPT_MAX:
 +	default:
 +		return "Invalid QP type";
 +	}
  }
  
 +static struct ib_qp *mlx5_ib_create_dct(struct ib_pd *pd,
 +					struct ib_qp_init_attr *attr,
 +					struct mlx5_ib_create_qp *ucmd,
 +					struct ib_udata *udata)
 +{
 +	struct mlx5_ib_ucontext *ucontext = rdma_udata_to_drv_context(
 +		udata, struct mlx5_ib_ucontext, ibucontext);
 +	struct mlx5_ib_qp *qp;
 +	int err = 0;
 +	u32 uidx = MLX5_IB_DEFAULT_UIDX;
++=======
+ static int create_dct(struct ib_pd *pd, struct mlx5_ib_qp *qp,
+ 		      struct mlx5_create_qp_params *params)
+ {
+ 	struct ib_qp_init_attr *attr = params->attr;
+ 	struct mlx5_ib_create_qp *ucmd = params->ucmd;
+ 	u32 uidx = params->uidx;
++>>>>>>> f78d358cec90 (RDMA/mlx5: Group all create QP parameters to simplify in-kernel interfaces)
  	void *dctc;
  
 +	if (!attr->srq || !attr->recv_cq)
 +		return ERR_PTR(-EINVAL);
 +
 +	err = get_qp_user_index(ucontext, ucmd, sizeof(*ucmd), &uidx);
 +	if (err)
 +		return ERR_PTR(err);
 +
 +	qp = kzalloc(sizeof(*qp), GFP_KERNEL);
 +	if (!qp)
 +		return ERR_PTR(-ENOMEM);
 +
  	qp->dct.in = kzalloc(MLX5_ST_SZ_BYTES(create_dct_in), GFP_KERNEL);
 -	if (!qp->dct.in)
 -		return -ENOMEM;
 +	if (!qp->dct.in) {
 +		err = -ENOMEM;
 +		goto err_free;
 +	}
  
  	MLX5_SET(create_dct_in, qp->dct.in, uid, to_mpd(pd)->uid);
  	dctc = MLX5_ADDR_OF(create_dct_in, qp->dct.in, dct_context_entry);
@@@ -2729,51 -2535,429 +2959,477 @@@ struct ib_qp *mlx5_ib_create_qp(struct 
  	case IB_QPT_SMI:
  	case MLX5_IB_QPT_HW_GSI:
  	case MLX5_IB_QPT_REG_UMR:
++<<<<<<< HEAD
 +	case MLX5_IB_QPT_DCI:
 +		qp = kzalloc(sizeof(*qp), GFP_KERNEL);
 +		if (!qp)
 +			return ERR_PTR(-ENOMEM);
 +
 +		err = create_qp_common(dev, pd, init_attr, udata, qp);
 +		if (err) {
 +			mlx5_ib_dbg(dev, "create_qp_common failed\n");
 +			kfree(qp);
 +			return ERR_PTR(err);
 +		}
 +
 +		if (is_qp0(init_attr->qp_type))
 +			qp->ibqp.qp_num = 0;
 +		else if (is_qp1(init_attr->qp_type))
 +			qp->ibqp.qp_num = 1;
 +		else
 +			qp->ibqp.qp_num = qp->trans_qp.base.mqp.qpn;
 +
 +		mlx5_ib_dbg(dev, "ib qpnum 0x%x, mlx qpn 0x%x, rcqn 0x%x, scqn 0x%x\n",
 +			    qp->ibqp.qp_num, qp->trans_qp.base.mqp.qpn,
 +			    init_attr->recv_cq ? to_mcq(init_attr->recv_cq)->mcq.cqn : -1,
 +			    init_attr->send_cq ? to_mcq(init_attr->send_cq)->mcq.cqn : -1);
 +
 +		qp->trans_qp.xrcdn = xrcdn;
 +
 +		break;
 +
 +	case IB_QPT_GSI:
 +		return mlx5_ib_gsi_create_qp(pd, init_attr);
 +
 +	case IB_QPT_RAW_IPV6:
 +	case IB_QPT_RAW_ETHERTYPE:
 +	case IB_QPT_MAX:
 +	default:
 +		mlx5_ib_dbg(dev, "unsupported qp type %d\n",
 +			    init_attr->qp_type);
 +		/* Don't support raw QPs */
 +		return ERR_PTR(-EINVAL);
 +	}
 +
 +	if (verbs_init_attr->qp_type == IB_QPT_DRIVER)
 +		qp->qp_sub_type = init_attr->qp_type;
 +
 +	return &qp->ibqp;
++=======
+ 	case IB_QPT_DRIVER:
+ 	case IB_QPT_GSI:
+ 		break;
+ 	default:
+ 		goto out;
+ 	}
+ 
+ 	*type = attr->qp_type;
+ 	return 0;
+ 
+ out:
+ 	mlx5_ib_dbg(dev, "Unsupported QP type %d\n", attr->qp_type);
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static int check_valid_flow(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 			    struct ib_qp_init_attr *attr,
+ 			    struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_ucontext *ucontext = rdma_udata_to_drv_context(
+ 		udata, struct mlx5_ib_ucontext, ibucontext);
+ 
+ 	if (!udata) {
+ 		/* Kernel create_qp callers */
+ 		if (attr->rwq_ind_tbl)
+ 			return -EOPNOTSUPP;
+ 
+ 		switch (attr->qp_type) {
+ 		case IB_QPT_RAW_PACKET:
+ 		case IB_QPT_DRIVER:
+ 			return -EOPNOTSUPP;
+ 		default:
+ 			return 0;
+ 		}
+ 	}
+ 
+ 	/* Userspace create_qp callers */
+ 	if (attr->qp_type == IB_QPT_RAW_PACKET && !ucontext->cqe_version) {
+ 		mlx5_ib_dbg(dev,
+ 			"Raw Packet QP is only supported for CQE version > 0\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (attr->qp_type != IB_QPT_RAW_PACKET && attr->rwq_ind_tbl) {
+ 		mlx5_ib_dbg(dev,
+ 			    "Wrong QP type %d for the RWQ indirect table\n",
+ 			    attr->qp_type);
+ 		return -EINVAL;
+ 	}
+ 
+ 	switch (attr->qp_type) {
+ 	case IB_QPT_SMI:
+ 	case MLX5_IB_QPT_HW_GSI:
+ 	case MLX5_IB_QPT_REG_UMR:
+ 	case IB_QPT_GSI:
+ 		mlx5_ib_dbg(dev, "Kernel doesn't support QP type %d\n",
+ 			    attr->qp_type);
+ 		return -EINVAL;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * We don't need to see this warning, it means that kernel code
+ 	 * missing ib_pd. Placed here to catch developer's mistakes.
+ 	 */
+ 	WARN_ONCE(!pd && attr->qp_type != IB_QPT_XRC_TGT,
+ 		  "There is a missing PD pointer assignment\n");
+ 	return 0;
+ }
+ 
+ static void process_vendor_flag(struct mlx5_ib_dev *dev, int *flags, int flag,
+ 				bool cond, struct mlx5_ib_qp *qp)
+ {
+ 	if (!(*flags & flag))
+ 		return;
+ 
+ 	if (cond) {
+ 		qp->flags_en |= flag;
+ 		*flags &= ~flag;
+ 		return;
+ 	}
+ 
+ 	if (flag == MLX5_QP_FLAG_SCATTER_CQE) {
+ 		/*
+ 		 * We don't return error if this flag was provided,
+ 		 * and mlx5 doesn't have right capability.
+ 		 */
+ 		*flags &= ~MLX5_QP_FLAG_SCATTER_CQE;
+ 		return;
+ 	}
+ 	mlx5_ib_dbg(dev, "Vendor create QP flag 0x%X is not supported\n", flag);
+ }
+ 
+ static int process_vendor_flags(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
+ 				void *ucmd, struct ib_qp_init_attr *attr)
+ {
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	bool cond;
+ 	int flags;
+ 
+ 	if (attr->rwq_ind_tbl)
+ 		flags = ((struct mlx5_ib_create_qp_rss *)ucmd)->flags;
+ 	else
+ 		flags = ((struct mlx5_ib_create_qp *)ucmd)->flags;
+ 
+ 	switch (flags & (MLX5_QP_FLAG_TYPE_DCT | MLX5_QP_FLAG_TYPE_DCI)) {
+ 	case MLX5_QP_FLAG_TYPE_DCI:
+ 		qp->type = MLX5_IB_QPT_DCI;
+ 		break;
+ 	case MLX5_QP_FLAG_TYPE_DCT:
+ 		qp->type = MLX5_IB_QPT_DCT;
+ 		break;
+ 	default:
+ 		if (qp->type != IB_QPT_DRIVER)
+ 			break;
+ 		/*
+ 		 * It is IB_QPT_DRIVER and or no subtype or
+ 		 * wrong subtype were provided.
+ 		 */
+ 		return -EINVAL;
+ 	}
+ 
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_TYPE_DCI, true, qp);
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_TYPE_DCT, true, qp);
+ 
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_SIGNATURE, true, qp);
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_SCATTER_CQE,
+ 			    MLX5_CAP_GEN(mdev, sctr_data_cqe), qp);
+ 
+ 	if (qp->type == IB_QPT_RAW_PACKET) {
+ 		cond = MLX5_CAP_ETH(mdev, tunnel_stateless_vxlan) ||
+ 		       MLX5_CAP_ETH(mdev, tunnel_stateless_gre) ||
+ 		       MLX5_CAP_ETH(mdev, tunnel_stateless_geneve_rx);
+ 		process_vendor_flag(dev, &flags, MLX5_QP_FLAG_TUNNEL_OFFLOADS,
+ 				    cond, qp);
+ 		process_vendor_flag(dev, &flags,
+ 				    MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_UC, true,
+ 				    qp);
+ 		process_vendor_flag(dev, &flags,
+ 				    MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_MC, true,
+ 				    qp);
+ 	}
+ 
+ 	if (qp->type == IB_QPT_RC)
+ 		process_vendor_flag(dev, &flags,
+ 				    MLX5_QP_FLAG_PACKET_BASED_CREDIT_MODE,
+ 				    MLX5_CAP_GEN(mdev, qp_packet_based), qp);
+ 
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_BFREG_INDEX, true, qp);
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_UAR_PAGE_INDEX, true, qp);
+ 
+ 	if (flags)
+ 		mlx5_ib_dbg(dev, "udata has unsupported flags 0x%X\n", flags);
+ 
+ 	return (flags) ? -EINVAL : 0;
+ }
+ 
+ static void process_create_flag(struct mlx5_ib_dev *dev, int *flags, int flag,
+ 				bool cond, struct mlx5_ib_qp *qp)
+ {
+ 	if (!(*flags & flag))
+ 		return;
+ 
+ 	if (cond) {
+ 		qp->flags |= flag;
+ 		*flags &= ~flag;
+ 		return;
+ 	}
+ 
+ 	if (flag == MLX5_IB_QP_CREATE_WC_TEST) {
+ 		/*
+ 		 * Special case, if condition didn't meet, it won't be error,
+ 		 * just different in-kernel flow.
+ 		 */
+ 		*flags &= ~MLX5_IB_QP_CREATE_WC_TEST;
+ 		return;
+ 	}
+ 	mlx5_ib_dbg(dev, "Verbs create QP flag 0x%X is not supported\n", flag);
+ }
+ 
+ static int process_create_flags(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
+ 				struct ib_qp_init_attr *attr)
+ {
+ 	enum ib_qp_type qp_type = qp->type;
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	int create_flags = attr->create_flags;
+ 	bool cond;
+ 
+ 	if (qp_type == MLX5_IB_QPT_DCT)
+ 		return (create_flags) ? -EINVAL : 0;
+ 
+ 	if (qp_type == IB_QPT_RAW_PACKET && attr->rwq_ind_tbl)
+ 		return (create_flags) ? -EINVAL : 0;
+ 
+ 	process_create_flag(dev, &create_flags,
+ 			    IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK,
+ 			    MLX5_CAP_GEN(mdev, block_lb_mc), qp);
+ 	process_create_flag(dev, &create_flags, IB_QP_CREATE_CROSS_CHANNEL,
+ 			    MLX5_CAP_GEN(mdev, cd), qp);
+ 	process_create_flag(dev, &create_flags, IB_QP_CREATE_MANAGED_SEND,
+ 			    MLX5_CAP_GEN(mdev, cd), qp);
+ 	process_create_flag(dev, &create_flags, IB_QP_CREATE_MANAGED_RECV,
+ 			    MLX5_CAP_GEN(mdev, cd), qp);
+ 
+ 	if (qp_type == IB_QPT_UD) {
+ 		process_create_flag(dev, &create_flags,
+ 				    IB_QP_CREATE_IPOIB_UD_LSO,
+ 				    MLX5_CAP_GEN(mdev, ipoib_basic_offloads),
+ 				    qp);
+ 		cond = MLX5_CAP_GEN(mdev, port_type) == MLX5_CAP_PORT_TYPE_IB;
+ 		process_create_flag(dev, &create_flags, IB_QP_CREATE_SOURCE_QPN,
+ 				    cond, qp);
+ 	}
+ 
+ 	if (qp_type == IB_QPT_RAW_PACKET) {
+ 		cond = MLX5_CAP_GEN(mdev, eth_net_offloads) &&
+ 		       MLX5_CAP_ETH(mdev, scatter_fcs);
+ 		process_create_flag(dev, &create_flags,
+ 				    IB_QP_CREATE_SCATTER_FCS, cond, qp);
+ 
+ 		cond = MLX5_CAP_GEN(mdev, eth_net_offloads) &&
+ 		       MLX5_CAP_ETH(mdev, vlan_cap);
+ 		process_create_flag(dev, &create_flags,
+ 				    IB_QP_CREATE_CVLAN_STRIPPING, cond, qp);
+ 	}
+ 
+ 	process_create_flag(dev, &create_flags,
+ 			    IB_QP_CREATE_PCI_WRITE_END_PADDING,
+ 			    MLX5_CAP_GEN(mdev, end_pad), qp);
+ 
+ 	process_create_flag(dev, &create_flags, MLX5_IB_QP_CREATE_WC_TEST,
+ 			    qp_type != MLX5_IB_QPT_REG_UMR, qp);
+ 	process_create_flag(dev, &create_flags, MLX5_IB_QP_CREATE_SQPN_QP1,
+ 			    true, qp);
+ 
+ 	if (create_flags)
+ 		mlx5_ib_dbg(dev, "Create QP has unsupported flags 0x%X\n",
+ 			    create_flags);
+ 
+ 	return (create_flags) ? -EINVAL : 0;
+ }
+ 
+ static size_t process_udata_size(struct ib_qp_init_attr *attr,
+ 				 struct ib_udata *udata)
+ {
+ 	size_t ucmd = sizeof(struct mlx5_ib_create_qp);
+ 	size_t inlen = udata->inlen;
+ 
+ 	if (attr->qp_type == IB_QPT_DRIVER)
+ 		return (inlen < ucmd) ? 0 : ucmd;
+ 
+ 	if (!attr->rwq_ind_tbl)
+ 		return ucmd;
+ 
+ 	if (inlen < offsetofend(struct mlx5_ib_create_qp_rss, flags))
+ 		return 0;
+ 
+ 	ucmd = sizeof(struct mlx5_ib_create_qp_rss);
+ 	if (inlen > ucmd && !ib_is_udata_cleared(udata, ucmd, inlen - ucmd))
+ 		return 0;
+ 
+ 	return min(ucmd, inlen);
+ }
+ 
+ static int create_raw_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 			 struct mlx5_ib_qp *qp,
+ 			 struct mlx5_create_qp_params *params)
+ {
+ 	if (params->is_rss_raw)
+ 		return create_rss_raw_qp_tir(dev, pd, qp, params);
+ 
+ 	return create_user_qp(dev, pd, qp, params);
+ }
+ 
+ static int check_qp_attr(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
+ 			 struct ib_qp_init_attr *attr)
+ {
+ 	int ret = 0;
+ 
+ 	switch (qp->type) {
+ 	case MLX5_IB_QPT_DCT:
+ 		ret = (!attr->srq || !attr->recv_cq) ? -EINVAL : 0;
+ 		break;
+ 	case MLX5_IB_QPT_DCI:
+ 		ret = (attr->cap.max_recv_wr || attr->cap.max_recv_sge) ?
+ 			      -EINVAL :
+ 			      0;
+ 		break;
+ 	case IB_QPT_RAW_PACKET:
+ 		ret = (attr->rwq_ind_tbl && attr->send_cq) ? -EINVAL : 0;
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	if (ret)
+ 		mlx5_ib_dbg(dev, "QP type %d has wrong attributes\n", qp->type);
+ 
+ 	return ret;
+ }
+ 
+ static int get_qp_uidx(struct mlx5_ib_qp *qp,
+ 		       struct mlx5_create_qp_params *params)
+ {
+ 	struct mlx5_ib_create_qp *ucmd = params->ucmd;
+ 	struct ib_udata *udata = params->udata;
+ 	struct mlx5_ib_ucontext *ucontext = rdma_udata_to_drv_context(
+ 		udata, struct mlx5_ib_ucontext, ibucontext);
+ 
+ 	if (params->is_rss_raw)
+ 		return 0;
+ 
+ 	return get_qp_user_index(ucontext, ucmd, sizeof(*ucmd), &params->uidx);
+ }
+ 
+ struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd, struct ib_qp_init_attr *attr,
+ 				struct ib_udata *udata)
+ {
+ 	struct mlx5_create_qp_params params = {};
+ 	struct mlx5_ib_dev *dev;
+ 	struct mlx5_ib_qp *qp;
+ 	enum ib_qp_type type;
+ 	u16 xrcdn = 0;
+ 	int err;
+ 
+ 	dev = pd ? to_mdev(pd->device) :
+ 		   to_mdev(to_mxrcd(attr->xrcd)->ibxrcd.device);
+ 
+ 	err = check_qp_type(dev, attr, &type);
+ 	if (err)
+ 		return ERR_PTR(err);
+ 
+ 	err = check_valid_flow(dev, pd, attr, udata);
+ 	if (err)
+ 		return ERR_PTR(err);
+ 
+ 	if (attr->qp_type == IB_QPT_GSI)
+ 		return mlx5_ib_gsi_create_qp(pd, attr);
+ 
+ 	params.udata = udata;
+ 	params.uidx = MLX5_IB_DEFAULT_UIDX;
+ 	params.attr = attr;
+ 	params.is_rss_raw = !!attr->rwq_ind_tbl;
+ 
+ 	if (udata) {
+ 		params.inlen = process_udata_size(attr, udata);
+ 		if (!params.inlen)
+ 			return ERR_PTR(-EINVAL);
+ 
+ 		params.ucmd = kzalloc(params.inlen, GFP_KERNEL);
+ 		if (!params.ucmd)
+ 			return ERR_PTR(-ENOMEM);
+ 
+ 		err = ib_copy_from_udata(params.ucmd, udata, params.inlen);
+ 		if (err)
+ 			goto free_ucmd;
+ 	}
+ 
+ 	qp = kzalloc(sizeof(*qp), GFP_KERNEL);
+ 	if (!qp) {
+ 		err = -ENOMEM;
+ 		goto free_ucmd;
+ 	}
+ 
+ 	qp->type = type;
+ 	if (udata) {
+ 		err = process_vendor_flags(dev, qp, params.ucmd, attr);
+ 		if (err)
+ 			goto free_qp;
+ 
+ 		err = get_qp_uidx(qp, &params);
+ 		if (err)
+ 			goto free_qp;
+ 	}
+ 	err = process_create_flags(dev, qp, attr);
+ 	if (err)
+ 		goto free_qp;
+ 
+ 	err = check_qp_attr(dev, qp, attr);
+ 	if (err)
+ 		goto free_qp;
+ 
+ 	switch (qp->type) {
+ 	case IB_QPT_RAW_PACKET:
+ 		err = create_raw_qp(dev, pd, qp, &params);
+ 		break;
+ 	case MLX5_IB_QPT_DCT:
+ 		err = create_dct(pd, qp, &params);
+ 		break;
+ 	case IB_QPT_XRC_TGT:
+ 		xrcdn = to_mxrcd(attr->xrcd)->xrcdn;
+ 		err = create_xrc_tgt_qp(dev, qp, &params);
+ 		break;
+ 	default:
+ 		if (udata)
+ 			err = create_user_qp(dev, pd, qp, &params);
+ 		else
+ 			err = create_kernel_qp(dev, pd, qp, &params);
+ 	}
+ 	if (err) {
+ 		mlx5_ib_err(dev, "create_qp failed %d\n", err);
+ 		goto free_qp;
+ 	}
+ 
+ 	kfree(params.ucmd);
+ 
+ 	if (is_qp0(attr->qp_type))
+ 		qp->ibqp.qp_num = 0;
+ 	else if (is_qp1(attr->qp_type))
+ 		qp->ibqp.qp_num = 1;
+ 	else
+ 		qp->ibqp.qp_num = qp->trans_qp.base.mqp.qpn;
+ 
+ 	qp->trans_qp.xrcdn = xrcdn;
+ 
+ 	return &qp->ibqp;
+ 
+ free_qp:
+ 	kfree(qp);
+ free_ucmd:
+ 	kfree(params.ucmd);
+ 	return ERR_PTR(err);
++>>>>>>> f78d358cec90 (RDMA/mlx5: Group all create QP parameters to simplify in-kernel interfaces)
  }
  
  static int mlx5_ib_destroy_dct(struct mlx5_ib_qp *mqp)
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
