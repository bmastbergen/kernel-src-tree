blk-mq: Relocate hctx_may_queue()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author John Garry <john.garry@huawei.com>
commit a0235d230f3245aa4bd70b514d5effb24be61acd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/a0235d23.failed

blk-mq.h and blk-mq-tag.h include on each other, which is less than ideal.

Locate hctx_may_queue() to blk-mq.h, as it is not really tag specific code.

In this way, we can drop the blk-mq-tag.h include of blk-mq.h

	Signed-off-by: John Garry <john.garry@huawei.com>
	Tested-by: Douglas Gilbert <dgilbert@interlog.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit a0235d230f3245aa4bd70b514d5effb24be61acd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-tag.h
diff --cc block/blk-mq-tag.h
index 8b623c8832c2,7d3e6b333a4a..000000000000
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@@ -79,18 -79,6 +77,21 @@@ static inline void blk_mq_tag_idle(stru
  	__blk_mq_tag_idle(hctx);
  }
  
++<<<<<<< HEAD
 +/*
 + * This helper should only be used for flush request to share tag
 + * with the request cloned from, and both the two requests can't be
 + * in flight at the same time. The caller has to make sure the tag
 + * can't be freed.
 + */
 +static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 +		unsigned int tag, struct request *rq)
 +{
 +	hctx->tags->rqs[tag] = rq;
 +}
 +
++=======
++>>>>>>> a0235d230f32 (blk-mq: Relocate hctx_may_queue())
  static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
  					  unsigned int tag)
  {
* Unmerged path block/blk-mq-tag.h
diff --git a/block/blk-mq.h b/block/blk-mq.h
index ddd814f1bef1..7cb09d18bbb5 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -262,4 +262,36 @@ static inline struct blk_plug *blk_mq_plug(struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * For shared tag users, we track the number of currently active users
+ * and attempt to provide a fair share of the tag depth for each of them.
+ */
+static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
+				  struct sbitmap_queue *bt)
+{
+	unsigned int depth, users;
+
+	if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+		return true;
+	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		return true;
+
+	/*
+	 * Don't try dividing an ant
+	 */
+	if (bt->sb.depth == 1)
+		return true;
+
+	users = atomic_read(&hctx->tags->active_queues);
+	if (!users)
+		return true;
+
+	/*
+	 * Allow at least some tags
+	 */
+	depth = max((bt->sb.depth + users - 1) / users, 4U);
+	return atomic_read(&hctx->nr_active) < depth;
+}
+
+
 #endif
