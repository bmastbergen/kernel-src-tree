mm/vmscan.c: calculate reclaimed slab caches in all reclaim paths

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yafang Shao <laoar.shao@gmail.com>
commit 0308f7cf19c9741837f5b4c8cde14342bba72604
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/0308f7cf.failed

There are six different reclaim paths by now:

 - kswapd reclaim path
 - node reclaim path
 - hibernate preallocate memory reclaim path
 - direct reclaim path
 - memcg reclaim path
 - memcg softlimit reclaim path

The slab caches reclaimed in these paths are only calculated in the
above three paths.

There're some drawbacks if we don't calculate the reclaimed slab caches.

 - The sc->nr_reclaimed isn't correct if there're some slab caches
   relcaimed in this path.

 - The slab caches may be reclaimed thoroughly if there're lots of
   reclaimable slab caches and few page caches.

   Let's take an easy example for this case. If one memcg is full of
   slab caches and the limit of it is 512M, in other words there're
   approximately 512M slab caches in this memcg. Then the limit of the
   memcg is reached and the memcg reclaim begins, and then in this memcg
   reclaim path it will continuesly reclaim the slab caches until the
   sc->priority drops to 0. After this reclaim stops, you will find
   there're few slab caches left, which is less than 20M in my test
   case. While after this patch applied the number is greater than 300M
   and the sc->priority only drops to 3.

Link: http://lkml.kernel.org/r/1561112086-6169-3-git-send-email-laoar.shao@gmail.com
	Signed-off-by: Yafang Shao <laoar.shao@gmail.com>
	Reviewed-by: Kirill Tkhai <ktkhai@virtuozzo.com>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Kirill Tkhai <ktkhai@virtuozzo.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0308f7cf19c9741837f5b4c8cde14342bba72604)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmscan.c
diff --cc mm/vmscan.c
index aedd3e5691d7,88d740db3216..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -3344,12 -3269,16 +3349,17 @@@ unsigned long try_to_free_mem_cgroup_pa
  		.may_swap = may_swap,
  		.may_shrinkslab = 1,
  	};
++<<<<<<< HEAD
++=======
+ 
+ 	current->reclaim_state = &sc.reclaim_state;
++>>>>>>> 0308f7cf19c9 (mm/vmscan.c: calculate reclaimed slab caches in all reclaim paths)
  	/*
 -	 * Unlike direct reclaim via alloc_pages(), memcg's reclaim doesn't
 -	 * take care of from where we get pages. So the node where we start the
 -	 * scan does not need to be the current node.
 +	 * Traverse the ZONELIST_FALLBACK zonelist of the current node to put
 +	 * equal pressure on all the nodes. This is based on the assumption that
 +	 * the reclaim does not bail out early.
  	 */
 -	nid = mem_cgroup_select_victim_node(memcg);
 -
 -	zonelist = &NODE_DATA(nid)->node_zonelists[ZONELIST_FALLBACK];
 +	struct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);
  
  	trace_mm_vmscan_memcg_reclaim_begin(0, sc.gfp_mask);
  
* Unmerged path mm/vmscan.c
