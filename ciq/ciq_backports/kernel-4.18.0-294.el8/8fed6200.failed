x86/mm: Break out kernel address space handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit 8fed62000039058adfd8b663344e2f448aed1e7a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/8fed6200.failed

The page fault handler (__do_page_fault())  basically has two sections:
one for handling faults in the kernel portion of the address space
and another for faults in the user portion of the address space.

But, these two parts don't stick out that well.  Let's make that more
clear from code separation and naming.  Pull kernel fault
handling into its own helper, and reflect that naming by renaming
spurious_fault() -> spurious_kernel_fault().

Also, rewrite the vmalloc() handling comment a bit.  It was a bit
stale and also glossed over the reserved bit handling.

	Cc: x86@kernel.org
	Cc: Jann Horn <jannh@google.com>
	Cc: Sean Christopherson <sean.j.christopherson@intel.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: http://lkml.kernel.org/r/20180928160222.401F4E10@viggo.jf.intel.com
(cherry picked from commit 8fed62000039058adfd8b663344e2f448aed1e7a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/fault.c
diff --cc arch/x86/mm/fault.c
index 387b6dbf768c,c7e32f453852..000000000000
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@@ -1212,38 -1279,9 +1264,30 @@@ __do_page_fault(struct pt_regs *regs, u
  	if (unlikely(kmmio_fault(regs, address)))
  		return;
  
- 	/*
- 	 * We fault-in kernel-space virtual memory on-demand. The
- 	 * 'reference' page table is init_mm.pgd.
- 	 *
- 	 * NOTE! We MUST NOT take any locks for this case. We may
- 	 * be in an interrupt or a critical region, and should
- 	 * only copy the information from the master page table,
- 	 * nothing more.
- 	 *
- 	 * This verifies that the fault happens in kernel space
- 	 * (hw_error_code & 4) == 0, and that the fault was not a
- 	 * protection error (hw_error_code & 9) == 0.
- 	 */
+ 	/* Was the fault on kernel-controlled part of the address space? */
  	if (unlikely(fault_in_kernel_space(address))) {
++<<<<<<< HEAD
 +		if (!(hw_error_code & (X86_PF_RSVD | X86_PF_USER | X86_PF_PROT))) {
 +			if (vmalloc_fault(address) >= 0)
 +				return;
 +		}
 +
 +		/* Can handle a stale RO->RW TLB: */
 +		if (spurious_fault(hw_error_code, address))
 +			return;
 +
 +		/* kprobes don't want to hook the spurious faults: */
 +		if (kprobes_fault(regs))
 +			return;
 +		/*
 +		 * Don't take the mm semaphore here. If we fixup a prefetch
 +		 * fault we could otherwise deadlock:
 +		 */
 +		bad_area_nosemaphore(regs, hw_error_code, address);
 +
++=======
+ 		do_kern_addr_fault(regs, hw_error_code, address);
++>>>>>>> 8fed62000039 (x86/mm: Break out kernel address space handling)
  		return;
  	}
  
* Unmerged path arch/x86/mm/fault.c
