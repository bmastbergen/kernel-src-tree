bpf: Implement bpf iterator for array maps

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yonghong Song <yhs@fb.com>
commit d3cc2ab546adc6e52b65f36f7c34820d2830d0c9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/d3cc2ab5.failed

The bpf iterators for array and percpu array
are implemented. Similar to hash maps, for percpu
array map, bpf program will receive values
from all cpus.

	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200723184115.590532-1-yhs@fb.com
(cherry picked from commit d3cc2ab546adc6e52b65f36f7c34820d2830d0c9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/arraymap.c
#	kernel/bpf/map_iter.c
diff --cc kernel/bpf/arraymap.c
index bf32bbe740a2,8ff419b632a6..000000000000
--- a/kernel/bpf/arraymap.c
+++ b/kernel/bpf/arraymap.c
@@@ -502,6 -487,143 +502,146 @@@ static int array_map_mmap(struct bpf_ma
  				   vma->vm_pgoff + pgoff);
  }
  
++<<<<<<< HEAD
++=======
+ struct bpf_iter_seq_array_map_info {
+ 	struct bpf_map *map;
+ 	void *percpu_value_buf;
+ 	u32 index;
+ };
+ 
+ static void *bpf_array_map_seq_start(struct seq_file *seq, loff_t *pos)
+ {
+ 	struct bpf_iter_seq_array_map_info *info = seq->private;
+ 	struct bpf_map *map = info->map;
+ 	struct bpf_array *array;
+ 	u32 index;
+ 
+ 	if (info->index >= map->max_entries)
+ 		return NULL;
+ 
+ 	if (*pos == 0)
+ 		++*pos;
+ 	array = container_of(map, struct bpf_array, map);
+ 	index = info->index & array->index_mask;
+ 	if (info->percpu_value_buf)
+ 	       return array->pptrs[index];
+ 	return array->value + array->elem_size * index;
+ }
+ 
+ static void *bpf_array_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+ {
+ 	struct bpf_iter_seq_array_map_info *info = seq->private;
+ 	struct bpf_map *map = info->map;
+ 	struct bpf_array *array;
+ 	u32 index;
+ 
+ 	++*pos;
+ 	++info->index;
+ 	if (info->index >= map->max_entries)
+ 		return NULL;
+ 
+ 	array = container_of(map, struct bpf_array, map);
+ 	index = info->index & array->index_mask;
+ 	if (info->percpu_value_buf)
+ 	       return array->pptrs[index];
+ 	return array->value + array->elem_size * index;
+ }
+ 
+ static int __bpf_array_map_seq_show(struct seq_file *seq, void *v)
+ {
+ 	struct bpf_iter_seq_array_map_info *info = seq->private;
+ 	struct bpf_iter__bpf_map_elem ctx = {};
+ 	struct bpf_map *map = info->map;
+ 	struct bpf_iter_meta meta;
+ 	struct bpf_prog *prog;
+ 	int off = 0, cpu = 0;
+ 	void __percpu **pptr;
+ 	u32 size;
+ 
+ 	meta.seq = seq;
+ 	prog = bpf_iter_get_info(&meta, v == NULL);
+ 	if (!prog)
+ 		return 0;
+ 
+ 	ctx.meta = &meta;
+ 	ctx.map = info->map;
+ 	if (v) {
+ 		ctx.key = &info->index;
+ 
+ 		if (!info->percpu_value_buf) {
+ 			ctx.value = v;
+ 		} else {
+ 			pptr = v;
+ 			size = round_up(map->value_size, 8);
+ 			for_each_possible_cpu(cpu) {
+ 				bpf_long_memcpy(info->percpu_value_buf + off,
+ 						per_cpu_ptr(pptr, cpu),
+ 						size);
+ 				off += size;
+ 			}
+ 			ctx.value = info->percpu_value_buf;
+ 		}
+ 	}
+ 
+ 	return bpf_iter_run_prog(prog, &ctx);
+ }
+ 
+ static int bpf_array_map_seq_show(struct seq_file *seq, void *v)
+ {
+ 	return __bpf_array_map_seq_show(seq, v);
+ }
+ 
+ static void bpf_array_map_seq_stop(struct seq_file *seq, void *v)
+ {
+ 	if (!v)
+ 		(void)__bpf_array_map_seq_show(seq, NULL);
+ }
+ 
+ static int bpf_iter_init_array_map(void *priv_data,
+ 				   struct bpf_iter_aux_info *aux)
+ {
+ 	struct bpf_iter_seq_array_map_info *seq_info = priv_data;
+ 	struct bpf_map *map = aux->map;
+ 	void *value_buf;
+ 	u32 buf_size;
+ 
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
+ 		buf_size = round_up(map->value_size, 8) * num_possible_cpus();
+ 		value_buf = kmalloc(buf_size, GFP_USER | __GFP_NOWARN);
+ 		if (!value_buf)
+ 			return -ENOMEM;
+ 
+ 		seq_info->percpu_value_buf = value_buf;
+ 	}
+ 
+ 	seq_info->map = map;
+ 	return 0;
+ }
+ 
+ static void bpf_iter_fini_array_map(void *priv_data)
+ {
+ 	struct bpf_iter_seq_array_map_info *seq_info = priv_data;
+ 
+ 	kfree(seq_info->percpu_value_buf);
+ }
+ 
+ static const struct seq_operations bpf_array_map_seq_ops = {
+ 	.start	= bpf_array_map_seq_start,
+ 	.next	= bpf_array_map_seq_next,
+ 	.stop	= bpf_array_map_seq_stop,
+ 	.show	= bpf_array_map_seq_show,
+ };
+ 
+ static const struct bpf_iter_seq_info iter_seq_info = {
+ 	.seq_ops		= &bpf_array_map_seq_ops,
+ 	.init_seq_private	= bpf_iter_init_array_map,
+ 	.fini_seq_private	= bpf_iter_fini_array_map,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_array_map_info),
+ };
+ 
+ static int array_map_btf_id;
++>>>>>>> d3cc2ab546ad (bpf: Implement bpf iterator for array maps)
  const struct bpf_map_ops array_map_ops = {
  	.map_alloc_check = array_map_alloc_check,
  	.map_alloc = array_map_alloc,
@@@ -518,8 -640,12 +658,14 @@@
  	.map_check_btf = array_map_check_btf,
  	.map_lookup_batch = generic_map_lookup_batch,
  	.map_update_batch = generic_map_update_batch,
++<<<<<<< HEAD
++=======
+ 	.map_btf_name = "bpf_array",
+ 	.map_btf_id = &array_map_btf_id,
+ 	.iter_seq_info = &iter_seq_info,
++>>>>>>> d3cc2ab546ad (bpf: Implement bpf iterator for array maps)
  };
  
 -static int percpu_array_map_btf_id;
  const struct bpf_map_ops percpu_array_map_ops = {
  	.map_alloc_check = array_map_alloc_check,
  	.map_alloc = array_map_alloc,
@@@ -530,6 -656,9 +676,12 @@@
  	.map_delete_elem = array_map_delete_elem,
  	.map_seq_show_elem = percpu_array_map_seq_show_elem,
  	.map_check_btf = array_map_check_btf,
++<<<<<<< HEAD
++=======
+ 	.map_btf_name = "bpf_array",
+ 	.map_btf_id = &percpu_array_map_btf_id,
+ 	.iter_seq_info = &iter_seq_info,
++>>>>>>> d3cc2ab546ad (bpf: Implement bpf iterator for array maps)
  };
  
  static int fd_array_map_alloc_check(union bpf_attr *attr)
diff --cc kernel/bpf/map_iter.c
index af759897e5bd,fbe1f557cb88..000000000000
--- a/kernel/bpf/map_iter.c
+++ b/kernel/bpf/map_iter.c
@@@ -77,17 -78,82 +77,86 @@@ static const struct seq_operations bpf_
  	.show	= bpf_map_seq_show,
  };
  
++<<<<<<< HEAD
++=======
+ BTF_ID_LIST(btf_bpf_map_id)
+ BTF_ID(struct, bpf_map)
+ 
+ static const struct bpf_iter_seq_info bpf_map_seq_info = {
+ 	.seq_ops		= &bpf_map_seq_ops,
+ 	.init_seq_private	= NULL,
+ 	.fini_seq_private	= NULL,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
+ };
+ 
+ static struct bpf_iter_reg bpf_map_reg_info = {
+ 	.target			= "bpf_map",
+ 	.ctx_arg_info_size	= 1,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__bpf_map, map),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ 	.seq_info		= &bpf_map_seq_info,
+ };
+ 
+ static int bpf_iter_check_map(struct bpf_prog *prog,
+ 			      struct bpf_iter_aux_info *aux)
+ {
+ 	u32 key_acc_size, value_acc_size, key_size, value_size;
+ 	struct bpf_map *map = aux->map;
+ 	bool is_percpu = false;
+ 
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
+ 		is_percpu = true;
+ 	else if (map->map_type != BPF_MAP_TYPE_HASH &&
+ 		 map->map_type != BPF_MAP_TYPE_LRU_HASH &&
+ 		 map->map_type != BPF_MAP_TYPE_ARRAY)
+ 		return -EINVAL;
+ 
+ 	key_acc_size = prog->aux->max_rdonly_access;
+ 	value_acc_size = prog->aux->max_rdwr_access;
+ 	key_size = map->key_size;
+ 	if (!is_percpu)
+ 		value_size = map->value_size;
+ 	else
+ 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
+ 
+ 	if (key_acc_size > key_size || value_acc_size > value_size)
+ 		return -EACCES;
+ 
+ 	return 0;
+ }
+ 
+ DEFINE_BPF_ITER_FUNC(bpf_map_elem, struct bpf_iter_meta *meta,
+ 		     struct bpf_map *map, void *key, void *value)
+ 
+ static const struct bpf_iter_reg bpf_map_elem_reg_info = {
+ 	.target			= "bpf_map_elem",
+ 	.check_target		= bpf_iter_check_map,
+ 	.req_linfo		= BPF_ITER_LINK_MAP_FD,
+ 	.ctx_arg_info_size	= 2,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__bpf_map_elem, key),
+ 		  PTR_TO_RDONLY_BUF_OR_NULL },
+ 		{ offsetof(struct bpf_iter__bpf_map_elem, value),
+ 		  PTR_TO_RDWR_BUF_OR_NULL },
+ 	},
+ };
+ 
++>>>>>>> d3cc2ab546ad (bpf: Implement bpf iterator for array maps)
  static int __init bpf_map_iter_init(void)
  {
 -	int ret;
 -
 -	bpf_map_reg_info.ctx_arg_info[0].btf_id = *btf_bpf_map_id;
 -	ret = bpf_iter_reg_target(&bpf_map_reg_info);
 -	if (ret)
 -		return ret;
 -
 -	return bpf_iter_reg_target(&bpf_map_elem_reg_info);
 +	struct bpf_iter_reg reg_info = {
 +		.target			= "bpf_map",
 +		.seq_ops		= &bpf_map_seq_ops,
 +		.init_seq_private	= NULL,
 +		.fini_seq_private	= NULL,
 +		.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
 +	};
 +
 +	return bpf_iter_reg_target(&reg_info);
  }
  
  late_initcall(bpf_map_iter_init);
* Unmerged path kernel/bpf/arraymap.c
* Unmerged path kernel/bpf/map_iter.c
