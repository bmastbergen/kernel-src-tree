xfs: split the iomap ops for buffered vs direct writes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Christoph Hellwig <hch@lst.de>
commit f150b4234397448c6abab8785e58a222bfd9ec00
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f150b423.failed

Instead of lots of magic conditionals in the main write_begin
handler this make the intent very clear.  Thing will become even
better once we support delayed allocations for extent size hints
and realtime allocations.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
(cherry picked from commit f150b4234397448c6abab8785e58a222bfd9ec00)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_iomap.c
diff --cc fs/xfs/xfs_iomap.c
index 3a96f66f0ce2,a706da8ffe22..000000000000
--- a/fs/xfs/xfs_iomap.c
+++ b/fs/xfs/xfs_iomap.c
@@@ -530,8 -530,330 +530,333 @@@ check_writeio
  	return alloc_blocks;
  }
  
++<<<<<<< HEAD
++=======
+ int
+ xfs_iomap_write_unwritten(
+ 	xfs_inode_t	*ip,
+ 	xfs_off_t	offset,
+ 	xfs_off_t	count,
+ 	bool		update_isize)
+ {
+ 	xfs_mount_t	*mp = ip->i_mount;
+ 	xfs_fileoff_t	offset_fsb;
+ 	xfs_filblks_t	count_fsb;
+ 	xfs_filblks_t	numblks_fsb;
+ 	int		nimaps;
+ 	xfs_trans_t	*tp;
+ 	xfs_bmbt_irec_t imap;
+ 	struct inode	*inode = VFS_I(ip);
+ 	xfs_fsize_t	i_size;
+ 	uint		resblks;
+ 	int		error;
+ 
+ 	trace_xfs_unwritten_convert(ip, offset, count);
+ 
+ 	offset_fsb = XFS_B_TO_FSBT(mp, offset);
+ 	count_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + count);
+ 	count_fsb = (xfs_filblks_t)(count_fsb - offset_fsb);
+ 
+ 	/*
+ 	 * Reserve enough blocks in this transaction for two complete extent
+ 	 * btree splits.  We may be converting the middle part of an unwritten
+ 	 * extent and in this case we will insert two new extents in the btree
+ 	 * each of which could cause a full split.
+ 	 *
+ 	 * This reservation amount will be used in the first call to
+ 	 * xfs_bmbt_split() to select an AG with enough space to satisfy the
+ 	 * rest of the operation.
+ 	 */
+ 	resblks = XFS_DIOSTRAT_SPACE_RES(mp, 0) << 1;
+ 
+ 	do {
+ 		/*
+ 		 * Set up a transaction to convert the range of extents
+ 		 * from unwritten to real. Do allocations in a loop until
+ 		 * we have covered the range passed in.
+ 		 *
+ 		 * Note that we can't risk to recursing back into the filesystem
+ 		 * here as we might be asked to write out the same inode that we
+ 		 * complete here and might deadlock on the iolock.
+ 		 */
+ 		error = xfs_trans_alloc(mp, &M_RES(mp)->tr_write, resblks, 0,
+ 				XFS_TRANS_RESERVE, &tp);
+ 		if (error)
+ 			return error;
+ 
+ 		xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 		xfs_trans_ijoin(tp, ip, 0);
+ 
+ 		/*
+ 		 * Modify the unwritten extent state of the buffer.
+ 		 */
+ 		nimaps = 1;
+ 		error = xfs_bmapi_write(tp, ip, offset_fsb, count_fsb,
+ 					XFS_BMAPI_CONVERT, resblks, &imap,
+ 					&nimaps);
+ 		if (error)
+ 			goto error_on_bmapi_transaction;
+ 
+ 		/*
+ 		 * Log the updated inode size as we go.  We have to be careful
+ 		 * to only log it up to the actual write offset if it is
+ 		 * halfway into a block.
+ 		 */
+ 		i_size = XFS_FSB_TO_B(mp, offset_fsb + count_fsb);
+ 		if (i_size > offset + count)
+ 			i_size = offset + count;
+ 		if (update_isize && i_size > i_size_read(inode))
+ 			i_size_write(inode, i_size);
+ 		i_size = xfs_new_eof(ip, i_size);
+ 		if (i_size) {
+ 			ip->i_d.di_size = i_size;
+ 			xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);
+ 		}
+ 
+ 		error = xfs_trans_commit(tp);
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 		if (error)
+ 			return error;
+ 
+ 		if (unlikely(!xfs_valid_startblock(ip, imap.br_startblock)))
+ 			return xfs_alert_fsblock_zero(ip, &imap);
+ 
+ 		if ((numblks_fsb = imap.br_blockcount) == 0) {
+ 			/*
+ 			 * The numblks_fsb value should always get
+ 			 * smaller, otherwise the loop is stuck.
+ 			 */
+ 			ASSERT(imap.br_blockcount);
+ 			break;
+ 		}
+ 		offset_fsb += numblks_fsb;
+ 		count_fsb -= numblks_fsb;
+ 	} while (count_fsb > 0);
+ 
+ 	return 0;
+ 
+ error_on_bmapi_transaction:
+ 	xfs_trans_cancel(tp);
+ 	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	return error;
+ }
+ 
+ static inline bool
+ imap_needs_alloc(
+ 	struct inode		*inode,
+ 	struct xfs_bmbt_irec	*imap,
+ 	int			nimaps)
+ {
+ 	return !nimaps ||
+ 		imap->br_startblock == HOLESTARTBLOCK ||
+ 		imap->br_startblock == DELAYSTARTBLOCK ||
+ 		(IS_DAX(inode) && imap->br_state == XFS_EXT_UNWRITTEN);
+ }
+ 
+ static inline bool
+ needs_cow_for_zeroing(
+ 	struct xfs_bmbt_irec	*imap,
+ 	int			nimaps)
+ {
+ 	return nimaps &&
+ 		imap->br_startblock != HOLESTARTBLOCK &&
+ 		imap->br_state != XFS_EXT_UNWRITTEN;
+ }
+ 
  static int
- xfs_file_iomap_begin_delay(
+ xfs_ilock_for_iomap(
+ 	struct xfs_inode	*ip,
+ 	unsigned		flags,
+ 	unsigned		*lockmode)
+ {
+ 	unsigned		mode = XFS_ILOCK_SHARED;
+ 	bool			is_write = flags & (IOMAP_WRITE | IOMAP_ZERO);
+ 
+ 	/*
+ 	 * COW writes may allocate delalloc space or convert unwritten COW
+ 	 * extents, so we need to make sure to take the lock exclusively here.
+ 	 */
+ 	if (xfs_is_cow_inode(ip) && is_write) {
+ 		/*
+ 		 * FIXME: It could still overwrite on unshared extents and not
+ 		 * need allocation.
+ 		 */
+ 		if (flags & IOMAP_NOWAIT)
+ 			return -EAGAIN;
+ 		mode = XFS_ILOCK_EXCL;
+ 	}
+ 
+ 	/*
+ 	 * Extents not yet cached requires exclusive access, don't block.  This
+ 	 * is an opencoded xfs_ilock_data_map_shared() call but with
+ 	 * non-blocking behaviour.
+ 	 */
+ 	if (!(ip->i_df.if_flags & XFS_IFEXTENTS)) {
+ 		if (flags & IOMAP_NOWAIT)
+ 			return -EAGAIN;
+ 		mode = XFS_ILOCK_EXCL;
+ 	}
+ 
+ relock:
+ 	if (flags & IOMAP_NOWAIT) {
+ 		if (!xfs_ilock_nowait(ip, mode))
+ 			return -EAGAIN;
+ 	} else {
+ 		xfs_ilock(ip, mode);
+ 	}
+ 
+ 	/*
+ 	 * The reflink iflag could have changed since the earlier unlocked
+ 	 * check, so if we got ILOCK_SHARED for a write and but we're now a
+ 	 * reflink inode we have to switch to ILOCK_EXCL and relock.
+ 	 */
+ 	if (mode == XFS_ILOCK_SHARED && is_write && xfs_is_cow_inode(ip)) {
+ 		xfs_iunlock(ip, mode);
+ 		mode = XFS_ILOCK_EXCL;
+ 		goto relock;
+ 	}
+ 
+ 	*lockmode = mode;
+ 	return 0;
+ }
+ 
+ static int
+ xfs_direct_write_iomap_begin(
+ 	struct inode		*inode,
+ 	loff_t			offset,
+ 	loff_t			length,
+ 	unsigned		flags,
+ 	struct iomap		*iomap,
+ 	struct iomap		*srcmap)
+ {
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_bmbt_irec	imap, cmap;
+ 	xfs_fileoff_t		offset_fsb = XFS_B_TO_FSBT(mp, offset);
+ 	xfs_fileoff_t		end_fsb = xfs_iomap_end_fsb(mp, offset, length);
+ 	int			nimaps = 1, error = 0;
+ 	bool			shared = false;
+ 	u16			iomap_flags = 0;
+ 	unsigned		lockmode;
+ 
+ 	ASSERT(flags & (IOMAP_WRITE | IOMAP_ZERO));
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		return -EIO;
+ 
+ 	/*
+ 	 * Lock the inode in the manner required for the specified operation and
+ 	 * check for as many conditions that would result in blocking as
+ 	 * possible. This removes most of the non-blocking checks from the
+ 	 * mapping code below.
+ 	 */
+ 	error = xfs_ilock_for_iomap(ip, flags, &lockmode);
+ 	if (error)
+ 		return error;
+ 
+ 	error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb, &imap,
+ 			       &nimaps, 0);
+ 	if (error)
+ 		goto out_unlock;
+ 
+ 	/*
+ 	 * Break shared extents if necessary. Checks for non-blocking IO have
+ 	 * been done up front, so we don't need to do them here.
+ 	 */
+ 	if (xfs_is_cow_inode(ip)) {
+ 		/* if zeroing doesn't need COW allocation, then we are done. */
+ 		if ((flags & IOMAP_ZERO) &&
+ 		    !needs_cow_for_zeroing(&imap, nimaps))
+ 			goto out_found;
+ 
+ 		/* may drop and re-acquire the ilock */
+ 		error = xfs_reflink_allocate_cow(ip, &imap, &cmap, &shared,
+ 				&lockmode, flags & IOMAP_DIRECT);
+ 		if (error)
+ 			goto out_unlock;
+ 		if (shared)
+ 			goto out_found_cow;
+ 		end_fsb = imap.br_startoff + imap.br_blockcount;
+ 		length = XFS_FSB_TO_B(mp, end_fsb) - offset;
+ 	}
+ 
+ 	/* Don't need to allocate over holes when doing zeroing operations. */
+ 	if (flags & IOMAP_ZERO)
+ 		goto out_found;
+ 
+ 	if (!imap_needs_alloc(inode, &imap, nimaps))
+ 		goto out_found;
+ 
+ 	/* If nowait is set bail since we are going to make allocations. */
+ 	if (flags & IOMAP_NOWAIT) {
+ 		error = -EAGAIN;
+ 		goto out_unlock;
+ 	}
+ 
+ 	/*
+ 	 * We cap the maximum length we map to a sane size  to keep the chunks
+ 	 * of work done where somewhat symmetric with the work writeback does.
+ 	 * This is a completely arbitrary number pulled out of thin air as a
+ 	 * best guess for initial testing.
+ 	 *
+ 	 * Note that the values needs to be less than 32-bits wide until the
+ 	 * lower level functions are updated.
+ 	 */
+ 	length = min_t(loff_t, length, 1024 * PAGE_SIZE);
+ 
+ 	/*
+ 	 * xfs_iomap_write_direct() expects the shared lock. It is unlocked on
+ 	 * return.
+ 	 */
+ 	if (lockmode == XFS_ILOCK_EXCL)
+ 		xfs_ilock_demote(ip, lockmode);
+ 	error = xfs_iomap_write_direct(ip, offset, length, &imap,
+ 			nimaps);
+ 	if (error)
+ 		return error;
+ 
+ 	iomap_flags |= IOMAP_F_NEW;
+ 	trace_xfs_iomap_alloc(ip, offset, length, XFS_DATA_FORK, &imap);
+ 
+ out_finish:
+ 	/*
+ 	 * Writes that span EOF might trigger an IO size update on completion,
+ 	 * so consider them to be dirty for the purposes of O_DSYNC even if
+ 	 * there is no other metadata changes pending or have been made here.
+ 	 */
+ 	if (offset + length > i_size_read(inode))
+ 		iomap_flags |= IOMAP_F_DIRTY;
+ 	return xfs_bmbt_to_iomap(ip, iomap, &imap, iomap_flags);
+ 
+ out_found:
+ 	ASSERT(nimaps);
+ 	xfs_iunlock(ip, lockmode);
+ 	trace_xfs_iomap_found(ip, offset, length, XFS_DATA_FORK, &imap);
+ 	goto out_finish;
+ 
+ out_found_cow:
+ 	xfs_iunlock(ip, lockmode);
+ 	length = XFS_FSB_TO_B(mp, cmap.br_startoff + cmap.br_blockcount);
+ 	trace_xfs_iomap_found(ip, offset, length - offset, XFS_COW_FORK, &cmap);
+ 	if (imap.br_startblock != HOLESTARTBLOCK) {
+ 		error = xfs_bmbt_to_iomap(ip, srcmap, &imap, 0);
+ 		if (error)
+ 			return error;
+ 	}
+ 	return xfs_bmbt_to_iomap(ip, iomap, &cmap, IOMAP_F_SHARED);
+ 
+ out_unlock:
+ 	xfs_iunlock(ip, lockmode);
+ 	return error;
+ }
+ 
+ const struct iomap_ops xfs_direct_write_iomap_ops = {
+ 	.iomap_begin		= xfs_direct_write_iomap_begin,
+ };
+ 
++>>>>>>> f150b4234397 (xfs: split the iomap ops for buffered vs direct writes)
+ static int
+ xfs_buffered_write_iomap_begin(
  	struct inode		*inode,
  	loff_t			offset,
  	loff_t			count,
diff --git a/fs/xfs/xfs_bmap_util.c b/fs/xfs/xfs_bmap_util.c
index a22d392d8517..cc8122532bfd 100644
--- a/fs/xfs/xfs_bmap_util.c
+++ b/fs/xfs/xfs_bmap_util.c
@@ -1107,7 +1107,8 @@ xfs_free_file_space(
 		return 0;
 	if (offset + len > XFS_ISIZE(ip))
 		len = XFS_ISIZE(ip) - offset;
-	error = iomap_zero_range(VFS_I(ip), offset, len, NULL, &xfs_iomap_ops);
+	error = iomap_zero_range(VFS_I(ip), offset, len, NULL,
+			&xfs_buffered_write_iomap_ops);
 	if (error)
 		return error;
 
diff --git a/fs/xfs/xfs_file.c b/fs/xfs/xfs_file.c
index a00a3cc98633..fe55cb10a2b4 100644
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@ -342,7 +342,7 @@ xfs_file_aio_write_checks(
 	
 		trace_xfs_zero_eof(ip, isize, iocb->ki_pos - isize);
 		error = iomap_zero_range(inode, isize, iocb->ki_pos - isize,
-				NULL, &xfs_iomap_ops);
+				NULL, &xfs_buffered_write_iomap_ops);
 		if (error)
 			return error;
 	} else
@@ -542,7 +542,8 @@ xfs_file_dio_aio_write(
 	 * If unaligned, this is the only IO in-flight. Wait on it before we
 	 * release the iolock to prevent subsequent overlapping IO.
 	 */
-	ret = iomap_dio_rw(iocb, from, &xfs_iomap_ops, &xfs_dio_write_ops,
+	ret = iomap_dio_rw(iocb, from, &xfs_direct_write_iomap_ops,
+			   &xfs_dio_write_ops,
 			   is_sync_kiocb(iocb) || unaligned_io);
 out:
 	xfs_iunlock(ip, iolock);
@@ -582,7 +583,7 @@ xfs_file_dax_write(
 	count = iov_iter_count(from);
 
 	trace_xfs_file_dax_write(ip, count, pos);
-	ret = dax_iomap_rw(iocb, from, &xfs_iomap_ops);
+	ret = dax_iomap_rw(iocb, from, &xfs_direct_write_iomap_ops);
 	if (ret > 0 && iocb->ki_pos > i_size_read(inode)) {
 		i_size_write(inode, iocb->ki_pos);
 		error = xfs_setfilesize(ip, pos, ret);
@@ -629,7 +630,8 @@ xfs_file_buffered_aio_write(
 	current->backing_dev_info = inode_to_bdi(inode);
 
 	trace_xfs_file_buffered_write(ip, iov_iter_count(from), iocb->ki_pos);
-	ret = iomap_file_buffered_write(iocb, from, &xfs_iomap_ops);
+	ret = iomap_file_buffered_write(iocb, from,
+			&xfs_buffered_write_iomap_ops);
 	if (likely(ret >= 0))
 		iocb->ki_pos += ret;
 
@@ -1173,12 +1175,14 @@ __xfs_filemap_fault(
 
 		ret = dax_iomap_fault(vmf, pe_size, &pfn, NULL,
 				(write_fault && !vmf->cow_page) ?
-				 &xfs_iomap_ops : &xfs_read_iomap_ops);
+				 &xfs_direct_write_iomap_ops :
+				 &xfs_read_iomap_ops);
 		if (ret & VM_FAULT_NEEDDSYNC)
 			ret = dax_finish_sync_fault(vmf, pe_size, pfn);
 	} else {
 		if (write_fault)
-			ret = iomap_page_mkwrite(vmf, &xfs_iomap_ops);
+			ret = iomap_page_mkwrite(vmf,
+					&xfs_buffered_write_iomap_ops);
 		else
 			ret = filemap_fault(vmf);
 	}
* Unmerged path fs/xfs/xfs_iomap.c
diff --git a/fs/xfs/xfs_iomap.h b/fs/xfs/xfs_iomap.h
index 61b1fc3e5143..7aed28275089 100644
--- a/fs/xfs/xfs_iomap.h
+++ b/fs/xfs/xfs_iomap.h
@@ -39,7 +39,8 @@ xfs_aligned_fsb_count(
 	return count_fsb;
 }
 
-extern const struct iomap_ops xfs_iomap_ops;
+extern const struct iomap_ops xfs_buffered_write_iomap_ops;
+extern const struct iomap_ops xfs_direct_write_iomap_ops;
 extern const struct iomap_ops xfs_read_iomap_ops;
 extern const struct iomap_ops xfs_seek_iomap_ops;
 extern const struct iomap_ops xfs_xattr_iomap_ops;
diff --git a/fs/xfs/xfs_iops.c b/fs/xfs/xfs_iops.c
index fcf573df2621..1d4e6f7d8e96 100644
--- a/fs/xfs/xfs_iops.c
+++ b/fs/xfs/xfs_iops.c
@@ -883,10 +883,10 @@ xfs_setattr_size(
 	if (newsize > oldsize) {
 		trace_xfs_zero_eof(ip, oldsize, newsize - oldsize);
 		error = iomap_zero_range(inode, oldsize, newsize - oldsize,
-				&did_zeroing, &xfs_iomap_ops);
+				&did_zeroing, &xfs_buffered_write_iomap_ops);
 	} else {
 		error = iomap_truncate_page(inode, newsize, &did_zeroing,
-				&xfs_iomap_ops);
+				&xfs_buffered_write_iomap_ops);
 	}
 
 	if (error)
diff --git a/fs/xfs/xfs_reflink.c b/fs/xfs/xfs_reflink.c
index 19a6e4644123..1e18b4024b82 100644
--- a/fs/xfs/xfs_reflink.c
+++ b/fs/xfs/xfs_reflink.c
@@ -1270,7 +1270,7 @@ xfs_reflink_zero_posteof(
 
 	trace_xfs_zero_eof(ip, isize, pos - isize);
 	return iomap_zero_range(VFS_I(ip), isize, pos - isize, NULL,
-			&xfs_iomap_ops);
+			&xfs_buffered_write_iomap_ops);
 }
 
 /*
@@ -1527,7 +1527,8 @@ xfs_reflink_unshare(
 
 	inode_dio_wait(inode);
 
-	error = iomap_file_unshare(inode, offset, len, &xfs_iomap_ops);
+	error = iomap_file_unshare(inode, offset, len,
+			&xfs_buffered_write_iomap_ops);
 	if (error)
 		goto out;
 	error = filemap_write_and_wait(inode->i_mapping);
