mm/memory_hotplug: rename mhp_restrictions to mhp_params

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Logan Gunthorpe <logang@deltatee.com>
commit f5637d3b42ab0465ef71d5fb8461bce97fba95e8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f5637d3b.failed

The mhp_restrictions struct really doesn't specify anything resembling a
restriction anymore so rename it to be mhp_params as it is a list of
extended parameters.

	Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: David Hildenbrand <david@redhat.com>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Eric Badger <ebadger@gigaio.com>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Will Deacon <will@kernel.org>
Link: http://lkml.kernel.org/r/20200306170846.9333-3-logang@deltatee.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f5637d3b42ab0465ef71d5fb8461bce97fba95e8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/mm/mmu.c
#	arch/sh/mm/init.c
diff --cc arch/arm64/mm/mmu.c
index 4e2da4771636,6d4e9c2b4ed0..000000000000
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@@ -1069,11 -1356,27 +1069,11 @@@ int pud_free_pmd_page(pud_t *pudp, unsi
  	return 1;
  }
  
 -int p4d_free_pud_page(p4d_t *p4d, unsigned long addr)
 -{
 -	return 0;	/* Don't attempt a block mapping */
 -}
 -
  #ifdef CONFIG_MEMORY_HOTPLUG
 -static void __remove_pgd_mapping(pgd_t *pgdir, unsigned long start, u64 size)
 -{
 -	unsigned long end = start + size;
 -
 -	WARN_ON(pgdir != init_mm.pgd);
 -	WARN_ON((start < PAGE_OFFSET) || (end > PAGE_END));
 -
 -	unmap_hotplug_range(start, end, false);
 -	free_empty_tables(start, end, PAGE_OFFSET, PAGE_END);
 -}
 -
  int arch_add_memory(int nid, u64 start, u64 size,
- 			struct mhp_restrictions *restrictions)
+ 		    struct mhp_params *params)
  {
 -	int ret, flags = 0;
 +	int flags = 0;
  
  	if (rodata_full || debug_pagealloc_enabled())
  		flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
@@@ -1083,9 -1386,14 +1083,18 @@@
  
  	memblock_clear_nomap(start, size);
  
++<<<<<<< HEAD
 +	return __add_pages(nid, start >> PAGE_SHIFT, size >> PAGE_SHIFT,
 +			   restrictions);
++=======
+ 	ret = __add_pages(nid, start >> PAGE_SHIFT, size >> PAGE_SHIFT,
+ 			   params);
+ 	if (ret)
+ 		__remove_pgd_mapping(swapper_pg_dir,
+ 				     __phys_to_virt(start), size);
+ 	return ret;
++>>>>>>> f5637d3b42ab (mm/memory_hotplug: rename mhp_restrictions to mhp_params)
  }
 -
  void arch_remove_memory(int nid, u64 start, u64 size,
  			struct vmem_altmap *altmap)
  {
diff --cc arch/sh/mm/init.c
index db47300bc72b,e5114c053364..000000000000
--- a/arch/sh/mm/init.c
+++ b/arch/sh/mm/init.c
@@@ -415,28 -404,16 +415,37 @@@ void __init mem_init(void
  	mem_init_done = 1;
  }
  
 +void free_initmem(void)
 +{
 +	free_initmem_default(-1);
 +}
 +
 +#ifdef CONFIG_BLK_DEV_INITRD
 +void free_initrd_mem(unsigned long start, unsigned long end)
 +{
 +	free_reserved_area((void *)start, (void *)end, -1, "initrd");
 +}
 +#endif
 +
  #ifdef CONFIG_MEMORY_HOTPLUG
++<<<<<<< HEAD
 +int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
 +		bool want_memblock)
++=======
+ int arch_add_memory(int nid, u64 start, u64 size,
+ 		    struct mhp_params *params)
++>>>>>>> f5637d3b42ab (mm/memory_hotplug: rename mhp_restrictions to mhp_params)
  {
  	unsigned long start_pfn = PFN_DOWN(start);
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  	int ret;
  
  	/* We only have ZONE_NORMAL, so this is easy.. */
++<<<<<<< HEAD
 +	ret = __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
++=======
+ 	ret = __add_pages(nid, start_pfn, nr_pages, params);
++>>>>>>> f5637d3b42ab (mm/memory_hotplug: rename mhp_restrictions to mhp_params)
  	if (unlikely(ret))
  		printk("%s: Failed, __add_pages() == %d\n", __func__, ret);
  
* Unmerged path arch/arm64/mm/mmu.c
diff --git a/arch/ia64/mm/init.c b/arch/ia64/mm/init.c
index 7647c5debd81..df38e308fe29 100644
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@ -646,13 +646,13 @@ mem_init (void)
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 int arch_add_memory(int nid, u64 start, u64 size,
-			struct mhp_restrictions *restrictions)
+		    struct mhp_params *params)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 	int ret;
 
-	ret = __add_pages(nid, start_pfn, nr_pages, restrictions);
+	ret = __add_pages(nid, start_pfn, nr_pages, params);
 	if (ret)
 		printk("%s: Problem encountered in __add_pages() as ret=%d\n",
 		       __func__,  ret);
diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index fa9ce75915ae..8283fba2f45f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -137,7 +137,7 @@ static void flush_dcache_range_chunked(unsigned long start, unsigned long stop,
 }
 
 int __ref arch_add_memory(int nid, u64 start, u64 size,
-			struct mhp_restrictions *restrictions)
+			  struct mhp_params *params)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
@@ -153,7 +153,7 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 		return -EFAULT;
 	}
 
-	return __add_pages(nid, start_pfn, nr_pages, restrictions);
+	return __add_pages(nid, start_pfn, nr_pages, params);
 }
 
 void __ref arch_remove_memory(int nid, u64 start, u64 size,
diff --git a/arch/s390/mm/init.c b/arch/s390/mm/init.c
index c73cc156f3ba..3459697eeae3 100644
--- a/arch/s390/mm/init.c
+++ b/arch/s390/mm/init.c
@@ -277,20 +277,20 @@ device_initcall(s390_cma_mem_init);
 #endif /* CONFIG_CMA */
 
 int arch_add_memory(int nid, u64 start, u64 size,
-		struct mhp_restrictions *restrictions)
+		    struct mhp_params *params)
 {
 	unsigned long start_pfn = PFN_DOWN(start);
 	unsigned long size_pages = PFN_DOWN(size);
 	int rc;
 
-	if (WARN_ON_ONCE(restrictions->altmap))
+	if (WARN_ON_ONCE(params->altmap))
 		return -EINVAL;
 
 	rc = vmem_add_mapping(start, size);
 	if (rc)
 		return rc;
 
-	rc = __add_pages(nid, start_pfn, size_pages, restrictions);
+	rc = __add_pages(nid, start_pfn, size_pages, params);
 	if (rc)
 		vmem_remove_mapping(start, size);
 	return rc;
* Unmerged path arch/sh/mm/init.c
diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c
index a97fb2a9390b..4b7a6c335a14 100644
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@ -855,12 +855,12 @@ void __init mem_init(void)
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 int arch_add_memory(int nid, u64 start, u64 size,
-			struct mhp_restrictions *restrictions)
+		    struct mhp_params *params)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 
-	return __add_pages(nid, start_pfn, nr_pages, restrictions);
+	return __add_pages(nid, start_pfn, nr_pages, params);
 }
 
 void arch_remove_memory(int nid, u64 start, u64 size,
diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c
index 05348c61e15d..e1b9026a494f 100644
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@ -843,11 +843,11 @@ static void update_end_of_memory_vars(u64 start, u64 size)
 }
 
 int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
-				struct mhp_restrictions *restrictions)
+	      struct mhp_params *params)
 {
 	int ret;
 
-	ret = __add_pages(nid, start_pfn, nr_pages, restrictions);
+	ret = __add_pages(nid, start_pfn, nr_pages, params);
 	WARN_ON_ONCE(ret);
 
 	/* update max_pfn, max_low_pfn and high_memory */
@@ -858,14 +858,14 @@ int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
 }
 
 int arch_add_memory(int nid, u64 start, u64 size,
-			struct mhp_restrictions *restrictions)
+		    struct mhp_params *params)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 
 	init_memory_mapping(start, start + size);
 
-	return add_pages(nid, start_pfn, nr_pages, restrictions);
+	return add_pages(nid, start_pfn, nr_pages, params);
 }
 
 #define PAGE_INUSE 0xFD
diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h
index 70b16b05a960..28cb1ca3296d 100644
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@ -58,10 +58,10 @@ enum {
 };
 
 /*
- * Restrictions for the memory hotplug:
- * altmap: alternative allocator for memmap array
+ * Extended parameters for memory hotplug:
+ * altmap: alternative allocator for memmap array (optional)
  */
-struct mhp_restrictions {
+struct mhp_params {
 	struct vmem_altmap *altmap;
 };
 
@@ -112,7 +112,7 @@ extern int restore_online_page_callback(online_page_callback_t callback);
 extern int try_online_node(int nid);
 
 extern int arch_add_memory(int nid, u64 start, u64 size,
-			struct mhp_restrictions *restrictions);
+			   struct mhp_params *params);
 extern u64 max_mem_size;
 
 extern int memhp_online_type_from_str(const char *str);
@@ -133,17 +133,17 @@ extern void __remove_pages(unsigned long start_pfn, unsigned long nr_pages,
 
 /* reasonably generic interface to expand the physical pages */
 extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
-		       struct mhp_restrictions *restrictions);
+		       struct mhp_params *params);
 
 #ifndef CONFIG_ARCH_HAS_ADD_PAGES
 static inline int add_pages(int nid, unsigned long start_pfn,
-		unsigned long nr_pages, struct mhp_restrictions *restrictions)
+		unsigned long nr_pages, struct mhp_params *params)
 {
-	return __add_pages(nid, start_pfn, nr_pages, restrictions);
+	return __add_pages(nid, start_pfn, nr_pages, params);
 }
 #else /* ARCH_HAS_ADD_PAGES */
 int add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
-	      struct mhp_restrictions *restrictions);
+	      struct mhp_params *params);
 #endif /* ARCH_HAS_ADD_PAGES */
 
 #ifdef CONFIG_NUMA
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index 226a3dc8d15f..cf7423e04e9d 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -303,12 +303,12 @@ static int check_hotplug_memory_addressable(unsigned long pfn,
  * add the new pages.
  */
 int __ref __add_pages(int nid, unsigned long pfn, unsigned long nr_pages,
-		struct mhp_restrictions *restrictions)
+		struct mhp_params *params)
 {
 	const unsigned long end_pfn = pfn + nr_pages;
 	unsigned long cur_nr_pages;
 	int err;
-	struct vmem_altmap *altmap = restrictions->altmap;
+	struct vmem_altmap *altmap = params->altmap;
 
 	err = check_hotplug_memory_addressable(pfn, nr_pages);
 	if (err)
@@ -1006,7 +1006,7 @@ static int online_memory_block(struct memory_block *mem, void *arg)
  */
 int __ref add_memory_resource(int nid, struct resource *res)
 {
-	struct mhp_restrictions restrictions = {};
+	struct mhp_params params = {};
 	u64 start, size;
 	bool new_node = false;
 	int ret;
@@ -1034,7 +1034,7 @@ int __ref add_memory_resource(int nid, struct resource *res)
 	new_node = ret;
 
 	/* call arch's memory hotadd */
-	ret = arch_add_memory(nid, start, size, &restrictions);
+	ret = arch_add_memory(nid, start, size, &params);
 	if (ret < 0)
 		goto error;
 
diff --git a/mm/memremap.c b/mm/memremap.c
index bbf457c4f166..b0b5170843ff 100644
--- a/mm/memremap.c
+++ b/mm/memremap.c
@@ -184,7 +184,7 @@ void *memremap_pages(struct dev_pagemap *pgmap, int nid)
 {
 	struct resource *res = &pgmap->res;
 	struct dev_pagemap *conflict_pgmap;
-	struct mhp_restrictions restrictions = {
+	struct mhp_params params = {
 		/*
 		 * We do not want any optional features only our own memmap
 		 */
@@ -302,7 +302,7 @@ void *memremap_pages(struct dev_pagemap *pgmap, int nid)
 	 */
 	if (pgmap->type == MEMORY_DEVICE_PRIVATE) {
 		error = add_pages(nid, PHYS_PFN(res->start),
-				PHYS_PFN(resource_size(res)), &restrictions);
+				PHYS_PFN(resource_size(res)), &params);
 	} else {
 		error = kasan_add_zero_shadow(__va(res->start), resource_size(res));
 		if (error) {
@@ -311,7 +311,7 @@ void *memremap_pages(struct dev_pagemap *pgmap, int nid)
 		}
 
 		error = arch_add_memory(nid, res->start, resource_size(res),
-					&restrictions);
+					&params);
 	}
 
 	if (!error) {
@@ -319,7 +319,7 @@ void *memremap_pages(struct dev_pagemap *pgmap, int nid)
 
 		zone = &NODE_DATA(nid)->node_zones[ZONE_DEVICE];
 		move_pfn_range_to_zone(zone, PHYS_PFN(res->start),
-				PHYS_PFN(resource_size(res)), restrictions.altmap);
+				PHYS_PFN(resource_size(res)), params.altmap);
 	}
 
 	mem_hotplug_done();
