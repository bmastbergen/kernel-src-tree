mm/memory_hotplug: add pgprot_t to mhp_params

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Logan Gunthorpe <logang@deltatee.com>
commit bfeb022f8fe4c5afdcfd7a3d868fac9765f9bcad
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/bfeb022f.failed

devm_memremap_pages() is currently used by the PCI P2PDMA code to create
struct page mappings for IO memory.  At present, these mappings are
created with PAGE_KERNEL which implies setting the PAT bits to be WB.
However, on x86, an mtrr register will typically override this and force
the cache type to be UC-.  In the case firmware doesn't set this
register it is effectively WB and will typically result in a machine
check exception when it's accessed.

Other arches are not currently likely to function correctly seeing they
don't have any MTRR registers to fall back on.

To solve this, provide a way to specify the pgprot value explicitly to
arch_add_memory().

Of the arches that support MEMORY_HOTPLUG: x86_64, and arm64 need a
simple change to pass the pgprot_t down to their respective functions
which set up the page tables.  For x86_32, set the page tables
explicitly using _set_memory_prot() (seeing they are already mapped).

For ia64, s390 and sh, reject anything but PAGE_KERNEL settings -- this
should be fine, for now, seeing these architectures don't support
ZONE_DEVICE.

A check in __add_pages() is also added to ensure the pgprot parameter
was set for all arches.

	Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: David Hildenbrand <david@redhat.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Dan Williams <dan.j.williams@intel.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Eric Badger <ebadger@gigaio.com>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Will Deacon <will@kernel.org>
Link: http://lkml.kernel.org/r/20200306170846.9333-7-logang@deltatee.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit bfeb022f8fe4c5afdcfd7a3d868fac9765f9bcad)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/ia64/mm/init.c
#	arch/powerpc/mm/mem.c
#	arch/x86/mm/init_64.c
#	include/linux/memory_hotplug.h
#	mm/memory_hotplug.c
diff --cc arch/ia64/mm/init.c
index 7647c5debd81,d637b4ea3147..000000000000
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@@ -652,7 -676,10 +652,14 @@@ int arch_add_memory(int nid, u64 start
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = __add_pages(nid, start_pfn, nr_pages, restrictions);
++=======
+ 	if (WARN_ON_ONCE(params->pgprot.pgprot != PAGE_KERNEL.pgprot))
+ 		return -EINVAL;
+ 
+ 	ret = __add_pages(nid, start_pfn, nr_pages, params);
++>>>>>>> bfeb022f8fe4 (mm/memory_hotplug: add pgprot_t to mhp_params)
  	if (ret)
  		printk("%s: Problem encountered in __add_pages() as ret=%d\n",
  		       __func__,  ret);
diff --cc arch/powerpc/mm/mem.c
index fa9ce75915ae,041ed7cfd341..000000000000
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@@ -146,7 -132,8 +146,12 @@@ int __ref arch_add_memory(int nid, u64 
  	resize_hpt_for_hotplug(memblock_phys_mem_size());
  
  	start = (unsigned long)__va(start);
++<<<<<<< HEAD
 +	rc = create_section_mapping(start, start + size, nid);
++=======
+ 	rc = create_section_mapping(start, start + size, nid,
+ 				    params->pgprot);
++>>>>>>> bfeb022f8fe4 (mm/memory_hotplug: add pgprot_t to mhp_params)
  	if (rc) {
  		pr_warn("Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
  			start, start + size, rc);
diff --cc arch/x86/mm/init_64.c
index 05348c61e15d,3b289c2f75cd..000000000000
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@@ -863,9 -867,9 +863,13 @@@ int arch_add_memory(int nid, u64 start
  	unsigned long start_pfn = start >> PAGE_SHIFT;
  	unsigned long nr_pages = size >> PAGE_SHIFT;
  
++<<<<<<< HEAD
 +	init_memory_mapping(start, start + size);
++=======
+ 	init_memory_mapping(start, start + size, params->pgprot);
++>>>>>>> bfeb022f8fe4 (mm/memory_hotplug: add pgprot_t to mhp_params)
  
 -	return add_pages(nid, start_pfn, nr_pages, params);
 +	return add_pages(nid, start_pfn, nr_pages, restrictions);
  }
  
  #define PAGE_INUSE 0xFD
diff --cc include/linux/memory_hotplug.h
index 70b16b05a960,93d9ada74ddd..000000000000
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@@ -58,11 -58,14 +58,19 @@@ enum 
  };
  
  /*
++<<<<<<< HEAD
 + * Restrictions for the memory hotplug:
 + * altmap: alternative allocator for memmap array
++=======
+  * Extended parameters for memory hotplug:
+  * altmap: alternative allocator for memmap array (optional)
+  * pgprot: page protection flags to apply to newly created page tables
+  *	(required)
++>>>>>>> bfeb022f8fe4 (mm/memory_hotplug: add pgprot_t to mhp_params)
   */
 -struct mhp_params {
 +struct mhp_restrictions {
  	struct vmem_altmap *altmap;
+ 	pgprot_t pgprot;
  };
  
  /*
diff --cc mm/memory_hotplug.c
index 226a3dc8d15f,fc0aad0bc1f5..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -308,8 -309,11 +308,11 @@@ int __ref __add_pages(int nid, unsigne
  	const unsigned long end_pfn = pfn + nr_pages;
  	unsigned long cur_nr_pages;
  	int err;
 -	struct vmem_altmap *altmap = params->altmap;
 +	struct vmem_altmap *altmap = restrictions->altmap;
  
+ 	if (WARN_ON_ONCE(!params->pgprot.pgprot))
+ 		return -EINVAL;
+ 
  	err = check_hotplug_memory_addressable(pfn, nr_pages);
  	if (err)
  		return err;
@@@ -1006,7 -1005,7 +1009,11 @@@ static int online_memory_block(struct m
   */
  int __ref add_memory_resource(int nid, struct resource *res)
  {
++<<<<<<< HEAD
 +	struct mhp_restrictions restrictions = {};
++=======
+ 	struct mhp_params params = { .pgprot = PAGE_KERNEL };
++>>>>>>> bfeb022f8fe4 (mm/memory_hotplug: add pgprot_t to mhp_params)
  	u64 start, size;
  	bool new_node = false;
  	int ret;
diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
index 4e2da4771636..40eebe4a2758 100644
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@ -1079,7 +1079,8 @@ int arch_add_memory(int nid, u64 start, u64 size,
 		flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
 
 	__create_pgd_mapping(swapper_pg_dir, start, __phys_to_virt(start),
-			     size, PAGE_KERNEL, __pgd_pgtable_alloc, flags);
+			     size, params->pgprot, __pgd_pgtable_alloc,
+			     flags);
 
 	memblock_clear_nomap(start, size);
 
* Unmerged path arch/ia64/mm/init.c
* Unmerged path arch/powerpc/mm/mem.c
diff --git a/arch/s390/mm/init.c b/arch/s390/mm/init.c
index c73cc156f3ba..172c4c5f8bea 100644
--- a/arch/s390/mm/init.c
+++ b/arch/s390/mm/init.c
@@ -286,6 +286,9 @@ int arch_add_memory(int nid, u64 start, u64 size,
 	if (WARN_ON_ONCE(restrictions->altmap))
 		return -EINVAL;
 
+	if (WARN_ON_ONCE(params->pgprot.pgprot != PAGE_KERNEL.pgprot))
+		return -EINVAL;
+
 	rc = vmem_add_mapping(start, size);
 	if (rc)
 		return rc;
diff --git a/arch/sh/mm/init.c b/arch/sh/mm/init.c
index db47300bc72b..63c79d234e86 100644
--- a/arch/sh/mm/init.c
+++ b/arch/sh/mm/init.c
@@ -435,6 +435,9 @@ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 	int ret;
 
+	if (WARN_ON_ONCE(params->pgprot.pgprot != PAGE_KERNEL.pgprot)
+		return -EINVAL;
+
 	/* We only have ZONE_NORMAL, so this is easy.. */
 	ret = __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
 	if (unlikely(ret))
diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c
index a97fb2a9390b..9183e436a440 100644
--- a/arch/x86/mm/init_32.c
+++ b/arch/x86/mm/init_32.c
@@ -859,6 +859,18 @@ int arch_add_memory(int nid, u64 start, u64 size,
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
+	int ret;
+
+	/*
+	 * The page tables were already mapped at boot so if the caller
+	 * requests a different mapping type then we must change all the
+	 * pages with __set_memory_prot().
+	 */
+	if (params->pgprot.pgprot != PAGE_KERNEL.pgprot) {
+		ret = __set_memory_prot(start, nr_pages, params->pgprot);
+		if (ret)
+			return ret;
+	}
 
 	return __add_pages(nid, start_pfn, nr_pages, restrictions);
 }
* Unmerged path arch/x86/mm/init_64.c
* Unmerged path include/linux/memory_hotplug.h
* Unmerged path mm/memory_hotplug.c
diff --git a/mm/memremap.c b/mm/memremap.c
index bbf457c4f166..86e5b28e7309 100644
--- a/mm/memremap.c
+++ b/mm/memremap.c
@@ -189,8 +189,8 @@ void *memremap_pages(struct dev_pagemap *pgmap, int nid)
 		 * We do not want any optional features only our own memmap
 		 */
 		.altmap = pgmap_altmap(pgmap),
+		.pgprot = PAGE_KERNEL,
 	};
-	pgprot_t pgprot = PAGE_KERNEL;
 	int error, is_ram;
 	bool need_devmap_managed = true;
 
@@ -282,8 +282,8 @@ void *memremap_pages(struct dev_pagemap *pgmap, int nid)
 	if (nid < 0)
 		nid = numa_mem_id();
 
-	error = track_pfn_remap(NULL, &pgprot, PHYS_PFN(res->start), 0,
-			resource_size(res));
+	error = track_pfn_remap(NULL, &params.pgprot, PHYS_PFN(res->start),
+				0, resource_size(res));
 	if (error)
 		goto err_pfn_remap;
 
