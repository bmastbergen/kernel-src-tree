bpf: Introduce helper bpf_get_task_stack()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Song Liu <songliubraving@fb.com>
commit fa28dcb82a38f8e3993b0fae9106b1a80b59e4f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/fa28dcb8.failed

Introduce helper bpf_get_task_stack(), which dumps stack trace of given
task. This is different to bpf_get_stack(), which gets stack track of
current task. One potential use case of bpf_get_task_stack() is to call
it from bpf_iter__task and dump all /proc/<pid>/stack to a seq_file.

bpf_get_task_stack() uses stack_trace_save_tsk() instead of
get_perf_callchain() for kernel stack. The benefit of this choice is that
stack_trace_save_tsk() doesn't require changes in arch/. The downside of
using stack_trace_save_tsk() is that stack_trace_save_tsk() dumps the
stack trace to unsigned long array. For 32-bit systems, we need to
translate it to u64 array.

	Signed-off-by: Song Liu <songliubraving@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
Link: https://lore.kernel.org/bpf/20200630062846.664389-3-songliubraving@fb.com
(cherry picked from commit fa28dcb82a38f8e3993b0fae9106b1a80b59e4f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	kernel/trace/bpf_trace.c
#	scripts/bpf_helpers_doc.py
#	tools/include/uapi/linux/bpf.h
diff --cc include/uapi/linux/bpf.h
index af3754a78dbd,da9bf35a26f8..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -3127,6 -3138,186 +3127,189 @@@ union bpf_attr 
   * 		0 on success, or a negative error in case of failure:
   *
   *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
++<<<<<<< HEAD
++=======
+  *
+  * u64 bpf_sk_cgroup_id(struct bpf_sock *sk)
+  *	Description
+  *		Return the cgroup v2 id of the socket *sk*.
+  *
+  *		*sk* must be a non-**NULL** pointer to a full socket, e.g. one
+  *		returned from **bpf_sk_lookup_xxx**\ (),
+  *		**bpf_sk_fullsock**\ (), etc. The format of returned id is
+  *		same as in **bpf_skb_cgroup_id**\ ().
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		the **CONFIG_SOCK_CGROUP_DATA** configuration option.
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * u64 bpf_sk_ancestor_cgroup_id(struct bpf_sock *sk, int ancestor_level)
+  *	Description
+  *		Return id of cgroup v2 that is ancestor of cgroup associated
+  *		with the *sk* at the *ancestor_level*.  The root cgroup is at
+  *		*ancestor_level* zero and each step down the hierarchy
+  *		increments the level. If *ancestor_level* == level of cgroup
+  *		associated with *sk*, then return value will be same as that
+  *		of **bpf_sk_cgroup_id**\ ().
+  *
+  *		The helper is useful to implement policies based on cgroups
+  *		that are upper in hierarchy than immediate cgroup associated
+  *		with *sk*.
+  *
+  *		The format of returned id and helper limitations are same as in
+  *		**bpf_sk_cgroup_id**\ ().
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * int bpf_ringbuf_output(void *ringbuf, void *data, u64 size, u64 flags)
+  * 	Description
+  * 		Copy *size* bytes from *data* into a ring buffer *ringbuf*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		0, on success;
+  * 		< 0, on error.
+  *
+  * void *bpf_ringbuf_reserve(void *ringbuf, u64 size, u64 flags)
+  * 	Description
+  * 		Reserve *size* bytes of payload in a ring buffer *ringbuf*.
+  * 	Return
+  * 		Valid pointer with *size* bytes of memory available; NULL,
+  * 		otherwise.
+  *
+  * void bpf_ringbuf_submit(void *data, u64 flags)
+  * 	Description
+  * 		Submit reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * void bpf_ringbuf_discard(void *data, u64 flags)
+  * 	Description
+  * 		Discard reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * u64 bpf_ringbuf_query(void *ringbuf, u64 flags)
+  *	Description
+  *		Query various characteristics of provided ring buffer. What
+  *		exactly is queries is determined by *flags*:
+  *		  - BPF_RB_AVAIL_DATA - amount of data not yet consumed;
+  *		  - BPF_RB_RING_SIZE - the size of ring buffer;
+  *		  - BPF_RB_CONS_POS - consumer position (can wrap around);
+  *		  - BPF_RB_PROD_POS - producer(s) position (can wrap around);
+  *		Data returned is just a momentary snapshots of actual values
+  *		and could be inaccurate, so this facility should be used to
+  *		power heuristics and for reporting, not to make 100% correct
+  *		calculation.
+  *	Return
+  *		Requested value, or 0, if flags are not recognized.
+  *
+  * long bpf_csum_level(struct sk_buff *skb, u64 level)
+  * 	Description
+  * 		Change the skbs checksum level by one layer up or down, or
+  * 		reset it entirely to none in order to have the stack perform
+  * 		checksum validation. The level is applicable to the following
+  * 		protocols: TCP, UDP, GRE, SCTP, FCOE. For example, a decap of
+  * 		| ETH | IP | UDP | GUE | IP | TCP | into | ETH | IP | TCP |
+  * 		through **bpf_skb_adjust_room**\ () helper with passing in
+  * 		**BPF_F_ADJ_ROOM_NO_CSUM_RESET** flag would require one	call
+  * 		to **bpf_csum_level**\ () with **BPF_CSUM_LEVEL_DEC** since
+  * 		the UDP header is removed. Similarly, an encap of the latter
+  * 		into the former could be accompanied by a helper call to
+  * 		**bpf_csum_level**\ () with **BPF_CSUM_LEVEL_INC** if the
+  * 		skb is still intended to be processed in higher layers of the
+  * 		stack instead of just egressing at tc.
+  *
+  * 		There are three supported level settings at this time:
+  *
+  * 		* **BPF_CSUM_LEVEL_INC**: Increases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_DEC**: Decreases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_RESET**: Resets skb->csum_level to 0 and
+  * 		  sets CHECKSUM_NONE to force checksum validation by the stack.
+  * 		* **BPF_CSUM_LEVEL_QUERY**: No-op, returns the current
+  * 		  skb->csum_level.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure. In the
+  * 		case of **BPF_CSUM_LEVEL_QUERY**, the current skb->csum_level
+  * 		is returned or the error code -EACCES in case the skb is not
+  * 		subject to CHECKSUM_UNNECESSARY.
+  *
+  * struct tcp6_sock *bpf_skc_to_tcp6_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_sock *bpf_skc_to_tcp_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_timewait_sock *bpf_skc_to_tcp_timewait_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_timewait_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_request_sock *bpf_skc_to_tcp_request_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_request_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct udp6_sock *bpf_skc_to_udp6_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *udp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * long bpf_get_task_stack(struct task_struct *task, void *buf, u32 size, u64 flags)
+  *	Description
+  *		Return a user or a kernel stack in bpf program provided buffer.
+  *		To achieve this, the helper needs *task*, which is a valid
+  *		pointer to struct task_struct. To store the stacktrace, the
+  *		bpf program provides *buf* with	a nonnegative *size*.
+  *
+  *		The last argument, *flags*, holds the number of stack frames to
+  *		skip (from 0 to 255), masked with
+  *		**BPF_F_SKIP_FIELD_MASK**. The next bits can be used to set
+  *		the following flags:
+  *
+  *		**BPF_F_USER_STACK**
+  *			Collect a user space stack instead of a kernel stack.
+  *		**BPF_F_USER_BUILD_ID**
+  *			Collect buildid+offset instead of ips for user stack,
+  *			only valid if **BPF_F_USER_STACK** is also specified.
+  *
+  *		**bpf_get_task_stack**\ () can collect up to
+  *		**PERF_MAX_STACK_DEPTH** both kernel and user frames, subject
+  *		to sufficient large buffer size. Note that
+  *		this limit can be controlled with the **sysctl** program, and
+  *		that it should be manually increased in order to profile long
+  *		user stacks (such as stacks for Java programs). To do so, use:
+  *
+  *		::
+  *
+  *			# sysctl kernel.perf_event_max_stack=<new value>
+  *	Return
+  *		A non-negative value equal to or less than *size* on success,
+  *		or a negative error in case of failure.
+  *
++>>>>>>> fa28dcb82a38 (bpf: Introduce helper bpf_get_task_stack())
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3256,7 -3447,22 +3439,26 @@@
  	FN(sk_assign),			\
  	FN(ktime_get_boot_ns),		\
  	FN(seq_printf),			\
++<<<<<<< HEAD
 +	FN(seq_write),
++=======
+ 	FN(seq_write),			\
+ 	FN(sk_cgroup_id),		\
+ 	FN(sk_ancestor_cgroup_id),	\
+ 	FN(ringbuf_output),		\
+ 	FN(ringbuf_reserve),		\
+ 	FN(ringbuf_submit),		\
+ 	FN(ringbuf_discard),		\
+ 	FN(ringbuf_query),		\
+ 	FN(csum_level),			\
+ 	FN(skc_to_tcp6_sock),		\
+ 	FN(skc_to_tcp_sock),		\
+ 	FN(skc_to_tcp_timewait_sock),	\
+ 	FN(skc_to_tcp_request_sock),	\
+ 	FN(skc_to_udp6_sock),		\
+ 	FN(get_task_stack),		\
+ 	/* */
++>>>>>>> fa28dcb82a38 (bpf: Introduce helper bpf_get_task_stack())
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --cc kernel/trace/bpf_trace.c
index 29a70cbf26d7,977ba3b6f6c6..000000000000
--- a/kernel/trace/bpf_trace.c
+++ b/kernel/trace/bpf_trace.c
@@@ -1081,6 -1125,20 +1081,23 @@@ bpf_tracing_func_proto(enum bpf_func_i
  		return &bpf_perf_event_read_value_proto;
  	case BPF_FUNC_get_ns_current_pid_tgid:
  		return &bpf_get_ns_current_pid_tgid_proto;
++<<<<<<< HEAD
++=======
+ 	case BPF_FUNC_ringbuf_output:
+ 		return &bpf_ringbuf_output_proto;
+ 	case BPF_FUNC_ringbuf_reserve:
+ 		return &bpf_ringbuf_reserve_proto;
+ 	case BPF_FUNC_ringbuf_submit:
+ 		return &bpf_ringbuf_submit_proto;
+ 	case BPF_FUNC_ringbuf_discard:
+ 		return &bpf_ringbuf_discard_proto;
+ 	case BPF_FUNC_ringbuf_query:
+ 		return &bpf_ringbuf_query_proto;
+ 	case BPF_FUNC_jiffies64:
+ 		return &bpf_jiffies64_proto;
+ 	case BPF_FUNC_get_task_stack:
+ 		return &bpf_get_task_stack_proto;
++>>>>>>> fa28dcb82a38 (bpf: Introduce helper bpf_get_task_stack())
  	default:
  		return NULL;
  	}
diff --cc scripts/bpf_helpers_doc.py
index 91fa668fa860,6843376733df..000000000000
--- a/scripts/bpf_helpers_doc.py
+++ b/scripts/bpf_helpers_doc.py
@@@ -421,6 -421,12 +421,15 @@@ class PrinterHelpers(Printer)
              'struct sockaddr',
              'struct tcphdr',
              'struct seq_file',
++<<<<<<< HEAD
++=======
+             'struct tcp6_sock',
+             'struct tcp_sock',
+             'struct tcp_timewait_sock',
+             'struct tcp_request_sock',
+             'struct udp6_sock',
+             'struct task_struct',
++>>>>>>> fa28dcb82a38 (bpf: Introduce helper bpf_get_task_stack())
  
              'struct __sk_buff',
              'struct sk_msg_md',
@@@ -458,6 -464,12 +467,15 @@@
              'struct sockaddr',
              'struct tcphdr',
              'struct seq_file',
++<<<<<<< HEAD
++=======
+             'struct tcp6_sock',
+             'struct tcp_sock',
+             'struct tcp_timewait_sock',
+             'struct tcp_request_sock',
+             'struct udp6_sock',
+             'struct task_struct',
++>>>>>>> fa28dcb82a38 (bpf: Introduce helper bpf_get_task_stack())
      }
      mapped_types = {
              'u8': '__u8',
diff --cc tools/include/uapi/linux/bpf.h
index b34454b6dad0,da9bf35a26f8..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -3081,25 -3116,208 +3081,209 @@@ union bpf_attr 
   *		valid address but requiring a major memory fault. If reading kernel memory
   *		fails, the string for **%s** will be an empty string, and the ip
   *		address for **%p{i,I}{4,6}** will be 0. Not returning error to
 - *		bpf program is consistent with what **bpf_trace_printk**\ () does for now.
 + *		bpf program is consistent with what bpf_trace_printk() does for now.
   * 	Return
 - * 		0 on success, or a negative error in case of failure:
 - *
 - *		**-EBUSY** if per-CPU memory copy buffer is busy, can try again
 - *		by returning 1 from bpf program.
 - *
 - *		**-EINVAL** if arguments are invalid, or if *fmt* is invalid/unsupported.
 + * 		0 on success, or a negative errno in case of failure.
   *
 - *		**-E2BIG** if *fmt* contains too many format specifiers.
 + *		* **-EBUSY**		Percpu memory copy buffer is busy, can try again
 + *					by returning 1 from bpf program.
 + *		* **-EINVAL**		Invalid arguments, or invalid/unsupported formats.
 + *		* **-E2BIG**		Too many format specifiers.
 + *		* **-EOVERFLOW**	Overflow happens, the same object will be tried again.
   *
 - *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
 - *
 - * long bpf_seq_write(struct seq_file *m, const void *data, u32 len)
 + * int bpf_seq_write(struct seq_file *m, const void *data, u32 len)
   * 	Description
 - * 		**bpf_seq_write**\ () uses seq_file **seq_write**\ () to write the data.
 + * 		seq_write uses seq_file seq_write() to write the data.
   * 		The *m* represents the seq_file. The *data* and *len* represent the
 - * 		data to write in bytes.
 + *		data to write in bytes.
   * 	Return
 - * 		0 on success, or a negative error in case of failure:
 + * 		0 on success, or a negative errno in case of failure.
   *
++<<<<<<< HEAD
 + *		* **-EOVERFLOW**	Overflow happens, the same object will be tried again.
++=======
+  *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
+  *
+  * u64 bpf_sk_cgroup_id(struct bpf_sock *sk)
+  *	Description
+  *		Return the cgroup v2 id of the socket *sk*.
+  *
+  *		*sk* must be a non-**NULL** pointer to a full socket, e.g. one
+  *		returned from **bpf_sk_lookup_xxx**\ (),
+  *		**bpf_sk_fullsock**\ (), etc. The format of returned id is
+  *		same as in **bpf_skb_cgroup_id**\ ().
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		the **CONFIG_SOCK_CGROUP_DATA** configuration option.
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * u64 bpf_sk_ancestor_cgroup_id(struct bpf_sock *sk, int ancestor_level)
+  *	Description
+  *		Return id of cgroup v2 that is ancestor of cgroup associated
+  *		with the *sk* at the *ancestor_level*.  The root cgroup is at
+  *		*ancestor_level* zero and each step down the hierarchy
+  *		increments the level. If *ancestor_level* == level of cgroup
+  *		associated with *sk*, then return value will be same as that
+  *		of **bpf_sk_cgroup_id**\ ().
+  *
+  *		The helper is useful to implement policies based on cgroups
+  *		that are upper in hierarchy than immediate cgroup associated
+  *		with *sk*.
+  *
+  *		The format of returned id and helper limitations are same as in
+  *		**bpf_sk_cgroup_id**\ ().
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * int bpf_ringbuf_output(void *ringbuf, void *data, u64 size, u64 flags)
+  * 	Description
+  * 		Copy *size* bytes from *data* into a ring buffer *ringbuf*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		0, on success;
+  * 		< 0, on error.
+  *
+  * void *bpf_ringbuf_reserve(void *ringbuf, u64 size, u64 flags)
+  * 	Description
+  * 		Reserve *size* bytes of payload in a ring buffer *ringbuf*.
+  * 	Return
+  * 		Valid pointer with *size* bytes of memory available; NULL,
+  * 		otherwise.
+  *
+  * void bpf_ringbuf_submit(void *data, u64 flags)
+  * 	Description
+  * 		Submit reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * void bpf_ringbuf_discard(void *data, u64 flags)
+  * 	Description
+  * 		Discard reserved ring buffer sample, pointed to by *data*.
+  * 		If BPF_RB_NO_WAKEUP is specified in *flags*, no notification of
+  * 		new data availability is sent.
+  * 		IF BPF_RB_FORCE_WAKEUP is specified in *flags*, notification of
+  * 		new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * u64 bpf_ringbuf_query(void *ringbuf, u64 flags)
+  *	Description
+  *		Query various characteristics of provided ring buffer. What
+  *		exactly is queries is determined by *flags*:
+  *		  - BPF_RB_AVAIL_DATA - amount of data not yet consumed;
+  *		  - BPF_RB_RING_SIZE - the size of ring buffer;
+  *		  - BPF_RB_CONS_POS - consumer position (can wrap around);
+  *		  - BPF_RB_PROD_POS - producer(s) position (can wrap around);
+  *		Data returned is just a momentary snapshots of actual values
+  *		and could be inaccurate, so this facility should be used to
+  *		power heuristics and for reporting, not to make 100% correct
+  *		calculation.
+  *	Return
+  *		Requested value, or 0, if flags are not recognized.
+  *
+  * long bpf_csum_level(struct sk_buff *skb, u64 level)
+  * 	Description
+  * 		Change the skbs checksum level by one layer up or down, or
+  * 		reset it entirely to none in order to have the stack perform
+  * 		checksum validation. The level is applicable to the following
+  * 		protocols: TCP, UDP, GRE, SCTP, FCOE. For example, a decap of
+  * 		| ETH | IP | UDP | GUE | IP | TCP | into | ETH | IP | TCP |
+  * 		through **bpf_skb_adjust_room**\ () helper with passing in
+  * 		**BPF_F_ADJ_ROOM_NO_CSUM_RESET** flag would require one	call
+  * 		to **bpf_csum_level**\ () with **BPF_CSUM_LEVEL_DEC** since
+  * 		the UDP header is removed. Similarly, an encap of the latter
+  * 		into the former could be accompanied by a helper call to
+  * 		**bpf_csum_level**\ () with **BPF_CSUM_LEVEL_INC** if the
+  * 		skb is still intended to be processed in higher layers of the
+  * 		stack instead of just egressing at tc.
+  *
+  * 		There are three supported level settings at this time:
+  *
+  * 		* **BPF_CSUM_LEVEL_INC**: Increases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_DEC**: Decreases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_RESET**: Resets skb->csum_level to 0 and
+  * 		  sets CHECKSUM_NONE to force checksum validation by the stack.
+  * 		* **BPF_CSUM_LEVEL_QUERY**: No-op, returns the current
+  * 		  skb->csum_level.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure. In the
+  * 		case of **BPF_CSUM_LEVEL_QUERY**, the current skb->csum_level
+  * 		is returned or the error code -EACCES in case the skb is not
+  * 		subject to CHECKSUM_UNNECESSARY.
+  *
+  * struct tcp6_sock *bpf_skc_to_tcp6_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_sock *bpf_skc_to_tcp_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_timewait_sock *bpf_skc_to_tcp_timewait_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_timewait_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_request_sock *bpf_skc_to_tcp_request_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_request_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct udp6_sock *bpf_skc_to_udp6_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *udp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * long bpf_get_task_stack(struct task_struct *task, void *buf, u32 size, u64 flags)
+  *	Description
+  *		Return a user or a kernel stack in bpf program provided buffer.
+  *		To achieve this, the helper needs *task*, which is a valid
+  *		pointer to struct task_struct. To store the stacktrace, the
+  *		bpf program provides *buf* with	a nonnegative *size*.
+  *
+  *		The last argument, *flags*, holds the number of stack frames to
+  *		skip (from 0 to 255), masked with
+  *		**BPF_F_SKIP_FIELD_MASK**. The next bits can be used to set
+  *		the following flags:
+  *
+  *		**BPF_F_USER_STACK**
+  *			Collect a user space stack instead of a kernel stack.
+  *		**BPF_F_USER_BUILD_ID**
+  *			Collect buildid+offset instead of ips for user stack,
+  *			only valid if **BPF_F_USER_STACK** is also specified.
+  *
+  *		**bpf_get_task_stack**\ () can collect up to
+  *		**PERF_MAX_STACK_DEPTH** both kernel and user frames, subject
+  *		to sufficient large buffer size. Note that
+  *		this limit can be controlled with the **sysctl** program, and
+  *		that it should be manually increased in order to profile long
+  *		user stacks (such as stacks for Java programs). To do so, use:
+  *
+  *		::
+  *
+  *			# sysctl kernel.perf_event_max_stack=<new value>
+  *	Return
+  *		A non-negative value equal to or less than *size* on success,
+  *		or a negative error in case of failure.
+  *
++>>>>>>> fa28dcb82a38 (bpf: Introduce helper bpf_get_task_stack())
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
@@@ -3229,7 -3447,22 +3413,26 @@@
  	FN(sk_assign),			\
  	FN(ktime_get_boot_ns),		\
  	FN(seq_printf),			\
++<<<<<<< HEAD
 +	FN(seq_write),
++=======
+ 	FN(seq_write),			\
+ 	FN(sk_cgroup_id),		\
+ 	FN(sk_ancestor_cgroup_id),	\
+ 	FN(ringbuf_output),		\
+ 	FN(ringbuf_reserve),		\
+ 	FN(ringbuf_submit),		\
+ 	FN(ringbuf_discard),		\
+ 	FN(ringbuf_query),		\
+ 	FN(csum_level),			\
+ 	FN(skc_to_tcp6_sock),		\
+ 	FN(skc_to_tcp_sock),		\
+ 	FN(skc_to_tcp_timewait_sock),	\
+ 	FN(skc_to_tcp_request_sock),	\
+ 	FN(skc_to_udp6_sock),		\
+ 	FN(get_task_stack),		\
+ 	/* */
++>>>>>>> fa28dcb82a38 (bpf: Introduce helper bpf_get_task_stack())
  
  /* integer value in 'imm' field of BPF_CALL instruction selects which helper
   * function eBPF program intends to call
diff --git a/include/linux/bpf.h b/include/linux/bpf.h
index 531af1e44e56..3879a5571929 100644
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@ -1576,6 +1576,7 @@ extern const struct bpf_func_proto bpf_get_current_uid_gid_proto;
 extern const struct bpf_func_proto bpf_get_current_comm_proto;
 extern const struct bpf_func_proto bpf_get_stackid_proto;
 extern const struct bpf_func_proto bpf_get_stack_proto;
+extern const struct bpf_func_proto bpf_get_task_stack_proto;
 extern const struct bpf_func_proto bpf_sock_map_update_proto;
 extern const struct bpf_func_proto bpf_sock_hash_update_proto;
 extern const struct bpf_func_proto bpf_get_current_cgroup_id_proto;
* Unmerged path include/uapi/linux/bpf.h
diff --git a/kernel/bpf/stackmap.c b/kernel/bpf/stackmap.c
index 4153fd4ee538..141848843d2a 100644
--- a/kernel/bpf/stackmap.c
+++ b/kernel/bpf/stackmap.c
@@ -358,6 +358,40 @@ static void stack_map_get_build_id_offset(struct bpf_stack_build_id *id_offs,
 	}
 }
 
+static struct perf_callchain_entry *
+get_callchain_entry_for_task(struct task_struct *task, u32 init_nr)
+{
+	struct perf_callchain_entry *entry;
+	int rctx;
+
+	entry = get_callchain_entry(&rctx);
+
+	if (!entry)
+		return NULL;
+
+	entry->nr = init_nr +
+		stack_trace_save_tsk(task, (unsigned long *)(entry->ip + init_nr),
+				     sysctl_perf_event_max_stack - init_nr, 0);
+
+	/* stack_trace_save_tsk() works on unsigned long array, while
+	 * perf_callchain_entry uses u64 array. For 32-bit systems, it is
+	 * necessary to fix this mismatch.
+	 */
+	if (__BITS_PER_LONG != 64) {
+		unsigned long *from = (unsigned long *) entry->ip;
+		u64 *to = entry->ip;
+		int i;
+
+		/* copy data from the end to avoid using extra buffer */
+		for (i = entry->nr - 1; i >= (int)init_nr; i--)
+			to[i] = (u64)(from[i]);
+	}
+
+	put_callchain_entry(rctx);
+
+	return entry;
+}
+
 BPF_CALL_3(bpf_get_stackid, struct pt_regs *, regs, struct bpf_map *, map,
 	   u64, flags)
 {
@@ -458,8 +492,8 @@ const struct bpf_func_proto bpf_get_stackid_proto = {
 	.arg3_type	= ARG_ANYTHING,
 };
 
-BPF_CALL_4(bpf_get_stack, struct pt_regs *, regs, void *, buf, u32, size,
-	   u64, flags)
+static long __bpf_get_stack(struct pt_regs *regs, struct task_struct *task,
+			    void *buf, u32 size, u64 flags)
 {
 	u32 init_nr, trace_nr, copy_len, elem_size, num_elem;
 	bool user_build_id = flags & BPF_F_USER_BUILD_ID;
@@ -481,13 +515,22 @@ BPF_CALL_4(bpf_get_stack, struct pt_regs *, regs, void *, buf, u32, size,
 	if (unlikely(size % elem_size))
 		goto clear;
 
+	/* cannot get valid user stack for task without user_mode regs */
+	if (task && user && !user_mode(regs))
+		goto err_fault;
+
 	num_elem = size / elem_size;
 	if (sysctl_perf_event_max_stack < num_elem)
 		init_nr = 0;
 	else
 		init_nr = sysctl_perf_event_max_stack - num_elem;
-	trace = get_perf_callchain(regs, init_nr, kernel, user,
-				   sysctl_perf_event_max_stack, false, false);
+
+	if (kernel && task)
+		trace = get_callchain_entry_for_task(task, init_nr);
+	else
+		trace = get_perf_callchain(regs, init_nr, kernel, user,
+					   sysctl_perf_event_max_stack,
+					   false, false);
 	if (unlikely(!trace))
 		goto err_fault;
 
@@ -515,6 +558,12 @@ BPF_CALL_4(bpf_get_stack, struct pt_regs *, regs, void *, buf, u32, size,
 	return err;
 }
 
+BPF_CALL_4(bpf_get_stack, struct pt_regs *, regs, void *, buf, u32, size,
+	   u64, flags)
+{
+	return __bpf_get_stack(regs, NULL, buf, size, flags);
+}
+
 const struct bpf_func_proto bpf_get_stack_proto = {
 	.func		= bpf_get_stack,
 	.gpl_only	= true,
@@ -525,6 +574,26 @@ const struct bpf_func_proto bpf_get_stack_proto = {
 	.arg4_type	= ARG_ANYTHING,
 };
 
+BPF_CALL_4(bpf_get_task_stack, struct task_struct *, task, void *, buf,
+	   u32, size, u64, flags)
+{
+	struct pt_regs *regs = task_pt_regs(task);
+
+	return __bpf_get_stack(regs, task, buf, size, flags);
+}
+
+static int bpf_get_task_stack_btf_ids[5];
+const struct bpf_func_proto bpf_get_task_stack_proto = {
+	.func		= bpf_get_task_stack,
+	.gpl_only	= false,
+	.ret_type	= RET_INTEGER,
+	.arg1_type	= ARG_PTR_TO_BTF_ID,
+	.arg2_type	= ARG_PTR_TO_UNINIT_MEM,
+	.arg3_type	= ARG_CONST_SIZE_OR_ZERO,
+	.arg4_type	= ARG_ANYTHING,
+	.btf_id		= bpf_get_task_stack_btf_ids,
+};
+
 /* Called from eBPF program */
 static void *stack_map_lookup_elem(struct bpf_map *map, void *key)
 {
diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c
index b8aec186e303..0cf39df56914 100644
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@ -4692,7 +4692,9 @@ static int check_helper_call(struct bpf_verifier_env *env, int func_id, int insn
 	if (err)
 		return err;
 
-	if (func_id == BPF_FUNC_get_stack && !env->prog->has_callchain_buf) {
+	if ((func_id == BPF_FUNC_get_stack ||
+	     func_id == BPF_FUNC_get_task_stack) &&
+	    !env->prog->has_callchain_buf) {
 		const char *err_str;
 
 #ifdef CONFIG_PERF_EVENTS
* Unmerged path kernel/trace/bpf_trace.c
* Unmerged path scripts/bpf_helpers_doc.py
* Unmerged path tools/include/uapi/linux/bpf.h
