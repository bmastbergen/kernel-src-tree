mm: replace memmap_context by meminit_context

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Laurent Dufour <ldufour@linux.ibm.com>
commit c1d0da83358a2316d9be7f229f26126dbaa07468
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/c1d0da83.failed

Patch series "mm: fix memory to node bad links in sysfs", v3.

Sometimes, firmware may expose interleaved memory layout like this:

 Early memory node ranges
   node   1: [mem 0x0000000000000000-0x000000011fffffff]
   node   2: [mem 0x0000000120000000-0x000000014fffffff]
   node   1: [mem 0x0000000150000000-0x00000001ffffffff]
   node   0: [mem 0x0000000200000000-0x000000048fffffff]
   node   2: [mem 0x0000000490000000-0x00000007ffffffff]

In that case, we can see memory blocks assigned to multiple nodes in
sysfs:

  $ ls -l /sys/devices/system/memory/memory21
  total 0
  lrwxrwxrwx 1 root root     0 Aug 24 05:27 node1 -> ../../node/node1
  lrwxrwxrwx 1 root root     0 Aug 24 05:27 node2 -> ../../node/node2
  -rw-r--r-- 1 root root 65536 Aug 24 05:27 online
  -r--r--r-- 1 root root 65536 Aug 24 05:27 phys_device
  -r--r--r-- 1 root root 65536 Aug 24 05:27 phys_index
  drwxr-xr-x 2 root root     0 Aug 24 05:27 power
  -r--r--r-- 1 root root 65536 Aug 24 05:27 removable
  -rw-r--r-- 1 root root 65536 Aug 24 05:27 state
  lrwxrwxrwx 1 root root     0 Aug 24 05:25 subsystem -> ../../../../bus/memory
  -rw-r--r-- 1 root root 65536 Aug 24 05:25 uevent
  -r--r--r-- 1 root root 65536 Aug 24 05:27 valid_zones

The same applies in the node's directory with a memory21 link in both
the node1 and node2's directory.

This is wrong but doesn't prevent the system to run.  However when
later, one of these memory blocks is hot-unplugged and then hot-plugged,
the system is detecting an inconsistency in the sysfs layout and a
BUG_ON() is raised:

  kernel BUG at /Users/laurent/src/linux-ppc/mm/memory_hotplug.c:1084!
  LE PAGE_SIZE=64K MMU=Hash SMP NR_CPUS=2048 NUMA pSeries
  Modules linked in: rpadlpar_io rpaphp pseries_rng rng_core vmx_crypto gf128mul binfmt_misc ip_tables x_tables xfs libcrc32c crc32c_vpmsum autofs4
  CPU: 8 PID: 10256 Comm: drmgr Not tainted 5.9.0-rc1+ #25
  Call Trace:
    add_memory_resource+0x23c/0x340 (unreliable)
    __add_memory+0x5c/0xf0
    dlpar_add_lmb+0x1b4/0x500
    dlpar_memory+0x1f8/0xb80
    handle_dlpar_errorlog+0xc0/0x190
    dlpar_store+0x198/0x4a0
    kobj_attr_store+0x30/0x50
    sysfs_kf_write+0x64/0x90
    kernfs_fop_write+0x1b0/0x290
    vfs_write+0xe8/0x290
    ksys_write+0xdc/0x130
    system_call_exception+0x160/0x270
    system_call_common+0xf0/0x27c

This has been seen on PowerPC LPAR.

The root cause of this issue is that when node's memory is registered,
the range used can overlap another node's range, thus the memory block
is registered to multiple nodes in sysfs.

There are two issues here:

 (a) The sysfs memory and node's layouts are broken due to these
     multiple links

 (b) The link errors in link_mem_sections() should not lead to a system
     panic.

To address (a) register_mem_sect_under_node should not rely on the
system state to detect whether the link operation is triggered by a hot
plug operation or not.  This is addressed by the patches 1 and 2 of this
series.

Issue (b) will be addressed separately.

This patch (of 2):

The memmap_context enum is used to detect whether a memory operation is
due to a hot-add operation or happening at boot time.

Make it general to the hotplug operation and rename it as
meminit_context.

There is no functional change introduced by this patch

	Suggested-by: David Hildenbrand <david@redhat.com>
	Signed-off-by: Laurent Dufour <ldufour@linux.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: David Hildenbrand <david@redhat.com>
	Reviewed-by: Oscar Salvador <osalvador@suse.de>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: "Rafael J . Wysocki" <rafael@kernel.org>
	Cc: Nathan Lynch <nathanl@linux.ibm.com>
	Cc: Scott Cheloha <cheloha@linux.ibm.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Fenghua Yu <fenghua.yu@intel.com>
	Cc: <stable@vger.kernel.org>
Link: https://lkml.kernel.org/r/20200915094143.79181-1-ldufour@linux.ibm.com
Link: https://lkml.kernel.org/r/20200915132624.9723-1-ldufour@linux.ibm.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c1d0da83358a2316d9be7f229f26126dbaa07468)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmzone.h
#	mm/page_alloc.c
diff --cc include/linux/mmzone.h
index 142aaa10ed9e,0f7a4ff4b059..000000000000
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@@ -862,19 -815,24 +862,31 @@@ static inline int zone_id(const struct 
  
  void build_all_zonelists(pg_data_t *pgdat);
  void wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,
 -		   enum zone_type highest_zoneidx);
 +		   enum zone_type classzone_idx);
  bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 -			 int highest_zoneidx, unsigned int alloc_flags,
 +			 int classzone_idx, unsigned int alloc_flags,
  			 long free_pages);
  bool zone_watermark_ok(struct zone *z, unsigned int order,
 -		unsigned long mark, int highest_zoneidx,
 +		unsigned long mark, int classzone_idx,
  		unsigned int alloc_flags);
  bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
++<<<<<<< HEAD
 +		unsigned long mark, int classzone_idx);
 +enum memmap_context {
 +	MEMMAP_EARLY,
 +	MEMMAP_HOTPLUG,
++=======
+ 		unsigned long mark, int highest_zoneidx);
+ /*
+  * Memory initialization context, use to differentiate memory added by
+  * the platform statically or via memory hotplug interface.
+  */
+ enum meminit_context {
+ 	MEMINIT_EARLY,
+ 	MEMINIT_HOTPLUG,
++>>>>>>> c1d0da83358a (mm: replace memmap_context by meminit_context)
  };
+ 
  extern void init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
  				     unsigned long size);
  
diff --cc mm/page_alloc.c
index 0f3a8eeb4d26,5661fa164f13..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -5824,15 -6007,7 +5824,19 @@@ void __meminit memmap_init_zone(unsigne
  		 * There can be holes in boot-time mem_map[]s handed to this
  		 * function.  They do not exist on hotplugged memory.
  		 */
++<<<<<<< HEAD
 +		if (context == MEMMAP_EARLY) {
 +			if (!early_pfn_valid(pfn)) {
 +				pfn = next_pfn(pfn);
 +				continue;
 +			}
 +			if (!early_pfn_in_nid(pfn, nid)) {
 +				pfn++;
 +				continue;
 +			}
++=======
+ 		if (context == MEMINIT_EARLY) {
++>>>>>>> c1d0da83358a (mm: replace memmap_context by meminit_context)
  			if (overlap_memmap_init(zone, &pfn))
  				continue;
  			if (defer_init(nid, pfn, end_pfn))
@@@ -5947,10 -6122,25 +5951,32 @@@ static void __meminit zone_init_free_li
  	}
  }
  
++<<<<<<< HEAD
 +#ifndef __HAVE_ARCH_MEMMAP_INIT
 +#define memmap_init(size, nid, zone, start_pfn) \
 +	memmap_init_zone((size), (nid), (zone), (start_pfn), MEMMAP_EARLY, NULL)
 +#endif
++=======
+ void __meminit __weak memmap_init(unsigned long size, int nid,
+ 				  unsigned long zone,
+ 				  unsigned long range_start_pfn)
+ {
+ 	unsigned long start_pfn, end_pfn;
+ 	unsigned long range_end_pfn = range_start_pfn + size;
+ 	int i;
+ 
+ 	for_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {
+ 		start_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);
+ 		end_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);
+ 
+ 		if (end_pfn > start_pfn) {
+ 			size = end_pfn - start_pfn;
+ 			memmap_init_zone(size, nid, zone, start_pfn,
+ 					 MEMINIT_EARLY, NULL);
+ 		}
+ 	}
+ }
++>>>>>>> c1d0da83358a (mm: replace memmap_context by meminit_context)
  
  static int zone_batchsize(struct zone *zone)
  {
diff --git a/arch/ia64/mm/init.c b/arch/ia64/mm/init.c
index 7647c5debd81..1cbeecd96565 100644
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@ -498,7 +498,7 @@ virtual_memmap_init(u64 start, u64 end, void *arg)
 	if (map_start < map_end)
 		memmap_init_zone((unsigned long)(map_end - map_start),
 				 args->nid, args->zone, page_to_pfn(map_start),
-				 MEMMAP_EARLY, NULL);
+				 MEMINIT_EARLY, NULL);
 	return 0;
 }
 
@@ -507,8 +507,8 @@ memmap_init (unsigned long size, int nid, unsigned long zone,
 	     unsigned long start_pfn)
 {
 	if (!vmem_map) {
-		memmap_init_zone(size, nid, zone, start_pfn, MEMMAP_EARLY,
-				NULL);
+		memmap_init_zone(size, nid, zone, start_pfn,
+				 MEMINIT_EARLY, NULL);
 	} else {
 		struct page *start;
 		struct memmap_init_callback_data args;
diff --git a/include/linux/mm.h b/include/linux/mm.h
index ef77bd76b21c..e2ebd9f19e6d 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2101,7 +2101,7 @@ extern int __meminit __early_pfn_to_nid(unsigned long pfn,
 
 extern void set_dma_reserve(unsigned long new_dma_reserve);
 extern void memmap_init_zone(unsigned long, int, unsigned long, unsigned long,
-		enum memmap_context, struct vmem_altmap *);
+		enum meminit_context, struct vmem_altmap *);
 extern void setup_per_zone_wmarks(void);
 extern int __meminit init_per_zone_wmark_min(void);
 extern void mem_init(void);
* Unmerged path include/linux/mmzone.h
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index b425e9969aa2..7ee847d50f11 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -706,7 +706,7 @@ void __ref move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
 	 * are reserved so nobody should be touching them so we should be safe
 	 */
 	memmap_init_zone(nr_pages, nid, zone_idx(zone), start_pfn,
-			MEMMAP_HOTPLUG, altmap);
+			 MEMINIT_HOTPLUG, altmap);
 
 	set_zone_contiguous(zone);
 }
* Unmerged path mm/page_alloc.c
