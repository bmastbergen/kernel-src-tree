mm: convert zone->managed_pages to atomic variable

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Arun KS <arunks@codeaurora.org>
commit 9705bea5f833f4fc21d5bef5fce7348427f76ea4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/9705bea5.failed

totalram_pages, zone->managed_pages and totalhigh_pages updates are
protected by managed_page_count_lock, but readers never care about it.
Convert these variables to atomic to avoid readers potentially seeing a
store tear.

This patch converts zone->managed_pages.  Subsequent patches will convert
totalram_panges, totalhigh_pages and eventually managed_page_count_lock
will be removed.

Main motivation was that managed_page_count_lock handling was complicating
things.  It was discussed in length here,
https://lore.kernel.org/patchwork/patch/995739/#1181785 So it seemes
better to remove the lock and convert variables to atomic, with preventing
poteintial store-to-read tearing as a bonus.

Link: http://lkml.kernel.org/r/1542090790-21750-3-git-send-email-arunks@codeaurora.org
	Signed-off-by: Arun KS <arunks@codeaurora.org>
	Suggested-by: Michal Hocko <mhocko@suse.com>
	Suggested-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Reviewed-by: David Hildenbrand <david@redhat.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 9705bea5f833f4fc21d5bef5fce7348427f76ea4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index 65ed4f0345ce,4b5c4ff68f18..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -6392,6 -6285,72 +6392,75 @@@ static unsigned long __paginginit calc_
  	return PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ static void pgdat_init_split_queue(struct pglist_data *pgdat)
+ {
+ 	spin_lock_init(&pgdat->split_queue_lock);
+ 	INIT_LIST_HEAD(&pgdat->split_queue);
+ 	pgdat->split_queue_len = 0;
+ }
+ #else
+ static void pgdat_init_split_queue(struct pglist_data *pgdat) {}
+ #endif
+ 
+ #ifdef CONFIG_COMPACTION
+ static void pgdat_init_kcompactd(struct pglist_data *pgdat)
+ {
+ 	init_waitqueue_head(&pgdat->kcompactd_wait);
+ }
+ #else
+ static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}
+ #endif
+ 
+ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)
+ {
+ 	pgdat_resize_init(pgdat);
+ 
+ 	pgdat_init_split_queue(pgdat);
+ 	pgdat_init_kcompactd(pgdat);
+ 
+ 	init_waitqueue_head(&pgdat->kswapd_wait);
+ 	init_waitqueue_head(&pgdat->pfmemalloc_wait);
+ 
+ 	pgdat_page_ext_init(pgdat);
+ 	spin_lock_init(&pgdat->lru_lock);
+ 	lruvec_init(node_lruvec(pgdat));
+ }
+ 
+ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,
+ 							unsigned long remaining_pages)
+ {
+ 	atomic_long_set(&zone->managed_pages, remaining_pages);
+ 	zone_set_nid(zone, nid);
+ 	zone->name = zone_names[idx];
+ 	zone->zone_pgdat = NODE_DATA(nid);
+ 	spin_lock_init(&zone->lock);
+ 	zone_seqlock_init(zone);
+ 	zone_pcp_init(zone);
+ }
+ 
+ /*
+  * Set up the zone data structures
+  * - init pgdat internals
+  * - init all zones belonging to this node
+  *
+  * NOTE: this function is only called during memory hotplug
+  */
+ #ifdef CONFIG_MEMORY_HOTPLUG
+ void __ref free_area_init_core_hotplug(int nid)
+ {
+ 	enum zone_type z;
+ 	pg_data_t *pgdat = NODE_DATA(nid);
+ 
+ 	pgdat_init_internals(pgdat);
+ 	for (z = 0; z < MAX_NR_ZONES; z++)
+ 		zone_init_internals(&pgdat->node_zones[z], z, nid, 0);
+ }
+ #endif
+ 
++>>>>>>> 9705bea5f833 (mm: convert zone->managed_pages to atomic variable)
  /*
   * Set up the zone data structures:
   *   - mark all pages reserved
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9b2a1294b7c0..29ad10e2fb76 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -511,7 +511,7 @@ struct zone {
 	 * adjust_managed_page_count() should be used instead of directly
 	 * touching zone->managed_pages and totalram_pages.
 	 */
-	unsigned long		managed_pages;
+	atomic_long_t		managed_pages;
 	unsigned long		spanned_pages;
 	unsigned long		present_pages;
 
@@ -604,6 +604,11 @@ enum pgdat_flags {
 	PGDAT_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
 };
 
+static inline unsigned long zone_managed_pages(struct zone *zone)
+{
+	return (unsigned long)atomic_long_read(&zone->managed_pages);
+}
+
 static inline unsigned long zone_end_pfn(const struct zone *zone)
 {
 	return zone->zone_start_pfn + zone->spanned_pages;
@@ -904,7 +909,7 @@ static inline int local_memory_node(int node_id) { return node_id; };
  */
 static inline bool managed_zone(struct zone *zone)
 {
-	return zone->managed_pages;
+	return zone_managed_pages(zone);
 }
 
 /* Returns true if a zone has memory */
diff --git a/lib/show_mem.c b/lib/show_mem.c
index 0beaa1d899aa..eefe67d50e84 100644
--- a/lib/show_mem.c
+++ b/lib/show_mem.c
@@ -28,7 +28,7 @@ void show_mem(unsigned int filter, nodemask_t *nodemask)
 				continue;
 
 			total += zone->present_pages;
-			reserved += zone->present_pages - zone->managed_pages;
+			reserved += zone->present_pages - zone_managed_pages(zone);
 
 			if (is_highmem_idx(zoneid))
 				highmem += zone->present_pages;
diff --git a/mm/memblock.c b/mm/memblock.c
index f519399b14b5..73e3bb36b360 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -2017,7 +2017,7 @@ void reset_node_managed_pages(pg_data_t *pgdat)
 	struct zone *z;
 
 	for (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)
-		z->managed_pages = 0;
+		atomic_long_set(&z->managed_pages, 0);
 }
 
 void __init reset_all_zones_managed_pages(void)
* Unmerged path mm/page_alloc.c
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 8285022e8e12..b523159d0fab 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -227,7 +227,7 @@ int calculate_normal_threshold(struct zone *zone)
 	 * 125		1024		10	16-32 GB	9
 	 */
 
-	mem = zone->managed_pages >> (27 - PAGE_SHIFT);
+	mem = zone_managed_pages(zone) >> (27 - PAGE_SHIFT);
 
 	threshold = 2 * fls(num_online_cpus()) * (1 + fls(mem));
 
@@ -1579,7 +1579,7 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 		   high_wmark_pages(zone),
 		   zone->spanned_pages,
 		   zone->present_pages,
-		   zone->managed_pages);
+		   zone_managed_pages(zone));
 
 	seq_printf(m,
 		   "\n        protection: (%ld",
