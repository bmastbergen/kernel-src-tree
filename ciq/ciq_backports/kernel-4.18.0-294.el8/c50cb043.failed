KVM: arm64: Remove __hyp_text macro, use build rules instead

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author David Brazdil <dbrazdil@google.com>
commit c50cb04303cb88c517715b078e3e010c024af1a5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/c50cb043.failed

With nVHE code now fully separated from the rest of the kernel, the effects of
the __hyp_text macro (which had to be applied on all nVHE code) can be
achieved with build rules instead. The macro used to:
  (a) move code to .hyp.text ELF section, now done by renaming .text using
      `objcopy`, and
  (b) `notrace` and `__noscs` would negate effects of CC_FLAGS_FTRACE and
      CC_FLAGS_SCS, respectivelly, now those flags are  erased from
      KBUILD_CFLAGS (same way as in EFI stub).

Note that by removing __hyp_text from code shared with VHE, all VHE code is now
compiled into .text and without `notrace` and `__noscs`.

Use of '.pushsection .hyp.text' removed from assembly files as this is now also
covered by the build rules.

For MAINTAINERS: if needed to re-run, uses of macro were removed with the
following command. Formatting was fixed up manually.

  find arch/arm64/kvm/hyp -type f -name '*.c' -o -name '*.h' \
       -exec sed -i 's/ __hyp_text//g' {} +

	Signed-off-by: David Brazdil <dbrazdil@google.com>
	Signed-off-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20200625131420.71444-15-dbrazdil@google.com
(cherry picked from commit c50cb04303cb88c517715b078e3e010c024af1a5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/kvm_hyp.h
#	arch/arm64/kvm/hyp/debug-sr.c
#	arch/arm64/kvm/hyp/include/hyp/switch.h
#	arch/arm64/kvm/hyp/nvhe/Makefile
#	arch/arm64/kvm/hyp/nvhe/debug-sr.c
#	arch/arm64/kvm/hyp/nvhe/switch.c
#	arch/arm64/kvm/hyp/nvhe/sysreg-sr.c
#	arch/arm64/kvm/hyp/sysreg-sr.c
#	arch/arm64/kvm/hyp/tlb.c
diff --cc arch/arm64/include/asm/kvm_hyp.h
index 7f82a7f81a9a,46689e7db46c..000000000000
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@@ -23,8 -12,6 +23,11 @@@
  #include <asm/alternative.h>
  #include <asm/sysreg.h>
  
++<<<<<<< HEAD
 +#define __hyp_text __section(.hyp.text) notrace
 +
++=======
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead)
  #define read_sysreg_elx(r,nvh,vh)					\
  	({								\
  		u64 reg;						\
diff --cc arch/arm64/kvm/hyp/debug-sr.c
index d825b78b3f10,24e8acf9ec10..000000000000
--- a/arch/arm64/kvm/hyp/debug-sr.c
+++ b/arch/arm64/kvm/hyp/debug-sr.c
@@@ -96,53 -88,9 +96,59 @@@
  	default:	write_debug(ptr[0], reg, 0);			\
  	}
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/debug-sr.c
 +static void __hyp_text __debug_save_spe_nvhe(u64 *pmscr_el1)
 +{
 +	u64 reg;
 +
 +	/* Clear pmscr in case of early return */
 +	*pmscr_el1 = 0;
 +
 +	/* SPE present on this CPU? */
 +	if (!cpuid_feature_extract_unsigned_field(read_sysreg(id_aa64dfr0_el1),
 +						  ID_AA64DFR0_PMSVER_SHIFT))
 +		return;
 +
 +	/* Yes; is it owned by EL3? */
 +	reg = read_sysreg_s(SYS_PMBIDR_EL1);
 +	if (reg & BIT(SYS_PMBIDR_EL1_P_SHIFT))
 +		return;
 +
 +	/* No; is the host actually using the thing? */
 +	reg = read_sysreg_s(SYS_PMBLIMITR_EL1);
 +	if (!(reg & BIT(SYS_PMBLIMITR_EL1_E_SHIFT)))
 +		return;
 +
 +	/* Yes; save the control register and disable data generation */
 +	*pmscr_el1 = read_sysreg_s(SYS_PMSCR_EL1);
 +	write_sysreg_s(0, SYS_PMSCR_EL1);
 +	isb();
 +
 +	/* Now drain all buffered data to memory */
 +	psb_csync();
 +	dsb(nsh);
 +}
 +
 +static void __hyp_text __debug_restore_spe_nvhe(u64 pmscr_el1)
 +{
 +	if (!pmscr_el1)
 +		return;
 +
 +	/* The host page table is installed, but not yet synchronised */
 +	isb();
 +
 +	/* Re-enable data generation */
 +	write_sysreg_s(pmscr_el1, SYS_PMSCR_EL1);
 +}
 +
 +static void __hyp_text __debug_save_state(struct kvm_vcpu *vcpu,
 +					  struct kvm_guest_debug_arch *dbg,
 +					  struct kvm_cpu_context *ctxt)
++=======
+ static inline void __debug_save_state(struct kvm_vcpu *vcpu,
+ 				      struct kvm_guest_debug_arch *dbg,
+ 				      struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/debug-sr.h
  {
  	u64 aa64dfr0;
  	int brps, wrps;
@@@ -159,9 -107,9 +165,15 @@@
  	ctxt->sys_regs[MDCCINT_EL1] = read_sysreg(mdccint_el1);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/debug-sr.c
 +static void __hyp_text __debug_restore_state(struct kvm_vcpu *vcpu,
 +					     struct kvm_guest_debug_arch *dbg,
 +					     struct kvm_cpu_context *ctxt)
++=======
+ static inline void __debug_restore_state(struct kvm_vcpu *vcpu,
+ 					 struct kvm_guest_debug_arch *dbg,
+ 					 struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/debug-sr.h
  {
  	u64 aa64dfr0;
  	int brps, wrps;
@@@ -179,7 -127,7 +191,11 @@@
  	write_sysreg(ctxt->sys_regs[MDCCINT_EL1], mdccint_el1);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/debug-sr.c
 +void __hyp_text __debug_switch_to_guest(struct kvm_vcpu *vcpu)
++=======
+ static inline void __debug_switch_to_guest_common(struct kvm_vcpu *vcpu)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/debug-sr.h
  {
  	struct kvm_cpu_context *host_ctxt;
  	struct kvm_cpu_context *guest_ctxt;
@@@ -205,7 -146,7 +221,11 @@@
  	__debug_restore_state(vcpu, guest_dbg, guest_ctxt);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/debug-sr.c
 +void __hyp_text __debug_switch_to_host(struct kvm_vcpu *vcpu)
++=======
+ static inline void __debug_switch_to_host_common(struct kvm_vcpu *vcpu)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/debug-sr.h
  {
  	struct kvm_cpu_context *host_ctxt;
  	struct kvm_cpu_context *guest_ctxt;
diff --cc arch/arm64/kvm/hyp/sysreg-sr.c
index a4eba45f8075,6e04e061f762..000000000000
--- a/arch/arm64/kvm/hyp/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/sysreg-sr.c
@@@ -23,30 -15,18 +23,42 @@@
  #include <asm/kvm_emulate.h>
  #include <asm/kvm_hyp.h>
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +/*
 + * Non-VHE: Both host and guest must save everything.
 + *
 + * VHE: Host and guest must save mdscr_el1 and sp_el0 (and the PC and
 + * pstate, which are handled as part of the el2 return state) on every
 + * switch (sp_el0 is being dealt with in the assembly code).
 + * tpidr_el0 and tpidrro_el0 only need to be switched when going
 + * to host userspace or a different VCPU.  EL1 registers only need to be
 + * switched when potentially going to run a different VCPU.  The latter two
 + * classes are handled as part of kvm_arch_vcpu_load and kvm_arch_vcpu_put.
 + */
 +
 +static void __hyp_text __sysreg_save_common_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_save_common_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	ctxt->sys_regs[MDSCR_EL1]	= read_sysreg(mdscr_el1);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +static void __hyp_text __sysreg_save_user_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_save_user_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	ctxt->sys_regs[TPIDR_EL0]	= read_sysreg(tpidr_el0);
  	ctxt->sys_regs[TPIDRRO_EL0]	= read_sysreg(tpidrro_el0);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +static void __hyp_text __sysreg_save_el1_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_save_el1_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	ctxt->sys_regs[CSSELR_EL1]	= read_sysreg(csselr_el1);
  	ctxt->sys_regs[SCTLR_EL1]	= read_sysreg_el1(SYS_SCTLR);
@@@ -71,7 -51,7 +83,11 @@@
  	ctxt->gp_regs.spsr[KVM_SPSR_EL1]= read_sysreg_el1(SYS_SPSR);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +static void __hyp_text __sysreg_save_el2_return_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_save_el2_return_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	ctxt->gp_regs.regs.pc		= read_sysreg_el2(SYS_ELR);
  	ctxt->gp_regs.regs.pstate	= read_sysreg_el2(SYS_SPSR);
@@@ -80,39 -60,18 +96,51 @@@
  		ctxt->sys_regs[DISR_EL1] = read_sysreg_s(SYS_VDISR_EL2);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +void __hyp_text __sysreg_save_state_nvhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_save_el1_state(ctxt);
 +	__sysreg_save_common_state(ctxt);
 +	__sysreg_save_user_state(ctxt);
 +	__sysreg_save_el2_return_state(ctxt);
 +}
 +
 +void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_save_common_state(ctxt);
 +}
 +NOKPROBE_SYMBOL(sysreg_save_host_state_vhe);
 +
 +void sysreg_save_guest_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_save_common_state(ctxt);
 +	__sysreg_save_el2_return_state(ctxt);
 +}
 +NOKPROBE_SYMBOL(sysreg_save_guest_state_vhe);
 +
 +static void __hyp_text __sysreg_restore_common_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_restore_common_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	write_sysreg(ctxt->sys_regs[MDSCR_EL1],	  mdscr_el1);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +static void __hyp_text __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	write_sysreg(ctxt->sys_regs[TPIDR_EL0],		tpidr_el0);
  	write_sysreg(ctxt->sys_regs[TPIDRRO_EL0],	tpidrro_el0);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +static void __hyp_text __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	write_sysreg(ctxt->sys_regs[MPIDR_EL1],		vmpidr_el2);
  	write_sysreg(ctxt->sys_regs[CSSELR_EL1],	csselr_el1);
@@@ -171,8 -130,7 +199,12 @@@
  	write_sysreg_el1(ctxt->gp_regs.spsr[KVM_SPSR_EL1],SYS_SPSR);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +static void __hyp_text
 +__sysreg_restore_el2_return_state(struct kvm_cpu_context *ctxt)
++=======
+ static inline void __sysreg_restore_el2_return_state(struct kvm_cpu_context *ctxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	u64 pstate = ctxt->gp_regs.regs.pstate;
  	u64 mode = pstate & PSR_AA32_MODE_MASK;
@@@ -198,28 -156,7 +230,32 @@@
  		write_sysreg_s(ctxt->sys_regs[DISR_EL1], SYS_VDISR_EL2);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +void __hyp_text __sysreg_restore_state_nvhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_restore_el1_state(ctxt);
 +	__sysreg_restore_common_state(ctxt);
 +	__sysreg_restore_user_state(ctxt);
 +	__sysreg_restore_el2_return_state(ctxt);
 +}
 +
 +void sysreg_restore_host_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_restore_common_state(ctxt);
 +}
 +NOKPROBE_SYMBOL(sysreg_restore_host_state_vhe);
 +
 +void sysreg_restore_guest_state_vhe(struct kvm_cpu_context *ctxt)
 +{
 +	__sysreg_restore_common_state(ctxt);
 +	__sysreg_restore_el2_return_state(ctxt);
 +}
 +NOKPROBE_SYMBOL(sysreg_restore_guest_state_vhe);
 +
 +void __hyp_text __sysreg32_save_state(struct kvm_vcpu *vcpu)
++=======
+ static inline void __sysreg32_save_state(struct kvm_vcpu *vcpu)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	u64 *spsr, *sysreg;
  
@@@ -241,7 -178,7 +277,11 @@@
  		sysreg[DBGVCR32_EL2] = read_sysreg(dbgvcr32_el2);
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/sysreg-sr.c
 +void __hyp_text __sysreg32_restore_state(struct kvm_vcpu *vcpu)
++=======
+ static inline void __sysreg32_restore_state(struct kvm_vcpu *vcpu)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
  {
  	u64 *spsr, *sysreg;
  
diff --cc arch/arm64/kvm/hyp/tlb.c
index d82b0d7000d0,d4475f8340c4..000000000000
--- a/arch/arm64/kvm/hyp/tlb.c
+++ b/arch/arm64/kvm/hyp/tlb.c
@@@ -22,59 -9,12 +22,63 @@@
  #include <asm/tlbflush.h>
  
  struct tlb_inv_context {
 +	unsigned long	flags;
  	u64		tcr;
 +	u64		sctlr;
  };
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/tlb.c
 +static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm *kvm,
 +						 struct tlb_inv_context *cxt)
++=======
+ static void __tlb_switch_to_guest(struct kvm *kvm, struct tlb_inv_context *cxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/nvhe/tlb.c
 +{
 +	u64 val;
 +
 +	local_irq_save(cxt->flags);
 +
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 +		/*
 +		 * For CPUs that are affected by ARM errata 1165522 or 1530923,
 +		 * we cannot trust stage-1 to be in a correct state at that
 +		 * point. Since we do not want to force a full load of the
 +		 * vcpu state, we prevent the EL1 page-table walker to
 +		 * allocate new TLBs. This is done by setting the EPD bits
 +		 * in the TCR_EL1 register. We also need to prevent it to
 +		 * allocate IPA->PA walks, so we enable the S1 MMU...
 +		 */
 +		val = cxt->tcr = read_sysreg_el1(SYS_TCR);
 +		val |= TCR_EPD1_MASK | TCR_EPD0_MASK;
 +		write_sysreg_el1(val, SYS_TCR);
 +		val = cxt->sctlr = read_sysreg_el1(SYS_SCTLR);
 +		val |= SCTLR_ELx_M;
 +		write_sysreg_el1(val, SYS_SCTLR);
 +	}
 +
 +	/*
 +	 * With VHE enabled, we have HCR_EL2.{E2H,TGE} = {1,1}, and
 +	 * most TLB operations target EL2/EL0. In order to affect the
 +	 * guest TLBs (EL1/EL0), we need to change one of these two
 +	 * bits. Changing E2H is impossible (goodbye TTBR1_EL2), so
 +	 * let's flip TGE before executing the TLB operation.
 +	 *
 +	 * ARM erratum 1165522 requires some special handling (again),
 +	 * as we need to make sure both stages of translation are in
 +	 * place before clearing TGE. __load_guest_stage2() already
 +	 * has an ISB in order to deal with this.
 +	 */
 +	__load_guest_stage2(kvm);
 +	val = read_sysreg(hcr_el2);
 +	val &= ~HCR_TGE;
 +	write_sysreg(val, hcr_el2);
 +	isb();
 +}
 +
 +static void __hyp_text __tlb_switch_to_guest_nvhe(struct kvm *kvm,
 +						  struct tlb_inv_context *cxt)
  {
 -	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
  		u64 val;
  
  		/*
@@@ -95,37 -35,7 +99,41 @@@
  	asm(ALTERNATIVE("isb", "nop", ARM64_WORKAROUND_SPECULATIVE_AT));
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/tlb.c
 +static void __hyp_text __tlb_switch_to_guest(struct kvm *kvm,
 +					     struct tlb_inv_context *cxt)
 +{
 +	if (has_vhe())
 +		__tlb_switch_to_guest_vhe(kvm, cxt);
 +	else
 +		__tlb_switch_to_guest_nvhe(kvm, cxt);
 +}
 +
 +static void __hyp_text __tlb_switch_to_host_vhe(struct kvm *kvm,
 +						struct tlb_inv_context *cxt)
 +{
 +	/*
 +	 * We're done with the TLB operation, let's restore the host's
 +	 * view of HCR_EL2.
 +	 */
 +	write_sysreg(0, vttbr_el2);
 +	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 +	isb();
 +
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
 +		/* Restore the registers to what they were */
 +		write_sysreg_el1(cxt->tcr, SYS_TCR);
 +		write_sysreg_el1(cxt->sctlr, SYS_SCTLR);
 +	}
 +
 +	local_irq_restore(cxt->flags);
 +}
 +
 +static void __hyp_text __tlb_switch_to_host_nvhe(struct kvm *kvm,
 +						 struct tlb_inv_context *cxt)
++=======
+ static void __tlb_switch_to_host(struct kvm *kvm, struct tlb_inv_context *cxt)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/nvhe/tlb.c
  {
  	write_sysreg(0, vttbr_el2);
  
@@@ -137,16 -47,7 +145,20 @@@
  	}
  }
  
++<<<<<<< HEAD:arch/arm64/kvm/hyp/tlb.c
 +static void __hyp_text __tlb_switch_to_host(struct kvm *kvm,
 +					    struct tlb_inv_context *cxt)
 +{
 +	if (has_vhe())
 +		__tlb_switch_to_host_vhe(kvm, cxt);
 +	else
 +		__tlb_switch_to_host_nvhe(kvm, cxt);
 +}
 +
 +void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
++=======
+ void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
++>>>>>>> c50cb04303cb (KVM: arm64: Remove __hyp_text macro, use build rules instead):arch/arm64/kvm/hyp/nvhe/tlb.c
  {
  	struct tlb_inv_context cxt;
  
* Unmerged path arch/arm64/kvm/hyp/include/hyp/switch.h
* Unmerged path arch/arm64/kvm/hyp/nvhe/Makefile
* Unmerged path arch/arm64/kvm/hyp/nvhe/debug-sr.c
* Unmerged path arch/arm64/kvm/hyp/nvhe/switch.c
* Unmerged path arch/arm64/kvm/hyp/nvhe/sysreg-sr.c
diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 837b6fe6c602..753c8b8c6705 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -525,7 +525,7 @@ static __always_inline void kvm_skip_instr(struct kvm_vcpu *vcpu, bool is_wide_i
  * Skip an instruction which has been emulated at hyp while most guest sysregs
  * are live.
  */
-static __always_inline void __hyp_text __kvm_skip_instr(struct kvm_vcpu *vcpu)
+static __always_inline void __kvm_skip_instr(struct kvm_vcpu *vcpu)
 {
 	*vcpu_pc(vcpu) = read_sysreg_el2(SYS_ELR);
 	vcpu->arch.ctxt.gp_regs.regs.pstate = read_sysreg_el2(SYS_SPSR);
* Unmerged path arch/arm64/include/asm/kvm_hyp.h
diff --git a/arch/arm64/kvm/hyp/aarch32.c b/arch/arm64/kvm/hyp/aarch32.c
index 25c0e47d57cb..f9ff67dfbf0b 100644
--- a/arch/arm64/kvm/hyp/aarch32.c
+++ b/arch/arm64/kvm/hyp/aarch32.c
@@ -44,7 +44,7 @@ static const unsigned short cc_map[16] = {
 /*
  * Check if a trapped instruction should have been executed or not.
  */
-bool __hyp_text kvm_condition_valid32(const struct kvm_vcpu *vcpu)
+bool kvm_condition_valid32(const struct kvm_vcpu *vcpu)
 {
 	unsigned long cpsr;
 	u32 cpsr_cond;
@@ -93,7 +93,7 @@ bool __hyp_text kvm_condition_valid32(const struct kvm_vcpu *vcpu)
  *
  * IT[7:0] -> CPSR[26:25],CPSR[15:10]
  */
-static void __hyp_text kvm_adjust_itstate(struct kvm_vcpu *vcpu)
+static void kvm_adjust_itstate(struct kvm_vcpu *vcpu)
 {
 	unsigned long itbits, cond;
 	unsigned long cpsr = *vcpu_cpsr(vcpu);
@@ -123,7 +123,7 @@ static void __hyp_text kvm_adjust_itstate(struct kvm_vcpu *vcpu)
  * kvm_skip_instr - skip a trapped instruction and proceed to the next
  * @vcpu: The vcpu pointer
  */
-void __hyp_text kvm_skip_instr32(struct kvm_vcpu *vcpu, bool is_wide_instr)
+void kvm_skip_instr32(struct kvm_vcpu *vcpu, bool is_wide_instr)
 {
 	u32 pc = *vcpu_pc(vcpu);
 	bool is_thumb;
* Unmerged path arch/arm64/kvm/hyp/debug-sr.c
diff --git a/arch/arm64/kvm/hyp/entry.S b/arch/arm64/kvm/hyp/entry.S
index 7adf930b33a6..12fb14c878d7 100644
--- a/arch/arm64/kvm/hyp/entry.S
+++ b/arch/arm64/kvm/hyp/entry.S
@@ -32,7 +32,6 @@
 #define CPU_SP_EL0_OFFSET	(CPU_XREG_OFFSET(30) + 8)
 
 	.text
-	.pushsection	.hyp.text, "ax"
 
 /*
  * We treat x18 as callee-saved as the host may use it as a platform
diff --git a/arch/arm64/kvm/hyp/fpsimd.S b/arch/arm64/kvm/hyp/fpsimd.S
index 416bff833e8c..d39a809e96a9 100644
--- a/arch/arm64/kvm/hyp/fpsimd.S
+++ b/arch/arm64/kvm/hyp/fpsimd.S
@@ -20,7 +20,6 @@
 #include <asm/fpsimdmacros.h>
 
 	.text
-	.pushsection	.hyp.text, "ax"
 
 SYM_FUNC_START(__fpsimd_save_state)
 	fpsimd_save	x0, 1
diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
index 93c38d0cb3dd..57cd08b644c0 100644
--- a/arch/arm64/kvm/hyp/hyp-entry.S
+++ b/arch/arm64/kvm/hyp/hyp-entry.S
@@ -27,7 +27,6 @@
 #include <asm/mmu.h>
 
 	.text
-	.pushsection	.hyp.text, "ax"
 
 .macro do_el2_call
 	/*
* Unmerged path arch/arm64/kvm/hyp/include/hyp/switch.h
* Unmerged path arch/arm64/kvm/hyp/nvhe/Makefile
* Unmerged path arch/arm64/kvm/hyp/nvhe/debug-sr.c
* Unmerged path arch/arm64/kvm/hyp/nvhe/switch.c
* Unmerged path arch/arm64/kvm/hyp/nvhe/sysreg-sr.c
* Unmerged path arch/arm64/kvm/hyp/sysreg-sr.c
diff --git a/arch/arm64/kvm/hyp/timer-sr.c b/arch/arm64/kvm/hyp/timer-sr.c
index a4bc2ae25f40..e942730caaa7 100644
--- a/arch/arm64/kvm/hyp/timer-sr.c
+++ b/arch/arm64/kvm/hyp/timer-sr.c
@@ -30,7 +30,7 @@ void __hyp_text __kvm_timer_set_cntvoff(u64 cntvoff)
  * Should only be called on non-VHE systems.
  * VHE systems use EL2 timers and configure EL1 timers in kvm_timer_init_vhe().
  */
-void __hyp_text __timer_disable_traps(struct kvm_vcpu *vcpu)
+void __timer_disable_traps(struct kvm_vcpu *vcpu)
 {
 	u64 val;
 
@@ -44,7 +44,7 @@ void __hyp_text __timer_disable_traps(struct kvm_vcpu *vcpu)
  * Should only be called on non-VHE systems.
  * VHE systems use EL2 timers and configure EL1 timers in kvm_timer_init_vhe().
  */
-void __hyp_text __timer_enable_traps(struct kvm_vcpu *vcpu)
+void __timer_enable_traps(struct kvm_vcpu *vcpu)
 {
 	u64 val;
 
* Unmerged path arch/arm64/kvm/hyp/tlb.c
diff --git a/arch/arm64/kvm/hyp/vgic-v2-cpuif-proxy.c b/arch/arm64/kvm/hyp/vgic-v2-cpuif-proxy.c
index 0538f5d49947..af605a609020 100644
--- a/arch/arm64/kvm/hyp/vgic-v2-cpuif-proxy.c
+++ b/arch/arm64/kvm/hyp/vgic-v2-cpuif-proxy.c
@@ -24,7 +24,7 @@
 #include <asm/kvm_hyp.h>
 #include <asm/kvm_mmu.h>
 
-static bool __hyp_text __is_be(struct kvm_vcpu *vcpu)
+static bool __is_be(struct kvm_vcpu *vcpu)
 {
 	if (vcpu_mode_is_32bit(vcpu))
 		return !!(read_sysreg_el2(SYS_SPSR) & PSR_AA32_E_BIT);
@@ -43,7 +43,7 @@ static bool __hyp_text __is_be(struct kvm_vcpu *vcpu)
  *  0: Not a GICV access
  * -1: Illegal GICV access successfully performed
  */
-int __hyp_text __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu)
+int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
 	struct vgic_dist *vgic = &kvm->arch.vgic;
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 4b0478f96de5..2bfb098853d4 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -27,7 +27,7 @@
 #define vtr_to_nr_pre_bits(v)		((((u32)(v) >> 26) & 7) + 1)
 #define vtr_to_nr_apr_regs(v)		(1 << (vtr_to_nr_pre_bits(v) - 5))
 
-static u64 __hyp_text __gic_v3_get_lr(unsigned int lr)
+static u64 __gic_v3_get_lr(unsigned int lr)
 {
 	switch (lr & 0xf) {
 	case 0:
@@ -67,7 +67,7 @@ static u64 __hyp_text __gic_v3_get_lr(unsigned int lr)
 	unreachable();
 }
 
-static void __hyp_text __gic_v3_set_lr(u64 val, int lr)
+static void __gic_v3_set_lr(u64 val, int lr)
 {
 	switch (lr & 0xf) {
 	case 0:
@@ -121,7 +121,7 @@ static void __hyp_text __gic_v3_set_lr(u64 val, int lr)
 	}
 }
 
-static void __hyp_text __vgic_v3_write_ap0rn(u32 val, int n)
+static void __vgic_v3_write_ap0rn(u32 val, int n)
 {
 	switch (n) {
 	case 0:
@@ -139,7 +139,7 @@ static void __hyp_text __vgic_v3_write_ap0rn(u32 val, int n)
 	}
 }
 
-static void __hyp_text __vgic_v3_write_ap1rn(u32 val, int n)
+static void __vgic_v3_write_ap1rn(u32 val, int n)
 {
 	switch (n) {
 	case 0:
@@ -157,7 +157,7 @@ static void __hyp_text __vgic_v3_write_ap1rn(u32 val, int n)
 	}
 }
 
-static u32 __hyp_text __vgic_v3_read_ap0rn(int n)
+static u32 __vgic_v3_read_ap0rn(int n)
 {
 	u32 val;
 
@@ -181,7 +181,7 @@ static u32 __hyp_text __vgic_v3_read_ap0rn(int n)
 	return val;
 }
 
-static u32 __hyp_text __vgic_v3_read_ap1rn(int n)
+static u32 __vgic_v3_read_ap1rn(int n)
 {
 	u32 val;
 
@@ -205,7 +205,7 @@ static u32 __hyp_text __vgic_v3_read_ap1rn(int n)
 	return val;
 }
 
-void __hyp_text __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
+void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
 
@@ -240,7 +240,7 @@ void __hyp_text __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
-void __hyp_text __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
+void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
 	int i;
@@ -266,7 +266,7 @@ void __hyp_text __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
-void __hyp_text __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if)
+void __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if)
 {
 	/*
 	 * VFIQEn is RES1 if ICC_SRE_EL1.SRE is 1. This causes a
@@ -313,7 +313,7 @@ void __hyp_text __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if)
 		write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
 }
 
-void __hyp_text __vgic_v3_deactivate_traps(struct vgic_v3_cpu_if *cpu_if)
+void __vgic_v3_deactivate_traps(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 val;
 
@@ -339,7 +339,7 @@ void __hyp_text __vgic_v3_deactivate_traps(struct vgic_v3_cpu_if *cpu_if)
 		write_gicreg(0, ICH_HCR_EL2);
 }
 
-void __hyp_text __vgic_v3_save_aprs(struct vgic_v3_cpu_if *cpu_if)
+void __vgic_v3_save_aprs(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 val;
 	u32 nr_pre_bits;
@@ -372,7 +372,7 @@ void __hyp_text __vgic_v3_save_aprs(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
-void __hyp_text __vgic_v3_restore_aprs(struct vgic_v3_cpu_if *cpu_if)
+void __vgic_v3_restore_aprs(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 val;
 	u32 nr_pre_bits;
@@ -405,7 +405,7 @@ void __hyp_text __vgic_v3_restore_aprs(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
-void __hyp_text __vgic_v3_init_lrs(void)
+void __vgic_v3_init_lrs(void)
 {
 	int max_lr_idx = vtr_to_max_lr_idx(read_gicreg(ICH_VTR_EL2));
 	int i;
@@ -414,28 +414,28 @@ void __hyp_text __vgic_v3_init_lrs(void)
 		__gic_v3_set_lr(0, i);
 }
 
-u64 __hyp_text __vgic_v3_get_ich_vtr_el2(void)
+u64 __vgic_v3_get_ich_vtr_el2(void)
 {
 	return read_gicreg(ICH_VTR_EL2);
 }
 
-u64 __hyp_text __vgic_v3_read_vmcr(void)
+u64 __vgic_v3_read_vmcr(void)
 {
 	return read_gicreg(ICH_VMCR_EL2);
 }
 
-void __hyp_text __vgic_v3_write_vmcr(u32 vmcr)
+void __vgic_v3_write_vmcr(u32 vmcr)
 {
 	write_gicreg(vmcr, ICH_VMCR_EL2);
 }
 
-static int __hyp_text __vgic_v3_bpr_min(void)
+static int __vgic_v3_bpr_min(void)
 {
 	/* See Pseudocode for VPriorityGroup */
 	return 8 - vtr_to_nr_pre_bits(read_gicreg(ICH_VTR_EL2));
 }
 
-static int __hyp_text __vgic_v3_get_group(struct kvm_vcpu *vcpu)
+static int __vgic_v3_get_group(struct kvm_vcpu *vcpu)
 {
 	u32 esr = kvm_vcpu_get_hsr(vcpu);
 	u8 crm = (esr & ESR_ELx_SYS64_ISS_CRM_MASK) >> ESR_ELx_SYS64_ISS_CRM_SHIFT;
@@ -445,9 +445,8 @@ static int __hyp_text __vgic_v3_get_group(struct kvm_vcpu *vcpu)
 
 #define GICv3_IDLE_PRIORITY	0xff
 
-static int __hyp_text __vgic_v3_highest_priority_lr(struct kvm_vcpu *vcpu,
-						    u32 vmcr,
-						    u64 *lr_val)
+static int __vgic_v3_highest_priority_lr(struct kvm_vcpu *vcpu, u32 vmcr,
+					 u64 *lr_val)
 {
 	unsigned int used_lrs = vcpu->arch.vgic_cpu.vgic_v3.used_lrs;
 	u8 priority = GICv3_IDLE_PRIORITY;
@@ -485,8 +484,8 @@ static int __hyp_text __vgic_v3_highest_priority_lr(struct kvm_vcpu *vcpu,
 	return lr;
 }
 
-static int __hyp_text __vgic_v3_find_active_lr(struct kvm_vcpu *vcpu,
-					       int intid, u64 *lr_val)
+static int __vgic_v3_find_active_lr(struct kvm_vcpu *vcpu, int intid,
+				    u64 *lr_val)
 {
 	unsigned int used_lrs = vcpu->arch.vgic_cpu.vgic_v3.used_lrs;
 	int i;
@@ -505,7 +504,7 @@ static int __hyp_text __vgic_v3_find_active_lr(struct kvm_vcpu *vcpu,
 	return -1;
 }
 
-static int __hyp_text __vgic_v3_get_highest_active_priority(void)
+static int __vgic_v3_get_highest_active_priority(void)
 {
 	u8 nr_apr_regs = vtr_to_nr_apr_regs(read_gicreg(ICH_VTR_EL2));
 	u32 hap = 0;
@@ -537,12 +536,12 @@ static int __hyp_text __vgic_v3_get_highest_active_priority(void)
 	return GICv3_IDLE_PRIORITY;
 }
 
-static unsigned int __hyp_text __vgic_v3_get_bpr0(u32 vmcr)
+static unsigned int __vgic_v3_get_bpr0(u32 vmcr)
 {
 	return (vmcr & ICH_VMCR_BPR0_MASK) >> ICH_VMCR_BPR0_SHIFT;
 }
 
-static unsigned int __hyp_text __vgic_v3_get_bpr1(u32 vmcr)
+static unsigned int __vgic_v3_get_bpr1(u32 vmcr)
 {
 	unsigned int bpr;
 
@@ -561,7 +560,7 @@ static unsigned int __hyp_text __vgic_v3_get_bpr1(u32 vmcr)
  * Convert a priority to a preemption level, taking the relevant BPR
  * into account by zeroing the sub-priority bits.
  */
-static u8 __hyp_text __vgic_v3_pri_to_pre(u8 pri, u32 vmcr, int grp)
+static u8 __vgic_v3_pri_to_pre(u8 pri, u32 vmcr, int grp)
 {
 	unsigned int bpr;
 
@@ -579,7 +578,7 @@ static u8 __hyp_text __vgic_v3_pri_to_pre(u8 pri, u32 vmcr, int grp)
  * matter what the guest does with its BPR, we can always set/get the
  * same value of a priority.
  */
-static void __hyp_text __vgic_v3_set_active_priority(u8 pri, u32 vmcr, int grp)
+static void __vgic_v3_set_active_priority(u8 pri, u32 vmcr, int grp)
 {
 	u8 pre, ap;
 	u32 val;
@@ -598,7 +597,7 @@ static void __hyp_text __vgic_v3_set_active_priority(u8 pri, u32 vmcr, int grp)
 	}
 }
 
-static int __hyp_text __vgic_v3_clear_highest_active_priority(void)
+static int __vgic_v3_clear_highest_active_priority(void)
 {
 	u8 nr_apr_regs = vtr_to_nr_apr_regs(read_gicreg(ICH_VTR_EL2));
 	u32 hap = 0;
@@ -636,7 +635,7 @@ static int __hyp_text __vgic_v3_clear_highest_active_priority(void)
 	return GICv3_IDLE_PRIORITY;
 }
 
-static void __hyp_text __vgic_v3_read_iar(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_read_iar(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u64 lr_val;
 	u8 lr_prio, pmr;
@@ -672,7 +671,7 @@ static void __hyp_text __vgic_v3_read_iar(struct kvm_vcpu *vcpu, u32 vmcr, int r
 	vcpu_set_reg(vcpu, rt, ICC_IAR1_EL1_SPURIOUS);
 }
 
-static void __hyp_text __vgic_v3_clear_active_lr(int lr, u64 lr_val)
+static void __vgic_v3_clear_active_lr(int lr, u64 lr_val)
 {
 	lr_val &= ~ICH_LR_ACTIVE_BIT;
 	if (lr_val & ICH_LR_HW) {
@@ -685,7 +684,7 @@ static void __hyp_text __vgic_v3_clear_active_lr(int lr, u64 lr_val)
 	__gic_v3_set_lr(lr_val, lr);
 }
 
-static void __hyp_text __vgic_v3_bump_eoicount(void)
+static void __vgic_v3_bump_eoicount(void)
 {
 	u32 hcr;
 
@@ -694,8 +693,7 @@ static void __hyp_text __vgic_v3_bump_eoicount(void)
 	write_gicreg(hcr, ICH_HCR_EL2);
 }
 
-static void __hyp_text __vgic_v3_write_dir(struct kvm_vcpu *vcpu,
-					   u32 vmcr, int rt)
+static void __vgic_v3_write_dir(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u32 vid = vcpu_get_reg(vcpu, rt);
 	u64 lr_val;
@@ -718,7 +716,7 @@ static void __hyp_text __vgic_v3_write_dir(struct kvm_vcpu *vcpu,
 	__vgic_v3_clear_active_lr(lr, lr_val);
 }
 
-static void __hyp_text __vgic_v3_write_eoir(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_write_eoir(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u32 vid = vcpu_get_reg(vcpu, rt);
 	u64 lr_val;
@@ -755,17 +753,17 @@ static void __hyp_text __vgic_v3_write_eoir(struct kvm_vcpu *vcpu, u32 vmcr, int
 	__vgic_v3_clear_active_lr(lr, lr_val);
 }
 
-static void __hyp_text __vgic_v3_read_igrpen0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_read_igrpen0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	vcpu_set_reg(vcpu, rt, !!(vmcr & ICH_VMCR_ENG0_MASK));
 }
 
-static void __hyp_text __vgic_v3_read_igrpen1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_read_igrpen1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	vcpu_set_reg(vcpu, rt, !!(vmcr & ICH_VMCR_ENG1_MASK));
 }
 
-static void __hyp_text __vgic_v3_write_igrpen0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_write_igrpen0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u64 val = vcpu_get_reg(vcpu, rt);
 
@@ -777,7 +775,7 @@ static void __hyp_text __vgic_v3_write_igrpen0(struct kvm_vcpu *vcpu, u32 vmcr,
 	__vgic_v3_write_vmcr(vmcr);
 }
 
-static void __hyp_text __vgic_v3_write_igrpen1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_write_igrpen1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u64 val = vcpu_get_reg(vcpu, rt);
 
@@ -789,17 +787,17 @@ static void __hyp_text __vgic_v3_write_igrpen1(struct kvm_vcpu *vcpu, u32 vmcr,
 	__vgic_v3_write_vmcr(vmcr);
 }
 
-static void __hyp_text __vgic_v3_read_bpr0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_read_bpr0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	vcpu_set_reg(vcpu, rt, __vgic_v3_get_bpr0(vmcr));
 }
 
-static void __hyp_text __vgic_v3_read_bpr1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_read_bpr1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	vcpu_set_reg(vcpu, rt, __vgic_v3_get_bpr1(vmcr));
 }
 
-static void __hyp_text __vgic_v3_write_bpr0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_write_bpr0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u64 val = vcpu_get_reg(vcpu, rt);
 	u8 bpr_min = __vgic_v3_bpr_min() - 1;
@@ -816,7 +814,7 @@ static void __hyp_text __vgic_v3_write_bpr0(struct kvm_vcpu *vcpu, u32 vmcr, int
 	__vgic_v3_write_vmcr(vmcr);
 }
 
-static void __hyp_text __vgic_v3_write_bpr1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
+static void __vgic_v3_write_bpr1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u64 val = vcpu_get_reg(vcpu, rt);
 	u8 bpr_min = __vgic_v3_bpr_min();
@@ -836,7 +834,7 @@ static void __hyp_text __vgic_v3_write_bpr1(struct kvm_vcpu *vcpu, u32 vmcr, int
 	__vgic_v3_write_vmcr(vmcr);
 }
 
-static void __hyp_text __vgic_v3_read_apxrn(struct kvm_vcpu *vcpu, int rt, int n)
+static void __vgic_v3_read_apxrn(struct kvm_vcpu *vcpu, int rt, int n)
 {
 	u32 val;
 
@@ -848,7 +846,7 @@ static void __hyp_text __vgic_v3_read_apxrn(struct kvm_vcpu *vcpu, int rt, int n
 	vcpu_set_reg(vcpu, rt, val);
 }
 
-static void __hyp_text __vgic_v3_write_apxrn(struct kvm_vcpu *vcpu, int rt, int n)
+static void __vgic_v3_write_apxrn(struct kvm_vcpu *vcpu, int rt, int n)
 {
 	u32 val = vcpu_get_reg(vcpu, rt);
 
@@ -858,56 +856,49 @@ static void __hyp_text __vgic_v3_write_apxrn(struct kvm_vcpu *vcpu, int rt, int
 		__vgic_v3_write_ap1rn(val, n);
 }
 
-static void __hyp_text __vgic_v3_read_apxr0(struct kvm_vcpu *vcpu,
+static void __vgic_v3_read_apxr0(struct kvm_vcpu *vcpu,
 					    u32 vmcr, int rt)
 {
 	__vgic_v3_read_apxrn(vcpu, rt, 0);
 }
 
-static void __hyp_text __vgic_v3_read_apxr1(struct kvm_vcpu *vcpu,
+static void __vgic_v3_read_apxr1(struct kvm_vcpu *vcpu,
 					    u32 vmcr, int rt)
 {
 	__vgic_v3_read_apxrn(vcpu, rt, 1);
 }
 
-static void __hyp_text __vgic_v3_read_apxr2(struct kvm_vcpu *vcpu,
-					    u32 vmcr, int rt)
+static void __vgic_v3_read_apxr2(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	__vgic_v3_read_apxrn(vcpu, rt, 2);
 }
 
-static void __hyp_text __vgic_v3_read_apxr3(struct kvm_vcpu *vcpu,
-					    u32 vmcr, int rt)
+static void __vgic_v3_read_apxr3(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	__vgic_v3_read_apxrn(vcpu, rt, 3);
 }
 
-static void __hyp_text __vgic_v3_write_apxr0(struct kvm_vcpu *vcpu,
-					     u32 vmcr, int rt)
+static void __vgic_v3_write_apxr0(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	__vgic_v3_write_apxrn(vcpu, rt, 0);
 }
 
-static void __hyp_text __vgic_v3_write_apxr1(struct kvm_vcpu *vcpu,
-					     u32 vmcr, int rt)
+static void __vgic_v3_write_apxr1(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	__vgic_v3_write_apxrn(vcpu, rt, 1);
 }
 
-static void __hyp_text __vgic_v3_write_apxr2(struct kvm_vcpu *vcpu,
-					     u32 vmcr, int rt)
+static void __vgic_v3_write_apxr2(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	__vgic_v3_write_apxrn(vcpu, rt, 2);
 }
 
-static void __hyp_text __vgic_v3_write_apxr3(struct kvm_vcpu *vcpu,
-					     u32 vmcr, int rt)
+static void __vgic_v3_write_apxr3(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	__vgic_v3_write_apxrn(vcpu, rt, 3);
 }
 
-static void __hyp_text __vgic_v3_read_hppir(struct kvm_vcpu *vcpu,
-					    u32 vmcr, int rt)
+static void __vgic_v3_read_hppir(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u64 lr_val;
 	int lr, lr_grp, grp;
@@ -926,16 +917,14 @@ static void __hyp_text __vgic_v3_read_hppir(struct kvm_vcpu *vcpu,
 	vcpu_set_reg(vcpu, rt, lr_val & ICH_LR_VIRTUAL_ID_MASK);
 }
 
-static void __hyp_text __vgic_v3_read_pmr(struct kvm_vcpu *vcpu,
-					  u32 vmcr, int rt)
+static void __vgic_v3_read_pmr(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	vmcr &= ICH_VMCR_PMR_MASK;
 	vmcr >>= ICH_VMCR_PMR_SHIFT;
 	vcpu_set_reg(vcpu, rt, vmcr);
 }
 
-static void __hyp_text __vgic_v3_write_pmr(struct kvm_vcpu *vcpu,
-					   u32 vmcr, int rt)
+static void __vgic_v3_write_pmr(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u32 val = vcpu_get_reg(vcpu, rt);
 
@@ -947,15 +936,13 @@ static void __hyp_text __vgic_v3_write_pmr(struct kvm_vcpu *vcpu,
 	write_gicreg(vmcr, ICH_VMCR_EL2);
 }
 
-static void __hyp_text __vgic_v3_read_rpr(struct kvm_vcpu *vcpu,
-					  u32 vmcr, int rt)
+static void __vgic_v3_read_rpr(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u32 val = __vgic_v3_get_highest_active_priority();
 	vcpu_set_reg(vcpu, rt, val);
 }
 
-static void __hyp_text __vgic_v3_read_ctlr(struct kvm_vcpu *vcpu,
-					   u32 vmcr, int rt)
+static void __vgic_v3_read_ctlr(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u32 vtr, val;
 
@@ -976,8 +963,7 @@ static void __hyp_text __vgic_v3_read_ctlr(struct kvm_vcpu *vcpu,
 	vcpu_set_reg(vcpu, rt, val);
 }
 
-static void __hyp_text __vgic_v3_write_ctlr(struct kvm_vcpu *vcpu,
-					    u32 vmcr, int rt)
+static void __vgic_v3_write_ctlr(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 {
 	u32 val = vcpu_get_reg(vcpu, rt);
 
@@ -994,7 +980,7 @@ static void __hyp_text __vgic_v3_write_ctlr(struct kvm_vcpu *vcpu,
 	write_gicreg(vmcr, ICH_VMCR_EL2);
 }
 
-int __hyp_text __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu)
+int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu)
 {
 	int rt;
 	u32 esr;
