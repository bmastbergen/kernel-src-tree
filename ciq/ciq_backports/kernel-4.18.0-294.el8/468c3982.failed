mm: memcontrol: switch to native NR_ANON_THPS counter

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 468c398233da208521a0f84c2068012a66a7489d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/468c3982.failed

With rmap memcg locking already in place for NR_ANON_MAPPED, it's just a
small step to remove the MEMCG_RSS_HUGE wart and switch memcg to the
native NR_ANON_THPS accounting sites.

[hannes@cmpxchg.org: fixes]
  Link: http://lkml.kernel.org/r/20200512121750.GA397968@cmpxchg.org
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Tested-by: Naresh Kamboju <naresh.kamboju@linaro.org>
	Reviewed-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Acked-by: Randy Dunlap <rdunlap@infradead.org>	[build-tested]
	Cc: Alex Shi <alex.shi@linux.alibaba.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Roman Gushchin <guro@fb.com>
	Cc: Shakeel Butt <shakeelb@google.com>
	Cc: Balbir Singh <bsingharora@gmail.com>
Link: http://lkml.kernel.org/r/20200508183105.225460-12-hannes@cmpxchg.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 468c398233da208521a0f84c2068012a66a7489d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	mm/memcontrol.c
#	mm/rmap.c
diff --cc include/linux/memcontrol.h
index fa05dbf9d553,63a31a6c3c69..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -38,10 -29,7 +38,14 @@@ struct kmem_cache
  
  /* Cgroup-specific page state, on top of universal node page state */
  enum memcg_stat_item {
++<<<<<<< HEAD
 +	MEMCG_CACHE = NR_VM_NODE_STAT_ITEMS,
 +	MEMCG_RSS,
 +	MEMCG_RSS_HUGE,
 +	MEMCG_SWAP,
++=======
+ 	MEMCG_SWAP = NR_VM_NODE_STAT_ITEMS,
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  	MEMCG_SOCK,
  	/* XXX: why are these zone and not node counters? */
  	MEMCG_KERNEL_STACK_KB,
diff --cc mm/memcontrol.c
index e98656a65024,17587ea2745a..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -846,25 -834,8 +846,28 @@@ static unsigned long memcg_events_local
  
  static void mem_cgroup_charge_statistics(struct mem_cgroup *memcg,
  					 struct page *page,
 -					 int nr_pages)
 +					 bool compound, int nr_pages)
  {
++<<<<<<< HEAD
 +	/*
 +	 * Here, RSS means 'mapped anon' and anon's SwapCache. Shmem/tmpfs is
 +	 * counted as CACHE even if it's on ANON LRU.
 +	 */
 +	if (PageAnon(page))
 +		__mod_memcg_state(memcg, MEMCG_RSS, nr_pages);
 +	else {
 +		__mod_memcg_state(memcg, MEMCG_CACHE, nr_pages);
 +		if (PageSwapBacked(page))
 +			__mod_memcg_state(memcg, NR_SHMEM, nr_pages);
 +	}
 +
 +	if (compound) {
 +		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 +		__mod_memcg_state(memcg, MEMCG_RSS_HUGE, nr_pages);
 +	}
 +
++=======
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  	/* pagein of a big page is an event. So, ignore page size */
  	if (nr_pages > 0)
  		__count_memcg_events(memcg, PGPGIN, 1);
@@@ -3837,9 -3805,11 +3834,17 @@@ static int memcg_numa_stat_show(struct 
  #endif /* CONFIG_NUMA */
  
  static const unsigned int memcg1_stats[] = {
++<<<<<<< HEAD
 +	MEMCG_CACHE,
 +	MEMCG_RSS,
 +	MEMCG_RSS_HUGE,
++=======
+ 	NR_FILE_PAGES,
+ 	NR_ANON_MAPPED,
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	NR_ANON_THPS,
+ #endif
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  	NR_SHMEM,
  	NR_FILE_MAPPED,
  	NR_FILE_DIRTY,
@@@ -5329,24 -5444,43 +5341,42 @@@ static int mem_cgroup_move_account(stru
  	from_vec = mem_cgroup_lruvec(from, pgdat);
  	to_vec = mem_cgroup_lruvec(to, pgdat);
  
 -	lock_page_memcg(page);
 +	spin_lock_irqsave(&from->move_lock, flags);
  
++<<<<<<< HEAD
 +	if (!anon && page_mapped(page)) {
 +		__mod_lruvec_state(from_vec, NR_FILE_MAPPED, -nr_pages);
 +		__mod_lruvec_state(to_vec, NR_FILE_MAPPED, nr_pages);
 +	}
++=======
+ 	if (PageAnon(page)) {
+ 		if (page_mapped(page)) {
+ 			__mod_lruvec_state(from_vec, NR_ANON_MAPPED, -nr_pages);
+ 			__mod_lruvec_state(to_vec, NR_ANON_MAPPED, nr_pages);
+ 			if (PageTransHuge(page)) {
+ 				__mod_lruvec_state(from_vec, NR_ANON_THPS,
+ 						   -nr_pages);
+ 				__mod_lruvec_state(to_vec, NR_ANON_THPS,
+ 						   nr_pages);
+ 			}
+ 
+ 		}
+ 	} else {
+ 		__mod_lruvec_state(from_vec, NR_FILE_PAGES, -nr_pages);
+ 		__mod_lruvec_state(to_vec, NR_FILE_PAGES, nr_pages);
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  
 -		if (PageSwapBacked(page)) {
 -			__mod_lruvec_state(from_vec, NR_SHMEM, -nr_pages);
 -			__mod_lruvec_state(to_vec, NR_SHMEM, nr_pages);
 -		}
 -
 -		if (page_mapped(page)) {
 -			__mod_lruvec_state(from_vec, NR_FILE_MAPPED, -nr_pages);
 -			__mod_lruvec_state(to_vec, NR_FILE_MAPPED, nr_pages);
 -		}
 -
 -		if (PageDirty(page)) {
 -			struct address_space *mapping = page_mapping(page);
 +	/*
 +	 * move_lock grabbed above and caller set from->moving_account, so
 +	 * mod_memcg_page_state will serialize updates to PageDirty.
 +	 * So mapping should be stable for dirty pages.
 +	 */
 +	if (!anon && PageDirty(page)) {
 +		struct address_space *mapping = page_mapping(page);
  
 -			if (mapping_cap_account_dirty(mapping)) {
 -				__mod_lruvec_state(from_vec, NR_FILE_DIRTY,
 -						   -nr_pages);
 -				__mod_lruvec_state(to_vec, NR_FILE_DIRTY,
 -						   nr_pages);
 -			}
 +		if (mapping_cap_account_dirty(mapping)) {
 +			__mod_lruvec_state(from_vec, NR_FILE_DIRTY, -nr_pages);
 +			__mod_lruvec_state(to_vec, NR_FILE_DIRTY, nr_pages);
  		}
  	}
  
@@@ -6453,14 -6646,36 +6483,17 @@@ void mem_cgroup_cancel_charge(struct pa
  	cancel_charge(memcg, nr_pages);
  }
  
 -/**
 - * mem_cgroup_charge - charge a newly allocated page to a cgroup
 - * @page: page to charge
 - * @mm: mm context of the victim
 - * @gfp_mask: reclaim mode
 - * @lrucare: page might be on the LRU already
 - *
 - * Try to charge @page to the memcg that @mm belongs to, reclaiming
 - * pages according to @gfp_mask if necessary.
 - *
 - * Returns 0 on success. Otherwise, an error code is returned.
 - */
 -int mem_cgroup_charge(struct page *page, struct mm_struct *mm, gfp_t gfp_mask,
 -		      bool lrucare)
 -{
 -	struct mem_cgroup *memcg;
 -	int ret;
 -
 -	ret = mem_cgroup_try_charge(page, mm, gfp_mask, &memcg);
 -	if (ret)
 -		return ret;
 -	mem_cgroup_commit_charge(page, memcg, lrucare);
 -	return 0;
 -}
 -
  struct uncharge_gather {
  	struct mem_cgroup *memcg;
 -	unsigned long nr_pages;
  	unsigned long pgpgout;
 +	unsigned long nr_anon;
 +	unsigned long nr_file;
  	unsigned long nr_kmem;
++<<<<<<< HEAD
 +	unsigned long nr_huge;
 +	unsigned long nr_shmem;
++=======
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  	struct page *dummy_page;
  };
  
@@@ -6484,12 -6698,8 +6517,15 @@@ static void uncharge_batch(const struc
  	}
  
  	local_irq_save(flags);
++<<<<<<< HEAD
 +	__mod_memcg_state(ug->memcg, MEMCG_RSS, -ug->nr_anon);
 +	__mod_memcg_state(ug->memcg, MEMCG_CACHE, -ug->nr_file);
 +	__mod_memcg_state(ug->memcg, MEMCG_RSS_HUGE, -ug->nr_huge);
 +	__mod_memcg_state(ug->memcg, NR_SHMEM, -ug->nr_shmem);
++=======
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  	__count_memcg_events(ug->memcg, PGPGOUT, ug->pgpgout);
 -	__this_cpu_add(ug->memcg->vmstats_percpu->nr_page_events, ug->nr_pages);
 +	__this_cpu_add(ug->memcg->vmstats_percpu->nr_page_events, nr_pages);
  	memcg_check_events(ug->memcg, ug->dummy_page);
  	local_irq_restore(flags);
  
@@@ -6520,23 -6730,13 +6556,26 @@@ static void uncharge_page(struct page *
  		ug->memcg = page->mem_cgroup;
  	}
  
 -	nr_pages = compound_nr(page);
 -	ug->nr_pages += nr_pages;
 -
  	if (!PageKmemcg(page)) {
++<<<<<<< HEAD
 +		unsigned int nr_pages = 1;
 +
 +		if (PageTransHuge(page)) {
 +			nr_pages <<= compound_order(page);
 +			ug->nr_huge += nr_pages;
 +		}
 +		if (PageAnon(page))
 +			ug->nr_anon += nr_pages;
 +		else {
 +			ug->nr_file += nr_pages;
 +			if (PageSwapBacked(page))
 +				ug->nr_shmem += nr_pages;
 +		}
++=======
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  		ug->pgpgout++;
  	} else {
 -		ug->nr_kmem += nr_pages;
 +		ug->nr_kmem += 1 << compound_order(page);
  		__ClearPageKmemcg(page);
  	}
  
diff --cc mm/rmap.c
index 884554872969,ad4a0fdcc94c..000000000000
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@@ -1126,13 -1138,14 +1126,18 @@@ void do_page_add_anon_rmap(struct page 
  		 * disabled.
  		 */
  		if (compound)
++<<<<<<< HEAD
 +			__inc_node_page_state(page, NR_ANON_THPS);
 +		__mod_node_page_state(page_pgdat(page), NR_ANON_MAPPED, nr);
++=======
+ 			__inc_lruvec_page_state(page, NR_ANON_THPS);
+ 		__mod_lruvec_page_state(page, NR_ANON_MAPPED, nr);
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  	}
 -
 -	if (unlikely(PageKsm(page))) {
 -		unlock_page_memcg(page);
 +	if (unlikely(PageKsm(page)))
  		return;
 -	}
 +
 +	VM_BUG_ON_PAGE(!PageLocked(page), page);
  
  	/* address might be in next vma when migration races vma_adjust */
  	if (first)
@@@ -1164,7 -1177,10 +1169,14 @@@ void page_add_new_anon_rmap(struct pag
  		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
  		/* increment count (starts at -1) */
  		atomic_set(compound_mapcount_ptr(page), 0);
++<<<<<<< HEAD
 +		__inc_node_page_state(page, NR_ANON_THPS);
++=======
+ 		if (hpage_pincount_available(page))
+ 			atomic_set(compound_pincount_ptr(page), 0);
+ 
+ 		__inc_lruvec_page_state(page, NR_ANON_THPS);
++>>>>>>> 468c398233da (mm: memcontrol: switch to native NR_ANON_THPS counter)
  	} else {
  		/* Anon THP always mapped first with PMD */
  		VM_BUG_ON_PAGE(PageTransCompound(page), page);
* Unmerged path include/linux/memcontrol.h
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 4033c78cc361..d3a59c1a03e5 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -2272,15 +2272,17 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 			atomic_inc(&page[i]._mapcount);
 	}
 
+	lock_page_memcg(page);
 	if (atomic_add_negative(-1, compound_mapcount_ptr(page))) {
 		/* Last compound_mapcount is gone. */
-		__dec_node_page_state(page, NR_ANON_THPS);
+		__dec_lruvec_page_state(page, NR_ANON_THPS);
 		if (TestClearPageDoubleMap(page)) {
 			/* No need in mapcount reference anymore */
 			for (i = 0; i < HPAGE_PMD_NR; i++)
 				atomic_dec(&page[i]._mapcount);
 		}
 	}
+	unlock_page_memcg(page);
 
 	smp_wmb(); /* make pte visible before pmd */
 	pmd_populate(mm, pmd, pgtable);
* Unmerged path mm/memcontrol.c
* Unmerged path mm/rmap.c
