RDMA/mlx5: Separate XRC_TGT QP creation from common flow

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit 04bcc1c2d0d7bfa0bffa5853d9a127fb4f4cd943
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/04bcc1c2.failed

XRC_TGT QP doesn't fail into kernel or user flow separation. It is
initiated by the user, but is created through in-kernel verbs flow
and doesn't have PD and udata in similar way to kernel QPs.

So let's separate creation of that QP type from the common flow.

Link: https://lore.kernel.org/r/20200427154636.381474-28-leon@kernel.org
	Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 04bcc1c2d0d7bfa0bffa5853d9a127fb4f4cd943)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 08f1eef60c1f,8890c172f7e5..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -1960,9 -1912,79 +1959,85 @@@ static int get_atomic_mode(struct mlx5_
  	return atomic_mode;
  }
  
++<<<<<<< HEAD
 +static inline bool check_flags_mask(uint64_t input, uint64_t supported)
 +{
 +	return (input & ~supported) == 0;
++=======
+ static int create_xrc_tgt_qp(struct mlx5_ib_dev *dev,
+ 			     struct ib_qp_init_attr *attr,
+ 			     struct mlx5_ib_qp *qp, struct ib_udata *udata,
+ 			     u32 uidx)
+ {
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_qp_base *base;
+ 	unsigned long flags;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	mutex_init(&qp->mutex);
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	in = kvzalloc(inlen, GFP_KERNEL);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, MLX5_QP_ST_XRC);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 	MLX5_SET(qpc, qpc, pd, to_mpd(devr->p0)->pdn);
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 	if (qp->flags & IB_QP_CREATE_CROSS_CHANNEL)
+ 		MLX5_SET(qpc, qpc, cd_master, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_SEND)
+ 		MLX5_SET(qpc, qpc, cd_slave_send, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_RECV)
+ 		MLX5_SET(qpc, qpc, cd_slave_receive, 1);
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, MLX5_SRQ_RQ);
+ 	MLX5_SET(qpc, qpc, no_sq, 1);
+ 	MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, cqn_snd, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, srqn_rmpn_xrqn, to_msrq(devr->s0)->msrq.srqn);
+ 	MLX5_SET(qpc, qpc, xrcd, to_mxrcd(attr->xrcd)->xrcdn);
+ 	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
+ 
+ 	/* 0xffffff means we ask to work with cqe version 0 */
+ 	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
+ 		MLX5_SET(qpc, qpc, user_index, uidx);
+ 
+ 	if (qp->flags & IB_QP_CREATE_PCI_WRITE_END_PADDING) {
+ 		MLX5_SET(qpc, qpc, end_padding_mode,
+ 			 MLX5_WQ_END_PAD_MODE_ALIGN);
+ 		/* Special case to clean flag */
+ 		qp->flags &= ~IB_QP_CREATE_PCI_WRITE_END_PADDING;
+ 	}
+ 
+ 	base = &qp->trans_qp.base;
+ 	err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
+ 	kvfree(in);
+ 	if (err) {
+ 		destroy_qp_user(dev, NULL, qp, base, udata);
+ 		return err;
+ 	}
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	return 0;
++>>>>>>> 04bcc1c2d0d7 (RDMA/mlx5: Separate XRC_TGT QP creation from common flow)
  }
  
  static int create_qp_common(struct mlx5_ib_dev *dev, struct ib_pd *pd,
@@@ -2150,44 -2032,30 +2225,63 @@@
  		return err;
  	}
  
++<<<<<<< HEAD
 +	if (pd) {
 +		if (udata) {
 +			__u32 max_wqes =
 +				1 << MLX5_CAP_GEN(mdev, log_max_qp_sz);
 +			mlx5_ib_dbg(dev, "requested sq_wqe_count (%d)\n", ucmd.sq_wqe_count);
 +			if (ucmd.rq_wqe_shift != qp->rq.wqe_shift ||
 +			    ucmd.rq_wqe_count != qp->rq.wqe_cnt) {
 +				mlx5_ib_dbg(dev, "invalid rq params\n");
 +				return -EINVAL;
 +			}
 +			if (ucmd.sq_wqe_count > max_wqes) {
 +				mlx5_ib_dbg(dev, "requested sq_wqe_count (%d) > max allowed (%d)\n",
 +					    ucmd.sq_wqe_count, max_wqes);
 +				return -EINVAL;
 +			}
 +			if (init_attr->create_flags &
 +			    MLX5_IB_QP_CREATE_SQPN_QP1) {
 +				mlx5_ib_dbg(dev, "user-space is not allowed to create UD QPs spoofing as QP1\n");
 +				return -EINVAL;
 +			}
 +			err = create_user_qp(dev, pd, qp, udata, init_attr, &in,
 +					     &resp, &inlen, base);
 +			if (err)
 +				mlx5_ib_dbg(dev, "err %d\n", err);
 +		} else {
 +			err = create_kernel_qp(dev, init_attr, qp, &in, &inlen,
 +					       base);
 +			if (err)
 +				mlx5_ib_dbg(dev, "err %d\n", err);
 +		}
++=======
+ 	if (udata) {
+ 		__u32 max_wqes = 1 << MLX5_CAP_GEN(mdev, log_max_qp_sz);
++>>>>>>> 04bcc1c2d0d7 (RDMA/mlx5: Separate XRC_TGT QP creation from common flow)
+ 
+ 		mlx5_ib_dbg(dev, "requested sq_wqe_count (%d)\n",
+ 			    ucmd->sq_wqe_count);
+ 		if (ucmd->rq_wqe_shift != qp->rq.wqe_shift ||
+ 		    ucmd->rq_wqe_count != qp->rq.wqe_cnt) {
+ 			mlx5_ib_dbg(dev, "invalid rq params\n");
+ 			return -EINVAL;
+ 		}
+ 		if (ucmd->sq_wqe_count > max_wqes) {
+ 			mlx5_ib_dbg(
+ 				dev,
+ 				"requested sq_wqe_count (%d) > max allowed (%d)\n",
+ 				ucmd->sq_wqe_count, max_wqes);
+ 			return -EINVAL;
+ 		}
+ 		err = create_user_qp(dev, pd, qp, udata, init_attr, &in, &resp,
+ 				     &inlen, base, ucmd);
+ 	} else
+ 		err = create_kernel_qp(dev, init_attr, qp, &in, &inlen, base);
  
- 		if (err)
- 			return err;
- 	} else {
- 		in = kvzalloc(inlen, GFP_KERNEL);
- 		if (!in)
- 			return -ENOMEM;
- 	}
+ 	if (err)
+ 		return err;
  
  	if (is_sqp(init_attr->qp_type))
  		qp->port = init_attr->port_num;
@@@ -2653,127 -2739,112 +2736,152 @@@ static int set_mlx_qp_type(struct mlx5_
  }
  
  struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd,
 -				struct ib_qp_init_attr *init_attr,
 +				struct ib_qp_init_attr *verbs_init_attr,
  				struct ib_udata *udata)
  {
 -	u32 uidx = MLX5_IB_DEFAULT_UIDX;
  	struct mlx5_ib_dev *dev;
  	struct mlx5_ib_qp *qp;
 -	enum ib_qp_type type;
 -	void *ucmd = NULL;
  	u16 xrcdn = 0;
  	int err;
 +	struct ib_qp_init_attr mlx_init_attr;
 +	struct ib_qp_init_attr *init_attr = verbs_init_attr;
 +	struct mlx5_ib_ucontext *ucontext = rdma_udata_to_drv_context(
 +		udata, struct mlx5_ib_ucontext, ibucontext);
  
 -	dev = pd ? to_mdev(pd->device) :
 -		   to_mdev(to_mxrcd(init_attr->xrcd)->ibxrcd.device);
 +	if (pd) {
 +		dev = to_mdev(pd->device);
  
 -	err = check_qp_type(dev, init_attr, &type);
 -	if (err) {
 -		mlx5_ib_dbg(dev, "Unsupported QP type %d\n",
 -			    init_attr->qp_type);
 -		return ERR_PTR(err);
 +		if (init_attr->qp_type == IB_QPT_RAW_PACKET) {
 +			if (!ucontext) {
 +				mlx5_ib_dbg(dev, "Raw Packet QP is not supported for kernel consumers\n");
 +				return ERR_PTR(-EINVAL);
 +			} else if (!ucontext->cqe_version) {
 +				mlx5_ib_dbg(dev, "Raw Packet QP is only supported for CQE version > 0\n");
 +				return ERR_PTR(-EINVAL);
 +			}
 +		}
 +	} else {
 +		/* being cautious here */
 +		if (init_attr->qp_type != IB_QPT_XRC_TGT &&
 +		    init_attr->qp_type != MLX5_IB_QPT_REG_UMR) {
 +			pr_warn("%s: no PD for transport %s\n", __func__,
 +				ib_qp_type_str(init_attr->qp_type));
 +			return ERR_PTR(-EINVAL);
 +		}
 +		dev = to_mdev(to_mxrcd(init_attr->xrcd)->ibxrcd.device);
  	}
  
 -	err = check_valid_flow(dev, pd, init_attr, udata);
 -	if (err)
 -		return ERR_PTR(err);
 +	if (init_attr->qp_type == IB_QPT_DRIVER) {
 +		struct mlx5_ib_create_qp ucmd;
  
 -	if (init_attr->qp_type == IB_QPT_GSI)
 -		return mlx5_ib_gsi_create_qp(pd, init_attr);
 +		init_attr = &mlx_init_attr;
 +		memcpy(init_attr, verbs_init_attr, sizeof(*verbs_init_attr));
 +		err = set_mlx_qp_type(dev, init_attr, &ucmd, udata);
 +		if (err)
 +			return ERR_PTR(err);
  
 -	if (udata) {
 -		size_t inlen =
 -			process_udata_size(init_attr, udata);
 +		if (init_attr->qp_type == MLX5_IB_QPT_DCI) {
 +			if (init_attr->cap.max_recv_wr ||
 +			    init_attr->cap.max_recv_sge) {
 +				mlx5_ib_dbg(dev, "DCI QP requires zero size receive queue\n");
 +				return ERR_PTR(-EINVAL);
 +			}
 +		} else {
 +			return mlx5_ib_create_dct(pd, init_attr, &ucmd, udata);
 +		}
 +	}
  
 -		if (!inlen)
 -			return ERR_PTR(-EINVAL);
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_XRC_TGT:
 +	case IB_QPT_XRC_INI:
 +		if (!MLX5_CAP_GEN(dev->mdev, xrc)) {
 +			mlx5_ib_dbg(dev, "XRC not supported\n");
 +			return ERR_PTR(-ENOSYS);
 +		}
 +		init_attr->recv_cq = NULL;
 +		if (init_attr->qp_type == IB_QPT_XRC_TGT) {
 +			xrcdn = to_mxrcd(init_attr->xrcd)->xrcdn;
 +			init_attr->send_cq = NULL;
 +		}
  
 -		ucmd = kzalloc(inlen, GFP_KERNEL);
 -		if (!ucmd)
 +		/* fall through */
 +	case IB_QPT_RAW_PACKET:
 +	case IB_QPT_RC:
 +	case IB_QPT_UC:
 +	case IB_QPT_UD:
 +	case IB_QPT_SMI:
 +	case MLX5_IB_QPT_HW_GSI:
 +	case MLX5_IB_QPT_REG_UMR:
 +	case MLX5_IB_QPT_DCI:
 +		qp = kzalloc(sizeof(*qp), GFP_KERNEL);
 +		if (!qp)
  			return ERR_PTR(-ENOMEM);
  
 -		err = ib_copy_from_udata(ucmd, udata, inlen);
 -		if (err)
 -			goto free_ucmd;
 -	}
 +		err = create_qp_common(dev, pd, init_attr, udata, qp);
 +		if (err) {
 +			mlx5_ib_dbg(dev, "create_qp_common failed\n");
 +			kfree(qp);
 +			return ERR_PTR(err);
 +		}
  
 -	qp = kzalloc(sizeof(*qp), GFP_KERNEL);
 -	if (!qp) {
 -		err = -ENOMEM;
 -		goto free_ucmd;
 -	}
 +		if (is_qp0(init_attr->qp_type))
 +			qp->ibqp.qp_num = 0;
 +		else if (is_qp1(init_attr->qp_type))
 +			qp->ibqp.qp_num = 1;
 +		else
 +			qp->ibqp.qp_num = qp->trans_qp.base.mqp.qpn;
  
 -	qp->type = type;
 -	if (udata) {
 -		err = process_vendor_flags(dev, qp, ucmd, init_attr);
 -		if (err)
 -			goto free_qp;
 +		mlx5_ib_dbg(dev, "ib qpnum 0x%x, mlx qpn 0x%x, rcqn 0x%x, scqn 0x%x\n",
 +			    qp->ibqp.qp_num, qp->trans_qp.base.mqp.qpn,
 +			    init_attr->recv_cq ? to_mcq(init_attr->recv_cq)->mcq.cqn : -1,
 +			    init_attr->send_cq ? to_mcq(init_attr->send_cq)->mcq.cqn : -1);
  
 -		err = get_qp_uidx(qp, udata, ucmd, init_attr, &uidx);
 -		if (err)
 -			goto free_qp;
 -	}
 -	err = process_create_flags(dev, qp, init_attr);
 -	if (err)
 -		goto free_qp;
 +		qp->trans_qp.xrcdn = xrcdn;
 +
++<<<<<<< HEAD
 +		break;
 +
 +	case IB_QPT_GSI:
 +		return mlx5_ib_gsi_create_qp(pd, init_attr);
  
 +	case IB_QPT_RAW_IPV6:
 +	case IB_QPT_RAW_ETHERTYPE:
 +	case IB_QPT_MAX:
 +	default:
 +		mlx5_ib_dbg(dev, "unsupported qp type %d\n",
 +			    init_attr->qp_type);
 +		/* Don't support raw QPs */
 +		return ERR_PTR(-EINVAL);
++=======
+ 	err = check_qp_attr(dev, qp, init_attr);
+ 	if (err)
+ 		goto free_qp;
+ 
+ 	switch (qp->type) {
+ 	case IB_QPT_RAW_PACKET:
+ 		err = create_raw_qp(pd, qp, init_attr, ucmd, udata, uidx);
+ 		break;
+ 	case MLX5_IB_QPT_DCT:
+ 		err = create_dct(pd, qp, init_attr, ucmd, uidx);
+ 		break;
+ 	case IB_QPT_XRC_TGT:
+ 		xrcdn = to_mxrcd(init_attr->xrcd)->xrcdn;
+ 		err = create_xrc_tgt_qp(dev, init_attr, qp, udata, uidx);
+ 		break;
+ 	default:
+ 		err = create_qp_common(dev, pd, init_attr, ucmd, udata, qp,
+ 				       uidx);
+ 	}
+ 	if (err) {
+ 		mlx5_ib_dbg(dev, "create_qp failed %d\n", err);
+ 		goto free_qp;
++>>>>>>> 04bcc1c2d0d7 (RDMA/mlx5: Separate XRC_TGT QP creation from common flow)
  	}
  
 -	kfree(ucmd);
 -
 -	if (is_qp0(init_attr->qp_type))
 -		qp->ibqp.qp_num = 0;
 -	else if (is_qp1(init_attr->qp_type))
 -		qp->ibqp.qp_num = 1;
 -	else
 -		qp->ibqp.qp_num = qp->trans_qp.base.mqp.qpn;
 -
 -	qp->trans_qp.xrcdn = xrcdn;
 +	if (verbs_init_attr->qp_type == IB_QPT_DRIVER)
 +		qp->qp_sub_type = init_attr->qp_type;
  
  	return &qp->ibqp;
 -
 -free_qp:
 -	kfree(qp);
 -free_ucmd:
 -	kfree(ucmd);
 -	return ERR_PTR(err);
  }
  
  static int mlx5_ib_destroy_dct(struct mlx5_ib_qp *mqp)
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
