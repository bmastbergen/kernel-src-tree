mm: convert totalram_pages and totalhigh_pages variables to atomic

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Arun KS <arunks@codeaurora.org>
commit ca79b0c211af63fa3276f0e3fd7dd9ada2439839
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/ca79b0c2.failed

totalram_pages and totalhigh_pages are made static inline function.

Main motivation was that managed_page_count_lock handling was complicating
things.  It was discussed in length here,
https://lore.kernel.org/patchwork/patch/995739/#1181785 So it seemes
better to remove the lock and convert variables to atomic, with preventing
poteintial store-to-read tearing as a bonus.

[akpm@linux-foundation.org: coding style fixes]
Link: http://lkml.kernel.org/r/1542090790-21750-4-git-send-email-arunks@codeaurora.org
	Signed-off-by: Arun KS <arunks@codeaurora.org>
	Suggested-by: Michal Hocko <mhocko@suse.com>
	Suggested-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: David Hildenbrand <david@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ca79b0c211af63fa3276f0e3fd7dd9ada2439839)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/csky/mm/init.c
#	drivers/gpu/drm/i915/i915_gem.c
#	drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
#	drivers/misc/vmw_balloon.c
#	fs/fuse/inode.c
#	mm/page_alloc.c
diff --cc drivers/gpu/drm/i915/i915_gem.c
index 5f6e63952821,a9de07bb72c8..000000000000
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@@ -856,234 -1030,4510 +856,2396 @@@ err
  	return ret;
  }
  
 -static void
 -shmem_clflush_swizzled_range(char *addr, unsigned long length,
 -			     bool swizzled)
 -{
 -	if (unlikely(swizzled)) {
 -		unsigned long start = (unsigned long) addr;
 -		unsigned long end = (unsigned long) addr + length;
 -
 -		/* For swizzling simply ensure that we always flush both
 -		 * channels. Lame, but simple and it works. Swizzled
 -		 * pwrite/pread is far from a hotpath - current userspace
 -		 * doesn't use it at all. */
 -		start = round_down(start, 128);
 -		end = round_up(end, 128);
 -
 -		drm_clflush_virt_range((void *)start, end - start);
 -	} else {
 -		drm_clflush_virt_range(addr, length);
 -	}
 -
 -}
 -
 -/* Only difference to the fast-path function is that this can handle bit17
 - * and uses non-atomic copy and kmap functions. */
 -static int
 -shmem_pread_slow(struct page *page, int offset, int length,
 -		 char __user *user_data,
 -		 bool page_do_bit17_swizzling, bool needs_clflush)
 -{
 -	char *vaddr;
 -	int ret;
 -
 -	vaddr = kmap(page);
 -	if (needs_clflush)
 -		shmem_clflush_swizzled_range(vaddr + offset, length,
 -					     page_do_bit17_swizzling);
 -
 -	if (page_do_bit17_swizzling)
 -		ret = __copy_to_user_swizzled(user_data, vaddr, offset, length);
 -	else
 -		ret = __copy_to_user(user_data, vaddr + offset, length);
 -	kunmap(page);
 -
 -	return ret ? - EFAULT : 0;
 -}
 -
 -static int
 -shmem_pread(struct page *page, int offset, int length, char __user *user_data,
 -	    bool page_do_bit17_swizzling, bool needs_clflush)
 +/**
 + * Called when user space has done writes to this buffer
 + * @dev: drm device
 + * @data: ioctl data blob
 + * @file: drm file
 + */
 +int
 +i915_gem_sw_finish_ioctl(struct drm_device *dev, void *data,
 +			 struct drm_file *file)
  {
 -	int ret;
 +	struct drm_i915_gem_sw_finish *args = data;
 +	struct drm_i915_gem_object *obj;
  
 -	ret = -ENODEV;
 -	if (!page_do_bit17_swizzling) {
 -		char *vaddr = kmap_atomic(page);
 +	obj = i915_gem_object_lookup(file, args->handle);
 +	if (!obj)
 +		return -ENOENT;
  
 -		if (needs_clflush)
 -			drm_clflush_virt_range(vaddr + offset, length);
 -		ret = __copy_to_user_inatomic(user_data, vaddr + offset, length);
 -		kunmap_atomic(vaddr);
 -	}
 -	if (ret == 0)
 -		return 0;
 +	/*
 +	 * Proxy objects are barred from CPU access, so there is no
 +	 * need to ban sw_finish as it is a nop.
 +	 */
 +
 +	/* Pinned buffers may be scanout, so flush the cache */
 +	i915_gem_object_flush_if_display(obj);
 +	i915_gem_object_put(obj);
  
 -	return shmem_pread_slow(page, offset, length, user_data,
 -				page_do_bit17_swizzling, needs_clflush);
 +	return 0;
  }
  
 -static int
 -i915_gem_shmem_pread(struct drm_i915_gem_object *obj,
 -		     struct drm_i915_gem_pread *args)
 +void i915_gem_runtime_suspend(struct drm_i915_private *i915)
  {
 -	char __user *user_data;
 -	u64 remain;
 -	unsigned int obj_do_bit17_swizzling;
 -	unsigned int needs_clflush;
 -	unsigned int idx, offset;
 -	int ret;
 -
 -	obj_do_bit17_swizzling = 0;
 -	if (i915_gem_object_needs_bit17_swizzle(obj))
 -		obj_do_bit17_swizzling = BIT(17);
 -
 -	ret = mutex_lock_interruptible(&obj->base.dev->struct_mutex);
 -	if (ret)
 -		return ret;
 -
 -	ret = i915_gem_obj_prepare_shmem_read(obj, &needs_clflush);
 -	mutex_unlock(&obj->base.dev->struct_mutex);
 -	if (ret)
 -		return ret;
 +	struct drm_i915_gem_object *obj, *on;
 +	int i;
  
 -	remain = args->size;
 -	user_data = u64_to_user_ptr(args->data_ptr);
 -	offset = offset_in_page(args->offset);
 -	for (idx = args->offset >> PAGE_SHIFT; remain; idx++) {
 -		struct page *page = i915_gem_object_get_page(obj, idx);
 -		unsigned int length = min_t(u64, remain, PAGE_SIZE - offset);
 +	/*
 +	 * Only called during RPM suspend. All users of the userfault_list
 +	 * must be holding an RPM wakeref to ensure that this can not
 +	 * run concurrently with themselves (and use the struct_mutex for
 +	 * protection between themselves).
 +	 */
  
 -		ret = shmem_pread(page, offset, length, user_data,
 -				  page_to_phys(page) & obj_do_bit17_swizzling,
 -				  needs_clflush);
 -		if (ret)
 -			break;
 +	list_for_each_entry_safe(obj, on,
 +				 &i915->ggtt.userfault_list, userfault_link)
 +		__i915_gem_object_release_mmap_gtt(obj);
  
 -		remain -= length;
 -		user_data += length;
 -		offset = 0;
 -	}
 +	/*
 +	 * The fence will be lost when the device powers down. If any were
 +	 * in use by hardware (i.e. they are pinned), we should not be powering
 +	 * down! All other fences will be reacquired by the user upon waking.
 +	 */
 +	for (i = 0; i < i915->ggtt.num_fences; i++) {
 +		struct i915_fence_reg *reg = &i915->ggtt.fence_regs[i];
  
 -	i915_gem_obj_finish_shmem_access(obj);
 -	return ret;
 -}
 +		/*
 +		 * Ideally we want to assert that the fence register is not
 +		 * live at this point (i.e. that no piece of code will be
 +		 * trying to write through fence + GTT, as that both violates
 +		 * our tracking of activity and associated locking/barriers,
 +		 * but also is illegal given that the hw is powered down).
 +		 *
 +		 * Previously we used reg->pin_count as a "liveness" indicator.
 +		 * That is not sufficient, and we need a more fine-grained
 +		 * tool if we want to have a sanity check here.
 +		 */
  
 -static inline bool
 -gtt_user_read(struct io_mapping *mapping,
 -	      loff_t base, int offset,
 -	      char __user *user_data, int length)
 -{
 -	void __iomem *vaddr;
 -	unsigned long unwritten;
 +		if (!reg->vma)
 +			continue;
  
 -	/* We can use the cpu mem copy function because this is X86. */
 -	vaddr = io_mapping_map_atomic_wc(mapping, base);
 -	unwritten = __copy_to_user_inatomic(user_data,
 -					    (void __force *)vaddr + offset,
 -					    length);
 -	io_mapping_unmap_atomic(vaddr);
 -	if (unwritten) {
 -		vaddr = io_mapping_map_wc(mapping, base, PAGE_SIZE);
 -		unwritten = copy_to_user(user_data,
 -					 (void __force *)vaddr + offset,
 -					 length);
 -		io_mapping_unmap(vaddr);
 +		GEM_BUG_ON(i915_vma_has_userfault(reg->vma));
 +		reg->dirty = true;
  	}
 -	return unwritten;
  }
  
 -static int
 -i915_gem_gtt_pread(struct drm_i915_gem_object *obj,
 -		   const struct drm_i915_gem_pread *args)
++<<<<<<< HEAD
++=======
++static int i915_gem_object_create_mmap_offset(struct drm_i915_gem_object *obj)
+ {
 -	struct drm_i915_private *i915 = to_i915(obj->base.dev);
 -	struct i915_ggtt *ggtt = &i915->ggtt;
 -	struct drm_mm_node node;
 -	struct i915_vma *vma;
 -	void __user *user_data;
 -	u64 remain, offset;
 -	int ret;
 -
 -	ret = mutex_lock_interruptible(&i915->drm.struct_mutex);
 -	if (ret)
 -		return ret;
 -
 -	intel_runtime_pm_get(i915);
 -	vma = i915_gem_object_ggtt_pin(obj, NULL, 0, 0,
 -				       PIN_MAPPABLE |
 -				       PIN_NONFAULT |
 -				       PIN_NONBLOCK);
 -	if (!IS_ERR(vma)) {
 -		node.start = i915_ggtt_offset(vma);
 -		node.allocated = false;
 -		ret = i915_vma_put_fence(vma);
 -		if (ret) {
 -			i915_vma_unpin(vma);
 -			vma = ERR_PTR(ret);
 -		}
 -	}
 -	if (IS_ERR(vma)) {
 -		ret = insert_mappable_node(ggtt, &node, PAGE_SIZE);
 -		if (ret)
 -			goto out_unlock;
 -		GEM_BUG_ON(!node.allocated);
 -	}
 -
 -	ret = i915_gem_object_set_to_gtt_domain(obj, false);
 -	if (ret)
 -		goto out_unpin;
 -
 -	mutex_unlock(&i915->drm.struct_mutex);
++	struct drm_i915_private *dev_priv = to_i915(obj->base.dev);
++	int err;
+ 
 -	user_data = u64_to_user_ptr(args->data_ptr);
 -	remain = args->size;
 -	offset = args->offset;
++	err = drm_gem_create_mmap_offset(&obj->base);
++	if (likely(!err))
++		return 0;
+ 
 -	while (remain > 0) {
 -		/* Operation in this page
 -		 *
 -		 * page_base = page offset within aperture
 -		 * page_offset = offset within page
 -		 * page_length = bytes to copy for this page
 -		 */
 -		u32 page_base = node.start;
 -		unsigned page_offset = offset_in_page(offset);
 -		unsigned page_length = PAGE_SIZE - page_offset;
 -		page_length = remain < page_length ? remain : page_length;
 -		if (node.allocated) {
 -			wmb();
 -			ggtt->vm.insert_page(&ggtt->vm,
 -					     i915_gem_object_get_dma_address(obj, offset >> PAGE_SHIFT),
 -					     node.start, I915_CACHE_NONE, 0);
 -			wmb();
 -		} else {
 -			page_base += offset & PAGE_MASK;
 -		}
++	/* Attempt to reap some mmap space from dead objects */
++	do {
++		err = i915_gem_wait_for_idle(dev_priv,
++					     I915_WAIT_INTERRUPTIBLE,
++					     MAX_SCHEDULE_TIMEOUT);
++		if (err)
++			break;
+ 
 -		if (gtt_user_read(&ggtt->iomap, page_base, page_offset,
 -				  user_data, page_length)) {
 -			ret = -EFAULT;
++		i915_gem_drain_freed_objects(dev_priv);
++		err = drm_gem_create_mmap_offset(&obj->base);
++		if (!err)
+ 			break;
 -		}
+ 
 -		remain -= page_length;
 -		user_data += page_length;
 -		offset += page_length;
 -	}
++	} while (flush_delayed_work(&dev_priv->gt.retire_work));
+ 
 -	mutex_lock(&i915->drm.struct_mutex);
 -out_unpin:
 -	if (node.allocated) {
 -		wmb();
 -		ggtt->vm.clear_range(&ggtt->vm, node.start, node.size);
 -		remove_mappable_node(&node);
 -	} else {
 -		i915_vma_unpin(vma);
 -	}
 -out_unlock:
 -	intel_runtime_pm_put(i915);
 -	mutex_unlock(&i915->drm.struct_mutex);
++	return err;
++}
+ 
 -	return ret;
++static void i915_gem_object_free_mmap_offset(struct drm_i915_gem_object *obj)
++{
++	drm_gem_free_mmap_offset(&obj->base);
+ }
+ 
 -/**
 - * Reads data from the object referenced by handle.
 - * @dev: drm device pointer
 - * @data: ioctl data blob
 - * @file: drm file pointer
 - *
 - * On error, the contents of *data are undefined.
 - */
+ int
 -i915_gem_pread_ioctl(struct drm_device *dev, void *data,
 -		     struct drm_file *file)
++i915_gem_mmap_gtt(struct drm_file *file,
++		  struct drm_device *dev,
++		  uint32_t handle,
++		  uint64_t *offset)
+ {
 -	struct drm_i915_gem_pread *args = data;
+ 	struct drm_i915_gem_object *obj;
+ 	int ret;
+ 
 -	if (args->size == 0)
 -		return 0;
 -
 -	if (!access_ok(VERIFY_WRITE,
 -		       u64_to_user_ptr(args->data_ptr),
 -		       args->size))
 -		return -EFAULT;
 -
 -	obj = i915_gem_object_lookup(file, args->handle);
++	obj = i915_gem_object_lookup(file, handle);
+ 	if (!obj)
+ 		return -ENOENT;
+ 
 -	/* Bounds check source.  */
 -	if (range_overflows_t(u64, args->offset, args->size, obj->base.size)) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 -
 -	trace_i915_gem_object_pread(obj, args->offset, args->size);
 -
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE,
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   to_rps_client(file));
 -	if (ret)
 -		goto out;
 -
 -	ret = i915_gem_object_pin_pages(obj);
 -	if (ret)
 -		goto out;
 -
 -	ret = i915_gem_shmem_pread(obj, args);
 -	if (ret == -EFAULT || ret == -ENODEV)
 -		ret = i915_gem_gtt_pread(obj, args);
++	ret = i915_gem_object_create_mmap_offset(obj);
++	if (ret == 0)
++		*offset = drm_vma_node_offset_addr(&obj->base.vma_node);
+ 
 -	i915_gem_object_unpin_pages(obj);
 -out:
+ 	i915_gem_object_put(obj);
+ 	return ret;
+ }
+ 
 -/* This is the fast write path which cannot handle
 - * page faults in the source data
++/**
++ * i915_gem_mmap_gtt_ioctl - prepare an object for GTT mmap'ing
++ * @dev: DRM device
++ * @data: GTT mapping ioctl data
++ * @file: GEM object info
++ *
++ * Simply returns the fake offset to userspace so it can mmap it.
++ * The mmap call will end up in drm_gem_mmap(), which will set things
++ * up so we can get faults in the handler above.
++ *
++ * The fault handler will take care of binding the object into the GTT
++ * (since it may have been evicted to make room for something), allocating
++ * a fence register, and mapping the appropriate aperture address into
++ * userspace.
+  */
++int
++i915_gem_mmap_gtt_ioctl(struct drm_device *dev, void *data,
++			struct drm_file *file)
++{
++	struct drm_i915_gem_mmap_gtt *args = data;
+ 
 -static inline bool
 -ggtt_write(struct io_mapping *mapping,
 -	   loff_t base, int offset,
 -	   char __user *user_data, int length)
++	return i915_gem_mmap_gtt(file, dev, args->handle, &args->offset);
++}
++
++/* Immediately discard the backing storage */
++static void
++i915_gem_object_truncate(struct drm_i915_gem_object *obj)
+ {
 -	void __iomem *vaddr;
 -	unsigned long unwritten;
++	i915_gem_object_free_mmap_offset(obj);
+ 
 -	/* We can use the cpu mem copy function because this is X86. */
 -	vaddr = io_mapping_map_atomic_wc(mapping, base);
 -	unwritten = __copy_from_user_inatomic_nocache((void __force *)vaddr + offset,
 -						      user_data, length);
 -	io_mapping_unmap_atomic(vaddr);
 -	if (unwritten) {
 -		vaddr = io_mapping_map_wc(mapping, base, PAGE_SIZE);
 -		unwritten = copy_from_user((void __force *)vaddr + offset,
 -					   user_data, length);
 -		io_mapping_unmap(vaddr);
++	if (obj->base.filp == NULL)
++		return;
++
++	/* Our goal here is to return as much of the memory as
++	 * is possible back to the system as we are called from OOM.
++	 * To do this we must instruct the shmfs to drop all of its
++	 * backing pages, *now*.
++	 */
++	shmem_truncate_range(file_inode(obj->base.filp), 0, (loff_t)-1);
++	obj->mm.madv = __I915_MADV_PURGED;
++	obj->mm.pages = ERR_PTR(-EFAULT);
++}
++
++/* Try to discard unwanted pages */
++void __i915_gem_object_invalidate(struct drm_i915_gem_object *obj)
++{
++	struct address_space *mapping;
++
++	lockdep_assert_held(&obj->mm.lock);
++	GEM_BUG_ON(i915_gem_object_has_pages(obj));
++
++	switch (obj->mm.madv) {
++	case I915_MADV_DONTNEED:
++		i915_gem_object_truncate(obj);
++	case __I915_MADV_PURGED:
++		return;
+ 	}
+ 
 -	return unwritten;
++	if (obj->base.filp == NULL)
++		return;
++
++	mapping = obj->base.filp->f_mapping,
++	invalidate_mapping_pages(mapping, 0, (loff_t)-1);
+ }
+ 
 -/**
 - * This is the fast pwrite path, where we copy the data directly from the
 - * user into the GTT, uncached.
 - * @obj: i915 GEM object
 - * @args: pwrite arguments structure
++/*
++ * Move pages to appropriate lru and release the pagevec, decrementing the
++ * ref count of those pages.
+  */
 -static int
 -i915_gem_gtt_pwrite_fast(struct drm_i915_gem_object *obj,
 -			 const struct drm_i915_gem_pwrite *args)
++static void check_release_pagevec(struct pagevec *pvec)
+ {
 -	struct drm_i915_private *i915 = to_i915(obj->base.dev);
 -	struct i915_ggtt *ggtt = &i915->ggtt;
 -	struct drm_mm_node node;
 -	struct i915_vma *vma;
 -	u64 remain, offset;
 -	void __user *user_data;
 -	int ret;
++	check_move_unevictable_pages(pvec);
++	__pagevec_release(pvec);
++	cond_resched();
++}
+ 
 -	ret = mutex_lock_interruptible(&i915->drm.struct_mutex);
 -	if (ret)
 -		return ret;
++static void
++i915_gem_object_put_pages_gtt(struct drm_i915_gem_object *obj,
++			      struct sg_table *pages)
++{
++	struct sgt_iter sgt_iter;
++	struct pagevec pvec;
++	struct page *page;
+ 
 -	if (i915_gem_object_has_struct_page(obj)) {
 -		/*
 -		 * Avoid waking the device up if we can fallback, as
 -		 * waking/resuming is very slow (worst-case 10-100 ms
 -		 * depending on PCI sleeps and our own resume time).
 -		 * This easily dwarfs any performance advantage from
 -		 * using the cache bypass of indirect GGTT access.
 -		 */
 -		if (!intel_runtime_pm_get_if_in_use(i915)) {
 -			ret = -EFAULT;
 -			goto out_unlock;
 -		}
 -	} else {
 -		/* No backing pages, no fallback, we must force GGTT access */
 -		intel_runtime_pm_get(i915);
 -	}
++	__i915_gem_object_release_shmem(obj, pages, true);
+ 
 -	vma = i915_gem_object_ggtt_pin(obj, NULL, 0, 0,
 -				       PIN_MAPPABLE |
 -				       PIN_NONFAULT |
 -				       PIN_NONBLOCK);
 -	if (!IS_ERR(vma)) {
 -		node.start = i915_ggtt_offset(vma);
 -		node.allocated = false;
 -		ret = i915_vma_put_fence(vma);
 -		if (ret) {
 -			i915_vma_unpin(vma);
 -			vma = ERR_PTR(ret);
 -		}
 -	}
 -	if (IS_ERR(vma)) {
 -		ret = insert_mappable_node(ggtt, &node, PAGE_SIZE);
 -		if (ret)
 -			goto out_rpm;
 -		GEM_BUG_ON(!node.allocated);
 -	}
++	i915_gem_gtt_finish_pages(obj, pages);
+ 
 -	ret = i915_gem_object_set_to_gtt_domain(obj, true);
 -	if (ret)
 -		goto out_unpin;
++	if (i915_gem_object_needs_bit17_swizzle(obj))
++		i915_gem_object_save_bit_17_swizzle(obj, pages);
+ 
 -	mutex_unlock(&i915->drm.struct_mutex);
++	mapping_clear_unevictable(file_inode(obj->base.filp)->i_mapping);
+ 
 -	intel_fb_obj_invalidate(obj, ORIGIN_CPU);
++	pagevec_init(&pvec);
++	for_each_sgt_page(page, sgt_iter, pages) {
++		if (obj->mm.dirty)
++			set_page_dirty(page);
+ 
 -	user_data = u64_to_user_ptr(args->data_ptr);
 -	offset = args->offset;
 -	remain = args->size;
 -	while (remain) {
 -		/* Operation in this page
 -		 *
 -		 * page_base = page offset within aperture
 -		 * page_offset = offset within page
 -		 * page_length = bytes to copy for this page
 -		 */
 -		u32 page_base = node.start;
 -		unsigned int page_offset = offset_in_page(offset);
 -		unsigned int page_length = PAGE_SIZE - page_offset;
 -		page_length = remain < page_length ? remain : page_length;
 -		if (node.allocated) {
 -			wmb(); /* flush the write before we modify the GGTT */
 -			ggtt->vm.insert_page(&ggtt->vm,
 -					     i915_gem_object_get_dma_address(obj, offset >> PAGE_SHIFT),
 -					     node.start, I915_CACHE_NONE, 0);
 -			wmb(); /* flush modifications to the GGTT (insert_page) */
 -		} else {
 -			page_base += offset & PAGE_MASK;
 -		}
 -		/* If we get a fault while copying data, then (presumably) our
 -		 * source page isn't available.  Return the error and we'll
 -		 * retry in the slow path.
 -		 * If the object is non-shmem backed, we retry again with the
 -		 * path that handles page fault.
 -		 */
 -		if (ggtt_write(&ggtt->iomap, page_base, page_offset,
 -			       user_data, page_length)) {
 -			ret = -EFAULT;
 -			break;
 -		}
++		if (obj->mm.madv == I915_MADV_WILLNEED)
++			mark_page_accessed(page);
+ 
 -		remain -= page_length;
 -		user_data += page_length;
 -		offset += page_length;
++		if (!pagevec_add(&pvec, page))
++			check_release_pagevec(&pvec);
+ 	}
 -	intel_fb_obj_flush(obj, ORIGIN_CPU);
++	if (pagevec_count(&pvec))
++		check_release_pagevec(&pvec);
++	obj->mm.dirty = false;
+ 
 -	mutex_lock(&i915->drm.struct_mutex);
 -out_unpin:
 -	if (node.allocated) {
 -		wmb();
 -		ggtt->vm.clear_range(&ggtt->vm, node.start, node.size);
 -		remove_mappable_node(&node);
 -	} else {
 -		i915_vma_unpin(vma);
 -	}
 -out_rpm:
 -	intel_runtime_pm_put(i915);
 -out_unlock:
 -	mutex_unlock(&i915->drm.struct_mutex);
 -	return ret;
++	sg_free_table(pages);
++	kfree(pages);
+ }
+ 
 -static int
 -shmem_pwrite_slow(struct page *page, int offset, int length,
 -		  char __user *user_data,
 -		  bool page_do_bit17_swizzling,
 -		  bool needs_clflush_before,
 -		  bool needs_clflush_after)
++static void __i915_gem_object_reset_page_iter(struct drm_i915_gem_object *obj)
+ {
 -	char *vaddr;
 -	int ret;
 -
 -	vaddr = kmap(page);
 -	if (unlikely(needs_clflush_before || page_do_bit17_swizzling))
 -		shmem_clflush_swizzled_range(vaddr + offset, length,
 -					     page_do_bit17_swizzling);
 -	if (page_do_bit17_swizzling)
 -		ret = __copy_from_user_swizzled(vaddr, offset, user_data,
 -						length);
 -	else
 -		ret = __copy_from_user(vaddr + offset, user_data, length);
 -	if (needs_clflush_after)
 -		shmem_clflush_swizzled_range(vaddr + offset, length,
 -					     page_do_bit17_swizzling);
 -	kunmap(page);
++	struct radix_tree_iter iter;
++	void __rcu **slot;
+ 
 -	return ret ? -EFAULT : 0;
++	rcu_read_lock();
++	radix_tree_for_each_slot(slot, &obj->mm.get_page.radix, &iter, 0)
++		radix_tree_delete(&obj->mm.get_page.radix, iter.index);
++	rcu_read_unlock();
+ }
+ 
 -/* Per-page copy function for the shmem pwrite fastpath.
 - * Flushes invalid cachelines before writing to the target if
 - * needs_clflush_before is set and flushes out any written cachelines after
 - * writing if needs_clflush is set.
 - */
 -static int
 -shmem_pwrite(struct page *page, int offset, int len, char __user *user_data,
 -	     bool page_do_bit17_swizzling,
 -	     bool needs_clflush_before,
 -	     bool needs_clflush_after)
++static struct sg_table *
++__i915_gem_object_unset_pages(struct drm_i915_gem_object *obj)
+ {
 -	int ret;
++	struct drm_i915_private *i915 = to_i915(obj->base.dev);
++	struct sg_table *pages;
+ 
 -	ret = -ENODEV;
 -	if (!page_do_bit17_swizzling) {
 -		char *vaddr = kmap_atomic(page);
++	pages = fetch_and_zero(&obj->mm.pages);
++	if (!pages)
++		return NULL;
++
++	spin_lock(&i915->mm.obj_lock);
++	list_del(&obj->mm.link);
++	spin_unlock(&i915->mm.obj_lock);
++
++	if (obj->mm.mapping) {
++		void *ptr;
+ 
 -		if (needs_clflush_before)
 -			drm_clflush_virt_range(vaddr + offset, len);
 -		ret = __copy_from_user_inatomic(vaddr + offset, user_data, len);
 -		if (needs_clflush_after)
 -			drm_clflush_virt_range(vaddr + offset, len);
++		ptr = page_mask_bits(obj->mm.mapping);
++		if (is_vmalloc_addr(ptr))
++			vunmap(ptr);
++		else
++			kunmap(kmap_to_page(ptr));
+ 
 -		kunmap_atomic(vaddr);
++		obj->mm.mapping = NULL;
+ 	}
 -	if (ret == 0)
 -		return ret;
+ 
 -	return shmem_pwrite_slow(page, offset, len, user_data,
 -				 page_do_bit17_swizzling,
 -				 needs_clflush_before,
 -				 needs_clflush_after);
++	__i915_gem_object_reset_page_iter(obj);
++	obj->mm.page_sizes.phys = obj->mm.page_sizes.sg = 0;
++
++	return pages;
+ }
+ 
 -static int
 -i915_gem_shmem_pwrite(struct drm_i915_gem_object *obj,
 -		      const struct drm_i915_gem_pwrite *args)
++void __i915_gem_object_put_pages(struct drm_i915_gem_object *obj,
++				 enum i915_mm_subclass subclass)
+ {
 -	struct drm_i915_private *i915 = to_i915(obj->base.dev);
 -	void __user *user_data;
 -	u64 remain;
 -	unsigned int obj_do_bit17_swizzling;
 -	unsigned int partial_cacheline_write;
 -	unsigned int needs_clflush;
 -	unsigned int offset, idx;
 -	int ret;
++	struct sg_table *pages;
+ 
 -	ret = mutex_lock_interruptible(&i915->drm.struct_mutex);
 -	if (ret)
 -		return ret;
++	if (i915_gem_object_has_pinned_pages(obj))
++		return;
+ 
 -	ret = i915_gem_obj_prepare_shmem_write(obj, &needs_clflush);
 -	mutex_unlock(&i915->drm.struct_mutex);
 -	if (ret)
 -		return ret;
++	GEM_BUG_ON(obj->bind_count);
++	if (!i915_gem_object_has_pages(obj))
++		return;
+ 
 -	obj_do_bit17_swizzling = 0;
 -	if (i915_gem_object_needs_bit17_swizzle(obj))
 -		obj_do_bit17_swizzling = BIT(17);
++	/* May be called by shrinker from within get_pages() (on another bo) */
++	mutex_lock_nested(&obj->mm.lock, subclass);
++	if (unlikely(atomic_read(&obj->mm.pages_pin_count)))
++		goto unlock;
+ 
 -	/* If we don't overwrite a cacheline completely we need to be
 -	 * careful to have up-to-date data by first clflushing. Don't
 -	 * overcomplicate things and flush the entire patch.
++	/*
++	 * ->put_pages might need to allocate memory for the bit17 swizzle
++	 * array, hence protect them from being reaped by removing them from gtt
++	 * lists early.
+ 	 */
 -	partial_cacheline_write = 0;
 -	if (needs_clflush & CLFLUSH_BEFORE)
 -		partial_cacheline_write = boot_cpu_data.x86_clflush_size - 1;
 -
 -	user_data = u64_to_user_ptr(args->data_ptr);
 -	remain = args->size;
 -	offset = offset_in_page(args->offset);
 -	for (idx = args->offset >> PAGE_SHIFT; remain; idx++) {
 -		struct page *page = i915_gem_object_get_page(obj, idx);
 -		unsigned int length = min_t(u64, remain, PAGE_SIZE - offset);
 -
 -		ret = shmem_pwrite(page, offset, length, user_data,
 -				   page_to_phys(page) & obj_do_bit17_swizzling,
 -				   (offset | length) & partial_cacheline_write,
 -				   needs_clflush & CLFLUSH_AFTER);
 -		if (ret)
 -			break;
 -
 -		remain -= length;
 -		user_data += length;
 -		offset = 0;
 -	}
++	pages = __i915_gem_object_unset_pages(obj);
++	if (!IS_ERR(pages))
++		obj->ops->put_pages(obj, pages);
+ 
 -	intel_fb_obj_flush(obj, ORIGIN_CPU);
 -	i915_gem_obj_finish_shmem_access(obj);
 -	return ret;
++unlock:
++	mutex_unlock(&obj->mm.lock);
+ }
+ 
 -/**
 - * Writes data to the object referenced by handle.
 - * @dev: drm device
 - * @data: ioctl data blob
 - * @file: drm file
 - *
 - * On error, the contents of the buffer that were to be modified are undefined.
 - */
 -int
 -i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 -		      struct drm_file *file)
++bool i915_sg_trim(struct sg_table *orig_st)
+ {
 -	struct drm_i915_gem_pwrite *args = data;
 -	struct drm_i915_gem_object *obj;
 -	int ret;
 -
 -	if (args->size == 0)
 -		return 0;
 -
 -	if (!access_ok(VERIFY_READ,
 -		       u64_to_user_ptr(args->data_ptr),
 -		       args->size))
 -		return -EFAULT;
 -
 -	obj = i915_gem_object_lookup(file, args->handle);
 -	if (!obj)
 -		return -ENOENT;
 -
 -	/* Bounds check destination. */
 -	if (range_overflows_t(u64, args->offset, args->size, obj->base.size)) {
 -		ret = -EINVAL;
 -		goto err;
 -	}
 -
 -	/* Writes not allowed into this read-only object */
 -	if (i915_gem_object_is_readonly(obj)) {
 -		ret = -EINVAL;
 -		goto err;
 -	}
 -
 -	trace_i915_gem_object_pwrite(obj, args->offset, args->size);
 -
 -	ret = -ENODEV;
 -	if (obj->ops->pwrite)
 -		ret = obj->ops->pwrite(obj, args);
 -	if (ret != -ENODEV)
 -		goto err;
++	struct sg_table new_st;
++	struct scatterlist *sg, *new_sg;
++	unsigned int i;
+ 
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE |
 -				   I915_WAIT_ALL,
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   to_rps_client(file));
 -	if (ret)
 -		goto err;
++	if (orig_st->nents == orig_st->orig_nents)
++		return false;
+ 
 -	ret = i915_gem_object_pin_pages(obj);
 -	if (ret)
 -		goto err;
++	if (sg_alloc_table(&new_st, orig_st->nents, GFP_KERNEL | __GFP_NOWARN))
++		return false;
+ 
 -	ret = -EFAULT;
 -	/* We can only do the GTT pwrite on untiled buffers, as otherwise
 -	 * it would end up going through the fenced access, and we'll get
 -	 * different detiling behavior between reading and writing.
 -	 * pread/pwrite currently are reading and writing from the CPU
 -	 * perspective, requiring manual detiling by the client.
 -	 */
 -	if (!i915_gem_object_has_struct_page(obj) ||
 -	    cpu_write_needs_clflush(obj))
 -		/* Note that the gtt paths might fail with non-page-backed user
 -		 * pointers (e.g. gtt mappings when moving data between
 -		 * textures). Fallback to the shmem path in that case.
 -		 */
 -		ret = i915_gem_gtt_pwrite_fast(obj, args);
++	new_sg = new_st.sgl;
++	for_each_sg(orig_st->sgl, sg, orig_st->nents, i) {
++		sg_set_page(new_sg, sg_page(sg), sg->length, 0);
++		sg_dma_address(new_sg) = sg_dma_address(sg);
++		sg_dma_len(new_sg) = sg_dma_len(sg);
+ 
 -	if (ret == -EFAULT || ret == -ENOSPC) {
 -		if (obj->phys_handle)
 -			ret = i915_gem_phys_pwrite(obj, args, file);
 -		else
 -			ret = i915_gem_shmem_pwrite(obj, args);
++		new_sg = sg_next(new_sg);
+ 	}
++	GEM_BUG_ON(new_sg); /* Should walk exactly nents and hit the end */
+ 
 -	i915_gem_object_unpin_pages(obj);
 -err:
 -	i915_gem_object_put(obj);
 -	return ret;
 -}
 -
 -static void i915_gem_object_bump_inactive_ggtt(struct drm_i915_gem_object *obj)
 -{
 -	struct drm_i915_private *i915;
 -	struct list_head *list;
 -	struct i915_vma *vma;
 -
 -	GEM_BUG_ON(!i915_gem_object_has_pinned_pages(obj));
 -
 -	for_each_ggtt_vma(vma, obj) {
 -		if (i915_vma_is_active(vma))
 -			continue;
 -
 -		if (!drm_mm_node_allocated(&vma->node))
 -			continue;
 -
 -		list_move_tail(&vma->vm_link, &vma->vm->inactive_list);
 -	}
++	sg_free_table(orig_st);
+ 
 -	i915 = to_i915(obj->base.dev);
 -	spin_lock(&i915->mm.obj_lock);
 -	list = obj->bind_count ? &i915->mm.bound_list : &i915->mm.unbound_list;
 -	list_move_tail(&obj->mm.link, list);
 -	spin_unlock(&i915->mm.obj_lock);
++	*orig_st = new_st;
++	return true;
+ }
+ 
 -/**
 - * Called when user space prepares to use an object with the CPU, either
 - * through the mmap ioctl's mapping or a GTT mapping.
 - * @dev: drm device
 - * @data: ioctl data blob
 - * @file: drm file
 - */
 -int
 -i915_gem_set_domain_ioctl(struct drm_device *dev, void *data,
 -			  struct drm_file *file)
++static int i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)
+ {
 -	struct drm_i915_gem_set_domain *args = data;
 -	struct drm_i915_gem_object *obj;
 -	uint32_t read_domains = args->read_domains;
 -	uint32_t write_domain = args->write_domain;
 -	int err;
 -
 -	/* Only handle setting domains to types used by the CPU. */
 -	if ((write_domain | read_domains) & I915_GEM_GPU_DOMAINS)
 -		return -EINVAL;
 -
 -	/* Having something in the write domain implies it's in the read
 -	 * domain, and only that read domain.  Enforce that in the request.
 -	 */
 -	if (write_domain != 0 && read_domains != write_domain)
 -		return -EINVAL;
 -
 -	obj = i915_gem_object_lookup(file, args->handle);
 -	if (!obj)
 -		return -ENOENT;
++	struct drm_i915_private *dev_priv = to_i915(obj->base.dev);
++	const unsigned long page_count = obj->base.size / PAGE_SIZE;
++	unsigned long i;
++	struct address_space *mapping;
++	struct sg_table *st;
++	struct scatterlist *sg;
++	struct sgt_iter sgt_iter;
++	struct page *page;
++	unsigned long last_pfn = 0;	/* suppress gcc warning */
++	unsigned int max_segment = i915_sg_segment_size();
++	unsigned int sg_page_sizes;
++	struct pagevec pvec;
++	gfp_t noreclaim;
++	int ret;
+ 
 -	/* Try to flush the object off the GPU without holding the lock.
 -	 * We will repeat the flush holding the lock in the normal manner
 -	 * to catch cases where we are gazumped.
++	/*
++	 * Assert that the object is not currently in any GPU domain. As it
++	 * wasn't in the GTT, there shouldn't be any way it could have been in
++	 * a GPU cache
+ 	 */
 -	err = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE |
 -				   I915_WAIT_PRIORITY |
 -				   (write_domain ? I915_WAIT_ALL : 0),
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   to_rps_client(file));
 -	if (err)
 -		goto out;
++	GEM_BUG_ON(obj->read_domains & I915_GEM_GPU_DOMAINS);
++	GEM_BUG_ON(obj->write_domain & I915_GEM_GPU_DOMAINS);
+ 
+ 	/*
 -	 * Proxy objects do not control access to the backing storage, ergo
 -	 * they cannot be used as a means to manipulate the cache domain
 -	 * tracking for that backing storage. The proxy object is always
 -	 * considered to be outside of any cache domain.
++	 * If there's no chance of allocating enough pages for the whole
++	 * object, bail early.
+ 	 */
 -	if (i915_gem_object_is_proxy(obj)) {
 -		err = -ENXIO;
 -		goto out;
++	if (page_count > totalram_pages())
++		return -ENOMEM;
++
++	st = kmalloc(sizeof(*st), GFP_KERNEL);
++	if (st == NULL)
++		return -ENOMEM;
++
++rebuild_st:
++	if (sg_alloc_table(st, page_count, GFP_KERNEL)) {
++		kfree(st);
++		return -ENOMEM;
+ 	}
+ 
+ 	/*
 -	 * Flush and acquire obj->pages so that we are coherent through
 -	 * direct access in memory with previous cached writes through
 -	 * shmemfs and that our cache domain tracking remains valid.
 -	 * For example, if the obj->filp was moved to swap without us
 -	 * being notified and releasing the pages, we would mistakenly
 -	 * continue to assume that the obj remained out of the CPU cached
 -	 * domain.
++	 * Get the list of pages out of our struct file.  They'll be pinned
++	 * at this point until we release them.
++	 *
++	 * Fail silently without starting the shrinker
+ 	 */
 -	err = i915_gem_object_pin_pages(obj);
 -	if (err)
 -		goto out;
 -
 -	err = i915_mutex_lock_interruptible(dev);
 -	if (err)
 -		goto out_unpin;
++	mapping = obj->base.filp->f_mapping;
++	mapping_set_unevictable(mapping);
++	noreclaim = mapping_gfp_constraint(mapping, ~__GFP_RECLAIM);
++	noreclaim |= __GFP_NORETRY | __GFP_NOWARN;
+ 
 -	if (read_domains & I915_GEM_DOMAIN_WC)
 -		err = i915_gem_object_set_to_wc_domain(obj, write_domain);
 -	else if (read_domains & I915_GEM_DOMAIN_GTT)
 -		err = i915_gem_object_set_to_gtt_domain(obj, write_domain);
 -	else
 -		err = i915_gem_object_set_to_cpu_domain(obj, write_domain);
++	sg = st->sgl;
++	st->nents = 0;
++	sg_page_sizes = 0;
++	for (i = 0; i < page_count; i++) {
++		const unsigned int shrink[] = {
++			I915_SHRINK_BOUND | I915_SHRINK_UNBOUND | I915_SHRINK_PURGEABLE,
++			0,
++		}, *s = shrink;
++		gfp_t gfp = noreclaim;
+ 
 -	/* And bump the LRU for this access */
 -	i915_gem_object_bump_inactive_ggtt(obj);
++		do {
++			cond_resched();
++			page = shmem_read_mapping_page_gfp(mapping, i, gfp);
++			if (likely(!IS_ERR(page)))
++				break;
+ 
 -	mutex_unlock(&dev->struct_mutex);
++			if (!*s) {
++				ret = PTR_ERR(page);
++				goto err_sg;
++			}
+ 
 -	if (write_domain != 0)
 -		intel_fb_obj_invalidate(obj,
 -					fb_write_origin(obj, write_domain));
++			i915_gem_shrink(dev_priv, 2 * page_count, NULL, *s++);
+ 
 -out_unpin:
 -	i915_gem_object_unpin_pages(obj);
 -out:
 -	i915_gem_object_put(obj);
 -	return err;
 -}
++			/*
++			 * We've tried hard to allocate the memory by reaping
++			 * our own buffer, now let the real VM do its job and
++			 * go down in flames if truly OOM.
++			 *
++			 * However, since graphics tend to be disposable,
++			 * defer the oom here by reporting the ENOMEM back
++			 * to userspace.
++			 */
++			if (!*s) {
++				/* reclaim and warn, but no oom */
++				gfp = mapping_gfp_mask(mapping);
+ 
 -/**
 - * Called when user space has done writes to this buffer
 - * @dev: drm device
 - * @data: ioctl data blob
 - * @file: drm file
 - */
 -int
 -i915_gem_sw_finish_ioctl(struct drm_device *dev, void *data,
 -			 struct drm_file *file)
 -{
 -	struct drm_i915_gem_sw_finish *args = data;
 -	struct drm_i915_gem_object *obj;
++				/*
++				 * Our bo are always dirty and so we require
++				 * kswapd to reclaim our pages (direct reclaim
++				 * does not effectively begin pageout of our
++				 * buffers on its own). However, direct reclaim
++				 * only waits for kswapd when under allocation
++				 * congestion. So as a result __GFP_RECLAIM is
++				 * unreliable and fails to actually reclaim our
++				 * dirty pages -- unless you try over and over
++				 * again with !__GFP_NORETRY. However, we still
++				 * want to fail this allocation rather than
++				 * trigger the out-of-memory killer and for
++				 * this we want __GFP_RETRY_MAYFAIL.
++				 */
++				gfp |= __GFP_RETRY_MAYFAIL;
++			}
++		} while (1);
+ 
 -	obj = i915_gem_object_lookup(file, args->handle);
 -	if (!obj)
 -		return -ENOENT;
++		if (!i ||
++		    sg->length >= max_segment ||
++		    page_to_pfn(page) != last_pfn + 1) {
++			if (i) {
++				sg_page_sizes |= sg->length;
++				sg = sg_next(sg);
++			}
++			st->nents++;
++			sg_set_page(sg, page, PAGE_SIZE, 0);
++		} else {
++			sg->length += PAGE_SIZE;
++		}
++		last_pfn = page_to_pfn(page);
+ 
 -	/*
 -	 * Proxy objects are barred from CPU access, so there is no
 -	 * need to ban sw_finish as it is a nop.
 -	 */
++		/* Check that the i965g/gm workaround works. */
++		WARN_ON((gfp & __GFP_DMA32) && (last_pfn >= 0x00100000UL));
++	}
++	if (sg) { /* loop terminated early; short sg table */
++		sg_page_sizes |= sg->length;
++		sg_mark_end(sg);
++	}
+ 
 -	/* Pinned buffers may be scanout, so flush the cache */
 -	i915_gem_object_flush_if_display(obj);
 -	i915_gem_object_put(obj);
++	/* Trim unused sg entries to avoid wasting memory. */
++	i915_sg_trim(st);
++
++	ret = i915_gem_gtt_prepare_pages(obj, st);
++	if (ret) {
++		/*
++		 * DMA remapping failed? One possible cause is that
++		 * it could not reserve enough large entries, asking
++		 * for PAGE_SIZE chunks instead may be helpful.
++		 */
++		if (max_segment > PAGE_SIZE) {
++			for_each_sgt_page(page, sgt_iter, st)
++				put_page(page);
++			sg_free_table(st);
++
++			max_segment = PAGE_SIZE;
++			goto rebuild_st;
++		} else {
++			dev_warn(&dev_priv->drm.pdev->dev,
++				 "Failed to DMA remap %lu pages\n",
++				 page_count);
++			goto err_pages;
++		}
++	}
++
++	if (i915_gem_object_needs_bit17_swizzle(obj))
++		i915_gem_object_do_bit_17_swizzle(obj, st);
++
++	__i915_gem_object_set_pages(obj, st, sg_page_sizes);
+ 
+ 	return 0;
++
++err_sg:
++	sg_mark_end(sg);
++err_pages:
++	mapping_clear_unevictable(mapping);
++	pagevec_init(&pvec);
++	for_each_sgt_page(page, sgt_iter, st) {
++		if (!pagevec_add(&pvec, page))
++			check_release_pagevec(&pvec);
++	}
++	if (pagevec_count(&pvec))
++		check_release_pagevec(&pvec);
++	sg_free_table(st);
++	kfree(st);
++
++	/*
++	 * shmemfs first checks if there is enough memory to allocate the page
++	 * and reports ENOSPC should there be insufficient, along with the usual
++	 * ENOMEM for a genuine allocation failure.
++	 *
++	 * We use ENOSPC in our driver to mean that we have run out of aperture
++	 * space and so want to translate the error from shmemfs back to our
++	 * usual understanding of ENOMEM.
++	 */
++	if (ret == -ENOSPC)
++		ret = -ENOMEM;
++
++	return ret;
+ }
+ 
 -/**
 - * i915_gem_mmap_ioctl - Maps the contents of an object, returning the address
 - *			 it is mapped to.
 - * @dev: drm device
 - * @data: ioctl data blob
 - * @file: drm file
 - *
 - * While the mapping holds a reference on the contents of the object, it doesn't
 - * imply a ref on the object itself.
 - *
 - * IMPORTANT:
 - *
 - * DRM driver writers who look a this function as an example for how to do GEM
 - * mmap support, please don't implement mmap support like here. The modern way
 - * to implement DRM mmap support is with an mmap offset ioctl (like
 - * i915_gem_mmap_gtt) and then using the mmap syscall on the DRM fd directly.
 - * That way debug tooling like valgrind will understand what's going on, hiding
 - * the mmap call in a driver private ioctl will break that. The i915 driver only
 - * does cpu mmaps this way because we didn't know better.
 - */
 -int
 -i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
 -		    struct drm_file *file)
++void __i915_gem_object_set_pages(struct drm_i915_gem_object *obj,
++				 struct sg_table *pages,
++				 unsigned int sg_page_sizes)
+ {
 -	struct drm_i915_gem_mmap *args = data;
 -	struct drm_i915_gem_object *obj;
 -	unsigned long addr;
++	struct drm_i915_private *i915 = to_i915(obj->base.dev);
++	unsigned long supported = INTEL_INFO(i915)->page_sizes;
++	int i;
+ 
 -	if (args->flags & ~(I915_MMAP_WC))
 -		return -EINVAL;
++	lockdep_assert_held(&obj->mm.lock);
+ 
 -	if (args->flags & I915_MMAP_WC && !boot_cpu_has(X86_FEATURE_PAT))
 -		return -ENODEV;
++	obj->mm.get_page.sg_pos = pages->sgl;
++	obj->mm.get_page.sg_idx = 0;
+ 
 -	obj = i915_gem_object_lookup(file, args->handle);
 -	if (!obj)
 -		return -ENOENT;
++	obj->mm.pages = pages;
+ 
 -	/* prime objects have no backing filp to GEM mmap
 -	 * pages from.
 -	 */
 -	if (!obj->base.filp) {
 -		i915_gem_object_put(obj);
 -		return -ENXIO;
++	if (i915_gem_object_is_tiled(obj) &&
++	    i915->quirks & QUIRK_PIN_SWIZZLED_PAGES) {
++		GEM_BUG_ON(obj->mm.quirked);
++		__i915_gem_object_pin_pages(obj);
++		obj->mm.quirked = true;
+ 	}
+ 
 -	addr = vm_mmap(obj->base.filp, 0, args->size,
 -		       PROT_READ | PROT_WRITE, MAP_SHARED,
 -		       args->offset);
 -	if (args->flags & I915_MMAP_WC) {
 -		struct mm_struct *mm = current->mm;
 -		struct vm_area_struct *vma;
 -
 -		if (down_write_killable(&mm->mmap_sem)) {
 -			i915_gem_object_put(obj);
 -			return -EINTR;
 -		}
 -		vma = find_vma(mm, addr);
 -		if (vma)
 -			vma->vm_page_prot =
 -				pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
 -		else
 -			addr = -ENOMEM;
 -		up_write(&mm->mmap_sem);
++	GEM_BUG_ON(!sg_page_sizes);
++	obj->mm.page_sizes.phys = sg_page_sizes;
+ 
 -		/* This may race, but that's ok, it only gets set */
 -		WRITE_ONCE(obj->frontbuffer_ggtt_origin, ORIGIN_CPU);
++	/*
++	 * Calculate the supported page-sizes which fit into the given
++	 * sg_page_sizes. This will give us the page-sizes which we may be able
++	 * to use opportunistically when later inserting into the GTT. For
++	 * example if phys=2G, then in theory we should be able to use 1G, 2M,
++	 * 64K or 4K pages, although in practice this will depend on a number of
++	 * other factors.
++	 */
++	obj->mm.page_sizes.sg = 0;
++	for_each_set_bit(i, &supported, ilog2(I915_GTT_MAX_PAGE_SIZE) + 1) {
++		if (obj->mm.page_sizes.phys & ~0u << i)
++			obj->mm.page_sizes.sg |= BIT(i);
+ 	}
 -	i915_gem_object_put(obj);
 -	if (IS_ERR((void *)addr))
 -		return addr;
 -
 -	args->addr_ptr = (uint64_t) addr;
++	GEM_BUG_ON(!HAS_PAGE_SIZES(i915, obj->mm.page_sizes.sg));
+ 
 -	return 0;
++	spin_lock(&i915->mm.obj_lock);
++	list_add(&obj->mm.link, &i915->mm.unbound_list);
++	spin_unlock(&i915->mm.obj_lock);
+ }
+ 
 -static unsigned int tile_row_pages(const struct drm_i915_gem_object *obj)
++static int ____i915_gem_object_get_pages(struct drm_i915_gem_object *obj)
+ {
 -	return i915_gem_object_get_tile_row_size(obj) >> PAGE_SHIFT;
++	int err;
++
++	if (unlikely(obj->mm.madv != I915_MADV_WILLNEED)) {
++		DRM_DEBUG("Attempting to obtain a purgeable object\n");
++		return -EFAULT;
++	}
++
++	err = obj->ops->get_pages(obj);
++	GEM_BUG_ON(!err && !i915_gem_object_has_pages(obj));
++
++	return err;
+ }
+ 
 -/**
 - * i915_gem_mmap_gtt_version - report the current feature set for GTT mmaps
 - *
 - * A history of the GTT mmap interface:
 - *
 - * 0 - Everything had to fit into the GTT. Both parties of a memcpy had to
 - *     aligned and suitable for fencing, and still fit into the available
 - *     mappable space left by the pinned display objects. A classic problem
 - *     we called the page-fault-of-doom where we would ping-pong between
 - *     two objects that could not fit inside the GTT and so the memcpy
 - *     would page one object in at the expense of the other between every
 - *     single byte.
 - *
 - * 1 - Objects can be any size, and have any compatible fencing (X Y, or none
 - *     as set via i915_gem_set_tiling() [DRM_I915_GEM_SET_TILING]). If the
 - *     object is too large for the available space (or simply too large
 - *     for the mappable aperture!), a view is created instead and faulted
 - *     into userspace. (This view is aligned and sized appropriately for
 - *     fenced access.)
 - *
 - * 2 - Recognise WC as a separate cache domain so that we can flush the
 - *     delayed writes via GTT before performing direct access via WC.
 - *
 - * Restrictions:
 - *
 - *  * snoopable objects cannot be accessed via the GTT. It can cause machine
 - *    hangs on some architectures, corruption on others. An attempt to service
 - *    a GTT page fault from a snoopable object will generate a SIGBUS.
 - *
 - *  * the object must be able to fit into RAM (physical memory, though no
 - *    limited to the mappable aperture).
 - *
 - *
 - * Caveats:
 - *
 - *  * a new GTT page fault will synchronize rendering from the GPU and flush
 - *    all data to system memory. Subsequent access will not be synchronized.
 - *
 - *  * all mappings are revoked on runtime device suspend.
 - *
 - *  * there are only 8, 16 or 32 fence registers to share between all users
 - *    (older machines require fence register for display and blitter access
 - *    as well). Contention of the fence registers will cause the previous users
 - *    to be unmapped and any new access will generate new page faults.
 - *
 - *  * running out of memory while servicing a fault may generate a SIGBUS,
 - *    rather than the expected SIGSEGV.
++/* Ensure that the associated pages are gathered from the backing storage
++ * and pinned into our object. i915_gem_object_pin_pages() may be called
++ * multiple times before they are released by a single call to
++ * i915_gem_object_unpin_pages() - once the pages are no longer referenced
++ * either as a result of memory pressure (reaping pages under the shrinker)
++ * or as the object is itself released.
+  */
 -int i915_gem_mmap_gtt_version(void)
++int __i915_gem_object_get_pages(struct drm_i915_gem_object *obj)
+ {
 -	return 2;
 -}
++	int err;
+ 
 -static inline struct i915_ggtt_view
 -compute_partial_view(const struct drm_i915_gem_object *obj,
 -		     pgoff_t page_offset,
 -		     unsigned int chunk)
 -{
 -	struct i915_ggtt_view view;
++	err = mutex_lock_interruptible(&obj->mm.lock);
++	if (err)
++		return err;
+ 
 -	if (i915_gem_object_is_tiled(obj))
 -		chunk = roundup(chunk, tile_row_pages(obj));
++	if (unlikely(!i915_gem_object_has_pages(obj))) {
++		GEM_BUG_ON(i915_gem_object_has_pinned_pages(obj));
+ 
 -	view.type = I915_GGTT_VIEW_PARTIAL;
 -	view.partial.offset = rounddown(page_offset, chunk);
 -	view.partial.size =
 -		min_t(unsigned int, chunk,
 -		      (obj->base.size >> PAGE_SHIFT) - view.partial.offset);
++		err = ____i915_gem_object_get_pages(obj);
++		if (err)
++			goto unlock;
+ 
 -	/* If the partial covers the entire object, just create a normal VMA. */
 -	if (chunk >= obj->base.size >> PAGE_SHIFT)
 -		view.type = I915_GGTT_VIEW_NORMAL;
++		smp_mb__before_atomic();
++	}
++	atomic_inc(&obj->mm.pages_pin_count);
+ 
 -	return view;
++unlock:
++	mutex_unlock(&obj->mm.lock);
++	return err;
+ }
+ 
 -/**
 - * i915_gem_fault - fault a page into the GTT
 - * @vmf: fault info
 - *
 - * The fault handler is set up by drm_gem_mmap() when a object is GTT mapped
 - * from userspace.  The fault handler takes care of binding the object to
 - * the GTT (if needed), allocating and programming a fence register (again,
 - * only if needed based on whether the old reg is still valid or the object
 - * is tiled) and inserting a new PTE into the faulting process.
 - *
 - * Note that the faulting process may involve evicting existing objects
 - * from the GTT and/or fence registers to make room.  So performance may
 - * suffer if the GTT working set is large or there are few fence registers
 - * left.
 - *
 - * The current feature set supported by i915_gem_fault() and thus GTT mmaps
 - * is exposed via I915_PARAM_MMAP_GTT_VERSION (see i915_gem_mmap_gtt_version).
 - */
 -vm_fault_t i915_gem_fault(struct vm_fault *vmf)
++/* The 'mapping' part of i915_gem_object_pin_map() below */
++static void *i915_gem_object_map(const struct drm_i915_gem_object *obj,
++				 enum i915_map_type type)
+ {
 -#define MIN_CHUNK_PAGES (SZ_1M >> PAGE_SHIFT)
 -	struct vm_area_struct *area = vmf->vma;
 -	struct drm_i915_gem_object *obj = to_intel_bo(area->vm_private_data);
 -	struct drm_device *dev = obj->base.dev;
 -	struct drm_i915_private *dev_priv = to_i915(dev);
 -	struct i915_ggtt *ggtt = &dev_priv->ggtt;
 -	bool write = area->vm_flags & VM_WRITE;
 -	struct i915_vma *vma;
 -	pgoff_t page_offset;
 -	int ret;
++	unsigned long n_pages = obj->base.size >> PAGE_SHIFT;
++	struct sg_table *sgt = obj->mm.pages;
++	struct sgt_iter sgt_iter;
++	struct page *page;
++	struct page *stack_pages[32];
++	struct page **pages = stack_pages;
++	unsigned long i = 0;
++	pgprot_t pgprot;
++	void *addr;
+ 
 -	/* Sanity check that we allow writing into this object */
 -	if (i915_gem_object_is_readonly(obj) && write)
 -		return VM_FAULT_SIGBUS;
++	/* A single page can always be kmapped */
++	if (n_pages == 1 && type == I915_MAP_WB)
++		return kmap(sg_page(sgt->sgl));
+ 
 -	/* We don't use vmf->pgoff since that has the fake offset */
 -	page_offset = (vmf->address - area->vm_start) >> PAGE_SHIFT;
++	if (n_pages > ARRAY_SIZE(stack_pages)) {
++		/* Too big for stack -- allocate temporary array instead */
++		pages = kvmalloc_array(n_pages, sizeof(*pages), GFP_KERNEL);
++		if (!pages)
++			return NULL;
++	}
+ 
 -	trace_i915_gem_object_fault(obj, page_offset, true, write);
++	for_each_sgt_page(page, sgt_iter, sgt)
++		pages[i++] = page;
+ 
 -	/* Try to flush the object off the GPU first without holding the lock.
 -	 * Upon acquiring the lock, we will perform our sanity checks and then
 -	 * repeat the flush holding the lock in the normal manner to catch cases
 -	 * where we are gazumped.
 -	 */
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE,
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   NULL);
 -	if (ret)
 -		goto err;
++	/* Check that we have the expected number of pages */
++	GEM_BUG_ON(i != n_pages);
+ 
 -	ret = i915_gem_object_pin_pages(obj);
 -	if (ret)
 -		goto err;
++	switch (type) {
++	default:
++		MISSING_CASE(type);
++		/* fallthrough to use PAGE_KERNEL anyway */
++	case I915_MAP_WB:
++		pgprot = PAGE_KERNEL;
++		break;
++	case I915_MAP_WC:
++		pgprot = pgprot_writecombine(PAGE_KERNEL_IO);
++		break;
++	}
++	addr = vmap(pages, n_pages, 0, pgprot);
+ 
 -	intel_runtime_pm_get(dev_priv);
++	if (pages != stack_pages)
++		kvfree(pages);
+ 
 -	ret = i915_mutex_lock_interruptible(dev);
 -	if (ret)
 -		goto err_rpm;
++	return addr;
++}
+ 
 -	/* Access to snoopable pages through the GTT is incoherent. */
 -	if (obj->cache_level != I915_CACHE_NONE && !HAS_LLC(dev_priv)) {
 -		ret = -EFAULT;
 -		goto err_unlock;
 -	}
++/* get, pin, and map the pages of the object into kernel space */
++void *i915_gem_object_pin_map(struct drm_i915_gem_object *obj,
++			      enum i915_map_type type)
++{
++	enum i915_map_type has_type;
++	bool pinned;
++	void *ptr;
++	int ret;
+ 
++	if (unlikely(!i915_gem_object_has_struct_page(obj)))
++		return ERR_PTR(-ENXIO);
++
++	ret = mutex_lock_interruptible(&obj->mm.lock);
++	if (ret)
++		return ERR_PTR(ret);
+ 
 -	/* Now pin it into the GTT as needed */
 -	vma = i915_gem_object_ggtt_pin(obj, NULL, 0, 0,
 -				       PIN_MAPPABLE |
 -				       PIN_NONBLOCK |
 -				       PIN_NONFAULT);
 -	if (IS_ERR(vma)) {
 -		/* Use a partial view if it is bigger than available space */
 -		struct i915_ggtt_view view =
 -			compute_partial_view(obj, page_offset, MIN_CHUNK_PAGES);
 -		unsigned int flags;
++	pinned = !(type & I915_MAP_OVERRIDE);
++	type &= ~I915_MAP_OVERRIDE;
+ 
 -		flags = PIN_MAPPABLE;
 -		if (view.type == I915_GGTT_VIEW_NORMAL)
 -			flags |= PIN_NONBLOCK; /* avoid warnings for pinned */
++	if (!atomic_inc_not_zero(&obj->mm.pages_pin_count)) {
++		if (unlikely(!i915_gem_object_has_pages(obj))) {
++			GEM_BUG_ON(i915_gem_object_has_pinned_pages(obj));
+ 
 -		/*
 -		 * Userspace is now writing through an untracked VMA, abandon
 -		 * all hope that the hardware is able to track future writes.
 -		 */
 -		obj->frontbuffer_ggtt_origin = ORIGIN_CPU;
++			ret = ____i915_gem_object_get_pages(obj);
++			if (ret)
++				goto err_unlock;
+ 
 -		vma = i915_gem_object_ggtt_pin(obj, &view, 0, 0, flags);
 -		if (IS_ERR(vma) && !view.type) {
 -			flags = PIN_MAPPABLE;
 -			view.type = I915_GGTT_VIEW_PARTIAL;
 -			vma = i915_gem_object_ggtt_pin(obj, &view, 0, 0, flags);
++			smp_mb__before_atomic();
+ 		}
++		atomic_inc(&obj->mm.pages_pin_count);
++		pinned = false;
+ 	}
 -	if (IS_ERR(vma)) {
 -		ret = PTR_ERR(vma);
 -		goto err_unlock;
 -	}
++	GEM_BUG_ON(!i915_gem_object_has_pages(obj));
+ 
 -	ret = i915_gem_object_set_to_gtt_domain(obj, write);
 -	if (ret)
 -		goto err_unpin;
++	ptr = page_unpack_bits(obj->mm.mapping, &has_type);
++	if (ptr && has_type != type) {
++		if (pinned) {
++			ret = -EBUSY;
++			goto err_unpin;
++		}
+ 
 -	ret = i915_vma_pin_fence(vma);
 -	if (ret)
 -		goto err_unpin;
 -
 -	/* Finally, remap it using the new GTT offset */
 -	ret = remap_io_mapping(area,
 -			       area->vm_start + (vma->ggtt_view.partial.offset << PAGE_SHIFT),
 -			       (ggtt->gmadr.start + vma->node.start) >> PAGE_SHIFT,
 -			       min_t(u64, vma->size, area->vm_end - area->vm_start),
 -			       &ggtt->iomap);
 -	if (ret)
 -		goto err_fence;
++		if (is_vmalloc_addr(ptr))
++			vunmap(ptr);
++		else
++			kunmap(kmap_to_page(ptr));
++
++		ptr = obj->mm.mapping = NULL;
++	}
++
++	if (!ptr) {
++		ptr = i915_gem_object_map(obj, type);
++		if (!ptr) {
++			ret = -ENOMEM;
++			goto err_unpin;
++		}
+ 
 -	/* Mark as being mmapped into userspace for later revocation */
 -	assert_rpm_wakelock_held(dev_priv);
 -	if (!i915_vma_set_userfault(vma) && !obj->userfault_count++)
 -		list_add(&obj->userfault_link, &dev_priv->mm.userfault_list);
 -	GEM_BUG_ON(!obj->userfault_count);
++		obj->mm.mapping = page_pack_bits(ptr, type);
++	}
+ 
 -	i915_vma_set_ggtt_write(vma);
++out_unlock:
++	mutex_unlock(&obj->mm.lock);
++	return ptr;
+ 
 -err_fence:
 -	i915_vma_unpin_fence(vma);
+ err_unpin:
 -	__i915_vma_unpin(vma);
++	atomic_dec(&obj->mm.pages_pin_count);
+ err_unlock:
 -	mutex_unlock(&dev->struct_mutex);
 -err_rpm:
 -	intel_runtime_pm_put(dev_priv);
 -	i915_gem_object_unpin_pages(obj);
 -err:
 -	switch (ret) {
 -	case -EIO:
 -		/*
 -		 * We eat errors when the gpu is terminally wedged to avoid
 -		 * userspace unduly crashing (gl has no provisions for mmaps to
 -		 * fail). But any other -EIO isn't ours (e.g. swap in failure)
 -		 * and so needs to be reported.
 -		 */
 -		if (!i915_terminally_wedged(&dev_priv->gpu_error))
 -			return VM_FAULT_SIGBUS;
 -		/* else: fall through */
 -	case -EAGAIN:
 -		/*
 -		 * EAGAIN means the gpu is hung and we'll wait for the error
 -		 * handler to reset everything when re-faulting in
 -		 * i915_mutex_lock_interruptible.
 -		 */
 -	case 0:
 -	case -ERESTARTSYS:
 -	case -EINTR:
 -	case -EBUSY:
 -		/*
 -		 * EBUSY is ok: this just means that another thread
 -		 * already did the job.
 -		 */
 -		return VM_FAULT_NOPAGE;
 -	case -ENOMEM:
 -		return VM_FAULT_OOM;
 -	case -ENOSPC:
 -	case -EFAULT:
 -		return VM_FAULT_SIGBUS;
 -	default:
 -		WARN_ONCE(ret, "unhandled error in i915_gem_fault: %i\n", ret);
 -		return VM_FAULT_SIGBUS;
 -	}
++	ptr = ERR_PTR(ret);
++	goto out_unlock;
+ }
+ 
 -static void __i915_gem_object_release_mmap(struct drm_i915_gem_object *obj)
++static int
++i915_gem_object_pwrite_gtt(struct drm_i915_gem_object *obj,
++			   const struct drm_i915_gem_pwrite *arg)
+ {
 -	struct i915_vma *vma;
++	struct address_space *mapping = obj->base.filp->f_mapping;
++	char __user *user_data = u64_to_user_ptr(arg->data_ptr);
++	u64 remain, offset;
++	unsigned int pg;
+ 
 -	GEM_BUG_ON(!obj->userfault_count);
++	/* Before we instantiate/pin the backing store for our use, we
++	 * can prepopulate the shmemfs filp efficiently using a write into
++	 * the pagecache. We avoid the penalty of instantiating all the
++	 * pages, important if the user is just writing to a few and never
++	 * uses the object on the GPU, and using a direct write into shmemfs
++	 * allows it to avoid the cost of retrieving a page (either swapin
++	 * or clearing-before-use) before it is overwritten.
++	 */
++	if (i915_gem_object_has_pages(obj))
++		return -ENODEV;
+ 
 -	obj->userfault_count = 0;
 -	list_del(&obj->userfault_link);
 -	drm_vma_node_unmap(&obj->base.vma_node,
 -			   obj->base.dev->anon_inode->i_mapping);
++	if (obj->mm.madv != I915_MADV_WILLNEED)
++		return -EFAULT;
+ 
 -	for_each_ggtt_vma(vma, obj)
 -		i915_vma_unset_userfault(vma);
 -}
++	/* Before the pages are instantiated the object is treated as being
++	 * in the CPU domain. The pages will be clflushed as required before
++	 * use, and we can freely write into the pages directly. If userspace
++	 * races pwrite with any other operation; corruption will ensue -
++	 * that is userspace's prerogative!
++	 */
+ 
 -/**
 - * i915_gem_release_mmap - remove physical page mappings
 - * @obj: obj in question
 - *
 - * Preserve the reservation of the mmapping with the DRM core code, but
 - * relinquish ownership of the pages back to the system.
 - *
 - * It is vital that we remove the page mapping if we have mapped a tiled
 - * object through the GTT and then lose the fence register due to
 - * resource pressure. Similarly if the object has been moved out of the
 - * aperture, than pages mapped into userspace must be revoked. Removing the
 - * mapping will then trigger a page fault on the next user access, allowing
 - * fixup by i915_gem_fault().
 - */
 -void
 -i915_gem_release_mmap(struct drm_i915_gem_object *obj)
 -{
 -	struct drm_i915_private *i915 = to_i915(obj->base.dev);
++	remain = arg->size;
++	offset = arg->offset;
++	pg = offset_in_page(offset);
+ 
 -	/* Serialisation between user GTT access and our code depends upon
 -	 * revoking the CPU's PTE whilst the mutex is held. The next user
 -	 * pagefault then has to wait until we release the mutex.
 -	 *
 -	 * Note that RPM complicates somewhat by adding an additional
 -	 * requirement that operations to the GGTT be made holding the RPM
 -	 * wakeref.
 -	 */
 -	lockdep_assert_held(&i915->drm.struct_mutex);
 -	intel_runtime_pm_get(i915);
++	do {
++		unsigned int len, unwritten;
++		struct page *page;
++		void *data, *vaddr;
++		int err;
+ 
 -	if (!obj->userfault_count)
 -		goto out;
++		len = PAGE_SIZE - pg;
++		if (len > remain)
++			len = remain;
+ 
 -	__i915_gem_object_release_mmap(obj);
++		err = pagecache_write_begin(obj->base.filp, mapping,
++					    offset, len, 0,
++					    &page, &data);
++		if (err < 0)
++			return err;
+ 
 -	/* Ensure that the CPU's PTE are revoked and there are not outstanding
 -	 * memory transactions from userspace before we return. The TLB
 -	 * flushing implied above by changing the PTE above *should* be
 -	 * sufficient, an extra barrier here just provides us with a bit
 -	 * of paranoid documentation about our requirement to serialise
 -	 * memory writes before touching registers / GSM.
 -	 */
 -	wmb();
++		vaddr = kmap(page);
++		unwritten = copy_from_user(vaddr + pg, user_data, len);
++		kunmap(page);
+ 
 -out:
 -	intel_runtime_pm_put(i915);
 -}
++		err = pagecache_write_end(obj->base.filp, mapping,
++					  offset, len, len - unwritten,
++					  page, data);
++		if (err < 0)
++			return err;
+ 
 -void i915_gem_runtime_suspend(struct drm_i915_private *dev_priv)
 -{
 -	struct drm_i915_gem_object *obj, *on;
 -	int i;
++		if (unwritten)
++			return -EFAULT;
+ 
 -	/*
 -	 * Only called during RPM suspend. All users of the userfault_list
 -	 * must be holding an RPM wakeref to ensure that this can not
 -	 * run concurrently with themselves (and use the struct_mutex for
 -	 * protection between themselves).
 -	 */
++		remain -= len;
++		user_data += len;
++		offset += len;
++		pg = 0;
++	} while (remain);
+ 
 -	list_for_each_entry_safe(obj, on,
 -				 &dev_priv->mm.userfault_list, userfault_link)
 -		__i915_gem_object_release_mmap(obj);
++	return 0;
++}
+ 
 -	/* The fence will be lost when the device powers down. If any were
 -	 * in use by hardware (i.e. they are pinned), we should not be powering
 -	 * down! All other fences will be reacquired by the user upon waking.
 -	 */
 -	for (i = 0; i < dev_priv->num_fence_regs; i++) {
 -		struct drm_i915_fence_reg *reg = &dev_priv->fence_regs[i];
++static void i915_gem_client_mark_guilty(struct drm_i915_file_private *file_priv,
++					const struct i915_gem_context *ctx)
++{
++	unsigned int score;
++	unsigned long prev_hang;
+ 
 -		/* Ideally we want to assert that the fence register is not
 -		 * live at this point (i.e. that no piece of code will be
 -		 * trying to write through fence + GTT, as that both violates
 -		 * our tracking of activity and associated locking/barriers,
 -		 * but also is illegal given that the hw is powered down).
 -		 *
 -		 * Previously we used reg->pin_count as a "liveness" indicator.
 -		 * That is not sufficient, and we need a more fine-grained
 -		 * tool if we want to have a sanity check here.
 -		 */
++	if (i915_gem_context_is_banned(ctx))
++		score = I915_CLIENT_SCORE_CONTEXT_BAN;
++	else
++		score = 0;
+ 
 -		if (!reg->vma)
 -			continue;
++	prev_hang = xchg(&file_priv->hang_timestamp, jiffies);
++	if (time_before(jiffies, prev_hang + I915_CLIENT_FAST_HANG_JIFFIES))
++		score += I915_CLIENT_SCORE_HANG_FAST;
+ 
 -		GEM_BUG_ON(i915_vma_has_userfault(reg->vma));
 -		reg->dirty = true;
++	if (score) {
++		atomic_add(score, &file_priv->ban_score);
++
++		DRM_DEBUG_DRIVER("client %s: gained %u ban score, now %u\n",
++				 ctx->name, score,
++				 atomic_read(&file_priv->ban_score));
+ 	}
+ }
+ 
 -static int i915_gem_object_create_mmap_offset(struct drm_i915_gem_object *obj)
++static void i915_gem_context_mark_guilty(struct i915_gem_context *ctx)
+ {
 -	struct drm_i915_private *dev_priv = to_i915(obj->base.dev);
 -	int err;
++	unsigned int score;
++	bool banned, bannable;
+ 
 -	err = drm_gem_create_mmap_offset(&obj->base);
 -	if (likely(!err))
 -		return 0;
++	atomic_inc(&ctx->guilty_count);
+ 
 -	/* Attempt to reap some mmap space from dead objects */
 -	do {
 -		err = i915_gem_wait_for_idle(dev_priv,
 -					     I915_WAIT_INTERRUPTIBLE,
 -					     MAX_SCHEDULE_TIMEOUT);
 -		if (err)
 -			break;
++	bannable = i915_gem_context_is_bannable(ctx);
++	score = atomic_add_return(CONTEXT_SCORE_GUILTY, &ctx->ban_score);
++	banned = score >= CONTEXT_SCORE_BAN_THRESHOLD;
+ 
 -		i915_gem_drain_freed_objects(dev_priv);
 -		err = drm_gem_create_mmap_offset(&obj->base);
 -		if (!err)
 -			break;
++	/* Cool contexts don't accumulate client ban score */
++	if (!bannable)
++		return;
+ 
 -	} while (flush_delayed_work(&dev_priv->gt.retire_work));
++	if (banned) {
++		DRM_DEBUG_DRIVER("context %s: guilty %d, score %u, banned\n",
++				 ctx->name, atomic_read(&ctx->guilty_count),
++				 score);
++		i915_gem_context_set_banned(ctx);
++	}
+ 
 -	return err;
++	if (!IS_ERR_OR_NULL(ctx->file_priv))
++		i915_gem_client_mark_guilty(ctx->file_priv, ctx);
+ }
+ 
 -static void i915_gem_object_free_mmap_offset(struct drm_i915_gem_object *obj)
++static void i915_gem_context_mark_innocent(struct i915_gem_context *ctx)
+ {
 -	drm_gem_free_mmap_offset(&obj->base);
++	atomic_inc(&ctx->active_count);
+ }
+ 
 -int
 -i915_gem_mmap_gtt(struct drm_file *file,
 -		  struct drm_device *dev,
 -		  uint32_t handle,
 -		  uint64_t *offset)
++struct i915_request *
++i915_gem_find_active_request(struct intel_engine_cs *engine)
+ {
 -	struct drm_i915_gem_object *obj;
 -	int ret;
++	struct i915_request *request, *active = NULL;
++	unsigned long flags;
+ 
 -	obj = i915_gem_object_lookup(file, handle);
 -	if (!obj)
 -		return -ENOENT;
++	/*
++	 * We are called by the error capture, reset and to dump engine
++	 * state at random points in time. In particular, note that neither is
++	 * crucially ordered with an interrupt. After a hang, the GPU is dead
++	 * and we assume that no more writes can happen (we waited long enough
++	 * for all writes that were in transaction to be flushed) - adding an
++	 * extra delay for a recent interrupt is pointless. Hence, we do
++	 * not need an engine->irq_seqno_barrier() before the seqno reads.
++	 * At all other times, we must assume the GPU is still running, but
++	 * we only care about the snapshot of this moment.
++	 */
++	spin_lock_irqsave(&engine->timeline.lock, flags);
++	list_for_each_entry(request, &engine->timeline.requests, link) {
++		if (__i915_request_completed(request, request->global_seqno))
++			continue;
+ 
 -	ret = i915_gem_object_create_mmap_offset(obj);
 -	if (ret == 0)
 -		*offset = drm_vma_node_offset_addr(&obj->base.vma_node);
++		active = request;
++		break;
++	}
++	spin_unlock_irqrestore(&engine->timeline.lock, flags);
+ 
 -	i915_gem_object_put(obj);
 -	return ret;
++	return active;
+ }
+ 
 -/**
 - * i915_gem_mmap_gtt_ioctl - prepare an object for GTT mmap'ing
 - * @dev: DRM device
 - * @data: GTT mapping ioctl data
 - * @file: GEM object info
 - *
 - * Simply returns the fake offset to userspace so it can mmap it.
 - * The mmap call will end up in drm_gem_mmap(), which will set things
 - * up so we can get faults in the handler above.
 - *
 - * The fault handler will take care of binding the object into the GTT
 - * (since it may have been evicted to make room for something), allocating
 - * a fence register, and mapping the appropriate aperture address into
 - * userspace.
++/*
++ * Ensure irq handler finishes, and not run again.
++ * Also return the active request so that we only search for it once.
+  */
 -int
 -i915_gem_mmap_gtt_ioctl(struct drm_device *dev, void *data,
 -			struct drm_file *file)
++struct i915_request *
++i915_gem_reset_prepare_engine(struct intel_engine_cs *engine)
+ {
 -	struct drm_i915_gem_mmap_gtt *args = data;
 -
 -	return i915_gem_mmap_gtt(file, dev, args->handle, &args->offset);
 -}
++	struct i915_request *request;
+ 
 -/* Immediately discard the backing storage */
 -static void
 -i915_gem_object_truncate(struct drm_i915_gem_object *obj)
 -{
 -	i915_gem_object_free_mmap_offset(obj);
++	/*
++	 * During the reset sequence, we must prevent the engine from
++	 * entering RC6. As the context state is undefined until we restart
++	 * the engine, if it does enter RC6 during the reset, the state
++	 * written to the powercontext is undefined and so we may lose
++	 * GPU state upon resume, i.e. fail to restart after a reset.
++	 */
++	intel_uncore_forcewake_get(engine->i915, FORCEWAKE_ALL);
+ 
 -	if (obj->base.filp == NULL)
 -		return;
++	request = engine->reset.prepare(engine);
++	if (request && request->fence.error == -EIO)
++		request = ERR_PTR(-EIO); /* Previous reset failed! */
+ 
 -	/* Our goal here is to return as much of the memory as
 -	 * is possible back to the system as we are called from OOM.
 -	 * To do this we must instruct the shmfs to drop all of its
 -	 * backing pages, *now*.
 -	 */
 -	shmem_truncate_range(file_inode(obj->base.filp), 0, (loff_t)-1);
 -	obj->mm.madv = __I915_MADV_PURGED;
 -	obj->mm.pages = ERR_PTR(-EFAULT);
++	return request;
+ }
+ 
 -/* Try to discard unwanted pages */
 -void __i915_gem_object_invalidate(struct drm_i915_gem_object *obj)
++int i915_gem_reset_prepare(struct drm_i915_private *dev_priv)
+ {
 -	struct address_space *mapping;
++	struct intel_engine_cs *engine;
++	struct i915_request *request;
++	enum intel_engine_id id;
++	int err = 0;
+ 
 -	lockdep_assert_held(&obj->mm.lock);
 -	GEM_BUG_ON(i915_gem_object_has_pages(obj));
++	for_each_engine(engine, dev_priv, id) {
++		request = i915_gem_reset_prepare_engine(engine);
++		if (IS_ERR(request)) {
++			err = PTR_ERR(request);
++			continue;
++		}
+ 
 -	switch (obj->mm.madv) {
 -	case I915_MADV_DONTNEED:
 -		i915_gem_object_truncate(obj);
 -	case __I915_MADV_PURGED:
 -		return;
++		engine->hangcheck.active_request = request;
+ 	}
+ 
 -	if (obj->base.filp == NULL)
 -		return;
 -
 -	mapping = obj->base.filp->f_mapping,
 -	invalidate_mapping_pages(mapping, 0, (loff_t)-1);
 -}
++	i915_gem_revoke_fences(dev_priv);
++	intel_uc_sanitize(dev_priv);
+ 
 -/*
 - * Move pages to appropriate lru and release the pagevec, decrementing the
 - * ref count of those pages.
 - */
 -static void check_release_pagevec(struct pagevec *pvec)
 -{
 -	check_move_unevictable_pages(pvec);
 -	__pagevec_release(pvec);
 -	cond_resched();
++	return err;
+ }
+ 
 -static void
 -i915_gem_object_put_pages_gtt(struct drm_i915_gem_object *obj,
 -			      struct sg_table *pages)
++static void engine_skip_context(struct i915_request *request)
+ {
 -	struct sgt_iter sgt_iter;
 -	struct pagevec pvec;
 -	struct page *page;
 -
 -	__i915_gem_object_release_shmem(obj, pages, true);
 -
 -	i915_gem_gtt_finish_pages(obj, pages);
 -
 -	if (i915_gem_object_needs_bit17_swizzle(obj))
 -		i915_gem_object_save_bit_17_swizzle(obj, pages);
++	struct intel_engine_cs *engine = request->engine;
++	struct i915_gem_context *hung_ctx = request->gem_context;
++	struct i915_timeline *timeline = request->timeline;
++	unsigned long flags;
+ 
 -	mapping_clear_unevictable(file_inode(obj->base.filp)->i_mapping);
++	GEM_BUG_ON(timeline == &engine->timeline);
+ 
 -	pagevec_init(&pvec);
 -	for_each_sgt_page(page, sgt_iter, pages) {
 -		if (obj->mm.dirty)
 -			set_page_dirty(page);
++	spin_lock_irqsave(&engine->timeline.lock, flags);
++	spin_lock(&timeline->lock);
+ 
 -		if (obj->mm.madv == I915_MADV_WILLNEED)
 -			mark_page_accessed(page);
++	list_for_each_entry_continue(request, &engine->timeline.requests, link)
++		if (request->gem_context == hung_ctx)
++			i915_request_skip(request, -EIO);
+ 
 -		if (!pagevec_add(&pvec, page))
 -			check_release_pagevec(&pvec);
 -	}
 -	if (pagevec_count(&pvec))
 -		check_release_pagevec(&pvec);
 -	obj->mm.dirty = false;
++	list_for_each_entry(request, &timeline->requests, link)
++		i915_request_skip(request, -EIO);
+ 
 -	sg_free_table(pages);
 -	kfree(pages);
++	spin_unlock(&timeline->lock);
++	spin_unlock_irqrestore(&engine->timeline.lock, flags);
+ }
+ 
 -static void __i915_gem_object_reset_page_iter(struct drm_i915_gem_object *obj)
++/* Returns the request if it was guilty of the hang */
++static struct i915_request *
++i915_gem_reset_request(struct intel_engine_cs *engine,
++		       struct i915_request *request,
++		       bool stalled)
+ {
 -	struct radix_tree_iter iter;
 -	void __rcu **slot;
 -
 -	rcu_read_lock();
 -	radix_tree_for_each_slot(slot, &obj->mm.get_page.radix, &iter, 0)
 -		radix_tree_delete(&obj->mm.get_page.radix, iter.index);
 -	rcu_read_unlock();
 -}
++	/* The guilty request will get skipped on a hung engine.
++	 *
++	 * Users of client default contexts do not rely on logical
++	 * state preserved between batches so it is safe to execute
++	 * queued requests following the hang. Non default contexts
++	 * rely on preserved state, so skipping a batch loses the
++	 * evolution of the state and it needs to be considered corrupted.
++	 * Executing more queued batches on top of corrupted state is
++	 * risky. But we take the risk by trying to advance through
++	 * the queued requests in order to make the client behaviour
++	 * more predictable around resets, by not throwing away random
++	 * amount of batches it has prepared for execution. Sophisticated
++	 * clients can use gem_reset_stats_ioctl and dma fence status
++	 * (exported via sync_file info ioctl on explicit fences) to observe
++	 * when it loses the context state and should rebuild accordingly.
++	 *
++	 * The context ban, and ultimately the client ban, mechanism are safety
++	 * valves if client submission ends up resulting in nothing more than
++	 * subsequent hangs.
++	 */
+ 
 -static struct sg_table *
 -__i915_gem_object_unset_pages(struct drm_i915_gem_object *obj)
 -{
 -	struct drm_i915_private *i915 = to_i915(obj->base.dev);
 -	struct sg_table *pages;
++	if (i915_request_completed(request)) {
++		GEM_TRACE("%s pardoned global=%d (fence %llx:%d), current %d\n",
++			  engine->name, request->global_seqno,
++			  request->fence.context, request->fence.seqno,
++			  intel_engine_get_seqno(engine));
++		stalled = false;
++	}
+ 
 -	pages = fetch_and_zero(&obj->mm.pages);
 -	if (!pages)
 -		return NULL;
++	if (stalled) {
++		i915_gem_context_mark_guilty(request->gem_context);
++		i915_request_skip(request, -EIO);
+ 
 -	spin_lock(&i915->mm.obj_lock);
 -	list_del(&obj->mm.link);
 -	spin_unlock(&i915->mm.obj_lock);
 -
 -	if (obj->mm.mapping) {
 -		void *ptr;
 -
 -		ptr = page_mask_bits(obj->mm.mapping);
 -		if (is_vmalloc_addr(ptr))
 -			vunmap(ptr);
 -		else
 -			kunmap(kmap_to_page(ptr));
 -
 -		obj->mm.mapping = NULL;
 -	}
 -
 -	__i915_gem_object_reset_page_iter(obj);
 -	obj->mm.page_sizes.phys = obj->mm.page_sizes.sg = 0;
 -
 -	return pages;
 -}
 -
 -void __i915_gem_object_put_pages(struct drm_i915_gem_object *obj,
 -				 enum i915_mm_subclass subclass)
 -{
 -	struct sg_table *pages;
 -
 -	if (i915_gem_object_has_pinned_pages(obj))
 -		return;
 -
 -	GEM_BUG_ON(obj->bind_count);
 -	if (!i915_gem_object_has_pages(obj))
 -		return;
 -
 -	/* May be called by shrinker from within get_pages() (on another bo) */
 -	mutex_lock_nested(&obj->mm.lock, subclass);
 -	if (unlikely(atomic_read(&obj->mm.pages_pin_count)))
 -		goto unlock;
 -
 -	/*
 -	 * ->put_pages might need to allocate memory for the bit17 swizzle
 -	 * array, hence protect them from being reaped by removing them from gtt
 -	 * lists early.
 -	 */
 -	pages = __i915_gem_object_unset_pages(obj);
 -	if (!IS_ERR(pages))
 -		obj->ops->put_pages(obj, pages);
 -
 -unlock:
 -	mutex_unlock(&obj->mm.lock);
 -}
 -
 -bool i915_sg_trim(struct sg_table *orig_st)
 -{
 -	struct sg_table new_st;
 -	struct scatterlist *sg, *new_sg;
 -	unsigned int i;
 -
 -	if (orig_st->nents == orig_st->orig_nents)
 -		return false;
 -
 -	if (sg_alloc_table(&new_st, orig_st->nents, GFP_KERNEL | __GFP_NOWARN))
 -		return false;
 -
 -	new_sg = new_st.sgl;
 -	for_each_sg(orig_st->sgl, sg, orig_st->nents, i) {
 -		sg_set_page(new_sg, sg_page(sg), sg->length, 0);
 -		sg_dma_address(new_sg) = sg_dma_address(sg);
 -		sg_dma_len(new_sg) = sg_dma_len(sg);
 -
 -		new_sg = sg_next(new_sg);
 -	}
 -	GEM_BUG_ON(new_sg); /* Should walk exactly nents and hit the end */
 -
 -	sg_free_table(orig_st);
 -
 -	*orig_st = new_st;
 -	return true;
 -}
 -
 -static int i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)
 -{
 -	struct drm_i915_private *dev_priv = to_i915(obj->base.dev);
 -	const unsigned long page_count = obj->base.size / PAGE_SIZE;
 -	unsigned long i;
 -	struct address_space *mapping;
 -	struct sg_table *st;
 -	struct scatterlist *sg;
 -	struct sgt_iter sgt_iter;
 -	struct page *page;
 -	unsigned long last_pfn = 0;	/* suppress gcc warning */
 -	unsigned int max_segment = i915_sg_segment_size();
 -	unsigned int sg_page_sizes;
 -	struct pagevec pvec;
 -	gfp_t noreclaim;
 -	int ret;
 -
 -	/*
 -	 * Assert that the object is not currently in any GPU domain. As it
 -	 * wasn't in the GTT, there shouldn't be any way it could have been in
 -	 * a GPU cache
 -	 */
 -	GEM_BUG_ON(obj->read_domains & I915_GEM_GPU_DOMAINS);
 -	GEM_BUG_ON(obj->write_domain & I915_GEM_GPU_DOMAINS);
 -
 -	/*
 -	 * If there's no chance of allocating enough pages for the whole
 -	 * object, bail early.
 -	 */
 -	if (page_count > totalram_pages())
 -		return -ENOMEM;
 -
 -	st = kmalloc(sizeof(*st), GFP_KERNEL);
 -	if (st == NULL)
 -		return -ENOMEM;
 -
 -rebuild_st:
 -	if (sg_alloc_table(st, page_count, GFP_KERNEL)) {
 -		kfree(st);
 -		return -ENOMEM;
 -	}
 -
 -	/*
 -	 * Get the list of pages out of our struct file.  They'll be pinned
 -	 * at this point until we release them.
 -	 *
 -	 * Fail silently without starting the shrinker
 -	 */
 -	mapping = obj->base.filp->f_mapping;
 -	mapping_set_unevictable(mapping);
 -	noreclaim = mapping_gfp_constraint(mapping, ~__GFP_RECLAIM);
 -	noreclaim |= __GFP_NORETRY | __GFP_NOWARN;
 -
 -	sg = st->sgl;
 -	st->nents = 0;
 -	sg_page_sizes = 0;
 -	for (i = 0; i < page_count; i++) {
 -		const unsigned int shrink[] = {
 -			I915_SHRINK_BOUND | I915_SHRINK_UNBOUND | I915_SHRINK_PURGEABLE,
 -			0,
 -		}, *s = shrink;
 -		gfp_t gfp = noreclaim;
 -
 -		do {
 -			cond_resched();
 -			page = shmem_read_mapping_page_gfp(mapping, i, gfp);
 -			if (likely(!IS_ERR(page)))
 -				break;
 -
 -			if (!*s) {
 -				ret = PTR_ERR(page);
 -				goto err_sg;
 -			}
 -
 -			i915_gem_shrink(dev_priv, 2 * page_count, NULL, *s++);
 -
 -			/*
 -			 * We've tried hard to allocate the memory by reaping
 -			 * our own buffer, now let the real VM do its job and
 -			 * go down in flames if truly OOM.
 -			 *
 -			 * However, since graphics tend to be disposable,
 -			 * defer the oom here by reporting the ENOMEM back
 -			 * to userspace.
 -			 */
 -			if (!*s) {
 -				/* reclaim and warn, but no oom */
 -				gfp = mapping_gfp_mask(mapping);
 -
 -				/*
 -				 * Our bo are always dirty and so we require
 -				 * kswapd to reclaim our pages (direct reclaim
 -				 * does not effectively begin pageout of our
 -				 * buffers on its own). However, direct reclaim
 -				 * only waits for kswapd when under allocation
 -				 * congestion. So as a result __GFP_RECLAIM is
 -				 * unreliable and fails to actually reclaim our
 -				 * dirty pages -- unless you try over and over
 -				 * again with !__GFP_NORETRY. However, we still
 -				 * want to fail this allocation rather than
 -				 * trigger the out-of-memory killer and for
 -				 * this we want __GFP_RETRY_MAYFAIL.
 -				 */
 -				gfp |= __GFP_RETRY_MAYFAIL;
 -			}
 -		} while (1);
 -
 -		if (!i ||
 -		    sg->length >= max_segment ||
 -		    page_to_pfn(page) != last_pfn + 1) {
 -			if (i) {
 -				sg_page_sizes |= sg->length;
 -				sg = sg_next(sg);
 -			}
 -			st->nents++;
 -			sg_set_page(sg, page, PAGE_SIZE, 0);
 -		} else {
 -			sg->length += PAGE_SIZE;
 -		}
 -		last_pfn = page_to_pfn(page);
 -
 -		/* Check that the i965g/gm workaround works. */
 -		WARN_ON((gfp & __GFP_DMA32) && (last_pfn >= 0x00100000UL));
 -	}
 -	if (sg) { /* loop terminated early; short sg table */
 -		sg_page_sizes |= sg->length;
 -		sg_mark_end(sg);
 -	}
 -
 -	/* Trim unused sg entries to avoid wasting memory. */
 -	i915_sg_trim(st);
 -
 -	ret = i915_gem_gtt_prepare_pages(obj, st);
 -	if (ret) {
 -		/*
 -		 * DMA remapping failed? One possible cause is that
 -		 * it could not reserve enough large entries, asking
 -		 * for PAGE_SIZE chunks instead may be helpful.
 -		 */
 -		if (max_segment > PAGE_SIZE) {
 -			for_each_sgt_page(page, sgt_iter, st)
 -				put_page(page);
 -			sg_free_table(st);
 -
 -			max_segment = PAGE_SIZE;
 -			goto rebuild_st;
 -		} else {
 -			dev_warn(&dev_priv->drm.pdev->dev,
 -				 "Failed to DMA remap %lu pages\n",
 -				 page_count);
 -			goto err_pages;
 -		}
 -	}
 -
 -	if (i915_gem_object_needs_bit17_swizzle(obj))
 -		i915_gem_object_do_bit_17_swizzle(obj, st);
 -
 -	__i915_gem_object_set_pages(obj, st, sg_page_sizes);
 -
 -	return 0;
 -
 -err_sg:
 -	sg_mark_end(sg);
 -err_pages:
 -	mapping_clear_unevictable(mapping);
 -	pagevec_init(&pvec);
 -	for_each_sgt_page(page, sgt_iter, st) {
 -		if (!pagevec_add(&pvec, page))
 -			check_release_pagevec(&pvec);
 -	}
 -	if (pagevec_count(&pvec))
 -		check_release_pagevec(&pvec);
 -	sg_free_table(st);
 -	kfree(st);
 -
 -	/*
 -	 * shmemfs first checks if there is enough memory to allocate the page
 -	 * and reports ENOSPC should there be insufficient, along with the usual
 -	 * ENOMEM for a genuine allocation failure.
 -	 *
 -	 * We use ENOSPC in our driver to mean that we have run out of aperture
 -	 * space and so want to translate the error from shmemfs back to our
 -	 * usual understanding of ENOMEM.
 -	 */
 -	if (ret == -ENOSPC)
 -		ret = -ENOMEM;
 -
 -	return ret;
 -}
 -
 -void __i915_gem_object_set_pages(struct drm_i915_gem_object *obj,
 -				 struct sg_table *pages,
 -				 unsigned int sg_page_sizes)
 -{
 -	struct drm_i915_private *i915 = to_i915(obj->base.dev);
 -	unsigned long supported = INTEL_INFO(i915)->page_sizes;
 -	int i;
 -
 -	lockdep_assert_held(&obj->mm.lock);
 -
 -	obj->mm.get_page.sg_pos = pages->sgl;
 -	obj->mm.get_page.sg_idx = 0;
 -
 -	obj->mm.pages = pages;
 -
 -	if (i915_gem_object_is_tiled(obj) &&
 -	    i915->quirks & QUIRK_PIN_SWIZZLED_PAGES) {
 -		GEM_BUG_ON(obj->mm.quirked);
 -		__i915_gem_object_pin_pages(obj);
 -		obj->mm.quirked = true;
 -	}
 -
 -	GEM_BUG_ON(!sg_page_sizes);
 -	obj->mm.page_sizes.phys = sg_page_sizes;
 -
 -	/*
 -	 * Calculate the supported page-sizes which fit into the given
 -	 * sg_page_sizes. This will give us the page-sizes which we may be able
 -	 * to use opportunistically when later inserting into the GTT. For
 -	 * example if phys=2G, then in theory we should be able to use 1G, 2M,
 -	 * 64K or 4K pages, although in practice this will depend on a number of
 -	 * other factors.
 -	 */
 -	obj->mm.page_sizes.sg = 0;
 -	for_each_set_bit(i, &supported, ilog2(I915_GTT_MAX_PAGE_SIZE) + 1) {
 -		if (obj->mm.page_sizes.phys & ~0u << i)
 -			obj->mm.page_sizes.sg |= BIT(i);
 -	}
 -	GEM_BUG_ON(!HAS_PAGE_SIZES(i915, obj->mm.page_sizes.sg));
 -
 -	spin_lock(&i915->mm.obj_lock);
 -	list_add(&obj->mm.link, &i915->mm.unbound_list);
 -	spin_unlock(&i915->mm.obj_lock);
 -}
 -
 -static int ____i915_gem_object_get_pages(struct drm_i915_gem_object *obj)
 -{
 -	int err;
 -
 -	if (unlikely(obj->mm.madv != I915_MADV_WILLNEED)) {
 -		DRM_DEBUG("Attempting to obtain a purgeable object\n");
 -		return -EFAULT;
 -	}
 -
 -	err = obj->ops->get_pages(obj);
 -	GEM_BUG_ON(!err && !i915_gem_object_has_pages(obj));
 -
 -	return err;
 -}
 -
 -/* Ensure that the associated pages are gathered from the backing storage
 - * and pinned into our object. i915_gem_object_pin_pages() may be called
 - * multiple times before they are released by a single call to
 - * i915_gem_object_unpin_pages() - once the pages are no longer referenced
 - * either as a result of memory pressure (reaping pages under the shrinker)
 - * or as the object is itself released.
 - */
 -int __i915_gem_object_get_pages(struct drm_i915_gem_object *obj)
 -{
 -	int err;
 -
 -	err = mutex_lock_interruptible(&obj->mm.lock);
 -	if (err)
 -		return err;
 -
 -	if (unlikely(!i915_gem_object_has_pages(obj))) {
 -		GEM_BUG_ON(i915_gem_object_has_pinned_pages(obj));
 -
 -		err = ____i915_gem_object_get_pages(obj);
 -		if (err)
 -			goto unlock;
 -
 -		smp_mb__before_atomic();
 -	}
 -	atomic_inc(&obj->mm.pages_pin_count);
 -
 -unlock:
 -	mutex_unlock(&obj->mm.lock);
 -	return err;
 -}
 -
 -/* The 'mapping' part of i915_gem_object_pin_map() below */
 -static void *i915_gem_object_map(const struct drm_i915_gem_object *obj,
 -				 enum i915_map_type type)
 -{
 -	unsigned long n_pages = obj->base.size >> PAGE_SHIFT;
 -	struct sg_table *sgt = obj->mm.pages;
 -	struct sgt_iter sgt_iter;
 -	struct page *page;
 -	struct page *stack_pages[32];
 -	struct page **pages = stack_pages;
 -	unsigned long i = 0;
 -	pgprot_t pgprot;
 -	void *addr;
 -
 -	/* A single page can always be kmapped */
 -	if (n_pages == 1 && type == I915_MAP_WB)
 -		return kmap(sg_page(sgt->sgl));
 -
 -	if (n_pages > ARRAY_SIZE(stack_pages)) {
 -		/* Too big for stack -- allocate temporary array instead */
 -		pages = kvmalloc_array(n_pages, sizeof(*pages), GFP_KERNEL);
 -		if (!pages)
 -			return NULL;
 -	}
 -
 -	for_each_sgt_page(page, sgt_iter, sgt)
 -		pages[i++] = page;
 -
 -	/* Check that we have the expected number of pages */
 -	GEM_BUG_ON(i != n_pages);
 -
 -	switch (type) {
 -	default:
 -		MISSING_CASE(type);
 -		/* fallthrough to use PAGE_KERNEL anyway */
 -	case I915_MAP_WB:
 -		pgprot = PAGE_KERNEL;
 -		break;
 -	case I915_MAP_WC:
 -		pgprot = pgprot_writecombine(PAGE_KERNEL_IO);
 -		break;
 -	}
 -	addr = vmap(pages, n_pages, 0, pgprot);
 -
 -	if (pages != stack_pages)
 -		kvfree(pages);
 -
 -	return addr;
 -}
 -
 -/* get, pin, and map the pages of the object into kernel space */
 -void *i915_gem_object_pin_map(struct drm_i915_gem_object *obj,
 -			      enum i915_map_type type)
 -{
 -	enum i915_map_type has_type;
 -	bool pinned;
 -	void *ptr;
 -	int ret;
 -
 -	if (unlikely(!i915_gem_object_has_struct_page(obj)))
 -		return ERR_PTR(-ENXIO);
 -
 -	ret = mutex_lock_interruptible(&obj->mm.lock);
 -	if (ret)
 -		return ERR_PTR(ret);
 -
 -	pinned = !(type & I915_MAP_OVERRIDE);
 -	type &= ~I915_MAP_OVERRIDE;
 -
 -	if (!atomic_inc_not_zero(&obj->mm.pages_pin_count)) {
 -		if (unlikely(!i915_gem_object_has_pages(obj))) {
 -			GEM_BUG_ON(i915_gem_object_has_pinned_pages(obj));
 -
 -			ret = ____i915_gem_object_get_pages(obj);
 -			if (ret)
 -				goto err_unlock;
 -
 -			smp_mb__before_atomic();
 -		}
 -		atomic_inc(&obj->mm.pages_pin_count);
 -		pinned = false;
 -	}
 -	GEM_BUG_ON(!i915_gem_object_has_pages(obj));
 -
 -	ptr = page_unpack_bits(obj->mm.mapping, &has_type);
 -	if (ptr && has_type != type) {
 -		if (pinned) {
 -			ret = -EBUSY;
 -			goto err_unpin;
 -		}
 -
 -		if (is_vmalloc_addr(ptr))
 -			vunmap(ptr);
 -		else
 -			kunmap(kmap_to_page(ptr));
 -
 -		ptr = obj->mm.mapping = NULL;
 -	}
 -
 -	if (!ptr) {
 -		ptr = i915_gem_object_map(obj, type);
 -		if (!ptr) {
 -			ret = -ENOMEM;
 -			goto err_unpin;
 -		}
 -
 -		obj->mm.mapping = page_pack_bits(ptr, type);
 -	}
 -
 -out_unlock:
 -	mutex_unlock(&obj->mm.lock);
 -	return ptr;
 -
 -err_unpin:
 -	atomic_dec(&obj->mm.pages_pin_count);
 -err_unlock:
 -	ptr = ERR_PTR(ret);
 -	goto out_unlock;
 -}
 -
 -static int
 -i915_gem_object_pwrite_gtt(struct drm_i915_gem_object *obj,
 -			   const struct drm_i915_gem_pwrite *arg)
 -{
 -	struct address_space *mapping = obj->base.filp->f_mapping;
 -	char __user *user_data = u64_to_user_ptr(arg->data_ptr);
 -	u64 remain, offset;
 -	unsigned int pg;
 -
 -	/* Before we instantiate/pin the backing store for our use, we
 -	 * can prepopulate the shmemfs filp efficiently using a write into
 -	 * the pagecache. We avoid the penalty of instantiating all the
 -	 * pages, important if the user is just writing to a few and never
 -	 * uses the object on the GPU, and using a direct write into shmemfs
 -	 * allows it to avoid the cost of retrieving a page (either swapin
 -	 * or clearing-before-use) before it is overwritten.
 -	 */
 -	if (i915_gem_object_has_pages(obj))
 -		return -ENODEV;
 -
 -	if (obj->mm.madv != I915_MADV_WILLNEED)
 -		return -EFAULT;
 -
 -	/* Before the pages are instantiated the object is treated as being
 -	 * in the CPU domain. The pages will be clflushed as required before
 -	 * use, and we can freely write into the pages directly. If userspace
 -	 * races pwrite with any other operation; corruption will ensue -
 -	 * that is userspace's prerogative!
 -	 */
 -
 -	remain = arg->size;
 -	offset = arg->offset;
 -	pg = offset_in_page(offset);
 -
 -	do {
 -		unsigned int len, unwritten;
 -		struct page *page;
 -		void *data, *vaddr;
 -		int err;
 -
 -		len = PAGE_SIZE - pg;
 -		if (len > remain)
 -			len = remain;
 -
 -		err = pagecache_write_begin(obj->base.filp, mapping,
 -					    offset, len, 0,
 -					    &page, &data);
 -		if (err < 0)
 -			return err;
 -
 -		vaddr = kmap(page);
 -		unwritten = copy_from_user(vaddr + pg, user_data, len);
 -		kunmap(page);
 -
 -		err = pagecache_write_end(obj->base.filp, mapping,
 -					  offset, len, len - unwritten,
 -					  page, data);
 -		if (err < 0)
 -			return err;
 -
 -		if (unwritten)
 -			return -EFAULT;
 -
 -		remain -= len;
 -		user_data += len;
 -		offset += len;
 -		pg = 0;
 -	} while (remain);
 -
 -	return 0;
 -}
 -
 -static void i915_gem_client_mark_guilty(struct drm_i915_file_private *file_priv,
 -					const struct i915_gem_context *ctx)
 -{
 -	unsigned int score;
 -	unsigned long prev_hang;
 -
 -	if (i915_gem_context_is_banned(ctx))
 -		score = I915_CLIENT_SCORE_CONTEXT_BAN;
 -	else
 -		score = 0;
 -
 -	prev_hang = xchg(&file_priv->hang_timestamp, jiffies);
 -	if (time_before(jiffies, prev_hang + I915_CLIENT_FAST_HANG_JIFFIES))
 -		score += I915_CLIENT_SCORE_HANG_FAST;
 -
 -	if (score) {
 -		atomic_add(score, &file_priv->ban_score);
 -
 -		DRM_DEBUG_DRIVER("client %s: gained %u ban score, now %u\n",
 -				 ctx->name, score,
 -				 atomic_read(&file_priv->ban_score));
 -	}
 -}
 -
 -static void i915_gem_context_mark_guilty(struct i915_gem_context *ctx)
 -{
 -	unsigned int score;
 -	bool banned, bannable;
 -
 -	atomic_inc(&ctx->guilty_count);
 -
 -	bannable = i915_gem_context_is_bannable(ctx);
 -	score = atomic_add_return(CONTEXT_SCORE_GUILTY, &ctx->ban_score);
 -	banned = score >= CONTEXT_SCORE_BAN_THRESHOLD;
 -
 -	/* Cool contexts don't accumulate client ban score */
 -	if (!bannable)
 -		return;
 -
 -	if (banned) {
 -		DRM_DEBUG_DRIVER("context %s: guilty %d, score %u, banned\n",
 -				 ctx->name, atomic_read(&ctx->guilty_count),
 -				 score);
 -		i915_gem_context_set_banned(ctx);
 -	}
 -
 -	if (!IS_ERR_OR_NULL(ctx->file_priv))
 -		i915_gem_client_mark_guilty(ctx->file_priv, ctx);
 -}
 -
 -static void i915_gem_context_mark_innocent(struct i915_gem_context *ctx)
 -{
 -	atomic_inc(&ctx->active_count);
 -}
 -
 -struct i915_request *
 -i915_gem_find_active_request(struct intel_engine_cs *engine)
 -{
 -	struct i915_request *request, *active = NULL;
 -	unsigned long flags;
 -
 -	/*
 -	 * We are called by the error capture, reset and to dump engine
 -	 * state at random points in time. In particular, note that neither is
 -	 * crucially ordered with an interrupt. After a hang, the GPU is dead
 -	 * and we assume that no more writes can happen (we waited long enough
 -	 * for all writes that were in transaction to be flushed) - adding an
 -	 * extra delay for a recent interrupt is pointless. Hence, we do
 -	 * not need an engine->irq_seqno_barrier() before the seqno reads.
 -	 * At all other times, we must assume the GPU is still running, but
 -	 * we only care about the snapshot of this moment.
 -	 */
 -	spin_lock_irqsave(&engine->timeline.lock, flags);
 -	list_for_each_entry(request, &engine->timeline.requests, link) {
 -		if (__i915_request_completed(request, request->global_seqno))
 -			continue;
 -
 -		active = request;
 -		break;
 -	}
 -	spin_unlock_irqrestore(&engine->timeline.lock, flags);
 -
 -	return active;
 -}
 -
 -/*
 - * Ensure irq handler finishes, and not run again.
 - * Also return the active request so that we only search for it once.
 - */
 -struct i915_request *
 -i915_gem_reset_prepare_engine(struct intel_engine_cs *engine)
 -{
 -	struct i915_request *request;
 -
 -	/*
 -	 * During the reset sequence, we must prevent the engine from
 -	 * entering RC6. As the context state is undefined until we restart
 -	 * the engine, if it does enter RC6 during the reset, the state
 -	 * written to the powercontext is undefined and so we may lose
 -	 * GPU state upon resume, i.e. fail to restart after a reset.
 -	 */
 -	intel_uncore_forcewake_get(engine->i915, FORCEWAKE_ALL);
 -
 -	request = engine->reset.prepare(engine);
 -	if (request && request->fence.error == -EIO)
 -		request = ERR_PTR(-EIO); /* Previous reset failed! */
 -
 -	return request;
 -}
 -
 -int i915_gem_reset_prepare(struct drm_i915_private *dev_priv)
 -{
 -	struct intel_engine_cs *engine;
 -	struct i915_request *request;
 -	enum intel_engine_id id;
 -	int err = 0;
 -
 -	for_each_engine(engine, dev_priv, id) {
 -		request = i915_gem_reset_prepare_engine(engine);
 -		if (IS_ERR(request)) {
 -			err = PTR_ERR(request);
 -			continue;
 -		}
 -
 -		engine->hangcheck.active_request = request;
 -	}
 -
 -	i915_gem_revoke_fences(dev_priv);
 -	intel_uc_sanitize(dev_priv);
 -
 -	return err;
 -}
 -
 -static void engine_skip_context(struct i915_request *request)
 -{
 -	struct intel_engine_cs *engine = request->engine;
 -	struct i915_gem_context *hung_ctx = request->gem_context;
 -	struct i915_timeline *timeline = request->timeline;
 -	unsigned long flags;
 -
 -	GEM_BUG_ON(timeline == &engine->timeline);
 -
 -	spin_lock_irqsave(&engine->timeline.lock, flags);
 -	spin_lock(&timeline->lock);
 -
 -	list_for_each_entry_continue(request, &engine->timeline.requests, link)
 -		if (request->gem_context == hung_ctx)
 -			i915_request_skip(request, -EIO);
 -
 -	list_for_each_entry(request, &timeline->requests, link)
 -		i915_request_skip(request, -EIO);
 -
 -	spin_unlock(&timeline->lock);
 -	spin_unlock_irqrestore(&engine->timeline.lock, flags);
 -}
 -
 -/* Returns the request if it was guilty of the hang */
 -static struct i915_request *
 -i915_gem_reset_request(struct intel_engine_cs *engine,
 -		       struct i915_request *request,
 -		       bool stalled)
 -{
 -	/* The guilty request will get skipped on a hung engine.
 -	 *
 -	 * Users of client default contexts do not rely on logical
 -	 * state preserved between batches so it is safe to execute
 -	 * queued requests following the hang. Non default contexts
 -	 * rely on preserved state, so skipping a batch loses the
 -	 * evolution of the state and it needs to be considered corrupted.
 -	 * Executing more queued batches on top of corrupted state is
 -	 * risky. But we take the risk by trying to advance through
 -	 * the queued requests in order to make the client behaviour
 -	 * more predictable around resets, by not throwing away random
 -	 * amount of batches it has prepared for execution. Sophisticated
 -	 * clients can use gem_reset_stats_ioctl and dma fence status
 -	 * (exported via sync_file info ioctl on explicit fences) to observe
 -	 * when it loses the context state and should rebuild accordingly.
 -	 *
 -	 * The context ban, and ultimately the client ban, mechanism are safety
 -	 * valves if client submission ends up resulting in nothing more than
 -	 * subsequent hangs.
 -	 */
 -
 -	if (i915_request_completed(request)) {
 -		GEM_TRACE("%s pardoned global=%d (fence %llx:%d), current %d\n",
 -			  engine->name, request->global_seqno,
 -			  request->fence.context, request->fence.seqno,
 -			  intel_engine_get_seqno(engine));
 -		stalled = false;
 -	}
 -
 -	if (stalled) {
 -		i915_gem_context_mark_guilty(request->gem_context);
 -		i915_request_skip(request, -EIO);
 -
 -		/* If this context is now banned, skip all pending requests. */
 -		if (i915_gem_context_is_banned(request->gem_context))
 -			engine_skip_context(request);
 -	} else {
 -		/*
 -		 * Since this is not the hung engine, it may have advanced
 -		 * since the hang declaration. Double check by refinding
 -		 * the active request at the time of the reset.
 -		 */
 -		request = i915_gem_find_active_request(engine);
 -		if (request) {
 -			unsigned long flags;
 -
 -			i915_gem_context_mark_innocent(request->gem_context);
 -			dma_fence_set_error(&request->fence, -EAGAIN);
 -
 -			/* Rewind the engine to replay the incomplete rq */
 -			spin_lock_irqsave(&engine->timeline.lock, flags);
 -			request = list_prev_entry(request, link);
 -			if (&request->link == &engine->timeline.requests)
 -				request = NULL;
 -			spin_unlock_irqrestore(&engine->timeline.lock, flags);
 -		}
 -	}
 -
 -	return request;
 -}
 -
 -void i915_gem_reset_engine(struct intel_engine_cs *engine,
 -			   struct i915_request *request,
 -			   bool stalled)
 -{
 -	/*
 -	 * Make sure this write is visible before we re-enable the interrupt
 -	 * handlers on another CPU, as tasklet_enable() resolves to just
 -	 * a compiler barrier which is insufficient for our purpose here.
 -	 */
 -	smp_store_mb(engine->irq_posted, 0);
 -
 -	if (request)
 -		request = i915_gem_reset_request(engine, request, stalled);
 -
 -	/* Setup the CS to resume from the breadcrumb of the hung request */
 -	engine->reset.reset(engine, request);
 -}
 -
 -void i915_gem_reset(struct drm_i915_private *dev_priv,
 -		    unsigned int stalled_mask)
 -{
 -	struct intel_engine_cs *engine;
 -	enum intel_engine_id id;
 -
 -	lockdep_assert_held(&dev_priv->drm.struct_mutex);
 -
 -	i915_retire_requests(dev_priv);
 -
 -	for_each_engine(engine, dev_priv, id) {
 -		struct intel_context *ce;
 -
 -		i915_gem_reset_engine(engine,
 -				      engine->hangcheck.active_request,
 -				      stalled_mask & ENGINE_MASK(id));
 -		ce = fetch_and_zero(&engine->last_retired_context);
 -		if (ce)
 -			intel_context_unpin(ce);
 -
 -		/*
 -		 * Ostensibily, we always want a context loaded for powersaving,
 -		 * so if the engine is idle after the reset, send a request
 -		 * to load our scratch kernel_context.
 -		 *
 -		 * More mysteriously, if we leave the engine idle after a reset,
 -		 * the next userspace batch may hang, with what appears to be
 -		 * an incoherent read by the CS (presumably stale TLB). An
 -		 * empty request appears sufficient to paper over the glitch.
 -		 */
 -		if (intel_engine_is_idle(engine)) {
 -			struct i915_request *rq;
 -
 -			rq = i915_request_alloc(engine,
 -						dev_priv->kernel_context);
 -			if (!IS_ERR(rq))
 -				i915_request_add(rq);
 -		}
 -	}
 -
 -	i915_gem_restore_fences(dev_priv);
 -}
 -
 -void i915_gem_reset_finish_engine(struct intel_engine_cs *engine)
 -{
 -	engine->reset.finish(engine);
 -
 -	intel_uncore_forcewake_put(engine->i915, FORCEWAKE_ALL);
 -}
 -
 -void i915_gem_reset_finish(struct drm_i915_private *dev_priv)
 -{
 -	struct intel_engine_cs *engine;
 -	enum intel_engine_id id;
 -
 -	lockdep_assert_held(&dev_priv->drm.struct_mutex);
 -
 -	for_each_engine(engine, dev_priv, id) {
 -		engine->hangcheck.active_request = NULL;
 -		i915_gem_reset_finish_engine(engine);
 -	}
 -}
 -
 -static void nop_submit_request(struct i915_request *request)
 -{
 -	unsigned long flags;
 -
 -	GEM_TRACE("%s fence %llx:%d -> -EIO\n",
 -		  request->engine->name,
 -		  request->fence.context, request->fence.seqno);
 -	dma_fence_set_error(&request->fence, -EIO);
 -
 -	spin_lock_irqsave(&request->engine->timeline.lock, flags);
 -	__i915_request_submit(request);
 -	intel_engine_init_global_seqno(request->engine, request->global_seqno);
 -	spin_unlock_irqrestore(&request->engine->timeline.lock, flags);
 -}
 -
 -void i915_gem_set_wedged(struct drm_i915_private *i915)
 -{
 -	struct intel_engine_cs *engine;
 -	enum intel_engine_id id;
 -
 -	GEM_TRACE("start\n");
 -
 -	if (GEM_SHOW_DEBUG()) {
 -		struct drm_printer p = drm_debug_printer(__func__);
 -
 -		for_each_engine(engine, i915, id)
 -			intel_engine_dump(engine, &p, "%s\n", engine->name);
 -	}
 -
 -	if (test_and_set_bit(I915_WEDGED, &i915->gpu_error.flags))
 -		goto out;
 -
 -	/*
 -	 * First, stop submission to hw, but do not yet complete requests by
 -	 * rolling the global seqno forward (since this would complete requests
 -	 * for which we haven't set the fence error to EIO yet).
 -	 */
 -	for_each_engine(engine, i915, id)
 -		i915_gem_reset_prepare_engine(engine);
 -
 -	/* Even if the GPU reset fails, it should still stop the engines */
 -	if (INTEL_GEN(i915) >= 5)
 -		intel_gpu_reset(i915, ALL_ENGINES);
 -
 -	for_each_engine(engine, i915, id) {
 -		engine->submit_request = nop_submit_request;
 -		engine->schedule = NULL;
 -	}
 -	i915->caps.scheduler = 0;
 -
 -	/*
 -	 * Make sure no request can slip through without getting completed by
 -	 * either this call here to intel_engine_init_global_seqno, or the one
 -	 * in nop_submit_request.
 -	 */
 -	synchronize_rcu();
 -
 -	/* Mark all executing requests as skipped */
 -	for_each_engine(engine, i915, id)
 -		engine->cancel_requests(engine);
 -
 -	for_each_engine(engine, i915, id) {
 -		i915_gem_reset_finish_engine(engine);
 -		intel_engine_wakeup(engine);
 -	}
 -
 -out:
 -	GEM_TRACE("end\n");
 -
 -	wake_up_all(&i915->gpu_error.reset_queue);
 -}
 -
 -bool i915_gem_unset_wedged(struct drm_i915_private *i915)
 -{
 -	struct i915_timeline *tl;
 -
 -	lockdep_assert_held(&i915->drm.struct_mutex);
 -	if (!test_bit(I915_WEDGED, &i915->gpu_error.flags))
 -		return true;
 -
 -	GEM_TRACE("start\n");
 -
 -	/*
 -	 * Before unwedging, make sure that all pending operations
 -	 * are flushed and errored out - we may have requests waiting upon
 -	 * third party fences. We marked all inflight requests as EIO, and
 -	 * every execbuf since returned EIO, for consistency we want all
 -	 * the currently pending requests to also be marked as EIO, which
 -	 * is done inside our nop_submit_request - and so we must wait.
 -	 *
 -	 * No more can be submitted until we reset the wedged bit.
 -	 */
 -	list_for_each_entry(tl, &i915->gt.timelines, link) {
 -		struct i915_request *rq;
 -
 -		rq = i915_gem_active_peek(&tl->last_request,
 -					  &i915->drm.struct_mutex);
 -		if (!rq)
 -			continue;
 -
 -		/*
 -		 * We can't use our normal waiter as we want to
 -		 * avoid recursively trying to handle the current
 -		 * reset. The basic dma_fence_default_wait() installs
 -		 * a callback for dma_fence_signal(), which is
 -		 * triggered by our nop handler (indirectly, the
 -		 * callback enables the signaler thread which is
 -		 * woken by the nop_submit_request() advancing the seqno
 -		 * and when the seqno passes the fence, the signaler
 -		 * then signals the fence waking us up).
 -		 */
 -		if (dma_fence_default_wait(&rq->fence, true,
 -					   MAX_SCHEDULE_TIMEOUT) < 0)
 -			return false;
 -	}
 -	i915_retire_requests(i915);
 -	GEM_BUG_ON(i915->gt.active_requests);
 -
 -	if (!intel_gpu_reset(i915, ALL_ENGINES))
 -		intel_engines_sanitize(i915);
 -
 -	/*
 -	 * Undo nop_submit_request. We prevent all new i915 requests from
 -	 * being queued (by disallowing execbuf whilst wedged) so having
 -	 * waited for all active requests above, we know the system is idle
 -	 * and do not have to worry about a thread being inside
 -	 * engine->submit_request() as we swap over. So unlike installing
 -	 * the nop_submit_request on reset, we can do this from normal
 -	 * context and do not require stop_machine().
 -	 */
 -	intel_engines_reset_default_submission(i915);
 -	i915_gem_contexts_lost(i915);
 -
 -	GEM_TRACE("end\n");
 -
 -	smp_mb__before_atomic(); /* complete takeover before enabling execbuf */
 -	clear_bit(I915_WEDGED, &i915->gpu_error.flags);
 -
 -	return true;
 -}
 -
 -static void
 -i915_gem_retire_work_handler(struct work_struct *work)
 -{
 -	struct drm_i915_private *dev_priv =
 -		container_of(work, typeof(*dev_priv), gt.retire_work.work);
 -	struct drm_device *dev = &dev_priv->drm;
 -
 -	/* Come back later if the device is busy... */
 -	if (mutex_trylock(&dev->struct_mutex)) {
 -		i915_retire_requests(dev_priv);
 -		mutex_unlock(&dev->struct_mutex);
 -	}
 -
 -	/*
 -	 * Keep the retire handler running until we are finally idle.
 -	 * We do not need to do this test under locking as in the worst-case
 -	 * we queue the retire worker once too often.
 -	 */
 -	if (READ_ONCE(dev_priv->gt.awake))
 -		queue_delayed_work(dev_priv->wq,
 -				   &dev_priv->gt.retire_work,
 -				   round_jiffies_up_relative(HZ));
 -}
 -
 -static void shrink_caches(struct drm_i915_private *i915)
 -{
 -	/*
 -	 * kmem_cache_shrink() discards empty slabs and reorders partially
 -	 * filled slabs to prioritise allocating from the mostly full slabs,
 -	 * with the aim of reducing fragmentation.
 -	 */
 -	kmem_cache_shrink(i915->priorities);
 -	kmem_cache_shrink(i915->dependencies);
 -	kmem_cache_shrink(i915->requests);
 -	kmem_cache_shrink(i915->luts);
 -	kmem_cache_shrink(i915->vmas);
 -	kmem_cache_shrink(i915->objects);
 -}
 -
 -struct sleep_rcu_work {
 -	union {
 -		struct rcu_head rcu;
 -		struct work_struct work;
 -	};
 -	struct drm_i915_private *i915;
 -	unsigned int epoch;
 -};
 -
 -static inline bool
 -same_epoch(struct drm_i915_private *i915, unsigned int epoch)
 -{
 -	/*
 -	 * There is a small chance that the epoch wrapped since we started
 -	 * sleeping. If we assume that epoch is at least a u32, then it will
 -	 * take at least 2^32 * 100ms for it to wrap, or about 326 years.
 -	 */
 -	return epoch == READ_ONCE(i915->gt.epoch);
 -}
 -
 -static void __sleep_work(struct work_struct *work)
 -{
 -	struct sleep_rcu_work *s = container_of(work, typeof(*s), work);
 -	struct drm_i915_private *i915 = s->i915;
 -	unsigned int epoch = s->epoch;
 -
 -	kfree(s);
 -	if (same_epoch(i915, epoch))
 -		shrink_caches(i915);
 -}
 -
 -static void __sleep_rcu(struct rcu_head *rcu)
 -{
 -	struct sleep_rcu_work *s = container_of(rcu, typeof(*s), rcu);
 -	struct drm_i915_private *i915 = s->i915;
 -
 -	destroy_rcu_head(&s->rcu);
 -
 -	if (same_epoch(i915, s->epoch)) {
 -		INIT_WORK(&s->work, __sleep_work);
 -		queue_work(i915->wq, &s->work);
 -	} else {
 -		kfree(s);
 -	}
 -}
 -
 -static inline bool
 -new_requests_since_last_retire(const struct drm_i915_private *i915)
 -{
 -	return (READ_ONCE(i915->gt.active_requests) ||
 -		work_pending(&i915->gt.idle_work.work));
 -}
 -
 -static void assert_kernel_context_is_current(struct drm_i915_private *i915)
 -{
 -	struct intel_engine_cs *engine;
 -	enum intel_engine_id id;
 -
 -	if (i915_terminally_wedged(&i915->gpu_error))
 -		return;
 -
 -	GEM_BUG_ON(i915->gt.active_requests);
 -	for_each_engine(engine, i915, id) {
 -		GEM_BUG_ON(__i915_gem_active_peek(&engine->timeline.last_request));
 -		GEM_BUG_ON(engine->last_retired_context !=
 -			   to_intel_context(i915->kernel_context, engine));
 -	}
 -}
 -
 -static void
 -i915_gem_idle_work_handler(struct work_struct *work)
 -{
 -	struct drm_i915_private *dev_priv =
 -		container_of(work, typeof(*dev_priv), gt.idle_work.work);
 -	unsigned int epoch = I915_EPOCH_INVALID;
 -	bool rearm_hangcheck;
 -
 -	if (!READ_ONCE(dev_priv->gt.awake))
 -		return;
 -
 -	if (READ_ONCE(dev_priv->gt.active_requests))
 -		return;
 -
 -	/*
 -	 * Flush out the last user context, leaving only the pinned
 -	 * kernel context resident. When we are idling on the kernel_context,
 -	 * no more new requests (with a context switch) are emitted and we
 -	 * can finally rest. A consequence is that the idle work handler is
 -	 * always called at least twice before idling (and if the system is
 -	 * idle that implies a round trip through the retire worker).
 -	 */
 -	mutex_lock(&dev_priv->drm.struct_mutex);
 -	i915_gem_switch_to_kernel_context(dev_priv);
 -	mutex_unlock(&dev_priv->drm.struct_mutex);
 -
 -	GEM_TRACE("active_requests=%d (after switch-to-kernel-context)\n",
 -		  READ_ONCE(dev_priv->gt.active_requests));
 -
 -	/*
 -	 * Wait for last execlists context complete, but bail out in case a
 -	 * new request is submitted. As we don't trust the hardware, we
 -	 * continue on if the wait times out. This is necessary to allow
 -	 * the machine to suspend even if the hardware dies, and we will
 -	 * try to recover in resume (after depriving the hardware of power,
 -	 * it may be in a better mmod).
 -	 */
 -	__wait_for(if (new_requests_since_last_retire(dev_priv)) return,
 -		   intel_engines_are_idle(dev_priv),
 -		   I915_IDLE_ENGINES_TIMEOUT * 1000,
 -		   10, 500);
 -
 -	rearm_hangcheck =
 -		cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
 -
 -	if (!mutex_trylock(&dev_priv->drm.struct_mutex)) {
 -		/* Currently busy, come back later */
 -		mod_delayed_work(dev_priv->wq,
 -				 &dev_priv->gt.idle_work,
 -				 msecs_to_jiffies(50));
 -		goto out_rearm;
 -	}
 -
 -	/*
 -	 * New request retired after this work handler started, extend active
 -	 * period until next instance of the work.
 -	 */
 -	if (new_requests_since_last_retire(dev_priv))
 -		goto out_unlock;
 -
 -	epoch = __i915_gem_park(dev_priv);
 -
 -	assert_kernel_context_is_current(dev_priv);
 -
 -	rearm_hangcheck = false;
 -out_unlock:
 -	mutex_unlock(&dev_priv->drm.struct_mutex);
 -
 -out_rearm:
 -	if (rearm_hangcheck) {
 -		GEM_BUG_ON(!dev_priv->gt.awake);
 -		i915_queue_hangcheck(dev_priv);
 -	}
 -
 -	/*
 -	 * When we are idle, it is an opportune time to reap our caches.
 -	 * However, we have many objects that utilise RCU and the ordered
 -	 * i915->wq that this work is executing on. To try and flush any
 -	 * pending frees now we are idle, we first wait for an RCU grace
 -	 * period, and then queue a task (that will run last on the wq) to
 -	 * shrink and re-optimize the caches.
 -	 */
 -	if (same_epoch(dev_priv, epoch)) {
 -		struct sleep_rcu_work *s = kmalloc(sizeof(*s), GFP_KERNEL);
 -		if (s) {
 -			init_rcu_head(&s->rcu);
 -			s->i915 = dev_priv;
 -			s->epoch = epoch;
 -			call_rcu(&s->rcu, __sleep_rcu);
 -		}
 -	}
 -}
 -
 -void i915_gem_close_object(struct drm_gem_object *gem, struct drm_file *file)
 -{
 -	struct drm_i915_private *i915 = to_i915(gem->dev);
 -	struct drm_i915_gem_object *obj = to_intel_bo(gem);
 -	struct drm_i915_file_private *fpriv = file->driver_priv;
 -	struct i915_lut_handle *lut, *ln;
 -
 -	mutex_lock(&i915->drm.struct_mutex);
 -
 -	list_for_each_entry_safe(lut, ln, &obj->lut_list, obj_link) {
 -		struct i915_gem_context *ctx = lut->ctx;
 -		struct i915_vma *vma;
 -
 -		GEM_BUG_ON(ctx->file_priv == ERR_PTR(-EBADF));
 -		if (ctx->file_priv != fpriv)
 -			continue;
 -
 -		vma = radix_tree_delete(&ctx->handles_vma, lut->handle);
 -		GEM_BUG_ON(vma->obj != obj);
 -
 -		/* We allow the process to have multiple handles to the same
 -		 * vma, in the same fd namespace, by virtue of flink/open.
 -		 */
 -		GEM_BUG_ON(!vma->open_count);
 -		if (!--vma->open_count && !i915_vma_is_ggtt(vma))
 -			i915_vma_close(vma);
 -
 -		list_del(&lut->obj_link);
 -		list_del(&lut->ctx_link);
 -
 -		kmem_cache_free(i915->luts, lut);
 -		__i915_gem_object_release_unless_active(obj);
 -	}
 -
 -	mutex_unlock(&i915->drm.struct_mutex);
 -}
 -
 -static unsigned long to_wait_timeout(s64 timeout_ns)
 -{
 -	if (timeout_ns < 0)
 -		return MAX_SCHEDULE_TIMEOUT;
 -
 -	if (timeout_ns == 0)
 -		return 0;
 -
 -	return nsecs_to_jiffies_timeout(timeout_ns);
 -}
 -
 -/**
 - * i915_gem_wait_ioctl - implements DRM_IOCTL_I915_GEM_WAIT
 - * @dev: drm device pointer
 - * @data: ioctl data blob
 - * @file: drm file pointer
 - *
 - * Returns 0 if successful, else an error is returned with the remaining time in
 - * the timeout parameter.
 - *  -ETIME: object is still busy after timeout
 - *  -ERESTARTSYS: signal interrupted the wait
 - *  -ENONENT: object doesn't exist
 - * Also possible, but rare:
 - *  -EAGAIN: incomplete, restart syscall
 - *  -ENOMEM: damn
 - *  -ENODEV: Internal IRQ fail
 - *  -E?: The add request failed
 - *
 - * The wait ioctl with a timeout of 0 reimplements the busy ioctl. With any
 - * non-zero timeout parameter the wait ioctl will wait for the given number of
 - * nanoseconds on an object becoming unbusy. Since the wait itself does so
 - * without holding struct_mutex the object may become re-busied before this
 - * function completes. A similar but shorter * race condition exists in the busy
 - * ioctl
 - */
 -int
 -i915_gem_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file)
 -{
 -	struct drm_i915_gem_wait *args = data;
 -	struct drm_i915_gem_object *obj;
 -	ktime_t start;
 -	long ret;
 -
 -	if (args->flags != 0)
 -		return -EINVAL;
 -
 -	obj = i915_gem_object_lookup(file, args->bo_handle);
 -	if (!obj)
 -		return -ENOENT;
 -
 -	start = ktime_get();
 -
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE |
 -				   I915_WAIT_PRIORITY |
 -				   I915_WAIT_ALL,
 -				   to_wait_timeout(args->timeout_ns),
 -				   to_rps_client(file));
 -
 -	if (args->timeout_ns > 0) {
 -		args->timeout_ns -= ktime_to_ns(ktime_sub(ktime_get(), start));
 -		if (args->timeout_ns < 0)
 -			args->timeout_ns = 0;
 -
 -		/*
 -		 * Apparently ktime isn't accurate enough and occasionally has a
 -		 * bit of mismatch in the jiffies<->nsecs<->ktime loop. So patch
 -		 * things up to make the test happy. We allow up to 1 jiffy.
 -		 *
 -		 * This is a regression from the timespec->ktime conversion.
 -		 */
 -		if (ret == -ETIME && !nsecs_to_jiffies(args->timeout_ns))
 -			args->timeout_ns = 0;
 -
 -		/* Asked to wait beyond the jiffie/scheduler precision? */
 -		if (ret == -ETIME && args->timeout_ns)
 -			ret = -EAGAIN;
 -	}
 -
 -	i915_gem_object_put(obj);
 -	return ret;
 -}
 -
 -static long wait_for_timeline(struct i915_timeline *tl,
 -			      unsigned int flags, long timeout)
 -{
 -	struct i915_request *rq;
 -
 -	rq = i915_gem_active_get_unlocked(&tl->last_request);
 -	if (!rq)
 -		return timeout;
 -
 -	/*
 -	 * "Race-to-idle".
 -	 *
 -	 * Switching to the kernel context is often used a synchronous
 -	 * step prior to idling, e.g. in suspend for flushing all
 -	 * current operations to memory before sleeping. These we
 -	 * want to complete as quickly as possible to avoid prolonged
 -	 * stalls, so allow the gpu to boost to maximum clocks.
 -	 */
 -	if (flags & I915_WAIT_FOR_IDLE_BOOST)
 -		gen6_rps_boost(rq, NULL);
 -
 -	timeout = i915_request_wait(rq, flags, timeout);
 -	i915_request_put(rq);
 -
 -	return timeout;
 -}
 -
 -static int wait_for_engines(struct drm_i915_private *i915)
 -{
 -	if (wait_for(intel_engines_are_idle(i915), I915_IDLE_ENGINES_TIMEOUT)) {
 -		dev_err(i915->drm.dev,
 -			"Failed to idle engines, declaring wedged!\n");
 -		GEM_TRACE_DUMP();
 -		i915_gem_set_wedged(i915);
 -		return -EIO;
 -	}
 -
 -	return 0;
 -}
 -
 -int i915_gem_wait_for_idle(struct drm_i915_private *i915,
 -			   unsigned int flags, long timeout)
 -{
 -	GEM_TRACE("flags=%x (%s), timeout=%ld%s\n",
 -		  flags, flags & I915_WAIT_LOCKED ? "locked" : "unlocked",
 -		  timeout, timeout == MAX_SCHEDULE_TIMEOUT ? " (forever)" : "");
 -
 -	/* If the device is asleep, we have no requests outstanding */
 -	if (!READ_ONCE(i915->gt.awake))
 -		return 0;
 -
 -	if (flags & I915_WAIT_LOCKED) {
 -		struct i915_timeline *tl;
 -		int err;
 -
 -		lockdep_assert_held(&i915->drm.struct_mutex);
 -
 -		list_for_each_entry(tl, &i915->gt.timelines, link) {
 -			timeout = wait_for_timeline(tl, flags, timeout);
 -			if (timeout < 0)
 -				return timeout;
 -		}
 -		if (GEM_SHOW_DEBUG() && !timeout) {
 -			/* Presume that timeout was non-zero to begin with! */
 -			dev_warn(&i915->drm.pdev->dev,
 -				 "Missed idle-completion interrupt!\n");
 -			GEM_TRACE_DUMP();
 -		}
 -
 -		err = wait_for_engines(i915);
 -		if (err)
 -			return err;
 -
 -		i915_retire_requests(i915);
 -		GEM_BUG_ON(i915->gt.active_requests);
 -	} else {
 -		struct intel_engine_cs *engine;
 -		enum intel_engine_id id;
 -
 -		for_each_engine(engine, i915, id) {
 -			struct i915_timeline *tl = &engine->timeline;
 -
 -			timeout = wait_for_timeline(tl, flags, timeout);
 -			if (timeout < 0)
 -				return timeout;
 -		}
 -	}
 -
 -	return 0;
 -}
 -
 -static void __i915_gem_object_flush_for_display(struct drm_i915_gem_object *obj)
 -{
 -	/*
 -	 * We manually flush the CPU domain so that we can override and
 -	 * force the flush for the display, and perform it asyncrhonously.
 -	 */
 -	flush_write_domain(obj, ~I915_GEM_DOMAIN_CPU);
 -	if (obj->cache_dirty)
 -		i915_gem_clflush_object(obj, I915_CLFLUSH_FORCE);
 -	obj->write_domain = 0;
 -}
 -
 -void i915_gem_object_flush_if_display(struct drm_i915_gem_object *obj)
 -{
 -	if (!READ_ONCE(obj->pin_global))
 -		return;
 -
 -	mutex_lock(&obj->base.dev->struct_mutex);
 -	__i915_gem_object_flush_for_display(obj);
 -	mutex_unlock(&obj->base.dev->struct_mutex);
 -}
 -
 -/**
 - * Moves a single object to the WC read, and possibly write domain.
 - * @obj: object to act on
 - * @write: ask for write access or read only
 - *
 - * This function returns when the move is complete, including waiting on
 - * flushes to occur.
 - */
 -int
 -i915_gem_object_set_to_wc_domain(struct drm_i915_gem_object *obj, bool write)
 -{
 -	int ret;
 -
 -	lockdep_assert_held(&obj->base.dev->struct_mutex);
 -
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE |
 -				   I915_WAIT_LOCKED |
 -				   (write ? I915_WAIT_ALL : 0),
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   NULL);
 -	if (ret)
 -		return ret;
 -
 -	if (obj->write_domain == I915_GEM_DOMAIN_WC)
 -		return 0;
 -
 -	/* Flush and acquire obj->pages so that we are coherent through
 -	 * direct access in memory with previous cached writes through
 -	 * shmemfs and that our cache domain tracking remains valid.
 -	 * For example, if the obj->filp was moved to swap without us
 -	 * being notified and releasing the pages, we would mistakenly
 -	 * continue to assume that the obj remained out of the CPU cached
 -	 * domain.
 -	 */
 -	ret = i915_gem_object_pin_pages(obj);
 -	if (ret)
 -		return ret;
 -
 -	flush_write_domain(obj, ~I915_GEM_DOMAIN_WC);
++		/* If this context is now banned, skip all pending requests. */
++		if (i915_gem_context_is_banned(request->gem_context))
++			engine_skip_context(request);
++	} else {
++		/*
++		 * Since this is not the hung engine, it may have advanced
++		 * since the hang declaration. Double check by refinding
++		 * the active request at the time of the reset.
++		 */
++		request = i915_gem_find_active_request(engine);
++		if (request) {
++			unsigned long flags;
+ 
 -	/* Serialise direct access to this object with the barriers for
 -	 * coherent writes from the GPU, by effectively invalidating the
 -	 * WC domain upon first access.
 -	 */
 -	if ((obj->read_domains & I915_GEM_DOMAIN_WC) == 0)
 -		mb();
++			i915_gem_context_mark_innocent(request->gem_context);
++			dma_fence_set_error(&request->fence, -EAGAIN);
+ 
 -	/* It should now be out of any other write domains, and we can update
 -	 * the domain values for our changes.
 -	 */
 -	GEM_BUG_ON((obj->write_domain & ~I915_GEM_DOMAIN_WC) != 0);
 -	obj->read_domains |= I915_GEM_DOMAIN_WC;
 -	if (write) {
 -		obj->read_domains = I915_GEM_DOMAIN_WC;
 -		obj->write_domain = I915_GEM_DOMAIN_WC;
 -		obj->mm.dirty = true;
++			/* Rewind the engine to replay the incomplete rq */
++			spin_lock_irqsave(&engine->timeline.lock, flags);
++			request = list_prev_entry(request, link);
++			if (&request->link == &engine->timeline.requests)
++				request = NULL;
++			spin_unlock_irqrestore(&engine->timeline.lock, flags);
++		}
+ 	}
+ 
 -	i915_gem_object_unpin_pages(obj);
 -	return 0;
++	return request;
+ }
+ 
 -/**
 - * Moves a single object to the GTT read, and possibly write domain.
 - * @obj: object to act on
 - * @write: ask for write access or read only
 - *
 - * This function returns when the move is complete, including waiting on
 - * flushes to occur.
 - */
 -int
 -i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
++void i915_gem_reset_engine(struct intel_engine_cs *engine,
++			   struct i915_request *request,
++			   bool stalled)
+ {
 -	int ret;
 -
 -	lockdep_assert_held(&obj->base.dev->struct_mutex);
 -
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE |
 -				   I915_WAIT_LOCKED |
 -				   (write ? I915_WAIT_ALL : 0),
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   NULL);
 -	if (ret)
 -		return ret;
 -
 -	if (obj->write_domain == I915_GEM_DOMAIN_GTT)
 -		return 0;
 -
 -	/* Flush and acquire obj->pages so that we are coherent through
 -	 * direct access in memory with previous cached writes through
 -	 * shmemfs and that our cache domain tracking remains valid.
 -	 * For example, if the obj->filp was moved to swap without us
 -	 * being notified and releasing the pages, we would mistakenly
 -	 * continue to assume that the obj remained out of the CPU cached
 -	 * domain.
 -	 */
 -	ret = i915_gem_object_pin_pages(obj);
 -	if (ret)
 -		return ret;
 -
 -	flush_write_domain(obj, ~I915_GEM_DOMAIN_GTT);
 -
 -	/* Serialise direct access to this object with the barriers for
 -	 * coherent writes from the GPU, by effectively invalidating the
 -	 * GTT domain upon first access.
++	/*
++	 * Make sure this write is visible before we re-enable the interrupt
++	 * handlers on another CPU, as tasklet_enable() resolves to just
++	 * a compiler barrier which is insufficient for our purpose here.
+ 	 */
 -	if ((obj->read_domains & I915_GEM_DOMAIN_GTT) == 0)
 -		mb();
++	smp_store_mb(engine->irq_posted, 0);
+ 
 -	/* It should now be out of any other write domains, and we can update
 -	 * the domain values for our changes.
 -	 */
 -	GEM_BUG_ON((obj->write_domain & ~I915_GEM_DOMAIN_GTT) != 0);
 -	obj->read_domains |= I915_GEM_DOMAIN_GTT;
 -	if (write) {
 -		obj->read_domains = I915_GEM_DOMAIN_GTT;
 -		obj->write_domain = I915_GEM_DOMAIN_GTT;
 -		obj->mm.dirty = true;
 -	}
++	if (request)
++		request = i915_gem_reset_request(engine, request, stalled);
+ 
 -	i915_gem_object_unpin_pages(obj);
 -	return 0;
++	/* Setup the CS to resume from the breadcrumb of the hung request */
++	engine->reset.reset(engine, request);
+ }
+ 
 -/**
 - * Changes the cache-level of an object across all VMA.
 - * @obj: object to act on
 - * @cache_level: new cache level to set for the object
 - *
 - * After this function returns, the object will be in the new cache-level
 - * across all GTT and the contents of the backing storage will be coherent,
 - * with respect to the new cache-level. In order to keep the backing storage
 - * coherent for all users, we only allow a single cache level to be set
 - * globally on the object and prevent it from being changed whilst the
 - * hardware is reading from the object. That is if the object is currently
 - * on the scanout it will be set to uncached (or equivalent display
 - * cache coherency) and all non-MOCS GPU access will also be uncached so
 - * that all direct access to the scanout remains coherent.
 - */
 -int i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
 -				    enum i915_cache_level cache_level)
++void i915_gem_reset(struct drm_i915_private *dev_priv,
++		    unsigned int stalled_mask)
+ {
 -	struct i915_vma *vma;
 -	int ret;
 -
 -	lockdep_assert_held(&obj->base.dev->struct_mutex);
 -
 -	if (obj->cache_level == cache_level)
 -		return 0;
 -
 -	/* Inspect the list of currently bound VMA and unbind any that would
 -	 * be invalid given the new cache-level. This is principally to
 -	 * catch the issue of the CS prefetch crossing page boundaries and
 -	 * reading an invalid PTE on older architectures.
 -	 */
 -restart:
 -	list_for_each_entry(vma, &obj->vma_list, obj_link) {
 -		if (!drm_mm_node_allocated(&vma->node))
 -			continue;
++	struct intel_engine_cs *engine;
++	enum intel_engine_id id;
+ 
 -		if (i915_vma_is_pinned(vma)) {
 -			DRM_DEBUG("can not change the cache level of pinned objects\n");
 -			return -EBUSY;
 -		}
++	lockdep_assert_held(&dev_priv->drm.struct_mutex);
+ 
 -		if (!i915_vma_is_closed(vma) &&
 -		    i915_gem_valid_gtt_space(vma, cache_level))
 -			continue;
++	i915_retire_requests(dev_priv);
+ 
 -		ret = i915_vma_unbind(vma);
 -		if (ret)
 -			return ret;
++	for_each_engine(engine, dev_priv, id) {
++		struct intel_context *ce;
+ 
 -		/* As unbinding may affect other elements in the
 -		 * obj->vma_list (due to side-effects from retiring
 -		 * an active vma), play safe and restart the iterator.
 -		 */
 -		goto restart;
 -	}
++		i915_gem_reset_engine(engine,
++				      engine->hangcheck.active_request,
++				      stalled_mask & ENGINE_MASK(id));
++		ce = fetch_and_zero(&engine->last_retired_context);
++		if (ce)
++			intel_context_unpin(ce);
+ 
 -	/* We can reuse the existing drm_mm nodes but need to change the
 -	 * cache-level on the PTE. We could simply unbind them all and
 -	 * rebind with the correct cache-level on next use. However since
 -	 * we already have a valid slot, dma mapping, pages etc, we may as
 -	 * rewrite the PTE in the belief that doing so tramples upon less
 -	 * state and so involves less work.
 -	 */
 -	if (obj->bind_count) {
 -		/* Before we change the PTE, the GPU must not be accessing it.
 -		 * If we wait upon the object, we know that all the bound
 -		 * VMA are no longer active.
++		/*
++		 * Ostensibily, we always want a context loaded for powersaving,
++		 * so if the engine is idle after the reset, send a request
++		 * to load our scratch kernel_context.
++		 *
++		 * More mysteriously, if we leave the engine idle after a reset,
++		 * the next userspace batch may hang, with what appears to be
++		 * an incoherent read by the CS (presumably stale TLB). An
++		 * empty request appears sufficient to paper over the glitch.
+ 		 */
 -		ret = i915_gem_object_wait(obj,
 -					   I915_WAIT_INTERRUPTIBLE |
 -					   I915_WAIT_LOCKED |
 -					   I915_WAIT_ALL,
 -					   MAX_SCHEDULE_TIMEOUT,
 -					   NULL);
 -		if (ret)
 -			return ret;
 -
 -		if (!HAS_LLC(to_i915(obj->base.dev)) &&
 -		    cache_level != I915_CACHE_NONE) {
 -			/* Access to snoopable pages through the GTT is
 -			 * incoherent and on some machines causes a hard
 -			 * lockup. Relinquish the CPU mmaping to force
 -			 * userspace to refault in the pages and we can
 -			 * then double check if the GTT mapping is still
 -			 * valid for that pointer access.
 -			 */
 -			i915_gem_release_mmap(obj);
 -
 -			/* As we no longer need a fence for GTT access,
 -			 * we can relinquish it now (and so prevent having
 -			 * to steal a fence from someone else on the next
 -			 * fence request). Note GPU activity would have
 -			 * dropped the fence as all snoopable access is
 -			 * supposed to be linear.
 -			 */
 -			for_each_ggtt_vma(vma, obj) {
 -				ret = i915_vma_put_fence(vma);
 -				if (ret)
 -					return ret;
 -			}
 -		} else {
 -			/* We either have incoherent backing store and
 -			 * so no GTT access or the architecture is fully
 -			 * coherent. In such cases, existing GTT mmaps
 -			 * ignore the cache bit in the PTE and we can
 -			 * rewrite it without confusing the GPU or having
 -			 * to force userspace to fault back in its mmaps.
 -			 */
 -		}
 -
 -		list_for_each_entry(vma, &obj->vma_list, obj_link) {
 -			if (!drm_mm_node_allocated(&vma->node))
 -				continue;
++		if (intel_engine_is_idle(engine)) {
++			struct i915_request *rq;
+ 
 -			ret = i915_vma_bind(vma, cache_level, PIN_UPDATE);
 -			if (ret)
 -				return ret;
++			rq = i915_request_alloc(engine,
++						dev_priv->kernel_context);
++			if (!IS_ERR(rq))
++				i915_request_add(rq);
+ 		}
+ 	}
+ 
 -	list_for_each_entry(vma, &obj->vma_list, obj_link)
 -		vma->node.color = cache_level;
 -	i915_gem_object_set_cache_coherency(obj, cache_level);
 -	obj->cache_dirty = true; /* Always invalidate stale cachelines */
++	i915_gem_restore_fences(dev_priv);
++}
+ 
 -	return 0;
++void i915_gem_reset_finish_engine(struct intel_engine_cs *engine)
++{
++	engine->reset.finish(engine);
++
++	intel_uncore_forcewake_put(engine->i915, FORCEWAKE_ALL);
+ }
+ 
 -int i915_gem_get_caching_ioctl(struct drm_device *dev, void *data,
 -			       struct drm_file *file)
++void i915_gem_reset_finish(struct drm_i915_private *dev_priv)
+ {
 -	struct drm_i915_gem_caching *args = data;
 -	struct drm_i915_gem_object *obj;
 -	int err = 0;
++	struct intel_engine_cs *engine;
++	enum intel_engine_id id;
+ 
 -	rcu_read_lock();
 -	obj = i915_gem_object_lookup_rcu(file, args->handle);
 -	if (!obj) {
 -		err = -ENOENT;
 -		goto out;
++	lockdep_assert_held(&dev_priv->drm.struct_mutex);
++
++	for_each_engine(engine, dev_priv, id) {
++		engine->hangcheck.active_request = NULL;
++		i915_gem_reset_finish_engine(engine);
+ 	}
++}
+ 
 -	switch (obj->cache_level) {
 -	case I915_CACHE_LLC:
 -	case I915_CACHE_L3_LLC:
 -		args->caching = I915_CACHING_CACHED;
 -		break;
++static void nop_submit_request(struct i915_request *request)
++{
++	unsigned long flags;
+ 
 -	case I915_CACHE_WT:
 -		args->caching = I915_CACHING_DISPLAY;
 -		break;
++	GEM_TRACE("%s fence %llx:%d -> -EIO\n",
++		  request->engine->name,
++		  request->fence.context, request->fence.seqno);
++	dma_fence_set_error(&request->fence, -EIO);
+ 
 -	default:
 -		args->caching = I915_CACHING_NONE;
 -		break;
 -	}
 -out:
 -	rcu_read_unlock();
 -	return err;
++	spin_lock_irqsave(&request->engine->timeline.lock, flags);
++	__i915_request_submit(request);
++	intel_engine_init_global_seqno(request->engine, request->global_seqno);
++	spin_unlock_irqrestore(&request->engine->timeline.lock, flags);
+ }
+ 
 -int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
 -			       struct drm_file *file)
++void i915_gem_set_wedged(struct drm_i915_private *i915)
+ {
 -	struct drm_i915_private *i915 = to_i915(dev);
 -	struct drm_i915_gem_caching *args = data;
 -	struct drm_i915_gem_object *obj;
 -	enum i915_cache_level level;
 -	int ret = 0;
++	struct intel_engine_cs *engine;
++	enum intel_engine_id id;
+ 
 -	switch (args->caching) {
 -	case I915_CACHING_NONE:
 -		level = I915_CACHE_NONE;
 -		break;
 -	case I915_CACHING_CACHED:
 -		/*
 -		 * Due to a HW issue on BXT A stepping, GPU stores via a
 -		 * snooped mapping may leave stale data in a corresponding CPU
 -		 * cacheline, whereas normally such cachelines would get
 -		 * invalidated.
 -		 */
 -		if (!HAS_LLC(i915) && !HAS_SNOOP(i915))
 -			return -ENODEV;
++	GEM_TRACE("start\n");
+ 
 -		level = I915_CACHE_LLC;
 -		break;
 -	case I915_CACHING_DISPLAY:
 -		level = HAS_WT(i915) ? I915_CACHE_WT : I915_CACHE_NONE;
 -		break;
 -	default:
 -		return -EINVAL;
++	if (GEM_SHOW_DEBUG()) {
++		struct drm_printer p = drm_debug_printer(__func__);
++
++		for_each_engine(engine, i915, id)
++			intel_engine_dump(engine, &p, "%s\n", engine->name);
+ 	}
+ 
 -	obj = i915_gem_object_lookup(file, args->handle);
 -	if (!obj)
 -		return -ENOENT;
++	if (test_and_set_bit(I915_WEDGED, &i915->gpu_error.flags))
++		goto out;
+ 
+ 	/*
 -	 * The caching mode of proxy object is handled by its generator, and
 -	 * not allowed to be changed by userspace.
++	 * First, stop submission to hw, but do not yet complete requests by
++	 * rolling the global seqno forward (since this would complete requests
++	 * for which we haven't set the fence error to EIO yet).
+ 	 */
 -	if (i915_gem_object_is_proxy(obj)) {
 -		ret = -ENXIO;
 -		goto out;
 -	}
++	for_each_engine(engine, i915, id)
++		i915_gem_reset_prepare_engine(engine);
+ 
 -	if (obj->cache_level == level)
 -		goto out;
++	/* Even if the GPU reset fails, it should still stop the engines */
++	if (INTEL_GEN(i915) >= 5)
++		intel_gpu_reset(i915, ALL_ENGINES);
+ 
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE,
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   to_rps_client(file));
 -	if (ret)
 -		goto out;
++	for_each_engine(engine, i915, id) {
++		engine->submit_request = nop_submit_request;
++		engine->schedule = NULL;
++	}
++	i915->caps.scheduler = 0;
+ 
 -	ret = i915_mutex_lock_interruptible(dev);
 -	if (ret)
 -		goto out;
++	/*
++	 * Make sure no request can slip through without getting completed by
++	 * either this call here to intel_engine_init_global_seqno, or the one
++	 * in nop_submit_request.
++	 */
++	synchronize_rcu();
+ 
 -	ret = i915_gem_object_set_cache_level(obj, level);
 -	mutex_unlock(&dev->struct_mutex);
++	/* Mark all executing requests as skipped */
++	for_each_engine(engine, i915, id)
++		engine->cancel_requests(engine);
++
++	for_each_engine(engine, i915, id) {
++		i915_gem_reset_finish_engine(engine);
++		intel_engine_wakeup(engine);
++	}
+ 
+ out:
 -	i915_gem_object_put(obj);
 -	return ret;
++	GEM_TRACE("end\n");
++
++	wake_up_all(&i915->gpu_error.reset_queue);
+ }
+ 
 -/*
 - * Prepare buffer for display plane (scanout, cursors, etc). Can be called from
 - * an uninterruptible phase (modesetting) and allows any flushes to be pipelined
 - * (for pageflips). We only flush the caches while preparing the buffer for
 - * display, the callers are responsible for frontbuffer flush.
 - */
 -struct i915_vma *
 -i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
 -				     u32 alignment,
 -				     const struct i915_ggtt_view *view,
 -				     unsigned int flags)
++bool i915_gem_unset_wedged(struct drm_i915_private *i915)
+ {
 -	struct i915_vma *vma;
 -	int ret;
++	struct i915_timeline *tl;
+ 
 -	lockdep_assert_held(&obj->base.dev->struct_mutex);
++	lockdep_assert_held(&i915->drm.struct_mutex);
++	if (!test_bit(I915_WEDGED, &i915->gpu_error.flags))
++		return true;
+ 
 -	/* Mark the global pin early so that we account for the
 -	 * display coherency whilst setting up the cache domains.
 -	 */
 -	obj->pin_global++;
++	GEM_TRACE("start\n");
+ 
 -	/* The display engine is not coherent with the LLC cache on gen6.  As
 -	 * a result, we make sure that the pinning that is about to occur is
 -	 * done with uncached PTEs. This is lowest common denominator for all
 -	 * chipsets.
++	/*
++	 * Before unwedging, make sure that all pending operations
++	 * are flushed and errored out - we may have requests waiting upon
++	 * third party fences. We marked all inflight requests as EIO, and
++	 * every execbuf since returned EIO, for consistency we want all
++	 * the currently pending requests to also be marked as EIO, which
++	 * is done inside our nop_submit_request - and so we must wait.
+ 	 *
 -	 * However for gen6+, we could do better by using the GFDT bit instead
 -	 * of uncaching, which would allow us to flush all the LLC-cached data
 -	 * with that bit in the PTE to main memory with just one PIPE_CONTROL.
 -	 */
 -	ret = i915_gem_object_set_cache_level(obj,
 -					      HAS_WT(to_i915(obj->base.dev)) ?
 -					      I915_CACHE_WT : I915_CACHE_NONE);
 -	if (ret) {
 -		vma = ERR_PTR(ret);
 -		goto err_unpin_global;
 -	}
 -
 -	/* As the user may map the buffer once pinned in the display plane
 -	 * (e.g. libkms for the bootup splash), we have to ensure that we
 -	 * always use map_and_fenceable for all scanout buffers. However,
 -	 * it may simply be too big to fit into mappable, in which case
 -	 * put it anyway and hope that userspace can cope (but always first
 -	 * try to preserve the existing ABI).
 -	 */
 -	vma = ERR_PTR(-ENOSPC);
 -	if ((flags & PIN_MAPPABLE) == 0 &&
 -	    (!view || view->type == I915_GGTT_VIEW_NORMAL))
 -		vma = i915_gem_object_ggtt_pin(obj, view, 0, alignment,
 -					       flags |
 -					       PIN_MAPPABLE |
 -					       PIN_NONBLOCK);
 -	if (IS_ERR(vma))
 -		vma = i915_gem_object_ggtt_pin(obj, view, 0, alignment, flags);
 -	if (IS_ERR(vma))
 -		goto err_unpin_global;
 -
 -	vma->display_alignment = max_t(u64, vma->display_alignment, alignment);
 -
 -	__i915_gem_object_flush_for_display(obj);
 -
 -	/* It should now be out of any other write domains, and we can update
 -	 * the domain values for our changes.
++	 * No more can be submitted until we reset the wedged bit.
+ 	 */
 -	obj->read_domains |= I915_GEM_DOMAIN_GTT;
++	list_for_each_entry(tl, &i915->gt.timelines, link) {
++		struct i915_request *rq;
+ 
 -	return vma;
++		rq = i915_gem_active_peek(&tl->last_request,
++					  &i915->drm.struct_mutex);
++		if (!rq)
++			continue;
+ 
 -err_unpin_global:
 -	obj->pin_global--;
 -	return vma;
 -}
++		/*
++		 * We can't use our normal waiter as we want to
++		 * avoid recursively trying to handle the current
++		 * reset. The basic dma_fence_default_wait() installs
++		 * a callback for dma_fence_signal(), which is
++		 * triggered by our nop handler (indirectly, the
++		 * callback enables the signaler thread which is
++		 * woken by the nop_submit_request() advancing the seqno
++		 * and when the seqno passes the fence, the signaler
++		 * then signals the fence waking us up).
++		 */
++		if (dma_fence_default_wait(&rq->fence, true,
++					   MAX_SCHEDULE_TIMEOUT) < 0)
++			return false;
++	}
++	i915_retire_requests(i915);
++	GEM_BUG_ON(i915->gt.active_requests);
+ 
 -void
 -i915_gem_object_unpin_from_display_plane(struct i915_vma *vma)
 -{
 -	lockdep_assert_held(&vma->vm->i915->drm.struct_mutex);
++	if (!intel_gpu_reset(i915, ALL_ENGINES))
++		intel_engines_sanitize(i915);
+ 
 -	if (WARN_ON(vma->obj->pin_global == 0))
 -		return;
++	/*
++	 * Undo nop_submit_request. We prevent all new i915 requests from
++	 * being queued (by disallowing execbuf whilst wedged) so having
++	 * waited for all active requests above, we know the system is idle
++	 * and do not have to worry about a thread being inside
++	 * engine->submit_request() as we swap over. So unlike installing
++	 * the nop_submit_request on reset, we can do this from normal
++	 * context and do not require stop_machine().
++	 */
++	intel_engines_reset_default_submission(i915);
++	i915_gem_contexts_lost(i915);
+ 
 -	if (--vma->obj->pin_global == 0)
 -		vma->display_alignment = I915_GTT_MIN_ALIGNMENT;
++	GEM_TRACE("end\n");
+ 
 -	/* Bump the LRU to try and avoid premature eviction whilst flipping  */
 -	i915_gem_object_bump_inactive_ggtt(vma->obj);
++	smp_mb__before_atomic(); /* complete takeover before enabling execbuf */
++	clear_bit(I915_WEDGED, &i915->gpu_error.flags);
+ 
 -	i915_vma_unpin(vma);
++	return true;
+ }
+ 
 -/**
 - * Moves a single object to the CPU read, and possibly write domain.
 - * @obj: object to act on
 - * @write: requesting write or read-only access
 - *
 - * This function returns when the move is complete, including waiting on
 - * flushes to occur.
 - */
 -int
 -i915_gem_object_set_to_cpu_domain(struct drm_i915_gem_object *obj, bool write)
++static void
++i915_gem_retire_work_handler(struct work_struct *work)
+ {
 -	int ret;
 -
 -	lockdep_assert_held(&obj->base.dev->struct_mutex);
 -
 -	ret = i915_gem_object_wait(obj,
 -				   I915_WAIT_INTERRUPTIBLE |
 -				   I915_WAIT_LOCKED |
 -				   (write ? I915_WAIT_ALL : 0),
 -				   MAX_SCHEDULE_TIMEOUT,
 -				   NULL);
 -	if (ret)
 -		return ret;
 -
 -	flush_write_domain(obj, ~I915_GEM_DOMAIN_CPU);
++	struct drm_i915_private *dev_priv =
++		container_of(work, typeof(*dev_priv), gt.retire_work.work);
++	struct drm_device *dev = &dev_priv->drm;
+ 
 -	/* Flush the CPU cache if it's still invalid. */
 -	if ((obj->read_domains & I915_GEM_DOMAIN_CPU) == 0) {
 -		i915_gem_clflush_object(obj, I915_CLFLUSH_SYNC);
 -		obj->read_domains |= I915_GEM_DOMAIN_CPU;
++	/* Come back later if the device is busy... */
++	if (mutex_trylock(&dev->struct_mutex)) {
++		i915_retire_requests(dev_priv);
++		mutex_unlock(&dev->struct_mutex);
+ 	}
+ 
 -	/* It should now be out of any other write domains, and we can update
 -	 * the domain values for our changes.
++	/*
++	 * Keep the retire handler running until we are finally idle.
++	 * We do not need to do this test under locking as in the worst-case
++	 * we queue the retire worker once too often.
+ 	 */
 -	GEM_BUG_ON(obj->write_domain & ~I915_GEM_DOMAIN_CPU);
++	if (READ_ONCE(dev_priv->gt.awake))
++		queue_delayed_work(dev_priv->wq,
++				   &dev_priv->gt.retire_work,
++				   round_jiffies_up_relative(HZ));
++}
+ 
 -	/* If we're writing through the CPU, then the GPU read domains will
 -	 * need to be invalidated at next use.
++static void shrink_caches(struct drm_i915_private *i915)
++{
++	/*
++	 * kmem_cache_shrink() discards empty slabs and reorders partially
++	 * filled slabs to prioritise allocating from the mostly full slabs,
++	 * with the aim of reducing fragmentation.
+ 	 */
 -	if (write)
 -		__start_cpu_write(obj);
++	kmem_cache_shrink(i915->priorities);
++	kmem_cache_shrink(i915->dependencies);
++	kmem_cache_shrink(i915->requests);
++	kmem_cache_shrink(i915->luts);
++	kmem_cache_shrink(i915->vmas);
++	kmem_cache_shrink(i915->objects);
++}
+ 
 -	return 0;
++struct sleep_rcu_work {
++	union {
++		struct rcu_head rcu;
++		struct work_struct work;
++	};
++	struct drm_i915_private *i915;
++	unsigned int epoch;
++};
++
++static inline bool
++same_epoch(struct drm_i915_private *i915, unsigned int epoch)
++{
++	/*
++	 * There is a small chance that the epoch wrapped since we started
++	 * sleeping. If we assume that epoch is at least a u32, then it will
++	 * take at least 2^32 * 100ms for it to wrap, or about 326 years.
++	 */
++	return epoch == READ_ONCE(i915->gt.epoch);
+ }
+ 
 -/* Throttle our rendering by waiting until the ring has completed our requests
 - * emitted over 20 msec ago.
 - *
 - * Note that if we were to use the current jiffies each time around the loop,
 - * we wouldn't escape the function with any frames outstanding if the time to
 - * render a frame was over 20ms.
 - *
 - * This should get us reasonable parallelism between CPU and GPU but also
 - * relatively low latency when blocking on a particular request to finish.
 - */
 -static int
 -i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
++static void __sleep_work(struct work_struct *work)
+ {
 -	struct drm_i915_private *dev_priv = to_i915(dev);
 -	struct drm_i915_file_private *file_priv = file->driver_priv;
 -	unsigned long recent_enough = jiffies - DRM_I915_THROTTLE_JIFFIES;
 -	struct i915_request *request, *target = NULL;
 -	long ret;
++	struct sleep_rcu_work *s = container_of(work, typeof(*s), work);
++	struct drm_i915_private *i915 = s->i915;
++	unsigned int epoch = s->epoch;
+ 
 -	/* ABI: return -EIO if already wedged */
 -	if (i915_terminally_wedged(&dev_priv->gpu_error))
 -		return -EIO;
++	kfree(s);
++	if (same_epoch(i915, epoch))
++		shrink_caches(i915);
++}
+ 
 -	spin_lock(&file_priv->mm.lock);
 -	list_for_each_entry(request, &file_priv->mm.request_list, client_link) {
 -		if (time_after_eq(request->emitted_jiffies, recent_enough))
 -			break;
++static void __sleep_rcu(struct rcu_head *rcu)
++{
++	struct sleep_rcu_work *s = container_of(rcu, typeof(*s), rcu);
++	struct drm_i915_private *i915 = s->i915;
+ 
 -		if (target) {
 -			list_del(&target->client_link);
 -			target->file_priv = NULL;
 -		}
++	destroy_rcu_head(&s->rcu);
+ 
 -		target = request;
++	if (same_epoch(i915, s->epoch)) {
++		INIT_WORK(&s->work, __sleep_work);
++		queue_work(i915->wq, &s->work);
++	} else {
++		kfree(s);
+ 	}
 -	if (target)
 -		i915_request_get(target);
 -	spin_unlock(&file_priv->mm.lock);
 -
 -	if (target == NULL)
 -		return 0;
 -
 -	ret = i915_request_wait(target,
 -				I915_WAIT_INTERRUPTIBLE,
 -				MAX_SCHEDULE_TIMEOUT);
 -	i915_request_put(target);
 -
 -	return ret < 0 ? ret : 0;
+ }
+ 
 -struct i915_vma *
 -i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
 -			 const struct i915_ggtt_view *view,
 -			 u64 size,
 -			 u64 alignment,
 -			 u64 flags)
++static inline bool
++new_requests_since_last_retire(const struct drm_i915_private *i915)
+ {
 -	struct drm_i915_private *dev_priv = to_i915(obj->base.dev);
 -	struct i915_address_space *vm = &dev_priv->ggtt.vm;
 -	struct i915_vma *vma;
 -	int ret;
++	return (READ_ONCE(i915->gt.active_requests) ||
++		work_pending(&i915->gt.idle_work.work));
++}
+ 
 -	lockdep_assert_held(&obj->base.dev->struct_mutex);
++static void assert_kernel_context_is_current(struct drm_i915_private *i915)
++{
++	struct intel_engine_cs *engine;
++	enum intel_engine_id id;
+ 
 -	if (flags & PIN_MAPPABLE &&
 -	    (!view || view->type == I915_GGTT_VIEW_NORMAL)) {
 -		/* If the required space is larger than the available
 -		 * aperture, we will not able to find a slot for the
 -		 * object and unbinding the object now will be in
 -		 * vain. Worse, doing so may cause us to ping-pong
 -		 * the object in and out of the Global GTT and
 -		 * waste a lot of cycles under the mutex.
 -		 */
 -		if (obj->base.size > dev_priv->ggtt.mappable_end)
 -			return ERR_PTR(-E2BIG);
++	if (i915_terminally_wedged(&i915->gpu_error))
++		return;
+ 
 -		/* If NONBLOCK is set the caller is optimistically
 -		 * trying to cache the full object within the mappable
 -		 * aperture, and *must* have a fallback in place for
 -		 * situations where we cannot bind the object. We
 -		 * can be a little more lax here and use the fallback
 -		 * more often to avoid costly migrations of ourselves
 -		 * and other objects within the aperture.
 -		 *
 -		 * Half-the-aperture is used as a simple heuristic.
 -		 * More interesting would to do search for a free
 -		 * block prior to making the commitment to unbind.
 -		 * That caters for the self-harm case, and with a
 -		 * little more heuristics (e.g. NOFAULT, NOEVICT)
 -		 * we could try to minimise harm to others.
 -		 */
 -		if (flags & PIN_NONBLOCK &&
 -		    obj->base.size > dev_priv->ggtt.mappable_end / 2)
 -			return ERR_PTR(-ENOSPC);
++	GEM_BUG_ON(i915->gt.active_requests);
++	for_each_engine(engine, i915, id) {
++		GEM_BUG_ON(__i915_gem_active_peek(&engine->timeline.last_request));
++		GEM_BUG_ON(engine->last_retired_context !=
++			   to_intel_context(i915->kernel_context, engine));
+ 	}
++}
+ 
 -	vma = i915_vma_instance(obj, vm, view);
 -	if (unlikely(IS_ERR(vma)))
 -		return vma;
 -
 -	if (i915_vma_misplaced(vma, size, alignment, flags)) {
 -		if (flags & PIN_NONBLOCK) {
 -			if (i915_vma_is_pinned(vma) || i915_vma_is_active(vma))
 -				return ERR_PTR(-ENOSPC);
++static void
++i915_gem_idle_work_handler(struct work_struct *work)
++{
++	struct drm_i915_private *dev_priv =
++		container_of(work, typeof(*dev_priv), gt.idle_work.work);
++	unsigned int epoch = I915_EPOCH_INVALID;
++	bool rearm_hangcheck;
+ 
 -			if (flags & PIN_MAPPABLE &&
 -			    vma->fence_size > dev_priv->ggtt.mappable_end / 2)
 -				return ERR_PTR(-ENOSPC);
 -		}
++	if (!READ_ONCE(dev_priv->gt.awake))
++		return;
+ 
 -		WARN(i915_vma_is_pinned(vma),
 -		     "bo is already pinned in ggtt with incorrect alignment:"
 -		     " offset=%08x, req.alignment=%llx,"
 -		     " req.map_and_fenceable=%d, vma->map_and_fenceable=%d\n",
 -		     i915_ggtt_offset(vma), alignment,
 -		     !!(flags & PIN_MAPPABLE),
 -		     i915_vma_is_map_and_fenceable(vma));
 -		ret = i915_vma_unbind(vma);
 -		if (ret)
 -			return ERR_PTR(ret);
 -	}
++	if (READ_ONCE(dev_priv->gt.active_requests))
++		return;
+ 
 -	ret = i915_vma_pin(vma, size, alignment, flags | PIN_GLOBAL);
 -	if (ret)
 -		return ERR_PTR(ret);
++	/*
++	 * Flush out the last user context, leaving only the pinned
++	 * kernel context resident. When we are idling on the kernel_context,
++	 * no more new requests (with a context switch) are emitted and we
++	 * can finally rest. A consequence is that the idle work handler is
++	 * always called at least twice before idling (and if the system is
++	 * idle that implies a round trip through the retire worker).
++	 */
++	mutex_lock(&dev_priv->drm.struct_mutex);
++	i915_gem_switch_to_kernel_context(dev_priv);
++	mutex_unlock(&dev_priv->drm.struct_mutex);
+ 
 -	return vma;
 -}
++	GEM_TRACE("active_requests=%d (after switch-to-kernel-context)\n",
++		  READ_ONCE(dev_priv->gt.active_requests));
+ 
 -static __always_inline unsigned int __busy_read_flag(unsigned int id)
 -{
 -	/* Note that we could alias engines in the execbuf API, but
 -	 * that would be very unwise as it prevents userspace from
 -	 * fine control over engine selection. Ahem.
 -	 *
 -	 * This should be something like EXEC_MAX_ENGINE instead of
 -	 * I915_NUM_ENGINES.
++	/*
++	 * Wait for last execlists context complete, but bail out in case a
++	 * new request is submitted. As we don't trust the hardware, we
++	 * continue on if the wait times out. This is necessary to allow
++	 * the machine to suspend even if the hardware dies, and we will
++	 * try to recover in resume (after depriving the hardware of power,
++	 * it may be in a better mmod).
+ 	 */
 -	BUILD_BUG_ON(I915_NUM_ENGINES > 16);
 -	return 0x10000 << id;
 -}
++	__wait_for(if (new_requests_since_last_retire(dev_priv)) return,
++		   intel_engines_are_idle(dev_priv),
++		   I915_IDLE_ENGINES_TIMEOUT * 1000,
++		   10, 500);
+ 
 -static __always_inline unsigned int __busy_write_id(unsigned int id)
 -{
 -	/* The uABI guarantees an active writer is also amongst the read
 -	 * engines. This would be true if we accessed the activity tracking
 -	 * under the lock, but as we perform the lookup of the object and
 -	 * its activity locklessly we can not guarantee that the last_write
 -	 * being active implies that we have set the same engine flag from
 -	 * last_read - hence we always set both read and write busy for
 -	 * last_write.
 -	 */
 -	return id | __busy_read_flag(id);
 -}
++	rearm_hangcheck =
++		cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
+ 
 -static __always_inline unsigned int
 -__busy_set_if_active(const struct dma_fence *fence,
 -		     unsigned int (*flag)(unsigned int id))
 -{
 -	struct i915_request *rq;
++	if (!mutex_trylock(&dev_priv->drm.struct_mutex)) {
++		/* Currently busy, come back later */
++		mod_delayed_work(dev_priv->wq,
++				 &dev_priv->gt.idle_work,
++				 msecs_to_jiffies(50));
++		goto out_rearm;
++	}
+ 
 -	/* We have to check the current hw status of the fence as the uABI
 -	 * guarantees forward progress. We could rely on the idle worker
 -	 * to eventually flush us, but to minimise latency just ask the
 -	 * hardware.
 -	 *
 -	 * Note we only report on the status of native fences.
++	/*
++	 * New request retired after this work handler started, extend active
++	 * period until next instance of the work.
+ 	 */
 -	if (!dma_fence_is_i915(fence))
 -		return 0;
++	if (new_requests_since_last_retire(dev_priv))
++		goto out_unlock;
+ 
 -	/* opencode to_request() in order to avoid const warnings */
 -	rq = container_of(fence, struct i915_request, fence);
 -	if (i915_request_completed(rq))
 -		return 0;
++	epoch = __i915_gem_park(dev_priv);
+ 
 -	return flag(rq->engine->uabi_id);
 -}
++	assert_kernel_context_is_current(dev_priv);
+ 
 -static __always_inline unsigned int
 -busy_check_reader(const struct dma_fence *fence)
 -{
 -	return __busy_set_if_active(fence, __busy_read_flag);
 -}
++	rearm_hangcheck = false;
++out_unlock:
++	mutex_unlock(&dev_priv->drm.struct_mutex);
+ 
 -static __always_inline unsigned int
 -busy_check_writer(const struct dma_fence *fence)
 -{
 -	if (!fence)
 -		return 0;
++out_rearm:
++	if (rearm_hangcheck) {
++		GEM_BUG_ON(!dev_priv->gt.awake);
++		i915_queue_hangcheck(dev_priv);
++	}
+ 
 -	return __busy_set_if_active(fence, __busy_write_id);
++	/*
++	 * When we are idle, it is an opportune time to reap our caches.
++	 * However, we have many objects that utilise RCU and the ordered
++	 * i915->wq that this work is executing on. To try and flush any
++	 * pending frees now we are idle, we first wait for an RCU grace
++	 * period, and then queue a task (that will run last on the wq) to
++	 * shrink and re-optimize the caches.
++	 */
++	if (same_epoch(dev_priv, epoch)) {
++		struct sleep_rcu_work *s = kmalloc(sizeof(*s), GFP_KERNEL);
++		if (s) {
++			init_rcu_head(&s->rcu);
++			s->i915 = dev_priv;
++			s->epoch = epoch;
++			call_rcu(&s->rcu, __sleep_rcu);
++		}
++	}
+ }
+ 
 -int
 -i915_gem_busy_ioctl(struct drm_device *dev, void *data,
 -		    struct drm_file *file)
++void i915_gem_close_object(struct drm_gem_object *gem, struct drm_file *file)
+ {
 -	struct drm_i915_gem_busy *args = data;
 -	struct drm_i915_gem_object *obj;
 -	struct reservation_object_list *list;
 -	unsigned int seq;
 -	int err;
++	struct drm_i915_private *i915 = to_i915(gem->dev);
++	struct drm_i915_gem_object *obj = to_intel_bo(gem);
++	struct drm_i915_file_private *fpriv = file->driver_priv;
++	struct i915_lut_handle *lut, *ln;
+ 
 -	err = -ENOENT;
 -	rcu_read_lock();
 -	obj = i915_gem_object_lookup_rcu(file, args->handle);
 -	if (!obj)
 -		goto out;
++	mutex_lock(&i915->drm.struct_mutex);
+ 
 -	/* A discrepancy here is that we do not report the status of
 -	 * non-i915 fences, i.e. even though we may report the object as idle,
 -	 * a call to set-domain may still stall waiting for foreign rendering.
 -	 * This also means that wait-ioctl may report an object as busy,
 -	 * where busy-ioctl considers it idle.
 -	 *
 -	 * We trade the ability to warn of foreign fences to report on which
 -	 * i915 engines are active for the object.
 -	 *
 -	 * Alternatively, we can trade that extra information on read/write
 -	 * activity with
 -	 *	args->busy =
 -	 *		!reservation_object_test_signaled_rcu(obj->resv, true);
 -	 * to report the overall busyness. This is what the wait-ioctl does.
 -	 *
 -	 */
 -retry:
 -	seq = raw_read_seqcount(&obj->resv->seq);
++	list_for_each_entry_safe(lut, ln, &obj->lut_list, obj_link) {
++		struct i915_gem_context *ctx = lut->ctx;
++		struct i915_vma *vma;
++
++		GEM_BUG_ON(ctx->file_priv == ERR_PTR(-EBADF));
++		if (ctx->file_priv != fpriv)
++			continue;
+ 
 -	/* Translate the exclusive fence to the READ *and* WRITE engine */
 -	args->busy = busy_check_writer(rcu_dereference(obj->resv->fence_excl));
++		vma = radix_tree_delete(&ctx->handles_vma, lut->handle);
++		GEM_BUG_ON(vma->obj != obj);
+ 
 -	/* Translate shared fences to READ set of engines */
 -	list = rcu_dereference(obj->resv->fence);
 -	if (list) {
 -		unsigned int shared_count = list->shared_count, i;
++		/* We allow the process to have multiple handles to the same
++		 * vma, in the same fd namespace, by virtue of flink/open.
++		 */
++		GEM_BUG_ON(!vma->open_count);
++		if (!--vma->open_count && !i915_vma_is_ggtt(vma))
++			i915_vma_close(vma);
+ 
 -		for (i = 0; i < shared_count; ++i) {
 -			struct dma_fence *fence =
 -				rcu_dereference(list->shared[i]);
++		list_del(&lut->obj_link);
++		list_del(&lut->ctx_link);
+ 
 -			args->busy |= busy_check_reader(fence);
 -		}
++		kmem_cache_free(i915->luts, lut);
++		__i915_gem_object_release_unless_active(obj);
+ 	}
+ 
 -	if (args->busy && read_seqcount_retry(&obj->resv->seq, seq))
 -		goto retry;
 -
 -	err = 0;
 -out:
 -	rcu_read_unlock();
 -	return err;
++	mutex_unlock(&i915->drm.struct_mutex);
+ }
+ 
 -int
 -i915_gem_throttle_ioctl(struct drm_device *dev, void *data,
 -			struct drm_file *file_priv)
++static unsigned long to_wait_timeout(s64 timeout_ns)
+ {
 -	return i915_gem_ring_throttle(dev, file_priv);
++	if (timeout_ns < 0)
++		return MAX_SCHEDULE_TIMEOUT;
++
++	if (timeout_ns == 0)
++		return 0;
++
++	return nsecs_to_jiffies_timeout(timeout_ns);
+ }
+ 
++/**
++ * i915_gem_wait_ioctl - implements DRM_IOCTL_I915_GEM_WAIT
++ * @dev: drm device pointer
++ * @data: ioctl data blob
++ * @file: drm file pointer
++ *
++ * Returns 0 if successful, else an error is returned with the remaining time in
++ * the timeout parameter.
++ *  -ETIME: object is still busy after timeout
++ *  -ERESTARTSYS: signal interrupted the wait
++ *  -ENONENT: object doesn't exist
++ * Also possible, but rare:
++ *  -EAGAIN: incomplete, restart syscall
++ *  -ENOMEM: damn
++ *  -ENODEV: Internal IRQ fail
++ *  -E?: The add request failed
++ *
++ * The wait ioctl with a timeout of 0 reimplements the busy ioctl. With any
++ * non-zero timeout parameter the wait ioctl will wait for the given number of
++ * nanoseconds on an object becoming unbusy. Since the wait itself does so
++ * without holding struct_mutex the object may become re-busied before this
++ * function completes. A similar but shorter * race condition exists in the busy
++ * ioctl
++ */
+ int
 -i915_gem_madvise_ioctl(struct drm_device *dev, void *data,
 -		       struct drm_file *file_priv)
++i915_gem_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file)
+ {
 -	struct drm_i915_private *dev_priv = to_i915(dev);
 -	struct drm_i915_gem_madvise *args = data;
++	struct drm_i915_gem_wait *args = data;
+ 	struct drm_i915_gem_object *obj;
 -	int err;
++	ktime_t start;
++	long ret;
+ 
 -	switch (args->madv) {
 -	case I915_MADV_DONTNEED:
 -	case I915_MADV_WILLNEED:
 -	    break;
 -	default:
 -	    return -EINVAL;
 -	}
++	if (args->flags != 0)
++		return -EINVAL;
+ 
 -	obj = i915_gem_object_lookup(file_priv, args->handle);
++	obj = i915_gem_object_lookup(file, args->bo_handle);
+ 	if (!obj)
+ 		return -ENOENT;
+ 
 -	err = mutex_lock_interruptible(&obj->mm.lock);
 -	if (err)
 -		goto out;
++	start = ktime_get();
+ 
 -	if (i915_gem_object_has_pages(obj) &&
 -	    i915_gem_object_is_tiled(obj) &&
 -	    dev_priv->quirks & QUIRK_PIN_SWIZZLED_PAGES) {
 -		if (obj->mm.madv == I915_MADV_WILLNEED) {
 -			GEM_BUG_ON(!obj->mm.quirked);
 -			__i915_gem_object_unpin_pages(obj);
 -			obj->mm.quirked = false;
 -		}
 -		if (args->madv == I915_MADV_WILLNEED) {
 -			GEM_BUG_ON(obj->mm.quirked);
 -			__i915_gem_object_pin_pages(obj);
 -			obj->mm.quirked = true;
 -		}
 -	}
++	ret = i915_gem_object_wait(obj,
++				   I915_WAIT_INTERRUPTIBLE |
++				   I915_WAIT_PRIORITY |
++				   I915_WAIT_ALL,
++				   to_wait_timeout(args->timeout_ns),
++				   to_rps_client(file));
+ 
 -	if (obj->mm.madv != __I915_MADV_PURGED)
 -		obj->mm.madv = args->madv;
++	if (args->timeout_ns > 0) {
++		args->timeout_ns -= ktime_to_ns(ktime_sub(ktime_get(), start));
++		if (args->timeout_ns < 0)
++			args->timeout_ns = 0;
+ 
 -	/* if the object is no longer attached, discard its backing storage */
 -	if (obj->mm.madv == I915_MADV_DONTNEED &&
 -	    !i915_gem_object_has_pages(obj))
 -		i915_gem_object_truncate(obj);
++		/*
++		 * Apparently ktime isn't accurate enough and occasionally has a
++		 * bit of mismatch in the jiffies<->nsecs<->ktime loop. So patch
++		 * things up to make the test happy. We allow up to 1 jiffy.
++		 *
++		 * This is a regression from the timespec->ktime conversion.
++		 */
++		if (ret == -ETIME && !nsecs_to_jiffies(args->timeout_ns))
++			args->timeout_ns = 0;
+ 
 -	args->retained = obj->mm.madv != __I915_MADV_PURGED;
 -	mutex_unlock(&obj->mm.lock);
++		/* Asked to wait beyond the jiffie/scheduler precision? */
++		if (ret == -ETIME && args->timeout_ns)
++			ret = -EAGAIN;
++	}
+ 
 -out:
+ 	i915_gem_object_put(obj);
 -	return err;
 -}
 -
 -static void
 -frontbuffer_retire(struct i915_gem_active *active, struct i915_request *request)
 -{
 -	struct drm_i915_gem_object *obj =
 -		container_of(active, typeof(*obj), frontbuffer_write);
 -
 -	intel_fb_obj_flush(obj, ORIGIN_CS);
++	return ret;
+ }
+ 
 -void i915_gem_object_init(struct drm_i915_gem_object *obj,
 -			  const struct drm_i915_gem_object_ops *ops)
++static long wait_for_timeline(struct i915_timeline *tl,
++			      unsigned int flags, long timeout)
+ {
 -	mutex_init(&obj->mm.lock);
 -
 -	INIT_LIST_HEAD(&obj->vma_list);
 -	INIT_LIST_HEAD(&obj->lut_list);
 -	INIT_LIST_HEAD(&obj->batch_pool_link);
 -
 -	init_rcu_head(&obj->rcu);
 -
 -	obj->ops = ops;
 -
 -	reservation_object_init(&obj->__builtin_resv);
 -	obj->resv = &obj->__builtin_resv;
 -
 -	obj->frontbuffer_ggtt_origin = ORIGIN_GTT;
 -	init_request_active(&obj->frontbuffer_write, frontbuffer_retire);
 -
 -	obj->mm.madv = I915_MADV_WILLNEED;
 -	INIT_RADIX_TREE(&obj->mm.get_page.radix, GFP_KERNEL | __GFP_NOWARN);
 -	mutex_init(&obj->mm.get_page.lock);
 -
 -	i915_gem_info_add_obj(to_i915(obj->base.dev), obj->base.size);
 -}
 -
 -static const struct drm_i915_gem_object_ops i915_gem_object_ops = {
 -	.flags = I915_GEM_OBJECT_HAS_STRUCT_PAGE |
 -		 I915_GEM_OBJECT_IS_SHRINKABLE,
 -
 -	.get_pages = i915_gem_object_get_pages_gtt,
 -	.put_pages = i915_gem_object_put_pages_gtt,
 -
 -	.pwrite = i915_gem_object_pwrite_gtt,
 -};
++	struct i915_request *rq;
+ 
 -static int i915_gem_object_create_shmem(struct drm_device *dev,
 -					struct drm_gem_object *obj,
 -					size_t size)
 -{
 -	struct drm_i915_private *i915 = to_i915(dev);
 -	unsigned long flags = VM_NORESERVE;
 -	struct file *filp;
++	rq = i915_gem_active_get_unlocked(&tl->last_request);
++	if (!rq)
++		return timeout;
+ 
 -	drm_gem_private_object_init(dev, obj, size);
++	/*
++	 * "Race-to-idle".
++	 *
++	 * Switching to the kernel context is often used a synchronous
++	 * step prior to idling, e.g. in suspend for flushing all
++	 * current operations to memory before sleeping. These we
++	 * want to complete as quickly as possible to avoid prolonged
++	 * stalls, so allow the gpu to boost to maximum clocks.
++	 */
++	if (flags & I915_WAIT_FOR_IDLE_BOOST)
++		gen6_rps_boost(rq, NULL);
+ 
 -	if (i915->mm.gemfs)
 -		filp = shmem_file_setup_with_mnt(i915->mm.gemfs, "i915", size,
 -						 flags);
 -	else
 -		filp = shmem_file_setup("i915", size, flags);
++	timeout = i915_request_wait(rq, flags, timeout);
++	i915_request_put(rq);
+ 
 -	if (IS_ERR(filp))
 -		return PTR_ERR(filp);
++	return timeout;
++}
+ 
 -	obj->filp = filp;
++static int wait_for_engines(struct drm_i915_private *i915)
++{
++	if (wait_for(intel_engines_are_idle(i915), I915_IDLE_ENGINES_TIMEOUT)) {
++		dev_err(i915->drm.dev,
++			"Failed to idle engines, declaring wedged!\n");
++		GEM_TRACE_DUMP();
++		i915_gem_set_wedged(i915);
++		return -EIO;
++	}
+ 
+ 	return 0;
+ }
+ 
 -struct drm_i915_gem_object *
 -i915_gem_object_create(struct drm_i915_private *dev_priv, u64 size)
++int i915_gem_wait_for_idle(struct drm_i915_private *i915,
++			   unsigned int flags, long timeout)
+ {
 -	struct drm_i915_gem_object *obj;
 -	struct address_space *mapping;
 -	unsigned int cache_level;
 -	gfp_t mask;
 -	int ret;
 -
 -	/* There is a prevalence of the assumption that we fit the object's
 -	 * page count inside a 32bit _signed_ variable. Let's document this and
 -	 * catch if we ever need to fix it. In the meantime, if you do spot
 -	 * such a local variable, please consider fixing!
 -	 */
 -	if (size >> PAGE_SHIFT > INT_MAX)
 -		return ERR_PTR(-E2BIG);
++	GEM_TRACE("flags=%x (%s), timeout=%ld%s\n",
++		  flags, flags & I915_WAIT_LOCKED ? "locked" : "unlocked",
++		  timeout, timeout == MAX_SCHEDULE_TIMEOUT ? " (forever)" : "");
+ 
 -	if (overflows_type(size, obj->base.size))
 -		return ERR_PTR(-E2BIG);
++	/* If the device is asleep, we have no requests outstanding */
++	if (!READ_ONCE(i915->gt.awake))
++		return 0;
+ 
 -	obj = i915_gem_object_alloc(dev_priv);
 -	if (obj == NULL)
 -		return ERR_PTR(-ENOMEM);
++	if (flags & I915_WAIT_LOCKED) {
++		struct i915_timeline *tl;
++		int err;
+ 
 -	ret = i915_gem_object_create_shmem(&dev_priv->drm, &obj->base, size);
 -	if (ret)
 -		goto fail;
++		lockdep_assert_held(&i915->drm.struct_mutex);
+ 
 -	mask = GFP_HIGHUSER | __GFP_RECLAIMABLE;
 -	if (IS_I965GM(dev_priv) || IS_I965G(dev_priv)) {
 -		/* 965gm cannot relocate objects above 4GiB. */
 -		mask &= ~__GFP_HIGHMEM;
 -		mask |= __GFP_DMA32;
 -	}
++		list_for_each_entry(tl, &i915->gt.timelines, link) {
++			timeout = wait_for_timeline(tl, flags, timeout);
++			if (timeout < 0)
++				return timeout;
++		}
++		if (GEM_SHOW_DEBUG() && !timeout) {
++			/* Presume that timeout was non-zero to begin with! */
++			dev_warn(&i915->drm.pdev->dev,
++				 "Missed idle-completion interrupt!\n");
++			GEM_TRACE_DUMP();
++		}
+ 
 -	mapping = obj->base.filp->f_mapping;
 -	mapping_set_gfp_mask(mapping, mask);
 -	GEM_BUG_ON(!(mapping_gfp_mask(mapping) & __GFP_RECLAIM));
 -
 -	i915_gem_object_init(obj, &i915_gem_object_ops);
 -
 -	obj->write_domain = I915_GEM_DOMAIN_CPU;
 -	obj->read_domains = I915_GEM_DOMAIN_CPU;
 -
 -	if (HAS_LLC(dev_priv))
 -		/* On some devices, we can have the GPU use the LLC (the CPU
 -		 * cache) for about a 10% performance improvement
 -		 * compared to uncached.  Graphics requests other than
 -		 * display scanout are coherent with the CPU in
 -		 * accessing this cache.  This means in this mode we
 -		 * don't need to clflush on the CPU side, and on the
 -		 * GPU side we only need to flush internal caches to
 -		 * get data visible to the CPU.
 -		 *
 -		 * However, we maintain the display planes as UC, and so
 -		 * need to rebind when first used as such.
 -		 */
 -		cache_level = I915_CACHE_LLC;
 -	else
 -		cache_level = I915_CACHE_NONE;
++		err = wait_for_engines(i915);
++		if (err)
++			return err;
+ 
 -	i915_gem_object_set_cache_coherency(obj, cache_level);
++		i915_retire_requests(i915);
++		GEM_BUG_ON(i915->gt.active_requests);
++	} else {
++		struct intel_engine_cs *engine;
++		enum intel_engine_id id;
+ 
 -	trace_i915_gem_object_create(obj);
++		for_each_engine(engine, i915, id) {
++			struct i915_timeline *tl = &engine->timeline;
+ 
 -	return obj;
++			timeout = wait_for_timeline(tl, flags, timeout);
++			if (timeout < 0)
++				return timeout;
++		}
++	}
+ 
 -fail:
 -	i915_gem_object_free(obj);
 -	return ERR_PTR(ret);
++	return 0;
+ }
+ 
 -static bool discard_backing_storage(struct drm_i915_gem_object *obj)
++static void __i915_gem_object_flush_for_display(struct drm_i915_gem_object *obj)
+ {
 -	/* If we are the last user of the backing storage (be it shmemfs
 -	 * pages or stolen etc), we know that the pages are going to be
 -	 * immediately released. In this case, we can then skip copying
 -	 * back the contents from the GPU.
++	/*
++	 * We manually flush the CPU domain so that we can override and
++	 * force the flush for the display, and perform it asyncrhonously.
+ 	 */
++	flush_write_domain(obj, ~I915_GEM_DOMAIN_CPU);
++	if (obj->cache_dirty)
++		i915_gem_clflush_object(obj, I915_CLFLUSH_FORCE);
++	obj->write_domain = 0;
++}
+ 
 -	if (obj->mm.madv != I915_MADV_WILLNEED)
 -		return false;
 -
 -	if (obj->base.filp == NULL)
 -		return true;
++void i915_gem_object_flush_if_display(struct drm_i915_gem_object *obj)
++{
++	if (!READ_ONCE(obj->pin_global))
++		return;
+ 
 -	/* At first glance, this looks racy, but then again so would be
 -	 * userspace racing mmap against close. However, the first external
 -	 * reference to the filp can only be obtained through the
 -	 * i915_gem_mmap_ioctl() which safeguards us against the user
 -	 * acquiring such a reference whilst we are in the middle of
 -	 * freeing the object.
 -	 */
 -	return atomic_long_read(&obj->base.filp->f_count) == 1;
++	mutex_lock(&obj->base.dev->struct_mutex);
++	__i915_gem_object_flush_for_display(obj);
++	mutex_unlock(&obj->base.dev->struct_mutex);
+ }
+ 
 -static void __i915_gem_free_objects(struct drm_i915_private *i915,
 -				    struct llist_node *freed)
++/**
++ * Moves a single object to the WC read, and possibly write domain.
++ * @obj: object to act on
++ * @write: ask for write access or read only
++ *
++ * This function returns when the move is complete, including waiting on
++ * flushes to occur.
++ */
++int
++i915_gem_object_set_to_wc_domain(struct drm_i915_gem_object *obj, bool write)
+ {
 -	struct drm_i915_gem_object *obj, *on;
++	int ret;
+ 
 -	intel_runtime_pm_get(i915);
 -	llist_for_each_entry_safe(obj, on, freed, freed) {
 -		struct i915_vma *vma, *vn;
++	lockdep_assert_held(&obj->base.dev->struct_mutex);
+ 
 -		trace_i915_gem_object_destroy(obj);
++	ret = i915_gem_object_wait(obj,
++				   I915_WAIT_INTERRUPTIBLE |
++				   I915_WAIT_LOCKED |
++				   (write ? I915_WAIT_ALL : 0),
++				   MAX_SCHEDULE_TIMEOUT,
++				   NULL);
++	if (ret)
++		return ret;
+ 
 -		mutex_lock(&i915->drm.struct_mutex);
++	if (obj->write_domain == I915_GEM_DOMAIN_WC)
++		return 0;
+ 
 -		GEM_BUG_ON(i915_gem_object_is_active(obj));
 -		list_for_each_entry_safe(vma, vn,
 -					 &obj->vma_list, obj_link) {
 -			GEM_BUG_ON(i915_vma_is_active(vma));
 -			vma->flags &= ~I915_VMA_PIN_MASK;
 -			i915_vma_destroy(vma);
 -		}
 -		GEM_BUG_ON(!list_empty(&obj->vma_list));
 -		GEM_BUG_ON(!RB_EMPTY_ROOT(&obj->vma_tree));
 -
 -		/* This serializes freeing with the shrinker. Since the free
 -		 * is delayed, first by RCU then by the workqueue, we want the
 -		 * shrinker to be able to free pages of unreferenced objects,
 -		 * or else we may oom whilst there are plenty of deferred
 -		 * freed objects.
 -		 */
 -		if (i915_gem_object_has_pages(obj)) {
 -			spin_lock(&i915->mm.obj_lock);
 -			list_del_init(&obj->mm.link);
 -			spin_unlock(&i915->mm.obj_lock);
 -		}
++	/* Flush and acquire obj->pages so that we are coherent through
++	 * direct access in memory with previous cached writes through
++	 * shmemfs and that our cache domain tracking remains valid.
++	 * For example, if the obj->filp was moved to swap without us
++	 * being notified and releasing the pages, we would mistakenly
++	 * continue to assume that the obj remained out of the CPU cached
++	 * domain.
++	 */
++	ret = i915_gem_object_pin_pages(obj);
++	if (ret)
++		return ret;
++
++	flush_write_domain(obj, ~I915_GEM_DOMAIN_WC);
++
++	/* Serialise direct access to this object with the barriers for
++	 * coherent writes from the GPU, by effectively invalidating the
++	 * WC domain upon first access.
++	 */
++	if ((obj->read_domains & I915_GEM_DOMAIN_WC) == 0)
++		mb();
+ 
 -		mutex_unlock(&i915->drm.struct_mutex);
++	/* It should now be out of any other write domains, and we can update
++	 * the domain values for our changes.
++	 */
++	GEM_BUG_ON((obj->write_domain & ~I915_GEM_DOMAIN_WC) != 0);
++	obj->read_domains |= I915_GEM_DOMAIN_WC;
++	if (write) {
++		obj->read_domains = I915_GEM_DOMAIN_WC;
++		obj->write_domain = I915_GEM_DOMAIN_WC;
++		obj->mm.dirty = true;
++	}
+ 
 -		GEM_BUG_ON(obj->bind_count);
 -		GEM_BUG_ON(obj->userfault_count);
 -		GEM_BUG_ON(atomic_read(&obj->frontbuffer_bits));
 -		GEM_BUG_ON(!list_empty(&obj->lut_list));
++	i915_gem_object_unpin_pages(obj);
++	return 0;
++}
+ 
 -		if (obj->ops->release)
 -			obj->ops->release(obj);
++/**
++ * Moves a single object to the GTT read, and possibly write domain.
++ * @obj: object to act on
++ * @write: ask for write access or read only
++ *
++ * This function returns when the move is complete, including waiting on
++ * flushes to occur.
++ */
++int
++i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
++{
++	int ret;
+ 
 -		if (WARN_ON(i915_gem_object_has_pinned_pages(obj)))
 -			atomic_set(&obj->mm.pages_pin_count, 0);
 -		__i915_gem_object_put_pages(obj, I915_MM_NORMAL);
 -		GEM_BUG_ON(i915_gem_object_has_pages(obj));
++	lockdep_assert_held(&obj->base.dev->struct_mutex);
+ 
 -		if (obj->base.import_attach)
 -			drm_prime_gem_destroy(&obj->base, NULL);
++	ret = i915_gem_object_wait(obj,
++				   I915_WAIT_INTERRUPTIBLE |
++				   I915_WAIT_LOCKED |
++				   (write ? I915_WAIT_ALL : 0),
++				   MAX_SCHEDULE_TIMEOUT,
++				   NULL);
++	if (ret)
++		return ret;
+ 
 -		reservation_object_fini(&obj->__builtin_resv);
 -		drm_gem_object_release(&obj->base);
 -		i915_gem_info_remove_obj(i915, obj->base.size);
++	if (obj->write_domain == I915_GEM_DOMAIN_GTT)
++		return 0;
+ 
 -		kfree(obj->bit_17);
 -		i915_gem_object_free(obj);
++	/* Flush and acquire obj->pages so that we are coherent through
++	 * direct access in memory with previous cached writes through
++	 * shmemfs and that our cache domain tracking remains valid.
++	 * For example, if the obj->filp was moved to swap without us
++	 * being notified and releasing the pages, we would mistakenly
++	 * continue to assume that the obj remained out of the CPU cached
++	 * domain.
++	 */
++	ret = i915_gem_object_pin_pages(obj);
++	if (ret)
++		return ret;
+ 
 -		GEM_BUG_ON(!atomic_read(&i915->mm.free_count));
 -		atomic_dec(&i915->mm.free_count);
++	flush_write_domain(obj, ~I915_GEM_DOMAIN_GTT);
+ 
 -		if (on)
 -			cond_resched();
 -	}
 -	intel_runtime_pm_put(i915);
 -}
++	/* Serialise direct access to this object with the barriers for
++	 * coherent writes from the GPU, by effectively invalidating the
++	 * GTT domain upon first access.
++	 */
++	if ((obj->read_domains & I915_GEM_DOMAIN_GTT) == 0)
++		mb();
+ 
 -static void i915_gem_flush_free_objects(struct drm_i915_private *i915)
 -{
 -	struct llist_node *freed;
 -
 -	/* Free the oldest, most stale object to keep the free_list short */
 -	freed = NULL;
 -	if (!llist_empty(&i915->mm.free_list)) { /* quick test for hotpath */
 -		/* Only one consumer of llist_del_first() allowed */
 -		spin_lock(&i915->mm.free_lock);
 -		freed = llist_del_first(&i915->mm.free_list);
 -		spin_unlock(&i915->mm.free_lock);
 -	}
 -	if (unlikely(freed)) {
 -		freed->next = NULL;
 -		__i915_gem_free_objects(i915, freed);
++	/* It should now be out of any other write domains, and we can update
++	 * the domain values for our changes.
++	 */
++	GEM_BUG_ON((obj->write_domain & ~I915_GEM_DOMAIN_GTT) != 0);
++	obj->read_domains |= I915_GEM_DOMAIN_GTT;
++	if (write) {
++		obj->read_domains = I915_GEM_DOMAIN_GTT;
++		obj->write_domain = I915_GEM_DOMAIN_GTT;
++		obj->mm.dirty = true;
+ 	}
++
++	i915_gem_object_unpin_pages(obj);
++	return 0;
+ }
+ 
 -static void __i915_gem_free_work(struct work_struct *work)
++/**
++ * Changes the cache-level of an object across all VMA.
++ * @obj: object to act on
++ * @cache_level: new cache level to set for the object
++ *
++ * After this function returns, the object will be in the new cache-level
++ * across all GTT and the contents of the backing storage will be coherent,
++ * with respect to the new cache-level. In order to keep the backing storage
++ * coherent for all users, we only allow a single cache level to be set
++ * globally on the object and prevent it from being changed whilst the
++ * hardware is reading from the object. That is if the object is currently
++ * on the scanout it will be set to uncached (or equivalent display
++ * cache coherency) and all non-MOCS GPU access will also be uncached so
++ * that all direct access to the scanout remains coherent.
++ */
++int i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
++				    enum i915_cache_level cache_level)
+ {
 -	struct drm_i915_private *i915 =
 -		container_of(work, struct drm_i915_private, mm.free_work);
 -	struct llist_node *freed;
++	struct i915_vma *vma;
++	int ret;
+ 
 -	/*
 -	 * All file-owned VMA should have been released by this point through
 -	 * i915_gem_close_object(), or earlier by i915_gem_context_close().
 -	 * However, the object may also be bound into the global GTT (e.g.
 -	 * older GPUs without per-process support, or for direct access through
 -	 * the GTT either for the user or for scanout). Those VMA still need to
 -	 * unbound now.
 -	 */
++	lockdep_assert_held(&obj->base.dev->struct_mutex);
++
++	if (obj->cache_level == cache_level)
++		return 0;
+ 
 -	spin_lock(&i915->mm.free_lock);
 -	while ((freed = llist_del_all(&i915->mm.free_list))) {
 -		spin_unlock(&i915->mm.free_lock);
++	/* Inspect the list of currently bound VMA and unbind any that would
++	 * be invalid given the new cache-level. This is principally to
++	 * catch the issue of the CS prefetch crossing page boundaries and
++	 * reading an invalid PTE on older architectures.
++	 */
++restart:
++	list_for_each_entry(vma, &obj->vma_list, obj_link) {
++		if (!drm_mm_node_allocated(&vma->node))
++			continue;
+ 
 -		__i915_gem_free_objects(i915, freed);
 -		if (need_resched())
 -			return;
++		if (i915_vma_is_pinned(vma)) {
++			DRM_DEBUG("can not change the cache level of pinned objects\n");
++			return -EBUSY;
++		}
+ 
 -		spin_lock(&i915->mm.free_lock);
 -	}
 -	spin_unlock(&i915->mm.free_lock);
 -}
++		if (!i915_vma_is_closed(vma) &&
++		    i915_gem_valid_gtt_space(vma, cache_level))
++			continue;
+ 
 -static void __i915_gem_free_object_rcu(struct rcu_head *head)
 -{
 -	struct drm_i915_gem_object *obj =
 -		container_of(head, typeof(*obj), rcu);
 -	struct drm_i915_private *i915 = to_i915(obj->base.dev);
++		ret = i915_vma_unbind(vma);
++		if (ret)
++			return ret;
+ 
 -	/*
 -	 * We reuse obj->rcu for the freed list, so we had better not treat
 -	 * it like a rcu_head from this point forwards. And we expect all
 -	 * objects to be freed via this path.
 -	 */
 -	destroy_rcu_head(&obj->rcu);
++		/* As unbinding may affect other elements in the
++		 * obj->vma_list (due to side-effects from retiring
++		 * an active vma), play safe and restart the iterator.
++		 */
++		goto restart;
++	}
+ 
 -	/*
 -	 * Since we require blocking on struct_mutex to unbind the freed
 -	 * object from the GPU before releasing resources back to the
 -	 * system, we can not do that directly from the RCU callback (which may
 -	 * be a softirq context), but must instead then defer that work onto a
 -	 * kthread. We use the RCU callback rather than move the freed object
 -	 * directly onto the work queue so that we can mix between using the
 -	 * worker and performing frees directly from subsequent allocations for
 -	 * crude but effective memory throttling.
++	/* We can reuse the existing drm_mm nodes but need to change the
++	 * cache-level on the PTE. We could simply unbind them all and
++	 * rebind with the correct cache-level on next use. However since
++	 * we already have a valid slot, dma mapping, pages etc, we may as
++	 * rewrite the PTE in the belief that doing so tramples upon less
++	 * state and so involves less work.
+ 	 */
 -	if (llist_add(&obj->freed, &i915->mm.free_list))
 -		queue_work(i915->wq, &i915->mm.free_work);
 -}
++	if (obj->bind_count) {
++		/* Before we change the PTE, the GPU must not be accessing it.
++		 * If we wait upon the object, we know that all the bound
++		 * VMA are no longer active.
++		 */
++		ret = i915_gem_object_wait(obj,
++					   I915_WAIT_INTERRUPTIBLE |
++					   I915_WAIT_LOCKED |
++					   I915_WAIT_ALL,
++					   MAX_SCHEDULE_TIMEOUT,
++					   NULL);
++		if (ret)
++			return ret;
+ 
 -void i915_gem_free_object(struct drm_gem_object *gem_obj)
 -{
 -	struct drm_i915_gem_object *obj = to_intel_bo(gem_obj);
++		if (!HAS_LLC(to_i915(obj->base.dev)) &&
++		    cache_level != I915_CACHE_NONE) {
++			/* Access to snoopable pages through the GTT is
++			 * incoherent and on some machines causes a hard
++			 * lockup. Relinquish the CPU mmaping to force
++			 * userspace to refault in the pages and we can
++			 * then double check if the GTT mapping is still
++			 * valid for that pointer access.
++			 */
++			i915_gem_release_mmap(obj);
+ 
 -	if (obj->mm.quirked)
 -		__i915_gem_object_unpin_pages(obj);
++			/* As we no longer need a fence for GTT access,
++			 * we can relinquish it now (and so prevent having
++			 * to steal a fence from someone else on the next
++			 * fence request). Note GPU activity would have
++			 * dropped the fence as all snoopable access is
++			 * supposed to be linear.
++			 */
++			for_each_ggtt_vma(vma, obj) {
++				ret = i915_vma_put_fence(vma);
++				if (ret)
++					return ret;
++			}
++		} else {
++			/* We either have incoherent backing store and
++			 * so no GTT access or the architecture is fully
++			 * coherent. In such cases, existing GTT mmaps
++			 * ignore the cache bit in the PTE and we can
++			 * rewrite it without confusing the GPU or having
++			 * to force userspace to fault back in its mmaps.
++			 */
++		}
+ 
 -	if (discard_backing_storage(obj))
 -		obj->mm.madv = I915_MADV_DONTNEED;
++		list_for_each_entry(vma, &obj->vma_list, obj_link) {
++			if (!drm_mm_node_allocated(&vma->node))
++				continue;
+ 
 -	/*
 -	 * Before we free the object, make sure any pure RCU-only
 -	 * read-side critical sections are complete, e.g.
 -	 * i915_gem_busy_ioctl(). For the corresponding synchronized
 -	 * lookup see i915_gem_object_lookup_rcu().
 -	 */
 -	atomic_inc(&to_i915(obj->base.dev)->mm.free_count);
 -	call_rcu(&obj->rcu, __i915_gem_free_object_rcu);
 -}
++			ret = i915_vma_bind(vma, cache_level, PIN_UPDATE);
++			if (ret)
++				return ret;
++		}
++	}
+ 
 -void __i915_gem_object_release_unless_active(struct drm_i915_gem_object *obj)
 -{
 -	lockdep_assert_held(&obj->base.dev->struct_mutex);
++	list_for_each_entry(vma, &obj->vma_list, obj_link)
++		vma->node.color = cache_level;
++	i915_gem_object_set_cache_coherency(obj, cache_level);
++	obj->cache_dirty = true; /* Always invalidate stale cachelines */
+ 
 -	if (!i915_gem_object_has_active_reference(obj) &&
 -	    i915_gem_object_is_active(obj))
 -		i915_gem_object_set_active_reference(obj);
 -	else
 -		i915_gem_object_put(obj);
++	return 0;
+ }
+ 
 -void i915_gem_sanitize(struct drm_i915_private *i915)
++int i915_gem_get_caching_ioctl(struct drm_device *dev, void *data,
++			       struct drm_file *file)
+ {
 -	int err;
 -
 -	GEM_TRACE("\n");
 -
 -	mutex_lock(&i915->drm.struct_mutex);
 -
 -	intel_runtime_pm_get(i915);
 -	intel_uncore_forcewake_get(i915, FORCEWAKE_ALL);
++	struct drm_i915_gem_caching *args = data;
++	struct drm_i915_gem_object *obj;
++	int err = 0;
+ 
 -	/*
 -	 * As we have just resumed the machine and woken the device up from
 -	 * deep PCI sleep (presumably D3_cold), assume the HW has been reset
 -	 * back to defaults, recovering from whatever wedged state we left it
 -	 * in and so worth trying to use the device once more.
 -	 */
 -	if (i915_terminally_wedged(&i915->gpu_error))
 -		i915_gem_unset_wedged(i915);
++	rcu_read_lock();
++	obj = i915_gem_object_lookup_rcu(file, args->handle);
++	if (!obj) {
++		err = -ENOENT;
++		goto out;
++	}
+ 
 -	/*
 -	 * If we inherit context state from the BIOS or earlier occupants
 -	 * of the GPU, the GPU may be in an inconsistent state when we
 -	 * try to take over. The only way to remove the earlier state
 -	 * is by resetting. However, resetting on earlier gen is tricky as
 -	 * it may impact the display and we are uncertain about the stability
 -	 * of the reset, so this could be applied to even earlier gen.
 -	 */
 -	err = -ENODEV;
 -	if (INTEL_GEN(i915) >= 5 && intel_has_gpu_reset(i915))
 -		err = WARN_ON(intel_gpu_reset(i915, ALL_ENGINES));
 -	if (!err)
 -		intel_engines_sanitize(i915);
++	switch (obj->cache_level) {
++	case I915_CACHE_LLC:
++	case I915_CACHE_L3_LLC:
++		args->caching = I915_CACHING_CACHED;
++		break;
+ 
 -	intel_uncore_forcewake_put(i915, FORCEWAKE_ALL);
 -	intel_runtime_pm_put(i915);
++	case I915_CACHE_WT:
++		args->caching = I915_CACHING_DISPLAY;
++		break;
+ 
 -	i915_gem_contexts_lost(i915);
 -	mutex_unlock(&i915->drm.struct_mutex);
++	default:
++		args->caching = I915_CACHING_NONE;
++		break;
++	}
++out:
++	rcu_read_unlock();
++	return err;
+ }
+ 
 -int i915_gem_suspend(struct drm_i915_private *i915)
++int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
++			       struct drm_file *file)
+ {
 -	int ret;
++	struct drm_i915_private *i915 = to_i915(dev);
++	struct drm_i915_gem_caching *args = data;
++	struct drm_i915_gem_object *obj;
++	enum i915_cache_level level;
++	int ret = 0;
+ 
 -	GEM_TRACE("\n");
++	switch (args->caching) {
++	case I915_CACHING_NONE:
++		level = I915_CACHE_NONE;
++		break;
++	case I915_CACHING_CACHED:
++		/*
++		 * Due to a HW issue on BXT A stepping, GPU stores via a
++		 * snooped mapping may leave stale data in a corresponding CPU
++		 * cacheline, whereas normally such cachelines would get
++		 * invalidated.
++		 */
++		if (!HAS_LLC(i915) && !HAS_SNOOP(i915))
++			return -ENODEV;
+ 
 -	intel_runtime_pm_get(i915);
 -	intel_suspend_gt_powersave(i915);
++		level = I915_CACHE_LLC;
++		break;
++	case I915_CACHING_DISPLAY:
++		level = HAS_WT(i915) ? I915_CACHE_WT : I915_CACHE_NONE;
++		break;
++	default:
++		return -EINVAL;
++	}
+ 
 -	mutex_lock(&i915->drm.struct_mutex);
++	obj = i915_gem_object_lookup(file, args->handle);
++	if (!obj)
++		return -ENOENT;
+ 
+ 	/*
 -	 * We have to flush all the executing contexts to main memory so
 -	 * that they can saved in the hibernation image. To ensure the last
 -	 * context image is coherent, we have to switch away from it. That
 -	 * leaves the i915->kernel_context still active when
 -	 * we actually suspend, and its image in memory may not match the GPU
 -	 * state. Fortunately, the kernel_context is disposable and we do
 -	 * not rely on its state.
++	 * The caching mode of proxy object is handled by its generator, and
++	 * not allowed to be changed by userspace.
+ 	 */
 -	if (!i915_terminally_wedged(&i915->gpu_error)) {
 -		ret = i915_gem_switch_to_kernel_context(i915);
 -		if (ret)
 -			goto err_unlock;
 -
 -		ret = i915_gem_wait_for_idle(i915,
 -					     I915_WAIT_INTERRUPTIBLE |
 -					     I915_WAIT_LOCKED |
 -					     I915_WAIT_FOR_IDLE_BOOST,
 -					     MAX_SCHEDULE_TIMEOUT);
 -		if (ret && ret != -EIO)
 -			goto err_unlock;
 -
 -		assert_kernel_context_is_current(i915);
++	if (i915_gem_object_is_proxy(obj)) {
++		ret = -ENXIO;
++		goto out;
+ 	}
 -	i915_retire_requests(i915); /* ensure we flush after wedging */
 -
 -	mutex_unlock(&i915->drm.struct_mutex);
+ 
 -	intel_uc_suspend(i915);
 -
 -	cancel_delayed_work_sync(&i915->gpu_error.hangcheck_work);
 -	cancel_delayed_work_sync(&i915->gt.retire_work);
++	if (obj->cache_level == level)
++		goto out;
+ 
 -	/*
 -	 * As the idle_work is rearming if it detects a race, play safe and
 -	 * repeat the flush until it is definitely idle.
 -	 */
 -	drain_delayed_work(&i915->gt.idle_work);
++	ret = i915_gem_object_wait(obj,
++				   I915_WAIT_INTERRUPTIBLE,
++				   MAX_SCHEDULE_TIMEOUT,
++				   to_rps_client(file));
++	if (ret)
++		goto out;
+ 
 -	/*
 -	 * Assert that we successfully flushed all the work and
 -	 * reset the GPU back to its idle, low power state.
 -	 */
 -	WARN_ON(i915->gt.awake);
 -	if (WARN_ON(!intel_engines_are_idle(i915)))
 -		i915_gem_set_wedged(i915); /* no hope, discard everything */
++	ret = i915_mutex_lock_interruptible(dev);
++	if (ret)
++		goto out;
+ 
 -	intel_runtime_pm_put(i915);
 -	return 0;
++	ret = i915_gem_object_set_cache_level(obj, level);
++	mutex_unlock(&dev->struct_mutex);
+ 
 -err_unlock:
 -	mutex_unlock(&i915->drm.struct_mutex);
 -	intel_runtime_pm_put(i915);
++out:
++	i915_gem_object_put(obj);
+ 	return ret;
+ }
+ 
 -void i915_gem_suspend_late(struct drm_i915_private *i915)
++/*
++ * Prepare buffer for display plane (scanout, cursors, etc). Can be called from
++ * an uninterruptible phase (modesetting) and allows any flushes to be pipelined
++ * (for pageflips). We only flush the caches while preparing the buffer for
++ * display, the callers are responsible for frontbuffer flush.
++ */
++struct i915_vma *
++i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
++				     u32 alignment,
++				     const struct i915_ggtt_view *view,
++				     unsigned int flags)
+ {
 -	struct drm_i915_gem_object *obj;
 -	struct list_head *phases[] = {
 -		&i915->mm.unbound_list,
 -		&i915->mm.bound_list,
 -		NULL
 -	}, **phase;
++	struct i915_vma *vma;
++	int ret;
+ 
 -	/*
 -	 * Neither the BIOS, ourselves or any other kernel
 -	 * expects the system to be in execlists mode on startup,
 -	 * so we need to reset the GPU back to legacy mode. And the only
 -	 * known way to disable logical contexts is through a GPU reset.
 -	 *
 -	 * So in order to leave the system in a known default configuration,
 -	 * always reset the GPU upon unload and suspend. Afterwards we then
 -	 * clean up the GEM state tracking, flushing off the requests and
 -	 * leaving the system in a known idle state.
 -	 *
 -	 * Note that is of the upmost importance that the GPU is idle and
 -	 * all stray writes are flushed *before* we dismantle the backing
 -	 * storage for the pinned objects.
 -	 *
 -	 * However, since we are uncertain that resetting the GPU on older
 -	 * machines is a good idea, we don't - just in case it leaves the
 -	 * machine in an unusable condition.
++	lockdep_assert_held(&obj->base.dev->struct_mutex);
++
++	/* Mark the global pin early so that we account for the
++	 * display coherency whilst setting up the cache domains.
+ 	 */
++	obj->pin_global++;
+ 
 -	mutex_lock(&i915->drm.struct_mutex);
 -	for (phase = phases; *phase; phase++) {
 -		list_for_each_entry(obj, *phase, mm.link)
 -			WARN_ON(i915_gem_object_set_to_gtt_domain(obj, false));
++	/* The display engine is not coherent with the LLC cache on gen6.  As
++	 * a result, we make sure that the pinning that is about to occur is
++	 * done with uncached PTEs. This is lowest common denominator for all
++	 * chipsets.
++	 *
++	 * However for gen6+, we could do better by using the GFDT bit instead
++	 * of uncaching, which would allow us to flush all the LLC-cached data
++	 * with that bit in the PTE to main memory with just one PIPE_CONTROL.
++	 */
++	ret = i915_gem_object_set_cache_level(obj,
++					      HAS_WT(to_i915(obj->base.dev)) ?
++					      I915_CACHE_WT : I915_CACHE_NONE);
++	if (ret) {
++		vma = ERR_PTR(ret);
++		goto err_unpin_global;
+ 	}
 -	mutex_unlock(&i915->drm.struct_mutex);
 -
 -	intel_uc_sanitize(i915);
 -	i915_gem_sanitize(i915);
 -}
 -
 -void i915_gem_resume(struct drm_i915_private *i915)
 -{
 -	GEM_TRACE("\n");
 -
 -	WARN_ON(i915->gt.awake);
+ 
 -	mutex_lock(&i915->drm.struct_mutex);
 -	intel_uncore_forcewake_get(i915, FORCEWAKE_ALL);
 -
 -	i915_gem_restore_gtt_mappings(i915);
 -	i915_gem_restore_fences(i915);
 -
 -	/*
 -	 * As we didn't flush the kernel context before suspend, we cannot
 -	 * guarantee that the context image is complete. So let's just reset
 -	 * it and start again.
++	/* As the user may map the buffer once pinned in the display plane
++	 * (e.g. libkms for the bootup splash), we have to ensure that we
++	 * always use map_and_fenceable for all scanout buffers. However,
++	 * it may simply be too big to fit into mappable, in which case
++	 * put it anyway and hope that userspace can cope (but always first
++	 * try to preserve the existing ABI).
+ 	 */
 -	i915->gt.resume(i915);
 -
 -	if (i915_gem_init_hw(i915))
 -		goto err_wedged;
 -
 -	intel_uc_resume(i915);
 -
 -	/* Always reload a context for powersaving. */
 -	if (i915_gem_switch_to_kernel_context(i915))
 -		goto err_wedged;
 -
 -out_unlock:
 -	intel_uncore_forcewake_put(i915, FORCEWAKE_ALL);
 -	mutex_unlock(&i915->drm.struct_mutex);
 -	return;
++	vma = ERR_PTR(-ENOSPC);
++	if ((flags & PIN_MAPPABLE) == 0 &&
++	    (!view || view->type == I915_GGTT_VIEW_NORMAL))
++		vma = i915_gem_object_ggtt_pin(obj, view, 0, alignment,
++					       flags |
++					       PIN_MAPPABLE |
++					       PIN_NONBLOCK);
++	if (IS_ERR(vma))
++		vma = i915_gem_object_ggtt_pin(obj, view, 0, alignment, flags);
++	if (IS_ERR(vma))
++		goto err_unpin_global;
+ 
 -err_wedged:
 -	if (!i915_terminally_wedged(&i915->gpu_error)) {
 -		DRM_ERROR("failed to re-initialize GPU, declaring wedged!\n");
 -		i915_gem_set_wedged(i915);
 -	}
 -	goto out_unlock;
 -}
++	vma->display_alignment = max_t(u64, vma->display_alignment, alignment);
+ 
 -void i915_gem_init_swizzling(struct drm_i915_private *dev_priv)
 -{
 -	if (INTEL_GEN(dev_priv) < 5 ||
 -	    dev_priv->mm.bit_6_swizzle_x == I915_BIT_6_SWIZZLE_NONE)
 -		return;
++	__i915_gem_object_flush_for_display(obj);
+ 
 -	I915_WRITE(DISP_ARB_CTL, I915_READ(DISP_ARB_CTL) |
 -				 DISP_TILE_SURFACE_SWIZZLING);
++	/* It should now be out of any other write domains, and we can update
++	 * the domain values for our changes.
++	 */
++	obj->read_domains |= I915_GEM_DOMAIN_GTT;
+ 
 -	if (IS_GEN5(dev_priv))
 -		return;
++	return vma;
+ 
 -	I915_WRITE(TILECTL, I915_READ(TILECTL) | TILECTL_SWZCTL);
 -	if (IS_GEN6(dev_priv))
 -		I915_WRITE(ARB_MODE, _MASKED_BIT_ENABLE(ARB_MODE_SWIZZLE_SNB));
 -	else if (IS_GEN7(dev_priv))
 -		I915_WRITE(ARB_MODE, _MASKED_BIT_ENABLE(ARB_MODE_SWIZZLE_IVB));
 -	else if (IS_GEN8(dev_priv))
 -		I915_WRITE(GAMTARBMODE, _MASKED_BIT_ENABLE(ARB_MODE_SWIZZLE_BDW));
 -	else
 -		BUG();
++err_unpin_global:
++	obj->pin_global--;
++	return vma;
+ }
+ 
 -static void init_unused_ring(struct drm_i915_private *dev_priv, u32 base)
++void
++i915_gem_object_unpin_from_display_plane(struct i915_vma *vma)
+ {
 -	I915_WRITE(RING_CTL(base), 0);
 -	I915_WRITE(RING_HEAD(base), 0);
 -	I915_WRITE(RING_TAIL(base), 0);
 -	I915_WRITE(RING_START(base), 0);
 -}
++	lockdep_assert_held(&vma->vm->i915->drm.struct_mutex);
+ 
 -static void init_unused_rings(struct drm_i915_private *dev_priv)
 -{
 -	if (IS_I830(dev_priv)) {
 -		init_unused_ring(dev_priv, PRB1_BASE);
 -		init_unused_ring(dev_priv, SRB0_BASE);
 -		init_unused_ring(dev_priv, SRB1_BASE);
 -		init_unused_ring(dev_priv, SRB2_BASE);
 -		init_unused_ring(dev_priv, SRB3_BASE);
 -	} else if (IS_GEN2(dev_priv)) {
 -		init_unused_ring(dev_priv, SRB0_BASE);
 -		init_unused_ring(dev_priv, SRB1_BASE);
 -	} else if (IS_GEN3(dev_priv)) {
 -		init_unused_ring(dev_priv, PRB1_BASE);
 -		init_unused_ring(dev_priv, PRB2_BASE);
 -	}
 -}
++	if (WARN_ON(vma->obj->pin_global == 0))
++		return;
+ 
 -static int __i915_gem_restart_engines(void *data)
 -{
 -	struct drm_i915_private *i915 = data;
 -	struct intel_engine_cs *engine;
 -	enum intel_engine_id id;
 -	int err;
++	if (--vma->obj->pin_global == 0)
++		vma->display_alignment = I915_GTT_MIN_ALIGNMENT;
+ 
 -	for_each_engine(engine, i915, id) {
 -		err = engine->init_hw(engine);
 -		if (err) {
 -			DRM_ERROR("Failed to restart %s (%d)\n",
 -				  engine->name, err);
 -			return err;
 -		}
 -	}
++	/* Bump the LRU to try and avoid premature eviction whilst flipping  */
++	i915_gem_object_bump_inactive_ggtt(vma->obj);
+ 
 -	return 0;
++	i915_vma_unpin(vma);
+ }
+ 
 -int i915_gem_init_hw(struct drm_i915_private *dev_priv)
++/**
++ * Moves a single object to the CPU read, and possibly write domain.
++ * @obj: object to act on
++ * @write: requesting write or read-only access
++ *
++ * This function returns when the move is complete, including waiting on
++ * flushes to occur.
++ */
++int
++i915_gem_object_set_to_cpu_domain(struct drm_i915_gem_object *obj, bool write)
+ {
+ 	int ret;
+ 
 -	dev_priv->gt.last_init_time = ktime_get();
 -
 -	/* Double layer security blanket, see i915_gem_init() */
 -	intel_uncore_forcewake_get(dev_priv, FORCEWAKE_ALL);
 -
 -	if (HAS_EDRAM(dev_priv) && INTEL_GEN(dev_priv) < 9)
 -		I915_WRITE(HSW_IDICR, I915_READ(HSW_IDICR) | IDIHASHMSK(0xf));
 -
 -	if (IS_HASWELL(dev_priv))
 -		I915_WRITE(MI_PREDICATE_RESULT_2, IS_HSW_GT3(dev_priv) ?
 -			   LOWER_SLICE_ENABLED : LOWER_SLICE_DISABLED);
 -
 -	/* Apply the GT workarounds... */
 -	intel_gt_apply_workarounds(dev_priv);
 -	/* ...and determine whether they are sticking. */
 -	intel_gt_verify_workarounds(dev_priv, "init");
 -
 -	i915_gem_init_swizzling(dev_priv);
 -
 -	/*
 -	 * At least 830 can leave some of the unused rings
 -	 * "active" (ie. head != tail) after resume which
 -	 * will prevent c3 entry. Makes sure all unused rings
 -	 * are totally idle.
 -	 */
 -	init_unused_rings(dev_priv);
 -
 -	BUG_ON(!dev_priv->kernel_context);
 -	if (i915_terminally_wedged(&dev_priv->gpu_error)) {
 -		ret = -EIO;
 -		goto out;
 -	}
++	lockdep_assert_held(&obj->base.dev->struct_mutex);
+ 
 -	ret = i915_ppgtt_init_hw(dev_priv);
 -	if (ret) {
 -		DRM_ERROR("Enabling PPGTT failed (%d)\n", ret);
 -		goto out;
 -	}
++	ret = i915_gem_object_wait(obj,
++				   I915_WAIT_INTERRUPTIBLE |
++				   I915_WAIT_LOCKED |
++				   (write ? I915_WAIT_ALL : 0),
++				   MAX_SCHEDULE_TIMEOUT,
++				   NULL);
++	if (ret)
++		return ret;
+ 
 -	ret = intel_wopcm_init_hw(&dev_priv->wopcm);
 -	if (ret) {
 -		DRM_ERROR("Enabling WOPCM failed (%d)\n", ret);
 -		goto out;
 -	}
++	flush_write_domain(obj, ~I915_GEM_DOMAIN_CPU);
+ 
 -	/* We can't enable contexts until all firmware is loaded */
 -	ret = intel_uc_init_hw(dev_priv);
 -	if (ret) {
 -		DRM_ERROR("Enabling uc failed (%d)\n", ret);
 -		goto out;
++	/* Flush the CPU cache if it's still invalid. */
++	if ((obj->read_domains & I915_GEM_DOMAIN_CPU) == 0) {
++		i915_gem_clflush_object(obj, I915_CLFLUSH_SYNC);
++		obj->read_domains |= I915_GEM_DOMAIN_CPU;
+ 	}
+ 
 -	intel_mocs_init_l3cc_table(dev_priv);
 -
 -	/* Only when the HW is re-initialised, can we replay the requests */
 -	ret = __i915_gem_restart_engines(dev_priv);
 -	if (ret)
 -		goto cleanup_uc;
++	/* It should now be out of any other write domains, and we can update
++	 * the domain values for our changes.
++	 */
++	GEM_BUG_ON(obj->write_domain & ~I915_GEM_DOMAIN_CPU);
+ 
 -	intel_uncore_forcewake_put(dev_priv, FORCEWAKE_ALL);
++	/* If we're writing through the CPU, then the GPU read domains will
++	 * need to be invalidated at next use.
++	 */
++	if (write)
++		__start_cpu_write(obj);
+ 
+ 	return 0;
 -
 -cleanup_uc:
 -	intel_uc_fini_hw(dev_priv);
 -out:
 -	intel_uncore_forcewake_put(dev_priv, FORCEWAKE_ALL);
 -
 -	return ret;
+ }
+ 
 -static int __intel_engines_record_defaults(struct drm_i915_private *i915)
++/* Throttle our rendering by waiting until the ring has completed our requests
++ * emitted over 20 msec ago.
++ *
++ * Note that if we were to use the current jiffies each time around the loop,
++ * we wouldn't escape the function with any frames outstanding if the time to
++ * render a frame was over 20ms.
++ *
++ * This should get us reasonable parallelism between CPU and GPU but also
++ * relatively low latency when blocking on a particular request to finish.
++ */
++static int
++i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
+ {
 -	struct i915_gem_context *ctx;
 -	struct intel_engine_cs *engine;
 -	enum intel_engine_id id;
 -	int err;
 -
 -	/*
 -	 * As we reset the gpu during very early sanitisation, the current
 -	 * register state on the GPU should reflect its defaults values.
 -	 * We load a context onto the hw (with restore-inhibit), then switch
 -	 * over to a second context to save that default register state. We
 -	 * can then prime every new context with that state so they all start
 -	 * from the same default HW values.
 -	 */
++	struct drm_i915_private *dev_priv = to_i915(dev);
++	struct drm_i915_file_private *file_priv = file->driver_priv;
++	unsigned long recent_enough = jiffies - DRM_I915_THROTTLE_JIFFIES;
++	struct i915_request *request, *target = NULL;
++	long ret;
+ 
 -	ctx = i915_gem_context_create_kernel(i915, 0);
 -	if (IS_ERR(ctx))
 -		return PTR_ERR(ctx);
++	/* ABI: return -EIO if already wedged */
++	if (i915_terminally_wedged(&dev_priv->gpu_error))
++		return -EIO;
+ 
 -	for_each_engine(engine, i915, id) {
 -		struct i915_request *rq;
++	spin_lock(&file_priv->mm.lock);
++	list_for_each_entry(request, &file_priv->mm.request_list, client_link) {
++		if (time_after_eq(request->emitted_jiffies, recent_enough))
++			break;
+ 
 -		rq = i915_request_alloc(engine, ctx);
 -		if (IS_ERR(rq)) {
 -			err = PTR_ERR(rq);
 -			goto out_ctx;
++		if (target) {
++			list_del(&target->client_link);
++			target->file_priv = NULL;
+ 		}
+ 
 -		err = 0;
 -		if (engine->init_context)
 -			err = engine->init_context(rq);
 -
 -		i915_request_add(rq);
 -		if (err)
 -			goto err_active;
++		target = request;
+ 	}
++	if (target)
++		i915_request_get(target);
++	spin_unlock(&file_priv->mm.lock);
+ 
 -	err = i915_gem_switch_to_kernel_context(i915);
 -	if (err)
 -		goto err_active;
 -
 -	if (i915_gem_wait_for_idle(i915, I915_WAIT_LOCKED, HZ / 5)) {
 -		i915_gem_set_wedged(i915);
 -		err = -EIO; /* Caller will declare us wedged */
 -		goto err_active;
 -	}
++	if (target == NULL)
++		return 0;
+ 
 -	assert_kernel_context_is_current(i915);
++	ret = i915_request_wait(target,
++				I915_WAIT_INTERRUPTIBLE,
++				MAX_SCHEDULE_TIMEOUT);
++	i915_request_put(target);
+ 
 -	/*
 -	 * Immediately park the GPU so that we enable powersaving and
 -	 * treat it as idle. The next time we issue a request, we will
 -	 * unpark and start using the engine->pinned_default_state, otherwise
 -	 * it is in limbo and an early reset may fail.
 -	 */
 -	__i915_gem_park(i915);
++	return ret < 0 ? ret : 0;
++}
+ 
 -	for_each_engine(engine, i915, id) {
 -		struct i915_vma *state;
 -		void *vaddr;
++>>>>>>> ca79b0c211af (mm: convert totalram_pages and totalhigh_pages variables to atomic)
 +struct i915_vma *
 +i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
 +			 const struct i915_ggtt_view *view,
 +			 u64 size,
 +			 u64 alignment,
 +			 u64 flags)
 +{
 +	struct drm_i915_private *i915 = to_i915(obj->base.dev);
 +	struct i915_ggtt *ggtt = &i915->ggtt;
 +	struct i915_vma *vma;
 +	int ret;
  
 -		GEM_BUG_ON(to_intel_context(ctx, engine)->pin_count);
 +	if (i915_gem_object_never_bind_ggtt(obj))
 +		return ERR_PTR(-ENODEV);
  
 -		state = to_intel_context(ctx, engine)->state;
 -		if (!state)
 -			continue;
 +	if (flags & PIN_MAPPABLE &&
 +	    (!view || view->type == I915_GGTT_VIEW_NORMAL)) {
 +		/*
 +		 * If the required space is larger than the available
 +		 * aperture, we will not able to find a slot for the
 +		 * object and unbinding the object now will be in
 +		 * vain. Worse, doing so may cause us to ping-pong
 +		 * the object in and out of the Global GTT and
 +		 * waste a lot of cycles under the mutex.
 +		 */
 +		if (obj->base.size > ggtt->mappable_end)
 +			return ERR_PTR(-E2BIG);
  
  		/*
 -		 * As we will hold a reference to the logical state, it will
 -		 * not be torn down with the context, and importantly the
 -		 * object will hold onto its vma (making it possible for a
 -		 * stray GTT write to corrupt our defaults). Unmap the vma
 -		 * from the GTT to prevent such accidents and reclaim the
 -		 * space.
 +		 * If NONBLOCK is set the caller is optimistically
 +		 * trying to cache the full object within the mappable
 +		 * aperture, and *must* have a fallback in place for
 +		 * situations where we cannot bind the object. We
 +		 * can be a little more lax here and use the fallback
 +		 * more often to avoid costly migrations of ourselves
 +		 * and other objects within the aperture.
 +		 *
 +		 * Half-the-aperture is used as a simple heuristic.
 +		 * More interesting would to do search for a free
 +		 * block prior to making the commitment to unbind.
 +		 * That caters for the self-harm case, and with a
 +		 * little more heuristics (e.g. NOFAULT, NOEVICT)
 +		 * we could try to minimise harm to others.
  		 */
 -		err = i915_vma_unbind(state);
 -		if (err)
 -			goto err_active;
 +		if (flags & PIN_NONBLOCK &&
 +		    obj->base.size > ggtt->mappable_end / 2)
 +			return ERR_PTR(-ENOSPC);
 +	}
  
 -		err = i915_gem_object_set_to_cpu_domain(state->obj, false);
 -		if (err)
 -			goto err_active;
 +	vma = i915_vma_instance(obj, &ggtt->vm, view);
 +	if (IS_ERR(vma))
 +		return vma;
  
 -		engine->default_state = i915_gem_object_get(state->obj);
 +	if (i915_vma_misplaced(vma, size, alignment, flags)) {
 +		if (flags & PIN_NONBLOCK) {
 +			if (i915_vma_is_pinned(vma) || i915_vma_is_active(vma))
 +				return ERR_PTR(-ENOSPC);
  
 -		/* Check we can acquire the image of the context state */
 -		vaddr = i915_gem_object_pin_map(engine->default_state,
 -						I915_MAP_FORCE_WB);
 -		if (IS_ERR(vaddr)) {
 -			err = PTR_ERR(vaddr);
 -			goto err_active;
 +			if (flags & PIN_MAPPABLE &&
 +			    vma->fence_size > ggtt->mappable_end / 2)
 +				return ERR_PTR(-ENOSPC);
  		}
  
 -		i915_gem_object_unpin_map(engine->default_state);
 +		ret = i915_vma_unbind(vma);
 +		if (ret)
 +			return ERR_PTR(ret);
  	}
  
 -	if (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)) {
 -		unsigned int found = intel_engines_has_context_isolation(i915);
 +	if (vma->fence && !i915_gem_object_is_tiled(obj)) {
 +		mutex_lock(&ggtt->vm.mutex);
 +		ret = i915_vma_revoke_fence(vma);
 +		mutex_unlock(&ggtt->vm.mutex);
 +		if (ret)
 +			return ERR_PTR(ret);
 +	}
  
 -		/*
 -		 * Make sure that classes with multiple engine instances all
 -		 * share the same basic configuration.
 -		 */
 -		for_each_engine(engine, i915, id) {
 -			unsigned int bit = BIT(engine->uabi_class);
 -			unsigned int expected = engine->default_state ? bit : 0;
 +	ret = i915_vma_pin(vma, size, alignment, flags | PIN_GLOBAL);
 +	if (ret)
 +		return ERR_PTR(ret);
  
 -			if ((found & bit) != expected) {
 -				DRM_ERROR("mismatching default context state for class %d on engine %s\n",
 -					  engine->uabi_class, engine->name);
 -			}
 -		}
 +	return vma;
 +}
 +
 +int
 +i915_gem_madvise_ioctl(struct drm_device *dev, void *data,
 +		       struct drm_file *file_priv)
 +{
 +	struct drm_i915_private *i915 = to_i915(dev);
 +	struct drm_i915_gem_madvise *args = data;
 +	struct drm_i915_gem_object *obj;
 +	int err;
 +
 +	switch (args->madv) {
 +	case I915_MADV_DONTNEED:
 +	case I915_MADV_WILLNEED:
 +	    break;
 +	default:
 +	    return -EINVAL;
  	}
  
 -out_ctx:
 -	i915_gem_context_set_closed(ctx);
 -	i915_gem_context_put(ctx);
 -	return err;
 +	obj = i915_gem_object_lookup(file_priv, args->handle);
 +	if (!obj)
 +		return -ENOENT;
  
 -err_active:
 -	/*
 -	 * If we have to abandon now, we expect the engines to be idle
 -	 * and ready to be torn-down. First try to flush any remaining
 -	 * request, ensure we are pointing at the kernel context and
 -	 * then remove it.
 -	 */
 -	if (WARN_ON(i915_gem_switch_to_kernel_context(i915)))
 -		goto out_ctx;
 +	err = mutex_lock_interruptible(&obj->mm.lock);
 +	if (err)
 +		goto out;
 +
 +	if (i915_gem_object_has_pages(obj) &&
 +	    i915_gem_object_is_tiled(obj) &&
 +	    i915->quirks & QUIRK_PIN_SWIZZLED_PAGES) {
 +		if (obj->mm.madv == I915_MADV_WILLNEED) {
 +			GEM_BUG_ON(!obj->mm.quirked);
 +			__i915_gem_object_unpin_pages(obj);
 +			obj->mm.quirked = false;
 +		}
 +		if (args->madv == I915_MADV_WILLNEED) {
 +			GEM_BUG_ON(obj->mm.quirked);
 +			__i915_gem_object_pin_pages(obj);
 +			obj->mm.quirked = true;
 +		}
 +	}
 +
 +	if (obj->mm.madv != __I915_MADV_PURGED)
 +		obj->mm.madv = args->madv;
  
 -	if (WARN_ON(i915_gem_wait_for_idle(i915,
 -					   I915_WAIT_LOCKED,
 -					   MAX_SCHEDULE_TIMEOUT)))
 -		goto out_ctx;
 +	if (i915_gem_object_has_pages(obj)) {
 +		struct list_head *list;
  
 -	i915_gem_contexts_lost(i915);
 -	goto out_ctx;
 -}
 +		if (i915_gem_object_is_shrinkable(obj)) {
 +			unsigned long flags;
  
 -static int
 -i915_gem_init_scratch(struct drm_i915_private *i915, unsigned int size)
 -{
 -	struct drm_i915_gem_object *obj;
 -	struct i915_vma *vma;
 -	int ret;
 +			spin_lock_irqsave(&i915->mm.obj_lock, flags);
  
 -	obj = i915_gem_object_create_stolen(i915, size);
 -	if (!obj)
 -		obj = i915_gem_object_create_internal(i915, size);
 -	if (IS_ERR(obj)) {
 -		DRM_ERROR("Failed to allocate scratch page\n");
 -		return PTR_ERR(obj);
 -	}
 +			if (obj->mm.madv != I915_MADV_WILLNEED)
 +				list = &i915->mm.purge_list;
 +			else
 +				list = &i915->mm.shrink_list;
 +			list_move_tail(&obj->mm.link, list);
  
 -	vma = i915_vma_instance(obj, &i915->ggtt.vm, NULL);
 -	if (IS_ERR(vma)) {
 -		ret = PTR_ERR(vma);
 -		goto err_unref;
 +			spin_unlock_irqrestore(&i915->mm.obj_lock, flags);
 +		}
  	}
  
 -	ret = i915_vma_pin(vma, 0, 0, PIN_GLOBAL | PIN_HIGH);
 -	if (ret)
 -		goto err_unref;
 +	/* if the object is no longer attached, discard its backing storage */
 +	if (obj->mm.madv == I915_MADV_DONTNEED &&
 +	    !i915_gem_object_has_pages(obj))
 +		i915_gem_object_truncate(obj);
  
 -	i915->gt.scratch = vma;
 -	return 0;
 +	args->retained = obj->mm.madv != __I915_MADV_PURGED;
 +	mutex_unlock(&obj->mm.lock);
  
 -err_unref:
 +out:
  	i915_gem_object_put(obj);
 -	return ret;
 -}
 -
 -static void i915_gem_fini_scratch(struct drm_i915_private *i915)
 -{
 -	i915_vma_unpin_and_release(&i915->gt.scratch, 0);
 +	return err;
  }
  
  int i915_gem_init(struct drm_i915_private *dev_priv)
diff --cc drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
index b342bef5e7c9,a9ed0ecc94e2..000000000000
--- a/drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
@@@ -1248,8 -1245,8 +1248,11 @@@ static int exercise_mock(struct drm_i91
  				     unsigned long end_time))
  {
  	const u64 limit = totalram_pages() << PAGE_SHIFT;
++<<<<<<< HEAD
 +	struct i915_address_space *vm;
++=======
++>>>>>>> ca79b0c211af (mm: convert totalram_pages and totalhigh_pages variables to atomic)
  	struct i915_gem_context *ctx;
 -	struct i915_hw_ppgtt *ppgtt;
  	IGT_TIMEOUT(end_time);
  	int err;
  
diff --cc drivers/misc/vmw_balloon.c
index 56c6f79a5c5a,e6126a4b95d3..000000000000
--- a/drivers/misc/vmw_balloon.c
+++ b/drivers/misc/vmw_balloon.c
@@@ -373,420 -519,446 +373,424 @@@ static bool vmballoon_check_status(stru
   * algorithm to the one most appropriate for the guest. This command
   * is normally issued after sending "start" command and is part of
   * standard reset sequence.
 - *
 - * Return: zero on success or appropriate error code.
   */
 -static int vmballoon_send_guest_id(struct vmballoon *b)
 +static bool vmballoon_send_guest_id(struct vmballoon *b)
  {
 -	unsigned long status;
 +	unsigned long status, dummy = 0;
  
 -	status = vmballoon_cmd(b, VMW_BALLOON_CMD_GUEST_ID,
 -			       VMW_BALLOON_GUEST_ID, 0);
 +	status = VMWARE_BALLOON_CMD(GUEST_ID, VMW_BALLOON_GUEST_ID, dummy,
 +				dummy);
  
 -	return status == VMW_BALLOON_SUCCESS ? 0 : -EIO;
 -}
 +	STATS_INC(b->stats.guest_type);
  
 -/**
 - * vmballoon_page_order() - return the order of the page
 - * @page_size: the size of the page.
 - *
 - * Return: the allocation order.
 - */
 -static inline
 -unsigned int vmballoon_page_order(enum vmballoon_page_size_type page_size)
 -{
 -	return page_size == VMW_BALLOON_2M_PAGE ? VMW_BALLOON_2M_ORDER : 0;
 +	if (vmballoon_check_status(b, status))
 +		return true;
 +
 +	pr_debug("%s - failed, hv returns %ld\n", __func__, status);
 +	STATS_INC(b->stats.guest_type_fail);
 +	return false;
  }
  
 -/**
 - * vmballoon_page_in_frames() - returns the number of frames in a page.
 - * @page_size: the size of the page.
 - *
 - * Return: the number of 4k frames.
 - */
 -static inline unsigned int
 -vmballoon_page_in_frames(enum vmballoon_page_size_type page_size)
 +static u16 vmballoon_page_size(bool is_2m_page)
  {
 -	return 1 << vmballoon_page_order(page_size);
 +	if (is_2m_page)
 +		return 1 << VMW_BALLOON_2M_SHIFT;
 +
 +	return 1;
  }
  
 -/**
 - * vmballoon_send_get_target() - Retrieve desired balloon size from the host.
 - *
 - * @b: pointer to the balloon.
 - *
 - * Return: zero on success, EINVAL if limit does not fit in 32-bit, as required
 - * by the host-guest protocol and EIO if an error occurred in communicating with
 - * the host.
 +/*
 + * Retrieve desired balloon size from the host.
   */
 -static int vmballoon_send_get_target(struct vmballoon *b)
 +static bool vmballoon_send_get_target(struct vmballoon *b, u32 *new_target)
  {
  	unsigned long status;
 +	unsigned long target;
  	unsigned long limit;
 +	unsigned long dummy = 0;
 +	u32 limit32;
  
++<<<<<<< HEAD
 +	/*
 +	 * si_meminfo() is cheap. Moreover, we want to provide dynamic
 +	 * max balloon size later. So let us call si_meminfo() every
 +	 * iteration.
 +	 */
 +	si_meminfo(&b->sysinfo);
 +	limit = b->sysinfo.totalram;
++=======
+ 	limit = totalram_pages();
++>>>>>>> ca79b0c211af (mm: convert totalram_pages and totalhigh_pages variables to atomic)
  
  	/* Ensure limit fits in 32-bits */
 -	if (limit != (u32)limit)
 -		return -EINVAL;
 +	limit32 = (u32)limit;
 +	if (limit != limit32)
 +		return false;
  
 -	status = vmballoon_cmd(b, VMW_BALLOON_CMD_GET_TARGET, limit, 0);
 +	/* update stats */
 +	STATS_INC(b->stats.target);
  
 -	return status == VMW_BALLOON_SUCCESS ? 0 : -EIO;
 +	status = VMWARE_BALLOON_CMD(GET_TARGET, limit, dummy, target);
 +	if (vmballoon_check_status(b, status)) {
 +		*new_target = target;
 +		return true;
 +	}
 +
 +	pr_debug("%s - failed, hv returns %ld\n", __func__, status);
 +	STATS_INC(b->stats.target_fail);
 +	return false;
  }
  
 -/**
 - * vmballoon_alloc_page_list - allocates a list of pages.
 - *
 - * @b: pointer to the balloon.
 - * @ctl: pointer for the %struct vmballoon_ctl, which defines the operation.
 - * @req_n_pages: the number of requested pages.
 - *
 - * Tries to allocate @req_n_pages. Add them to the list of balloon pages in
 - * @ctl.pages and updates @ctl.n_pages to reflect the number of pages.
 - *
 - * Return: zero on success or error code otherwise.
 +/*
 + * Notify the host about allocated page so that host can use it without
 + * fear that guest will need it. Host may reject some pages, we need to
 + * check the return value and maybe submit a different page.
   */
 -static int vmballoon_alloc_page_list(struct vmballoon *b,
 -				     struct vmballoon_ctl *ctl,
 -				     unsigned int req_n_pages)
 +static int vmballoon_send_lock_page(struct vmballoon *b, unsigned long pfn,
 +				unsigned int *hv_status, unsigned int *target)
  {
 -	struct page *page;
 -	unsigned int i;
 -
 -	for (i = 0; i < req_n_pages; i++) {
 -		if (ctl->page_size == VMW_BALLOON_2M_PAGE)
 -			page = alloc_pages(VMW_HUGE_PAGE_ALLOC_FLAGS,
 -					   VMW_BALLOON_2M_ORDER);
 -		else
 -			page = alloc_page(VMW_PAGE_ALLOC_FLAGS);
 -
 -		/* Update statistics */
 -		vmballoon_stats_page_inc(b, VMW_BALLOON_PAGE_STAT_ALLOC,
 -					 ctl->page_size);
 +	unsigned long status, dummy = 0;
 +	u32 pfn32;
  
 -		if (page) {
 -			/* Success. Add the page to the list and continue. */
 -			list_add(&page->lru, &ctl->pages);
 -			continue;
 -		}
 +	pfn32 = (u32)pfn;
 +	if (pfn32 != pfn)
 +		return -1;
  
 -		/* Allocation failed. Update statistics and stop. */
 -		vmballoon_stats_page_inc(b, VMW_BALLOON_PAGE_STAT_ALLOC_FAIL,
 -					 ctl->page_size);
 -		break;
 -	}
 +	STATS_INC(b->stats.lock[false]);
  
 -	ctl->n_pages = i;
 +	*hv_status = status = VMWARE_BALLOON_CMD(LOCK, pfn, dummy, *target);
 +	if (vmballoon_check_status(b, status))
 +		return 0;
  
 -	return req_n_pages == ctl->n_pages ? 0 : -ENOMEM;
 +	pr_debug("%s - ppn %lx, hv returns %ld\n", __func__, pfn, status);
 +	STATS_INC(b->stats.lock_fail[false]);
 +	return 1;
  }
  
 -/**
 - * vmballoon_handle_one_result - Handle lock/unlock result for a single page.
 - *
 - * @b: pointer for %struct vmballoon.
 - * @page: pointer for the page whose result should be handled.
 - * @page_size: size of the page.
 - * @status: status of the operation as provided by the hypervisor.
 - */
 -static int vmballoon_handle_one_result(struct vmballoon *b, struct page *page,
 -				       enum vmballoon_page_size_type page_size,
 -				       unsigned long status)
 +static int vmballoon_send_batched_lock(struct vmballoon *b,
 +		unsigned int num_pages, bool is_2m_pages, unsigned int *target)
  {
 -	/* On success do nothing. The page is already on the balloon list. */
 -	if (likely(status == VMW_BALLOON_SUCCESS))
 -		return 0;
 +	unsigned long status;
 +	unsigned long pfn = PHYS_PFN(virt_to_phys(b->batch_page));
  
 -	pr_debug("%s: failed comm pfn %lx status %lu page_size %s\n", __func__,
 -		 page_to_pfn(page), status,
 -		 vmballoon_page_size_names[page_size]);
 +	STATS_INC(b->stats.lock[is_2m_pages]);
  
 -	/* Error occurred */
 -	vmballoon_stats_page_inc(b, VMW_BALLOON_PAGE_STAT_REFUSED_ALLOC,
 -				 page_size);
 +	if (is_2m_pages)
 +		status = VMWARE_BALLOON_CMD(BATCHED_2M_LOCK, pfn, num_pages,
 +				*target);
 +	else
 +		status = VMWARE_BALLOON_CMD(BATCHED_LOCK, pfn, num_pages,
 +				*target);
  
 -	return -EIO;
 +	if (vmballoon_check_status(b, status))
 +		return 0;
 +
 +	pr_debug("%s - batch ppn %lx, hv returns %ld\n", __func__, pfn, status);
 +	STATS_INC(b->stats.lock_fail[is_2m_pages]);
 +	return 1;
  }
  
 -/**
 - * vmballoon_status_page - returns the status of (un)lock operation
 - *
 - * @b: pointer to the balloon.
 - * @idx: index for the page for which the operation is performed.
 - * @p: pointer to where the page struct is returned.
 - *
 - * Following a lock or unlock operation, returns the status of the operation for
 - * an individual page. Provides the page that the operation was performed on on
 - * the @page argument.
 - *
 - * Returns: The status of a lock or unlock operation for an individual page.
 +/*
 + * Notify the host that guest intends to release given page back into
 + * the pool of available (to the guest) pages.
   */
 -static unsigned long vmballoon_status_page(struct vmballoon *b, int idx,
 -					   struct page **p)
 +static bool vmballoon_send_unlock_page(struct vmballoon *b, unsigned long pfn,
 +							unsigned int *target)
  {
 -	if (static_branch_likely(&vmw_balloon_batching)) {
 -		/* batching mode */
 -		*p = pfn_to_page(b->batch_page[idx].pfn);
 -		return b->batch_page[idx].status;
 -	}
 +	unsigned long status, dummy = 0;
 +	u32 pfn32;
  
 -	/* non-batching mode */
 -	*p = b->page;
 +	pfn32 = (u32)pfn;
 +	if (pfn32 != pfn)
 +		return false;
  
 -	/*
 -	 * If a failure occurs, the indication will be provided in the status
 -	 * of the entire operation, which is considered before the individual
 -	 * page status. So for non-batching mode, the indication is always of
 -	 * success.
 -	 */
 -	return VMW_BALLOON_SUCCESS;
 +	STATS_INC(b->stats.unlock[false]);
 +
 +	status = VMWARE_BALLOON_CMD(UNLOCK, pfn, dummy, *target);
 +	if (vmballoon_check_status(b, status))
 +		return true;
 +
 +	pr_debug("%s - ppn %lx, hv returns %ld\n", __func__, pfn, status);
 +	STATS_INC(b->stats.unlock_fail[false]);
 +	return false;
  }
  
 -/**
 - * vmballoon_lock_op - notifies the host about inflated/deflated pages.
 - * @b: pointer to the balloon.
 - * @num_pages: number of inflated/deflated pages.
 - * @page_size: size of the page.
 - * @op: the type of operation (lock or unlock).
 - *
 - * Notify the host about page(s) that were ballooned (or removed from the
 - * balloon) so that host can use it without fear that guest will need it (or
 - * stop using them since the VM does). Host may reject some pages, we need to
 - * check the return value and maybe submit a different page. The pages that are
 - * inflated/deflated are pointed by @b->page.
 - *
 - * Return: result as provided by the hypervisor.
 - */
 -static unsigned long vmballoon_lock_op(struct vmballoon *b,
 -				       unsigned int num_pages,
 -				       enum vmballoon_page_size_type page_size,
 -				       enum vmballoon_op op)
 +static bool vmballoon_send_batched_unlock(struct vmballoon *b,
 +		unsigned int num_pages, bool is_2m_pages, unsigned int *target)
  {
 -	unsigned long cmd, pfn;
 -
 -	lockdep_assert_held(&b->comm_lock);
 +	unsigned long status;
 +	unsigned long pfn = PHYS_PFN(virt_to_phys(b->batch_page));
  
 -	if (static_branch_likely(&vmw_balloon_batching)) {
 -		if (op == VMW_BALLOON_INFLATE)
 -			cmd = page_size == VMW_BALLOON_2M_PAGE ?
 -				VMW_BALLOON_CMD_BATCHED_2M_LOCK :
 -				VMW_BALLOON_CMD_BATCHED_LOCK;
 -		else
 -			cmd = page_size == VMW_BALLOON_2M_PAGE ?
 -				VMW_BALLOON_CMD_BATCHED_2M_UNLOCK :
 -				VMW_BALLOON_CMD_BATCHED_UNLOCK;
 +	STATS_INC(b->stats.unlock[is_2m_pages]);
  
 -		pfn = PHYS_PFN(virt_to_phys(b->batch_page));
 -	} else {
 -		cmd = op == VMW_BALLOON_INFLATE ? VMW_BALLOON_CMD_LOCK :
 -						  VMW_BALLOON_CMD_UNLOCK;
 -		pfn = page_to_pfn(b->page);
 +	if (is_2m_pages)
 +		status = VMWARE_BALLOON_CMD(BATCHED_2M_UNLOCK, pfn, num_pages,
 +				*target);
 +	else
 +		status = VMWARE_BALLOON_CMD(BATCHED_UNLOCK, pfn, num_pages,
 +				*target);
  
 -		/* In non-batching mode, PFNs must fit in 32-bit */
 -		if (unlikely(pfn != (u32)pfn))
 -			return VMW_BALLOON_ERROR_PPN_INVALID;
 -	}
 +	if (vmballoon_check_status(b, status))
 +		return true;
  
 -	return vmballoon_cmd(b, cmd, pfn, num_pages);
 +	pr_debug("%s - batch ppn %lx, hv returns %ld\n", __func__, pfn, status);
 +	STATS_INC(b->stats.unlock_fail[is_2m_pages]);
 +	return false;
  }
  
 -/**
 - * vmballoon_add_page - adds a page towards lock/unlock operation.
 - *
 - * @b: pointer to the balloon.
 - * @idx: index of the page to be ballooned in this batch.
 - * @p: pointer to the page that is about to be ballooned.
 - *
 - * Adds the page to be ballooned. Must be called while holding @comm_lock.
 - */
 -static void vmballoon_add_page(struct vmballoon *b, unsigned int idx,
 -			       struct page *p)
 +static struct page *vmballoon_alloc_page(gfp_t flags, bool is_2m_page)
  {
 -	lockdep_assert_held(&b->comm_lock);
 +	if (is_2m_page)
 +		return alloc_pages(flags, VMW_BALLOON_2M_SHIFT);
  
 -	if (static_branch_likely(&vmw_balloon_batching))
 -		b->batch_page[idx] = (struct vmballoon_batch_entry)
 -					{ .pfn = page_to_pfn(p) };
 +	return alloc_page(flags);
 +}
 +
 +static void vmballoon_free_page(struct page *page, bool is_2m_page)
 +{
 +	if (is_2m_page)
 +		__free_pages(page, VMW_BALLOON_2M_SHIFT);
  	else
 -		b->page = p;
 +		__free_page(page);
  }
  
 -/**
 - * vmballoon_lock - lock or unlock a batch of pages.
 - *
 - * @b: pointer to the balloon.
 - * @ctl: pointer for the %struct vmballoon_ctl, which defines the operation.
 - *
 - * Notifies the host of about ballooned pages (after inflation or deflation,
 - * according to @ctl). If the host rejects the page put it on the
 - * @ctl refuse list. These refused page are then released when moving to the
 - * next size of pages.
 - *
 - * Note that we neither free any @page here nor put them back on the ballooned
 - * pages list. Instead we queue it for later processing. We do that for several
 - * reasons. First, we do not want to free the page under the lock. Second, it
 - * allows us to unify the handling of lock and unlock. In the inflate case, the
 - * caller will check if there are too many refused pages and release them.
 - * Although it is not identical to the past behavior, it should not affect
 - * performance.
 +/*
 + * Quickly release all pages allocated for the balloon. This function is
 + * called when host decides to "reset" balloon for one reason or another.
 + * Unlike normal "deflate" we do not (shall not) notify host of the pages
 + * being released.
   */
 -static int vmballoon_lock(struct vmballoon *b, struct vmballoon_ctl *ctl)
 +static void vmballoon_pop(struct vmballoon *b)
  {
 -	unsigned long batch_status;
 -	struct page *page;
 -	unsigned int i, num_pages;
 -
 -	num_pages = ctl->n_pages;
 -	if (num_pages == 0)
 -		return 0;
 -
 -	/* communication with the host is done under the communication lock */
 -	spin_lock(&b->comm_lock);
 -
 -	i = 0;
 -	list_for_each_entry(page, &ctl->pages, lru)
 -		vmballoon_add_page(b, i++, page);
 +	struct page *page, *next;
 +	unsigned is_2m_pages;
 +
 +	for (is_2m_pages = 0; is_2m_pages < VMW_BALLOON_NUM_PAGE_SIZES;
 +			is_2m_pages++) {
 +		struct vmballoon_page_size *page_size =
 +				&b->page_sizes[is_2m_pages];
 +		u16 size_per_page = vmballoon_page_size(is_2m_pages);
 +
 +		list_for_each_entry_safe(page, next, &page_size->pages, lru) {
 +			list_del(&page->lru);
 +			vmballoon_free_page(page, is_2m_pages);
 +			STATS_INC(b->stats.free[is_2m_pages]);
 +			b->size -= size_per_page;
 +			cond_resched();
 +		}
 +	}
  
 -	batch_status = vmballoon_lock_op(b, ctl->n_pages, ctl->page_size,
 -					 ctl->op);
 +	/* Clearing the batch_page unconditionally has no adverse effect */
 +	free_page((unsigned long)b->batch_page);
 +	b->batch_page = NULL;
 +}
  
 -	/*
 -	 * Iterate over the pages in the provided list. Since we are changing
 -	 * @ctl->n_pages we are saving the original value in @num_pages and
 -	 * use this value to bound the loop.
 -	 */
 -	for (i = 0; i < num_pages; i++) {
 -		unsigned long status;
 +/*
 + * Notify the host of a ballooned page. If host rejects the page put it on the
 + * refuse list, those refused page are then released at the end of the
 + * inflation cycle.
 + */
 +static int vmballoon_lock_page(struct vmballoon *b, unsigned int num_pages,
 +				bool is_2m_pages, unsigned int *target)
 +{
 +	int locked, hv_status;
 +	struct page *page = b->page;
 +	struct vmballoon_page_size *page_size = &b->page_sizes[false];
  
 -		status = vmballoon_status_page(b, i, &page);
 +	/* is_2m_pages can never happen as 2m pages support implies batching */
  
 -		/*
 -		 * Failure of the whole batch overrides a single operation
 -		 * results.
 -		 */
 -		if (batch_status != VMW_BALLOON_SUCCESS)
 -			status = batch_status;
 +	locked = vmballoon_send_lock_page(b, page_to_pfn(page), &hv_status,
 +								target);
 +	if (locked > 0) {
 +		STATS_INC(b->stats.refused_alloc[false]);
  
 -		/* Continue if no error happened */
 -		if (!vmballoon_handle_one_result(b, page, ctl->page_size,
 -						 status))
 -			continue;
 +		if (hv_status == VMW_BALLOON_ERROR_RESET ||
 +				hv_status == VMW_BALLOON_ERROR_PPN_NOTNEEDED) {
 +			vmballoon_free_page(page, false);
 +			return -EIO;
 +		}
  
  		/*
 -		 * Error happened. Move the pages to the refused list and update
 -		 * the pages number.
 +		 * Place page on the list of non-balloonable pages
 +		 * and retry allocation, unless we already accumulated
 +		 * too many of them, in which case take a breather.
  		 */
 -		list_move(&page->lru, &ctl->refused_pages);
 -		ctl->n_pages--;
 -		ctl->n_refused_pages++;
 +		if (page_size->n_refused_pages < VMW_BALLOON_MAX_REFUSED) {
 +			page_size->n_refused_pages++;
 +			list_add(&page->lru, &page_size->refused_pages);
 +		} else {
 +			vmballoon_free_page(page, false);
 +		}
 +		return -EIO;
  	}
  
 -	spin_unlock(&b->comm_lock);
 +	/* track allocated page */
 +	list_add(&page->lru, &page_size->pages);
  
 -	return batch_status == VMW_BALLOON_SUCCESS ? 0 : -EIO;
 +	/* update balloon size */
 +	b->size++;
 +
 +	return 0;
  }
  
 -/**
 - * vmballoon_release_page_list() - Releases a page list
 - *
 - * @page_list: list of pages to release.
 - * @n_pages: pointer to the number of pages.
 - * @page_size: whether the pages in the list are 2MB (or else 4KB).
 - *
 - * Releases the list of pages and zeros the number of pages.
 - */
 -static void vmballoon_release_page_list(struct list_head *page_list,
 -				       int *n_pages,
 -				       enum vmballoon_page_size_type page_size)
 +static int vmballoon_lock_batched_page(struct vmballoon *b,
 +		unsigned int num_pages, bool is_2m_pages, unsigned int *target)
  {
 -	struct page *page, *tmp;
 +	int locked, i;
 +	u16 size_per_page = vmballoon_page_size(is_2m_pages);
  
 -	list_for_each_entry_safe(page, tmp, page_list, lru) {
 -		list_del(&page->lru);
 -		__free_pages(page, vmballoon_page_order(page_size));
 +	locked = vmballoon_send_batched_lock(b, num_pages, is_2m_pages,
 +			target);
 +	if (locked > 0) {
 +		for (i = 0; i < num_pages; i++) {
 +			u64 pa = vmballoon_batch_get_pa(b->batch_page, i);
 +			struct page *p = pfn_to_page(pa >> PAGE_SHIFT);
 +
 +			vmballoon_free_page(p, is_2m_pages);
 +		}
 +
 +		return -EIO;
  	}
  
 -	*n_pages = 0;
 -}
 +	for (i = 0; i < num_pages; i++) {
 +		u64 pa = vmballoon_batch_get_pa(b->batch_page, i);
 +		struct page *p = pfn_to_page(pa >> PAGE_SHIFT);
 +		struct vmballoon_page_size *page_size =
 +				&b->page_sizes[is_2m_pages];
  
 +		locked = vmballoon_batch_get_status(b->batch_page, i);
  
 -/*
 - * Release pages that were allocated while attempting to inflate the
 - * balloon but were refused by the host for one reason or another.
 - */
 -static void vmballoon_release_refused_pages(struct vmballoon *b,
 -					    struct vmballoon_ctl *ctl)
 -{
 -	vmballoon_stats_page_inc(b, VMW_BALLOON_PAGE_STAT_REFUSED_FREE,
 -				 ctl->page_size);
 +		switch (locked) {
 +		case VMW_BALLOON_SUCCESS:
 +			list_add(&p->lru, &page_size->pages);
 +			b->size += size_per_page;
 +			break;
 +		case VMW_BALLOON_ERROR_PPN_PINNED:
 +		case VMW_BALLOON_ERROR_PPN_INVALID:
 +			if (page_size->n_refused_pages
 +					< VMW_BALLOON_MAX_REFUSED) {
 +				list_add(&p->lru, &page_size->refused_pages);
 +				page_size->n_refused_pages++;
 +				break;
 +			}
 +			/* Fallthrough */
 +		case VMW_BALLOON_ERROR_RESET:
 +		case VMW_BALLOON_ERROR_PPN_NOTNEEDED:
 +			vmballoon_free_page(p, is_2m_pages);
 +			break;
 +		default:
 +			/* This should never happen */
 +			WARN_ON_ONCE(true);
 +		}
 +	}
  
 -	vmballoon_release_page_list(&ctl->refused_pages, &ctl->n_refused_pages,
 -				    ctl->page_size);
 +	return 0;
  }
  
 -/**
 - * vmballoon_change - retrieve the required balloon change
 - *
 - * @b: pointer for the balloon.
 - *
 - * Return: the required change for the balloon size. A positive number
 - * indicates inflation, a negative number indicates a deflation.
 +/*
 + * Release the page allocated for the balloon. Note that we first notify
 + * the host so it can make sure the page will be available for the guest
 + * to use, if needed.
   */
 -static int64_t vmballoon_change(struct vmballoon *b)
 +static int vmballoon_unlock_page(struct vmballoon *b, unsigned int num_pages,
 +		bool is_2m_pages, unsigned int *target)
  {
 -	int64_t size, target;
 +	struct page *page = b->page;
 +	struct vmballoon_page_size *page_size = &b->page_sizes[false];
  
 -	size = atomic64_read(&b->size);
 -	target = READ_ONCE(b->target);
 +	/* is_2m_pages can never happen as 2m pages support implies batching */
  
 -	/*
 -	 * We must cast first because of int sizes
 -	 * Otherwise we might get huge positives instead of negatives
 -	 */
 +	if (!vmballoon_send_unlock_page(b, page_to_pfn(page), target)) {
 +		list_add(&page->lru, &page_size->pages);
 +		return -EIO;
 +	}
  
 -	if (b->reset_required)
 -		return 0;
 +	/* deallocate page */
 +	vmballoon_free_page(page, false);
 +	STATS_INC(b->stats.free[false]);
  
 -	/* consider a 2MB slack on deflate, unless the balloon is emptied */
 -	if (target < size && target != 0 &&
 -	    size - target < vmballoon_page_in_frames(VMW_BALLOON_2M_PAGE))
 -		return 0;
 +	/* update balloon size */
 +	b->size--;
  
 -	return target - size;
 +	return 0;
  }
  
 -/**
 - * vmballoon_enqueue_page_list() - Enqueues list of pages after inflation.
 - *
 - * @b: pointer to balloon.
 - * @pages: list of pages to enqueue.
 - * @n_pages: pointer to number of pages in list. The value is zeroed.
 - * @page_size: whether the pages are 2MB or 4KB pages.
 - *
 - * Enqueues the provides list of pages in the ballooned page list, clears the
 - * list and zeroes the number of pages that was provided.
 - */
 -static void vmballoon_enqueue_page_list(struct vmballoon *b,
 -					struct list_head *pages,
 -					unsigned int *n_pages,
 -					enum vmballoon_page_size_type page_size)
 +static int vmballoon_unlock_batched_page(struct vmballoon *b,
 +				unsigned int num_pages, bool is_2m_pages,
 +				unsigned int *target)
  {
 -	struct vmballoon_page_size *page_size_info = &b->page_sizes[page_size];
 +	int locked, i, ret = 0;
 +	bool hv_success;
 +	u16 size_per_page = vmballoon_page_size(is_2m_pages);
 +
 +	hv_success = vmballoon_send_batched_unlock(b, num_pages, is_2m_pages,
 +			target);
 +	if (!hv_success)
 +		ret = -EIO;
  
 -	list_splice_init(pages, &page_size_info->pages);
 -	*n_pages = 0;
 +	for (i = 0; i < num_pages; i++) {
 +		u64 pa = vmballoon_batch_get_pa(b->batch_page, i);
 +		struct page *p = pfn_to_page(pa >> PAGE_SHIFT);
 +		struct vmballoon_page_size *page_size =
 +				&b->page_sizes[is_2m_pages];
 +
 +		locked = vmballoon_batch_get_status(b->batch_page, i);
 +		if (!hv_success || locked != VMW_BALLOON_SUCCESS) {
 +			/*
 +			 * That page wasn't successfully unlocked by the
 +			 * hypervisor, re-add it to the list of pages owned by
 +			 * the balloon driver.
 +			 */
 +			list_add(&p->lru, &page_size->pages);
 +		} else {
 +			/* deallocate page */
 +			vmballoon_free_page(p, is_2m_pages);
 +			STATS_INC(b->stats.free[is_2m_pages]);
 +
 +			/* update balloon size */
 +			b->size -= size_per_page;
 +		}
 +	}
 +
 +	return ret;
  }
  
 -/**
 - * vmballoon_dequeue_page_list() - Dequeues page lists for deflation.
 - *
 - * @b: pointer to balloon.
 - * @pages: list of pages to enqueue.
 - * @n_pages: pointer to number of pages in list. The value is zeroed.
 - * @page_size: whether the pages are 2MB or 4KB pages.
 - * @n_req_pages: the number of requested pages.
 - *
 - * Dequeues the number of requested pages from the balloon for deflation. The
 - * number of dequeued pages may be lower, if not enough pages in the requested
 - * size are available.
 +/*
 + * Release pages that were allocated while attempting to inflate the
 + * balloon but were refused by the host for one reason or another.
   */
 -static void vmballoon_dequeue_page_list(struct vmballoon *b,
 -					struct list_head *pages,
 -					unsigned int *n_pages,
 -					enum vmballoon_page_size_type page_size,
 -					unsigned int n_req_pages)
 +static void vmballoon_release_refused_pages(struct vmballoon *b,
 +		bool is_2m_pages)
  {
 -	struct vmballoon_page_size *page_size_info = &b->page_sizes[page_size];
 -	struct page *page, *tmp;
 -	unsigned int i = 0;
 +	struct page *page, *next;
 +	struct vmballoon_page_size *page_size =
 +			&b->page_sizes[is_2m_pages];
  
 -	list_for_each_entry_safe(page, tmp, &page_size_info->pages, lru) {
 -		list_move(&page->lru, pages);
 -		if (++i == n_req_pages)
 -			break;
 +	list_for_each_entry_safe(page, next, &page_size->refused_pages, lru) {
 +		list_del(&page->lru);
 +		vmballoon_free_page(page, is_2m_pages);
 +		STATS_INC(b->stats.refused_free[is_2m_pages]);
  	}
 -	*n_pages = i;
 +
 +	page_size->n_refused_pages = 0;
  }
  
 -/**
 - * vmballoon_inflate() - Inflate the balloon towards its target size.
 - *
 - * @b: pointer to the balloon.
 +static void vmballoon_add_page(struct vmballoon *b, int idx, struct page *p)
 +{
 +	b->page = p;
 +}
 +
 +static void vmballoon_add_batched_page(struct vmballoon *b, int idx,
 +				struct page *p)
 +{
 +	vmballoon_batch_set_pa(b->batch_page, idx,
 +			(u64)page_to_pfn(p) << PAGE_SHIFT);
 +}
 +
 +/*
 + * Inflate the balloon towards its target size. Note that we try to limit
 + * the rate of allocation to make sure we are not choking the rest of the
 + * system.
   */
  static void vmballoon_inflate(struct vmballoon *b)
  {
diff --cc fs/fuse/inode.c
index 10b75246113a,76baaa6be393..000000000000
--- a/fs/fuse/inode.c
+++ b/fs/fuse/inode.c
@@@ -830,12 -823,9 +830,17 @@@ static const struct super_operations fu
  
  static void sanitize_global_limit(unsigned *limit)
  {
 +	/*
 +	 * The default maximum number of async requests is calculated to consume
 +	 * 1/2^13 of the total memory, assuming 392 bytes per request.
 +	 */
  	if (*limit == 0)
++<<<<<<< HEAD
 +		*limit = ((totalram_pages << PAGE_SHIFT) >> 13) / 392;
++=======
+ 		*limit = ((totalram_pages() << PAGE_SHIFT) >> 13) /
+ 			 sizeof(struct fuse_req);
++>>>>>>> ca79b0c211af (mm: convert totalram_pages and totalhigh_pages variables to atomic)
  
  	if (*limit >= 1 << 16)
  		*limit = (1 << 16) - 1;
diff --cc mm/page_alloc.c
index 65ed4f0345ce,eb2027892ef9..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -15,10 -14,9 +15,11 @@@
   *          (lots of bits borrowed from Ingo Molnar & Andrew Morton)
   */
  
 +#include <linux/rh_kabi.h>
 +
  #include <linux/stddef.h>
  #include <linux/mm.h>
+ #include <linux/highmem.h>
  #include <linux/swap.h>
  #include <linux/interrupt.h>
  #include <linux/pagemap.h>
@@@ -7158,11 -7078,11 +7160,16 @@@ early_param("movablecore", cmdline_pars
  void adjust_managed_page_count(struct page *page, long count)
  {
  	spin_lock(&managed_page_count_lock);
++<<<<<<< HEAD
 +	page_zone(page)->managed_pages += count;
 +	totalram_pages += count;
++=======
+ 	atomic_long_add(count, &page_zone(page)->managed_pages);
+ 	totalram_pages_add(count);
++>>>>>>> ca79b0c211af (mm: convert totalram_pages and totalhigh_pages variables to atomic)
  #ifdef CONFIG_HIGHMEM
  	if (PageHighMem(page))
- 		totalhigh_pages += count;
+ 		totalhigh_pages_add(count);
  #endif
  	spin_unlock(&managed_page_count_lock);
  }
@@@ -7193,9 -7125,9 +7200,15 @@@ EXPORT_SYMBOL(free_reserved_area)
  void free_highmem_page(struct page *page)
  {
  	__free_reserved_page(page);
++<<<<<<< HEAD
 +	totalram_pages++;
 +	page_zone(page)->managed_pages++;
 +	totalhigh_pages++;
++=======
+ 	totalram_pages_inc();
+ 	atomic_long_inc(&page_zone(page)->managed_pages);
+ 	totalhigh_pages_inc();
++>>>>>>> ca79b0c211af (mm: convert totalram_pages and totalhigh_pages variables to atomic)
  }
  #endif
  
* Unmerged path arch/csky/mm/init.c
* Unmerged path arch/csky/mm/init.c
diff --git a/arch/powerpc/platforms/pseries/cmm.c b/arch/powerpc/platforms/pseries/cmm.c
index 317181692fcb..3468cd9263cb 100644
--- a/arch/powerpc/platforms/pseries/cmm.c
+++ b/arch/powerpc/platforms/pseries/cmm.c
@@ -208,7 +208,7 @@ static long cmm_alloc_pages(long nr)
 
 		pa->page[pa->index++] = addr;
 		loaned_pages++;
-		totalram_pages--;
+		totalram_pages_dec();
 		spin_unlock(&cmm_lock);
 		nr--;
 	}
@@ -247,7 +247,7 @@ static long cmm_free_pages(long nr)
 		free_page(addr);
 		loaned_pages--;
 		nr--;
-		totalram_pages++;
+		totalram_pages_inc();
 	}
 	spin_unlock(&cmm_lock);
 	cmm_dbg("End request with %ld pages unfulfilled\n", nr);
@@ -291,7 +291,7 @@ static void cmm_get_mpp(void)
 	int rc;
 	struct hvcall_mpp_data mpp_data;
 	signed long active_pages_target, page_loan_request, target;
-	signed long total_pages = totalram_pages + loaned_pages;
+	signed long total_pages = totalram_pages() + loaned_pages;
 	signed long min_mem_pages = (min_mem_mb * 1024 * 1024) / PAGE_SIZE;
 
 	rc = h_get_mpp(&mpp_data);
@@ -322,7 +322,7 @@ static void cmm_get_mpp(void)
 
 	cmm_dbg("delta = %ld, loaned = %lu, target = %lu, oom = %lu, totalram = %lu\n",
 		page_loan_request, loaned_pages, loaned_pages_target,
-		oom_freed_pages, totalram_pages);
+		oom_freed_pages, totalram_pages());
 }
 
 static struct notifier_block cmm_oom_nb = {
@@ -586,7 +586,7 @@ static int cmm_mem_going_offline(void *arg)
 			free_page(pa_curr->page[idx]);
 			freed++;
 			loaned_pages--;
-			totalram_pages++;
+			totalram_pages_inc();
 			pa_curr->page[idx] = pa_last->page[--pa_last->index];
 			if (pa_last->index == 0) {
 				if (pa_curr == pa_last)
diff --git a/arch/s390/mm/init.c b/arch/s390/mm/init.c
index c73cc156f3ba..46702645d427 100644
--- a/arch/s390/mm/init.c
+++ b/arch/s390/mm/init.c
@@ -66,7 +66,7 @@ static void __init setup_zero_pages(void)
 	order = 7;
 
 	/* Limit number of empty zero pages for small memory sizes */
-	while (order > 2 && (totalram_pages >> 10) < (1UL << order))
+	while (order > 2 && (totalram_pages() >> 10) < (1UL << order))
 		order--;
 
 	empty_zero_page = __get_free_pages(GFP_KERNEL | __GFP_ZERO, order);
diff --git a/arch/um/kernel/mem.c b/arch/um/kernel/mem.c
index 2da209687a22..8d21a83dd289 100644
--- a/arch/um/kernel/mem.c
+++ b/arch/um/kernel/mem.c
@@ -51,7 +51,7 @@ void __init mem_init(void)
 
 	/* this will put all low memory onto the freelists */
 	memblock_free_all();
-	max_low_pfn = totalram_pages;
+	max_low_pfn = totalram_pages();
 	max_pfn = max_low_pfn;
 	mem_init_print_info(NULL);
 	kmalloc_ok = 1;
diff --git a/arch/x86/kernel/cpu/microcode/core.c b/arch/x86/kernel/cpu/microcode/core.c
index 3662fd82388b..c28d1636edb1 100644
--- a/arch/x86/kernel/cpu/microcode/core.c
+++ b/arch/x86/kernel/cpu/microcode/core.c
@@ -426,7 +426,7 @@ static ssize_t microcode_write(struct file *file, const char __user *buf,
 			       size_t len, loff_t *ppos)
 {
 	ssize_t ret = -EINVAL;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 
 	if ((len >> PAGE_SHIFT) > nr_pages) {
 		pr_err("too much data (max %ld pages)\n", nr_pages);
diff --git a/drivers/char/agp/backend.c b/drivers/char/agp/backend.c
index 38ffb281df97..004a3ce8ba72 100644
--- a/drivers/char/agp/backend.c
+++ b/drivers/char/agp/backend.c
@@ -115,9 +115,9 @@ static int agp_find_max(void)
 	long memory, index, result;
 
 #if PAGE_SHIFT < 20
-	memory = totalram_pages >> (20 - PAGE_SHIFT);
+	memory = totalram_pages() >> (20 - PAGE_SHIFT);
 #else
-	memory = totalram_pages << (PAGE_SHIFT - 20);
+	memory = totalram_pages() << (PAGE_SHIFT - 20);
 #endif
 	index = 1;
 
* Unmerged path drivers/gpu/drm/i915/i915_gem.c
* Unmerged path drivers/gpu/drm/i915/selftests/i915_gem_gtt.c
diff --git a/drivers/hv/hv_balloon.c b/drivers/hv/hv_balloon.c
index 09bd4fa6b6c6..877537206ba5 100644
--- a/drivers/hv/hv_balloon.c
+++ b/drivers/hv/hv_balloon.c
@@ -1096,7 +1096,7 @@ static void process_info(struct hv_dynmem_device *dm, struct dm_info_msg *msg)
 static unsigned long compute_balloon_floor(void)
 {
 	unsigned long min_pages;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 #define MB2PAGES(mb) ((mb) << (20 - PAGE_SHIFT))
 	/* Simple continuous piecewiese linear function:
 	 *  max MiB -> min MiB  gradient
diff --git a/drivers/md/dm-bufio.c b/drivers/md/dm-bufio.c
index 343235d56ed0..e0aa576a2435 100644
--- a/drivers/md/dm-bufio.c
+++ b/drivers/md/dm-bufio.c
@@ -1950,7 +1950,7 @@ static int __init dm_bufio_init(void)
 	dm_bufio_allocated_vmalloc = 0;
 	dm_bufio_current_allocated = 0;
 
-	mem = (__u64)mult_frac(totalram_pages - totalhigh_pages,
+	mem = (__u64)mult_frac(totalram_pages() - totalhigh_pages(),
 			       DM_BUFIO_MEMORY_PERCENT, 100) << PAGE_SHIFT;
 
 	if (mem > ULONG_MAX)
diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c
index 5446ce17e38b..6ab9ffd9b501 100644
--- a/drivers/md/dm-crypt.c
+++ b/drivers/md/dm-crypt.c
@@ -2424,7 +2424,7 @@ static int crypt_wipe_key(struct crypt_config *cc)
 
 static void crypt_calculate_pages_per_client(void)
 {
-	unsigned long pages = (totalram_pages - totalhigh_pages) * DM_CRYPT_MEMORY_PERCENT / 100;
+	unsigned long pages = (totalram_pages() - totalhigh_pages()) * DM_CRYPT_MEMORY_PERCENT / 100;
 
 	if (!dm_crypt_clients_n)
 		return;
diff --git a/drivers/md/dm-integrity.c b/drivers/md/dm-integrity.c
index 9ccc7b885db7..19550e8e6644 100644
--- a/drivers/md/dm-integrity.c
+++ b/drivers/md/dm-integrity.c
@@ -3505,7 +3505,7 @@ static int create_journal(struct dm_integrity_c *ic, char **error)
 	journal_pages = roundup((__u64)ic->journal_sections * ic->journal_section_sectors,
 				PAGE_SIZE >> SECTOR_SHIFT) >> (PAGE_SHIFT - SECTOR_SHIFT);
 	journal_desc_size = journal_pages * sizeof(struct page_list);
-	if (journal_pages >= totalram_pages - totalhigh_pages || journal_desc_size > ULONG_MAX) {
+	if (journal_pages >= totalram_pages() - totalhigh_pages() || journal_desc_size > ULONG_MAX) {
 		*error = "Journal doesn't fit into memory";
 		r = -ENOMEM;
 		goto bad;
diff --git a/drivers/md/dm-stats.c b/drivers/md/dm-stats.c
index 53188825f289..35d368c418d0 100644
--- a/drivers/md/dm-stats.c
+++ b/drivers/md/dm-stats.c
@@ -85,7 +85,7 @@ static bool __check_shared_memory(size_t alloc_size)
 	a = shared_memory_amount + alloc_size;
 	if (a < shared_memory_amount)
 		return false;
-	if (a >> PAGE_SHIFT > totalram_pages / DM_STATS_MEMORY_FACTOR)
+	if (a >> PAGE_SHIFT > totalram_pages() / DM_STATS_MEMORY_FACTOR)
 		return false;
 #ifdef CONFIG_MMU
 	if (a > (VMALLOC_END - VMALLOC_START) / DM_STATS_VMALLOC_FACTOR)
diff --git a/drivers/media/platform/mtk-vpu/mtk_vpu.c b/drivers/media/platform/mtk-vpu/mtk_vpu.c
index 1ff6a93262b7..88ab4922d881 100644
--- a/drivers/media/platform/mtk-vpu/mtk_vpu.c
+++ b/drivers/media/platform/mtk-vpu/mtk_vpu.c
@@ -856,7 +856,7 @@ static int mtk_vpu_probe(struct platform_device *pdev)
 	/* Set PTCM to 96K and DTCM to 32K */
 	vpu_cfg_writel(vpu, 0x2, VPU_TCM_CFG);
 
-	vpu->enable_4GB = !!(totalram_pages > (SZ_2G >> PAGE_SHIFT));
+	vpu->enable_4GB = !!(totalram_pages() > (SZ_2G >> PAGE_SHIFT));
 	dev_info(dev, "4GB mode %u\n", vpu->enable_4GB);
 
 	if (vpu->enable_4GB) {
* Unmerged path drivers/misc/vmw_balloon.c
diff --git a/drivers/parisc/ccio-dma.c b/drivers/parisc/ccio-dma.c
index 614823617b8b..067cbaf1eebf 100644
--- a/drivers/parisc/ccio-dma.c
+++ b/drivers/parisc/ccio-dma.c
@@ -1255,7 +1255,7 @@ ccio_ioc_init(struct ioc *ioc)
 	** Hot-Plug/Removal of PCI cards. (aka PCI OLARD).
 	*/
 
-	iova_space_size = (u32) (totalram_pages / count_parisc_driver(&ccio_driver));
+	iova_space_size = (u32) (totalram_pages() / count_parisc_driver(&ccio_driver));
 
 	/* limit IOVA space size to 1MB-1GB */
 
@@ -1294,7 +1294,7 @@ ccio_ioc_init(struct ioc *ioc)
 
 	DBG_INIT("%s() hpa 0x%p mem %luMB IOV %dMB (%d bits)\n",
 			__func__, ioc->ioc_regs,
-			(unsigned long) totalram_pages >> (20 - PAGE_SHIFT),
+			(unsigned long) totalram_pages() >> (20 - PAGE_SHIFT),
 			iova_space_size>>20,
 			iov_order + PAGE_SHIFT);
 
diff --git a/drivers/parisc/sba_iommu.c b/drivers/parisc/sba_iommu.c
index 11de0eccf968..b61ae314e948 100644
--- a/drivers/parisc/sba_iommu.c
+++ b/drivers/parisc/sba_iommu.c
@@ -1419,7 +1419,7 @@ sba_ioc_init(struct parisc_device *sba, struct ioc *ioc, int ioc_num)
 	** for DMA hints - ergo only 30 bits max.
 	*/
 
-	iova_space_size = (u32) (totalram_pages/global_ioc_cnt);
+	iova_space_size = (u32) (totalram_pages()/global_ioc_cnt);
 
 	/* limit IOVA space size to 1MB-1GB */
 	if (iova_space_size < (1 << (20 - PAGE_SHIFT))) {
@@ -1444,7 +1444,7 @@ sba_ioc_init(struct parisc_device *sba, struct ioc *ioc, int ioc_num)
 	DBG_INIT("%s() hpa 0x%lx mem %ldMB IOV %dMB (%d bits)\n",
 			__func__,
 			ioc->ioc_hpa,
-			(unsigned long) totalram_pages >> (20 - PAGE_SHIFT),
+			(unsigned long) totalram_pages() >> (20 - PAGE_SHIFT),
 			iova_space_size>>20,
 			iov_order + PAGE_SHIFT);
 
diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c
index 701eb9f3b0f1..45dc363a844a 100644
--- a/drivers/staging/android/ion/ion_system_heap.c
+++ b/drivers/staging/android/ion/ion_system_heap.c
@@ -110,7 +110,7 @@ static int ion_system_heap_allocate(struct ion_heap *heap,
 	unsigned long size_remaining = PAGE_ALIGN(size);
 	unsigned int max_order = orders[0];
 
-	if (size / PAGE_SIZE > totalram_pages / 2)
+	if (size / PAGE_SIZE > totalram_pages() / 2)
 		return -ENOMEM;
 
 	INIT_LIST_HEAD(&pages);
diff --git a/drivers/xen/xen-selfballoon.c b/drivers/xen/xen-selfballoon.c
index 5165aa82bf7d..246f6122c9ee 100644
--- a/drivers/xen/xen-selfballoon.c
+++ b/drivers/xen/xen-selfballoon.c
@@ -189,7 +189,7 @@ static void selfballoon_process(struct work_struct *work)
 	bool reset_timer = false;
 
 	if (xen_selfballooning_enabled) {
-		cur_pages = totalram_pages;
+		cur_pages = totalram_pages();
 		tgt_pages = cur_pages; /* default is no change */
 		goal_pages = vm_memory_committed() +
 				totalreserve_pages +
@@ -227,7 +227,7 @@ static void selfballoon_process(struct work_struct *work)
 		if (tgt_pages < floor_pages)
 			tgt_pages = floor_pages;
 		balloon_set_new_target(tgt_pages +
-			balloon_stats.current_pages - totalram_pages);
+			balloon_stats.current_pages - totalram_pages());
 		reset_timer = true;
 	}
 #ifdef CONFIG_FRONTSWAP
@@ -569,7 +569,7 @@ int xen_selfballoon_init(bool use_selfballooning, bool use_frontswap_selfshrink)
 	 * much more reliably and response faster in some cases.
 	 */
 	if (!selfballoon_reserved_mb) {
-		reserve_pages = totalram_pages / 10;
+		reserve_pages = totalram_pages() / 10;
 		selfballoon_reserved_mb = PAGES2MB(reserve_pages);
 	}
 	schedule_delayed_work(&selfballoon_worker, selfballoon_interval * HZ);
diff --git a/fs/ceph/super.h b/fs/ceph/super.h
index 9baecbdb54c9..dc619e7a4dca 100644
--- a/fs/ceph/super.h
+++ b/fs/ceph/super.h
@@ -873,7 +873,7 @@ static inline int default_congestion_kb(void)
 	 * This allows larger machines to have larger/more transfers.
 	 * Limit the default to 256M
 	 */
-	congestion_kb = (16*int_sqrt(totalram_pages)) << (PAGE_SHIFT-10);
+	congestion_kb = (16*int_sqrt(totalram_pages())) << (PAGE_SHIFT-10);
 	if (congestion_kb > 256*1024)
 		congestion_kb = 256*1024;
 
diff --git a/fs/file_table.c b/fs/file_table.c
index 6b59752775c9..25930a082b68 100644
--- a/fs/file_table.c
+++ b/fs/file_table.c
@@ -375,7 +375,7 @@ void __init files_init(void)
 void __init files_maxfiles_init(void)
 {
 	unsigned long n;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 	unsigned long memreserve = (nr_pages - nr_free_pages()) * 3/2;
 
 	memreserve = min(memreserve, nr_pages - 1);
* Unmerged path fs/fuse/inode.c
diff --git a/fs/nfs/write.c b/fs/nfs/write.c
index 37d20a006b47..c51f11e85128 100644
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@ -2160,7 +2160,7 @@ int __init nfs_init_writepagecache(void)
 	 * This allows larger machines to have larger/more transfers.
 	 * Limit the default to 256M
 	 */
-	nfs_congestion_kb = (16*int_sqrt(totalram_pages)) << (PAGE_SHIFT-10);
+	nfs_congestion_kb = (16*int_sqrt(totalram_pages())) << (PAGE_SHIFT-10);
 	if (nfs_congestion_kb > 256*1024)
 		nfs_congestion_kb = 256*1024;
 
diff --git a/fs/nfsd/nfscache.c b/fs/nfsd/nfscache.c
index f3595fcead23..0c10bfea039e 100644
--- a/fs/nfsd/nfscache.c
+++ b/fs/nfsd/nfscache.c
@@ -69,7 +69,7 @@ static unsigned int
 nfsd_cache_size_limit(void)
 {
 	unsigned int limit;
-	unsigned long low_pages = totalram_pages - totalhigh_pages;
+	unsigned long low_pages = totalram_pages() - totalhigh_pages();
 
 	limit = (16 * int_sqrt(low_pages)) << (PAGE_SHIFT-10);
 	return min_t(unsigned int, limit, 256*1024);
diff --git a/fs/ntfs/malloc.h b/fs/ntfs/malloc.h
index ab172e5f51d9..5becc8acc8f4 100644
--- a/fs/ntfs/malloc.h
+++ b/fs/ntfs/malloc.h
@@ -47,7 +47,7 @@ static inline void *__ntfs_malloc(unsigned long size, gfp_t gfp_mask)
 		return kmalloc(PAGE_SIZE, gfp_mask & ~__GFP_HIGHMEM);
 		/* return (void *)__get_free_page(gfp_mask); */
 	}
-	if (likely((size >> PAGE_SHIFT) < totalram_pages))
+	if (likely((size >> PAGE_SHIFT) < totalram_pages()))
 		return __vmalloc(size, gfp_mask, PAGE_KERNEL);
 	return NULL;
 }
diff --git a/fs/proc/base.c b/fs/proc/base.c
index 36f0d67b90e5..490144b72557 100644
--- a/fs/proc/base.c
+++ b/fs/proc/base.c
@@ -526,7 +526,7 @@ static const struct file_operations proc_lstats_operations = {
 static int proc_oom_score(struct seq_file *m, struct pid_namespace *ns,
 			  struct pid *pid, struct task_struct *task)
 {
-	unsigned long totalpages = totalram_pages + total_swap_pages;
+	unsigned long totalpages = totalram_pages() + total_swap_pages;
 	unsigned long points = 0;
 
 	points = oom_badness(task, NULL, NULL, totalpages) *
diff --git a/include/linux/highmem.h b/include/linux/highmem.h
index 0690679832d4..ea5cdbd8c2c3 100644
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@ -36,7 +36,31 @@ static inline void invalidate_kernel_vmap_range(void *vaddr, int size)
 
 /* declarations for linux/mm/highmem.c */
 unsigned int nr_free_highpages(void);
-extern unsigned long totalhigh_pages;
+extern atomic_long_t _totalhigh_pages;
+static inline unsigned long totalhigh_pages(void)
+{
+	return (unsigned long)atomic_long_read(&_totalhigh_pages);
+}
+
+static inline void totalhigh_pages_inc(void)
+{
+	atomic_long_inc(&_totalhigh_pages);
+}
+
+static inline void totalhigh_pages_dec(void)
+{
+	atomic_long_dec(&_totalhigh_pages);
+}
+
+static inline void totalhigh_pages_add(long count)
+{
+	atomic_long_add(count, &_totalhigh_pages);
+}
+
+static inline void totalhigh_pages_set(long val)
+{
+	atomic_long_set(&_totalhigh_pages, val);
+}
 
 void kmap_flush_unused(void);
 
@@ -51,7 +75,7 @@ static inline struct page *kmap_to_page(void *addr)
 	return virt_to_page(addr);
 }
 
-#define totalhigh_pages 0UL
+static inline unsigned long totalhigh_pages(void) { return 0UL; }
 
 #ifndef ARCH_HAS_KMAP
 static inline void *kmap(struct page *page)
diff --git a/include/linux/mm.h b/include/linux/mm.h
index cb077fce907f..609359a6b5f3 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -52,7 +52,32 @@ static inline void set_max_mapnr(unsigned long limit)
 static inline void set_max_mapnr(unsigned long limit) { }
 #endif
 
-extern unsigned long totalram_pages;
+extern atomic_long_t _totalram_pages;
+static inline unsigned long totalram_pages(void)
+{
+	return (unsigned long)atomic_long_read(&_totalram_pages);
+}
+
+static inline void totalram_pages_inc(void)
+{
+	atomic_long_inc(&_totalram_pages);
+}
+
+static inline void totalram_pages_dec(void)
+{
+	atomic_long_dec(&_totalram_pages);
+}
+
+static inline void totalram_pages_add(long count)
+{
+	atomic_long_add(count, &_totalram_pages);
+}
+
+static inline void totalram_pages_set(long val)
+{
+	atomic_long_set(&_totalram_pages, val);
+}
+
 extern void * high_memory;
 extern int page_cluster;
 
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 9ca0891cb4b1..3afc7527f218 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -314,7 +314,6 @@ void workingset_update_node(struct xa_node *node);
 } while (0)
 
 /* linux/mm/page_alloc.c */
-extern unsigned long totalram_pages;
 extern unsigned long totalreserve_pages;
 extern unsigned long nr_free_buffer_pages(void);
 extern unsigned long nr_free_pagecache_pages(void);
diff --git a/kernel/fork.c b/kernel/fork.c
index 15627b0e0e5d..beb753f652a7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -750,7 +750,7 @@ void __init __weak arch_task_cache_init(void) { }
 static void set_max_threads(unsigned int max_threads_suggested)
 {
 	u64 threads;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 
 	/*
 	 * The number of threads shall be limited such that the thread
diff --git a/kernel/kexec_core.c b/kernel/kexec_core.c
index 91b04d3378de..fd5c95ff9251 100644
--- a/kernel/kexec_core.c
+++ b/kernel/kexec_core.c
@@ -152,7 +152,7 @@ int sanity_check_segment_list(struct kimage *image)
 	int i;
 	unsigned long nr_segments = image->nr_segments;
 	unsigned long total_pages = 0;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 
 	/*
 	 * Verify we have good destination addresses.  The caller is
diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c
index 3b6cfb11c77b..5709b52ff142 100644
--- a/kernel/power/snapshot.c
+++ b/kernel/power/snapshot.c
@@ -105,7 +105,7 @@ unsigned long image_size;
 
 void __init hibernate_image_size_init(void)
 {
-	image_size = ((totalram_pages * 2) / 5) * PAGE_SIZE;
+	image_size = ((totalram_pages() * 2) / 5) * PAGE_SIZE;
 }
 
 /*
diff --git a/mm/highmem.c b/mm/highmem.c
index 59db3223a5d6..107b10f9878e 100644
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@ -105,9 +105,8 @@ static inline wait_queue_head_t *get_pkmap_wait_queue_head(unsigned int color)
 }
 #endif
 
-unsigned long totalhigh_pages __read_mostly;
-EXPORT_SYMBOL(totalhigh_pages);
-
+atomic_long_t _totalhigh_pages __read_mostly;
+EXPORT_SYMBOL(_totalhigh_pages);
 
 EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
 
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 60b47b7d7614..de8f3242047d 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -413,7 +413,7 @@ static int __init hugepage_init(void)
 	 * where the extra memory used could hurt more than TLB overhead
 	 * is likely to save.  The admin can still enable it through /sys.
 	 */
-	if (totalram_pages < (512 << (20 - PAGE_SHIFT))) {
+	if (totalram_pages() < (512 << (20 - PAGE_SHIFT))) {
 		transparent_hugepage_flags = 0;
 		return 0;
 	}
diff --git a/mm/kasan/quarantine.c b/mm/kasan/quarantine.c
index b209dbaefde8..5835c0f583d3 100644
--- a/mm/kasan/quarantine.c
+++ b/mm/kasan/quarantine.c
@@ -236,7 +236,7 @@ void quarantine_reduce(void)
 	 * Update quarantine size in case of hotplug. Allocate a fraction of
 	 * the installed memory to quarantine minus per-cpu queue limits.
 	 */
-	total_size = (READ_ONCE(totalram_pages) << PAGE_SHIFT) /
+	total_size = (totalram_pages() << PAGE_SHIFT) /
 		QUARANTINE_FRACTION;
 	percpu_quarantines = QUARANTINE_PERCPU_SIZE * num_online_cpus();
 	new_quarantine_size = (total_size < percpu_quarantines) ?
diff --git a/mm/memblock.c b/mm/memblock.c
index f519399b14b5..0cac376dc05b 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -1643,7 +1643,7 @@ void __init __memblock_free_late(phys_addr_t base, phys_addr_t size)
 
 	for (; cursor < end; cursor++) {
 		memblock_free_pages(pfn_to_page(cursor), cursor, 0);
-		totalram_pages++;
+		totalram_pages_inc();
 	}
 }
 
@@ -2045,7 +2045,7 @@ unsigned long __init memblock_free_all(void)
 	reset_all_zones_managed_pages();
 
 	pages = free_low_memory_core_early();
-	totalram_pages += pages;
+	totalram_pages_add(pages);
 
 	return pages;
 }
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 6838a530789b..33917105a3a2 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -146,7 +146,7 @@ static void __meminit mm_compute_batch(void)
 	s32 batch = max_t(s32, nr*2, 32);
 
 	/* batch size set to 0.4% of (total memory/#cpus), or max int32 */
-	memsized_batch = min_t(u64, (totalram_pages/nr)/256, 0x7fffffff);
+	memsized_batch = min_t(u64, (totalram_pages()/nr)/256, 0x7fffffff);
 
 	vm_committed_as_batch = max_t(s32, memsized_batch, batch);
 }
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index e09692e9bf25..84d32f9fa394 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -261,7 +261,7 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)
 	}
 
 	/* Default to all available memory */
-	oc->totalpages = totalram_pages + total_swap_pages;
+	oc->totalpages = totalram_pages() + total_swap_pages;
 
 	if (!IS_ENABLED(CONFIG_NUMA))
 		return CONSTRAINT_NONE;
* Unmerged path mm/page_alloc.c
diff --git a/mm/shmem.c b/mm/shmem.c
index bdcec51e26ad..2f4ef8f11f74 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -122,13 +122,14 @@ struct shmem_options {
 #ifdef CONFIG_TMPFS
 static unsigned long shmem_default_max_blocks(void)
 {
-	return totalram_pages / 2;
+	return totalram_pages() / 2;
 }
 
 static unsigned long shmem_default_max_inodes(void)
 {
-	unsigned long nr_pages = totalram_pages;
-	return min(nr_pages - totalhigh_pages, nr_pages / 2);
+	unsigned long nr_pages = totalram_pages();
+
+	return min(nr_pages - totalhigh_pages(), nr_pages / 2);
 }
 #endif
 
@@ -3395,7 +3396,7 @@ static int shmem_parse_options(char *options, struct shmem_options *ctx)
 			size = memparse(value,&rest);
 			if (*rest == '%') {
 				size <<= PAGE_SHIFT;
-				size *= totalram_pages;
+				size *= totalram_pages();
 				do_div(size, 100);
 				rest++;
 			}
diff --git a/mm/slab.c b/mm/slab.c
index 67152fb6c8c4..c76f8159a944 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -1236,7 +1236,7 @@ void __init kmem_cache_init(void)
 	 * page orders on machines with more than 32MB of memory if
 	 * not overridden on the command line.
 	 */
-	if (!slab_max_order_set && totalram_pages > (32 << 20) >> PAGE_SHIFT)
+	if (!slab_max_order_set && totalram_pages() > (32 << 20) >> PAGE_SHIFT)
 		slab_max_order = SLAB_MAX_ORDER_HI;
 
 	/* Bootstrap is tricky, because several objects are allocated
diff --git a/mm/swap.c b/mm/swap.c
index 48630e045f06..1380f7ba88ca 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -1029,7 +1029,7 @@ EXPORT_SYMBOL(pagevec_lookup_range_nr_tag);
  */
 void __init swap_setup(void)
 {
-	unsigned long megs = totalram_pages >> (20 - PAGE_SHIFT);
+	unsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);
 
 	/* Use a smaller cluster for small-memory machines */
 	if (megs < 16)
diff --git a/mm/util.c b/mm/util.c
index e99de9d3c8ae..fa771b7d1b17 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -658,7 +658,7 @@ unsigned long vm_commit_limit(void)
 	if (sysctl_overcommit_kbytes)
 		allowed = sysctl_overcommit_kbytes >> (PAGE_SHIFT - 10);
 	else
-		allowed = ((totalram_pages - hugetlb_total_pages())
+		allowed = ((totalram_pages() - hugetlb_total_pages())
 			   * sysctl_overcommit_ratio / 100);
 	allowed += total_swap_pages;
 
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index 3dfb61482a83..a4e46863fa01 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -1710,7 +1710,7 @@ void *vmap(struct page **pages, unsigned int count,
 
 	might_sleep();
 
-	if (count > totalram_pages)
+	if (count > totalram_pages())
 		return NULL;
 
 	size = (unsigned long)count << PAGE_SHIFT;
@@ -1815,7 +1815,7 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,
 	unsigned long real_size = size;
 
 	size = PAGE_ALIGN(size);
-	if (!size || (size >> PAGE_SHIFT) > totalram_pages)
+	if (!size || (size >> PAGE_SHIFT) > totalram_pages())
 		goto fail;
 
 	area = __get_vm_area_node(size, align, VM_ALLOC | VM_UNINITIALIZED |
diff --git a/mm/workingset.c b/mm/workingset.c
index 5839340c5e30..dfc05bfcff5a 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -541,7 +541,7 @@ static int __init workingset_init(void)
 	 * double the initial memory by using totalram_pages as-is.
 	 */
 	timestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;
-	max_order = fls_long(totalram_pages - 1);
+	max_order = fls_long(totalram_pages() - 1);
 	if (max_order > timestamp_bits)
 		bucket_order = max_order - timestamp_bits;
 	pr_info("workingset: timestamp_bits=%d max_order=%d bucket_order=%u\n",
diff --git a/mm/zswap.c b/mm/zswap.c
index cd91fd9d96b8..a4e4d36ec085 100644
--- a/mm/zswap.c
+++ b/mm/zswap.c
@@ -219,8 +219,8 @@ static const struct zpool_ops zswap_zpool_ops = {
 
 static bool zswap_is_full(void)
 {
-	return totalram_pages * zswap_max_pool_percent / 100 <
-		DIV_ROUND_UP(zswap_pool_total_size, PAGE_SIZE);
+	return totalram_pages() * zswap_max_pool_percent / 100 <
+			DIV_ROUND_UP(zswap_pool_total_size, PAGE_SIZE);
 }
 
 static void zswap_update_total_size(void)
diff --git a/net/dccp/proto.c b/net/dccp/proto.c
index 8cc6869510b7..e6bc0668fe90 100644
--- a/net/dccp/proto.c
+++ b/net/dccp/proto.c
@@ -1131,7 +1131,7 @@ EXPORT_SYMBOL_GPL(dccp_debug);
 static int __init dccp_init(void)
 {
 	unsigned long goal;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 	int ehash_order, bhash_order, i;
 	int rc;
 
diff --git a/net/decnet/dn_route.c b/net/decnet/dn_route.c
index 1524ca85437b..8d1bb00e5bbb 100644
--- a/net/decnet/dn_route.c
+++ b/net/decnet/dn_route.c
@@ -1868,7 +1868,7 @@ void __init dn_route_init(void)
 	dn_route_timer.expires = jiffies + decnet_dst_gc_interval * HZ;
 	add_timer(&dn_route_timer);
 
-	goal = totalram_pages >> (26 - PAGE_SHIFT);
+	goal = totalram_pages() >> (26 - PAGE_SHIFT);
 
 	for(order = 0; (1UL << order) < goal; order++)
 		/* NOTHING */;
diff --git a/net/ipv4/tcp_metrics.c b/net/ipv4/tcp_metrics.c
index 4a8938245205..f262f2cace29 100644
--- a/net/ipv4/tcp_metrics.c
+++ b/net/ipv4/tcp_metrics.c
@@ -1001,7 +1001,7 @@ static int __net_init tcp_net_metrics_init(struct net *net)
 
 	slots = tcpmhash_entries;
 	if (!slots) {
-		if (totalram_pages >= 128 * 1024)
+		if (totalram_pages() >= 128 * 1024)
 			slots = 16 * 1024;
 		else
 			slots = 8 * 1024;
diff --git a/net/netfilter/nf_conntrack_core.c b/net/netfilter/nf_conntrack_core.c
index d1869daa8705..c68dec04a6cd 100644
--- a/net/netfilter/nf_conntrack_core.c
+++ b/net/netfilter/nf_conntrack_core.c
@@ -2588,7 +2588,7 @@ static __always_inline unsigned int total_extension_size(void)
 
 int nf_conntrack_init_start(void)
 {
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 	int max_factor = 8;
 	int ret = -ENOMEM;
 	int i;
diff --git a/net/netfilter/xt_hashlimit.c b/net/netfilter/xt_hashlimit.c
index 88b520ba2abc..8d86e39d6280 100644
--- a/net/netfilter/xt_hashlimit.c
+++ b/net/netfilter/xt_hashlimit.c
@@ -274,7 +274,7 @@ static int htable_create(struct net *net, struct hashlimit_cfg3 *cfg,
 	struct xt_hashlimit_htable *hinfo;
 	const struct seq_operations *ops;
 	unsigned int size, i;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 	int ret;
 
 	if (cfg->size) {
diff --git a/net/sctp/protocol.c b/net/sctp/protocol.c
index 3bcd7a2ffd3a..41c0a1cca35c 100644
--- a/net/sctp/protocol.c
+++ b/net/sctp/protocol.c
@@ -1393,7 +1393,7 @@ static __init int sctp_init(void)
 	int status = -EINVAL;
 	unsigned long goal;
 	unsigned long limit;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 	int max_share;
 	int order;
 	int num_entries;
diff --git a/security/integrity/ima/ima_kexec.c b/security/integrity/ima/ima_kexec.c
index 16bd18747cfa..d6f32807b347 100644
--- a/security/integrity/ima/ima_kexec.c
+++ b/security/integrity/ima/ima_kexec.c
@@ -106,7 +106,7 @@ void ima_add_kexec_buffer(struct kimage *image)
 		kexec_segment_size = ALIGN(ima_get_binary_runtime_size() +
 					   PAGE_SIZE / 2, PAGE_SIZE);
 	if ((kexec_segment_size == ULONG_MAX) ||
-	    ((kexec_segment_size >> PAGE_SHIFT) > totalram_pages / 2)) {
+	    ((kexec_segment_size >> PAGE_SHIFT) > totalram_pages() / 2)) {
 		pr_err("Binary measurement list too large.\n");
 		return;
 	}
