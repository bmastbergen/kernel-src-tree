lpfc: nvmet: Add Send LS Request and Abort LS Request support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author James Smart <jsmart2021@gmail.com>
commit 54840bed372c7779f23ece8514853fa83887b02e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/54840bed.failed

Now that common helpers exist, add the ability to Send an NVME LS Request
and to Abort an outstanding LS Request to the nvmet side of the driver.

	Signed-off-by: Paul Ely <paul.ely@broadcom.com>
	Signed-off-by: James Smart <jsmart2021@gmail.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 54840bed372c7779f23ece8514853fa83887b02e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_nvmet.c
diff --cc drivers/scsi/lpfc/lpfc_nvmet.c
index 0778dbb7937a,1c6bbbba70b5..000000000000
--- a/drivers/scsi/lpfc/lpfc_nvmet.c
+++ b/drivers/scsi/lpfc/lpfc_nvmet.c
@@@ -1172,6 -1282,132 +1172,135 @@@ lpfc_nvmet_defer_rcv(struct nvmet_fc_ta
  	spin_unlock_irqrestore(&ctxp->ctxlock, iflag);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * lpfc_nvmet_ls_req_cmp - completion handler for a nvme ls request
+  * @phba: Pointer to HBA context object
+  * @cmdwqe: Pointer to driver command WQE object.
+  * @wcqe: Pointer to driver response CQE object.
+  *
+  * This function is the completion handler for NVME LS requests.
+  * The function updates any states and statistics, then calls the
+  * generic completion handler to finish completion of the request.
+  **/
+ static void
+ lpfc_nvmet_ls_req_cmp(struct lpfc_hba *phba, struct lpfc_iocbq *cmdwqe,
+ 		       struct lpfc_wcqe_complete *wcqe)
+ {
+ 	__lpfc_nvme_ls_req_cmp(phba, cmdwqe->vport, cmdwqe, wcqe);
+ }
+ 
+ /**
+  * lpfc_nvmet_ls_req - Issue an Link Service request
+  * @targetport - pointer to target instance registered with nvmet transport.
+  * @hosthandle - hosthandle set by the driver in a prior ls_rqst_rcv.
+  *               Driver sets this value to the ndlp pointer.
+  * @pnvme_lsreq - the transport nvme_ls_req structure for the LS
+  *
+  * Driver registers this routine to handle any link service request
+  * from the nvme_fc transport to a remote nvme-aware port.
+  *
+  * Return value :
+  *   0 - Success
+  *   non-zero: various error codes, in form of -Exxx
+  **/
+ static int
+ lpfc_nvmet_ls_req(struct nvmet_fc_target_port *targetport,
+ 		  void *hosthandle,
+ 		  struct nvmefc_ls_req *pnvme_lsreq)
+ {
+ 	struct lpfc_nvmet_tgtport *lpfc_nvmet = targetport->private;
+ 	struct lpfc_hba *phba;
+ 	struct lpfc_nodelist *ndlp;
+ 	int ret;
+ 	u32 hstate;
+ 
+ 	if (!lpfc_nvmet)
+ 		return -EINVAL;
+ 
+ 	phba = lpfc_nvmet->phba;
+ 	if (phba->pport->load_flag & FC_UNLOADING)
+ 		return -EINVAL;
+ 
+ 	hstate = atomic_read(&lpfc_nvmet->state);
+ 	if (hstate == LPFC_NVMET_INV_HOST_ACTIVE)
+ 		return -EACCES;
+ 
+ 	ndlp = (struct lpfc_nodelist *)hosthandle;
+ 
+ 	ret = __lpfc_nvme_ls_req(phba->pport, ndlp, pnvme_lsreq,
+ 				 lpfc_nvmet_ls_req_cmp);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * lpfc_nvmet_ls_abort - Abort a prior NVME LS request
+  * @targetport: Transport targetport, that LS was issued from.
+  * @hosthandle - hosthandle set by the driver in a prior ls_rqst_rcv.
+  *               Driver sets this value to the ndlp pointer.
+  * @pnvme_lsreq - the transport nvme_ls_req structure for LS to be aborted
+  *
+  * Driver registers this routine to abort an NVME LS request that is
+  * in progress (from the transports perspective).
+  **/
+ static void
+ lpfc_nvmet_ls_abort(struct nvmet_fc_target_port *targetport,
+ 		    void *hosthandle,
+ 		    struct nvmefc_ls_req *pnvme_lsreq)
+ {
+ 	struct lpfc_nvmet_tgtport *lpfc_nvmet = targetport->private;
+ 	struct lpfc_hba *phba;
+ 	struct lpfc_nodelist *ndlp;
+ 	int ret;
+ 
+ 	phba = lpfc_nvmet->phba;
+ 	if (phba->pport->load_flag & FC_UNLOADING)
+ 		return;
+ 
+ 	ndlp = (struct lpfc_nodelist *)hosthandle;
+ 
+ 	ret = __lpfc_nvme_ls_abort(phba->pport, ndlp, pnvme_lsreq);
+ 	if (!ret)
+ 		atomic_inc(&lpfc_nvmet->xmt_ls_abort);
+ }
+ 
+ static void
+ lpfc_nvmet_host_release(void *hosthandle)
+ {
+ 	struct lpfc_nodelist *ndlp = hosthandle;
+ 	struct lpfc_hba *phba = NULL;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 
+ 	phba = ndlp->phba;
+ 	if (!phba->targetport || !phba->targetport->private)
+ 		return;
+ 
+ 	lpfc_printf_log(phba, KERN_ERR, LOG_NVME,
+ 			"6202 NVMET XPT releasing hosthandle x%px\n",
+ 			hosthandle);
+ 	tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
+ 	atomic_set(&tgtp->state, 0);
+ }
+ 
+ static void
+ lpfc_nvmet_discovery_event(struct nvmet_fc_target_port *tgtport)
+ {
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	struct lpfc_hba *phba;
+ 	uint32_t rc;
+ 
+ 	tgtp = tgtport->private;
+ 	phba = tgtp->phba;
+ 
+ 	rc = lpfc_issue_els_rscn(phba->pport, 0);
+ 	lpfc_printf_log(phba, KERN_ERR, LOG_NVME,
+ 			"6420 NVMET subsystem change: Notification %s\n",
+ 			(rc) ? "Failed" : "Sent");
+ }
+ 
++>>>>>>> 54840bed372c (lpfc: nvmet: Add Send LS Request and Abort LS Request support)
  static struct nvmet_fc_target_template lpfc_tgttemplate = {
  	.targetport_delete = lpfc_nvmet_targetport_delete,
  	.xmt_ls_rsp     = lpfc_nvmet_xmt_ls_rsp,
@@@ -1179,6 -1415,10 +1308,13 @@@
  	.fcp_abort      = lpfc_nvmet_xmt_fcp_abort,
  	.fcp_req_release = lpfc_nvmet_xmt_fcp_release,
  	.defer_rcv	= lpfc_nvmet_defer_rcv,
++<<<<<<< HEAD
++=======
+ 	.discovery_event = lpfc_nvmet_discovery_event,
+ 	.ls_req         = lpfc_nvmet_ls_req,
+ 	.ls_abort       = lpfc_nvmet_ls_abort,
+ 	.host_release   = lpfc_nvmet_host_release,
++>>>>>>> 54840bed372c (lpfc: nvmet: Add Send LS Request and Abort LS Request support)
  
  	.max_hw_queues  = 1,
  	.max_sgl_segments = LPFC_NVMET_DEFAULT_SEGS,
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
