KVM: x86: acknowledgment mechanism for async pf page ready notifications

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 557a961abbe06ed9dfd3b55ef7bd6e68295cda3d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/557a961a.failed

If two page ready notifications happen back to back the second one is not
delivered and the only mechanism we currently have is
kvm_check_async_pf_completion() check in vcpu_run() loop. The check will
only be performed with the next vmexit when it happens and in some cases
it may take a while. With interrupt based page ready notification delivery
the situation is even worse: unlike exceptions, interrupts are not handled
immediately so we must check if the slot is empty. This is slow and
unnecessary. Introduce dedicated MSR_KVM_ASYNC_PF_ACK MSR to communicate
the fact that the slot is free and host should check its notification
queue. Mandate using it for interrupt based 'page ready' APF event
delivery.

As kvm_check_async_pf_completion() is going away from vcpu_run() we need
a way to communicate the fact that vcpu->async_pf.done queue has
transitioned from empty to non-empty state. Introduce
kvm_arch_async_page_present_queued() and KVM_REQ_APF_READY to do the job.

	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
Message-Id: <20200525144125.143875-7-vkuznets@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 557a961abbe06ed9dfd3b55ef7bd6e68295cda3d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/virt/kvm/msr.rst
#	arch/x86/include/uapi/asm/kvm_para.h
#	arch/x86/kvm/x86.c
diff --cc Documentation/virt/kvm/msr.rst
index 33892036672d,9b107889b033..000000000000
--- a/Documentation/virt/kvm/msr.rst
+++ b/Documentation/virt/kvm/msr.rst
@@@ -190,35 -190,63 +190,89 @@@ MSR_KVM_ASYNC_PF_EN
  	0x4b564d02
  
  data:
 -	Asynchronous page fault (APF) control MSR.
 -
 -	Bits 63-6 hold 64-byte aligned physical address of a 64 byte memory area
 -	which must be in guest RAM and must be zeroed. This memory is expected
 -	to hold a copy of the following structure::
 -
 +	Bits 63-6 hold 64-byte aligned physical address of a
 +	64 byte memory area which must be in guest RAM and must be
 +	zeroed. Bits 5-3 are reserved and should be zero. Bit 0 is 1
 +	when asynchronous page faults are enabled on the vcpu 0 when
 +	disabled. Bit 1 is 1 if asynchronous page faults can be injected
 +	when vcpu is in cpl == 0. Bit 2 is 1 if asynchronous page faults
 +	are delivered to L1 as #PF vmexits.  Bit 2 can be set only if
 +	KVM_FEATURE_ASYNC_PF_VMEXIT is present in CPUID.
 +
 +	First 4 byte of 64 byte memory location will be written to by
 +	the hypervisor at the time of asynchronous page fault (APF)
 +	injection to indicate type of asynchronous page fault. Value
 +	of 1 means that the page referred to by the page fault is not
 +	present. Value 2 means that the page is now available. Disabling
 +	interrupt inhibits APFs. Guest must not enable interrupt
 +	before the reason is read, or it may be overwritten by another
 +	APF. Since APF uses the same exception vector as regular page
 +	fault guest must reset the reason to 0 before it does
 +	something that can generate normal page fault.  If during page
 +	fault APF reason is 0 it means that this is regular page
 +	fault.
 +
++<<<<<<< HEAD
 +	During delivery of type 1 APF cr2 contains a token that will
 +	be used to notify a guest when missing page becomes
 +	available. When page becomes available type 2 APF is sent with
 +	cr2 set to the token associated with the page. There is special
 +	kind of token 0xffffffff which tells vcpu that it should wake
 +	up all processes waiting for APFs and no individual type 2 APFs
 +	will be sent.
++=======
+ 	  struct kvm_vcpu_pv_apf_data {
+ 		/* Used for 'page not present' events delivered via #PF */
+ 		__u32 flags;
+ 
+ 		/* Used for 'page ready' events delivered via interrupt notification */
+ 		__u32 token;
+ 
+ 		__u8 pad[56];
+ 		__u32 enabled;
+ 	  };
+ 
+ 	Bits 5-4 of the MSR are reserved and should be zero. Bit 0 is set to 1
+ 	when asynchronous page faults are enabled on the vcpu, 0 when disabled.
+ 	Bit 1 is 1 if asynchronous page faults can be injected when vcpu is in
+ 	cpl == 0. Bit 2 is 1 if asynchronous page faults are delivered to L1 as
+ 	#PF vmexits.  Bit 2 can be set only if KVM_FEATURE_ASYNC_PF_VMEXIT is
+ 	present in CPUID. Bit 3 enables interrupt based delivery of 'page ready'
+ 	events.
+ 
+ 	'Page not present' events are currently always delivered as synthetic
+ 	#PF exception. During delivery of these events APF CR2 register contains
+ 	a token that will be used to notify the guest when missing page becomes
+ 	available. Also, to make it possible to distinguish between real #PF and
+ 	APF, first 4 bytes of 64 byte memory location ('flags') will be written
+ 	to by the hypervisor at the time of injection. Only first bit of 'flags'
+ 	is currently supported, when set, it indicates that the guest is dealing
+ 	with asynchronous 'page not present' event. If during a page fault APF
+ 	'flags' is '0' it means that this is regular page fault. Guest is
+ 	supposed to clear 'flags' when it is done handling #PF exception so the
+ 	next event can be delivered.
+ 
+ 	Note, since APF 'page not present' events use the same exception vector
+ 	as regular page fault, guest must reset 'flags' to '0' before it does
+ 	something that can generate normal page fault.
+ 
+ 	Bytes 5-7 of 64 byte memory location ('token') will be written to by the
+ 	hypervisor at the time of APF 'page ready' event injection. The content
+ 	of these bytes is a token which was previously delivered as 'page not
+ 	present' event. The event indicates the page in now available. Guest is
+ 	supposed to write '0' to 'token' when it is done handling 'page ready'
+ 	event and to write 1' to MSR_KVM_ASYNC_PF_ACK after clearing the location;
+ 	writing to the MSR forces KVM to re-scan its queue and deliver the next
+ 	pending notification.
+ 
+ 	Note, MSR_KVM_ASYNC_PF_INT MSR specifying the interrupt vector for 'page
+ 	ready' APF delivery needs to be written to before enabling APF mechanism
+ 	in MSR_KVM_ASYNC_PF_EN or interrupt #0 can get injected.
+ 
+ 	Note, previously, 'page ready' events were delivered via the same #PF
+ 	exception as 'page not present' events but this is now deprecated. If
+ 	bit 3 (interrupt based delivery) is not set APF events are not delivered.
++>>>>>>> 557a961abbe0 (KVM: x86: acknowledgment mechanism for async pf page ready notifications)
  
  	If APF is disabled while there are outstanding APFs, they will
  	not be delivered.
@@@ -319,3 -348,27 +373,30 @@@ data
  
  	KVM guests can request the host not to poll on HLT, for example if
  	they are performing polling themselves.
++<<<<<<< HEAD
++=======
+ 
+ MSR_KVM_ASYNC_PF_INT:
+ 	0x4b564d06
+ 
+ data:
+ 	Second asynchronous page fault (APF) control MSR.
+ 
+ 	Bits 0-7: APIC vector for delivery of 'page ready' APF events.
+ 	Bits 8-63: Reserved
+ 
+ 	Interrupt vector for asynchnonous 'page ready' notifications delivery.
+ 	The vector has to be set up before asynchronous page fault mechanism
+ 	is enabled in MSR_KVM_ASYNC_PF_EN.
+ 
+ MSR_KVM_ASYNC_PF_ACK:
+ 	0x4b564d07
+ 
+ data:
+ 	Asynchronous page fault (APF) acknowledgment.
+ 
+ 	When the guest is done processing 'page ready' APF event and 'token'
+ 	field in 'struct kvm_vcpu_pv_apf_data' is cleared it is supposed to
+ 	write '1' to bit 0 of the MSR, this causes the host to re-scan its queue
+ 	and check if there are more notifications pending.
++>>>>>>> 557a961abbe0 (KVM: x86: acknowledgment mechanism for async pf page ready notifications)
diff --cc arch/x86/include/uapi/asm/kvm_para.h
index 2a8e0b6b9805,7ac20df80ba8..000000000000
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@@ -50,6 -50,8 +50,11 @@@
  #define MSR_KVM_STEAL_TIME  0x4b564d03
  #define MSR_KVM_PV_EOI_EN      0x4b564d04
  #define MSR_KVM_POLL_CONTROL	0x4b564d05
++<<<<<<< HEAD
++=======
+ #define MSR_KVM_ASYNC_PF_INT	0x4b564d06
+ #define MSR_KVM_ASYNC_PF_ACK	0x4b564d07
++>>>>>>> 557a961abbe0 (KVM: x86: acknowledgment mechanism for async pf page ready notifications)
  
  struct kvm_steal_time {
  	__u64 steal;
diff --cc arch/x86/kvm/x86.c
index acc910a907d8,c9d709a672f3..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -1244,7 -1248,7 +1244,11 @@@ static const u32 emulated_msrs_all[] = 
  	HV_X64_MSR_TSC_EMULATION_STATUS,
  
  	MSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME,
++<<<<<<< HEAD
 +	MSR_KVM_PV_EOI_EN,
++=======
+ 	MSR_KVM_PV_EOI_EN, MSR_KVM_ASYNC_PF_INT, MSR_KVM_ASYNC_PF_ACK,
++>>>>>>> 557a961abbe0 (KVM: x86: acknowledgment mechanism for async pf page ready notifications)
  
  	MSR_IA32_TSC_ADJUST,
  	MSR_IA32_TSCDEADLINE,
@@@ -2913,6 -2942,16 +2917,19 @@@ int kvm_set_msr_common(struct kvm_vcpu 
  		if (kvm_pv_enable_async_pf(vcpu, data))
  			return 1;
  		break;
++<<<<<<< HEAD
++=======
+ 	case MSR_KVM_ASYNC_PF_INT:
+ 		if (kvm_pv_enable_async_pf_int(vcpu, data))
+ 			return 1;
+ 		break;
+ 	case MSR_KVM_ASYNC_PF_ACK:
+ 		if (data & 0x1) {
+ 			vcpu->arch.apf.pageready_pending = false;
+ 			kvm_check_async_pf_completion(vcpu);
+ 		}
+ 		break;
++>>>>>>> 557a961abbe0 (KVM: x86: acknowledgment mechanism for async pf page ready notifications)
  	case MSR_KVM_STEAL_TIME:
  
  		if (unlikely(!sched_info_on()))
@@@ -3187,8 -3226,14 +3204,11 @@@ int kvm_get_msr_common(struct kvm_vcpu 
  		msr_info->data = vcpu->arch.time;
  		break;
  	case MSR_KVM_ASYNC_PF_EN:
 -		msr_info->data = vcpu->arch.apf.msr_en_val;
 -		break;
 -	case MSR_KVM_ASYNC_PF_INT:
 -		msr_info->data = vcpu->arch.apf.msr_int_val;
 +		msr_info->data = vcpu->arch.apf.msr_val;
  		break;
+ 	case MSR_KVM_ASYNC_PF_ACK:
+ 		msr_info->data = 0;
+ 		break;
  	case MSR_KVM_STEAL_TIME:
  		msr_info->data = vcpu->arch.st.msr_val;
  		break;
@@@ -10471,27 -10563,29 +10491,43 @@@ void kvm_arch_async_page_present(struc
  		kvm_del_async_pf_gfn(vcpu, work->arch.gfn);
  	trace_kvm_async_pf_ready(work->arch.token, work->cr2_or_gpa);
  
++<<<<<<< HEAD
 +	if ((work->wakeup_all || work->notpresent_injected) &&
 +	    vcpu->arch.apf.msr_val & KVM_ASYNC_PF_ENABLED &&
 +	    !apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)) {
 +			fault.vector = PF_VECTOR;
 +			fault.error_code_valid = true;
 +			fault.error_code = 0;
 +			fault.nested_page_fault = false;
 +			fault.address = work->arch.token;
 +			fault.async_page_fault = true;
 +			kvm_inject_page_fault(vcpu, &fault);
 +	}
++=======
+ 	if (kvm_pv_async_pf_enabled(vcpu) &&
+ 	    !apf_put_user_ready(vcpu, work->arch.token)) {
+ 		vcpu->arch.apf.pageready_pending = true;
+ 		kvm_apic_set_irq(vcpu, &irq, NULL);
+ 	}
+ 
++>>>>>>> 557a961abbe0 (KVM: x86: acknowledgment mechanism for async pf page ready notifications)
  	vcpu->arch.apf.halted = false;
  	vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
  }
  
+ void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)
+ {
+ 	kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ 	if (!vcpu->arch.apf.pageready_pending)
+ 		kvm_vcpu_kick(vcpu);
+ }
+ 
  bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
  {
 -	if (!kvm_pv_async_pf_enabled(vcpu))
 +	if (!(vcpu->arch.apf.msr_val & KVM_ASYNC_PF_ENABLED))
  		return true;
  	else
 -		return apf_pageready_slot_free(vcpu);
 +		return kvm_can_do_async_pf(vcpu);
  }
  
  void kvm_arch_start_assignment(struct kvm *kvm)
* Unmerged path Documentation/virt/kvm/msr.rst
diff --git a/arch/s390/include/asm/kvm_host.h b/arch/s390/include/asm/kvm_host.h
index 5edacad5d30d..586258ec6271 100644
--- a/arch/s390/include/asm/kvm_host.h
+++ b/arch/s390/include/asm/kvm_host.h
@@ -982,6 +982,8 @@ bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);
 
+static inline void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu) {}
+
 void kvm_arch_crypto_clear_masks(struct kvm *kvm);
 void kvm_arch_crypto_set_masks(struct kvm *kvm, unsigned long *apm,
 			       unsigned long *aqm, unsigned long *adm);
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5e5db7bc5e26..466619476862 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -89,6 +89,7 @@
 #define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
 #define KVM_REQ_HV_TLB_FLUSH \
 	KVM_ARCH_REQ_FLAGS(27, KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_APF_READY		KVM_ARCH_REQ(28)
 
 #define CR0_RESERVED_BITS                                               \
 	(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \
@@ -776,6 +777,7 @@ struct kvm_vcpu_arch {
 		u32 host_apf_reason;
 		unsigned long nested_apf_token;
 		bool delivery_as_pf_vmexit;
+		bool pageready_pending;
 	} apf;
 
 	/* OSVW MSRs (AMD only) */
@@ -1660,6 +1662,7 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 			       struct kvm_async_pf *work);
+void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu);
 bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
* Unmerged path arch/x86/include/uapi/asm/kvm_para.h
* Unmerged path arch/x86/kvm/x86.c
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index 3fd130499ee3..8b8161f83bfb 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -63,6 +63,7 @@ static void async_pf_execute(struct work_struct *work)
 	unsigned long addr = apf->addr;
 	gpa_t cr2_or_gpa = apf->cr2_or_gpa;
 	int locked = 1;
+	bool first;
 
 	might_sleep();
 
@@ -81,10 +82,14 @@ static void async_pf_execute(struct work_struct *work)
 		kvm_arch_async_page_present(vcpu, apf);
 
 	spin_lock(&vcpu->async_pf.lock);
+	first = list_empty(&vcpu->async_pf.done);
 	list_add_tail(&apf->link, &vcpu->async_pf.done);
 	apf->vcpu = NULL;
 	spin_unlock(&vcpu->async_pf.lock);
 
+	if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+		kvm_arch_async_page_present_queued(vcpu);
+
 	/*
 	 * apf may be freed by kvm_check_async_pf_completion() after
 	 * this point
@@ -207,6 +212,7 @@ int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
+	bool first;
 
 	if (!list_empty_careful(&vcpu->async_pf.done))
 		return 0;
@@ -219,9 +225,13 @@ int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)
 	INIT_LIST_HEAD(&work->queue); /* for list_del to work */
 
 	spin_lock(&vcpu->async_pf.lock);
+	first = list_empty(&vcpu->async_pf.done);
 	list_add_tail(&work->link, &vcpu->async_pf.done);
 	spin_unlock(&vcpu->async_pf.lock);
 
+	if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
+		kvm_arch_async_page_present_queued(vcpu);
+
 	vcpu->async_pf.queued++;
 	return 0;
 }
