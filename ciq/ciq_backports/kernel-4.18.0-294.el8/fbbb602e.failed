mm: deactivations shouldn't bias the LRU balance

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit fbbb602e40c270e884bc545161b238074b20aaae
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/fbbb602e.failed

Operations like MADV_FREE, FADV_DONTNEED etc.  currently move any affected
active pages to the inactive list to accelerate their reclaim (good) but
also steer page reclaim toward that LRU type, or away from the other
(bad).

The reason why this is undesirable is that such operations are not part of
the regular page aging cycle, and rather a fluke that doesn't say much
about the remaining pages on that list; they might all be in heavy use,
and once the chunk of easy victims has been purged, the VM continues to
apply elevated pressure on those remaining hot pages.  The other LRU,
meanwhile, might have easily reclaimable pages, and there was never a need
to steer away from it in the first place.

As the previous patch outlined, we should focus on recording actually
observed cost to steer the balance rather than speculating about the
potential value of one LRU list over the other.  In that spirit, leave
explicitely deactivated pages to the LRU algorithm to pick up, and let
rotations decide which list is the easiest to reclaim.

[cai@lca.pw: fix set-but-not-used warning]
  Link: http://lkml.kernel.org/r/20200522133335.GA624@Qians-MacBook-Air.local
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Qian Cai <cai@lca.pw>
Link: http://lkml.kernel.org/r/20200520232525.798933-10-hannes@cmpxchg.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit fbbb602e40c270e884bc545161b238074b20aaae)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap.c
diff --cc mm/swap.c
index 70728521e27e,7d552af25797..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -519,7 -512,6 +519,10 @@@ static void lru_deactivate_file_fn(stru
  		return;
  
  	active = PageActive(page);
++<<<<<<< HEAD
 +	file = page_is_file_cache(page);
++=======
++>>>>>>> fbbb602e40c2 (mm: deactivations shouldn't bias the LRU balance)
  	lru = page_lru_base_type(page);
  
  	del_page_from_lru_list(page, lruvec, lru + active);
@@@ -545,14 -537,12 +548,20 @@@
  
  	if (active)
  		__count_vm_event(PGDEACTIVATE);
++<<<<<<< HEAD
 +	update_page_reclaim_stat(lruvec, file, 0);
++=======
++>>>>>>> fbbb602e40c2 (mm: deactivations shouldn't bias the LRU balance)
  }
  
  static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,
  			    void *arg)
  {
  	if (PageLRU(page) && PageActive(page) && !PageUnevictable(page)) {
++<<<<<<< HEAD
 +		int file = page_is_file_cache(page);
++=======
++>>>>>>> fbbb602e40c2 (mm: deactivations shouldn't bias the LRU balance)
  		int lru = page_lru_base_type(page);
  
  		del_page_from_lru_list(page, lruvec, lru + LRU_ACTIVE);
@@@ -561,7 -551,6 +570,10 @@@
  		add_page_to_lru_list(page, lruvec, lru);
  
  		__count_vm_events(PGDEACTIVATE, hpage_nr_pages(page));
++<<<<<<< HEAD
 +		update_page_reclaim_stat(lruvec, file, 0);
++=======
++>>>>>>> fbbb602e40c2 (mm: deactivations shouldn't bias the LRU balance)
  	}
  }
  
@@@ -586,7 -575,6 +598,10 @@@ static void lru_lazyfree_fn(struct pag
  
  		__count_vm_events(PGLAZYFREE, hpage_nr_pages(page));
  		count_memcg_page_event(page, PGLAZYFREE);
++<<<<<<< HEAD
 +		update_page_reclaim_stat(lruvec, 1, 0);
++=======
++>>>>>>> fbbb602e40c2 (mm: deactivations shouldn't bias the LRU balance)
  	}
  }
  
* Unmerged path mm/swap.c
