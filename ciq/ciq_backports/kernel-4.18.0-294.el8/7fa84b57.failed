RDMA/mlx5: Initialize QP mutex for the debug kernels

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit 7fa84b5708cf359efe43ce99fd1f3c8765cacd23
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/7fa84b57.failed

In DCT and RSS RAW QP creation flows, the QP mutex wasn't initialized and
the magic field inside lock was missing. This caused to the following
kernel warning for kernels build with CONFIG_DEBUG_MUTEXES.

 DEBUG_LOCKS_WARN_ON(lock->magic != lock)
 WARNING: CPU: 3 PID: 16261 at kernel/locking/mutex.c:938 __mutex_lock+0x60e/0x940
 Modules linked in: bonding nf_tables ipip tunnel4 geneve ip6_udp_tunnel udp_tunnel ip6_gre ip6_tunnel tunnel6 ip_gre gre ip_tunnel mlx5_ib mlx5_core mlxfw ptp pps_core rdma_ucm ib_uverbs ib_ipoib ib_umad openvswitch nsh xt_MASQUERADE nf_conntrack_netlink nfnetlink iptable_nat xt_addrtype xt_conntrack nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 br_netfilter overlay ib_srp scsi_transport_srp rpcrdma ib_iser libiscsi scsi_transport_iscsi rdma_cm iw_cm ib_cm ib_core [last unloaded: mlxfw]
 CPU: 3 PID: 16261 Comm: ib_send_bw Not tainted 5.8.0-rc4_for_upstream_min_debug_2020_07_08_22_04 #1
 Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS rel-1.12.1-0-ga5cab58e9a3f-prebuilt.qemu.org 04/01/2014
 RIP: 0010:__mutex_lock+0x60e/0x940
 Code: c0 0f 84 6d fa ff ff 44 8b 15 4e 9d ba 00 45 85 d2 0f 85 5d fa ff ff 48 c7 c6 f2 de 2b 82 48 c7 c7 f1 8a 2b 82 e8 d2 4d 72 ff <0f> 0b 4c 8b 4d 88 e9 3f fa ff ff f6 c2 04 0f 84 37 fe ff ff 48 89
 RSP: 0018:ffff88810bb8b870 EFLAGS: 00010286
 RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000
 RDX: ffff88829f1dd880 RSI: 0000000000000000 RDI: ffffffff81192afa
 RBP: ffff88810bb8b910 R08: 0000000000000000 R09: 0000000000000028
 R10: 0000000000000000 R11: 0000000000003f85 R12: 0000000000000002
 R13: ffff88827d8d3ce0 R14: ffffffffa059f615 R15: ffff8882a4d02610
 FS:  00007f3f6988e740(0000) GS:ffff8882f5b80000(0000) knlGS:0000000000000000
 CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
 CR2: 0000556556158000 CR3: 000000010a63c005 CR4: 0000000000360ea0
 DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
 DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
 Call Trace:
  ? cmd_exec+0x947/0xe60 [mlx5_core]
  ? __mutex_lock+0x76/0x940
  ? mlx5_ib_qp_set_counter+0x25/0xa0 [mlx5_ib]
  mlx5_ib_qp_set_counter+0x25/0xa0 [mlx5_ib]
  mlx5_ib_counter_bind_qp+0x9b/0xe0 [mlx5_ib]
  __rdma_counter_bind_qp+0x6b/0xa0 [ib_core]
  rdma_counter_bind_qp_auto+0x363/0x520 [ib_core]
  _ib_modify_qp+0x316/0x580 [ib_core]
  ib_modify_qp_with_udata+0x19/0x30 [ib_core]
  modify_qp+0x4c4/0x600 [ib_uverbs]
  ib_uverbs_ex_modify_qp+0x87/0xe0 [ib_uverbs]
  ib_uverbs_handler_UVERBS_METHOD_INVOKE_WRITE+0x129/0x1c0 [ib_uverbs]
  ib_uverbs_cmd_verbs.isra.5+0x5d5/0x11f0 [ib_uverbs]
  ? ib_uverbs_handler_UVERBS_METHOD_QUERY_CONTEXT+0x120/0x120 [ib_uverbs]
  ? lock_acquire+0xb9/0x3a0
  ? ib_uverbs_ioctl+0xd0/0x210 [ib_uverbs]
  ? ib_uverbs_ioctl+0x175/0x210 [ib_uverbs]
  ib_uverbs_ioctl+0x14b/0x210 [ib_uverbs]
  ? ib_uverbs_ioctl+0xd0/0x210 [ib_uverbs]
  ksys_ioctl+0x234/0x7d0
  ? exc_page_fault+0x202/0x640
  ? do_syscall_64+0x1f/0x2e0
  __x64_sys_ioctl+0x16/0x20
  do_syscall_64+0x59/0x2e0
  ? asm_exc_page_fault+0x8/0x30
  ? rcu_read_lock_sched_held+0x52/0x60
  entry_SYSCALL_64_after_hwframe+0x44/0xa9

Fixes: b4aaa1f0b415 ("IB/mlx5: Handle type IB_QPT_DRIVER when creating a QP")
Link: https://lore.kernel.org/r/20200730082719.1582397-2-leon@kernel.org
	Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 7fa84b5708cf359efe43ce99fd1f3c8765cacd23)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 886e16185f47,1225b8d77510..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -1960,15 -1836,93 +1960,91 @@@ static int get_atomic_mode(struct mlx5_
  	return atomic_mode;
  }
  
 -static int create_xrc_tgt_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 -			     struct mlx5_create_qp_params *params)
 +static inline bool check_flags_mask(uint64_t input, uint64_t supported)
  {
++<<<<<<< HEAD
 +	return (input & ~supported) == 0;
++=======
+ 	struct mlx5_ib_create_qp *ucmd = params->ucmd;
+ 	struct ib_qp_init_attr *attr = params->attr;
+ 	u32 uidx = params->uidx;
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	u32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_qp_base *base;
+ 	unsigned long flags;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	in = kvzalloc(inlen, GFP_KERNEL);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	if (MLX5_CAP_GEN(mdev, ece_support) && ucmd)
+ 		MLX5_SET(create_qp_in, in, ece, ucmd->ece_options);
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, MLX5_QP_ST_XRC);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 	MLX5_SET(qpc, qpc, pd, to_mpd(devr->p0)->pdn);
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 	if (qp->flags & IB_QP_CREATE_CROSS_CHANNEL)
+ 		MLX5_SET(qpc, qpc, cd_master, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_SEND)
+ 		MLX5_SET(qpc, qpc, cd_slave_send, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_RECV)
+ 		MLX5_SET(qpc, qpc, cd_slave_receive, 1);
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, MLX5_SRQ_RQ);
+ 	MLX5_SET(qpc, qpc, no_sq, 1);
+ 	MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, cqn_snd, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, srqn_rmpn_xrqn, to_msrq(devr->s0)->msrq.srqn);
+ 	MLX5_SET(qpc, qpc, xrcd, to_mxrcd(attr->xrcd)->xrcdn);
+ 	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
+ 
+ 	/* 0xffffff means we ask to work with cqe version 0 */
+ 	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
+ 		MLX5_SET(qpc, qpc, user_index, uidx);
+ 
+ 	if (qp->flags & IB_QP_CREATE_PCI_WRITE_END_PADDING) {
+ 		MLX5_SET(qpc, qpc, end_padding_mode,
+ 			 MLX5_WQ_END_PAD_MODE_ALIGN);
+ 		/* Special case to clean flag */
+ 		qp->flags &= ~IB_QP_CREATE_PCI_WRITE_END_PADDING;
+ 	}
+ 
+ 	base = &qp->trans_qp.base;
+ 	err = mlx5_qpc_create_qp(dev, &base->mqp, in, inlen, out);
+ 	kvfree(in);
+ 	if (err)
+ 		return err;
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 	if (MLX5_CAP_GEN(mdev, ece_support))
+ 		params->resp.ece_options = MLX5_GET(create_qp_out, out, ece);
+ 
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	qp->trans_qp.xrcdn = to_mxrcd(attr->xrcd)->xrcdn;
+ 	return 0;
++>>>>>>> 7fa84b5708cf (RDMA/mlx5: Initialize QP mutex for the debug kernels)
  }
  
 -static int create_user_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 -			  struct mlx5_ib_qp *qp,
 -			  struct mlx5_create_qp_params *params)
 +static int create_qp_common(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata, struct mlx5_ib_qp *qp)
  {
 -	struct ib_qp_init_attr *init_attr = params->attr;
 -	struct mlx5_ib_create_qp *ucmd = params->ucmd;
 -	u32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};
 -	struct ib_udata *udata = params->udata;
 -	u32 uidx = params->uidx;
  	struct mlx5_ib_resources *devr = &dev->devr;
  	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
  	struct mlx5_core_dev *mdev = dev->mdev;
@@@ -2305,26 -2065,153 +2380,141 @@@
  	}
  
  	if (init_attr->qp_type == IB_QPT_RAW_PACKET ||
 -	    qp->flags & IB_QP_CREATE_SOURCE_QPN) {
 -		qp->raw_packet_qp.sq.ubuffer.buf_addr = ucmd->sq_buf_addr;
 +	    qp->flags & MLX5_IB_QP_UNDERLAY) {
 +		qp->raw_packet_qp.sq.ubuffer.buf_addr = ucmd.sq_buf_addr;
  		raw_packet_qp_copy_info(qp, &qp->raw_packet_qp);
  		err = create_raw_packet_qp(dev, qp, in, inlen, pd, udata,
++<<<<<<< HEAD
 +					   &resp);
++=======
+ 					   &params->resp);
+ 	} else
+ 		err = mlx5_qpc_create_qp(dev, &base->mqp, in, inlen, out);
+ 
+ 	kvfree(in);
+ 	if (err)
+ 		goto err_create;
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 	if (MLX5_CAP_GEN(mdev, ece_support))
+ 		params->resp.ece_options = MLX5_GET(create_qp_out, out, ece);
+ 
+ 	get_cqs(qp->type, init_attr->send_cq, init_attr->recv_cq,
+ 		&send_cq, &recv_cq);
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	mlx5_ib_lock_cqs(send_cq, recv_cq);
+ 	/* Maintain device to QPs access, needed for further handling via reset
+ 	 * flow
+ 	 */
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	/* Maintain CQ to QPs access, needed for further handling via reset flow
+ 	 */
+ 	if (send_cq)
+ 		list_add_tail(&qp->cq_send_list, &send_cq->list_send_qp);
+ 	if (recv_cq)
+ 		list_add_tail(&qp->cq_recv_list, &recv_cq->list_recv_qp);
+ 	mlx5_ib_unlock_cqs(send_cq, recv_cq);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	return 0;
+ 
+ err_create:
+ 	destroy_qp(dev, qp, base, udata);
+ 	return err;
+ }
+ 
+ static int create_kernel_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 			    struct mlx5_ib_qp *qp,
+ 			    struct mlx5_create_qp_params *params)
+ {
+ 	struct ib_qp_init_attr *attr = params->attr;
+ 	u32 uidx = params->uidx;
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	u32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_cq *send_cq;
+ 	struct mlx5_ib_cq *recv_cq;
+ 	unsigned long flags;
+ 	struct mlx5_ib_qp_base *base;
+ 	int mlx5_st;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	spin_lock_init(&qp->sq.lock);
+ 	spin_lock_init(&qp->rq.lock);
+ 
+ 	mlx5_st = to_mlx5_st(qp->type);
+ 	if (mlx5_st < 0)
+ 		return -EINVAL;
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	base = &qp->trans_qp.base;
+ 
+ 	qp->has_rq = qp_has_rq(attr);
+ 	err = set_rq_size(dev, &attr->cap, qp->has_rq, qp, NULL);
+ 	if (err) {
+ 		mlx5_ib_dbg(dev, "err %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	err = _create_kernel_qp(dev, attr, qp, &in, &inlen, base);
+ 	if (err)
+ 		return err;
+ 
+ 	if (is_sqp(attr->qp_type))
+ 		qp->port = attr->port_num;
+ 
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, mlx5_st);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 
+ 	if (attr->qp_type != MLX5_IB_QPT_REG_UMR)
+ 		MLX5_SET(qpc, qpc, pd, to_mpd(pd ? pd : devr->p0)->pdn);
+ 	else
+ 		MLX5_SET(qpc, qpc, latency_sensitive, 1);
+ 
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 
+ 	if (qp->rq.wqe_cnt) {
+ 		MLX5_SET(qpc, qpc, log_rq_stride, qp->rq.wqe_shift - 4);
+ 		MLX5_SET(qpc, qpc, log_rq_size, ilog2(qp->rq.wqe_cnt));
+ 	}
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, get_rx_type(qp, attr));
+ 
+ 	if (qp->sq.wqe_cnt)
+ 		MLX5_SET(qpc, qpc, log_sq_size, ilog2(qp->sq.wqe_cnt));
+ 	else
+ 		MLX5_SET(qpc, qpc, no_sq, 1);
+ 
+ 	if (attr->srq) {
+ 		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x0)->xrcdn);
+ 		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
+ 			 to_msrq(attr->srq)->msrq.srqn);
++>>>>>>> 7fa84b5708cf (RDMA/mlx5: Initialize QP mutex for the debug kernels)
  	} else {
 -		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x1)->xrcdn);
 -		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
 -			 to_msrq(devr->s1)->msrq.srqn);
 +		err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
  	}
  
 -	if (attr->send_cq)
 -		MLX5_SET(qpc, qpc, cqn_snd, to_mcq(attr->send_cq)->mcq.cqn);
 -
 -	if (attr->recv_cq)
 -		MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(attr->recv_cq)->mcq.cqn);
 -
 -	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
 +	if (err) {
 +		mlx5_ib_dbg(dev, "create qp failed\n");
 +		goto err_create;
 +	}
  
 -	/* 0xffffff means we ask to work with cqe version 0 */
 -	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
 -		MLX5_SET(qpc, qpc, user_index, uidx);
 +	kvfree(in);
  
 -	/* we use IB_QP_CREATE_IPOIB_UD_LSO to indicates ipoib qp */
 -	if (qp->flags & IB_QP_CREATE_IPOIB_UD_LSO)
 -		MLX5_SET(qpc, qpc, ulp_stateless_offload_mode, 1);
 +	base->container_mibqp = qp;
 +	base->mqp.event = mlx5_ib_qp_event;
  
 -	err = mlx5_qpc_create_qp(dev, &base->mqp, in, inlen, out);
 -	kvfree(in);
 -	if (err)
 -		goto err_create;
 -
 -	base->container_mibqp = qp;
 -	base->mqp.event = mlx5_ib_qp_event;
 -
 -	get_cqs(qp->type, attr->send_cq, attr->recv_cq,
 +	get_cqs(init_attr->qp_type, init_attr->send_cq, init_attr->recv_cq,
  		&send_cq, &recv_cq);
  	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
  	mlx5_ib_lock_cqs(send_cq, recv_cq);
@@@ -2795,6 -2879,152 +2985,155 @@@ static int mlx5_ib_destroy_dct(struct m
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int check_ucmd_data(struct mlx5_ib_dev *dev,
+ 			   struct mlx5_create_qp_params *params)
+ {
+ 	struct ib_udata *udata = params->udata;
+ 	size_t size, last;
+ 	int ret;
+ 
+ 	if (params->is_rss_raw)
+ 		/*
+ 		 * These QPs don't have "reserved" field in their
+ 		 * create_qp input struct, so their data is always valid.
+ 		 */
+ 		last = sizeof(struct mlx5_ib_create_qp_rss);
+ 	else
+ 		last = offsetof(struct mlx5_ib_create_qp, reserved);
+ 
+ 	if (udata->inlen <= last)
+ 		return 0;
+ 
+ 	/*
+ 	 * User provides different create_qp structures based on the
+ 	 * flow and we need to know if he cleared memory after our
+ 	 * struct create_qp ends.
+ 	 */
+ 	size = udata->inlen - last;
+ 	ret = ib_is_udata_cleared(params->udata, last, size);
+ 	if (!ret)
+ 		mlx5_ib_dbg(
+ 			dev,
+ 			"udata is not cleared, inlen = %zu, ucmd = %zu, last = %zu, size = %zu\n",
+ 			udata->inlen, params->ucmd_size, last, size);
+ 	return ret ? 0 : -EINVAL;
+ }
+ 
+ struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd, struct ib_qp_init_attr *attr,
+ 				struct ib_udata *udata)
+ {
+ 	struct mlx5_create_qp_params params = {};
+ 	struct mlx5_ib_dev *dev;
+ 	struct mlx5_ib_qp *qp;
+ 	enum ib_qp_type type;
+ 	int err;
+ 
+ 	dev = pd ? to_mdev(pd->device) :
+ 		   to_mdev(to_mxrcd(attr->xrcd)->ibxrcd.device);
+ 
+ 	err = check_qp_type(dev, attr, &type);
+ 	if (err)
+ 		return ERR_PTR(err);
+ 
+ 	err = check_valid_flow(dev, pd, attr, udata);
+ 	if (err)
+ 		return ERR_PTR(err);
+ 
+ 	if (attr->qp_type == IB_QPT_GSI)
+ 		return mlx5_ib_gsi_create_qp(pd, attr);
+ 
+ 	params.udata = udata;
+ 	params.uidx = MLX5_IB_DEFAULT_UIDX;
+ 	params.attr = attr;
+ 	params.is_rss_raw = !!attr->rwq_ind_tbl;
+ 
+ 	if (udata) {
+ 		err = process_udata_size(dev, &params);
+ 		if (err)
+ 			return ERR_PTR(err);
+ 
+ 		err = check_ucmd_data(dev, &params);
+ 		if (err)
+ 			return ERR_PTR(err);
+ 
+ 		params.ucmd = kzalloc(params.ucmd_size, GFP_KERNEL);
+ 		if (!params.ucmd)
+ 			return ERR_PTR(-ENOMEM);
+ 
+ 		err = ib_copy_from_udata(params.ucmd, udata, params.inlen);
+ 		if (err)
+ 			goto free_ucmd;
+ 	}
+ 
+ 	qp = kzalloc(sizeof(*qp), GFP_KERNEL);
+ 	if (!qp) {
+ 		err = -ENOMEM;
+ 		goto free_ucmd;
+ 	}
+ 
+ 	mutex_init(&qp->mutex);
+ 	qp->type = type;
+ 	if (udata) {
+ 		err = process_vendor_flags(dev, qp, params.ucmd, attr);
+ 		if (err)
+ 			goto free_qp;
+ 
+ 		err = get_qp_uidx(qp, &params);
+ 		if (err)
+ 			goto free_qp;
+ 	}
+ 	err = process_create_flags(dev, qp, attr);
+ 	if (err)
+ 		goto free_qp;
+ 
+ 	err = check_qp_attr(dev, qp, attr);
+ 	if (err)
+ 		goto free_qp;
+ 
+ 	err = create_qp(dev, pd, qp, &params);
+ 	if (err)
+ 		goto free_qp;
+ 
+ 	kfree(params.ucmd);
+ 	params.ucmd = NULL;
+ 
+ 	if (udata)
+ 		/*
+ 		 * It is safe to copy response for all user create QP flows,
+ 		 * including MLX5_IB_QPT_DCT, which doesn't need it.
+ 		 * In that case, resp will be filled with zeros.
+ 		 */
+ 		err = ib_copy_to_udata(udata, &params.resp, params.outlen);
+ 	if (err)
+ 		goto destroy_qp;
+ 
+ 	return &qp->ibqp;
+ 
+ destroy_qp:
+ 	if (qp->type == MLX5_IB_QPT_DCT) {
+ 		mlx5_ib_destroy_dct(qp);
+ 	} else {
+ 		/*
+ 		 * These lines below are temp solution till QP allocation
+ 		 * will be moved to be under IB/core responsiblity.
+ 		 */
+ 		qp->ibqp.send_cq = attr->send_cq;
+ 		qp->ibqp.recv_cq = attr->recv_cq;
+ 		qp->ibqp.pd = pd;
+ 		destroy_qp_common(dev, qp, udata);
+ 	}
+ 
+ 	qp = NULL;
+ free_qp:
+ 	kfree(qp);
+ free_ucmd:
+ 	kfree(params.ucmd);
+ 	return ERR_PTR(err);
+ }
+ 
++>>>>>>> 7fa84b5708cf (RDMA/mlx5: Initialize QP mutex for the debug kernels)
  int mlx5_ib_destroy_qp(struct ib_qp *qp, struct ib_udata *udata)
  {
  	struct mlx5_ib_dev *dev = to_mdev(qp->device);
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
