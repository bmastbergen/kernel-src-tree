x86/cpu: Fix typos and improve the comments in sync_core()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Ingo Molnar <mingo@kernel.org>
commit 40eb0cb4939e462acfedea8c8064571e886b9773
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/40eb0cb4.failed

- Fix typos.

- Move the compiler barrier comment to the top, because it's valid for the
  whole function, not just the legacy branch.

	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20200818053130.GA3161093@gmail.com
	Reviewed-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
(cherry picked from commit 40eb0cb4939e462acfedea8c8064571e886b9773)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/sync_core.h
diff --cc arch/x86/include/asm/sync_core.h
index c67caafd3381,0fd4a9dfb29c..000000000000
--- a/arch/x86/include/asm/sync_core.h
+++ b/arch/x86/include/asm/sync_core.h
@@@ -5,6 -5,88 +5,91 @@@
  #include <linux/preempt.h>
  #include <asm/processor.h>
  #include <asm/cpufeature.h>
++<<<<<<< HEAD
++=======
+ #include <asm/special_insns.h>
+ 
+ #ifdef CONFIG_X86_32
+ static inline void iret_to_self(void)
+ {
+ 	asm volatile (
+ 		"pushfl\n\t"
+ 		"pushl %%cs\n\t"
+ 		"pushl $1f\n\t"
+ 		"iret\n\t"
+ 		"1:"
+ 		: ASM_CALL_CONSTRAINT : : "memory");
+ }
+ #else
+ static inline void iret_to_self(void)
+ {
+ 	unsigned int tmp;
+ 
+ 	asm volatile (
+ 		"mov %%ss, %0\n\t"
+ 		"pushq %q0\n\t"
+ 		"pushq %%rsp\n\t"
+ 		"addq $8, (%%rsp)\n\t"
+ 		"pushfq\n\t"
+ 		"mov %%cs, %0\n\t"
+ 		"pushq %q0\n\t"
+ 		"pushq $1f\n\t"
+ 		"iretq\n\t"
+ 		"1:"
+ 		: "=&r" (tmp), ASM_CALL_CONSTRAINT : : "cc", "memory");
+ }
+ #endif /* CONFIG_X86_32 */
+ 
+ /*
+  * This function forces the icache and prefetched instruction stream to
+  * catch up with reality in two very specific cases:
+  *
+  *  a) Text was modified using one virtual address and is about to be executed
+  *     from the same physical page at a different virtual address.
+  *
+  *  b) Text was modified on a different CPU, may subsequently be
+  *     executed on this CPU, and you want to make sure the new version
+  *     gets executed.  This generally means you're calling this in an IPI.
+  *
+  * If you're calling this for a different reason, you're probably doing
+  * it wrong.
+  *
+  * Like all of Linux's memory ordering operations, this is a
+  * compiler barrier as well.
+  */
+ static inline void sync_core(void)
+ {
+ 	/*
+ 	 * The SERIALIZE instruction is the most straightforward way to
+ 	 * do this, but it is not universally available.
+ 	 */
+ 	if (static_cpu_has(X86_FEATURE_SERIALIZE)) {
+ 		serialize();
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * For all other processors, there are quite a few ways to do this.
+ 	 * IRET-to-self is nice because it works on every CPU, at any CPL
+ 	 * (so it's compatible with paravirtualization), and it never exits
+ 	 * to a hypervisor.  The only downsides are that it's a bit slow
+ 	 * (it seems to be a bit more than 2x slower than the fastest
+ 	 * options) and that it unmasks NMIs.  The "push %cs" is needed,
+ 	 * because in paravirtual environments __KERNEL_CS may not be a
+ 	 * valid CS value when we do IRET directly.
+ 	 *
+ 	 * In case NMI unmasking or performance ever becomes a problem,
+ 	 * the next best option appears to be MOV-to-CR2 and an
+ 	 * unconditional jump.  That sequence also works on all CPUs,
+ 	 * but it will fault at CPL3 (i.e. Xen PV).
+ 	 *
+ 	 * CPUID is the conventional way, but it's nasty: it doesn't
+ 	 * exist on some 486-like CPUs, and it usually exits to a
+ 	 * hypervisor.
+ 	 */
+ 	iret_to_self();
+ }
++>>>>>>> 40eb0cb4939e (x86/cpu: Fix typos and improve the comments in sync_core())
  
  /*
   * Ensure that a core serializing instruction is issued before returning
* Unmerged path arch/x86/include/asm/sync_core.h
