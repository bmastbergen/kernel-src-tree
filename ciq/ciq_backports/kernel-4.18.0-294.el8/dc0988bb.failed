bpf: Do not use bucket_lock for hashmap iterator

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yonghong Song <yhs@fb.com>
commit dc0988bbe1bd41e2fa555e4a6f890b819a34b49b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/dc0988bb.failed

Currently, for hashmap, the bpf iterator will grab a bucket lock, a
spinlock, before traversing the elements in the bucket. This can ensure
all bpf visted elements are valid. But this mechanism may cause
deadlock if update/deletion happens to the same bucket of the
visited map in the program. For example, if we added bpf_map_update_elem()
call to the same visited element in selftests bpf_iter_bpf_hash_map.c,
we will have the following deadlock:

  ============================================
  WARNING: possible recursive locking detected
  5.9.0-rc1+ #841 Not tainted
  --------------------------------------------
  test_progs/1750 is trying to acquire lock:
  ffff9a5bb73c5e70 (&htab->buckets[i].raw_lock){....}-{2:2}, at: htab_map_update_elem+0x1cf/0x410

  but task is already holding lock:
  ffff9a5bb73c5e20 (&htab->buckets[i].raw_lock){....}-{2:2}, at: bpf_hash_map_seq_find_next+0x94/0x120

  other info that might help us debug this:
   Possible unsafe locking scenario:

         CPU0
         ----
    lock(&htab->buckets[i].raw_lock);
    lock(&htab->buckets[i].raw_lock);

   *** DEADLOCK ***
   ...
  Call Trace:
   dump_stack+0x78/0xa0
   __lock_acquire.cold.74+0x209/0x2e3
   lock_acquire+0xba/0x380
   ? htab_map_update_elem+0x1cf/0x410
   ? __lock_acquire+0x639/0x20c0
   _raw_spin_lock_irqsave+0x3b/0x80
   ? htab_map_update_elem+0x1cf/0x410
   htab_map_update_elem+0x1cf/0x410
   ? lock_acquire+0xba/0x380
   bpf_prog_ad6dab10433b135d_dump_bpf_hash_map+0x88/0xa9c
   ? find_held_lock+0x34/0xa0
   bpf_iter_run_prog+0x81/0x16e
   __bpf_hash_map_seq_show+0x145/0x180
   bpf_seq_read+0xff/0x3d0
   vfs_read+0xad/0x1c0
   ksys_read+0x5f/0xe0
   do_syscall_64+0x33/0x40
   entry_SYSCALL_64_after_hwframe+0x44/0xa9
  ...

The bucket_lock first grabbed in seq_ops->next() called by bpf_seq_read(),
and then grabbed again in htab_map_update_elem() in the bpf program, causing
deadlocks.

Actually, we do not need bucket_lock here, we can just use rcu_read_lock()
similar to netlink iterator where the rcu_read_{lock,unlock} likes below:
 seq_ops->start():
     rcu_read_lock();
 seq_ops->next():
     rcu_read_unlock();
     /* next element */
     rcu_read_lock();
 seq_ops->stop();
     rcu_read_unlock();

Compared to old bucket_lock mechanism, if concurrent updata/delete happens,
we may visit stale elements, miss some elements, or repeat some elements.
I think this is a reasonable compromise. For users wanting to avoid
stale, missing/repeated accesses, bpf_map batch access syscall interface
can be used.

	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200902235340.2001375-1-yhs@fb.com
(cherry picked from commit dc0988bbe1bd41e2fa555e4a6f890b819a34b49b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/hashtab.c
diff --cc kernel/bpf/hashtab.c
index 496ea743bae8,7df28a45c66b..000000000000
--- a/kernel/bpf/hashtab.c
+++ b/kernel/bpf/hashtab.c
@@@ -1628,6 -1618,190 +1628,193 @@@ htab_lru_map_lookup_and_delete_batch(st
  						  true, false);
  }
  
++<<<<<<< HEAD
++=======
+ struct bpf_iter_seq_hash_map_info {
+ 	struct bpf_map *map;
+ 	struct bpf_htab *htab;
+ 	void *percpu_value_buf; // non-zero means percpu hash
+ 	u32 bucket_id;
+ 	u32 skip_elems;
+ };
+ 
+ static struct htab_elem *
+ bpf_hash_map_seq_find_next(struct bpf_iter_seq_hash_map_info *info,
+ 			   struct htab_elem *prev_elem)
+ {
+ 	const struct bpf_htab *htab = info->htab;
+ 	u32 skip_elems = info->skip_elems;
+ 	u32 bucket_id = info->bucket_id;
+ 	struct hlist_nulls_head *head;
+ 	struct hlist_nulls_node *n;
+ 	struct htab_elem *elem;
+ 	struct bucket *b;
+ 	u32 i, count;
+ 
+ 	if (bucket_id >= htab->n_buckets)
+ 		return NULL;
+ 
+ 	/* try to find next elem in the same bucket */
+ 	if (prev_elem) {
+ 		/* no update/deletion on this bucket, prev_elem should be still valid
+ 		 * and we won't skip elements.
+ 		 */
+ 		n = rcu_dereference_raw(hlist_nulls_next_rcu(&prev_elem->hash_node));
+ 		elem = hlist_nulls_entry_safe(n, struct htab_elem, hash_node);
+ 		if (elem)
+ 			return elem;
+ 
+ 		/* not found, unlock and go to the next bucket */
+ 		b = &htab->buckets[bucket_id++];
+ 		rcu_read_unlock();
+ 		skip_elems = 0;
+ 	}
+ 
+ 	for (i = bucket_id; i < htab->n_buckets; i++) {
+ 		b = &htab->buckets[i];
+ 		rcu_read_lock();
+ 
+ 		count = 0;
+ 		head = &b->head;
+ 		hlist_nulls_for_each_entry_rcu(elem, n, head, hash_node) {
+ 			if (count >= skip_elems) {
+ 				info->bucket_id = i;
+ 				info->skip_elems = count;
+ 				return elem;
+ 			}
+ 			count++;
+ 		}
+ 
+ 		rcu_read_unlock();
+ 		skip_elems = 0;
+ 	}
+ 
+ 	info->bucket_id = i;
+ 	info->skip_elems = 0;
+ 	return NULL;
+ }
+ 
+ static void *bpf_hash_map_seq_start(struct seq_file *seq, loff_t *pos)
+ {
+ 	struct bpf_iter_seq_hash_map_info *info = seq->private;
+ 	struct htab_elem *elem;
+ 
+ 	elem = bpf_hash_map_seq_find_next(info, NULL);
+ 	if (!elem)
+ 		return NULL;
+ 
+ 	if (*pos == 0)
+ 		++*pos;
+ 	return elem;
+ }
+ 
+ static void *bpf_hash_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+ {
+ 	struct bpf_iter_seq_hash_map_info *info = seq->private;
+ 
+ 	++*pos;
+ 	++info->skip_elems;
+ 	return bpf_hash_map_seq_find_next(info, v);
+ }
+ 
+ static int __bpf_hash_map_seq_show(struct seq_file *seq, struct htab_elem *elem)
+ {
+ 	struct bpf_iter_seq_hash_map_info *info = seq->private;
+ 	u32 roundup_key_size, roundup_value_size;
+ 	struct bpf_iter__bpf_map_elem ctx = {};
+ 	struct bpf_map *map = info->map;
+ 	struct bpf_iter_meta meta;
+ 	int ret = 0, off = 0, cpu;
+ 	struct bpf_prog *prog;
+ 	void __percpu *pptr;
+ 
+ 	meta.seq = seq;
+ 	prog = bpf_iter_get_info(&meta, elem == NULL);
+ 	if (prog) {
+ 		ctx.meta = &meta;
+ 		ctx.map = info->map;
+ 		if (elem) {
+ 			roundup_key_size = round_up(map->key_size, 8);
+ 			ctx.key = elem->key;
+ 			if (!info->percpu_value_buf) {
+ 				ctx.value = elem->key + roundup_key_size;
+ 			} else {
+ 				roundup_value_size = round_up(map->value_size, 8);
+ 				pptr = htab_elem_get_ptr(elem, map->key_size);
+ 				for_each_possible_cpu(cpu) {
+ 					bpf_long_memcpy(info->percpu_value_buf + off,
+ 							per_cpu_ptr(pptr, cpu),
+ 							roundup_value_size);
+ 					off += roundup_value_size;
+ 				}
+ 				ctx.value = info->percpu_value_buf;
+ 			}
+ 		}
+ 		ret = bpf_iter_run_prog(prog, &ctx);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int bpf_hash_map_seq_show(struct seq_file *seq, void *v)
+ {
+ 	return __bpf_hash_map_seq_show(seq, v);
+ }
+ 
+ static void bpf_hash_map_seq_stop(struct seq_file *seq, void *v)
+ {
+ 	if (!v)
+ 		(void)__bpf_hash_map_seq_show(seq, NULL);
+ 	else
+ 		rcu_read_unlock();
+ }
+ 
+ static int bpf_iter_init_hash_map(void *priv_data,
+ 				  struct bpf_iter_aux_info *aux)
+ {
+ 	struct bpf_iter_seq_hash_map_info *seq_info = priv_data;
+ 	struct bpf_map *map = aux->map;
+ 	void *value_buf;
+ 	u32 buf_size;
+ 
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+ 		buf_size = round_up(map->value_size, 8) * num_possible_cpus();
+ 		value_buf = kmalloc(buf_size, GFP_USER | __GFP_NOWARN);
+ 		if (!value_buf)
+ 			return -ENOMEM;
+ 
+ 		seq_info->percpu_value_buf = value_buf;
+ 	}
+ 
+ 	seq_info->map = map;
+ 	seq_info->htab = container_of(map, struct bpf_htab, map);
+ 	return 0;
+ }
+ 
+ static void bpf_iter_fini_hash_map(void *priv_data)
+ {
+ 	struct bpf_iter_seq_hash_map_info *seq_info = priv_data;
+ 
+ 	kfree(seq_info->percpu_value_buf);
+ }
+ 
+ static const struct seq_operations bpf_hash_map_seq_ops = {
+ 	.start	= bpf_hash_map_seq_start,
+ 	.next	= bpf_hash_map_seq_next,
+ 	.stop	= bpf_hash_map_seq_stop,
+ 	.show	= bpf_hash_map_seq_show,
+ };
+ 
+ static const struct bpf_iter_seq_info iter_seq_info = {
+ 	.seq_ops		= &bpf_hash_map_seq_ops,
+ 	.init_seq_private	= bpf_iter_init_hash_map,
+ 	.fini_seq_private	= bpf_iter_fini_hash_map,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_hash_map_info),
+ };
+ 
+ static int htab_map_btf_id;
++>>>>>>> dc0988bbe1bd (bpf: Do not use bucket_lock for hashmap iterator)
  const struct bpf_map_ops htab_map_ops = {
  	.map_alloc_check = htab_map_alloc_check,
  	.map_alloc = htab_map_alloc,
* Unmerged path kernel/bpf/hashtab.c
