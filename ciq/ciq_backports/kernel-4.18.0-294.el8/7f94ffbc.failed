kasan: add hooks implementation for tag-based mode

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Andrey Konovalov <andreyknvl@google.com>
commit 7f94ffbc4c6a1bdb51d39965e4f2acaa19bd798f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/7f94ffbc.failed

This commit adds tag-based KASAN specific hooks implementation and
adjusts common generic and tag-based KASAN ones.

1. When a new slab cache is created, tag-based KASAN rounds up the size of
   the objects in this cache to KASAN_SHADOW_SCALE_SIZE (== 16).

2. On each kmalloc tag-based KASAN generates a random tag, sets the shadow
   memory, that corresponds to this object to this tag, and embeds this
   tag value into the top byte of the returned pointer.

3. On each kfree tag-based KASAN poisons the shadow memory with a random
   tag to allow detection of use-after-free bugs.

The rest of the logic of the hook implementation is very much similar to
the one provided by generic KASAN. Tag-based KASAN saves allocation and
free stack metadata to the slab object the same way generic KASAN does.

Link: http://lkml.kernel.org/r/bda78069e3b8422039794050ddcb2d53d053ed41.1544099024.git.andreyknvl@google.com
	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Mark Rutland <mark.rutland@arm.com>
	Cc: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 7f94ffbc4c6a1bdb51d39965e4f2acaa19bd798f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/kasan/kasan.c
#	mm/kasan/kasan.h
#	mm/kasan/tags.c
diff --cc mm/kasan/kasan.c
index 2913fdd4758a,27f0cae336c9..000000000000
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@@ -116,199 -218,11 +136,200 @@@ void kasan_unpoison_stack_above_sp_to(c
  	kasan_unpoison_shadow(sp, size);
  }
  
 +/*
 + * All functions below always inlined so compiler could
 + * perform better optimizations in each of __asan_loadX/__assn_storeX
 + * depending on memory access size X.
 + */
 +
 +static __always_inline bool memory_is_poisoned_1(unsigned long addr)
 +{
 +	s8 shadow_value = *(s8 *)kasan_mem_to_shadow((void *)addr);
 +
 +	if (unlikely(shadow_value)) {
 +		s8 last_accessible_byte = addr & KASAN_SHADOW_MASK;
 +		return unlikely(last_accessible_byte >= shadow_value);
 +	}
 +
 +	return false;
 +}
 +
 +static __always_inline bool memory_is_poisoned_2_4_8(unsigned long addr,
 +						unsigned long size)
 +{
 +	u8 *shadow_addr = (u8 *)kasan_mem_to_shadow((void *)addr);
 +
 +	/*
 +	 * Access crosses 8(shadow size)-byte boundary. Such access maps
 +	 * into 2 shadow bytes, so we need to check them both.
 +	 */
 +	if (unlikely(((addr + size - 1) & KASAN_SHADOW_MASK) < size - 1))
 +		return *shadow_addr || memory_is_poisoned_1(addr + size - 1);
 +
 +	return memory_is_poisoned_1(addr + size - 1);
 +}
 +
 +static __always_inline bool memory_is_poisoned_16(unsigned long addr)
 +{
 +	u16 *shadow_addr = (u16 *)kasan_mem_to_shadow((void *)addr);
 +
 +	/* Unaligned 16-bytes access maps into 3 shadow bytes. */
 +	if (unlikely(!IS_ALIGNED(addr, KASAN_SHADOW_SCALE_SIZE)))
 +		return *shadow_addr || memory_is_poisoned_1(addr + 15);
 +
 +	return *shadow_addr;
 +}
 +
 +static __always_inline unsigned long bytes_is_nonzero(const u8 *start,
 +					size_t size)
 +{
 +	while (size) {
 +		if (unlikely(*start))
 +			return (unsigned long)start;
 +		start++;
 +		size--;
 +	}
 +
 +	return 0;
 +}
 +
 +static __always_inline unsigned long memory_is_nonzero(const void *start,
 +						const void *end)
 +{
 +	unsigned int words;
 +	unsigned long ret;
 +	unsigned int prefix = (unsigned long)start % 8;
 +
 +	if (end - start <= 16)
 +		return bytes_is_nonzero(start, end - start);
 +
 +	if (prefix) {
 +		prefix = 8 - prefix;
 +		ret = bytes_is_nonzero(start, prefix);
 +		if (unlikely(ret))
 +			return ret;
 +		start += prefix;
 +	}
 +
 +	words = (end - start) / 8;
 +	while (words) {
 +		if (unlikely(*(u64 *)start))
 +			return bytes_is_nonzero(start, 8);
 +		start += 8;
 +		words--;
 +	}
 +
 +	return bytes_is_nonzero(start, (end - start) % 8);
 +}
 +
 +static __always_inline bool memory_is_poisoned_n(unsigned long addr,
 +						size_t size)
 +{
 +	unsigned long ret;
 +
 +	ret = memory_is_nonzero(kasan_mem_to_shadow((void *)addr),
 +			kasan_mem_to_shadow((void *)addr + size - 1) + 1);
 +
 +	if (unlikely(ret)) {
 +		unsigned long last_byte = addr + size - 1;
 +		s8 *last_shadow = (s8 *)kasan_mem_to_shadow((void *)last_byte);
 +
 +		if (unlikely(ret != (unsigned long)last_shadow ||
 +			((long)(last_byte & KASAN_SHADOW_MASK) >= *last_shadow)))
 +			return true;
 +	}
 +	return false;
 +}
 +
 +static __always_inline bool memory_is_poisoned(unsigned long addr, size_t size)
 +{
 +	if (__builtin_constant_p(size)) {
 +		switch (size) {
 +		case 1:
 +			return memory_is_poisoned_1(addr);
 +		case 2:
 +		case 4:
 +		case 8:
 +			return memory_is_poisoned_2_4_8(addr, size);
 +		case 16:
 +			return memory_is_poisoned_16(addr);
 +		default:
 +			BUILD_BUG();
 +		}
 +	}
 +
 +	return memory_is_poisoned_n(addr, size);
 +}
 +
 +static __always_inline void check_memory_region_inline(unsigned long addr,
 +						size_t size, bool write,
 +						unsigned long ret_ip)
 +{
 +	if (unlikely(size == 0))
 +		return;
 +
 +	if (unlikely((void *)addr <
 +		kasan_shadow_to_mem((void *)KASAN_SHADOW_START))) {
 +		kasan_report(addr, size, write, ret_ip);
 +		return;
 +	}
 +
 +	if (likely(!memory_is_poisoned(addr, size)))
 +		return;
 +
 +	kasan_report(addr, size, write, ret_ip);
 +}
 +
 +static void check_memory_region(unsigned long addr,
 +				size_t size, bool write,
 +				unsigned long ret_ip)
 +{
 +	check_memory_region_inline(addr, size, write, ret_ip);
 +}
 +
 +void kasan_check_read(const volatile void *p, unsigned int size)
 +{
 +	check_memory_region((unsigned long)p, size, false, _RET_IP_);
 +}
 +EXPORT_SYMBOL(kasan_check_read);
 +
 +void kasan_check_write(const volatile void *p, unsigned int size)
 +{
 +	check_memory_region((unsigned long)p, size, true, _RET_IP_);
 +}
 +EXPORT_SYMBOL(kasan_check_write);
 +
 +#undef memset
 +void *memset(void *addr, int c, size_t len)
 +{
 +	check_memory_region((unsigned long)addr, len, true, _RET_IP_);
 +
 +	return __memset(addr, c, len);
 +}
 +
 +#undef memmove
 +void *memmove(void *dest, const void *src, size_t len)
 +{
 +	check_memory_region((unsigned long)src, len, false, _RET_IP_);
 +	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
 +
 +	return __memmove(dest, src, len);
 +}
 +
 +#undef memcpy
 +void *memcpy(void *dest, const void *src, size_t len)
 +{
 +	check_memory_region((unsigned long)src, len, false, _RET_IP_);
 +	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
 +
 +	return __memcpy(dest, src, len);
 +}
 +
  void kasan_alloc_pages(struct page *page, unsigned int order)
  {
- 	if (likely(!PageHighMem(page)))
- 		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+ 	if (unlikely(PageHighMem(page)))
+ 		return;
+ 	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
  }
  
  void kasan_free_pages(struct page *page, unsigned int order)
@@@ -323,8 -237,11 +344,11 @@@
   * Adaptive redzone policy taken from the userspace AddressSanitizer runtime.
   * For larger allocations larger redzones are used.
   */
 -static inline unsigned int optimal_redzone(unsigned int object_size)
 +static unsigned int optimal_redzone(unsigned int object_size)
  {
+ 	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+ 		return 0;
+ 
  	return
  		object_size <= 64        - 16   ? 16 :
  		object_size <= 128       - 32   ? 32 :
@@@ -413,62 -336,30 +440,89 @@@ void kasan_poison_object_data(struct km
  			KASAN_KMALLOC_REDZONE);
  }
  
++<<<<<<< HEAD:mm/kasan/kasan.c
 +static inline int in_irqentry_text(unsigned long ptr)
 +{
 +	return (ptr >= (unsigned long)&__irqentry_text_start &&
 +		ptr < (unsigned long)&__irqentry_text_end) ||
 +		(ptr >= (unsigned long)&__softirqentry_text_start &&
 +		 ptr < (unsigned long)&__softirqentry_text_end);
 +}
 +
 +static inline void filter_irq_stacks(struct stack_trace *trace)
 +{
 +	int i;
 +
 +	if (!trace->nr_entries)
 +		return;
 +	for (i = 0; i < trace->nr_entries; i++)
 +		if (in_irqentry_text(trace->entries[i])) {
 +			/* Include the irqentry function into the stack. */
 +			trace->nr_entries = i + 1;
 +			break;
 +		}
 +}
 +
 +static inline depot_stack_handle_t save_stack(gfp_t flags)
 +{
 +	unsigned long entries[KASAN_STACK_DEPTH];
 +	struct stack_trace trace = {
 +		.nr_entries = 0,
 +		.entries = entries,
 +		.max_entries = KASAN_STACK_DEPTH,
 +		.skip = 0
 +	};
 +
 +	save_stack_trace(&trace);
 +	filter_irq_stacks(&trace);
 +
 +	return depot_save_stack(&trace, flags);
 +}
 +
 +static inline void set_track(struct kasan_track *track, gfp_t flags)
 +{
 +	track->pid = current->pid;
 +	track->stack = save_stack(flags);
 +}
 +
 +struct kasan_alloc_meta *get_alloc_info(struct kmem_cache *cache,
 +					const void *object)
 +{
 +	BUILD_BUG_ON(sizeof(struct kasan_alloc_meta) > 32);
 +	return (void *)object + cache->kasan_info.alloc_meta_offset;
 +}
 +
 +struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
 +				      const void *object)
 +{
 +	BUILD_BUG_ON(sizeof(struct kasan_free_meta) > 32);
 +	return (void *)object + cache->kasan_info.free_meta_offset;
++=======
+ /*
+  * Since it's desirable to only call object contructors once during slab
+  * allocation, we preassign tags to all such objects. Also preassign tags for
+  * SLAB_TYPESAFE_BY_RCU slabs to avoid use-after-free reports.
+  * For SLAB allocator we can't preassign tags randomly since the freelist is
+  * stored as an array of indexes instead of a linked list. Assign tags based
+  * on objects indexes, so that objects that are next to each other get
+  * different tags.
+  * After a tag is assigned, the object always gets allocated with the same tag.
+  * The reason is that we can't change tags for objects with constructors on
+  * reallocation (even for non-SLAB_TYPESAFE_BY_RCU), because the constructor
+  * code can save the pointer to the object somewhere (e.g. in the object
+  * itself). Then if we retag it, the old saved pointer will become invalid.
+  */
+ static u8 assign_tag(struct kmem_cache *cache, const void *object, bool new)
+ {
+ 	if (!cache->ctor && !(cache->flags & SLAB_TYPESAFE_BY_RCU))
+ 		return new ? KASAN_TAG_KERNEL : random_tag();
+ 
+ #ifdef CONFIG_SLAB
+ 	return (u8)obj_to_index(cache, virt_to_page(object), (void *)object);
+ #else
+ 	return new ? random_tag() : get_tag(object);
+ #endif
++>>>>>>> 7f94ffbc4c6a (kasan: add hooks implementation for tag-based mode):mm/kasan/common.c
  }
  
  void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
diff --cc mm/kasan/kasan.h
index c12dcfde2ebd,ea51b2d898ec..000000000000
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@@ -8,6 -8,11 +8,14 @@@
  #define KASAN_SHADOW_SCALE_SIZE (1UL << KASAN_SHADOW_SCALE_SHIFT)
  #define KASAN_SHADOW_MASK       (KASAN_SHADOW_SCALE_SIZE - 1)
  
++<<<<<<< HEAD
++=======
+ #define KASAN_TAG_KERNEL	0xFF /* native kernel pointers tag */
+ #define KASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
+ #define KASAN_TAG_MAX		0xFD /* maximum value for random tags */
+ 
+ #ifdef CONFIG_KASAN_GENERIC
++>>>>>>> 7f94ffbc4c6a (kasan: add hooks implementation for tag-based mode)
  #define KASAN_FREE_PAGE         0xFF  /* page was freed */
  #define KASAN_PAGE_REDZONE      0xFE  /* redzone for kmalloc_large allocations */
  #define KASAN_KMALLOC_REDZONE   0xFC  /* redzone inside slub object */
* Unmerged path mm/kasan/tags.c
* Unmerged path mm/kasan/kasan.c
* Unmerged path mm/kasan/kasan.h
* Unmerged path mm/kasan/tags.c
