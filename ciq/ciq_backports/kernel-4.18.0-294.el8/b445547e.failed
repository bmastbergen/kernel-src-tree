blk-mq, elevator: Count requests per hctx to improve performance

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Kashyap Desai <kashyap.desai@broadcom.com>
commit b445547ec1bbd3e7bf4b1c142550942f70527d95
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/b445547e.failed

High CPU utilization on "native_queued_spin_lock_slowpath" due to lock
contention is possible for mq-deadline and bfq IO schedulers
when nr_hw_queues is more than one.

It is because kblockd work queue can submit IO from all online CPUs
(through blk_mq_run_hw_queues()) even though only one hctx has pending
commands.

The elevator callback .has_work for mq-deadline and bfq scheduler considers
pending work if there are any IOs on request queue but it does not account
hctx context.

Add a per-hctx 'elevator_queued' count to the hctx to avoid triggering
the elevator even though there are no requests queued.

[jpg: Relocated atomic_dec() in dd_dispatch_request(), update commit message per Kashyap]

	Signed-off-by: Kashyap Desai <kashyap.desai@broadcom.com>
	Signed-off-by: Hannes Reinecke <hare@suse.de>
	Signed-off-by: John Garry <john.garry@huawei.com>
	Tested-by: Douglas Gilbert <dgilbert@interlog.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit b445547ec1bbd3e7bf4b1c142550942f70527d95)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/blk-mq.h
diff --cc include/linux/blk-mq.h
index 200eeffcb439,b23eeca4d677..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -14,56 -15,147 +14,63 @@@ struct blk_flush_queue
   */
  struct blk_mq_hw_ctx {
  	struct {
 -		/** @lock: Protects the dispatch list. */
  		spinlock_t		lock;
 -		/**
 -		 * @dispatch: Used for requests that are ready to be
 -		 * dispatched to the hardware but for some reason (e.g. lack of
 -		 * resources) could not be sent to the hardware. As soon as the
 -		 * driver can send new requests, requests at this list will
 -		 * be sent first for a fairer dispatch.
 -		 */
  		struct list_head	dispatch;
 -		 /**
 -		  * @state: BLK_MQ_S_* flags. Defines the state of the hw
 -		  * queue (active, scheduled to restart, stopped).
 -		  */
 -		unsigned long		state;
 +		unsigned long		state;		/* BLK_MQ_S_* flags */
  	} ____cacheline_aligned_in_smp;
  
 -	/**
 -	 * @run_work: Used for scheduling a hardware queue run at a later time.
 -	 */
  	struct delayed_work	run_work;
 -	/** @cpumask: Map of available CPUs where this hctx can run. */
  	cpumask_var_t		cpumask;
 -	/**
 -	 * @next_cpu: Used by blk_mq_hctx_next_cpu() for round-robin CPU
 -	 * selection from @cpumask.
 -	 */
  	int			next_cpu;
 -	/**
 -	 * @next_cpu_batch: Counter of how many works left in the batch before
 -	 * changing to the next CPU.
 -	 */
  	int			next_cpu_batch;
  
 -	/** @flags: BLK_MQ_F_* flags. Defines the behaviour of the queue. */
 -	unsigned long		flags;
 +	unsigned long		flags;		/* BLK_MQ_F_* flags */
  
 -	/**
 -	 * @sched_data: Pointer owned by the IO scheduler attached to a request
 -	 * queue. It's up to the IO scheduler how to use this pointer.
 -	 */
  	void			*sched_data;
 -	/**
 -	 * @queue: Pointer to the request queue that owns this hardware context.
 -	 */
  	struct request_queue	*queue;
 -	/** @fq: Queue of requests that need to perform a flush operation. */
  	struct blk_flush_queue	*fq;
  
 -	/**
 -	 * @driver_data: Pointer to data owned by the block driver that created
 -	 * this hctx
 -	 */
  	void			*driver_data;
  
 -	/**
 -	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
 -	 * pending request in that software queue.
 -	 */
  	struct sbitmap		ctx_map;
  
 -	/**
 -	 * @dispatch_from: Software queue to be used when no scheduler was
 -	 * selected.
 -	 */
  	struct blk_mq_ctx	*dispatch_from;
 -	/**
 -	 * @dispatch_busy: Number used by blk_mq_update_dispatch_busy() to
 -	 * decide if the hw_queue is busy using Exponential Weighted Moving
 -	 * Average algorithm.
 -	 */
  	unsigned int		dispatch_busy;
  
 -	/** @type: HCTX_TYPE_* flags. Type of hardware queue. */
  	unsigned short		type;
 -	/** @nr_ctx: Number of software queues. */
  	unsigned short		nr_ctx;
 -	/** @ctxs: Array of software queues. */
  	struct blk_mq_ctx	**ctxs;
  
 -	/** @dispatch_wait_lock: Lock for dispatch_wait queue. */
  	spinlock_t		dispatch_wait_lock;
 -	/**
 -	 * @dispatch_wait: Waitqueue to put requests when there is no tag
 -	 * available at the moment, to wait for another try in the future.
 -	 */
  	wait_queue_entry_t	dispatch_wait;
 -
 -	/**
 -	 * @wait_index: Index of next available dispatch_wait queue to insert
 -	 * requests.
 -	 */
  	atomic_t		wait_index;
  
 -	/**
 -	 * @tags: Tags owned by the block driver. A tag at this set is only
 -	 * assigned when a request is dispatched from a hardware queue.
 -	 */
  	struct blk_mq_tags	*tags;
 -	/**
 -	 * @sched_tags: Tags owned by I/O scheduler. If there is an I/O
 -	 * scheduler associated with a request queue, a tag is assigned when
 -	 * that request is allocated. Else, this member is not used.
 -	 */
  	struct blk_mq_tags	*sched_tags;
  
 -	/** @queued: Number of queued requests. */
  	unsigned long		queued;
 -	/** @run: Number of dispatched requests. */
  	unsigned long		run;
  #define BLK_MQ_MAX_DISPATCH_ORDER	7
 -	/** @dispatched: Number of dispatch requests by queue. */
  	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
  
 -	/** @numa_node: NUMA node the storage adapter has been connected to. */
  	unsigned int		numa_node;
 -	/** @queue_num: Index of this hardware queue. */
  	unsigned int		queue_num;
  
 -	/**
 -	 * @nr_active: Number of active requests. Only used when a tag set is
 -	 * shared across request queues.
 -	 */
  	atomic_t		nr_active;
++<<<<<<< HEAD
 +	RH_KABI_DEPRECATE(unsigned int,	nr_expired)
++=======
+ 	/**
+ 	 * @elevator_queued: Number of queued requests on hctx.
+ 	 */
+ 	atomic_t                elevator_queued;
++>>>>>>> b445547ec1bb (blk-mq, elevator: Count requests per hctx to improve performance)
  
 -	/** @cpuhp_online: List to store request if CPU is going to die */
 -	struct hlist_node	cpuhp_online;
 -	/** @cpuhp_dead: List to store request if some CPU die. */
  	struct hlist_node	cpuhp_dead;
 -	/** @kobj: Kernel object for sysfs. */
  	struct kobject		kobj;
  
 -	/** @poll_considered: Count times blk_poll() was called. */
  	unsigned long		poll_considered;
 -	/** @poll_invoked: Count how many requests blk_poll() polled. */
  	unsigned long		poll_invoked;
 -	/** @poll_success: Count how many polled requests were completed. */
  	unsigned long		poll_success;
  
  #ifdef CONFIG_BLK_DEBUG_FS
diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 014cb3d8ad1b..8f3a72fe19f7 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -4648,6 +4648,9 @@ static bool bfq_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct bfq_data *bfqd = hctx->queue->elevator->elevator_data;
 
+	if (!atomic_read(&hctx->elevator_queued))
+		return false;
+
 	/*
 	 * Avoiding lock: a race on bfqd->busy_queues should cause at
 	 * most a call to dispatch for nothing
@@ -5561,6 +5564,7 @@ static void bfq_insert_requests(struct blk_mq_hw_ctx *hctx,
 		rq = list_first_entry(list, struct request, queuelist);
 		list_del_init(&rq->queuelist);
 		bfq_insert_request(hctx, rq, at_head);
+		atomic_inc(&hctx->elevator_queued);
 	}
 }
 
@@ -5940,6 +5944,7 @@ static void bfq_finish_requeue_request(struct request *rq)
 
 		bfq_completed_request(bfqq, bfqd);
 		bfq_finish_requeue_request_body(bfqq);
+		atomic_dec(&rq->mq_hctx->elevator_queued);
 
 		spin_unlock_irqrestore(&bfqd->lock, flags);
 	} else {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 7330c42c292b..846747586660 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -2522,6 +2522,7 @@ blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		goto free_hctx;
 
 	atomic_set(&hctx->nr_active, 0);
+	atomic_set(&hctx->elevator_queued, 0);
 	if (node == NUMA_NO_NODE)
 		node = set->numa_node;
 	hctx->numa_node = node;
diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index b943c475617f..01f4bb4a72ee 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -395,6 +395,8 @@ static struct request *dd_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	    !list_empty(&dd->fifo_list[WRITE]))
 		blk_mq_sched_mark_restart_hctx(hctx);
 	spin_unlock(&dd->lock);
+	if (rq)
+		atomic_dec(&rq->mq_hctx->elevator_queued);
 
 	return rq;
 }
@@ -541,6 +543,7 @@ static void dd_insert_requests(struct blk_mq_hw_ctx *hctx,
 		rq = list_first_entry(list, struct request, queuelist);
 		list_del_init(&rq->queuelist);
 		dd_insert_request(hctx, rq, at_head);
+		atomic_inc(&hctx->elevator_queued);
 	}
 	spin_unlock(&dd->lock);
 }
@@ -578,6 +581,9 @@ static bool dd_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct deadline_data *dd = hctx->queue->elevator->elevator_data;
 
+	if (!atomic_read(&hctx->elevator_queued))
+		return false;
+
 	return !list_empty_careful(&dd->dispatch) ||
 		!list_empty_careful(&dd->fifo_list[0]) ||
 		!list_empty_careful(&dd->fifo_list[1]);
* Unmerged path include/linux/blk-mq.h
