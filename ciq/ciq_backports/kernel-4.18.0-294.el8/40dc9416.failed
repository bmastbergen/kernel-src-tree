mptcp: schedule work for better snd subflow selection

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Paolo Abeni <pabeni@redhat.com>
commit 40dc9416cc957ac8b74d09550a808fabfd4435f8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/40dc9416.failed

Otherwise the packet scheduler policy will not be
enforced when pushing pending data at MPTCP-level
ack reception time.

	Reviewed-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit 40dc9416cc957ac8b74d09550a808fabfd4435f8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/mptcp/protocol.c
diff --cc net/mptcp/protocol.c
index 509aa48ee70d,8cb582eee286..000000000000
--- a/net/mptcp/protocol.c
+++ b/net/mptcp/protocol.c
@@@ -1429,20 -2232,27 +1429,28 @@@ static void mptcp_worker(struct work_st
  {
  	struct mptcp_sock *msk = container_of(work, struct mptcp_sock, work);
  	struct sock *ssk, *sk = &msk->sk.icsk_inet.sk;
 -	struct mptcp_sendmsg_info info = {};
 +	int orig_len, orig_offset, mss_now = 0, size_goal = 0;
  	struct mptcp_data_frag *dfrag;
 +	u64 orig_write_seq;
  	size_t copied = 0;
 -	int state, ret;
 +	struct msghdr msg = {
 +		.msg_flags = MSG_DONTWAIT,
 +	};
 +	long timeo = 0;
  
  	lock_sock(sk);
++<<<<<<< HEAD
 +	mptcp_clean_una_wakeup(sk);
++=======
+ 	state = sk->sk_state;
+ 	if (unlikely(state == TCP_CLOSE))
+ 		goto unlock;
+ 
+ 	mptcp_push_pending(sk, 0);
++>>>>>>> 40dc9416cc95 (mptcp: schedule work for better snd subflow selection)
  	mptcp_check_data_fin_ack(sk);
  	__mptcp_flush_join_list(msk);
 -
 -	mptcp_check_fastclose(msk);
 -
 -	if (test_and_clear_bit(MPTCP_WORK_CLOSE_SUBFLOW, &msk->flags))
 -		__mptcp_close_subflow(msk);
 -
 -	if (msk->pm.status)
 -		pm_work(msk);
 +	__mptcp_move_skbs(msk);
  
  	if (test_and_clear_bit(MPTCP_WORK_EOF, &msk->flags))
  		mptcp_check_for_eof(msk);
@@@ -1989,12 -2884,35 +1997,37 @@@ static int mptcp_getsockopt(struct soc
  	return -EOPNOTSUPP;
  }
  
 -void __mptcp_data_acked(struct sock *sk)
 -{
 -	if (!sock_owned_by_user(sk))
 -		__mptcp_clean_una(sk);
 -	else
 -		set_bit(MPTCP_CLEAN_UNA, &mptcp_sk(sk)->flags);
 +#define MPTCP_DEFERRED_ALL (TCPF_DELACK_TIMER_DEFERRED | \
 +			    TCPF_WRITE_TIMER_DEFERRED)
  
++<<<<<<< HEAD
 +/* this is very alike tcp_release_cb() but we must handle differently a
 + * different set of events
 + */
++=======
+ 	if (mptcp_pending_data_fin_ack(sk))
+ 		mptcp_schedule_work(sk);
+ }
+ 
+ void __mptcp_check_push(struct sock *sk, struct sock *ssk)
+ {
+ 	if (!mptcp_send_head(sk))
+ 		return;
+ 
+ 	if (!sock_owned_by_user(sk)) {
+ 		if (mptcp_subflow_get_send(mptcp_sk(sk)) == ssk)
+ 			__mptcp_subflow_push_pending(sk, ssk);
+ 		else
+ 			mptcp_schedule_work(sk);
+ 	} else {
+ 		set_bit(MPTCP_PUSH_PENDING, &mptcp_sk(sk)->flags);
+ 	}
+ }
+ 
+ #define MPTCP_DEFERRED_ALL (TCPF_WRITE_TIMER_DEFERRED)
+ 
+ /* processes deferred events and flush wmem */
++>>>>>>> 40dc9416cc95 (mptcp: schedule work for better snd subflow selection)
  static void mptcp_release_cb(struct sock *sk)
  {
  	unsigned long flags, nflags;
* Unmerged path net/mptcp/protocol.c
