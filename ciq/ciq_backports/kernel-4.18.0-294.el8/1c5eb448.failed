tracing: Rename trace_buffer to array_buffer

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Steven Rostedt (VMware) <rostedt@goodmis.org>
commit 1c5eb4481e0151d579f738175497f998840f7bbc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/1c5eb448.failed

As we are working to remove the generic "ring_buffer" name that is used by
both tracing and perf, the ring_buffer name for tracing will be renamed to
trace_buffer, and perf's ring buffer will be renamed to perf_buffer.

As there already exists a trace_buffer that is used by the trace_arrays, it
needs to be first renamed to array_buffer.

Link: https://lore.kernel.org/r/20191213153553.GE20583@krava

	Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
(cherry picked from commit 1c5eb4481e0151d579f738175497f998840f7bbc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/trace/trace.c
#	kernel/trace/trace.h
#	kernel/trace/trace_functions_graph.c
#	kernel/trace/trace_kdb.c
diff --cc kernel/trace/trace.c
index db7e8ffb4c4e,67084b7945ff..000000000000
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@@ -947,9 -988,57 +947,63 @@@ void tracing_snapshot(void
  }
  EXPORT_SYMBOL_GPL(tracing_snapshot);
  
++<<<<<<< HEAD
 +static int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,
 +					struct trace_buffer *size_buf, int cpu_id);
 +static void set_buffer_entries(struct trace_buffer *buf, unsigned long val);
++=======
+ /**
+  * tracing_snapshot_cond - conditionally take a snapshot of the current buffer.
+  * @tr:		The tracing instance to snapshot
+  * @cond_data:	The data to be tested conditionally, and possibly saved
+  *
+  * This is the same as tracing_snapshot() except that the snapshot is
+  * conditional - the snapshot will only happen if the
+  * cond_snapshot.update() implementation receiving the cond_data
+  * returns true, which means that the trace array's cond_snapshot
+  * update() operation used the cond_data to determine whether the
+  * snapshot should be taken, and if it was, presumably saved it along
+  * with the snapshot.
+  */
+ void tracing_snapshot_cond(struct trace_array *tr, void *cond_data)
+ {
+ 	tracing_snapshot_instance_cond(tr, cond_data);
+ }
+ EXPORT_SYMBOL_GPL(tracing_snapshot_cond);
+ 
+ /**
+  * tracing_snapshot_cond_data - get the user data associated with a snapshot
+  * @tr:		The tracing instance
+  *
+  * When the user enables a conditional snapshot using
+  * tracing_snapshot_cond_enable(), the user-defined cond_data is saved
+  * with the snapshot.  This accessor is used to retrieve it.
+  *
+  * Should not be called from cond_snapshot.update(), since it takes
+  * the tr->max_lock lock, which the code calling
+  * cond_snapshot.update() has already done.
+  *
+  * Returns the cond_data associated with the trace array's snapshot.
+  */
+ void *tracing_cond_snapshot_data(struct trace_array *tr)
+ {
+ 	void *cond_data = NULL;
+ 
+ 	arch_spin_lock(&tr->max_lock);
+ 
+ 	if (tr->cond_snapshot)
+ 		cond_data = tr->cond_snapshot->cond_data;
+ 
+ 	arch_spin_unlock(&tr->max_lock);
+ 
+ 	return cond_data;
+ }
+ EXPORT_SYMBOL_GPL(tracing_cond_snapshot_data);
+ 
+ static int resize_buffer_duplicate_size(struct array_buffer *trace_buf,
+ 					struct array_buffer *size_buf, int cpu_id);
+ static void set_buffer_entries(struct array_buffer *buf, unsigned long val);
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  
  int tracing_alloc_snapshot_instance(struct trace_array *tr)
  {
@@@ -1089,10 -1292,10 +1143,10 @@@ void disable_trace_on_warning(void
   *
   * Shows real state of the ring buffer if it is enabled or not.
   */
 -bool tracer_tracing_is_on(struct trace_array *tr)
 +int tracer_tracing_is_on(struct trace_array *tr)
  {
- 	if (tr->trace_buffer.buffer)
- 		return ring_buffer_record_is_on(tr->trace_buffer.buffer);
+ 	if (tr->array_buffer.buffer)
+ 		return ring_buffer_record_is_on(tr->array_buffer.buffer);
  	return !tr->buffer_disabled;
  }
  
@@@ -1381,9 -1655,15 +1435,17 @@@ update_max_tr(struct trace_array *tr, s
  	else
  		ring_buffer_record_off(tr->max_buffer.buffer);
  
++<<<<<<< HEAD
 +	swap(tr->trace_buffer.buffer, tr->max_buffer.buffer);
++=======
+ #ifdef CONFIG_TRACER_SNAPSHOT
+ 	if (tr->cond_snapshot && !tr->cond_snapshot->update(tr, cond_data))
+ 		goto out_unlock;
+ #endif
+ 	swap(tr->array_buffer.buffer, tr->max_buffer.buffer);
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  
  	__update_max_tr(tr, tsk, cpu);
 -
 - out_unlock:
  	arch_spin_unlock(&tr->max_lock);
  }
  
@@@ -1672,7 -1962,7 +1734,11 @@@ int __init register_tracer(struct trace
  	return ret;
  }
  
++<<<<<<< HEAD
 +void tracing_reset(struct trace_buffer *buf, int cpu)
++=======
+ static void tracing_reset_cpu(struct array_buffer *buf, int cpu)
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  {
  	struct ring_buffer *buffer = buf->buffer;
  
@@@ -2863,9 -3154,10 +2929,9 @@@ void trace_printk_init_buffers(void
  	 * directly here. If the global_trace.buffer is already
  	 * allocated here, then this was called by module code.
  	 */
- 	if (global_trace.trace_buffer.buffer)
+ 	if (global_trace.array_buffer.buffer)
  		tracing_start_cmdline_record();
  }
 -EXPORT_SYMBOL_GPL(trace_printk_init_buffers);
  
  void trace_printk_start_comm(void)
  {
@@@ -3304,10 -3602,32 +3370,36 @@@ static void s_stop(struct seq_file *m, 
  }
  
  static void
++<<<<<<< HEAD
 +get_total_entries(struct trace_buffer *buf,
++=======
+ get_total_entries_cpu(struct array_buffer *buf, unsigned long *total,
+ 		      unsigned long *entries, int cpu)
+ {
+ 	unsigned long count;
+ 
+ 	count = ring_buffer_entries_cpu(buf->buffer, cpu);
+ 	/*
+ 	 * If this buffer has skipped entries, then we hold all
+ 	 * entries for the trace and we need to ignore the
+ 	 * ones before the time stamp.
+ 	 */
+ 	if (per_cpu_ptr(buf->data, cpu)->skipped_entries) {
+ 		count -= per_cpu_ptr(buf->data, cpu)->skipped_entries;
+ 		/* total is the same as the entries */
+ 		*total = count;
+ 	} else
+ 		*total = count +
+ 			ring_buffer_overrun_cpu(buf->buffer, cpu);
+ 	*entries = count;
+ }
+ 
+ static void
+ get_total_entries(struct array_buffer *buf,
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  		  unsigned long *total, unsigned long *entries)
  {
 -	unsigned long t, e;
 +	unsigned long count;
  	int cpu;
  
  	*total = 0;
@@@ -3331,6 -3640,30 +3423,33 @@@
  	}
  }
  
++<<<<<<< HEAD
++=======
+ unsigned long trace_total_entries_cpu(struct trace_array *tr, int cpu)
+ {
+ 	unsigned long total, entries;
+ 
+ 	if (!tr)
+ 		tr = &global_trace;
+ 
+ 	get_total_entries_cpu(&tr->array_buffer, &total, &entries, cpu);
+ 
+ 	return entries;
+ }
+ 
+ unsigned long trace_total_entries(struct trace_array *tr)
+ {
+ 	unsigned long total, entries;
+ 
+ 	if (!tr)
+ 		tr = &global_trace;
+ 
+ 	get_total_entries(&tr->array_buffer, &total, &entries);
+ 
+ 	return entries;
+ }
+ 
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  static void print_lat_help_header(struct seq_file *m)
  {
  	seq_puts(m, "#                  _------=> CPU#            \n"
@@@ -3890,7 -4220,8 +4009,12 @@@ __tracing_open(struct inode *inode, str
  	if (iter->cpu_file == RING_BUFFER_ALL_CPUS) {
  		for_each_tracing_cpu(cpu) {
  			iter->buffer_iter[cpu] =
++<<<<<<< HEAD
 +				ring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);
++=======
+ 				ring_buffer_read_prepare(iter->array_buffer->buffer,
+ 							 cpu, GFP_KERNEL);
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  		}
  		ring_buffer_read_prepare_sync();
  		for_each_tracing_cpu(cpu) {
@@@ -3900,7 -4231,8 +4024,12 @@@
  	} else {
  		cpu = iter->cpu_file;
  		iter->buffer_iter[cpu] =
++<<<<<<< HEAD
 +			ring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);
++=======
+ 			ring_buffer_read_prepare(iter->array_buffer->buffer,
+ 						 cpu, GFP_KERNEL);
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  		ring_buffer_read_prepare_sync();
  		ring_buffer_read_start(iter->buffer_iter[cpu]);
  		tracing_iter_reset(iter, cpu);
@@@ -6454,11 -6858,13 +6583,18 @@@ tracing_snapshot_write(struct file *fil
  			break;
  		}
  #endif
++<<<<<<< HEAD
 +		if (!tr->allocated_snapshot) {
++=======
+ 		if (tr->allocated_snapshot)
+ 			ret = resize_buffer_duplicate_size(&tr->max_buffer,
+ 					&tr->array_buffer, iter->cpu_file);
+ 		else
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  			ret = tracing_alloc_snapshot_instance(tr);
 -		if (ret < 0)
 -			break;
 +			if (ret < 0)
 +				break;
 +		}
  		local_irq_disable();
  		/* Now, we're going to swap */
  		if (iter->cpu_file == RING_BUFFER_ALL_CPUS)
@@@ -6891,8 -7540,8 +7027,13 @@@ tracing_buffers_splice_read(struct fil
  			break;
  		}
  
++<<<<<<< HEAD
 +		ref->ref = 1;
 +		ref->buffer = iter->trace_buffer->buffer;
++=======
+ 		refcount_set(&ref->refcount, 1);
+ 		ref->buffer = iter->array_buffer->buffer;
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  		ref->page = ring_buffer_alloc_read_page(ref->buffer, iter->cpu_file);
  		if (IS_ERR(ref->page)) {
  			ret = PTR_ERR(ref->page);
diff --cc kernel/trace/trace.h
index ac5533d9bbcd,fd679fe92c1f..000000000000
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@@ -588,8 -685,7 +588,12 @@@ trace_buffer_iter(struct trace_iterato
  
  int tracer_init(struct tracer *t, struct trace_array *tr);
  int tracing_is_enabled(void);
++<<<<<<< HEAD
 +void tracing_reset(struct trace_buffer *buf, int cpu);
 +void tracing_reset_online_cpus(struct trace_buffer *buf);
++=======
+ void tracing_reset_online_cpus(struct array_buffer *buf);
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  void tracing_reset_current(int cpu);
  void tracing_reset_all_online_cpus(void);
  int tracing_open_generic(struct inode *inode, struct file *filp);
diff --cc kernel/trace/trace_functions_graph.c
index 603d5b957c73,79b2c2df00c5..000000000000
--- a/kernel/trace/trace_functions_graph.c
+++ b/kernel/trace/trace_functions_graph.c
@@@ -521,9 -243,16 +521,9 @@@ void trace_graph_return(struct ftrace_g
  	int cpu;
  	int pc;
  
 -	ftrace_graph_addr_finish(trace);
 -
 -	if (trace_recursion_test(TRACE_GRAPH_NOTRACE_BIT)) {
 -		trace_recursion_clear(TRACE_GRAPH_NOTRACE_BIT);
 -		return;
 -	}
 -
  	local_irq_save(flags);
  	cpu = raw_smp_processor_id();
- 	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+ 	data = per_cpu_ptr(tr->array_buffer.data, cpu);
  	disabled = atomic_inc_return(&data->disabled);
  	if (likely(disabled == 1)) {
  		pc = preempt_count();
@@@ -751,6 -499,17 +751,20 @@@ static void print_graph_abs_time(u64 t
  }
  
  static void
++<<<<<<< HEAD
++=======
+ print_graph_rel_time(struct trace_iterator *iter, struct trace_seq *s)
+ {
+ 	unsigned long long usecs;
+ 
+ 	usecs = iter->ts - iter->array_buffer->time_start;
+ 	do_div(usecs, NSEC_PER_USEC);
+ 
+ 	trace_seq_printf(s, "%9llu us |  ", usecs);
+ }
+ 
+ static void
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  print_graph_irq(struct trace_iterator *iter, unsigned long addr,
  		enum trace_type type, int cpu, pid_t pid, u32 flags)
  {
diff --cc kernel/trace/trace_kdb.c
index d953c163a079,9da76104f7a2..000000000000
--- a/kernel/trace/trace_kdb.c
+++ b/kernel/trace/trace_kdb.c
@@@ -51,14 -43,16 +51,24 @@@ static void ftrace_dump_buf(int skip_li
  	if (cpu_file == RING_BUFFER_ALL_CPUS) {
  		for_each_tracing_cpu(cpu) {
  			iter.buffer_iter[cpu] =
++<<<<<<< HEAD
 +			ring_buffer_read_prepare(iter.trace_buffer->buffer, cpu);
++=======
+ 			ring_buffer_read_prepare(iter.array_buffer->buffer,
+ 						 cpu, GFP_ATOMIC);
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  			ring_buffer_read_start(iter.buffer_iter[cpu]);
  			tracing_iter_reset(&iter, cpu);
  		}
  	} else {
  		iter.cpu_file = cpu_file;
  		iter.buffer_iter[cpu_file] =
++<<<<<<< HEAD
 +			ring_buffer_read_prepare(iter.trace_buffer->buffer, cpu_file);
++=======
+ 			ring_buffer_read_prepare(iter.array_buffer->buffer,
+ 						 cpu_file, GFP_ATOMIC);
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  		ring_buffer_read_start(iter.buffer_iter[cpu_file]);
  		tracing_iter_reset(&iter, cpu_file);
  	}
@@@ -127,7 -119,29 +137,33 @@@ static int kdb_ftdump(int argc, const c
  	}
  
  	kdb_trap_printk++;
++<<<<<<< HEAD
 +	ftrace_dump_buf(skip_lines, cpu_file);
++=======
+ 
+ 	trace_init_global_iter(&iter);
+ 	iter.buffer_iter = buffer_iter;
+ 
+ 	for_each_tracing_cpu(cpu) {
+ 		atomic_inc(&per_cpu_ptr(iter.array_buffer->data, cpu)->disabled);
+ 	}
+ 
+ 	/* A negative skip_entries means skip all but the last entries */
+ 	if (skip_entries < 0) {
+ 		if (cpu_file == RING_BUFFER_ALL_CPUS)
+ 			cnt = trace_total_entries(NULL);
+ 		else
+ 			cnt = trace_total_entries_cpu(NULL, cpu_file);
+ 		skip_entries = max(cnt + skip_entries, 0);
+ 	}
+ 
+ 	ftrace_dump_buf(skip_entries, cpu_file);
+ 
+ 	for_each_tracing_cpu(cpu) {
+ 		atomic_dec(&per_cpu_ptr(iter.array_buffer->data, cpu)->disabled);
+ 	}
+ 
++>>>>>>> 1c5eb4481e01 (tracing: Rename trace_buffer to array_buffer)
  	kdb_trap_printk--;
  
  	return 0;
diff --git a/include/linux/trace_events.h b/include/linux/trace_events.h
index eeeb0ae86109..da8cc8ac5334 100644
--- a/include/linux/trace_events.h
+++ b/include/linux/trace_events.h
@@ -11,7 +11,7 @@
 #include <linux/tracepoint.h>
 
 struct trace_array;
-struct trace_buffer;
+struct array_buffer;
 struct tracer;
 struct dentry;
 struct bpf_prog;
@@ -74,7 +74,7 @@ struct trace_entry {
 struct trace_iterator {
 	struct trace_array	*tr;
 	struct tracer		*trace;
-	struct trace_buffer	*trace_buffer;
+	struct array_buffer	*array_buffer;
 	void			*private;
 	int			cpu_file;
 	struct mutex		mutex;
diff --git a/kernel/trace/blktrace.c b/kernel/trace/blktrace.c
index c53b2838dac9..7933362651f8 100644
--- a/kernel/trace/blktrace.c
+++ b/kernel/trace/blktrace.c
@@ -91,7 +91,7 @@ static void trace_note(struct blk_trace *bt, pid_t pid, int action,
 	ssize_t cgid_len = cgid ? sizeof(*cgid) : 0;
 
 	if (blk_tracer) {
-		buffer = blk_tr->trace_buffer.buffer;
+		buffer = blk_tr->array_buffer.buffer;
 		pc = preempt_count();
 		event = trace_buffer_lock_reserve(buffer, TRACE_BLK,
 						  sizeof(*t) + len + cgid_len,
@@ -264,7 +264,7 @@ static void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,
 	if (blk_tracer) {
 		tracing_record_cmdline(current);
 
-		buffer = blk_tr->trace_buffer.buffer;
+		buffer = blk_tr->array_buffer.buffer;
 		pc = preempt_count();
 		event = trace_buffer_lock_reserve(buffer, TRACE_BLK,
 						  sizeof(*t) + pdu_len + cgid_len,
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index 611f151189c6..7b79565328f8 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -187,7 +187,7 @@ static void ftrace_pid_func(unsigned long ip, unsigned long parent_ip,
 {
 	struct trace_array *tr = op->private;
 
-	if (tr && this_cpu_read(tr->trace_buffer.data->ftrace_ignore_pid))
+	if (tr && this_cpu_read(tr->array_buffer.data->ftrace_ignore_pid))
 		return;
 
 	op->saved_func(ip, parent_ip, op, regs);
@@ -6872,7 +6872,7 @@ ftrace_filter_pid_sched_switch_probe(void *data, bool preempt,
 
 	pid_list = rcu_dereference_sched(tr->function_pids);
 
-	this_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ftrace_ignore_pid,
 		       trace_ignore_this_task(pid_list, next));
 }
 
@@ -6926,7 +6926,7 @@ static void clear_ftrace_pids(struct trace_array *tr)
 	unregister_trace_sched_switch(ftrace_filter_pid_sched_switch_probe, tr);
 
 	for_each_possible_cpu(cpu)
-		per_cpu_ptr(tr->trace_buffer.data, cpu)->ftrace_ignore_pid = false;
+		per_cpu_ptr(tr->array_buffer.data, cpu)->ftrace_ignore_pid = false;
 
 	rcu_assign_pointer(tr->function_pids, NULL);
 
@@ -7049,7 +7049,7 @@ static void ignore_task_cpu(void *data)
 	pid_list = rcu_dereference_protected(tr->function_pids,
 					     mutex_is_locked(&ftrace_lock));
 
-	this_cpu_write(tr->trace_buffer.data->ftrace_ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ftrace_ignore_pid,
 		       trace_ignore_this_task(pid_list, current));
 }
 
* Unmerged path kernel/trace/trace.c
* Unmerged path kernel/trace/trace.h
diff --git a/kernel/trace/trace_branch.c b/kernel/trace/trace_branch.c
index 4ad967453b6f..b57baaef7607 100644
--- a/kernel/trace/trace_branch.c
+++ b/kernel/trace/trace_branch.c
@@ -55,12 +55,12 @@ probe_likely_condition(struct ftrace_likely_data *f, int val, int expect)
 
 	raw_local_irq_save(flags);
 	current->trace_recursion |= TRACE_BRANCH_BIT;
-	data = this_cpu_ptr(tr->trace_buffer.data);
+	data = this_cpu_ptr(tr->array_buffer.data);
 	if (atomic_read(&data->disabled))
 		goto out;
 
 	pc = preempt_count();
-	buffer = tr->trace_buffer.buffer;
+	buffer = tr->array_buffer.buffer;
 	event = trace_buffer_lock_reserve(buffer, TRACE_BRANCH,
 					  sizeof(*entry), flags, pc);
 	if (!event)
diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7e052a5fce39..01d68915121c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -243,7 +243,7 @@ bool trace_event_ignore_this_pid(struct trace_event_file *trace_file)
 	if (!pid_list)
 		return false;
 
-	data = this_cpu_ptr(tr->trace_buffer.data);
+	data = this_cpu_ptr(tr->array_buffer.data);
 
 	return data->ignore_pid;
 }
@@ -552,7 +552,7 @@ event_filter_pid_sched_switch_probe_pre(void *data, bool preempt,
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, prev) &&
 		       trace_ignore_this_task(pid_list, next));
 }
@@ -566,7 +566,7 @@ event_filter_pid_sched_switch_probe_post(void *data, bool preempt,
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, next));
 }
 
@@ -577,12 +577,12 @@ event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
 	struct trace_pid_list *pid_list;
 
 	/* Nothing to do if we are already tracing */
-	if (!this_cpu_read(tr->trace_buffer.data->ignore_pid))
+	if (!this_cpu_read(tr->array_buffer.data->ignore_pid))
 		return;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, task));
 }
 
@@ -593,13 +593,13 @@ event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
 	struct trace_pid_list *pid_list;
 
 	/* Nothing to do if we are not tracing */
-	if (this_cpu_read(tr->trace_buffer.data->ignore_pid))
+	if (this_cpu_read(tr->array_buffer.data->ignore_pid))
 		return;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	/* Set tracing if current is enabled */
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, current));
 }
 
@@ -631,7 +631,7 @@ static void __ftrace_clear_event_pids(struct trace_array *tr)
 	}
 
 	for_each_possible_cpu(cpu)
-		per_cpu_ptr(tr->trace_buffer.data, cpu)->ignore_pid = false;
+		per_cpu_ptr(tr->array_buffer.data, cpu)->ignore_pid = false;
 
 	rcu_assign_pointer(tr->filtered_pids, NULL);
 
@@ -1584,7 +1584,7 @@ static void ignore_task_cpu(void *data)
 	pid_list = rcu_dereference_protected(tr->filtered_pids,
 					     mutex_is_locked(&event_mutex));
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, current));
 }
 
diff --git a/kernel/trace/trace_events_hist.c b/kernel/trace/trace_events_hist.c
index 735725777f88..fb81aa643941 100644
--- a/kernel/trace/trace_events_hist.c
+++ b/kernel/trace/trace_events_hist.c
@@ -656,7 +656,7 @@ static notrace void trace_event_raw_event_synth(void *__data,
 	 * Avoid ring buffer recursion detection, as this event
 	 * is being performed within another event.
 	 */
-	buffer = trace_file->tr->trace_buffer.buffer;
+	buffer = trace_file->tr->array_buffer.buffer;
 	ring_buffer_nest_start(buffer);
 
 	entry = trace_event_buffer_reserve(&fbuffer, trace_file,
diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b611cd36e22d..8a4c8d5c2c98 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -101,7 +101,7 @@ static int function_trace_init(struct trace_array *tr)
 
 	ftrace_init_array_ops(tr, func);
 
-	tr->trace_buffer.cpu = get_cpu();
+	tr->array_buffer.cpu = get_cpu();
 	put_cpu();
 
 	tracing_start_cmdline_record();
@@ -118,7 +118,7 @@ static void function_trace_reset(struct trace_array *tr)
 
 static void function_trace_start(struct trace_array *tr)
 {
-	tracing_reset_online_cpus(&tr->trace_buffer);
+	tracing_reset_online_cpus(&tr->array_buffer);
 }
 
 static void
@@ -143,7 +143,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 		goto out;
 
 	cpu = smp_processor_id();
-	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	if (!atomic_read(&data->disabled)) {
 		local_save_flags(flags);
 		trace_function(tr, ip, parent_ip, flags, pc);
@@ -192,7 +192,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	 */
 	local_irq_save(flags);
 	cpu = raw_smp_processor_id();
-	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1)) {
* Unmerged path kernel/trace/trace_functions_graph.c
diff --git a/kernel/trace/trace_hwlat.c b/kernel/trace/trace_hwlat.c
index bccff460cd35..3e4693af8859 100644
--- a/kernel/trace/trace_hwlat.c
+++ b/kernel/trace/trace_hwlat.c
@@ -106,7 +106,7 @@ static void trace_hwlat_sample(struct hwlat_sample *sample)
 {
 	struct trace_array *tr = hwlat_trace;
 	struct trace_event_call *call = &event_hwlat;
-	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer *buffer = tr->array_buffer.buffer;
 	struct ring_buffer_event *event;
 	struct hwlat_entry *entry;
 	unsigned long flags;
diff --git a/kernel/trace/trace_irqsoff.c b/kernel/trace/trace_irqsoff.c
index f8daa754cce2..0779ddd6ca1b 100644
--- a/kernel/trace/trace_irqsoff.c
+++ b/kernel/trace/trace_irqsoff.c
@@ -121,7 +121,7 @@ static int func_prolog_dec(struct trace_array *tr,
 	if (!irqs_disabled_flags(*flags) && !preempt_count())
 		return 0;
 
-	*data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	*data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	disabled = atomic_inc_return(&(*data)->disabled);
 
 	if (likely(disabled == 1))
@@ -166,7 +166,7 @@ static int irqsoff_display_graph(struct trace_array *tr, int set)
 		per_cpu(tracing_cpu, cpu) = 0;
 
 	tr->max_latency = 0;
-	tracing_reset_online_cpus(&irqsoff_trace->trace_buffer);
+	tracing_reset_online_cpus(&irqsoff_trace->array_buffer);
 
 	return start_irqsoff_tracer(irqsoff_trace, set);
 }
@@ -382,7 +382,7 @@ start_critical_timing(unsigned long ip, unsigned long parent_ip)
 	if (per_cpu(tracing_cpu, cpu))
 		return;
 
-	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 
 	if (unlikely(!data) || atomic_read(&data->disabled))
 		return;
@@ -420,7 +420,7 @@ stop_critical_timing(unsigned long ip, unsigned long parent_ip)
 	if (!tracer_enabled || !tracing_is_enabled())
 		return;
 
-	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 
 	if (unlikely(!data) ||
 	    !data->critical_start || atomic_read(&data->disabled))
* Unmerged path kernel/trace/trace_kdb.c
diff --git a/kernel/trace/trace_mmiotrace.c b/kernel/trace/trace_mmiotrace.c
index b0388016b687..c30137148759 100644
--- a/kernel/trace/trace_mmiotrace.c
+++ b/kernel/trace/trace_mmiotrace.c
@@ -32,7 +32,7 @@ static void mmio_reset_data(struct trace_array *tr)
 	overrun_detected = false;
 	prev_overruns = 0;
 
-	tracing_reset_online_cpus(&tr->trace_buffer);
+	tracing_reset_online_cpus(&tr->array_buffer);
 }
 
 static int mmio_trace_init(struct trace_array *tr)
@@ -122,7 +122,7 @@ static void mmio_close(struct trace_iterator *iter)
 static unsigned long count_overruns(struct trace_iterator *iter)
 {
 	unsigned long cnt = atomic_xchg(&dropped_count, 0);
-	unsigned long over = ring_buffer_overruns(iter->trace_buffer->buffer);
+	unsigned long over = ring_buffer_overruns(iter->array_buffer->buffer);
 
 	if (over > prev_overruns)
 		cnt += over - prev_overruns;
@@ -297,7 +297,7 @@ static void __trace_mmiotrace_rw(struct trace_array *tr,
 				struct mmiotrace_rw *rw)
 {
 	struct trace_event_call *call = &event_mmiotrace_rw;
-	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer *buffer = tr->array_buffer.buffer;
 	struct ring_buffer_event *event;
 	struct trace_mmiotrace_rw *entry;
 	int pc = preempt_count();
@@ -318,7 +318,7 @@ static void __trace_mmiotrace_rw(struct trace_array *tr,
 void mmio_trace_rw(struct mmiotrace_rw *rw)
 {
 	struct trace_array *tr = mmio_trace_array;
-	struct trace_array_cpu *data = per_cpu_ptr(tr->trace_buffer.data, smp_processor_id());
+	struct trace_array_cpu *data = per_cpu_ptr(tr->array_buffer.data, smp_processor_id());
 	__trace_mmiotrace_rw(tr, data, rw);
 }
 
@@ -327,7 +327,7 @@ static void __trace_mmiotrace_map(struct trace_array *tr,
 				struct mmiotrace_map *map)
 {
 	struct trace_event_call *call = &event_mmiotrace_map;
-	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer *buffer = tr->array_buffer.buffer;
 	struct ring_buffer_event *event;
 	struct trace_mmiotrace_map *entry;
 	int pc = preempt_count();
@@ -351,7 +351,7 @@ void mmio_trace_mapping(struct mmiotrace_map *map)
 	struct trace_array_cpu *data;
 
 	preempt_disable();
-	data = per_cpu_ptr(tr->trace_buffer.data, smp_processor_id());
+	data = per_cpu_ptr(tr->array_buffer.data, smp_processor_id());
 	__trace_mmiotrace_map(tr, data, map);
 	preempt_enable();
 }
diff --git a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c
index 46b86fcbfb93..b3a50f6a245c 100644
--- a/kernel/trace/trace_output.c
+++ b/kernel/trace/trace_output.c
@@ -544,7 +544,7 @@ lat_print_timestamp(struct trace_iterator *iter, u64 next_ts)
 	struct trace_array *tr = iter->tr;
 	unsigned long verbose = tr->trace_flags & TRACE_ITER_VERBOSE;
 	unsigned long in_ns = iter->iter_flags & TRACE_FILE_TIME_IN_NS;
-	unsigned long long abs_ts = iter->ts - iter->trace_buffer->time_start;
+	unsigned long long abs_ts = iter->ts - iter->array_buffer->time_start;
 	unsigned long long rel_ts = next_ts - iter->ts;
 	struct trace_seq *s = &iter->seq;
 
diff --git a/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
index 98425ce52340..8db663ff0126 100644
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@ -89,7 +89,7 @@ func_prolog_preempt_disable(struct trace_array *tr,
 	if (cpu != wakeup_current_cpu)
 		goto out_enable;
 
-	*data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	*data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	disabled = atomic_inc_return(&(*data)->disabled);
 	if (unlikely(disabled != 1))
 		goto out;
@@ -387,7 +387,7 @@ tracing_sched_switch_trace(struct trace_array *tr,
 			   unsigned long flags, int pc)
 {
 	struct trace_event_call *call = &event_context_switch;
-	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer *buffer = tr->array_buffer.buffer;
 	struct ring_buffer_event *event;
 	struct ctx_switch_entry *entry;
 
@@ -417,7 +417,7 @@ tracing_sched_wakeup_trace(struct trace_array *tr,
 	struct trace_event_call *call = &event_wakeup;
 	struct ring_buffer_event *event;
 	struct ctx_switch_entry *entry;
-	struct ring_buffer *buffer = tr->trace_buffer.buffer;
+	struct ring_buffer *buffer = tr->array_buffer.buffer;
 
 	event = trace_buffer_lock_reserve(buffer, TRACE_WAKE,
 					  sizeof(*entry), flags, pc);
@@ -468,7 +468,7 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 
 	/* disable local data, not wakeup_cpu data */
 	cpu = raw_smp_processor_id();
-	disabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);
+	disabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);
 	if (likely(disabled != 1))
 		goto out;
 
@@ -480,7 +480,7 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 		goto out_unlock;
 
 	/* The task we are waiting for is waking up */
-	data = per_cpu_ptr(wakeup_trace->trace_buffer.data, wakeup_cpu);
+	data = per_cpu_ptr(wakeup_trace->array_buffer.data, wakeup_cpu);
 
 	__trace_function(wakeup_trace, CALLER_ADDR0, CALLER_ADDR1, flags, pc);
 	tracing_sched_switch_trace(wakeup_trace, prev, next, flags, pc);
@@ -502,7 +502,7 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 	arch_spin_unlock(&wakeup_lock);
 	local_irq_restore(flags);
 out:
-	atomic_dec(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);
+	atomic_dec(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);
 }
 
 static void __wakeup_reset(struct trace_array *tr)
@@ -521,7 +521,7 @@ static void wakeup_reset(struct trace_array *tr)
 {
 	unsigned long flags;
 
-	tracing_reset_online_cpus(&tr->trace_buffer);
+	tracing_reset_online_cpus(&tr->array_buffer);
 
 	local_irq_save(flags);
 	arch_spin_lock(&wakeup_lock);
@@ -559,7 +559,7 @@ probe_wakeup(void *ignore, struct task_struct *p)
 		return;
 
 	pc = preempt_count();
-	disabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);
+	disabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);
 	if (unlikely(disabled != 1))
 		goto out;
 
@@ -591,7 +591,7 @@ probe_wakeup(void *ignore, struct task_struct *p)
 
 	local_save_flags(flags);
 
-	data = per_cpu_ptr(wakeup_trace->trace_buffer.data, wakeup_cpu);
+	data = per_cpu_ptr(wakeup_trace->array_buffer.data, wakeup_cpu);
 	data->preempt_timestamp = ftrace_now(cpu);
 	tracing_sched_wakeup_trace(wakeup_trace, p, current, flags, pc);
 
@@ -605,7 +605,7 @@ probe_wakeup(void *ignore, struct task_struct *p)
 out_locked:
 	arch_spin_unlock(&wakeup_lock);
 out:
-	atomic_dec(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);
+	atomic_dec(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);
 }
 
 static void start_wakeup_tracer(struct trace_array *tr)
diff --git a/kernel/trace/trace_selftest.c b/kernel/trace/trace_selftest.c
index 11e9daa4a568..795c9dfb68d1 100644
--- a/kernel/trace/trace_selftest.c
+++ b/kernel/trace/trace_selftest.c
@@ -23,7 +23,7 @@ static inline int trace_valid_entry(struct trace_entry *entry)
 	return 0;
 }
 
-static int trace_test_buffer_cpu(struct trace_buffer *buf, int cpu)
+static int trace_test_buffer_cpu(struct array_buffer *buf, int cpu)
 {
 	struct ring_buffer_event *event;
 	struct trace_entry *entry;
@@ -60,7 +60,7 @@ static int trace_test_buffer_cpu(struct trace_buffer *buf, int cpu)
  * Test the trace buffer to see if all the elements
  * are still sane.
  */
-static int __maybe_unused trace_test_buffer(struct trace_buffer *buf, unsigned long *count)
+static int __maybe_unused trace_test_buffer(struct array_buffer *buf, unsigned long *count)
 {
 	unsigned long flags, cnt = 0;
 	int cpu, ret = 0;
@@ -362,7 +362,7 @@ static int trace_selftest_startup_dynamic_tracing(struct tracer *trace,
 	msleep(100);
 
 	/* we should have nothing in the buffer */
-	ret = trace_test_buffer(&tr->trace_buffer, &count);
+	ret = trace_test_buffer(&tr->array_buffer, &count);
 	if (ret)
 		goto out;
 
@@ -383,7 +383,7 @@ static int trace_selftest_startup_dynamic_tracing(struct tracer *trace,
 	ftrace_enabled = 0;
 
 	/* check the trace buffer */
-	ret = trace_test_buffer(&tr->trace_buffer, &count);
+	ret = trace_test_buffer(&tr->array_buffer, &count);
 
 	ftrace_enabled = 1;
 	tracing_start();
@@ -682,7 +682,7 @@ trace_selftest_startup_function(struct tracer *trace, struct trace_array *tr)
 	ftrace_enabled = 0;
 
 	/* check the trace buffer */
-	ret = trace_test_buffer(&tr->trace_buffer, &count);
+	ret = trace_test_buffer(&tr->array_buffer, &count);
 
 	ftrace_enabled = 1;
 	trace->reset(tr);
@@ -763,7 +763,7 @@ trace_selftest_startup_function_graph(struct tracer *trace,
 	 * Simulate the init() callback but we attach a watchdog callback
 	 * to detect and recover from possible hangs
 	 */
-	tracing_reset_online_cpus(&tr->trace_buffer);
+	tracing_reset_online_cpus(&tr->array_buffer);
 	set_graph_array(tr);
 	ret = register_ftrace_graph(&trace_graph_return,
 				    &trace_graph_entry_watchdog);
@@ -786,7 +786,7 @@ trace_selftest_startup_function_graph(struct tracer *trace,
 	tracing_stop();
 
 	/* check the trace buffer */
-	ret = trace_test_buffer(&tr->trace_buffer, &count);
+	ret = trace_test_buffer(&tr->array_buffer, &count);
 
 	trace->reset(tr);
 	tracing_start();
@@ -841,7 +841,7 @@ trace_selftest_startup_irqsoff(struct tracer *trace, struct trace_array *tr)
 	/* stop the tracing. */
 	tracing_stop();
 	/* check both trace buffers */
-	ret = trace_test_buffer(&tr->trace_buffer, NULL);
+	ret = trace_test_buffer(&tr->array_buffer, NULL);
 	if (!ret)
 		ret = trace_test_buffer(&tr->max_buffer, &count);
 	trace->reset(tr);
@@ -903,7 +903,7 @@ trace_selftest_startup_preemptoff(struct tracer *trace, struct trace_array *tr)
 	/* stop the tracing. */
 	tracing_stop();
 	/* check both trace buffers */
-	ret = trace_test_buffer(&tr->trace_buffer, NULL);
+	ret = trace_test_buffer(&tr->array_buffer, NULL);
 	if (!ret)
 		ret = trace_test_buffer(&tr->max_buffer, &count);
 	trace->reset(tr);
@@ -969,7 +969,7 @@ trace_selftest_startup_preemptirqsoff(struct tracer *trace, struct trace_array *
 	/* stop the tracing. */
 	tracing_stop();
 	/* check both trace buffers */
-	ret = trace_test_buffer(&tr->trace_buffer, NULL);
+	ret = trace_test_buffer(&tr->array_buffer, NULL);
 	if (ret)
 		goto out;
 
@@ -999,7 +999,7 @@ trace_selftest_startup_preemptirqsoff(struct tracer *trace, struct trace_array *
 	/* stop the tracing. */
 	tracing_stop();
 	/* check both trace buffers */
-	ret = trace_test_buffer(&tr->trace_buffer, NULL);
+	ret = trace_test_buffer(&tr->array_buffer, NULL);
 	if (ret)
 		goto out;
 
@@ -1129,7 +1129,7 @@ trace_selftest_startup_wakeup(struct tracer *trace, struct trace_array *tr)
 	/* stop the tracing. */
 	tracing_stop();
 	/* check both trace buffers */
-	ret = trace_test_buffer(&tr->trace_buffer, NULL);
+	ret = trace_test_buffer(&tr->array_buffer, NULL);
 	if (!ret)
 		ret = trace_test_buffer(&tr->max_buffer, &count);
 
@@ -1170,7 +1170,7 @@ trace_selftest_startup_branch(struct tracer *trace, struct trace_array *tr)
 	/* stop the tracing. */
 	tracing_stop();
 	/* check the trace buffer */
-	ret = trace_test_buffer(&tr->trace_buffer, &count);
+	ret = trace_test_buffer(&tr->array_buffer, &count);
 	trace->reset(tr);
 	tracing_start();
 
diff --git a/kernel/trace/trace_syscalls.c b/kernel/trace/trace_syscalls.c
index f93a56d2db27..fb28c7a4d5cb 100644
--- a/kernel/trace/trace_syscalls.c
+++ b/kernel/trace/trace_syscalls.c
@@ -339,7 +339,7 @@ static void ftrace_syscall_enter(void *data, struct pt_regs *regs, long id)
 	local_save_flags(irq_flags);
 	pc = preempt_count();
 
-	buffer = tr->trace_buffer.buffer;
+	buffer = tr->array_buffer.buffer;
 	event = trace_buffer_lock_reserve(buffer,
 			sys_data->enter_event->event.type, size, irq_flags, pc);
 	if (!event)
@@ -384,7 +384,7 @@ static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)
 	local_save_flags(irq_flags);
 	pc = preempt_count();
 
-	buffer = tr->trace_buffer.buffer;
+	buffer = tr->array_buffer.buffer;
 	event = trace_buffer_lock_reserve(buffer,
 			sys_data->exit_event->event.type, sizeof(*entry),
 			irq_flags, pc);
