treewide: add checks for the return value of memblock_alloc*()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Mike Rapoport <rppt@linux.ibm.com>
commit 8a7f97b902f4fb0d94b355b6b3f1fbd7154cafb9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/8a7f97b9.failed

Add check for the return value of memblock_alloc*() functions and call
panic() in case of error.  The panic message repeats the one used by
panicing memblock allocators with adjustment of parameters to include
only relevant ones.

The replacement was mostly automated with semantic patches like the one
below with manual massaging of format strings.

  @@
  expression ptr, size, align;
  @@
  ptr = memblock_alloc(size, align);
  + if (!ptr)
  + 	panic("%s: Failed to allocate %lu bytes align=0x%lx\n", __func__, size, align);

[anders.roxell@linaro.org: use '%pa' with 'phys_addr_t' type]
  Link: http://lkml.kernel.org/r/20190131161046.21886-1-anders.roxell@linaro.org
[rppt@linux.ibm.com: fix format strings for panics after memblock_alloc]
  Link: http://lkml.kernel.org/r/1548950940-15145-1-git-send-email-rppt@linux.ibm.com
[rppt@linux.ibm.com: don't panic if the allocation in sparse_buffer_init fails]
  Link: http://lkml.kernel.org/r/20190131074018.GD28876@rapoport-lnx
[akpm@linux-foundation.org: fix xtensa printk warning]
Link: http://lkml.kernel.org/r/1548057848-15136-20-git-send-email-rppt@linux.ibm.com
	Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
	Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
	Reviewed-by: Guo Ren <ren_guo@c-sky.com>		[c-sky]
	Acked-by: Paul Burton <paul.burton@mips.com>		[MIPS]
	Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>	[s390]
	Reviewed-by: Juergen Gross <jgross@suse.com>		[Xen]
	Reviewed-by: Geert Uytterhoeven <geert@linux-m68k.org>	[m68k]
	Acked-by: Max Filippov <jcmvbkbc@gmail.com>		[xtensa]
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Christophe Leroy <christophe.leroy@c-s.fr>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Dennis Zhou <dennis@kernel.org>
	Cc: Greentime Hu <green.hu@gmail.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Guan Xuetao <gxt@pku.edu.cn>
	Cc: Guo Ren <guoren@kernel.org>
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Simek <monstr@monstr.eu>
	Cc: Petr Mladek <pmladek@suse.com>
	Cc: Richard Weinberger <richard@nod.at>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Rob Herring <robh+dt@kernel.org>
	Cc: Rob Herring <robh@kernel.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Stafford Horne <shorne@gmail.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Vineet Gupta <vgupta@synopsys.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8a7f97b902f4fb0d94b355b6b3f1fbd7154cafb9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/alpha/kernel/core_cia.c
#	arch/alpha/kernel/core_marvel.c
#	arch/alpha/kernel/pci-noop.c
#	arch/alpha/kernel/pci.c
#	arch/alpha/kernel/pci_iommu.c
#	arch/arm/kernel/setup.c
#	arch/arm/mm/mmu.c
#	arch/arm64/mm/kasan_init.c
#	arch/c6x/mm/dma-coherent.c
#	arch/c6x/mm/init.c
#	arch/csky/mm/highmem.c
#	arch/h8300/mm/init.c
#	arch/m68k/mm/mcfmmu.c
#	arch/m68k/sun3/sun3dvma.c
#	arch/microblaze/mm/init.c
#	arch/mips/kernel/setup.c
#	arch/mips/kernel/traps.c
#	arch/nds32/mm/init.c
#	arch/openrisc/mm/ioremap.c
#	arch/powerpc/kernel/dt_cpu_ftrs.c
#	arch/powerpc/kernel/setup-common.c
#	arch/powerpc/kernel/setup_64.c
#	arch/powerpc/mm/book3s32/mmu.c
#	arch/powerpc/mm/book3s64/hash_utils.c
#	arch/powerpc/mm/book3s64/pgtable.c
#	arch/powerpc/mm/book3s64/radix_pgtable.c
#	arch/powerpc/mm/pgtable-book3e.c
#	arch/powerpc/platforms/pasemi/iommu.c
#	arch/powerpc/platforms/powernv/opal.c
#	arch/s390/kernel/setup.c
#	arch/s390/kernel/smp.c
#	arch/s390/numa/numa.c
#	arch/sh/mm/numa.c
#	arch/um/drivers/net_kern.c
#	arch/um/drivers/vector_kern.c
#	arch/um/kernel/initrd.c
#	arch/unicore32/kernel/setup.c
#	arch/unicore32/mm/mmu.c
diff --cc arch/alpha/kernel/core_cia.c
index 867e8730b0c5,f489170201c3..000000000000
--- a/arch/alpha/kernel/core_cia.c
+++ b/arch/alpha/kernel/core_cia.c
@@@ -331,7 -331,10 +331,14 @@@ cia_prepare_tbia_workaround(int window
  	long i;
  
  	/* Use minimal 1K map. */
++<<<<<<< HEAD
 +	ppte = memblock_alloc_from(CIA_BROKEN_TBIA_SIZE, 32768, 0);
++=======
+ 	ppte = memblock_alloc(CIA_BROKEN_TBIA_SIZE, 32768);
+ 	if (!ppte)
+ 		panic("%s: Failed to allocate %u bytes align=0x%x\n",
+ 		      __func__, CIA_BROKEN_TBIA_SIZE, 32768);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	pte = (virt_to_phys(ppte) >> (PAGE_SHIFT - 1)) | 1;
  
  	for (i = 0; i < CIA_BROKEN_TBIA_SIZE / sizeof(unsigned long); ++i)
diff --cc arch/alpha/kernel/core_marvel.c
index 8a568c4d8e81,1db9d0eb2922..000000000000
--- a/arch/alpha/kernel/core_marvel.c
+++ b/arch/alpha/kernel/core_marvel.c
@@@ -82,7 -82,10 +82,14 @@@ mk_resource_name(int pe, int port, cha
  	char *name;
  	
  	sprintf(tmp, "PCI %s PE %d PORT %d", str, pe, port);
++<<<<<<< HEAD
 +	name = memblock_alloc(strlen(tmp) + 1, 0);
++=======
+ 	name = memblock_alloc(strlen(tmp) + 1, SMP_CACHE_BYTES);
+ 	if (!name)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      strlen(tmp) + 1);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	strcpy(name, tmp);
  
  	return name;
@@@ -117,7 -120,10 +124,14 @@@ alloc_io7(unsigned int pe
  		return NULL;
  	}
  
++<<<<<<< HEAD
 +	io7 = memblock_alloc(sizeof(*io7), 0);
++=======
+ 	io7 = memblock_alloc(sizeof(*io7), SMP_CACHE_BYTES);
+ 	if (!io7)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(*io7));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	io7->pe = pe;
  	raw_spin_lock_init(&io7->irq_lock);
  
diff --cc arch/alpha/kernel/pci-noop.c
index a9378ee0c2f1,ae82061edae9..000000000000
--- a/arch/alpha/kernel/pci-noop.c
+++ b/arch/alpha/kernel/pci-noop.c
@@@ -33,7 -33,10 +33,14 @@@ alloc_pci_controller(void
  {
  	struct pci_controller *hose;
  
++<<<<<<< HEAD
 +	hose = memblock_alloc(sizeof(*hose), 0);
++=======
+ 	hose = memblock_alloc(sizeof(*hose), SMP_CACHE_BYTES);
+ 	if (!hose)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(*hose));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	*hose_tail = hose;
  	hose_tail = &hose->next;
@@@ -44,7 -47,13 +51,17 @@@
  struct resource * __init
  alloc_resource(void)
  {
++<<<<<<< HEAD
 +	return memblock_alloc(sizeof(struct resource), 0);
++=======
+ 	void *ptr = memblock_alloc(sizeof(struct resource), SMP_CACHE_BYTES);
+ 
+ 	if (!ptr)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(struct resource));
+ 
+ 	return ptr;
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  }
  
  SYSCALL_DEFINE3(pciconfig_iobase, long, which, unsigned long, bus,
diff --cc arch/alpha/kernel/pci.c
index 13937e72d875,64fbfb0763b2..000000000000
--- a/arch/alpha/kernel/pci.c
+++ b/arch/alpha/kernel/pci.c
@@@ -392,7 -392,10 +392,14 @@@ alloc_pci_controller(void
  {
  	struct pci_controller *hose;
  
++<<<<<<< HEAD
 +	hose = memblock_alloc(sizeof(*hose), 0);
++=======
+ 	hose = memblock_alloc(sizeof(*hose), SMP_CACHE_BYTES);
+ 	if (!hose)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(*hose));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	*hose_tail = hose;
  	hose_tail = &hose->next;
@@@ -403,7 -406,13 +410,17 @@@
  struct resource * __init
  alloc_resource(void)
  {
++<<<<<<< HEAD
 +	return memblock_alloc(sizeof(struct resource), 0);
++=======
+ 	void *ptr = memblock_alloc(sizeof(struct resource), SMP_CACHE_BYTES);
+ 
+ 	if (!ptr)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(struct resource));
+ 
+ 	return ptr;
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  }
  
  
diff --cc arch/alpha/kernel/pci_iommu.c
index 82cf950bda2a,3034d6d936d2..000000000000
--- a/arch/alpha/kernel/pci_iommu.c
+++ b/arch/alpha/kernel/pci_iommu.c
@@@ -79,7 -79,10 +79,14 @@@ iommu_arena_new_node(int nid, struct pc
  		printk("%s: couldn't allocate arena from node %d\n"
  		       "    falling back to system-wide allocation\n",
  		       __func__, nid);
++<<<<<<< HEAD
 +		arena = memblock_alloc(sizeof(*arena), 0);
++=======
+ 		arena = memblock_alloc(sizeof(*arena), SMP_CACHE_BYTES);
+ 		if (!arena)
+ 			panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 			      sizeof(*arena));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	}
  
  	arena->ptes = memblock_alloc_node(sizeof(*arena), align, nid);
@@@ -87,13 -90,22 +94,31 @@@
  		printk("%s: couldn't allocate arena ptes from node %d\n"
  		       "    falling back to system-wide allocation\n",
  		       __func__, nid);
++<<<<<<< HEAD
 +		arena->ptes = memblock_alloc_from(mem_size, align, 0);
++=======
+ 		arena->ptes = memblock_alloc(mem_size, align);
+ 		if (!arena->ptes)
+ 			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 			      __func__, mem_size, align);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	}
  
  #else /* CONFIG_DISCONTIGMEM */
  
++<<<<<<< HEAD
 +	arena = memblock_alloc(sizeof(*arena), 0);
 +	arena->ptes = memblock_alloc_from(mem_size, align, 0);
++=======
+ 	arena = memblock_alloc(sizeof(*arena), SMP_CACHE_BYTES);
+ 	if (!arena)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(*arena));
+ 	arena->ptes = memblock_alloc(mem_size, align);
+ 	if (!arena->ptes)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, mem_size, align);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  #endif /* CONFIG_DISCONTIGMEM */
  
diff --cc arch/arm/kernel/setup.c
index a123f4d2709c,5d78b6ac0429..000000000000
--- a/arch/arm/kernel/setup.c
+++ b/arch/arm/kernel/setup.c
@@@ -856,7 -866,10 +856,14 @@@ static void __init request_standard_res
  		 */
  		boot_alias_start = phys_to_idmap(start);
  		if (arm_has_idmap_alias() && boot_alias_start != IDMAP_INVALID_ADDR) {
++<<<<<<< HEAD
 +			res = memblock_alloc(sizeof(*res), 0);
++=======
+ 			res = memblock_alloc(sizeof(*res), SMP_CACHE_BYTES);
+ 			if (!res)
+ 				panic("%s: Failed to allocate %zu bytes\n",
+ 				      __func__, sizeof(*res));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  			res->name = "System RAM (boot alias)";
  			res->start = boot_alias_start;
  			res->end = phys_to_idmap(end);
@@@ -864,7 -877,10 +871,14 @@@
  			request_resource(&iomem_resource, res);
  		}
  
++<<<<<<< HEAD
 +		res = memblock_alloc(sizeof(*res), 0);
++=======
+ 		res = memblock_alloc(sizeof(*res), SMP_CACHE_BYTES);
+ 		if (!res)
+ 			panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 			      sizeof(*res));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  		res->name  = "System RAM";
  		res->start = start;
  		res->end = end;
diff --cc arch/arm/mm/mmu.c
index 1af8fec3bd19,f3ce34113f89..000000000000
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@@ -693,16 -719,15 +693,26 @@@ EXPORT_SYMBOL(phys_mem_access_prot)
  
  #define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
  
 +static void __init *early_alloc_aligned(unsigned long sz, unsigned long align)
 +{
 +	void *ptr = __va(memblock_phys_alloc(sz, align));
 +	memset(ptr, 0, sz);
 +	return ptr;
 +}
 +
  static void __init *early_alloc(unsigned long sz)
  {
++<<<<<<< HEAD
 +	return early_alloc_aligned(sz, sz);
++=======
+ 	void *ptr = memblock_alloc(sz, sz);
+ 
+ 	if (!ptr)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, sz, sz);
+ 
+ 	return ptr;
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  }
  
  static void *__init late_alloc(unsigned long sz)
@@@ -974,7 -999,10 +984,14 @@@ void __init iotable_init(struct map_des
  	if (!nr)
  		return;
  
++<<<<<<< HEAD
 +	svm = early_alloc_aligned(sizeof(*svm) * nr, __alignof__(*svm));
++=======
+ 	svm = memblock_alloc(sizeof(*svm) * nr, __alignof__(*svm));
+ 	if (!svm)
+ 		panic("%s: Failed to allocate %zu bytes align=0x%zx\n",
+ 		      __func__, sizeof(*svm) * nr, __alignof__(*svm));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	for (md = io_desc; nr; md++, nr--) {
  		create_mapping(md);
@@@ -996,7 -1024,10 +1013,14 @@@ void __init vm_reserve_area_early(unsig
  	struct vm_struct *vm;
  	struct static_vm *svm;
  
++<<<<<<< HEAD
 +	svm = early_alloc_aligned(sizeof(*svm), __alignof__(*svm));
++=======
+ 	svm = memblock_alloc(sizeof(*svm), __alignof__(*svm));
+ 	if (!svm)
+ 		panic("%s: Failed to allocate %zu bytes align=0x%zx\n",
+ 		      __func__, sizeof(*svm), __alignof__(*svm));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	vm = &svm->vm;
  	vm->addr = (void *)addr;
diff --cc arch/arm64/mm/kasan_init.c
index 26fefff7f86f,296de39ddee5..000000000000
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@@ -40,9 -40,27 +40,30 @@@ static phys_addr_t __init kasan_alloc_z
  	void *p = memblock_alloc_try_nid(PAGE_SIZE, PAGE_SIZE,
  					      __pa(MAX_DMA_ADDRESS),
  					      MEMBLOCK_ALLOC_KASAN, node);
+ 	if (!p)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx nid=%d from=%llx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE, node,
+ 		      __pa(MAX_DMA_ADDRESS));
+ 
+ 	return __pa(p);
+ }
+ 
++<<<<<<< HEAD
++=======
+ static phys_addr_t __init kasan_alloc_raw_page(int node)
+ {
+ 	void *p = memblock_alloc_try_nid_raw(PAGE_SIZE, PAGE_SIZE,
+ 						__pa(MAX_DMA_ADDRESS),
+ 						MEMBLOCK_ALLOC_KASAN, node);
+ 	if (!p)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx nid=%d from=%llx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE, node,
+ 		      __pa(MAX_DMA_ADDRESS));
+ 
  	return __pa(p);
  }
  
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  static pte_t *__init kasan_pte_offset(pmd_t *pmdp, unsigned long addr, int node,
  				      bool early)
  {
diff --cc arch/c6x/mm/dma-coherent.c
index 01305c787201,0d3701bc88f6..000000000000
--- a/arch/c6x/mm/dma-coherent.c
+++ b/arch/c6x/mm/dma-coherent.c
@@@ -135,11 -136,12 +135,20 @@@ void __init coherent_mem_init(phys_addr
  	if (dma_size & (PAGE_SIZE - 1))
  		++dma_pages;
  
++<<<<<<< HEAD
 +	bitmap_phys = memblock_phys_alloc(BITS_TO_LONGS(dma_pages) * sizeof(long),
 +					  sizeof(long));
 +
 +	dma_bitmap = phys_to_virt(bitmap_phys);
 +	memset(dma_bitmap, 0, dma_pages * PAGE_SIZE);
++=======
+ 	dma_bitmap = memblock_alloc(BITS_TO_LONGS(dma_pages) * sizeof(long),
+ 				    sizeof(long));
+ 	if (!dma_bitmap)
+ 		panic("%s: Failed to allocate %zu bytes align=0x%zx\n",
+ 		      __func__, BITS_TO_LONGS(dma_pages) * sizeof(long),
+ 		      sizeof(long));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  }
  
  static void c6x_dma_sync(struct device *dev, phys_addr_t paddr, size_t size,
diff --cc arch/c6x/mm/init.c
index af5ada0520be,fe582c3a1794..000000000000
--- a/arch/c6x/mm/init.c
+++ b/arch/c6x/mm/init.c
@@@ -40,7 -40,9 +40,13 @@@ void __init paging_init(void
  
  	empty_zero_page      = (unsigned long) memblock_alloc(PAGE_SIZE,
  							      PAGE_SIZE);
++<<<<<<< HEAD
 +	memset((void *)empty_zero_page, 0, PAGE_SIZE);
++=======
+ 	if (!empty_zero_page)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	/*
  	 * Set up user data space
diff --cc arch/h8300/mm/init.c
index 6519252ac4db,0f04a5e9aa4f..000000000000
--- a/arch/h8300/mm/init.c
+++ b/arch/h8300/mm/init.c
@@@ -68,7 -68,9 +68,13 @@@ void __init paging_init(void
  	 * to a couple of allocated pages.
  	 */
  	empty_zero_page = (unsigned long)memblock_alloc(PAGE_SIZE, PAGE_SIZE);
++<<<<<<< HEAD
 +	memset((void *)empty_zero_page, 0, PAGE_SIZE);
++=======
+ 	if (!empty_zero_page)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	/*
  	 * Set up SFC/DFC registers (user data space).
diff --cc arch/m68k/mm/mcfmmu.c
index 80e7822f9f44,6cb1e41d58d0..000000000000
--- a/arch/m68k/mm/mcfmmu.c
+++ b/arch/m68k/mm/mcfmmu.c
@@@ -44,7 -44,9 +44,13 @@@ void __init paging_init(void
  	int i;
  
  	empty_zero_page = (void *) memblock_alloc(PAGE_SIZE, PAGE_SIZE);
++<<<<<<< HEAD
 +	memset((void *) empty_zero_page, 0, PAGE_SIZE);
++=======
+ 	if (!empty_zero_page)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	pg_dir = swapper_pg_dir;
  	memset(swapper_pg_dir, 0, sizeof(swapper_pg_dir));
diff --cc arch/m68k/sun3/sun3dvma.c
index 8be8b750c629,399f3d06125f..000000000000
--- a/arch/m68k/sun3/sun3dvma.c
+++ b/arch/m68k/sun3/sun3dvma.c
@@@ -268,7 -268,10 +268,14 @@@ void __init dvma_init(void
  	list_add(&(hole->list), &hole_list);
  
  	iommu_use = memblock_alloc(IOMMU_TOTAL_ENTRIES * sizeof(unsigned long),
++<<<<<<< HEAD
 +				   0);
++=======
+ 				   SMP_CACHE_BYTES);
+ 	if (!iommu_use)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      IOMMU_TOTAL_ENTRIES * sizeof(unsigned long));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	dvma_unmap_iommu(DVMA_START, DVMA_SIZE);
  
diff --cc arch/microblaze/mm/init.c
index 8c14988f52f2,7e97d44f6538..000000000000
--- a/arch/microblaze/mm/init.c
+++ b/arch/microblaze/mm/init.c
@@@ -373,12 -374,14 +373,22 @@@ void * __ref zalloc_maybe_bootmem(size_
  {
  	void *p;
  
- 	if (mem_init_done)
+ 	if (mem_init_done) {
  		p = kzalloc(size, mask);
++<<<<<<< HEAD
 +	else {
 +		p = memblock_alloc(size, 0);
 +		if (p)
 +			memset(p, 0, size);
 +	}
++=======
+ 	} else {
+ 		p = memblock_alloc(size, SMP_CACHE_BYTES);
+ 		if (!p)
+ 			panic("%s: Failed to allocate %zu bytes\n",
+ 			      __func__, size);
+ 	}
+ 
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	return p;
  }
diff --cc arch/mips/kernel/setup.c
index 1c6765a9a570,8d1dc6c71173..000000000000
--- a/arch/mips/kernel/setup.c
+++ b/arch/mips/kernel/setup.c
@@@ -951,7 -918,10 +951,14 @@@ static void __init resource_init(void
  		if (end >= HIGHMEM_START)
  			end = HIGHMEM_START - 1;
  
++<<<<<<< HEAD
 +		res = memblock_alloc(sizeof(struct resource), 0);
++=======
+ 		res = memblock_alloc(sizeof(struct resource), SMP_CACHE_BYTES);
+ 		if (!res)
+ 			panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 			      sizeof(struct resource));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  		res->start = start;
  		res->end = end;
diff --cc arch/mips/kernel/traps.c
index 0bb7c1d05d89,98ca55d62201..000000000000
--- a/arch/mips/kernel/traps.c
+++ b/arch/mips/kernel/traps.c
@@@ -2270,7 -2293,10 +2270,14 @@@ void __init trap_init(void
  		phys_addr_t ebase_pa;
  
  		ebase = (unsigned long)
++<<<<<<< HEAD
 +			memblock_alloc_from(size, 1 << fls(size), 0);
++=======
+ 			memblock_alloc(size, 1 << fls(size));
+ 		if (!ebase)
+ 			panic("%s: Failed to allocate %lu bytes align=0x%x\n",
+ 			      __func__, size, 1 << fls(size));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  		/*
  		 * Try to ensure ebase resides in KSeg0 if possible.
diff --cc arch/nds32/mm/init.c
index 131104bd2538,1d03633f89a9..000000000000
--- a/arch/nds32/mm/init.c
+++ b/arch/nds32/mm/init.c
@@@ -80,8 -78,10 +80,15 @@@ static void __init map_ram(void
  		}
  
  		/* Alloc one page for holding PTE's... */
++<<<<<<< HEAD
 +		pte = (pte_t *) __va(memblock_phys_alloc(PAGE_SIZE, PAGE_SIZE));
 +		memset(pte, 0, PAGE_SIZE);
++=======
+ 		pte = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 		if (!pte)
+ 			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 			      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  		set_pmd(pme, __pmd(__pa(pte) + _PAGE_KERNEL_TABLE));
  
  		/* Fill the newly allocated page with PTE'S */
@@@ -113,8 -113,10 +120,15 @@@ static void __init fixedrange_init(void
  	pgd = swapper_pg_dir + pgd_index(vaddr);
  	pud = pud_offset(pgd, vaddr);
  	pmd = pmd_offset(pud, vaddr);
++<<<<<<< HEAD
 +	fixmap_pmd_p = (pmd_t *) __va(memblock_phys_alloc(PAGE_SIZE, PAGE_SIZE));
 +	memset(fixmap_pmd_p, 0, PAGE_SIZE);
++=======
+ 	fixmap_pmd_p = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 	if (!fixmap_pmd_p)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	set_pmd(pmd, __pmd(__pa(fixmap_pmd_p) + _PAGE_KERNEL_TABLE));
  
  #ifdef CONFIG_HIGHMEM
@@@ -126,8 -128,10 +140,15 @@@
  	pgd = swapper_pg_dir + pgd_index(vaddr);
  	pud = pud_offset(pgd, vaddr);
  	pmd = pmd_offset(pud, vaddr);
++<<<<<<< HEAD
 +	pte = (pte_t *) __va(memblock_phys_alloc(PAGE_SIZE, PAGE_SIZE));
 +	memset(pte, 0, PAGE_SIZE);
++=======
+ 	pte = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 	if (!pte)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	set_pmd(pmd, __pmd(__pa(pte) + _PAGE_KERNEL_TABLE));
  	pkmap_page_table = pte;
  #endif /* CONFIG_HIGHMEM */
@@@ -152,8 -156,10 +173,15 @@@ void __init paging_init(void
  	fixedrange_init();
  
  	/* allocate space for empty_zero_page */
++<<<<<<< HEAD
 +	zero_page = __va(memblock_phys_alloc(PAGE_SIZE, PAGE_SIZE));
 +	memset(zero_page, 0, PAGE_SIZE);
++=======
+ 	zero_page = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 	if (!zero_page)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	zone_sizes_init();
  
  	empty_zero_page = virt_to_page(zero_page);
diff --cc arch/openrisc/mm/ioremap.c
index c9697529b3f0,a8509950dbbc..000000000000
--- a/arch/openrisc/mm/ioremap.c
+++ b/arch/openrisc/mm/ioremap.c
@@@ -124,12 -123,13 +124,21 @@@ pte_t __ref *pte_alloc_one_kernel(struc
  	pte_t *pte;
  
  	if (likely(mem_init_done)) {
++<<<<<<< HEAD
 +		pte = (pte_t *) __get_free_page(GFP_KERNEL);
 +	} else {
 +		pte = (pte_t *) __va(memblock_phys_alloc(PAGE_SIZE, PAGE_SIZE));
++=======
+ 		pte = (pte_t *)get_zeroed_page(GFP_KERNEL);
+ 	} else {
+ 		pte = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 		if (!pte)
+ 			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 			      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	}
  
 +	if (pte)
 +		clear_page(pte);
  	return pte;
  }
diff --cc arch/powerpc/kernel/dt_cpu_ftrs.c
index fed0d7c18d63,c66fd3ce6478..000000000000
--- a/arch/powerpc/kernel/dt_cpu_ftrs.c
+++ b/arch/powerpc/kernel/dt_cpu_ftrs.c
@@@ -1107,7 -1004,12 +1107,16 @@@ static int __init dt_cpu_ftrs_scan_call
  	/* Count and allocate space for cpu features */
  	of_scan_flat_dt_subnodes(node, count_cpufeatures_subnodes,
  						&nr_dt_cpu_features);
++<<<<<<< HEAD
 +	dt_cpu_features = __va(memblock_phys_alloc(sizeof(struct dt_cpu_feature) * nr_dt_cpu_features, PAGE_SIZE));
++=======
+ 	dt_cpu_features = memblock_alloc(sizeof(struct dt_cpu_feature) * nr_dt_cpu_features, PAGE_SIZE);
+ 	if (!dt_cpu_features)
+ 		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
+ 		      __func__,
+ 		      sizeof(struct dt_cpu_feature) * nr_dt_cpu_features,
+ 		      PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	cpufeatures_setup_start(isa);
  
diff --cc arch/powerpc/kernel/setup-common.c
index 11470c38fac4,2e5dfb6e0823..000000000000
--- a/arch/powerpc/kernel/setup-common.c
+++ b/arch/powerpc/kernel/setup-common.c
@@@ -459,8 -459,11 +459,16 @@@ void __init smp_setup_cpu_maps(void
  
  	DBG("smp_setup_cpu_maps()\n");
  
++<<<<<<< HEAD
 +	cpu_to_phys_id = __va(memblock_phys_alloc(nr_cpu_ids * sizeof(u32), __alignof__(u32)));
 +	memset(cpu_to_phys_id, 0, nr_cpu_ids * sizeof(u32));
++=======
+ 	cpu_to_phys_id = memblock_alloc(nr_cpu_ids * sizeof(u32),
+ 					__alignof__(u32));
+ 	if (!cpu_to_phys_id)
+ 		panic("%s: Failed to allocate %zu bytes align=0x%zx\n",
+ 		      __func__, nr_cpu_ids * sizeof(u32), __alignof__(u32));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	for_each_node_by_type(dn, "cpu") {
  		const __be32 *intserv;
diff --cc arch/powerpc/kernel/setup_64.c
index 9ea4a32e7bd3,ba404dd9ce1d..000000000000
--- a/arch/powerpc/kernel/setup_64.c
+++ b/arch/powerpc/kernel/setup_64.c
@@@ -932,8 -902,13 +932,18 @@@ static void __ref init_fallback_flush(v
  	 * hardware prefetch runoff. We don't have a recipe for load patterns to
  	 * reliably avoid the prefetcher.
  	 */
++<<<<<<< HEAD
 +	l1d_flush_fallback_area = __va(memblock_alloc_base(l1d_size * 2, l1d_size, limit));
 +	memset(l1d_flush_fallback_area, 0, l1d_size * 2);
++=======
+ 	l1d_flush_fallback_area = memblock_alloc_try_nid(l1d_size * 2,
+ 						l1d_size, MEMBLOCK_LOW_LIMIT,
+ 						limit, NUMA_NO_NODE);
+ 	if (!l1d_flush_fallback_area)
+ 		panic("%s: Failed to allocate %llu bytes align=0x%llx max_addr=%pa\n",
+ 		      __func__, l1d_size * 2, l1d_size, &limit);
+ 
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	for_each_possible_cpu(cpu) {
  		struct paca_struct *paca = paca_ptrs[cpu];
diff --cc arch/powerpc/mm/book3s32/mmu.c
index 9d4a70484613,f29d2f118b44..000000000000
--- a/arch/powerpc/mm/book3s32/mmu.c
+++ b/arch/powerpc/mm/book3s32/mmu.c
@@@ -270,8 -339,10 +270,15 @@@ void __init MMU_init_hw(void
  	 * Find some memory for the hash table.
  	 */
  	if ( ppc_md.progress ) ppc_md.progress("hash:find piece", 0x322);
++<<<<<<< HEAD:arch/powerpc/mm/book3s32/mmu.c
 +	Hash = __va(memblock_phys_alloc(Hash_size, Hash_size));
 +	memset(Hash, 0, Hash_size);
++=======
+ 	Hash = memblock_alloc(Hash_size, Hash_size);
+ 	if (!Hash)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, Hash_size, Hash_size);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*()):arch/powerpc/mm/ppc_mmu_32.c
  	_SDR1 = __pa(Hash) | SDR1_LOW_BITS;
  
  	Hash_end = (struct hash_pte *) ((unsigned long)Hash + Hash_size);
diff --cc arch/powerpc/mm/book3s64/hash_utils.c
index c7f37e28c477,0a4f939a8161..000000000000
--- a/arch/powerpc/mm/book3s64/hash_utils.c
+++ b/arch/powerpc/mm/book3s64/hash_utils.c
@@@ -928,9 -912,12 +928,18 @@@ static void __init htab_initialize(void
  #ifdef CONFIG_DEBUG_PAGEALLOC
  	if (debug_pagealloc_enabled()) {
  		linear_map_hash_count = memblock_end_of_DRAM() >> PAGE_SHIFT;
++<<<<<<< HEAD:arch/powerpc/mm/book3s64/hash_utils.c
 +		linear_map_hash_slots = __va(memblock_alloc_base(
 +				linear_map_hash_count, 1, ppc64_rma_size));
 +		memset(linear_map_hash_slots, 0, linear_map_hash_count);
++=======
+ 		linear_map_hash_slots = memblock_alloc_try_nid(
+ 				linear_map_hash_count, 1, MEMBLOCK_LOW_LIMIT,
+ 				ppc64_rma_size,	NUMA_NO_NODE);
+ 		if (!linear_map_hash_slots)
+ 			panic("%s: Failed to allocate %lu bytes max_addr=%pa\n",
+ 			      __func__, linear_map_hash_count, &ppc64_rma_size);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*()):arch/powerpc/mm/hash_utils_64.c
  	}
  #endif /* CONFIG_DEBUG_PAGEALLOC */
  
diff --cc arch/powerpc/mm/book3s64/pgtable.c
index 066033167e2d,a4341aba0af4..000000000000
--- a/arch/powerpc/mm/book3s64/pgtable.c
+++ b/arch/powerpc/mm/book3s64/pgtable.c
@@@ -205,11 -195,11 +205,18 @@@ void __init mmu_partition_table_init(vo
  	unsigned long ptcr;
  
  	BUILD_BUG_ON_MSG((PATB_SIZE_SHIFT > 36), "Partition table size too large.");
 +	partition_tb = __va(memblock_alloc_base(patb_size, patb_size,
 +						MEMBLOCK_ALLOC_ANYWHERE));
 +
  	/* Initialize the Partition Table with no entries */
++<<<<<<< HEAD:arch/powerpc/mm/book3s64/pgtable.c
 +	memset((void *)partition_tb, 0, patb_size);
++=======
+ 	partition_tb = memblock_alloc(patb_size, patb_size);
+ 	if (!partition_tb)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, patb_size, patb_size);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*()):arch/powerpc/mm/pgtable-book3s64.c
  
  	/*
  	 * update partition table control register,
diff --cc arch/powerpc/mm/book3s64/radix_pgtable.c
index c27ff2daed34,154472a28c77..000000000000
--- a/arch/powerpc/mm/book3s64/radix_pgtable.c
+++ b/arch/powerpc/mm/book3s64/radix_pgtable.c
@@@ -52,26 -51,22 +52,42 @@@ static int native_register_process_tabl
  static __ref void *early_alloc_pgtable(unsigned long size, int nid,
  			unsigned long region_start, unsigned long region_end)
  {
++<<<<<<< HEAD:arch/powerpc/mm/book3s64/radix_pgtable.c
 +	unsigned long pa = 0;
 +	void *pt;
++=======
+ 	phys_addr_t min_addr = MEMBLOCK_LOW_LIMIT;
+ 	phys_addr_t max_addr = MEMBLOCK_ALLOC_ANYWHERE;
+ 	void *ptr;
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*()):arch/powerpc/mm/pgtable-radix.c
  
 -	if (region_start)
 -		min_addr = region_start;
 -	if (region_end)
 -		max_addr = region_end;
 +	if (region_start || region_end) /* has region hint */
 +		pa = memblock_alloc_range(size, size, region_start, region_end,
 +						MEMBLOCK_NONE);
 +	else if (nid != -1) /* has node hint */
 +		pa = memblock_alloc_base_nid(size, size,
 +						MEMBLOCK_ALLOC_ANYWHERE,
 +						nid, MEMBLOCK_NONE);
  
++<<<<<<< HEAD:arch/powerpc/mm/book3s64/radix_pgtable.c
 +	if (!pa)
 +		pa = memblock_alloc_base(size, size, MEMBLOCK_ALLOC_ANYWHERE);
 +
 +	BUG_ON(!pa);
 +
 +	pt = __va(pa);
 +	memset(pt, 0, size);
 +
 +	return pt;
++=======
+ 	ptr = memblock_alloc_try_nid(size, size, min_addr, max_addr, nid);
+ 
+ 	if (!ptr)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx nid=%d from=%pa max_addr=%pa\n",
+ 		      __func__, size, size, nid, &min_addr, &max_addr);
+ 
+ 	return ptr;
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*()):arch/powerpc/mm/pgtable-radix.c
  }
  
  static int early_map_kernel_page(unsigned long ea, unsigned long pa,
diff --cc arch/powerpc/mm/pgtable-book3e.c
index 64ec19cb306a,1032ef7aaf62..000000000000
--- a/arch/powerpc/mm/pgtable-book3e.c
+++ b/arch/powerpc/mm/pgtable-book3e.c
@@@ -57,12 -57,16 +57,25 @@@ void vmemmap_remove_mapping(unsigned lo
  
  static __ref void *early_alloc_pgtable(unsigned long size)
  {
++<<<<<<< HEAD
 +	void *pt;
 +
 +	pt = __va(memblock_alloc_base(size, size, __pa(MAX_DMA_ADDRESS)));
 +	memset(pt, 0, size);
 +
 +	return pt;
++=======
+ 	void *ptr;
+ 
+ 	ptr = memblock_alloc_try_nid(size, size, MEMBLOCK_LOW_LIMIT,
+ 				     __pa(MAX_DMA_ADDRESS), NUMA_NO_NODE);
+ 
+ 	if (!ptr)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx max_addr=%lx\n",
+ 		      __func__, size, size, __pa(MAX_DMA_ADDRESS));
+ 
+ 	return ptr;
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  }
  
  /*
diff --cc arch/powerpc/platforms/pasemi/iommu.c
index 943bce1a37db,044c6089462c..000000000000
--- a/arch/powerpc/platforms/pasemi/iommu.c
+++ b/arch/powerpc/platforms/pasemi/iommu.c
@@@ -208,9 -208,14 +208,18 @@@ static int __init iob_init(struct devic
  	pr_debug(" -> %s\n", __func__);
  
  	/* For 2G space, 8x64 pages (2^21 bytes) is max total l2 size */
++<<<<<<< HEAD
 +	iob_l2_base = (u32 *)__va(memblock_alloc_base(1UL<<21, 1UL<<21, 0x80000000));
++=======
+ 	iob_l2_base = memblock_alloc_try_nid_raw(1UL << 21, 1UL << 21,
+ 					MEMBLOCK_LOW_LIMIT, 0x80000000,
+ 					NUMA_NO_NODE);
+ 	if (!iob_l2_base)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx max_addr=%x\n",
+ 		      __func__, 1UL << 21, 1UL << 21, 0x80000000);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
 -	pr_info("IOBMAP L2 allocated at: %p\n", iob_l2_base);
 +	printk(KERN_INFO "IOBMAP L2 allocated at: %p\n", iob_l2_base);
  
  	/* Allocate a spare page to map all invalid IOTLB pages. */
  	tmp = memblock_phys_alloc(IOBMAP_PAGE_SIZE, IOBMAP_PAGE_SIZE);
diff --cc arch/powerpc/platforms/powernv/opal.c
index 054491677ba0,2b0eca104f86..000000000000
--- a/arch/powerpc/platforms/powernv/opal.c
+++ b/arch/powerpc/platforms/powernv/opal.c
@@@ -173,8 -170,10 +173,15 @@@ int __init early_init_dt_scan_recoverab
  	/*
  	 * Allocate a buffer to hold the MC recoverable ranges.
  	 */
++<<<<<<< HEAD
 +	mc_recoverable_range =__va(memblock_phys_alloc(size, __alignof__(u64)));
 +	memset(mc_recoverable_range, 0, size);
++=======
+ 	mc_recoverable_range = memblock_alloc(size, __alignof__(u64));
+ 	if (!mc_recoverable_range)
+ 		panic("%s: Failed to allocate %u bytes align=0x%lx\n",
+ 		      __func__, size, __alignof__(u64));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	for (i = 0; i < mc_recoverable_range_len; i++) {
  		mc_recoverable_range[i].start_addr =
diff --cc arch/s390/kernel/setup.c
index 6b3f55f796a3,2c642af526ce..000000000000
--- a/arch/s390/kernel/setup.c
+++ b/arch/s390/kernel/setup.c
@@@ -363,8 -418,15 +367,20 @@@ static void __init setup_lowcore_dat_of
  	lc->last_update_timer = S390_lowcore.last_update_timer;
  	lc->last_update_clock = S390_lowcore.last_update_clock;
  
++<<<<<<< HEAD
 +	restart_stack = memblock_alloc(ASYNC_SIZE, ASYNC_SIZE);
 +	restart_stack += ASYNC_SIZE;
++=======
+ 	/*
+ 	 * Allocate the global restart stack which is the same for
+ 	 * all CPUs in cast *one* of them does a PSW restart.
+ 	 */
+ 	restart_stack = memblock_alloc(THREAD_SIZE, THREAD_SIZE);
+ 	if (!restart_stack)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, THREAD_SIZE, THREAD_SIZE);
+ 	restart_stack += STACK_INIT_OFFSET;
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	/*
  	 * Set up PSW restart to call ipl.c:do_restart(). Copy the relevant
diff --cc arch/s390/kernel/smp.c
index 7bb2e38846d7,3fe1c77c361b..000000000000
--- a/arch/s390/kernel/smp.c
+++ b/arch/s390/kernel/smp.c
@@@ -649,7 -656,11 +649,15 @@@ void __init smp_save_dump_cpus(void
  		/* No previous system present, normal boot. */
  		return;
  	/* Allocate a page as dumping area for the store status sigps */
++<<<<<<< HEAD
 +	page = memblock_alloc_base(PAGE_SIZE, PAGE_SIZE, 1UL << 31);
++=======
+ 	page = memblock_phys_alloc_range(PAGE_SIZE, PAGE_SIZE, 0, 1UL << 31);
+ 	if (!page)
+ 		panic("ERROR: Failed to allocate %lx bytes below %lx\n",
+ 		      PAGE_SIZE, 1UL << 31);
+ 
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	/* Set multi-threading state to the previous system. */
  	pcpu_set_smt(sclp.mtid_prev);
  	boot_cpu_addr = stap();
diff --cc arch/s390/numa/numa.c
index 2a3f8da9ee0f,8eb9e9743f5d..000000000000
--- a/arch/s390/numa/numa.c
+++ b/arch/s390/numa/numa.c
@@@ -104,8 -92,12 +104,17 @@@ static void __init numa_setup_memory(vo
  	} while (cur_base < end_of_dram);
  
  	/* Allocate and fill out node_data */
++<<<<<<< HEAD
 +	for (nid = 0; nid < MAX_NUMNODES; nid++)
 +		NODE_DATA(nid) = alloc_node_data();
++=======
+ 	for (nid = 0; nid < MAX_NUMNODES; nid++) {
+ 		NODE_DATA(nid) = memblock_alloc(sizeof(pg_data_t), 8);
+ 		if (!NODE_DATA(nid))
+ 			panic("%s: Failed to allocate %zu bytes align=0x%x\n",
+ 			      __func__, sizeof(pg_data_t), 8);
+ 	}
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	for_each_online_node(nid) {
  		unsigned long start_pfn, end_pfn;
diff --cc arch/sh/mm/numa.c
index 830e8b3684e4,f7e4439deb17..000000000000
--- a/arch/sh/mm/numa.c
+++ b/arch/sh/mm/numa.c
@@@ -41,9 -41,12 +41,18 @@@ void __init setup_bootmem_node(int nid
  	__add_active_range(nid, start_pfn, end_pfn);
  
  	/* Node-local pgdat */
++<<<<<<< HEAD
 +	NODE_DATA(nid) = __va(memblock_alloc_base(sizeof(struct pglist_data),
 +					     SMP_CACHE_BYTES, end));
 +	memset(NODE_DATA(nid), 0, sizeof(struct pglist_data));
++=======
+ 	NODE_DATA(nid) = memblock_alloc_node(sizeof(struct pglist_data),
+ 					     SMP_CACHE_BYTES, nid);
+ 	if (!NODE_DATA(nid))
+ 		panic("%s: Failed to allocate %zu bytes align=0x%x nid=%d\n",
+ 		      __func__, sizeof(struct pglist_data), SMP_CACHE_BYTES,
+ 		      nid);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	NODE_DATA(nid)->node_start_pfn = start_pfn;
  	NODE_DATA(nid)->node_spanned_pages = end_pfn - start_pfn;
diff --cc arch/um/drivers/net_kern.c
index d5b6a1cb3a4c,6e5be5fb4143..000000000000
--- a/arch/um/drivers/net_kern.c
+++ b/arch/um/drivers/net_kern.c
@@@ -650,7 -648,10 +650,14 @@@ static int __init eth_setup(char *str
  		return 1;
  	}
  
++<<<<<<< HEAD
 +	new = memblock_alloc(sizeof(*new), 0);
++=======
+ 	new = memblock_alloc(sizeof(*new), SMP_CACHE_BYTES);
+ 	if (!new)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(*new));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	INIT_LIST_HEAD(&new->list);
  	new->index = n;
diff --cc arch/um/drivers/vector_kern.c
index e7bfb21d7532,596e7056f376..000000000000
--- a/arch/um/drivers/vector_kern.c
+++ b/arch/um/drivers/vector_kern.c
@@@ -1580,7 -1575,10 +1580,14 @@@ static int __init vector_setup(char *st
  				 str, error);
  		return 1;
  	}
++<<<<<<< HEAD
 +	new = memblock_alloc(sizeof(*new), 0);
++=======
+ 	new = memblock_alloc(sizeof(*new), SMP_CACHE_BYTES);
+ 	if (!new)
+ 		panic("%s: Failed to allocate %zu bytes\n", __func__,
+ 		      sizeof(*new));
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  	INIT_LIST_HEAD(&new->list);
  	new->unit = n;
  	new->arguments = str;
diff --cc arch/um/kernel/initrd.c
index 3678f5b05e42,1dcd310cb34d..000000000000
--- a/arch/um/kernel/initrd.c
+++ b/arch/um/kernel/initrd.c
@@@ -36,7 -36,9 +36,13 @@@ int __init read_initrd(void
  		return 0;
  	}
  
++<<<<<<< HEAD
 +	area = memblock_alloc(size, 0);
++=======
+ 	area = memblock_alloc(size, SMP_CACHE_BYTES);
+ 	if (!area)
+ 		panic("%s: Failed to allocate %llu bytes\n", __func__, size);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	if (load_initrd(initrd, area, size) == -1)
  		return 0;
diff --cc arch/unicore32/kernel/setup.c
index b2c38b32ea57,d3239cf2e837..000000000000
--- a/arch/unicore32/kernel/setup.c
+++ b/arch/unicore32/kernel/setup.c
@@@ -206,7 -206,11 +206,15 @@@ request_standard_resources(struct memin
  		if (mi->bank[i].size == 0)
  			continue;
  
++<<<<<<< HEAD
 +		res = memblock_alloc_low(sizeof(*res), 0);
++=======
+ 		res = memblock_alloc_low(sizeof(*res), SMP_CACHE_BYTES);
+ 		if (!res)
+ 			panic("%s: Failed to allocate %zu bytes align=%x\n",
+ 			      __func__, sizeof(*res), SMP_CACHE_BYTES);
+ 
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  		res->name  = "System RAM";
  		res->start = mi->bank[i].start;
  		res->end   = mi->bank[i].start + mi->bank[i].size - 1;
diff --cc arch/unicore32/mm/mmu.c
index 040a8c279761,aa2060beb408..000000000000
--- a/arch/unicore32/mm/mmu.c
+++ b/arch/unicore32/mm/mmu.c
@@@ -152,7 -145,13 +152,17 @@@ static pte_t * __init early_pte_alloc(p
  		unsigned long prot)
  {
  	if (pmd_none(*pmd)) {
++<<<<<<< HEAD
 +		pte_t *pte = early_alloc(PTRS_PER_PTE * sizeof(pte_t));
++=======
+ 		size_t size = PTRS_PER_PTE * sizeof(pte_t);
+ 		pte_t *pte = memblock_alloc(size, size);
+ 
+ 		if (!pte)
+ 			panic("%s: Failed to allocate %zu bytes align=%zx\n",
+ 			      __func__, size, size);
+ 
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  		__pmd_populate(pmd, __pa(pte) | prot);
  	}
  	BUG_ON(pmd_bad(*pmd));
@@@ -354,7 -353,10 +364,14 @@@ static void __init devicemaps_init(void
  	/*
  	 * Allocate the vector page early.
  	 */
++<<<<<<< HEAD
 +	vectors = early_alloc(PAGE_SIZE);
++=======
+ 	vectors = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 	if (!vectors)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	for (addr = VMALLOC_END; addr; addr += PGDIR_SIZE)
  		pmd_clear(pmd_off_k(addr));
@@@ -431,7 -433,10 +448,14 @@@ void __init paging_init(void
  	top_pmd = pmd_off_k(0xffff0000);
  
  	/* allocate the zero page. */
++<<<<<<< HEAD
 +	zero_page = early_alloc(PAGE_SIZE);
++=======
+ 	zero_page = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 	if (!zero_page)
+ 		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+ 		      __func__, PAGE_SIZE, PAGE_SIZE);
++>>>>>>> 8a7f97b902f4 (treewide: add checks for the return value of memblock_alloc*())
  
  	bootmem_init();
  
* Unmerged path arch/csky/mm/highmem.c
* Unmerged path arch/alpha/kernel/core_cia.c
* Unmerged path arch/alpha/kernel/core_marvel.c
* Unmerged path arch/alpha/kernel/pci-noop.c
* Unmerged path arch/alpha/kernel/pci.c
* Unmerged path arch/alpha/kernel/pci_iommu.c
diff --git a/arch/arc/mm/highmem.c b/arch/arc/mm/highmem.c
index 48e700151810..11f57e2ced8a 100644
--- a/arch/arc/mm/highmem.c
+++ b/arch/arc/mm/highmem.c
@@ -124,6 +124,10 @@ static noinline pte_t * __init alloc_kmap_pgtable(unsigned long kvaddr)
 	pmd_k = pmd_offset(pud_k, kvaddr);
 
 	pte_k = (pte_t *)memblock_alloc_low(PAGE_SIZE, PAGE_SIZE);
+	if (!pte_k)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
+
 	pmd_populate_kernel(&init_mm, pmd_k, pte_k);
 	return pte_k;
 }
* Unmerged path arch/arm/kernel/setup.c
* Unmerged path arch/arm/mm/mmu.c
diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c
index 733db6f0c609..3b91acee177d 100644
--- a/arch/arm64/kernel/setup.c
+++ b/arch/arm64/kernel/setup.c
@@ -209,6 +209,7 @@ static void __init request_standard_resources(void)
 	struct memblock_region *region;
 	struct resource *res;
 	unsigned long i = 0;
+	size_t res_size;
 
 	kernel_code.start   = __pa_symbol(_text);
 	kernel_code.end     = __pa_symbol(__init_begin - 1);
@@ -216,9 +217,10 @@ static void __init request_standard_resources(void)
 	kernel_data.end     = __pa_symbol(_end - 1);
 
 	num_standard_resources = memblock.memory.cnt;
-	standard_resources = memblock_alloc_low(num_standard_resources *
-					        sizeof(*standard_resources),
-					        SMP_CACHE_BYTES);
+	res_size = num_standard_resources * sizeof(*standard_resources);
+	standard_resources = memblock_alloc_low(res_size, SMP_CACHE_BYTES);
+	if (!standard_resources)
+		panic("%s: Failed to allocate %zu bytes\n", __func__, res_size);
 
 	for_each_memblock(memory, region) {
 		res = &standard_resources[i++];
* Unmerged path arch/arm64/mm/kasan_init.c
* Unmerged path arch/c6x/mm/dma-coherent.c
* Unmerged path arch/c6x/mm/init.c
* Unmerged path arch/csky/mm/highmem.c
* Unmerged path arch/h8300/mm/init.c
diff --git a/arch/m68k/atari/stram.c b/arch/m68k/atari/stram.c
index 6ffc204eb07d..6152f9f631d2 100644
--- a/arch/m68k/atari/stram.c
+++ b/arch/m68k/atari/stram.c
@@ -97,6 +97,10 @@ void __init atari_stram_reserve_pages(void *start_mem)
 		pr_debug("atari_stram pool: kernel in ST-RAM, using alloc_bootmem!\n");
 		stram_pool.start = (resource_size_t)memblock_alloc_low(pool_size,
 								       PAGE_SIZE);
+		if (!stram_pool.start)
+			panic("%s: Failed to allocate %lu bytes align=%lx\n",
+			      __func__, pool_size, PAGE_SIZE);
+
 		stram_pool.end = stram_pool.start + pool_size - 1;
 		request_resource(&iomem_resource, &stram_pool);
 		stram_virt_offset = 0;
diff --git a/arch/m68k/mm/init.c b/arch/m68k/mm/init.c
index 732a836d10f3..6428ca8e700a 100644
--- a/arch/m68k/mm/init.c
+++ b/arch/m68k/mm/init.c
@@ -95,6 +95,9 @@ void __init paging_init(void)
 	high_memory = (void *) end_mem;
 
 	empty_zero_page = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+	if (!empty_zero_page)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
 
 	/*
 	 * Set up SFC/DFC registers (user data space).
* Unmerged path arch/m68k/mm/mcfmmu.c
diff --git a/arch/m68k/mm/motorola.c b/arch/m68k/mm/motorola.c
index 35a3624e1a61..460acebadaaa 100644
--- a/arch/m68k/mm/motorola.c
+++ b/arch/m68k/mm/motorola.c
@@ -55,6 +55,9 @@ static pte_t * __init kernel_page_table(void)
 	pte_t *ptablep;
 
 	ptablep = (pte_t *)memblock_alloc_low(PAGE_SIZE, PAGE_SIZE);
+	if (!ptablep)
+		panic("%s: Failed to allocate %lu bytes align=%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
 
 	clear_page(ptablep);
 	__flush_page_to_ram(ptablep);
@@ -96,6 +99,9 @@ static pmd_t * __init kernel_ptr_table(void)
 	if (((unsigned long)last_pgtable & ~PAGE_MASK) == 0) {
 		last_pgtable = (pmd_t *)memblock_alloc_low(PAGE_SIZE,
 							   PAGE_SIZE);
+		if (!last_pgtable)
+			panic("%s: Failed to allocate %lu bytes align=%lx\n",
+			      __func__, PAGE_SIZE, PAGE_SIZE);
 
 		clear_page(last_pgtable);
 		__flush_page_to_ram(last_pgtable);
@@ -290,6 +296,9 @@ void __init paging_init(void)
 	 * to a couple of allocated pages
 	 */
 	empty_zero_page = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+	if (!empty_zero_page)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
 
 	/*
 	 * Set up SFC/DFC registers
diff --git a/arch/m68k/mm/sun3mmu.c b/arch/m68k/mm/sun3mmu.c
index f736db48a2e1..eca1c46bb90a 100644
--- a/arch/m68k/mm/sun3mmu.c
+++ b/arch/m68k/mm/sun3mmu.c
@@ -46,6 +46,9 @@ void __init paging_init(void)
 	unsigned long size;
 
 	empty_zero_page = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+	if (!empty_zero_page)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
 
 	address = PAGE_OFFSET;
 	pg_dir = swapper_pg_dir;
@@ -56,6 +59,9 @@ void __init paging_init(void)
 	size = (size + PAGE_SIZE) & ~(PAGE_SIZE-1);
 
 	next_pgtable = (unsigned long)memblock_alloc(size, PAGE_SIZE);
+	if (!next_pgtable)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, size, PAGE_SIZE);
 	bootmem_end = (next_pgtable + size + PAGE_SIZE) & PAGE_MASK;
 
 	/* Map whole memory from PAGE_OFFSET (0x0E000000) */
* Unmerged path arch/m68k/sun3/sun3dvma.c
* Unmerged path arch/microblaze/mm/init.c
diff --git a/arch/mips/cavium-octeon/dma-octeon.c b/arch/mips/cavium-octeon/dma-octeon.c
index a598807e1e94..fe699a52b00f 100644
--- a/arch/mips/cavium-octeon/dma-octeon.c
+++ b/arch/mips/cavium-octeon/dma-octeon.c
@@ -280,6 +280,9 @@ void __init plat_swiotlb_setup(void)
 	swiotlbsize = swiotlb_nslabs << IO_TLB_SHIFT;
 
 	octeon_swiotlb = memblock_alloc_low(swiotlbsize, PAGE_SIZE);
+	if (!octeon_swiotlb)
+		panic("%s: Failed to allocate %zu bytes align=%lx\n",
+		      __func__, swiotlbsize, PAGE_SIZE);
 
 	if (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)
 		panic("Cannot allocate SWIOTLB buffer");
* Unmerged path arch/mips/kernel/setup.c
* Unmerged path arch/mips/kernel/traps.c
diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 1e3df5832fd7..ce230a72c724 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -246,6 +246,11 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 				if (pmd_none(*pmd)) {
 					pte = (pte_t *) memblock_alloc_low(PAGE_SIZE,
 									   PAGE_SIZE);
+					if (!pte)
+						panic("%s: Failed to allocate %lu bytes align=%lx\n",
+						      __func__, PAGE_SIZE,
+						      PAGE_SIZE);
+
 					set_pmd(pmd, __pmd((unsigned long)pte));
 					BUG_ON(pte != pte_offset_kernel(pmd, 0));
 				}
* Unmerged path arch/nds32/mm/init.c
* Unmerged path arch/openrisc/mm/ioremap.c
* Unmerged path arch/powerpc/kernel/dt_cpu_ftrs.c
diff --git a/arch/powerpc/kernel/pci_32.c b/arch/powerpc/kernel/pci_32.c
index e7203bae6902..a1a079e95dca 100644
--- a/arch/powerpc/kernel/pci_32.c
+++ b/arch/powerpc/kernel/pci_32.c
@@ -206,6 +206,9 @@ pci_create_OF_bus_map(void)
 
 	of_prop = memblock_alloc(sizeof(struct property) + 256,
 				 SMP_CACHE_BYTES);
+	if (!of_prop)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      sizeof(struct property) + 256);
 	dn = of_find_node_by_path("/");
 	if (dn) {
 		memset(of_prop, -1, sizeof(struct property) + 256);
* Unmerged path arch/powerpc/kernel/setup-common.c
* Unmerged path arch/powerpc/kernel/setup_64.c
diff --git a/arch/powerpc/lib/alloc.c b/arch/powerpc/lib/alloc.c
index dedf88a76f58..ce180870bd52 100644
--- a/arch/powerpc/lib/alloc.c
+++ b/arch/powerpc/lib/alloc.c
@@ -15,6 +15,9 @@ void * __ref zalloc_maybe_bootmem(size_t size, gfp_t mask)
 		p = kzalloc(size, mask);
 	else {
 		p = memblock_alloc(size, SMP_CACHE_BYTES);
+		if (!p)
+			panic("%s: Failed to allocate %zu bytes\n", __func__,
+			      size);
 	}
 	return p;
 }
* Unmerged path arch/powerpc/mm/book3s32/mmu.c
* Unmerged path arch/powerpc/mm/book3s64/hash_utils.c
* Unmerged path arch/powerpc/mm/book3s64/pgtable.c
* Unmerged path arch/powerpc/mm/book3s64/radix_pgtable.c
diff --git a/arch/powerpc/mm/mmu_context_nohash.c b/arch/powerpc/mm/mmu_context_nohash.c
index b177263bee42..9eb5e55463be 100644
--- a/arch/powerpc/mm/mmu_context_nohash.c
+++ b/arch/powerpc/mm/mmu_context_nohash.c
@@ -462,10 +462,19 @@ void __init mmu_context_init(void)
 	 * Allocate the maps used by context management
 	 */
 	context_map = memblock_alloc(CTX_MAP_SIZE, SMP_CACHE_BYTES);
+	if (!context_map)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      CTX_MAP_SIZE);
 	context_mm = memblock_alloc(sizeof(void *) * (LAST_CONTEXT + 1),
 				    SMP_CACHE_BYTES);
+	if (!context_mm)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      sizeof(void *) * (LAST_CONTEXT + 1));
 #ifdef CONFIG_SMP
 	stale_map[boot_cpuid] = memblock_alloc(CTX_MAP_SIZE, SMP_CACHE_BYTES);
+	if (!stale_map[boot_cpuid])
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      CTX_MAP_SIZE);
 
 	cpuhp_setup_state_nocalls(CPUHP_POWERPC_MMU_CTX_PREPARE,
 				  "powerpc/mmu/ctx:prepare",
* Unmerged path arch/powerpc/mm/pgtable-book3e.c
* Unmerged path arch/powerpc/platforms/pasemi/iommu.c
diff --git a/arch/powerpc/platforms/powermac/nvram.c b/arch/powerpc/platforms/powermac/nvram.c
index ae54d7fe68f3..e0a1d1573460 100644
--- a/arch/powerpc/platforms/powermac/nvram.c
+++ b/arch/powerpc/platforms/powermac/nvram.c
@@ -514,6 +514,9 @@ static int __init core99_nvram_setup(struct device_node *dp, unsigned long addr)
 		return -EINVAL;
 	}
 	nvram_image = memblock_alloc(NVRAM_SIZE, SMP_CACHE_BYTES);
+	if (!nvram_image)
+		panic("%s: Failed to allocate %u bytes\n", __func__,
+		      NVRAM_SIZE);
 	nvram_data = ioremap(addr, NVRAM_SIZE*2);
 	nvram_naddrs = 1; /* Make sure we get the correct case */
 
* Unmerged path arch/powerpc/platforms/powernv/opal.c
diff --git a/arch/powerpc/platforms/powernv/pci-ioda.c b/arch/powerpc/platforms/powernv/pci-ioda.c
index 27eedcf13f99..9b65ef8e5392 100644
--- a/arch/powerpc/platforms/powernv/pci-ioda.c
+++ b/arch/powerpc/platforms/powernv/pci-ioda.c
@@ -3752,6 +3752,9 @@ static void __init pnv_pci_init_ioda_phb(struct device_node *np,
 	pr_debug("  PHB-ID  : 0x%016llx\n", phb_id);
 
 	phb = memblock_alloc(sizeof(*phb), SMP_CACHE_BYTES);
+	if (!phb)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      sizeof(*phb));
 
 	/* Allocate PCI controller */
 	phb->hose = hose = pcibios_alloc_controller(np);
@@ -3798,6 +3801,9 @@ static void __init pnv_pci_init_ioda_phb(struct device_node *np,
 		phb->diag_data_size = PNV_PCI_DIAG_BUF_SIZE;
 
 	phb->diag_data = memblock_alloc(phb->diag_data_size, SMP_CACHE_BYTES);
+	if (!phb->diag_data)
+		panic("%s: Failed to allocate %u bytes\n", __func__,
+		      phb->diag_data_size);
 
 	/* Parse 32-bit and IO ranges (if any) */
 	pci_process_bridge_OF_ranges(hose, np, !hose->global_number);
@@ -3857,6 +3863,8 @@ static void __init pnv_pci_init_ioda_phb(struct device_node *np,
 	pemap_off = size;
 	size += phb->ioda.total_pe_num * sizeof(struct pnv_ioda_pe);
 	aux = memblock_alloc(size, SMP_CACHE_BYTES);
+	if (!aux)
+		panic("%s: Failed to allocate %lu bytes\n", __func__, size);
 	phb->ioda.pe_alloc = aux;
 	phb->ioda.m64_segmap = aux + m64map_off;
 	phb->ioda.m32_segmap = aux + m32map_off;
diff --git a/arch/powerpc/platforms/ps3/setup.c b/arch/powerpc/platforms/ps3/setup.c
index 658bfab3350b..4ce5458eb0f8 100644
--- a/arch/powerpc/platforms/ps3/setup.c
+++ b/arch/powerpc/platforms/ps3/setup.c
@@ -127,6 +127,9 @@ static void __init prealloc(struct ps3_prealloc *p)
 		return;
 
 	p->address = memblock_alloc(p->size, p->align);
+	if (!p->address)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, p->size, p->align);
 
 	printk(KERN_INFO "%s: %lu bytes at %p\n", p->name, p->size,
 	       p->address);
diff --git a/arch/powerpc/sysdev/msi_bitmap.c b/arch/powerpc/sysdev/msi_bitmap.c
index 26a117ee51b0..12724ea8579f 100644
--- a/arch/powerpc/sysdev/msi_bitmap.c
+++ b/arch/powerpc/sysdev/msi_bitmap.c
@@ -129,6 +129,9 @@ int __ref msi_bitmap_alloc(struct msi_bitmap *bmp, unsigned int irq_count,
 		bmp->bitmap = kzalloc(size, GFP_KERNEL);
 	else {
 		bmp->bitmap = memblock_alloc(size, SMP_CACHE_BYTES);
+		if (!bmp->bitmap)
+			panic("%s: Failed to allocate %u bytes\n", __func__,
+			      size);
 		/* the bitmap won't be freed from memblock allocator */
 		kmemleak_not_leak(bmp->bitmap);
 	}
* Unmerged path arch/s390/kernel/setup.c
* Unmerged path arch/s390/kernel/smp.c
diff --git a/arch/s390/kernel/topology.c b/arch/s390/kernel/topology.c
index 7d2631e79ee9..c5ceb7e128c1 100644
--- a/arch/s390/kernel/topology.c
+++ b/arch/s390/kernel/topology.c
@@ -522,6 +522,9 @@ static void __init alloc_masks(struct sysinfo_15_1_x *info,
 	nr_masks = max(nr_masks, 1);
 	for (i = 0; i < nr_masks; i++) {
 		mask->next = memblock_alloc(sizeof(*mask->next), 8);
+		if (!mask->next)
+			panic("%s: Failed to allocate %zu bytes align=0x%x\n",
+			      __func__, sizeof(*mask->next), 8);
 		mask = mask->next;
 	}
 }
@@ -540,6 +543,9 @@ void __init topology_init_early(void)
 	if (!MACHINE_HAS_TOPOLOGY)
 		goto out;
 	tl_info = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+	if (!tl_info)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
 	info = tl_info;
 	store_topology(info);
 	pr_info("The CPU configuration topology of the machine is: %d %d %d %d %d %d / %d\n",
diff --git a/arch/s390/numa/mode_emu.c b/arch/s390/numa/mode_emu.c
index bfba273c32c0..71a12a4f4906 100644
--- a/arch/s390/numa/mode_emu.c
+++ b/arch/s390/numa/mode_emu.c
@@ -313,6 +313,9 @@ static void __ref create_core_to_node_map(void)
 	int i;
 
 	emu_cores = memblock_alloc(sizeof(*emu_cores), 8);
+	if (!emu_cores)
+		panic("%s: Failed to allocate %zu bytes align=0x%x\n",
+		      __func__, sizeof(*emu_cores), 8);
 	for (i = 0; i < ARRAY_SIZE(emu_cores->to_node_id); i++)
 		emu_cores->to_node_id[i] = NODE_ID_FREE;
 }
* Unmerged path arch/s390/numa/numa.c
diff --git a/arch/sh/mm/init.c b/arch/sh/mm/init.c
index db47300bc72b..c34f6f75487b 100644
--- a/arch/sh/mm/init.c
+++ b/arch/sh/mm/init.c
@@ -128,6 +128,9 @@ static pmd_t * __init one_md_table_init(pud_t *pud)
 		pmd_t *pmd;
 
 		pmd = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+		if (!pmd)
+			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+			      __func__, PAGE_SIZE, PAGE_SIZE);
 		pud_populate(&init_mm, pud, pmd);
 		BUG_ON(pmd != pmd_offset(pud, 0));
 	}
@@ -141,6 +144,9 @@ static pte_t * __init one_page_table_init(pmd_t *pmd)
 		pte_t *pte;
 
 		pte = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+		if (!pte)
+			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+			      __func__, PAGE_SIZE, PAGE_SIZE);
 		pmd_populate_kernel(&init_mm, pmd, pte);
 		BUG_ON(pte != pte_offset_kernel(pmd, 0));
 	}
* Unmerged path arch/sh/mm/numa.c
* Unmerged path arch/um/drivers/net_kern.c
* Unmerged path arch/um/drivers/vector_kern.c
* Unmerged path arch/um/kernel/initrd.c
diff --git a/arch/um/kernel/mem.c b/arch/um/kernel/mem.c
index 2da209687a22..1f2a6528445c 100644
--- a/arch/um/kernel/mem.c
+++ b/arch/um/kernel/mem.c
@@ -66,6 +66,10 @@ static void __init one_page_table_init(pmd_t *pmd)
 	if (pmd_none(*pmd)) {
 		pte_t *pte = (pte_t *) memblock_alloc_low(PAGE_SIZE,
 							  PAGE_SIZE);
+		if (!pte)
+			panic("%s: Failed to allocate %lu bytes align=%lx\n",
+			      __func__, PAGE_SIZE, PAGE_SIZE);
+
 		set_pmd(pmd, __pmd(_KERNPG_TABLE +
 					   (unsigned long) __pa(pte)));
 		if (pte != pte_offset_kernel(pmd, 0))
@@ -77,6 +81,10 @@ static void __init one_md_table_init(pud_t *pud)
 {
 #ifdef CONFIG_3_LEVEL_PGTABLES
 	pmd_t *pmd_table = (pmd_t *) memblock_alloc_low(PAGE_SIZE, PAGE_SIZE);
+	if (!pmd_table)
+		panic("%s: Failed to allocate %lu bytes align=%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
+
 	set_pud(pud, __pud(_KERNPG_TABLE + (unsigned long) __pa(pmd_table)));
 	if (pmd_table != pmd_offset(pud, 0))
 		BUG();
@@ -126,6 +134,10 @@ static void __init fixaddr_user_init( void)
 
 	fixrange_init( FIXADDR_USER_START, FIXADDR_USER_END, swapper_pg_dir);
 	v = (unsigned long) memblock_alloc_low(size, PAGE_SIZE);
+	if (!v)
+		panic("%s: Failed to allocate %lu bytes align=%lx\n",
+		      __func__, size, PAGE_SIZE);
+
 	memcpy((void *) v , (void *) FIXADDR_USER_START, size);
 	p = __pa(v);
 	for ( ; size > 0; size -= PAGE_SIZE, vaddr += PAGE_SIZE,
@@ -146,6 +158,10 @@ void __init paging_init(void)
 
 	empty_zero_page = (unsigned long *) memblock_alloc_low(PAGE_SIZE,
 							       PAGE_SIZE);
+	if (!empty_zero_page)
+		panic("%s: Failed to allocate %lu bytes align=%lx\n",
+		      __func__, PAGE_SIZE, PAGE_SIZE);
+
 	for (i = 0; i < ARRAY_SIZE(zones_size); i++)
 		zones_size[i] = 0;
 
* Unmerged path arch/unicore32/kernel/setup.c
* Unmerged path arch/unicore32/mm/mmu.c
diff --git a/arch/x86/kernel/acpi/boot.c b/arch/x86/kernel/acpi/boot.c
index a43bf26d1a18..29657fdfb7ff 100644
--- a/arch/x86/kernel/acpi/boot.c
+++ b/arch/x86/kernel/acpi/boot.c
@@ -936,6 +936,9 @@ static int __init acpi_parse_hpet(struct acpi_table_header *table)
 #define HPET_RESOURCE_NAME_SIZE 9
 	hpet_res = memblock_alloc(sizeof(*hpet_res) + HPET_RESOURCE_NAME_SIZE,
 				  SMP_CACHE_BYTES);
+	if (!hpet_res)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      sizeof(*hpet_res) + HPET_RESOURCE_NAME_SIZE);
 
 	hpet_res->name = (void *)&hpet_res[1];
 	hpet_res->flags = IORESOURCE_MEM;
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index fbac72520919..eb8fa2e54280 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -2633,6 +2633,8 @@ static struct resource * __init ioapic_setup_resources(void)
 	n *= nr_ioapics;
 
 	mem = memblock_alloc(n, SMP_CACHE_BYTES);
+	if (!mem)
+		panic("%s: Failed to allocate %lu bytes\n", __func__, n);
 	res = (void *)mem;
 
 	mem += sizeof(struct resource) * nr_ioapics;
@@ -2677,6 +2679,9 @@ void __init io_apic_init_mappings(void)
 #endif
 			ioapic_phys = (unsigned long)memblock_alloc(PAGE_SIZE,
 								    PAGE_SIZE);
+			if (!ioapic_phys)
+				panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+				      __func__, PAGE_SIZE, PAGE_SIZE);
 			ioapic_phys = __pa(ioapic_phys);
 		}
 		set_fixmap_nocache(idx, ioapic_phys);
diff --git a/arch/x86/kernel/e820.c b/arch/x86/kernel/e820.c
index 7e8defafd141..b285564d4c30 100644
--- a/arch/x86/kernel/e820.c
+++ b/arch/x86/kernel/e820.c
@@ -1117,6 +1117,9 @@ void __init e820__reserve_resources(void)
 
 	res = memblock_alloc(sizeof(*res) * e820_table->nr_entries,
 			     SMP_CACHE_BYTES);
+	if (!res)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      sizeof(*res) * e820_table->nr_entries);
 	e820_res = res;
 
 	for (i = 0; i < e820_table->nr_entries; i++) {
diff --git a/arch/x86/platform/olpc/olpc_dt.c b/arch/x86/platform/olpc/olpc_dt.c
index 24d2175a9480..9d67c1434607 100644
--- a/arch/x86/platform/olpc/olpc_dt.c
+++ b/arch/x86/platform/olpc/olpc_dt.c
@@ -142,6 +142,9 @@ void * __init prom_early_alloc(unsigned long size)
 		 * wasted bootmem) and hand off chunks of it to callers.
 		 */
 		res = memblock_alloc(chunk_size, SMP_CACHE_BYTES);
+		if (!res)
+			panic("%s: Failed to allocate %zu bytes\n", __func__,
+			      chunk_size);
 		BUG_ON(!res);
 		prom_early_allocated += chunk_size;
 		memset(res, 0, chunk_size);
diff --git a/arch/x86/xen/p2m.c b/arch/x86/xen/p2m.c
index c0ca6726a7d5..dcdee6e6e84b 100644
--- a/arch/x86/xen/p2m.c
+++ b/arch/x86/xen/p2m.c
@@ -179,8 +179,15 @@ static void p2m_init_identity(unsigned long *p2m, unsigned long pfn)
 
 static void * __ref alloc_p2m_page(void)
 {
-	if (unlikely(!slab_is_available()))
-		return memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+	if (unlikely(!slab_is_available())) {
+		void *ptr = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+
+		if (!ptr)
+			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+			      __func__, PAGE_SIZE, PAGE_SIZE);
+
+		return ptr;
+	}
 
 	return (void *)__get_free_page(GFP_KERNEL);
 }
diff --git a/arch/xtensa/mm/kasan_init.c b/arch/xtensa/mm/kasan_init.c
index 1734cda6bc4a..7dfac0a75171 100644
--- a/arch/xtensa/mm/kasan_init.c
+++ b/arch/xtensa/mm/kasan_init.c
@@ -45,6 +45,10 @@ static void __init populate(void *start, void *end)
 	pmd_t *pmd = pmd_offset(pgd, vaddr);
 	pte_t *pte = memblock_alloc(n_pages * sizeof(pte_t), PAGE_SIZE);
 
+	if (!pte)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, n_pages * sizeof(pte_t), PAGE_SIZE);
+
 	pr_debug("%s: %p - %p\n", __func__, start, end);
 
 	for (i = j = 0; i < n_pmds; ++i) {
diff --git a/arch/xtensa/mm/mmu.c b/arch/xtensa/mm/mmu.c
index a4dcfd39bc5c..2fb7d1172228 100644
--- a/arch/xtensa/mm/mmu.c
+++ b/arch/xtensa/mm/mmu.c
@@ -32,6 +32,9 @@ static void * __init init_pmd(unsigned long vaddr, unsigned long n_pages)
 		 __func__, vaddr, n_pages);
 
 	pte = memblock_alloc_low(n_pages * sizeof(pte_t), PAGE_SIZE);
+	if (!pte)
+		panic("%s: Failed to allocate %zu bytes align=%lx\n",
+		      __func__, n_pages * sizeof(pte_t), PAGE_SIZE);
 
 	for (i = 0; i < n_pages; ++i)
 		pte_clear(NULL, 0, pte + i);
diff --git a/drivers/clk/ti/clk.c b/drivers/clk/ti/clk.c
index e205af814582..68d23b6c5119 100644
--- a/drivers/clk/ti/clk.c
+++ b/drivers/clk/ti/clk.c
@@ -343,6 +343,9 @@ void __init omap2_clk_legacy_provider_init(int index, void __iomem *mem)
 	struct clk_iomap *io;
 
 	io = memblock_alloc(sizeof(*io), SMP_CACHE_BYTES);
+	if (!io)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      sizeof(*io));
 
 	io->mem = mem;
 
diff --git a/drivers/macintosh/smu.c b/drivers/macintosh/smu.c
index 3f0da2cc7a34..6231b0111b03 100644
--- a/drivers/macintosh/smu.c
+++ b/drivers/macintosh/smu.c
@@ -493,6 +493,9 @@ int __init smu_init (void)
 	}
 
 	smu = memblock_alloc(sizeof(struct smu_device), SMP_CACHE_BYTES);
+	if (!smu)
+		panic("%s: Failed to allocate %zu bytes\n", __func__,
+		      sizeof(struct smu_device));
 
 	spin_lock_init(&smu->lock);
 	INIT_LIST_HEAD(&smu->cmd_list);
diff --git a/drivers/of/fdt.c b/drivers/of/fdt.c
index 6aee080762a3..5825d57e97ec 100644
--- a/drivers/of/fdt.c
+++ b/drivers/of/fdt.c
@@ -1184,7 +1184,13 @@ int __init __weak early_init_dt_reserve_memory_arch(phys_addr_t base,
 
 static void * __init early_init_dt_alloc_memory_arch(u64 size, u64 align)
 {
-	return memblock_alloc(size, align);
+	void *ptr = memblock_alloc(size, align);
+
+	if (!ptr)
+		panic("%s: Failed to allocate %llu bytes align=0x%llx\n",
+		      __func__, size, align);
+
+	return ptr;
 }
 
 bool __init early_init_dt_verify(void *params)
diff --git a/drivers/of/unittest.c b/drivers/of/unittest.c
index c3cb30f18ba3..1ff70171661e 100644
--- a/drivers/of/unittest.c
+++ b/drivers/of/unittest.c
@@ -2725,7 +2725,13 @@ static struct device_node *overlay_base_root;
 
 static void * __init dt_alloc_memory(u64 size, u64 align)
 {
-	return memblock_alloc(size, align);
+	void *ptr = memblock_alloc(size, align);
+
+	if (!ptr)
+		panic("%s: Failed to allocate %llu bytes align=0x%llx\n",
+		      __func__, size, align);
+
+	return ptr;
 }
 
 /*
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 017ffaa17b2a..7dd030dd3b2a 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -215,10 +215,13 @@ int __ref xen_swiotlb_init(int verbose, bool early)
 	/*
 	 * Get IO TLB memory from any location.
 	 */
-	if (early)
+	if (early) {
 		xen_io_tlb_start = memblock_alloc(PAGE_ALIGN(bytes),
 						  PAGE_SIZE);
-	else {
+		if (!xen_io_tlb_start)
+			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+			      __func__, PAGE_ALIGN(bytes), PAGE_SIZE);
+	} else {
 #define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))
 #define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)
 		while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index fc52a7a0134e..d7a520806b7e 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -216,13 +216,13 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 	alloc_size = PAGE_ALIGN(io_tlb_nslabs * sizeof(int));
 	io_tlb_list = memblock_alloc(alloc_size, PAGE_SIZE);
 	if (!io_tlb_list)
-		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 		      __func__, alloc_size, PAGE_SIZE);
 
 	alloc_size = PAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t));
 	io_tlb_orig_addr = memblock_alloc(alloc_size, PAGE_SIZE);
 	if (!io_tlb_orig_addr)
-		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 		      __func__, alloc_size, PAGE_SIZE);
 
 	for (i = 0; i < io_tlb_nslabs; i++) {
diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c
index 98502fc502c6..b397b6bcf5fb 100644
--- a/kernel/power/snapshot.c
+++ b/kernel/power/snapshot.c
@@ -965,6 +965,9 @@ void __init __register_nosave_region(unsigned long start_pfn,
 		/* This allocation cannot fail */
 		region = memblock_alloc(sizeof(struct nosave_region),
 					SMP_CACHE_BYTES);
+		if (!region)
+			panic("%s: Failed to allocate %zu bytes\n", __func__,
+			      sizeof(struct nosave_region));
 	}
 	region->start_pfn = start_pfn;
 	region->end_pfn = end_pfn;
diff --git a/lib/cpumask.c b/lib/cpumask.c
index 955aa1a6a2f6..65d757c0c13e 100644
--- a/lib/cpumask.c
+++ b/lib/cpumask.c
@@ -165,6 +165,9 @@ EXPORT_SYMBOL(zalloc_cpumask_var);
 void __init alloc_bootmem_cpumask_var(cpumask_var_t *mask)
 {
 	*mask = memblock_alloc(cpumask_size(), SMP_CACHE_BYTES);
+	if (!*mask)
+		panic("%s: Failed to allocate %u bytes\n", __func__,
+		      cpumask_size());
 }
 
 /**
diff --git a/mm/kasan/kasan_init.c b/mm/kasan/kasan_init.c
index f39a7f65a279..a9d036ae70b6 100644
--- a/mm/kasan/kasan_init.c
+++ b/mm/kasan/kasan_init.c
@@ -82,8 +82,14 @@ static inline bool kasan_early_shadow_page_entry(pte_t pte)
 
 static __init void *early_alloc(size_t size, int node)
 {
-	return memblock_alloc_try_nid(size, size, __pa(MAX_DMA_ADDRESS),
-					MEMBLOCK_ALLOC_ACCESSIBLE, node);
+	void *ptr = memblock_alloc_try_nid(size, size, __pa(MAX_DMA_ADDRESS),
+					   MEMBLOCK_ALLOC_ACCESSIBLE, node);
+
+	if (!ptr)
+		panic("%s: Failed to allocate %zu bytes align=%zx nid=%d from=%llx\n",
+		      __func__, size, size, node, (u64)__pa(MAX_DMA_ADDRESS));
+
+	return ptr;
 }
 
 static void __ref zero_pte_populate(pmd_t *pmd, unsigned long addr,
diff --git a/mm/sparse.c b/mm/sparse.c
index 2b18ff252bdb..7991daf3d9f9 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -65,11 +65,15 @@ static noinline struct mem_section __ref *sparse_index_alloc(int nid)
 	unsigned long array_size = SECTIONS_PER_ROOT *
 				   sizeof(struct mem_section);
 
-	if (slab_is_available())
+	if (slab_is_available()) {
 		section = kzalloc_node(array_size, GFP_KERNEL, nid);
-	else
+	} else {
 		section = memblock_alloc_node(array_size, SMP_CACHE_BYTES,
 					      nid);
+		if (!section)
+			panic("%s: Failed to allocate %lu bytes nid=%d\n",
+			      __func__, array_size, nid);
+	}
 
 	return section;
 }
@@ -250,6 +254,9 @@ void __init memory_present(int nid, unsigned long start, unsigned long end)
 		size = sizeof(struct mem_section*) * NR_SECTION_ROOTS;
 		align = 1 << (INTERNODE_CACHE_SHIFT);
 		mem_section = memblock_alloc(size, align);
+		if (!mem_section)
+			panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+			      __func__, size, align);
 	}
 #endif
 
@@ -420,13 +427,18 @@ struct page __init *__populate_section_memmap(unsigned long pfn,
 {
 	unsigned long size = section_map_size();
 	struct page *map = sparse_buffer_alloc(size);
+	phys_addr_t addr = __pa(MAX_DMA_ADDRESS);
 
 	if (map)
 		return map;
 
 	map = memblock_alloc_try_nid(size,
-					  PAGE_SIZE, __pa(MAX_DMA_ADDRESS),
+					  PAGE_SIZE, addr,
 					  MEMBLOCK_ALLOC_ACCESSIBLE, nid);
+	if (!map)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx nid=%d from=%pa\n",
+		      __func__, size, PAGE_SIZE, nid, &addr);
+
 	return map;
 }
 #endif /* !CONFIG_SPARSEMEM_VMEMMAP */
@@ -436,10 +448,11 @@ static void *sparsemap_buf_end __meminitdata;
 
 static void __init sparse_buffer_init(unsigned long size, int nid)
 {
+	phys_addr_t addr = __pa(MAX_DMA_ADDRESS);
 	WARN_ON(sparsemap_buf);	/* forgot to call sparse_buffer_fini()? */
 	sparsemap_buf =
 		memblock_alloc_try_nid_raw(size, PAGE_SIZE,
-						__pa(MAX_DMA_ADDRESS),
+						addr,
 						MEMBLOCK_ALLOC_ACCESSIBLE, nid);
 	sparsemap_buf_end = sparsemap_buf + size;
 }
