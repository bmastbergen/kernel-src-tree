mm/page_alloc: Introduce free_area_init_core_hotplug

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Oscar Salvador <osalvador@suse.de>
commit 03e85f9d5f1f8c74f127c5f7a87575d74a78d248
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/03e85f9d.failed

Currently, whenever a new node is created/re-used from the memhotplug
path, we call free_area_init_node()->free_area_init_core().  But there is
some code that we do not really need to run when we are coming from such
path.

free_area_init_core() performs the following actions:

1) Initializes pgdat internals, such as spinlock, waitqueues and more.
2) Account # nr_all_pages and # nr_kernel_pages. These values are used later on
   when creating hash tables.
3) Account number of managed_pages per zone, substracting dma_reserved and
   memmap pages.
4) Initializes some fields of the zone structure data
5) Calls init_currently_empty_zone to initialize all the freelists
6) Calls memmap_init to initialize all pages belonging to certain zone

When called from memhotplug path, free_area_init_core() only performs
actions #1 and #4.

Action #2 is pointless as the zones do not have any pages since either the
node was freed, or we are re-using it, eitherway all zones belonging to
this node should have 0 pages.  For the same reason, action #3 results
always in manages_pages being 0.

Action #5 and #6 are performed later on when onlining the pages:
 online_pages()->move_pfn_range_to_zone()->init_currently_empty_zone()
 online_pages()->move_pfn_range_to_zone()->memmap_init_zone()

This patch does two things:

First, moves the node/zone initializtion to their own function, so it
allows us to create a small version of free_area_init_core, where we only
perform:

1) Initialization of pgdat internals, such as spinlock, waitqueues and more
4) Initialization of some fields of the zone structure data

These two functions are: pgdat_init_internals() and zone_init_internals().

The second thing this patch does, is to introduce
free_area_init_core_hotplug(), the memhotplug version of
free_area_init_core():

Currently, we call free_area_init_node() from the memhotplug path.  In
there, we set some pgdat's fields, and call calculate_node_totalpages().
calculate_node_totalpages() calculates the # of pages the node has.

Since the node is either new, or we are re-using it, the zones belonging
to this node should not have any pages, so there is no point to calculate
this now.

Actually, we re-set these values to 0 later on with the calls to:

reset_node_managed_pages()
reset_node_present_pages()

The # of pages per node and the # of pages per zone will be calculated when
onlining the pages:

online_pages()->move_pfn_range()->move_pfn_range_to_zone()->resize_zone_range()
online_pages()->move_pfn_range()->move_pfn_range_to_zone()->resize_pgdat_range()

Also, since free_area_init_core/free_area_init_node will now only get called during early init, let us replace
__paginginit with __init, so their code gets freed up.

[osalvador@techadventures.net: fix section usage]
  Link: http://lkml.kernel.org/r/20180731101752.GA473@techadventures.net
[osalvador@suse.de: v6]
  Link: http://lkml.kernel.org/r/20180801122348.21588-6-osalvador@techadventures.net
Link: http://lkml.kernel.org/r/20180730101757.28058-5-osalvador@techadventures.net
	Signed-off-by: Oscar Salvador <osalvador@suse.de>
	Reviewed-by: Pavel Tatashin <pasha.tatashin@oracle.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Pasha Tatashin <Pavel.Tatashin@microsoft.com>
	Cc: Aaron Lu <aaron.lu@intel.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 03e85f9d5f1f8c74f127c5f7a87575d74a78d248)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memory_hotplug.h
#	mm/page_alloc.c
diff --cc include/linux/memory_hotplug.h
index 70b16b05a960,34a28227068d..000000000000
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@@ -334,27 -316,24 +334,33 @@@ static inline int offline_pages(unsigne
  	return -EINVAL;
  }
  
 -static inline void remove_memory(int nid, u64 start, u64 size) {}
 +static inline int remove_memory(int nid, u64 start, u64 size)
 +{
 +	return -EBUSY;
 +}
 +
 +static inline void __remove_memory(int nid, u64 start, u64 size) {}
  #endif /* CONFIG_MEMORY_HOTREMOVE */
  
++<<<<<<< HEAD
 +extern int __add_memory(int nid, u64 start, u64 size);
++=======
+ extern void __ref free_area_init_core_hotplug(int nid);
+ extern int walk_memory_range(unsigned long start_pfn, unsigned long end_pfn,
+ 		void *arg, int (*func)(struct memory_block *, void *));
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
  extern int add_memory(int nid, u64 start, u64 size);
 -extern int add_memory_resource(int nid, struct resource *resource, bool online);
 -extern int arch_add_memory(int nid, u64 start, u64 size,
 -		struct vmem_altmap *altmap, bool want_memblock);
 +extern int add_memory_resource(int nid, struct resource *resource);
  extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
  		unsigned long nr_pages, struct vmem_altmap *altmap);
 -extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 +extern void remove_pfn_range_from_zone(struct zone *zone,
 +				       unsigned long start_pfn,
 +				       unsigned long nr_pages);
  extern bool is_memblock_offlined(struct memory_block *mem);
 -extern void remove_memory(int nid, u64 start, u64 size);
 -extern int sparse_add_one_section(struct pglist_data *pgdat,
 -		unsigned long start_pfn, struct vmem_altmap *altmap);
 -extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
 +extern int sparse_add_section(int nid, unsigned long pfn,
 +		unsigned long nr_pages, struct vmem_altmap *altmap);
 +extern void sparse_remove_section(struct mem_section *ms,
 +		unsigned long pfn, unsigned long nr_pages,
  		unsigned long map_offset, struct vmem_altmap *altmap);
  extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
  					  unsigned long pnum);
diff --cc mm/page_alloc.c
index fa941d4649d5,c677c1506d73..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -6338,7 -6140,7 +6338,11 @@@ static inline void setup_usemap(struct 
  #ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE
  
  /* Initialise the number of pages represented by NR_PAGEBLOCK_BITS */
++<<<<<<< HEAD
 +void __paginginit set_pageblock_order(void)
++=======
+ void __init set_pageblock_order(void)
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
  {
  	unsigned int order;
  
@@@ -6366,14 -6168,14 +6370,23 @@@
   * include/linux/pageblock-flags.h for the values of pageblock_order based on
   * the kernel config
   */
++<<<<<<< HEAD
 +void __paginginit set_pageblock_order(void)
++=======
+ void __init set_pageblock_order(void)
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
  {
  }
  
  #endif /* CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */
  
++<<<<<<< HEAD
 +static unsigned long __paginginit calc_memmap_size(unsigned long spanned_pages,
 +						   unsigned long present_pages)
++=======
+ static unsigned long __init calc_memmap_size(unsigned long spanned_pages,
+ 						unsigned long present_pages)
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
  {
  	unsigned long pages = spanned_pages;
  
@@@ -6392,34 -6194,99 +6405,117 @@@
  	return PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;
  }
  
++<<<<<<< HEAD
 +/*
 + * Set up the zone data structures:
 + *   - mark all pages reserved
 + *   - mark all memory queues empty
 + *   - clear the memory bitmaps
 + *
 + * NOTE: pgdat should get zeroed by caller.
 + */
 +static void __paginginit free_area_init_core(struct pglist_data *pgdat)
++=======
+ #ifdef CONFIG_NUMA_BALANCING
+ static void pgdat_init_numabalancing(struct pglist_data *pgdat)
  {
- 	enum zone_type j;
- 	int nid = pgdat->node_id;
+ 	spin_lock_init(&pgdat->numabalancing_migrate_lock);
+ 	pgdat->numabalancing_migrate_nr_pages = 0;
+ 	pgdat->numabalancing_migrate_next_window = jiffies;
+ }
+ #else
+ static void pgdat_init_numabalancing(struct pglist_data *pgdat) {}
+ #endif
+ 
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ static void pgdat_init_split_queue(struct pglist_data *pgdat)
+ {
+ 	spin_lock_init(&pgdat->split_queue_lock);
+ 	INIT_LIST_HEAD(&pgdat->split_queue);
+ 	pgdat->split_queue_len = 0;
+ }
+ #else
+ static void pgdat_init_split_queue(struct pglist_data *pgdat) {}
+ #endif
+ 
+ #ifdef CONFIG_COMPACTION
+ static void pgdat_init_kcompactd(struct pglist_data *pgdat)
+ {
+ 	init_waitqueue_head(&pgdat->kcompactd_wait);
+ }
+ #else
+ static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}
+ #endif
  
+ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
+ {
  	pgdat_resize_init(pgdat);
 -
 -	pgdat_init_numabalancing(pgdat);
 -	pgdat_init_split_queue(pgdat);
 -	pgdat_init_kcompactd(pgdat);
 -
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +	spin_lock_init(&pgdat->split_queue_lock);
 +	INIT_LIST_HEAD(&pgdat->split_queue);
 +	pgdat->split_queue_len = 0;
 +#endif
  	init_waitqueue_head(&pgdat->kswapd_wait);
  	init_waitqueue_head(&pgdat->pfmemalloc_wait);
 -
 +#ifdef CONFIG_COMPACTION
 +	init_waitqueue_head(&pgdat->kcompactd_wait);
 +#endif
  	pgdat_page_ext_init(pgdat);
  	spin_lock_init(&pgdat->lru_lock);
++<<<<<<< HEAD
 +	lruvec_init(&pgdat->__lruvec);
++=======
+ 	lruvec_init(node_lruvec(pgdat));
+ }
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
  
+ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,
+ 							unsigned long remaining_pages)
+ {
+ 	zone->managed_pages = remaining_pages;
+ 	zone_set_nid(zone, nid);
+ 	zone->name = zone_names[idx];
+ 	zone->zone_pgdat = NODE_DATA(nid);
+ 	spin_lock_init(&zone->lock);
+ 	zone_seqlock_init(zone);
+ 	zone_pcp_init(zone);
+ }
+ 
+ /*
+  * Set up the zone data structures
+  * - init pgdat internals
+  * - init all zones belonging to this node
+  *
+  * NOTE: this function is only called during memory hotplug
+  */
+ #ifdef CONFIG_MEMORY_HOTPLUG
+ void __ref free_area_init_core_hotplug(int nid)
+ {
+ 	enum zone_type z;
+ 	pg_data_t *pgdat = NODE_DATA(nid);
+ 
+ 	pgdat_init_internals(pgdat);
+ 	for (z = 0; z < MAX_NR_ZONES; z++)
+ 		zone_init_internals(&pgdat->node_zones[z], z, nid, 0);
+ }
+ #endif
+ 
+ /*
+  * Set up the zone data structures:
+  *   - mark all pages reserved
+  *   - mark all memory queues empty
+  *   - clear the memory bitmaps
+  *
+  * NOTE: pgdat should get zeroed by caller.
+  * NOTE: this function is only called during early init.
+  */
+ static void __init free_area_init_core(struct pglist_data *pgdat)
+ {
+ 	enum zone_type j;
+ 	int nid = pgdat->node_id;
+ 
+ 	pgdat_init_internals(pgdat);
  	pgdat->per_cpu_nodestats = &boot_nodestats;
  
  	for (j = 0; j < MAX_NR_ZONES; j++) {
@@@ -6533,8 -6394,24 +6623,29 @@@ static void __ref alloc_node_mem_map(st
  static void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }
  #endif /* CONFIG_FLAT_NODE_MEM_MAP */
  
++<<<<<<< HEAD
 +void __paginginit free_area_init_node(int nid, unsigned long *zones_size,
 +		unsigned long node_start_pfn, unsigned long *zholes_size)
++=======
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static inline void pgdat_set_deferred_range(pg_data_t *pgdat)
+ {
+ 	/*
+ 	 * We start only with one section of pages, more pages are added as
+ 	 * needed until the rest of deferred pages are initialized.
+ 	 */
+ 	pgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,
+ 						pgdat->node_spanned_pages);
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ }
+ #else
+ static inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}
+ #endif
+ 
+ void __init free_area_init_node(int nid, unsigned long *zones_size,
+ 				   unsigned long node_start_pfn,
+ 				   unsigned long *zholes_size)
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
  {
  	pg_data_t *pgdat = NODE_DATA(nid);
  	unsigned long start_pfn = 0;
@@@ -6605,42 -6446,31 +6716,46 @@@ static u64 __init init_unavailable_rang
   * initialized by going through __init_single_page(). But, there are some
   * struct pages which are reserved in memblock allocator and their fields
   * may be accessed (for example page_to_pfn() on some configuration accesses
 - * flags). We must explicitly zero those struct pages.
 + * flags). We must explicitly initialize those struct pages.
 + *
 + * This function also addresses a similar issue where struct pages are left
 + * uninitialized because the physical address range is not covered by
 + * memblock.memory or memblock.reserved. That could happen when memblock
 + * layout is manually configured via memmap=, or when the highest physical
 + * address (max_pfn) does not end on a section boundary.
   */
++<<<<<<< HEAD
 +static void __init init_unavailable_mem(void)
++=======
+ void __init zero_resv_unavail(void)
++>>>>>>> 03e85f9d5f1f (mm/page_alloc: Introduce free_area_init_core_hotplug)
  {
  	phys_addr_t start, end;
 -	unsigned long pfn;
  	u64 i, pgcnt;
 +	phys_addr_t next = 0;
  
  	/*
 -	 * Loop through ranges that are reserved, but do not have reported
 -	 * physical memory backing.
 +	 * Loop through unavailable ranges not covered by memblock.memory.
  	 */
  	pgcnt = 0;
 -	for_each_resv_unavail_range(i, &start, &end) {
 -		for (pfn = PFN_DOWN(start); pfn < PFN_UP(end); pfn++) {
 -			if (!pfn_valid(ALIGN_DOWN(pfn, pageblock_nr_pages))) {
 -				pfn = ALIGN_DOWN(pfn, pageblock_nr_pages)
 -					+ pageblock_nr_pages - 1;
 -				continue;
 -			}
 -			mm_zero_struct_page(pfn_to_page(pfn));
 -			pgcnt++;
 -		}
 +	for_each_mem_range(i, &memblock.memory, NULL,
 +			NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end, NULL) {
 +		if (next < start)
 +			pgcnt += init_unavailable_range(PFN_DOWN(next),
 +							PFN_UP(start));
 +		next = end;
  	}
  
 +	/*
 +	 * Early sections always have a fully populated memmap for the whole
 +	 * section - see pfn_valid(). If the last section has holes at the
 +	 * end and that section is marked "online", the memmap will be
 +	 * considered initialized. Make sure that memmap has a well defined
 +	 * state.
 +	 */
 +	pgcnt += init_unavailable_range(PFN_DOWN(next),
 +					round_up(max_pfn, PAGES_PER_SECTION));
 +
  	/*
  	 * Struct pages that do not have backing memory. This could be because
  	 * firmware is using some of this memory, or for some other reasons.
* Unmerged path include/linux/memory_hotplug.h
diff --git a/include/linux/mm.h b/include/linux/mm.h
index cb077fce907f..6d521af1d566 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1971,7 +1971,7 @@ static inline spinlock_t *pud_lock(struct mm_struct *mm, pud_t *pud)
 
 extern void __init pagecache_init(void);
 extern void free_area_init(unsigned long * zones_size);
-extern void free_area_init_node(int nid, unsigned long * zones_size,
+extern void __init free_area_init_node(int nid, unsigned long * zones_size,
 		unsigned long zone_start_pfn, unsigned long *zholes_size);
 extern void free_initmem(void);
 
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index b425e9969aa2..3d87ef2a4b85 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -868,8 +868,6 @@ static void reset_node_present_pages(pg_data_t *pgdat)
 static pg_data_t __ref *hotadd_new_pgdat(int nid, u64 start)
 {
 	struct pglist_data *pgdat;
-	unsigned long zones_size[MAX_NR_ZONES] = {0};
-	unsigned long zholes_size[MAX_NR_ZONES] = {0};
 	unsigned long start_pfn = PFN_DOWN(start);
 
 	pgdat = NODE_DATA(nid);
@@ -892,8 +890,11 @@ static pg_data_t __ref *hotadd_new_pgdat(int nid, u64 start)
 
 	/* we can use NODE_DATA(nid) from here */
 
+	pgdat->node_id = nid;
+	pgdat->node_start_pfn = start_pfn;
+
 	/* init node's zones as empty zones, we don't have any present pages.*/
-	free_area_init_node(nid, zones_size, start_pfn, zholes_size);
+	free_area_init_core_hotplug(nid);
 	pgdat->per_cpu_nodestats = alloc_percpu(struct per_cpu_nodestat);
 
 	/*
@@ -902,19 +903,12 @@ static pg_data_t __ref *hotadd_new_pgdat(int nid, u64 start)
 	 */
 	build_all_zonelists(pgdat);
 
-	/*
-	 * zone->managed_pages is set to an approximate value in
-	 * free_area_init_core(), which will cause
-	 * /sys/device/system/node/nodeX/meminfo has wrong data.
-	 * So reset it to 0 before any memory is onlined.
-	 */
-	reset_node_managed_pages(pgdat);
-
 	/*
 	 * When memory is hot-added, all the memory is in offline state. So
 	 * clear all zones' present_pages because they will be updated in
 	 * online_pages() and offline_pages().
 	 */
+	reset_node_managed_pages(pgdat);
 	reset_node_present_pages(pgdat);
 
 	return pgdat;
* Unmerged path mm/page_alloc.c
