mm/swap: implement workingset detection for anonymous LRU

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit aae466b0052e1888edd1d7f473d4310d64936196
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/aae466b0.failed

This patch implements workingset detection for anonymous LRU.  All the
infrastructure is implemented by the previous patches so this patch just
activates the workingset detection by installing/retrieving the shadow
entry and adding refault calculation.

	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Minchan Kim <minchan@kernel.org>
Link: http://lkml.kernel.org/r/1595490560-15117-6-git-send-email-iamjoonsoo.kim@lge.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit aae466b0052e1888edd1d7f473d4310d64936196)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swap.h
#	mm/memory.c
#	mm/swap_state.c
#	mm/vmscan.c
diff --cc include/linux/swap.h
index 826d775d2a0d,661046994db4..000000000000
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@@ -411,9 -414,14 +411,17 @@@ extern struct address_space *swapper_sp
  extern unsigned long total_swapcache_pages(void);
  extern void show_swap_cache_info(void);
  extern int add_to_swap(struct page *page);
++<<<<<<< HEAD
 +extern int add_to_swap_cache(struct page *, swp_entry_t, gfp_t);
 +extern void __delete_from_swap_cache(struct page *, swp_entry_t entry);
++=======
+ extern void *get_shadow_from_swap_cache(swp_entry_t entry);
+ extern int add_to_swap_cache(struct page *page, swp_entry_t entry,
+ 			gfp_t gfp, void **shadowp);
+ extern void __delete_from_swap_cache(struct page *page,
+ 			swp_entry_t entry, void *shadow);
++>>>>>>> aae466b0052e (mm/swap: implement workingset detection for anonymous LRU)
  extern void delete_from_swap_cache(struct page *);
 -extern void clear_shadow_from_swap_cache(int type, unsigned long begin,
 -				unsigned long end);
  extern void free_page_and_swap_cache(struct page *);
  extern void free_pages_and_swap_cache(struct page **, int);
  extern struct page *lookup_swap_cache(swp_entry_t entry,
@@@ -566,8 -574,13 +574,13 @@@ static inline int add_to_swap(struct pa
  	return 0;
  }
  
+ static inline void *get_shadow_from_swap_cache(swp_entry_t entry)
+ {
+ 	return NULL;
+ }
+ 
  static inline int add_to_swap_cache(struct page *page, swp_entry_t entry,
 -					gfp_t gfp_mask, void **shadowp)
 +							gfp_t gfp_mask)
  {
  	return -1;
  }
diff --cc mm/memory.c
index 583eb7e0dd7f,de311fc7639e..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3045,7 -3137,24 +3046,26 @@@ vm_fault_t do_swap_page(struct vm_faul
  				__SetPageLocked(page);
  				__SetPageSwapBacked(page);
  				set_page_private(page, entry.val);
++<<<<<<< HEAD
 +				lru_cache_add_anon(page);
++=======
+ 
+ 				/* Tell memcg to use swap ownership records */
+ 				SetPageSwapCache(page);
+ 				err = mem_cgroup_charge(page, vma->vm_mm,
+ 							GFP_KERNEL);
+ 				ClearPageSwapCache(page);
+ 				if (err) {
+ 					ret = VM_FAULT_OOM;
+ 					goto out_page;
+ 				}
+ 
+ 				shadow = get_shadow_from_swap_cache(entry);
+ 				if (shadow)
+ 					workingset_refault(page, shadow);
+ 
+ 				lru_cache_add(page);
++>>>>>>> aae466b0052e (mm/swap: implement workingset detection for anonymous LRU)
  				swap_readpage(page, true);
  			}
  		} else {
diff --cc mm/swap_state.c
index e2aded84261e,b73aabdfd35a..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -106,15 -106,32 +106,29 @@@ void show_swap_cache_info(void
  	printk("Total swap = %lukB\n", total_swap_pages << (PAGE_SHIFT - 10));
  }
  
+ void *get_shadow_from_swap_cache(swp_entry_t entry)
+ {
+ 	struct address_space *address_space = swap_address_space(entry);
+ 	pgoff_t idx = swp_offset(entry);
+ 	struct page *page;
+ 
+ 	page = find_get_entry(address_space, idx);
+ 	if (xa_is_value(page))
+ 		return page;
+ 	if (page)
+ 		put_page(page);
+ 	return NULL;
+ }
+ 
  /*
 - * add_to_swap_cache resembles add_to_page_cache_locked on swapper_space,
 + * __add_to_swap_cache resembles add_to_page_cache_locked on swapper_space,
   * but sets SwapCache flag and private instead of mapping and index.
   */
 -int add_to_swap_cache(struct page *page, swp_entry_t entry,
 -			gfp_t gfp, void **shadowp)
 +int __add_to_swap_cache(struct page *page, swp_entry_t entry)
  {
 -	struct address_space *address_space = swap_address_space(entry);
 +	int error, i, nr = hpage_nr_pages(page);
 +	struct address_space *address_space;
  	pgoff_t idx = swp_offset(entry);
 -	XA_STATE_ORDER(xas, &address_space->i_pages, idx, compound_order(page));
 -	unsigned long i, nr = hpage_nr_pages(page);
 -	void *old;
  
  	VM_BUG_ON_PAGE(!PageLocked(page), page);
  	VM_BUG_ON_PAGE(PageSwapCache(page), page);
@@@ -379,12 -418,14 +393,18 @@@ struct page *__read_swap_cache_async(sw
  			struct vm_area_struct *vma, unsigned long addr,
  			bool *new_page_allocated)
  {
 +	struct page *found_page = NULL, *new_page = NULL;
  	struct swap_info_struct *si;
++<<<<<<< HEAD
 +	int err;
++=======
+ 	struct page *page;
+ 	void *shadow = NULL;
+ 
++>>>>>>> aae466b0052e (mm/swap: implement workingset detection for anonymous LRU)
  	*new_page_allocated = false;
  
 -	for (;;) {
 -		int err;
 +	do {
  		/*
  		 * First check the swap cache.  Since this is normally
  		 * called after lookup_swap_cache() failed, re-calling
@@@ -430,47 -464,54 +450,81 @@@
  		 * Swap entry may have been freed since our caller observed it.
  		 */
  		err = swapcache_prepare(entry);
 -		if (!err)
 +		if (err == -EEXIST) {
 +			radix_tree_preload_end();
 +			/*
 +			 * We might race against get_swap_page() and stumble
 +			 * across a SWAP_HAS_CACHE swap_map entry whose page
 +			 * has not been brought into the swapcache yet.
 +			 */
 +			cond_resched();
 +			continue;
 +		}
 +		if (err) {		/* swp entry is obsolete ? */
 +			radix_tree_preload_end();
  			break;
 +		}
  
 -		put_page(page);
 -		if (err != -EEXIST)
 -			return NULL;
 -
 +		/* May fail (-ENOMEM) if radix-tree node allocation failed. */
 +		__SetPageLocked(new_page);
 +		__SetPageSwapBacked(new_page);
 +		err = __add_to_swap_cache(new_page, entry);
 +		if (likely(!err)) {
 +			radix_tree_preload_end();
 +			/*
 +			 * Initiate read into locked page and return.
 +			 */
 +			SetPageWorkingset(new_page);
 +			lru_cache_add_anon(new_page);
 +			*new_page_allocated = true;
 +			return new_page;
 +		}
 +		radix_tree_preload_end();
 +		__ClearPageLocked(new_page);
  		/*
 -		 * We might race against __delete_from_swap_cache(), and
 -		 * stumble across a swap_map entry whose SWAP_HAS_CACHE
 -		 * has not yet been cleared.  Or race against another
 -		 * __read_swap_cache_async(), which has set SWAP_HAS_CACHE
 -		 * in swap_map, but not yet added its page to swap cache.
 +		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
 +		 * clear SWAP_HAS_CACHE flag.
  		 */
 -		cond_resched();
 -	}
 -
 +		put_swap_page(new_page, entry);
 +	} while (err != -ENOMEM);
 +
++<<<<<<< HEAD
 +	if (new_page)
 +		put_page(new_page);
 +	return found_page;
++=======
+ 	/*
+ 	 * The swap entry is ours to swap in. Prepare the new page.
+ 	 */
+ 
+ 	__SetPageLocked(page);
+ 	__SetPageSwapBacked(page);
+ 
+ 	/* May fail (-ENOMEM) if XArray node allocation failed. */
+ 	if (add_to_swap_cache(page, entry, gfp_mask & GFP_RECLAIM_MASK, &shadow)) {
+ 		put_swap_page(page, entry);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	if (mem_cgroup_charge(page, NULL, gfp_mask)) {
+ 		delete_from_swap_cache(page);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	if (shadow)
+ 		workingset_refault(page, shadow);
+ 
+ 	/* Caller will initiate read into locked page */
+ 	SetPageWorkingset(page);
+ 	lru_cache_add(page);
+ 	*new_page_allocated = true;
+ 	return page;
+ 
+ fail_unlock:
+ 	unlock_page(page);
+ 	put_page(page);
+ 	return NULL;
++>>>>>>> aae466b0052e (mm/swap: implement workingset detection for anonymous LRU)
  }
  
  /*
diff --cc mm/vmscan.c
index d33ed25f0511,66d73fea80e4..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -905,7 -897,9 +906,13 @@@ static int __remove_mapping(struct addr
  	if (PageSwapCache(page)) {
  		swp_entry_t swap = { .val = page_private(page) };
  		mem_cgroup_swapout(page, swap);
++<<<<<<< HEAD
 +		__delete_from_swap_cache(page, swap);
++=======
+ 		if (reclaimed && !mapping_exiting(mapping))
+ 			shadow = workingset_eviction(page, target_memcg);
+ 		__delete_from_swap_cache(page, swap, shadow);
++>>>>>>> aae466b0052e (mm/swap: implement workingset detection for anonymous LRU)
  		xa_unlock_irqrestore(&mapping->i_pages, flags);
  		put_swap_page(page, swap);
  	} else {
* Unmerged path include/linux/swap.h
* Unmerged path mm/memory.c
* Unmerged path mm/swap_state.c
* Unmerged path mm/vmscan.c
diff --git a/mm/workingset.c b/mm/workingset.c
index 44c5c225f293..85db8c0c1888 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -345,15 +345,22 @@ void workingset_refault(struct page *page, void *shadow)
 	/*
 	 * Compare the distance to the existing workingset size. We
 	 * don't activate pages that couldn't stay resident even if
-	 * all the memory was available to the page cache. Whether
-	 * cache can compete with anon or not depends on having swap.
+	 * all the memory was available to the workingset. Whether
+	 * workingset competition needs to consider anon or not depends
+	 * on having swap.
 	 */
 	workingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);
-	if (mem_cgroup_get_nr_swap_pages(memcg) > 0) {
+	if (!file) {
 		workingset_size += lruvec_page_state(eviction_lruvec,
-						     NR_INACTIVE_ANON);
+						     NR_INACTIVE_FILE);
+	}
+	if (mem_cgroup_get_nr_swap_pages(memcg) > 0) {
 		workingset_size += lruvec_page_state(eviction_lruvec,
 						     NR_ACTIVE_ANON);
+		if (file) {
+			workingset_size += lruvec_page_state(eviction_lruvec,
+						     NR_INACTIVE_ANON);
+		}
 	}
 	if (refault_distance > workingset_size)
 		goto out;
