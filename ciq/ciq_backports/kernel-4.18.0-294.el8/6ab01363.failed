mm: use zone and order instead of free area in free_list manipulators

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Alexander Duyck <alexander.h.duyck@linux.intel.com>
commit 6ab0136310961ebf4b5ecb565f0bf52c233dc093
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/6ab01363.failed

In order to enable the use of the zone from the list manipulator functions
I will need access to the zone pointer.  As it turns out most of the
accessors were always just being directly passed &zone->free_area[order]
anyway so it would make sense to just fold that into the function itself
and pass the zone and order as arguments instead of the free area.

In order to be able to reference the zone we need to move the declaration
of the functions down so that we have the zone defined before we define
the list manipulation functions.  Since the functions are only used in the
file mm/page_alloc.c we can just move them there to reduce noise in the
header.

	Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Reviewed-by: David Hildenbrand <david@redhat.com>
	Reviewed-by: Pankaj Gupta <pagupta@redhat.com>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Luiz Capitulino <lcapitulino@redhat.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Nitesh Narayan Lal <nitesh@redhat.com>
	Cc: Oscar Salvador <osalvador@suse.de>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Wei Wang <wei.w.wang@intel.com>
	Cc: Yang Zhang <yang.zhang.wz@gmail.com>
	Cc: wei qi <weiqi4@huawei.com>
Link: http://lkml.kernel.org/r/20200211224613.29318.43080.stgit@localhost.localdomain
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 6ab0136310961ebf4b5ecb565f0bf52c233dc093)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mmzone.h
#	mm/page_alloc.c
diff --cc include/linux/mmzone.h
index ce2990f782e1,42b77d3b68e8..000000000000
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@@ -102,41 -100,6 +102,44 @@@ struct free_area 
  	unsigned long		nr_free;
  };
  
++<<<<<<< HEAD
 +/* Used for pages not on another list */
 +static inline void add_to_free_area(struct page *page, struct free_area *area,
 +			     int migratetype)
 +{
 +	list_add(&page->lru, &area->free_list[migratetype]);
 +	area->nr_free++;
 +}
 +
 +/* Used for pages not on another list */
 +static inline void add_to_free_area_tail(struct page *page, struct free_area *area,
 +				  int migratetype)
 +{
 +	list_add_tail(&page->lru, &area->free_list[migratetype]);
 +	area->nr_free++;
 +}
 +
 +#ifdef CONFIG_SHUFFLE_PAGE_ALLOCATOR
 +/* Used to preserve page allocation order entropy */
 +void add_to_free_area_random(struct page *page, struct free_area *area,
 +		int migratetype);
 +#else
 +static inline void add_to_free_area_random(struct page *page,
 +		struct free_area *area, int migratetype)
 +{
 +	add_to_free_area(page, area, migratetype);
 +}
 +#endif
 +
 +/* Used for pages which are on another list */
 +static inline void move_to_free_area(struct page *page, struct free_area *area,
 +			     int migratetype)
 +{
 +	list_move(&page->lru, &area->free_list[migratetype]);
 +}
 +
++=======
++>>>>>>> 6ab013631096 (mm: use zone and order instead of free area in free_list manipulators)
  static inline struct page *get_page_from_free_area(struct free_area *area,
  					    int migratetype)
  {
diff --cc mm/page_alloc.c
index 24899db67281,14bdf3608a6b..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -743,32 -792,144 +743,152 @@@ static inline void set_page_order(struc
   *
   * For recording page's order, we use page_private(page).
   */
 -static inline bool page_is_buddy(struct page *page, struct page *buddy,
 +static inline int page_is_buddy(struct page *page, struct page *buddy,
  							unsigned int order)
  {
 -	if (!page_is_guard(buddy) && !PageBuddy(buddy))
 -		return false;
 +	if (page_is_guard(buddy) && page_order(buddy) == order) {
 +		if (page_zone_id(page) != page_zone_id(buddy))
 +			return 0;
  
 -	if (page_order(buddy) != order)
 -		return false;
 +		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
  
 -	/*
 -	 * zone check is done late to avoid uselessly calculating
 -	 * zone/node ids for pages that could never merge.
 -	 */
 -	if (page_zone_id(page) != page_zone_id(buddy))
 -		return false;
 +		return 1;
 +	}
  
 -	VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
 +	if (PageBuddy(buddy) && page_order(buddy) == order) {
 +		/*
 +		 * zone check is done late to avoid uselessly
 +		 * calculating zone/node ids for pages that could
 +		 * never merge.
 +		 */
 +		if (page_zone_id(page) != page_zone_id(buddy))
 +			return 0;
  
 -	return true;
 -}
 +		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
  
++<<<<<<< HEAD
 +		return 1;
 +	}
 +	return 0;
++=======
+ #ifdef CONFIG_COMPACTION
+ static inline struct capture_control *task_capc(struct zone *zone)
+ {
+ 	struct capture_control *capc = current->capture_control;
+ 
+ 	return capc &&
+ 		!(current->flags & PF_KTHREAD) &&
+ 		!capc->page &&
+ 		capc->cc->zone == zone &&
+ 		capc->cc->direct_compaction ? capc : NULL;
+ }
+ 
+ static inline bool
+ compaction_capture(struct capture_control *capc, struct page *page,
+ 		   int order, int migratetype)
+ {
+ 	if (!capc || order != capc->cc->order)
+ 		return false;
+ 
+ 	/* Do not accidentally pollute CMA or isolated regions*/
+ 	if (is_migrate_cma(migratetype) ||
+ 	    is_migrate_isolate(migratetype))
+ 		return false;
+ 
+ 	/*
+ 	 * Do not let lower order allocations polluate a movable pageblock.
+ 	 * This might let an unmovable request use a reclaimable pageblock
+ 	 * and vice-versa but no more than normal fallback logic which can
+ 	 * have trouble finding a high-order free page.
+ 	 */
+ 	if (order < pageblock_order && migratetype == MIGRATE_MOVABLE)
+ 		return false;
+ 
+ 	capc->page = page;
+ 	return true;
+ }
+ 
+ #else
+ static inline struct capture_control *task_capc(struct zone *zone)
+ {
+ 	return NULL;
+ }
+ 
+ static inline bool
+ compaction_capture(struct capture_control *capc, struct page *page,
+ 		   int order, int migratetype)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_COMPACTION */
+ 
+ /* Used for pages not on another list */
+ static inline void add_to_free_list(struct page *page, struct zone *zone,
+ 				    unsigned int order, int migratetype)
+ {
+ 	struct free_area *area = &zone->free_area[order];
+ 
+ 	list_add(&page->lru, &area->free_list[migratetype]);
+ 	area->nr_free++;
+ }
+ 
+ /* Used for pages not on another list */
+ static inline void add_to_free_list_tail(struct page *page, struct zone *zone,
+ 					 unsigned int order, int migratetype)
+ {
+ 	struct free_area *area = &zone->free_area[order];
+ 
+ 	list_add_tail(&page->lru, &area->free_list[migratetype]);
+ 	area->nr_free++;
+ }
+ 
+ /* Used for pages which are on another list */
+ static inline void move_to_free_list(struct page *page, struct zone *zone,
+ 				     unsigned int order, int migratetype)
+ {
+ 	struct free_area *area = &zone->free_area[order];
+ 
+ 	list_move(&page->lru, &area->free_list[migratetype]);
+ }
+ 
+ static inline void del_page_from_free_list(struct page *page, struct zone *zone,
+ 					   unsigned int order)
+ {
+ 	list_del(&page->lru);
+ 	__ClearPageBuddy(page);
+ 	set_page_private(page, 0);
+ 	zone->free_area[order].nr_free--;
+ }
+ 
+ /*
+  * If this is not the largest possible page, check if the buddy
+  * of the next-highest order is free. If it is, it's possible
+  * that pages are being freed that will coalesce soon. In case,
+  * that is happening, add the free page to the tail of the list
+  * so it's less likely to be used soon and more likely to be merged
+  * as a higher order page
+  */
+ static inline bool
+ buddy_merge_likely(unsigned long pfn, unsigned long buddy_pfn,
+ 		   struct page *page, unsigned int order)
+ {
+ 	struct page *higher_page, *higher_buddy;
+ 	unsigned long combined_pfn;
+ 
+ 	if (order >= MAX_ORDER - 2)
+ 		return false;
+ 
+ 	if (!pfn_valid_within(buddy_pfn))
+ 		return false;
+ 
+ 	combined_pfn = buddy_pfn & pfn;
+ 	higher_page = page + (combined_pfn - pfn);
+ 	buddy_pfn = __find_buddy_pfn(combined_pfn, order + 1);
+ 	higher_buddy = higher_page + (buddy_pfn - combined_pfn);
+ 
+ 	return pfn_valid_within(buddy_pfn) &&
+ 	       page_is_buddy(higher_page, higher_buddy, order + 1);
++>>>>>>> 6ab013631096 (mm: use zone and order instead of free area in free_list manipulators)
  }
  
  /*
@@@ -800,10 -961,12 +920,14 @@@ static inline void __free_one_page(stru
  		struct zone *zone, unsigned int order,
  		int migratetype)
  {
 -	struct capture_control *capc = task_capc(zone);
 -	unsigned long uninitialized_var(buddy_pfn);
  	unsigned long combined_pfn;
++<<<<<<< HEAD
 +	unsigned long uninitialized_var(buddy_pfn);
++=======
+ 	unsigned int max_order;
++>>>>>>> 6ab013631096 (mm: use zone and order instead of free area in free_list manipulators)
  	struct page *buddy;
 -	bool to_tail;
 +	unsigned int max_order;
  
  	max_order = min_t(unsigned int, MAX_ORDER, pageblock_order + 1);
  
@@@ -867,35 -1035,15 +991,47 @@@ continue_merging
  done_merging:
  	set_page_order(page, order);
  
++<<<<<<< HEAD
 +	/*
 +	 * If this is not the largest possible page, check if the buddy
 +	 * of the next-highest order is free. If it is, it's possible
 +	 * that pages are being freed that will coalesce soon. In case,
 +	 * that is happening, add the free page to the tail of the list
 +	 * so it's less likely to be used soon and more likely to be merged
 +	 * as a higher order page
 +	 */
 +	if ((order < MAX_ORDER-2) && pfn_valid_within(buddy_pfn)
 +			&& !is_shuffle_order(order)) {
 +		struct page *higher_page, *higher_buddy;
 +		combined_pfn = buddy_pfn & pfn;
 +		higher_page = page + (combined_pfn - pfn);
 +		buddy_pfn = __find_buddy_pfn(combined_pfn, order + 1);
 +		higher_buddy = higher_page + (buddy_pfn - combined_pfn);
 +		if (pfn_valid_within(buddy_pfn) &&
 +		    page_is_buddy(higher_page, higher_buddy, order + 1)) {
 +			add_to_free_area_tail(page, &zone->free_area[order],
 +					      migratetype);
 +			return;
 +		}
 +	}
 +
 +	if (is_shuffle_order(order))
 +		add_to_free_area_random(page, &zone->free_area[order],
 +				migratetype);
 +	else
 +		add_to_free_area(page, &zone->free_area[order], migratetype);
 +
++=======
+ 	if (is_shuffle_order(order))
+ 		to_tail = shuffle_pick_tail();
+ 	else
+ 		to_tail = buddy_merge_likely(pfn, buddy_pfn, page, order);
+ 
+ 	if (to_tail)
+ 		add_to_free_list_tail(page, zone, order, migratetype);
+ 	else
+ 		add_to_free_list(page, zone, order, migratetype);
++>>>>>>> 6ab013631096 (mm: use zone and order instead of free area in free_list manipulators)
  }
  
  /*
@@@ -8426,11 -8757,7 +8557,15 @@@ __offline_isolated_pages(unsigned long 
  		BUG_ON(!PageBuddy(page));
  		order = page_order(page);
  		offlined_pages += 1 << order;
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_VM
 +		pr_info("remove from free list %lx %d %lx\n",
 +			pfn, 1 << order, end_pfn);
 +#endif
 +		del_page_from_free_area(page, &zone->free_area[order]);
++=======
+ 		del_page_from_free_list(page, zone, order);
++>>>>>>> 6ab013631096 (mm: use zone and order instead of free area in free_list manipulators)
  		pfn += (1 << order);
  	}
  	spin_unlock_irqrestore(&zone->lock, flags);
* Unmerged path include/linux/mmzone.h
* Unmerged path mm/page_alloc.c
