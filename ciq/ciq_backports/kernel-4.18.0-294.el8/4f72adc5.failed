x86/vdso: Simplify the invalid vclock case

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 4f72adc5068294268387a81a6bf91d9bb07ecc5c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/4f72adc5.failed

The code flow for the vclocks is convoluted as it requires the vclocks
which can be invalidated separately from the vsyscall_gtod_data sequence to
store the fact in a separate variable. That's inefficient.

Restructure the code so the vclock readout returns cycles and the
conversion to nanoseconds is handled at the call site.

If the clock gets invalidated or vclock is already VCLOCK_NONE, return
U64_MAX as the cycle value, which is invalid for all clocks and leave the
sequence loop immediately in that case by calling the fallback function
directly.

This allows to remove the gettimeofday fallback as it now uses the
clock_gettime() fallback and does the nanoseconds to microseconds
conversion in the same way as it does when the vclock is functional. It
does not make a difference whether the division by 1000 happens in the
kernel fallback or in userspace.

Generates way better code and gains a few cycles back.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Andy Lutomirski <luto@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Matt Rickard <matt@softrans.com.au>
	Cc: Stephen Boyd <sboyd@kernel.org>
	Cc: John Stultz <john.stultz@linaro.org>
	Cc: Florian Weimer <fweimer@redhat.com>
	Cc: "K. Y. Srinivasan" <kys@microsoft.com>
	Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
	Cc: devel@linuxdriverproject.org
	Cc: virtualization@lists.linux-foundation.org
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Juergen Gross <jgross@suse.com>
Link: https://lkml.kernel.org/r/20180917130707.657928937@linutronix.de

(cherry picked from commit 4f72adc5068294268387a81a6bf91d9bb07ecc5c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/vdso/vclock_gettime.c
diff --cc arch/x86/entry/vdso/vclock_gettime.c
index f94186cf5962,40105024a210..000000000000
--- a/arch/x86/entry/vdso/vclock_gettime.c
+++ b/arch/x86/entry/vdso/vclock_gettime.c
@@@ -149,8 -121,8 +121,13 @@@ static notrace u64 vread_pvclock(void
  	return last;
  }
  #endif
++<<<<<<< HEAD
 +#ifdef CONFIG_HYPERV_TIMER
 +static notrace u64 vread_hvclock(int *mode)
++=======
+ #ifdef CONFIG_HYPERV_TSCPAGE
+ static notrace u64 vread_hvclock(void)
++>>>>>>> 4f72adc50682 (x86/vdso: Simplify the invalid vclock case)
  {
  	const struct ms_hyperv_tsc_page *tsc_pg =
  		(const struct ms_hyperv_tsc_page *)&hvclock_page;
@@@ -184,25 -151,19 +156,25 @@@ notrace static u64 vread_tsc(void
  	return last;
  }
  
- notrace static inline u64 vgetsns(int *mode)
+ notrace static inline u64 vgetcyc(int mode)
  {
- 	u64 v;
- 	cycles_t cycles;
- 
- 	if (gtod->vclock_mode == VCLOCK_TSC)
- 		cycles = vread_tsc();
+ 	if (mode == VCLOCK_TSC)
+ 		return vread_tsc();
  #ifdef CONFIG_PARAVIRT_CLOCK
- 	else if (gtod->vclock_mode == VCLOCK_PVCLOCK)
- 		cycles = vread_pvclock(mode);
+ 	else if (mode == VCLOCK_PVCLOCK)
+ 		return vread_pvclock();
  #endif
++<<<<<<< HEAD
 +#ifdef CONFIG_HYPERV_TIMER
 +	else if (gtod->vclock_mode == VCLOCK_HVCLOCK)
 +		cycles = vread_hvclock(mode);
++=======
+ #ifdef CONFIG_HYPERV_TSCPAGE
+ 	else if (mode == VCLOCK_HVCLOCK)
+ 		return vread_hvclock();
++>>>>>>> 4f72adc50682 (x86/vdso: Simplify the invalid vclock case)
  #endif
- 	else
- 		return 0;
- 	v = cycles - gtod->cycle_last;
- 	return v * gtod->mult;
+ 	return U64_MAX;
  }
  
  notrace static int do_hres(clockid_t clk, struct timespec *ts)
* Unmerged path arch/x86/entry/vdso/vclock_gettime.c
