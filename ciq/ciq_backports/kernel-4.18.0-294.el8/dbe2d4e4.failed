mm, compaction: round-robin the order while searching the free lists for a target

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Mel Gorman <mgorman@techsingularity.net>
commit dbe2d4e4f12e07c6a2215e3603a5f77056323081
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/dbe2d4e4.failed

As compaction proceeds and creates high-order blocks, the free list
search gets less efficient as the larger blocks are used as compaction
targets.  Eventually, the larger blocks will be behind the migration
scanner for partially migrated pageblocks and the search fails.  This
patch round-robins what orders are searched so that larger blocks can be
ignored and find smaller blocks that can be used as migration targets.

The overall impact was small on 1-socket but it avoids corner cases
where the migration/free scanners meet prematurely or situations where
many of the pageblocks encountered by the free scanner are almost full
instead of being properly packed.  Previous testing had indicated that
without this patch there were occasional large spikes in the free
scanner without this patch.

[dan.carpenter@oracle.com: fix static checker warning]
Link: http://lkml.kernel.org/r/20190118175136.31341-20-mgorman@techsingularity.net
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: YueHaibing <yuehaibing@huawei.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit dbe2d4e4f12e07c6a2215e3603a5f77056323081)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/compaction.c
#	mm/internal.h
diff --cc mm/compaction.c
index 79db11f23bf2,b3055983a80f..000000000000
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@@ -1054,6 -1078,248 +1054,251 @@@ static inline bool compact_scanners_met
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Used when scanning for a suitable migration target which scans freelists
+  * in reverse. Reorders the list such as the unscanned pages are scanned
+  * first on the next iteration of the free scanner
+  */
+ static void
+ move_freelist_head(struct list_head *freelist, struct page *freepage)
+ {
+ 	LIST_HEAD(sublist);
+ 
+ 	if (!list_is_last(freelist, &freepage->lru)) {
+ 		list_cut_before(&sublist, freelist, &freepage->lru);
+ 		if (!list_empty(&sublist))
+ 			list_splice_tail(&sublist, freelist);
+ 	}
+ }
+ 
+ /*
+  * Similar to move_freelist_head except used by the migration scanner
+  * when scanning forward. It's possible for these list operations to
+  * move against each other if they search the free list exactly in
+  * lockstep.
+  */
+ static void
+ move_freelist_tail(struct list_head *freelist, struct page *freepage)
+ {
+ 	LIST_HEAD(sublist);
+ 
+ 	if (!list_is_first(freelist, &freepage->lru)) {
+ 		list_cut_position(&sublist, freelist, &freepage->lru);
+ 		if (!list_empty(&sublist))
+ 			list_splice_tail(&sublist, freelist);
+ 	}
+ }
+ 
+ static void
+ fast_isolate_around(struct compact_control *cc, unsigned long pfn, unsigned long nr_isolated)
+ {
+ 	unsigned long start_pfn, end_pfn;
+ 	struct page *page = pfn_to_page(pfn);
+ 
+ 	/* Do not search around if there are enough pages already */
+ 	if (cc->nr_freepages >= cc->nr_migratepages)
+ 		return;
+ 
+ 	/* Minimise scanning during async compaction */
+ 	if (cc->direct_compaction && cc->mode == MIGRATE_ASYNC)
+ 		return;
+ 
+ 	/* Pageblock boundaries */
+ 	start_pfn = pageblock_start_pfn(pfn);
+ 	end_pfn = min(start_pfn + pageblock_nr_pages, zone_end_pfn(cc->zone));
+ 
+ 	/* Scan before */
+ 	if (start_pfn != pfn) {
+ 		isolate_freepages_block(cc, &start_pfn, pfn, &cc->freepages, false);
+ 		if (cc->nr_freepages >= cc->nr_migratepages)
+ 			return;
+ 	}
+ 
+ 	/* Scan after */
+ 	start_pfn = pfn + nr_isolated;
+ 	if (start_pfn != end_pfn)
+ 		isolate_freepages_block(cc, &start_pfn, end_pfn, &cc->freepages, false);
+ 
+ 	/* Skip this pageblock in the future as it's full or nearly full */
+ 	if (cc->nr_freepages < cc->nr_migratepages)
+ 		set_pageblock_skip(page);
+ }
+ 
+ /* Search orders in round-robin fashion */
+ static int next_search_order(struct compact_control *cc, int order)
+ {
+ 	order--;
+ 	if (order < 0)
+ 		order = cc->order - 1;
+ 
+ 	/* Search wrapped around? */
+ 	if (order == cc->search_order) {
+ 		cc->search_order--;
+ 		if (cc->search_order < 0)
+ 			cc->search_order = cc->order - 1;
+ 		return -1;
+ 	}
+ 
+ 	return order;
+ }
+ 
+ static unsigned long
+ fast_isolate_freepages(struct compact_control *cc)
+ {
+ 	unsigned int limit = min(1U, freelist_scan_limit(cc) >> 1);
+ 	unsigned int nr_scanned = 0;
+ 	unsigned long low_pfn, min_pfn, high_pfn = 0, highest = 0;
+ 	unsigned long nr_isolated = 0;
+ 	unsigned long distance;
+ 	struct page *page = NULL;
+ 	bool scan_start = false;
+ 	int order;
+ 
+ 	/* Full compaction passes in a negative order */
+ 	if (cc->order <= 0)
+ 		return cc->free_pfn;
+ 
+ 	/*
+ 	 * If starting the scan, use a deeper search and use the highest
+ 	 * PFN found if a suitable one is not found.
+ 	 */
+ 	if (cc->free_pfn == pageblock_start_pfn(zone_end_pfn(cc->zone) - 1)) {
+ 		limit = pageblock_nr_pages >> 1;
+ 		scan_start = true;
+ 	}
+ 
+ 	/*
+ 	 * Preferred point is in the top quarter of the scan space but take
+ 	 * a pfn from the top half if the search is problematic.
+ 	 */
+ 	distance = (cc->free_pfn - cc->migrate_pfn);
+ 	low_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 2));
+ 	min_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 1));
+ 
+ 	if (WARN_ON_ONCE(min_pfn > low_pfn))
+ 		low_pfn = min_pfn;
+ 
+ 	/*
+ 	 * Search starts from the last successful isolation order or the next
+ 	 * order to search after a previous failure
+ 	 */
+ 	cc->search_order = min_t(unsigned int, cc->order - 1, cc->search_order);
+ 
+ 	for (order = cc->search_order;
+ 	     !page && order >= 0;
+ 	     order = next_search_order(cc, order)) {
+ 		struct free_area *area = &cc->zone->free_area[order];
+ 		struct list_head *freelist;
+ 		struct page *freepage;
+ 		unsigned long flags;
+ 		unsigned int order_scanned = 0;
+ 
+ 		if (!area->nr_free)
+ 			continue;
+ 
+ 		spin_lock_irqsave(&cc->zone->lock, flags);
+ 		freelist = &area->free_list[MIGRATE_MOVABLE];
+ 		list_for_each_entry_reverse(freepage, freelist, lru) {
+ 			unsigned long pfn;
+ 
+ 			order_scanned++;
+ 			nr_scanned++;
+ 			pfn = page_to_pfn(freepage);
+ 
+ 			if (pfn >= highest)
+ 				highest = pageblock_start_pfn(pfn);
+ 
+ 			if (pfn >= low_pfn) {
+ 				cc->fast_search_fail = 0;
+ 				cc->search_order = order;
+ 				page = freepage;
+ 				break;
+ 			}
+ 
+ 			if (pfn >= min_pfn && pfn > high_pfn) {
+ 				high_pfn = pfn;
+ 
+ 				/* Shorten the scan if a candidate is found */
+ 				limit >>= 1;
+ 			}
+ 
+ 			if (order_scanned >= limit)
+ 				break;
+ 		}
+ 
+ 		/* Use a minimum pfn if a preferred one was not found */
+ 		if (!page && high_pfn) {
+ 			page = pfn_to_page(high_pfn);
+ 
+ 			/* Update freepage for the list reorder below */
+ 			freepage = page;
+ 		}
+ 
+ 		/* Reorder to so a future search skips recent pages */
+ 		move_freelist_head(freelist, freepage);
+ 
+ 		/* Isolate the page if available */
+ 		if (page) {
+ 			if (__isolate_free_page(page, order)) {
+ 				set_page_private(page, order);
+ 				nr_isolated = 1 << order;
+ 				cc->nr_freepages += nr_isolated;
+ 				list_add_tail(&page->lru, &cc->freepages);
+ 				count_compact_events(COMPACTISOLATED, nr_isolated);
+ 			} else {
+ 				/* If isolation fails, abort the search */
+ 				order = -1;
+ 				page = NULL;
+ 			}
+ 		}
+ 
+ 		spin_unlock_irqrestore(&cc->zone->lock, flags);
+ 
+ 		/*
+ 		 * Smaller scan on next order so the total scan ig related
+ 		 * to freelist_scan_limit.
+ 		 */
+ 		if (order_scanned >= limit)
+ 			limit = min(1U, limit >> 1);
+ 	}
+ 
+ 	if (!page) {
+ 		cc->fast_search_fail++;
+ 		if (scan_start) {
+ 			/*
+ 			 * Use the highest PFN found above min. If one was
+ 			 * not found, be pessemistic for direct compaction
+ 			 * and use the min mark.
+ 			 */
+ 			if (highest) {
+ 				page = pfn_to_page(highest);
+ 				cc->free_pfn = highest;
+ 			} else {
+ 				if (cc->direct_compaction) {
+ 					page = pfn_to_page(min_pfn);
+ 					cc->free_pfn = min_pfn;
+ 				}
+ 			}
+ 		}
+ 	}
+ 
+ 	if (highest && highest >= cc->zone->compact_cached_free_pfn) {
+ 		highest -= pageblock_nr_pages;
+ 		cc->zone->compact_cached_free_pfn = highest;
+ 	}
+ 
+ 	cc->total_free_scanned += nr_scanned;
+ 	if (!page)
+ 		return cc->free_pfn;
+ 
+ 	low_pfn = page_to_pfn(page);
+ 	fast_isolate_around(cc, low_pfn, nr_isolated);
+ 	return low_pfn;
+ }
+ 
+ /*
++>>>>>>> dbe2d4e4f12e (mm, compaction: round-robin the order while searching the free lists for a target)
   * Based on information in the current compact_control, find blocks
   * suitable for isolating free pages from and then isolate them.
   */
@@@ -1731,7 -2158,12 +1976,8 @@@ static enum compact_result compact_zone
  {
  	enum compact_result ret;
  	struct compact_control cc = {
 -		.nr_freepages = 0,
 -		.nr_migratepages = 0,
 -		.total_migrate_scanned = 0,
 -		.total_free_scanned = 0,
  		.order = order,
+ 		.search_order = order,
  		.gfp_mask = gfp_mask,
  		.zone = zone,
  		.mode = (prio == COMPACT_PRIO_ASYNC) ?
@@@ -1955,6 -2395,9 +2201,12 @@@ static void kcompactd_do_work(pg_data_
  	struct zone *zone;
  	struct compact_control cc = {
  		.order = pgdat->kcompactd_max_order,
++<<<<<<< HEAD
++=======
+ 		.search_order = pgdat->kcompactd_max_order,
+ 		.total_migrate_scanned = 0,
+ 		.total_free_scanned = 0,
++>>>>>>> dbe2d4e4f12e (mm, compaction: round-robin the order while searching the free lists for a target)
  		.classzone_idx = pgdat->kcompactd_classzone_idx,
  		.mode = MIGRATE_SYNC_LIGHT,
  		.ignore_skip_hint = false,
diff --cc mm/internal.h
index 9193c2ad54f9,31bb0be6fd52..000000000000
--- a/mm/internal.h
+++ b/mm/internal.h
@@@ -191,6 -191,9 +191,11 @@@ struct compact_control 
  	struct zone *zone;
  	unsigned long total_migrate_scanned;
  	unsigned long total_free_scanned;
++<<<<<<< HEAD
++=======
+ 	unsigned short fast_search_fail;/* failures to use free list searches */
+ 	short search_order;		/* order to start a fast search at */
++>>>>>>> dbe2d4e4f12e (mm, compaction: round-robin the order while searching the free lists for a target)
  	const gfp_t gfp_mask;		/* gfp mask of a direct compactor */
  	int order;			/* order a direct compactor needs */
  	int migratetype;		/* migratetype of direct compactor */
* Unmerged path mm/compaction.c
* Unmerged path mm/internal.h
