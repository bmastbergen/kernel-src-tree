powerpc/64s: Add cp_abort after tlbiel to invalidate copy-buffer address

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Nicholas Piggin <npiggin@gmail.com>
commit 05504b42562066ae27ce3e7dcec37f81dea476cb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/05504b42.failed

The copy buffer is implemented as a real address in the nest which is
translated from EA by copy, and used for memory access by paste. This
requires that it be invalidated by TLB invalidation.

TLBIE does invalidate the copy buffer, but TLBIEL does not. Add
cp_abort to the tlbiel sequence.

	Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
[mpe: Fixup whitespace and comment formatting]
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20200916030234.4110379-2-npiggin@gmail.com
(cherry picked from commit 05504b42562066ae27ce3e7dcec37f81dea476cb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/synch.h
#	arch/powerpc/mm/book3s64/hash_native.c
#	arch/powerpc/mm/book3s64/radix_tlb.c
diff --cc arch/powerpc/include/asm/synch.h
index 6ec546090ba1,1d67bc8d7bc6..000000000000
--- a/arch/powerpc/include/asm/synch.h
+++ b/arch/powerpc/include/asm/synch.h
@@@ -3,8 -3,9 +3,14 @@@
  #define _ASM_POWERPC_SYNCH_H 
  #ifdef __KERNEL__
  
++<<<<<<< HEAD
 +#include <linux/stringify.h>
 +#include <asm/feature-fixups.h>
++=======
+ #include <asm/cputable.h>
+ #include <asm/feature-fixups.h>
+ #include <asm/ppc-opcode.h>
++>>>>>>> 05504b425620 (powerpc/64s: Add cp_abort after tlbiel to invalidate copy-buffer address)
  
  #ifndef __ASSEMBLY__
  extern unsigned int __start___lwsync_fixup, __stop___lwsync_fixup;
diff --cc arch/powerpc/mm/book3s64/hash_native.c
index 49c02b696af4,0203cdf48c54..000000000000
--- a/arch/powerpc/mm/book3s64/hash_native.c
+++ b/arch/powerpc/mm/book3s64/hash_native.c
@@@ -114,7 -110,9 +114,13 @@@ static void tlbiel_all_isa300(unsigned 
  	 */
  	tlbiel_hash_set_isa300(0, is, 0, 2, 1);
  
++<<<<<<< HEAD
 +	asm volatile("ptesync": : :"memory");
++=======
+ 	ppc_after_tlbiel_barrier();
+ 
+ 	asm volatile(PPC_ISA_3_0_INVALIDATE_ERAT "; isync" : : :"memory");
++>>>>>>> 05504b425620 (powerpc/64s: Add cp_abort after tlbiel to invalidate copy-buffer address)
  }
  
  void hash__tlbiel_all(unsigned int action)
diff --cc arch/powerpc/mm/book3s64/radix_tlb.c
index 35a168bbe137,b487b489d4b6..000000000000
--- a/arch/powerpc/mm/book3s64/radix_tlb.c
+++ b/arch/powerpc/mm/book3s64/radix_tlb.c
@@@ -326,8 -304,8 +326,13 @@@ static inline void _tlbiel_pid(unsigne
  	for (set = 1; set < POWER9_TLB_SETS_RADIX ; set++)
  		__tlbiel_pid(pid, set, RIC_FLUSH_TLB);
  
++<<<<<<< HEAD
 +	asm volatile("ptesync": : :"memory");
 +	asm volatile(PPC_INVALIDATE_ERAT "; isync" : : :"memory");
++=======
+ 	ppc_after_tlbiel_barrier();
+ 	asm volatile(PPC_RADIX_INVALIDATE_ERAT_USER "; isync" : : :"memory");
++>>>>>>> 05504b425620 (powerpc/64s: Add cp_abort after tlbiel to invalidate copy-buffer address)
  }
  
  static inline void _tlbie_pid(unsigned long pid, unsigned long ric)
@@@ -829,11 -949,9 +834,17 @@@ is_local
  			if (hflush)
  				__tlbiel_va_range(hstart, hend, pid,
  						PMD_SIZE, MMU_PAGE_2M);
++<<<<<<< HEAD
 +			if (gflush)
 +				__tlbiel_va_range(gstart, gend, pid,
 +						PUD_SIZE, MMU_PAGE_1G);
 +			asm volatile("ptesync": : :"memory");
 +		} else {
++=======
+ 			ppc_after_tlbiel_barrier();
+ 		} else if (cputlb_use_tlbie()) {
+ 			asm volatile("ptesync": : :"memory");
++>>>>>>> 05504b425620 (powerpc/64s: Add cp_abort after tlbiel to invalidate copy-buffer address)
  			__tlbie_va_range(start, end, pid, page_size, mmu_virtual_psize);
  			if (hflush)
  				__tlbie_va_range(hstart, hend, pid,
* Unmerged path arch/powerpc/include/asm/synch.h
* Unmerged path arch/powerpc/mm/book3s64/hash_native.c
* Unmerged path arch/powerpc/mm/book3s64/radix_tlb.c
