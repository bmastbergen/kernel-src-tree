dm: fix bio splitting and its bio completion order for regular IO

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Mike Snitzer <snitzer@redhat.com>
commit ee1dfad5325ff1cfb2239e564cd411b3bfe8667a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/ee1dfad5.failed

dm_queue_split() is removed because __split_and_process_bio() _must_
handle splitting bios to ensure proper bio submission and completion
ordering as a bio is split.

Otherwise, multiple recursive calls to ->submit_bio will cause multiple
split bios to be allocated from the same ->bio_split mempool at the same
time. This would result in deadlock in low memory conditions because no
progress could be made (only one bio is available in ->bio_split
mempool).

This fix has been verified to still fix the loss of performance, due
to excess splitting, that commit 120c9257f5f1 provided.

Fixes: 120c9257f5f1 ("Revert "dm: always call blk_queue_split() in dm_process_bio()"")
	Cc: stable@vger.kernel.org # 5.0+, requires custom backport due to 5.9 changes
	Reported-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit ee1dfad5325ff1cfb2239e564cd411b3bfe8667a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm.c
diff --cc drivers/md/dm.c
index 8fa57aee9bd7,d948cd522431..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -1687,23 -1724,6 +1687,26 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void dm_queue_split(struct mapped_device *md, struct dm_target *ti, struct bio **bio)
 +{
 +	unsigned len, sector_count;
 +
 +	sector_count = bio_sectors(*bio);
 +	len = min_t(sector_t, max_io_len((*bio)->bi_iter.bi_sector, ti), sector_count);
 +
 +	if (sector_count > len) {
 +		struct bio *split = bio_split(*bio, len, GFP_NOIO, &md->queue->bio_split);
 +
 +		bio_chain(split, *bio);
 +		trace_block_split(md->queue, split, (*bio)->bi_iter.bi_sector);
 +		generic_make_request(*bio);
 +		*bio = split;
 +	}
 +}
 +
++=======
++>>>>>>> ee1dfad5325f (dm: fix bio splitting and its bio completion order for regular IO)
  static blk_qc_t dm_process_bio(struct mapped_device *md,
  			       struct dm_table *map, struct bio *bio)
  {
@@@ -1730,20 -1750,18 +1733,24 @@@
  	 */
  	if (current->bio_list) {
  		if (is_abnormal_io(bio))
++<<<<<<< HEAD
 +			blk_queue_split(md->queue, &bio);
 +		else
 +			dm_queue_split(md, ti, &bio);
++=======
+ 			blk_queue_split(&bio);
+ 		/* regular IO is split by __split_and_process_bio */
++>>>>>>> ee1dfad5325f (dm: fix bio splitting and its bio completion order for regular IO)
  	}
  
  	if (dm_get_md_type(md) == DM_TYPE_NVME_BIO_BASED)
  		return __process_bio(md, map, bio, ti);
- 	else
- 		return __split_and_process_bio(md, map, bio);
+ 	return __split_and_process_bio(md, map, bio);
  }
  
 -static blk_qc_t dm_submit_bio(struct bio *bio)
 +static blk_qc_t dm_make_request(struct request_queue *q, struct bio *bio)
  {
 -	struct mapped_device *md = bio->bi_disk->private_data;
 +	struct mapped_device *md = q->queuedata;
  	blk_qc_t ret = BLK_QC_T_NONE;
  	int srcu_idx;
  	struct dm_table *map;
* Unmerged path drivers/md/dm.c
