powerpc/mmu_gather: enable RCU_TABLE_FREE even for !SMP case

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
commit 12e4d53f3f04e81f9e83d6fc10edc7314ab9f6b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/12e4d53f.failed

Patch series "Fixup page directory freeing", v4.

This is a repost of patch series from Peter with the arch specific changes
except ppc64 dropped.  ppc64 changes are added here because we are redoing
the patch series on top of ppc64 changes.  This makes it easy to backport
these changes.  Only the first 2 patches need to be backported to stable.

The thing is, on anything SMP, freeing page directories should observe the
exact same order as normal page freeing:

 1) unhook page/directory
 2) TLB invalidate
 3) free page/directory

Without this, any concurrent page-table walk could end up with a
Use-after-Free.  This is esp.  trivial for anything that has software
page-table walkers (HAVE_FAST_GUP / software TLB fill) or the hardware
caches partial page-walks (ie.  caches page directories).

Even on UP this might give issues since mmu_gather is preemptible these
days.  An interrupt or preempted task accessing user pages might stumble
into the free page if the hardware caches page directories.

This patch series fixes ppc64 and add generic MMU_GATHER changes to
support the conversion of other architectures.  I haven't added patches
w.r.t other architecture because they are yet to be acked.

This patch (of 9):

A followup patch is going to make sure we correctly invalidate page walk
cache before we free page table pages.  In order to keep things simple
enable RCU_TABLE_FREE even for !SMP so that we don't have to fixup the
!SMP case differently in the followup patch

!SMP case is right now broken for radix translation w.r.t page walk
cache flush.  We can get interrupted in between page table free and
that would imply we have page walk cache entries pointing to tables
which got freed already.  Michael said "both our platforms that run on
Power9 force SMP on in Kconfig, so the !SMP case is unlikely to be a
problem for anyone in practice, unless they've hacked their kernel to
build it !SMP."

Link: http://lkml.kernel.org/r/20200116064531.483522-2-aneesh.kumar@linux.ibm.com
	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Michael Ellerman <mpe@ellerman.id.au>
	Cc: <stable@vger.kernel.org>

	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 12e4d53f3f04e81f9e83d6fc10edc7314ab9f6b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/Kconfig
#	arch/powerpc/include/asm/book3s/64/pgalloc.h
#	arch/powerpc/include/asm/nohash/pgalloc.h
diff --cc arch/powerpc/Kconfig
index f95f7924e00e,54b7f2af7cb1..000000000000
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@@ -209,7 -222,9 +209,13 @@@ config PP
  	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI && !HAVE_HARDLOCKUP_DETECTOR_ARCH
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
++<<<<<<< HEAD
 +	select HAVE_RCU_TABLE_FREE		if SMP
++=======
+ 	select HAVE_RCU_TABLE_FREE
+ 	select HAVE_RCU_TABLE_NO_INVALIDATE	if HAVE_RCU_TABLE_FREE
+ 	select HAVE_MMU_GATHER_PAGE_SIZE
++>>>>>>> 12e4d53f3f04 (powerpc/mmu_gather: enable RCU_TABLE_FREE even for !SMP case)
  	select HAVE_REGS_AND_STACK_ACCESS_API
  	select HAVE_RELIABLE_STACKTRACE		if PPC_BOOK3S_64 && CPU_LITTLE_ENDIAN
  	select HAVE_SYSCALL_TRACEPOINTS
diff --cc arch/powerpc/include/asm/book3s/64/pgalloc.h
index 39e5b697d581,a41e91bd0580..000000000000
--- a/arch/powerpc/include/asm/book3s/64/pgalloc.h
+++ b/arch/powerpc/include/asm/book3s/64/pgalloc.h
@@@ -19,37 -16,11 +19,40 @@@ struct vmemmap_backing 
  };
  extern struct vmemmap_backing *vmemmap_list;
  
 +/*
 + * Functions that deal with pagetables that could be at any level of
 + * the table need to be passed an "index_size" so they know how to
 + * handle allocation.  For PTE pages (which are linked to a struct
 + * page for now, and drawn from the main get_free_pages() pool), the
 + * allocation size will be (2^index_size * sizeof(pointer)) and
 + * allocations are drawn from the kmem_cache in PGT_CACHE(index_size).
 + *
 + * The maximum index size needs to be big enough to allow any
 + * pagetable sizes we need, but small enough to fit in the low bits of
 + * any page table pointer.  In other words all pagetables, even tiny
 + * ones, must be aligned to allow at least enough low 0 bits to
 + * contain this value.  This value is also used as a mask, so it must
 + * be one less than a power of two.
 + */
 +#define MAX_PGTABLE_INDEX_SIZE	0xf
 +
 +extern struct kmem_cache *pgtable_cache[];
 +#define PGT_CACHE(shift) ({				\
 +			BUG_ON(!(shift));		\
 +			pgtable_cache[(shift) - 1];	\
 +		})
 +
 +extern pte_t *pte_fragment_alloc(struct mm_struct *, unsigned long, int);
  extern pmd_t *pmd_fragment_alloc(struct mm_struct *, unsigned long);
 +extern void pte_fragment_free(unsigned long *, int);
  extern void pmd_fragment_free(unsigned long *);
  extern void pgtable_free_tlb(struct mmu_gather *tlb, void *table, int shift);
- #ifdef CONFIG_SMP
  extern void __tlb_remove_table(void *_table);
++<<<<<<< HEAD
 +#endif
++=======
+ void pte_frag_destroy(void *pte_frag);
++>>>>>>> 12e4d53f3f04 (powerpc/mmu_gather: enable RCU_TABLE_FREE even for !SMP case)
  
  static inline pgd_t *radix__pgd_alloc(struct mm_struct *mm)
  {
diff --cc arch/powerpc/include/asm/nohash/pgalloc.h
index 0634f2949438,29c43665a753..000000000000
--- a/arch/powerpc/include/asm/nohash/pgalloc.h
+++ b/arch/powerpc/include/asm/nohash/pgalloc.h
@@@ -21,4 -33,40 +21,43 @@@ static inline void tlb_flush_pgtable(st
  #else
  #include <asm/nohash/32/pgalloc.h>
  #endif
++<<<<<<< HEAD
++=======
+ 
+ static inline void pgtable_free(void *table, int shift)
+ {
+ 	if (!shift) {
+ 		pte_fragment_free((unsigned long *)table, 0);
+ 	} else {
+ 		BUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);
+ 		kmem_cache_free(PGT_CACHE(shift), table);
+ 	}
+ }
+ 
+ #define get_hugepd_cache_index(x)	(x)
+ 
+ static inline void pgtable_free_tlb(struct mmu_gather *tlb, void *table, int shift)
+ {
+ 	unsigned long pgf = (unsigned long)table;
+ 
+ 	BUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);
+ 	pgf |= shift;
+ 	tlb_remove_table(tlb, (void *)pgf);
+ }
+ 
+ static inline void __tlb_remove_table(void *_table)
+ {
+ 	void *table = (void *)((unsigned long)_table & ~MAX_PGTABLE_INDEX_SIZE);
+ 	unsigned shift = (unsigned long)_table & MAX_PGTABLE_INDEX_SIZE;
+ 
+ 	pgtable_free(table, shift);
+ }
+ 
+ static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t table,
+ 				  unsigned long address)
+ {
+ 	tlb_flush_pgtable(tlb, address);
+ 	pgtable_free_tlb(tlb, table, 0);
+ }
++>>>>>>> 12e4d53f3f04 (powerpc/mmu_gather: enable RCU_TABLE_FREE even for !SMP case)
  #endif /* _ASM_POWERPC_NOHASH_PGALLOC_H */
* Unmerged path arch/powerpc/Kconfig
diff --git a/arch/powerpc/include/asm/book3s/32/pgalloc.h b/arch/powerpc/include/asm/book3s/32/pgalloc.h
index 82e44b1a00ae..79ba3fbb512e 100644
--- a/arch/powerpc/include/asm/book3s/32/pgalloc.h
+++ b/arch/powerpc/include/asm/book3s/32/pgalloc.h
@@ -110,7 +110,6 @@ static inline void pgtable_free(void *table, unsigned index_size)
 #define check_pgt_cache()	do { } while (0)
 #define get_hugepd_cache_index(x)  (x)
 
-#ifdef CONFIG_SMP
 static inline void pgtable_free_tlb(struct mmu_gather *tlb,
 				    void *table, int shift)
 {
@@ -127,13 +126,6 @@ static inline void __tlb_remove_table(void *_table)
 
 	pgtable_free(table, shift);
 }
-#else
-static inline void pgtable_free_tlb(struct mmu_gather *tlb,
-				    void *table, int shift)
-{
-	pgtable_free(table, shift);
-}
-#endif
 
 static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t table,
 				  unsigned long address)
* Unmerged path arch/powerpc/include/asm/book3s/64/pgalloc.h
* Unmerged path arch/powerpc/include/asm/nohash/pgalloc.h
diff --git a/arch/powerpc/mm/book3s64/pgtable.c b/arch/powerpc/mm/book3s64/pgtable.c
index e3da07e289f2..816aed5da4fb 100644
--- a/arch/powerpc/mm/book3s64/pgtable.c
+++ b/arch/powerpc/mm/book3s64/pgtable.c
@@ -469,7 +469,6 @@ static inline void pgtable_free(void *table, int index)
 	}
 }
 
-#ifdef CONFIG_SMP
 void pgtable_free_tlb(struct mmu_gather *tlb, void *table, int index)
 {
 	unsigned long pgf = (unsigned long)table;
@@ -486,12 +485,6 @@ void __tlb_remove_table(void *_table)
 
 	return pgtable_free(table, index);
 }
-#else
-void pgtable_free_tlb(struct mmu_gather *tlb, void *table, int index)
-{
-	return pgtable_free(table, index);
-}
-#endif
 
 /*
  * For hash translation mode, we use the deposited table to store hash slot
