mm, debug_pagealloc: don't rely on static keys too early

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 8e57f8acbbd121ecfb0c9dc13b8b030f86c6bd3b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/8e57f8ac.failed

Commit 96a2b03f281d ("mm, debug_pagelloc: use static keys to enable
debugging") has introduced a static key to reduce overhead when
debug_pagealloc is compiled in but not enabled.  It relied on the
assumption that jump_label_init() is called before parse_early_param()
as in start_kernel(), so when the "debug_pagealloc=on" option is parsed,
it is safe to enable the static key.

However, it turns out multiple architectures call parse_early_param()
earlier from their setup_arch().  x86 also calls jump_label_init() even
earlier, so no issue was found while testing the commit, but same is not
true for e.g.  ppc64 and s390 where the kernel would not boot with
debug_pagealloc=on as found by our QA.

To fix this without tricky changes to init code of multiple
architectures, this patch partially reverts the static key conversion
from 96a2b03f281d.  Init-time and non-fastpath calls (such as in arch
code) of debug_pagealloc_enabled() will again test a simple bool
variable.  Fastpath mm code is converted to a new
debug_pagealloc_enabled_static() variant that relies on the static key,
which is enabled in a well-defined point in mm_init() where it's
guaranteed that jump_label_init() has been called, regardless of
architecture.

[sfr@canb.auug.org.au: export _debug_pagealloc_enabled_early]
  Link: http://lkml.kernel.org/r/20200106164944.063ac07b@canb.auug.org.au
Link: http://lkml.kernel.org/r/20191219130612.23171-1-vbabka@suse.cz
Fixes: 96a2b03f281d ("mm, debug_pagelloc: use static keys to enable debugging")
	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Qian Cai <cai@lca.pw>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8e57f8acbbd121ecfb0c9dc13b8b030f86c6bd3b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	init/main.c
#	mm/page_alloc.c
diff --cc include/linux/mm.h
index ef77bd76b21c,cfaa8feecfe8..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -2544,13 -2634,49 +2544,54 @@@ static inline void kernel_poison_pages(
  					int enable) { }
  #endif
  
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_PAGEALLOC_ENABLE_DEFAULT
 +DECLARE_STATIC_KEY_TRUE(_debug_pagealloc_enabled);
++=======
+ #ifdef CONFIG_INIT_ON_ALLOC_DEFAULT_ON
+ DECLARE_STATIC_KEY_TRUE(init_on_alloc);
  #else
- DECLARE_STATIC_KEY_FALSE(_debug_pagealloc_enabled);
+ DECLARE_STATIC_KEY_FALSE(init_on_alloc);
+ #endif
+ static inline bool want_init_on_alloc(gfp_t flags)
+ {
+ 	if (static_branch_unlikely(&init_on_alloc) &&
+ 	    !page_poisoning_enabled())
+ 		return true;
+ 	return flags & __GFP_ZERO;
+ }
+ 
+ #ifdef CONFIG_INIT_ON_FREE_DEFAULT_ON
+ DECLARE_STATIC_KEY_TRUE(init_on_free);
+ #else
+ DECLARE_STATIC_KEY_FALSE(init_on_free);
+ #endif
+ static inline bool want_init_on_free(void)
+ {
+ 	return static_branch_unlikely(&init_on_free) &&
+ 	       !page_poisoning_enabled();
+ }
+ 
+ #ifdef CONFIG_DEBUG_PAGEALLOC
+ extern void init_debug_pagealloc(void);
++>>>>>>> 8e57f8acbbd1 (mm, debug_pagealloc: don't rely on static keys too early)
+ #else
+ static inline void init_debug_pagealloc(void) {}
  #endif
+ extern bool _debug_pagealloc_enabled_early;
+ DECLARE_STATIC_KEY_FALSE(_debug_pagealloc_enabled);
  
  static inline bool debug_pagealloc_enabled(void)
+ {
+ 	return IS_ENABLED(CONFIG_DEBUG_PAGEALLOC) &&
+ 		_debug_pagealloc_enabled_early;
+ }
+ 
+ /*
+  * For use in fast paths after init_debug_pagealloc() has run, or when a
+  * false negative result is not harmful when called too early.
+  */
+ static inline bool debug_pagealloc_enabled_static(void)
  {
  	if (!IS_ENABLED(CONFIG_DEBUG_PAGEALLOC))
  		return false;
diff --cc init/main.c
index 9a8e05eb7480,da1bc0b60a7d..000000000000
--- a/init/main.c
+++ b/init/main.c
@@@ -524,9 -553,13 +524,14 @@@ static void __init mm_init(void
  	 * bigger than MAX_ORDER unless SPARSEMEM.
  	 */
  	page_ext_init_flatmem();
++<<<<<<< HEAD
++=======
+ 	init_debug_pagealloc();
+ 	report_meminit();
++>>>>>>> 8e57f8acbbd1 (mm, debug_pagealloc: don't rely on static keys too early)
  	mem_init();
  	kmem_cache_init();
 -	kmemleak_init();
  	pgtable_init();
 -	debug_objects_mem_init();
  	vmalloc_init();
  	ioremap_huge_init();
  	/* Should be run before the first non-init thread is created */
diff --cc mm/page_alloc.c
index 0f3a8eeb4d26,d047bf7d8fd4..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -1055,9 -1168,18 +1048,14 @@@ static __always_inline bool free_pages_
  		debug_check_no_obj_freed(page_address(page),
  					   PAGE_SIZE << order);
  	}
 -	if (want_init_on_free())
 -		kernel_init_free_pages(page, 1 << order);
 -
 -	kernel_poison_pages(page, 1 << order, 0);
 -	/*
 -	 * arch_free_page() can make the page's contents inaccessible.  s390
 -	 * does this.  So nothing which can access the page's contents should
 -	 * happen after this.
 -	 */
  	arch_free_page(page, order);
++<<<<<<< HEAD
 +	kernel_poison_pages(page, 1 << order, 0);
 +	if (debug_pagealloc_enabled())
++=======
+ 
+ 	if (debug_pagealloc_enabled_static())
++>>>>>>> 8e57f8acbbd1 (mm, debug_pagealloc: don't rely on static keys too early)
  		kernel_map_pages(page, 1 << order, 0);
  
  	kasan_free_nondeferred_pages(page, order);
* Unmerged path include/linux/mm.h
* Unmerged path init/main.c
* Unmerged path mm/page_alloc.c
diff --git a/mm/slab.c b/mm/slab.c
index 30f99e328ad8..8624088ddd05 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -1455,7 +1455,7 @@ static void kmem_rcu_free(struct rcu_head *head)
 #if DEBUG
 static bool is_debug_pagealloc_cache(struct kmem_cache *cachep)
 {
-	if (debug_pagealloc_enabled() && OFF_SLAB(cachep) &&
+	if (debug_pagealloc_enabled_static() && OFF_SLAB(cachep) &&
 		(cachep->size % PAGE_SIZE) == 0)
 		return true;
 
@@ -2075,7 +2075,7 @@ int __kmem_cache_create(struct kmem_cache *cachep, slab_flags_t flags)
 	 * to check size >= 256. It guarantees that all necessary small
 	 * sized slab is initialized in current slab initialization sequence.
 	 */
-	if (debug_pagealloc_enabled() && (flags & SLAB_POISON) &&
+	if (debug_pagealloc_enabled_static() && (flags & SLAB_POISON) &&
 		size >= 256 && cachep->object_size > cache_line_size()) {
 		if (size < PAGE_SIZE || size % PAGE_SIZE == 0) {
 			size_t tmp_size = ALIGN(size, PAGE_SIZE);
diff --git a/mm/slub.c b/mm/slub.c
index 42c9659b52eb..48aac03dd673 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -290,7 +290,7 @@ static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 	unsigned long freepointer_addr;
 	void *p;
 
-	if (!debug_pagealloc_enabled())
+	if (!debug_pagealloc_enabled_static())
 		return get_freepointer(s, object);
 
 	freepointer_addr = (unsigned long)object + s->offset;
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index c82a0db1aefc..601bb965249c 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -752,7 +752,7 @@ static void free_unmap_vmap_area(struct vmap_area *va)
 {
 	flush_cache_vunmap(va->va_start, va->va_end);
 	unmap_vmap_area(va);
-	if (debug_pagealloc_enabled())
+	if (debug_pagealloc_enabled_static())
 		flush_tlb_kernel_range(va->va_start, va->va_end);
 
 	free_vmap_area_noflush(va);
@@ -1052,7 +1052,7 @@ static void vb_free(const void *addr, unsigned long size)
 
 	vunmap_page_range((unsigned long)addr, (unsigned long)addr + size);
 
-	if (debug_pagealloc_enabled())
+	if (debug_pagealloc_enabled_static())
 		flush_tlb_kernel_range((unsigned long)addr,
 					(unsigned long)addr + size);
 
