mptcp: add OoO related mibs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Paolo Abeni <pabeni@redhat.com>
commit 06242e44b9fbd59be0bd4a4ed82b38f8f2c3f4b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/06242e44.failed

Add a bunch of MPTCP mibs related to MPTCP OoO data
processing.

	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Reviewed-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 06242e44b9fbd59be0bd4a4ed82b38f8f2c3f4b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/mptcp/protocol.c
#	net/mptcp/subflow.c
diff --cc net/mptcp/protocol.c
index 691a71ca323f,ec9c38d3acc7..000000000000
--- a/net/mptcp/protocol.c
+++ b/net/mptcp/protocol.c
@@@ -139,16 -128,143 +139,150 @@@ static bool mptcp_try_coalesce(struct s
  	    !skb_try_coalesce(to, from, &fragstolen, &delta))
  		return false;
  
++<<<<<<< HEAD
++=======
+ 	pr_debug("colesced seq %llx into %llx new len %d new end seq %llx",
+ 		 MPTCP_SKB_CB(from)->map_seq, MPTCP_SKB_CB(to)->map_seq,
+ 		 to->len, MPTCP_SKB_CB(from)->end_seq);
+ 	MPTCP_SKB_CB(to)->end_seq = MPTCP_SKB_CB(from)->end_seq;
++>>>>>>> 06242e44b9fb (mptcp: add OoO related mibs)
  	kfree_skb_partial(from, fragstolen);
  	atomic_add(delta, &sk->sk_rmem_alloc);
  	sk_mem_charge(sk, delta);
  	return true;
  }
  
 -static bool mptcp_ooo_try_coalesce(struct mptcp_sock *msk, struct sk_buff *to,
 -				   struct sk_buff *from)
 +static void __mptcp_move_skb(struct mptcp_sock *msk, struct sock *ssk,
 +			     struct sk_buff *skb,
 +			     unsigned int offset, size_t copy_len)
  {
++<<<<<<< HEAD
++=======
+ 	if (MPTCP_SKB_CB(from)->map_seq != MPTCP_SKB_CB(to)->end_seq)
+ 		return false;
+ 
+ 	return mptcp_try_coalesce((struct sock *)msk, to, from);
+ }
+ 
+ /* "inspired" by tcp_data_queue_ofo(), main differences:
+  * - use mptcp seqs
+  * - don't cope with sacks
+  */
+ static void mptcp_data_queue_ofo(struct mptcp_sock *msk, struct sk_buff *skb)
+ {
+ 	struct sock *sk = (struct sock *)msk;
+ 	struct rb_node **p, *parent;
+ 	u64 seq, end_seq, max_seq;
+ 	struct sk_buff *skb1;
+ 
+ 	seq = MPTCP_SKB_CB(skb)->map_seq;
+ 	end_seq = MPTCP_SKB_CB(skb)->end_seq;
+ 	max_seq = tcp_space(sk);
+ 	max_seq = max_seq > 0 ? max_seq + msk->ack_seq : msk->ack_seq;
+ 
+ 	pr_debug("msk=%p seq=%llx limit=%llx empty=%d", msk, seq, max_seq,
+ 		 RB_EMPTY_ROOT(&msk->out_of_order_queue));
+ 	if (after64(seq, max_seq)) {
+ 		/* out of window */
+ 		mptcp_drop(sk, skb);
+ 		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_NODSSWINDOW);
+ 		return;
+ 	}
+ 
+ 	p = &msk->out_of_order_queue.rb_node;
+ 	MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_OFOQUEUE);
+ 	if (RB_EMPTY_ROOT(&msk->out_of_order_queue)) {
+ 		rb_link_node(&skb->rbnode, NULL, p);
+ 		rb_insert_color(&skb->rbnode, &msk->out_of_order_queue);
+ 		msk->ooo_last_skb = skb;
+ 		goto end;
+ 	}
+ 
+ 	/* with 2 subflows, adding at end of ooo queue is quite likely
+ 	 * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.
+ 	 */
+ 	if (mptcp_ooo_try_coalesce(msk, msk->ooo_last_skb, skb)) {
+ 		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_OFOMERGE);
+ 		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_OFOQUEUETAIL);
+ 		return;
+ 	}
+ 
+ 	/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */
+ 	if (!before64(seq, MPTCP_SKB_CB(msk->ooo_last_skb)->end_seq)) {
+ 		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_OFOQUEUETAIL);
+ 		parent = &msk->ooo_last_skb->rbnode;
+ 		p = &parent->rb_right;
+ 		goto insert;
+ 	}
+ 
+ 	/* Find place to insert this segment. Handle overlaps on the way. */
+ 	parent = NULL;
+ 	while (*p) {
+ 		parent = *p;
+ 		skb1 = rb_to_skb(parent);
+ 		if (before64(seq, MPTCP_SKB_CB(skb1)->map_seq)) {
+ 			p = &parent->rb_left;
+ 			continue;
+ 		}
+ 		if (before64(seq, MPTCP_SKB_CB(skb1)->end_seq)) {
+ 			if (!after64(end_seq, MPTCP_SKB_CB(skb1)->end_seq)) {
+ 				/* All the bits are present. Drop. */
+ 				mptcp_drop(sk, skb);
+ 				MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DUPDATA);
+ 				return;
+ 			}
+ 			if (after64(seq, MPTCP_SKB_CB(skb1)->map_seq)) {
+ 				/* partial overlap:
+ 				 *     |     skb      |
+ 				 *  |     skb1    |
+ 				 * continue traversing
+ 				 */
+ 			} else {
+ 				/* skb's seq == skb1's seq and skb covers skb1.
+ 				 * Replace skb1 with skb.
+ 				 */
+ 				rb_replace_node(&skb1->rbnode, &skb->rbnode,
+ 						&msk->out_of_order_queue);
+ 				mptcp_drop(sk, skb1);
+ 				MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DUPDATA);
+ 				goto merge_right;
+ 			}
+ 		} else if (mptcp_ooo_try_coalesce(msk, skb1, skb)) {
+ 			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_OFOMERGE);
+ 			return;
+ 		}
+ 		p = &parent->rb_right;
+ 	}
+ 
+ insert:
+ 	/* Insert segment into RB tree. */
+ 	rb_link_node(&skb->rbnode, parent, p);
+ 	rb_insert_color(&skb->rbnode, &msk->out_of_order_queue);
+ 
+ merge_right:
+ 	/* Remove other segments covered by skb. */
+ 	while ((skb1 = skb_rb_next(skb)) != NULL) {
+ 		if (before64(end_seq, MPTCP_SKB_CB(skb1)->end_seq))
+ 			break;
+ 		rb_erase(&skb1->rbnode, &msk->out_of_order_queue);
+ 		mptcp_drop(sk, skb1);
+ 		MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DUPDATA);
+ 	}
+ 	/* If there is no skb after us, we are the last_skb ! */
+ 	if (!skb1)
+ 		msk->ooo_last_skb = skb;
+ 
+ end:
+ 	skb_condense(skb);
+ 	skb_set_owner_r(skb, sk);
+ }
+ 
+ static bool __mptcp_move_skb(struct mptcp_sock *msk, struct sock *ssk,
+ 			     struct sk_buff *skb, unsigned int offset,
+ 			     size_t copy_len)
+ {
+ 	struct mptcp_subflow_context *subflow = mptcp_subflow_ctx(ssk);
++>>>>>>> 06242e44b9fb (mptcp: add OoO related mibs)
  	struct sock *sk = (struct sock *)msk;
  	struct sk_buff *tail;
  
@@@ -156,15 -272,36 +290,32 @@@
  
  	skb_ext_reset(skb);
  	skb_orphan(skb);
 -
 -	/* the skb map_seq accounts for the skb offset:
 -	 * mptcp_subflow_get_mapped_dsn() is based on the current tp->copied_seq
 -	 * value
 -	 */
 -	MPTCP_SKB_CB(skb)->map_seq = mptcp_subflow_get_mapped_dsn(subflow);
 -	MPTCP_SKB_CB(skb)->end_seq = MPTCP_SKB_CB(skb)->map_seq + copy_len;
  	MPTCP_SKB_CB(skb)->offset = offset;
 +	msk->ack_seq += copy_len;
  
 -	if (MPTCP_SKB_CB(skb)->map_seq == msk->ack_seq) {
 -		/* in sequence */
 -		msk->ack_seq += copy_len;
 -		tail = skb_peek_tail(&sk->sk_receive_queue);
 -		if (tail && mptcp_try_coalesce(sk, tail, skb))
 -			return true;
 +	tail = skb_peek_tail(&sk->sk_receive_queue);
 +	if (tail && mptcp_try_coalesce(sk, tail, skb))
 +		return;
  
++<<<<<<< HEAD
 +	skb_set_owner_r(skb, sk);
 +	__skb_queue_tail(&sk->sk_receive_queue, skb);
++=======
+ 		skb_set_owner_r(skb, sk);
+ 		__skb_queue_tail(&sk->sk_receive_queue, skb);
+ 		return true;
+ 	} else if (after64(MPTCP_SKB_CB(skb)->map_seq, msk->ack_seq)) {
+ 		mptcp_data_queue_ofo(msk, skb);
+ 		return false;
+ 	}
+ 
+ 	/* old data, keep it simple and drop the whole pkt, sender
+ 	 * will retransmit as needed, if needed.
+ 	 */
+ 	MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DUPDATA);
+ 	mptcp_drop(sk, skb);
+ 	return false;
++>>>>>>> 06242e44b9fb (mptcp: add OoO related mibs)
  }
  
  static void mptcp_stop_timer(struct sock *sk)
@@@ -417,6 -519,49 +568,52 @@@ static bool __mptcp_move_skbs_from_subf
  	return done;
  }
  
++<<<<<<< HEAD
++=======
+ static bool mptcp_ofo_queue(struct mptcp_sock *msk)
+ {
+ 	struct sock *sk = (struct sock *)msk;
+ 	struct sk_buff *skb, *tail;
+ 	bool moved = false;
+ 	struct rb_node *p;
+ 	u64 end_seq;
+ 
+ 	p = rb_first(&msk->out_of_order_queue);
+ 	pr_debug("msk=%p empty=%d", msk, RB_EMPTY_ROOT(&msk->out_of_order_queue));
+ 	while (p) {
+ 		skb = rb_to_skb(p);
+ 		if (after64(MPTCP_SKB_CB(skb)->map_seq, msk->ack_seq))
+ 			break;
+ 
+ 		p = rb_next(p);
+ 		rb_erase(&skb->rbnode, &msk->out_of_order_queue);
+ 
+ 		if (unlikely(!after64(MPTCP_SKB_CB(skb)->end_seq,
+ 				      msk->ack_seq))) {
+ 			mptcp_drop(sk, skb);
+ 			MPTCP_INC_STATS(sock_net(sk), MPTCP_MIB_DUPDATA);
+ 			continue;
+ 		}
+ 
+ 		end_seq = MPTCP_SKB_CB(skb)->end_seq;
+ 		tail = skb_peek_tail(&sk->sk_receive_queue);
+ 		if (!tail || !mptcp_ooo_try_coalesce(msk, tail, skb)) {
+ 			int delta = msk->ack_seq - MPTCP_SKB_CB(skb)->map_seq;
+ 
+ 			/* skip overlapping data, if any */
+ 			pr_debug("uncoalesced seq=%llx ack seq=%llx delta=%d",
+ 				 MPTCP_SKB_CB(skb)->map_seq, msk->ack_seq,
+ 				 delta);
+ 			MPTCP_SKB_CB(skb)->offset += delta;
+ 			__skb_queue_tail(&sk->sk_receive_queue, skb);
+ 		}
+ 		msk->ack_seq = end_seq;
+ 		moved = true;
+ 	}
+ 	return moved;
+ }
+ 
++>>>>>>> 06242e44b9fb (mptcp: add OoO related mibs)
  /* In most cases we will be able to lock the mptcp socket.  If its already
   * owned, we need to defer to the work queue to avoid ABBA deadlock.
   */
diff --cc net/mptcp/subflow.c
index 750f1161d6e1,8c9418d1901b..000000000000
--- a/net/mptcp/subflow.c
+++ b/net/mptcp/subflow.c
@@@ -768,16 -805,23 +768,27 @@@ validate_seq
  	return MAPPING_OK;
  }
  
 -static void mptcp_subflow_discard_data(struct sock *ssk, struct sk_buff *skb,
 -				       unsigned int limit)
 +static int subflow_read_actor(read_descriptor_t *desc,
 +			      struct sk_buff *skb,
 +			      unsigned int offset, size_t len)
  {
 -	struct mptcp_subflow_context *subflow = mptcp_subflow_ctx(ssk);
 -	bool fin = TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN;
 -	u32 incr;
 +	size_t copy_len = min(desc->count, len);
  
 -	incr = limit >= skb->len ? skb->len + fin : limit;
 +	desc->count -= copy_len;
  
++<<<<<<< HEAD
 +	pr_debug("flushed %zu bytes, %zu left", copy_len, desc->count);
 +	return copy_len;
++=======
+ 	pr_debug("discarding=%d len=%d seq=%d", incr, skb->len,
+ 		 subflow->map_subflow_seq);
+ 	MPTCP_INC_STATS(sock_net(ssk), MPTCP_MIB_DUPDATA);
+ 	tcp_sk(ssk)->copied_seq += incr;
+ 	if (!before(tcp_sk(ssk)->copied_seq, TCP_SKB_CB(skb)->end_seq))
+ 		sk_eat_skb(ssk, skb);
+ 	if (mptcp_subflow_get_map_offset(subflow) >= subflow->map_data_len)
+ 		subflow->map_valid = 0;
++>>>>>>> 06242e44b9fb (mptcp: add OoO related mibs)
  }
  
  static bool subflow_check_data_avail(struct sock *ssk)
diff --git a/net/mptcp/mib.c b/net/mptcp/mib.c
index 526ebb63e722..1b0cdbfa43ef 100644
--- a/net/mptcp/mib.c
+++ b/net/mptcp/mib.c
@@ -22,6 +22,11 @@ static const struct snmp_mib mptcp_snmp_list[] = {
 	SNMP_MIB_ITEM("MPJoinAckHMacFailure", MPTCP_MIB_JOINACKMAC),
 	SNMP_MIB_ITEM("DSSNotMatching", MPTCP_MIB_DSSNOMATCH),
 	SNMP_MIB_ITEM("InfiniteMapRx", MPTCP_MIB_INFINITEMAPRX),
+	SNMP_MIB_ITEM("OFOQueueTail", MPTCP_MIB_OFOQUEUETAIL),
+	SNMP_MIB_ITEM("OFOQueue", MPTCP_MIB_OFOQUEUE),
+	SNMP_MIB_ITEM("OFOMerge", MPTCP_MIB_OFOMERGE),
+	SNMP_MIB_ITEM("NoDSSInWindow", MPTCP_MIB_NODSSWINDOW),
+	SNMP_MIB_ITEM("DuplicateData", MPTCP_MIB_DUPDATA),
 	SNMP_MIB_SENTINEL
 };
 
diff --git a/net/mptcp/mib.h b/net/mptcp/mib.h
index 5579e7107989..d33447b07c28 100644
--- a/net/mptcp/mib.h
+++ b/net/mptcp/mib.h
@@ -15,6 +15,11 @@ enum linux_mptcp_mib_field {
 	MPTCP_MIB_JOINACKMAC,		/* HMAC was wrong on ACK + MP_JOIN */
 	MPTCP_MIB_DSSNOMATCH,		/* Received a new mapping that did not match the previous one */
 	MPTCP_MIB_INFINITEMAPRX,	/* Received an infinite mapping */
+	MPTCP_MIB_OFOQUEUETAIL,	/* Segments inserted into OoO queue tail */
+	MPTCP_MIB_OFOQUEUE,		/* Segments inserted into OoO queue */
+	MPTCP_MIB_OFOMERGE,		/* Segments merged in OoO queue */
+	MPTCP_MIB_NODSSWINDOW,		/* Segments not in MPTCP windows */
+	MPTCP_MIB_DUPDATA,		/* Segments discarded due to duplicate DSS */
 	__MPTCP_MIB_MAX
 };
 
* Unmerged path net/mptcp/protocol.c
* Unmerged path net/mptcp/subflow.c
