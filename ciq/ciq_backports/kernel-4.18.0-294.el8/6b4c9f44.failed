filemap: drop the mmap_sem for all blocking operations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Josef Bacik <josef@toxicpanda.com>
commit 6b4c9f4469819a0c1a38a0a4541337e0f9bf6c11
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/6b4c9f44.failed

Currently we only drop the mmap_sem if there is contention on the page
lock.  The idea is that we issue readahead and then go to lock the page
while it is under IO and we want to not hold the mmap_sem during the IO.

The problem with this is the assumption that the readahead does anything.
In the case that the box is under extreme memory or IO pressure we may end
up not reading anything at all for readahead, which means we will end up
reading in the page under the mmap_sem.

Even if the readahead does something, it could get throttled because of io
pressure on the system and the process is in a lower priority cgroup.

Holding the mmap_sem while doing IO is problematic because it can cause
system-wide priority inversions.  Consider some large company that does a
lot of web traffic.  This large company has load balancing logic in it's
core web server, cause some engineer thought this was a brilliant plan.
This load balancing logic gets statistics from /proc about the system,
which trip over processes mmap_sem for various reasons.  Now the web
server application is in a protected cgroup, but these other processes may
not be, and if they are being throttled while their mmap_sem is held we'll
stall, and cause this nice death spiral.

Instead rework filemap fault path to drop the mmap sem at any point that
we may do IO or block for an extended period of time.  This includes while
issuing readahead, locking the page, or needing to call ->readpage because
readahead did not occur.  Then once we have a fully uptodate page we can
return with VM_FAULT_RETRY and come back again to find our nicely in-cache
page that was gotten outside of the mmap_sem.

This patch also adds a new helper for locking the page with the mmap_sem
dropped.  This doesn't make sense currently as generally speaking if the
page is already locked it'll have been read in (unless there was an error)
before it was unlocked.  However a forthcoming patchset will change this
with the ability to abort read-ahead bio's if necessary, making it more
likely that we could contend for a page lock and still have a not uptodate
page.  This allows us to deal with this case by grabbing the lock and
issuing the IO without the mmap_sem held, and then returning
VM_FAULT_RETRY to come back around.

[josef@toxicpanda.com: v6]
  Link: http://lkml.kernel.org/r/20181212152757.10017-1-josef@toxicpanda.com
[kirill@shutemov.name: fix race in filemap_fault()]
  Link: http://lkml.kernel.org/r/20181228235106.okk3oastsnpxusxs@kshutemo-mobl1
[akpm@linux-foundation.org: coding style fixes]
Link: http://lkml.kernel.org/r/20181211173801.29535-4-josef@toxicpanda.com
	Signed-off-by: Josef Bacik <josef@toxicpanda.com>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Tested-by: syzbot+b437b5a429d680cf2217@syzkaller.appspotmail.com
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 6b4c9f4469819a0c1a38a0a4541337e0f9bf6c11)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/filemap.c
diff --cc mm/filemap.c
index c5561788ab4c,2815cb79a246..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -2375,46 -2389,75 +2375,108 @@@ out
  EXPORT_SYMBOL(generic_file_read_iter);
  
  #ifdef CONFIG_MMU
 +/**
 + * page_cache_read - adds requested page to the page cache if not already there
 + * @file:	file to read
 + * @offset:	page index
 + * @gfp_mask:	memory allocation flags
 + *
 + * This adds the requested page to the page cache if it isn't already there,
 + * and schedules an I/O to read in its contents from disk.
 + */
 +static int page_cache_read(struct file *file, pgoff_t offset, gfp_t gfp_mask)
 +{
 +	struct address_space *mapping = file->f_mapping;
 +	struct page *page;
 +	int ret;
 +
 +	do {
 +		page = __page_cache_alloc(gfp_mask);
 +		if (!page)
 +			return -ENOMEM;
 +
 +		ret = add_to_page_cache_lru(page, mapping, offset, gfp_mask);
 +		if (ret == 0)
 +			ret = mapping->a_ops->readpage(file, page);
 +		else if (ret == -EEXIST)
 +			ret = 0; /* losing race to add is OK */
 +
 +		put_page(page);
 +
 +	} while (ret == AOP_TRUNCATED_PAGE);
 +
 +	return ret;
 +}
 +
  #define MMAP_LOTSAMISS  (100)
+ static struct file *maybe_unlock_mmap_for_io(struct vm_fault *vmf,
+ 					     struct file *fpin)
+ {
+ 	int flags = vmf->flags;
+ 
+ 	if (fpin)
+ 		return fpin;
+ 
+ 	/*
+ 	 * FAULT_FLAG_RETRY_NOWAIT means we don't want to wait on page locks or
+ 	 * anything, so we only pin the file and drop the mmap_sem if only
+ 	 * FAULT_FLAG_ALLOW_RETRY is set.
+ 	 */
+ 	if ((flags & (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT)) ==
+ 	    FAULT_FLAG_ALLOW_RETRY) {
+ 		fpin = get_file(vmf->vma->vm_file);
+ 		up_read(&vmf->vma->vm_mm->mmap_sem);
+ 	}
+ 	return fpin;
+ }
+ 
+ /*
+  * lock_page_maybe_drop_mmap - lock the page, possibly dropping the mmap_sem
+  * @vmf - the vm_fault for this fault.
+  * @page - the page to lock.
+  * @fpin - the pointer to the file we may pin (or is already pinned).
+  *
+  * This works similar to lock_page_or_retry in that it can drop the mmap_sem.
+  * It differs in that it actually returns the page locked if it returns 1 and 0
+  * if it couldn't lock the page.  If we did have to drop the mmap_sem then fpin
+  * will point to the pinned file and needs to be fput()'ed at a later point.
+  */
+ static int lock_page_maybe_drop_mmap(struct vm_fault *vmf, struct page *page,
+ 				     struct file **fpin)
+ {
+ 	if (trylock_page(page))
+ 		return 1;
+ 
+ 	if (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)
+ 		return 0;
+ 
+ 	*fpin = maybe_unlock_mmap_for_io(vmf, *fpin);
+ 	if (vmf->flags & FAULT_FLAG_KILLABLE) {
+ 		if (__lock_page_killable(page)) {
+ 			/*
+ 			 * We didn't have the right flags to drop the mmap_sem,
+ 			 * but all fault_handlers only check for fatal signals
+ 			 * if we return VM_FAULT_RETRY, so we need to drop the
+ 			 * mmap_sem here and return 0 if we don't have a fpin.
+ 			 */
+ 			if (*fpin == NULL)
+ 				up_read(&vmf->vma->vm_mm->mmap_sem);
+ 			return 0;
+ 		}
+ 	} else
+ 		__lock_page(page);
+ 	return 1;
+ }
+ 
  
  /*
-  * Synchronous readahead happens when we don't even find
-  * a page in the page cache at all.
+  * Synchronous readahead happens when we don't even find a page in the page
+  * cache at all.  We don't want to perform IO under the mmap sem, so if we have
+  * to drop the mmap sem we return the file that was pinned in order for us to do
+  * that.  If we didn't pin a file then we return NULL.  The file that is
+  * returned needs to be fput()'ed when we're done with it.
   */
- static void do_sync_mmap_readahead(struct vm_fault *vmf)
+ static struct file *do_sync_mmap_readahead(struct vm_fault *vmf)
  {
  	struct file *file = vmf->vma->vm_file;
  	struct file_ra_state *ra = &file->f_ra;
@@@ -2529,17 -2584,20 +2601,26 @@@ vm_fault_t filemap_fault(struct vm_faul
  		count_vm_event(PGMAJFAULT);
  		count_memcg_event_mm(vmf->vma->vm_mm, PGMAJFAULT);
  		ret = VM_FAULT_MAJOR;
+ 		fpin = do_sync_mmap_readahead(vmf);
  retry_find:
++<<<<<<< HEAD
 +		page = find_get_page(mapping, offset);
 +		if (!page)
 +			goto no_cached_page;
++=======
+ 		page = pagecache_get_page(mapping, offset,
+ 					  FGP_CREAT|FGP_FOR_MMAP,
+ 					  vmf->gfp_mask);
+ 		if (!page) {
+ 			if (fpin)
+ 				goto out_retry;
+ 			return vmf_error(-ENOMEM);
+ 		}
++>>>>>>> 6b4c9f446981 (filemap: drop the mmap_sem for all blocking operations)
  	}
  
- 	if (!lock_page_or_retry(page, vmf->vma->vm_mm, vmf->flags)) {
- 		put_page(page);
- 		return ret | VM_FAULT_RETRY;
- 	}
+ 	if (!lock_page_maybe_drop_mmap(vmf, page, &fpin))
+ 		goto out_retry;
  
  	/* Did it get truncated? */
  	if (unlikely(page->mapping != mapping)) {
* Unmerged path mm/filemap.c
