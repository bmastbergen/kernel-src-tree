rcu/tree: Defer kvfree_rcu() allocation to a clean context

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Uladzislau Rezki (Sony) <urezki@gmail.com>
commit 56292e8609e39537297a7468dda4d87b9bd81d6a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/56292e86.failed

The current memmory-allocation interface causes the following difficulties
for kvfree_rcu():

a) If built with CONFIG_PROVE_RAW_LOCK_NESTING, the lockdep will
   complain about violation of the nesting rules, as in "BUG: Invalid
   wait context".  This Kconfig option checks for proper raw_spinlock
   vs. spinlock nesting, in particular, it is not legal to acquire a
   spinlock_t while holding a raw_spinlock_t.

   This is a problem because kfree_rcu() uses raw_spinlock_t whereas the
   "page allocator" internally deals with spinlock_t to access to its
   zones. The code also can be broken from higher level of view:
   <snip>
       raw_spin_lock(&some_lock);
       kfree_rcu(some_pointer, some_field_offset);
   <snip>

b) If built with CONFIG_PREEMPT_RT, spinlock_t is converted into
   sleeplock.  This means that invoking the page allocator from atomic
   contexts results in "BUG: scheduling while atomic".

c) Please note that call_rcu() is already invoked from raw atomic context,
   so it is only reasonable to expaect that kfree_rcu() and kvfree_rcu()
   will also be called from atomic raw context.

This commit therefore defers page allocation to a clean context using the
combination of an hrtimer and a workqueue.  The hrtimer stage is required
in order to avoid deadlocks with the scheduler.  This deferred allocation
is required only when kvfree_rcu()'s per-CPU page cache is empty.

Link: https://lore.kernel.org/lkml/20200630164543.4mdcf6zb4zfclhln@linutronix.de/
Fixes: 3042f83f19be ("rcu: Support reclaim for head-less object")
	Reported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
(cherry picked from commit 56292e8609e39537297a7468dda4d87b9bd81d6a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index 4aa7b6bbcde6,01918d8cffb3..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -163,6 -165,21 +163,24 @@@ module_param(gp_init_delay, int, 0444)
  static int gp_cleanup_delay;
  module_param(gp_cleanup_delay, int, 0444);
  
++<<<<<<< HEAD
++=======
+ // Add delay to rcu_read_unlock() for strict grace periods.
+ static int rcu_unlock_delay;
+ #ifdef CONFIG_RCU_STRICT_GRACE_PERIOD
+ module_param(rcu_unlock_delay, int, 0444);
+ #endif
+ 
+ /*
+  * This rcu parameter is runtime-read-only. It reflects
+  * a minimum allowed number of objects which can be cached
+  * per-CPU. Object size is equal to one page. This value
+  * can be changed at boot time.
+  */
+ static int rcu_min_cached_objs = 5;
+ module_param(rcu_min_cached_objs, int, 0444);
+ 
++>>>>>>> 56292e8609e3 (rcu/tree: Defer kvfree_rcu() allocation to a clean context)
  /* Retrieve RCU kthreads priority for rcutorture */
  int rcu_get_gp_kthreads_prio(void)
  {
@@@ -2702,7 -3082,17 +2720,21 @@@ struct kfree_rcu_cpu_work 
   * @lock: Synchronize access to this structure
   * @monitor_work: Promote @head to @head_free after KFREE_DRAIN_JIFFIES
   * @monitor_todo: Tracks whether a @monitor_work delayed work is pending
++<<<<<<< HEAD
 + * @initialized: The @lock and @rcu_work fields have been initialized
++=======
+  * @initialized: The @rcu_work fields have been initialized
+  * @count: Number of objects for which GP not started
+  * @bkvcache:
+  *	A simple cache list that contains objects for reuse purpose.
+  *	In order to save some per-cpu space the list is singular.
+  *	Even though it is lockless an access has to be protected by the
+  *	per-cpu lock.
+  * @page_cache_work: A work to refill the cache when it is empty
+  * @work_in_progress: Indicates that page_cache_work is running
+  * @hrtimer: A hrtimer for scheduling a page_cache_work
+  * @nr_bkv_objs: number of allocated objects at @bkvcache.
++>>>>>>> 56292e8609e3 (rcu/tree: Defer kvfree_rcu() allocation to a clean context)
   *
   * This is a per-CPU structure.  The reason that it is not included in
   * the rcu_data structure is to permit this code to be extracted from
@@@ -2716,9 -3107,74 +2748,20 @@@ struct kfree_rcu_cpu 
  	struct delayed_work monitor_work;
  	bool monitor_todo;
  	bool initialized;
++<<<<<<< HEAD
++=======
+ 	int count;
+ 
+ 	struct work_struct page_cache_work;
+ 	atomic_t work_in_progress;
+ 	struct hrtimer hrtimer;
+ 
+ 	struct llist_head bkvcache;
+ 	int nr_bkv_objs;
++>>>>>>> 56292e8609e3 (rcu/tree: Defer kvfree_rcu() allocation to a clean context)
  };
  
 -static DEFINE_PER_CPU(struct kfree_rcu_cpu, krc) = {
 -	.lock = __RAW_SPIN_LOCK_UNLOCKED(krc.lock),
 -};
 -
 -static __always_inline void
 -debug_rcu_bhead_unqueue(struct kvfree_rcu_bulk_data *bhead)
 -{
 -#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD
 -	int i;
 -
 -	for (i = 0; i < bhead->nr_records; i++)
 -		debug_rcu_head_unqueue((struct rcu_head *)(bhead->records[i]));
 -#endif
 -}
 -
 -static inline struct kfree_rcu_cpu *
 -krc_this_cpu_lock(unsigned long *flags)
 -{
 -	struct kfree_rcu_cpu *krcp;
 -
 -	local_irq_save(*flags);	// For safely calling this_cpu_ptr().
 -	krcp = this_cpu_ptr(&krc);
 -	raw_spin_lock(&krcp->lock);
 -
 -	return krcp;
 -}
 -
 -static inline void
 -krc_this_cpu_unlock(struct kfree_rcu_cpu *krcp, unsigned long flags)
 -{
 -	raw_spin_unlock(&krcp->lock);
 -	local_irq_restore(flags);
 -}
 -
 -static inline struct kvfree_rcu_bulk_data *
 -get_cached_bnode(struct kfree_rcu_cpu *krcp)
 -{
 -	if (!krcp->nr_bkv_objs)
 -		return NULL;
 -
 -	krcp->nr_bkv_objs--;
 -	return (struct kvfree_rcu_bulk_data *)
 -		llist_del_first(&krcp->bkvcache);
 -}
 -
 -static inline bool
 -put_cached_bnode(struct kfree_rcu_cpu *krcp,
 -	struct kvfree_rcu_bulk_data *bnode)
 -{
 -	// Check the limit.
 -	if (krcp->nr_bkv_objs >= rcu_min_cached_objs)
 -		return false;
 -
 -	llist_add((struct llist_node *) bnode, &krcp->bkvcache);
 -	krcp->nr_bkv_objs++;
 -	return true;
 -
 -}
 +static DEFINE_PER_CPU(struct kfree_rcu_cpu, krc);
  
  /*
   * This function is invoked in workqueue context after a grace period.
@@@ -2734,17 -3192,74 +2777,62 @@@ static void kfree_rcu_work(struct work_
  	krwp = container_of(to_rcu_work(work),
  			    struct kfree_rcu_cpu_work, rcu_work);
  	krcp = krwp->krcp;
 -
 -	raw_spin_lock_irqsave(&krcp->lock, flags);
 -	// Channels 1 and 2.
 -	for (i = 0; i < FREE_N_CHANNELS; i++) {
 -		bkvhead[i] = krwp->bkvhead_free[i];
 -		krwp->bkvhead_free[i] = NULL;
 -	}
 -
 -	// Channel 3.
 +	spin_lock_irqsave(&krcp->lock, flags);
  	head = krwp->head_free;
  	krwp->head_free = NULL;
 -	raw_spin_unlock_irqrestore(&krcp->lock, flags);
 +	spin_unlock_irqrestore(&krcp->lock, flags);
  
++<<<<<<< HEAD
 +	// List "head" is now private, so traverse locklessly.
++=======
+ 	// Handle two first channels.
+ 	for (i = 0; i < FREE_N_CHANNELS; i++) {
+ 		for (; bkvhead[i]; bkvhead[i] = bnext) {
+ 			bnext = bkvhead[i]->next;
+ 			debug_rcu_bhead_unqueue(bkvhead[i]);
+ 
+ 			rcu_lock_acquire(&rcu_callback_map);
+ 			if (i == 0) { // kmalloc() / kfree().
+ 				trace_rcu_invoke_kfree_bulk_callback(
+ 					rcu_state.name, bkvhead[i]->nr_records,
+ 					bkvhead[i]->records);
+ 
+ 				kfree_bulk(bkvhead[i]->nr_records,
+ 					bkvhead[i]->records);
+ 			} else { // vmalloc() / vfree().
+ 				for (j = 0; j < bkvhead[i]->nr_records; j++) {
+ 					trace_rcu_invoke_kvfree_callback(
+ 						rcu_state.name,
+ 						bkvhead[i]->records[j], 0);
+ 
+ 					vfree(bkvhead[i]->records[j]);
+ 				}
+ 			}
+ 			rcu_lock_release(&rcu_callback_map);
+ 
+ 			raw_spin_lock_irqsave(&krcp->lock, flags);
+ 			if (put_cached_bnode(krcp, bkvhead[i]))
+ 				bkvhead[i] = NULL;
+ 			raw_spin_unlock_irqrestore(&krcp->lock, flags);
+ 
+ 			if (bkvhead[i])
+ 				free_page((unsigned long) bkvhead[i]);
+ 
+ 			cond_resched_tasks_rcu_qs();
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Emergency case only. It can happen under low memory
+ 	 * condition when an allocation gets failed, so the "bulk"
+ 	 * path can not be temporary maintained.
+ 	 */
++>>>>>>> 56292e8609e3 (rcu/tree: Defer kvfree_rcu() allocation to a clean context)
  	for (; head; head = next) {
 -		unsigned long offset = (unsigned long)head->func;
 -		void *ptr = (void *)head - offset;
 -
  		next = head->next;
 -		debug_rcu_head_unqueue((struct rcu_head *)ptr);
 -		rcu_lock_acquire(&rcu_callback_map);
 -		trace_rcu_invoke_kvfree_callback(rcu_state.name, head, offset);
 -
 -		if (!WARN_ON_ONCE(!__is_kvfree_rcu_offset(offset)))
 -			kvfree(ptr);
 -
 -		rcu_lock_release(&rcu_callback_map);
 +		// Potentially optimize with kfree_bulk in future.
 +		debug_rcu_head_unqueue(head);
 +		__rcu_reclaim(rcu_state.name, head);
  		cond_resched_tasks_rcu_qs();
  	}
  }
@@@ -2810,7 -3357,93 +2898,97 @@@ static void kfree_rcu_monitor(struct wo
  	if (krcp->monitor_todo)
  		kfree_rcu_drain_unlock(krcp, flags);
  	else
++<<<<<<< HEAD
 +		spin_unlock_irqrestore(&krcp->lock, flags);
++=======
+ 		raw_spin_unlock_irqrestore(&krcp->lock, flags);
+ }
+ 
+ static enum hrtimer_restart
+ schedule_page_work_fn(struct hrtimer *t)
+ {
+ 	struct kfree_rcu_cpu *krcp =
+ 		container_of(t, struct kfree_rcu_cpu, hrtimer);
+ 
+ 	queue_work(system_highpri_wq, &krcp->page_cache_work);
+ 	return HRTIMER_NORESTART;
+ }
+ 
+ static void fill_page_cache_func(struct work_struct *work)
+ {
+ 	struct kvfree_rcu_bulk_data *bnode;
+ 	struct kfree_rcu_cpu *krcp =
+ 		container_of(work, struct kfree_rcu_cpu,
+ 			page_cache_work);
+ 	unsigned long flags;
+ 	bool pushed;
+ 	int i;
+ 
+ 	for (i = 0; i < rcu_min_cached_objs; i++) {
+ 		bnode = (struct kvfree_rcu_bulk_data *)
+ 			__get_free_page(GFP_KERNEL | __GFP_NOWARN);
+ 
+ 		if (bnode) {
+ 			raw_spin_lock_irqsave(&krcp->lock, flags);
+ 			pushed = put_cached_bnode(krcp, bnode);
+ 			raw_spin_unlock_irqrestore(&krcp->lock, flags);
+ 
+ 			if (!pushed) {
+ 				free_page((unsigned long) bnode);
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	atomic_set(&krcp->work_in_progress, 0);
+ }
+ 
+ static void
+ run_page_cache_worker(struct kfree_rcu_cpu *krcp)
+ {
+ 	if (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&
+ 			!atomic_xchg(&krcp->work_in_progress, 1)) {
+ 		hrtimer_init(&krcp->hrtimer, CLOCK_MONOTONIC,
+ 			HRTIMER_MODE_REL);
+ 		krcp->hrtimer.function = schedule_page_work_fn;
+ 		hrtimer_start(&krcp->hrtimer, 0, HRTIMER_MODE_REL);
+ 	}
+ }
+ 
+ static inline bool
+ kvfree_call_rcu_add_ptr_to_bulk(struct kfree_rcu_cpu *krcp, void *ptr)
+ {
+ 	struct kvfree_rcu_bulk_data *bnode;
+ 	int idx;
+ 
+ 	if (unlikely(!krcp->initialized))
+ 		return false;
+ 
+ 	lockdep_assert_held(&krcp->lock);
+ 	idx = !!is_vmalloc_addr(ptr);
+ 
+ 	/* Check if a new block is required. */
+ 	if (!krcp->bkvhead[idx] ||
+ 			krcp->bkvhead[idx]->nr_records == KVFREE_BULK_MAX_ENTR) {
+ 		bnode = get_cached_bnode(krcp);
+ 		/* Switch to emergency path. */
+ 		if (!bnode)
+ 			return false;
+ 
+ 		/* Initialize the new block. */
+ 		bnode->nr_records = 0;
+ 		bnode->next = krcp->bkvhead[idx];
+ 
+ 		/* Attach it to the head. */
+ 		krcp->bkvhead[idx] = bnode;
+ 	}
+ 
+ 	/* Finally insert. */
+ 	krcp->bkvhead[idx]->records
+ 		[krcp->bkvhead[idx]->nr_records++] = ptr;
+ 
+ 	return true;
++>>>>>>> 56292e8609e3 (rcu/tree: Defer kvfree_rcu() allocation to a clean context)
  }
  
  /*
@@@ -2854,11 -3486,27 +3032,30 @@@ void kfree_call_rcu(struct rcu_head *he
  		// Probable double kfree_rcu(), just leak.
  		WARN_ONCE(1, "%s(): Double-freed call. rcu_head %p\n",
  			  __func__, head);
 -
 -		// Mark as success and leave.
 -		success = true;
  		goto unlock_return;
  	}
++<<<<<<< HEAD
 +	head->func = func;
 +	head->next = krcp->head;
 +	krcp->head = head;
++=======
+ 
+ 	success = kvfree_call_rcu_add_ptr_to_bulk(krcp, ptr);
+ 	if (!success) {
+ 		run_page_cache_worker(krcp);
+ 
+ 		if (head == NULL)
+ 			// Inline if kvfree_rcu(one_arg) call.
+ 			goto unlock_return;
+ 
+ 		head->func = func;
+ 		head->next = krcp->head;
+ 		krcp->head = head;
+ 		success = true;
+ 	}
+ 
+ 	WRITE_ONCE(krcp->count, krcp->count + 1);
++>>>>>>> 56292e8609e3 (rcu/tree: Defer kvfree_rcu() allocation to a clean context)
  
  	// Set timer to drain after KFREE_DRAIN_JIFFIES.
  	if (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&
@@@ -3773,12 -4516,17 +3970,18 @@@ static void __init kfree_rcu_batch_init
  	for_each_possible_cpu(cpu) {
  		struct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);
  
 -		for (i = 0; i < KFREE_N_BATCHES; i++) {
 -			INIT_RCU_WORK(&krcp->krw_arr[i].rcu_work, kfree_rcu_work);
 +		spin_lock_init(&krcp->lock);
 +		for (i = 0; i < KFREE_N_BATCHES; i++)
  			krcp->krw_arr[i].krcp = krcp;
++<<<<<<< HEAD
++=======
+ 		}
+ 
++>>>>>>> 56292e8609e3 (rcu/tree: Defer kvfree_rcu() allocation to a clean context)
  		INIT_DELAYED_WORK(&krcp->monitor_work, kfree_rcu_monitor);
+ 		INIT_WORK(&krcp->page_cache_work, fill_page_cache_func);
  		krcp->initialized = true;
  	}
 -	if (register_shrinker(&kfree_rcu_shrinker))
 -		pr_err("Failed to register kfree_rcu() shrinker!\n");
  }
  
  void __init rcu_init(void)
* Unmerged path kernel/rcu/tree.c
