powerpc/mm: make gup_hugepte() static

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Christophe Leroy <christophe.leroy@c-s.fr>
commit 0001e5aa5c028c11570f2e641f0198287f4808ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/0001e5aa.failed

gup_huge_pd() is the only user of gup_hugepte() and it is
located in the same file. This patch moves gup_huge_pd()
after gup_hugepte() and makes gup_hugepte() static.

	Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 0001e5aa5c028c11570f2e641f0198287f4808ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/pgtable.h
#	arch/powerpc/mm/hugetlbpage.c
diff --cc arch/powerpc/include/asm/pgtable.h
index b03a1ec5ba40,c51846da41a7..000000000000
--- a/arch/powerpc/include/asm/pgtable.h
+++ b/arch/powerpc/include/asm/pgtable.h
@@@ -48,9 -77,18 +48,24 @@@ extern void paging_init(void)
  
  #include <asm-generic/pgtable.h>
  
++<<<<<<< HEAD
 +extern int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,
 +		       unsigned long end, int write,
 +		       struct page **pages, int *nr);
++=======
+ 
+ /*
+  * This gets called at the end of handling a page fault, when
+  * the kernel has put a new PTE into the page table for the process.
+  * We use it to ensure coherency between the i-cache and d-cache
+  * for the page which has just been mapped in.
+  * On machines which use an MMU hash table, we use this to put a
+  * corresponding HPTE into the hash table ahead of time, instead of
+  * waiting for the inevitable extra hash-table miss exception.
+  */
+ extern void update_mmu_cache(struct vm_area_struct *, unsigned long, pte_t *);
+ 
++>>>>>>> 0001e5aa5c02 (powerpc/mm: make gup_hugepte() static)
  #ifndef CONFIG_TRANSPARENT_HUGEPAGE
  #define pmd_large(pmd)		0
  #endif
diff --cc arch/powerpc/mm/hugetlbpage.c
index 2ee76f7b28cf,95cc9f3d97e2..000000000000
--- a/arch/powerpc/mm/hugetlbpage.c
+++ b/arch/powerpc/mm/hugetlbpage.c
@@@ -756,109 -734,8 +739,114 @@@ void flush_dcache_icache_hugepage(struc
  	}
  }
  
++<<<<<<< HEAD
 +#endif /* CONFIG_HUGETLB_PAGE */
 +
 +/*
 + * We have 4 cases for pgds and pmds:
 + * (1) invalid (all zeroes)
 + * (2) pointer to next table, as normal; bottom 6 bits == 0
 + * (3) leaf pte for huge page _PAGE_PTE set
 + * (4) hugepd pointer, _PAGE_PTE = 0 and bits [2..6] indicate size of table
 + *
 + * So long as we atomically load page table pointers we are safe against teardown,
 + * we can follow the address down to the the page and take a ref on it.
 + * This function need to be called with interrupts disabled. We use this variant
 + * when we have MSR[EE] = 0 but the paca->irq_soft_mask = IRQS_ENABLED
 + */
 +pte_t *__find_linux_pte(pgd_t *pgdir, unsigned long ea,
 +			bool *is_thp, unsigned *hpage_shift)
 +{
 +	pgd_t pgd, *pgdp;
 +	pud_t pud, *pudp;
 +	pmd_t pmd, *pmdp;
 +	pte_t *ret_pte;
 +	hugepd_t *hpdp = NULL;
 +	unsigned pdshift = PGDIR_SHIFT;
 +
 +	if (hpage_shift)
 +		*hpage_shift = 0;
 +
 +	if (is_thp)
 +		*is_thp = false;
 +
 +	pgdp = pgdir + pgd_index(ea);
 +	pgd  = READ_ONCE(*pgdp);
 +	/*
 +	 * Always operate on the local stack value. This make sure the
 +	 * value don't get updated by a parallel THP split/collapse,
 +	 * page fault or a page unmap. The return pte_t * is still not
 +	 * stable. So should be checked there for above conditions.
 +	 */
 +	if (pgd_none(pgd))
 +		return NULL;
 +	else if (pgd_huge(pgd)) {
 +		ret_pte = (pte_t *) pgdp;
 +		goto out;
 +	} else if (is_hugepd(__hugepd(pgd_val(pgd))))
 +		hpdp = (hugepd_t *)&pgd;
 +	else {
 +		/*
 +		 * Even if we end up with an unmap, the pgtable will not
 +		 * be freed, because we do an rcu free and here we are
 +		 * irq disabled
 +		 */
 +		pdshift = PUD_SHIFT;
 +		pudp = pud_offset(&pgd, ea);
 +		pud  = READ_ONCE(*pudp);
 +
 +		if (pud_none(pud))
 +			return NULL;
 +		else if (pud_huge(pud)) {
 +			ret_pte = (pte_t *) pudp;
 +			goto out;
 +		} else if (is_hugepd(__hugepd(pud_val(pud))))
 +			hpdp = (hugepd_t *)&pud;
 +		else {
 +			pdshift = PMD_SHIFT;
 +			pmdp = pmd_offset(&pud, ea);
 +			pmd  = READ_ONCE(*pmdp);
 +			/*
 +			 * A hugepage collapse is captured by pmd_none, because
 +			 * it mark the pmd none and do a hpte invalidate.
 +			 */
 +			if (pmd_none(pmd))
 +				return NULL;
 +
 +			if (pmd_trans_huge(pmd) || pmd_devmap(pmd)) {
 +				if (is_thp)
 +					*is_thp = true;
 +				ret_pte = (pte_t *) pmdp;
 +				goto out;
 +			}
 +
 +			if (pmd_huge(pmd)) {
 +				ret_pte = (pte_t *) pmdp;
 +				goto out;
 +			} else if (is_hugepd(__hugepd(pmd_val(pmd))))
 +				hpdp = (hugepd_t *)&pmd;
 +			else
 +				return pte_offset_kernel(&pmd, ea);
 +		}
 +	}
 +	if (!hpdp)
 +		return NULL;
 +
 +	ret_pte = hugepte_offset(*hpdp, ea, pdshift);
 +	pdshift = hugepd_shift(*hpdp);
 +out:
 +	if (hpage_shift)
 +		*hpage_shift = pdshift;
 +	return ret_pte;
 +}
 +EXPORT_SYMBOL_GPL(__find_linux_pte);
 +
 +int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,
 +		unsigned long end, int write, struct page **pages, int *nr)
++=======
+ static int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,
+ 		       unsigned long end, int write, struct page **pages, int *nr)
++>>>>>>> 0001e5aa5c02 (powerpc/mm: make gup_hugepte() static)
  {
  	unsigned long pte_end;
  	struct page *head, *page;
* Unmerged path arch/powerpc/include/asm/pgtable.h
* Unmerged path arch/powerpc/mm/hugetlbpage.c
