KVM: nSVM: rename nested vmcb to vmcb12

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Maxim Levitsky <mlevitsk@redhat.com>
commit 0dd16b5b0c9bd62fbff3ea375cdd5125e19317c6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/0dd16b5b.failed

This is to be more consistient with VMX, and to support
upcoming addition of vmcb02

Hopefully no functional changes.

	Signed-off-by: Maxim Levitsky <mlevitsk@redhat.com>
Message-Id: <20200827171145.374620-3-mlevitsk@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 0dd16b5b0c9bd62fbff3ea375cdd5125e19317c6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/nested.c
diff --cc arch/x86/kvm/svm/nested.c
index 3a222ac541d6,e73bdd44b24a..000000000000
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@@ -215,19 -215,39 +215,49 @@@ static bool nested_vmcb_check_controls(
  	return true;
  }
  
++<<<<<<< HEAD
 +static bool nested_vmcb_checks(struct vmcb *vmcb)
 +{
 +	if ((vmcb->save.efer & EFER_SVME) == 0)
++=======
+ static bool nested_vmcb_checks(struct vcpu_svm *svm, struct vmcb *vmcb12)
+ {
+ 	bool vmcb12_lma;
+ 
+ 	if ((vmcb12->save.efer & EFER_SVME) == 0)
++>>>>>>> 0dd16b5b0c9b (KVM: nSVM: rename nested vmcb to vmcb12)
  		return false;
  
- 	if (((vmcb->save.cr0 & X86_CR0_CD) == 0) &&
- 	    (vmcb->save.cr0 & X86_CR0_NW))
+ 	if (((vmcb12->save.cr0 & X86_CR0_CD) == 0) && (vmcb12->save.cr0 & X86_CR0_NW))
  		return false;
  
- 	if (!kvm_dr6_valid(vmcb->save.dr6) || !kvm_dr7_valid(vmcb->save.dr7))
+ 	if (!kvm_dr6_valid(vmcb12->save.dr6) || !kvm_dr7_valid(vmcb12->save.dr7))
  		return false;
  
++<<<<<<< HEAD
 +	return nested_vmcb_check_controls(&vmcb->control);
++=======
+ 	vmcb12_lma = (vmcb12->save.efer & EFER_LME) && (vmcb12->save.cr0 & X86_CR0_PG);
+ 
+ 	if (!vmcb12_lma) {
+ 		if (vmcb12->save.cr4 & X86_CR4_PAE) {
+ 			if (vmcb12->save.cr3 & MSR_CR3_LEGACY_PAE_RESERVED_MASK)
+ 				return false;
+ 		} else {
+ 			if (vmcb12->save.cr3 & MSR_CR3_LEGACY_RESERVED_MASK)
+ 				return false;
+ 		}
+ 	} else {
+ 		if (!(vmcb12->save.cr4 & X86_CR4_PAE) ||
+ 		    !(vmcb12->save.cr0 & X86_CR0_PE) ||
+ 		    (vmcb12->save.cr3 & MSR_CR3_LONG_RESERVED_MASK))
+ 			return false;
+ 	}
+ 	if (kvm_valid_cr4(&svm->vcpu, vmcb12->save.cr4))
+ 		return false;
+ 
+ 	return nested_vmcb_check_controls(&vmcb12->control);
++>>>>>>> 0dd16b5b0c9b (KVM: nSVM: rename nested vmcb to vmcb12)
  }
  
  static void load_nested_vmcb_control(struct vcpu_svm *svm,
@@@ -449,13 -469,13 +479,21 @@@ int nested_svm_vmrun(struct vcpu_svm *s
  
  	ret = kvm_skip_emulated_instruction(&svm->vcpu);
  
- 	nested_vmcb = map.hva;
+ 	vmcb12 = map.hva;
  
++<<<<<<< HEAD
 +	if (!nested_vmcb_checks(nested_vmcb)) {
 +		nested_vmcb->control.exit_code    = SVM_EXIT_ERR;
 +		nested_vmcb->control.exit_code_hi = 0;
 +		nested_vmcb->control.exit_info_1  = 0;
 +		nested_vmcb->control.exit_info_2  = 0;
++=======
+ 	if (!nested_vmcb_checks(svm, vmcb12)) {
+ 		vmcb12->control.exit_code    = SVM_EXIT_ERR;
+ 		vmcb12->control.exit_code_hi = 0;
+ 		vmcb12->control.exit_info_1  = 0;
+ 		vmcb12->control.exit_info_2  = 0;
++>>>>>>> 0dd16b5b0c9b (KVM: nSVM: rename nested vmcb to vmcb12)
  		goto out;
  	}
  
* Unmerged path arch/x86/kvm/svm/nested.c
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 81a672112dd5..2274429c6fe5 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -1090,7 +1090,7 @@ static void init_vmcb(struct vcpu_svm *svm)
 	}
 	svm->asid_generation = 0;
 
-	svm->nested.vmcb = 0;
+	svm->nested.vmcb12_gpa = 0;
 	svm->vcpu.arch.hflags = 0;
 
 	if (!kvm_pause_in_guest(svm->vcpu.kvm)) {
@@ -3843,7 +3843,7 @@ static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
 		/* FED8h - SVM Guest */
 		put_smstate(u64, smstate, 0x7ed8, 1);
 		/* FEE0h - SVM Guest VMCB Physical Address */
-		put_smstate(u64, smstate, 0x7ee0, svm->nested.vmcb);
+		put_smstate(u64, smstate, 0x7ee0, svm->nested.vmcb12_gpa);
 
 		svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 		svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
@@ -3865,7 +3865,7 @@ static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, const char *smstate)
 	if (guest_cpuid_has(vcpu, X86_FEATURE_LM)) {
 		u64 saved_efer = GET_SMSTATE(u64, smstate, 0x7ed0);
 		u64 guest = GET_SMSTATE(u64, smstate, 0x7ed8);
-		u64 vmcb = GET_SMSTATE(u64, smstate, 0x7ee0);
+		u64 vmcb12_gpa = GET_SMSTATE(u64, smstate, 0x7ee0);
 
 		if (guest) {
 			if (!guest_cpuid_has(vcpu, X86_FEATURE_SVM))
@@ -3875,10 +3875,10 @@ static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, const char *smstate)
 				return 1;
 
 			if (kvm_vcpu_map(&svm->vcpu,
-					 gpa_to_gfn(vmcb), &map) == -EINVAL)
+					 gpa_to_gfn(vmcb12_gpa), &map) == -EINVAL)
 				return 1;
 
-			ret = enter_svm_guest_mode(svm, vmcb, map.hva);
+			ret = enter_svm_guest_mode(svm, vmcb12_gpa, map.hva);
 			kvm_vcpu_unmap(&svm->vcpu, &map, true);
 		}
 	}
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index 7e2440454c08..7e23c63a51db 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -85,7 +85,7 @@ struct svm_nested_state {
 	struct vmcb *hsave;
 	u64 hsave_msr;
 	u64 vm_cr_msr;
-	u64 vmcb;
+	u64 vmcb12_gpa;
 	u32 host_intercept_exceptions;
 
 	/* These are the merged vectors */
