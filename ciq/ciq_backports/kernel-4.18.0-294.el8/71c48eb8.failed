tcp: add tcp_sock_set_keepidle

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 71c48eb81c9ecb6fed49dc33e7c9b621fdcb7bf8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/71c48eb8.failed

Add a helper to directly set the TCP_KEEP_IDLE sockopt from kernel
space without going through a fake uaccess.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 71c48eb81c9ecb6fed49dc33e7c9b621fdcb7bf8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/tcp.h
#	net/ipv4/tcp.c
#	net/sunrpc/xprtsock.c
diff --cc include/linux/tcp.h
index 723bf168e2c0,5724dd84a85e..000000000000
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@@ -495,4 -497,11 +495,14 @@@ static inline u16 tcp_mss_clamp(const s
  int tcp_skb_shift(struct sk_buff *to, struct sk_buff *from, int pcount,
  		  int shiftlen);
  
++<<<<<<< HEAD
++=======
+ void tcp_sock_set_cork(struct sock *sk, bool on);
+ int tcp_sock_set_keepidle(struct sock *sk, int val);
+ void tcp_sock_set_nodelay(struct sock *sk);
+ void tcp_sock_set_quickack(struct sock *sk, int val);
+ int tcp_sock_set_syncnt(struct sock *sk, int val);
+ void tcp_sock_set_user_timeout(struct sock *sk, u32 val);
+ 
++>>>>>>> 71c48eb81c9e (tcp: add tcp_sock_set_keepidle)
  #endif	/* _LINUX_TCP_H */
diff --cc net/ipv4/tcp.c
index 8f4b529b1711,bdf0ff933351..000000000000
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@@ -2746,6 -2786,154 +2746,157 @@@ static int tcp_repair_options_est(struc
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ DEFINE_STATIC_KEY_FALSE(tcp_tx_delay_enabled);
+ EXPORT_SYMBOL(tcp_tx_delay_enabled);
+ 
+ static void tcp_enable_tx_delay(void)
+ {
+ 	if (!static_branch_unlikely(&tcp_tx_delay_enabled)) {
+ 		static int __tcp_tx_delay_enabled = 0;
+ 
+ 		if (cmpxchg(&__tcp_tx_delay_enabled, 0, 1) == 0) {
+ 			static_branch_enable(&tcp_tx_delay_enabled);
+ 			pr_info("TCP_TX_DELAY enabled\n");
+ 		}
+ 	}
+ }
+ 
+ /* When set indicates to always queue non-full frames.  Later the user clears
+  * this option and we transmit any pending partial frames in the queue.  This is
+  * meant to be used alongside sendfile() to get properly filled frames when the
+  * user (for example) must write out headers with a write() call first and then
+  * use sendfile to send out the data parts.
+  *
+  * TCP_CORK can be set together with TCP_NODELAY and it is stronger than
+  * TCP_NODELAY.
+  */
+ static void __tcp_sock_set_cork(struct sock *sk, bool on)
+ {
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 
+ 	if (on) {
+ 		tp->nonagle |= TCP_NAGLE_CORK;
+ 	} else {
+ 		tp->nonagle &= ~TCP_NAGLE_CORK;
+ 		if (tp->nonagle & TCP_NAGLE_OFF)
+ 			tp->nonagle |= TCP_NAGLE_PUSH;
+ 		tcp_push_pending_frames(sk);
+ 	}
+ }
+ 
+ void tcp_sock_set_cork(struct sock *sk, bool on)
+ {
+ 	lock_sock(sk);
+ 	__tcp_sock_set_cork(sk, on);
+ 	release_sock(sk);
+ }
+ EXPORT_SYMBOL(tcp_sock_set_cork);
+ 
+ /* TCP_NODELAY is weaker than TCP_CORK, so that this option on corked socket is
+  * remembered, but it is not activated until cork is cleared.
+  *
+  * However, when TCP_NODELAY is set we make an explicit push, which overrides
+  * even TCP_CORK for currently queued segments.
+  */
+ static void __tcp_sock_set_nodelay(struct sock *sk, bool on)
+ {
+ 	if (on) {
+ 		tcp_sk(sk)->nonagle |= TCP_NAGLE_OFF|TCP_NAGLE_PUSH;
+ 		tcp_push_pending_frames(sk);
+ 	} else {
+ 		tcp_sk(sk)->nonagle &= ~TCP_NAGLE_OFF;
+ 	}
+ }
+ 
+ void tcp_sock_set_nodelay(struct sock *sk)
+ {
+ 	lock_sock(sk);
+ 	__tcp_sock_set_nodelay(sk, true);
+ 	release_sock(sk);
+ }
+ EXPORT_SYMBOL(tcp_sock_set_nodelay);
+ 
+ static void __tcp_sock_set_quickack(struct sock *sk, int val)
+ {
+ 	if (!val) {
+ 		inet_csk_enter_pingpong_mode(sk);
+ 		return;
+ 	}
+ 
+ 	inet_csk_exit_pingpong_mode(sk);
+ 	if ((1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT) &&
+ 	    inet_csk_ack_scheduled(sk)) {
+ 		inet_csk(sk)->icsk_ack.pending |= ICSK_ACK_PUSHED;
+ 		tcp_cleanup_rbuf(sk, 1);
+ 		if (!(val & 1))
+ 			inet_csk_enter_pingpong_mode(sk);
+ 	}
+ }
+ 
+ void tcp_sock_set_quickack(struct sock *sk, int val)
+ {
+ 	lock_sock(sk);
+ 	__tcp_sock_set_quickack(sk, val);
+ 	release_sock(sk);
+ }
+ EXPORT_SYMBOL(tcp_sock_set_quickack);
+ 
+ int tcp_sock_set_syncnt(struct sock *sk, int val)
+ {
+ 	if (val < 1 || val > MAX_TCP_SYNCNT)
+ 		return -EINVAL;
+ 
+ 	lock_sock(sk);
+ 	inet_csk(sk)->icsk_syn_retries = val;
+ 	release_sock(sk);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tcp_sock_set_syncnt);
+ 
+ void tcp_sock_set_user_timeout(struct sock *sk, u32 val)
+ {
+ 	lock_sock(sk);
+ 	inet_csk(sk)->icsk_user_timeout = val;
+ 	release_sock(sk);
+ }
+ EXPORT_SYMBOL(tcp_sock_set_user_timeout);
+ 
+ static int __tcp_sock_set_keepidle(struct sock *sk, int val)
+ {
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 
+ 	if (val < 1 || val > MAX_TCP_KEEPIDLE)
+ 		return -EINVAL;
+ 
+ 	tp->keepalive_time = val * HZ;
+ 	if (sock_flag(sk, SOCK_KEEPOPEN) &&
+ 	    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) {
+ 		u32 elapsed = keepalive_time_elapsed(tp);
+ 
+ 		if (tp->keepalive_time > elapsed)
+ 			elapsed = tp->keepalive_time - elapsed;
+ 		else
+ 			elapsed = 0;
+ 		inet_csk_reset_keepalive_timer(sk, elapsed);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int tcp_sock_set_keepidle(struct sock *sk, int val)
+ {
+ 	int err;
+ 
+ 	lock_sock(sk);
+ 	err = __tcp_sock_set_keepidle(sk, val);
+ 	release_sock(sk);
+ 	return err;
+ }
+ EXPORT_SYMBOL(tcp_sock_set_keepidle);
+ 
++>>>>>>> 71c48eb81c9e (tcp: add tcp_sock_set_keepidle)
  /*
   *	Socket option code for TCP.
   */
diff --cc net/sunrpc/xprtsock.c
index 839c49330785,473290f7c5c0..000000000000
--- a/net/sunrpc/xprtsock.c
+++ b/net/sunrpc/xprtsock.c
@@@ -2122,10 -2106,8 +2122,15 @@@ static void xs_tcp_set_socket_timeouts(
  	spin_unlock(&xprt->transport_lock);
  
  	/* TCP Keepalive options */
++<<<<<<< HEAD
 +	kernel_setsockopt(sock, SOL_SOCKET, SO_KEEPALIVE,
 +			(char *)&opt_on, sizeof(opt_on));
 +	kernel_setsockopt(sock, SOL_TCP, TCP_KEEPIDLE,
 +			(char *)&keepidle, sizeof(keepidle));
++=======
+ 	sock_set_keepalive(sock->sk);
+ 	tcp_sock_set_keepidle(sock->sk, keepidle);
++>>>>>>> 71c48eb81c9e (tcp: add tcp_sock_set_keepidle)
  	kernel_setsockopt(sock, SOL_TCP, TCP_KEEPINTVL,
  			(char *)&keepidle, sizeof(keepidle));
  	kernel_setsockopt(sock, SOL_TCP, TCP_KEEPCNT,
* Unmerged path include/linux/tcp.h
* Unmerged path net/ipv4/tcp.c
diff --git a/net/rds/tcp_listen.c b/net/rds/tcp_listen.c
index 27d79afe25f7..277e33a5fb04 100644
--- a/net/rds/tcp_listen.c
+++ b/net/rds/tcp_listen.c
@@ -56,10 +56,7 @@ int rds_tcp_keepalive(struct socket *sock)
 	if (ret < 0)
 		goto bail;
 
-	ret = kernel_setsockopt(sock, IPPROTO_TCP, TCP_KEEPIDLE,
-				(char *)&keepidle, sizeof(keepidle));
-	if (ret < 0)
-		goto bail;
+	tcp_sock_set_keepidle(sock->sk, keepidle);
 
 	/* KEEPINTVL is the interval between successive probes. We follow
 	 * the model in xs_tcp_finish_connecting() and re-use keepidle.
* Unmerged path net/sunrpc/xprtsock.c
