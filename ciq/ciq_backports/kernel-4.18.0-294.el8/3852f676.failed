mm/swapcache: support to handle the shadow entries

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit 3852f6768ede542ed48b9077bedf482c7ecb6327
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/3852f676.failed

Workingset detection for anonymous page will be implemented in the
following patch and it requires to store the shadow entries into the
swapcache.  This patch implements an infrastructure to store the shadow
entry in the swapcache.

	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Vlastimil Babka <vbabka@suse.cz>
Link: http://lkml.kernel.org/r/1595490560-15117-5-git-send-email-iamjoonsoo.kim@lge.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3852f6768ede542ed48b9077bedf482c7ecb6327)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap_state.c
diff --cc mm/swap_state.c
index e2aded84261e,a29b33c7c236..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -107,14 -107,17 +107,25 @@@ void show_swap_cache_info(void
  }
  
  /*
 - * add_to_swap_cache resembles add_to_page_cache_locked on swapper_space,
 + * __add_to_swap_cache resembles add_to_page_cache_locked on swapper_space,
   * but sets SwapCache flag and private instead of mapping and index.
   */
++<<<<<<< HEAD
 +int __add_to_swap_cache(struct page *page, swp_entry_t entry)
++=======
+ int add_to_swap_cache(struct page *page, swp_entry_t entry,
+ 			gfp_t gfp, void **shadowp)
++>>>>>>> 3852f6768ede (mm/swapcache: support to handle the shadow entries)
  {
 -	struct address_space *address_space = swap_address_space(entry);
 +	int error, i, nr = hpage_nr_pages(page);
 +	struct address_space *address_space;
  	pgoff_t idx = swp_offset(entry);
++<<<<<<< HEAD
++=======
+ 	XA_STATE_ORDER(xas, &address_space->i_pages, idx, compound_order(page));
+ 	unsigned long i, nr = hpage_nr_pages(page);
+ 	void *old;
++>>>>>>> 3852f6768ede (mm/swapcache: support to handle the shadow entries)
  
  	VM_BUG_ON_PAGE(!PageLocked(page), page);
  	VM_BUG_ON_PAGE(PageSwapCache(page), page);
@@@ -123,16 -126,26 +134,39 @@@
  	page_ref_add(page, nr);
  	SetPageSwapCache(page);
  
++<<<<<<< HEAD
 +	address_space = swap_address_space(entry);
 +	xa_lock_irq(&address_space->i_pages);
 +	for (i = 0; i < nr; i++) {
 +		set_page_private(page + i, entry.val + i);
 +		error = radix_tree_insert(&address_space->i_pages,
 +					  idx + i, page + i);
 +		if (unlikely(error))
 +			break;
 +	}
 +	if (likely(!error)) {
++=======
+ 	do {
+ 		unsigned long nr_shadows = 0;
+ 
+ 		xas_lock_irq(&xas);
+ 		xas_create_range(&xas);
+ 		if (xas_error(&xas))
+ 			goto unlock;
+ 		for (i = 0; i < nr; i++) {
+ 			VM_BUG_ON_PAGE(xas.xa_index != idx + i, page);
+ 			old = xas_load(&xas);
+ 			if (xa_is_value(old)) {
+ 				nr_shadows++;
+ 				if (shadowp)
+ 					*shadowp = old;
+ 			}
+ 			set_page_private(page + i, entry.val + i);
+ 			xas_store(&xas, page);
+ 			xas_next(&xas);
+ 		}
+ 		address_space->nrexceptional -= nr_shadows;
++>>>>>>> 3852f6768ede (mm/swapcache: support to handle the shadow entries)
  		address_space->nrpages += nr;
  		__mod_node_page_state(page_pgdat(page), NR_FILE_PAGES, nr);
  		ADD_CACHE_INFO(add_total, nr);
@@@ -185,8 -178,8 +220,13 @@@ void __delete_from_swap_cache(struct pa
  	VM_BUG_ON_PAGE(PageWriteback(page), page);
  
  	for (i = 0; i < nr; i++) {
++<<<<<<< HEAD
 +		void *entry = xas_store(&xas, NULL);
 +		VM_BUG_ON_PAGE(entry != page + i, entry);
++=======
+ 		void *entry = xas_store(&xas, shadow);
+ 		VM_BUG_ON_PAGE(entry != page, entry);
++>>>>>>> 3852f6768ede (mm/swapcache: support to handle the shadow entries)
  		set_page_private(page + i, 0);
  		xas_next(&xas);
  	}
@@@ -227,8 -222,7 +269,12 @@@ int add_to_swap(struct page *page
  	 * Add it to the swap cache.
  	 */
  	err = add_to_swap_cache(page, entry,
++<<<<<<< HEAD
 +			__GFP_HIGH|__GFP_NOMEMALLOC|__GFP_NOWARN);
 +	/* -ENOMEM radix-tree allocation failure */
++=======
+ 			__GFP_HIGH|__GFP_NOMEMALLOC|__GFP_NOWARN, NULL);
++>>>>>>> 3852f6768ede (mm/swapcache: support to handle the shadow entries)
  	if (err)
  		/*
  		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
@@@ -430,47 -449,56 +507,83 @@@ struct page *__read_swap_cache_async(sw
  		 * Swap entry may have been freed since our caller observed it.
  		 */
  		err = swapcache_prepare(entry);
 -		if (!err)
 +		if (err == -EEXIST) {
 +			radix_tree_preload_end();
 +			/*
 +			 * We might race against get_swap_page() and stumble
 +			 * across a SWAP_HAS_CACHE swap_map entry whose page
 +			 * has not been brought into the swapcache yet.
 +			 */
 +			cond_resched();
 +			continue;
 +		}
 +		if (err) {		/* swp entry is obsolete ? */
 +			radix_tree_preload_end();
  			break;
 +		}
  
 -		put_page(page);
 -		if (err != -EEXIST)
 -			return NULL;
 -
 +		/* May fail (-ENOMEM) if radix-tree node allocation failed. */
 +		__SetPageLocked(new_page);
 +		__SetPageSwapBacked(new_page);
 +		err = __add_to_swap_cache(new_page, entry);
 +		if (likely(!err)) {
 +			radix_tree_preload_end();
 +			/*
 +			 * Initiate read into locked page and return.
 +			 */
 +			SetPageWorkingset(new_page);
 +			lru_cache_add_anon(new_page);
 +			*new_page_allocated = true;
 +			return new_page;
 +		}
 +		radix_tree_preload_end();
 +		__ClearPageLocked(new_page);
  		/*
 -		 * We might race against __delete_from_swap_cache(), and
 -		 * stumble across a swap_map entry whose SWAP_HAS_CACHE
 -		 * has not yet been cleared.  Or race against another
 -		 * __read_swap_cache_async(), which has set SWAP_HAS_CACHE
 -		 * in swap_map, but not yet added its page to swap cache.
 +		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
 +		 * clear SWAP_HAS_CACHE flag.
  		 */
 -		cond_resched();
 -	}
 -
 +		put_swap_page(new_page, entry);
 +	} while (err != -ENOMEM);
 +
++<<<<<<< HEAD
 +	if (new_page)
 +		put_page(new_page);
 +	return found_page;
++=======
+ 	/*
+ 	 * The swap entry is ours to swap in. Prepare the new page.
+ 	 */
+ 
+ 	__SetPageLocked(page);
+ 	__SetPageSwapBacked(page);
+ 
+ 	/* May fail (-ENOMEM) if XArray node allocation failed. */
+ 	if (add_to_swap_cache(page, entry, gfp_mask & GFP_RECLAIM_MASK, NULL)) {
+ 		put_swap_page(page, entry);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	if (mem_cgroup_charge(page, NULL, gfp_mask)) {
+ 		delete_from_swap_cache(page);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	/* XXX: Move to lru_cache_add() when it supports new vs putback */
+ 	spin_lock_irq(&page_pgdat(page)->lru_lock);
+ 	lru_note_cost_page(page);
+ 	spin_unlock_irq(&page_pgdat(page)->lru_lock);
+ 
+ 	/* Caller will initiate read into locked page */
+ 	SetPageWorkingset(page);
+ 	lru_cache_add(page);
+ 	*new_page_allocated = true;
+ 	return page;
+ 
+ fail_unlock:
+ 	unlock_page(page);
+ 	put_page(page);
+ 	return NULL;
++>>>>>>> 3852f6768ede (mm/swapcache: support to handle the shadow entries)
  }
  
  /*
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 826d775d2a0d..b207b1454028 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -411,9 +411,13 @@ extern struct address_space *swapper_spaces[];
 extern unsigned long total_swapcache_pages(void);
 extern void show_swap_cache_info(void);
 extern int add_to_swap(struct page *page);
-extern int add_to_swap_cache(struct page *, swp_entry_t, gfp_t);
-extern void __delete_from_swap_cache(struct page *, swp_entry_t entry);
+extern int add_to_swap_cache(struct page *page, swp_entry_t entry,
+			gfp_t gfp, void **shadowp);
+extern void __delete_from_swap_cache(struct page *page,
+			swp_entry_t entry, void *shadow);
 extern void delete_from_swap_cache(struct page *);
+extern void clear_shadow_from_swap_cache(int type, unsigned long begin,
+				unsigned long end);
 extern void free_page_and_swap_cache(struct page *);
 extern void free_pages_and_swap_cache(struct page **, int);
 extern struct page *lookup_swap_cache(swp_entry_t entry,
@@ -567,13 +571,13 @@ static inline int add_to_swap(struct page *page)
 }
 
 static inline int add_to_swap_cache(struct page *page, swp_entry_t entry,
-							gfp_t gfp_mask)
+					gfp_t gfp_mask, void **shadowp)
 {
 	return -1;
 }
 
 static inline void __delete_from_swap_cache(struct page *page,
-							swp_entry_t entry)
+					swp_entry_t entry, void *shadow)
 {
 }
 
@@ -581,6 +585,11 @@ static inline void delete_from_swap_cache(struct page *page)
 {
 }
 
+static inline void clear_shadow_from_swap_cache(int type, unsigned long begin,
+				unsigned long end)
+{
+}
+
 static inline int page_swapcount(struct page *page)
 {
 	return 0;
diff --git a/mm/shmem.c b/mm/shmem.c
index 1b4f8fc6c563..b08851c4dbb4 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1353,7 +1353,8 @@ static int shmem_writepage(struct page *page, struct writeback_control *wbc)
 		list_add_tail(&info->swaplist, &shmem_swaplist);
 
 	if (add_to_swap_cache(page, swap,
-			__GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN) == 0) {
+			__GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN,
+			NULL) == 0) {
 		spin_lock_irq(&info->lock);
 		shmem_recalc_inode(inode);
 		info->swapped++;
* Unmerged path mm/swap_state.c
diff --git a/mm/swapfile.c b/mm/swapfile.c
index b77fb155eaf4..d803d6326cc8 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -650,6 +650,7 @@ static void add_to_avail_list(struct swap_info_struct *p)
 static void swap_range_free(struct swap_info_struct *si, unsigned long offset,
 			    unsigned int nr_entries)
 {
+	unsigned long begin = offset;
 	unsigned long end = offset + nr_entries - 1;
 	void (*swap_slot_free_notify)(struct block_device *, unsigned long);
 
@@ -675,6 +676,7 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,
 			swap_slot_free_notify(si->bdev, offset);
 		offset++;
 	}
+	clear_shadow_from_swap_cache(si->type, begin, end);
 }
 
 static void set_cluster_next(struct swap_info_struct *si, unsigned long next)
diff --git a/mm/vmscan.c b/mm/vmscan.c
index d33ed25f0511..50980d95929e 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -905,7 +905,7 @@ static int __remove_mapping(struct address_space *mapping, struct page *page,
 	if (PageSwapCache(page)) {
 		swp_entry_t swap = { .val = page_private(page) };
 		mem_cgroup_swapout(page, swap);
-		__delete_from_swap_cache(page, swap);
+		__delete_from_swap_cache(page, swap, NULL);
 		xa_unlock_irqrestore(&mapping->i_pages, flags);
 		put_swap_page(page, swap);
 	} else {
