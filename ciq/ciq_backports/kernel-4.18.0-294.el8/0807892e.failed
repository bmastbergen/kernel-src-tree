xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Björn Töpel <bjorn.topel@intel.com>
commit 0807892ecb35734b7ce6f7c29b078f1b60151c94
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/0807892e.failed

There are no users of MEM_TYPE_ZERO_COPY. Remove all corresponding
code, including the "handle" member of struct xdp_buff.

rfc->v1: Fixed spelling in commit message. (Björn)

	Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20200520192103.355233-13-bjorn.topel@gmail.com
(cherry picked from commit 0807892ecb35734b7ce6f7c29b078f1b60151c94)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/xdp.h
#	include/net/xdp_sock.h
#	include/net/xdp_sock_drv.h
#	include/trace/events/xdp.h
#	net/core/xdp.c
#	net/xdp/xdp_umem.c
#	net/xdp/xsk.c
#	net/xdp/xsk_buff_pool.c
#	net/xdp/xsk_queue.h
diff --cc include/net/xdp.h
index 058b0ba6d8b8,90f11760bd12..000000000000
--- a/include/net/xdp.h
+++ b/include/net/xdp.h
@@@ -39,7 -39,7 +39,11 @@@ enum xdp_mem_type 
  	MEM_TYPE_PAGE_SHARED = 0, /* Split-page refcnt based model */
  	MEM_TYPE_PAGE_ORDER0,     /* Orig XDP full page model */
  	MEM_TYPE_PAGE_POOL,
++<<<<<<< HEAD
 +	MEM_TYPE_ZERO_COPY,
++=======
+ 	MEM_TYPE_XSK_BUFF_POOL,
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  	MEM_TYPE_MAX,
  };
  
@@@ -84,10 -66,20 +84,9 @@@ struct xdp_buff 
  	void *data_end;
  	void *data_meta;
  	void *data_hard_start;
- 	unsigned long handle;
  	struct xdp_rxq_info *rxq;
 -	u32 frame_sz; /* frame size to deduce data_hard_end/reserved tailroom*/
  };
  
 -/* Reserve memory area at end-of data area.
 - *
 - * This macro reserves tailroom in the XDP buffer by limiting the
 - * XDP/BPF data access to data_hard_end.  Notice same area (and size)
 - * is used for XDP_PASS, when constructing the SKB via build_skb().
 - */
 -#define xdp_data_hard_end(xdp)				\
 -	((xdp)->data_hard_start + (xdp)->frame_sz -	\
 -	 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
 -
  struct xdp_frame {
  	void *data;
  	u16 len;
@@@ -122,7 -114,7 +121,11 @@@ struct xdp_frame *convert_to_xdp_frame(
  	int metasize;
  	int headroom;
  
++<<<<<<< HEAD
 +	if (xdp->rxq->mem.type == MEM_TYPE_ZERO_COPY)
++=======
+ 	if (xdp->rxq->mem.type == MEM_TYPE_XSK_BUFF_POOL)
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  		return xdp_convert_zc_to_xdp_frame(xdp);
  
  	/* Assure headroom is available for storing info */
diff --cc include/net/xdp_sock.h
index c57bb0fdba67,96bfc5f5f24e..000000000000
--- a/include/net/xdp_sock.h
+++ b/include/net/xdp_sock.h
@@@ -15,28 -15,16 +15,24 @@@
  
  struct net_device;
  struct xsk_queue;
 -struct xdp_buff;
  
- struct xdp_umem_page {
- 	void *addr;
- 	dma_addr_t dma;
- };
- 
- struct xdp_umem_fq_reuse {
- 	u32 nentries;
- 	u32 length;
- 	u64 handles[];
- };
- 
  struct xdp_umem {
  	struct xsk_queue *fq;
  	struct xsk_queue *cq;
++<<<<<<< HEAD
 +	struct xdp_umem_page *pages;
 +	u64 chunk_mask;
 +	u64 size;
 +	u32 headroom;
 +	u32 chunk_size_nohr;
++=======
+ 	struct xsk_buff_pool *pool;
+ 	u64 size;
+ 	u32 headroom;
+ 	u32 chunk_size;
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  	struct user_struct *user;
 +	unsigned long address;
  	refcount_t users;
  	struct work_struct work;
  	struct page **pgs;
@@@ -140,230 -94,13 +135,235 @@@ static inline struct xdp_sock *__xsk_ma
  	return xs;
  }
  
++<<<<<<< HEAD
 +static inline u64 xsk_umem_extract_addr(u64 addr)
 +{
 +	return addr & XSK_UNALIGNED_BUF_ADDR_MASK;
 +}
 +
 +static inline u64 xsk_umem_extract_offset(u64 addr)
 +{
 +	return addr >> XSK_UNALIGNED_BUF_OFFSET_SHIFT;
 +}
 +
 +static inline u64 xsk_umem_add_offset_to_addr(u64 addr)
 +{
 +	return xsk_umem_extract_addr(addr) + xsk_umem_extract_offset(addr);
 +}
 +
 +static inline char *xdp_umem_get_data(struct xdp_umem *umem, u64 addr)
 +{
 +	unsigned long page_addr;
++=======
+ #else
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
 +
 +	addr = xsk_umem_add_offset_to_addr(addr);
 +	page_addr = (unsigned long)umem->pages[addr >> PAGE_SHIFT].addr;
 +
 +	return (char *)(page_addr & PAGE_MASK) + (addr & ~PAGE_MASK);
 +}
 +
 +static inline dma_addr_t xdp_umem_get_dma(struct xdp_umem *umem, u64 addr)
 +{
 +	addr = xsk_umem_add_offset_to_addr(addr);
 +
 +	return umem->pages[addr >> PAGE_SHIFT].dma + (addr & ~PAGE_MASK);
 +}
 +
 +/* Reuse-queue aware version of FILL queue helpers */
 +static inline bool xsk_umem_has_addrs_rq(struct xdp_umem *umem, u32 cnt)
 +{
 +	struct xdp_umem_fq_reuse *rq = umem->fq_reuse;
 +
 +	if (rq->length >= cnt)
 +		return true;
 +
 +	return xsk_umem_has_addrs(umem, cnt - rq->length);
 +}
 +
 +static inline bool xsk_umem_peek_addr_rq(struct xdp_umem *umem, u64 *addr)
 +{
 +	struct xdp_umem_fq_reuse *rq = umem->fq_reuse;
 +
 +	if (!rq->length)
 +		return xsk_umem_peek_addr(umem, addr);
  
 +	*addr = rq->handles[rq->length - 1];
 +	return addr;
 +}
 +
 +static inline void xsk_umem_release_addr_rq(struct xdp_umem *umem)
 +{
 +	struct xdp_umem_fq_reuse *rq = umem->fq_reuse;
 +
 +	if (!rq->length)
 +		xsk_umem_release_addr(umem);
 +	else
 +		rq->length--;
 +}
 +
 +static inline void xsk_umem_fq_reuse(struct xdp_umem *umem, u64 addr)
 +{
 +	struct xdp_umem_fq_reuse *rq = umem->fq_reuse;
 +
 +	rq->handles[rq->length++] = addr;
 +}
 +
 +/* Handle the offset appropriately depending on aligned or unaligned mode.
 + * For unaligned mode, we store the offset in the upper 16-bits of the address.
 + * For aligned mode, we simply add the offset to the address.
 + */
 +static inline u64 xsk_umem_adjust_offset(struct xdp_umem *umem, u64 address,
 +					 u64 offset)
 +{
 +	if (umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG)
 +		return address + (offset << XSK_UNALIGNED_BUF_OFFSET_SHIFT);
 +	else
 +		return address + offset;
 +}
 +
 +static inline u32 xsk_umem_xdp_frame_sz(struct xdp_umem *umem)
 +{
 +	return umem->chunk_size_nohr;
 +}
 +
 +#else
  static inline int xsk_generic_rcv(struct xdp_sock *xs, struct xdp_buff *xdp)
  {
  	return -ENOTSUPP;
  }
  
 +static inline bool xsk_is_setup_for_bpf_map(struct xdp_sock *xs)
 +{
 +	return false;
 +}
 +
 +static inline bool xsk_umem_has_addrs(struct xdp_umem *umem, u32 cnt)
 +{
 +	return false;
 +}
 +
 +static inline u64 *xsk_umem_peek_addr(struct xdp_umem *umem, u64 *addr)
 +{
 +	return NULL;
 +}
 +
 +static inline void xsk_umem_release_addr(struct xdp_umem *umem)
 +{
 +}
 +
 +static inline void xsk_umem_complete_tx(struct xdp_umem *umem, u32 nb_entries)
 +{
 +}
 +
 +static inline bool xsk_umem_consume_tx(struct xdp_umem *umem,
 +				       struct xdp_desc *desc)
 +{
 +	return false;
 +}
 +
 +static inline void xsk_umem_consume_tx_done(struct xdp_umem *umem)
 +{
 +}
 +
 +static inline struct xdp_umem_fq_reuse *xsk_reuseq_prepare(u32 nentries)
 +{
 +	return NULL;
 +}
 +
 +static inline struct xdp_umem_fq_reuse *xsk_reuseq_swap(
 +	struct xdp_umem *umem,
 +	struct xdp_umem_fq_reuse *newq)
 +{
 +	return NULL;
 +}
 +static inline void xsk_reuseq_free(struct xdp_umem_fq_reuse *rq)
 +{
 +}
 +
 +static inline struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
 +						     u16 queue_id)
 +{
 +	return NULL;
 +}
 +
++<<<<<<< HEAD
 +static inline u64 xsk_umem_extract_addr(u64 addr)
 +{
 +	return 0;
 +}
 +
 +static inline u64 xsk_umem_extract_offset(u64 addr)
 +{
 +	return 0;
 +}
 +
 +static inline u64 xsk_umem_add_offset_to_addr(u64 addr)
 +{
 +	return 0;
 +}
 +
 +static inline char *xdp_umem_get_data(struct xdp_umem *umem, u64 addr)
 +{
 +	return NULL;
 +}
 +
 +static inline dma_addr_t xdp_umem_get_dma(struct xdp_umem *umem, u64 addr)
 +{
 +	return 0;
 +}
 +
 +static inline bool xsk_umem_has_addrs_rq(struct xdp_umem *umem, u32 cnt)
 +{
 +	return false;
 +}
 +
 +static inline u64 *xsk_umem_peek_addr_rq(struct xdp_umem *umem, u64 *addr)
 +{
 +	return NULL;
 +}
 +
 +static inline void xsk_umem_release_addr_rq(struct xdp_umem *umem)
 +{
 +}
 +
 +static inline void xsk_umem_fq_reuse(struct xdp_umem *umem, u64 addr)
 +{
 +}
 +
 +static inline void xsk_set_rx_need_wakeup(struct xdp_umem *umem)
 +{
 +}
 +
 +static inline void xsk_set_tx_need_wakeup(struct xdp_umem *umem)
 +{
 +}
 +
 +static inline void xsk_clear_rx_need_wakeup(struct xdp_umem *umem)
 +{
 +}
 +
 +static inline void xsk_clear_tx_need_wakeup(struct xdp_umem *umem)
 +{
 +}
 +
 +static inline bool xsk_umem_uses_need_wakeup(struct xdp_umem *umem)
 +{
 +	return false;
 +}
 +
 +static inline u64 xsk_umem_adjust_offset(struct xdp_umem *umem, u64 handle,
 +					 u64 offset)
 +{
 +	return 0;
 +}
 +
 +static inline u32 xsk_umem_xdp_frame_sz(struct xdp_umem *umem)
 +{
 +	return 0;
 +}
 +
  static inline int __xsk_map_redirect(struct xdp_sock *xs, struct xdp_buff *xdp)
  {
  	return -EOPNOTSUPP;
@@@ -378,6 -115,7 +378,8 @@@ static inline struct xdp_sock *__xsk_ma
  {
  	return NULL;
  }
 -
++=======
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  #endif /* CONFIG_XDP_SOCKETS */
  
  #endif /* _LINUX_XDP_SOCK_H */
diff --cc include/trace/events/xdp.h
index b95d65e8c628,b73d3e141323..000000000000
--- a/include/trace/events/xdp.h
+++ b/include/trace/events/xdp.h
@@@ -287,7 -287,7 +287,11 @@@ TRACE_EVENT(xdp_devmap_xmit
  	FN(PAGE_SHARED)		\
  	FN(PAGE_ORDER0)		\
  	FN(PAGE_POOL)		\
++<<<<<<< HEAD
 +	FN(ZERO_COPY)
++=======
+ 	FN(XSK_BUFF_POOL)
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  
  #define __MEM_TYPE_TP_FN(x)	\
  	TRACE_DEFINE_ENUM(MEM_TYPE_##x);
diff --cc net/core/xdp.c
index 81089ebb85ca,a8c2f243367d..000000000000
--- a/net/core/xdp.c
+++ b/net/core/xdp.c
@@@ -361,7 -338,7 +337,11 @@@ EXPORT_SYMBOL_GPL(xdp_rxq_info_reg_mem_
   * of xdp_frames/pages in those cases.
   */
  static void __xdp_return(void *data, struct xdp_mem_info *mem, bool napi_direct,
++<<<<<<< HEAD
 +			 unsigned long handle)
++=======
+ 			 struct xdp_buff *xdp)
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  {
  	struct xdp_mem_allocator *xa;
  	struct page *page;
@@@ -383,13 -360,10 +363,20 @@@
  		page = virt_to_page(data); /* Assumes order0 page*/
  		put_page(page);
  		break;
++<<<<<<< HEAD
 +	case MEM_TYPE_ZERO_COPY:
 +		/* NB! Only valid from an xdp_buff! */
 +		rcu_read_lock();
 +		/* mem->id is valid, checked in xdp_rxq_info_reg_mem_model() */
 +		xa = rhashtable_lookup(mem_id_ht, &mem->id, mem_id_rht_params);
 +		xa->zc_alloc->free(xa->zc_alloc, handle);
 +		rcu_read_unlock();
++=======
+ 	case MEM_TYPE_XSK_BUFF_POOL:
+ 		/* NB! Only valid from an xdp_buff! */
+ 		xsk_buff_free(xdp);
+ 		break;
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  	default:
  		/* Not possible, checked in xdp_rxq_info_reg_mem_model() */
  		break;
@@@ -398,19 -372,19 +385,31 @@@
  
  void xdp_return_frame(struct xdp_frame *xdpf)
  {
++<<<<<<< HEAD
 +	__xdp_return(xdpf->data, &xdpf->mem, false, 0);
++=======
+ 	__xdp_return(xdpf->data, &xdpf->mem, false, NULL);
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  }
  EXPORT_SYMBOL_GPL(xdp_return_frame);
  
  void xdp_return_frame_rx_napi(struct xdp_frame *xdpf)
  {
++<<<<<<< HEAD
 +	__xdp_return(xdpf->data, &xdpf->mem, true, 0);
++=======
+ 	__xdp_return(xdpf->data, &xdpf->mem, true, NULL);
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  }
  EXPORT_SYMBOL_GPL(xdp_return_frame_rx_napi);
  
  void xdp_return_buff(struct xdp_buff *xdp)
  {
++<<<<<<< HEAD
 +	__xdp_return(xdp->data, &xdp->rxq->mem, true, xdp->handle);
++=======
+ 	__xdp_return(xdp->data, &xdp->rxq->mem, true, xdp);
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  }
  EXPORT_SYMBOL_GPL(xdp_return_buff);
  
diff --cc net/xdp/xdp_umem.c
index 3fc509e2f7bd,19e59d1a5e9f..000000000000
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@@ -179,40 -179,9 +179,9 @@@ void xdp_umem_clear_dev(struct xdp_ume
  	umem->zc = false;
  }
  
- static void xdp_umem_unmap_pages(struct xdp_umem *umem)
- {
- 	unsigned int i;
- 
- 	for (i = 0; i < umem->npgs; i++)
- 		if (PageHighMem(umem->pgs[i]))
- 			vunmap(umem->pages[i].addr);
- }
- 
- static int xdp_umem_map_pages(struct xdp_umem *umem)
- {
- 	unsigned int i;
- 	void *addr;
- 
- 	for (i = 0; i < umem->npgs; i++) {
- 		if (PageHighMem(umem->pgs[i]))
- 			addr = vmap(&umem->pgs[i], 1, VM_MAP, PAGE_KERNEL);
- 		else
- 			addr = page_address(umem->pgs[i]);
- 
- 		if (!addr) {
- 			xdp_umem_unmap_pages(umem);
- 			return -ENOMEM;
- 		}
- 
- 		umem->pages[i].addr = addr;
- 	}
- 
- 	return 0;
- }
- 
  static void xdp_umem_unpin_pages(struct xdp_umem *umem)
  {
 -	unpin_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
 +	put_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
  
  	kfree(umem->pgs);
  	umem->pgs = NULL;
@@@ -244,14 -213,9 +213,15 @@@ static void xdp_umem_release(struct xdp
  		umem->cq = NULL;
  	}
  
++<<<<<<< HEAD
 +	xsk_reuseq_destroy(umem);
 +
 +	xdp_umem_unmap_pages(umem);
++=======
+ 	xp_destroy(umem->pool);
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  	xdp_umem_unpin_pages(umem);
  
- 	kvfree(umem->pages);
- 	umem->pages = NULL;
- 
  	xdp_umem_unaccount_pages(umem);
  	kfree(umem);
  }
@@@ -389,13 -349,10 +359,20 @@@ static int xdp_umem_reg(struct xdp_ume
  	if (headroom >= chunk_size - XDP_PACKET_HEADROOM)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	umem->address = (unsigned long)addr;
 +	umem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK
 +					    : ~((u64)chunk_size - 1);
 +	umem->size = size;
 +	umem->headroom = headroom;
 +	umem->chunk_size_nohr = chunk_size - headroom;
 +	umem->npgs = (u32)npgs;
++=======
+ 	umem->size = size;
+ 	umem->headroom = headroom;
+ 	umem->chunk_size = chunk_size;
+ 	umem->npgs = size / PAGE_SIZE;
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  	umem->pgs = NULL;
  	umem->user = NULL;
  	umem->flags = mr->flags;
@@@ -412,18 -369,13 +389,28 @@@
  	if (err)
  		goto out_account;
  
++<<<<<<< HEAD
 +	umem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),
 +			       GFP_KERNEL_ACCOUNT);
 +	if (!umem->pages) {
 +		err = -ENOMEM;
 +		goto out_pin;
 +	}
 +
 +	err = xdp_umem_map_pages(umem);
 +	if (!err)
 +		return 0;
 +
 +	kvfree(umem->pages);
++=======
+ 	umem->pool = xp_create(umem->pgs, umem->npgs, chunks, chunk_size,
+ 			       headroom, size, unaligned_chunks);
+ 	if (!umem->pool) {
+ 		err = -ENOMEM;
+ 		goto out_pin;
+ 	}
+ 	return 0;
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  
  out_pin:
  	xdp_umem_unpin_pages(umem);
diff --cc net/xdp/xsk.c
index 6a30422ca010,3f2ab732ab8b..000000000000
--- a/net/xdp/xsk.c
+++ b/net/xdp/xsk.c
@@@ -213,8 -185,9 +195,14 @@@ static int xsk_rcv(struct xdp_sock *xs
  
  	len = xdp->data_end - xdp->data;
  
++<<<<<<< HEAD
 +	return (xdp->rxq->mem.type == MEM_TYPE_ZERO_COPY) ?
 +		__xsk_rcv_zc(xs, xdp, len) : __xsk_rcv(xs, xdp, len);
++=======
+ 	return xdp->rxq->mem.type == MEM_TYPE_XSK_BUFF_POOL ?
+ 		__xsk_rcv_zc(xs, xdp, len) :
+ 		__xsk_rcv(xs, xdp, len, explicit_free);
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  }
  
  static void xsk_flush(struct xdp_sock *xs)
diff --cc net/xdp/xsk_queue.h
index a322a7dac58c,16bf15864788..000000000000
--- a/net/xdp/xsk_queue.h
+++ b/net/xdp/xsk_queue.h
@@@ -105,68 -104,15 +103,80 @@@ struct xsk_queue 
  
  /* Functions that read and validate content from consumer rings. */
  
++<<<<<<< HEAD
 +static inline bool xskq_cons_crosses_non_contig_pg(struct xdp_umem *umem,
 +						   u64 addr,
 +						   u64 length)
 +{
 +	bool cross_pg = (addr & (PAGE_SIZE - 1)) + length > PAGE_SIZE;
 +	bool next_pg_contig =
 +		(unsigned long)umem->pages[(addr >> PAGE_SHIFT)].addr &
 +			XSK_NEXT_PG_CONTIG_MASK;
 +
 +	return cross_pg && !next_pg_contig;
 +}
 +
 +static inline bool xskq_cons_is_valid_unaligned(struct xsk_queue *q,
 +						u64 addr,
 +						u64 length,
 +						struct xdp_umem *umem)
 +{
 +	u64 base_addr = xsk_umem_extract_addr(addr);
 +
 +	addr = xsk_umem_add_offset_to_addr(addr);
 +	if (base_addr >= q->umem_size || addr >= q->umem_size ||
 +	    xskq_cons_crosses_non_contig_pg(umem, addr, length)) {
 +		q->invalid_descs++;
 +		return false;
 +	}
 +
 +	return true;
 +}
 +
 +static inline bool xskq_cons_is_valid_addr(struct xsk_queue *q, u64 addr)
 +{
 +	if (addr >= q->umem_size) {
 +		q->invalid_descs++;
 +		return false;
 +	}
 +
 +	return true;
 +}
 +
 +static inline bool xskq_cons_read_addr(struct xsk_queue *q, u64 *addr,
 +				       struct xdp_umem *umem)
 +{
 +	struct xdp_umem_ring *ring = (struct xdp_umem_ring *)q->ring;
 +
 +	while (q->cached_cons != q->cached_prod) {
 +		u32 idx = q->cached_cons & q->ring_mask;
 +
 +		*addr = ring->desc[idx] & q->chunk_mask;
 +
 +		if (umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG) {
 +			if (xskq_cons_is_valid_unaligned(q, *addr,
 +							 umem->chunk_size_nohr,
 +							 umem))
 +				return true;
 +			goto out;
 +		}
 +
 +		if (xskq_cons_is_valid_addr(q, *addr))
 +			return true;
 +
 +out:
 +		q->cached_cons++;
++=======
+ static inline bool xskq_cons_read_addr_unchecked(struct xsk_queue *q, u64 *addr)
+ {
+ 	struct xdp_umem_ring *ring = (struct xdp_umem_ring *)q->ring;
+ 
+ 	if (q->cached_cons != q->cached_prod) {
+ 		u32 idx = q->cached_cons & q->ring_mask;
+ 
+ 		*addr = ring->desc[idx];
+ 		return true;
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  	}
  
  	return false;
@@@ -252,12 -181,11 +262,20 @@@ static inline bool xskq_cons_has_entrie
  	return entries >= cnt;
  }
  
++<<<<<<< HEAD
 +static inline bool xskq_cons_peek_addr(struct xsk_queue *q, u64 *addr,
 +				       struct xdp_umem *umem)
 +{
 +	if (q->cached_prod == q->cached_cons)
 +		xskq_cons_get_entries(q);
 +	return xskq_cons_read_addr(q, addr, umem);
++=======
+ static inline bool xskq_cons_peek_addr_unchecked(struct xsk_queue *q, u64 *addr)
+ {
+ 	if (q->cached_prod == q->cached_cons)
+ 		xskq_cons_get_entries(q);
+ 	return xskq_cons_read_addr_unchecked(q, addr);
++>>>>>>> 0807892ecb35 (xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code)
  }
  
  static inline bool xskq_cons_peek_desc(struct xsk_queue *q,
* Unmerged path include/net/xdp_sock_drv.h
* Unmerged path net/xdp/xsk_buff_pool.c
diff --git a/drivers/net/hyperv/netvsc_bpf.c b/drivers/net/hyperv/netvsc_bpf.c
index 1e0c024b0a93..8e4141552423 100644
--- a/drivers/net/hyperv/netvsc_bpf.c
+++ b/drivers/net/hyperv/netvsc_bpf.c
@@ -50,7 +50,6 @@ u32 netvsc_run_xdp(struct net_device *ndev, struct netvsc_channel *nvchan,
 	xdp->data_end = xdp->data + len;
 	xdp->rxq = &nvchan->xdp_rxq;
 	xdp->frame_sz = PAGE_SIZE;
-	xdp->handle = 0;
 
 	memcpy(xdp->data, data, len);
 
* Unmerged path include/net/xdp.h
* Unmerged path include/net/xdp_sock.h
* Unmerged path include/net/xdp_sock_drv.h
* Unmerged path include/trace/events/xdp.h
* Unmerged path net/core/xdp.c
* Unmerged path net/xdp/xdp_umem.c
* Unmerged path net/xdp/xsk.c
* Unmerged path net/xdp/xsk_buff_pool.c
diff --git a/net/xdp/xsk_queue.c b/net/xdp/xsk_queue.c
index 57fb81bd593c..c33b5d985b9b 100644
--- a/net/xdp/xsk_queue.c
+++ b/net/xdp/xsk_queue.c
@@ -9,15 +9,6 @@
 
 #include "xsk_queue.h"
 
-void xskq_set_umem(struct xsk_queue *q, u64 umem_size, u64 chunk_mask)
-{
-	if (!q)
-		return;
-
-	q->umem_size = umem_size;
-	q->chunk_mask = chunk_mask;
-}
-
 static size_t xskq_get_ring_size(struct xsk_queue *q, bool umem_queue)
 {
 	struct xdp_umem_ring *umem_ring;
@@ -63,56 +54,3 @@ void xskq_destroy(struct xsk_queue *q)
 	page_frag_free(q->ring);
 	kfree(q);
 }
-
-struct xdp_umem_fq_reuse *xsk_reuseq_prepare(u32 nentries)
-{
-	struct xdp_umem_fq_reuse *newq;
-
-	/* Check for overflow */
-	if (nentries > (u32)roundup_pow_of_two(nentries))
-		return NULL;
-	nentries = roundup_pow_of_two(nentries);
-
-	newq = kvmalloc(struct_size(newq, handles, nentries), GFP_KERNEL);
-	if (!newq)
-		return NULL;
-	memset(newq, 0, offsetof(typeof(*newq), handles));
-
-	newq->nentries = nentries;
-	return newq;
-}
-EXPORT_SYMBOL_GPL(xsk_reuseq_prepare);
-
-struct xdp_umem_fq_reuse *xsk_reuseq_swap(struct xdp_umem *umem,
-					  struct xdp_umem_fq_reuse *newq)
-{
-	struct xdp_umem_fq_reuse *oldq = umem->fq_reuse;
-
-	if (!oldq) {
-		umem->fq_reuse = newq;
-		return NULL;
-	}
-
-	if (newq->nentries < oldq->length)
-		return newq;
-
-	memcpy(newq->handles, oldq->handles,
-	       array_size(oldq->length, sizeof(u64)));
-	newq->length = oldq->length;
-
-	umem->fq_reuse = newq;
-	return oldq;
-}
-EXPORT_SYMBOL_GPL(xsk_reuseq_swap);
-
-void xsk_reuseq_free(struct xdp_umem_fq_reuse *rq)
-{
-	kvfree(rq);
-}
-EXPORT_SYMBOL_GPL(xsk_reuseq_free);
-
-void xsk_reuseq_destroy(struct xdp_umem *umem)
-{
-	xsk_reuseq_free(umem->fq_reuse);
-	umem->fq_reuse = NULL;
-}
* Unmerged path net/xdp/xsk_queue.h
