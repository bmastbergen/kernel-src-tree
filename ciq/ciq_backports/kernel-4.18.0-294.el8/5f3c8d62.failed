rcu/tree: Maintain separate array for vmalloc ptrs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Uladzislau Rezki (Sony) <urezki@gmail.com>
commit 5f3c8d620447d509e534962e23f7edfb85f4e533
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/5f3c8d62.failed

To do so, we use an array of kvfree_rcu_bulk_data structures.
It consists of two elements:
 - index number 0 corresponds to slab pointers.
 - index number 1 corresponds to vmalloc pointers.

Keeping vmalloc pointers separated from slab pointers makes
it possible to invoke the right freeing API for the right
kind of pointer.

It also prepares us for future headless support for vmalloc
and SLAB objects. Such objects cannot be queued on a linked
list and are instead directly into an array.

	Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
	Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
	Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
Co-developed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
	Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
(cherry picked from commit 5f3c8d620447d509e534962e23f7edfb85f4e533)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/rcu/tree.c
diff --cc kernel/rcu/tree.c
index 4aa7b6bbcde6,67c4b984c499..000000000000
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@@ -54,8 -54,11 +54,10 @@@
  #include <linux/oom.h>
  #include <linux/smpboot.h>
  #include <linux/jiffies.h>
 -#include <linux/slab.h>
  #include <linux/sched/isolation.h>
  #include <linux/sched/clock.h>
+ #include <linux/vmalloc.h>
+ #include <linux/mm.h>
  #include "../time/tick-internal.h"
  
  #include "tree.h"
@@@ -2681,23 -2968,47 +2683,59 @@@ EXPORT_SYMBOL_GPL(call_rcu)
  /* Maximum number of jiffies to wait before draining a batch. */
  #define KFREE_DRAIN_JIFFIES (HZ / 50)
  #define KFREE_N_BATCHES 2
+ #define FREE_N_CHANNELS 2
  
  /**
++<<<<<<< HEAD
 + * struct kfree_rcu_cpu_work - single batch of kfree_rcu() requests
 + * @rcu_work: Let queue_rcu_work() invoke workqueue handler after grace period
 + * @head_free: List of kfree_rcu() objects waiting for a grace period
++=======
+  * struct kvfree_rcu_bulk_data - single block to store kvfree_rcu() pointers
+  * @nr_records: Number of active pointers in the array
+  * @next: Next bulk object in the block chain
+  * @records: Array of the kvfree_rcu() pointers
+  */
+ struct kvfree_rcu_bulk_data {
+ 	unsigned long nr_records;
+ 	struct kvfree_rcu_bulk_data *next;
+ 	void *records[];
+ };
+ 
+ /*
+  * This macro defines how many entries the "records" array
+  * will contain. It is based on the fact that the size of
+  * kvfree_rcu_bulk_data structure becomes exactly one page.
+  */
+ #define KVFREE_BULK_MAX_ENTR \
+ 	((PAGE_SIZE - sizeof(struct kvfree_rcu_bulk_data)) / sizeof(void *))
+ 
+ /**
+  * struct kfree_rcu_cpu_work - single batch of kfree_rcu() requests
+  * @rcu_work: Let queue_rcu_work() invoke workqueue handler after grace period
+  * @head_free: List of kfree_rcu() objects waiting for a grace period
+  * @bkvhead_free: Bulk-List of kvfree_rcu() objects waiting for a grace period
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
   * @krcp: Pointer to @kfree_rcu_cpu structure
   */
  
  struct kfree_rcu_cpu_work {
  	struct rcu_work rcu_work;
  	struct rcu_head *head_free;
++<<<<<<< HEAD
++=======
+ 	struct kvfree_rcu_bulk_data *bkvhead_free[FREE_N_CHANNELS];
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  	struct kfree_rcu_cpu *krcp;
  };
  
  /**
   * struct kfree_rcu_cpu - batch up kfree_rcu() requests for RCU grace period
   * @head: List of kfree_rcu() objects not yet waiting for a grace period
++<<<<<<< HEAD
++=======
+  * @bkvhead: Bulk-List of kvfree_rcu() objects not yet waiting for a grace period
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
   * @krw_arr: Array of batches of kfree_rcu() objects waiting for a grace period
   * @lock: Synchronize access to this structure
   * @monitor_work: Promote @head to @head_free after KFREE_DRAIN_JIFFIES
@@@ -2711,14 -3023,83 +2749,79 @@@
   */
  struct kfree_rcu_cpu {
  	struct rcu_head *head;
++<<<<<<< HEAD
++=======
+ 	struct kvfree_rcu_bulk_data *bkvhead[FREE_N_CHANNELS];
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  	struct kfree_rcu_cpu_work krw_arr[KFREE_N_BATCHES];
 -	raw_spinlock_t lock;
 +	spinlock_t lock;
  	struct delayed_work monitor_work;
  	bool monitor_todo;
  	bool initialized;
 -	int count;
 -
 -	/*
 -	 * A simple cache list that contains objects for
 -	 * reuse purpose. In order to save some per-cpu
 -	 * space the list is singular. Even though it is
 -	 * lockless an access has to be protected by the
 -	 * per-cpu lock.
 -	 */
 -	struct llist_head bkvcache;
 -	int nr_bkv_objs;
  };
  
++<<<<<<< HEAD
 +static DEFINE_PER_CPU(struct kfree_rcu_cpu, krc);
++=======
+ static DEFINE_PER_CPU(struct kfree_rcu_cpu, krc) = {
+ 	.lock = __RAW_SPIN_LOCK_UNLOCKED(krc.lock),
+ };
+ 
+ static __always_inline void
+ debug_rcu_bhead_unqueue(struct kvfree_rcu_bulk_data *bhead)
+ {
+ #ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD
+ 	int i;
+ 
+ 	for (i = 0; i < bhead->nr_records; i++)
+ 		debug_rcu_head_unqueue((struct rcu_head *)(bhead->records[i]));
+ #endif
+ }
+ 
+ static inline struct kfree_rcu_cpu *
+ krc_this_cpu_lock(unsigned long *flags)
+ {
+ 	struct kfree_rcu_cpu *krcp;
+ 
+ 	local_irq_save(*flags);	// For safely calling this_cpu_ptr().
+ 	krcp = this_cpu_ptr(&krc);
+ 	raw_spin_lock(&krcp->lock);
+ 
+ 	return krcp;
+ }
+ 
+ static inline void
+ krc_this_cpu_unlock(struct kfree_rcu_cpu *krcp, unsigned long flags)
+ {
+ 	raw_spin_unlock(&krcp->lock);
+ 	local_irq_restore(flags);
+ }
+ 
+ static inline struct kvfree_rcu_bulk_data *
+ get_cached_bnode(struct kfree_rcu_cpu *krcp)
+ {
+ 	if (!krcp->nr_bkv_objs)
+ 		return NULL;
+ 
+ 	krcp->nr_bkv_objs--;
+ 	return (struct kvfree_rcu_bulk_data *)
+ 		llist_del_first(&krcp->bkvcache);
+ }
+ 
+ static inline bool
+ put_cached_bnode(struct kfree_rcu_cpu *krcp,
+ 	struct kvfree_rcu_bulk_data *bnode)
+ {
+ 	// Check the limit.
+ 	if (krcp->nr_bkv_objs >= rcu_min_cached_objs)
+ 		return false;
+ 
+ 	llist_add((struct llist_node *) bnode, &krcp->bkvcache);
+ 	krcp->nr_bkv_objs++;
+ 	return true;
+ 
+ }
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  
  /*
   * This function is invoked in workqueue context after a grace period.
@@@ -2734,17 -3117,74 +2839,86 @@@ static void kfree_rcu_work(struct work_
  	krwp = container_of(to_rcu_work(work),
  			    struct kfree_rcu_cpu_work, rcu_work);
  	krcp = krwp->krcp;
++<<<<<<< HEAD
 +	spin_lock_irqsave(&krcp->lock, flags);
 +	head = krwp->head_free;
 +	krwp->head_free = NULL;
 +	spin_unlock_irqrestore(&krcp->lock, flags);
 +
 +	// List "head" is now private, so traverse locklessly.
++=======
+ 
+ 	raw_spin_lock_irqsave(&krcp->lock, flags);
+ 	// Channels 1 and 2.
+ 	for (i = 0; i < FREE_N_CHANNELS; i++) {
+ 		bkvhead[i] = krwp->bkvhead_free[i];
+ 		krwp->bkvhead_free[i] = NULL;
+ 	}
+ 
+ 	// Channel 3.
+ 	head = krwp->head_free;
+ 	krwp->head_free = NULL;
+ 	raw_spin_unlock_irqrestore(&krcp->lock, flags);
+ 
+ 	// Handle two first channels.
+ 	for (i = 0; i < FREE_N_CHANNELS; i++) {
+ 		for (; bkvhead[i]; bkvhead[i] = bnext) {
+ 			bnext = bkvhead[i]->next;
+ 			debug_rcu_bhead_unqueue(bkvhead[i]);
+ 
+ 			rcu_lock_acquire(&rcu_callback_map);
+ 			if (i == 0) { // kmalloc() / kfree().
+ 				trace_rcu_invoke_kfree_bulk_callback(
+ 					rcu_state.name, bkvhead[i]->nr_records,
+ 					bkvhead[i]->records);
+ 
+ 				kfree_bulk(bkvhead[i]->nr_records,
+ 					bkvhead[i]->records);
+ 			} else { // vmalloc() / vfree().
+ 				for (j = 0; j < bkvhead[i]->nr_records; j++) {
+ 					trace_rcu_invoke_kfree_callback(
+ 						rcu_state.name,
+ 						bkvhead[i]->records[j], 0);
+ 
+ 					vfree(bkvhead[i]->records[j]);
+ 				}
+ 			}
+ 			rcu_lock_release(&rcu_callback_map);
+ 
+ 			krcp = krc_this_cpu_lock(&flags);
+ 			if (put_cached_bnode(krcp, bkvhead[i]))
+ 				bkvhead[i] = NULL;
+ 			krc_this_cpu_unlock(krcp, flags);
+ 
+ 			if (bkvhead[i])
+ 				free_page((unsigned long) bkvhead[i]);
+ 
+ 			cond_resched_tasks_rcu_qs();
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Emergency case only. It can happen under low memory
+ 	 * condition when an allocation gets failed, so the "bulk"
+ 	 * path can not be temporary maintained.
+ 	 */
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  	for (; head; head = next) {
 -		unsigned long offset = (unsigned long)head->func;
 -		void *ptr = (void *)head - offset;
 -
  		next = head->next;
++<<<<<<< HEAD
 +		// Potentially optimize with kfree_bulk in future.
 +		debug_rcu_head_unqueue(head);
 +		__rcu_reclaim(rcu_state.name, head);
++=======
+ 		debug_rcu_head_unqueue((struct rcu_head *)ptr);
+ 		rcu_lock_acquire(&rcu_callback_map);
+ 		trace_rcu_invoke_kfree_callback(rcu_state.name, head, offset);
+ 
+ 		if (!WARN_ON_ONCE(!__is_kfree_rcu_offset(offset)))
+ 			kvfree(ptr);
+ 
+ 		rcu_lock_release(&rcu_callback_map);
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  		cond_resched_tasks_rcu_qs();
  	}
  }
@@@ -2757,26 -3197,58 +2931,79 @@@
   */
  static inline bool queue_kfree_rcu_work(struct kfree_rcu_cpu *krcp)
  {
++<<<<<<< HEAD
 +	int i;
 +	struct kfree_rcu_cpu_work *krwp = NULL;
 +
 +	lockdep_assert_held(&krcp->lock);
 +	for (i = 0; i < KFREE_N_BATCHES; i++)
 +		if (!krcp->krw_arr[i].head_free) {
 +			krwp = &(krcp->krw_arr[i]);
 +			break;
 +		}
 +
 +	// If a previous RCU batch is in progress, we cannot immediately
 +	// queue another one, so return false to tell caller to retry.
 +	if (!krwp)
 +		return false;
++=======
+ 	struct kfree_rcu_cpu_work *krwp;
+ 	bool repeat = false;
+ 	int i, j;
+ 
+ 	lockdep_assert_held(&krcp->lock);
+ 
+ 	for (i = 0; i < KFREE_N_BATCHES; i++) {
+ 		krwp = &(krcp->krw_arr[i]);
+ 
+ 		/*
+ 		 * Try to detach bkvhead or head and attach it over any
+ 		 * available corresponding free channel. It can be that
+ 		 * a previous RCU batch is in progress, it means that
+ 		 * immediately to queue another one is not possible so
+ 		 * return false to tell caller to retry.
+ 		 */
+ 		if ((krcp->bkvhead[0] && !krwp->bkvhead_free[0]) ||
+ 			(krcp->bkvhead[1] && !krwp->bkvhead_free[1]) ||
+ 				(krcp->head && !krwp->head_free)) {
+ 			// Channel 1 corresponds to SLAB ptrs.
+ 			// Channel 2 corresponds to vmalloc ptrs.
+ 			for (j = 0; j < FREE_N_CHANNELS; j++) {
+ 				if (!krwp->bkvhead_free[j]) {
+ 					krwp->bkvhead_free[j] = krcp->bkvhead[j];
+ 					krcp->bkvhead[j] = NULL;
+ 				}
+ 			}
+ 
+ 			// Channel 3 corresponds to emergency path.
+ 			if (!krwp->head_free) {
+ 				krwp->head_free = krcp->head;
+ 				krcp->head = NULL;
+ 			}
+ 
+ 			WRITE_ONCE(krcp->count, 0);
+ 
+ 			/*
+ 			 * One work is per one batch, so there are three
+ 			 * "free channels", the batch can handle. It can
+ 			 * be that the work is in the pending state when
+ 			 * channels have been detached following by each
+ 			 * other.
+ 			 */
+ 			queue_rcu_work(system_wq, &krwp->rcu_work);
+ 		}
+ 
+ 		// Repeat if any "free" corresponding channel is still busy.
+ 		if (krcp->bkvhead[0] || krcp->bkvhead[1] || krcp->head)
+ 			repeat = true;
+ 	}
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  
 -	return !repeat;
 +	krwp->head_free = krcp->head;
 +	krcp->head = NULL;
 +	INIT_RCU_WORK(&krwp->rcu_work, kfree_rcu_work);
 +	queue_rcu_work(system_wq, &krwp->rcu_work);
 +	return true;
  }
  
  static inline void kfree_rcu_drain_unlock(struct kfree_rcu_cpu *krcp,
@@@ -2810,32 -3282,72 +3037,101 @@@ static void kfree_rcu_monitor(struct wo
  	if (krcp->monitor_todo)
  		kfree_rcu_drain_unlock(krcp, flags);
  	else
++<<<<<<< HEAD
 +		spin_unlock_irqrestore(&krcp->lock, flags);
 +}
 +
 +/*
 + * This version of kfree_call_rcu does not do batching of kfree_rcu() requests.
 + * Used only by rcuperf torture test for comparison with kfree_rcu_batch().
 + */
 +void kfree_call_rcu_nobatch(struct rcu_head *head, rcu_callback_t func)
 +{
 +	__call_rcu(head, func, 1);
 +}
 +EXPORT_SYMBOL_GPL(kfree_call_rcu_nobatch);
 +
 +/*
 + * Queue a request for lazy invocation of kfree() after a grace period.
 + *
 + * Each kfree_call_rcu() request is added to a batch. The batch will be drained
 + * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch
 + * will be kfree'd in workqueue context. This allows us to:
 + *
 + * 1.	Batch requests together to reduce the number of grace periods during
 + *	heavy kfree_rcu() load.
 + *
 + * 2.	It makes it possible to use kfree_bulk() on a large number of
 + *	kfree_rcu() requests thus reducing cache misses and the per-object
 + *	overhead of kfree().
++=======
+ 		raw_spin_unlock_irqrestore(&krcp->lock, flags);
+ }
+ 
+ static inline bool
+ kvfree_call_rcu_add_ptr_to_bulk(struct kfree_rcu_cpu *krcp, void *ptr)
+ {
+ 	struct kvfree_rcu_bulk_data *bnode;
+ 	int idx;
+ 
+ 	if (unlikely(!krcp->initialized))
+ 		return false;
+ 
+ 	lockdep_assert_held(&krcp->lock);
+ 	idx = !!is_vmalloc_addr(ptr);
+ 
+ 	/* Check if a new block is required. */
+ 	if (!krcp->bkvhead[idx] ||
+ 			krcp->bkvhead[idx]->nr_records == KVFREE_BULK_MAX_ENTR) {
+ 		bnode = get_cached_bnode(krcp);
+ 		if (!bnode) {
+ 			/*
+ 			 * To keep this path working on raw non-preemptible
+ 			 * sections, prevent the optional entry into the
+ 			 * allocator as it uses sleeping locks. In fact, even
+ 			 * if the caller of kfree_rcu() is preemptible, this
+ 			 * path still is not, as krcp->lock is a raw spinlock.
+ 			 * With additional page pre-allocation in the works,
+ 			 * hitting this return is going to be much less likely.
+ 			 */
+ 			if (IS_ENABLED(CONFIG_PREEMPT_RT))
+ 				return false;
+ 
+ 			bnode = (struct kvfree_rcu_bulk_data *)
+ 				__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
+ 		}
+ 
+ 		/* Switch to emergency path. */
+ 		if (unlikely(!bnode))
+ 			return false;
+ 
+ 		/* Initialize the new block. */
+ 		bnode->nr_records = 0;
+ 		bnode->next = krcp->bkvhead[idx];
+ 
+ 		/* Attach it to the head. */
+ 		krcp->bkvhead[idx] = bnode;
+ 	}
+ 
+ 	/* Finally insert. */
+ 	krcp->bkvhead[idx]->records
+ 		[krcp->bkvhead[idx]->nr_records++] = ptr;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * Queue a request for lazy invocation of appropriate free routine after a
+  * grace period. Please note there are three paths are maintained, two are the
+  * main ones that use array of pointers interface and third one is emergency
+  * one, that is used only when the main path can not be maintained temporary,
+  * due to memory pressure.
+  *
+  * Each kfree_call_rcu() request is added to a batch. The batch will be drained
+  * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch will
+  * be free'd in workqueue context. This allows us to: batch requests together to
+  * reduce the number of grace periods during heavy kfree_rcu()/kvfree_rcu() load.
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
   */
  void kfree_call_rcu(struct rcu_head *head, rcu_callback_t func)
  {
@@@ -2856,9 -3365,18 +3152,24 @@@
  			  __func__, head);
  		goto unlock_return;
  	}
++<<<<<<< HEAD
 +	head->func = func;
 +	head->next = krcp->head;
 +	krcp->head = head;
++=======
+ 
+ 	/*
+ 	 * Under high memory pressure GFP_NOWAIT can fail,
+ 	 * in that case the emergency path is maintained.
+ 	 */
+ 	if (unlikely(!kvfree_call_rcu_add_ptr_to_bulk(krcp, ptr))) {
+ 		head->func = func;
+ 		head->next = krcp->head;
+ 		krcp->head = head;
+ 	}
+ 
+ 	WRITE_ONCE(krcp->count, krcp->count + 1);
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  
  	// Set timer to drain after KFREE_DRAIN_JIFFIES.
  	if (rcu_scheduler_active == RCU_SCHEDULER_RUNNING &&
@@@ -3772,10 -4351,23 +4083,29 @@@ static void __init kfree_rcu_batch_init
  
  	for_each_possible_cpu(cpu) {
  		struct kfree_rcu_cpu *krcp = per_cpu_ptr(&krc, cpu);
++<<<<<<< HEAD
++=======
+ 		struct kvfree_rcu_bulk_data *bnode;
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  
 -		for (i = 0; i < KFREE_N_BATCHES; i++) {
 -			INIT_RCU_WORK(&krcp->krw_arr[i].rcu_work, kfree_rcu_work);
 +		spin_lock_init(&krcp->lock);
 +		for (i = 0; i < KFREE_N_BATCHES; i++)
  			krcp->krw_arr[i].krcp = krcp;
++<<<<<<< HEAD
++=======
+ 		}
+ 
+ 		for (i = 0; i < rcu_min_cached_objs; i++) {
+ 			bnode = (struct kvfree_rcu_bulk_data *)
+ 				__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
+ 
+ 			if (bnode)
+ 				put_cached_bnode(krcp, bnode);
+ 			else
+ 				pr_err("Failed to preallocate for %d CPU!\n", cpu);
+ 		}
+ 
++>>>>>>> 5f3c8d620447 (rcu/tree: Maintain separate array for vmalloc ptrs)
  		INIT_DELAYED_WORK(&krcp->monitor_work, kfree_rcu_monitor);
  		krcp->initialized = true;
  	}
* Unmerged path kernel/rcu/tree.c
