mlock: fix unevictable_pgs event counts on THP

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Hugh Dickins <hughd@google.com>
commit 0964730bf46b4e271c5ecad5badbbd95737c087b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/0964730b.failed

5.8 commit 5d91f31faf8e ("mm: swap: fix vmstats for huge page") has
established that vm_events should count every subpage of a THP, including
unevictable_pgs_culled and unevictable_pgs_rescued; but
lru_cache_add_inactive_or_unevictable() was not doing so for
unevictable_pgs_mlocked, and mm/mlock.c was not doing so for
unevictable_pgs mlocked, munlocked, cleared and stranded.

Fix them; but THPs don't go the pagevec way in mlock.c, so no fixes needed
on that path.

Fixes: 5d91f31faf8e ("mm: swap: fix vmstats for huge page")
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: Yang Shi <shy828301@gmail.com>
	Cc: Alex Shi <alex.shi@linux.alibaba.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Qian Cai <cai@lca.pw>
Link: http://lkml.kernel.org/r/alpine.LSU.2.11.2008301408230.5954@eggly.anvils
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0964730bf46b4e271c5ecad5badbbd95737c087b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mlock.c
#	mm/swap.c
diff --cc mm/mlock.c
index 0964aa0dd7bd,884b1216da6a..000000000000
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@@ -61,9 -63,9 +63,15 @@@ void clear_page_mlock(struct page *page
  	if (!TestClearPageMlocked(page))
  		return;
  
++<<<<<<< HEAD
 +	mod_zone_page_state(page_zone(page), NR_MLOCK,
 +			    -hpage_nr_pages(page));
 +	count_vm_event(UNEVICTABLE_PGCLEARED);
++=======
+ 	nr_pages = thp_nr_pages(page);
+ 	mod_zone_page_state(page_zone(page), NR_MLOCK, -nr_pages);
+ 	count_vm_events(UNEVICTABLE_PGCLEARED, nr_pages);
++>>>>>>> 0964730bf46b (mlock: fix unevictable_pgs event counts on THP)
  	/*
  	 * The previous TestClearPageMlocked() corresponds to the smp_mb()
  	 * in __pagevec_lru_add_fn().
@@@ -94,9 -96,10 +102,16 @@@ void mlock_vma_page(struct page *page
  	VM_BUG_ON_PAGE(PageCompound(page) && PageDoubleMap(page), page);
  
  	if (!TestSetPageMlocked(page)) {
++<<<<<<< HEAD
 +		mod_zone_page_state(page_zone(page), NR_MLOCK,
 +				    hpage_nr_pages(page));
 +		count_vm_event(UNEVICTABLE_PGMLOCKED);
++=======
+ 		int nr_pages = thp_nr_pages(page);
+ 
+ 		mod_zone_page_state(page_zone(page), NR_MLOCK, nr_pages);
+ 		count_vm_events(UNEVICTABLE_PGMLOCKED, nr_pages);
++>>>>>>> 0964730bf46b (mlock: fix unevictable_pgs event counts on THP)
  		if (!isolate_lru_page(page))
  			putback_lru_page(page);
  	}
diff --cc mm/swap.c
index 70728521e27e,e7bdf094f76a..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -461,22 -485,23 +461,33 @@@ void lru_cache_add(struct page *page
   * directly back onto it's zone's unevictable list, it does NOT use a
   * per cpu pagevec.
   */
 -void lru_cache_add_inactive_or_unevictable(struct page *page,
 +void lru_cache_add_active_or_unevictable(struct page *page,
  					 struct vm_area_struct *vma)
  {
 -	bool unevictable;
 -
  	VM_BUG_ON_PAGE(PageLRU(page), page);
  
++<<<<<<< HEAD
 +	if (likely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) != VM_LOCKED))
 +		SetPageActive(page);
 +	else if (!TestSetPageMlocked(page)) {
++=======
+ 	unevictable = (vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) == VM_LOCKED;
+ 	if (unlikely(unevictable) && !TestSetPageMlocked(page)) {
+ 		int nr_pages = thp_nr_pages(page);
++>>>>>>> 0964730bf46b (mlock: fix unevictable_pgs event counts on THP)
  		/*
  		 * We use the irq-unsafe __mod_zone_page_stat because this
  		 * counter is not modified from interrupt context, and the pte
  		 * lock is held(spinlock), which implies preemption disabled.
  		 */
++<<<<<<< HEAD
 +		__mod_zone_page_state(page_zone(page), NR_MLOCK,
 +				    hpage_nr_pages(page));
 +		count_vm_event(UNEVICTABLE_PGMLOCKED);
++=======
+ 		__mod_zone_page_state(page_zone(page), NR_MLOCK, nr_pages);
+ 		count_vm_events(UNEVICTABLE_PGMLOCKED, nr_pages);
++>>>>>>> 0964730bf46b (mlock: fix unevictable_pgs event counts on THP)
  	}
  	lru_cache_add(page);
  }
* Unmerged path mm/mlock.c
* Unmerged path mm/swap.c
