RDMA: Remove the udata parameter from alloc_mr callback

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Gal Pressman <galpress@amazon.com>
commit 42a3b153966c9cd9a90f6a669d1ffed7fef2d325
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/42a3b153.failed

Allocating an MR flow can only be initiated by kernel users, and not from
userspace so a udata parameter is redundant.

Link: https://lore.kernel.org/r/20200706120343.10816-4-galpress@amazon.com
	Signed-off-by: Gal Pressman <galpress@amazon.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 42a3b153966c9cd9a90f6a669d1ffed7fef2d325)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hns/hns_roce_device.h
#	drivers/infiniband/hw/hns/hns_roce_mr.c
diff --cc drivers/infiniband/hw/hns/hns_roce_device.h
index 4f2300b49109,5b946b5bd586..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_device.h
+++ b/drivers/infiniband/hw/hns/hns_roce_device.h
@@@ -965,14 -1190,21 +965,21 @@@ struct ib_mr *hns_roce_reg_user_mr(stru
  int hns_roce_rereg_user_mr(struct ib_mr *mr, int flags, u64 start, u64 length,
  			   u64 virt_addr, int mr_access_flags, struct ib_pd *pd,
  			   struct ib_udata *udata);
++<<<<<<< HEAD
++=======
+ struct ib_mr *hns_roce_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
+ 				u32 max_num_sg);
+ int hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
+ 		       unsigned int *sg_offset);
++>>>>>>> 42a3b153966c (RDMA: Remove the udata parameter from alloc_mr callback)
  int hns_roce_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata);
 -int hns_roce_hw_destroy_mpt(struct hns_roce_dev *hr_dev,
 -			    struct hns_roce_cmd_mailbox *mailbox,
 -			    unsigned long mpt_index);
 +int hns_roce_hw2sw_mpt(struct hns_roce_dev *hr_dev,
 +		       struct hns_roce_cmd_mailbox *mailbox,
 +		       unsigned long mpt_index);
  unsigned long key_to_hw_index(u32 key);
  
 -struct ib_mw *hns_roce_alloc_mw(struct ib_pd *pd, enum ib_mw_type,
 -				struct ib_udata *udata);
 -int hns_roce_dealloc_mw(struct ib_mw *ibmw);
 -
 -void hns_roce_buf_free(struct hns_roce_dev *hr_dev, struct hns_roce_buf *buf);
 +void hns_roce_buf_free(struct hns_roce_dev *hr_dev, u32 size,
 +		       struct hns_roce_buf *buf);
  int hns_roce_buf_alloc(struct hns_roce_dev *hr_dev, u32 size, u32 max_direct,
  		       struct hns_roce_buf *buf, u32 page_shift);
  
diff --cc drivers/infiniband/hw/hns/hns_roce_mr.c
index ba1754ca6d84,1380cdab5701..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_mr.c
+++ b/drivers/infiniband/hw/hns/hns_roce_mr.c
@@@ -1211,3 -412,683 +1211,686 @@@ int hns_roce_dereg_mr(struct ib_mr *ibm
  
  	return ret;
  }
++<<<<<<< HEAD
++=======
+ 
+ struct ib_mr *hns_roce_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
+ 				u32 max_num_sg)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(pd->device);
+ 	struct device *dev = hr_dev->dev;
+ 	struct hns_roce_mr *mr;
+ 	u64 length;
+ 	int ret;
+ 
+ 	if (mr_type != IB_MR_TYPE_MEM_REG)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	if (max_num_sg > HNS_ROCE_FRMR_MAX_PA) {
+ 		dev_err(dev, "max_num_sg larger than %d\n",
+ 			HNS_ROCE_FRMR_MAX_PA);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	mr = kzalloc(sizeof(*mr), GFP_KERNEL);
+ 	if (!mr)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mr->type = MR_TYPE_FRMR;
+ 
+ 	/* Allocate memory region key */
+ 	length = max_num_sg * (1 << PAGE_SHIFT);
+ 	ret = alloc_mr_key(hr_dev, mr, to_hr_pd(pd)->pdn, 0, length, 0);
+ 	if (ret)
+ 		goto err_free;
+ 
+ 	ret = alloc_mr_pbl(hr_dev, mr, length, NULL, 0, 0);
+ 	if (ret)
+ 		goto err_key;
+ 
+ 	ret = hns_roce_mr_enable(hr_dev, mr);
+ 	if (ret)
+ 		goto err_pbl;
+ 
+ 	mr->ibmr.rkey = mr->ibmr.lkey = mr->key;
+ 	mr->ibmr.length = length;
+ 
+ 	return &mr->ibmr;
+ 
+ err_key:
+ 	free_mr_key(hr_dev, mr);
+ err_pbl:
+ 	free_mr_pbl(hr_dev, mr);
+ err_free:
+ 	kfree(mr);
+ 	return ERR_PTR(ret);
+ }
+ 
+ static int hns_roce_set_page(struct ib_mr *ibmr, u64 addr)
+ {
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 
+ 	if (likely(mr->npages < mr->pbl_mtr.hem_cfg.buf_pg_count)) {
+ 		mr->page_list[mr->npages++] = addr;
+ 		return 0;
+ 	}
+ 
+ 	return -ENOBUFS;
+ }
+ 
+ int hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
+ 		       unsigned int *sg_offset)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 	struct hns_roce_mtr *mtr = &mr->pbl_mtr;
+ 	int ret = 0;
+ 
+ 	mr->npages = 0;
+ 	mr->page_list = kvcalloc(mr->pbl_mtr.hem_cfg.buf_pg_count,
+ 				 sizeof(dma_addr_t), GFP_KERNEL);
+ 	if (!mr->page_list)
+ 		return ret;
+ 
+ 	ret = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, hns_roce_set_page);
+ 	if (ret < 1) {
+ 		ibdev_err(ibdev, "failed to store sg pages %d %d, cnt = %d.\n",
+ 			  mr->npages, mr->pbl_mtr.hem_cfg.buf_pg_count, ret);
+ 		goto err_page_list;
+ 	}
+ 
+ 	mtr->hem_cfg.region[0].offset = 0;
+ 	mtr->hem_cfg.region[0].count = mr->npages;
+ 	mtr->hem_cfg.region[0].hopnum = mr->pbl_hop_num;
+ 	mtr->hem_cfg.region_count = 1;
+ 	ret = hns_roce_mtr_map(hr_dev, mtr, mr->page_list, mr->npages);
+ 	if (ret) {
+ 		ibdev_err(ibdev, "failed to map sg mtr, ret = %d.\n", ret);
+ 		ret = 0;
+ 	} else {
+ 		mr->pbl_mtr.hem_cfg.buf_pg_shift = ilog2(ibmr->page_size);
+ 		ret = mr->npages;
+ 	}
+ 
+ err_page_list:
+ 	kvfree(mr->page_list);
+ 	mr->page_list = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static void hns_roce_mw_free(struct hns_roce_dev *hr_dev,
+ 			     struct hns_roce_mw *mw)
+ {
+ 	struct device *dev = hr_dev->dev;
+ 	int ret;
+ 
+ 	if (mw->enabled) {
+ 		ret = hns_roce_hw_destroy_mpt(hr_dev, NULL,
+ 					      key_to_hw_index(mw->rkey) &
+ 					      (hr_dev->caps.num_mtpts - 1));
+ 		if (ret)
+ 			dev_warn(dev, "MW DESTROY_MPT failed (%d)\n", ret);
+ 
+ 		hns_roce_table_put(hr_dev, &hr_dev->mr_table.mtpt_table,
+ 				   key_to_hw_index(mw->rkey));
+ 	}
+ 
+ 	hns_roce_bitmap_free(&hr_dev->mr_table.mtpt_bitmap,
+ 			     key_to_hw_index(mw->rkey), BITMAP_NO_RR);
+ }
+ 
+ static int hns_roce_mw_enable(struct hns_roce_dev *hr_dev,
+ 			      struct hns_roce_mw *mw)
+ {
+ 	struct hns_roce_mr_table *mr_table = &hr_dev->mr_table;
+ 	struct hns_roce_cmd_mailbox *mailbox;
+ 	struct device *dev = hr_dev->dev;
+ 	unsigned long mtpt_idx = key_to_hw_index(mw->rkey);
+ 	int ret;
+ 
+ 	/* prepare HEM entry memory */
+ 	ret = hns_roce_table_get(hr_dev, &mr_table->mtpt_table, mtpt_idx);
+ 	if (ret)
+ 		return ret;
+ 
+ 	mailbox = hns_roce_alloc_cmd_mailbox(hr_dev);
+ 	if (IS_ERR(mailbox)) {
+ 		ret = PTR_ERR(mailbox);
+ 		goto err_table;
+ 	}
+ 
+ 	ret = hr_dev->hw->mw_write_mtpt(mailbox->buf, mw);
+ 	if (ret) {
+ 		dev_err(dev, "MW write mtpt fail!\n");
+ 		goto err_page;
+ 	}
+ 
+ 	ret = hns_roce_hw_create_mpt(hr_dev, mailbox,
+ 				     mtpt_idx & (hr_dev->caps.num_mtpts - 1));
+ 	if (ret) {
+ 		dev_err(dev, "MW CREATE_MPT failed (%d)\n", ret);
+ 		goto err_page;
+ 	}
+ 
+ 	mw->enabled = 1;
+ 
+ 	hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 
+ 	return 0;
+ 
+ err_page:
+ 	hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 
+ err_table:
+ 	hns_roce_table_put(hr_dev, &mr_table->mtpt_table, mtpt_idx);
+ 
+ 	return ret;
+ }
+ 
+ struct ib_mw *hns_roce_alloc_mw(struct ib_pd *ib_pd, enum ib_mw_type type,
+ 				struct ib_udata *udata)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ib_pd->device);
+ 	struct hns_roce_mw *mw;
+ 	unsigned long index = 0;
+ 	int ret;
+ 
+ 	mw = kmalloc(sizeof(*mw), GFP_KERNEL);
+ 	if (!mw)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	/* Allocate a key for mw from bitmap */
+ 	ret = hns_roce_bitmap_alloc(&hr_dev->mr_table.mtpt_bitmap, &index);
+ 	if (ret)
+ 		goto err_bitmap;
+ 
+ 	mw->rkey = hw_index_to_key(index);
+ 
+ 	mw->ibmw.rkey = mw->rkey;
+ 	mw->ibmw.type = type;
+ 	mw->pdn = to_hr_pd(ib_pd)->pdn;
+ 	mw->pbl_hop_num = hr_dev->caps.pbl_hop_num;
+ 	mw->pbl_ba_pg_sz = hr_dev->caps.pbl_ba_pg_sz;
+ 	mw->pbl_buf_pg_sz = hr_dev->caps.pbl_buf_pg_sz;
+ 
+ 	ret = hns_roce_mw_enable(hr_dev, mw);
+ 	if (ret)
+ 		goto err_mw;
+ 
+ 	return &mw->ibmw;
+ 
+ err_mw:
+ 	hns_roce_mw_free(hr_dev, mw);
+ 
+ err_bitmap:
+ 	kfree(mw);
+ 
+ 	return ERR_PTR(ret);
+ }
+ 
+ int hns_roce_dealloc_mw(struct ib_mw *ibmw)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmw->device);
+ 	struct hns_roce_mw *mw = to_hr_mw(ibmw);
+ 
+ 	hns_roce_mw_free(hr_dev, mw);
+ 	kfree(mw);
+ 
+ 	return 0;
+ }
+ 
+ static int mtr_map_region(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			  dma_addr_t *pages, struct hns_roce_buf_region *region)
+ {
+ 	__le64 *mtts;
+ 	int offset;
+ 	int count;
+ 	int npage;
+ 	u64 addr;
+ 	int end;
+ 	int i;
+ 
+ 	/* if hopnum is 0, buffer cannot store BAs, so skip write mtt */
+ 	if (!region->hopnum)
+ 		return 0;
+ 
+ 	offset = region->offset;
+ 	end = offset + region->count;
+ 	npage = 0;
+ 	while (offset < end) {
+ 		mtts = hns_roce_hem_list_find_mtt(hr_dev, &mtr->hem_list,
+ 						  offset, &count, NULL);
+ 		if (!mtts)
+ 			return -ENOBUFS;
+ 
+ 		for (i = 0; i < count; i++) {
+ 			if (hr_dev->hw_rev == HNS_ROCE_HW_VER1)
+ 				addr = to_hr_hw_page_addr(pages[npage]);
+ 			else
+ 				addr = pages[npage];
+ 
+ 			mtts[i] = cpu_to_le64(addr);
+ 			npage++;
+ 		}
+ 		offset += count;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static inline bool mtr_has_mtt(struct hns_roce_buf_attr *attr)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < attr->region_count; i++)
+ 		if (attr->region[i].hopnum != HNS_ROCE_HOP_NUM_0 &&
+ 		    attr->region[i].hopnum > 0)
+ 			return true;
+ 
+ 	/* because the mtr only one root base address, when hopnum is 0 means
+ 	 * root base address equals the first buffer address, thus all alloced
+ 	 * memory must in a continuous space accessed by direct mode.
+ 	 */
+ 	return false;
+ }
+ 
+ static inline size_t mtr_bufs_size(struct hns_roce_buf_attr *attr)
+ {
+ 	size_t size = 0;
+ 	int i;
+ 
+ 	for (i = 0; i < attr->region_count; i++)
+ 		size += attr->region[i].size;
+ 
+ 	return size;
+ }
+ 
+ static inline int mtr_umem_page_count(struct ib_umem *umem,
+ 				      unsigned int page_shift)
+ {
+ 	int count = ib_umem_page_count(umem);
+ 
+ 	if (page_shift >= PAGE_SHIFT)
+ 		count >>= page_shift - PAGE_SHIFT;
+ 	else
+ 		count <<= PAGE_SHIFT - page_shift;
+ 
+ 	return count;
+ }
+ 
+ static inline size_t mtr_kmem_direct_size(bool is_direct, size_t alloc_size,
+ 					  unsigned int page_shift)
+ {
+ 	if (is_direct)
+ 		return ALIGN(alloc_size, 1 << page_shift);
+ 	else
+ 		return HNS_HW_DIRECT_PAGE_COUNT << page_shift;
+ }
+ 
+ /*
+  * check the given pages in continuous address space
+  * Returns 0 on success, or the error page num.
+  */
+ static inline int mtr_check_direct_pages(dma_addr_t *pages, int page_count,
+ 					 unsigned int page_shift)
+ {
+ 	size_t page_size = 1 << page_shift;
+ 	int i;
+ 
+ 	for (i = 1; i < page_count; i++)
+ 		if (pages[i] - pages[i - 1] != page_size)
+ 			return i;
+ 
+ 	return 0;
+ }
+ 
+ static void mtr_free_bufs(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr)
+ {
+ 	/* release user buffers */
+ 	if (mtr->umem) {
+ 		ib_umem_release(mtr->umem);
+ 		mtr->umem = NULL;
+ 	}
+ 
+ 	/* release kernel buffers */
+ 	if (mtr->kmem) {
+ 		hns_roce_buf_free(hr_dev, mtr->kmem);
+ 		kfree(mtr->kmem);
+ 		mtr->kmem = NULL;
+ 	}
+ }
+ 
+ static int mtr_alloc_bufs(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			  struct hns_roce_buf_attr *buf_attr, bool is_direct,
+ 			  struct ib_udata *udata, unsigned long user_addr)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	unsigned int max_pg_shift = buf_attr->page_shift;
+ 	unsigned int best_pg_shift = 0;
+ 	int all_pg_count = 0;
+ 	size_t direct_size;
+ 	size_t total_size;
+ 	unsigned long tmp;
+ 	int ret = 0;
+ 
+ 	total_size = mtr_bufs_size(buf_attr);
+ 	if (total_size < 1) {
+ 		ibdev_err(ibdev, "Failed to check mtr size\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (udata) {
+ 		mtr->kmem = NULL;
+ 		mtr->umem = ib_umem_get(ibdev, user_addr, total_size,
+ 					buf_attr->user_access);
+ 		if (IS_ERR_OR_NULL(mtr->umem)) {
+ 			ibdev_err(ibdev, "Failed to get umem, ret %ld\n",
+ 				  PTR_ERR(mtr->umem));
+ 			return -ENOMEM;
+ 		}
+ 		if (buf_attr->fixed_page) {
+ 			best_pg_shift = max_pg_shift;
+ 		} else {
+ 			tmp = GENMASK(max_pg_shift, 0);
+ 			ret = ib_umem_find_best_pgsz(mtr->umem, tmp, user_addr);
+ 			best_pg_shift = (ret <= PAGE_SIZE) ?
+ 					PAGE_SHIFT : ilog2(ret);
+ 		}
+ 		all_pg_count = mtr_umem_page_count(mtr->umem, best_pg_shift);
+ 		ret = 0;
+ 	} else {
+ 		mtr->umem = NULL;
+ 		mtr->kmem = kzalloc(sizeof(*mtr->kmem), GFP_KERNEL);
+ 		if (!mtr->kmem) {
+ 			ibdev_err(ibdev, "Failed to alloc kmem\n");
+ 			return -ENOMEM;
+ 		}
+ 		direct_size = mtr_kmem_direct_size(is_direct, total_size,
+ 						   max_pg_shift);
+ 		ret = hns_roce_buf_alloc(hr_dev, total_size, direct_size,
+ 					 mtr->kmem, max_pg_shift);
+ 		if (ret) {
+ 			ibdev_err(ibdev, "Failed to alloc kmem, ret %d\n", ret);
+ 			goto err_alloc_mem;
+ 		} else {
+ 			best_pg_shift = max_pg_shift;
+ 			all_pg_count = mtr->kmem->npages;
+ 		}
+ 	}
+ 
+ 	/* must bigger than minimum hardware page shift */
+ 	if (best_pg_shift < HNS_HW_PAGE_SHIFT || all_pg_count < 1) {
+ 		ret = -EINVAL;
+ 		ibdev_err(ibdev, "Failed to check mtr page shift %d count %d\n",
+ 			  best_pg_shift, all_pg_count);
+ 		goto err_alloc_mem;
+ 	}
+ 
+ 	mtr->hem_cfg.buf_pg_shift = best_pg_shift;
+ 	mtr->hem_cfg.buf_pg_count = all_pg_count;
+ 
+ 	return 0;
+ err_alloc_mem:
+ 	mtr_free_bufs(hr_dev, mtr);
+ 	return ret;
+ }
+ 
+ static int mtr_get_pages(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			 dma_addr_t *pages, int count, unsigned int page_shift)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	int npage;
+ 	int err;
+ 
+ 	if (mtr->umem)
+ 		npage = hns_roce_get_umem_bufs(hr_dev, pages, count, 0,
+ 					       mtr->umem, page_shift);
+ 	else
+ 		npage = hns_roce_get_kmem_bufs(hr_dev, pages, count, 0,
+ 					       mtr->kmem);
+ 
+ 	if (mtr->hem_cfg.is_direct && npage > 1) {
+ 		err = mtr_check_direct_pages(pages, npage, page_shift);
+ 		if (err) {
+ 			ibdev_err(ibdev, "Failed to check %s direct page-%d\n",
+ 				  mtr->umem ? "user" : "kernel", err);
+ 			npage = err;
+ 		}
+ 	}
+ 
+ 	return npage;
+ }
+ 
+ int hns_roce_mtr_map(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 		     dma_addr_t *pages, int page_cnt)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	struct hns_roce_buf_region *r;
+ 	int err;
+ 	int i;
+ 
+ 	for (i = 0; i < mtr->hem_cfg.region_count; i++) {
+ 		r = &mtr->hem_cfg.region[i];
+ 		if (r->offset + r->count > page_cnt) {
+ 			err = -EINVAL;
+ 			ibdev_err(ibdev,
+ 				  "Failed to check mtr%d end %d + %d, max %d\n",
+ 				  i, r->offset, r->count, page_cnt);
+ 			return err;
+ 		}
+ 
+ 		err = mtr_map_region(hr_dev, mtr, &pages[r->offset], r);
+ 		if (err) {
+ 			ibdev_err(ibdev,
+ 				  "Failed to map mtr%d offset %d, err %d\n",
+ 				  i, r->offset, err);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int hns_roce_mtr_find(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 		      int offset, u64 *mtt_buf, int mtt_max, u64 *base_addr)
+ {
+ 	int mtt_count;
+ 	int total = 0;
+ 	__le64 *mtts;
+ 	int npage;
+ 	u64 addr;
+ 	int left;
+ 
+ 	if (!mtt_buf || mtt_max < 1)
+ 		goto done;
+ 
+ 	/* no mtt memory in direct mode, so just return the buffer address */
+ 	if (mtr->hem_cfg.is_direct) {
+ 		npage = offset;
+ 		for (total = 0; total < mtt_max; total++, npage++) {
+ 			addr = mtr->hem_cfg.root_ba +
+ 			       (npage << mtr->hem_cfg.buf_pg_shift);
+ 
+ 			if (hr_dev->hw_rev == HNS_ROCE_HW_VER1)
+ 				mtt_buf[total] = to_hr_hw_page_addr(addr);
+ 			else
+ 				mtt_buf[total] = addr;
+ 		}
+ 
+ 		goto done;
+ 	}
+ 
+ 	left = mtt_max;
+ 	while (left > 0) {
+ 		mtt_count = 0;
+ 		mtts = hns_roce_hem_list_find_mtt(hr_dev, &mtr->hem_list,
+ 						  offset + total,
+ 						  &mtt_count, NULL);
+ 		if (!mtts || !mtt_count)
+ 			goto done;
+ 
+ 		npage = min(mtt_count, left);
+ 		left -= npage;
+ 		for (mtt_count = 0; mtt_count < npage; mtt_count++)
+ 			mtt_buf[total++] = le64_to_cpu(mtts[mtt_count]);
+ 	}
+ 
+ done:
+ 	if (base_addr)
+ 		*base_addr = mtr->hem_cfg.root_ba;
+ 
+ 	return total;
+ }
+ 
+ /* convert buffer size to page index and page count */
+ static unsigned int mtr_init_region(struct hns_roce_buf_attr *attr,
+ 				    int page_cnt,
+ 				    struct hns_roce_buf_region *regions,
+ 				    int region_cnt, unsigned int page_shift)
+ {
+ 	unsigned int page_size = 1 << page_shift;
+ 	int max_region = attr->region_count;
+ 	struct hns_roce_buf_region *r;
+ 	unsigned int i = 0;
+ 	int page_idx = 0;
+ 
+ 	for (; i < region_cnt && i < max_region && page_idx < page_cnt; i++) {
+ 		r = &regions[i];
+ 		r->hopnum = attr->region[i].hopnum == HNS_ROCE_HOP_NUM_0 ?
+ 			    0 : attr->region[i].hopnum;
+ 		r->offset = page_idx;
+ 		r->count = DIV_ROUND_UP(attr->region[i].size, page_size);
+ 		page_idx += r->count;
+ 	}
+ 
+ 	return i;
+ }
+ 
+ /**
+  * hns_roce_mtr_create - Create hns memory translate region.
+  *
+  * @mtr: memory translate region
+  * @init_attr: init attribute for creating mtr
+  * @page_shift: page shift for multi-hop base address table
+  * @udata: user space context, if it's NULL, means kernel space
+  * @user_addr: userspace virtual address to start at
+  * @buf_alloced: mtr has private buffer, true means need to alloc
+  */
+ int hns_roce_mtr_create(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr,
+ 			struct hns_roce_buf_attr *buf_attr,
+ 			unsigned int page_shift, struct ib_udata *udata,
+ 			unsigned long user_addr)
+ {
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	dma_addr_t *pages = NULL;
+ 	int region_cnt = 0;
+ 	int all_pg_cnt;
+ 	int get_pg_cnt;
+ 	bool has_mtt;
+ 	int err = 0;
+ 
+ 	has_mtt = mtr_has_mtt(buf_attr);
+ 	/* if buffer only need mtt, just init the hem cfg */
+ 	if (buf_attr->mtt_only) {
+ 		mtr->hem_cfg.buf_pg_shift = buf_attr->page_shift;
+ 		mtr->hem_cfg.buf_pg_count = mtr_bufs_size(buf_attr) >>
+ 					    buf_attr->page_shift;
+ 		mtr->umem = NULL;
+ 		mtr->kmem = NULL;
+ 	} else {
+ 		err = mtr_alloc_bufs(hr_dev, mtr, buf_attr, !has_mtt, udata,
+ 				     user_addr);
+ 		if (err) {
+ 			ibdev_err(ibdev, "Failed to alloc mtr bufs, err %d\n",
+ 				  err);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	/* alloc mtt memory */
+ 	all_pg_cnt = mtr->hem_cfg.buf_pg_count;
+ 	hns_roce_hem_list_init(&mtr->hem_list);
+ 	mtr->hem_cfg.is_direct = !has_mtt;
+ 	mtr->hem_cfg.ba_pg_shift = page_shift;
+ 	mtr->hem_cfg.region_count = 0;
+ 	region_cnt = mtr_init_region(buf_attr, all_pg_cnt,
+ 				     mtr->hem_cfg.region,
+ 				     ARRAY_SIZE(mtr->hem_cfg.region),
+ 				     mtr->hem_cfg.buf_pg_shift);
+ 	if (region_cnt < 1) {
+ 		err = -ENOBUFS;
+ 		ibdev_err(ibdev, "failed to init mtr region %d\n", region_cnt);
+ 		goto err_alloc_bufs;
+ 	}
+ 
+ 	mtr->hem_cfg.region_count = region_cnt;
+ 
+ 	if (has_mtt) {
+ 		err = hns_roce_hem_list_request(hr_dev, &mtr->hem_list,
+ 						mtr->hem_cfg.region, region_cnt,
+ 						page_shift);
+ 		if (err) {
+ 			ibdev_err(ibdev, "Failed to request mtr hem, err %d\n",
+ 				  err);
+ 			goto err_alloc_bufs;
+ 		}
+ 		mtr->hem_cfg.root_ba = mtr->hem_list.root_ba;
+ 	}
+ 
+ 	/* no buffer to map */
+ 	if (buf_attr->mtt_only)
+ 		return 0;
+ 
+ 	/* alloc a tmp array to store buffer's dma address */
+ 	pages = kvcalloc(all_pg_cnt, sizeof(dma_addr_t), GFP_KERNEL);
+ 	if (!pages) {
+ 		err = -ENOMEM;
+ 		ibdev_err(ibdev, "Failed to alloc mtr page list %d\n",
+ 			  all_pg_cnt);
+ 		goto err_alloc_hem_list;
+ 	}
+ 
+ 	get_pg_cnt = mtr_get_pages(hr_dev, mtr, pages, all_pg_cnt,
+ 				   mtr->hem_cfg.buf_pg_shift);
+ 	if (get_pg_cnt != all_pg_cnt) {
+ 		ibdev_err(ibdev, "Failed to get mtr page %d != %d\n",
+ 			  get_pg_cnt, all_pg_cnt);
+ 		err = -ENOBUFS;
+ 		goto err_alloc_page_list;
+ 	}
+ 
+ 	if (!has_mtt) {
+ 		mtr->hem_cfg.root_ba = pages[0];
+ 	} else {
+ 		/* write buffer's dma address to BA table */
+ 		err = hns_roce_mtr_map(hr_dev, mtr, pages, all_pg_cnt);
+ 		if (err) {
+ 			ibdev_err(ibdev, "Failed to map mtr pages, err %d\n",
+ 				  err);
+ 			goto err_alloc_page_list;
+ 		}
+ 	}
+ 
+ 	/* drop tmp array */
+ 	kvfree(pages);
+ 	return 0;
+ err_alloc_page_list:
+ 	kvfree(pages);
+ err_alloc_hem_list:
+ 	hns_roce_hem_list_release(hr_dev, &mtr->hem_list);
+ err_alloc_bufs:
+ 	mtr_free_bufs(hr_dev, mtr);
+ 	return err;
+ }
+ 
+ void hns_roce_mtr_destroy(struct hns_roce_dev *hr_dev, struct hns_roce_mtr *mtr)
+ {
+ 	/* release multi-hop addressing resource */
+ 	hns_roce_hem_list_release(hr_dev, &mtr->hem_list);
+ 
+ 	/* free buffers */
+ 	mtr_free_bufs(hr_dev, mtr);
+ }
++>>>>>>> 42a3b153966c (RDMA: Remove the udata parameter from alloc_mr callback)
diff --git a/drivers/infiniband/core/verbs.c b/drivers/infiniband/core/verbs.c
index 61716b7b6011..4333357d8d90 100644
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@ -2062,7 +2062,7 @@ struct ib_mr *ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
 		goto out;
 	}
 
-	mr = pd->device->ops.alloc_mr(pd, mr_type, max_num_sg, NULL);
+	mr = pd->device->ops.alloc_mr(pd, mr_type, max_num_sg);
 	if (IS_ERR(mr))
 		goto out;
 
diff --git a/drivers/infiniband/hw/bnxt_re/ib_verbs.c b/drivers/infiniband/hw/bnxt_re/ib_verbs.c
index 3ed548754f9b..a7d82b859c63 100644
--- a/drivers/infiniband/hw/bnxt_re/ib_verbs.c
+++ b/drivers/infiniband/hw/bnxt_re/ib_verbs.c
@@ -3572,7 +3572,7 @@ int bnxt_re_map_mr_sg(struct ib_mr *ib_mr, struct scatterlist *sg, int sg_nents,
 }
 
 struct ib_mr *bnxt_re_alloc_mr(struct ib_pd *ib_pd, enum ib_mr_type type,
-			       u32 max_num_sg, struct ib_udata *udata)
+			       u32 max_num_sg)
 {
 	struct bnxt_re_pd *pd = container_of(ib_pd, struct bnxt_re_pd, ib_pd);
 	struct bnxt_re_dev *rdev = pd->rdev;
diff --git a/drivers/infiniband/hw/bnxt_re/ib_verbs.h b/drivers/infiniband/hw/bnxt_re/ib_verbs.h
index 5cc187a5bc02..8abe206e1444 100644
--- a/drivers/infiniband/hw/bnxt_re/ib_verbs.h
+++ b/drivers/infiniband/hw/bnxt_re/ib_verbs.h
@@ -194,7 +194,7 @@ struct ib_mr *bnxt_re_get_dma_mr(struct ib_pd *pd, int mr_access_flags);
 int bnxt_re_map_mr_sg(struct ib_mr *ib_mr, struct scatterlist *sg, int sg_nents,
 		      unsigned int *sg_offset);
 struct ib_mr *bnxt_re_alloc_mr(struct ib_pd *ib_pd, enum ib_mr_type mr_type,
-			       u32 max_num_sg, struct ib_udata *udata);
+			       u32 max_num_sg);
 int bnxt_re_dereg_mr(struct ib_mr *mr, struct ib_udata *udata);
 struct ib_mw *bnxt_re_alloc_mw(struct ib_pd *ib_pd, enum ib_mw_type type,
 			       struct ib_udata *udata);
diff --git a/drivers/infiniband/hw/cxgb4/iw_cxgb4.h b/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
index ab603a2f475e..f00c146364c0 100644
--- a/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
+++ b/drivers/infiniband/hw/cxgb4/iw_cxgb4.h
@@ -1035,7 +1035,7 @@ int c4iw_reject_cr(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len);
 void c4iw_qp_add_ref(struct ib_qp *qp);
 void c4iw_qp_rem_ref(struct ib_qp *qp);
 struct ib_mr *c4iw_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			    u32 max_num_sg, struct ib_udata *udata);
+			    u32 max_num_sg);
 int c4iw_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 		   unsigned int *sg_offset);
 int c4iw_dealloc_mw(struct ib_mw *mw);
diff --git a/drivers/infiniband/hw/cxgb4/mem.c b/drivers/infiniband/hw/cxgb4/mem.c
index c8ea4e0aa7ba..b1702c9f01f2 100644
--- a/drivers/infiniband/hw/cxgb4/mem.c
+++ b/drivers/infiniband/hw/cxgb4/mem.c
@@ -684,7 +684,7 @@ int c4iw_dealloc_mw(struct ib_mw *mw)
 }
 
 struct ib_mr *c4iw_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			    u32 max_num_sg, struct ib_udata *udata)
+			    u32 max_num_sg)
 {
 	struct c4iw_dev *rhp;
 	struct c4iw_pd *php;
* Unmerged path drivers/infiniband/hw/hns/hns_roce_device.h
* Unmerged path drivers/infiniband/hw/hns/hns_roce_mr.c
diff --git a/drivers/infiniband/hw/i40iw/i40iw_verbs.c b/drivers/infiniband/hw/i40iw/i40iw_verbs.c
index 3f2024d7f075..f5230aa7d02a 100644
--- a/drivers/infiniband/hw/i40iw/i40iw_verbs.c
+++ b/drivers/infiniband/hw/i40iw/i40iw_verbs.c
@@ -1557,10 +1557,9 @@ static int i40iw_hw_alloc_stag(struct i40iw_device *iwdev, struct i40iw_mr *iwmr
  * @pd: ibpd pointer
  * @mr_type: memory for stag registrion
  * @max_num_sg: man number of pages
- * @udata: user data or NULL for kernel objects
  */
 static struct ib_mr *i40iw_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-				    u32 max_num_sg, struct ib_udata *udata)
+				    u32 max_num_sg)
 {
 	struct i40iw_pd *iwpd = to_iwpd(pd);
 	struct i40iw_device *iwdev = to_iwdev(pd->device);
diff --git a/drivers/infiniband/hw/mlx4/mlx4_ib.h b/drivers/infiniband/hw/mlx4/mlx4_ib.h
index ebdeffd1dcd9..8639760fe466 100644
--- a/drivers/infiniband/hw/mlx4/mlx4_ib.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib.h
@@ -729,7 +729,7 @@ struct ib_mw *mlx4_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 			       struct ib_udata *udata);
 int mlx4_ib_dealloc_mw(struct ib_mw *mw);
 struct ib_mr *mlx4_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			       u32 max_num_sg, struct ib_udata *udata);
+			       u32 max_num_sg);
 int mlx4_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 		      unsigned int *sg_offset);
 int mlx4_ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);
diff --git a/drivers/infiniband/hw/mlx4/mr.c b/drivers/infiniband/hw/mlx4/mr.c
index ae6f31fd06b5..5c055c5a4b10 100644
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@ -655,7 +655,7 @@ int mlx4_ib_dealloc_mw(struct ib_mw *ibmw)
 }
 
 struct ib_mr *mlx4_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			       u32 max_num_sg, struct ib_udata *udata)
+			       u32 max_num_sg)
 {
 	struct mlx4_ib_dev *dev = to_mdev(pd->device);
 	struct mlx4_ib_mr *mr;
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 57f36e88b609..2cf13d56e3d9 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1211,7 +1211,7 @@ int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 			  struct ib_pd *pd, struct ib_udata *udata);
 int mlx5_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata);
 struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			       u32 max_num_sg, struct ib_udata *udata);
+			       u32 max_num_sg);
 struct ib_mr *mlx5_ib_alloc_mr_integrity(struct ib_pd *pd,
 					 u32 max_num_sg,
 					 u32 max_num_meta_sg);
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index 981fcb8144fa..883dff047263 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -1847,7 +1847,7 @@ static struct ib_mr *__mlx5_ib_alloc_mr(struct ib_pd *pd,
 }
 
 struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			       u32 max_num_sg, struct ib_udata *udata)
+			       u32 max_num_sg)
 {
 	return __mlx5_ib_alloc_mr(pd, mr_type, max_num_sg, 0);
 }
diff --git a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
index 81119ba280d9..3a70d8b7ada9 100644
--- a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.c
@@ -2909,7 +2909,7 @@ int ocrdma_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags cq_flags)
 }
 
 struct ib_mr *ocrdma_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,
-			      u32 max_num_sg, struct ib_udata *udata)
+			      u32 max_num_sg)
 {
 	int status;
 	struct ocrdma_mr *mr;
diff --git a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.h b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
index 32488da1b752..aa0c982b78df 100644
--- a/drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
+++ b/drivers/infiniband/hw/ocrdma/ocrdma_verbs.h
@@ -103,7 +103,7 @@ struct ib_mr *ocrdma_get_dma_mr(struct ib_pd *, int acc);
 struct ib_mr *ocrdma_reg_user_mr(struct ib_pd *, u64 start, u64 length,
 				 u64 virt, int acc, struct ib_udata *);
 struct ib_mr *ocrdma_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			      u32 max_num_sg, struct ib_udata *udata);
+			      u32 max_num_sg);
 int ocrdma_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 		     unsigned int *sg_offset);
 
diff --git a/drivers/infiniband/hw/qedr/verbs.c b/drivers/infiniband/hw/qedr/verbs.c
index 3769f258df53..926d07b6a618 100644
--- a/drivers/infiniband/hw/qedr/verbs.c
+++ b/drivers/infiniband/hw/qedr/verbs.c
@@ -3006,7 +3006,7 @@ static struct qedr_mr *__qedr_alloc_mr(struct ib_pd *ibpd,
 }
 
 struct ib_mr *qedr_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,
-			    u32 max_num_sg, struct ib_udata *udata)
+			    u32 max_num_sg)
 {
 	struct qedr_mr *mr;
 
diff --git a/drivers/infiniband/hw/qedr/verbs.h b/drivers/infiniband/hw/qedr/verbs.h
index 18027844eb87..2c119ec1a64e 100644
--- a/drivers/infiniband/hw/qedr/verbs.h
+++ b/drivers/infiniband/hw/qedr/verbs.h
@@ -84,7 +84,7 @@ int qedr_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,
 		   int sg_nents, unsigned int *sg_offset);
 
 struct ib_mr *qedr_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			    u32 max_num_sg, struct ib_udata *udata);
+			    u32 max_num_sg);
 int qedr_poll_cq(struct ib_cq *, int num_entries, struct ib_wc *wc);
 int qedr_post_send(struct ib_qp *, const struct ib_send_wr *,
 		   const struct ib_send_wr **bad_wr);
diff --git a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_mr.c b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_mr.c
index c61e665ff261..fde0c326d5e2 100644
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_mr.c
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_mr.c
@@ -202,7 +202,7 @@ struct ib_mr *pvrdma_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
  * @return: ib_mr pointer on success, otherwise returns an errno.
  */
 struct ib_mr *pvrdma_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			      u32 max_num_sg, struct ib_udata *udata)
+			      u32 max_num_sg)
 {
 	struct pvrdma_dev *dev = to_vdev(pd->device);
 	struct pvrdma_user_mr *mr;
diff --git a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
index e4a48f5c0c85..9f88f43d8157 100644
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.h
@@ -406,7 +406,7 @@ struct ib_mr *pvrdma_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				 struct ib_udata *udata);
 int pvrdma_dereg_mr(struct ib_mr *mr, struct ib_udata *udata);
 struct ib_mr *pvrdma_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			      u32 max_num_sg, struct ib_udata *udata);
+			      u32 max_num_sg);
 int pvrdma_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,
 		     int sg_nents, unsigned int *sg_offset);
 int pvrdma_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
diff --git a/drivers/infiniband/sw/rdmavt/mr.c b/drivers/infiniband/sw/rdmavt/mr.c
index 7222d72cd32b..9b67dfe709be 100644
--- a/drivers/infiniband/sw/rdmavt/mr.c
+++ b/drivers/infiniband/sw/rdmavt/mr.c
@@ -577,7 +577,7 @@ int rvt_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
  * Return: the memory region on success, otherwise return an errno.
  */
 struct ib_mr *rvt_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			   u32 max_num_sg, struct ib_udata *udata)
+			   u32 max_num_sg)
 {
 	struct rvt_mr *mr;
 
diff --git a/drivers/infiniband/sw/rdmavt/mr.h b/drivers/infiniband/sw/rdmavt/mr.h
index 780fc63af98b..b3aba359401b 100644
--- a/drivers/infiniband/sw/rdmavt/mr.h
+++ b/drivers/infiniband/sw/rdmavt/mr.h
@@ -71,7 +71,7 @@ struct ib_mr *rvt_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 			      struct ib_udata *udata);
 int rvt_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata);
 struct ib_mr *rvt_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			   u32 max_num_sg, struct ib_udata *udata);
+			   u32 max_num_sg);
 int rvt_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,
 		  int sg_nents, unsigned int *sg_offset);
 
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.c b/drivers/infiniband/sw/rxe/rxe_verbs.c
index 9dd4bd7aea92..e2c47195a187 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@ -974,7 +974,7 @@ static int rxe_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
 }
 
 static struct ib_mr *rxe_alloc_mr(struct ib_pd *ibpd, enum ib_mr_type mr_type,
-				  u32 max_num_sg, struct ib_udata *udata)
+				  u32 max_num_sg)
 {
 	struct rxe_dev *rxe = to_rdev(ibpd->device);
 	struct rxe_pd *pd = to_rpd(ibpd);
diff --git a/drivers/infiniband/sw/siw/siw_verbs.c b/drivers/infiniband/sw/siw/siw_verbs.c
index d5390d498c61..e81a143b786c 100644
--- a/drivers/infiniband/sw/siw/siw_verbs.c
+++ b/drivers/infiniband/sw/siw/siw_verbs.c
@@ -1374,7 +1374,7 @@ struct ib_mr *siw_reg_user_mr(struct ib_pd *pd, u64 start, u64 len,
 }
 
 struct ib_mr *siw_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
-			   u32 max_sge, struct ib_udata *udata)
+			   u32 max_sge)
 {
 	struct siw_device *sdev = to_siw_dev(pd->device);
 	struct siw_mr *mr = NULL;
diff --git a/drivers/infiniband/sw/siw/siw_verbs.h b/drivers/infiniband/sw/siw/siw_verbs.h
index 1a731989fad6..9335c48c01de 100644
--- a/drivers/infiniband/sw/siw/siw_verbs.h
+++ b/drivers/infiniband/sw/siw/siw_verbs.h
@@ -69,7 +69,7 @@ int siw_req_notify_cq(struct ib_cq *base_cq, enum ib_cq_notify_flags flags);
 struct ib_mr *siw_reg_user_mr(struct ib_pd *base_pd, u64 start, u64 len,
 			      u64 rnic_va, int rights, struct ib_udata *udata);
 struct ib_mr *siw_alloc_mr(struct ib_pd *base_pd, enum ib_mr_type mr_type,
-			   u32 max_sge, struct ib_udata *udata);
+			   u32 max_sge);
 struct ib_mr *siw_get_dma_mr(struct ib_pd *base_pd, int rights);
 int siw_map_mr_sg(struct ib_mr *base_mr, struct scatterlist *sl, int num_sle,
 		  unsigned int *sg_off);
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d8b936de6087..0ab6365757dc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2476,7 +2476,7 @@ struct ib_device_ops {
 			     struct ib_pd *pd, struct ib_udata *udata);
 	int (*dereg_mr)(struct ib_mr *mr, struct ib_udata *udata);
 	struct ib_mr *(*alloc_mr)(struct ib_pd *pd, enum ib_mr_type mr_type,
-				  u32 max_num_sg, struct ib_udata *udata);
+				  u32 max_num_sg);
 	struct ib_mr *(*alloc_mr_integrity)(struct ib_pd *pd,
 					    u32 max_num_data_sg,
 					    u32 max_num_meta_sg);
