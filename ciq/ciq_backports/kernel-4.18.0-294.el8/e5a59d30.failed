mm/khugepaged.c: fix khugepaged's request size in collapse_file

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author David Howells <dhowells@redhat.com>
commit e5a59d308f52bb0052af5790c22173651b187465
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/e5a59d30.failed

collapse_file() in khugepaged passes PAGE_SIZE as the number of pages to
be read to page_cache_sync_readahead().  The intent was probably to read
a single page.  Fix it to use the number of pages to the end of the
window instead.

Fixes: 99cb0dbd47a1 ("mm,thp: add read-only THP support for (non-shmem) FS")
	Signed-off-by: David Howells <dhowells@redhat.com>
	Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Acked-by: Song Liu <songliubraving@fb.com>
	Acked-by: Yang Shi <shy828301@gmail.com>
	Acked-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
	Cc: Eric Biggers <ebiggers@google.com>
Link: https://lkml.kernel.org/r/20200903140844.14194-2-willy@infradead.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e5a59d308f52bb0052af5790c22173651b187465)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
diff --cc mm/khugepaged.c
index da8945bc784c,cfa0dba5fd3b..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1337,60 -1634,114 +1337,107 @@@ static void collapse_file(struct mm_str
  		result = SCAN_CGROUP_CHARGE_FAIL;
  		goto out;
  	}
 -	count_memcg_page_event(new_page, THP_COLLAPSE_ALLOC);
 -
 -	/* This will be less messy when we use multi-index entries */
 -	do {
 -		xas_lock_irq(&xas);
 -		xas_create_range(&xas);
 -		if (!xas_error(&xas))
 -			break;
 -		xas_unlock_irq(&xas);
 -		if (!xas_nomem(&xas, GFP_KERNEL)) {
 -			result = SCAN_FAIL;
 -			goto out;
 -		}
 -	} while (1);
  
 -	__SetPageLocked(new_page);
 -	if (is_shmem)
 -		__SetPageSwapBacked(new_page);
  	new_page->index = start;
  	new_page->mapping = mapping;
 +	__SetPageSwapBacked(new_page);
 +	__SetPageLocked(new_page);
 +	BUG_ON(!page_ref_freeze(new_page, 1));
 +
  
  	/*
 -	 * At this point the new_page is locked and not up-to-date.
 -	 * It's safe to insert it into the page cache, because nobody would
 -	 * be able to map it or use it in another way until we unlock it.
 +	 * At this point the new_page is 'frozen' (page_count() is zero), locked
 +	 * and not up-to-date. It's safe to insert it into radix tree, because
 +	 * nobody would be able to map it or use it in other way until we
 +	 * unfreeze it.
  	 */
  
 -	xas_set(&xas, start);
 -	for (index = start; index < end; index++) {
 -		struct page *page = xas_next(&xas);
 +	index = start;
 +	xa_lock_irq(&mapping->i_pages);
 +	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start) {
 +		int n = min(iter.index, end) - index;
  
 -		VM_BUG_ON(index != xas.xa_index);
 -		if (is_shmem) {
 -			if (!page) {
 -				/*
 -				 * Stop if extent has been truncated or
 -				 * hole-punched, and is now completely
 -				 * empty.
 -				 */
 -				if (index == start) {
 -					if (!xas_next_entry(&xas, end - 1)) {
 -						result = SCAN_TRUNCATED;
 -						goto xa_locked;
 -					}
 -					xas_set(&xas, index);
 -				}
 -				if (!shmem_charge(mapping->host, 1)) {
 -					result = SCAN_FAIL;
 -					goto xa_locked;
 -				}
 -				xas_store(&xas, new_page);
 -				nr_none++;
 -				continue;
 -			}
 +		/*
 +		 * Handle holes in the radix tree: charge it from shmem and
 +		 * insert relevant subpage of new_page into the radix-tree.
 +		 */
 +		if (n && !shmem_charge(mapping->host, n)) {
 +			result = SCAN_FAIL;
 +			break;
 +		}
 +		nr_none += n;
 +		for (; index < min(iter.index, end); index++) {
 +			radix_tree_insert(&mapping->i_pages, index,
 +					new_page + (index % HPAGE_PMD_NR));
 +		}
 +
++<<<<<<< HEAD
 +		/* We are done. */
 +		if (index >= end)
 +			break;
  
 +		page = radix_tree_deref_slot_protected(slot,
 +				&mapping->i_pages.xa_lock);
 +		if (xa_is_value(page) || !PageUptodate(page)) {
 +			xa_unlock_irq(&mapping->i_pages);
 +			/* swap in or instantiate fallocated page */
 +			if (shmem_getpage(mapping->host, index, &page,
 +						SGP_NOHUGE)) {
++=======
+ 			if (xa_is_value(page) || !PageUptodate(page)) {
+ 				xas_unlock_irq(&xas);
+ 				/* swap in or instantiate fallocated page */
+ 				if (shmem_getpage(mapping->host, index, &page,
+ 						  SGP_NOHUGE)) {
+ 					result = SCAN_FAIL;
+ 					goto xa_unlocked;
+ 				}
+ 			} else if (trylock_page(page)) {
+ 				get_page(page);
+ 				xas_unlock_irq(&xas);
+ 			} else {
+ 				result = SCAN_PAGE_LOCK;
+ 				goto xa_locked;
+ 			}
+ 		} else {	/* !is_shmem */
+ 			if (!page || xa_is_value(page)) {
+ 				xas_unlock_irq(&xas);
+ 				page_cache_sync_readahead(mapping, &file->f_ra,
+ 							  file, index,
+ 							  end - index);
+ 				/* drain pagevecs to help isolate_lru_page() */
+ 				lru_add_drain();
+ 				page = find_lock_page(mapping, index);
+ 				if (unlikely(page == NULL)) {
+ 					result = SCAN_FAIL;
+ 					goto xa_unlocked;
+ 				}
+ 			} else if (PageDirty(page)) {
+ 				/*
+ 				 * khugepaged only works on read-only fd,
+ 				 * so this page is dirty because it hasn't
+ 				 * been flushed since first write. There
+ 				 * won't be new dirty pages.
+ 				 *
+ 				 * Trigger async flush here and hope the
+ 				 * writeback is done when khugepaged
+ 				 * revisits this page.
+ 				 *
+ 				 * This is a one-off situation. We are not
+ 				 * forcing writeback in loop.
+ 				 */
+ 				xas_unlock_irq(&xas);
+ 				filemap_flush(mapping);
++>>>>>>> e5a59d308f52 (mm/khugepaged.c: fix khugepaged's request size in collapse_file)
  				result = SCAN_FAIL;
 -				goto xa_unlocked;
 -			} else if (trylock_page(page)) {
 -				get_page(page);
 -				xas_unlock_irq(&xas);
 -			} else {
 -				result = SCAN_PAGE_LOCK;
 -				goto xa_locked;
 +				goto tree_unlocked;
  			}
 +			xa_lock_irq(&mapping->i_pages);
 +		} else if (trylock_page(page)) {
 +			get_page(page);
 +		} else {
 +			result = SCAN_PAGE_LOCK;
 +			break;
  		}
  
  		/*
* Unmerged path mm/khugepaged.c
