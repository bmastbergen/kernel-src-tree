RDMA/mlx5: Refactor mlx5_ib_alloc_ucontext() response

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yishai Hadas <yishaih@mellanox.com>
commit 45ec21c971eddfb5e8e953e49a9dbe780f4a4997
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/45ec21c9.failed

Refactor mlx5_ib_alloc_ucontext() to set its response fields in a
cleaner way.

It includes,
- Move the relevant code to a self contained function.
- Calculate the response length once and drop redundant code all around.
- Reuse previously set ucontext fields once preparing the response.

The self contained function will be used in next patch as part of
implementing the query ucontext functionality.

Link: https://lore.kernel.org/r/20200630093916.332097-5-leon@kernel.org
	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 45ec21c971eddfb5e8e953e49a9dbe780f4a4997)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index dd63b26ba3a7,56039fa6c530..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -1781,8 -1864,8 +1866,7 @@@ static int mlx5_ib_alloc_ucontext(struc
  	int err;
  	size_t min_req_v2 = offsetof(struct mlx5_ib_alloc_ucontext_req_v2,
  				     max_cqe_version);
- 	u32 dump_fill_mkey;
  	bool lib_uar_4k;
 -	bool lib_uar_dyn;
  
  	if (!dev->ib_active)
  		return -EAGAIN;
@@@ -1809,40 -1892,15 +1893,9 @@@
  	if (req.num_low_latency_bfregs > req.total_num_bfregs - 1)
  		return -EINVAL;
  
- 	resp.qp_tab_size = 1 << MLX5_CAP_GEN(dev->mdev, log_max_qp);
- 	if (dev->wc_support)
- 		resp.bf_reg_size = 1 << MLX5_CAP_GEN(dev->mdev, log_bf_reg_size);
- 	resp.cache_line_size = cache_line_size();
- 	resp.max_sq_desc_sz = MLX5_CAP_GEN(dev->mdev, max_wqe_sz_sq);
- 	resp.max_rq_desc_sz = MLX5_CAP_GEN(dev->mdev, max_wqe_sz_rq);
- 	resp.max_send_wqebb = 1 << MLX5_CAP_GEN(dev->mdev, log_max_qp_sz);
- 	resp.max_recv_wr = 1 << MLX5_CAP_GEN(dev->mdev, log_max_qp_sz);
- 	resp.max_srq_recv_wr = 1 << MLX5_CAP_GEN(dev->mdev, log_max_srq_sz);
- 	resp.cqe_version = min_t(__u8,
- 				 (__u8)MLX5_CAP_GEN(dev->mdev, cqe_version),
- 				 req.max_cqe_version);
- 	resp.log_uar_size = MLX5_CAP_GEN(dev->mdev, uar_4k) ?
- 				MLX5_ADAPTER_PAGE_SHIFT : PAGE_SHIFT;
- 	resp.num_uars_per_page = MLX5_CAP_GEN(dev->mdev, uar_4k) ?
- 					MLX5_CAP_GEN(dev->mdev, num_of_uars_per_page) : 1;
- 	resp.response_length = min(offsetof(typeof(resp), response_length) +
- 				   sizeof(resp.response_length), udata->outlen);
- 
- 	if (mlx5_accel_ipsec_device_caps(dev->mdev) & MLX5_ACCEL_IPSEC_CAP_DEVICE) {
- 		if (mlx5_get_flow_namespace(dev->mdev, MLX5_FLOW_NAMESPACE_EGRESS))
- 			resp.flow_action_flags |= MLX5_USER_ALLOC_UCONTEXT_FLOW_ACTION_FLAGS_ESP_AES_GCM;
- 		if (mlx5_accel_ipsec_device_caps(dev->mdev) & MLX5_ACCEL_IPSEC_CAP_REQUIRED_METADATA)
- 			resp.flow_action_flags |= MLX5_USER_ALLOC_UCONTEXT_FLOW_ACTION_FLAGS_ESP_AES_GCM_REQ_METADATA;
- 		if (MLX5_CAP_FLOWTABLE(dev->mdev, flow_table_properties_nic_receive.ft_field_support.outer_esp_spi))
- 			resp.flow_action_flags |= MLX5_USER_ALLOC_UCONTEXT_FLOW_ACTION_FLAGS_ESP_AES_GCM_SPI_STEERING;
- 		if (mlx5_accel_ipsec_device_caps(dev->mdev) & MLX5_ACCEL_IPSEC_CAP_TX_IV_IS_ESN)
- 			resp.flow_action_flags |= MLX5_USER_ALLOC_UCONTEXT_FLOW_ACTION_FLAGS_ESP_AES_GCM_TX_IV_IS_ESN;
- 		/* MLX5_USER_ALLOC_UCONTEXT_FLOW_ACTION_FLAGS_ESP_AES_GCM_FULL_OFFLOAD is currently always 0 */
- 	}
- 
  	lib_uar_4k = req.lib_caps & MLX5_LIB_CAP_4K_UAR;
 -	lib_uar_dyn = req.lib_caps & MLX5_LIB_CAP_DYN_UAR;
  	bfregi = &context->bfregi;
  
 -	if (lib_uar_dyn) {
 -		bfregi->lib_uar_dyn = lib_uar_dyn;
 -		goto uar_done;
 -	}
 -
  	/* updates req->total_num_bfregs */
  	err = calc_total_bfregs(dev, lib_uar_4k, &req, bfregi);
  	if (err)
@@@ -1884,74 -1943,15 +1937,80 @@@
  	INIT_LIST_HEAD(&context->db_page_list);
  	mutex_init(&context->db_page_mutex);
  
++<<<<<<< HEAD
 +	resp.tot_bfregs = req.total_num_bfregs;
 +	resp.num_ports = dev->num_ports;
 +
 +	if (offsetofend(typeof(resp), cqe_version) <= udata->outlen)
 +		resp.response_length += sizeof(resp.cqe_version);
 +
 +	if (offsetofend(typeof(resp), cmds_supp_uhw) <= udata->outlen) {
 +		resp.cmds_supp_uhw |= MLX5_USER_CMDS_SUPP_UHW_QUERY_DEVICE |
 +				      MLX5_USER_CMDS_SUPP_UHW_CREATE_AH;
 +		resp.response_length += sizeof(resp.cmds_supp_uhw);
 +	}
 +
 +	if (offsetofend(typeof(resp), eth_min_inline) <= udata->outlen) {
 +		if (mlx5_ib_port_link_layer(ibdev, 1) == IB_LINK_LAYER_ETHERNET) {
 +			mlx5_query_min_inline(dev->mdev, &resp.eth_min_inline);
 +			resp.eth_min_inline++;
 +		}
 +		resp.response_length += sizeof(resp.eth_min_inline);
 +	}
 +
 +	if (offsetofend(typeof(resp), clock_info_versions) <= udata->outlen) {
 +		if (mdev->clock_info)
 +			resp.clock_info_versions = BIT(MLX5_IB_CLOCK_INFO_V1);
 +		resp.response_length += sizeof(resp.clock_info_versions);
 +	}
 +
 +	/*
 +	 * We don't want to expose information from the PCI bar that is located
 +	 * after 4096 bytes, so if the arch only supports larger pages, let's
 +	 * pretend we don't support reading the HCA's core clock. This is also
 +	 * forced by mmap function.
 +	 */
 +	if (offsetofend(typeof(resp), hca_core_clock_offset) <= udata->outlen) {
 +		if (PAGE_SIZE <= 4096) {
 +			resp.comp_mask |=
 +				MLX5_IB_ALLOC_UCONTEXT_RESP_MASK_CORE_CLOCK_OFFSET;
 +			resp.hca_core_clock_offset =
 +				offsetof(struct mlx5_init_seg, internal_timer_h) % PAGE_SIZE;
 +		}
 +		resp.response_length += sizeof(resp.hca_core_clock_offset);
 +	}
 +
 +	if (offsetofend(typeof(resp), log_uar_size) <= udata->outlen)
 +		resp.response_length += sizeof(resp.log_uar_size);
 +
 +	if (offsetofend(typeof(resp), num_uars_per_page) <= udata->outlen)
 +		resp.response_length += sizeof(resp.num_uars_per_page);
 +
 +	if (offsetofend(typeof(resp), num_dyn_bfregs) <= udata->outlen) {
 +		resp.num_dyn_bfregs = bfregi->num_dyn_bfregs;
 +		resp.response_length += sizeof(resp.num_dyn_bfregs);
 +	}
 +
 +	if (offsetofend(typeof(resp), dump_fill_mkey) <= udata->outlen) {
 +		if (MLX5_CAP_GEN(dev->mdev, dump_fill_mkey)) {
 +			resp.dump_fill_mkey = dump_fill_mkey;
 +			resp.comp_mask |=
 +				MLX5_IB_ALLOC_UCONTEXT_RESP_MASK_DUMP_FILL_MKEY;
 +		}
 +		resp.response_length += sizeof(resp.dump_fill_mkey);
 +	}
 +
++=======
+ 	context->cqe_version = min_t(__u8,
+ 				 (__u8)MLX5_CAP_GEN(dev->mdev, cqe_version),
+ 				 req.max_cqe_version);
+ 
+ 	err = set_ucontext_resp(uctx, &resp);
+ 	if (err)
+ 		goto out_mdev;
+ 
+ 	resp.response_length = min(udata->outlen, sizeof(resp));
++>>>>>>> 45ec21c971ed (RDMA/mlx5: Refactor mlx5_ib_alloc_ucontext() response)
  	err = ib_copy_to_udata(udata, &resp, resp.response_length);
  	if (err)
  		goto out_mdev;
* Unmerged path drivers/infiniband/hw/mlx5/main.c
