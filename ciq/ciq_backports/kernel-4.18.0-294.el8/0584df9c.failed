lockdep: Refactor IRQ trace events fields into struct

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Marco Elver <elver@google.com>
commit 0584df9c12f449124d0bfef9899e5365604ee7a9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/0584df9c.failed

Refactor the IRQ trace events fields, used for printing information
about the IRQ trace events, into a separate struct 'irqtrace_events'.

This improves readability by separating the information only used in
reporting, as well as enables (simplified) storing/restoring of
irqtrace_events snapshots.

No functional change intended.

	Signed-off-by: Marco Elver <elver@google.com>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20200729110916.3920464-1-elver@google.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 0584df9c12f449124d0bfef9899e5365604ee7a9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/irqflags.h
#	include/linux/sched.h
#	kernel/fork.c
#	kernel/locking/lockdep.c
diff --cc include/linux/irqflags.h
index b53a9f136087,bd5c55755447..000000000000
--- a/include/linux/irqflags.h
+++ b/include/linux/irqflags.h
@@@ -14,22 -14,55 +14,44 @@@
  
  #include <linux/typecheck.h>
  #include <asm/irqflags.h>
 -#include <asm/percpu.h>
 -
 -/* Currently lockdep_softirqs_on/off is used only by lockdep */
 -#ifdef CONFIG_PROVE_LOCKING
 -  extern void lockdep_softirqs_on(unsigned long ip);
 -  extern void lockdep_softirqs_off(unsigned long ip);
 -  extern void lockdep_hardirqs_on_prepare(unsigned long ip);
 -  extern void lockdep_hardirqs_on(unsigned long ip);
 -  extern void lockdep_hardirqs_off(unsigned long ip);
 -#else
 -  static inline void lockdep_softirqs_on(unsigned long ip) { }
 -  static inline void lockdep_softirqs_off(unsigned long ip) { }
 -  static inline void lockdep_hardirqs_on_prepare(unsigned long ip) { }
 -  static inline void lockdep_hardirqs_on(unsigned long ip) { }
 -  static inline void lockdep_hardirqs_off(unsigned long ip) { }
 -#endif
  
  #ifdef CONFIG_TRACE_IRQFLAGS
++<<<<<<< HEAD
 +  extern void trace_softirqs_on(unsigned long ip);
 +  extern void trace_softirqs_off(unsigned long ip);
++=======
+ 
+ /* Per-task IRQ trace events information. */
+ struct irqtrace_events {
+ 	unsigned int	irq_events;
+ 	unsigned long	hardirq_enable_ip;
+ 	unsigned long	hardirq_disable_ip;
+ 	unsigned int	hardirq_enable_event;
+ 	unsigned int	hardirq_disable_event;
+ 	unsigned long	softirq_disable_ip;
+ 	unsigned long	softirq_enable_ip;
+ 	unsigned int	softirq_disable_event;
+ 	unsigned int	softirq_enable_event;
+ };
+ 
+ DECLARE_PER_CPU(int, hardirqs_enabled);
+ DECLARE_PER_CPU(int, hardirq_context);
+ 
+   extern void trace_hardirqs_on_prepare(void);
+   extern void trace_hardirqs_off_finish(void);
++>>>>>>> 0584df9c12f4 (lockdep: Refactor IRQ trace events fields into struct)
    extern void trace_hardirqs_on(void);
    extern void trace_hardirqs_off(void);
 -# define lockdep_hardirq_context()	(this_cpu_read(hardirq_context))
 -# define lockdep_softirq_context(p)	((p)->softirq_context)
 -# define lockdep_hardirqs_enabled()	(this_cpu_read(hardirqs_enabled))
 -# define lockdep_softirqs_enabled(p)	((p)->softirqs_enabled)
 -# define lockdep_hardirq_enter()			\
 -do {							\
 -	if (this_cpu_inc_return(hardirq_context) == 1)	\
 -		current->hardirq_threaded = 0;		\
 +# define trace_hardirq_context(p)	((p)->hardirq_context)
 +# define trace_softirq_context(p)	((p)->softirq_context)
 +# define trace_hardirqs_enabled(p)	((p)->hardirqs_enabled)
 +# define trace_softirqs_enabled(p)	((p)->softirqs_enabled)
 +# define trace_hardirq_enter()			\
 +do {						\
 +	if (!current->hardirq_context++)	\
 +		current->hardirq_threaded = 0;	\
  } while (0)
 -# define lockdep_hardirq_threaded()		\
 +# define trace_hardirq_threaded()		\
  do {						\
  	current->hardirq_threaded = 1;		\
  } while (0)
diff --cc include/linux/sched.h
index 6a1641d92452,52e0fdd6a555..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -936,19 -976,14 +937,23 @@@ struct task_struct 
  	struct mutex_waiter		*blocked_on;
  #endif
  
 -#ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 -	int				non_block_count;
 -#endif
 -
  #ifdef CONFIG_TRACE_IRQFLAGS
- 	unsigned int			irq_events;
+ 	struct irqtrace_events		irqtrace;
  	unsigned int			hardirq_threaded;
++<<<<<<< HEAD
 +	unsigned long			hardirq_enable_ip;
 +	unsigned long			hardirq_disable_ip;
 +	unsigned int			hardirq_enable_event;
 +	unsigned int			hardirq_disable_event;
 +	int				hardirqs_enabled;
 +	int				hardirq_context;
 +	unsigned long			softirq_disable_ip;
 +	unsigned long			softirq_enable_ip;
 +	unsigned int			softirq_disable_event;
 +	unsigned int			softirq_enable_event;
++=======
+ 	u64				hardirq_chain_key;
++>>>>>>> 0584df9c12f4 (lockdep: Refactor IRQ trace events fields into struct)
  	int				softirqs_enabled;
  	int				softirq_context;
  	int				irq_config;
diff --cc kernel/fork.c
index 15627b0e0e5d,56a640799680..000000000000
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@@ -1879,19 -2035,11 +1879,27 @@@ static __latent_entropy struct task_str
  	seqcount_init(&p->mems_allowed_seq);
  #endif
  #ifdef CONFIG_TRACE_IRQFLAGS
++<<<<<<< HEAD
 +	p->irq_events = 0;
 +	p->hardirqs_enabled = 0;
 +	p->hardirq_enable_ip = 0;
 +	p->hardirq_enable_event = 0;
 +	p->hardirq_disable_ip = _THIS_IP_;
 +	p->hardirq_disable_event = 0;
 +	p->softirqs_enabled = 1;
 +	p->softirq_enable_ip = _THIS_IP_;
 +	p->softirq_enable_event = 0;
 +	p->softirq_disable_ip = 0;
 +	p->softirq_disable_event = 0;
 +	p->hardirq_context = 0;
 +	p->softirq_context = 0;
++=======
+ 	memset(&p->irqtrace, 0, sizeof(p->irqtrace));
+ 	p->irqtrace.hardirq_disable_ip	= _THIS_IP_;
+ 	p->irqtrace.softirq_enable_ip	= _THIS_IP_;
+ 	p->softirqs_enabled		= 1;
+ 	p->softirq_context		= 0;
++>>>>>>> 0584df9c12f4 (lockdep: Refactor IRQ trace events fields into struct)
  #endif
  
  	p->pagefault_disabled = 0;
diff --cc kernel/locking/lockdep.c
index 908dcc016f45,7b5800374c40..000000000000
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@@ -3682,31 -3688,92 +3684,95 @@@ __visible void trace_hardirqs_on_caller
  	 * Can't allow enabling interrupts while in an interrupt handler,
  	 * that's general bad form and such. Recursion, limited stack etc..
  	 */
 -	if (DEBUG_LOCKS_WARN_ON(lockdep_hardirq_context()))
 +	if (DEBUG_LOCKS_WARN_ON(current->hardirq_context))
  		return;
  
 -	current->hardirq_chain_key = current->curr_chain_key;
 -
  	current->lockdep_recursion++;
 -	__trace_hardirqs_on_caller();
 +	__trace_hardirqs_on_caller(ip);
  	lockdep_recursion_finish();
  }
 -EXPORT_SYMBOL_GPL(lockdep_hardirqs_on_prepare);
 +EXPORT_SYMBOL(trace_hardirqs_on_caller);
  
 -void noinstr lockdep_hardirqs_on(unsigned long ip)
 +void trace_hardirqs_on(void)
  {
++<<<<<<< HEAD
 +	trace_hardirqs_on_caller(CALLER_ADDR0);
++=======
+ 	struct irqtrace_events *trace = &current->irqtrace;
+ 
+ 	if (unlikely(!debug_locks))
+ 		return;
+ 
+ 	/*
+ 	 * NMIs can happen in the middle of local_irq_{en,dis}able() where the
+ 	 * tracking state and hardware state are out of sync.
+ 	 *
+ 	 * NMIs must save lockdep_hardirqs_enabled() to restore IRQ state from,
+ 	 * and not rely on hardware state like normal interrupts.
+ 	 */
+ 	if (unlikely(in_nmi())) {
+ 		/*
+ 		 * Skip:
+ 		 *  - recursion check, because NMI can hit lockdep;
+ 		 *  - hardware state check, because above;
+ 		 *  - chain_key check, see lockdep_hardirqs_on_prepare().
+ 		 */
+ 		goto skip_checks;
+ 	}
+ 
+ 	if (unlikely(current->lockdep_recursion & LOCKDEP_RECURSION_MASK))
+ 		return;
+ 
+ 	if (lockdep_hardirqs_enabled()) {
+ 		/*
+ 		 * Neither irq nor preemption are disabled here
+ 		 * so this is racy by nature but losing one hit
+ 		 * in a stat is not a big deal.
+ 		 */
+ 		__debug_atomic_inc(redundant_hardirqs_on);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * We're enabling irqs and according to our state above irqs weren't
+ 	 * already enabled, yet we find the hardware thinks they are in fact
+ 	 * enabled.. someone messed up their IRQ state tracing.
+ 	 */
+ 	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+ 		return;
+ 
+ 	/*
+ 	 * Ensure the lock stack remained unchanged between
+ 	 * lockdep_hardirqs_on_prepare() and lockdep_hardirqs_on().
+ 	 */
+ 	DEBUG_LOCKS_WARN_ON(current->hardirq_chain_key !=
+ 			    current->curr_chain_key);
+ 
+ skip_checks:
+ 	/* we'll do an OFF -> ON transition: */
+ 	this_cpu_write(hardirqs_enabled, 1);
+ 	trace->hardirq_enable_ip = ip;
+ 	trace->hardirq_enable_event = ++trace->irq_events;
+ 	debug_atomic_inc(hardirqs_on_events);
++>>>>>>> 0584df9c12f4 (lockdep: Refactor IRQ trace events fields into struct)
  }
 -EXPORT_SYMBOL_GPL(lockdep_hardirqs_on);
 +EXPORT_SYMBOL(trace_hardirqs_on);
  
  /*
   * Hardirqs were disabled:
   */
 -void noinstr lockdep_hardirqs_off(unsigned long ip)
 +__visible void trace_hardirqs_off_caller(unsigned long ip)
  {
++<<<<<<< HEAD
 +	struct task_struct *curr = current;
 +
 +	time_hardirqs_off(CALLER_ADDR0, ip);
++=======
+ 	if (unlikely(!debug_locks))
+ 		return;
++>>>>>>> 0584df9c12f4 (lockdep: Refactor IRQ trace events fields into struct)
  
 -	/*
 -	 * Matching lockdep_hardirqs_on(), allow NMIs in the middle of lockdep;
 -	 * they will restore the software state. This ensures the software
 -	 * state is consistent inside NMIs as well.
 -	 */
 -	if (unlikely(!in_nmi() && (current->lockdep_recursion & LOCKDEP_RECURSION_MASK)))
 +	if (unlikely(!debug_locks || current->lockdep_recursion))
  		return;
  
  	/*
@@@ -3716,31 -3783,28 +3782,43 @@@
  	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
  		return;
  
++<<<<<<< HEAD
 +	if (curr->hardirqs_enabled) {
 +		/*
 +		 * We have done an ON -> OFF transition:
 +		 */
 +		curr->hardirqs_enabled = 0;
 +		curr->hardirq_disable_ip = ip;
 +		curr->hardirq_disable_event = ++curr->irq_events;
++=======
+ 	if (lockdep_hardirqs_enabled()) {
+ 		struct irqtrace_events *trace = &current->irqtrace;
+ 
+ 		/*
+ 		 * We have done an ON -> OFF transition:
+ 		 */
+ 		this_cpu_write(hardirqs_enabled, 0);
+ 		trace->hardirq_disable_ip = ip;
+ 		trace->hardirq_disable_event = ++trace->irq_events;
++>>>>>>> 0584df9c12f4 (lockdep: Refactor IRQ trace events fields into struct)
  		debug_atomic_inc(hardirqs_off_events);
 -	} else {
 +	} else
  		debug_atomic_inc(redundant_hardirqs_off);
 -	}
  }
 -EXPORT_SYMBOL_GPL(lockdep_hardirqs_off);
 +EXPORT_SYMBOL(trace_hardirqs_off_caller);
 +
 +void trace_hardirqs_off(void)
 +{
 +	trace_hardirqs_off_caller(CALLER_ADDR0);
 +}
 +EXPORT_SYMBOL(trace_hardirqs_off);
  
  /*
   * Softirqs will be enabled:
   */
 -void lockdep_softirqs_on(unsigned long ip)
 +void trace_softirqs_on(unsigned long ip)
  {
- 	struct task_struct *curr = current;
+ 	struct irqtrace_events *trace = &current->irqtrace;
  
  	if (unlikely(!debug_locks || current->lockdep_recursion))
  		return;
@@@ -3770,18 -3834,16 +3848,21 @@@
  	 * usage bit for all held locks, if hardirqs are
  	 * enabled too:
  	 */
++<<<<<<< HEAD
 +	if (curr->hardirqs_enabled)
 +		mark_held_locks(curr, LOCK_ENABLED_SOFTIRQ);
++=======
+ 	if (lockdep_hardirqs_enabled())
+ 		mark_held_locks(current, LOCK_ENABLED_SOFTIRQ);
++>>>>>>> 0584df9c12f4 (lockdep: Refactor IRQ trace events fields into struct)
  	lockdep_recursion_finish();
  }
  
  /*
   * Softirqs were disabled:
   */
 -void lockdep_softirqs_off(unsigned long ip)
 +void trace_softirqs_off(unsigned long ip)
  {
- 	struct task_struct *curr = current;
- 
  	if (unlikely(!debug_locks || current->lockdep_recursion))
  		return;
  
* Unmerged path include/linux/irqflags.h
* Unmerged path include/linux/sched.h
* Unmerged path kernel/fork.c
* Unmerged path kernel/locking/lockdep.c
