mm: introduce Reported pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Alexander Duyck <alexander.h.duyck@linux.intel.com>
commit 36e66c554b5c6a9d17a229faca7a61693527b0bd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/36e66c55.failed

In order to pave the way for free page reporting in virtualized
environments we will need a way to get pages out of the free lists and
identify those pages after they have been returned.  To accomplish this,
this patch adds the concept of a Reported Buddy, which is essentially
meant to just be the Uptodate flag used in conjunction with the Buddy page
type.

To prevent the reported pages from leaking outside of the buddy lists I
added a check to clear the PageReported bit in the del_page_from_free_list
function.  As a result any reported page that is split, merged, or
allocated will have the flag cleared prior to the PageBuddy value being
cleared.

The process for reporting pages is fairly simple.  Once we free a page
that meets the minimum order for page reporting we will schedule a worker
thread to start 2s or more in the future.  That worker thread will begin
working from the lowest supported page reporting order up to MAX_ORDER - 1
pulling unreported pages from the free list and storing them in the
scatterlist.

When processing each individual free list it is necessary for the worker
thread to release the zone lock when it needs to stop and report the full
scatterlist of pages.  To reduce the work of the next iteration the worker
thread will rotate the free list so that the first unreported page in the
free list becomes the first entry in the list.

It will then call a reporting function providing information on how many
entries are in the scatterlist.  Once the function completes it will
return the pages to the free area from which they were allocated and start
over pulling more pages from the free areas until there are no longer
enough pages to report on to keep the worker busy, or we have processed as
many pages as were contained in the free area when we started processing
the list.

The worker thread will work in a round-robin fashion making its way though
each zone requesting reporting, and through each reportable free list
within that zone.  Once all free areas within the zone have been processed
it will check to see if there have been any requests for reporting while
it was processing.  If so it will reschedule the worker thread to start up
again in roughly 2s and exit.

	Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Luiz Capitulino <lcapitulino@redhat.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Nitesh Narayan Lal <nitesh@redhat.com>
	Cc: Oscar Salvador <osalvador@suse.de>
	Cc: Pankaj Gupta <pagupta@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Wei Wang <wei.w.wang@intel.com>
	Cc: Yang Zhang <yang.zhang.wz@gmail.com>
	Cc: wei qi <weiqi4@huawei.com>
Link: http://lkml.kernel.org/r/20200211224635.29318.19750.stgit@localhost.localdomain
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 36e66c554b5c6a9d17a229faca7a61693527b0bd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/Makefile
#	mm/page_alloc.c
diff --cc mm/Makefile
index 14638478a5a7,fccd3756b25f..000000000000
--- a/mm/Makefile
+++ b/mm/Makefile
@@@ -103,6 -107,8 +103,11 @@@ obj-$(CONFIG_DEBUG_PAGE_REF) += debug_p
  obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
  obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
  obj-$(CONFIG_ZONE_DEVICE) += memremap.o
 -obj-$(CONFIG_HMM_MIRROR) += hmm.o
 +obj-$(CONFIG_HMM) += hmm.o
  obj-$(CONFIG_MEMFD_CREATE) += memfd.o
  obj-$(CONFIG_MAPPING_DIRTY_HELPERS) += mapping_dirty_helpers.o
++<<<<<<< HEAD
++=======
+ obj-$(CONFIG_PTDUMP_CORE) += ptdump.o
+ obj-$(CONFIG_PAGE_REPORTING) += page_reporting.o
++>>>>>>> 36e66c554b5c (mm: introduce Reported pages)
diff --cc mm/page_alloc.c
index 0f3a8eeb4d26,114c56c3685d..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -743,32 -793,148 +744,156 @@@ static inline void set_page_order(struc
   *
   * For recording page's order, we use page_private(page).
   */
 -static inline bool page_is_buddy(struct page *page, struct page *buddy,
 +static inline int page_is_buddy(struct page *page, struct page *buddy,
  							unsigned int order)
  {
 -	if (!page_is_guard(buddy) && !PageBuddy(buddy))
 -		return false;
 +	if (page_is_guard(buddy) && page_order(buddy) == order) {
 +		if (page_zone_id(page) != page_zone_id(buddy))
 +			return 0;
  
 -	if (page_order(buddy) != order)
 -		return false;
 +		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
  
 -	/*
 -	 * zone check is done late to avoid uselessly calculating
 -	 * zone/node ids for pages that could never merge.
 -	 */
 -	if (page_zone_id(page) != page_zone_id(buddy))
 -		return false;
 +		return 1;
 +	}
  
 -	VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
 +	if (PageBuddy(buddy) && page_order(buddy) == order) {
 +		/*
 +		 * zone check is done late to avoid uselessly
 +		 * calculating zone/node ids for pages that could
 +		 * never merge.
 +		 */
 +		if (page_zone_id(page) != page_zone_id(buddy))
 +			return 0;
  
 -	return true;
 -}
 +		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
  
++<<<<<<< HEAD
 +		return 1;
 +	}
 +	return 0;
++=======
+ #ifdef CONFIG_COMPACTION
+ static inline struct capture_control *task_capc(struct zone *zone)
+ {
+ 	struct capture_control *capc = current->capture_control;
+ 
+ 	return capc &&
+ 		!(current->flags & PF_KTHREAD) &&
+ 		!capc->page &&
+ 		capc->cc->zone == zone &&
+ 		capc->cc->direct_compaction ? capc : NULL;
+ }
+ 
+ static inline bool
+ compaction_capture(struct capture_control *capc, struct page *page,
+ 		   int order, int migratetype)
+ {
+ 	if (!capc || order != capc->cc->order)
+ 		return false;
+ 
+ 	/* Do not accidentally pollute CMA or isolated regions*/
+ 	if (is_migrate_cma(migratetype) ||
+ 	    is_migrate_isolate(migratetype))
+ 		return false;
+ 
+ 	/*
+ 	 * Do not let lower order allocations polluate a movable pageblock.
+ 	 * This might let an unmovable request use a reclaimable pageblock
+ 	 * and vice-versa but no more than normal fallback logic which can
+ 	 * have trouble finding a high-order free page.
+ 	 */
+ 	if (order < pageblock_order && migratetype == MIGRATE_MOVABLE)
+ 		return false;
+ 
+ 	capc->page = page;
+ 	return true;
+ }
+ 
+ #else
+ static inline struct capture_control *task_capc(struct zone *zone)
+ {
+ 	return NULL;
+ }
+ 
+ static inline bool
+ compaction_capture(struct capture_control *capc, struct page *page,
+ 		   int order, int migratetype)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_COMPACTION */
+ 
+ /* Used for pages not on another list */
+ static inline void add_to_free_list(struct page *page, struct zone *zone,
+ 				    unsigned int order, int migratetype)
+ {
+ 	struct free_area *area = &zone->free_area[order];
+ 
+ 	list_add(&page->lru, &area->free_list[migratetype]);
+ 	area->nr_free++;
+ }
+ 
+ /* Used for pages not on another list */
+ static inline void add_to_free_list_tail(struct page *page, struct zone *zone,
+ 					 unsigned int order, int migratetype)
+ {
+ 	struct free_area *area = &zone->free_area[order];
+ 
+ 	list_add_tail(&page->lru, &area->free_list[migratetype]);
+ 	area->nr_free++;
+ }
+ 
+ /* Used for pages which are on another list */
+ static inline void move_to_free_list(struct page *page, struct zone *zone,
+ 				     unsigned int order, int migratetype)
+ {
+ 	struct free_area *area = &zone->free_area[order];
+ 
+ 	list_move(&page->lru, &area->free_list[migratetype]);
+ }
+ 
+ static inline void del_page_from_free_list(struct page *page, struct zone *zone,
+ 					   unsigned int order)
+ {
+ 	/* clear reported state and update reported page count */
+ 	if (page_reported(page))
+ 		__ClearPageReported(page);
+ 
+ 	list_del(&page->lru);
+ 	__ClearPageBuddy(page);
+ 	set_page_private(page, 0);
+ 	zone->free_area[order].nr_free--;
+ }
+ 
+ /*
+  * If this is not the largest possible page, check if the buddy
+  * of the next-highest order is free. If it is, it's possible
+  * that pages are being freed that will coalesce soon. In case,
+  * that is happening, add the free page to the tail of the list
+  * so it's less likely to be used soon and more likely to be merged
+  * as a higher order page
+  */
+ static inline bool
+ buddy_merge_likely(unsigned long pfn, unsigned long buddy_pfn,
+ 		   struct page *page, unsigned int order)
+ {
+ 	struct page *higher_page, *higher_buddy;
+ 	unsigned long combined_pfn;
+ 
+ 	if (order >= MAX_ORDER - 2)
+ 		return false;
+ 
+ 	if (!pfn_valid_within(buddy_pfn))
+ 		return false;
+ 
+ 	combined_pfn = buddy_pfn & pfn;
+ 	higher_page = page + (combined_pfn - pfn);
+ 	buddy_pfn = __find_buddy_pfn(combined_pfn, order + 1);
+ 	higher_buddy = higher_page + (buddy_pfn - combined_pfn);
+ 
+ 	return pfn_valid_within(buddy_pfn) &&
+ 	       page_is_buddy(higher_page, higher_buddy, order + 1);
++>>>>>>> 36e66c554b5c (mm: introduce Reported pages)
  }
  
  /*
@@@ -798,12 -964,14 +923,12 @@@
  static inline void __free_one_page(struct page *page,
  		unsigned long pfn,
  		struct zone *zone, unsigned int order,
- 		int migratetype)
+ 		int migratetype, bool report)
  {
 -	struct capture_control *capc = task_capc(zone);
 -	unsigned long uninitialized_var(buddy_pfn);
  	unsigned long combined_pfn;
 -	unsigned int max_order;
 +	unsigned long uninitialized_var(buddy_pfn);
  	struct page *buddy;
 -	bool to_tail;
 +	unsigned int max_order;
  
  	max_order = min_t(unsigned int, MAX_ORDER, pageblock_order + 1);
  
@@@ -867,35 -1040,19 +992,43 @@@ continue_merging
  done_merging:
  	set_page_order(page, order);
  
 +	/*
 +	 * If this is not the largest possible page, check if the buddy
 +	 * of the next-highest order is free. If it is, it's possible
 +	 * that pages are being freed that will coalesce soon. In case,
 +	 * that is happening, add the free page to the tail of the list
 +	 * so it's less likely to be used soon and more likely to be merged
 +	 * as a higher order page
 +	 */
 +	if ((order < MAX_ORDER-2) && pfn_valid_within(buddy_pfn)
 +			&& !is_shuffle_order(order)) {
 +		struct page *higher_page, *higher_buddy;
 +		combined_pfn = buddy_pfn & pfn;
 +		higher_page = page + (combined_pfn - pfn);
 +		buddy_pfn = __find_buddy_pfn(combined_pfn, order + 1);
 +		higher_buddy = higher_page + (buddy_pfn - combined_pfn);
 +		if (pfn_valid_within(buddy_pfn) &&
 +		    page_is_buddy(higher_page, higher_buddy, order + 1)) {
 +			add_to_free_area_tail(page, &zone->free_area[order],
 +					      migratetype);
 +			return;
 +		}
 +	}
 +
  	if (is_shuffle_order(order))
 -		to_tail = shuffle_pick_tail();
 +		add_to_free_area_random(page, &zone->free_area[order],
 +				migratetype);
  	else
 -		to_tail = buddy_merge_likely(pfn, buddy_pfn, page, order);
++<<<<<<< HEAD
 +		add_to_free_area(page, &zone->free_area[order], migratetype);
  
 -	if (to_tail)
 -		add_to_free_list_tail(page, zone, order, migratetype);
 -	else
++=======
+ 		add_to_free_list(page, zone, order, migratetype);
+ 
+ 	/* Notify page reporting subsystem of freed page */
+ 	if (report)
+ 		page_reporting_notify_free(order);
++>>>>>>> 36e66c554b5c (mm: introduce Reported pages)
  }
  
  /*
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 0b130803572d..924ac6abcf37 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -138,6 +138,9 @@ enum pageflags {
 
 	/* non-lru isolated movable page */
 	PG_isolated = PG_reclaim,
+
+	/* Only valid for buddy pages. Used to track pages that are reported */
+	PG_reported = PG_uptodate,
 };
 
 #ifndef __GENERATING_BOUNDS_H
@@ -404,6 +407,14 @@ TESTCLEARFLAG(Young, young, PF_ANY)
 PAGEFLAG(Idle, idle, PF_ANY)
 #endif
 
+/*
+ * PageReported() is used to track reported free pages within the Buddy
+ * allocator. We can use the non-atomic version of the test and set
+ * operations as both should be shielded with the zone lock to prevent
+ * any possible races on the setting or clearing of the bit.
+ */
+__PAGEFLAG(Reported, reported, PF_NO_COMPOUND)
+
 /*
  * On an anonymous page mapped into a user virtual memory area,
  * page->mapping points to its anon_vma, not to a struct address_space;
diff --git a/include/linux/page_reporting.h b/include/linux/page_reporting.h
new file mode 100644
index 000000000000..32355486f572
--- /dev/null
+++ b/include/linux/page_reporting.h
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _LINUX_PAGE_REPORTING_H
+#define _LINUX_PAGE_REPORTING_H
+
+#include <linux/mmzone.h>
+#include <linux/scatterlist.h>
+
+#define PAGE_REPORTING_CAPACITY		32
+
+struct page_reporting_dev_info {
+	/* function that alters pages to make them "reported" */
+	int (*report)(struct page_reporting_dev_info *prdev,
+		      struct scatterlist *sg, unsigned int nents);
+
+	/* work struct for processing reports */
+	struct delayed_work work;
+
+	/* Current state of page reporting */
+	atomic_t state;
+};
+
+/* Tear-down and bring-up for page reporting devices */
+void page_reporting_unregister(struct page_reporting_dev_info *prdev);
+int page_reporting_register(struct page_reporting_dev_info *prdev);
+#endif /*_LINUX_PAGE_REPORTING_H */
diff --git a/mm/Kconfig b/mm/Kconfig
index ba2bdef1230d..d1e0d5953b21 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -234,6 +234,17 @@ config COMPACTION
           it and then we would be really interested to hear about that at
           linux-mm@kvack.org.
 
+#
+# support for free page reporting
+config PAGE_REPORTING
+	bool "Free page reporting"
+	def_bool n
+	help
+	  Free page reporting allows for the incremental acquisition of
+	  free pages from the buddy allocator for the purpose of reporting
+	  those pages to another entity, such as a hypervisor, so that the
+	  memory can be freed within the host for other uses.
+
 #
 # support for page migration
 #
* Unmerged path mm/Makefile
* Unmerged path mm/page_alloc.c
diff --git a/mm/page_reporting.c b/mm/page_reporting.c
new file mode 100644
index 000000000000..1047c6872d4f
--- /dev/null
+++ b/mm/page_reporting.c
@@ -0,0 +1,319 @@
+// SPDX-License-Identifier: GPL-2.0
+#include <linux/mm.h>
+#include <linux/mmzone.h>
+#include <linux/page_reporting.h>
+#include <linux/gfp.h>
+#include <linux/export.h>
+#include <linux/delay.h>
+#include <linux/scatterlist.h>
+
+#include "page_reporting.h"
+#include "internal.h"
+
+#define PAGE_REPORTING_DELAY	(2 * HZ)
+static struct page_reporting_dev_info __rcu *pr_dev_info __read_mostly;
+
+enum {
+	PAGE_REPORTING_IDLE = 0,
+	PAGE_REPORTING_REQUESTED,
+	PAGE_REPORTING_ACTIVE
+};
+
+/* request page reporting */
+static void
+__page_reporting_request(struct page_reporting_dev_info *prdev)
+{
+	unsigned int state;
+
+	/* Check to see if we are in desired state */
+	state = atomic_read(&prdev->state);
+	if (state == PAGE_REPORTING_REQUESTED)
+		return;
+
+	/*
+	 *  If reporting is already active there is nothing we need to do.
+	 *  Test against 0 as that represents PAGE_REPORTING_IDLE.
+	 */
+	state = atomic_xchg(&prdev->state, PAGE_REPORTING_REQUESTED);
+	if (state != PAGE_REPORTING_IDLE)
+		return;
+
+	/*
+	 * Delay the start of work to allow a sizable queue to build. For
+	 * now we are limiting this to running no more than once every
+	 * couple of seconds.
+	 */
+	schedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);
+}
+
+/* notify prdev of free page reporting request */
+void __page_reporting_notify(void)
+{
+	struct page_reporting_dev_info *prdev;
+
+	/*
+	 * We use RCU to protect the pr_dev_info pointer. In almost all
+	 * cases this should be present, however in the unlikely case of
+	 * a shutdown this will be NULL and we should exit.
+	 */
+	rcu_read_lock();
+	prdev = rcu_dereference(pr_dev_info);
+	if (likely(prdev))
+		__page_reporting_request(prdev);
+
+	rcu_read_unlock();
+}
+
+static void
+page_reporting_drain(struct page_reporting_dev_info *prdev,
+		     struct scatterlist *sgl, unsigned int nents, bool reported)
+{
+	struct scatterlist *sg = sgl;
+
+	/*
+	 * Drain the now reported pages back into their respective
+	 * free lists/areas. We assume at least one page is populated.
+	 */
+	do {
+		struct page *page = sg_page(sg);
+		int mt = get_pageblock_migratetype(page);
+		unsigned int order = get_order(sg->length);
+
+		__putback_isolated_page(page, order, mt);
+
+		/* If the pages were not reported due to error skip flagging */
+		if (!reported)
+			continue;
+
+		/*
+		 * If page was not comingled with another page we can
+		 * consider the result to be "reported" since the page
+		 * hasn't been modified, otherwise we will need to
+		 * report on the new larger page when we make our way
+		 * up to that higher order.
+		 */
+		if (PageBuddy(page) && page_order(page) == order)
+			__SetPageReported(page);
+	} while ((sg = sg_next(sg)));
+
+	/* reinitialize scatterlist now that it is empty */
+	sg_init_table(sgl, nents);
+}
+
+/*
+ * The page reporting cycle consists of 4 stages, fill, report, drain, and
+ * idle. We will cycle through the first 3 stages until we cannot obtain a
+ * full scatterlist of pages, in that case we will switch to idle.
+ */
+static int
+page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,
+		     unsigned int order, unsigned int mt,
+		     struct scatterlist *sgl, unsigned int *offset)
+{
+	struct free_area *area = &zone->free_area[order];
+	struct list_head *list = &area->free_list[mt];
+	unsigned int page_len = PAGE_SIZE << order;
+	struct page *page, *next;
+	int err = 0;
+
+	/*
+	 * Perform early check, if free area is empty there is
+	 * nothing to process so we can skip this free_list.
+	 */
+	if (list_empty(list))
+		return err;
+
+	spin_lock_irq(&zone->lock);
+
+	/* loop through free list adding unreported pages to sg list */
+	list_for_each_entry_safe(page, next, list, lru) {
+		/* We are going to skip over the reported pages. */
+		if (PageReported(page))
+			continue;
+
+		/* Attempt to pull page from list */
+		if (!__isolate_free_page(page, order))
+			break;
+
+		/* Add page to scatter list */
+		--(*offset);
+		sg_set_page(&sgl[*offset], page, page_len, 0);
+
+		/* If scatterlist isn't full grab more pages */
+		if (*offset)
+			continue;
+
+		/* release lock before waiting on report processing */
+		spin_unlock_irq(&zone->lock);
+
+		/* begin processing pages in local list */
+		err = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);
+
+		/* reset offset since the full list was reported */
+		*offset = PAGE_REPORTING_CAPACITY;
+
+		/* reacquire zone lock and resume processing */
+		spin_lock_irq(&zone->lock);
+
+		/* flush reported pages from the sg list */
+		page_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);
+
+		/*
+		 * Reset next to first entry, the old next isn't valid
+		 * since we dropped the lock to report the pages
+		 */
+		next = list_first_entry(list, struct page, lru);
+
+		/* exit on error */
+		if (err)
+			break;
+	}
+
+	spin_unlock_irq(&zone->lock);
+
+	return err;
+}
+
+static int
+page_reporting_process_zone(struct page_reporting_dev_info *prdev,
+			    struct scatterlist *sgl, struct zone *zone)
+{
+	unsigned int order, mt, leftover, offset = PAGE_REPORTING_CAPACITY;
+	unsigned long watermark;
+	int err = 0;
+
+	/* Generate minimum watermark to be able to guarantee progress */
+	watermark = low_wmark_pages(zone) +
+		    (PAGE_REPORTING_CAPACITY << PAGE_REPORTING_MIN_ORDER);
+
+	/*
+	 * Cancel request if insufficient free memory or if we failed
+	 * to allocate page reporting statistics for the zone.
+	 */
+	if (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))
+		return err;
+
+	/* Process each free list starting from lowest order/mt */
+	for (order = PAGE_REPORTING_MIN_ORDER; order < MAX_ORDER; order++) {
+		for (mt = 0; mt < MIGRATE_TYPES; mt++) {
+			/* We do not pull pages from the isolate free list */
+			if (is_migrate_isolate(mt))
+				continue;
+
+			err = page_reporting_cycle(prdev, zone, order, mt,
+						   sgl, &offset);
+			if (err)
+				return err;
+		}
+	}
+
+	/* report the leftover pages before going idle */
+	leftover = PAGE_REPORTING_CAPACITY - offset;
+	if (leftover) {
+		sgl = &sgl[offset];
+		err = prdev->report(prdev, sgl, leftover);
+
+		/* flush any remaining pages out from the last report */
+		spin_lock_irq(&zone->lock);
+		page_reporting_drain(prdev, sgl, leftover, !err);
+		spin_unlock_irq(&zone->lock);
+	}
+
+	return err;
+}
+
+static void page_reporting_process(struct work_struct *work)
+{
+	struct delayed_work *d_work = to_delayed_work(work);
+	struct page_reporting_dev_info *prdev =
+		container_of(d_work, struct page_reporting_dev_info, work);
+	int err = 0, state = PAGE_REPORTING_ACTIVE;
+	struct scatterlist *sgl;
+	struct zone *zone;
+
+	/*
+	 * Change the state to "Active" so that we can track if there is
+	 * anyone requests page reporting after we complete our pass. If
+	 * the state is not altered by the end of the pass we will switch
+	 * to idle and quit scheduling reporting runs.
+	 */
+	atomic_set(&prdev->state, state);
+
+	/* allocate scatterlist to store pages being reported on */
+	sgl = kmalloc_array(PAGE_REPORTING_CAPACITY, sizeof(*sgl), GFP_KERNEL);
+	if (!sgl)
+		goto err_out;
+
+	sg_init_table(sgl, PAGE_REPORTING_CAPACITY);
+
+	for_each_zone(zone) {
+		err = page_reporting_process_zone(prdev, sgl, zone);
+		if (err)
+			break;
+	}
+
+	kfree(sgl);
+err_out:
+	/*
+	 * If the state has reverted back to requested then there may be
+	 * additional pages to be processed. We will defer for 2s to allow
+	 * more pages to accumulate.
+	 */
+	state = atomic_cmpxchg(&prdev->state, state, PAGE_REPORTING_IDLE);
+	if (state == PAGE_REPORTING_REQUESTED)
+		schedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);
+}
+
+static DEFINE_MUTEX(page_reporting_mutex);
+DEFINE_STATIC_KEY_FALSE(page_reporting_enabled);
+
+int page_reporting_register(struct page_reporting_dev_info *prdev)
+{
+	int err = 0;
+
+	mutex_lock(&page_reporting_mutex);
+
+	/* nothing to do if already in use */
+	if (rcu_access_pointer(pr_dev_info)) {
+		err = -EBUSY;
+		goto err_out;
+	}
+
+	/* initialize state and work structures */
+	atomic_set(&prdev->state, PAGE_REPORTING_IDLE);
+	INIT_DELAYED_WORK(&prdev->work, &page_reporting_process);
+
+	/* Begin initial flush of zones */
+	__page_reporting_request(prdev);
+
+	/* Assign device to allow notifications */
+	rcu_assign_pointer(pr_dev_info, prdev);
+
+	/* enable page reporting notification */
+	if (!static_key_enabled(&page_reporting_enabled)) {
+		static_branch_enable(&page_reporting_enabled);
+		pr_info("Free page reporting enabled\n");
+	}
+err_out:
+	mutex_unlock(&page_reporting_mutex);
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(page_reporting_register);
+
+void page_reporting_unregister(struct page_reporting_dev_info *prdev)
+{
+	mutex_lock(&page_reporting_mutex);
+
+	if (rcu_access_pointer(pr_dev_info) == prdev) {
+		/* Disable page reporting notification */
+		RCU_INIT_POINTER(pr_dev_info, NULL);
+		synchronize_rcu();
+
+		/* Flush any existing work, and lock it out */
+		cancel_delayed_work_sync(&prdev->work);
+	}
+
+	mutex_unlock(&page_reporting_mutex);
+}
+EXPORT_SYMBOL_GPL(page_reporting_unregister);
diff --git a/mm/page_reporting.h b/mm/page_reporting.h
new file mode 100644
index 000000000000..aa6d37f4dc22
--- /dev/null
+++ b/mm/page_reporting.h
@@ -0,0 +1,54 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _MM_PAGE_REPORTING_H
+#define _MM_PAGE_REPORTING_H
+
+#include <linux/mmzone.h>
+#include <linux/pageblock-flags.h>
+#include <linux/page-isolation.h>
+#include <linux/jump_label.h>
+#include <linux/slab.h>
+#include <asm/pgtable.h>
+#include <linux/scatterlist.h>
+
+#define PAGE_REPORTING_MIN_ORDER	pageblock_order
+
+#ifdef CONFIG_PAGE_REPORTING
+DECLARE_STATIC_KEY_FALSE(page_reporting_enabled);
+void __page_reporting_notify(void);
+
+static inline bool page_reported(struct page *page)
+{
+	return static_branch_unlikely(&page_reporting_enabled) &&
+	       PageReported(page);
+}
+
+/**
+ * page_reporting_notify_free - Free page notification to start page processing
+ *
+ * This function is meant to act as a screener for __page_reporting_notify
+ * which will determine if a give zone has crossed over the high-water mark
+ * that will justify us beginning page treatment. If we have crossed that
+ * threshold then it will start the process of pulling some pages and
+ * placing them in the batch list for treatment.
+ */
+static inline void page_reporting_notify_free(unsigned int order)
+{
+	/* Called from hot path in __free_one_page() */
+	if (!static_branch_unlikely(&page_reporting_enabled))
+		return;
+
+	/* Determine if we have crossed reporting threshold */
+	if (order < PAGE_REPORTING_MIN_ORDER)
+		return;
+
+	/* This will add a few cycles, but should be called infrequently */
+	__page_reporting_notify();
+}
+#else /* CONFIG_PAGE_REPORTING */
+#define page_reported(_page)	false
+
+static inline void page_reporting_notify_free(unsigned int order)
+{
+}
+#endif /* CONFIG_PAGE_REPORTING */
+#endif /*_MM_PAGE_REPORTING_H */
