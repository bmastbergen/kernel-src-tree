mm: swap: memcg: fix memcg stats for huge pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit 21e330fc632d6a288f73de48045b782cc51d501a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/21e330fc.failed

The commit 2262185c5b28 ("mm: per-cgroup memory reclaim stats") added
PGLAZYFREE, PGACTIVATE & PGDEACTIVATE stats for cgroups but missed
couple of places and PGLAZYFREE missed huge page handling. Fix that.
Also for PGLAZYFREE use the irq-unsafe function to update as the irq is
already disabled.

Fixes: 2262185c5b28 ("mm: per-cgroup memory reclaim stats")
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
Link: http://lkml.kernel.org/r/20200527182947.251343-1-shakeelb@google.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 21e330fc632d6a288f73de48045b782cc51d501a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap.c
diff --cc mm/swap.c
index 70728521e27e,dbcab84c6fce..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -275,8 -319,8 +275,9 @@@ static void __activate_page(struct pag
  			    void *arg)
  {
  	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
 +		int file = page_is_file_cache(page);
  		int lru = page_lru_base_type(page);
+ 		int nr_pages = hpage_nr_pages(page);
  
  		del_page_from_lru_list(page, lruvec, lru);
  		SetPageActive(page);
@@@ -284,8 -328,9 +285,14 @@@
  		add_page_to_lru_list(page, lruvec, lru);
  		trace_mm_lru_activate(page);
  
++<<<<<<< HEAD
 +		__count_vm_event(PGACTIVATE);
 +		update_page_reclaim_stat(lruvec, file, 1);
++=======
+ 		__count_vm_events(PGACTIVATE, nr_pages);
+ 		__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE,
+ 				     nr_pages);
++>>>>>>> 21e330fc632d (mm: swap: memcg: fix memcg stats for huge pages)
  	}
  }
  
@@@ -540,28 -565,31 +547,43 @@@ static void lru_deactivate_file_fn(stru
  		 * We moves tha page into tail of inactive.
  		 */
  		add_page_to_lru_list_tail(page, lruvec, lru);
 -		__count_vm_events(PGROTATED, nr_pages);
 +		__count_vm_event(PGROTATED);
  	}
  
++<<<<<<< HEAD
 +	if (active)
 +		__count_vm_event(PGDEACTIVATE);
 +	update_page_reclaim_stat(lruvec, file, 0);
++=======
+ 	if (active) {
+ 		__count_vm_events(PGDEACTIVATE, nr_pages);
+ 		__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,
+ 				     nr_pages);
+ 	}
++>>>>>>> 21e330fc632d (mm: swap: memcg: fix memcg stats for huge pages)
  }
  
  static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,
  			    void *arg)
  {
  	if (PageLRU(page) && PageActive(page) && !PageUnevictable(page)) {
 +		int file = page_is_file_cache(page);
  		int lru = page_lru_base_type(page);
+ 		int nr_pages = hpage_nr_pages(page);
  
  		del_page_from_lru_list(page, lruvec, lru + LRU_ACTIVE);
  		ClearPageActive(page);
  		ClearPageReferenced(page);
  		add_page_to_lru_list(page, lruvec, lru);
  
++<<<<<<< HEAD
 +		__count_vm_events(PGDEACTIVATE, hpage_nr_pages(page));
 +		update_page_reclaim_stat(lruvec, file, 0);
++=======
+ 		__count_vm_events(PGDEACTIVATE, nr_pages);
+ 		__count_memcg_events(lruvec_memcg(lruvec), PGDEACTIVATE,
+ 				     nr_pages);
++>>>>>>> 21e330fc632d (mm: swap: memcg: fix memcg stats for huge pages)
  	}
  }
  
@@@ -584,9 -613,9 +607,15 @@@ static void lru_lazyfree_fn(struct pag
  		ClearPageSwapBacked(page);
  		add_page_to_lru_list(page, lruvec, LRU_INACTIVE_FILE);
  
++<<<<<<< HEAD
 +		__count_vm_events(PGLAZYFREE, hpage_nr_pages(page));
 +		count_memcg_page_event(page, PGLAZYFREE);
 +		update_page_reclaim_stat(lruvec, 1, 0);
++=======
+ 		__count_vm_events(PGLAZYFREE, nr_pages);
+ 		__count_memcg_events(lruvec_memcg(lruvec), PGLAZYFREE,
+ 				     nr_pages);
++>>>>>>> 21e330fc632d (mm: swap: memcg: fix memcg stats for huge pages)
  	}
  }
  
* Unmerged path mm/swap.c
