kasan, mm, arm64: tag non slab memory allocated via pagealloc

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Andrey Konovalov <andreyknvl@google.com>
commit 2813b9c0296259fb11e75c839bab2d958ba4f96c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/2813b9c0.failed

Tag-based KASAN doesn't check memory accesses through pointers tagged with
0xff.  When page_address is used to get pointer to memory that corresponds
to some page, the tag of the resulting pointer gets set to 0xff, even
though the allocated memory might have been tagged differently.

For slab pages it's impossible to recover the correct tag to return from
page_address, since the page might contain multiple slab objects tagged
with different values, and we can't know in advance which one of them is
going to get accessed.  For non slab pages however, we can recover the tag
in page_address, since the whole page was marked with the same tag.

This patch adds tagging to non slab memory allocated with pagealloc.  To
set the tag of the pointer returned from page_address, the tag gets stored
to page->flags when the memory gets allocated.

Link: http://lkml.kernel.org/r/d758ddcef46a5abc9970182b9137e2fbee202a2c.1544099024.git.andreyknvl@google.com
	Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
	Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
	Acked-by: Will Deacon <will.deacon@arm.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Mark Rutland <mark.rutland@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 2813b9c0296259fb11e75c839bab2d958ba4f96c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/kasan/kasan.c
diff --cc mm/kasan/kasan.c
index 2913fdd4758a,195ca385cf7a..000000000000
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@@ -116,199 -218,18 +116,212 @@@ void kasan_unpoison_stack_above_sp_to(c
  	kasan_unpoison_shadow(sp, size);
  }
  
 +/*
 + * All functions below always inlined so compiler could
 + * perform better optimizations in each of __asan_loadX/__assn_storeX
 + * depending on memory access size X.
 + */
 +
 +static __always_inline bool memory_is_poisoned_1(unsigned long addr)
 +{
 +	s8 shadow_value = *(s8 *)kasan_mem_to_shadow((void *)addr);
 +
 +	if (unlikely(shadow_value)) {
 +		s8 last_accessible_byte = addr & KASAN_SHADOW_MASK;
 +		return unlikely(last_accessible_byte >= shadow_value);
 +	}
 +
 +	return false;
 +}
 +
 +static __always_inline bool memory_is_poisoned_2_4_8(unsigned long addr,
 +						unsigned long size)
 +{
 +	u8 *shadow_addr = (u8 *)kasan_mem_to_shadow((void *)addr);
 +
 +	/*
 +	 * Access crosses 8(shadow size)-byte boundary. Such access maps
 +	 * into 2 shadow bytes, so we need to check them both.
 +	 */
 +	if (unlikely(((addr + size - 1) & KASAN_SHADOW_MASK) < size - 1))
 +		return *shadow_addr || memory_is_poisoned_1(addr + size - 1);
 +
 +	return memory_is_poisoned_1(addr + size - 1);
 +}
 +
 +static __always_inline bool memory_is_poisoned_16(unsigned long addr)
 +{
 +	u16 *shadow_addr = (u16 *)kasan_mem_to_shadow((void *)addr);
 +
 +	/* Unaligned 16-bytes access maps into 3 shadow bytes. */
 +	if (unlikely(!IS_ALIGNED(addr, KASAN_SHADOW_SCALE_SIZE)))
 +		return *shadow_addr || memory_is_poisoned_1(addr + 15);
 +
 +	return *shadow_addr;
 +}
 +
 +static __always_inline unsigned long bytes_is_nonzero(const u8 *start,
 +					size_t size)
 +{
 +	while (size) {
 +		if (unlikely(*start))
 +			return (unsigned long)start;
 +		start++;
 +		size--;
 +	}
 +
 +	return 0;
 +}
 +
 +static __always_inline unsigned long memory_is_nonzero(const void *start,
 +						const void *end)
 +{
 +	unsigned int words;
 +	unsigned long ret;
 +	unsigned int prefix = (unsigned long)start % 8;
 +
 +	if (end - start <= 16)
 +		return bytes_is_nonzero(start, end - start);
 +
 +	if (prefix) {
 +		prefix = 8 - prefix;
 +		ret = bytes_is_nonzero(start, prefix);
 +		if (unlikely(ret))
 +			return ret;
 +		start += prefix;
 +	}
 +
 +	words = (end - start) / 8;
 +	while (words) {
 +		if (unlikely(*(u64 *)start))
 +			return bytes_is_nonzero(start, 8);
 +		start += 8;
 +		words--;
 +	}
 +
 +	return bytes_is_nonzero(start, (end - start) % 8);
 +}
 +
 +static __always_inline bool memory_is_poisoned_n(unsigned long addr,
 +						size_t size)
 +{
 +	unsigned long ret;
 +
 +	ret = memory_is_nonzero(kasan_mem_to_shadow((void *)addr),
 +			kasan_mem_to_shadow((void *)addr + size - 1) + 1);
 +
 +	if (unlikely(ret)) {
 +		unsigned long last_byte = addr + size - 1;
 +		s8 *last_shadow = (s8 *)kasan_mem_to_shadow((void *)last_byte);
 +
 +		if (unlikely(ret != (unsigned long)last_shadow ||
 +			((long)(last_byte & KASAN_SHADOW_MASK) >= *last_shadow)))
 +			return true;
 +	}
 +	return false;
 +}
 +
 +static __always_inline bool memory_is_poisoned(unsigned long addr, size_t size)
 +{
 +	if (__builtin_constant_p(size)) {
 +		switch (size) {
 +		case 1:
 +			return memory_is_poisoned_1(addr);
 +		case 2:
 +		case 4:
 +		case 8:
 +			return memory_is_poisoned_2_4_8(addr, size);
 +		case 16:
 +			return memory_is_poisoned_16(addr);
 +		default:
 +			BUILD_BUG();
 +		}
 +	}
 +
 +	return memory_is_poisoned_n(addr, size);
 +}
 +
 +static __always_inline void check_memory_region_inline(unsigned long addr,
 +						size_t size, bool write,
 +						unsigned long ret_ip)
 +{
 +	if (unlikely(size == 0))
 +		return;
 +
 +	if (unlikely((void *)addr <
 +		kasan_shadow_to_mem((void *)KASAN_SHADOW_START))) {
 +		kasan_report(addr, size, write, ret_ip);
 +		return;
 +	}
 +
 +	if (likely(!memory_is_poisoned(addr, size)))
 +		return;
 +
 +	kasan_report(addr, size, write, ret_ip);
 +}
 +
 +static void check_memory_region(unsigned long addr,
 +				size_t size, bool write,
 +				unsigned long ret_ip)
 +{
 +	check_memory_region_inline(addr, size, write, ret_ip);
 +}
 +
 +void kasan_check_read(const volatile void *p, unsigned int size)
 +{
 +	check_memory_region((unsigned long)p, size, false, _RET_IP_);
 +}
 +EXPORT_SYMBOL(kasan_check_read);
 +
 +void kasan_check_write(const volatile void *p, unsigned int size)
 +{
 +	check_memory_region((unsigned long)p, size, true, _RET_IP_);
 +}
 +EXPORT_SYMBOL(kasan_check_write);
 +
 +#undef memset
 +void *memset(void *addr, int c, size_t len)
 +{
 +	check_memory_region((unsigned long)addr, len, true, _RET_IP_);
 +
 +	return __memset(addr, c, len);
 +}
 +
 +#undef memmove
 +void *memmove(void *dest, const void *src, size_t len)
 +{
 +	check_memory_region((unsigned long)src, len, false, _RET_IP_);
 +	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
 +
 +	return __memmove(dest, src, len);
 +}
 +
 +#undef memcpy
 +void *memcpy(void *dest, const void *src, size_t len)
 +{
 +	check_memory_region((unsigned long)src, len, false, _RET_IP_);
 +	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
 +
 +	return __memcpy(dest, src, len);
 +}
 +
  void kasan_alloc_pages(struct page *page, unsigned int order)
  {
++<<<<<<< HEAD:mm/kasan/kasan.c
 +	if (likely(!PageHighMem(page)))
 +		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
++=======
+ 	u8 tag;
+ 	unsigned long i;
+ 
+ 	if (unlikely(PageHighMem(page)))
+ 		return;
+ 
+ 	tag = random_tag();
+ 	for (i = 0; i < (1 << order); i++)
+ 		page_kasan_tag_set(page + i, tag);
+ 	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
++>>>>>>> 2813b9c02962 (kasan, mm, arm64: tag non slab memory allocated via pagealloc):mm/kasan/common.c
  }
  
  void kasan_free_pages(struct page *page, unsigned int order)
diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index df263607ae15..680063c114f5 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -318,7 +318,13 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
 #define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
-#define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
+#define page_to_virt(page)	({					\
+	unsigned long __addr =						\
+		((__page_to_voff(page)) | PAGE_OFFSET);			\
+	__addr = __tag_set(__addr, page_kasan_tag(page));		\
+	((void *)__addr);						\
+})
+
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
 
 #define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
diff --git a/include/linux/mm.h b/include/linux/mm.h
index c2872a52dcb3..5587b414d644 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -810,6 +810,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGOFF		(SECTIONS_PGOFF - NODES_WIDTH)
 #define ZONES_PGOFF		(NODES_PGOFF - ZONES_WIDTH)
 #define LAST_CPUPID_PGOFF	(ZONES_PGOFF - LAST_CPUPID_WIDTH)
+#define KASAN_TAG_PGOFF		(LAST_CPUPID_PGOFF - KASAN_TAG_WIDTH)
 
 /*
  * Define the bit shifts to access each section.  For non-existent
@@ -820,6 +821,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGSHIFT		(NODES_PGOFF * (NODES_WIDTH != 0))
 #define ZONES_PGSHIFT		(ZONES_PGOFF * (ZONES_WIDTH != 0))
 #define LAST_CPUPID_PGSHIFT	(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))
+#define KASAN_TAG_PGSHIFT	(KASAN_TAG_PGOFF * (KASAN_TAG_WIDTH != 0))
 
 /* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
 #ifdef NODE_NOT_IN_PAGE_FLAGS
@@ -842,6 +844,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_MASK		((1UL << NODES_WIDTH) - 1)
 #define SECTIONS_MASK		((1UL << SECTIONS_WIDTH) - 1)
 #define LAST_CPUPID_MASK	((1UL << LAST_CPUPID_SHIFT) - 1)
+#define KASAN_TAG_MASK		((1UL << KASAN_TAG_WIDTH) - 1)
 #define ZONEID_MASK		((1UL << ZONEID_SHIFT) - 1)
 
 static inline enum zone_type page_zonenum(const struct page *page)
@@ -1131,6 +1134,32 @@ static inline bool cpupid_match_pid(struct task_struct *task, int cpupid)
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+#ifdef CONFIG_KASAN_SW_TAGS
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag)
+{
+	page->flags &= ~(KASAN_TAG_MASK << KASAN_TAG_PGSHIFT);
+	page->flags |= (tag & KASAN_TAG_MASK) << KASAN_TAG_PGSHIFT;
+}
+
+static inline void page_kasan_tag_reset(struct page *page)
+{
+	page_kasan_tag_set(page, 0xff);
+}
+#else
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return 0xff;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag) { }
+static inline void page_kasan_tag_reset(struct page *page) { }
+#endif
+
 static inline struct zone *page_zone(const struct page *page)
 {
 	return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];
diff --git a/include/linux/page-flags-layout.h b/include/linux/page-flags-layout.h
index 7ec86bf31ce4..1dda31825ec4 100644
--- a/include/linux/page-flags-layout.h
+++ b/include/linux/page-flags-layout.h
@@ -82,6 +82,16 @@
 #define LAST_CPUPID_WIDTH 0
 #endif
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define KASAN_TAG_WIDTH 8
+#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH+LAST_CPUPID_WIDTH+KASAN_TAG_WIDTH \
+	> BITS_PER_LONG - NR_PAGEFLAGS
+#error "KASAN: not enough bits in page flags for tag"
+#endif
+#else
+#define KASAN_TAG_WIDTH 0
+#endif
+
 /*
  * We are going to use the flags for the page to node mapping if its in
  * there.  This includes the case where there is no node, so it is implicit.
diff --git a/mm/cma.c b/mm/cma.c
index 5b955fd3dcea..955b1494a9ae 100644
--- a/mm/cma.c
+++ b/mm/cma.c
@@ -420,6 +420,7 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 	unsigned long pfn = -1;
 	unsigned long start = 0;
 	unsigned long bitmap_maxno, bitmap_no, bitmap_count;
+	size_t i;
 	struct page *page = NULL;
 	int ret = -ENOMEM;
 
@@ -479,6 +480,16 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 
 	trace_cma_alloc(pfn, page, count, align);
 
+	/*
+	 * CMA can allocate multiple page blocks, which results in different
+	 * blocks being marked with different tags. Reset the tags to ignore
+	 * those page blocks.
+	 */
+	if (page) {
+		for (i = 0; i < count; i++)
+			page_kasan_tag_reset(page + i);
+	}
+
 	if (ret && !no_warn) {
 		pr_err("%s: alloc failed, req-size: %zu pages, ret: %d\n",
 			__func__, count, ret);
* Unmerged path mm/kasan/kasan.c
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index f2e58d80b8ed..d252316dc1ac 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1220,6 +1220,7 @@ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
 	init_page_count(page);
 	page_mapcount_reset(page);
 	page_cpupid_reset_last(page);
+	page_kasan_tag_reset(page);
 
 	INIT_LIST_HEAD(&page->lru);
 #ifdef WANT_PAGE_VIRTUAL
diff --git a/mm/slab.c b/mm/slab.c
index 3772f828c847..16b2848cd115 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2351,7 +2351,7 @@ static void *alloc_slabmgmt(struct kmem_cache *cachep,
 	void *freelist;
 	void *addr = page_address(page);
 
-	page->s_mem = addr + colour_off;
+	page->s_mem = kasan_reset_tag(addr) + colour_off;
 	page->active = 0;
 
 	if (OBJFREELIST_SLAB(cachep))
