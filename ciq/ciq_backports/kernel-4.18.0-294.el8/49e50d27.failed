mm: memcontrol: prepare move_account for removal of private page type counters

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 49e50d277ba2bb2c6e64632bc3193585674a2261
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/49e50d27.failed

When memcg uses the generic vmstat counters, it doesn't need to do
anything at charging and uncharging time.  It does, however, need to
migrate counts when pages move to a different cgroup in move_account.

Prepare the move_account function for the arrival of NR_FILE_PAGES,
NR_ANON_MAPPED, NR_ANON_THPS etc.  by having a branch for files and a
branch for anon, which can then divided into sub-branches.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Alex Shi <alex.shi@linux.alibaba.com>
	Reviewed-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Roman Gushchin <guro@fb.com>
	Cc: Shakeel Butt <shakeelb@google.com>
	Cc: Balbir Singh <bsingharora@gmail.com>
Link: http://lkml.kernel.org/r/20200508183105.225460-8-hannes@cmpxchg.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 49e50d277ba2bb2c6e64632bc3193585674a2261)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index ef707fd00e25,f82ae3723760..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -5299,10 -5432,8 +5299,9 @@@ static int mem_cgroup_move_account(stru
  {
  	struct lruvec *from_vec, *to_vec;
  	struct pglist_data *pgdat;
 +	unsigned long flags;
  	unsigned int nr_pages = compound ? hpage_nr_pages(page) : 1;
  	int ret;
- 	bool anon;
  
  	VM_BUG_ON(from == to);
  	VM_BUG_ON_PAGE(PageLRU(page), page);
@@@ -5326,24 -5455,23 +5323,33 @@@
  	from_vec = mem_cgroup_lruvec(from, pgdat);
  	to_vec = mem_cgroup_lruvec(to, pgdat);
  
 -	lock_page_memcg(page);
 +	spin_lock_irqsave(&from->move_lock, flags);
  
- 	if (!anon && page_mapped(page)) {
- 		__mod_lruvec_state(from_vec, NR_FILE_MAPPED, -nr_pages);
- 		__mod_lruvec_state(to_vec, NR_FILE_MAPPED, nr_pages);
- 	}
+ 	if (!PageAnon(page)) {
+ 		if (page_mapped(page)) {
+ 			__mod_lruvec_state(from_vec, NR_FILE_MAPPED, -nr_pages);
+ 			__mod_lruvec_state(to_vec, NR_FILE_MAPPED, nr_pages);
+ 		}
  
++<<<<<<< HEAD
 +	/*
 +	 * move_lock grabbed above and caller set from->moving_account, so
 +	 * mod_memcg_page_state will serialize updates to PageDirty.
 +	 * So mapping should be stable for dirty pages.
 +	 */
 +	if (!anon && PageDirty(page)) {
 +		struct address_space *mapping = page_mapping(page);
- 
- 		if (mapping_cap_account_dirty(mapping)) {
- 			__mod_lruvec_state(from_vec, NR_FILE_DIRTY, -nr_pages);
- 			__mod_lruvec_state(to_vec, NR_FILE_DIRTY, nr_pages);
++=======
+ 		if (PageDirty(page)) {
+ 			struct address_space *mapping = page_mapping(page);
++>>>>>>> 49e50d277ba2 (mm: memcontrol: prepare move_account for removal of private page type counters)
+ 
+ 			if (mapping_cap_account_dirty(mapping)) {
+ 				__mod_lruvec_state(from_vec, NR_FILE_DIRTY,
+ 						   -nr_pages);
+ 				__mod_lruvec_state(to_vec, NR_FILE_DIRTY,
+ 						   nr_pages);
+ 			}
  		}
  	}
  
* Unmerged path mm/memcontrol.c
