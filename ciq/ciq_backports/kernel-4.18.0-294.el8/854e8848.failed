mm: clean up free_area_init_node() and its helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Mike Rapoport <rppt@linux.ibm.com>
commit 854e8848c5841b4199a70f1838f55999cecbf3b6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/854e8848.failed

free_area_init_node() now always uses memblock info and the zone PFN
limits so it does not need the backwards compatibility functions to
calculate the zone spanned and absent pages.  The removal of the compat_
versions of zone_{abscent,spanned}_pages_in_node() in turn, makes
zone_size and zhole_size parameters unused.

The node_start_pfn is determined by get_pfn_range_for_nid(), so there is
no need to pass it to free_area_init_node().

As a result, the only required parameter to free_area_init_node() is the
node ID, all the rest are removed along with no longer used
compat_zone_{abscent,spanned}_pages_in_node() helpers.

	Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Tested-by: Hoan Tran <hoan@os.amperecomputing.com>	[arm64]
	Cc: Baoquan He <bhe@redhat.com>
	Cc: Brian Cain <bcain@codeaurora.org>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Greentime Hu <green.hu@gmail.com>
	Cc: Greg Ungerer <gerg@linux-m68k.org>
	Cc: Guan Xuetao <gxt@pku.edu.cn>
	Cc: Guo Ren <guoren@kernel.org>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Helge Deller <deller@gmx.de>
	Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Ley Foon Tan <ley.foon.tan@intel.com>
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Max Filippov <jcmvbkbc@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Michal Simek <monstr@monstr.eu>
	Cc: Nick Hu <nickhu@andestech.com>
	Cc: Paul Walmsley <paul.walmsley@sifive.com>
	Cc: Richard Weinberger <richard@nod.at>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Stafford Horne <shorne@gmail.com>
	Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Vineet Gupta <vgupta@synopsys.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
Link: http://lkml.kernel.org/r/20200412194859.12663-20-rppt@kernel.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 854e8848c5841b4199a70f1838f55999cecbf3b6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index f9684b405897,ee7ef328c9de..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -6389,45 -6547,9 +6387,51 @@@ static unsigned long __init zone_absent
  	return nr_absent;
  }
  
++<<<<<<< HEAD
 +#else /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
 +static inline unsigned long __init zone_spanned_pages_in_node(int nid,
 +					unsigned long zone_type,
 +					unsigned long node_start_pfn,
 +					unsigned long node_end_pfn,
 +					unsigned long *zone_start_pfn,
 +					unsigned long *zone_end_pfn,
 +					unsigned long *zones_size)
 +{
 +	unsigned int zone;
 +
 +	*zone_start_pfn = node_start_pfn;
 +	for (zone = 0; zone < zone_type; zone++)
 +		*zone_start_pfn += zones_size[zone];
 +
 +	*zone_end_pfn = *zone_start_pfn + zones_size[zone_type];
 +
 +	return zones_size[zone_type];
 +}
 +
 +static inline unsigned long __init zone_absent_pages_in_node(int nid,
 +						unsigned long zone_type,
 +						unsigned long node_start_pfn,
 +						unsigned long node_end_pfn,
 +						unsigned long *zholes_size)
 +{
 +	if (!zholes_size)
 +		return 0;
 +
 +	return zholes_size[zone_type];
 +}
 +
 +#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
 +
 +static void __init calculate_node_totalpages(struct pglist_data *pgdat,
 +						unsigned long node_start_pfn,
 +						unsigned long node_end_pfn,
 +						unsigned long *zones_size,
 +						unsigned long *zholes_size)
++=======
+ static void __init calculate_node_totalpages(struct pglist_data *pgdat,
+ 						unsigned long node_start_pfn,
+ 						unsigned long node_end_pfn)
++>>>>>>> 854e8848c584 (mm: clean up free_area_init_node() and its helpers)
  {
  	unsigned long realtotalpages = 0, totalpages = 0;
  	enum zone_type i;
@@@ -6435,17 -6557,21 +6439,32 @@@
  	for (i = 0; i < MAX_NR_ZONES; i++) {
  		struct zone *zone = pgdat->node_zones + i;
  		unsigned long zone_start_pfn, zone_end_pfn;
 -		unsigned long spanned, absent;
  		unsigned long size, real_size;
  
++<<<<<<< HEAD
 +		size = zone_spanned_pages_in_node(pgdat->node_id, i,
 +						  node_start_pfn,
 +						  node_end_pfn,
 +						  &zone_start_pfn,
 +						  &zone_end_pfn,
 +						  zones_size);
 +		real_size = size - zone_absent_pages_in_node(pgdat->node_id, i,
 +						  node_start_pfn, node_end_pfn,
 +						  zholes_size);
++=======
+ 		spanned = zone_spanned_pages_in_node(pgdat->node_id, i,
+ 						     node_start_pfn,
+ 						     node_end_pfn,
+ 						     &zone_start_pfn,
+ 						     &zone_end_pfn);
+ 		absent = zone_absent_pages_in_node(pgdat->node_id, i,
+ 						   node_start_pfn,
+ 						   node_end_pfn);
+ 
+ 		size = spanned;
+ 		real_size = size - absent;
+ 
++>>>>>>> 854e8848c584 (mm: clean up free_area_init_node() and its helpers)
  		if (size)
  			zone->zone_start_pfn = zone_start_pfn;
  		else
@@@ -6699,8 -6880,16 +6718,21 @@@ static void __ref alloc_node_mem_map(st
  static void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }
  #endif /* CONFIG_FLAT_NODE_MEM_MAP */
  
++<<<<<<< HEAD
 +void __paginginit free_area_init_node(int nid, unsigned long *zones_size,
 +		unsigned long node_start_pfn, unsigned long *zholes_size)
++=======
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static inline void pgdat_set_deferred_range(pg_data_t *pgdat)
+ {
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ }
+ #else
+ static inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}
+ #endif
+ 
+ static void __init free_area_init_node(int nid)
++>>>>>>> 854e8848c584 (mm: clean up free_area_init_node() and its helpers)
  {
  	pg_data_t *pgdat = NODE_DATA(nid);
  	unsigned long start_pfn = 0;
@@@ -6709,34 -6898,28 +6741,52 @@@
  	/* pg_data_t should be reset to zero when it's allocated */
  	WARN_ON(pgdat->nr_zones || pgdat->kswapd_classzone_idx);
  
+ 	get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
+ 
  	pgdat->node_id = nid;
- 	pgdat->node_start_pfn = node_start_pfn;
+ 	pgdat->node_start_pfn = start_pfn;
  	pgdat->per_cpu_nodestats = NULL;
++<<<<<<< HEAD
 +#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 +	get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
 +	pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,
 +		(u64)start_pfn << PAGE_SHIFT,
 +		end_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);
 +#else
 +	start_pfn = node_start_pfn;
 +#endif
 +	calculate_node_totalpages(pgdat, start_pfn, end_pfn,
 +				  zones_size, zholes_size);
++=======
+ 
+ 	pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,
+ 		(u64)start_pfn << PAGE_SHIFT,
+ 		end_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);
+ 	calculate_node_totalpages(pgdat, start_pfn, end_pfn);
++>>>>>>> 854e8848c584 (mm: clean up free_area_init_node() and its helpers)
  
  	alloc_node_mem_map(pgdat);
 -	pgdat_set_deferred_range(pgdat);
  
 +#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
 +	/*
 +	 * We start only with one section of pages, more pages are added as
 +	 * needed until the rest of deferred pages are initialized.
 +	 */
 +	pgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,
 +					 pgdat->node_spanned_pages);
 +	pgdat->first_deferred_pfn = ULONG_MAX;
 +#endif
  	free_area_init_core(pgdat);
  }
  
++<<<<<<< HEAD
++=======
+ void __init free_area_init_memoryless_node(int nid)
+ {
+ 	free_area_init_node(nid);
+ }
+ 
++>>>>>>> 854e8848c584 (mm: clean up free_area_init_node() and its helpers)
  #if !defined(CONFIG_FLAT_NODE_MEM_MAP)
  /*
   * Initialize all valid struct pages in the range [spfn, epfn) and mark them
@@@ -7253,8 -7447,7 +7303,12 @@@ void __init free_area_init_nodes(unsign
  	init_unavailable_mem();
  	for_each_online_node(nid) {
  		pg_data_t *pgdat = NODE_DATA(nid);
++<<<<<<< HEAD
 +		free_area_init_node(nid, NULL,
 +				find_min_pfn_for_node(nid), NULL);
++=======
+ 		free_area_init_node(nid);
++>>>>>>> 854e8848c584 (mm: clean up free_area_init_node() and its helpers)
  
  		/* Any memory on that node */
  		if (pgdat->node_present_pages)
* Unmerged path mm/page_alloc.c
