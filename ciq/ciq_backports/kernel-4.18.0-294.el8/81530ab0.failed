RDMA/mlx5: Allow providing extra scatter CQE QP flag

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit 81530ab08ef002f90b7dc68f5a69816b0e3be803
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/81530ab0.failed

Scatter CQE feature relies on two flags MLX5_QP_FLAG_SCATTER_CQE and
MLX5_QP_FLAG_ALLOW_SCATTER_CQE, both of them can be provided without
relation to device capability.

Relax global validity check to allow MLX5_QP_FLAG_ALLOW_SCATTER_CQE QP
flag.

Existing user applications are failing on this new validity check.

Fixes: 90ecb37a751b ("RDMA/mlx5: Change scatter CQE flag to be set like other vendor flags")
Fixes: 37518fa49f76 ("RDMA/mlx5: Process all vendor flags in one place")
Link: https://lore.kernel.org/r/20200728120255.805733-1-leon@kernel.org
	Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 81530ab08ef002f90b7dc68f5a69816b0e3be803)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 886e16185f47,42620f88e393..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -2226,11 -2009,9 +2225,17 @@@ static int create_qp_common(struct mlx5
  			 rcqe_sz == 128 ? MLX5_RES_SCAT_DATA64_CQE :
  					  MLX5_RES_SCAT_DATA32_CQE);
  	}
++<<<<<<< HEAD
 +	if (qp->scat_cqe && (qp->qp_sub_type == MLX5_IB_QPT_DCI ||
 +			     init_attr->qp_type == IB_QPT_RC))
 +		configure_requester_scat_cqe(dev, init_attr,
 +					     udata ? &ucmd : NULL,
 +					     qpc);
++=======
+ 	if ((qp->flags_en & MLX5_QP_FLAG_SCATTER_CQE) &&
+ 	    (qp->type == MLX5_IB_QPT_DCI || qp->type == IB_QPT_RC))
+ 		configure_requester_scat_cqe(dev, qp, init_attr, qpc);
++>>>>>>> 81530ab08ef0 (RDMA/mlx5: Allow providing extra scatter CQE QP flag)
  
  	if (qp->rq.wqe_cnt) {
  		MLX5_SET(qpc, qpc, log_rq_stride, qp->rq.wqe_shift - 4);
@@@ -2652,128 -2524,344 +2657,376 @@@ static int set_mlx_qp_type(struct mlx5_
  	return 0;
  }
  
++<<<<<<< HEAD
 +struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd,
 +				struct ib_qp_init_attr *verbs_init_attr,
 +				struct ib_udata *udata)
++=======
+ static void process_vendor_flag(struct mlx5_ib_dev *dev, int *flags, int flag,
+ 				bool cond, struct mlx5_ib_qp *qp)
+ {
+ 	if (!(*flags & flag))
+ 		return;
+ 
+ 	if (cond) {
+ 		qp->flags_en |= flag;
+ 		*flags &= ~flag;
+ 		return;
+ 	}
+ 
+ 	switch (flag) {
+ 	case MLX5_QP_FLAG_SCATTER_CQE:
+ 	case MLX5_QP_FLAG_ALLOW_SCATTER_CQE:
+ 		/*
+ 			 * We don't return error if these flags were provided,
+ 			 * and mlx5 doesn't have right capability.
+ 			 */
+ 		*flags &= ~(MLX5_QP_FLAG_SCATTER_CQE |
+ 			    MLX5_QP_FLAG_ALLOW_SCATTER_CQE);
+ 		return;
+ 	default:
+ 		break;
+ 	}
+ 	mlx5_ib_dbg(dev, "Vendor create QP flag 0x%X is not supported\n", flag);
+ }
+ 
+ static int process_vendor_flags(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
+ 				void *ucmd, struct ib_qp_init_attr *attr)
+ {
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	bool cond;
+ 	int flags;
+ 
+ 	if (attr->rwq_ind_tbl)
+ 		flags = ((struct mlx5_ib_create_qp_rss *)ucmd)->flags;
+ 	else
+ 		flags = ((struct mlx5_ib_create_qp *)ucmd)->flags;
+ 
+ 	switch (flags & (MLX5_QP_FLAG_TYPE_DCT | MLX5_QP_FLAG_TYPE_DCI)) {
+ 	case MLX5_QP_FLAG_TYPE_DCI:
+ 		qp->type = MLX5_IB_QPT_DCI;
+ 		break;
+ 	case MLX5_QP_FLAG_TYPE_DCT:
+ 		qp->type = MLX5_IB_QPT_DCT;
+ 		break;
+ 	default:
+ 		if (qp->type != IB_QPT_DRIVER)
+ 			break;
+ 		/*
+ 		 * It is IB_QPT_DRIVER and or no subtype or
+ 		 * wrong subtype were provided.
+ 		 */
+ 		return -EINVAL;
+ 	}
+ 
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_TYPE_DCI, true, qp);
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_TYPE_DCT, true, qp);
+ 
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_SIGNATURE, true, qp);
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_SCATTER_CQE,
+ 			    MLX5_CAP_GEN(mdev, sctr_data_cqe), qp);
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_ALLOW_SCATTER_CQE,
+ 			    MLX5_CAP_GEN(mdev, sctr_data_cqe), qp);
+ 
+ 	if (qp->type == IB_QPT_RAW_PACKET) {
+ 		cond = MLX5_CAP_ETH(mdev, tunnel_stateless_vxlan) ||
+ 		       MLX5_CAP_ETH(mdev, tunnel_stateless_gre) ||
+ 		       MLX5_CAP_ETH(mdev, tunnel_stateless_geneve_rx);
+ 		process_vendor_flag(dev, &flags, MLX5_QP_FLAG_TUNNEL_OFFLOADS,
+ 				    cond, qp);
+ 		process_vendor_flag(dev, &flags,
+ 				    MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_UC, true,
+ 				    qp);
+ 		process_vendor_flag(dev, &flags,
+ 				    MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_MC, true,
+ 				    qp);
+ 	}
+ 
+ 	if (qp->type == IB_QPT_RC)
+ 		process_vendor_flag(dev, &flags,
+ 				    MLX5_QP_FLAG_PACKET_BASED_CREDIT_MODE,
+ 				    MLX5_CAP_GEN(mdev, qp_packet_based), qp);
+ 
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_BFREG_INDEX, true, qp);
+ 	process_vendor_flag(dev, &flags, MLX5_QP_FLAG_UAR_PAGE_INDEX, true, qp);
+ 
+ 	cond = qp->flags_en & ~(MLX5_QP_FLAG_TUNNEL_OFFLOADS |
+ 				MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_UC |
+ 				MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_MC);
+ 	if (attr->rwq_ind_tbl && cond) {
+ 		mlx5_ib_dbg(dev, "RSS RAW QP has unsupported flags 0x%X\n",
+ 			    cond);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (flags)
+ 		mlx5_ib_dbg(dev, "udata has unsupported flags 0x%X\n", flags);
+ 
+ 	return (flags) ? -EINVAL : 0;
+ 	}
+ 
+ static void process_create_flag(struct mlx5_ib_dev *dev, int *flags, int flag,
+ 				bool cond, struct mlx5_ib_qp *qp)
+ {
+ 	if (!(*flags & flag))
+ 		return;
+ 
+ 	if (cond) {
+ 		qp->flags |= flag;
+ 		*flags &= ~flag;
+ 		return;
+ 	}
+ 
+ 	if (flag == MLX5_IB_QP_CREATE_WC_TEST) {
+ 		/*
+ 		 * Special case, if condition didn't meet, it won't be error,
+ 		 * just different in-kernel flow.
+ 		 */
+ 		*flags &= ~MLX5_IB_QP_CREATE_WC_TEST;
+ 		return;
+ 	}
+ 	mlx5_ib_dbg(dev, "Verbs create QP flag 0x%X is not supported\n", flag);
+ }
+ 
+ static int process_create_flags(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
+ 				struct ib_qp_init_attr *attr)
+ {
+ 	enum ib_qp_type qp_type = qp->type;
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	int create_flags = attr->create_flags;
+ 	bool cond;
+ 
+ 	if (qp->type == IB_QPT_UD && dev->profile == &raw_eth_profile)
+ 		if (create_flags & ~MLX5_IB_QP_CREATE_WC_TEST)
+ 			return -EINVAL;
+ 
+ 	if (qp_type == MLX5_IB_QPT_DCT)
+ 		return (create_flags) ? -EINVAL : 0;
+ 
+ 	if (qp_type == IB_QPT_RAW_PACKET && attr->rwq_ind_tbl)
+ 		return (create_flags) ? -EINVAL : 0;
+ 
+ 	process_create_flag(dev, &create_flags, IB_QP_CREATE_NETIF_QP,
+ 			    mlx5_get_flow_namespace(dev->mdev,
+ 						    MLX5_FLOW_NAMESPACE_BYPASS),
+ 			    qp);
+ 	process_create_flag(dev, &create_flags,
+ 			    IB_QP_CREATE_INTEGRITY_EN,
+ 			    MLX5_CAP_GEN(mdev, sho), qp);
+ 	process_create_flag(dev, &create_flags,
+ 			    IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK,
+ 			    MLX5_CAP_GEN(mdev, block_lb_mc), qp);
+ 	process_create_flag(dev, &create_flags, IB_QP_CREATE_CROSS_CHANNEL,
+ 			    MLX5_CAP_GEN(mdev, cd), qp);
+ 	process_create_flag(dev, &create_flags, IB_QP_CREATE_MANAGED_SEND,
+ 			    MLX5_CAP_GEN(mdev, cd), qp);
+ 	process_create_flag(dev, &create_flags, IB_QP_CREATE_MANAGED_RECV,
+ 			    MLX5_CAP_GEN(mdev, cd), qp);
+ 
+ 	if (qp_type == IB_QPT_UD) {
+ 		process_create_flag(dev, &create_flags,
+ 				    IB_QP_CREATE_IPOIB_UD_LSO,
+ 				    MLX5_CAP_GEN(mdev, ipoib_basic_offloads),
+ 				    qp);
+ 		cond = MLX5_CAP_GEN(mdev, port_type) == MLX5_CAP_PORT_TYPE_IB;
+ 		process_create_flag(dev, &create_flags, IB_QP_CREATE_SOURCE_QPN,
+ 				    cond, qp);
+ 	}
+ 
+ 	if (qp_type == IB_QPT_RAW_PACKET) {
+ 		cond = MLX5_CAP_GEN(mdev, eth_net_offloads) &&
+ 		       MLX5_CAP_ETH(mdev, scatter_fcs);
+ 		process_create_flag(dev, &create_flags,
+ 				    IB_QP_CREATE_SCATTER_FCS, cond, qp);
+ 
+ 		cond = MLX5_CAP_GEN(mdev, eth_net_offloads) &&
+ 		       MLX5_CAP_ETH(mdev, vlan_cap);
+ 		process_create_flag(dev, &create_flags,
+ 				    IB_QP_CREATE_CVLAN_STRIPPING, cond, qp);
+ 	}
+ 
+ 	process_create_flag(dev, &create_flags,
+ 			    IB_QP_CREATE_PCI_WRITE_END_PADDING,
+ 			    MLX5_CAP_GEN(mdev, end_pad), qp);
+ 
+ 	process_create_flag(dev, &create_flags, MLX5_IB_QP_CREATE_WC_TEST,
+ 			    qp_type != MLX5_IB_QPT_REG_UMR, qp);
+ 	process_create_flag(dev, &create_flags, MLX5_IB_QP_CREATE_SQPN_QP1,
+ 			    true, qp);
+ 
+ 	if (create_flags)
+ 		mlx5_ib_dbg(dev, "Create QP has unsupported flags 0x%X\n",
+ 			    create_flags);
+ 
+ 	return (create_flags) ? -EINVAL : 0;
+ }
+ 
+ static int process_udata_size(struct mlx5_ib_dev *dev,
+ 			      struct mlx5_create_qp_params *params)
+ {
+ 	size_t ucmd = sizeof(struct mlx5_ib_create_qp);
+ 	struct ib_udata *udata = params->udata;
+ 	size_t outlen = udata->outlen;
+ 	size_t inlen = udata->inlen;
+ 
+ 	params->outlen = min(outlen, sizeof(struct mlx5_ib_create_qp_resp));
+ 	params->ucmd_size = ucmd;
+ 	if (!params->is_rss_raw) {
+ 		/* User has old rdma-core, which doesn't support ECE */
+ 		size_t min_inlen =
+ 			offsetof(struct mlx5_ib_create_qp, ece_options);
+ 
+ 		/*
+ 		 * We will check in check_ucmd_data() that user
+ 		 * cleared everything after inlen.
+ 		 */
+ 		params->inlen = (inlen < min_inlen) ? 0 : min(inlen, ucmd);
+ 		goto out;
+ 	}
+ 
+ 	/* RSS RAW QP */
+ 	if (inlen < offsetofend(struct mlx5_ib_create_qp_rss, flags))
+ 		return -EINVAL;
+ 
+ 	if (outlen < offsetofend(struct mlx5_ib_create_qp_resp, bfreg_index))
+ 		return -EINVAL;
+ 
+ 	ucmd = sizeof(struct mlx5_ib_create_qp_rss);
+ 	params->ucmd_size = ucmd;
+ 	if (inlen > ucmd && !ib_is_udata_cleared(udata, ucmd, inlen - ucmd))
+ 		return -EINVAL;
+ 
+ 	params->inlen = min(ucmd, inlen);
+ out:
+ 	if (!params->inlen)
+ 		mlx5_ib_dbg(dev, "udata is too small\n");
+ 
+ 	return (params->inlen) ? 0 : -EINVAL;
+ }
+ 
+ static int create_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 		     struct mlx5_ib_qp *qp,
+ 		     struct mlx5_create_qp_params *params)
++>>>>>>> 81530ab08ef0 (RDMA/mlx5: Allow providing extra scatter CQE QP flag)
  {
 +	struct mlx5_ib_dev *dev;
 +	struct mlx5_ib_qp *qp;
 +	u16 xrcdn = 0;
  	int err;
 +	struct ib_qp_init_attr mlx_init_attr;
 +	struct ib_qp_init_attr *init_attr = verbs_init_attr;
 +	struct mlx5_ib_ucontext *ucontext = rdma_udata_to_drv_context(
 +		udata, struct mlx5_ib_ucontext, ibucontext);
  
 -	if (params->is_rss_raw) {
 -		err = create_rss_raw_qp_tir(dev, pd, qp, params);
 -		goto out;
 -	}
 +	if (pd) {
 +		dev = to_mdev(pd->device);
  
 -	if (qp->type == MLX5_IB_QPT_DCT) {
 -		err = create_dct(dev, pd, qp, params);
 -		goto out;
 +		if (init_attr->qp_type == IB_QPT_RAW_PACKET) {
 +			if (!ucontext) {
 +				mlx5_ib_dbg(dev, "Raw Packet QP is not supported for kernel consumers\n");
 +				return ERR_PTR(-EINVAL);
 +			} else if (!ucontext->cqe_version) {
 +				mlx5_ib_dbg(dev, "Raw Packet QP is only supported for CQE version > 0\n");
 +				return ERR_PTR(-EINVAL);
 +			}
 +		}
 +	} else {
 +		/* being cautious here */
 +		if (init_attr->qp_type != IB_QPT_XRC_TGT &&
 +		    init_attr->qp_type != MLX5_IB_QPT_REG_UMR) {
 +			pr_warn("%s: no PD for transport %s\n", __func__,
 +				ib_qp_type_str(init_attr->qp_type));
 +			return ERR_PTR(-EINVAL);
 +		}
 +		dev = to_mdev(to_mxrcd(init_attr->xrcd)->ibxrcd.device);
  	}
  
 -	if (qp->type == IB_QPT_XRC_TGT) {
 -		err = create_xrc_tgt_qp(dev, qp, params);
 -		goto out;
 -	}
 +	if (init_attr->qp_type == IB_QPT_DRIVER) {
 +		struct mlx5_ib_create_qp ucmd;
  
 -	if (params->udata)
 -		err = create_user_qp(dev, pd, qp, params);
 -	else
 -		err = create_kernel_qp(dev, pd, qp, params);
 +		init_attr = &mlx_init_attr;
 +		memcpy(init_attr, verbs_init_attr, sizeof(*verbs_init_attr));
 +		err = set_mlx_qp_type(dev, init_attr, &ucmd, udata);
 +		if (err)
 +			return ERR_PTR(err);
  
 -out:
 -	if (err) {
 -		mlx5_ib_err(dev, "Create QP type %d failed\n", qp->type);
 -		return err;
 +		if (init_attr->qp_type == MLX5_IB_QPT_DCI) {
 +			if (init_attr->cap.max_recv_wr ||
 +			    init_attr->cap.max_recv_sge) {
 +				mlx5_ib_dbg(dev, "DCI QP requires zero size receive queue\n");
 +				return ERR_PTR(-EINVAL);
 +			}
 +		} else {
 +			return mlx5_ib_create_dct(pd, init_attr, &ucmd, udata);
 +		}
  	}
  
 -	if (is_qp0(qp->type))
 -		qp->ibqp.qp_num = 0;
 -	else if (is_qp1(qp->type))
 -		qp->ibqp.qp_num = 1;
 -	else
 -		qp->ibqp.qp_num = qp->trans_qp.base.mqp.qpn;
 -
 -	mlx5_ib_dbg(dev,
 -		"QP type %d, ib qpn 0x%X, mlx qpn 0x%x, rcqn 0x%x, scqn 0x%x, ece 0x%x\n",
 -		qp->type, qp->ibqp.qp_num, qp->trans_qp.base.mqp.qpn,
 -		params->attr->recv_cq ? to_mcq(params->attr->recv_cq)->mcq.cqn :
 -					-1,
 -		params->attr->send_cq ? to_mcq(params->attr->send_cq)->mcq.cqn :
 -					-1,
 -		params->resp.ece_options);
 +	switch (init_attr->qp_type) {
 +	case IB_QPT_XRC_TGT:
 +	case IB_QPT_XRC_INI:
 +		if (!MLX5_CAP_GEN(dev->mdev, xrc)) {
 +			mlx5_ib_dbg(dev, "XRC not supported\n");
 +			return ERR_PTR(-ENOSYS);
 +		}
 +		init_attr->recv_cq = NULL;
 +		if (init_attr->qp_type == IB_QPT_XRC_TGT) {
 +			xrcdn = to_mxrcd(init_attr->xrcd)->xrcdn;
 +			init_attr->send_cq = NULL;
 +		}
  
 -	return 0;
 -}
 +		/* fall through */
 +	case IB_QPT_RAW_PACKET:
 +	case IB_QPT_RC:
 +	case IB_QPT_UC:
 +	case IB_QPT_UD:
 +	case IB_QPT_SMI:
 +	case MLX5_IB_QPT_HW_GSI:
 +	case MLX5_IB_QPT_REG_UMR:
 +	case MLX5_IB_QPT_DCI:
 +		qp = kzalloc(sizeof(*qp), GFP_KERNEL);
 +		if (!qp)
 +			return ERR_PTR(-ENOMEM);
  
 -static int check_qp_attr(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 -			 struct ib_qp_init_attr *attr)
 -{
 -	int ret = 0;
 +		err = create_qp_common(dev, pd, init_attr, udata, qp);
 +		if (err) {
 +			mlx5_ib_dbg(dev, "create_qp_common failed\n");
 +			kfree(qp);
 +			return ERR_PTR(err);
 +		}
  
 -	switch (qp->type) {
 -	case MLX5_IB_QPT_DCT:
 -		ret = (!attr->srq || !attr->recv_cq) ? -EINVAL : 0;
 -		break;
 -	case MLX5_IB_QPT_DCI:
 -		ret = (attr->cap.max_recv_wr || attr->cap.max_recv_sge) ?
 -			      -EINVAL :
 -			      0;
 -		break;
 -	case IB_QPT_RAW_PACKET:
 -		ret = (attr->rwq_ind_tbl && attr->send_cq) ? -EINVAL : 0;
 -		break;
 -	default:
 -		break;
 -	}
 +		if (is_qp0(init_attr->qp_type))
 +			qp->ibqp.qp_num = 0;
 +		else if (is_qp1(init_attr->qp_type))
 +			qp->ibqp.qp_num = 1;
 +		else
 +			qp->ibqp.qp_num = qp->trans_qp.base.mqp.qpn;
  
 -	if (ret)
 -		mlx5_ib_dbg(dev, "QP type %d has wrong attributes\n", qp->type);
 +		mlx5_ib_dbg(dev, "ib qpnum 0x%x, mlx qpn 0x%x, rcqn 0x%x, scqn 0x%x\n",
 +			    qp->ibqp.qp_num, qp->trans_qp.base.mqp.qpn,
 +			    init_attr->recv_cq ? to_mcq(init_attr->recv_cq)->mcq.cqn : -1,
 +			    init_attr->send_cq ? to_mcq(init_attr->send_cq)->mcq.cqn : -1);
  
 -	return ret;
 -}
 +		qp->trans_qp.xrcdn = xrcdn;
  
 -static int get_qp_uidx(struct mlx5_ib_qp *qp,
 -		       struct mlx5_create_qp_params *params)
 -{
 -	struct mlx5_ib_create_qp *ucmd = params->ucmd;
 -	struct ib_udata *udata = params->udata;
 -	struct mlx5_ib_ucontext *ucontext = rdma_udata_to_drv_context(
 -		udata, struct mlx5_ib_ucontext, ibucontext);
 +		break;
  
 -	if (params->is_rss_raw)
 -		return 0;
 +	case IB_QPT_GSI:
 +		return mlx5_ib_gsi_create_qp(pd, init_attr);
 +
 +	case IB_QPT_RAW_IPV6:
 +	case IB_QPT_RAW_ETHERTYPE:
 +	case IB_QPT_MAX:
 +	default:
 +		mlx5_ib_dbg(dev, "unsupported qp type %d\n",
 +			    init_attr->qp_type);
 +		/* Don't support raw QPs */
 +		return ERR_PTR(-EINVAL);
 +	}
  
 -	return get_qp_user_index(ucontext, ucmd, sizeof(*ucmd), &params->uidx);
 +	if (verbs_init_attr->qp_type == IB_QPT_DRIVER)
 +		qp->qp_sub_type = init_attr->qp_type;
 +
 +	return &qp->ibqp;
  }
  
  static int mlx5_ib_destroy_dct(struct mlx5_ib_qp *mqp)
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
