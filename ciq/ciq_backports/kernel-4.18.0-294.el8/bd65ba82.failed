KVM: VMX: Add vmx_setup_uret_msr() to handle lookup and swap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit bd65ba82b324e1765121f3602f9b0a89b7aa1c08
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/bd65ba82.failed

Add vmx_setup_uret_msr() to wrap the lookup and manipulation of the uret
MSRs array during setup_msrs().  In addition to consolidating code, this
eliminates move_msr_up(), which while being a very literally description
of the function, isn't exacly helpful in understanding the net effect of
the code.

No functional change intended.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Message-Id: <20200923180409.32255-12-sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit bd65ba82b324e1765121f3602f9b0a89b7aa1c08)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx/vmx.c
diff --cc arch/x86/kvm/vmx/vmx.c
index 985a4bfb7517,3300e373fadf..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -1616,16 -1614,19 +1616,23 @@@ static void vmx_queue_exception(struct 
  	vmx_clear_hlt(vcpu);
  }
  
- /*
-  * Swap MSR entry in host/guest MSR entry array.
-  */
- static void move_msr_up(struct vcpu_vmx *vmx, int from, int to)
+ static void vmx_setup_uret_msr(struct vcpu_vmx *vmx, unsigned int msr)
  {
++<<<<<<< HEAD
 +	struct shared_msr_entry tmp;
++=======
+ 	struct vmx_uret_msr tmp;
+ 	int from, to;
+ 
+ 	from = __vmx_find_uret_msr(vmx, msr);
+ 	if (from < 0)
+ 		return;
+ 	to = vmx->nr_active_uret_msrs++;
++>>>>>>> bd65ba82b324 (KVM: VMX: Add vmx_setup_uret_msr() to handle lookup and swap)
  
 -	tmp = vmx->guest_uret_msrs[to];
 -	vmx->guest_uret_msrs[to] = vmx->guest_uret_msrs[from];
 -	vmx->guest_uret_msrs[from] = tmp;
 +	tmp = vmx->guest_msrs[to];
 +	vmx->guest_msrs[to] = vmx->guest_msrs[from];
 +	vmx->guest_msrs[from] = tmp;
  }
  
  /*
@@@ -1635,38 -1636,26 +1642,58 @@@
   */
  static void setup_msrs(struct vcpu_vmx *vmx)
  {
++<<<<<<< HEAD
 +	int save_nmsrs, index;
 +
 +	save_nmsrs = 0;
++=======
+ 	vmx->guest_uret_msrs_loaded = false;
+ 	vmx->nr_active_uret_msrs = 0;
++>>>>>>> bd65ba82b324 (KVM: VMX: Add vmx_setup_uret_msr() to handle lookup and swap)
  #ifdef CONFIG_X86_64
  	/*
  	 * The SYSCALL MSRs are only needed on long mode guests, and only
  	 * when EFER.SCE is set.
  	 */
  	if (is_long_mode(&vmx->vcpu) && (vmx->vcpu.arch.efer & EFER_SCE)) {
++<<<<<<< HEAD
 +		index = __find_msr_index(vmx, MSR_STAR);
 +		if (index >= 0)
 +			move_msr_up(vmx, index, save_nmsrs++);
 +		index = __find_msr_index(vmx, MSR_LSTAR);
 +		if (index >= 0)
 +			move_msr_up(vmx, index, save_nmsrs++);
 +		index = __find_msr_index(vmx, MSR_SYSCALL_MASK);
 +		if (index >= 0)
 +			move_msr_up(vmx, index, save_nmsrs++);
 +	}
 +#endif
 +	index = __find_msr_index(vmx, MSR_EFER);
 +	if (index >= 0 && update_transition_efer(vmx, index))
 +		move_msr_up(vmx, index, save_nmsrs++);
 +	index = __find_msr_index(vmx, MSR_TSC_AUX);
 +	if (index >= 0 && guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP))
 +		move_msr_up(vmx, index, save_nmsrs++);
 +	index = __find_msr_index(vmx, MSR_IA32_TSX_CTRL);
 +	if (index >= 0)
 +		move_msr_up(vmx, index, save_nmsrs++);
 +
 +	vmx->save_nmsrs = save_nmsrs;
 +	vmx->guest_msrs_ready = false;
++=======
+ 		vmx_setup_uret_msr(vmx, MSR_STAR);
+ 		vmx_setup_uret_msr(vmx, MSR_LSTAR);
+ 		vmx_setup_uret_msr(vmx, MSR_SYSCALL_MASK);
+ 	}
+ #endif
+ 	if (update_transition_efer(vmx))
+ 		vmx_setup_uret_msr(vmx, MSR_EFER);
+ 
+ 	if (guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP))
+ 		vmx_setup_uret_msr(vmx, MSR_TSC_AUX);
+ 
+ 	vmx_setup_uret_msr(vmx, MSR_IA32_TSX_CTRL);
++>>>>>>> bd65ba82b324 (KVM: VMX: Add vmx_setup_uret_msr() to handle lookup and swap)
  
  	if (cpu_has_vmx_msr_bitmap())
  		vmx_update_msr_bitmap(&vmx->vcpu);
* Unmerged path arch/x86/kvm/vmx/vmx.c
