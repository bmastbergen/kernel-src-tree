mm: /proc/pid/smaps: factor out mem stats gathering

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 8e68d689afe3284a5bc1663562d3e0a45d1c64fd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/8e68d689.failed

To prepare for handling /proc/pid/smaps_rollup differently from
/proc/pid/smaps factor out vma mem stats gathering from show_smap() - it
will be used by both.

Link: http://lkml.kernel.org/r/20180723111933.15443-3-vbabka@suse.cz
	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Alexey Dobriyan <adobriyan@gmail.com>
	Cc: Daniel Colascione <dancol@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8e68d689afe3284a5bc1663562d3e0a45d1c64fd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/proc/task_mmu.c
diff --cc fs/proc/task_mmu.c
index b0419ee12f91,d2ca88c92d9d..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -702,51 -700,22 +702,64 @@@ static int smaps_hugetlb_range(pte_t *p
  	}
  	return 0;
  }
 +#else
 +#define smaps_hugetlb_range	NULL
  #endif /* HUGETLB_PAGE */
  
++<<<<<<< HEAD
 +static const struct mm_walk_ops smaps_walk_ops = {
 +	.pmd_entry		= smaps_pte_range,
 +	.hugetlb_entry		= smaps_hugetlb_range,
 +};
 +
 +static const struct mm_walk_ops smaps_shmem_walk_ops = {
 +	.pmd_entry		= smaps_pte_range,
 +	.hugetlb_entry		= smaps_hugetlb_range,
 +	.pte_hole		= smaps_pte_hole,
 +};
 +
 +#define SEQ_PUT_DEC(str, val) \
 +		seq_put_decimal_ull_width(m, str, (val) >> 10, 8)
 +static int show_smap(struct seq_file *m, void *v)
 +{
 +	struct proc_maps_private *priv = m->private;
 +	struct vm_area_struct *vma = v;
 +	struct mem_size_stats mss_stack;
 +	struct mem_size_stats *mss;
 +	int ret = 0;
 +	bool rollup_mode;
 +	bool last_vma;
 +	bool walking = false;
 +
 +	if (priv->rollup) {
 +		rollup_mode = true;
 +		mss = priv->rollup;
 +		if (mss->first) {
 +			mss->first_vma_start = vma->vm_start;
 +			mss->first = false;
 +		}
 +		last_vma = !m_next_vma(priv, vma);
 +	} else {
 +		rollup_mode = false;
 +		memset(&mss_stack, 0, sizeof(mss_stack));
 +		mss = &mss_stack;
 +	}
++=======
+ static void smap_gather_stats(struct vm_area_struct *vma,
+ 			     struct mem_size_stats *mss)
+ {
+ 	struct mm_walk smaps_walk = {
+ 		.pmd_entry = smaps_pte_range,
+ #ifdef CONFIG_HUGETLB_PAGE
+ 		.hugetlb_entry = smaps_hugetlb_range,
+ #endif
+ 		.mm = vma->vm_mm,
+ 	};
 -
 -	smaps_walk.private = mss;
++>>>>>>> 8e68d689afe3 (mm: /proc/pid/smaps: factor out mem stats gathering)
  
  #ifdef CONFIG_SHMEM
 +	/* In case of smaps_rollup, reset the value from previous vma */
 +	mss->check_shmem_swap = false;
  	if (vma->vm_file && shmem_mapping(vma->vm_file->f_mapping)) {
  		/*
  		 * For shared or readonly shmem mappings we know that all
@@@ -772,10 -740,38 +785,39 @@@
  #endif
  
  	/* mmap_sem is held in m_start */
 -	walk_page_vma(vma, &smaps_walk);
 +	if (!walking)
 +		walk_page_vma(vma, &smaps_walk_ops, mss);
  	if (vma->vm_flags & VM_LOCKED)
  		mss->pss_locked += mss->pss;
+ }
+ 
+ #define SEQ_PUT_DEC(str, val) \
+ 		seq_put_decimal_ull_width(m, str, (val) >> 10, 8)
+ static int show_smap(struct seq_file *m, void *v)
+ {
+ 	struct proc_maps_private *priv = m->private;
+ 	struct vm_area_struct *vma = v;
+ 	struct mem_size_stats mss_stack;
+ 	struct mem_size_stats *mss;
+ 	int ret = 0;
+ 	bool rollup_mode;
+ 	bool last_vma;
+ 
+ 	if (priv->rollup) {
+ 		rollup_mode = true;
+ 		mss = priv->rollup;
+ 		if (mss->first) {
+ 			mss->first_vma_start = vma->vm_start;
+ 			mss->first = false;
+ 		}
+ 		last_vma = !m_next_vma(priv, vma);
+ 	} else {
+ 		rollup_mode = false;
+ 		memset(&mss_stack, 0, sizeof(mss_stack));
+ 		mss = &mss_stack;
+ 	}
+ 
+ 	smap_gather_stats(vma, mss);
  
  	if (!rollup_mode) {
  		show_map_vma(m, vma);
* Unmerged path fs/proc/task_mmu.c
