seqlock: Extend seqcount API with associated locks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Ahmed S. Darwish <a.darwish@linutronix.de>
commit 55f3560df975f557c48aa6afc636808f31ecb87a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/55f3560d.failed

A sequence counter write side critical section must be protected by some
form of locking to serialize writers. If the serialization primitive is
not disabling preemption implicitly, preemption has to be explicitly
disabled before entering the write side critical section.

There is no built-in debugging mechanism to verify that the lock used
for writer serialization is held and preemption is disabled. Some usage
sites like dma-buf have explicit lockdep checks for the writer-side
lock, but this covers only a small portion of the sequence counter usage
in the kernel.

Add new sequence counter types which allows to associate a lock to the
sequence counter at initialization time. The seqcount API functions are
extended to provide appropriate lockdep assertions depending on the
seqcount/lock type.

For sequence counters with associated locks that do not implicitly
disable preemption, preemption protection is enforced in the sequence
counter write side functions. This removes the need to explicitly add
preempt_disable/enable() around the write side critical sections: the
write_begin/end() functions for these new sequence counter types
automatically do this.

Introduce the following seqcount types with associated locks:

     seqcount_spinlock_t
     seqcount_raw_spinlock_t
     seqcount_rwlock_t
     seqcount_mutex_t
     seqcount_ww_mutex_t

Extend the seqcount read and write functions to branch out to the
specific seqcount_LOCKTYPE_t implementation at compile-time. This avoids
kernel API explosion per each new seqcount_LOCKTYPE_t added. Add such
compile-time type detection logic into a new, internal, seqlock header.

Document the proper seqcount_LOCKTYPE_t usage, and rationale, at
Documentation/locking/seqlock.rst.

If lockdep is disabled, this lock association is compiled out and has
neither storage size nor runtime overhead.

	Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20200720155530.1173732-10-a.darwish@linutronix.de
(cherry picked from commit 55f3560df975f557c48aa6afc636808f31ecb87a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/locking/seqlock.rst
#	include/linux/seqlock.h
diff --cc include/linux/seqlock.h
index 362623ec6c41,8c16a494c968..000000000000
--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@@ -1,43 -1,26 +1,56 @@@
  /* SPDX-License-Identifier: GPL-2.0 */
  #ifndef __LINUX_SEQLOCK_H
  #define __LINUX_SEQLOCK_H
 -
  /*
 - * seqcount_t / seqlock_t - a reader-writer consistency mechanism with
 - * lockless readers (read-only retry loops), and no writer starvation.
 - *
 - * See Documentation/locking/seqlock.rst
 - *
 + * Reader/writer consistent mechanism without starving writers. This type of
 + * lock for data where the reader wants a consistent set of information
 + * and is willing to retry if the information changes. There are two types
 + * of readers:
 + * 1. Sequence readers which never block a writer but they may have to retry
 + *    if a writer is in progress by detecting change in sequence number.
 + *    Writers do not wait for a sequence reader.
 + * 2. Locking readers which will wait if a writer or another locking reader
 + *    is in progress. A locking reader in progress will also block a writer
 + *    from going forward. Unlike the regular rwlock, the read lock here is
 + *    exclusive so that only one locking reader can get it.
 + *
 + * This is not as cache friendly as brlock. Also, this may not work well
 + * for data that contains pointers, because any writer could
 + * invalidate a pointer that a reader was following.
 + *
++<<<<<<< HEAD
 + * Expected non-blocking reader usage:
 + * 	do {
 + *	    seq = read_seqbegin(&foo);
 + * 	...
 + *      } while (read_seqretry(&foo, seq));
 + *
 + *
 + * On non-SMP the spin locks disappear but the writer still needs
 + * to increment the sequence variables because an interrupt routine could
 + * change the state of the data.
 + *
 + * Based on x86_64 vsyscall gettimeofday 
 + * by Keith Owens and Andrea Arcangeli
++=======
+  * Copyrights:
+  * - Based on x86_64 vsyscall gettimeofday: Keith Owens, Andrea Arcangeli
+  * - Sequence counters with associated locks, (C) 2020 Linutronix GmbH
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   */
  
- #include <linux/spinlock.h>
- #include <linux/preempt.h>
- #include <linux/lockdep.h>
  #include <linux/compiler.h>
++<<<<<<< HEAD
 +#include <linux/kcsan.h>
++=======
+ #include <linux/kcsan-checks.h>
+ #include <linux/lockdep.h>
+ #include <linux/mutex.h>
+ #include <linux/preempt.h>
+ #include <linux/spinlock.h>
+ #include <linux/ww_mutex.h>
+ 
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  #include <asm/processor.h>
  
  /*
@@@ -56,10 -39,28 +69,35 @@@
  #define KCSAN_SEQLOCK_REGION_MAX 1000
  
  /*
++<<<<<<< HEAD
 + * Version using sequence counter only.
 + * This can be used when code has its own mutex protecting the
 + * updating starting before the write_seqcountbeqin() and ending
 + * after the write_seqcount_end().
++=======
+  * Sequence counters (seqcount_t)
+  *
+  * This is the raw counting mechanism, without any writer protection.
+  *
+  * Write side critical sections must be serialized and non-preemptible.
+  *
+  * If readers can be invoked from hardirq or softirq contexts,
+  * interrupts or bottom halves must also be respectively disabled before
+  * entering the write section.
+  *
+  * This mechanism can't be used if the protected data contains pointers,
+  * as the writer can invalidate a pointer that a reader is following.
+  *
+  * If the write serialization mechanism is one of the common kernel
+  * locking primitives, use a sequence counter with associated lock
+  * (seqcount_LOCKTYPE_t) instead.
+  *
+  * If it's desired to automatically handle the sequence counter writer
+  * serialization and non-preemptibility requirements, use a sequential
+  * lock (seqlock_t) instead.
+  *
+  * See Documentation/locking/seqlock.rst
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   */
  typedef struct seqcount {
  	unsigned sequence;
@@@ -105,13 -110,273 +143,276 @@@ static inline void seqcount_lockdep_rea
  # define seqcount_lockdep_reader_access(x)
  #endif
  
 -/**
 - * SEQCNT_ZERO() - static initializer for seqcount_t
 - * @name: Name of the seqcount_t instance
 - */
 -#define SEQCNT_ZERO(name) { .sequence = 0, SEQCOUNT_DEP_MAP_INIT(name) }
 +#define SEQCNT_ZERO(lockname) { .sequence = 0, SEQCOUNT_DEP_MAP_INIT(lockname)}
 +
  
+ /*
+  * Sequence counters with associated locks (seqcount_LOCKTYPE_t)
+  *
+  * A sequence counter which associates the lock used for writer
+  * serialization at initialization time. This enables lockdep to validate
+  * that the write side critical section is properly serialized.
+  *
+  * For associated locks which do not implicitly disable preemption,
+  * preemption protection is enforced in the write side function.
+  *
+  * Lockdep is never used in any for the raw write variants.
+  *
+  * See Documentation/locking/seqlock.rst
+  */
+ 
+ #ifdef CONFIG_LOCKDEP
+ #define __SEQ_LOCKDEP(expr)	expr
+ #else
+ #define __SEQ_LOCKDEP(expr)
+ #endif
+ 
+ #define SEQCOUNT_LOCKTYPE_ZERO(seq_name, assoc_lock) {			\
+ 	.seqcount		= SEQCNT_ZERO(seq_name.seqcount),	\
+ 	__SEQ_LOCKDEP(.lock	= (assoc_lock))				\
+ }
+ 
+ #define seqcount_locktype_init(s, assoc_lock)				\
+ do {									\
+ 	seqcount_init(&(s)->seqcount);					\
+ 	__SEQ_LOCKDEP((s)->lock = (assoc_lock));			\
+ } while (0)
+ 
+ /**
+  * typedef seqcount_spinlock_t - sequence counter with spinlock associated
+  * @seqcount:	The real sequence counter
+  * @lock:	Pointer to the associated spinlock
+  *
+  * A plain sequence counter with external writer synchronization by a
+  * spinlock. The spinlock is associated to the sequence count in the
+  * static initializer or init function. This enables lockdep to validate
+  * that the write side critical section is properly serialized.
+  */
+ typedef struct seqcount_spinlock {
+ 	seqcount_t	seqcount;
+ 	__SEQ_LOCKDEP(spinlock_t	*lock);
+ } seqcount_spinlock_t;
+ 
+ /**
+  * SEQCNT_SPINLOCK_ZERO - static initializer for seqcount_spinlock_t
+  * @name:	Name of the seqcount_spinlock_t instance
+  * @lock:	Pointer to the associated spinlock
+  */
+ #define SEQCNT_SPINLOCK_ZERO(name, lock)				\
+ 	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ 
+ /**
+  * seqcount_spinlock_init - runtime initializer for seqcount_spinlock_t
+  * @s:		Pointer to the seqcount_spinlock_t instance
+  * @lock:	Pointer to the associated spinlock
+  */
+ #define seqcount_spinlock_init(s, lock)					\
+ 	seqcount_locktype_init(s, lock)
+ 
+ /**
+  * typedef seqcount_raw_spinlock_t - sequence count with raw spinlock associated
+  * @seqcount:	The real sequence counter
+  * @lock:	Pointer to the associated raw spinlock
+  *
+  * A plain sequence counter with external writer synchronization by a
+  * raw spinlock. The raw spinlock is associated to the sequence count in
+  * the static initializer or init function. This enables lockdep to
+  * validate that the write side critical section is properly serialized.
+  */
+ typedef struct seqcount_raw_spinlock {
+ 	seqcount_t      seqcount;
+ 	__SEQ_LOCKDEP(raw_spinlock_t	*lock);
+ } seqcount_raw_spinlock_t;
+ 
+ /**
+  * SEQCNT_RAW_SPINLOCK_ZERO - static initializer for seqcount_raw_spinlock_t
+  * @name:	Name of the seqcount_raw_spinlock_t instance
+  * @lock:	Pointer to the associated raw_spinlock
+  */
+ #define SEQCNT_RAW_SPINLOCK_ZERO(name, lock)				\
+ 	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ 
+ /**
+  * seqcount_raw_spinlock_init - runtime initializer for seqcount_raw_spinlock_t
+  * @s:		Pointer to the seqcount_raw_spinlock_t instance
+  * @lock:	Pointer to the associated raw_spinlock
+  */
+ #define seqcount_raw_spinlock_init(s, lock)				\
+ 	seqcount_locktype_init(s, lock)
+ 
+ /**
+  * typedef seqcount_rwlock_t - sequence count with rwlock associated
+  * @seqcount:	The real sequence counter
+  * @lock:	Pointer to the associated rwlock
+  *
+  * A plain sequence counter with external writer synchronization by a
+  * rwlock. The rwlock is associated to the sequence count in the static
+  * initializer or init function. This enables lockdep to validate that
+  * the write side critical section is properly serialized.
+  */
+ typedef struct seqcount_rwlock {
+ 	seqcount_t      seqcount;
+ 	__SEQ_LOCKDEP(rwlock_t		*lock);
+ } seqcount_rwlock_t;
+ 
+ /**
+  * SEQCNT_RWLOCK_ZERO - static initializer for seqcount_rwlock_t
+  * @name:	Name of the seqcount_rwlock_t instance
+  * @lock:	Pointer to the associated rwlock
+  */
+ #define SEQCNT_RWLOCK_ZERO(name, lock)					\
+ 	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ 
+ /**
+  * seqcount_rwlock_init - runtime initializer for seqcount_rwlock_t
+  * @s:		Pointer to the seqcount_rwlock_t instance
+  * @lock:	Pointer to the associated rwlock
+  */
+ #define seqcount_rwlock_init(s, lock)					\
+ 	seqcount_locktype_init(s, lock)
+ 
+ /**
+  * typedef seqcount_mutex_t - sequence count with mutex associated
+  * @seqcount:	The real sequence counter
+  * @lock:	Pointer to the associated mutex
+  *
+  * A plain sequence counter with external writer synchronization by a
+  * mutex. The mutex is associated to the sequence counter in the static
+  * initializer or init function. This enables lockdep to validate that
+  * the write side critical section is properly serialized.
+  *
+  * The write side API functions write_seqcount_begin()/end() automatically
+  * disable and enable preemption when used with seqcount_mutex_t.
+  */
+ typedef struct seqcount_mutex {
+ 	seqcount_t      seqcount;
+ 	__SEQ_LOCKDEP(struct mutex	*lock);
+ } seqcount_mutex_t;
+ 
+ /**
+  * SEQCNT_MUTEX_ZERO - static initializer for seqcount_mutex_t
+  * @name:	Name of the seqcount_mutex_t instance
+  * @lock:	Pointer to the associated mutex
+  */
+ #define SEQCNT_MUTEX_ZERO(name, lock)					\
+ 	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ 
+ /**
+  * seqcount_mutex_init - runtime initializer for seqcount_mutex_t
+  * @s:		Pointer to the seqcount_mutex_t instance
+  * @lock:	Pointer to the associated mutex
+  */
+ #define seqcount_mutex_init(s, lock)					\
+ 	seqcount_locktype_init(s, lock)
+ 
  /**
+  * typedef seqcount_ww_mutex_t - sequence count with ww_mutex associated
+  * @seqcount:	The real sequence counter
+  * @lock:	Pointer to the associated ww_mutex
+  *
+  * A plain sequence counter with external writer synchronization by a
+  * ww_mutex. The ww_mutex is associated to the sequence counter in the static
+  * initializer or init function. This enables lockdep to validate that
+  * the write side critical section is properly serialized.
+  *
+  * The write side API functions write_seqcount_begin()/end() automatically
+  * disable and enable preemption when used with seqcount_ww_mutex_t.
+  */
+ typedef struct seqcount_ww_mutex {
+ 	seqcount_t      seqcount;
+ 	__SEQ_LOCKDEP(struct ww_mutex	*lock);
+ } seqcount_ww_mutex_t;
+ 
+ /**
+  * SEQCNT_WW_MUTEX_ZERO - static initializer for seqcount_ww_mutex_t
+  * @name:	Name of the seqcount_ww_mutex_t instance
+  * @lock:	Pointer to the associated ww_mutex
+  */
+ #define SEQCNT_WW_MUTEX_ZERO(name, lock)				\
+ 	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ 
+ /**
+  * seqcount_ww_mutex_init - runtime initializer for seqcount_ww_mutex_t
+  * @s:		Pointer to the seqcount_ww_mutex_t instance
+  * @lock:	Pointer to the associated ww_mutex
+  */
+ #define seqcount_ww_mutex_init(s, lock)					\
+ 	seqcount_locktype_init(s, lock)
+ 
+ /*
+  * @preempt: Is the associated write serialization lock preemtpible?
+  */
+ #define SEQCOUNT_LOCKTYPE(locktype, preempt, lockmember)		\
+ static inline seqcount_t *						\
+ __seqcount_##locktype##_ptr(seqcount_##locktype##_t *s)			\
+ {									\
+ 	return &s->seqcount;						\
+ }									\
+ 									\
+ static inline bool							\
+ __seqcount_##locktype##_preemptible(seqcount_##locktype##_t *s)		\
+ {									\
+ 	return preempt;							\
+ }									\
+ 									\
+ static inline void							\
+ __seqcount_##locktype##_assert(seqcount_##locktype##_t *s)		\
+ {									\
+ 	__SEQ_LOCKDEP(lockdep_assert_held(lockmember));			\
+ }
+ 
+ /*
+  * Similar hooks, but for plain seqcount_t
+  */
+ 
+ static inline seqcount_t *__seqcount_ptr(seqcount_t *s)
+ {
+ 	return s;
+ }
+ 
+ static inline bool __seqcount_preemptible(seqcount_t *s)
+ {
+ 	return false;
+ }
+ 
+ static inline void __seqcount_assert(seqcount_t *s)
+ {
+ 	lockdep_assert_preemption_disabled();
+ }
+ 
+ /*
+  * @s: Pointer to seqcount_locktype_t, generated hooks first parameter.
+  */
+ SEQCOUNT_LOCKTYPE(raw_spinlock,	false,	s->lock)
+ SEQCOUNT_LOCKTYPE(spinlock,	false,	s->lock)
+ SEQCOUNT_LOCKTYPE(rwlock,	false,	s->lock)
+ SEQCOUNT_LOCKTYPE(mutex,	true,	s->lock)
+ SEQCOUNT_LOCKTYPE(ww_mutex,	true,	&s->lock->base)
+ 
+ #define __seqprop_case(s, locktype, prop)				\
+ 	seqcount_##locktype##_t: __seqcount_##locktype##_##prop((void *)(s))
+ 
+ #define __seqprop(s, prop) _Generic(*(s),				\
+ 	seqcount_t:		__seqcount_##prop((void *)(s)),		\
+ 	__seqprop_case((s),	raw_spinlock,	prop),			\
+ 	__seqprop_case((s),	spinlock,	prop),			\
+ 	__seqprop_case((s),	rwlock,		prop),			\
+ 	__seqprop_case((s),	mutex,		prop),			\
+ 	__seqprop_case((s),	ww_mutex,	prop))
+ 
+ #define __to_seqcount_t(s)				__seqprop(s, ptr)
+ #define __associated_lock_exists_and_is_preemptible(s)	__seqprop(s, preemptible)
+ #define __assert_write_section_is_protected(s)		__seqprop(s, assert)
+ 
+ /**
++<<<<<<< HEAD
 + * __read_seqcount_begin - begin a seq-read critical section (without barrier)
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
++=======
+  * __read_seqcount_begin() - begin a seqcount_t read section w/o barrier
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   *
   * __read_seqcount_begin is like read_seqcount_begin, but has no smp_rmb()
   * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
@@@ -120,8 -385,13 +421,11 @@@
   *
   * Use carefully, only in critical code, and comment how the barrier is
   * provided.
 - *
 - * Return: count to be passed to read_seqcount_retry()
   */
- static inline unsigned __read_seqcount_begin(const seqcount_t *s)
+ #define __read_seqcount_begin(s)					\
+ 	__read_seqcount_t_begin(__to_seqcount_t(s))
+ 
+ static inline unsigned __read_seqcount_t_begin(const seqcount_t *s)
  {
  	unsigned ret;
  
@@@ -136,15 -406,51 +440,54 @@@ repeat
  }
  
  /**
++<<<<<<< HEAD
 + * raw_read_seqcount - Read the raw seqcount
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
++=======
+  * raw_read_seqcount_begin() - begin a seqcount_t read section w/o lockdep
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * Return: count to be passed to read_seqcount_retry()
+  */
+ #define raw_read_seqcount_begin(s)					\
+ 	raw_read_seqcount_t_begin(__to_seqcount_t(s))
+ 
+ static inline unsigned raw_read_seqcount_t_begin(const seqcount_t *s)
+ {
+ 	unsigned ret = __read_seqcount_t_begin(s);
+ 	smp_rmb();
+ 	return ret;
+ }
+ 
+ /**
+  * read_seqcount_begin() - begin a seqcount_t read critical section
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * Return: count to be passed to read_seqcount_retry()
+  */
+ #define read_seqcount_begin(s)						\
+ 	read_seqcount_t_begin(__to_seqcount_t(s))
+ 
+ static inline unsigned read_seqcount_t_begin(const seqcount_t *s)
+ {
+ 	seqcount_lockdep_reader_access(s);
+ 	return raw_read_seqcount_t_begin(s);
+ }
+ 
+ /**
+  * raw_read_seqcount() - read the raw seqcount_t counter value
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   *
   * raw_read_seqcount opens a read critical section of the given
 - * seqcount_t, without any lockdep checking, and without checking or
 - * masking the sequence counter LSB. Calling code is responsible for
 - * handling that.
 - *
 - * Return: count to be passed to read_seqcount_retry()
 + * seqcount without any lockdep checking and without checking or
 + * masking the LSB. Calling code is responsible for handling that.
   */
- static inline unsigned raw_read_seqcount(const seqcount_t *s)
+ #define raw_read_seqcount(s)						\
+ 	raw_read_seqcount_t(__to_seqcount_t(s))
+ 
+ static inline unsigned raw_read_seqcount_t(const seqcount_t *s)
  {
  	unsigned ret = READ_ONCE(s->sequence);
  	smp_rmb();
@@@ -153,51 -459,26 +496,60 @@@
  }
  
  /**
++<<<<<<< HEAD
 + * raw_read_seqcount_begin - start seq-read critical section w/o lockdep
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
++=======
+  * raw_seqcount_begin() - begin a seqcount_t read critical section w/o
+  *                        lockdep and w/o counter stabilization
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   *
 - * raw_seqcount_begin opens a read critical section of the given
 - * seqcount_t. Unlike read_seqcount_begin(), this function will not wait
 - * for the count to stabilize. If a writer is active when it begins, it
 - * will fail the read_seqcount_retry() at the end of the read critical
 - * section instead of stabilizing at the beginning of it.
 + * raw_read_seqcount_begin opens a read critical section of the given
 + * seqcount, but without any lockdep checking. Validity of the critical
 + * section is tested by checking read_seqcount_retry function.
 + */
 +static inline unsigned raw_read_seqcount_begin(const seqcount_t *s)
 +{
 +	unsigned ret = __read_seqcount_begin(s);
 +	smp_rmb();
 +	return ret;
 +}
 +
 +/**
 + * read_seqcount_begin - begin a seq-read critical section
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
   *
 - * Use this only in special kernel hot paths where the read section is
 - * small and has a high probability of success through other external
 - * means. It will save a single branching instruction.
 + * read_seqcount_begin opens a read critical section of the given seqcount.
 + * Validity of the critical section is tested by checking read_seqcount_retry
 + * function.
 + */
 +static inline unsigned read_seqcount_begin(const seqcount_t *s)
 +{
 +	seqcount_lockdep_reader_access(s);
 +	return raw_read_seqcount_begin(s);
 +}
 +
 +/**
 + * raw_seqcount_begin - begin a seq-read critical section
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
   *
 - * Return: count to be passed to read_seqcount_retry()
 + * raw_seqcount_begin opens a read critical section of the given seqcount.
 + * Validity of the critical section is tested by checking read_seqcount_retry
 + * function.
 + *
 + * Unlike read_seqcount_begin(), this function will not wait for the count
 + * to stabilize. If a writer is active when we begin, we will fail the
 + * read_seqcount_retry() instead of stabilizing at the beginning of the
 + * critical section.
   */
- static inline unsigned raw_seqcount_begin(const seqcount_t *s)
+ #define raw_seqcount_begin(s)						\
+ 	raw_seqcount_t_begin(__to_seqcount_t(s))
+ 
+ static inline unsigned raw_seqcount_t_begin(const seqcount_t *s)
  {
  	/*
  	 * If the counter is odd, let read_seqcount_retry() fail
@@@ -207,10 -488,9 +559,16 @@@
  }
  
  /**
++<<<<<<< HEAD
 + * __read_seqcount_retry - end a seq-read critical section (without barrier)
 + * @s: pointer to seqcount_t
 + * @start: count, from read_seqcount_begin
 + * Returns: 1 if retry is required, else 0
++=======
+  * __read_seqcount_retry() - end a seqcount_t read section w/o barrier
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  * @start: count, from read_seqcount_begin()
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   *
   * __read_seqcount_retry is like read_seqcount_retry, but has no smp_rmb()
   * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
@@@ -219,39 -499,70 +577,83 @@@
   *
   * Use carefully, only in critical code, and comment how the barrier is
   * provided.
 - *
 - * Return: true if a read section retry is required, else false
   */
- static inline int __read_seqcount_retry(const seqcount_t *s, unsigned start)
+ #define __read_seqcount_retry(s, start)					\
+ 	__read_seqcount_t_retry(__to_seqcount_t(s), start)
+ 
+ static inline int __read_seqcount_t_retry(const seqcount_t *s, unsigned start)
  {
  	kcsan_atomic_next(0);
  	return unlikely(READ_ONCE(s->sequence) != start);
  }
  
  /**
++<<<<<<< HEAD
 + * read_seqcount_retry - end a seq-read critical section
 + * @s: pointer to seqcount_t
 + * @start: count, from read_seqcount_begin
 + * Returns: 1 if retry is required, else 0
++=======
+  * read_seqcount_retry() - end a seqcount_t read critical section
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  * @start: count, from read_seqcount_begin()
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   *
 - * read_seqcount_retry closes the read critical section of given
 - * seqcount_t.  If the critical section was invalid, it must be ignored
 - * (and typically retried).
 - *
 - * Return: true if a read section retry is required, else false
 + * read_seqcount_retry closes a read critical section of the given seqcount.
 + * If the critical section was invalid, it must be ignored (and typically
 + * retried).
   */
- static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)
+ #define read_seqcount_retry(s, start)					\
+ 	read_seqcount_t_retry(__to_seqcount_t(s), start)
+ 
+ static inline int read_seqcount_t_retry(const seqcount_t *s, unsigned start)
  {
  	smp_rmb();
- 	return __read_seqcount_retry(s, start);
+ 	return __read_seqcount_t_retry(s, start);
  }
  
++<<<<<<< HEAD
 +
 +
 +static inline void raw_write_seqcount_begin(seqcount_t *s)
++=======
+ /**
+  * raw_write_seqcount_begin() - start a seqcount_t write section w/o lockdep
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  */
+ #define raw_write_seqcount_begin(s)					\
+ do {									\
+ 	if (__associated_lock_exists_and_is_preemptible(s))		\
+ 		preempt_disable();					\
+ 									\
+ 	raw_write_seqcount_t_begin(__to_seqcount_t(s));			\
+ } while (0)
+ 
+ static inline void raw_write_seqcount_t_begin(seqcount_t *s)
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  {
  	kcsan_nestable_atomic_begin();
  	s->sequence++;
  	smp_wmb();
  }
  
++<<<<<<< HEAD
 +static inline void raw_write_seqcount_end(seqcount_t *s)
++=======
+ /**
+  * raw_write_seqcount_end() - end a seqcount_t write section w/o lockdep
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  */
+ #define raw_write_seqcount_end(s)					\
+ do {									\
+ 	raw_write_seqcount_t_end(__to_seqcount_t(s));			\
+ 									\
+ 	if (__associated_lock_exists_and_is_preemptible(s))		\
+ 		preempt_enable();					\
+ } while (0)
+ 
+ static inline void raw_write_seqcount_t_end(seqcount_t *s)
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  {
  	smp_wmb();
  	s->sequence++;
@@@ -259,14 -570,84 +661,95 @@@
  }
  
  /**
++<<<<<<< HEAD
 + * raw_write_seqcount_barrier - do a seq write barrier
 + * @s: pointer to seqcount_t
 + *
 + * This can be used to provide an ordering guarantee instead of the
 + * usual consistency guarantee. It is one wmb cheaper, because we can
 + * collapse the two back-to-back wmb()s.
 + *
 + * Note that, writes surrounding the barrier should be declared atomic (e.g.
++=======
+  * write_seqcount_begin_nested() - start a seqcount_t write section with
+  *                                 custom lockdep nesting level
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  * @subclass: lockdep nesting level
+  *
+  * See Documentation/locking/lockdep-design.rst
+  */
+ #define write_seqcount_begin_nested(s, subclass)			\
+ do {									\
+ 	__assert_write_section_is_protected(s);				\
+ 									\
+ 	if (__associated_lock_exists_and_is_preemptible(s))		\
+ 		preempt_disable();					\
+ 									\
+ 	write_seqcount_t_begin_nested(__to_seqcount_t(s), subclass);	\
+ } while (0)
+ 
+ static inline void write_seqcount_t_begin_nested(seqcount_t *s, int subclass)
+ {
+ 	raw_write_seqcount_t_begin(s);
+ 	seqcount_acquire(&s->dep_map, subclass, 0, _RET_IP_);
+ }
+ 
+ /**
+  * write_seqcount_begin() - start a seqcount_t write side critical section
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * write_seqcount_begin opens a write side critical section of the given
+  * seqcount_t.
+  *
+  * Context: seqcount_t write side critical sections must be serialized and
+  * non-preemptible. If readers can be invoked from hardirq or softirq
+  * context, interrupts or bottom halves must be respectively disabled.
+  */
+ #define write_seqcount_begin(s)						\
+ do {									\
+ 	__assert_write_section_is_protected(s);				\
+ 									\
+ 	if (__associated_lock_exists_and_is_preemptible(s))		\
+ 		preempt_disable();					\
+ 									\
+ 	write_seqcount_t_begin(__to_seqcount_t(s));			\
+ } while (0)
+ 
+ static inline void write_seqcount_t_begin(seqcount_t *s)
+ {
+ 	write_seqcount_t_begin_nested(s, 0);
+ }
+ 
+ /**
+  * write_seqcount_end() - end a seqcount_t write side critical section
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * The write section must've been opened with write_seqcount_begin().
+  */
+ #define write_seqcount_end(s)						\
+ do {									\
+ 	write_seqcount_t_end(__to_seqcount_t(s));			\
+ 									\
+ 	if (__associated_lock_exists_and_is_preemptible(s))		\
+ 		preempt_enable();					\
+ } while (0)
+ 
+ static inline void write_seqcount_t_end(seqcount_t *s)
+ {
+ 	seqcount_release(&s->dep_map, _RET_IP_);
+ 	raw_write_seqcount_t_end(s);
+ }
+ 
+ /**
+  * raw_write_seqcount_barrier() - do a seqcount_t write barrier
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * This can be used to provide an ordering guarantee instead of the usual
+  * consistency guarantee. It is one wmb cheaper, because it can collapse
+  * the two back-to-back wmb()s.
+  *
+  * Note that writes surrounding the barrier should be declared atomic (e.g.
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   * via WRITE_ONCE): a) to ensure the writes become visible to other threads
   * atomically, avoiding compiler optimizations; b) to document which writes are
   * meant to propagate to the reader critical section. This is necessary because
@@@ -308,7 -692,44 +794,48 @@@ static inline void raw_write_seqcount_t
  	kcsan_nestable_atomic_end();
  }
  
++<<<<<<< HEAD
 +static inline int raw_read_seqcount_latch(seqcount_t *s)
++=======
+ /**
+  * write_seqcount_invalidate() - invalidate in-progress seqcount_t read
+  *                               side operations
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * After write_seqcount_invalidate, no seqcount_t read side operations
+  * will complete successfully and see data older than this.
+  */
+ #define write_seqcount_invalidate(s)					\
+ 	write_seqcount_t_invalidate(__to_seqcount_t(s))
+ 
+ static inline void write_seqcount_t_invalidate(seqcount_t *s)
+ {
+ 	smp_wmb();
+ 	kcsan_nestable_atomic_begin();
+ 	s->sequence+=2;
+ 	kcsan_nestable_atomic_end();
+ }
+ 
+ /**
+  * raw_read_seqcount_latch() - pick even/odd seqcount_t latch data copy
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * Use seqcount_t latching to switch between two storage places protected
+  * by a sequence counter. Doing so allows having interruptible, preemptible,
+  * seqcount_t write side critical sections.
+  *
+  * Check raw_write_seqcount_latch() for more details and a full reader and
+  * writer usage example.
+  *
+  * Return: sequence counter raw value. Use the lowest bit as an index for
+  * picking which data copy to read. The full counter value must then be
+  * checked with read_seqcount_retry().
+  */
+ #define raw_read_seqcount_latch(s)					\
+ 	raw_read_seqcount_t_latch(__to_seqcount_t(s))
+ 
+ static inline int raw_read_seqcount_t_latch(seqcount_t *s)
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  {
  	/* Pairs with the first smp_wmb() in raw_write_seqcount_latch() */
  	int seq = READ_ONCE(s->sequence); /* ^^^ */
@@@ -316,8 -737,8 +843,13 @@@
  }
  
  /**
++<<<<<<< HEAD
 + * raw_write_seqcount_latch - redirect readers to even/odd copy
 + * @s: pointer to seqcount_t
++=======
+  * raw_write_seqcount_latch() - redirect readers to even/odd copy
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   *
   * The latch technique is a multiversion concurrency control method that allows
   * queries during non-atomic modifications. If you can guarantee queries never
@@@ -494,36 -918,72 +1029,48 @@@ static inline unsigned read_seqretry(co
  static inline void write_seqlock(seqlock_t *sl)
  {
  	spin_lock(&sl->lock);
++<<<<<<< HEAD
 +	write_seqcount_begin(&sl->seqcount);
++=======
+ 	write_seqcount_t_begin(&sl->seqcount);
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  }
  
 -/**
 - * write_sequnlock() - end a seqlock_t write side critical section
 - * @sl: Pointer to seqlock_t
 - *
 - * write_sequnlock closes the (serialized and non-preemptible) write side
 - * critical section of given seqlock_t.
 - */
  static inline void write_sequnlock(seqlock_t *sl)
  {
- 	write_seqcount_end(&sl->seqcount);
+ 	write_seqcount_t_end(&sl->seqcount);
  	spin_unlock(&sl->lock);
  }
  
  static inline void write_seqlock_bh(seqlock_t *sl)
  {
  	spin_lock_bh(&sl->lock);
++<<<<<<< HEAD
 +	write_seqcount_begin(&sl->seqcount);
++=======
+ 	write_seqcount_t_begin(&sl->seqcount);
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  }
  
 -/**
 - * write_sequnlock_bh() - end a softirqs-disabled seqlock_t write section
 - * @sl: Pointer to seqlock_t
 - *
 - * write_sequnlock_bh closes the serialized, non-preemptible, and
 - * softirqs-disabled, seqlock_t write side critical section opened with
 - * write_seqlock_bh().
 - */
  static inline void write_sequnlock_bh(seqlock_t *sl)
  {
- 	write_seqcount_end(&sl->seqcount);
+ 	write_seqcount_t_end(&sl->seqcount);
  	spin_unlock_bh(&sl->lock);
  }
  
  static inline void write_seqlock_irq(seqlock_t *sl)
  {
  	spin_lock_irq(&sl->lock);
++<<<<<<< HEAD
 +	write_seqcount_begin(&sl->seqcount);
++=======
+ 	write_seqcount_t_begin(&sl->seqcount);
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  }
  
 -/**
 - * write_sequnlock_irq() - end a non-interruptible seqlock_t write section
 - * @sl: Pointer to seqlock_t
 - *
 - * write_sequnlock_irq closes the serialized and non-interruptible
 - * seqlock_t write side section opened with write_seqlock_irq().
 - */
  static inline void write_sequnlock_irq(seqlock_t *sl)
  {
- 	write_seqcount_end(&sl->seqcount);
+ 	write_seqcount_t_end(&sl->seqcount);
  	spin_unlock_irq(&sl->lock);
  }
  
@@@ -532,7 -992,7 +1079,11 @@@ static inline unsigned long __write_seq
  	unsigned long flags;
  
  	spin_lock_irqsave(&sl->lock, flags);
++<<<<<<< HEAD
 +	write_seqcount_begin(&sl->seqcount);
++=======
+ 	write_seqcount_t_begin(&sl->seqcount);
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
  	return flags;
  }
  
@@@ -546,10 -1025,21 +1097,28 @@@ write_sequnlock_irqrestore(seqlock_t *s
  	spin_unlock_irqrestore(&sl->lock, flags);
  }
  
++<<<<<<< HEAD
 +/*
 + * A locking reader exclusively locks out other writers and locking readers,
 + * but doesn't update the sequence number. Acts like a normal spin_lock/unlock.
 + * Don't need preempt_disable() because that is in the spin_lock already.
++=======
+ /**
+  * read_seqlock_excl() - begin a seqlock_t locking reader section
+  * @sl:	Pointer to seqlock_t
+  *
+  * read_seqlock_excl opens a seqlock_t locking reader critical section.  A
+  * locking reader exclusively locks out *both* other writers *and* other
+  * locking readers, but it does not update the embedded sequence number.
+  *
+  * Locking readers act like a normal spin_lock()/spin_unlock().
+  *
+  * Context: if the seqlock_t write section, *or other read sections*, can
+  * be invoked from hardirq or softirq contexts, use the _irqsave or _bh
+  * variant of this function instead.
+  *
+  * The opened read section must be closed with read_sequnlock_excl().
++>>>>>>> 55f3560df975 (seqlock: Extend seqcount API with associated locks)
   */
  static inline void read_seqlock_excl(seqlock_t *sl)
  {
* Unmerged path Documentation/locking/seqlock.rst
* Unmerged path Documentation/locking/seqlock.rst
* Unmerged path include/linux/seqlock.h
