mm/mmu_notifier: use structure for invalidate_range_start/end calls v2

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Jérôme Glisse <jglisse@redhat.com>
commit ac46d4f3c43241ffa23d5bf36153a0830c0e02cc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/ac46d4f3.failed

To avoid having to change many call sites everytime we want to add a
parameter use a structure to group all parameters for the mmu_notifier
invalidate_range_start/end cakks.  No functional changes with this patch.

[akpm@linux-foundation.org: coding style fixes]
Link: http://lkml.kernel.org/r/20181205053628.3210-3-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Acked-by: Christian König <christian.koenig@amd.com>
	Acked-by: Jan Kara <jack@suse.cz>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Ross Zwisler <zwisler@kernel.org>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Radim Krcmar <rkrcmar@redhat.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Felix Kuehling <felix.kuehling@amd.com>
	Cc: Ralph Campbell <rcampbell@nvidia.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
From: Jérôme Glisse <jglisse@redhat.com>
Subject: mm/mmu_notifier: use structure for invalidate_range_start/end calls v3

fix build warning in migrate.c when CONFIG_MMU_NOTIFIER=n

Link: http://lkml.kernel.org/r/20181213171330.8489-3-jglisse@redhat.com
	Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ac46d4f3c43241ffa23d5bf36153a0830c0e02cc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	include/linux/mmu_notifier.h
#	mm/hugetlb.c
#	mm/madvise.c
#	mm/migrate.c
#	mm/mmu_notifier.c
#	mm/oom_kill.c
diff --cc include/linux/mm.h
index c0a5ccbd3b5a,ea1f12d15365..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -1345,6 -1408,54 +1345,57 @@@ void zap_page_range(struct vm_area_stru
  void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
  		unsigned long start, unsigned long end);
  
++<<<<<<< HEAD
++=======
+ /**
+  * mm_walk - callbacks for walk_page_range
+  * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry
+  *	       this handler should only handle pud_trans_huge() puds.
+  *	       the pmd_entry or pte_entry callbacks will be used for
+  *	       regular PUDs.
+  * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry
+  *	       this handler is required to be able to handle
+  *	       pmd_trans_huge() pmds.  They may simply choose to
+  *	       split_huge_page() instead of handling it explicitly.
+  * @pte_entry: if set, called for each non-empty PTE (4th-level) entry
+  * @pte_hole: if set, called for each hole at all levels
+  * @hugetlb_entry: if set, called for each hugetlb entry
+  * @test_walk: caller specific callback function to determine whether
+  *             we walk over the current vma or not. Returning 0
+  *             value means "do page table walk over the current vma,"
+  *             and a negative one means "abort current page table walk
+  *             right now." 1 means "skip the current vma."
+  * @mm:        mm_struct representing the target process of page table walk
+  * @vma:       vma currently walked (NULL if walking outside vmas)
+  * @private:   private data for callbacks' usage
+  *
+  * (see the comment on walk_page_range() for more details)
+  */
+ struct mm_walk {
+ 	int (*pud_entry)(pud_t *pud, unsigned long addr,
+ 			 unsigned long next, struct mm_walk *walk);
+ 	int (*pmd_entry)(pmd_t *pmd, unsigned long addr,
+ 			 unsigned long next, struct mm_walk *walk);
+ 	int (*pte_entry)(pte_t *pte, unsigned long addr,
+ 			 unsigned long next, struct mm_walk *walk);
+ 	int (*pte_hole)(unsigned long addr, unsigned long next,
+ 			struct mm_walk *walk);
+ 	int (*hugetlb_entry)(pte_t *pte, unsigned long hmask,
+ 			     unsigned long addr, unsigned long next,
+ 			     struct mm_walk *walk);
+ 	int (*test_walk)(unsigned long addr, unsigned long next,
+ 			struct mm_walk *walk);
+ 	struct mm_struct *mm;
+ 	struct vm_area_struct *vma;
+ 	void *private;
+ };
+ 
+ struct mmu_notifier_range;
+ 
+ int walk_page_range(unsigned long addr, unsigned long end,
+ 		struct mm_walk *walk);
+ int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk);
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
  		unsigned long end, unsigned long floor, unsigned long ceiling);
  int copy_page_range(struct mm_struct *dst, struct mm_struct *src,
diff --cc include/linux/mmu_notifier.h
index 9dde8bfe9ddf,4050ec1c3b45..000000000000
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@@ -417,10 -220,8 +417,15 @@@ extern int __mmu_notifier_test_young(st
  				     unsigned long address);
  extern void __mmu_notifier_change_pte(struct mm_struct *mm,
  				      unsigned long address, pte_t pte);
++<<<<<<< HEAD
 +extern void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
 +				  unsigned long start, unsigned long end);
 +extern void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
 +				  unsigned long start, unsigned long end,
++=======
+ extern int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *r);
+ extern void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *r,
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  				  bool only_end);
  extern void __mmu_notifier_invalidate_range(struct mm_struct *mm,
  				  unsigned long start, unsigned long end);
@@@ -465,27 -265,37 +470,44 @@@ static inline void mmu_notifier_change_
  		__mmu_notifier_change_pte(mm, address, pte);
  }
  
- static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
- 				  unsigned long start, unsigned long end)
+ static inline void
+ mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
  {
++<<<<<<< HEAD
 +	lock_map_acquire(&__mmu_notifier_invalidate_range_start_map);
 +	if (mm_has_notifiers(mm))
 +		__mmu_notifier_invalidate_range_start(mm, start, end);
 +	lock_map_release(&__mmu_notifier_invalidate_range_start_map);
++=======
+ 	if (mm_has_notifiers(range->mm)) {
+ 		range->blockable = true;
+ 		__mmu_notifier_invalidate_range_start(range);
+ 	}
  }
  
- static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
- 				  unsigned long start, unsigned long end)
+ static inline int
+ mmu_notifier_invalidate_range_start_nonblock(struct mmu_notifier_range *range)
  {
- 	if (mm_has_notifiers(mm))
- 		__mmu_notifier_invalidate_range_end(mm, start, end, false);
+ 	if (mm_has_notifiers(range->mm)) {
+ 		range->blockable = false;
+ 		return __mmu_notifier_invalidate_range_start(range);
+ 	}
+ 	return 0;
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  }
  
- static inline void mmu_notifier_invalidate_range_only_end(struct mm_struct *mm,
- 				  unsigned long start, unsigned long end)
+ static inline void
+ mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)
  {
- 	if (mm_has_notifiers(mm))
- 		__mmu_notifier_invalidate_range_end(mm, start, end, true);
+ 	if (mm_has_notifiers(range->mm))
+ 		__mmu_notifier_invalidate_range_end(range, false);
+ }
+ 
+ static inline void
+ mmu_notifier_invalidate_range_only_end(struct mmu_notifier_range *range)
+ {
+ 	if (mm_has_notifiers(range->mm))
+ 		__mmu_notifier_invalidate_range_end(range, true);
  }
  
  static inline void mmu_notifier_invalidate_range(struct mm_struct *mm,
@@@ -650,8 -488,14 +700,19 @@@ mmu_notifier_invalidate_range_start(str
  {
  }
  
++<<<<<<< HEAD
 +static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
 +				  unsigned long start, unsigned long end)
++=======
+ static inline int
+ mmu_notifier_invalidate_range_start_nonblock(struct mmu_notifier_range *range)
+ {
+ 	return 0;
+ }
+ 
+ static inline
+ void mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  {
  }
  
diff --cc mm/hugetlb.c
index f57bae9f1103,12000ba5c868..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -3795,8 -3546,8 +3795,13 @@@ static vm_fault_t hugetlb_cow(struct mm
  	struct page *old_page, *new_page;
  	int outside_reserve = 0;
  	vm_fault_t ret = 0;
++<<<<<<< HEAD
 +	unsigned long mmun_start;	/* For mmu_notifiers */
 +	unsigned long mmun_end;		/* For mmu_notifiers */
++=======
+ 	unsigned long haddr = address & huge_page_mask(h);
+ 	struct mmu_notifier_range range;
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  
  	pte = huge_ptep_get(ptep);
  	old_page = pte_page(pte);
@@@ -3874,10 -3624,10 +3879,15 @@@ retry_avoidcopy
  	copy_user_huge_page(new_page, old_page, address, vma,
  			    pages_per_huge_page(h));
  	__SetPageUptodate(new_page);
 -	set_page_huge_active(new_page);
  
++<<<<<<< HEAD
 +	mmun_start = address & huge_page_mask(h);
 +	mmun_end = mmun_start + huge_page_size(h);
 +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
++=======
+ 	mmu_notifier_range_init(&range, mm, haddr, haddr + huge_page_size(h));
+ 	mmu_notifier_invalidate_range_start(&range);
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  
  	/*
  	 * Retake the page table lock to check for racing updates
@@@ -3890,20 -3639,19 +3900,26 @@@
  		ClearPagePrivate(new_page);
  
  		/* Break COW */
++<<<<<<< HEAD
 +		huge_ptep_clear_flush(vma, address, ptep);
 +		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);
 +		set_huge_pte_at(mm, address, ptep,
++=======
+ 		huge_ptep_clear_flush(vma, haddr, ptep);
+ 		mmu_notifier_invalidate_range(mm, range.start, range.end);
+ 		set_huge_pte_at(mm, haddr, ptep,
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  				make_huge_pte(vma, new_page, 1));
  		page_remove_rmap(old_page, true);
 -		hugepage_add_new_anon_rmap(new_page, vma, haddr);
 +		hugepage_add_new_anon_rmap(new_page, vma, address);
 +		set_page_huge_active(new_page);
  		/* Make the old page be freed below */
  		new_page = old_page;
  	}
  	spin_unlock(ptl);
- 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
+ 	mmu_notifier_invalidate_range_end(&range);
  out_release_all:
 -	restore_reserve_on_error(h, vma, haddr, new_page);
 +	restore_reserve_on_error(h, vma, address, new_page);
  	put_page(new_page);
  out_release_old:
  	put_page(old_page);
diff --cc mm/madvise.c
index 861219a8a609,21a7881a2db4..000000000000
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@@ -448,24 -466,22 +448,32 @@@ static int madvise_free_single_vma(stru
  	if (!vma_is_anonymous(vma))
  		return -EINVAL;
  
- 	start = max(vma->vm_start, start_addr);
- 	if (start >= vma->vm_end)
+ 	range.start = max(vma->vm_start, start_addr);
+ 	if (range.start >= vma->vm_end)
  		return -EINVAL;
- 	end = min(vma->vm_end, end_addr);
- 	if (end <= vma->vm_start)
+ 	range.end = min(vma->vm_end, end_addr);
+ 	if (range.end <= vma->vm_start)
  		return -EINVAL;
+ 	mmu_notifier_range_init(&range, mm, range.start, range.end);
  
  	lru_add_drain();
- 	tlb_gather_mmu(&tlb, mm, start, end);
+ 	tlb_gather_mmu(&tlb, mm, range.start, range.end);
  	update_hiwater_rss(mm);
  
++<<<<<<< HEAD
 +	mmu_notifier_invalidate_range_start(mm, start, end);
 +	tlb_start_vma(&tlb, vma);
 +	walk_page_range(vma->vm_mm, start, end,
 +			&madvise_free_walk_ops, &tlb);
 +	tlb_end_vma(&tlb, vma);
 +	mmu_notifier_invalidate_range_end(mm, start, end);
 +	tlb_finish_mmu(&tlb, start, end);
++=======
+ 	mmu_notifier_invalidate_range_start(&range);
+ 	madvise_free_page_range(&tlb, vma, range.start, range.end);
+ 	mmu_notifier_invalidate_range_end(&range);
+ 	tlb_finish_mmu(&tlb, range.start, range.end);
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  
  	return 0;
  }
diff --cc mm/migrate.c
index b6aa7a000a4e,462163f5f278..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -2338,16 -2299,24 +2338,37 @@@ static const struct mm_walk_ops migrate
   */
  static void migrate_vma_collect(struct migrate_vma *migrate)
  {
++<<<<<<< HEAD
 +	mmu_notifier_invalidate_range_start(migrate->vma->vm_mm,
 +					    migrate->start,
 +					    migrate->end);
 +
 +	walk_page_range(migrate->vma->vm_mm, migrate->start, migrate->end,
 +			&migrate_vma_walk_ops, migrate);
 +
 +	mmu_notifier_invalidate_range_end(migrate->vma->vm_mm,
 +					  migrate->start,
 +					  migrate->end);
++=======
+ 	struct mmu_notifier_range range;
+ 	struct mm_walk mm_walk;
+ 
+ 	mm_walk.pmd_entry = migrate_vma_collect_pmd;
+ 	mm_walk.pte_entry = NULL;
+ 	mm_walk.pte_hole = migrate_vma_collect_hole;
+ 	mm_walk.hugetlb_entry = NULL;
+ 	mm_walk.test_walk = NULL;
+ 	mm_walk.vma = migrate->vma;
+ 	mm_walk.mm = migrate->vma->vm_mm;
+ 	mm_walk.private = migrate;
+ 
+ 	mmu_notifier_range_init(&range, mm_walk.mm, migrate->start,
+ 				migrate->end);
+ 	mmu_notifier_invalidate_range_start(&range);
+ 	walk_page_range(migrate->start, migrate->end, &mm_walk);
+ 	mmu_notifier_invalidate_range_end(&range);
+ 
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  	migrate->end = migrate->start + (migrate->npages << PAGE_SHIFT);
  }
  
@@@ -2883,12 -2762,10 +2904,11 @@@ void migrate_vma_pages(struct migrate_v
  	 * did already call it.
  	 */
  	if (notified)
- 		mmu_notifier_invalidate_range_only_end(mm, mmu_start,
- 						       migrate->end);
+ 		mmu_notifier_invalidate_range_only_end(&range);
  }
 +EXPORT_SYMBOL(migrate_vma_pages);
  
 -/*
 +/**
   * migrate_vma_finalize() - restore CPU page table entry
   * @migrate: migrate struct containing all migration information
   *
diff --cc mm/mmu_notifier.c
index 00c16156e424,9c884abc7850..000000000000
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@@ -432,69 -167,38 +432,94 @@@ void __mmu_notifier_change_pte(struct m
  	srcu_read_unlock(&srcu, id);
  }
  
++<<<<<<< HEAD
 +static void mn_itree_invalidate(struct mmu_notifier_mm *mmn_mm,
 +				const struct mmu_notifier_range *range)
 +{
 +	struct mmu_interval_notifier *mni;
 +	unsigned long cur_seq;
 +
 +	for (mni = mn_itree_inv_start_range(mmn_mm, range, &cur_seq); mni;
 +	     mni = mn_itree_inv_next(mni, range)) {
 +
 +		mni->ops->invalidate(mni, range, cur_seq);
 +	}
 +}
 +
 +static void mn_hlist_invalidate_range_start(struct mmu_notifier_mm *mmn_mm,
 +					    struct mmu_notifier_range *range)
++=======
+ int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  {
  	struct mmu_notifier *mn;
 -	int ret = 0;
  	int id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mmn_mm->list, hlist) {
 +		if (mn->ops->invalidate_range_start) {
 +			mn->ops->invalidate_range_start(mn, range->mm, range->start, range->end);
++=======
+ 	hlist_for_each_entry_rcu(mn, &range->mm->mmu_notifier_mm->list, hlist) {
+ 		if (mn->ops->invalidate_range_start) {
+ 			int _ret = mn->ops->invalidate_range_start(mn, range);
+ 			if (_ret) {
+ 				pr_info("%pS callback failed with %d in %sblockable context.\n",
+ 					mn->ops->invalidate_range_start, _ret,
+ 					!range->blockable ? "non-" : "");
+ 				ret = _ret;
+ 			}
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  		}
  	}
  	srcu_read_unlock(&srcu, id);
 +}
  
 -	return ret;
++<<<<<<< HEAD
 +static inline void mmu_notifier_range_init(struct mmu_notifier_range *range,
 +					   struct mm_struct *mm,
 +					   unsigned long start,
 +					   unsigned long end)
 +{
 +	memset(range, 0, sizeof(*range));
 +	range->mm = mm;
 +	range->start = start;
 +	range->end = end;
 +}
 +
 +void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
 +					   unsigned long start, unsigned long end)
 +{
 +	struct mmu_notifier_mm *mmn_mm = mm->mmu_notifier_mm;
 +	struct mmu_notifier_range range;
 +
 +	mmu_notifier_range_init(&range, mm, start, end);
 +
 +	if (mmn_mm->has_itree) {
 +		mn_itree_invalidate(mmn_mm, &range);
 +	}
 +	if (!hlist_empty(&mmn_mm->list))
 +		mn_hlist_invalidate_range_start(mmn_mm, &range);
  }
 -EXPORT_SYMBOL_GPL(__mmu_notifier_invalidate_range_start);
  
 +static void mn_hlist_invalidate_end(struct mmu_notifier_mm *mmn_mm,
 +				    struct mmu_notifier_range *range,
 +				    bool only_end)
++=======
+ void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range,
+ 					 bool only_end)
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  {
  	struct mmu_notifier *mn;
  	int id;
  
  	id = srcu_read_lock(&srcu);
++<<<<<<< HEAD
 +	hlist_for_each_entry_rcu(mn, &mmn_mm->list, hlist) {
++=======
+ 	hlist_for_each_entry_rcu(mn, &range->mm->mmu_notifier_mm->list, hlist) {
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  		/*
  		 * Call invalidate_range here too to avoid the need for the
  		 * subsystem of having to register an invalidate_range_end
@@@ -512,9 -216,8 +537,14 @@@
  			mn->ops->invalidate_range(mn, range->mm,
  						  range->start,
  						  range->end);
++<<<<<<< HEAD
 +		if (mn->ops->invalidate_range_end) {
 +			mn->ops->invalidate_range_end(mn, range->mm, range->start, range->end);
 +		}
++=======
+ 		if (mn->ops->invalidate_range_end)
+ 			mn->ops->invalidate_range_end(mn, range);
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  	}
  	srcu_read_unlock(&srcu, id);
  }
diff --cc mm/oom_kill.c
index e09692e9bf25,f0e8cd9edb1a..000000000000
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@@ -515,19 -528,32 +515,32 @@@ void __oom_reap_task_mm(struct mm_struc
  		 * count elevated without a good reason.
  		 */
  		if (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {
- 			const unsigned long start = vma->vm_start;
- 			const unsigned long end = vma->vm_end;
+ 			struct mmu_notifier_range range;
  			struct mmu_gather tlb;
  
++<<<<<<< HEAD
 +			tlb_gather_mmu(&tlb, mm, start, end);
 +			mmu_notifier_invalidate_range_start(mm, start, end);
 +			unmap_page_range(&tlb, vma, start, end, NULL);
 +			mmu_notifier_invalidate_range_end(mm, start, end);
 +			tlb_finish_mmu(&tlb, start, end);
++=======
+ 			mmu_notifier_range_init(&range, mm, vma->vm_start,
+ 						vma->vm_end);
+ 			tlb_gather_mmu(&tlb, mm, range.start, range.end);
+ 			if (mmu_notifier_invalidate_range_start_nonblock(&range)) {
+ 				tlb_finish_mmu(&tlb, range.start, range.end);
+ 				ret = false;
+ 				continue;
+ 			}
+ 			unmap_page_range(&tlb, vma, range.start, range.end, NULL);
+ 			mmu_notifier_invalidate_range_end(&range);
+ 			tlb_finish_mmu(&tlb, range.start, range.end);
++>>>>>>> ac46d4f3c432 (mm/mmu_notifier: use structure for invalidate_range_start/end calls v2)
  		}
  	}
 -
 -	return ret;
  }
  
 -/*
 - * Reaps the address space of the give task.
 - *
 - * Returns true on success and false if none or part of the address space
 - * has been reclaimed and the caller should retry later.
 - */
  static bool oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)
  {
  	bool ret = true;
diff --git a/fs/dax.c b/fs/dax.c
index 0beb53b5a7e9..ed3b4ea199af 100644
--- a/fs/dax.c
+++ b/fs/dax.c
@@ -791,7 +791,8 @@ static void dax_entry_mkclean(struct address_space *mapping, pgoff_t index,
 
 	i_mmap_lock_read(mapping);
 	vma_interval_tree_foreach(vma, &mapping->i_mmap, index, index) {
-		unsigned long address, start, end;
+		struct mmu_notifier_range range;
+		unsigned long address;
 
 		cond_resched();
 
@@ -805,7 +806,8 @@ static void dax_entry_mkclean(struct address_space *mapping, pgoff_t index,
 		 * call mmu_notifier_invalidate_range_start() on our behalf
 		 * before taking any lock.
 		 */
-		if (follow_pte_pmd(vma->vm_mm, address, &start, &end, &ptep, &pmdp, &ptl))
+		if (follow_pte_pmd(vma->vm_mm, address, &range,
+				   &ptep, &pmdp, &ptl))
 			continue;
 
 		/*
@@ -847,7 +849,7 @@ static void dax_entry_mkclean(struct address_space *mapping, pgoff_t index,
 			pte_unmap_unlock(ptep, ptl);
 		}
 
-		mmu_notifier_invalidate_range_end(vma->vm_mm, start, end);
+		mmu_notifier_invalidate_range_end(&range);
 	}
 	i_mmap_unlock_read(mapping);
 }
diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index ee0c6b420a36..d1dc85c7b4f7 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -1119,6 +1119,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 		return -ESRCH;
 	mm = get_task_mm(task);
 	if (mm) {
+		struct mmu_notifier_range range;
 		struct clear_refs_private cp = {
 			.type = type,
 		};
@@ -1174,12 +1175,14 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 				downgrade_write(&mm->mmap_sem);
 				break;
 			}
-			mmu_notifier_invalidate_range_start(mm, 0, -1);
+
+			mmu_notifier_range_init(&range, mm, 0, -1UL);
+			mmu_notifier_invalidate_range_start(&range);
 		}
 		walk_page_range(mm, 0, mm->highest_vm_end, &clear_refs_walk_ops,
 				&cp);
 		if (type == CLEAR_REFS_SOFT_DIRTY)
-			mmu_notifier_invalidate_range_end(mm, 0, -1);
+			mmu_notifier_invalidate_range_end(&range);
 		tlb_finish_mmu(&tlb, 0, -1);
 		up_read(&mm->mmap_sem);
 out_mm:
* Unmerged path include/linux/mm.h
* Unmerged path include/linux/mmu_notifier.h
diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c
index fe7280616fbe..3184ba9e32fb 100644
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@ -158,11 +158,11 @@ static int __replace_page(struct vm_area_struct *vma, unsigned long addr,
 		.address = addr,
 	};
 	int err;
-	/* For mmu_notifiers */
-	const unsigned long mmun_start = addr;
-	const unsigned long mmun_end   = addr + PAGE_SIZE;
+	struct mmu_notifier_range range;
 	struct mem_cgroup *memcg;
 
+	mmu_notifier_range_init(&range, mm, addr, addr + PAGE_SIZE);
+
 	VM_BUG_ON_PAGE(PageTransHuge(old_page), old_page);
 
 	err = mem_cgroup_try_charge(new_page, vma->vm_mm, GFP_KERNEL, &memcg,
@@ -173,7 +173,7 @@ static int __replace_page(struct vm_area_struct *vma, unsigned long addr,
 	/* For try_to_free_swap() and munlock_vma_page() below */
 	lock_page(old_page);
 
-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_start(&range);
 	err = -EAGAIN;
 	if (!page_vma_mapped_walk(&pvmw)) {
 		mem_cgroup_cancel_charge(new_page, memcg, false);
@@ -207,7 +207,7 @@ static int __replace_page(struct vm_area_struct *vma, unsigned long addr,
 
 	err = 0;
  unlock:
-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_end(&range);
 	unlock_page(old_page);
 	return err;
 }
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 60b47b7d7614..651cc6c620c0 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1170,8 +1170,7 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 	int i;
 	vm_fault_t ret = 0;
 	struct page **pages;
-	unsigned long mmun_start;	/* For mmu_notifiers */
-	unsigned long mmun_end;		/* For mmu_notifiers */
+	struct mmu_notifier_range range;
 
 	pages = kmalloc_array(HPAGE_PMD_NR, sizeof(struct page *),
 			      GFP_KERNEL);
@@ -1209,9 +1208,9 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 		cond_resched();
 	}
 
-	mmun_start = haddr;
-	mmun_end   = haddr + HPAGE_PMD_SIZE;
-	mmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);
+	mmu_notifier_range_init(&range, vma->vm_mm, haddr,
+				haddr + HPAGE_PMD_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
 
 	vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
 	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
@@ -1256,8 +1255,7 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 	 * No need to double call mmu_notifier->invalidate_range() callback as
 	 * the above pmdp_huge_clear_flush_notify() did already call it.
 	 */
-	mmu_notifier_invalidate_range_only_end(vma->vm_mm, mmun_start,
-						mmun_end);
+	mmu_notifier_invalidate_range_only_end(&range);
 
 	ret |= VM_FAULT_WRITE;
 	put_page(page);
@@ -1267,7 +1265,7 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 
 out_free_pages:
 	spin_unlock(vmf->ptl);
-	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_end(&range);
 	for (i = 0; i < HPAGE_PMD_NR; i++) {
 		memcg = (void *)page_private(pages[i]);
 		set_page_private(pages[i], 0);
@@ -1284,8 +1282,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 	struct page *page = NULL, *new_page;
 	struct mem_cgroup *memcg;
 	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
-	unsigned long mmun_start;	/* For mmu_notifiers */
-	unsigned long mmun_end;		/* For mmu_notifiers */
+	struct mmu_notifier_range range;
 	gfp_t huge_gfp;			/* for allocation and charge */
 	vm_fault_t ret = 0;
 
@@ -1375,9 +1372,9 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 				    vma, HPAGE_PMD_NR);
 	__SetPageUptodate(new_page);
 
-	mmun_start = haddr;
-	mmun_end   = haddr + HPAGE_PMD_SIZE;
-	mmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);
+	mmu_notifier_range_init(&range, vma->vm_mm, haddr,
+				haddr + HPAGE_PMD_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
 
 	spin_lock(vmf->ptl);
 	if (page)
@@ -1412,8 +1409,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 	 * No need to double call mmu_notifier->invalidate_range() callback as
 	 * the above pmdp_huge_clear_flush_notify() did already call it.
 	 */
-	mmu_notifier_invalidate_range_only_end(vma->vm_mm, mmun_start,
-					       mmun_end);
+	mmu_notifier_invalidate_range_only_end(&range);
 out:
 	return ret;
 out_unlock:
@@ -2052,14 +2048,15 @@ void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,
 		unsigned long address)
 {
 	spinlock_t *ptl;
-	struct mm_struct *mm = vma->vm_mm;
-	unsigned long haddr = address & HPAGE_PUD_MASK;
+	struct mmu_notifier_range range;
 
-	mmu_notifier_invalidate_range_start(mm, haddr, haddr + HPAGE_PUD_SIZE);
-	ptl = pud_lock(mm, pud);
+	mmu_notifier_range_init(&range, vma->vm_mm, address & HPAGE_PUD_MASK,
+				(address & HPAGE_PUD_MASK) + HPAGE_PUD_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
+	ptl = pud_lock(vma->vm_mm, pud);
 	if (unlikely(!pud_trans_huge(*pud) && !pud_devmap(*pud)))
 		goto out;
-	__split_huge_pud_locked(vma, pud, haddr);
+	__split_huge_pud_locked(vma, pud, range.start);
 
 out:
 	spin_unlock(ptl);
@@ -2067,8 +2064,7 @@ void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,
 	 * No need to double call mmu_notifier->invalidate_range() callback as
 	 * the above pudp_huge_clear_flush_notify() did already call it.
 	 */
-	mmu_notifier_invalidate_range_only_end(mm, haddr, haddr +
-					       HPAGE_PUD_SIZE);
+	mmu_notifier_invalidate_range_only_end(&range);
 }
 #endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
 
@@ -2270,11 +2266,12 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 		unsigned long address, bool freeze, struct page *page)
 {
 	spinlock_t *ptl;
-	struct mm_struct *mm = vma->vm_mm;
-	unsigned long haddr = address & HPAGE_PMD_MASK;
+	struct mmu_notifier_range range;
 
-	mmu_notifier_invalidate_range_start(mm, haddr, haddr + HPAGE_PMD_SIZE);
-	ptl = pmd_lock(mm, pmd);
+	mmu_notifier_range_init(&range, vma->vm_mm, address & HPAGE_PMD_MASK,
+				(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
+	ptl = pmd_lock(vma->vm_mm, pmd);
 
 	/*
 	 * If caller asks to setup a migration entries, we need a page to check
@@ -2290,7 +2287,7 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 			clear_page_mlock(page);
 	} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))
 		goto out;
-	__split_huge_pmd_locked(vma, pmd, haddr, freeze);
+	__split_huge_pmd_locked(vma, pmd, range.start, freeze);
 out:
 	spin_unlock(ptl);
 	/*
@@ -2306,8 +2303,7 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 	 *     any further changes to individual pte will notify. So no need
 	 *     to call mmu_notifier->invalidate_range()
 	 */
-	mmu_notifier_invalidate_range_only_end(mm, haddr, haddr +
-					       HPAGE_PMD_SIZE);
+	mmu_notifier_invalidate_range_only_end(&range);
 }
 
 void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,
* Unmerged path mm/hugetlb.c
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index d3c6d0ee7dd7..5ecd76521060 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -944,8 +944,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	int isolated = 0, result = 0;
 	struct mem_cgroup *memcg;
 	struct vm_area_struct *vma;
-	unsigned long mmun_start;	/* For mmu_notifiers */
-	unsigned long mmun_end;		/* For mmu_notifiers */
+	struct mmu_notifier_range range;
 	gfp_t gfp;
 
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
@@ -1020,9 +1019,8 @@ static void collapse_huge_page(struct mm_struct *mm,
 	pte = pte_offset_map(pmd, address);
 	pte_ptl = pte_lockptr(mm, pmd);
 
-	mmun_start = address;
-	mmun_end   = address + HPAGE_PMD_SIZE;
-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
+	mmu_notifier_range_init(&range, mm, address, address + HPAGE_PMD_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
 	pmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */
 	/*
 	 * After this gup_fast can't run anymore. This also removes
@@ -1032,7 +1030,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 */
 	_pmd = pmdp_collapse_flush(vma, address, pmd);
 	spin_unlock(pmd_ptl);
-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_end(&range);
 
 	spin_lock(pte_ptl);
 	isolated = __collapse_huge_page_isolate(vma, address, pte);
diff --git a/mm/ksm.c b/mm/ksm.c
index 034e032e72e5..509ac9da40ba 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -1042,8 +1042,7 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,
 	};
 	int swapped;
 	int err = -EFAULT;
-	unsigned long mmun_start;	/* For mmu_notifiers */
-	unsigned long mmun_end;		/* For mmu_notifiers */
+	struct mmu_notifier_range range;
 
 	pvmw.address = page_address_in_vma(page, vma);
 	if (pvmw.address == -EFAULT)
@@ -1051,9 +1050,9 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,
 
 	BUG_ON(PageTransCompound(page));
 
-	mmun_start = pvmw.address;
-	mmun_end   = pvmw.address + PAGE_SIZE;
-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
+	mmu_notifier_range_init(&range, mm, pvmw.address,
+				pvmw.address + PAGE_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
 
 	if (!page_vma_mapped_walk(&pvmw))
 		goto out_mn;
@@ -1105,7 +1104,7 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,
 out_unlock:
 	page_vma_mapped_walk_done(&pvmw);
 out_mn:
-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_end(&range);
 out:
 	return err;
 }
@@ -1129,8 +1128,7 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,
 	spinlock_t *ptl;
 	unsigned long addr;
 	int err = -EFAULT;
-	unsigned long mmun_start;	/* For mmu_notifiers */
-	unsigned long mmun_end;		/* For mmu_notifiers */
+	struct mmu_notifier_range range;
 
 	addr = page_address_in_vma(page, vma);
 	if (addr == -EFAULT)
@@ -1140,9 +1138,8 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,
 	if (!pmd)
 		goto out;
 
-	mmun_start = addr;
-	mmun_end   = addr + PAGE_SIZE;
-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
+	mmu_notifier_range_init(&range, mm, addr, addr + PAGE_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
 
 	ptep = pte_offset_map_lock(mm, pmd, addr, &ptl);
 	if (!pte_same(*ptep, orig_pte)) {
@@ -1188,7 +1185,7 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,
 	pte_unmap_unlock(ptep, ptl);
 	err = 0;
 out_mn:
-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_end(&range);
 out:
 	return err;
 }
* Unmerged path mm/madvise.c
diff --git a/mm/memory.c b/mm/memory.c
index 25fae8673e61..26257d3a288a 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1186,8 +1186,7 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	unsigned long next;
 	unsigned long addr = vma->vm_start;
 	unsigned long end = vma->vm_end;
-	unsigned long mmun_start;	/* For mmu_notifiers */
-	unsigned long mmun_end;		/* For mmu_notifiers */
+	struct mmu_notifier_range range;
 	bool is_cow;
 	int ret;
 
@@ -1221,11 +1220,11 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	 * is_cow_mapping() returns true.
 	 */
 	is_cow = is_cow_mapping(vma->vm_flags);
-	mmun_start = addr;
-	mmun_end   = end;
-	if (is_cow)
-		mmu_notifier_invalidate_range_start(src_mm, mmun_start,
-						    mmun_end);
+
+	if (is_cow) {
+		mmu_notifier_range_init(&range, src_mm, addr, end);
+		mmu_notifier_invalidate_range_start(&range);
+	}
 
 	ret = 0;
 	dst_pgd = pgd_offset(dst_mm, addr);
@@ -1242,7 +1241,7 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
 
 	if (is_cow)
-		mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);
+		mmu_notifier_invalidate_range_end(&range);
 	return ret;
 }
 
@@ -1545,12 +1544,13 @@ void unmap_vmas(struct mmu_gather *tlb,
 		struct vm_area_struct *vma, unsigned long start_addr,
 		unsigned long end_addr)
 {
-	struct mm_struct *mm = vma->vm_mm;
+	struct mmu_notifier_range range;
 
-	mmu_notifier_invalidate_range_start(mm, start_addr, end_addr);
+	mmu_notifier_range_init(&range, vma->vm_mm, start_addr, end_addr);
+	mmu_notifier_invalidate_range_start(&range);
 	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next)
 		unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);
-	mmu_notifier_invalidate_range_end(mm, start_addr, end_addr);
+	mmu_notifier_invalidate_range_end(&range);
 }
 
 /**
@@ -1564,18 +1564,18 @@ void unmap_vmas(struct mmu_gather *tlb,
 void zap_page_range(struct vm_area_struct *vma, unsigned long start,
 		unsigned long size)
 {
-	struct mm_struct *mm = vma->vm_mm;
+	struct mmu_notifier_range range;
 	struct mmu_gather tlb;
-	unsigned long end = start + size;
 
 	lru_add_drain();
-	tlb_gather_mmu(&tlb, mm, start, end);
-	update_hiwater_rss(mm);
-	mmu_notifier_invalidate_range_start(mm, start, end);
-	for ( ; vma && vma->vm_start < end; vma = vma->vm_next)
-		unmap_single_vma(&tlb, vma, start, end, NULL);
-	mmu_notifier_invalidate_range_end(mm, start, end);
-	tlb_finish_mmu(&tlb, start, end);
+	mmu_notifier_range_init(&range, vma->vm_mm, start, start + size);
+	tlb_gather_mmu(&tlb, vma->vm_mm, start, range.end);
+	update_hiwater_rss(vma->vm_mm);
+	mmu_notifier_invalidate_range_start(&range);
+	for ( ; vma && vma->vm_start < range.end; vma = vma->vm_next)
+		unmap_single_vma(&tlb, vma, start, range.end, NULL);
+	mmu_notifier_invalidate_range_end(&range);
+	tlb_finish_mmu(&tlb, start, range.end);
 }
 
 /**
@@ -1590,17 +1590,17 @@ void zap_page_range(struct vm_area_struct *vma, unsigned long start,
 static void zap_page_range_single(struct vm_area_struct *vma, unsigned long address,
 		unsigned long size, struct zap_details *details)
 {
-	struct mm_struct *mm = vma->vm_mm;
+	struct mmu_notifier_range range;
 	struct mmu_gather tlb;
-	unsigned long end = address + size;
 
 	lru_add_drain();
-	tlb_gather_mmu(&tlb, mm, address, end);
-	update_hiwater_rss(mm);
-	mmu_notifier_invalidate_range_start(mm, address, end);
-	unmap_single_vma(&tlb, vma, address, end, details);
-	mmu_notifier_invalidate_range_end(mm, address, end);
-	tlb_finish_mmu(&tlb, address, end);
+	mmu_notifier_range_init(&range, vma->vm_mm, address, address + size);
+	tlb_gather_mmu(&tlb, vma->vm_mm, address, range.end);
+	update_hiwater_rss(vma->vm_mm);
+	mmu_notifier_invalidate_range_start(&range);
+	unmap_single_vma(&tlb, vma, address, range.end, details);
+	mmu_notifier_invalidate_range_end(&range);
+	tlb_finish_mmu(&tlb, address, range.end);
 }
 
 /**
@@ -2589,9 +2589,8 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 	struct page *new_page = NULL;
 	pte_t entry;
 	int page_copied = 0;
-	const unsigned long mmun_start = vmf->address & PAGE_MASK;
-	const unsigned long mmun_end = mmun_start + PAGE_SIZE;
 	struct mem_cgroup *memcg;
+	struct mmu_notifier_range range;
 
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
@@ -2614,7 +2613,9 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 
 	__SetPageUptodate(new_page);
 
-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
+	mmu_notifier_range_init(&range, mm, vmf->address & PAGE_MASK,
+				(vmf->address & PAGE_MASK) + PAGE_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
 
 	/*
 	 * Re-check the pte - we dropped the lock
@@ -2691,7 +2692,7 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 	 * No need to double call mmu_notifier->invalidate_range() callback as
 	 * the above ptep_clear_flush_notify() did already call it.
 	 */
-	mmu_notifier_invalidate_range_only_end(mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_only_end(&range);
 	if (old_page) {
 		/*
 		 * Don't let another task, with possibly unlocked vma,
@@ -4372,7 +4373,7 @@ int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
 #endif /* __PAGETABLE_PMD_FOLDED */
 
 static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,
-			    unsigned long *start, unsigned long *end,
+			    struct mmu_notifier_range *range,
 			    pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp)
 {
 	pgd_t *pgd;
@@ -4400,10 +4401,10 @@ static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,
 		if (!pmdpp)
 			goto out;
 
-		if (start && end) {
-			*start = address & PMD_MASK;
-			*end = *start + PMD_SIZE;
-			mmu_notifier_invalidate_range_start(mm, *start, *end);
+		if (range) {
+			mmu_notifier_range_init(range, mm, address & PMD_MASK,
+					     (address & PMD_MASK) + PMD_SIZE);
+			mmu_notifier_invalidate_range_start(range);
 		}
 		*ptlp = pmd_lock(mm, pmd);
 		if (pmd_huge(*pmd)) {
@@ -4411,17 +4412,17 @@ static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,
 			return 0;
 		}
 		spin_unlock(*ptlp);
-		if (start && end)
-			mmu_notifier_invalidate_range_end(mm, *start, *end);
+		if (range)
+			mmu_notifier_invalidate_range_end(range);
 	}
 
 	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
 		goto out;
 
-	if (start && end) {
-		*start = address & PAGE_MASK;
-		*end = *start + PAGE_SIZE;
-		mmu_notifier_invalidate_range_start(mm, *start, *end);
+	if (range) {
+		range->start = address & PAGE_MASK;
+		range->end = range->start + PAGE_SIZE;
+		mmu_notifier_invalidate_range_start(range);
 	}
 	ptep = pte_offset_map_lock(mm, pmd, address, ptlp);
 	if (!pte_present(*ptep))
@@ -4430,8 +4431,8 @@ static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,
 	return 0;
 unlock:
 	pte_unmap_unlock(ptep, *ptlp);
-	if (start && end)
-		mmu_notifier_invalidate_range_end(mm, *start, *end);
+	if (range)
+		mmu_notifier_invalidate_range_end(range);
 out:
 	return -EINVAL;
 }
@@ -4443,20 +4444,20 @@ static inline int follow_pte(struct mm_struct *mm, unsigned long address,
 
 	/* (void) is needed to make gcc happy */
 	(void) __cond_lock(*ptlp,
-			   !(res = __follow_pte_pmd(mm, address, NULL, NULL,
+			   !(res = __follow_pte_pmd(mm, address, NULL,
 						    ptepp, NULL, ptlp)));
 	return res;
 }
 
 int follow_pte_pmd(struct mm_struct *mm, unsigned long address,
-			     unsigned long *start, unsigned long *end,
-			     pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp)
+		   struct mmu_notifier_range *range,
+		   pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp)
 {
 	int res;
 
 	/* (void) is needed to make gcc happy */
 	(void) __cond_lock(*ptlp,
-			   !(res = __follow_pte_pmd(mm, address, start, end,
+			   !(res = __follow_pte_pmd(mm, address, range,
 						    ptepp, pmdpp, ptlp)));
 	return res;
 }
* Unmerged path mm/migrate.c
* Unmerged path mm/mmu_notifier.c
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 4ca6272cce89..b46d250031bc 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -192,11 +192,12 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 		pgprot_t newprot, int dirty_accountable, int prot_numa)
 {
 	pmd_t *pmd;
-	struct mm_struct *mm = vma->vm_mm;
 	unsigned long next;
 	unsigned long pages = 0;
 	unsigned long nr_huge_updates = 0;
-	unsigned long mni_start = 0;
+	struct mmu_notifier_range range;
+
+	range.start = 0;
 
 	pmd = pmd_offset(pud, addr);
 	do {
@@ -217,9 +218,9 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 			goto next;
 
 		/* invoke the mmu notifier if the pmd is populated */
-		if (!mni_start) {
-			mni_start = addr;
-			mmu_notifier_invalidate_range_start(mm, mni_start, end);
+		if (!range.start) {
+			mmu_notifier_range_init(&range, vma->vm_mm, addr, end);
+			mmu_notifier_invalidate_range_start(&range);
 		}
 
 		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
@@ -248,8 +249,8 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 		cond_resched();
 	} while (pmd++, addr = next, addr != end);
 
-	if (mni_start)
-		mmu_notifier_invalidate_range_end(mm, mni_start, end);
+	if (range.start)
+		mmu_notifier_invalidate_range_end(&range);
 
 	if (nr_huge_updates)
 		count_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);
diff --git a/mm/mremap.c b/mm/mremap.c
index 33d8bbe24ddd..be64fcb763ab 100644
--- a/mm/mremap.c
+++ b/mm/mremap.c
@@ -197,16 +197,14 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 		bool need_rmap_locks)
 {
 	unsigned long extent, next, old_end;
+	struct mmu_notifier_range range;
 	pmd_t *old_pmd, *new_pmd;
-	unsigned long mmun_start;	/* For mmu_notifiers */
-	unsigned long mmun_end;		/* For mmu_notifiers */
 
 	old_end = old_addr + len;
 	flush_cache_range(vma, old_addr, old_end);
 
-	mmun_start = old_addr;
-	mmun_end   = old_end;
-	mmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);
+	mmu_notifier_range_init(&range, vma->vm_mm, old_addr, old_end);
+	mmu_notifier_invalidate_range_start(&range);
 
 	for (; old_addr < old_end; old_addr += extent, new_addr += extent) {
 		cond_resched();
@@ -247,7 +245,7 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 			  new_pmd, new_addr, need_rmap_locks);
 	}
 
-	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
+	mmu_notifier_invalidate_range_end(&range);
 
 	return len + old_addr - old_end;	/* how much done */
 }
* Unmerged path mm/oom_kill.c
diff --git a/mm/rmap.c b/mm/rmap.c
index 4ca7a0db9645..675d1ba864bc 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -895,15 +895,17 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,
 		.address = address,
 		.flags = PVMW_SYNC,
 	};
-	unsigned long start = address, end;
+	struct mmu_notifier_range range;
 	int *cleaned = arg;
 
 	/*
 	 * We have to assume the worse case ie pmd for invalidation. Note that
 	 * the page can not be free from this function.
 	 */
-	end = min(vma->vm_end, start + (PAGE_SIZE << compound_order(page)));
-	mmu_notifier_invalidate_range_start(vma->vm_mm, start, end);
+	mmu_notifier_range_init(&range, vma->vm_mm, address,
+				min(vma->vm_end, address +
+				    (PAGE_SIZE << compound_order(page))));
+	mmu_notifier_invalidate_range_start(&range);
 
 	while (page_vma_mapped_walk(&pvmw)) {
 		unsigned long cstart;
@@ -955,7 +957,7 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,
 			(*cleaned)++;
 	}
 
-	mmu_notifier_invalidate_range_end(vma->vm_mm, start, end);
+	mmu_notifier_invalidate_range_end(&range);
 
 	return true;
 }
@@ -1351,7 +1353,7 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 	pte_t pteval;
 	struct page *subpage;
 	bool ret = true;
-	unsigned long start = address, end;
+	struct mmu_notifier_range range;
 	enum ttu_flags flags = (enum ttu_flags)arg;
 
 	/* munlock has nothing to gain from examining un-locked vmas */
@@ -1375,15 +1377,18 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 	 * Note that the page can not be free in this function as call of
 	 * try_to_unmap() must hold a reference on the page.
 	 */
-	end = min(vma->vm_end, start + (PAGE_SIZE << compound_order(page)));
+	mmu_notifier_range_init(&range, vma->vm_mm, vma->vm_start,
+				min(vma->vm_end, vma->vm_start +
+				    (PAGE_SIZE << compound_order(page))));
 	if (PageHuge(page)) {
 		/*
 		 * If sharing is possible, start and end will be adjusted
 		 * accordingly.
 		 */
-		adjust_range_if_pmd_sharing_possible(vma, &start, &end);
+		adjust_range_if_pmd_sharing_possible(vma, &range.start,
+						     &range.end);
 	}
-	mmu_notifier_invalidate_range_start(vma->vm_mm, start, end);
+	mmu_notifier_invalidate_range_start(&range);
 
 	while (page_vma_mapped_walk(&pvmw)) {
 #ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
@@ -1434,9 +1439,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 				 * we must flush them all.  start/end were
 				 * already adjusted above to cover this range.
 				 */
-				flush_cache_range(vma, start, end);
-				flush_tlb_range(vma, start, end);
-				mmu_notifier_invalidate_range(mm, start, end);
+				flush_cache_range(vma, range.start, range.end);
+				flush_tlb_range(vma, range.start, range.end);
+				mmu_notifier_invalidate_range(mm, range.start,
+							      range.end);
 
 				/*
 				 * The ref count of the PMD page was dropped
@@ -1664,7 +1670,7 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
 		put_page(page);
 	}
 
-	mmu_notifier_invalidate_range_end(vma->vm_mm, start, end);
+	mmu_notifier_invalidate_range_end(&range);
 
 	return ret;
 }
