iommu/arm-smmu: Abstract GR0 accesses

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit 00320ce6505821b405992c241ebe4c79f178bf8f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/00320ce6.failed

Clean up the remaining accesses to GR0 registers, so that everything is
now neatly abstracted. This folds up the Non-Secure alias quirk as the
first step towards moving it out of the way entirely. Although GR0 does
technically contain some 64-bit registers (sGFAR and the weird SMMUv2
HYPC and MONC stuff), they're not ones we have any need to access.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 00320ce6505821b405992c241ebe4c79f178bf8f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm-smmu.c
diff --cc drivers/iommu/arm-smmu.c
index 9a0dbef6026f,e9fd9117109e..000000000000
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@@ -79,33 -69,6 +79,36 @@@
  /* Maximum number of context banks per SMMU */
  #define ARM_SMMU_MAX_CBS		128
  
++<<<<<<< HEAD
 +/* SMMU global address space */
 +#define ARM_SMMU_GR0(smmu)		((smmu)->base)
 +
 +/*
 + * SMMU global address space with conditional offset to access secure
 + * aliases of non-secure registers (e.g. nsCR0: 0x400, nsGFSR: 0x448,
 + * nsGFSYNR0: 0x450)
 + */
 +#define ARM_SMMU_GR0_NS(smmu)						\
 +	((smmu)->base +							\
 +		((smmu->options & ARM_SMMU_OPT_SECURE_CFG_ACCESS)	\
 +			? 0x400 : 0))
 +
 +/*
 + * Some 64-bit registers only make sense to write atomically, but in such
 + * cases all the data relevant to AArch32 formats lies within the lower word,
 + * therefore this actually makes more sense than it might first appear.
 + */
 +#ifdef CONFIG_64BIT
 +#define smmu_write_atomic_lq		writeq_relaxed
 +#else
 +#define smmu_write_atomic_lq		writel_relaxed
 +#endif
 +
 +/* Translation context bank */
 +#define ARM_SMMU_CB(smmu, n)	((smmu)->cb_base + ((n) << (smmu)->pgshift))
 +
++=======
++>>>>>>> 00320ce65058 (iommu/arm-smmu: Abstract GR0 accesses)
  #define MSI_IOVA_BASE			0x8000000
  #define MSI_IOVA_LENGTH			0x100000
  
@@@ -290,8 -270,26 +314,28 @@@ static void arm_smmu_writel(struct arm_
  	writel_relaxed(val, arm_smmu_page(smmu, page) + offset);
  }
  
++<<<<<<< HEAD
++=======
+ static u64 arm_smmu_readq(struct arm_smmu_device *smmu, int page, int offset)
+ {
+ 	return readq_relaxed(arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ static void arm_smmu_writeq(struct arm_smmu_device *smmu, int page, int offset,
+ 			    u64 val)
+ {
+ 	writeq_relaxed(val, arm_smmu_page(smmu, page) + offset);
+ }
+ 
+ #define ARM_SMMU_GR0		0
++>>>>>>> 00320ce65058 (iommu/arm-smmu: Abstract GR0 accesses)
  #define ARM_SMMU_GR1		1
 -#define ARM_SMMU_CB(s, n)	((s)->numpage + (n))
  
+ #define arm_smmu_gr0_read(s, o)		\
+ 	arm_smmu_readl((s), ARM_SMMU_GR0, (o))
+ #define arm_smmu_gr0_write(s, o, v)	\
+ 	arm_smmu_writel((s), ARM_SMMU_GR0, (o), (v))
+ 
  #define arm_smmu_gr1_read(s, o)		\
  	arm_smmu_readl((s), ARM_SMMU_GR1, (o))
  #define arm_smmu_gr1_write(s, o, v)	\
@@@ -476,8 -484,8 +520,13 @@@ static void arm_smmu_tlb_sync_global(st
  	unsigned long flags;
  
  	spin_lock_irqsave(&smmu->global_sync_lock, flags);
++<<<<<<< HEAD
 +	__arm_smmu_tlb_sync(smmu, base + ARM_SMMU_GR0_sTLBGSYNC,
 +			    base + ARM_SMMU_GR0_sTLBGSTATUS);
++=======
+ 	__arm_smmu_tlb_sync(smmu, ARM_SMMU_GR0, ARM_SMMU_GR0_sTLBGSYNC,
+ 			    ARM_SMMU_GR0_sTLBGSTATUS);
++>>>>>>> 00320ce65058 (iommu/arm-smmu: Abstract GR0 accesses)
  	spin_unlock_irqrestore(&smmu->global_sync_lock, flags);
  }
  
@@@ -576,75 -593,30 +625,75 @@@ static void arm_smmu_tlb_inv_vmid_nosyn
  					 size_t granule, bool leaf, void *cookie)
  {
  	struct arm_smmu_domain *smmu_domain = cookie;
- 	void __iomem *base = ARM_SMMU_GR0(smmu_domain->smmu);
+ 	struct arm_smmu_device *smmu = smmu_domain->smmu;
  
- 	if (smmu_domain->smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
+ 	if (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
  		wmb();
  
- 	writel_relaxed(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
+ 	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_TLBIVMID, smmu_domain->cfg.vmid);
  }
  
 -static const struct iommu_gather_ops arm_smmu_s1_tlb_ops = {
 -	.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
 -	.tlb_add_flush	= arm_smmu_tlb_inv_range_s1,
 -	.tlb_sync	= arm_smmu_tlb_sync_context,
 +static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
 +				  size_t granule, void *cookie)
 +{
 +	struct arm_smmu_domain *smmu_domain = cookie;
 +	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
 +
 +	ops->tlb_inv_range(iova, size, granule, false, cookie);
 +	ops->tlb_sync(cookie);
 +}
 +
 +static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
 +				  size_t granule, void *cookie)
 +{
 +	struct arm_smmu_domain *smmu_domain = cookie;
 +	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
 +
 +	ops->tlb_inv_range(iova, size, granule, true, cookie);
 +	ops->tlb_sync(cookie);
 +}
 +
 +static void arm_smmu_tlb_add_page(struct iommu_iotlb_gather *gather,
 +				  unsigned long iova, size_t granule,
 +				  void *cookie)
 +{
 +	struct arm_smmu_domain *smmu_domain = cookie;
 +	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
 +
 +	ops->tlb_inv_range(iova, granule, granule, true, cookie);
 +}
 +
 +static const struct arm_smmu_flush_ops arm_smmu_s1_tlb_ops = {
 +	.tlb = {
 +		.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
 +		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 +		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 +		.tlb_add_page	= arm_smmu_tlb_add_page,
 +	},
 +	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync		= arm_smmu_tlb_sync_context,
  };
  
 -static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v2 = {
 -	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 -	.tlb_add_flush	= arm_smmu_tlb_inv_range_s2,
 -	.tlb_sync	= arm_smmu_tlb_sync_context,
 +static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v2 = {
 +	.tlb = {
 +		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 +		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 +		.tlb_add_page	= arm_smmu_tlb_add_page,
 +	},
 +	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync		= arm_smmu_tlb_sync_context,
  };
  
 -static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v1 = {
 -	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 -	.tlb_add_flush	= arm_smmu_tlb_inv_vmid_nosync,
 -	.tlb_sync	= arm_smmu_tlb_sync_vmid,
 +static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v1 = {
 +	.tlb = {
 +		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 +		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 +		.tlb_add_page	= arm_smmu_tlb_add_page,
 +	},
 +	.tlb_inv_range		= arm_smmu_tlb_inv_vmid_nosync,
 +	.tlb_sync		= arm_smmu_tlb_sync_vmid,
  };
  
  static irqreturn_t arm_smmu_context_fault(int irq, void *dev)
@@@ -1144,15 -1108,15 +1191,27 @@@ static void arm_smmu_test_smr_masks(str
  	 * bits are set, so check each one separately. We can reject
  	 * masters later if they try to claim IDs outside these masks.
  	 */
++<<<<<<< HEAD
 +	smr = smmu->streamid_mask << SMR_ID_SHIFT;
 +	writel_relaxed(smr, gr0_base + ARM_SMMU_GR0_SMR(0));
 +	smr = readl_relaxed(gr0_base + ARM_SMMU_GR0_SMR(0));
 +	smmu->streamid_mask = smr >> SMR_ID_SHIFT;
 +
 +	smr = smmu->streamid_mask << SMR_MASK_SHIFT;
 +	writel_relaxed(smr, gr0_base + ARM_SMMU_GR0_SMR(0));
 +	smr = readl_relaxed(gr0_base + ARM_SMMU_GR0_SMR(0));
 +	smmu->smr_mask_mask = smr >> SMR_MASK_SHIFT;
++=======
+ 	smr = FIELD_PREP(SMR_ID, smmu->streamid_mask);
+ 	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(0), smr);
+ 	smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(0));
+ 	smmu->streamid_mask = FIELD_GET(SMR_ID, smr);
+ 
+ 	smr = FIELD_PREP(SMR_MASK, smmu->streamid_mask);
+ 	arm_smmu_gr0_write(smmu, ARM_SMMU_GR0_SMR(0), smr);
+ 	smr = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_SMR(0));
+ 	smmu->smr_mask_mask = FIELD_GET(SMR_MASK, smr);
++>>>>>>> 00320ce65058 (iommu/arm-smmu: Abstract GR0 accesses)
  }
  
  static int arm_smmu_find_sme(struct arm_smmu_device *smmu, u16 id, u16 mask)
@@@ -1792,9 -1768,9 +1850,15 @@@ static void arm_smmu_device_reset(struc
  		 * clear CACHE_LOCK bit of ACR first. And, CACHE_LOCK
  		 * bit is only present in MMU-500r2 onwards.
  		 */
++<<<<<<< HEAD
 +		reg = readl_relaxed(gr0_base + ARM_SMMU_GR0_ID7);
 +		major = (reg >> ID7_MAJOR_SHIFT) & ID7_MAJOR_MASK;
 +		reg = readl_relaxed(gr0_base + ARM_SMMU_GR0_sACR);
++=======
+ 		reg = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_ID7);
+ 		major = FIELD_GET(ID7_MAJOR, reg);
+ 		reg = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_sACR);
++>>>>>>> 00320ce65058 (iommu/arm-smmu: Abstract GR0 accesses)
  		if (major >= 2)
  			reg &= ~ARM_MMU500_ACR_CACHE_LOCK;
  		/*
@@@ -1879,8 -1853,7 +1943,12 @@@ static int arm_smmu_id_size_to_bits(in
  
  static int arm_smmu_device_cfg_probe(struct arm_smmu_device *smmu)
  {
++<<<<<<< HEAD
 +	unsigned long size;
 +	void __iomem *gr0_base = ARM_SMMU_GR0(smmu);
++=======
+ 	unsigned int size;
++>>>>>>> 00320ce65058 (iommu/arm-smmu: Abstract GR0 accesses)
  	u32 id;
  	bool cttw_reg, cttw_fw = smmu->features & ARM_SMMU_FEAT_COHERENT_WALK;
  	int i;
@@@ -2021,8 -1995,8 +2089,13 @@@
  		return -ENOMEM;
  
  	/* ID2 */
++<<<<<<< HEAD
 +	id = readl_relaxed(gr0_base + ARM_SMMU_GR0_ID2);
 +	size = arm_smmu_id_size_to_bits((id >> ID2_IAS_SHIFT) & ID2_IAS_MASK);
++=======
+ 	id = arm_smmu_gr0_read(smmu, ARM_SMMU_GR0_ID2);
+ 	size = arm_smmu_id_size_to_bits(FIELD_GET(ID2_IAS, id));
++>>>>>>> 00320ce65058 (iommu/arm-smmu: Abstract GR0 accesses)
  	smmu->ipa_size = size;
  
  	/* The output mask is also applied for bypass */
* Unmerged path drivers/iommu/arm-smmu.c
