lib/vdso: Cleanup clock mode storage leftovers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit f86fd32db706613fe8d0104057efa6e83e0d7e8f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f86fd32d.failed

Now that all architectures are converted to use the generic storage the
helpers and conditionals can be removed.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
	Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
Link: https://lkml.kernel.org/r/20200207124403.470699892@linutronix.de



(cherry picked from commit f86fd32db706613fe8d0104057efa6e83e0d7e8f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/mm/Kconfig
#	arch/arm64/Kconfig
#	arch/mips/Kconfig
#	arch/x86/Kconfig
#	include/asm-generic/vdso/vsyscall.h
#	include/linux/clocksource.h
#	kernel/time/vsyscall.c
#	lib/vdso/Kconfig
#	lib/vdso/gettimeofday.c
diff --cc arch/arm/mm/Kconfig
index 96a7b6cf459b,65e4482e3849..000000000000
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@@ -888,7 -896,10 +888,12 @@@ config VDS
  	bool "Enable VDSO for acceleration of some system calls"
  	depends on AEABI && MMU && CPU_V7
  	default y if ARM_ARCH_TIMER
 -	select HAVE_GENERIC_VDSO
  	select GENERIC_TIME_VSYSCALL
++<<<<<<< HEAD
++=======
+ 	select GENERIC_VDSO_32
+ 	select GENERIC_GETTIMEOFDAY
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  	help
  	  Place in the process address space an ELF shared object
  	  providing fast implementations of gettimeofday and
diff --cc arch/arm64/Kconfig
index d3f3af6e3b68,c6c32fb7f546..000000000000
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@@ -104,8 -109,10 +104,12 @@@ config ARM6
  	select GENERIC_STRNCPY_FROM_USER
  	select GENERIC_STRNLEN_USER
  	select GENERIC_TIME_VSYSCALL
++<<<<<<< HEAD
++=======
+ 	select GENERIC_GETTIMEOFDAY
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  	select HANDLE_DOMAIN_IRQ
  	select HARDIRQS_SW_RESEND
 -	select HAVE_PCI
  	select HAVE_ACPI_APEI if (ACPI && EFI)
  	select HAVE_ALIGNED_STRUCT_PAGE if SLUB
  	select HAVE_ARCH_AUDITSYSCALL
diff --cc arch/mips/Kconfig
index 8ef87c03cf44,654369a7209d..000000000000
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@@ -31,7 -37,9 +31,11 @@@ config MIP
  	select GENERIC_SCHED_CLOCK if !CAVIUM_OCTEON_SOC
  	select GENERIC_SMP_IDLE_THREAD
  	select GENERIC_TIME_VSYSCALL
++<<<<<<< HEAD
++=======
+ 	select GUP_GET_PTE_LOW_HIGH if CPU_MIPS32 && PHYS_ADDR_T_64BIT
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  	select HANDLE_DOMAIN_IRQ
 -	select HAVE_ARCH_COMPILER_H
  	select HAVE_ARCH_JUMP_LABEL
  	select HAVE_ARCH_KGDB
  	select HAVE_ARCH_MMAP_RND_BITS if MMU
diff --cc arch/x86/Kconfig
index 3fc0a4c91af5,8b995db3d10f..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -127,7 -124,9 +127,13 @@@ config X8
  	select GENERIC_STRNCPY_FROM_USER
  	select GENERIC_STRNLEN_USER
  	select GENERIC_TIME_VSYSCALL
++<<<<<<< HEAD
 +	select HARDIRQS_SW_RESEND
++=======
+ 	select GENERIC_GETTIMEOFDAY
+ 	select GENERIC_VDSO_TIME_NS
+ 	select GUP_GET_PTE_LOW_HIGH		if X86_PAE
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  	select HARDLOCKUP_CHECK_TIMESTAMP	if X86_64
  	select HAVE_ACPI_APEI			if ACPI
  	select HAVE_ACPI_APEI_NMI		if ACPI
diff --cc include/asm-generic/vdso/vsyscall.h
index e94b19782c92,4a28797495d7..000000000000
--- a/include/asm-generic/vdso/vsyscall.h
+++ b/include/asm-generic/vdso/vsyscall.h
@@@ -18,20 -18,6 +18,23 @@@ static __always_inline int __arch_updat
  }
  #endif /* __arch_update_vdso_data */
  
++<<<<<<< HEAD
 +#ifndef __arch_get_clock_mode
 +static __always_inline int __arch_get_clock_mode(struct timekeeper *tk)
 +{
 +	return 0;
 +}
 +#endif /* __arch_get_clock_mode */
 +
 +#ifndef __arch_use_vsyscall
 +static __always_inline int __arch_use_vsyscall(struct vdso_data *vdata)
 +{
 +	return 1;
 +}
 +#endif /* __arch_use_vsyscall */
 +
++=======
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  #ifndef __arch_update_vsyscall
  static __always_inline void __arch_update_vsyscall(struct vdso_data *vdata,
  						   struct timekeeper *tk)
diff --cc include/linux/clocksource.h
index 1b7db49717c9,7fefe0b21a14..000000000000
--- a/include/linux/clocksource.h
+++ b/include/linux/clocksource.h
@@@ -23,10 -23,19 +23,26 @@@
  struct clocksource;
  struct module;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_ARCH_CLOCKSOURCE_DATA
 +#include <asm/clocksource.h>
 +#endif
 +
++=======
+ #if defined(CONFIG_ARCH_CLOCKSOURCE_DATA) || \
+     defined(CONFIG_GENERIC_GETTIMEOFDAY)
+ #include <asm/clocksource.h>
+ #endif
+ 
+ enum vdso_clock_mode {
+ 	VDSO_CLOCKMODE_NONE,
+ #ifdef CONFIG_GENERIC_GETTIMEOFDAY
+ 	VDSO_ARCH_CLOCKMODES,
+ #endif
+ 	VDSO_CLOCKMODE_MAX,
+ };
+ 
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  /**
   * struct clocksource - hardware abstraction for a free running counter
   *	Provides mostly state-free accessors to the underlying hardware.
diff --cc kernel/time/vsyscall.c
index 4bc37ac3bb05,d31a5ef4ade5..000000000000
--- a/kernel/time/vsyscall.c
+++ b/kernel/time/vsyscall.c
@@@ -95,8 -77,14 +95,19 @@@ void update_vsyscall(struct timekeeper 
  	/* copy vsyscall data */
  	vdso_write_begin(vdata);
  
++<<<<<<< HEAD
 +	vdata[CS_HRES_COARSE].clock_mode	= __arch_get_clock_mode(tk);
 +	vdata[CS_RAW].clock_mode		= __arch_get_clock_mode(tk);
++=======
+ 	clock_mode = tk->tkr_mono.clock->vdso_clock_mode;
+ 	vdata[CS_HRES_COARSE].clock_mode	= clock_mode;
+ 	vdata[CS_RAW].clock_mode		= clock_mode;
+ 
+ 	/* CLOCK_REALTIME also required for time() */
+ 	vdso_ts		= &vdata[CS_HRES_COARSE].basetime[CLOCK_REALTIME];
+ 	vdso_ts->sec	= tk->xtime_sec;
+ 	vdso_ts->nsec	= tk->tkr_mono.xtime_nsec;
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  
  	/* CLOCK_REALTIME_COARSE */
  	vdso_ts		= &vdata[CS_HRES_COARSE].basetime[CLOCK_REALTIME_COARSE];
diff --cc lib/vdso/Kconfig
index cc00364bd2c2,d883ac299508..000000000000
--- a/lib/vdso/Kconfig
+++ b/lib/vdso/Kconfig
@@@ -24,13 -24,10 +24,18 @@@ config GENERIC_COMPAT_VDS
  	help
  	  This config option enables the compat VDSO layer.
  
 -config GENERIC_VDSO_TIME_NS
 -	bool
 +config CROSS_COMPILE_COMPAT_VDSO
 +	string "32 bit Toolchain prefix for compat vDSO"
 +	default ""
 +	depends on GENERIC_COMPAT_VDSO
  	help
++<<<<<<< HEAD
 +	  Defines the cross-compiler prefix for compiling compat vDSO.
 +	  If a 64 bit compiler (i.e. x86_64) can compile the VDSO for
 +	  32 bit, it does not need to define this parameter.
++=======
+ 	  Selected by architectures which support time namespaces in the
+ 	  VDSO
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  
  endif
diff --cc lib/vdso/gettimeofday.c
index 632c43443888,00f8d1f1405b..000000000000
--- a/lib/vdso/gettimeofday.c
+++ b/lib/vdso/gettimeofday.c
@@@ -26,23 -27,120 +26,133 @@@
  #include <asm/vdso/gettimeofday.h>
  #endif /* ENABLE_COMPAT_VDSO */
  
++<<<<<<< HEAD
 +static int do_hres(const struct vdso_data *vd, clockid_t clk,
 +		   struct __kernel_timespec *ts)
++=======
+ #ifndef vdso_calc_delta
+ /*
+  * Default implementation which works for all sane clocksources. That
+  * obviously excludes x86/TSC.
+  */
+ static __always_inline
+ u64 vdso_calc_delta(u64 cycles, u64 last, u64 mask, u32 mult)
+ {
+ 	return ((cycles - last) & mask) * mult;
+ }
+ #endif
+ 
+ #ifndef __arch_vdso_hres_capable
+ static inline bool __arch_vdso_hres_capable(void)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ #ifdef CONFIG_TIME_NS
+ static int do_hres_timens(const struct vdso_data *vdns, clockid_t clk,
+ 			  struct __kernel_timespec *ts)
+ {
+ 	const struct vdso_data *vd = __arch_get_timens_vdso_data();
+ 	const struct timens_offset *offs = &vdns->offset[clk];
+ 	const struct vdso_timestamp *vdso_ts;
+ 	u64 cycles, last, ns;
+ 	u32 seq;
+ 	s64 sec;
+ 
+ 	if (clk != CLOCK_MONOTONIC_RAW)
+ 		vd = &vd[CS_HRES_COARSE];
+ 	else
+ 		vd = &vd[CS_RAW];
+ 	vdso_ts = &vd->basetime[clk];
+ 
+ 	do {
+ 		seq = vdso_read_begin(vd);
+ 
+ 		if (unlikely(vd->clock_mode == VDSO_CLOCKMODE_NONE))
+ 			return -1;
+ 
+ 		cycles = __arch_get_hw_counter(vd->clock_mode);
+ 		ns = vdso_ts->nsec;
+ 		last = vd->cycle_last;
+ 		ns += vdso_calc_delta(cycles, last, vd->mask, vd->mult);
+ 		ns >>= vd->shift;
+ 		sec = vdso_ts->sec;
+ 	} while (unlikely(vdso_read_retry(vd, seq)));
+ 
+ 	/* Add the namespace offset */
+ 	sec += offs->sec;
+ 	ns += offs->nsec;
+ 
+ 	/*
+ 	 * Do this outside the loop: a race inside the loop could result
+ 	 * in __iter_div_u64_rem() being extremely slow.
+ 	 */
+ 	ts->tv_sec = sec + __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);
+ 	ts->tv_nsec = ns;
+ 
+ 	return 0;
+ }
+ #else
+ static __always_inline const struct vdso_data *__arch_get_timens_vdso_data(void)
+ {
+ 	return NULL;
+ }
+ 
+ static int do_hres_timens(const struct vdso_data *vdns, clockid_t clk,
+ 			  struct __kernel_timespec *ts)
+ {
+ 	return -EINVAL;
+ }
+ #endif
+ 
+ static __always_inline int do_hres(const struct vdso_data *vd, clockid_t clk,
+ 				   struct __kernel_timespec *ts)
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  {
  	const struct vdso_timestamp *vdso_ts = &vd->basetime[clk];
  	u64 cycles, last, sec, ns;
  	u32 seq;
  
 -	/* Allows to compile the high resolution parts out */
 -	if (!__arch_vdso_hres_capable())
 -		return -1;
 -
  	do {
++<<<<<<< HEAD
 +		seq = vdso_read_begin(vd);
 +		cycles = __arch_get_hw_counter(vd->clock_mode) &
 +			vd->mask;
 +		ns = vdso_ts->nsec;
 +		last = vd->cycle_last;
 +		if (unlikely((s64)cycles < 0))
 +			return clock_gettime_fallback(clk, ts);
 +		if (cycles > last)
 +			ns += (cycles - last) * vd->mult;
++=======
+ 		/*
+ 		 * Open coded to handle VCLOCK_TIMENS. Time namespace
+ 		 * enabled tasks have a special VVAR page installed which
+ 		 * has vd->seq set to 1 and vd->clock_mode set to
+ 		 * VCLOCK_TIMENS. For non time namespace affected tasks
+ 		 * this does not affect performance because if vd->seq is
+ 		 * odd, i.e. a concurrent update is in progress the extra
+ 		 * check for vd->clock_mode is just a few extra
+ 		 * instructions while spin waiting for vd->seq to become
+ 		 * even again.
+ 		 */
+ 		while (unlikely((seq = READ_ONCE(vd->seq)) & 1)) {
+ 			if (IS_ENABLED(CONFIG_TIME_NS) &&
+ 			    vd->clock_mode == VCLOCK_TIMENS)
+ 				return do_hres_timens(vd, clk, ts);
+ 			cpu_relax();
+ 		}
+ 		smp_rmb();
+ 
+ 		if (unlikely(vd->clock_mode == VDSO_CLOCKMODE_NONE))
+ 			return -1;
+ 
+ 		cycles = __arch_get_hw_counter(vd->clock_mode);
+ 		ns = vdso_ts->nsec;
+ 		last = vd->cycle_last;
+ 		ns += vdso_calc_delta(cycles, last, vd->mask, vd->mult);
++>>>>>>> f86fd32db706 (lib/vdso: Cleanup clock mode storage leftovers)
  		ns >>= vd->shift;
  		sec = vdso_ts->sec;
  	} while (unlikely(vdso_read_retry(vd, seq)));
* Unmerged path arch/arm/mm/Kconfig
* Unmerged path arch/arm64/Kconfig
* Unmerged path arch/mips/Kconfig
* Unmerged path arch/x86/Kconfig
* Unmerged path include/asm-generic/vdso/vsyscall.h
* Unmerged path include/linux/clocksource.h
* Unmerged path kernel/time/vsyscall.c
* Unmerged path lib/vdso/Kconfig
* Unmerged path lib/vdso/gettimeofday.c
