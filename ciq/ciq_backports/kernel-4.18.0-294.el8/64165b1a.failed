mm: swapoff: take notice of completion sooner

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Hugh Dickins <hughd@google.com>
commit 64165b1affc5bc16231ac971e66aae7d68d57f2c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/64165b1a.failed

The old try_to_unuse() implementation was driven by find_next_to_unuse(),
which terminated as soon as all the swap had been freed.

Add inuse_pages checks now (alongside signal_pending()) to stop scanning
mms and swap_map once finished.

The same ought to be done in shmem_unuse() too, but never was before,
and needs a different interface: so leave it as is for now.

Link: http://lkml.kernel.org/r/alpine.LSU.2.11.1904081258200.1523@eggly.anvils
Fixes: b56a2d8af914 ("mm: rid swapoff of quadratic complexity")
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Cc: "Alex Xu (Hello71)" <alex_y_xu@yahoo.ca>
	Cc: Huang Ying <ying.huang@intel.com>
	Cc: Kelley Nielsen <kelleynnn@gmail.com>
	Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Vineeth Pillai <vpillai@digitalocean.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 64165b1affc5bc16231ac971e66aae7d68d57f2c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swapfile.c
diff --cc mm/swapfile.c
index b77fb155eaf4,71383625a582..000000000000
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@@ -2174,226 -2026,110 +2174,305 @@@ static unsigned int find_next_to_unuse(
  int try_to_unuse(unsigned int type, bool frontswap,
  		 unsigned long pages_to_unuse)
  {
 -	struct mm_struct *prev_mm;
 -	struct mm_struct *mm;
 -	struct list_head *p;
 -	int retval = 0;
  	struct swap_info_struct *si = swap_info[type];
 +	struct mm_struct *start_mm;
 +	volatile unsigned char *swap_map; /* swap_map is accessed without
 +					   * locking. Mark it as volatile
 +					   * to prevent compiler doing
 +					   * something odd.
 +					   */
 +	unsigned char swcount;
  	struct page *page;
  	swp_entry_t entry;
 -	unsigned int i;
 +	unsigned int i = 0;
 +	int retval = 0;
  
 -	if (!si->inuse_pages)
 -		return 0;
 +	/*
 +	 * When searching mms for an entry, a good strategy is to
 +	 * start at the first mm we freed the previous entry from
 +	 * (though actually we don't notice whether we or coincidence
 +	 * freed the entry).  Initialize this start_mm with a hold.
 +	 *
 +	 * A simpler strategy would be to start at the last mm we
 +	 * freed the previous entry from; but that would take less
 +	 * advantage of mmlist ordering, which clusters forked mms
 +	 * together, child after parent.  If we race with dup_mmap(), we
 +	 * prefer to resolve parent before child, lest we miss entries
 +	 * duplicated after we scanned child: using last mm would invert
 +	 * that.
 +	 */
 +	start_mm = &init_mm;
 +	mmget(&init_mm);
  
++<<<<<<< HEAD
 +	/*
 +	 * Keep on scanning until all entries have gone.  Usually,
 +	 * one pass through swap_map is enough, but not necessarily:
 +	 * there are races when an instance of an entry might be missed.
 +	 */
 +	while ((i = find_next_to_unuse(si, i, frontswap)) != 0) {
 +		if (signal_pending(current)) {
 +			retval = -EINTR;
 +			break;
 +		}
++=======
+ 	if (!frontswap)
+ 		pages_to_unuse = 0;
+ 
+ retry:
+ 	retval = shmem_unuse(type, frontswap, &pages_to_unuse);
+ 	if (retval)
+ 		goto out;
+ 
+ 	prev_mm = &init_mm;
+ 	mmget(prev_mm);
+ 
+ 	spin_lock(&mmlist_lock);
+ 	p = &init_mm.mmlist;
+ 	while (si->inuse_pages &&
+ 	       !signal_pending(current) &&
+ 	       (p = p->next) != &init_mm.mmlist) {
++>>>>>>> 64165b1affc5 (mm: swapoff: take notice of completion sooner)
  
 -		mm = list_entry(p, struct mm_struct, mmlist);
 -		if (!mmget_not_zero(mm))
 +		/*
 +		 * Get a page for the entry, using the existing swap
 +		 * cache page if there is one.  Otherwise, get a clean
 +		 * page and read the swap into it.
 +		 */
 +		swap_map = &si->swap_map[i];
 +		entry = swp_entry(type, i);
 +		page = read_swap_cache_async(entry,
 +					GFP_HIGHUSER_MOVABLE, NULL, 0, false);
 +		if (!page) {
 +			/*
 +			 * Either swap_duplicate() failed because entry
 +			 * has been freed independently, and will not be
 +			 * reused since sys_swapoff() already disabled
 +			 * allocation from here, or alloc_page() failed.
 +			 */
 +			swcount = *swap_map;
 +			/*
 +			 * We don't hold lock here, so the swap entry could be
 +			 * SWAP_MAP_BAD (when the cluster is discarding).
 +			 * Instead of fail out, We can just skip the swap
 +			 * entry because swapoff will wait for discarding
 +			 * finish anyway.
 +			 */
 +			if (!swcount || swcount == SWAP_MAP_BAD)
 +				continue;
 +			retval = -ENOMEM;
 +			break;
 +		}
 +
 +		/*
 +		 * Don't hold on to start_mm if it looks like exiting.
 +		 */
 +		if (atomic_read(&start_mm->mm_users) == 1) {
 +			mmput(start_mm);
 +			start_mm = &init_mm;
 +			mmget(&init_mm);
 +		}
 +
 +		/*
 +		 * Wait for and lock page.  When do_swap_page races with
 +		 * try_to_unuse, do_swap_page can handle the fault much
 +		 * faster than try_to_unuse can locate the entry.  This
 +		 * apparently redundant "wait_on_page_locked" lets try_to_unuse
 +		 * defer to do_swap_page in such a case - in some tests,
 +		 * do_swap_page and try_to_unuse repeatedly compete.
 +		 */
 +		wait_on_page_locked(page);
 +		wait_on_page_writeback(page);
 +		lock_page(page);
 +		wait_on_page_writeback(page);
 +
 +		/*
 +		 * Remove all references to entry.
 +		 */
 +		swcount = *swap_map;
 +		if (swap_count(swcount) == SWAP_MAP_SHMEM) {
 +			retval = shmem_unuse(entry, page);
 +			/* page has already been unlocked and released */
 +			if (retval < 0)
 +				break;
  			continue;
 -		spin_unlock(&mmlist_lock);
 -		mmput(prev_mm);
 -		prev_mm = mm;
 -		retval = unuse_mm(mm, type, frontswap, &pages_to_unuse);
 +		}
 +		if (swap_count(swcount) && start_mm != &init_mm)
 +			retval = unuse_mm(start_mm, entry, page);
 +
 +		if (swap_count(*swap_map)) {
 +			int set_start_mm = (*swap_map >= swcount);
 +			struct list_head *p = &start_mm->mmlist;
 +			struct mm_struct *new_start_mm = start_mm;
 +			struct mm_struct *prev_mm = start_mm;
 +			struct mm_struct *mm;
 +
 +			mmget(new_start_mm);
 +			mmget(prev_mm);
 +			spin_lock(&mmlist_lock);
 +			while (swap_count(*swap_map) && !retval &&
 +					(p = p->next) != &start_mm->mmlist) {
 +				mm = list_entry(p, struct mm_struct, mmlist);
 +				if (!mmget_not_zero(mm))
 +					continue;
 +				spin_unlock(&mmlist_lock);
 +				mmput(prev_mm);
 +				prev_mm = mm;
  
 -		if (retval) {
 +				cond_resched();
 +
 +				swcount = *swap_map;
 +				if (!swap_count(swcount)) /* any usage ? */
 +					;
 +				else if (mm == &init_mm)
 +					set_start_mm = 1;
 +				else
 +					retval = unuse_mm(mm, entry, page);
 +
 +				if (set_start_mm && *swap_map < swcount) {
 +					mmput(new_start_mm);
 +					mmget(mm);
 +					new_start_mm = mm;
 +					set_start_mm = 0;
 +				}
 +				spin_lock(&mmlist_lock);
 +			}
 +			spin_unlock(&mmlist_lock);
  			mmput(prev_mm);
 -			goto out;
 +			mmput(start_mm);
 +			start_mm = new_start_mm;
 +		}
 +		if (retval) {
 +			unlock_page(page);
 +			put_page(page);
 +			break;
  		}
  
 +		/*
 +		 * If a reference remains (rare), we would like to leave
 +		 * the page in the swap cache; but try_to_unmap could
 +		 * then re-duplicate the entry once we drop page lock,
 +		 * so we might loop indefinitely; also, that page could
 +		 * not be swapped out to other storage meanwhile.  So:
 +		 * delete from cache even if there's another reference,
 +		 * after ensuring that the data has been saved to disk -
 +		 * since if the reference remains (rarer), it will be
 +		 * read from disk into another page.  Splitting into two
 +		 * pages would be incorrect if swap supported "shared
 +		 * private" pages, but they are handled by tmpfs files.
 +		 *
 +		 * Given how unuse_vma() targets one particular offset
 +		 * in an anon_vma, once the anon_vma has been determined,
 +		 * this splitting happens to be just what is needed to
 +		 * handle where KSM pages have been swapped out: re-reading
 +		 * is unnecessarily slow, but we can fix that later on.
 +		 */
 +		if (swap_count(*swap_map) &&
 +		     PageDirty(page) && PageSwapCache(page)) {
 +			struct writeback_control wbc = {
 +				.sync_mode = WB_SYNC_NONE,
 +			};
 +
 +			swap_writepage(compound_head(page), &wbc);
 +			lock_page(page);
 +			wait_on_page_writeback(page);
 +		}
 +
 +		/*
 +		 * It is conceivable that a racing task removed this page from
 +		 * swap cache just before we acquired the page lock at the top,
 +		 * or while we dropped it in unuse_mm().  The page might even
 +		 * be back in swap cache on another swap area: that we must not
 +		 * delete, since it may not have been written out to swap yet.
 +		 */
 +		if (PageSwapCache(page) &&
 +		    likely(page_private(page) == entry.val) &&
 +		    (!PageTransCompound(page) ||
 +		     !swap_page_trans_huge_swapped(si, entry)))
 +			delete_from_swap_cache(compound_head(page));
 +
 +		/*
 +		 * So we could skip searching mms once swap count went
 +		 * to 1, we did not mark any present ptes as dirty: must
 +		 * mark page dirty so shrink_page_list will preserve it.
 +		 */
 +		SetPageDirty(page);
 +		unlock_page(page);
 +		put_page(page);
 +
  		/*
  		 * Make sure that we aren't completely killing
  		 * interactive performance.
  		 */
  		cond_resched();
++<<<<<<< HEAD
 +		if (frontswap && pages_to_unuse > 0) {
 +			if (!--pages_to_unuse)
 +				break;
 +		}
 +	}
 +
 +	mmput(start_mm);
 +	return retval;
++=======
+ 		spin_lock(&mmlist_lock);
+ 	}
+ 	spin_unlock(&mmlist_lock);
+ 
+ 	mmput(prev_mm);
+ 
+ 	i = 0;
+ 	while (si->inuse_pages &&
+ 	       !signal_pending(current) &&
+ 	       (i = find_next_to_unuse(si, i, frontswap)) != 0) {
+ 
+ 		entry = swp_entry(type, i);
+ 		page = find_get_page(swap_address_space(entry), i);
+ 		if (!page)
+ 			continue;
+ 
+ 		/*
+ 		 * It is conceivable that a racing task removed this page from
+ 		 * swap cache just before we acquired the page lock. The page
+ 		 * might even be back in swap cache on another swap area. But
+ 		 * that is okay, try_to_free_swap() only removes stale pages.
+ 		 */
+ 		lock_page(page);
+ 		wait_on_page_writeback(page);
+ 		try_to_free_swap(page);
+ 		unlock_page(page);
+ 		put_page(page);
+ 
+ 		/*
+ 		 * For frontswap, we just need to unuse pages_to_unuse, if
+ 		 * it was specified. Need not check frontswap again here as
+ 		 * we already zeroed out pages_to_unuse if not frontswap.
+ 		 */
+ 		if (pages_to_unuse && --pages_to_unuse == 0)
+ 			goto out;
+ 	}
+ 
+ 	/*
+ 	 * Lets check again to see if there are still swap entries in the map.
+ 	 * If yes, we would need to do retry the unuse logic again.
+ 	 * Under global memory pressure, swap entries can be reinserted back
+ 	 * into process space after the mmlist loop above passes over them.
+ 	 *
+ 	 * Limit the number of retries? No: when shmem_unuse()'s igrab() fails,
+ 	 * a shmem inode using swap is being evicted; and when mmget_not_zero()
+ 	 * above fails, that mm is likely to be freeing swap from exit_mmap().
+ 	 * Both proceed at their own independent pace: we could move them to
+ 	 * separate lists, and wait for those lists to be emptied; but it's
+ 	 * easier and more robust (though cpu-intensive) just to keep retrying.
+ 	 */
+ 	if (si->inuse_pages) {
+ 		if (!signal_pending(current))
+ 			goto retry;
+ 		retval = -EINTR;
+ 	}
+ out:
+ 	return (retval == FRONTSWAP_PAGES_UNUSED) ? 0 : retval;
++>>>>>>> 64165b1affc5 (mm: swapoff: take notice of completion sooner)
  }
  
  /*
* Unmerged path mm/swapfile.c
