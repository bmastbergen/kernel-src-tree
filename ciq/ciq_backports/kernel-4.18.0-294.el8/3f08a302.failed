mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Mike Rapoport <rppt@linux.ibm.com>
commit 3f08a302f533f74ad2e909e7a61274aa7eebc0ab
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/3f08a302.failed

CONFIG_HAVE_MEMBLOCK_NODE_MAP is used to differentiate initialization of
nodes and zones structures between the systems that have region to node
mapping in memblock and those that don't.

Currently all the NUMA architectures enable this option and for the
non-NUMA systems we can presume that all the memory belongs to node 0 and
therefore the compile time configuration option is not required.

The remaining few architectures that use DISCONTIGMEM without NUMA are
easily updated to use memblock_add_node() instead of memblock_add() and
thus have proper correspondence of memblock regions to NUMA nodes.

Still, free_area_init_node() must have a backward compatible version
because its semantics with and without CONFIG_HAVE_MEMBLOCK_NODE_MAP is
different.  Once all the architectures will use the new semantics, the
entire compatibility layer can be dropped.

To avoid addition of extra run time memory to store node id for
architectures that keep memblock but have only a single node, the node id
field of the memblock_region is guarded by CONFIG_NEED_MULTIPLE_NODES and
the corresponding accessors presume that in those cases it is always 0.

	Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Tested-by: Hoan Tran <hoan@os.amperecomputing.com>	[arm64]
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>	[arm64]
	Cc: Baoquan He <bhe@redhat.com>
	Cc: Brian Cain <bcain@codeaurora.org>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Greentime Hu <green.hu@gmail.com>
	Cc: Greg Ungerer <gerg@linux-m68k.org>
	Cc: Guan Xuetao <gxt@pku.edu.cn>
	Cc: Guo Ren <guoren@kernel.org>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Helge Deller <deller@gmx.de>
	Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Ley Foon Tan <ley.foon.tan@intel.com>
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Matt Turner <mattst88@gmail.com>
	Cc: Max Filippov <jcmvbkbc@gmail.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Michal Simek <monstr@monstr.eu>
	Cc: Nick Hu <nickhu@andestech.com>
	Cc: Paul Walmsley <paul.walmsley@sifive.com>
	Cc: Richard Weinberger <richard@nod.at>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: Stafford Horne <shorne@gmail.com>
	Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Vineet Gupta <vgupta@synopsys.com>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
Link: http://lkml.kernel.org/r/20200412194859.12663-4-rppt@kernel.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3f08a302f533f74ad2e909e7a61274aa7eebc0ab)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/features/vm/numa-memblock/arch-support.txt
#	arch/alpha/mm/numa.c
#	arch/m68k/mm/motorola.c
#	arch/mips/Kconfig
#	arch/riscv/Kconfig
#	arch/sh/Kconfig
#	arch/sparc/Kconfig
#	include/linux/mm.h
#	mm/page_alloc.c
diff --cc arch/alpha/mm/numa.c
index 3fcec1de97f0,a24cd13e71cb..000000000000
--- a/arch/alpha/mm/numa.c
+++ b/arch/alpha/mm/numa.c
@@@ -156,6 -144,9 +156,12 @@@ setup_memory_node(int nid, void *kernel
  	if (!nid && (node_max_pfn < end_kernel_pfn || node_min_pfn > start_kernel_pfn))
  		panic("kernel loaded out of ram");
  
++<<<<<<< HEAD
++=======
+ 	memblock_add_node(PFN_PHYS(node_min_pfn),
+ 			  (node_max_pfn - node_min_pfn) << PAGE_SHIFT, nid);
+ 
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  	/* Zone start phys-addr must be 2^(MAX_ORDER-1) aligned.
  	   Note that we round this down, not up - node memory
  	   has much larger alignment than 8Mb, so it's safe. */
diff --cc arch/m68k/mm/motorola.c
index 35a3624e1a61,84ab5963cabb..000000000000
--- a/arch/m68k/mm/motorola.c
+++ b/arch/m68k/mm/motorola.c
@@@ -228,6 -386,7 +228,10 @@@ void __init paging_init(void
  
  	min_addr = m68k_memory[0].addr;
  	max_addr = min_addr + m68k_memory[0].size;
++<<<<<<< HEAD
++=======
+ 	memblock_add_node(m68k_memory[0].addr, m68k_memory[0].size, 0);
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  	for (i = 1; i < m68k_num_memory;) {
  		if (m68k_memory[i].addr < min_addr) {
  			printk("Ignoring memory chunk at 0x%lx:0x%lx before the first chunk\n",
@@@ -238,6 -397,7 +242,10 @@@
  				(m68k_num_memory - i) * sizeof(struct m68k_mem_info));
  			continue;
  		}
++<<<<<<< HEAD
++=======
+ 		memblock_add_node(m68k_memory[i].addr, m68k_memory[i].size, i);
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  		addr = m68k_memory[i].addr + m68k_memory[i].size;
  		if (addr > max_addr)
  			max_addr = addr;
diff --cc arch/mips/Kconfig
index 8ef87c03cf44,94a91b5b7759..000000000000
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@@ -58,7 -71,7 +58,11 @@@ config MIP
  	select HAVE_IRQ_TIME_ACCOUNTING
  	select HAVE_KPROBES
  	select HAVE_KRETPROBES
++<<<<<<< HEAD
 +	select HAVE_MEMBLOCK_NODE_MAP
++=======
+ 	select HAVE_LD_DEAD_CODE_DATA_ELIMINATION
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  	select HAVE_MOD_ARCH_SPECIFIC
  	select HAVE_NMI
  	select HAVE_OPROFILE
diff --cc arch/riscv/Kconfig
index 9c4898b73d6c,5c07ca4d5cd6..000000000000
--- a/arch/riscv/Kconfig
+++ b/arch/riscv/Kconfig
@@@ -19,21 -18,27 +19,32 @@@ config RISC
  	select ARCH_WANT_FRAME_POINTERS
  	select CLONE_BACKWARDS
  	select COMMON_CLK
 +	select DMA_DIRECT_OPS
  	select GENERIC_CLOCKEVENTS
 +	select GENERIC_CPU_DEVICES
  	select GENERIC_IRQ_SHOW
  	select GENERIC_PCI_IOMAP
 -	select GENERIC_SCHED_CLOCK
 -	select GENERIC_STRNCPY_FROM_USER if MMU
 -	select GENERIC_STRNLEN_USER if MMU
 +	select GENERIC_STRNCPY_FROM_USER
 +	select GENERIC_STRNLEN_USER
  	select GENERIC_SMP_IDLE_THREAD
++<<<<<<< HEAD
 +	select GENERIC_ATOMIC64 if !64BIT || !RISCV_ISA_A
 +	select HAVE_MEMBLOCK_NODE_MAP
 +	select HAVE_DMA_CONTIGUOUS
 +	select HAVE_GENERIC_DMA_COHERENT
++=======
+ 	select GENERIC_ATOMIC64 if !64BIT
+ 	select GENERIC_IOREMAP
+ 	select GENERIC_PTDUMP if MMU
+ 	select HAVE_ARCH_AUDITSYSCALL
+ 	select HAVE_ARCH_SECCOMP_FILTER
+ 	select HAVE_ASM_MODVERSIONS
+ 	select HAVE_DMA_CONTIGUOUS if MMU
+ 	select HAVE_FUTEX_CMPXCHG if FUTEX
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  	select HAVE_PERF_EVENTS
 -	select HAVE_PERF_REGS
 -	select HAVE_PERF_USER_STACK_DUMP
 -	select HAVE_SYSCALL_TRACEPOINTS
  	select IRQ_DOMAIN
 +	select RISCV_ISA_A if SMP
  	select SPARSE_IRQ
  	select SYSCTL_EXCEPTION_TRACE
  	select HAVE_ARCH_TRACEHOOK
diff --cc arch/sh/Kconfig
index f0c3c20df572,0424b8f2f8d3..000000000000
--- a/arch/sh/Kconfig
+++ b/arch/sh/Kconfig
@@@ -4,14 -4,12 +4,17 @@@ config SUPER
  	select ARCH_HAS_PTE_SPECIAL
  	select ARCH_HAS_TICK_BROADCAST if GENERIC_CLOCKEVENTS_BROADCAST
  	select ARCH_MIGHT_HAVE_PC_PARPORT
 +	select ARCH_NO_COHERENT_DMA_MMAP if !MMU
  	select HAVE_PATA_PLATFORM
  	select CLKDEV_LOOKUP
 -	select DMA_DECLARE_COHERENT
  	select HAVE_IDE if HAS_IOPORT_MAP
++<<<<<<< HEAD
 +	select HAVE_MEMBLOCK_NODE_MAP
 +	select ARCH_DISCARD_MEMBLOCK
++=======
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  	select HAVE_OPROFILE
 +	select HAVE_GENERIC_DMA_COHERENT
  	select HAVE_ARCH_TRACEHOOK
  	select HAVE_PERF_EVENTS
  	select HAVE_DEBUG_BUGVERBOSE
diff --cc arch/sparc/Kconfig
index 12d490d796fe,795206b7b552..000000000000
--- a/arch/sparc/Kconfig
+++ b/arch/sparc/Kconfig
@@@ -59,8 -64,7 +59,12 @@@ config SPARC6
  	select HAVE_FUNCTION_GRAPH_TRACER
  	select HAVE_KRETPROBES
  	select HAVE_KPROBES
++<<<<<<< HEAD
 +	select HAVE_RCU_TABLE_FREE if SMP
 +	select HAVE_MEMBLOCK_NODE_MAP
++=======
+ 	select MMU_GATHER_RCU_TABLE_FREE if SMP
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
  	select HAVE_DYNAMIC_FTRACE
  	select HAVE_FTRACE_MCOUNT_RECORD
diff --cc include/linux/mm.h
index c2872a52dcb3,5f15d8723167..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -2082,12 -2437,8 +2078,17 @@@ extern void free_bootmem_with_active_re
  						unsigned long max_low_pfn);
  extern void sparse_memory_present_with_active_regions(int nid);
  
++<<<<<<< HEAD
 +#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
 +
 +#if !defined(CONFIG_HAVE_MEMBLOCK_NODE_MAP) && \
 +    !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)
 +static inline int __early_pfn_to_nid(unsigned long pfn,
 +					struct mminit_pfnnid_cache *state)
++=======
+ #ifndef CONFIG_NEED_MULTIPLE_NODES
+ static inline int early_pfn_to_nid(unsigned long pfn)
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  {
  	return 0;
  }
diff --cc mm/page_alloc.c
index f9684b405897,430e35384b78..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -300,11 -347,10 +299,10 @@@ static bool mirrored_kernelcore __memin
  /* movable_zone is the "real" zone pages in ZONE_MOVABLE are taken from */
  int movable_zone;
  EXPORT_SYMBOL(movable_zone);
- #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
  
  #if MAX_NUMNODES > 1
 -unsigned int nr_node_ids __read_mostly = MAX_NUMNODES;
 -unsigned int nr_online_nodes __read_mostly = 1;
 +int nr_node_ids __read_mostly = MAX_NUMNODES;
 +int nr_online_nodes __read_mostly = 1;
  EXPORT_SYMBOL(nr_node_ids);
  EXPORT_SYMBOL(nr_online_nodes);
  #endif
@@@ -6699,8 -6964,19 +6711,24 @@@ static void __ref alloc_node_mem_map(st
  static void __ref alloc_node_mem_map(struct pglist_data *pgdat) { }
  #endif /* CONFIG_FLAT_NODE_MEM_MAP */
  
++<<<<<<< HEAD
 +void __paginginit free_area_init_node(int nid, unsigned long *zones_size,
 +		unsigned long node_start_pfn, unsigned long *zholes_size)
++=======
+ #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
+ static inline void pgdat_set_deferred_range(pg_data_t *pgdat)
+ {
+ 	pgdat->first_deferred_pfn = ULONG_MAX;
+ }
+ #else
+ static inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}
+ #endif
+ 
+ static void __init __free_area_init_node(int nid, unsigned long *zones_size,
+ 					 unsigned long node_start_pfn,
+ 					 unsigned long *zholes_size,
+ 					 bool compat)
++>>>>>>> 3f08a302f533 (mm: remove CONFIG_HAVE_MEMBLOCK_NODE_MAP option)
  {
  	pg_data_t *pgdat = NODE_DATA(nid);
  	unsigned long start_pfn = 0;
@@@ -6712,28 -6988,20 +6740,28 @@@
  	pgdat->node_id = nid;
  	pgdat->node_start_pfn = node_start_pfn;
  	pgdat->per_cpu_nodestats = NULL;
- #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
- 	get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
- 	pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,
- 		(u64)start_pfn << PAGE_SHIFT,
- 		end_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);
- #else
- 	start_pfn = node_start_pfn;
- #endif
+ 	if (!compat) {
+ 		get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
+ 		pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,
+ 			(u64)start_pfn << PAGE_SHIFT,
+ 			end_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0);
+ 	} else {
+ 		start_pfn = node_start_pfn;
+ 	}
  	calculate_node_totalpages(pgdat, start_pfn, end_pfn,
- 				  zones_size, zholes_size);
+ 				  zones_size, zholes_size, compat);
  
  	alloc_node_mem_map(pgdat);
 -	pgdat_set_deferred_range(pgdat);
  
 +#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
 +	/*
 +	 * We start only with one section of pages, more pages are added as
 +	 * needed until the rest of deferred pages are initialized.
 +	 */
 +	pgdat->static_init_pgcnt = min_t(unsigned long, PAGES_PER_SECTION,
 +					 pgdat->node_spanned_pages);
 +	pgdat->first_deferred_pfn = ULONG_MAX;
 +#endif
  	free_area_init_core(pgdat);
  }
  
@@@ -7319,18 -7590,14 +7353,16 @@@ static int __init cmdline_parse_movable
  early_param("kernelcore", cmdline_parse_kernelcore);
  early_param("movablecore", cmdline_parse_movablecore);
  
- #endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
- 
  void adjust_managed_page_count(struct page *page, long count)
  {
 -	atomic_long_add(count, &page_zone(page)->managed_pages);
 -	totalram_pages_add(count);
 +	spin_lock(&managed_page_count_lock);
 +	page_zone(page)->managed_pages += count;
 +	totalram_pages += count;
  #ifdef CONFIG_HIGHMEM
  	if (PageHighMem(page))
 -		totalhigh_pages_add(count);
 +		totalhigh_pages += count;
  #endif
 +	spin_unlock(&managed_page_count_lock);
  }
  EXPORT_SYMBOL(adjust_managed_page_count);
  
* Unmerged path Documentation/features/vm/numa-memblock/arch-support.txt
* Unmerged path Documentation/features/vm/numa-memblock/arch-support.txt
* Unmerged path arch/alpha/mm/numa.c
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index d3f3af6e3b68..90640a58b20e 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -140,7 +140,6 @@ config ARM64
 	select HAVE_GENERIC_DMA_COHERENT
 	select HAVE_HW_BREAKPOINT if PERF_EVENTS
 	select HAVE_IRQ_TIME_ACCOUNTING
-	select HAVE_MEMBLOCK_NODE_MAP if NUMA
 	select HAVE_NMI
 	select HAVE_PATA_PLATFORM
 	select HAVE_PERF_EVENTS
diff --git a/arch/ia64/Kconfig b/arch/ia64/Kconfig
index d6f777303de9..59b237dd65cd 100644
--- a/arch/ia64/Kconfig
+++ b/arch/ia64/Kconfig
@@ -30,7 +30,6 @@ config IA64
 	select HAVE_FUNCTION_TRACER
 	select TTY
 	select HAVE_ARCH_TRACEHOOK
-	select HAVE_MEMBLOCK_NODE_MAP
 	select HAVE_VIRT_CPU_ACCOUNTING
 	select ARCH_HAS_DMA_MARK_CLEAN
 	select ARCH_HAS_SG_CHAIN
* Unmerged path arch/m68k/mm/motorola.c
diff --git a/arch/microblaze/Kconfig b/arch/microblaze/Kconfig
index 4bf3ba3244df..9b91f5ca08fa 100644
--- a/arch/microblaze/Kconfig
+++ b/arch/microblaze/Kconfig
@@ -23,7 +23,6 @@ config MICROBLAZE
 	select HAVE_FTRACE_MCOUNT_RECORD
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_FUNCTION_TRACER
-	select HAVE_MEMBLOCK_NODE_MAP
 	select HAVE_OPROFILE
 	select IRQ_DOMAIN
 	select XILINX_INTC
* Unmerged path arch/mips/Kconfig
diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig
index f95f7924e00e..dc10b3e235d9 100644
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@ -198,7 +198,6 @@ config PPC
 	select HAVE_KRETPROBES
 	select HAVE_LD_DEAD_CODE_DATA_ELIMINATION
 	select HAVE_LIVEPATCH			if HAVE_DYNAMIC_FTRACE_WITH_REGS
-	select HAVE_MEMBLOCK_NODE_MAP
 	select HAVE_MOD_ARCH_SPECIFIC
 	select HAVE_NMI				if PERF_EVENTS || (PPC64 && PPC_BOOK3S)
 	select HAVE_HARDLOCKUP_DETECTOR_ARCH	if (PPC64 && PPC_BOOK3S)
* Unmerged path arch/riscv/Kconfig
diff --git a/arch/s390/Kconfig b/arch/s390/Kconfig
index 247a8ecc8c7b..25342a2b8565 100644
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@ -157,7 +157,6 @@ config S390
 	select HAVE_LIVEPATCH
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
-	select HAVE_MEMBLOCK_NODE_MAP
 	select HAVE_MEMBLOCK_PHYS_MAP
 	select HAVE_MOD_ARCH_SPECIFIC
 	select HAVE_OPROFILE
* Unmerged path arch/sh/Kconfig
* Unmerged path arch/sparc/Kconfig
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 45bca1b0716d..5456cd4752b6 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -182,7 +182,6 @@ config X86
 	select HAVE_KRETPROBES
 	select HAVE_KVM
 	select HAVE_LIVEPATCH			if X86_64
-	select HAVE_MEMBLOCK_NODE_MAP
 	select HAVE_MIXED_BREAKPOINTS_REGS
 	select HAVE_MOD_ARCH_SPECIFIC
 	select HAVE_NMI
diff --git a/include/linux/memblock.h b/include/linux/memblock.h
index 8577915e5aea..9605b797cf90 100644
--- a/include/linux/memblock.h
+++ b/include/linux/memblock.h
@@ -57,7 +57,7 @@ struct memblock_region {
 	phys_addr_t base;
 	phys_addr_t size;
 	enum memblock_flags flags;
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+#ifdef CONFIG_NEED_MULTIPLE_NODES
 	int nid;
 #endif
 };
@@ -227,7 +227,6 @@ static inline bool memblock_is_nomap(struct memblock_region *m)
 	return m->flags & MEMBLOCK_NOMAP;
 }
 
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 int memblock_search_pfn_nid(unsigned long pfn, unsigned long *start_pfn,
 			    unsigned long  *end_pfn);
 void __next_mem_pfn_range(int *idx, int nid, unsigned long *out_start_pfn,
@@ -246,7 +245,6 @@ void __next_mem_pfn_range(int *idx, int nid, unsigned long *out_start_pfn,
 #define for_each_mem_pfn_range(i, nid, p_start, p_end, p_nid)		\
 	for (i = -1, __next_mem_pfn_range(&i, nid, p_start, p_end, p_nid); \
 	     i >= 0; __next_mem_pfn_range(&i, nid, p_start, p_end, p_nid))
-#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
 
 #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
 void __next_mem_pfn_range_in_zone(u64 *idx, struct zone *zone,
@@ -325,10 +323,10 @@ int __init deferred_page_init_max_threads(const struct cpumask *node_cpumask);
 	for_each_mem_range_rev(i, &memblock.memory, &memblock.reserved,	\
 			       nid, flags, p_start, p_end, p_nid)
 
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 int memblock_set_node(phys_addr_t base, phys_addr_t size,
 		      struct memblock_type *type, int nid);
 
+#ifdef CONFIG_NEED_MULTIPLE_NODES
 static inline void memblock_set_region_node(struct memblock_region *r, int nid)
 {
 	r->nid = nid;
@@ -347,7 +345,7 @@ static inline int memblock_get_region_node(const struct memblock_region *r)
 {
 	return 0;
 }
-#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
+#endif /* CONFIG_NEED_MULTIPLE_NODES */
 
 /* Flags for memblock allocation APIs */
 #define MEMBLOCK_ALLOC_ANYWHERE	(~(phys_addr_t)0)
* Unmerged path include/linux/mm.h
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index ce2990f782e1..fadb1ff57164 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -939,7 +939,7 @@ extern int movable_zone;
 #ifdef CONFIG_HIGHMEM
 static inline int zone_movable_is_highmem(void)
 {
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+#ifdef CONFIG_NEED_MULTIPLE_NODES
 	return movable_zone == ZONE_HIGHMEM;
 #else
 	return (ZONE_MOVABLE - 1) == ZONE_HIGHMEM;
diff --git a/mm/Kconfig b/mm/Kconfig
index ba2bdef1230d..fd7b0ddaf399 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -124,9 +124,6 @@ config SPARSEMEM_VMEMMAP
 	 pfn_to_page and page_to_pfn operations.  This is the most
 	 efficient option when sufficient kernel resources are available.
 
-config HAVE_MEMBLOCK_NODE_MAP
-	bool
-
 config HAVE_MEMBLOCK_PHYS_MAP
 	bool
 
diff --git a/mm/memblock.c b/mm/memblock.c
index a30c86c866c5..e5bc1641649d 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -620,7 +620,7 @@ int __init_memblock memblock_add_range(struct memblock_type *type,
 		 * area, insert that portion.
 		 */
 		if (rbase > base) {
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+#ifdef CONFIG_NEED_MULTIPLE_NODES
 			WARN_ON(nid != memblock_get_region_node(rgn));
 #endif
 			WARN_ON(flags != rgn->flags);
@@ -1181,7 +1181,6 @@ void __init_memblock __next_mem_range_rev(u64 *idx, int nid,
 	*idx = ULLONG_MAX;
 }
 
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 /*
  * Common iterator interface used to define for_each_mem_pfn_range().
  */
@@ -1231,6 +1230,7 @@ void __init_memblock __next_mem_pfn_range(int *idx, int nid,
 int __init_memblock memblock_set_node(phys_addr_t base, phys_addr_t size,
 				      struct memblock_type *type, int nid)
 {
+#ifdef CONFIG_NEED_MULTIPLE_NODES
 	int start_rgn, end_rgn;
 	int i, ret;
 
@@ -1242,9 +1242,10 @@ int __init_memblock memblock_set_node(phys_addr_t base, phys_addr_t size,
 		memblock_set_region_node(&type->regions[i], nid);
 
 	memblock_merge_regions(type);
+#endif
 	return 0;
 }
-#endif /* CONFIG_HAVE_MEMBLOCK_NODE_MAP */
+
 #ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT
 /**
  * __next_mem_pfn_range_in_zone - iterator for for_each_*_range_in_zone()
@@ -1818,7 +1819,6 @@ bool __init_memblock memblock_is_map_memory(phys_addr_t addr)
 	return !memblock_is_nomap(&memblock.memory.regions[i]);
 }
 
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 int __init_memblock memblock_search_pfn_nid(unsigned long pfn,
 			 unsigned long *start_pfn, unsigned long *end_pfn)
 {
@@ -1833,7 +1833,6 @@ int __init_memblock memblock_search_pfn_nid(unsigned long pfn,
 
 	return memblock_get_region_node(&type->regions[mid]);
 }
-#endif
 
 /**
  * memblock_is_region_memory - check if a region is a subset of memory
@@ -1924,7 +1923,7 @@ static void __init_memblock memblock_dump(struct memblock_type *type)
 		size = rgn->size;
 		end = base + size - 1;
 		flags = rgn->flags;
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+#ifdef CONFIG_NEED_MULTIPLE_NODES
 		if (memblock_get_region_node(rgn) != MAX_NUMNODES)
 			snprintf(nid_buf, sizeof(nid_buf), " on node %d",
 				 memblock_get_region_node(rgn));
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index b425e9969aa2..672f824fdb2f 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -1381,11 +1381,7 @@ check_pages_isolated_cb(unsigned long start_pfn, unsigned long nr_pages,
 
 static int __init cmdline_parse_movable_node(char *p)
 {
-#ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 	movable_node_enabled = true;
-#else
-	pr_warn("movable_node parameter depends on CONFIG_HAVE_MEMBLOCK_NODE_MAP to work properly\n");
-#endif
 	return 0;
 }
 early_param("movable_node", cmdline_parse_movable_node);
* Unmerged path mm/page_alloc.c
