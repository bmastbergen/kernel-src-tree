x86/cpu: Unify cpu_init()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
Rebuild_CHGLOG: - [x86] cpu: Unify cpu_init() (Vitaly Kuznetsov) [1868080]
Rebuild_FUZZ: 91.30%
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 505b789996f64bdbfcc5847dd4b5076fc7c50274
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/505b7899.failed

Similar to copy_thread_tls() the 32bit and 64bit implementations of
cpu_init() are very similar and unification avoids duplicate changes in the
future.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Andy Lutomirski <luto@kernel.org>

(cherry picked from commit 505b789996f64bdbfcc5847dd4b5076fc7c50274)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/common.c
diff --cc arch/x86/kernel/cpu/common.c
index d6857fdc914b,d52ec1a82120..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -52,11 -53,7 +52,13 @@@
  #include <asm/microcode_intel.h>
  #include <asm/intel-family.h>
  #include <asm/cpu_device_id.h>
++<<<<<<< HEAD
 +#include <asm/spec_ctrl.h>
 +
 +#ifdef CONFIG_X86_LOCAL_APIC
++=======
++>>>>>>> 505b789996f6 (x86/cpu: Unify cpu_init())
  #include <asm/uv/uv.h>
- #endif
  
  #include "cpu.h"
  
@@@ -1753,12 -1746,12 +1755,12 @@@ static void wait_for_master_cpu(int cpu
  }
  
  #ifdef CONFIG_X86_64
- static void setup_getcpu(int cpu)
+ static inline void setup_getcpu(int cpu)
  {
 -	unsigned long cpudata = vdso_encode_cpunode(cpu, early_cpu_to_node(cpu));
 +	unsigned long cpudata = vdso_encode_cpu_node(cpu, early_cpu_to_node(cpu));
  	struct desc_struct d = { };
  
 -	if (boot_cpu_has(X86_FEATURE_RDTSCP))
 +	if (static_cpu_has(X86_FEATURE_RDTSCP))
  		write_rdtscp_aux(cpudata);
  
  	/* Store CPU and node number in limit. */
@@@ -1771,41 -1764,61 +1773,85 @@@
  	d.p = 1;		/* Present */
  	d.d = 1;		/* 32-bit */
  
 -	write_gdt_entry(get_cpu_gdt_rw(cpu), GDT_ENTRY_CPUNODE, &d, DESCTYPE_S);
 +	write_gdt_entry(get_cpu_gdt_rw(cpu), GDT_ENTRY_CPU_NUMBER, &d, DESCTYPE_S);
  }
+ 
+ static inline void ucode_cpu_init(int cpu)
+ {
+ 	if (cpu)
+ 		load_ucode_ap();
+ }
+ 
+ static inline void tss_setup_ist(struct tss_struct *tss)
+ {
+ 	/* Set up the per-CPU TSS IST stacks */
+ 	tss->x86_tss.ist[IST_INDEX_DF] = __this_cpu_ist_top_va(DF);
+ 	tss->x86_tss.ist[IST_INDEX_NMI] = __this_cpu_ist_top_va(NMI);
+ 	tss->x86_tss.ist[IST_INDEX_DB] = __this_cpu_ist_top_va(DB);
+ 	tss->x86_tss.ist[IST_INDEX_MCE] = __this_cpu_ist_top_va(MCE);
+ }
+ 
+ static inline void gdt_setup_doublefault_tss(int cpu) { }
+ 
+ #else /* CONFIG_X86_64 */
+ 
+ static inline void setup_getcpu(int cpu) { }
+ 
+ static inline void ucode_cpu_init(int cpu)
+ {
+ 	show_ucode_info_early();
+ }
+ 
+ static inline void tss_setup_ist(struct tss_struct *tss) { }
+ 
+ static inline void gdt_setup_doublefault_tss(int cpu)
+ {
+ #ifdef CONFIG_DOUBLEFAULT
+ 	/* Set up the doublefault TSS pointer in the GDT */
+ 	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
  #endif
+ }
+ #endif /* !CONFIG_X86_64 */
  
  /*
   * cpu_init() initializes state that is per-CPU. Some data is already
   * initialized (naturally) in the bootstrap process, such as the GDT
   * and IDT. We reload them nevertheless, this function acts as a
   * 'CPU state barrier', nothing should get across.
 + * A lot of state is already set up in PDA init for 64 bit
   */
- #ifdef CONFIG_X86_64
- 
  void cpu_init(void)
  {
++<<<<<<< HEAD
 +	struct orig_ist *oist;
 +	struct task_struct *me;
 +	struct tss_struct *t;
 +	unsigned long v;
 +	int cpu = raw_smp_processor_id();
 +	int i;
 +
 +	wait_for_master_cpu(cpu);
 +
 +	/*
 +	 * Initialize the CR4 shadow before doing anything that could
 +	 * try to read it.
 +	 */
 +	cr4_init_shadow();
 +
 +	if (cpu)
 +		load_ucode_ap();
 +
 +	t = &per_cpu(cpu_tss_rw, cpu);
 +	oist = &per_cpu(orig_ist, cpu);
++=======
+ 	struct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);
+ 	struct task_struct *cur = current;
+ 	int cpu = raw_smp_processor_id();
+ 
+ 	wait_for_master_cpu(cpu);
+ 
+ 	ucode_cpu_init(cpu);
++>>>>>>> 505b789996f6 (x86/cpu: Unify cpu_init())
  
  #ifdef CONFIG_NUMA
  	if (this_cpu_read(numa_node) == 0 &&
@@@ -1824,58 -1837,38 +1870,57 @@@
  	 * Initialize the per-CPU GDT with the boot GDT,
  	 * and set up the GDT descriptor:
  	 */
- 
  	switch_to_new_gdt(cpu);
- 	loadsegment(fs, 0);
- 
  	load_current_idt();
  
- 	memset(me->thread.tls_array, 0, GDT_ENTRY_TLS_ENTRIES * 8);
- 	syscall_init();
+ 	if (IS_ENABLED(CONFIG_X86_64)) {
+ 		loadsegment(fs, 0);
+ 		memset(cur->thread.tls_array, 0, GDT_ENTRY_TLS_ENTRIES * 8);
+ 		syscall_init();
  
- 	wrmsrl(MSR_FS_BASE, 0);
- 	wrmsrl(MSR_KERNEL_GS_BASE, 0);
- 	barrier();
+ 		wrmsrl(MSR_FS_BASE, 0);
+ 		wrmsrl(MSR_KERNEL_GS_BASE, 0);
+ 		barrier();
  
++<<<<<<< HEAD
 +	x86_configure_nx();
 +	x2apic_setup();
 +
 +	/*
 +	 * set up and load the per-CPU TSS
 +	 */
 +	if (!oist->ist[0]) {
 +		char *estacks = get_cpu_entry_area(cpu)->exception_stacks;
 +
 +		for (v = 0; v < N_EXCEPTION_STACKS; v++) {
 +			estacks += exception_stack_sizes[v];
 +			oist->ist[v] = t->x86_tss.ist[v] =
 +					(unsigned long)estacks;
 +			if (v == DEBUG_STACK-1)
 +				per_cpu(debug_stack_addr, cpu) = (unsigned long)estacks;
 +		}
++=======
+ 		x2apic_setup();
++>>>>>>> 505b789996f6 (x86/cpu: Unify cpu_init())
  	}
  
- 	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
- 
- 	/*
- 	 * <= is required because the CPU will access up to
- 	 * 8 bits beyond the end of the IO permission bitmap.
- 	 */
- 	for (i = 0; i <= IO_BITMAP_LONGS; i++)
- 		t->io_bitmap[i] = ~0UL;
- 
  	mmgrab(&init_mm);
- 	me->active_mm = &init_mm;
- 	BUG_ON(me->mm);
+ 	cur->active_mm = &init_mm;
+ 	BUG_ON(cur->mm);
  	initialize_tlbstate_and_flush();
- 	enter_lazy_tlb(&init_mm, me);
+ 	enter_lazy_tlb(&init_mm, cur);
  
- 	/*
- 	 * Initialize the TSS.  sp0 points to the entry trampoline stack
- 	 * regardless of what task is running.
- 	 */
+ 	/* Initialize the TSS. */
+ 	tss_setup_ist(tss);
+ 	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
+ 	memset(tss->io_bitmap, 0xff, sizeof(tss->io_bitmap));
  	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
+ 
  	load_TR_desc();
+ 	/*
+ 	 * sp0 points to the entry trampoline stack regardless of what task
+ 	 * is running.
+ 	 */
  	load_sp0((unsigned long)(cpu_entry_stack(cpu) + 1));
  
  	load_mm_ldt(&init_mm);
@@@ -1891,86 -1886,6 +1938,89 @@@
  	load_fixmap_gdt(cpu);
  }
  
++<<<<<<< HEAD
 +#else
 +
 +void cpu_init(void)
 +{
 +	int cpu = smp_processor_id();
 +	struct task_struct *curr = current;
 +	struct tss_struct *t = &per_cpu(cpu_tss_rw, cpu);
 +
 +	wait_for_master_cpu(cpu);
 +
 +	/*
 +	 * Initialize the CR4 shadow before doing anything that could
 +	 * try to read it.
 +	 */
 +	cr4_init_shadow();
 +
 +	show_ucode_info_early();
 +
 +	pr_info("Initializing CPU#%d\n", cpu);
 +
 +	if (cpu_feature_enabled(X86_FEATURE_VME) ||
 +	    boot_cpu_has(X86_FEATURE_TSC) ||
 +	    boot_cpu_has(X86_FEATURE_DE))
 +		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 +
 +	load_current_idt();
 +	switch_to_new_gdt(cpu);
 +
 +	/*
 +	 * Set up and load the per-CPU TSS and LDT
 +	 */
 +	mmgrab(&init_mm);
 +	curr->active_mm = &init_mm;
 +	BUG_ON(curr->mm);
 +	initialize_tlbstate_and_flush();
 +	enter_lazy_tlb(&init_mm, curr);
 +
 +	/*
 +	 * Initialize the TSS.  sp0 points to the entry trampoline stack
 +	 * regardless of what task is running.
 +	 */
 +	set_tss_desc(cpu, &get_cpu_entry_area(cpu)->tss.x86_tss);
 +	load_TR_desc();
 +	load_sp0((unsigned long)(cpu_entry_stack(cpu) + 1));
 +
 +	load_mm_ldt(&init_mm);
 +
 +	t->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET;
 +
 +#ifdef CONFIG_DOUBLEFAULT
 +	/* Set up doublefault TSS pointer in the GDT */
 +	__set_tss_desc(cpu, GDT_ENTRY_DOUBLEFAULT_TSS, &doublefault_tss);
 +#endif
 +
 +	clear_all_debug_regs();
 +	dbg_restore_debug_regs();
 +
 +	fpu__init_cpu();
 +
 +	load_fixmap_gdt(cpu);
 +}
 +#endif
 +
 +static void bsp_resume(void)
 +{
 +	if (this_cpu->c_bsp_resume)
 +		this_cpu->c_bsp_resume(&boot_cpu_data);
 +}
 +
 +static struct syscore_ops cpu_syscore_ops = {
 +	.resume		= bsp_resume,
 +};
 +
 +static int __init init_cpu_syscore(void)
 +{
 +	register_syscore_ops(&cpu_syscore_ops);
 +	return 0;
 +}
 +core_initcall(init_cpu_syscore);
 +
++=======
++>>>>>>> 505b789996f6 (x86/cpu: Unify cpu_init())
  /*
   * The microcode loader calls this upon late microcode load to recheck features,
   * only when microcode has been updated. Caller holds microcode_mutex and CPU
* Unmerged path arch/x86/kernel/cpu/common.c
