mm: vmscan: move file exhaustion detection to the node level

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 53138cea7f398d2cdd0fa22adeec7e16093e1ebd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/53138cea.failed

Patch series "mm: fix page aging across multiple cgroups".

When applications are put into unconfigured cgroups for memory accounting
purposes, the cgrouping itself should not change the behavior of the page
reclaim code.  We expect the VM to reclaim the coldest pages in the
system.  But right now the VM can reclaim hot pages in one cgroup while
there is eligible cold cache in others.

This is because one part of the reclaim algorithm isn't truly cgroup
hierarchy aware: the inactive/active list balancing.  That is the part
that is supposed to protect hot cache data from one-off streaming IO.

The recursive cgroup reclaim scheme will scan and rotate the physical LRU
lists of each eligible cgroup at the same rate in a round-robin fashion,
thereby establishing a relative order among the pages of all those
cgroups.  However, the inactive/active balancing decisions are made
locally within each cgroup, so when a cgroup is running low on cold pages,
its hot pages will get reclaimed - even when sibling cgroups have plenty
of cold cache eligible in the same reclaim run.

For example:

   [root@ham ~]# head -n1 /proc/meminfo
   MemTotal:        1016336 kB

   [root@ham ~]# ./reclaimtest2.sh
   Establishing 50M active files in cgroup A...
   Hot pages cached: 12800/12800 workingset-a
   Linearly scanning through 18G of file data in cgroup B:
   real    0m4.269s
   user    0m0.051s
   sys     0m4.182s
   Hot pages cached: 134/12800 workingset-a

The streaming IO in B, which doesn't benefit from caching at all, pushes
out most of the workingset in A.

Solution

This series fixes the problem by elevating inactive/active balancing
decisions to the toplevel of the reclaim run.  This is either a cgroup
that hit its limit, or straight-up global reclaim if there is physical
memory pressure.  From there, it takes a recursive view of the cgroup
subtree to decide whether page deactivation is necessary.

In the test above, the VM will then recognize that cgroup B has plenty of
eligible cold cache, and that the hot pages in A can be spared:

   [root@ham ~]# ./reclaimtest2.sh
   Establishing 50M active files in cgroup A...
   Hot pages cached: 12800/12800 workingset-a
   Linearly scanning through 18G of file data in cgroup B:
   real    0m4.244s
   user    0m0.064s
   sys     0m4.177s
   Hot pages cached: 12800/12800 workingset-a

Implementation

Whether active pages can be deactivated or not is influenced by two
factors: the inactive list dropping below a minimum size relative to the
active list, and the occurence of refaults.

This patch series first moves refault detection to the reclaim root, then
enforces the minimum inactive size based on a recursive view of the cgroup
tree's LRUs.

History

Note that this actually never worked correctly in Linux cgroups.  In the
past it worked for global reclaim and leaf limit reclaim only (we used to
have two physical LRU linkages per page), but it never worked for
intermediate limit reclaim over multiple leaf cgroups.

We're noticing this now because 1) we're putting everything into cgroups
for accounting, not just the things we want to control and 2) we're moving
away from leaf limits that invoke reclaim on individual cgroups, toward
large tree reclaim, triggered by high-level limits, or physical memory
pressure that is influenced by local protections such as memory.low and
memory.min instead.

This patch (of 3):

When file pages are lower than the watermark on a node, we try to force
scan anonymous pages to counter-act the balancing algorithms preference
for new file pages when they are likely thrashing.  This is a node-level
decision, but it's currently made each time we look at an lruvec.  This is
unnecessarily expensive and also a layering violation that makes the code
harder to understand.

Clean this up by making the check once per node and setting a flag in the
scan_control.

Link: http://lkml.kernel.org/r/20191107205334.158354-2-hannes@cmpxchg.org
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Reviewed-by: Shakeel Butt <shakeelb@google.com>
	Reviewed-by: Suren Baghdasaryan <surenb@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Rik van Riel <riel@surriel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 53138cea7f398d2cdd0fa22adeec7e16093e1ebd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmscan.c
diff --cc mm/vmscan.c
index fdc405f00cf6,725b5d4784f7..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -2356,45 -2292,16 +2359,50 @@@ static void get_scan_count(struct lruve
  	}
  
  	/*
- 	 * Prevent the reclaimer from falling into the cache trap: as
- 	 * cache pages start out inactive, every cache fault will tip
- 	 * the scan balance towards the file LRU.  And as the file LRU
- 	 * shrinks, so does the window for rotation from references.
- 	 * This means we have a runaway feedback loop where a tiny
- 	 * thrashing file LRU becomes infinitely more attractive than
- 	 * anon pages.  Try to detect this based on file LRU size.
+ 	 * If the system is almost out of file pages, force-scan anon.
+ 	 * But only if there are enough inactive anonymous pages on
+ 	 * the LRU. Otherwise, the small LRU gets thrashed.
  	 */
++<<<<<<< HEAD
 +	if (global_reclaim(sc)) {
 +		unsigned long pgdatfile;
 +		unsigned long pgdatfree;
 +		int z;
 +		unsigned long total_high_wmark = 0;
 +
 +		pgdatfree = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
 +		pgdatfile = node_page_state(pgdat, NR_ACTIVE_FILE) +
 +			   node_page_state(pgdat, NR_INACTIVE_FILE);
 +
 +		for (z = 0; z < MAX_NR_ZONES; z++) {
 +			struct zone *zone = &pgdat->node_zones[z];
 +			if (!managed_zone(zone))
 +				continue;
 +
 +			total_high_wmark += high_wmark_pages(zone);
 +		}
 +
 +		if (unlikely(pgdatfile + pgdatfree <= total_high_wmark)) {
 +			/*
 +			 * Force SCAN_ANON if there are enough inactive
 +			 * anonymous pages on the LRU in eligible zones.
 +			 * Otherwise, the small LRU gets thrashed.
 +			 */
 +			if (!inactive_list_is_low(lruvec, false, sc, false) &&
 +			    lruvec_lru_size(lruvec, LRU_INACTIVE_ANON, sc->reclaim_idx)
 +					>> sc->priority) {
 +				scan_balance = SCAN_ANON;
 +				goto out;
 +			}
 +		}
++=======
+ 	if (sc->file_is_tiny &&
+ 	    !inactive_list_is_low(lruvec, false, sc, false) &&
+ 	    lruvec_lru_size(lruvec, LRU_INACTIVE_ANON,
+ 			    sc->reclaim_idx) >> sc->priority) {
+ 		scan_balance = SCAN_ANON;
+ 		goto out;
++>>>>>>> 53138cea7f39 (mm: vmscan: move file exhaustion detection to the node level)
  	}
  
  	/*
@@@ -2766,143 -2717,124 +2774,177 @@@ static bool shrink_node(pg_data_t *pgda
  {
  	struct reclaim_state *reclaim_state = current->reclaim_state;
  	unsigned long nr_reclaimed, nr_scanned;
 -	struct lruvec *target_lruvec;
  	bool reclaimable = false;
  
 -	target_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);
 +	do {
 +		struct mem_cgroup *root = sc->target_mem_cgroup;
 +		struct mem_cgroup *memcg;
  
 -again:
 -	memset(&sc->nr, 0, sizeof(sc->nr));
 +		memset(&sc->nr, 0, sizeof(sc->nr));
  
 -	nr_reclaimed = sc->nr_reclaimed;
 -	nr_scanned = sc->nr_scanned;
 +		nr_reclaimed = sc->nr_reclaimed;
 +		nr_scanned = sc->nr_scanned;
  
++<<<<<<< HEAD
 +		memcg = mem_cgroup_iter(root, NULL, NULL);
 +		do {
 +			unsigned long reclaimed;
 +			unsigned long scanned;
++=======
+ 	/*
+ 	 * Prevent the reclaimer from falling into the cache trap: as
+ 	 * cache pages start out inactive, every cache fault will tip
+ 	 * the scan balance towards the file LRU.  And as the file LRU
+ 	 * shrinks, so does the window for rotation from references.
+ 	 * This means we have a runaway feedback loop where a tiny
+ 	 * thrashing file LRU becomes infinitely more attractive than
+ 	 * anon pages.  Try to detect this based on file LRU size.
+ 	 */
+ 	if (!cgroup_reclaim(sc)) {
+ 		unsigned long file;
+ 		unsigned long free;
+ 		int z;
+ 		unsigned long total_high_wmark = 0;
+ 
+ 		free = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);
+ 		file = node_page_state(pgdat, NR_ACTIVE_FILE) +
+ 			   node_page_state(pgdat, NR_INACTIVE_FILE);
+ 
+ 		for (z = 0; z < MAX_NR_ZONES; z++) {
+ 			struct zone *zone = &pgdat->node_zones[z];
+ 			if (!managed_zone(zone))
+ 				continue;
+ 
+ 			total_high_wmark += high_wmark_pages(zone);
+ 		}
+ 
+ 		sc->file_is_tiny = file + free <= total_high_wmark;
+ 	}
+ 
+ 	shrink_node_memcgs(pgdat, sc);
++>>>>>>> 53138cea7f39 (mm: vmscan: move file exhaustion detection to the node level)
  
 -	if (reclaim_state) {
 -		sc->nr_reclaimed += reclaim_state->reclaimed_slab;
 -		reclaim_state->reclaimed_slab = 0;
 -	}
 +			switch (mem_cgroup_protected(root, memcg)) {
 +			case MEMCG_PROT_MIN:
 +				/*
 +				 * Hard protection.
 +				 * If there is no reclaimable memory, OOM.
 +				 */
 +				continue;
 +			case MEMCG_PROT_LOW:
 +				/*
 +				 * Soft protection.
 +				 * Respect the protection only as long as
 +				 * there is an unprotected supply
 +				 * of reclaimable memory from other cgroups.
 +				 */
 +				if (!sc->memcg_low_reclaim) {
 +					sc->memcg_low_skipped = 1;
 +					continue;
 +				}
 +				memcg_memory_event(memcg, MEMCG_LOW);
 +				break;
 +			case MEMCG_PROT_NONE:
 +				/*
 +				 * All protection thresholds breached. We may
 +				 * still choose to vary the scan pressure
 +				 * applied based on by how much the cgroup in
 +				 * question has exceeded its protection
 +				 * thresholds (see get_scan_count).
 +				 */
 +				break;
 +			}
  
 -	/* Record the subtree's reclaim efficiency */
 -	vmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,
 -		   sc->nr_scanned - nr_scanned,
 -		   sc->nr_reclaimed - nr_reclaimed);
 +			reclaimed = sc->nr_reclaimed;
 +			scanned = sc->nr_scanned;
 +			shrink_node_memcg(pgdat, memcg, sc);
  
 -	if (sc->nr_reclaimed - nr_reclaimed)
 -		reclaimable = true;
 +			shrink_slab(sc->gfp_mask, pgdat->node_id, memcg,
 +					sc->priority);
  
 -	if (current_is_kswapd()) {
 -		/*
 -		 * If reclaim is isolating dirty pages under writeback,
 -		 * it implies that the long-lived page allocation rate
 -		 * is exceeding the page laundering rate. Either the
 -		 * global limits are not being effective at throttling
 -		 * processes due to the page distribution throughout
 -		 * zones or there is heavy usage of a slow backing
 -		 * device. The only option is to throttle from reclaim
 -		 * context which is not ideal as there is no guarantee
 -		 * the dirtying process is throttled in the same way
 -		 * balance_dirty_pages() manages.
 -		 *
 -		 * Once a node is flagged PGDAT_WRITEBACK, kswapd will
 -		 * count the number of pages under pages flagged for
 -		 * immediate reclaim and stall if any are encountered
 -		 * in the nr_immediate check below.
 -		 */
 -		if (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)
 -			set_bit(PGDAT_WRITEBACK, &pgdat->flags);
 +			/* Record the group's reclaim efficiency */
 +			vmpressure(sc->gfp_mask, memcg, false,
 +				   sc->nr_scanned - scanned,
 +				   sc->nr_reclaimed - reclaimed);
 +
 +		} while ((memcg = mem_cgroup_iter(root, memcg, NULL)));
 +
 +		if (reclaim_state) {
 +			sc->nr_reclaimed += reclaim_state->reclaimed_slab;
 +			reclaim_state->reclaimed_slab = 0;
 +		}
  
 -		/* Allow kswapd to start writing pages during reclaim.*/
 -		if (sc->nr.unqueued_dirty == sc->nr.file_taken)
 -			set_bit(PGDAT_DIRTY, &pgdat->flags);
 +		/* Record the subtree's reclaim efficiency */
 +		vmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,
 +			   sc->nr_scanned - nr_scanned,
 +			   sc->nr_reclaimed - nr_reclaimed);
 +
 +		if (sc->nr_reclaimed - nr_reclaimed)
 +			reclaimable = true;
 +
 +		if (current_is_kswapd()) {
 +			/*
 +			 * If reclaim is isolating dirty pages under writeback,
 +			 * it implies that the long-lived page allocation rate
 +			 * is exceeding the page laundering rate. Either the
 +			 * global limits are not being effective at throttling
 +			 * processes due to the page distribution throughout
 +			 * zones or there is heavy usage of a slow backing
 +			 * device. The only option is to throttle from reclaim
 +			 * context which is not ideal as there is no guarantee
 +			 * the dirtying process is throttled in the same way
 +			 * balance_dirty_pages() manages.
 +			 *
 +			 * Once a node is flagged PGDAT_WRITEBACK, kswapd will
 +			 * count the number of pages under pages flagged for
 +			 * immediate reclaim and stall if any are encountered
 +			 * in the nr_immediate check below.
 +			 */
 +			if (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)
 +				set_bit(PGDAT_WRITEBACK, &pgdat->flags);
 +
 +			/*
 +			 * Tag a node as congested if all the dirty pages
 +			 * scanned were backed by a congested BDI and
 +			 * wait_iff_congested will stall.
 +			 */
 +			if (sc->nr.dirty && sc->nr.dirty == sc->nr.congested)
 +				set_bit(PGDAT_CONGESTED, &pgdat->flags);
 +
 +			/* Allow kswapd to start writing pages during reclaim.*/
 +			if (sc->nr.unqueued_dirty == sc->nr.file_taken)
 +				set_bit(PGDAT_DIRTY, &pgdat->flags);
 +
 +			/*
 +			 * If kswapd scans pages marked marked for immediate
 +			 * reclaim and under writeback (nr_immediate), it
 +			 * implies that pages are cycling through the LRU
 +			 * faster than they are written so also forcibly stall.
 +			 */
 +			if (sc->nr.immediate)
 +				congestion_wait(BLK_RW_ASYNC, HZ/10);
 +		}
  
  		/*
 -		 * If kswapd scans pages marked marked for immediate
 -		 * reclaim and under writeback (nr_immediate), it
 -		 * implies that pages are cycling through the LRU
 -		 * faster than they are written so also forcibly stall.
 +		 * Legacy memcg will stall in page writeback so avoid forcibly
 +		 * stalling in wait_iff_congested().
  		 */
 -		if (sc->nr.immediate)
 -			congestion_wait(BLK_RW_ASYNC, HZ/10);
 -	}
 -
 -	/*
 -	 * Tag a node/memcg as congested if all the dirty pages
 -	 * scanned were backed by a congested BDI and
 -	 * wait_iff_congested will stall.
 -	 *
 -	 * Legacy memcg will stall in page writeback so avoid forcibly
 -	 * stalling in wait_iff_congested().
 -	 */
 -	if ((current_is_kswapd() ||
 -	     (cgroup_reclaim(sc) && writeback_throttling_sane(sc))) &&
 -	    sc->nr.dirty && sc->nr.dirty == sc->nr.congested)
 -		set_bit(LRUVEC_CONGESTED, &target_lruvec->flags);
 +		if (!global_reclaim(sc) && sane_reclaim(sc) &&
 +		    sc->nr.dirty && sc->nr.dirty == sc->nr.congested)
 +			set_memcg_congestion(pgdat, root, true);
  
 -	/*
 -	 * Stall direct reclaim for IO completions if underlying BDIs
 -	 * and node is congested. Allow kswapd to continue until it
 -	 * starts encountering unqueued dirty pages or cycling through
 -	 * the LRU too quickly.
 -	 */
 -	if (!current_is_kswapd() && current_may_throttle() &&
 -	    !sc->hibernation_mode &&
 -	    test_bit(LRUVEC_CONGESTED, &target_lruvec->flags))
 -		wait_iff_congested(BLK_RW_ASYNC, HZ/10);
 +		/*
 +		 * Stall direct reclaim for IO completions if underlying BDIs
 +		 * and node is congested. Allow kswapd to continue until it
 +		 * starts encountering unqueued dirty pages or cycling through
 +		 * the LRU too quickly.
 +		 */
 +		if (!sc->hibernation_mode && !current_is_kswapd() &&
 +		   current_may_throttle() && pgdat_memcg_congested(pgdat, root))
 +			wait_iff_congested(BLK_RW_ASYNC, HZ/10);
  
 -	if (should_continue_reclaim(pgdat, sc->nr_reclaimed - nr_reclaimed,
 -				    sc))
 -		goto again;
 +	} while (should_continue_reclaim(pgdat, sc->nr_reclaimed - nr_reclaimed,
 +					 sc));
  
  	/*
  	 * Kswapd gives up on balancing particular nodes after too
* Unmerged path mm/vmscan.c
