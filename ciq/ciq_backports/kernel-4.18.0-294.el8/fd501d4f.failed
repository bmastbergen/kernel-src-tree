x86/entry: Remove DBn stacks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
Rebuild_CHGLOG: - [x86] entry: Remove DBn stacks (Vitaly Kuznetsov) [1868080]
Rebuild_FUZZ: 92.31%
commit-author Peter Zijlstra <peterz@infradead.org>
commit fd501d4f0399700011acde486576c7c1eb8e7a61
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/fd501d4f.failed

Both #DB itself, as all other IST users (NMI, #MC) now clear DR7 on
entry. Combined with not allowing breakpoints on entry/noinstr/NOKPROBE
text and no single step (EFLAGS.TF) inside the #DB handler should guarantee
no nested #DB.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20200529213321.303027161@infradead.org



(cherry picked from commit fd501d4f0399700011acde486576c7c1eb8e7a61)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/entry_64.S
#	arch/x86/include/asm/cpu_entry_area.h
#	arch/x86/kernel/asm-offsets_64.c
#	arch/x86/kernel/dumpstack_64.c
#	arch/x86/mm/cpu_entry_area.c
diff --cc arch/x86/entry/entry_64.S
index 955c9ec809bc,8ecaeee53653..000000000000
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@@ -401,230 -313,177 +401,291 @@@ END(spurious_entries_start
  #endif
  .endm
  
 -/**
 - * idtentry_body - Macro to emit code calling the C function
 - * @cfunc:		C function to be called
 - * @has_error_code:	Hardware pushed error code on stack
 +/*
 + * Enters the IRQ stack if we're not already using it.  NMI-safe.  Clobbers
 + * flags and puts old RSP into old_rsp, and leaves all other GPRs alone.
 + * Requires kernel GSBASE.
 + *
 + * The invariant is that, if irq_count != -1, then the IRQ stack is in use.
   */
 -.macro idtentry_body cfunc has_error_code:req
 +.macro ENTER_IRQ_STACK regs=1 old_rsp save_ret=0
 +	DEBUG_ENTRY_ASSERT_IRQS_OFF
  
 -	call	error_entry
 -	UNWIND_HINT_REGS
 +	.if \save_ret
 +	/*
 +	 * If save_ret is set, the original stack contains one additional
 +	 * entry -- the return address. Therefore, move the address one
 +	 * entry below %rsp to \old_rsp.
 +	 */
 +	leaq	8(%rsp), \old_rsp
 +	.else
 +	movq	%rsp, \old_rsp
 +	.endif
 +
 +	.if \regs
 +	UNWIND_HINT_REGS base=\old_rsp
 +	.endif
 +
 +	incl	PER_CPU_VAR(irq_count)
 +	jnz	.Lirq_stack_push_old_rsp_\@
 +
 +	/*
 +	 * Right now, if we just incremented irq_count to zero, we've
 +	 * claimed the IRQ stack but we haven't switched to it yet.
 +	 *
 +	 * If anything is added that can interrupt us here without using IST,
 +	 * it must be *extremely* careful to limit its stack usage.  This
 +	 * could include kprobes and a hypothetical future IST-less #DB
 +	 * handler.
 +	 *
 +	 * The OOPS unwinder relies on the word at the top of the IRQ
 +	 * stack linking back to the previous RSP for the entire time we're
 +	 * on the IRQ stack.  For this to work reliably, we need to write
 +	 * it before we actually move ourselves to the IRQ stack.
 +	 */
 +
 +	movq	\old_rsp, PER_CPU_VAR(irq_stack_backing_store + IRQ_STACK_SIZE - 8)
 +	movq	PER_CPU_VAR(hardirq_stack_ptr), %rsp
 +
 +#ifdef CONFIG_DEBUG_ENTRY
 +	/*
 +	 * If the first movq above becomes wrong due to IRQ stack layout
 +	 * changes, the only way we'll notice is if we try to unwind right
 +	 * here.  Assert that we set up the stack right to catch this type
 +	 * of bug quickly.
 +	 */
 +	cmpq	-8(%rsp), \old_rsp
 +	je	.Lirq_stack_okay\@
 +	ud2
 +	.Lirq_stack_okay\@:
 +#endif
  
 -	movq	%rsp, %rdi			/* pt_regs pointer into 1st argument*/
 +.Lirq_stack_push_old_rsp_\@:
 +	pushq	\old_rsp
  
 -	.if \has_error_code == 1
 -		movq	ORIG_RAX(%rsp), %rsi	/* get error code into 2nd argument*/
 -		movq	$-1, ORIG_RAX(%rsp)	/* no syscall to restart */
 +	.if \regs
 +	UNWIND_HINT_REGS indirect=1
  	.endif
  
 -	call	\cfunc
 +	.if \save_ret
 +	/*
 +	 * Push the return address to the stack. This return address can
 +	 * be found at the "real" original RSP, which was offset by 8 at
 +	 * the beginning of this macro.
 +	 */
 +	pushq	-8(\old_rsp)
 +	.endif
 +.endm
 +
 +/*
 + * Undoes ENTER_IRQ_STACK.
 + */
 +.macro LEAVE_IRQ_STACK regs=1
 +	DEBUG_ENTRY_ASSERT_IRQS_OFF
 +	/* We need to be off the IRQ stack before decrementing irq_count. */
 +	popq	%rsp
 +
 +	.if \regs
 +	UNWIND_HINT_REGS
 +	.endif
  
 -	jmp	error_return
 +	/*
 +	 * As in ENTER_IRQ_STACK, irq_count == 0, we are still claiming
 +	 * the irq stack but we're not on it.
 +	 */
 +
 +	decl	PER_CPU_VAR(irq_count)
  .endm
  
 -/**
 - * idtentry - Macro to generate entry stubs for simple IDT entries
 - * @vector:		Vector number
 - * @asmsym:		ASM symbol for the entry point
 - * @cfunc:		C function to be called
 - * @has_error_code:	Hardware pushed error code on stack
 +/*
 + * Interrupt entry helper function.
   *
 - * The macro emits code to set up the kernel context for straight forward
 - * and simple IDT entries. No IST stack, no paranoid entry checks.
 + * Entry runs with interrupts off. Stack layout at entry:
 + * +----------------------------------------------------+
 + * | regs->ss						|
 + * | regs->rsp						|
 + * | regs->eflags					|
 + * | regs->cs						|
 + * | regs->ip						|
 + * +----------------------------------------------------+
 + * | regs->orig_ax = ~(interrupt number)		|
 + * +----------------------------------------------------+
 + * | return address					|
 + * +----------------------------------------------------+
   */
 -.macro idtentry vector asmsym cfunc has_error_code:req
 -SYM_CODE_START(\asmsym)
 -	UNWIND_HINT_IRET_REGS offset=\has_error_code*8
 +ENTRY(interrupt_entry)
 +	UNWIND_HINT_IRET_REGS offset=16
  	ASM_CLAC
 +	cld
  
 -	.if \has_error_code == 0
 -		pushq	$-1			/* ORIG_RAX: no syscall to restart */
 -	.endif
 -
 -	.if \vector == X86_TRAP_BP
 -		/*
 -		 * If coming from kernel space, create a 6-word gap to allow the
 -		 * int3 handler to emulate a call instruction.
 -		 */
 -		testb	$3, CS-ORIG_RAX(%rsp)
 -		jnz	.Lfrom_usermode_no_gap_\@
 -		.rept	6
 -		pushq	5*8(%rsp)
 -		.endr
 -		UNWIND_HINT_IRET_REGS offset=8
 -.Lfrom_usermode_no_gap_\@:
 -	.endif
 +	testb	$3, CS-ORIG_RAX+8(%rsp)
 +	jz	1f
 +	SWAPGS
 +	FENCE_SWAPGS_USER_ENTRY
 +	/*
 +	 * Switch to the thread stack. The IRET frame and orig_ax are
 +	 * on the stack, as well as the return address. RDI..R12 are
 +	 * not (yet) on the stack and space has not (yet) been
 +	 * allocated for them.
 +	 */
 +	pushq	%rdi
  
 -	idtentry_body \cfunc \has_error_code
 +	/* Need to switch before accessing the thread stack. */
 +	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi
 +	IBRS_ENTRY
 +	movq	%rsp, %rdi
 +	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
  
 +	 /*
 +	  * We have RDI, return address, and orig_ax on the stack on
 +	  * top of the IRET frame. That means offset=24
 +	  */
 +	UNWIND_HINT_IRET_REGS base=%rdi offset=24
 +
++<<<<<<< HEAD
 +	pushq	7*8(%rdi)		/* regs->ss */
 +	pushq	6*8(%rdi)		/* regs->rsp */
 +	pushq	5*8(%rdi)		/* regs->eflags */
 +	pushq	4*8(%rdi)		/* regs->cs */
 +	pushq	3*8(%rdi)		/* regs->ip */
++=======
+ _ASM_NOKPROBE(\asmsym)
+ SYM_CODE_END(\asmsym)
+ .endm
+ 
+ /*
+  * Interrupt entry/exit.
+  *
+  + The interrupt stubs push (vector) onto the stack, which is the error_code
+  * position of idtentry exceptions, and jump to one of the two idtentry points
+  * (common/spurious).
+  *
+  * common_interrupt is a hotpath, align it to a cache line
+  */
+ .macro idtentry_irq vector cfunc
+ 	.p2align CONFIG_X86_L1_CACHE_SHIFT
+ 	idtentry \vector asm_\cfunc \cfunc has_error_code=1
+ .endm
+ 
+ /*
+  * System vectors which invoke their handlers directly and are not
+  * going through the regular common device interrupt handling code.
+  */
+ .macro idtentry_sysvec vector cfunc
+ 	idtentry \vector asm_\cfunc \cfunc has_error_code=0
+ .endm
+ 
+ /**
+  * idtentry_mce_db - Macro to generate entry stubs for #MC and #DB
+  * @vector:		Vector number
+  * @asmsym:		ASM symbol for the entry point
+  * @cfunc:		C function to be called
+  *
+  * The macro emits code to set up the kernel context for #MC and #DB
+  *
+  * If the entry comes from user space it uses the normal entry path
+  * including the return to user space work and preemption checks on
+  * exit.
+  *
+  * If hits in kernel mode then it needs to go through the paranoid
+  * entry as the exception can hit any random state. No preemption
+  * check on exit to keep the paranoid path simple.
+  */
+ .macro idtentry_mce_db vector asmsym cfunc
+ SYM_CODE_START(\asmsym)
++>>>>>>> fd501d4f0399 (x86/entry: Remove DBn stacks)
  	UNWIND_HINT_IRET_REGS
 -	ASM_CLAC
 +	pushq	2*8(%rdi)		/* regs->orig_ax */
 +	pushq	8(%rdi)			/* return address */
  
 -	pushq	$-1			/* ORIG_RAX: no syscall to restart */
 +	movq	(%rdi), %rdi
 +	jmp	2f
 +1:
 +	FENCE_SWAPGS_KERNEL_ENTRY
 +2:
 +	PUSH_AND_CLEAR_REGS save_ret=1
 +	ENCODE_FRAME_POINTER 8
  
 -	/*
 -	 * If the entry is from userspace, switch stacks and treat it as
 -	 * a normal entry.
 -	 */
 -	testb	$3, CS-ORIG_RAX(%rsp)
 -	jnz	.Lfrom_usermode_switch_stack_\@
 +	testb	$3, CS+8(%rsp)
 +	jz	1f
  
  	/*
 -	 * paranoid_entry returns SWAPGS flag for paranoid_exit in EBX.
 -	 * EBX == 0 -> SWAPGS, EBX == 1 -> no SWAPGS
 +	 * IRQ from user mode.
 +	 *
 +	 * We need to tell lockdep that IRQs are off.  We can't do this until
 +	 * we fix gsbase, and we should do it before enter_from_user_mode
 +	 * (which can take locks).  Since TRACE_IRQS_OFF is idempotent,
 +	 * the simplest way to handle it is to just call it twice if
 +	 * we enter from user mode.  There's no reason to optimize this since
 +	 * TRACE_IRQS_OFF is a no-op if lockdep is off.
  	 */
 -	call	paranoid_entry
 +	TRACE_IRQS_OFF
  
 -	UNWIND_HINT_REGS
 +	CALL_enter_from_user_mode
  
 -	movq	%rsp, %rdi		/* pt_regs pointer */
 +1:
 +	ENTER_IRQ_STACK old_rsp=%rdi save_ret=1
 +	/* We entered an interrupt context - irqs are off: */
 +	TRACE_IRQS_OFF
  
 +	ret
 +END(interrupt_entry)
 +_ASM_NOKPROBE(interrupt_entry)
 +
++<<<<<<< HEAD
 +
 +/* Interrupt entry/exit. */
++=======
+ 	call	\cfunc
+ 
+ 	jmp	paranoid_exit
+ 
+ 	/* Switch to the regular task stack and use the noist entry point */
+ .Lfrom_usermode_switch_stack_\@:
+ 	idtentry_body noist_\cfunc, has_error_code=0
+ 
+ _ASM_NOKPROBE(\asmsym)
+ SYM_CODE_END(\asmsym)
+ .endm
++>>>>>>> fd501d4f0399 (x86/entry: Remove DBn stacks)
  
  /*
 - * Double fault entry. Straight paranoid. No checks from which context
 - * this comes because for the espfix induced #DF this would do the wrong
 - * thing.
 + * The interrupt stubs push (~vector+0x80) onto the stack and
 + * then jump to common_spurious/interrupt.
   */
 -.macro idtentry_df vector asmsym cfunc
 -SYM_CODE_START(\asmsym)
 -	UNWIND_HINT_IRET_REGS offset=8
 -	ASM_CLAC
 -
 -	/*
 -	 * paranoid_entry returns SWAPGS flag for paranoid_exit in EBX.
 -	 * EBX == 0 -> SWAPGS, EBX == 1 -> no SWAPGS
 -	 */
 -	call	paranoid_entry
 -	UNWIND_HINT_REGS
 -
 -	movq	%rsp, %rdi		/* pt_regs pointer into first argument */
 -	movq	ORIG_RAX(%rsp), %rsi	/* get error code into 2nd argument*/
 -	movq	$-1, ORIG_RAX(%rsp)	/* no syscall to restart */
 -	call	\cfunc
 -
 -	jmp	paranoid_exit
 +common_spurious:
 +	addq	$-0x80, (%rsp)			/* Adjust vector to [-256, -1] range */
 +	call	interrupt_entry
 +	UNWIND_HINT_REGS indirect=1
 +	call	smp_spurious_interrupt		/* rdi points to pt_regs */
 +	jmp	ret_from_intr
 +END(common_spurious)
 +_ASM_NOKPROBE(common_spurious)
 +
 +/* common_interrupt is a hotpath. Align it */
 +	.p2align CONFIG_X86_L1_CACHE_SHIFT
 +common_interrupt:
 +	addq	$-0x80, (%rsp)			/* Adjust vector to [-256, -1] range */
 +	call	interrupt_entry
 +	UNWIND_HINT_REGS indirect=1
 +	call	do_IRQ	/* rdi points to pt_regs */
 +	/* 0(%rsp): old RSP */
 +ret_from_intr:
 +	DISABLE_INTERRUPTS(CLBR_ANY)
 +	TRACE_IRQS_OFF
 +
 +	LEAVE_IRQ_STACK
  
 -_ASM_NOKPROBE(\asmsym)
 -SYM_CODE_END(\asmsym)
 -.endm
 +	testb	$3, CS(%rsp)
 +	jz	retint_kernel
  
 -/*
 - * Include the defines which emit the idt entries which are shared
 - * shared between 32 and 64 bit.
 - */
 -#include <asm/idtentry.h>
 +	/* Interrupt came from user space */
 +GLOBAL(retint_user)
 +	mov	%rsp,%rdi
 +	call	prepare_exit_to_usermode
 +	TRACE_IRQS_IRETQ
  
 -SYM_CODE_START_LOCAL(common_interrupt_return)
 -SYM_INNER_LABEL(swapgs_restore_regs_and_return_to_usermode, SYM_L_GLOBAL)
 +GLOBAL(swapgs_restore_regs_and_return_to_usermode)
  #ifdef CONFIG_DEBUG_ENTRY
  	/* Assert that pt_regs indicates user mode. */
  	testb	$3, CS(%rsp)
diff --cc arch/x86/include/asm/cpu_entry_area.h
index 29c706415443,8902fdb7de13..000000000000
--- a/arch/x86/include/asm/cpu_entry_area.h
+++ b/arch/x86/include/asm/cpu_entry_area.h
@@@ -6,6 -6,66 +6,69 @@@
  #include <linux/percpu-defs.h>
  #include <asm/processor.h>
  #include <asm/intel_ds.h>
++<<<<<<< HEAD
++=======
+ #include <asm/pgtable_areas.h>
+ 
+ #ifdef CONFIG_X86_64
+ 
+ /* Macro to enforce the same ordering and stack sizes */
+ #define ESTACKS_MEMBERS(guardsize)		\
+ 	char	DF_stack_guard[guardsize];	\
+ 	char	DF_stack[EXCEPTION_STKSZ];	\
+ 	char	NMI_stack_guard[guardsize];	\
+ 	char	NMI_stack[EXCEPTION_STKSZ];	\
+ 	char	DB_stack_guard[guardsize];	\
+ 	char	DB_stack[EXCEPTION_STKSZ];	\
+ 	char	MCE_stack_guard[guardsize];	\
+ 	char	MCE_stack[EXCEPTION_STKSZ];	\
+ 	char	IST_top_guard[guardsize];	\
+ 
+ /* The exception stacks' physical storage. No guard pages required */
+ struct exception_stacks {
+ 	ESTACKS_MEMBERS(0)
+ };
+ 
+ /* The effective cpu entry area mapping with guard pages. */
+ struct cea_exception_stacks {
+ 	ESTACKS_MEMBERS(PAGE_SIZE)
+ };
+ 
+ /*
+  * The exception stack ordering in [cea_]exception_stacks
+  */
+ enum exception_stack_ordering {
+ 	ESTACK_DF,
+ 	ESTACK_NMI,
+ 	ESTACK_DB,
+ 	ESTACK_MCE,
+ 	N_EXCEPTION_STACKS
+ };
+ 
+ #define CEA_ESTACK_SIZE(st)					\
+ 	sizeof(((struct cea_exception_stacks *)0)->st## _stack)
+ 
+ #define CEA_ESTACK_BOT(ceastp, st)				\
+ 	((unsigned long)&(ceastp)->st## _stack)
+ 
+ #define CEA_ESTACK_TOP(ceastp, st)				\
+ 	(CEA_ESTACK_BOT(ceastp, st) + CEA_ESTACK_SIZE(st))
+ 
+ #define CEA_ESTACK_OFFS(st)					\
+ 	offsetof(struct cea_exception_stacks, st## _stack)
+ 
+ #define CEA_ESTACK_PAGES					\
+ 	(sizeof(struct cea_exception_stacks) / PAGE_SIZE)
+ 
+ #endif
+ 
+ #ifdef CONFIG_X86_32
+ struct doublefault_stack {
+ 	unsigned long stack[(PAGE_SIZE - sizeof(struct x86_hw_tss)) / sizeof(unsigned long)];
+ 	struct x86_hw_tss tss;
+ } __aligned(PAGE_SIZE);
+ #endif
++>>>>>>> fd501d4f0399 (x86/entry: Remove DBn stacks)
  
  /*
   * cpu_entry_area is a percpu region that contains things needed by the CPU
diff --cc arch/x86/kernel/asm-offsets_64.c
index 817625756a5f,828be792231e..000000000000
--- a/arch/x86/kernel/asm-offsets_64.c
+++ b/arch/x86/kernel/asm-offsets_64.c
@@@ -64,7 -57,6 +64,10 @@@ int main(void
  	BLANK();
  #undef ENTRY
  
++<<<<<<< HEAD
 +	OFFSET(TSS_ist, tss_struct, x86_tss.ist);
++=======
++>>>>>>> fd501d4f0399 (x86/entry: Remove DBn stacks)
  	BLANK();
  
  #ifdef CONFIG_STACKPROTECTOR
diff --cc arch/x86/kernel/dumpstack_64.c
index 35a75d2d7b5a,4a94d38cd141..000000000000
--- a/arch/x86/kernel/dumpstack_64.c
+++ b/arch/x86/kernel/dumpstack_64.c
@@@ -16,18 -16,14 +16,26 @@@
  #include <linux/bug.h>
  #include <linux/nmi.h>
  
 -#include <asm/cpu_entry_area.h>
  #include <asm/stacktrace.h>
  
++<<<<<<< HEAD
 +static char *exception_stack_names[N_EXCEPTION_STACKS] = {
 +		[ DOUBLEFAULT_STACK-1	]	= "#DF",
 +		[ NMI_STACK-1		]	= "NMI",
 +		[ DEBUG_STACK-1		]	= "#DB",
 +		[ MCE_STACK-1		]	= "#MC",
 +};
 +
 +static unsigned long exception_stack_sizes[N_EXCEPTION_STACKS] = {
 +	[0 ... N_EXCEPTION_STACKS - 1]		= EXCEPTION_STKSZ,
 +	[DEBUG_STACK - 1]			= DEBUG_STKSZ
++=======
+ static const char * const exception_stack_names[] = {
+ 		[ ESTACK_DF	]	= "#DF",
+ 		[ ESTACK_NMI	]	= "NMI",
+ 		[ ESTACK_DB	]	= "#DB",
+ 		[ ESTACK_MCE	]	= "#MC",
++>>>>>>> fd501d4f0399 (x86/entry: Remove DBn stacks)
  };
  
  const char *stack_type_name(enum stack_type type)
@@@ -52,11 -48,45 +60,47 @@@
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * struct estack_pages - Page descriptor for exception stacks
+  * @offs:	Offset from the start of the exception stack area
+  * @size:	Size of the exception stack
+  * @type:	Type to store in the stack_info struct
+  */
+ struct estack_pages {
+ 	u32	offs;
+ 	u16	size;
+ 	u16	type;
+ };
+ 
+ #define EPAGERANGE(st)							\
+ 	[PFN_DOWN(CEA_ESTACK_OFFS(st)) ...				\
+ 	 PFN_DOWN(CEA_ESTACK_OFFS(st) + CEA_ESTACK_SIZE(st) - 1)] = {	\
+ 		.offs	= CEA_ESTACK_OFFS(st),				\
+ 		.size	= CEA_ESTACK_SIZE(st),				\
+ 		.type	= STACK_TYPE_EXCEPTION + ESTACK_ ##st, }
+ 
+ /*
+  * Array of exception stack page descriptors. If the stack is larger than
+  * PAGE_SIZE, all pages covering a particular stack will have the same
+  * info. The guard pages including the not mapped DB2 stack are zeroed
+  * out.
+  */
+ static const
+ struct estack_pages estack_pages[CEA_ESTACK_PAGES] ____cacheline_aligned = {
+ 	EPAGERANGE(DF),
+ 	EPAGERANGE(NMI),
+ 	EPAGERANGE(DB),
+ 	EPAGERANGE(MCE),
+ };
+ 
++>>>>>>> fd501d4f0399 (x86/entry: Remove DBn stacks)
  static bool in_exception_stack(unsigned long *stack, struct stack_info *info)
  {
 -	unsigned long begin, end, stk = (unsigned long)stack;
 -	const struct estack_pages *ep;
 +	unsigned long *begin, *end;
  	struct pt_regs *regs;
 -	unsigned int k;
 +	unsigned k;
  
  	BUILD_BUG_ON(N_EXCEPTION_STACKS != 4);
  
diff --cc arch/x86/mm/cpu_entry_area.c
index 26fa2a5a8715,770b613790b3..000000000000
--- a/arch/x86/mm/cpu_entry_area.c
+++ b/arch/x86/mm/cpu_entry_area.c
@@@ -78,9 -82,48 +78,50 @@@ static void __init percpu_setup_debug_s
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_X86_64
+ 
+ #define cea_map_stack(name) do {					\
+ 	npages = sizeof(estacks->name## _stack) / PAGE_SIZE;		\
+ 	cea_map_percpu_pages(cea->estacks.name## _stack,		\
+ 			estacks->name## _stack, npages, PAGE_KERNEL);	\
+ 	} while (0)
+ 
+ static void __init percpu_setup_exception_stacks(unsigned int cpu)
+ {
+ 	struct exception_stacks *estacks = per_cpu_ptr(&exception_stacks, cpu);
+ 	struct cpu_entry_area *cea = get_cpu_entry_area(cpu);
+ 	unsigned int npages;
+ 
+ 	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);
+ 
+ 	per_cpu(cea_exception_stacks, cpu) = &cea->estacks;
+ 
+ 	/*
+ 	 * The exceptions stack mappings in the per cpu area are protected
+ 	 * by guard pages so each stack must be mapped separately. DB2 is
+ 	 * not mapped; it just exists to catch triple nesting of #DB.
+ 	 */
+ 	cea_map_stack(DF);
+ 	cea_map_stack(NMI);
+ 	cea_map_stack(DB);
+ 	cea_map_stack(MCE);
+ }
+ #else
+ static inline void percpu_setup_exception_stacks(unsigned int cpu)
+ {
+ 	struct cpu_entry_area *cea = get_cpu_entry_area(cpu);
+ 
+ 	cea_map_percpu_pages(&cea->doublefault_stack,
+ 			     &per_cpu(doublefault_stack, cpu), 1, PAGE_KERNEL);
+ }
+ #endif
+ 
++>>>>>>> fd501d4f0399 (x86/entry: Remove DBn stacks)
  /* Setup the fixmap mappings only once per-processor */
 -static void __init setup_cpu_entry_area(unsigned int cpu)
 +static void __init setup_cpu_entry_area(int cpu)
  {
 -	struct cpu_entry_area *cea = get_cpu_entry_area(cpu);
  #ifdef CONFIG_X86_64
  	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */
  	pgprot_t gdt_prot = PAGE_KERNEL_RO;
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/include/asm/cpu_entry_area.h
* Unmerged path arch/x86/kernel/asm-offsets_64.c
* Unmerged path arch/x86/kernel/dumpstack_64.c
* Unmerged path arch/x86/mm/cpu_entry_area.c
