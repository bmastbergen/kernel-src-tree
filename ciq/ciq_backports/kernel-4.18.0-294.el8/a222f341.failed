mm: generalize putback scan functions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Kirill Tkhai <ktkhai@virtuozzo.com>
commit a222f341586834073c2bbea225be38216eb5d993
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/a222f341.failed

This combines two similar functions move_active_pages_to_lru() and
putback_inactive_pages() into single move_pages_to_lru().  This remove
duplicate code and makes object file size smaller.

Before:
   text	   data	    bss	    dec	    hex	filename
  57082	   4732	    128	  61942	   f1f6	mm/vmscan.o
After:
   text	   data	    bss	    dec	    hex	filename
  55112	   4600	    128	  59840	   e9c0	mm/vmscan.o

Note, that now we are checking for !page_evictable() coming from
shrink_active_list(), which shouldn't change any behavior since that path
works with evictable pages only.

Link: http://lkml.kernel.org/r/155290129627.31489.8321971028677203248.stgit@localhost.localdomain
	Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
	Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a222f341586834073c2bbea225be38216eb5d993)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmscan.c
diff --cc mm/vmscan.c
index a8963fa6828f,40ff747e0b33..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -1968,14 -1955,21 +1992,14 @@@ shrink_inactive_list(unsigned long nr_t
  
  	spin_lock_irq(&pgdat->lru_lock);
  
 -	if (current_is_kswapd()) {
 -		if (global_reclaim(sc))
 -			__count_vm_events(PGSTEAL_KSWAPD, nr_reclaimed);
 -		count_memcg_events(lruvec_memcg(lruvec), PGSTEAL_KSWAPD,
 -				   nr_reclaimed);
 -	} else {
 -		if (global_reclaim(sc))
 -			__count_vm_events(PGSTEAL_DIRECT, nr_reclaimed);
 -		count_memcg_events(lruvec_memcg(lruvec), PGSTEAL_DIRECT,
 -				   nr_reclaimed);
 -	}
 -	reclaim_stat->recent_rotated[0] = stat.nr_activate[0];
 -	reclaim_stat->recent_rotated[1] = stat.nr_activate[1];
 +	item = current_is_kswapd() ? PGSTEAL_KSWAPD : PGSTEAL_DIRECT;
 +	if (global_reclaim(sc))
 +		__count_vm_events(item, nr_reclaimed);
 +	__count_memcg_events(lruvec_memcg(lruvec), item, nr_reclaimed);
 +	reclaim_stat->recent_rotated[0] += stat.nr_activate[0];
 +	reclaim_stat->recent_rotated[1] += stat.nr_activate[1];
  
- 	putback_inactive_pages(lruvec, &page_list);
+ 	move_pages_to_lru(lruvec, &page_list);
  
  	__mod_node_page_state(pgdat, NR_ISOLATED_ANON + file, -nr_taken);
  
@@@ -2012,72 -2006,6 +2036,75 @@@
  	return nr_reclaimed;
  }
  
++<<<<<<< HEAD
 +/*
 + * This moves pages from the active list to the inactive list.
 + *
 + * We move them the other way if the page is referenced by one or more
 + * processes, from rmap.
 + *
 + * If the pages are mostly unmapped, the processing is fast and it is
 + * appropriate to hold zone_lru_lock across the whole operation.  But if
 + * the pages are mapped, the processing is slow (page_referenced()) so we
 + * should drop zone_lru_lock around each page.  It's impossible to balance
 + * this, so instead we remove the pages from the LRU while processing them.
 + * It is safe to rely on PG_active against the non-LRU pages in here because
 + * nobody will play with that bit on a non-LRU page.
 + *
 + * The downside is that we have to touch page->_refcount against each page.
 + * But we had to alter page->flags anyway.
 + *
 + * Returns the number of pages moved to the given lru.
 + */
 +
 +static unsigned move_active_pages_to_lru(struct lruvec *lruvec,
 +				     struct list_head *list,
 +				     enum lru_list lru)
 +{
 +	struct pglist_data *pgdat = lruvec_pgdat(lruvec);
 +	LIST_HEAD(pages_to_free);
 +	struct page *page;
 +	int nr_pages;
 +	int nr_moved = 0;
 +
 +	while (!list_empty(list)) {
 +		page = lru_to_page(list);
 +		lruvec = mem_cgroup_page_lruvec(page, pgdat);
 +
 +		VM_BUG_ON_PAGE(PageLRU(page), page);
 +		SetPageLRU(page);
 +
 +		nr_pages = hpage_nr_pages(page);
 +		update_lru_size(lruvec, lru, page_zonenum(page), nr_pages);
 +		list_move(&page->lru, &lruvec->lists[lru]);
 +
 +		if (put_page_testzero(page)) {
 +			__ClearPageLRU(page);
 +			__ClearPageActive(page);
 +			del_page_from_lru_list(page, lruvec, lru);
 +
 +			if (unlikely(PageCompound(page))) {
 +				spin_unlock_irq(&pgdat->lru_lock);
 +				mem_cgroup_uncharge(page);
 +				(*get_compound_page_dtor(page))(page);
 +				spin_lock_irq(&pgdat->lru_lock);
 +			} else
 +				list_add(&page->lru, &pages_to_free);
 +		} else {
 +			nr_moved += nr_pages;
 +		}
 +	}
 +
 +	/*
 +	 * To save our caller's stack, now use input list for pages to free.
 +	 */
 +	list_splice(&pages_to_free, list);
 +
 +	return nr_moved;
 +}
 +
++=======
++>>>>>>> a222f3415868 (mm: generalize putback scan functions)
  static void shrink_active_list(unsigned long nr_to_scan,
  			       struct lruvec *lruvec,
  			       struct scan_control *sc,
* Unmerged path mm/vmscan.c
