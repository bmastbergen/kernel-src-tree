sched/fair: Take into account runnable_avg to classify group

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Vincent Guittot <vincent.guittot@linaro.org>
commit 070f5e860ee2bf588c99ef7b4c202451faa48236
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/070f5e86.failed

Take into account the new runnable_avg signal to classify a group and to
mitigate the volatility of util_avg in face of intensive migration or
new task with random utilization.

	Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
	Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Cc: Juri Lelli <juri.lelli@redhat.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Valentin Schneider <valentin.schneider@arm.com>
	Cc: Phil Auld <pauld@redhat.com>
	Cc: Hillf Danton <hdanton@sina.com>
Link: https://lore.kernel.org/r/20200224095223.13361-10-mgorman@techsingularity.net
(cherry picked from commit 070f5e860ee2bf588c99ef7b4c202451faa48236)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 96ed6a414530,87521acb3698..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5507,6 -5464,29 +5507,32 @@@ static unsigned long cpu_load_without(s
  	return load;
  }
  
++<<<<<<< HEAD
++=======
+ static unsigned long cpu_runnable(struct rq *rq)
+ {
+ 	return cfs_rq_runnable_avg(&rq->cfs);
+ }
+ 
+ static unsigned long cpu_runnable_without(struct rq *rq, struct task_struct *p)
+ {
+ 	struct cfs_rq *cfs_rq;
+ 	unsigned int runnable;
+ 
+ 	/* Task has no contribution or is new */
+ 	if (cpu_of(rq) != task_cpu(p) || !READ_ONCE(p->se.avg.last_update_time))
+ 		return cpu_runnable(rq);
+ 
+ 	cfs_rq = &rq->cfs;
+ 	runnable = READ_ONCE(cfs_rq->avg.runnable_avg);
+ 
+ 	/* Discount task's runnable from CPU's runnable */
+ 	lsub_positive(&runnable, p->se.avg.runnable_avg);
+ 
+ 	return runnable;
+ }
+ 
++>>>>>>> 070f5e860ee2 (sched/fair: Take into account runnable_avg to classify group)
  static unsigned long capacity_of(int cpu)
  {
  	return cpu_rq(cpu)->cpu_capacity;
* Unmerged path kernel/sched/fair.c
