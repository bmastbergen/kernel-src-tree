mm/vmscan: restore active/inactive ratio for anonymous LRU

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Joonsoo Kim <iamjoonsoo.kim@lge.com>
commit 4002570c5c585c4c9a889e45dd104ed663257eec
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/4002570c.failed

Now that workingset detection is implemented for anonymous LRU, we don't
need large inactive list to allow detecting frequently accessed pages
before they are reclaimed, anymore.  This effectively reverts the
temporary measure put in by commit "mm/vmscan: make active/inactive ratio
as 1:1 for anon lru".

	Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Minchan Kim <minchan@kernel.org>
Link: http://lkml.kernel.org/r/1595490560-15117-7-git-send-email-iamjoonsoo.kim@lge.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4002570c5c585c4c9a889e45dd104ed663257eec)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmscan.c
diff --cc mm/vmscan.c
index d33ed25f0511,846b7e9b8d2e..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -2230,43 -2196,21 +2230,51 @@@ unsigned long reclaim_pages(struct list
   *    1TB     101        10GB
   *   10TB     320        32GB
   */
 -static bool inactive_is_low(struct lruvec *lruvec, enum lru_list inactive_lru)
 +static bool inactive_list_is_low(struct lruvec *lruvec, bool file,
 +				 struct scan_control *sc, bool trace)
  {
 -	enum lru_list active_lru = inactive_lru + LRU_ACTIVE;
 +	enum lru_list active_lru = file * LRU_FILE + LRU_ACTIVE;
 +	struct pglist_data *pgdat = lruvec_pgdat(lruvec);
 +	enum lru_list inactive_lru = file * LRU_FILE;
  	unsigned long inactive, active;
  	unsigned long inactive_ratio;
 +	struct lruvec *target_lruvec;
 +	unsigned long refaults;
  	unsigned long gb;
  
 -	inactive = lruvec_page_state(lruvec, NR_LRU_BASE + inactive_lru);
 -	active = lruvec_page_state(lruvec, NR_LRU_BASE + active_lru);
 +	inactive = lruvec_lru_size(lruvec, inactive_lru, sc->reclaim_idx);
 +	active = lruvec_lru_size(lruvec, active_lru, sc->reclaim_idx);
 +
++<<<<<<< HEAD
 +	/*
 +	 * When refaults are being observed, it means a new workingset
 +	 * is being established. Disable active list protection to get
 +	 * rid of the stale workingset quickly.
 +	 */
 +	target_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);
 +	refaults = lruvec_page_state(target_lruvec, WORKINGSET_ACTIVATE);
 +	if (file && target_lruvec->refaults != refaults) {
 +		inactive_ratio = 0;
 +	} else {
 +		gb = (inactive + active) >> (30 - PAGE_SHIFT);
 +		if (gb)
 +			inactive_ratio = int_sqrt(10 * gb);
 +		else
 +			inactive_ratio = 1;
 +	}
  
 +	if (trace)
 +		trace_mm_vmscan_inactive_list_is_low(pgdat->node_id, sc->reclaim_idx,
 +			lruvec_lru_size(lruvec, inactive_lru, MAX_NR_ZONES), inactive,
 +			lruvec_lru_size(lruvec, active_lru, MAX_NR_ZONES), active,
 +			inactive_ratio, file);
++=======
+ 	gb = (inactive + active) >> (30 - PAGE_SHIFT);
+ 	if (gb)
+ 		inactive_ratio = int_sqrt(10 * gb);
+ 	else
+ 		inactive_ratio = 1;
++>>>>>>> 4002570c5c58 (mm/vmscan: restore active/inactive ratio for anonymous LRU)
  
  	return inactive * inactive_ratio < active;
  }
* Unmerged path mm/vmscan.c
