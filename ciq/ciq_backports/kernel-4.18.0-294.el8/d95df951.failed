kvm: tracing: Fix unmatched kvm_entry and kvm_exit events

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Lorenzo Brescia <lorenzo.brescia@edu.unito.it>
commit d95df9510679757bdfc22376d351cdf367b3a604
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/d95df951.failed

On VMX, if we exit and then re-enter immediately without leaving
the vmx_vcpu_run() function, the kvm_entry event is not logged.
That means we will see one (or more) kvm_exit, without its (their)
corresponding kvm_entry, as shown here:

 CPU-1979 [002] 89.871187: kvm_entry: vcpu 1
 CPU-1979 [002] 89.871218: kvm_exit:  reason MSR_WRITE
 CPU-1979 [002] 89.871259: kvm_exit:  reason MSR_WRITE

It also seems possible for a kvm_entry event to be logged, but then
we leave vmx_vcpu_run() right away (if vmx->emulation_required is
true). In this case, we will have a spurious kvm_entry event in the
trace.

Fix these situations by moving trace_kvm_entry() inside vmx_vcpu_run()
(where trace_kvm_exit() already is).

A trace obtained with this patch applied looks like this:

 CPU-14295 [000] 8388.395387: kvm_entry: vcpu 0
 CPU-14295 [000] 8388.395392: kvm_exit:  reason MSR_WRITE
 CPU-14295 [000] 8388.395393: kvm_entry: vcpu 0
 CPU-14295 [000] 8388.395503: kvm_exit:  reason EXTERNAL_INTERRUPT

Of course, not calling trace_kvm_entry() in common x86 code any
longer means that we need to adjust the SVM side of things too.

	Signed-off-by: Lorenzo Brescia <lorenzo.brescia@edu.unito.it>
	Signed-off-by: Dario Faggioli <dfaggioli@suse.com>
Message-Id: <160873470698.11652.13483635328769030605.stgit@Wayrath>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit d95df9510679757bdfc22376d351cdf367b3a604)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/x86.c
index 22f86370fb94,1f64e8b97605..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -10762,6 -11308,257 +10760,260 @@@ int kvm_handle_memory_failure(struct kv
  }
  EXPORT_SYMBOL_GPL(kvm_handle_memory_failure);
  
++<<<<<<< HEAD
++=======
+ int kvm_handle_invpcid(struct kvm_vcpu *vcpu, unsigned long type, gva_t gva)
+ {
+ 	bool pcid_enabled;
+ 	struct x86_exception e;
+ 	unsigned i;
+ 	unsigned long roots_to_free = 0;
+ 	struct {
+ 		u64 pcid;
+ 		u64 gla;
+ 	} operand;
+ 	int r;
+ 
+ 	r = kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e);
+ 	if (r != X86EMUL_CONTINUE)
+ 		return kvm_handle_memory_failure(vcpu, r, &e);
+ 
+ 	if (operand.pcid >> 12 != 0) {
+ 		kvm_inject_gp(vcpu, 0);
+ 		return 1;
+ 	}
+ 
+ 	pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);
+ 
+ 	switch (type) {
+ 	case INVPCID_TYPE_INDIV_ADDR:
+ 		if ((!pcid_enabled && (operand.pcid != 0)) ||
+ 		    is_noncanonical_address(operand.gla, vcpu)) {
+ 			kvm_inject_gp(vcpu, 0);
+ 			return 1;
+ 		}
+ 		kvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);
+ 		return kvm_skip_emulated_instruction(vcpu);
+ 
+ 	case INVPCID_TYPE_SINGLE_CTXT:
+ 		if (!pcid_enabled && (operand.pcid != 0)) {
+ 			kvm_inject_gp(vcpu, 0);
+ 			return 1;
+ 		}
+ 
+ 		if (kvm_get_active_pcid(vcpu) == operand.pcid) {
+ 			kvm_mmu_sync_roots(vcpu);
+ 			kvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);
+ 		}
+ 
+ 		for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
+ 			if (kvm_get_pcid(vcpu, vcpu->arch.mmu->prev_roots[i].pgd)
+ 			    == operand.pcid)
+ 				roots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);
+ 
+ 		kvm_mmu_free_roots(vcpu, vcpu->arch.mmu, roots_to_free);
+ 		/*
+ 		 * If neither the current cr3 nor any of the prev_roots use the
+ 		 * given PCID, then nothing needs to be done here because a
+ 		 * resync will happen anyway before switching to any other CR3.
+ 		 */
+ 
+ 		return kvm_skip_emulated_instruction(vcpu);
+ 
+ 	case INVPCID_TYPE_ALL_NON_GLOBAL:
+ 		/*
+ 		 * Currently, KVM doesn't mark global entries in the shadow
+ 		 * page tables, so a non-global flush just degenerates to a
+ 		 * global flush. If needed, we could optimize this later by
+ 		 * keeping track of global entries in shadow page tables.
+ 		 */
+ 
+ 		fallthrough;
+ 	case INVPCID_TYPE_ALL_INCL_GLOBAL:
+ 		kvm_mmu_unload(vcpu);
+ 		return kvm_skip_emulated_instruction(vcpu);
+ 
+ 	default:
+ 		BUG(); /* We have already checked above that type <= 3 */
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(kvm_handle_invpcid);
+ 
+ static int complete_sev_es_emulated_mmio(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_run *run = vcpu->run;
+ 	struct kvm_mmio_fragment *frag;
+ 	unsigned int len;
+ 
+ 	BUG_ON(!vcpu->mmio_needed);
+ 
+ 	/* Complete previous fragment */
+ 	frag = &vcpu->mmio_fragments[vcpu->mmio_cur_fragment];
+ 	len = min(8u, frag->len);
+ 	if (!vcpu->mmio_is_write)
+ 		memcpy(frag->data, run->mmio.data, len);
+ 
+ 	if (frag->len <= 8) {
+ 		/* Switch to the next fragment. */
+ 		frag++;
+ 		vcpu->mmio_cur_fragment++;
+ 	} else {
+ 		/* Go forward to the next mmio piece. */
+ 		frag->data += len;
+ 		frag->gpa += len;
+ 		frag->len -= len;
+ 	}
+ 
+ 	if (vcpu->mmio_cur_fragment >= vcpu->mmio_nr_fragments) {
+ 		vcpu->mmio_needed = 0;
+ 
+ 		// VMG change, at this point, we're always done
+ 		// RIP has already been advanced
+ 		return 1;
+ 	}
+ 
+ 	// More MMIO is needed
+ 	run->mmio.phys_addr = frag->gpa;
+ 	run->mmio.len = min(8u, frag->len);
+ 	run->mmio.is_write = vcpu->mmio_is_write;
+ 	if (run->mmio.is_write)
+ 		memcpy(run->mmio.data, frag->data, min(8u, frag->len));
+ 	run->exit_reason = KVM_EXIT_MMIO;
+ 
+ 	vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+ 
+ 	return 0;
+ }
+ 
+ int kvm_sev_es_mmio_write(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,
+ 			  void *data)
+ {
+ 	int handled;
+ 	struct kvm_mmio_fragment *frag;
+ 
+ 	if (!data)
+ 		return -EINVAL;
+ 
+ 	handled = write_emultor.read_write_mmio(vcpu, gpa, bytes, data);
+ 	if (handled == bytes)
+ 		return 1;
+ 
+ 	bytes -= handled;
+ 	gpa += handled;
+ 	data += handled;
+ 
+ 	/*TODO: Check if need to increment number of frags */
+ 	frag = vcpu->mmio_fragments;
+ 	vcpu->mmio_nr_fragments = 1;
+ 	frag->len = bytes;
+ 	frag->gpa = gpa;
+ 	frag->data = data;
+ 
+ 	vcpu->mmio_needed = 1;
+ 	vcpu->mmio_cur_fragment = 0;
+ 
+ 	vcpu->run->mmio.phys_addr = gpa;
+ 	vcpu->run->mmio.len = min(8u, frag->len);
+ 	vcpu->run->mmio.is_write = 1;
+ 	memcpy(vcpu->run->mmio.data, frag->data, min(8u, frag->len));
+ 	vcpu->run->exit_reason = KVM_EXIT_MMIO;
+ 
+ 	vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(kvm_sev_es_mmio_write);
+ 
+ int kvm_sev_es_mmio_read(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,
+ 			 void *data)
+ {
+ 	int handled;
+ 	struct kvm_mmio_fragment *frag;
+ 
+ 	if (!data)
+ 		return -EINVAL;
+ 
+ 	handled = read_emultor.read_write_mmio(vcpu, gpa, bytes, data);
+ 	if (handled == bytes)
+ 		return 1;
+ 
+ 	bytes -= handled;
+ 	gpa += handled;
+ 	data += handled;
+ 
+ 	/*TODO: Check if need to increment number of frags */
+ 	frag = vcpu->mmio_fragments;
+ 	vcpu->mmio_nr_fragments = 1;
+ 	frag->len = bytes;
+ 	frag->gpa = gpa;
+ 	frag->data = data;
+ 
+ 	vcpu->mmio_needed = 1;
+ 	vcpu->mmio_cur_fragment = 0;
+ 
+ 	vcpu->run->mmio.phys_addr = gpa;
+ 	vcpu->run->mmio.len = min(8u, frag->len);
+ 	vcpu->run->mmio.is_write = 0;
+ 	vcpu->run->exit_reason = KVM_EXIT_MMIO;
+ 
+ 	vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+ 
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(kvm_sev_es_mmio_read);
+ 
+ static int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)
+ {
+ 	memcpy(vcpu->arch.guest_ins_data, vcpu->arch.pio_data,
+ 	       vcpu->arch.pio.count * vcpu->arch.pio.size);
+ 	vcpu->arch.pio.count = 0;
+ 
+ 	return 1;
+ }
+ 
+ static int kvm_sev_es_outs(struct kvm_vcpu *vcpu, unsigned int size,
+ 			   unsigned int port, void *data,  unsigned int count)
+ {
+ 	int ret;
+ 
+ 	ret = emulator_pio_out_emulated(vcpu->arch.emulate_ctxt, size, port,
+ 					data, count);
+ 	if (ret)
+ 		return ret;
+ 
+ 	vcpu->arch.pio.count = 0;
+ 
+ 	return 0;
+ }
+ 
+ static int kvm_sev_es_ins(struct kvm_vcpu *vcpu, unsigned int size,
+ 			  unsigned int port, void *data, unsigned int count)
+ {
+ 	int ret;
+ 
+ 	ret = emulator_pio_in_emulated(vcpu->arch.emulate_ctxt, size, port,
+ 				       data, count);
+ 	if (ret) {
+ 		vcpu->arch.pio.count = 0;
+ 	} else {
+ 		vcpu->arch.guest_ins_data = data;
+ 		vcpu->arch.complete_userspace_io = complete_sev_es_emulated_ins;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ int kvm_sev_es_string_io(struct kvm_vcpu *vcpu, unsigned int size,
+ 			 unsigned int port, void *data,  unsigned int count,
+ 			 int in)
+ {
+ 	return in ? kvm_sev_es_ins(vcpu, size, port, data, count)
+ 		  : kvm_sev_es_outs(vcpu, size, port, data, count);
+ }
+ EXPORT_SYMBOL_GPL(kvm_sev_es_string_io);
+ 
+ EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_entry);
++>>>>>>> d95df9510679 (kvm: tracing: Fix unmatched kvm_entry and kvm_exit events)
  EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_exit);
  EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_fast_mmio);
  EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_inj_virq);
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 72f42a4954e3..e66159db5804 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -3402,6 +3402,8 @@ static fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 
+	trace_kvm_entry(vcpu);
+
 	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 38e1b12def35..5817134071bc 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -6573,6 +6573,8 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	if (vmx->emulation_required)
 		return EXIT_FASTPATH_NONE;
 
+	trace_kvm_entry(vcpu);
+
 	if (vmx->ple_window_dirty) {
 		vmx->ple_window_dirty = false;
 		vmcs_write32(PLE_WINDOW, vmx->ple_window);
* Unmerged path arch/x86/kvm/x86.c
