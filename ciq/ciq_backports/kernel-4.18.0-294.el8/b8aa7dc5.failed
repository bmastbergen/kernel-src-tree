crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit b8aa7dc5c7535f9abfca4bceb0ade9ee10cf5f54
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/b8aa7dc5.failed

Set the flag CRYPTO_ALG_ALLOCATES_MEMORY in the crypto drivers that
allocate memory.

drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c: sun8i_ce_cipher
drivers/crypto/allwinner/sun8i-ss/sun8i-ss-core.c: sun8i_ss_cipher
drivers/crypto/amlogic/amlogic-gxl-core.c: meson_cipher
drivers/crypto/axis/artpec6_crypto.c: artpec6_crypto_common_init
drivers/crypto/bcm/cipher.c: spu_skcipher_rx_sg_create
drivers/crypto/caam/caamalg.c: aead_edesc_alloc
drivers/crypto/caam/caamalg_qi.c: aead_edesc_alloc
drivers/crypto/caam/caamalg_qi2.c: aead_edesc_alloc
drivers/crypto/caam/caamhash.c: hash_digest_key
drivers/crypto/cavium/cpt/cptvf_algs.c: process_request
drivers/crypto/cavium/nitrox/nitrox_aead.c: nitrox_process_se_request
drivers/crypto/cavium/nitrox/nitrox_skcipher.c: nitrox_process_se_request
drivers/crypto/ccp/ccp-crypto-aes-cmac.c: ccp_do_cmac_update
drivers/crypto/ccp/ccp-crypto-aes-galois.c: ccp_crypto_enqueue_request
drivers/crypto/ccp/ccp-crypto-aes-xts.c: ccp_crypto_enqueue_request
drivers/crypto/ccp/ccp-crypto-aes.c: ccp_crypto_enqueue_request
drivers/crypto/ccp/ccp-crypto-des3.c: ccp_crypto_enqueue_request
drivers/crypto/ccp/ccp-crypto-sha.c: ccp_crypto_enqueue_request
drivers/crypto/chelsio/chcr_algo.c: create_cipher_wr
drivers/crypto/hisilicon/sec/sec_algs.c: sec_alloc_and_fill_hw_sgl
drivers/crypto/hisilicon/sec2/sec_crypto.c: sec_alloc_req_id
drivers/crypto/inside-secure/safexcel_cipher.c: safexcel_queue_req
drivers/crypto/inside-secure/safexcel_hash.c: safexcel_ahash_enqueue
drivers/crypto/ixp4xx_crypto.c: ablk_perform
drivers/crypto/marvell/cesa/cipher.c: mv_cesa_skcipher_dma_req_init
drivers/crypto/marvell/cesa/hash.c: mv_cesa_ahash_dma_req_init
drivers/crypto/marvell/octeontx/otx_cptvf_algs.c: create_ctx_hdr
drivers/crypto/n2_core.c: n2_compute_chunks
drivers/crypto/picoxcell_crypto.c: spacc_sg_to_ddt
drivers/crypto/qat/qat_common/qat_algs.c: qat_alg_skcipher_encrypt
drivers/crypto/qce/skcipher.c: qce_skcipher_async_req_handle
drivers/crypto/talitos.c : talitos_edesc_alloc
drivers/crypto/virtio/virtio_crypto_algs.c: __virtio_crypto_skcipher_do_req
drivers/crypto/xilinx/zynqmp-aes-gcm.c: zynqmp_aes_aead_cipher

	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
[EB: avoid overly-long lines]
	Signed-off-by: Eric Biggers <ebiggers@google.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit b8aa7dc5c7535f9abfca4bceb0ade9ee10cf5f54)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c
#	drivers/crypto/allwinner/sun8i-ss/sun8i-ss-core.c
#	drivers/crypto/amlogic/amlogic-gxl-core.c
#	drivers/crypto/axis/artpec6_crypto.c
#	drivers/crypto/bcm/cipher.c
#	drivers/crypto/caam/caamalg.c
#	drivers/crypto/caam/caamalg_qi.c
#	drivers/crypto/caam/caamalg_qi2.c
#	drivers/crypto/caam/caamhash.c
#	drivers/crypto/cavium/cpt/cptvf_algs.c
#	drivers/crypto/cavium/nitrox/nitrox_aead.c
#	drivers/crypto/cavium/nitrox/nitrox_skcipher.c
#	drivers/crypto/ccp/ccp-crypto-aes-cmac.c
#	drivers/crypto/ccp/ccp-crypto-aes-galois.c
#	drivers/crypto/ccp/ccp-crypto-aes-xts.c
#	drivers/crypto/ccp/ccp-crypto-aes.c
#	drivers/crypto/ccp/ccp-crypto-des3.c
#	drivers/crypto/ccp/ccp-crypto-sha.c
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/hisilicon/sec/sec_algs.c
#	drivers/crypto/hisilicon/sec2/sec_crypto.c
#	drivers/crypto/inside-secure/safexcel_cipher.c
#	drivers/crypto/inside-secure/safexcel_hash.c
#	drivers/crypto/ixp4xx_crypto.c
#	drivers/crypto/marvell/octeontx/otx_cptvf_algs.c
#	drivers/crypto/n2_core.c
#	drivers/crypto/picoxcell_crypto.c
#	drivers/crypto/qce/skcipher.c
#	drivers/crypto/talitos.c
#	drivers/crypto/virtio/virtio_crypto_algs.c
#	drivers/crypto/xilinx/zynqmp-aes-gcm.c
diff --cc drivers/crypto/axis/artpec6_crypto.c
index 0fb8bbf41a8d,1a46eeddf082..000000000000
--- a/drivers/crypto/axis/artpec6_crypto.c
+++ b/drivers/crypto/axis/artpec6_crypto.c
@@@ -2704,7 -2630,8 +2704,12 @@@ static struct ahash_alg hash_algos[] = 
  			.cra_name = "sha1",
  			.cra_driver_name = "artpec-sha1",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			.cra_blocksize = SHA1_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct artpec6_hashalg_context),
  			.cra_alignmask = 3,
@@@ -2727,7 -2654,8 +2732,12 @@@
  			.cra_name = "sha256",
  			.cra_driver_name = "artpec-sha256",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			.cra_blocksize = SHA256_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct artpec6_hashalg_context),
  			.cra_alignmask = 3,
@@@ -2751,7 -2679,8 +2761,12 @@@
  			.cra_name = "hmac(sha256)",
  			.cra_driver_name = "artpec-hmac-sha256",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			.cra_blocksize = SHA256_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct artpec6_hashalg_context),
  			.cra_alignmask = 3,
@@@ -2867,8 -2699,8 +2882,13 @@@ static struct skcipher_alg crypto_algos
  			.cra_name = "ecb(aes)",
  			.cra_driver_name = "artpec6-ecb-aes",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct artpec6_cryptotfm_context),
  			.cra_alignmask = 3,
@@@ -2888,8 -2720,8 +2908,13 @@@
  			.cra_name = "ctr(aes)",
  			.cra_driver_name = "artpec6-ctr-aes",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
 +				     CRYPTO_ALG_ASYNC |
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				     CRYPTO_ALG_NEED_FALLBACK,
  			.cra_blocksize = 1,
  			.cra_ctxsize = sizeof(struct artpec6_cryptotfm_context),
@@@ -2911,8 -2743,8 +2936,13 @@@
  			.cra_name = "cbc(aes)",
  			.cra_driver_name = "artpec6-cbc-aes",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct artpec6_cryptotfm_context),
  			.cra_alignmask = 3,
@@@ -2933,8 -2765,8 +2963,13 @@@
  			.cra_name = "xts(aes)",
  			.cra_driver_name = "artpec6-xts-aes",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			.cra_blocksize = 1,
  			.cra_ctxsize = sizeof(struct artpec6_cryptotfm_context),
  			.cra_alignmask = 3,
@@@ -2964,7 -2796,8 +2999,12 @@@ static struct aead_alg aead_algos[] = 
  			.cra_name = "gcm(aes)",
  			.cra_driver_name = "artpec-gcm-aes",
  			.cra_priority = 300,
++<<<<<<< HEAD
 +			.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC |
++=======
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				     CRYPTO_ALG_KERN_DRIVER_ONLY,
  			.cra_blocksize = 1,
  			.cra_ctxsize = sizeof(struct artpec6_cryptotfm_context),
diff --cc drivers/crypto/bcm/cipher.c
index 035f3b8c059b,8a7fa1ae1ade..000000000000
--- a/drivers/crypto/bcm/cipher.c
+++ b/drivers/crypto/bcm/cipher.c
@@@ -3928,8 -3841,8 +3958,13 @@@ static struct iproc_alg_s driver_algs[
  				    .cra_name = "md5",
  				    .cra_driver_name = "md5-iproc",
  				    .cra_blocksize = MD5_BLOCK_WORDS * 4,
++<<<<<<< HEAD
 +				    .cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				    .cra_flags = CRYPTO_ALG_ASYNC |
+ 						 CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				}
  		      },
  	 .cipher_info = {
@@@ -4616,23 -4535,21 +4651,33 @@@ static int spu_register_ablkcipher(stru
  	    (spu->spu_type == SPU_TYPE_SPU2))
  		return 0;
  
++<<<<<<< HEAD
 +	crypto->cra_module = THIS_MODULE;
 +	crypto->cra_priority = cipher_pri;
 +	crypto->cra_alignmask = 0;
 +	crypto->cra_ctxsize = sizeof(struct iproc_ctx_s);
 +	INIT_LIST_HEAD(&crypto->cra_list);
++=======
+ 	crypto->base.cra_module = THIS_MODULE;
+ 	crypto->base.cra_priority = cipher_pri;
+ 	crypto->base.cra_alignmask = 0;
+ 	crypto->base.cra_ctxsize = sizeof(struct iproc_ctx_s);
+ 	crypto->base.cra_flags = CRYPTO_ALG_ASYNC |
+ 				 CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				 CRYPTO_ALG_KERN_DRIVER_ONLY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
 +
 +	crypto->cra_init = ablkcipher_cra_init;
 +	crypto->cra_exit = generic_cra_exit;
 +	crypto->cra_type = &crypto_ablkcipher_type;
 +	crypto->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |
 +				CRYPTO_ALG_KERN_DRIVER_ONLY;
  
 -	crypto->init = skcipher_init_tfm;
 -	crypto->exit = skcipher_exit_tfm;
 -	crypto->setkey = skcipher_setkey;
 -	crypto->encrypt = skcipher_encrypt;
 -	crypto->decrypt = skcipher_decrypt;
 +	crypto->cra_ablkcipher.setkey = ablkcipher_setkey;
 +	crypto->cra_ablkcipher.encrypt = ablkcipher_encrypt;
 +	crypto->cra_ablkcipher.decrypt = ablkcipher_decrypt;
  
 -	err = crypto_register_skcipher(crypto);
 +	err = crypto_register_alg(crypto);
  	/* Mark alg as having been registered, if successful */
  	if (err == 0)
  		driver_alg->registered = true;
@@@ -4663,8 -4580,8 +4708,13 @@@ static int spu_register_ahash(struct ip
  	hash->halg.base.cra_ctxsize = sizeof(struct iproc_ctx_s);
  	hash->halg.base.cra_init = ahash_cra_init;
  	hash->halg.base.cra_exit = generic_cra_exit;
++<<<<<<< HEAD
 +	hash->halg.base.cra_type = &crypto_ahash_type;
 +	hash->halg.base.cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC;
++=======
+ 	hash->halg.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 				    CRYPTO_ALG_ALLOCATES_MEMORY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  	hash->halg.statesize = sizeof(struct spu_hash_export_s);
  
  	if (driver_alg->auth_info.mode != HASH_MODE_HMAC) {
@@@ -4703,9 -4624,8 +4753,13 @@@ static int spu_register_aead(struct ipr
  	aead->base.cra_priority = aead_pri;
  	aead->base.cra_alignmask = 0;
  	aead->base.cra_ctxsize = sizeof(struct iproc_ctx_s);
 +	INIT_LIST_HEAD(&aead->base.cra_list);
  
++<<<<<<< HEAD
 +	aead->base.cra_flags |= CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC;
++=======
+ 	aead->base.cra_flags |= CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  	/* setkey set in alg initialization */
  	aead->setauthsize = aead_setauthsize;
  	aead->encrypt = aead_encrypt;
diff --cc drivers/crypto/caam/caamalg.c
index 00f298134128,e94e7f27f1d0..000000000000
--- a/drivers/crypto/caam/caamalg.c
+++ b/drivers/crypto/caam/caamalg.c
@@@ -3382,47 -3405,18 +3382,55 @@@ static void __exit caam_algapi_exit(voi
  	}
  }
  
 -static void caam_skcipher_alg_init(struct caam_skcipher_alg *t_alg)
 +static struct caam_crypto_alg *caam_alg_alloc(struct caam_alg_template
 +					      *template)
  {
 -	struct skcipher_alg *alg = &t_alg->skcipher;
 +	struct caam_crypto_alg *t_alg;
 +	struct crypto_alg *alg;
  
++<<<<<<< HEAD
 +	t_alg = kzalloc(sizeof(*t_alg), GFP_KERNEL);
 +	if (!t_alg) {
 +		pr_err("failed to allocate t_alg\n");
 +		return ERR_PTR(-ENOMEM);
 +	}
++=======
+ 	alg->base.cra_module = THIS_MODULE;
+ 	alg->base.cra_priority = CAAM_CRA_PRIORITY;
+ 	alg->base.cra_ctxsize = sizeof(struct caam_ctx);
+ 	alg->base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY |
+ 			      CRYPTO_ALG_KERN_DRIVER_ONLY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
 +
 +	alg = &t_alg->crypto_alg;
 +
 +	snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", template->name);
 +	snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
 +		 template->driver_name);
 +	alg->cra_module = THIS_MODULE;
 +	alg->cra_init = caam_cra_init;
 +	alg->cra_exit = caam_cra_exit;
 +	alg->cra_priority = CAAM_CRA_PRIORITY;
 +	alg->cra_blocksize = template->blocksize;
 +	alg->cra_alignmask = 0;
 +	alg->cra_ctxsize = sizeof(struct caam_ctx);
 +	alg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY |
 +			 template->type;
 +	switch (template->type) {
 +	case CRYPTO_ALG_TYPE_GIVCIPHER:
 +		alg->cra_type = &crypto_givcipher_type;
 +		alg->cra_ablkcipher = template->template_ablkcipher;
 +		break;
 +	case CRYPTO_ALG_TYPE_ABLKCIPHER:
 +		alg->cra_type = &crypto_ablkcipher_type;
 +		alg->cra_ablkcipher = template->template_ablkcipher;
 +		break;
 +	}
  
 -	alg->init = caam_cra_init;
 -	alg->exit = caam_cra_exit;
 +	t_alg->caam.class1_alg_type = template->class1_alg_type;
 +	t_alg->caam.class2_alg_type = template->class2_alg_type;
 +
 +	return t_alg;
  }
  
  static void caam_aead_alg_init(struct caam_aead_alg *t_alg)
diff --cc drivers/crypto/caam/caamalg_qi.c
index 21727553740d,efe8f15a4a51..000000000000
--- a/drivers/crypto/caam/caamalg_qi.c
+++ b/drivers/crypto/caam/caamalg_qi.c
@@@ -2684,45 -2495,18 +2684,53 @@@ static void __exit caam_qi_algapi_exit(
  	}
  }
  
 -static void caam_skcipher_alg_init(struct caam_skcipher_alg *t_alg)
 +static struct caam_crypto_alg *caam_alg_alloc(struct caam_alg_template
 +					      *template)
  {
 -	struct skcipher_alg *alg = &t_alg->skcipher;
 +	struct caam_crypto_alg *t_alg;
 +	struct crypto_alg *alg;
  
++<<<<<<< HEAD
 +	t_alg = kzalloc(sizeof(*t_alg), GFP_KERNEL);
 +	if (!t_alg)
 +		return ERR_PTR(-ENOMEM);
++=======
+ 	alg->base.cra_module = THIS_MODULE;
+ 	alg->base.cra_priority = CAAM_CRA_PRIORITY;
+ 	alg->base.cra_ctxsize = sizeof(struct caam_ctx);
+ 	alg->base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY |
+ 			      CRYPTO_ALG_KERN_DRIVER_ONLY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
 +
 +	alg = &t_alg->crypto_alg;
 +
 +	snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", template->name);
 +	snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
 +		 template->driver_name);
 +	alg->cra_module = THIS_MODULE;
 +	alg->cra_init = caam_cra_init;
 +	alg->cra_exit = caam_cra_exit;
 +	alg->cra_priority = CAAM_CRA_PRIORITY;
 +	alg->cra_blocksize = template->blocksize;
 +	alg->cra_alignmask = 0;
 +	alg->cra_ctxsize = sizeof(struct caam_ctx);
 +	alg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY |
 +			 template->type;
 +	switch (template->type) {
 +	case CRYPTO_ALG_TYPE_GIVCIPHER:
 +		alg->cra_type = &crypto_givcipher_type;
 +		alg->cra_ablkcipher = template->template_ablkcipher;
 +		break;
 +	case CRYPTO_ALG_TYPE_ABLKCIPHER:
 +		alg->cra_type = &crypto_ablkcipher_type;
 +		alg->cra_ablkcipher = template->template_ablkcipher;
 +		break;
 +	}
  
 -	alg->init = caam_cra_init;
 -	alg->exit = caam_cra_exit;
 +	t_alg->caam.class1_alg_type = template->class1_alg_type;
 +	t_alg->caam.class2_alg_type = template->class2_alg_type;
 +
 +	return t_alg;
  }
  
  static void caam_aead_alg_init(struct caam_aead_alg *t_alg)
diff --cc drivers/crypto/caam/caamhash.c
index 0beb28196e20,e8a6d8bc43b5..000000000000
--- a/drivers/crypto/caam/caamhash.c
+++ b/drivers/crypto/caam/caamhash.c
@@@ -1846,8 -1927,7 +1846,12 @@@ caam_hash_alloc(struct caam_hash_templa
  	alg->cra_priority = CAAM_CRA_PRIORITY;
  	alg->cra_blocksize = template->blocksize;
  	alg->cra_alignmask = 0;
++<<<<<<< HEAD
 +	alg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_TYPE_AHASH;
 +	alg->cra_type = &crypto_ahash_type;
++=======
+ 	alg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  
  	t_alg->alg_type = template->alg_type;
  
diff --cc drivers/crypto/cavium/cpt/cptvf_algs.c
index 486cbc6b27aa,5af0dc2a8909..000000000000
--- a/drivers/crypto/cavium/cpt/cptvf_algs.c
+++ b/drivers/crypto/cavium/cpt/cptvf_algs.c
@@@ -369,132 -340,113 +369,242 @@@ static int cvm_enc_dec_init(struct cryp
  	return 0;
  }
  
++<<<<<<< HEAD
 +struct crypto_alg algs[] = { {
 +	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
 +	.cra_blocksize = AES_BLOCK_SIZE,
 +	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
 +	.cra_alignmask = 7,
 +	.cra_priority = 4001,
 +	.cra_name = "xts(aes)",
 +	.cra_driver_name = "cavium-xts-aes",
 +	.cra_type = &crypto_ablkcipher_type,
 +	.cra_u = {
 +		.ablkcipher = {
 +			.ivsize = AES_BLOCK_SIZE,
 +			.min_keysize = 2 * AES_MIN_KEY_SIZE,
 +			.max_keysize = 2 * AES_MAX_KEY_SIZE,
 +			.setkey = cvm_xts_setkey,
 +			.encrypt = cvm_encrypt,
 +			.decrypt = cvm_decrypt,
 +		},
 +	},
 +	.cra_init = cvm_enc_dec_init,
 +	.cra_module = THIS_MODULE,
 +}, {
 +	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
 +	.cra_blocksize = AES_BLOCK_SIZE,
 +	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
 +	.cra_alignmask = 7,
 +	.cra_priority = 4001,
 +	.cra_name = "cbc(aes)",
 +	.cra_driver_name = "cavium-cbc-aes",
 +	.cra_type = &crypto_ablkcipher_type,
 +	.cra_u = {
 +		.ablkcipher = {
 +			.ivsize = AES_BLOCK_SIZE,
 +			.min_keysize = AES_MIN_KEY_SIZE,
 +			.max_keysize = AES_MAX_KEY_SIZE,
 +			.setkey = cvm_cbc_aes_setkey,
 +			.encrypt = cvm_encrypt,
 +			.decrypt = cvm_decrypt,
 +		},
 +	},
 +	.cra_init = cvm_enc_dec_init,
 +	.cra_module = THIS_MODULE,
 +}, {
 +	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
 +	.cra_blocksize = AES_BLOCK_SIZE,
 +	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
 +	.cra_alignmask = 7,
 +	.cra_priority = 4001,
 +	.cra_name = "ecb(aes)",
 +	.cra_driver_name = "cavium-ecb-aes",
 +	.cra_type = &crypto_ablkcipher_type,
 +	.cra_u = {
 +		.ablkcipher = {
 +			.ivsize = AES_BLOCK_SIZE,
 +			.min_keysize = AES_MIN_KEY_SIZE,
 +			.max_keysize = AES_MAX_KEY_SIZE,
 +			.setkey = cvm_ecb_aes_setkey,
 +			.encrypt = cvm_encrypt,
 +			.decrypt = cvm_decrypt,
 +		},
 +	},
 +	.cra_init = cvm_enc_dec_init,
 +	.cra_module = THIS_MODULE,
 +}, {
 +	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
 +	.cra_blocksize = AES_BLOCK_SIZE,
 +	.cra_ctxsize = sizeof(struct cvm_enc_ctx),
 +	.cra_alignmask = 7,
 +	.cra_priority = 4001,
 +	.cra_name = "cfb(aes)",
 +	.cra_driver_name = "cavium-cfb-aes",
 +	.cra_type = &crypto_ablkcipher_type,
 +	.cra_u = {
 +		.ablkcipher = {
 +			.ivsize = AES_BLOCK_SIZE,
 +			.min_keysize = AES_MIN_KEY_SIZE,
 +			.max_keysize = AES_MAX_KEY_SIZE,
 +			.setkey = cvm_cfb_aes_setkey,
 +			.encrypt = cvm_encrypt,
 +			.decrypt = cvm_decrypt,
 +		},
 +	},
 +	.cra_init = cvm_enc_dec_init,
 +	.cra_module = THIS_MODULE,
 +}, {
 +	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
 +	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 +	.cra_ctxsize = sizeof(struct cvm_des3_ctx),
 +	.cra_alignmask = 7,
 +	.cra_priority = 4001,
 +	.cra_name = "cbc(des3_ede)",
 +	.cra_driver_name = "cavium-cbc-des3_ede",
 +	.cra_type = &crypto_ablkcipher_type,
 +	.cra_u = {
 +		.ablkcipher = {
 +			.min_keysize = DES3_EDE_KEY_SIZE,
 +			.max_keysize = DES3_EDE_KEY_SIZE,
 +			.ivsize = DES_BLOCK_SIZE,
 +			.setkey = cvm_cbc_des3_setkey,
 +			.encrypt = cvm_encrypt,
 +			.decrypt = cvm_decrypt,
 +		},
 +	},
 +	.cra_init = cvm_enc_dec_init,
 +	.cra_module = THIS_MODULE,
 +}, {
 +	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
 +	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 +	.cra_ctxsize = sizeof(struct cvm_des3_ctx),
 +	.cra_alignmask = 7,
 +	.cra_priority = 4001,
 +	.cra_name = "ecb(des3_ede)",
 +	.cra_driver_name = "cavium-ecb-des3_ede",
 +	.cra_type = &crypto_ablkcipher_type,
 +	.cra_u = {
 +		.ablkcipher = {
 +			.min_keysize = DES3_EDE_KEY_SIZE,
 +			.max_keysize = DES3_EDE_KEY_SIZE,
 +			.ivsize = DES_BLOCK_SIZE,
 +			.setkey = cvm_ecb_des3_setkey,
 +			.encrypt = cvm_encrypt,
 +			.decrypt = cvm_decrypt,
 +		},
 +	},
 +	.cra_init = cvm_enc_dec_init,
 +	.cra_module = THIS_MODULE,
++=======
+ static struct skcipher_alg algs[] = { {
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 	.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct cvm_enc_ctx),
+ 	.base.cra_alignmask	= 7,
+ 	.base.cra_priority	= 4001,
+ 	.base.cra_name		= "xts(aes)",
+ 	.base.cra_driver_name	= "cavium-xts-aes",
+ 	.base.cra_module	= THIS_MODULE,
+ 
+ 	.ivsize			= AES_BLOCK_SIZE,
+ 	.min_keysize		= 2 * AES_MIN_KEY_SIZE,
+ 	.max_keysize		= 2 * AES_MAX_KEY_SIZE,
+ 	.setkey			= cvm_xts_setkey,
+ 	.encrypt		= cvm_encrypt,
+ 	.decrypt		= cvm_decrypt,
+ 	.init			= cvm_enc_dec_init,
+ }, {
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 	.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct cvm_enc_ctx),
+ 	.base.cra_alignmask	= 7,
+ 	.base.cra_priority	= 4001,
+ 	.base.cra_name		= "cbc(aes)",
+ 	.base.cra_driver_name	= "cavium-cbc-aes",
+ 	.base.cra_module	= THIS_MODULE,
+ 
+ 	.ivsize			= AES_BLOCK_SIZE,
+ 	.min_keysize		= AES_MIN_KEY_SIZE,
+ 	.max_keysize		= AES_MAX_KEY_SIZE,
+ 	.setkey			= cvm_cbc_aes_setkey,
+ 	.encrypt		= cvm_encrypt,
+ 	.decrypt		= cvm_decrypt,
+ 	.init			= cvm_enc_dec_init,
+ }, {
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 	.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct cvm_enc_ctx),
+ 	.base.cra_alignmask	= 7,
+ 	.base.cra_priority	= 4001,
+ 	.base.cra_name		= "ecb(aes)",
+ 	.base.cra_driver_name	= "cavium-ecb-aes",
+ 	.base.cra_module	= THIS_MODULE,
+ 
+ 	.min_keysize		= AES_MIN_KEY_SIZE,
+ 	.max_keysize		= AES_MAX_KEY_SIZE,
+ 	.setkey			= cvm_ecb_aes_setkey,
+ 	.encrypt		= cvm_encrypt,
+ 	.decrypt		= cvm_decrypt,
+ 	.init			= cvm_enc_dec_init,
+ }, {
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 	.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct cvm_enc_ctx),
+ 	.base.cra_alignmask	= 7,
+ 	.base.cra_priority	= 4001,
+ 	.base.cra_name		= "cfb(aes)",
+ 	.base.cra_driver_name	= "cavium-cfb-aes",
+ 	.base.cra_module	= THIS_MODULE,
+ 
+ 	.ivsize			= AES_BLOCK_SIZE,
+ 	.min_keysize		= AES_MIN_KEY_SIZE,
+ 	.max_keysize		= AES_MAX_KEY_SIZE,
+ 	.setkey			= cvm_cfb_aes_setkey,
+ 	.encrypt		= cvm_encrypt,
+ 	.decrypt		= cvm_decrypt,
+ 	.init			= cvm_enc_dec_init,
+ }, {
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 	.base.cra_blocksize	= DES3_EDE_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct cvm_des3_ctx),
+ 	.base.cra_alignmask	= 7,
+ 	.base.cra_priority	= 4001,
+ 	.base.cra_name		= "cbc(des3_ede)",
+ 	.base.cra_driver_name	= "cavium-cbc-des3_ede",
+ 	.base.cra_module	= THIS_MODULE,
+ 
+ 	.min_keysize		= DES3_EDE_KEY_SIZE,
+ 	.max_keysize		= DES3_EDE_KEY_SIZE,
+ 	.ivsize			= DES_BLOCK_SIZE,
+ 	.setkey			= cvm_cbc_des3_setkey,
+ 	.encrypt		= cvm_encrypt,
+ 	.decrypt		= cvm_decrypt,
+ 	.init			= cvm_enc_dec_init,
+ }, {
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 	.base.cra_blocksize	= DES3_EDE_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct cvm_des3_ctx),
+ 	.base.cra_alignmask	= 7,
+ 	.base.cra_priority	= 4001,
+ 	.base.cra_name		= "ecb(des3_ede)",
+ 	.base.cra_driver_name	= "cavium-ecb-des3_ede",
+ 	.base.cra_module	= THIS_MODULE,
+ 
+ 	.min_keysize		= DES3_EDE_KEY_SIZE,
+ 	.max_keysize		= DES3_EDE_KEY_SIZE,
+ 	.ivsize			= DES_BLOCK_SIZE,
+ 	.setkey			= cvm_ecb_des3_setkey,
+ 	.encrypt		= cvm_encrypt,
+ 	.decrypt		= cvm_decrypt,
+ 	.init			= cvm_enc_dec_init,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  } };
  
  static inline int cav_register_algs(void)
diff --cc drivers/crypto/ccp/ccp-crypto-aes-cmac.c
index b40eab055048,11a305fa19e6..000000000000
--- a/drivers/crypto/ccp/ccp-crypto-aes-cmac.c
+++ b/drivers/crypto/ccp/ccp-crypto-aes-cmac.c
@@@ -393,7 -377,8 +393,12 @@@ int ccp_register_aes_cmac_algs(struct l
  	base = &halg->base;
  	snprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, "cmac(aes)");
  	snprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, "cmac-aes-ccp");
++<<<<<<< HEAD
 +	base->cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC |
++=======
+ 	base->cra_flags = CRYPTO_ALG_ASYNC |
+ 			  CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			  CRYPTO_ALG_KERN_DRIVER_ONLY |
  			  CRYPTO_ALG_NEED_FALLBACK;
  	base->cra_blocksize = AES_BLOCK_SIZE;
diff --cc drivers/crypto/ccp/ccp-crypto-aes-galois.c
index 4062c800b0eb,1c1c939f5c39..000000000000
--- a/drivers/crypto/ccp/ccp-crypto-aes-galois.c
+++ b/drivers/crypto/ccp/ccp-crypto-aes-galois.c
@@@ -171,8 -171,8 +171,13 @@@ static struct aead_alg ccp_aes_gcm_defa
  	.ivsize = GCM_AES_IV_SIZE,
  	.maxauthsize = AES_BLOCK_SIZE,
  	.base = {
++<<<<<<< HEAD
 +		.cra_flags	= CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				  CRYPTO_ALG_ASYNC |
++=======
+ 		.cra_flags	= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				  CRYPTO_ALG_KERN_DRIVER_ONLY |
  				  CRYPTO_ALG_NEED_FALLBACK,
  		.cra_blocksize	= AES_BLOCK_SIZE,
diff --cc drivers/crypto/ccp/ccp-crypto-aes-xts.c
index 469cb8baef0b,6849261ca47d..000000000000
--- a/drivers/crypto/ccp/ccp-crypto-aes-xts.c
+++ b/drivers/crypto/ccp/ccp-crypto-aes-xts.c
@@@ -239,30 -238,31 +239,41 @@@ static int ccp_register_aes_xts_alg(str
  
  	alg = &ccp_alg->alg;
  
 -	snprintf(alg->base.cra_name, CRYPTO_MAX_ALG_NAME, "%s", def->name);
 -	snprintf(alg->base.cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
 +	snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", def->name);
 +	snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
  		 def->drv_name);
++<<<<<<< HEAD
 +	alg->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |
 +			 CRYPTO_ALG_KERN_DRIVER_ONLY |
 +			 CRYPTO_ALG_NEED_FALLBACK;
 +	alg->cra_blocksize = AES_BLOCK_SIZE;
 +	alg->cra_ctxsize = sizeof(struct ccp_ctx);
 +	alg->cra_priority = CCP_CRA_PRIORITY;
 +	alg->cra_type = &crypto_ablkcipher_type;
 +	alg->cra_ablkcipher.setkey = ccp_aes_xts_setkey;
 +	alg->cra_ablkcipher.encrypt = ccp_aes_xts_encrypt;
 +	alg->cra_ablkcipher.decrypt = ccp_aes_xts_decrypt;
 +	alg->cra_ablkcipher.min_keysize = AES_MIN_KEY_SIZE * 2;
 +	alg->cra_ablkcipher.max_keysize = AES_MAX_KEY_SIZE * 2;
 +	alg->cra_ablkcipher.ivsize = AES_BLOCK_SIZE;
 +	alg->cra_init = ccp_aes_xts_cra_init;
 +	alg->cra_exit = ccp_aes_xts_cra_exit;
 +	alg->cra_module = THIS_MODULE;
++=======
+ 	alg->base.cra_flags	= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				  CRYPTO_ALG_NEED_FALLBACK;
+ 	alg->base.cra_blocksize	= AES_BLOCK_SIZE;
+ 	alg->base.cra_ctxsize	= sizeof(struct ccp_ctx);
+ 	alg->base.cra_priority	= CCP_CRA_PRIORITY;
+ 	alg->base.cra_module	= THIS_MODULE;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  
 -	alg->setkey		= ccp_aes_xts_setkey;
 -	alg->encrypt		= ccp_aes_xts_encrypt;
 -	alg->decrypt		= ccp_aes_xts_decrypt;
 -	alg->min_keysize	= AES_MIN_KEY_SIZE * 2;
 -	alg->max_keysize	= AES_MAX_KEY_SIZE * 2;
 -	alg->ivsize		= AES_BLOCK_SIZE;
 -	alg->init		= ccp_aes_xts_init_tfm;
 -	alg->exit		= ccp_aes_xts_exit_tfm;
 -
 -	ret = crypto_register_skcipher(alg);
 +	ret = crypto_register_alg(alg);
  	if (ret) {
 -		pr_err("%s skcipher algorithm registration error (%d)\n",
 -		       alg->base.cra_name, ret);
 +		pr_err("%s ablkcipher algorithm registration error (%d)\n",
 +		       alg->cra_name, ret);
  		kfree(ccp_alg);
  		return ret;
  	}
diff --cc drivers/crypto/ccp/ccp-crypto-aes.c
index 560addac91a9,e6dcd8cedd53..000000000000
--- a/drivers/crypto/ccp/ccp-crypto-aes.c
+++ b/drivers/crypto/ccp/ccp-crypto-aes.c
@@@ -206,50 -203,40 +206,79 @@@ static int ccp_aes_rfc3686_cra_init(str
  	return 0;
  }
  
 -static const struct skcipher_alg ccp_aes_defaults = {
 -	.setkey			= ccp_aes_setkey,
 -	.encrypt		= ccp_aes_encrypt,
 -	.decrypt		= ccp_aes_decrypt,
 -	.min_keysize		= AES_MIN_KEY_SIZE,
 -	.max_keysize		= AES_MAX_KEY_SIZE,
 -	.init			= ccp_aes_init_tfm,
 +static void ccp_aes_rfc3686_cra_exit(struct crypto_tfm *tfm)
 +{
 +}
  
++<<<<<<< HEAD
 +static struct crypto_alg ccp_aes_defaults = {
 +	.cra_flags	= CRYPTO_ALG_TYPE_ABLKCIPHER |
 +			  CRYPTO_ALG_ASYNC |
 +			  CRYPTO_ALG_KERN_DRIVER_ONLY |
 +			  CRYPTO_ALG_NEED_FALLBACK,
 +	.cra_blocksize	= AES_BLOCK_SIZE,
 +	.cra_ctxsize	= sizeof(struct ccp_ctx),
 +	.cra_priority	= CCP_CRA_PRIORITY,
 +	.cra_type	= &crypto_ablkcipher_type,
 +	.cra_init	= ccp_aes_cra_init,
 +	.cra_exit	= ccp_aes_cra_exit,
 +	.cra_module	= THIS_MODULE,
 +	.cra_ablkcipher	= {
 +		.setkey		= ccp_aes_setkey,
 +		.encrypt	= ccp_aes_encrypt,
 +		.decrypt	= ccp_aes_decrypt,
 +		.min_keysize	= AES_MIN_KEY_SIZE,
 +		.max_keysize	= AES_MAX_KEY_SIZE,
 +	},
 +};
 +
 +static struct crypto_alg ccp_aes_rfc3686_defaults = {
 +	.cra_flags	= CRYPTO_ALG_TYPE_ABLKCIPHER |
 +			   CRYPTO_ALG_ASYNC |
 +			   CRYPTO_ALG_KERN_DRIVER_ONLY |
 +			   CRYPTO_ALG_NEED_FALLBACK,
 +	.cra_blocksize	= CTR_RFC3686_BLOCK_SIZE,
 +	.cra_ctxsize	= sizeof(struct ccp_ctx),
 +	.cra_priority	= CCP_CRA_PRIORITY,
 +	.cra_type	= &crypto_ablkcipher_type,
 +	.cra_init	= ccp_aes_rfc3686_cra_init,
 +	.cra_exit	= ccp_aes_rfc3686_cra_exit,
 +	.cra_module	= THIS_MODULE,
 +	.cra_ablkcipher	= {
 +		.setkey		= ccp_aes_rfc3686_setkey,
 +		.encrypt	= ccp_aes_rfc3686_encrypt,
 +		.decrypt	= ccp_aes_rfc3686_decrypt,
 +		.min_keysize	= AES_MIN_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
 +		.max_keysize	= AES_MAX_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
 +	},
++=======
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				  CRYPTO_ALG_NEED_FALLBACK,
+ 	.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct ccp_ctx),
+ 	.base.cra_priority	= CCP_CRA_PRIORITY,
+ 	.base.cra_module	= THIS_MODULE,
+ };
+ 
+ static const struct skcipher_alg ccp_aes_rfc3686_defaults = {
+ 	.setkey			= ccp_aes_rfc3686_setkey,
+ 	.encrypt		= ccp_aes_rfc3686_encrypt,
+ 	.decrypt		= ccp_aes_rfc3686_decrypt,
+ 	.min_keysize		= AES_MIN_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 	.max_keysize		= AES_MAX_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 	.init			= ccp_aes_rfc3686_init_tfm,
+ 
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				  CRYPTO_ALG_NEED_FALLBACK,
+ 	.base.cra_blocksize	= CTR_RFC3686_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct ccp_ctx),
+ 	.base.cra_priority	= CCP_CRA_PRIORITY,
+ 	.base.cra_module	= THIS_MODULE,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  };
  
  struct ccp_aes_def {
diff --cc drivers/crypto/ccp/ccp-crypto-des3.c
index 6ff9de1b4546,ec97daf0fcb7..000000000000
--- a/drivers/crypto/ccp/ccp-crypto-des3.c
+++ b/drivers/crypto/ccp/ccp-crypto-des3.c
@@@ -128,29 -127,22 +128,40 @@@ static int ccp_des3_cra_init(struct cry
  	return 0;
  }
  
 -static const struct skcipher_alg ccp_des3_defaults = {
 -	.setkey			= ccp_des3_setkey,
 -	.encrypt		= ccp_des3_encrypt,
 -	.decrypt		= ccp_des3_decrypt,
 -	.min_keysize		= DES3_EDE_KEY_SIZE,
 -	.max_keysize		= DES3_EDE_KEY_SIZE,
 -	.init			= ccp_des3_init_tfm,
 +static void ccp_des3_cra_exit(struct crypto_tfm *tfm)
 +{
 +}
  
++<<<<<<< HEAD
 +static struct crypto_alg ccp_des3_defaults = {
 +	.cra_flags	= CRYPTO_ALG_TYPE_ABLKCIPHER |
 +		CRYPTO_ALG_ASYNC |
 +		CRYPTO_ALG_KERN_DRIVER_ONLY |
 +		CRYPTO_ALG_NEED_FALLBACK,
 +	.cra_blocksize	= DES3_EDE_BLOCK_SIZE,
 +	.cra_ctxsize	= sizeof(struct ccp_ctx),
 +	.cra_priority	= CCP_CRA_PRIORITY,
 +	.cra_type	= &crypto_ablkcipher_type,
 +	.cra_init	= ccp_des3_cra_init,
 +	.cra_exit	= ccp_des3_cra_exit,
 +	.cra_module	= THIS_MODULE,
 +	.cra_ablkcipher	= {
 +		.setkey		= ccp_des3_setkey,
 +		.encrypt	= ccp_des3_encrypt,
 +		.decrypt	= ccp_des3_decrypt,
 +		.min_keysize	= DES3_EDE_KEY_SIZE,
 +		.max_keysize	= DES3_EDE_KEY_SIZE,
 +	},
++=======
+ 	.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 				  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				  CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				  CRYPTO_ALG_NEED_FALLBACK,
+ 	.base.cra_blocksize	= DES3_EDE_BLOCK_SIZE,
+ 	.base.cra_ctxsize	= sizeof(struct ccp_ctx),
+ 	.base.cra_priority	= CCP_CRA_PRIORITY,
+ 	.base.cra_module	= THIS_MODULE,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  };
  
  struct ccp_des3_def {
diff --cc drivers/crypto/ccp/ccp-crypto-sha.c
index 4bf6c1a7b9ee,8fbfdb9e8cd3..000000000000
--- a/drivers/crypto/ccp/ccp-crypto-sha.c
+++ b/drivers/crypto/ccp/ccp-crypto-sha.c
@@@ -490,7 -486,8 +490,12 @@@ static int ccp_register_sha_alg(struct 
  	snprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, "%s", def->name);
  	snprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
  		 def->drv_name);
++<<<<<<< HEAD
 +	base->cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC |
++=======
+ 	base->cra_flags = CRYPTO_ALG_ASYNC |
+ 			  CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			  CRYPTO_ALG_KERN_DRIVER_ONLY |
  			  CRYPTO_ALG_NEED_FALLBACK;
  	base->cra_blocksize = def->block_size;
diff --cc drivers/crypto/chelsio/chcr_algo.c
index f9f74edb7306,9b68d62149ad..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -4423,26 -4428,26 +4423,38 @@@ static int chcr_register_alg(void
  		if (driver_algs[i].is_registered)
  			continue;
  		switch (driver_algs[i].type & CRYPTO_ALG_TYPE_MASK) {
 -		case CRYPTO_ALG_TYPE_SKCIPHER:
 -			driver_algs[i].alg.skcipher.base.cra_priority =
 +		case CRYPTO_ALG_TYPE_ABLKCIPHER:
 +			driver_algs[i].alg.crypto.cra_priority =
  				CHCR_CRA_PRIORITY;
++<<<<<<< HEAD
 +			driver_algs[i].alg.crypto.cra_module = THIS_MODULE;
 +			driver_algs[i].alg.crypto.cra_flags =
 +				CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |
++=======
+ 			driver_algs[i].alg.skcipher.base.cra_module = THIS_MODULE;
+ 			driver_algs[i].alg.skcipher.base.cra_flags =
+ 				CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_ASYNC |
+ 				CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				CRYPTO_ALG_NEED_FALLBACK;
 -			driver_algs[i].alg.skcipher.base.cra_ctxsize =
 +			driver_algs[i].alg.crypto.cra_ctxsize =
  				sizeof(struct chcr_context) +
  				sizeof(struct ablk_ctx);
 -			driver_algs[i].alg.skcipher.base.cra_alignmask = 0;
 -
 -			err = crypto_register_skcipher(&driver_algs[i].alg.skcipher);
 -			name = driver_algs[i].alg.skcipher.base.cra_driver_name;
 +			driver_algs[i].alg.crypto.cra_alignmask = 0;
 +			driver_algs[i].alg.crypto.cra_type =
 +				&crypto_ablkcipher_type;
 +			err = crypto_register_alg(&driver_algs[i].alg.crypto);
 +			name = driver_algs[i].alg.crypto.cra_driver_name;
  			break;
  		case CRYPTO_ALG_TYPE_AEAD:
  			driver_algs[i].alg.aead.base.cra_flags =
++<<<<<<< HEAD
 +				CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC |
 +				CRYPTO_ALG_NEED_FALLBACK;
++=======
+ 				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK |
+ 				CRYPTO_ALG_ALLOCATES_MEMORY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			driver_algs[i].alg.aead.encrypt = chcr_aead_encrypt;
  			driver_algs[i].alg.aead.decrypt = chcr_aead_decrypt;
  			driver_algs[i].alg.aead.init = chcr_aead_cra_init;
@@@ -4462,10 -4467,10 +4474,15 @@@
  			a_hash->halg.statesize = SZ_AHASH_REQ_CTX;
  			a_hash->halg.base.cra_priority = CHCR_CRA_PRIORITY;
  			a_hash->halg.base.cra_module = THIS_MODULE;
++<<<<<<< HEAD
 +			a_hash->halg.base.cra_flags = AHASH_CRA_FLAGS;
++=======
+ 			a_hash->halg.base.cra_flags =
+ 				CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			a_hash->halg.base.cra_alignmask = 0;
  			a_hash->halg.base.cra_exit = NULL;
 +			a_hash->halg.base.cra_type = &crypto_ahash_type;
  
  			if (driver_algs[i].type == CRYPTO_ALG_TYPE_HMAC) {
  				a_hash->halg.base.cra_init = chcr_hmac_cra_init;
diff --cc drivers/crypto/inside-secure/safexcel_cipher.c
index 6bb60fda2043,1ac3253b7903..000000000000
--- a/drivers/crypto/inside-secure/safexcel_cipher.c
+++ b/drivers/crypto/inside-secure/safexcel_cipher.c
@@@ -837,8 -1298,9 +837,14 @@@ struct safexcel_alg_template safexcel_a
  		.base = {
  			.cra_name = "ecb(aes)",
  			.cra_driver_name = "safexcel-ecb-aes",
++<<<<<<< HEAD
 +			.cra_priority = 300,
 +			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_ASYNC |
++=======
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				     CRYPTO_ALG_KERN_DRIVER_ONLY,
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
@@@ -874,13 -1336,363 +880,372 @@@ struct safexcel_alg_template safexcel_a
  		.base = {
  			.cra_name = "cbc(aes)",
  			.cra_driver_name = "safexcel-cbc-aes",
++<<<<<<< HEAD
 +			.cra_priority = 300,
 +			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_ASYNC |
++=======
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				     CRYPTO_ALG_KERN_DRIVER_ONLY,
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
  			.cra_alignmask = 0,
++<<<<<<< HEAD
 +			.cra_init = safexcel_skcipher_cra_init,
++=======
+ 			.cra_init = safexcel_skcipher_aes_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aes_cfb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cfb_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aes_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = AES_MIN_KEY_SIZE,
+ 		.max_keysize = AES_MAX_KEY_SIZE,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cfb(aes)",
+ 			.cra_driver_name = "safexcel-cfb-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_cfb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aes_ofb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_OFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ofb_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aes_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = AES_MIN_KEY_SIZE,
+ 		.max_keysize = AES_MAX_KEY_SIZE,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "ofb(aes)",
+ 			.cra_driver_name = "safexcel-ofb-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_ofb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aesctr_setkey(struct crypto_skcipher *ctfm,
+ 					   const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 	unsigned int keylen;
+ 
+ 	/* last 4 bytes of key are the nonce! */
+ 	ctx->nonce = *(u32 *)(key + len - CTR_RFC3686_NONCE_SIZE);
+ 	/* exclude the nonce here */
+ 	keylen = len - CTR_RFC3686_NONCE_SIZE;
+ 	ret = aes_expandkey(&aes, key, keylen);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < keylen / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < keylen / sizeof(u32); i++)
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 
+ 	ctx->key_len = keylen;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_aes_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aesctr_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		/* Add nonce size */
+ 		.min_keysize = AES_MIN_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.max_keysize = AES_MAX_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc3686(ctr(aes))",
+ 			.cra_driver_name = "safexcel-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_ctr_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_des_setkey(struct crypto_skcipher *ctfm, const u8 *key,
+ 			       unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_skcipher_ctx(ctfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	int ret;
+ 
+ 	ret = verify_skcipher_des_key(ctfm, key);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* if context exits and key changed, need to invalidate it */
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, len))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, len);
+ 	ctx->key_len = len;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_des_cbc_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_DES;
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CBC;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES_KEY_SIZE,
+ 		.max_keysize = DES_KEY_SIZE,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cbc(des)",
+ 			.cra_driver_name = "safexcel-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_des_ecb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_DES;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_ECB;
+ 	ctx->blocksz = 0;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ecb_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES_KEY_SIZE,
+ 		.max_keysize = DES_KEY_SIZE,
+ 		.base = {
+ 			.cra_name = "ecb(des)",
+ 			.cra_driver_name = "safexcel-ecb-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des_ecb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_des3_ede_setkey(struct crypto_skcipher *ctfm,
+ 				   const u8 *key, unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_skcipher_ctx(ctfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	int err;
+ 
+ 	err = verify_skcipher_des3_key(ctfm, key);
+ 	if (err)
+ 		return err;
+ 
+ 	/* if context exits and key changed, need to invalidate it */
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, len))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, len);
+ 	ctx->key_len = len;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_des3_cbc_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_3DES;
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CBC;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des3_ede_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES3_EDE_KEY_SIZE,
+ 		.max_keysize = DES3_EDE_KEY_SIZE,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cbc(des3_ede)",
+ 			.cra_driver_name = "safexcel-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des3_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_des3_ecb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_3DES;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_ECB;
+ 	ctx->blocksz = 0;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ecb_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_DES,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_des3_ede_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = DES3_EDE_KEY_SIZE,
+ 		.max_keysize = DES3_EDE_KEY_SIZE,
+ 		.base = {
+ 			.cra_name = "ecb(des3_ede)",
+ 			.cra_driver_name = "safexcel-ecb-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_des3_ecb_cra_init,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			.cra_exit = safexcel_skcipher_cra_exit,
  			.cra_module = THIS_MODULE,
  		},
@@@ -942,8 -1758,9 +1307,14 @@@ struct safexcel_alg_template safexcel_a
  		.base = {
  			.cra_name = "authenc(hmac(sha1),cbc(aes))",
  			.cra_driver_name = "safexcel-authenc-hmac-sha1-cbc-aes",
++<<<<<<< HEAD
 +			.cra_priority = 300,
 +			.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC |
++=======
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				     CRYPTO_ALG_KERN_DRIVER_ONLY,
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
@@@ -976,8 -1794,9 +1347,14 @@@ struct safexcel_alg_template safexcel_a
  		.base = {
  			.cra_name = "authenc(hmac(sha256),cbc(aes))",
  			.cra_driver_name = "safexcel-authenc-hmac-sha256-cbc-aes",
++<<<<<<< HEAD
 +			.cra_priority = 300,
 +			.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC |
++=======
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				     CRYPTO_ALG_KERN_DRIVER_ONLY,
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
@@@ -1010,8 -1830,9 +1387,14 @@@ struct safexcel_alg_template safexcel_a
  		.base = {
  			.cra_name = "authenc(hmac(sha224),cbc(aes))",
  			.cra_driver_name = "safexcel-authenc-hmac-sha224-cbc-aes",
++<<<<<<< HEAD
 +			.cra_priority = 300,
 +			.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC |
++=======
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  				     CRYPTO_ALG_KERN_DRIVER_ONLY,
  			.cra_blocksize = AES_BLOCK_SIZE,
  			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
@@@ -1022,3 -1843,1930 +1405,1933 @@@
  		},
  	},
  };
++<<<<<<< HEAD
++=======
+ 
+ static int safexcel_aead_sha512_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;
+ 	ctx->state_sz = SHA512_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_cbc_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),cbc(aes))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-cbc-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = AES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;
+ 	ctx->state_sz = SHA512_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_cbc_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),cbc(aes))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-cbc-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = AES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha1_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha1_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha1_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha256_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha256_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha256_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA256_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha256),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha256-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha256_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha224_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha224_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha224_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA224_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha224),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha224-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha224_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha512_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha512_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_des3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha384_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_3DES; /* override default */
+ 	ctx->blocksz = DES3_EDE_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_cbc_des3_ede = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES3_EDE_BLOCK_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),cbc(des3_ede))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-cbc-des3_ede",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_des3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha1_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha1_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha1_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha256_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha256_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha256_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA256_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha256),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha256-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha256_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha224_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha224_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha224_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA224_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha224),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha224-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha224_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha512_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha512_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_des_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha384_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_DES; /* override default */
+ 	ctx->blocksz = DES_BLOCK_SIZE;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_cbc_des = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_DES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = DES_BLOCK_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),cbc(des))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-cbc-des",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = DES_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_des_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha1_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha1_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha1_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha256_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha256_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha256_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA256_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha256),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha256-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha256_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha224_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha224_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha224_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_256,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA224_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha224),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha224-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha224_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha512_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha512_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha512_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA512_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha512),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha512-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha512_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sha384_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sha384_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD; /* override default */
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha384_ctr_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_SHA2_512,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA384_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha384),rfc3686(ctr(aes)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha384-ctr-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sha384_ctr_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_aesxts_setkey(struct crypto_skcipher *ctfm,
+ 					   const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 	unsigned int keylen;
+ 
+ 	/* Check for illegal XTS keys */
+ 	ret = xts_verify_key(ctfm, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Only half of the key data is cipher key */
+ 	keylen = (len >> 1);
+ 	ret = aes_expandkey(&aes, key, keylen);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < keylen / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < keylen / sizeof(u32); i++)
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 
+ 	/* The other half is the tweak key */
+ 	ret = aes_expandkey(&aes, (u8 *)(key + keylen), keylen);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < keylen / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i + keylen / sizeof(u32)]) !=
+ 			    aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < keylen / sizeof(u32); i++)
+ 		ctx->key[i + keylen / sizeof(u32)] =
+ 			cpu_to_le32(aes.key_enc[i]);
+ 
+ 	ctx->key_len = keylen << 1;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_aes_xts_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_AES;
+ 	ctx->blocksz = AES_BLOCK_SIZE;
+ 	ctx->xts  = 1;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_XTS;
+ 	return 0;
+ }
+ 
+ static int safexcel_encrypt_xts(struct skcipher_request *req)
+ {
+ 	if (req->cryptlen < XTS_BLOCK_SIZE)
+ 		return -EINVAL;
+ 	return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 				  SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_decrypt_xts(struct skcipher_request *req)
+ {
+ 	if (req->cryptlen < XTS_BLOCK_SIZE)
+ 		return -EINVAL;
+ 	return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 				  SAFEXCEL_DECRYPT);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_xts_aes = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_AES_XTS,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_aesxts_setkey,
+ 		.encrypt = safexcel_encrypt_xts,
+ 		.decrypt = safexcel_decrypt_xts,
+ 		/* XTS actually uses 2 AES keys glued together */
+ 		.min_keysize = AES_MIN_KEY_SIZE * 2,
+ 		.max_keysize = AES_MAX_KEY_SIZE * 2,
+ 		.ivsize = XTS_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "xts(aes)",
+ 			.cra_driver_name = "safexcel-xts-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = XTS_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_aes_xts_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_gcm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				    unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	u32 hashkey[AES_BLOCK_SIZE >> 2];
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret) {
+ 		memzero_explicit(&aes, sizeof(aes));
+ 		return ret;
+ 	}
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < len / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < len / sizeof(u32); i++)
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 
+ 	ctx->key_len = len;
+ 
+ 	/* Compute hash key by encrypting zeroes with cipher key */
+ 	crypto_cipher_clear_flags(ctx->hkaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->hkaes, crypto_aead_get_flags(ctfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->hkaes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	memset(hashkey, 0, AES_BLOCK_SIZE);
+ 	crypto_cipher_encrypt_one(ctx->hkaes, (u8 *)hashkey, (u8 *)hashkey);
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < AES_BLOCK_SIZE / sizeof(u32); i++) {
+ 			if (be32_to_cpu(ctx->ipad[i]) != hashkey[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < AES_BLOCK_SIZE / sizeof(u32); i++)
+ 		ctx->ipad[i] = cpu_to_be32(hashkey[i]);
+ 
+ 	memzero_explicit(hashkey, AES_BLOCK_SIZE);
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_gcm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_GHASH;
+ 	ctx->state_sz = GHASH_BLOCK_SIZE;
+ 	ctx->xcm = EIP197_XCM_MODE_GCM;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_XCM; /* override default */
+ 
+ 	ctx->hkaes = crypto_alloc_cipher("aes", 0, 0);
+ 	return PTR_ERR_OR_ZERO(ctx->hkaes);
+ }
+ 
+ static void safexcel_aead_gcm_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_cipher(ctx->hkaes);
+ 	safexcel_aead_cra_exit(tfm);
+ }
+ 
+ static int safexcel_aead_gcm_setauthsize(struct crypto_aead *tfm,
+ 					 unsigned int authsize)
+ {
+ 	return crypto_gcm_check_authsize(authsize);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_gcm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_GHASH,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_gcm_setkey,
+ 		.setauthsize = safexcel_aead_gcm_setauthsize,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = GCM_AES_IV_SIZE,
+ 		.maxauthsize = GHASH_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "gcm(aes)",
+ 			.cra_driver_name = "safexcel-gcm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_gcm_cra_init,
+ 			.cra_exit = safexcel_aead_gcm_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_ccm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				    unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret) {
+ 		memzero_explicit(&aes, sizeof(aes));
+ 		return ret;
+ 	}
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma) {
+ 		for (i = 0; i < len / sizeof(u32); i++) {
+ 			if (le32_to_cpu(ctx->key[i]) != aes.key_enc[i]) {
+ 				ctx->base.needs_inv = true;
+ 				break;
+ 			}
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < len / sizeof(u32); i++) {
+ 		ctx->key[i] = cpu_to_le32(aes.key_enc[i]);
+ 		ctx->ipad[i + 2 * AES_BLOCK_SIZE / sizeof(u32)] =
+ 			cpu_to_be32(aes.key_enc[i]);
+ 	}
+ 
+ 	ctx->key_len = len;
+ 	ctx->state_sz = 2 * AES_BLOCK_SIZE + len;
+ 
+ 	if (len == AES_KEYSIZE_192)
+ 		ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;
+ 	else if (len == AES_KEYSIZE_256)
+ 		ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;
+ 	else
+ 		ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_ccm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 	ctx->state_sz = 3 * AES_BLOCK_SIZE;
+ 	ctx->xcm = EIP197_XCM_MODE_CCM;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_XCM; /* override default */
+ 	ctx->ctrinit = 0;
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_ccm_setauthsize(struct crypto_aead *tfm,
+ 					 unsigned int authsize)
+ {
+ 	/* Borrowed from crypto/ccm.c */
+ 	switch (authsize) {
+ 	case 4:
+ 	case 6:
+ 	case 8:
+ 	case 10:
+ 	case 12:
+ 	case 14:
+ 	case 16:
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_ccm_encrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	if (req->iv[0] < 1 || req->iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_ccm_decrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	if (req->iv[0] < 1 || req->iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_DECRYPT);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ccm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_CBC_MAC_ALL,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_ccm_setkey,
+ 		.setauthsize = safexcel_aead_ccm_setauthsize,
+ 		.encrypt = safexcel_ccm_encrypt,
+ 		.decrypt = safexcel_ccm_decrypt,
+ 		.ivsize = AES_BLOCK_SIZE,
+ 		.maxauthsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "ccm(aes)",
+ 			.cra_driver_name = "safexcel-ccm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_ccm_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static void safexcel_chacha20_setkey(struct safexcel_cipher_ctx *ctx,
+ 				     const u8 *key)
+ {
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, CHACHA_KEY_SIZE))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, CHACHA_KEY_SIZE);
+ 	ctx->key_len = CHACHA_KEY_SIZE;
+ }
+ 
+ static int safexcel_skcipher_chacha20_setkey(struct crypto_skcipher *ctfm,
+ 					     const u8 *key, unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_skcipher_ctx(ctfm);
+ 
+ 	if (len != CHACHA_KEY_SIZE)
+ 		return -EINVAL;
+ 
+ 	safexcel_chacha20_setkey(ctx, key);
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_skcipher_chacha20_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_CHACHA20;
+ 	ctx->ctrinit = 0;
+ 	ctx->mode = CONTEXT_CONTROL_CHACHA20_MODE_256_32;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_chacha20 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_CHACHA20,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_chacha20_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = CHACHA_KEY_SIZE,
+ 		.max_keysize = CHACHA_KEY_SIZE,
+ 		.ivsize = CHACHA_IV_SIZE,
+ 		.base = {
+ 			.cra_name = "chacha20",
+ 			.cra_driver_name = "safexcel-chacha20",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_chacha20_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_chachapoly_setkey(struct crypto_aead *ctfm,
+ 				    const u8 *key, unsigned int len)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_aead_ctx(ctfm);
+ 
+ 	if (ctx->aead  == EIP197_AEAD_TYPE_IPSEC_ESP &&
+ 	    len > EIP197_AEAD_IPSEC_NONCE_SIZE) {
+ 		/* ESP variant has nonce appended to key */
+ 		len -= EIP197_AEAD_IPSEC_NONCE_SIZE;
+ 		ctx->nonce = *(u32 *)(key + len);
+ 	}
+ 	if (len != CHACHA_KEY_SIZE)
+ 		return -EINVAL;
+ 
+ 	safexcel_chacha20_setkey(ctx, key);
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_chachapoly_setauthsize(struct crypto_aead *tfm,
+ 					 unsigned int authsize)
+ {
+ 	if (authsize != POLY1305_DIGEST_SIZE)
+ 		return -EINVAL;
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_chachapoly_crypt(struct aead_request *req,
+ 					  enum safexcel_cipher_direction dir)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(aead);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 	u32 key[CHACHA_KEY_SIZE / sizeof(u32) + 1];
+ 	int ret = 0;
+ 
+ 	/*
+ 	 * Instead of wasting time detecting umpteen silly corner cases,
+ 	 * just dump all "small" requests to the fallback implementation.
+ 	 * HW would not be faster on such small requests anyway.
+ 	 */
+ 	if (likely((ctx->aead != EIP197_AEAD_TYPE_IPSEC_ESP ||
+ 		    req->assoclen >= EIP197_AEAD_IPSEC_IV_SIZE) &&
+ 		   req->cryptlen > POLY1305_DIGEST_SIZE)) {
+ 		return safexcel_queue_req(&req->base, creq, dir);
+ 	}
+ 
+ 	/* HW cannot do full (AAD+payload) zero length, use fallback */
+ 	memcpy(key, ctx->key, CHACHA_KEY_SIZE);
+ 	if (ctx->aead == EIP197_AEAD_TYPE_IPSEC_ESP) {
+ 		/* ESP variant has nonce appended to the key */
+ 		key[CHACHA_KEY_SIZE / sizeof(u32)] = ctx->nonce;
+ 		ret = crypto_aead_setkey(ctx->fback, (u8 *)key,
+ 					 CHACHA_KEY_SIZE +
+ 					 EIP197_AEAD_IPSEC_NONCE_SIZE);
+ 	} else {
+ 		ret = crypto_aead_setkey(ctx->fback, (u8 *)key,
+ 					 CHACHA_KEY_SIZE);
+ 	}
+ 	if (ret) {
+ 		crypto_aead_clear_flags(aead, CRYPTO_TFM_REQ_MASK);
+ 		crypto_aead_set_flags(aead, crypto_aead_get_flags(ctx->fback) &
+ 					    CRYPTO_TFM_REQ_MASK);
+ 		return ret;
+ 	}
+ 
+ 	aead_request_set_tfm(subreq, ctx->fback);
+ 	aead_request_set_callback(subreq, req->base.flags, req->base.complete,
+ 				  req->base.data);
+ 	aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 			       req->iv);
+ 	aead_request_set_ad(subreq, req->assoclen);
+ 
+ 	return (dir ==  SAFEXCEL_ENCRYPT) ?
+ 		crypto_aead_encrypt(subreq) :
+ 		crypto_aead_decrypt(subreq);
+ }
+ 
+ static int safexcel_aead_chachapoly_encrypt(struct aead_request *req)
+ {
+ 	return safexcel_aead_chachapoly_crypt(req, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_aead_chachapoly_decrypt(struct aead_request *req)
+ {
+ 	return safexcel_aead_chachapoly_crypt(req, SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_aead_fallback_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct crypto_aead *aead = __crypto_aead_cast(tfm);
+ 	struct aead_alg *alg = crypto_aead_alg(aead);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 
+ 	/* Allocate fallback implementation */
+ 	ctx->fback = crypto_alloc_aead(alg->base.cra_name, 0,
+ 				       CRYPTO_ALG_ASYNC |
+ 				       CRYPTO_ALG_NEED_FALLBACK);
+ 	if (IS_ERR(ctx->fback))
+ 		return PTR_ERR(ctx->fback);
+ 
+ 	crypto_aead_set_reqsize(aead, max(sizeof(struct safexcel_cipher_req),
+ 					  sizeof(struct aead_request) +
+ 					  crypto_aead_reqsize(ctx->fback)));
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_aead_chachapoly_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_fallback_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_CHACHA20;
+ 	ctx->mode = CONTEXT_CONTROL_CHACHA20_MODE_256_32 |
+ 		    CONTEXT_CONTROL_CHACHA20_MODE_CALC_OTK;
+ 	ctx->ctrinit = 0;
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_POLY1305;
+ 	ctx->state_sz = 0; /* Precomputed by HW */
+ 	return 0;
+ }
+ 
+ static void safexcel_aead_fallback_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_aead(ctx->fback);
+ 	safexcel_aead_cra_exit(tfm);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_chachapoly = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_CHACHA20 | SAFEXCEL_ALG_POLY1305,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_chachapoly_setkey,
+ 		.setauthsize = safexcel_aead_chachapoly_setauthsize,
+ 		.encrypt = safexcel_aead_chachapoly_encrypt,
+ 		.decrypt = safexcel_aead_chachapoly_decrypt,
+ 		.ivsize = CHACHAPOLY_IV_SIZE,
+ 		.maxauthsize = POLY1305_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc7539(chacha20,poly1305)",
+ 			.cra_driver_name = "safexcel-chacha20-poly1305",
+ 			/* +1 to put it above HW chacha + SW poly */
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY + 1,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				     CRYPTO_ALG_NEED_FALLBACK,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_chachapoly_cra_init,
+ 			.cra_exit = safexcel_aead_fallback_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_chachapolyesp_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_chachapoly_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP;
+ 	ctx->aadskip = EIP197_AEAD_IPSEC_IV_SIZE;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_chachapoly_esp = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_CHACHA20 | SAFEXCEL_ALG_POLY1305,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_chachapoly_setkey,
+ 		.setauthsize = safexcel_aead_chachapoly_setauthsize,
+ 		.encrypt = safexcel_aead_chachapoly_encrypt,
+ 		.decrypt = safexcel_aead_chachapoly_decrypt,
+ 		.ivsize = CHACHAPOLY_IV_SIZE - EIP197_AEAD_IPSEC_NONCE_SIZE,
+ 		.maxauthsize = POLY1305_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc7539esp(chacha20,poly1305)",
+ 			.cra_driver_name = "safexcel-chacha20-poly1305-esp",
+ 			/* +1 to put it above HW chacha + SW poly */
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY + 1,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				     CRYPTO_ALG_NEED_FALLBACK,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_chachapolyesp_cra_init,
+ 			.cra_exit = safexcel_aead_fallback_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_setkey(struct crypto_skcipher *ctfm,
+ 					const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct safexcel_crypto_priv *priv = ctx->priv;
+ 
+ 	if (len != SM4_KEY_SIZE)
+ 		return -EINVAL;
+ 
+ 	if (priv->flags & EIP197_TRC_CACHE && ctx->base.ctxr_dma)
+ 		if (memcmp(ctx->key, key, SM4_KEY_SIZE))
+ 			ctx->base.needs_inv = true;
+ 
+ 	memcpy(ctx->key, key, SM4_KEY_SIZE);
+ 	ctx->key_len = SM4_KEY_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sm4_blk_encrypt(struct skcipher_request *req)
+ {
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else
+ 		return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 					  SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_sm4_blk_decrypt(struct skcipher_request *req)
+ {
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else
+ 		return safexcel_queue_req(&req->base, skcipher_request_ctx(req),
+ 					  SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_skcipher_sm4_ecb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_ECB;
+ 	ctx->blocksz = 0;
+ 	ctx->ivmask = EIP197_OPTION_2_TOKEN_IV_CMD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ecb_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_sm4_blk_encrypt,
+ 		.decrypt = safexcel_sm4_blk_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.base = {
+ 			.cra_name = "ecb(sm4)",
+ 			.cra_driver_name = "safexcel-ecb-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_ecb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_cbc_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CBC;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbc_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_sm4_blk_encrypt,
+ 		.decrypt = safexcel_sm4_blk_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cbc(sm4)",
+ 			.cra_driver_name = "safexcel-cbc-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_cbc_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_ofb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_OFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ofb_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "ofb(sm4)",
+ 			.cra_driver_name = "safexcel-ofb-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_ofb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4_cfb_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CFB;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cfb_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_AES_XFB,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		.min_keysize = SM4_KEY_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "cfb(sm4)",
+ 			.cra_driver_name = "safexcel-cfb-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_cfb_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_skcipher_sm4ctr_setkey(struct crypto_skcipher *ctfm,
+ 					   const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_skcipher_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* last 4 bytes of key are the nonce! */
+ 	ctx->nonce = *(u32 *)(key + len - CTR_RFC3686_NONCE_SIZE);
+ 	/* exclude the nonce here */
+ 	len -= CTR_RFC3686_NONCE_SIZE;
+ 
+ 	return safexcel_skcipher_sm4_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_skcipher_sm4_ctr_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_skcipher_cra_init(tfm);
+ 	ctx->alg  = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_ctr_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_SKCIPHER,
+ 	.algo_mask = SAFEXCEL_ALG_SM4,
+ 	.alg.skcipher = {
+ 		.setkey = safexcel_skcipher_sm4ctr_setkey,
+ 		.encrypt = safexcel_encrypt,
+ 		.decrypt = safexcel_decrypt,
+ 		/* Add nonce size */
+ 		.min_keysize = SM4_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.max_keysize = SM4_KEY_SIZE + CTR_RFC3686_NONCE_SIZE,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc3686(ctr(sm4))",
+ 			.cra_driver_name = "safexcel-ctr-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_skcipher_sm4_ctr_cra_init,
+ 			.cra_exit = safexcel_skcipher_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sm4_blk_encrypt(struct aead_request *req)
+ {
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, aead_request_ctx(req),
+ 				  SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_aead_sm4_blk_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if ((req->cryptlen - crypto_aead_authsize(tfm)) & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, aead_request_ctx(req),
+ 				  SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sha1_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA1;
+ 	ctx->state_sz = SHA1_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_cbc_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_sm4_blk_encrypt,
+ 		.decrypt = safexcel_aead_sm4_blk_decrypt,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),cbc(sm4))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-cbc-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4cbc_sha1_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_fallback_setkey(struct crypto_aead *ctfm,
+ 					 const u8 *key, unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* Keep fallback cipher synchronized */
+ 	return crypto_aead_setkey(ctx->fback, (u8 *)key, len) ?:
+ 	       safexcel_aead_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_aead_fallback_setauthsize(struct crypto_aead *ctfm,
+ 					      unsigned int authsize)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* Keep fallback cipher synchronized */
+ 	return crypto_aead_setauthsize(ctx->fback, authsize);
+ }
+ 
+ static int safexcel_aead_fallback_crypt(struct aead_request *req,
+ 					enum safexcel_cipher_direction dir)
+ {
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(aead);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 
+ 	aead_request_set_tfm(subreq, ctx->fback);
+ 	aead_request_set_callback(subreq, req->base.flags, req->base.complete,
+ 				  req->base.data);
+ 	aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 			       req->iv);
+ 	aead_request_set_ad(subreq, req->assoclen);
+ 
+ 	return (dir ==  SAFEXCEL_ENCRYPT) ?
+ 		crypto_aead_encrypt(subreq) :
+ 		crypto_aead_decrypt(subreq);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sm3_encrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if (req->cryptlen & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else if (req->cryptlen || req->assoclen) /* If input length > 0 only */
+ 		return safexcel_queue_req(&req->base, creq, SAFEXCEL_ENCRYPT);
+ 
+ 	/* HW cannot do full (AAD+payload) zero length, use fallback */
+ 	return safexcel_aead_fallback_crypt(req, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sm3_decrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 
+ 	/* Workaround for HW bug: EIP96 4.3 does not report blocksize error */
+ 	if ((req->cryptlen - crypto_aead_authsize(tfm)) & (SM4_BLOCK_SIZE - 1))
+ 		return -EINVAL;
+ 	else if (req->cryptlen > crypto_aead_authsize(tfm) || req->assoclen)
+ 		/* If input length > 0 only */
+ 		return safexcel_queue_req(&req->base, creq, SAFEXCEL_DECRYPT);
+ 
+ 	/* HW cannot do full (AAD+payload) zero length, use fallback */
+ 	return safexcel_aead_fallback_crypt(req, SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_aead_sm4cbc_sm3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_fallback_cra_init(tfm);
+ 	ctx->alg = SAFEXCEL_SM4;
+ 	ctx->blocksz = SM4_BLOCK_SIZE;
+ 	ctx->hash_alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;
+ 	ctx->state_sz = SM3_DIGEST_SIZE;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sm3_cbc_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SM3,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_fallback_setkey,
+ 		.setauthsize = safexcel_aead_fallback_setauthsize,
+ 		.encrypt = safexcel_aead_sm4cbc_sm3_encrypt,
+ 		.decrypt = safexcel_aead_sm4cbc_sm3_decrypt,
+ 		.ivsize = SM4_BLOCK_SIZE,
+ 		.maxauthsize = SM3_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sm3),cbc(sm4))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sm3-cbc-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				     CRYPTO_ALG_NEED_FALLBACK,
+ 			.cra_blocksize = SM4_BLOCK_SIZE,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4cbc_sm3_cra_init,
+ 			.cra_exit = safexcel_aead_fallback_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sm4ctr_sha1_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sm4cbc_sha1_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sha1_ctr_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SHA1,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SHA1_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sha1),rfc3686(ctr(sm4)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sha1-ctr-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4ctr_sha1_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_aead_sm4ctr_sm3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_aead_sm4cbc_sm3_cra_init(tfm);
+ 	ctx->mode = CONTEXT_CONTROL_CRYPTO_MODE_CTR_LOAD;
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_authenc_hmac_sm3_ctr_sm4 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_SM4 | SAFEXCEL_ALG_SM3,
+ 	.alg.aead = {
+ 		.setkey = safexcel_aead_setkey,
+ 		.encrypt = safexcel_aead_encrypt,
+ 		.decrypt = safexcel_aead_decrypt,
+ 		.ivsize = CTR_RFC3686_IV_SIZE,
+ 		.maxauthsize = SM3_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "authenc(hmac(sm3),rfc3686(ctr(sm4)))",
+ 			.cra_driver_name = "safexcel-authenc-hmac-sm3-ctr-sm4",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_aead_sm4ctr_sm3_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_rfc4106_gcm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				       unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* last 4 bytes of key are the nonce! */
+ 	ctx->nonce = *(u32 *)(key + len - CTR_RFC3686_NONCE_SIZE);
+ 
+ 	len -= CTR_RFC3686_NONCE_SIZE;
+ 	return safexcel_aead_gcm_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_rfc4106_gcm_setauthsize(struct crypto_aead *tfm,
+ 					    unsigned int authsize)
+ {
+ 	return crypto_rfc4106_check_authsize(authsize);
+ }
+ 
+ static int safexcel_rfc4106_encrypt(struct aead_request *req)
+ {
+ 	return crypto_ipsec_check_assoclen(req->assoclen) ?:
+ 	       safexcel_aead_encrypt(req);
+ }
+ 
+ static int safexcel_rfc4106_decrypt(struct aead_request *req)
+ {
+ 	return crypto_ipsec_check_assoclen(req->assoclen) ?:
+ 	       safexcel_aead_decrypt(req);
+ }
+ 
+ static int safexcel_rfc4106_gcm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_gcm_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP;
+ 	ctx->aadskip = EIP197_AEAD_IPSEC_IV_SIZE;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_rfc4106_gcm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_GHASH,
+ 	.alg.aead = {
+ 		.setkey = safexcel_rfc4106_gcm_setkey,
+ 		.setauthsize = safexcel_rfc4106_gcm_setauthsize,
+ 		.encrypt = safexcel_rfc4106_encrypt,
+ 		.decrypt = safexcel_rfc4106_decrypt,
+ 		.ivsize = GCM_RFC4106_IV_SIZE,
+ 		.maxauthsize = GHASH_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc4106(gcm(aes))",
+ 			.cra_driver_name = "safexcel-rfc4106-gcm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_rfc4106_gcm_cra_init,
+ 			.cra_exit = safexcel_aead_gcm_cra_exit,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_rfc4543_gcm_setauthsize(struct crypto_aead *tfm,
+ 					    unsigned int authsize)
+ {
+ 	if (authsize != GHASH_DIGEST_SIZE)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_rfc4543_gcm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_gcm_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP_GMAC;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_rfc4543_gcm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_GHASH,
+ 	.alg.aead = {
+ 		.setkey = safexcel_rfc4106_gcm_setkey,
+ 		.setauthsize = safexcel_rfc4543_gcm_setauthsize,
+ 		.encrypt = safexcel_rfc4106_encrypt,
+ 		.decrypt = safexcel_rfc4106_decrypt,
+ 		.ivsize = GCM_RFC4543_IV_SIZE,
+ 		.maxauthsize = GHASH_DIGEST_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc4543(gcm(aes))",
+ 			.cra_driver_name = "safexcel-rfc4543-gcm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_rfc4543_gcm_cra_init,
+ 			.cra_exit = safexcel_aead_gcm_cra_exit,
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_rfc4309_ccm_setkey(struct crypto_aead *ctfm, const u8 *key,
+ 				       unsigned int len)
+ {
+ 	struct crypto_tfm *tfm = crypto_aead_tfm(ctfm);
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	/* First byte of the nonce = L = always 3 for RFC4309 (4 byte ctr) */
+ 	*(u8 *)&ctx->nonce = EIP197_AEAD_IPSEC_COUNTER_SIZE - 1;
+ 	/* last 3 bytes of key are the nonce! */
+ 	memcpy((u8 *)&ctx->nonce + 1, key + len -
+ 	       EIP197_AEAD_IPSEC_CCM_NONCE_SIZE,
+ 	       EIP197_AEAD_IPSEC_CCM_NONCE_SIZE);
+ 
+ 	len -= EIP197_AEAD_IPSEC_CCM_NONCE_SIZE;
+ 	return safexcel_aead_ccm_setkey(ctfm, key, len);
+ }
+ 
+ static int safexcel_rfc4309_ccm_setauthsize(struct crypto_aead *tfm,
+ 					    unsigned int authsize)
+ {
+ 	/* Borrowed from crypto/ccm.c */
+ 	switch (authsize) {
+ 	case 8:
+ 	case 12:
+ 	case 16:
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_rfc4309_ccm_encrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	/* Borrowed from crypto/ccm.c */
+ 	if (req->assoclen != 16 && req->assoclen != 20)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_ENCRYPT);
+ }
+ 
+ static int safexcel_rfc4309_ccm_decrypt(struct aead_request *req)
+ {
+ 	struct safexcel_cipher_req *creq = aead_request_ctx(req);
+ 
+ 	/* Borrowed from crypto/ccm.c */
+ 	if (req->assoclen != 16 && req->assoclen != 20)
+ 		return -EINVAL;
+ 
+ 	return safexcel_queue_req(&req->base, creq, SAFEXCEL_DECRYPT);
+ }
+ 
+ static int safexcel_rfc4309_ccm_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_cipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_aead_ccm_cra_init(tfm);
+ 	ctx->aead  = EIP197_AEAD_TYPE_IPSEC_ESP;
+ 	ctx->aadskip = EIP197_AEAD_IPSEC_IV_SIZE;
+ 	return ret;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_rfc4309_ccm = {
+ 	.type = SAFEXCEL_ALG_TYPE_AEAD,
+ 	.algo_mask = SAFEXCEL_ALG_AES | SAFEXCEL_ALG_CBC_MAC_ALL,
+ 	.alg.aead = {
+ 		.setkey = safexcel_rfc4309_ccm_setkey,
+ 		.setauthsize = safexcel_rfc4309_ccm_setauthsize,
+ 		.encrypt = safexcel_rfc4309_ccm_encrypt,
+ 		.decrypt = safexcel_rfc4309_ccm_decrypt,
+ 		.ivsize = EIP197_AEAD_IPSEC_IV_SIZE,
+ 		.maxauthsize = AES_BLOCK_SIZE,
+ 		.base = {
+ 			.cra_name = "rfc4309(ccm(aes))",
+ 			.cra_driver_name = "safexcel-rfc4309-ccm-aes",
+ 			.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 			.cra_flags = CRYPTO_ALG_ASYNC |
+ 				     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.cra_blocksize = 1,
+ 			.cra_ctxsize = sizeof(struct safexcel_cipher_ctx),
+ 			.cra_alignmask = 0,
+ 			.cra_init = safexcel_rfc4309_ccm_cra_init,
+ 			.cra_exit = safexcel_aead_cra_exit,
+ 			.cra_module = THIS_MODULE,
+ 		},
+ 	},
+ };
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
diff --cc drivers/crypto/inside-secure/safexcel_hash.c
index c77b0e1655a8,16a467969d8e..000000000000
--- a/drivers/crypto/inside-secure/safexcel_hash.c
+++ b/drivers/crypto/inside-secure/safexcel_hash.c
@@@ -769,8 -990,9 +769,9 @@@ struct safexcel_alg_template safexcel_a
  			.base = {
  				.cra_name = "sha1",
  				.cra_driver_name = "safexcel-sha1",
 -				.cra_priority = SAFEXCEL_CRA_PRIORITY,
 +				.cra_priority = 300,
  				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
  					     CRYPTO_ALG_KERN_DRIVER_ONLY,
  				.cra_blocksize = SHA1_BLOCK_SIZE,
  				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
@@@ -1003,8 -1234,9 +1004,9 @@@ struct safexcel_alg_template safexcel_a
  			.base = {
  				.cra_name = "hmac(sha1)",
  				.cra_driver_name = "safexcel-hmac-sha1",
 -				.cra_priority = SAFEXCEL_CRA_PRIORITY,
 +				.cra_priority = 300,
  				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
  					     CRYPTO_ALG_KERN_DRIVER_ONLY,
  				.cra_blocksize = SHA1_BLOCK_SIZE,
  				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
@@@ -1065,8 -1291,9 +1067,9 @@@ struct safexcel_alg_template safexcel_a
  			.base = {
  				.cra_name = "sha256",
  				.cra_driver_name = "safexcel-sha256",
 -				.cra_priority = SAFEXCEL_CRA_PRIORITY,
 +				.cra_priority = 300,
  				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
  					     CRYPTO_ALG_KERN_DRIVER_ONLY,
  				.cra_blocksize = SHA256_BLOCK_SIZE,
  				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
@@@ -1127,8 -1348,9 +1130,9 @@@ struct safexcel_alg_template safexcel_a
  			.base = {
  				.cra_name = "sha224",
  				.cra_driver_name = "safexcel-sha224",
 -				.cra_priority = SAFEXCEL_CRA_PRIORITY,
 +				.cra_priority = 300,
  				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
  					     CRYPTO_ALG_KERN_DRIVER_ONLY,
  				.cra_blocksize = SHA224_BLOCK_SIZE,
  				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
@@@ -1183,8 -1420,9 +1187,9 @@@ struct safexcel_alg_template safexcel_a
  			.base = {
  				.cra_name = "hmac(sha224)",
  				.cra_driver_name = "safexcel-hmac-sha224",
 -				.cra_priority = SAFEXCEL_CRA_PRIORITY,
 +				.cra_priority = 300,
  				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
  					     CRYPTO_ALG_KERN_DRIVER_ONLY,
  				.cra_blocksize = SHA224_BLOCK_SIZE,
  				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
@@@ -1239,8 -1492,9 +1244,9 @@@ struct safexcel_alg_template safexcel_a
  			.base = {
  				.cra_name = "hmac(sha256)",
  				.cra_driver_name = "safexcel-hmac-sha256",
 -				.cra_priority = SAFEXCEL_CRA_PRIORITY,
 +				.cra_priority = 300,
  				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
  					     CRYPTO_ALG_KERN_DRIVER_ONLY,
  				.cra_blocksize = SHA256_BLOCK_SIZE,
  				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
@@@ -1251,3 -1505,1627 +1257,1630 @@@
  		},
  	},
  };
++<<<<<<< HEAD
++=======
+ 
+ static int safexcel_sha512_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sha512_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_sha512_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha512_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_sha512_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha512",
+ 				.cra_driver_name = "safexcel-sha512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha384_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sha384_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_sha384_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha384_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_sha384_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha384",
+ 				.cra_driver_name = "safexcel-sha384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha512_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				       unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sha512",
+ 					SHA512_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_sha512_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, SHA512_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= SHA512_BLOCK_SIZE;
+ 	req->processed	= SHA512_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha512_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_sha512_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha512_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_sha512_digest,
+ 		.setkey = safexcel_hmac_sha512_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha512)",
+ 				.cra_driver_name = "safexcel-hmac-sha512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha384_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				       unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sha384",
+ 					SHA512_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_sha384_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, SHA512_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= SHA512_BLOCK_SIZE;
+ 	req->processed	= SHA512_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SHA512_DIGEST_SIZE;
+ 	req->digest_sz = SHA512_DIGEST_SIZE;
+ 	req->block_sz = SHA512_BLOCK_SIZE;
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha384_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_sha384_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA2_512,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha384_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_sha384_digest,
+ 		.setkey = safexcel_hmac_sha384_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SHA384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha384)",
+ 				.cra_driver_name = "safexcel-hmac-sha384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SHA384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_md5_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_MD5;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = MD5_DIGEST_SIZE;
+ 	req->digest_sz = MD5_DIGEST_SIZE;
+ 	req->block_sz = MD5_HMAC_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_md5_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_md5_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_md5 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_MD5,
+ 	.alg.ahash = {
+ 		.init = safexcel_md5_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_md5_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = MD5_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "md5",
+ 				.cra_driver_name = "safexcel-md5",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_md5_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, MD5_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= MD5_HMAC_BLOCK_SIZE;
+ 	req->processed	= MD5_HMAC_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_MD5;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = MD5_DIGEST_SIZE;
+ 	req->digest_sz = MD5_DIGEST_SIZE;
+ 	req->block_sz = MD5_HMAC_BLOCK_SIZE;
+ 	req->len_is_le = true; /* MD5 is little endian! ... */
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_md5_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				     unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-md5",
+ 					MD5_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_md5_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_md5_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_md5 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_MD5,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_md5_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_md5_digest,
+ 		.setkey = safexcel_hmac_md5_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = MD5_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(md5)",
+ 				.cra_driver_name = "safexcel-hmac-md5",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_crc32_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret = safexcel_ahash_cra_init(tfm);
+ 
+ 	/* Default 'key' is all zeroes */
+ 	memset(ctx->ipad, 0, sizeof(u32));
+ 	return ret;
+ }
+ 
+ static int safexcel_crc32_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from loaded key */
+ 	req->state[0]	= (__force __le32)le32_to_cpu(~ctx->ipad[0]);
+ 	/* Set processed to non-zero to enable invalidation detection */
+ 	req->len	= sizeof(u32);
+ 	req->processed	= sizeof(u32);
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_CRC32;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_XCM;
+ 	req->state_sz = sizeof(u32);
+ 	req->digest_sz = sizeof(u32);
+ 	req->block_sz = sizeof(u32);
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_crc32_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				 unsigned int keylen)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 
+ 	if (keylen != sizeof(u32))
+ 		return -EINVAL;
+ 
+ 	memcpy(ctx->ipad, key, sizeof(u32));
+ 	return 0;
+ }
+ 
+ static int safexcel_crc32_digest(struct ahash_request *areq)
+ {
+ 	return safexcel_crc32_init(areq) ?: safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_crc32 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_crc32_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_crc32_digest,
+ 		.setkey = safexcel_crc32_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = sizeof(u32),
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "crc32",
+ 				.cra_driver_name = "safexcel-crc32",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_OPTIONAL_KEY |
+ 					     CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = 1,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_crc32_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_cbcmac_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from loaded keys */
+ 	memcpy(req->state, ctx->ipad, ctx->key_sz);
+ 	/* Set processed to non-zero to enable invalidation detection */
+ 	req->len	= AES_BLOCK_SIZE;
+ 	req->processed	= AES_BLOCK_SIZE;
+ 
+ 	req->digest   = CONTEXT_CONTROL_DIGEST_XCM;
+ 	req->state_sz = ctx->key_sz;
+ 	req->digest_sz = AES_BLOCK_SIZE;
+ 	req->block_sz = AES_BLOCK_SIZE;
+ 	req->xcbcmac  = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_cbcmac_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				 unsigned int len)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 	struct crypto_aes_ctx aes;
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	memset(ctx->ipad, 0, 2 * AES_BLOCK_SIZE);
+ 	for (i = 0; i < len / sizeof(u32); i++)
+ 		ctx->ipad[i + 8] = (__force __le32)cpu_to_be32(aes.key_enc[i]);
+ 
+ 	if (len == AES_KEYSIZE_192) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else if (len == AES_KEYSIZE_256) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 		ctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	}
+ 	ctx->cbcmac  = true;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_cbcmac_digest(struct ahash_request *areq)
+ {
+ 	return safexcel_cbcmac_init(areq) ?: safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cbcmac = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_cbcmac_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_cbcmac_digest,
+ 		.setkey = safexcel_cbcmac_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = AES_BLOCK_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "cbcmac(aes)",
+ 				.cra_driver_name = "safexcel-cbcmac-aes",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = 1,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_xcbcmac_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				 unsigned int len)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 	struct crypto_aes_ctx aes;
+ 	u32 key_tmp[3 * AES_BLOCK_SIZE / sizeof(u32)];
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* precompute the XCBC key material */
+ 	crypto_cipher_clear_flags(ctx->kaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->kaes, crypto_ahash_get_flags(tfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->kaes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)key_tmp + 2 * AES_BLOCK_SIZE,
+ 		"\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1\x1");
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)key_tmp,
+ 		"\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2\x2");
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)key_tmp + AES_BLOCK_SIZE,
+ 		"\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3\x3");
+ 	for (i = 0; i < 3 * AES_BLOCK_SIZE / sizeof(u32); i++)
+ 		ctx->ipad[i] =
+ 			cpu_to_le32((__force u32)cpu_to_be32(key_tmp[i]));
+ 
+ 	crypto_cipher_clear_flags(ctx->kaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->kaes, crypto_ahash_get_flags(tfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->kaes,
+ 				   (u8 *)key_tmp + 2 * AES_BLOCK_SIZE,
+ 				   AES_MIN_KEY_SIZE);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 	ctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	ctx->cbcmac = false;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ static int safexcel_xcbcmac_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_ahash_cra_init(tfm);
+ 	ctx->kaes = crypto_alloc_cipher("aes", 0, 0);
+ 	return PTR_ERR_OR_ZERO(ctx->kaes);
+ }
+ 
+ static void safexcel_xcbcmac_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_cipher(ctx->kaes);
+ 	safexcel_ahash_cra_exit(tfm);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_xcbcmac = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_cbcmac_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_cbcmac_digest,
+ 		.setkey = safexcel_xcbcmac_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = AES_BLOCK_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "xcbc(aes)",
+ 				.cra_driver_name = "safexcel-xcbc-aes",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = AES_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_xcbcmac_cra_init,
+ 				.cra_exit = safexcel_xcbcmac_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_cmac_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				unsigned int len)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));
+ 	struct crypto_aes_ctx aes;
+ 	__be64 consts[4];
+ 	u64 _const[2];
+ 	u8 msb_mask, gfmask;
+ 	int ret, i;
+ 
+ 	ret = aes_expandkey(&aes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	for (i = 0; i < len / sizeof(u32); i++)
+ 		ctx->ipad[i + 8] =
+ 			cpu_to_le32((__force u32)cpu_to_be32(aes.key_enc[i]));
+ 
+ 	/* precompute the CMAC key material */
+ 	crypto_cipher_clear_flags(ctx->kaes, CRYPTO_TFM_REQ_MASK);
+ 	crypto_cipher_set_flags(ctx->kaes, crypto_ahash_get_flags(tfm) &
+ 				CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_cipher_setkey(ctx->kaes, key, len);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* code below borrowed from crypto/cmac.c */
+ 	/* encrypt the zero block */
+ 	memset(consts, 0, AES_BLOCK_SIZE);
+ 	crypto_cipher_encrypt_one(ctx->kaes, (u8 *)consts, (u8 *)consts);
+ 
+ 	gfmask = 0x87;
+ 	_const[0] = be64_to_cpu(consts[1]);
+ 	_const[1] = be64_to_cpu(consts[0]);
+ 
+ 	/* gf(2^128) multiply zero-ciphertext with u and u^2 */
+ 	for (i = 0; i < 4; i += 2) {
+ 		msb_mask = ((s64)_const[1] >> 63) & gfmask;
+ 		_const[1] = (_const[1] << 1) | (_const[0] >> 63);
+ 		_const[0] = (_const[0] << 1) ^ msb_mask;
+ 
+ 		consts[i + 0] = cpu_to_be64(_const[1]);
+ 		consts[i + 1] = cpu_to_be64(_const[0]);
+ 	}
+ 	/* end of code borrowed from crypto/cmac.c */
+ 
+ 	for (i = 0; i < 2 * AES_BLOCK_SIZE / sizeof(u32); i++)
+ 		ctx->ipad[i] = (__force __le32)cpu_to_be32(((u32 *)consts)[i]);
+ 
+ 	if (len == AES_KEYSIZE_192) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC192;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else if (len == AES_KEYSIZE_256) {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC256;
+ 		ctx->key_sz = AES_MAX_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	} else {
+ 		ctx->alg    = CONTEXT_CONTROL_CRYPTO_ALG_XCBC128;
+ 		ctx->key_sz = AES_MIN_KEY_SIZE + 2 * AES_BLOCK_SIZE;
+ 	}
+ 	ctx->cbcmac = false;
+ 
+ 	memzero_explicit(&aes, sizeof(aes));
+ 	return 0;
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_cmac = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = 0,
+ 	.alg.ahash = {
+ 		.init = safexcel_cbcmac_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_cbcmac_digest,
+ 		.setkey = safexcel_cmac_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = AES_BLOCK_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "cmac(aes)",
+ 				.cra_driver_name = "safexcel-cmac-aes",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = AES_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_xcbcmac_cra_init,
+ 				.cra_exit = safexcel_xcbcmac_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sm3_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SM3_DIGEST_SIZE;
+ 	req->digest_sz = SM3_DIGEST_SIZE;
+ 	req->block_sz = SM3_BLOCK_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_sm3_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_sm3_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sm3 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SM3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sm3_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_sm3_digest,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SM3_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sm3",
+ 				.cra_driver_name = "safexcel-sm3",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SM3_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sm3_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				    unsigned int keylen)
+ {
+ 	return safexcel_hmac_alg_setkey(tfm, key, keylen, "safexcel-sm3",
+ 					SM3_DIGEST_SIZE);
+ }
+ 
+ static int safexcel_hmac_sm3_init(struct ahash_request *areq)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(crypto_ahash_reqtfm(areq));
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Start from ipad precompute */
+ 	memcpy(req->state, ctx->ipad, SM3_DIGEST_SIZE);
+ 	/* Already processed the key^ipad part now! */
+ 	req->len	= SM3_BLOCK_SIZE;
+ 	req->processed	= SM3_BLOCK_SIZE;
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SM3;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_PRECOMPUTED;
+ 	req->state_sz = SM3_DIGEST_SIZE;
+ 	req->digest_sz = SM3_DIGEST_SIZE;
+ 	req->block_sz = SM3_BLOCK_SIZE;
+ 	req->hmac = true;
+ 
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sm3_digest(struct ahash_request *areq)
+ {
+ 	int ret = safexcel_hmac_sm3_init(areq);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	return safexcel_ahash_finup(areq);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sm3 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SM3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sm3_init,
+ 		.update = safexcel_ahash_update,
+ 		.final = safexcel_ahash_final,
+ 		.finup = safexcel_ahash_finup,
+ 		.digest = safexcel_hmac_sm3_digest,
+ 		.setkey = safexcel_hmac_sm3_setkey,
+ 		.export = safexcel_ahash_export,
+ 		.import = safexcel_ahash_import,
+ 		.halg = {
+ 			.digestsize = SM3_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sm3)",
+ 				.cra_driver_name = "safexcel-hmac-sm3",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 				.cra_blocksize = SM3_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_ahash_cra_init,
+ 				.cra_exit = safexcel_ahash_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_224_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_224;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_224_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_224_DIGEST_SIZE;
+ 	req->block_sz = SHA3_224_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_fbcheck(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 	int ret = 0;
+ 
+ 	if (ctx->do_fallback) {
+ 		ahash_request_set_tfm(subreq, ctx->fback);
+ 		ahash_request_set_callback(subreq, req->base.flags,
+ 					   req->base.complete, req->base.data);
+ 		ahash_request_set_crypt(subreq, req->src, req->result,
+ 					req->nbytes);
+ 		if (!ctx->fb_init_done) {
+ 			if (ctx->fb_do_setkey) {
+ 				/* Set fallback cipher HMAC key */
+ 				u8 key[SHA3_224_BLOCK_SIZE];
+ 
+ 				memcpy(key, ctx->ipad,
+ 				       crypto_ahash_blocksize(ctx->fback) / 2);
+ 				memcpy(key +
+ 				       crypto_ahash_blocksize(ctx->fback) / 2,
+ 				       ctx->opad,
+ 				       crypto_ahash_blocksize(ctx->fback) / 2);
+ 				ret = crypto_ahash_setkey(ctx->fback, key,
+ 					crypto_ahash_blocksize(ctx->fback));
+ 				memzero_explicit(key,
+ 					crypto_ahash_blocksize(ctx->fback));
+ 				ctx->fb_do_setkey = false;
+ 			}
+ 			ret = ret ?: crypto_ahash_init(subreq);
+ 			ctx->fb_init_done = true;
+ 		}
+ 	}
+ 	return ret;
+ }
+ 
+ static int safexcel_sha3_update(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_update(subreq);
+ }
+ 
+ static int safexcel_sha3_final(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_final(subreq);
+ }
+ 
+ static int safexcel_sha3_finup(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback |= !req->nbytes;
+ 	if (ctx->do_fallback)
+ 		/* Update or ex/import happened or len 0, cannot use the HW */
+ 		return safexcel_sha3_fbcheck(req) ?:
+ 		       crypto_ahash_finup(subreq);
+ 	else
+ 		return safexcel_ahash_finup(req);
+ }
+ 
+ static int safexcel_sha3_digest_fallback(struct ahash_request *req)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	ctx->fb_init_done = false;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_finup(subreq);
+ }
+ 
+ static int safexcel_sha3_224_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_224_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_sha3_export(struct ahash_request *req, void *out)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_export(subreq, out);
+ }
+ 
+ static int safexcel_sha3_import(struct ahash_request *req, const void *in)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct ahash_request *subreq = ahash_request_ctx(req);
+ 
+ 	ctx->do_fallback = true;
+ 	return safexcel_sha3_fbcheck(req) ?: crypto_ahash_import(subreq, in);
+ 	// return safexcel_ahash_import(req, in);
+ }
+ 
+ static int safexcel_sha3_cra_init(struct crypto_tfm *tfm)
+ {
+ 	struct crypto_ahash *ahash = __crypto_ahash_cast(tfm);
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	safexcel_ahash_cra_init(tfm);
+ 
+ 	/* Allocate fallback implementation */
+ 	ctx->fback = crypto_alloc_ahash(crypto_tfm_alg_name(tfm), 0,
+ 					CRYPTO_ALG_ASYNC |
+ 					CRYPTO_ALG_NEED_FALLBACK);
+ 	if (IS_ERR(ctx->fback))
+ 		return PTR_ERR(ctx->fback);
+ 
+ 	/* Update statesize from fallback algorithm! */
+ 	crypto_hash_alg_common(ahash)->statesize =
+ 		crypto_ahash_statesize(ctx->fback);
+ 	crypto_ahash_set_reqsize(ahash, max(sizeof(struct safexcel_ahash_req),
+ 					    sizeof(struct ahash_request) +
+ 					    crypto_ahash_reqsize(ctx->fback)));
+ 	return 0;
+ }
+ 
+ static void safexcel_sha3_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_ahash(ctx->fback);
+ 	safexcel_ahash_cra_exit(tfm);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_224 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_224_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_224_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_224_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-224",
+ 				.cra_driver_name = "safexcel-sha3-224",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_224_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_256_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_256;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_256_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_256_DIGEST_SIZE;
+ 	req->block_sz = SHA3_256_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_256_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_256_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_256 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_256_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_256_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_256_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-256",
+ 				.cra_driver_name = "safexcel-sha3-256",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_256_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_384_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_384_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_384_DIGEST_SIZE;
+ 	req->block_sz = SHA3_384_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_384_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_384_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_384_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_384_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-384",
+ 				.cra_driver_name = "safexcel-sha3-384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_sha3_512_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_INITIAL;
+ 	req->state_sz = SHA3_512_DIGEST_SIZE;
+ 	req->digest_sz = SHA3_512_DIGEST_SIZE;
+ 	req->block_sz = SHA3_512_BLOCK_SIZE;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_sha3_512_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_sha3_512_init(req) ?: safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length hash, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_sha3_512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_sha3_512_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_sha3_512_digest,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "sha3-512",
+ 				.cra_driver_name = "safexcel-sha3-512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_sha3_cra_init,
+ 				.cra_exit = safexcel_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_cra_init(struct crypto_tfm *tfm, const char *alg)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	int ret;
+ 
+ 	ret = safexcel_sha3_cra_init(tfm);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* Allocate precalc basic digest implementation */
+ 	ctx->shpre = crypto_alloc_shash(alg, 0, CRYPTO_ALG_NEED_FALLBACK);
+ 	if (IS_ERR(ctx->shpre))
+ 		return PTR_ERR(ctx->shpre);
+ 
+ 	ctx->shdesc = kmalloc(sizeof(*ctx->shdesc) +
+ 			      crypto_shash_descsize(ctx->shpre), GFP_KERNEL);
+ 	if (!ctx->shdesc) {
+ 		crypto_free_shash(ctx->shpre);
+ 		return -ENOMEM;
+ 	}
+ 	ctx->shdesc->tfm = ctx->shpre;
+ 	return 0;
+ }
+ 
+ static void safexcel_hmac_sha3_cra_exit(struct crypto_tfm *tfm)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_ahash(ctx->fback);
+ 	crypto_free_shash(ctx->shpre);
+ 	kfree(ctx->shdesc);
+ 	safexcel_ahash_cra_exit(tfm);
+ }
+ 
+ static int safexcel_hmac_sha3_setkey(struct crypto_ahash *tfm, const u8 *key,
+ 				     unsigned int keylen)
+ {
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	int ret = 0;
+ 
+ 	if (keylen > crypto_ahash_blocksize(tfm)) {
+ 		/*
+ 		 * If the key is larger than the blocksize, then hash it
+ 		 * first using our fallback cipher
+ 		 */
+ 		ret = crypto_shash_digest(ctx->shdesc, key, keylen,
+ 					  (u8 *)ctx->ipad);
+ 		keylen = crypto_shash_digestsize(ctx->shpre);
+ 
+ 		/*
+ 		 * If the digest is larger than half the blocksize, we need to
+ 		 * move the rest to opad due to the way our HMAC infra works.
+ 		 */
+ 		if (keylen > crypto_ahash_blocksize(tfm) / 2)
+ 			/* Buffers overlap, need to use memmove iso memcpy! */
+ 			memmove(ctx->opad,
+ 				(u8 *)ctx->ipad +
+ 					crypto_ahash_blocksize(tfm) / 2,
+ 				keylen - crypto_ahash_blocksize(tfm) / 2);
+ 	} else {
+ 		/*
+ 		 * Copy the key to our ipad & opad buffers
+ 		 * Note that ipad and opad each contain one half of the key,
+ 		 * to match the existing HMAC driver infrastructure.
+ 		 */
+ 		if (keylen <= crypto_ahash_blocksize(tfm) / 2) {
+ 			memcpy(ctx->ipad, key, keylen);
+ 		} else {
+ 			memcpy(ctx->ipad, key,
+ 			       crypto_ahash_blocksize(tfm) / 2);
+ 			memcpy(ctx->opad,
+ 			       key + crypto_ahash_blocksize(tfm) / 2,
+ 			       keylen - crypto_ahash_blocksize(tfm) / 2);
+ 		}
+ 	}
+ 
+ 	/* Pad key with zeroes */
+ 	if (keylen <= crypto_ahash_blocksize(tfm) / 2) {
+ 		memset((u8 *)ctx->ipad + keylen, 0,
+ 		       crypto_ahash_blocksize(tfm) / 2 - keylen);
+ 		memset(ctx->opad, 0, crypto_ahash_blocksize(tfm) / 2);
+ 	} else {
+ 		memset((u8 *)ctx->opad + keylen -
+ 		       crypto_ahash_blocksize(tfm) / 2, 0,
+ 		       crypto_ahash_blocksize(tfm) - keylen);
+ 	}
+ 
+ 	/* If doing fallback, still need to set the new key! */
+ 	ctx->fb_do_setkey = true;
+ 	return ret;
+ }
+ 
+ static int safexcel_hmac_sha3_224_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_224_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_224_BLOCK_SIZE;
+ 	req->processed	= SHA3_224_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_224;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_224_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_224_DIGEST_SIZE;
+ 	req->block_sz = SHA3_224_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_224_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_224_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_224_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-224");
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_224 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_224_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_224_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_224_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-224)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-224",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_224_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_224_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_256_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_256_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_256_BLOCK_SIZE;
+ 	req->processed	= SHA3_256_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_256;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_256_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_256_DIGEST_SIZE;
+ 	req->block_sz = SHA3_256_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_256_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_256_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_256_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-256");
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_256 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_256_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_256_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_256_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-256)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-256",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_256_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_256_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_384_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_384_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_384_BLOCK_SIZE;
+ 	req->processed	= SHA3_384_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_384;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_384_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_384_DIGEST_SIZE;
+ 	req->block_sz = SHA3_384_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_384_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_384_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_384_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-384");
+ }
+ 
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_384 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_384_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_384_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_384_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-384)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-384",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_384_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_384_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
+ 
+ static int safexcel_hmac_sha3_512_init(struct ahash_request *areq)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+ 	struct safexcel_ahash_ctx *ctx = crypto_ahash_ctx(tfm);
+ 	struct safexcel_ahash_req *req = ahash_request_ctx(areq);
+ 
+ 	memset(req, 0, sizeof(*req));
+ 
+ 	/* Copy (half of) the key */
+ 	memcpy(req->state, ctx->ipad, SHA3_512_BLOCK_SIZE / 2);
+ 	/* Start of HMAC should have len == processed == blocksize */
+ 	req->len	= SHA3_512_BLOCK_SIZE;
+ 	req->processed	= SHA3_512_BLOCK_SIZE;
+ 	ctx->alg = CONTEXT_CONTROL_CRYPTO_ALG_SHA3_512;
+ 	req->digest = CONTEXT_CONTROL_DIGEST_HMAC;
+ 	req->state_sz = SHA3_512_BLOCK_SIZE / 2;
+ 	req->digest_sz = SHA3_512_DIGEST_SIZE;
+ 	req->block_sz = SHA3_512_BLOCK_SIZE;
+ 	req->hmac = true;
+ 	ctx->do_fallback = false;
+ 	ctx->fb_init_done = false;
+ 	return 0;
+ }
+ 
+ static int safexcel_hmac_sha3_512_digest(struct ahash_request *req)
+ {
+ 	if (req->nbytes)
+ 		return safexcel_hmac_sha3_512_init(req) ?:
+ 		       safexcel_ahash_finup(req);
+ 
+ 	/* HW cannot do zero length HMAC, use fallback instead */
+ 	return safexcel_sha3_digest_fallback(req);
+ }
+ 
+ static int safexcel_hmac_sha3_512_cra_init(struct crypto_tfm *tfm)
+ {
+ 	return safexcel_hmac_sha3_cra_init(tfm, "sha3-512");
+ }
+ struct safexcel_alg_template safexcel_alg_hmac_sha3_512 = {
+ 	.type = SAFEXCEL_ALG_TYPE_AHASH,
+ 	.algo_mask = SAFEXCEL_ALG_SHA3,
+ 	.alg.ahash = {
+ 		.init = safexcel_hmac_sha3_512_init,
+ 		.update = safexcel_sha3_update,
+ 		.final = safexcel_sha3_final,
+ 		.finup = safexcel_sha3_finup,
+ 		.digest = safexcel_hmac_sha3_512_digest,
+ 		.setkey = safexcel_hmac_sha3_setkey,
+ 		.export = safexcel_sha3_export,
+ 		.import = safexcel_sha3_import,
+ 		.halg = {
+ 			.digestsize = SHA3_512_DIGEST_SIZE,
+ 			.statesize = sizeof(struct safexcel_ahash_export_state),
+ 			.base = {
+ 				.cra_name = "hmac(sha3-512)",
+ 				.cra_driver_name = "safexcel-hmac-sha3-512",
+ 				.cra_priority = SAFEXCEL_CRA_PRIORITY,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 					     CRYPTO_ALG_NEED_FALLBACK,
+ 				.cra_blocksize = SHA3_512_BLOCK_SIZE,
+ 				.cra_ctxsize = sizeof(struct safexcel_ahash_ctx),
+ 				.cra_init = safexcel_hmac_sha3_512_cra_init,
+ 				.cra_exit = safexcel_hmac_sha3_cra_exit,
+ 				.cra_module = THIS_MODULE,
+ 			},
+ 		},
+ 	},
+ };
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
diff --cc drivers/crypto/ixp4xx_crypto.c
index ea0ea400cf82,f478bb0a566a..000000000000
--- a/drivers/crypto/ixp4xx_crypto.c
+++ b/drivers/crypto/ixp4xx_crypto.c
@@@ -1457,26 -1401,25 +1457,40 @@@ static int __init ixp_module_init(void
  		}
  
  		/* block ciphers */
++<<<<<<< HEAD
 +		cra->cra_type = &crypto_ablkcipher_type;
 +		cra->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				 CRYPTO_ALG_KERN_DRIVER_ONLY |
 +				 CRYPTO_ALG_ASYNC;
 +		if (!cra->cra_ablkcipher.setkey)
 +			cra->cra_ablkcipher.setkey = ablk_setkey;
 +		if (!cra->cra_ablkcipher.encrypt)
 +			cra->cra_ablkcipher.encrypt = ablk_encrypt;
 +		if (!cra->cra_ablkcipher.decrypt)
 +			cra->cra_ablkcipher.decrypt = ablk_decrypt;
 +		cra->cra_init = init_tfm_ablk;
++=======
+ 		cra->base.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 				      CRYPTO_ALG_ASYNC |
+ 				      CRYPTO_ALG_ALLOCATES_MEMORY;
+ 		if (!cra->setkey)
+ 			cra->setkey = ablk_setkey;
+ 		if (!cra->encrypt)
+ 			cra->encrypt = ablk_encrypt;
+ 		if (!cra->decrypt)
+ 			cra->decrypt = ablk_decrypt;
+ 		cra->init = init_tfm_ablk;
+ 		cra->exit = exit_tfm_ablk;
 -
 -		cra->base.cra_ctxsize = sizeof(struct ixp_ctx);
 -		cra->base.cra_module = THIS_MODULE;
 -		cra->base.cra_alignmask = 3;
 -		cra->base.cra_priority = 300;
 -		if (crypto_register_skcipher(cra))
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
 +
 +		cra->cra_ctxsize = sizeof(struct ixp_ctx);
 +		cra->cra_module = THIS_MODULE;
 +		cra->cra_alignmask = 3;
 +		cra->cra_priority = 300;
 +		cra->cra_exit = exit_tfm;
 +		if (crypto_register_alg(cra))
  			printk(KERN_ERR "Failed to register '%s'\n",
 -				cra->base.cra_name);
 +				cra->cra_name);
  		else
  			ixp4xx_algos[i].registered = 1;
  	}
diff --cc drivers/crypto/n2_core.c
index 8030bb60fb1c,d8aec5153b21..000000000000
--- a/drivers/crypto/n2_core.c
+++ b/drivers/crypto/n2_core.c
@@@ -1393,25 -1376,24 +1393,34 @@@ static int __n2_register_one_cipher(con
  	if (!p)
  		return -ENOMEM;
  
 -	alg = &p->skcipher;
 -	*alg = tmpl->skcipher;
 +	alg = &p->alg;
  
++<<<<<<< HEAD
 +	snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", tmpl->name);
 +	snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s-n2", tmpl->drv_name);
 +	alg->cra_priority = N2_CRA_PRIORITY;
 +	alg->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +			 CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC;
 +	alg->cra_blocksize = tmpl->block_size;
++=======
+ 	snprintf(alg->base.cra_name, CRYPTO_MAX_ALG_NAME, "%s", tmpl->name);
+ 	snprintf(alg->base.cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s-n2", tmpl->drv_name);
+ 	alg->base.cra_priority = N2_CRA_PRIORITY;
+ 	alg->base.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |
+ 			      CRYPTO_ALG_ALLOCATES_MEMORY;
+ 	alg->base.cra_blocksize = tmpl->block_size;
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  	p->enc_type = tmpl->enc_type;
 -	alg->base.cra_ctxsize = sizeof(struct n2_skcipher_context);
 -	alg->base.cra_module = THIS_MODULE;
 -	alg->init = n2_skcipher_init_tfm;
 -
 -	list_add(&p->entry, &skcipher_algs);
 -	err = crypto_register_skcipher(alg);
 +	alg->cra_ctxsize = sizeof(struct n2_cipher_context);
 +	alg->cra_type = &crypto_ablkcipher_type;
 +	alg->cra_u.ablkcipher = tmpl->ablkcipher;
 +	alg->cra_init = n2_cipher_cra_init;
 +	alg->cra_module = THIS_MODULE;
 +
 +	list_add(&p->entry, &cipher_algs);
 +	err = crypto_register_alg(alg);
  	if (err) {
 -		pr_err("%s alg registration failed\n", alg->base.cra_name);
 +		pr_err("%s alg registration failed\n", alg->cra_name);
  		list_del(&p->entry);
  		kfree(p);
  	} else {
diff --cc drivers/crypto/picoxcell_crypto.c
index 6a0a275f56f6,dac6eb37fff9..000000000000
--- a/drivers/crypto/picoxcell_crypto.c
+++ b/drivers/crypto/picoxcell_crypto.c
@@@ -1254,27 -1227,25 +1254,49 @@@ static struct spacc_alg ipsec_engine_al
  		.key_offs = 0,
  		.iv_offs = AES_MAX_KEY_SIZE,
  		.alg = {
++<<<<<<< HEAD
 +			.cra_name = "cbc(aes)",
 +			.cra_driver_name = "cbc-aes-picoxcell",
 +			.cra_priority = SPACC_CRYPTO_ALG_PRIORITY,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				     CRYPTO_ALG_KERN_DRIVER_ONLY |
 +				     CRYPTO_ALG_ASYNC |
 +				     CRYPTO_ALG_NEED_FALLBACK,
 +			.cra_blocksize = AES_BLOCK_SIZE,
 +			.cra_ctxsize = sizeof(struct spacc_ablk_ctx),
 +			.cra_type = &crypto_ablkcipher_type,
 +			.cra_module = THIS_MODULE,
 +			.cra_ablkcipher = {
 +				.setkey = spacc_aes_setkey,
 +				.encrypt = spacc_ablk_encrypt,
 +				.decrypt = spacc_ablk_decrypt,
 +				.min_keysize = AES_MIN_KEY_SIZE,
 +				.max_keysize = AES_MAX_KEY_SIZE,
 +				.ivsize = AES_BLOCK_SIZE,
 +			},
 +			.cra_init = spacc_ablk_cra_init,
 +			.cra_exit = spacc_ablk_cra_exit,
++=======
+ 			.base.cra_name		= "cbc(aes)",
+ 			.base.cra_driver_name	= "cbc-aes-picoxcell",
+ 			.base.cra_priority	= SPACC_CRYPTO_ALG_PRIORITY,
+ 			.base.cra_flags		= CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 						  CRYPTO_ALG_ASYNC |
+ 						  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 						  CRYPTO_ALG_NEED_FALLBACK,
+ 			.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 			.base.cra_ctxsize	= sizeof(struct spacc_ablk_ctx),
+ 			.base.cra_module	= THIS_MODULE,
+ 
+ 			.setkey			= spacc_aes_setkey,
+ 			.encrypt 		= spacc_ablk_encrypt,
+ 			.decrypt 		= spacc_ablk_decrypt,
+ 			.min_keysize 		= AES_MIN_KEY_SIZE,
+ 			.max_keysize 		= AES_MAX_KEY_SIZE,
+ 			.ivsize			= AES_BLOCK_SIZE,
+ 			.init			= spacc_ablk_init_tfm,
+ 			.exit			= spacc_ablk_exit_tfm,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  	},
  	{
@@@ -1282,25 -1253,24 +1304,46 @@@
  		.iv_offs = AES_MAX_KEY_SIZE,
  		.ctrl_default = SPA_CTRL_CIPH_ALG_AES | SPA_CTRL_CIPH_MODE_ECB,
  		.alg = {
++<<<<<<< HEAD
 +			.cra_name = "ecb(aes)",
 +			.cra_driver_name = "ecb-aes-picoxcell",
 +			.cra_priority = SPACC_CRYPTO_ALG_PRIORITY,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				CRYPTO_ALG_KERN_DRIVER_ONLY |
 +				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 +			.cra_blocksize = AES_BLOCK_SIZE,
 +			.cra_ctxsize = sizeof(struct spacc_ablk_ctx),
 +			.cra_type = &crypto_ablkcipher_type,
 +			.cra_module = THIS_MODULE,
 +			.cra_ablkcipher = {
 +				.setkey = spacc_aes_setkey,
 +				.encrypt = spacc_ablk_encrypt,
 +				.decrypt = spacc_ablk_decrypt,
 +				.min_keysize = AES_MIN_KEY_SIZE,
 +				.max_keysize = AES_MAX_KEY_SIZE,
 +			},
 +			.cra_init = spacc_ablk_cra_init,
 +			.cra_exit = spacc_ablk_cra_exit,
++=======
+ 			.base.cra_name		= "ecb(aes)",
+ 			.base.cra_driver_name	= "ecb-aes-picoxcell",
+ 			.base.cra_priority	= SPACC_CRYPTO_ALG_PRIORITY,
+ 			.base.cra_flags		= CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 						  CRYPTO_ALG_ASYNC |
+ 						  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 						  CRYPTO_ALG_NEED_FALLBACK,
+ 			.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 			.base.cra_ctxsize	= sizeof(struct spacc_ablk_ctx),
+ 			.base.cra_module	= THIS_MODULE,
+ 
+ 			.setkey			= spacc_aes_setkey,
+ 			.encrypt 		= spacc_ablk_encrypt,
+ 			.decrypt 		= spacc_ablk_decrypt,
+ 			.min_keysize 		= AES_MIN_KEY_SIZE,
+ 			.max_keysize 		= AES_MAX_KEY_SIZE,
+ 			.init			= spacc_ablk_init_tfm,
+ 			.exit			= spacc_ablk_exit_tfm,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  	},
  	{
@@@ -1308,26 -1278,24 +1351,47 @@@
  		.iv_offs = 0,
  		.ctrl_default = SPA_CTRL_CIPH_ALG_DES | SPA_CTRL_CIPH_MODE_CBC,
  		.alg = {
++<<<<<<< HEAD
 +			.cra_name = "cbc(des)",
 +			.cra_driver_name = "cbc-des-picoxcell",
 +			.cra_priority = SPACC_CRYPTO_ALG_PRIORITY,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +					CRYPTO_ALG_ASYNC |
 +					CRYPTO_ALG_KERN_DRIVER_ONLY,
 +			.cra_blocksize = DES_BLOCK_SIZE,
 +			.cra_ctxsize = sizeof(struct spacc_ablk_ctx),
 +			.cra_type = &crypto_ablkcipher_type,
 +			.cra_module = THIS_MODULE,
 +			.cra_ablkcipher = {
 +				.setkey = spacc_des_setkey,
 +				.encrypt = spacc_ablk_encrypt,
 +				.decrypt = spacc_ablk_decrypt,
 +				.min_keysize = DES_KEY_SIZE,
 +				.max_keysize = DES_KEY_SIZE,
 +				.ivsize = DES_BLOCK_SIZE,
 +			},
 +			.cra_init = spacc_ablk_cra_init,
 +			.cra_exit = spacc_ablk_cra_exit,
++=======
+ 			.base.cra_name		= "cbc(des)",
+ 			.base.cra_driver_name	= "cbc-des-picoxcell",
+ 			.base.cra_priority	= SPACC_CRYPTO_ALG_PRIORITY,
+ 			.base.cra_flags		= CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 						  CRYPTO_ALG_ASYNC |
+ 						  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.base.cra_blocksize	= DES_BLOCK_SIZE,
+ 			.base.cra_ctxsize	= sizeof(struct spacc_ablk_ctx),
+ 			.base.cra_module	= THIS_MODULE,
+ 
+ 			.setkey			= spacc_des_setkey,
+ 			.encrypt		= spacc_ablk_encrypt,
+ 			.decrypt		= spacc_ablk_decrypt,
+ 			.min_keysize		= DES_KEY_SIZE,
+ 			.max_keysize		= DES_KEY_SIZE,
+ 			.ivsize			= DES_BLOCK_SIZE,
+ 			.init			= spacc_ablk_init_tfm,
+ 			.exit			= spacc_ablk_exit_tfm,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  	},
  	{
@@@ -1335,25 -1303,23 +1399,45 @@@
  		.iv_offs = 0,
  		.ctrl_default = SPA_CTRL_CIPH_ALG_DES | SPA_CTRL_CIPH_MODE_ECB,
  		.alg = {
++<<<<<<< HEAD
 +			.cra_name = "ecb(des)",
 +			.cra_driver_name = "ecb-des-picoxcell",
 +			.cra_priority = SPACC_CRYPTO_ALG_PRIORITY,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +					CRYPTO_ALG_ASYNC |
 +					CRYPTO_ALG_KERN_DRIVER_ONLY,
 +			.cra_blocksize = DES_BLOCK_SIZE,
 +			.cra_ctxsize = sizeof(struct spacc_ablk_ctx),
 +			.cra_type = &crypto_ablkcipher_type,
 +			.cra_module = THIS_MODULE,
 +			.cra_ablkcipher = {
 +				.setkey = spacc_des_setkey,
 +				.encrypt = spacc_ablk_encrypt,
 +				.decrypt = spacc_ablk_decrypt,
 +				.min_keysize = DES_KEY_SIZE,
 +				.max_keysize = DES_KEY_SIZE,
 +			},
 +			.cra_init = spacc_ablk_cra_init,
 +			.cra_exit = spacc_ablk_cra_exit,
++=======
+ 			.base.cra_name		= "ecb(des)",
+ 			.base.cra_driver_name	= "ecb-des-picoxcell",
+ 			.base.cra_priority	= SPACC_CRYPTO_ALG_PRIORITY,
+ 			.base.cra_flags		= CRYPTO_ALG_KERN_DRIVER_ONLY |
+ 						  CRYPTO_ALG_ASYNC |
+ 						  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.base.cra_blocksize	= DES_BLOCK_SIZE,
+ 			.base.cra_ctxsize	= sizeof(struct spacc_ablk_ctx),
+ 			.base.cra_module	= THIS_MODULE,
+ 
+ 			.setkey			= spacc_des_setkey,
+ 			.encrypt		= spacc_ablk_encrypt,
+ 			.decrypt		= spacc_ablk_decrypt,
+ 			.min_keysize		= DES_KEY_SIZE,
+ 			.max_keysize		= DES_KEY_SIZE,
+ 			.init			= spacc_ablk_init_tfm,
+ 			.exit			= spacc_ablk_exit_tfm,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  	},
  	{
@@@ -1361,26 -1327,24 +1445,47 @@@
  		.iv_offs = 0,
  		.ctrl_default = SPA_CTRL_CIPH_ALG_DES | SPA_CTRL_CIPH_MODE_CBC,
  		.alg = {
++<<<<<<< HEAD
 +			.cra_name = "cbc(des3_ede)",
 +			.cra_driver_name = "cbc-des3-ede-picoxcell",
 +			.cra_priority = SPACC_CRYPTO_ALG_PRIORITY,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +					CRYPTO_ALG_ASYNC |
 +					CRYPTO_ALG_KERN_DRIVER_ONLY,
 +			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 +			.cra_ctxsize = sizeof(struct spacc_ablk_ctx),
 +			.cra_type = &crypto_ablkcipher_type,
 +			.cra_module = THIS_MODULE,
 +			.cra_ablkcipher = {
 +				.setkey = spacc_des3_setkey,
 +				.encrypt = spacc_ablk_encrypt,
 +				.decrypt = spacc_ablk_decrypt,
 +				.min_keysize = DES3_EDE_KEY_SIZE,
 +				.max_keysize = DES3_EDE_KEY_SIZE,
 +				.ivsize = DES3_EDE_BLOCK_SIZE,
 +			},
 +			.cra_init = spacc_ablk_cra_init,
 +			.cra_exit = spacc_ablk_cra_exit,
++=======
+ 			.base.cra_name		= "cbc(des3_ede)",
+ 			.base.cra_driver_name	= "cbc-des3-ede-picoxcell",
+ 			.base.cra_priority	= SPACC_CRYPTO_ALG_PRIORITY,
+ 			.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 						  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 						  CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.base.cra_blocksize	= DES3_EDE_BLOCK_SIZE,
+ 			.base.cra_ctxsize	= sizeof(struct spacc_ablk_ctx),
+ 			.base.cra_module	= THIS_MODULE,
+ 
+ 			.setkey			= spacc_des3_setkey,
+ 			.encrypt		= spacc_ablk_encrypt,
+ 			.decrypt		= spacc_ablk_decrypt,
+ 			.min_keysize		= DES3_EDE_KEY_SIZE,
+ 			.max_keysize		= DES3_EDE_KEY_SIZE,
+ 			.ivsize			= DES3_EDE_BLOCK_SIZE,
+ 			.init			= spacc_ablk_init_tfm,
+ 			.exit			= spacc_ablk_exit_tfm,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  	},
  	{
@@@ -1388,25 -1352,23 +1493,45 @@@
  		.iv_offs = 0,
  		.ctrl_default = SPA_CTRL_CIPH_ALG_DES | SPA_CTRL_CIPH_MODE_ECB,
  		.alg = {
++<<<<<<< HEAD
 +			.cra_name = "ecb(des3_ede)",
 +			.cra_driver_name = "ecb-des3-ede-picoxcell",
 +			.cra_priority = SPACC_CRYPTO_ALG_PRIORITY,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +					CRYPTO_ALG_ASYNC |
 +					CRYPTO_ALG_KERN_DRIVER_ONLY,
 +			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 +			.cra_ctxsize = sizeof(struct spacc_ablk_ctx),
 +			.cra_type = &crypto_ablkcipher_type,
 +			.cra_module = THIS_MODULE,
 +			.cra_ablkcipher = {
 +				.setkey = spacc_des3_setkey,
 +				.encrypt = spacc_ablk_encrypt,
 +				.decrypt = spacc_ablk_decrypt,
 +				.min_keysize = DES3_EDE_KEY_SIZE,
 +				.max_keysize = DES3_EDE_KEY_SIZE,
 +			},
 +			.cra_init = spacc_ablk_cra_init,
 +			.cra_exit = spacc_ablk_cra_exit,
++=======
+ 			.base.cra_name		= "ecb(des3_ede)",
+ 			.base.cra_driver_name	= "ecb-des3-ede-picoxcell",
+ 			.base.cra_priority	= SPACC_CRYPTO_ALG_PRIORITY,
+ 			.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 						  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 						  CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.base.cra_blocksize	= DES3_EDE_BLOCK_SIZE,
+ 			.base.cra_ctxsize	= sizeof(struct spacc_ablk_ctx),
+ 			.base.cra_module	= THIS_MODULE,
+ 
+ 			.setkey			= spacc_des3_setkey,
+ 			.encrypt		= spacc_ablk_encrypt,
+ 			.decrypt		= spacc_ablk_decrypt,
+ 			.min_keysize		= DES3_EDE_KEY_SIZE,
+ 			.max_keysize		= DES3_EDE_KEY_SIZE,
+ 			.init			= spacc_ablk_init_tfm,
+ 			.exit			= spacc_ablk_exit_tfm,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  	},
  };
@@@ -1602,26 -1570,24 +1733,47 @@@ static struct spacc_alg l2_engine_algs[
  		.ctrl_default = SPA_CTRL_CIPH_ALG_KASUMI |
  				SPA_CTRL_CIPH_MODE_F8,
  		.alg = {
++<<<<<<< HEAD
 +			.cra_name = "f8(kasumi)",
 +			.cra_driver_name = "f8-kasumi-picoxcell",
 +			.cra_priority = SPACC_CRYPTO_ALG_PRIORITY,
 +			.cra_flags = CRYPTO_ALG_TYPE_GIVCIPHER |
 +					CRYPTO_ALG_ASYNC |
 +					CRYPTO_ALG_KERN_DRIVER_ONLY,
 +			.cra_blocksize = 8,
 +			.cra_ctxsize = sizeof(struct spacc_ablk_ctx),
 +			.cra_type = &crypto_ablkcipher_type,
 +			.cra_module = THIS_MODULE,
 +			.cra_ablkcipher = {
 +				.setkey = spacc_kasumi_f8_setkey,
 +				.encrypt = spacc_ablk_encrypt,
 +				.decrypt = spacc_ablk_decrypt,
 +				.min_keysize = 16,
 +				.max_keysize = 16,
 +				.ivsize = 8,
 +			},
 +			.cra_init = spacc_ablk_cra_init,
 +			.cra_exit = spacc_ablk_cra_exit,
++=======
+ 			.base.cra_name		= "f8(kasumi)",
+ 			.base.cra_driver_name	= "f8-kasumi-picoxcell",
+ 			.base.cra_priority	= SPACC_CRYPTO_ALG_PRIORITY,
+ 			.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 						  CRYPTO_ALG_ALLOCATES_MEMORY |
+ 						  CRYPTO_ALG_KERN_DRIVER_ONLY,
+ 			.base.cra_blocksize	= 8,
+ 			.base.cra_ctxsize	= sizeof(struct spacc_ablk_ctx),
+ 			.base.cra_module	= THIS_MODULE,
+ 
+ 			.setkey			= spacc_kasumi_f8_setkey,
+ 			.encrypt		= spacc_ablk_encrypt,
+ 			.decrypt		= spacc_ablk_decrypt,
+ 			.min_keysize		= 16,
+ 			.max_keysize		= 16,
+ 			.ivsize			= 8,
+ 			.init			= spacc_ablk_init_tfm,
+ 			.exit			= spacc_ablk_exit_tfm,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  	},
  };
diff --cc drivers/crypto/talitos.c
index 001a50ae58dd,7c547352a862..000000000000
--- a/drivers/crypto/talitos.c
+++ b/drivers/crypto/talitos.c
@@@ -2360,9 -2284,10 +2361,10 @@@ static struct talitos_alg_template driv
  			.base = {
  				.cra_name = "authenc(hmac(sha1),cbc(aes))",
  				.cra_driver_name = "authenc-hmac-sha1-"
 -						   "cbc-aes-talitos-hsna",
 +						   "cbc-aes-talitos",
  				.cra_blocksize = AES_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = AES_BLOCK_SIZE,
  			.maxauthsize = SHA1_DIGEST_SIZE,
@@@ -2405,9 -2331,10 +2408,10 @@@
  				.cra_name = "authenc(hmac(sha1),"
  					    "cbc(des3_ede))",
  				.cra_driver_name = "authenc-hmac-sha1-"
 -						   "cbc-3des-talitos-hsna",
 +						   "cbc-3des-talitos",
  				.cra_blocksize = DES3_EDE_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = DES3_EDE_BLOCK_SIZE,
  			.maxauthsize = SHA1_DIGEST_SIZE,
@@@ -2448,9 -2376,10 +2453,10 @@@
  			.base = {
  				.cra_name = "authenc(hmac(sha224),cbc(aes))",
  				.cra_driver_name = "authenc-hmac-sha224-"
 -						   "cbc-aes-talitos-hsna",
 +						   "cbc-aes-talitos",
  				.cra_blocksize = AES_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = AES_BLOCK_SIZE,
  			.maxauthsize = SHA224_DIGEST_SIZE,
@@@ -2493,9 -2423,10 +2500,10 @@@
  				.cra_name = "authenc(hmac(sha224),"
  					    "cbc(des3_ede))",
  				.cra_driver_name = "authenc-hmac-sha224-"
 -						   "cbc-3des-talitos-hsna",
 +						   "cbc-3des-talitos",
  				.cra_blocksize = DES3_EDE_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = DES3_EDE_BLOCK_SIZE,
  			.maxauthsize = SHA224_DIGEST_SIZE,
@@@ -2536,9 -2468,10 +2545,10 @@@
  			.base = {
  				.cra_name = "authenc(hmac(sha256),cbc(aes))",
  				.cra_driver_name = "authenc-hmac-sha256-"
 -						   "cbc-aes-talitos-hsna",
 +						   "cbc-aes-talitos",
  				.cra_blocksize = AES_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = AES_BLOCK_SIZE,
  			.maxauthsize = SHA256_DIGEST_SIZE,
@@@ -2581,9 -2515,10 +2592,10 @@@
  				.cra_name = "authenc(hmac(sha256),"
  					    "cbc(des3_ede))",
  				.cra_driver_name = "authenc-hmac-sha256-"
 -						   "cbc-3des-talitos-hsna",
 +						   "cbc-3des-talitos",
  				.cra_blocksize = DES3_EDE_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = DES3_EDE_BLOCK_SIZE,
  			.maxauthsize = SHA256_DIGEST_SIZE,
@@@ -2710,9 -2650,10 +2727,10 @@@
  			.base = {
  				.cra_name = "authenc(hmac(md5),cbc(aes))",
  				.cra_driver_name = "authenc-hmac-md5-"
 -						   "cbc-aes-talitos-hsna",
 +						   "cbc-aes-talitos",
  				.cra_blocksize = AES_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = AES_BLOCK_SIZE,
  			.maxauthsize = MD5_DIGEST_SIZE,
@@@ -2753,9 -2695,10 +2772,10 @@@
  			.base = {
  				.cra_name = "authenc(hmac(md5),cbc(des3_ede))",
  				.cra_driver_name = "authenc-hmac-md5-"
 -						   "cbc-3des-talitos-hsna",
 +						   "cbc-3des-talitos",
  				.cra_blocksize = DES3_EDE_BLOCK_SIZE,
- 				.cra_flags = CRYPTO_ALG_ASYNC,
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
  			},
  			.ivsize = DES3_EDE_BLOCK_SIZE,
  			.maxauthsize = MD5_DIGEST_SIZE,
@@@ -2770,123 -2713,109 +2790,219 @@@
  				     DESC_HDR_MODE1_MDEU_PAD |
  				     DESC_HDR_MODE1_MDEU_MD5_HMAC,
  	},
++<<<<<<< HEAD
 +	/* ABLKCIPHER algorithms. */
 +	{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
 +		.alg.crypto = {
 +			.cra_name = "ecb(aes)",
 +			.cra_driver_name = "ecb-aes-talitos",
 +			.cra_blocksize = AES_BLOCK_SIZE,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
 +			.cra_ablkcipher = {
 +				.min_keysize = AES_MIN_KEY_SIZE,
 +				.max_keysize = AES_MAX_KEY_SIZE,
 +				.ivsize = AES_BLOCK_SIZE,
 +			}
++=======
+ 	/* SKCIPHER algorithms. */
+ 	{	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+ 		.alg.skcipher = {
+ 			.base.cra_name = "ecb(aes)",
+ 			.base.cra_driver_name = "ecb-aes-talitos",
+ 			.base.cra_blocksize = AES_BLOCK_SIZE,
+ 			.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.min_keysize = AES_MIN_KEY_SIZE,
+ 			.max_keysize = AES_MAX_KEY_SIZE,
+ 			.setkey = skcipher_aes_setkey,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
  				     DESC_HDR_SEL0_AESU,
  	},
++<<<<<<< HEAD
 +	{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
 +		.alg.crypto = {
 +			.cra_name = "cbc(aes)",
 +			.cra_driver_name = "cbc-aes-talitos",
 +			.cra_blocksize = AES_BLOCK_SIZE,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +                                     CRYPTO_ALG_ASYNC,
 +			.cra_ablkcipher = {
 +				.min_keysize = AES_MIN_KEY_SIZE,
 +				.max_keysize = AES_MAX_KEY_SIZE,
 +				.ivsize = AES_BLOCK_SIZE,
 +			}
++=======
+ 	{	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+ 		.alg.skcipher = {
+ 			.base.cra_name = "cbc(aes)",
+ 			.base.cra_driver_name = "cbc-aes-talitos",
+ 			.base.cra_blocksize = AES_BLOCK_SIZE,
+ 			.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.min_keysize = AES_MIN_KEY_SIZE,
+ 			.max_keysize = AES_MAX_KEY_SIZE,
+ 			.ivsize = AES_BLOCK_SIZE,
+ 			.setkey = skcipher_aes_setkey,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
  				     DESC_HDR_SEL0_AESU |
  				     DESC_HDR_MODE0_AESU_CBC,
  	},
++<<<<<<< HEAD
 +	{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
 +		.alg.crypto = {
 +			.cra_name = "ctr(aes)",
 +			.cra_driver_name = "ctr-aes-talitos",
 +			.cra_blocksize = AES_BLOCK_SIZE,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
 +			.cra_ablkcipher = {
 +				.min_keysize = AES_MIN_KEY_SIZE,
 +				.max_keysize = AES_MAX_KEY_SIZE,
 +				.ivsize = AES_BLOCK_SIZE,
 +			}
++=======
+ 	{	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+ 		.alg.skcipher = {
+ 			.base.cra_name = "ctr(aes)",
+ 			.base.cra_driver_name = "ctr-aes-talitos",
+ 			.base.cra_blocksize = 1,
+ 			.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.min_keysize = AES_MIN_KEY_SIZE,
+ 			.max_keysize = AES_MAX_KEY_SIZE,
+ 			.ivsize = AES_BLOCK_SIZE,
+ 			.setkey = skcipher_aes_setkey,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_AESU_CTR_NONSNOOP |
  				     DESC_HDR_SEL0_AESU |
  				     DESC_HDR_MODE0_AESU_CTR,
  	},
++<<<<<<< HEAD
 +	{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
 +		.alg.crypto = {
 +			.cra_name = "ecb(des)",
 +			.cra_driver_name = "ecb-des-talitos",
 +			.cra_blocksize = DES_BLOCK_SIZE,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
 +			.cra_ablkcipher = {
 +				.min_keysize = DES_KEY_SIZE,
 +				.max_keysize = DES_KEY_SIZE,
 +				.ivsize = DES_BLOCK_SIZE,
 +				.setkey = ablkcipher_des_setkey,
 +			}
++=======
+ 	{	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+ 		.alg.skcipher = {
+ 			.base.cra_name = "ecb(des)",
+ 			.base.cra_driver_name = "ecb-des-talitos",
+ 			.base.cra_blocksize = DES_BLOCK_SIZE,
+ 			.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.min_keysize = DES_KEY_SIZE,
+ 			.max_keysize = DES_KEY_SIZE,
+ 			.setkey = skcipher_des_setkey,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
  				     DESC_HDR_SEL0_DEU,
  	},
++<<<<<<< HEAD
 +	{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
 +		.alg.crypto = {
 +			.cra_name = "cbc(des)",
 +			.cra_driver_name = "cbc-des-talitos",
 +			.cra_blocksize = DES_BLOCK_SIZE,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
 +			.cra_ablkcipher = {
 +				.min_keysize = DES_KEY_SIZE,
 +				.max_keysize = DES_KEY_SIZE,
 +				.ivsize = DES_BLOCK_SIZE,
 +				.setkey = ablkcipher_des_setkey,
 +			}
++=======
+ 	{	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+ 		.alg.skcipher = {
+ 			.base.cra_name = "cbc(des)",
+ 			.base.cra_driver_name = "cbc-des-talitos",
+ 			.base.cra_blocksize = DES_BLOCK_SIZE,
+ 			.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.min_keysize = DES_KEY_SIZE,
+ 			.max_keysize = DES_KEY_SIZE,
+ 			.ivsize = DES_BLOCK_SIZE,
+ 			.setkey = skcipher_des_setkey,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
  				     DESC_HDR_SEL0_DEU |
  				     DESC_HDR_MODE0_DEU_CBC,
  	},
++<<<<<<< HEAD
 +	{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
 +		.alg.crypto = {
 +			.cra_name = "ecb(des3_ede)",
 +			.cra_driver_name = "ecb-3des-talitos",
 +			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +				     CRYPTO_ALG_ASYNC,
 +			.cra_ablkcipher = {
 +				.min_keysize = DES3_EDE_KEY_SIZE,
 +				.max_keysize = DES3_EDE_KEY_SIZE,
 +				.ivsize = DES3_EDE_BLOCK_SIZE,
 +				.setkey = ablkcipher_des3_setkey,
 +			}
++=======
+ 	{	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+ 		.alg.skcipher = {
+ 			.base.cra_name = "ecb(des3_ede)",
+ 			.base.cra_driver_name = "ecb-3des-talitos",
+ 			.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.min_keysize = DES3_EDE_KEY_SIZE,
+ 			.max_keysize = DES3_EDE_KEY_SIZE,
+ 			.setkey = skcipher_des3_setkey,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
  				     DESC_HDR_SEL0_DEU |
  				     DESC_HDR_MODE0_DEU_3DES,
  	},
++<<<<<<< HEAD
 +	{	.type = CRYPTO_ALG_TYPE_ABLKCIPHER,
 +		.alg.crypto = {
 +			.cra_name = "cbc(des3_ede)",
 +			.cra_driver_name = "cbc-3des-talitos",
 +			.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 +			.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
 +                                     CRYPTO_ALG_ASYNC,
 +			.cra_ablkcipher = {
 +				.min_keysize = DES3_EDE_KEY_SIZE,
 +				.max_keysize = DES3_EDE_KEY_SIZE,
 +				.ivsize = DES3_EDE_BLOCK_SIZE,
 +				.setkey = ablkcipher_des3_setkey,
 +			}
++=======
+ 	{	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+ 		.alg.skcipher = {
+ 			.base.cra_name = "cbc(des3_ede)",
+ 			.base.cra_driver_name = "cbc-3des-talitos",
+ 			.base.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+ 			.base.cra_flags = CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 			.min_keysize = DES3_EDE_KEY_SIZE,
+ 			.max_keysize = DES3_EDE_KEY_SIZE,
+ 			.ivsize = DES3_EDE_BLOCK_SIZE,
+ 			.setkey = skcipher_des3_setkey,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
  			             DESC_HDR_SEL0_DEU |
@@@ -2902,8 -2831,8 +3018,13 @@@
  				.cra_name = "md5",
  				.cra_driver_name = "md5-talitos",
  				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -2918,8 -2847,8 +3039,13 @@@
  				.cra_name = "sha1",
  				.cra_driver_name = "sha1-talitos",
  				.cra_blocksize = SHA1_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -2934,8 -2863,8 +3060,13 @@@
  				.cra_name = "sha224",
  				.cra_driver_name = "sha224-talitos",
  				.cra_blocksize = SHA224_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -2950,8 -2879,8 +3081,13 @@@
  				.cra_name = "sha256",
  				.cra_driver_name = "sha256-talitos",
  				.cra_blocksize = SHA256_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -2966,8 -2895,8 +3102,13 @@@
  				.cra_name = "sha384",
  				.cra_driver_name = "sha384-talitos",
  				.cra_blocksize = SHA384_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -2982,8 -2911,8 +3123,13 @@@
  				.cra_name = "sha512",
  				.cra_driver_name = "sha512-talitos",
  				.cra_blocksize = SHA512_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -2998,8 -2927,8 +3144,13 @@@
  				.cra_name = "hmac(md5)",
  				.cra_driver_name = "hmac-md5-talitos",
  				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -3014,8 -2943,8 +3165,13 @@@
  				.cra_name = "hmac(sha1)",
  				.cra_driver_name = "hmac-sha1-talitos",
  				.cra_blocksize = SHA1_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -3030,8 -2959,8 +3186,13 @@@
  				.cra_name = "hmac(sha224)",
  				.cra_driver_name = "hmac-sha224-talitos",
  				.cra_blocksize = SHA224_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -3046,8 -2975,8 +3207,13 @@@
  				.cra_name = "hmac(sha256)",
  				.cra_driver_name = "hmac-sha256-talitos",
  				.cra_blocksize = SHA256_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -3062,8 -2991,8 +3228,13 @@@
  				.cra_name = "hmac(sha384)",
  				.cra_driver_name = "hmac-sha384-talitos",
  				.cra_blocksize = SHA384_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
@@@ -3078,8 -3007,8 +3249,13 @@@
  				.cra_name = "hmac(sha512)",
  				.cra_driver_name = "hmac-sha512-talitos",
  				.cra_blocksize = SHA512_BLOCK_SIZE,
++<<<<<<< HEAD
 +				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
 +					     CRYPTO_ALG_ASYNC,
++=======
+ 				.cra_flags = CRYPTO_ALG_ASYNC |
+ 					     CRYPTO_ALG_ALLOCATES_MEMORY,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  			}
  		},
  		.desc_hdr_template = DESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |
diff --cc drivers/crypto/virtio/virtio_crypto_algs.c
index af6a908dfa7a,b2601958282e..000000000000
--- a/drivers/crypto/virtio/virtio_crypto_algs.c
+++ b/drivers/crypto/virtio/virtio_crypto_algs.c
@@@ -560,38 -574,42 +560,61 @@@ int virtio_crypto_ablkcipher_crypt_req
  	return 0;
  }
  
 -static void virtio_crypto_skcipher_finalize_req(
 +static void virtio_crypto_ablkcipher_finalize_req(
  	struct virtio_crypto_sym_request *vc_sym_req,
 -	struct skcipher_request *req,
 +	struct ablkcipher_request *req,
  	int err)
  {
 -	if (vc_sym_req->encrypt)
 -		scatterwalk_map_and_copy(req->iv, req->dst,
 -					 req->cryptlen - AES_BLOCK_SIZE,
 -					 AES_BLOCK_SIZE, 0);
 +	crypto_finalize_ablkcipher_request(vc_sym_req->base.dataq->engine,
 +					   req, err);
  	kzfree(vc_sym_req->iv);
  	virtcrypto_clear_request(&vc_sym_req->base);
 -
 -	crypto_finalize_skcipher_request(vc_sym_req->base.dataq->engine,
 -					   req, err);
  }
  
++<<<<<<< HEAD
 +static struct crypto_alg virtio_crypto_algs[] = { {
 +	.cra_name = "cbc(aes)",
 +	.cra_driver_name = "virtio_crypto_aes_cbc",
 +	.cra_priority = 150,
 +	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
 +	.cra_blocksize = AES_BLOCK_SIZE,
 +	.cra_ctxsize  = sizeof(struct virtio_crypto_ablkcipher_ctx),
 +	.cra_alignmask = 0,
 +	.cra_module = THIS_MODULE,
 +	.cra_type = &crypto_ablkcipher_type,
 +	.cra_init = virtio_crypto_ablkcipher_init,
 +	.cra_exit = virtio_crypto_ablkcipher_exit,
 +	.cra_u = {
 +	   .ablkcipher = {
 +			.setkey = virtio_crypto_ablkcipher_setkey,
 +			.decrypt = virtio_crypto_ablkcipher_decrypt,
 +			.encrypt = virtio_crypto_ablkcipher_encrypt,
 +			.min_keysize = AES_MIN_KEY_SIZE,
 +			.max_keysize = AES_MAX_KEY_SIZE,
 +			.ivsize = AES_BLOCK_SIZE,
 +		},
++=======
+ static struct virtio_crypto_algo virtio_crypto_algs[] = { {
+ 	.algonum = VIRTIO_CRYPTO_CIPHER_AES_CBC,
+ 	.service = VIRTIO_CRYPTO_SERVICE_CIPHER,
+ 	.algo = {
+ 		.base.cra_name		= "cbc(aes)",
+ 		.base.cra_driver_name	= "virtio_crypto_aes_cbc",
+ 		.base.cra_priority	= 150,
+ 		.base.cra_flags		= CRYPTO_ALG_ASYNC |
+ 					  CRYPTO_ALG_ALLOCATES_MEMORY,
+ 		.base.cra_blocksize	= AES_BLOCK_SIZE,
+ 		.base.cra_ctxsize	= sizeof(struct virtio_crypto_skcipher_ctx),
+ 		.base.cra_module	= THIS_MODULE,
+ 		.init			= virtio_crypto_skcipher_init,
+ 		.exit			= virtio_crypto_skcipher_exit,
+ 		.setkey			= virtio_crypto_skcipher_setkey,
+ 		.decrypt		= virtio_crypto_skcipher_decrypt,
+ 		.encrypt		= virtio_crypto_skcipher_encrypt,
+ 		.min_keysize		= AES_MIN_KEY_SIZE,
+ 		.max_keysize		= AES_MAX_KEY_SIZE,
+ 		.ivsize			= AES_BLOCK_SIZE,
++>>>>>>> b8aa7dc5c753 (crypto: drivers - set the flag CRYPTO_ALG_ALLOCATES_MEMORY)
  	},
  } };
  
* Unmerged path drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c
* Unmerged path drivers/crypto/allwinner/sun8i-ss/sun8i-ss-core.c
* Unmerged path drivers/crypto/amlogic/amlogic-gxl-core.c
* Unmerged path drivers/crypto/caam/caamalg_qi2.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_aead.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_skcipher.c
* Unmerged path drivers/crypto/hisilicon/sec/sec_algs.c
* Unmerged path drivers/crypto/hisilicon/sec2/sec_crypto.c
* Unmerged path drivers/crypto/marvell/octeontx/otx_cptvf_algs.c
* Unmerged path drivers/crypto/qce/skcipher.c
* Unmerged path drivers/crypto/xilinx/zynqmp-aes-gcm.c
* Unmerged path drivers/crypto/allwinner/sun8i-ce/sun8i-ce-core.c
* Unmerged path drivers/crypto/allwinner/sun8i-ss/sun8i-ss-core.c
* Unmerged path drivers/crypto/amlogic/amlogic-gxl-core.c
* Unmerged path drivers/crypto/axis/artpec6_crypto.c
* Unmerged path drivers/crypto/bcm/cipher.c
* Unmerged path drivers/crypto/caam/caamalg.c
* Unmerged path drivers/crypto/caam/caamalg_qi.c
* Unmerged path drivers/crypto/caam/caamalg_qi2.c
* Unmerged path drivers/crypto/caam/caamhash.c
* Unmerged path drivers/crypto/cavium/cpt/cptvf_algs.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_aead.c
* Unmerged path drivers/crypto/cavium/nitrox/nitrox_skcipher.c
* Unmerged path drivers/crypto/ccp/ccp-crypto-aes-cmac.c
* Unmerged path drivers/crypto/ccp/ccp-crypto-aes-galois.c
* Unmerged path drivers/crypto/ccp/ccp-crypto-aes-xts.c
* Unmerged path drivers/crypto/ccp/ccp-crypto-aes.c
* Unmerged path drivers/crypto/ccp/ccp-crypto-des3.c
* Unmerged path drivers/crypto/ccp/ccp-crypto-sha.c
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/hisilicon/sec/sec_algs.c
* Unmerged path drivers/crypto/hisilicon/sec2/sec_crypto.c
* Unmerged path drivers/crypto/inside-secure/safexcel_cipher.c
* Unmerged path drivers/crypto/inside-secure/safexcel_hash.c
* Unmerged path drivers/crypto/ixp4xx_crypto.c
diff --git a/drivers/crypto/marvell/cipher.c b/drivers/crypto/marvell/cipher.c
index 177a27b6e4db..93ed1744816f 100644
--- a/drivers/crypto/marvell/cipher.c
+++ b/drivers/crypto/marvell/cipher.c
@@ -521,7 +521,8 @@ struct skcipher_alg mv_cesa_ecb_des_alg = {
 		.cra_name = "ecb(des)",
 		.cra_driver_name = "mv-ecb-des",
 		.cra_priority = 300,
-		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |
+			     CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = DES_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct mv_cesa_des_ctx),
 		.cra_alignmask = 0,
@@ -571,7 +572,8 @@ struct skcipher_alg mv_cesa_cbc_des_alg = {
 		.cra_name = "cbc(des)",
 		.cra_driver_name = "mv-cbc-des",
 		.cra_priority = 300,
-		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |
+			     CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = DES_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct mv_cesa_des_ctx),
 		.cra_alignmask = 0,
@@ -629,7 +631,8 @@ struct skcipher_alg mv_cesa_ecb_des3_ede_alg = {
 		.cra_name = "ecb(des3_ede)",
 		.cra_driver_name = "mv-ecb-des3-ede",
 		.cra_priority = 300,
-		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |
+			     CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct mv_cesa_des3_ctx),
 		.cra_alignmask = 0,
@@ -682,7 +685,8 @@ struct skcipher_alg mv_cesa_cbc_des3_ede_alg = {
 		.cra_name = "cbc(des3_ede)",
 		.cra_driver_name = "mv-cbc-des3-ede",
 		.cra_priority = 300,
-		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |
+			     CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = DES3_EDE_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct mv_cesa_des3_ctx),
 		.cra_alignmask = 0,
@@ -754,7 +758,8 @@ struct skcipher_alg mv_cesa_ecb_aes_alg = {
 		.cra_name = "ecb(aes)",
 		.cra_driver_name = "mv-ecb-aes",
 		.cra_priority = 300,
-		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |
+			     CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = AES_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct mv_cesa_aes_ctx),
 		.cra_alignmask = 0,
@@ -803,7 +808,8 @@ struct skcipher_alg mv_cesa_cbc_aes_alg = {
 		.cra_name = "cbc(aes)",
 		.cra_driver_name = "mv-cbc-aes",
 		.cra_priority = 300,
-		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY | CRYPTO_ALG_ASYNC |
+			     CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = AES_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct mv_cesa_aes_ctx),
 		.cra_alignmask = 0,
diff --git a/drivers/crypto/marvell/hash.c b/drivers/crypto/marvell/hash.c
index e34d80b6b7e5..33275a68b191 100644
--- a/drivers/crypto/marvell/hash.c
+++ b/drivers/crypto/marvell/hash.c
@@ -919,6 +919,7 @@ struct ahash_alg mv_md5_alg = {
 			.cra_driver_name = "mv-md5",
 			.cra_priority = 300,
 			.cra_flags = CRYPTO_ALG_ASYNC |
+				     CRYPTO_ALG_ALLOCATES_MEMORY |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
 			.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
 			.cra_ctxsize = sizeof(struct mv_cesa_hash_ctx),
@@ -989,6 +990,7 @@ struct ahash_alg mv_sha1_alg = {
 			.cra_driver_name = "mv-sha1",
 			.cra_priority = 300,
 			.cra_flags = CRYPTO_ALG_ASYNC |
+				     CRYPTO_ALG_ALLOCATES_MEMORY |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
 			.cra_blocksize = SHA1_BLOCK_SIZE,
 			.cra_ctxsize = sizeof(struct mv_cesa_hash_ctx),
@@ -1062,6 +1064,7 @@ struct ahash_alg mv_sha256_alg = {
 			.cra_driver_name = "mv-sha256",
 			.cra_priority = 300,
 			.cra_flags = CRYPTO_ALG_ASYNC |
+				     CRYPTO_ALG_ALLOCATES_MEMORY |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
 			.cra_blocksize = SHA256_BLOCK_SIZE,
 			.cra_ctxsize = sizeof(struct mv_cesa_hash_ctx),
@@ -1298,6 +1301,7 @@ struct ahash_alg mv_ahmac_md5_alg = {
 			.cra_driver_name = "mv-hmac-md5",
 			.cra_priority = 300,
 			.cra_flags = CRYPTO_ALG_ASYNC |
+				     CRYPTO_ALG_ALLOCATES_MEMORY |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
 			.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
 			.cra_ctxsize = sizeof(struct mv_cesa_hmac_ctx),
@@ -1368,6 +1372,7 @@ struct ahash_alg mv_ahmac_sha1_alg = {
 			.cra_driver_name = "mv-hmac-sha1",
 			.cra_priority = 300,
 			.cra_flags = CRYPTO_ALG_ASYNC |
+				     CRYPTO_ALG_ALLOCATES_MEMORY |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
 			.cra_blocksize = SHA1_BLOCK_SIZE,
 			.cra_ctxsize = sizeof(struct mv_cesa_hmac_ctx),
@@ -1438,6 +1443,7 @@ struct ahash_alg mv_ahmac_sha256_alg = {
 			.cra_driver_name = "mv-hmac-sha256",
 			.cra_priority = 300,
 			.cra_flags = CRYPTO_ALG_ASYNC |
+				     CRYPTO_ALG_ALLOCATES_MEMORY |
 				     CRYPTO_ALG_KERN_DRIVER_ONLY,
 			.cra_blocksize = SHA256_BLOCK_SIZE,
 			.cra_ctxsize = sizeof(struct mv_cesa_hmac_ctx),
* Unmerged path drivers/crypto/marvell/octeontx/otx_cptvf_algs.c
* Unmerged path drivers/crypto/n2_core.c
* Unmerged path drivers/crypto/picoxcell_crypto.c
diff --git a/drivers/crypto/qat/qat_common/qat_algs.c b/drivers/crypto/qat/qat_common/qat_algs.c
index a491ab0a3264..d552dbcfe0a0 100644
--- a/drivers/crypto/qat/qat_common/qat_algs.c
+++ b/drivers/crypto/qat/qat_common/qat_algs.c
@@ -1303,7 +1303,7 @@ static struct aead_alg qat_aeads[] = { {
 		.cra_name = "authenc(hmac(sha1),cbc(aes))",
 		.cra_driver_name = "qat_aes_cbc_hmac_sha1",
 		.cra_priority = 4001,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = AES_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct qat_alg_aead_ctx),
 		.cra_module = THIS_MODULE,
@@ -1320,7 +1320,7 @@ static struct aead_alg qat_aeads[] = { {
 		.cra_name = "authenc(hmac(sha256),cbc(aes))",
 		.cra_driver_name = "qat_aes_cbc_hmac_sha256",
 		.cra_priority = 4001,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = AES_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct qat_alg_aead_ctx),
 		.cra_module = THIS_MODULE,
@@ -1337,7 +1337,7 @@ static struct aead_alg qat_aeads[] = { {
 		.cra_name = "authenc(hmac(sha512),cbc(aes))",
 		.cra_driver_name = "qat_aes_cbc_hmac_sha512",
 		.cra_priority = 4001,
-		.cra_flags = CRYPTO_ALG_ASYNC,
+		.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,
 		.cra_blocksize = AES_BLOCK_SIZE,
 		.cra_ctxsize = sizeof(struct qat_alg_aead_ctx),
 		.cra_module = THIS_MODULE,
@@ -1355,7 +1355,7 @@ static struct skcipher_alg qat_skciphers[] = { {
 	.base.cra_name = "cbc(aes)",
 	.base.cra_driver_name = "qat_aes_cbc",
 	.base.cra_priority = 4001,
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,
 	.base.cra_blocksize = AES_BLOCK_SIZE,
 	.base.cra_ctxsize = sizeof(struct qat_alg_skcipher_ctx),
 	.base.cra_alignmask = 0,
@@ -1373,7 +1373,7 @@ static struct skcipher_alg qat_skciphers[] = { {
 	.base.cra_name = "ctr(aes)",
 	.base.cra_driver_name = "qat_aes_ctr",
 	.base.cra_priority = 4001,
-	.base.cra_flags = CRYPTO_ALG_ASYNC,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_ALLOCATES_MEMORY,
 	.base.cra_blocksize = 1,
 	.base.cra_ctxsize = sizeof(struct qat_alg_skcipher_ctx),
 	.base.cra_alignmask = 0,
@@ -1391,7 +1391,8 @@ static struct skcipher_alg qat_skciphers[] = { {
 	.base.cra_name = "xts(aes)",
 	.base.cra_driver_name = "qat_aes_xts",
 	.base.cra_priority = 4001,
-	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+	.base.cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK |
+			  CRYPTO_ALG_ALLOCATES_MEMORY,
 	.base.cra_blocksize = AES_BLOCK_SIZE,
 	.base.cra_ctxsize = sizeof(struct qat_alg_skcipher_ctx),
 	.base.cra_alignmask = 0,
* Unmerged path drivers/crypto/qce/skcipher.c
* Unmerged path drivers/crypto/talitos.c
* Unmerged path drivers/crypto/virtio/virtio_crypto_algs.c
* Unmerged path drivers/crypto/xilinx/zynqmp-aes-gcm.c
