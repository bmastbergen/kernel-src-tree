mptcp: rework poll+nospace handling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Florian Westphal <fw@strlen.de>
commit 8edf08649eede6e5a3e39a3d38c63f30039a0c1e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/8edf0864.failed

MPTCP maintains a status bit, MPTCP_SEND_SPACE, that is set when at
least one subflow and the mptcp socket itself are writeable.

mptcp_poll returns EPOLLOUT if the bit is set.

mptcp_sendmsg makes sure MPTCP_SEND_SPACE gets cleared when last write
has used up all subflows or the mptcp socket wmem.

This reworks nospace handling as follows:

MPTCP_SEND_SPACE is replaced with MPTCP_NOSPACE, i.e. inverted meaning.
This bit is set when the mptcp socket is not writeable.
The mptcp-level ack path schedule will then schedule the mptcp worker
to allow it to free already-acked data (and reduce wmem usage).

This will then wake userspace processes that wait for a POLLOUT event.

sendmsg will set MPTCP_NOSPACE only when it has to wait for more
wmem (blocking I/O case).

poll path will set MPTCP_NOSPACE in case the mptcp socket is
not writeable.

Normal tcp-level notification (SOCK_NOSPACE) is only enabled
in case the subflow socket has no available wmem.

	Signed-off-by: Florian Westphal <fw@strlen.de>
	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit 8edf08649eede6e5a3e39a3d38c63f30039a0c1e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/mptcp/protocol.c
diff --cc net/mptcp/protocol.c
index d41791292d73,7fcd26011a3d..000000000000
--- a/net/mptcp/protocol.c
+++ b/net/mptcp/protocol.c
@@@ -506,10 -724,10 +506,17 @@@ void mptcp_data_acked(struct sock *sk
  {
  	mptcp_reset_timer(sk);
  
++<<<<<<< HEAD
 +	if ((!test_bit(MPTCP_SEND_SPACE, &mptcp_sk(sk)->flags) ||
 +	     (inet_sk_state_load(sk) != TCP_ESTABLISHED)) &&
 +	    schedule_work(&mptcp_sk(sk)->work))
 +		sock_hold(sk);
++=======
+ 	if ((test_bit(MPTCP_NOSPACE, &mptcp_sk(sk)->flags) ||
+ 	     mptcp_send_head(sk) ||
+ 	     (inet_sk_state_load(sk) != TCP_ESTABLISHED)))
+ 		mptcp_schedule_work(sk);
++>>>>>>> 8edf08649eed (mptcp: rework poll+nospace handling)
  }
  
  void mptcp_subflow_eof(struct sock *sk)
@@@ -876,52 -1027,195 +864,135 @@@ static void mptcp_nospace(struct mptcp_
  
  	mptcp_for_each_subflow(msk, subflow) {
  		struct sock *ssk = mptcp_subflow_tcp_sock(subflow);
+ 		bool ssk_writeable = sk_stream_is_writeable(ssk);
  		struct socket *sock = READ_ONCE(ssk->sk_socket);
  
+ 		if (ssk_writeable || !sock)
+ 			continue;
+ 
  		/* enables ssk->write_space() callbacks */
- 		if (sock)
- 			set_bit(SOCK_NOSPACE, &sock->flags);
+ 		set_bit(SOCK_NOSPACE, &sock->flags);
  	}
+ 
+ 	/* mptcp_data_acked() could run just before we set the NOSPACE bit,
+ 	 * so explicitly check for snd_una value
+ 	 */
+ 	mptcp_clean_una((struct sock *)msk);
  }
  
 -static bool mptcp_subflow_active(struct mptcp_subflow_context *subflow)
 +static struct sock *mptcp_subflow_get_send(struct mptcp_sock *msk)
  {
 -	struct sock *ssk = mptcp_subflow_tcp_sock(subflow);
 -
 -	/* can't send if JOIN hasn't completed yet (i.e. is usable for mptcp) */
 -	if (subflow->request_join && !subflow->fully_established)
 -		return false;
 -
 -	/* only send if our side has not closed yet */
 -	return ((1 << ssk->sk_state) & (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT));
 -}
 -
 -#define MPTCP_SEND_BURST_SIZE		((1 << 16) - \
 -					 sizeof(struct tcphdr) - \
 -					 MAX_TCP_OPTION_SPACE - \
 -					 sizeof(struct ipv6hdr) - \
 -					 sizeof(struct frag_hdr))
 -
 -struct subflow_send_info {
 -	struct sock *ssk;
 -	u64 ratio;
 -};
 -
 -static struct sock *mptcp_subflow_get_send(struct mptcp_sock *msk,
 -					   u32 *sndbuf)
 -{
 -	struct subflow_send_info send_info[2];
  	struct mptcp_subflow_context *subflow;
 -	int i, nr_active = 0;
 -	struct sock *ssk;
 -	u64 ratio;
 -	u32 pace;
 +	struct sock *sk = (struct sock *)msk;
 +	struct sock *backup = NULL;
 +	bool free;
  
 -	sock_owned_by_me((struct sock *)msk);
 +	sock_owned_by_me(sk);
  
 -	*sndbuf = 0;
  	if (!mptcp_ext_cache_refill(msk))
  		return NULL;
  
 -	if (__mptcp_check_fallback(msk)) {
 -		if (!msk->first)
 -			return NULL;
 -		*sndbuf = msk->first->sk_sndbuf;
 -		return sk_stream_memory_free(msk->first) ? msk->first : NULL;
 -	}
 +	mptcp_for_each_subflow(msk, subflow) {
 +		struct sock *ssk = mptcp_subflow_tcp_sock(subflow);
  
 -	/* re-use last subflow, if the burst allow that */
 -	if (msk->last_snd && msk->snd_burst > 0 &&
 -	    sk_stream_memory_free(msk->last_snd) &&
 -	    mptcp_subflow_active(mptcp_subflow_ctx(msk->last_snd))) {
 -		mptcp_for_each_subflow(msk, subflow) {
 -			ssk =  mptcp_subflow_tcp_sock(subflow);
 -			*sndbuf = max(tcp_sk(ssk)->snd_wnd, *sndbuf);
 +		free = sk_stream_is_writeable(subflow->tcp_sock);
 +		if (!free) {
 +			mptcp_nospace(msk);
 +			return NULL;
  		}
 -		return msk->last_snd;
 -	}
 -
 -	/* pick the subflow with the lower wmem/wspace ratio */
 -	for (i = 0; i < 2; ++i) {
 -		send_info[i].ssk = NULL;
 -		send_info[i].ratio = -1;
 -	}
 -	mptcp_for_each_subflow(msk, subflow) {
 -		ssk =  mptcp_subflow_tcp_sock(subflow);
 -		if (!mptcp_subflow_active(subflow))
 -			continue;
  
 -		nr_active += !subflow->backup;
 -		*sndbuf = max(tcp_sk(ssk)->snd_wnd, *sndbuf);
 -		if (!sk_stream_memory_free(subflow->tcp_sock))
 -			continue;
 +		if (subflow->backup) {
 +			if (!backup)
 +				backup = ssk;
  
 -		pace = READ_ONCE(ssk->sk_pacing_rate);
 -		if (!pace)
  			continue;
 -
 -		ratio = div_u64((u64)READ_ONCE(ssk->sk_wmem_queued) << 32,
 -				pace);
 -		if (ratio < send_info[subflow->backup].ratio) {
 -			send_info[subflow->backup].ssk = ssk;
 -			send_info[subflow->backup].ratio = ratio;
  		}
 -	}
  
 -	pr_debug("msk=%p nr_active=%d ssk=%p:%lld backup=%p:%lld",
 -		 msk, nr_active, send_info[0].ssk, send_info[0].ratio,
 -		 send_info[1].ssk, send_info[1].ratio);
 -
 -	/* pick the best backup if no other subflow is active */
 -	if (!nr_active)
 -		send_info[0].ssk = send_info[1].ssk;
 -
 -	if (send_info[0].ssk) {
 -		msk->last_snd = send_info[0].ssk;
 -		msk->snd_burst = min_t(int, MPTCP_SEND_BURST_SIZE,
 -				       sk_stream_wspace(msk->last_snd));
 -		return msk->last_snd;
 +		return ssk;
  	}
 -	return NULL;
 +
 +	return backup;
  }
  
++<<<<<<< HEAD
 +static void ssk_check_wmem(struct mptcp_sock *msk)
 +{
 +	if (unlikely(!mptcp_is_writeable(msk)))
 +		mptcp_nospace(msk);
++=======
+ static void mptcp_push_release(struct sock *sk, struct sock *ssk,
+ 			       struct mptcp_sendmsg_info *info)
+ {
+ 	mptcp_set_timeout(sk, ssk);
+ 	tcp_push(ssk, 0, info->mss_now, tcp_sk(ssk)->nonagle, info->size_goal);
+ 	release_sock(ssk);
+ }
+ 
+ static void mptcp_push_pending(struct sock *sk, unsigned int flags)
+ {
+ 	struct sock *prev_ssk = NULL, *ssk = NULL;
+ 	struct mptcp_sock *msk = mptcp_sk(sk);
+ 	struct mptcp_sendmsg_info info = {
+ 				.flags = flags,
+ 	};
+ 	struct mptcp_data_frag *dfrag;
+ 	int len, copied = 0;
+ 	u32 sndbuf;
+ 
+ 	while ((dfrag = mptcp_send_head(sk))) {
+ 		info.sent = dfrag->already_sent;
+ 		info.limit = dfrag->data_len;
+ 		len = dfrag->data_len - dfrag->already_sent;
+ 		while (len > 0) {
+ 			int ret = 0;
+ 
+ 			prev_ssk = ssk;
+ 			__mptcp_flush_join_list(msk);
+ 			ssk = mptcp_subflow_get_send(msk, &sndbuf);
+ 
+ 			/* do auto tuning */
+ 			if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK) &&
+ 			    sndbuf > READ_ONCE(sk->sk_sndbuf))
+ 				WRITE_ONCE(sk->sk_sndbuf, sndbuf);
+ 
+ 			/* try to keep the subflow socket lock across
+ 			 * consecutive xmit on the same socket
+ 			 */
+ 			if (ssk != prev_ssk && prev_ssk)
+ 				mptcp_push_release(sk, prev_ssk, &info);
+ 			if (!ssk)
+ 				goto out;
+ 
+ 			if (ssk != prev_ssk || !prev_ssk)
+ 				lock_sock(ssk);
+ 
+ 			ret = mptcp_sendmsg_frag(sk, ssk, dfrag, &info);
+ 			if (ret <= 0) {
+ 				mptcp_push_release(sk, ssk, &info);
+ 				goto out;
+ 			}
+ 
+ 			info.sent += ret;
+ 			dfrag->already_sent += ret;
+ 			msk->snd_nxt += ret;
+ 			msk->snd_burst -= ret;
+ 			copied += ret;
+ 			len -= ret;
+ 		}
+ 		WRITE_ONCE(msk->first_pending, mptcp_send_next(sk));
+ 	}
+ 
+ 	/* at this point we held the socket lock for the last subflow we used */
+ 	if (ssk)
+ 		mptcp_push_release(sk, ssk, &info);
+ 
+ out:
+ 	/* start the timer, if it's not pending */
+ 	if (!mptcp_timer_pending(sk))
+ 		mptcp_reset_timer(sk);
+ 	if (copied)
+ 		__mptcp_check_send_data_fin(sk);
++>>>>>>> 8edf08649eed (mptcp: rework poll+nospace handling)
  }
  
  static int mptcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
@@@ -946,119 -1239,93 +1017,127 @@@
  			goto out;
  	}
  
 -	pfrag = sk_page_frag(sk);
 +restart:
  	mptcp_clean_una(sk);
  
 -	while (msg_data_left(msg)) {
 -		struct mptcp_data_frag *dfrag;
 -		int frag_truesize = 0;
 -		bool dfrag_collapsed;
 -		size_t psize, offset;
 +	if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN)) {
 +		ret = -EPIPE;
 +		goto out;
 +	}
  
 -		if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN)) {
 -			ret = -EPIPE;
 -			goto out;
 +	__mptcp_flush_join_list(msk);
 +	ssk = mptcp_subflow_get_send(msk);
 +	while (!sk_stream_memory_free(sk) || !ssk) {
 +		if (ssk) {
 +			/* make sure retransmit timer is
 +			 * running before we wait for memory.
 +			 *
 +			 * The retransmit timer might be needed
 +			 * to make the peer send an up-to-date
 +			 * MPTCP Ack.
 +			 */
 +			mptcp_set_timeout(sk, ssk);
 +			if (!mptcp_timer_pending(sk))
 +				mptcp_reset_timer(sk);
  		}
  
 -		/* reuse tail pfrag, if possible, or carve a new one from the
 -		 * page allocator
 -		 */
 -		dfrag = mptcp_pending_tail(sk);
 -		dfrag_collapsed = mptcp_frag_can_collapse_to(msk, pfrag, dfrag);
 -		if (!dfrag_collapsed) {
 -			if (!sk_stream_memory_free(sk)) {
 -				mptcp_push_pending(sk, msg->msg_flags);
 -				if (!sk_stream_memory_free(sk))
 -					goto wait_for_memory;
 -			}
 -			if (!mptcp_page_frag_refill(sk, pfrag))
 -				goto wait_for_memory;
 +		mptcp_nospace(msk);
++<<<<<<< HEAD
++=======
++		if (mptcp_timer_pending(sk))
++			mptcp_reset_timer(sk);
++>>>>>>> 8edf08649eed (mptcp: rework poll+nospace handling)
 +		ret = sk_stream_wait_memory(sk, &timeo);
 +		if (ret)
 +			goto out;
  
 -			dfrag = mptcp_carve_data_frag(msk, pfrag, pfrag->offset);
 -			frag_truesize = dfrag->overhead;
 -		}
 +		mptcp_clean_una(sk);
  
 -		/* we do not bound vs wspace, to allow a single packet.
 -		 * memory accounting will prevent execessive memory usage
 -		 * anyway
 -		 */
 -		offset = dfrag->offset + dfrag->data_len;
 -		psize = pfrag->size - offset;
 -		psize = min_t(size_t, psize, msg_data_left(msg));
 -		if (!sk_wmem_schedule(sk, psize + frag_truesize))
 -			goto wait_for_memory;
 -
 -		if (copy_page_from_iter(dfrag->page, offset, psize,
 -					&msg->msg_iter) != psize) {
 -			ret = -EFAULT;
 +		ssk = mptcp_subflow_get_send(msk);
 +		if (list_empty(&msk->conn_list)) {
 +			ret = -ENOTCONN;
  			goto out;
  		}
 +	}
  
 -		/* data successfully copied into the write queue */
 -		copied += psize;
 -		dfrag->data_len += psize;
 -		frag_truesize += psize;
 -		pfrag->offset += frag_truesize;
 -		WRITE_ONCE(msk->write_seq, msk->write_seq + psize);
 +	pr_debug("conn_list->subflow=%p", ssk);
  
 -		/* charge data on mptcp pending queue to the msk socket
 -		 * Note: we charge such data both to sk and ssk
 +	lock_sock(ssk);
 +	tx_ok = msg_data_left(msg);
 +	while (tx_ok) {
 +		ret = mptcp_sendmsg_frag(sk, ssk, msg, NULL, &timeo, &mss_now,
 +					 &size_goal);
 +		if (ret < 0) {
 +			if (ret == -EAGAIN && timeo > 0) {
 +				mptcp_set_timeout(sk, ssk);
 +				release_sock(ssk);
 +				goto restart;
 +			}
 +			break;
 +		}
 +
 +		copied += ret;
 +
 +		tx_ok = msg_data_left(msg);
 +		if (!tx_ok)
 +			break;
 +
 +		if (!sk_stream_memory_free(ssk) ||
 +		    !mptcp_ext_cache_refill(msk)) {
 +			tcp_push(ssk, msg->msg_flags, mss_now,
 +				 tcp_sk(ssk)->nonagle, size_goal);
 +			mptcp_set_timeout(sk, ssk);
 +			release_sock(ssk);
 +			goto restart;
 +		}
 +
 +		/* memory is charged to mptcp level socket as well, i.e.
 +		 * if msg is very large, mptcp socket may run out of buffer
 +		 * space.  mptcp_clean_una() will release data that has
 +		 * been acked at mptcp level in the mean time, so there is
 +		 * a good chance we can continue sending data right away.
 +		 *
 +		 * Normally, when the tcp subflow can accept more data, then
 +		 * so can the MPTCP socket.  However, we need to cope with
 +		 * peers that might lag behind in their MPTCP-level
 +		 * acknowledgements, i.e.  data might have been acked at
 +		 * tcp level only.  So, we must also check the MPTCP socket
 +		 * limits before we send more data.
  		 */
 -		sk_wmem_queued_add(sk, frag_truesize);
 -		sk->sk_forward_alloc -= frag_truesize;
 -		if (!dfrag_collapsed) {
 -			get_page(dfrag->page);
 -			list_add_tail(&dfrag->list, &msk->rtx_queue);
 -			if (!msk->first_pending)
 -				WRITE_ONCE(msk->first_pending, dfrag);
 +		if (unlikely(!sk_stream_memory_free(sk))) {
 +			tcp_push(ssk, msg->msg_flags, mss_now,
 +				 tcp_sk(ssk)->nonagle, size_goal);
 +			mptcp_clean_una(sk);
 +			if (!sk_stream_memory_free(sk)) {
 +				/* can't send more for now, need to wait for
 +				 * MPTCP-level ACKs from peer.
 +				 *
 +				 * Wakeup will happen via mptcp_clean_una().
 +				 */
 +				mptcp_set_timeout(sk, ssk);
 +				release_sock(ssk);
 +				goto restart;
 +			}
  		}
 -		pr_debug("msk=%p dfrag at seq=%lld len=%d sent=%d new=%d", msk,
 -			 dfrag->data_seq, dfrag->data_len, dfrag->already_sent,
 -			 !dfrag_collapsed);
 +	}
  
 -		if (!mptcp_ext_cache_refill(msk))
 -			goto wait_for_memory;
 -		continue;
 +	mptcp_set_timeout(sk, ssk);
 +	if (copied) {
 +		tcp_push(ssk, msg->msg_flags, mss_now, tcp_sk(ssk)->nonagle,
 +			 size_goal);
  
 -wait_for_memory:
 -		mptcp_nospace(msk);
 -		if (mptcp_timer_pending(sk))
 +		/* start the timer, if it's not pending */
 +		if (!mptcp_timer_pending(sk))
  			mptcp_reset_timer(sk);
 -		ret = sk_stream_wait_memory(sk, &timeo);
 -		if (ret)
 -			goto out;
  	}
  
 -	if (copied)
 -		mptcp_push_pending(sk, msg->msg_flags);
 -
 +	release_sock(ssk);
  out:
++<<<<<<< HEAD
 +	msk->snd_nxt = msk->write_seq;
 +	ssk_check_wmem(msk);
++=======
++>>>>>>> 8edf08649eed (mptcp: rework poll+nospace handling)
  	release_sock(sk);
  	return copied ? : ret;
  }
@@@ -1508,8 -1902,9 +1587,7 @@@ static int __mptcp_init_sock(struct soc
  	INIT_LIST_HEAD(&msk->conn_list);
  	INIT_LIST_HEAD(&msk->join_list);
  	INIT_LIST_HEAD(&msk->rtx_queue);
- 	__set_bit(MPTCP_SEND_SPACE, &msk->flags);
  	INIT_WORK(&msk->work, mptcp_worker);
 -	msk->out_of_order_queue = RB_ROOT;
 -	msk->first_pending = NULL;
  
  	msk->first = NULL;
  	inet_csk(sk)->icsk_sync_mss = mptcp_sync_mss;
@@@ -2118,13 -2599,6 +2196,16 @@@ bool mptcp_finish_join(struct sock *sk
  	return true;
  }
  
++<<<<<<< HEAD
 +static bool mptcp_memory_free(const struct sock *sk)
 +{
 +	struct mptcp_sock *msk = mptcp_sk(sk);
 +
 +	return test_bit(MPTCP_SEND_SPACE, &msk->flags);
 +}
 +
++=======
++>>>>>>> 8edf08649eed (mptcp: rework poll+nospace handling)
  static struct proto mptcp_prot = {
  	.name		= "MPTCP",
  	.owner		= THIS_MODULE,
* Unmerged path net/mptcp/protocol.c
diff --git a/net/mptcp/protocol.h b/net/mptcp/protocol.h
index a60ec79c4e54..628b6d50800a 100644
--- a/net/mptcp/protocol.h
+++ b/net/mptcp/protocol.h
@@ -86,7 +86,7 @@
 
 /* MPTCP socket flags */
 #define MPTCP_DATA_READY	0
-#define MPTCP_SEND_SPACE	1
+#define MPTCP_NOSPACE		1
 #define MPTCP_WORK_RTX		2
 #define MPTCP_WORK_EOF		3
 #define MPTCP_FALLBACK_DONE	4
diff --git a/net/mptcp/subflow.c b/net/mptcp/subflow.c
index dcdd522ad5a5..f5952dfc1d88 100644
--- a/net/mptcp/subflow.c
+++ b/net/mptcp/subflow.c
@@ -968,17 +968,16 @@ static void subflow_data_ready(struct sock *sk)
 static void subflow_write_space(struct sock *sk)
 {
 	struct mptcp_subflow_context *subflow = mptcp_subflow_ctx(sk);
+	struct socket *sock = READ_ONCE(sk->sk_socket);
 	struct sock *parent = subflow->conn;
 
 	if (!sk_stream_is_writeable(sk))
 		return;
 
-	if (sk_stream_is_writeable(parent)) {
-		set_bit(MPTCP_SEND_SPACE, &mptcp_sk(parent)->flags);
-		smp_mb__after_atomic();
-		/* set SEND_SPACE before sk_stream_write_space clears NOSPACE */
-		sk_stream_write_space(parent);
-	}
+	if (sock && sk_stream_is_writeable(parent))
+		clear_bit(SOCK_NOSPACE, &sock->flags);
+
+	sk_stream_write_space(parent);
 }
 
 static struct inet_connection_sock_af_ops *
