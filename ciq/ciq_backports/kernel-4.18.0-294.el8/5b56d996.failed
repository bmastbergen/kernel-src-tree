mm/compaction.c: abort search if isolation fails

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Qian Cai <cai@lca.pw>
commit 5b56d996dd50a9d2ca87c25ebb50c07b255b7e04
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/5b56d996.failed

Running LTP oom01 in a tight loop or memory stress testing put the system
in a low-memory situation could triggers random memory corruption like
page flag corruption below due to in fast_isolate_freepages(), if
isolation fails, next_search_order() does not abort the search immediately
could lead to improper accesses.

UBSAN: Undefined behaviour in ./include/linux/mm.h:1195:50
index 7 is out of range for type 'zone [5]'
Call Trace:
 dump_stack+0x62/0x9a
 ubsan_epilogue+0xd/0x7f
 __ubsan_handle_out_of_bounds+0x14d/0x192
 __isolate_free_page+0x52c/0x600
 compaction_alloc+0x886/0x25f0
 unmap_and_move+0x37/0x1e70
 migrate_pages+0x2ca/0xb20
 compact_zone+0x19cb/0x3620
 kcompactd_do_work+0x2df/0x680
 kcompactd+0x1d8/0x6c0
 kthread+0x32c/0x3f0
 ret_from_fork+0x35/0x40
------------[ cut here ]------------
kernel BUG at mm/page_alloc.c:3124!
invalid opcode: 0000 [#1] SMP DEBUG_PAGEALLOC KASAN PTI
RIP: 0010:__isolate_free_page+0x464/0x600
RSP: 0000:ffff888b9e1af848 EFLAGS: 00010007
RAX: 0000000030000000 RBX: ffff888c39fcf0f8 RCX: 0000000000000000
RDX: 1ffff111873f9e25 RSI: 0000000000000004 RDI: ffffed1173c35ef6
RBP: ffff888b9e1af898 R08: fffffbfff4fc2461 R09: fffffbfff4fc2460
R10: fffffbfff4fc2460 R11: ffffffffa7e12303 R12: 0000000000000008
R13: dffffc0000000000 R14: 0000000000000000 R15: 0000000000000007
FS:  0000000000000000(0000) GS:ffff888ba8e80000(0000)
knlGS:0000000000000000
CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
CR2: 00007fc7abc00000 CR3: 0000000752416004 CR4: 00000000001606a0
DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
Call Trace:
 compaction_alloc+0x886/0x25f0
 unmap_and_move+0x37/0x1e70
 migrate_pages+0x2ca/0xb20
 compact_zone+0x19cb/0x3620
 kcompactd_do_work+0x2df/0x680
 kcompactd+0x1d8/0x6c0
 kthread+0x32c/0x3f0
 ret_from_fork+0x35/0x40

Link: http://lkml.kernel.org/r/20190320192648.52499-1-cai@lca.pw
Fixes: dbe2d4e4f12e ("mm, compaction: round-robin the order while searching the free lists for a target")
	Signed-off-by: Qian Cai <cai@lca.pw>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
	Cc: Mikhail Gavrilov <mikhail.v.gavrilov@gmail.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
(cherry picked from commit 5b56d996dd50a9d2ca87c25ebb50c07b255b7e04)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/compaction.c
diff --cc mm/compaction.c
index 79db11f23bf2,3319e0872d01..000000000000
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@@ -1054,6 -1178,248 +1054,251 @@@ static inline bool compact_scanners_met
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Used when scanning for a suitable migration target which scans freelists
+  * in reverse. Reorders the list such as the unscanned pages are scanned
+  * first on the next iteration of the free scanner
+  */
+ static void
+ move_freelist_head(struct list_head *freelist, struct page *freepage)
+ {
+ 	LIST_HEAD(sublist);
+ 
+ 	if (!list_is_last(freelist, &freepage->lru)) {
+ 		list_cut_before(&sublist, freelist, &freepage->lru);
+ 		if (!list_empty(&sublist))
+ 			list_splice_tail(&sublist, freelist);
+ 	}
+ }
+ 
+ /*
+  * Similar to move_freelist_head except used by the migration scanner
+  * when scanning forward. It's possible for these list operations to
+  * move against each other if they search the free list exactly in
+  * lockstep.
+  */
+ static void
+ move_freelist_tail(struct list_head *freelist, struct page *freepage)
+ {
+ 	LIST_HEAD(sublist);
+ 
+ 	if (!list_is_first(freelist, &freepage->lru)) {
+ 		list_cut_position(&sublist, freelist, &freepage->lru);
+ 		if (!list_empty(&sublist))
+ 			list_splice_tail(&sublist, freelist);
+ 	}
+ }
+ 
+ static void
+ fast_isolate_around(struct compact_control *cc, unsigned long pfn, unsigned long nr_isolated)
+ {
+ 	unsigned long start_pfn, end_pfn;
+ 	struct page *page = pfn_to_page(pfn);
+ 
+ 	/* Do not search around if there are enough pages already */
+ 	if (cc->nr_freepages >= cc->nr_migratepages)
+ 		return;
+ 
+ 	/* Minimise scanning during async compaction */
+ 	if (cc->direct_compaction && cc->mode == MIGRATE_ASYNC)
+ 		return;
+ 
+ 	/* Pageblock boundaries */
+ 	start_pfn = pageblock_start_pfn(pfn);
+ 	end_pfn = min(start_pfn + pageblock_nr_pages, zone_end_pfn(cc->zone));
+ 
+ 	/* Scan before */
+ 	if (start_pfn != pfn) {
+ 		isolate_freepages_block(cc, &start_pfn, pfn, &cc->freepages, 1, false);
+ 		if (cc->nr_freepages >= cc->nr_migratepages)
+ 			return;
+ 	}
+ 
+ 	/* Scan after */
+ 	start_pfn = pfn + nr_isolated;
+ 	if (start_pfn != end_pfn)
+ 		isolate_freepages_block(cc, &start_pfn, end_pfn, &cc->freepages, 1, false);
+ 
+ 	/* Skip this pageblock in the future as it's full or nearly full */
+ 	if (cc->nr_freepages < cc->nr_migratepages)
+ 		set_pageblock_skip(page);
+ }
+ 
+ /* Search orders in round-robin fashion */
+ static int next_search_order(struct compact_control *cc, int order)
+ {
+ 	order--;
+ 	if (order < 0)
+ 		order = cc->order - 1;
+ 
+ 	/* Search wrapped around? */
+ 	if (order == cc->search_order) {
+ 		cc->search_order--;
+ 		if (cc->search_order < 0)
+ 			cc->search_order = cc->order - 1;
+ 		return -1;
+ 	}
+ 
+ 	return order;
+ }
+ 
+ static unsigned long
+ fast_isolate_freepages(struct compact_control *cc)
+ {
+ 	unsigned int limit = min(1U, freelist_scan_limit(cc) >> 1);
+ 	unsigned int nr_scanned = 0;
+ 	unsigned long low_pfn, min_pfn, high_pfn = 0, highest = 0;
+ 	unsigned long nr_isolated = 0;
+ 	unsigned long distance;
+ 	struct page *page = NULL;
+ 	bool scan_start = false;
+ 	int order;
+ 
+ 	/* Full compaction passes in a negative order */
+ 	if (cc->order <= 0)
+ 		return cc->free_pfn;
+ 
+ 	/*
+ 	 * If starting the scan, use a deeper search and use the highest
+ 	 * PFN found if a suitable one is not found.
+ 	 */
+ 	if (cc->free_pfn >= cc->zone->compact_init_free_pfn) {
+ 		limit = pageblock_nr_pages >> 1;
+ 		scan_start = true;
+ 	}
+ 
+ 	/*
+ 	 * Preferred point is in the top quarter of the scan space but take
+ 	 * a pfn from the top half if the search is problematic.
+ 	 */
+ 	distance = (cc->free_pfn - cc->migrate_pfn);
+ 	low_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 2));
+ 	min_pfn = pageblock_start_pfn(cc->free_pfn - (distance >> 1));
+ 
+ 	if (WARN_ON_ONCE(min_pfn > low_pfn))
+ 		low_pfn = min_pfn;
+ 
+ 	/*
+ 	 * Search starts from the last successful isolation order or the next
+ 	 * order to search after a previous failure
+ 	 */
+ 	cc->search_order = min_t(unsigned int, cc->order - 1, cc->search_order);
+ 
+ 	for (order = cc->search_order;
+ 	     !page && order >= 0;
+ 	     order = next_search_order(cc, order)) {
+ 		struct free_area *area = &cc->zone->free_area[order];
+ 		struct list_head *freelist;
+ 		struct page *freepage;
+ 		unsigned long flags;
+ 		unsigned int order_scanned = 0;
+ 
+ 		if (!area->nr_free)
+ 			continue;
+ 
+ 		spin_lock_irqsave(&cc->zone->lock, flags);
+ 		freelist = &area->free_list[MIGRATE_MOVABLE];
+ 		list_for_each_entry_reverse(freepage, freelist, lru) {
+ 			unsigned long pfn;
+ 
+ 			order_scanned++;
+ 			nr_scanned++;
+ 			pfn = page_to_pfn(freepage);
+ 
+ 			if (pfn >= highest)
+ 				highest = pageblock_start_pfn(pfn);
+ 
+ 			if (pfn >= low_pfn) {
+ 				cc->fast_search_fail = 0;
+ 				cc->search_order = order;
+ 				page = freepage;
+ 				break;
+ 			}
+ 
+ 			if (pfn >= min_pfn && pfn > high_pfn) {
+ 				high_pfn = pfn;
+ 
+ 				/* Shorten the scan if a candidate is found */
+ 				limit >>= 1;
+ 			}
+ 
+ 			if (order_scanned >= limit)
+ 				break;
+ 		}
+ 
+ 		/* Use a minimum pfn if a preferred one was not found */
+ 		if (!page && high_pfn) {
+ 			page = pfn_to_page(high_pfn);
+ 
+ 			/* Update freepage for the list reorder below */
+ 			freepage = page;
+ 		}
+ 
+ 		/* Reorder to so a future search skips recent pages */
+ 		move_freelist_head(freelist, freepage);
+ 
+ 		/* Isolate the page if available */
+ 		if (page) {
+ 			if (__isolate_free_page(page, order)) {
+ 				set_page_private(page, order);
+ 				nr_isolated = 1 << order;
+ 				cc->nr_freepages += nr_isolated;
+ 				list_add_tail(&page->lru, &cc->freepages);
+ 				count_compact_events(COMPACTISOLATED, nr_isolated);
+ 			} else {
+ 				/* If isolation fails, abort the search */
+ 				order = cc->search_order + 1;
+ 				page = NULL;
+ 			}
+ 		}
+ 
+ 		spin_unlock_irqrestore(&cc->zone->lock, flags);
+ 
+ 		/*
+ 		 * Smaller scan on next order so the total scan ig related
+ 		 * to freelist_scan_limit.
+ 		 */
+ 		if (order_scanned >= limit)
+ 			limit = min(1U, limit >> 1);
+ 	}
+ 
+ 	if (!page) {
+ 		cc->fast_search_fail++;
+ 		if (scan_start) {
+ 			/*
+ 			 * Use the highest PFN found above min. If one was
+ 			 * not found, be pessemistic for direct compaction
+ 			 * and use the min mark.
+ 			 */
+ 			if (highest) {
+ 				page = pfn_to_page(highest);
+ 				cc->free_pfn = highest;
+ 			} else {
+ 				if (cc->direct_compaction) {
+ 					page = pfn_to_page(min_pfn);
+ 					cc->free_pfn = min_pfn;
+ 				}
+ 			}
+ 		}
+ 	}
+ 
+ 	if (highest && highest >= cc->zone->compact_cached_free_pfn) {
+ 		highest -= pageblock_nr_pages;
+ 		cc->zone->compact_cached_free_pfn = highest;
+ 	}
+ 
+ 	cc->total_free_scanned += nr_scanned;
+ 	if (!page)
+ 		return cc->free_pfn;
+ 
+ 	low_pfn = page_to_pfn(page);
+ 	fast_isolate_around(cc, low_pfn, nr_isolated);
+ 	return low_pfn;
+ }
+ 
+ /*
++>>>>>>> 5b56d996dd50 (mm/compaction.c: abort search if isolation fails)
   * Based on information in the current compact_control, find blocks
   * suitable for isolating free pages from and then isolate them.
   */
* Unmerged path mm/compaction.c
