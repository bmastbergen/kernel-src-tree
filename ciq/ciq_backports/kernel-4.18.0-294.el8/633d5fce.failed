dma-direct: always align allocation size in dma_direct_alloc_pages()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author David Rientjes <rientjes@google.com>
commit 633d5fce78a61e8727674467944939f55b0bcfab
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/633d5fce.failed

dma_alloc_contiguous() does size >> PAGE_SHIFT and set_memory_decrypted()
works at page granularity.  It's necessary to page align the allocation
size in dma_direct_alloc_pages() for consistent behavior.

This also fixes an issue when arch_dma_prep_coherent() is called on an
unaligned allocation size for dma_alloc_need_uncached() when
CONFIG_DMA_DIRECT_REMAP is disabled but CONFIG_ARCH_HAS_DMA_SET_UNCACHED
is enabled.

	Signed-off-by: David Rientjes <rientjes@google.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 633d5fce78a61e8727674467944939f55b0bcfab)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/direct.c
diff --cc kernel/dma/direct.c
index 50772983c03a,c93e3c8e3d01..000000000000
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@@ -76,10 -76,42 +76,9 @@@ static bool dma_coherent_ok(struct devi
  			min_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);
  }
  
 -/*
 - * Decrypting memory is allowed to block, so if this device requires
 - * unencrypted memory it must come from atomic pools.
 - */
 -static inline bool dma_should_alloc_from_pool(struct device *dev, gfp_t gfp,
 -					      unsigned long attrs)
 -{
 -	if (!IS_ENABLED(CONFIG_DMA_COHERENT_POOL))
 -		return false;
 -	if (gfpflags_allow_blocking(gfp))
 -		return false;
 -	if (force_dma_unencrypted(dev))
 -		return true;
 -	if (!IS_ENABLED(CONFIG_DMA_DIRECT_REMAP))
 -		return false;
 -	if (dma_alloc_need_uncached(dev, attrs))
 -		return true;
 -	return false;
 -}
 -
 -static inline bool dma_should_free_from_pool(struct device *dev,
 -					     unsigned long attrs)
 -{
 -	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL))
 -		return true;
 -	if ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) &&
 -	    !force_dma_unencrypted(dev))
 -		return false;
 -	if (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP))
 -		return true;
 -	return false;
 -}
 -
 -static struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 +struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
  		gfp_t gfp, unsigned long attrs)
  {
- 	size_t alloc_size = PAGE_ALIGN(size);
  	int node = dev_to_node(dev);
  	struct page *page = NULL;
  	u64 phys_limit;
@@@ -89,11 -123,11 +90,17 @@@
  
  	/* we always manually zero the memory once we are done: */
  	gfp &= ~__GFP_ZERO;
++<<<<<<< HEAD
 +	gfp |= __dma_direct_optimal_gfp_mask(dev, dev->coherent_dma_mask,
 +			&phys_limit);
 +	page = dma_alloc_contiguous(dev, alloc_size, gfp);
++=======
+ 	gfp |= dma_direct_optimal_gfp_mask(dev, dev->coherent_dma_mask,
+ 					   &phys_limit);
+ 	page = dma_alloc_contiguous(dev, size, gfp);
++>>>>>>> 633d5fce78a6 (dma-direct: always align allocation size in dma_direct_alloc_pages())
  	if (page && !dma_coherent_ok(dev, page_to_phys(page), size)) {
- 		dma_free_contiguous(dev, page, alloc_size);
+ 		dma_free_contiguous(dev, page, size);
  		page = NULL;
  	}
  again:
@@@ -125,10 -159,10 +132,17 @@@ void *dma_direct_alloc_pages(struct dev
  	struct page *page;
  	void *ret;
  
++<<<<<<< HEAD
 +	if (IS_ENABLED(CONFIG_DMA_DIRECT_REMAP) &&
 +	    dma_alloc_need_uncached(dev, attrs) &&
 +	    !gfpflags_allow_blocking(gfp)) {
 +		ret = dma_alloc_from_pool(PAGE_ALIGN(size), &page, gfp);
++=======
+ 	size = PAGE_ALIGN(size);
+ 
+ 	if (dma_should_alloc_from_pool(dev, gfp, attrs)) {
+ 		ret = dma_alloc_from_pool(dev, size, &page, gfp);
++>>>>>>> 633d5fce78a6 (dma-direct: always align allocation size in dma_direct_alloc_pages())
  		if (!ret)
  			return NULL;
  		goto done;
* Unmerged path kernel/dma/direct.c
