mptcp: update rtx timeout only if required.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Paolo Abeni <pabeni@redhat.com>
commit b680a214ec281dbd44b5ebbf3f126a57f1ecf0f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/b680a214.failed

We must start the retransmission timer only there are
pending data in the rtx queue.
Otherwise we can hit a WARN_ON in mptcp_reset_timer(),
as syzbot demonstrated.

Reported-and-tested-by: syzbot+42aa53dafb66a07e5a24@syzkaller.appspotmail.com
Fixes: d9ca1de8c0cd ("mptcp: move page frag allocation in mptcp_sendmsg()")
	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Reviewed-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
	Reported-by: Naresh Kamboju <naresh.kamboju@linaro.org>
	Tested-by: Naresh Kamboju <naresh.kamboju@linaro.org>
Link: https://lore.kernel.org/r/1a72039f112cae048c44d398ffa14e0a1432db3d.1605737083.git.pabeni@redhat.com
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit b680a214ec281dbd44b5ebbf3f126a57f1ecf0f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/mptcp/protocol.c
diff --cc net/mptcp/protocol.c
index d41791292d73,aeda4357de9a..000000000000
--- a/net/mptcp/protocol.c
+++ b/net/mptcp/protocol.c
@@@ -896,32 -1131,142 +896,105 @@@ static struct sock *mptcp_subflow_get_s
  	if (!mptcp_ext_cache_refill(msk))
  		return NULL;
  
 -	if (__mptcp_check_fallback(msk)) {
 -		if (!msk->first)
 -			return NULL;
 -		*sndbuf = msk->first->sk_sndbuf;
 -		return sk_stream_memory_free(msk->first) ? msk->first : NULL;
 -	}
 +	mptcp_for_each_subflow(msk, subflow) {
 +		struct sock *ssk = mptcp_subflow_tcp_sock(subflow);
  
 -	/* re-use last subflow, if the burst allow that */
 -	if (msk->last_snd && msk->snd_burst > 0 &&
 -	    sk_stream_memory_free(msk->last_snd) &&
 -	    mptcp_subflow_active(mptcp_subflow_ctx(msk->last_snd))) {
 -		mptcp_for_each_subflow(msk, subflow) {
 -			ssk =  mptcp_subflow_tcp_sock(subflow);
 -			*sndbuf = max(tcp_sk(ssk)->snd_wnd, *sndbuf);
 +		free = sk_stream_is_writeable(subflow->tcp_sock);
 +		if (!free) {
 +			mptcp_nospace(msk);
 +			return NULL;
  		}
 -		return msk->last_snd;
 -	}
 -
 -	/* pick the subflow with the lower wmem/wspace ratio */
 -	for (i = 0; i < 2; ++i) {
 -		send_info[i].ssk = NULL;
 -		send_info[i].ratio = -1;
 -	}
 -	mptcp_for_each_subflow(msk, subflow) {
 -		ssk =  mptcp_subflow_tcp_sock(subflow);
 -		if (!mptcp_subflow_active(subflow))
 -			continue;
  
 -		nr_active += !subflow->backup;
 -		*sndbuf = max(tcp_sk(ssk)->snd_wnd, *sndbuf);
 -		if (!sk_stream_memory_free(subflow->tcp_sock))
 -			continue;
 +		if (subflow->backup) {
 +			if (!backup)
 +				backup = ssk;
  
 -		pace = READ_ONCE(ssk->sk_pacing_rate);
 -		if (!pace)
  			continue;
 -
 -		ratio = div_u64((u64)READ_ONCE(ssk->sk_wmem_queued) << 32,
 -				pace);
 -		if (ratio < send_info[subflow->backup].ratio) {
 -			send_info[subflow->backup].ssk = ssk;
 -			send_info[subflow->backup].ratio = ratio;
  		}
 -	}
 -
 -	pr_debug("msk=%p nr_active=%d ssk=%p:%lld backup=%p:%lld",
 -		 msk, nr_active, send_info[0].ssk, send_info[0].ratio,
 -		 send_info[1].ssk, send_info[1].ratio);
 -
 -	/* pick the best backup if no other subflow is active */
 -	if (!nr_active)
 -		send_info[0].ssk = send_info[1].ssk;
  
 -	if (send_info[0].ssk) {
 -		msk->last_snd = send_info[0].ssk;
 -		msk->snd_burst = min_t(int, MPTCP_SEND_BURST_SIZE,
 -				       sk_stream_wspace(msk->last_snd));
 -		return msk->last_snd;
 +		return ssk;
  	}
 -	return NULL;
 +
 +	return backup;
  }
  
 -static void mptcp_push_release(struct sock *sk, struct sock *ssk,
 -			       struct mptcp_sendmsg_info *info)
 +static void ssk_check_wmem(struct mptcp_sock *msk)
  {
++<<<<<<< HEAD
 +	if (unlikely(!mptcp_is_writeable(msk)))
 +		mptcp_nospace(msk);
++=======
+ 	mptcp_set_timeout(sk, ssk);
+ 	tcp_push(ssk, 0, info->mss_now, tcp_sk(ssk)->nonagle, info->size_goal);
+ 	release_sock(ssk);
+ }
+ 
+ static void mptcp_push_pending(struct sock *sk, unsigned int flags)
+ {
+ 	struct sock *prev_ssk = NULL, *ssk = NULL;
+ 	struct mptcp_sock *msk = mptcp_sk(sk);
+ 	struct mptcp_sendmsg_info info = {
+ 				.flags = flags,
+ 	};
+ 	struct mptcp_data_frag *dfrag;
+ 	int len, copied = 0;
+ 	u32 sndbuf;
+ 
+ 	while ((dfrag = mptcp_send_head(sk))) {
+ 		info.sent = dfrag->already_sent;
+ 		info.limit = dfrag->data_len;
+ 		len = dfrag->data_len - dfrag->already_sent;
+ 		while (len > 0) {
+ 			int ret = 0;
+ 
+ 			prev_ssk = ssk;
+ 			__mptcp_flush_join_list(msk);
+ 			ssk = mptcp_subflow_get_send(msk, &sndbuf);
+ 
+ 			/* do auto tuning */
+ 			if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK) &&
+ 			    sndbuf > READ_ONCE(sk->sk_sndbuf))
+ 				WRITE_ONCE(sk->sk_sndbuf, sndbuf);
+ 
+ 			/* try to keep the subflow socket lock across
+ 			 * consecutive xmit on the same socket
+ 			 */
+ 			if (ssk != prev_ssk && prev_ssk)
+ 				mptcp_push_release(sk, prev_ssk, &info);
+ 			if (!ssk)
+ 				goto out;
+ 
+ 			if (ssk != prev_ssk || !prev_ssk)
+ 				lock_sock(ssk);
+ 
+ 			ret = mptcp_sendmsg_frag(sk, ssk, dfrag, &info);
+ 			if (ret <= 0) {
+ 				mptcp_push_release(sk, ssk, &info);
+ 				goto out;
+ 			}
+ 
+ 			info.sent += ret;
+ 			dfrag->already_sent += ret;
+ 			msk->snd_nxt += ret;
+ 			msk->snd_burst -= ret;
+ 			copied += ret;
+ 			len -= ret;
+ 		}
+ 		WRITE_ONCE(msk->first_pending, mptcp_send_next(sk));
+ 	}
+ 
+ 	/* at this point we held the socket lock for the last subflow we used */
+ 	if (ssk)
+ 		mptcp_push_release(sk, ssk, &info);
+ 
+ out:
+ 	if (copied) {
+ 		/* start the timer, if it's not pending */
+ 		if (!mptcp_timer_pending(sk))
+ 			mptcp_reset_timer(sk);
+ 		__mptcp_check_send_data_fin(sk);
+ 	}
++>>>>>>> b680a214ec28 (mptcp: update rtx timeout only if required.)
  }
  
  static int mptcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
* Unmerged path net/mptcp/protocol.c
