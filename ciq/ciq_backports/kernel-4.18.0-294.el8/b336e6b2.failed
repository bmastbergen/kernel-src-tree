net/mlx5e: kTLS, Enforce HW TX csum offload with kTLS

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Tariq Toukan <tariqt@nvidia.com>
commit b336e6b25e2d053c482ee4339787e6428f390864
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/b336e6b2.failed

Checksum calculation cannot be done in SW for TX kTLS HW offloaded
packets.
Offload it to the device, disregard the declared state of the TX
csum offload feature.

Fixes: d2ead1f360e8 ("net/mlx5e: Add kTLS TX HW offload support")
	Signed-off-by: Tariq Toukan <tariqt@nvidia.com>
	Reviewed-by: Maxim Mikityanskiy <maximmi@mellanox.com>
	Reviewed-by: Boris Pismenny <borisp@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit b336e6b25e2d053c482ee4339787e6428f390864)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index d2c9bb371f27,d97203cf6a00..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -145,8 -144,26 +145,10 @@@ static inline void mlx5e_insert_vlan(vo
  	memcpy(&vhdr->h_vlan_encapsulated_proto, skb->data + cpy1_sz, cpy2_sz);
  }
  
 -/* If packet is not IP's CHECKSUM_PARTIAL (e.g. icmd packet),
 - * need to set L3 checksum flag for IPsec
 - */
 -static void
 -ipsec_txwqe_build_eseg_csum(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 -			    struct mlx5_wqe_eth_seg *eseg)
 -{
 -	eseg->cs_flags = MLX5_ETH_WQE_L3_CSUM;
 -	if (skb->encapsulation) {
 -		eseg->cs_flags |= MLX5_ETH_WQE_L3_INNER_CSUM;
 -		sq->stats->csum_partial_inner++;
 -	} else {
 -		sq->stats->csum_partial++;
 -	}
 -}
 -
  static inline void
- mlx5e_txwqe_build_eseg_csum(struct mlx5e_txqsq *sq, struct sk_buff *skb, struct mlx5_wqe_eth_seg *eseg)
+ mlx5e_txwqe_build_eseg_csum(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 			    struct mlx5e_accel_tx_state *accel,
+ 			    struct mlx5_wqe_eth_seg *eseg)
  {
  	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
  		eseg->cs_flags = MLX5_ETH_WQE_L3_CSUM;
@@@ -158,6 -175,14 +160,17 @@@
  			eseg->cs_flags |= MLX5_ETH_WQE_L4_CSUM;
  			sq->stats->csum_partial++;
  		}
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_MLX5_EN_TLS
+ 	} else if (unlikely(accel && accel->tls.tls_tisn)) {
+ 		eseg->cs_flags = MLX5_ETH_WQE_L3_CSUM | MLX5_ETH_WQE_L4_CSUM;
+ 		sq->stats->csum_partial++;
+ #endif
+ 	} else if (unlikely(eseg->flow_table_metadata & cpu_to_be32(MLX5_ETH_WQE_FT_META_IPSEC))) {
+ 		ipsec_txwqe_build_eseg_csum(sq, skb, eseg);
+ 
++>>>>>>> b336e6b25e2d (net/mlx5e: kTLS, Enforce HW TX csum offload with kTLS)
  	} else
  		sq->stats->csum_none++;
  }
@@@ -405,6 -463,168 +418,171 @@@ err_drop
  	dev_kfree_skb_any(skb);
  }
  
++<<<<<<< HEAD
++=======
+ static bool mlx5e_tx_skb_supports_mpwqe(struct sk_buff *skb, struct mlx5e_tx_attr *attr)
+ {
+ 	return !skb_is_nonlinear(skb) && !skb_vlan_tag_present(skb) && !attr->ihs &&
+ 	       !attr->insz;
+ }
+ 
+ static bool mlx5e_tx_mpwqe_same_eseg(struct mlx5e_txqsq *sq, struct mlx5_wqe_eth_seg *eseg)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 
+ 	/* Assumes the session is already running and has at least one packet. */
+ 	return !memcmp(&session->wqe->eth, eseg, MLX5E_ACCEL_ESEG_LEN);
+ }
+ 
+ static void mlx5e_tx_mpwqe_session_start(struct mlx5e_txqsq *sq,
+ 					 struct mlx5_wqe_eth_seg *eseg)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u16 pi;
+ 
+ 	pi = mlx5e_txqsq_get_next_pi(sq, MLX5E_TX_MPW_MAX_WQEBBS);
+ 	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
+ 	prefetchw(wqe->data);
+ 
+ 	*session = (struct mlx5e_tx_mpwqe) {
+ 		.wqe = wqe,
+ 		.bytes_count = 0,
+ 		.ds_count = MLX5E_TX_WQE_EMPTY_DS_COUNT,
+ 		.pkt_count = 0,
+ 		.inline_on = 0,
+ 	};
+ 
+ 	memcpy(&session->wqe->eth, eseg, MLX5E_ACCEL_ESEG_LEN);
+ 
+ 	sq->stats->mpwqe_blks++;
+ }
+ 
+ static bool mlx5e_tx_mpwqe_session_is_active(struct mlx5e_txqsq *sq)
+ {
+ 	return sq->mpwqe.wqe;
+ }
+ 
+ static void mlx5e_tx_mpwqe_add_dseg(struct mlx5e_txqsq *sq, struct mlx5e_xmit_data *txd)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 
+ 	dseg = (struct mlx5_wqe_data_seg *)session->wqe + session->ds_count;
+ 
+ 	session->pkt_count++;
+ 	session->bytes_count += txd->len;
+ 
+ 	dseg->addr = cpu_to_be64(txd->dma_addr);
+ 	dseg->byte_count = cpu_to_be32(txd->len);
+ 	dseg->lkey = sq->mkey_be;
+ 	session->ds_count++;
+ 
+ 	sq->stats->mpwqe_pkts++;
+ }
+ 
+ static struct mlx5_wqe_ctrl_seg *mlx5e_tx_mpwqe_session_complete(struct mlx5e_txqsq *sq)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 	u8 ds_count = session->ds_count;
+ 	struct mlx5_wqe_ctrl_seg *cseg;
+ 	struct mlx5e_tx_wqe_info *wi;
+ 	u16 pi;
+ 
+ 	cseg = &session->wqe->ctrl;
+ 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_ENHANCED_MPSW);
+ 	cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_count);
+ 
+ 	pi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);
+ 	wi = &sq->db.wqe_info[pi];
+ 	*wi = (struct mlx5e_tx_wqe_info) {
+ 		.skb = NULL,
+ 		.num_bytes = session->bytes_count,
+ 		.num_wqebbs = DIV_ROUND_UP(ds_count, MLX5_SEND_WQEBB_NUM_DS),
+ 		.num_dma = session->pkt_count,
+ 		.num_fifo_pkts = session->pkt_count,
+ 	};
+ 
+ 	sq->pc += wi->num_wqebbs;
+ 
+ 	session->wqe = NULL;
+ 
+ 	mlx5e_tx_check_stop(sq);
+ 
+ 	return cseg;
+ }
+ 
+ static void
+ mlx5e_sq_xmit_mpwqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 		    struct mlx5_wqe_eth_seg *eseg, bool xmit_more)
+ {
+ 	struct mlx5_wqe_ctrl_seg *cseg;
+ 	struct mlx5e_xmit_data txd;
+ 
+ 	if (!mlx5e_tx_mpwqe_session_is_active(sq)) {
+ 		mlx5e_tx_mpwqe_session_start(sq, eseg);
+ 	} else if (!mlx5e_tx_mpwqe_same_eseg(sq, eseg)) {
+ 		mlx5e_tx_mpwqe_session_complete(sq);
+ 		mlx5e_tx_mpwqe_session_start(sq, eseg);
+ 	}
+ 
+ 	sq->stats->xmit_more += xmit_more;
+ 
+ 	txd.data = skb->data;
+ 	txd.len = skb->len;
+ 
+ 	txd.dma_addr = dma_map_single(sq->pdev, txd.data, txd.len, DMA_TO_DEVICE);
+ 	if (unlikely(dma_mapping_error(sq->pdev, txd.dma_addr)))
+ 		goto err_unmap;
+ 	mlx5e_dma_push(sq, txd.dma_addr, txd.len, MLX5E_DMA_MAP_SINGLE);
+ 
+ 	mlx5e_skb_fifo_push(sq, skb);
+ 
+ 	mlx5e_tx_mpwqe_add_dseg(sq, &txd);
+ 
+ 	mlx5e_tx_skb_update_hwts_flags(skb);
+ 
+ 	if (unlikely(mlx5e_tx_mpwqe_is_full(&sq->mpwqe))) {
+ 		/* Might stop the queue and affect the retval of __netdev_tx_sent_queue. */
+ 		cseg = mlx5e_tx_mpwqe_session_complete(sq);
+ 
+ 		if (__netdev_tx_sent_queue(sq->txq, txd.len, xmit_more))
+ 			mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ 	} else if (__netdev_tx_sent_queue(sq->txq, txd.len, xmit_more)) {
+ 		/* Might stop the queue, but we were asked to ring the doorbell anyway. */
+ 		cseg = mlx5e_tx_mpwqe_session_complete(sq);
+ 
+ 		mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ 	}
+ 
+ 	return;
+ 
+ err_unmap:
+ 	mlx5e_dma_unmap_wqe_err(sq, 1);
+ 	sq->stats->dropped++;
+ 	dev_kfree_skb_any(skb);
+ }
+ 
+ void mlx5e_tx_mpwqe_ensure_complete(struct mlx5e_txqsq *sq)
+ {
+ 	/* Unlikely in non-MPWQE workloads; not important in MPWQE workloads. */
+ 	if (unlikely(mlx5e_tx_mpwqe_session_is_active(sq)))
+ 		mlx5e_tx_mpwqe_session_complete(sq);
+ }
+ 
+ static bool mlx5e_txwqe_build_eseg(struct mlx5e_priv *priv, struct mlx5e_txqsq *sq,
+ 				   struct sk_buff *skb, struct mlx5e_accel_tx_state *accel,
+ 				   struct mlx5_wqe_eth_seg *eseg)
+ {
+ 	if (unlikely(!mlx5e_accel_tx_eseg(priv, skb, eseg)))
+ 		return false;
+ 
+ 	mlx5e_txwqe_build_eseg_csum(sq, skb, accel, eseg);
+ 
+ 	return true;
+ }
+ 
++>>>>>>> b336e6b25e2d (net/mlx5e: kTLS, Enforce HW TX csum offload with kTLS)
  netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)
  {
  	struct mlx5e_priv *priv = netdev_priv(dev);
@@@ -417,21 -639,92 +595,105 @@@
  
  	/* May send SKBs and WQEs. */
  	if (unlikely(!mlx5e_accel_tx_begin(dev, sq, skb, &accel)))
 -		return NETDEV_TX_OK;
 +		goto out;
  
++<<<<<<< HEAD
 +	pi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);
 +	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
 +
 +	/* May update the WQE, but may not post other WQEs. */
 +	if (unlikely(!mlx5e_accel_tx_finish(priv, sq, skb, wqe, &accel)))
 +		goto out;
++=======
+ 	mlx5e_sq_xmit_prepare(sq, skb, &accel, &attr);
+ 
+ 	if (test_bit(MLX5E_SQ_STATE_MPWQE, &sq->state)) {
+ 		if (mlx5e_tx_skb_supports_mpwqe(skb, &attr)) {
+ 			struct mlx5_wqe_eth_seg eseg = {};
+ 
+ 			if (unlikely(!mlx5e_txwqe_build_eseg(priv, sq, skb, &accel, &eseg)))
+ 				return NETDEV_TX_OK;
+ 
+ 			mlx5e_sq_xmit_mpwqe(sq, skb, &eseg, netdev_xmit_more());
+ 			return NETDEV_TX_OK;
+ 		}
+ 
+ 		mlx5e_tx_mpwqe_ensure_complete(sq);
+ 	}
+ 
+ 	mlx5e_sq_calc_wqe_attr(skb, &attr, &wqe_attr);
+ 	pi = mlx5e_txqsq_get_next_pi(sq, wqe_attr.num_wqebbs);
+ 	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
+ 
+ 	/* May update the WQE, but may not post other WQEs. */
+ 	mlx5e_accel_tx_finish(sq, wqe, &accel,
+ 			      (struct mlx5_wqe_inline_seg *)(wqe->data + wqe_attr.ds_cnt_inl));
+ 	if (unlikely(!mlx5e_txwqe_build_eseg(priv, sq, skb, &accel, &wqe->eth)))
+ 		return NETDEV_TX_OK;
++>>>>>>> b336e6b25e2d (net/mlx5e: kTLS, Enforce HW TX csum offload with kTLS)
  
 -	mlx5e_sq_xmit_wqe(sq, skb, &attr, &wqe_attr, wqe, pi, netdev_xmit_more());
 +	mlx5e_sq_xmit(sq, skb, wqe, pi, netdev_xmit_more());
  
 +out:
  	return NETDEV_TX_OK;
  }
  
++<<<<<<< HEAD
++=======
+ void mlx5e_sq_xmit_simple(struct mlx5e_txqsq *sq, struct sk_buff *skb, bool xmit_more)
+ {
+ 	struct mlx5e_tx_wqe_attr wqe_attr;
+ 	struct mlx5e_tx_attr attr;
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u16 pi;
+ 
+ 	mlx5e_sq_xmit_prepare(sq, skb, NULL, &attr);
+ 	mlx5e_sq_calc_wqe_attr(skb, &attr, &wqe_attr);
+ 	pi = mlx5e_txqsq_get_next_pi(sq, wqe_attr.num_wqebbs);
+ 	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
+ 	mlx5e_txwqe_build_eseg_csum(sq, skb, NULL, &wqe->eth);
+ 	mlx5e_sq_xmit_wqe(sq, skb, &attr, &wqe_attr, wqe, pi, xmit_more);
+ }
+ 
+ static void mlx5e_tx_wi_dma_unmap(struct mlx5e_txqsq *sq, struct mlx5e_tx_wqe_info *wi,
+ 				  u32 *dma_fifo_cc)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < wi->num_dma; i++) {
+ 		struct mlx5e_sq_dma *dma = mlx5e_dma_get(sq, (*dma_fifo_cc)++);
+ 
+ 		mlx5e_tx_dma_unmap(sq->pdev, dma);
+ 	}
+ }
+ 
+ static void mlx5e_consume_skb(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 			      struct mlx5_cqe64 *cqe, int napi_budget)
+ {
+ 	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {
+ 		struct skb_shared_hwtstamps hwts = {};
+ 		u64 ts = get_cqe_ts(cqe);
+ 
+ 		hwts.hwtstamp = mlx5_timecounter_cyc2time(sq->clock, ts);
+ 		skb_tstamp_tx(skb, &hwts);
+ 	}
+ 
+ 	napi_consume_skb(skb, napi_budget);
+ }
+ 
+ static void mlx5e_tx_wi_consume_fifo_skbs(struct mlx5e_txqsq *sq, struct mlx5e_tx_wqe_info *wi,
+ 					  struct mlx5_cqe64 *cqe, int napi_budget)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < wi->num_fifo_pkts; i++) {
+ 		struct sk_buff *skb = mlx5e_skb_fifo_pop(sq);
+ 
+ 		mlx5e_consume_skb(sq, skb, cqe, napi_budget);
+ 	}
+ }
+ 
++>>>>>>> b336e6b25e2d (net/mlx5e: kTLS, Enforce HW TX csum offload with kTLS)
  bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget)
  {
  	struct mlx5e_sq_stats *stats;
@@@ -664,17 -952,18 +926,17 @@@ void mlx5i_sq_xmit(struct mlx5e_txqsq *
  
  	mlx5i_txwqe_build_datagram(av, dqpn, dqkey, datagram);
  
- 	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
+ 	mlx5e_txwqe_build_eseg_csum(sq, skb, NULL, eseg);
  
 -	eseg->mss = attr.mss;
 +	eseg->mss = mss;
  
 -	if (attr.ihs) {
 -		memcpy(eseg->inline_hdr.start, skb->data, attr.ihs);
 -		eseg->inline_hdr.sz = cpu_to_be16(attr.ihs);
 -		dseg += wqe_attr.ds_cnt_inl;
 +	if (ihs) {
 +		memcpy(eseg->inline_hdr.start, skb->data, ihs);
 +		eseg->inline_hdr.sz = cpu_to_be16(ihs);
 +		dseg += ds_cnt_inl;
  	}
  
 -	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr.ihs,
 -					  attr.headlen, dseg);
 +	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + ihs, headlen, dseg);
  	if (unlikely(num_dma < 0))
  		goto err_drop;
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
