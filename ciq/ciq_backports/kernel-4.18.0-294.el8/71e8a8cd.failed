iommu/arm-smmu: Split arm_smmu_tlb_inv_range_nosync()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit 71e8a8cdaff995a46e3e186a736636747cbd2f50
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/71e8a8cd.failed

Since we now use separate iommu_gather_ops for stage 1 and stage 2
contexts, we may as well divide up the monolithic callback into its
respective stage 1 and stage 2 parts.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 71e8a8cdaff995a46e3e186a736636747cbd2f50)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/arm-smmu.c
diff --cc drivers/iommu/arm-smmu.c
index 6374a0f18d9a,5b12e96d7878..000000000000
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@@ -562,67 -556,22 +570,80 @@@ static void arm_smmu_tlb_inv_vmid_nosyn
  	writel_relaxed(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
  }
  
++<<<<<<< HEAD
 +static void arm_smmu_tlb_inv_walk(unsigned long iova, size_t size,
 +				  size_t granule, void *cookie)
 +{
 +	struct arm_smmu_domain *smmu_domain = cookie;
 +	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
 +
 +	ops->tlb_inv_range(iova, size, granule, false, cookie);
 +	ops->tlb_sync(cookie);
 +}
 +
 +static void arm_smmu_tlb_inv_leaf(unsigned long iova, size_t size,
 +				  size_t granule, void *cookie)
 +{
 +	struct arm_smmu_domain *smmu_domain = cookie;
 +	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
 +
 +	ops->tlb_inv_range(iova, size, granule, true, cookie);
 +	ops->tlb_sync(cookie);
 +}
 +
 +static void arm_smmu_tlb_add_page(struct iommu_iotlb_gather *gather,
 +				  unsigned long iova, size_t granule,
 +				  void *cookie)
 +{
 +	struct arm_smmu_domain *smmu_domain = cookie;
 +	const struct arm_smmu_flush_ops *ops = smmu_domain->flush_ops;
 +
 +	ops->tlb_inv_range(iova, granule, granule, true, cookie);
 +}
 +
 +static const struct arm_smmu_flush_ops arm_smmu_s1_tlb_ops = {
 +	.tlb = {
 +		.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
 +		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 +		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 +		.tlb_add_page	= arm_smmu_tlb_add_page,
 +	},
 +	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync		= arm_smmu_tlb_sync_context,
 +};
 +
 +static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v2 = {
 +	.tlb = {
 +		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 +		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 +		.tlb_add_page	= arm_smmu_tlb_add_page,
 +	},
 +	.tlb_inv_range		= arm_smmu_tlb_inv_range_nosync,
 +	.tlb_sync		= arm_smmu_tlb_sync_context,
++=======
+ static const struct iommu_gather_ops arm_smmu_s1_tlb_ops = {
+ 	.tlb_flush_all	= arm_smmu_tlb_inv_context_s1,
+ 	.tlb_add_flush	= arm_smmu_tlb_inv_range_s1,
+ 	.tlb_sync	= arm_smmu_tlb_sync_context,
+ };
+ 
+ static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v2 = {
+ 	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
+ 	.tlb_add_flush	= arm_smmu_tlb_inv_range_s2,
+ 	.tlb_sync	= arm_smmu_tlb_sync_context,
++>>>>>>> 71e8a8cdaff9 (iommu/arm-smmu: Split arm_smmu_tlb_inv_range_nosync())
  };
  
 -static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v1 = {
 -	.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 -	.tlb_add_flush	= arm_smmu_tlb_inv_vmid_nosync,
 -	.tlb_sync	= arm_smmu_tlb_sync_vmid,
 +static const struct arm_smmu_flush_ops arm_smmu_s2_tlb_ops_v1 = {
 +	.tlb = {
 +		.tlb_flush_all	= arm_smmu_tlb_inv_context_s2,
 +		.tlb_flush_walk	= arm_smmu_tlb_inv_walk,
 +		.tlb_flush_leaf	= arm_smmu_tlb_inv_leaf,
 +		.tlb_add_page	= arm_smmu_tlb_add_page,
 +	},
 +	.tlb_inv_range		= arm_smmu_tlb_inv_vmid_nosync,
 +	.tlb_sync		= arm_smmu_tlb_sync_vmid,
  };
  
  static irqreturn_t arm_smmu_context_fault(int irq, void *dev)
* Unmerged path drivers/iommu/arm-smmu.c
