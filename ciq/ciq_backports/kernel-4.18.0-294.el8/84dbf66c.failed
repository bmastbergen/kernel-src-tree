powerpc/smp: Use GFP_ATOMIC while allocating tmp mask

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Srikar Dronamraju <srikar@linux.vnet.ibm.com>
commit 84dbf66c63472069e5eb40b810731367618cd8b5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/84dbf66c.failed

Qian Cai reported a regression where CPU Hotplug fails with the latest
powerpc/next

BUG: sleeping function called from invalid context at mm/slab.h:494
in_atomic(): 1, irqs_disabled(): 1, non_block: 0, pid: 0, name: swapper/88
no locks held by swapper/88/0.
irq event stamp: 18074448
hardirqs last  enabled at (18074447): [<c0000000001a2a7c>] tick_nohz_idle_enter+0x9c/0x110
hardirqs last disabled at (18074448): [<c000000000106798>] do_idle+0x138/0x3b0
do_idle at kernel/sched/idle.c:253 (discriminator 1)
softirqs last  enabled at (18074440): [<c0000000000bbec4>] irq_enter_rcu+0x94/0xa0
softirqs last disabled at (18074439): [<c0000000000bbea0>] irq_enter_rcu+0x70/0xa0
CPU: 88 PID: 0 Comm: swapper/88 Tainted: G        W         5.9.0-rc8-next-20201007 #1
Call Trace:
[c00020000a4bfcf0] [c000000000649e98] dump_stack+0xec/0x144 (unreliable)
[c00020000a4bfd30] [c0000000000f6c34] ___might_sleep+0x2f4/0x310
[c00020000a4bfdb0] [c000000000354f94] slab_pre_alloc_hook.constprop.82+0x124/0x190
[c00020000a4bfe00] [c00000000035e9e8] __kmalloc_node+0x88/0x3a0
slab_alloc_node at mm/slub.c:2817
(inlined by) __kmalloc_node at mm/slub.c:4013
[c00020000a4bfe80] [c0000000006494d8] alloc_cpumask_var_node+0x38/0x80
kmalloc_node at include/linux/slab.h:577
(inlined by) alloc_cpumask_var_node at lib/cpumask.c:116
[c00020000a4bfef0] [c00000000003eedc] start_secondary+0x27c/0x800
update_mask_by_l2 at arch/powerpc/kernel/smp.c:1267
(inlined by) add_cpu_to_masks at arch/powerpc/kernel/smp.c:1387
(inlined by) start_secondary at arch/powerpc/kernel/smp.c:1420
[c00020000a4bff90] [c00000000000c468] start_secondary_resume+0x10/0x14

Allocating a temporary mask while performing a CPU Hotplug operation
with CONFIG_CPUMASK_OFFSTACK enabled, leads to calling a sleepable
function from a atomic context. Fix this by allocating the temporary
mask with GFP_ATOMIC flag. Also instead of having to allocate twice,
allocate the mask in the caller so that we only have to allocate once.
If the allocation fails, assume the mask to be same as sibling mask, which
will make the scheduler to drop this domain for this CPU.

Fixes: 70a94089d7f7 ("powerpc/smp: Optimize update_coregroup_mask")
Fixes: 3ab33d6dc3e9 ("powerpc/smp: Optimize update_mask_by_l2")
	Reported-by: Qian Cai <cai@redhat.com>
	Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20201019042716.106234-3-srikar@linux.vnet.ibm.com
(cherry picked from commit 84dbf66c63472069e5eb40b810731367618cd8b5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kernel/smp.c
diff --cc arch/powerpc/kernel/smp.c
index f47c03e3bb80,3c6b9822f978..000000000000
--- a/arch/powerpc/kernel/smp.c
+++ b/arch/powerpc/kernel/smp.c
@@@ -1289,10 -1326,46 +1283,53 @@@ static inline void add_cpu_to_smallcore
  	}
  }
  
++<<<<<<< HEAD
 +static void add_cpu_to_masks(int cpu)
 +{
 +	int first_thread = cpu_first_thread_sibling(cpu);
 +	int pkg_id = get_physical_package_id(cpu);
++=======
+ static void update_coregroup_mask(int cpu, cpumask_var_t *mask)
+ {
+ 	struct cpumask *(*submask_fn)(int) = cpu_sibling_mask;
+ 	int coregroup_id = cpu_to_coregroup_id(cpu);
+ 	int i;
+ 
+ 	if (shared_caches)
+ 		submask_fn = cpu_l2_cache_mask;
+ 
+ 	if (!*mask) {
+ 		/* Assume only siblings are part of this CPU's coregroup */
+ 		for_each_cpu(i, submask_fn(cpu))
+ 			set_cpus_related(cpu, i, cpu_coregroup_mask);
+ 
+ 		return;
+ 	}
+ 
+ 	cpumask_and(*mask, cpu_online_mask, cpu_cpu_mask(cpu));
+ 
+ 	/* Update coregroup mask with all the CPUs that are part of submask */
+ 	or_cpumasks_related(cpu, cpu, submask_fn, cpu_coregroup_mask);
+ 
+ 	/* Skip all CPUs already part of coregroup mask */
+ 	cpumask_andnot(*mask, *mask, cpu_coregroup_mask(cpu));
+ 
+ 	for_each_cpu(i, *mask) {
+ 		/* Skip all CPUs not part of this coregroup */
+ 		if (coregroup_id == cpu_to_coregroup_id(i)) {
+ 			or_cpumasks_related(cpu, i, submask_fn, cpu_coregroup_mask);
+ 			cpumask_andnot(*mask, *mask, submask_fn(i));
+ 		} else {
+ 			cpumask_andnot(*mask, *mask, cpu_coregroup_mask(i));
+ 		}
+ 	}
+ }
+ 
+ static void add_cpu_to_masks(int cpu)
+ {
+ 	int first_thread = cpu_first_thread_sibling(cpu);
+ 	cpumask_var_t mask;
++>>>>>>> 84dbf66c6347 (powerpc/smp: Use GFP_ATOMIC while allocating tmp mask)
  	int i;
  
  	/*
@@@ -1307,27 -1379,15 +1344,37 @@@
  			set_cpus_related(i, cpu, cpu_sibling_mask);
  
  	add_cpu_to_smallcore_masks(cpu);
- 	update_mask_by_l2(cpu);
  
+ 	/* In CPU-hotplug path, hence use GFP_ATOMIC */
+ 	alloc_cpumask_var_node(&mask, GFP_ATOMIC, cpu_to_node(cpu));
+ 	update_mask_by_l2(cpu, &mask);
+ 
++<<<<<<< HEAD
 +	if (pkg_id == -1) {
 +		struct cpumask *(*mask)(int) = cpu_sibling_mask;
 +
 +		/*
 +		 * Copy the sibling mask into core sibling mask and
 +		 * mark any CPUs on the same chip as this CPU.
 +		 */
 +		if (shared_caches)
 +			mask = cpu_l2_cache_mask;
 +
 +		for_each_cpu(i, mask(cpu))
 +			set_cpus_related(cpu, i, cpu_core_mask);
 +
 +		return;
 +	}
 +
 +	for_each_cpu(i, cpu_online_mask)
 +		if (get_physical_package_id(i) == pkg_id)
 +			set_cpus_related(cpu, i, cpu_core_mask);
++=======
+ 	if (has_coregroup_support())
+ 		update_coregroup_mask(cpu, &mask);
+ 
+ 	free_cpumask_var(mask);
++>>>>>>> 84dbf66c6347 (powerpc/smp: Use GFP_ATOMIC while allocating tmp mask)
  }
  
  /* Activate a secondary processor. */
* Unmerged path arch/powerpc/kernel/smp.c
