mm, slub: introduce kmem_cache_debug_flags()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 59052e89fc89e3e6bef0151052e093566e446851
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/59052e89.failed

There are few places that call kmem_cache_debug(s) (which tests if any of
debug flags are enabled for a cache) immediately followed by a test for a
specific flag.  The compiler can probably eliminate the extra check, but
we can make the code nicer by introducing kmem_cache_debug_flags() that
works like kmem_cache_debug() (including the static key check) but tests
for specific flag(s).  The next patches will add more users.

[vbabka@suse.cz: change return from int to bool, per Kees.  Add VM_WARN_ON_ONCE() for invalid flags, per Roman]
  Link: http://lkml.kernel.org/r/949b90ed-e0f0-07d7-4d21-e30ec0958a7c@suse.cz

	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Roman Gushchin <guro@fb.com>
	Acked-by: Christoph Lameter <cl@linux.com>
	Acked-by: Kees Cook <keescook@chromium.org>
	Cc: Jann Horn <jannh@google.com>
	Cc: Vijayanand Jitta <vjitta@codeaurora.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Pekka Enberg <penberg@kernel.org>
Link: http://lkml.kernel.org/r/20200610163135.17364-8-vbabka@suse.cz
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 59052e89fc89e3e6bef0151052e093566e446851)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 4642d7769e3e,97074631a2d1..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -116,13 -114,32 +116,43 @@@
   * 			the fast path and disables lockless freelists.
   */
  
++<<<<<<< HEAD
 +static inline int kmem_cache_debug(struct kmem_cache *s)
++=======
+ #ifdef CONFIG_SLUB_DEBUG
+ #ifdef CONFIG_SLUB_DEBUG_ON
+ DEFINE_STATIC_KEY_TRUE(slub_debug_enabled);
+ #else
+ DEFINE_STATIC_KEY_FALSE(slub_debug_enabled);
+ #endif
+ #endif
+ 
+ /*
+  * Returns true if any of the specified slub_debug flags is enabled for the
+  * cache. Use only for flags parsed by setup_slub_debug() as it also enables
+  * the static key.
+  */
+ static inline bool kmem_cache_debug_flags(struct kmem_cache *s, slab_flags_t flags)
++>>>>>>> 59052e89fc89 (mm, slub: introduce kmem_cache_debug_flags())
  {
+ 	VM_WARN_ON_ONCE(!(flags & SLAB_DEBUG_FLAGS));
  #ifdef CONFIG_SLUB_DEBUG
++<<<<<<< HEAD
 +	return unlikely(s->flags & SLAB_DEBUG_FLAGS);
 +#else
 +	return 0;
 +#endif
++=======
+ 	if (static_branch_unlikely(&slub_debug_enabled))
+ 		return s->flags & flags;
+ #endif
+ 	return false;
+ }
+ 
+ static inline bool kmem_cache_debug(struct kmem_cache *s)
+ {
+ 	return kmem_cache_debug_flags(s, SLAB_DEBUG_FLAGS);
++>>>>>>> 59052e89fc89 (mm, slub: introduce kmem_cache_debug_flags())
  }
  
  void *fixup_red_left(struct kmem_cache *s, void *p)
* Unmerged path mm/slub.c
