mm: fold and remove lru_cache_add_anon() and lru_cache_add_file()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 6058eaec816f29fbe33c9d35694614c9a4ed75ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/6058eaec.failed

They're the same function, and for the purpose of all callers they are
equivalent to lru_cache_add().

[akpm@linux-foundation.org: fix it for local_lock changes]
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Rik van Riel <riel@surriel.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Link: http://lkml.kernel.org/r/20200520232525.798933-5-hannes@cmpxchg.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 6058eaec816f29fbe33c9d35694614c9a4ed75ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
#	mm/memory.c
#	mm/shmem.c
#	mm/swap.c
#	mm/swap_state.c
diff --cc mm/khugepaged.c
index 0af263adde1a,3f032487825b..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1549,29 -1877,16 +1549,37 @@@ tree_unlocked
  			index++;
  		}
  
++<<<<<<< HEAD
 +		local_irq_save(flags);
 +		__inc_node_page_state(new_page, NR_SHMEM_THPS);
 +		if (nr_none) {
 +			__mod_node_page_state(zone->zone_pgdat, NR_FILE_PAGES, nr_none);
 +			__mod_node_page_state(zone->zone_pgdat, NR_SHMEM, nr_none);
 +		}
 +		local_irq_restore(flags);
++=======
+ 		SetPageUptodate(new_page);
+ 		page_ref_add(new_page, HPAGE_PMD_NR - 1);
+ 		if (is_shmem)
+ 			set_page_dirty(new_page);
+ 		lru_cache_add(new_page);
++>>>>>>> 6058eaec816f (mm: fold and remove lru_cache_add_anon() and lru_cache_add_file())
  
  		/*
 -		 * Remove pte page tables, so we can re-fault the page as huge.
 +		 * Remove pte page tables, so we can re-faulti
 +		 * the page as huge.
  		 */
  		retract_page_tables(mapping, start);
 +
 +		/* Everything is ready, let's unfreeze the new_page */
 +		set_page_dirty(new_page);
 +		SetPageUptodate(new_page);
 +		page_ref_unfreeze(new_page, HPAGE_PMD_NR);
 +		mem_cgroup_commit_charge(new_page, memcg, false, true);
 +		count_memcg_events(memcg, THP_COLLAPSE_ALLOC, 1);
 +		lru_cache_add_anon(new_page);
 +		unlock_page(new_page);
 +
  		*hpage = NULL;
  
  		khugepaged_pages_collapsed++;
diff --cc mm/memory.c
index 583eb7e0dd7f,3431e76d0e75..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3045,7 -3128,18 +3045,20 @@@ vm_fault_t do_swap_page(struct vm_faul
  				__SetPageLocked(page);
  				__SetPageSwapBacked(page);
  				set_page_private(page, entry.val);
++<<<<<<< HEAD
 +				lru_cache_add_anon(page);
++=======
+ 
+ 				/* Tell memcg to use swap ownership records */
+ 				SetPageSwapCache(page);
+ 				err = mem_cgroup_charge(page, vma->vm_mm,
+ 							GFP_KERNEL);
+ 				ClearPageSwapCache(page);
+ 				if (err)
+ 					goto out_page;
+ 
+ 				lru_cache_add(page);
++>>>>>>> 6058eaec816f (mm: fold and remove lru_cache_add_anon() and lru_cache_add_file())
  				swap_readpage(page, true);
  			}
  		} else {
diff --cc mm/shmem.c
index 10f03436a6bf,ea95a3e46fbb..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -1586,8 -1609,7 +1586,12 @@@ static int shmem_replace_page(struct pa
  		 */
  		oldpage = newpage;
  	} else {
++<<<<<<< HEAD
 +		mem_cgroup_migrate(oldpage, newpage);
 +		lru_cache_add_anon(newpage);
++=======
+ 		lru_cache_add(newpage);
++>>>>>>> 6058eaec816f (mm: fold and remove lru_cache_add_anon() and lru_cache_add_file())
  		*pagep = newpage;
  	}
  
@@@ -1851,28 -1855,15 +1855,32 @@@ alloc_nohuge
  	if (sgp == SGP_WRITE)
  		__SetPageReferenced(page);
  
 +	error = mem_cgroup_try_charge_delay(page, charge_mm, gfp, &memcg,
 +					    PageTransHuge(page));
 +	if (error) {
 +		if (PageTransHuge(page)) {
 +			count_vm_event(THP_FILE_FALLBACK);
 +			count_vm_event(THP_FILE_FALLBACK_CHARGE);
 +		}
 +		goto unacct;
++<<<<<<< HEAD
 +	}
  	error = shmem_add_to_page_cache(page, mapping, hindex,
 -					NULL, gfp & GFP_RECLAIM_MASK,
 -					charge_mm);
 -	if (error)
 +					NULL, gfp & GFP_RECLAIM_MASK);
 +	if (error) {
 +		mem_cgroup_cancel_charge(page, memcg,
 +					 PageTransHuge(page));
  		goto unacct;
 +	}
 +	mem_cgroup_commit_charge(page, memcg, false,
 +				 PageTransHuge(page));
 +	lru_cache_add_anon(page);
++=======
+ 	lru_cache_add(page);
++>>>>>>> 6058eaec816f (mm: fold and remove lru_cache_add_anon() and lru_cache_add_file())
  
  	spin_lock_irq(&info->lock);
 -	info->alloced += compound_nr(page);
 +	info->alloced += 1 << compound_order(page);
  	inode->i_blocks += BLOCKS_PER_PAGE << compound_order(page);
  	shmem_recalc_inode(inode);
  	spin_unlock_irq(&info->lock);
@@@ -2363,9 -2374,9 +2371,9 @@@ static int shmem_mfill_atomic_pte(struc
  
  	ret = -EEXIST;
  	if (!pte_none(*dst_pte))
 -		goto out_release_unlock;
 +		goto out_release_uncharge_unlock;
  
- 	lru_cache_add_anon(page);
+ 	lru_cache_add(page);
  
  	spin_lock_irq(&info->lock);
  	info->alloced++;
diff --cc mm/swap.c
index 70728521e27e,6196d792c952..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -406,35 -424,6 +406,38 @@@ void mark_page_accessed(struct page *pa
  }
  EXPORT_SYMBOL(mark_page_accessed);
  
++<<<<<<< HEAD
 +static void __lru_cache_add(struct page *page)
 +{
 +	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
 +
 +	get_page(page);
 +	if (!pagevec_add(pvec, page) || PageCompound(page))
 +		__pagevec_lru_add(pvec);
 +	put_cpu_var(lru_add_pvec);
 +}
 +
 +/**
 + * lru_cache_add_anon - add a page to the page lists
 + * @page: the page to add
 + */
 +void lru_cache_add_anon(struct page *page)
 +{
 +	if (PageActive(page))
 +		ClearPageActive(page);
 +	__lru_cache_add(page);
 +}
 +
 +void lru_cache_add_file(struct page *page)
 +{
 +	if (PageActive(page))
 +		ClearPageActive(page);
 +	__lru_cache_add(page);
 +}
 +EXPORT_SYMBOL(lru_cache_add_file);
 +
++=======
++>>>>>>> 6058eaec816f (mm: fold and remove lru_cache_add_anon() and lru_cache_add_file())
  /**
   * lru_cache_add - add a page to a page list
   * @page: the page to be added to the LRU.
diff --cc mm/swap_state.c
index e2aded84261e,fa089002125f..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -430,47 -405,51 +430,78 @@@ struct page *__read_swap_cache_async(sw
  		 * Swap entry may have been freed since our caller observed it.
  		 */
  		err = swapcache_prepare(entry);
 -		if (!err)
 +		if (err == -EEXIST) {
 +			radix_tree_preload_end();
 +			/*
 +			 * We might race against get_swap_page() and stumble
 +			 * across a SWAP_HAS_CACHE swap_map entry whose page
 +			 * has not been brought into the swapcache yet.
 +			 */
 +			cond_resched();
 +			continue;
 +		}
 +		if (err) {		/* swp entry is obsolete ? */
 +			radix_tree_preload_end();
  			break;
 +		}
  
 -		put_page(page);
 -		if (err != -EEXIST)
 -			return NULL;
 -
 +		/* May fail (-ENOMEM) if radix-tree node allocation failed. */
 +		__SetPageLocked(new_page);
 +		__SetPageSwapBacked(new_page);
 +		err = __add_to_swap_cache(new_page, entry);
 +		if (likely(!err)) {
 +			radix_tree_preload_end();
 +			/*
 +			 * Initiate read into locked page and return.
 +			 */
 +			SetPageWorkingset(new_page);
 +			lru_cache_add_anon(new_page);
 +			*new_page_allocated = true;
 +			return new_page;
 +		}
 +		radix_tree_preload_end();
 +		__ClearPageLocked(new_page);
  		/*
 -		 * We might race against __delete_from_swap_cache(), and
 -		 * stumble across a swap_map entry whose SWAP_HAS_CACHE
 -		 * has not yet been cleared.  Or race against another
 -		 * __read_swap_cache_async(), which has set SWAP_HAS_CACHE
 -		 * in swap_map, but not yet added its page to swap cache.
 +		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
 +		 * clear SWAP_HAS_CACHE flag.
  		 */
 -		cond_resched();
 -	}
 -
 +		put_swap_page(new_page, entry);
 +	} while (err != -ENOMEM);
 +
++<<<<<<< HEAD
 +	if (new_page)
 +		put_page(new_page);
 +	return found_page;
++=======
+ 	/*
+ 	 * The swap entry is ours to swap in. Prepare the new page.
+ 	 */
+ 
+ 	__SetPageLocked(page);
+ 	__SetPageSwapBacked(page);
+ 
+ 	/* May fail (-ENOMEM) if XArray node allocation failed. */
+ 	if (add_to_swap_cache(page, entry, gfp_mask & GFP_KERNEL)) {
+ 		put_swap_page(page, entry);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	if (mem_cgroup_charge(page, NULL, gfp_mask)) {
+ 		delete_from_swap_cache(page);
+ 		goto fail_unlock;
+ 	}
+ 
+ 	/* Caller will initiate read into locked page */
+ 	SetPageWorkingset(page);
+ 	lru_cache_add(page);
+ 	*new_page_allocated = true;
+ 	return page;
+ 
+ fail_unlock:
+ 	unlock_page(page);
+ 	put_page(page);
+ 	return NULL;
++>>>>>>> 6058eaec816f (mm: fold and remove lru_cache_add_anon() and lru_cache_add_file())
  }
  
  /*
diff --git a/fs/cifs/file.c b/fs/cifs/file.c
index d8dd97303ee2..27089ff6025f 100644
--- a/fs/cifs/file.c
+++ b/fs/cifs/file.c
@@ -4164,7 +4164,7 @@ cifs_readv_complete(struct work_struct *work)
 	for (i = 0; i < rdata->nr_pages; i++) {
 		struct page *page = rdata->pages[i];
 
-		lru_cache_add_file(page);
+		lru_cache_add(page);
 
 		if (rdata->result == 0 ||
 		    (rdata->result == -EAGAIN && got_bytes)) {
@@ -4234,7 +4234,7 @@ readpages_fill_pages(struct TCP_Server_Info *server,
 			 * fill them until the writes are flushed.
 			 */
 			zero_user(page, 0, PAGE_SIZE);
-			lru_cache_add_file(page);
+			lru_cache_add(page);
 			flush_dcache_page(page);
 			SetPageUptodate(page);
 			unlock_page(page);
@@ -4244,7 +4244,7 @@ readpages_fill_pages(struct TCP_Server_Info *server,
 			continue;
 		} else {
 			/* no need to hold page hostage */
-			lru_cache_add_file(page);
+			lru_cache_add(page);
 			unlock_page(page);
 			put_page(page);
 			rdata->pages[i] = NULL;
@@ -4442,7 +4442,7 @@ static int cifs_readpages(struct file *file, struct address_space *mapping,
 			/* best to give up if we're out of mem */
 			list_for_each_entry_safe(page, tpage, &tmplist, lru) {
 				list_del(&page->lru);
-				lru_cache_add_file(page);
+				lru_cache_add(page);
 				unlock_page(page);
 				put_page(page);
 			}
@@ -4481,7 +4481,7 @@ static int cifs_readpages(struct file *file, struct address_space *mapping,
 			add_credits_and_wake_if(server, &rdata->credits, 0);
 			for (i = 0; i < rdata->nr_pages; i++) {
 				page = rdata->pages[i];
-				lru_cache_add_file(page);
+				lru_cache_add(page);
 				unlock_page(page);
 				put_page(page);
 			}
diff --git a/fs/fuse/dev.c b/fs/fuse/dev.c
index eaf5d6cf9b45..9474d3c56654 100644
--- a/fs/fuse/dev.c
+++ b/fs/fuse/dev.c
@@ -839,7 +839,7 @@ static int fuse_try_move_page(struct fuse_copy_state *cs, struct page **pagep)
 	get_page(newpage);
 
 	if (!(buf->flags & PIPE_BUF_FLAG_LRU))
-		lru_cache_add_file(newpage);
+		lru_cache_add(newpage);
 
 	err = 0;
 	spin_lock(&cs->req->waitq.lock);
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 8552871db183..c43b8fccf0e7 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -326,8 +326,6 @@ extern unsigned long nr_free_pagecache_pages(void);
 
 /* linux/mm/swap.c */
 extern void lru_cache_add(struct page *);
-extern void lru_cache_add_anon(struct page *page);
-extern void lru_cache_add_file(struct page *page);
 extern void lru_add_page_tail(struct page *page, struct page *page_tail,
 			 struct lruvec *lruvec, struct list_head *head);
 extern void activate_page(struct page *);
* Unmerged path mm/khugepaged.c
* Unmerged path mm/memory.c
* Unmerged path mm/shmem.c
* Unmerged path mm/swap.c
* Unmerged path mm/swap_state.c
