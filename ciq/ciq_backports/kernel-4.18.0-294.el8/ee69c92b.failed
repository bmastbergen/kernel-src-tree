KVM: x86: Return bool instead of int for CR4 and SREGS validity checks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Sean Christopherson <sean.j.christopherson@intel.com>
commit ee69c92bac61f4379e97f40b259a1c1257e5987f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/ee69c92b.failed

Rework the common CR4 and SREGS checks to return a bool instead of an
int, i.e. true/false instead of 0/-EINVAL, and add "is" to the name to
clarify the polarity of the return value (which is effectively inverted
by this change).

No functional changed intended.

	Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
Message-Id: <20201007014417.29276-6-sean.j.christopherson@intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit ee69c92bac61f4379e97f40b259a1c1257e5987f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/nested.c
#	arch/x86/kvm/x86.c
#	arch/x86/kvm/x86.h
diff --cc arch/x86/kvm/svm/nested.c
index a41617fb2750,b0f37183e8f5..000000000000
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@@ -229,19 -225,39 +229,42 @@@ static bool nested_vmcb_check_controls(
  	return true;
  }
  
 -static bool nested_vmcb_checks(struct vcpu_svm *svm, struct vmcb *vmcb12)
 +static bool nested_vmcb_checks(struct vmcb *vmcb)
  {
 -	bool vmcb12_lma;
 -
 -	if ((vmcb12->save.efer & EFER_SVME) == 0)
 +	if ((vmcb->save.efer & EFER_SVME) == 0)
  		return false;
  
 -	if (((vmcb12->save.cr0 & X86_CR0_CD) == 0) && (vmcb12->save.cr0 & X86_CR0_NW))
 +	if (((vmcb->save.cr0 & X86_CR0_CD) == 0) &&
 +	    (vmcb->save.cr0 & X86_CR0_NW))
  		return false;
  
 -	if (!kvm_dr6_valid(vmcb12->save.dr6) || !kvm_dr7_valid(vmcb12->save.dr7))
 +	if (!kvm_dr6_valid(vmcb->save.dr6) || !kvm_dr7_valid(vmcb->save.dr7))
  		return false;
  
++<<<<<<< HEAD
 +	return nested_vmcb_check_controls(&vmcb->control);
++=======
+ 	vmcb12_lma = (vmcb12->save.efer & EFER_LME) && (vmcb12->save.cr0 & X86_CR0_PG);
+ 
+ 	if (!vmcb12_lma) {
+ 		if (vmcb12->save.cr4 & X86_CR4_PAE) {
+ 			if (vmcb12->save.cr3 & MSR_CR3_LEGACY_PAE_RESERVED_MASK)
+ 				return false;
+ 		} else {
+ 			if (vmcb12->save.cr3 & MSR_CR3_LEGACY_RESERVED_MASK)
+ 				return false;
+ 		}
+ 	} else {
+ 		if (!(vmcb12->save.cr4 & X86_CR4_PAE) ||
+ 		    !(vmcb12->save.cr0 & X86_CR0_PE) ||
+ 		    (vmcb12->save.cr3 & MSR_CR3_LONG_MBZ_MASK))
+ 			return false;
+ 	}
+ 	if (!kvm_is_valid_cr4(&svm->vcpu, vmcb12->save.cr4))
+ 		return false;
+ 
+ 	return nested_vmcb_check_controls(&vmcb12->control);
++>>>>>>> ee69c92bac61 (KVM: x86: Return bool instead of int for CR4 and SREGS validity checks)
  }
  
  static void load_nested_vmcb_control(struct vcpu_svm *svm,
diff --cc arch/x86/kvm/x86.c
index 7e227ac0ce01,2db86702cac4..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -958,19 -964,17 +958,24 @@@ int kvm_set_xcr(struct kvm_vcpu *vcpu, 
  }
  EXPORT_SYMBOL_GPL(kvm_set_xcr);
  
++<<<<<<< HEAD
 +static int kvm_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
++=======
+ bool kvm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
++>>>>>>> ee69c92bac61 (KVM: x86: Return bool instead of int for CR4 and SREGS validity checks)
  {
  	if (cr4 & cr4_reserved_bits)
- 		return -EINVAL;
+ 		return false;
  
  	if (cr4 & vcpu->arch.cr4_guest_rsvd_bits)
- 		return -EINVAL;
- 
- 	if (!kvm_x86_ops.is_valid_cr4(vcpu, cr4))
- 		return -EINVAL;
+ 		return false;
  
- 	return 0;
+ 	return kvm_x86_ops.is_valid_cr4(vcpu, cr4);
  }
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL_GPL(kvm_is_valid_cr4);
++>>>>>>> ee69c92bac61 (KVM: x86: Return bool instead of int for CR4 and SREGS validity checks)
  
  int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
  {
diff --cc arch/x86/kvm/x86.h
index 9d742f062ef2,764c967a1993..000000000000
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@@ -368,8 -368,8 +368,12 @@@ static inline bool kvm_dr6_valid(u64 da
  
  void kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu);
  void kvm_load_host_xsave_state(struct kvm_vcpu *vcpu);
 +
  int kvm_spec_ctrl_test_value(u64 value);
++<<<<<<< HEAD
++=======
+ bool kvm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
++>>>>>>> ee69c92bac61 (KVM: x86: Return bool instead of int for CR4 and SREGS validity checks)
  bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu);
  int kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,
  			      struct x86_exception *e);
* Unmerged path arch/x86/kvm/svm/nested.c
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 8ec808eb6369..3146516348b3 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -3041,7 +3041,7 @@ static bool vmx_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	/*
 	 * We operate under the default treatment of SMM, so VMX cannot be
 	 * enabled under SMM.  Note, whether or not VMXE is allowed at all is
-	 * handled by kvm_valid_cr4().
+	 * handled by kvm_is_valid_cr4().
 	 */
 	if ((cr4 & X86_CR4_VMXE) && is_smm(vcpu))
 		return false;
* Unmerged path arch/x86/kvm/x86.c
* Unmerged path arch/x86/kvm/x86.h
