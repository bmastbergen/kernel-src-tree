x86/cpu: Relocate sync_core() to sync_core.h

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
commit 9998a9832c4027e907353e5e05fde730cf624b77
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/9998a983.failed

Having sync_core() in processor.h is problematic since it is not possible
to check for hardware capabilities via the *cpu_has() family of macros.
The latter needs the definitions in processor.h.

It also looks more intuitive to relocate the function to sync_core.h.

This changeset does not make changes in functionality.

	Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Reviewed-by: Tony Luck <tony.luck@intel.com>
Link: https://lore.kernel.org/r/20200727043132.15082-3-ricardo.neri-calderon@linux.intel.com
(cherry picked from commit 9998a9832c4027e907353e5e05fde730cf624b77)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/processor.h
#	arch/x86/kernel/cpu/mce/core.c
diff --cc arch/x86/include/asm/processor.h
index bb72d1a49158,68ba42fdd184..000000000000
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@@ -662,72 -678,6 +662,75 @@@ static inline unsigned int cpuid_edx(un
  	return edx;
  }
  
++<<<<<<< HEAD
 +/*
 + * This function forces the icache and prefetched instruction stream to
 + * catch up with reality in two very specific cases:
 + *
 + *  a) Text was modified using one virtual address and is about to be executed
 + *     from the same physical page at a different virtual address.
 + *
 + *  b) Text was modified on a different CPU, may subsequently be
 + *     executed on this CPU, and you want to make sure the new version
 + *     gets executed.  This generally means you're calling this in a IPI.
 + *
 + * If you're calling this for a different reason, you're probably doing
 + * it wrong.
 + */
 +static inline void sync_core(void)
 +{
 +	/*
 +	 * There are quite a few ways to do this.  IRET-to-self is nice
 +	 * because it works on every CPU, at any CPL (so it's compatible
 +	 * with paravirtualization), and it never exits to a hypervisor.
 +	 * The only down sides are that it's a bit slow (it seems to be
 +	 * a bit more than 2x slower than the fastest options) and that
 +	 * it unmasks NMIs.  The "push %cs" is needed because, in
 +	 * paravirtual environments, __KERNEL_CS may not be a valid CS
 +	 * value when we do IRET directly.
 +	 *
 +	 * In case NMI unmasking or performance ever becomes a problem,
 +	 * the next best option appears to be MOV-to-CR2 and an
 +	 * unconditional jump.  That sequence also works on all CPUs,
 +	 * but it will fault at CPL3 (i.e. Xen PV).
 +	 *
 +	 * CPUID is the conventional way, but it's nasty: it doesn't
 +	 * exist on some 486-like CPUs, and it usually exits to a
 +	 * hypervisor.
 +	 *
 +	 * Like all of Linux's memory ordering operations, this is a
 +	 * compiler barrier as well.
 +	 */
 +#ifdef CONFIG_X86_32
 +	asm volatile (
 +		"pushfl\n\t"
 +		"pushl %%cs\n\t"
 +		"pushl $1f\n\t"
 +		"iret\n\t"
 +		"1:"
 +		: ASM_CALL_CONSTRAINT : : "memory");
 +#else
 +	unsigned int tmp;
 +
 +	asm volatile (
 +		UNWIND_HINT_SAVE
 +		"mov %%ss, %0\n\t"
 +		"pushq %q0\n\t"
 +		"pushq %%rsp\n\t"
 +		"addq $8, (%%rsp)\n\t"
 +		"pushfq\n\t"
 +		"mov %%cs, %0\n\t"
 +		"pushq %q0\n\t"
 +		"pushq $1f\n\t"
 +		"iretq\n\t"
 +		UNWIND_HINT_RESTORE
 +		"1:"
 +		: "=&r" (tmp), ASM_CALL_CONSTRAINT : : "cc", "memory");
 +#endif
 +}
 +
++=======
++>>>>>>> 9998a9832c40 (x86/cpu: Relocate sync_core() to sync_core.h)
  extern void select_idle_routine(const struct cpuinfo_x86 *c);
  extern void amd_e400_c1e_apic_setup(void);
  
diff --cc arch/x86/kernel/cpu/mce/core.c
index df108ae34b27,9246595c07d7..000000000000
--- a/arch/x86/kernel/cpu/mce/core.c
+++ b/arch/x86/kernel/cpu/mce/core.c
@@@ -43,6 -42,9 +43,12 @@@
  #include <linux/export.h>
  #include <linux/jump_label.h>
  #include <linux/set_memory.h>
++<<<<<<< HEAD
++=======
+ #include <linux/sync_core.h>
+ #include <linux/task_work.h>
+ #include <linux/hardirq.h>
++>>>>>>> 9998a9832c40 (x86/cpu: Relocate sync_core() to sync_core.h)
  
  #include <asm/intel-family.h>
  #include <asm/processor.h>
* Unmerged path arch/x86/include/asm/processor.h
diff --git a/arch/x86/include/asm/sync_core.h b/arch/x86/include/asm/sync_core.h
index c67caafd3381..9c5573f2c333 100644
--- a/arch/x86/include/asm/sync_core.h
+++ b/arch/x86/include/asm/sync_core.h
@@ -6,6 +6,70 @@
 #include <asm/processor.h>
 #include <asm/cpufeature.h>
 
+/*
+ * This function forces the icache and prefetched instruction stream to
+ * catch up with reality in two very specific cases:
+ *
+ *  a) Text was modified using one virtual address and is about to be executed
+ *     from the same physical page at a different virtual address.
+ *
+ *  b) Text was modified on a different CPU, may subsequently be
+ *     executed on this CPU, and you want to make sure the new version
+ *     gets executed.  This generally means you're calling this in a IPI.
+ *
+ * If you're calling this for a different reason, you're probably doing
+ * it wrong.
+ */
+static inline void sync_core(void)
+{
+	/*
+	 * There are quite a few ways to do this.  IRET-to-self is nice
+	 * because it works on every CPU, at any CPL (so it's compatible
+	 * with paravirtualization), and it never exits to a hypervisor.
+	 * The only down sides are that it's a bit slow (it seems to be
+	 * a bit more than 2x slower than the fastest options) and that
+	 * it unmasks NMIs.  The "push %cs" is needed because, in
+	 * paravirtual environments, __KERNEL_CS may not be a valid CS
+	 * value when we do IRET directly.
+	 *
+	 * In case NMI unmasking or performance ever becomes a problem,
+	 * the next best option appears to be MOV-to-CR2 and an
+	 * unconditional jump.  That sequence also works on all CPUs,
+	 * but it will fault at CPL3 (i.e. Xen PV).
+	 *
+	 * CPUID is the conventional way, but it's nasty: it doesn't
+	 * exist on some 486-like CPUs, and it usually exits to a
+	 * hypervisor.
+	 *
+	 * Like all of Linux's memory ordering operations, this is a
+	 * compiler barrier as well.
+	 */
+#ifdef CONFIG_X86_32
+	asm volatile (
+		"pushfl\n\t"
+		"pushl %%cs\n\t"
+		"pushl $1f\n\t"
+		"iret\n\t"
+		"1:"
+		: ASM_CALL_CONSTRAINT : : "memory");
+#else
+	unsigned int tmp;
+
+	asm volatile (
+		"mov %%ss, %0\n\t"
+		"pushq %q0\n\t"
+		"pushq %%rsp\n\t"
+		"addq $8, (%%rsp)\n\t"
+		"pushfq\n\t"
+		"mov %%cs, %0\n\t"
+		"pushq %q0\n\t"
+		"pushq $1f\n\t"
+		"iretq\n\t"
+		"1:"
+		: "=&r" (tmp), ASM_CALL_CONSTRAINT : : "cc", "memory");
+#endif
+}
+
 /*
  * Ensure that a core serializing instruction is issued before returning
  * to user-mode. x86 implements return to user-space through sysexit,
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index e36ef258e53b..2ba3aea41b85 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -13,6 +13,7 @@
 #include <linux/kdebug.h>
 #include <linux/kprobes.h>
 #include <linux/bsearch.h>
+#include <linux/sync_core.h>
 #include <asm/text-patching.h>
 #include <asm/alternative.h>
 #include <asm/sections.h>
* Unmerged path arch/x86/kernel/cpu/mce/core.c
diff --git a/drivers/misc/sgi-gru/grufault.c b/drivers/misc/sgi-gru/grufault.c
index 93be82fc338a..5558296a0cf6 100644
--- a/drivers/misc/sgi-gru/grufault.c
+++ b/drivers/misc/sgi-gru/grufault.c
@@ -33,6 +33,7 @@
 #include <linux/io.h>
 #include <linux/uaccess.h>
 #include <linux/security.h>
+#include <linux/sync_core.h>
 #include <linux/prefetch.h>
 #include <asm/pgtable.h>
 #include "gru.h"
diff --git a/drivers/misc/sgi-gru/gruhandles.c b/drivers/misc/sgi-gru/gruhandles.c
index 1ee8e82ba710..9602867e64af 100644
--- a/drivers/misc/sgi-gru/gruhandles.c
+++ b/drivers/misc/sgi-gru/gruhandles.c
@@ -29,6 +29,7 @@
 #define GRU_OPERATION_TIMEOUT	(((cycles_t) local_cpu_data->itc_freq)*10)
 #define CLKS2NSEC(c)		((c) *1000000000 / local_cpu_data->itc_freq)
 #else
+#include <linux/sync_core.h>
 #include <asm/tsc.h>
 #define GRU_OPERATION_TIMEOUT	((cycles_t) tsc_khz*10*1000)
 #define CLKS2NSEC(c)		((c) * 1000000 / tsc_khz)
diff --git a/drivers/misc/sgi-gru/grukservices.c b/drivers/misc/sgi-gru/grukservices.c
index 030769018461..61000be6b2aa 100644
--- a/drivers/misc/sgi-gru/grukservices.c
+++ b/drivers/misc/sgi-gru/grukservices.c
@@ -29,6 +29,7 @@
 #include <linux/miscdevice.h>
 #include <linux/proc_fs.h>
 #include <linux/interrupt.h>
+#include <linux/sync_core.h>
 #include <linux/uaccess.h>
 #include <linux/delay.h>
 #include <linux/export.h>
