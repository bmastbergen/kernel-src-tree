arm64: kvm: hyp: use cpus_have_final_cap()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
Rebuild_CHGLOG: - [arm64] kvm: hyp: use cpus_have_final_cap() (Auger Eric) [1882794]
Rebuild_FUZZ: 90.91%
commit-author Mark Rutland <mark.rutland@arm.com>
commit b5475d8caedb71476f999a858ea3f8c24c5f9e50
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/b5475d8c.failed

The KVM hyp code is only run after system capabilities have been
finalized, and thus all const cap checks have been patched. This is
noted in in __cpu_init_hyp_mode(), where we BUG() if called too early:

| /*
|  * Call initialization code, and switch to the full blown HYP code.
|  * If the cpucaps haven't been finalized yet, something has gone very
|  * wrong, and hyp will crash and burn when it uses any
|  * cpus_have_const_cap() wrapper.
|  */

Given this, the hyp code can use cpus_have_final_cap() and avoid
generating code to check the cpu_hwcaps array, which would be unsafe to
run in hyp context.

This patch migrate the KVM hyp code to cpus_have_final_cap(), avoiding
this redundant code generation, and making it possible to detect if we
accidentally invoke this code too early. In the latter case, the BUG()
in cpus_have_final_cap() will cause a hyp panic.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Reviewed-by: Marc Zyngier <maz@kernel.org>
	Cc: James Morse <james.morse@arm.com>
	Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
	Cc: Suzuki Poulouse <suzuki.poulose@arm.com>
	Cc: Will Deacon <will@kernel.org>
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit b5475d8caedb71476f999a858ea3f8c24c5f9e50)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kvm/hyp/switch.c
#	arch/arm64/kvm/hyp/sysreg-sr.c
#	arch/arm64/kvm/hyp/tlb.c
diff --cc arch/arm64/kvm/hyp/switch.c
index 78cca9b5a0e9,27fcdff08dd6..000000000000
--- a/arch/arm64/kvm/hyp/switch.c
+++ b/arch/arm64/kvm/hyp/switch.c
@@@ -149,7 -127,7 +149,11 @@@ static void __hyp_text __activate_traps
  
  	write_sysreg(val, cptr_el2);
  
++<<<<<<< HEAD
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
++=======
+ 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_NVHE)) {
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  		struct kvm_cpu_context *ctxt = &vcpu->arch.ctxt;
  
  		isb();
@@@ -203,7 -181,7 +207,11 @@@ static void __hyp_text __deactivate_tra
  {
  	u64 mdcr_el2 = read_sysreg(mdcr_el2);
  
++<<<<<<< HEAD
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
++=======
+ 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_NVHE)) {
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  		u64 val;
  
  		/*
diff --cc arch/arm64/kvm/hyp/sysreg-sr.c
index a4eba45f8075,75b1925763f1..000000000000
--- a/arch/arm64/kvm/hyp/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/sysreg-sr.c
@@@ -117,8 -118,7 +117,12 @@@ static void __hyp_text __sysreg_restore
  	write_sysreg(ctxt->sys_regs[MPIDR_EL1],		vmpidr_el2);
  	write_sysreg(ctxt->sys_regs[CSSELR_EL1],	csselr_el1);
  
++<<<<<<< HEAD
 +	if (has_vhe() ||
 +	    !cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
++=======
+ 	if (!cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_NVHE)) {
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  		write_sysreg_el1(ctxt->sys_regs[SCTLR_EL1],	SYS_SCTLR);
  		write_sysreg_el1(ctxt->sys_regs[TCR_EL1],	SYS_TCR);
  	} else	if (!ctxt->__hyp_running_vcpu) {
@@@ -148,8 -149,7 +152,12 @@@
  	write_sysreg(ctxt->sys_regs[PAR_EL1],		par_el1);
  	write_sysreg(ctxt->sys_regs[TPIDR_EL1],		tpidr_el1);
  
++<<<<<<< HEAD
 +	if (!has_vhe() &&
 +	    cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT) &&
++=======
+ 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_NVHE) &&
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  	    ctxt->__hyp_running_vcpu) {
  		/*
  		 * Must only be done for host registers, hence the context
diff --cc arch/arm64/kvm/hyp/tlb.c
index d82b0d7000d0,ceaddbe4279f..000000000000
--- a/arch/arm64/kvm/hyp/tlb.c
+++ b/arch/arm64/kvm/hyp/tlb.c
@@@ -34,7 -23,7 +34,11 @@@ static void __hyp_text __tlb_switch_to_
  
  	local_irq_save(cxt->flags);
  
++<<<<<<< HEAD
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
++=======
+ 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_VHE)) {
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  		/*
  		 * For CPUs that are affected by ARM errata 1165522 or 1530923,
  		 * we cannot trust stage-1 to be in a correct state at that
@@@ -74,7 -63,7 +78,11 @@@
  static void __hyp_text __tlb_switch_to_guest_nvhe(struct kvm *kvm,
  						  struct tlb_inv_context *cxt)
  {
++<<<<<<< HEAD
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
++=======
+ 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_NVHE)) {
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  		u64 val;
  
  		/*
@@@ -115,7 -103,7 +123,11 @@@ static void __hyp_text __tlb_switch_to_
  	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
  	isb();
  
++<<<<<<< HEAD
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
++=======
+ 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_VHE)) {
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  		/* Restore the registers to what they were */
  		write_sysreg_el1(cxt->tcr, SYS_TCR);
  		write_sysreg_el1(cxt->sctlr, SYS_SCTLR);
@@@ -129,7 -117,7 +141,11 @@@ static void __hyp_text __tlb_switch_to_
  {
  	write_sysreg(0, vttbr_el2);
  
++<<<<<<< HEAD
 +	if (cpus_have_const_cap(ARM64_WORKAROUND_SPECULATIVE_AT)) {
++=======
+ 	if (cpus_have_final_cap(ARM64_WORKAROUND_SPECULATIVE_AT_NVHE)) {
++>>>>>>> b5475d8caedb (arm64: kvm: hyp: use cpus_have_final_cap())
  		/* Ensure write of the host VMID */
  		isb();
  		/* Restore the host's TCR_EL1 */
* Unmerged path arch/arm64/kvm/hyp/switch.c
* Unmerged path arch/arm64/kvm/hyp/sysreg-sr.c
* Unmerged path arch/arm64/kvm/hyp/tlb.c
