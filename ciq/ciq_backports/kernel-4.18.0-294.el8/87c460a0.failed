mm/khugepaged: collapse_shmem() without freezing new_page

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Hugh Dickins <hughd@google.com>
commit 87c460a0bded56195b5eb497d44709777ef7b415
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/87c460a0.failed

khugepaged's collapse_shmem() does almost all of its work, to assemble
the huge new_page from 512 scattered old pages, with the new_page's
refcount frozen to 0 (and refcounts of all old pages so far also frozen
to 0).  Including shmem_getpage() to read in any which were out on swap,
memory reclaim if necessary to allocate their intermediate pages, and
copying over all the data from old to new.

Imagine the frozen refcount as a spinlock held, but without any lock
debugging to highlight the abuse: it's not good, and under serious load
heads into lockups - speculative getters of the page are not expecting
to spin while khugepaged is rescheduled.

One can get a little further under load by hacking around elsewhere; but
fortunately, freezing the new_page turns out to have been entirely
unnecessary, with no hacks needed elsewhere.

The huge new_page lock is already held throughout, and guards all its
subpages as they are brought one by one into the page cache tree; and
anything reading the data in that page, without the lock, before it has
been marked PageUptodate, would already be in the wrong.  So simply
eliminate the freezing of the new_page.

Each of the old pages remains frozen with refcount 0 after it has been
replaced by a new_page subpage in the page cache tree, until they are
all unfrozen on success or failure: just as before.  They could be
unfrozen sooner, but cause no problem once no longer visible to
find_get_entry(), filemap_map_pages() and other speculative lookups.

Link: http://lkml.kernel.org/r/alpine.LSU.2.11.1811261527570.2275@eggly.anvils
Fixes: f3f0e1d2150b2 ("khugepaged: add support of collapse for tmpfs/shmem pages")
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: <stable@vger.kernel.org>	[4.8+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 87c460a0bded56195b5eb497d44709777ef7b415)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
diff --cc mm/khugepaged.c
index 180728fbfcba,55930cbed3fd..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1290,19 -1287,19 +1290,29 @@@ static void retract_page_tables(struct 
   * collapse_shmem - collapse small tmpfs/shmem pages into huge one.
   *
   * Basic scheme is simple, details are more complex:
++<<<<<<< HEAD
 + *  - allocate and freeze a new huge page;
 + *  - scan over radix tree replacing old pages the new one
++=======
+  *  - allocate and lock a new huge page;
+  *  - scan page cache replacing old pages with the new one
++>>>>>>> 87c460a0bded (mm/khugepaged: collapse_shmem() without freezing new_page)
   *    + swap in pages if necessary;
   *    + fill in gaps;
 - *    + keep old pages around in case rollback is required;
 - *  - if replacing succeeds:
 + *    + keep old pages around in case if rollback is required;
 + *  - if replacing succeed:
   *    + copy data over;
   *    + free old pages;
-  *    + unfreeze huge page;
+  *    + unlock huge page;
   *  - if replacing failed;
   *    + put all pages back and unfreeze them;
++<<<<<<< HEAD
 + *    + restore gaps in the radix-tree;
 + *    + free huge page;
++=======
+  *    + restore gaps in the page cache;
+  *    + unlock and free huge page;
++>>>>>>> 87c460a0bded (mm/khugepaged: collapse_shmem() without freezing new_page)
   */
  static void collapse_shmem(struct mm_struct *mm,
  		struct address_space *mapping, pgoff_t start,
@@@ -1333,47 -1329,56 +1343,56 @@@
  		goto out;
  	}
  
 -	__SetPageLocked(new_page);
 -	__SetPageSwapBacked(new_page);
  	new_page->index = start;
  	new_page->mapping = mapping;
++<<<<<<< HEAD
 +	__SetPageSwapBacked(new_page);
 +	__SetPageLocked(new_page);
 +	BUG_ON(!page_ref_freeze(new_page, 1));
++=======
++>>>>>>> 87c460a0bded (mm/khugepaged: collapse_shmem() without freezing new_page)
 +
  
  	/*
++<<<<<<< HEAD
 +	 * At this point the new_page is 'frozen' (page_count() is zero), locked
 +	 * and not up-to-date. It's safe to insert it into radix tree, because
 +	 * nobody would be able to map it or use it in other way until we
 +	 * unfreeze it.
++=======
+ 	 * At this point the new_page is locked and not up-to-date.
+ 	 * It's safe to insert it into the page cache, because nobody would
+ 	 * be able to map it or use it in another way until we unlock it.
++>>>>>>> 87c460a0bded (mm/khugepaged: collapse_shmem() without freezing new_page)
  	 */
  
 -	/* This will be less messy when we use multi-index entries */
 -	do {
 -		xas_lock_irq(&xas);
 -		xas_create_range(&xas);
 -		if (!xas_error(&xas))
 -			break;
 -		xas_unlock_irq(&xas);
 -		if (!xas_nomem(&xas, GFP_KERNEL))
 -			goto out;
 -	} while (1);
 -
 -	xas_set(&xas, start);
 -	for (index = start; index < end; index++) {
 -		struct page *page = xas_next(&xas);
 +	index = start;
 +	xa_lock_irq(&mapping->i_pages);
 +	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start) {
 +		int n = min(iter.index, end) - index;
  
 -		VM_BUG_ON(index != xas.xa_index);
 -		if (!page) {
 -			/*
 -			 * Stop if extent has been truncated or hole-punched,
 -			 * and is now completely empty.
 -			 */
 -			if (index == start) {
 -				if (!xas_next_entry(&xas, end - 1)) {
 -					result = SCAN_TRUNCATED;
 -					goto xa_locked;
 -				}
 -				xas_set(&xas, index);
 -			}
 -			if (!shmem_charge(mapping->host, 1)) {
 -				result = SCAN_FAIL;
 -				goto xa_locked;
 -			}
 -			xas_store(&xas, new_page + (index % HPAGE_PMD_NR));
 -			nr_none++;
 -			continue;
 +		/*
 +		 * Handle holes in the radix tree: charge it from shmem and
 +		 * insert relevant subpage of new_page into the radix-tree.
 +		 */
 +		if (n && !shmem_charge(mapping->host, n)) {
 +			result = SCAN_FAIL;
 +			break;
 +		}
 +		nr_none += n;
 +		for (; index < min(iter.index, end); index++) {
 +			radix_tree_insert(&mapping->i_pages, index,
 +					new_page + (index % HPAGE_PMD_NR));
  		}
  
 +		/* We are done. */
 +		if (index >= end)
 +			break;
 +
 +		page = radix_tree_deref_slot_protected(slot,
 +				&mapping->i_pages.xa_lock);
  		if (xa_is_value(page) || !PageUptodate(page)) {
 -			xas_unlock_irq(&xas);
 +			xa_unlock_irq(&mapping->i_pages);
  			/* swap in or instantiate fallocated page */
  			if (shmem_getpage(mapping->host, index, &page,
  						SGP_NOHUGE)) {
@@@ -1508,29 -1489,16 +1527,37 @@@ tree_unlocked
  			index++;
  		}
  
++<<<<<<< HEAD
 +		local_irq_save(flags);
 +		__inc_node_page_state(new_page, NR_SHMEM_THPS);
 +		if (nr_none) {
 +			__mod_node_page_state(zone->zone_pgdat, NR_FILE_PAGES, nr_none);
 +			__mod_node_page_state(zone->zone_pgdat, NR_SHMEM, nr_none);
 +		}
 +		local_irq_restore(flags);
++=======
+ 		SetPageUptodate(new_page);
+ 		page_ref_add(new_page, HPAGE_PMD_NR - 1);
+ 		set_page_dirty(new_page);
+ 		mem_cgroup_commit_charge(new_page, memcg, false, true);
+ 		lru_cache_add_anon(new_page);
++>>>>>>> 87c460a0bded (mm/khugepaged: collapse_shmem() without freezing new_page)
  
  		/*
 -		 * Remove pte page tables, so we can re-fault the page as huge.
 +		 * Remove pte page tables, so we can re-faulti
 +		 * the page as huge.
  		 */
  		retract_page_tables(mapping, start);
 +
 +		/* Everything is ready, let's unfreeze the new_page */
 +		set_page_dirty(new_page);
 +		SetPageUptodate(new_page);
 +		page_ref_unfreeze(new_page, HPAGE_PMD_NR);
 +		mem_cgroup_commit_charge(new_page, memcg, false, true);
 +		count_memcg_events(memcg, THP_COLLAPSE_ALLOC, 1);
 +		lru_cache_add_anon(new_page);
 +		unlock_page(new_page);
 +
  		*hpage = NULL;
  
  		khugepaged_pages_collapsed++;
@@@ -1558,22 -1528,21 +1585,20 @@@
  			/* Unfreeze the page. */
  			list_del(&page->lru);
  			page_ref_unfreeze(page, 2);
 -			xas_store(&xas, page);
 -			xas_pause(&xas);
 -			xas_unlock_irq(&xas);
 -			unlock_page(page);
 +			radix_tree_replace_slot(&mapping->i_pages, slot, page);
 +			slot = radix_tree_iter_resume(slot, &iter);
 +			xa_unlock_irq(&mapping->i_pages);
  			putback_lru_page(page);
 -			xas_lock_irq(&xas);
 +			unlock_page(page);
 +			xa_lock_irq(&mapping->i_pages);
  		}
  		VM_BUG_ON(nr_none);
 -		xas_unlock_irq(&xas);
 +		xa_unlock_irq(&mapping->i_pages);
  
- 		/* Unfreeze new_page, caller would take care about freeing it */
- 		page_ref_unfreeze(new_page, 1);
  		mem_cgroup_cancel_charge(new_page, memcg, true);
 +		unlock_page(new_page);
  		new_page->mapping = NULL;
  	}
 -
 -	unlock_page(new_page);
  out:
  	VM_BUG_ON(!list_empty(&pagelist));
  	/* TODO: tracepoints */
* Unmerged path mm/khugepaged.c
