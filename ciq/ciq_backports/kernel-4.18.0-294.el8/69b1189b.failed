dmaengine: Remove dma_device_satisfies_mask() wrapper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Geert Uytterhoeven <geert+renesas@glider.be>
commit 69b1189ba2cd6643474312004f10685324e38f58
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/69b1189b.failed

Commit aa1e6f1a385eb2b0 ("dmaengine: kill struct dma_client and
supporting infrastructure") removed the last user of the
dma_device_satisfies_mask() wrapper.

Remove the wrapper, and rename __dma_device_satisfies_mask() to
dma_device_satisfies_mask(), to get rid of one more function starting
with a double underscore.

	Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
	Acked-by: Arnd Bergmann <arnd@arndb.de>
Link: https://lore.kernel.org/r/20200121093311.28639-2-geert+renesas@glider.be
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 69b1189ba2cd6643474312004f10685324e38f58)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/dmaengine.c
diff --cc drivers/dma/dmaengine.c
index 07608c490b38,7550dbdf5488..000000000000
--- a/drivers/dma/dmaengine.c
+++ b/drivers/dma/dmaengine.c
@@@ -163,11 -164,152 +163,160 @@@ static struct class dma_devclass = 
  
  /* --- client and device registration --- */
  
++<<<<<<< HEAD
 +#define dma_device_satisfies_mask(device, mask) \
 +	__dma_device_satisfies_mask((device), &(mask))
 +static int
 +__dma_device_satisfies_mask(struct dma_device *device,
 +			    const dma_cap_mask_t *want)
++=======
+ /**
+  * dma_cap_mask_all - enable iteration over all operation types
+  */
+ static dma_cap_mask_t dma_cap_mask_all;
+ 
+ /**
+  * dma_chan_tbl_ent - tracks channel allocations per core/operation
+  * @chan - associated channel for this entry
+  */
+ struct dma_chan_tbl_ent {
+ 	struct dma_chan *chan;
+ };
+ 
+ /**
+  * channel_table - percpu lookup table for memory-to-memory offload providers
+  */
+ static struct dma_chan_tbl_ent __percpu *channel_table[DMA_TX_TYPE_END];
+ 
+ static int __init dma_channel_table_init(void)
+ {
+ 	enum dma_transaction_type cap;
+ 	int err = 0;
+ 
+ 	bitmap_fill(dma_cap_mask_all.bits, DMA_TX_TYPE_END);
+ 
+ 	/* 'interrupt', 'private', and 'slave' are channel capabilities,
+ 	 * but are not associated with an operation so they do not need
+ 	 * an entry in the channel_table
+ 	 */
+ 	clear_bit(DMA_INTERRUPT, dma_cap_mask_all.bits);
+ 	clear_bit(DMA_PRIVATE, dma_cap_mask_all.bits);
+ 	clear_bit(DMA_SLAVE, dma_cap_mask_all.bits);
+ 
+ 	for_each_dma_cap_mask(cap, dma_cap_mask_all) {
+ 		channel_table[cap] = alloc_percpu(struct dma_chan_tbl_ent);
+ 		if (!channel_table[cap]) {
+ 			err = -ENOMEM;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (err) {
+ 		pr_err("dmaengine dma_channel_table_init failure: %d\n", err);
+ 		for_each_dma_cap_mask(cap, dma_cap_mask_all)
+ 			free_percpu(channel_table[cap]);
+ 	}
+ 
+ 	return err;
+ }
+ arch_initcall(dma_channel_table_init);
+ 
+ /**
+  * dma_chan_is_local - returns true if the channel is in the same numa-node as
+  *	the cpu
+  */
+ static bool dma_chan_is_local(struct dma_chan *chan, int cpu)
+ {
+ 	int node = dev_to_node(chan->device->dev);
+ 	return node == NUMA_NO_NODE ||
+ 		cpumask_test_cpu(cpu, cpumask_of_node(node));
+ }
+ 
+ /**
+  * min_chan - returns the channel with min count and in the same numa-node as
+  *	the cpu
+  * @cap: capability to match
+  * @cpu: cpu index which the channel should be close to
+  *
+  * If some channels are close to the given cpu, the one with the lowest
+  * reference count is returned. Otherwise, cpu is ignored and only the
+  * reference count is taken into account.
+  * Must be called under dma_list_mutex.
+  */
+ static struct dma_chan *min_chan(enum dma_transaction_type cap, int cpu)
+ {
+ 	struct dma_device *device;
+ 	struct dma_chan *chan;
+ 	struct dma_chan *min = NULL;
+ 	struct dma_chan *localmin = NULL;
+ 
+ 	list_for_each_entry(device, &dma_device_list, global_node) {
+ 		if (!dma_has_cap(cap, device->cap_mask) ||
+ 		    dma_has_cap(DMA_PRIVATE, device->cap_mask))
+ 			continue;
+ 		list_for_each_entry(chan, &device->channels, device_node) {
+ 			if (!chan->client_count)
+ 				continue;
+ 			if (!min || chan->table_count < min->table_count)
+ 				min = chan;
+ 
+ 			if (dma_chan_is_local(chan, cpu))
+ 				if (!localmin ||
+ 				    chan->table_count < localmin->table_count)
+ 					localmin = chan;
+ 		}
+ 	}
+ 
+ 	chan = localmin ? localmin : min;
+ 
+ 	if (chan)
+ 		chan->table_count++;
+ 
+ 	return chan;
+ }
+ 
+ /**
+  * dma_channel_rebalance - redistribute the available channels
+  *
+  * Optimize for cpu isolation (each cpu gets a dedicated channel for an
+  * operation type) in the SMP case,  and operation isolation (avoid
+  * multi-tasking channels) in the non-SMP case.  Must be called under
+  * dma_list_mutex.
+  */
+ static void dma_channel_rebalance(void)
+ {
+ 	struct dma_chan *chan;
+ 	struct dma_device *device;
+ 	int cpu;
+ 	int cap;
+ 
+ 	/* undo the last distribution */
+ 	for_each_dma_cap_mask(cap, dma_cap_mask_all)
+ 		for_each_possible_cpu(cpu)
+ 			per_cpu_ptr(channel_table[cap], cpu)->chan = NULL;
+ 
+ 	list_for_each_entry(device, &dma_device_list, global_node) {
+ 		if (dma_has_cap(DMA_PRIVATE, device->cap_mask))
+ 			continue;
+ 		list_for_each_entry(chan, &device->channels, device_node)
+ 			chan->table_count = 0;
+ 	}
+ 
+ 	/* don't populate the channel_table if no clients are available */
+ 	if (!dmaengine_ref_count)
+ 		return;
+ 
+ 	/* redistribute available channels */
+ 	for_each_dma_cap_mask(cap, dma_cap_mask_all)
+ 		for_each_online_cpu(cpu) {
+ 			chan = min_chan(cap, cpu);
+ 			per_cpu_ptr(channel_table[cap], cpu)->chan = chan;
+ 		}
+ }
+ 
+ static int dma_device_satisfies_mask(struct dma_device *device,
+ 				     const dma_cap_mask_t *want)
++>>>>>>> 69b1189ba2cd (dmaengine: Remove dma_device_satisfies_mask() wrapper)
  {
  	dma_cap_mask_t has;
  
* Unmerged path drivers/dma/dmaengine.c
