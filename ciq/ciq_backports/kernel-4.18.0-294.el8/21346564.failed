y2038: vdso: change time_t to __kernel_old_time_t

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Arnd Bergmann <arnd@arndb.de>
commit 21346564ccad17b928cf0d51584608531d91d298
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/21346564.failed

Only x86 uses the 'time' syscall in vdso, so change that to
__kernel_old_time_t as a preparation for removing 'time_t' and
'__kernel_time_t' later.

	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Arnd Bergmann <arnd@arndb.de>
(cherry picked from commit 21346564ccad17b928cf0d51584608531d91d298)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/vdso/vclock_gettime.c
diff --cc arch/x86/entry/vdso/vclock_gettime.c
index 23aabf6cd315,7d70935b6758..000000000000
--- a/arch/x86/entry/vdso/vclock_gettime.c
+++ b/arch/x86/entry/vdso/vclock_gettime.c
@@@ -6,296 -7,79 +6,334 @@@
   *
   * 32 Bit compat layer by Stefani Seibold <stefani@seibold.net>
   *  sponsored by Rohde & Schwarz GmbH & Co. KG Munich/Germany
 + *
 + * The code should have no internal unresolved relocations.
 + * Check with readelf after changing.
   */
 +
 +#include <uapi/linux/time.h>
 +#include <asm/vgtod.h>
 +#include <asm/vvar.h>
 +#include <asm/unistd.h>
 +#include <asm/msr.h>
 +#include <asm/pvclock.h>
 +#include <clocksource/hyperv_timer.h>
 +#include <linux/math64.h>
  #include <linux/time.h>
  #include <linux/kernel.h>
 -#include <linux/types.h>
  
 -#include "../../../../lib/vdso/gettimeofday.c"
 +#define gtod (&VVAR(vsyscall_gtod_data))
  
++<<<<<<< HEAD
 +extern int __vdso_clock_gettime(clockid_t clock, struct timespec *ts);
 +extern int __vdso_gettimeofday(struct timeval *tv, struct timezone *tz);
 +extern time_t __vdso_time(time_t *t);
++=======
+ extern int __vdso_gettimeofday(struct __kernel_old_timeval *tv, struct timezone *tz);
+ extern __kernel_old_time_t __vdso_time(__kernel_old_time_t *t);
++>>>>>>> 21346564ccad (y2038: vdso: change time_t to __kernel_old_time_t)
 +
 +#ifdef CONFIG_PARAVIRT_CLOCK
 +extern u8 pvclock_page
 +	__attribute__((visibility("hidden")));
 +#endif
 +
 +#ifdef CONFIG_HYPERV_TIMER
 +extern u8 hvclock_page
 +	__attribute__((visibility("hidden")));
 +#endif
 +
 +#ifndef BUILD_VDSO32
  
 -int __vdso_gettimeofday(struct __kernel_old_timeval *tv, struct timezone *tz)
 +notrace static long vdso_fallback_gettime(long clock, struct timespec *ts)
  {
 -	return __cvdso_gettimeofday(tv, tz);
 +	long ret;
 +	asm ("syscall" : "=a" (ret), "=m" (*ts) :
 +	     "0" (__NR_clock_gettime), "D" (clock), "S" (ts) :
 +	     "rcx", "r11");
 +	return ret;
  }
  
++<<<<<<< HEAD
 +notrace static long vdso_fallback_gtod(struct timeval *tv, struct timezone *tz)
++=======
+ int gettimeofday(struct __kernel_old_timeval *, struct timezone *)
+ 	__attribute__((weak, alias("__vdso_gettimeofday")));
+ 
+ __kernel_old_time_t __vdso_time(__kernel_old_time_t *t)
++>>>>>>> 21346564ccad (y2038: vdso: change time_t to __kernel_old_time_t)
  {
 -	return __cvdso_time(t);
 +	long ret;
 +
 +	asm ("syscall" : "=a" (ret), "=m" (*tv), "=m" (*tz) :
 +	     "0" (__NR_gettimeofday), "D" (tv), "S" (tz) :
 +	     "memory", "rcx", "r11");
 +	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ __kernel_old_time_t time(__kernel_old_time_t *t)	__attribute__((weak, alias("__vdso_time")));
+ 
+ 
+ #if defined(CONFIG_X86_64) && !defined(BUILD_VDSO32_64)
+ /* both 64-bit and x32 use these */
+ extern int __vdso_clock_gettime(clockid_t clock, struct __kernel_timespec *ts);
+ extern int __vdso_clock_getres(clockid_t clock, struct __kernel_timespec *res);
+ 
+ int __vdso_clock_gettime(clockid_t clock, struct __kernel_timespec *ts)
+ {
+ 	return __cvdso_clock_gettime(clock, ts);
+ }
+ 
+ int clock_gettime(clockid_t, struct __kernel_timespec *)
+ 	__attribute__((weak, alias("__vdso_clock_gettime")));
+ 
+ int __vdso_clock_getres(clockid_t clock,
+ 			struct __kernel_timespec *res)
+ {
+ 	return __cvdso_clock_getres(clock, res);
+ }
+ int clock_getres(clockid_t, struct __kernel_timespec *)
+ 	__attribute__((weak, alias("__vdso_clock_getres")));
++>>>>>>> 21346564ccad (y2038: vdso: change time_t to __kernel_old_time_t)
  
  #else
 -/* i386 only */
 -extern int __vdso_clock_gettime(clockid_t clock, struct old_timespec32 *ts);
 -extern int __vdso_clock_getres(clockid_t clock, struct old_timespec32 *res);
  
 -int __vdso_clock_gettime(clockid_t clock, struct old_timespec32 *ts)
 +notrace static long vdso_fallback_gettime(long clock, struct timespec *ts)
  {
 -	return __cvdso_clock_gettime32(clock, ts);
 +	long ret;
 +
 +	asm (
 +		"mov %%ebx, %%edx \n"
 +		"mov %[clock], %%ebx \n"
 +		"call __kernel_vsyscall \n"
 +		"mov %%edx, %%ebx \n"
 +		: "=a" (ret), "=m" (*ts)
 +		: "0" (__NR_clock_gettime), [clock] "g" (clock), "c" (ts)
 +		: "edx");
 +	return ret;
  }
  
 -int clock_gettime(clockid_t, struct old_timespec32 *)
 -	__attribute__((weak, alias("__vdso_clock_gettime")));
 +notrace static long vdso_fallback_gtod(struct timeval *tv, struct timezone *tz)
 +{
 +	long ret;
 +
 +	asm (
 +		"mov %%ebx, %%edx \n"
 +		"mov %[tv], %%ebx \n"
 +		"call __kernel_vsyscall \n"
 +		"mov %%edx, %%ebx \n"
 +		: "=a" (ret), "=m" (*tv), "=m" (*tz)
 +		: "0" (__NR_gettimeofday), [tv] "g" (tv), "c" (tz)
 +		: "memory", "edx");
 +	return ret;
 +}
 +
 +#endif
  
 -int __vdso_clock_gettime64(clockid_t clock, struct __kernel_timespec *ts)
 +#ifdef CONFIG_PARAVIRT_CLOCK
 +static notrace const struct pvclock_vsyscall_time_info *get_pvti0(void)
  {
 -	return __cvdso_clock_gettime(clock, ts);
 +	return (const struct pvclock_vsyscall_time_info *)&pvclock_page;
  }
  
 -int clock_gettime64(clockid_t, struct __kernel_timespec *)
 -	__attribute__((weak, alias("__vdso_clock_gettime64")));
 +static notrace u64 vread_pvclock(int *mode)
 +{
 +	const struct pvclock_vcpu_time_info *pvti = &get_pvti0()->pvti;
 +	u64 ret;
 +	u64 last;
 +	u32 version;
 +
 +	/*
 +	 * Note: The kernel and hypervisor must guarantee that cpu ID
 +	 * number maps 1:1 to per-CPU pvclock time info.
 +	 *
 +	 * Because the hypervisor is entirely unaware of guest userspace
 +	 * preemption, it cannot guarantee that per-CPU pvclock time
 +	 * info is updated if the underlying CPU changes or that that
 +	 * version is increased whenever underlying CPU changes.
 +	 *
 +	 * On KVM, we are guaranteed that pvti updates for any vCPU are
 +	 * atomic as seen by *all* vCPUs.  This is an even stronger
 +	 * guarantee than we get with a normal seqlock.
 +	 *
 +	 * On Xen, we don't appear to have that guarantee, but Xen still
 +	 * supplies a valid seqlock using the version field.
 +	 *
 +	 * We only do pvclock vdso timing at all if
 +	 * PVCLOCK_TSC_STABLE_BIT is set, and we interpret that bit to
 +	 * mean that all vCPUs have matching pvti and that the TSC is
 +	 * synced, so we can just look at vCPU 0's pvti.
 +	 */
 +
 +	do {
 +		version = pvclock_read_begin(pvti);
 +
 +		if (unlikely(!(pvti->flags & PVCLOCK_TSC_STABLE_BIT))) {
 +			*mode = VCLOCK_NONE;
 +			return 0;
 +		}
 +
 +		ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
 +	} while (pvclock_read_retry(pvti, version));
 +
 +	/* refer to vread_tsc() comment for rationale */
 +	last = gtod->cycle_last;
 +
 +	if (likely(ret >= last))
 +		return ret;
  
 -int __vdso_clock_getres(clockid_t clock, struct old_timespec32 *res)
 +	return last;
 +}
 +#endif
 +#ifdef CONFIG_HYPERV_TIMER
 +static notrace u64 vread_hvclock(int *mode)
  {
 -	return __cvdso_clock_getres_time32(clock, res);
 +	const struct ms_hyperv_tsc_page *tsc_pg =
 +		(const struct ms_hyperv_tsc_page *)&hvclock_page;
 +	u64 current_tick = hv_read_tsc_page(tsc_pg);
 +
 +	if (current_tick != U64_MAX)
 +		return current_tick;
 +
 +	*mode = VCLOCK_NONE;
 +	return 0;
  }
 +#endif
  
 -int clock_getres(clockid_t, struct old_timespec32 *)
 -	__attribute__((weak, alias("__vdso_clock_getres")));
 +notrace static u64 vread_tsc(void)
 +{
 +	u64 ret = (u64)rdtsc_ordered();
 +	u64 last = gtod->cycle_last;
 +
 +	if (likely(ret >= last))
 +		return ret;
 +
 +	/*
 +	 * GCC likes to generate cmov here, but this branch is extremely
 +	 * predictable (it's just a function of time and the likely is
 +	 * very likely) and there's a data dependence, so force GCC
 +	 * to generate a branch instead.  I don't barrier() because
 +	 * we don't actually need a barrier, and if this function
 +	 * ever gets inlined it will generate worse code.
 +	 */
 +	asm volatile ("");
 +	return last;
 +}
 +
 +notrace static inline u64 vgetsns(int *mode)
 +{
 +	u64 v;
 +	cycles_t cycles;
 +
 +	if (gtod->vclock_mode == VCLOCK_TSC)
 +		cycles = vread_tsc();
 +#ifdef CONFIG_PARAVIRT_CLOCK
 +	else if (gtod->vclock_mode == VCLOCK_PVCLOCK)
 +		cycles = vread_pvclock(mode);
 +#endif
 +#ifdef CONFIG_HYPERV_TIMER
 +	else if (gtod->vclock_mode == VCLOCK_HVCLOCK)
 +		cycles = vread_hvclock(mode);
  #endif
 +	else
 +		return 0;
 +	v = cycles - gtod->cycle_last;
 +	return v * gtod->mult;
 +}
 +
 +notrace static int do_hres(clockid_t clk, struct timespec *ts)
 +{
 +	struct vgtod_ts *base = &gtod->basetime[clk];
 +	unsigned int seq;
 +	int mode;
 +	u64 ns;
 +
 +	do {
 +		seq = gtod_read_begin(gtod);
 +		mode = gtod->vclock_mode;
 +		ts->tv_sec = base->sec;
 +		ns = base->nsec;
 +		ns += vgetsns(&mode);
 +		ns >>= gtod->shift;
 +	} while (unlikely(gtod_read_retry(gtod, seq)));
 +
 +	ts->tv_sec += __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);
 +	ts->tv_nsec = ns;
 +
 +	return mode;
 +}
 +
 +notrace static void do_coarse(clockid_t clk, struct timespec *ts)
 +{
 +	struct vgtod_ts *base = &gtod->basetime[clk];
 +	unsigned int seq;
 +
 +	do {
 +		seq = gtod_read_begin(gtod);
 +		ts->tv_sec = base->sec;
 +		ts->tv_nsec = base->nsec;
 +	} while (unlikely(gtod_read_retry(gtod, seq)));
 +}
 +
 +notrace int __vdso_clock_gettime(clockid_t clock, struct timespec *ts)
 +{
 +	unsigned int msk;
 +
 +	/* Sort out negative (CPU/FD) and invalid clocks */
 +	if (unlikely((unsigned int) clock >= MAX_CLOCKS))
 +		return vdso_fallback_gettime(clock, ts);
 +
 +	/*
 +	 * Convert the clockid to a bitmask and use it to check which
 +	 * clocks are handled in the VDSO directly.
 +	 */
 +	msk = 1U << clock;
 +	if (likely(msk & VGTOD_HRES)) {
 +		if (do_hres(clock, ts) != VCLOCK_NONE)
 +			return 0;
 +	} else if (msk & VGTOD_COARSE) {
 +		do_coarse(clock, ts);
 +		return 0;
 +	}
 +	return vdso_fallback_gettime(clock, ts);
 +}
 +
 +int clock_gettime(clockid_t, struct timespec *)
 +	__attribute__((weak, alias("__vdso_clock_gettime")));
 +
 +notrace int __vdso_gettimeofday(struct timeval *tv, struct timezone *tz)
 +{
 +	if (likely(tv != NULL)) {
 +		struct timespec *ts = (struct timespec *) tv;
 +
 +		if (unlikely(do_hres(CLOCK_REALTIME, ts) == VCLOCK_NONE))
 +			return vdso_fallback_gtod(tv, tz);
 +		tv->tv_usec /= 1000;
 +	}
 +	if (unlikely(tz != NULL)) {
 +		tz->tz_minuteswest = gtod->tz_minuteswest;
 +		tz->tz_dsttime = gtod->tz_dsttime;
 +	}
 +
 +	return 0;
 +}
 +int gettimeofday(struct timeval *, struct timezone *)
 +	__attribute__((weak, alias("__vdso_gettimeofday")));
 +
 +/*
 + * This will break when the xtime seconds get inaccurate, but that is
 + * unlikely
 + */
 +notrace time_t __vdso_time(time_t *t)
 +{
 +	/* This is atomic on x86 so we don't need any locks. */
 +	time_t result = READ_ONCE(gtod->basetime[CLOCK_REALTIME].sec);
 +
 +	if (t)
 +		*t = result;
 +	return result;
 +}
 +time_t time(time_t *t)
 +	__attribute__((weak, alias("__vdso_time")));
* Unmerged path arch/x86/entry/vdso/vclock_gettime.c
diff --git a/arch/x86/entry/vsyscall/vsyscall_64.c b/arch/x86/entry/vsyscall/vsyscall_64.c
index 593db0833f6b..ad8cc6ae94af 100644
--- a/arch/x86/entry/vsyscall/vsyscall_64.c
+++ b/arch/x86/entry/vsyscall/vsyscall_64.c
@@ -173,7 +173,7 @@ bool emulate_vsyscall(struct pt_regs *regs, unsigned long address)
 		break;
 
 	case 1:
-		if (!write_ok_or_segv(regs->di, sizeof(time_t))) {
+		if (!write_ok_or_segv(regs->di, sizeof(__kernel_old_time_t))) {
 			ret = -EFAULT;
 			goto check_fault;
 		}
diff --git a/arch/x86/um/vdso/um_vdso.c b/arch/x86/um/vdso/um_vdso.c
index 8f9c6a65112a..9a4a93d0c05f 100644
--- a/arch/x86/um/vdso/um_vdso.c
+++ b/arch/x86/um/vdso/um_vdso.c
@@ -40,7 +40,7 @@ int __vdso_gettimeofday(struct timeval *tv, struct timezone *tz)
 int gettimeofday(struct timeval *, struct timezone *)
 	__attribute__((weak, alias("__vdso_gettimeofday")));
 
-time_t __vdso_time(time_t *t)
+__kernel_old_time_t __vdso_time(__kernel_old_time_t *t)
 {
 	long secs;
 
@@ -50,7 +50,7 @@ time_t __vdso_time(time_t *t)
 
 	return secs;
 }
-time_t time(time_t *t) __attribute__((weak, alias("__vdso_time")));
+__kernel_old_time_t time(__kernel_old_time_t *t) __attribute__((weak, alias("__vdso_time")));
 
 long
 __vdso_getcpu(unsigned *cpu, unsigned *node, struct getcpu_cache *unused)
diff --git a/lib/vdso/gettimeofday.c b/lib/vdso/gettimeofday.c
index b30c5801ec9b..b12b426a02bb 100644
--- a/lib/vdso/gettimeofday.c
+++ b/lib/vdso/gettimeofday.c
@@ -140,10 +140,10 @@ __cvdso_gettimeofday(struct __kernel_old_timeval *tv, struct timezone *tz)
 }
 
 #ifdef VDSO_HAS_TIME
-static __maybe_unused time_t __cvdso_time(time_t *time)
+static __maybe_unused __kernel_old_time_t __cvdso_time(__kernel_old_time_t *time)
 {
 	const struct vdso_data *vd = __arch_get_vdso_data();
-	time_t t = READ_ONCE(vd[CS_HRES_COARSE].basetime[CLOCK_REALTIME].sec);
+	__kernel_old_time_t t = READ_ONCE(vd[CS_HRES_COARSE].basetime[CLOCK_REALTIME].sec);
 
 	if (time)
 		*time = t;
