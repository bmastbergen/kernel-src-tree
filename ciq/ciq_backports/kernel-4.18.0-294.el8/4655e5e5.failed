mm,thp: recheck each page before collapsing file THP

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
Rebuild_CHGLOG: - [mm] mm, thp: recheck each page before collapsing file THP (Waiman Long) [1877019]
Rebuild_FUZZ: 99.05%
commit-author Song Liu <songliubraving@fb.com>
commit 4655e5e5f387264fd22a835bcfbe4af6691ff774
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/4655e5e5.failed

In collapse_file(), for !is_shmem case, current check cannot guarantee
the locked page is up-to-date.  Specifically, xas_unlock_irq() should
not be called before lock_page() and get_page(); and it is necessary to
recheck PageUptodate() after locking the page.

With this bug and CONFIG_READ_ONLY_THP_FOR_FS=y, madvise(HUGE)'ed .text
may contain corrupted data.  This is because khugepaged mistakenly
collapses some not up-to-date sub pages into a huge page, and assumes
the huge page is up-to-date.  This will NOT corrupt data in the disk,
because the page is read-only and never written back.  Fix this by
properly checking PageUptodate() after locking the page.  This check
replaces "VM_BUG_ON_PAGE(!PageUptodate(page), page);".

Also, move PageDirty() check after locking the page.  Current khugepaged
should not try to collapse dirty file THP, because it is limited to
read-only .text.  The only case we hit a dirty page here is when the
page hasn't been written since write.  Bail out and retry when this
happens.

syzbot reported bug on previous version of this patch.

Link: http://lkml.kernel.org/r/20191106060930.2571389-2-songliubraving@fb.com
Fixes: 99cb0dbd47a1 ("mm,thp: add read-only THP support for (non-shmem) FS")
	Signed-off-by: Song Liu <songliubraving@fb.com>
	Reported-by: syzbot+efb9e48b9fbdc49bb34a@syzkaller.appspotmail.com
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: William Kucharski <william.kucharski@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4655e5e5f387264fd22a835bcfbe4af6691ff774)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
diff --cc mm/khugepaged.c
index 34da5213cf13,a8a57bebb5fa..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1335,59 -1520,95 +1335,96 @@@ static void collapse_file(struct mm_str
  		goto out;
  	}
  
 -	/* This will be less messy when we use multi-index entries */
 -	do {
 -		xas_lock_irq(&xas);
 -		xas_create_range(&xas);
 -		if (!xas_error(&xas))
 -			break;
 -		xas_unlock_irq(&xas);
 -		if (!xas_nomem(&xas, GFP_KERNEL)) {
 -			mem_cgroup_cancel_charge(new_page, memcg, true);
 -			result = SCAN_FAIL;
 -			goto out;
 -		}
 -	} while (1);
 -
 -	__SetPageLocked(new_page);
 -	if (is_shmem)
 -		__SetPageSwapBacked(new_page);
  	new_page->index = start;
  	new_page->mapping = mapping;
 +	__SetPageSwapBacked(new_page);
 +	__SetPageLocked(new_page);
 +	BUG_ON(!page_ref_freeze(new_page, 1));
 +
  
  	/*
 -	 * At this point the new_page is locked and not up-to-date.
 -	 * It's safe to insert it into the page cache, because nobody would
 -	 * be able to map it or use it in another way until we unlock it.
 +	 * At this point the new_page is 'frozen' (page_count() is zero), locked
 +	 * and not up-to-date. It's safe to insert it into radix tree, because
 +	 * nobody would be able to map it or use it in other way until we
 +	 * unfreeze it.
  	 */
  
 -	xas_set(&xas, start);
 -	for (index = start; index < end; index++) {
 -		struct page *page = xas_next(&xas);
 +	index = start;
 +	xa_lock_irq(&mapping->i_pages);
 +	radix_tree_for_each_slot(slot, &mapping->i_pages, &iter, start) {
 +		int n = min(iter.index, end) - index;
  
 -		VM_BUG_ON(index != xas.xa_index);
 -		if (is_shmem) {
 -			if (!page) {
 -				/*
 -				 * Stop if extent has been truncated or
 -				 * hole-punched, and is now completely
 -				 * empty.
 -				 */
 -				if (index == start) {
 -					if (!xas_next_entry(&xas, end - 1)) {
 -						result = SCAN_TRUNCATED;
 -						goto xa_locked;
 -					}
 -					xas_set(&xas, index);
 -				}
 -				if (!shmem_charge(mapping->host, 1)) {
 -					result = SCAN_FAIL;
 -					goto xa_locked;
 -				}
 -				xas_store(&xas, new_page);
 -				nr_none++;
 -				continue;
 -			}
 +		/*
 +		 * Handle holes in the radix tree: charge it from shmem and
 +		 * insert relevant subpage of new_page into the radix-tree.
 +		 */
 +		if (n && !shmem_charge(mapping->host, n)) {
 +			result = SCAN_FAIL;
 +			break;
 +		}
 +		nr_none += n;
 +		for (; index < min(iter.index, end); index++) {
 +			radix_tree_insert(&mapping->i_pages, index,
 +					new_page + (index % HPAGE_PMD_NR));
 +		}
 +
++<<<<<<< HEAD
 +		/* We are done. */
 +		if (index >= end)
 +			break;
  
 +		page = radix_tree_deref_slot_protected(slot,
 +				&mapping->i_pages.xa_lock);
 +		if (xa_is_value(page) || !PageUptodate(page)) {
 +			xa_unlock_irq(&mapping->i_pages);
 +			/* swap in or instantiate fallocated page */
 +			if (shmem_getpage(mapping->host, index, &page,
 +						SGP_NOHUGE)) {
 +				result = SCAN_FAIL;
 +				goto tree_unlocked;
++=======
+ 			if (xa_is_value(page) || !PageUptodate(page)) {
+ 				xas_unlock_irq(&xas);
+ 				/* swap in or instantiate fallocated page */
+ 				if (shmem_getpage(mapping->host, index, &page,
+ 						  SGP_NOHUGE)) {
+ 					result = SCAN_FAIL;
+ 					goto xa_unlocked;
+ 				}
+ 			} else if (trylock_page(page)) {
+ 				get_page(page);
+ 				xas_unlock_irq(&xas);
+ 			} else {
+ 				result = SCAN_PAGE_LOCK;
+ 				goto xa_locked;
+ 			}
+ 		} else {	/* !is_shmem */
+ 			if (!page || xa_is_value(page)) {
+ 				xas_unlock_irq(&xas);
+ 				page_cache_sync_readahead(mapping, &file->f_ra,
+ 							  file, index,
+ 							  PAGE_SIZE);
+ 				/* drain pagevecs to help isolate_lru_page() */
+ 				lru_add_drain();
+ 				page = find_lock_page(mapping, index);
+ 				if (unlikely(page == NULL)) {
+ 					result = SCAN_FAIL;
+ 					goto xa_unlocked;
+ 				}
+ 			} else if (trylock_page(page)) {
+ 				get_page(page);
+ 				xas_unlock_irq(&xas);
+ 			} else {
+ 				result = SCAN_PAGE_LOCK;
+ 				goto xa_locked;
++>>>>>>> 4655e5e5f387 (mm,thp: recheck each page before collapsing file THP)
  			}
 +			xa_lock_irq(&mapping->i_pages);
 +		} else if (trylock_page(page)) {
 +			get_page(page);
 +		} else {
 +			result = SCAN_PAGE_LOCK;
 +			break;
  		}
  
  		/*
@@@ -1410,11 -1636,26 +1452,21 @@@
  			result = SCAN_TRUNCATED;
  			goto out_unlock;
  		}
 +		xa_unlock_irq(&mapping->i_pages);
  
+ 		if (!is_shmem && PageDirty(page)) {
+ 			/*
+ 			 * khugepaged only works on read-only fd, so this
+ 			 * page is dirty because it hasn't been flushed
+ 			 * since first write.
+ 			 */
+ 			result = SCAN_FAIL;
+ 			goto out_unlock;
+ 		}
+ 
  		if (isolate_lru_page(page)) {
  			result = SCAN_DEL_PAGE_LRU;
 -			goto out_unlock;
 -		}
 -
 -		if (page_has_private(page) &&
 -		    !try_to_release_page(page, GFP_KERNEL)) {
 -			result = SCAN_PAGE_HAS_PRIVATE;
 -			goto out_unlock;
 +			goto out_isolate_failed;
  		}
  
  		if (page_mapped(page))
* Unmerged path mm/khugepaged.c
