KVM: nSVM: implement on demand allocation of the nested state

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Maxim Levitsky <mlevitsk@redhat.com>
commit 2fcf4876ada8a293d3b92a1033b8b990a7c613d3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/2fcf4876.failed

This way we don't waste memory on VMs which don't use nesting
virtualization even when the host enabled it for them.

	Signed-off-by: Maxim Levitsky <mlevitsk@redhat.com>
Message-Id: <20201001112954.6258-5-mlevitsk@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 2fcf4876ada8a293d3b92a1033b8b990a7c613d3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/nested.c
#	arch/x86/kvm/svm/svm.c
diff --cc arch/x86/kvm/svm/nested.c
index a41617fb2750,9e4c226dbf7d..000000000000
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@@ -463,13 -479,16 +463,24 @@@ int nested_svm_vmrun(struct vcpu_svm *s
  
  	ret = kvm_skip_emulated_instruction(&svm->vcpu);
  
 -	vmcb12 = map.hva;
 +	nested_vmcb = map.hva;
  
++<<<<<<< HEAD
 +	if (!nested_vmcb_checks(nested_vmcb)) {
 +		nested_vmcb->control.exit_code    = SVM_EXIT_ERR;
 +		nested_vmcb->control.exit_code_hi = 0;
 +		nested_vmcb->control.exit_info_1  = 0;
 +		nested_vmcb->control.exit_info_2  = 0;
++=======
+ 	if (WARN_ON_ONCE(!svm->nested.initialized))
+ 		return -EINVAL;
+ 
+ 	if (!nested_vmcb_checks(svm, vmcb12)) {
+ 		vmcb12->control.exit_code    = SVM_EXIT_ERR;
+ 		vmcb12->control.exit_code_hi = 0;
+ 		vmcb12->control.exit_info_1  = 0;
+ 		vmcb12->control.exit_info_2  = 0;
++>>>>>>> 2fcf4876ada8 (KVM: nSVM: implement on demand allocation of the nested state)
  		goto out;
  	}
  
diff --cc arch/x86/kvm/svm/svm.c
index 9f8427c36daa,dc4fe579d460..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -610,11 -662,17 +629,21 @@@ static void set_msr_interception(u32 *m
  	msrpm[offset] = tmp;
  }
  
 -static void set_msr_interception(struct kvm_vcpu *vcpu, u32 *msrpm, u32 msr,
 -				 int read, int write)
 +static u32 *svm_vcpu_init_msrpm(void)
  {
++<<<<<<< HEAD
 +	int i;
++=======
+ 	set_shadow_msr_intercept(vcpu, msr, read, write);
+ 	set_msr_interception_bitmap(vcpu, msrpm, msr, read, write);
+ }
+ 
+ u32 *svm_vcpu_alloc_msrpm(void)
+ {
+ 	struct page *pages = alloc_pages(GFP_KERNEL_ACCOUNT, MSRPM_ALLOC_ORDER);
++>>>>>>> 2fcf4876ada8 (KVM: nSVM: implement on demand allocation of the nested state)
  	u32 *msrpm;
 +	struct page *pages = alloc_pages(GFP_KERNEL_ACCOUNT, MSRPM_ALLOC_ORDER);
  
  	if (!pages)
  		return NULL;
@@@ -622,15 -680,22 +651,26 @@@
  	msrpm = page_address(pages);
  	memset(msrpm, 0xff, PAGE_SIZE * (1 << MSRPM_ALLOC_ORDER));
  
++<<<<<<< HEAD
++=======
+ 	return msrpm;
+ }
+ 
+ void svm_vcpu_init_msrpm(struct kvm_vcpu *vcpu, u32 *msrpm)
+ {
+ 	int i;
+ 
++>>>>>>> 2fcf4876ada8 (KVM: nSVM: implement on demand allocation of the nested state)
  	for (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++) {
  		if (!direct_access_msrs[i].always)
  			continue;
 -		set_msr_interception(vcpu, msrpm, direct_access_msrs[i].index, 1, 1);
 +		set_msr_interception(msrpm, direct_access_msrs[i].index, 1, 1);
  	}
 +	return msrpm;
  }
  
- static void svm_vcpu_free_msrpm(u32 *msrpm)
+ 
+ void svm_vcpu_free_msrpm(u32 *msrpm)
  {
  	__free_pages(virt_to_page(msrpm), MSRPM_ALLOC_ORDER);
  }
@@@ -1217,15 -1308,11 +1252,23 @@@ static int svm_create_vcpu(struct kvm_v
  	if (irqchip_in_kernel(vcpu->kvm) && kvm_apicv_activated(vcpu->kvm))
  		svm->avic_is_running = true;
  
++<<<<<<< HEAD
 +	svm->nested.hsave = page_address(hsave_page);
 +
 +	svm->msrpm = svm_vcpu_init_msrpm();
++=======
+ 	svm->msrpm = svm_vcpu_alloc_msrpm();
++>>>>>>> 2fcf4876ada8 (KVM: nSVM: implement on demand allocation of the nested state)
  	if (!svm->msrpm)
- 		goto error_free_hsave_page;
+ 		goto error_free_vmcb_page;
  
++<<<<<<< HEAD
 +	svm->nested.msrpm = svm_vcpu_init_msrpm();
 +	if (!svm->nested.msrpm)
 +		goto error_free_msrpm;
++=======
+ 	svm_vcpu_init_msrpm(vcpu, svm->msrpm);
++>>>>>>> 2fcf4876ada8 (KVM: nSVM: implement on demand allocation of the nested state)
  
  	svm->vmcb = page_address(vmcb_page);
  	svm->vmcb_pa = __sme_set(page_to_pfn(vmcb_page) << PAGE_SHIFT);
@@@ -3931,10 -4037,13 +3970,17 @@@ static int svm_pre_leave_smm(struct kvm
  				return 1;
  
  			if (kvm_vcpu_map(&svm->vcpu,
 -					 gpa_to_gfn(vmcb12_gpa), &map) == -EINVAL)
 +					 gpa_to_gfn(vmcb), &map) == -EINVAL)
  				return 1;
  
++<<<<<<< HEAD
 +			ret = enter_svm_guest_mode(svm, vmcb, map.hva);
++=======
+ 			if (svm_allocate_nested(svm))
+ 				return 1;
+ 
+ 			ret = enter_svm_guest_mode(svm, vmcb12_gpa, map.hva);
++>>>>>>> 2fcf4876ada8 (KVM: nSVM: implement on demand allocation of the nested state)
  			kvm_vcpu_unmap(&svm->vcpu, &map, true);
  		}
  	}
* Unmerged path arch/x86/kvm/svm/nested.c
* Unmerged path arch/x86/kvm/svm/svm.c
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index d8ef52cb7879..657d1c18a7ed 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -97,6 +97,8 @@ struct svm_nested_state {
 
 	/* cache for control fields of the guest */
 	struct vmcb_control_area ctl;
+
+	bool initialized;
 };
 
 struct vcpu_svm {
@@ -349,6 +351,10 @@ static inline bool gif_set(struct vcpu_svm *svm)
 #define MSR_INVALID			0xffffffffU
 
 u32 svm_msrpm_offset(u32 msr);
+u32 *svm_vcpu_alloc_msrpm(void);
+void svm_vcpu_init_msrpm(struct kvm_vcpu *vcpu, u32 *msrpm);
+void svm_vcpu_free_msrpm(u32 *msrpm);
+
 int svm_set_efer(struct kvm_vcpu *vcpu, u64 efer);
 void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 int svm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
@@ -390,6 +396,8 @@ static inline bool nested_exit_on_nmi(struct vcpu_svm *svm)
 int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb_gpa,
 			 struct vmcb *nested_vmcb);
 void svm_leave_nested(struct vcpu_svm *svm);
+void svm_free_nested(struct vcpu_svm *svm);
+int svm_allocate_nested(struct vcpu_svm *svm);
 int nested_svm_vmrun(struct vcpu_svm *svm);
 void nested_svm_vmloadsave(struct vmcb *from_vmcb, struct vmcb *to_vmcb);
 int nested_svm_vmexit(struct vcpu_svm *svm);
