x86/vdso: Prevent segfaults due to hoisted vclock reads

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Andy Lutomirski <luto@kernel.org>
commit ff17bbe0bb405ad8b36e55815d381841f9fdeebc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/ff17bbe0.failed

GCC 5.5.0 sometimes cleverly hoists reads of the pvclock and/or hvclock
pages before the vclock mode checks.  This creates a path through
vclock_gettime() in which no vclock is enabled at all (due to disabled
TSC on old CPUs, for example) but the pvclock or hvclock page
nevertheless read.  This will segfault on bare metal.

This fixes commit 459e3a21535a ("gcc-9: properly declare the
{pv,hv}clock_page storage") in the sense that, before that commit, GCC
didn't seem to generate the offending code.  There was nothing wrong
with that commit per se, and -stable maintainers should backport this to
all supported kernels regardless of whether the offending commit was
present, since the same crash could just as easily be triggered by the
phase of the moon.

On GCC 9.1.1, this doesn't seem to affect the generated code at all, so
I'm not too concerned about performance regressions from this fix.

	Cc: stable@vger.kernel.org
	Cc: x86@kernel.org
	Cc: Borislav Petkov <bp@alien8.de>
	Reported-by: Duncan Roe <duncan_roe@optusnet.com.au>
	Signed-off-by: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ff17bbe0bb405ad8b36e55815d381841f9fdeebc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/vdso/vclock_gettime.c
diff --cc arch/x86/entry/vdso/vclock_gettime.c
index 23aabf6cd315,4aed41f638bb..000000000000
--- a/arch/x86/entry/vdso/vclock_gettime.c
+++ b/arch/x86/entry/vdso/vclock_gettime.c
@@@ -164,45 -124,30 +164,68 @@@ static notrace u64 vread_hvclock(int *m
  }
  #endif
  
 -notrace static inline u64 vgetcyc(int mode)
 +notrace static u64 vread_tsc(void)
 +{
++<<<<<<< HEAD
 +	u64 ret = (u64)rdtsc_ordered();
 +	u64 last = gtod->cycle_last;
 +
 +	if (likely(ret >= last))
 +		return ret;
 +
 +	/*
 +	 * GCC likes to generate cmov here, but this branch is extremely
 +	 * predictable (it's just a function of time and the likely is
 +	 * very likely) and there's a data dependence, so force GCC
 +	 * to generate a branch instead.  I don't barrier() because
 +	 * we don't actually need a barrier, and if this function
 +	 * ever gets inlined it will generate worse code.
 +	 */
 +	asm volatile ("");
 +	return last;
 +}
 +
 +notrace static inline u64 vgetsns(int *mode)
  {
 +	u64 v;
 +	cycles_t cycles;
 +
 +	if (gtod->vclock_mode == VCLOCK_TSC)
 +		cycles = vread_tsc();
 +#ifdef CONFIG_PARAVIRT_CLOCK
 +	else if (gtod->vclock_mode == VCLOCK_PVCLOCK)
 +		cycles = vread_pvclock(mode);
 +#endif
 +#ifdef CONFIG_HYPERV_TIMER
 +	else if (gtod->vclock_mode == VCLOCK_HVCLOCK)
 +		cycles = vread_hvclock(mode);
++=======
+ 	if (mode == VCLOCK_TSC)
+ 		return (u64)rdtsc_ordered();
+ 
+ 	/*
+ 	 * For any memory-mapped vclock type, we need to make sure that gcc
+ 	 * doesn't cleverly hoist a load before the mode check.  Otherwise we
+ 	 * might end up touching the memory-mapped page even if the vclock in
+ 	 * question isn't enabled, which will segfault.  Hence the barriers.
+ 	 */
+ #ifdef CONFIG_PARAVIRT_CLOCK
+ 	if (mode == VCLOCK_PVCLOCK) {
+ 		barrier();
+ 		return vread_pvclock();
+ 	}
+ #endif
+ #ifdef CONFIG_HYPERV_TSCPAGE
+ 	if (mode == VCLOCK_HVCLOCK) {
+ 		barrier();
+ 		return vread_hvclock();
+ 	}
++>>>>>>> ff17bbe0bb40 (x86/vdso: Prevent segfaults due to hoisted vclock reads)
  #endif
 -	return U64_MAX;
 +	else
 +		return 0;
 +	v = cycles - gtod->cycle_last;
 +	return v * gtod->mult;
  }
  
  notrace static int do_hres(clockid_t clk, struct timespec *ts)
* Unmerged path arch/x86/entry/vdso/vclock_gettime.c
