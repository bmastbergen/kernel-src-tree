x86/mmu: Allocate/free a PASID

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Fenghua Yu <fenghua.yu@intel.com>
commit 20f0afd1fb3d7d44f4a3db5a4b6e904410862140
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/20f0afd1.failed

A PASID is allocated for an "mm" the first time any thread binds to an
SVA-capable device and is freed from the "mm" when the SVA is unbound
by the last thread. It's possible for the "mm" to have different PASID
values in different binding/unbinding SVA cycles.

The mm's PASID (non-zero for valid PASID or 0 for invalid PASID) is
propagated to a per-thread PASID MSR for all threads within the mm
through IPI, context switch, or inherited. This is done to ensure that a
running thread has the right PASID in the MSR matching the mm's PASID.

 [ bp: s/SVM/SVA/g; massage. ]

	Suggested-by: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Tony Luck <tony.luck@intel.com>
Link: https://lkml.kernel.org/r/1600187413-163670-10-git-send-email-fenghua.yu@intel.com
(cherry picked from commit 20f0afd1fb3d7d44f4a3db5a4b6e904410862140)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/xstate.c
diff --cc arch/x86/kernel/fpu/xstate.c
index 59e66b3b5b95,5d8047441a0a..000000000000
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@@ -1251,3 -1230,232 +1251,235 @@@ int copy_user_to_xstate(struct xregs_st
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Save only supervisor states to the kernel buffer.  This blows away all
+  * old states, and is intended to be used only in __fpu__restore_sig(), where
+  * user states are restored from the user buffer.
+  */
+ void copy_supervisor_to_kernel(struct xregs_state *xstate)
+ {
+ 	struct xstate_header *header;
+ 	u64 max_bit, min_bit;
+ 	u32 lmask, hmask;
+ 	int err, i;
+ 
+ 	if (WARN_ON(!boot_cpu_has(X86_FEATURE_XSAVES)))
+ 		return;
+ 
+ 	if (!xfeatures_mask_supervisor())
+ 		return;
+ 
+ 	max_bit = __fls(xfeatures_mask_supervisor());
+ 	min_bit = __ffs(xfeatures_mask_supervisor());
+ 
+ 	lmask = xfeatures_mask_supervisor();
+ 	hmask = xfeatures_mask_supervisor() >> 32;
+ 	XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
+ 
+ 	/* We should never fault when copying to a kernel buffer: */
+ 	if (WARN_ON_FPU(err))
+ 		return;
+ 
+ 	/*
+ 	 * At this point, the buffer has only supervisor states and must be
+ 	 * converted back to normal kernel format.
+ 	 */
+ 	header = &xstate->header;
+ 	header->xcomp_bv |= xfeatures_mask_all;
+ 
+ 	/*
+ 	 * This only moves states up in the buffer.  Start with
+ 	 * the last state and move backwards so that states are
+ 	 * not overwritten until after they are moved.  Note:
+ 	 * memmove() allows overlapping src/dst buffers.
+ 	 */
+ 	for (i = max_bit; i >= min_bit; i--) {
+ 		u8 *xbuf = (u8 *)xstate;
+ 
+ 		if (!((header->xfeatures >> i) & 1))
+ 			continue;
+ 
+ 		/* Move xfeature 'i' into its normal location */
+ 		memmove(xbuf + xstate_comp_offsets[i],
+ 			xbuf + xstate_supervisor_only_offsets[i],
+ 			xstate_sizes[i]);
+ 	}
+ }
+ 
+ /**
+  * copy_dynamic_supervisor_to_kernel() - Save dynamic supervisor states to
+  *                                       an xsave area
+  * @xstate: A pointer to an xsave area
+  * @mask: Represent the dynamic supervisor features saved into the xsave area
+  *
+  * Only the dynamic supervisor states sets in the mask are saved into the xsave
+  * area (See the comment in XFEATURE_MASK_DYNAMIC for the details of dynamic
+  * supervisor feature). Besides the dynamic supervisor states, the legacy
+  * region and XSAVE header are also saved into the xsave area. The supervisor
+  * features in the XFEATURE_MASK_SUPERVISOR_SUPPORTED and
+  * XFEATURE_MASK_SUPERVISOR_UNSUPPORTED are not saved.
+  *
+  * The xsave area must be 64-bytes aligned.
+  */
+ void copy_dynamic_supervisor_to_kernel(struct xregs_state *xstate, u64 mask)
+ {
+ 	u64 dynamic_mask = xfeatures_mask_dynamic() & mask;
+ 	u32 lmask, hmask;
+ 	int err;
+ 
+ 	if (WARN_ON_FPU(!boot_cpu_has(X86_FEATURE_XSAVES)))
+ 		return;
+ 
+ 	if (WARN_ON_FPU(!dynamic_mask))
+ 		return;
+ 
+ 	lmask = dynamic_mask;
+ 	hmask = dynamic_mask >> 32;
+ 
+ 	XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
+ 
+ 	/* Should never fault when copying to a kernel buffer */
+ 	WARN_ON_FPU(err);
+ }
+ 
+ /**
+  * copy_kernel_to_dynamic_supervisor() - Restore dynamic supervisor states from
+  *                                       an xsave area
+  * @xstate: A pointer to an xsave area
+  * @mask: Represent the dynamic supervisor features restored from the xsave area
+  *
+  * Only the dynamic supervisor states sets in the mask are restored from the
+  * xsave area (See the comment in XFEATURE_MASK_DYNAMIC for the details of
+  * dynamic supervisor feature). Besides the dynamic supervisor states, the
+  * legacy region and XSAVE header are also restored from the xsave area. The
+  * supervisor features in the XFEATURE_MASK_SUPERVISOR_SUPPORTED and
+  * XFEATURE_MASK_SUPERVISOR_UNSUPPORTED are not restored.
+  *
+  * The xsave area must be 64-bytes aligned.
+  */
+ void copy_kernel_to_dynamic_supervisor(struct xregs_state *xstate, u64 mask)
+ {
+ 	u64 dynamic_mask = xfeatures_mask_dynamic() & mask;
+ 	u32 lmask, hmask;
+ 	int err;
+ 
+ 	if (WARN_ON_FPU(!boot_cpu_has(X86_FEATURE_XSAVES)))
+ 		return;
+ 
+ 	if (WARN_ON_FPU(!dynamic_mask))
+ 		return;
+ 
+ 	lmask = dynamic_mask;
+ 	hmask = dynamic_mask >> 32;
+ 
+ 	XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
+ 
+ 	/* Should never fault when copying from a kernel buffer */
+ 	WARN_ON_FPU(err);
+ }
+ 
+ #ifdef CONFIG_PROC_PID_ARCH_STATUS
+ /*
+  * Report the amount of time elapsed in millisecond since last AVX512
+  * use in the task.
+  */
+ static void avx512_status(struct seq_file *m, struct task_struct *task)
+ {
+ 	unsigned long timestamp = READ_ONCE(task->thread.fpu.avx512_timestamp);
+ 	long delta;
+ 
+ 	if (!timestamp) {
+ 		/*
+ 		 * Report -1 if no AVX512 usage
+ 		 */
+ 		delta = -1;
+ 	} else {
+ 		delta = (long)(jiffies - timestamp);
+ 		/*
+ 		 * Cap to LONG_MAX if time difference > LONG_MAX
+ 		 */
+ 		if (delta < 0)
+ 			delta = LONG_MAX;
+ 		delta = jiffies_to_msecs(delta);
+ 	}
+ 
+ 	seq_put_decimal_ll(m, "AVX512_elapsed_ms:\t", delta);
+ 	seq_putc(m, '\n');
+ }
+ 
+ /*
+  * Report architecture specific information
+  */
+ int proc_pid_arch_status(struct seq_file *m, struct pid_namespace *ns,
+ 			struct pid *pid, struct task_struct *task)
+ {
+ 	/*
+ 	 * Report AVX512 state if the processor and build option supported.
+ 	 */
+ 	if (cpu_feature_enabled(X86_FEATURE_AVX512F))
+ 		avx512_status(m, task);
+ 
+ 	return 0;
+ }
+ #endif /* CONFIG_PROC_PID_ARCH_STATUS */
+ 
+ #ifdef CONFIG_IOMMU_SUPPORT
+ void update_pasid(void)
+ {
+ 	u64 pasid_state;
+ 	u32 pasid;
+ 
+ 	if (!cpu_feature_enabled(X86_FEATURE_ENQCMD))
+ 		return;
+ 
+ 	if (!current->mm)
+ 		return;
+ 
+ 	pasid = READ_ONCE(current->mm->pasid);
+ 	/* Set the valid bit in the PASID MSR/state only for valid pasid. */
+ 	pasid_state = pasid == PASID_DISABLED ?
+ 		      pasid : pasid | MSR_IA32_PASID_VALID;
+ 
+ 	/*
+ 	 * No need to hold fregs_lock() since the task's fpstate won't
+ 	 * be changed by others (e.g. ptrace) while the task is being
+ 	 * switched to or is in IPI.
+ 	 */
+ 	if (!test_thread_flag(TIF_NEED_FPU_LOAD)) {
+ 		/* The MSR is active and can be directly updated. */
+ 		wrmsrl(MSR_IA32_PASID, pasid_state);
+ 	} else {
+ 		struct fpu *fpu = &current->thread.fpu;
+ 		struct ia32_pasid_state *ppasid_state;
+ 		struct xregs_state *xsave;
+ 
+ 		/*
+ 		 * The CPU's xstate registers are not currently active. Just
+ 		 * update the PASID state in the memory buffer here. The
+ 		 * PASID MSR will be loaded when returning to user mode.
+ 		 */
+ 		xsave = &fpu->state.xsave;
+ 		xsave->header.xfeatures |= XFEATURE_MASK_PASID;
+ 		ppasid_state = get_xsave_addr(xsave, XFEATURE_PASID);
+ 		/*
+ 		 * Since XFEATURE_MASK_PASID is set in xfeatures, ppasid_state
+ 		 * won't be NULL and no need to check its value.
+ 		 *
+ 		 * Only update the task's PASID state when it's different
+ 		 * from the mm's pasid.
+ 		 */
+ 		if (ppasid_state->pasid != pasid_state) {
+ 			/*
+ 			 * Invalid fpregs so that state restoring will pick up
+ 			 * the PASID state.
+ 			 */
+ 			__fpu_invalidate_fpregs_state(fpu);
+ 			ppasid_state->pasid = pasid_state;
+ 		}
+ 	}
+ }
+ #endif /* CONFIG_IOMMU_SUPPORT */
++>>>>>>> 20f0afd1fb3d (x86/mmu: Allocate/free a PASID)
diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
index 04a438ef41cb..e73e6267218f 100644
--- a/arch/x86/include/asm/fpu/api.h
+++ b/arch/x86/include/asm/fpu/api.h
@@ -67,4 +67,16 @@ extern void switch_fpu_return(void);
  */
 extern int cpu_has_xfeatures(u64 xfeatures_mask, const char **feature_name);
 
+/*
+ * Tasks that are not using SVA have mm->pasid set to zero to note that they
+ * will not have the valid bit set in MSR_IA32_PASID while they are running.
+ */
+#define PASID_DISABLED	0
+
+#ifdef CONFIG_IOMMU_SUPPORT
+/* Update current's PASID MSR/state by mm's PASID. */
+void update_pasid(void);
+#else
+static inline void update_pasid(void) { }
+#endif
 #endif /* _ASM_X86_FPU_API_H */
diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index b93475333997..800cd0ba7114 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -646,6 +646,13 @@ static inline void switch_fpu_finish(struct fpu *new_fpu)
 			pkru_val = pk->pkru;
 	}
 	__write_pkru(pkru_val);
+
+	/*
+	 * Expensive PASID MSR write will be avoided in update_pasid() because
+	 * TIF_NEED_FPU_LOAD was set. And the PASID state won't be updated
+	 * unless it's different from mm->pasid to reduce overhead.
+	 */
+	update_pasid();
 }
 
 /*
* Unmerged path arch/x86/kernel/fpu/xstate.c
diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c
index 969ee9183c39..a17b24f0899b 100644
--- a/drivers/iommu/intel-svm.c
+++ b/drivers/iommu/intel-svm.c
@@ -27,6 +27,7 @@
 #include <linux/mm_types.h>
 #include <linux/ioasid.h>
 #include <asm/page.h>
+#include <asm/fpu/api.h>
 
 #include "intel-pasid.h"
 
@@ -458,6 +459,24 @@ int intel_svm_unbind_gpasid(struct device *dev, int pasid)
 	return ret;
 }
 
+static void _load_pasid(void *unused)
+{
+	update_pasid();
+}
+
+static void load_pasid(struct mm_struct *mm, u32 pasid)
+{
+	mutex_lock(&mm->context.lock);
+
+	/* Synchronize with READ_ONCE in update_pasid(). */
+	smp_store_release(&mm->pasid, pasid);
+
+	/* Update PASID MSR on all CPUs running the mm's tasks. */
+	on_each_cpu_mask(mm_cpumask(mm), _load_pasid, NULL, true);
+
+	mutex_unlock(&mm->context.lock);
+}
+
 /* Caller must hold pasid_mutex, mm reference */
 static int
 intel_svm_bind_mm(struct device *dev, int flags, struct svm_dev_ops *ops,
@@ -604,6 +623,10 @@ intel_svm_bind_mm(struct device *dev, int flags, struct svm_dev_ops *ops,
 		}
 
 		list_add_tail(&svm->list, &global_svm_list);
+		if (mm) {
+			/* The newly allocated pasid is loaded to the mm. */
+			load_pasid(mm, svm->pasid);
+		}
 	} else {
 		/*
 		 * Binding a new device with existing PASID, need to setup
@@ -667,8 +690,11 @@ static int intel_svm_unbind_mm(struct device *dev, int pasid)
 
 			if (list_empty(&svm->devs)) {
 				ioasid_free(svm->pasid);
-				if (svm->mm)
+				if (svm->mm) {
 					mmu_notifier_unregister(&svm->notifier, svm->mm);
+					/* Clear mm's pasid. */
+					load_pasid(svm->mm, PASID_DISABLED);
+				}
 				list_del(&svm->list);
 				/* We mandate that no page faults may be outstanding
 				 * for the PASID when intel_svm_unbind_mm() is called.
