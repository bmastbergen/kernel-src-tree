RDMA/mlx5: Reduce amount of duplication in QP destroy

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Leon Romanovsky <leon@kernel.org>
commit 747c519cdbe4a3f6a616d50c19bcb97413abe384
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/747c519c.failed

Delete both PD argument and checks if udata was provided, in favour
of unified destroy QP functions.

Link: https://lore.kernel.org/r/20200427154636.381474-30-leon@kernel.org
	Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 747c519cdbe4a3f6a616d50c19bcb97413abe384)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/qp.c
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 08f1eef60c1f,6b390b0e43af..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -1209,24 -1213,10 +1220,11 @@@ err_buf
  	return err;
  }
  
- static void destroy_qp_kernel(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp)
- {
- 	kvfree(qp->sq.wqe_head);
- 	kvfree(qp->sq.w_list);
- 	kvfree(qp->sq.wrid);
- 	kvfree(qp->sq.wr_data);
- 	kvfree(qp->rq.wrid);
- 	if (qp->db.db)
- 		mlx5_db_free(dev->mdev, &qp->db);
- 	if (qp->buf.frags)
- 		mlx5_frag_buf_free(dev->mdev, &qp->buf);
- }
- 
  static u32 get_rx_type(struct mlx5_ib_qp *qp, struct ib_qp_init_attr *attr)
  {
 -	if (attr->srq || (qp->type == IB_QPT_XRC_TGT) ||
 -	    (qp->type == MLX5_IB_QPT_DCI) || (qp->type == IB_QPT_XRC_INI))
 +	if (attr->srq || (attr->qp_type == IB_QPT_XRC_TGT) ||
 +	    (qp->qp_sub_type == MLX5_IB_QPT_DCI) ||
 +	    (attr->qp_type == IB_QPT_XRC_INI))
  		return MLX5_SRQ_RQ;
  	else if (!qp->has_rq)
  		return MLX5_ZERO_LEN_RQ;
@@@ -1960,14 -1909,86 +1958,85 @@@ static int get_atomic_mode(struct mlx5_
  	return atomic_mode;
  }
  
 -static int create_xrc_tgt_qp(struct mlx5_ib_dev *dev,
 -			     struct ib_qp_init_attr *attr,
 -			     struct mlx5_ib_qp *qp, struct ib_udata *udata,
 -			     u32 uidx)
 +static inline bool check_flags_mask(uint64_t input, uint64_t supported)
  {
++<<<<<<< HEAD
 +	return (input & ~supported) == 0;
++=======
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_qp_base *base;
+ 	unsigned long flags;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	mutex_init(&qp->mutex);
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	in = kvzalloc(inlen, GFP_KERNEL);
+ 	if (!in)
+ 		return -ENOMEM;
+ 
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, MLX5_QP_ST_XRC);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 	MLX5_SET(qpc, qpc, pd, to_mpd(devr->p0)->pdn);
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 	if (qp->flags & IB_QP_CREATE_CROSS_CHANNEL)
+ 		MLX5_SET(qpc, qpc, cd_master, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_SEND)
+ 		MLX5_SET(qpc, qpc, cd_slave_send, 1);
+ 	if (qp->flags & IB_QP_CREATE_MANAGED_RECV)
+ 		MLX5_SET(qpc, qpc, cd_slave_receive, 1);
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, MLX5_SRQ_RQ);
+ 	MLX5_SET(qpc, qpc, no_sq, 1);
+ 	MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, cqn_snd, to_mcq(devr->c0)->mcq.cqn);
+ 	MLX5_SET(qpc, qpc, srqn_rmpn_xrqn, to_msrq(devr->s0)->msrq.srqn);
+ 	MLX5_SET(qpc, qpc, xrcd, to_mxrcd(attr->xrcd)->xrcdn);
+ 	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
+ 
+ 	/* 0xffffff means we ask to work with cqe version 0 */
+ 	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
+ 		MLX5_SET(qpc, qpc, user_index, uidx);
+ 
+ 	if (qp->flags & IB_QP_CREATE_PCI_WRITE_END_PADDING) {
+ 		MLX5_SET(qpc, qpc, end_padding_mode,
+ 			 MLX5_WQ_END_PAD_MODE_ALIGN);
+ 		/* Special case to clean flag */
+ 		qp->flags &= ~IB_QP_CREATE_PCI_WRITE_END_PADDING;
+ 	}
+ 
+ 	base = &qp->trans_qp.base;
+ 	err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
+ 	kvfree(in);
+ 	if (err) {
+ 		destroy_qp(dev, qp, base, udata);
+ 		return err;
+ 	}
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	return 0;
++>>>>>>> 747c519cdbe4 (RDMA/mlx5: Reduce amount of duplication in QP destroy)
  }
  
 -static int create_user_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 -			  struct ib_qp_init_attr *init_attr,
 -			  struct mlx5_ib_create_qp *ucmd,
 -			  struct ib_udata *udata, struct mlx5_ib_qp *qp,
 -			  u32 uidx)
 +static int create_qp_common(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 +			    struct ib_qp_init_attr *init_attr,
 +			    struct ib_udata *udata, struct mlx5_ib_qp *qp)
  {
  	struct mlx5_ib_resources *devr = &dev->devr;
  	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
@@@ -2344,11 -2168,134 +2413,142 @@@
  	return 0;
  
  err_create:
++<<<<<<< HEAD
 +	if (udata)
 +		destroy_qp_user(dev, pd, qp, base, udata);
 +	else
 +		destroy_qp_kernel(dev, qp);
 +	kvfree(in);
++=======
+ 	destroy_qp(dev, qp, base, udata);
+ 	return err;
+ }
+ 
+ static int create_kernel_qp(struct mlx5_ib_dev *dev, struct ib_pd *pd,
+ 			    struct ib_qp_init_attr *attr, struct mlx5_ib_qp *qp,
+ 			    u32 uidx)
+ {
+ 	struct mlx5_ib_resources *devr = &dev->devr;
+ 	int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_cq *send_cq;
+ 	struct mlx5_ib_cq *recv_cq;
+ 	unsigned long flags;
+ 	struct mlx5_ib_qp_base *base;
+ 	int mlx5_st;
+ 	void *qpc;
+ 	u32 *in;
+ 	int err;
+ 
+ 	mutex_init(&qp->mutex);
+ 	spin_lock_init(&qp->sq.lock);
+ 	spin_lock_init(&qp->rq.lock);
+ 
+ 	mlx5_st = to_mlx5_st(qp->type);
+ 	if (mlx5_st < 0)
+ 		return -EINVAL;
+ 
+ 	if (attr->sq_sig_type == IB_SIGNAL_ALL_WR)
+ 		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+ 
+ 	base = &qp->trans_qp.base;
+ 
+ 	qp->has_rq = qp_has_rq(attr);
+ 	err = set_rq_size(dev, &attr->cap, qp->has_rq, qp, NULL);
+ 	if (err) {
+ 		mlx5_ib_dbg(dev, "err %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	err = _create_kernel_qp(dev, attr, qp, &in, &inlen, base);
+ 	if (err)
+ 		return err;
+ 
+ 	if (is_sqp(attr->qp_type))
+ 		qp->port = attr->port_num;
+ 
+ 	qpc = MLX5_ADDR_OF(create_qp_in, in, qpc);
+ 
+ 	MLX5_SET(qpc, qpc, st, mlx5_st);
+ 	MLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);
+ 
+ 	if (attr->qp_type != MLX5_IB_QPT_REG_UMR)
+ 		MLX5_SET(qpc, qpc, pd, to_mpd(pd ? pd : devr->p0)->pdn);
+ 	else
+ 		MLX5_SET(qpc, qpc, latency_sensitive, 1);
+ 
+ 
+ 	if (qp->flags & IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK)
+ 		MLX5_SET(qpc, qpc, block_lb_mc, 1);
+ 
+ 	if (qp->rq.wqe_cnt) {
+ 		MLX5_SET(qpc, qpc, log_rq_stride, qp->rq.wqe_shift - 4);
+ 		MLX5_SET(qpc, qpc, log_rq_size, ilog2(qp->rq.wqe_cnt));
+ 	}
+ 
+ 	MLX5_SET(qpc, qpc, rq_type, get_rx_type(qp, attr));
+ 
+ 	if (qp->sq.wqe_cnt)
+ 		MLX5_SET(qpc, qpc, log_sq_size, ilog2(qp->sq.wqe_cnt));
+ 	else
+ 		MLX5_SET(qpc, qpc, no_sq, 1);
+ 
+ 	if (attr->srq) {
+ 		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x0)->xrcdn);
+ 		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
+ 			 to_msrq(attr->srq)->msrq.srqn);
+ 	} else {
+ 		MLX5_SET(qpc, qpc, xrcd, to_mxrcd(devr->x1)->xrcdn);
+ 		MLX5_SET(qpc, qpc, srqn_rmpn_xrqn,
+ 			 to_msrq(devr->s1)->msrq.srqn);
+ 	}
+ 
+ 	if (attr->send_cq)
+ 		MLX5_SET(qpc, qpc, cqn_snd, to_mcq(attr->send_cq)->mcq.cqn);
+ 
+ 	if (attr->recv_cq)
+ 		MLX5_SET(qpc, qpc, cqn_rcv, to_mcq(attr->recv_cq)->mcq.cqn);
+ 
+ 	MLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);
+ 
+ 	/* 0xffffff means we ask to work with cqe version 0 */
+ 	if (MLX5_CAP_GEN(mdev, cqe_version) == MLX5_CQE_VERSION_V1)
+ 		MLX5_SET(qpc, qpc, user_index, uidx);
+ 
+ 	/* we use IB_QP_CREATE_IPOIB_UD_LSO to indicates ipoib qp */
+ 	if (qp->flags & IB_QP_CREATE_IPOIB_UD_LSO)
+ 		MLX5_SET(qpc, qpc, ulp_stateless_offload_mode, 1);
+ 
+ 	err = mlx5_core_create_qp(dev, &base->mqp, in, inlen);
+ 	kvfree(in);
+ 	if (err)
+ 		goto err_create;
+ 
+ 	base->container_mibqp = qp;
+ 	base->mqp.event = mlx5_ib_qp_event;
+ 
+ 	get_cqs(qp->type, attr->send_cq, attr->recv_cq,
+ 		&send_cq, &recv_cq);
+ 	spin_lock_irqsave(&dev->reset_flow_resource_lock, flags);
+ 	mlx5_ib_lock_cqs(send_cq, recv_cq);
+ 	/* Maintain device to QPs access, needed for further handling via reset
+ 	 * flow
+ 	 */
+ 	list_add_tail(&qp->qps_list, &dev->qp_list);
+ 	/* Maintain CQ to QPs access, needed for further handling via reset flow
+ 	 */
+ 	if (send_cq)
+ 		list_add_tail(&qp->cq_send_list, &send_cq->list_send_qp);
+ 	if (recv_cq)
+ 		list_add_tail(&qp->cq_recv_list, &recv_cq->list_recv_qp);
+ 	mlx5_ib_unlock_cqs(send_cq, recv_cq);
+ 	spin_unlock_irqrestore(&dev->reset_flow_resource_lock, flags);
+ 
+ 	return 0;
+ 
+ err_create:
+ 	destroy_qp(dev, qp, base, NULL);
++>>>>>>> 747c519cdbe4 (RDMA/mlx5: Reduce amount of duplication in QP destroy)
  	return err;
  }
  
@@@ -2522,49 -2465,76 +2722,46 @@@ static void destroy_qp_common(struct ml
  				     base->mqp.qpn);
  	}
  
- 	if (udata)
- 		destroy_qp_user(dev, &get_pd(qp)->ibpd, qp, base, udata);
- 	else
- 		destroy_qp_kernel(dev, qp);
+ 	destroy_qp(dev, qp, base, udata);
  }
  
 -static int create_dct(struct ib_pd *pd, struct mlx5_ib_qp *qp,
 -		      struct ib_qp_init_attr *attr,
 -		      struct mlx5_ib_create_qp *ucmd, u32 uidx)
 +static const char *ib_qp_type_str(enum ib_qp_type type)
  {
 -	void *dctc;
 -
 -	qp->dct.in = kzalloc(MLX5_ST_SZ_BYTES(create_dct_in), GFP_KERNEL);
 -	if (!qp->dct.in)
 -		return -ENOMEM;
 -
 -	MLX5_SET(create_dct_in, qp->dct.in, uid, to_mpd(pd)->uid);
 -	dctc = MLX5_ADDR_OF(create_dct_in, qp->dct.in, dct_context_entry);
 -	MLX5_SET(dctc, dctc, pd, to_mpd(pd)->pdn);
 -	MLX5_SET(dctc, dctc, srqn_xrqn, to_msrq(attr->srq)->msrq.srqn);
 -	MLX5_SET(dctc, dctc, cqn, to_mcq(attr->recv_cq)->mcq.cqn);
 -	MLX5_SET64(dctc, dctc, dc_access_key, ucmd->access_key);
 -	MLX5_SET(dctc, dctc, user_index, uidx);
 -
 -	if (qp->flags_en & MLX5_QP_FLAG_SCATTER_CQE) {
 -		int rcqe_sz = mlx5_ib_get_cqe_size(attr->recv_cq);
 -
 -		if (rcqe_sz == 128)
 -			MLX5_SET(dctc, dctc, cs_res, MLX5_RES_SCAT_DATA64_CQE);
 -	}
 -
 -	qp->state = IB_QPS_RESET;
 -
 -	return 0;
 -}
 -
 -static int check_qp_type(struct mlx5_ib_dev *dev, struct ib_qp_init_attr *attr,
 -			 enum ib_qp_type *type)
 -{
 -	if (attr->qp_type == IB_QPT_DRIVER && !MLX5_CAP_GEN(dev->mdev, dct))
 -		goto out;
 -
 -	switch (attr->qp_type) {
 -	case IB_QPT_XRC_TGT:
 -	case IB_QPT_XRC_INI:
 -		if (!MLX5_CAP_GEN(dev->mdev, xrc))
 -			goto out;
 -		fallthrough;
 -	case IB_QPT_RAW_PACKET:
 +	switch (type) {
 +	case IB_QPT_SMI:
 +		return "IB_QPT_SMI";
 +	case IB_QPT_GSI:
 +		return "IB_QPT_GSI";
  	case IB_QPT_RC:
 +		return "IB_QPT_RC";
  	case IB_QPT_UC:
 +		return "IB_QPT_UC";
  	case IB_QPT_UD:
 -	case IB_QPT_SMI:
 -	case MLX5_IB_QPT_HW_GSI:
 +		return "IB_QPT_UD";
 +	case IB_QPT_RAW_IPV6:
 +		return "IB_QPT_RAW_IPV6";
 +	case IB_QPT_RAW_ETHERTYPE:
 +		return "IB_QPT_RAW_ETHERTYPE";
 +	case IB_QPT_XRC_INI:
 +		return "IB_QPT_XRC_INI";
 +	case IB_QPT_XRC_TGT:
 +		return "IB_QPT_XRC_TGT";
 +	case IB_QPT_RAW_PACKET:
 +		return "IB_QPT_RAW_PACKET";
  	case MLX5_IB_QPT_REG_UMR:
 +		return "MLX5_IB_QPT_REG_UMR";
  	case IB_QPT_DRIVER:
 -	case IB_QPT_GSI:
 -		break;
 +		return "IB_QPT_DRIVER";
 +	case IB_QPT_MAX:
  	default:
 -		goto out;
 +		return "Invalid QP type";
  	}
 -
 -	*type = attr->qp_type;
 -	return 0;
 -
 -out:
 -	mlx5_ib_dbg(dev, "Unsupported QP type %d\n", attr->qp_type);
 -	return -EOPNOTSUPP;
  }
  
 -static int check_valid_flow(struct mlx5_ib_dev *dev, struct ib_pd *pd,
 -			    struct ib_qp_init_attr *attr,
 -			    struct ib_udata *udata)
 +static struct ib_qp *mlx5_ib_create_dct(struct ib_pd *pd,
 +					struct ib_qp_init_attr *attr,
 +					struct mlx5_ib_create_qp *ucmd,
 +					struct ib_udata *udata)
  {
  	struct mlx5_ib_ucontext *ucontext = rdma_udata_to_drv_context(
  		udata, struct mlx5_ib_ucontext, ibucontext);
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
