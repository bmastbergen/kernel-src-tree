usercopy: Avoid soft lockups in test_check_nonzero_user()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Michael Ellerman <mpe@ellerman.id.au>
commit f418dddffc8007945fd5962380ebde770a240cf5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f418dddf.failed

On a machine with a 64K PAGE_SIZE, the nested for loops in
test_check_nonzero_user() can lead to soft lockups, eg:

  watchdog: BUG: soft lockup - CPU#4 stuck for 22s! [modprobe:611]
  Modules linked in: test_user_copy(+) vmx_crypto gf128mul crc32c_vpmsum virtio_balloon ip_tables x_tables autofs4
  CPU: 4 PID: 611 Comm: modprobe Tainted: G             L    5.4.0-rc1-gcc-8.2.0-00001-gf5a1a536fa14-dirty #1151
  ...
  NIP __might_sleep+0x20/0xc0
  LR  __might_fault+0x40/0x60
  Call Trace:
    check_zeroed_user+0x12c/0x200
    test_user_copy_init+0x67c/0x1210 [test_user_copy]
    do_one_initcall+0x60/0x340
    do_init_module+0x7c/0x2f0
    load_module+0x2d94/0x30e0
    __do_sys_finit_module+0xc8/0x150
    system_call+0x5c/0x68

Even with a 4K PAGE_SIZE the test takes multiple seconds. Instead
tweak it to only scan a 1024 byte region, but make it cross the
page boundary.

Fixes: f5a1a536fa14 ("lib: introduce copy_struct_from_user() helper")
	Suggested-by: Aleksa Sarai <cyphar@cyphar.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
	Reviewed-by: Aleksa Sarai <cyphar@cyphar.com>
	Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
Link: https://lore.kernel.org/r/20191016122732.13467-1-mpe@ellerman.id.au
	Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
(cherry picked from commit f418dddffc8007945fd5962380ebde770a240cf5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/test_user_copy.c
diff --cc lib/test_user_copy.c
index e161f0498f42,5ff04d8fe971..000000000000
--- a/lib/test_user_copy.c
+++ b/lib/test_user_copy.c
@@@ -39,14 -31,152 +39,155 @@@
  # define TEST_U64
  #endif
  
 -#define test(condition, msg, ...)					\
 -({									\
 -	int cond = (condition);						\
 -	if (cond)							\
 -		pr_warn("[%d] " msg "\n", __LINE__, ##__VA_ARGS__);	\
 -	cond;								\
 +#define test(condition, msg)		\
 +({					\
 +	int cond = (condition);		\
 +	if (cond)			\
 +		pr_warn("%s\n", msg);	\
 +	cond;				\
  })
  
++<<<<<<< HEAD
++=======
+ static bool is_zeroed(void *from, size_t size)
+ {
+ 	return memchr_inv(from, 0x0, size) == NULL;
+ }
+ 
+ static int test_check_nonzero_user(char *kmem, char __user *umem, size_t size)
+ {
+ 	int ret = 0;
+ 	size_t start, end, i, zero_start, zero_end;
+ 
+ 	if (test(size < 2 * PAGE_SIZE, "buffer too small"))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * We want to cross a page boundary to exercise the code more
+ 	 * effectively. We also don't want to make the size we scan too large,
+ 	 * otherwise the test can take a long time and cause soft lockups. So
+ 	 * scan a 1024 byte region across the page boundary.
+ 	 */
+ 	size = 1024;
+ 	start = PAGE_SIZE - (size / 2);
+ 
+ 	kmem += start;
+ 	umem += start;
+ 
+ 	zero_start = size / 4;
+ 	zero_end = size - zero_start;
+ 
+ 	/*
+ 	 * We conduct a series of check_nonzero_user() tests on a block of
+ 	 * memory with the following byte-pattern (trying every possible
+ 	 * [start,end] pair):
+ 	 *
+ 	 *   [ 00 ff 00 ff ... 00 00 00 00 ... ff 00 ff 00 ]
+ 	 *
+ 	 * And we verify that check_nonzero_user() acts identically to
+ 	 * memchr_inv().
+ 	 */
+ 
+ 	memset(kmem, 0x0, size);
+ 	for (i = 1; i < zero_start; i += 2)
+ 		kmem[i] = 0xff;
+ 	for (i = zero_end; i < size; i += 2)
+ 		kmem[i] = 0xff;
+ 
+ 	ret |= test(copy_to_user(umem, kmem, size),
+ 		    "legitimate copy_to_user failed");
+ 
+ 	for (start = 0; start <= size; start++) {
+ 		for (end = start; end <= size; end++) {
+ 			size_t len = end - start;
+ 			int retval = check_zeroed_user(umem + start, len);
+ 			int expected = is_zeroed(kmem + start, len);
+ 
+ 			ret |= test(retval != expected,
+ 				    "check_nonzero_user(=%d) != memchr_inv(=%d) mismatch (start=%zu, end=%zu)",
+ 				    retval, expected, start, end);
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int test_copy_struct_from_user(char *kmem, char __user *umem,
+ 				      size_t size)
+ {
+ 	int ret = 0;
+ 	char *umem_src = NULL, *expected = NULL;
+ 	size_t ksize, usize;
+ 
+ 	umem_src = kmalloc(size, GFP_KERNEL);
+ 	ret = test(umem_src == NULL, "kmalloc failed");
+ 	if (ret)
+ 		goto out_free;
+ 
+ 	expected = kmalloc(size, GFP_KERNEL);
+ 	ret = test(expected == NULL, "kmalloc failed");
+ 	if (ret)
+ 		goto out_free;
+ 
+ 	/* Fill umem with a fixed byte pattern. */
+ 	memset(umem_src, 0x3e, size);
+ 	ret |= test(copy_to_user(umem, umem_src, size),
+ 		    "legitimate copy_to_user failed");
+ 
+ 	/* Check basic case -- (usize == ksize). */
+ 	ksize = size;
+ 	usize = size;
+ 
+ 	memcpy(expected, umem_src, ksize);
+ 
+ 	memset(kmem, 0x0, size);
+ 	ret |= test(copy_struct_from_user(kmem, ksize, umem, usize),
+ 		    "copy_struct_from_user(usize == ksize) failed");
+ 	ret |= test(memcmp(kmem, expected, ksize),
+ 		    "copy_struct_from_user(usize == ksize) gives unexpected copy");
+ 
+ 	/* Old userspace case -- (usize < ksize). */
+ 	ksize = size;
+ 	usize = size / 2;
+ 
+ 	memcpy(expected, umem_src, usize);
+ 	memset(expected + usize, 0x0, ksize - usize);
+ 
+ 	memset(kmem, 0x0, size);
+ 	ret |= test(copy_struct_from_user(kmem, ksize, umem, usize),
+ 		    "copy_struct_from_user(usize < ksize) failed");
+ 	ret |= test(memcmp(kmem, expected, ksize),
+ 		    "copy_struct_from_user(usize < ksize) gives unexpected copy");
+ 
+ 	/* New userspace (-E2BIG) case -- (usize > ksize). */
+ 	ksize = size / 2;
+ 	usize = size;
+ 
+ 	memset(kmem, 0x0, size);
+ 	ret |= test(copy_struct_from_user(kmem, ksize, umem, usize) != -E2BIG,
+ 		    "copy_struct_from_user(usize > ksize) didn't give E2BIG");
+ 
+ 	/* New userspace (success) case -- (usize > ksize). */
+ 	ksize = size / 2;
+ 	usize = size;
+ 
+ 	memcpy(expected, umem_src, ksize);
+ 	ret |= test(clear_user(umem + ksize, usize - ksize),
+ 		    "legitimate clear_user failed");
+ 
+ 	memset(kmem, 0x0, size);
+ 	ret |= test(copy_struct_from_user(kmem, ksize, umem, usize),
+ 		    "copy_struct_from_user(usize > ksize) failed");
+ 	ret |= test(memcmp(kmem, expected, ksize),
+ 		    "copy_struct_from_user(usize > ksize) gives unexpected copy");
+ 
+ out_free:
+ 	kfree(expected);
+ 	kfree(umem_src);
+ 	return ret;
+ }
+ 
++>>>>>>> f418dddffc80 (usercopy: Avoid soft lockups in test_check_nonzero_user())
  static int __init test_user_copy_init(void)
  {
  	int ret = 0;
* Unmerged path lib/test_user_copy.c
