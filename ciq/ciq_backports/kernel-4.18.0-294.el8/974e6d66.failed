mm, hugetlbfs: pass fault address to cow handler

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Huang Ying <ying.huang@intel.com>
commit 974e6d66b6b5c6e2d6a3ccc18b2f9a0b472be5b4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/974e6d66.failed

This is to take better advantage of the general huge page copying
optimization.  Where, the target subpage will be copied last to avoid
the cache lines of target subpage to be evicted when copying other
subpages.  This works better if the address of the target subpage is
available when copying huge page.  So hugetlbfs page fault handlers are
changed to pass that information to hugetlb_cow().  This will benefit
workloads which don't access the begin of the hugetlbfs huge page after
the page fault under heavy cache contention.

Link: http://lkml.kernel.org/r/20180524005851.4079-5-ying.huang@intel.com
	Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
	Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Andi Kleen <andi.kleen@intel.com>
	Cc: Jan Kara <jack@suse.cz>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Shaohua Li <shli@fb.com>
	Cc: Christopher Lameter <cl@linux.com>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Punit Agrawal <punit.agrawal@arm.com>
	Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 974e6d66b6b5c6e2d6a3ccc18b2f9a0b472be5b4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 350141219e0f,f1bcaae0d73a..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -3766,17 -3508,17 +3766,22 @@@ static void unmap_ref_private(struct mm
   * cannot race with other handlers or page migration.
   * Keep the pte_same checks anyway to make transition from the mutex easier.
   */
++<<<<<<< HEAD
 +static vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
++=======
+ static int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
++>>>>>>> 974e6d66b6b5 (mm, hugetlbfs: pass fault address to cow handler)
  		       unsigned long address, pte_t *ptep,
  		       struct page *pagecache_page, spinlock_t *ptl)
  {
  	pte_t pte;
  	struct hstate *h = hstate_vma(vma);
  	struct page *old_page, *new_page;
 -	int ret = 0, outside_reserve = 0;
 +	int outside_reserve = 0;
 +	vm_fault_t ret = 0;
  	unsigned long mmun_start;	/* For mmu_notifiers */
  	unsigned long mmun_end;		/* For mmu_notifiers */
+ 	unsigned long haddr = address & huge_page_mask(h);
  
  	pte = huge_ptep_get(ptep);
  	old_page = pte_page(pte);
* Unmerged path mm/hugetlb.c
