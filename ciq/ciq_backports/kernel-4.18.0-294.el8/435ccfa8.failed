tcp: Prevent low rmem stalls with SO_RCVLOWAT.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Arjun Roy <arjunroy@google.com>
commit 435ccfa894e35e3d4a1799e6ac030e48a7b69ef5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/435ccfa8.failed

With SO_RCVLOWAT, under memory pressure,
it is possible to enter a state where:

1. We have not received enough bytes to satisfy SO_RCVLOWAT.
2. We have not entered buffer pressure (see tcp_rmem_pressure()).
3. But, we do not have enough buffer space to accept more packets.

In this case, we advertise 0 rwnd (due to #3) but the application does
not drain the receive queue (no wakeup because of #1 and #2) so the
flow stalls.

Modify the heuristic for SO_RCVLOWAT so that, if we are advertising
rwnd<=rcv_mss, force a wakeup to prevent a stall.

Without this patch, setting tcp_rmem to 6143 and disabling TCP
autotune causes a stalled flow. With this patch, no stall occurs. This
is with RPC-style traffic with large messages.

Fixes: 03f45c883c6f ("tcp: avoid extra wakeups for SO_RCVLOWAT users")
	Signed-off-by: Arjun Roy <arjunroy@google.com>
	Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
	Acked-by: Neal Cardwell <ncardwell@google.com>
	Signed-off-by: Eric Dumazet <edumazet@google.com>
Link: https://lore.kernel.org/r/20201023184709.217614-1-arjunroy.kdev@gmail.com
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit 435ccfa894e35e3d4a1799e6ac030e48a7b69ef5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp.c
#	net/ipv4/tcp_input.c
diff --cc net/ipv4/tcp.c
index 93a220a78f1b,b2bc3d7fe9e8..000000000000
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@@ -489,9 -478,19 +489,25 @@@ static void tcp_tx_timestamp(struct soc
  static inline bool tcp_stream_is_readable(const struct tcp_sock *tp,
  					  int target, struct sock *sk)
  {
++<<<<<<< HEAD
 +	return (tp->rcv_nxt - tp->copied_seq >= target) ||
 +		(sk->sk_prot->stream_memory_read ?
 +		sk->sk_prot->stream_memory_read(sk) : false);
++=======
+ 	int avail = READ_ONCE(tp->rcv_nxt) - READ_ONCE(tp->copied_seq);
+ 
+ 	if (avail > 0) {
+ 		if (avail >= target)
+ 			return true;
+ 		if (tcp_rmem_pressure(sk))
+ 			return true;
+ 		if (tcp_receive_window(tp) <= inet_csk(sk)->icsk_ack.rcv_mss)
+ 			return true;
+ 	}
+ 	if (sk->sk_prot->stream_memory_read)
+ 		return sk->sk_prot->stream_memory_read(sk);
+ 	return false;
++>>>>>>> 435ccfa894e3 (tcp: Prevent low rmem stalls with SO_RCVLOWAT.)
  }
  
  /*
diff --cc net/ipv4/tcp_input.c
index 6542b07d9471,389d1b340248..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -4765,7 -4907,9 +4765,13 @@@ void tcp_data_ready(struct sock *sk
  	const struct tcp_sock *tp = tcp_sk(sk);
  	int avail = tp->rcv_nxt - tp->copied_seq;
  
++<<<<<<< HEAD
 +	if (avail < sk->sk_rcvlowat && !sock_flag(sk, SOCK_DONE))
++=======
+ 	if (avail < sk->sk_rcvlowat && !tcp_rmem_pressure(sk) &&
+ 	    !sock_flag(sk, SOCK_DONE) &&
+ 	    tcp_receive_window(tp) > inet_csk(sk)->icsk_ack.rcv_mss)
++>>>>>>> 435ccfa894e3 (tcp: Prevent low rmem stalls with SO_RCVLOWAT.)
  		return;
  
  	sk->sk_data_ready(sk);
* Unmerged path net/ipv4/tcp.c
* Unmerged path net/ipv4/tcp_input.c
