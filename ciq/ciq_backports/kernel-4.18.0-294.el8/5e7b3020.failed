bpf: Change uapi for bpf iterator map elements

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yonghong Song <yhs@fb.com>
commit 5e7b30205cef80f6bb922e61834437ca7bff5837
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/5e7b3020.failed

Commit a5cbe05a6673 ("bpf: Implement bpf iterator for
map elements") added bpf iterator support for
map elements. The map element bpf iterator requires
info to identify a particular map. In the above
commit, the attr->link_create.target_fd is used
to carry map_fd and an enum bpf_iter_link_info
is added to uapi to specify the target_fd actually
representing a map_fd:
    enum bpf_iter_link_info {
	BPF_ITER_LINK_UNSPEC = 0,
	BPF_ITER_LINK_MAP_FD = 1,

	MAX_BPF_ITER_LINK_INFO,
    };

This is an extensible approach as we can grow
enumerator for pid, cgroup_id, etc. and we can
unionize target_fd for pid, cgroup_id, etc.
But in the future, there are chances that
more complex customization may happen, e.g.,
for tasks, it could be filtered based on
both cgroup_id and user_id.

This patch changed the uapi to have fields
	__aligned_u64	iter_info;
	__u32		iter_info_len;
for additional iter_info for link_create.
The iter_info is defined as
	union bpf_iter_link_info {
		struct {
			__u32   map_fd;
		} map;
	};

So future extension for additional customization
will be easier. The bpf_iter_link_info will be
passed to target callback to validate and generic
bpf_iter framework does not need to deal it any
more.

Note that map_fd = 0 will be considered invalid
and -EBADF will be returned to user space.

Fixes: a5cbe05a6673 ("bpf: Implement bpf iterator for map elements")
	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
	Acked-by: John Fastabend <john.fastabend@gmail.com>
Link: https://lore.kernel.org/bpf/20200805055056.1457463-1-yhs@fb.com
(cherry picked from commit 5e7b30205cef80f6bb922e61834437ca7bff5837)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	kernel/bpf/bpf_iter.c
#	kernel/bpf/map_iter.c
#	kernel/bpf/syscall.c
diff --cc include/linux/bpf.h
index 23b2ce704f08,55f694b63164..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -1148,19 -1205,51 +1148,39 @@@ struct bpf_link *bpf_link_get_from_fd(u
  int bpf_obj_pin_user(u32 ufd, const char __user *pathname);
  int bpf_obj_get_user(const char __user *pathname, int flags);
  
 -#define BPF_ITER_FUNC_PREFIX "bpf_iter_"
 -#define DEFINE_BPF_ITER_FUNC(target, args...)			\
 -	extern int bpf_iter_ ## target(args);			\
 -	int __init bpf_iter_ ## target(args) { return 0; }
 +typedef int (*bpf_iter_init_seq_priv_t)(void *private_data);
 +typedef void (*bpf_iter_fini_seq_priv_t)(void *private_data);
  
++<<<<<<< HEAD
 +struct bpf_iter_reg {
 +	const char *target;
 +	const struct seq_operations *seq_ops;
 +	bpf_iter_init_seq_priv_t init_seq_private;
 +	bpf_iter_fini_seq_priv_t fini_seq_private;
 +	u32 seq_priv_size;
++=======
+ struct bpf_iter_aux_info {
+ 	struct bpf_map *map;
+ };
+ 
+ typedef int (*bpf_iter_attach_target_t)(struct bpf_prog *prog,
+ 					union bpf_iter_link_info *linfo,
+ 					struct bpf_iter_aux_info *aux);
+ typedef void (*bpf_iter_detach_target_t)(struct bpf_iter_aux_info *aux);
+ 
+ #define BPF_ITER_CTX_ARG_MAX 2
+ struct bpf_iter_reg {
+ 	const char *target;
+ 	bpf_iter_attach_target_t attach_target;
+ 	bpf_iter_detach_target_t detach_target;
+ 	u32 ctx_arg_info_size;
+ 	struct bpf_ctx_arg_aux ctx_arg_info[BPF_ITER_CTX_ARG_MAX];
+ 	const struct bpf_iter_seq_info *seq_info;
++>>>>>>> 5e7b30205cef (bpf: Change uapi for bpf iterator map elements)
  };
  
 -struct bpf_iter_meta {
 -	__bpf_md_ptr(struct seq_file *, seq);
 -	u64 session_id;
 -	u64 seq_num;
 -};
 -
 -struct bpf_iter__bpf_map_elem {
 -	__bpf_md_ptr(struct bpf_iter_meta *, meta);
 -	__bpf_md_ptr(struct bpf_map *, map);
 -	__bpf_md_ptr(void *, key);
 -	__bpf_md_ptr(void *, value);
 -};
 -
 -int bpf_iter_reg_target(const struct bpf_iter_reg *reg_info);
 -void bpf_iter_unreg_target(const struct bpf_iter_reg *reg_info);
 -bool bpf_iter_prog_supported(struct bpf_prog *prog);
 -int bpf_iter_link_attach(const union bpf_attr *attr, struct bpf_prog *prog);
 -int bpf_iter_new_fd(struct bpf_link *link);
 -bool bpf_link_is_iter(struct bpf_link *link);
 -struct bpf_prog *bpf_iter_get_info(struct bpf_iter_meta *meta, bool in_stop);
 -int bpf_iter_run_prog(struct bpf_prog *prog, void *ctx);
 +int bpf_iter_reg_target(struct bpf_iter_reg *reg_info);
 +void bpf_iter_unreg_target(const char *target);
  
  int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
  int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
diff --cc kernel/bpf/bpf_iter.c
index 5a8119d17d14,b6715964b685..000000000000
--- a/kernel/bpf/bpf_iter.c
+++ b/kernel/bpf/bpf_iter.c
@@@ -57,3 -292,301 +57,304 @@@ void bpf_iter_unreg_target(const char *
  
  	WARN_ON(found == false);
  }
++<<<<<<< HEAD
++=======
+ 
+ static void cache_btf_id(struct bpf_iter_target_info *tinfo,
+ 			 struct bpf_prog *prog)
+ {
+ 	tinfo->btf_id = prog->aux->attach_btf_id;
+ }
+ 
+ bool bpf_iter_prog_supported(struct bpf_prog *prog)
+ {
+ 	const char *attach_fname = prog->aux->attach_func_name;
+ 	u32 prog_btf_id = prog->aux->attach_btf_id;
+ 	const char *prefix = BPF_ITER_FUNC_PREFIX;
+ 	struct bpf_iter_target_info *tinfo;
+ 	int prefix_len = strlen(prefix);
+ 	bool supported = false;
+ 
+ 	if (strncmp(attach_fname, prefix, prefix_len))
+ 		return false;
+ 
+ 	mutex_lock(&targets_mutex);
+ 	list_for_each_entry(tinfo, &targets, list) {
+ 		if (tinfo->btf_id && tinfo->btf_id == prog_btf_id) {
+ 			supported = true;
+ 			break;
+ 		}
+ 		if (!strcmp(attach_fname + prefix_len, tinfo->reg_info->target)) {
+ 			cache_btf_id(tinfo, prog);
+ 			supported = true;
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&targets_mutex);
+ 
+ 	if (supported) {
+ 		prog->aux->ctx_arg_info_size = tinfo->reg_info->ctx_arg_info_size;
+ 		prog->aux->ctx_arg_info = tinfo->reg_info->ctx_arg_info;
+ 	}
+ 
+ 	return supported;
+ }
+ 
+ static void bpf_iter_link_release(struct bpf_link *link)
+ {
+ 	struct bpf_iter_link *iter_link =
+ 		container_of(link, struct bpf_iter_link, link);
+ 
+ 	if (iter_link->tinfo->reg_info->detach_target)
+ 		iter_link->tinfo->reg_info->detach_target(&iter_link->aux);
+ }
+ 
+ static void bpf_iter_link_dealloc(struct bpf_link *link)
+ {
+ 	struct bpf_iter_link *iter_link =
+ 		container_of(link, struct bpf_iter_link, link);
+ 
+ 	kfree(iter_link);
+ }
+ 
+ static int bpf_iter_link_replace(struct bpf_link *link,
+ 				 struct bpf_prog *new_prog,
+ 				 struct bpf_prog *old_prog)
+ {
+ 	int ret = 0;
+ 
+ 	mutex_lock(&link_mutex);
+ 	if (old_prog && link->prog != old_prog) {
+ 		ret = -EPERM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	if (link->prog->type != new_prog->type ||
+ 	    link->prog->expected_attach_type != new_prog->expected_attach_type ||
+ 	    link->prog->aux->attach_btf_id != new_prog->aux->attach_btf_id) {
+ 		ret = -EINVAL;
+ 		goto out_unlock;
+ 	}
+ 
+ 	old_prog = xchg(&link->prog, new_prog);
+ 	bpf_prog_put(old_prog);
+ 
+ out_unlock:
+ 	mutex_unlock(&link_mutex);
+ 	return ret;
+ }
+ 
+ static const struct bpf_link_ops bpf_iter_link_lops = {
+ 	.release = bpf_iter_link_release,
+ 	.dealloc = bpf_iter_link_dealloc,
+ 	.update_prog = bpf_iter_link_replace,
+ };
+ 
+ bool bpf_link_is_iter(struct bpf_link *link)
+ {
+ 	return link->ops == &bpf_iter_link_lops;
+ }
+ 
+ int bpf_iter_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)
+ {
+ 	union bpf_iter_link_info __user *ulinfo;
+ 	struct bpf_link_primer link_primer;
+ 	struct bpf_iter_target_info *tinfo;
+ 	union bpf_iter_link_info linfo;
+ 	struct bpf_iter_link *link;
+ 	u32 prog_btf_id, linfo_len;
+ 	bool existed = false;
+ 	int err;
+ 
+ 	if (attr->link_create.target_fd || attr->link_create.flags)
+ 		return -EINVAL;
+ 
+ 	memset(&linfo, 0, sizeof(union bpf_iter_link_info));
+ 
+ 	ulinfo = u64_to_user_ptr(attr->link_create.iter_info);
+ 	linfo_len = attr->link_create.iter_info_len;
+ 	if (!ulinfo ^ !linfo_len)
+ 		return -EINVAL;
+ 
+ 	if (ulinfo) {
+ 		err = bpf_check_uarg_tail_zero(ulinfo, sizeof(linfo),
+ 					       linfo_len);
+ 		if (err)
+ 			return err;
+ 		linfo_len = min_t(u32, linfo_len, sizeof(linfo));
+ 		if (copy_from_user(&linfo, ulinfo, linfo_len))
+ 			return -EFAULT;
+ 	}
+ 
+ 	prog_btf_id = prog->aux->attach_btf_id;
+ 	mutex_lock(&targets_mutex);
+ 	list_for_each_entry(tinfo, &targets, list) {
+ 		if (tinfo->btf_id == prog_btf_id) {
+ 			existed = true;
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&targets_mutex);
+ 	if (!existed)
+ 		return -ENOENT;
+ 
+ 	link = kzalloc(sizeof(*link), GFP_USER | __GFP_NOWARN);
+ 	if (!link)
+ 		return -ENOMEM;
+ 
+ 	bpf_link_init(&link->link, BPF_LINK_TYPE_ITER, &bpf_iter_link_lops, prog);
+ 	link->tinfo = tinfo;
+ 
+ 	err  = bpf_link_prime(&link->link, &link_primer);
+ 	if (err) {
+ 		kfree(link);
+ 		return err;
+ 	}
+ 
+ 	if (tinfo->reg_info->attach_target) {
+ 		err = tinfo->reg_info->attach_target(prog, &linfo, &link->aux);
+ 		if (err) {
+ 			bpf_link_cleanup(&link_primer);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	return bpf_link_settle(&link_primer);
+ }
+ 
+ static void init_seq_meta(struct bpf_iter_priv_data *priv_data,
+ 			  struct bpf_iter_target_info *tinfo,
+ 			  const struct bpf_iter_seq_info *seq_info,
+ 			  struct bpf_prog *prog)
+ {
+ 	priv_data->tinfo = tinfo;
+ 	priv_data->seq_info = seq_info;
+ 	priv_data->prog = prog;
+ 	priv_data->session_id = atomic64_inc_return(&session_id);
+ 	priv_data->seq_num = 0;
+ 	priv_data->done_stop = false;
+ }
+ 
+ static int prepare_seq_file(struct file *file, struct bpf_iter_link *link,
+ 			    const struct bpf_iter_seq_info *seq_info)
+ {
+ 	struct bpf_iter_priv_data *priv_data;
+ 	struct bpf_iter_target_info *tinfo;
+ 	struct bpf_prog *prog;
+ 	u32 total_priv_dsize;
+ 	struct seq_file *seq;
+ 	int err = 0;
+ 
+ 	mutex_lock(&link_mutex);
+ 	prog = link->link.prog;
+ 	bpf_prog_inc(prog);
+ 	mutex_unlock(&link_mutex);
+ 
+ 	tinfo = link->tinfo;
+ 	total_priv_dsize = offsetof(struct bpf_iter_priv_data, target_private) +
+ 			   seq_info->seq_priv_size;
+ 	priv_data = __seq_open_private(file, seq_info->seq_ops,
+ 				       total_priv_dsize);
+ 	if (!priv_data) {
+ 		err = -ENOMEM;
+ 		goto release_prog;
+ 	}
+ 
+ 	if (seq_info->init_seq_private) {
+ 		err = seq_info->init_seq_private(priv_data->target_private, &link->aux);
+ 		if (err)
+ 			goto release_seq_file;
+ 	}
+ 
+ 	init_seq_meta(priv_data, tinfo, seq_info, prog);
+ 	seq = file->private_data;
+ 	seq->private = priv_data->target_private;
+ 
+ 	return 0;
+ 
+ release_seq_file:
+ 	seq_release_private(file->f_inode, file);
+ 	file->private_data = NULL;
+ release_prog:
+ 	bpf_prog_put(prog);
+ 	return err;
+ }
+ 
+ int bpf_iter_new_fd(struct bpf_link *link)
+ {
+ 	struct bpf_iter_link *iter_link;
+ 	struct file *file;
+ 	unsigned int flags;
+ 	int err, fd;
+ 
+ 	if (link->ops != &bpf_iter_link_lops)
+ 		return -EINVAL;
+ 
+ 	flags = O_RDONLY | O_CLOEXEC;
+ 	fd = get_unused_fd_flags(flags);
+ 	if (fd < 0)
+ 		return fd;
+ 
+ 	file = anon_inode_getfile("bpf_iter", &bpf_iter_fops, NULL, flags);
+ 	if (IS_ERR(file)) {
+ 		err = PTR_ERR(file);
+ 		goto free_fd;
+ 	}
+ 
+ 	iter_link = container_of(link, struct bpf_iter_link, link);
+ 	err = prepare_seq_file(file, iter_link, __get_seq_info(iter_link));
+ 	if (err)
+ 		goto free_file;
+ 
+ 	fd_install(fd, file);
+ 	return fd;
+ 
+ free_file:
+ 	fput(file);
+ free_fd:
+ 	put_unused_fd(fd);
+ 	return err;
+ }
+ 
+ struct bpf_prog *bpf_iter_get_info(struct bpf_iter_meta *meta, bool in_stop)
+ {
+ 	struct bpf_iter_priv_data *iter_priv;
+ 	struct seq_file *seq;
+ 	void *seq_priv;
+ 
+ 	seq = meta->seq;
+ 	if (seq->file->f_op != &bpf_iter_fops)
+ 		return NULL;
+ 
+ 	seq_priv = seq->private;
+ 	iter_priv = container_of(seq_priv, struct bpf_iter_priv_data,
+ 				 target_private);
+ 
+ 	if (in_stop && iter_priv->done_stop)
+ 		return NULL;
+ 
+ 	meta->session_id = iter_priv->session_id;
+ 	meta->seq_num = iter_priv->seq_num;
+ 
+ 	return iter_priv->prog;
+ }
+ 
+ int bpf_iter_run_prog(struct bpf_prog *prog, void *ctx)
+ {
+ 	int ret;
+ 
+ 	rcu_read_lock();
+ 	migrate_disable();
+ 	ret = BPF_PROG_RUN(prog, ctx);
+ 	migrate_enable();
+ 	rcu_read_unlock();
+ 
+ 	/* bpf program can only return 0 or 1:
+ 	 *  0 : okay
+ 	 *  1 : retry the same object
+ 	 * The bpf_iter_run_prog() return value
+ 	 * will be seq_ops->show() return value.
+ 	 */
+ 	return ret == 0 ? 0 : -EAGAIN;
+ }
++>>>>>>> 5e7b30205cef (bpf: Change uapi for bpf iterator map elements)
diff --cc kernel/bpf/map_iter.c
index af759897e5bd,af86048e5afd..000000000000
--- a/kernel/bpf/map_iter.c
+++ b/kernel/bpf/map_iter.c
@@@ -77,17 -78,103 +77,107 @@@ static const struct seq_operations bpf_
  	.show	= bpf_map_seq_show,
  };
  
++<<<<<<< HEAD
++=======
+ BTF_ID_LIST(btf_bpf_map_id)
+ BTF_ID(struct, bpf_map)
+ 
+ static const struct bpf_iter_seq_info bpf_map_seq_info = {
+ 	.seq_ops		= &bpf_map_seq_ops,
+ 	.init_seq_private	= NULL,
+ 	.fini_seq_private	= NULL,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
+ };
+ 
+ static struct bpf_iter_reg bpf_map_reg_info = {
+ 	.target			= "bpf_map",
+ 	.ctx_arg_info_size	= 1,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__bpf_map, map),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ 	.seq_info		= &bpf_map_seq_info,
+ };
+ 
+ static int bpf_iter_attach_map(struct bpf_prog *prog,
+ 			       union bpf_iter_link_info *linfo,
+ 			       struct bpf_iter_aux_info *aux)
+ {
+ 	u32 key_acc_size, value_acc_size, key_size, value_size;
+ 	struct bpf_map *map;
+ 	bool is_percpu = false;
+ 	int err = -EINVAL;
+ 
+ 	if (!linfo->map.map_fd)
+ 		return -EBADF;
+ 
+ 	map = bpf_map_get_with_uref(linfo->map.map_fd);
+ 	if (IS_ERR(map))
+ 		return PTR_ERR(map);
+ 
+ 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
+ 	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
+ 		is_percpu = true;
+ 	else if (map->map_type != BPF_MAP_TYPE_HASH &&
+ 		 map->map_type != BPF_MAP_TYPE_LRU_HASH &&
+ 		 map->map_type != BPF_MAP_TYPE_ARRAY)
+ 		goto put_map;
+ 
+ 	key_acc_size = prog->aux->max_rdonly_access;
+ 	value_acc_size = prog->aux->max_rdwr_access;
+ 	key_size = map->key_size;
+ 	if (!is_percpu)
+ 		value_size = map->value_size;
+ 	else
+ 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
+ 
+ 	if (key_acc_size > key_size || value_acc_size > value_size) {
+ 		err = -EACCES;
+ 		goto put_map;
+ 	}
+ 
+ 	aux->map = map;
+ 	return 0;
+ 
+ put_map:
+ 	bpf_map_put_with_uref(map);
+ 	return err;
+ }
+ 
+ static void bpf_iter_detach_map(struct bpf_iter_aux_info *aux)
+ {
+ 	bpf_map_put_with_uref(aux->map);
+ }
+ 
+ DEFINE_BPF_ITER_FUNC(bpf_map_elem, struct bpf_iter_meta *meta,
+ 		     struct bpf_map *map, void *key, void *value)
+ 
+ static const struct bpf_iter_reg bpf_map_elem_reg_info = {
+ 	.target			= "bpf_map_elem",
+ 	.attach_target		= bpf_iter_attach_map,
+ 	.detach_target		= bpf_iter_detach_map,
+ 	.ctx_arg_info_size	= 2,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__bpf_map_elem, key),
+ 		  PTR_TO_RDONLY_BUF_OR_NULL },
+ 		{ offsetof(struct bpf_iter__bpf_map_elem, value),
+ 		  PTR_TO_RDWR_BUF_OR_NULL },
+ 	},
+ };
+ 
++>>>>>>> 5e7b30205cef (bpf: Change uapi for bpf iterator map elements)
  static int __init bpf_map_iter_init(void)
  {
 -	int ret;
 -
 -	bpf_map_reg_info.ctx_arg_info[0].btf_id = *btf_bpf_map_id;
 -	ret = bpf_iter_reg_target(&bpf_map_reg_info);
 -	if (ret)
 -		return ret;
 -
 -	return bpf_iter_reg_target(&bpf_map_elem_reg_info);
 +	struct bpf_iter_reg reg_info = {
 +		.target			= "bpf_map",
 +		.seq_ops		= &bpf_map_seq_ops,
 +		.init_seq_private	= NULL,
 +		.fini_seq_private	= NULL,
 +		.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
 +	};
 +
 +	return bpf_iter_reg_target(&reg_info);
  }
  
  late_initcall(bpf_map_iter_init);
diff --cc kernel/bpf/syscall.c
index e4798fe81b2b,86299a292214..000000000000
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@@ -3813,7 -3874,16 +3813,20 @@@ err_put
  	return err;
  }
  
++<<<<<<< HEAD
 +#define BPF_LINK_CREATE_LAST_FIELD link_create.flags
++=======
+ static int tracing_bpf_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)
+ {
+ 	if (attr->link_create.attach_type == BPF_TRACE_ITER &&
+ 	    prog->expected_attach_type == BPF_TRACE_ITER)
+ 		return bpf_iter_link_attach(attr, prog);
+ 
+ 	return -EINVAL;
+ }
+ 
+ #define BPF_LINK_CREATE_LAST_FIELD link_create.iter_info_len
++>>>>>>> 5e7b30205cef (bpf: Change uapi for bpf iterator map elements)
  static int link_create(union bpf_attr *attr)
  {
  	enum bpf_prog_type ptype;
* Unmerged path include/linux/bpf.h
diff --git a/include/uapi/linux/bpf.h b/include/uapi/linux/bpf.h
index f92a8fa8cc76..bce74efd44c8 100644
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@ -81,6 +81,12 @@ struct bpf_cgroup_storage_key {
 	__u32	attach_type;		/* program attach type */
 };
 
+union bpf_iter_link_info {
+	struct {
+		__u32	map_fd;
+	} map;
+};
+
 /* BPF syscall commands, see bpf(2) man-page for details. */
 enum bpf_cmd {
 	BPF_MAP_CREATE,
@@ -606,6 +612,8 @@ union bpf_attr {
 		__u32		target_fd;	/* object to attach to */
 		__u32		attach_type;	/* attach type */
 		__u32		flags;		/* extra flags */
+		__aligned_u64	iter_info;	/* extra bpf_iter_link_info */
+		__u32		iter_info_len;	/* iter_info length */
 	} link_create;
 
 	struct { /* struct used by BPF_LINK_UPDATE command */
* Unmerged path kernel/bpf/bpf_iter.c
* Unmerged path kernel/bpf/map_iter.c
* Unmerged path kernel/bpf/syscall.c
diff --git a/net/core/bpf_sk_storage.c b/net/core/bpf_sk_storage.c
index be5b44826f5e..196684ab8975 100644
--- a/net/core/bpf_sk_storage.c
+++ b/net/core/bpf_sk_storage.c
@@ -1381,18 +1381,39 @@ static int bpf_iter_init_sk_storage_map(void *priv_data,
 	return 0;
 }
 
-static int bpf_iter_check_map(struct bpf_prog *prog,
-			      struct bpf_iter_aux_info *aux)
+static int bpf_iter_attach_map(struct bpf_prog *prog,
+			       union bpf_iter_link_info *linfo,
+			       struct bpf_iter_aux_info *aux)
 {
-	struct bpf_map *map = aux->map;
+	struct bpf_map *map;
+	int err = -EINVAL;
+
+	if (!linfo->map.map_fd)
+		return -EBADF;
+
+	map = bpf_map_get_with_uref(linfo->map.map_fd);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
 
 	if (map->map_type != BPF_MAP_TYPE_SK_STORAGE)
-		return -EINVAL;
+		goto put_map;
 
-	if (prog->aux->max_rdonly_access > map->value_size)
-		return -EACCES;
+	if (prog->aux->max_rdonly_access > map->value_size) {
+		err = -EACCES;
+		goto put_map;
+	}
 
+	aux->map = map;
 	return 0;
+
+put_map:
+	bpf_map_put_with_uref(map);
+	return err;
+}
+
+static void bpf_iter_detach_map(struct bpf_iter_aux_info *aux)
+{
+	bpf_map_put_with_uref(aux->map);
 }
 
 static const struct seq_operations bpf_sk_storage_map_seq_ops = {
@@ -1411,8 +1432,8 @@ static const struct bpf_iter_seq_info iter_seq_info = {
 
 static struct bpf_iter_reg bpf_sk_storage_map_reg_info = {
 	.target			= "bpf_sk_storage_map",
-	.check_target		= bpf_iter_check_map,
-	.req_linfo		= BPF_ITER_LINK_MAP_FD,
+	.attach_target		= bpf_iter_attach_map,
+	.detach_target		= bpf_iter_detach_map,
 	.ctx_arg_info_size	= 2,
 	.ctx_arg_info		= {
 		{ offsetof(struct bpf_iter__bpf_sk_storage_map, sk),
