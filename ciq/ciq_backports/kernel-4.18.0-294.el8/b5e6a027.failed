seqcount: More consistent seqprop names

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit b5e6a027bd327daa679ca55182a920659e2cbb90
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/b5e6a027.failed

Attempt uniformity and brevity.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
(cherry picked from commit b5e6a027bd327daa679ca55182a920659e2cbb90)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/seqlock.h
diff --cc include/linux/seqlock.h
index 362623ec6c41,a076f783aa36..000000000000
--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@@ -105,13 -111,149 +105,131 @@@ static inline void seqcount_lockdep_rea
  # define seqcount_lockdep_reader_access(x)
  #endif
  
 -/**
 - * SEQCNT_ZERO() - static initializer for seqcount_t
 - * @name: Name of the seqcount_t instance
 - */
 -#define SEQCNT_ZERO(name) { .sequence = 0, SEQCOUNT_DEP_MAP_INIT(name) }
 -
 -/*
 - * Sequence counters with associated locks (seqcount_LOCKTYPE_t)
 - *
 - * A sequence counter which associates the lock used for writer
 - * serialization at initialization time. This enables lockdep to validate
 - * that the write side critical section is properly serialized.
 - *
 - * For associated locks which do not implicitly disable preemption,
 - * preemption protection is enforced in the write side function.
 - *
 - * Lockdep is never used in any for the raw write variants.
 - *
 - * See Documentation/locking/seqlock.rst
 - */
 +#define SEQCNT_ZERO(lockname) { .sequence = 0, SEQCOUNT_DEP_MAP_INIT(lockname)}
  
 -#ifdef CONFIG_LOCKDEP
 -#define __SEQ_LOCK(expr)	expr
 -#else
 -#define __SEQ_LOCK(expr)
 -#endif
  
  /**
++<<<<<<< HEAD
 + * __read_seqcount_begin - begin a seq-read critical section (without barrier)
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
++=======
+  * typedef seqcount_LOCKNAME_t - sequence counter with LOCKTYPR associated
+  * @seqcount:	The real sequence counter
+  * @lock:	Pointer to the associated spinlock
+  *
+  * A plain sequence counter with external writer synchronization by a
+  * spinlock. The spinlock is associated to the sequence count in the
+  * static initializer or init function. This enables lockdep to validate
+  * that the write side critical section is properly serialized.
+  */
+ 
+ /**
+  * seqcount_LOCKNAME_init() - runtime initializer for seqcount_LOCKNAME_t
+  * @s:		Pointer to the seqcount_LOCKNAME_t instance
+  * @lock:	Pointer to the associated LOCKTYPE
+  */
+ 
+ /*
+  * SEQCOUNT_LOCKTYPE() - Instantiate seqcount_LOCKNAME_t and helpers
+  * @locktype:		actual typename
+  * @lockname:		name
+  * @preemptible:	preemptibility of above locktype
+  * @lockmember:		argument for lockdep_assert_held()
+  */
+ #define SEQCOUNT_LOCKTYPE(locktype, lockname, preemptible, lockmember)	\
+ typedef struct seqcount_##lockname {					\
+ 	seqcount_t		seqcount;				\
+ 	__SEQ_LOCK(locktype	*lock);					\
+ } seqcount_##lockname##_t;						\
+ 									\
+ static __always_inline void						\
+ seqcount_##lockname##_init(seqcount_##lockname##_t *s, locktype *lock)	\
+ {									\
+ 	seqcount_init(&s->seqcount);					\
+ 	__SEQ_LOCK(s->lock = lock);					\
+ }									\
+ 									\
+ static __always_inline seqcount_t *					\
+ __seqcount_##lockname##_ptr(seqcount_##lockname##_t *s)			\
+ {									\
+ 	return &s->seqcount;						\
+ }									\
+ 									\
+ static __always_inline bool						\
+ __seqcount_##lockname##_preemptible(seqcount_##lockname##_t *s)		\
+ {									\
+ 	return preemptible;						\
+ }									\
+ 									\
+ static __always_inline void						\
+ __seqcount_##lockname##_assert(seqcount_##lockname##_t *s)		\
+ {									\
+ 	__SEQ_LOCK(lockdep_assert_held(lockmember));			\
+ }
+ 
+ /*
+  * __seqprop() for seqcount_t
+  */
+ 
+ static inline seqcount_t *__seqcount_ptr(seqcount_t *s)
+ {
+ 	return s;
+ }
+ 
+ static inline bool __seqcount_preemptible(seqcount_t *s)
+ {
+ 	return false;
+ }
+ 
+ static inline void __seqcount_assert(seqcount_t *s)
+ {
+ 	lockdep_assert_preemption_disabled();
+ }
+ 
+ SEQCOUNT_LOCKTYPE(raw_spinlock_t,	raw_spinlock,	false,	s->lock)
+ SEQCOUNT_LOCKTYPE(spinlock_t,		spinlock,	false,	s->lock)
+ SEQCOUNT_LOCKTYPE(rwlock_t,		rwlock,		false,	s->lock)
+ SEQCOUNT_LOCKTYPE(struct mutex,		mutex,		true,	s->lock)
+ SEQCOUNT_LOCKTYPE(struct ww_mutex,	ww_mutex,	true,	&s->lock->base)
+ 
+ /**
+  * SEQCNT_LOCKNAME_ZERO - static initializer for seqcount_LOCKNAME_t
+  * @name:	Name of the seqcount_LOCKNAME_t instance
+  * @lock:	Pointer to the associated LOCKTYPE
+  */
+ 
+ #define SEQCOUNT_LOCKTYPE_ZERO(seq_name, assoc_lock) {			\
+ 	.seqcount		= SEQCNT_ZERO(seq_name.seqcount),	\
+ 	__SEQ_LOCK(.lock	= (assoc_lock))				\
+ }
+ 
+ #define SEQCNT_SPINLOCK_ZERO(name, lock)	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ #define SEQCNT_RAW_SPINLOCK_ZERO(name, lock)	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ #define SEQCNT_RWLOCK_ZERO(name, lock)		SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ #define SEQCNT_MUTEX_ZERO(name, lock)		SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ #define SEQCNT_WW_MUTEX_ZERO(name, lock) 	SEQCOUNT_LOCKTYPE_ZERO(name, lock)
+ 
+ 
+ #define __seqprop_case(s, lockname, prop)				\
+ 	seqcount_##lockname##_t: __seqcount_##lockname##_##prop((void *)(s))
+ 
+ #define __seqprop(s, prop) _Generic(*(s),				\
+ 	seqcount_t:		__seqcount_##prop((void *)(s)),		\
+ 	__seqprop_case((s),	raw_spinlock,	prop),			\
+ 	__seqprop_case((s),	spinlock,	prop),			\
+ 	__seqprop_case((s),	rwlock,		prop),			\
+ 	__seqprop_case((s),	mutex,		prop),			\
+ 	__seqprop_case((s),	ww_mutex,	prop))
+ 
+ #define __seqcount_ptr(s)		__seqprop(s, ptr)
+ #define __seqcount_lock_preemptible(s)	__seqprop(s, preemptible)
+ #define __seqcount_assert_lock_held(s)	__seqprop(s, assert)
+ 
+ /**
+  * __read_seqcount_begin() - begin a seqcount_t read section w/o barrier
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
   *
   * __read_seqcount_begin is like read_seqcount_begin, but has no smp_rmb()
   * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
@@@ -120,8 -262,13 +238,15 @@@
   *
   * Use carefully, only in critical code, and comment how the barrier is
   * provided.
 - *
 - * Return: count to be passed to read_seqcount_retry()
   */
++<<<<<<< HEAD
 +static inline unsigned __read_seqcount_begin(const seqcount_t *s)
++=======
+ #define __read_seqcount_begin(s)					\
+ 	__read_seqcount_t_begin(__seqcount_ptr(s))
+ 
+ static inline unsigned __read_seqcount_t_begin(const seqcount_t *s)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
  	unsigned ret;
  
@@@ -136,15 -283,51 +261,58 @@@ repeat
  }
  
  /**
++<<<<<<< HEAD
 + * raw_read_seqcount - Read the raw seqcount
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
++=======
+  * raw_read_seqcount_begin() - begin a seqcount_t read section w/o lockdep
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * Return: count to be passed to read_seqcount_retry()
+  */
+ #define raw_read_seqcount_begin(s)					\
+ 	raw_read_seqcount_t_begin(__seqcount_ptr(s))
+ 
+ static inline unsigned raw_read_seqcount_t_begin(const seqcount_t *s)
+ {
+ 	unsigned ret = __read_seqcount_t_begin(s);
+ 	smp_rmb();
+ 	return ret;
+ }
+ 
+ /**
+  * read_seqcount_begin() - begin a seqcount_t read critical section
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * Return: count to be passed to read_seqcount_retry()
+  */
+ #define read_seqcount_begin(s)						\
+ 	read_seqcount_t_begin(__seqcount_ptr(s))
+ 
+ static inline unsigned read_seqcount_t_begin(const seqcount_t *s)
+ {
+ 	seqcount_lockdep_reader_access(s);
+ 	return raw_read_seqcount_t_begin(s);
+ }
+ 
+ /**
+  * raw_read_seqcount() - read the raw seqcount_t counter value
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
   *
   * raw_read_seqcount opens a read critical section of the given
 - * seqcount_t, without any lockdep checking, and without checking or
 - * masking the sequence counter LSB. Calling code is responsible for
 - * handling that.
 - *
 - * Return: count to be passed to read_seqcount_retry()
 + * seqcount without any lockdep checking and without checking or
 + * masking the LSB. Calling code is responsible for handling that.
   */
++<<<<<<< HEAD
 +static inline unsigned raw_read_seqcount(const seqcount_t *s)
++=======
+ #define raw_read_seqcount(s)						\
+ 	raw_read_seqcount_t(__seqcount_ptr(s))
+ 
+ static inline unsigned raw_read_seqcount_t(const seqcount_t *s)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
  	unsigned ret = READ_ONCE(s->sequence);
  	smp_rmb();
@@@ -153,51 -336,26 +321,56 @@@
  }
  
  /**
 - * raw_seqcount_begin() - begin a seqcount_t read critical section w/o
 - *                        lockdep and w/o counter stabilization
 - * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
 - *
 - * raw_seqcount_begin opens a read critical section of the given
 - * seqcount_t. Unlike read_seqcount_begin(), this function will not wait
 - * for the count to stabilize. If a writer is active when it begins, it
 - * will fail the read_seqcount_retry() at the end of the read critical
 - * section instead of stabilizing at the beginning of it.
 + * raw_read_seqcount_begin - start seq-read critical section w/o lockdep
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
   *
 - * Use this only in special kernel hot paths where the read section is
 - * small and has a high probability of success through other external
 - * means. It will save a single branching instruction.
 - *
 - * Return: count to be passed to read_seqcount_retry()
 + * raw_read_seqcount_begin opens a read critical section of the given
 + * seqcount, but without any lockdep checking. Validity of the critical
 + * section is tested by checking read_seqcount_retry function.
   */
++<<<<<<< HEAD
 +static inline unsigned raw_read_seqcount_begin(const seqcount_t *s)
 +{
 +	unsigned ret = __read_seqcount_begin(s);
 +	smp_rmb();
 +	return ret;
 +}
++=======
+ #define raw_seqcount_begin(s)						\
+ 	raw_seqcount_t_begin(__seqcount_ptr(s))
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
 +
 +/**
 + * read_seqcount_begin - begin a seq-read critical section
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
 + *
 + * read_seqcount_begin opens a read critical section of the given seqcount.
 + * Validity of the critical section is tested by checking read_seqcount_retry
 + * function.
 + */
 +static inline unsigned read_seqcount_begin(const seqcount_t *s)
 +{
 +	seqcount_lockdep_reader_access(s);
 +	return raw_read_seqcount_begin(s);
 +}
  
 -static inline unsigned raw_seqcount_t_begin(const seqcount_t *s)
 +/**
 + * raw_seqcount_begin - begin a seq-read critical section
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
 + *
 + * raw_seqcount_begin opens a read critical section of the given seqcount.
 + * Validity of the critical section is tested by checking read_seqcount_retry
 + * function.
 + *
 + * Unlike read_seqcount_begin(), this function will not wait for the count
 + * to stabilize. If a writer is active when we begin, we will fail the
 + * read_seqcount_retry() instead of stabilizing at the beginning of the
 + * critical section.
 + */
 +static inline unsigned raw_seqcount_begin(const seqcount_t *s)
  {
  	/*
  	 * If the counter is odd, let read_seqcount_retry() fail
@@@ -219,39 -376,70 +392,83 @@@
   *
   * Use carefully, only in critical code, and comment how the barrier is
   * provided.
 - *
 - * Return: true if a read section retry is required, else false
   */
++<<<<<<< HEAD
 +static inline int __read_seqcount_retry(const seqcount_t *s, unsigned start)
++=======
+ #define __read_seqcount_retry(s, start)					\
+ 	__read_seqcount_t_retry(__seqcount_ptr(s), start)
+ 
+ static inline int __read_seqcount_t_retry(const seqcount_t *s, unsigned start)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
  	kcsan_atomic_next(0);
  	return unlikely(READ_ONCE(s->sequence) != start);
  }
  
  /**
 - * read_seqcount_retry() - end a seqcount_t read critical section
 - * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
 - * @start: count, from read_seqcount_begin()
 + * read_seqcount_retry - end a seq-read critical section
 + * @s: pointer to seqcount_t
 + * @start: count, from read_seqcount_begin
 + * Returns: 1 if retry is required, else 0
   *
 - * read_seqcount_retry closes the read critical section of given
 - * seqcount_t.  If the critical section was invalid, it must be ignored
 - * (and typically retried).
 - *
 - * Return: true if a read section retry is required, else false
 + * read_seqcount_retry closes a read critical section of the given seqcount.
 + * If the critical section was invalid, it must be ignored (and typically
 + * retried).
   */
++<<<<<<< HEAD
 +static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)
++=======
+ #define read_seqcount_retry(s, start)					\
+ 	read_seqcount_t_retry(__seqcount_ptr(s), start)
+ 
+ static inline int read_seqcount_t_retry(const seqcount_t *s, unsigned start)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
  	smp_rmb();
 -	return __read_seqcount_t_retry(s, start);
 +	return __read_seqcount_retry(s, start);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * raw_write_seqcount_begin() - start a seqcount_t write section w/o lockdep
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  */
+ #define raw_write_seqcount_begin(s)					\
+ do {									\
+ 	if (__seqcount_lock_preemptible(s))				\
+ 		preempt_disable();					\
+ 									\
+ 	raw_write_seqcount_t_begin(__seqcount_ptr(s));			\
+ } while (0)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  
 -static inline void raw_write_seqcount_t_begin(seqcount_t *s)
 +
 +static inline void raw_write_seqcount_begin(seqcount_t *s)
  {
  	kcsan_nestable_atomic_begin();
  	s->sequence++;
  	smp_wmb();
  }
  
++<<<<<<< HEAD
 +static inline void raw_write_seqcount_end(seqcount_t *s)
++=======
+ /**
+  * raw_write_seqcount_end() - end a seqcount_t write section w/o lockdep
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  */
+ #define raw_write_seqcount_end(s)					\
+ do {									\
+ 	raw_write_seqcount_t_end(__seqcount_ptr(s));			\
+ 									\
+ 	if (__seqcount_lock_preemptible(s))				\
+ 		preempt_enable();					\
+ } while (0)
+ 
+ static inline void raw_write_seqcount_t_end(seqcount_t *s)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
  	smp_wmb();
  	s->sequence++;
@@@ -259,14 -447,84 +476,90 @@@
  }
  
  /**
 - * write_seqcount_begin_nested() - start a seqcount_t write section with
 - *                                 custom lockdep nesting level
 - * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
 - * @subclass: lockdep nesting level
 + * raw_write_seqcount_barrier - do a seq write barrier
 + * @s: pointer to seqcount_t
 + *
++<<<<<<< HEAD
 + * This can be used to provide an ordering guarantee instead of the
 + * usual consistency guarantee. It is one wmb cheaper, because we can
 + * collapse the two back-to-back wmb()s.
   *
 + * Note that, writes surrounding the barrier should be declared atomic (e.g.
++=======
+  * See Documentation/locking/lockdep-design.rst
+  */
+ #define write_seqcount_begin_nested(s, subclass)			\
+ do {									\
+ 	__seqcount_assert_lock_held(s);					\
+ 									\
+ 	if (__seqcount_lock_preemptible(s))				\
+ 		preempt_disable();					\
+ 									\
+ 	write_seqcount_t_begin_nested(__seqcount_ptr(s), subclass);	\
+ } while (0)
+ 
+ static inline void write_seqcount_t_begin_nested(seqcount_t *s, int subclass)
+ {
+ 	raw_write_seqcount_t_begin(s);
+ 	seqcount_acquire(&s->dep_map, subclass, 0, _RET_IP_);
+ }
+ 
+ /**
+  * write_seqcount_begin() - start a seqcount_t write side critical section
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * write_seqcount_begin opens a write side critical section of the given
+  * seqcount_t.
+  *
+  * Context: seqcount_t write side critical sections must be serialized and
+  * non-preemptible. If readers can be invoked from hardirq or softirq
+  * context, interrupts or bottom halves must be respectively disabled.
+  */
+ #define write_seqcount_begin(s)						\
+ do {									\
+ 	__seqcount_assert_lock_held(s);					\
+ 									\
+ 	if (__seqcount_lock_preemptible(s))				\
+ 		preempt_disable();					\
+ 									\
+ 	write_seqcount_t_begin(__seqcount_ptr(s));			\
+ } while (0)
+ 
+ static inline void write_seqcount_t_begin(seqcount_t *s)
+ {
+ 	write_seqcount_t_begin_nested(s, 0);
+ }
+ 
+ /**
+  * write_seqcount_end() - end a seqcount_t write side critical section
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * The write section must've been opened with write_seqcount_begin().
+  */
+ #define write_seqcount_end(s)						\
+ do {									\
+ 	write_seqcount_t_end(__seqcount_ptr(s));			\
+ 									\
+ 	if (__seqcount_lock_preemptible(s))				\
+ 		preempt_enable();					\
+ } while (0)
+ 
+ static inline void write_seqcount_t_end(seqcount_t *s)
+ {
+ 	seqcount_release(&s->dep_map, _RET_IP_);
+ 	raw_write_seqcount_t_end(s);
+ }
+ 
+ /**
+  * raw_write_seqcount_barrier() - do a seqcount_t write barrier
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * This can be used to provide an ordering guarantee instead of the usual
+  * consistency guarantee. It is one wmb cheaper, because it can collapse
+  * the two back-to-back wmb()s.
+  *
+  * Note that writes surrounding the barrier should be declared atomic (e.g.
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
   * via WRITE_ONCE): a) to ensure the writes become visible to other threads
   * atomically, avoiding compiler optimizations; b) to document which writes are
   * meant to propagate to the reader critical section. This is necessary because
@@@ -299,7 -557,10 +592,14 @@@
   *		WRITE_ONCE(X, false);
   *      }
   */
++<<<<<<< HEAD
 +static inline void raw_write_seqcount_barrier(seqcount_t *s)
++=======
+ #define raw_write_seqcount_barrier(s)					\
+ 	raw_write_seqcount_t_barrier(__seqcount_ptr(s))
+ 
+ static inline void raw_write_seqcount_t_barrier(seqcount_t *s)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
  	kcsan_nestable_atomic_begin();
  	s->sequence++;
@@@ -308,7 -569,44 +608,48 @@@
  	kcsan_nestable_atomic_end();
  }
  
++<<<<<<< HEAD
 +static inline int raw_read_seqcount_latch(seqcount_t *s)
++=======
+ /**
+  * write_seqcount_invalidate() - invalidate in-progress seqcount_t read
+  *                               side operations
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * After write_seqcount_invalidate, no seqcount_t read side operations
+  * will complete successfully and see data older than this.
+  */
+ #define write_seqcount_invalidate(s)					\
+ 	write_seqcount_t_invalidate(__seqcount_ptr(s))
+ 
+ static inline void write_seqcount_t_invalidate(seqcount_t *s)
+ {
+ 	smp_wmb();
+ 	kcsan_nestable_atomic_begin();
+ 	s->sequence+=2;
+ 	kcsan_nestable_atomic_end();
+ }
+ 
+ /**
+  * raw_read_seqcount_latch() - pick even/odd seqcount_t latch data copy
+  * @s: Pointer to seqcount_t or any of the seqcount_locktype_t variants
+  *
+  * Use seqcount_t latching to switch between two storage places protected
+  * by a sequence counter. Doing so allows having interruptible, preemptible,
+  * seqcount_t write side critical sections.
+  *
+  * Check raw_write_seqcount_latch() for more details and a full reader and
+  * writer usage example.
+  *
+  * Return: sequence counter raw value. Use the lowest bit as an index for
+  * picking which data copy to read. The full counter value must then be
+  * checked with read_seqcount_retry().
+  */
+ #define raw_read_seqcount_latch(s)					\
+ 	raw_read_seqcount_t_latch(__seqcount_ptr(s))
+ 
+ static inline int raw_read_seqcount_t_latch(seqcount_t *s)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
  	/* Pairs with the first smp_wmb() in raw_write_seqcount_latch() */
  	int seq = READ_ONCE(s->sequence); /* ^^^ */
@@@ -396,7 -694,10 +737,14 @@@
   *	When data is a dynamic data structure; one should use regular RCU
   *	patterns to manage the lifetimes of the objects within.
   */
++<<<<<<< HEAD
 +static inline void raw_write_seqcount_latch(seqcount_t *s)
++=======
+ #define raw_write_seqcount_latch(s)					\
+ 	raw_write_seqcount_t_latch(__seqcount_ptr(s))
+ 
+ static inline void raw_write_seqcount_t_latch(seqcount_t *s)
++>>>>>>> b5e6a027bd32 (seqcount: More consistent seqprop names)
  {
         smp_wmb();      /* prior stores before incrementing "sequence" */
         s->sequence++;
* Unmerged path include/linux/seqlock.h
