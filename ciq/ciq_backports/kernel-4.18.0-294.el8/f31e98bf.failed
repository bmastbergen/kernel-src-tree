arm64: arch_timer: mark functions as __always_inline

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Anders Roxell <anders.roxell@linaro.org>
commit f31e98bfae1c8792701ef03acd47344866cb2e14
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f31e98bf.failed

If CONFIG_FUNCTION_GRAPH_TRACER is enabled function
arch_counter_get_cntvct() is marked as notrace. However, function
__arch_counter_get_cntvct is marked as inline. If
CONFIG_OPTIMIZE_INLINING is set that will make the two functions
tracable which they shouldn't.

Rework so that functions __arch_counter_get_* are marked with
__always_inline so they will be inlined even if CONFIG_OPTIMIZE_INLINING
is turned on.

Fixes: 0ea415390cd3 ("clocksource/arm_arch_timer: Use arch_timer_read_counter to access stable counters")
	Acked-by: Marc Zyngier <marc.zyngier@arm.com>
	Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
	Signed-off-by: Will Deacon <will.deacon@arm.com>
(cherry picked from commit f31e98bfae1c8792701ef03acd47344866cb2e14)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/arch_timer.h
diff --cc arch/arm64/include/asm/arch_timer.h
index 48b2100f4aaa,50b3ab7ded4f..000000000000
--- a/arch/arm64/include/asm/arch_timer.h
+++ b/arch/arm64/include/asm/arch_timer.h
@@@ -174,30 -174,67 +174,53 @@@ static inline void arch_timer_set_cntkc
  	isb();
  }
  
++<<<<<<< HEAD
 +static inline u64 __arch_counter_get_cntpct_stable(void)
++=======
+ /*
+  * Ensure that reads of the counter are treated the same as memory reads
+  * for the purposes of ordering by subsequent memory barriers.
+  *
+  * This insanity brought to you by speculative system register reads,
+  * out-of-order memory accesses, sequence locks and Thomas Gleixner.
+  *
+  * http://lists.infradead.org/pipermail/linux-arm-kernel/2019-February/631195.html
+  */
+ #define arch_counter_enforce_ordering(val) do {				\
+ 	u64 tmp, _val = (val);						\
+ 									\
+ 	asm volatile(							\
+ 	"	eor	%0, %1, %1\n"					\
+ 	"	add	%0, sp, %0\n"					\
+ 	"	ldr	xzr, [%0]"					\
+ 	: "=r" (tmp) : "r" (_val));					\
+ } while (0)
+ 
+ static __always_inline u64 __arch_counter_get_cntpct_stable(void)
++>>>>>>> f31e98bfae1c (arm64: arch_timer: mark functions as __always_inline)
  {
 -	u64 cnt;
 -
  	isb();
 -	cnt = arch_timer_reg_read_stable(cntpct_el0);
 -	arch_counter_enforce_ordering(cnt);
 -	return cnt;
 +	return arch_timer_reg_read_stable(cntpct_el0);
  }
  
- static inline u64 __arch_counter_get_cntpct(void)
+ static __always_inline u64 __arch_counter_get_cntpct(void)
  {
 -	u64 cnt;
 -
  	isb();
 -	cnt = read_sysreg(cntpct_el0);
 -	arch_counter_enforce_ordering(cnt);
 -	return cnt;
 +	return read_sysreg(cntpct_el0);
  }
  
- static inline u64 __arch_counter_get_cntvct_stable(void)
+ static __always_inline u64 __arch_counter_get_cntvct_stable(void)
  {
 -	u64 cnt;
 -
  	isb();
 -	cnt = arch_timer_reg_read_stable(cntvct_el0);
 -	arch_counter_enforce_ordering(cnt);
 -	return cnt;
 +	return arch_timer_reg_read_stable(cntvct_el0);
  }
  
- static inline u64 __arch_counter_get_cntvct(void)
+ static __always_inline u64 __arch_counter_get_cntvct(void)
  {
 -	u64 cnt;
 -
  	isb();
 -	cnt = read_sysreg(cntvct_el0);
 -	arch_counter_enforce_ordering(cnt);
 -	return cnt;
 +	return read_sysreg(cntvct_el0);
  }
  
 -#undef arch_counter_enforce_ordering
 -
  static inline int arch_timer_arch_init(void)
  {
  	return 0;
* Unmerged path arch/arm64/include/asm/arch_timer.h
