mm: swap: fix vmstats for huge pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit 5d91f31faf8ebed2acfc3a1d6ac344f95c488d66
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/5d91f31f.failed

Many of the callbacks called by pagevec_lru_move_fn() does not correctly
update the vmstats for huge pages. Fix that. Also __pagevec_lru_add_fn()
use the irq-unsafe alternative to update the stat as the irqs are
already disabled.

	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Johannes Weiner <hannes@cmpxchg.org>
Link: http://lkml.kernel.org/r/20200527182916.249910-1-shakeelb@google.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5d91f31faf8ebed2acfc3a1d6ac344f95c488d66)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap.c
diff --cc mm/swap.c
index 70728521e27e,fa07d31184ca..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -284,8 -327,7 +284,12 @@@ static void __activate_page(struct pag
  		add_page_to_lru_list(page, lruvec, lru);
  		trace_mm_lru_activate(page);
  
++<<<<<<< HEAD
 +		__count_vm_event(PGACTIVATE);
 +		update_page_reclaim_stat(lruvec, file, 1);
++=======
+ 		__count_vm_events(PGACTIVATE, hpage_nr_pages(page));
++>>>>>>> 5d91f31faf8e (mm: swap: fix vmstats for huge pages)
  	}
  }
  
@@@ -505,8 -527,9 +509,9 @@@ void lru_cache_add_active_or_unevictabl
  static void lru_deactivate_file_fn(struct page *page, struct lruvec *lruvec,
  			      void *arg)
  {
 -	int lru;
 +	int lru, file;
  	bool active;
+ 	int nr_pages = hpage_nr_pages(page);
  
  	if (!PageLRU(page))
  		return;
@@@ -544,8 -566,7 +549,12 @@@
  	}
  
  	if (active)
++<<<<<<< HEAD
 +		__count_vm_event(PGDEACTIVATE);
 +	update_page_reclaim_stat(lruvec, file, 0);
++=======
+ 		__count_vm_events(PGDEACTIVATE, nr_pages);
++>>>>>>> 5d91f31faf8e (mm: swap: fix vmstats for huge pages)
  }
  
  static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,
@@@ -944,10 -996,8 +954,10 @@@ static void __pagevec_lru_add_fn(struc
  
  	if (page_evictable(page)) {
  		lru = page_lru(page);
 +		update_page_reclaim_stat(lruvec, page_is_file_cache(page),
 +					 PageActive(page));
  		if (was_unevictable)
- 			count_vm_event(UNEVICTABLE_PGRESCUED);
+ 			__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);
  	} else {
  		lru = LRU_UNEVICTABLE;
  		ClearPageActive(page);
* Unmerged path mm/swap.c
