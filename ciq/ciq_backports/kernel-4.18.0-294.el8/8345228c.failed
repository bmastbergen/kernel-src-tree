lib/vdso: Allow architectures to override the ns shift operation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Christophe Leroy <christophe.leroy@c-s.fr>
commit 8345228ccf31f94e3ff7ec5458ac7cc13cb323fa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/8345228c.failed

On powerpc/32, GCC (8.1) generates pretty bad code for the ns >>= vd->shift
operation taking into account that the shift is always <= 32 and the upper
part of the result is likely to be zero. GCC makes reversed assumptions
considering the shift to be likely >= 32 and the upper part to be like not
zero.

unsigned long long shift(unsigned long long x, unsigned char s)
{
	return x >> s;
}

results in:

00000018 <shift>:
  18:	35 25 ff e0 	addic.  r9,r5,-32
  1c:	41 80 00 10 	blt     2c <shift+0x14>
  20:	7c 64 4c 30 	srw     r4,r3,r9
  24:	38 60 00 00 	li      r3,0
  28:	4e 80 00 20 	blr
  2c:	54 69 08 3c 	rlwinm  r9,r3,1,0,30
  30:	21 45 00 1f 	subfic  r10,r5,31
  34:	7c 84 2c 30 	srw     r4,r4,r5
  38:	7d 29 50 30 	slw     r9,r9,r10
  3c:	7c 63 2c 30 	srw     r3,r3,r5
  40:	7d 24 23 78 	or      r4,r9,r4
  44:	4e 80 00 20 	blr

Even when forcing the shift to be smaller than 32 with an &= 31, it still
considers the shift as likely >= 32.

Move the default shift implementation into an inline which can be redefined
in architecture code via a macro.

[ tglx: Made the shift argument u32 and removed the __arch prefix ]

	Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
	Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
Link: https://lore.kernel.org/r/b3d449de856982ed060a71e6ace8eeca4654e685.1580399657.git.christophe.leroy@c-s.fr
Link: https://lkml.kernel.org/r/20200207124403.857649978@linutronix.de



(cherry picked from commit 8345228ccf31f94e3ff7ec5458ac7cc13cb323fa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/vdso/gettimeofday.c
diff --cc lib/vdso/gettimeofday.c
index 632c43443888,b95aef97501e..000000000000
--- a/lib/vdso/gettimeofday.c
+++ b/lib/vdso/gettimeofday.c
@@@ -26,24 -27,135 +26,124 @@@
  #include <asm/vdso/gettimeofday.h>
  #endif /* ENABLE_COMPAT_VDSO */
  
++<<<<<<< HEAD
 +static int do_hres(const struct vdso_data *vd, clockid_t clk,
 +		   struct __kernel_timespec *ts)
++=======
+ #ifndef vdso_calc_delta
+ /*
+  * Default implementation which works for all sane clocksources. That
+  * obviously excludes x86/TSC.
+  */
+ static __always_inline
+ u64 vdso_calc_delta(u64 cycles, u64 last, u64 mask, u32 mult)
+ {
+ 	return ((cycles - last) & mask) * mult;
+ }
+ #endif
+ 
+ #ifndef vdso_shift_ns
+ static __always_inline u64 vdso_shift_ns(u64 ns, u32 shift)
+ {
+ 	return ns >> shift;
+ }
+ #endif
+ 
+ #ifndef __arch_vdso_hres_capable
+ static inline bool __arch_vdso_hres_capable(void)
+ {
+ 	return true;
+ }
+ #endif
+ 
+ #ifndef vdso_clocksource_ok
+ static inline bool vdso_clocksource_ok(const struct vdso_data *vd)
+ {
+ 	return vd->clock_mode != VDSO_CLOCKMODE_NONE;
+ }
+ #endif
+ 
+ #ifdef CONFIG_TIME_NS
+ static int do_hres_timens(const struct vdso_data *vdns, clockid_t clk,
+ 			  struct __kernel_timespec *ts)
+ {
+ 	const struct vdso_data *vd = __arch_get_timens_vdso_data();
+ 	const struct timens_offset *offs = &vdns->offset[clk];
+ 	const struct vdso_timestamp *vdso_ts;
+ 	u64 cycles, last, ns;
+ 	u32 seq;
+ 	s64 sec;
+ 
+ 	if (clk != CLOCK_MONOTONIC_RAW)
+ 		vd = &vd[CS_HRES_COARSE];
+ 	else
+ 		vd = &vd[CS_RAW];
+ 	vdso_ts = &vd->basetime[clk];
+ 
+ 	do {
+ 		seq = vdso_read_begin(vd);
+ 
+ 		if (unlikely(!vdso_clocksource_ok(vd)))
+ 			return -1;
+ 
+ 		cycles = __arch_get_hw_counter(vd->clock_mode);
+ 		ns = vdso_ts->nsec;
+ 		last = vd->cycle_last;
+ 		ns += vdso_calc_delta(cycles, last, vd->mask, vd->mult);
+ 		ns = vdso_shift_ns(ns, vd->shift);
+ 		sec = vdso_ts->sec;
+ 	} while (unlikely(vdso_read_retry(vd, seq)));
+ 
+ 	/* Add the namespace offset */
+ 	sec += offs->sec;
+ 	ns += offs->nsec;
+ 
+ 	/*
+ 	 * Do this outside the loop: a race inside the loop could result
+ 	 * in __iter_div_u64_rem() being extremely slow.
+ 	 */
+ 	ts->tv_sec = sec + __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);
+ 	ts->tv_nsec = ns;
+ 
+ 	return 0;
+ }
+ #else
+ static __always_inline const struct vdso_data *__arch_get_timens_vdso_data(void)
+ {
+ 	return NULL;
+ }
+ 
+ static int do_hres_timens(const struct vdso_data *vdns, clockid_t clk,
+ 			  struct __kernel_timespec *ts)
+ {
+ 	return -EINVAL;
+ }
+ #endif
+ 
+ static __always_inline int do_hres(const struct vdso_data *vd, clockid_t clk,
+ 				   struct __kernel_timespec *ts)
++>>>>>>> 8345228ccf31 (lib/vdso: Allow architectures to override the ns shift operation)
  {
  	const struct vdso_timestamp *vdso_ts = &vd->basetime[clk];
  	u64 cycles, last, sec, ns;
  	u32 seq;
  
 -	/* Allows to compile the high resolution parts out */
 -	if (!__arch_vdso_hres_capable())
 -		return -1;
 -
  	do {
 -		/*
 -		 * Open coded to handle VDSO_CLOCKMODE_TIMENS. Time namespace
 -		 * enabled tasks have a special VVAR page installed which
 -		 * has vd->seq set to 1 and vd->clock_mode set to
 -		 * VDSO_CLOCKMODE_TIMENS. For non time namespace affected tasks
 -		 * this does not affect performance because if vd->seq is
 -		 * odd, i.e. a concurrent update is in progress the extra
 -		 * check for vd->clock_mode is just a few extra
 -		 * instructions while spin waiting for vd->seq to become
 -		 * even again.
 -		 */
 -		while (unlikely((seq = READ_ONCE(vd->seq)) & 1)) {
 -			if (IS_ENABLED(CONFIG_TIME_NS) &&
 -			    vd->clock_mode == VDSO_CLOCKMODE_TIMENS)
 -				return do_hres_timens(vd, clk, ts);
 -			cpu_relax();
 -		}
 -		smp_rmb();
 -
 -		if (unlikely(!vdso_clocksource_ok(vd)))
 -			return -1;
 -
 -		cycles = __arch_get_hw_counter(vd->clock_mode);
 +		seq = vdso_read_begin(vd);
 +		cycles = __arch_get_hw_counter(vd->clock_mode) &
 +			vd->mask;
  		ns = vdso_ts->nsec;
  		last = vd->cycle_last;
++<<<<<<< HEAD
 +		if (unlikely((s64)cycles < 0))
 +			return clock_gettime_fallback(clk, ts);
 +		if (cycles > last)
 +			ns += (cycles - last) * vd->mult;
 +		ns >>= vd->shift;
++=======
+ 		ns += vdso_calc_delta(cycles, last, vd->mask, vd->mult);
+ 		ns = vdso_shift_ns(ns, vd->shift);
++>>>>>>> 8345228ccf31 (lib/vdso: Allow architectures to override the ns shift operation)
  		sec = vdso_ts->sec;
  	} while (unlikely(vdso_read_retry(vd, seq)));
  
* Unmerged path lib/vdso/gettimeofday.c
