bpf: Fix bpf_ringbuf_output() signature to return long

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Andrii Nakryiko <andriin@fb.com>
commit e1613b5714ee6c186c9628e9958edf65e9d9cddd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/e1613b57.failed

Due to bpf tree fix merge, bpf_ringbuf_output() signature ended up with int as
a return type, while all other helpers got converted to returning long. So fix
it in bpf-next now.

Fixes: b0659d8a950d ("bpf: Fix definition of bpf_ringbuf_output() helper in UAPI comments")
	Signed-off-by: Andrii Nakryiko <andriin@fb.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Song Liu <songliubraving@fb.com>
Link: https://lore.kernel.org/bpf/20200727224715.652037-1-andriin@fb.com
(cherry picked from commit e1613b5714ee6c186c9628e9958edf65e9d9cddd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/uapi/linux/bpf.h
#	tools/include/uapi/linux/bpf.h
diff --cc include/uapi/linux/bpf.h
index f92a8fa8cc76,eb5e0c38eb2c..000000000000
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@@ -3127,6 -3208,187 +3127,190 @@@ union bpf_attr 
   * 		0 on success, or a negative error in case of failure:
   *
   *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
++<<<<<<< HEAD
++=======
+  *
+  * u64 bpf_sk_cgroup_id(struct bpf_sock *sk)
+  *	Description
+  *		Return the cgroup v2 id of the socket *sk*.
+  *
+  *		*sk* must be a non-**NULL** pointer to a full socket, e.g. one
+  *		returned from **bpf_sk_lookup_xxx**\ (),
+  *		**bpf_sk_fullsock**\ (), etc. The format of returned id is
+  *		same as in **bpf_skb_cgroup_id**\ ().
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		the **CONFIG_SOCK_CGROUP_DATA** configuration option.
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * u64 bpf_sk_ancestor_cgroup_id(struct bpf_sock *sk, int ancestor_level)
+  *	Description
+  *		Return id of cgroup v2 that is ancestor of cgroup associated
+  *		with the *sk* at the *ancestor_level*.  The root cgroup is at
+  *		*ancestor_level* zero and each step down the hierarchy
+  *		increments the level. If *ancestor_level* == level of cgroup
+  *		associated with *sk*, then return value will be same as that
+  *		of **bpf_sk_cgroup_id**\ ().
+  *
+  *		The helper is useful to implement policies based on cgroups
+  *		that are upper in hierarchy than immediate cgroup associated
+  *		with *sk*.
+  *
+  *		The format of returned id and helper limitations are same as in
+  *		**bpf_sk_cgroup_id**\ ().
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_ringbuf_output(void *ringbuf, void *data, u64 size, u64 flags)
+  * 	Description
+  * 		Copy *size* bytes from *data* into a ring buffer *ringbuf*.
+  * 		If **BPF_RB_NO_WAKEUP** is specified in *flags*, no notification
+  * 		of new data availability is sent.
+  * 		If **BPF_RB_FORCE_WAKEUP** is specified in *flags*, notification
+  * 		of new data availability is sent unconditionally.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * void *bpf_ringbuf_reserve(void *ringbuf, u64 size, u64 flags)
+  * 	Description
+  * 		Reserve *size* bytes of payload in a ring buffer *ringbuf*.
+  * 	Return
+  * 		Valid pointer with *size* bytes of memory available; NULL,
+  * 		otherwise.
+  *
+  * void bpf_ringbuf_submit(void *data, u64 flags)
+  * 	Description
+  * 		Submit reserved ring buffer sample, pointed to by *data*.
+  * 		If **BPF_RB_NO_WAKEUP** is specified in *flags*, no notification
+  * 		of new data availability is sent.
+  * 		If **BPF_RB_FORCE_WAKEUP** is specified in *flags*, notification
+  * 		of new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * void bpf_ringbuf_discard(void *data, u64 flags)
+  * 	Description
+  * 		Discard reserved ring buffer sample, pointed to by *data*.
+  * 		If **BPF_RB_NO_WAKEUP** is specified in *flags*, no notification
+  * 		of new data availability is sent.
+  * 		If **BPF_RB_FORCE_WAKEUP** is specified in *flags*, notification
+  * 		of new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * u64 bpf_ringbuf_query(void *ringbuf, u64 flags)
+  *	Description
+  *		Query various characteristics of provided ring buffer. What
+  *		exactly is queries is determined by *flags*:
+  *
+  *		* **BPF_RB_AVAIL_DATA**: Amount of data not yet consumed.
+  *		* **BPF_RB_RING_SIZE**: The size of ring buffer.
+  *		* **BPF_RB_CONS_POS**: Consumer position (can wrap around).
+  *		* **BPF_RB_PROD_POS**: Producer(s) position (can wrap around).
+  *
+  *		Data returned is just a momentary snapshot of actual values
+  *		and could be inaccurate, so this facility should be used to
+  *		power heuristics and for reporting, not to make 100% correct
+  *		calculation.
+  *	Return
+  *		Requested value, or 0, if *flags* are not recognized.
+  *
+  * long bpf_csum_level(struct sk_buff *skb, u64 level)
+  * 	Description
+  * 		Change the skbs checksum level by one layer up or down, or
+  * 		reset it entirely to none in order to have the stack perform
+  * 		checksum validation. The level is applicable to the following
+  * 		protocols: TCP, UDP, GRE, SCTP, FCOE. For example, a decap of
+  * 		| ETH | IP | UDP | GUE | IP | TCP | into | ETH | IP | TCP |
+  * 		through **bpf_skb_adjust_room**\ () helper with passing in
+  * 		**BPF_F_ADJ_ROOM_NO_CSUM_RESET** flag would require one	call
+  * 		to **bpf_csum_level**\ () with **BPF_CSUM_LEVEL_DEC** since
+  * 		the UDP header is removed. Similarly, an encap of the latter
+  * 		into the former could be accompanied by a helper call to
+  * 		**bpf_csum_level**\ () with **BPF_CSUM_LEVEL_INC** if the
+  * 		skb is still intended to be processed in higher layers of the
+  * 		stack instead of just egressing at tc.
+  *
+  * 		There are three supported level settings at this time:
+  *
+  * 		* **BPF_CSUM_LEVEL_INC**: Increases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_DEC**: Decreases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_RESET**: Resets skb->csum_level to 0 and
+  * 		  sets CHECKSUM_NONE to force checksum validation by the stack.
+  * 		* **BPF_CSUM_LEVEL_QUERY**: No-op, returns the current
+  * 		  skb->csum_level.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure. In the
+  * 		case of **BPF_CSUM_LEVEL_QUERY**, the current skb->csum_level
+  * 		is returned or the error code -EACCES in case the skb is not
+  * 		subject to CHECKSUM_UNNECESSARY.
+  *
+  * struct tcp6_sock *bpf_skc_to_tcp6_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_sock *bpf_skc_to_tcp_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_timewait_sock *bpf_skc_to_tcp_timewait_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_timewait_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_request_sock *bpf_skc_to_tcp_request_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_request_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct udp6_sock *bpf_skc_to_udp6_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *udp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * long bpf_get_task_stack(struct task_struct *task, void *buf, u32 size, u64 flags)
+  *	Description
+  *		Return a user or a kernel stack in bpf program provided buffer.
+  *		To achieve this, the helper needs *task*, which is a valid
+  *		pointer to struct task_struct. To store the stacktrace, the
+  *		bpf program provides *buf* with	a nonnegative *size*.
+  *
+  *		The last argument, *flags*, holds the number of stack frames to
+  *		skip (from 0 to 255), masked with
+  *		**BPF_F_SKIP_FIELD_MASK**. The next bits can be used to set
+  *		the following flags:
+  *
+  *		**BPF_F_USER_STACK**
+  *			Collect a user space stack instead of a kernel stack.
+  *		**BPF_F_USER_BUILD_ID**
+  *			Collect buildid+offset instead of ips for user stack,
+  *			only valid if **BPF_F_USER_STACK** is also specified.
+  *
+  *		**bpf_get_task_stack**\ () can collect up to
+  *		**PERF_MAX_STACK_DEPTH** both kernel and user frames, subject
+  *		to sufficient large buffer size. Note that
+  *		this limit can be controlled with the **sysctl** program, and
+  *		that it should be manually increased in order to profile long
+  *		user stacks (such as stacks for Java programs). To do so, use:
+  *
+  *		::
+  *
+  *			# sysctl kernel.perf_event_max_stack=<new value>
+  *	Return
+  *		A non-negative value equal to or less than *size* on success,
+  *		or a negative error in case of failure.
+  *
++>>>>>>> e1613b5714ee (bpf: Fix bpf_ringbuf_output() signature to return long)
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
diff --cc tools/include/uapi/linux/bpf.h
index 9f6d3977ecf3,eb5e0c38eb2c..000000000000
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@@ -3081,25 -3186,209 +3081,211 @@@ union bpf_attr 
   *		valid address but requiring a major memory fault. If reading kernel memory
   *		fails, the string for **%s** will be an empty string, and the ip
   *		address for **%p{i,I}{4,6}** will be 0. Not returning error to
 - *		bpf program is consistent with what **bpf_trace_printk**\ () does for now.
 + *		bpf program is consistent with what bpf_trace_printk() does for now.
   * 	Return
 - * 		0 on success, or a negative error in case of failure:
 - *
 - *		**-EBUSY** if per-CPU memory copy buffer is busy, can try again
 - *		by returning 1 from bpf program.
 + * 		0 on success, or a negative errno in case of failure.
   *
 - *		**-EINVAL** if arguments are invalid, or if *fmt* is invalid/unsupported.
 + *		* **-EBUSY**		Percpu memory copy buffer is busy, can try again
 + *					by returning 1 from bpf program.
 + *		* **-EINVAL**		Invalid arguments, or invalid/unsupported formats.
 + *		* **-E2BIG**		Too many format specifiers.
 + *		* **-EOVERFLOW**	Overflow happens, the same object will be tried again.
   *
 - *		**-E2BIG** if *fmt* contains too many format specifiers.
 - *
 - *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
 - *
 - * long bpf_seq_write(struct seq_file *m, const void *data, u32 len)
 + * int bpf_seq_write(struct seq_file *m, const void *data, u32 len)
   * 	Description
 - * 		**bpf_seq_write**\ () uses seq_file **seq_write**\ () to write the data.
 + * 		seq_write uses seq_file seq_write() to write the data.
   * 		The *m* represents the seq_file. The *data* and *len* represent the
 - * 		data to write in bytes.
 + *		data to write in bytes.
   * 	Return
++<<<<<<< HEAD
 + * 		0 on success, or a negative errno in case of failure.
++=======
+  * 		0 on success, or a negative error in case of failure:
+  *
+  *		**-EOVERFLOW** if an overflow happened: The same object will be tried again.
+  *
+  * u64 bpf_sk_cgroup_id(struct bpf_sock *sk)
+  *	Description
+  *		Return the cgroup v2 id of the socket *sk*.
+  *
+  *		*sk* must be a non-**NULL** pointer to a full socket, e.g. one
+  *		returned from **bpf_sk_lookup_xxx**\ (),
+  *		**bpf_sk_fullsock**\ (), etc. The format of returned id is
+  *		same as in **bpf_skb_cgroup_id**\ ().
+  *
+  *		This helper is available only if the kernel was compiled with
+  *		the **CONFIG_SOCK_CGROUP_DATA** configuration option.
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * u64 bpf_sk_ancestor_cgroup_id(struct bpf_sock *sk, int ancestor_level)
+  *	Description
+  *		Return id of cgroup v2 that is ancestor of cgroup associated
+  *		with the *sk* at the *ancestor_level*.  The root cgroup is at
+  *		*ancestor_level* zero and each step down the hierarchy
+  *		increments the level. If *ancestor_level* == level of cgroup
+  *		associated with *sk*, then return value will be same as that
+  *		of **bpf_sk_cgroup_id**\ ().
+  *
+  *		The helper is useful to implement policies based on cgroups
+  *		that are upper in hierarchy than immediate cgroup associated
+  *		with *sk*.
+  *
+  *		The format of returned id and helper limitations are same as in
+  *		**bpf_sk_cgroup_id**\ ().
+  *	Return
+  *		The id is returned or 0 in case the id could not be retrieved.
+  *
+  * long bpf_ringbuf_output(void *ringbuf, void *data, u64 size, u64 flags)
+  * 	Description
+  * 		Copy *size* bytes from *data* into a ring buffer *ringbuf*.
+  * 		If **BPF_RB_NO_WAKEUP** is specified in *flags*, no notification
+  * 		of new data availability is sent.
+  * 		If **BPF_RB_FORCE_WAKEUP** is specified in *flags*, notification
+  * 		of new data availability is sent unconditionally.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure.
+  *
+  * void *bpf_ringbuf_reserve(void *ringbuf, u64 size, u64 flags)
+  * 	Description
+  * 		Reserve *size* bytes of payload in a ring buffer *ringbuf*.
+  * 	Return
+  * 		Valid pointer with *size* bytes of memory available; NULL,
+  * 		otherwise.
+  *
+  * void bpf_ringbuf_submit(void *data, u64 flags)
+  * 	Description
+  * 		Submit reserved ring buffer sample, pointed to by *data*.
+  * 		If **BPF_RB_NO_WAKEUP** is specified in *flags*, no notification
+  * 		of new data availability is sent.
+  * 		If **BPF_RB_FORCE_WAKEUP** is specified in *flags*, notification
+  * 		of new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * void bpf_ringbuf_discard(void *data, u64 flags)
+  * 	Description
+  * 		Discard reserved ring buffer sample, pointed to by *data*.
+  * 		If **BPF_RB_NO_WAKEUP** is specified in *flags*, no notification
+  * 		of new data availability is sent.
+  * 		If **BPF_RB_FORCE_WAKEUP** is specified in *flags*, notification
+  * 		of new data availability is sent unconditionally.
+  * 	Return
+  * 		Nothing. Always succeeds.
+  *
+  * u64 bpf_ringbuf_query(void *ringbuf, u64 flags)
+  *	Description
+  *		Query various characteristics of provided ring buffer. What
+  *		exactly is queries is determined by *flags*:
+  *
+  *		* **BPF_RB_AVAIL_DATA**: Amount of data not yet consumed.
+  *		* **BPF_RB_RING_SIZE**: The size of ring buffer.
+  *		* **BPF_RB_CONS_POS**: Consumer position (can wrap around).
+  *		* **BPF_RB_PROD_POS**: Producer(s) position (can wrap around).
+  *
+  *		Data returned is just a momentary snapshot of actual values
+  *		and could be inaccurate, so this facility should be used to
+  *		power heuristics and for reporting, not to make 100% correct
+  *		calculation.
+  *	Return
+  *		Requested value, or 0, if *flags* are not recognized.
+  *
+  * long bpf_csum_level(struct sk_buff *skb, u64 level)
+  * 	Description
+  * 		Change the skbs checksum level by one layer up or down, or
+  * 		reset it entirely to none in order to have the stack perform
+  * 		checksum validation. The level is applicable to the following
+  * 		protocols: TCP, UDP, GRE, SCTP, FCOE. For example, a decap of
+  * 		| ETH | IP | UDP | GUE | IP | TCP | into | ETH | IP | TCP |
+  * 		through **bpf_skb_adjust_room**\ () helper with passing in
+  * 		**BPF_F_ADJ_ROOM_NO_CSUM_RESET** flag would require one	call
+  * 		to **bpf_csum_level**\ () with **BPF_CSUM_LEVEL_DEC** since
+  * 		the UDP header is removed. Similarly, an encap of the latter
+  * 		into the former could be accompanied by a helper call to
+  * 		**bpf_csum_level**\ () with **BPF_CSUM_LEVEL_INC** if the
+  * 		skb is still intended to be processed in higher layers of the
+  * 		stack instead of just egressing at tc.
+  *
+  * 		There are three supported level settings at this time:
+  *
+  * 		* **BPF_CSUM_LEVEL_INC**: Increases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_DEC**: Decreases skb->csum_level for skbs
+  * 		  with CHECKSUM_UNNECESSARY.
+  * 		* **BPF_CSUM_LEVEL_RESET**: Resets skb->csum_level to 0 and
+  * 		  sets CHECKSUM_NONE to force checksum validation by the stack.
+  * 		* **BPF_CSUM_LEVEL_QUERY**: No-op, returns the current
+  * 		  skb->csum_level.
+  * 	Return
+  * 		0 on success, or a negative error in case of failure. In the
+  * 		case of **BPF_CSUM_LEVEL_QUERY**, the current skb->csum_level
+  * 		is returned or the error code -EACCES in case the skb is not
+  * 		subject to CHECKSUM_UNNECESSARY.
+  *
+  * struct tcp6_sock *bpf_skc_to_tcp6_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_sock *bpf_skc_to_tcp_sock(void *sk)
+  *	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_timewait_sock *bpf_skc_to_tcp_timewait_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_timewait_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct tcp_request_sock *bpf_skc_to_tcp_request_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *tcp_request_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * struct udp6_sock *bpf_skc_to_udp6_sock(void *sk)
+  * 	Description
+  *		Dynamically cast a *sk* pointer to a *udp6_sock* pointer.
+  *	Return
+  *		*sk* if casting is valid, or NULL otherwise.
+  *
+  * long bpf_get_task_stack(struct task_struct *task, void *buf, u32 size, u64 flags)
+  *	Description
+  *		Return a user or a kernel stack in bpf program provided buffer.
+  *		To achieve this, the helper needs *task*, which is a valid
+  *		pointer to struct task_struct. To store the stacktrace, the
+  *		bpf program provides *buf* with	a nonnegative *size*.
+  *
+  *		The last argument, *flags*, holds the number of stack frames to
+  *		skip (from 0 to 255), masked with
+  *		**BPF_F_SKIP_FIELD_MASK**. The next bits can be used to set
+  *		the following flags:
+  *
+  *		**BPF_F_USER_STACK**
+  *			Collect a user space stack instead of a kernel stack.
+  *		**BPF_F_USER_BUILD_ID**
+  *			Collect buildid+offset instead of ips for user stack,
+  *			only valid if **BPF_F_USER_STACK** is also specified.
+  *
+  *		**bpf_get_task_stack**\ () can collect up to
+  *		**PERF_MAX_STACK_DEPTH** both kernel and user frames, subject
+  *		to sufficient large buffer size. Note that
+  *		this limit can be controlled with the **sysctl** program, and
+  *		that it should be manually increased in order to profile long
+  *		user stacks (such as stacks for Java programs). To do so, use:
+  *
+  *		::
+  *
+  *			# sysctl kernel.perf_event_max_stack=<new value>
+  *	Return
+  *		A non-negative value equal to or less than *size* on success,
+  *		or a negative error in case of failure.
++>>>>>>> e1613b5714ee (bpf: Fix bpf_ringbuf_output() signature to return long)
   *
 + *		* **-EOVERFLOW**	Overflow happens, the same object will be tried again.
   */
  #define __BPF_FUNC_MAPPER(FN)		\
  	FN(unspec),			\
* Unmerged path include/uapi/linux/bpf.h
* Unmerged path tools/include/uapi/linux/bpf.h
