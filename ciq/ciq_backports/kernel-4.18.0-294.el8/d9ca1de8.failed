mptcp: move page frag allocation in mptcp_sendmsg()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Paolo Abeni <pabeni@redhat.com>
commit d9ca1de8c0cd7a8ca2a0506e1741418741848e53
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/d9ca1de8.failed

mptcp_sendmsg() is refactored so that first it copies
the data provided from user space into the send queue,
and then tries to spool the send queue via sendmsg_frag.

There a subtle change in the mptcp level collapsing on
consecutive data fragment: we now allow that only on unsent
data.

The latter don't need to deal with msghdr data anymore
and can be simplified in a relevant way.

snd_nxt and write_seq are now tracked independently.

Overall this allows some relevant cleanup and will
allow sending pending mptcp data on msk una update in
later patch.

Co-developed-by: Florian Westphal <fw@strlen.de>
	Signed-off-by: Florian Westphal <fw@strlen.de>
	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit d9ca1de8c0cd7a8ca2a0506e1741418741848e53)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/mptcp/protocol.c
diff --cc net/mptcp/protocol.c
index d41791292d73,9b30c4b39159..000000000000
--- a/net/mptcp/protocol.c
+++ b/net/mptcp/protocol.c
@@@ -39,6 -42,9 +39,12 @@@ struct mptcp_skb_cb 
  
  static struct percpu_counter mptcp_sockets_allocated;
  
++<<<<<<< HEAD
++=======
+ static void __mptcp_destroy_sock(struct sock *sk);
+ static void __mptcp_check_send_data_fin(struct sock *sk);
+ 
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  /* If msk has an initial subflow socket, and the MP_CAPABLE handshake has not
   * completed yet or has failed, return the subflow socket.
   * Otherwise return NULL.
@@@ -709,104 -954,56 +731,104 @@@ mptcp_carve_data_frag(const struct mptc
  	return dfrag;
  }
  
++<<<<<<< HEAD
 +static int mptcp_sendmsg_frag(struct sock *sk, struct sock *ssk,
 +			      struct msghdr *msg, struct mptcp_data_frag *dfrag,
 +			      long *timeo, int *pmss_now,
 +			      int *ps_goal)
 +{
 +	int mss_now, avail_size, size_goal, offset, ret, frag_truesize = 0;
 +	bool dfrag_collapsed, can_collapse = false;
++=======
+ struct mptcp_sendmsg_info {
+ 	int mss_now;
+ 	int size_goal;
+ 	u16 limit;
+ 	u16 sent;
+ 	unsigned int flags;
+ };
+ 
+ static int mptcp_sendmsg_frag(struct sock *sk, struct sock *ssk,
+ 			      struct mptcp_data_frag *dfrag,
+ 			      struct mptcp_sendmsg_info *info)
+ {
+ 	u64 data_seq = dfrag->data_seq + info->sent;
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  	struct mptcp_sock *msk = mptcp_sk(sk);
  	struct mptcp_ext *mpext = NULL;
- 	bool retransmission = !!dfrag;
  	struct sk_buff *skb, *tail;
- 	struct page_frag *pfrag;
- 	struct page *page;
- 	u64 *write_seq;
- 	size_t psize;
+ 	bool can_collapse = false;
+ 	int avail_size;
+ 	size_t ret;
  
++<<<<<<< HEAD
 +	/* use the mptcp page cache so that we can easily move the data
 +	 * from one substream to another, but do per subflow memory accounting
 +	 * Note: pfrag is used only !retransmission, but the compiler if
 +	 * fooled into a warning if we don't init here
 +	 */
 +	pfrag = sk_page_frag(sk);
 +	while ((!retransmission && !mptcp_page_frag_refill(ssk, pfrag)) ||
 +	       !mptcp_ext_cache_refill(msk)) {
 +		ret = sk_stream_wait_memory(ssk, timeo);
 +		if (ret)
 +			return ret;
 +
 +		/* if sk_stream_wait_memory() sleeps snd_una can change
 +		 * significantly, refresh the rtx queue
 +		 */
 +		mptcp_clean_una(sk);
 +	}
 +	if (!retransmission) {
 +		write_seq = &msk->write_seq;
 +		page = pfrag->page;
 +	} else {
 +		write_seq = &dfrag->data_seq;
 +		page = dfrag->page;
 +	}
 +
 +	/* compute copy limit */
 +	mss_now = tcp_send_mss(ssk, &size_goal, msg->msg_flags);
 +	*pmss_now = mss_now;
 +	*ps_goal = size_goal;
 +	avail_size = size_goal;
++=======
+ 	pr_debug("msk=%p ssk=%p sending dfrag at seq=%lld len=%d already sent=%d",
+ 		 msk, ssk, dfrag->data_seq, dfrag->data_len, info->sent);
+ 
+ 	/* compute send limit */
+ 	info->mss_now = tcp_send_mss(ssk, &info->size_goal, info->flags);
+ 	avail_size = info->size_goal;
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  	skb = tcp_write_queue_tail(ssk);
  	if (skb) {
- 		mpext = skb_ext_find(skb, SKB_EXT_MPTCP);
- 
  		/* Limit the write to the size available in the
  		 * current skb, if any, so that we create at most a new skb.
  		 * Explicitly tells TCP internals to avoid collapsing on later
  		 * queue management operation, to avoid breaking the ext <->
  		 * SSN association set here
  		 */
++<<<<<<< HEAD
 +		can_collapse = (size_goal - skb->len > 0) &&
 +			      mptcp_skb_can_collapse_to(*write_seq, skb, mpext);
++=======
+ 		mpext = skb_ext_find(skb, SKB_EXT_MPTCP);
+ 		can_collapse = (info->size_goal - skb->len > 0) &&
+ 			 mptcp_skb_can_collapse_to(data_seq, skb, mpext);
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  		if (!can_collapse)
  			TCP_SKB_CB(skb)->eor = 1;
  		else
 -			avail_size = info->size_goal - skb->len;
 +			avail_size = size_goal - skb->len;
  	}
  
- 	if (!retransmission) {
- 		/* reuse tail pfrag, if possible, or carve a new one from the
- 		 * page allocator
- 		 */
- 		dfrag = mptcp_rtx_tail(sk);
- 		offset = pfrag->offset;
- 		dfrag_collapsed = mptcp_frag_can_collapse_to(msk, pfrag, dfrag);
- 		if (!dfrag_collapsed) {
- 			dfrag = mptcp_carve_data_frag(msk, pfrag, offset);
- 			offset = dfrag->offset;
- 			frag_truesize = dfrag->overhead;
- 		}
- 		psize = min_t(size_t, pfrag->size - offset, avail_size);
- 
- 		/* Copy to page */
- 		pr_debug("left=%zu", msg_data_left(msg));
- 		psize = copy_page_from_iter(pfrag->page, offset,
- 					    min_t(size_t, msg_data_left(msg),
- 						  psize),
- 					    &msg->msg_iter);
- 		pr_debug("left=%zu", msg_data_left(msg));
- 		if (!psize)
- 			return -EINVAL;
- 
- 		if (!sk_wmem_schedule(sk, psize + dfrag->overhead)) {
- 			iov_iter_revert(&msg->msg_iter, psize);
- 			return -ENOMEM;
- 		}
- 	} else {
- 		offset = dfrag->offset;
- 		psize = min_t(size_t, dfrag->data_len, avail_size);
- 	}
+ 	if (WARN_ON_ONCE(info->sent > info->limit ||
+ 			 info->limit > dfrag->data_len))
+ 		return 0;
  
- 	tail = tcp_build_frag(ssk, psize, msg->msg_flags, page, offset, &psize);
+ 	ret = info->limit - info->sent;
+ 	tail = tcp_build_frag(ssk, avail_size, info->flags, dfrag->page,
+ 			      dfrag->offset + info->sent, &ret);
  	if (!tail) {
  		tcp_remove_empty_skb(sk, tcp_write_queue_tail(ssk));
  		return -ENOMEM;
@@@ -924,13 -1160,86 +918,93 @@@ static void ssk_check_wmem(struct mptcp
  		mptcp_nospace(msk);
  }
  
+ static void mptcp_push_release(struct sock *sk, struct sock *ssk,
+ 			       struct mptcp_sendmsg_info *info)
+ {
+ 	mptcp_set_timeout(sk, ssk);
+ 	tcp_push(ssk, 0, info->mss_now, tcp_sk(ssk)->nonagle, info->size_goal);
+ 	release_sock(ssk);
+ }
+ 
+ static void mptcp_push_pending(struct sock *sk, unsigned int flags)
+ {
+ 	struct sock *prev_ssk = NULL, *ssk = NULL;
+ 	struct mptcp_sock *msk = mptcp_sk(sk);
+ 	struct mptcp_sendmsg_info info = {
+ 				.flags = flags,
+ 	};
+ 	struct mptcp_data_frag *dfrag;
+ 	int len, copied = 0;
+ 	u32 sndbuf;
+ 
+ 	while ((dfrag = mptcp_send_head(sk))) {
+ 		info.sent = dfrag->already_sent;
+ 		info.limit = dfrag->data_len;
+ 		len = dfrag->data_len - dfrag->already_sent;
+ 		while (len > 0) {
+ 			int ret = 0;
+ 
+ 			prev_ssk = ssk;
+ 			__mptcp_flush_join_list(msk);
+ 			ssk = mptcp_subflow_get_send(msk, &sndbuf);
+ 
+ 			/* do auto tuning */
+ 			if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK) &&
+ 			    sndbuf > READ_ONCE(sk->sk_sndbuf))
+ 				WRITE_ONCE(sk->sk_sndbuf, sndbuf);
+ 
+ 			/* try to keep the subflow socket lock across
+ 			 * consecutive xmit on the same socket
+ 			 */
+ 			if (ssk != prev_ssk && prev_ssk)
+ 				mptcp_push_release(sk, prev_ssk, &info);
+ 			if (!ssk)
+ 				goto out;
+ 
+ 			if (ssk != prev_ssk || !prev_ssk)
+ 				lock_sock(ssk);
+ 
+ 			ret = mptcp_sendmsg_frag(sk, ssk, dfrag, &info);
+ 			if (ret <= 0) {
+ 				mptcp_push_release(sk, ssk, &info);
+ 				goto out;
+ 			}
+ 
+ 			info.sent += ret;
+ 			dfrag->already_sent += ret;
+ 			msk->snd_nxt += ret;
+ 			msk->snd_burst -= ret;
+ 			copied += ret;
+ 			len -= ret;
+ 		}
+ 		WRITE_ONCE(msk->first_pending, mptcp_send_next(sk));
+ 	}
+ 
+ 	/* at this point we held the socket lock for the last subflow we used */
+ 	if (ssk)
+ 		mptcp_push_release(sk, ssk, &info);
+ 
+ out:
+ 	/* start the timer, if it's not pending */
+ 	if (!mptcp_timer_pending(sk))
+ 		mptcp_reset_timer(sk);
+ 	if (copied)
+ 		__mptcp_check_send_data_fin(sk);
+ }
+ 
  static int mptcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
  {
 +	int mss_now = 0, size_goal = 0, ret = 0;
  	struct mptcp_sock *msk = mptcp_sk(sk);
++<<<<<<< HEAD
 +	size_t copied = 0;
 +	struct sock *ssk;
 +	bool tx_ok;
++=======
+ 	struct page_frag *pfrag;
+ 	size_t copied = 0;
+ 	int ret = 0;
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  	long timeo;
  
  	if (msg->msg_flags & ~(MSG_MORE | MSG_DONTWAIT | MSG_NOSIGNAL))
@@@ -946,118 -1255,94 +1020,200 @@@
  			goto out;
  	}
  
++<<<<<<< HEAD
 +restart:
++=======
+ 	pfrag = sk_page_frag(sk);
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  	mptcp_clean_una(sk);
  
- 	if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN)) {
- 		ret = -EPIPE;
- 		goto out;
- 	}
+ 	while (msg_data_left(msg)) {
+ 		struct mptcp_data_frag *dfrag;
+ 		int frag_truesize = 0;
+ 		bool dfrag_collapsed;
+ 		size_t psize, offset;
  
++<<<<<<< HEAD
 +	__mptcp_flush_join_list(msk);
 +	ssk = mptcp_subflow_get_send(msk);
 +	while (!sk_stream_memory_free(sk) || !ssk) {
 +		if (ssk) {
 +			/* make sure retransmit timer is
 +			 * running before we wait for memory.
 +			 *
 +			 * The retransmit timer might be needed
 +			 * to make the peer send an up-to-date
 +			 * MPTCP Ack.
 +			 */
 +			mptcp_set_timeout(sk, ssk);
 +			if (!mptcp_timer_pending(sk))
 +				mptcp_reset_timer(sk);
++=======
+ 		if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN)) {
+ 			ret = -EPIPE;
+ 			goto out;
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
+ 		}
+ 
+ 		/* reuse tail pfrag, if possible, or carve a new one from the
+ 		 * page allocator
+ 		 */
+ 		dfrag = mptcp_pending_tail(sk);
+ 		dfrag_collapsed = mptcp_frag_can_collapse_to(msk, pfrag, dfrag);
+ 		if (!dfrag_collapsed) {
+ 			if (!sk_stream_memory_free(sk)) {
+ 				mptcp_push_pending(sk, msg->msg_flags);
+ 				if (!sk_stream_memory_free(sk))
+ 					goto wait_for_memory;
+ 			}
+ 			if (!mptcp_page_frag_refill(sk, pfrag))
+ 				goto wait_for_memory;
+ 
+ 			dfrag = mptcp_carve_data_frag(msk, pfrag, pfrag->offset);
+ 			frag_truesize = dfrag->overhead;
  		}
  
+ 		/* we do not bound vs wspace, to allow a single packet.
+ 		 * memory accounting will prevent execessive memory usage
+ 		 * anyway
+ 		 */
+ 		offset = dfrag->offset + dfrag->data_len;
+ 		psize = pfrag->size - offset;
+ 		psize = min_t(size_t, psize, msg_data_left(msg));
+ 		if (!sk_wmem_schedule(sk, psize + frag_truesize))
+ 			goto wait_for_memory;
+ 
+ 		if (copy_page_from_iter(dfrag->page, offset, psize,
+ 					&msg->msg_iter) != psize) {
+ 			ret = -EFAULT;
+ 			goto out;
+ 		}
+ 
+ 		/* data successfully copied into the write queue */
+ 		copied += psize;
+ 		dfrag->data_len += psize;
+ 		frag_truesize += psize;
+ 		pfrag->offset += frag_truesize;
+ 		WRITE_ONCE(msk->write_seq, msk->write_seq + psize);
+ 
+ 		/* charge data on mptcp pending queue to the msk socket
+ 		 * Note: we charge such data both to sk and ssk
+ 		 */
+ 		sk_wmem_queued_add(sk, frag_truesize);
+ 		sk->sk_forward_alloc -= frag_truesize;
+ 		if (!dfrag_collapsed) {
+ 			get_page(dfrag->page);
+ 			list_add_tail(&dfrag->list, &msk->rtx_queue);
+ 			if (!msk->first_pending)
+ 				WRITE_ONCE(msk->first_pending, dfrag);
+ 		}
+ 		pr_debug("msk=%p dfrag at seq=%lld len=%d sent=%d new=%d", msk,
+ 			 dfrag->data_seq, dfrag->data_len, dfrag->already_sent,
+ 			 !dfrag_collapsed);
+ 
+ 		if (!mptcp_ext_cache_refill(msk))
+ 			goto wait_for_memory;
+ 		continue;
+ 
+ wait_for_memory:
  		mptcp_nospace(msk);
+ 		mptcp_clean_una(sk);
+ 		if (mptcp_timer_pending(sk))
+ 			mptcp_reset_timer(sk);
  		ret = sk_stream_wait_memory(sk, &timeo);
  		if (ret)
  			goto out;
++<<<<<<< HEAD
 +
 +		mptcp_clean_una(sk);
 +
 +		ssk = mptcp_subflow_get_send(msk);
 +		if (list_empty(&msk->conn_list)) {
 +			ret = -ENOTCONN;
 +			goto out;
 +		}
 +	}
 +
 +	pr_debug("conn_list->subflow=%p", ssk);
 +
 +	lock_sock(ssk);
 +	tx_ok = msg_data_left(msg);
 +	while (tx_ok) {
 +		ret = mptcp_sendmsg_frag(sk, ssk, msg, NULL, &timeo, &mss_now,
 +					 &size_goal);
 +		if (ret < 0) {
 +			if (ret == -EAGAIN && timeo > 0) {
 +				mptcp_set_timeout(sk, ssk);
 +				release_sock(ssk);
 +				goto restart;
 +			}
 +			break;
 +		}
 +
 +		copied += ret;
 +
 +		tx_ok = msg_data_left(msg);
 +		if (!tx_ok)
 +			break;
 +
 +		if (!sk_stream_memory_free(ssk) ||
 +		    !mptcp_ext_cache_refill(msk)) {
 +			tcp_push(ssk, msg->msg_flags, mss_now,
 +				 tcp_sk(ssk)->nonagle, size_goal);
 +			mptcp_set_timeout(sk, ssk);
 +			release_sock(ssk);
 +			goto restart;
 +		}
 +
 +		/* memory is charged to mptcp level socket as well, i.e.
 +		 * if msg is very large, mptcp socket may run out of buffer
 +		 * space.  mptcp_clean_una() will release data that has
 +		 * been acked at mptcp level in the mean time, so there is
 +		 * a good chance we can continue sending data right away.
 +		 *
 +		 * Normally, when the tcp subflow can accept more data, then
 +		 * so can the MPTCP socket.  However, we need to cope with
 +		 * peers that might lag behind in their MPTCP-level
 +		 * acknowledgements, i.e.  data might have been acked at
 +		 * tcp level only.  So, we must also check the MPTCP socket
 +		 * limits before we send more data.
 +		 */
 +		if (unlikely(!sk_stream_memory_free(sk))) {
 +			tcp_push(ssk, msg->msg_flags, mss_now,
 +				 tcp_sk(ssk)->nonagle, size_goal);
 +			mptcp_clean_una(sk);
 +			if (!sk_stream_memory_free(sk)) {
 +				/* can't send more for now, need to wait for
 +				 * MPTCP-level ACKs from peer.
 +				 *
 +				 * Wakeup will happen via mptcp_clean_una().
 +				 */
 +				mptcp_set_timeout(sk, ssk);
 +				release_sock(ssk);
 +				goto restart;
 +			}
 +		}
 +	}
 +
 +	mptcp_set_timeout(sk, ssk);
 +	if (copied) {
 +		tcp_push(ssk, msg->msg_flags, mss_now, tcp_sk(ssk)->nonagle,
 +			 size_goal);
 +
 +		/* start the timer, if it's not pending */
 +		if (!mptcp_timer_pending(sk))
 +			mptcp_reset_timer(sk);
 +	}
 +
 +	release_sock(ssk);
++=======
+ 	}
+ 
+ 	if (copied)
+ 		mptcp_push_pending(sk, msg->msg_flags);
+ 
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  out:
- 	msk->snd_nxt = msk->write_seq;
  	ssk_check_wmem(msk);
  	release_sock(sk);
  	return copied ? : ret;
@@@ -1373,6 -1680,9 +1529,12 @@@ static struct sock *mptcp_subflow_get_r
  
  	sock_owned_by_me((const struct sock *)msk);
  
++<<<<<<< HEAD
++=======
+ 	if (__mptcp_check_fallback(msk))
+ 		return NULL;
+ 
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  	mptcp_for_each_subflow(msk, subflow) {
  		struct sock *ssk = mptcp_subflow_tcp_sock(subflow);
  
@@@ -1426,16 -1822,17 +1588,21 @@@ static void mptcp_worker(struct work_st
  {
  	struct mptcp_sock *msk = container_of(work, struct mptcp_sock, work);
  	struct sock *ssk, *sk = &msk->sk.icsk_inet.sk;
 -	struct mptcp_sendmsg_info info = {};
 +	int orig_len, orig_offset, mss_now = 0, size_goal = 0;
  	struct mptcp_data_frag *dfrag;
++<<<<<<< HEAD
 +	u64 orig_write_seq;
 +	size_t copied = 0;
 +	struct msghdr msg = {
 +		.msg_flags = MSG_DONTWAIT,
 +	};
 +	long timeo = 0;
++=======
+ 	size_t copied = 0;
+ 	int state, ret;
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  
  	lock_sock(sk);
 -	set_bit(MPTCP_WORKER_RUNNING, &msk->flags);
 -	state = sk->sk_state;
 -	if (unlikely(state == TCP_CLOSE))
 -		goto unlock;
 -
  	mptcp_clean_una_wakeup(sk);
  	mptcp_check_data_fin_ack(sk);
  	__mptcp_flush_join_list(msk);
@@@ -1462,12 -1877,11 +1629,20 @@@
  
  	lock_sock(ssk);
  
++<<<<<<< HEAD
 +	orig_len = dfrag->data_len;
 +	orig_offset = dfrag->offset;
 +	orig_write_seq = dfrag->data_seq;
 +	while (dfrag->data_len > 0) {
 +		int ret = mptcp_sendmsg_frag(sk, ssk, &msg, dfrag, &timeo,
 +					     &mss_now, &size_goal);
++=======
+ 	/* limit retransmission to the bytes already sent on some subflows */
+ 	info.sent = 0;
+ 	info.limit = dfrag->already_sent;
+ 	while (info.sent < dfrag->already_sent) {
+ 		ret = mptcp_sendmsg_frag(sk, ssk, dfrag, &info);
++>>>>>>> d9ca1de8c0cd (mptcp: move page frag allocation in mptcp_sendmsg())
  		if (ret < 0)
  			break;
  
@@@ -1480,13 -1893,9 +1654,9 @@@
  			break;
  	}
  	if (copied)
 -		tcp_push(ssk, 0, info.mss_now, tcp_sk(ssk)->nonagle,
 -			 info.size_goal);
 +		tcp_push(ssk, msg.msg_flags, mss_now, tcp_sk(ssk)->nonagle,
 +			 size_goal);
  
- 	dfrag->data_seq = orig_write_seq;
- 	dfrag->offset = orig_offset;
- 	dfrag->data_len = orig_len;
- 
  	mptcp_set_timeout(sk, ssk);
  	release_sock(ssk);
  
* Unmerged path net/mptcp/protocol.c
