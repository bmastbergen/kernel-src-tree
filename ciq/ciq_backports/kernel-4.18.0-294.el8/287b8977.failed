powerpc/pseries/cmm: Use adjust_managed_page_count() insted of totalram_pages_*

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author David Hildenbrand <david@redhat.com>
commit 287b89773d8172df049f0f4c27946b2ae4ac4b41
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/287b8977.failed

adjust_managed_page_count() performs a totalram_pages_add(), but also
adjusts the managed pages of the zone. Let's use that instead, similar
to virtio-balloon. Use it before freeing a page.

	Signed-off-by: David Hildenbrand <david@redhat.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20191031142933.10779-6-david@redhat.com
(cherry picked from commit 287b89773d8172df049f0f4c27946b2ae4ac4b41)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/pseries/cmm.c
diff --cc arch/powerpc/platforms/pseries/cmm.c
index 317181692fcb,f82c468ca2c4..000000000000
--- a/arch/powerpc/platforms/pseries/cmm.c
+++ b/arch/powerpc/platforms/pseries/cmm.c
@@@ -206,9 -162,9 +206,13 @@@ static long cmm_alloc_pages(long nr
  			break;
  		}
  
 -		list_add(&page->lru, &cmm_page_list);
 +		pa->page[pa->index++] = addr;
  		loaned_pages++;
++<<<<<<< HEAD
 +		totalram_pages--;
++=======
+ 		adjust_managed_page_count(page, -1);
++>>>>>>> 287b89773d81 (powerpc/pseries/cmm: Use adjust_managed_page_count() insted of totalram_pages_*)
  		spin_unlock(&cmm_lock);
  		nr--;
  	}
@@@ -231,23 -186,15 +235,32 @@@ static long cmm_free_pages(long nr
  
  	cmm_dbg("Begin free of %ld pages.\n", nr);
  	spin_lock(&cmm_lock);
 -	list_for_each_entry_safe(page, tmp, &cmm_page_list, lru) {
 -		if (!nr)
 +	pa = cmm_page_list;
 +	while (nr) {
 +		if (!pa || pa->index <= 0)
  			break;
++<<<<<<< HEAD
 +		addr = pa->page[--pa->index];
 +
 +		if (pa->index == 0) {
 +			pa = pa->next;
 +			free_page((unsigned long) cmm_page_list);
 +			cmm_page_list = pa;
 +		}
 +
 +		plpar_page_set_active(__pa(addr));
 +		free_page(addr);
 +		loaned_pages--;
 +		nr--;
 +		totalram_pages++;
++=======
+ 		plpar_page_set_active(page);
+ 		list_del(&page->lru);
+ 		adjust_managed_page_count(page, 1);
+ 		__free_page(page);
+ 		loaned_pages--;
+ 		nr--;
++>>>>>>> 287b89773d81 (powerpc/pseries/cmm: Use adjust_managed_page_count() insted of totalram_pages_*)
  	}
  	spin_unlock(&cmm_lock);
  	cmm_dbg("End request with %ld pages unfulfilled\n", nr);
@@@ -575,58 -512,16 +588,71 @@@ static int cmm_mem_going_offline(void *
  	spin_lock(&cmm_lock);
  
  	/* Search the page list for pages in the range to be offlined */
++<<<<<<< HEAD
 +	pa_last = pa_curr = cmm_page_list;
 +	while (pa_curr) {
 +		for (idx = (pa_curr->index - 1); (idx + 1) > 0; idx--) {
 +			if ((pa_curr->page[idx] < start_page) ||
 +			    (pa_curr->page[idx] >= end_page))
 +				continue;
 +
 +			plpar_page_set_active(__pa(pa_curr->page[idx]));
 +			free_page(pa_curr->page[idx]);
 +			freed++;
 +			loaned_pages--;
 +			totalram_pages++;
 +			pa_curr->page[idx] = pa_last->page[--pa_last->index];
 +			if (pa_last->index == 0) {
 +				if (pa_curr == pa_last)
 +					pa_curr = pa_last->next;
 +				pa_last = pa_last->next;
 +				free_page((unsigned long)cmm_page_list);
 +				cmm_page_list = pa_last;
 +			}
 +		}
 +		pa_curr = pa_curr->next;
 +	}
 +
 +	/* Search for page list structures in the range to be offlined */
 +	pa_last = NULL;
 +	pa_curr = cmm_page_list;
 +	while (pa_curr) {
 +		if (((unsigned long)pa_curr >= start_page) &&
 +				((unsigned long)pa_curr < end_page)) {
 +			npa = (struct cmm_page_array *)__get_free_page(
 +					GFP_NOIO | __GFP_NOWARN |
 +					__GFP_NORETRY | __GFP_NOMEMALLOC);
 +			if (!npa) {
 +				spin_unlock(&cmm_lock);
 +				cmm_dbg("Failed to allocate memory for list "
 +						"management. Memory hotplug "
 +						"failed.\n");
 +				return -ENOMEM;
 +			}
 +			memcpy(npa, pa_curr, PAGE_SIZE);
 +			if (pa_curr == cmm_page_list)
 +				cmm_page_list = npa;
 +			if (pa_last)
 +				pa_last->next = npa;
 +			free_page((unsigned long) pa_curr);
 +			freed++;
 +			pa_curr = npa;
 +		}
 +
 +		pa_last = pa_curr;
 +		pa_curr = pa_curr->next;
++=======
+ 	list_for_each_entry_safe(page, tmp, &cmm_page_list, lru) {
+ 		if (page_to_pfn(page) < marg->start_pfn ||
+ 		    page_to_pfn(page) >= marg->start_pfn + marg->nr_pages)
+ 			continue;
+ 		plpar_page_set_active(page);
+ 		list_del(&page->lru);
+ 		adjust_managed_page_count(page, 1);
+ 		__free_page(page);
+ 		freed++;
+ 		loaned_pages--;
++>>>>>>> 287b89773d81 (powerpc/pseries/cmm: Use adjust_managed_page_count() insted of totalram_pages_*)
  	}
  
  	spin_unlock(&cmm_lock);
* Unmerged path arch/powerpc/platforms/pseries/cmm.c
