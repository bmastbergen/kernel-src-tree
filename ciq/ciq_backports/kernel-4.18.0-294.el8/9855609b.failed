mm: memcg/slab: use a single set of kmem_caches for all accounted allocations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Roman Gushchin <guro@fb.com>
commit 9855609bde03e2472b99a95e869d29ee1e78a751
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/9855609b.failed

This is fairly big but mostly red patch, which makes all accounted slab
allocations use a single set of kmem_caches instead of creating a separate
set for each memory cgroup.

Because the number of non-root kmem_caches is now capped by the number of
root kmem_caches, there is no need to shrink or destroy them prematurely.
They can be perfectly destroyed together with their root counterparts.
This allows to dramatically simplify the management of non-root
kmem_caches and delete a ton of code.

This patch performs the following changes:
1) introduces memcg_params.memcg_cache pointer to represent the
   kmem_cache which will be used for all non-root allocations
2) reuses the existing memcg kmem_cache creation mechanism
   to create memcg kmem_cache on the first allocation attempt
3) memcg kmem_caches are named <kmemcache_name>-memcg,
   e.g. dentry-memcg
4) simplifies memcg_kmem_get_cache() to just return memcg kmem_cache
   or schedule it's creation and return the root cache
5) removes almost all non-root kmem_cache management code
   (separate refcounter, reparenting, shrinking, etc)
6) makes slab debugfs to display root_mem_cgroup css id and never
   show :dead and :deact flags in the memcg_slabinfo attribute.

Following patches in the series will simplify the kmem_cache creation.

	Signed-off-by: Roman Gushchin <guro@fb.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Shakeel Butt <shakeelb@google.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Tejun Heo <tj@kernel.org>
Link: http://lkml.kernel.org/r/20200623174037.3951353-13-guro@fb.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 9855609bde03e2472b99a95e869d29ee1e78a751)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	include/linux/slab.h
#	mm/memcontrol.c
#	mm/slab.h
diff --cc include/linux/memcontrol.h
index 72fba94a643f,11fd18b3d6c6..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -303,14 -317,8 +303,19 @@@ struct mem_cgroup 
          /* Index in the kmem_cache->memcg_params.memcg_caches array */
  	int kmemcg_id;
  	enum memcg_kmem_state kmem_state;
++<<<<<<< HEAD
 +	struct list_head kmem_caches;
 +#endif
 +
 +	RH_KABI_DEPRECATE(int, last_scanned_node)
 +#if MAX_NUMNODES > 1
 +	RH_KABI_DEPRECATE(nodemask_t, scan_nodes)
 +	RH_KABI_DEPRECATE(atomic_t, numainfo_events)
 +	RH_KABI_DEPRECATE(atomic_t, numainfo_updating)
++=======
+ 	struct obj_cgroup __rcu *objcg;
+ 	struct list_head objcg_list; /* list of inherited objcgs */
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  #endif
  
  #ifdef CONFIG_CGROUP_WRITEBACK
@@@ -1348,7 -1399,11 +1353,10 @@@ static inline bool mem_cgroup_under_soc
  #endif
  
  struct kmem_cache *memcg_kmem_get_cache(struct kmem_cache *cachep);
++<<<<<<< HEAD
 +void memcg_kmem_put_cache(struct kmem_cache *cachep);
++=======
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  
  #ifdef CONFIG_MEMCG_KMEM
  int __memcg_kmem_charge(struct mem_cgroup *memcg, gfp_t gfp,
diff --cc include/linux/slab.h
index 7964239d627e,8b1f91e320f9..000000000000
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@@ -599,73 -579,6 +598,76 @@@ static __always_inline void *kmalloc_no
  	return __kmalloc_node(size, flags, node);
  }
  
++<<<<<<< HEAD
 +struct memcg_cache_array {
 +	struct rcu_head rcu;
 +	struct kmem_cache *entries[0];
 +};
 +
 +/*
 + * This is the main placeholder for memcg-related information in kmem caches.
 + * Both the root cache and the child caches will have it. For the root cache,
 + * this will hold a dynamically allocated array large enough to hold
 + * information about the currently limited memcgs in the system. To allow the
 + * array to be accessed without taking any locks, on relocation we free the old
 + * version only after a grace period.
 + *
 + * Root and child caches hold different metadata.
 + *
 + * @root_cache:	Common to root and child caches.  NULL for root, pointer to
 + *		the root cache for children.
 + *
 + * The following fields are specific to root caches.
 + *
 + * @memcg_caches: kmemcg ID indexed table of child caches.  This table is
 + *		used to index child cachces during allocation and cleared
 + *		early during shutdown.
 + *
 + * @root_caches_node: List node for slab_root_caches list.
 + *
 + * @children:	List of all child caches.  While the child caches are also
 + *		reachable through @memcg_caches, a child cache remains on
 + *		this list until it is actually destroyed.
 + *
 + * The following fields are specific to child caches.
 + *
 + * @memcg:	Pointer to the memcg this cache belongs to.
 + *
 + * @children_node: List node for @root_cache->children list.
 + *
 + * @kmem_caches_node: List node for @memcg->kmem_caches list.
 + */
 +struct memcg_cache_params {
 +	struct kmem_cache *root_cache;
 +	union {
 +		struct {
 +			struct memcg_cache_array __rcu *memcg_caches;
 +			struct list_head __root_caches_node;
 +			struct list_head children;
 +			bool dying;
 +		};
 +		struct {
 +			struct mem_cgroup *memcg;
 +			struct list_head children_node;
 +			struct list_head kmem_caches_node;
 +			RH_KABI_BROKEN_INSERT(struct percpu_ref refcnt)
 +
 +			void (*RH_KABI_RENAME(deact_fn,
 +					      work_fn))(struct kmem_cache *);
 +			union {
 +				struct rcu_head RH_KABI_RENAME(deact_rcu_head,
 +							       rcu_head);
 +				struct work_struct RH_KABI_RENAME(deact_work,
 +								  work);
 +			};
 +		};
 +	};
 +};
 +
 +int memcg_update_all_caches(int num_memcgs);
 +
++=======
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  /**
   * kmalloc_array - allocate memory for an array.
   * @n: number of elements.
diff --cc mm/memcontrol.c
index 13b0a0f8cd33,874704c4a48a..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -262,8 -257,100 +262,8 @@@ struct cgroup_subsys_state *vmpressure_
  }
  
  #ifdef CONFIG_MEMCG_KMEM
 -extern spinlock_t css_set_lock;
 -
 -static void obj_cgroup_release(struct percpu_ref *ref)
 -{
 -	struct obj_cgroup *objcg = container_of(ref, struct obj_cgroup, refcnt);
 -	struct mem_cgroup *memcg;
 -	unsigned int nr_bytes;
 -	unsigned int nr_pages;
 -	unsigned long flags;
 -
 -	/*
 -	 * At this point all allocated objects are freed, and
 -	 * objcg->nr_charged_bytes can't have an arbitrary byte value.
 -	 * However, it can be PAGE_SIZE or (x * PAGE_SIZE).
 -	 *
 -	 * The following sequence can lead to it:
 -	 * 1) CPU0: objcg == stock->cached_objcg
 -	 * 2) CPU1: we do a small allocation (e.g. 92 bytes),
 -	 *          PAGE_SIZE bytes are charged
 -	 * 3) CPU1: a process from another memcg is allocating something,
 -	 *          the stock if flushed,
 -	 *          objcg->nr_charged_bytes = PAGE_SIZE - 92
 -	 * 5) CPU0: we do release this object,
 -	 *          92 bytes are added to stock->nr_bytes
 -	 * 6) CPU0: stock is flushed,
 -	 *          92 bytes are added to objcg->nr_charged_bytes
 -	 *
 -	 * In the result, nr_charged_bytes == PAGE_SIZE.
 -	 * This page will be uncharged in obj_cgroup_release().
 -	 */
 -	nr_bytes = atomic_read(&objcg->nr_charged_bytes);
 -	WARN_ON_ONCE(nr_bytes & (PAGE_SIZE - 1));
 -	nr_pages = nr_bytes >> PAGE_SHIFT;
 -
 -	spin_lock_irqsave(&css_set_lock, flags);
 -	memcg = obj_cgroup_memcg(objcg);
 -	if (nr_pages)
 -		__memcg_kmem_uncharge(memcg, nr_pages);
 -	list_del(&objcg->list);
 -	mem_cgroup_put(memcg);
 -	spin_unlock_irqrestore(&css_set_lock, flags);
 -
 -	percpu_ref_exit(ref);
 -	kfree_rcu(objcg, rcu);
 -}
 -
 -static struct obj_cgroup *obj_cgroup_alloc(void)
 -{
 -	struct obj_cgroup *objcg;
 -	int ret;
 -
 -	objcg = kzalloc(sizeof(struct obj_cgroup), GFP_KERNEL);
 -	if (!objcg)
 -		return NULL;
 -
 -	ret = percpu_ref_init(&objcg->refcnt, obj_cgroup_release, 0,
 -			      GFP_KERNEL);
 -	if (ret) {
 -		kfree(objcg);
 -		return NULL;
 -	}
 -	INIT_LIST_HEAD(&objcg->list);
 -	return objcg;
 -}
 -
 -static void memcg_reparent_objcgs(struct mem_cgroup *memcg,
 -				  struct mem_cgroup *parent)
 -{
 -	struct obj_cgroup *objcg, *iter;
 -
 -	objcg = rcu_replace_pointer(memcg->objcg, NULL, true);
 -
 -	spin_lock_irq(&css_set_lock);
 -
 -	/* Move active objcg to the parent's list */
 -	xchg(&objcg->memcg, parent);
 -	css_get(&parent->css);
 -	list_add(&objcg->list, &parent->objcg_list);
 -
 -	/* Move already reparented objcgs to the parent's list */
 -	list_for_each_entry(iter, &memcg->objcg_list, list) {
 -		css_get(&parent->css);
 -		xchg(&iter->memcg, parent);
 -		css_put(&memcg->css);
 -	}
 -	list_splice(&memcg->objcg_list, &parent->objcg_list);
 -
 -	spin_unlock_irq(&css_set_lock);
 -
 -	percpu_ref_kill(&objcg->refcnt);
 -}
 -
  /*
-  * This will be the memcg's index in each cache's ->memcg_params.memcg_caches.
+  * This will be used as a shrinker list's index.
   * The main reason for not using cgroup id for this:
   *  this works better in sparse environments, where we have a lot of memcgs,
   *  but only a few kmem-limited. Or also, if we have, for instance, 200
@@@ -489,10 -569,17 +489,24 @@@ ino_t page_cgroup_ino(struct page *page
  	unsigned long ino = 0;
  
  	rcu_read_lock();
++<<<<<<< HEAD
 +	if (PageSlab(page) && !PageTail(page))
 +		memcg = memcg_from_slab_page(page);
 +	else
 +		memcg = READ_ONCE(page->mem_cgroup);
++=======
+ 	memcg = page->mem_cgroup;
+ 
+ 	/*
+ 	 * The lowest bit set means that memcg isn't a valid
+ 	 * memcg pointer, but a obj_cgroups pointer.
+ 	 * In this case the page is shared and doesn't belong
+ 	 * to any specific memory cgroup.
+ 	 */
+ 	if ((unsigned long) memcg & 0x1UL)
+ 		memcg = NULL;
+ 
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  	while (memcg && !(memcg->css.flags & CSS_ONLINE))
  		memcg = parent_mem_cgroup(memcg);
  	if (memcg)
@@@ -2884,80 -2947,15 +2891,69 @@@ static void memcg_schedule_kmem_cache_c
   */
  struct kmem_cache *memcg_kmem_get_cache(struct kmem_cache *cachep)
  {
- 	struct mem_cgroup *memcg;
  	struct kmem_cache *memcg_cachep;
- 	struct memcg_cache_array *arr;
- 	int kmemcg_id;
- 
- 	VM_BUG_ON(!is_root_cache(cachep));
  
- 	if (memcg_kmem_bypass())
+ 	memcg_cachep = READ_ONCE(cachep->memcg_params.memcg_cache);
+ 	if (unlikely(!memcg_cachep)) {
+ 		memcg_schedule_kmem_cache_create(cachep);
  		return cachep;
++<<<<<<< HEAD
 +
 +	rcu_read_lock();
 +
 +	if (unlikely(current->active_memcg))
 +		memcg = current->active_memcg;
 +	else
 +		memcg = mem_cgroup_from_task(current);
 +
 +	if (!memcg || memcg == root_mem_cgroup)
 +		goto out_unlock;
 +
 +	kmemcg_id = READ_ONCE(memcg->kmemcg_id);
 +	if (kmemcg_id < 0)
 +		goto out_unlock;
 +
 +	arr = rcu_dereference(cachep->memcg_params.memcg_caches);
 +
 +	/*
 +	 * Make sure we will access the up-to-date value. The code updating
 +	 * memcg_caches issues a write barrier to match the data dependency
 +	 * barrier inside READ_ONCE() (see memcg_create_kmem_cache()).
 +	 */
 +	memcg_cachep = READ_ONCE(arr->entries[kmemcg_id]);
 +
 +	/*
 +	 * If we are in a safe context (can wait, and not in interrupt
 +	 * context), we could be be predictable and return right away.
 +	 * This would guarantee that the allocation being performed
 +	 * already belongs in the new cache.
 +	 *
 +	 * However, there are some clashes that can arrive from locking.
 +	 * For instance, because we acquire the slab_mutex while doing
 +	 * memcg_create_kmem_cache, this means no further allocation
 +	 * could happen with the slab_mutex held. So it's better to
 +	 * defer everything.
 +	 *
 +	 * If the memcg is dying or memcg_cache is about to be released,
 +	 * don't bother creating new kmem_caches. Because memcg_cachep
 +	 * is ZEROed as the fist step of kmem offlining, we don't need
 +	 * percpu_ref_tryget_live() here. css_tryget_online() check in
 +	 * memcg_schedule_kmem_cache_create() will prevent us from
 +	 * creation of a new kmem_cache.
 +	 */
 +	if (unlikely(!memcg_cachep))
 +		memcg_schedule_kmem_cache_create(memcg, cachep);
 +	else if (percpu_ref_tryget(&memcg_cachep->memcg_params.refcnt))
 +		cachep = memcg_cachep;
 +out_unlock:
 +	rcu_read_unlock();
 +	return cachep;
 +}
++=======
+ 	}
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  
- /**
-  * memcg_kmem_put_cache: drop reference taken by memcg_kmem_get_cache
-  * @cachep: the cache returned by memcg_kmem_get_cache
-  */
- void memcg_kmem_put_cache(struct kmem_cache *cachep)
- {
- 	if (!is_root_cache(cachep))
- 		percpu_ref_put(&cachep->memcg_params.refcnt);
+ 	return memcg_cachep;
  }
  
  /**
@@@ -3528,10 -3664,7 +3518,14 @@@ static void memcg_offline_kmem(struct m
  	if (!parent)
  		parent = root_mem_cgroup;
  
++<<<<<<< HEAD
 +	/*
 +	 * Deactivate and reparent kmem_caches.
 +	 */
 +	memcg_deactivate_kmem_caches(memcg, parent);
++=======
+ 	memcg_reparent_objcgs(memcg, parent);
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  
  	kmemcg_id = memcg->kmemcg_id;
  	BUG_ON(kmemcg_id < 0);
diff --cc mm/slab.h
index 45ad57de9d88,e716b80befc2..000000000000
--- a/mm/slab.h
+++ b/mm/slab.h
@@@ -30,6 -30,28 +30,31 @@@ struct kmem_cache 
  	struct list_head list;	/* List of all slab caches on the system */
  };
  
++<<<<<<< HEAD
++=======
+ #else /* !CONFIG_SLOB */
+ 
+ /*
+  * This is the main placeholder for memcg-related information in kmem caches.
+  * Both the root cache and the child cache will have it. Some fields are used
+  * in both cases, other are specific to root caches.
+  *
+  * @root_cache:	Common to root and child caches.  NULL for root, pointer to
+  *		the root cache for children.
+  *
+  * The following fields are specific to root caches.
+  *
+  * @memcg_cache: pointer to memcg kmem cache, used by all non-root memory
+  *		cgroups.
+  * @root_caches_node: list node for slab_root_caches list.
+  */
+ struct memcg_cache_params {
+ 	struct kmem_cache *root_cache;
+ 
+ 	struct kmem_cache *memcg_cache;
+ 	struct list_head __root_caches_node;
+ };
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  #endif /* CONFIG_SLOB */
  
  #ifdef CONFIG_SLAB
@@@ -257,92 -298,143 +272,162 @@@ static inline struct kmem_cache *memcg_
  	return s->memcg_params.root_cache;
  }
  
++<<<<<<< HEAD
 +/*
 + * Expects a pointer to a slab page. Please note, that PageSlab() check
 + * isn't sufficient, as it returns true also for tail compound slab pages,
 + * which do not have slab_cache pointer set.
 + * So this function assumes that the page can pass PageSlab() && !PageTail()
 + * check.
 + *
 + * The kmem_cache can be reparented asynchronously. The caller must ensure
 + * the memcg lifetime, e.g. by taking rcu_read_lock() or cgroup_mutex.
 + */
 +static inline struct mem_cgroup *memcg_from_slab_page(struct page *page)
++=======
+ static inline struct kmem_cache *memcg_cache(struct kmem_cache *s)
  {
- 	struct kmem_cache *s;
+ 	if (is_root_cache(s))
+ 		return s->memcg_params.memcg_cache;
+ 	return NULL;
+ }
  
- 	s = READ_ONCE(page->slab_cache);
- 	if (s && !is_root_cache(s))
- 		return READ_ONCE(s->memcg_params.memcg);
+ static inline struct obj_cgroup **page_obj_cgroups(struct page *page)
+ {
+ 	/*
+ 	 * page->mem_cgroup and page->obj_cgroups are sharing the same
+ 	 * space. To distinguish between them in case we don't know for sure
+ 	 * that the page is a slab page (e.g. page_cgroup_ino()), let's
+ 	 * always set the lowest bit of obj_cgroups.
+ 	 */
+ 	return (struct obj_cgroup **)
+ 		((unsigned long)page->obj_cgroups & ~0x1UL);
+ }
  
- 	return NULL;
+ static inline bool page_has_obj_cgroups(struct page *page)
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
+ {
+ 	return ((unsigned long)page->obj_cgroups & 0x1UL);
  }
  
 -static inline int memcg_alloc_page_obj_cgroups(struct page *page,
 -					       struct kmem_cache *s, gfp_t gfp)
 +/*
 + * Charge the slab page belonging to the non-root kmem_cache.
 + * Can be called for non-root kmem_caches only.
 + */
 +static __always_inline int memcg_charge_slab(struct page *page,
 +					     gfp_t gfp, int order,
 +					     struct kmem_cache *s)
  {
 -	unsigned int objects = objs_per_slab_page(s, page);
 -	void *vec;
 +	int nr_pages = 1 << order;
 +	struct mem_cgroup *memcg;
 +	struct lruvec *lruvec;
 +	int ret;
 +
 +	rcu_read_lock();
 +	memcg = READ_ONCE(s->memcg_params.memcg);
 +	while (memcg && !css_tryget_online(&memcg->css))
 +		memcg = parent_mem_cgroup(memcg);
 +	rcu_read_unlock();
  
 -	vec = kcalloc_node(objects, sizeof(struct obj_cgroup *), gfp,
 -			   page_to_nid(page));
 -	if (!vec)
 -		return -ENOMEM;
++<<<<<<< HEAD
 +	if (unlikely(!memcg || mem_cgroup_is_root(memcg))) {
 +		mod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),
 +				    nr_pages);
 +		percpu_ref_get_many(&s->memcg_params.refcnt, nr_pages);
 +		return 0;
 +	}
  
 +	ret = memcg_kmem_charge(memcg, gfp, nr_pages);
 +	if (ret)
 +		goto out;
 +
 +	lruvec = mem_cgroup_lruvec(memcg, page_pgdat(page));
 +	mod_lruvec_state(lruvec, cache_vmstat_idx(s), nr_pages);
 +
 +	/* transer try_charge() page references to kmem_cache */
 +	percpu_ref_get_many(&s->memcg_params.refcnt, nr_pages);
 +	css_put_many(&memcg->css, nr_pages);
 +out:
 +	css_put(&memcg->css);
 +	return ret;
++=======
+ 	kmemleak_not_leak(vec);
+ 	page->obj_cgroups = (struct obj_cgroup **) ((unsigned long)vec | 0x1UL);
+ 	return 0;
+ }
+ 
+ static inline void memcg_free_page_obj_cgroups(struct page *page)
+ {
+ 	kfree(page_obj_cgroups(page));
+ 	page->obj_cgroups = NULL;
+ }
+ 
+ static inline size_t obj_full_size(struct kmem_cache *s)
+ {
+ 	/*
+ 	 * For each accounted object there is an extra space which is used
+ 	 * to store obj_cgroup membership. Charge it too.
+ 	 */
+ 	return s->size + sizeof(struct obj_cgroup *);
+ }
+ 
+ static inline struct kmem_cache *memcg_slab_pre_alloc_hook(struct kmem_cache *s,
+ 						struct obj_cgroup **objcgp,
+ 						size_t objects, gfp_t flags)
+ {
+ 	struct kmem_cache *cachep;
+ 	struct obj_cgroup *objcg;
+ 
+ 	if (memcg_kmem_bypass())
+ 		return s;
+ 
+ 	cachep = memcg_kmem_get_cache(s);
+ 	if (is_root_cache(cachep))
+ 		return s;
+ 
+ 	objcg = get_obj_cgroup_from_current();
+ 	if (!objcg)
+ 		return s;
+ 
+ 	if (obj_cgroup_charge(objcg, flags, objects * obj_full_size(s))) {
+ 		obj_cgroup_put(objcg);
+ 		cachep = NULL;
+ 	}
+ 
+ 	*objcgp = objcg;
+ 	return cachep;
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  }
  
 -static inline void mod_objcg_state(struct obj_cgroup *objcg,
 -				   struct pglist_data *pgdat,
 -				   int idx, int nr)
 +/*
 + * Uncharge a slab page belonging to a non-root kmem_cache.
 + * Can be called for non-root kmem_caches only.
 + */
 +static __always_inline void memcg_uncharge_slab(struct page *page, int order,
 +						struct kmem_cache *s)
  {
 +	int nr_pages = 1 << order;
  	struct mem_cgroup *memcg;
  	struct lruvec *lruvec;
  
  	rcu_read_lock();
 -	memcg = obj_cgroup_memcg(objcg);
 -	lruvec = mem_cgroup_lruvec(memcg, pgdat);
 -	mod_memcg_lruvec_state(lruvec, idx, nr);
 -	rcu_read_unlock();
 -}
 -
 -static inline void memcg_slab_post_alloc_hook(struct kmem_cache *s,
 -					      struct obj_cgroup *objcg,
 -					      size_t size, void **p)
 -{
 -	struct page *page;
 -	unsigned long off;
 -	size_t i;
 -
 -	for (i = 0; i < size; i++) {
 -		if (likely(p[i])) {
 -			page = virt_to_head_page(p[i]);
 -			off = obj_to_index(s, page, p[i]);
 -			obj_cgroup_get(objcg);
 -			page_obj_cgroups(page)[off] = objcg;
 -			mod_objcg_state(objcg, page_pgdat(page),
 -					cache_vmstat_idx(s), obj_full_size(s));
 -		} else {
 -			obj_cgroup_uncharge(objcg, obj_full_size(s));
 -		}
 +	memcg = READ_ONCE(s->memcg_params.memcg);
 +	if (likely(!mem_cgroup_is_root(memcg))) {
 +		lruvec = mem_cgroup_lruvec(memcg, page_pgdat(page));
 +		mod_lruvec_state(lruvec, cache_vmstat_idx(s), -nr_pages);
 +		memcg_kmem_uncharge(memcg, nr_pages);
 +	} else {
 +		mod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),
 +				    -nr_pages);
  	}
++<<<<<<< HEAD
 +	rcu_read_unlock();
++=======
+ 	obj_cgroup_put(objcg);
+ }
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  
 -static inline void memcg_slab_free_hook(struct kmem_cache *s, struct page *page,
 -					void *p)
 -{
 -	struct obj_cgroup *objcg;
 -	unsigned int off;
 -
 -	if (!memcg_kmem_enabled() || is_root_cache(s))
 -		return;
 -
 -	off = obj_to_index(s, page, p);
 -	objcg = page_obj_cgroups(page)[off];
 -	page_obj_cgroups(page)[off] = NULL;
 -
 -	obj_cgroup_uncharge(objcg, obj_full_size(s));
 -	mod_objcg_state(objcg, page_pgdat(page), cache_vmstat_idx(s),
 -			-obj_full_size(s));
 -
 -	obj_cgroup_put(objcg);
 +	percpu_ref_put_many(&s->memcg_params.refcnt, nr_pages);
  }
  
  extern void slab_init_memcg_params(struct kmem_cache *);
@@@ -409,25 -535,27 +500,47 @@@ static __always_inline int charge_slab_
  					    gfp_t gfp, int order,
  					    struct kmem_cache *s)
  {
++<<<<<<< HEAD
 +	if (is_root_cache(s)) {
 +		mod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),
 +				    1 << order);
 +		return 0;
 +	}
 +
 +	return memcg_charge_slab(page, gfp, order, s);
++=======
+ 	if (memcg_kmem_enabled() && !is_root_cache(s)) {
+ 		int ret;
+ 
+ 		ret = memcg_alloc_page_obj_cgroups(page, s, gfp);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	mod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),
+ 			    PAGE_SIZE << order);
+ 	return 0;
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  }
  
  static __always_inline void uncharge_slab_page(struct page *page, int order,
  					       struct kmem_cache *s)
  {
++<<<<<<< HEAD
 +	if (is_root_cache(s)) {
 +		mod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),
 +				    -(1 << order));
 +		return;
 +	}
 +
 +	memcg_uncharge_slab(page, order, s);
++=======
+ 	if (memcg_kmem_enabled() && !is_root_cache(s))
+ 		memcg_free_page_obj_cgroups(page);
+ 
+ 	mod_node_page_state(page_pgdat(page), cache_vmstat_idx(s),
+ 			    -(PAGE_SIZE << order));
++>>>>>>> 9855609bde03 (mm: memcg/slab: use a single set of kmem_caches for all accounted allocations)
  }
  
  static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
* Unmerged path include/linux/memcontrol.h
* Unmerged path include/linux/slab.h
* Unmerged path mm/memcontrol.c
diff --git a/mm/slab.c b/mm/slab.c
index 98c009baf3ea..c72058aacd43 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -1279,7 +1279,7 @@ void __init kmem_cache_init(void)
 				  nr_node_ids * sizeof(struct kmem_cache_node *),
 				  SLAB_HWCACHE_ALIGN, 0, 0);
 	list_add(&kmem_cache->list, &slab_caches);
-	memcg_link_cache(kmem_cache, NULL);
+	memcg_link_cache(kmem_cache);
 	slab_state = PARTIAL;
 
 	/*
@@ -2306,17 +2306,6 @@ int __kmem_cache_shrink(struct kmem_cache *cachep)
 	return (ret ? 1 : 0);
 }
 
-#ifdef CONFIG_MEMCG
-void __kmemcg_cache_deactivate(struct kmem_cache *cachep)
-{
-	__kmem_cache_shrink(cachep);
-}
-
-void __kmemcg_cache_deactivate_after_rcu(struct kmem_cache *s)
-{
-}
-#endif
-
 int __kmem_cache_shutdown(struct kmem_cache *cachep)
 {
 	return __kmem_cache_shrink(cachep);
@@ -3909,7 +3898,8 @@ static int do_tune_cpucache(struct kmem_cache *cachep, int limit,
 		return ret;
 
 	lockdep_assert_held(&slab_mutex);
-	for_each_memcg_cache(c, cachep) {
+	c = memcg_cache(cachep);
+	if (c) {
 		/* return value determined by the root cache only */
 		__do_tune_cpucache(c, limit, batchcount, shared, gfp);
 	}
* Unmerged path mm/slab.h
diff --git a/mm/slab_common.c b/mm/slab_common.c
index dd4f84ee402c..6fb368632b5c 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -133,141 +133,36 @@ int __kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t nr,
 #ifdef CONFIG_MEMCG_KMEM
 
 LIST_HEAD(slab_root_caches);
-static DEFINE_SPINLOCK(memcg_kmem_wq_lock);
-
-static void kmemcg_cache_shutdown(struct percpu_ref *percpu_ref);
 
 void slab_init_memcg_params(struct kmem_cache *s)
 {
 	s->memcg_params.root_cache = NULL;
-	RCU_INIT_POINTER(s->memcg_params.memcg_caches, NULL);
-	INIT_LIST_HEAD(&s->memcg_params.children);
-	s->memcg_params.dying = false;
+	s->memcg_params.memcg_cache = NULL;
 }
 
-static int init_memcg_params(struct kmem_cache *s,
-			     struct kmem_cache *root_cache)
+static void init_memcg_params(struct kmem_cache *s,
+			      struct kmem_cache *root_cache)
 {
-	struct memcg_cache_array *arr;
-
-	if (root_cache) {
-		int ret = percpu_ref_init(&s->memcg_params.refcnt,
-					  kmemcg_cache_shutdown,
-					  0, GFP_KERNEL);
-		if (ret)
-			return ret;
-
+	if (root_cache)
 		s->memcg_params.root_cache = root_cache;
-		INIT_LIST_HEAD(&s->memcg_params.children_node);
-		INIT_LIST_HEAD(&s->memcg_params.kmem_caches_node);
-		return 0;
-	}
-
-	slab_init_memcg_params(s);
-
-	if (!memcg_nr_cache_ids)
-		return 0;
-
-	arr = kvzalloc(sizeof(struct memcg_cache_array) +
-		       memcg_nr_cache_ids * sizeof(void *),
-		       GFP_KERNEL);
-	if (!arr)
-		return -ENOMEM;
-
-	RCU_INIT_POINTER(s->memcg_params.memcg_caches, arr);
-	return 0;
-}
-
-static void destroy_memcg_params(struct kmem_cache *s)
-{
-	if (is_root_cache(s)) {
-		kvfree(rcu_access_pointer(s->memcg_params.memcg_caches));
-	} else {
-		mem_cgroup_put(s->memcg_params.memcg);
-		WRITE_ONCE(s->memcg_params.memcg, NULL);
-		percpu_ref_exit(&s->memcg_params.refcnt);
-	}
-}
-
-static void free_memcg_params(struct rcu_head *rcu)
-{
-	struct memcg_cache_array *old;
-
-	old = container_of(rcu, struct memcg_cache_array, rcu);
-	kvfree(old);
-}
-
-static int update_memcg_params(struct kmem_cache *s, int new_array_size)
-{
-	struct memcg_cache_array *old, *new;
-
-	new = kvzalloc(sizeof(struct memcg_cache_array) +
-		       new_array_size * sizeof(void *), GFP_KERNEL);
-	if (!new)
-		return -ENOMEM;
-
-	old = rcu_dereference_protected(s->memcg_params.memcg_caches,
-					lockdep_is_held(&slab_mutex));
-	if (old)
-		memcpy(new->entries, old->entries,
-		       memcg_nr_cache_ids * sizeof(void *));
-
-	rcu_assign_pointer(s->memcg_params.memcg_caches, new);
-	if (old)
-		call_rcu(&old->rcu, free_memcg_params);
-	return 0;
+	else
+		slab_init_memcg_params(s);
 }
 
-int memcg_update_all_caches(int num_memcgs)
+void memcg_link_cache(struct kmem_cache *s)
 {
-	struct kmem_cache *s;
-	int ret = 0;
-
-	mutex_lock(&slab_mutex);
-	list_for_each_entry(s, &slab_root_caches, root_caches_node) {
-		ret = update_memcg_params(s, num_memcgs);
-		/*
-		 * Instead of freeing the memory, we'll just leave the caches
-		 * up to this point in an updated state.
-		 */
-		if (ret)
-			break;
-	}
-	mutex_unlock(&slab_mutex);
-	return ret;
-}
-
-void memcg_link_cache(struct kmem_cache *s, struct mem_cgroup *memcg)
-{
-	if (is_root_cache(s)) {
+	if (is_root_cache(s))
 		list_add(&s->root_caches_node, &slab_root_caches);
-	} else {
-		css_get(&memcg->css);
-		s->memcg_params.memcg = memcg;
-		list_add(&s->memcg_params.children_node,
-			 &s->memcg_params.root_cache->memcg_params.children);
-		list_add(&s->memcg_params.kmem_caches_node,
-			 &s->memcg_params.memcg->kmem_caches);
-	}
 }
 
 static void memcg_unlink_cache(struct kmem_cache *s)
 {
-	if (is_root_cache(s)) {
+	if (is_root_cache(s))
 		list_del(&s->root_caches_node);
-	} else {
-		list_del(&s->memcg_params.children_node);
-		list_del(&s->memcg_params.kmem_caches_node);
-	}
 }
 #else
-static inline int init_memcg_params(struct kmem_cache *s,
-				    struct kmem_cache *root_cache)
-{
-	return 0;
-}
-
-static inline void destroy_memcg_params(struct kmem_cache *s)
+static inline void init_memcg_params(struct kmem_cache *s,
+				     struct kmem_cache *root_cache)
 {
 }
 
@@ -328,14 +223,6 @@ int slab_unmergeable(struct kmem_cache *s)
 	if (s->refcount < 0)
 		return 1;
 
-#ifdef CONFIG_MEMCG_KMEM
-	/*
-	 * Skip the dying kmem_cache.
-	 */
-	if (s->memcg_params.dying)
-		return 1;
-#endif
-
 	return 0;
 }
 
@@ -390,7 +277,7 @@ static struct kmem_cache *create_cache(const char *name,
 		unsigned int object_size, unsigned int align,
 		slab_flags_t flags, unsigned int useroffset,
 		unsigned int usersize, void (*ctor)(void *),
-		struct mem_cgroup *memcg, struct kmem_cache *root_cache)
+		struct kmem_cache *root_cache)
 {
 	struct kmem_cache *s;
 	int err;
@@ -410,24 +297,20 @@ static struct kmem_cache *create_cache(const char *name,
 	s->useroffset = useroffset;
 	s->usersize = usersize;
 
-	err = init_memcg_params(s, root_cache);
-	if (err)
-		goto out_free_cache;
-
+	init_memcg_params(s, root_cache);
 	err = __kmem_cache_create(s, flags);
 	if (err)
 		goto out_free_cache;
 
 	s->refcount = 1;
 	list_add(&s->list, &slab_caches);
-	memcg_link_cache(s, memcg);
+	memcg_link_cache(s);
 out:
 	if (err)
 		return ERR_PTR(err);
 	return s;
 
 out_free_cache:
-	destroy_memcg_params(s);
 	kmem_cache_free(kmem_cache, s);
 	goto out;
 }
@@ -514,7 +397,7 @@ kmem_cache_create_usercopy(const char *name,
 
 	s = create_cache(cache_name, size,
 			 calculate_alignment(flags, align, size),
-			 flags, useroffset, usersize, ctor, NULL, NULL);
+			 flags, useroffset, usersize, ctor, NULL);
 	if (IS_ERR(s)) {
 		err = PTR_ERR(s);
 		kfree_const(cache_name);
@@ -639,51 +522,27 @@ static int shutdown_cache(struct kmem_cache *s)
 
 #ifdef CONFIG_MEMCG_KMEM
 /*
- * memcg_create_kmem_cache - Create a cache for a memory cgroup.
- * @memcg: The memory cgroup the new cache is for.
+ * memcg_create_kmem_cache - Create a cache for non-root memory cgroups.
  * @root_cache: The parent of the new cache.
  *
  * This function attempts to create a kmem cache that will serve allocation
- * requests going from @memcg to @root_cache. The new cache inherits properties
- * from its parent.
+ * requests going all non-root memory cgroups to @root_cache. The new cache
+ * inherits properties from its parent.
  */
-void memcg_create_kmem_cache(struct mem_cgroup *memcg,
-			     struct kmem_cache *root_cache)
+void memcg_create_kmem_cache(struct kmem_cache *root_cache)
 {
-	static char memcg_name_buf[NAME_MAX + 1]; /* protected by slab_mutex */
-	struct cgroup_subsys_state *css = &memcg->css;
-	struct memcg_cache_array *arr;
 	struct kmem_cache *s = NULL;
 	char *cache_name;
-	int idx;
 
 	get_online_cpus();
 	get_online_mems();
 
 	mutex_lock(&slab_mutex);
 
-	/*
-	 * The memory cgroup could have been offlined while the cache
-	 * creation work was pending.
-	 */
-	if (memcg->kmem_state != KMEM_ONLINE)
+	if (root_cache->memcg_params.memcg_cache)
 		goto out_unlock;
 
-	idx = memcg_cache_id(memcg);
-	arr = rcu_dereference_protected(root_cache->memcg_params.memcg_caches,
-					lockdep_is_held(&slab_mutex));
-
-	/*
-	 * Since per-memcg caches are created asynchronously on first
-	 * allocation (see memcg_kmem_get_cache()), several threads can try to
-	 * create the same cache, but only one of them may succeed.
-	 */
-	if (arr->entries[idx])
-		goto out_unlock;
-
-	cgroup_name(css->cgroup, memcg_name_buf, sizeof(memcg_name_buf));
-	cache_name = kasprintf(GFP_KERNEL, "%s(%llu:%s)", root_cache->name,
-			       css->serial_nr, memcg_name_buf);
+	cache_name = kasprintf(GFP_KERNEL, "%s-memcg", root_cache->name);
 	if (!cache_name)
 		goto out_unlock;
 
@@ -691,7 +550,7 @@ void memcg_create_kmem_cache(struct mem_cgroup *memcg,
 			 root_cache->align,
 			 root_cache->flags & CACHE_CREATE_MASK,
 			 root_cache->useroffset, root_cache->usersize,
-			 root_cache->ctor, memcg, root_cache);
+			 root_cache->ctor, root_cache);
 	/*
 	 * If we could not create a memcg cache, do not complain, because
 	 * that's not critical at all as we can always proceed with the root
@@ -708,7 +567,7 @@ void memcg_create_kmem_cache(struct mem_cgroup *memcg,
 	 * initialized.
 	 */
 	smp_wmb();
-	arr->entries[idx] = s;
+	root_cache->memcg_params.memcg_cache = s;
 
 out_unlock:
 	mutex_unlock(&slab_mutex);
@@ -717,200 +576,18 @@ void memcg_create_kmem_cache(struct mem_cgroup *memcg,
 	put_online_cpus();
 }
 
-static void kmemcg_workfn(struct work_struct *work)
-{
-	struct kmem_cache *s = container_of(work, struct kmem_cache,
-					    memcg_params.work);
-
-	get_online_cpus();
-	get_online_mems();
-
-	mutex_lock(&slab_mutex);
-	s->memcg_params.work_fn(s);
-	mutex_unlock(&slab_mutex);
-
-	put_online_mems();
-	put_online_cpus();
-}
-
-static void kmemcg_rcufn(struct rcu_head *head)
-{
-	struct kmem_cache *s = container_of(head, struct kmem_cache,
-					    memcg_params.rcu_head);
-
-	/*
-	 * We need to grab blocking locks.  Bounce to ->work.  The
-	 * work item shares the space with the RCU head and can't be
-	 * initialized earlier.
-	 */
-	INIT_WORK(&s->memcg_params.work, kmemcg_workfn);
-	queue_work(memcg_kmem_cache_wq, &s->memcg_params.work);
-}
-
-static void kmemcg_cache_shutdown_fn(struct kmem_cache *s)
-{
-	WARN_ON(shutdown_cache(s));
-}
-
-static void kmemcg_cache_shutdown(struct percpu_ref *percpu_ref)
-{
-	struct kmem_cache *s = container_of(percpu_ref, struct kmem_cache,
-					    memcg_params.refcnt);
-	unsigned long flags;
-
-	spin_lock_irqsave(&memcg_kmem_wq_lock, flags);
-	if (s->memcg_params.root_cache->memcg_params.dying)
-		goto unlock;
-
-	s->memcg_params.work_fn = kmemcg_cache_shutdown_fn;
-	INIT_WORK(&s->memcg_params.work, kmemcg_workfn);
-	queue_work(memcg_kmem_cache_wq, &s->memcg_params.work);
-
-unlock:
-	spin_unlock_irqrestore(&memcg_kmem_wq_lock, flags);
-}
-
-static void kmemcg_cache_deactivate_after_rcu(struct kmem_cache *s)
-{
-	__kmemcg_cache_deactivate_after_rcu(s);
-	percpu_ref_kill(&s->memcg_params.refcnt);
-}
-
-static void kmemcg_cache_deactivate(struct kmem_cache *s)
-{
-	if (WARN_ON_ONCE(is_root_cache(s)))
-		return;
-
-	__kmemcg_cache_deactivate(s);
-	s->flags |= SLAB_DEACTIVATED;
-
-	/*
-	 * memcg_kmem_wq_lock is used to synchronize memcg_params.dying
-	 * flag and make sure that no new kmem_cache deactivation tasks
-	 * are queued (see flush_memcg_workqueue() ).
-	 */
-	spin_lock_irq(&memcg_kmem_wq_lock);
-	if (s->memcg_params.root_cache->memcg_params.dying)
-		goto unlock;
-
-	s->memcg_params.work_fn = kmemcg_cache_deactivate_after_rcu;
-	call_rcu(&s->memcg_params.rcu_head, kmemcg_rcufn);
-unlock:
-	spin_unlock_irq(&memcg_kmem_wq_lock);
-}
-
-void memcg_deactivate_kmem_caches(struct mem_cgroup *memcg,
-				  struct mem_cgroup *parent)
-{
-	int idx;
-	struct memcg_cache_array *arr;
-	struct kmem_cache *s, *c;
-	unsigned int nr_reparented;
-
-	idx = memcg_cache_id(memcg);
-
-	get_online_cpus();
-	get_online_mems();
-
-	mutex_lock(&slab_mutex);
-	list_for_each_entry(s, &slab_root_caches, root_caches_node) {
-		arr = rcu_dereference_protected(s->memcg_params.memcg_caches,
-						lockdep_is_held(&slab_mutex));
-		c = arr->entries[idx];
-		if (!c)
-			continue;
-
-		kmemcg_cache_deactivate(c);
-		arr->entries[idx] = NULL;
-	}
-	nr_reparented = 0;
-	list_for_each_entry(s, &memcg->kmem_caches,
-			    memcg_params.kmem_caches_node) {
-		WRITE_ONCE(s->memcg_params.memcg, parent);
-		css_put(&memcg->css);
-		nr_reparented++;
-	}
-	if (nr_reparented) {
-		list_splice_init(&memcg->kmem_caches,
-				 &parent->kmem_caches);
-		css_get_many(&parent->css, nr_reparented);
-	}
-	mutex_unlock(&slab_mutex);
-
-	put_online_mems();
-	put_online_cpus();
-}
-
 static int shutdown_memcg_caches(struct kmem_cache *s)
 {
-	struct memcg_cache_array *arr;
-	struct kmem_cache *c, *c2;
-	LIST_HEAD(busy);
-	int i;
-
 	BUG_ON(!is_root_cache(s));
 
-	/*
-	 * First, shutdown active caches, i.e. caches that belong to online
-	 * memory cgroups.
-	 */
-	arr = rcu_dereference_protected(s->memcg_params.memcg_caches,
-					lockdep_is_held(&slab_mutex));
-	for_each_memcg_cache_index(i) {
-		c = arr->entries[i];
-		if (!c)
-			continue;
-		if (shutdown_cache(c))
-			/*
-			 * The cache still has objects. Move it to a temporary
-			 * list so as not to try to destroy it for a second
-			 * time while iterating over inactive caches below.
-			 */
-			list_move(&c->memcg_params.children_node, &busy);
-		else
-			/*
-			 * The cache is empty and will be destroyed soon. Clear
-			 * the pointer to it in the memcg_caches array so that
-			 * it will never be accessed even if the root cache
-			 * stays alive.
-			 */
-			arr->entries[i] = NULL;
-	}
-
-	/*
-	 * Second, shutdown all caches left from memory cgroups that are now
-	 * offline.
-	 */
-	list_for_each_entry_safe(c, c2, &s->memcg_params.children,
-				 memcg_params.children_node)
-		shutdown_cache(c);
+	if (s->memcg_params.memcg_cache)
+		WARN_ON(shutdown_cache(s->memcg_params.memcg_cache));
 
-	list_splice(&busy, &s->memcg_params.children);
-
-	/*
-	 * A cache being destroyed must be empty. In particular, this means
-	 * that all per memcg caches attached to it must be empty too.
-	 */
-	if (!list_empty(&s->memcg_params.children))
-		return -EBUSY;
 	return 0;
 }
 
-static void memcg_set_kmem_cache_dying(struct kmem_cache *s)
-{
-	spin_lock_irq(&memcg_kmem_wq_lock);
-	s->memcg_params.dying = true;
-	spin_unlock_irq(&memcg_kmem_wq_lock);
-}
-
 static void flush_memcg_workqueue(struct kmem_cache *s)
 {
-	/*
-	 * SLAB and SLUB deactivate the kmem_caches through call_rcu. Make
-	 * sure all registered rcu callbacks have been invoked.
-	 */
-	rcu_barrier();
-
 	/*
 	 * SLAB and SLUB create memcg kmem_caches through workqueue and SLUB
 	 * deactivates the memcg kmem_caches through workqueue. Make sure all
@@ -918,30 +595,21 @@ static void flush_memcg_workqueue(struct kmem_cache *s)
 	 */
 	if (likely(memcg_kmem_cache_wq))
 		flush_workqueue(memcg_kmem_cache_wq);
-
-	/*
-	 * If we're racing with children kmem_cache deactivation, it might
-	 * take another rcu grace period to complete their destruction.
-	 * At this moment the corresponding percpu_ref_kill() call should be
-	 * done, but it might take another rcu grace period to complete
-	 * switching to the atomic mode.
-	 * Please, note that we check without grabbing the slab_mutex. It's safe
-	 * because at this moment the children list can't grow.
-	 */
-	if (!list_empty(&s->memcg_params.children))
-		rcu_barrier();
 }
 #else
 static inline int shutdown_memcg_caches(struct kmem_cache *s)
 {
 	return 0;
 }
+
+static inline void flush_memcg_workqueue(struct kmem_cache *s)
+{
+}
 #endif /* CONFIG_MEMCG_KMEM */
 
 void slab_kmem_cache_release(struct kmem_cache *s)
 {
 	__kmem_cache_release(s);
-	destroy_memcg_params(s);
 	kfree_const(s->name);
 	kmem_cache_free(kmem_cache, s);
 }
@@ -953,6 +621,8 @@ void kmem_cache_destroy(struct kmem_cache *s)
 	if (unlikely(!s))
 		return;
 
+	flush_memcg_workqueue(s);
+
 	get_online_cpus();
 	get_online_mems();
 
@@ -962,22 +632,6 @@ void kmem_cache_destroy(struct kmem_cache *s)
 	if (s->refcount)
 		goto out_unlock;
 
-#ifdef CONFIG_MEMCG_KMEM
-	memcg_set_kmem_cache_dying(s);
-
-	mutex_unlock(&slab_mutex);
-
-	put_online_mems();
-	put_online_cpus();
-
-	flush_memcg_workqueue(s);
-
-	get_online_cpus();
-	get_online_mems();
-
-	mutex_lock(&slab_mutex);
-#endif
-
 	err = shutdown_memcg_caches(s);
 	if (!err)
 		err = shutdown_cache(s);
@@ -1017,7 +671,7 @@ int kmem_cache_shrink(struct kmem_cache *cachep)
 EXPORT_SYMBOL(kmem_cache_shrink);
 
 /**
- * kmem_cache_shrink_all - shrink a cache and all memcg caches for root cache
+ * kmem_cache_shrink_all - shrink root and memcg caches
  * @s: The cache pointer
  */
 void kmem_cache_shrink_all(struct kmem_cache *s)
@@ -1034,21 +688,11 @@ void kmem_cache_shrink_all(struct kmem_cache *s)
 	kasan_cache_shrink(s);
 	__kmem_cache_shrink(s);
 
-	/*
-	 * We have to take the slab_mutex to protect from the memcg list
-	 * modification.
-	 */
-	mutex_lock(&slab_mutex);
-	for_each_memcg_cache(c, s) {
-		/*
-		 * Don't need to shrink deactivated memcg caches.
-		 */
-		if (s->flags & SLAB_DEACTIVATED)
-			continue;
+	c = memcg_cache(s);
+	if (c) {
 		kasan_cache_shrink(c);
 		__kmem_cache_shrink(c);
 	}
-	mutex_unlock(&slab_mutex);
 	put_online_mems();
 	put_online_cpus();
 }
@@ -1103,7 +747,7 @@ struct kmem_cache *__init create_kmalloc_cache(const char *name,
 
 	create_boot_cache(s, name, size, flags, useroffset, usersize);
 	list_add(&s->list, &slab_caches);
-	memcg_link_cache(s, NULL);
+	memcg_link_cache(s);
 	s->refcount = 1;
 	return s;
 }
@@ -1472,7 +1116,8 @@ memcg_accumulate_slabinfo(struct kmem_cache *s, struct slabinfo *info)
 	if (!is_root_cache(s))
 		return;
 
-	for_each_memcg_cache(c, s) {
+	c = memcg_cache(s);
+	if (c) {
 		memset(&sinfo, 0, sizeof(sinfo));
 		get_slabinfo(c, &sinfo);
 
@@ -1603,7 +1248,7 @@ module_init(slab_proc_init);
 
 #if defined(CONFIG_DEBUG_FS) && defined(CONFIG_MEMCG_KMEM)
 /*
- * Display information about kmem caches that have child memcg caches.
+ * Display information about kmem caches that have memcg cache.
  */
 static int memcg_slabinfo_show(struct seq_file *m, void *unused)
 {
@@ -1615,9 +1260,9 @@ static int memcg_slabinfo_show(struct seq_file *m, void *unused)
 	seq_puts(m, " <active_slabs> <num_slabs>\n");
 	list_for_each_entry(s, &slab_root_caches, root_caches_node) {
 		/*
-		 * Skip kmem caches that don't have any memcg children.
+		 * Skip kmem caches that don't have the memcg cache.
 		 */
-		if (list_empty(&s->memcg_params.children))
+		if (!s->memcg_params.memcg_cache)
 			continue;
 
 		memset(&sinfo, 0, sizeof(sinfo));
@@ -1626,23 +1271,13 @@ static int memcg_slabinfo_show(struct seq_file *m, void *unused)
 			   cache_name(s), sinfo.active_objs, sinfo.num_objs,
 			   sinfo.active_slabs, sinfo.num_slabs);
 
-		for_each_memcg_cache(c, s) {
-			struct cgroup_subsys_state *css;
-			char *status = "";
-
-			css = &c->memcg_params.memcg->css;
-			if (!(css->flags & CSS_ONLINE))
-				status = ":dead";
-			else if (c->flags & SLAB_DEACTIVATED)
-				status = ":deact";
-
-			memset(&sinfo, 0, sizeof(sinfo));
-			get_slabinfo(c, &sinfo);
-			seq_printf(m, "%-17s %4d%-6s %6lu %6lu %6lu %6lu\n",
-				   cache_name(c), css->id, status,
-				   sinfo.active_objs, sinfo.num_objs,
-				   sinfo.active_slabs, sinfo.num_slabs);
-		}
+		c = s->memcg_params.memcg_cache;
+		memset(&sinfo, 0, sizeof(sinfo));
+		get_slabinfo(c, &sinfo);
+		seq_printf(m, "%-17s %4d %6lu %6lu %6lu %6lu\n",
+			   cache_name(c), root_mem_cgroup->css.id,
+			   sinfo.active_objs, sinfo.num_objs,
+			   sinfo.active_slabs, sinfo.num_slabs);
 	}
 	mutex_unlock(&slab_mutex);
 	return 0;
diff --git a/mm/slub.c b/mm/slub.c
index 135072f4c635..603890c72f0b 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -4195,36 +4195,6 @@ int __kmem_cache_shrink(struct kmem_cache *s)
 	return ret;
 }
 
-#ifdef CONFIG_MEMCG
-void __kmemcg_cache_deactivate_after_rcu(struct kmem_cache *s)
-{
-	/*
-	 * Called with all the locks held after a sched RCU grace period.
-	 * Even if @s becomes empty after shrinking, we can't know that @s
-	 * doesn't have allocations already in-flight and thus can't
-	 * destroy @s until the associated memcg is released.
-	 *
-	 * However, let's remove the sysfs files for empty caches here.
-	 * Each cache has a lot of interface files which aren't
-	 * particularly useful for empty draining caches; otherwise, we can
-	 * easily end up with millions of unnecessary sysfs files on
-	 * systems which have a lot of memory and transient cgroups.
-	 */
-	if (!__kmem_cache_shrink(s))
-		sysfs_slab_remove(s);
-}
-
-void __kmemcg_cache_deactivate(struct kmem_cache *s)
-{
-	/*
-	 * Disable empty slabs caching. Used to avoid pinning offline
-	 * memory cgroups by kmem pages that can be freed.
-	 */
-	slub_set_cpu_partial(s, 0);
-	s->min_partial = 0;
-}
-#endif	/* CONFIG_MEMCG */
-
 static int slab_mem_going_offline_callback(void *arg)
 {
 	struct kmem_cache *s;
@@ -4381,7 +4351,7 @@ static struct kmem_cache * __init bootstrap(struct kmem_cache *static_cache)
 	}
 	slab_init_memcg_params(s);
 	list_add(&s->list, &slab_caches);
-	memcg_link_cache(s, NULL);
+	memcg_link_cache(s);
 	return s;
 }
 
@@ -4449,7 +4419,8 @@ __kmem_cache_alias(const char *name, unsigned int size, unsigned int align,
 		s->object_size = max(s->object_size, size);
 		s->inuse = max(s->inuse, ALIGN(size, sizeof(void *)));
 
-		for_each_memcg_cache(c, s) {
+		c = memcg_cache(s);
+		if (c) {
 			c->object_size = s->object_size;
 			c->inuse = max(c->inuse, ALIGN(size, sizeof(void *)));
 		}
@@ -5582,7 +5553,8 @@ static ssize_t slab_attr_store(struct kobject *kobj,
 		 * directly either failed or succeeded, in which case we loop
 		 * through the descendants with best-effort propagation.
 		 */
-		for_each_memcg_cache(c, s)
+		c = memcg_cache(s);
+		if (c)
 			attribute->store(c, buf, len);
 		mutex_unlock(&slab_mutex);
 	}
