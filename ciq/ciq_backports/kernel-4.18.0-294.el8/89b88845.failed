seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Ahmed S. Darwish <a.darwish@linutronix.de>
commit 89b88845e05752b3d684eaf147f457c8dfa99c5f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/89b88845.failed

seqlock.h is now included by kernel's RST documentation, but a small
number of the the exported seqlock.h functions are kernel-doc annotated.

Add kernel-doc for all seqlock.h exported APIs.

	Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20200720155530.1173732-6-a.darwish@linutronix.de
(cherry picked from commit 89b88845e05752b3d684eaf147f457c8dfa99c5f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/seqlock.h
diff --cc include/linux/seqlock.h
index 07f6a4bc9dd1,85fb3ac93ffb..000000000000
--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@@ -136,30 -137,10 +144,33 @@@ repeat
  }
  
  /**
++<<<<<<< HEAD
 + * raw_read_seqcount - Read the raw seqcount
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
 + *
 + * raw_read_seqcount opens a read critical section of the given
 + * seqcount without any lockdep checking and without checking or
 + * masking the LSB. Calling code is responsible for handling that.
 + */
 +static inline unsigned raw_read_seqcount(const seqcount_t *s)
 +{
 +	unsigned ret = READ_ONCE(s->sequence);
 +	smp_rmb();
 +	kcsan_atomic_next(KCSAN_SEQLOCK_REGION_MAX);
 +	return ret;
 +}
 +
 +/**
 + * raw_read_seqcount_begin - start seq-read critical section w/o lockdep
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
++=======
+  * raw_read_seqcount_begin() - begin a seqcount_t read section w/o lockdep
+  * @s: Pointer to seqcount_t
++>>>>>>> 89b88845e057 (seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs)
   *
-  * raw_read_seqcount_begin opens a read critical section of the given
-  * seqcount, but without any lockdep checking. Validity of the critical
-  * section is tested by checking read_seqcount_retry function.
+  * Return: count to be passed to read_seqcount_retry()
   */
  static inline unsigned raw_read_seqcount_begin(const seqcount_t *s)
  {
@@@ -184,18 -162,40 +192,46 @@@ static inline unsigned read_seqcount_be
  }
  
  /**
++<<<<<<< HEAD
 + * raw_seqcount_begin - begin a seq-read critical section
 + * @s: pointer to seqcount_t
 + * Returns: count to be passed to read_seqcount_retry
++=======
+  * raw_read_seqcount() - read the raw seqcount_t counter value
+  * @s: Pointer to seqcount_t
   *
-  * raw_seqcount_begin opens a read critical section of the given seqcount.
-  * Validity of the critical section is tested by checking read_seqcount_retry
-  * function.
+  * raw_read_seqcount opens a read critical section of the given
+  * seqcount_t, without any lockdep checking, and without checking or
+  * masking the sequence counter LSB. Calling code is responsible for
+  * handling that.
   *
-  * Unlike read_seqcount_begin(), this function will not wait for the count
-  * to stabilize. If a writer is active when we begin, we will fail the
-  * read_seqcount_retry() instead of stabilizing at the beginning of the
-  * critical section.
+  * Return: count to be passed to read_seqcount_retry()
+  */
+ static inline unsigned raw_read_seqcount(const seqcount_t *s)
+ {
+ 	unsigned ret = READ_ONCE(s->sequence);
+ 	smp_rmb();
+ 	kcsan_atomic_next(KCSAN_SEQLOCK_REGION_MAX);
+ 	return ret;
+ }
+ 
+ /**
+  * raw_seqcount_begin() - begin a seqcount_t read critical section w/o
+  *                        lockdep and w/o counter stabilization
+  * @s: Pointer to seqcount_t
++>>>>>>> 89b88845e057 (seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs)
+  *
+  * raw_seqcount_begin opens a read critical section of the given
+  * seqcount_t. Unlike read_seqcount_begin(), this function will not wait
+  * for the count to stabilize. If a writer is active when it begins, it
+  * will fail the read_seqcount_retry() at the end of the read critical
+  * section instead of stabilizing at the beginning of it.
+  *
+  * Use this only in special kernel hot paths where the read section is
+  * small and has a high probability of success through other external
+  * means. It will save a single branching instruction.
+  *
+  * Return: count to be passed to read_seqcount_retry()
   */
  static inline unsigned raw_seqcount_begin(const seqcount_t *s)
  {
@@@ -241,8 -243,10 +279,15 @@@ static inline int read_seqcount_retry(c
  	return __read_seqcount_retry(s, start);
  }
  
++<<<<<<< HEAD
 +
 +
++=======
+ /**
+  * raw_write_seqcount_begin() - start a seqcount_t write section w/o lockdep
+  * @s: Pointer to seqcount_t
+  */
++>>>>>>> 89b88845e057 (seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs)
  static inline void raw_write_seqcount_begin(seqcount_t *s)
  {
  	kcsan_nestable_atomic_begin();
@@@ -257,15 -265,57 +306,60 @@@ static inline void raw_write_seqcount_e
  	kcsan_nestable_atomic_end();
  }
  
++<<<<<<< HEAD
++=======
  /**
-  * raw_write_seqcount_barrier - do a seq write barrier
-  * @s: pointer to seqcount_t
+  * write_seqcount_begin_nested() - start a seqcount_t write section with
+  *                                 custom lockdep nesting level
+  * @s: Pointer to seqcount_t
+  * @subclass: lockdep nesting level
   *
-  * This can be used to provide an ordering guarantee instead of the
-  * usual consistency guarantee. It is one wmb cheaper, because we can
-  * collapse the two back-to-back wmb()s.
+  * See Documentation/locking/lockdep-design.rst
+  */
+ static inline void write_seqcount_begin_nested(seqcount_t *s, int subclass)
+ {
+ 	raw_write_seqcount_begin(s);
+ 	seqcount_acquire(&s->dep_map, subclass, 0, _RET_IP_);
+ }
+ 
+ /**
+  * write_seqcount_begin() - start a seqcount_t write side critical section
+  * @s: Pointer to seqcount_t
+  *
+  * write_seqcount_begin opens a write side critical section of the given
+  * seqcount_t.
+  *
+  * Context: seqcount_t write side critical sections must be serialized and
+  * non-preemptible. If readers can be invoked from hardirq or softirq
+  * context, interrupts or bottom halves must be respectively disabled.
+  */
+ static inline void write_seqcount_begin(seqcount_t *s)
+ {
+ 	write_seqcount_begin_nested(s, 0);
+ }
+ 
+ /**
+  * write_seqcount_end() - end a seqcount_t write side critical section
+  * @s: Pointer to seqcount_t
+  *
+  * The write section must've been opened with write_seqcount_begin().
+  */
+ static inline void write_seqcount_end(seqcount_t *s)
+ {
+ 	seqcount_release(&s->dep_map, _RET_IP_);
+ 	raw_write_seqcount_end(s);
+ }
+ 
++>>>>>>> 89b88845e057 (seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs)
+ /**
+  * raw_write_seqcount_barrier() - do a seqcount_t write barrier
+  * @s: Pointer to seqcount_t
+  *
+  * This can be used to provide an ordering guarantee instead of the usual
+  * consistency guarantee. It is one wmb cheaper, because it can collapse
+  * the two back-to-back wmb()s.
   *
 - * Note that writes surrounding the barrier should be declared atomic (e.g.
 + * Note that, writes surrounding the barrier should be declared atomic (e.g.
   * via WRITE_ONCE): a) to ensure the writes become visible to other threads
   * atomically, avoiding compiler optimizations; b) to document which writes are
   * meant to propagate to the reader critical section. This is necessary because
@@@ -307,6 -357,37 +401,40 @@@ static inline void raw_write_seqcount_b
  	kcsan_nestable_atomic_end();
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * write_seqcount_invalidate() - invalidate in-progress seqcount_t read
+  *                               side operations
+  * @s: Pointer to seqcount_t
+  *
+  * After write_seqcount_invalidate, no seqcount_t read side operations
+  * will complete successfully and see data older than this.
+  */
+ static inline void write_seqcount_invalidate(seqcount_t *s)
+ {
+ 	smp_wmb();
+ 	kcsan_nestable_atomic_begin();
+ 	s->sequence+=2;
+ 	kcsan_nestable_atomic_end();
+ }
+ 
+ /**
+  * raw_read_seqcount_latch() - pick even/odd seqcount_t latch data copy
+  * @s: Pointer to seqcount_t
+  *
+  * Use seqcount_t latching to switch between two storage places protected
+  * by a sequence counter. Doing so allows having interruptible, preemptible,
+  * seqcount_t write side critical sections.
+  *
+  * Check raw_write_seqcount_latch() for more details and a full reader and
+  * writer usage example.
+  *
+  * Return: sequence counter raw value. Use the lowest bit as an index for
+  * picking which data copy to read. The full counter value must then be
+  * checked with read_seqcount_retry().
+  */
++>>>>>>> 89b88845e057 (seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs)
  static inline int raw_read_seqcount_latch(seqcount_t *s)
  {
  	/* Pairs with the first smp_wmb() in raw_write_seqcount_latch() */
@@@ -561,34 -712,14 +789,45 @@@ static inline void read_sequnlock_excl(
  }
  
  /**
++<<<<<<< HEAD
 + * read_seqbegin_or_lock - begin a sequence number check or locking block
 + * @lock: sequence lock
 + * @seq : sequence number to be checked
 + *
 + * First try it once optimistically without taking the lock. If that fails,
 + * take the lock. The sequence number is also used as a marker for deciding
 + * whether to be a reader (even) or writer (odd).
 + * N.B. seq must be initialized to an even number to begin with.
 + */
 +static inline void read_seqbegin_or_lock(seqlock_t *lock, int *seq)
 +{
 +	if (!(*seq & 1))	/* Even */
 +		*seq = read_seqbegin(lock);
 +	else			/* Odd */
 +		read_seqlock_excl(lock);
 +}
 +
 +static inline int need_seqretry(seqlock_t *lock, int seq)
 +{
 +	return !(seq & 1) && read_seqretry(lock, seq);
 +}
 +
 +static inline void done_seqretry(seqlock_t *lock, int seq)
 +{
 +	if (seq & 1)
 +		read_sequnlock_excl(lock);
 +}
 +
++=======
+  * read_seqlock_excl_bh() - start a seqlock_t locking reader section with
+  *			    softirqs disabled
+  * @sl: Pointer to seqlock_t
+  *
+  * _bh variant of read_seqlock_excl(). Use this variant only if the
+  * seqlock_t write side section, *or other read sections*, can be invoked
+  * from softirq contexts.
+  */
++>>>>>>> 89b88845e057 (seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs)
  static inline void read_seqlock_excl_bh(seqlock_t *sl)
  {
  	spin_lock_bh(&sl->lock);
@@@ -626,6 -793,91 +901,94 @@@ read_sequnlock_excl_irqrestore(seqlock_
  	spin_unlock_irqrestore(&sl->lock, flags);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * read_seqbegin_or_lock() - begin a seqlock_t lockless or locking reader
+  * @lock: Pointer to seqlock_t
+  * @seq : Marker and return parameter. If the passed value is even, the
+  * reader will become a *lockless* seqlock_t reader as in read_seqbegin().
+  * If the passed value is odd, the reader will become a *locking* reader
+  * as in read_seqlock_excl().  In the first call to this function, the
+  * caller *must* initialize and pass an even value to @seq; this way, a
+  * lockless read can be optimistically tried first.
+  *
+  * read_seqbegin_or_lock is an API designed to optimistically try a normal
+  * lockless seqlock_t read section first.  If an odd counter is found, the
+  * lockless read trial has failed, and the next read iteration transforms
+  * itself into a full seqlock_t locking reader.
+  *
+  * This is typically used to avoid seqlock_t lockless readers starvation
+  * (too much retry loops) in the case of a sharp spike in write side
+  * activity.
+  *
+  * Context: if the seqlock_t write section, *or other read sections*, can
+  * be invoked from hardirq or softirq contexts, use the _irqsave or _bh
+  * variant of this function instead.
+  *
+  * Check Documentation/locking/seqlock.rst for template example code.
+  *
+  * Return: the encountered sequence counter value, through the @seq
+  * parameter, which is overloaded as a return parameter. This returned
+  * value must be checked with need_seqretry(). If the read section need to
+  * be retried, this returned value must also be passed as the @seq
+  * parameter of the next read_seqbegin_or_lock() iteration.
+  */
+ static inline void read_seqbegin_or_lock(seqlock_t *lock, int *seq)
+ {
+ 	if (!(*seq & 1))	/* Even */
+ 		*seq = read_seqbegin(lock);
+ 	else			/* Odd */
+ 		read_seqlock_excl(lock);
+ }
+ 
+ /**
+  * need_seqretry() - validate seqlock_t "locking or lockless" read section
+  * @lock: Pointer to seqlock_t
+  * @seq: sequence count, from read_seqbegin_or_lock()
+  *
+  * Return: true if a read section retry is required, false otherwise
+  */
+ static inline int need_seqretry(seqlock_t *lock, int seq)
+ {
+ 	return !(seq & 1) && read_seqretry(lock, seq);
+ }
+ 
+ /**
+  * done_seqretry() - end seqlock_t "locking or lockless" reader section
+  * @lock: Pointer to seqlock_t
+  * @seq: count, from read_seqbegin_or_lock()
+  *
+  * done_seqretry finishes the seqlock_t read side critical section started
+  * with read_seqbegin_or_lock() and validated by need_seqretry().
+  */
+ static inline void done_seqretry(seqlock_t *lock, int seq)
+ {
+ 	if (seq & 1)
+ 		read_sequnlock_excl(lock);
+ }
+ 
+ /**
+  * read_seqbegin_or_lock_irqsave() - begin a seqlock_t lockless reader, or
+  *                                   a non-interruptible locking reader
+  * @lock: Pointer to seqlock_t
+  * @seq:  Marker and return parameter. Check read_seqbegin_or_lock().
+  *
+  * This is the _irqsave variant of read_seqbegin_or_lock(). Use it only if
+  * the seqlock_t write section, *or other read sections*, can be invoked
+  * from hardirq context.
+  *
+  * Note: Interrupts will be disabled only for "locking reader" mode.
+  *
+  * Return:
+  *
+  *   1. The saved local interrupts state in case of a locking reader, to
+  *      be passed to done_seqretry_irqrestore().
+  *
+  *   2. The encountered sequence counter value, returned through @seq
+  *      overloaded as a return parameter. Check read_seqbegin_or_lock().
+  */
++>>>>>>> 89b88845e057 (seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs)
  static inline unsigned long
  read_seqbegin_or_lock_irqsave(seqlock_t *lock, int *seq)
  {
* Unmerged path include/linux/seqlock.h
