bpf: Enable bpf_iter targets registering ctx argument types

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Yonghong Song <yhs@fb.com>
commit 3c32cc1bceba8a1755dc35cd97516f6c67856844
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/3c32cc1b.failed

Commit b121b341e598 ("bpf: Add PTR_TO_BTF_ID_OR_NULL
support") adds a field btf_id_or_null_non0_off to
bpf_prog->aux structure to indicate that the
first ctx argument is PTR_TO_BTF_ID reg_type and
all others are PTR_TO_BTF_ID_OR_NULL.
This approach does not really scale if we have
other different reg types in the future, e.g.,
a pointer to a buffer.

This patch enables bpf_iter targets registering ctx argument
reg types which may be different from the default one.
For example, for pointers to structures, the default reg_type
is PTR_TO_BTF_ID for tracing program. The target can register
a particular pointer type as PTR_TO_BTF_ID_OR_NULL which can
be used by the verifier to enforce accesses.

	Signed-off-by: Yonghong Song <yhs@fb.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Acked-by: Andrii Nakryiko <andriin@fb.com>
Link: https://lore.kernel.org/bpf/20200513180221.2949882-1-yhs@fb.com
(cherry picked from commit 3c32cc1bceba8a1755dc35cd97516f6c67856844)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/bpf.h
#	kernel/bpf/bpf_iter.c
#	kernel/bpf/btf.c
#	kernel/bpf/map_iter.c
#	kernel/bpf/task_iter.c
#	kernel/bpf/verifier.c
#	net/ipv6/ip6_fib.c
#	net/ipv6/route.c
#	net/netlink/af_netlink.c
diff --cc include/linux/bpf.h
index 531af1e44e56,c45d198ac38c..000000000000
--- a/include/linux/bpf.h
+++ b/include/linux/bpf.h
@@@ -656,37 -643,42 +656,58 @@@ struct bpf_jit_poke_descriptor 
  	u16 reason;
  };
  
+ /* reg_type info for ctx arguments */
+ struct bpf_ctx_arg_aux {
+ 	u32 offset;
+ 	enum bpf_reg_type reg_type;
+ };
+ 
  struct bpf_prog_aux {
 -	atomic64_t refcnt;
 +	RH_KABI_BROKEN_REPLACE(atomic_t refcnt, atomic64_t refcnt)
  	u32 used_map_cnt;
  	u32 max_ctx_offset;
 -	u32 max_pkt_offset;
 -	u32 max_tp_access;
 +	/* not protected by KABI, safe to extend in the middle */
 +	RH_KABI_BROKEN_INSERT(u32 max_pkt_offset)
 +	RH_KABI_BROKEN_INSERT(u32 max_tp_access)
  	u32 stack_depth;
  	u32 id;
  	u32 func_cnt; /* used by non-func prog as the number of func progs */
++<<<<<<< HEAD
 +	RH_KABI_BROKEN_INSERT(u32 func_idx) /* 0 for non-func prog, the index in func array for func prog */
 +	RH_KABI_BROKEN_INSERT(u32 attach_btf_id) /* in-kernel BTF type id to attach to */
 +	RH_KABI_BROKEN_INSERT(struct bpf_prog *linked_prog)
 +	RH_KABI_BROKEN_INSERT(bool verifier_zext) /* Zero extensions has been inserted by verifier. */
 +	bool offload_requested;
 +	RH_KABI_BROKEN_INSERT(bool attach_btf_trace) /* true if attaching to BTF-enabled raw tp */
 +	RH_KABI_BROKEN_INSERT(bool func_proto_unreliable)
 +	RH_KABI_BROKEN_INSERT(enum bpf_tramp_prog_type trampoline_prog_type)
 +	RH_KABI_BROKEN_INSERT(struct bpf_trampoline *trampoline)
 +	RH_KABI_BROKEN_INSERT(struct hlist_node tramp_hlist)
++=======
+ 	u32 func_idx; /* 0 for non-func prog, the index in func array for func prog */
+ 	u32 attach_btf_id; /* in-kernel BTF type id to attach to */
+ 	u32 ctx_arg_info_size;
+ 	const struct bpf_ctx_arg_aux *ctx_arg_info;
+ 	struct bpf_prog *linked_prog;
+ 	bool verifier_zext; /* Zero extensions has been inserted by verifier. */
+ 	bool offload_requested;
+ 	bool attach_btf_trace; /* true if attaching to BTF-enabled raw tp */
+ 	bool func_proto_unreliable;
+ 	enum bpf_tramp_prog_type trampoline_prog_type;
+ 	struct bpf_trampoline *trampoline;
+ 	struct hlist_node tramp_hlist;
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  	/* BTF_KIND_FUNC_PROTO for valid attach_btf_id */
 -	const struct btf_type *attach_func_proto;
 +	RH_KABI_BROKEN_INSERT(const struct btf_type *attach_func_proto)
  	/* function name for valid attach_btf_id */
 -	const char *attach_func_name;
 +	RH_KABI_BROKEN_INSERT(const char *attach_func_name)
  	struct bpf_prog **func;
  	void *jit_data; /* JIT specific data. arch dependent */
 -	struct bpf_jit_poke_descriptor *poke_tab;
 -	u32 size_poke_tab;
 -	struct bpf_ksym ksym;
 +	RH_KABI_BROKEN_INSERT(struct bpf_jit_poke_descriptor *poke_tab)
 +	RH_KABI_BROKEN_INSERT(u32 size_poke_tab)
 +	RH_KABI_BROKEN_REMOVE(struct latch_tree_node ksym_tnode)
 +	RH_KABI_BROKEN_REMOVE(struct list_head ksym_lnode)
 +	RH_KABI_BROKEN_INSERT(struct bpf_ksym ksym)
  	const struct bpf_prog_ops *ops;
  	struct bpf_map **used_maps;
  	struct bpf_prog *prog;
@@@ -1156,10 -1153,24 +1178,12 @@@ struct bpf_iter_reg 
  	bpf_iter_init_seq_priv_t init_seq_private;
  	bpf_iter_fini_seq_priv_t fini_seq_private;
  	u32 seq_priv_size;
+ 	u32 ctx_arg_info_size;
+ 	struct bpf_ctx_arg_aux ctx_arg_info[BPF_ITER_CTX_ARG_MAX];
  };
  
 -struct bpf_iter_meta {
 -	__bpf_md_ptr(struct seq_file *, seq);
 -	u64 session_id;
 -	u64 seq_num;
 -};
 -
 -int bpf_iter_reg_target(const struct bpf_iter_reg *reg_info);
 -void bpf_iter_unreg_target(const struct bpf_iter_reg *reg_info);
 -bool bpf_iter_prog_supported(struct bpf_prog *prog);
 -int bpf_iter_link_attach(const union bpf_attr *attr, struct bpf_prog *prog);
 -int bpf_iter_new_fd(struct bpf_link *link);
 -bool bpf_link_is_iter(struct bpf_link *link);
 -struct bpf_prog *bpf_iter_get_info(struct bpf_iter_meta *meta, bool in_stop);
 -int bpf_iter_run_prog(struct bpf_prog *prog, void *ctx);
 +int bpf_iter_reg_target(struct bpf_iter_reg *reg_info);
 +void bpf_iter_unreg_target(const char *target);
  
  int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);
  int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);
diff --cc kernel/bpf/bpf_iter.c
index 5a8119d17d14,dd612b80b9fe..000000000000
--- a/kernel/bpf/bpf_iter.c
+++ b/kernel/bpf/bpf_iter.c
@@@ -57,3 -275,265 +57,268 @@@ void bpf_iter_unreg_target(const char *
  
  	WARN_ON(found == false);
  }
++<<<<<<< HEAD
++=======
+ 
+ static void cache_btf_id(struct bpf_iter_target_info *tinfo,
+ 			 struct bpf_prog *prog)
+ {
+ 	tinfo->btf_id = prog->aux->attach_btf_id;
+ }
+ 
+ bool bpf_iter_prog_supported(struct bpf_prog *prog)
+ {
+ 	const char *attach_fname = prog->aux->attach_func_name;
+ 	u32 prog_btf_id = prog->aux->attach_btf_id;
+ 	const char *prefix = BPF_ITER_FUNC_PREFIX;
+ 	struct bpf_iter_target_info *tinfo;
+ 	int prefix_len = strlen(prefix);
+ 	bool supported = false;
+ 
+ 	if (strncmp(attach_fname, prefix, prefix_len))
+ 		return false;
+ 
+ 	mutex_lock(&targets_mutex);
+ 	list_for_each_entry(tinfo, &targets, list) {
+ 		if (tinfo->btf_id && tinfo->btf_id == prog_btf_id) {
+ 			supported = true;
+ 			break;
+ 		}
+ 		if (!strcmp(attach_fname + prefix_len, tinfo->reg_info->target)) {
+ 			cache_btf_id(tinfo, prog);
+ 			supported = true;
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&targets_mutex);
+ 
+ 	if (supported) {
+ 		prog->aux->ctx_arg_info_size = tinfo->reg_info->ctx_arg_info_size;
+ 		prog->aux->ctx_arg_info = tinfo->reg_info->ctx_arg_info;
+ 	}
+ 
+ 	return supported;
+ }
+ 
+ static void bpf_iter_link_release(struct bpf_link *link)
+ {
+ }
+ 
+ static void bpf_iter_link_dealloc(struct bpf_link *link)
+ {
+ 	struct bpf_iter_link *iter_link =
+ 		container_of(link, struct bpf_iter_link, link);
+ 
+ 	kfree(iter_link);
+ }
+ 
+ static int bpf_iter_link_replace(struct bpf_link *link,
+ 				 struct bpf_prog *new_prog,
+ 				 struct bpf_prog *old_prog)
+ {
+ 	int ret = 0;
+ 
+ 	mutex_lock(&link_mutex);
+ 	if (old_prog && link->prog != old_prog) {
+ 		ret = -EPERM;
+ 		goto out_unlock;
+ 	}
+ 
+ 	if (link->prog->type != new_prog->type ||
+ 	    link->prog->expected_attach_type != new_prog->expected_attach_type ||
+ 	    link->prog->aux->attach_btf_id != new_prog->aux->attach_btf_id) {
+ 		ret = -EINVAL;
+ 		goto out_unlock;
+ 	}
+ 
+ 	old_prog = xchg(&link->prog, new_prog);
+ 	bpf_prog_put(old_prog);
+ 
+ out_unlock:
+ 	mutex_unlock(&link_mutex);
+ 	return ret;
+ }
+ 
+ static const struct bpf_link_ops bpf_iter_link_lops = {
+ 	.release = bpf_iter_link_release,
+ 	.dealloc = bpf_iter_link_dealloc,
+ 	.update_prog = bpf_iter_link_replace,
+ };
+ 
+ bool bpf_link_is_iter(struct bpf_link *link)
+ {
+ 	return link->ops == &bpf_iter_link_lops;
+ }
+ 
+ int bpf_iter_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)
+ {
+ 	struct bpf_link_primer link_primer;
+ 	struct bpf_iter_target_info *tinfo;
+ 	struct bpf_iter_link *link;
+ 	bool existed = false;
+ 	u32 prog_btf_id;
+ 	int err;
+ 
+ 	if (attr->link_create.target_fd || attr->link_create.flags)
+ 		return -EINVAL;
+ 
+ 	prog_btf_id = prog->aux->attach_btf_id;
+ 	mutex_lock(&targets_mutex);
+ 	list_for_each_entry(tinfo, &targets, list) {
+ 		if (tinfo->btf_id == prog_btf_id) {
+ 			existed = true;
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&targets_mutex);
+ 	if (!existed)
+ 		return -ENOENT;
+ 
+ 	link = kzalloc(sizeof(*link), GFP_USER | __GFP_NOWARN);
+ 	if (!link)
+ 		return -ENOMEM;
+ 
+ 	bpf_link_init(&link->link, BPF_LINK_TYPE_ITER, &bpf_iter_link_lops, prog);
+ 	link->tinfo = tinfo;
+ 
+ 	err  = bpf_link_prime(&link->link, &link_primer);
+ 	if (err) {
+ 		kfree(link);
+ 		return err;
+ 	}
+ 
+ 	return bpf_link_settle(&link_primer);
+ }
+ 
+ static void init_seq_meta(struct bpf_iter_priv_data *priv_data,
+ 			  struct bpf_iter_target_info *tinfo,
+ 			  struct bpf_prog *prog)
+ {
+ 	priv_data->tinfo = tinfo;
+ 	priv_data->prog = prog;
+ 	priv_data->session_id = atomic64_inc_return(&session_id);
+ 	priv_data->seq_num = 0;
+ 	priv_data->done_stop = false;
+ }
+ 
+ static int prepare_seq_file(struct file *file, struct bpf_iter_link *link)
+ {
+ 	struct bpf_iter_priv_data *priv_data;
+ 	struct bpf_iter_target_info *tinfo;
+ 	struct bpf_prog *prog;
+ 	u32 total_priv_dsize;
+ 	struct seq_file *seq;
+ 	int err = 0;
+ 
+ 	mutex_lock(&link_mutex);
+ 	prog = link->link.prog;
+ 	bpf_prog_inc(prog);
+ 	mutex_unlock(&link_mutex);
+ 
+ 	tinfo = link->tinfo;
+ 	total_priv_dsize = offsetof(struct bpf_iter_priv_data, target_private) +
+ 			   tinfo->reg_info->seq_priv_size;
+ 	priv_data = __seq_open_private(file, tinfo->reg_info->seq_ops,
+ 				       total_priv_dsize);
+ 	if (!priv_data) {
+ 		err = -ENOMEM;
+ 		goto release_prog;
+ 	}
+ 
+ 	if (tinfo->reg_info->init_seq_private) {
+ 		err = tinfo->reg_info->init_seq_private(priv_data->target_private);
+ 		if (err)
+ 			goto release_seq_file;
+ 	}
+ 
+ 	init_seq_meta(priv_data, tinfo, prog);
+ 	seq = file->private_data;
+ 	seq->private = priv_data->target_private;
+ 
+ 	return 0;
+ 
+ release_seq_file:
+ 	seq_release_private(file->f_inode, file);
+ 	file->private_data = NULL;
+ release_prog:
+ 	bpf_prog_put(prog);
+ 	return err;
+ }
+ 
+ int bpf_iter_new_fd(struct bpf_link *link)
+ {
+ 	struct file *file;
+ 	unsigned int flags;
+ 	int err, fd;
+ 
+ 	if (link->ops != &bpf_iter_link_lops)
+ 		return -EINVAL;
+ 
+ 	flags = O_RDONLY | O_CLOEXEC;
+ 	fd = get_unused_fd_flags(flags);
+ 	if (fd < 0)
+ 		return fd;
+ 
+ 	file = anon_inode_getfile("bpf_iter", &bpf_iter_fops, NULL, flags);
+ 	if (IS_ERR(file)) {
+ 		err = PTR_ERR(file);
+ 		goto free_fd;
+ 	}
+ 
+ 	err = prepare_seq_file(file,
+ 			       container_of(link, struct bpf_iter_link, link));
+ 	if (err)
+ 		goto free_file;
+ 
+ 	fd_install(fd, file);
+ 	return fd;
+ 
+ free_file:
+ 	fput(file);
+ free_fd:
+ 	put_unused_fd(fd);
+ 	return err;
+ }
+ 
+ struct bpf_prog *bpf_iter_get_info(struct bpf_iter_meta *meta, bool in_stop)
+ {
+ 	struct bpf_iter_priv_data *iter_priv;
+ 	struct seq_file *seq;
+ 	void *seq_priv;
+ 
+ 	seq = meta->seq;
+ 	if (seq->file->f_op != &bpf_iter_fops)
+ 		return NULL;
+ 
+ 	seq_priv = seq->private;
+ 	iter_priv = container_of(seq_priv, struct bpf_iter_priv_data,
+ 				 target_private);
+ 
+ 	if (in_stop && iter_priv->done_stop)
+ 		return NULL;
+ 
+ 	meta->session_id = iter_priv->session_id;
+ 	meta->seq_num = iter_priv->seq_num;
+ 
+ 	return iter_priv->prog;
+ }
+ 
+ int bpf_iter_run_prog(struct bpf_prog *prog, void *ctx)
+ {
+ 	int ret;
+ 
+ 	rcu_read_lock();
+ 	migrate_disable();
+ 	ret = BPF_PROG_RUN(prog, ctx);
+ 	migrate_enable();
+ 	rcu_read_unlock();
+ 
+ 	/* bpf program can only return 0 or 1:
+ 	 *  0 : okay
+ 	 *  1 : retry the same object
+ 	 * The bpf_iter_run_prog() return value
+ 	 * will be seq_ops->show() return value.
+ 	 */
+ 	return ret == 0 ? 0 : -EAGAIN;
+ }
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
diff --cc kernel/bpf/btf.c
index def14e76b9b3,58c9af1d4808..000000000000
--- a/kernel/bpf/btf.c
+++ b/kernel/bpf/btf.c
@@@ -3792,6 -3791,14 +3792,17 @@@ bool btf_ctx_access(int off, int size, 
  
  	/* this is a pointer to another type */
  	info->reg_type = PTR_TO_BTF_ID;
++<<<<<<< HEAD
++=======
+ 	for (i = 0; i < prog->aux->ctx_arg_info_size; i++) {
+ 		const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];
+ 
+ 		if (ctx_arg_info->offset == off) {
+ 			info->reg_type = ctx_arg_info->reg_type;
+ 			break;
+ 		}
+ 	}
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  
  	if (tgt_prog) {
  		ret = btf_translate_to_vmlinux(log, btf, t, tgt_prog->type, arg);
diff --cc kernel/bpf/map_iter.c
index 8162e0c00b9f,c69071e334bf..000000000000
--- a/kernel/bpf/map_iter.c
+++ b/kernel/bpf/map_iter.c
@@@ -81,17 -81,22 +81,33 @@@ static const struct seq_operations bpf_
  	.show	= bpf_map_seq_show,
  };
  
++<<<<<<< HEAD
++=======
+ static const struct bpf_iter_reg bpf_map_reg_info = {
+ 	.target			= "bpf_map",
+ 	.seq_ops		= &bpf_map_seq_ops,
+ 	.init_seq_private	= NULL,
+ 	.fini_seq_private	= NULL,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
+ 	.ctx_arg_info_size	= 1,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__bpf_map, map),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ };
+ 
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  static int __init bpf_map_iter_init(void)
  {
 -	return bpf_iter_reg_target(&bpf_map_reg_info);
 +	struct bpf_iter_reg reg_info = {
 +		.target			= "bpf_map",
 +		.seq_ops		= &bpf_map_seq_ops,
 +		.init_seq_private	= NULL,
 +		.fini_seq_private	= NULL,
 +		.seq_priv_size		= sizeof(struct bpf_iter_seq_map_info),
 +	};
 +
 +	return bpf_iter_reg_target(&reg_info);
  }
  
  late_initcall(bpf_map_iter_init);
diff --cc kernel/bpf/task_iter.c
index 135ad297b957,a9b7264dda08..000000000000
--- a/kernel/bpf/task_iter.c
+++ b/kernel/bpf/task_iter.c
@@@ -312,22 -306,36 +312,53 @@@ static const struct seq_operations task
  	.show	= task_file_seq_show,
  };
  
++<<<<<<< HEAD
++=======
+ static const struct bpf_iter_reg task_reg_info = {
+ 	.target			= "task",
+ 	.seq_ops		= &task_seq_ops,
+ 	.init_seq_private	= init_seq_pidns,
+ 	.fini_seq_private	= fini_seq_pidns,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_task_info),
+ 	.ctx_arg_info_size	= 1,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__task, task),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ };
+ 
+ static const struct bpf_iter_reg task_file_reg_info = {
+ 	.target			= "task_file",
+ 	.seq_ops		= &task_file_seq_ops,
+ 	.init_seq_private	= init_seq_pidns,
+ 	.fini_seq_private	= fini_seq_pidns,
+ 	.seq_priv_size		= sizeof(struct bpf_iter_seq_task_file_info),
+ 	.ctx_arg_info_size	= 2,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__task_file, task),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 		{ offsetof(struct bpf_iter__task_file, file),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ };
+ 
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  static int __init task_iter_init(void)
  {
 +	struct bpf_iter_reg task_file_reg_info = {
 +		.target			= "task_file",
 +		.seq_ops		= &task_file_seq_ops,
 +		.init_seq_private	= init_seq_pidns,
 +		.fini_seq_private	= fini_seq_pidns,
 +		.seq_priv_size		= sizeof(struct bpf_iter_seq_task_file_info),
 +	};
 +	struct bpf_iter_reg task_reg_info = {
 +		.target			= "task",
 +		.seq_ops		= &task_seq_ops,
 +		.init_seq_private	= init_seq_pidns,
 +		.fini_seq_private	= fini_seq_pidns,
 +		.seq_priv_size		= sizeof(struct bpf_iter_seq_task_info),
 +	};
  	int ret;
  
  	ret = bpf_iter_reg_target(&task_reg_info);
diff --cc kernel/bpf/verifier.c
index e98b26227300,a3f2af756fd6..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -10694,6 -10639,22 +10694,25 @@@ static int check_attach_btf_id(struct b
  		prog->aux->attach_func_proto = t;
  		prog->aux->attach_btf_trace = true;
  		return 0;
++<<<<<<< HEAD
++=======
+ 	case BPF_TRACE_ITER:
+ 		if (!btf_type_is_func(t)) {
+ 			verbose(env, "attach_btf_id %u is not a function\n",
+ 				btf_id);
+ 			return -EINVAL;
+ 		}
+ 		t = btf_type_by_id(btf, t->type);
+ 		if (!btf_type_is_func_proto(t))
+ 			return -EINVAL;
+ 		prog->aux->attach_func_name = tname;
+ 		prog->aux->attach_func_proto = t;
+ 		if (!bpf_iter_prog_supported(prog))
+ 			return -EINVAL;
+ 		ret = btf_distill_func_proto(&env->log, btf, t,
+ 					     tname, &fmodel);
+ 		return ret;
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  	default:
  		if (!prog_extension)
  			return -EINVAL;
diff --cc net/ipv6/ip6_fib.c
index 971d9d79307f,250ff52c674e..000000000000
--- a/net/ipv6/ip6_fib.c
+++ b/net/ipv6/ip6_fib.c
@@@ -2604,6 -2637,62 +2604,65 @@@ static void ipv6_route_seq_stop(struct 
  	rcu_read_unlock_bh();
  }
  
++<<<<<<< HEAD
++=======
+ #if IS_BUILTIN(CONFIG_IPV6) && defined(CONFIG_BPF_SYSCALL)
+ static int ipv6_route_prog_seq_show(struct bpf_prog *prog,
+ 				    struct bpf_iter_meta *meta,
+ 				    void *v)
+ {
+ 	struct bpf_iter__ipv6_route ctx;
+ 
+ 	ctx.meta = meta;
+ 	ctx.rt = v;
+ 	return bpf_iter_run_prog(prog, &ctx);
+ }
+ 
+ static int ipv6_route_seq_show(struct seq_file *seq, void *v)
+ {
+ 	struct ipv6_route_iter *iter = seq->private;
+ 	struct bpf_iter_meta meta;
+ 	struct bpf_prog *prog;
+ 	int ret;
+ 
+ 	meta.seq = seq;
+ 	prog = bpf_iter_get_info(&meta, false);
+ 	if (!prog)
+ 		return ipv6_route_native_seq_show(seq, v);
+ 
+ 	ret = ipv6_route_prog_seq_show(prog, &meta, v);
+ 	iter->w.leaf = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static void ipv6_route_seq_stop(struct seq_file *seq, void *v)
+ {
+ 	struct bpf_iter_meta meta;
+ 	struct bpf_prog *prog;
+ 
+ 	if (!v) {
+ 		meta.seq = seq;
+ 		prog = bpf_iter_get_info(&meta, true);
+ 		if (prog)
+ 			(void)ipv6_route_prog_seq_show(prog, &meta, v);
+ 	}
+ 
+ 	ipv6_route_native_seq_stop(seq, v);
+ }
+ #else
+ static int ipv6_route_seq_show(struct seq_file *seq, void *v)
+ {
+ 	return ipv6_route_native_seq_show(seq, v);
+ }
+ 
+ static void ipv6_route_seq_stop(struct seq_file *seq, void *v)
+ {
+ 	ipv6_route_native_seq_stop(seq, v);
+ }
+ #endif
+ 
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  const struct seq_operations ipv6_route_seq_ops = {
  	.start	= ipv6_route_seq_start,
  	.next	= ipv6_route_seq_next,
diff --cc net/ipv6/route.c
index 089b773d7497,22e56465f14d..000000000000
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@@ -5694,6 -6393,35 +5694,38 @@@ void __init ip6_route_init_special_entr
    #endif
  }
  
++<<<<<<< HEAD
++=======
+ #if IS_BUILTIN(CONFIG_IPV6)
+ #if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)
+ DEFINE_BPF_ITER_FUNC(ipv6_route, struct bpf_iter_meta *meta, struct fib6_info *rt)
+ 
+ static const struct bpf_iter_reg ipv6_route_reg_info = {
+ 	.target			= "ipv6_route",
+ 	.seq_ops		= &ipv6_route_seq_ops,
+ 	.init_seq_private	= bpf_iter_init_seq_net,
+ 	.fini_seq_private	= bpf_iter_fini_seq_net,
+ 	.seq_priv_size		= sizeof(struct ipv6_route_iter),
+ 	.ctx_arg_info_size	= 1,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__ipv6_route, rt),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ };
+ 
+ static int __init bpf_iter_register(void)
+ {
+ 	return bpf_iter_reg_target(&ipv6_route_reg_info);
+ }
+ 
+ static void bpf_iter_unregister(void)
+ {
+ 	bpf_iter_unreg_target(&ipv6_route_reg_info);
+ }
+ #endif
+ #endif
+ 
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  int __init ip6_route_init(void)
  {
  	int ret;
diff --cc net/netlink/af_netlink.c
index 9b0e55efc3d6,4f2c3b14ddbf..000000000000
--- a/net/netlink/af_netlink.c
+++ b/net/netlink/af_netlink.c
@@@ -2745,6 -2802,26 +2745,29 @@@ static const struct rhashtable_params n
  	.automatic_shrinking = true,
  };
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_BPF_SYSCALL) && defined(CONFIG_PROC_FS)
+ static const struct bpf_iter_reg netlink_reg_info = {
+ 	.target			= "netlink",
+ 	.seq_ops		= &netlink_seq_ops,
+ 	.init_seq_private	= bpf_iter_init_seq_net,
+ 	.fini_seq_private	= bpf_iter_fini_seq_net,
+ 	.seq_priv_size		= sizeof(struct nl_seq_iter),
+ 	.ctx_arg_info_size	= 1,
+ 	.ctx_arg_info		= {
+ 		{ offsetof(struct bpf_iter__netlink, sk),
+ 		  PTR_TO_BTF_ID_OR_NULL },
+ 	},
+ };
+ 
+ static int __init bpf_iter_register(void)
+ {
+ 	return bpf_iter_reg_target(&netlink_reg_info);
+ }
+ #endif
+ 
++>>>>>>> 3c32cc1bceba (bpf: Enable bpf_iter targets registering ctx argument types)
  static int __init netlink_proto_init(void)
  {
  	int i;
* Unmerged path include/linux/bpf.h
diff --git a/include/net/ip6_fib.h b/include/net/ip6_fib.h
index 2a9c7aee2cbc..805c37d00fd6 100644
--- a/include/net/ip6_fib.h
+++ b/include/net/ip6_fib.h
@@ -495,6 +495,13 @@ static inline bool fib6_metric_locked(struct fib6_info *f6i, int metric)
 	return !!(f6i->fib6_metrics->metrics[RTAX_LOCK - 1] & (1 << metric));
 }
 
+#if IS_BUILTIN(CONFIG_IPV6) && defined(CONFIG_BPF_SYSCALL)
+struct bpf_iter__ipv6_route {
+	__bpf_md_ptr(struct bpf_iter_meta *, meta);
+	__bpf_md_ptr(struct fib6_info *, rt);
+};
+#endif
+
 #ifdef CONFIG_IPV6_MULTIPLE_TABLES
 int fib6_rules_init(void);
 void fib6_rules_cleanup(void);
* Unmerged path kernel/bpf/bpf_iter.c
* Unmerged path kernel/bpf/btf.c
* Unmerged path kernel/bpf/map_iter.c
* Unmerged path kernel/bpf/task_iter.c
* Unmerged path kernel/bpf/verifier.c
* Unmerged path net/ipv6/ip6_fib.c
* Unmerged path net/ipv6/route.c
* Unmerged path net/netlink/af_netlink.c
