RDMA/mlx5: Assign dev to DM MR

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Maor Gottlieb <maorg@nvidia.com>
commit ca991a7d14d4835b302bcd182fdbf54470f45520
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/ca991a7d.failed

Currently, DM MR registration flow doesn't set the mlx5_ib_dev pointer and
can cause a NULL pointer dereference if userspace dumps the MR via rdma
tool.

Assign the IB device together with the other fields and remove the
redundant reference of mlx5_ib_dev from mlx5_ib_mr.

	Cc: stable@vger.kernel.org
Fixes: 6c29f57ea475 ("IB/mlx5: Device memory mr registration support")
Link: https://lore.kernel.org/r/20201203190807.127189-1-leon@kernel.org
	Signed-off-by: Maor Gottlieb <maorg@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit ca991a7d14d4835b302bcd182fdbf54470f45520)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index be99598f2e37,c33d6fd64fb6..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -606,10 -668,7 +606,13 @@@ struct mlx5_ib_mr 
  	struct ib_umem	       *umem;
  	struct mlx5_shared_mr_info	*smr_info;
  	struct list_head	list;
 +	unsigned int		order;
  	struct mlx5_cache_ent  *cache_ent;
++<<<<<<< HEAD
 +	int			npages;
 +	struct mlx5_ib_dev     *dev;
++=======
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
  	struct mlx5_core_sig_ctx    *sig;
  	void			*descs_alloc;
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 0ba952308926,6fa869c30e3f..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -171,9 -175,7 +171,8 @@@ static struct mlx5_ib_mr *alloc_cache_m
  	mr = kzalloc(sizeof(*mr), GFP_KERNEL);
  	if (!mr)
  		return NULL;
 +	mr->order = ent->order;
  	mr->cache_ent = ent;
- 	mr->dev = ent->dev;
  
  	set_mkc_access_pd_addr_fields(mkc, 0, 0, ent->dev->umrc.pd);
  	MLX5_SET(mkc, mkc, free, 1);
@@@ -968,17 -924,42 +967,33 @@@ static struct mlx5_cache_ent *mr_cache_
  	return &cache->ent[order];
  }
  
++<<<<<<< HEAD
 +static struct mlx5_ib_mr *
 +alloc_mr_from_cache(struct ib_pd *pd, struct ib_umem *umem, u64 virt_addr,
 +		    u64 len, int npages, int page_shift, unsigned int order,
 +		    int access_flags)
++=======
+ static void set_mr_fields(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
+ 			  u64 length, int access_flags)
+ {
+ 	mr->ibmr.lkey = mr->mmkey.key;
+ 	mr->ibmr.rkey = mr->mmkey.key;
+ 	mr->ibmr.length = length;
+ 	mr->ibmr.device = &dev->ib_dev;
+ 	mr->access_flags = access_flags;
+ }
+ 
+ static struct mlx5_ib_mr *alloc_cacheable_mr(struct ib_pd *pd,
+ 					     struct ib_umem *umem, u64 iova,
+ 					     int access_flags)
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 -	struct mlx5_cache_ent *ent;
 +	struct mlx5_cache_ent *ent = mr_cache_ent_from_order(dev, order);
  	struct mlx5_ib_mr *mr;
 -	unsigned int page_size;
 -
 -	page_size = mlx5_umem_find_best_pgsz(umem, mkc, log_page_size, 0, iova);
 -	if (WARN_ON(!page_size))
 -		return ERR_PTR(-EINVAL);
 -	ent = mr_cache_ent_from_order(
 -		dev, order_base_2(ib_umem_num_dma_blocks(umem, page_size)));
 -	/*
 -	 * Matches access in alloc_cache_mr(). If the MR can't come from the
 -	 * cache then synchronously create an uncached one.
 -	 */
 -	if (!ent || ent->limit == 0 ||
 -	    !mlx5_ib_can_reconfig_with_umr(dev, 0, access_flags)) {
 -		mutex_lock(&dev->slow_path_mutex);
 -		mr = reg_create(pd, umem, iova, access_flags, page_size, false);
 -		mutex_unlock(&dev->slow_path_mutex);
 -		return mr;
 -	}
  
 +	if (!ent)
 +		return ERR_PTR(-E2BIG);
  	mr = get_cache_mr(ent);
  	if (!mr) {
  		mr = create_cache_mr(ent);
@@@ -1001,14 -989,144 +1016,154 @@@
  			    MLX5_UMR_MTT_ALIGNMENT)
  #define MLX5_SPARE_UMR_CHUNK 0x10000
  
++<<<<<<< HEAD
 +int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
 +		       int page_shift, int flags)
 +{
 +	struct mlx5_ib_dev *dev = mr->dev;
 +	struct device *ddev = dev->ib_dev.dev.parent;
 +	int size;
++=======
+ /*
+  * Allocate a temporary buffer to hold the per-page information to transfer to
+  * HW. For efficiency this should be as large as it can be, but buffer
+  * allocation failure is not allowed, so try smaller sizes.
+  */
+ static void *mlx5_ib_alloc_xlt(size_t *nents, size_t ent_size, gfp_t gfp_mask)
+ {
+ 	const size_t xlt_chunk_align =
+ 		MLX5_UMR_MTT_ALIGNMENT / sizeof(ent_size);
+ 	size_t size;
+ 	void *res = NULL;
+ 
+ 	static_assert(PAGE_SIZE % MLX5_UMR_MTT_ALIGNMENT == 0);
+ 
+ 	/*
+ 	 * MLX5_IB_UPD_XLT_ATOMIC doesn't signal an atomic context just that the
+ 	 * allocation can't trigger any kind of reclaim.
+ 	 */
+ 	might_sleep();
+ 
+ 	gfp_mask |= __GFP_ZERO;
+ 
+ 	/*
+ 	 * If the system already has a suitable high order page then just use
+ 	 * that, but don't try hard to create one. This max is about 1M, so a
+ 	 * free x86 huge page will satisfy it.
+ 	 */
+ 	size = min_t(size_t, ent_size * ALIGN(*nents, xlt_chunk_align),
+ 		     MLX5_MAX_UMR_CHUNK);
+ 	*nents = size / ent_size;
+ 	res = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,
+ 				       get_order(size));
+ 	if (res)
+ 		return res;
+ 
+ 	if (size > MLX5_SPARE_UMR_CHUNK) {
+ 		size = MLX5_SPARE_UMR_CHUNK;
+ 		*nents = get_order(size) / ent_size;
+ 		res = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,
+ 					       get_order(size));
+ 		if (res)
+ 			return res;
+ 	}
+ 
+ 	*nents = PAGE_SIZE / ent_size;
+ 	res = (void *)__get_free_page(gfp_mask);
+ 	if (res)
+ 		return res;
+ 
+ 	mutex_lock(&xlt_emergency_page_mutex);
+ 	memset(xlt_emergency_page, 0, PAGE_SIZE);
+ 	return xlt_emergency_page;
+ }
+ 
+ static void mlx5_ib_free_xlt(void *xlt, size_t length)
+ {
+ 	if (xlt == xlt_emergency_page) {
+ 		mutex_unlock(&xlt_emergency_page_mutex);
+ 		return;
+ 	}
+ 
+ 	free_pages((unsigned long)xlt, get_order(length));
+ }
+ 
+ /*
+  * Create a MLX5_IB_SEND_UMR_UPDATE_XLT work request and XLT buffer ready for
+  * submission.
+  */
+ static void *mlx5_ib_create_xlt_wr(struct mlx5_ib_mr *mr,
+ 				   struct mlx5_umr_wr *wr, struct ib_sge *sg,
+ 				   size_t nents, size_t ent_size,
+ 				   unsigned int flags)
+ {
+ 	struct mlx5_ib_dev *dev = mr_to_mdev(mr);
+ 	struct device *ddev = &dev->mdev->pdev->dev;
+ 	dma_addr_t dma;
+ 	void *xlt;
+ 
+ 	xlt = mlx5_ib_alloc_xlt(&nents, ent_size,
+ 				flags & MLX5_IB_UPD_XLT_ATOMIC ? GFP_ATOMIC :
+ 								 GFP_KERNEL);
+ 	sg->length = nents * ent_size;
+ 	dma = dma_map_single(ddev, xlt, sg->length, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(ddev, dma)) {
+ 		mlx5_ib_err(dev, "unable to map DMA during XLT update.\n");
+ 		mlx5_ib_free_xlt(xlt, sg->length);
+ 		return NULL;
+ 	}
+ 	sg->addr = dma;
+ 	sg->lkey = dev->umrc.pd->local_dma_lkey;
+ 
+ 	memset(wr, 0, sizeof(*wr));
+ 	wr->wr.send_flags = MLX5_IB_SEND_UMR_UPDATE_XLT;
+ 	if (!(flags & MLX5_IB_UPD_XLT_ENABLE))
+ 		wr->wr.send_flags |= MLX5_IB_SEND_UMR_FAIL_IF_FREE;
+ 	wr->wr.sg_list = sg;
+ 	wr->wr.num_sge = 1;
+ 	wr->wr.opcode = MLX5_IB_WR_UMR;
+ 	wr->pd = mr->ibmr.pd;
+ 	wr->mkey = mr->mmkey.key;
+ 	wr->length = mr->mmkey.size;
+ 	wr->virt_addr = mr->mmkey.iova;
+ 	wr->access_flags = mr->access_flags;
+ 	wr->page_shift = mr->page_shift;
+ 	wr->xlt_size = sg->length;
+ 	return xlt;
+ }
+ 
+ static void mlx5_ib_unmap_free_xlt(struct mlx5_ib_dev *dev, void *xlt,
+ 				   struct ib_sge *sg)
+ {
+ 	struct device *ddev = &dev->mdev->pdev->dev;
+ 
+ 	dma_unmap_single(ddev, sg->addr, sg->length, DMA_TO_DEVICE);
+ 	mlx5_ib_free_xlt(xlt, sg->length);
+ }
+ 
+ static unsigned int xlt_wr_final_send_flags(unsigned int flags)
+ {
+ 	unsigned int res = 0;
+ 
+ 	if (flags & MLX5_IB_UPD_XLT_ENABLE)
+ 		res |= MLX5_IB_SEND_UMR_ENABLE_MR |
+ 		       MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS |
+ 		       MLX5_IB_SEND_UMR_UPDATE_TRANSLATION;
+ 	if (flags & MLX5_IB_UPD_XLT_PD || flags & MLX5_IB_UPD_XLT_ACCESS)
+ 		res |= MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
+ 	if (flags & MLX5_IB_UPD_XLT_ADDR)
+ 		res |= MLX5_IB_SEND_UMR_UPDATE_TRANSLATION;
+ 	return res;
+ }
+ 
+ int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
+ 		       int page_shift, int flags)
+ {
+ 	struct mlx5_ib_dev *dev = mr_to_mdev(mr);
+ 	struct device *ddev = &dev->mdev->pdev->dev;
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  	void *xlt;
 +	dma_addr_t dma;
  	struct mlx5_umr_wr wr;
  	struct ib_sge sg;
  	int err = 0;
@@@ -1135,14 -1193,71 +1290,32 @@@
  
  		err = mlx5_ib_post_send_wait(dev, &wr);
  	}
 -	sg.length = orig_sg_length;
 -	mlx5_ib_unmap_free_xlt(dev, xlt, &sg);
 -	return err;
 -}
 +	dma_unmap_single(ddev, dma, size, DMA_TO_DEVICE);
  
++<<<<<<< HEAD
 +free_xlt:
 +	if (use_emergency_page)
 +		mlx5_ib_put_xlt_emergency_page();
 +	else
 +		free_pages((unsigned long)xlt, get_order(size));
++=======
+ /*
+  * Send the DMA list to the HW for a normal MR using UMR.
+  */
+ static int mlx5_ib_update_mr_pas(struct mlx5_ib_mr *mr, unsigned int flags)
+ {
+ 	struct mlx5_ib_dev *dev = mr_to_mdev(mr);
+ 	struct device *ddev = &dev->mdev->pdev->dev;
+ 	struct ib_block_iter biter;
+ 	struct mlx5_mtt *cur_mtt;
+ 	struct mlx5_umr_wr wr;
+ 	size_t orig_sg_length;
+ 	struct mlx5_mtt *mtt;
+ 	size_t final_size;
+ 	struct ib_sge sg;
+ 	int err = 0;
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  
 -	if (WARN_ON(mr->umem->is_odp))
 -		return -EINVAL;
 -
 -	mtt = mlx5_ib_create_xlt_wr(mr, &wr, &sg,
 -				    ib_umem_num_dma_blocks(mr->umem,
 -							   1 << mr->page_shift),
 -				    sizeof(*mtt), flags);
 -	if (!mtt)
 -		return -ENOMEM;
 -	orig_sg_length = sg.length;
 -
 -	cur_mtt = mtt;
 -	rdma_for_each_block (mr->umem->sg_head.sgl, &biter, mr->umem->nmap,
 -			     BIT(mr->page_shift)) {
 -		if (cur_mtt == (void *)mtt + sg.length) {
 -			dma_sync_single_for_device(ddev, sg.addr, sg.length,
 -						   DMA_TO_DEVICE);
 -			err = mlx5_ib_post_send_wait(dev, &wr);
 -			if (err)
 -				goto err;
 -			dma_sync_single_for_cpu(ddev, sg.addr, sg.length,
 -						DMA_TO_DEVICE);
 -			wr.offset += sg.length;
 -			cur_mtt = mtt;
 -		}
 -
 -		cur_mtt->ptag =
 -			cpu_to_be64(rdma_block_iter_dma_address(&biter) |
 -				    MLX5_IB_MTT_PRESENT);
 -		cur_mtt++;
 -	}
 -
 -	final_size = (void *)cur_mtt - (void *)mtt;
 -	sg.length = ALIGN(final_size, MLX5_UMR_MTT_ALIGNMENT);
 -	memset(cur_mtt, 0, sg.length - final_size);
 -	wr.wr.send_flags |= xlt_wr_final_send_flags(flags);
 -	wr.xlt_size = sg.length;
 -
 -	dma_sync_single_for_device(ddev, sg.addr, sg.length, DMA_TO_DEVICE);
 -	err = mlx5_ib_post_send_wait(dev, &wr);
 -
 -err:
 -	sg.length = orig_sg_length;
 -	mlx5_ib_unmap_free_xlt(dev, mtt, &sg);
  	return err;
  }
  
@@@ -1213,7 -1335,8 +1386,12 @@@ static struct mlx5_ib_mr *reg_create(st
  	}
  	mr->mmkey.type = MLX5_MKEY_MR;
  	mr->desc_size = sizeof(struct mlx5_mtt);
++<<<<<<< HEAD
 +	mr->dev = dev;
++=======
+ 	mr->umem = umem;
+ 	set_mr_fields(dev, mr, umem->length, access_flags);
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  	kvfree(in);
  
  	mlx5_ib_dbg(dev, "mkey = 0x%x\n", mr->mmkey.key);
@@@ -1464,139 -1588,214 +1642,139 @@@ int mlx5_mr_cache_invalidate(struct mlx
  	umrwr.mkey = mr->mmkey.key;
  	umrwr.ignore_free_state = 1;
  
- 	return mlx5_ib_post_send_wait(mr->dev, &umrwr);
+ 	return mlx5_ib_post_send_wait(mr_to_mdev(mr), &umrwr);
  }
  
 -/*
 - * True if the change in access flags can be done via UMR, only some access
 - * flags can be updated.
 - */
 -static bool can_use_umr_rereg_access(struct mlx5_ib_dev *dev,
 -				     unsigned int current_access_flags,
 -				     unsigned int target_access_flags)
 -{
 -	unsigned int diffs = current_access_flags ^ target_access_flags;
 -
 -	if (diffs & ~(IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE |
 -		      IB_ACCESS_REMOTE_READ | IB_ACCESS_RELAXED_ORDERING))
 -		return false;
 -	return mlx5_ib_can_reconfig_with_umr(dev, current_access_flags,
 -					     target_access_flags);
 -}
 -
 -static int umr_rereg_pd_access(struct mlx5_ib_mr *mr, struct ib_pd *pd,
 -			       int access_flags)
 -{
 -	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 -	struct mlx5_umr_wr umrwr = {
 -		.wr = {
 -			.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE |
 -				      MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS,
 -			.opcode = MLX5_IB_WR_UMR,
 -		},
 -		.mkey = mr->mmkey.key,
 -		.pd = pd,
 -		.access_flags = access_flags,
 -	};
 +static int rereg_umr(struct ib_pd *pd, struct mlx5_ib_mr *mr,
 +		     int access_flags, int flags)
 +{
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct mlx5_umr_wr umrwr = {};
  	int err;
  
 -	err = mlx5_ib_post_send_wait(dev, &umrwr);
 -	if (err)
 -		return err;
 +	umrwr.wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE;
  
 -	mr->access_flags = access_flags;
 -	mr->mmkey.pd = to_mpd(pd)->pdn;
 -	return 0;
 -}
 +	umrwr.wr.opcode = MLX5_IB_WR_UMR;
 +	umrwr.mkey = mr->mmkey.key;
  
 -static bool can_use_umr_rereg_pas(struct mlx5_ib_mr *mr,
 -				  struct ib_umem *new_umem,
 -				  int new_access_flags, u64 iova,
 -				  unsigned long *page_size)
 -{
 -	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 +	if (flags & IB_MR_REREG_PD || flags & IB_MR_REREG_ACCESS) {
 +		umrwr.pd = pd;
 +		umrwr.access_flags = access_flags;
 +		umrwr.wr.send_flags |= MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
 +	}
  
 -	/* We only track the allocated sizes of MRs from the cache */
 -	if (!mr->cache_ent)
 -		return false;
 -	if (!mlx5_ib_can_load_pas_with_umr(dev, new_umem->length))
 -		return false;
 +	err = mlx5_ib_post_send_wait(dev, &umrwr);
  
 -	*page_size =
 -		mlx5_umem_find_best_pgsz(new_umem, mkc, log_page_size, 0, iova);
 -	if (WARN_ON(!*page_size))
 -		return false;
 -	return (1ULL << mr->cache_ent->order) >=
 -	       ib_umem_num_dma_blocks(new_umem, *page_size);
 +	return err;
  }
  
 -static int umr_rereg_pas(struct mlx5_ib_mr *mr, struct ib_pd *pd,
 -			 int access_flags, int flags, struct ib_umem *new_umem,
 -			 u64 iova, unsigned long page_size)
 +int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 +			  u64 length, u64 virt_addr, int new_access_flags,
 +			  struct ib_pd *new_pd, struct ib_udata *udata)
  {
 -	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 -	int upd_flags = MLX5_IB_UPD_XLT_ADDR | MLX5_IB_UPD_XLT_ENABLE;
 -	struct ib_umem *old_umem = mr->umem;
 +	struct mlx5_ib_dev *dev = to_mdev(ib_mr->device);
 +	struct mlx5_ib_mr *mr = to_mmr(ib_mr);
 +	struct ib_pd *pd = (flags & IB_MR_REREG_PD) ? new_pd : ib_mr->pd;
 +	int access_flags = flags & IB_MR_REREG_ACCESS ?
 +			    new_access_flags :
 +			    mr->access_flags;
 +	int page_shift = 0;
 +	int upd_flags = 0;
 +	int npages = 0;
 +	int ncont = 0;
 +	int order = 0;
 +	u64 addr, len;
  	int err;
  
 -	/*
 -	 * To keep everything simple the MR is revoked before we start to mess
 -	 * with it. This ensure the change is atomic relative to any use of the
 -	 * MR.
 -	 */
 -	err = mlx5_mr_cache_invalidate(mr);
 -	if (err)
 -		return err;
 +	mlx5_ib_dbg(dev, "start 0x%llx, virt_addr 0x%llx, length 0x%llx, access_flags 0x%x\n",
 +		    start, virt_addr, length, access_flags);
  
 -	if (flags & IB_MR_REREG_PD) {
 -		mr->ibmr.pd = pd;
 -		mr->mmkey.pd = to_mpd(pd)->pdn;
 -		upd_flags |= MLX5_IB_UPD_XLT_PD;
 -	}
 -	if (flags & IB_MR_REREG_ACCESS) {
 -		mr->access_flags = access_flags;
 -		upd_flags |= MLX5_IB_UPD_XLT_ACCESS;
 +	atomic_sub(mr->npages, &dev->mdev->priv.reg_pages);
 +
 +	if (!mr->umem)
 +		return -EINVAL;
 +
 +	if (is_odp_mr(mr))
 +		return -EOPNOTSUPP;
 +
 +	if (flags & IB_MR_REREG_TRANS) {
 +		addr = virt_addr;
 +		len = length;
 +	} else {
 +		addr = mr->umem->address;
 +		len = mr->umem->length;
  	}
  
 -	mr->ibmr.length = new_umem->length;
 -	mr->mmkey.iova = iova;
 -	mr->mmkey.size = new_umem->length;
 -	mr->page_shift = order_base_2(page_size);
 -	mr->umem = new_umem;
 -	err = mlx5_ib_update_mr_pas(mr, upd_flags);
 -	if (err) {
 +	if (flags != IB_MR_REREG_PD) {
  		/*
 -		 * The MR is revoked at this point so there is no issue to free
 -		 * new_umem.
 +		 * Replace umem. This needs to be done whether or not UMR is
 +		 * used.
  		 */
 -		mr->umem = old_umem;
 -		return err;
 +		flags |= IB_MR_REREG_TRANS;
 +		ib_umem_release(mr->umem);
 +		mr->umem = NULL;
 +		err = mr_umem_get(dev, udata, addr, len, access_flags,
 +				  &mr->umem, &npages, &page_shift, &ncont,
 +				  &order);
 +		if (err)
 +			goto err;
  	}
  
 -	atomic_sub(ib_umem_num_pages(old_umem), &dev->mdev->priv.reg_pages);
 -	ib_umem_release(old_umem);
 -	atomic_add(ib_umem_num_pages(new_umem), &dev->mdev->priv.reg_pages);
 -	return 0;
 -}
 -
 -struct ib_mr *mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 -				    u64 length, u64 iova, int new_access_flags,
 -				    struct ib_pd *new_pd,
 -				    struct ib_udata *udata)
 -{
 -	struct mlx5_ib_dev *dev = to_mdev(ib_mr->device);
 -	struct mlx5_ib_mr *mr = to_mmr(ib_mr);
 -	int err;
 -
 -	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))
 -		return ERR_PTR(-EOPNOTSUPP);
 -
 -	mlx5_ib_dbg(
 -		dev,
 -		"start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\n",
 -		start, iova, length, new_access_flags);
 +	if (!mlx5_ib_can_use_umr(dev, true, access_flags) ||
 +	    (flags & IB_MR_REREG_TRANS && !use_umr_mtt_update(mr, addr, len))) {
 +		/*
 +		 * UMR can't be used - MKey needs to be replaced.
 +		 */
 +		if (mr->cache_ent)
 +			detach_mr_from_cache(mr);
 +		err = destroy_mkey(dev, mr);
 +		if (err)
 +			goto err;
  
 -	if (flags & ~(IB_MR_REREG_TRANS | IB_MR_REREG_PD | IB_MR_REREG_ACCESS))
 -		return ERR_PTR(-EOPNOTSUPP);
 +		mr = reg_create(ib_mr, pd, addr, len, mr->umem, ncont,
 +				page_shift, access_flags, true);
  
 -	if (!(flags & IB_MR_REREG_ACCESS))
 -		new_access_flags = mr->access_flags;
 -	if (!(flags & IB_MR_REREG_PD))
 -		new_pd = ib_mr->pd;
 -
 -	if (!(flags & IB_MR_REREG_TRANS)) {
 -		struct ib_umem *umem;
 -
 -		/* Fast path for PD/access change */
 -		if (can_use_umr_rereg_access(dev, mr->access_flags,
 -					     new_access_flags)) {
 -			err = umr_rereg_pd_access(mr, new_pd, new_access_flags);
 -			if (err)
 -				return ERR_PTR(err);
 -			return NULL;
 +		if (IS_ERR(mr)) {
 +			err = PTR_ERR(mr);
 +			mr = to_mmr(ib_mr);
 +			goto err;
  		}
 -		/* DM or ODP MR's don't have a umem so we can't re-use it */
 -		if (!mr->umem || is_odp_mr(mr))
 -			goto recreate;
 -
 +	} else {
  		/*
 -		 * Only one active MR can refer to a umem at one time, revoke
 -		 * the old MR before assigning the umem to the new one.
 +		 * Send a UMR WQE
  		 */
 -		err = mlx5_mr_cache_invalidate(mr);
 -		if (err)
 -			return ERR_PTR(err);
 -		umem = mr->umem;
 -		mr->umem = NULL;
 -		atomic_sub(ib_umem_num_pages(umem), &dev->mdev->priv.reg_pages);
 -
 -		return create_real_mr(new_pd, umem, mr->mmkey.iova,
 -				      new_access_flags);
 -	}
 +		mr->ibmr.pd = pd;
 +		mr->access_flags = access_flags;
 +		mr->mmkey.iova = addr;
 +		mr->mmkey.size = len;
 +		mr->mmkey.pd = to_mpd(pd)->pdn;
  
 -	/*
 -	 * DM doesn't have a PAS list so we can't re-use it, odp does but the
 -	 * logic around releasing the umem is different
 -	 */
 -	if (!mr->umem || is_odp_mr(mr))
 -		goto recreate;
 -
 -	if (!(new_access_flags & IB_ACCESS_ON_DEMAND) &&
 -	    can_use_umr_rereg_access(dev, mr->access_flags, new_access_flags)) {
 -		struct ib_umem *new_umem;
 -		unsigned long page_size;
 -
 -		new_umem = ib_umem_get(&dev->ib_dev, start, length,
 -				       new_access_flags);
 -		if (IS_ERR(new_umem))
 -			return ERR_CAST(new_umem);
 -
 -		/* Fast path for PAS change */
 -		if (can_use_umr_rereg_pas(mr, new_umem, new_access_flags, iova,
 -					  &page_size)) {
 -			err = umr_rereg_pas(mr, new_pd, new_access_flags, flags,
 -					    new_umem, iova, page_size);
 -			if (err) {
 -				ib_umem_release(new_umem);
 -				return ERR_PTR(err);
 -			}
 -			return NULL;
 +		if (flags & IB_MR_REREG_TRANS) {
 +			upd_flags = MLX5_IB_UPD_XLT_ADDR;
 +			if (flags & IB_MR_REREG_PD)
 +				upd_flags |= MLX5_IB_UPD_XLT_PD;
 +			if (flags & IB_MR_REREG_ACCESS)
 +				upd_flags |= MLX5_IB_UPD_XLT_ACCESS;
 +			err = mlx5_ib_update_xlt(mr, 0, npages, page_shift,
 +						 upd_flags);
 +		} else {
 +			err = rereg_umr(pd, mr, access_flags, flags);
  		}
 -		return create_real_mr(new_pd, new_umem, iova, new_access_flags);
 +
 +		if (err)
 +			goto err;
  	}
  
 -	/*
 -	 * Everything else has no state we can preserve, just create a new MR
 -	 * from scratch
 -	 */
 -recreate:
 -	return mlx5_ib_reg_user_mr(new_pd, start, length, iova,
 -				   new_access_flags, udata);
 +	set_mr_fields(dev, mr, npages, len, access_flags);
 +
 +	return 0;
 +
 +err:
 +	ib_umem_release(mr->umem);
 +	mr->umem = NULL;
 +
 +	clean_mr(dev, mr);
 +	return err;
  }
  
  static int
diff --cc drivers/infiniband/hw/mlx5/odp.c
index aa43ffecd61f,aa2413b50adc..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -463,12 -476,13 +463,21 @@@ static struct mlx5_ib_mr *implicit_get_
  	if (IS_ERR(odp))
  		return ERR_CAST(odp);
  
++<<<<<<< HEAD
 +	ret = mr = mlx5_mr_cache_alloc(imr->dev, MLX5_IMR_MTT_CACHE_ENTRY);
++=======
+ 	ret = mr = mlx5_mr_cache_alloc(
+ 		mr_to_mdev(imr), MLX5_IMR_MTT_CACHE_ENTRY, imr->access_flags);
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  	if (IS_ERR(mr))
  		goto out_umem;
  
  	mr->ibmr.pd = imr->ibmr.pd;
++<<<<<<< HEAD
 +	mr->access_flags = imr->access_flags;
++=======
+ 	mr->ibmr.device = &mr_to_mdev(imr)->ib_dev;
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  	mr->umem = &odp->umem;
  	mr->ibmr.lkey = mr->mmkey.key;
  	mr->ibmr.rkey = mr->mmkey.key;
@@@ -539,7 -556,9 +548,8 @@@ struct mlx5_ib_mr *mlx5_ib_alloc_implic
  	imr->umem = &umem_odp->umem;
  	imr->ibmr.lkey = imr->mmkey.key;
  	imr->ibmr.rkey = imr->mmkey.key;
+ 	imr->ibmr.device = &dev->ib_dev;
  	imr->umem = &umem_odp->umem;
 -	imr->is_odp_implicit = true;
  	atomic_set(&imr->num_deferred_work, 0);
  	init_waitqueue_head(&imr->q_deferred_work);
  	xa_init(&imr->implicit_children);
@@@ -821,6 -817,7 +831,10 @@@ static int pagefault_mr(struct mlx5_ib_
  {
  	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
  
++<<<<<<< HEAD
++=======
+ 	lockdep_assert_held(&mr_to_mdev(mr)->odp_srcu);
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  	if (unlikely(io_virt < mr->mmkey.iova))
  		return -EFAULT;
  
@@@ -1776,6 -1782,12 +1790,14 @@@ static void mlx5_ib_prefetch_mr_work(st
  	int ret;
  	u32 i;
  
++<<<<<<< HEAD
++=======
+ 	/* We rely on IB/core that work is executed if we have num_sge != 0 only. */
+ 	WARN_ON(!work->num_sge);
+ 	dev = mr_to_mdev(work->frags[0].mr);
+ 	/* SRCU should be held when calling to mlx5_odp_populate_xlt() */
+ 	srcu_key = srcu_read_lock(&dev->odp_srcu);
++>>>>>>> ca991a7d14d4 (RDMA/mlx5: Assign dev to DM MR)
  	for (i = 0; i < work->num_sge; ++i) {
  		ret = pagefault_mr(work->frags[i].mr, work->frags[i].io_virt,
  				   work->frags[i].length, &bytes_mapped,
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
diff --git a/drivers/infiniband/hw/mlx5/restrack.c b/drivers/infiniband/hw/mlx5/restrack.c
index 32c6d0397946..59300931faee 100644
--- a/drivers/infiniband/hw/mlx5/restrack.c
+++ b/drivers/infiniband/hw/mlx5/restrack.c
@@ -116,7 +116,7 @@ int mlx5_ib_fill_res_mr_entry_raw(struct sk_buff *msg, struct ib_mr *ibmr)
 {
 	struct mlx5_ib_mr *mr = to_mmr(ibmr);
 
-	return fill_res_raw(msg, mr->dev, MLX5_SGMT_TYPE_PRM_QUERY_MKEY,
+	return fill_res_raw(msg, mr_to_mdev(mr), MLX5_SGMT_TYPE_PRM_QUERY_MKEY,
 			    mlx5_mkey_to_idx(mr->mmkey.key));
 }
 
