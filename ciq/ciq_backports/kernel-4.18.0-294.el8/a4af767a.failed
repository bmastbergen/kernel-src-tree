x86/cpu_entry_area: Prepare for IST guard pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit a4af767ae59cc579569bbfe49513a0037d5989ee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/a4af767a.failed

To allow guard pages between the IST stacks each stack needs to be
mapped individually.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Sean Christopherson <sean.j.christopherson@intel.com>
	Cc: x86-ml <x86@kernel.org>
Link: https://lkml.kernel.org/r/20190414160144.592691557@linutronix.de
(cherry picked from commit a4af767ae59cc579569bbfe49513a0037d5989ee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/cpu_entry_area.c
diff --cc arch/x86/mm/cpu_entry_area.c
index 26fa2a5a8715,2b1407662a6d..000000000000
--- a/arch/x86/mm/cpu_entry_area.c
+++ b/arch/x86/mm/cpu_entry_area.c
@@@ -78,9 -77,38 +78,37 @@@ static void __init percpu_setup_debug_s
  #endif
  }
  
+ #ifdef CONFIG_X86_64
+ 
+ #define cea_map_stack(name) do {					\
+ 	npages = sizeof(estacks->name## _stack) / PAGE_SIZE;		\
+ 	cea_map_percpu_pages(cea->estacks.name## _stack,		\
+ 			estacks->name## _stack, npages, PAGE_KERNEL);	\
+ 	} while (0)
+ 
+ static void __init percpu_setup_exception_stacks(unsigned int cpu)
+ {
+ 	struct exception_stacks *estacks = per_cpu_ptr(&exception_stacks, cpu);
+ 	struct cpu_entry_area *cea = get_cpu_entry_area(cpu);
+ 	unsigned int npages;
+ 
+ 	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);
+ 	/*
+ 	 * The exceptions stack mappings in the per cpu area are protected
+ 	 * by guard pages so each stack must be mapped separately.
+ 	 */
+ 	cea_map_stack(DF);
+ 	cea_map_stack(NMI);
+ 	cea_map_stack(DB);
+ 	cea_map_stack(MCE);
+ }
+ #else
+ static inline void percpu_setup_exception_stacks(unsigned int cpu) {}
+ #endif
+ 
  /* Setup the fixmap mappings only once per-processor */
 -static void __init setup_cpu_entry_area(unsigned int cpu)
 +static void __init setup_cpu_entry_area(int cpu)
  {
 -	struct cpu_entry_area *cea = get_cpu_entry_area(cpu);
  #ifdef CONFIG_X86_64
  	/* On 64-bit systems, we use a read-only fixmap GDT and TSS. */
  	pgprot_t gdt_prot = PAGE_KERNEL_RO;
@@@ -141,17 -159,11 +169,22 @@@
  			     sizeof(struct tss_struct) / PAGE_SIZE, tss_prot);
  
  #ifdef CONFIG_X86_32
 -	per_cpu(cpu_entry_area, cpu) = cea;
 +	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);
  #endif
  
++<<<<<<< HEAD
 +#ifdef CONFIG_X86_64
 +	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);
 +	BUILD_BUG_ON(sizeof(exception_stacks) !=
 +		     sizeof(((struct cpu_entry_area *)0)->exception_stacks));
 +	cea_map_percpu_pages(&get_cpu_entry_area(cpu)->exception_stacks,
 +			     &per_cpu(exception_stacks, cpu),
 +			     sizeof(exception_stacks) / PAGE_SIZE, PAGE_KERNEL);
 +#endif
++=======
+ 	percpu_setup_exception_stacks(cpu);
+ 
++>>>>>>> a4af767ae59c (x86/cpu_entry_area: Prepare for IST guard pages)
  	percpu_setup_debug_store(cpu);
  }
  
* Unmerged path arch/x86/mm/cpu_entry_area.c
