x86/vdso: Rearrange do_hres() to improve code generation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Andy Lutomirski <luto@kernel.org>
commit 99c19e6a8fe4a95fa0dac191207a1d40461b1604
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/99c19e6a.failed

vgetcyc() is full of barriers, so fetching values out of the vvar
page before vgetcyc() for use after vgetcyc() results in poor code
generation.  Put vgetcyc() first to avoid this problem.

Also, pull the tv_sec division into the loop and put all the ts
writes together.  The old code wrote ts->tv_sec on each iteration
before the syscall fallback check and then added in the offset
afterwards, which forced the compiler to pointlessly copy base->sec
to ts->tv_sec on each iteration.  The new version seems to generate
sensible code.

Saves several cycles.  With this patch applied, the result is faster
than before the clock_gettime() rewrite.

	Signed-off-by: Andy Lutomirski <luto@kernel.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/3c05644d010b72216aa286a6d20b5078d5fae5cd.1538762487.git.luto@kernel.org

(cherry picked from commit 99c19e6a8fe4a95fa0dac191207a1d40461b1604)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/vdso/vclock_gettime.c
diff --cc arch/x86/entry/vdso/vclock_gettime.c
index 7eb640252452,007b3fe9d727..000000000000
--- a/arch/x86/entry/vdso/vclock_gettime.c
+++ b/arch/x86/entry/vdso/vclock_gettime.c
@@@ -208,23 -142,30 +208,42 @@@ notrace static inline u64 vgetsns(int *
  notrace static int do_hres(clockid_t clk, struct timespec *ts)
  {
  	struct vgtod_ts *base = &gtod->basetime[clk];
++<<<<<<< HEAD
++=======
+ 	u64 cycles, last, sec, ns;
++>>>>>>> 99c19e6a8fe4 (x86/vdso: Rearrange do_hres() to improve code generation)
  	unsigned int seq;
 +	int mode;
 +	u64 ns;
  
  	do {
  		seq = gtod_read_begin(gtod);
++<<<<<<< HEAD
 +		mode = gtod->vclock_mode;
 +		ts->tv_sec = base->sec;
 +		ns = base->nsec;
 +		ns += vgetsns(&mode);
++=======
+ 		cycles = vgetcyc(gtod->vclock_mode);
+ 		ns = base->nsec;
+ 		last = gtod->cycle_last;
+ 		if (unlikely((s64)cycles < 0))
+ 			return vdso_fallback_gettime(clk, ts);
+ 		if (cycles > last)
+ 			ns += (cycles - last) * gtod->mult;
++>>>>>>> 99c19e6a8fe4 (x86/vdso: Rearrange do_hres() to improve code generation)
  		ns >>= gtod->shift;
+ 		sec = base->sec;
  	} while (unlikely(gtod_read_retry(gtod, seq)));
  
- 	ts->tv_sec += __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);
+ 	/*
+ 	 * Do this outside the loop: a race inside the loop could result
+ 	 * in __iter_div_u64_rem() being extremely slow.
+ 	 */
+ 	ts->tv_sec = sec + __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);
  	ts->tv_nsec = ns;
  
 -	return 0;
 +	return mode;
  }
  
  notrace static void do_coarse(clockid_t clk, struct timespec *ts)
* Unmerged path arch/x86/entry/vdso/vclock_gettime.c
