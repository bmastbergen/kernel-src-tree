mm, tree-wide: rename put_user_page*() to unpin_user_page*()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author John Hubbard <jhubbard@nvidia.com>
commit f1f6a7dd9b53aafd81b696b9017036e7b08e57ea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/f1f6a7dd.failed

In order to provide a clearer, more symmetric API for pinning and
unpinning DMA pages.  This way, pin_user_pages*() calls match up with
unpin_user_pages*() calls, and the API is a lot closer to being
self-explanatory.

Link: http://lkml.kernel.org/r/20200107224558.2362728-23-jhubbard@nvidia.com
	Signed-off-by: John Hubbard <jhubbard@nvidia.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Cc: Alex Williamson <alex.williamson@redhat.com>
	Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
	Cc: Björn Töpel <bjorn.topel@intel.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Hans Verkuil <hverkuil-cisco@xs4all.nl>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Jason Gunthorpe <jgg@mellanox.com>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: Jens Axboe <axboe@kernel.dk>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Kirill A. Shutemov <kirill@shutemov.name>
	Cc: Leon Romanovsky <leonro@mellanox.com>
	Cc: Mauro Carvalho Chehab <mchehab@kernel.org>
	Cc: Mike Rapoport <rppt@linux.ibm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f1f6a7dd9b53aafd81b696b9017036e7b08e57ea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/core-api/pin_user_pages.rst
#	drivers/platform/goldfish/goldfish_pipe.c
#	drivers/vfio/vfio_iommu_type1.c
#	include/linux/mm.h
diff --cc drivers/platform/goldfish/goldfish_pipe.c
index 0b1ae0eaa477,1ab207ec9c94..000000000000
--- a/drivers/platform/goldfish/goldfish_pipe.c
+++ b/drivers/platform/goldfish/goldfish_pipe.c
@@@ -409,10 -360,10 +409,15 @@@ static int transfer_max_buffers(struct 
  
  	*consumed_size = pipe->command_buffer->rw_params.consumed_size;
  
++<<<<<<< HEAD
 +	release_user_pages(pages, pages_count, is_write, *consumed_size);
++=======
+ 	unpin_user_pages_dirty_lock(pipe->pages, pages_count,
+ 				    !is_write && *consumed_size > 0);
++>>>>>>> f1f6a7dd9b53 (mm, tree-wide: rename put_user_page*() to unpin_user_page*())
  
  	mutex_unlock(&pipe->lock);
 +
  	return 0;
  }
  
diff --cc drivers/vfio/vfio_iommu_type1.c
index f1a7cfd55b17,a177bf2c6683..000000000000
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@@ -312,9 -309,8 +312,14 @@@ static int put_pfn(unsigned long pfn, i
  {
  	if (!is_invalid_reserved_pfn(pfn)) {
  		struct page *page = pfn_to_page(pfn);
++<<<<<<< HEAD
 +		if (prot & IOMMU_WRITE)
 +			SetPageDirty(page);
 +		put_page(page);
++=======
+ 
+ 		unpin_user_pages_dirty_lock(&page, 1, prot & IOMMU_WRITE);
++>>>>>>> f1f6a7dd9b53 (mm, tree-wide: rename put_user_page*() to unpin_user_page*())
  		return 1;
  	}
  	return 0;
diff --cc include/linux/mm.h
index ef77bd76b21c,fc543eb45de1..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -969,21 -1039,19 +969,32 @@@ static inline void put_page(struct pag
  }
  
  /**
-  * put_user_page() - release a gup-pinned page
+  * unpin_user_page() - release a gup-pinned page
   * @page:            pointer to page to be released
   *
++<<<<<<< HEAD
 + * Pages that were pinned via get_user_pages*() must be released via
 + * either put_user_page(), or one of the put_user_pages*() routines
 + * below. This is so that eventually, pages that are pinned via
 + * get_user_pages*() can be separately tracked and uniquely handled. In
 + * particular, interactions with RDMA and filesystems need special
 + * handling.
 + *
 + * put_user_page() and put_page() are not interchangeable, despite this early
 + * implementation that makes them look the same. put_user_page() calls must
 + * be perfectly matched up with get_user_page() calls.
++=======
+  * Pages that were pinned via pin_user_pages*() must be released via either
+  * unpin_user_page(), or one of the unpin_user_pages*() routines. This is so
+  * that eventually such pages can be separately tracked and uniquely handled. In
+  * particular, interactions with RDMA and filesystems need special handling.
+  *
+  * unpin_user_page() and put_page() are not interchangeable, despite this early
+  * implementation that makes them look the same. unpin_user_page() calls must
+  * be perfectly matched up with pin*() calls.
++>>>>>>> f1f6a7dd9b53 (mm, tree-wide: rename put_user_page*() to unpin_user_page*())
   */
- static inline void put_user_page(struct page *page)
+ static inline void unpin_user_page(struct page *page)
  {
  	put_page(page);
  }
@@@ -2492,13 -2589,16 +2503,18 @@@ struct page *follow_page(struct vm_area
  #define FOLL_COW	0x4000	/* internal GUP flag */
  #define FOLL_ANON	0x8000	/* don't do file mappings */
  #define FOLL_LONGTERM	0x10000	/* mapping lifetime is indefinite: see below */
++<<<<<<< HEAD
++=======
+ #define FOLL_SPLIT_PMD	0x20000	/* split huge pmd before returning */
+ #define FOLL_PIN	0x40000	/* pages must be released via unpin_user_page */
++>>>>>>> f1f6a7dd9b53 (mm, tree-wide: rename put_user_page*() to unpin_user_page*())
  
  /*
 - * FOLL_PIN and FOLL_LONGTERM may be used in various combinations with each
 - * other. Here is what they mean, and how to use them:
 + * NOTE on FOLL_LONGTERM:
   *
   * FOLL_LONGTERM indicates that the page will be held for an indefinite time
 - * period _often_ under userspace control.  This is in contrast to
 - * iov_iter_get_pages(), whose usages are transient.
 + * period _often_ under userspace control.  This is contrasted with
 + * iov_iter_get_pages() where usages which are transient.
   *
   * FIXME: For pages which are part of a filesystem, mappings are subject to the
   * lifetime enforced by the filesystem and we need guarantees that longterm
@@@ -2513,11 -2613,39 +2529,42 @@@
   * Currently only get_user_pages() and get_user_pages_fast() support this flag
   * and calls to get_user_pages_[un]locked are specifically not allowed.  This
   * is due to an incompatibility with the FS DAX check and
 - * FAULT_FLAG_ALLOW_RETRY.
 + * FAULT_FLAG_ALLOW_RETRY
   *
 - * In the CMA case: long term pins in a CMA region would unnecessarily fragment
 - * that region.  And so, CMA attempts to migrate the page before pinning, when
 + * In the CMA case: longterm pins in a CMA region would unnecessarily fragment
 + * that region.  And so CMA attempts to migrate the page before pinning when
   * FOLL_LONGTERM is specified.
++<<<<<<< HEAD
++=======
+  *
+  * FOLL_PIN indicates that a special kind of tracking (not just page->_refcount,
+  * but an additional pin counting system) will be invoked. This is intended for
+  * anything that gets a page reference and then touches page data (for example,
+  * Direct IO). This lets the filesystem know that some non-file-system entity is
+  * potentially changing the pages' data. In contrast to FOLL_GET (whose pages
+  * are released via put_page()), FOLL_PIN pages must be released, ultimately, by
+  * a call to unpin_user_page().
+  *
+  * FOLL_PIN is similar to FOLL_GET: both of these pin pages. They use different
+  * and separate refcounting mechanisms, however, and that means that each has
+  * its own acquire and release mechanisms:
+  *
+  *     FOLL_GET: get_user_pages*() to acquire, and put_page() to release.
+  *
+  *     FOLL_PIN: pin_user_pages*() to acquire, and unpin_user_pages to release.
+  *
+  * FOLL_PIN and FOLL_GET are mutually exclusive for a given function call.
+  * (The underlying pages may experience both FOLL_GET-based and FOLL_PIN-based
+  * calls applied to them, and that's perfectly OK. This is a constraint on the
+  * callers, not on the pages.)
+  *
+  * FOLL_PIN should be set internally by the pin_user_pages*() APIs, never
+  * directly by the caller. That's in order to help avoid mismatches when
+  * releasing pages: get_user_pages*() pages must be released via put_page(),
+  * while pin_user_pages*() pages must be released via unpin_user_page().
+  *
+  * Please see Documentation/vm/pin_user_pages.rst for more information.
++>>>>>>> f1f6a7dd9b53 (mm, tree-wide: rename put_user_page*() to unpin_user_page*())
   */
  
  static inline int vm_fault_to_errno(vm_fault_t vm_fault, int foll_flags)
* Unmerged path Documentation/core-api/pin_user_pages.rst
* Unmerged path Documentation/core-api/pin_user_pages.rst
diff --git a/arch/powerpc/mm/book3s64/iommu_api.c b/arch/powerpc/mm/book3s64/iommu_api.c
index 0745e8211697..d3a3ae59c120 100644
--- a/arch/powerpc/mm/book3s64/iommu_api.c
+++ b/arch/powerpc/mm/book3s64/iommu_api.c
@@ -176,7 +176,7 @@ static long mm_iommu_do_alloc(struct mm_struct *mm, unsigned long ua,
 
 free_exit:
 	/* free the references taken */
-	put_user_pages(mem->hpages, pinned);
+	unpin_user_pages(mem->hpages, pinned);
 
 	vfree(mem->hpas);
 	kfree(mem);
@@ -222,7 +222,7 @@ static void mm_iommu_unpin(struct mm_iommu_table_group_mem_t *mem)
 		if (mem->hpas[i] & MM_IOMMU_TABLE_GROUP_PAGE_DIRTY)
 			SetPageDirty(page);
 
-		put_user_page(page);
+		unpin_user_page(page);
 
 		mem->hpas[i] = 0;
 	}
diff --git a/drivers/gpu/drm/via/via_dmablit.c b/drivers/gpu/drm/via/via_dmablit.c
index 84cc4fe82e28..c43765b6d83e 100644
--- a/drivers/gpu/drm/via/via_dmablit.c
+++ b/drivers/gpu/drm/via/via_dmablit.c
@@ -185,8 +185,8 @@ via_free_sg_info(struct pci_dev *pdev, drm_via_sg_info_t *vsg)
 		kfree(vsg->desc_pages);
 		/* fall through */
 	case dr_via_pages_locked:
-		put_user_pages_dirty_lock(vsg->pages, vsg->num_pages,
-					  (vsg->direction == DMA_FROM_DEVICE));
+		unpin_user_pages_dirty_lock(vsg->pages, vsg->num_pages,
+					   (vsg->direction == DMA_FROM_DEVICE));
 		/* fall through */
 	case dr_via_pages_alloc:
 		vfree(vsg->pages);
diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c
index cd3083157554..baf8e1bd1265 100644
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@ -54,7 +54,7 @@ static void __ib_umem_release(struct ib_device *dev, struct ib_umem *umem, int d
 
 	for_each_sg_page(umem->sg_head.sgl, &sg_iter, umem->sg_nents, 0) {
 		page = sg_page_iter_page(&sg_iter);
-		put_user_pages_dirty_lock(&page, 1, umem->writable && dirty);
+		unpin_user_pages_dirty_lock(&page, 1, umem->writable && dirty);
 	}
 
 	sg_free_table(&umem->sg_head);
diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 469acb961fbd..48dd0a2f7910 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -118,7 +118,7 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
 			     size_t npages, bool dirty)
 {
-	put_user_pages_dirty_lock(p, npages, dirty);
+	unpin_user_pages_dirty_lock(p, npages, dirty);
 
 	if (mm) { /* during close after signal, mm can be NULL */
 		atomic64_sub(npages, &mm->pinned_vm);
diff --git a/drivers/infiniband/hw/mthca/mthca_memfree.c b/drivers/infiniband/hw/mthca/mthca_memfree.c
index 99108f3dcf01..e9c4edffba12 100644
--- a/drivers/infiniband/hw/mthca/mthca_memfree.c
+++ b/drivers/infiniband/hw/mthca/mthca_memfree.c
@@ -481,7 +481,7 @@ int mthca_map_user_db(struct mthca_dev *dev, struct mthca_uar *uar,
 
 	ret = pci_map_sg(dev->pdev, &db_tab->page[i].mem, 1, PCI_DMA_TODEVICE);
 	if (ret < 0) {
-		put_user_page(pages[0]);
+		unpin_user_page(pages[0]);
 		goto out;
 	}
 
@@ -489,7 +489,7 @@ int mthca_map_user_db(struct mthca_dev *dev, struct mthca_uar *uar,
 				 mthca_uarc_virt(dev, uar, i));
 	if (ret) {
 		pci_unmap_sg(dev->pdev, &db_tab->page[i].mem, 1, PCI_DMA_TODEVICE);
-		put_user_page(sg_page(&db_tab->page[i].mem));
+		unpin_user_page(sg_page(&db_tab->page[i].mem));
 		goto out;
 	}
 
@@ -555,7 +555,7 @@ void mthca_cleanup_user_db_tab(struct mthca_dev *dev, struct mthca_uar *uar,
 		if (db_tab->page[i].uvirt) {
 			mthca_UNMAP_ICM(dev, mthca_uarc_virt(dev, uar, i), 1);
 			pci_unmap_sg(dev->pdev, &db_tab->page[i].mem, 1, PCI_DMA_TODEVICE);
-			put_user_page(sg_page(&db_tab->page[i].mem));
+			unpin_user_page(sg_page(&db_tab->page[i].mem));
 		}
 	}
 
diff --git a/drivers/infiniband/hw/qib/qib_user_pages.c b/drivers/infiniband/hw/qib/qib_user_pages.c
index 1fad32275208..52552cfdecbc 100644
--- a/drivers/infiniband/hw/qib/qib_user_pages.c
+++ b/drivers/infiniband/hw/qib/qib_user_pages.c
@@ -40,7 +40,7 @@
 static void __qib_release_user_pages(struct page **p, size_t num_pages,
 				     int dirty)
 {
-	put_user_pages_dirty_lock(p, num_pages, dirty);
+	unpin_user_pages_dirty_lock(p, num_pages, dirty);
 }
 
 /*
diff --git a/drivers/infiniband/hw/qib/qib_user_sdma.c b/drivers/infiniband/hw/qib/qib_user_sdma.c
index a1a1ec4adffc..4a05e6926bdb 100644
--- a/drivers/infiniband/hw/qib/qib_user_sdma.c
+++ b/drivers/infiniband/hw/qib/qib_user_sdma.c
@@ -320,7 +320,7 @@ static int qib_user_sdma_page_to_frags(const struct qib_devdata *dd,
 		 * the caller can ignore this page.
 		 */
 		if (put) {
-			put_user_page(page);
+			unpin_user_page(page);
 		} else {
 			/* coalesce case */
 			kunmap(page);
@@ -634,7 +634,7 @@ static void qib_user_sdma_free_pkt_frag(struct device *dev,
 			kunmap(pkt->addr[i].page);
 
 		if (pkt->addr[i].put_page)
-			put_user_page(pkt->addr[i].page);
+			unpin_user_page(pkt->addr[i].page);
 		else
 			__free_page(pkt->addr[i].page);
 	} else if (pkt->addr[i].kvaddr) {
@@ -709,7 +709,7 @@ static int qib_user_sdma_pin_pages(const struct qib_devdata *dd,
 	/* if error, return all pages not managed by pkt */
 free_pages:
 	while (i < j)
-		put_user_page(pages[i++]);
+		unpin_user_page(pages[i++]);
 
 done:
 	return ret;
diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.c b/drivers/infiniband/hw/usnic/usnic_uiom.c
index 62e6ffa9ad78..572f5a9e4e08 100644
--- a/drivers/infiniband/hw/usnic/usnic_uiom.c
+++ b/drivers/infiniband/hw/usnic/usnic_uiom.c
@@ -75,7 +75,7 @@ static void usnic_uiom_put_pages(struct list_head *chunk_list, int dirty)
 		for_each_sg(chunk->page_list, sg, chunk->nents, i) {
 			page = sg_page(sg);
 			pa = sg_phys(sg);
-			put_user_pages_dirty_lock(&page, 1, dirty);
+			unpin_user_pages_dirty_lock(&page, 1, dirty);
 			usnic_dbg("pa: %pa\n", &pa);
 		}
 		kfree(chunk);
diff --git a/drivers/infiniband/sw/siw/siw_mem.c b/drivers/infiniband/sw/siw/siw_mem.c
index 308ece0c55fd..7b4f0b0f2f0c 100644
--- a/drivers/infiniband/sw/siw/siw_mem.c
+++ b/drivers/infiniband/sw/siw/siw_mem.c
@@ -63,7 +63,7 @@ struct siw_mem *siw_mem_id2obj(struct siw_device *sdev, int stag_index)
 static void siw_free_plist(struct siw_page_chunk *chunk, int num_pages,
 			   bool dirty)
 {
-	put_user_pages_dirty_lock(chunk->plist, num_pages, dirty);
+	unpin_user_pages_dirty_lock(chunk->plist, num_pages, dirty);
 }
 
 void siw_umem_release(struct siw_umem *umem, bool dirty)
diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c
index 8aeb97812634..f86f06c92ef9 100644
--- a/drivers/media/v4l2-core/videobuf-dma-sg.c
+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c
@@ -352,8 +352,8 @@ int videobuf_dma_free(struct videobuf_dmabuf *dma)
 	BUG_ON(dma->sglen);
 
 	if (dma->pages) {
-		put_user_pages_dirty_lock(dma->pages, dma->nr_pages,
-					  dma->direction == DMA_FROM_DEVICE);
+		unpin_user_pages_dirty_lock(dma->pages, dma->nr_pages,
+					    dma->direction == DMA_FROM_DEVICE);
 		kfree(dma->pages);
 		dma->pages = NULL;
 	}
* Unmerged path drivers/platform/goldfish/goldfish_pipe.c
* Unmerged path drivers/vfio/vfio_iommu_type1.c
diff --git a/fs/io_uring.c b/fs/io_uring.c
index 62a77af88798..82b6b1cd6ab9 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -7136,7 +7136,7 @@ static int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)
 		struct io_mapped_ubuf *imu = &ctx->user_bufs[i];
 
 		for (j = 0; j < imu->nr_bvecs; j++)
-			put_user_page(imu->bvec[j].bv_page);
+			unpin_user_page(imu->bvec[j].bv_page);
 
 		if (ctx->account_mem)
 			io_unaccount_mem(ctx->user, imu->nr_bvecs);
@@ -7281,7 +7281,7 @@ static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 			 * release any pages we did get
 			 */
 			if (pret > 0)
-				put_user_pages(pages, pret);
+				unpin_user_pages(pages, pret);
 			if (ctx->account_mem)
 				io_unaccount_mem(ctx->user, nr_pages);
 			kvfree(imu->bvec);
* Unmerged path include/linux/mm.h
diff --git a/mm/gup.c b/mm/gup.c
index 64143166cb9e..fb157e0fb1a8 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -29,7 +29,7 @@ struct follow_page_context {
 };
 
 /**
- * put_user_pages_dirty_lock() - release and optionally dirty gup-pinned pages
+ * unpin_user_pages_dirty_lock() - release and optionally dirty gup-pinned pages
  * @pages:  array of pages to be maybe marked dirty, and definitely released.
  * @npages: number of pages in the @pages array.
  * @make_dirty: whether to mark the pages dirty
@@ -39,19 +39,19 @@ struct follow_page_context {
  *
  * For each page in the @pages array, make that page (or its head page, if a
  * compound page) dirty, if @make_dirty is true, and if the page was previously
- * listed as clean. In any case, releases all pages using put_user_page(),
- * possibly via put_user_pages(), for the non-dirty case.
+ * listed as clean. In any case, releases all pages using unpin_user_page(),
+ * possibly via unpin_user_pages(), for the non-dirty case.
  *
- * Please see the put_user_page() documentation for details.
+ * Please see the unpin_user_page() documentation for details.
  *
  * set_page_dirty_lock() is used internally. If instead, set_page_dirty() is
  * required, then the caller should a) verify that this is really correct,
  * because _lock() is usually required, and b) hand code it:
- * set_page_dirty_lock(), put_user_page().
+ * set_page_dirty_lock(), unpin_user_page().
  *
  */
-void put_user_pages_dirty_lock(struct page **pages, unsigned long npages,
-			       bool make_dirty)
+void unpin_user_pages_dirty_lock(struct page **pages, unsigned long npages,
+				 bool make_dirty)
 {
 	unsigned long index;
 
@@ -62,7 +62,7 @@ void put_user_pages_dirty_lock(struct page **pages, unsigned long npages,
 	 */
 
 	if (!make_dirty) {
-		put_user_pages(pages, npages);
+		unpin_user_pages(pages, npages);
 		return;
 	}
 
@@ -90,21 +90,21 @@ void put_user_pages_dirty_lock(struct page **pages, unsigned long npages,
 		 */
 		if (!PageDirty(page))
 			set_page_dirty_lock(page);
-		put_user_page(page);
+		unpin_user_page(page);
 	}
 }
-EXPORT_SYMBOL(put_user_pages_dirty_lock);
+EXPORT_SYMBOL(unpin_user_pages_dirty_lock);
 
 /**
- * put_user_pages() - release an array of gup-pinned pages.
+ * unpin_user_pages() - release an array of gup-pinned pages.
  * @pages:  array of pages to be marked dirty and released.
  * @npages: number of pages in the @pages array.
  *
- * For each page in the @pages array, release the page using put_user_page().
+ * For each page in the @pages array, release the page using unpin_user_page().
  *
- * Please see the put_user_page() documentation for details.
+ * Please see the unpin_user_page() documentation for details.
  */
-void put_user_pages(struct page **pages, unsigned long npages)
+void unpin_user_pages(struct page **pages, unsigned long npages)
 {
 	unsigned long index;
 
@@ -114,9 +114,9 @@ void put_user_pages(struct page **pages, unsigned long npages)
 	 * single operation to the head page should suffice.
 	 */
 	for (index = 0; index < npages; index++)
-		put_user_page(pages[index]);
+		unpin_user_page(pages[index]);
 }
-EXPORT_SYMBOL(put_user_pages);
+EXPORT_SYMBOL(unpin_user_pages);
 
 static struct page *no_page_table(struct vm_area_struct *vma,
 		unsigned int flags)
diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c
index ede8b00206f4..3d42183e12a8 100644
--- a/mm/process_vm_access.c
+++ b/mm/process_vm_access.c
@@ -130,8 +130,8 @@ static int process_vm_rw_single_vec(unsigned long addr,
 		pa += pinned_pages * PAGE_SIZE;
 
 		/* If vm_write is set, the pages need to be made dirty: */
-		put_user_pages_dirty_lock(process_pages, pinned_pages,
-					  vm_write);
+		unpin_user_pages_dirty_lock(process_pages, pinned_pages,
+					    vm_write);
 	}
 
 	return rc;
diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 50d11f11514c..0a8ebd9e5b63 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -212,7 +212,7 @@ static int xdp_umem_map_pages(struct xdp_umem *umem)
 
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
-	put_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
+	unpin_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
 
 	kfree(umem->pgs);
 	umem->pgs = NULL;
