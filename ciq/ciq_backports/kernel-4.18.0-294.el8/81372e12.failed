libbpf: Add btf__set_fd() for more control over loaded BTF FD

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Andrii Nakryiko <andriin@fb.com>
commit 81372e121802fd57892a0b44d93cc747d9568627
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/81372e12.failed

Add setter for BTF FD to allow application more fine-grained control in more
advanced scenarios. Storing BTF FD inside `struct btf` provides little benefit
and probably would be better done differently (e.g., btf__load() could just
return FD on success), but we are stuck with this due to backwards
compatibility. The main problem is that it's impossible to load BTF and than
free user-space memory, but keep FD intact, because `struct btf` assumes
ownership of that FD upon successful load and will attempt to close it during
btf__free(). To allow callers (e.g., libbpf itself for BTF sanitization) to
have more control over this, add btf__set_fd() to allow to reset FD
arbitrarily, if necessary.

	Signed-off-by: Andrii Nakryiko <andriin@fb.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
Link: https://lore.kernel.org/bpf/20200708015318.3827358-3-andriin@fb.com
(cherry picked from commit 81372e121802fd57892a0b44d93cc747d9568627)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/lib/bpf/libbpf.map
diff --cc tools/lib/bpf/libbpf.map
index 69599528ddbe,c5d5c7664c3b..000000000000
--- a/tools/lib/bpf/libbpf.map
+++ b/tools/lib/bpf/libbpf.map
@@@ -286,3 -254,39 +286,42 @@@ LIBBPF_0.0.8 
  		bpf_program__set_lsm;
  		bpf_set_link_xdp_fd_opts;
  } LIBBPF_0.0.7;
++<<<<<<< HEAD
++=======
+ 
+ LIBBPF_0.0.9 {
+ 	global:
+ 		bpf_enable_stats;
+ 		bpf_iter_create;
+ 		bpf_link_get_fd_by_id;
+ 		bpf_link_get_next_id;
+ 		bpf_program__attach_iter;
+ 		bpf_program__attach_netns;
+ 		perf_buffer__consume;
+ 		ring_buffer__add;
+ 		ring_buffer__consume;
+ 		ring_buffer__free;
+ 		ring_buffer__new;
+ 		ring_buffer__poll;
+ } LIBBPF_0.0.8;
+ 
+ LIBBPF_0.1.0 {
+ 	global:
+ 		bpf_map__ifindex;
+ 		bpf_map__key_size;
+ 		bpf_map__map_flags;
+ 		bpf_map__max_entries;
+ 		bpf_map__numa_node;
+ 		bpf_map__set_key_size;
+ 		bpf_map__set_map_flags;
+ 		bpf_map__set_max_entries;
+ 		bpf_map__set_numa_node;
+ 		bpf_map__set_type;
+ 		bpf_map__set_value_size;
+ 		bpf_map__type;
+ 		bpf_map__value_size;
+ 		bpf_program__autoload;
+ 		bpf_program__set_autoload;
+ 		btf__set_fd;
+ } LIBBPF_0.0.9;
++>>>>>>> 81372e121802 (libbpf: Add btf__set_fd() for more control over loaded BTF FD)
diff --git a/tools/lib/bpf/btf.c b/tools/lib/bpf/btf.c
index bfef3d606b54..c8861c9e3635 100644
--- a/tools/lib/bpf/btf.c
+++ b/tools/lib/bpf/btf.c
@@ -389,7 +389,7 @@ void btf__free(struct btf *btf)
 	if (!btf)
 		return;
 
-	if (btf->fd != -1)
+	if (btf->fd >= 0)
 		close(btf->fd);
 
 	free(btf->data);
@@ -700,6 +700,11 @@ int btf__fd(const struct btf *btf)
 	return btf->fd;
 }
 
+void btf__set_fd(struct btf *btf, int fd)
+{
+	btf->fd = fd;
+}
+
 const void *btf__get_raw_data(const struct btf *btf, __u32 *size)
 {
 	*size = btf->data_size;
diff --git a/tools/lib/bpf/btf.h b/tools/lib/bpf/btf.h
index 06cd1731c154..173eff23c472 100644
--- a/tools/lib/bpf/btf.h
+++ b/tools/lib/bpf/btf.h
@@ -79,6 +79,7 @@ LIBBPF_API __s64 btf__resolve_size(const struct btf *btf, __u32 type_id);
 LIBBPF_API int btf__resolve_type(const struct btf *btf, __u32 type_id);
 LIBBPF_API int btf__align_of(const struct btf *btf, __u32 id);
 LIBBPF_API int btf__fd(const struct btf *btf);
+LIBBPF_API void btf__set_fd(struct btf *btf, int fd);
 LIBBPF_API const void *btf__get_raw_data(const struct btf *btf, __u32 *size);
 LIBBPF_API const char *btf__name_by_offset(const struct btf *btf, __u32 offset);
 LIBBPF_API int btf__get_from_id(__u32 id, struct btf **btf);
* Unmerged path tools/lib/bpf/libbpf.map
