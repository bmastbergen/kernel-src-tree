mm: remove use-once cache bias from LRU balancing

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit 9682468747390c14962114f261cd76ba188ed987
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/96824687.failed

When the splitlru patches divided page cache and swap-backed pages into
separate LRU lists, the pressure balance between the lists was biased to
account for the fact that streaming IO can cause memory pressure with a
flood of pages that are used only once.  New page cache additions would
tip the balance toward the file LRU, and repeat access would neutralize
that bias again.  This ensured that page reclaim would always go for
used-once cache first.

Since e9868505987a ("mm,vmscan: only evict file pages when we have
plenty"), page reclaim generally skips over swap-backed memory entirely as
long as there is used-once cache present, and will apply the LRU balancing
when only repeatedly accessed cache pages are left - at which point the
previous use-once bias will have been neutralized.  This makes the
use-once cache balancing bias unnecessary.

	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Rik van Riel <riel@surriel.com>
Link: http://lkml.kernel.org/r/20200520232525.798933-7-hannes@cmpxchg.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 9682468747390c14962114f261cd76ba188ed987)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap.c
diff --cc mm/swap.c
index 70728521e27e,116b609c25c1..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -275,7 -293,6 +275,10 @@@ static void __activate_page(struct pag
  			    void *arg)
  {
  	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
++<<<<<<< HEAD
 +		int file = page_is_file_cache(page);
++=======
++>>>>>>> 968246874739 (mm: remove use-once cache bias from LRU balancing)
  		int lru = page_lru_base_type(page);
  
  		del_page_from_lru_list(page, lruvec, lru);
@@@ -285,7 -302,6 +288,10 @@@
  		trace_mm_lru_activate(page);
  
  		__count_vm_event(PGACTIVATE);
++<<<<<<< HEAD
 +		update_page_reclaim_stat(lruvec, file, 1);
++=======
++>>>>>>> 968246874739 (mm: remove use-once cache bias from LRU balancing)
  	}
  }
  
@@@ -944,8 -973,6 +950,11 @@@ static void __pagevec_lru_add_fn(struc
  
  	if (page_evictable(page)) {
  		lru = page_lru(page);
++<<<<<<< HEAD
 +		update_page_reclaim_stat(lruvec, page_is_file_cache(page),
 +					 PageActive(page));
++=======
++>>>>>>> 968246874739 (mm: remove use-once cache bias from LRU balancing)
  		if (was_unevictable)
  			count_vm_event(UNEVICTABLE_PGRESCUED);
  	} else {
* Unmerged path mm/swap.c
