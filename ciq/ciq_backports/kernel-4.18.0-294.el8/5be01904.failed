net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Raed Salem <raeds@mellanox.com>
commit 5be019040cb7bab4caf152cacadffee91a78b506
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/5be01904.failed

In the TX data path, spot packets with xfrm stack IPsec offload
indication.

Fill Software-Parser segment in TX descriptor so that the hardware
may parse the ESP protocol, and perform TX checksum offload on the
inner payload.

Support GSO, by providing the trailer data and ICV placeholder
so HW can fill it post encryption operation.

Padding alignment cannot be performed in HW (ConnectX-6Dx) due to
a bug. Software can overcome this limitation by adding NETIF_F_HW_ESP to
the gso_partial_features field in netdev so the packets being
aligned by the stack.

l4_inner_checksum cannot be offloaded by HW for IPsec tunnel type packet.

Note that for GSO SKBs, the stack does not include an ESP trailer,
unlike the non-GSO case.

Below is the iperf3 performance report on two server of 24 cores
Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz with ConnectX6-DX.
All the bandwidth test uses iperf3 TCP traffic with packet size 128KB.
Each tunnel uses one iperf3 stream with one thread (option -P1).
TX crypto offload shows improvements on both bandwidth
and CPU utilization.

----------------------------------------------------------------------
Mode            |  Num tunnel | BW     | Send CPU util | Recv CPU util
                |             | (Gbps) | (Average %)   | (Average %)
----------------------------------------------------------------------
Cryto offload   |             |        |               |
(RX only)       | 1           | 4.7    | 4.2           | 3.5
----------------------------------------------------------------------
Cryto offload   |             |        |               |
(RX only)       | 24          | 15.6   | 20            | 10
----------------------------------------------------------------------
Non-offload     | 1           | 4.6    | 4             | 5
----------------------------------------------------------------------
Non-offload     | 24          | 11.9   | 16            | 12
----------------------------------------------------------------------
Cryto offload   |             |        |               |
(TX & RX)       | 1           | 11.9   | 2.1           | 5.9
----------------------------------------------------------------------
Cryto offload   |             |        |               |
(TX & RX)       | 24          | 38     | 9.5           | 27.5
----------------------------------------------------------------------
Cryto offload   |             |        |               |
(TX only)       | 1           | 4.7    | 0.7           | 5
----------------------------------------------------------------------
Cryto offload   |             |        |               |
(TX only)       | 24          | 14.5   | 6             | 20

Regression tests show no degradation on non-ipsec and
non-offload-ipsec traffics. The packet rate test uses pktgen UDP to
transmit on single CPU, the instructions and cycles are measured on
the transmit CPU.

before:
----------------------------------------------------------------------
Non-offload             | 1           | 4.7    | 4.2           | 5.1
----------------------------------------------------------------------
Non-offload             | 24          | 11.2   | 14            | 15
----------------------------------------------------------------------
Non-ipsec               | 1           | 28     | 4             | 5.7
----------------------------------------------------------------------
Non-ipsec               | 24          | 68.3   | 17.8          | 39.7
----------------------------------------------------------------------
Non-ipsec packet rate(BURST=1000 BC=5 NCPUS=1 SIZE=60)
13.56Mpps, 456 instructions/pkt, 191 cycles/pkt

after:
----------------------------------------------------------------------
Non-offload             | 1           | 4.69    | 4.2          | 5
----------------------------------------------------------------------
Non-offload             | 24          | 11.9   | 13.5          | 15.1
----------------------------------------------------------------------
Non-ipsec               | 1           | 29     | 3.2           | 5.5
----------------------------------------------------------------------
Non-ipsec               | 24          | 68.2   | 18.5          | 39.8
----------------------------------------------------------------------
Non-ipsec packet rate: 13.56Mpps, 472 instructions/pkt, 191 cycles/pkt

	Signed-off-by: Raed Salem <raeds@mellanox.com>
	Signed-off-by: Huy Nguyen <huyn@mellanox.com>
	Reviewed-by: Maxim Mikityanskiy <maximmi@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 5be019040cb7bab4caf152cacadffee91a78b506)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index fac145dcf2ce,899b98aca0d3..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@@ -127,24 -138,69 +137,76 @@@ static inline bool mlx5e_accel_tx_begin
  	return true;
  }
  
++<<<<<<< HEAD
 +static inline bool mlx5e_accel_tx_finish(struct mlx5e_priv *priv,
 +					 struct mlx5e_txqsq *sq,
 +					 struct sk_buff *skb,
++=======
+ static inline bool mlx5e_accel_tx_is_ipsec_flow(struct mlx5e_accel_tx_state *state)
+ {
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 	return mlx5e_ipsec_is_tx_flow(&state->ipsec);
+ #endif
+ 
+ 	return false;
+ }
+ 
+ static inline unsigned int mlx5e_accel_tx_ids_len(struct mlx5e_txqsq *sq,
+ 						  struct mlx5e_accel_tx_state *state)
+ {
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 	if (test_bit(MLX5E_SQ_STATE_IPSEC, &sq->state))
+ 		return mlx5e_ipsec_tx_ids_len(&state->ipsec);
+ #endif
+ 
+ 	return 0;
+ }
+ 
+ /* Part of the eseg touched by TX offloads */
+ #define MLX5E_ACCEL_ESEG_LEN offsetof(struct mlx5_wqe_eth_seg, mss)
+ 
+ static inline bool mlx5e_accel_tx_eseg(struct mlx5e_priv *priv,
+ 				       struct sk_buff *skb,
+ 				       struct mlx5_wqe_eth_seg *eseg)
+ {
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 	if (xfrm_offload(skb))
+ 		mlx5e_ipsec_tx_build_eseg(priv, skb, eseg);
+ #endif
+ 
+ #if IS_ENABLED(CONFIG_GENEVE)
+ 	if (skb->encapsulation)
+ 		mlx5e_tx_tunnel_accel(skb, eseg);
+ #endif
+ 
+ 	return true;
+ }
+ 
+ static inline void mlx5e_accel_tx_finish(struct mlx5e_txqsq *sq,
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  					 struct mlx5e_tx_wqe *wqe,
- 					 struct mlx5e_accel_tx_state *state)
+ 					 struct mlx5e_accel_tx_state *state,
+ 					 struct mlx5_wqe_inline_seg *inlseg)
  {
  #ifdef CONFIG_MLX5_EN_TLS
  	mlx5e_tls_handle_tx_wqe(sq, &wqe->ctrl, &state->tls);
  #endif
  
  #ifdef CONFIG_MLX5_EN_IPSEC
++<<<<<<< HEAD
 +	if (test_bit(MLX5E_SQ_STATE_IPSEC, &sq->state)) {
 +		if (unlikely(!mlx5e_ipsec_handle_tx_skb(priv, &wqe->eth, skb)))
 +			return false;
 +	}
 +#endif
 +
 +	return true;
++=======
+ 	if (test_bit(MLX5E_SQ_STATE_IPSEC, &sq->state) &&
+ 	    state->ipsec.xo && state->ipsec.tailen)
+ 		mlx5e_ipsec_handle_tx_wqe(wqe, &state->ipsec, inlseg);
+ #endif
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  }
  
 -static inline int mlx5e_accel_init_rx(struct mlx5e_priv *priv)
 -{
 -	return mlx5e_ktls_init_rx(priv);
 -}
 -
 -static inline void mlx5e_accel_cleanup_rx(struct mlx5e_priv *priv)
 -{
 -	mlx5e_ktls_cleanup_rx(priv);
 -}
  #endif /* __MLX5E_EN_ACCEL_H__ */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index d2c9bb371f27,82b4419af9d4..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -233,14 -252,25 +253,32 @@@ dma_unmap_wqe_err
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
 +static bool mlx5e_transport_inline_tx_wqe(struct mlx5_wqe_ctrl_seg *cseg)
 +{
 +	return cseg && !!cseg->tis_tir_num;
 +}
++=======
+ struct mlx5e_tx_attr {
+ 	u32 num_bytes;
+ 	u16 headlen;
+ 	u16 ihs;
+ 	__be16 mss;
+ 	u16 insz;
+ 	u8 opcode;
+ };
+ 
+ struct mlx5e_tx_wqe_attr {
+ 	u16 ds_cnt;
+ 	u16 ds_cnt_inl;
+ 	u16 ds_cnt_ids;
+ 	u8 num_wqebbs;
+ };
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  
  static u8
 -mlx5e_tx_wqe_inline_mode(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 -			 struct mlx5e_accel_tx_state *accel)
 +mlx5e_tx_wqe_inline_mode(struct mlx5e_txqsq *sq, struct mlx5_wqe_ctrl_seg *cseg,
 +			 struct sk_buff *skb)
  {
  	u8 mode;
  
@@@ -256,9 -288,91 +294,93 @@@
  	return mode;
  }
  
++<<<<<<< HEAD
++=======
+ static void mlx5e_sq_xmit_prepare(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 				  struct mlx5e_accel_tx_state *accel,
+ 				  struct mlx5e_tx_attr *attr)
+ {
+ 	struct mlx5e_sq_stats *stats = sq->stats;
+ 
+ 	if (skb_is_gso(skb)) {
+ 		u16 ihs = mlx5e_tx_get_gso_ihs(sq, skb);
+ 
+ 		*attr = (struct mlx5e_tx_attr) {
+ 			.opcode    = MLX5_OPCODE_LSO,
+ 			.mss       = cpu_to_be16(skb_shinfo(skb)->gso_size),
+ 			.ihs       = ihs,
+ 			.num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs,
+ 			.headlen   = skb_headlen(skb) - ihs,
+ 		};
+ 
+ 		stats->packets += skb_shinfo(skb)->gso_segs;
+ 	} else {
+ 		u8 mode = mlx5e_tx_wqe_inline_mode(sq, skb, accel);
+ 		u16 ihs = mlx5e_calc_min_inline(mode, skb);
+ 
+ 		*attr = (struct mlx5e_tx_attr) {
+ 			.opcode    = MLX5_OPCODE_SEND,
+ 			.mss       = cpu_to_be16(0),
+ 			.ihs       = ihs,
+ 			.num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN),
+ 			.headlen   = skb_headlen(skb) - ihs,
+ 		};
+ 
+ 		stats->packets++;
+ 	}
+ 
+ 	attr->insz = mlx5e_accel_tx_ids_len(sq, accel);
+ 	stats->bytes += attr->num_bytes;
+ }
+ 
+ static void mlx5e_sq_calc_wqe_attr(struct sk_buff *skb, const struct mlx5e_tx_attr *attr,
+ 				   struct mlx5e_tx_wqe_attr *wqe_attr)
+ {
+ 	u16 ds_cnt = MLX5E_TX_WQE_EMPTY_DS_COUNT;
+ 	u16 ds_cnt_inl = 0;
+ 	u16 ds_cnt_ids = 0;
+ 
+ 	if (attr->insz)
+ 		ds_cnt_ids = DIV_ROUND_UP(sizeof(struct mlx5_wqe_inline_seg) + attr->insz,
+ 					  MLX5_SEND_WQE_DS);
+ 
+ 	ds_cnt += !!attr->headlen + skb_shinfo(skb)->nr_frags + ds_cnt_ids;
+ 	if (attr->ihs) {
+ 		u16 inl = attr->ihs - INL_HDR_START_SZ;
+ 
+ 		if (skb_vlan_tag_present(skb))
+ 			inl += VLAN_HLEN;
+ 
+ 		ds_cnt_inl = DIV_ROUND_UP(inl, MLX5_SEND_WQE_DS);
+ 		ds_cnt += ds_cnt_inl;
+ 	}
+ 
+ 	*wqe_attr = (struct mlx5e_tx_wqe_attr) {
+ 		.ds_cnt     = ds_cnt,
+ 		.ds_cnt_inl = ds_cnt_inl,
+ 		.ds_cnt_ids = ds_cnt_ids,
+ 		.num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS),
+ 	};
+ }
+ 
+ static void mlx5e_tx_skb_update_hwts_flags(struct sk_buff *skb)
+ {
+ 	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+ 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+ }
+ 
+ static void mlx5e_tx_check_stop(struct mlx5e_txqsq *sq)
+ {
+ 	if (unlikely(!mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, sq->stop_room))) {
+ 		netif_tx_stop_queue(sq->txq);
+ 		sq->stats->stopped++;
+ 	}
+ }
+ 
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  static inline void
  mlx5e_txwqe_complete(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 -		     const struct mlx5e_tx_attr *attr,
 -		     const struct mlx5e_tx_wqe_attr *wqe_attr, u8 num_dma,
 +		     u8 opcode, u16 ds_cnt, u8 num_wqebbs, u32 num_bytes, u8 num_dma,
  		     struct mlx5e_tx_wqe_info *wi, struct mlx5_wqe_ctrl_seg *cseg,
  		     bool xmit_more)
  {
@@@ -365,24 -422,18 +487,33 @@@ void mlx5e_sq_xmit(struct mlx5e_txqsq *
  	eseg = &wqe->eth;
  	dseg =  wqe->data;
  
 -	eseg->mss = attr->mss;
 +#if IS_ENABLED(CONFIG_GENEVE)
 +	if (skb->encapsulation)
 +		mlx5e_tx_tunnel_accel(skb, eseg);
 +#endif
 +	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
  
 -	if (attr->ihs) {
 +	eseg->mss = mss;
 +
 +	if (ihs) {
  		if (skb_vlan_tag_present(skb)) {
++<<<<<<< HEAD
 +			eseg->inline_hdr.sz = cpu_to_be16(ihs + VLAN_HLEN);
 +			mlx5e_insert_vlan(eseg->inline_hdr.start, skb, ihs);
 +			stats->added_vlan_packets++;
 +		} else {
 +			eseg->inline_hdr.sz = cpu_to_be16(ihs);
 +			memcpy(eseg->inline_hdr.start, skb->data, ihs);
++=======
+ 			eseg->inline_hdr.sz |= cpu_to_be16(attr->ihs + VLAN_HLEN);
+ 			mlx5e_insert_vlan(eseg->inline_hdr.start, skb, attr->ihs);
+ 			stats->added_vlan_packets++;
+ 		} else {
+ 			eseg->inline_hdr.sz |= cpu_to_be16(attr->ihs);
+ 			memcpy(eseg->inline_hdr.start, skb->data, attr->ihs);
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  		}
 -		dseg += wqe_attr->ds_cnt_inl;
 +		dseg += ds_cnt_inl;
  	} else if (skb_vlan_tag_present(skb)) {
  		eseg->insert.type = cpu_to_be16(MLX5_ETH_WQE_INSERT_VLAN);
  		if (skb->vlan_proto == cpu_to_be16(ETH_P_8021AD))
@@@ -391,7 -442,9 +522,13 @@@
  		stats->added_vlan_packets++;
  	}
  
++<<<<<<< HEAD
 +	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + ihs, headlen, dseg);
++=======
+ 	dseg += wqe_attr->ds_cnt_ids;
+ 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr->ihs,
+ 					  attr->headlen, dseg);
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  	if (unlikely(num_dma < 0))
  		goto err_drop;
  
@@@ -405,6 -457,167 +542,170 @@@ err_drop
  	dev_kfree_skb_any(skb);
  }
  
++<<<<<<< HEAD
++=======
+ static bool mlx5e_tx_skb_supports_mpwqe(struct sk_buff *skb, struct mlx5e_tx_attr *attr)
+ {
+ 	return !skb_is_nonlinear(skb) && !skb_vlan_tag_present(skb) && !attr->ihs &&
+ 	       !attr->insz;
+ }
+ 
+ static bool mlx5e_tx_mpwqe_same_eseg(struct mlx5e_txqsq *sq, struct mlx5_wqe_eth_seg *eseg)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 
+ 	/* Assumes the session is already running and has at least one packet. */
+ 	return !memcmp(&session->wqe->eth, eseg, MLX5E_ACCEL_ESEG_LEN);
+ }
+ 
+ static void mlx5e_tx_mpwqe_session_start(struct mlx5e_txqsq *sq,
+ 					 struct mlx5_wqe_eth_seg *eseg)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u16 pi;
+ 
+ 	pi = mlx5e_txqsq_get_next_pi(sq, MLX5E_TX_MPW_MAX_WQEBBS);
+ 	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
+ 	prefetchw(wqe->data);
+ 
+ 	*session = (struct mlx5e_tx_mpwqe) {
+ 		.wqe = wqe,
+ 		.bytes_count = 0,
+ 		.ds_count = MLX5E_TX_WQE_EMPTY_DS_COUNT,
+ 		.pkt_count = 0,
+ 		.inline_on = 0,
+ 	};
+ 
+ 	memcpy(&session->wqe->eth, eseg, MLX5E_ACCEL_ESEG_LEN);
+ 
+ 	sq->stats->mpwqe_blks++;
+ }
+ 
+ static bool mlx5e_tx_mpwqe_session_is_active(struct mlx5e_txqsq *sq)
+ {
+ 	return sq->mpwqe.wqe;
+ }
+ 
+ static void mlx5e_tx_mpwqe_add_dseg(struct mlx5e_txqsq *sq, struct mlx5e_xmit_data *txd)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 
+ 	dseg = (struct mlx5_wqe_data_seg *)session->wqe + session->ds_count;
+ 
+ 	session->pkt_count++;
+ 	session->bytes_count += txd->len;
+ 
+ 	dseg->addr = cpu_to_be64(txd->dma_addr);
+ 	dseg->byte_count = cpu_to_be32(txd->len);
+ 	dseg->lkey = sq->mkey_be;
+ 	session->ds_count++;
+ 
+ 	sq->stats->mpwqe_pkts++;
+ }
+ 
+ static struct mlx5_wqe_ctrl_seg *mlx5e_tx_mpwqe_session_complete(struct mlx5e_txqsq *sq)
+ {
+ 	struct mlx5e_tx_mpwqe *session = &sq->mpwqe;
+ 	u8 ds_count = session->ds_count;
+ 	struct mlx5_wqe_ctrl_seg *cseg;
+ 	struct mlx5e_tx_wqe_info *wi;
+ 	u16 pi;
+ 
+ 	cseg = &session->wqe->ctrl;
+ 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_ENHANCED_MPSW);
+ 	cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_count);
+ 
+ 	pi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);
+ 	wi = &sq->db.wqe_info[pi];
+ 	*wi = (struct mlx5e_tx_wqe_info) {
+ 		.skb = NULL,
+ 		.num_bytes = session->bytes_count,
+ 		.num_wqebbs = DIV_ROUND_UP(ds_count, MLX5_SEND_WQEBB_NUM_DS),
+ 		.num_dma = session->pkt_count,
+ 		.num_fifo_pkts = session->pkt_count,
+ 	};
+ 
+ 	sq->pc += wi->num_wqebbs;
+ 
+ 	session->wqe = NULL;
+ 
+ 	mlx5e_tx_check_stop(sq);
+ 
+ 	return cseg;
+ }
+ 
+ static void
+ mlx5e_sq_xmit_mpwqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 		    struct mlx5_wqe_eth_seg *eseg, bool xmit_more)
+ {
+ 	struct mlx5_wqe_ctrl_seg *cseg;
+ 	struct mlx5e_xmit_data txd;
+ 
+ 	if (!mlx5e_tx_mpwqe_session_is_active(sq)) {
+ 		mlx5e_tx_mpwqe_session_start(sq, eseg);
+ 	} else if (!mlx5e_tx_mpwqe_same_eseg(sq, eseg)) {
+ 		mlx5e_tx_mpwqe_session_complete(sq);
+ 		mlx5e_tx_mpwqe_session_start(sq, eseg);
+ 	}
+ 
+ 	sq->stats->xmit_more += xmit_more;
+ 
+ 	txd.data = skb->data;
+ 	txd.len = skb->len;
+ 
+ 	txd.dma_addr = dma_map_single(sq->pdev, txd.data, txd.len, DMA_TO_DEVICE);
+ 	if (unlikely(dma_mapping_error(sq->pdev, txd.dma_addr)))
+ 		goto err_unmap;
+ 	mlx5e_dma_push(sq, txd.dma_addr, txd.len, MLX5E_DMA_MAP_SINGLE);
+ 
+ 	mlx5e_skb_fifo_push(sq, skb);
+ 
+ 	mlx5e_tx_mpwqe_add_dseg(sq, &txd);
+ 
+ 	mlx5e_tx_skb_update_hwts_flags(skb);
+ 
+ 	if (unlikely(mlx5e_tx_mpwqe_is_full(&sq->mpwqe))) {
+ 		/* Might stop the queue and affect the retval of __netdev_tx_sent_queue. */
+ 		cseg = mlx5e_tx_mpwqe_session_complete(sq);
+ 
+ 		if (__netdev_tx_sent_queue(sq->txq, txd.len, xmit_more))
+ 			mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ 	} else if (__netdev_tx_sent_queue(sq->txq, txd.len, xmit_more)) {
+ 		/* Might stop the queue, but we were asked to ring the doorbell anyway. */
+ 		cseg = mlx5e_tx_mpwqe_session_complete(sq);
+ 
+ 		mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ 	}
+ 
+ 	return;
+ 
+ err_unmap:
+ 	mlx5e_dma_unmap_wqe_err(sq, 1);
+ 	sq->stats->dropped++;
+ 	dev_kfree_skb_any(skb);
+ }
+ 
+ void mlx5e_tx_mpwqe_ensure_complete(struct mlx5e_txqsq *sq)
+ {
+ 	/* Unlikely in non-MPWQE workloads; not important in MPWQE workloads. */
+ 	if (unlikely(mlx5e_tx_mpwqe_session_is_active(sq)))
+ 		mlx5e_tx_mpwqe_session_complete(sq);
+ }
+ 
+ static bool mlx5e_txwqe_build_eseg(struct mlx5e_priv *priv, struct mlx5e_txqsq *sq,
+ 				   struct sk_buff *skb, struct mlx5_wqe_eth_seg *eseg)
+ {
+ 	if (unlikely(!mlx5e_accel_tx_eseg(priv, skb, eseg)))
+ 		return false;
+ 
+ 	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)
  {
  	struct mlx5e_priv *priv = netdev_priv(dev);
@@@ -423,12 -655,13 +724,19 @@@
  	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
  
  	/* May update the WQE, but may not post other WQEs. */
++<<<<<<< HEAD
 +	if (unlikely(!mlx5e_accel_tx_finish(priv, sq, skb, wqe, &accel)))
 +		goto out;
++=======
+ 	mlx5e_accel_tx_finish(sq, wqe, &accel,
+ 			      (struct mlx5_wqe_inline_seg *)(wqe->data + wqe_attr.ds_cnt_inl));
+ 	if (unlikely(!mlx5e_txwqe_build_eseg(priv, sq, skb, &wqe->eth)))
+ 		return NETDEV_TX_OK;
++>>>>>>> 5be019040cb7 (net/mlx5e: IPsec: Add Connect-X IPsec Tx data path offload)
  
 -	mlx5e_sq_xmit_wqe(sq, skb, &attr, &wqe_attr, wqe, pi, netdev_xmit_more());
 +	mlx5e_sq_xmit(sq, skb, wqe, pi, netdev_xmit_more());
  
 +out:
  	return NETDEV_TX_OK;
  }
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.c b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.c
index 8f8b95de0247..51efff955513 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec.c
@@ -566,6 +566,9 @@ void mlx5e_ipsec_build_netdev(struct mlx5e_priv *priv)
 		return;
 	}
 
+	if (mlx5_is_ipsec_device(mdev))
+		netdev->gso_partial_features |= NETIF_F_GSO_ESP;
+
 	mlx5_core_dbg(mdev, "mlx5e: ESP GSO capability turned on\n");
 	netdev->features |= NETIF_F_GSO_ESP;
 	netdev->hw_features |= NETIF_F_GSO_ESP;
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.c
index 424bd4e75f9b..59f531c272a7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.c
@@ -34,7 +34,7 @@
 #include <crypto/aead.h>
 #include <net/xfrm.h>
 #include <net/esp.h>
-
+#include "accel/ipsec_offload.h"
 #include "en_accel/ipsec_rxtx.h"
 #include "en_accel/ipsec.h"
 #include "accel/accel.h"
@@ -233,18 +233,94 @@ static void mlx5e_ipsec_set_metadata(struct sk_buff *skb,
 		   ntohs(mdata->content.tx.seq));
 }
 
-bool mlx5e_ipsec_handle_tx_skb(struct mlx5e_priv *priv,
-			       struct mlx5_wqe_eth_seg *eseg,
-			       struct sk_buff *skb)
+void mlx5e_ipsec_handle_tx_wqe(struct mlx5e_tx_wqe *wqe,
+			       struct mlx5e_accel_tx_ipsec_state *ipsec_st,
+			       struct mlx5_wqe_inline_seg *inlseg)
+{
+	inlseg->byte_count = cpu_to_be32(ipsec_st->tailen | MLX5_INLINE_SEG);
+	esp_output_fill_trailer((u8 *)inlseg->data, 0, ipsec_st->plen, ipsec_st->xo->proto);
+}
+
+static int mlx5e_ipsec_set_state(struct mlx5e_priv *priv,
+				 struct sk_buff *skb,
+				 struct xfrm_state *x,
+				 struct xfrm_offload *xo,
+				 struct mlx5e_accel_tx_ipsec_state *ipsec_st)
+{
+	unsigned int blksize, clen, alen, plen;
+	struct crypto_aead *aead;
+	unsigned int tailen;
+
+	ipsec_st->x = x;
+	ipsec_st->xo = xo;
+	if (mlx5_is_ipsec_device(priv->mdev)) {
+		aead = x->data;
+		alen = crypto_aead_authsize(aead);
+		blksize = ALIGN(crypto_aead_blocksize(aead), 4);
+		clen = ALIGN(skb->len + 2, blksize);
+		plen = max_t(u32, clen - skb->len, 4);
+		tailen = plen + alen;
+		ipsec_st->plen = plen;
+		ipsec_st->tailen = tailen;
+	}
+
+	return 0;
+}
+
+void mlx5e_ipsec_tx_build_eseg(struct mlx5e_priv *priv, struct sk_buff *skb,
+			       struct mlx5_wqe_eth_seg *eseg)
 {
 	struct xfrm_offload *xo = xfrm_offload(skb);
-	struct mlx5e_ipsec_metadata *mdata;
-	struct mlx5e_ipsec_sa_entry *sa_entry;
+	struct xfrm_encap_tmpl  *encap;
 	struct xfrm_state *x;
 	struct sec_path *sp;
+	u8 l3_proto;
+
+	sp = skb_sec_path(skb);
+	if (unlikely(sp->len != 1))
+		return;
+
+	x = xfrm_input_state(skb);
+	if (unlikely(!x))
+		return;
+
+	if (unlikely(!x->xso.offload_handle ||
+		     (skb->protocol != htons(ETH_P_IP) &&
+		      skb->protocol != htons(ETH_P_IPV6))))
+		return;
+
+	mlx5e_ipsec_set_swp(skb, eseg, x->props.mode, xo);
 
-	if (!xo)
-		return true;
+	l3_proto = (x->props.family == AF_INET) ?
+		   ((struct iphdr *)skb_network_header(skb))->protocol :
+		   ((struct ipv6hdr *)skb_network_header(skb))->nexthdr;
+
+	if (mlx5_is_ipsec_device(priv->mdev)) {
+		eseg->flow_table_metadata |= cpu_to_be32(MLX5_ETH_WQE_FT_META_IPSEC);
+		eseg->trailer |= cpu_to_be32(MLX5_ETH_WQE_INSERT_TRAILER);
+		encap = x->encap;
+		if (!encap) {
+			eseg->trailer |= (l3_proto == IPPROTO_ESP) ?
+				cpu_to_be32(MLX5_ETH_WQE_TRAILER_HDR_OUTER_IP_ASSOC) :
+				cpu_to_be32(MLX5_ETH_WQE_TRAILER_HDR_OUTER_L4_ASSOC);
+		} else if (encap->encap_type == UDP_ENCAP_ESPINUDP) {
+			eseg->trailer |= (l3_proto == IPPROTO_ESP) ?
+				cpu_to_be32(MLX5_ETH_WQE_TRAILER_HDR_INNER_IP_ASSOC) :
+				cpu_to_be32(MLX5_ETH_WQE_TRAILER_HDR_INNER_L4_ASSOC);
+		}
+	}
+}
+
+bool mlx5e_ipsec_handle_tx_skb(struct net_device *netdev,
+			       struct sk_buff *skb,
+			       struct mlx5e_accel_tx_ipsec_state *ipsec_st)
+{
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	struct xfrm_offload *xo = xfrm_offload(skb);
+	struct mlx5e_ipsec_sa_entry *sa_entry;
+	struct mlx5e_ipsec_metadata *mdata;
+	struct xfrm_state *x;
+	struct sec_path *sp;
 
 	sp = skb_sec_path(skb);
 	if (unlikely(sp->len != 1)) {
@@ -270,15 +346,21 @@ bool mlx5e_ipsec_handle_tx_skb(struct mlx5e_priv *priv,
 			atomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_trailer);
 			goto drop;
 		}
-	mdata = mlx5e_ipsec_add_metadata(skb);
-	if (IS_ERR(mdata)) {
-		atomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_metadata);
-		goto drop;
+
+	if (MLX5_CAP_GEN(priv->mdev, fpga)) {
+		mdata = mlx5e_ipsec_add_metadata(skb);
+		if (IS_ERR(mdata)) {
+			atomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_metadata);
+			goto drop;
+		}
 	}
-	mlx5e_ipsec_set_swp(skb, eseg, x->props.mode, xo);
+
 	sa_entry = (struct mlx5e_ipsec_sa_entry *)x->xso.offload_handle;
 	sa_entry->set_iv_op(skb, x, xo);
-	mlx5e_ipsec_set_metadata(skb, mdata, xo);
+	if (MLX5_CAP_GEN(priv->mdev, fpga))
+		mlx5e_ipsec_set_metadata(skb, mdata, xo);
+
+	mlx5e_ipsec_set_state(priv, skb, x, xo, ipsec_st);
 
 	return true;
 
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.h
index 2a47673da5a4..5d365bd50c51 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.h
@@ -43,6 +43,13 @@
 #define MLX5_IPSEC_METADATA_SYNDROM_MASK     (0x7F)
 #define MLX5_IPSEC_METADATA_HANDLE(metadata) (((metadata) >> 8) & 0xFF)
 
+struct mlx5e_accel_tx_ipsec_state {
+	struct xfrm_offload *xo;
+	struct xfrm_state *x;
+	u32 tailen;
+	u32 plen;
+};
+
 #ifdef CONFIG_MLX5_EN_IPSEC
 
 struct sk_buff *mlx5e_ipsec_handle_rx_skb(struct net_device *netdev,
@@ -56,16 +63,32 @@ void mlx5e_ipsec_set_iv_esn(struct sk_buff *skb, struct xfrm_state *x,
 			    struct xfrm_offload *xo);
 void mlx5e_ipsec_set_iv(struct sk_buff *skb, struct xfrm_state *x,
 			struct xfrm_offload *xo);
-bool mlx5e_ipsec_handle_tx_skb(struct mlx5e_priv *priv,
-			       struct mlx5_wqe_eth_seg *eseg,
-			       struct sk_buff *skb);
+bool mlx5e_ipsec_handle_tx_skb(struct net_device *netdev,
+			       struct sk_buff *skb,
+			       struct mlx5e_accel_tx_ipsec_state *ipsec_st);
+void mlx5e_ipsec_handle_tx_wqe(struct mlx5e_tx_wqe *wqe,
+			       struct mlx5e_accel_tx_ipsec_state *ipsec_st,
+			       struct mlx5_wqe_inline_seg *inlseg);
 void mlx5e_ipsec_offload_handle_rx_skb(struct net_device *netdev,
 				       struct sk_buff *skb,
 				       struct mlx5_cqe64 *cqe);
+static inline unsigned int mlx5e_ipsec_tx_ids_len(struct mlx5e_accel_tx_ipsec_state *ipsec_st)
+{
+	return ipsec_st->tailen;
+}
+
 static inline bool mlx5_ipsec_is_rx_flow(struct mlx5_cqe64 *cqe)
 {
 	return !!(MLX5_IPSEC_METADATA_MARKER_MASK & be32_to_cpu(cqe->ft_metadata));
 }
+
+static inline bool mlx5e_ipsec_is_tx_flow(struct mlx5e_accel_tx_ipsec_state *ipsec_st)
+{
+	return ipsec_st->x;
+}
+
+void mlx5e_ipsec_tx_build_eseg(struct mlx5e_priv *priv, struct sk_buff *skb,
+			       struct mlx5_wqe_eth_seg *eseg);
 #else
 static inline
 void mlx5e_ipsec_offload_handle_rx_skb(struct net_device *netdev,
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
