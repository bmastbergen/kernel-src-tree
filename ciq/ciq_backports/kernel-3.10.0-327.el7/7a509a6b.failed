NVMe: Fix locking on abort handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Keith Busch <keith.busch@intel.com>
commit 7a509a6b07dd5a08d91f8a7e0cccb9a6438ce439
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/7a509a6b.failed

The queues and device need to be locked when messing with them.

	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 7a509a6b07dd5a08d91f8a7e0cccb9a6438ce439)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index 48e1152870d9,cb529e9a82dd..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -1205,84 -1011,119 +1205,141 @@@ int nvme_set_features(struct nvme_dev *
   * Schedule controller reset if the command was already aborted once before and
   * still hasn't been returned to the driver, or if this is the admin queue.
   */
 -static void nvme_abort_req(struct request *req)
 +static void nvme_abort_cmd(int cmdid, struct nvme_queue *nvmeq)
  {
 -	struct nvme_cmd_info *cmd_rq = blk_mq_rq_to_pdu(req);
 -	struct nvme_queue *nvmeq = cmd_rq->nvmeq;
 -	struct nvme_dev *dev = nvmeq->dev;
 -	struct request *abort_req;
 -	struct nvme_cmd_info *abort_cmd;
 +	int a_cmdid;
  	struct nvme_command cmd;
 +	struct nvme_dev *dev = nvmeq->dev;
 +	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
 +	struct nvme_queue *adminq;
  
++<<<<<<< HEAD
 +	if (!nvmeq->qid || info[cmdid].aborted) {
++=======
+ 	if (!nvmeq->qid || cmd_rq->aborted) {
+ 		unsigned long flags;
+ 
+ 		spin_lock_irqsave(&dev_list_lock, flags);
++>>>>>>> 7a509a6b07dd (NVMe: Fix locking on abort handling)
  		if (work_busy(&dev->reset_work))
- 			return;
+ 			goto out;
  		list_del_init(&dev->node);
  		dev_warn(&dev->pci_dev->dev,
 -			"I/O %d QID %d timeout, reset controller\n",
 -							req->tag, nvmeq->qid);
 -		dev->reset_workfn = nvme_reset_failed_dev;
 +			"I/O %d QID %d timeout, reset controller\n", cmdid,
 +								nvmeq->qid);
 +		PREPARE_WORK(&dev->reset_work, nvme_reset_failed_dev);
  		queue_work(nvme_workq, &dev->reset_work);
+  out:
+ 		spin_unlock_irqrestore(&dev_list_lock, flags);
  		return;
  	}
  
  	if (!dev->abort_limit)
  		return;
  
 -	abort_req = blk_mq_alloc_request(dev->admin_q, WRITE, GFP_ATOMIC,
 -									false);
 -	if (IS_ERR(abort_req))
 +	adminq = rcu_dereference(dev->queues[0]);
 +	a_cmdid = alloc_cmdid(adminq, CMD_CTX_ABORT, special_completion,
 +								ADMIN_TIMEOUT);
 +	if (a_cmdid < 0)
  		return;
  
 -	abort_cmd = blk_mq_rq_to_pdu(abort_req);
 -	nvme_set_info(abort_cmd, abort_req, abort_completion);
 -
  	memset(&cmd, 0, sizeof(cmd));
  	cmd.abort.opcode = nvme_admin_abort_cmd;
 -	cmd.abort.cid = req->tag;
 -	cmd.abort.sqid = cpu_to_le16(nvmeq->qid);
 -	cmd.abort.command_id = abort_req->tag;
 +	cmd.abort.cid = cmdid;
 +	cmd.abort.sqid = nvmeq->qid;
 +	cmd.abort.command_id = a_cmdid;
  
  	--dev->abort_limit;
 -	cmd_rq->aborted = 1;
 +	info[cmdid].aborted = 1;
 +	info[cmdid].timeout = jiffies + ADMIN_TIMEOUT;
  
 -	dev_warn(nvmeq->q_dmadev, "Aborting I/O %d QID %d\n", req->tag,
 +	dev_warn(nvmeq->q_dmadev, "Aborting I/O %d QID %d\n", cmdid,
  							nvmeq->qid);
 -	if (nvme_submit_cmd(dev->queues[0], &cmd) < 0) {
 -		dev_warn(nvmeq->q_dmadev,
 -				"Could not abort I/O %d QID %d",
 -				req->tag, nvmeq->qid);
 -		blk_mq_free_request(abort_req);
 -	}
 +	nvme_submit_cmd(adminq, &cmd);
  }
  
 -static void nvme_cancel_queue_ios(struct blk_mq_hw_ctx *hctx,
 -				struct request *req, void *data, bool reserved)
 +/**
 + * nvme_cancel_ios - Cancel outstanding I/Os
 + * @queue: The queue to cancel I/Os on
 + * @timeout: True to only cancel I/Os which have timed out
 + */
 +static void nvme_cancel_ios(struct nvme_queue *nvmeq, bool timeout)
  {
 -	struct nvme_queue *nvmeq = data;
 -	void *ctx;
 -	nvme_completion_fn fn;
 -	struct nvme_cmd_info *cmd;
 -	struct nvme_completion cqe;
 +	int depth = nvmeq->q_depth - 1;
 +	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
 +	unsigned long now = jiffies;
 +	int cmdid;
  
 -	if (!blk_mq_request_started(req))
 -		return;
 +	for_each_set_bit(cmdid, nvmeq->cmdid_data, depth) {
 +		void *ctx;
 +		nvme_completion_fn fn;
 +		static struct nvme_completion cqe = {
 +			.status = cpu_to_le16(NVME_SC_ABORT_REQ << 1),
 +		};
  
++<<<<<<< HEAD
 +		if (timeout && !time_after(now, info[cmdid].timeout))
 +			continue;
 +		if (info[cmdid].ctx == CMD_CTX_CANCELLED)
 +			continue;
 +		if (timeout && info[cmdid].ctx == CMD_CTX_ASYNC)
 +			continue;
 +		if (timeout && nvmeq->dev->initialized) {
 +			nvme_abort_cmd(cmdid, nvmeq);
 +			continue;
 +		}
 +		dev_warn(nvmeq->q_dmadev, "Cancelling I/O %d QID %d\n", cmdid,
 +								nvmeq->qid);
 +		ctx = cancel_cmdid(nvmeq, cmdid, &fn);
 +		fn(nvmeq, ctx, &cqe);
 +	}
++=======
+ 	cmd = blk_mq_rq_to_pdu(req);
+ 
+ 	if (cmd->ctx == CMD_CTX_CANCELLED)
+ 		return;
+ 
+ 	if (blk_queue_dying(req->q))
+ 		cqe.status = cpu_to_le16((NVME_SC_ABORT_REQ | NVME_SC_DNR) << 1);
+ 	else
+ 		cqe.status = cpu_to_le16(NVME_SC_ABORT_REQ << 1);
+ 
+ 
+ 	dev_warn(nvmeq->q_dmadev, "Cancelling I/O %d QID %d\n",
+ 						req->tag, nvmeq->qid);
+ 	ctx = cancel_cmd_info(cmd, &fn);
+ 	fn(nvmeq, ctx, &cqe);
+ }
+ 
+ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
+ {
+ 	struct nvme_cmd_info *cmd = blk_mq_rq_to_pdu(req);
+ 	struct nvme_queue *nvmeq = cmd->nvmeq;
+ 
+ 	/*
+ 	 * The aborted req will be completed on receiving the abort req.
+ 	 * We enable the timer again. If hit twice, it'll cause a device reset,
+ 	 * as the device then is in a faulty state.
+ 	 */
+ 	int ret = BLK_EH_RESET_TIMER;
+ 
+ 	dev_warn(nvmeq->q_dmadev, "Timeout I/O %d QID %d\n", req->tag,
+ 							nvmeq->qid);
+ 
+ 	spin_lock_irq(&nvmeq->q_lock);
+ 	if (!nvmeq->dev->initialized) {
+ 		/*
+ 		 * Force cancelled command frees the request, which requires we
+ 		 * return BLK_EH_NOT_HANDLED.
+ 		 */
+ 		nvme_cancel_queue_ios(nvmeq->hctx, req, nvmeq, reserved);
+ 		ret = BLK_EH_NOT_HANDLED;
+ 	} else
+ 		nvme_abort_req(req);
+ 	spin_unlock_irq(&nvmeq->q_lock);
+ 
+ 	return ret;
++>>>>>>> 7a509a6b07dd (NVMe: Fix locking on abort handling)
  }
  
  static void nvme_free_queue(struct nvme_queue *nvmeq)
* Unmerged path drivers/block/nvme-core.c
