xfs: introduce xfs_buf_submit[_wait]

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit 595bff75dce51e0d6d94877b4b6d11b4747a63fd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/595bff75.failed

There is a lot of cookie-cutter code that looks like:

	if (shutdown)
		handle buffer error
	xfs_buf_iorequest(bp)
	error = xfs_buf_iowait(bp)
	if (error)
		handle buffer error

spread through XFS. There's significant complexity now in
xfs_buf_iorequest() to specifically handle this sort of synchronous
IO pattern, but there's all sorts of nasty surprises in different
error handling code dependent on who owns the buffer references and
the locks.

Pull this pattern into a single helper, where we can hide all the
synchronous IO warts and hence make the error handling for all the
callers much saner. This removes the need for a special extra
reference to protect IO completion processing, as we can now hold a
single reference across dispatch and waiting, simplifying the sync
IO smeantics and error handling.

In doing this, also rename xfs_buf_iorequest to xfs_buf_submit and
make it explicitly handle on asynchronous IO. This forces all users
to be switched specifically to one interface or the other and
removes any ambiguity between how the interfaces are to be used. It
also means that xfs_buf_iowait() goes away.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 595bff75dce51e0d6d94877b4b6d11b4747a63fd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_bmap_util.c
#	fs/xfs/xfs_buf.c
#	fs/xfs/xfs_log_recover.c
#	fs/xfs/xfs_trans_buf.c
diff --cc fs/xfs/xfs_bmap_util.c
index 247d2c73079e,c53cc036d6e7..000000000000
--- a/fs/xfs/xfs_bmap_util.c
+++ b/fs/xfs/xfs_bmap_util.c
@@@ -1157,12 -1157,7 +1157,16 @@@ xfs_zero_remaining_bytes
  		XFS_BUF_READ(bp);
  		XFS_BUF_SET_ADDR(bp, xfs_fsb_to_db(ip, imap.br_startblock));
  
++<<<<<<< HEAD
 +		if (XFS_FORCED_SHUTDOWN(mp)) {
 +			error = XFS_ERROR(EIO);
 +			break;
 +		}
 +		xfs_buf_iorequest(bp);
 +		error = xfs_buf_iowait(bp);
++=======
+ 		error = xfs_buf_submit_wait(bp);
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  		if (error) {
  			xfs_buf_ioerror_alert(bp,
  					"xfs_zero_remaining_bytes(read)");
@@@ -1175,12 -1170,7 +1179,16 @@@
  		XFS_BUF_UNREAD(bp);
  		XFS_BUF_WRITE(bp);
  
++<<<<<<< HEAD
 +		if (XFS_FORCED_SHUTDOWN(mp)) {
 +			error = XFS_ERROR(EIO);
 +			break;
 +		}
 +		xfs_buf_iorequest(bp);
 +		error = xfs_buf_iowait(bp);
++=======
+ 		error = xfs_buf_submit_wait(bp);
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  		if (error) {
  			xfs_buf_ioerror_alert(bp,
  					"xfs_zero_remaining_bytes(write)");
diff --cc fs/xfs/xfs_buf.c
index 84e0417dc6f7,d99ec8335750..000000000000
--- a/fs/xfs/xfs_buf.c
+++ b/fs/xfs/xfs_buf.c
@@@ -1046,40 -1024,26 +1042,40 @@@ xfs_buf_iodone_work
  		(*(bp->b_iodone))(bp);
  	else if (bp->b_flags & XBF_ASYNC)
  		xfs_buf_relse(bp);
++<<<<<<< HEAD
 +	else {
 +		ASSERT(read && bp->b_ops);
++=======
+ 	else
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  		complete(&bp->b_iowait);
- 
- 		/* release the !XBF_ASYNC ref now we are done. */
- 		xfs_buf_rele(bp);
- 	}
  }
  
 -static void
 -xfs_buf_ioend_work(
 -	struct work_struct	*work)
 +void
 +xfs_buf_ioend(
 +	struct xfs_buf	*bp,
 +	int		schedule)
  {
 -	struct xfs_buf		*bp =
 -		container_of(work, xfs_buf_t, b_iodone_work);
 +	bool		read = !!(bp->b_flags & XBF_READ);
  
 -	xfs_buf_ioend(bp);
 -}
 +	trace_xfs_buf_iodone(bp, _RET_IP_);
  
 -void
 -xfs_buf_ioend_async(
 -	struct xfs_buf	*bp)
 -{
 -	INIT_WORK(&bp->b_iodone_work, xfs_buf_ioend_work);
 -	queue_work(xfslogd_workqueue, &bp->b_iodone_work);
 +	if (bp->b_error == 0)
 +		bp->b_flags |= XBF_DONE;
 +
 +	if (bp->b_iodone || (read && bp->b_ops) || (bp->b_flags & XBF_ASYNC)) {
 +		if (schedule) {
 +			INIT_WORK(&bp->b_iodone_work, xfs_buf_iodone_work);
 +			queue_work(bp->b_target->bt_mount->m_buf_workqueue,
 +				   &bp->b_iodone_work);
 +		} else {
 +			xfs_buf_iodone_work(&bp->b_iodone_work);
 +		}
 +	} else {
 +		bp->b_flags &= ~(XBF_READ | XBF_WRITE | XBF_READ_AHEAD);
 +		complete(&bp->b_iowait);
 +		xfs_buf_rele(bp);
 +	}
  }
  
  void
@@@ -1183,24 -1075,10 +1179,28 @@@ xfs_bwrite
  	ASSERT(xfs_buf_islocked(bp));
  
  	bp->b_flags |= XBF_WRITE;
 -	bp->b_flags &= ~(XBF_ASYNC | XBF_READ | _XBF_DELWRI_Q |
 -			 XBF_WRITE_FAIL | XBF_DONE);
 +	bp->b_flags &= ~(XBF_ASYNC | XBF_READ | _XBF_DELWRI_Q | XBF_WRITE_FAIL);
 +
++<<<<<<< HEAD
 +	if (XFS_FORCED_SHUTDOWN(bp->b_target->bt_mount)) {
 +		trace_xfs_bdstrat_shut(bp, _RET_IP_);
 +
 +		/*
 +		 * Metadata write that didn't get logged but written anyway.
 +		 * These aren't associated with a transaction, and can be
 +		 * ignored.
 +		 */
 +		if (!bp->b_iodone)
 +			return xfs_bioerror_relse(bp);
 +		return xfs_bioerror(bp);
 +	}
  
 +	xfs_buf_iorequest(bp);
 +
 +	error = xfs_buf_iowait(bp);
++=======
+ 	error = xfs_buf_submit_wait(bp);
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  	if (error) {
  		xfs_force_shutdown(bp->b_target->bt_mount,
  				   SHUTDOWN_META_IO_ERROR);
@@@ -1313,10 -1187,10 +1313,10 @@@ next_chunk
  	} else {
  		/*
  		 * This is guaranteed not to be the last io reference count
- 		 * because the caller (xfs_buf_iorequest) holds a count itself.
+ 		 * because the caller (xfs_buf_submit) holds a count itself.
  		 */
  		atomic_dec(&bp->b_io_remaining);
 -		xfs_buf_ioerror(bp, -EIO);
 +		xfs_buf_ioerror(bp, EIO);
  		bio_put(bio);
  	}
  
@@@ -1414,26 -1304,18 +1430,15 @@@ xfs_buf_submit
  	if (bp->b_flags & XBF_WRITE)
  		xfs_buf_wait_unpin(bp);
  
 -	/* clear the internal error state to avoid spurious errors */
 -	bp->b_io_error = 0;
 -
  	/*
- 	 * Take references to the buffer. For XBF_ASYNC buffers, holding a
- 	 * reference for as long as submission takes is all that is necessary
- 	 * here. The IO inherits the lock and hold count from the submitter,
- 	 * and these are release during IO completion processing. Taking a hold
- 	 * over submission ensures that the buffer is not freed until we have
- 	 * completed all processing, regardless of when IO errors occur or are
- 	 * reported.
- 	 *
- 	 * However, for synchronous IO, the IO does not inherit the submitters
- 	 * reference count, nor the buffer lock. Hence we need to take an extra
- 	 * reference to the buffer for the for the IO context so that we can
- 	 * guarantee the buffer is not freed until all IO completion processing
- 	 * is done. Otherwise the caller can drop their reference while the IO
- 	 * is still in progress and hence trigger a use-after-free situation.
+ 	 * The caller's reference is released during I/O completion.
+ 	 * This occurs some time after the last b_io_remaining reference is
+ 	 * released, so after we drop our Io reference we have to have some
+ 	 * other reference to ensure the buffer doesn't go away from underneath
+ 	 * us. Take a direct reference to ensure we have safe access to the
+ 	 * buffer until we are finished with it.
  	 */
  	xfs_buf_hold(bp);
- 	if (!(bp->b_flags & XBF_ASYNC))
- 		xfs_buf_hold(bp);
- 
  
  	/*
  	 * Set the count to 1 initially, this will stop an I/O completion
@@@ -1444,19 -1326,19 +1449,33 @@@
  	_xfs_buf_ioapply(bp);
  
  	/*
++<<<<<<< HEAD
 +	 * If _xfs_buf_ioapply failed or we are doing synchronous IO that
 +	 * completes extremely quickly, we can get back here with only the IO
 +	 * reference we took above. _xfs_buf_ioend will drop it to zero. Run
 +	 * completion processing synchronously so that we don't return to the
 +	 * caller with completion still pending. This avoids unnecessary context
 +	 * switches associated with the end_io workqueue.
 +	 */
 +	if (bp->b_error || !(bp->b_flags & XBF_ASYNC))
 +		_xfs_buf_ioend(bp, 0);
 +	else
 +		_xfs_buf_ioend(bp, 1);
++=======
+ 	 * If _xfs_buf_ioapply failed, we can get back here with only the IO
+ 	 * reference we took above. If we drop it to zero, run completion so
+ 	 * that we don't return to the caller with completion still pending.
+ 	 */
+ 	if (atomic_dec_and_test(&bp->b_io_remaining) == 1) {
+ 		if (bp->b_error)
+ 			xfs_buf_ioend(bp);
+ 		else
+ 			xfs_buf_ioend_async(bp);
+ 	}
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  
  	xfs_buf_rele(bp);
+ 	/* Note: it is not safe to reference bp now we've dropped our ref */
  }
  
  /*
@@@ -1836,16 -1807,7 +1899,20 @@@ __xfs_buf_delwri_submit
  		else
  			list_del_init(&bp->b_list);
  
++<<<<<<< HEAD
 +		if (XFS_FORCED_SHUTDOWN(bp->b_target->bt_mount)) {
 +			trace_xfs_bdstrat_shut(bp, _RET_IP_);
 +
 +			if (!bp->b_iodone)
 +				xfs_bioerror_relse(bp);
 +			else
 +				xfs_bioerror(bp);
 +			continue;
 +		}
 +		xfs_buf_iorequest(bp);
++=======
+ 		xfs_buf_submit(bp);
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  	}
  	blk_finish_plug(&plug);
  
diff --cc fs/xfs/xfs_log_recover.c
index 7b97b53b2b95,980e2968b907..000000000000
--- a/fs/xfs/xfs_log_recover.c
+++ b/fs/xfs/xfs_log_recover.c
@@@ -194,12 -193,8 +194,17 @@@ xlog_bread_noalign
  	bp->b_io_length = nbblks;
  	bp->b_error = 0;
  
++<<<<<<< HEAD
 +	if (XFS_FORCED_SHUTDOWN(log->l_mp))
 +		return XFS_ERROR(EIO);
 +
 +	xfs_buf_iorequest(bp);
 +	error = xfs_buf_iowait(bp);
 +	if (error)
++=======
+ 	error = xfs_buf_submit_wait(bp);
+ 	if (error && !XFS_FORCED_SHUTDOWN(log->l_mp))
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  		xfs_buf_ioerror_alert(bp, __func__);
  	return error;
  }
@@@ -379,12 -374,14 +384,14 @@@ xlog_recover_iodone
  		 * We're not going to bother about retrying
  		 * this during recovery. One strike!
  		 */
- 		xfs_buf_ioerror_alert(bp, __func__);
- 		xfs_force_shutdown(bp->b_target->bt_mount,
- 					SHUTDOWN_META_IO_ERROR);
+ 		if (!XFS_FORCED_SHUTDOWN(bp->b_target->bt_mount)) {
+ 			xfs_buf_ioerror_alert(bp, __func__);
+ 			xfs_force_shutdown(bp->b_target->bt_mount,
+ 						SHUTDOWN_META_IO_ERROR);
+ 		}
  	}
  	bp->b_iodone = NULL;
 -	xfs_buf_ioend(bp);
 +	xfs_buf_ioend(bp, 0);
  }
  
  /*
@@@ -4400,16 -4425,12 +4407,22 @@@ xlog_do_recover
  	XFS_BUF_UNASYNC(bp);
  	bp->b_ops = &xfs_sb_buf_ops;
  
++<<<<<<< HEAD
 +	if (XFS_FORCED_SHUTDOWN(log->l_mp)) {
 +		xfs_buf_relse(bp);
 +		return XFS_ERROR(EIO);
 +	}
 +
 +	xfs_buf_iorequest(bp);
 +	error = xfs_buf_iowait(bp);
++=======
+ 	error = xfs_buf_submit_wait(bp);
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  	if (error) {
- 		xfs_buf_ioerror_alert(bp, __func__);
- 		ASSERT(0);
+ 		if (!XFS_FORCED_SHUTDOWN(log->l_mp)) {
+ 			xfs_buf_ioerror_alert(bp, __func__);
+ 			ASSERT(0);
+ 		}
  		xfs_buf_relse(bp);
  		return error;
  	}
diff --cc fs/xfs/xfs_trans_buf.c
index fe41e8efbe96,e2b2216b1635..000000000000
--- a/fs/xfs/xfs_trans_buf.c
+++ b/fs/xfs/xfs_trans_buf.c
@@@ -318,20 -318,10 +318,25 @@@ xfs_trans_read_buf_map
  			XFS_BUF_READ(bp);
  			bp->b_ops = ops;
  
++<<<<<<< HEAD
 +			/*
 +			 * XXX(hch): clean up the error handling here to be less
 +			 * of a mess..
 +			 */
 +			if (XFS_FORCED_SHUTDOWN(mp)) {
 +				trace_xfs_bdstrat_shut(bp, _RET_IP_);
 +				xfs_bioerror_relse(bp);
 +			} else {
 +				xfs_buf_iorequest(bp);
 +			}
 +
 +			error = xfs_buf_iowait(bp);
++=======
+ 			error = xfs_buf_submit_wait(bp);
++>>>>>>> 595bff75dce5 (xfs: introduce xfs_buf_submit[_wait])
  			if (error) {
- 				xfs_buf_ioerror_alert(bp, __func__);
+ 				if (!XFS_FORCED_SHUTDOWN(mp))
+ 					xfs_buf_ioerror_alert(bp, __func__);
  				xfs_buf_relse(bp);
  				/*
  				 * We can gracefully recover from most read
* Unmerged path fs/xfs/xfs_bmap_util.c
* Unmerged path fs/xfs/xfs_buf.c
diff --git a/fs/xfs/xfs_buf.h b/fs/xfs/xfs_buf.h
index 75e98ced9b68..0d18f6b21687 100644
--- a/fs/xfs/xfs_buf.h
+++ b/fs/xfs/xfs_buf.h
@@ -286,8 +286,8 @@ extern int xfs_bwrite(struct xfs_buf *bp);
 extern void xfs_buf_ioend(xfs_buf_t *,	int);
 extern void xfs_buf_ioerror(xfs_buf_t *, int);
 extern void xfs_buf_ioerror_alert(struct xfs_buf *, const char *func);
-extern void xfs_buf_iorequest(xfs_buf_t *);
-extern int xfs_buf_iowait(xfs_buf_t *);
+extern void xfs_buf_submit(struct xfs_buf *bp);
+extern int xfs_buf_submit_wait(struct xfs_buf *bp);
 extern void xfs_buf_iomove(xfs_buf_t *, size_t, size_t, void *,
 				xfs_buf_rw_t);
 #define xfs_buf_zero(bp, off, len) \
diff --git a/fs/xfs/xfs_buf_item.c b/fs/xfs/xfs_buf_item.c
index fc73b0d1e302..7c40e91a80cc 100644
--- a/fs/xfs/xfs_buf_item.c
+++ b/fs/xfs/xfs_buf_item.c
@@ -1081,7 +1081,7 @@ xfs_buf_iodone_callbacks(
 	 * a way to shut the filesystem down if the writes keep failing.
 	 *
 	 * In practice we'll shut the filesystem down soon as non-transient
-	 * erorrs tend to affect the whole device and a failing log write
+	 * errors tend to affect the whole device and a failing log write
 	 * will make us give up.  But we really ought to do better here.
 	 */
 	if (XFS_BUF_ISASYNC(bp)) {
@@ -1094,7 +1094,7 @@ xfs_buf_iodone_callbacks(
 		if (!(bp->b_flags & (XBF_STALE|XBF_WRITE_FAIL))) {
 			bp->b_flags |= XBF_WRITE | XBF_ASYNC |
 				       XBF_DONE | XBF_WRITE_FAIL;
-			xfs_buf_iorequest(bp);
+			xfs_buf_submit(bp);
 		} else {
 			xfs_buf_relse(bp);
 		}
diff --git a/fs/xfs/xfs_log.c b/fs/xfs/xfs_log.c
index b7fb382e491a..daa4354996b3 100644
--- a/fs/xfs/xfs_log.c
+++ b/fs/xfs/xfs_log.c
@@ -1688,7 +1688,7 @@ xlog_bdstrat(
 		return 0;
 	}
 
-	xfs_buf_iorequest(bp);
+	xfs_buf_submit(bp);
 	return 0;
 }
 
* Unmerged path fs/xfs/xfs_log_recover.c
diff --git a/fs/xfs/xfs_trace.h b/fs/xfs/xfs_trace.h
index 27fdd8bb499c..78b8a73ff346 100644
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@ -349,7 +349,8 @@ DEFINE_BUF_EVENT(xfs_buf_free);
 DEFINE_BUF_EVENT(xfs_buf_hold);
 DEFINE_BUF_EVENT(xfs_buf_rele);
 DEFINE_BUF_EVENT(xfs_buf_iodone);
-DEFINE_BUF_EVENT(xfs_buf_iorequest);
+DEFINE_BUF_EVENT(xfs_buf_submit);
+DEFINE_BUF_EVENT(xfs_buf_submit_wait);
 DEFINE_BUF_EVENT(xfs_buf_bawrite);
 DEFINE_BUF_EVENT(xfs_buf_lock);
 DEFINE_BUF_EVENT(xfs_buf_lock_done);
* Unmerged path fs/xfs/xfs_trans_buf.c
