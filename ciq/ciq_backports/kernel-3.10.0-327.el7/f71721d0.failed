xfs: writeback and inval. file range to be shifted by collapse

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Brian Foster <bfoster@redhat.com>
commit f71721d061e872a39b2680d13f309c1eb6893438
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/f71721d0.failed

The collapse range operation currently writes the entire file before
starting the collapse to avoid changes in the in-core extent list due to
writeback causing the extent count to change. Now that collapse range is
fsb based rather than extent index based it can sustain changes in the
extent list during the shift sequence without disruption.

Modify xfs_collapse_file_space() to writeback and invalidate pages
associated with the range of the file to be shifted.
xfs_free_file_space() currently has similar behavior, but the space free
need only affect the region of the file that is freed and this could
change in the future.

Also update the comments to reflect the current implementation. We
retain the eofblocks trim permanently as a best option for dealing with
delalloc extents. We don't shift delalloc extents because this scenario
only occurs with post-eof preallocation (since data must be flushed such
that the cache can be invalidated and data can be shifted). That means
said space must also be initialized before being shifted into the
accessible region of the file only to be immediately truncated off as
the last part of the collapse. In other words, the eofblocks trim will
happen anyways, we just run it first to ensure the file remains in a
consistent state throughout the collapse.

Finally, detect and fail explicitly in the event of a delalloc extent
during the extent shift. The implementation does not support delalloc
extents and the caller is expected to prevent this scenario in advance
as is done by collapse.

	Signed-off-by: Brian Foster <bfoster@redhat.com>
	Reviewed-by: Dave Chinner <dchinner@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit f71721d061e872a39b2680d13f309c1eb6893438)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/libxfs/xfs_bmap.c
#	fs/xfs/xfs_bmap_util.c
diff --cc fs/xfs/libxfs/xfs_bmap.c
index b6f15ccf3239,79c981984dca..000000000000
--- a/fs/xfs/libxfs/xfs_bmap.c
+++ b/fs/xfs/libxfs/xfs_bmap.c
@@@ -5402,3 -5402,320 +5402,323 @@@ error0
  	}
  	return error;
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Determine whether an extent shift can be accomplished by a merge with the
+  * extent that precedes the target hole of the shift.
+  */
+ STATIC bool
+ xfs_bmse_can_merge(
+ 	struct xfs_bmbt_irec	*left,	/* preceding extent */
+ 	struct xfs_bmbt_irec	*got,	/* current extent to shift */
+ 	xfs_fileoff_t		shift)	/* shift fsb */
+ {
+ 	xfs_fileoff_t		startoff;
+ 
+ 	startoff = got->br_startoff - shift;
+ 
+ 	/*
+ 	 * The extent, once shifted, must be adjacent in-file and on-disk with
+ 	 * the preceding extent.
+ 	 */
+ 	if ((left->br_startoff + left->br_blockcount != startoff) ||
+ 	    (left->br_startblock + left->br_blockcount != got->br_startblock) ||
+ 	    (left->br_state != got->br_state) ||
+ 	    (left->br_blockcount + got->br_blockcount > MAXEXTLEN))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * A bmap extent shift adjusts the file offset of an extent to fill a preceding
+  * hole in the file. If an extent shift would result in the extent being fully
+  * adjacent to the extent that currently precedes the hole, we can merge with
+  * the preceding extent rather than do the shift.
+  *
+  * This function assumes the caller has verified a shift-by-merge is possible
+  * with the provided extents via xfs_bmse_can_merge().
+  */
+ STATIC int
+ xfs_bmse_merge(
+ 	struct xfs_inode		*ip,
+ 	int				whichfork,
+ 	xfs_fileoff_t			shift,		/* shift fsb */
+ 	int				current_ext,	/* idx of gotp */
+ 	struct xfs_bmbt_rec_host	*gotp,		/* extent to shift */
+ 	struct xfs_bmbt_rec_host	*leftp,		/* preceding extent */
+ 	struct xfs_btree_cur		*cur,
+ 	int				*logflags)	/* output */
+ {
+ 	struct xfs_ifork		*ifp;
+ 	struct xfs_bmbt_irec		got;
+ 	struct xfs_bmbt_irec		left;
+ 	xfs_filblks_t			blockcount;
+ 	int				error, i;
+ 
+ 	ifp = XFS_IFORK_PTR(ip, whichfork);
+ 	xfs_bmbt_get_all(gotp, &got);
+ 	xfs_bmbt_get_all(leftp, &left);
+ 	blockcount = left.br_blockcount + got.br_blockcount;
+ 
+ 	ASSERT(xfs_isilocked(ip, XFS_IOLOCK_EXCL));
+ 	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL));
+ 	ASSERT(xfs_bmse_can_merge(&left, &got, shift));
+ 
+ 	/*
+ 	 * Merge the in-core extents. Note that the host record pointers and
+ 	 * current_ext index are invalid once the extent has been removed via
+ 	 * xfs_iext_remove().
+ 	 */
+ 	xfs_bmbt_set_blockcount(leftp, blockcount);
+ 	xfs_iext_remove(ip, current_ext, 1, 0);
+ 
+ 	/*
+ 	 * Update the on-disk extent count, the btree if necessary and log the
+ 	 * inode.
+ 	 */
+ 	XFS_IFORK_NEXT_SET(ip, whichfork,
+ 			   XFS_IFORK_NEXTENTS(ip, whichfork) - 1);
+ 	*logflags |= XFS_ILOG_CORE;
+ 	if (!cur) {
+ 		*logflags |= XFS_ILOG_DEXT;
+ 		return 0;
+ 	}
+ 
+ 	/* lookup and remove the extent to merge */
+ 	error = xfs_bmbt_lookup_eq(cur, got.br_startoff, got.br_startblock,
+ 				   got.br_blockcount, &i);
+ 	if (error)
+ 		goto out_error;
+ 	XFS_WANT_CORRUPTED_GOTO(i == 1, out_error);
+ 
+ 	error = xfs_btree_delete(cur, &i);
+ 	if (error)
+ 		goto out_error;
+ 	XFS_WANT_CORRUPTED_GOTO(i == 1, out_error);
+ 
+ 	/* lookup and update size of the previous extent */
+ 	error = xfs_bmbt_lookup_eq(cur, left.br_startoff, left.br_startblock,
+ 				   left.br_blockcount, &i);
+ 	if (error)
+ 		goto out_error;
+ 	XFS_WANT_CORRUPTED_GOTO(i == 1, out_error);
+ 
+ 	left.br_blockcount = blockcount;
+ 
+ 	error = xfs_bmbt_update(cur, left.br_startoff, left.br_startblock,
+ 				left.br_blockcount, left.br_state);
+ 	if (error)
+ 		goto out_error;
+ 
+ 	return 0;
+ 
+ out_error:
+ 	return error;
+ }
+ 
+ /*
+  * Shift a single extent.
+  */
+ STATIC int
+ xfs_bmse_shift_one(
+ 	struct xfs_inode		*ip,
+ 	int				whichfork,
+ 	xfs_fileoff_t			offset_shift_fsb,
+ 	int				*current_ext,
+ 	struct xfs_bmbt_rec_host	*gotp,
+ 	struct xfs_btree_cur		*cur,
+ 	int				*logflags)
+ {
+ 	struct xfs_ifork		*ifp;
+ 	xfs_fileoff_t			startoff;
+ 	struct xfs_bmbt_rec_host	*leftp;
+ 	struct xfs_bmbt_irec		got;
+ 	struct xfs_bmbt_irec		left;
+ 	int				error;
+ 	int				i;
+ 
+ 	ifp = XFS_IFORK_PTR(ip, whichfork);
+ 
+ 	xfs_bmbt_get_all(gotp, &got);
+ 	startoff = got.br_startoff - offset_shift_fsb;
+ 
+ 	/* delalloc extents should be prevented by caller */
+ 	XFS_WANT_CORRUPTED_GOTO(!isnullstartblock(got.br_startblock),
+ 				out_error);
+ 
+ 	/*
+ 	 * If this is the first extent in the file, make sure there's enough
+ 	 * room at the start of the file and jump right to the shift as there's
+ 	 * no left extent to merge.
+ 	 */
+ 	if (*current_ext == 0) {
+ 		if (got.br_startoff < offset_shift_fsb)
+ 			return -EINVAL;
+ 		goto shift_extent;
+ 	}
+ 
+ 	/* grab the left extent and check for a large enough hole */
+ 	leftp = xfs_iext_get_ext(ifp, *current_ext - 1);
+ 	xfs_bmbt_get_all(leftp, &left);
+ 
+ 	if (startoff < left.br_startoff + left.br_blockcount)
+ 		return -EINVAL;
+ 
+ 	/* check whether to merge the extent or shift it down */
+ 	if (!xfs_bmse_can_merge(&left, &got, offset_shift_fsb))
+ 		goto shift_extent;
+ 
+ 	return xfs_bmse_merge(ip, whichfork, offset_shift_fsb, *current_ext,
+ 			      gotp, leftp, cur, logflags);
+ 
+ shift_extent:
+ 	/*
+ 	 * Increment the extent index for the next iteration, update the start
+ 	 * offset of the in-core extent and update the btree if applicable.
+ 	 */
+ 	(*current_ext)++;
+ 	xfs_bmbt_set_startoff(gotp, startoff);
+ 	*logflags |= XFS_ILOG_CORE;
+ 	if (!cur) {
+ 		*logflags |= XFS_ILOG_DEXT;
+ 		return 0;
+ 	}
+ 
+ 	error = xfs_bmbt_lookup_eq(cur, got.br_startoff, got.br_startblock,
+ 				   got.br_blockcount, &i);
+ 	if (error)
+ 		return error;
+ 	XFS_WANT_CORRUPTED_GOTO(i == 1, out_error);
+ 
+ 	got.br_startoff = startoff;
+ 	error = xfs_bmbt_update(cur, got.br_startoff, got.br_startblock,
+ 				got.br_blockcount, got.br_state);
+ 	if (error)
+ 		return error;
+ 
+ 	return 0;
+ 
+ out_error:
+ 	return error;
+ }
+ 
+ /*
+  * Shift extent records to the left to cover a hole.
+  *
+  * The maximum number of extents to be shifted in a single operation is
+  * @num_exts. @start_fsb specifies the file offset to start the shift and the
+  * file offset where we've left off is returned in @next_fsb. @offset_shift_fsb
+  * is the length by which each extent is shifted. If there is no hole to shift
+  * the extents into, this will be considered invalid operation and we abort
+  * immediately.
+  */
+ int
+ xfs_bmap_shift_extents(
+ 	struct xfs_trans	*tp,
+ 	struct xfs_inode	*ip,
+ 	xfs_fileoff_t		start_fsb,
+ 	xfs_fileoff_t		offset_shift_fsb,
+ 	int			*done,
+ 	xfs_fileoff_t		*next_fsb,
+ 	xfs_fsblock_t		*firstblock,
+ 	struct xfs_bmap_free	*flist,
+ 	int			num_exts)
+ {
+ 	struct xfs_btree_cur		*cur = NULL;
+ 	struct xfs_bmbt_rec_host	*gotp;
+ 	struct xfs_bmbt_irec            got;
+ 	struct xfs_mount		*mp = ip->i_mount;
+ 	struct xfs_ifork		*ifp;
+ 	xfs_extnum_t			nexts = 0;
+ 	xfs_extnum_t			current_ext;
+ 	int				error = 0;
+ 	int				whichfork = XFS_DATA_FORK;
+ 	int				logflags = 0;
+ 	int				total_extents;
+ 
+ 	if (unlikely(XFS_TEST_ERROR(
+ 	    (XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_EXTENTS &&
+ 	     XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_BTREE),
+ 	     mp, XFS_ERRTAG_BMAPIFORMAT, XFS_RANDOM_BMAPIFORMAT))) {
+ 		XFS_ERROR_REPORT("xfs_bmap_shift_extents",
+ 				 XFS_ERRLEVEL_LOW, mp);
+ 		return -EFSCORRUPTED;
+ 	}
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		return -EIO;
+ 
+ 	ASSERT(xfs_isilocked(ip, XFS_IOLOCK_EXCL));
+ 	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL));
+ 
+ 	ifp = XFS_IFORK_PTR(ip, whichfork);
+ 	if (!(ifp->if_flags & XFS_IFEXTENTS)) {
+ 		/* Read in all the extents */
+ 		error = xfs_iread_extents(tp, ip, whichfork);
+ 		if (error)
+ 			return error;
+ 	}
+ 
+ 	if (ifp->if_flags & XFS_IFBROOT) {
+ 		cur = xfs_bmbt_init_cursor(mp, tp, ip, whichfork);
+ 		cur->bc_private.b.firstblock = *firstblock;
+ 		cur->bc_private.b.flist = flist;
+ 		cur->bc_private.b.flags = 0;
+ 	}
+ 
+ 	/*
+ 	 * Look up the extent index for the fsb where we start shifting. We can
+ 	 * henceforth iterate with current_ext as extent list changes are locked
+ 	 * out via ilock.
+ 	 *
+ 	 * gotp can be null in 2 cases: 1) if there are no extents or 2)
+ 	 * start_fsb lies in a hole beyond which there are no extents. Either
+ 	 * way, we are done.
+ 	 */
+ 	gotp = xfs_iext_bno_to_ext(ifp, start_fsb, &current_ext);
+ 	if (!gotp) {
+ 		*done = 1;
+ 		goto del_cursor;
+ 	}
+ 
+ 	/*
+ 	 * There may be delalloc extents in the data fork before the range we
+ 	 * are collapsing out, so we cannot use the count of real extents here.
+ 	 * Instead we have to calculate it from the incore fork.
+ 	 */
+ 	total_extents = ifp->if_bytes / sizeof(xfs_bmbt_rec_t);
+ 	while (nexts++ < num_exts && current_ext < total_extents) {
+ 		error = xfs_bmse_shift_one(ip, whichfork, offset_shift_fsb,
+ 					&current_ext, gotp, cur, &logflags);
+ 		if (error)
+ 			goto del_cursor;
+ 
+ 		/* update total extent count and grab the next record */
+ 		total_extents = ifp->if_bytes / sizeof(xfs_bmbt_rec_t);
+ 		if (current_ext >= total_extents)
+ 			break;
+ 		gotp = xfs_iext_get_ext(ifp, current_ext);
+ 	}
+ 
+ 	/* Check if we are done */
+ 	if (current_ext == total_extents) {
+ 		*done = 1;
+ 	} else if (next_fsb) {
+ 		xfs_bmbt_get_all(gotp, &got);
+ 		*next_fsb = got.br_startoff;
+ 	}
+ 
+ del_cursor:
+ 	if (cur)
+ 		xfs_btree_del_cursor(cur,
+ 			error ? XFS_BTREE_ERROR : XFS_BTREE_NOERROR);
+ 
+ 	if (logflags)
+ 		xfs_trans_log_inode(tp, ip, logflags);
+ 
+ 	return error;
+ }
++>>>>>>> f71721d061e8 (xfs: writeback and inval. file range to be shifted by collapse)
diff --cc fs/xfs/xfs_bmap_util.c
index e2ea28ff57bf,eae763f4600e..000000000000
--- a/fs/xfs/xfs_bmap_util.c
+++ b/fs/xfs/xfs_bmap_util.c
@@@ -1436,6 -1435,126 +1436,129 @@@ out
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * xfs_collapse_file_space()
+  *	This routine frees disk space and shift extent for the given file.
+  *	The first thing we do is to free data blocks in the specified range
+  *	by calling xfs_free_file_space(). It would also sync dirty data
+  *	and invalidate page cache over the region on which collapse range
+  *	is working. And Shift extent records to the left to cover a hole.
+  * RETURNS:
+  *	0 on success
+  *	errno on error
+  *
+  */
+ int
+ xfs_collapse_file_space(
+ 	struct xfs_inode	*ip,
+ 	xfs_off_t		offset,
+ 	xfs_off_t		len)
+ {
+ 	int			done = 0;
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_trans	*tp;
+ 	int			error;
+ 	struct xfs_bmap_free	free_list;
+ 	xfs_fsblock_t		first_block;
+ 	int			committed;
+ 	xfs_fileoff_t		start_fsb;
+ 	xfs_fileoff_t		next_fsb;
+ 	xfs_fileoff_t		shift_fsb;
+ 
+ 	ASSERT(xfs_isilocked(ip, XFS_IOLOCK_EXCL));
+ 
+ 	trace_xfs_collapse_file_space(ip);
+ 
+ 	next_fsb = XFS_B_TO_FSB(mp, offset + len);
+ 	shift_fsb = XFS_B_TO_FSB(mp, len);
+ 
+ 	error = xfs_free_file_space(ip, offset, len);
+ 	if (error)
+ 		return error;
+ 
+ 	/*
+ 	 * Trim eofblocks to avoid shifting uninitialized post-eof preallocation
+ 	 * into the accessible region of the file.
+ 	 */
+ 	if (xfs_can_free_eofblocks(ip, true)) {
+ 		error = xfs_free_eofblocks(mp, ip, false);
+ 		if (error)
+ 			return error;
+ 	}
+ 
+ 	/*
+ 	 * Writeback and invalidate cache for the remainder of the file as we're
+ 	 * about to shift down every extent from the collapse range to EOF. The
+ 	 * free of the collapse range above might have already done some of
+ 	 * this, but we shouldn't rely on it to do anything outside of the range
+ 	 * that was freed.
+ 	 */
+ 	error = filemap_write_and_wait_range(VFS_I(ip)->i_mapping,
+ 					     offset + len, -1);
+ 	if (error)
+ 		return error;
+ 	error = invalidate_inode_pages2_range(VFS_I(ip)->i_mapping,
+ 					(offset + len) >> PAGE_CACHE_SHIFT, -1);
+ 	if (error)
+ 		return error;
+ 
+ 	while (!error && !done) {
+ 		tp = xfs_trans_alloc(mp, XFS_TRANS_DIOSTRAT);
+ 		/*
+ 		 * We would need to reserve permanent block for transaction.
+ 		 * This will come into picture when after shifting extent into
+ 		 * hole we found that adjacent extents can be merged which
+ 		 * may lead to freeing of a block during record update.
+ 		 */
+ 		error = xfs_trans_reserve(tp, &M_RES(mp)->tr_write,
+ 				XFS_DIOSTRAT_SPACE_RES(mp, 0), 0);
+ 		if (error) {
+ 			xfs_trans_cancel(tp, 0);
+ 			break;
+ 		}
+ 
+ 		xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 		error = xfs_trans_reserve_quota(tp, mp, ip->i_udquot,
+ 				ip->i_gdquot, ip->i_pdquot,
+ 				XFS_DIOSTRAT_SPACE_RES(mp, 0), 0,
+ 				XFS_QMOPT_RES_REGBLKS);
+ 		if (error)
+ 			goto out;
+ 
+ 		xfs_trans_ijoin(tp, ip, 0);
+ 
+ 		xfs_bmap_init(&free_list, &first_block);
+ 
+ 		/*
+ 		 * We are using the write transaction in which max 2 bmbt
+ 		 * updates are allowed
+ 		 */
+ 		start_fsb = next_fsb;
+ 		error = xfs_bmap_shift_extents(tp, ip, start_fsb, shift_fsb,
+ 				&done, &next_fsb, &first_block, &free_list,
+ 				XFS_BMAP_MAX_SHIFT_EXTENTS);
+ 		if (error)
+ 			goto out;
+ 
+ 		error = xfs_bmap_finish(&tp, &free_list, &committed);
+ 		if (error)
+ 			goto out;
+ 
+ 		error = xfs_trans_commit(tp, XFS_TRANS_RELEASE_LOG_RES);
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	}
+ 
+ 	return error;
+ 
+ out:
+ 	xfs_trans_cancel(tp, XFS_TRANS_RELEASE_LOG_RES | XFS_TRANS_ABORT);
+ 	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	return error;
+ }
+ 
+ /*
++>>>>>>> f71721d061e8 (xfs: writeback and inval. file range to be shifted by collapse)
   * We need to check that the format of the data fork in the temporary inode is
   * valid for the target inode before doing the swap. This is not a problem with
   * attr1 because of the fixed fork offset, but attr2 has a dynamically sized
* Unmerged path fs/xfs/libxfs/xfs_bmap.c
* Unmerged path fs/xfs/xfs_bmap_util.c
