powerpc/iommu/powernv: Release replaced TCE

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] iommu/powernv: Release replaced TCE (David Gibson) [1213665]
Rebuild_FUZZ: 89.74%
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit 05c6cfb9dce0d13d37e9d007ee6a4af36f1c0a58
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/05c6cfb9.failed

At the moment writing new TCE value to the IOMMU table fails with EBUSY
if there is a valid entry already. However PAPR specification allows
the guest to write new TCE value without clearing it first.

Another problem this patch is addressing is the use of pool locks for
external IOMMU users such as VFIO. The pool locks are to protect
DMA page allocator rather than entries and since the host kernel does
not control what pages are in use, there is no point in pool locks and
exchange()+put_page(oldtce) is sufficient to avoid possible races.

This adds an exchange() callback to iommu_table_ops which does the same
thing as set() plus it returns replaced TCE and DMA direction so
the caller can release the pages afterwards. The exchange() receives
a physical address unlike set() which receives linear mapping address;
and returns a physical address as the clear() does.

This implements exchange() for P5IOC2/IODA/IODA2. This adds a requirement
for a platform to have exchange() implemented in order to support VFIO.

This replaces iommu_tce_build() and iommu_clear_tce() with
a single iommu_tce_xchg().

This makes sure that TCE permission bits are not set in TCE passed to
IOMMU API as those are to be calculated by platform code from
DMA direction.

This moves SetPageDirty() to the IOMMU code to make it work for both
VFIO ioctl interface in in-kernel TCE acceleration (when it becomes
available later).

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
[aw: for the vfio related changes]
	Acked-by: Alex Williamson <alex.williamson@redhat.com>
	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 05c6cfb9dce0d13d37e9d007ee6a4af36f1c0a58)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/iommu.h
#	arch/powerpc/kernel/iommu.c
#	arch/powerpc/platforms/powernv/pci-ioda.c
#	arch/powerpc/platforms/powernv/pci-p5ioc2.c
#	arch/powerpc/platforms/powernv/pci.c
#	arch/powerpc/platforms/powernv/pci.h
#	drivers/vfio/vfio_iommu_spapr_tce.c
diff --cc arch/powerpc/include/asm/iommu.h
index 2d866433cb3d,4636734604d7..000000000000
--- a/arch/powerpc/include/asm/iommu.h
+++ b/arch/powerpc/include/asm/iommu.h
@@@ -43,6 -44,38 +43,41 @@@
  extern int iommu_is_off;
  extern int iommu_force_on;
  
++<<<<<<< HEAD
++=======
+ struct iommu_table_ops {
+ 	/*
+ 	 * When called with direction==DMA_NONE, it is equal to clear().
+ 	 * uaddr is a linear map address.
+ 	 */
+ 	int (*set)(struct iommu_table *tbl,
+ 			long index, long npages,
+ 			unsigned long uaddr,
+ 			enum dma_data_direction direction,
+ 			struct dma_attrs *attrs);
+ #ifdef CONFIG_IOMMU_API
+ 	/*
+ 	 * Exchanges existing TCE with new TCE plus direction bits;
+ 	 * returns old TCE and DMA direction mask.
+ 	 * @tce is a physical address.
+ 	 */
+ 	int (*exchange)(struct iommu_table *tbl,
+ 			long index,
+ 			unsigned long *hpa,
+ 			enum dma_data_direction *direction);
+ #endif
+ 	void (*clear)(struct iommu_table *tbl,
+ 			long index, long npages);
+ 	/* get() returns a physical address */
+ 	unsigned long (*get)(struct iommu_table *tbl, long index);
+ 	void (*flush)(struct iommu_table *tbl);
+ };
+ 
+ /* These are used by VIO */
+ extern struct iommu_table_ops iommu_table_lpar_multi_ops;
+ extern struct iommu_table_ops iommu_table_pseries_ops;
+ 
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  /*
   * IOMAP_MAX_ORDER defines the largest contiguous block
   * of dma space we can get.  IOMAP_MAX_ORDER = 13
@@@ -114,8 -169,10 +149,10 @@@ extern void iommu_register_group(struc
  extern int iommu_add_device(struct device *dev);
  extern void iommu_del_device(struct device *dev);
  extern int __init tce_iommu_bus_notifier_init(void);
+ extern long iommu_tce_xchg(struct iommu_table *tbl, unsigned long entry,
+ 		unsigned long *hpa, enum dma_data_direction *direction);
  #else
 -static inline void iommu_register_group(struct iommu_table_group *table_group,
 +static inline void iommu_register_group(struct iommu_table *tbl,
  					int pci_domain_number,
  					unsigned long pe_num)
  {
diff --cc arch/powerpc/kernel/iommu.c
index a8ba9f468d1a,a8e3490b54e3..000000000000
--- a/arch/powerpc/kernel/iommu.c
+++ b/arch/powerpc/kernel/iommu.c
@@@ -989,44 -982,16 +986,50 @@@ int iommu_tce_put_param_check(struct io
  }
  EXPORT_SYMBOL_GPL(iommu_tce_put_param_check);
  
- unsigned long iommu_clear_tce(struct iommu_table *tbl, unsigned long entry)
+ long iommu_tce_xchg(struct iommu_table *tbl, unsigned long entry,
+ 		unsigned long *hpa, enum dma_data_direction *direction)
  {
- 	unsigned long oldtce;
- 	struct iommu_pool *pool = get_pool(tbl, entry);
+ 	long ret;
  
- 	spin_lock(&(pool->lock));
+ 	ret = tbl->it_ops->exchange(tbl, entry, hpa, direction);
  
++<<<<<<< HEAD
 +	oldtce = ppc_md.tce_get(tbl, entry);
 +	if (oldtce & (TCE_PCI_WRITE | TCE_PCI_READ))
 +		ppc_md.tce_free(tbl, entry, 1);
 +	else
 +		oldtce = 0;
 +
 +	spin_unlock(&(pool->lock));
 +
 +	return oldtce;
 +}
 +EXPORT_SYMBOL_GPL(iommu_clear_tce);
 +
 +/*
 + * hwaddr is a kernel virtual address here (0xc... bazillion),
 + * tce_build converts it to a physical address.
 + */
 +int iommu_tce_build(struct iommu_table *tbl, unsigned long entry,
 +		unsigned long hwaddr, enum dma_data_direction direction)
 +{
 +	int ret = -EBUSY;
 +	unsigned long oldtce;
 +	struct iommu_pool *pool = get_pool(tbl, entry);
 +
 +	spin_lock(&(pool->lock));
 +
 +	oldtce = ppc_md.tce_get(tbl, entry);
 +	/* Add new entry if it is not busy */
 +	if (!(oldtce & (TCE_PCI_WRITE | TCE_PCI_READ)))
 +		ret = ppc_md.tce_build(tbl, entry, 1, hwaddr, direction, NULL);
 +
 +	spin_unlock(&(pool->lock));
++=======
+ 	if (!ret && ((*direction == DMA_FROM_DEVICE) ||
+ 			(*direction == DMA_BIDIRECTIONAL)))
+ 		SetPageDirty(pfn_to_page(*hpa >> PAGE_SHIFT));
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  
  	/* if (unlikely(ret))
  		pr_err("iommu_tce: %s failed on hwaddr=%lx ioba=%lx kva=%lx ret=%d\n",
@@@ -1039,7 -1004,22 +1042,26 @@@ EXPORT_SYMBOL_GPL(iommu_tce_xchg)
  
  int iommu_take_ownership(struct iommu_table *tbl)
  {
++<<<<<<< HEAD
 +	unsigned long sz = (tbl->it_size + 7) >> 3;
++=======
+ 	unsigned long flags, i, sz = (tbl->it_size + 7) >> 3;
+ 	int ret = 0;
+ 
+ 	/*
+ 	 * VFIO does not control TCE entries allocation and the guest
+ 	 * can write new TCEs on top of existing ones so iommu_tce_build()
+ 	 * must be able to release old pages. This functionality
+ 	 * requires exchange() callback defined so if it is not
+ 	 * implemented, we disallow taking ownership over the table.
+ 	 */
+ 	if (!tbl->it_ops->exchange)
+ 		return -EINVAL;
+ 
+ 	spin_lock_irqsave(&tbl->large_pool.lock, flags);
+ 	for (i = 0; i < tbl->nr_pools; i++)
+ 		spin_lock(&tbl->pools[i].lock);
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  
  	if (tbl->it_offset == 0)
  		clear_bit(0, tbl->it_map);
diff --cc arch/powerpc/platforms/powernv/pci-ioda.c
index 109e90d84e34,ecbc071a143e..000000000000
--- a/arch/powerpc/platforms/powernv/pci-ioda.c
+++ b/arch/powerpc/platforms/powernv/pci-ioda.c
@@@ -628,16 -1724,74 +628,81 @@@ static void pnv_pci_ioda1_tce_invalidat
  	 */
  }
  
++<<<<<<< HEAD
 +static void pnv_pci_ioda2_tce_invalidate(struct pnv_ioda_pe *pe,
 +					 struct iommu_table *tbl,
 +					 __be64 *startp, __be64 *endp)
++=======
+ static int pnv_ioda1_tce_build(struct iommu_table *tbl, long index,
+ 		long npages, unsigned long uaddr,
+ 		enum dma_data_direction direction,
+ 		struct dma_attrs *attrs)
+ {
+ 	int ret = pnv_tce_build(tbl, index, npages, uaddr, direction,
+ 			attrs);
+ 
+ 	if (!ret && (tbl->it_type & TCE_PCI_SWINV_CREATE))
+ 		pnv_pci_ioda1_tce_invalidate(tbl, index, npages, false);
+ 
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_IOMMU_API
+ static int pnv_ioda1_tce_xchg(struct iommu_table *tbl, long index,
+ 		unsigned long *hpa, enum dma_data_direction *direction)
+ {
+ 	long ret = pnv_tce_xchg(tbl, index, hpa, direction);
+ 
+ 	if (!ret && (tbl->it_type &
+ 			(TCE_PCI_SWINV_CREATE | TCE_PCI_SWINV_FREE)))
+ 		pnv_pci_ioda1_tce_invalidate(tbl, index, 1, false);
+ 
+ 	return ret;
+ }
+ #endif
+ 
+ static void pnv_ioda1_tce_free(struct iommu_table *tbl, long index,
+ 		long npages)
+ {
+ 	pnv_tce_free(tbl, index, npages);
+ 
+ 	if (tbl->it_type & TCE_PCI_SWINV_FREE)
+ 		pnv_pci_ioda1_tce_invalidate(tbl, index, npages, false);
+ }
+ 
+ static struct iommu_table_ops pnv_ioda1_iommu_ops = {
+ 	.set = pnv_ioda1_tce_build,
+ #ifdef CONFIG_IOMMU_API
+ 	.exchange = pnv_ioda1_tce_xchg,
+ #endif
+ 	.clear = pnv_ioda1_tce_free,
+ 	.get = pnv_tce_get,
+ };
+ 
+ static inline void pnv_pci_ioda2_tce_invalidate_entire(struct pnv_ioda_pe *pe)
+ {
+ 	/* 01xb - invalidate TCEs that match the specified PE# */
+ 	unsigned long val = (0x4ull << 60) | (pe->pe_number & 0xFF);
+ 	struct pnv_phb *phb = pe->phb;
+ 
+ 	if (!phb->ioda.tce_inval_reg)
+ 		return;
+ 
+ 	mb(); /* Ensure above stores are visible */
+ 	__raw_writeq(cpu_to_be64(val), phb->ioda.tce_inval_reg);
+ }
+ 
+ static void pnv_pci_ioda2_do_tce_invalidate(unsigned pe_number, bool rm,
+ 		__be64 __iomem *invalidate, unsigned shift,
+ 		unsigned long index, unsigned long npages)
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  {
  	unsigned long start, end, inc;
 +	__be64 __iomem *invalidate = (__be64 __iomem *)tbl->it_index;
  
  	/* We'll invalidate DMA address in PE scope */
 -	start = 0x2ull << 60;
 -	start |= (pe_number & 0xFF);
 +	start = 0x2ul << 60;
 +	start |= (pe->pe_number & 0xFF);
  	end = start;
  
  	/* Figure out the start, end and step */
@@@ -654,19 -1809,70 +719,68 @@@
  	}
  }
  
 -static void pnv_pci_ioda2_tce_invalidate(struct iommu_table *tbl,
 -		unsigned long index, unsigned long npages, bool rm)
 +void pnv_pci_ioda_tce_invalidate(struct iommu_table *tbl,
 +				 __be64 *startp, __be64 *endp)
  {
 -	struct iommu_table_group_link *tgl;
 -
 -	list_for_each_entry_rcu(tgl, &tbl->it_group_list, next) {
 -		struct pnv_ioda_pe *pe = container_of(tgl->table_group,
 -				struct pnv_ioda_pe, table_group);
 -		__be64 __iomem *invalidate = rm ?
 -			(__be64 __iomem *)pe->phb->ioda.tce_inval_reg_phys :
 -			pe->phb->ioda.tce_inval_reg;
 -
 -		pnv_pci_ioda2_do_tce_invalidate(pe->pe_number, rm,
 -			invalidate, tbl->it_page_shift,
 -			index, npages);
 -	}
 +	struct pnv_ioda_pe *pe = container_of(tbl, struct pnv_ioda_pe,
 +					      tce32_table);
 +	struct pnv_phb *phb = pe->phb;
 +
 +	if (phb->type == PNV_PHB_IODA1)
 +		pnv_pci_ioda1_tce_invalidate(tbl, startp, endp);
 +	else
 +		pnv_pci_ioda2_tce_invalidate(pe, tbl, startp, endp);
  }
  
++<<<<<<< HEAD
++=======
+ static int pnv_ioda2_tce_build(struct iommu_table *tbl, long index,
+ 		long npages, unsigned long uaddr,
+ 		enum dma_data_direction direction,
+ 		struct dma_attrs *attrs)
+ {
+ 	int ret = pnv_tce_build(tbl, index, npages, uaddr, direction,
+ 			attrs);
+ 
+ 	if (!ret && (tbl->it_type & TCE_PCI_SWINV_CREATE))
+ 		pnv_pci_ioda2_tce_invalidate(tbl, index, npages, false);
+ 
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_IOMMU_API
+ static int pnv_ioda2_tce_xchg(struct iommu_table *tbl, long index,
+ 		unsigned long *hpa, enum dma_data_direction *direction)
+ {
+ 	long ret = pnv_tce_xchg(tbl, index, hpa, direction);
+ 
+ 	if (!ret && (tbl->it_type &
+ 			(TCE_PCI_SWINV_CREATE | TCE_PCI_SWINV_FREE)))
+ 		pnv_pci_ioda2_tce_invalidate(tbl, index, 1, false);
+ 
+ 	return ret;
+ }
+ #endif
+ 
+ static void pnv_ioda2_tce_free(struct iommu_table *tbl, long index,
+ 		long npages)
+ {
+ 	pnv_tce_free(tbl, index, npages);
+ 
+ 	if (tbl->it_type & TCE_PCI_SWINV_FREE)
+ 		pnv_pci_ioda2_tce_invalidate(tbl, index, npages, false);
+ }
+ 
+ static struct iommu_table_ops pnv_ioda2_iommu_ops = {
+ 	.set = pnv_ioda2_tce_build,
+ #ifdef CONFIG_IOMMU_API
+ 	.exchange = pnv_ioda2_tce_xchg,
+ #endif
+ 	.clear = pnv_ioda2_tce_free,
+ 	.get = pnv_tce_get,
+ };
+ 
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  static void pnv_pci_ioda_setup_dma_pe(struct pnv_phb *phb,
  				      struct pnv_ioda_pe *pe, unsigned int base,
  				      unsigned int segs)
diff --cc arch/powerpc/platforms/powernv/pci-p5ioc2.c
index 94ce3481490b,eaec85b63b34..000000000000
--- a/arch/powerpc/platforms/powernv/pci-p5ioc2.c
+++ b/arch/powerpc/platforms/powernv/pci-p5ioc2.c
@@@ -83,6 -83,15 +83,18 @@@ static void pnv_pci_init_p5ioc2_msis(st
  static void pnv_pci_init_p5ioc2_msis(struct pnv_phb *phb) { }
  #endif /* CONFIG_PCI_MSI */
  
++<<<<<<< HEAD
++=======
+ static struct iommu_table_ops pnv_p5ioc2_iommu_ops = {
+ 	.set = pnv_tce_build,
+ #ifdef CONFIG_IOMMU_API
+ 	.exchange = pnv_tce_xchg,
+ #endif
+ 	.clear = pnv_tce_free,
+ 	.get = pnv_tce_get,
+ };
+ 
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  static void pnv_pci_p5ioc2_dma_dev_setup(struct pnv_phb *phb,
  					 struct pci_dev *pdev)
  {
diff --cc arch/powerpc/platforms/powernv/pci.c
index 17649771621c,d465b9c32388..000000000000
--- a/arch/powerpc/platforms/powernv/pci.c
+++ b/arch/powerpc/platforms/powernv/pci.c
@@@ -624,22 -598,113 +624,44 @@@ static int pnv_tce_build(struct iommu_t
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void pnv_tce_free(struct iommu_table *tbl, long index, long npages)
++=======
+ #ifdef CONFIG_IOMMU_API
+ int pnv_tce_xchg(struct iommu_table *tbl, long index,
+ 		unsigned long *hpa, enum dma_data_direction *direction)
+ {
+ 	u64 proto_tce = iommu_direction_to_tce_perm(*direction);
+ 	unsigned long newtce = *hpa | proto_tce, oldtce;
+ 	unsigned long idx = index - tbl->it_offset;
+ 
+ 	BUG_ON(*hpa & ~IOMMU_PAGE_MASK(tbl));
+ 
+ 	oldtce = xchg(pnv_tce(tbl, idx), cpu_to_be64(newtce));
+ 	*hpa = be64_to_cpu(oldtce) & ~(TCE_PCI_READ | TCE_PCI_WRITE);
+ 	*direction = iommu_tce_direction(oldtce);
+ 
+ 	return 0;
+ }
+ #endif
+ 
+ void pnv_tce_free(struct iommu_table *tbl, long index, long npages)
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  {
 -	long i;
 +	__be64 *tcep, *tces;
  
 -	for (i = 0; i < npages; i++) {
 -		unsigned long idx = index - tbl->it_offset + i;
 +	tces = tcep = ((__be64 *)tbl->it_base) + index - tbl->it_offset;
  
 -		*(pnv_tce(tbl, idx)) = cpu_to_be64(0);
 -	}
 -}
 +	while (npages--)
 +		*(tcep++) = cpu_to_be64(0);
  
 -unsigned long pnv_tce_get(struct iommu_table *tbl, long index)
 -{
 -	return *(pnv_tce(tbl, index - tbl->it_offset));
 +	if (tbl->it_type & TCE_PCI_SWINV_FREE)
 +		pnv_pci_ioda_tce_invalidate(tbl, tces, tcep - 1);
  }
  
 -struct iommu_table *pnv_pci_table_alloc(int nid)
 +static unsigned long pnv_tce_get(struct iommu_table *tbl, long index)
  {
 -	struct iommu_table *tbl;
 -
 -	tbl = kzalloc_node(sizeof(struct iommu_table), GFP_KERNEL, nid);
 -	INIT_LIST_HEAD_RCU(&tbl->it_group_list);
 -
 -	return tbl;
 -}
 -
 -long pnv_pci_link_table_and_group(int node, int num,
 -		struct iommu_table *tbl,
 -		struct iommu_table_group *table_group)
 -{
 -	struct iommu_table_group_link *tgl = NULL;
 -
 -	if (WARN_ON(!tbl || !table_group))
 -		return -EINVAL;
 -
 -	tgl = kzalloc_node(sizeof(struct iommu_table_group_link), GFP_KERNEL,
 -			node);
 -	if (!tgl)
 -		return -ENOMEM;
 -
 -	tgl->table_group = table_group;
 -	list_add_rcu(&tgl->next, &tbl->it_group_list);
 -
 -	table_group->tables[num] = tbl;
 -
 -	return 0;
 -}
 -
 -static void pnv_iommu_table_group_link_free(struct rcu_head *head)
 -{
 -	struct iommu_table_group_link *tgl = container_of(head,
 -			struct iommu_table_group_link, rcu);
 -
 -	kfree(tgl);
 -}
 -
 -void pnv_pci_unlink_table_and_group(struct iommu_table *tbl,
 -		struct iommu_table_group *table_group)
 -{
 -	long i;
 -	bool found;
 -	struct iommu_table_group_link *tgl;
 -
 -	if (!tbl || !table_group)
 -		return;
 -
 -	/* Remove link to a group from table's list of attached groups */
 -	found = false;
 -	list_for_each_entry_rcu(tgl, &tbl->it_group_list, next) {
 -		if (tgl->table_group == table_group) {
 -			list_del_rcu(&tgl->next);
 -			call_rcu(&tgl->rcu, pnv_iommu_table_group_link_free);
 -			found = true;
 -			break;
 -		}
 -	}
 -	if (WARN_ON(!found))
 -		return;
 -
 -	/* Clean a pointer to iommu_table in iommu_table_group::tables[] */
 -	found = false;
 -	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
 -		if (table_group->tables[i] == tbl) {
 -			table_group->tables[i] = NULL;
 -			found = true;
 -			break;
 -		}
 -	}
 -	WARN_ON(!found);
 +	return ((u64 *)tbl->it_base)[index - tbl->it_offset];
  }
  
  void pnv_pci_setup_iommu_table(struct iommu_table *tbl,
diff --cc arch/powerpc/platforms/powernv/pci.h
index 6092ce3351f9,8ef2d28aded0..000000000000
--- a/arch/powerpc/platforms/powernv/pci.h
+++ b/arch/powerpc/platforms/powernv/pci.h
@@@ -181,9 -203,13 +181,19 @@@ struct pnv_phb 
  };
  
  extern struct pci_ops pnv_pci_ops;
++<<<<<<< HEAD
 +#ifdef CONFIG_EEH
 +extern struct pnv_eeh_ops ioda_eeh_ops;
 +#endif
++=======
+ extern int pnv_tce_build(struct iommu_table *tbl, long index, long npages,
+ 		unsigned long uaddr, enum dma_data_direction direction,
+ 		struct dma_attrs *attrs);
+ extern void pnv_tce_free(struct iommu_table *tbl, long index, long npages);
+ extern int pnv_tce_xchg(struct iommu_table *tbl, long index,
+ 		unsigned long *hpa, enum dma_data_direction *direction);
+ extern unsigned long pnv_tce_get(struct iommu_table *tbl, long index);
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  
  void pnv_pci_dump_phb_diag_data(struct pci_controller *hose,
  				unsigned char *log_buff);
diff --cc drivers/vfio/vfio_iommu_spapr_tce.c
index e65bc73cc8a8,a9e2d13c03c0..000000000000
--- a/drivers/vfio/vfio_iommu_spapr_tce.c
+++ b/drivers/vfio/vfio_iommu_spapr_tce.c
@@@ -336,8 -382,9 +341,14 @@@ static long tce_iommu_ioctl(void *iommu
  	}
  	case VFIO_IOMMU_MAP_DMA: {
  		struct vfio_iommu_type1_dma_map param;
++<<<<<<< HEAD
 +		struct iommu_table *tbl = container->tbl;
 +		unsigned long tce;
++=======
+ 		struct iommu_table *tbl = NULL;
+ 		long num;
+ 		enum dma_data_direction direction;
++>>>>>>> 05c6cfb9dce0 (powerpc/iommu/powernv: Release replaced TCE)
  
  		if (!container->enabled)
  			return -EPERM;
* Unmerged path arch/powerpc/include/asm/iommu.h
* Unmerged path arch/powerpc/kernel/iommu.c
* Unmerged path arch/powerpc/platforms/powernv/pci-ioda.c
* Unmerged path arch/powerpc/platforms/powernv/pci-p5ioc2.c
* Unmerged path arch/powerpc/platforms/powernv/pci.c
* Unmerged path arch/powerpc/platforms/powernv/pci.h
* Unmerged path drivers/vfio/vfio_iommu_spapr_tce.c
