vfio: powerpc/spapr: Register memory and define IOMMU v2

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit 2157e7b82f3b81f57bd80cd67cef09ef26e5f74c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/2157e7b8.failed

The existing implementation accounts the whole DMA window in
the locked_vm counter. This is going to be worse with multiple
containers and huge DMA windows. Also, real-time accounting would requite
additional tracking of accounted pages due to the page size difference -
IOMMU uses 4K pages and system uses 4K or 64K pages.

Another issue is that actual pages pinning/unpinning happens on every
DMA map/unmap request. This does not affect the performance much now as
we spend way too much time now on switching context between
guest/userspace/host but this will start to matter when we add in-kernel
DMA map/unmap acceleration.

This introduces a new IOMMU type for SPAPR - VFIO_SPAPR_TCE_v2_IOMMU.
New IOMMU deprecates VFIO_IOMMU_ENABLE/VFIO_IOMMU_DISABLE and introduces
2 new ioctls to register/unregister DMA memory -
VFIO_IOMMU_SPAPR_REGISTER_MEMORY and VFIO_IOMMU_SPAPR_UNREGISTER_MEMORY -
which receive user space address and size of a memory region which
needs to be pinned/unpinned and counted in locked_vm.
New IOMMU splits physical pages pinning and TCE table update
into 2 different operations. It requires:
1) guest pages to be registered first
2) consequent map/unmap requests to work only with pre-registered memory.
For the default single window case this means that the entire guest
(instead of 2GB) needs to be pinned before using VFIO.
When a huge DMA window is added, no additional pinning will be
required, otherwise it would be guest RAM + 2GB.

The new memory registration ioctls are not supported by
VFIO_SPAPR_TCE_IOMMU. Dynamic DMA window and in-kernel acceleration
will require memory to be preregistered in order to work.

The accounting is done per the user process.

This advertises v2 SPAPR TCE IOMMU and restricts what the userspace
can do with v1 or v2 IOMMUs.

In order to support memory pre-registration, we need a way to track
the use of every registered memory region and only allow unregistration
if a region is not in use anymore. So we need a way to tell from what
region the just cleared TCE was from.

This adds a userspace view of the TCE table into iommu_table struct.
It contains userspace address, one per TCE entry. The table is only
allocated when the ownership over an IOMMU group is taken which means
it is only used from outside of the powernv code (such as VFIO).

As v2 IOMMU supports IODA2 and pre-IODA2 IOMMUs (which do not support
DDW API), this creates a default DMA window for IODA2 for consistency.

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
[aw: for the vfio related changes]
	Acked-by: Alex Williamson <alex.williamson@redhat.com>
	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 2157e7b82f3b81f57bd80cd67cef09ef26e5f74c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/iommu.h
#	drivers/vfio/vfio_iommu_spapr_tce.c
#	include/uapi/linux/vfio.h
diff --cc arch/powerpc/include/asm/iommu.h
index 2d866433cb3d,f9957eb4c659..000000000000
--- a/arch/powerpc/include/asm/iommu.h
+++ b/arch/powerpc/include/asm/iommu.h
@@@ -74,12 -111,16 +74,23 @@@ struct iommu_table 
  	struct iommu_pool pools[IOMMU_NR_POOLS];
  	unsigned long *it_map;       /* A simple allocation bitmap for now */
  	unsigned long  it_page_shift;/* table iommu page size */
++<<<<<<< HEAD
 +#ifdef CONFIG_IOMMU_API
 +	struct iommu_group *it_group;
 +#endif
 +	void (*set_bypass)(struct iommu_table *tbl, bool enable);
++=======
+ 	struct list_head it_group_list;/* List of iommu_table_group_link */
+ 	unsigned long *it_userspace; /* userspace view of the table */
+ 	struct iommu_table_ops *it_ops;
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  };
  
+ #define IOMMU_TABLE_USERSPACE_ENTRY(tbl, entry) \
+ 		((tbl)->it_userspace ? \
+ 			&((tbl)->it_userspace[(entry) - (tbl)->it_offset]) : \
+ 			NULL)
+ 
  /* Pure 2^n version of get_order */
  static inline __attribute_const__
  int get_iommu_order(unsigned long size, struct iommu_table *tbl)
diff --cc drivers/vfio/vfio_iommu_spapr_tce.c
index e65bc73cc8a8,91a32239bd0a..000000000000
--- a/drivers/vfio/vfio_iommu_spapr_tce.c
+++ b/drivers/vfio/vfio_iommu_spapr_tce.c
@@@ -88,11 -95,84 +95,88 @@@ struct tce_iommu_group 
   */
  struct tce_container {
  	struct mutex lock;
++<<<<<<< HEAD
 +	struct iommu_table *tbl;
++=======
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  	bool enabled;
+ 	bool v2;
  	unsigned long locked_pages;
+ 	struct iommu_table *tables[IOMMU_TABLE_GROUP_MAX_TABLES];
+ 	struct list_head group_list;
  };
  
+ static long tce_iommu_unregister_pages(struct tce_container *container,
+ 		__u64 vaddr, __u64 size)
+ {
+ 	struct mm_iommu_table_group_mem_t *mem;
+ 
+ 	if ((vaddr & ~PAGE_MASK) || (size & ~PAGE_MASK))
+ 		return -EINVAL;
+ 
+ 	mem = mm_iommu_find(vaddr, size >> PAGE_SHIFT);
+ 	if (!mem)
+ 		return -ENOENT;
+ 
+ 	return mm_iommu_put(mem);
+ }
+ 
+ static long tce_iommu_register_pages(struct tce_container *container,
+ 		__u64 vaddr, __u64 size)
+ {
+ 	long ret = 0;
+ 	struct mm_iommu_table_group_mem_t *mem = NULL;
+ 	unsigned long entries = size >> PAGE_SHIFT;
+ 
+ 	if ((vaddr & ~PAGE_MASK) || (size & ~PAGE_MASK) ||
+ 			((vaddr + size) < vaddr))
+ 		return -EINVAL;
+ 
+ 	ret = mm_iommu_get(vaddr, entries, &mem);
+ 	if (ret)
+ 		return ret;
+ 
+ 	container->enabled = true;
+ 
+ 	return 0;
+ }
+ 
+ static long tce_iommu_userspace_view_alloc(struct iommu_table *tbl)
+ {
+ 	unsigned long cb = _ALIGN_UP(sizeof(tbl->it_userspace[0]) *
+ 			tbl->it_size, PAGE_SIZE);
+ 	unsigned long *uas;
+ 	long ret;
+ 
+ 	BUG_ON(tbl->it_userspace);
+ 
+ 	ret = try_increment_locked_vm(cb >> PAGE_SHIFT);
+ 	if (ret)
+ 		return ret;
+ 
+ 	uas = vzalloc(cb);
+ 	if (!uas) {
+ 		decrement_locked_vm(cb >> PAGE_SHIFT);
+ 		return -ENOMEM;
+ 	}
+ 	tbl->it_userspace = uas;
+ 
+ 	return 0;
+ }
+ 
+ static void tce_iommu_userspace_view_free(struct iommu_table *tbl)
+ {
+ 	unsigned long cb = _ALIGN_UP(sizeof(tbl->it_userspace[0]) *
+ 			tbl->it_size, PAGE_SIZE);
+ 
+ 	if (!tbl->it_userspace)
+ 		return;
+ 
+ 	vfree(tbl->it_userspace);
+ 	tbl->it_userspace = NULL;
+ 	decrement_locked_vm(cb >> PAGE_SHIFT);
+ }
+ 
  static bool tce_page_is_contained(struct page *page, unsigned page_shift)
  {
  	/*
@@@ -103,14 -183,40 +187,50 @@@
  	return (PAGE_SHIFT + compound_order(compound_head(page))) >= page_shift;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool tce_groups_attached(struct tce_container *container)
+ {
+ 	return !list_empty(&container->group_list);
+ }
+ 
+ static long tce_iommu_find_table(struct tce_container *container,
+ 		phys_addr_t ioba, struct iommu_table **ptbl)
+ {
+ 	long i;
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		struct iommu_table *tbl = container->tables[i];
+ 
+ 		if (tbl) {
+ 			unsigned long entry = ioba >> tbl->it_page_shift;
+ 			unsigned long start = tbl->it_offset;
+ 			unsigned long end = start + tbl->it_size;
+ 
+ 			if ((start <= entry) && (entry < end)) {
+ 				*ptbl = tbl;
+ 				return i;
+ 			}
+ 		}
+ 	}
+ 
+ 	return -1;
+ }
+ 
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  static int tce_iommu_enable(struct tce_container *container)
  {
  	int ret = 0;
  	unsigned long locked;
++<<<<<<< HEAD
 +	struct iommu_table *tbl = container->tbl;
 +
 +	if (!container->tbl)
 +		return -ENXIO;
++=======
+ 	struct iommu_table_group *table_group;
+ 	struct tce_iommu_group *tcegrp;
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  
  	if (!current->mm)
  		return -ESRCH; /* process exited */
@@@ -142,8 -248,24 +262,24 @@@
  	 * this is that we cannot tell here the amount of RAM used by the guest
  	 * as this information is only available from KVM and VFIO is
  	 * KVM agnostic.
 -	 *
 -	 * So we do not allow enabling a container without a group attached
 -	 * as there is no way to know how much we should increment
 -	 * the locked_vm counter.
  	 */
++<<<<<<< HEAD
 +	locked = (tbl->it_size << tbl->it_page_shift) >> PAGE_SHIFT;
++=======
+ 	if (!tce_groups_attached(container))
+ 		return -ENODEV;
+ 
+ 	tcegrp = list_first_entry(&container->group_list,
+ 			struct tce_iommu_group, next);
+ 	table_group = iommu_group_get_iommudata(tcegrp->grp);
+ 	if (!table_group)
+ 		return -ENODEV;
+ 
+ 	if (!table_group->tce32_size)
+ 		return -EPERM;
+ 
+ 	locked = table_group->tce32_size >> PAGE_SHIFT;
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  	ret = try_increment_locked_vm(locked);
  	if (ret)
  		return ret;
@@@ -189,11 -319,30 +333,37 @@@ static void tce_iommu_free_table(struc
  static void tce_iommu_release(void *iommu_data)
  {
  	struct tce_container *container = iommu_data;
+ 	struct iommu_table_group *table_group;
+ 	struct tce_iommu_group *tcegrp;
+ 	long i;
  
++<<<<<<< HEAD
 +	WARN_ON(container->tbl && !container->tbl->it_group);
 +
 +	if (container->tbl && container->tbl->it_group)
 +		tce_iommu_detach_group(iommu_data, container->tbl->it_group);
++=======
+ 	while (tce_groups_attached(container)) {
+ 		tcegrp = list_first_entry(&container->group_list,
+ 				struct tce_iommu_group, next);
+ 		table_group = iommu_group_get_iommudata(tcegrp->grp);
+ 		tce_iommu_detach_group(iommu_data, tcegrp->grp);
+ 	}
+ 
+ 	/*
+ 	 * If VFIO created a table, it was not disposed
+ 	 * by tce_iommu_detach_group() so do it now.
+ 	 */
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		struct iommu_table *tbl = container->tables[i];
+ 
+ 		if (!tbl)
+ 			continue;
+ 
+ 		tce_iommu_clear(container, tbl, tbl->it_offset, tbl->it_size);
+ 		tce_iommu_free_table(tbl);
+ 	}
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  
  	tce_iommu_disable(container);
  	mutex_destroy(&container->lock);
@@@ -221,14 -404,26 +432,26 @@@ static int tce_iommu_clear(struct tce_c
  		struct iommu_table *tbl,
  		unsigned long entry, unsigned long pages)
  {
 -	unsigned long oldhpa;
 -	long ret;
 -	enum dma_data_direction direction;
 +	unsigned long oldtce;
  
  	for ( ; pages; --pages, ++entry) {
 -		direction = DMA_NONE;
 -		oldhpa = 0;
 -		ret = iommu_tce_xchg(tbl, entry, &oldhpa, &direction);
 -		if (ret)
 +		oldtce = iommu_clear_tce(tbl, entry);
 +		if (!oldtce)
  			continue;
  
++<<<<<<< HEAD
 +		tce_iommu_unuse_page(container, oldtce);
++=======
+ 		if (direction == DMA_NONE)
+ 			continue;
+ 
+ 		if (container->v2) {
+ 			tce_iommu_unuse_page_v2(tbl, entry);
+ 			continue;
+ 		}
+ 
+ 		tce_iommu_unuse_page(container, oldhpa);
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  	}
  
  	return 0;
@@@ -289,6 -489,110 +512,113 @@@ static long tce_iommu_build(struct tce_
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static long tce_iommu_build_v2(struct tce_container *container,
+ 		struct iommu_table *tbl,
+ 		unsigned long entry, unsigned long tce, unsigned long pages,
+ 		enum dma_data_direction direction)
+ {
+ 	long i, ret = 0;
+ 	struct page *page;
+ 	unsigned long hpa;
+ 	enum dma_data_direction dirtmp;
+ 
+ 	for (i = 0; i < pages; ++i) {
+ 		struct mm_iommu_table_group_mem_t *mem = NULL;
+ 		unsigned long *pua = IOMMU_TABLE_USERSPACE_ENTRY(tbl,
+ 				entry + i);
+ 
+ 		ret = tce_iommu_prereg_ua_to_hpa(tce, IOMMU_PAGE_SIZE(tbl),
+ 				&hpa, &mem);
+ 		if (ret)
+ 			break;
+ 
+ 		page = pfn_to_page(hpa >> PAGE_SHIFT);
+ 		if (!tce_page_is_contained(page, tbl->it_page_shift)) {
+ 			ret = -EPERM;
+ 			break;
+ 		}
+ 
+ 		/* Preserve offset within IOMMU page */
+ 		hpa |= tce & IOMMU_PAGE_MASK(tbl) & ~PAGE_MASK;
+ 		dirtmp = direction;
+ 
+ 		/* The registered region is being unregistered */
+ 		if (mm_iommu_mapped_inc(mem))
+ 			break;
+ 
+ 		ret = iommu_tce_xchg(tbl, entry + i, &hpa, &dirtmp);
+ 		if (ret) {
+ 			/* dirtmp cannot be DMA_NONE here */
+ 			tce_iommu_unuse_page_v2(tbl, entry + i);
+ 			pr_err("iommu_tce: %s failed ioba=%lx, tce=%lx, ret=%ld\n",
+ 					__func__, entry << tbl->it_page_shift,
+ 					tce, ret);
+ 			break;
+ 		}
+ 
+ 		if (dirtmp != DMA_NONE)
+ 			tce_iommu_unuse_page_v2(tbl, entry + i);
+ 
+ 		*pua = tce;
+ 
+ 		tce += IOMMU_PAGE_SIZE(tbl);
+ 	}
+ 
+ 	if (ret)
+ 		tce_iommu_clear(container, tbl, entry, i);
+ 
+ 	return ret;
+ }
+ 
+ static long tce_iommu_create_table(struct tce_container *container,
+ 			struct iommu_table_group *table_group,
+ 			int num,
+ 			__u32 page_shift,
+ 			__u64 window_size,
+ 			__u32 levels,
+ 			struct iommu_table **ptbl)
+ {
+ 	long ret, table_size;
+ 
+ 	table_size = table_group->ops->get_table_size(page_shift, window_size,
+ 			levels);
+ 	if (!table_size)
+ 		return -EINVAL;
+ 
+ 	ret = try_increment_locked_vm(table_size >> PAGE_SHIFT);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = table_group->ops->create_table(table_group, num,
+ 			page_shift, window_size, levels, ptbl);
+ 
+ 	WARN_ON(!ret && !(*ptbl)->it_ops->free);
+ 	WARN_ON(!ret && ((*ptbl)->it_allocated_size != table_size));
+ 
+ 	if (!ret && container->v2) {
+ 		ret = tce_iommu_userspace_view_alloc(*ptbl);
+ 		if (ret)
+ 			(*ptbl)->it_ops->free(*ptbl);
+ 	}
+ 
+ 	if (ret)
+ 		decrement_locked_vm(table_size >> PAGE_SHIFT);
+ 
+ 	return ret;
+ }
+ 
+ static void tce_iommu_free_table(struct iommu_table *tbl)
+ {
+ 	unsigned long pages = tbl->it_allocated_size >> PAGE_SHIFT;
+ 
+ 	tce_iommu_userspace_view_free(tbl);
+ 	tbl->it_ops->free(tbl);
+ 	decrement_locked_vm(pages);
+ }
+ 
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  static long tce_iommu_ioctl(void *iommu_data,
  				 unsigned int cmd, unsigned long arg)
  {
@@@ -311,9 -616,17 +642,23 @@@
  
  	case VFIO_IOMMU_SPAPR_TCE_GET_INFO: {
  		struct vfio_iommu_spapr_tce_info info;
++<<<<<<< HEAD
 +		struct iommu_table *tbl = container->tbl;
 +
 +		if (WARN_ON(!tbl))
++=======
+ 		struct tce_iommu_group *tcegrp;
+ 		struct iommu_table_group *table_group;
+ 
+ 		if (!tce_groups_attached(container))
+ 			return -ENXIO;
+ 
+ 		tcegrp = list_first_entry(&container->group_list,
+ 				struct tce_iommu_group, next);
+ 		table_group = iommu_group_get_iommudata(tcegrp->grp);
+ 
+ 		if (!table_group)
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  			return -ENXIO;
  
  		minsz = offsetofend(struct vfio_iommu_spapr_tce_info,
@@@ -374,9 -693,18 +719,24 @@@
  		if (ret)
  			return ret;
  
++<<<<<<< HEAD
 +		ret = tce_iommu_build(container, tbl,
 +				param.iova >> tbl->it_page_shift,
 +				tce, param.size >> tbl->it_page_shift);
++=======
+ 		if (container->v2)
+ 			ret = tce_iommu_build_v2(container, tbl,
+ 					param.iova >> tbl->it_page_shift,
+ 					param.vaddr,
+ 					param.size >> tbl->it_page_shift,
+ 					direction);
+ 		else
+ 			ret = tce_iommu_build(container, tbl,
+ 					param.iova >> tbl->it_page_shift,
+ 					param.vaddr,
+ 					param.size >> tbl->it_page_shift,
+ 					direction);
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  
  		iommu_flush_tce(tbl);
  
@@@ -432,49 -820,210 +850,247 @@@
  		tce_iommu_disable(container);
  		mutex_unlock(&container->lock);
  		return 0;
++<<<<<<< HEAD
 +	case VFIO_EEH_PE_OP:
 +		if (!container->tbl || !container->tbl->it_group)
 +			return -ENODEV;
 +
 +		return vfio_spapr_iommu_eeh_ioctl(container->tbl->it_group,
 +						  cmd, arg);
++=======
+ 
+ 	case VFIO_EEH_PE_OP: {
+ 		struct tce_iommu_group *tcegrp;
+ 
+ 		ret = 0;
+ 		list_for_each_entry(tcegrp, &container->group_list, next) {
+ 			ret = vfio_spapr_iommu_eeh_ioctl(tcegrp->grp,
+ 					cmd, arg);
+ 			if (ret)
+ 				return ret;
+ 		}
+ 		return ret;
+ 	}
+ 
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  	}
  
  	return -ENOTTY;
  }
  
++<<<<<<< HEAD
++=======
+ static void tce_iommu_release_ownership(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		struct iommu_table *tbl = container->tables[i];
+ 
+ 		if (!tbl)
+ 			continue;
+ 
+ 		tce_iommu_clear(container, tbl, tbl->it_offset, tbl->it_size);
+ 		tce_iommu_userspace_view_free(tbl);
+ 		if (tbl->it_map)
+ 			iommu_release_ownership(tbl);
+ 
+ 		container->tables[i] = NULL;
+ 	}
+ }
+ 
+ static int tce_iommu_take_ownership(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	int i, j, rc = 0;
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		struct iommu_table *tbl = table_group->tables[i];
+ 
+ 		if (!tbl || !tbl->it_map)
+ 			continue;
+ 
+ 		rc = tce_iommu_userspace_view_alloc(tbl);
+ 		if (!rc)
+ 			rc = iommu_take_ownership(tbl);
+ 
+ 		if (rc) {
+ 			for (j = 0; j < i; ++j)
+ 				iommu_release_ownership(
+ 						table_group->tables[j]);
+ 
+ 			return rc;
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i)
+ 		container->tables[i] = table_group->tables[i];
+ 
+ 	return 0;
+ }
+ 
+ static void tce_iommu_release_ownership_ddw(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	long i;
+ 
+ 	if (!table_group->ops->unset_window) {
+ 		WARN_ON_ONCE(1);
+ 		return;
+ 	}
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i)
+ 		table_group->ops->unset_window(table_group, i);
+ 
+ 	table_group->ops->release_ownership(table_group);
+ }
+ 
+ static long tce_iommu_take_ownership_ddw(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	long i, ret = 0;
+ 	struct iommu_table *tbl = NULL;
+ 
+ 	if (!table_group->ops->create_table || !table_group->ops->set_window ||
+ 			!table_group->ops->release_ownership) {
+ 		WARN_ON_ONCE(1);
+ 		return -EFAULT;
+ 	}
+ 
+ 	table_group->ops->take_ownership(table_group);
+ 
+ 	/*
+ 	 * If it the first group attached, check if there is
+ 	 * a default DMA window and create one if none as
+ 	 * the userspace expects it to exist.
+ 	 */
+ 	if (!tce_groups_attached(container) && !container->tables[0]) {
+ 		ret = tce_iommu_create_table(container,
+ 				table_group,
+ 				0, /* window number */
+ 				IOMMU_PAGE_SHIFT_4K,
+ 				table_group->tce32_size,
+ 				1, /* default levels */
+ 				&tbl);
+ 		if (ret)
+ 			goto release_exit;
+ 		else
+ 			container->tables[0] = tbl;
+ 	}
+ 
+ 	/* Set all windows to the new group */
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		tbl = container->tables[i];
+ 
+ 		if (!tbl)
+ 			continue;
+ 
+ 		/* Set the default window to a new group */
+ 		ret = table_group->ops->set_window(table_group, i, tbl);
+ 		if (ret)
+ 			goto release_exit;
+ 	}
+ 
+ 	return 0;
+ 
+ release_exit:
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i)
+ 		table_group->ops->unset_window(table_group, i);
+ 
+ 	table_group->ops->release_ownership(table_group);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  static int tce_iommu_attach_group(void *iommu_data,
  		struct iommu_group *iommu_group)
  {
  	int ret;
  	struct tce_container *container = iommu_data;
++<<<<<<< HEAD
 +	struct iommu_table *tbl = iommu_group_get_iommudata(iommu_group);
++=======
+ 	struct iommu_table_group *table_group;
+ 	struct tce_iommu_group *tcegrp = NULL;
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  
 +	BUG_ON(!tbl);
  	mutex_lock(&container->lock);
  
  	/* pr_debug("tce_vfio: Attaching group #%u to iommu %p\n",
  			iommu_group_id(iommu_group), iommu_group); */
++<<<<<<< HEAD
 +	if (container->tbl) {
 +		pr_warn("tce_vfio: Only one group per IOMMU container is allowed, existing id=%d, attaching id=%d\n",
 +				iommu_group_id(container->tbl->it_group),
 +				iommu_group_id(iommu_group));
++=======
+ 	table_group = iommu_group_get_iommudata(iommu_group);
+ 
+ 	if (tce_groups_attached(container) && (!table_group->ops ||
+ 			!table_group->ops->take_ownership ||
+ 			!table_group->ops->release_ownership)) {
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  		ret = -EBUSY;
  		goto unlock_exit;
  	}
  
++<<<<<<< HEAD
 +	if (container->enabled) {
 +		pr_err("tce_vfio: attaching group #%u to enabled container\n",
 +				iommu_group_id(iommu_group));
 +		ret = -EBUSY;
 +		goto unlock_exit;
 +	}
 +
 +	ret = iommu_take_ownership(tbl);
 +	if (!ret)
 +		container->tbl = tbl;
++=======
+ 	/* Check if new group has the same iommu_ops (i.e. compatible) */
+ 	list_for_each_entry(tcegrp, &container->group_list, next) {
+ 		struct iommu_table_group *table_group_tmp;
+ 
+ 		if (tcegrp->grp == iommu_group) {
+ 			pr_warn("tce_vfio: Group %d is already attached\n",
+ 					iommu_group_id(iommu_group));
+ 			ret = -EBUSY;
+ 			goto unlock_exit;
+ 		}
+ 		table_group_tmp = iommu_group_get_iommudata(tcegrp->grp);
+ 		if (table_group_tmp->ops != table_group->ops) {
+ 			pr_warn("tce_vfio: Group %d is incompatible with group %d\n",
+ 					iommu_group_id(iommu_group),
+ 					iommu_group_id(tcegrp->grp));
+ 			ret = -EPERM;
+ 			goto unlock_exit;
+ 		}
+ 	}
+ 
+ 	tcegrp = kzalloc(sizeof(*tcegrp), GFP_KERNEL);
+ 	if (!tcegrp) {
+ 		ret = -ENOMEM;
+ 		goto unlock_exit;
+ 	}
+ 
+ 	if (!table_group->ops || !table_group->ops->take_ownership ||
+ 			!table_group->ops->release_ownership)
+ 		ret = tce_iommu_take_ownership(container, table_group);
+ 	else
+ 		ret = tce_iommu_take_ownership_ddw(container, table_group);
+ 
+ 	if (!ret) {
+ 		tcegrp->grp = iommu_group;
+ 		list_add(&tcegrp->next, &container->group_list);
+ 	}
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  
  unlock_exit:
+ 	if (ret && tcegrp)
+ 		kfree(tcegrp);
+ 
  	mutex_unlock(&container->lock);
  
  	return ret;
@@@ -484,28 -1033,35 +1100,61 @@@ static void tce_iommu_detach_group(voi
  		struct iommu_group *iommu_group)
  {
  	struct tce_container *container = iommu_data;
++<<<<<<< HEAD
 +	struct iommu_table *tbl = iommu_group_get_iommudata(iommu_group);
++=======
+ 	struct iommu_table_group *table_group;
+ 	bool found = false;
+ 	struct tce_iommu_group *tcegrp;
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  
 +	BUG_ON(!tbl);
  	mutex_lock(&container->lock);
++<<<<<<< HEAD
 +	if (tbl != container->tbl) {
 +		pr_warn("tce_vfio: detaching group #%u, expected group is #%u\n",
 +				iommu_group_id(iommu_group),
 +				iommu_group_id(tbl->it_group));
 +		goto unlock_exit;
 +	}
 +
 +	if (container->enabled) {
 +		pr_warn("tce_vfio: detaching group #%u from enabled container, forcing disable\n",
 +				iommu_group_id(tbl->it_group));
 +		tce_iommu_disable(container);
 +	}
 +
 +	/* pr_debug("tce_vfio: detaching group #%u from iommu %p\n",
 +	   iommu_group_id(iommu_group), iommu_group); */
 +	container->tbl = NULL;
 +	tce_iommu_clear(container, tbl, tbl->it_offset, tbl->it_size);
 +	iommu_release_ownership(tbl);
++=======
+ 
+ 	list_for_each_entry(tcegrp, &container->group_list, next) {
+ 		if (tcegrp->grp == iommu_group) {
+ 			found = true;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (!found) {
+ 		pr_warn("tce_vfio: detaching unattached group #%u\n",
+ 				iommu_group_id(iommu_group));
+ 		goto unlock_exit;
+ 	}
+ 
+ 	list_del(&tcegrp->next);
+ 	kfree(tcegrp);
+ 
+ 	table_group = iommu_group_get_iommudata(iommu_group);
+ 	BUG_ON(!table_group);
+ 
+ 	if (!table_group->ops || !table_group->ops->release_ownership)
+ 		tce_iommu_release_ownership(container, table_group);
+ 	else
+ 		tce_iommu_release_ownership_ddw(container, table_group);
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  
  unlock_exit:
  	mutex_unlock(&container->lock);
diff --cc include/uapi/linux/vfio.h
index 871378e92821,fa84391a0d00..000000000000
--- a/include/uapi/linux/vfio.h
+++ b/include/uapi/linux/vfio.h
@@@ -33,6 -33,11 +33,14 @@@
  /* Check if EEH is supported */
  #define VFIO_EEH			5
  
++<<<<<<< HEAD
++=======
+ /* Two-stage IOMMU */
+ #define VFIO_TYPE1_NESTING_IOMMU	6	/* Implies v2 */
+ 
+ #define VFIO_SPAPR_TCE_v2_IOMMU		7
+ 
++>>>>>>> 2157e7b82f3b (vfio: powerpc/spapr: Register memory and define IOMMU v2)
  /*
   * The IOCTL interface is designed for extensibility by embedding the
   * structure length (argsz) and flags into structures passed between
diff --git a/Documentation/vfio.txt b/Documentation/vfio.txt
index 96978eced341..7dcf2b532670 100644
--- a/Documentation/vfio.txt
+++ b/Documentation/vfio.txt
@@ -289,10 +289,12 @@ PPC64 sPAPR implementation note
 
 This implementation has some specifics:
 
-1) Only one IOMMU group per container is supported as an IOMMU group
-represents the minimal entity which isolation can be guaranteed for and
-groups are allocated statically, one per a Partitionable Endpoint (PE)
+1) On older systems (POWER7 with P5IOC2/IODA1) only one IOMMU group per
+container is supported as an IOMMU table is allocated at the boot time,
+one table per a IOMMU group which is a Partitionable Endpoint (PE)
 (PE is often a PCI domain but not always).
+Newer systems (POWER8 with IODA2) have improved hardware design which allows
+to remove this limitation and have multiple IOMMU groups per a VFIO container.
 
 2) The hardware supports so called DMA windows - the PCI address range
 within which DMA transfer is allowed, any attempt to access address space
@@ -427,6 +429,29 @@ The code flow from the example above should be slightly changed:
 
 	....
 
+5) There is v2 of SPAPR TCE IOMMU. It deprecates VFIO_IOMMU_ENABLE/
+VFIO_IOMMU_DISABLE and implements 2 new ioctls:
+VFIO_IOMMU_SPAPR_REGISTER_MEMORY and VFIO_IOMMU_SPAPR_UNREGISTER_MEMORY
+(which are unsupported in v1 IOMMU).
+
+PPC64 paravirtualized guests generate a lot of map/unmap requests,
+and the handling of those includes pinning/unpinning pages and updating
+mm::locked_vm counter to make sure we do not exceed the rlimit.
+The v2 IOMMU splits accounting and pinning into separate operations:
+
+- VFIO_IOMMU_SPAPR_REGISTER_MEMORY/VFIO_IOMMU_SPAPR_UNREGISTER_MEMORY ioctls
+receive a user space address and size of the block to be pinned.
+Bisecting is not supported and VFIO_IOMMU_UNREGISTER_MEMORY is expected to
+be called with the exact address and size used for registering
+the memory block. The userspace is not expected to call these often.
+The ranges are stored in a linked list in a VFIO container.
+
+- VFIO_IOMMU_MAP_DMA/VFIO_IOMMU_UNMAP_DMA ioctls only update the actual
+IOMMU table and do not do pinning; instead these check that the userspace
+address is from pre-registered range.
+
+This separation helps in optimizing DMA for guests.
+
 -------------------------------------------------------------------------------
 
 [1] VFIO was originally an acronym for "Virtual Function I/O" in its
* Unmerged path arch/powerpc/include/asm/iommu.h
* Unmerged path drivers/vfio/vfio_iommu_spapr_tce.c
* Unmerged path include/uapi/linux/vfio.h
