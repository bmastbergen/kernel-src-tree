crypto: nx - respect sg limit bounds when building sg lists for SHA

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [crypto] nx - respect sg limit bounds when building sg lists for SHA (Herbert Xu) [1250733]
Rebuild_FUZZ: 93.65%
commit-author Jan Stancek <jstancek@redhat.com>
commit d3392f41f6d3cd0a034bd0aca47fabea2b47218e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/d3392f41.failed

Commit 000851119e80 changed sha256/512 update functions to
pass more data to nx_build_sg_list(), which ends with
sg list overflows and usually with update functions failing
for data larger than max_sg_len * NX_PAGE_SIZE.

This happens because:
- both "total" and "to_process" are updated, which leads to
  "to_process" getting overflowed for some data lengths
  For example:
    In first iteration "total" is 50, and let's assume "to_process"
    is 30 due to sg limits. At the end of first iteration "total" is
    set to 20. At start of 2nd iteration "to_process" overflows on:
      to_process = total - to_process;
- "in_sg" is not reset to nx_ctx->in_sg after each iteration
- nx_build_sg_list() is hitting overflow because the amount of data
  passed to it would require more than sgmax elements
- as consequence of previous item, data stored in overflowed sg list
  may no longer be aligned to SHA*_BLOCK_SIZE

This patch changes sha256/512 update functions so that "to_process"
respects sg limits and never tries to pass more data to
nx_build_sg_list() to avoid overflows. "to_process" is calculated
as minimum of "total" and sg limits at start of every iteration.

Fixes: 000851119e80 ("crypto: nx - Fix SHA concurrence issue and sg
		      limit bounds")
	Signed-off-by: Jan Stancek <jstancek@redhat.com>
	Cc: stable@vger.kernel.org
	Cc: Leonidas Da Silva Barbosa <leosilva@linux.vnet.ibm.com>
	Cc: Marcelo Henrique Cerri <mhcerri@linux.vnet.ibm.com>
	Cc: Fionnuala Gunter <fin@linux.vnet.ibm.com>
	Cc: "David S. Miller" <davem@davemloft.net>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit d3392f41f6d3cd0a034bd0aca47fabea2b47218e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/nx/nx-sha256.c
#	drivers/crypto/nx/nx-sha512.c
diff --cc drivers/crypto/nx/nx-sha256.c
index 23621da624c3,becb738c897b..000000000000
--- a/drivers/crypto/nx/nx-sha256.c
+++ b/drivers/crypto/nx/nx-sha256.c
@@@ -74,6 -71,7 +74,10 @@@ static int nx_sha256_update(struct shas
  	struct sha256_state *sctx = shash_desc_ctx(desc);
  	struct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(&desc->tfm->base);
  	struct nx_csbcpb *csbcpb = (struct nx_csbcpb *)nx_ctx->csbcpb;
++<<<<<<< HEAD
++=======
+ 	struct nx_sg *out_sg;
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  	u64 to_process = 0, leftover, total;
  	unsigned long irq_flags;
  	int rc = 0;
@@@ -97,38 -96,57 +101,74 @@@
  	NX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;
  	NX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;
  
++<<<<<<< HEAD
++=======
+ 	max_sg_len = min_t(u64, nx_ctx->ap->sglen,
+ 			nx_driver.of.max_sg_len/sizeof(struct nx_sg));
+ 	max_sg_len = min_t(u64, max_sg_len,
+ 			nx_ctx->ap->databytelen/NX_PAGE_SIZE);
+ 
+ 	data_len = SHA256_DIGEST_SIZE;
+ 	out_sg = nx_build_sg_list(nx_ctx->out_sg, (u8 *)sctx->state,
+ 				  &data_len, max_sg_len);
+ 	nx_ctx->op.outlen = (nx_ctx->out_sg - out_sg) * sizeof(struct nx_sg);
+ 
+ 	if (data_len != SHA256_DIGEST_SIZE) {
+ 		rc = -EINVAL;
+ 		goto out;
+ 	}
+ 
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  	do {
- 		/*
- 		 * to_process: the SHA256_BLOCK_SIZE data chunk to process in
- 		 * this update. This value is also restricted by the sg list
- 		 * limits.
- 		 */
- 		to_process = total - to_process;
- 		to_process = to_process & ~(SHA256_BLOCK_SIZE - 1);
+ 		int used_sgs = 0;
+ 		struct nx_sg *in_sg = nx_ctx->in_sg;
  
  		if (buf_len) {
  			data_len = buf_len;
++<<<<<<< HEAD
 +			rc = nx_sha_build_sg_list(nx_ctx, nx_ctx->in_sg,
 +						  &nx_ctx->op.inlen,
 +						  &data_len,
 +						  (u8 *) sctx->buf,
 +						  NX_DS_SHA256);
++=======
+ 			in_sg = nx_build_sg_list(in_sg,
+ 						 (u8 *) sctx->buf,
+ 						 &data_len,
+ 						 max_sg_len);
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  
 -			if (data_len != buf_len) {
 -				rc = -EINVAL;
 +			if (rc || data_len != buf_len)
  				goto out;
++<<<<<<< HEAD
++=======
+ 			}
+ 			used_sgs = in_sg - nx_ctx->in_sg;
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  		}
  
+ 		/* to_process: SHA256_BLOCK_SIZE aligned chunk to be
+ 		 * processed in this iteration. This value is restricted
+ 		 * by sg list limits and number of sgs we already used
+ 		 * for leftover data. (see above)
+ 		 * In ideal case, we could allow NX_PAGE_SIZE * max_sg_len,
+ 		 * but because data may not be aligned, we need to account
+ 		 * for that too. */
+ 		to_process = min_t(u64, total,
+ 			(max_sg_len - 1 - used_sgs) * NX_PAGE_SIZE);
+ 		to_process = to_process & ~(SHA256_BLOCK_SIZE - 1);
+ 
  		data_len = to_process - buf_len;
 -		in_sg = nx_build_sg_list(in_sg, (u8 *) data,
 -					 &data_len, max_sg_len);
 +		rc = nx_sha_build_sg_list(nx_ctx, nx_ctx->in_sg,
 +					  &nx_ctx->op.inlen,
 +					  &data_len,
 +					  (u8 *) data,
 +					  NX_DS_SHA256);
  
 -		nx_ctx->op.inlen = (nx_ctx->in_sg - in_sg) * sizeof(struct nx_sg);
 +		if (rc)
 +			goto out;
  
- 		to_process = (data_len + buf_len);
+ 		to_process = data_len + buf_len;
  		leftover = total - to_process;
  
  		/*
diff --cc drivers/crypto/nx/nx-sha512.c
index b3adf1022673,b6e183d58d73..000000000000
--- a/drivers/crypto/nx/nx-sha512.c
+++ b/drivers/crypto/nx/nx-sha512.c
@@@ -73,6 -71,7 +73,10 @@@ static int nx_sha512_update(struct shas
  	struct sha512_state *sctx = shash_desc_ctx(desc);
  	struct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(&desc->tfm->base);
  	struct nx_csbcpb *csbcpb = (struct nx_csbcpb *)nx_ctx->csbcpb;
++<<<<<<< HEAD
++=======
+ 	struct nx_sg *out_sg;
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  	u64 to_process, leftover = 0, total;
  	unsigned long irq_flags;
  	int rc = 0;
@@@ -96,39 -96,61 +100,73 @@@
  	NX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;
  	NX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;
  
++<<<<<<< HEAD
++=======
+ 	max_sg_len = min_t(u64, nx_ctx->ap->sglen,
+ 			nx_driver.of.max_sg_len/sizeof(struct nx_sg));
+ 	max_sg_len = min_t(u64, max_sg_len,
+ 			nx_ctx->ap->databytelen/NX_PAGE_SIZE);
+ 
+ 	data_len = SHA512_DIGEST_SIZE;
+ 	out_sg = nx_build_sg_list(nx_ctx->out_sg, (u8 *)sctx->state,
+ 				  &data_len, max_sg_len);
+ 	nx_ctx->op.outlen = (nx_ctx->out_sg - out_sg) * sizeof(struct nx_sg);
+ 
+ 	if (data_len != SHA512_DIGEST_SIZE) {
+ 		rc = -EINVAL;
+ 		goto out;
+ 	}
+ 
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  	do {
- 		/*
- 		 * to_process: the SHA512_BLOCK_SIZE data chunk to process in
- 		 * this update. This value is also restricted by the sg list
- 		 * limits.
- 		 */
- 		to_process = total - leftover;
- 		to_process = to_process & ~(SHA512_BLOCK_SIZE - 1);
- 		leftover = total - to_process;
+ 		int used_sgs = 0;
+ 		struct nx_sg *in_sg = nx_ctx->in_sg;
  
  		if (buf_len) {
  			data_len = buf_len;
++<<<<<<< HEAD
 +			rc = nx_sha_build_sg_list(nx_ctx, nx_ctx->in_sg,
 +						  &nx_ctx->op.inlen,
 +						  &data_len,
 +						  (u8 *) sctx->buf,
 +						  NX_DS_SHA512);
++=======
+ 			in_sg = nx_build_sg_list(in_sg,
+ 						 (u8 *) sctx->buf,
+ 						 &data_len, max_sg_len);
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  
 -			if (data_len != buf_len) {
 -				rc = -EINVAL;
 +			if (rc || data_len != buf_len)
  				goto out;
++<<<<<<< HEAD
++=======
+ 			}
+ 			used_sgs = in_sg - nx_ctx->in_sg;
++>>>>>>> d3392f41f6d3 (crypto: nx - respect sg limit bounds when building sg lists for SHA)
  		}
  
+ 		/* to_process: SHA512_BLOCK_SIZE aligned chunk to be
+ 		 * processed in this iteration. This value is restricted
+ 		 * by sg list limits and number of sgs we already used
+ 		 * for leftover data. (see above)
+ 		 * In ideal case, we could allow NX_PAGE_SIZE * max_sg_len,
+ 		 * but because data may not be aligned, we need to account
+ 		 * for that too. */
+ 		to_process = min_t(u64, total,
+ 			(max_sg_len - 1 - used_sgs) * NX_PAGE_SIZE);
+ 		to_process = to_process & ~(SHA512_BLOCK_SIZE - 1);
+ 
  		data_len = to_process - buf_len;
 -		in_sg = nx_build_sg_list(in_sg, (u8 *) data,
 -					 &data_len, max_sg_len);
 -
 -		nx_ctx->op.inlen = (nx_ctx->in_sg - in_sg) * sizeof(struct nx_sg);
 +		rc = nx_sha_build_sg_list(nx_ctx, nx_ctx->in_sg,
 +					  &nx_ctx->op.inlen,
 +					  &data_len,
 +					  (u8 *) data,
 +					  NX_DS_SHA512);
  
 -		if (data_len != (to_process - buf_len)) {
 -			rc = -EINVAL;
 +		if (rc || data_len != (to_process - buf_len))
  			goto out;
 -		}
  
- 		to_process = (data_len + buf_len);
+ 		to_process = data_len + buf_len;
  		leftover = total - to_process;
  
  		/*
* Unmerged path drivers/crypto/nx/nx-sha256.c
* Unmerged path drivers/crypto/nx/nx-sha512.c
