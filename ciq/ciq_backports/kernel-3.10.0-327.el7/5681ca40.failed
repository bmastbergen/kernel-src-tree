xfs: Remove icsb infrastructure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Dave Chinner <dchinner@redhat.com>
commit 5681ca40064fdb3efe477a604d690ab0425708b3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/5681ca40.failed

Now that the in-core superblock infrastructure has been replaced with
generic per-cpu counters, we don't need it anymore. Nuke it from
orbit so we are sure that it won't haunt us again...

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 5681ca40064fdb3efe477a604d690ab0425708b3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_fsops.c
#	fs/xfs/xfs_iomap.c
#	fs/xfs/xfs_mount.c
#	fs/xfs/xfs_mount.h
#	fs/xfs/xfs_super.c
diff --cc fs/xfs/xfs_fsops.c
index 5f4d1f6b208b,16e62edc9dae..000000000000
--- a/fs/xfs/xfs_fsops.c
+++ b/fs/xfs/xfs_fsops.c
@@@ -647,12 -637,13 +647,20 @@@ xfs_fs_counts
  	xfs_mount_t		*mp,
  	xfs_fsop_counts_t	*cnt)
  {
++<<<<<<< HEAD
 +	xfs_icsb_sync_counters(mp, XFS_ICSB_LAZY_COUNT);
++=======
+ 	cnt->allocino = percpu_counter_read_positive(&mp->m_icount);
+ 	cnt->freeino = percpu_counter_read_positive(&mp->m_ifree);
+ 	cnt->freedata = percpu_counter_read_positive(&mp->m_fdblocks) -
+ 							XFS_ALLOC_SET_ASIDE(mp);
+ 
++>>>>>>> 5681ca40064f (xfs: Remove icsb infrastructure)
  	spin_lock(&mp->m_sb_lock);
 +	cnt->freedata = mp->m_sb.sb_fdblocks - XFS_ALLOC_SET_ASIDE(mp);
  	cnt->freertx = mp->m_sb.sb_frextents;
 +	cnt->freeino = mp->m_sb.sb_ifree;
 +	cnt->allocino = mp->m_sb.sb_icount;
  	spin_unlock(&mp->m_sb_lock);
  	return 0;
  }
diff --cc fs/xfs/xfs_iomap.c
index a2c5a821da94,38e633bad8c2..000000000000
--- a/fs/xfs/xfs_iomap.c
+++ b/fs/xfs/xfs_iomap.c
@@@ -465,8 -460,7 +465,12 @@@ xfs_iomap_prealloc_size
  	alloc_blocks = XFS_FILEOFF_MIN(roundup_pow_of_two(MAXEXTLEN),
  				       alloc_blocks);
  
++<<<<<<< HEAD
 +	xfs_icsb_sync_counters(mp, XFS_ICSB_LAZY_COUNT);
 +	freesp = mp->m_sb.sb_fdblocks;
++=======
+ 	freesp = percpu_counter_read_positive(&mp->m_fdblocks);
++>>>>>>> 5681ca40064f (xfs: Remove icsb infrastructure)
  	if (freesp < mp->m_low_space[XFS_LOWSP_5_PCNT]) {
  		shift = 2;
  		if (freesp < mp->m_low_space[XFS_LOWSP_4_PCNT])
diff --cc fs/xfs/xfs_mount.c
index 1f0460bd27b8,05b392e35e35..000000000000
--- a/fs/xfs/xfs_mount.c
+++ b/fs/xfs/xfs_mount.c
@@@ -1472,573 -1451,3 +1457,576 @@@ xfs_dev_is_read_only
  	}
  	return 0;
  }
++<<<<<<< HEAD
 +
 +#ifdef HAVE_PERCPU_SB
 +/*
 + * Per-cpu incore superblock counters
 + *
 + * Simple concept, difficult implementation
 + *
 + * Basically, replace the incore superblock counters with a distributed per cpu
 + * counter for contended fields (e.g.  free block count).
 + *
 + * Difficulties arise in that the incore sb is used for ENOSPC checking, and
 + * hence needs to be accurately read when we are running low on space. Hence
 + * there is a method to enable and disable the per-cpu counters based on how
 + * much "stuff" is available in them.
 + *
 + * Basically, a counter is enabled if there is enough free resource to justify
 + * running a per-cpu fast-path. If the per-cpu counter runs out (i.e. a local
 + * ENOSPC), then we disable the counters to synchronise all callers and
 + * re-distribute the available resources.
 + *
 + * If, once we redistributed the available resources, we still get a failure,
 + * we disable the per-cpu counter and go through the slow path.
 + *
 + * The slow path is the current xfs_mod_incore_sb() function.  This means that
 + * when we disable a per-cpu counter, we need to drain its resources back to
 + * the global superblock. We do this after disabling the counter to prevent
 + * more threads from queueing up on the counter.
 + *
 + * Essentially, this means that we still need a lock in the fast path to enable
 + * synchronisation between the global counters and the per-cpu counters. This
 + * is not a problem because the lock will be local to a CPU almost all the time
 + * and have little contention except when we get to ENOSPC conditions.
 + *
 + * Basically, this lock becomes a barrier that enables us to lock out the fast
 + * path while we do things like enabling and disabling counters and
 + * synchronising the counters.
 + *
 + * Locking rules:
 + *
 + * 	1. m_sb_lock before picking up per-cpu locks
 + * 	2. per-cpu locks always picked up via for_each_online_cpu() order
 + * 	3. accurate counter sync requires m_sb_lock + per cpu locks
 + * 	4. modifying per-cpu counters requires holding per-cpu lock
 + * 	5. modifying global counters requires holding m_sb_lock
 + *	6. enabling or disabling a counter requires holding the m_sb_lock 
 + *	   and _none_ of the per-cpu locks.
 + *
 + * Disabled counters are only ever re-enabled by a balance operation
 + * that results in more free resources per CPU than a given threshold.
 + * To ensure counters don't remain disabled, they are rebalanced when
 + * the global resource goes above a higher threshold (i.e. some hysteresis
 + * is present to prevent thrashing).
 + */
 +
 +#ifdef CONFIG_HOTPLUG_CPU
 +/*
 + * hot-plug CPU notifier support.
 + *
 + * We need a notifier per filesystem as we need to be able to identify
 + * the filesystem to balance the counters out. This is achieved by
 + * having a notifier block embedded in the xfs_mount_t and doing pointer
 + * magic to get the mount pointer from the notifier block address.
 + */
 +STATIC int
 +xfs_icsb_cpu_notify(
 +	struct notifier_block *nfb,
 +	unsigned long action,
 +	void *hcpu)
 +{
 +	xfs_icsb_cnts_t *cntp;
 +	xfs_mount_t	*mp;
 +
 +	mp = (xfs_mount_t *)container_of(nfb, xfs_mount_t, m_icsb_notifier);
 +	cntp = (xfs_icsb_cnts_t *)
 +			per_cpu_ptr(mp->m_sb_cnts, (unsigned long)hcpu);
 +	switch (action) {
 +	case CPU_UP_PREPARE:
 +	case CPU_UP_PREPARE_FROZEN:
 +		/* Easy Case - initialize the area and locks, and
 +		 * then rebalance when online does everything else for us. */
 +		memset(cntp, 0, sizeof(xfs_icsb_cnts_t));
 +		break;
 +	case CPU_ONLINE:
 +	case CPU_ONLINE_FROZEN:
 +		xfs_icsb_lock(mp);
 +		xfs_icsb_balance_counter(mp, XFS_SBS_ICOUNT, 0);
 +		xfs_icsb_balance_counter(mp, XFS_SBS_IFREE, 0);
 +		xfs_icsb_balance_counter(mp, XFS_SBS_FDBLOCKS, 0);
 +		xfs_icsb_unlock(mp);
 +		break;
 +	case CPU_DEAD:
 +	case CPU_DEAD_FROZEN:
 +		/* Disable all the counters, then fold the dead cpu's
 +		 * count into the total on the global superblock and
 +		 * re-enable the counters. */
 +		xfs_icsb_lock(mp);
 +		spin_lock(&mp->m_sb_lock);
 +		xfs_icsb_disable_counter(mp, XFS_SBS_ICOUNT);
 +		xfs_icsb_disable_counter(mp, XFS_SBS_IFREE);
 +		xfs_icsb_disable_counter(mp, XFS_SBS_FDBLOCKS);
 +
 +		mp->m_sb.sb_icount += cntp->icsb_icount;
 +		mp->m_sb.sb_ifree += cntp->icsb_ifree;
 +		mp->m_sb.sb_fdblocks += cntp->icsb_fdblocks;
 +
 +		memset(cntp, 0, sizeof(xfs_icsb_cnts_t));
 +
 +		xfs_icsb_balance_counter_locked(mp, XFS_SBS_ICOUNT, 0);
 +		xfs_icsb_balance_counter_locked(mp, XFS_SBS_IFREE, 0);
 +		xfs_icsb_balance_counter_locked(mp, XFS_SBS_FDBLOCKS, 0);
 +		spin_unlock(&mp->m_sb_lock);
 +		xfs_icsb_unlock(mp);
 +		break;
 +	}
 +
 +	return NOTIFY_OK;
 +}
 +#endif /* CONFIG_HOTPLUG_CPU */
 +
 +int
 +xfs_icsb_init_counters(
 +	xfs_mount_t	*mp)
 +{
 +	xfs_icsb_cnts_t *cntp;
 +	int		i;
 +
 +	mp->m_sb_cnts = alloc_percpu(xfs_icsb_cnts_t);
 +	if (mp->m_sb_cnts == NULL)
 +		return -ENOMEM;
 +
 +	for_each_online_cpu(i) {
 +		cntp = (xfs_icsb_cnts_t *)per_cpu_ptr(mp->m_sb_cnts, i);
 +		memset(cntp, 0, sizeof(xfs_icsb_cnts_t));
 +	}
 +
 +	mutex_init(&mp->m_icsb_mutex);
 +
 +	/*
 +	 * start with all counters disabled so that the
 +	 * initial balance kicks us off correctly
 +	 */
 +	mp->m_icsb_counters = -1;
 +
 +#ifdef CONFIG_HOTPLUG_CPU
 +	mp->m_icsb_notifier.notifier_call = xfs_icsb_cpu_notify;
 +	mp->m_icsb_notifier.priority = 0;
 +	register_hotcpu_notifier(&mp->m_icsb_notifier);
 +#endif /* CONFIG_HOTPLUG_CPU */
 +
 +	return 0;
 +}
 +
 +void
 +xfs_icsb_reinit_counters(
 +	xfs_mount_t	*mp)
 +{
 +	xfs_icsb_lock(mp);
 +	/*
 +	 * start with all counters disabled so that the
 +	 * initial balance kicks us off correctly
 +	 */
 +	mp->m_icsb_counters = -1;
 +	xfs_icsb_balance_counter(mp, XFS_SBS_ICOUNT, 0);
 +	xfs_icsb_balance_counter(mp, XFS_SBS_IFREE, 0);
 +	xfs_icsb_balance_counter(mp, XFS_SBS_FDBLOCKS, 0);
 +	xfs_icsb_unlock(mp);
 +}
 +
 +void
 +xfs_icsb_destroy_counters(
 +	xfs_mount_t	*mp)
 +{
 +	if (mp->m_sb_cnts) {
 +		unregister_hotcpu_notifier(&mp->m_icsb_notifier);
 +		free_percpu(mp->m_sb_cnts);
 +	}
 +	mutex_destroy(&mp->m_icsb_mutex);
 +}
 +
 +STATIC void
 +xfs_icsb_lock_cntr(
 +	xfs_icsb_cnts_t	*icsbp)
 +{
 +	while (test_and_set_bit(XFS_ICSB_FLAG_LOCK, &icsbp->icsb_flags)) {
 +		ndelay(1000);
 +	}
 +}
 +
 +STATIC void
 +xfs_icsb_unlock_cntr(
 +	xfs_icsb_cnts_t	*icsbp)
 +{
 +	clear_bit(XFS_ICSB_FLAG_LOCK, &icsbp->icsb_flags);
 +}
 +
 +
 +STATIC void
 +xfs_icsb_lock_all_counters(
 +	xfs_mount_t	*mp)
 +{
 +	xfs_icsb_cnts_t *cntp;
 +	int		i;
 +
 +	for_each_online_cpu(i) {
 +		cntp = (xfs_icsb_cnts_t *)per_cpu_ptr(mp->m_sb_cnts, i);
 +		xfs_icsb_lock_cntr(cntp);
 +	}
 +}
 +
 +STATIC void
 +xfs_icsb_unlock_all_counters(
 +	xfs_mount_t	*mp)
 +{
 +	xfs_icsb_cnts_t *cntp;
 +	int		i;
 +
 +	for_each_online_cpu(i) {
 +		cntp = (xfs_icsb_cnts_t *)per_cpu_ptr(mp->m_sb_cnts, i);
 +		xfs_icsb_unlock_cntr(cntp);
 +	}
 +}
 +
 +STATIC void
 +xfs_icsb_count(
 +	xfs_mount_t	*mp,
 +	xfs_icsb_cnts_t	*cnt,
 +	int		flags)
 +{
 +	xfs_icsb_cnts_t *cntp;
 +	int		i;
 +
 +	memset(cnt, 0, sizeof(xfs_icsb_cnts_t));
 +
 +	if (!(flags & XFS_ICSB_LAZY_COUNT))
 +		xfs_icsb_lock_all_counters(mp);
 +
 +	for_each_online_cpu(i) {
 +		cntp = (xfs_icsb_cnts_t *)per_cpu_ptr(mp->m_sb_cnts, i);
 +		cnt->icsb_icount += cntp->icsb_icount;
 +		cnt->icsb_ifree += cntp->icsb_ifree;
 +		cnt->icsb_fdblocks += cntp->icsb_fdblocks;
 +	}
 +
 +	if (!(flags & XFS_ICSB_LAZY_COUNT))
 +		xfs_icsb_unlock_all_counters(mp);
 +}
 +
 +STATIC int
 +xfs_icsb_counter_disabled(
 +	xfs_mount_t	*mp,
 +	xfs_sb_field_t	field)
 +{
 +	ASSERT((field >= XFS_SBS_ICOUNT) && (field <= XFS_SBS_FDBLOCKS));
 +	return test_bit(field, &mp->m_icsb_counters);
 +}
 +
 +STATIC void
 +xfs_icsb_disable_counter(
 +	xfs_mount_t	*mp,
 +	xfs_sb_field_t	field)
 +{
 +	xfs_icsb_cnts_t	cnt;
 +
 +	ASSERT((field >= XFS_SBS_ICOUNT) && (field <= XFS_SBS_FDBLOCKS));
 +
 +	/*
 +	 * If we are already disabled, then there is nothing to do
 +	 * here. We check before locking all the counters to avoid
 +	 * the expensive lock operation when being called in the
 +	 * slow path and the counter is already disabled. This is
 +	 * safe because the only time we set or clear this state is under
 +	 * the m_icsb_mutex.
 +	 */
 +	if (xfs_icsb_counter_disabled(mp, field))
 +		return;
 +
 +	xfs_icsb_lock_all_counters(mp);
 +	if (!test_and_set_bit(field, &mp->m_icsb_counters)) {
 +		/* drain back to superblock */
 +
 +		xfs_icsb_count(mp, &cnt, XFS_ICSB_LAZY_COUNT);
 +		switch(field) {
 +		case XFS_SBS_ICOUNT:
 +			mp->m_sb.sb_icount = cnt.icsb_icount;
 +			break;
 +		case XFS_SBS_IFREE:
 +			mp->m_sb.sb_ifree = cnt.icsb_ifree;
 +			break;
 +		case XFS_SBS_FDBLOCKS:
 +			mp->m_sb.sb_fdblocks = cnt.icsb_fdblocks;
 +			break;
 +		default:
 +			BUG();
 +		}
 +	}
 +
 +	xfs_icsb_unlock_all_counters(mp);
 +}
 +
 +STATIC void
 +xfs_icsb_enable_counter(
 +	xfs_mount_t	*mp,
 +	xfs_sb_field_t	field,
 +	uint64_t	count,
 +	uint64_t	resid)
 +{
 +	xfs_icsb_cnts_t	*cntp;
 +	int		i;
 +
 +	ASSERT((field >= XFS_SBS_ICOUNT) && (field <= XFS_SBS_FDBLOCKS));
 +
 +	xfs_icsb_lock_all_counters(mp);
 +	for_each_online_cpu(i) {
 +		cntp = per_cpu_ptr(mp->m_sb_cnts, i);
 +		switch (field) {
 +		case XFS_SBS_ICOUNT:
 +			cntp->icsb_icount = count + resid;
 +			break;
 +		case XFS_SBS_IFREE:
 +			cntp->icsb_ifree = count + resid;
 +			break;
 +		case XFS_SBS_FDBLOCKS:
 +			cntp->icsb_fdblocks = count + resid;
 +			break;
 +		default:
 +			BUG();
 +			break;
 +		}
 +		resid = 0;
 +	}
 +	clear_bit(field, &mp->m_icsb_counters);
 +	xfs_icsb_unlock_all_counters(mp);
 +}
 +
 +void
 +xfs_icsb_sync_counters_locked(
 +	xfs_mount_t	*mp,
 +	int		flags)
 +{
 +	xfs_icsb_cnts_t	cnt;
 +
 +	xfs_icsb_count(mp, &cnt, flags);
 +
 +	if (!xfs_icsb_counter_disabled(mp, XFS_SBS_ICOUNT))
 +		mp->m_sb.sb_icount = cnt.icsb_icount;
 +	if (!xfs_icsb_counter_disabled(mp, XFS_SBS_IFREE))
 +		mp->m_sb.sb_ifree = cnt.icsb_ifree;
 +	if (!xfs_icsb_counter_disabled(mp, XFS_SBS_FDBLOCKS))
 +		mp->m_sb.sb_fdblocks = cnt.icsb_fdblocks;
 +}
 +
 +/*
 + * Accurate update of per-cpu counters to incore superblock
 + */
 +void
 +xfs_icsb_sync_counters(
 +	xfs_mount_t	*mp,
 +	int		flags)
 +{
 +	spin_lock(&mp->m_sb_lock);
 +	xfs_icsb_sync_counters_locked(mp, flags);
 +	spin_unlock(&mp->m_sb_lock);
 +}
 +
 +/*
 + * Balance and enable/disable counters as necessary.
 + *
 + * Thresholds for re-enabling counters are somewhat magic.  inode counts are
 + * chosen to be the same number as single on disk allocation chunk per CPU, and
 + * free blocks is something far enough zero that we aren't going thrash when we
 + * get near ENOSPC. We also need to supply a minimum we require per cpu to
 + * prevent looping endlessly when xfs_alloc_space asks for more than will
 + * be distributed to a single CPU but each CPU has enough blocks to be
 + * reenabled.
 + *
 + * Note that we can be called when counters are already disabled.
 + * xfs_icsb_disable_counter() optimises the counter locking in this case to
 + * prevent locking every per-cpu counter needlessly.
 + */
 +
 +#define XFS_ICSB_INO_CNTR_REENABLE	(uint64_t)64
 +#define XFS_ICSB_FDBLK_CNTR_REENABLE(mp) \
 +		(uint64_t)(512 + XFS_ALLOC_SET_ASIDE(mp))
 +STATIC void
 +xfs_icsb_balance_counter_locked(
 +	xfs_mount_t	*mp,
 +	xfs_sb_field_t  field,
 +	int		min_per_cpu)
 +{
 +	uint64_t	count, resid;
 +	int		weight = num_online_cpus();
 +	uint64_t	min = (uint64_t)min_per_cpu;
 +
 +	/* disable counter and sync counter */
 +	xfs_icsb_disable_counter(mp, field);
 +
 +	/* update counters  - first CPU gets residual*/
 +	switch (field) {
 +	case XFS_SBS_ICOUNT:
 +		count = mp->m_sb.sb_icount;
 +		resid = do_div(count, weight);
 +		if (count < max(min, XFS_ICSB_INO_CNTR_REENABLE))
 +			return;
 +		break;
 +	case XFS_SBS_IFREE:
 +		count = mp->m_sb.sb_ifree;
 +		resid = do_div(count, weight);
 +		if (count < max(min, XFS_ICSB_INO_CNTR_REENABLE))
 +			return;
 +		break;
 +	case XFS_SBS_FDBLOCKS:
 +		count = mp->m_sb.sb_fdblocks;
 +		resid = do_div(count, weight);
 +		if (count < max(min, XFS_ICSB_FDBLK_CNTR_REENABLE(mp)))
 +			return;
 +		break;
 +	default:
 +		BUG();
 +		count = resid = 0;	/* quiet, gcc */
 +		break;
 +	}
 +
 +	xfs_icsb_enable_counter(mp, field, count, resid);
 +}
 +
 +STATIC void
 +xfs_icsb_balance_counter(
 +	xfs_mount_t	*mp,
 +	xfs_sb_field_t  fields,
 +	int		min_per_cpu)
 +{
 +	spin_lock(&mp->m_sb_lock);
 +	xfs_icsb_balance_counter_locked(mp, fields, min_per_cpu);
 +	spin_unlock(&mp->m_sb_lock);
 +}
 +
 +int
 +xfs_icsb_modify_counters(
 +	xfs_mount_t	*mp,
 +	xfs_sb_field_t	field,
 +	int64_t		delta,
 +	int		rsvd)
 +{
 +	xfs_icsb_cnts_t	*icsbp;
 +	long long	lcounter;	/* long counter for 64 bit fields */
 +	int		ret = 0;
 +
 +	might_sleep();
 +again:
 +	preempt_disable();
 +	icsbp = this_cpu_ptr(mp->m_sb_cnts);
 +
 +	/*
 +	 * if the counter is disabled, go to slow path
 +	 */
 +	if (unlikely(xfs_icsb_counter_disabled(mp, field)))
 +		goto slow_path;
 +	xfs_icsb_lock_cntr(icsbp);
 +	if (unlikely(xfs_icsb_counter_disabled(mp, field))) {
 +		xfs_icsb_unlock_cntr(icsbp);
 +		goto slow_path;
 +	}
 +
 +	switch (field) {
 +	case XFS_SBS_ICOUNT:
 +		lcounter = icsbp->icsb_icount;
 +		lcounter += delta;
 +		if (unlikely(lcounter < 0))
 +			goto balance_counter;
 +		icsbp->icsb_icount = lcounter;
 +		break;
 +
 +	case XFS_SBS_IFREE:
 +		lcounter = icsbp->icsb_ifree;
 +		lcounter += delta;
 +		if (unlikely(lcounter < 0))
 +			goto balance_counter;
 +		icsbp->icsb_ifree = lcounter;
 +		break;
 +
 +	case XFS_SBS_FDBLOCKS:
 +		BUG_ON((mp->m_resblks - mp->m_resblks_avail) != 0);
 +
 +		lcounter = icsbp->icsb_fdblocks - XFS_ALLOC_SET_ASIDE(mp);
 +		lcounter += delta;
 +		if (unlikely(lcounter < 0))
 +			goto balance_counter;
 +		icsbp->icsb_fdblocks = lcounter + XFS_ALLOC_SET_ASIDE(mp);
 +		break;
 +	default:
 +		BUG();
 +		break;
 +	}
 +	xfs_icsb_unlock_cntr(icsbp);
 +	preempt_enable();
 +	return 0;
 +
 +slow_path:
 +	preempt_enable();
 +
 +	/*
 +	 * serialise with a mutex so we don't burn lots of cpu on
 +	 * the superblock lock. We still need to hold the superblock
 +	 * lock, however, when we modify the global structures.
 +	 */
 +	xfs_icsb_lock(mp);
 +
 +	/*
 +	 * Now running atomically.
 +	 *
 +	 * If the counter is enabled, someone has beaten us to rebalancing.
 +	 * Drop the lock and try again in the fast path....
 +	 */
 +	if (!(xfs_icsb_counter_disabled(mp, field))) {
 +		xfs_icsb_unlock(mp);
 +		goto again;
 +	}
 +
 +	/*
 +	 * The counter is currently disabled. Because we are
 +	 * running atomically here, we know a rebalance cannot
 +	 * be in progress. Hence we can go straight to operating
 +	 * on the global superblock. We do not call xfs_mod_incore_sb()
 +	 * here even though we need to get the m_sb_lock. Doing so
 +	 * will cause us to re-enter this function and deadlock.
 +	 * Hence we get the m_sb_lock ourselves and then call
 +	 * xfs_mod_incore_sb_unlocked() as the unlocked path operates
 +	 * directly on the global counters.
 +	 */
 +	spin_lock(&mp->m_sb_lock);
 +	ret = xfs_mod_incore_sb_unlocked(mp, field, delta, rsvd);
 +	spin_unlock(&mp->m_sb_lock);
 +
 +	/*
 +	 * Now that we've modified the global superblock, we
 +	 * may be able to re-enable the distributed counters
 +	 * (e.g. lots of space just got freed). After that
 +	 * we are done.
 +	 */
 +	if (ret != ENOSPC)
 +		xfs_icsb_balance_counter(mp, field, 0);
 +	xfs_icsb_unlock(mp);
 +	return ret;
 +
 +balance_counter:
 +	xfs_icsb_unlock_cntr(icsbp);
 +	preempt_enable();
 +
 +	/*
 +	 * We may have multiple threads here if multiple per-cpu
 +	 * counters run dry at the same time. This will mean we can
 +	 * do more balances than strictly necessary but it is not
 +	 * the common slowpath case.
 +	 */
 +	xfs_icsb_lock(mp);
 +
 +	/*
 +	 * running atomically.
 +	 *
 +	 * This will leave the counter in the correct state for future
 +	 * accesses. After the rebalance, we simply try again and our retry
 +	 * will either succeed through the fast path or slow path without
 +	 * another balance operation being required.
 +	 */
 +	xfs_icsb_balance_counter(mp, field, delta);
 +	xfs_icsb_unlock(mp);
 +	goto again;
 +}
 +
 +#endif
++=======
++>>>>>>> 5681ca40064f (xfs: Remove icsb infrastructure)
diff --cc fs/xfs/xfs_mount.h
index 77ab2563aabb,205f23a240a7..000000000000
--- a/fs/xfs/xfs_mount.h
+++ b/fs/xfs/xfs_mount.h
@@@ -29,44 -27,6 +27,47 @@@ struct xfs_quotainfo
  struct xfs_dir_ops;
  struct xfs_da_geometry;
  
++<<<<<<< HEAD
 +#ifdef HAVE_PERCPU_SB
 +
 +/*
 + * Valid per-cpu incore superblock counters. Note that if you add new counters,
 + * you may need to define new counter disabled bit field descriptors as there
 + * are more possible fields in the superblock that can fit in a bitfield on a
 + * 32 bit platform. The XFS_SBS_* values for the current current counters just
 + * fit.
 + */
 +typedef struct xfs_icsb_cnts {
 +	uint64_t	icsb_fdblocks;
 +	uint64_t	icsb_ifree;
 +	uint64_t	icsb_icount;
 +	unsigned long	icsb_flags;
 +} xfs_icsb_cnts_t;
 +
 +#define XFS_ICSB_FLAG_LOCK	(1 << 0)	/* counter lock bit */
 +
 +#define XFS_ICSB_LAZY_COUNT	(1 << 1)	/* accuracy not needed */
 +
 +extern int	xfs_icsb_init_counters(struct xfs_mount *);
 +extern void	xfs_icsb_reinit_counters(struct xfs_mount *);
 +extern void	xfs_icsb_destroy_counters(struct xfs_mount *);
 +extern void	xfs_icsb_sync_counters(struct xfs_mount *, int);
 +extern void	xfs_icsb_sync_counters_locked(struct xfs_mount *, int);
 +extern int	xfs_icsb_modify_counters(struct xfs_mount *, xfs_sb_field_t,
 +						int64_t, int);
 +
 +#else
 +#define xfs_icsb_init_counters(mp)		(0)
 +#define xfs_icsb_destroy_counters(mp)		do { } while (0)
 +#define xfs_icsb_reinit_counters(mp)		do { } while (0)
 +#define xfs_icsb_sync_counters(mp, flags)	do { } while (0)
 +#define xfs_icsb_sync_counters_locked(mp, flags) do { } while (0)
 +#define xfs_icsb_modify_counters(mp, field, delta, rsvd) \
 +	xfs_mod_incore_sb(mp, field, delta, rsvd)
 +#endif
 +
++=======
++>>>>>>> 5681ca40064f (xfs: Remove icsb infrastructure)
  /* dynamic preallocation free space thresholds, 5% down to 1% */
  enum {
  	XFS_LOWSP_1_PCNT = 0,
@@@ -152,19 -115,8 +153,13 @@@ typedef struct xfs_mount 
  	const struct xfs_nameops *m_dirnameops;	/* vector of dir name ops */
  	const struct xfs_dir_ops *m_dir_inode_ops; /* vector of dir inode ops */
  	const struct xfs_dir_ops *m_nondir_inode_ops; /* !dir inode ops */
 +	int			m_dirblksize;	/* directory block sz--bytes */
 +	int			m_dirblkfsbs;	/* directory block sz--fsbs */
 +	xfs_dablk_t		m_dirdatablk;	/* blockno of dir data v2 */
 +	xfs_dablk_t		m_dirleafblk;	/* blockno of dir non-data v2 */
 +	xfs_dablk_t		m_dirfreeblk;	/* blockno of dirfreeindex v2 */
  	uint			m_chsize;	/* size of next field */
  	atomic_t		m_active_trans;	/* number trans frozen */
- #ifdef HAVE_PERCPU_SB
- 	xfs_icsb_cnts_t __percpu *m_sb_cnts;	/* per-cpu superblock counters */
- 	unsigned long		m_icsb_counters; /* disabled per-cpu counters */
- 	struct notifier_block	m_icsb_notifier; /* hotplug cpu notifier */
- 	struct mutex		m_icsb_mutex;	/* balancer sync lock */
- #endif
  	struct xfs_mru_cache	*m_filestream;  /* per-mount filestream data */
  	struct delayed_work	m_reclaim_work;	/* background inode reclaim */
  	struct delayed_work	m_eofblocks_work; /* background eof blocks
diff --cc fs/xfs/xfs_super.c
index a9ec2c211f1a,53c56a913778..000000000000
--- a/fs/xfs/xfs_super.c
+++ b/fs/xfs/xfs_super.c
@@@ -1107,7 -1081,9 +1090,13 @@@ xfs_fs_statfs
  	statp->f_fsid.val[0] = (u32)id;
  	statp->f_fsid.val[1] = (u32)(id >> 32);
  
++<<<<<<< HEAD
 +	xfs_icsb_sync_counters(mp, XFS_ICSB_LAZY_COUNT);
++=======
+ 	icount = percpu_counter_sum(&mp->m_icount);
+ 	ifree = percpu_counter_sum(&mp->m_ifree);
+ 	fdblocks = percpu_counter_sum(&mp->m_fdblocks);
++>>>>>>> 5681ca40064f (xfs: Remove icsb infrastructure)
  
  	spin_lock(&mp->m_sb_lock);
  	statp->f_bsize = sbp->sb_blocksize;
@@@ -1464,7 -1483,7 +1498,11 @@@ xfs_fs_fill_super
  	if (error)
  		goto out_close_devices;
  
++<<<<<<< HEAD
 +	error = -xfs_icsb_init_counters(mp);
++=======
+ 	error = xfs_init_percpu_counters(mp);
++>>>>>>> 5681ca40064f (xfs: Remove icsb infrastructure)
  	if (error)
  		goto out_destroy_workqueues;
  
diff --git a/fs/xfs/libxfs/xfs_sb.c b/fs/xfs/libxfs/xfs_sb.c
index 07f47e5690d3..76149f6e7b91 100644
--- a/fs/xfs/libxfs/xfs_sb.c
+++ b/fs/xfs/libxfs/xfs_sb.c
@@ -810,17 +810,15 @@ xfs_initialize_perag_data(
 		btree += pag->pagf_btreeblks;
 		xfs_perag_put(pag);
 	}
-	/*
-	 * Overwrite incore superblock counters with just-read data
-	 */
+
+	/* Overwrite incore superblock counters with just-read data */
 	spin_lock(&mp->m_sb_lock);
 	sbp->sb_ifree = ifree;
 	sbp->sb_icount = ialloc;
 	sbp->sb_fdblocks = bfree + bfreelst + btree;
 	spin_unlock(&mp->m_sb_lock);
 
-	/* Fixup the per-cpu counters as well. */
-	xfs_icsb_reinit_counters(mp);
+	xfs_reinit_percpu_counters(mp);
 
 	return 0;
 }
* Unmerged path fs/xfs/xfs_fsops.c
* Unmerged path fs/xfs/xfs_iomap.c
diff --git a/fs/xfs/xfs_linux.h b/fs/xfs/xfs_linux.h
index 8312771c2b5b..319b842cdf30 100644
--- a/fs/xfs/xfs_linux.h
+++ b/fs/xfs/xfs_linux.h
@@ -129,15 +129,6 @@ typedef __uint64_t __psunsigned_t;
 #undef XFS_NATIVE_HOST
 #endif
 
-/*
- * Feature macros (disable/enable)
- */
-#ifdef CONFIG_SMP
-#define HAVE_PERCPU_SB	/* per cpu superblock counters are a 2.6 feature */
-#else
-#undef  HAVE_PERCPU_SB	/* per cpu superblock counters are a 2.6 feature */
-#endif
-
 #define irix_sgid_inherit	xfs_params.sgid_inherit.val
 #define irix_symlink_mode	xfs_params.symlink_mode.val
 #define xfs_panic_mask		xfs_params.panic_mask.val
diff --git a/fs/xfs/xfs_log_recover.c b/fs/xfs/xfs_log_recover.c
index 7b97b53b2b95..ee1fc3ed47d9 100644
--- a/fs/xfs/xfs_log_recover.c
+++ b/fs/xfs/xfs_log_recover.c
@@ -4419,10 +4419,10 @@ xlog_do_recover(
 	xfs_sb_from_disk(sbp, XFS_BUF_TO_SBP(bp));
 	ASSERT(sbp->sb_magicnum == XFS_SB_MAGIC);
 	ASSERT(xfs_sb_good_version(sbp));
+	xfs_reinit_percpu_counters(log->l_mp);
+
 	xfs_buf_relse(bp);
 
-	/* We've re-read the superblock so re-initialize per-cpu counters */
-	xfs_icsb_reinit_counters(log->l_mp);
 
 	xlog_recover_check_summary(log);
 
* Unmerged path fs/xfs/xfs_mount.c
* Unmerged path fs/xfs/xfs_mount.h
* Unmerged path fs/xfs/xfs_super.c
diff --git a/fs/xfs/xfs_super.h b/fs/xfs/xfs_super.h
index b4cfe21d8fb0..187d93f4f5ea 100644
--- a/fs/xfs/xfs_super.h
+++ b/fs/xfs/xfs_super.h
@@ -83,6 +83,8 @@ extern const struct export_operations xfs_export_operations;
 extern const struct xattr_handler *xfs_xattr_handlers[];
 extern const struct quotactl_ops xfs_quotactl_operations;
 
+extern void xfs_reinit_percpu_counters(struct xfs_mount *mp);
+
 #define XFS_M(sb)		((struct xfs_mount *)((sb)->s_fs_info))
 
 #endif	/* __XFS_SUPER_H__ */
