hrtimer: Make offset update smarter

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 868a3e915f7f5eba8f8cb4f7da2276760807c51c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/868a3e91.failed

On every tick/hrtimer interrupt we update the offset variables of the
clock bases. That's silly because these offsets change very seldom.

Add a sequence counter to the time keeping code which keeps track of
the offset updates (clock_was_set()). Have a sequence cache in the
hrtimer cpu bases to evaluate whether the offsets must be updated or
not. This allows us later to avoid pointless cacheline pollution.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
	Acked-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
	Cc: Marcelo Tosatti <mtosatti@redhat.com>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: John Stultz <john.stultz@linaro.org>
Link: http://lkml.kernel.org/r/20150414203501.132820245@linutronix.de
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: John Stultz <john.stultz@linaro.org>
(cherry picked from commit 868a3e915f7f5eba8f8cb4f7da2276760807c51c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/timekeeper_internal.h
#	kernel/hrtimer.c
#	kernel/time/timekeeping.c
#	kernel/time/timekeeping.h
diff --cc include/linux/timekeeper_internal.h
index 4e31044f893e,6f8276ae579c..000000000000
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@@ -10,65 -10,90 +10,148 @@@
  #include <linux/jiffies.h>
  #include <linux/time.h>
  
++<<<<<<< HEAD
 +/* Structure holding internal timekeeping values. */
 +struct timekeeper {
 +	/* Current clocksource used for timekeeping. */
 +	struct clocksource	*clock;
 +	/* NTP adjusted clock multiplier */
 +	u32			mult;
 +	/* The shift value of the current clocksource. */
 +	u32			shift;
 +	/* Number of clock cycles in one NTP interval. */
++=======
+ /**
+  * struct tk_read_base - base structure for timekeeping readout
+  * @clock:	Current clocksource used for timekeeping.
+  * @read:	Read function of @clock
+  * @mask:	Bitmask for two's complement subtraction of non 64bit clocks
+  * @cycle_last: @clock cycle value at last update
+  * @mult:	(NTP adjusted) multiplier for scaled math conversion
+  * @shift:	Shift value for scaled math conversion
+  * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+  * @base:	ktime_t (nanoseconds) base time for readout
+  *
+  * This struct has size 56 byte on 64 bit. Together with a seqcount it
+  * occupies a single 64byte cache line.
+  *
+  * The struct is separate from struct timekeeper as it is also used
+  * for a fast NMI safe accessors.
+  */
+ struct tk_read_base {
+ 	struct clocksource	*clock;
+ 	cycle_t			(*read)(struct clocksource *cs);
+ 	cycle_t			mask;
+ 	cycle_t			cycle_last;
+ 	u32			mult;
+ 	u32			shift;
+ 	u64			xtime_nsec;
+ 	ktime_t			base;
+ };
+ 
+ /**
+  * struct timekeeper - Structure holding internal timekeeping values.
+  * @tkr_mono:		The readout base structure for CLOCK_MONOTONIC
+  * @tkr_raw:		The readout base structure for CLOCK_MONOTONIC_RAW
+  * @xtime_sec:		Current CLOCK_REALTIME time in seconds
+  * @ktime_sec:		Current CLOCK_MONOTONIC time in seconds
+  * @wall_to_monotonic:	CLOCK_REALTIME to CLOCK_MONOTONIC offset
+  * @offs_real:		Offset clock monotonic -> clock realtime
+  * @offs_boot:		Offset clock monotonic -> clock boottime
+  * @offs_tai:		Offset clock monotonic -> clock tai
+  * @tai_offset:		The current UTC to TAI offset in seconds
+  * @clock_was_set_seq:	The sequence number of clock was set events
+  * @raw_time:		Monotonic raw base time in timespec64 format
+  * @cycle_interval:	Number of clock cycles in one NTP interval
+  * @xtime_interval:	Number of clock shifted nano seconds in one NTP
+  *			interval.
+  * @xtime_remainder:	Shifted nano seconds left over when rounding
+  *			@cycle_interval
+  * @raw_interval:	Raw nano seconds accumulated per NTP interval.
+  * @ntp_error:		Difference between accumulated time and NTP time in ntp
+  *			shifted nano seconds.
+  * @ntp_error_shift:	Shift conversion between clock shifted nano seconds and
+  *			ntp shifted nano seconds.
+  *
+  * Note: For timespec(64) based interfaces wall_to_monotonic is what
+  * we need to add to xtime (or xtime corrected for sub jiffie times)
+  * to get to monotonic time.  Monotonic is pegged at zero at system
+  * boot time, so wall_to_monotonic will be negative, however, we will
+  * ALWAYS keep the tv_nsec part positive so we can use the usual
+  * normalization.
+  *
+  * wall_to_monotonic is moved after resume from suspend for the
+  * monotonic time not to jump. We need to add total_sleep_time to
+  * wall_to_monotonic to get the real boot based time offset.
+  *
+  * wall_to_monotonic is no longer the boot time, getboottime must be
+  * used instead.
+  */
+ struct timekeeper {
+ 	struct tk_read_base	tkr_mono;
+ 	struct tk_read_base	tkr_raw;
+ 	u64			xtime_sec;
+ 	unsigned long		ktime_sec;
+ 	struct timespec64	wall_to_monotonic;
+ 	ktime_t			offs_real;
+ 	ktime_t			offs_boot;
+ 	ktime_t			offs_tai;
+ 	s32			tai_offset;
+ 	unsigned int		clock_was_set_seq;
+ 	struct timespec64	raw_time;
+ 
+ 	/* The following members are for timekeeping internal use */
++>>>>>>> 868a3e915f7f (hrtimer: Make offset update smarter)
  	cycle_t			cycle_interval;
 +	/* Last cycle value (also stored in clock->cycle_last) */
 +	cycle_t			cycle_last;
 +	/* Number of clock shifted nano seconds in one NTP interval. */
  	u64			xtime_interval;
 +	/* shifted nano seconds left over when rounding cycle_interval */
  	s64			xtime_remainder;
 +	/* Raw nano seconds accumulated per NTP interval. */
  	u32			raw_interval;
 +
 +	/* Current CLOCK_REALTIME time in seconds */
 +	u64			xtime_sec;
 +	/* Clock shifted nano seconds */
 +	u64			xtime_nsec;
 +
 +	/* Difference between accumulated time and NTP time in ntp
 +	 * shifted nano seconds. */
 +	s64			ntp_error;
 +	/* Shift conversion between clock shifted nano seconds and
 +	 * ntp shifted nano seconds. */
 +	u32			ntp_error_shift;
 +
 +	/*
 +	 * wall_to_monotonic is what we need to add to xtime (or xtime corrected
 +	 * for sub jiffie times) to get to monotonic time.  Monotonic is pegged
 +	 * at zero at system boot time, so wall_to_monotonic will be negative,
 +	 * however, we will ALWAYS keep the tv_nsec part positive so we can use
 +	 * the usual normalization.
 +	 *
 +	 * wall_to_monotonic is moved after resume from suspend for the
 +	 * monotonic time not to jump. We need to add total_sleep_time to
 +	 * wall_to_monotonic to get the real boot based time offset.
 +	 *
 +	 * - wall_to_monotonic is no longer the boot time, getboottime must be
 +	 * used instead.
 +	 */
 +	struct timespec		wall_to_monotonic;
 +	/* Offset clock monotonic -> clock realtime */
 +	ktime_t			offs_real;
 +	/* time spent in suspend */
 +	struct timespec		total_sleep_time;
 +	/* Offset clock monotonic -> clock boottime */
 +	ktime_t			offs_boot;
 +	/* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
 +	struct timespec		raw_time;
 +	/* The current UTC to TAI offset in seconds */
 +	s32			tai_offset;
 +	/* Offset clock monotonic -> clock tai */
 +	ktime_t			offs_tai;
 +
  	/* The ntp_tick_length() value currently being used.
  	 * This cached copy ensures we consistently apply the tick
  	 * length for an entire tick, as ntp_tick_length may change
diff --cc kernel/hrtimer.c
index fc849ec7e7d8,8ce9b3138017..000000000000
--- a/kernel/hrtimer.c
+++ b/kernel/hrtimer.c
@@@ -498,6 -414,47 +498,50 @@@ static inline void debug_deactivate(str
  	trace_hrtimer_cancel(timer);
  }
  
++<<<<<<< HEAD:kernel/hrtimer.c
++=======
+ #if defined(CONFIG_NO_HZ_COMMON) || defined(CONFIG_HIGH_RES_TIMERS)
+ static ktime_t __hrtimer_get_next_event(struct hrtimer_cpu_base *cpu_base)
+ {
+ 	struct hrtimer_clock_base *base = cpu_base->clock_base;
+ 	ktime_t expires, expires_next = { .tv64 = KTIME_MAX };
+ 	int i;
+ 
+ 	for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++, base++) {
+ 		struct timerqueue_node *next;
+ 		struct hrtimer *timer;
+ 
+ 		next = timerqueue_getnext(&base->active);
+ 		if (!next)
+ 			continue;
+ 
+ 		timer = container_of(next, struct hrtimer, node);
+ 		expires = ktime_sub(hrtimer_get_expires(timer), base->offset);
+ 		if (expires.tv64 < expires_next.tv64)
+ 			expires_next = expires;
+ 	}
+ 	/*
+ 	 * clock_was_set() might have changed base->offset of any of
+ 	 * the clock bases so the result might be negative. Fix it up
+ 	 * to prevent a false positive in clockevents_program_event().
+ 	 */
+ 	if (expires_next.tv64 < 0)
+ 		expires_next.tv64 = 0;
+ 	return expires_next;
+ }
+ #endif
+ 
+ static inline ktime_t hrtimer_update_base(struct hrtimer_cpu_base *base)
+ {
+ 	ktime_t *offs_real = &base->clock_base[HRTIMER_BASE_REALTIME].offset;
+ 	ktime_t *offs_boot = &base->clock_base[HRTIMER_BASE_BOOTTIME].offset;
+ 	ktime_t *offs_tai = &base->clock_base[HRTIMER_BASE_TAI].offset;
+ 
+ 	return ktime_get_update_offsets_now(&base->clock_was_set_seq,
+ 					    offs_real, offs_boot, offs_tai);
+ }
+ 
++>>>>>>> 868a3e915f7f (hrtimer: Make offset update smarter):kernel/time/hrtimer.c
  /* High resolution timer related functions */
  #ifdef CONFIG_HIGH_RES_TIMERS
  
diff --cc kernel/time/timekeeping.c
index a690422a076b,3365e32dc208..000000000000
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@@ -258,7 -597,14 +258,18 @@@ static void timekeeping_update(struct t
  	update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);
  
  	if (action & TK_MIRROR)
++<<<<<<< HEAD
 +		memcpy(&shadow_timekeeper, &timekeeper, sizeof(timekeeper));
++=======
+ 		memcpy(&shadow_timekeeper, &tk_core.timekeeper,
+ 		       sizeof(tk_core.timekeeper));
+ 
+ 	update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);
+ 	update_fast_timekeeper(&tk->tkr_raw,  &tk_fast_raw);
+ 
+ 	if (action & TK_CLOCK_WAS_SET)
+ 		tk->clock_was_set_seq++;
++>>>>>>> 868a3e915f7f (hrtimer: Make offset update smarter)
  }
  
  /**
@@@ -1549,87 -1929,41 +1560,115 @@@ void do_timer(unsigned long ticks
  }
  
  /**
++<<<<<<< HEAD
 + * ktime_get_update_offsets_tick - hrtimer helper
++=======
+  * ktime_get_update_offsets_now - hrtimer helper
+  * @cwsseq:	pointer to check and store the clock was set sequence number
++>>>>>>> 868a3e915f7f (hrtimer: Make offset update smarter)
   * @offs_real:	pointer to storage for monotonic -> realtime offset
   * @offs_boot:	pointer to storage for monotonic -> boottime offset
   * @offs_tai:	pointer to storage for monotonic -> clock tai offset
   *
++<<<<<<< HEAD
 + * Returns monotonic time at last tick and various offsets
 + */
 +ktime_t ktime_get_update_offsets_tick(ktime_t *offs_real, ktime_t *offs_boot,
 +							ktime_t *offs_tai)
++=======
+  * Returns current monotonic time and updates the offsets if the
+  * sequence number in @cwsseq and timekeeper.clock_was_set_seq are
+  * different.
+  *
+  * Called from hrtimer_interrupt() or retrigger_next_event()
+  */
+ ktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,
+ 				     ktime_t *offs_boot, ktime_t *offs_tai)
++>>>>>>> 868a3e915f7f (hrtimer: Make offset update smarter)
  {
 -	struct timekeeper *tk = &tk_core.timekeeper;
 +	struct timekeeper *tk = &timekeeper;
 +	struct timespec ts;
 +	ktime_t now;
  	unsigned int seq;
 -	ktime_t base;
 -	u64 nsecs;
  
  	do {
 -		seq = read_seqcount_begin(&tk_core.seq);
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +
++<<<<<<< HEAD
 +		ts = tk_xtime(tk);
  
 +		*offs_real = tk->offs_real;
 +		*offs_boot = tk->offs_boot;
 +		*offs_tai = tk->offs_tai;
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
++=======
+ 		base = tk->tkr_mono.base;
+ 		nsecs = timekeeping_get_ns(&tk->tkr_mono);
+ 		if (*cwsseq != tk->clock_was_set_seq) {
+ 			*cwsseq = tk->clock_was_set_seq;
+ 			*offs_real = tk->offs_real;
+ 			*offs_boot = tk->offs_boot;
+ 			*offs_tai = tk->offs_tai;
+ 		}
+ 	} while (read_seqcount_retry(&tk_core.seq, seq));
++>>>>>>> 868a3e915f7f (hrtimer: Make offset update smarter)
 +
 +	now = ktime_set(ts.tv_sec, ts.tv_nsec);
 +	now = ktime_sub(now, *offs_real);
 +	return now;
 +}
 +
 +#ifdef CONFIG_HIGH_RES_TIMERS
 +/**
 + * ktime_get_update_offsets_now - hrtimer helper
 + * @offs_real:	pointer to storage for monotonic -> realtime offset
 + * @offs_boot:	pointer to storage for monotonic -> boottime offset
 + *
 + * Returns current monotonic time and updates the offsets
 + * Called from hrtimer_interupt() or retrigger_next_event()
 + */
 +ktime_t ktime_get_update_offsets_now(ktime_t *offs_real, ktime_t *offs_boot,
 +							ktime_t *offs_tai)
 +{
 +	struct timekeeper *tk = &timekeeper;
 +	ktime_t now;
 +	unsigned int seq;
 +	u64 secs, nsecs;
 +
 +	do {
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +
 +		secs = tk->xtime_sec;
 +		nsecs = timekeeping_get_ns(tk);
 +
 +		*offs_real = tk->offs_real;
 +		*offs_boot = tk->offs_boot;
 +		*offs_tai = tk->offs_tai;
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
  
 -	return ktime_add_ns(base, nsecs);
 +	now = ktime_add_ns(ktime_set(secs, 0), nsecs);
 +	now = ktime_sub(now, *offs_real);
 +	return now;
 +}
 +#endif
 +
 +/**
 + * ktime_get_monotonic_offset() - get wall_to_monotonic in ktime_t format
 + */
 +ktime_t ktime_get_monotonic_offset(void)
 +{
 +	struct timekeeper *tk = &timekeeper;
 +	unsigned long seq;
 +	struct timespec wtom;
 +
 +	do {
 +		seq = read_seqcount_begin(&timekeeper_seq);
 +		wtom = tk->wall_to_monotonic;
 +	} while (read_seqcount_retry(&timekeeper_seq, seq));
 +
 +	return timespec_to_ktime(wtom);
  }
 +EXPORT_SYMBOL_GPL(ktime_get_monotonic_offset);
  
  /**
   * do_adjtimex() - Accessor function to NTP __do_adjtimex function
* Unmerged path kernel/time/timekeeping.h
diff --git a/include/linux/hrtimer.h b/include/linux/hrtimer.h
index 9944ae0be366..73ddbdb50598 100644
--- a/include/linux/hrtimer.h
+++ b/include/linux/hrtimer.h
@@ -167,7 +167,7 @@ enum  hrtimer_base_type {
  *			and timers
  * @cpu:		cpu number
  * @active_bases:	Bitfield to mark bases with active timers
- * @clock_was_set:	Indicates that clock was set from irq context.
+ * @clock_was_set_seq:	Sequence counter of clock was set events
  * @expires_next:	absolute time of the next event which was scheduled
  *			via clock_set_next_event()
  * @hres_active:	State of high resolution mode
@@ -182,7 +182,7 @@ struct hrtimer_cpu_base {
 	raw_spinlock_t			lock;
 	unsigned int			cpu;
 	unsigned int			active_bases;
-	unsigned int			clock_was_set;
+	unsigned int			clock_was_set_seq;
 #ifdef CONFIG_HIGH_RES_TIMERS
 	ktime_t				expires_next;
 	int				hres_active;
* Unmerged path include/linux/timekeeper_internal.h
* Unmerged path kernel/hrtimer.c
* Unmerged path kernel/time/timekeeping.c
* Unmerged path kernel/time/timekeeping.h
