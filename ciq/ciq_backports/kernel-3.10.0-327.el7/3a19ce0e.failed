tcp: IPv6 support for fastopen server

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Daniel Lee <longinus00@gmail.com>
commit 3a19ce0eec32667b835d8dc887002019fc6b3a02
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/3a19ce0e.failed

After all the preparatory works, supporting IPv6 in Fast Open is now easy.
We pretty much just mirror v4 code. The only difference is how we
generate the Fast Open cookie for IPv6 sockets. Since Fast Open cookie
is 128 bits and we use AES 128, we use CBC-MAC to encrypt both the
source and destination IPv6 addresses since the cookie is a MAC tag.

	Signed-off-by: Daniel Lee <longinus00@gmail.com>
	Signed-off-by: Yuchung Cheng <ycheng@google.com>
	Signed-off-by: Jerry Chu <hkchu@google.com>
	Acked-by: Neal Cardwell <ncardwell@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3a19ce0eec32667b835d8dc887002019fc6b3a02)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_fastopen.c
diff --cc net/ipv4/tcp_fastopen.c
index ab7bd35bb312,62e48cf84e60..000000000000
--- a/net/ipv4/tcp_fastopen.c
+++ b/net/ipv4/tcp_fastopen.c
@@@ -58,34 -72,224 +58,118 @@@ error:		kfree(ctx)
  	return err;
  }
  
- /* Computes the fastopen cookie for the IP path.
-  * The path is a 128 bits long (pad with zeros for IPv4).
-  *
-  * The caller must check foc->len to determine if a valid cookie
-  * has been generated successfully.
- */
- void tcp_fastopen_cookie_gen(__be32 src, __be32 dst,
- 			     struct tcp_fastopen_cookie *foc)
+ static bool __tcp_fastopen_cookie_gen(const void *path,
+ 				      struct tcp_fastopen_cookie *foc)
  {
- 	__be32 path[4] = { src, dst, 0, 0 };
  	struct tcp_fastopen_context *ctx;
+ 	bool ok = false;
  
 -	tcp_fastopen_init_key_once(true);
 -
  	rcu_read_lock();
  	ctx = rcu_dereference(tcp_fastopen_ctx);
  	if (ctx) {
- 		crypto_cipher_encrypt_one(ctx->tfm, foc->val, (__u8 *)path);
+ 		crypto_cipher_encrypt_one(ctx->tfm, foc->val, path);
  		foc->len = TCP_FASTOPEN_COOKIE_SIZE;
+ 		ok = true;
  	}
  	rcu_read_unlock();
+ 	return ok;
+ }
+ 
+ /* Generate the fastopen cookie by doing aes128 encryption on both
+  * the source and destination addresses. Pad 0s for IPv4 or IPv4-mapped-IPv6
+  * addresses. For the longer IPv6 addresses use CBC-MAC.
+  *
+  * XXX (TFO) - refactor when TCP_FASTOPEN_COOKIE_SIZE != AES_BLOCK_SIZE.
+  */
+ static bool tcp_fastopen_cookie_gen(struct request_sock *req,
+ 				    struct sk_buff *syn,
+ 				    struct tcp_fastopen_cookie *foc)
+ {
+ 	if (req->rsk_ops->family == AF_INET) {
+ 		const struct iphdr *iph = ip_hdr(syn);
+ 
+ 		__be32 path[4] = { iph->saddr, iph->daddr, 0, 0 };
+ 		return __tcp_fastopen_cookie_gen(path, foc);
+ 	}
+ 
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	if (req->rsk_ops->family == AF_INET6) {
+ 		const struct ipv6hdr *ip6h = ipv6_hdr(syn);
+ 		struct tcp_fastopen_cookie tmp;
+ 
+ 		if (__tcp_fastopen_cookie_gen(&ip6h->saddr, &tmp)) {
+ 			struct in6_addr *buf = (struct in6_addr *) tmp.val;
+ 			int i = 4;
+ 
+ 			for (i = 0; i < 4; i++)
+ 				buf->s6_addr32[i] ^= ip6h->daddr.s6_addr32[i];
+ 			return __tcp_fastopen_cookie_gen(buf, foc);
+ 		}
+ 	}
+ #endif
+ 	return false;
  }
  
 -static bool tcp_fastopen_create_child(struct sock *sk,
 -				      struct sk_buff *skb,
 -				      struct dst_entry *dst,
 -				      struct request_sock *req)
 +static int __init tcp_fastopen_init(void)
  {
 -	struct tcp_sock *tp = tcp_sk(sk);
 -	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
 -	struct sock *child;
 -
 -	req->num_retrans = 0;
 -	req->num_timeout = 0;
 -	req->sk = NULL;
 -
 -	child = inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb, req, NULL);
 -	if (child == NULL)
 -		return false;
 -
 -	spin_lock(&queue->fastopenq->lock);
 -	queue->fastopenq->qlen++;
 -	spin_unlock(&queue->fastopenq->lock);
 -
 -	/* Initialize the child socket. Have to fix some values to take
 -	 * into account the child is a Fast Open socket and is created
 -	 * only out of the bits carried in the SYN packet.
 -	 */
 -	tp = tcp_sk(child);
 -
 -	tp->fastopen_rsk = req;
 -	/* Do a hold on the listner sk so that if the listener is being
 -	 * closed, the child that has been accepted can live on and still
 -	 * access listen_lock.
 -	 */
 -	sock_hold(sk);
 -	tcp_rsk(req)->listener = sk;
 -
 -	/* RFC1323: The window in SYN & SYN/ACK segments is never
 -	 * scaled. So correct it appropriately.
 -	 */
 -	tp->snd_wnd = ntohs(tcp_hdr(skb)->window);
 -
 -	/* Activate the retrans timer so that SYNACK can be retransmitted.
 -	 * The request socket is not added to the SYN table of the parent
 -	 * because it's been added to the accept queue directly.
 -	 */
 -	inet_csk_reset_xmit_timer(child, ICSK_TIME_RETRANS,
 -				  TCP_TIMEOUT_INIT, TCP_RTO_MAX);
 +	__u8 key[TCP_FASTOPEN_KEY_LENGTH];
  
 -	/* Add the child socket directly into the accept queue */
 -	inet_csk_reqsk_queue_add(sk, req, child);
 -
 -	/* Now finish processing the fastopen child socket. */
 -	inet_csk(child)->icsk_af_ops->rebuild_header(child);
 -	tcp_init_congestion_control(child);
 -	tcp_mtup_init(child);
 -	tcp_init_metrics(child);
 -	tcp_init_buffer_space(child);
 -
 -	/* Queue the data carried in the SYN packet. We need to first
 -	 * bump skb's refcnt because the caller will attempt to free it.
 -	 *
 -	 * XXX (TFO) - we honor a zero-payload TFO request for now,
 -	 * (any reason not to?) but no need to queue the skb since
 -	 * there is no data. How about SYN+FIN?
 -	 */
 -	if (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq + 1) {
 -		skb = skb_get(skb);
 -		skb_dst_drop(skb);
 -		__skb_pull(skb, tcp_hdr(skb)->doff * 4);
 -		skb_set_owner_r(skb, child);
 -		__skb_queue_tail(&child->sk_receive_queue, skb);
 -		tp->syn_data_acked = 1;
 -	}
 -	tcp_rsk(req)->rcv_nxt = tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 -	sk->sk_data_ready(sk);
 -	bh_unlock_sock(child);
 -	sock_put(child);
 -	WARN_ON(req->sk == NULL);
 -	return true;
 -}
 -EXPORT_SYMBOL(tcp_fastopen_create_child);
 -
 -static bool tcp_fastopen_queue_check(struct sock *sk)
 -{
 -	struct fastopen_queue *fastopenq;
 -
 -	/* Make sure the listener has enabled fastopen, and we don't
 -	 * exceed the max # of pending TFO requests allowed before trying
 -	 * to validating the cookie in order to avoid burning CPU cycles
 -	 * unnecessarily.
 -	 *
 -	 * XXX (TFO) - The implication of checking the max_qlen before
 -	 * processing a cookie request is that clients can't differentiate
 -	 * between qlen overflow causing Fast Open to be disabled
 -	 * temporarily vs a server not supporting Fast Open at all.
 -	 */
 -	fastopenq = inet_csk(sk)->icsk_accept_queue.fastopenq;
 -	if (fastopenq == NULL || fastopenq->max_qlen == 0)
 -		return false;
 -
 -	if (fastopenq->qlen >= fastopenq->max_qlen) {
 -		struct request_sock *req1;
 -		spin_lock(&fastopenq->lock);
 -		req1 = fastopenq->rskq_rst_head;
 -		if ((req1 == NULL) || time_after(req1->expires, jiffies)) {
 -			spin_unlock(&fastopenq->lock);
 -			NET_INC_STATS_BH(sock_net(sk),
 -					 LINUX_MIB_TCPFASTOPENLISTENOVERFLOW);
 -			return false;
 -		}
 -		fastopenq->rskq_rst_head = req1->dl_next;
 -		fastopenq->qlen--;
 -		spin_unlock(&fastopenq->lock);
 -		reqsk_free(req1);
 -	}
 -	return true;
 +	get_random_bytes(key, sizeof(key));
 +	tcp_fastopen_reset_cipher(key, sizeof(key));
 +	return 0;
  }
  
++<<<<<<< HEAD
 +late_initcall(tcp_fastopen_init);
++=======
+ /* Returns true if we should perform Fast Open on the SYN. The cookie (foc)
+  * may be updated and return the client in the SYN-ACK later. E.g., Fast Open
+  * cookie request (foc->len == 0).
+  */
+ bool tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
+ 		      struct request_sock *req,
+ 		      struct tcp_fastopen_cookie *foc,
+ 		      struct dst_entry *dst)
+ {
+ 	struct tcp_fastopen_cookie valid_foc = { .len = -1 };
+ 	bool syn_data = TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq + 1;
+ 
+ 	if (!((sysctl_tcp_fastopen & TFO_SERVER_ENABLE) &&
+ 	      (syn_data || foc->len >= 0) &&
+ 	      tcp_fastopen_queue_check(sk))) {
+ 		foc->len = -1;
+ 		return false;
+ 	}
+ 
+ 	if (syn_data && (sysctl_tcp_fastopen & TFO_SERVER_COOKIE_NOT_REQD))
+ 		goto fastopen;
+ 
+ 	if (tcp_fastopen_cookie_gen(req, skb, &valid_foc) &&
+ 	    foc->len == TCP_FASTOPEN_COOKIE_SIZE &&
+ 	    foc->len == valid_foc.len &&
+ 	    !memcmp(foc->val, valid_foc.val, foc->len)) {
+ 		/* Cookie is valid. Create a (full) child socket to accept
+ 		 * the data in SYN before returning a SYN-ACK to ack the
+ 		 * data. If we fail to create the socket, fall back and
+ 		 * ack the ISN only but includes the same cookie.
+ 		 *
+ 		 * Note: Data-less SYN with valid cookie is allowed to send
+ 		 * data in SYN_RECV state.
+ 		 */
+ fastopen:
+ 		if (tcp_fastopen_create_child(sk, skb, dst, req)) {
+ 			foc->len = -1;
+ 			NET_INC_STATS_BH(sock_net(sk),
+ 					 LINUX_MIB_TCPFASTOPENPASSIVE);
+ 			return true;
+ 		}
+ 	}
+ 
+ 	NET_INC_STATS_BH(sock_net(sk), foc->len ?
+ 			 LINUX_MIB_TCPFASTOPENPASSIVEFAIL :
+ 			 LINUX_MIB_TCPFASTOPENCOOKIEREQD);
+ 	*foc = valid_foc;
+ 	return false;
+ }
+ EXPORT_SYMBOL(tcp_try_fastopen);
++>>>>>>> 3a19ce0eec32 (tcp: IPv6 support for fastopen server)
* Unmerged path net/ipv4/tcp_fastopen.c
diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c
index fdc8f8ca98c1..9ace5e5e759c 100644
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@ -474,7 +474,8 @@ out:
 static int tcp_v6_send_synack(struct sock *sk, struct dst_entry *dst,
 			      struct flowi6 *fl6,
 			      struct request_sock *req,
-			      u16 queue_mapping)
+			      u16 queue_mapping,
+			      struct tcp_fastopen_cookie *foc)
 {
 	struct inet_request_sock *ireq = inet_rsk(req);
 	struct ipv6_pinfo *np = inet6_sk(sk);
@@ -485,7 +486,7 @@ static int tcp_v6_send_synack(struct sock *sk, struct dst_entry *dst,
 	if (!dst && (dst = inet6_csk_route_req(sk, fl6, req)) == NULL)
 		goto done;
 
-	skb = tcp_make_synack(sk, dst, req, NULL);
+	skb = tcp_make_synack(sk, dst, req, foc);
 
 	if (skb) {
 		__tcp_v6_send_check(skb, &ireq->ir_v6_loc_addr,
@@ -506,7 +507,7 @@ static int tcp_v6_rtx_synack(struct sock *sk, struct request_sock *req)
 	struct flowi6 fl6;
 	int res;
 
-	res = tcp_v6_send_synack(sk, NULL, &fl6, req, 0);
+	res = tcp_v6_send_synack(sk, NULL, &fl6, req, 0, NULL);
 	if (!res) {
 		TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_RETRANSSEGS);
 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);
@@ -925,7 +926,12 @@ static void tcp_v6_timewait_ack(struct sock *sk, struct sk_buff *skb)
 static void tcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,
 				  struct request_sock *req)
 {
-	tcp_v6_send_ack(skb, tcp_rsk(req)->snt_isn + 1, tcp_rsk(req)->rcv_isn + 1,
+	/* sk->sk_state == TCP_LISTEN -> for regular TCP_SYN_RECV
+	 * sk->sk_state == TCP_SYN_RECV -> for Fast Open.
+	 */
+	tcp_v6_send_ack(skb, (sk->sk_state == TCP_LISTEN) ?
+			tcp_rsk(req)->snt_isn + 1 : tcp_sk(sk)->snd_nxt,
+			tcp_rsk(req)->rcv_nxt,
 			req->rcv_wnd, tcp_time_stamp, req->ts_recent, sk->sk_bound_dev_if,
 			tcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr),
 			0, 0);
@@ -977,8 +983,10 @@ static int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
 	struct tcp_sock *tp = tcp_sk(sk);
 	__u32 isn = TCP_SKB_CB(skb)->when;
 	struct dst_entry *dst = NULL;
+	struct tcp_fastopen_cookie foc = { .len = -1 };
+	bool want_cookie = false, fastopen;
 	struct flowi6 fl6;
-	bool want_cookie = false;
+	int err;
 
 	if (skb->protocol == htons(ETH_P_IP))
 		return tcp_v4_conn_request(sk, skb);
@@ -1009,7 +1017,7 @@ static int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
 	tcp_clear_options(&tmp_opt);
 	tmp_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);
 	tmp_opt.user_mss = tp->rx_opt.user_mss;
-	tcp_parse_options(skb, &tmp_opt, 0, NULL);
+	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
 
 	if (want_cookie && !tmp_opt.saw_tstamp)
 		tcp_clear_options(&tmp_opt);
@@ -1081,19 +1089,27 @@ static int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
 		isn = tcp_v6_init_sequence(skb);
 	}
 have_isn:
-	tcp_rsk(req)->snt_isn = isn;
 
 	if (security_inet_conn_request(sk, skb, req))
 		goto drop_and_release;
 
-	if (tcp_v6_send_synack(sk, dst, &fl6, req,
-			       skb_get_queue_mapping(skb)) ||
-	    want_cookie)
+	if (!dst && (dst = inet6_csk_route_req(sk, &fl6, req)) == NULL)
 		goto drop_and_free;
 
+	tcp_rsk(req)->snt_isn = isn;
 	tcp_rsk(req)->snt_synack = tcp_time_stamp;
-	tcp_rsk(req)->listener = NULL;
-	inet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+	tcp_openreq_init_rwin(req, sk, dst);
+	fastopen = !want_cookie &&
+		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+	err = tcp_v6_send_synack(sk, dst, &fl6, req,
+				 skb_get_queue_mapping(skb), &foc);
+	if (!fastopen) {
+		if (err || want_cookie)
+			goto drop_and_free;
+
+		tcp_rsk(req)->listener = NULL;
+		inet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+	}
 	return 0;
 
 drop_and_release:
