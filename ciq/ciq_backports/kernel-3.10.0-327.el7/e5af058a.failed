KVM: x86/vPMU: reorder PMU functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [x86] kvm: vpmu: reorder PMU functions (Wei Huang) [1076010]
Rebuild_FUZZ: 94.12%
commit-author Wei Huang <wehuang@redhat.com>
commit e5af058aacd55e578b3c57b1582b90c4290b77f9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/e5af058a.failed

Keep called functions closer to their callers, and init/destroy
functions next to each other.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit e5af058aacd55e578b3c57b1582b90c4290b77f9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/pmu.c
diff --cc arch/x86/kvm/pmu.c
index ed5c6727d1e3,f38ad84be87e..000000000000
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@@ -87,13 -83,7 +87,17 @@@ static struct kvm_pmc *global_idx_to_pm
  		return get_fixed_pmc_idx(pmu, idx - INTEL_PMC_IDX_FIXED);
  }
  
++<<<<<<< HEAD
 +void kvm_deliver_pmi(struct kvm_vcpu *vcpu)
 +{
 +	if (vcpu->arch.apic)
 +		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 +}
 +
 +static void trigger_pmi(struct irq_work *irq_work)
++=======
+ static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
++>>>>>>> e5af058aacd5 (KVM: x86/vPMU: reorder PMU functions)
  {
  	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
  	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);
@@@ -315,10 -315,69 +319,73 @@@ static void global_ctrl_changed(struct 
  	pmu->global_ctrl = data;
  
  	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)
 -		reprogram_counter(pmu, bit);
 +		reprogram_idx(pmu, bit);
  }
  
++<<<<<<< HEAD
 +bool kvm_pmu_msr(struct kvm_vcpu *vcpu, u32 msr)
++=======
+ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
+ 	u64 bitmask;
+ 	int bit;
+ 
+ 	bitmask = pmu->reprogram_pmi;
+ 
+ 	for_each_set_bit(bit, (unsigned long *)&bitmask, X86_PMC_IDX_MAX) {
+ 		struct kvm_pmc *pmc = global_idx_to_pmc(pmu, bit);
+ 
+ 		if (unlikely(!pmc || !pmc->perf_event)) {
+ 			clear_bit(bit, (unsigned long *)&pmu->reprogram_pmi);
+ 			continue;
+ 		}
+ 
+ 		reprogram_counter(pmu, bit);
+ 	}
+ }
+ 
+ /* check if idx is a valid index to access PMU */
+ int kvm_pmu_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned idx)
+ {
+ 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
+ 	bool fixed = idx & (1u << 30);
+ 	idx &= ~(3u << 30);
+ 	return (!fixed && idx >= pmu->nr_arch_gp_counters) ||
+ 		(fixed && idx >= pmu->nr_arch_fixed_counters);
+ }
+ 
+ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
+ {
+ 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
+ 	bool fast_mode = idx & (1u << 31);
+ 	bool fixed = idx & (1u << 30);
+ 	struct kvm_pmc *counters;
+ 	u64 ctr_val;
+ 
+ 	idx &= ~(3u << 30);
+ 	if (!fixed && idx >= pmu->nr_arch_gp_counters)
+ 		return 1;
+ 	if (fixed && idx >= pmu->nr_arch_fixed_counters)
+ 		return 1;
+ 	counters = fixed ? pmu->fixed_counters : pmu->gp_counters;
+ 
+ 	ctr_val = pmc_read_counter(&counters[idx]);
+ 	if (fast_mode)
+ 		ctr_val = (u32)ctr_val;
+ 
+ 	*data = ctr_val;
+ 	return 0;
+ }
+ 
+ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
+ {
+ 	if (vcpu->arch.apic)
+ 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ }
+ 
+ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
++>>>>>>> e5af058aacd5 (KVM: x86/vPMU: reorder PMU functions)
  {
  	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
  	int ret;
@@@ -427,38 -486,11 +494,46 @@@ int kvm_pmu_set_msr(struct kvm_vcpu *vc
  	return 1;
  }
  
++<<<<<<< HEAD
 +int kvm_pmu_check_pmc(struct kvm_vcpu *vcpu, unsigned pmc)
 +{
 +	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 +	bool fixed = pmc & (1u << 30);
 +	pmc &= ~(3u << 30);
 +	return (!fixed && pmc >= pmu->nr_arch_gp_counters) ||
 +		(fixed && pmc >= pmu->nr_arch_fixed_counters);
 +}
 +
 +int kvm_pmu_read_pmc(struct kvm_vcpu *vcpu, unsigned pmc, u64 *data)
 +{
 +	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 +	bool fast_mode = pmc & (1u << 31);
 +	bool fixed = pmc & (1u << 30);
 +	struct kvm_pmc *counters;
 +	u64 ctr;
 +
 +	pmc &= ~(3u << 30);
 +	if (!fixed && pmc >= pmu->nr_arch_gp_counters)
 +		return 1;
 +	if (fixed && pmc >= pmu->nr_arch_fixed_counters)
 +		return 1;
 +	counters = fixed ? pmu->fixed_counters : pmu->gp_counters;
 +	ctr = read_pmc(&counters[pmc]);
 +	if (fast_mode)
 +		ctr = (u32)ctr;
 +	*data = ctr;
 +
 +	return 0;
 +}
 +
 +void kvm_pmu_cpuid_update(struct kvm_vcpu *vcpu)
++=======
+ /* refresh PMU settings. This function generally is called when underlying
+  * settings are changed (such as changes of PMU CPUID by guest VMs), which
+  * should rarely happen.
+  */
+ void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
++>>>>>>> e5af058aacd5 (KVM: x86/vPMU: reorder PMU functions)
  {
  	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
  	struct kvm_cpuid_entry2 *entry;
@@@ -525,50 -576,11 +619,56 @@@ void kvm_pmu_init(struct kvm_vcpu *vcpu
  		pmu->fixed_counters[i].vcpu = vcpu;
  		pmu->fixed_counters[i].idx = i + INTEL_PMC_IDX_FIXED;
  	}
 -	init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
 -	kvm_pmu_refresh(vcpu);
 +	init_irq_work(&pmu->irq_work, trigger_pmi);
 +	kvm_pmu_cpuid_update(vcpu);
 +}
 +
++<<<<<<< HEAD
 +void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 +	int i;
 +
 +	irq_work_sync(&pmu->irq_work);
 +	for (i = 0; i < INTEL_PMC_MAX_GENERIC; i++) {
 +		struct kvm_pmc *pmc = &pmu->gp_counters[i];
 +		stop_counter(pmc);
 +		pmc->counter = pmc->eventsel = 0;
 +	}
 +
 +	for (i = 0; i < INTEL_PMC_MAX_FIXED; i++)
 +		stop_counter(&pmu->fixed_counters[i]);
 +
 +	pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status =
 +		pmu->global_ovf_ctrl = 0;
  }
  
++=======
++>>>>>>> e5af058aacd5 (KVM: x86/vPMU: reorder PMU functions)
  void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
  {
  	kvm_pmu_reset(vcpu);
  }
++<<<<<<< HEAD
 +
 +void kvm_handle_pmu_event(struct kvm_vcpu *vcpu)
 +{
 +	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 +	u64 bitmask;
 +	int bit;
 +
 +	bitmask = pmu->reprogram_pmi;
 +
 +	for_each_set_bit(bit, (unsigned long *)&bitmask, X86_PMC_IDX_MAX) {
 +		struct kvm_pmc *pmc = global_idx_to_pmc(pmu, bit);
 +
 +		if (unlikely(!pmc || !pmc->perf_event)) {
 +			clear_bit(bit, (unsigned long *)&pmu->reprogram_pmi);
 +			continue;
 +		}
 +
 +		reprogram_idx(pmu, bit);
 +	}
 +}
++=======
++>>>>>>> e5af058aacd5 (KVM: x86/vPMU: reorder PMU functions)
* Unmerged path arch/x86/kvm/pmu.c
