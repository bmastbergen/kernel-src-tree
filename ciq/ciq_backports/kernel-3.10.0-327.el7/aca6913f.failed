powerpc/powernv/ioda2: Introduce helpers to allocate TCE pages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] powernv/ioda2: Introduce helpers to allocate TCE pages (David Gibson) [1213665]
Rebuild_FUZZ: 93.10%
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit aca6913f555176363cda21518ff79da7469ebd64
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/aca6913f.failed

This is a part of moving TCE table allocation into an iommu_ops
callback to support multiple IOMMU groups per one VFIO container.

This moves the code which allocates the actual TCE tables to helpers:
pnv_pci_ioda2_table_alloc_pages() and pnv_pci_ioda2_table_free_pages().
These do not allocate/free the iommu_table struct.

This enforces window size to be a power of two.

This should cause no behavioural change.

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
	Reviewed-by: Gavin Shan <gwshan@linux.vnet.ibm.com>
	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit aca6913f555176363cda21518ff79da7469ebd64)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/powernv/pci-ioda.c
diff --cc arch/powerpc/platforms/powernv/pci-ioda.c
index 109e90d84e34,e61610cccf6e..000000000000
--- a/arch/powerpc/platforms/powernv/pci-ioda.c
+++ b/arch/powerpc/platforms/powernv/pci-ioda.c
@@@ -36,10 -38,20 +36,26 @@@
  #include <asm/tce.h>
  #include <asm/xics.h>
  #include <asm/debug.h>
++<<<<<<< HEAD
++=======
+ #include <asm/firmware.h>
+ #include <asm/pnv-pci.h>
+ #include <asm/mmzone.h>
+ 
+ #include <misc/cxl-base.h>
++>>>>>>> aca6913f5551 (powerpc/powernv/ioda2: Introduce helpers to allocate TCE pages)
  
  #include "powernv.h"
  #include "pci.h"
  
++<<<<<<< HEAD
++=======
+ /* 256M DMA window, 4K TCE pages, 8 bytes TCE */
+ #define TCE32_TABLE_SIZE	((0x10000000 / 0x1000) * 8)
+ 
+ static void pnv_pci_ioda2_table_free_pages(struct iommu_table *tbl);
+ 
++>>>>>>> aca6913f5551 (powerpc/powernv/ioda2: Introduce helpers to allocate TCE pages)
  static void pe_level_printk(const struct pnv_ioda_pe *pe, const char *level,
  			    const char *fmt, ...)
  {
@@@ -522,6 -1149,441 +538,444 @@@ static void pnv_pci_ioda_setup_PEs(void
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_PCI_IOV
+ static int pnv_pci_vf_release_m64(struct pci_dev *pdev)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	int                    i, j;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	for (i = 0; i < PCI_SRIOV_NUM_BARS; i++)
+ 		for (j = 0; j < M64_PER_IOV; j++) {
+ 			if (pdn->m64_wins[i][j] == IODA_INVALID_M64)
+ 				continue;
+ 			opal_pci_phb_mmio_enable(phb->opal_id,
+ 				OPAL_M64_WINDOW_TYPE, pdn->m64_wins[i][j], 0);
+ 			clear_bit(pdn->m64_wins[i][j], &phb->ioda.m64_bar_alloc);
+ 			pdn->m64_wins[i][j] = IODA_INVALID_M64;
+ 		}
+ 
+ 	return 0;
+ }
+ 
+ static int pnv_pci_vf_assign_m64(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	unsigned int           win;
+ 	struct resource       *res;
+ 	int                    i, j;
+ 	int64_t                rc;
+ 	int                    total_vfs;
+ 	resource_size_t        size, start;
+ 	int                    pe_num;
+ 	int                    vf_groups;
+ 	int                    vf_per_group;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 	total_vfs = pci_sriov_get_totalvfs(pdev);
+ 
+ 	/* Initialize the m64_wins to IODA_INVALID_M64 */
+ 	for (i = 0; i < PCI_SRIOV_NUM_BARS; i++)
+ 		for (j = 0; j < M64_PER_IOV; j++)
+ 			pdn->m64_wins[i][j] = IODA_INVALID_M64;
+ 
+ 	if (pdn->m64_per_iov == M64_PER_IOV) {
+ 		vf_groups = (num_vfs <= M64_PER_IOV) ? num_vfs: M64_PER_IOV;
+ 		vf_per_group = (num_vfs <= M64_PER_IOV)? 1:
+ 			roundup_pow_of_two(num_vfs) / pdn->m64_per_iov;
+ 	} else {
+ 		vf_groups = 1;
+ 		vf_per_group = 1;
+ 	}
+ 
+ 	for (i = 0; i < PCI_SRIOV_NUM_BARS; i++) {
+ 		res = &pdev->resource[i + PCI_IOV_RESOURCES];
+ 		if (!res->flags || !res->parent)
+ 			continue;
+ 
+ 		if (!pnv_pci_is_mem_pref_64(res->flags))
+ 			continue;
+ 
+ 		for (j = 0; j < vf_groups; j++) {
+ 			do {
+ 				win = find_next_zero_bit(&phb->ioda.m64_bar_alloc,
+ 						phb->ioda.m64_bar_idx + 1, 0);
+ 
+ 				if (win >= phb->ioda.m64_bar_idx + 1)
+ 					goto m64_failed;
+ 			} while (test_and_set_bit(win, &phb->ioda.m64_bar_alloc));
+ 
+ 			pdn->m64_wins[i][j] = win;
+ 
+ 			if (pdn->m64_per_iov == M64_PER_IOV) {
+ 				size = pci_iov_resource_size(pdev,
+ 							PCI_IOV_RESOURCES + i);
+ 				size = size * vf_per_group;
+ 				start = res->start + size * j;
+ 			} else {
+ 				size = resource_size(res);
+ 				start = res->start;
+ 			}
+ 
+ 			/* Map the M64 here */
+ 			if (pdn->m64_per_iov == M64_PER_IOV) {
+ 				pe_num = pdn->offset + j;
+ 				rc = opal_pci_map_pe_mmio_window(phb->opal_id,
+ 						pe_num, OPAL_M64_WINDOW_TYPE,
+ 						pdn->m64_wins[i][j], 0);
+ 			}
+ 
+ 			rc = opal_pci_set_phb_mem_window(phb->opal_id,
+ 						 OPAL_M64_WINDOW_TYPE,
+ 						 pdn->m64_wins[i][j],
+ 						 start,
+ 						 0, /* unused */
+ 						 size);
+ 
+ 
+ 			if (rc != OPAL_SUCCESS) {
+ 				dev_err(&pdev->dev, "Failed to map M64 window #%d: %lld\n",
+ 					win, rc);
+ 				goto m64_failed;
+ 			}
+ 
+ 			if (pdn->m64_per_iov == M64_PER_IOV)
+ 				rc = opal_pci_phb_mmio_enable(phb->opal_id,
+ 				     OPAL_M64_WINDOW_TYPE, pdn->m64_wins[i][j], 2);
+ 			else
+ 				rc = opal_pci_phb_mmio_enable(phb->opal_id,
+ 				     OPAL_M64_WINDOW_TYPE, pdn->m64_wins[i][j], 1);
+ 
+ 			if (rc != OPAL_SUCCESS) {
+ 				dev_err(&pdev->dev, "Failed to enable M64 window #%d: %llx\n",
+ 					win, rc);
+ 				goto m64_failed;
+ 			}
+ 		}
+ 	}
+ 	return 0;
+ 
+ m64_failed:
+ 	pnv_pci_vf_release_m64(pdev);
+ 	return -EBUSY;
+ }
+ 
+ static void pnv_pci_ioda2_release_dma_pe(struct pci_dev *dev, struct pnv_ioda_pe *pe)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct iommu_table    *tbl;
+ 	unsigned long         addr;
+ 	int64_t               rc;
+ 
+ 	bus = dev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	tbl = pe->table_group.tables[0];
+ 	addr = tbl->it_base;
+ 
+ 	opal_pci_map_pe_dma_window(phb->opal_id, pe->pe_number,
+ 				   pe->pe_number << 1, 1, __pa(addr),
+ 				   0, 0x1000);
+ 
+ 	rc = opal_pci_map_pe_dma_window_real(pe->phb->opal_id,
+ 				        pe->pe_number,
+ 				        (pe->pe_number << 1) + 1,
+ 				        pe->tce_bypass_base,
+ 				        0);
+ 	if (rc)
+ 		pe_warn(pe, "OPAL error %ld release DMA window\n", rc);
+ 
+ 	pnv_pci_unlink_table_and_group(tbl, &pe->table_group);
+ 	if (pe->table_group.group) {
+ 		iommu_group_put(pe->table_group.group);
+ 		BUG_ON(pe->table_group.group);
+ 	}
+ 	pnv_pci_ioda2_table_free_pages(tbl);
+ 	iommu_free_table(tbl, of_node_full_name(dev->dev.of_node));
+ }
+ 
+ static void pnv_ioda_release_vf_PE(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pnv_ioda_pe    *pe, *pe_n;
+ 	struct pci_dn         *pdn;
+ 	u16                    vf_index;
+ 	int64_t                rc;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	if (!pdev->is_physfn)
+ 		return;
+ 
+ 	if (pdn->m64_per_iov == M64_PER_IOV && num_vfs > M64_PER_IOV) {
+ 		int   vf_group;
+ 		int   vf_per_group;
+ 		int   vf_index1;
+ 
+ 		vf_per_group = roundup_pow_of_two(num_vfs) / pdn->m64_per_iov;
+ 
+ 		for (vf_group = 0; vf_group < M64_PER_IOV; vf_group++)
+ 			for (vf_index = vf_group * vf_per_group;
+ 				vf_index < (vf_group + 1) * vf_per_group &&
+ 				vf_index < num_vfs;
+ 				vf_index++)
+ 				for (vf_index1 = vf_group * vf_per_group;
+ 					vf_index1 < (vf_group + 1) * vf_per_group &&
+ 					vf_index1 < num_vfs;
+ 					vf_index1++){
+ 
+ 					rc = opal_pci_set_peltv(phb->opal_id,
+ 						pdn->offset + vf_index,
+ 						pdn->offset + vf_index1,
+ 						OPAL_REMOVE_PE_FROM_DOMAIN);
+ 
+ 					if (rc)
+ 					    dev_warn(&pdev->dev, "%s: Failed to unlink same group PE#%d(%lld)\n",
+ 						__func__,
+ 						pdn->offset + vf_index1, rc);
+ 				}
+ 	}
+ 
+ 	list_for_each_entry_safe(pe, pe_n, &phb->ioda.pe_list, list) {
+ 		if (pe->parent_dev != pdev)
+ 			continue;
+ 
+ 		pnv_pci_ioda2_release_dma_pe(pdev, pe);
+ 
+ 		/* Remove from list */
+ 		mutex_lock(&phb->ioda.pe_list_mutex);
+ 		list_del(&pe->list);
+ 		mutex_unlock(&phb->ioda.pe_list_mutex);
+ 
+ 		pnv_ioda_deconfigure_pe(phb, pe);
+ 
+ 		pnv_ioda_free_pe(phb, pe->pe_number);
+ 	}
+ }
+ 
+ void pnv_pci_sriov_disable(struct pci_dev *pdev)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	struct pci_sriov      *iov;
+ 	u16 num_vfs;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 	iov = pdev->sriov;
+ 	num_vfs = pdn->num_vfs;
+ 
+ 	/* Release VF PEs */
+ 	pnv_ioda_release_vf_PE(pdev, num_vfs);
+ 
+ 	if (phb->type == PNV_PHB_IODA2) {
+ 		if (pdn->m64_per_iov == 1)
+ 			pnv_pci_vf_resource_shift(pdev, -pdn->offset);
+ 
+ 		/* Release M64 windows */
+ 		pnv_pci_vf_release_m64(pdev);
+ 
+ 		/* Release PE numbers */
+ 		bitmap_clear(phb->ioda.pe_alloc, pdn->offset, num_vfs);
+ 		pdn->offset = 0;
+ 	}
+ }
+ 
+ static void pnv_pci_ioda2_setup_dma_pe(struct pnv_phb *phb,
+ 				       struct pnv_ioda_pe *pe);
+ static void pnv_ioda_setup_vf_PE(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pnv_ioda_pe    *pe;
+ 	int                    pe_num;
+ 	u16                    vf_index;
+ 	struct pci_dn         *pdn;
+ 	int64_t                rc;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	if (!pdev->is_physfn)
+ 		return;
+ 
+ 	/* Reserve PE for each VF */
+ 	for (vf_index = 0; vf_index < num_vfs; vf_index++) {
+ 		pe_num = pdn->offset + vf_index;
+ 
+ 		pe = &phb->ioda.pe_array[pe_num];
+ 		pe->pe_number = pe_num;
+ 		pe->phb = phb;
+ 		pe->flags = PNV_IODA_PE_VF;
+ 		pe->pbus = NULL;
+ 		pe->parent_dev = pdev;
+ 		pe->tce32_seg = -1;
+ 		pe->mve_number = -1;
+ 		pe->rid = (pci_iov_virtfn_bus(pdev, vf_index) << 8) |
+ 			   pci_iov_virtfn_devfn(pdev, vf_index);
+ 
+ 		pe_info(pe, "VF %04d:%02d:%02d.%d associated with PE#%d\n",
+ 			hose->global_number, pdev->bus->number,
+ 			PCI_SLOT(pci_iov_virtfn_devfn(pdev, vf_index)),
+ 			PCI_FUNC(pci_iov_virtfn_devfn(pdev, vf_index)), pe_num);
+ 
+ 		if (pnv_ioda_configure_pe(phb, pe)) {
+ 			/* XXX What do we do here ? */
+ 			if (pe_num)
+ 				pnv_ioda_free_pe(phb, pe_num);
+ 			pe->pdev = NULL;
+ 			continue;
+ 		}
+ 
+ 		/* Put PE to the list */
+ 		mutex_lock(&phb->ioda.pe_list_mutex);
+ 		list_add_tail(&pe->list, &phb->ioda.pe_list);
+ 		mutex_unlock(&phb->ioda.pe_list_mutex);
+ 
+ 		pnv_pci_ioda2_setup_dma_pe(phb, pe);
+ 	}
+ 
+ 	if (pdn->m64_per_iov == M64_PER_IOV && num_vfs > M64_PER_IOV) {
+ 		int   vf_group;
+ 		int   vf_per_group;
+ 		int   vf_index1;
+ 
+ 		vf_per_group = roundup_pow_of_two(num_vfs) / pdn->m64_per_iov;
+ 
+ 		for (vf_group = 0; vf_group < M64_PER_IOV; vf_group++) {
+ 			for (vf_index = vf_group * vf_per_group;
+ 			     vf_index < (vf_group + 1) * vf_per_group &&
+ 			     vf_index < num_vfs;
+ 			     vf_index++) {
+ 				for (vf_index1 = vf_group * vf_per_group;
+ 				     vf_index1 < (vf_group + 1) * vf_per_group &&
+ 				     vf_index1 < num_vfs;
+ 				     vf_index1++) {
+ 
+ 					rc = opal_pci_set_peltv(phb->opal_id,
+ 						pdn->offset + vf_index,
+ 						pdn->offset + vf_index1,
+ 						OPAL_ADD_PE_TO_DOMAIN);
+ 
+ 					if (rc)
+ 					    dev_warn(&pdev->dev, "%s: Failed to link same group PE#%d(%lld)\n",
+ 						__func__,
+ 						pdn->offset + vf_index1, rc);
+ 				}
+ 			}
+ 		}
+ 	}
+ }
+ 
+ int pnv_pci_sriov_enable(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	int                    ret;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	if (phb->type == PNV_PHB_IODA2) {
+ 		/* Calculate available PE for required VFs */
+ 		mutex_lock(&phb->ioda.pe_alloc_mutex);
+ 		pdn->offset = bitmap_find_next_zero_area(
+ 			phb->ioda.pe_alloc, phb->ioda.total_pe,
+ 			0, num_vfs, 0);
+ 		if (pdn->offset >= phb->ioda.total_pe) {
+ 			mutex_unlock(&phb->ioda.pe_alloc_mutex);
+ 			dev_info(&pdev->dev, "Failed to enable VF%d\n", num_vfs);
+ 			pdn->offset = 0;
+ 			return -EBUSY;
+ 		}
+ 		bitmap_set(phb->ioda.pe_alloc, pdn->offset, num_vfs);
+ 		pdn->num_vfs = num_vfs;
+ 		mutex_unlock(&phb->ioda.pe_alloc_mutex);
+ 
+ 		/* Assign M64 window accordingly */
+ 		ret = pnv_pci_vf_assign_m64(pdev, num_vfs);
+ 		if (ret) {
+ 			dev_info(&pdev->dev, "Not enough M64 window resources\n");
+ 			goto m64_failed;
+ 		}
+ 
+ 		/*
+ 		 * When using one M64 BAR to map one IOV BAR, we need to shift
+ 		 * the IOV BAR according to the PE# allocated to the VFs.
+ 		 * Otherwise, the PE# for the VF will conflict with others.
+ 		 */
+ 		if (pdn->m64_per_iov == 1) {
+ 			ret = pnv_pci_vf_resource_shift(pdev, pdn->offset);
+ 			if (ret)
+ 				goto m64_failed;
+ 		}
+ 	}
+ 
+ 	/* Setup VF PEs */
+ 	pnv_ioda_setup_vf_PE(pdev, num_vfs);
+ 
+ 	return 0;
+ 
+ m64_failed:
+ 	bitmap_clear(phb->ioda.pe_alloc, pdn->offset, num_vfs);
+ 	pdn->offset = 0;
+ 
+ 	return ret;
+ }
+ 
+ int pcibios_sriov_disable(struct pci_dev *pdev)
+ {
+ 	pnv_pci_sriov_disable(pdev);
+ 
+ 	/* Release PCI data */
+ 	remove_dev_pci_data(pdev);
+ 	return 0;
+ }
+ 
+ int pcibios_sriov_enable(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	/* Allocate PCI data */
+ 	add_dev_pci_data(pdev);
+ 
+ 	pnv_pci_sriov_enable(pdev, num_vfs);
+ 	return 0;
+ }
+ #endif /* CONFIG_PCI_IOV */
+ 
++>>>>>>> aca6913f5551 (powerpc/powernv/ioda2: Introduce helpers to allocate TCE pages)
  static void pnv_pci_ioda_dma_dev_setup(struct pnv_phb *phb, struct pci_dev *pdev)
  {
  	struct pci_dn *pdn = pci_get_pdn(pdev);
@@@ -799,50 -1998,133 +1253,109 @@@ static void pnv_pci_ioda2_set_bypass(st
  		pe->tce_bypass_enabled = enable;
  }
  
 -#ifdef CONFIG_IOMMU_API
 -static void pnv_ioda2_take_ownership(struct iommu_table_group *table_group)
 -{
 -	struct pnv_ioda_pe *pe = container_of(table_group, struct pnv_ioda_pe,
 -						table_group);
 -
 -	iommu_take_ownership(table_group->tables[0]);
 -	pnv_pci_ioda2_set_bypass(pe, false);
 -}
 -
 -static void pnv_ioda2_release_ownership(struct iommu_table_group *table_group)
 -{
 -	struct pnv_ioda_pe *pe = container_of(table_group, struct pnv_ioda_pe,
 -						table_group);
 -
 -	iommu_release_ownership(table_group->tables[0]);
 -	pnv_pci_ioda2_set_bypass(pe, true);
 -}
 -
 -static struct iommu_table_group_ops pnv_pci_ioda2_ops = {
 -	.take_ownership = pnv_ioda2_take_ownership,
 -	.release_ownership = pnv_ioda2_release_ownership,
 -};
 -#endif
 -
 -static void pnv_pci_ioda_setup_opal_tce_kill(struct pnv_phb *phb)
 +static void pnv_pci_ioda2_setup_bypass_pe(struct pnv_phb *phb,
 +					  struct pnv_ioda_pe *pe)
  {
 -	const __be64 *swinvp;
 +	/* TVE #1 is selected by PCI address bit 59 */
 +	pe->tce_bypass_base = 1ull << 59;
  
 -	/* OPAL variant of PHB3 invalidated TCEs */
 -	swinvp = of_get_property(phb->hose->dn, "ibm,opal-tce-kill", NULL);
 -	if (!swinvp)
 -		return;
 +	/* Install set_bypass callback for VFIO */
 +	pe->tce32_table.set_bypass = pnv_pci_ioda2_set_bypass;
  
 -	phb->ioda.tce_inval_reg_phys = be64_to_cpup(swinvp);
 -	phb->ioda.tce_inval_reg = ioremap(phb->ioda.tce_inval_reg_phys, 8);
 +	/* Enable bypass by default */
 +	pnv_pci_ioda2_set_bypass(&pe->tce32_table, true);
  }
  
+ static __be64 *pnv_pci_ioda2_table_do_alloc_pages(int nid, unsigned shift)
+ {
+ 	struct page *tce_mem = NULL;
+ 	__be64 *addr;
+ 	unsigned order = max_t(unsigned, shift, PAGE_SHIFT) - PAGE_SHIFT;
+ 
+ 	tce_mem = alloc_pages_node(nid, GFP_KERNEL, order);
+ 	if (!tce_mem) {
+ 		pr_err("Failed to allocate a TCE memory, order=%d\n", order);
+ 		return NULL;
+ 	}
+ 	addr = page_address(tce_mem);
+ 	memset(addr, 0, 1UL << (order + PAGE_SHIFT));
+ 
+ 	return addr;
+ }
+ 
+ static long pnv_pci_ioda2_table_alloc_pages(int nid, __u64 bus_offset,
+ 		__u32 page_shift, __u64 window_size, struct iommu_table *tbl)
+ {
+ 	void *addr;
+ 	const unsigned window_shift = ilog2(window_size);
+ 	unsigned entries_shift = window_shift - page_shift;
+ 	unsigned table_shift = max_t(unsigned, entries_shift + 3, PAGE_SHIFT);
+ 	const unsigned long tce_table_size = 1UL << table_shift;
+ 
+ 	if ((window_size > memory_hotplug_max()) || !is_power_of_2(window_size))
+ 		return -EINVAL;
+ 
+ 	/* Allocate TCE table */
+ 	addr = pnv_pci_ioda2_table_do_alloc_pages(nid, table_shift);
+ 	if (!addr)
+ 		return -ENOMEM;
+ 
+ 	/* Setup linux iommu table */
+ 	pnv_pci_setup_iommu_table(tbl, addr, tce_table_size, bus_offset,
+ 			page_shift);
+ 
+ 	pr_devel("Created TCE table: ws=%08llx ts=%lx @%08llx\n",
+ 			window_size, tce_table_size, bus_offset);
+ 
+ 	return 0;
+ }
+ 
+ static void pnv_pci_ioda2_table_free_pages(struct iommu_table *tbl)
+ {
+ 	if (!tbl->it_size)
+ 		return;
+ 
+ 	free_pages(tbl->it_base, get_order(tbl->it_size << 3));
+ }
+ 
  static void pnv_pci_ioda2_setup_dma_pe(struct pnv_phb *phb,
  				       struct pnv_ioda_pe *pe)
  {
++<<<<<<< HEAD
 +	struct page *tce_mem = NULL;
 +	void *addr;
 +	const __be64 *swinvp;
++=======
++>>>>>>> aca6913f5551 (powerpc/powernv/ioda2: Introduce helpers to allocate TCE pages)
  	struct iommu_table *tbl;
- 	unsigned int tce_table_size, end;
  	int64_t rc;
  
  	/* We shouldn't already have a 32-bit DMA associated */
  	if (WARN_ON(pe->tce32_seg >= 0))
  		return;
  
 -	/* TVE #1 is selected by PCI address bit 59 */
 -	pe->tce_bypass_base = 1ull << 59;
 -
 -	tbl = pnv_pci_table_alloc(phb->hose->node);
 -	iommu_register_group(&pe->table_group, phb->hose->global_number,
 -			pe->pe_number);
 -	pnv_pci_link_table_and_group(phb->hose->node, 0, tbl, &pe->table_group);
 -
  	/* The PE will reserve all possible 32-bits space */
  	pe->tce32_seg = 0;
- 	end = (1 << ilog2(phb->ioda.m32_pci_base));
- 	tce_table_size = (end / 0x1000) * 8;
  	pe_info(pe, "Setting up 32-bit TCE table at 0..%08x\n",
- 		end);
+ 		phb->ioda.m32_pci_base);
  
- 	/* Allocate TCE table */
- 	tce_mem = alloc_pages_node(phb->hose->node, GFP_KERNEL,
- 				   get_order(tce_table_size));
- 	if (!tce_mem) {
- 		pe_err(pe, "Failed to allocate a 32-bit TCE memory\n");
++<<<<<<< HEAD
++=======
+ 	/* Setup linux iommu table */
+ 	rc = pnv_pci_ioda2_table_alloc_pages(pe->phb->hose->node,
+ 			0, IOMMU_PAGE_SHIFT_4K, phb->ioda.m32_pci_base, tbl);
+ 	if (rc) {
+ 		pe_err(pe, "Failed to create 32-bit TCE table, err %ld", rc);
  		goto fail;
  	}
- 	addr = page_address(tce_mem);
- 	memset(addr, 0, tce_table_size);
  
+ 	tbl->it_ops = &pnv_ioda2_iommu_ops;
+ 	iommu_init_table(tbl, phb->hose->node);
+ #ifdef CONFIG_IOMMU_API
+ 	pe->table_group.ops = &pnv_pci_ioda2_ops;
+ #endif
+ 
++>>>>>>> aca6913f5551 (powerpc/powernv/ioda2: Introduce helpers to allocate TCE pages)
  	/*
  	 * Map TCE table through TVT. The TVE index is the PE number
  	 * shifted by 1 bit for 32-bits DMA space.
@@@ -886,8 -2163,11 +1399,16 @@@
  fail:
  	if (pe->tce32_seg >= 0)
  		pe->tce32_seg = -1;
++<<<<<<< HEAD
 +	if (tce_mem)
 +		__free_pages(tce_mem, get_order(tce_table_size));
++=======
+ 	if (tbl) {
+ 		pnv_pci_ioda2_table_free_pages(tbl);
+ 		pnv_pci_unlink_table_and_group(tbl, &pe->table_group);
+ 		iommu_free_table(tbl, "pnv");
+ 	}
++>>>>>>> aca6913f5551 (powerpc/powernv/ioda2: Introduce helpers to allocate TCE pages)
  }
  
  static void pnv_ioda_setup_dma(struct pnv_phb *phb)
* Unmerged path arch/powerpc/platforms/powernv/pci-ioda.c
