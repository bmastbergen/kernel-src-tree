x86/asm/entry/64: Remove a bogus 'ret_from_fork' optimization

jira LE-1907
cve CVE-2015-2830
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [x86] kernel: Remove a bogus 'ret_from_fork' optimization (Mateusz Guzik) [1209235] {CVE-2015-2830}
Rebuild_FUZZ: 83.93%
commit-author Andy Lutomirski <luto@amacapital.net>
commit 1e3fbb8a1d814f35e2e689cf87714d38d9f3564d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/1e3fbb8a.failed

'ret_from_fork' checks TIF_IA32 to determine whether 'pt_regs' and
the related state make sense for 'ret_from_sys_call'.  This is
entirely the wrong check.  TS_COMPAT would make a little more
sense, but there's really no point in keeping this optimization
at all.

This fixes a return to the wrong user CS if we came from int
0x80 in a 64-bit task.

	Signed-off-by: Andy Lutomirski <luto@amacapital.net>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/4710be56d76ef994ddf59087aad98c000fbab9a4.1424989793.git.luto@amacapital.net
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 1e3fbb8a1d814f35e2e689cf87714d38d9f3564d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/entry_64.S
diff --cc arch/x86/kernel/entry_64.S
index 76be67905b79,622ce4254893..000000000000
--- a/arch/x86/kernel/entry_64.S
+++ b/arch/x86/kernel/entry_64.S
@@@ -730,6 -515,60 +730,49 @@@ END(stub_x32_execve
  #endif
  
  /*
++<<<<<<< HEAD
++=======
+  * A newly forked process directly context switches into this address.
+  *
+  * rdi: prev task we switched from
+  */
+ ENTRY(ret_from_fork)
+ 	DEFAULT_FRAME
+ 
+ 	LOCK ; btr $TIF_FORK,TI_flags(%r8)
+ 
+ 	pushq_cfi $0x0002
+ 	popfq_cfi				# reset kernel eflags
+ 
+ 	call schedule_tail			# rdi: 'prev' task parameter
+ 
+ 	GET_THREAD_INFO(%rcx)
+ 
+ 	RESTORE_EXTRA_REGS
+ 
+ 	testl $3,CS(%rsp)			# from kernel_thread?
+ 	jz   1f
+ 
+ 	/*
+ 	 * By the time we get here, we have no idea whether our pt_regs,
+ 	 * ti flags, and ti status came from the 64-bit SYSCALL fast path,
+ 	 * the slow path, or one of the ia32entry paths.
+ 	 * Use int_ret_from_sys_call to return, since it can safely handle
+ 	 * all of the above.
+ 	 */
+ 	jmp  int_ret_from_sys_call
+ 
+ 1:
+ 	movq %rbp, %rdi
+ 	call *%rbx
+ 	movl $0, RAX(%rsp)
+ 	RESTORE_EXTRA_REGS
+ 	jmp int_ret_from_sys_call
+ 	CFI_ENDPROC
+ END(ret_from_fork)
+ 
+ /*
++>>>>>>> 1e3fbb8a1d81 (x86/asm/entry/64: Remove a bogus 'ret_from_fork' optimization)
   * Build the entry stubs and pointer table with some assembler magic.
   * We pack 7 stubs into a single 32-byte chunk, which will fit in a
   * single cache line on all modern x86 implementations.
* Unmerged path arch/x86/kernel/entry_64.S
