KVM: MMU: Add mmu help functions to support PML

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [kvm] mmu: Add mmu help functions to support PML (Bandan Das) [1209995]
Rebuild_FUZZ: 94.38%
commit-author Kai Huang <kai.huang@linux.intel.com>
commit f4b4b1808690c37c7c703d43789c1988c5e7fdeb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/f4b4b180.failed

This patch adds new mmu layer functions to clear/set D-bit for memory slot, and
to write protect superpages for memory slot.

In case of PML, CPU logs the dirty GPA automatically to PML buffer when CPU
updates D-bit from 0 to 1, therefore we don't have to write protect 4K pages,
instead, we only need to clear D-bit in order to log that GPA.

For superpages, we still write protect it and let page fault code to handle
dirty page logging, as we still need to split superpage to 4K pages in PML.

As PML is always enabled during guest's lifetime, to eliminate unnecessary PML
GPA logging, we set D-bit manually for the slot with dirty logging disabled.

	Signed-off-by: Kai Huang <kai.huang@linux.intel.com>
	Reviewed-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f4b4b1808690c37c7c703d43789c1988c5e7fdeb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 2e21e1e0dc32,c438224cca34..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -1215,8 -1215,62 +1215,62 @@@ static bool __rmap_write_protect(struc
  	return flush;
  }
  
+ static bool spte_clear_dirty(struct kvm *kvm, u64 *sptep)
+ {
+ 	u64 spte = *sptep;
+ 
+ 	rmap_printk("rmap_clear_dirty: spte %p %llx\n", sptep, *sptep);
+ 
+ 	spte &= ~shadow_dirty_mask;
+ 
+ 	return mmu_spte_update(sptep, spte);
+ }
+ 
+ static bool __rmap_clear_dirty(struct kvm *kvm, unsigned long *rmapp)
+ {
+ 	u64 *sptep;
+ 	struct rmap_iterator iter;
+ 	bool flush = false;
+ 
+ 	for (sptep = rmap_get_first(*rmapp, &iter); sptep;) {
+ 		BUG_ON(!(*sptep & PT_PRESENT_MASK));
+ 
+ 		flush |= spte_clear_dirty(kvm, sptep);
+ 		sptep = rmap_get_next(&iter);
+ 	}
+ 
+ 	return flush;
+ }
+ 
+ static bool spte_set_dirty(struct kvm *kvm, u64 *sptep)
+ {
+ 	u64 spte = *sptep;
+ 
+ 	rmap_printk("rmap_set_dirty: spte %p %llx\n", sptep, *sptep);
+ 
+ 	spte |= shadow_dirty_mask;
+ 
+ 	return mmu_spte_update(sptep, spte);
+ }
+ 
+ static bool __rmap_set_dirty(struct kvm *kvm, unsigned long *rmapp)
+ {
+ 	u64 *sptep;
+ 	struct rmap_iterator iter;
+ 	bool flush = false;
+ 
+ 	for (sptep = rmap_get_first(*rmapp, &iter); sptep;) {
+ 		BUG_ON(!(*sptep & PT_PRESENT_MASK));
+ 
+ 		flush |= spte_set_dirty(kvm, sptep);
+ 		sptep = rmap_get_next(&iter);
+ 	}
+ 
+ 	return flush;
+ }
+ 
  /**
 - * kvm_mmu_write_protect_pt_masked - write protect selected PT level pages
 + * kvm_arch_mmu_write_protect_pt_masked - write protect selected PT level pages
   * @kvm: kvm instance
   * @slot: slot to protect
   * @gfn_offset: start of the BITS_PER_LONG pages we care about
@@@ -1241,6 -1295,49 +1295,52 @@@ void kvm_arch_mmu_write_protect_pt_mask
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * kvm_mmu_clear_dirty_pt_masked - clear MMU D-bit for PT level pages
+  * @kvm: kvm instance
+  * @slot: slot to clear D-bit
+  * @gfn_offset: start of the BITS_PER_LONG pages we care about
+  * @mask: indicates which pages we should clear D-bit
+  *
+  * Used for PML to re-log the dirty GPAs after userspace querying dirty_bitmap.
+  */
+ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
+ 				     struct kvm_memory_slot *slot,
+ 				     gfn_t gfn_offset, unsigned long mask)
+ {
+ 	unsigned long *rmapp;
+ 
+ 	while (mask) {
+ 		rmapp = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ 				      PT_PAGE_TABLE_LEVEL, slot);
+ 		__rmap_clear_dirty(kvm, rmapp);
+ 
+ 		/* clear the first set bit */
+ 		mask &= mask - 1;
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_clear_dirty_pt_masked);
+ 
+ /**
+  * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected
+  * PT level pages.
+  *
+  * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to
+  * enable dirty logging for them.
+  *
+  * Used when we do not need to care about huge page mappings: e.g. during dirty
+  * logging we do not have any such mappings.
+  */
+ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
+ 				struct kvm_memory_slot *slot,
+ 				gfn_t gfn_offset, unsigned long mask)
+ {
+ 	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+ }
+ 
++>>>>>>> f4b4b1808690 (KVM: MMU: Add mmu help functions to support PML)
  static bool rmap_write_protect(struct kvm *kvm, u64 gfn)
  {
  	struct kvm_memory_slot *slot;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 85892f251941..ef31f0813c7c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -825,6 +825,15 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
+void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
+				   struct kvm_memory_slot *memslot);
+void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
+					struct kvm_memory_slot *memslot);
+void kvm_mmu_slot_set_dirty(struct kvm *kvm,
+			    struct kvm_memory_slot *memslot);
+void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
+				   struct kvm_memory_slot *slot,
+				   gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
* Unmerged path arch/x86/kvm/mmu.c
