hpsa: cleanup reset

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Webb Scales <webbnh@hp.com>
commit d604f5336aee7e67377bdbcd354ea6a7d3979dcb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/d604f533.failed

Synchronize completion the reset with completion of outstanding commands

Extending the newly-added synchronous abort functionality,
now also synchronize resets with the completion of outstanding commands.
Rename the wait queue to reflect the fact that it's being used for both
types of waits.  Also, don't complete commands which are terminated
due to a reset operation.

fix for controller lockup during reset

	Reviewed-by: Scott Teel <scott.teel@pmcs.com>
	Reviewed-by: Kevin Barnett <kevin.barnett@pmcs.com>
	Reviewed-by: Tomas Henzl <thenzl@redhat.com>
	Reviewed-by: Hannes Reinecke <hare@Suse.de>
	Signed-off-by: Webb Scales <webbnh@hp.com>
	Signed-off-by: Don Brace <don.brace@pmcs.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: James Bottomley <JBottomley@Odin.com>
(cherry picked from commit d604f5336aee7e67377bdbcd354ea6a7d3979dcb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/hpsa.c
#	drivers/scsi/hpsa.h
#	drivers/scsi/hpsa_cmd.h
diff --cc drivers/scsi/hpsa.c
index 86152e9b07dd,e037c14a8019..000000000000
--- a/drivers/scsi/hpsa.c
+++ b/drivers/scsi/hpsa.c
@@@ -275,6 -278,38 +275,41 @@@ static inline struct ctlr_info *shost_t
  	return (struct ctlr_info *) *priv;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool hpsa_is_cmd_idle(struct CommandList *c)
+ {
+ 	return c->scsi_cmd == SCSI_CMD_IDLE;
+ }
+ 
+ static inline bool hpsa_is_pending_event(struct CommandList *c)
+ {
+ 	return c->abort_pending || c->reset_pending;
+ }
+ 
+ /* extract sense key, asc, and ascq from sense data.  -1 means invalid. */
+ static void decode_sense_data(const u8 *sense_data, int sense_data_len,
+ 			u8 *sense_key, u8 *asc, u8 *ascq)
+ {
+ 	struct scsi_sense_hdr sshdr;
+ 	bool rc;
+ 
+ 	*sense_key = -1;
+ 	*asc = -1;
+ 	*ascq = -1;
+ 
+ 	if (sense_data_len < 1)
+ 		return;
+ 
+ 	rc = scsi_normalize_sense(sense_data, sense_data_len, &sshdr);
+ 	if (rc) {
+ 		*sense_key = sshdr.sense_key;
+ 		*asc = sshdr.asc;
+ 		*ascq = sshdr.ascq;
+ 	}
+ }
+ 
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  static int check_for_unit_attention(struct ctlr_info *h,
  	struct CommandList *c)
  {
@@@ -868,6 -980,14 +903,17 @@@ static void enqueue_cmd_and_start_io(st
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void enqueue_cmd_and_start_io(struct ctlr_info *h, struct CommandList *c)
+ {
+ 	if (unlikely(hpsa_is_pending_event(c)))
+ 		return finish_cmd(c);
+ 
+ 	__enqueue_cmd_and_start_io(h, c, DEFAULT_REPLY_QUEUE);
+ }
+ 
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  static inline int is_hba_lunid(unsigned char scsi3addr[])
  {
  	return memcmp(scsi3addr, RAID_CTLR_LUNID, 8) == 0;
@@@ -1273,6 -1431,98 +1319,101 @@@ static void hpsa_show_volume_status(str
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Figure the list of physical drive pointers for a logical drive with
+  * raid offload configured.
+  */
+ static void hpsa_figure_phys_disk_ptrs(struct ctlr_info *h,
+ 				struct hpsa_scsi_dev_t *dev[], int ndevices,
+ 				struct hpsa_scsi_dev_t *logical_drive)
+ {
+ 	struct raid_map_data *map = &logical_drive->raid_map;
+ 	struct raid_map_disk_data *dd = &map->data[0];
+ 	int i, j;
+ 	int total_disks_per_row = le16_to_cpu(map->data_disks_per_row) +
+ 				le16_to_cpu(map->metadata_disks_per_row);
+ 	int nraid_map_entries = le16_to_cpu(map->row_cnt) *
+ 				le16_to_cpu(map->layout_map_count) *
+ 				total_disks_per_row;
+ 	int nphys_disk = le16_to_cpu(map->layout_map_count) *
+ 				total_disks_per_row;
+ 	int qdepth;
+ 
+ 	if (nraid_map_entries > RAID_MAP_MAX_ENTRIES)
+ 		nraid_map_entries = RAID_MAP_MAX_ENTRIES;
+ 
+ 	logical_drive->nphysical_disks = nraid_map_entries;
+ 
+ 	qdepth = 0;
+ 	for (i = 0; i < nraid_map_entries; i++) {
+ 		logical_drive->phys_disk[i] = NULL;
+ 		if (!logical_drive->offload_config)
+ 			continue;
+ 		for (j = 0; j < ndevices; j++) {
+ 			if (dev[j]->devtype != TYPE_DISK)
+ 				continue;
+ 			if (is_logical_dev_addr_mode(dev[j]->scsi3addr))
+ 				continue;
+ 			if (dev[j]->ioaccel_handle != dd[i].ioaccel_handle)
+ 				continue;
+ 
+ 			logical_drive->phys_disk[i] = dev[j];
+ 			if (i < nphys_disk)
+ 				qdepth = min(h->nr_cmds, qdepth +
+ 				    logical_drive->phys_disk[i]->queue_depth);
+ 			break;
+ 		}
+ 
+ 		/*
+ 		 * This can happen if a physical drive is removed and
+ 		 * the logical drive is degraded.  In that case, the RAID
+ 		 * map data will refer to a physical disk which isn't actually
+ 		 * present.  And in that case offload_enabled should already
+ 		 * be 0, but we'll turn it off here just in case
+ 		 */
+ 		if (!logical_drive->phys_disk[i]) {
+ 			logical_drive->offload_enabled = 0;
+ 			logical_drive->offload_to_be_enabled = 0;
+ 			logical_drive->queue_depth = 8;
+ 		}
+ 	}
+ 	if (nraid_map_entries)
+ 		/*
+ 		 * This is correct for reads, too high for full stripe writes,
+ 		 * way too high for partial stripe writes
+ 		 */
+ 		logical_drive->queue_depth = qdepth;
+ 	else
+ 		logical_drive->queue_depth = h->nr_cmds;
+ }
+ 
+ static void hpsa_update_log_drive_phys_drive_ptrs(struct ctlr_info *h,
+ 				struct hpsa_scsi_dev_t *dev[], int ndevices)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ndevices; i++) {
+ 		if (dev[i]->devtype != TYPE_DISK)
+ 			continue;
+ 		if (!is_logical_dev_addr_mode(dev[i]->scsi3addr))
+ 			continue;
+ 
+ 		/*
+ 		 * If offload is currently enabled, the RAID map and
+ 		 * phys_disk[] assignment *better* not be changing
+ 		 * and since it isn't changing, we do not need to
+ 		 * update it.
+ 		 */
+ 		if (dev[i]->offload_enabled)
+ 			continue;
+ 
+ 		hpsa_figure_phys_disk_ptrs(h, dev, ndevices, dev[i]);
+ 	}
+ }
+ 
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  static void adjust_hpsa_scsi_table(struct ctlr_info *h, int hostno,
  	struct hpsa_scsi_dev_t *sd[], int nsds)
  {
@@@ -1643,6 -1998,87 +1784,90 @@@ static int handle_ioaccel_mode2_error(s
  	return retry;	/* retry on raid path? */
  }
  
++<<<<<<< HEAD
++=======
+ static void hpsa_cmd_resolve_events(struct ctlr_info *h,
+ 		struct CommandList *c)
+ {
+ 	bool do_wake = false;
+ 
+ 	/*
+ 	 * Prevent the following race in the abort handler:
+ 	 *
+ 	 * 1. LLD is requested to abort a SCSI command
+ 	 * 2. The SCSI command completes
+ 	 * 3. The struct CommandList associated with step 2 is made available
+ 	 * 4. New I/O request to LLD to another LUN re-uses struct CommandList
+ 	 * 5. Abort handler follows scsi_cmnd->host_scribble and
+ 	 *    finds struct CommandList and tries to aborts it
+ 	 * Now we have aborted the wrong command.
+ 	 *
+ 	 * Reset c->scsi_cmd here so that the abort or reset handler will know
+ 	 * this command has completed.  Then, check to see if the handler is
+ 	 * waiting for this command, and, if so, wake it.
+ 	 */
+ 	c->scsi_cmd = SCSI_CMD_IDLE;
+ 	mb();	/* Declare command idle before checking for pending events. */
+ 	if (c->abort_pending) {
+ 		do_wake = true;
+ 		c->abort_pending = false;
+ 	}
+ 	if (c->reset_pending) {
+ 		unsigned long flags;
+ 		struct hpsa_scsi_dev_t *dev;
+ 
+ 		/*
+ 		 * There appears to be a reset pending; lock the lock and
+ 		 * reconfirm.  If so, then decrement the count of outstanding
+ 		 * commands and wake the reset command if this is the last one.
+ 		 */
+ 		spin_lock_irqsave(&h->lock, flags);
+ 		dev = c->reset_pending;		/* Re-fetch under the lock. */
+ 		if (dev && atomic_dec_and_test(&dev->reset_cmds_out))
+ 			do_wake = true;
+ 		c->reset_pending = NULL;
+ 		spin_unlock_irqrestore(&h->lock, flags);
+ 	}
+ 
+ 	if (do_wake)
+ 		wake_up_all(&h->event_sync_wait_queue);
+ }
+ 
+ static void hpsa_cmd_resolve_and_free(struct ctlr_info *h,
+ 				      struct CommandList *c)
+ {
+ 	hpsa_cmd_resolve_events(h, c);
+ 	cmd_tagged_free(h, c);
+ }
+ 
+ static void hpsa_cmd_free_and_done(struct ctlr_info *h,
+ 		struct CommandList *c, struct scsi_cmnd *cmd)
+ {
+ 	hpsa_cmd_resolve_and_free(h, c);
+ 	cmd->scsi_done(cmd);
+ }
+ 
+ static void hpsa_retry_cmd(struct ctlr_info *h, struct CommandList *c)
+ {
+ 	INIT_WORK(&c->work, hpsa_command_resubmit_worker);
+ 	queue_work_on(raw_smp_processor_id(), h->resubmit_wq, &c->work);
+ }
+ 
+ static void hpsa_set_scsi_cmd_aborted(struct scsi_cmnd *cmd)
+ {
+ 	cmd->result = DID_ABORT << 16;
+ }
+ 
+ static void hpsa_cmd_abort_and_free(struct ctlr_info *h, struct CommandList *c,
+ 				    struct scsi_cmnd *cmd)
+ {
+ 	hpsa_set_scsi_cmd_aborted(cmd);
+ 	dev_warn(&h->pdev->dev, "CDB %16phN was aborted with status 0x%x\n",
+ 			 c->Request.CDB, c->err_info->ScsiStatus);
+ 	hpsa_cmd_resolve_and_free(h, c);
+ }
+ 
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  static void process_ioaccel2_completion(struct ctlr_info *h,
  		struct CommandList *c, struct scsi_cmnd *cmd,
  		struct hpsa_scsi_dev_t *dev)
@@@ -1652,13 -2087,11 +1877,18 @@@
  
  	/* check for good status */
  	if (likely(c2->error_data.serv_response == 0 &&
 -			c2->error_data.status == 0))
 -		return hpsa_cmd_free_and_done(h, c, cmd);
 +			c2->error_data.status == 0)) {
 +		cmd_free(h, c);
 +		cmd->scsi_done(cmd);
 +		return;
 +	}
  
++<<<<<<< HEAD
 +	/* Any RAID offload error results in retry which will use
++=======
+ 	/*
+ 	 * Any RAID offload error results in retry which will use
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  	 * the normal I/O path so the controller can handle whatever's
  	 * wrong.
  	 */
@@@ -1710,6 -2166,31 +1940,30 @@@ static void complete_scsi_command(struc
  	cmd->result = (DID_OK << 16); 		/* host byte */
  	cmd->result |= (COMMAND_COMPLETE << 8);	/* msg byte */
  
++<<<<<<< HEAD
++=======
+ 	if (cp->cmd_type == CMD_IOACCEL2 || cp->cmd_type == CMD_IOACCEL1)
+ 		atomic_dec(&cp->phys_disk->ioaccel_cmds_out);
+ 
+ 	/*
+ 	 * We check for lockup status here as it may be set for
+ 	 * CMD_SCSI, CMD_IOACCEL1 and CMD_IOACCEL2 commands by
+ 	 * fail_all_oustanding_cmds()
+ 	 */
+ 	if (unlikely(ei->CommandStatus == CMD_CTLR_LOCKUP)) {
+ 		/* DID_NO_CONNECT will prevent a retry */
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		return hpsa_cmd_free_and_done(h, cp, cmd);
+ 	}
+ 
+ 	if ((unlikely(hpsa_is_pending_event(cp)))) {
+ 		if (cp->reset_pending)
+ 			return hpsa_cmd_resolve_and_free(h, cp);
+ 		if (cp->abort_pending)
+ 			return hpsa_cmd_abort_and_free(h, cp, cmd);
+ 	}
+ 
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  	if (cp->cmd_type == CMD_IOACCEL2)
  		return process_ioaccel2_completion(h, cp, cmd, dev);
  
@@@ -1752,10 -2218,7 +2006,14 @@@
  		if (is_logical_dev_addr_mode(dev->scsi3addr)) {
  			if (ei->CommandStatus == CMD_IOACCEL_DISABLED)
  				dev->offload_enabled = 0;
++<<<<<<< HEAD
 +			cmd->result = DID_SOFT_ERROR << 16;
 +			cmd_free(h, cp);
 +			cmd->scsi_done(cmd);
 +			return;
++=======
+ 			return hpsa_retry_cmd(h, cp);
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  		}
  	}
  
@@@ -2935,6 -3583,36 +3311,39 @@@ static int hpsa_hba_mode_enabled(struc
  	return hba_mode_enabled;
  }
  
++<<<<<<< HEAD
++=======
+ /* get physical drive ioaccel handle and queue depth */
+ static void hpsa_get_ioaccel_drive_info(struct ctlr_info *h,
+ 		struct hpsa_scsi_dev_t *dev,
+ 		u8 *lunaddrbytes,
+ 		struct bmic_identify_physical_device *id_phys)
+ {
+ 	int rc;
+ 	struct ext_report_lun_entry *rle =
+ 		(struct ext_report_lun_entry *) lunaddrbytes;
+ 
+ 	dev->ioaccel_handle = rle->ioaccel_handle;
+ 	if (PHYS_IOACCEL(lunaddrbytes) && dev->ioaccel_handle)
+ 		dev->hba_ioaccel_enabled = 1;
+ 	memset(id_phys, 0, sizeof(*id_phys));
+ 	rc = hpsa_bmic_id_physical_device(h, lunaddrbytes,
+ 			GET_BMIC_DRIVE_NUMBER(lunaddrbytes), id_phys,
+ 			sizeof(*id_phys));
+ 	if (!rc)
+ 		/* Reserve space for FW operations */
+ #define DRIVE_CMDS_RESERVED_FOR_FW 2
+ #define DRIVE_QUEUE_DEPTH 7
+ 		dev->queue_depth =
+ 			le16_to_cpu(id_phys->current_queue_depth_limit) -
+ 				DRIVE_CMDS_RESERVED_FOR_FW;
+ 	else
+ 		dev->queue_depth = DRIVE_QUEUE_DEPTH; /* conservative */
+ 	atomic_set(&dev->ioaccel_cmds_out, 0);
+ 	atomic_set(&dev->reset_cmds_out, 0);
+ }
+ 
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
  {
  	/* the idea here is we could get notified
@@@ -3980,7 -4666,180 +4389,184 @@@ static int hpsa_scsi_queue_command(stru
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int do_not_scan_if_controller_locked_up(struct ctlr_info *h)
++=======
+ static void hpsa_cmd_init(struct ctlr_info *h, int index,
+ 				struct CommandList *c)
+ {
+ 	dma_addr_t cmd_dma_handle, err_dma_handle;
+ 
+ 	/* Zero out all of commandlist except the last field, refcount */
+ 	memset(c, 0, offsetof(struct CommandList, refcount));
+ 	c->Header.tag = cpu_to_le64((u64) (index << DIRECT_LOOKUP_SHIFT));
+ 	cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+ 	c->err_info = h->errinfo_pool + index;
+ 	memset(c->err_info, 0, sizeof(*c->err_info));
+ 	err_dma_handle = h->errinfo_pool_dhandle
+ 	    + index * sizeof(*c->err_info);
+ 	c->cmdindex = index;
+ 	c->busaddr = (u32) cmd_dma_handle;
+ 	c->ErrDesc.Addr = cpu_to_le64((u64) err_dma_handle);
+ 	c->ErrDesc.Len = cpu_to_le32((u32) sizeof(*c->err_info));
+ 	c->h = h;
+ 	c->scsi_cmd = SCSI_CMD_IDLE;
+ }
+ 
+ static void hpsa_preinitialize_commands(struct ctlr_info *h)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < h->nr_cmds; i++) {
+ 		struct CommandList *c = h->cmd_pool + i;
+ 
+ 		hpsa_cmd_init(h, i, c);
+ 		atomic_set(&c->refcount, 0);
+ 	}
+ }
+ 
+ static inline void hpsa_cmd_partial_init(struct ctlr_info *h, int index,
+ 				struct CommandList *c)
+ {
+ 	dma_addr_t cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+ 
+ 	BUG_ON(c->cmdindex != index);
+ 
+ 	memset(c->Request.CDB, 0, sizeof(c->Request.CDB));
+ 	memset(c->err_info, 0, sizeof(*c->err_info));
+ 	c->busaddr = (u32) cmd_dma_handle;
+ }
+ 
+ static int hpsa_ioaccel_submit(struct ctlr_info *h,
+ 		struct CommandList *c, struct scsi_cmnd *cmd,
+ 		unsigned char *scsi3addr)
+ {
+ 	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
+ 	int rc = IO_ACCEL_INELIGIBLE;
+ 
+ 	cmd->host_scribble = (unsigned char *) c;
+ 
+ 	if (dev->offload_enabled) {
+ 		hpsa_cmd_init(h, c->cmdindex, c);
+ 		c->cmd_type = CMD_SCSI;
+ 		c->scsi_cmd = cmd;
+ 		rc = hpsa_scsi_ioaccel_raid_map(h, c);
+ 		if (rc < 0)     /* scsi_dma_map failed. */
+ 			rc = SCSI_MLQUEUE_HOST_BUSY;
+ 	} else if (dev->hba_ioaccel_enabled) {
+ 		hpsa_cmd_init(h, c->cmdindex, c);
+ 		c->cmd_type = CMD_SCSI;
+ 		c->scsi_cmd = cmd;
+ 		rc = hpsa_scsi_ioaccel_direct_map(h, c);
+ 		if (rc < 0)     /* scsi_dma_map failed. */
+ 			rc = SCSI_MLQUEUE_HOST_BUSY;
+ 	}
+ 	return rc;
+ }
+ 
+ static void hpsa_command_resubmit_worker(struct work_struct *work)
+ {
+ 	struct scsi_cmnd *cmd;
+ 	struct hpsa_scsi_dev_t *dev;
+ 	struct CommandList *c = container_of(work, struct CommandList, work);
+ 
+ 	cmd = c->scsi_cmd;
+ 	dev = cmd->device->hostdata;
+ 	if (!dev) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		return hpsa_cmd_free_and_done(c->h, c, cmd);
+ 	}
+ 	if (c->reset_pending)
+ 		return hpsa_cmd_resolve_and_free(c->h, c);
+ 	if (c->abort_pending)
+ 		return hpsa_cmd_abort_and_free(c->h, c, cmd);
+ 	if (c->cmd_type == CMD_IOACCEL2) {
+ 		struct ctlr_info *h = c->h;
+ 		struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+ 		int rc;
+ 
+ 		if (c2->error_data.serv_response ==
+ 				IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL) {
+ 			rc = hpsa_ioaccel_submit(h, c, cmd, dev->scsi3addr);
+ 			if (rc == 0)
+ 				return;
+ 			if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+ 				/*
+ 				 * If we get here, it means dma mapping failed.
+ 				 * Try again via scsi mid layer, which will
+ 				 * then get SCSI_MLQUEUE_HOST_BUSY.
+ 				 */
+ 				cmd->result = DID_IMM_RETRY << 16;
+ 				return hpsa_cmd_free_and_done(h, c, cmd);
+ 			}
+ 			/* else, fall thru and resubmit down CISS path */
+ 		}
+ 	}
+ 	hpsa_cmd_partial_init(c->h, c->cmdindex, c);
+ 	if (hpsa_ciss_submit(c->h, c, cmd, dev->scsi3addr)) {
+ 		/*
+ 		 * If we get here, it means dma mapping failed. Try
+ 		 * again via scsi mid layer, which will then get
+ 		 * SCSI_MLQUEUE_HOST_BUSY.
+ 		 *
+ 		 * hpsa_ciss_submit will have already freed c
+ 		 * if it encountered a dma mapping failure.
+ 		 */
+ 		cmd->result = DID_IMM_RETRY << 16;
+ 		cmd->scsi_done(cmd);
+ 	}
+ }
+ 
+ /* Running in struct Scsi_Host->host_lock less mode */
+ static int hpsa_scsi_queue_command(struct Scsi_Host *sh, struct scsi_cmnd *cmd)
+ {
+ 	struct ctlr_info *h;
+ 	struct hpsa_scsi_dev_t *dev;
+ 	unsigned char scsi3addr[8];
+ 	struct CommandList *c;
+ 	int rc = 0;
+ 
+ 	/* Get the ptr to our adapter structure out of cmd->host. */
+ 	h = sdev_to_hba(cmd->device);
+ 
+ 	BUG_ON(cmd->request->tag < 0);
+ 
+ 	dev = cmd->device->hostdata;
+ 	if (!dev) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 
+ 	memcpy(scsi3addr, dev->scsi3addr, sizeof(scsi3addr));
+ 
+ 	if (unlikely(lockup_detected(h))) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 	c = cmd_tagged_alloc(h, cmd);
+ 
+ 	/*
+ 	 * Call alternate submit routine for I/O accelerated commands.
+ 	 * Retries always go down the normal I/O path.
+ 	 */
+ 	if (likely(cmd->retries == 0 &&
+ 		cmd->request->cmd_type == REQ_TYPE_FS &&
+ 		h->acciopath_status)) {
+ 		rc = hpsa_ioaccel_submit(h, c, cmd, scsi3addr);
+ 		if (rc == 0)
+ 			return 0;
+ 		if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+ 			hpsa_cmd_resolve_and_free(h, c);
+ 			return SCSI_MLQUEUE_HOST_BUSY;
+ 		}
+ 	}
+ 	return hpsa_ciss_submit(h, c, cmd, scsi3addr);
+ }
+ 
+ static void hpsa_scan_complete(struct ctlr_info *h)
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  {
  	unsigned long flags;
  
@@@ -4197,19 -5114,38 +4783,50 @@@ static int hpsa_eh_device_reset_handler
  
  	dev = scsicmd->device->hostdata;
  	if (!dev) {
- 		dev_err(&h->pdev->dev, "hpsa_eh_device_reset_handler: "
- 			"device lookup failed.\n");
+ 		dev_err(&h->pdev->dev, "%s: device lookup failed\n", __func__);
  		return FAILED;
  	}
++<<<<<<< HEAD
 +	dev_warn(&h->pdev->dev, "resetting device %d:%d:%d:%d\n",
 +		h->scsi_host->host_no, dev->bus, dev->target, dev->lun);
 +	/* send a reset to the SCSI LUN which the command was sent to */
 +	rc = hpsa_send_reset(h, dev->scsi3addr, HPSA_RESET_TYPE_LUN);
 +	if (rc == 0 && wait_for_device_to_become_ready(h, dev->scsi3addr) == 0)
 +		return SUCCESS;
 +
 +	dev_warn(&h->pdev->dev, "resetting device failed.\n");
 +	return FAILED;
++=======
+ 
+ 	/* if controller locked up, we can guarantee command won't complete */
+ 	if (lockup_detected(h)) {
+ 		sprintf(msg, "cmd %d RESET FAILED, lockup detected",
+ 				hpsa_get_cmd_index(scsicmd));
+ 		hpsa_show_dev_msg(KERN_WARNING, h, dev, msg);
+ 		return FAILED;
+ 	}
+ 
+ 	/* this reset request might be the result of a lockup; check */
+ 	if (detect_controller_lockup(h)) {
+ 		sprintf(msg, "cmd %d RESET FAILED, new lockup detected",
+ 				hpsa_get_cmd_index(scsicmd));
+ 		hpsa_show_dev_msg(KERN_WARNING, h, dev, msg);
+ 		return FAILED;
+ 	}
+ 
+ 	/* Do not attempt on controller */
+ 	if (is_hba_lunid(dev->scsi3addr))
+ 		return SUCCESS;
+ 
+ 	hpsa_show_dev_msg(KERN_WARNING, h, dev, "resetting");
+ 
+ 	/* send a reset to the SCSI LUN which the command was sent to */
+ 	rc = hpsa_do_reset(h, dev, dev->scsi3addr, HPSA_RESET_TYPE_LUN,
+ 			   DEFAULT_REPLY_QUEUE);
+ 	sprintf(msg, "reset %s", rc == 0 ? "completed successfully" : "failed");
+ 	hpsa_show_dev_msg(KERN_WARNING, h, dev, msg);
+ 	return rc == 0 ? SUCCESS : FAILED;
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  }
  
  static void swizzle_abort_tag(u8 *tag)
@@@ -4347,7 -5325,7 +4964,11 @@@ static int hpsa_send_reset_as_abort_ioa
  			"Reset as abort: Resetting physical device at scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
  			psa[0], psa[1], psa[2], psa[3],
  			psa[4], psa[5], psa[6], psa[7]);
++<<<<<<< HEAD
 +	rc = hpsa_send_reset(h, psa, HPSA_RESET_TYPE_TARGET);
++=======
+ 	rc = hpsa_do_reset(h, dev, psa, HPSA_RESET_TYPE_TARGET, reply_queue);
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  	if (rc != 0) {
  		dev_warn(&h->pdev->dev,
  			"Reset as abort: Failed on physical device at scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
@@@ -4459,35 -5549,81 +5080,44 @@@ static int hpsa_eh_abort_handler(struc
  	 * by the firmware (but not to the scsi mid layer) but we can't
  	 * distinguish which.  Send the abort down.
  	 */
 -	if (wait_for_available_abort_cmd(h)) {
 -		dev_warn(&h->pdev->dev,
 -			"%s FAILED, timeout waiting for an abort command to become available.\n",
 -			msg);
 -		cmd_free(h, abort);
 -		return FAILED;
 -	}
 -	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort, reply_queue);
 -	atomic_inc(&h->abort_cmds_available);
 -	wake_up_all(&h->abort_cmd_wait_queue);
 +	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort);
  	if (rc != 0) {
 -		dev_warn(&h->pdev->dev, "%s SENT, FAILED\n", msg);
 -		hpsa_show_dev_msg(KERN_WARNING, h, dev,
 -				"FAILED to abort command");
 -		cmd_free(h, abort);
 +		dev_dbg(&h->pdev->dev, "%s Request FAILED.\n", msg);
 +		dev_warn(&h->pdev->dev, "FAILED abort on device C%d:B%d:T%d:L%d\n",
 +			h->scsi_host->host_no,
 +			dev->bus, dev->target, dev->lun);
  		return FAILED;
  	}
++<<<<<<< HEAD
 +	dev_info(&h->pdev->dev, "%s REQUEST SUCCEEDED.\n", msg);
++=======
+ 	dev_info(&h->pdev->dev, "%s SENT, SUCCESS\n", msg);
+ 	wait_event(h->event_sync_wait_queue,
+ 		   abort->scsi_cmd != sc || lockup_detected(h));
+ 	cmd_free(h, abort);
+ 	return !lockup_detected(h) ? SUCCESS : FAILED;
+ }
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  
 -/*
 - * For operations with an associated SCSI command, a command block is allocated
 - * at init, and managed by cmd_tagged_alloc() and cmd_tagged_free() using the
 - * block request tag as an index into a table of entries.  cmd_tagged_free() is
 - * the complement, although cmd_free() may be called instead.
 - */
 -static struct CommandList *cmd_tagged_alloc(struct ctlr_info *h,
 -					    struct scsi_cmnd *scmd)
 -{
 -	int idx = hpsa_get_cmd_index(scmd);
 -	struct CommandList *c = h->cmd_pool + idx;
 -
 -	if (idx < HPSA_NRESERVED_CMDS || idx >= h->nr_cmds) {
 -		dev_err(&h->pdev->dev, "Bad block tag: %d not in [%d..%d]\n",
 -			idx, HPSA_NRESERVED_CMDS, h->nr_cmds - 1);
 -		/* The index value comes from the block layer, so if it's out of
 -		 * bounds, it's probably not our bug.
 -		 */
 -		BUG();
 -	}
 -
 -	atomic_inc(&c->refcount);
 -	if (unlikely(!hpsa_is_cmd_idle(c))) {
 -		/*
 -		 * We expect that the SCSI layer will hand us a unique tag
 -		 * value.  Thus, there should never be a collision here between
 -		 * two requests...because if the selected command isn't idle
 -		 * then someone is going to be very disappointed.
 -		 */
 -		dev_err(&h->pdev->dev,
 -			"tag collision (tag=%d) in cmd_tagged_alloc().\n",
 -			idx);
 -		if (c->scsi_cmd != NULL)
 -			scsi_print_command(c->scsi_cmd);
 -		scsi_print_command(scmd);
 +	/* If the abort(s) above completed and actually aborted the
 +	 * command, then the command to be aborted should already be
 +	 * completed.  If not, wait around a bit more to see if they
 +	 * manage to complete normally.
 +	 */
 +#define ABORT_COMPLETE_WAIT_SECS 30
 +	for (i = 0; i < ABORT_COMPLETE_WAIT_SECS * 10; i++) {
 +		if (test_bit(abort->cmdindex & (BITS_PER_LONG - 1),
 +				h->cmd_pool_bits +
 +				(abort->cmdindex / BITS_PER_LONG)))
 +			msleep(100);
 +		else
 +			return SUCCESS;
  	}
 -
 -	hpsa_cmd_partial_init(h, idx, c);
 -	return c;
 +	dev_warn(&h->pdev->dev, "%s FAILED. Aborted command has not completed after %d seconds.\n",
 +		msg, ABORT_COMPLETE_WAIT_SECS);
 +	return FAILED;
  }
  
 -static void cmd_tagged_free(struct ctlr_info *h, struct CommandList *c)
 -{
 -	/*
 -	 * Release our reference to the block.  We don't need to do anything
 -	 * else to free it, because it is accessed by index.  (There's no point
 -	 * in checking the result of the decrement, since we cannot guarantee
 -	 * that there isn't a concurrent abort which is also accessing it.)
 -	 */
 -	(void)atomic_dec(&c->refcount);
 -}
  
  /*
   * For operations that cannot sleep, a command block is allocated at init,
@@@ -6805,17 -7970,19 +7435,23 @@@ reinit_after_soft_reset
  	/* make sure the board interrupts are off */
  	h->access.set_intr_mask(h, HPSA_INTR_OFF);
  
 -	rc = hpsa_request_irqs(h, do_hpsa_intr_msi, do_hpsa_intr_intx);
 -	if (rc)
 -		goto clean3;	/* shost, pci, lu, aer/h */
 +	if (hpsa_request_irqs(h, do_hpsa_intr_msi, do_hpsa_intr_intx))
 +		goto clean2;
 +	dev_info(&pdev->dev, "%s: <0x%x> at IRQ %d%s using DAC\n",
 +	       h->devname, pdev->device,
 +	       h->intr[h->intr_mode], dac ? "" : " not");
  	rc = hpsa_alloc_cmd_pool(h);
  	if (rc)
 -		goto clean4;	/* irq, shost, pci, lu, aer/h */
 -	rc = hpsa_alloc_sg_chain_blocks(h);
 -	if (rc)
 -		goto clean5;	/* cmd, irq, shost, pci, lu, aer/h */
 +		goto clean2_and_free_irqs;
 +	if (hpsa_allocate_sg_chain_blocks(h))
 +		goto clean4;
  	init_waitqueue_head(&h->scan_wait_queue);
++<<<<<<< HEAD
++=======
+ 	init_waitqueue_head(&h->abort_cmd_wait_queue);
+ 	init_waitqueue_head(&h->event_sync_wait_queue);
+ 	mutex_init(&h->reset_mutex);
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  	h->scan_finished = 1; /* no scan currently in progress */
  
  	pci_set_drvdata(pdev, h);
diff --cc drivers/scsi/hpsa.h
index bb1c5c5da1f2,6ee4da6b1153..000000000000
--- a/drivers/scsi/hpsa.h
+++ b/drivers/scsi/hpsa.h
@@@ -46,6 -46,12 +46,15 @@@ struct hpsa_scsi_dev_t 
  	unsigned char model[16];        /* bytes 16-31 of inquiry data */
  	unsigned char raid_level;	/* from inquiry page 0xC1 */
  	unsigned char volume_offline;	/* discovered via TUR or VPD */
++<<<<<<< HEAD
++=======
+ 	u16 queue_depth;		/* max queue_depth for this device */
+ 	atomic_t reset_cmds_out;	/* Count of commands to-be affected */
+ 	atomic_t ioaccel_cmds_out;	/* Only used for physical devices
+ 					 * counts commands sent to physical
+ 					 * device via "ioaccel" path.
+ 					 */
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  	u32 ioaccel_handle;
  	int offload_config;		/* I/O accel RAID offload configured */
  	int offload_enabled;		/* I/O accel RAID offload enabled */
@@@ -54,6 -62,22 +63,25 @@@
  					 */
  	struct raid_map_data raid_map;	/* I/O accelerator RAID map */
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Pointers from logical drive map indices to the phys drives that
+ 	 * make those logical drives.  Note, multiple logical drives may
+ 	 * share physical drives.  You can have for instance 5 physical
+ 	 * drives with 3 logical drives each using those same 5 physical
+ 	 * disks. We need these pointers for counting i/o's out to physical
+ 	 * devices in order to honor physical device queue depth limits.
+ 	 */
+ 	struct hpsa_scsi_dev_t *phys_disk[RAID_MAP_MAX_ENTRIES];
+ 	int nphysical_disks;
+ 	int supports_aborts;
+ #define HPSA_DO_NOT_EXPOSE	0x0
+ #define HPSA_SG_ATTACH		0x1
+ #define HPSA_ULD_ATTACH		0x2
+ #define HPSA_SCSI_ADD		(HPSA_SG_ATTACH | HPSA_ULD_ATTACH)
+ 	u8 expose_state;
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  };
  
  struct reply_queue_buffer {
@@@ -236,6 -263,13 +264,16 @@@ struct ctlr_info 
  	struct list_head offline_device_list;
  	int	acciopath_status;
  	int	raid_offload_debug;
++<<<<<<< HEAD
++=======
+ 	int	needs_abort_tags_swizzled;
+ 	struct workqueue_struct *resubmit_wq;
+ 	struct workqueue_struct *rescan_ctlr_wq;
+ 	atomic_t abort_cmds_available;
+ 	wait_queue_head_t abort_cmd_wait_queue;
+ 	wait_queue_head_t event_sync_wait_queue;
+ 	struct mutex reset_mutex;
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  };
  
  struct offline_device_entry {
diff --cc drivers/scsi/hpsa_cmd.h
index 9a0de7e7efe1,c601622cc98e..000000000000
--- a/drivers/scsi/hpsa_cmd.h
+++ b/drivers/scsi/hpsa_cmd.h
@@@ -403,7 -426,23 +403,27 @@@ struct CommandList 
  	int			   cmd_type;
  	long			   cmdindex;
  	struct completion *waiting;
++<<<<<<< HEAD
 +	void   *scsi_cmd;
++=======
+ 	struct scsi_cmnd *scsi_cmd;
+ 	struct work_struct work;
+ 
+ 	/*
+ 	 * For commands using either of the two "ioaccel" paths to
+ 	 * bypass the RAID stack and go directly to the physical disk
+ 	 * phys_disk is a pointer to the hpsa_scsi_dev_t to which the
+ 	 * i/o is destined.  We need to store that here because the command
+ 	 * may potentially encounter TASK SET FULL and need to be resubmitted
+ 	 * For "normal" i/o's not using the "ioaccel" paths, phys_disk is
+ 	 * not used.
+ 	 */
+ 	struct hpsa_scsi_dev_t *phys_disk;
+ 
+ 	int abort_pending;
+ 	struct hpsa_scsi_dev_t *reset_pending;
+ 	atomic_t refcount; /* Must be last to avoid memset in hpsa_cmd_init() */
++>>>>>>> d604f5336aee (hpsa: cleanup reset)
  } __aligned(COMMANDLIST_ALIGNMENT);
  
  /* Max S/G elements in I/O accelerator command */
* Unmerged path drivers/scsi/hpsa.c
* Unmerged path drivers/scsi/hpsa.h
* Unmerged path drivers/scsi/hpsa_cmd.h
