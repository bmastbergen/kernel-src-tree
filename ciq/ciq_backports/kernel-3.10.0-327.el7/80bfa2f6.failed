xen-blkif: drop struct blkif_request_segment_aligned

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Roger Pau Monne <roger.pau@citrix.com>
commit 80bfa2f6e2e81049fc6cd3bfaeedcb64db3a9ba6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/80bfa2f6.failed

This was wrongly introduced in commit 402b27f9, the only difference
between blkif_request_segment_aligned and blkif_request_segment is
that the former has a named padding, while both share the same
memory layout.

Also correct a few minor glitches in the description, including for it
to no longer assume PAGE_SIZE == 4096.

	Signed-off-by: Roger Pau Monn√© <roger.pau@citrix.com>
[Description fix by Jan Beulich]
	Signed-off-by: Jan Beulich <jbeulich@suse.com>
	Reported-by: Jan Beulich <jbeulich@suse.com>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: David Vrabel <david.vrabel@citrix.com>
	Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Tested-by: Matt Rushton <mrushton@amazon.com>
	Cc: Matt Wilson <msw@amazon.com>
	Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
(cherry picked from commit 80bfa2f6e2e81049fc6cd3bfaeedcb64db3a9ba6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/xen-blkback/blkback.c
#	drivers/block/xen-blkback/common.h
diff --cc drivers/block/xen-blkback/blkback.c
index 03dd4aa612db,e612627ae981..000000000000
--- a/drivers/block/xen-blkback/blkback.c
+++ b/drivers/block/xen-blkback/blkback.c
@@@ -595,49 -758,133 +595,116 @@@ static int xen_blkbk_map(struct blkif_r
  	 * so that when we access vaddr(pending_req,i) it has the contents of
  	 * the page from the other domain.
  	 */
 -	for (seg_idx = last_map, new_map_idx = 0; seg_idx < map_until; seg_idx++) {
 -		if (!pages[seg_idx]->persistent_gnt) {
 +	bitmap_zero(pending_req->unmap_seg, BLKIF_MAX_SEGMENTS_PER_REQUEST);
 +	for (i = 0, j = 0; i < nseg; i++) {
 +		if (!persistent_gnts[i] ||
 +		    persistent_gnts[i]->handle == BLKBACK_INVALID_HANDLE) {
  			/* This is a newly mapped grant */
 -			BUG_ON(new_map_idx >= segs_to_map);
 -			if (unlikely(map[new_map_idx].status != 0)) {
 +			BUG_ON(j >= segs_to_map);
 +			if (unlikely(map[j].status != 0)) {
  				pr_debug(DRV_PFX "invalid buffer -- could not remap it\n");
 -				pages[seg_idx]->handle = BLKBACK_INVALID_HANDLE;
 +				map[j].handle = BLKBACK_INVALID_HANDLE;
  				ret |= 1;
 -				goto next;
 +				if (persistent_gnts[i]) {
 +					rb_erase(&persistent_gnts[i]->node,
 +						 &blkif->persistent_gnts);
 +					blkif->persistent_gnt_c--;
 +					kfree(persistent_gnts[i]);
 +					persistent_gnts[i] = NULL;
 +				}
  			}
 -			pages[seg_idx]->handle = map[new_map_idx].handle;
 -		} else {
 -			continue;
  		}
 -		if (use_persistent_gnts &&
 -		    blkif->persistent_gnt_c < xen_blkif_max_pgrants) {
 -			/*
 -			 * We are using persistent grants, the grant is
 -			 * not mapped but we might have room for it.
 -			 */
 -			persistent_gnt = kmalloc(sizeof(struct persistent_gnt),
 -				                 GFP_KERNEL);
 -			if (!persistent_gnt) {
 +		if (persistent_gnts[i]) {
 +			if (persistent_gnts[i]->handle ==
 +			    BLKBACK_INVALID_HANDLE) {
  				/*
 -				 * If we don't have enough memory to
 -				 * allocate the persistent_gnt struct
 -				 * map this grant non-persistenly
 +				 * If this is a new persistent grant
 +				 * save the handler
  				 */
 -				goto next;
 +				persistent_gnts[i]->handle = map[j++].handle;
  			}
 -			persistent_gnt->gnt = map[new_map_idx].ref;
 -			persistent_gnt->handle = map[new_map_idx].handle;
 -			persistent_gnt->page = pages[seg_idx]->page;
 -			if (add_persistent_gnt(blkif,
 -			                       persistent_gnt)) {
 -				kfree(persistent_gnt);
 -				persistent_gnt = NULL;
 -				goto next;
 -			}
 -			pages[seg_idx]->persistent_gnt = persistent_gnt;
 -			pr_debug(DRV_PFX " grant %u added to the tree of persistent grants, using %u/%u\n",
 -				 persistent_gnt->gnt, blkif->persistent_gnt_c,
 -				 xen_blkif_max_pgrants);
 -			goto next;
 -		}
 -		if (use_persistent_gnts && !blkif->vbd.overflow_max_grants) {
 -			blkif->vbd.overflow_max_grants = 1;
 -			pr_debug(DRV_PFX " domain %u, device %#x is using maximum number of persistent grants\n",
 -			         blkif->domid, blkif->vbd.handle);
 +			pending_handle(pending_req, i) =
 +				persistent_gnts[i]->handle;
 +
 +			if (ret)
 +				continue;
 +		} else {
 +			pending_handle(pending_req, i) = map[j++].handle;
 +			bitmap_set(pending_req->unmap_seg, i, 1);
 +
 +			if (ret)
 +				continue;
  		}
 -		/*
 -		 * We could not map this grant persistently, so use it as
 -		 * a non-persistent grant.
 -		 */
 -next:
 -		new_map_idx++;
 +		seg[i].offset = (req->u.rw.seg[i].first_sect << 9);
  	}
 -	segs_to_map = 0;
 -	last_map = map_until;
 -	if (map_until != num)
 -		goto again;
 -
  	return ret;
++<<<<<<< HEAD
++=======
+ 
+ out_of_memory:
+ 	pr_alert(DRV_PFX "%s: out of memory\n", __func__);
+ 	put_free_pages(blkif, pages_to_gnt, segs_to_map);
+ 	return -ENOMEM;
+ }
+ 
+ static int xen_blkbk_map_seg(struct pending_req *pending_req)
+ {
+ 	int rc;
+ 
+ 	rc = xen_blkbk_map(pending_req->blkif, pending_req->segments,
+ 			   pending_req->nr_pages,
+ 	                   (pending_req->operation != BLKIF_OP_READ));
+ 
+ 	return rc;
+ }
+ 
+ static int xen_blkbk_parse_indirect(struct blkif_request *req,
+ 				    struct pending_req *pending_req,
+ 				    struct seg_buf seg[],
+ 				    struct phys_req *preq)
+ {
+ 	struct grant_page **pages = pending_req->indirect_pages;
+ 	struct xen_blkif *blkif = pending_req->blkif;
+ 	int indirect_grefs, rc, n, nseg, i;
+ 	struct blkif_request_segment *segments = NULL;
+ 
+ 	nseg = pending_req->nr_pages;
+ 	indirect_grefs = INDIRECT_PAGES(nseg);
+ 	BUG_ON(indirect_grefs > BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST);
+ 
+ 	for (i = 0; i < indirect_grefs; i++)
+ 		pages[i]->gref = req->u.indirect.indirect_grefs[i];
+ 
+ 	rc = xen_blkbk_map(blkif, pages, indirect_grefs, true);
+ 	if (rc)
+ 		goto unmap;
+ 
+ 	for (n = 0, i = 0; n < nseg; n++) {
+ 		if ((n % SEGS_PER_INDIRECT_FRAME) == 0) {
+ 			/* Map indirect segments */
+ 			if (segments)
+ 				kunmap_atomic(segments);
+ 			segments = kmap_atomic(pages[n/SEGS_PER_INDIRECT_FRAME]->page);
+ 		}
+ 		i = n % SEGS_PER_INDIRECT_FRAME;
+ 		pending_req->segments[n]->gref = segments[i].gref;
+ 		seg[n].nsec = segments[i].last_sect -
+ 			segments[i].first_sect + 1;
+ 		seg[n].offset = (segments[i].first_sect << 9);
+ 		if ((segments[i].last_sect >= (PAGE_SIZE >> 9)) ||
+ 		    (segments[i].last_sect < segments[i].first_sect)) {
+ 			rc = -EINVAL;
+ 			goto unmap;
+ 		}
+ 		preq->nr_sects += seg[n].nsec;
+ 	}
+ 
+ unmap:
+ 	if (segments)
+ 		kunmap_atomic(segments);
+ 	xen_blkbk_unmap(blkif, pages, indirect_grefs);
+ 	return rc;
++>>>>>>> 80bfa2f6e2e8 (xen-blkif: drop struct blkif_request_segment_aligned)
  }
  
  static int dispatch_discard_io(struct xen_blkif *blkif,
diff --cc drivers/block/xen-blkback/common.h
index 60103e2517ba,9eb34e24b4fe..000000000000
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@@ -50,6 -50,19 +50,22 @@@
  		 __func__, __LINE__, ##args)
  
  
++<<<<<<< HEAD
++=======
+ /*
+  * This is the maximum number of segments that would be allowed in indirect
+  * requests. This value will also be passed to the frontend.
+  */
+ #define MAX_INDIRECT_SEGMENTS 256
+ 
+ #define SEGS_PER_INDIRECT_FRAME \
+ 	(PAGE_SIZE/sizeof(struct blkif_request_segment))
+ #define MAX_INDIRECT_PAGES \
+ 	((MAX_INDIRECT_SEGMENTS + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
+ #define INDIRECT_PAGES(_segs) \
+ 	((_segs + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
+ 
++>>>>>>> 80bfa2f6e2e8 (xen-blkif: drop struct blkif_request_segment_aligned)
  /* Not a real protocol.  Used to generate ring structs which contain
   * the elements common to all protocols only.  This way we get a
   * compiler-checkable way to use common struct elements, so we can
* Unmerged path drivers/block/xen-blkback/blkback.c
* Unmerged path drivers/block/xen-blkback/common.h
diff --git a/drivers/block/xen-blkfront.c b/drivers/block/xen-blkfront.c
index 629a673151e0..5ca909f94253 100644
--- a/drivers/block/xen-blkfront.c
+++ b/drivers/block/xen-blkfront.c
@@ -162,7 +162,7 @@ static DEFINE_SPINLOCK(minor_lock);
 #define DEV_NAME	"xvd"	/* name in /dev */
 
 #define SEGS_PER_INDIRECT_FRAME \
-	(PAGE_SIZE/sizeof(struct blkif_request_segment_aligned))
+	(PAGE_SIZE/sizeof(struct blkif_request_segment))
 #define INDIRECT_GREFS(_segs) \
 	((_segs + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
 
@@ -393,7 +393,7 @@ static int blkif_queue_request(struct request *req)
 	unsigned long id;
 	unsigned int fsect, lsect;
 	int i, ref, n;
-	struct blkif_request_segment_aligned *segments = NULL;
+	struct blkif_request_segment *segments = NULL;
 
 	/*
 	 * Used to store if we are able to queue the request by just using
@@ -550,7 +550,7 @@ static int blkif_queue_request(struct request *req)
 			} else {
 				n = i % SEGS_PER_INDIRECT_FRAME;
 				segments[n] =
-					(struct blkif_request_segment_aligned) {
+					(struct blkif_request_segment) {
 							.gref       = ref,
 							.first_sect = fsect,
 							.last_sect  = lsect };
diff --git a/include/xen/interface/io/blkif.h b/include/xen/interface/io/blkif.h
index 65e12099ef89..d28fb3241fae 100644
--- a/include/xen/interface/io/blkif.h
+++ b/include/xen/interface/io/blkif.h
@@ -113,13 +113,13 @@ typedef uint64_t blkif_sector_t;
  * it's less than the number provided by the backend. The indirect_grefs field
  * in blkif_request_indirect should be filled by the frontend with the
  * grant references of the pages that are holding the indirect segments.
- * This pages are filled with an array of blkif_request_segment_aligned
- * that hold the information about the segments. The number of indirect
- * pages to use is determined by the maximum number of segments
- * a indirect request contains. Every indirect page can contain a maximum
- * of 512 segments (PAGE_SIZE/sizeof(blkif_request_segment_aligned)),
- * so to calculate the number of indirect pages to use we have to do
- * ceil(indirect_segments/512).
+ * These pages are filled with an array of blkif_request_segment that hold the
+ * information about the segments. The number of indirect pages to use is
+ * determined by the number of segments an indirect request contains. Every
+ * indirect page can contain a maximum of
+ * (PAGE_SIZE / sizeof(struct blkif_request_segment)) segments, so to
+ * calculate the number of indirect pages to use we have to do
+ * ceil(indirect_segments / (PAGE_SIZE / sizeof(struct blkif_request_segment))).
  *
  * If a backend does not recognize BLKIF_OP_INDIRECT, it should *not*
  * create the "feature-max-indirect-segments" node!
@@ -135,13 +135,12 @@ typedef uint64_t blkif_sector_t;
 
 #define BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST 8
 
-struct blkif_request_segment_aligned {
-	grant_ref_t gref;        /* reference to I/O buffer frame        */
-	/* @first_sect: first sector in frame to transfer (inclusive).   */
-	/* @last_sect: last sector in frame to transfer (inclusive).     */
-	uint8_t     first_sect, last_sect;
-	uint16_t    _pad; /* padding to make it 8 bytes, so it's cache-aligned */
-} __attribute__((__packed__));
+struct blkif_request_segment {
+		grant_ref_t gref;        /* reference to I/O buffer frame        */
+		/* @first_sect: first sector in frame to transfer (inclusive).   */
+		/* @last_sect: last sector in frame to transfer (inclusive).     */
+		uint8_t     first_sect, last_sect;
+};
 
 struct blkif_request_rw {
 	uint8_t        nr_segments;  /* number of segments                   */
@@ -151,12 +150,7 @@ struct blkif_request_rw {
 #endif
 	uint64_t       id;           /* private guest value, echoed in resp  */
 	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
-	struct blkif_request_segment {
-		grant_ref_t gref;        /* reference to I/O buffer frame        */
-		/* @first_sect: first sector in frame to transfer (inclusive).   */
-		/* @last_sect: last sector in frame to transfer (inclusive).     */
-		uint8_t     first_sect, last_sect;
-	} seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
 } __attribute__((__packed__));
 
 struct blkif_request_discard {
