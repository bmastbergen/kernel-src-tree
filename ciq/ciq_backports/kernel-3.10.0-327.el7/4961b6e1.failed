sched: core: Use hrtimer_start[_expires]()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 4961b6e11825c2b05b516374b1800fc5dfc2cb78
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/4961b6e1.failed

hrtimer_start() now enforces a timer interrupt when an already expired
timer is enqueued.

Get rid of the __hrtimer_start_range_ns() invocations and the loops
around it.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Peter Zijlstra <peterz@infradead.org>
	Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
	Cc: Marcelo Tosatti <mtosatti@redhat.com>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
Link: http://lkml.kernel.org/r/20150414203502.531131739@linutronix.de
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit 4961b6e11825c2b05b516374b1800fc5dfc2cb78)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/fair.c
diff --cc kernel/sched/core.c
index 9a510f9b4081,3026678113e7..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -89,40 -90,13 +89,29 @@@
  #define CREATE_TRACE_POINTS
  #include <trace/events/sched.h>
  
 +#ifdef smp_mb__before_atomic
 +void __smp_mb__before_atomic(void)
 +{
 +	smp_mb__before_atomic();
 +}
 +EXPORT_SYMBOL(__smp_mb__before_atomic);
 +#endif
 +
 +#ifdef smp_mb__after_atomic
 +void __smp_mb__after_atomic(void)
 +{
 +	smp_mb__after_atomic();
 +}
 +EXPORT_SYMBOL(__smp_mb__after_atomic);
 +#endif
 +
  void start_bandwidth_timer(struct hrtimer *period_timer, ktime_t period)
  {
- 	unsigned long delta;
- 	ktime_t soft, hard, now;
- 
- 	for (;;) {
- 		if (hrtimer_active(period_timer))
- 			break;
- 
- 		now = hrtimer_cb_get_time(period_timer);
- 		hrtimer_forward(period_timer, now, period);
+ 	if (hrtimer_active(period_timer))
+ 		return;
  
- 		soft = hrtimer_get_softexpires(period_timer);
- 		hard = hrtimer_get_expires(period_timer);
- 		delta = ktime_to_ns(ktime_sub(hard, soft));
- 		__hrtimer_start_range_ns(period_timer, soft, delta,
- 					 HRTIMER_MODE_ABS_PINNED, 0);
- 	}
+ 	hrtimer_forward_now(period_timer, period);
+ 	hrtimer_start_expires(period_timer, HRTIMER_MODE_ABS_PINNED);
  }
  
  DEFINE_MUTEX(sched_domains_mutex);
@@@ -422,6 -343,14 +411,17 @@@ static enum hrtimer_restart hrtick(stru
  }
  
  #ifdef CONFIG_SMP
++<<<<<<< HEAD
++=======
+ 
+ static void __hrtick_restart(struct rq *rq)
+ {
+ 	struct hrtimer *timer = &rq->hrtick_timer;
+ 
+ 	hrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED);
+ }
+ 
++>>>>>>> 4961b6e11825 (sched: core: Use hrtimer_start[_expires]())
  /*
   * called from hardirq (IPI) context
   */
@@@ -486,8 -423,13 +486,18 @@@ static __init void init_hrtick(void
   */
  void hrtick_start(struct rq *rq, u64 delay)
  {
++<<<<<<< HEAD
 +	__hrtimer_start_range_ns(&rq->hrtick_timer, ns_to_ktime(delay), 0,
 +			HRTIMER_MODE_REL_PINNED, 0);
++=======
+ 	/*
+ 	 * Don't schedule slices shorter than 10000ns, that just
+ 	 * doesn't make sense. Rely on vruntime for fairness.
+ 	 */
+ 	delay = max_t(u64, delay, 10000LL);
+ 	hrtimer_start(&rq->hrtick_timer, ns_to_ktime(delay),
+ 		      HRTIMER_MODE_REL_PINNED);
++>>>>>>> 4961b6e11825 (sched: core: Use hrtimer_start[_expires]())
  }
  
  static inline void init_hrtick(void)
diff --cc kernel/sched/fair.c
index 6868c5f9839f,854881b2526b..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -3423,7 -3846,13 +3423,17 @@@ static const u64 min_bandwidth_expirati
  /* how long we wait to gather additional slack before distributing */
  static const u64 cfs_bandwidth_slack_period = 5 * NSEC_PER_MSEC;
  
++<<<<<<< HEAD
 +/* are we near the end of the current quota period? */
++=======
+ /*
+  * Are we near the end of the current quota period?
+  *
+  * Requires cfs_b->lock for hrtimer_expires_remaining to be safe against the
+  * hrtimer base being cleared by hrtimer_start. In the case of
+  * migrate_hrtimers, base is never cleared, so we are fine.
+  */
++>>>>>>> 4961b6e11825 (sched: core: Use hrtimer_start[_expires]())
  static int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire)
  {
  	struct hrtimer *refresh_timer = &cfs_b->period_timer;
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/fair.c
