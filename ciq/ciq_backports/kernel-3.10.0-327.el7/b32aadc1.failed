powerpc/powernv: Fix race in updating core_idle_state

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] powernv: Fix race in updating core_idle_state (Steve Best) [1237270]
Rebuild_FUZZ: 91.84%
commit-author Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
commit b32aadc1a8ed84afbe924cd2ced31cd6a2e67074
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/b32aadc1.failed

core_idle_state is maintained for each core. It uses 0-7 bits to track
whether a thread in the core has entered fastsleep or winkle. 8th bit is
used as a lock bit.
The lock bit is set in these 2 scenarios-
 - The thread is first in subcore to wakeup from sleep/winkle.
 - If its the last thread in the core about to enter sleep/winkle

While the lock bit is set, if any other thread in the core wakes up, it
loops until the lock bit is cleared before proceeding in the wakeup
path. This helps prevent race conditions w.r.t fastsleep workaround and
prevents threads from switching to process context before core/subcore
resources are restored.

But, in the path to sleep/winkle entry, we currently don't check for
lock-bit. This exposes us to following race when running with subcore
on-

First thread in the subcorea		Another thread in the same
waking up		   		core entering sleep/winkle

lwarx   r15,0,r14
ori     r15,r15,PNV_CORE_IDLE_LOCK_BIT
stwcx.  r15,0,r14
[Code to restore subcore state]

						lwarx   r15,0,r14
						[clear thread bit]
						stwcx.  r15,0,r14

andi.   r15,r15,PNV_CORE_IDLE_THREAD_BITS
stw     r15,0(r14)

Here, after the thread entering sleep clears its thread bit in
core_idle_state, the value is overwritten by the thread waking up.
In such cases when the core enters fastsleep, code mistakes an idle
thread as running. Because of this, the first thread waking up from
fastsleep which is supposed to resync timebase skips it. So we can
end up having a core with stale timebase value.

This patch fixes the above race by looping on the lock bit even while
entering the idle states.

	Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Fixes: 7b54e9f213f76 'powernv/powerpc: Add winkle support for offline cpus'
	Cc: stable@vger.kernel.org # 3.19+
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit b32aadc1a8ed84afbe924cd2ced31cd6a2e67074)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kernel/idle_power7.S
diff --cc arch/powerpc/kernel/idle_power7.S
index e5aba6abbe6c,112ccf497562..000000000000
--- a/arch/powerpc/kernel/idle_power7.S
+++ b/arch/powerpc/kernel/idle_power7.S
@@@ -35,10 -51,25 +35,26 @@@
  
  	.text
  
+ /*
+  * Used by threads when the lock bit of core_idle_state is set.
+  * Threads will spin in HMT_LOW until the lock bit is cleared.
+  * r14 - pointer to core_idle_state
+  * r15 - used to load contents of core_idle_state
+  */
+ 
+ core_idle_lock_held:
+ 	HMT_LOW
+ 3:	lwz	r15,0(r14)
+ 	andi.   r15,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	bne	3b
+ 	HMT_MEDIUM
+ 	lwarx	r15,0,r14
+ 	blr
+ 
  /*
   * Pass requested state in r3:
 - *	r3 - PNV_THREAD_NAP/SLEEP/WINKLE
 + * 	0 - nap
 + * 	1 - sleep
   *
   * To check IRQ_HAPPENED in r4
   * 	0 - don't check
@@@ -123,12 -155,87 +139,91 @@@ power7_enter_nap_mode
  	li	r4,KVM_HWTHREAD_IN_NAP
  	stb	r4,HSTATE_HWTHREAD_STATE(r13)
  #endif
 -	stb	r3,PACA_THREAD_IDLE_STATE(r13)
 -	cmpwi	cr3,r3,PNV_THREAD_SLEEP
 -	bge	cr3,2f
 +	cmpwi	cr0,r3,1
 +	beq	2f
  	IDLE_STATE_ENTER_SEQ(PPC_NAP)
  	/* No return */
++<<<<<<< HEAD
 +2:	IDLE_STATE_ENTER_SEQ(PPC_SLEEP)
 +	/* No return */
++=======
+ 2:
+ 	/* Sleep or winkle */
+ 	lbz	r7,PACA_THREAD_MASK(r13)
+ 	ld	r14,PACA_CORE_IDLE_STATE_PTR(r13)
+ lwarx_loop1:
+ 	lwarx	r15,0,r14
+ 
+ 	andi.   r9,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	bnel	core_idle_lock_held
+ 
+ 	andc	r15,r15,r7			/* Clear thread bit */
+ 
+ 	andi.	r15,r15,PNV_CORE_IDLE_THREAD_BITS
+ 
+ /*
+  * If cr0 = 0, then current thread is the last thread of the core entering
+  * sleep. Last thread needs to execute the hardware bug workaround code if
+  * required by the platform.
+  * Make the workaround call unconditionally here. The below branch call is
+  * patched out when the idle states are discovered if the platform does not
+  * require it.
+  */
+ .global pnv_fastsleep_workaround_at_entry
+ pnv_fastsleep_workaround_at_entry:
+ 	beq	fastsleep_workaround_at_entry
+ 
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop1
+ 	isync
+ 
+ common_enter: /* common code for all the threads entering sleep or winkle */
+ 	bgt	cr3,enter_winkle
+ 	IDLE_STATE_ENTER_SEQ(PPC_SLEEP)
+ 
+ fastsleep_workaround_at_entry:
+ 	ori	r15,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop1
+ 	isync
+ 
+ 	/* Fast sleep workaround */
+ 	li	r3,1
+ 	li	r4,1
+ 	li	r0,OPAL_CONFIG_CPU_IDLE_STATE
+ 	bl	opal_call_realmode
+ 
+ 	/* Clear Lock bit */
+ 	li	r0,0
+ 	lwsync
+ 	stw	r0,0(r14)
+ 	b	common_enter
+ 
+ enter_winkle:
+ 	/*
+ 	 * Note all register i.e per-core, per-subcore or per-thread is saved
+ 	 * here since any thread in the core might wake up first
+ 	 */
+ 	mfspr	r3,SPRN_SDR1
+ 	std	r3,_SDR1(r1)
+ 	mfspr	r3,SPRN_RPR
+ 	std	r3,_RPR(r1)
+ 	mfspr	r3,SPRN_SPURR
+ 	std	r3,_SPURR(r1)
+ 	mfspr	r3,SPRN_PURR
+ 	std	r3,_PURR(r1)
+ 	mfspr	r3,SPRN_TSCR
+ 	std	r3,_TSCR(r1)
+ 	mfspr	r3,SPRN_DSCR
+ 	std	r3,_DSCR(r1)
+ 	mfspr	r3,SPRN_AMOR
+ 	std	r3,_AMOR(r1)
+ 	mfspr	r3,SPRN_WORT
+ 	std	r3,_WORT(r1)
+ 	mfspr	r3,SPRN_WORC
+ 	std	r3,_WORC(r1)
+ 	IDLE_STATE_ENTER_SEQ(PPC_WINKLE)
++>>>>>>> b32aadc1a8ed (powerpc/powernv: Fix race in updating core_idle_state)
  
  _GLOBAL(power7_idle)
  	/* Now check if user or arch enabled NAP mode */
@@@ -210,11 -299,167 +305,98 @@@ _GLOBAL(power7_wakeup_tb_loss
  BEGIN_FTR_SECTION
  	CHECK_HMI_INTERRUPT
  END_FTR_SECTION_IFSET(CPU_FTR_HVMODE)
++<<<<<<< HEAD
++=======
+ 
+ 	lbz	r7,PACA_THREAD_MASK(r13)
+ 	ld	r14,PACA_CORE_IDLE_STATE_PTR(r13)
+ lwarx_loop2:
+ 	lwarx	r15,0,r14
+ 	andi.	r9,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	/*
+ 	 * Lock bit is set in one of the 2 cases-
+ 	 * a. In the sleep/winkle enter path, the last thread is executing
+ 	 * fastsleep workaround code.
+ 	 * b. In the wake up path, another thread is executing fastsleep
+ 	 * workaround undo code or resyncing timebase or restoring context
+ 	 * In either case loop until the lock bit is cleared.
+ 	 */
+ 	bnel	core_idle_lock_held
+ 
+ 	cmpwi	cr2,r15,0
+ 	lbz	r4,PACA_SUBCORE_SIBLING_MASK(r13)
+ 	and	r4,r4,r15
+ 	cmpwi	cr1,r4,0	/* Check if first in subcore */
+ 
+ 	/*
+ 	 * At this stage
+ 	 * cr1 - 0b0100 if first thread to wakeup in subcore
+ 	 * cr2 - 0b0100 if first thread to wakeup in core
+ 	 * cr3-  0b0010 if waking up from sleep or winkle
+ 	 * cr4 - 0b0100 if waking up from winkle
+ 	 */
+ 
+ 	or	r15,r15,r7		/* Set thread bit */
+ 
+ 	beq	cr1,first_thread_in_subcore
+ 
+ 	/* Not first thread in subcore to wake up */
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop2
+ 	isync
+ 	b	common_exit
+ 
+ first_thread_in_subcore:
+ 	/* First thread in subcore to wakeup */
+ 	ori	r15,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop2
+ 	isync
+ 
+ 	/*
+ 	 * If waking up from sleep, subcore state is not lost. Hence
+ 	 * skip subcore state restore
+ 	 */
+ 	bne	cr4,subcore_state_restored
+ 
+ 	/* Restore per-subcore state */
+ 	ld      r4,_SDR1(r1)
+ 	mtspr   SPRN_SDR1,r4
+ 	ld      r4,_RPR(r1)
+ 	mtspr   SPRN_RPR,r4
+ 	ld	r4,_AMOR(r1)
+ 	mtspr	SPRN_AMOR,r4
+ 
+ subcore_state_restored:
+ 	/*
+ 	 * Check if the thread is also the first thread in the core. If not,
+ 	 * skip to clear_lock.
+ 	 */
+ 	bne	cr2,clear_lock
+ 
+ first_thread_in_core:
+ 
+ 	/*
+ 	 * First thread in the core waking up from fastsleep. It needs to
+ 	 * call the fastsleep workaround code if the platform requires it.
+ 	 * Call it unconditionally here. The below branch instruction will
+ 	 * be patched out when the idle states are discovered if platform
+ 	 * does not require workaround.
+ 	 */
+ .global pnv_fastsleep_workaround_at_exit
+ pnv_fastsleep_workaround_at_exit:
+ 	b	fastsleep_workaround_at_exit
+ 
+ timebase_resync:
+ 	/* Do timebase resync if we are waking up from sleep. Use cr3 value
+ 	 * set in exceptions-64s.S */
+ 	ble	cr3,clear_lock
++>>>>>>> b32aadc1a8ed (powerpc/powernv: Fix race in updating core_idle_state)
  	/* Time base re-sync */
 -	li	r0,OPAL_RESYNC_TIMEBASE
 +	li	r3,OPAL_RESYNC_TIMEBASE
  	bl	opal_call_realmode;
 -	/* TODO: Check r3 for failure */
 -
 -	/*
 -	 * If waking up from sleep, per core state is not lost, skip to
 -	 * clear_lock.
 -	 */
 -	bne	cr4,clear_lock
 -
 -	/* Restore per core state */
 -	ld	r4,_TSCR(r1)
 -	mtspr	SPRN_TSCR,r4
 -	ld	r4,_WORC(r1)
 -	mtspr	SPRN_WORC,r4
 -
 -clear_lock:
 -	andi.	r15,r15,PNV_CORE_IDLE_THREAD_BITS
 -	lwsync
 -	stw	r15,0(r14)
 -
 -common_exit:
 -	/*
 -	 * Common to all threads.
 -	 *
 -	 * If waking up from sleep, hypervisor state is not lost. Hence
 -	 * skip hypervisor state restore.
 -	 */
 -	bne	cr4,hypervisor_state_restored
 -
 -	/* Waking up from winkle */
 -
 -	/* Restore per thread state */
 -	bl	__restore_cpu_power8
 -
 -	/* Restore SLB  from PACA */
 -	ld	r8,PACA_SLBSHADOWPTR(r13)
 -
 -	.rept	SLB_NUM_BOLTED
 -	li	r3, SLBSHADOW_SAVEAREA
 -	LDX_BE	r5, r8, r3
 -	addi	r3, r3, 8
 -	LDX_BE	r6, r8, r3
 -	andis.	r7,r5,SLB_ESID_V@h
 -	beq	1f
 -	slbmte	r6,r5
 -1:	addi	r8,r8,16
 -	.endr
 -
 -	ld	r4,_SPURR(r1)
 -	mtspr	SPRN_SPURR,r4
 -	ld	r4,_PURR(r1)
 -	mtspr	SPRN_PURR,r4
 -	ld	r4,_DSCR(r1)
 -	mtspr	SPRN_DSCR,r4
 -	ld	r4,_WORT(r1)
 -	mtspr	SPRN_WORT,r4
  
 -hypervisor_state_restored:
 -
 -	li	r5,PNV_THREAD_RUNNING
 -	stb     r5,PACA_THREAD_IDLE_STATE(r13)
 -
 -	mtspr	SPRN_SRR1,r16
 -#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 -	li      r0,KVM_HWTHREAD_IN_KERNEL
 -	stb     r0,HSTATE_HWTHREAD_STATE(r13)
 -	/* Order setting hwthread_state vs. testing hwthread_req */
 -	sync
 -	lbz     r0,HSTATE_HWTHREAD_REQ(r13)
 -	cmpwi   r0,0
 -	beq     6f
 -	b       kvm_start_guest
 -6:
 -#endif
 +	/* TODO: Check r3 for failure */
  
  	REST_NVGPRS(r1)
  	REST_GPR(2, r1)
* Unmerged path arch/powerpc/kernel/idle_power7.S
