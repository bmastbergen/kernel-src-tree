tcp: use dctcp if enabled on the route to the initiator

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit c3a8d9474684d391b0afc3970d9b249add15ec07
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/c3a8d947.failed

Currently, the following case doesn't use DCTCP, even if it should:
A responder has f.e. Cubic as system wide default, but for a specific
route to the initiating host, DCTCP is being set in RTAX_CC_ALGO. The
initiating host then uses DCTCP as congestion control, but since the
initiator sets ECT(0), tcp_ecn_create_request() doesn't set ecn_ok,
and we have to fall back to Reno after 3WHS completes.

We were thinking on how to solve this in a minimal, non-intrusive
way without bloating tcp_ecn_create_request() needlessly: lets cache
the CA ecn option flag in RTAX_FEATURES. In other words, when ECT(0)
is set on the SYN packet, set ecn_ok=1 iff route RTAX_FEATURES
contains the unexposed (internal-only) DST_FEATURE_ECN_CA. This allows
to only do a single metric feature lookup inside tcp_ecn_create_request().

Joint work with Florian Westphal.

	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: Florian Westphal <fw@strlen.de>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c3a8d9474684d391b0afc3970d9b249add15ec07)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/tcp.h
#	net/core/rtnetlink.c
#	net/ipv4/fib_semantics.c
#	net/ipv4/tcp_cong.c
#	net/ipv4/tcp_input.c
#	net/ipv6/route.c
diff --cc include/net/tcp.h
index aff4173509ce,0cab28cd43a9..000000000000
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@@ -828,26 -868,43 +828,47 @@@ struct tcp_congestion_ops 
  	struct module 	*owner;
  };
  
 -int tcp_register_congestion_control(struct tcp_congestion_ops *type);
 -void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
 -
 -void tcp_assign_congestion_control(struct sock *sk);
 -void tcp_init_congestion_control(struct sock *sk);
 -void tcp_cleanup_congestion_control(struct sock *sk);
 -int tcp_set_default_congestion_control(const char *name);
 -void tcp_get_default_congestion_control(char *name);
 -void tcp_get_available_congestion_control(char *buf, size_t len);
 -void tcp_get_allowed_congestion_control(char *buf, size_t len);
 -int tcp_set_allowed_congestion_control(char *allowed);
 -int tcp_set_congestion_control(struct sock *sk, const char *name);
 -u32 tcp_slow_start(struct tcp_sock *tp, u32 acked);
 -void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked);
 -
 -u32 tcp_reno_ssthresh(struct sock *sk);
 -void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 +extern int tcp_register_congestion_control(struct tcp_congestion_ops *type);
 +extern void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
 +
 +extern void tcp_init_congestion_control(struct sock *sk);
 +extern void tcp_cleanup_congestion_control(struct sock *sk);
 +extern int tcp_set_default_congestion_control(const char *name);
 +extern void tcp_get_default_congestion_control(char *name);
 +extern void tcp_get_available_congestion_control(char *buf, size_t len);
 +extern void tcp_get_allowed_congestion_control(char *buf, size_t len);
 +extern int tcp_set_allowed_congestion_control(char *allowed);
 +extern int tcp_set_congestion_control(struct sock *sk, const char *name);
 +extern void tcp_slow_start(struct tcp_sock *tp);
 +extern void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 +
 +extern struct tcp_congestion_ops tcp_init_congestion_ops;
 +extern u32 tcp_reno_ssthresh(struct sock *sk);
 +extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight);
 +extern u32 tcp_reno_min_cwnd(const struct sock *sk);
  extern struct tcp_congestion_ops tcp_reno;
  
++<<<<<<< HEAD
++=======
+ struct tcp_congestion_ops *tcp_ca_find_key(u32 key);
+ u32 tcp_ca_get_key_by_name(const char *name, bool *ecn_ca);
+ #ifdef CONFIG_INET
+ char *tcp_ca_get_name_by_key(u32 key, char *buffer);
+ #else
+ static inline char *tcp_ca_get_name_by_key(u32 key, char *buffer)
+ {
+ 	return NULL;
+ }
+ #endif
+ 
+ static inline bool tcp_ca_needs_ecn(const struct sock *sk)
+ {
+ 	const struct inet_connection_sock *icsk = inet_csk(sk);
+ 
+ 	return icsk->icsk_ca_ops->flags & TCP_CONG_NEEDS_ECN;
+ }
+ 
++>>>>>>> c3a8d9474684 (tcp: use dctcp if enabled on the route to the initiator)
  static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)
  {
  	struct inet_connection_sock *icsk = inet_csk(sk);
diff --cc net/core/rtnetlink.c
index 84ac04f7b576,a466821d1441..000000000000
--- a/net/core/rtnetlink.c
+++ b/net/core/rtnetlink.c
@@@ -607,9 -670,25 +607,30 @@@ int rtnetlink_put_metrics(struct sk_buf
  
  	for (i = 0; i < RTAX_MAX; i++) {
  		if (metrics[i]) {
++<<<<<<< HEAD
++=======
+ 			if (i == RTAX_CC_ALGO - 1) {
+ 				char tmp[TCP_CA_NAME_MAX], *name;
+ 
+ 				name = tcp_ca_get_name_by_key(metrics[i], tmp);
+ 				if (!name)
+ 					continue;
+ 				if (nla_put_string(skb, i + 1, name))
+ 					goto nla_put_failure;
+ 			} else if (i == RTAX_FEATURES - 1) {
+ 				u32 user_features = metrics[i] & RTAX_FEATURE_MASK;
+ 
+ 				BUILD_BUG_ON(RTAX_FEATURE_MASK & DST_FEATURE_MASK);
+ 				if (nla_put_u32(skb, i + 1, user_features))
+ 					goto nla_put_failure;
+ 			} else {
+ 				if (nla_put_u32(skb, i + 1, metrics[i]))
+ 					goto nla_put_failure;
+ 			}
++>>>>>>> c3a8d9474684 (tcp: use dctcp if enabled on the route to the initiator)
  			valid++;
 +			if (nla_put_u32(skb, i+1, metrics[i]))
 +				goto nla_put_failure;
  		}
  	}
  
diff --cc net/ipv4/fib_semantics.c
index 33ffde3d859d,992a9597daf8..000000000000
--- a/net/ipv4/fib_semantics.c
+++ b/net/ipv4/fib_semantics.c
@@@ -772,6 -859,67 +772,70 @@@ __be32 fib_info_update_nh_saddr(struct 
  	return nh->nh_saddr;
  }
  
++<<<<<<< HEAD
++=======
+ static bool fib_valid_prefsrc(struct fib_config *cfg, __be32 fib_prefsrc)
+ {
+ 	if (cfg->fc_type != RTN_LOCAL || !cfg->fc_dst ||
+ 	    fib_prefsrc != cfg->fc_dst) {
+ 		int tb_id = cfg->fc_table;
+ 
+ 		if (tb_id == RT_TABLE_MAIN)
+ 			tb_id = RT_TABLE_LOCAL;
+ 
+ 		if (inet_addr_type_table(cfg->fc_nlinfo.nl_net,
+ 					 fib_prefsrc, tb_id) != RTN_LOCAL) {
+ 			return false;
+ 		}
+ 	}
+ 	return true;
+ }
+ 
+ static int
+ fib_convert_metrics(struct fib_info *fi, const struct fib_config *cfg)
+ {
+ 	bool ecn_ca = false;
+ 	struct nlattr *nla;
+ 	int remaining;
+ 
+ 	if (!cfg->fc_mx)
+ 		return 0;
+ 
+ 	nla_for_each_attr(nla, cfg->fc_mx, cfg->fc_mx_len, remaining) {
+ 		int type = nla_type(nla);
+ 		u32 val;
+ 
+ 		if (!type)
+ 			continue;
+ 		if (type > RTAX_MAX)
+ 			return -EINVAL;
+ 
+ 		if (type == RTAX_CC_ALGO) {
+ 			char tmp[TCP_CA_NAME_MAX];
+ 
+ 			nla_strlcpy(tmp, nla, sizeof(tmp));
+ 			val = tcp_ca_get_key_by_name(tmp, &ecn_ca);
+ 			if (val == TCP_CA_UNSPEC)
+ 				return -EINVAL;
+ 		} else {
+ 			val = nla_get_u32(nla);
+ 		}
+ 		if (type == RTAX_ADVMSS && val > 65535 - 40)
+ 			val = 65535 - 40;
+ 		if (type == RTAX_MTU && val > 65535 - 15)
+ 			val = 65535 - 15;
+ 		if (type == RTAX_FEATURES && (val & ~RTAX_FEATURE_MASK))
+ 			return -EINVAL;
+ 		fi->fib_metrics[type - 1] = val;
+ 	}
+ 
+ 	if (ecn_ca)
+ 		fi->fib_metrics[RTAX_FEATURES - 1] |= DST_FEATURE_ECN_CA;
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> c3a8d9474684 (tcp: use dctcp if enabled on the route to the initiator)
  struct fib_info *fib_create_info(struct fib_config *cfg)
  {
  	int err;
diff --cc net/ipv4/tcp_cong.c
index 511547b93096,93c4dc3ab23f..000000000000
--- a/net/ipv4/tcp_cong.c
+++ b/net/ipv4/tcp_cong.c
@@@ -75,8 -105,52 +75,46 @@@ void tcp_unregister_congestion_control(
  }
  EXPORT_SYMBOL_GPL(tcp_unregister_congestion_control);
  
++<<<<<<< HEAD
++=======
+ u32 tcp_ca_get_key_by_name(const char *name, bool *ecn_ca)
+ {
+ 	const struct tcp_congestion_ops *ca;
+ 	u32 key = TCP_CA_UNSPEC;
+ 
+ 	might_sleep();
+ 
+ 	rcu_read_lock();
+ 	ca = __tcp_ca_find_autoload(name);
+ 	if (ca) {
+ 		key = ca->key;
+ 		*ecn_ca = ca->flags & TCP_CONG_NEEDS_ECN;
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	return key;
+ }
+ EXPORT_SYMBOL_GPL(tcp_ca_get_key_by_name);
+ 
+ char *tcp_ca_get_name_by_key(u32 key, char *buffer)
+ {
+ 	const struct tcp_congestion_ops *ca;
+ 	char *ret = NULL;
+ 
+ 	rcu_read_lock();
+ 	ca = tcp_ca_find_key(key);
+ 	if (ca)
+ 		ret = strncpy(buffer, ca->name,
+ 			      TCP_CA_NAME_MAX);
+ 	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(tcp_ca_get_name_by_key);
+ 
++>>>>>>> c3a8d9474684 (tcp: use dctcp if enabled on the route to the initiator)
  /* Assign choice of congestion control. */
 -void tcp_assign_congestion_control(struct sock *sk)
 +void tcp_init_congestion_control(struct sock *sk)
  {
  	struct inet_connection_sock *icsk = inet_csk(sk);
  	struct tcp_congestion_ops *ca;
diff --cc net/ipv4/tcp_input.c
index 18722c045417,a8f515bb19c4..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -5832,3 -5967,289 +5832,292 @@@ discard
  	return 0;
  }
  EXPORT_SYMBOL(tcp_rcv_state_process);
++<<<<<<< HEAD
++=======
+ 
+ static inline void pr_drop_req(struct request_sock *req, __u16 port, int family)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 
+ 	if (family == AF_INET)
+ 		net_dbg_ratelimited("drop open request from %pI4/%u\n",
+ 				    &ireq->ir_rmt_addr, port);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	else if (family == AF_INET6)
+ 		net_dbg_ratelimited("drop open request from %pI6/%u\n",
+ 				    &ireq->ir_v6_rmt_addr, port);
+ #endif
+ }
+ 
+ /* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set
+  *
+  * If we receive a SYN packet with these bits set, it means a
+  * network is playing bad games with TOS bits. In order to
+  * avoid possible false congestion notifications, we disable
+  * TCP ECN negotiation.
+  *
+  * Exception: tcp_ca wants ECN. This is required for DCTCP
+  * congestion control: Linux DCTCP asserts ECT on all packets,
+  * including SYN, which is most optimal solution; however,
+  * others, such as FreeBSD do not.
+  */
+ static void tcp_ecn_create_request(struct request_sock *req,
+ 				   const struct sk_buff *skb,
+ 				   const struct sock *listen_sk,
+ 				   const struct dst_entry *dst)
+ {
+ 	const struct tcphdr *th = tcp_hdr(skb);
+ 	const struct net *net = sock_net(listen_sk);
+ 	bool th_ecn = th->ece && th->cwr;
+ 	bool ect, ecn_ok;
+ 	u32 ecn_ok_dst;
+ 
+ 	if (!th_ecn)
+ 		return;
+ 
+ 	ect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);
+ 	ecn_ok_dst = dst_feature(dst, DST_FEATURE_ECN_MASK);
+ 	ecn_ok = net->ipv4.sysctl_tcp_ecn || ecn_ok_dst;
+ 
+ 	if ((!ect && ecn_ok) || tcp_ca_needs_ecn(listen_sk) ||
+ 	    (ecn_ok_dst & DST_FEATURE_ECN_CA))
+ 		inet_rsk(req)->ecn_ok = 1;
+ }
+ 
+ static void tcp_openreq_init(struct request_sock *req,
+ 			     const struct tcp_options_received *rx_opt,
+ 			     struct sk_buff *skb, const struct sock *sk)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 
+ 	req->rcv_wnd = 0;		/* So that tcp_send_synack() knows! */
+ 	req->cookie_ts = 0;
+ 	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
+ 	tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;
+ 	tcp_rsk(req)->snt_synack = tcp_time_stamp;
+ 	tcp_rsk(req)->last_oow_ack_time = 0;
+ 	req->mss = rx_opt->mss_clamp;
+ 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
+ 	ireq->tstamp_ok = rx_opt->tstamp_ok;
+ 	ireq->sack_ok = rx_opt->sack_ok;
+ 	ireq->snd_wscale = rx_opt->snd_wscale;
+ 	ireq->wscale_ok = rx_opt->wscale_ok;
+ 	ireq->acked = 0;
+ 	ireq->ecn_ok = 0;
+ 	ireq->ir_rmt_port = tcp_hdr(skb)->source;
+ 	ireq->ir_num = ntohs(tcp_hdr(skb)->dest);
+ 	ireq->ir_mark = inet_request_mark(sk, skb);
+ }
+ 
+ struct request_sock *inet_reqsk_alloc(const struct request_sock_ops *ops,
+ 				      struct sock *sk_listener)
+ {
+ 	struct request_sock *req = reqsk_alloc(ops, sk_listener);
+ 
+ 	if (req) {
+ 		struct inet_request_sock *ireq = inet_rsk(req);
+ 
+ 		kmemcheck_annotate_bitfield(ireq, flags);
+ 		ireq->opt = NULL;
+ 		atomic64_set(&ireq->ir_cookie, 0);
+ 		ireq->ireq_state = TCP_NEW_SYN_RECV;
+ 		write_pnet(&ireq->ireq_net, sock_net(sk_listener));
+ 		ireq->ireq_family = sk_listener->sk_family;
+ 	}
+ 
+ 	return req;
+ }
+ EXPORT_SYMBOL(inet_reqsk_alloc);
+ 
+ /*
+  * Return true if a syncookie should be sent
+  */
+ static bool tcp_syn_flood_action(struct sock *sk,
+ 				 const struct sk_buff *skb,
+ 				 const char *proto)
+ {
+ 	const char *msg = "Dropping request";
+ 	bool want_cookie = false;
+ 	struct listen_sock *lopt;
+ 
+ #ifdef CONFIG_SYN_COOKIES
+ 	if (sysctl_tcp_syncookies) {
+ 		msg = "Sending cookies";
+ 		want_cookie = true;
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPREQQFULLDOCOOKIES);
+ 	} else
+ #endif
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPREQQFULLDROP);
+ 
+ 	lopt = inet_csk(sk)->icsk_accept_queue.listen_opt;
+ 	if (!lopt->synflood_warned && sysctl_tcp_syncookies != 2) {
+ 		lopt->synflood_warned = 1;
+ 		pr_info("%s: Possible SYN flooding on port %d. %s.  Check SNMP counters.\n",
+ 			proto, ntohs(tcp_hdr(skb)->dest), msg);
+ 	}
+ 	return want_cookie;
+ }
+ 
+ static void tcp_reqsk_record_syn(const struct sock *sk,
+ 				 struct request_sock *req,
+ 				 const struct sk_buff *skb)
+ {
+ 	if (tcp_sk(sk)->save_syn) {
+ 		u32 len = skb_network_header_len(skb) + tcp_hdrlen(skb);
+ 		u32 *copy;
+ 
+ 		copy = kmalloc(len + sizeof(u32), GFP_ATOMIC);
+ 		if (copy) {
+ 			copy[0] = len;
+ 			memcpy(&copy[1], skb_network_header(skb), len);
+ 			req->saved_syn = copy;
+ 		}
+ 	}
+ }
+ 
+ int tcp_conn_request(struct request_sock_ops *rsk_ops,
+ 		     const struct tcp_request_sock_ops *af_ops,
+ 		     struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct tcp_options_received tmp_opt;
+ 	struct request_sock *req;
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 	struct dst_entry *dst = NULL;
+ 	__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;
+ 	bool want_cookie = false, fastopen;
+ 	struct flowi fl;
+ 	struct tcp_fastopen_cookie foc = { .len = -1 };
+ 	int err;
+ 
+ 
+ 	/* TW buckets are converted to open requests without
+ 	 * limitations, they conserve resources and peer is
+ 	 * evidently real one.
+ 	 */
+ 	if ((sysctl_tcp_syncookies == 2 ||
+ 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
+ 		want_cookie = tcp_syn_flood_action(sk, skb, rsk_ops->slab_name);
+ 		if (!want_cookie)
+ 			goto drop;
+ 	}
+ 
+ 
+ 	/* Accept backlog is full. If we have already queued enough
+ 	 * of warm entries in syn queue, drop request. It is better than
+ 	 * clogging syn queue with openreqs with exponentially increasing
+ 	 * timeout.
+ 	 */
+ 	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+ 		goto drop;
+ 	}
+ 
+ 	req = inet_reqsk_alloc(rsk_ops, sk);
+ 	if (!req)
+ 		goto drop;
+ 
+ 	tcp_rsk(req)->af_specific = af_ops;
+ 
+ 	tcp_clear_options(&tmp_opt);
+ 	tmp_opt.mss_clamp = af_ops->mss_clamp;
+ 	tmp_opt.user_mss  = tp->rx_opt.user_mss;
+ 	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
+ 
+ 	if (want_cookie && !tmp_opt.saw_tstamp)
+ 		tcp_clear_options(&tmp_opt);
+ 
+ 	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+ 	tcp_openreq_init(req, &tmp_opt, skb, sk);
+ 
+ 	/* Note: tcp_v6_init_req() might override ir_iif for link locals */
+ 	inet_rsk(req)->ir_iif = sk->sk_bound_dev_if;
+ 
+ 	af_ops->init_req(req, sk, skb);
+ 
+ 	if (security_inet_conn_request(sk, skb, req))
+ 		goto drop_and_free;
+ 
+ 	if (!want_cookie && !isn) {
+ 		/* VJ's idea. We save last timestamp seen
+ 		 * from the destination in peer table, when entering
+ 		 * state TIME-WAIT, and check against it before
+ 		 * accepting new connection request.
+ 		 *
+ 		 * If "isn" is not zero, this request hit alive
+ 		 * timewait bucket, so that all the necessary checks
+ 		 * are made in the function processing timewait state.
+ 		 */
+ 		if (tcp_death_row.sysctl_tw_recycle) {
+ 			bool strict;
+ 
+ 			dst = af_ops->route_req(sk, &fl, req, &strict);
+ 
+ 			if (dst && strict &&
+ 			    !tcp_peer_is_proven(req, dst, true,
+ 						tmp_opt.saw_tstamp)) {
+ 				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
+ 				goto drop_and_release;
+ 			}
+ 		}
+ 		/* Kill the following clause, if you dislike this way. */
+ 		else if (!sysctl_tcp_syncookies &&
+ 			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
+ 			  (sysctl_max_syn_backlog >> 2)) &&
+ 			 !tcp_peer_is_proven(req, dst, false,
+ 					     tmp_opt.saw_tstamp)) {
+ 			/* Without syncookies last quarter of
+ 			 * backlog is filled with destinations,
+ 			 * proven to be alive.
+ 			 * It means that we continue to communicate
+ 			 * to destinations, already remembered
+ 			 * to the moment of synflood.
+ 			 */
+ 			pr_drop_req(req, ntohs(tcp_hdr(skb)->source),
+ 				    rsk_ops->family);
+ 			goto drop_and_release;
+ 		}
+ 
+ 		isn = af_ops->init_seq(skb);
+ 	}
+ 	if (!dst) {
+ 		dst = af_ops->route_req(sk, &fl, req, NULL);
+ 		if (!dst)
+ 			goto drop_and_free;
+ 	}
+ 
+ 	tcp_ecn_create_request(req, skb, sk, dst);
+ 
+ 	if (want_cookie) {
+ 		isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
+ 		req->cookie_ts = tmp_opt.tstamp_ok;
+ 		if (!tmp_opt.tstamp_ok)
+ 			inet_rsk(req)->ecn_ok = 0;
+ 	}
+ 
+ 	tcp_rsk(req)->snt_isn = isn;
+ 	tcp_openreq_init_rwin(req, sk, dst);
+ 	fastopen = !want_cookie &&
+ 		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+ 	err = af_ops->send_synack(sk, dst, &fl, req,
+ 				  skb_get_queue_mapping(skb), &foc);
+ 	if (!fastopen) {
+ 		if (err || want_cookie)
+ 			goto drop_and_free;
+ 
+ 		tcp_rsk(req)->tfo_listener = false;
+ 		af_ops->queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+ 	}
+ 	tcp_reqsk_record_syn(sk, req, skb);
+ 
+ 	return 0;
+ 
+ drop_and_release:
+ 	dst_release(dst);
+ drop_and_free:
+ 	reqsk_free(req);
+ drop:
+ 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tcp_conn_request);
++>>>>>>> c3a8d9474684 (tcp: use dctcp if enabled on the route to the initiator)
diff --cc net/ipv6/route.c
index 40dd7c53aa88,f45cac6f8356..000000000000
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@@ -1452,9 -1695,58 +1452,64 @@@ out
  	return entries > rt_max_size;
  }
  
++<<<<<<< HEAD
 +/*
 + *
 + */
++=======
+ static int ip6_convert_metrics(struct mx6_config *mxc,
+ 			       const struct fib6_config *cfg)
+ {
+ 	bool ecn_ca = false;
+ 	struct nlattr *nla;
+ 	int remaining;
+ 	u32 *mp;
+ 
+ 	if (!cfg->fc_mx)
+ 		return 0;
+ 
+ 	mp = kzalloc(sizeof(u32) * RTAX_MAX, GFP_KERNEL);
+ 	if (unlikely(!mp))
+ 		return -ENOMEM;
+ 
+ 	nla_for_each_attr(nla, cfg->fc_mx, cfg->fc_mx_len, remaining) {
+ 		int type = nla_type(nla);
+ 		u32 val;
+ 
+ 		if (!type)
+ 			continue;
+ 		if (unlikely(type > RTAX_MAX))
+ 			goto err;
+ 
+ 		if (type == RTAX_CC_ALGO) {
+ 			char tmp[TCP_CA_NAME_MAX];
+ 
+ 			nla_strlcpy(tmp, nla, sizeof(tmp));
+ 			val = tcp_ca_get_key_by_name(tmp, &ecn_ca);
+ 			if (val == TCP_CA_UNSPEC)
+ 				goto err;
+ 		} else {
+ 			val = nla_get_u32(nla);
+ 		}
+ 		if (type == RTAX_FEATURES && (val & ~RTAX_FEATURE_MASK))
+ 			goto err;
+ 
+ 		mp[type - 1] = val;
+ 		__set_bit(type - 1, mxc->mx_valid);
+ 	}
+ 
+ 	if (ecn_ca) {
+ 		__set_bit(RTAX_FEATURES - 1, mxc->mx_valid);
+ 		mp[RTAX_FEATURES - 1] |= DST_FEATURE_ECN_CA;
+ 	}
+ 
+ 	mxc->mx = mp;
+ 	return 0;
+  err:
+ 	kfree(mp);
+ 	return -EINVAL;
+ }
++>>>>>>> c3a8d9474684 (tcp: use dctcp if enabled on the route to the initiator)
  
  int ip6_route_add(struct fib6_config *cfg)
  {
diff --git a/include/net/dst.h b/include/net/dst.h
index 82d356bbf71e..2f599bc7ec09 100644
--- a/include/net/dst.h
+++ b/include/net/dst.h
@@ -217,6 +217,12 @@ static inline void dst_metric_set(struct dst_entry *dst, int metric, u32 val)
 		p[metric-1] = val;
 }
 
+/* Kernel-internal feature bits that are unallocated in user space. */
+#define DST_FEATURE_ECN_CA	(1 << 31)
+
+#define DST_FEATURE_MASK	(DST_FEATURE_ECN_CA)
+#define DST_FEATURE_ECN_MASK	(DST_FEATURE_ECN_CA | RTAX_FEATURE_ECN)
+
 static inline u32
 dst_feature(const struct dst_entry *dst, u32 feature)
 {
* Unmerged path include/net/tcp.h
* Unmerged path net/core/rtnetlink.c
* Unmerged path net/ipv4/fib_semantics.c
* Unmerged path net/ipv4/tcp_cong.c
* Unmerged path net/ipv4/tcp_input.c
* Unmerged path net/ipv6/route.c
