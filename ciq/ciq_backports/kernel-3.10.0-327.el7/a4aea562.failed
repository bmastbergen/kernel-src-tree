NVMe: Convert to blk-mq

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Matias Bj√∏rling <m@bjorling.me>
commit a4aea5623d4a54682b6ff5c18196d7802f3e478f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/a4aea562.failed

This converts the NVMe driver to a blk-mq request-based driver.

The NVMe driver is currently bio-based and implements queue logic within
itself.  By using blk-mq, a lot of these responsibilities can be moved
and simplified.

The patch is divided into the following blocks:

 * Per-command data and cmdid have been moved into the struct request
   field. The cmdid_data can be retrieved using blk_mq_rq_to_pdu() and id
   maintenance are now handled by blk-mq through the rq->tag field.

 * The logic for splitting bio's has been moved into the blk-mq layer.
   The driver instead notifies the block layer about limited gap support in
   SG lists.

 * blk-mq handles timeouts and is reimplemented within nvme_timeout().
   This both includes abort handling and command cancelation.

 * Assignment of nvme queues to CPUs are replaced with the blk-mq
   version. The current blk-mq strategy is to assign the number of
   mapped queues and CPUs to provide synergy, while the nvme driver
   assign as many nvme hw queues as possible. This can be implemented in
   blk-mq if needed.

 * NVMe queues are merged with the tags structure of blk-mq.

 * blk-mq takes care of setup/teardown of nvme queues and guards invalid
   accesses. Therefore, RCU-usage for nvme queues can be removed.

 * IO tracing and accounting are handled by blk-mq and therefore removed.

 * Queue suspension logic is replaced with the logic from the block
   layer.

Contributions in this patch from:

  Sam Bradshaw <sbradshaw@micron.com>
  Jens Axboe <axboe@fb.com>
  Keith Busch <keith.busch@intel.com>
  Robert Nelson <rlnelson@google.com>

	Acked-by: Keith Busch <keith.busch@intel.com>
	Acked-by: Jens Axboe <axboe@fb.com>

Updated for new ->queue_rq() prototype.

	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit a4aea5623d4a54682b6ff5c18196d7802f3e478f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index dfa7c848f446,39050a3d10fd..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -573,167 -525,21 +524,182 @@@ int nvme_setup_prps(struct nvme_dev *de
  	return total_len;
  }
  
++<<<<<<< HEAD
 +struct nvme_bio_pair {
 +	struct bio b1, b2, *parent;
 +	struct bio_vec *bv1, *bv2;
 +	int err;
 +	atomic_t cnt;
 +};
 +
 +static void nvme_bio_pair_endio(struct bio *bio, int err)
 +{
 +	struct nvme_bio_pair *bp = bio->bi_private;
 +
 +	if (err)
 +		bp->err = err;
 +
 +	if (atomic_dec_and_test(&bp->cnt)) {
 +		bio_endio(bp->parent, bp->err);
 +		kfree(bp->bv1);
 +		kfree(bp->bv2);
 +		kfree(bp);
 +	}
 +}
 +
 +static struct nvme_bio_pair *nvme_bio_split(struct bio *bio, int idx,
 +							int len, int offset)
 +{
 +	struct nvme_bio_pair *bp;
 +
 +	BUG_ON(len > bio->bi_size);
 +	BUG_ON(idx > bio->bi_vcnt);
 +
 +	bp = kmalloc(sizeof(*bp), GFP_ATOMIC);
 +	if (!bp)
 +		return NULL;
 +	bp->err = 0;
 +
 +	bp->b1 = *bio;
 +	bp->b2 = *bio;
 +
 +	bp->b1.bi_size = len;
 +	bp->b2.bi_size -= len;
 +	bp->b1.bi_vcnt = idx;
 +	bp->b2.bi_idx = idx;
 +	bp->b2.bi_sector += len >> 9;
 +
 +	if (offset) {
 +		bp->bv1 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
 +								GFP_ATOMIC);
 +		if (!bp->bv1)
 +			goto split_fail_1;
 +
 +		bp->bv2 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
 +								GFP_ATOMIC);
 +		if (!bp->bv2)
 +			goto split_fail_2;
 +
 +		memcpy(bp->bv1, bio->bi_io_vec,
 +			bio->bi_max_vecs * sizeof(struct bio_vec));
 +		memcpy(bp->bv2, bio->bi_io_vec,
 +			bio->bi_max_vecs * sizeof(struct bio_vec));
 +
 +		bp->b1.bi_io_vec = bp->bv1;
 +		bp->b2.bi_io_vec = bp->bv2;
 +		bp->b2.bi_io_vec[idx].bv_offset += offset;
 +		bp->b2.bi_io_vec[idx].bv_len -= offset;
 +		bp->b1.bi_io_vec[idx].bv_len = offset;
 +		bp->b1.bi_vcnt++;
 +	} else
 +		bp->bv1 = bp->bv2 = NULL;
 +
 +	bp->b1.bi_private = bp;
 +	bp->b2.bi_private = bp;
 +
 +	bp->b1.bi_end_io = nvme_bio_pair_endio;
 +	bp->b2.bi_end_io = nvme_bio_pair_endio;
 +
 +	bp->parent = bio;
 +	atomic_set(&bp->cnt, 2);
 +
 +	return bp;
 +
 + split_fail_2:
 +	kfree(bp->bv1);
 + split_fail_1:
 +	kfree(bp);
 +	return NULL;
 +}
 +
 +static int nvme_split_and_submit(struct bio *bio, struct nvme_queue *nvmeq,
 +						int idx, int len, int offset)
 +{
 +	struct nvme_bio_pair *bp = nvme_bio_split(bio, idx, len, offset);
 +	if (!bp)
 +		return -ENOMEM;
 +
 +	trace_block_split(bdev_get_queue(bio->bi_bdev), bio,
 +					bio->bi_sector);
 +
 +	if (!waitqueue_active(&nvmeq->sq_full))
 +		add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
 +	bio_list_add(&nvmeq->sq_cong, &bp->b1);
 +	bio_list_add(&nvmeq->sq_cong, &bp->b2);
 +	wake_up(&nvmeq->sq_full);
 +
 +	return 0;
 +}
 +
 +/* NVMe scatterlists require no holes in the virtual address */
 +#define BIOVEC_NOT_VIRT_MERGEABLE(vec1, vec2)	((vec2)->bv_offset || \
 +			(((vec1)->bv_offset + (vec1)->bv_len) % PAGE_SIZE))
 +
 +static int nvme_map_bio(struct nvme_queue *nvmeq, struct nvme_iod *iod,
 +		struct bio *bio, enum dma_data_direction dma_dir, int psegs)
 +{
 +	struct bio_vec *bvec, *bvprv = NULL;
 +	struct scatterlist *sg = NULL;
 +	int i, length = 0, nsegs = 0, split_len = bio->bi_size;
 +
 +	if (nvmeq->dev->stripe_size)
 +		split_len = nvmeq->dev->stripe_size -
 +			((bio->bi_sector << 9) & (nvmeq->dev->stripe_size - 1));
 +
 +	sg_init_table(iod->sg, psegs);
 +	bio_for_each_segment(bvec, bio, i) {
 +		if (bvprv && BIOVEC_PHYS_MERGEABLE(bvprv, bvec)) {
 +			sg->length += bvec->bv_len;
 +		} else {
 +			if (bvprv && BIOVEC_NOT_VIRT_MERGEABLE(bvprv, bvec))
 +				return nvme_split_and_submit(bio, nvmeq, i,
 +								length, 0);
 +
 +			sg = sg ? sg + 1 : iod->sg;
 +			sg_set_page(sg, bvec->bv_page, bvec->bv_len,
 +							bvec->bv_offset);
 +			nsegs++;
 +		}
 +
 +		if (split_len - length < bvec->bv_len)
 +			return nvme_split_and_submit(bio, nvmeq, i, split_len,
 +							split_len - length);
 +		length += bvec->bv_len;
 +		bvprv = bvec;
 +	}
 +	iod->nents = nsegs;
 +	sg_mark_end(sg);
 +	if (dma_map_sg(nvmeq->q_dmadev, iod->sg, iod->nents, dma_dir) == 0)
 +		return -ENOMEM;
 +
 +	BUG_ON(length != bio->bi_size);
 +	return length;
 +}
 +
 +static int nvme_submit_discard(struct nvme_queue *nvmeq, struct nvme_ns *ns,
 +		struct bio *bio, struct nvme_iod *iod, int cmdid)
++=======
+ /*
+  * We reuse the small pool to allocate the 16-byte range here as it is not
+  * worth having a special pool for these or additional cases to handle freeing
+  * the iod.
+  */
+ static void nvme_submit_discard(struct nvme_queue *nvmeq, struct nvme_ns *ns,
+ 		struct request *req, struct nvme_iod *iod)
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  {
  	struct nvme_dsm_range *range =
  				(struct nvme_dsm_range *)iod_list(iod)[0];
  	struct nvme_command *cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
  
  	range->cattr = cpu_to_le32(0);
++<<<<<<< HEAD
 +	range->nlb = cpu_to_le32(bio->bi_size >> ns->lba_shift);
 +	range->slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
++=======
+ 	range->nlb = cpu_to_le32(blk_rq_bytes(req) >> ns->lba_shift);
+ 	range->slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  
  	memset(cmnd, 0, sizeof(*cmnd));
  	cmnd->dsm.opcode = nvme_cmd_dsm;
@@@ -803,9 -593,8 +753,14 @@@ static int nvme_submit_iod(struct nvme_
  	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
  	cmnd->rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
  	cmnd->rw.prp2 = cpu_to_le64(iod->first_dma);
++<<<<<<< HEAD
 +	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
 +	cmnd->rw.length =
 +		cpu_to_le16((bio->bi_size >> ns->lba_shift) - 1);
++=======
+ 	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+ 	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  	cmnd->rw.control = cpu_to_le16(control);
  	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
  
@@@ -813,47 -602,40 +768,72 @@@
  		nvmeq->sq_tail = 0;
  	writel(nvmeq->sq_tail, nvmeq->q_db);
  
 +	return 0;
 +
 +}
 +
++<<<<<<< HEAD
 +static int nvme_split_flush_data(struct nvme_queue *nvmeq, struct bio *bio)
 +{
 +	struct nvme_bio_pair *bp = nvme_bio_split(bio, 0, 0, 0);
 +	if (!bp)
 +		return -ENOMEM;
 +
 +	bp->b1.bi_phys_segments = 0;
 +	bp->b2.bi_rw &= ~REQ_FLUSH;
 +
 +	if (!waitqueue_active(&nvmeq->sq_full))
 +		add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
 +	bio_list_add(&nvmeq->sq_cong, &bp->b1);
 +	bio_list_add(&nvmeq->sq_cong, &bp->b2);
 +	wake_up(&nvmeq->sq_full);
 +
  	return 0;
  }
  
 +/*
 + * Called with local interrupts disabled and the q_lock held.  May not sleep.
 + */
 +static int nvme_submit_bio_queue(struct nvme_queue *nvmeq, struct nvme_ns *ns,
 +								struct bio *bio)
- {
++=======
+ static int nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
+ 			 const struct blk_mq_queue_data *bd)
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
+ {
+ 	struct nvme_ns *ns = hctx->queue->queuedata;
+ 	struct nvme_queue *nvmeq = hctx->driver_data;
+ 	struct request *req = bd->rq;
+ 	struct nvme_cmd_info *cmd = blk_mq_rq_to_pdu(req);
  	struct nvme_iod *iod;
++<<<<<<< HEAD
 +	int psegs = bio_phys_segments(ns->queue, bio);
 +	int result;
++=======
+ 	int psegs = req->nr_phys_segments;
+ 	int result = BLK_MQ_RQ_QUEUE_BUSY;
+ 	enum dma_data_direction dma_dir;
+ 	unsigned size = !(req->cmd_flags & REQ_DISCARD) ? blk_rq_bytes(req) :
+ 						sizeof(struct nvme_dsm_range);
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  
- 	if ((bio->bi_rw & REQ_FLUSH) && psegs)
- 		return nvme_split_flush_data(nvmeq, bio);
+ 	/*
+ 	 * Requeued IO has already been prepped
+ 	 */
+ 	iod = req->special;
+ 	if (iod)
+ 		goto submit_iod;
  
 -	iod = nvme_alloc_iod(psegs, size, ns->dev, GFP_ATOMIC);
 +	iod = nvme_alloc_iod(psegs, bio->bi_size, GFP_ATOMIC);
  	if (!iod)
- 		return -ENOMEM;
+ 		return result;
  
- 	iod->private = bio;
- 	if (bio->bi_rw & REQ_DISCARD) {
+ 	iod->private = req;
+ 	req->special = iod;
+ 
+ 	nvme_set_info(cmd, iod, req_completion);
+ 
+ 	if (req->cmd_flags & REQ_DISCARD) {
  		void *range;
  		/*
  		 * We reuse the small pool to allocate the 16-byte range here
@@@ -1054,38 -818,83 +1016,97 @@@ static int nvme_submit_sync_cmd(struct 
  	return cmdinfo.status;
  }
  
++<<<<<<< HEAD
 +int nvme_submit_async_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 +						struct async_cmd_info *cmdinfo,
 +						unsigned timeout)
++=======
+ static int nvme_submit_async_admin_req(struct nvme_dev *dev)
+ {
+ 	struct nvme_queue *nvmeq = dev->queues[0];
+ 	struct nvme_command c;
+ 	struct nvme_cmd_info *cmd_info;
+ 	struct request *req;
+ 
+ 	req = blk_mq_alloc_request(dev->admin_q, WRITE, GFP_KERNEL, false);
+ 	if (!req)
+ 		return -ENOMEM;
+ 
+ 	cmd_info = blk_mq_rq_to_pdu(req);
+ 	nvme_set_info(cmd_info, req, async_req_completion);
+ 
+ 	memset(&c, 0, sizeof(c));
+ 	c.common.opcode = nvme_admin_async_event;
+ 	c.common.command_id = req->tag;
+ 
+ 	return __nvme_submit_cmd(nvmeq, &c);
+ }
+ 
+ static int nvme_submit_admin_async_cmd(struct nvme_dev *dev,
+ 			struct nvme_command *cmd,
+ 			struct async_cmd_info *cmdinfo, unsigned timeout)
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  {
- 	int cmdid;
+ 	struct nvme_queue *nvmeq = dev->queues[0];
+ 	struct request *req;
+ 	struct nvme_cmd_info *cmd_rq;
+ 
+ 	req = blk_mq_alloc_request(dev->admin_q, WRITE, GFP_KERNEL, false);
+ 	if (!req)
+ 		return -ENOMEM;
  
- 	cmdid = alloc_cmdid_killable(nvmeq, cmdinfo, async_completion, timeout);
- 	if (cmdid < 0)
- 		return cmdid;
+ 	req->timeout = timeout;
+ 	cmd_rq = blk_mq_rq_to_pdu(req);
+ 	cmdinfo->req = req;
+ 	nvme_set_info(cmd_rq, cmdinfo, async_completion);
  	cmdinfo->status = -EINTR;
- 	cmd->common.command_id = cmdid;
+ 
+ 	cmd->common.command_id = req->tag;
+ 
  	return nvme_submit_cmd(nvmeq, cmd);
  }
  
- int nvme_submit_admin_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
- 								u32 *result)
+ int __nvme_submit_admin_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
+ 						u32 *result, unsigned timeout)
  {
- 	return nvme_submit_sync_cmd(dev, 0, cmd, result, ADMIN_TIMEOUT);
+ 	int res;
+ 	struct request *req;
+ 
+ 	req = blk_mq_alloc_request(dev->admin_q, WRITE, GFP_KERNEL, false);
+ 	if (!req)
+ 		return -ENOMEM;
+ 	res = nvme_submit_sync_cmd(req, cmd, result, timeout);
+ 	blk_put_request(req);
+ 	return res;
  }
  
- int nvme_submit_io_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
+ int nvme_submit_admin_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
  								u32 *result)
  {
- 	return nvme_submit_sync_cmd(dev, this_cpu_read(*dev->io_queue), cmd,
- 						result,	NVME_IO_TIMEOUT);
+ 	return __nvme_submit_admin_cmd(dev, cmd, result, ADMIN_TIMEOUT);
  }
  
+ int nvme_submit_io_cmd(struct nvme_dev *dev, struct nvme_ns *ns,
+ 					struct nvme_command *cmd, u32 *result)
+ {
+ 	int res;
+ 	struct request *req;
+ 
++<<<<<<< HEAD
 +int nvme_submit_admin_cmd_async(struct nvme_dev *dev, struct nvme_command *cmd,
 +						struct async_cmd_info *cmdinfo)
 +{
 +	return nvme_submit_async_cmd(raw_nvmeq(dev, 0), cmd, cmdinfo,
 +								ADMIN_TIMEOUT);
++=======
+ 	req = blk_mq_alloc_request(ns->queue, WRITE, (GFP_KERNEL|__GFP_WAIT),
+ 									false);
+ 	if (!req)
+ 		return -ENOMEM;
+ 	res = nvme_submit_sync_cmd(req, cmd, result, NVME_IO_TIMEOUT);
+ 	blk_put_request(req);
+ 	return res;
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  }
  
  static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
@@@ -1218,9 -1014,9 +1226,15 @@@ static void nvme_abort_req(struct reque
  			return;
  		list_del_init(&dev->node);
  		dev_warn(&dev->pci_dev->dev,
++<<<<<<< HEAD
 +			"I/O %d QID %d timeout, reset controller\n", cmdid,
 +								nvmeq->qid);
 +		PREPARE_WORK(&dev->reset_work, nvme_reset_failed_dev);
++=======
+ 			"I/O %d QID %d timeout, reset controller\n",
+ 							req->tag, nvmeq->qid);
+ 		dev->reset_workfn = nvme_reset_failed_dev;
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  		queue_work(nvme_workq, &dev->reset_work);
  		return;
  	}
@@@ -1228,83 -1024,75 +1242,81 @@@
  	if (!dev->abort_limit)
  		return;
  
- 	adminq = rcu_dereference(dev->queues[0]);
- 	a_cmdid = alloc_cmdid(adminq, CMD_CTX_ABORT, special_completion,
- 								ADMIN_TIMEOUT);
- 	if (a_cmdid < 0)
+ 	abort_req = blk_mq_alloc_request(dev->admin_q, WRITE, GFP_ATOMIC,
+ 									false);
+ 	if (!abort_req)
  		return;
  
+ 	abort_cmd = blk_mq_rq_to_pdu(abort_req);
+ 	nvme_set_info(abort_cmd, abort_req, abort_completion);
+ 
  	memset(&cmd, 0, sizeof(cmd));
  	cmd.abort.opcode = nvme_admin_abort_cmd;
++<<<<<<< HEAD
 +	cmd.abort.cid = cmdid;
 +	cmd.abort.sqid = nvmeq->qid;
 +	cmd.abort.command_id = a_cmdid;
++=======
+ 	cmd.abort.cid = req->tag;
+ 	cmd.abort.sqid = cpu_to_le16(nvmeq->qid);
+ 	cmd.abort.command_id = abort_req->tag;
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  
  	--dev->abort_limit;
- 	info[cmdid].aborted = 1;
- 	info[cmdid].timeout = jiffies + ADMIN_TIMEOUT;
+ 	cmd_rq->aborted = 1;
  
- 	dev_warn(nvmeq->q_dmadev, "Aborting I/O %d QID %d\n", cmdid,
+ 	dev_warn(nvmeq->q_dmadev, "Aborting I/O %d QID %d\n", req->tag,
  							nvmeq->qid);
- 	nvme_submit_cmd(adminq, &cmd);
+ 	if (nvme_submit_cmd(dev->queues[0], &cmd) < 0) {
+ 		dev_warn(nvmeq->q_dmadev,
+ 				"Could not abort I/O %d QID %d",
+ 				req->tag, nvmeq->qid);
+ 		blk_put_request(req);
+ 	}
  }
  
- /**
-  * nvme_cancel_ios - Cancel outstanding I/Os
-  * @queue: The queue to cancel I/Os on
-  * @timeout: True to only cancel I/Os which have timed out
-  */
- static void nvme_cancel_ios(struct nvme_queue *nvmeq, bool timeout)
+ static void nvme_cancel_queue_ios(struct blk_mq_hw_ctx *hctx,
+ 				struct request *req, void *data, bool reserved)
  {
- 	int depth = nvmeq->q_depth - 1;
- 	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
- 	unsigned long now = jiffies;
- 	int cmdid;
+ 	struct nvme_queue *nvmeq = data;
+ 	void *ctx;
+ 	nvme_completion_fn fn;
+ 	struct nvme_cmd_info *cmd;
+ 	static struct nvme_completion cqe = {
+ 		.status = cpu_to_le16(NVME_SC_ABORT_REQ << 1),
+ 	};
  
- 	for_each_set_bit(cmdid, nvmeq->cmdid_data, depth) {
- 		void *ctx;
- 		nvme_completion_fn fn;
- 		static struct nvme_completion cqe = {
- 			.status = cpu_to_le16(NVME_SC_ABORT_REQ << 1),
- 		};
+ 	cmd = blk_mq_rq_to_pdu(req);
  
- 		if (timeout && !time_after(now, info[cmdid].timeout))
- 			continue;
- 		if (info[cmdid].ctx == CMD_CTX_CANCELLED)
- 			continue;
- 		if (timeout && info[cmdid].ctx == CMD_CTX_ASYNC)
- 			continue;
- 		if (timeout && nvmeq->dev->initialized) {
- 			nvme_abort_cmd(cmdid, nvmeq);
- 			continue;
- 		}
- 		dev_warn(nvmeq->q_dmadev, "Cancelling I/O %d QID %d\n", cmdid,
- 								nvmeq->qid);
- 		ctx = cancel_cmdid(nvmeq, cmdid, &fn);
- 		fn(nvmeq, ctx, &cqe);
- 	}
+ 	if (cmd->ctx == CMD_CTX_CANCELLED)
+ 		return;
+ 
+ 	dev_warn(nvmeq->q_dmadev, "Cancelling I/O %d QID %d\n",
+ 						req->tag, nvmeq->qid);
+ 	ctx = cancel_cmd_info(cmd, &fn);
+ 	fn(nvmeq, ctx, &cqe);
  }
  
- static void nvme_free_queue(struct nvme_queue *nvmeq)
+ static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
  {
- 	spin_lock_irq(&nvmeq->q_lock);
- 	while (bio_list_peek(&nvmeq->sq_cong)) {
- 		struct bio *bio = bio_list_pop(&nvmeq->sq_cong);
- 		bio_endio(bio, -EIO);
- 	}
- 	while (!list_empty(&nvmeq->iod_bio)) {
- 		static struct nvme_completion cqe = {
- 			.status = cpu_to_le16(
- 				(NVME_SC_ABORT_REQ | NVME_SC_DNR) << 1),
- 		};
- 		struct nvme_iod *iod = list_first_entry(&nvmeq->iod_bio,
- 							struct nvme_iod,
- 							node);
- 		list_del(&iod->node);
- 		bio_completion(nvmeq, iod, &cqe);
- 	}
- 	spin_unlock_irq(&nvmeq->q_lock);
+ 	struct nvme_cmd_info *cmd = blk_mq_rq_to_pdu(req);
+ 	struct nvme_queue *nvmeq = cmd->nvmeq;
+ 
+ 	dev_warn(nvmeq->q_dmadev, "Timeout I/O %d QID %d\n", req->tag,
+ 							nvmeq->qid);
+ 	if (nvmeq->dev->initialized)
+ 		nvme_abort_req(req);
+ 
+ 	/*
+ 	 * The aborted req will be completed on receiving the abort req.
+ 	 * We enable the timer again. If hit twice, it'll cause a device reset,
+ 	 * as the device then is in a faulty state.
+ 	 */
+ 	return BLK_EH_RESET_TIMER;
+ }
  
+ static void nvme_free_queue(struct nvme_queue *nvmeq)
+ {
  	dma_free_coherent(nvmeq->q_dmadev, CQ_SIZE(nvmeq->q_depth),
  				(void *)nvmeq->cqes, nvmeq->cq_dma_addr);
  	dma_free_coherent(nvmeq->q_dmadev, SQ_SIZE(nvmeq->q_depth),
@@@ -2033,9 -1832,9 +2033,15 @@@ static int nvme_kthread(void *data
  					continue;
  				list_del_init(&dev->node);
  				dev_warn(&dev->pci_dev->dev,
++<<<<<<< HEAD
 +					"Failed status, reset controller\n");
 +				PREPARE_WORK(&dev->reset_work,
 +							nvme_reset_failed_dev);
++=======
+ 					"Failed status: %x, reset controller\n",
+ 					readl(&dev->bar->csts));
+ 				dev->reset_workfn = nvme_reset_failed_dev;
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  				queue_work(nvme_workq, &dev->reset_work);
  				continue;
  			}
@@@ -2955,29 -2635,36 +2857,35 @@@ static void nvme_reset_failed_dev(struc
  	nvme_dev_reset(dev);
  }
  
 -static void nvme_reset_workfn(struct work_struct *work)
 -{
 -	struct nvme_dev *dev = container_of(work, struct nvme_dev, reset_work);
 -	dev->reset_workfn(work);
 -}
 -
  static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
  {
- 	int result = -ENOMEM;
+ 	int node, result = -ENOMEM;
  	struct nvme_dev *dev;
  
- 	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+ 	node = dev_to_node(&pdev->dev);
+ 	if (node == NUMA_NO_NODE)
+ 		set_dev_node(&pdev->dev, 0);
+ 
+ 	dev = kzalloc_node(sizeof(*dev), GFP_KERNEL, node);
  	if (!dev)
  		return -ENOMEM;
- 	dev->entry = kcalloc(num_possible_cpus(), sizeof(*dev->entry),
- 								GFP_KERNEL);
+ 	dev->entry = kzalloc_node(num_possible_cpus() * sizeof(*dev->entry),
+ 							GFP_KERNEL, node);
  	if (!dev->entry)
  		goto free;
- 	dev->queues = kcalloc(num_possible_cpus() + 1, sizeof(void *),
- 								GFP_KERNEL);
+ 	dev->queues = kzalloc_node((num_possible_cpus() + 1) * sizeof(void *),
+ 							GFP_KERNEL, node);
  	if (!dev->queues)
  		goto free;
- 	dev->io_queue = alloc_percpu(unsigned short);
- 	if (!dev->io_queue)
- 		goto free;
  
  	INIT_LIST_HEAD(&dev->namespaces);
++<<<<<<< HEAD
 +	INIT_WORK(&dev->reset_work, nvme_reset_failed_dev);
 +	INIT_WORK(&dev->cpu_work, nvme_cpu_workfn);
++=======
+ 	dev->reset_workfn = nvme_reset_failed_dev;
+ 	INIT_WORK(&dev->reset_work, nvme_reset_workfn);
++>>>>>>> a4aea5623d4a (NVMe: Convert to blk-mq)
  	dev->pci_dev = pci_dev_get(pdev);
  	pci_set_drvdata(pdev, dev);
  	result = nvme_set_instance(dev);
* Unmerged path drivers/block/nvme-core.c
diff --git a/drivers/block/nvme-scsi.c b/drivers/block/nvme-scsi.c
index 046ae3321c5e..49f86d1a5aa2 100644
--- a/drivers/block/nvme-scsi.c
+++ b/drivers/block/nvme-scsi.c
@@ -2105,7 +2105,7 @@ static int nvme_trans_do_nvme_io(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 
 		nvme_offset += unit_num_blocks;
 
-		nvme_sc = nvme_submit_io_cmd(dev, &c, NULL);
+		nvme_sc = nvme_submit_io_cmd(dev, ns, &c, NULL);
 		if (nvme_sc != NVME_SC_SUCCESS) {
 			nvme_unmap_user_pages(dev,
 				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
@@ -2658,7 +2658,7 @@ static int nvme_trans_start_stop(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 			c.common.opcode = nvme_cmd_flush;
 			c.common.nsid = cpu_to_le32(ns->ns_id);
 
-			nvme_sc = nvme_submit_io_cmd(ns->dev, &c, NULL);
+			nvme_sc = nvme_submit_io_cmd(ns->dev, ns, &c, NULL);
 			res = nvme_trans_status_code(hdr, nvme_sc);
 			if (res)
 				goto out;
@@ -2686,7 +2686,7 @@ static int nvme_trans_synchronize_cache(struct nvme_ns *ns,
 	c.common.opcode = nvme_cmd_flush;
 	c.common.nsid = cpu_to_le32(ns->ns_id);
 
-	nvme_sc = nvme_submit_io_cmd(ns->dev, &c, NULL);
+	nvme_sc = nvme_submit_io_cmd(ns->dev, ns, &c, NULL);
 
 	res = nvme_trans_status_code(hdr, nvme_sc);
 	if (res)
@@ -2894,7 +2894,7 @@ static int nvme_trans_unmap(struct nvme_ns *ns, struct sg_io_hdr *hdr,
 	c.dsm.nr = cpu_to_le32(ndesc - 1);
 	c.dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
 
-	nvme_sc = nvme_submit_io_cmd(dev, &c, NULL);
+	nvme_sc = nvme_submit_io_cmd(dev, ns, &c, NULL);
 	res = nvme_trans_status_code(hdr, nvme_sc);
 
 	dma_free_coherent(&dev->pci_dev->dev, ndesc * sizeof(*range),
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index 40b77ddf02c4..bc4e10137e41 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -19,6 +19,7 @@
 #include <linux/pci.h>
 #include <linux/miscdevice.h>
 #include <linux/kref.h>
+#include <linux/blk-mq.h>
 
 struct nvme_bar {
 	__u64			cap;	/* Controller Capabilities */
@@ -70,8 +71,10 @@ extern unsigned char nvme_io_timeout;
  */
 struct nvme_dev {
 	struct list_head node;
-	struct nvme_queue __rcu **queues;
-	unsigned short __percpu *io_queue;
+	struct nvme_queue **queues;
+	struct request_queue *admin_q;
+	struct blk_mq_tag_set tagset;
+	struct blk_mq_tag_set admin_tagset;
 	u32 __iomem *dbs;
 	struct pci_dev *pci_dev;
 	struct dma_pool *prp_page_pool;
@@ -89,7 +92,6 @@ struct nvme_dev {
 	struct kref kref;
 	struct miscdevice miscdev;
 	struct work_struct reset_work;
-	struct work_struct cpu_work;
 	char name[12];
 	char serial[20];
 	char model[40];
@@ -132,7 +134,6 @@ struct nvme_iod {
 	int offset;		/* Of PRP list */
 	int nents;		/* Used in scatterlist */
 	int length;		/* Of data, in bytes */
-	unsigned long start_time;
 	dma_addr_t first_dma;
 	struct list_head node;
 	struct scatterlist sg[0];
@@ -150,12 +151,14 @@ static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
  */
 void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod);
 
-int nvme_setup_prps(struct nvme_dev *, struct nvme_iod *, int , gfp_t);
+int nvme_setup_prps(struct nvme_dev *, struct nvme_iod *, int, gfp_t);
 struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
 				unsigned long addr, unsigned length);
 void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
 			struct nvme_iod *iod);
-int nvme_submit_io_cmd(struct nvme_dev *, struct nvme_command *, u32 *);
+int nvme_submit_io_cmd(struct nvme_dev *, struct nvme_ns *,
+						struct nvme_command *, u32 *);
+int nvme_submit_flush_data(struct nvme_queue *nvmeq, struct nvme_ns *ns);
 int nvme_submit_admin_cmd(struct nvme_dev *, struct nvme_command *,
 							u32 *result);
 int nvme_identify(struct nvme_dev *, unsigned nsid, unsigned cns,
