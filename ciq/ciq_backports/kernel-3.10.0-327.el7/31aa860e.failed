bonding: add bond_tx_drop() helper

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 31aa860e0aafd3a7c5a31c2aae67b6534115ea41
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/31aa860e.failed

Because bonding stats are usually sum of slave stats, it was
not easy to account for tx drops at bonding layer.

We can use dev->tx_dropped for this, as this counter is later
added to the device stats (in dev_get_stats())

This extends the idea we had in commit ee6377147409a ("bonding: Simplify
the xmit function for modes that use xmit_hash") for bond_3ad_xor_xmit()
to other bonding modes.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Mahesh Bandewar <maheshb@google.com>
	Reviewed-by: Nikolay Aleksandrov <nikolay@redhat.com>
	Acked-by: Mahesh Bandewar <maheshb@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 31aa860e0aafd3a7c5a31c2aae67b6534115ea41)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/bonding/bond_alb.c
#	drivers/net/bonding/bond_main.c
diff --cc drivers/net/bonding/bond_alb.c
index 06a8df1ef842,baa58e79256a..000000000000
--- a/drivers/net/bonding/bond_alb.c
+++ b/drivers/net/bonding/bond_alb.c
@@@ -1313,9 -1292,81 +1313,71 @@@ void bond_alb_deinitialize(struct bondi
  
  	tlb_deinitialize(bond);
  
 -	if (bond_info->rlb_enabled)
 +	if (bond_info->rlb_enabled) {
  		rlb_deinitialize(bond);
 -}
 -
 -static int bond_do_alb_xmit(struct sk_buff *skb, struct bonding *bond,
 -			    struct slave *tx_slave)
 -{
 -	struct alb_bond_info *bond_info = &(BOND_ALB_INFO(bond));
 -	struct ethhdr *eth_data = eth_hdr(skb);
 -
 -	if (!tx_slave) {
 -		/* unbalanced or unassigned, send through primary */
 -		tx_slave = rcu_dereference(bond->curr_active_slave);
 -		if (bond->params.tlb_dynamic_lb)
 -			bond_info->unbalanced_load += skb->len;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	if (tx_slave && bond_slave_can_tx(tx_slave)) {
+ 		if (tx_slave != rcu_access_pointer(bond->curr_active_slave)) {
+ 			ether_addr_copy(eth_data->h_source,
+ 					tx_slave->dev->dev_addr);
+ 		}
+ 
+ 		bond_dev_queue_xmit(bond, skb, tx_slave->dev);
+ 		goto out;
+ 	}
+ 
+ 	if (tx_slave && bond->params.tlb_dynamic_lb) {
+ 		spin_lock(&bond->mode_lock);
+ 		__tlb_clear_slave(bond, tx_slave, 0);
+ 		spin_unlock(&bond->mode_lock);
+ 	}
+ 
+ 	/* no suitable interface, frame not sent */
+ 	bond_tx_drop(bond->dev, skb);
+ out:
+ 	return NETDEV_TX_OK;
+ }
+ 
+ int bond_tlb_xmit(struct sk_buff *skb, struct net_device *bond_dev)
+ {
+ 	struct bonding *bond = netdev_priv(bond_dev);
+ 	struct ethhdr *eth_data;
+ 	struct slave *tx_slave = NULL;
+ 	u32 hash_index;
+ 
+ 	skb_reset_mac_header(skb);
+ 	eth_data = eth_hdr(skb);
+ 
+ 	/* Do not TX balance any multicast or broadcast */
+ 	if (!is_multicast_ether_addr(eth_data->h_dest)) {
+ 		switch (skb->protocol) {
+ 		case htons(ETH_P_IP):
+ 		case htons(ETH_P_IPX):
+ 		    /* In case of IPX, it will falback to L2 hash */
+ 		case htons(ETH_P_IPV6):
+ 			hash_index = bond_xmit_hash(bond, skb);
+ 			if (bond->params.tlb_dynamic_lb) {
+ 				tx_slave = tlb_choose_channel(bond,
+ 							      hash_index & 0xFF,
+ 							      skb->len);
+ 			} else {
+ 				struct bond_up_slave *slaves;
+ 				unsigned int count;
+ 
+ 				slaves = rcu_dereference(bond->slave_arr);
+ 				count = slaves ? ACCESS_ONCE(slaves->count) : 0;
+ 				if (likely(count))
+ 					tx_slave = slaves->arr[hash_index %
+ 							       count];
+ 			}
+ 			break;
+ 		}
+ 	}
+ 	return bond_do_alb_xmit(skb, bond, tx_slave);
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
  }
  
  int bond_alb_xmit(struct sk_buff *skb, struct net_device *bond_dev)
diff --cc drivers/net/bonding/bond_main.c
index 723c3d160745,c7520082fb0d..000000000000
--- a/drivers/net/bonding/bond_main.c
+++ b/drivers/net/bonding/bond_main.c
@@@ -3839,6 -3485,79 +3839,82 @@@ unwind
  	return res;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * bond_xmit_slave_id - transmit skb through slave with slave_id
+  * @bond: bonding device that is transmitting
+  * @skb: buffer to transmit
+  * @slave_id: slave id up to slave_cnt-1 through which to transmit
+  *
+  * This function tries to transmit through slave with slave_id but in case
+  * it fails, it tries to find the first available slave for transmission.
+  * The skb is consumed in all cases, thus the function is void.
+  */
+ static void bond_xmit_slave_id(struct bonding *bond, struct sk_buff *skb, int slave_id)
+ {
+ 	struct list_head *iter;
+ 	struct slave *slave;
+ 	int i = slave_id;
+ 
+ 	/* Here we start from the slave with slave_id */
+ 	bond_for_each_slave_rcu(bond, slave, iter) {
+ 		if (--i < 0) {
+ 			if (bond_slave_can_tx(slave)) {
+ 				bond_dev_queue_xmit(bond, skb, slave->dev);
+ 				return;
+ 			}
+ 		}
+ 	}
+ 
+ 	/* Here we start from the first slave up to slave_id */
+ 	i = slave_id;
+ 	bond_for_each_slave_rcu(bond, slave, iter) {
+ 		if (--i < 0)
+ 			break;
+ 		if (bond_slave_can_tx(slave)) {
+ 			bond_dev_queue_xmit(bond, skb, slave->dev);
+ 			return;
+ 		}
+ 	}
+ 	/* no slave that can tx has been found */
+ 	bond_tx_drop(bond->dev, skb);
+ }
+ 
+ /**
+  * bond_rr_gen_slave_id - generate slave id based on packets_per_slave
+  * @bond: bonding device to use
+  *
+  * Based on the value of the bonding device's packets_per_slave parameter
+  * this function generates a slave id, which is usually used as the next
+  * slave to transmit through.
+  */
+ static u32 bond_rr_gen_slave_id(struct bonding *bond)
+ {
+ 	u32 slave_id;
+ 	struct reciprocal_value reciprocal_packets_per_slave;
+ 	int packets_per_slave = bond->params.packets_per_slave;
+ 
+ 	switch (packets_per_slave) {
+ 	case 0:
+ 		slave_id = prandom_u32();
+ 		break;
+ 	case 1:
+ 		slave_id = bond->rr_tx_counter;
+ 		break;
+ 	default:
+ 		reciprocal_packets_per_slave =
+ 			bond->params.reciprocal_packets_per_slave;
+ 		slave_id = reciprocal_divide(bond->rr_tx_counter,
+ 					     reciprocal_packets_per_slave);
+ 		break;
+ 	}
+ 	bond->rr_tx_counter++;
+ 
+ 	return slave_id;
+ }
+ 
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
  static int bond_xmit_roundrobin(struct sk_buff *skb, struct net_device *bond_dev)
  {
  	struct bonding *bond = netdev_priv(bond_dev);
@@@ -3853,33 -3571,20 +3929,41 @@@
  	 * send the join/membership reports.  The curr_active_slave found
  	 * will send all of this type of traffic.
  	 */
 -	if (iph->protocol == IPPROTO_IGMP && skb->protocol == htons(ETH_P_IP)) {
 -		slave = rcu_dereference(bond->curr_active_slave);
 -		if (slave)
 -			bond_dev_queue_xmit(bond, skb, slave->dev);
 -		else
 -			bond_xmit_slave_id(bond, skb, 0);
 +	if ((iph->protocol == IPPROTO_IGMP) &&
 +	    (skb->protocol == htons(ETH_P_IP))) {
 +		slave = bond->curr_active_slave;
 +		if (!slave)
 +			goto out;
  	} else {
 -		int slave_cnt = ACCESS_ONCE(bond->slave_cnt);
 +		/*
 +		 * Concurrent TX may collide on rr_tx_counter; we accept
 +		 * that as being rare enough not to justify using an
 +		 * atomic op here.
 +		 */
 +		slave_no = bond->rr_tx_counter++ % bond->slave_cnt;
  
++<<<<<<< HEAD
 +		bond_for_each_slave(bond, slave, i) {
 +			slave_no--;
 +			if (slave_no < 0)
 +				break;
++=======
+ 		if (likely(slave_cnt)) {
+ 			slave_id = bond_rr_gen_slave_id(bond);
+ 			bond_xmit_slave_id(bond, skb, slave_id % slave_cnt);
+ 		} else {
+ 			bond_tx_drop(bond_dev, skb);
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
 +		}
 +	}
 +
 +	start_at = slave;
 +	bond_for_each_slave_from(bond, slave, i, start_at) {
 +		if (IS_UP(slave->dev) &&
 +		    (slave->link == BOND_LINK_UP) &&
 +		    bond_is_active_slave(slave)) {
 +			res = bond_dev_queue_xmit(bond, skb, slave->dev);
 +			break;
  		}
  	}
  
@@@ -3901,54 -3598,156 +3985,81 @@@ static int bond_xmit_activebackup(struc
  {
  	struct bonding *bond = netdev_priv(bond_dev);
  	struct slave *slave;
 +	int res = 1;
  
 -	slave = rcu_dereference(bond->curr_active_slave);
 +	slave = bond->curr_active_slave;
  	if (slave)
++<<<<<<< HEAD
 +		res = bond_dev_queue_xmit(bond, skb, slave->dev);
 +
 +	if (res)
 +		/* no suitable interface, frame not sent */
 +		kfree_skb(skb);
++=======
+ 		bond_dev_queue_xmit(bond, skb, slave->dev);
+ 	else
+ 		bond_tx_drop(bond_dev, skb);
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
  
  	return NETDEV_TX_OK;
  }
  
 -/* Use this to update slave_array when (a) it's not appropriate to update
 - * slave_array right away (note that update_slave_array() may sleep)
 - * and / or (b) RTNL is not held.
 - */
 -void bond_slave_arr_work_rearm(struct bonding *bond, unsigned long delay)
 -{
 -	queue_delayed_work(bond->wq, &bond->slave_arr_work, delay);
 -}
 -
 -/* Slave array work handler. Holds only RTNL */
 -static void bond_slave_arr_handler(struct work_struct *work)
 -{
 -	struct bonding *bond = container_of(work, struct bonding,
 -					    slave_arr_work.work);
 -	int ret;
 -
 -	if (!rtnl_trylock())
 -		goto err;
 -
 -	ret = bond_update_slave_arr(bond, NULL);
 -	rtnl_unlock();
 -	if (ret) {
 -		pr_warn_ratelimited("Failed to update slave array from WT\n");
 -		goto err;
 -	}
 -	return;
 -
 -err:
 -	bond_slave_arr_work_rearm(bond, 1);
 -}
 -
 -/* Build the usable slaves array in control path for modes that use xmit-hash
 - * to determine the slave interface -
 - * (a) BOND_MODE_8023AD
 - * (b) BOND_MODE_XOR
 - * (c) BOND_MODE_TLB && tlb_dynamic_lb == 0
 - *
 - * The caller is expected to hold RTNL only and NO other lock!
 +/*
 + * In bond_xmit_xor() , we determine the output device by using a pre-
 + * determined xmit_hash_policy(), If the selected device is not enabled,
 + * find the next active slave.
   */
 -int bond_update_slave_arr(struct bonding *bond, struct slave *skipslave)
 +static int bond_xmit_xor(struct sk_buff *skb, struct net_device *bond_dev)
  {
 -	struct slave *slave;
 -	struct list_head *iter;
 -	struct bond_up_slave *new_arr, *old_arr;
 -	int slaves_in_agg;
 -	int agg_id = 0;
 -	int ret = 0;
 -
 -#ifdef CONFIG_LOCKDEP
 -	WARN_ON(lockdep_is_held(&bond->mode_lock));
 -#endif
 +	struct bonding *bond = netdev_priv(bond_dev);
 +	struct slave *slave, *start_at;
 +	int slave_no;
 +	int i;
 +	int res = 1;
  
 -	new_arr = kzalloc(offsetof(struct bond_up_slave, arr[bond->slave_cnt]),
 -			  GFP_KERNEL);
 -	if (!new_arr) {
 -		ret = -ENOMEM;
 -		pr_err("Failed to build slave-array.\n");
 -		goto out;
 -	}
 -	if (BOND_MODE(bond) == BOND_MODE_8023AD) {
 -		struct ad_info ad_info;
 +	slave_no = bond->xmit_hash_policy(skb, bond->slave_cnt);
  
 -		if (bond_3ad_get_active_agg_info(bond, &ad_info)) {
 -			pr_debug("bond_3ad_get_active_agg_info failed\n");
 -			kfree_rcu(new_arr, rcu);
 -			/* No active aggragator means it's not safe to use
 -			 * the previous array.
 -			 */
 -			old_arr = rtnl_dereference(bond->slave_arr);
 -			if (old_arr) {
 -				RCU_INIT_POINTER(bond->slave_arr, NULL);
 -				kfree_rcu(old_arr, rcu);
 -			}
 -			goto out;
 -		}
 -		slaves_in_agg = ad_info.ports;
 -		agg_id = ad_info.aggregator_id;
 +	bond_for_each_slave(bond, slave, i) {
 +		slave_no--;
 +		if (slave_no < 0)
 +			break;
  	}
 -	bond_for_each_slave(bond, slave, iter) {
 -		if (BOND_MODE(bond) == BOND_MODE_8023AD) {
 -			struct aggregator *agg;
  
 -			agg = SLAVE_AD_INFO(slave)->port.aggregator;
 -			if (!agg || agg->aggregator_identifier != agg_id)
 -				continue;
 -		}
 -		if (!bond_slave_can_tx(slave))
 -			continue;
 -		if (skipslave == slave)
 -			continue;
 -		new_arr->arr[new_arr->count++] = slave;
 -	}
 +	start_at = slave;
  
 -	old_arr = rtnl_dereference(bond->slave_arr);
 -	rcu_assign_pointer(bond->slave_arr, new_arr);
 -	if (old_arr)
 -		kfree_rcu(old_arr, rcu);
 -out:
 -	if (ret != 0 && skipslave) {
 -		int idx;
 -
 -		/* Rare situation where caller has asked to skip a specific
 -		 * slave but allocation failed (most likely!). BTW this is
 -		 * only possible when the call is initiated from
 -		 * __bond_release_one(). In this situation; overwrite the
 -		 * skipslave entry in the array with the last entry from the
 -		 * array to avoid a situation where the xmit path may choose
 -		 * this to-be-skipped slave to send a packet out.
 -		 */
 -		old_arr = rtnl_dereference(bond->slave_arr);
 -		for (idx = 0; idx < old_arr->count; idx++) {
 -			if (skipslave == old_arr->arr[idx]) {
 -				old_arr->arr[idx] =
 -				    old_arr->arr[old_arr->count-1];
 -				old_arr->count--;
 -				break;
 -			}
 +	bond_for_each_slave_from(bond, slave, i, start_at) {
 +		if (IS_UP(slave->dev) &&
 +		    (slave->link == BOND_LINK_UP) &&
 +		    bond_is_active_slave(slave)) {
 +			res = bond_dev_queue_xmit(bond, skb, slave->dev);
 +			break;
  		}
  	}
 -	return ret;
 -}
  
++<<<<<<< HEAD
 +	if (res) {
 +		/* no suitable interface, frame not sent */
 +		kfree_skb(skb);
++=======
+ /* Use this Xmit function for 3AD as well as XOR modes. The current
+  * usable slave array is formed in the control path. The xmit function
+  * just calculates hash and sends the packet out.
+  */
+ int bond_3ad_xor_xmit(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	struct bonding *bond = netdev_priv(dev);
+ 	struct slave *slave;
+ 	struct bond_up_slave *slaves;
+ 	unsigned int count;
+ 
+ 	slaves = rcu_dereference(bond->slave_arr);
+ 	count = slaves ? ACCESS_ONCE(slaves->count) : 0;
+ 	if (likely(count)) {
+ 		slave = slaves->arr[bond_xmit_hash(bond, skb) % count];
+ 		bond_dev_queue_xmit(bond, skb, slave->dev);
+ 	} else {
+ 		bond_tx_drop(dev, skb);
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
  	}
  
  	return NETDEV_TX_OK;
@@@ -3974,10 -3774,10 +4085,14 @@@ static int bond_xmit_broadcast(struct s
  			bond_dev_queue_xmit(bond, skb2, slave->dev);
  		}
  	}
 -	if (slave && bond_slave_is_up(slave) && slave->link == BOND_LINK_UP)
 +	if (slave && IS_UP(slave->dev) && slave->link == BOND_LINK_UP)
  		bond_dev_queue_xmit(bond, skb, slave->dev);
  	else
++<<<<<<< HEAD
 +		kfree_skb(skb);
++=======
+ 		bond_tx_drop(bond_dev, skb);
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
  
  	return NETDEV_TX_OK;
  }
@@@ -4068,21 -3844,20 +4183,25 @@@ static netdev_tx_t __bond_start_xmit(st
  		return bond_xmit_roundrobin(skb, dev);
  	case BOND_MODE_ACTIVEBACKUP:
  		return bond_xmit_activebackup(skb, dev);
 -	case BOND_MODE_8023AD:
  	case BOND_MODE_XOR:
 -		return bond_3ad_xor_xmit(skb, dev);
 +		return bond_xmit_xor(skb, dev);
  	case BOND_MODE_BROADCAST:
  		return bond_xmit_broadcast(skb, dev);
 +	case BOND_MODE_8023AD:
 +		return bond_3ad_xmit_xor(skb, dev);
  	case BOND_MODE_ALB:
 -		return bond_alb_xmit(skb, dev);
  	case BOND_MODE_TLB:
 -		return bond_tlb_xmit(skb, dev);
 +		return bond_alb_xmit(skb, dev);
  	default:
  		/* Should never happen, mode already checked */
 -		netdev_err(dev, "Unknown bonding mode %d\n", BOND_MODE(bond));
 +		pr_err("%s: Error: Unknown bonding mode %d\n",
 +		       dev->name, bond->params.mode);
  		WARN_ON_ONCE(1);
++<<<<<<< HEAD
 +		kfree_skb(skb);
++=======
+ 		bond_tx_drop(dev, skb);
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
  		return NETDEV_TX_OK;
  	}
  }
@@@ -4092,21 -3867,18 +4211,26 @@@ static netdev_tx_t bond_start_xmit(stru
  	struct bonding *bond = netdev_priv(dev);
  	netdev_tx_t ret = NETDEV_TX_OK;
  
 -	/* If we risk deadlock from transmitting this in the
 +	/*
 +	 * If we risk deadlock from transmitting this in the
  	 * netpoll path, tell netpoll to queue the frame for later tx
  	 */
 -	if (unlikely(is_netpoll_tx_blocked(dev)))
 +	if (is_netpoll_tx_blocked(dev))
  		return NETDEV_TX_BUSY;
  
 -	rcu_read_lock();
 -	if (bond_has_slaves(bond))
 +	read_lock(&bond->lock);
 +
 +	if (bond->slave_cnt)
  		ret = __bond_start_xmit(skb, dev);
  	else
++<<<<<<< HEAD
 +		kfree_skb(skb);
 +
 +	read_unlock(&bond->lock);
++=======
+ 		bond_tx_drop(dev, skb);
+ 	rcu_read_unlock();
++>>>>>>> 31aa860e0aaf (bonding: add bond_tx_drop() helper)
  
  	return ret;
  }
* Unmerged path drivers/net/bonding/bond_alb.c
* Unmerged path drivers/net/bonding/bond_main.c
diff --git a/drivers/net/bonding/bonding.h b/drivers/net/bonding/bonding.h
index 64c0cb81e478..838bf0127fd3 100644
--- a/drivers/net/bonding/bonding.h
+++ b/drivers/net/bonding/bonding.h
@@ -512,4 +512,10 @@ extern struct bond_parm_tbl ad_select_tbl[];
 /* exported from bond_netlink.c */
 extern struct rtnl_link_ops bond_link_ops;
 
+static inline void bond_tx_drop(struct net_device *dev, struct sk_buff *skb)
+{
+	atomic_long_inc(&dev->tx_dropped);
+	dev_kfree_skb_any(skb);
+}
+
 #endif /* _LINUX_BONDING_H */
