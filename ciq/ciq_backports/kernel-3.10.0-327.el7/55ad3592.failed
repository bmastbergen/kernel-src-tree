net/mlx4_core: Enable device recovery flow with SRIOV

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [netdrv] mlx4_core: Enable device recovery flow with SRIOV (Amir Vadai) [1164527 1164530 1164531 1164536 1164537]
Rebuild_FUZZ: 96.08%
commit-author Yishai Hadas <yishaih@mellanox.com>
commit 55ad359225b2232b9b8f04a0dfa169bd3a7d86d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/55ad3592.failed

In SRIOV, both the PF and the VF may attempt device recovery whenever they
assume that the device is not functioning.  When the PF driver resets the
device, the VF should detect this and attempt to reinitialize itself.

The VF must be able to reset itself under all circumstances, even
if the PF is not responsive.

The VF shall reset itself in the following cases:

1. Commands are not processed within reasonable time over the communication channel.
This is done considering device state and the correct return code based on
the command as was done in the native mode, done in the next patch.

2. The VF driver receives an internal error event reported by the PF on the
communication channel. This occurs when the PF driver resets the device or
when VF is out of sync with the PF.

Add 'VF reset' capability, which allows the VF to reinitialize itself even when the
PF is not responsive.

As PF and VF may run their reset flow simulantanisly, there are several cases
that are handled:
- Prevent freeing VF resources upon FLR, when PF is in its unloading stage.
- Prevent PF getting VF commands before it has finished initializing its resources.
- Upon VF startup, check that comm-channel is online before sending
  commands to the PF and getting timed-out.

	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 55ad359225b2232b9b8f04a0dfa169bd3a7d86d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx4/catas.c
#	drivers/net/ethernet/mellanox/mlx4/main.c
#	include/linux/mlx4/cmd.h
#	include/linux/mlx4/device.h
diff --cc drivers/net/ethernet/mellanox/mlx4/catas.c
index 9c656fe4983d,715de8affcc9..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/catas.c
+++ b/drivers/net/ethernet/mellanox/mlx4/catas.c
@@@ -40,17 -40,178 +40,184 @@@ enum 
  	MLX4_CATAS_POLL_INTERVAL	= 5 * HZ,
  };
  
 +static DEFINE_SPINLOCK(catas_lock);
  
 +static LIST_HEAD(catas_list);
 +static struct work_struct catas_work;
  
 -int mlx4_internal_err_reset = 1;
 -module_param_named(internal_err_reset, mlx4_internal_err_reset,  int, 0644);
 +static int internal_err_reset = 1;
 +module_param(internal_err_reset, int, 0644);
  MODULE_PARM_DESC(internal_err_reset,
- 		 "Reset device on internal errors if non-zero"
- 		 " (default 1, in SRIOV mode default is 0)");
+ 		 "Reset device on internal errors if non-zero (default 1)");
  
++<<<<<<< HEAD
++=======
+ static int read_vendor_id(struct mlx4_dev *dev)
+ {
+ 	u16 vendor_id = 0;
+ 	int ret;
+ 
+ 	ret = pci_read_config_word(dev->persist->pdev, 0, &vendor_id);
+ 	if (ret) {
+ 		mlx4_err(dev, "Failed to read vendor ID, ret=%d\n", ret);
+ 		return ret;
+ 	}
+ 
+ 	if (vendor_id == 0xffff) {
+ 		mlx4_err(dev, "PCI can't be accessed to read vendor id\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int mlx4_reset_master(struct mlx4_dev *dev)
+ {
+ 	int err = 0;
+ 
+ 	if (mlx4_is_master(dev))
+ 		mlx4_report_internal_err_comm_event(dev);
+ 
+ 	if (!pci_channel_offline(dev->persist->pdev)) {
+ 		err = read_vendor_id(dev);
+ 		/* If PCI can't be accessed to read vendor ID we assume that its
+ 		 * link was disabled and chip was already reset.
+ 		 */
+ 		if (err)
+ 			return 0;
+ 
+ 		err = mlx4_reset(dev);
+ 		if (err)
+ 			mlx4_err(dev, "Fail to reset HCA\n");
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int mlx4_reset_slave(struct mlx4_dev *dev)
+ {
+ #define COM_CHAN_RST_REQ_OFFSET 0x10
+ #define COM_CHAN_RST_ACK_OFFSET 0x08
+ 
+ 	u32 comm_flags;
+ 	u32 rst_req;
+ 	u32 rst_ack;
+ 	unsigned long end;
+ 	struct mlx4_priv *priv = mlx4_priv(dev);
+ 
+ 	if (pci_channel_offline(dev->persist->pdev))
+ 		return 0;
+ 
+ 	comm_flags = swab32(readl((__iomem char *)priv->mfunc.comm +
+ 				  MLX4_COMM_CHAN_FLAGS));
+ 	if (comm_flags == 0xffffffff) {
+ 		mlx4_err(dev, "VF reset is not needed\n");
+ 		return 0;
+ 	}
+ 
+ 	if (!(dev->caps.vf_caps & MLX4_VF_CAP_FLAG_RESET)) {
+ 		mlx4_err(dev, "VF reset is not supported\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	rst_req = (comm_flags & (u32)(1 << COM_CHAN_RST_REQ_OFFSET)) >>
+ 		COM_CHAN_RST_REQ_OFFSET;
+ 	rst_ack = (comm_flags & (u32)(1 << COM_CHAN_RST_ACK_OFFSET)) >>
+ 		COM_CHAN_RST_ACK_OFFSET;
+ 	if (rst_req != rst_ack) {
+ 		mlx4_err(dev, "Communication channel isn't sync, fail to send reset\n");
+ 		return -EIO;
+ 	}
+ 
+ 	rst_req ^= 1;
+ 	mlx4_warn(dev, "VF is sending reset request to Firmware\n");
+ 	comm_flags = rst_req << COM_CHAN_RST_REQ_OFFSET;
+ 	__raw_writel((__force u32)cpu_to_be32(comm_flags),
+ 		     (__iomem char *)priv->mfunc.comm + MLX4_COMM_CHAN_FLAGS);
+ 	/* Make sure that our comm channel write doesn't
+ 	 * get mixed in with writes from another CPU.
+ 	 */
+ 	mmiowb();
+ 
+ 	end = msecs_to_jiffies(MLX4_COMM_TIME) + jiffies;
+ 	while (time_before(jiffies, end)) {
+ 		comm_flags = swab32(readl((__iomem char *)priv->mfunc.comm +
+ 					  MLX4_COMM_CHAN_FLAGS));
+ 		rst_ack = (comm_flags & (u32)(1 << COM_CHAN_RST_ACK_OFFSET)) >>
+ 			COM_CHAN_RST_ACK_OFFSET;
+ 
+ 		/* Reading rst_req again since the communication channel can
+ 		 * be reset at any time by the PF and all its bits will be
+ 		 * set to zero.
+ 		 */
+ 		rst_req = (comm_flags & (u32)(1 << COM_CHAN_RST_REQ_OFFSET)) >>
+ 			COM_CHAN_RST_REQ_OFFSET;
+ 
+ 		if (rst_ack == rst_req) {
+ 			mlx4_warn(dev, "VF Reset succeed\n");
+ 			return 0;
+ 		}
+ 		cond_resched();
+ 	}
+ 	mlx4_err(dev, "Fail to send reset over the communication channel\n");
+ 	return -ETIMEDOUT;
+ }
+ 
+ static int mlx4_comm_internal_err(u32 slave_read)
+ {
+ 	return (u32)COMM_CHAN_EVENT_INTERNAL_ERR ==
+ 		(slave_read & (u32)COMM_CHAN_EVENT_INTERNAL_ERR) ? 1 : 0;
+ }
+ 
+ void mlx4_enter_error_state(struct mlx4_dev_persistent *persist)
+ {
+ 	int err;
+ 	struct mlx4_dev *dev;
+ 
+ 	if (!mlx4_internal_err_reset)
+ 		return;
+ 
+ 	mutex_lock(&persist->device_state_mutex);
+ 	if (persist->state & MLX4_DEVICE_STATE_INTERNAL_ERROR)
+ 		goto out;
+ 
+ 	dev = persist->dev;
+ 	mlx4_err(dev, "device is going to be reset\n");
+ 	if (mlx4_is_slave(dev))
+ 		err = mlx4_reset_slave(dev);
+ 	else
+ 		err = mlx4_reset_master(dev);
+ 	BUG_ON(err != 0);
+ 
+ 	dev->persist->state |= MLX4_DEVICE_STATE_INTERNAL_ERROR;
+ 	mlx4_err(dev, "device was reset successfully\n");
+ 	mutex_unlock(&persist->device_state_mutex);
+ 
+ 	/* At that step HW was already reset, now notify clients */
+ 	mlx4_dispatch_event(dev, MLX4_DEV_EVENT_CATASTROPHIC_ERROR, 0);
+ 	mlx4_cmd_wake_completions(dev);
+ 	return;
+ 
+ out:
+ 	mutex_unlock(&persist->device_state_mutex);
+ }
+ 
+ static void mlx4_handle_error_state(struct mlx4_dev_persistent *persist)
+ {
+ 	int err = 0;
+ 
+ 	mlx4_enter_error_state(persist);
+ 	mutex_lock(&persist->interface_state_mutex);
+ 	if (persist->interface_state & MLX4_INTERFACE_STATE_UP &&
+ 	    !(persist->interface_state & MLX4_INTERFACE_STATE_DELETION)) {
+ 		err = mlx4_restart_one(persist->pdev);
+ 		mlx4_info(persist->dev, "mlx4_restart_one was ended, ret=%d\n",
+ 			  err);
+ 	}
+ 	mutex_unlock(&persist->interface_state_mutex);
+ }
+ 
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  static void dump_err_buf(struct mlx4_dev *dev)
  {
  	struct mlx4_priv *priv = mlx4_priv(dev);
@@@ -67,27 -228,31 +234,41 @@@ static void poll_catas(unsigned long de
  {
  	struct mlx4_dev *dev = (struct mlx4_dev *) dev_ptr;
  	struct mlx4_priv *priv = mlx4_priv(dev);
+ 	u32 slave_read;
  
++<<<<<<< HEAD
 +	if (readl(priv->catas_err.map)) {
 +		/* If the device is off-line, we cannot try to recover it */
 +		if (pci_channel_offline(dev->pdev))
 +			mod_timer(&priv->catas_err.timer,
 +				  round_jiffies(jiffies + MLX4_CATAS_POLL_INTERVAL));
 +		else {
 +			dump_err_buf(dev);
 +			mlx4_dispatch_event(dev, MLX4_DEV_EVENT_CATASTROPHIC_ERROR, 0);
++=======
+ 	if (mlx4_is_slave(dev)) {
+ 		slave_read = swab32(readl(&priv->mfunc.comm->slave_read));
+ 		if (mlx4_comm_internal_err(slave_read)) {
+ 			mlx4_warn(dev, "Internal error detected on the communication channel\n");
+ 			goto internal_err;
+ 		}
+ 	} else if (readl(priv->catas_err.map)) {
+ 		dump_err_buf(dev);
+ 		goto internal_err;
+ 	}
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  
 -	if (dev->persist->state & MLX4_DEVICE_STATE_INTERNAL_ERROR) {
 -		mlx4_warn(dev, "Internal error mark was detected on device\n");
 -		goto internal_err;
 -	}
 +			if (internal_err_reset) {
 +				spin_lock(&catas_lock);
 +				list_add(&priv->catas_err.list, &catas_list);
 +				spin_unlock(&catas_lock);
  
 -	mod_timer(&priv->catas_err.timer,
 -		  round_jiffies(jiffies + MLX4_CATAS_POLL_INTERVAL));
 -	return;
 -
 -internal_err:
 -	if (mlx4_internal_err_reset)
 -		queue_work(dev->persist->catas_wq, &dev->persist->catas_work);
 +				queue_work(mlx4_wq, &catas_work);
 +			}
 +		}
 +	} else
 +		mod_timer(&priv->catas_err.timer,
 +			  round_jiffies(jiffies + MLX4_CATAS_POLL_INTERVAL));
  }
  
  static void catas_reset(struct work_struct *work)
@@@ -126,22 -269,21 +307,33 @@@ void mlx4_start_catas_poll(struct mlx4_
  	struct mlx4_priv *priv = mlx4_priv(dev);
  	phys_addr_t addr;
  
++<<<<<<< HEAD
 +	/*If we are in SRIOV the default of the module param must be 0*/
 +	if (mlx4_is_mfunc(dev))
 +		internal_err_reset = 0;
 +
++=======
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  	INIT_LIST_HEAD(&priv->catas_err.list);
  	init_timer(&priv->catas_err.timer);
  	priv->catas_err.map = NULL;
  
++<<<<<<< HEAD
 +	addr = pci_resource_start(dev->pdev, priv->fw.catas_bar) +
 +		priv->fw.catas_offset;
++=======
+ 	if (!mlx4_is_slave(dev)) {
+ 		addr = pci_resource_start(dev->persist->pdev,
+ 					  priv->fw.catas_bar) +
+ 					  priv->fw.catas_offset;
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  
- 	priv->catas_err.map = ioremap(addr, priv->fw.catas_size * 4);
- 	if (!priv->catas_err.map) {
- 		mlx4_warn(dev, "Failed to map internal error buffer at 0x%llx\n",
- 			  (unsigned long long) addr);
- 		return;
+ 		priv->catas_err.map = ioremap(addr, priv->fw.catas_size * 4);
+ 		if (!priv->catas_err.map) {
+ 			mlx4_warn(dev, "Failed to map internal error buffer at 0x%llx\n",
+ 				  (unsigned long long)addr);
+ 			return;
+ 		}
  	}
  
  	priv->catas_err.timer.data     = (unsigned long) dev;
diff --cc drivers/net/ethernet/mellanox/mlx4/main.c
index e21046536e8b,1baf1f1e2866..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/main.c
+++ b/drivers/net/ethernet/mellanox/mlx4/main.c
@@@ -105,8 -105,11 +105,10 @@@ MODULE_PARM_DESC(enable_64b_cqe_eqe
  		 "Enable 64 byte CQEs/EQEs when the FW supports this (default: True)");
  
  #define PF_CONTEXT_BEHAVIOUR_MASK	(MLX4_FUNC_CAP_64B_EQE_CQE | \
 -					 MLX4_FUNC_CAP_EQE_CQE_STRIDE | \
 -					 MLX4_FUNC_CAP_DMFS_A0_STATIC)
 +					 MLX4_FUNC_CAP_EQE_CQE_STRIDE)
  
+ #define RESET_PERSIST_MASK_FLAGS	(MLX4_FLAG_SRIOV)
+ 
  static char mlx4_version[] =
  	DRV_NAME ": Mellanox ConnectX core driver v"
  	DRV_VERSION " (" DRV_RELDATE ")\n";
@@@ -2336,13 -2547,29 +2389,34 @@@ static void mlx4_free_ownership(struct 
  				  !!((flags) & MLX4_FLAG_MASTER))
  
  static u64 mlx4_enable_sriov(struct mlx4_dev *dev, struct pci_dev *pdev,
- 			     u8 total_vfs, int existing_vfs)
+ 			     u8 total_vfs, int existing_vfs, int reset_flow)
  {
  	u64 dev_flags = dev->flags;
 -	int err = 0;
  
++<<<<<<< HEAD
 +	dev->dev_vfs = kzalloc(
 +			total_vfs * sizeof(*dev->dev_vfs),
 +			GFP_KERNEL);
++=======
+ 	if (reset_flow) {
+ 		dev->dev_vfs = kcalloc(total_vfs, sizeof(*dev->dev_vfs),
+ 				       GFP_KERNEL);
+ 		if (!dev->dev_vfs)
+ 			goto free_mem;
+ 		return dev_flags;
+ 	}
+ 
+ 	atomic_inc(&pf_loading);
+ 	if (dev->flags &  MLX4_FLAG_SRIOV) {
+ 		if (existing_vfs != total_vfs) {
+ 			mlx4_err(dev, "SR-IOV was already enabled, but with num_vfs (%d) different than requested (%d)\n",
+ 				 existing_vfs, total_vfs);
+ 			total_vfs = existing_vfs;
+ 		}
+ 	}
+ 
+ 	dev->dev_vfs = kzalloc(total_vfs * sizeof(*dev->dev_vfs), GFP_KERNEL);
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  	if (NULL == dev->dev_vfs) {
  		mlx4_err(dev, "Failed to allocate memory for VFs\n");
  		goto disable_sriov;
@@@ -2374,13 -2593,34 +2448,20 @@@
  	return dev_flags;
  
  disable_sriov:
++<<<<<<< HEAD
 +	dev->num_vfs = 0;
++=======
+ 	atomic_dec(&pf_loading);
+ free_mem:
+ 	dev->persist->num_vfs = 0;
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  	kfree(dev->dev_vfs);
  	return dev_flags & ~MLX4_FLAG_MASTER;
  }
  
 -enum {
 -	MLX4_DEV_CAP_CHECK_NUM_VFS_ABOVE_64 = -1,
 -};
 -
 -static int mlx4_check_dev_cap(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap,
 -			      int *nvfs)
 -{
 -	int requested_vfs = nvfs[0] + nvfs[1] + nvfs[2];
 -	/* Checking for 64 VFs as a limitation of CX2 */
 -	if (!(dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_80_VFS) &&
 -	    requested_vfs >= 64) {
 -		mlx4_err(dev, "Requested %d VFs, but FW does not support more than 64\n",
 -			 requested_vfs);
 -		return MLX4_DEV_CAP_CHECK_NUM_VFS_ABOVE_64;
 -	}
 -	return 0;
 -}
 -
  static int mlx4_load_one(struct pci_dev *pdev, int pci_dev_data,
- 			 int total_vfs, int *nvfs, struct mlx4_priv *priv)
+ 			 int total_vfs, int *nvfs, struct mlx4_priv *priv,
+ 			 int reset_flow)
  {
  	struct mlx4_dev *dev;
  	unsigned sum = 0;
@@@ -2476,6 -2722,61 +2557,64 @@@ slave_start
  		goto err_mfunc;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (mlx4_is_master(dev)) {
+ 		/* when we hit the goto slave_start below, dev_cap already initialized */
+ 		if (!dev_cap) {
+ 			dev_cap = kzalloc(sizeof(*dev_cap), GFP_KERNEL);
+ 
+ 			if (!dev_cap) {
+ 				err = -ENOMEM;
+ 				goto err_fw;
+ 			}
+ 
+ 			err = mlx4_QUERY_DEV_CAP(dev, dev_cap);
+ 			if (err) {
+ 				mlx4_err(dev, "QUERY_DEV_CAP command failed, aborting.\n");
+ 				goto err_fw;
+ 			}
+ 
+ 			if (mlx4_check_dev_cap(dev, dev_cap, nvfs))
+ 				goto err_fw;
+ 
+ 			if (!(dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_SYS_EQS)) {
+ 				u64 dev_flags = mlx4_enable_sriov(dev, pdev,
+ 								  total_vfs,
+ 								  existing_vfs,
+ 								  reset_flow);
+ 
+ 				mlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_ALL);
+ 				dev->flags = dev_flags;
+ 				if (!SRIOV_VALID_STATE(dev->flags)) {
+ 					mlx4_err(dev, "Invalid SRIOV state\n");
+ 					goto err_sriov;
+ 				}
+ 				err = mlx4_reset(dev);
+ 				if (err) {
+ 					mlx4_err(dev, "Failed to reset HCA, aborting.\n");
+ 					goto err_sriov;
+ 				}
+ 				goto slave_start;
+ 			}
+ 		} else {
+ 			/* Legacy mode FW requires SRIOV to be enabled before
+ 			 * doing QUERY_DEV_CAP, since max_eq's value is different if
+ 			 * SRIOV is enabled.
+ 			 */
+ 			memset(dev_cap, 0, sizeof(*dev_cap));
+ 			err = mlx4_QUERY_DEV_CAP(dev, dev_cap);
+ 			if (err) {
+ 				mlx4_err(dev, "QUERY_DEV_CAP command failed, aborting.\n");
+ 				goto err_fw;
+ 			}
+ 
+ 			if (mlx4_check_dev_cap(dev, dev_cap, nvfs))
+ 				goto err_fw;
+ 		}
+ 	}
+ 
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  	err = mlx4_init_hca(dev);
  	if (err) {
  		if (err == -EACCES) {
@@@ -2499,8 -2800,33 +2638,38 @@@
  			goto err_fw;
  	}
  
++<<<<<<< HEAD
 +	/* check if the device is functioning at its maximum possible speed
 +	 * ignoring function return code, just warn the user in case of PCI
++=======
+ 	if (mlx4_is_master(dev) && (dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_SYS_EQS)) {
+ 		u64 dev_flags = mlx4_enable_sriov(dev, pdev, total_vfs,
+ 						  existing_vfs, reset_flow);
+ 
+ 		if ((dev->flags ^ dev_flags) & (MLX4_FLAG_MASTER | MLX4_FLAG_SLAVE)) {
+ 			mlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_VHCR);
+ 			dev->flags = dev_flags;
+ 			err = mlx4_cmd_init(dev);
+ 			if (err) {
+ 				/* Only VHCR is cleaned up, so could still
+ 				 * send FW commands
+ 				 */
+ 				mlx4_err(dev, "Failed to init VHCR command interface, aborting\n");
+ 				goto err_close;
+ 			}
+ 		} else {
+ 			dev->flags = dev_flags;
+ 		}
+ 
+ 		if (!SRIOV_VALID_STATE(dev->flags)) {
+ 			mlx4_err(dev, "Invalid SRIOV state\n");
+ 			goto err_close;
+ 		}
+ 	}
+ 
+ 	/* check if the device is functioning at its maximum possible speed.
+ 	 * No return code for this call, just warn the user in case of PCI
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  	 * express device capabilities are under-satisfied by the bus.
  	 */
  	if (!mlx4_is_slave(dev))
@@@ -2604,9 -2943,10 +2784,13 @@@
  
  	priv->removed = 0;
  
++<<<<<<< HEAD
 +	if (mlx4_is_master(dev) && dev->num_vfs)
++=======
+ 	if (mlx4_is_master(dev) && dev->persist->num_vfs && !reset_flow)
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  		atomic_dec(&pf_loading);
  
 -	kfree(dev_cap);
  	return 0;
  
  err_port:
@@@ -2662,10 -3002,12 +2846,16 @@@ err_cmd
  	mlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_ALL);
  
  err_sriov:
- 	if (dev->flags & MLX4_FLAG_SRIOV && !existing_vfs)
+ 	if (dev->flags & MLX4_FLAG_SRIOV && !existing_vfs) {
  		pci_disable_sriov(pdev);
+ 		dev->flags &= ~MLX4_FLAG_SRIOV;
+ 	}
  
++<<<<<<< HEAD
 +	if (mlx4_is_master(dev) && dev->num_vfs)
++=======
+ 	if (mlx4_is_master(dev) && dev->persist->num_vfs && !reset_flow)
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  		atomic_dec(&pf_loading);
  
  	kfree(priv->dev.dev_vfs);
@@@ -2805,11 -3148,19 +2995,19 @@@ static int __mlx4_init_one(struct pci_d
  		}
  	}
  
++<<<<<<< HEAD
 +	err = mlx4_load_one(pdev, pci_dev_data, total_vfs, nvfs, priv);
++=======
+ 	err = mlx4_catas_init(&priv->dev);
+ 	if (err)
+ 		goto err_release_regions;
+ 
+ 	err = mlx4_load_one(pdev, pci_dev_data, total_vfs, nvfs, priv, 0);
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  	if (err)
 -		goto err_catas;
 -
 +		goto err_release_regions;
  	return 0;
  
 -err_catas:
 -	mlx4_catas_end(&priv->dev);
 -
  err_release_regions:
  	pci_release_regions(pdev);
  
@@@ -2843,27 -3206,37 +3041,37 @@@ static int mlx4_init_one(struct pci_de
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static void mlx4_clean_dev(struct mlx4_dev *dev)
+ {
+ 	struct mlx4_dev_persistent *persist = dev->persist;
+ 	struct mlx4_priv *priv = mlx4_priv(dev);
+ 	unsigned long	flags = (dev->flags & RESET_PERSIST_MASK_FLAGS);
+ 
+ 	memset(priv, 0, sizeof(*priv));
+ 	priv->dev.persist = persist;
+ 	priv->dev.flags = flags;
+ }
+ 
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  static void mlx4_unload_one(struct pci_dev *pdev)
  {
 -	struct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);
 -	struct mlx4_dev  *dev  = persist->dev;
 +	struct mlx4_dev  *dev  = pci_get_drvdata(pdev);
  	struct mlx4_priv *priv = mlx4_priv(dev);
  	int               pci_dev_data;
++<<<<<<< HEAD
 +	int p;
 +	int active_vfs = 0;
++=======
+ 	int p, i;
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  
  	if (priv->removed)
  		return;
  
 -	/* saving current ports type for further use */
 -	for (i = 0; i < dev->caps.num_ports; i++) {
 -		dev->persist->curr_port_type[i] = dev->caps.port_type[i + 1];
 -		dev->persist->curr_port_poss_type[i] = dev->caps.
 -						       possible_type[i + 1];
 -	}
 -
  	pci_dev_data = priv->pci_dev_data;
  
- 	/* Disabling SR-IOV is not allowed while there are active vf's */
- 	if (mlx4_is_master(dev)) {
- 		active_vfs = mlx4_how_many_lives_vf(dev);
- 		if (active_vfs) {
- 			pr_warn("Removing PF when there are active VF's !!\n");
- 			pr_warn("Will not disable SR-IOV.\n");
- 		}
- 	}
  	mlx4_stop_sense(dev);
  	mlx4_unregister_device(dev);
  
@@@ -2907,12 -3280,6 +3115,15 @@@
  
  	if (dev->flags & MLX4_FLAG_MSI_X)
  		pci_disable_msix(pdev);
++<<<<<<< HEAD
 +	if (dev->flags & MLX4_FLAG_SRIOV && !active_vfs) {
 +		mlx4_warn(dev, "Disabling SR-IOV\n");
 +		pci_disable_sriov(pdev);
 +		dev->flags &= ~MLX4_FLAG_SRIOV;
 +		dev->num_vfs = 0;
 +	}
++=======
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  
  	if (!mlx4_is_slave(dev))
  		mlx4_free_ownership(dev);
@@@ -2931,12 -3298,40 +3142,42 @@@
  
  static void mlx4_remove_one(struct pci_dev *pdev)
  {
 -	struct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);
 -	struct mlx4_dev  *dev  = persist->dev;
 +	struct mlx4_dev  *dev  = pci_get_drvdata(pdev);
  	struct mlx4_priv *priv = mlx4_priv(dev);
+ 	int active_vfs = 0;
  
++<<<<<<< HEAD
 +	mlx4_unload_one(pdev);
++=======
+ 	mutex_lock(&persist->interface_state_mutex);
+ 	persist->interface_state |= MLX4_INTERFACE_STATE_DELETION;
+ 	mutex_unlock(&persist->interface_state_mutex);
+ 
+ 	/* Disabling SR-IOV is not allowed while there are active vf's */
+ 	if (mlx4_is_master(dev) && dev->flags & MLX4_FLAG_SRIOV) {
+ 		active_vfs = mlx4_how_many_lives_vf(dev);
+ 		if (active_vfs) {
+ 			pr_warn("Removing PF when there are active VF's !!\n");
+ 			pr_warn("Will not disable SR-IOV.\n");
+ 		}
+ 	}
+ 
+ 	/* device marked to be under deletion running now without the lock
+ 	 * letting other tasks to be terminated
+ 	 */
+ 	if (persist->interface_state & MLX4_INTERFACE_STATE_UP)
+ 		mlx4_unload_one(pdev);
+ 	else
+ 		mlx4_info(dev, "%s: interface is down\n", __func__);
+ 	mlx4_catas_end(dev);
+ 	if (dev->flags & MLX4_FLAG_SRIOV && !active_vfs) {
+ 		mlx4_warn(dev, "Disabling SR-IOV\n");
+ 		pci_disable_sriov(pdev);
+ 	}
+ 
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  	pci_release_regions(pdev);
  	pci_disable_device(pdev);
 -	kfree(dev->persist);
  	kfree(priv);
  	pci_set_drvdata(pdev, NULL);
  }
@@@ -2949,11 -3364,11 +3190,11 @@@ int mlx4_restart_one(struct pci_dev *pd
  	int pci_dev_data, err, total_vfs;
  
  	pci_dev_data = priv->pci_dev_data;
 -	total_vfs = dev->persist->num_vfs;
 -	memcpy(nvfs, dev->persist->nvfs, sizeof(dev->persist->nvfs));
 +	total_vfs = dev->num_vfs;
 +	memcpy(nvfs, dev->nvfs, sizeof(dev->nvfs));
  
  	mlx4_unload_one(pdev);
- 	err = mlx4_load_one(pdev, pci_dev_data, total_vfs, nvfs, priv);
+ 	err = mlx4_load_one(pdev, pci_dev_data, total_vfs, nvfs, priv, 1);
  	if (err) {
  		mlx4_err(dev, "%s: ERROR: mlx4_load_one failed, pci_name=%s, err=%d\n",
  			 __func__, pci_name(pdev), err);
@@@ -3022,11 -3454,45 +3263,46 @@@ static pci_ers_result_t mlx4_pci_err_de
  
  static pci_ers_result_t mlx4_pci_slot_reset(struct pci_dev *pdev)
  {
 -	struct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);
 -	struct mlx4_dev	 *dev  = persist->dev;
 +	struct mlx4_dev	 *dev  = pci_get_drvdata(pdev);
  	struct mlx4_priv *priv = mlx4_priv(dev);
  	int               ret;
 -	int nvfs[MLX4_MAX_PORTS + 1] = {0, 0, 0};
 -	int total_vfs;
  
++<<<<<<< HEAD
 +	ret = __mlx4_init_one(pdev, priv->pci_dev_data, priv);
++=======
+ 	mlx4_err(dev, "mlx4_pci_slot_reset was called\n");
+ 	ret = pci_enable_device(pdev);
+ 	if (ret) {
+ 		mlx4_err(dev, "Can not re-enable device, ret=%d\n", ret);
+ 		return PCI_ERS_RESULT_DISCONNECT;
+ 	}
+ 
+ 	pci_set_master(pdev);
+ 	pci_restore_state(pdev);
+ 	pci_save_state(pdev);
+ 
+ 	total_vfs = dev->persist->num_vfs;
+ 	memcpy(nvfs, dev->persist->nvfs, sizeof(dev->persist->nvfs));
+ 
+ 	mutex_lock(&persist->interface_state_mutex);
+ 	if (!(persist->interface_state & MLX4_INTERFACE_STATE_UP)) {
+ 		ret = mlx4_load_one(pdev, priv->pci_dev_data, total_vfs, nvfs,
+ 				    priv, 1);
+ 		if (ret) {
+ 			mlx4_err(dev, "%s: mlx4_load_one failed, ret=%d\n",
+ 				 __func__,  ret);
+ 			goto end;
+ 		}
+ 
+ 		ret = restore_current_port_types(dev, dev->persist->
+ 						 curr_port_type, dev->persist->
+ 						 curr_port_poss_type);
+ 		if (ret)
+ 			mlx4_err(dev, "could not restore original port types (%d)\n", ret);
+ 	}
+ end:
+ 	mutex_unlock(&persist->interface_state_mutex);
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  
  	return ret ? PCI_ERS_RESULT_DISCONNECT : PCI_ERS_RESULT_RECOVERED;
  }
diff --cc include/linux/mlx4/cmd.h
index 64d25941b329,c989442ffc6a..000000000000
--- a/include/linux/mlx4/cmd.h
+++ b/include/linux/mlx4/cmd.h
@@@ -279,6 -279,8 +279,11 @@@ int mlx4_get_vf_config(struct mlx4_dev 
  int mlx4_set_vf_link_state(struct mlx4_dev *dev, int port, int vf, int link_state);
  int mlx4_config_dev_retrieval(struct mlx4_dev *dev,
  			      struct mlx4_config_dev_params *params);
++<<<<<<< HEAD
++=======
+ void mlx4_cmd_wake_completions(struct mlx4_dev *dev);
+ void mlx4_report_internal_err_comm_event(struct mlx4_dev *dev);
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  /*
   * mlx4_get_slave_default_vlan -
   * return true if VST ( default vlan)
diff --cc include/linux/mlx4/device.h
index 04d2bbb20a34,5ef54e145e4d..000000000000
--- a/include/linux/mlx4/device.h
+++ b/include/linux/mlx4/device.h
@@@ -519,6 -547,9 +523,12 @@@ struct mlx4_caps 
  	int			tunnel_offload_mode;
  	u8			rx_checksum_flags_port[MLX4_MAX_PORTS + 1];
  	u8			alloc_res_qp_mask;
++<<<<<<< HEAD
++=======
+ 	u32			dmfs_high_rate_qpn_base;
+ 	u32			dmfs_high_rate_qpn_range;
+ 	u32			vf_caps;
++>>>>>>> 55ad359225b2 (net/mlx4_core: Enable device recovery flow with SRIOV)
  };
  
  struct mlx4_buf_list {
* Unmerged path drivers/net/ethernet/mellanox/mlx4/catas.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/cmd.c b/drivers/net/ethernet/mellanox/mlx4/cmd.c
index b07100e60e38..b59a58862bfc 100644
--- a/drivers/net/ethernet/mellanox/mlx4/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx4/cmd.c
@@ -42,6 +42,7 @@
 #include <linux/mlx4/device.h>
 #include <linux/semaphore.h>
 #include <rdma/ib_smi.h>
+#include <linux/delay.h>
 
 #include <asm/io.h>
 
@@ -631,7 +632,7 @@ int __mlx4_cmd(struct mlx4_dev *dev, u64 in_param, u64 *out_param,
 EXPORT_SYMBOL_GPL(__mlx4_cmd);
 
 
-static int mlx4_ARM_COMM_CHANNEL(struct mlx4_dev *dev)
+int mlx4_ARM_COMM_CHANNEL(struct mlx4_dev *dev)
 {
 	return mlx4_cmd(dev, 0, 0, 0, MLX4_CMD_ARM_COMM_CHANNEL,
 			MLX4_CMD_TIME_CLASS_B, MLX4_CMD_NATIVE);
@@ -1847,8 +1848,11 @@ static void mlx4_master_do_cmd(struct mlx4_dev *dev, int slave, u8 cmd,
 		break;
 	case MLX4_COMM_CMD_VHCR_POST:
 		if ((slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR_EN) &&
-		    (slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR_POST))
+		    (slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR_POST)) {
+			mlx4_warn(dev, "slave:%d is out of sync, cmd=0x%x, last command=0x%x, reset is needed\n",
+				  slave, cmd, slave_state[slave].last_cmd);
 			goto reset_slave;
+		}
 
 		mutex_lock(&priv->cmd.slave_cmd_mutex);
 		if (mlx4_master_process_vhcr(dev, slave, NULL)) {
@@ -1882,7 +1886,18 @@ static void mlx4_master_do_cmd(struct mlx4_dev *dev, int slave, u8 cmd,
 
 reset_slave:
 	/* cleanup any slave resources */
-	mlx4_delete_all_resources_for_slave(dev, slave);
+	if (dev->persist->interface_state & MLX4_INTERFACE_STATE_UP)
+		mlx4_delete_all_resources_for_slave(dev, slave);
+
+	if (cmd != MLX4_COMM_CMD_RESET) {
+		mlx4_warn(dev, "Turn on internal error to force reset, slave=%d, cmd=0x%x\n",
+			  slave, cmd);
+		/* Turn on internal error letting slave reset itself immeditaly,
+		 * otherwise it might take till timeout on command is passed
+		 */
+		reply |= ((u32)COMM_CHAN_EVENT_INTERNAL_ERR);
+	}
+
 	spin_lock_irqsave(&priv->mfunc.master.slave_state_lock, flags);
 	if (!slave_state[slave].is_slave_going_down)
 		slave_state[slave].last_cmd = MLX4_COMM_CMD_RESET;
@@ -1958,17 +1973,28 @@ void mlx4_master_comm_channel(struct work_struct *work)
 static int sync_toggles(struct mlx4_dev *dev)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
-	int wr_toggle;
-	int rd_toggle;
+	u32 wr_toggle;
+	u32 rd_toggle;
 	unsigned long end;
 
-	wr_toggle = swab32(readl(&priv->mfunc.comm->slave_write)) >> 31;
-	end = jiffies + msecs_to_jiffies(5000);
+	wr_toggle = swab32(readl(&priv->mfunc.comm->slave_write));
+	if (wr_toggle == 0xffffffff)
+		end = jiffies + msecs_to_jiffies(30000);
+	else
+		end = jiffies + msecs_to_jiffies(5000);
 
 	while (time_before(jiffies, end)) {
-		rd_toggle = swab32(readl(&priv->mfunc.comm->slave_read)) >> 31;
-		if (rd_toggle == wr_toggle) {
-			priv->cmd.comm_toggle = rd_toggle;
+		rd_toggle = swab32(readl(&priv->mfunc.comm->slave_read));
+		if (wr_toggle == 0xffffffff || rd_toggle == 0xffffffff) {
+			/* PCI might be offline */
+			msleep(100);
+			wr_toggle = swab32(readl(&priv->mfunc.comm->
+					   slave_write));
+			continue;
+		}
+
+		if (rd_toggle >> 31 == wr_toggle >> 31) {
+			priv->cmd.comm_toggle = rd_toggle >> 31;
 			return 0;
 		}
 
@@ -2073,13 +2099,6 @@ int mlx4_multi_func_init(struct mlx4_dev *dev)
 		if (mlx4_init_resource_tracker(dev))
 			goto err_thread;
 
-		err = mlx4_ARM_COMM_CHANNEL(dev);
-		if (err) {
-			mlx4_err(dev, " Failed to arm comm channel eq: %x\n",
-				 err);
-			goto err_resource;
-		}
-
 	} else {
 		err = sync_toggles(dev);
 		if (err) {
@@ -2089,8 +2108,6 @@ int mlx4_multi_func_init(struct mlx4_dev *dev)
 	}
 	return 0;
 
-err_resource:
-	mlx4_free_resource_tracker(dev, RES_TR_FREE_ALL);
 err_thread:
 	flush_workqueue(priv->mfunc.master.comm_wq);
 	destroy_workqueue(priv->mfunc.master.comm_wq);
@@ -2166,6 +2183,27 @@ err:
 	return -ENOMEM;
 }
 
+void mlx4_report_internal_err_comm_event(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int slave;
+	u32 slave_read;
+
+	/* Report an internal error event to all
+	 * communication channels.
+	 */
+	for (slave = 0; slave < dev->num_slaves; slave++) {
+		slave_read = swab32(readl(&priv->mfunc.comm[slave].slave_read));
+		slave_read |= (u32)COMM_CHAN_EVENT_INTERNAL_ERR;
+		__raw_writel((__force u32)cpu_to_be32(slave_read),
+			     &priv->mfunc.comm[slave].slave_read);
+		/* Make sure that our comm channel write doesn't
+		 * get mixed in with writes from another CPU.
+		 */
+		mmiowb();
+	}
+}
+
 void mlx4_multi_func_cleanup(struct mlx4_dev *dev)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -2181,6 +2219,7 @@ void mlx4_multi_func_cleanup(struct mlx4_dev *dev)
 		kfree(priv->mfunc.master.slave_state);
 		kfree(priv->mfunc.master.vf_admin);
 		kfree(priv->mfunc.master.vf_oper);
+		dev->num_slaves = 0;
 	}
 
 	iounmap(priv->mfunc.comm);
diff --git a/drivers/net/ethernet/mellanox/mlx4/eq.c b/drivers/net/ethernet/mellanox/mlx4/eq.c
index c716d26415ff..9991c998f595 100644
--- a/drivers/net/ethernet/mellanox/mlx4/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/eq.c
@@ -430,8 +430,14 @@ void mlx4_master_handle_slave_flr(struct work_struct *work)
 		if (MLX4_COMM_CMD_FLR == slave_state[i].last_cmd) {
 			mlx4_dbg(dev, "mlx4_handle_slave_flr: clean slave: %d\n",
 				 i);
-
-			mlx4_delete_all_resources_for_slave(dev, i);
+			/* In case of 'Reset flow' FLR can be generated for
+			 * a slave before mlx4_load_one is done.
+			 * make sure interface is up before trying to delete
+			 * slave resources which weren't allocated yet.
+			 */
+			if (dev->persist->interface_state &
+			    MLX4_INTERFACE_STATE_UP)
+				mlx4_delete_all_resources_for_slave(dev, i);
 			/*return the slave to running mode*/
 			spin_lock_irqsave(&priv->mfunc.master.slave_state_lock, flags);
 			slave_state[i].last_cmd = MLX4_COMM_CMD_RESET;
diff --git a/drivers/net/ethernet/mellanox/mlx4/intf.c b/drivers/net/ethernet/mellanox/mlx4/intf.c
index 116895ac8b35..d7b503a78214 100644
--- a/drivers/net/ethernet/mellanox/mlx4/intf.c
+++ b/drivers/net/ethernet/mellanox/mlx4/intf.c
@@ -143,8 +143,7 @@ int mlx4_register_device(struct mlx4_dev *dev)
 		mlx4_add_device(intf, priv);
 
 	mutex_unlock(&intf_mutex);
-	if (!mlx4_is_slave(dev))
-		mlx4_start_catas_poll(dev);
+	mlx4_start_catas_poll(dev);
 
 	return 0;
 }
@@ -154,8 +153,7 @@ void mlx4_unregister_device(struct mlx4_dev *dev)
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_interface *intf;
 
-	if (!mlx4_is_slave(dev))
-		mlx4_stop_catas_poll(dev);
+	mlx4_stop_catas_poll(dev);
 	mutex_lock(&intf_mutex);
 
 	list_for_each_entry(intf, &intf_list, list)
* Unmerged path drivers/net/ethernet/mellanox/mlx4/main.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/mlx4.h b/drivers/net/ethernet/mellanox/mlx4/mlx4.h
index cebd1180702b..ffba4272a12a 100644
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4.h
@@ -85,7 +85,9 @@ enum {
 	MLX4_CLR_INT_SIZE	= 0x00008,
 	MLX4_SLAVE_COMM_BASE	= 0x0,
 	MLX4_COMM_PAGESIZE	= 0x1000,
-	MLX4_CLOCK_SIZE		= 0x00008
+	MLX4_CLOCK_SIZE		= 0x00008,
+	MLX4_COMM_CHAN_CAPS	= 0x8,
+	MLX4_COMM_CHAN_FLAGS	= 0xc
 };
 
 enum {
@@ -120,6 +122,8 @@ enum mlx4_mpt_state {
 };
 
 #define MLX4_COMM_TIME		10000
+#define MLX4_COMM_OFFLINE_TIME_OUT 30000
+
 enum {
 	MLX4_COMM_CMD_RESET,
 	MLX4_COMM_CMD_VHCR0,
@@ -1162,6 +1166,7 @@ enum {
 int mlx4_cmd_init(struct mlx4_dev *dev);
 void mlx4_cmd_cleanup(struct mlx4_dev *dev, int cleanup_mask);
 int mlx4_multi_func_init(struct mlx4_dev *dev);
+int mlx4_ARM_COMM_CHANNEL(struct mlx4_dev *dev);
 void mlx4_multi_func_cleanup(struct mlx4_dev *dev);
 void mlx4_cmd_event(struct mlx4_dev *dev, u16 token, u8 status, u64 out_param);
 int mlx4_cmd_use_events(struct mlx4_dev *dev);
* Unmerged path include/linux/mlx4/cmd.h
* Unmerged path include/linux/mlx4/device.h
