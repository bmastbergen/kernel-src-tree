IB/iser: Rewrite bounce buffer code path

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [infiniband] iser: Rewrite bounce buffer code path (Amir Vadai) [1164539]
Rebuild_FUZZ: 96.10%
commit-author Sagi Grimberg <sagig@mellanox.com>
commit ba943fb237ea48b01e3229f10cdb2a4274978a2d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/ba943fb2.failed

In some rare cases, IO operations may be not aligned to page
boundaries. This prevents iser from performing fast memory
registration. In order to overcome that iser uses a bounce
buffer to carry the transaction. We basically allocate a buffer
in the size of the transaction and perform a copy.

The buffer allocation using kmalloc is too restrictive since it
requires higher order (atomic) allocations for large transactions
(which may result in memory exhaustion fairly fast for some workloads).
We rewrite the bounce buffer code path to allocate scattered pages
and perform a copy between the transaction sg and the bounce sg.

	Reported-by: Alex Lyakas <alex@zadarastorage.com>
	Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit ba943fb237ea48b01e3229f10cdb2a4274978a2d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/ulp/iser/iscsi_iser.h
#	drivers/infiniband/ulp/iser/iser_initiator.c
#	drivers/infiniband/ulp/iser/iser_memory.c
diff --cc drivers/infiniband/ulp/iser/iscsi_iser.h
index 67a233350439,262ba1f8ee50..000000000000
--- a/drivers/infiniband/ulp/iser/iscsi_iser.h
+++ b/drivers/infiniband/ulp/iser/iscsi_iser.h
@@@ -197,15 -215,24 +197,36 @@@ enum iser_data_dir 
  	ISER_DIRS_NUM
  };
  
++<<<<<<< HEAD
 +struct iser_data_buf {
 +	void               *buf;      /* pointer to the sg list               */
 +	unsigned int       size;      /* num entries of this sg               */
 +	unsigned long      data_len;  /* total data len                       */
 +	unsigned int       dma_nents; /* returned by dma_map_sg               */
 +	char       	   *copy_buf; /* allocated copy buf for SGs unaligned *
 +	                               * for rdma which are copied            */
 +	struct scatterlist sg_single; /* SG-ified clone of a non SG SC or     *
 +				       * unaligned SG                         */
++=======
+ /**
+  * struct iser_data_buf - iSER data buffer
+  *
+  * @sg:           pointer to the sg list
+  * @size:         num entries of this sg
+  * @data_len:     total beffer byte len
+  * @dma_nents:    returned by dma_map_sg
+  * @orig_sg:      pointer to the original sg list (in case
+  *                we used a copy)
+  * @orig_size:    num entris of orig sg list
+  */
+ struct iser_data_buf {
+ 	struct scatterlist *sg;
+ 	unsigned int       size;
+ 	unsigned long      data_len;
+ 	unsigned int       dma_nents;
+ 	struct scatterlist *orig_sg;
+ 	unsigned int       orig_size;
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
    };
  
  /* fwd declarations */
diff --cc drivers/infiniband/ulp/iser/iser_initiator.c
index 065346a65b0d,3e2118e8ed87..000000000000
--- a/drivers/infiniband/ulp/iser/iser_initiator.c
+++ b/drivers/infiniband/ulp/iser/iser_initiator.c
@@@ -666,7 -674,7 +666,11 @@@ void iser_task_rdma_finalize(struct isc
  	/* if we were reading, copy back to unaligned sglist,
  	 * anyway dma_unmap and free the copy
  	 */
++<<<<<<< HEAD
 +	if (iser_task->data_copy[ISER_DIR_IN].copy_buf != NULL) {
++=======
+ 	if (iser_task->data[ISER_DIR_IN].orig_sg) {
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  		is_rdma_data_aligned = 0;
  		iser_finalize_rdma_unaligned_sg(iser_task,
  						&iser_task->data[ISER_DIR_IN],
@@@ -674,7 -681,7 +678,11 @@@
  						ISER_DIR_IN);
  	}
  
++<<<<<<< HEAD
 +	if (iser_task->data_copy[ISER_DIR_OUT].copy_buf != NULL) {
++=======
+ 	if (iser_task->data[ISER_DIR_OUT].orig_sg) {
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  		is_rdma_data_aligned = 0;
  		iser_finalize_rdma_unaligned_sg(iser_task,
  						&iser_task->data[ISER_DIR_OUT],
@@@ -682,7 -688,7 +690,11 @@@
  						ISER_DIR_OUT);
  	}
  
++<<<<<<< HEAD
 +	if (iser_task->prot_copy[ISER_DIR_IN].copy_buf != NULL) {
++=======
+ 	if (iser_task->prot[ISER_DIR_IN].orig_sg) {
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  		is_rdma_prot_aligned = 0;
  		iser_finalize_rdma_unaligned_sg(iser_task,
  						&iser_task->prot[ISER_DIR_IN],
@@@ -690,7 -695,7 +702,11 @@@
  						ISER_DIR_IN);
  	}
  
++<<<<<<< HEAD
 +	if (iser_task->prot_copy[ISER_DIR_OUT].copy_buf != NULL) {
++=======
+ 	if (iser_task->prot[ISER_DIR_OUT].orig_sg) {
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  		is_rdma_prot_aligned = 0;
  		iser_finalize_rdma_unaligned_sg(iser_task,
  						&iser_task->prot[ISER_DIR_OUT],
diff --cc drivers/infiniband/ulp/iser/iser_memory.c
index f40132fc970c,f0cdc961eb11..000000000000
--- a/drivers/infiniband/ulp/iser/iser_memory.c
+++ b/drivers/infiniband/ulp/iser/iser_memory.c
@@@ -39,66 -39,173 +39,190 @@@
  
  #include "iscsi_iser.h"
  
- #define ISER_KMALLOC_THRESHOLD 0x20000 /* 128K - kmalloc limit */
+ static void
+ iser_free_bounce_sg(struct iser_data_buf *data)
+ {
+ 	struct scatterlist *sg;
+ 	int count;
+ 
+ 	for_each_sg(data->sg, sg, data->size, count)
+ 		__free_page(sg_page(sg));
+ 
+ 	kfree(data->sg);
+ 
+ 	data->sg = data->orig_sg;
+ 	data->size = data->orig_size;
+ 	data->orig_sg = NULL;
+ 	data->orig_size = 0;
+ }
+ 
+ static int
+ iser_alloc_bounce_sg(struct iser_data_buf *data)
+ {
+ 	struct scatterlist *sg;
+ 	struct page *page;
+ 	unsigned long length = data->data_len;
+ 	int i = 0, nents = DIV_ROUND_UP(length, PAGE_SIZE);
+ 
+ 	sg = kcalloc(nents, sizeof(*sg), GFP_ATOMIC);
+ 	if (!sg)
+ 		goto err;
+ 
+ 	sg_init_table(sg, nents);
+ 	while (length) {
+ 		u32 page_len = min_t(u32, length, PAGE_SIZE);
+ 
+ 		page = alloc_page(GFP_ATOMIC);
+ 		if (!page)
+ 			goto err;
+ 
+ 		sg_set_page(&sg[i], page, page_len, 0);
+ 		length -= page_len;
+ 		i++;
+ 	}
+ 
+ 	data->orig_sg = data->sg;
+ 	data->orig_size = data->size;
+ 	data->sg = sg;
+ 	data->size = nents;
+ 
+ 	return 0;
+ 
+ err:
+ 	for (; i > 0; i--)
+ 		__free_page(sg_page(&sg[i - 1]));
+ 	kfree(sg);
+ 
+ 	return -ENOMEM;
+ }
+ 
+ static void
+ iser_copy_bounce(struct iser_data_buf *data, bool to_buffer)
+ {
+ 	struct scatterlist *osg, *bsg = data->sg;
+ 	void *oaddr, *baddr;
+ 	unsigned int left = data->data_len;
+ 	unsigned int bsg_off = 0;
+ 	int i;
+ 
+ 	for_each_sg(data->orig_sg, osg, data->orig_size, i) {
+ 		unsigned int copy_len, osg_off = 0;
+ 
+ 		oaddr = kmap_atomic(sg_page(osg)) + osg->offset;
+ 		copy_len = min(left, osg->length);
+ 		while (copy_len) {
+ 			unsigned int len = min(copy_len, bsg->length - bsg_off);
+ 
+ 			baddr = kmap_atomic(sg_page(bsg)) + bsg->offset;
+ 			if (to_buffer)
+ 				memcpy(baddr + bsg_off, oaddr + osg_off, len);
+ 			else
+ 				memcpy(oaddr + osg_off, baddr + bsg_off, len);
+ 
+ 			kunmap_atomic(baddr - bsg->offset);
+ 			osg_off += len;
+ 			bsg_off += len;
+ 			copy_len -= len;
+ 
+ 			if (bsg_off >= bsg->length) {
+ 				bsg = sg_next(bsg);
+ 				bsg_off = 0;
+ 			}
+ 		}
+ 		kunmap_atomic(oaddr - osg->offset);
+ 		left -= osg_off;
+ 	}
+ }
+ 
+ static inline void
+ iser_copy_from_bounce(struct iser_data_buf *data)
+ {
+ 	iser_copy_bounce(data, false);
+ }
+ 
+ static inline void
+ iser_copy_to_bounce(struct iser_data_buf *data)
+ {
+ 	iser_copy_bounce(data, true);
+ }
  
 -struct fast_reg_descriptor *
 -iser_reg_desc_get(struct ib_conn *ib_conn)
 -{
 -	struct fast_reg_descriptor *desc;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&ib_conn->lock, flags);
 -	desc = list_first_entry(&ib_conn->fastreg.pool,
 -				struct fast_reg_descriptor, list);
 -	list_del(&desc->list);
 -	spin_unlock_irqrestore(&ib_conn->lock, flags);
 -
 -	return desc;
 -}
 -
 -void
 -iser_reg_desc_put(struct ib_conn *ib_conn,
 -		  struct fast_reg_descriptor *desc)
 -{
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&ib_conn->lock, flags);
 -	list_add(&desc->list, &ib_conn->fastreg.pool);
 -	spin_unlock_irqrestore(&ib_conn->lock, flags);
 -}
 -
  /**
   * iser_start_rdma_unaligned_sg
   */
  static int iser_start_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,
  					struct iser_data_buf *data,
 +					struct iser_data_buf *data_copy,
  					enum iser_data_dir cmd_dir)
  {
++<<<<<<< HEAD
 +	struct ib_device *dev = iser_task->ib_conn->device->ib_device;
 +	struct scatterlist *sgl = (struct scatterlist *)data->buf;
 +	struct scatterlist *sg;
 +	char *mem = NULL;
 +	unsigned long  cmd_data_len = data->data_len;
 +	int dma_nents, i;
++=======
+ 	struct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;
+ 	int rc;
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  
- 	if (cmd_data_len > ISER_KMALLOC_THRESHOLD)
- 		mem = (void *)__get_free_pages(GFP_ATOMIC,
- 		      ilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);
- 	else
- 		mem = kmalloc(cmd_data_len, GFP_ATOMIC);
- 
- 	if (mem == NULL) {
- 		iser_err("Failed to allocate mem size %d %d for copying sglist\n",
- 			 data->size, (int)cmd_data_len);
- 		return -ENOMEM;
+ 	rc = iser_alloc_bounce_sg(data);
+ 	if (rc) {
+ 		iser_err("Failed to allocate bounce for data len %lu\n",
+ 			 data->data_len);
+ 		return rc;
  	}
  
++<<<<<<< HEAD
 +	if (cmd_dir == ISER_DIR_OUT) {
 +		/* copy the unaligned sg the buffer which is used for RDMA */
 +		int i;
 +		char *p, *from;
 +
 +		sgl = (struct scatterlist *)data->buf;
 +		p = mem;
 +		for_each_sg(sgl, sg, data->size, i) {
 +			from = kmap_atomic(sg_page(sg));
 +			memcpy(p,
 +			       from + sg->offset,
 +			       sg->length);
 +			kunmap_atomic(from);
 +			p += sg->length;
 +		}
 +	}
 +
 +	sg_init_one(&data_copy->sg_single, mem, cmd_data_len);
 +	data_copy->buf = &data_copy->sg_single;
 +	data_copy->size = 1;
 +	data_copy->copy_buf = mem;
 +
 +	dma_nents = ib_dma_map_sg(dev, &data_copy->sg_single, 1,
 +				  (cmd_dir == ISER_DIR_OUT) ?
 +				  DMA_TO_DEVICE : DMA_FROM_DEVICE);
 +	BUG_ON(dma_nents == 0);
 +
 +	data_copy->dma_nents = dma_nents;
 +	data_copy->data_len = cmd_data_len;
 +
++=======
+ 	if (cmd_dir == ISER_DIR_OUT)
+ 		iser_copy_to_bounce(data);
+ 
+ 	data->dma_nents = ib_dma_map_sg(dev, data->sg, data->size,
+ 					(cmd_dir == ISER_DIR_OUT) ?
+ 					DMA_TO_DEVICE : DMA_FROM_DEVICE);
+ 	if (!data->dma_nents) {
+ 		iser_err("Got dma_nents %d, something went wrong...\n",
+ 			 data->dma_nents);
+ 		rc = -ENOMEM;
+ 		goto err;
+ 	}
+ 
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  	return 0;
+ err:
+ 	iser_free_bounce_sg(data);
+ 	return rc;
  }
  
  /**
@@@ -107,51 -214,18 +231,54 @@@
  
  void iser_finalize_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,
  				     struct iser_data_buf *data,
 +				     struct iser_data_buf *data_copy,
  				     enum iser_data_dir cmd_dir)
  {
- 	struct ib_device *dev;
- 	unsigned long  cmd_data_len;
+ 	struct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;
  
++<<<<<<< HEAD
 +	dev = iser_task->ib_conn->device->ib_device;
 +
 +	ib_dma_unmap_sg(dev, &data_copy->sg_single, 1,
++=======
+ 	ib_dma_unmap_sg(dev, data->sg, data->size,
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  			(cmd_dir == ISER_DIR_OUT) ?
  			DMA_TO_DEVICE : DMA_FROM_DEVICE);
  
- 	if (cmd_dir == ISER_DIR_IN) {
- 		char *mem;
- 		struct scatterlist *sgl, *sg;
- 		unsigned char *p, *to;
- 		unsigned int sg_size;
- 		int i;
+ 	if (cmd_dir == ISER_DIR_IN)
+ 		iser_copy_from_bounce(data);
  
++<<<<<<< HEAD
 +		/* copy back read RDMA to unaligned sg */
 +		mem = data_copy->copy_buf;
 +
 +		sgl = (struct scatterlist *)data->buf;
 +		sg_size = data->size;
 +
 +		p = mem;
 +		for_each_sg(sgl, sg, sg_size, i) {
 +			to = kmap_atomic(sg_page(sg));
 +			memcpy(to + sg->offset,
 +			       p,
 +			       sg->length);
 +			kunmap_atomic(to);
 +			p += sg->length;
 +		}
 +	}
 +
 +	cmd_data_len = data->data_len;
 +
 +	if (cmd_data_len > ISER_KMALLOC_THRESHOLD)
 +		free_pages((unsigned long)data_copy->copy_buf,
 +			   ilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);
 +	else
 +		kfree(data_copy->copy_buf);
 +
 +	data_copy->copy_buf = NULL;
++=======
+ 	iser_free_bounce_sg(data);
++>>>>>>> ba943fb237ea (IB/iser: Rewrite bounce buffer code path)
  }
  
  #define IS_4K_ALIGNED(addr)	((((unsigned long)addr) & ~MASK_4K) == 0)
* Unmerged path drivers/infiniband/ulp/iser/iscsi_iser.h
* Unmerged path drivers/infiniband/ulp/iser/iser_initiator.c
* Unmerged path drivers/infiniband/ulp/iser/iser_memory.c
