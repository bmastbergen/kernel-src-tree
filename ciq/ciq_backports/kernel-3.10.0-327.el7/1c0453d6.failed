IB/ipoib: drop mcast_mutex usage

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [infiniband] ipoib: drop mcast_mutex usage (Doug Ledford) [1183881]
Rebuild_FUZZ: 95.08%
commit-author Doug Ledford <dledford@redhat.com>
commit 1c0453d64a341909bbf89cb68c9edaa6cff93850
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/1c0453d6.failed

We needed the mcast_mutex when we had to prevent the join completion
callback from having the value it stored in mcast->mc overwritten
by a delayed return from ib_sa_join_multicast.  By storing the return
of ib_sa_join_multicast in an intermediate variable, we prevent a
delayed return from ib_sa_join_multicast overwriting the valid
contents of mcast->mc, and we no longer need a mutex to force the
join callback to run after the return of ib_sa_join_multicast.  This
allows us to do away with the mutex entirely and protect our critical
sections with a just a spinlock instead.  This is highly desirable
as there were some places where we couldn't use a mutex because the
code was not allowed to sleep, and so we were currently using a mix
of mutex and spinlock to protect what we needed to protect.  Now we
only have a spin lock and the locking complexity is greatly reduced.

	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 1c0453d64a341909bbf89cb68c9edaa6cff93850)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/ulp/ipoib/ipoib_multicast.c
diff --cc drivers/infiniband/ulp/ipoib/ipoib_multicast.c
index ffb83b5f7e80,3203ebe9b100..000000000000
--- a/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
@@@ -66,6 -64,48 +64,51 @@@ struct ipoib_mcast_iter 
  	unsigned int       send_only;
  };
  
++<<<<<<< HEAD
++=======
+ /*
+  * This should be called with the priv->lock held
+  */
+ static void __ipoib_mcast_schedule_join_thread(struct ipoib_dev_priv *priv,
+ 					       struct ipoib_mcast *mcast,
+ 					       bool delay)
+ {
+ 	if (!test_bit(IPOIB_MCAST_RUN, &priv->flags))
+ 		return;
+ 
+ 	/*
+ 	 * We will be scheduling *something*, so cancel whatever is
+ 	 * currently scheduled first
+ 	 */
+ 	cancel_delayed_work(&priv->mcast_task);
+ 	if (mcast && delay) {
+ 		/*
+ 		 * We had a failure and want to schedule a retry later
+ 		 */
+ 		mcast->backoff *= 2;
+ 		if (mcast->backoff > IPOIB_MAX_BACKOFF_SECONDS)
+ 			mcast->backoff = IPOIB_MAX_BACKOFF_SECONDS;
+ 		mcast->delay_until = jiffies + (mcast->backoff * HZ);
+ 		/*
+ 		 * Mark this mcast for its delay, but restart the
+ 		 * task immediately.  The join task will make sure to
+ 		 * clear out all entries without delays, and then
+ 		 * schedule itself to run again when the earliest
+ 		 * delay expires
+ 		 */
+ 		queue_delayed_work(priv->wq, &priv->mcast_task, 0);
+ 	} else if (delay) {
+ 		/*
+ 		 * Special case of retrying after a failure to
+ 		 * allocate the broadcast multicast group, wait
+ 		 * 1 second and try again
+ 		 */
+ 		queue_delayed_work(priv->wq, &priv->mcast_task, HZ);
+ 	} else
+ 		queue_delayed_work(priv->wq, &priv->mcast_task, 0);
+ }
+ 
++>>>>>>> 1c0453d64a34 (IB/ipoib: drop mcast_mutex usage)
  static void ipoib_mcast_free(struct ipoib_mcast *mcast)
  {
  	struct net_device *dev = mcast->dev;
@@@ -396,49 -361,74 +439,111 @@@ static int ipoib_mcast_join_complete(in
  
  	if (!status) {
  		mcast->backoff = 1;
 -		mcast->delay_until = jiffies;
 +		mutex_lock(&mcast_mutex);
 +		if (test_bit(IPOIB_MCAST_RUN, &priv->flags))
 +			queue_delayed_work(ipoib_workqueue,
 +					   &priv->mcast_task, 0);
 +		mutex_unlock(&mcast_mutex);
  
  		/*
 -		 * Defer carrier on work to priv->wq to avoid a
 -		 * deadlock on rtnl_lock here.  Requeue our multicast
 -		 * work too, which will end up happening right after
 -		 * our carrier on task work and will allow us to
 -		 * send out all of the non-broadcast joins
 +		 * Defer carrier on work to ipoib_workqueue to avoid a
 +		 * deadlock on rtnl_lock here.
  		 */
++<<<<<<< HEAD
 +		if (mcast == priv->broadcast)
 +			queue_work(ipoib_workqueue, &priv->carrier_on_task);
 +
 +		status = 0;
 +		goto out;
 +	}
 +
 +	if (mcast->logcount++ < 20) {
 +		if (status == -ETIMEDOUT || status == -EAGAIN) {
 +			ipoib_dbg_mcast(priv, "multicast join failed for %pI6, status %d\n",
 +					mcast->mcmember.mgid.raw, status);
 +		} else {
 +			ipoib_warn(priv, "multicast join failed for %pI6, status %d\n",
 +				   mcast->mcmember.mgid.raw, status);
 +		}
 +	}
 +
 +	mcast->backoff *= 2;
 +	if (mcast->backoff > IPOIB_MAX_BACKOFF_SECONDS)
 +		mcast->backoff = IPOIB_MAX_BACKOFF_SECONDS;
 +
 +	/* Clear the busy flag so we try again */
 +	status = test_and_clear_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags);
 +
 +	mutex_lock(&mcast_mutex);
 +	spin_lock_irq(&priv->lock);
 +	if (test_bit(IPOIB_MCAST_RUN, &priv->flags))
 +		queue_delayed_work(ipoib_workqueue, &priv->mcast_task,
 +				   mcast->backoff * HZ);
 +	spin_unlock_irq(&priv->lock);
 +	mutex_unlock(&mcast_mutex);
 +out:
 +	complete(&mcast->done);
++=======
+ 		if (mcast == priv->broadcast) {
+ 			spin_lock_irq(&priv->lock);
+ 			queue_work(priv->wq, &priv->carrier_on_task);
+ 			__ipoib_mcast_schedule_join_thread(priv, NULL, 0);
+ 			goto out_locked;
+ 		}
+ 	} else {
+ 		if (mcast->logcount++ < 20) {
+ 			if (status == -ETIMEDOUT || status == -EAGAIN) {
+ 				ipoib_dbg_mcast(priv, "%smulticast join failed for %pI6, status %d\n",
+ 						test_bit(IPOIB_MCAST_FLAG_SENDONLY, &mcast->flags) ? "sendonly " : "",
+ 						mcast->mcmember.mgid.raw, status);
+ 			} else {
+ 				ipoib_warn(priv, "%smulticast join failed for %pI6, status %d\n",
+ 						test_bit(IPOIB_MCAST_FLAG_SENDONLY, &mcast->flags) ? "sendonly " : "",
+ 					   mcast->mcmember.mgid.raw, status);
+ 			}
+ 		}
+ 
+ 		if (test_bit(IPOIB_MCAST_FLAG_SENDONLY, &mcast->flags) &&
+ 		    mcast->backoff >= 2) {
+ 			/*
+ 			 * We only retry sendonly joins once before we drop
+ 			 * the packet and quit trying to deal with the
+ 			 * group.  However, we leave the group in the
+ 			 * mcast list as an unjoined group.  If we want to
+ 			 * try joining again, we simply queue up a packet
+ 			 * and restart the join thread.  The empty queue
+ 			 * is why the join thread ignores this group.
+ 			 */
+ 			mcast->backoff = 1;
+ 			netif_tx_lock_bh(dev);
+ 			while (!skb_queue_empty(&mcast->pkt_queue)) {
+ 				++dev->stats.tx_dropped;
+ 				dev_kfree_skb_any(skb_dequeue(&mcast->pkt_queue));
+ 			}
+ 			netif_tx_unlock_bh(dev);
+ 		} else {
+ 			spin_lock_irq(&priv->lock);
+ 			/* Requeue this join task with a backoff delay */
+ 			__ipoib_mcast_schedule_join_thread(priv, mcast, 1);
+ 			goto out_locked;
+ 		}
+ 	}
+ out:
+ 	spin_lock_irq(&priv->lock);
+ out_locked:
+ 	/*
+ 	 * Make sure to set mcast->mc before we clear the busy flag to avoid
+ 	 * racing with code that checks for BUSY before checking mcast->mc
+ 	 */
+ 	if (status)
+ 		mcast->mc = NULL;
+ 	else
+ 		mcast->mc = multicast;
+ 	clear_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags);
+ 	spin_unlock_irq(&priv->lock);
+ 	complete(&mcast->done);
+ 
++>>>>>>> 1c0453d64a34 (IB/ipoib: drop mcast_mutex usage)
  	return status;
  }
  
@@@ -487,29 -478,18 +593,44 @@@ static void ipoib_mcast_join(struct net
  		rec.hop_limit	  = priv->broadcast->mcmember.hop_limit;
  	}
  
++<<<<<<< HEAD
 +	set_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags);
 +	init_completion(&mcast->done);
 +	set_bit(IPOIB_MCAST_JOIN_STARTED, &mcast->flags);
 +
 +	mcast->mc = ib_sa_join_multicast(&ipoib_sa_client, priv->ca, priv->port,
 +					 &rec, comp_mask, GFP_KERNEL,
 +					 ipoib_mcast_join_complete, mcast);
 +	if (IS_ERR(mcast->mc)) {
 +		clear_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags);
 +		complete(&mcast->done);
 +		ret = PTR_ERR(mcast->mc);
 +		ipoib_warn(priv, "ib_sa_join_multicast failed, status %d\n", ret);
 +
 +		mcast->backoff *= 2;
 +		if (mcast->backoff > IPOIB_MAX_BACKOFF_SECONDS)
 +			mcast->backoff = IPOIB_MAX_BACKOFF_SECONDS;
 +
 +		mutex_lock(&mcast_mutex);
 +		if (test_bit(IPOIB_MCAST_RUN, &priv->flags))
 +			queue_delayed_work(ipoib_workqueue,
 +					   &priv->mcast_task,
 +					   mcast->backoff * HZ);
 +		mutex_unlock(&mcast_mutex);
++=======
+ 	multicast = ib_sa_join_multicast(&ipoib_sa_client, priv->ca, priv->port,
+ 					 &rec, comp_mask, GFP_KERNEL,
+ 					 ipoib_mcast_join_complete, mcast);
+ 	if (IS_ERR(multicast)) {
+ 		ret = PTR_ERR(multicast);
+ 		ipoib_warn(priv, "ib_sa_join_multicast failed, status %d\n", ret);
+ 		spin_lock_irq(&priv->lock);
+ 		/* Requeue this join task with a backoff delay */
+ 		__ipoib_mcast_schedule_join_thread(priv, mcast, 1);
+ 		clear_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags);
+ 		spin_unlock_irq(&priv->lock);
+ 		complete(&mcast->done);
++>>>>>>> 1c0453d64a34 (IB/ipoib: drop mcast_mutex usage)
  	}
  }
  
@@@ -536,6 -519,10 +657,13 @@@ void ipoib_mcast_join_task(struct work_
  	else
  		memcpy(priv->dev->dev_addr + 4, priv->local_gid.raw, sizeof (union ib_gid));
  
++<<<<<<< HEAD
++=======
+ 	spin_lock_irq(&priv->lock);
+ 	if (!test_bit(IPOIB_FLAG_OPER_UP, &priv->flags))
+ 		goto out;
+ 
++>>>>>>> 1c0453d64a34 (IB/ipoib: drop mcast_mutex usage)
  	if (!priv->broadcast) {
  		struct ipoib_mcast *broadcast;
  
@@@ -563,37 -547,62 +691,80 @@@
  	}
  
  	if (!test_bit(IPOIB_MCAST_FLAG_ATTACHED, &priv->broadcast->flags)) {
 -		if (IS_ERR_OR_NULL(priv->broadcast->mc) &&
 -		    !test_bit(IPOIB_MCAST_FLAG_BUSY, &priv->broadcast->flags)) {
 -			mcast = priv->broadcast;
 -			create = 0;
 -			if (mcast->backoff > 1 &&
 -			    time_before(jiffies, mcast->delay_until)) {
 -				delay_until = mcast->delay_until;
 -				mcast = NULL;
 +		if (!test_bit(IPOIB_MCAST_FLAG_BUSY, &priv->broadcast->flags))
 +			ipoib_mcast_join(dev, priv->broadcast, 0);
 +		return;
 +	}
 +
 +	while (1) {
 +		struct ipoib_mcast *mcast = NULL;
 +
 +		spin_lock_irq(&priv->lock);
 +		list_for_each_entry(mcast, &priv->multicast_list, list) {
 +			if (!test_bit(IPOIB_MCAST_FLAG_SENDONLY, &mcast->flags)
 +			    && !test_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags)
 +			    && !test_bit(IPOIB_MCAST_FLAG_ATTACHED, &mcast->flags)) {
 +				/* Found the next unjoined group */
 +				break;
  			}
  		}
 -		goto out;
 -	}
 +		spin_unlock_irq(&priv->lock);
  
++<<<<<<< HEAD
 +		if (&mcast->list == &priv->multicast_list) {
 +			/* All done */
 +			break;
++=======
+ 	/*
+ 	 * We'll never get here until the broadcast group is both allocated
+ 	 * and attached
+ 	 */
+ 	list_for_each_entry(mcast, &priv->multicast_list, list) {
+ 		if (IS_ERR_OR_NULL(mcast->mc) &&
+ 		    !test_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags) &&
+ 		    (!test_bit(IPOIB_MCAST_FLAG_SENDONLY, &mcast->flags) ||
+ 		     !skb_queue_empty(&mcast->pkt_queue))) {
+ 			if (mcast->backoff == 1 ||
+ 			    time_after_eq(jiffies, mcast->delay_until)) {
+ 				/* Found the next unjoined group */
+ 				init_completion(&mcast->done);
+ 				set_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags);
+ 				if (test_bit(IPOIB_MCAST_FLAG_SENDONLY, &mcast->flags))
+ 					create = 0;
+ 				else
+ 					create = 1;
+ 				spin_unlock_irq(&priv->lock);
+ 				ipoib_mcast_join(dev, mcast, create);
+ 				spin_lock_irq(&priv->lock);
+ 			} else if (!delay_until ||
+ 				 time_before(mcast->delay_until, delay_until))
+ 				delay_until = mcast->delay_until;
++>>>>>>> 1c0453d64a34 (IB/ipoib: drop mcast_mutex usage)
  		}
 +
 +		ipoib_mcast_join(dev, mcast, 1);
 +		return;
  	}
  
 -	mcast = NULL;
 -	ipoib_dbg_mcast(priv, "successfully started all multicast joins\n");
 +	ipoib_dbg_mcast(priv, "successfully joined all multicast groups\n");
  
++<<<<<<< HEAD
 +	clear_bit(IPOIB_MCAST_RUN, &priv->flags);
++=======
+ out:
+ 	if (delay_until) {
+ 		cancel_delayed_work(&priv->mcast_task);
+ 		queue_delayed_work(priv->wq, &priv->mcast_task,
+ 				   delay_until - jiffies);
+ 	}
+ 	if (mcast) {
+ 		init_completion(&mcast->done);
+ 		set_bit(IPOIB_MCAST_FLAG_BUSY, &mcast->flags);
+ 	}
+ 	spin_unlock_irq(&priv->lock);
+ 	if (mcast)
+ 		ipoib_mcast_join(dev, mcast, create);
++>>>>>>> 1c0453d64a34 (IB/ipoib: drop mcast_mutex usage)
  }
  
  int ipoib_mcast_start_thread(struct net_device *dev)
@@@ -602,27 -612,27 +774,35 @@@
  
  	ipoib_dbg_mcast(priv, "starting multicast thread\n");
  
++<<<<<<< HEAD
 +	mutex_lock(&mcast_mutex);
 +	if (!test_and_set_bit(IPOIB_MCAST_RUN, &priv->flags))
 +		queue_delayed_work(ipoib_workqueue, &priv->mcast_task, 0);
 +	mutex_unlock(&mcast_mutex);
++=======
+ 	spin_lock_irqsave(&priv->lock, flags);
+ 	set_bit(IPOIB_MCAST_RUN, &priv->flags);
+ 	__ipoib_mcast_schedule_join_thread(priv, NULL, 0);
+ 	spin_unlock_irqrestore(&priv->lock, flags);
++>>>>>>> 1c0453d64a34 (IB/ipoib: drop mcast_mutex usage)
  
  	return 0;
  }
  
 -int ipoib_mcast_stop_thread(struct net_device *dev)
 +int ipoib_mcast_stop_thread(struct net_device *dev, int flush)
  {
  	struct ipoib_dev_priv *priv = netdev_priv(dev);
+ 	unsigned long flags;
  
  	ipoib_dbg_mcast(priv, "stopping multicast thread\n");
  
- 	mutex_lock(&mcast_mutex);
+ 	spin_lock_irqsave(&priv->lock, flags);
  	clear_bit(IPOIB_MCAST_RUN, &priv->flags);
  	cancel_delayed_work(&priv->mcast_task);
- 	mutex_unlock(&mcast_mutex);
+ 	spin_unlock_irqrestore(&priv->lock, flags);
  
 -	flush_workqueue(priv->wq);
 +	if (flush)
 +		flush_workqueue(ipoib_workqueue);
  
  	return 0;
  }
* Unmerged path drivers/infiniband/ulp/ipoib/ipoib_multicast.c
