workqueue: Create low-level unbound workqueues cpumask

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Frederic Weisbecker <fweisbec@gmail.com>
commit b05a79280b346eb24ddb73b39988398015291075
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/b05a7928.failed

Create a cpumask that limits the affinity of all unbound workqueues.
This cpumask is controlled through a file at the root of the workqueue
sysfs directory.

It works on a lower-level than the per WQ_SYSFS workqueues cpumask files
such that the effective cpumask applied for a given unbound workqueue is
the intersection of /sys/devices/virtual/workqueue/$WORKQUEUE/cpumask and
the new /sys/devices/virtual/workqueue/cpumask file.

This patch implements the basic infrastructure and the read interface.
wq_unbound_cpumask is initially set to cpu_possible_mask.

	Cc: Christoph Lameter <cl@linux.com>
	Cc: Kevin Hilman <khilman@linaro.org>
	Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
	Cc: Mike Galbraith <bitbucket@online.de>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
	Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
	Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit b05a79280b346eb24ddb73b39988398015291075)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/workqueue.c
diff --cc kernel/workqueue.c
index 14a71630e038,9be75e2a4da6..000000000000
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@@ -280,9 -296,11 +280,11 @@@ static struct workqueue_attrs *wq_updat
  static DEFINE_MUTEX(wq_pool_mutex);	/* protects pools and workqueues list */
  static DEFINE_SPINLOCK(wq_mayday_lock);	/* protects wq->maydays list */
  
 -static LIST_HEAD(workqueues);		/* PR: list of all workqueues */
 +static LIST_HEAD(workqueues);		/* PL: list of all workqueues */
  static bool workqueue_freezing;		/* PL: have wqs started freezing? */
  
+ static cpumask_var_t wq_unbound_cpumask;
+ 
  /* the per-cpu worker pools */
  static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS],
  				     cpu_worker_pools);
@@@ -4909,6 -4731,343 +4911,346 @@@ out_unlock
  }
  #endif /* CONFIG_FREEZER */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_SYSFS
+ /*
+  * Workqueues with WQ_SYSFS flag set is visible to userland via
+  * /sys/bus/workqueue/devices/WQ_NAME.  All visible workqueues have the
+  * following attributes.
+  *
+  *  per_cpu	RO bool	: whether the workqueue is per-cpu or unbound
+  *  max_active	RW int	: maximum number of in-flight work items
+  *
+  * Unbound workqueues have the following extra attributes.
+  *
+  *  id		RO int	: the associated pool ID
+  *  nice	RW int	: nice value of the workers
+  *  cpumask	RW mask	: bitmask of allowed CPUs for the workers
+  */
+ struct wq_device {
+ 	struct workqueue_struct		*wq;
+ 	struct device			dev;
+ };
+ 
+ static struct workqueue_struct *dev_to_wq(struct device *dev)
+ {
+ 	struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);
+ 
+ 	return wq_dev->wq;
+ }
+ 
+ static ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,
+ 			    char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 
+ 	return scnprintf(buf, PAGE_SIZE, "%d\n", (bool)!(wq->flags & WQ_UNBOUND));
+ }
+ static DEVICE_ATTR_RO(per_cpu);
+ 
+ static ssize_t max_active_show(struct device *dev,
+ 			       struct device_attribute *attr, char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 
+ 	return scnprintf(buf, PAGE_SIZE, "%d\n", wq->saved_max_active);
+ }
+ 
+ static ssize_t max_active_store(struct device *dev,
+ 				struct device_attribute *attr, const char *buf,
+ 				size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int val;
+ 
+ 	if (sscanf(buf, "%d", &val) != 1 || val <= 0)
+ 		return -EINVAL;
+ 
+ 	workqueue_set_max_active(wq, val);
+ 	return count;
+ }
+ static DEVICE_ATTR_RW(max_active);
+ 
+ static struct attribute *wq_sysfs_attrs[] = {
+ 	&dev_attr_per_cpu.attr,
+ 	&dev_attr_max_active.attr,
+ 	NULL,
+ };
+ ATTRIBUTE_GROUPS(wq_sysfs);
+ 
+ static ssize_t wq_pool_ids_show(struct device *dev,
+ 				struct device_attribute *attr, char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	const char *delim = "";
+ 	int node, written = 0;
+ 
+ 	rcu_read_lock_sched();
+ 	for_each_node(node) {
+ 		written += scnprintf(buf + written, PAGE_SIZE - written,
+ 				     "%s%d:%d", delim, node,
+ 				     unbound_pwq_by_node(wq, node)->pool->id);
+ 		delim = " ";
+ 	}
+ 	written += scnprintf(buf + written, PAGE_SIZE - written, "\n");
+ 	rcu_read_unlock_sched();
+ 
+ 	return written;
+ }
+ 
+ static ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,
+ 			    char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int written;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	written = scnprintf(buf, PAGE_SIZE, "%d\n", wq->unbound_attrs->nice);
+ 	mutex_unlock(&wq->mutex);
+ 
+ 	return written;
+ }
+ 
+ /* prepare workqueue_attrs for sysfs store operations */
+ static struct workqueue_attrs *wq_sysfs_prep_attrs(struct workqueue_struct *wq)
+ {
+ 	struct workqueue_attrs *attrs;
+ 
+ 	attrs = alloc_workqueue_attrs(GFP_KERNEL);
+ 	if (!attrs)
+ 		return NULL;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	copy_workqueue_attrs(attrs, wq->unbound_attrs);
+ 	mutex_unlock(&wq->mutex);
+ 	return attrs;
+ }
+ 
+ static ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,
+ 			     const char *buf, size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	struct workqueue_attrs *attrs;
+ 	int ret;
+ 
+ 	attrs = wq_sysfs_prep_attrs(wq);
+ 	if (!attrs)
+ 		return -ENOMEM;
+ 
+ 	if (sscanf(buf, "%d", &attrs->nice) == 1 &&
+ 	    attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)
+ 		ret = apply_workqueue_attrs(wq, attrs);
+ 	else
+ 		ret = -EINVAL;
+ 
+ 	free_workqueue_attrs(attrs);
+ 	return ret ?: count;
+ }
+ 
+ static ssize_t wq_cpumask_show(struct device *dev,
+ 			       struct device_attribute *attr, char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int written;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	written = scnprintf(buf, PAGE_SIZE, "%*pb\n",
+ 			    cpumask_pr_args(wq->unbound_attrs->cpumask));
+ 	mutex_unlock(&wq->mutex);
+ 	return written;
+ }
+ 
+ static ssize_t wq_cpumask_store(struct device *dev,
+ 				struct device_attribute *attr,
+ 				const char *buf, size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	struct workqueue_attrs *attrs;
+ 	int ret;
+ 
+ 	attrs = wq_sysfs_prep_attrs(wq);
+ 	if (!attrs)
+ 		return -ENOMEM;
+ 
+ 	ret = cpumask_parse(buf, attrs->cpumask);
+ 	if (!ret)
+ 		ret = apply_workqueue_attrs(wq, attrs);
+ 
+ 	free_workqueue_attrs(attrs);
+ 	return ret ?: count;
+ }
+ 
+ static ssize_t wq_numa_show(struct device *dev, struct device_attribute *attr,
+ 			    char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int written;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	written = scnprintf(buf, PAGE_SIZE, "%d\n",
+ 			    !wq->unbound_attrs->no_numa);
+ 	mutex_unlock(&wq->mutex);
+ 
+ 	return written;
+ }
+ 
+ static ssize_t wq_numa_store(struct device *dev, struct device_attribute *attr,
+ 			     const char *buf, size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	struct workqueue_attrs *attrs;
+ 	int v, ret;
+ 
+ 	attrs = wq_sysfs_prep_attrs(wq);
+ 	if (!attrs)
+ 		return -ENOMEM;
+ 
+ 	ret = -EINVAL;
+ 	if (sscanf(buf, "%d", &v) == 1) {
+ 		attrs->no_numa = !v;
+ 		ret = apply_workqueue_attrs(wq, attrs);
+ 	}
+ 
+ 	free_workqueue_attrs(attrs);
+ 	return ret ?: count;
+ }
+ 
+ static struct device_attribute wq_sysfs_unbound_attrs[] = {
+ 	__ATTR(pool_ids, 0444, wq_pool_ids_show, NULL),
+ 	__ATTR(nice, 0644, wq_nice_show, wq_nice_store),
+ 	__ATTR(cpumask, 0644, wq_cpumask_show, wq_cpumask_store),
+ 	__ATTR(numa, 0644, wq_numa_show, wq_numa_store),
+ 	__ATTR_NULL,
+ };
+ 
+ static struct bus_type wq_subsys = {
+ 	.name				= "workqueue",
+ 	.dev_groups			= wq_sysfs_groups,
+ };
+ 
+ static ssize_t wq_unbound_cpumask_show(struct device *dev,
+ 		struct device_attribute *attr, char *buf)
+ {
+ 	int written;
+ 
+ 	written = scnprintf(buf, PAGE_SIZE, "%*pb\n",
+ 			    cpumask_pr_args(wq_unbound_cpumask));
+ 
+ 	return written;
+ }
+ 
+ static struct device_attribute wq_sysfs_cpumask_attr =
+ 	__ATTR(cpumask, 0444, wq_unbound_cpumask_show, NULL);
+ 
+ static int __init wq_sysfs_init(void)
+ {
+ 	int err;
+ 
+ 	err = subsys_virtual_register(&wq_subsys, NULL);
+ 	if (err)
+ 		return err;
+ 
+ 	return device_create_file(wq_subsys.dev_root, &wq_sysfs_cpumask_attr);
+ }
+ core_initcall(wq_sysfs_init);
+ 
+ static void wq_device_release(struct device *dev)
+ {
+ 	struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);
+ 
+ 	kfree(wq_dev);
+ }
+ 
+ /**
+  * workqueue_sysfs_register - make a workqueue visible in sysfs
+  * @wq: the workqueue to register
+  *
+  * Expose @wq in sysfs under /sys/bus/workqueue/devices.
+  * alloc_workqueue*() automatically calls this function if WQ_SYSFS is set
+  * which is the preferred method.
+  *
+  * Workqueue user should use this function directly iff it wants to apply
+  * workqueue_attrs before making the workqueue visible in sysfs; otherwise,
+  * apply_workqueue_attrs() may race against userland updating the
+  * attributes.
+  *
+  * Return: 0 on success, -errno on failure.
+  */
+ int workqueue_sysfs_register(struct workqueue_struct *wq)
+ {
+ 	struct wq_device *wq_dev;
+ 	int ret;
+ 
+ 	/*
+ 	 * Adjusting max_active or creating new pwqs by applyting
+ 	 * attributes breaks ordering guarantee.  Disallow exposing ordered
+ 	 * workqueues.
+ 	 */
+ 	if (WARN_ON(wq->flags & __WQ_ORDERED))
+ 		return -EINVAL;
+ 
+ 	wq->wq_dev = wq_dev = kzalloc(sizeof(*wq_dev), GFP_KERNEL);
+ 	if (!wq_dev)
+ 		return -ENOMEM;
+ 
+ 	wq_dev->wq = wq;
+ 	wq_dev->dev.bus = &wq_subsys;
+ 	wq_dev->dev.init_name = wq->name;
+ 	wq_dev->dev.release = wq_device_release;
+ 
+ 	/*
+ 	 * unbound_attrs are created separately.  Suppress uevent until
+ 	 * everything is ready.
+ 	 */
+ 	dev_set_uevent_suppress(&wq_dev->dev, true);
+ 
+ 	ret = device_register(&wq_dev->dev);
+ 	if (ret) {
+ 		kfree(wq_dev);
+ 		wq->wq_dev = NULL;
+ 		return ret;
+ 	}
+ 
+ 	if (wq->flags & WQ_UNBOUND) {
+ 		struct device_attribute *attr;
+ 
+ 		for (attr = wq_sysfs_unbound_attrs; attr->attr.name; attr++) {
+ 			ret = device_create_file(&wq_dev->dev, attr);
+ 			if (ret) {
+ 				device_unregister(&wq_dev->dev);
+ 				wq->wq_dev = NULL;
+ 				return ret;
+ 			}
+ 		}
+ 	}
+ 
+ 	dev_set_uevent_suppress(&wq_dev->dev, false);
+ 	kobject_uevent(&wq_dev->dev.kobj, KOBJ_ADD);
+ 	return 0;
+ }
+ 
+ /**
+  * workqueue_sysfs_unregister - undo workqueue_sysfs_register()
+  * @wq: the workqueue to unregister
+  *
+  * If @wq is registered to sysfs by workqueue_sysfs_register(), unregister.
+  */
+ static void workqueue_sysfs_unregister(struct workqueue_struct *wq)
+ {
+ 	struct wq_device *wq_dev = wq->wq_dev;
+ 
+ 	if (!wq->wq_dev)
+ 		return;
+ 
+ 	wq->wq_dev = NULL;
+ 	device_unregister(&wq_dev->dev);
+ }
+ #else	/* CONFIG_SYSFS */
+ static void workqueue_sysfs_unregister(struct workqueue_struct *wq)	{ }
+ #endif	/* CONFIG_SYSFS */
+ 
++>>>>>>> b05a79280b34 (workqueue: Create low-level unbound workqueues cpumask)
  static void __init wq_numa_init(void)
  {
  	cpumask_var_t *tbl;
@@@ -4960,12 -5115,11 +5302,15 @@@ static int __init init_workqueues(void
  	int std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };
  	int i, cpu;
  
 +	/* make sure we have enough bits for OFFQ pool ID */
 +	BUILD_BUG_ON((1LU << (BITS_PER_LONG - WORK_OFFQ_POOL_SHIFT)) <
 +		     WORK_CPU_END * NR_STD_WORKER_POOLS);
 +
  	WARN_ON(__alignof__(struct pool_workqueue) < __alignof__(long long));
  
+ 	BUG_ON(!alloc_cpumask_var(&wq_unbound_cpumask, GFP_KERNEL));
+ 	cpumask_copy(wq_unbound_cpumask, cpu_possible_mask);
+ 
  	pwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);
  
  	cpu_notifier(workqueue_cpu_up_callback, CPU_PRI_WORKQUEUE_UP);
* Unmerged path kernel/workqueue.c
