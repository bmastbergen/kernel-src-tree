netns: fix unbalanced spin_lock on error

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Nicolas Dichtel <nicolas.dichtel@6wind.com>
commit 0c58a2db91747c841d042b1d56615fb1eaf138c7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/0c58a2db.failed

Unlock was missing on error path.

Fixes: 95f38411df05 ("netns: use a spin_lock to protect nsid management")
	Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
	Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 0c58a2db91747c841d042b1d56615fb1eaf138c7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/net_namespace.c
diff --cc net/core/net_namespace.c
index 4cf6699528f0,d2f42da9479b..000000000000
--- a/net/core/net_namespace.c
+++ b/net/core/net_namespace.c
@@@ -397,6 -531,200 +397,203 @@@ static struct pernet_operations __net_i
  	.exit = net_ns_net_exit,
  };
  
++<<<<<<< HEAD
++=======
+ static struct nla_policy rtnl_net_policy[NETNSA_MAX + 1] = {
+ 	[NETNSA_NONE]		= { .type = NLA_UNSPEC },
+ 	[NETNSA_NSID]		= { .type = NLA_S32 },
+ 	[NETNSA_PID]		= { .type = NLA_U32 },
+ 	[NETNSA_FD]		= { .type = NLA_U32 },
+ };
+ 
+ static int rtnl_net_newid(struct sk_buff *skb, struct nlmsghdr *nlh)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct nlattr *tb[NETNSA_MAX + 1];
+ 	unsigned long flags;
+ 	struct net *peer;
+ 	int nsid, err;
+ 
+ 	err = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,
+ 			  rtnl_net_policy);
+ 	if (err < 0)
+ 		return err;
+ 	if (!tb[NETNSA_NSID])
+ 		return -EINVAL;
+ 	nsid = nla_get_s32(tb[NETNSA_NSID]);
+ 
+ 	if (tb[NETNSA_PID])
+ 		peer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));
+ 	else if (tb[NETNSA_FD])
+ 		peer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));
+ 	else
+ 		return -EINVAL;
+ 	if (IS_ERR(peer))
+ 		return PTR_ERR(peer);
+ 
+ 	spin_lock_irqsave(&nsid_lock, flags);
+ 	if (__peernet2id(net, peer) >= 0) {
+ 		spin_unlock_irqrestore(&nsid_lock, flags);
+ 		err = -EEXIST;
+ 		goto out;
+ 	}
+ 
+ 	err = alloc_netid(net, peer, nsid);
+ 	spin_unlock_irqrestore(&nsid_lock, flags);
+ 	if (err >= 0) {
+ 		rtnl_net_notifyid(net, RTM_NEWNSID, err);
+ 		err = 0;
+ 	}
+ out:
+ 	put_net(peer);
+ 	return err;
+ }
+ 
+ static int rtnl_net_get_size(void)
+ {
+ 	return NLMSG_ALIGN(sizeof(struct rtgenmsg))
+ 	       + nla_total_size(sizeof(s32)) /* NETNSA_NSID */
+ 	       ;
+ }
+ 
+ static int rtnl_net_fill(struct sk_buff *skb, u32 portid, u32 seq, int flags,
+ 			 int cmd, struct net *net, int nsid)
+ {
+ 	struct nlmsghdr *nlh;
+ 	struct rtgenmsg *rth;
+ 
+ 	nlh = nlmsg_put(skb, portid, seq, cmd, sizeof(*rth), flags);
+ 	if (!nlh)
+ 		return -EMSGSIZE;
+ 
+ 	rth = nlmsg_data(nlh);
+ 	rth->rtgen_family = AF_UNSPEC;
+ 
+ 	if (nla_put_s32(skb, NETNSA_NSID, nsid))
+ 		goto nla_put_failure;
+ 
+ 	nlmsg_end(skb, nlh);
+ 	return 0;
+ 
+ nla_put_failure:
+ 	nlmsg_cancel(skb, nlh);
+ 	return -EMSGSIZE;
+ }
+ 
+ static int rtnl_net_getid(struct sk_buff *skb, struct nlmsghdr *nlh)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct nlattr *tb[NETNSA_MAX + 1];
+ 	struct sk_buff *msg;
+ 	struct net *peer;
+ 	int err, id;
+ 
+ 	err = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,
+ 			  rtnl_net_policy);
+ 	if (err < 0)
+ 		return err;
+ 	if (tb[NETNSA_PID])
+ 		peer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));
+ 	else if (tb[NETNSA_FD])
+ 		peer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));
+ 	else
+ 		return -EINVAL;
+ 
+ 	if (IS_ERR(peer))
+ 		return PTR_ERR(peer);
+ 
+ 	msg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);
+ 	if (!msg) {
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	id = peernet2id(net, peer);
+ 	err = rtnl_net_fill(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq, 0,
+ 			    RTM_NEWNSID, net, id);
+ 	if (err < 0)
+ 		goto err_out;
+ 
+ 	err = rtnl_unicast(msg, net, NETLINK_CB(skb).portid);
+ 	goto out;
+ 
+ err_out:
+ 	nlmsg_free(msg);
+ out:
+ 	put_net(peer);
+ 	return err;
+ }
+ 
+ struct rtnl_net_dump_cb {
+ 	struct net *net;
+ 	struct sk_buff *skb;
+ 	struct netlink_callback *cb;
+ 	int idx;
+ 	int s_idx;
+ };
+ 
+ static int rtnl_net_dumpid_one(int id, void *peer, void *data)
+ {
+ 	struct rtnl_net_dump_cb *net_cb = (struct rtnl_net_dump_cb *)data;
+ 	int ret;
+ 
+ 	if (net_cb->idx < net_cb->s_idx)
+ 		goto cont;
+ 
+ 	ret = rtnl_net_fill(net_cb->skb, NETLINK_CB(net_cb->cb->skb).portid,
+ 			    net_cb->cb->nlh->nlmsg_seq, NLM_F_MULTI,
+ 			    RTM_NEWNSID, net_cb->net, id);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ cont:
+ 	net_cb->idx++;
+ 	return 0;
+ }
+ 
+ static int rtnl_net_dumpid(struct sk_buff *skb, struct netlink_callback *cb)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct rtnl_net_dump_cb net_cb = {
+ 		.net = net,
+ 		.skb = skb,
+ 		.cb = cb,
+ 		.idx = 0,
+ 		.s_idx = cb->args[0],
+ 	};
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&nsid_lock, flags);
+ 	idr_for_each(&net->netns_ids, rtnl_net_dumpid_one, &net_cb);
+ 	spin_unlock_irqrestore(&nsid_lock, flags);
+ 
+ 	cb->args[0] = net_cb.idx;
+ 	return skb->len;
+ }
+ 
+ static void rtnl_net_notifyid(struct net *net, int cmd, int id)
+ {
+ 	struct sk_buff *msg;
+ 	int err = -ENOMEM;
+ 
+ 	msg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);
+ 	if (!msg)
+ 		goto out;
+ 
+ 	err = rtnl_net_fill(msg, 0, 0, 0, cmd, net, id);
+ 	if (err < 0)
+ 		goto err_out;
+ 
+ 	rtnl_notify(msg, net, 0, RTNLGRP_NSID, NULL, 0);
+ 	return;
+ 
+ err_out:
+ 	nlmsg_free(msg);
+ out:
+ 	rtnl_set_sk_err(net, RTNLGRP_NSID, err);
+ }
+ 
++>>>>>>> 0c58a2db9174 (netns: fix unbalanced spin_lock on error)
  static int __init net_ns_init(void)
  {
  	struct net_generic *ng;
* Unmerged path net/core/net_namespace.c
