dm: requeue from blk-mq dm_mq_queue_rq() using BLK_MQ_RQ_QUEUE_BUSY

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Mike Snitzer <snitzer@redhat.com>
commit 45714fbed4556149d7f1730f5bae74f81d5e2cd5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/45714fbe.failed

Use BLK_MQ_RQ_QUEUE_BUSY to requeue a blk-mq request directly from the
DM blk-mq device's .queue_rq.  This cleans up the previous convoluted
handling of request requeueing that would return BLK_MQ_RQ_QUEUE_OK
(even though it wasn't) and then run blk_mq_requeue_request() followed
by blk_mq_kick_requeue_list().

Also, document that DM blk-mq ontop of old request_fn devices cannot
fail in clone_rq() since the clone request is preallocated as part of
the pdu.

	Reported-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 45714fbed4556149d7f1730f5bae74f81d5e2cd5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm.c
diff --cc drivers/md/dm.c
index 7270805de04c,1badfb250a18..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -2696,7 -2688,143 +2696,147 @@@ static int dm_init_request_based_queue(
  
  	elv_register_queue(md->queue);
  
++<<<<<<< HEAD
 +	return 1;
++=======
+ 	return 0;
+ }
+ 
+ static int dm_mq_init_request(void *data, struct request *rq,
+ 			      unsigned int hctx_idx, unsigned int request_idx,
+ 			      unsigned int numa_node)
+ {
+ 	struct mapped_device *md = data;
+ 	struct dm_rq_target_io *tio = blk_mq_rq_to_pdu(rq);
+ 
+ 	/*
+ 	 * Must initialize md member of tio, otherwise it won't
+ 	 * be available in dm_mq_queue_rq.
+ 	 */
+ 	tio->md = md;
+ 
+ 	return 0;
+ }
+ 
+ static int dm_mq_queue_rq(struct blk_mq_hw_ctx *hctx,
+ 			  const struct blk_mq_queue_data *bd)
+ {
+ 	struct request *rq = bd->rq;
+ 	struct dm_rq_target_io *tio = blk_mq_rq_to_pdu(rq);
+ 	struct mapped_device *md = tio->md;
+ 	int srcu_idx;
+ 	struct dm_table *map = dm_get_live_table(md, &srcu_idx);
+ 	struct dm_target *ti;
+ 	sector_t pos;
+ 
+ 	/* always use block 0 to find the target for flushes for now */
+ 	pos = 0;
+ 	if (!(rq->cmd_flags & REQ_FLUSH))
+ 		pos = blk_rq_pos(rq);
+ 
+ 	ti = dm_table_find_target(map, pos);
+ 	if (!dm_target_is_valid(ti)) {
+ 		dm_put_live_table(md, srcu_idx);
+ 		DMERR_LIMIT("request attempted access beyond the end of device");
+ 		/*
+ 		 * Must perform setup, that rq_completed() requires,
+ 		 * before returning BLK_MQ_RQ_QUEUE_ERROR
+ 		 */
+ 		dm_start_request(md, rq);
+ 		return BLK_MQ_RQ_QUEUE_ERROR;
+ 	}
+ 	dm_put_live_table(md, srcu_idx);
+ 
+ 	if (ti->type->busy && ti->type->busy(ti))
+ 		return BLK_MQ_RQ_QUEUE_BUSY;
+ 
+ 	dm_start_request(md, rq);
+ 
+ 	/* Init tio using md established in .init_request */
+ 	init_tio(tio, rq, md);
+ 
+ 	/*
+ 	 * Establish tio->ti before queuing work (map_tio_request)
+ 	 * or making direct call to map_request().
+ 	 */
+ 	tio->ti = ti;
+ 
+ 	/* Clone the request if underlying devices aren't blk-mq */
+ 	if (dm_table_get_type(map) == DM_TYPE_REQUEST_BASED) {
+ 		/* clone request is allocated at the end of the pdu */
+ 		tio->clone = (void *)blk_mq_rq_to_pdu(rq) + sizeof(struct dm_rq_target_io);
+ 		(void) clone_rq(rq, md, tio, GFP_ATOMIC);
+ 		queue_kthread_work(&md->kworker, &tio->work);
+ 	} else {
+ 		/* Direct call is fine since .queue_rq allows allocations */
+ 		if (map_request(tio, rq, md) == DM_MAPIO_REQUEUE) {
+ 			/* Undo dm_start_request() before requeuing */
+ 			rq_completed(md, rq_data_dir(rq), false);
+ 			return BLK_MQ_RQ_QUEUE_BUSY;
+ 		}
+ 	}
+ 
+ 	return BLK_MQ_RQ_QUEUE_OK;
+ }
+ 
+ static struct blk_mq_ops dm_mq_ops = {
+ 	.queue_rq = dm_mq_queue_rq,
+ 	.map_queue = blk_mq_map_queue,
+ 	.complete = dm_softirq_done,
+ 	.init_request = dm_mq_init_request,
+ };
+ 
+ static int dm_init_request_based_blk_mq_queue(struct mapped_device *md)
+ {
+ 	unsigned md_type = dm_get_md_type(md);
+ 	struct request_queue *q;
+ 	int err;
+ 
+ 	memset(&md->tag_set, 0, sizeof(md->tag_set));
+ 	md->tag_set.ops = &dm_mq_ops;
+ 	md->tag_set.queue_depth = BLKDEV_MAX_RQ;
+ 	md->tag_set.numa_node = NUMA_NO_NODE;
+ 	md->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+ 	md->tag_set.nr_hw_queues = 1;
+ 	if (md_type == DM_TYPE_REQUEST_BASED) {
+ 		/* make the memory for non-blk-mq clone part of the pdu */
+ 		md->tag_set.cmd_size = sizeof(struct dm_rq_target_io) + sizeof(struct request);
+ 	} else
+ 		md->tag_set.cmd_size = sizeof(struct dm_rq_target_io);
+ 	md->tag_set.driver_data = md;
+ 
+ 	err = blk_mq_alloc_tag_set(&md->tag_set);
+ 	if (err)
+ 		return err;
+ 
+ 	q = blk_mq_init_allocated_queue(&md->tag_set, md->queue);
+ 	if (IS_ERR(q)) {
+ 		err = PTR_ERR(q);
+ 		goto out_tag_set;
+ 	}
+ 	md->queue = q;
+ 	dm_init_md_queue(md);
+ 
+ 	/* backfill 'mq' sysfs registration normally done in blk_register_queue */
+ 	blk_mq_register_disk(md->disk);
+ 
+ 	if (md_type == DM_TYPE_REQUEST_BASED)
+ 		init_rq_based_worker_thread(md);
+ 
+ 	return 0;
+ 
+ out_tag_set:
+ 	blk_mq_free_tag_set(&md->tag_set);
+ 	return err;
+ }
+ 
+ static unsigned filter_md_type(unsigned type, struct mapped_device *md)
+ {
+ 	if (type == DM_TYPE_BIO_BASED)
+ 		return type;
+ 
+ 	return !md->use_blk_mq ? DM_TYPE_REQUEST_BASED : DM_TYPE_MQ_REQUEST_BASED;
++>>>>>>> 45714fbed455 (dm: requeue from blk-mq dm_mq_queue_rq() using BLK_MQ_RQ_QUEUE_BUSY)
  }
  
  /*
* Unmerged path drivers/md/dm.c
