crypto: seqiv - Move IV seeding into init function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [crypto] seqiv: Move IV seeding into init function (Herbert Xu) [1229738]
Rebuild_FUZZ: 87.91%
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit b7dcfab4a7cdc3103c7560cd2386036266b2740e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/b7dcfab4.failed

We currently do the IV seeding on the first givencrypt call in
order to conserve entropy.  However, this does not work with
DRBG which cannot be called from interrupt context.  In fact,
with DRBG we don't need to conserve entropy anyway.  So this
patch moves the seeding into the init function.

	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit b7dcfab4a7cdc3103c7560cd2386036266b2740e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	crypto/seqiv.c
diff --cc crypto/seqiv.c
index f2cba4ed6f25,42e4ee532d35..000000000000
--- a/crypto/seqiv.c
+++ b/crypto/seqiv.c
@@@ -186,50 -273,205 +186,252 @@@ static int seqiv_aead_givencrypt(struc
  	return err;
  }
  
++<<<<<<< HEAD
 +static int seqiv_givencrypt_first(struct skcipher_givcrypt_request *req)
 +{
 +	struct crypto_ablkcipher *geniv = skcipher_givcrypt_reqtfm(req);
 +	struct seqiv_ctx *ctx = crypto_ablkcipher_ctx(geniv);
 +	int err = 0;
 +
 +	spin_lock_bh(&ctx->lock);
 +	if (crypto_ablkcipher_crt(geniv)->givencrypt != seqiv_givencrypt_first)
 +		goto unlock;
 +
 +	crypto_ablkcipher_crt(geniv)->givencrypt = seqiv_givencrypt;
 +	err = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,
 +				   crypto_ablkcipher_ivsize(geniv));
 +
 +unlock:
 +	spin_unlock_bh(&ctx->lock);
 +
 +	if (err)
 +		return err;
 +
 +	return seqiv_givencrypt(req);
 +}
 +
 +static int seqiv_aead_givencrypt_first(struct aead_givcrypt_request *req)
 +{
 +	struct crypto_aead *geniv = aead_givcrypt_reqtfm(req);
 +	struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
 +	int err = 0;
 +
 +	spin_lock_bh(&ctx->lock);
 +	if (crypto_aead_crt(geniv)->givencrypt != seqiv_aead_givencrypt_first)
 +		goto unlock;
 +
 +	crypto_aead_crt(geniv)->givencrypt = seqiv_aead_givencrypt;
 +	err = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,
 +				   crypto_aead_ivsize(geniv));
 +
 +unlock:
 +	spin_unlock_bh(&ctx->lock);
 +
 +	if (err)
 +		return err;
 +
 +	return seqiv_aead_givencrypt(req);
++=======
+ static int seqniv_aead_encrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *geniv = crypto_aead_reqtfm(req);
+ 	struct seqiv_aead_ctx *ctx = crypto_aead_ctx(geniv);
+ 	struct seqniv_request_ctx *rctx = aead_request_ctx(req);
+ 	struct aead_request *subreq = &rctx->subreq;
+ 	struct scatterlist *dst;
+ 	crypto_completion_t compl;
+ 	void *data;
+ 	unsigned int ivsize = 8;
+ 	u8 buf[20] __attribute__ ((aligned(__alignof__(u32))));
+ 	int err;
+ 
+ 	if (req->cryptlen < ivsize)
+ 		return -EINVAL;
+ 
+ 	/* ESP AD is at most 12 bytes (ESN). */
+ 	if (req->assoclen > 12)
+ 		return -EINVAL;
+ 
+ 	aead_request_set_tfm(subreq, ctx->geniv.child);
+ 
+ 	compl = seqniv_aead_encrypt_complete;
+ 	data = req;
+ 
+ 	if (req->src != req->dst) {
+ 		struct blkcipher_desc desc = {
+ 			.tfm = ctx->null,
+ 		};
+ 
+ 		err = crypto_blkcipher_encrypt(&desc, req->dst, req->src,
+ 					       req->assoclen + req->cryptlen);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ 	dst = scatterwalk_ffwd(rctx->dst, req->dst, ivsize);
+ 
+ 	aead_request_set_callback(subreq, req->base.flags, compl, data);
+ 	aead_request_set_crypt(subreq, dst, dst,
+ 			       req->cryptlen - ivsize, req->iv);
+ 	aead_request_set_ad(subreq, req->assoclen);
+ 
+ 	memcpy(buf, req->iv, ivsize);
+ 	crypto_xor(buf, ctx->salt, ivsize);
+ 	memcpy(req->iv, buf, ivsize);
+ 
+ 	/* Swap order of IV and ESP AD for ICV generation. */
+ 	scatterwalk_map_and_copy(buf + ivsize, req->dst, 0, req->assoclen, 0);
+ 	scatterwalk_map_and_copy(buf, req->dst, 0, req->assoclen + ivsize, 1);
+ 
+ 	err = crypto_aead_encrypt(subreq);
+ 	seqniv_aead_encrypt_complete2(req, err);
+ 	return err;
+ }
+ 
+ static int seqiv_aead_encrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *geniv = crypto_aead_reqtfm(req);
+ 	struct seqiv_aead_ctx *ctx = crypto_aead_ctx(geniv);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 	crypto_completion_t compl;
+ 	void *data;
+ 	u8 *info;
+ 	unsigned int ivsize = 8;
+ 	int err;
+ 
+ 	if (req->cryptlen < ivsize)
+ 		return -EINVAL;
+ 
+ 	aead_request_set_tfm(subreq, ctx->geniv.child);
+ 
+ 	compl = req->base.complete;
+ 	data = req->base.data;
+ 	info = req->iv;
+ 
+ 	if (req->src != req->dst) {
+ 		struct blkcipher_desc desc = {
+ 			.tfm = ctx->null,
+ 		};
+ 
+ 		err = crypto_blkcipher_encrypt(&desc, req->dst, req->src,
+ 					       req->assoclen + req->cryptlen);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ 	if (unlikely(!IS_ALIGNED((unsigned long)info,
+ 				 crypto_aead_alignmask(geniv) + 1))) {
+ 		info = kmalloc(ivsize, req->base.flags &
+ 				       CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL:
+ 								  GFP_ATOMIC);
+ 		if (!info)
+ 			return -ENOMEM;
+ 
+ 		memcpy(info, req->iv, ivsize);
+ 		compl = seqiv_aead_encrypt_complete;
+ 		data = req;
+ 	}
+ 
+ 	aead_request_set_callback(subreq, req->base.flags, compl, data);
+ 	aead_request_set_crypt(subreq, req->dst, req->dst,
+ 			       req->cryptlen - ivsize, info);
+ 	aead_request_set_ad(subreq, req->assoclen + ivsize);
+ 
+ 	crypto_xor(info, ctx->salt, ivsize);
+ 	scatterwalk_map_and_copy(info, req->dst, req->assoclen, ivsize, 1);
+ 
+ 	err = crypto_aead_encrypt(subreq);
+ 	if (unlikely(info != req->iv))
+ 		seqiv_aead_encrypt_complete2(req, err);
+ 	return err;
+ }
+ 
+ static int seqniv_aead_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *geniv = crypto_aead_reqtfm(req);
+ 	struct seqiv_aead_ctx *ctx = crypto_aead_ctx(geniv);
+ 	struct seqniv_request_ctx *rctx = aead_request_ctx(req);
+ 	struct aead_request *subreq = &rctx->subreq;
+ 	struct scatterlist *dst;
+ 	crypto_completion_t compl;
+ 	void *data;
+ 	unsigned int ivsize = 8;
+ 	u8 buf[20];
+ 	int err;
+ 
+ 	if (req->cryptlen < ivsize + crypto_aead_authsize(geniv))
+ 		return -EINVAL;
+ 
+ 	aead_request_set_tfm(subreq, ctx->geniv.child);
+ 
+ 	compl = req->base.complete;
+ 	data = req->base.data;
+ 
+ 	if (req->assoclen > 12)
+ 		return -EINVAL;
+ 	else if (req->assoclen > 8) {
+ 		compl = seqniv_aead_decrypt_complete;
+ 		data = req;
+ 	}
+ 
+ 	if (req->src != req->dst) {
+ 		struct blkcipher_desc desc = {
+ 			.tfm = ctx->null,
+ 		};
+ 
+ 		err = crypto_blkcipher_encrypt(&desc, req->dst, req->src,
+ 					       req->assoclen + req->cryptlen);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ 	/* Move ESP AD forward for ICV generation. */
+ 	scatterwalk_map_and_copy(buf, req->dst, 0, req->assoclen + ivsize, 0);
+ 	memcpy(req->iv, buf + req->assoclen, ivsize);
+ 	scatterwalk_map_and_copy(buf, req->dst, ivsize, req->assoclen, 1);
+ 
+ 	dst = scatterwalk_ffwd(rctx->dst, req->dst, ivsize);
+ 
+ 	aead_request_set_callback(subreq, req->base.flags, compl, data);
+ 	aead_request_set_crypt(subreq, dst, dst,
+ 			       req->cryptlen - ivsize, req->iv);
+ 	aead_request_set_ad(subreq, req->assoclen);
+ 
+ 	err = crypto_aead_decrypt(subreq);
+ 	if (req->assoclen > 8)
+ 		seqniv_aead_decrypt_complete2(req, err);
+ 	return err;
+ }
+ 
+ static int seqiv_aead_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *geniv = crypto_aead_reqtfm(req);
+ 	struct seqiv_aead_ctx *ctx = crypto_aead_ctx(geniv);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 	crypto_completion_t compl;
+ 	void *data;
+ 	unsigned int ivsize = 8;
+ 
+ 	if (req->cryptlen < ivsize + crypto_aead_authsize(geniv))
+ 		return -EINVAL;
+ 
+ 	aead_request_set_tfm(subreq, ctx->geniv.child);
+ 
+ 	compl = req->base.complete;
+ 	data = req->base.data;
+ 
+ 	aead_request_set_callback(subreq, req->base.flags, compl, data);
+ 	aead_request_set_crypt(subreq, req->src, req->dst,
+ 			       req->cryptlen - ivsize, req->iv);
+ 	aead_request_set_ad(subreq, req->assoclen + ivsize);
+ 
+ 	scatterwalk_map_and_copy(req->iv, req->src, req->assoclen, ivsize, 0);
+ 	if (req->src != req->dst)
+ 		scatterwalk_map_and_copy(req->iv, req->dst,
+ 					 req->assoclen, ivsize, 1);
+ 
+ 	return crypto_aead_decrypt(subreq);
++>>>>>>> b7dcfab4a7cd (crypto: seqiv - Move IV seeding into init function)
  }
  
  static int seqiv_init(struct crypto_tfm *tfm)
@@@ -241,67 -483,197 +443,176 @@@
  
  	tfm->crt_ablkcipher.reqsize = sizeof(struct ablkcipher_request);
  
- 	return skcipher_geniv_init(tfm);
+ 	return crypto_rng_get_bytes(crypto_default_rng, ctx->salt,
+ 				    crypto_ablkcipher_ivsize(geniv)) ?:
+ 	       skcipher_geniv_init(tfm);
  }
  
 -static int seqiv_old_aead_init(struct crypto_tfm *tfm)
 +static int seqiv_aead_init(struct crypto_tfm *tfm)
  {
  	struct crypto_aead *geniv = __crypto_aead_cast(tfm);
  	struct seqiv_ctx *ctx = crypto_aead_ctx(geniv);
  
  	spin_lock_init(&ctx->lock);
  
 -	crypto_aead_set_reqsize(__crypto_aead_cast(tfm),
 -				sizeof(struct aead_request));
 +	tfm->crt_aead.reqsize = sizeof(struct aead_request);
  
- 	return aead_geniv_init(tfm);
+ 	return crypto_rng_get_bytes(crypto_default_rng, ctx->salt,
+ 				    crypto_aead_ivsize(geniv)) ?:
+ 	       aead_geniv_init(tfm);
  }
  
 -static int seqiv_aead_init_common(struct crypto_tfm *tfm, unsigned int reqsize)
 -{
 -	struct crypto_aead *geniv = __crypto_aead_cast(tfm);
 -	struct seqiv_aead_ctx *ctx = crypto_aead_ctx(geniv);
 -	int err;
 +static struct crypto_template seqiv_tmpl;
  
++<<<<<<< HEAD
 +static struct crypto_instance *seqiv_ablkcipher_alloc(struct rtattr **tb)
++=======
+ 	spin_lock_init(&ctx->geniv.lock);
+ 
+ 	crypto_aead_set_reqsize(geniv, sizeof(struct aead_request));
+ 
+ 	err = crypto_rng_get_bytes(crypto_default_rng, ctx->salt,
+ 				   crypto_aead_ivsize(geniv));
+ 	if (err)
+ 		goto out;
+ 
+ 	ctx->null = crypto_get_default_null_skcipher();
+ 	err = PTR_ERR(ctx->null);
+ 	if (IS_ERR(ctx->null))
+ 		goto out;
+ 
+ 	err = aead_geniv_init(tfm);
+ 	if (err)
+ 		goto drop_null;
+ 
+ 	ctx->geniv.child = geniv->child;
+ 	geniv->child = geniv;
+ 
+ out:
+ 	return err;
+ 
+ drop_null:
+ 	crypto_put_default_null_skcipher();
+ 	goto out;
+ }
+ 
+ static int seqiv_aead_init(struct crypto_tfm *tfm)
+ {
+ 	return seqiv_aead_init_common(tfm, sizeof(struct aead_request));
+ }
+ 
+ static int seqniv_aead_init(struct crypto_tfm *tfm)
+ {
+ 	return seqiv_aead_init_common(tfm, sizeof(struct seqniv_request_ctx));
+ }
+ 
+ static void seqiv_aead_exit(struct crypto_tfm *tfm)
+ {
+ 	struct seqiv_aead_ctx *ctx = crypto_tfm_ctx(tfm);
+ 
+ 	crypto_free_aead(ctx->geniv.child);
+ 	crypto_put_default_null_skcipher();
+ }
+ 
+ static int seqiv_ablkcipher_create(struct crypto_template *tmpl,
+ 				   struct rtattr **tb)
++>>>>>>> b7dcfab4a7cd (crypto: seqiv - Move IV seeding into init function)
  {
  	struct crypto_instance *inst;
 -	int err;
  
 -	inst = skcipher_geniv_alloc(tmpl, tb, 0, 0);
 +	inst = skcipher_geniv_alloc(&seqiv_tmpl, tb, 0, 0);
  
  	if (IS_ERR(inst))
 -		return PTR_ERR(inst);
 -
 -	err = -EINVAL;
 -	if (inst->alg.cra_ablkcipher.ivsize < sizeof(u64))
 -		goto free_inst;
 +		goto out;
  
- 	inst->alg.cra_ablkcipher.givencrypt = seqiv_givencrypt_first;
+ 	inst->alg.cra_ablkcipher.givencrypt = seqiv_givencrypt;
  
  	inst->alg.cra_init = seqiv_init;
  	inst->alg.cra_exit = skcipher_geniv_exit;
  
  	inst->alg.cra_ctxsize += inst->alg.cra_ablkcipher.ivsize;
 -	inst->alg.cra_ctxsize += sizeof(struct seqiv_ctx);
 -
 -	inst->alg.cra_alignmask |= __alignof__(u32) - 1;
 -
 -	err = crypto_register_instance(tmpl, inst);
 -	if (err)
 -		goto free_inst;
  
  out:
 -	return err;
 -
 -free_inst:
 -	skcipher_geniv_free(inst);
 -	goto out;
 +	return inst;
  }
  
 -static int seqiv_old_aead_create(struct crypto_template *tmpl,
 -				 struct aead_instance *aead)
 +static struct crypto_instance *seqiv_aead_alloc(struct rtattr **tb)
  {
 -	struct crypto_instance *inst = aead_crypto_instance(aead);
 -	int err = -EINVAL;
 +	struct crypto_instance *inst;
  
 -	if (inst->alg.cra_aead.ivsize < sizeof(u64))
 -		goto free_inst;
 +	inst = aead_geniv_alloc(&seqiv_tmpl, tb, 0, 0);
 +
 +	if (IS_ERR(inst))
 +		goto out;
  
- 	inst->alg.cra_aead.givencrypt = seqiv_aead_givencrypt_first;
+ 	inst->alg.cra_aead.givencrypt = seqiv_aead_givencrypt;
  
 -	inst->alg.cra_init = seqiv_old_aead_init;
 +	inst->alg.cra_init = seqiv_aead_init;
  	inst->alg.cra_exit = aead_geniv_exit;
  
  	inst->alg.cra_ctxsize = inst->alg.cra_aead.ivsize;
 -	inst->alg.cra_ctxsize += sizeof(struct seqiv_ctx);
 -
 -	err = crypto_register_instance(tmpl, inst);
 -	if (err)
 -		goto free_inst;
  
  out:
 -	return err;
 -
 -free_inst:
 -	aead_geniv_free(aead);
 -	goto out;
 +	return inst;
  }
  
++<<<<<<< HEAD
 +static struct crypto_instance *seqiv_alloc(struct rtattr **tb)
++=======
+ static int seqiv_aead_create(struct crypto_template *tmpl, struct rtattr **tb)
+ {
+ 	struct aead_instance *inst;
+ 	struct crypto_aead_spawn *spawn;
+ 	struct aead_alg *alg;
+ 	int err;
+ 
+ 	inst = aead_geniv_alloc(tmpl, tb, 0, 0);
+ 
+ 	if (IS_ERR(inst))
+ 		return PTR_ERR(inst);
+ 
+ 	inst->alg.base.cra_alignmask |= __alignof__(u32) - 1;
+ 
+ 	if (inst->alg.base.cra_aead.encrypt)
+ 		return seqiv_old_aead_create(tmpl, inst);
+ 
+ 	spawn = aead_instance_ctx(inst);
+ 	alg = crypto_spawn_aead_alg(spawn);
+ 
+ 	if (alg->base.cra_aead.encrypt)
+ 		goto done;
+ 
+ 	err = -EINVAL;
+ 	if (inst->alg.ivsize != sizeof(u64))
+ 		goto free_inst;
+ 
+ 	inst->alg.encrypt = seqiv_aead_encrypt;
+ 	inst->alg.decrypt = seqiv_aead_decrypt;
+ 
+ 	inst->alg.base.cra_init = seqiv_aead_init;
+ 	inst->alg.base.cra_exit = seqiv_aead_exit;
+ 
+ 	inst->alg.base.cra_ctxsize = sizeof(struct seqiv_aead_ctx);
+ 	inst->alg.base.cra_ctxsize += inst->alg.base.cra_aead.ivsize;
+ 
+ done:
+ 	err = aead_register_instance(tmpl, inst);
+ 	if (err)
+ 		goto free_inst;
+ 
+ out:
+ 	return err;
+ 
+ free_inst:
+ 	aead_geniv_free(inst);
+ 	goto out;
+ }
+ 
+ static int seqiv_create(struct crypto_template *tmpl, struct rtattr **tb)
++>>>>>>> b7dcfab4a7cd (crypto: seqiv - Move IV seeding into init function)
  {
  	struct crypto_attr_type *algt;
 +	struct crypto_instance *inst;
  	int err;
  
  	algt = crypto_get_attr_type(tb);
@@@ -320,12 -692,55 +631,39 @@@
  	if (IS_ERR(inst))
  		goto put_rng;
  
++<<<<<<< HEAD
 +	inst->alg.cra_alignmask |= __alignof__(u32) - 1;
 +	inst->alg.cra_ctxsize += sizeof(struct seqiv_ctx);
++=======
+ 	spawn = aead_instance_ctx(inst);
+ 	alg = crypto_spawn_aead_alg(spawn);
+ 
+ 	if (alg->base.cra_aead.encrypt)
+ 		goto done;
+ 
+ 	err = -EINVAL;
+ 	if (inst->alg.ivsize != sizeof(u64))
+ 		goto free_inst;
+ 
+ 	inst->alg.encrypt = seqniv_aead_encrypt;
+ 	inst->alg.decrypt = seqniv_aead_decrypt;
+ 
+ 	inst->alg.base.cra_init = seqniv_aead_init;
+ 	inst->alg.base.cra_exit = seqiv_aead_exit;
+ 
+ 	inst->alg.base.cra_alignmask |= __alignof__(u32) - 1;
+ 	inst->alg.base.cra_ctxsize = sizeof(struct seqiv_aead_ctx);
+ 	inst->alg.base.cra_ctxsize += inst->alg.ivsize;
+ 
+ done:
+ 	err = aead_register_instance(tmpl, inst);
+ 	if (err)
+ 		goto free_inst;
++>>>>>>> b7dcfab4a7cd (crypto: seqiv - Move IV seeding into init function)
  
  out:
 -	return err;
 +	return inst;
  
 -free_inst:
 -	aead_geniv_free(inst);
  put_rng:
  	crypto_put_default_rng();
  	goto out;
* Unmerged path crypto/seqiv.c
