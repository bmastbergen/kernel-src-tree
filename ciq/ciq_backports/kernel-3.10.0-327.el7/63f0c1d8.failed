perf/x86/intel: Track number of events that use the LBR callstack

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [perf] x86/intel: Track number of events that use the LBR callstack (Jiri Olsa) [1222189]
Rebuild_FUZZ: 96.00%
commit-author Yan, Zheng <zheng.z.yan@intel.com>
commit 63f0c1d84196b712fe9de99a8514486ab416d517
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/63f0c1d8.failed

When enabling/disabling an event, check if the event uses the LBR
callstack feature, adjust the LBR callstack usage count accordingly.
Later patch will use the usage count to decide if LBR stack should
be saved/restored.

	Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
	Signed-off-by: Kan Liang <kan.liang@intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: eranian@google.com
	Cc: jolsa@redhat.com
Link: http://lkml.kernel.org/r/1415156173-10035-9-git-send-email-kan.liang@intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 63f0c1d84196b712fe9de99a8514486ab416d517)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event_intel_lbr.c
diff --cc arch/x86/kernel/cpu/perf_event_intel_lbr.c
index 92348471a707,ac8e54200934..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel_lbr.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_lbr.c
@@@ -180,9 -180,40 +180,46 @@@ void intel_pmu_lbr_reset(void
  		intel_pmu_lbr_reset_64();
  }
  
++<<<<<<< HEAD
 +void intel_pmu_lbr_enable(struct perf_event *event)
 +{
 +	struct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);
++=======
+ void intel_pmu_lbr_sched_task(struct perf_event_context *ctx, bool sched_in)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 
+ 	if (!x86_pmu.lbr_nr)
+ 		return;
+ 
+ 	/*
+ 	 * When sampling the branck stack in system-wide, it may be
+ 	 * necessary to flush the stack on context switch. This happens
+ 	 * when the branch stack does not tag its entries with the pid
+ 	 * of the current task. Otherwise it becomes impossible to
+ 	 * associate a branch entry with a task. This ambiguity is more
+ 	 * likely to appear when the branch stack supports priv level
+ 	 * filtering and the user sets it to monitor only at the user
+ 	 * level (which could be a useful measurement in system-wide
+ 	 * mode). In that case, the risk is high of having a branch
+ 	 * stack with branch from multiple tasks.
+  	 */
+ 	if (sched_in) {
+ 		intel_pmu_lbr_reset();
+ 		cpuc->lbr_context = ctx;
+ 	}
+ }
+ 
+ static inline bool branch_user_callstack(unsigned br_sel)
+ {
+ 	return (br_sel & X86_BR_USER) && (br_sel & X86_BR_CALL_STACK);
+ }
+ 
+ void intel_pmu_lbr_enable(struct perf_event *event)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 	struct x86_perf_task_context *task_ctx;
++>>>>>>> 63f0c1d84196 (perf/x86/intel: Track number of events that use the LBR callstack)
  
  	if (!x86_pmu.lbr_nr)
  		return;
@@@ -197,18 -228,33 +234,35 @@@
  	}
  	cpuc->br_sel = event->hw.branch_reg.reg;
  
+ 	if (branch_user_callstack(cpuc->br_sel) && event->ctx &&
+ 					event->ctx->task_ctx_data) {
+ 		task_ctx = event->ctx->task_ctx_data;
+ 		task_ctx->lbr_callstack_users++;
+ 	}
+ 
  	cpuc->lbr_users++;
 -	perf_sched_cb_inc(event->ctx->pmu);
  }
  
  void intel_pmu_lbr_disable(struct perf_event *event)
  {
++<<<<<<< HEAD
 +	struct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);
++=======
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 	struct x86_perf_task_context *task_ctx;
++>>>>>>> 63f0c1d84196 (perf/x86/intel: Track number of events that use the LBR callstack)
  
  	if (!x86_pmu.lbr_nr)
  		return;
  
+ 	if (branch_user_callstack(cpuc->br_sel) && event->ctx &&
+ 					event->ctx->task_ctx_data) {
+ 		task_ctx = event->ctx->task_ctx_data;
+ 		task_ctx->lbr_callstack_users--;
+ 	}
+ 
  	cpuc->lbr_users--;
  	WARN_ON_ONCE(cpuc->lbr_users < 0);
 -	perf_sched_cb_dec(event->ctx->pmu);
  
  	if (cpuc->enabled && !cpuc->lbr_users) {
  		__intel_pmu_lbr_disable();
* Unmerged path arch/x86/kernel/cpu/perf_event_intel_lbr.c
