Btrfs, replace: write raid56 parity into the replace target device

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Miao Xie <miaox@cn.fujitsu.com>
commit 7603597690147a16b5cc77047d7570fa22a22673
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/76035976.failed

This function reused the code of parity scrub, and we just write
the right parity or corrected parity into the target device before
the parity scrub end.

	Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
(cherry picked from commit 7603597690147a16b5cc77047d7570fa22a22673)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/btrfs/raid56.c
#	fs/btrfs/scrub.c
diff --cc fs/btrfs/raid56.c
index 6c2905ba2744,5ece565bc5f0..000000000000
--- a/fs/btrfs/raid56.c
+++ b/fs/btrfs/raid56.c
@@@ -2102,3 -2206,483 +2102,486 @@@ static void read_rebuild_work(struct bt
  	rbio = container_of(work, struct btrfs_raid_bio, work);
  	__raid56_parity_recover(rbio);
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * The following code is used to scrub/replace the parity stripe
+  *
+  * Note: We need make sure all the pages that add into the scrub/replace
+  * raid bio are correct and not be changed during the scrub/replace. That
+  * is those pages just hold metadata or file data with checksum.
+  */
+ 
+ struct btrfs_raid_bio *
+ raid56_parity_alloc_scrub_rbio(struct btrfs_root *root, struct bio *bio,
+ 			       struct btrfs_bio *bbio, u64 *raid_map,
+ 			       u64 stripe_len, struct btrfs_device *scrub_dev,
+ 			       unsigned long *dbitmap, int stripe_nsectors)
+ {
+ 	struct btrfs_raid_bio *rbio;
+ 	int i;
+ 
+ 	rbio = alloc_rbio(root, bbio, raid_map, stripe_len);
+ 	if (IS_ERR(rbio))
+ 		return NULL;
+ 	bio_list_add(&rbio->bio_list, bio);
+ 	/*
+ 	 * This is a special bio which is used to hold the completion handler
+ 	 * and make the scrub rbio is similar to the other types
+ 	 */
+ 	ASSERT(!bio->bi_iter.bi_size);
+ 	rbio->operation = BTRFS_RBIO_PARITY_SCRUB;
+ 
+ 	for (i = 0; i < rbio->real_stripes; i++) {
+ 		if (bbio->stripes[i].dev == scrub_dev) {
+ 			rbio->scrubp = i;
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* Now we just support the sectorsize equals to page size */
+ 	ASSERT(root->sectorsize == PAGE_SIZE);
+ 	ASSERT(rbio->stripe_npages == stripe_nsectors);
+ 	bitmap_copy(rbio->dbitmap, dbitmap, stripe_nsectors);
+ 
+ 	return rbio;
+ }
+ 
+ void raid56_parity_add_scrub_pages(struct btrfs_raid_bio *rbio,
+ 				   struct page *page, u64 logical)
+ {
+ 	int stripe_offset;
+ 	int index;
+ 
+ 	ASSERT(logical >= rbio->raid_map[0]);
+ 	ASSERT(logical + PAGE_SIZE <= rbio->raid_map[0] +
+ 				rbio->stripe_len * rbio->nr_data);
+ 	stripe_offset = (int)(logical - rbio->raid_map[0]);
+ 	index = stripe_offset >> PAGE_CACHE_SHIFT;
+ 	rbio->bio_pages[index] = page;
+ }
+ 
+ /*
+  * We just scrub the parity that we have correct data on the same horizontal,
+  * so we needn't allocate all pages for all the stripes.
+  */
+ static int alloc_rbio_essential_pages(struct btrfs_raid_bio *rbio)
+ {
+ 	int i;
+ 	int bit;
+ 	int index;
+ 	struct page *page;
+ 
+ 	for_each_set_bit(bit, rbio->dbitmap, rbio->stripe_npages) {
+ 		for (i = 0; i < rbio->real_stripes; i++) {
+ 			index = i * rbio->stripe_npages + bit;
+ 			if (rbio->stripe_pages[index])
+ 				continue;
+ 
+ 			page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
+ 			if (!page)
+ 				return -ENOMEM;
+ 			rbio->stripe_pages[index] = page;
+ 			ClearPageUptodate(page);
+ 		}
+ 	}
+ 	return 0;
+ }
+ 
+ /*
+  * end io function used by finish_rmw.  When we finally
+  * get here, we've written a full stripe
+  */
+ static void raid_write_parity_end_io(struct bio *bio, int err)
+ {
+ 	struct btrfs_raid_bio *rbio = bio->bi_private;
+ 
+ 	if (err)
+ 		fail_bio_stripe(rbio, bio);
+ 
+ 	bio_put(bio);
+ 
+ 	if (!atomic_dec_and_test(&rbio->stripes_pending))
+ 		return;
+ 
+ 	err = 0;
+ 
+ 	if (atomic_read(&rbio->error))
+ 		err = -EIO;
+ 
+ 	rbio_orig_end_io(rbio, err, 0);
+ }
+ 
+ static noinline void finish_parity_scrub(struct btrfs_raid_bio *rbio,
+ 					 int need_check)
+ {
+ 	struct btrfs_bio *bbio = rbio->bbio;
+ 	void *pointers[rbio->real_stripes];
+ 	DECLARE_BITMAP(pbitmap, rbio->stripe_npages);
+ 	int nr_data = rbio->nr_data;
+ 	int stripe;
+ 	int pagenr;
+ 	int p_stripe = -1;
+ 	int q_stripe = -1;
+ 	struct page *p_page = NULL;
+ 	struct page *q_page = NULL;
+ 	struct bio_list bio_list;
+ 	struct bio *bio;
+ 	int is_replace = 0;
+ 	int ret;
+ 
+ 	bio_list_init(&bio_list);
+ 
+ 	if (rbio->real_stripes - rbio->nr_data == 1) {
+ 		p_stripe = rbio->real_stripes - 1;
+ 	} else if (rbio->real_stripes - rbio->nr_data == 2) {
+ 		p_stripe = rbio->real_stripes - 2;
+ 		q_stripe = rbio->real_stripes - 1;
+ 	} else {
+ 		BUG();
+ 	}
+ 
+ 	if (bbio->num_tgtdevs && bbio->tgtdev_map[rbio->scrubp]) {
+ 		is_replace = 1;
+ 		bitmap_copy(pbitmap, rbio->dbitmap, rbio->stripe_npages);
+ 	}
+ 
+ 	/*
+ 	 * Because the higher layers(scrubber) are unlikely to
+ 	 * use this area of the disk again soon, so don't cache
+ 	 * it.
+ 	 */
+ 	clear_bit(RBIO_CACHE_READY_BIT, &rbio->flags);
+ 
+ 	if (!need_check)
+ 		goto writeback;
+ 
+ 	p_page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
+ 	if (!p_page)
+ 		goto cleanup;
+ 	SetPageUptodate(p_page);
+ 
+ 	if (q_stripe != -1) {
+ 		q_page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
+ 		if (!q_page) {
+ 			__free_page(p_page);
+ 			goto cleanup;
+ 		}
+ 		SetPageUptodate(q_page);
+ 	}
+ 
+ 	atomic_set(&rbio->error, 0);
+ 
+ 	for_each_set_bit(pagenr, rbio->dbitmap, rbio->stripe_npages) {
+ 		struct page *p;
+ 		void *parity;
+ 		/* first collect one page from each data stripe */
+ 		for (stripe = 0; stripe < nr_data; stripe++) {
+ 			p = page_in_rbio(rbio, stripe, pagenr, 0);
+ 			pointers[stripe] = kmap(p);
+ 		}
+ 
+ 		/* then add the parity stripe */
+ 		pointers[stripe++] = kmap(p_page);
+ 
+ 		if (q_stripe != -1) {
+ 
+ 			/*
+ 			 * raid6, add the qstripe and call the
+ 			 * library function to fill in our p/q
+ 			 */
+ 			pointers[stripe++] = kmap(q_page);
+ 
+ 			raid6_call.gen_syndrome(rbio->real_stripes, PAGE_SIZE,
+ 						pointers);
+ 		} else {
+ 			/* raid5 */
+ 			memcpy(pointers[nr_data], pointers[0], PAGE_SIZE);
+ 			run_xor(pointers + 1, nr_data - 1, PAGE_CACHE_SIZE);
+ 		}
+ 
+ 		/* Check scrubbing pairty and repair it */
+ 		p = rbio_stripe_page(rbio, rbio->scrubp, pagenr);
+ 		parity = kmap(p);
+ 		if (memcmp(parity, pointers[rbio->scrubp], PAGE_CACHE_SIZE))
+ 			memcpy(parity, pointers[rbio->scrubp], PAGE_CACHE_SIZE);
+ 		else
+ 			/* Parity is right, needn't writeback */
+ 			bitmap_clear(rbio->dbitmap, pagenr, 1);
+ 		kunmap(p);
+ 
+ 		for (stripe = 0; stripe < rbio->real_stripes; stripe++)
+ 			kunmap(page_in_rbio(rbio, stripe, pagenr, 0));
+ 	}
+ 
+ 	__free_page(p_page);
+ 	if (q_page)
+ 		__free_page(q_page);
+ 
+ writeback:
+ 	/*
+ 	 * time to start writing.  Make bios for everything from the
+ 	 * higher layers (the bio_list in our rbio) and our p/q.  Ignore
+ 	 * everything else.
+ 	 */
+ 	for_each_set_bit(pagenr, rbio->dbitmap, rbio->stripe_npages) {
+ 		struct page *page;
+ 
+ 		page = rbio_stripe_page(rbio, rbio->scrubp, pagenr);
+ 		ret = rbio_add_io_page(rbio, &bio_list,
+ 			       page, rbio->scrubp, pagenr, rbio->stripe_len);
+ 		if (ret)
+ 			goto cleanup;
+ 	}
+ 
+ 	if (!is_replace)
+ 		goto submit_write;
+ 
+ 	for_each_set_bit(pagenr, pbitmap, rbio->stripe_npages) {
+ 		struct page *page;
+ 
+ 		page = rbio_stripe_page(rbio, rbio->scrubp, pagenr);
+ 		ret = rbio_add_io_page(rbio, &bio_list, page,
+ 				       bbio->tgtdev_map[rbio->scrubp],
+ 				       pagenr, rbio->stripe_len);
+ 		if (ret)
+ 			goto cleanup;
+ 	}
+ 
+ submit_write:
+ 	nr_data = bio_list_size(&bio_list);
+ 	if (!nr_data) {
+ 		/* Every parity is right */
+ 		rbio_orig_end_io(rbio, 0, 0);
+ 		return;
+ 	}
+ 
+ 	atomic_set(&rbio->stripes_pending, nr_data);
+ 
+ 	while (1) {
+ 		bio = bio_list_pop(&bio_list);
+ 		if (!bio)
+ 			break;
+ 
+ 		bio->bi_private = rbio;
+ 		bio->bi_end_io = raid_write_parity_end_io;
+ 		BUG_ON(!test_bit(BIO_UPTODATE, &bio->bi_flags));
+ 		submit_bio(WRITE, bio);
+ 	}
+ 	return;
+ 
+ cleanup:
+ 	rbio_orig_end_io(rbio, -EIO, 0);
+ }
+ 
+ static inline int is_data_stripe(struct btrfs_raid_bio *rbio, int stripe)
+ {
+ 	if (stripe >= 0 && stripe < rbio->nr_data)
+ 		return 1;
+ 	return 0;
+ }
+ 
+ /*
+  * While we're doing the parity check and repair, we could have errors
+  * in reading pages off the disk.  This checks for errors and if we're
+  * not able to read the page it'll trigger parity reconstruction.  The
+  * parity scrub will be finished after we've reconstructed the failed
+  * stripes
+  */
+ static void validate_rbio_for_parity_scrub(struct btrfs_raid_bio *rbio)
+ {
+ 	if (atomic_read(&rbio->error) > rbio->bbio->max_errors)
+ 		goto cleanup;
+ 
+ 	if (rbio->faila >= 0 || rbio->failb >= 0) {
+ 		int dfail = 0, failp = -1;
+ 
+ 		if (is_data_stripe(rbio, rbio->faila))
+ 			dfail++;
+ 		else if (is_parity_stripe(rbio->faila))
+ 			failp = rbio->faila;
+ 
+ 		if (is_data_stripe(rbio, rbio->failb))
+ 			dfail++;
+ 		else if (is_parity_stripe(rbio->failb))
+ 			failp = rbio->failb;
+ 
+ 		/*
+ 		 * Because we can not use a scrubbing parity to repair
+ 		 * the data, so the capability of the repair is declined.
+ 		 * (In the case of RAID5, we can not repair anything)
+ 		 */
+ 		if (dfail > rbio->bbio->max_errors - 1)
+ 			goto cleanup;
+ 
+ 		/*
+ 		 * If all data is good, only parity is correctly, just
+ 		 * repair the parity.
+ 		 */
+ 		if (dfail == 0) {
+ 			finish_parity_scrub(rbio, 0);
+ 			return;
+ 		}
+ 
+ 		/*
+ 		 * Here means we got one corrupted data stripe and one
+ 		 * corrupted parity on RAID6, if the corrupted parity
+ 		 * is scrubbing parity, luckly, use the other one to repair
+ 		 * the data, or we can not repair the data stripe.
+ 		 */
+ 		if (failp != rbio->scrubp)
+ 			goto cleanup;
+ 
+ 		__raid_recover_end_io(rbio);
+ 	} else {
+ 		finish_parity_scrub(rbio, 1);
+ 	}
+ 	return;
+ 
+ cleanup:
+ 	rbio_orig_end_io(rbio, -EIO, 0);
+ }
+ 
+ /*
+  * end io for the read phase of the rmw cycle.  All the bios here are physical
+  * stripe bios we've read from the disk so we can recalculate the parity of the
+  * stripe.
+  *
+  * This will usually kick off finish_rmw once all the bios are read in, but it
+  * may trigger parity reconstruction if we had any errors along the way
+  */
+ static void raid56_parity_scrub_end_io(struct bio *bio, int err)
+ {
+ 	struct btrfs_raid_bio *rbio = bio->bi_private;
+ 
+ 	if (err)
+ 		fail_bio_stripe(rbio, bio);
+ 	else
+ 		set_bio_pages_uptodate(bio);
+ 
+ 	bio_put(bio);
+ 
+ 	if (!atomic_dec_and_test(&rbio->stripes_pending))
+ 		return;
+ 
+ 	/*
+ 	 * this will normally call finish_rmw to start our write
+ 	 * but if there are any failed stripes we'll reconstruct
+ 	 * from parity first
+ 	 */
+ 	validate_rbio_for_parity_scrub(rbio);
+ }
+ 
+ static void raid56_parity_scrub_stripe(struct btrfs_raid_bio *rbio)
+ {
+ 	int bios_to_read = 0;
+ 	struct bio_list bio_list;
+ 	int ret;
+ 	int pagenr;
+ 	int stripe;
+ 	struct bio *bio;
+ 
+ 	ret = alloc_rbio_essential_pages(rbio);
+ 	if (ret)
+ 		goto cleanup;
+ 
+ 	bio_list_init(&bio_list);
+ 
+ 	atomic_set(&rbio->error, 0);
+ 	/*
+ 	 * build a list of bios to read all the missing parts of this
+ 	 * stripe
+ 	 */
+ 	for (stripe = 0; stripe < rbio->real_stripes; stripe++) {
+ 		for_each_set_bit(pagenr, rbio->dbitmap, rbio->stripe_npages) {
+ 			struct page *page;
+ 			/*
+ 			 * we want to find all the pages missing from
+ 			 * the rbio and read them from the disk.  If
+ 			 * page_in_rbio finds a page in the bio list
+ 			 * we don't need to read it off the stripe.
+ 			 */
+ 			page = page_in_rbio(rbio, stripe, pagenr, 1);
+ 			if (page)
+ 				continue;
+ 
+ 			page = rbio_stripe_page(rbio, stripe, pagenr);
+ 			/*
+ 			 * the bio cache may have handed us an uptodate
+ 			 * page.  If so, be happy and use it
+ 			 */
+ 			if (PageUptodate(page))
+ 				continue;
+ 
+ 			ret = rbio_add_io_page(rbio, &bio_list, page,
+ 				       stripe, pagenr, rbio->stripe_len);
+ 			if (ret)
+ 				goto cleanup;
+ 		}
+ 	}
+ 
+ 	bios_to_read = bio_list_size(&bio_list);
+ 	if (!bios_to_read) {
+ 		/*
+ 		 * this can happen if others have merged with
+ 		 * us, it means there is nothing left to read.
+ 		 * But if there are missing devices it may not be
+ 		 * safe to do the full stripe write yet.
+ 		 */
+ 		goto finish;
+ 	}
+ 
+ 	/*
+ 	 * the bbio may be freed once we submit the last bio.  Make sure
+ 	 * not to touch it after that
+ 	 */
+ 	atomic_set(&rbio->stripes_pending, bios_to_read);
+ 	while (1) {
+ 		bio = bio_list_pop(&bio_list);
+ 		if (!bio)
+ 			break;
+ 
+ 		bio->bi_private = rbio;
+ 		bio->bi_end_io = raid56_parity_scrub_end_io;
+ 
+ 		btrfs_bio_wq_end_io(rbio->fs_info, bio,
+ 				    BTRFS_WQ_ENDIO_RAID56);
+ 
+ 		BUG_ON(!test_bit(BIO_UPTODATE, &bio->bi_flags));
+ 		submit_bio(READ, bio);
+ 	}
+ 	/* the actual write will happen once the reads are done */
+ 	return;
+ 
+ cleanup:
+ 	rbio_orig_end_io(rbio, -EIO, 0);
+ 	return;
+ 
+ finish:
+ 	validate_rbio_for_parity_scrub(rbio);
+ }
+ 
+ static void scrub_parity_work(struct btrfs_work *work)
+ {
+ 	struct btrfs_raid_bio *rbio;
+ 
+ 	rbio = container_of(work, struct btrfs_raid_bio, work);
+ 	raid56_parity_scrub_stripe(rbio);
+ }
+ 
+ static void async_scrub_parity(struct btrfs_raid_bio *rbio)
+ {
+ 	btrfs_init_work(&rbio->work, btrfs_rmw_helper,
+ 			scrub_parity_work, NULL, NULL);
+ 
+ 	btrfs_queue_work(rbio->fs_info->rmw_workers,
+ 			 &rbio->work);
+ }
+ 
+ void raid56_parity_submit_scrub_rbio(struct btrfs_raid_bio *rbio)
+ {
+ 	if (!lock_stripe_add(rbio))
+ 		async_scrub_parity(rbio);
+ }
++>>>>>>> 760359769014 (Btrfs, replace: write raid56 parity into the replace target device)
diff --cc fs/btrfs/scrub.c
index 5189c8700c82,0ae837fd676d..000000000000
--- a/fs/btrfs/scrub.c
+++ b/fs/btrfs/scrub.c
@@@ -2269,6 -2662,323 +2269,326 @@@ static int get_raid56_logic_offset(u64 
  	return 1;
  }
  
++<<<<<<< HEAD
++=======
+ static void scrub_free_parity(struct scrub_parity *sparity)
+ {
+ 	struct scrub_ctx *sctx = sparity->sctx;
+ 	struct scrub_page *curr, *next;
+ 	int nbits;
+ 
+ 	nbits = bitmap_weight(sparity->ebitmap, sparity->nsectors);
+ 	if (nbits) {
+ 		spin_lock(&sctx->stat_lock);
+ 		sctx->stat.read_errors += nbits;
+ 		sctx->stat.uncorrectable_errors += nbits;
+ 		spin_unlock(&sctx->stat_lock);
+ 	}
+ 
+ 	list_for_each_entry_safe(curr, next, &sparity->spages, list) {
+ 		list_del_init(&curr->list);
+ 		scrub_page_put(curr);
+ 	}
+ 
+ 	kfree(sparity);
+ }
+ 
+ static void scrub_parity_bio_endio(struct bio *bio, int error)
+ {
+ 	struct scrub_parity *sparity = (struct scrub_parity *)bio->bi_private;
+ 	struct scrub_ctx *sctx = sparity->sctx;
+ 
+ 	if (error)
+ 		bitmap_or(sparity->ebitmap, sparity->ebitmap, sparity->dbitmap,
+ 			  sparity->nsectors);
+ 
+ 	scrub_free_parity(sparity);
+ 	scrub_pending_bio_dec(sctx);
+ 	bio_put(bio);
+ }
+ 
+ static void scrub_parity_check_and_repair(struct scrub_parity *sparity)
+ {
+ 	struct scrub_ctx *sctx = sparity->sctx;
+ 	struct bio *bio;
+ 	struct btrfs_raid_bio *rbio;
+ 	struct scrub_page *spage;
+ 	struct btrfs_bio *bbio = NULL;
+ 	u64 *raid_map = NULL;
+ 	u64 length;
+ 	int ret;
+ 
+ 	if (!bitmap_andnot(sparity->dbitmap, sparity->dbitmap, sparity->ebitmap,
+ 			   sparity->nsectors))
+ 		goto out;
+ 
+ 	length = sparity->logic_end - sparity->logic_start + 1;
+ 	ret = btrfs_map_sblock(sctx->dev_root->fs_info, WRITE,
+ 			       sparity->logic_start,
+ 			       &length, &bbio, 0, &raid_map);
+ 	if (ret || !bbio || !raid_map)
+ 		goto bbio_out;
+ 
+ 	bio = btrfs_io_bio_alloc(GFP_NOFS, 0);
+ 	if (!bio)
+ 		goto bbio_out;
+ 
+ 	bio->bi_iter.bi_sector = sparity->logic_start >> 9;
+ 	bio->bi_private = sparity;
+ 	bio->bi_end_io = scrub_parity_bio_endio;
+ 
+ 	rbio = raid56_parity_alloc_scrub_rbio(sctx->dev_root, bio, bbio,
+ 					      raid_map, length,
+ 					      sparity->scrub_dev,
+ 					      sparity->dbitmap,
+ 					      sparity->nsectors);
+ 	if (!rbio)
+ 		goto rbio_out;
+ 
+ 	list_for_each_entry(spage, &sparity->spages, list)
+ 		raid56_parity_add_scrub_pages(rbio, spage->page,
+ 					      spage->logical);
+ 
+ 	scrub_pending_bio_inc(sctx);
+ 	raid56_parity_submit_scrub_rbio(rbio);
+ 	return;
+ 
+ rbio_out:
+ 	bio_put(bio);
+ bbio_out:
+ 	kfree(bbio);
+ 	kfree(raid_map);
+ 	bitmap_or(sparity->ebitmap, sparity->ebitmap, sparity->dbitmap,
+ 		  sparity->nsectors);
+ 	spin_lock(&sctx->stat_lock);
+ 	sctx->stat.malloc_errors++;
+ 	spin_unlock(&sctx->stat_lock);
+ out:
+ 	scrub_free_parity(sparity);
+ }
+ 
+ static inline int scrub_calc_parity_bitmap_len(int nsectors)
+ {
+ 	return DIV_ROUND_UP(nsectors, BITS_PER_LONG) * (BITS_PER_LONG / 8);
+ }
+ 
+ static void scrub_parity_get(struct scrub_parity *sparity)
+ {
+ 	atomic_inc(&sparity->ref_count);
+ }
+ 
+ static void scrub_parity_put(struct scrub_parity *sparity)
+ {
+ 	if (!atomic_dec_and_test(&sparity->ref_count))
+ 		return;
+ 
+ 	scrub_parity_check_and_repair(sparity);
+ }
+ 
+ static noinline_for_stack int scrub_raid56_parity(struct scrub_ctx *sctx,
+ 						  struct map_lookup *map,
+ 						  struct btrfs_device *sdev,
+ 						  struct btrfs_path *path,
+ 						  u64 logic_start,
+ 						  u64 logic_end)
+ {
+ 	struct btrfs_fs_info *fs_info = sctx->dev_root->fs_info;
+ 	struct btrfs_root *root = fs_info->extent_root;
+ 	struct btrfs_root *csum_root = fs_info->csum_root;
+ 	struct btrfs_extent_item *extent;
+ 	u64 flags;
+ 	int ret;
+ 	int slot;
+ 	struct extent_buffer *l;
+ 	struct btrfs_key key;
+ 	u64 generation;
+ 	u64 extent_logical;
+ 	u64 extent_physical;
+ 	u64 extent_len;
+ 	struct btrfs_device *extent_dev;
+ 	struct scrub_parity *sparity;
+ 	int nsectors;
+ 	int bitmap_len;
+ 	int extent_mirror_num;
+ 	int stop_loop = 0;
+ 
+ 	nsectors = map->stripe_len / root->sectorsize;
+ 	bitmap_len = scrub_calc_parity_bitmap_len(nsectors);
+ 	sparity = kzalloc(sizeof(struct scrub_parity) + 2 * bitmap_len,
+ 			  GFP_NOFS);
+ 	if (!sparity) {
+ 		spin_lock(&sctx->stat_lock);
+ 		sctx->stat.malloc_errors++;
+ 		spin_unlock(&sctx->stat_lock);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	sparity->stripe_len = map->stripe_len;
+ 	sparity->nsectors = nsectors;
+ 	sparity->sctx = sctx;
+ 	sparity->scrub_dev = sdev;
+ 	sparity->logic_start = logic_start;
+ 	sparity->logic_end = logic_end;
+ 	atomic_set(&sparity->ref_count, 1);
+ 	INIT_LIST_HEAD(&sparity->spages);
+ 	sparity->dbitmap = sparity->bitmap;
+ 	sparity->ebitmap = (void *)sparity->bitmap + bitmap_len;
+ 
+ 	ret = 0;
+ 	while (logic_start < logic_end) {
+ 		if (btrfs_fs_incompat(fs_info, SKINNY_METADATA))
+ 			key.type = BTRFS_METADATA_ITEM_KEY;
+ 		else
+ 			key.type = BTRFS_EXTENT_ITEM_KEY;
+ 		key.objectid = logic_start;
+ 		key.offset = (u64)-1;
+ 
+ 		ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);
+ 		if (ret < 0)
+ 			goto out;
+ 
+ 		if (ret > 0) {
+ 			ret = btrfs_previous_extent_item(root, path, 0);
+ 			if (ret < 0)
+ 				goto out;
+ 			if (ret > 0) {
+ 				btrfs_release_path(path);
+ 				ret = btrfs_search_slot(NULL, root, &key,
+ 							path, 0, 0);
+ 				if (ret < 0)
+ 					goto out;
+ 			}
+ 		}
+ 
+ 		stop_loop = 0;
+ 		while (1) {
+ 			u64 bytes;
+ 
+ 			l = path->nodes[0];
+ 			slot = path->slots[0];
+ 			if (slot >= btrfs_header_nritems(l)) {
+ 				ret = btrfs_next_leaf(root, path);
+ 				if (ret == 0)
+ 					continue;
+ 				if (ret < 0)
+ 					goto out;
+ 
+ 				stop_loop = 1;
+ 				break;
+ 			}
+ 			btrfs_item_key_to_cpu(l, &key, slot);
+ 
+ 			if (key.type == BTRFS_METADATA_ITEM_KEY)
+ 				bytes = root->nodesize;
+ 			else
+ 				bytes = key.offset;
+ 
+ 			if (key.objectid + bytes <= logic_start)
+ 				goto next;
+ 
+ 			if (key.type != BTRFS_EXTENT_ITEM_KEY &&
+ 			    key.type != BTRFS_METADATA_ITEM_KEY)
+ 				goto next;
+ 
+ 			if (key.objectid > logic_end) {
+ 				stop_loop = 1;
+ 				break;
+ 			}
+ 
+ 			while (key.objectid >= logic_start + map->stripe_len)
+ 				logic_start += map->stripe_len;
+ 
+ 			extent = btrfs_item_ptr(l, slot,
+ 						struct btrfs_extent_item);
+ 			flags = btrfs_extent_flags(l, extent);
+ 			generation = btrfs_extent_generation(l, extent);
+ 
+ 			if (key.objectid < logic_start &&
+ 			    (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)) {
+ 				btrfs_err(fs_info,
+ 					  "scrub: tree block %llu spanning stripes, ignored. logical=%llu",
+ 					   key.objectid, logic_start);
+ 				goto next;
+ 			}
+ again:
+ 			extent_logical = key.objectid;
+ 			extent_len = bytes;
+ 
+ 			if (extent_logical < logic_start) {
+ 				extent_len -= logic_start - extent_logical;
+ 				extent_logical = logic_start;
+ 			}
+ 
+ 			if (extent_logical + extent_len >
+ 			    logic_start + map->stripe_len)
+ 				extent_len = logic_start + map->stripe_len -
+ 					     extent_logical;
+ 
+ 			scrub_parity_mark_sectors_data(sparity, extent_logical,
+ 						       extent_len);
+ 
+ 			scrub_remap_extent(fs_info, extent_logical,
+ 					   extent_len, &extent_physical,
+ 					   &extent_dev,
+ 					   &extent_mirror_num);
+ 
+ 			ret = btrfs_lookup_csums_range(csum_root,
+ 						extent_logical,
+ 						extent_logical + extent_len - 1,
+ 						&sctx->csum_list, 1);
+ 			if (ret)
+ 				goto out;
+ 
+ 			ret = scrub_extent_for_parity(sparity, extent_logical,
+ 						      extent_len,
+ 						      extent_physical,
+ 						      extent_dev, flags,
+ 						      generation,
+ 						      extent_mirror_num);
+ 			if (ret)
+ 				goto out;
+ 
+ 			scrub_free_csums(sctx);
+ 			if (extent_logical + extent_len <
+ 			    key.objectid + bytes) {
+ 				logic_start += map->stripe_len;
+ 
+ 				if (logic_start >= logic_end) {
+ 					stop_loop = 1;
+ 					break;
+ 				}
+ 
+ 				if (logic_start < key.objectid + bytes) {
+ 					cond_resched();
+ 					goto again;
+ 				}
+ 			}
+ next:
+ 			path->slots[0]++;
+ 		}
+ 
+ 		btrfs_release_path(path);
+ 
+ 		if (stop_loop)
+ 			break;
+ 
+ 		logic_start += map->stripe_len;
+ 	}
+ out:
+ 	if (ret < 0)
+ 		scrub_parity_mark_sectors_error(sparity, logic_start,
+ 						logic_end - logic_start + 1);
+ 	scrub_parity_put(sparity);
+ 	scrub_submit(sctx);
+ 	mutex_lock(&sctx->wr_ctx.wr_lock);
+ 	scrub_wr_submit(sctx);
+ 	mutex_unlock(&sctx->wr_ctx.wr_lock);
+ 
+ 	btrfs_release_path(path);
+ 	return ret < 0 ? ret : 0;
+ }
+ 
++>>>>>>> 760359769014 (Btrfs, replace: write raid56 parity into the replace target device)
  static noinline_for_stack int scrub_stripe(struct scrub_ctx *sctx,
  					   struct map_lookup *map,
  					   struct btrfs_device *scrub_dev,
* Unmerged path fs/btrfs/raid56.c
* Unmerged path fs/btrfs/scrub.c
