tcp: add tcp_conn_request

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Octavian Purdila <octavian.purdila@intel.com>
commit 1fb6f159fd21c640a28eb65fbd62ce8c9f6a777e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/1fb6f159.failed

Create tcp_conn_request and remove most of the code from
tcp_v4_conn_request and tcp_v6_conn_request.

	Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1fb6f159fd21c640a28eb65fbd62ce8c9f6a777e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/tcp.h
#	net/ipv4/tcp_ipv4.c
#	net/ipv6/tcp_ipv6.c
diff --cc include/net/tcp.h
index ba45accd7103,0d5389aecf18..000000000000
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@@ -1541,18 -1550,34 +1541,26 @@@ extern void tcp_v4_destroy_sock(struct 
  
  struct sk_buff *tcp_gso_segment(struct sk_buff *skb,
  				netdev_features_t features);
 -struct sk_buff **tcp_gro_receive(struct sk_buff **head, struct sk_buff *skb);
 -int tcp_gro_complete(struct sk_buff *skb);
 -
 -void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr, __be32 daddr);
 -
 -static inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)
 -{
 -	return tp->notsent_lowat ?: sysctl_tcp_notsent_lowat;
 -}
 -
 -static inline bool tcp_stream_memory_free(const struct sock *sk)
 -{
 -	const struct tcp_sock *tp = tcp_sk(sk);
 -	u32 notsent_bytes = tp->write_seq - tp->snd_nxt;
 +extern struct sk_buff **tcp_gro_receive(struct sk_buff **head,
 +					struct sk_buff *skb);
 +extern int tcp_gro_complete(struct sk_buff *skb);
  
 -	return notsent_bytes < tcp_notsent_lowat(tp);
 -}
 +extern void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr,
 +				__be32 daddr);
  
  #ifdef CONFIG_PROC_FS
 -int tcp4_proc_init(void);
 -void tcp4_proc_exit(void);
 +extern int tcp4_proc_init(void);
 +extern void tcp4_proc_exit(void);
  #endif
  
++<<<<<<< HEAD
++=======
+ int tcp_rtx_synack(struct sock *sk, struct request_sock *req);
+ int tcp_conn_request(struct request_sock_ops *rsk_ops,
+ 		     const struct tcp_request_sock_ops *af_ops,
+ 		     struct sock *sk, struct sk_buff *skb);
+ 
++>>>>>>> 1fb6f159fd21 (tcp: add tcp_conn_request)
  /* TCP af-specific functions */
  struct tcp_sock_af_ops {
  #ifdef CONFIG_TCP_MD5SIG
diff --cc net/ipv4/tcp_ipv4.c
index 7c1eb4426934,5dfebd2f2e38..000000000000
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@@ -1437,171 -1282,13 +1437,169 @@@ static int tcp_v4_conn_req_fastopen(str
  
  int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
  {
++<<<<<<< HEAD
 +	struct tcp_options_received tmp_opt;
 +	struct request_sock *req;
 +	struct inet_request_sock *ireq;
 +	struct tcp_sock *tp = tcp_sk(sk);
 +	struct dst_entry *dst = NULL;
 +	__be32 saddr = ip_hdr(skb)->saddr;
 +	__be32 daddr = ip_hdr(skb)->daddr;
 +	__u32 isn = TCP_SKB_CB(skb)->when;
 +	bool want_cookie = false;
 +	struct flowi4 fl4;
 +	struct tcp_fastopen_cookie foc = { .len = -1 };
 +	struct tcp_fastopen_cookie valid_foc = { .len = -1 };
 +	struct sk_buff *skb_synack;
 +	int do_fastopen;
 +
++=======
++>>>>>>> 1fb6f159fd21 (tcp: add tcp_conn_request)
  	/* Never answer to SYNs send to broadcast or multicast */
  	if (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))
  		goto drop;
  
- 	/* TW buckets are converted to open requests without
- 	 * limitations, they conserve resources and peer is
- 	 * evidently real one.
- 	 */
- 	if ((sysctl_tcp_syncookies == 2 ||
- 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
- 		want_cookie = tcp_syn_flood_action(sk, skb, "TCP");
- 		if (!want_cookie)
- 			goto drop;
- 	}
+ 	return tcp_conn_request(&tcp_request_sock_ops,
+ 				&tcp_request_sock_ipv4_ops, sk, skb);
  
++<<<<<<< HEAD
 +	/* Accept backlog is full. If we have already queued enough
 +	 * of warm entries in syn queue, drop request. It is better than
 +	 * clogging syn queue with openreqs with exponentially increasing
 +	 * timeout.
 +	 */
 +	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
 +		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
 +		goto drop;
 +	}
 +
 +	req = inet_reqsk_alloc(&tcp_request_sock_ops);
 +	if (!req)
 +		goto drop;
 +
 +#ifdef CONFIG_TCP_MD5SIG
 +	tcp_rsk(req)->af_specific = &tcp_request_sock_ipv4_ops;
 +#endif
 +
 +	tcp_clear_options(&tmp_opt);
 +	tmp_opt.mss_clamp = TCP_MSS_DEFAULT;
 +	tmp_opt.user_mss  = tp->rx_opt.user_mss;
 +	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
 +
 +	if (want_cookie && !tmp_opt.saw_tstamp)
 +		tcp_clear_options(&tmp_opt);
 +
 +	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
 +	tcp_openreq_init(req, &tmp_opt, skb);
 +
 +	ireq = inet_rsk(req);
 +	ireq->ir_loc_addr = daddr;
 +	ireq->ir_rmt_addr = saddr;
 +	ireq->no_srccheck = inet_sk(sk)->transparent;
 +	ireq->opt = tcp_v4_save_options(skb);
 +
 +	if (security_inet_conn_request(sk, skb, req))
 +		goto drop_and_free;
 +
 +	if (!want_cookie || tmp_opt.tstamp_ok)
 +		TCP_ECN_create_request(req, skb, sock_net(sk));
 +
 +	if (want_cookie) {
 +		isn = cookie_v4_init_sequence(sk, skb, &req->mss);
 +		req->cookie_ts = tmp_opt.tstamp_ok;
 +	} else if (!isn) {
 +		/* VJ's idea. We save last timestamp seen
 +		 * from the destination in peer table, when entering
 +		 * state TIME-WAIT, and check against it before
 +		 * accepting new connection request.
 +		 *
 +		 * If "isn" is not zero, this request hit alive
 +		 * timewait bucket, so that all the necessary checks
 +		 * are made in the function processing timewait state.
 +		 */
 +		if (tmp_opt.saw_tstamp &&
 +		    tcp_death_row.sysctl_tw_recycle &&
 +		    (dst = inet_csk_route_req(sk, &fl4, req)) != NULL &&
 +		    fl4.daddr == saddr) {
 +			if (!tcp_peer_is_proven(req, dst, true)) {
 +				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
 +				goto drop_and_release;
 +			}
 +		}
 +		/* Kill the following clause, if you dislike this way. */
 +		else if (!sysctl_tcp_syncookies &&
 +			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
 +			  (sysctl_max_syn_backlog >> 2)) &&
 +			 !tcp_peer_is_proven(req, dst, false)) {
 +			/* Without syncookies last quarter of
 +			 * backlog is filled with destinations,
 +			 * proven to be alive.
 +			 * It means that we continue to communicate
 +			 * to destinations, already remembered
 +			 * to the moment of synflood.
 +			 */
 +			LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI4/%u\n"),
 +				       &saddr, ntohs(tcp_hdr(skb)->source));
 +			goto drop_and_release;
 +		}
 +
 +		isn = tcp_v4_init_sequence(skb);
 +	}
 +	tcp_rsk(req)->snt_isn = isn;
 +
 +	if (dst == NULL) {
 +		dst = inet_csk_route_req(sk, &fl4, req);
 +		if (dst == NULL)
 +			goto drop_and_free;
 +	}
 +	do_fastopen = tcp_fastopen_check(sk, skb, req, &foc, &valid_foc);
 +
 +	/* We don't call tcp_v4_send_synack() directly because we need
 +	 * to make sure a child socket can be created successfully before
 +	 * sending back synack!
 +	 *
 +	 * XXX (TFO) - Ideally one would simply call tcp_v4_send_synack()
 +	 * (or better yet, call tcp_send_synack() in the child context
 +	 * directly, but will have to fix bunch of other code first)
 +	 * after syn_recv_sock() except one will need to first fix the
 +	 * latter to remove its dependency on the current implementation
 +	 * of tcp_v4_send_synack()->tcp_select_initial_window().
 +	 */
 +	skb_synack = tcp_make_synack(sk, dst, req,
 +	    fastopen_cookie_present(&valid_foc) ? &valid_foc : NULL);
 +
 +	if (skb_synack) {
 +		__tcp_v4_send_check(skb_synack, ireq->ir_loc_addr, ireq->ir_rmt_addr);
 +		skb_set_queue_mapping(skb_synack, skb_get_queue_mapping(skb));
 +	} else
 +		goto drop_and_free;
 +
 +	if (likely(!do_fastopen)) {
 +		int err;
 +		err = ip_build_and_send_pkt(skb_synack, sk, ireq->ir_loc_addr,
 +		     ireq->ir_rmt_addr, ireq->opt);
 +		err = net_xmit_eval(err);
 +		if (err || want_cookie)
 +			goto drop_and_free;
 +
 +		tcp_rsk(req)->snt_synack = tcp_time_stamp;
 +		tcp_rsk(req)->listener = NULL;
 +		/* Add the request_sock to the SYN table */
 +		inet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
 +		if (fastopen_cookie_present(&foc) && foc.len != 0)
 +			NET_INC_STATS_BH(sock_net(sk),
 +			    LINUX_MIB_TCPFASTOPENPASSIVEFAIL);
 +	} else if (tcp_v4_conn_req_fastopen(sk, skb, skb_synack, req))
 +		goto drop_and_free;
 +
 +	return 0;
 +
 +drop_and_release:
 +	dst_release(dst);
 +drop_and_free:
 +	reqsk_free(req);
++=======
++>>>>>>> 1fb6f159fd21 (tcp: add tcp_conn_request)
  drop:
  	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
  	return 0;
diff --cc net/ipv6/tcp_ipv6.c
index 45f11a2930c9,bc24ee21339a..000000000000
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@@ -965,141 -1008,17 +965,140 @@@ static struct sock *tcp_v6_hnd_req(stru
  	return sk;
  }
  
- /* FIXME: this is substantially similar to the ipv4 code.
-  * Can some kind of merge be done? -- erics
-  */
  static int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
  {
++<<<<<<< HEAD
 +	struct tcp_options_received tmp_opt;
 +	struct request_sock *req;
 +	struct inet_request_sock *ireq;
 +	struct ipv6_pinfo *np = inet6_sk(sk);
 +	struct tcp_sock *tp = tcp_sk(sk);
 +	__u32 isn = TCP_SKB_CB(skb)->when;
 +	struct dst_entry *dst = NULL;
 +	struct flowi6 fl6;
 +	bool want_cookie = false;
 +
++=======
++>>>>>>> 1fb6f159fd21 (tcp: add tcp_conn_request)
  	if (skb->protocol == htons(ETH_P_IP))
  		return tcp_v4_conn_request(sk, skb);
  
  	if (!ipv6_unicast_destination(skb))
  		goto drop;
  
- 	if ((sysctl_tcp_syncookies == 2 ||
- 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
- 		want_cookie = tcp_syn_flood_action(sk, skb, "TCPv6");
- 		if (!want_cookie)
- 			goto drop;
- 	}
+ 	return tcp_conn_request(&tcp6_request_sock_ops,
+ 				&tcp_request_sock_ipv6_ops, sk, skb);
  
++<<<<<<< HEAD
 +	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
 +		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
 +		goto drop;
 +	}
 +
 +	req = inet6_reqsk_alloc(&tcp6_request_sock_ops);
 +	if (req == NULL)
 +		goto drop;
 +
 +#ifdef CONFIG_TCP_MD5SIG
 +	tcp_rsk(req)->af_specific = &tcp_request_sock_ipv6_ops;
 +#endif
 +
 +	tcp_clear_options(&tmp_opt);
 +	tmp_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);
 +	tmp_opt.user_mss = tp->rx_opt.user_mss;
 +	tcp_parse_options(skb, &tmp_opt, 0, NULL);
 +
 +	if (want_cookie && !tmp_opt.saw_tstamp)
 +		tcp_clear_options(&tmp_opt);
 +
 +	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
 +	tcp_openreq_init(req, &tmp_opt, skb);
 +
 +	ireq = inet_rsk(req);
 +	ireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;
 +	ireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;
 +	if (!want_cookie || tmp_opt.tstamp_ok)
 +		TCP_ECN_create_request(req, skb, sock_net(sk));
 +
 +	ireq->ir_iif = sk->sk_bound_dev_if;
 +
 +	/* So that link locals have meaning */
 +	if (!sk->sk_bound_dev_if &&
 +	    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)
 +		ireq->ir_iif = inet6_iif(skb);
 +
 +	if (!isn) {
 +		if (ipv6_opt_accepted(sk, skb) ||
 +		    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||
 +		    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {
 +			atomic_inc(&skb->users);
 +			ireq->pktopts = skb;
 +		}
 +
 +		if (want_cookie) {
 +			isn = cookie_v6_init_sequence(sk, skb, &req->mss);
 +			req->cookie_ts = tmp_opt.tstamp_ok;
 +			goto have_isn;
 +		}
 +
 +		/* VJ's idea. We save last timestamp seen
 +		 * from the destination in peer table, when entering
 +		 * state TIME-WAIT, and check against it before
 +		 * accepting new connection request.
 +		 *
 +		 * If "isn" is not zero, this request hit alive
 +		 * timewait bucket, so that all the necessary checks
 +		 * are made in the function processing timewait state.
 +		 */
 +		if (tmp_opt.saw_tstamp &&
 +		    tcp_death_row.sysctl_tw_recycle &&
 +		    (dst = inet6_csk_route_req(sk, &fl6, req)) != NULL) {
 +			if (!tcp_peer_is_proven(req, dst, true)) {
 +				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
 +				goto drop_and_release;
 +			}
 +		}
 +		/* Kill the following clause, if you dislike this way. */
 +		else if (!sysctl_tcp_syncookies &&
 +			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
 +			  (sysctl_max_syn_backlog >> 2)) &&
 +			 !tcp_peer_is_proven(req, dst, false)) {
 +			/* Without syncookies last quarter of
 +			 * backlog is filled with destinations,
 +			 * proven to be alive.
 +			 * It means that we continue to communicate
 +			 * to destinations, already remembered
 +			 * to the moment of synflood.
 +			 */
 +			LIMIT_NETDEBUG(KERN_DEBUG "TCP: drop open request from %pI6/%u\n",
 +				       &ireq->ir_v6_rmt_addr, ntohs(tcp_hdr(skb)->source));
 +			goto drop_and_release;
 +		}
 +
 +		isn = tcp_v6_init_sequence(skb);
 +	}
 +have_isn:
 +	tcp_rsk(req)->snt_isn = isn;
 +
 +	if (security_inet_conn_request(sk, skb, req))
 +		goto drop_and_release;
 +
 +	if (tcp_v6_send_synack(sk, dst, &fl6, req,
 +			       skb_get_queue_mapping(skb)) ||
 +	    want_cookie)
 +		goto drop_and_free;
 +
 +	tcp_rsk(req)->snt_synack = tcp_time_stamp;
 +	tcp_rsk(req)->listener = NULL;
 +	inet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
 +	return 0;
 +
 +drop_and_release:
 +	dst_release(dst);
 +drop_and_free:
 +	reqsk_free(req);
++=======
++>>>>>>> 1fb6f159fd21 (tcp: add tcp_conn_request)
  drop:
  	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
  	return 0; /* don't send reset */
* Unmerged path include/net/tcp.h
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index a1bfad3e8197..80caea04aa36 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -5819,3 +5819,151 @@ discard:
 	return 0;
 }
 EXPORT_SYMBOL(tcp_rcv_state_process);
+
+static inline void pr_drop_req(struct request_sock *req, __u16 port, int family)
+{
+	struct inet_request_sock *ireq = inet_rsk(req);
+
+	if (family == AF_INET)
+		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI4/%u\n"),
+			       &ireq->ir_rmt_addr, port);
+	else
+		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI6/%u\n"),
+			       &ireq->ir_v6_rmt_addr, port);
+}
+
+int tcp_conn_request(struct request_sock_ops *rsk_ops,
+		     const struct tcp_request_sock_ops *af_ops,
+		     struct sock *sk, struct sk_buff *skb)
+{
+	struct tcp_options_received tmp_opt;
+	struct request_sock *req;
+	struct tcp_sock *tp = tcp_sk(sk);
+	struct dst_entry *dst = NULL;
+	__u32 isn = TCP_SKB_CB(skb)->when;
+	bool want_cookie = false, fastopen;
+	struct flowi fl;
+	struct tcp_fastopen_cookie foc = { .len = -1 };
+	int err;
+
+
+	/* TW buckets are converted to open requests without
+	 * limitations, they conserve resources and peer is
+	 * evidently real one.
+	 */
+	if ((sysctl_tcp_syncookies == 2 ||
+	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
+		want_cookie = tcp_syn_flood_action(sk, skb, rsk_ops->slab_name);
+		if (!want_cookie)
+			goto drop;
+	}
+
+
+	/* Accept backlog is full. If we have already queued enough
+	 * of warm entries in syn queue, drop request. It is better than
+	 * clogging syn queue with openreqs with exponentially increasing
+	 * timeout.
+	 */
+	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
+		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+		goto drop;
+	}
+
+	req = inet_reqsk_alloc(rsk_ops);
+	if (!req)
+		goto drop;
+
+	tcp_rsk(req)->af_specific = af_ops;
+
+	tcp_clear_options(&tmp_opt);
+	tmp_opt.mss_clamp = af_ops->mss_clamp;
+	tmp_opt.user_mss  = tp->rx_opt.user_mss;
+	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
+
+	if (want_cookie && !tmp_opt.saw_tstamp)
+		tcp_clear_options(&tmp_opt);
+
+	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+	tcp_openreq_init(req, &tmp_opt, skb, sk);
+
+	af_ops->init_req(req, sk, skb);
+
+	if (security_inet_conn_request(sk, skb, req))
+		goto drop_and_free;
+
+	if (!want_cookie || tmp_opt.tstamp_ok)
+		TCP_ECN_create_request(req, skb, sock_net(sk));
+
+	if (want_cookie) {
+		isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
+		req->cookie_ts = tmp_opt.tstamp_ok;
+	} else if (!isn) {
+		/* VJ's idea. We save last timestamp seen
+		 * from the destination in peer table, when entering
+		 * state TIME-WAIT, and check against it before
+		 * accepting new connection request.
+		 *
+		 * If "isn" is not zero, this request hit alive
+		 * timewait bucket, so that all the necessary checks
+		 * are made in the function processing timewait state.
+		 */
+		if (tmp_opt.saw_tstamp && tcp_death_row.sysctl_tw_recycle) {
+			bool strict;
+
+			dst = af_ops->route_req(sk, &fl, req, &strict);
+			if (dst && strict &&
+			    !tcp_peer_is_proven(req, dst, true)) {
+				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
+				goto drop_and_release;
+			}
+		}
+		/* Kill the following clause, if you dislike this way. */
+		else if (!sysctl_tcp_syncookies &&
+			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
+			  (sysctl_max_syn_backlog >> 2)) &&
+			 !tcp_peer_is_proven(req, dst, false)) {
+			/* Without syncookies last quarter of
+			 * backlog is filled with destinations,
+			 * proven to be alive.
+			 * It means that we continue to communicate
+			 * to destinations, already remembered
+			 * to the moment of synflood.
+			 */
+			pr_drop_req(req, ntohs(tcp_hdr(skb)->source),
+				    rsk_ops->family);
+			goto drop_and_release;
+		}
+
+		isn = af_ops->init_seq(skb);
+	}
+	if (!dst) {
+		dst = af_ops->route_req(sk, &fl, req, NULL);
+		if (!dst)
+			goto drop_and_free;
+	}
+
+	tcp_rsk(req)->snt_isn = isn;
+	tcp_openreq_init_rwin(req, sk, dst);
+	fastopen = !want_cookie &&
+		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+	err = af_ops->send_synack(sk, dst, &fl, req,
+				  skb_get_queue_mapping(skb), &foc);
+	if (!fastopen) {
+		if (err || want_cookie)
+			goto drop_and_free;
+
+		tcp_rsk(req)->listener = NULL;
+		af_ops->queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+	}
+
+	return 0;
+
+drop_and_release:
+	dst_release(dst);
+drop_and_free:
+	reqsk_free(req);
+drop:
+	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+	return 0;
+}
+EXPORT_SYMBOL(tcp_conn_request);
* Unmerged path net/ipv4/tcp_ipv4.c
* Unmerged path net/ipv6/tcp_ipv6.c
