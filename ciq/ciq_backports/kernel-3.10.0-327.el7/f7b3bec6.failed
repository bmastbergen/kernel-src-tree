net: allow setting ecn via routing table

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] tcp: allow setting ecn via routing table (Florian Westphal) [1212624]
Rebuild_FUZZ: 95.00%
commit-author Florian Westphal <fw@strlen.de>
commit f7b3bec6f5167efaf56b756abfafb924cb1d3050
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/f7b3bec6.failed

This patch allows to set ECN on a per-route basis in case the sysctl
tcp_ecn is not set to 1. In other words, when ECN is set for specific
routes, it provides a tcp_ecn=1 behaviour for that route while the rest
of the stack acts according to the global settings.

One can use 'ip route change dev $dev $net features ecn' to toggle this.

Having a more fine-grained per-route setting can be beneficial for various
reasons, for example, 1) within data centers, or 2) local ISPs may deploy
ECN support for their own video/streaming services [1], etc.

There was a recent measurement study/paper [2] which scanned the Alexa's
publicly available top million websites list from a vantage point in US,
Europe and Asia:

Half of the Alexa list will now happily use ECN (tcp_ecn=2, most likely
blamed to commit 255cac91c3 ("tcp: extend ECN sysctl to allow server-side
only ECN") ;)); the break in connectivity on-path was found is about
1 in 10,000 cases. Timeouts rather than receiving back RSTs were much
more common in the negotiation phase (and mostly seen in the Alexa
middle band, ranks around 50k-150k): from 12-thousand hosts on which
there _may_ be ECN-linked connection failures, only 79 failed with RST
when _not_ failing with RST when ECN is not requested.

It's unclear though, how much equipment in the wild actually marks CE
when buffers start to fill up.

We thought about a fallback to non-ECN for retransmitted SYNs as another
global option (which could perhaps one day be made default), but as Eric
points out, there's much more work needed to detect broken middleboxes.

Two examples Eric mentioned are buggy firewalls that accept only a single
SYN per flow, and middleboxes that successfully let an ECN flow establish,
but later mark CE for all packets (so cwnd converges to 1).

 [1] http://www.ietf.org/proceedings/89/slides/slides-89-tsvarea-1.pdf, p.15
 [2] http://ecn.ethz.ch/

Joint work with Daniel Borkmann.

Reference: http://thread.gmane.org/gmane.linux.network/335797
	Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Acked-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
	Signed-off-by: Florian Westphal <fw@strlen.de>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit f7b3bec6f5167efaf56b756abfafb924cb1d3050)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/tcp.h
#	net/ipv4/syncookies.c
#	net/ipv4/tcp_input.c
#	net/ipv4/tcp_output.c
#	net/ipv6/syncookies.c
diff --cc include/net/tcp.h
index 0373da38f3bc,f50f29faf76f..000000000000
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@@ -497,39 -486,24 +497,50 @@@ static inline u32 tcp_cookie_time(void
  	return val;
  }
  
++<<<<<<< HEAD
 +extern u32 __cookie_v4_init_sequence(const struct iphdr *iph,
 +				     const struct tcphdr *th, u16 *mssp);
 +extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
 +				     __u16 *mss);
 +#else
 +static inline __u32 cookie_v4_init_sequence(struct sock *sk,
 +					    struct sk_buff *skb,
 +					    __u16 *mss)
 +{
 +	return 0;
 +}
 +#endif
 +
 +extern __u32 cookie_init_timestamp(struct request_sock *req);
 +extern bool cookie_check_timestamp(struct tcp_options_received *opt,
 +				struct net *net, bool *ecn_ok);
++=======
+ u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
+ 			      u16 *mssp);
+ __u32 cookie_v4_init_sequence(struct sock *sk, const struct sk_buff *skb,
+ 			      __u16 *mss);
+ __u32 cookie_init_timestamp(struct request_sock *req);
+ bool cookie_timestamp_decode(struct tcp_options_received *opt);
+ bool cookie_ecn_ok(const struct tcp_options_received *opt,
+ 		   const struct net *net, const struct dst_entry *dst);
++>>>>>>> f7b3bec6f516 (net: allow setting ecn via routing table)
  
  /* From net/ipv6/syncookies.c */
 -int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
 -		      u32 cookie);
 -struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 -
 -u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
 -			      const struct tcphdr *th, u16 *mssp);
 -__u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
 -			      __u16 *mss);
 +extern int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
 +			     u32 cookie);
 +extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 +#ifdef CONFIG_SYN_COOKIES
 +extern u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
 +				     const struct tcphdr *th, u16 *mssp);
 +extern __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
 +				     __u16 *mss);
 +#else
 +static inline __u32 cookie_v6_init_sequence(struct sock *sk,
 +					    struct sk_buff *skb,
 +					    __u16 *mss)
 +{
 +	return 0;
 +}
  #endif
  /* tcp_output.c */
  
diff --cc net/ipv4/syncookies.c
index 84b29ed816a8,45fe60c5238e..000000000000
--- a/net/ipv4/syncookies.c
+++ b/net/ipv4/syncookies.c
@@@ -277,11 -270,26 +277,32 @@@ bool cookie_check_timestamp(struct tcp_
  
  	return sysctl_tcp_window_scaling != 0;
  }
 -EXPORT_SYMBOL(cookie_timestamp_decode);
 +EXPORT_SYMBOL(cookie_check_timestamp);
  
++<<<<<<< HEAD
 +struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
 +			     struct ip_options *opt)
 +{
++=======
+ bool cookie_ecn_ok(const struct tcp_options_received *tcp_opt,
+ 		   const struct net *net, const struct dst_entry *dst)
+ {
+ 	bool ecn_ok = tcp_opt->rcv_tsecr & TS_OPT_ECN;
+ 
+ 	if (!ecn_ok)
+ 		return false;
+ 
+ 	if (net->ipv4.sysctl_tcp_ecn)
+ 		return true;
+ 
+ 	return dst_feature(dst, RTAX_FEATURE_ECN);
+ }
+ EXPORT_SYMBOL(cookie_ecn_ok);
+ 
+ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct ip_options *opt = &TCP_SKB_CB(skb)->header.h4.opt;
++>>>>>>> f7b3bec6f516 (net: allow setting ecn via routing table)
  	struct tcp_options_received tcp_opt;
  	struct inet_request_sock *ireq;
  	struct tcp_request_sock *treq;
@@@ -385,6 -387,7 +406,10 @@@
  				  dst_metric(&rt->dst, RTAX_INITRWND));
  
  	ireq->rcv_wscale  = rcv_wscale;
++<<<<<<< HEAD
++=======
+ 	ireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), &rt->dst);
++>>>>>>> f7b3bec6f516 (net: allow setting ecn via routing table)
  
  	ret = get_cookie_sock(sk, skb, req, &rt->dst);
  	/* ip_queue_xmit() depends on our flow being setup
diff --cc net/ipv4/tcp_input.c
index dc1a3a297c20,196b4388116c..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -5818,3 -5846,195 +5818,198 @@@ discard
  	return 0;
  }
  EXPORT_SYMBOL(tcp_rcv_state_process);
++<<<<<<< HEAD
++=======
+ 
+ static inline void pr_drop_req(struct request_sock *req, __u16 port, int family)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 
+ 	if (family == AF_INET)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI4/%u\n"),
+ 			       &ireq->ir_rmt_addr, port);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	else if (family == AF_INET6)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI6/%u\n"),
+ 			       &ireq->ir_v6_rmt_addr, port);
+ #endif
+ }
+ 
+ /* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set
+  *
+  * If we receive a SYN packet with these bits set, it means a
+  * network is playing bad games with TOS bits. In order to
+  * avoid possible false congestion notifications, we disable
+  * TCP ECN negotiation.
+  *
+  * Exception: tcp_ca wants ECN. This is required for DCTCP
+  * congestion control; it requires setting ECT on all packets,
+  * including SYN. We inverse the test in this case: If our
+  * local socket wants ECN, but peer only set ece/cwr (but not
+  * ECT in IP header) its probably a non-DCTCP aware sender.
+  */
+ static void tcp_ecn_create_request(struct request_sock *req,
+ 				   const struct sk_buff *skb,
+ 				   const struct sock *listen_sk,
+ 				   const struct dst_entry *dst)
+ {
+ 	const struct tcphdr *th = tcp_hdr(skb);
+ 	const struct net *net = sock_net(listen_sk);
+ 	bool th_ecn = th->ece && th->cwr;
+ 	bool ect, need_ecn, ecn_ok;
+ 
+ 	if (!th_ecn)
+ 		return;
+ 
+ 	ect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);
+ 	need_ecn = tcp_ca_needs_ecn(listen_sk);
+ 	ecn_ok = net->ipv4.sysctl_tcp_ecn || dst_feature(dst, RTAX_FEATURE_ECN);
+ 
+ 	if (!ect && !need_ecn && ecn_ok)
+ 		inet_rsk(req)->ecn_ok = 1;
+ 	else if (ect && need_ecn)
+ 		inet_rsk(req)->ecn_ok = 1;
+ }
+ 
+ int tcp_conn_request(struct request_sock_ops *rsk_ops,
+ 		     const struct tcp_request_sock_ops *af_ops,
+ 		     struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct tcp_options_received tmp_opt;
+ 	struct request_sock *req;
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 	struct dst_entry *dst = NULL;
+ 	__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;
+ 	bool want_cookie = false, fastopen;
+ 	struct flowi fl;
+ 	struct tcp_fastopen_cookie foc = { .len = -1 };
+ 	int err;
+ 
+ 
+ 	/* TW buckets are converted to open requests without
+ 	 * limitations, they conserve resources and peer is
+ 	 * evidently real one.
+ 	 */
+ 	if ((sysctl_tcp_syncookies == 2 ||
+ 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
+ 		want_cookie = tcp_syn_flood_action(sk, skb, rsk_ops->slab_name);
+ 		if (!want_cookie)
+ 			goto drop;
+ 	}
+ 
+ 
+ 	/* Accept backlog is full. If we have already queued enough
+ 	 * of warm entries in syn queue, drop request. It is better than
+ 	 * clogging syn queue with openreqs with exponentially increasing
+ 	 * timeout.
+ 	 */
+ 	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+ 		goto drop;
+ 	}
+ 
+ 	req = inet_reqsk_alloc(rsk_ops);
+ 	if (!req)
+ 		goto drop;
+ 
+ 	tcp_rsk(req)->af_specific = af_ops;
+ 
+ 	tcp_clear_options(&tmp_opt);
+ 	tmp_opt.mss_clamp = af_ops->mss_clamp;
+ 	tmp_opt.user_mss  = tp->rx_opt.user_mss;
+ 	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
+ 
+ 	if (want_cookie && !tmp_opt.saw_tstamp)
+ 		tcp_clear_options(&tmp_opt);
+ 
+ 	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+ 	tcp_openreq_init(req, &tmp_opt, skb, sk);
+ 
+ 	af_ops->init_req(req, sk, skb);
+ 
+ 	if (security_inet_conn_request(sk, skb, req))
+ 		goto drop_and_free;
+ 
+ 	if (!want_cookie && !isn) {
+ 		/* VJ's idea. We save last timestamp seen
+ 		 * from the destination in peer table, when entering
+ 		 * state TIME-WAIT, and check against it before
+ 		 * accepting new connection request.
+ 		 *
+ 		 * If "isn" is not zero, this request hit alive
+ 		 * timewait bucket, so that all the necessary checks
+ 		 * are made in the function processing timewait state.
+ 		 */
+ 		if (tcp_death_row.sysctl_tw_recycle) {
+ 			bool strict;
+ 
+ 			dst = af_ops->route_req(sk, &fl, req, &strict);
+ 
+ 			if (dst && strict &&
+ 			    !tcp_peer_is_proven(req, dst, true,
+ 						tmp_opt.saw_tstamp)) {
+ 				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
+ 				goto drop_and_release;
+ 			}
+ 		}
+ 		/* Kill the following clause, if you dislike this way. */
+ 		else if (!sysctl_tcp_syncookies &&
+ 			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
+ 			  (sysctl_max_syn_backlog >> 2)) &&
+ 			 !tcp_peer_is_proven(req, dst, false,
+ 					     tmp_opt.saw_tstamp)) {
+ 			/* Without syncookies last quarter of
+ 			 * backlog is filled with destinations,
+ 			 * proven to be alive.
+ 			 * It means that we continue to communicate
+ 			 * to destinations, already remembered
+ 			 * to the moment of synflood.
+ 			 */
+ 			pr_drop_req(req, ntohs(tcp_hdr(skb)->source),
+ 				    rsk_ops->family);
+ 			goto drop_and_release;
+ 		}
+ 
+ 		isn = af_ops->init_seq(skb);
+ 	}
+ 	if (!dst) {
+ 		dst = af_ops->route_req(sk, &fl, req, NULL);
+ 		if (!dst)
+ 			goto drop_and_free;
+ 	}
+ 
+ 	tcp_ecn_create_request(req, skb, sk, dst);
+ 
+ 	if (want_cookie) {
+ 		isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
+ 		req->cookie_ts = tmp_opt.tstamp_ok;
+ 		if (!tmp_opt.tstamp_ok)
+ 			inet_rsk(req)->ecn_ok = 0;
+ 	}
+ 
+ 	tcp_rsk(req)->snt_isn = isn;
+ 	tcp_openreq_init_rwin(req, sk, dst);
+ 	fastopen = !want_cookie &&
+ 		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+ 	err = af_ops->send_synack(sk, dst, &fl, req,
+ 				  skb_get_queue_mapping(skb), &foc);
+ 	if (!fastopen) {
+ 		if (err || want_cookie)
+ 			goto drop_and_free;
+ 
+ 		tcp_rsk(req)->listener = NULL;
+ 		af_ops->queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+ 	}
+ 
+ 	return 0;
+ 
+ drop_and_release:
+ 	dst_release(dst);
+ drop_and_free:
+ 	reqsk_free(req);
+ drop:
+ 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tcp_conn_request);
++>>>>>>> f7b3bec6f516 (net: allow setting ecn via routing table)
diff --cc net/ipv4/tcp_output.c
index 5018c0817473,0b88158dd4a7..000000000000
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@@ -323,14 -328,28 +323,28 @@@ static inline void TCP_ECN_send_synack(
  }
  
  /* Packet ECN state for a SYN.  */
 -static void tcp_ecn_send_syn(struct sock *sk, struct sk_buff *skb)
 +static inline void TCP_ECN_send_syn(struct sock *sk, struct sk_buff *skb)
  {
  	struct tcp_sock *tp = tcp_sk(sk);
+ 	bool use_ecn = sock_net(sk)->ipv4.sysctl_tcp_ecn == 1 ||
+ 		       tcp_ca_needs_ecn(sk);
+ 
+ 	if (!use_ecn) {
+ 		const struct dst_entry *dst = __sk_dst_get(sk);
+ 
+ 		if (dst && dst_feature(dst, RTAX_FEATURE_ECN))
+ 			use_ecn = true;
+ 	}
  
  	tp->ecn_flags = 0;
++<<<<<<< HEAD
 +	if (sock_net(sk)->ipv4.sysctl_tcp_ecn == 1) {
++=======
+ 
+ 	if (use_ecn) {
++>>>>>>> f7b3bec6f516 (net: allow setting ecn via routing table)
  		TCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ECE | TCPHDR_CWR;
  		tp->ecn_flags = TCP_ECN_OK;
 -		if (tcp_ca_needs_ecn(sk))
 -			INET_ECN_xmit(sk);
  	}
  }
  
diff --cc net/ipv6/syncookies.c
index d04d3f1dd9b7,7337fc7947e2..000000000000
--- a/net/ipv6/syncookies.c
+++ b/net/ipv6/syncookies.c
@@@ -253,6 -262,7 +253,10 @@@ struct sock *cookie_v6_check(struct soc
  				  dst_metric(dst, RTAX_INITRWND));
  
  	ireq->rcv_wscale = rcv_wscale;
++<<<<<<< HEAD
++=======
+ 	ireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);
++>>>>>>> f7b3bec6f516 (net: allow setting ecn via routing table)
  
  	ret = get_cookie_sock(sk, skb, req, dst);
  out:
* Unmerged path include/net/tcp.h
* Unmerged path net/ipv4/syncookies.c
* Unmerged path net/ipv4/tcp_input.c
* Unmerged path net/ipv4/tcp_output.c
* Unmerged path net/ipv6/syncookies.c
