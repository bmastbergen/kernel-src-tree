powernv/powerpc: Add winkle support for offline cpus

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] powernv: Add winkle support for offline cpus (Gustavo Duarte) [1123121]
Rebuild_FUZZ: 91.67%
commit-author Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
commit 77b54e9f213f76a23736940cf94bcd765fc00f40
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/77b54e9f.failed

Winkle is a deep idle state supported in power8 chips. A core enters
winkle when all the threads of the core enter winkle. In this state
power supply to the entire chiplet i.e core, private L2 and private L3
is turned off. As a result it gives higher powersavings compared to
sleep.

But entering winkle results in a total hypervisor state loss. Hence the
hypervisor context has to be preserved before entering winkle and
restored upon wake up.

Power-on Reset Engine (PORE) is a dedicated engine which is responsible
for powering on the chiplet during wake up. It can be programmed to
restore the register contests of a few specific registers. This patch
uses PORE to restore register state wherever possible and uses stack to
save and restore rest of the necessary registers.

With hypervisor state restore things fall under three categories-
per-core state, per-subcore state and per-thread state. To manage this,
extend the infrastructure introduced for sleep. Mainly we add a paca
variable subcore_sibling_mask. Using this and the core_idle_state we can
distingush first thread in core and subcore.

	Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: linuxppc-dev@lists.ozlabs.org
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 77b54e9f213f76a23736940cf94bcd765fc00f40)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/opal.h
#	arch/powerpc/include/asm/paca.h
#	arch/powerpc/include/asm/processor.h
#	arch/powerpc/kernel/asm-offsets.c
#	arch/powerpc/kernel/exceptions-64s.S
#	arch/powerpc/kernel/idle_power7.S
#	arch/powerpc/platforms/powernv/opal-wrappers.S
#	arch/powerpc/platforms/powernv/setup.c
#	arch/powerpc/platforms/powernv/smp.c
diff --cc arch/powerpc/include/asm/opal.h
index e795ae10954f,eb95b675109b..000000000000
--- a/arch/powerpc/include/asm/opal.h
+++ b/arch/powerpc/include/asm/opal.h
@@@ -146,9 -155,30 +146,34 @@@ struct opal_sg_list 
  #define OPAL_GET_PARAM				89
  #define OPAL_SET_PARAM				90
  #define OPAL_DUMP_RESEND			91
 -#define OPAL_PCI_SET_PHB_CXL_MODE		93
  #define OPAL_DUMP_INFO2				94
++<<<<<<< HEAD
 +#define OPAL_REGISTER_DUMP_REGION		101
 +#define OPAL_UNREGISTER_DUMP_REGION		102
++=======
+ #define OPAL_PCI_ERR_INJECT			96
+ #define OPAL_PCI_EEH_FREEZE_SET			97
+ #define OPAL_HANDLE_HMI				98
+ #define OPAL_CONFIG_CPU_IDLE_STATE		99
+ #define OPAL_SLW_SET_REG			100
+ #define OPAL_REGISTER_DUMP_REGION		101
+ #define OPAL_UNREGISTER_DUMP_REGION		102
+ #define OPAL_WRITE_TPO				103
+ #define OPAL_READ_TPO				104
+ #define OPAL_IPMI_SEND				107
+ #define OPAL_IPMI_RECV				108
+ #define OPAL_I2C_REQUEST			109
+ 
+ /* Device tree flags */
+ 
+ /* Flags set in power-mgmt nodes in device tree if
+  * respective idle states are supported in the platform.
+  */
+ #define OPAL_PM_NAP_ENABLED	0x00010000
+ #define OPAL_PM_SLEEP_ENABLED	0x00020000
+ #define OPAL_PM_WINKLE_ENABLED	0x00040000
+ #define OPAL_PM_SLEEP_ENABLED_ER1	0x00080000
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  
  #ifndef __ASSEMBLY__
  
@@@ -862,8 -912,17 +887,19 @@@ int64_t opal_get_param(uint64_t token, 
  int64_t opal_set_param(uint64_t token, uint32_t param_id, uint64_t buffer,
  		uint64_t length);
  int64_t opal_sensor_read(uint32_t sensor_hndl, int token, __be32 *sensor_data);
 -int64_t opal_handle_hmi(void);
  int64_t opal_register_dump_region(uint32_t id, uint64_t start, uint64_t end);
  int64_t opal_unregister_dump_region(uint32_t id);
++<<<<<<< HEAD
++=======
+ int64_t opal_slw_set_reg(uint64_t cpu_pir, uint64_t sprn, uint64_t val);
+ int64_t opal_pci_set_phb_cxl_mode(uint64_t phb_id, uint64_t mode, uint64_t pe_number);
+ int64_t opal_ipmi_send(uint64_t interface, struct opal_ipmi_msg *msg,
+ 		uint64_t msg_len);
+ int64_t opal_ipmi_recv(uint64_t interface, struct opal_ipmi_msg *msg,
+ 		uint64_t *msg_len);
+ int64_t opal_i2c_request(uint64_t async_token, uint32_t bus_id,
+ 			 struct opal_i2c_request *oreq);
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  
  /* Internal functions */
  extern int early_init_dt_scan_opal(unsigned long node, const char *uname,
diff --cc arch/powerpc/include/asm/paca.h
index 82270c09d005,e5f22c6c4bf9..000000000000
--- a/arch/powerpc/include/asm/paca.h
+++ b/arch/powerpc/include/asm/paca.h
@@@ -149,11 -153,15 +149,21 @@@ struct paca_struct 
  #endif
  
  #ifdef CONFIG_PPC_POWERNV
++<<<<<<< HEAD
 +	/* Pointer to OPAL machine check event structure set by the
 +	 * early exception handler for use by high level C handler
 +	 */
 +	struct opal_machine_check_event *opal_mc_evt;
++=======
+ 	/* Per-core mask tracking idle threads and a lock bit-[L][TTTTTTTT] */
+ 	u32 *core_idle_state_ptr;
+ 	u8 thread_idle_state;		/* PNV_THREAD_RUNNING/NAP/SLEEP	*/
+ 	/* Mask to indicate thread id in core */
+ 	u8 thread_mask;
+ 	/* Mask to denote subcore sibling threads */
+ 	u8 subcore_sibling_mask;
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  #endif
 -
  #ifdef CONFIG_PPC_BOOK3S_64
  	/* Exclusive emergency stack pointer for machine check exception. */
  	void *mc_emergency_sp;
diff --cc arch/powerpc/include/asm/processor.h
index 208b4e9bf7ec,bf117d8fb45f..000000000000
--- a/arch/powerpc/include/asm/processor.h
+++ b/arch/powerpc/include/asm/processor.h
@@@ -497,7 -452,8 +497,12 @@@ enum idle_boot_override {IDLE_NO_OVERRI
  
  extern int powersave_nap;	/* set if nap mode can be used in idle loop */
  extern unsigned long power7_nap(int check_irq);
++<<<<<<< HEAD
 +extern void power7_sleep(void);
++=======
+ extern unsigned long power7_sleep(void);
+ extern unsigned long power7_winkle(void);
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  extern void flush_instruction_cache(void);
  extern void hard_reset_now(void);
  extern void poweroff_now(void);
diff --cc arch/powerpc/kernel/asm-offsets.c
index 3b59d088fb8a,f68de7a73faa..000000000000
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@@ -719,10 -727,14 +719,21 @@@ int main(void
  #endif
  
  #ifdef CONFIG_PPC_POWERNV
++<<<<<<< HEAD
 +	DEFINE(OPAL_MC_GPR3, offsetof(struct opal_machine_check_event, gpr3));
 +	DEFINE(OPAL_MC_SRR0, offsetof(struct opal_machine_check_event, srr0));
 +	DEFINE(OPAL_MC_SRR1, offsetof(struct opal_machine_check_event, srr1));
 +	DEFINE(PACA_OPAL_MC_EVT, offsetof(struct paca_struct, opal_mc_evt));
++=======
+ 	DEFINE(PACA_CORE_IDLE_STATE_PTR,
+ 			offsetof(struct paca_struct, core_idle_state_ptr));
+ 	DEFINE(PACA_THREAD_IDLE_STATE,
+ 			offsetof(struct paca_struct, thread_idle_state));
+ 	DEFINE(PACA_THREAD_MASK,
+ 			offsetof(struct paca_struct, thread_mask));
+ 	DEFINE(PACA_SUBCORE_SIBLING_MASK,
+ 			offsetof(struct paca_struct, subcore_sibling_mask));
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  #endif
  
  	return 0;
diff --cc arch/powerpc/kernel/exceptions-64s.S
index 82b10f56e2a0,c2df8150bd7a..000000000000
--- a/arch/powerpc/kernel/exceptions-64s.S
+++ b/arch/powerpc/kernel/exceptions-64s.S
@@@ -109,15 -108,28 +107,40 @@@ BEGIN_FTR_SECTIO
  	rlwinm.	r13,r13,47-31,30,31
  	beq	9f
  
++<<<<<<< HEAD
 +	/* waking up from powersave (nap) state */
 +	cmpwi	cr1,r13,2
 +	/* Total loss of HV state is fatal, we could try to use the
 +	 * PIR to locate a PACA, then use an emergency stack etc...
 +	 * OPAL v3 based powernv platforms have new idle states
 +	 * which fall in this catagory.
 +	 */
 +	bgt	cr1,8f
 +	GET_PACA(r13)
++=======
+ 	cmpwi	cr3,r13,2
+ 
+ 	/*
+ 	 * Check if last bit of HSPGR0 is set. This indicates whether we are
+ 	 * waking up from winkle.
+ 	 */
+ 	GET_PACA(r13)
+ 	clrldi	r5,r13,63
+ 	clrrdi	r13,r13,1
+ 	cmpwi	cr4,r5,1
+ 	mtspr	SPRN_HSPRG0,r13
+ 
+ 	lbz	r0,PACA_THREAD_IDLE_STATE(r13)
+ 	cmpwi   cr2,r0,PNV_THREAD_NAP
+ 	bgt     cr2,8f				/* Either sleep or Winkle */
+ 
+ 	/* Waking up from nap should not cause hypervisor state loss */
+ 	bgt	cr3,.
+ 
+ 	/* Waking up from nap */
+ 	li	r0,PNV_THREAD_RUNNING
+ 	stb	r0,PACA_THREAD_IDLE_STATE(r13)	/* Clear thread state */
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  
  #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
  	li	r0,KVM_HWTHREAD_IN_KERNEL
diff --cc arch/powerpc/kernel/idle_power7.S
index 60fb4853cd9a,05adc8bbdef8..000000000000
--- a/arch/powerpc/kernel/idle_power7.S
+++ b/arch/powerpc/kernel/idle_power7.S
@@@ -18,6 -18,8 +18,11 @@@
  #include <asm/hw_irq.h>
  #include <asm/kvm_book3s_asm.h>
  #include <asm/opal.h>
++<<<<<<< HEAD
++=======
+ #include <asm/cpuidle.h>
+ #include <asm/mmu-hash64.h>
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  
  #undef DEBUG
  
@@@ -123,12 -138,83 +142,92 @@@ power7_enter_nap_mode
  	li	r4,KVM_HWTHREAD_IN_NAP
  	stb	r4,HSTATE_HWTHREAD_STATE(r13)
  #endif
++<<<<<<< HEAD
 +	cmpwi	cr0,r3,1
 +	beq	2f
 +	IDLE_STATE_ENTER_SEQ(PPC_NAP)
 +	/* No return */
 +2:	IDLE_STATE_ENTER_SEQ(PPC_SLEEP)
 +	/* No return */
++=======
+ 	stb	r3,PACA_THREAD_IDLE_STATE(r13)
+ 	cmpwi	cr3,r3,PNV_THREAD_SLEEP
+ 	bge	cr3,2f
+ 	IDLE_STATE_ENTER_SEQ(PPC_NAP)
+ 	/* No return */
+ 2:
+ 	/* Sleep or winkle */
+ 	lbz	r7,PACA_THREAD_MASK(r13)
+ 	ld	r14,PACA_CORE_IDLE_STATE_PTR(r13)
+ lwarx_loop1:
+ 	lwarx	r15,0,r14
+ 	andc	r15,r15,r7			/* Clear thread bit */
+ 
+ 	andi.	r15,r15,PNV_CORE_IDLE_THREAD_BITS
+ 
+ /*
+  * If cr0 = 0, then current thread is the last thread of the core entering
+  * sleep. Last thread needs to execute the hardware bug workaround code if
+  * required by the platform.
+  * Make the workaround call unconditionally here. The below branch call is
+  * patched out when the idle states are discovered if the platform does not
+  * require it.
+  */
+ .global pnv_fastsleep_workaround_at_entry
+ pnv_fastsleep_workaround_at_entry:
+ 	beq	fastsleep_workaround_at_entry
+ 
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop1
+ 	isync
+ 
+ common_enter: /* common code for all the threads entering sleep or winkle */
+ 	bgt	cr3,enter_winkle
+ 	IDLE_STATE_ENTER_SEQ(PPC_SLEEP)
+ 
+ fastsleep_workaround_at_entry:
+ 	ori	r15,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop1
+ 	isync
+ 
+ 	/* Fast sleep workaround */
+ 	li	r3,1
+ 	li	r4,1
+ 	li	r0,OPAL_CONFIG_CPU_IDLE_STATE
+ 	bl	opal_call_realmode
+ 
+ 	/* Clear Lock bit */
+ 	li	r0,0
+ 	lwsync
+ 	stw	r0,0(r14)
+ 	b	common_enter
+ 
+ enter_winkle:
+ 	/*
+ 	 * Note all register i.e per-core, per-subcore or per-thread is saved
+ 	 * here since any thread in the core might wake up first
+ 	 */
+ 	mfspr	r3,SPRN_SDR1
+ 	std	r3,_SDR1(r1)
+ 	mfspr	r3,SPRN_RPR
+ 	std	r3,_RPR(r1)
+ 	mfspr	r3,SPRN_SPURR
+ 	std	r3,_SPURR(r1)
+ 	mfspr	r3,SPRN_PURR
+ 	std	r3,_PURR(r1)
+ 	mfspr	r3,SPRN_TSCR
+ 	std	r3,_TSCR(r1)
+ 	mfspr	r3,SPRN_DSCR
+ 	std	r3,_DSCR(r1)
+ 	mfspr	r3,SPRN_AMOR
+ 	std	r3,_AMOR(r1)
+ 	mfspr	r3,SPRN_WORT
+ 	std	r3,_WORT(r1)
+ 	mfspr	r3,SPRN_WORC
+ 	std	r3,_WORC(r1)
+ 	IDLE_STATE_ENTER_SEQ(PPC_WINKLE)
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  
  _GLOBAL(power7_idle)
  	/* Now check if user or arch enabled NAP mode */
@@@ -151,20 -237,218 +250,223 @@@ _GLOBAL(power7_sleep
  	b	power7_powersave_common
  	/* No return */
  
++<<<<<<< HEAD
++=======
+ _GLOBAL(power7_winkle)
+ 	li	r3,3
+ 	li	r4,1
+ 	b	power7_powersave_common
+ 	/* No return */
+ 
+ #define CHECK_HMI_INTERRUPT						\
+ 	mfspr	r0,SPRN_SRR1;						\
+ BEGIN_FTR_SECTION_NESTED(66);						\
+ 	rlwinm	r0,r0,45-31,0xf;  /* extract wake reason field (P8) */	\
+ FTR_SECTION_ELSE_NESTED(66);						\
+ 	rlwinm	r0,r0,45-31,0xe;  /* P7 wake reason field is 3 bits */	\
+ ALT_FTR_SECTION_END_NESTED_IFSET(CPU_FTR_ARCH_207S, 66);		\
+ 	cmpwi	r0,0xa;			/* Hypervisor maintenance ? */	\
+ 	bne	20f;							\
+ 	/* Invoke opal call to handle hmi */				\
+ 	ld	r2,PACATOC(r13);					\
+ 	ld	r1,PACAR1(r13);						\
+ 	std	r3,ORIG_GPR3(r1);	/* Save original r3 */		\
+ 	li	r0,OPAL_HANDLE_HMI;	/* Pass opal token argument*/	\
+ 	bl	opal_call_realmode;					\
+ 	ld	r3,ORIG_GPR3(r1);	/* Restore original r3 */	\
+ 20:	nop;
+ 
+ 
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  _GLOBAL(power7_wakeup_tb_loss)
  	ld	r2,PACATOC(r13);
  	ld	r1,PACAR1(r13)
 -	/*
 -	 * Before entering any idle state, the NVGPRs are saved in the stack
 -	 * and they are restored before switching to the process context. Hence
 -	 * until they are restored, they are free to be used.
 -	 *
 -	 * Save SRR1 in a NVGPR as it might be clobbered in opal_call_realmode
 -	 * (called in CHECK_HMI_INTERRUPT). SRR1 is required to determine the
 -	 * wakeup reason if we branch to kvm_start_guest.
 -	 */
  
++<<<<<<< HEAD
++=======
+ 	mfspr	r16,SPRN_SRR1
+ BEGIN_FTR_SECTION
+ 	CHECK_HMI_INTERRUPT
+ END_FTR_SECTION_IFSET(CPU_FTR_HVMODE)
+ 
+ 	lbz	r7,PACA_THREAD_MASK(r13)
+ 	ld	r14,PACA_CORE_IDLE_STATE_PTR(r13)
+ lwarx_loop2:
+ 	lwarx	r15,0,r14
+ 	andi.	r9,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	/*
+ 	 * Lock bit is set in one of the 2 cases-
+ 	 * a. In the sleep/winkle enter path, the last thread is executing
+ 	 * fastsleep workaround code.
+ 	 * b. In the wake up path, another thread is executing fastsleep
+ 	 * workaround undo code or resyncing timebase or restoring context
+ 	 * In either case loop until the lock bit is cleared.
+ 	 */
+ 	bne	core_idle_lock_held
+ 
+ 	cmpwi	cr2,r15,0
+ 	lbz	r4,PACA_SUBCORE_SIBLING_MASK(r13)
+ 	and	r4,r4,r15
+ 	cmpwi	cr1,r4,0	/* Check if first in subcore */
+ 
+ 	/*
+ 	 * At this stage
+ 	 * cr1 - 0b0100 if first thread to wakeup in subcore
+ 	 * cr2 - 0b0100 if first thread to wakeup in core
+ 	 * cr3-  0b0010 if waking up from sleep or winkle
+ 	 * cr4 - 0b0100 if waking up from winkle
+ 	 */
+ 
+ 	or	r15,r15,r7		/* Set thread bit */
+ 
+ 	beq	cr1,first_thread_in_subcore
+ 
+ 	/* Not first thread in subcore to wake up */
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop2
+ 	isync
+ 	b	common_exit
+ 
+ core_idle_lock_held:
+ 	HMT_LOW
+ core_idle_lock_loop:
+ 	lwz	r15,0(14)
+ 	andi.   r9,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	bne	core_idle_lock_loop
+ 	HMT_MEDIUM
+ 	b	lwarx_loop2
+ 
+ first_thread_in_subcore:
+ 	/* First thread in subcore to wakeup */
+ 	ori	r15,r15,PNV_CORE_IDLE_LOCK_BIT
+ 	stwcx.	r15,0,r14
+ 	bne-	lwarx_loop2
+ 	isync
+ 
+ 	/*
+ 	 * If waking up from sleep, subcore state is not lost. Hence
+ 	 * skip subcore state restore
+ 	 */
+ 	bne	cr4,subcore_state_restored
+ 
+ 	/* Restore per-subcore state */
+ 	ld      r4,_SDR1(r1)
+ 	mtspr   SPRN_SDR1,r4
+ 	ld      r4,_RPR(r1)
+ 	mtspr   SPRN_RPR,r4
+ 	ld	r4,_AMOR(r1)
+ 	mtspr	SPRN_AMOR,r4
+ 
+ subcore_state_restored:
+ 	/*
+ 	 * Check if the thread is also the first thread in the core. If not,
+ 	 * skip to clear_lock.
+ 	 */
+ 	bne	cr2,clear_lock
+ 
+ first_thread_in_core:
+ 
+ 	/*
+ 	 * First thread in the core waking up from fastsleep. It needs to
+ 	 * call the fastsleep workaround code if the platform requires it.
+ 	 * Call it unconditionally here. The below branch instruction will
+ 	 * be patched out when the idle states are discovered if platform
+ 	 * does not require workaround.
+ 	 */
+ .global pnv_fastsleep_workaround_at_exit
+ pnv_fastsleep_workaround_at_exit:
+ 	b	fastsleep_workaround_at_exit
+ 
+ timebase_resync:
+ 	/* Do timebase resync if we are waking up from sleep. Use cr3 value
+ 	 * set in exceptions-64s.S */
+ 	ble	cr3,clear_lock
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  	/* Time base re-sync */
  	li	r0,OPAL_RESYNC_TIMEBASE
 -	bl	opal_call_realmode;
 +	LOAD_REG_ADDR(r11,opal);
 +	ld	r12,8(r11);
 +	ld	r2,0(r11);
 +	mtctr	r12
 +	bctrl
 +
  	/* TODO: Check r3 for failure */
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * If waking up from sleep, per core state is not lost, skip to
+ 	 * clear_lock.
+ 	 */
+ 	bne	cr4,clear_lock
+ 
+ 	/* Restore per core state */
+ 	ld	r4,_TSCR(r1)
+ 	mtspr	SPRN_TSCR,r4
+ 	ld	r4,_WORC(r1)
+ 	mtspr	SPRN_WORC,r4
+ 
+ clear_lock:
+ 	andi.	r15,r15,PNV_CORE_IDLE_THREAD_BITS
+ 	lwsync
+ 	stw	r15,0(r14)
+ 
+ common_exit:
+ 	/*
+ 	 * Common to all threads.
+ 	 *
+ 	 * If waking up from sleep, hypervisor state is not lost. Hence
+ 	 * skip hypervisor state restore.
+ 	 */
+ 	bne	cr4,hypervisor_state_restored
+ 
+ 	/* Waking up from winkle */
+ 
+ 	/* Restore per thread state */
+ 	bl	__restore_cpu_power8
+ 
+ 	/* Restore SLB  from PACA */
+ 	ld	r8,PACA_SLBSHADOWPTR(r13)
+ 
+ 	.rept	SLB_NUM_BOLTED
+ 	li	r3, SLBSHADOW_SAVEAREA
+ 	LDX_BE	r5, r8, r3
+ 	addi	r3, r3, 8
+ 	LDX_BE	r6, r8, r3
+ 	andis.	r7,r5,SLB_ESID_V@h
+ 	beq	1f
+ 	slbmte	r6,r5
+ 1:	addi	r8,r8,16
+ 	.endr
+ 
+ 	ld	r4,_SPURR(r1)
+ 	mtspr	SPRN_SPURR,r4
+ 	ld	r4,_PURR(r1)
+ 	mtspr	SPRN_PURR,r4
+ 	ld	r4,_DSCR(r1)
+ 	mtspr	SPRN_DSCR,r4
+ 	ld	r4,_WORT(r1)
+ 	mtspr	SPRN_WORT,r4
+ 
+ hypervisor_state_restored:
+ 
+ 	li	r5,PNV_THREAD_RUNNING
+ 	stb     r5,PACA_THREAD_IDLE_STATE(r13)
+ 
+ 	mtspr	SPRN_SRR1,r16
+ #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+ 	li      r0,KVM_HWTHREAD_IN_KERNEL
+ 	stb     r0,HSTATE_HWTHREAD_STATE(r13)
+ 	/* Order setting hwthread_state vs. testing hwthread_req */
+ 	sync
+ 	lbz     r0,HSTATE_HWTHREAD_REQ(r13)
+ 	cmpwi   r0,0
+ 	beq     6f
+ 	b       kvm_start_guest
+ 6:
+ #endif
+ 
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  	REST_NVGPRS(r1)
  	REST_GPR(2, r1)
  	ld	r3,_CCR(r1)
diff --cc arch/powerpc/platforms/powernv/opal-wrappers.S
index 0f016cb7a5f5,54eca8b3b288..000000000000
--- a/arch/powerpc/platforms/powernv/opal-wrappers.S
+++ b/arch/powerpc/platforms/powernv/opal-wrappers.S
@@@ -146,5 -283,13 +146,10 @@@ OPAL_CALL(opal_sync_host_reboot,		OPAL_
  OPAL_CALL(opal_sensor_read,			OPAL_SENSOR_READ);
  OPAL_CALL(opal_get_param,			OPAL_GET_PARAM);
  OPAL_CALL(opal_set_param,			OPAL_SET_PARAM);
++<<<<<<< HEAD
++=======
+ OPAL_CALL(opal_handle_hmi,			OPAL_HANDLE_HMI);
+ OPAL_CALL(opal_slw_set_reg,			OPAL_SLW_SET_REG);
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  OPAL_CALL(opal_register_dump_region,		OPAL_REGISTER_DUMP_REGION);
  OPAL_CALL(opal_unregister_dump_region,		OPAL_UNREGISTER_DUMP_REGION);
 -OPAL_CALL(opal_pci_set_phb_cxl_mode,		OPAL_PCI_SET_PHB_CXL_MODE);
 -OPAL_CALL(opal_tpo_write,			OPAL_WRITE_TPO);
 -OPAL_CALL(opal_tpo_read,			OPAL_READ_TPO);
 -OPAL_CALL(opal_ipmi_send,			OPAL_IPMI_SEND);
 -OPAL_CALL(opal_ipmi_recv,			OPAL_IPMI_RECV);
 -OPAL_CALL(opal_i2c_request,			OPAL_I2C_REQUEST);
diff --cc arch/powerpc/platforms/powernv/setup.c
index d9b88fa7c5a3,b700a329c31d..000000000000
--- a/arch/powerpc/platforms/powernv/setup.c
+++ b/arch/powerpc/platforms/powernv/setup.c
@@@ -36,8 -36,12 +36,9 @@@
  #include <asm/opal.h>
  #include <asm/kexec.h>
  #include <asm/smp.h>
 -#include <asm/cputhreads.h>
 -#include <asm/cpuidle.h>
 -#include <asm/code-patching.h>
  
  #include "powernv.h"
+ #include "subcore.h"
  
  static void __init pnv_setup_arch(void)
  {
@@@ -280,6 -292,168 +281,171 @@@ static void __init pnv_setup_machdep_rt
  }
  #endif /* CONFIG_PPC_POWERNV_RTAS */
  
++<<<<<<< HEAD
++=======
+ static u32 supported_cpuidle_states;
+ 
+ int pnv_save_sprs_for_winkle(void)
+ {
+ 	int cpu;
+ 	int rc;
+ 
+ 	/*
+ 	 * hid0, hid1, hid4, hid5, hmeer and lpcr values are symmetric accross
+ 	 * all cpus at boot. Get these reg values of current cpu and use the
+ 	 * same accross all cpus.
+ 	 */
+ 	uint64_t lpcr_val = mfspr(SPRN_LPCR);
+ 	uint64_t hid0_val = mfspr(SPRN_HID0);
+ 	uint64_t hid1_val = mfspr(SPRN_HID1);
+ 	uint64_t hid4_val = mfspr(SPRN_HID4);
+ 	uint64_t hid5_val = mfspr(SPRN_HID5);
+ 	uint64_t hmeer_val = mfspr(SPRN_HMEER);
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		uint64_t pir = get_hard_smp_processor_id(cpu);
+ 		uint64_t hsprg0_val = (uint64_t)&paca[cpu];
+ 
+ 		/*
+ 		 * HSPRG0 is used to store the cpu's pointer to paca. Hence last
+ 		 * 3 bits are guaranteed to be 0. Program slw to restore HSPRG0
+ 		 * with 63rd bit set, so that when a thread wakes up at 0x100 we
+ 		 * can use this bit to distinguish between fastsleep and
+ 		 * deep winkle.
+ 		 */
+ 		hsprg0_val |= 1;
+ 
+ 		rc = opal_slw_set_reg(pir, SPRN_HSPRG0, hsprg0_val);
+ 		if (rc != 0)
+ 			return rc;
+ 
+ 		rc = opal_slw_set_reg(pir, SPRN_LPCR, lpcr_val);
+ 		if (rc != 0)
+ 			return rc;
+ 
+ 		/* HIDs are per core registers */
+ 		if (cpu_thread_in_core(cpu) == 0) {
+ 
+ 			rc = opal_slw_set_reg(pir, SPRN_HMEER, hmeer_val);
+ 			if (rc != 0)
+ 				return rc;
+ 
+ 			rc = opal_slw_set_reg(pir, SPRN_HID0, hid0_val);
+ 			if (rc != 0)
+ 				return rc;
+ 
+ 			rc = opal_slw_set_reg(pir, SPRN_HID1, hid1_val);
+ 			if (rc != 0)
+ 				return rc;
+ 
+ 			rc = opal_slw_set_reg(pir, SPRN_HID4, hid4_val);
+ 			if (rc != 0)
+ 				return rc;
+ 
+ 			rc = opal_slw_set_reg(pir, SPRN_HID5, hid5_val);
+ 			if (rc != 0)
+ 				return rc;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void pnv_alloc_idle_core_states(void)
+ {
+ 	int i, j;
+ 	int nr_cores = cpu_nr_cores();
+ 	u32 *core_idle_state;
+ 
+ 	/*
+ 	 * core_idle_state - First 8 bits track the idle state of each thread
+ 	 * of the core. The 8th bit is the lock bit. Initially all thread bits
+ 	 * are set. They are cleared when the thread enters deep idle state
+ 	 * like sleep and winkle. Initially the lock bit is cleared.
+ 	 * The lock bit has 2 purposes
+ 	 * a. While the first thread is restoring core state, it prevents
+ 	 * other threads in the core from switching to process context.
+ 	 * b. While the last thread in the core is saving the core state, it
+ 	 * prevents a different thread from waking up.
+ 	 */
+ 	for (i = 0; i < nr_cores; i++) {
+ 		int first_cpu = i * threads_per_core;
+ 		int node = cpu_to_node(first_cpu);
+ 
+ 		core_idle_state = kmalloc_node(sizeof(u32), GFP_KERNEL, node);
+ 		*core_idle_state = PNV_CORE_IDLE_THREAD_BITS;
+ 
+ 		for (j = 0; j < threads_per_core; j++) {
+ 			int cpu = first_cpu + j;
+ 
+ 			paca[cpu].core_idle_state_ptr = core_idle_state;
+ 			paca[cpu].thread_idle_state = PNV_THREAD_RUNNING;
+ 			paca[cpu].thread_mask = 1 << j;
+ 		}
+ 	}
+ 
+ 	update_subcore_sibling_mask();
+ 
+ 	if (supported_cpuidle_states & OPAL_PM_WINKLE_ENABLED)
+ 		pnv_save_sprs_for_winkle();
+ }
+ 
+ u32 pnv_get_supported_cpuidle_states(void)
+ {
+ 	return supported_cpuidle_states;
+ }
+ EXPORT_SYMBOL_GPL(pnv_get_supported_cpuidle_states);
+ 
+ static int __init pnv_init_idle_states(void)
+ {
+ 	struct device_node *power_mgt;
+ 	int dt_idle_states;
+ 	const __be32 *idle_state_flags;
+ 	u32 len_flags, flags;
+ 	int i;
+ 
+ 	supported_cpuidle_states = 0;
+ 
+ 	if (cpuidle_disable != IDLE_NO_OVERRIDE)
+ 		return 0;
+ 
+ 	if (!firmware_has_feature(FW_FEATURE_OPALv3))
+ 		return 0;
+ 
+ 	power_mgt = of_find_node_by_path("/ibm,opal/power-mgt");
+ 	if (!power_mgt) {
+ 		pr_warn("opal: PowerMgmt Node not found\n");
+ 		return 0;
+ 	}
+ 
+ 	idle_state_flags = of_get_property(power_mgt,
+ 			"ibm,cpu-idle-state-flags", &len_flags);
+ 	if (!idle_state_flags) {
+ 		pr_warn("DT-PowerMgmt: missing ibm,cpu-idle-state-flags\n");
+ 		return 0;
+ 	}
+ 
+ 	dt_idle_states = len_flags / sizeof(u32);
+ 
+ 	for (i = 0; i < dt_idle_states; i++) {
+ 		flags = be32_to_cpu(idle_state_flags[i]);
+ 		supported_cpuidle_states |= flags;
+ 	}
+ 	if (!(supported_cpuidle_states & OPAL_PM_SLEEP_ENABLED_ER1)) {
+ 		patch_instruction(
+ 			(unsigned int *)pnv_fastsleep_workaround_at_entry,
+ 			PPC_INST_NOP);
+ 		patch_instruction(
+ 			(unsigned int *)pnv_fastsleep_workaround_at_exit,
+ 			PPC_INST_NOP);
+ 	}
+ 	pnv_alloc_idle_core_states();
+ 	return 0;
+ }
+ 
+ subsys_initcall(pnv_init_idle_states);
+ 
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  static int __init pnv_probe(void)
  {
  	unsigned long root = of_get_flat_dt_root();
diff --cc arch/powerpc/platforms/powernv/smp.c
index 1c14ba216c76,6c551a28e899..000000000000
--- a/arch/powerpc/platforms/powernv/smp.c
+++ b/arch/powerpc/platforms/powernv/smp.c
@@@ -165,8 -167,17 +165,21 @@@ static void pnv_smp_cpu_kill_self(void
  	 */
  	mtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~(u64)LPCR_PECE1);
  	while (!generic_check_cpu_restart(cpu)) {
+ 
  		ppc64_runlatch_off();
++<<<<<<< HEAD
 +		srr1 = power7_nap(1);
++=======
+ 
+ 		if (idle_states & OPAL_PM_WINKLE_ENABLED)
+ 			srr1 = power7_winkle();
+ 		else if ((idle_states & OPAL_PM_SLEEP_ENABLED) ||
+ 				(idle_states & OPAL_PM_SLEEP_ENABLED_ER1))
+ 			srr1 = power7_sleep();
+ 		else
+ 			srr1 = power7_nap(1);
+ 
++>>>>>>> 77b54e9f213f (powernv/powerpc: Add winkle support for offline cpus)
  		ppc64_runlatch_on();
  
  		/*
* Unmerged path arch/powerpc/include/asm/opal.h
* Unmerged path arch/powerpc/include/asm/paca.h
diff --git a/arch/powerpc/include/asm/ppc-opcode.h b/arch/powerpc/include/asm/ppc-opcode.h
index 3132bb9365f3..f07f4f589896 100644
--- a/arch/powerpc/include/asm/ppc-opcode.h
+++ b/arch/powerpc/include/asm/ppc-opcode.h
@@ -191,6 +191,7 @@
 
 #define PPC_INST_NAP			0x4c000364
 #define PPC_INST_SLEEP			0x4c0003a4
+#define PPC_INST_WINKLE			0x4c0003e4
 
 /* A2 specific instructions */
 #define PPC_INST_ERATWE			0x7c0001a6
@@ -355,6 +356,7 @@
 
 #define PPC_NAP			stringify_in_c(.long PPC_INST_NAP)
 #define PPC_SLEEP		stringify_in_c(.long PPC_INST_SLEEP)
+#define PPC_WINKLE		stringify_in_c(.long PPC_INST_WINKLE)
 
 /* BHRB instructions */
 #define PPC_CLRBHRB		stringify_in_c(.long PPC_INST_CLRBHRB)
* Unmerged path arch/powerpc/include/asm/processor.h
diff --git a/arch/powerpc/include/asm/reg.h b/arch/powerpc/include/asm/reg.h
index aef5d642c625..14cf04dcf9db 100644
--- a/arch/powerpc/include/asm/reg.h
+++ b/arch/powerpc/include/asm/reg.h
@@ -373,6 +373,7 @@
 #define SPRN_DBAT7L	0x23F	/* Data BAT 7 Lower Register */
 #define SPRN_DBAT7U	0x23E	/* Data BAT 7 Upper Register */
 #define SPRN_PPR	0x380	/* SMT Thread status Register */
+#define SPRN_TSCR	0x399	/* Thread Switch Control Register */
 
 #define SPRN_DEC	0x016		/* Decrement Register */
 #define SPRN_DER	0x095		/* Debug Enable Regsiter */
@@ -726,6 +727,7 @@
 #define SPRN_BESCR	806	/* Branch event status and control register */
 #define   BESCR_GE	0x8000000000000000ULL /* Global Enable */
 #define SPRN_WORT	895	/* Workload optimization register - thread */
+#define SPRN_WORC	863	/* Workload optimization register - core */
 
 #define SPRN_PMC1	787
 #define SPRN_PMC2	788
* Unmerged path arch/powerpc/kernel/asm-offsets.c
* Unmerged path arch/powerpc/kernel/exceptions-64s.S
* Unmerged path arch/powerpc/kernel/idle_power7.S
* Unmerged path arch/powerpc/platforms/powernv/opal-wrappers.S
* Unmerged path arch/powerpc/platforms/powernv/setup.c
* Unmerged path arch/powerpc/platforms/powernv/smp.c
diff --git a/arch/powerpc/platforms/powernv/subcore.c b/arch/powerpc/platforms/powernv/subcore.c
index 894ecb3eb596..c34567142614 100644
--- a/arch/powerpc/platforms/powernv/subcore.c
+++ b/arch/powerpc/platforms/powernv/subcore.c
@@ -159,6 +159,18 @@ static void wait_for_sync_step(int step)
 	mb();
 }
 
+static void update_hid_in_slw(u64 hid0)
+{
+	u64 idle_states = pnv_get_supported_cpuidle_states();
+
+	if (idle_states & OPAL_PM_WINKLE_ENABLED) {
+		/* OPAL call to patch slw with the new HID0 value */
+		u64 cpu_pir = hard_smp_processor_id();
+
+		opal_slw_set_reg(cpu_pir, SPRN_HID0, hid0);
+	}
+}
+
 static void unsplit_core(void)
 {
 	u64 hid0, mask;
@@ -178,6 +190,7 @@ static void unsplit_core(void)
 	hid0 = mfspr(SPRN_HID0);
 	hid0 &= ~HID0_POWER8_DYNLPARDIS;
 	mtspr(SPRN_HID0, hid0);
+	update_hid_in_slw(hid0);
 
 	while (mfspr(SPRN_HID0) & mask)
 		cpu_relax();
@@ -214,6 +227,7 @@ static void split_core(int new_mode)
 	hid0  = mfspr(SPRN_HID0);
 	hid0 |= HID0_POWER8_DYNLPARDIS | split_parms[i].value;
 	mtspr(SPRN_HID0, hid0);
+	update_hid_in_slw(hid0);
 
 	/* Wait for it to happen */
 	while (!(mfspr(SPRN_HID0) & split_parms[i].mask))
@@ -250,6 +264,25 @@ bool cpu_core_split_required(void)
 	return true;
 }
 
+void update_subcore_sibling_mask(void)
+{
+	int cpu;
+	/*
+	 * sibling mask for the first cpu. Left shift this by required bits
+	 * to get sibling mask for the rest of the cpus.
+	 */
+	int sibling_mask_first_cpu =  (1 << threads_per_subcore) - 1;
+
+	for_each_possible_cpu(cpu) {
+		int tid = cpu_thread_in_core(cpu);
+		int offset = (tid / threads_per_subcore) * threads_per_subcore;
+		int mask = sibling_mask_first_cpu << offset;
+
+		paca[cpu].subcore_sibling_mask = mask;
+
+	}
+}
+
 static int cpu_update_split_mode(void *data)
 {
 	int cpu, new_mode = *(int *)data;
@@ -283,6 +316,7 @@ static int cpu_update_split_mode(void *data)
 		/* Make the new mode public */
 		subcores_per_core = new_mode;
 		threads_per_subcore = threads_per_core / subcores_per_core;
+		update_subcore_sibling_mask();
 
 		/* Make sure the new mode is written before we exit */
 		mb();
diff --git a/arch/powerpc/platforms/powernv/subcore.h b/arch/powerpc/platforms/powernv/subcore.h
index 148abc91debf..84e02ae52895 100644
--- a/arch/powerpc/platforms/powernv/subcore.h
+++ b/arch/powerpc/platforms/powernv/subcore.h
@@ -14,5 +14,12 @@
 #define SYNC_STEP_FINISHED	3	/* Set by secondary when split/unsplit is done */
 
 #ifndef __ASSEMBLY__
+
+#ifdef CONFIG_SMP
 void split_core_secondary_loop(u8 *state);
-#endif
+extern void update_subcore_sibling_mask(void);
+#else
+static inline void update_subcore_sibling_mask(void) { };
+#endif /* CONFIG_SMP */
+
+#endif /* __ASSEMBLY__ */
