blk-mq: avoid inserting requests before establishing new mapping

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Akinobu Mita <akinobu.mita@gmail.com>
commit 5778322e67ed34dc9f391a4a5cbcbb856071ceba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/5778322e.failed

Notifier callbacks for CPU_ONLINE action can be run on the other CPU
than the CPU which was just onlined.  So it is possible for the
process running on the just onlined CPU to insert request and run
hw queue before establishing new mapping which is done by
blk_mq_queue_reinit_notify().

This can cause a problem when the CPU has just been onlined first time
since the request queue was initialized.  At this time ctx->index_hw
for the CPU, which is the index in hctx->ctxs[] for this ctx, is still
zero before blk_mq_queue_reinit_notify() is called by notifier
callbacks for CPU_ONLINE action.

For example, there is a single hw queue (hctx) and two CPU queues
(ctx0 for CPU0, and ctx1 for CPU1).  Now CPU1 is just onlined and
a request is inserted into ctx1->rq_list and set bit0 in pending
bitmap as ctx1->index_hw is still zero.

And then while running hw queue, flush_busy_ctxs() finds bit0 is set
in pending bitmap and tries to retrieve requests in
hctx->ctxs[0]->rq_list.  But htx->ctxs[0] is a pointer to ctx0, so the
request in ctx1->rq_list is ignored.

Fix it by ensuring that new mapping is established before onlined cpu
starts running.

	Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
	Reviewed-by: Ming Lei <tom.leiming@gmail.com>
	Cc: Jens Axboe <axboe@kernel.dk>
	Cc: Ming Lei <tom.leiming@gmail.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 5778322e67ed34dc9f391a4a5cbcbb856071ceba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index c84efe52e26a,a5dbd069c9da..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1894,25 -2036,20 +1895,33 @@@ struct request_queue *blk_mq_init_queue
  	blk_mq_init_cpu_queues(q, set->nr_hw_queues);
  
  	if (blk_mq_init_hw_queues(q, set))
 -		goto err_hctxs;
 +		goto err_hw;
  
+ 	get_online_cpus();
  	mutex_lock(&all_q_mutex);
 -
  	list_add_tail(&q->all_q_node, &all_q_list);
++<<<<<<< HEAD
++=======
+ 	blk_mq_add_queue_tag_set(set, q);
+ 	blk_mq_map_swqueue(q, cpu_online_mask);
+ 
++>>>>>>> 5778322e67ed (blk-mq: avoid inserting requests before establishing new mapping)
  	mutex_unlock(&all_q_mutex);
+ 	put_online_cpus();
  
 +	blk_mq_add_queue_tag_set(set, q);
 +
 +	if (blk_mq_init_flush(q))
 +		goto err_hw_queues;
 +
 +	blk_mq_map_swqueue(q);
 +
  	return q;
  
 +err_hw_queues:
 +	blk_mq_exit_hw_queues(q, set, set->nr_hw_queues);
 +err_hw:
 +	blk_cleanup_queue(q);
  err_hctxs:
  	kfree(map);
  	for (i = 0; i < set->nr_hw_queues; i++) {
@@@ -1954,9 -2083,10 +1963,10 @@@ void blk_mq_free_queue(struct request_q
  }
  
  /* Basically redo blk_mq_init_queue with queue frozen */
- static void blk_mq_queue_reinit(struct request_queue *q)
+ static void blk_mq_queue_reinit(struct request_queue *q,
+ 				const struct cpumask *online_mask)
  {
 -	WARN_ON_ONCE(!atomic_read(&q->mq_freeze_depth));
 +	WARN_ON_ONCE(!q->mq_freeze_depth);
  
  	blk_mq_sysfs_unregister(q);
  
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 5f13f4d0bcce..99f65c0773ba 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -31,7 +31,8 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
-int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues)
+int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues,
+			    const struct cpumask *online_mask)
 {
 	unsigned int i, nr_cpus, nr_uniq_cpus, queue, first_sibling;
 	cpumask_var_t cpus;
@@ -41,7 +42,7 @@ int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues)
 
 	cpumask_clear(cpus);
 	nr_cpus = nr_uniq_cpus = 0;
-	for_each_online_cpu(i) {
+	for_each_cpu(i, online_mask) {
 		nr_cpus++;
 		first_sibling = get_first_sibling(i);
 		if (!cpumask_test_cpu(first_sibling, cpus))
@@ -51,7 +52,7 @@ int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues)
 
 	queue = 0;
 	for_each_possible_cpu(i) {
-		if (!cpu_online(i)) {
+		if (!cpumask_test_cpu(i, online_mask)) {
 			map[i] = 0;
 			continue;
 		}
@@ -95,7 +96,7 @@ unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set)
 	if (!map)
 		return NULL;
 
-	if (!blk_mq_update_queue_map(map, set->nr_hw_queues))
+	if (!blk_mq_update_queue_map(map, set->nr_hw_queues, cpu_online_mask))
 		return map;
 
 	kfree(map);
* Unmerged path block/blk-mq.c
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 488710318b99..644b96b1ccec 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -55,7 +55,8 @@ void blk_mq_disable_hotplug(void);
  * CPU -> queue mappings
  */
 extern unsigned int *blk_mq_make_queue_map(struct blk_mq_tag_set *set);
-extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues);
+extern int blk_mq_update_queue_map(unsigned int *map, unsigned int nr_queues,
+				   const struct cpumask *online_mask);
 extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 
 /*
