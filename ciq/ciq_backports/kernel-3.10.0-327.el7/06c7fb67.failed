IB/iser: Micro-optimize iser_handle_wc

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [infiniband] iser: Micro-optimize iser_handle_wc (Amir Vadai) [1164539]
Rebuild_FUZZ: 95.89%
commit-author Sagi Grimberg <sagig@mellanox.com>
commit 06c7fb6776ddb0ece4bcee8061eeda4ed4a771dc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/06c7fb67.failed

Use likely() for wc.status == IB_WC_SUCCESS

	Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Roland Dreier <roland@purestorage.com>
(cherry picked from commit 06c7fb6776ddb0ece4bcee8061eeda4ed4a771dc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/ulp/iser/iser_verbs.c
diff --cc drivers/infiniband/ulp/iser/iser_verbs.c
index 4cafd44ad193,695a2704bd43..000000000000
--- a/drivers/infiniband/ulp/iser/iser_verbs.c
+++ b/drivers/infiniband/ulp/iser/iser_verbs.c
@@@ -1118,81 -1215,80 +1118,94 @@@ static void iser_handle_comp_error(stru
  	}
  }
  
 -/**
 - * iser_handle_wc - handle a single work completion
 - * @wc: work completion
 - *
 - * Soft-IRQ context, work completion can be either
 - * SEND or RECV, and can turn out successful or
 - * with error (or flush error).
 - */
 -static void iser_handle_wc(struct ib_wc *wc)
 +static int iser_drain_tx_cq(struct iser_device  *device, int cq_index)
  {
 -	struct ib_conn *ib_conn;
 +	struct ib_cq  *cq = device->tx_cq[cq_index];
 +	struct ib_wc  wc;
  	struct iser_tx_desc *tx_desc;
 -	struct iser_rx_desc *rx_desc;
 -
 +	struct iser_conn *ib_conn;
 +	int completed_tx = 0;
 +
++<<<<<<< HEAD
 +	while (ib_poll_cq(cq, 1, &wc) == 1) {
 +		tx_desc	= (struct iser_tx_desc *) (unsigned long) wc.wr_id;
 +		ib_conn = wc.qp->qp_context;
 +		if (wc.status == IB_WC_SUCCESS) {
 +			if (wc.opcode == IB_WC_SEND)
 +				iser_snd_completion(tx_desc, ib_conn);
 +			else
 +				iser_err("expected opcode %d got %d\n",
 +					IB_WC_SEND, wc.opcode);
++=======
+ 	ib_conn = wc->qp->qp_context;
+ 	if (likely(wc->status == IB_WC_SUCCESS)) {
+ 		if (wc->opcode == IB_WC_RECV) {
+ 			rx_desc = (struct iser_rx_desc *)(uintptr_t)wc->wr_id;
+ 			iser_rcv_completion(rx_desc, wc->byte_len,
+ 					    ib_conn);
+ 		} else
+ 		if (wc->opcode == IB_WC_SEND) {
+ 			tx_desc = (struct iser_tx_desc *)(uintptr_t)wc->wr_id;
+ 			iser_snd_completion(tx_desc, ib_conn);
++>>>>>>> 06c7fb6776dd (IB/iser: Micro-optimize iser_handle_wc)
  		} else {
 -			iser_err("Unknown wc opcode %d\n", wc->opcode);
 +			iser_err("tx id %llx status %d vend_err %x\n",
 +				 wc.wr_id, wc.status, wc.vendor_err);
 +			if (wc.wr_id != ISER_FASTREG_LI_WRID) {
 +				atomic_dec(&ib_conn->post_send_buf_count);
 +				iser_handle_comp_error(tx_desc, ib_conn);
 +			}
  		}
 -	} else {
 -		if (wc->status != IB_WC_WR_FLUSH_ERR)
 -			iser_err("wr id %llx status %d vend_err %x\n",
 -				 wc->wr_id, wc->status, wc->vendor_err);
 -		else
 -			iser_dbg("flush error: wr id %llx\n", wc->wr_id);
 -
 -		if (wc->wr_id != ISER_FASTREG_LI_WRID &&
 -		    wc->wr_id != ISER_BEACON_WRID)
 -			iser_handle_comp_error(ib_conn, wc);
 -
 -		/* complete in case all flush errors were consumed */
 -		if (wc->wr_id == ISER_BEACON_WRID)
 -			complete(&ib_conn->flush_comp);
 +		completed_tx++;
  	}
 +	return completed_tx;
  }
  
 -/**
 - * iser_cq_tasklet_fn - iSER completion polling loop
 - * @data: iSER completion context
 - *
 - * Soft-IRQ context, polling connection CQ until
 - * either CQ was empty or we exausted polling budget
 - */
 +
  static void iser_cq_tasklet_fn(unsigned long data)
  {
 -	struct iser_comp *comp = (struct iser_comp *)data;
 -	struct ib_cq *cq = comp->cq;
 -	struct ib_wc *const wcs = comp->wcs;
 -	int i, n, completed = 0;
 -
 -	while ((n = ib_poll_cq(cq, ARRAY_SIZE(comp->wcs), wcs)) > 0) {
 -		for (i = 0; i < n; i++)
 -			iser_handle_wc(&wcs[i]);
 -
 -		completed += n;
 -		if (completed >= iser_cq_poll_limit)
 -			break;
 -	}
 -
 -	/*
 -	 * It is assumed here that arming CQ only once its empty
 -	 * would not cause interrupts to be missed.
 +	struct iser_cq_desc *cq_desc = (struct iser_cq_desc *)data;
 +	struct iser_device  *device = cq_desc->device;
 +	int cq_index = cq_desc->cq_index;
 +	struct ib_cq	     *cq = device->rx_cq[cq_index];
 +	 struct ib_wc	     wc;
 +	 struct iser_rx_desc *desc;
 +	 unsigned long	     xfer_len;
 +	struct iser_conn *ib_conn;
 +	int completed_tx, completed_rx = 0;
 +
 +	/* First do tx drain, so in a case where we have rx flushes and a successful
 +	 * tx completion we will still go through completion error handling.
  	 */
 +	completed_tx = iser_drain_tx_cq(device, cq_index);
 +
 +	while (ib_poll_cq(cq, 1, &wc) == 1) {
 +		desc	 = (struct iser_rx_desc *) (unsigned long) wc.wr_id;
 +		BUG_ON(desc == NULL);
 +		ib_conn = wc.qp->qp_context;
 +		if (wc.status == IB_WC_SUCCESS) {
 +			if (wc.opcode == IB_WC_RECV) {
 +				xfer_len = (unsigned long)wc.byte_len;
 +				iser_rcv_completion(desc, xfer_len, ib_conn);
 +			} else
 +				iser_err("expected opcode %d got %d\n",
 +					IB_WC_RECV, wc.opcode);
 +		} else {
 +			if (wc.status != IB_WC_WR_FLUSH_ERR)
 +				iser_err("rx id %llx status %d vend_err %x\n",
 +					wc.wr_id, wc.status, wc.vendor_err);
 +			ib_conn->post_recv_buf_count--;
 +			iser_handle_comp_error(NULL, ib_conn);
 +		}
 +		completed_rx++;
 +		if (!(completed_rx & 63))
 +			completed_tx += iser_drain_tx_cq(device, cq_index);
 +	}
 +	/* #warning "it is assumed here that arming CQ only once its empty" *
 +	 * " would not cause interrupts to be missed"                       */
  	ib_req_notify_cq(cq, IB_CQ_NEXT_COMP);
  
 -	iser_dbg("got %d completions\n", completed);
 +	iser_dbg("got %d rx %d tx completions\n", completed_rx, completed_tx);
  }
  
  static void iser_cq_callback(struct ib_cq *cq, void *cq_context)
* Unmerged path drivers/infiniband/ulp/iser/iser_verbs.c
