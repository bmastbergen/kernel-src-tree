workqueue: fix subtle pool management issue which can stall whole worker_pool

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Tejun Heo <tj@kernel.org>
commit 29187a9eeaf362d8422e62e17a22a6e115277a49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/29187a9e.failed

A worker_pool's forward progress is guaranteed by the fact that the
last idle worker assumes the manager role to create more workers and
summon the rescuers if creating workers doesn't succeed in timely
manner before proceeding to execute work items.

This manager role is implemented in manage_workers(), which indicates
whether the worker may proceed to work item execution with its return
value.  This is necessary because multiple workers may contend for the
manager role, and, if there already is a manager, others should
proceed to work item execution.

Unfortunately, the function also indicates that the worker may proceed
to work item execution if need_to_create_worker() is false at the head
of the function.  need_to_create_worker() tests the following
conditions.

	pending work items && !nr_running && !nr_idle

The first and third conditions are protected by pool->lock and thus
won't change while holding pool->lock; however, nr_running can change
asynchronously as other workers block and resume and while it's likely
to be zero, as someone woke this worker up in the first place, some
other workers could have become runnable inbetween making it non-zero.

If this happens, manage_worker() could return false even with zero
nr_idle making the worker, the last idle one, proceed to execute work
items.  If then all workers of the pool end up blocking on a resource
which can only be released by a work item which is pending on that
pool, the whole pool can deadlock as there's no one to create more
workers or summon the rescuers.

This patch fixes the problem by removing the early exit condition from
maybe_create_worker() and making manage_workers() return false iff
there's already another manager, which ensures that the last worker
doesn't start executing work items.

We can leave the early exit condition alone and just ignore the return
value but the only reason it was put there is because the
manage_workers() used to perform both creations and destructions of
workers and thus the function may be invoked while the pool is trying
to reduce the number of workers.  Now that manage_workers() is called
only when more workers are needed, the only case this early exit
condition is triggered is rare race conditions rendering it pointless.

Tested with simulated workload and modified workqueue code which
trigger the pool deadlock reliably without this patch.

	Signed-off-by: Tejun Heo <tj@kernel.org>
	Reported-by: Eric Sandeen <sandeen@sandeen.net>
Link: http://lkml.kernel.org/g/54B019F4.8030009@sandeen.net
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
	Cc: stable@vger.kernel.org
(cherry picked from commit 29187a9eeaf362d8422e62e17a22a6e115277a49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/workqueue.c
diff --cc kernel/workqueue.c
index 14a71630e038,beeeac9e0e3e..000000000000
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@@ -1921,12 -1841,8 +1921,15 @@@ static void pool_mayday_timeout(unsigne
   * spin_lock_irq(pool->lock) which may be released and regrabbed
   * multiple times.  Does GFP_KERNEL allocations.  Called only from
   * manager.
++<<<<<<< HEAD
 + *
 + * RETURNS:
 + * %false if no action was taken and pool->lock stayed locked, %true
 + * otherwise.
++=======
++>>>>>>> 29187a9eeaf3 (workqueue: fix subtle pool management issue which can stall whole worker_pool)
   */
- static bool maybe_create_worker(struct worker_pool *pool)
+ static void maybe_create_worker(struct worker_pool *pool)
  __releases(&pool->lock)
  __acquires(&pool->lock)
  {
@@@ -1963,49 -1864,15 +1964,48 @@@ restart
  
  	del_timer_sync(&pool->mayday_timer);
  	spin_lock_irq(&pool->lock);
 -	/*
 -	 * This is necessary even after a new worker was just successfully
 -	 * created as @pool->lock was dropped and the new worker might have
 -	 * already become busy.
 -	 */
  	if (need_to_create_worker(pool))
  		goto restart;
- 	return true;
  }
  
 +/**
 + * maybe_destroy_worker - destroy workers which have been idle for a while
 + * @pool: pool to destroy workers for
 + *
 + * Destroy @pool workers which have been idle for longer than
 + * IDLE_WORKER_TIMEOUT.
 + *
 + * LOCKING:
 + * spin_lock_irq(pool->lock) which may be released and regrabbed
 + * multiple times.  Called only from manager.
 + *
 + * RETURNS:
 + * %false if no action was taken and pool->lock stayed locked, %true
 + * otherwise.
 + */
 +static bool maybe_destroy_workers(struct worker_pool *pool)
 +{
 +	bool ret = false;
 +
 +	while (too_many_workers(pool)) {
 +		struct worker *worker;
 +		unsigned long expires;
 +
 +		worker = list_entry(pool->idle_list.prev, struct worker, entry);
 +		expires = worker->last_active + IDLE_WORKER_TIMEOUT;
 +
 +		if (time_before(jiffies, expires)) {
 +			mod_timer(&pool->idle_timer, expires);
 +			break;
 +		}
 +
 +		destroy_worker(worker);
 +		ret = true;
 +	}
 +
 +	return ret;
 +}
 +
  /**
   * manage_workers - manage worker pool
   * @worker: self
@@@ -2022,18 -1889,17 +2022,25 @@@
   * spin_lock_irq(pool->lock) which may be released and regrabbed
   * multiple times.  Does GFP_KERNEL allocations.
   *
++<<<<<<< HEAD
 + * RETURNS:
 + * spin_lock_irq(pool->lock) which may be released and regrabbed
 + * multiple times.  Does GFP_KERNEL allocations.
++=======
+  * Return:
+  * %false if the pool doesn't need management and the caller can safely
+  * start processing works, %true if management function was performed and
+  * the conditions that the caller verified before calling the function may
+  * no longer be true.
++>>>>>>> 29187a9eeaf3 (workqueue: fix subtle pool management issue which can stall whole worker_pool)
   */
  static bool manage_workers(struct worker *worker)
  {
  	struct worker_pool *pool = worker->pool;
- 	bool ret = false;
  
  	/*
 +	 * Managership is governed by two mutexes - manager_arb and
 +	 * manager_mutex.  manager_arb handles arbitration of manager role.
  	 * Anyone who successfully grabs manager_arb wins the arbitration
  	 * and becomes the manager.  mutex_trylock() on pool->manager_arb
  	 * failure while holding pool->lock reliably indicates that someone
@@@ -2042,42 -1908,14 +2049,46 @@@
  	 * grabbing manager_arb is responsible for actually performing
  	 * manager duties.  If manager_arb is grabbed and released without
  	 * actual management, the pool may stall indefinitely.
 +	 *
 +	 * manager_mutex is used for exclusion of actual management
 +	 * operations.  The holder of manager_mutex can be sure that none
 +	 * of management operations, including creation and destruction of
 +	 * workers, won't take place until the mutex is released.  Because
 +	 * manager_mutex doesn't interfere with manager role arbitration,
 +	 * it is guaranteed that the pool's management, while may be
 +	 * delayed, won't be disturbed by someone else grabbing
 +	 * manager_mutex.
  	 */
  	if (!mutex_trylock(&pool->manager_arb))
- 		return ret;
+ 		return false;
  
++<<<<<<< HEAD
 +	/*
 +	 * With manager arbitration won, manager_mutex would be free in
 +	 * most cases.  trylock first without dropping @pool->lock.
 +	 */
 +	if (unlikely(!mutex_trylock(&pool->manager_mutex))) {
 +		spin_unlock_irq(&pool->lock);
 +		mutex_lock(&pool->manager_mutex);
 +		spin_lock_irq(&pool->lock);
 +		ret = true;
 +	}
 +
 +	pool->flags &= ~POOL_MANAGE_WORKERS;
 +
 +	/*
 +	 * Destroy and then create so that may_start_working() is true
 +	 * on return.
 +	 */
 +	ret |= maybe_destroy_workers(pool);
 +	ret |= maybe_create_worker(pool);
++=======
+ 	maybe_create_worker(pool);
++>>>>>>> 29187a9eeaf3 (workqueue: fix subtle pool management issue which can stall whole worker_pool)
  
 +	mutex_unlock(&pool->manager_mutex);
  	mutex_unlock(&pool->manager_arb);
- 	return ret;
+ 	return true;
  }
  
  /**
* Unmerged path kernel/workqueue.c
