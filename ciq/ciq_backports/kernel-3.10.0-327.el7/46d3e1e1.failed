vfio: powerpc/spapr: powerpc/powernv/ioda2: Use DMA windows API in ownership control

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit 46d3e1e16294c587a74093b1f5474c1b33b72381
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/46d3e1e1.failed

Before the IOMMU user (VFIO) would take control over the IOMMU table
belonging to a specific IOMMU group. This approach did not allow sharing
tables between IOMMU groups attached to the same container.

This introduces a new IOMMU ownership flavour when the user can not
just control the existing IOMMU table but remove/create tables on demand.
If an IOMMU implements take/release_ownership() callbacks, this lets
the user have full control over the IOMMU group. When the ownership
is taken, the platform code removes all the windows so the caller must
create them.
Before returning the ownership back to the platform code, VFIO
unprograms and removes all the tables it created.

This changes IODA2's onwership handler to remove the existing table
rather than manipulating with the existing one. From now on,
iommu_take_ownership() and iommu_release_ownership() are only called
from the vfio_iommu_spapr_tce driver.

Old-style ownership is still supported allowing VFIO to run on older
P5IOC2 and IODA IO controllers.

No change in userspace-visible behaviour is expected. Since it recreates
TCE tables on each ownership change, related kernel traces will appear
more often.

This adds a pnv_pci_ioda2_setup_default_config() which is called
when PE is being configured at boot time and when the ownership is
passed from VFIO to the platform code.

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
[aw: for the vfio related changes]
	Acked-by: Alex Williamson <alex.williamson@redhat.com>
	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 46d3e1e16294c587a74093b1f5474c1b33b72381)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/powernv/pci-ioda.c
#	drivers/vfio/vfio_iommu_spapr_tce.c
diff --cc arch/powerpc/platforms/powernv/pci-ioda.c
index 109e90d84e34,b9f0f430e249..000000000000
--- a/arch/powerpc/platforms/powernv/pci-ioda.c
+++ b/arch/powerpc/platforms/powernv/pci-ioda.c
@@@ -799,95 -2037,361 +799,415 @@@ static void pnv_pci_ioda2_set_bypass(st
  		pe->tce_bypass_enabled = enable;
  }
  
 -static long pnv_pci_ioda2_table_alloc_pages(int nid, __u64 bus_offset,
 -		__u32 page_shift, __u64 window_size, __u32 levels,
 -		struct iommu_table *tbl);
 -
 -static long pnv_pci_ioda2_create_table(struct iommu_table_group *table_group,
 -		int num, __u32 page_shift, __u64 window_size, __u32 levels,
 -		struct iommu_table **ptbl)
 +static void pnv_pci_ioda2_setup_bypass_pe(struct pnv_phb *phb,
 +					  struct pnv_ioda_pe *pe)
  {
 -	struct pnv_ioda_pe *pe = container_of(table_group, struct pnv_ioda_pe,
 -			table_group);
 -	int nid = pe->phb->hose->node;
 -	__u64 bus_offset = num ? pe->tce_bypass_base : table_group->tce32_start;
 -	long ret;
 -	struct iommu_table *tbl;
 +	/* TVE #1 is selected by PCI address bit 59 */
 +	pe->tce_bypass_base = 1ull << 59;
  
 -	tbl = pnv_pci_table_alloc(nid);
 -	if (!tbl)
 -		return -ENOMEM;
 +	/* Install set_bypass callback for VFIO */
 +	pe->tce32_table.set_bypass = pnv_pci_ioda2_set_bypass;
  
++<<<<<<< HEAD
 +	/* Enable bypass by default */
 +	pnv_pci_ioda2_set_bypass(&pe->tce32_table, true);
++=======
+ 	ret = pnv_pci_ioda2_table_alloc_pages(nid,
+ 			bus_offset, page_shift, window_size,
+ 			levels, tbl);
+ 	if (ret) {
+ 		iommu_free_table(tbl, "pnv");
+ 		return ret;
+ 	}
+ 
+ 	tbl->it_ops = &pnv_ioda2_iommu_ops;
+ 	if (pe->phb->ioda.tce_inval_reg)
+ 		tbl->it_type |= (TCE_PCI_SWINV_CREATE | TCE_PCI_SWINV_FREE);
+ 
+ 	*ptbl = tbl;
+ 
+ 	return 0;
+ }
+ 
+ static long pnv_pci_ioda2_setup_default_config(struct pnv_ioda_pe *pe)
+ {
+ 	struct iommu_table *tbl = NULL;
+ 	long rc;
+ 
+ 	rc = pnv_pci_ioda2_create_table(&pe->table_group, 0,
+ 			IOMMU_PAGE_SHIFT_4K,
+ 			pe->table_group.tce32_size,
+ 			POWERNV_IOMMU_DEFAULT_LEVELS, &tbl);
+ 	if (rc) {
+ 		pe_err(pe, "Failed to create 32-bit TCE table, err %ld",
+ 				rc);
+ 		return rc;
+ 	}
+ 
+ 	iommu_init_table(tbl, pe->phb->hose->node);
+ 
+ 	rc = pnv_pci_ioda2_set_window(&pe->table_group, 0, tbl);
+ 	if (rc) {
+ 		pe_err(pe, "Failed to configure 32-bit TCE table, err %ld\n",
+ 				rc);
+ 		pnv_ioda2_table_free(tbl);
+ 		return rc;
+ 	}
+ 
+ 	if (!pnv_iommu_bypass_disabled)
+ 		pnv_pci_ioda2_set_bypass(pe, true);
+ 
+ 	/* OPAL variant of PHB3 invalidated TCEs */
+ 	if (pe->phb->ioda.tce_inval_reg)
+ 		tbl->it_type |= (TCE_PCI_SWINV_CREATE | TCE_PCI_SWINV_FREE);
+ 
+ 	/*
+ 	 * Setting table base here only for carrying iommu_group
+ 	 * further down to let iommu_add_device() do the job.
+ 	 * pnv_pci_ioda_dma_dev_setup will override it later anyway.
+ 	 */
+ 	if (pe->flags & PNV_IODA_PE_DEV)
+ 		set_iommu_table_base(&pe->pdev->dev, tbl);
+ 
+ 	return 0;
+ }
+ 
+ #ifdef CONFIG_IOMMU_API
+ static unsigned long pnv_pci_ioda2_get_table_size(__u32 page_shift,
+ 		__u64 window_size, __u32 levels)
+ {
+ 	unsigned long bytes = 0;
+ 	const unsigned window_shift = ilog2(window_size);
+ 	unsigned entries_shift = window_shift - page_shift;
+ 	unsigned table_shift = entries_shift + 3;
+ 	unsigned long tce_table_size = max(0x1000UL, 1UL << table_shift);
+ 	unsigned long direct_table_size;
+ 
+ 	if (!levels || (levels > POWERNV_IOMMU_MAX_LEVELS) ||
+ 			(window_size > memory_hotplug_max()) ||
+ 			!is_power_of_2(window_size))
+ 		return 0;
+ 
+ 	/* Calculate a direct table size from window_size and levels */
+ 	entries_shift = (entries_shift + levels - 1) / levels;
+ 	table_shift = entries_shift + 3;
+ 	table_shift = max_t(unsigned, table_shift, PAGE_SHIFT);
+ 	direct_table_size =  1UL << table_shift;
+ 
+ 	for ( ; levels; --levels) {
+ 		bytes += _ALIGN_UP(tce_table_size, direct_table_size);
+ 
+ 		tce_table_size /= direct_table_size;
+ 		tce_table_size <<= 3;
+ 		tce_table_size = _ALIGN_UP(tce_table_size, direct_table_size);
+ 	}
+ 
+ 	return bytes;
+ }
+ 
+ static long pnv_pci_ioda2_unset_window(struct iommu_table_group *table_group,
+ 		int num)
+ {
+ 	struct pnv_ioda_pe *pe = container_of(table_group, struct pnv_ioda_pe,
+ 			table_group);
+ 	struct pnv_phb *phb = pe->phb;
+ 	long ret;
+ 
+ 	pe_info(pe, "Removing DMA window #%d\n", num);
+ 
+ 	ret = opal_pci_map_pe_dma_window(phb->opal_id, pe->pe_number,
+ 			(pe->pe_number << 1) + num,
+ 			0/* levels */, 0/* table address */,
+ 			0/* table size */, 0/* page size */);
+ 	if (ret)
+ 		pe_warn(pe, "Unmapping failed, ret = %ld\n", ret);
+ 	else
+ 		pnv_pci_ioda2_tce_invalidate_entire(pe);
+ 
+ 	pnv_pci_unlink_table_and_group(table_group->tables[num], table_group);
+ 
+ 	return ret;
+ }
+ 
+ static void pnv_ioda2_take_ownership(struct iommu_table_group *table_group)
+ {
+ 	struct pnv_ioda_pe *pe = container_of(table_group, struct pnv_ioda_pe,
+ 						table_group);
+ 	/* Store @tbl as pnv_pci_ioda2_unset_window() resets it */
+ 	struct iommu_table *tbl = pe->table_group.tables[0];
+ 
+ 	pnv_pci_ioda2_set_bypass(pe, false);
+ 	pnv_pci_ioda2_unset_window(&pe->table_group, 0);
+ 	pnv_ioda2_table_free(tbl);
+ }
+ 
+ static void pnv_ioda2_release_ownership(struct iommu_table_group *table_group)
+ {
+ 	struct pnv_ioda_pe *pe = container_of(table_group, struct pnv_ioda_pe,
+ 						table_group);
+ 
+ 	pnv_pci_ioda2_setup_default_config(pe);
+ }
+ 
+ static struct iommu_table_group_ops pnv_pci_ioda2_ops = {
+ 	.get_table_size = pnv_pci_ioda2_get_table_size,
+ 	.create_table = pnv_pci_ioda2_create_table,
+ 	.set_window = pnv_pci_ioda2_set_window,
+ 	.unset_window = pnv_pci_ioda2_unset_window,
+ 	.take_ownership = pnv_ioda2_take_ownership,
+ 	.release_ownership = pnv_ioda2_release_ownership,
+ };
+ #endif
+ 
+ static void pnv_pci_ioda_setup_opal_tce_kill(struct pnv_phb *phb)
+ {
+ 	const __be64 *swinvp;
+ 
+ 	/* OPAL variant of PHB3 invalidated TCEs */
+ 	swinvp = of_get_property(phb->hose->dn, "ibm,opal-tce-kill", NULL);
+ 	if (!swinvp)
+ 		return;
+ 
+ 	phb->ioda.tce_inval_reg_phys = be64_to_cpup(swinvp);
+ 	phb->ioda.tce_inval_reg = ioremap(phb->ioda.tce_inval_reg_phys, 8);
+ }
+ 
+ static __be64 *pnv_pci_ioda2_table_do_alloc_pages(int nid, unsigned shift,
+ 		unsigned levels, unsigned long limit,
+ 		unsigned long *current_offset)
+ {
+ 	struct page *tce_mem = NULL;
+ 	__be64 *addr, *tmp;
+ 	unsigned order = max_t(unsigned, shift, PAGE_SHIFT) - PAGE_SHIFT;
+ 	unsigned long allocated = 1UL << (order + PAGE_SHIFT);
+ 	unsigned entries = 1UL << (shift - 3);
+ 	long i;
+ 
+ 	tce_mem = alloc_pages_node(nid, GFP_KERNEL, order);
+ 	if (!tce_mem) {
+ 		pr_err("Failed to allocate a TCE memory, order=%d\n", order);
+ 		return NULL;
+ 	}
+ 	addr = page_address(tce_mem);
+ 	memset(addr, 0, allocated);
+ 
+ 	--levels;
+ 	if (!levels) {
+ 		*current_offset += allocated;
+ 		return addr;
+ 	}
+ 
+ 	for (i = 0; i < entries; ++i) {
+ 		tmp = pnv_pci_ioda2_table_do_alloc_pages(nid, shift,
+ 				levels, limit, current_offset);
+ 		if (!tmp)
+ 			break;
+ 
+ 		addr[i] = cpu_to_be64(__pa(tmp) |
+ 				TCE_PCI_READ | TCE_PCI_WRITE);
+ 
+ 		if (*current_offset >= limit)
+ 			break;
+ 	}
+ 
+ 	return addr;
+ }
+ 
+ static void pnv_pci_ioda2_table_do_free_pages(__be64 *addr,
+ 		unsigned long size, unsigned level);
+ 
+ static long pnv_pci_ioda2_table_alloc_pages(int nid, __u64 bus_offset,
+ 		__u32 page_shift, __u64 window_size, __u32 levels,
+ 		struct iommu_table *tbl)
+ {
+ 	void *addr;
+ 	unsigned long offset = 0, level_shift;
+ 	const unsigned window_shift = ilog2(window_size);
+ 	unsigned entries_shift = window_shift - page_shift;
+ 	unsigned table_shift = max_t(unsigned, entries_shift + 3, PAGE_SHIFT);
+ 	const unsigned long tce_table_size = 1UL << table_shift;
+ 
+ 	if (!levels || (levels > POWERNV_IOMMU_MAX_LEVELS))
+ 		return -EINVAL;
+ 
+ 	if ((window_size > memory_hotplug_max()) || !is_power_of_2(window_size))
+ 		return -EINVAL;
+ 
+ 	/* Adjust direct table size from window_size and levels */
+ 	entries_shift = (entries_shift + levels - 1) / levels;
+ 	level_shift = entries_shift + 3;
+ 	level_shift = max_t(unsigned, level_shift, PAGE_SHIFT);
+ 
+ 	/* Allocate TCE table */
+ 	addr = pnv_pci_ioda2_table_do_alloc_pages(nid, level_shift,
+ 			levels, tce_table_size, &offset);
+ 
+ 	/* addr==NULL means that the first level allocation failed */
+ 	if (!addr)
+ 		return -ENOMEM;
+ 
+ 	/*
+ 	 * First level was allocated but some lower level failed as
+ 	 * we did not allocate as much as we wanted,
+ 	 * release partially allocated table.
+ 	 */
+ 	if (offset < tce_table_size) {
+ 		pnv_pci_ioda2_table_do_free_pages(addr,
+ 				1ULL << (level_shift - 3), levels - 1);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	/* Setup linux iommu table */
+ 	pnv_pci_setup_iommu_table(tbl, addr, tce_table_size, bus_offset,
+ 			page_shift);
+ 	tbl->it_level_size = 1ULL << (level_shift - 3);
+ 	tbl->it_indirect_levels = levels - 1;
+ 	tbl->it_allocated_size = offset;
+ 
+ 	pr_devel("Created TCE table: ws=%08llx ts=%lx @%08llx\n",
+ 			window_size, tce_table_size, bus_offset);
+ 
+ 	return 0;
+ }
+ 
+ static void pnv_pci_ioda2_table_do_free_pages(__be64 *addr,
+ 		unsigned long size, unsigned level)
+ {
+ 	const unsigned long addr_ul = (unsigned long) addr &
+ 			~(TCE_PCI_READ | TCE_PCI_WRITE);
+ 
+ 	if (level) {
+ 		long i;
+ 		u64 *tmp = (u64 *) addr_ul;
+ 
+ 		for (i = 0; i < size; ++i) {
+ 			unsigned long hpa = be64_to_cpu(tmp[i]);
+ 
+ 			if (!(hpa & (TCE_PCI_READ | TCE_PCI_WRITE)))
+ 				continue;
+ 
+ 			pnv_pci_ioda2_table_do_free_pages(__va(hpa), size,
+ 					level - 1);
+ 		}
+ 	}
+ 
+ 	free_pages(addr_ul, get_order(size << 3));
+ }
+ 
+ static void pnv_pci_ioda2_table_free_pages(struct iommu_table *tbl)
+ {
+ 	const unsigned long size = tbl->it_indirect_levels ?
+ 			tbl->it_level_size : tbl->it_size;
+ 
+ 	if (!tbl->it_size)
+ 		return;
+ 
+ 	pnv_pci_ioda2_table_do_free_pages((__be64 *)tbl->it_base, size,
+ 			tbl->it_indirect_levels);
++>>>>>>> 46d3e1e16294 (vfio: powerpc/spapr: powerpc/powernv/ioda2: Use DMA windows API in ownership control)
  }
  
  static void pnv_pci_ioda2_setup_dma_pe(struct pnv_phb *phb,
  				       struct pnv_ioda_pe *pe)
  {
++<<<<<<< HEAD
 +	struct page *tce_mem = NULL;
 +	void *addr;
 +	const __be64 *swinvp;
 +	struct iommu_table *tbl;
 +	unsigned int tce_table_size, end;
++=======
++>>>>>>> 46d3e1e16294 (vfio: powerpc/spapr: powerpc/powernv/ioda2: Use DMA windows API in ownership control)
  	int64_t rc;
  
  	/* We shouldn't already have a 32-bit DMA associated */
  	if (WARN_ON(pe->tce32_seg >= 0))
  		return;
  
 -	/* TVE #1 is selected by PCI address bit 59 */
 -	pe->tce_bypass_base = 1ull << 59;
 -
 -	iommu_register_group(&pe->table_group, phb->hose->global_number,
 -			pe->pe_number);
 -
  	/* The PE will reserve all possible 32-bits space */
  	pe->tce32_seg = 0;
 +	end = (1 << ilog2(phb->ioda.m32_pci_base));
 +	tce_table_size = (end / 0x1000) * 8;
  	pe_info(pe, "Setting up 32-bit TCE table at 0..%08x\n",
 -		phb->ioda.m32_pci_base);
 +		end);
 +
++<<<<<<< HEAD
 +	/* Allocate TCE table */
 +	tce_mem = alloc_pages_node(phb->hose->node, GFP_KERNEL,
 +				   get_order(tce_table_size));
 +	if (!tce_mem) {
 +		pe_err(pe, "Failed to allocate a 32-bit TCE memory\n");
 +		goto fail;
 +	}
 +	addr = page_address(tce_mem);
 +	memset(addr, 0, tce_table_size);
  
 +	/*
 +	 * Map TCE table through TVT. The TVE index is the PE number
 +	 * shifted by 1 bit for 32-bits DMA space.
 +	 */
 +	rc = opal_pci_map_pe_dma_window(phb->opal_id, pe->pe_number,
 +					pe->pe_number << 1, 1, __pa(addr),
 +					tce_table_size, 0x1000);
++=======
+ 	/* Setup linux iommu table */
+ 	pe->table_group.tce32_start = 0;
+ 	pe->table_group.tce32_size = phb->ioda.m32_pci_base;
+ 	pe->table_group.max_dynamic_windows_supported =
+ 			IOMMU_TABLE_GROUP_MAX_TABLES;
+ 	pe->table_group.max_levels = POWERNV_IOMMU_MAX_LEVELS;
+ 	pe->table_group.pgsizes = SZ_4K | SZ_64K | SZ_16M;
+ #ifdef CONFIG_IOMMU_API
+ 	pe->table_group.ops = &pnv_pci_ioda2_ops;
+ #endif
+ 
+ 	rc = pnv_pci_ioda2_setup_default_config(pe);
++>>>>>>> 46d3e1e16294 (vfio: powerpc/spapr: powerpc/powernv/ioda2: Use DMA windows API in ownership control)
  	if (rc) {
- 		pe_err(pe, "Failed to configure 32-bit TCE table,"
- 		       " err %ld\n", rc);
- 		goto fail;
+ 		if (pe->tce32_seg >= 0)
+ 			pe->tce32_seg = -1;
+ 		return;
  	}
  
++<<<<<<< HEAD
 +	/* Setup linux iommu table */
 +	tbl = &pe->tce32_table;
 +	pnv_pci_setup_iommu_table(tbl, addr, tce_table_size, 0,
 +			IOMMU_PAGE_SHIFT_4K);
 +
 +	/* OPAL variant of PHB3 invalidated TCEs */
 +	swinvp = of_get_property(phb->hose->dn, "ibm,opal-tce-kill", NULL);
 +	if (swinvp) {
 +		/* We need a couple more fields -- an address and a data
 +		 * to or.  Since the bus is only printed out on table free
 +		 * errors, and on the first pass the data will be a relative
 +		 * bus number, print that out instead.
 +		 */
 +		tbl->it_index = (unsigned long)ioremap(be64_to_cpup(swinvp), 8);
 +		tbl->it_type |= (TCE_PCI_SWINV_CREATE | TCE_PCI_SWINV_FREE);
 +	}
 +	iommu_init_table(tbl, phb->hose->node);
 +	iommu_register_group(tbl, phb->hose->global_number, pe->pe_number);
 +
 +	if (pe->pdev)
 +		set_iommu_table_base_and_group(&pe->pdev->dev, tbl);
 +	else
 +		pnv_ioda_setup_bus_dma(pe, pe->pbus, true);
 +
 +	/* Also create a bypass window */
 +	pnv_pci_ioda2_setup_bypass_pe(phb, pe);
 +	return;
 +fail:
 +	if (pe->tce32_seg >= 0)
 +		pe->tce32_seg = -1;
 +	if (tce_mem)
 +		__free_pages(tce_mem, get_order(tce_table_size));
++=======
+ 	if (pe->flags & PNV_IODA_PE_DEV)
+ 		iommu_add_device(&pe->pdev->dev);
+ 	else if (pe->flags & (PNV_IODA_PE_BUS | PNV_IODA_PE_BUS_ALL))
+ 		pnv_ioda_setup_bus_dma(pe, pe->pbus);
++>>>>>>> 46d3e1e16294 (vfio: powerpc/spapr: powerpc/powernv/ioda2: Use DMA windows API in ownership control)
  }
  
  static void pnv_ioda_setup_dma(struct pnv_phb *phb)
diff --cc drivers/vfio/vfio_iommu_spapr_tce.c
index e65bc73cc8a8,203caacf2242..000000000000
--- a/drivers/vfio/vfio_iommu_spapr_tce.c
+++ b/drivers/vfio/vfio_iommu_spapr_tce.c
@@@ -443,6 -541,108 +482,111 @@@ static long tce_iommu_ioctl(void *iommu
  	return -ENOTTY;
  }
  
++<<<<<<< HEAD
++=======
+ static void tce_iommu_release_ownership(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		struct iommu_table *tbl = table_group->tables[i];
+ 
+ 		if (!tbl)
+ 			continue;
+ 
+ 		tce_iommu_clear(container, tbl, tbl->it_offset, tbl->it_size);
+ 		if (tbl->it_map)
+ 			iommu_release_ownership(tbl);
+ 	}
+ }
+ 
+ static int tce_iommu_take_ownership(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	int i, j, rc = 0;
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		struct iommu_table *tbl = table_group->tables[i];
+ 
+ 		if (!tbl || !tbl->it_map)
+ 			continue;
+ 
+ 		rc = iommu_take_ownership(tbl);
+ 		if (rc) {
+ 			for (j = 0; j < i; ++j)
+ 				iommu_release_ownership(
+ 						table_group->tables[j]);
+ 
+ 			return rc;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void tce_iommu_release_ownership_ddw(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	long i;
+ 
+ 	if (!table_group->ops->unset_window) {
+ 		WARN_ON_ONCE(1);
+ 		return;
+ 	}
+ 
+ 	for (i = 0; i < IOMMU_TABLE_GROUP_MAX_TABLES; ++i) {
+ 		/* Store table pointer as unset_window resets it */
+ 		struct iommu_table *tbl = table_group->tables[i];
+ 
+ 		if (!tbl)
+ 			continue;
+ 
+ 		table_group->ops->unset_window(table_group, i);
+ 		tce_iommu_clear(container, tbl,
+ 				tbl->it_offset, tbl->it_size);
+ 		tce_iommu_free_table(tbl);
+ 	}
+ 
+ 	table_group->ops->release_ownership(table_group);
+ }
+ 
+ static long tce_iommu_take_ownership_ddw(struct tce_container *container,
+ 		struct iommu_table_group *table_group)
+ {
+ 	long ret;
+ 	struct iommu_table *tbl = NULL;
+ 
+ 	if (!table_group->ops->create_table || !table_group->ops->set_window ||
+ 			!table_group->ops->release_ownership) {
+ 		WARN_ON_ONCE(1);
+ 		return -EFAULT;
+ 	}
+ 
+ 	table_group->ops->take_ownership(table_group);
+ 
+ 	ret = tce_iommu_create_table(container,
+ 			table_group,
+ 			0, /* window number */
+ 			IOMMU_PAGE_SHIFT_4K,
+ 			table_group->tce32_size,
+ 			1, /* default levels */
+ 			&tbl);
+ 	if (!ret) {
+ 		ret = table_group->ops->set_window(table_group, 0, tbl);
+ 		if (ret)
+ 			tce_iommu_free_table(tbl);
+ 		else
+ 			table_group->tables[0] = tbl;
+ 	}
+ 
+ 	if (ret)
+ 		table_group->ops->release_ownership(table_group);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 46d3e1e16294 (vfio: powerpc/spapr: powerpc/powernv/ioda2: Use DMA windows API in ownership control)
  static int tce_iommu_attach_group(void *iommu_data,
  		struct iommu_group *iommu_group)
  {
* Unmerged path arch/powerpc/platforms/powernv/pci-ioda.c
* Unmerged path drivers/vfio/vfio_iommu_spapr_tce.c
