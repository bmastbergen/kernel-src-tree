KVM: nVMX: nested TPR shadow/threshold emulation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [kvm] nvmx: nested TPR shadow/threshold emulation (Bandan Das) [1209995]
Rebuild_FUZZ: 94.51%
commit-author Wanpeng Li <wanpeng.li@linux.intel.com>
commit a7c0b07d570848e50fce4d31ac01313484d6b844
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/a7c0b07d.failed

This patch fix bug https://bugzilla.kernel.org/show_bug.cgi?id=61411

TPR shadow/threshold feature is important to speed up the Windows guest.
Besides, it is a must feature for certain VMM.

We map virtual APIC page address and TPR threshold from L1 VMCS. If
TPR_BELOW_THRESHOLD VM exit is triggered by L2 guest and L1 interested
in, we inject it into L1 VMM for handling.

	Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
[Add PAGE_ALIGNED check, do not write useless virtual APIC page address
 if TPR shadowing is disabled. - Paolo]
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit a7c0b07d570848e50fce4d31ac01313484d6b844)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index ccfe5bb586e5,73ba2a265f85..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -7979,6 -7939,55 +7991,58 @@@ static void vmx_inject_page_fault_neste
  		kvm_inject_page_fault(vcpu, fault);
  }
  
++<<<<<<< HEAD
++=======
+ static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu,
+ 					struct vmcs12 *vmcs12)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 
+ 	if (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {
+ 		/* TODO: Also verify bits beyond physical address width are 0 */
+ 		if (!PAGE_ALIGNED(vmcs12->apic_access_addr))
+ 			return false;
+ 
+ 		/*
+ 		 * Translate L1 physical address to host physical
+ 		 * address for vmcs02. Keep the page pinned, so this
+ 		 * physical address remains valid. We keep a reference
+ 		 * to it so we can release it later.
+ 		 */
+ 		if (vmx->nested.apic_access_page) /* shouldn't happen */
+ 			nested_release_page(vmx->nested.apic_access_page);
+ 		vmx->nested.apic_access_page =
+ 			nested_get_page(vcpu, vmcs12->apic_access_addr);
+ 	}
+ 
+ 	if (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {
+ 		/* TODO: Also verify bits beyond physical address width are 0 */
+ 		if (!PAGE_ALIGNED(vmcs12->virtual_apic_page_addr))
+ 			return false;
+ 
+ 		if (vmx->nested.virtual_apic_page) /* shouldn't happen */
+ 			nested_release_page(vmx->nested.virtual_apic_page);
+ 		vmx->nested.virtual_apic_page =
+ 			nested_get_page(vcpu, vmcs12->virtual_apic_page_addr);
+ 
+ 		/*
+ 		 * Failing the vm entry is _not_ what the processor does
+ 		 * but it's basically the only possibility we have.
+ 		 * We could still enter the guest if CR8 load exits are
+ 		 * enabled, CR8 store exits are enabled, and virtualize APIC
+ 		 * access is disabled; in this case the processor would never
+ 		 * use the TPR shadow and we could simply clear the bit from
+ 		 * the execution control.  But such a configuration is useless,
+ 		 * so let's keep the code simple.
+ 		 */
+ 		if (!vmx->nested.virtual_apic_page)
+ 			return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
++>>>>>>> a7c0b07d5708 (KVM: nVMX: nested TPR shadow/threshold emulation)
  static void vmx_start_preemption_timer(struct kvm_vcpu *vcpu)
  {
  	u64 preemption_timeout = get_vmcs12(vcpu)->vmx_preemption_timer_value;
@@@ -8946,13 -8952,11 +9017,17 @@@ static void nested_vmx_vmexit(struct kv
  		nested_release_page(vmx->nested.apic_access_page);
  		vmx->nested.apic_access_page = 0;
  	}
+ 	if (vmx->nested.virtual_apic_page) {
+ 		nested_release_page(vmx->nested.virtual_apic_page);
+ 		vmx->nested.virtual_apic_page = 0;
+ 	}
  
 +	/*
 +	 * We are now running in L2, mmu_notifier will force to reload the
 +	 * page's hpa for L2 vmcs. Need to reload it for L1 before entering L1.
 +	 */
 +	kvm_vcpu_reload_apic_access_page(vcpu);
 +
  	/*
  	 * Exiting from L2 to L1, we're now back to L1 which thinks it just
  	 * finished a VMLAUNCH or VMRESUME instruction, so we need to set the
* Unmerged path arch/x86/kvm/vmx.c
