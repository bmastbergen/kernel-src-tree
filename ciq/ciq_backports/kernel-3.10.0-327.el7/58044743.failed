i40e: Fix RS bit update in Tx path and disable force WB workaround

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Anjali Singhai <anjali.singhai@intel.com>
commit 5804474311912c1b80601a1afee052e8df962cd4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/58044743.failed

This patch fixes the issue of forcing WB too often causing us to not
benefit from NAPI.

Without this patch we were forcing WB/arming interrupt too often taking
away the benefits of NAPI and causing a performance impact.

With this patch we disable force WB in the clean routine for X710
and XL710 adapters. X722 adapters do not enable interrupt to force
a WB and benefit from WB_ON_ITR and hence force WB is left enabled
for those adapters.
For XL710 and X710 adapters if we have less than 4 packets pending
a software Interrupt triggered from service task will force a WB.

This patch also changes the conditions for setting RS bit as described
in code comments. This optimizes when the HW does a tail bump and when
it does a WB. It also optimizes when we do a wmb.

	Signed-off-by: Anjali Singhai Jain <anjali.singhai@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 5804474311912c1b80601a1afee052e8df962cd4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_txrx.c
#	drivers/net/ethernet/intel/i40e/i40e_txrx.h
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.c
index fd5c1122b275,3ce4900c0c43..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@@ -772,44 -726,23 +772,49 @@@ static bool i40e_clean_tx_irq(struct i4
  	tx_ring->q_vector->tx.total_bytes += total_bytes;
  	tx_ring->q_vector->tx.total_packets += total_packets;
  
- 	/* check to see if there are any non-cache aligned descriptors
- 	 * waiting to be written back, and kick the hardware to force
- 	 * them to be written back in case of napi polling
- 	 */
- 	if (budget &&
- 	    !((i & WB_STRIDE) == WB_STRIDE) &&
- 	    !test_bit(__I40E_DOWN, &tx_ring->vsi->state) &&
- 	    (I40E_DESC_UNUSED(tx_ring) != tx_ring->count))
- 		tx_ring->arm_wb = true;
- 	else
- 		tx_ring->arm_wb = false;
+ 	if (tx_ring->flags & I40E_TXR_FLAGS_WB_ON_ITR) {
+ 		unsigned int j = 0;
+ 
+ 		/* check to see if there are < 4 descriptors
+ 		 * waiting to be written back, then kick the hardware to force
+ 		 * them to be written back in case we stay in NAPI.
+ 		 * In this mode on X722 we do not enable Interrupt.
+ 		 */
+ 		j = i40e_get_tx_pending(tx_ring);
+ 
+ 		if (budget &&
+ 		    ((j / (WB_STRIDE + 1)) == 0) && (j != 0) &&
+ 		    !test_bit(__I40E_DOWN, &tx_ring->vsi->state) &&
+ 		    (I40E_DESC_UNUSED(tx_ring) != tx_ring->count))
+ 			tx_ring->arm_wb = true;
+ 	}
  
 +	if (check_for_tx_hang(tx_ring) && i40e_check_tx_hang(tx_ring)) {
 +		/* schedule immediate reset if we believe we hung */
 +		dev_info(tx_ring->dev, "Detected Tx Unit Hang\n"
 +			 "  VSI                  <%d>\n"
 +			 "  Tx Queue             <%d>\n"
 +			 "  next_to_use          <%x>\n"
 +			 "  next_to_clean        <%x>\n",
 +			 tx_ring->vsi->seid,
 +			 tx_ring->queue_index,
 +			 tx_ring->next_to_use, i);
 +
 +		netif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);
 +
 +		dev_info(tx_ring->dev,
 +			 "tx hang detected on queue %d, reset requested\n",
 +			 tx_ring->queue_index);
 +
 +		/* do not fire the reset immediately, wait for the stack to
 +		 * decide we are truly stuck, also prevents every queue from
 +		 * simultaneously requesting a reset
 +		 */
 +
 +		/* the adapter is about to reset, no point in enabling polling */
 +		budget = 1;
 +	}
 +
  	netdev_tx_completed_queue(netdev_get_tx_queue(tx_ring->netdev,
  						      tx_ring->queue_index),
  				  total_packets, total_bytes);
@@@ -2612,12 -2594,71 +2596,78 @@@ static inline void i40e_tx_map(struct i
  
  	tx_ring->next_to_use = i;
  
+ 	netdev_tx_sent_queue(netdev_get_tx_queue(tx_ring->netdev,
+ 						 tx_ring->queue_index),
+ 						 first->bytecount);
  	i40e_maybe_stop_tx(tx_ring, DESC_NEEDED);
+ 
+ 	/* Algorithm to optimize tail and RS bit setting:
+ 	 * if xmit_more is supported
+ 	 *	if xmit_more is true
+ 	 *		do not update tail and do not mark RS bit.
+ 	 *	if xmit_more is false and last xmit_more was false
+ 	 *		if every packet spanned less than 4 desc
+ 	 *			then set RS bit on 4th packet and update tail
+ 	 *			on every packet
+ 	 *		else
+ 	 *			update tail and set RS bit on every packet.
+ 	 *	if xmit_more is false and last_xmit_more was true
+ 	 *		update tail and set RS bit.
+ 	 *
+ 	 * Optimization: wmb to be issued only in case of tail update.
+ 	 * Also optimize the Descriptor WB path for RS bit with the same
+ 	 * algorithm.
+ 	 *
+ 	 * Note: If there are less than 4 packets
+ 	 * pending and interrupts were disabled the service task will
+ 	 * trigger a force WB.
+ 	 */
+ 	if (skb->xmit_more  &&
+ 	    !netif_xmit_stopped(netdev_get_tx_queue(tx_ring->netdev,
+ 						    tx_ring->queue_index))) {
+ 		tx_ring->flags |= I40E_TXR_FLAGS_LAST_XMIT_MORE_SET;
+ 		tail_bump = false;
+ 	} else if (!skb->xmit_more &&
+ 		   !netif_xmit_stopped(netdev_get_tx_queue(tx_ring->netdev,
+ 						       tx_ring->queue_index)) &&
+ 		   (!(tx_ring->flags & I40E_TXR_FLAGS_LAST_XMIT_MORE_SET)) &&
+ 		   (tx_ring->packet_stride < WB_STRIDE) &&
+ 		   (desc_count < WB_STRIDE)) {
+ 		tx_ring->packet_stride++;
+ 	} else {
+ 		tx_ring->packet_stride = 0;
+ 		tx_ring->flags &= ~I40E_TXR_FLAGS_LAST_XMIT_MORE_SET;
+ 		do_rs = true;
+ 	}
+ 	if (do_rs)
+ 		tx_ring->packet_stride = 0;
+ 
+ 	tx_desc->cmd_type_offset_bsz =
+ 			build_ctob(td_cmd, td_offset, size, td_tag) |
+ 			cpu_to_le64((u64)(do_rs ? I40E_TXD_CMD :
+ 						  I40E_TX_DESC_CMD_EOP) <<
+ 						  I40E_TXD_QW1_CMD_SHIFT);
+ 
  	/* notify HW of packet */
++<<<<<<< HEAD
 +	if (!skb->xmit_more ||
 +	    netif_xmit_stopped(netdev_get_tx_queue(tx_ring->netdev,
 +						   tx_ring->queue_index)))
 +		writel(i, tx_ring->tail);
++=======
+ 	if (!tail_bump)
+ 		prefetchw(tx_desc + 1);
++>>>>>>> 580447431191 (i40e: Fix RS bit update in Tx path and disable force WB workaround)
+ 
+ 	if (tail_bump) {
+ 		/* Force memory writes to complete before letting h/w
+ 		 * know there are new descriptors to fetch.  (Only
+ 		 * applicable for weak-ordered memory model archs,
+ 		 * such as IA-64).
+ 		 */
+ 		wmb();
+ 		writel(i, tx_ring->tail);
+ 	}
  
  	return;
  
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.h
index 94b8be9f72bb,a3978c2b5fc9..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
@@@ -252,7 -256,13 +252,16 @@@ struct i40e_ring 
  
  	bool ring_active;		/* is ring online or not */
  	bool arm_wb;		/* do something to arm write back */
+ 	u8 packet_stride;
  
++<<<<<<< HEAD
++=======
+ 	u16 flags;
+ #define I40E_TXR_FLAGS_WB_ON_ITR	BIT(0)
+ #define I40E_TXR_FLAGS_OUTER_UDP_CSUM	BIT(1)
+ #define I40E_TXR_FLAGS_LAST_XMIT_MORE_SET BIT(2)
+ 
++>>>>>>> 580447431191 (i40e: Fix RS bit update in Tx path and disable force WB workaround)
  	/* stats structs */
  	struct i40e_queue_stats	stats;
  	struct u64_stats_sync syncp;
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.c
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.h
