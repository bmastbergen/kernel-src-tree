KVM: x86: dump VMCS on invalid entry

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [kvm] x86: dump VMCS on invalid entry (Paolo Bonzini) [1202825]
Rebuild_FUZZ: 92.54%
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 4eb64dce8d0a3ceccc0036383e4683b7c3ea7a50
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/4eb64dce.failed

Code and format roughly based on Xen's vmcs_dump_vcpu.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 4eb64dce8d0a3ceccc0036383e4683b7c3ea7a50)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 8cdd0e21b5ed,63849a0334a6..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -7250,6 -7607,241 +7250,244 @@@ static void vmx_get_exit_info(struct kv
  	*info2 = vmcs_read32(VM_EXIT_INTR_INFO);
  }
  
++<<<<<<< HEAD
++=======
+ static int vmx_enable_pml(struct vcpu_vmx *vmx)
+ {
+ 	struct page *pml_pg;
+ 	u32 exec_control;
+ 
+ 	pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);
+ 	if (!pml_pg)
+ 		return -ENOMEM;
+ 
+ 	vmx->pml_pg = pml_pg;
+ 
+ 	vmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));
+ 	vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
+ 
+ 	exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
+ 	exec_control |= SECONDARY_EXEC_ENABLE_PML;
+ 	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);
+ 
+ 	return 0;
+ }
+ 
+ static void vmx_disable_pml(struct vcpu_vmx *vmx)
+ {
+ 	u32 exec_control;
+ 
+ 	ASSERT(vmx->pml_pg);
+ 	__free_page(vmx->pml_pg);
+ 	vmx->pml_pg = NULL;
+ 
+ 	exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
+ 	exec_control &= ~SECONDARY_EXEC_ENABLE_PML;
+ 	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);
+ }
+ 
+ static void vmx_flush_pml_buffer(struct vcpu_vmx *vmx)
+ {
+ 	struct kvm *kvm = vmx->vcpu.kvm;
+ 	u64 *pml_buf;
+ 	u16 pml_idx;
+ 
+ 	pml_idx = vmcs_read16(GUEST_PML_INDEX);
+ 
+ 	/* Do nothing if PML buffer is empty */
+ 	if (pml_idx == (PML_ENTITY_NUM - 1))
+ 		return;
+ 
+ 	/* PML index always points to next available PML buffer entity */
+ 	if (pml_idx >= PML_ENTITY_NUM)
+ 		pml_idx = 0;
+ 	else
+ 		pml_idx++;
+ 
+ 	pml_buf = page_address(vmx->pml_pg);
+ 	for (; pml_idx < PML_ENTITY_NUM; pml_idx++) {
+ 		u64 gpa;
+ 
+ 		gpa = pml_buf[pml_idx];
+ 		WARN_ON(gpa & (PAGE_SIZE - 1));
+ 		mark_page_dirty(kvm, gpa >> PAGE_SHIFT);
+ 	}
+ 
+ 	/* reset PML index */
+ 	vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
+ }
+ 
+ /*
+  * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.
+  * Called before reporting dirty_bitmap to userspace.
+  */
+ static void kvm_flush_pml_buffers(struct kvm *kvm)
+ {
+ 	int i;
+ 	struct kvm_vcpu *vcpu;
+ 	/*
+ 	 * We only need to kick vcpu out of guest mode here, as PML buffer
+ 	 * is flushed at beginning of all VMEXITs, and it's obvious that only
+ 	 * vcpus running in guest are possible to have unflushed GPAs in PML
+ 	 * buffer.
+ 	 */
+ 	kvm_for_each_vcpu(i, vcpu, kvm)
+ 		kvm_vcpu_kick(vcpu);
+ }
+ 
+ static void vmx_dump_sel(char *name, uint32_t sel)
+ {
+ 	pr_err("%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
+ 	       name, vmcs_read32(sel),
+ 	       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),
+ 	       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),
+ 	       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));
+ }
+ 
+ static void vmx_dump_dtsel(char *name, uint32_t limit)
+ {
+ 	pr_err("%s                           limit=0x%08x, base=0x%016lx\n",
+ 	       name, vmcs_read32(limit),
+ 	       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));
+ }
+ 
+ static void dump_vmcs(void)
+ {
+ 	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
+ 	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
+ 	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
+ 	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
+ 	u32 secondary_exec_control = 0;
+ 	unsigned long cr4 = vmcs_readl(GUEST_CR4);
+ 	u64 efer = vmcs_readl(GUEST_IA32_EFER);
+ 	int i, n;
+ 
+ 	if (cpu_has_secondary_exec_ctrls())
+ 		secondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
+ 
+ 	pr_err("*** Guest State ***\n");
+ 	pr_err("CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
+ 	       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),
+ 	       vmcs_readl(CR0_GUEST_HOST_MASK));
+ 	pr_err("CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
+ 	       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));
+ 	pr_err("CR3 = 0x%016lx\n", vmcs_readl(GUEST_CR3));
+ 	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&
+ 	    (cr4 & X86_CR4_PAE) && !(efer & EFER_LMA))
+ 	{
+ 		pr_err("PDPTR0 = 0x%016lx  PDPTR1 = 0x%016lx\n",
+ 		       vmcs_readl(GUEST_PDPTR0), vmcs_readl(GUEST_PDPTR1));
+ 		pr_err("PDPTR2 = 0x%016lx  PDPTR3 = 0x%016lx\n",
+ 		       vmcs_readl(GUEST_PDPTR2), vmcs_readl(GUEST_PDPTR3));
+ 	}
+ 	pr_err("RSP = 0x%016lx  RIP = 0x%016lx\n",
+ 	       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
+ 	pr_err("RFLAGS=0x%08lx         DR7 = 0x%016lx\n",
+ 	       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));
+ 	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
+ 	       vmcs_readl(GUEST_SYSENTER_ESP),
+ 	       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));
+ 	vmx_dump_sel("CS:  ", GUEST_CS_SELECTOR);
+ 	vmx_dump_sel("DS:  ", GUEST_DS_SELECTOR);
+ 	vmx_dump_sel("SS:  ", GUEST_SS_SELECTOR);
+ 	vmx_dump_sel("ES:  ", GUEST_ES_SELECTOR);
+ 	vmx_dump_sel("FS:  ", GUEST_FS_SELECTOR);
+ 	vmx_dump_sel("GS:  ", GUEST_GS_SELECTOR);
+ 	vmx_dump_dtsel("GDTR:", GUEST_GDTR_LIMIT);
+ 	vmx_dump_sel("LDTR:", GUEST_LDTR_SELECTOR);
+ 	vmx_dump_dtsel("IDTR:", GUEST_IDTR_LIMIT);
+ 	vmx_dump_sel("TR:  ", GUEST_TR_SELECTOR);
+ 	if ((vmexit_ctl & (VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER)) ||
+ 	    (vmentry_ctl & (VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_IA32_EFER)))
+ 		pr_err("EFER =     0x%016llx  PAT = 0x%016lx\n",
+ 		       efer, vmcs_readl(GUEST_IA32_PAT));
+ 	pr_err("DebugCtl = 0x%016lx  DebugExceptions = 0x%016lx\n",
+ 	       vmcs_readl(GUEST_IA32_DEBUGCTL),
+ 	       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));
+ 	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)
+ 		pr_err("PerfGlobCtl = 0x%016lx\n",
+ 		       vmcs_readl(GUEST_IA32_PERF_GLOBAL_CTRL));
+ 	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)
+ 		pr_err("BndCfgS = 0x%016lx\n", vmcs_readl(GUEST_BNDCFGS));
+ 	pr_err("Interruptibility = %08x  ActivityState = %08x\n",
+ 	       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
+ 	       vmcs_read32(GUEST_ACTIVITY_STATE));
+ 	if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+ 		pr_err("InterruptStatus = %04x\n",
+ 		       vmcs_read16(GUEST_INTR_STATUS));
+ 
+ 	pr_err("*** Host State ***\n");
+ 	pr_err("RIP = 0x%016lx  RSP = 0x%016lx\n",
+ 	       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));
+ 	pr_err("CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\n",
+ 	       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),
+ 	       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),
+ 	       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),
+ 	       vmcs_read16(HOST_TR_SELECTOR));
+ 	pr_err("FSBase=%016lx GSBase=%016lx TRBase=%016lx\n",
+ 	       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),
+ 	       vmcs_readl(HOST_TR_BASE));
+ 	pr_err("GDTBase=%016lx IDTBase=%016lx\n",
+ 	       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));
+ 	pr_err("CR0=%016lx CR3=%016lx CR4=%016lx\n",
+ 	       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),
+ 	       vmcs_readl(HOST_CR4));
+ 	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
+ 	       vmcs_readl(HOST_IA32_SYSENTER_ESP),
+ 	       vmcs_read32(HOST_IA32_SYSENTER_CS),
+ 	       vmcs_readl(HOST_IA32_SYSENTER_EIP));
+ 	if (vmexit_ctl & (VM_EXIT_LOAD_IA32_PAT | VM_EXIT_LOAD_IA32_EFER))
+ 		pr_err("EFER = 0x%016lx  PAT = 0x%016lx\n",
+ 		       vmcs_readl(HOST_IA32_EFER), vmcs_readl(HOST_IA32_PAT));
+ 	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)
+ 		pr_err("PerfGlobCtl = 0x%016lx\n",
+ 		       vmcs_readl(HOST_IA32_PERF_GLOBAL_CTRL));
+ 
+ 	pr_err("*** Control State ***\n");
+ 	pr_err("PinBased=%08x CPUBased=%08x SecondaryExec=%08x\n",
+ 	       pin_based_exec_ctrl, cpu_based_exec_ctrl, secondary_exec_control);
+ 	pr_err("EntryControls=%08x ExitControls=%08x\n", vmentry_ctl, vmexit_ctl);
+ 	pr_err("ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\n",
+ 	       vmcs_read32(EXCEPTION_BITMAP),
+ 	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),
+ 	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));
+ 	pr_err("VMEntry: intr_info=%08x errcode=%08x ilen=%08x\n",
+ 	       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
+ 	       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),
+ 	       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));
+ 	pr_err("VMExit: intr_info=%08x errcode=%08x ilen=%08x\n",
+ 	       vmcs_read32(VM_EXIT_INTR_INFO),
+ 	       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
+ 	       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));
+ 	pr_err("        reason=%08x qualification=%016lx\n",
+ 	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
+ 	pr_err("IDTVectoring: info=%08x errcode=%08x\n",
+ 	       vmcs_read32(IDT_VECTORING_INFO_FIELD),
+ 	       vmcs_read32(IDT_VECTORING_ERROR_CODE));
+ 	pr_err("TSC Offset = 0x%016lx\n", vmcs_readl(TSC_OFFSET));
+ 	if (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW)
+ 		pr_err("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));
+ 	if (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)
+ 		pr_err("PostedIntrVec = 0x%02x\n", vmcs_read16(POSTED_INTR_NV));
+ 	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))
+ 		pr_err("EPT pointer = 0x%016lx\n", vmcs_readl(EPT_POINTER));
+ 	n = vmcs_read32(CR3_TARGET_COUNT);
+ 	for (i = 0; i + 1 < n; i += 4)
+ 		pr_err("CR3 target%u=%016lx target%u=%016lx\n",
+ 		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2),
+ 		       i + 1, vmcs_readl(CR3_TARGET_VALUE0 + i * 2 + 2));
+ 	if (i < n)
+ 		pr_err("CR3 target%u=%016lx\n",
+ 		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2));
+ 	if (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)
+ 		pr_err("PLE Gap=%08x Window=%08x\n",
+ 		       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));
+ 	if (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)
+ 		pr_err("Virtual processor ID = 0x%04x\n",
+ 		       vmcs_read16(VIRTUAL_PROCESSOR_ID));
+ }
+ 
++>>>>>>> 4eb64dce8d0a (KVM: x86: dump VMCS on invalid entry)
  /*
   * The guest has exited.  See if we can fix it or if we need userspace
   * assistance.
* Unmerged path arch/x86/kvm/vmx.c
