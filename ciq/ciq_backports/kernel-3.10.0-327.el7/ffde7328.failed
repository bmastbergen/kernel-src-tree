net: Split netdev_alloc_frag into __alloc_page_frag and add __napi_alloc_frag

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] Split netdev_alloc_frag into __alloc_page_frag and add __napi_alloc_frag (Alexander Duyck) [1205273]
Rebuild_FUZZ: 96.64%
commit-author Alexander Duyck <alexander.h.duyck@redhat.com>
commit ffde7328a36d16e626bae8468571858d71cd010b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/ffde7328.failed

This patch splits the netdev_alloc_frag function up so that it can be used
on one of two page frag pools instead of being fixed on the
netdev_alloc_cache.  By doing this we can add a NAPI specific function
__napi_alloc_frag that accesses a pool that is only used from softirq
context.  The advantage to this is that we do not need to call
local_irq_save/restore which can be a significant savings.

I also took the opportunity to refactor the core bits that were placed in
__alloc_page_frag.  First I updated the allocation to do either a 32K
allocation or an order 0 page.  This is based on the changes in commmit
d9b2938aa where it was found that latencies could be reduced in case of
failures.  Then I also rewrote the logic to work from the end of the page to
the start.  By doing this the size value doesn't have to be used unless we
have run out of space for page fragments.  Finally I cleaned up the atomic
bits so that we just do an atomic_sub_and_test and if that returns true then
we set the page->_count via an atomic_set.  This way we can remove the extra
conditional for the atomic_read since it would have led to an atomic_inc in
the case of success anyway.

	Signed-off-by: Alexander Duyck <alexander.h.duyck@redhat.com>
	Acked-by: Alexei Starovoitov <ast@plumgrid.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ffde7328a36d16e626bae8468571858d71cd010b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/skbuff.h
#	net/core/skbuff.c
diff --cc include/linux/skbuff.h
index 67a799712ba7,736cc99f3f6c..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -2097,32 -2164,36 +2097,41 @@@ static inline struct sk_buff *netdev_al
  	return __netdev_alloc_skb_ip_align(dev, length, GFP_ATOMIC);
  }
  
++<<<<<<< HEAD
 +/*
 + *	__skb_alloc_page - allocate pages for ps-rx on a skb and preserve pfmemalloc data
 + *	@gfp_mask: alloc_pages_node mask. Set __GFP_NOMEMALLOC if not for network packet RX
 + *	@skb: skb to set pfmemalloc on if __GFP_MEMALLOC is used
 + *	@order: size of the allocation
++=======
+ void *napi_alloc_frag(unsigned int fragsz);
+ 
+ /**
+  * __dev_alloc_pages - allocate page for network Rx
+  * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx
+  * @order: size of the allocation
++>>>>>>> ffde7328a36d (net: Split netdev_alloc_frag into __alloc_page_frag and add __napi_alloc_frag)
   *
 - * Allocate a new page.
 + * 	Allocate a new page.
   *
 - * %NULL is returned if there is no free memory.
 + * 	%NULL is returned if there is no free memory.
  */
 -static inline struct page *__dev_alloc_pages(gfp_t gfp_mask,
 -					     unsigned int order)
 -{
 -	/* This piece of code contains several assumptions.
 -	 * 1.  This is for device Rx, therefor a cold page is preferred.
 -	 * 2.  The expectation is the user wants a compound page.
 -	 * 3.  If requesting a order 0 page it will not be compound
 -	 *     due to the check to see if order has a value in prep_new_page
 -	 * 4.  __GFP_MEMALLOC is ignored if __GFP_NOMEMALLOC is set due to
 -	 *     code in gfp_to_alloc_flags that should be enforcing this.
 -	 */
 -	gfp_mask |= __GFP_COLD | __GFP_COMP | __GFP_MEMALLOC;
 +static inline struct page *__skb_alloc_pages(gfp_t gfp_mask,
 +					      struct sk_buff *skb,
 +					      unsigned int order)
 +{
 +	struct page *page;
  
 -	return alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);
 -}
 +	gfp_mask |= __GFP_COLD;
  
 -static inline struct page *dev_alloc_pages(unsigned int order)
 -{
 -	return __dev_alloc_pages(GFP_ATOMIC, order);
 +	if (!(gfp_mask & __GFP_NOMEMALLOC))
 +		gfp_mask |= __GFP_MEMALLOC;
 +
 +	page = alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);
 +	if (skb && page && page->pfmemalloc)
 +		skb->pfmemalloc = true;
 +
 +	return page;
  }
  
  /**
diff --cc net/core/skbuff.c
index af49e6e94c48,56ed17cd2151..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -363,48 -336,85 +363,115 @@@ struct netdev_alloc_cache 
  	unsigned int		pagecnt_bias;
  };
  static DEFINE_PER_CPU(struct netdev_alloc_cache, netdev_alloc_cache);
+ static DEFINE_PER_CPU(struct netdev_alloc_cache, napi_alloc_cache);
  
- static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
+ static struct page *__page_frag_refill(struct netdev_alloc_cache *nc,
+ 				       gfp_t gfp_mask)
  {
- 	struct netdev_alloc_cache *nc;
- 	void *data = NULL;
- 	int order;
- 	unsigned long flags;
+ 	const unsigned int order = NETDEV_FRAG_PAGE_MAX_ORDER;
+ 	struct page *page = NULL;
+ 	gfp_t gfp = gfp_mask;
  
++<<<<<<< HEAD
 +	local_irq_save(flags);
 +	nc = &__get_cpu_var(netdev_alloc_cache);
 +	if (unlikely(!nc->frag.page)) {
++=======
+ 	if (order) {
+ 		gfp_mask |= __GFP_COMP | __GFP_NOWARN | __GFP_NORETRY;
+ 		page = alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);
+ 		nc->frag.size = PAGE_SIZE << (page ? order : 0);
+ 	}
+ 
+ 	if (unlikely(!page))
+ 		page = alloc_pages_node(NUMA_NO_NODE, gfp, 0);
+ 
+ 	nc->frag.page = page;
+ 
+ 	return page;
+ }
+ 
+ static void *__alloc_page_frag(struct netdev_alloc_cache __percpu *cache,
+ 			       unsigned int fragsz, gfp_t gfp_mask)
+ {
+ 	struct netdev_alloc_cache *nc = this_cpu_ptr(cache);
+ 	struct page *page = nc->frag.page;
+ 	unsigned int size;
+ 	int offset;
+ 
+ 	if (unlikely(!page)) {
++>>>>>>> ffde7328a36d (net: Split netdev_alloc_frag into __alloc_page_frag and add __napi_alloc_frag)
  refill:
- 		for (order = NETDEV_FRAG_PAGE_MAX_ORDER; ;) {
- 			gfp_t gfp = gfp_mask;
+ 		page = __page_frag_refill(nc, gfp_mask);
+ 		if (!page)
+ 			return NULL;
  
+ 		/* if size can vary use frag.size else just use PAGE_SIZE */
+ 		size = NETDEV_FRAG_PAGE_MAX_ORDER ? nc->frag.size : PAGE_SIZE;
+ 
++<<<<<<< HEAD
 +			if (order)
 +				gfp |= __GFP_COMP | __GFP_NOWARN;
 +			nc->frag.page = alloc_pages(gfp, order);
 +			if (likely(nc->frag.page))
 +				break;
 +			if (--order < 0)
 +				goto end;
 +		}
 +		nc->frag.size = PAGE_SIZE << order;
 +recycle:
 +		atomic_set(&nc->frag.page->_count, NETDEV_PAGECNT_MAX_BIAS);
 +		nc->pagecnt_bias = NETDEV_PAGECNT_MAX_BIAS;
 +		nc->frag.offset = 0;
 +	}
 +
 +	if (nc->frag.offset + fragsz > nc->frag.size) {
 +		/* avoid unnecessary locked operations if possible */
 +		if ((atomic_read(&nc->frag.page->_count) == nc->pagecnt_bias) ||
 +		    atomic_sub_and_test(nc->pagecnt_bias, &nc->frag.page->_count))
 +			goto recycle;
 +		goto refill;
++=======
+ 		/* Even if we own the page, we do not use atomic_set().
+ 		 * This would break get_page_unless_zero() users.
+ 		 */
+ 		atomic_add(size - 1, &page->_count);
+ 
+ 		/* reset page count bias and offset to start of new frag */
+ 		nc->pagecnt_bias = size;
+ 		nc->frag.offset = size;
+ 	}
+ 
+ 	offset = nc->frag.offset - fragsz;
+ 	if (unlikely(offset < 0)) {
+ 		if (!atomic_sub_and_test(nc->pagecnt_bias, &page->_count))
+ 			goto refill;
+ 
+ 		/* if size can vary use frag.size else just use PAGE_SIZE */
+ 		size = NETDEV_FRAG_PAGE_MAX_ORDER ? nc->frag.size : PAGE_SIZE;
+ 
+ 		/* OK, page count is 0, we can safely set it */
+ 		atomic_set(&page->_count, size);
+ 
+ 		/* reset page count bias and offset to start of new frag */
+ 		nc->pagecnt_bias = size;
+ 		offset = size - fragsz;
++>>>>>>> ffde7328a36d (net: Split netdev_alloc_frag into __alloc_page_frag and add __napi_alloc_frag)
  	}
  
- 	data = page_address(nc->frag.page) + nc->frag.offset;
- 	nc->frag.offset += fragsz;
  	nc->pagecnt_bias--;
- end:
+ 	nc->frag.offset = offset;
+ 
+ 	return page_address(page) + offset;
+ }
+ 
+ static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
+ {
+ 	unsigned long flags;
+ 	void *data;
+ 
+ 	local_irq_save(flags);
+ 	data = __alloc_page_frag(&netdev_alloc_cache, fragsz, gfp_mask);
  	local_irq_restore(flags);
  	return data;
  }
* Unmerged path include/linux/skbuff.h
* Unmerged path net/core/skbuff.c
