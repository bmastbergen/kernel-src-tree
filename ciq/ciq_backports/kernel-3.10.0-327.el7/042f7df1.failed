workqueue: Allow modifying low level unbound workqueue cpumask

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Lai Jiangshan <laijs@cn.fujitsu.com>
commit 042f7df15a4fff8eec42873f755aea848dcdedd1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/042f7df1.failed

Allow to modify the low-level unbound workqueues cpumask through
sysfs. This is performed by traversing the entire workqueue list
and calling apply_wqattrs_prepare() on the unbound workqueues
with the new low level mask. Only after all the preparation are done,
we commit them all together.

Ordered workqueues are ignored from the low level unbound workqueue
cpumask, it will be handled in near future.

All the (default & per-node) pwqs are mandatorily controlled by
the low level cpumask. If the user configured cpumask doesn't overlap
with the low level cpumask, the low level cpumask will be used for the
wq instead.

The comment of wq_calc_node_cpumask() is updated and explicitly
requires that its first argument should be the attrs of the default
pwq.

The default wq_unbound_cpumask is cpu_possible_mask.  The workqueue
subsystem doesn't know its best default value, let the system manager
or the other subsystem set it when needed.

Changed from V8:
  merge the calculating code for the attrs of the default pwq together.
  minor change the code&comments for saving the user configured attrs.
  remove unnecessary list_del().
  minor update the comment of wq_calc_node_cpumask().
  update the comment of workqueue_set_unbound_cpumask();

	Cc: Christoph Lameter <cl@linux.com>
	Cc: Kevin Hilman <khilman@linaro.org>
	Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
	Cc: Mike Galbraith <bitbucket@online.de>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
Original-patch-by: Frederic Weisbecker <fweisbec@gmail.com>
	Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 042f7df15a4fff8eec42873f755aea848dcdedd1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/workqueue.c
diff --cc kernel/workqueue.c
index 14a71630e038,a3915abc1983..000000000000
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@@ -280,9 -296,11 +280,14 @@@ static struct workqueue_attrs *wq_updat
  static DEFINE_MUTEX(wq_pool_mutex);	/* protects pools and workqueues list */
  static DEFINE_SPINLOCK(wq_mayday_lock);	/* protects wq->maydays list */
  
 -static LIST_HEAD(workqueues);		/* PR: list of all workqueues */
 +static LIST_HEAD(workqueues);		/* PL: list of all workqueues */
  static bool workqueue_freezing;		/* PL: have wqs started freezing? */
  
++<<<<<<< HEAD
++=======
+ static cpumask_var_t wq_unbound_cpumask; /* PL: low level cpumask for all unbound wqs */
+ 
++>>>>>>> 042f7df15a4f (workqueue: Allow modifying low level unbound workqueue cpumask)
  /* the per-cpu worker pools */
  static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS],
  				     cpu_worker_pools);
@@@ -3772,20 -3427,9 +3777,20 @@@ static struct pool_workqueue *alloc_unb
  	return pwq;
  }
  
 +/* undo alloc_unbound_pwq(), used only in the error path */
 +static void free_unbound_pwq(struct pool_workqueue *pwq)
 +{
 +	lockdep_assert_held(&wq_pool_mutex);
 +
 +	if (pwq) {
 +		put_unbound_pool(pwq->pool);
 +		kmem_cache_free(pwq_cache, pwq);
 +	}
 +}
 +
  /**
   * wq_calc_node_mask - calculate a wq_attrs' cpumask for the specified node
-  * @attrs: the wq_attrs of interest
+  * @attrs: the wq_attrs of the default pwq of the target workqueue
   * @node: the target NUMA node
   * @cpu_going_down: if >= 0, the CPU to consider as offline
   * @cpumask: outarg, the resulting cpumask
@@@ -3844,6 -3489,125 +3849,128 @@@ static struct pool_workqueue *numa_pwq_
  	return old_pwq;
  }
  
++<<<<<<< HEAD
++=======
+ /* context to store the prepared attrs & pwqs before applying */
+ struct apply_wqattrs_ctx {
+ 	struct workqueue_struct	*wq;		/* target workqueue */
+ 	struct workqueue_attrs	*attrs;		/* attrs to apply */
+ 	struct list_head	list;		/* queued for batching commit */
+ 	struct pool_workqueue	*dfl_pwq;
+ 	struct pool_workqueue	*pwq_tbl[];
+ };
+ 
+ /* free the resources after success or abort */
+ static void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)
+ {
+ 	if (ctx) {
+ 		int node;
+ 
+ 		for_each_node(node)
+ 			put_pwq_unlocked(ctx->pwq_tbl[node]);
+ 		put_pwq_unlocked(ctx->dfl_pwq);
+ 
+ 		free_workqueue_attrs(ctx->attrs);
+ 
+ 		kfree(ctx);
+ 	}
+ }
+ 
+ /* allocate the attrs and pwqs for later installation */
+ static struct apply_wqattrs_ctx *
+ apply_wqattrs_prepare(struct workqueue_struct *wq,
+ 		      const struct workqueue_attrs *attrs)
+ {
+ 	struct apply_wqattrs_ctx *ctx;
+ 	struct workqueue_attrs *new_attrs, *tmp_attrs;
+ 	int node;
+ 
+ 	lockdep_assert_held(&wq_pool_mutex);
+ 
+ 	ctx = kzalloc(sizeof(*ctx) + nr_node_ids * sizeof(ctx->pwq_tbl[0]),
+ 		      GFP_KERNEL);
+ 
+ 	new_attrs = alloc_workqueue_attrs(GFP_KERNEL);
+ 	tmp_attrs = alloc_workqueue_attrs(GFP_KERNEL);
+ 	if (!ctx || !new_attrs || !tmp_attrs)
+ 		goto out_free;
+ 
+ 	/*
+ 	 * Calculate the attrs of the default pwq.
+ 	 * If the user configured cpumask doesn't overlap with the
+ 	 * wq_unbound_cpumask, we fallback to the wq_unbound_cpumask.
+ 	 */
+ 	copy_workqueue_attrs(new_attrs, attrs);
+ 	cpumask_and(new_attrs->cpumask, new_attrs->cpumask, wq_unbound_cpumask);
+ 	if (unlikely(cpumask_empty(new_attrs->cpumask)))
+ 		cpumask_copy(new_attrs->cpumask, wq_unbound_cpumask);
+ 
+ 	/*
+ 	 * We may create multiple pwqs with differing cpumasks.  Make a
+ 	 * copy of @new_attrs which will be modified and used to obtain
+ 	 * pools.
+ 	 */
+ 	copy_workqueue_attrs(tmp_attrs, new_attrs);
+ 
+ 	/*
+ 	 * If something goes wrong during CPU up/down, we'll fall back to
+ 	 * the default pwq covering whole @attrs->cpumask.  Always create
+ 	 * it even if we don't use it immediately.
+ 	 */
+ 	ctx->dfl_pwq = alloc_unbound_pwq(wq, new_attrs);
+ 	if (!ctx->dfl_pwq)
+ 		goto out_free;
+ 
+ 	for_each_node(node) {
+ 		if (wq_calc_node_cpumask(new_attrs, node, -1, tmp_attrs->cpumask)) {
+ 			ctx->pwq_tbl[node] = alloc_unbound_pwq(wq, tmp_attrs);
+ 			if (!ctx->pwq_tbl[node])
+ 				goto out_free;
+ 		} else {
+ 			ctx->dfl_pwq->refcnt++;
+ 			ctx->pwq_tbl[node] = ctx->dfl_pwq;
+ 		}
+ 	}
+ 
+ 	/* save the user configured attrs and sanitize it. */
+ 	copy_workqueue_attrs(new_attrs, attrs);
+ 	cpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);
+ 	ctx->attrs = new_attrs;
+ 
+ 	ctx->wq = wq;
+ 	free_workqueue_attrs(tmp_attrs);
+ 	return ctx;
+ 
+ out_free:
+ 	free_workqueue_attrs(tmp_attrs);
+ 	free_workqueue_attrs(new_attrs);
+ 	apply_wqattrs_cleanup(ctx);
+ 	return NULL;
+ }
+ 
+ /* set attrs and install prepared pwqs, @ctx points to old pwqs on return */
+ static void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)
+ {
+ 	int node;
+ 
+ 	/* all pwqs have been created successfully, let's install'em */
+ 	mutex_lock(&ctx->wq->mutex);
+ 
+ 	copy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);
+ 
+ 	/* save the previous pwq and install the new one */
+ 	for_each_node(node)
+ 		ctx->pwq_tbl[node] = numa_pwq_tbl_install(ctx->wq, node,
+ 							  ctx->pwq_tbl[node]);
+ 
+ 	/* @dfl_pwq might not have been used, ensure it's linked */
+ 	link_pwq(ctx->dfl_pwq);
+ 	swap(ctx->wq->dfl_pwq, ctx->dfl_pwq);
+ 
+ 	mutex_unlock(&ctx->wq->mutex);
+ }
+ 
++>>>>>>> 042f7df15a4f (workqueue: Allow modifying low level unbound workqueue cpumask)
  /**
   * apply_workqueue_attrs - apply new workqueue_attrs to an unbound workqueue
   * @wq: the target workqueue
@@@ -4016,12 -3715,11 +4143,18 @@@ static void wq_update_unbound_numa(stru
  
  	/*
  	 * Let's determine what needs to be done.  If the target cpumask is
++<<<<<<< HEAD
 +	 * different from wq's, we need to compare it to @pwq's and create
 +	 * a new one if they don't match.  If the target cpumask equals
 +	 * wq's, the default pwq should be used.  If @pwq is already the
 +	 * default one, nothing to do; otherwise, install the default one.
++=======
+ 	 * different from the default pwq's, we need to compare it to @pwq's
+ 	 * and create a new one if they don't match.  If the target cpumask
+ 	 * equals the default pwq's, the default pwq should be used.
++>>>>>>> 042f7df15a4f (workqueue: Allow modifying low level unbound workqueue cpumask)
  	 */
- 	if (wq_calc_node_cpumask(wq->unbound_attrs, node, cpu_off, cpumask)) {
+ 	if (wq_calc_node_cpumask(wq->dfl_pwq->pool->attrs, node, cpu_off, cpumask)) {
  		if (cpumask_equal(cpumask, pwq->pool->attrs->cpumask))
  			goto out_unlock;
  	} else {
@@@ -4909,6 -4742,441 +5042,444 @@@ out_unlock
  }
  #endif /* CONFIG_FREEZER */
  
++<<<<<<< HEAD
++=======
+ static int workqueue_apply_unbound_cpumask(void)
+ {
+ 	LIST_HEAD(ctxs);
+ 	int ret = 0;
+ 	struct workqueue_struct *wq;
+ 	struct apply_wqattrs_ctx *ctx, *n;
+ 
+ 	lockdep_assert_held(&wq_pool_mutex);
+ 
+ 	list_for_each_entry(wq, &workqueues, list) {
+ 		if (!(wq->flags & WQ_UNBOUND))
+ 			continue;
+ 		/* creating multiple pwqs breaks ordering guarantee */
+ 		if (wq->flags & __WQ_ORDERED)
+ 			continue;
+ 
+ 		ctx = apply_wqattrs_prepare(wq, wq->unbound_attrs);
+ 		if (!ctx) {
+ 			ret = -ENOMEM;
+ 			break;
+ 		}
+ 
+ 		list_add_tail(&ctx->list, &ctxs);
+ 	}
+ 
+ 	list_for_each_entry_safe(ctx, n, &ctxs, list) {
+ 		if (!ret)
+ 			apply_wqattrs_commit(ctx);
+ 		apply_wqattrs_cleanup(ctx);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ /**
+  *  workqueue_set_unbound_cpumask - Set the low-level unbound cpumask
+  *  @cpumask: the cpumask to set
+  *
+  *  The low-level workqueues cpumask is a global cpumask that limits
+  *  the affinity of all unbound workqueues.  This function check the @cpumask
+  *  and apply it to all unbound workqueues and updates all pwqs of them.
+  *
+  *  Retun:	0	- Success
+  *  		-EINVAL	- Invalid @cpumask
+  *  		-ENOMEM	- Failed to allocate memory for attrs or pwqs.
+  */
+ int workqueue_set_unbound_cpumask(cpumask_var_t cpumask)
+ {
+ 	int ret = -EINVAL;
+ 	cpumask_var_t saved_cpumask;
+ 
+ 	if (!zalloc_cpumask_var(&saved_cpumask, GFP_KERNEL))
+ 		return -ENOMEM;
+ 
+ 	get_online_cpus();
+ 	cpumask_and(cpumask, cpumask, cpu_possible_mask);
+ 	if (!cpumask_empty(cpumask)) {
+ 		mutex_lock(&wq_pool_mutex);
+ 
+ 		/* save the old wq_unbound_cpumask. */
+ 		cpumask_copy(saved_cpumask, wq_unbound_cpumask);
+ 
+ 		/* update wq_unbound_cpumask at first and apply it to wqs. */
+ 		cpumask_copy(wq_unbound_cpumask, cpumask);
+ 		ret = workqueue_apply_unbound_cpumask();
+ 
+ 		/* restore the wq_unbound_cpumask when failed. */
+ 		if (ret < 0)
+ 			cpumask_copy(wq_unbound_cpumask, saved_cpumask);
+ 
+ 		mutex_unlock(&wq_pool_mutex);
+ 	}
+ 	put_online_cpus();
+ 
+ 	free_cpumask_var(saved_cpumask);
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_SYSFS
+ /*
+  * Workqueues with WQ_SYSFS flag set is visible to userland via
+  * /sys/bus/workqueue/devices/WQ_NAME.  All visible workqueues have the
+  * following attributes.
+  *
+  *  per_cpu	RO bool	: whether the workqueue is per-cpu or unbound
+  *  max_active	RW int	: maximum number of in-flight work items
+  *
+  * Unbound workqueues have the following extra attributes.
+  *
+  *  id		RO int	: the associated pool ID
+  *  nice	RW int	: nice value of the workers
+  *  cpumask	RW mask	: bitmask of allowed CPUs for the workers
+  */
+ struct wq_device {
+ 	struct workqueue_struct		*wq;
+ 	struct device			dev;
+ };
+ 
+ static struct workqueue_struct *dev_to_wq(struct device *dev)
+ {
+ 	struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);
+ 
+ 	return wq_dev->wq;
+ }
+ 
+ static ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,
+ 			    char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 
+ 	return scnprintf(buf, PAGE_SIZE, "%d\n", (bool)!(wq->flags & WQ_UNBOUND));
+ }
+ static DEVICE_ATTR_RO(per_cpu);
+ 
+ static ssize_t max_active_show(struct device *dev,
+ 			       struct device_attribute *attr, char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 
+ 	return scnprintf(buf, PAGE_SIZE, "%d\n", wq->saved_max_active);
+ }
+ 
+ static ssize_t max_active_store(struct device *dev,
+ 				struct device_attribute *attr, const char *buf,
+ 				size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int val;
+ 
+ 	if (sscanf(buf, "%d", &val) != 1 || val <= 0)
+ 		return -EINVAL;
+ 
+ 	workqueue_set_max_active(wq, val);
+ 	return count;
+ }
+ static DEVICE_ATTR_RW(max_active);
+ 
+ static struct attribute *wq_sysfs_attrs[] = {
+ 	&dev_attr_per_cpu.attr,
+ 	&dev_attr_max_active.attr,
+ 	NULL,
+ };
+ ATTRIBUTE_GROUPS(wq_sysfs);
+ 
+ static ssize_t wq_pool_ids_show(struct device *dev,
+ 				struct device_attribute *attr, char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	const char *delim = "";
+ 	int node, written = 0;
+ 
+ 	rcu_read_lock_sched();
+ 	for_each_node(node) {
+ 		written += scnprintf(buf + written, PAGE_SIZE - written,
+ 				     "%s%d:%d", delim, node,
+ 				     unbound_pwq_by_node(wq, node)->pool->id);
+ 		delim = " ";
+ 	}
+ 	written += scnprintf(buf + written, PAGE_SIZE - written, "\n");
+ 	rcu_read_unlock_sched();
+ 
+ 	return written;
+ }
+ 
+ static ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,
+ 			    char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int written;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	written = scnprintf(buf, PAGE_SIZE, "%d\n", wq->unbound_attrs->nice);
+ 	mutex_unlock(&wq->mutex);
+ 
+ 	return written;
+ }
+ 
+ /* prepare workqueue_attrs for sysfs store operations */
+ static struct workqueue_attrs *wq_sysfs_prep_attrs(struct workqueue_struct *wq)
+ {
+ 	struct workqueue_attrs *attrs;
+ 
+ 	attrs = alloc_workqueue_attrs(GFP_KERNEL);
+ 	if (!attrs)
+ 		return NULL;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	copy_workqueue_attrs(attrs, wq->unbound_attrs);
+ 	mutex_unlock(&wq->mutex);
+ 	return attrs;
+ }
+ 
+ static ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,
+ 			     const char *buf, size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	struct workqueue_attrs *attrs;
+ 	int ret;
+ 
+ 	attrs = wq_sysfs_prep_attrs(wq);
+ 	if (!attrs)
+ 		return -ENOMEM;
+ 
+ 	if (sscanf(buf, "%d", &attrs->nice) == 1 &&
+ 	    attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)
+ 		ret = apply_workqueue_attrs(wq, attrs);
+ 	else
+ 		ret = -EINVAL;
+ 
+ 	free_workqueue_attrs(attrs);
+ 	return ret ?: count;
+ }
+ 
+ static ssize_t wq_cpumask_show(struct device *dev,
+ 			       struct device_attribute *attr, char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int written;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	written = scnprintf(buf, PAGE_SIZE, "%*pb\n",
+ 			    cpumask_pr_args(wq->unbound_attrs->cpumask));
+ 	mutex_unlock(&wq->mutex);
+ 	return written;
+ }
+ 
+ static ssize_t wq_cpumask_store(struct device *dev,
+ 				struct device_attribute *attr,
+ 				const char *buf, size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	struct workqueue_attrs *attrs;
+ 	int ret;
+ 
+ 	attrs = wq_sysfs_prep_attrs(wq);
+ 	if (!attrs)
+ 		return -ENOMEM;
+ 
+ 	ret = cpumask_parse(buf, attrs->cpumask);
+ 	if (!ret)
+ 		ret = apply_workqueue_attrs(wq, attrs);
+ 
+ 	free_workqueue_attrs(attrs);
+ 	return ret ?: count;
+ }
+ 
+ static ssize_t wq_numa_show(struct device *dev, struct device_attribute *attr,
+ 			    char *buf)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	int written;
+ 
+ 	mutex_lock(&wq->mutex);
+ 	written = scnprintf(buf, PAGE_SIZE, "%d\n",
+ 			    !wq->unbound_attrs->no_numa);
+ 	mutex_unlock(&wq->mutex);
+ 
+ 	return written;
+ }
+ 
+ static ssize_t wq_numa_store(struct device *dev, struct device_attribute *attr,
+ 			     const char *buf, size_t count)
+ {
+ 	struct workqueue_struct *wq = dev_to_wq(dev);
+ 	struct workqueue_attrs *attrs;
+ 	int v, ret;
+ 
+ 	attrs = wq_sysfs_prep_attrs(wq);
+ 	if (!attrs)
+ 		return -ENOMEM;
+ 
+ 	ret = -EINVAL;
+ 	if (sscanf(buf, "%d", &v) == 1) {
+ 		attrs->no_numa = !v;
+ 		ret = apply_workqueue_attrs(wq, attrs);
+ 	}
+ 
+ 	free_workqueue_attrs(attrs);
+ 	return ret ?: count;
+ }
+ 
+ static struct device_attribute wq_sysfs_unbound_attrs[] = {
+ 	__ATTR(pool_ids, 0444, wq_pool_ids_show, NULL),
+ 	__ATTR(nice, 0644, wq_nice_show, wq_nice_store),
+ 	__ATTR(cpumask, 0644, wq_cpumask_show, wq_cpumask_store),
+ 	__ATTR(numa, 0644, wq_numa_show, wq_numa_store),
+ 	__ATTR_NULL,
+ };
+ 
+ static struct bus_type wq_subsys = {
+ 	.name				= "workqueue",
+ 	.dev_groups			= wq_sysfs_groups,
+ };
+ 
+ static ssize_t wq_unbound_cpumask_show(struct device *dev,
+ 		struct device_attribute *attr, char *buf)
+ {
+ 	int written;
+ 
+ 	mutex_lock(&wq_pool_mutex);
+ 	written = scnprintf(buf, PAGE_SIZE, "%*pb\n",
+ 			    cpumask_pr_args(wq_unbound_cpumask));
+ 	mutex_unlock(&wq_pool_mutex);
+ 
+ 	return written;
+ }
+ 
+ static ssize_t wq_unbound_cpumask_store(struct device *dev,
+ 		struct device_attribute *attr, const char *buf, size_t count)
+ {
+ 	cpumask_var_t cpumask;
+ 	int ret;
+ 
+ 	if (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))
+ 		return -ENOMEM;
+ 
+ 	ret = cpumask_parse(buf, cpumask);
+ 	if (!ret)
+ 		ret = workqueue_set_unbound_cpumask(cpumask);
+ 
+ 	free_cpumask_var(cpumask);
+ 	return ret ? ret : count;
+ }
+ 
+ static struct device_attribute wq_sysfs_cpumask_attr =
+ 	__ATTR(cpumask, 0644, wq_unbound_cpumask_show,
+ 	       wq_unbound_cpumask_store);
+ 
+ static int __init wq_sysfs_init(void)
+ {
+ 	int err;
+ 
+ 	err = subsys_virtual_register(&wq_subsys, NULL);
+ 	if (err)
+ 		return err;
+ 
+ 	return device_create_file(wq_subsys.dev_root, &wq_sysfs_cpumask_attr);
+ }
+ core_initcall(wq_sysfs_init);
+ 
+ static void wq_device_release(struct device *dev)
+ {
+ 	struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);
+ 
+ 	kfree(wq_dev);
+ }
+ 
+ /**
+  * workqueue_sysfs_register - make a workqueue visible in sysfs
+  * @wq: the workqueue to register
+  *
+  * Expose @wq in sysfs under /sys/bus/workqueue/devices.
+  * alloc_workqueue*() automatically calls this function if WQ_SYSFS is set
+  * which is the preferred method.
+  *
+  * Workqueue user should use this function directly iff it wants to apply
+  * workqueue_attrs before making the workqueue visible in sysfs; otherwise,
+  * apply_workqueue_attrs() may race against userland updating the
+  * attributes.
+  *
+  * Return: 0 on success, -errno on failure.
+  */
+ int workqueue_sysfs_register(struct workqueue_struct *wq)
+ {
+ 	struct wq_device *wq_dev;
+ 	int ret;
+ 
+ 	/*
+ 	 * Adjusting max_active or creating new pwqs by applyting
+ 	 * attributes breaks ordering guarantee.  Disallow exposing ordered
+ 	 * workqueues.
+ 	 */
+ 	if (WARN_ON(wq->flags & __WQ_ORDERED))
+ 		return -EINVAL;
+ 
+ 	wq->wq_dev = wq_dev = kzalloc(sizeof(*wq_dev), GFP_KERNEL);
+ 	if (!wq_dev)
+ 		return -ENOMEM;
+ 
+ 	wq_dev->wq = wq;
+ 	wq_dev->dev.bus = &wq_subsys;
+ 	wq_dev->dev.init_name = wq->name;
+ 	wq_dev->dev.release = wq_device_release;
+ 
+ 	/*
+ 	 * unbound_attrs are created separately.  Suppress uevent until
+ 	 * everything is ready.
+ 	 */
+ 	dev_set_uevent_suppress(&wq_dev->dev, true);
+ 
+ 	ret = device_register(&wq_dev->dev);
+ 	if (ret) {
+ 		kfree(wq_dev);
+ 		wq->wq_dev = NULL;
+ 		return ret;
+ 	}
+ 
+ 	if (wq->flags & WQ_UNBOUND) {
+ 		struct device_attribute *attr;
+ 
+ 		for (attr = wq_sysfs_unbound_attrs; attr->attr.name; attr++) {
+ 			ret = device_create_file(&wq_dev->dev, attr);
+ 			if (ret) {
+ 				device_unregister(&wq_dev->dev);
+ 				wq->wq_dev = NULL;
+ 				return ret;
+ 			}
+ 		}
+ 	}
+ 
+ 	dev_set_uevent_suppress(&wq_dev->dev, false);
+ 	kobject_uevent(&wq_dev->dev.kobj, KOBJ_ADD);
+ 	return 0;
+ }
+ 
+ /**
+  * workqueue_sysfs_unregister - undo workqueue_sysfs_register()
+  * @wq: the workqueue to unregister
+  *
+  * If @wq is registered to sysfs by workqueue_sysfs_register(), unregister.
+  */
+ static void workqueue_sysfs_unregister(struct workqueue_struct *wq)
+ {
+ 	struct wq_device *wq_dev = wq->wq_dev;
+ 
+ 	if (!wq->wq_dev)
+ 		return;
+ 
+ 	wq->wq_dev = NULL;
+ 	device_unregister(&wq_dev->dev);
+ }
+ #else	/* CONFIG_SYSFS */
+ static void workqueue_sysfs_unregister(struct workqueue_struct *wq)	{ }
+ #endif	/* CONFIG_SYSFS */
+ 
++>>>>>>> 042f7df15a4f (workqueue: Allow modifying low level unbound workqueue cpumask)
  static void __init wq_numa_init(void)
  {
  	cpumask_var_t *tbl;
diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index cce29aea497e..0397eb08fb69 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -422,6 +422,7 @@ struct workqueue_attrs *alloc_workqueue_attrs(gfp_t gfp_mask);
 void free_workqueue_attrs(struct workqueue_attrs *attrs);
 int apply_workqueue_attrs(struct workqueue_struct *wq,
 			  const struct workqueue_attrs *attrs);
+int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);
 
 extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
 			struct work_struct *work);
* Unmerged path kernel/workqueue.c
