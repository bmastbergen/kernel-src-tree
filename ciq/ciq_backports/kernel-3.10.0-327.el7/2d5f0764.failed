workqueue: split apply_workqueue_attrs() into 3 stages

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Lai Jiangshan <laijs@cn.fujitsu.com>
commit 2d5f0764b5264d2954ba6e3deb04f4f5de8e4476
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/2d5f0764.failed

Current apply_workqueue_attrs() includes pwqs-allocation and pwqs-installation,
so when we batch multiple apply_workqueue_attrs()s as a transaction, we can't
ensure the transaction must succeed or fail as a complete unit.

To solve this, we split apply_workqueue_attrs() into three stages.
The first stage does the preparation: allocation memory, pwqs.
The second stage does the attrs-installaion and pwqs-installation.
The third stage frees the allocated memory and (old or unused) pwqs.

As the result, batching multiple apply_workqueue_attrs()s can
succeed or fail as a complete unit:
	1) batch do all the first stage for all the workqueues
	2) only commit all when all the above succeed.

This patch is a preparation for the next patch ("Allow modifying low level
unbound workqueue cpumask") which will do a multiple apply_workqueue_attrs().

The patch doesn't have functionality changed except two minor adjustment:
	1) free_unbound_pwq() for the error path is removed, we use the
	   heavier version put_pwq_unlocked() instead since the error path
	   is rare. this adjustment simplifies the code.
	2) the memory-allocation is also moved into wq_pool_mutex.
	   this is needed to avoid to do the further splitting.

tj: minor updates to comments.

	Suggested-by: Tejun Heo <tj@kernel.org>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Kevin Hilman <khilman@linaro.org>
	Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
	Cc: Mike Galbraith <bitbucket@online.de>
	Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 2d5f0764b5264d2954ba6e3deb04f4f5de8e4476)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/workqueue.c
diff --cc kernel/workqueue.c
index 14a71630e038,26ff24924016..000000000000
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@@ -3667,336 -3650,498 +3667,387 @@@ static void pwq_unbound_release_workfn(
  }
  
  /**
 - * wq_update_unbound_numa - update NUMA affinity of a wq for CPU hot[un]plug
 - * @wq: the target workqueue
 - * @cpu: the CPU coming up or going down
 - * @online: whether @cpu is coming up or going down
 - *
 - * This function is to be called from %CPU_DOWN_PREPARE, %CPU_ONLINE and
 - * %CPU_DOWN_FAILED.  @cpu is being hot[un]plugged, update NUMA affinity of
 - * @wq accordingly.
 - *
 - * If NUMA affinity can't be adjusted due to memory allocation failure, it
 - * falls back to @wq->dfl_pwq which may not be optimal but is always
 - * correct.
 + * pwq_adjust_max_active - update a pwq's max_active to the current setting
 + * @pwq: target pool_workqueue
   *
 - * Note that when the last allowed CPU of a NUMA node goes offline for a
 - * workqueue with a cpumask spanning multiple nodes, the workers which were
 - * already executing the work items for the workqueue will lose their CPU
 - * affinity and may execute on any CPU.  This is similar to how per-cpu
 - * workqueues behave on CPU_DOWN.  If a workqueue user wants strict
 - * affinity, it's the user's responsibility to flush the work item from
 - * CPU_DOWN_PREPARE.
 + * If @pwq isn't freezing, set @pwq->max_active to the associated
 + * workqueue's saved_max_active and activate delayed work items
 + * accordingly.  If @pwq is freezing, clear @pwq->max_active to zero.
   */
 -static void wq_update_unbound_numa(struct workqueue_struct *wq, int cpu,
 -				   bool online)
 +static void pwq_adjust_max_active(struct pool_workqueue *pwq)
  {
 -	int node = cpu_to_node(cpu);
 -	int cpu_off = online ? -1 : cpu;
 -	struct pool_workqueue *old_pwq = NULL, *pwq;
 -	struct workqueue_attrs *target_attrs;
 -	cpumask_t *cpumask;
 +	struct workqueue_struct *wq = pwq->wq;
 +	bool freezable = wq->flags & WQ_FREEZABLE;
  
 -	lockdep_assert_held(&wq_pool_mutex);
 +	/* for @wq->saved_max_active */
 +	lockdep_assert_held(&wq->mutex);
  
 -	if (!wq_numa_enabled || !(wq->flags & WQ_UNBOUND))
 +	/* fast exit for non-freezable wqs */
 +	if (!freezable && pwq->max_active == wq->saved_max_active)
  		return;
  
 -	/*
 -	 * We don't wanna alloc/free wq_attrs for each wq for each CPU.
 -	 * Let's use a preallocated one.  The following buf is protected by
 -	 * CPU hotplug exclusion.
 -	 */
 -	target_attrs = wq_update_unbound_numa_attrs_buf;
 -	cpumask = target_attrs->cpumask;
 +	spin_lock_irq(&pwq->pool->lock);
  
 -	mutex_lock(&wq->mutex);
 -	if (wq->unbound_attrs->no_numa)
 -		goto out_unlock;
 +	if (!freezable || !(pwq->pool->flags & POOL_FREEZING)) {
 +		pwq->max_active = wq->saved_max_active;
  
 -	copy_workqueue_attrs(target_attrs, wq->unbound_attrs);
 -	pwq = unbound_pwq_by_node(wq, node);
 +		while (!list_empty(&pwq->delayed_works) &&
 +		       pwq->nr_active < pwq->max_active)
 +			pwq_activate_first_delayed(pwq);
  
 -	/*
 -	 * Let's determine what needs to be done.  If the target cpumask is
 -	 * different from wq's, we need to compare it to @pwq's and create
 -	 * a new one if they don't match.  If the target cpumask equals
 -	 * wq's, the default pwq should be used.
 -	 */
 -	if (wq_calc_node_cpumask(wq->unbound_attrs, node, cpu_off, cpumask)) {
 -		if (cpumask_equal(cpumask, pwq->pool->attrs->cpumask))
 -			goto out_unlock;
 +		/*
 +		 * Need to kick a worker after thawed or an unbound wq's
 +		 * max_active is bumped.  It's a slow path.  Do it always.
 +		 */
 +		wake_up_worker(pwq->pool);
  	} else {
 -		goto use_dfl_pwq;
 +		pwq->max_active = 0;
  	}
  
 -	mutex_unlock(&wq->mutex);
 +	spin_unlock_irq(&pwq->pool->lock);
 +}
  
 -	/* create a new pwq */
 -	pwq = alloc_unbound_pwq(wq, target_attrs);
 -	if (!pwq) {
 -		pr_warn("workqueue: allocation failed while updating NUMA affinity of \"%s\"\n",
 -			wq->name);
 -		mutex_lock(&wq->mutex);
 -		goto use_dfl_pwq;
 -	}
 +/* initialize newly alloced @pwq which is associated with @wq and @pool */
 +static void init_pwq(struct pool_workqueue *pwq, struct workqueue_struct *wq,
 +		     struct worker_pool *pool)
 +{
 +	BUG_ON((unsigned long)pwq & WORK_STRUCT_FLAG_MASK);
  
 -	/*
 -	 * Install the new pwq.  As this function is called only from CPU
 -	 * hotplug callbacks and applying a new attrs is wrapped with
 -	 * get/put_online_cpus(), @wq->unbound_attrs couldn't have changed
 -	 * inbetween.
 -	 */
 -	mutex_lock(&wq->mutex);
 -	old_pwq = numa_pwq_tbl_install(wq, node, pwq);
 -	goto out_unlock;
 +	memset(pwq, 0, sizeof(*pwq));
  
 -use_dfl_pwq:
 -	spin_lock_irq(&wq->dfl_pwq->pool->lock);
 -	get_pwq(wq->dfl_pwq);
 -	spin_unlock_irq(&wq->dfl_pwq->pool->lock);
 -	old_pwq = numa_pwq_tbl_install(wq, node, wq->dfl_pwq);
 -out_unlock:
 -	mutex_unlock(&wq->mutex);
 -	put_pwq_unlocked(old_pwq);
 +	pwq->pool = pool;
 +	pwq->wq = wq;
 +	pwq->flush_color = -1;
 +	pwq->refcnt = 1;
 +	INIT_LIST_HEAD(&pwq->delayed_works);
 +	INIT_LIST_HEAD(&pwq->pwqs_node);
 +	INIT_LIST_HEAD(&pwq->mayday_node);
 +	INIT_WORK(&pwq->unbound_release_work, pwq_unbound_release_workfn);
  }
  
 -static int alloc_and_link_pwqs(struct workqueue_struct *wq)
 +/* sync @pwq with the current state of its associated wq and link it */
 +static void link_pwq(struct pool_workqueue *pwq)
  {
 -	bool highpri = wq->flags & WQ_HIGHPRI;
 -	int cpu, ret;
 +	struct workqueue_struct *wq = pwq->wq;
  
 -	if (!(wq->flags & WQ_UNBOUND)) {
 -		wq->cpu_pwqs = alloc_percpu(struct pool_workqueue);
 -		if (!wq->cpu_pwqs)
 -			return -ENOMEM;
 -
 -		for_each_possible_cpu(cpu) {
 -			struct pool_workqueue *pwq =
 -				per_cpu_ptr(wq->cpu_pwqs, cpu);
 -			struct worker_pool *cpu_pools =
 -				per_cpu(cpu_worker_pools, cpu);
 -
 -			init_pwq(pwq, wq, &cpu_pools[highpri]);
 +	lockdep_assert_held(&wq->mutex);
  
 -			mutex_lock(&wq->mutex);
 -			link_pwq(pwq);
 -			mutex_unlock(&wq->mutex);
 -		}
 -		return 0;
 -	} else if (wq->flags & __WQ_ORDERED) {
 -		ret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);
 -		/* there should only be single pwq for ordering guarantee */
 -		WARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||
 -			      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),
 -		     "ordering guarantee broken for workqueue %s\n", wq->name);
 -		return ret;
 -	} else {
 -		return apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);
 -	}
 -}
 +	/* may be called multiple times, ignore if already linked */
 +	if (!list_empty(&pwq->pwqs_node))
 +		return;
  
 -static int wq_clamp_max_active(int max_active, unsigned int flags,
 -			       const char *name)
 -{
 -	int lim = flags & WQ_UNBOUND ? WQ_UNBOUND_MAX_ACTIVE : WQ_MAX_ACTIVE;
 +	/*
 +	 * Set the matching work_color.  This is synchronized with
 +	 * wq->mutex to avoid confusing flush_workqueue().
 +	 */
 +	pwq->work_color = wq->work_color;
  
 -	if (max_active < 1 || max_active > lim)
 -		pr_warn("workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\n",
 -			max_active, name, 1, lim);
 +	/* sync max_active to the current setting */
 +	pwq_adjust_max_active(pwq);
  
 -	return clamp_val(max_active, 1, lim);
 +	/* link in @pwq */
 +	list_add_rcu(&pwq->pwqs_node, &wq->pwqs);
  }
  
 -struct workqueue_struct *__alloc_workqueue_key(const char *fmt,
 -					       unsigned int flags,
 -					       int max_active,
 -					       struct lock_class_key *key,
 -					       const char *lock_name, ...)
 +/* obtain a pool matching @attr and create a pwq associating the pool and @wq */
 +static struct pool_workqueue *alloc_unbound_pwq(struct workqueue_struct *wq,
 +					const struct workqueue_attrs *attrs)
  {
 -	size_t tbl_size = 0;
 -	va_list args;
 -	struct workqueue_struct *wq;
 +	struct worker_pool *pool;
  	struct pool_workqueue *pwq;
  
 -	/* see the comment above the definition of WQ_POWER_EFFICIENT */
 -	if ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)
 -		flags |= WQ_UNBOUND;
 -
 -	/* allocate wq and format name */
 -	if (flags & WQ_UNBOUND)
 -		tbl_size = nr_node_ids * sizeof(wq->numa_pwq_tbl[0]);
 +	lockdep_assert_held(&wq_pool_mutex);
  
 -	wq = kzalloc(sizeof(*wq) + tbl_size, GFP_KERNEL);
 -	if (!wq)
 +	pool = get_unbound_pool(attrs);
 +	if (!pool)
  		return NULL;
  
 -	if (flags & WQ_UNBOUND) {
 -		wq->unbound_attrs = alloc_workqueue_attrs(GFP_KERNEL);
 -		if (!wq->unbound_attrs)
 -			goto err_free_wq;
 +	pwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);
 +	if (!pwq) {
 +		put_unbound_pool(pool);
 +		return NULL;
  	}
  
 -	va_start(args, lock_name);
 -	vsnprintf(wq->name, sizeof(wq->name), fmt, args);
 -	va_end(args);
 -
 -	max_active = max_active ?: WQ_DFL_ACTIVE;
 -	max_active = wq_clamp_max_active(max_active, flags, wq->name);
 +	init_pwq(pwq, wq, pool);
 +	return pwq;
 +}
  
- /* undo alloc_unbound_pwq(), used only in the error path */
- static void free_unbound_pwq(struct pool_workqueue *pwq)
- {
- 	lockdep_assert_held(&wq_pool_mutex);
- 
- 	if (pwq) {
- 		put_unbound_pool(pwq->pool);
- 		kmem_cache_free(pwq_cache, pwq);
- 	}
- }
- 
 -	/* init wq */
 -	wq->flags = flags;
 -	wq->saved_max_active = max_active;
 -	mutex_init(&wq->mutex);
 -	atomic_set(&wq->nr_pwqs_to_flush, 0);
 -	INIT_LIST_HEAD(&wq->pwqs);
 -	INIT_LIST_HEAD(&wq->flusher_queue);
 -	INIT_LIST_HEAD(&wq->flusher_overflow);
 -	INIT_LIST_HEAD(&wq->maydays);
 +/**
 + * wq_calc_node_mask - calculate a wq_attrs' cpumask for the specified node
 + * @attrs: the wq_attrs of interest
 + * @node: the target NUMA node
 + * @cpu_going_down: if >= 0, the CPU to consider as offline
 + * @cpumask: outarg, the resulting cpumask
 + *
 + * Calculate the cpumask a workqueue with @attrs should use on @node.  If
 + * @cpu_going_down is >= 0, that cpu is considered offline during
 + * calculation.  The result is stored in @cpumask.  This function returns
 + * %true if the resulting @cpumask is different from @attrs->cpumask,
 + * %false if equal.
 + *
 + * If NUMA affinity is not enabled, @attrs->cpumask is always used.  If
 + * enabled and @node has online CPUs requested by @attrs, the returned
 + * cpumask is the intersection of the possible CPUs of @node and
 + * @attrs->cpumask.
 + *
 + * The caller is responsible for ensuring that the cpumask of @node stays
 + * stable.
 + */
 +static bool wq_calc_node_cpumask(const struct workqueue_attrs *attrs, int node,
 +				 int cpu_going_down, cpumask_t *cpumask)
 +{
 +	if (!wq_numa_enabled || attrs->no_numa)
 +		goto use_dfl;
  
 -	lockdep_init_map(&wq->lockdep_map, lock_name, key, 0);
 -	INIT_LIST_HEAD(&wq->list);
 +	/* does @node have any online CPUs @attrs wants? */
 +	cpumask_and(cpumask, cpumask_of_node(node), attrs->cpumask);
 +	if (cpu_going_down >= 0)
 +		cpumask_clear_cpu(cpu_going_down, cpumask);
  
 -	if (alloc_and_link_pwqs(wq) < 0)
 -		goto err_free_wq;
 +	if (cpumask_empty(cpumask))
 +		goto use_dfl;
  
 -	/*
 -	 * Workqueues which may be used during memory reclaim should
 -	 * have a rescuer to guarantee forward progress.
 -	 */
 -	if (flags & WQ_MEM_RECLAIM) {
 -		struct worker *rescuer;
 +	/* yeap, return possible CPUs in @node that @attrs wants */
 +	cpumask_and(cpumask, attrs->cpumask, wq_numa_possible_cpumask[node]);
 +	return !cpumask_equal(cpumask, attrs->cpumask);
  
 -		rescuer = alloc_worker(NUMA_NO_NODE);
 -		if (!rescuer)
 -			goto err_destroy;
 +use_dfl:
 +	cpumask_copy(cpumask, attrs->cpumask);
 +	return false;
 +}
  
 -		rescuer->rescue_wq = wq;
 -		rescuer->task = kthread_create(rescuer_thread, rescuer, "%s",
 -					       wq->name);
 -		if (IS_ERR(rescuer->task)) {
 -			kfree(rescuer);
 -			goto err_destroy;
 -		}
 +/* install @pwq into @wq's numa_pwq_tbl[] for @node and return the old pwq */
 +static struct pool_workqueue *numa_pwq_tbl_install(struct workqueue_struct *wq,
 +						   int node,
 +						   struct pool_workqueue *pwq)
 +{
 +	struct pool_workqueue *old_pwq;
  
 -		wq->rescuer = rescuer;
 -		rescuer->task->flags |= PF_NO_SETAFFINITY;
 -		wake_up_process(rescuer->task);
 -	}
 +	lockdep_assert_held(&wq->mutex);
  
 -	if ((wq->flags & WQ_SYSFS) && workqueue_sysfs_register(wq))
 -		goto err_destroy;
 +	/* link_pwq() can handle duplicate calls */
 +	link_pwq(pwq);
  
 -	/*
 -	 * wq_pool_mutex protects global freeze state and workqueues list.
 -	 * Grab it, adjust max_active and add the new @wq to workqueues
 -	 * list.
 -	 */
 -	mutex_lock(&wq_pool_mutex);
 +	old_pwq = rcu_access_pointer(wq->numa_pwq_tbl[node]);
 +	rcu_assign_pointer(wq->numa_pwq_tbl[node], pwq);
 +	return old_pwq;
 +}
  
 -	mutex_lock(&wq->mutex);
 -	for_each_pwq(pwq, wq)
 -		pwq_adjust_max_active(pwq);
 -	mutex_unlock(&wq->mutex);
++/* context to store the prepared attrs & pwqs before applying */
++struct apply_wqattrs_ctx {
++	struct workqueue_struct	*wq;		/* target workqueue */
++	struct workqueue_attrs	*attrs;		/* attrs to apply */
++	struct pool_workqueue	*dfl_pwq;
++	struct pool_workqueue	*pwq_tbl[];
++};
+ 
 -	list_add_tail_rcu(&wq->list, &workqueues);
++/* free the resources after success or abort */
++static void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)
++{
++	if (ctx) {
++		int node;
+ 
 -	mutex_unlock(&wq_pool_mutex);
++		for_each_node(node)
++			put_pwq_unlocked(ctx->pwq_tbl[node]);
++		put_pwq_unlocked(ctx->dfl_pwq);
+ 
 -	return wq;
++		free_workqueue_attrs(ctx->attrs);
+ 
 -err_free_wq:
 -	free_workqueue_attrs(wq->unbound_attrs);
 -	kfree(wq);
 -	return NULL;
 -err_destroy:
 -	destroy_workqueue(wq);
 -	return NULL;
++		kfree(ctx);
++	}
+ }
 -EXPORT_SYMBOL_GPL(__alloc_workqueue_key);
+ 
 -/**
 - * destroy_workqueue - safely terminate a workqueue
 - * @wq: target workqueue
 - *
 - * Safely destroy a workqueue. All work currently pending will be done first.
 - */
 -void destroy_workqueue(struct workqueue_struct *wq)
++/* allocate the attrs and pwqs for later installation */
++static struct apply_wqattrs_ctx *
++apply_wqattrs_prepare(struct workqueue_struct *wq,
++		      const struct workqueue_attrs *attrs)
+ {
 -	struct pool_workqueue *pwq;
++	struct apply_wqattrs_ctx *ctx;
++	struct workqueue_attrs *new_attrs, *tmp_attrs;
+ 	int node;
+ 
 -	/* drain it before proceeding with destruction */
 -	drain_workqueue(wq);
++	lockdep_assert_held(&wq_pool_mutex);
+ 
 -	/* sanity checks */
 -	mutex_lock(&wq->mutex);
 -	for_each_pwq(pwq, wq) {
 -		int i;
++	ctx = kzalloc(sizeof(*ctx) + nr_node_ids * sizeof(ctx->pwq_tbl[0]),
++		      GFP_KERNEL);
+ 
 -		for (i = 0; i < WORK_NR_COLORS; i++) {
 -			if (WARN_ON(pwq->nr_in_flight[i])) {
 -				mutex_unlock(&wq->mutex);
 -				return;
 -			}
 -		}
++	new_attrs = alloc_workqueue_attrs(GFP_KERNEL);
++	tmp_attrs = alloc_workqueue_attrs(GFP_KERNEL);
++	if (!ctx || !new_attrs || !tmp_attrs)
++		goto out_free;
+ 
 -		if (WARN_ON((pwq != wq->dfl_pwq) && (pwq->refcnt > 1)) ||
 -		    WARN_ON(pwq->nr_active) ||
 -		    WARN_ON(!list_empty(&pwq->delayed_works))) {
 -			mutex_unlock(&wq->mutex);
 -			return;
 -		}
 -	}
 -	mutex_unlock(&wq->mutex);
++	/* make a copy of @attrs and sanitize it */
++	copy_workqueue_attrs(new_attrs, attrs);
++	cpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);
+ 
+ 	/*
 -	 * wq list is used to freeze wq, remove from list after
 -	 * flushing is complete in case freeze races us.
++	 * We may create multiple pwqs with differing cpumasks.  Make a
++	 * copy of @new_attrs which will be modified and used to obtain
++	 * pools.
+ 	 */
 -	mutex_lock(&wq_pool_mutex);
 -	list_del_rcu(&wq->list);
 -	mutex_unlock(&wq_pool_mutex);
++	copy_workqueue_attrs(tmp_attrs, new_attrs);
+ 
 -	workqueue_sysfs_unregister(wq);
 -
 -	if (wq->rescuer)
 -		kthread_stop(wq->rescuer->task);
++	/*
++	 * If something goes wrong during CPU up/down, we'll fall back to
++	 * the default pwq covering whole @attrs->cpumask.  Always create
++	 * it even if we don't use it immediately.
++	 */
++	ctx->dfl_pwq = alloc_unbound_pwq(wq, new_attrs);
++	if (!ctx->dfl_pwq)
++		goto out_free;
+ 
 -	if (!(wq->flags & WQ_UNBOUND)) {
 -		/*
 -		 * The base ref is never dropped on per-cpu pwqs.  Directly
 -		 * schedule RCU free.
 -		 */
 -		call_rcu_sched(&wq->rcu, rcu_free_wq);
 -	} else {
 -		/*
 -		 * We're the sole accessor of @wq at this point.  Directly
 -		 * access numa_pwq_tbl[] and dfl_pwq to put the base refs.
 -		 * @wq will be freed when the last pwq is released.
 -		 */
 -		for_each_node(node) {
 -			pwq = rcu_access_pointer(wq->numa_pwq_tbl[node]);
 -			RCU_INIT_POINTER(wq->numa_pwq_tbl[node], NULL);
 -			put_pwq_unlocked(pwq);
++	for_each_node(node) {
++		if (wq_calc_node_cpumask(attrs, node, -1, tmp_attrs->cpumask)) {
++			ctx->pwq_tbl[node] = alloc_unbound_pwq(wq, tmp_attrs);
++			if (!ctx->pwq_tbl[node])
++				goto out_free;
++		} else {
++			ctx->dfl_pwq->refcnt++;
++			ctx->pwq_tbl[node] = ctx->dfl_pwq;
+ 		}
 -
 -		/*
 -		 * Put dfl_pwq.  @wq may be freed any time after dfl_pwq is
 -		 * put.  Don't access it afterwards.
 -		 */
 -		pwq = wq->dfl_pwq;
 -		wq->dfl_pwq = NULL;
 -		put_pwq_unlocked(pwq);
+ 	}
++
++	ctx->attrs = new_attrs;
++	ctx->wq = wq;
++	free_workqueue_attrs(tmp_attrs);
++	return ctx;
++
++out_free:
++	free_workqueue_attrs(tmp_attrs);
++	free_workqueue_attrs(new_attrs);
++	apply_wqattrs_cleanup(ctx);
++	return NULL;
+ }
 -EXPORT_SYMBOL_GPL(destroy_workqueue);
+ 
 -/**
 - * workqueue_set_max_active - adjust max_active of a workqueue
 - * @wq: target workqueue
 - * @max_active: new max_active value.
 - *
 - * Set max_active of @wq to @max_active.
 - *
 - * CONTEXT:
 - * Don't call from IRQ context.
 - */
 -void workqueue_set_max_active(struct workqueue_struct *wq, int max_active)
++/* set attrs and install prepared pwqs, @ctx points to old pwqs on return */
++static void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)
+ {
 -	struct pool_workqueue *pwq;
 -
 -	/* disallow meddling with max_active for ordered workqueues */
 -	if (WARN_ON(wq->flags & __WQ_ORDERED))
 -		return;
++	int node;
+ 
 -	max_active = wq_clamp_max_active(max_active, wq->flags, wq->name);
++	/* all pwqs have been created successfully, let's install'em */
++	mutex_lock(&ctx->wq->mutex);
+ 
 -	mutex_lock(&wq->mutex);
++	copy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);
+ 
 -	wq->saved_max_active = max_active;
++	/* save the previous pwq and install the new one */
++	for_each_node(node)
++		ctx->pwq_tbl[node] = numa_pwq_tbl_install(ctx->wq, node,
++							  ctx->pwq_tbl[node]);
+ 
 -	for_each_pwq(pwq, wq)
 -		pwq_adjust_max_active(pwq);
++	/* @dfl_pwq might not have been used, ensure it's linked */
++	link_pwq(ctx->dfl_pwq);
++	swap(ctx->wq->dfl_pwq, ctx->dfl_pwq);
+ 
 -	mutex_unlock(&wq->mutex);
++	mutex_unlock(&ctx->wq->mutex);
+ }
 -EXPORT_SYMBOL_GPL(workqueue_set_max_active);
+ 
  /**
 - * current_is_workqueue_rescuer - is %current workqueue rescuer?
 + * apply_workqueue_attrs - apply new workqueue_attrs to an unbound workqueue
 + * @wq: the target workqueue
 + * @attrs: the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()
   *
 - * Determine whether %current is a workqueue rescuer.  Can be used from
 - * work functions to determine whether it's being run off the rescuer task.
 + * Apply @attrs to an unbound workqueue @wq.  Unless disabled, on NUMA
 + * machines, this function maps a separate pwq to each NUMA node with
 + * possibles CPUs in @attrs->cpumask so that work items are affine to the
 + * NUMA node it was issued on.  Older pwqs are released as in-flight work
 + * items finish.  Note that a work item which repeatedly requeues itself
 + * back-to-back will stay on its current pwq.
   *
 - * Return: %true if %current is a workqueue rescuer. %false otherwise.
 + * Performs GFP_KERNEL allocations.  Returns 0 on success and -errno on
 + * failure.
   */
 -bool current_is_workqueue_rescuer(void)
 +int apply_workqueue_attrs(struct workqueue_struct *wq,
 +			  const struct workqueue_attrs *attrs)
  {
- 	struct workqueue_attrs *new_attrs, *tmp_attrs;
- 	struct pool_workqueue **pwq_tbl, *dfl_pwq;
- 	int node, ret;
 -	struct worker *worker = current_wq_worker();
++	struct apply_wqattrs_ctx *ctx;
++	int ret = -ENOMEM;
  
 -	return worker && worker->rescue_wq;
 -}
 +	/* only unbound workqueues can change attributes */
 +	if (WARN_ON(!(wq->flags & WQ_UNBOUND)))
 +		return -EINVAL;
  
 -/**
 - * workqueue_congested - test whether a workqueue is congested
 - * @cpu: CPU in question
 - * @wq: target workqueue
 - *
 - * Test whether @wq's cpu workqueue for @cpu is congested.  There is
 - * no synchronization around this function and the test result is
 - * unreliable and only useful as advisory hints or for debugging.
 - *
 - * If @cpu is WORK_CPU_UNBOUND, the test is performed on the local CPU.
 - * Note that both per-cpu and unbound workqueues may be associated with
 - * multiple pool_workqueues which have separate congested states.  A
 - * workqueue being congested on one CPU doesn't mean the workqueue is also
 - * contested on other CPUs / NUMA nodes.
 - *
 - * Return:
 - * %true if congested, %false otherwise.
 - */
 -bool workqueue_congested(int cpu, struct workqueue_struct *wq)
 -{
 -	struct pool_workqueue *pwq;
 -	bool ret;
 +	/* creating multiple pwqs breaks ordering guarantee */
 +	if (WARN_ON((wq->flags & __WQ_ORDERED) && !list_empty(&wq->pwqs)))
 +		return -EINVAL;
  
 -	rcu_read_lock_sched();
++<<<<<<< HEAD
 +	pwq_tbl = kzalloc(wq_numa_tbl_len * sizeof(pwq_tbl[0]), GFP_KERNEL);
 +	new_attrs = alloc_workqueue_attrs(GFP_KERNEL);
 +	tmp_attrs = alloc_workqueue_attrs(GFP_KERNEL);
 +	if (!pwq_tbl || !new_attrs || !tmp_attrs)
 +		goto enomem;
  
 -	if (cpu == WORK_CPU_UNBOUND)
 -		cpu = smp_processor_id();
 +	/* make a copy of @attrs and sanitize it */
 +	copy_workqueue_attrs(new_attrs, attrs);
 +	cpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);
  
 -	if (!(wq->flags & WQ_UNBOUND))
 -		pwq = per_cpu_ptr(wq->cpu_pwqs, cpu);
 -	else
 -		pwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));
 +	/*
 +	 * We may create multiple pwqs with differing cpumasks.  Make a
 +	 * copy of @new_attrs which will be modified and used to obtain
 +	 * pools.
 +	 */
 +	copy_workqueue_attrs(tmp_attrs, new_attrs);
  
 -	ret = !list_empty(&pwq->delayed_works);
 -	rcu_read_unlock_sched();
++=======
++>>>>>>> 2d5f0764b526 (workqueue: split apply_workqueue_attrs() into 3 stages)
 +	/*
 +	 * CPUs should stay stable across pwq creations and installations.
 +	 * Pin CPUs, determine the target cpumask for each node and create
 +	 * pwqs accordingly.
 +	 */
 +	get_online_cpus();
  
 -	return ret;
 -}
 -EXPORT_SYMBOL_GPL(workqueue_congested);
 +	mutex_lock(&wq_pool_mutex);
- 
- 	/*
- 	 * If something goes wrong during CPU up/down, we'll fall back to
- 	 * the default pwq covering whole @attrs->cpumask.  Always create
- 	 * it even if we don't use it immediately.
- 	 */
- 	dfl_pwq = alloc_unbound_pwq(wq, new_attrs);
- 	if (!dfl_pwq)
- 		goto enomem_pwq;
- 
- 	for_each_node(node) {
- 		if (wq_calc_node_cpumask(attrs, node, -1, tmp_attrs->cpumask)) {
- 			pwq_tbl[node] = alloc_unbound_pwq(wq, tmp_attrs);
- 			if (!pwq_tbl[node])
- 				goto enomem_pwq;
- 		} else {
- 			dfl_pwq->refcnt++;
- 			pwq_tbl[node] = dfl_pwq;
- 		}
- 	}
- 
++	ctx = apply_wqattrs_prepare(wq, attrs);
 +	mutex_unlock(&wq_pool_mutex);
  
- 	/* all pwqs have been created successfully, let's install'em */
- 	mutex_lock(&wq->mutex);
- 
- 	copy_workqueue_attrs(wq->unbound_attrs, new_attrs);
- 
- 	/* save the previous pwq and install the new one */
- 	for_each_node(node)
- 		pwq_tbl[node] = numa_pwq_tbl_install(wq, node, pwq_tbl[node]);
- 
- 	/* @dfl_pwq might not have been used, ensure it's linked */
- 	link_pwq(dfl_pwq);
- 	swap(wq->dfl_pwq, dfl_pwq);
 -/**
 - * work_busy - test whether a work is currently pending or running
 - * @work: the work to be tested
 - *
 - * Test whether @work is currently pending or running.  There is no
 - * synchronization around this function and the test result is
 - * unreliable and only useful as advisory hints or for debugging.
 - *
 - * Return:
 - * OR'd bitmask of WORK_BUSY_* bits.
 - */
 -unsigned int work_busy(struct work_struct *work)
 -{
 -	struct worker_pool *pool;
 -	unsigned long flags;
 -	unsigned int ret = 0;
++	/* the ctx has been prepared successfully, let's commit it */
++	if (ctx) {
++		apply_wqattrs_commit(ctx);
++		ret = 0;
++	}
  
- 	mutex_unlock(&wq->mutex);
 -	if (work_pending(work))
 -		ret |= WORK_BUSY_PENDING;
++	put_online_cpus();
  
- 	/* put the old pwqs */
- 	for_each_node(node)
- 		put_pwq_unlocked(pwq_tbl[node]);
- 	put_pwq_unlocked(dfl_pwq);
 -	local_irq_save(flags);
 -	pool = get_work_pool(work);
 -	if (pool) {
 -		spin_lock(&pool->lock);
 -		if (find_worker_executing_work(pool, work))
 -			ret |= WORK_BUSY_RUNNING;
 -		spin_unlock(&pool->lock);
 -	}
 -	local_irq_restore(flags);
++	apply_wqattrs_cleanup(ctx);
  
- 	put_online_cpus();
- 	ret = 0;
- 	/* fall through */
- out_free:
- 	free_workqueue_attrs(tmp_attrs);
- 	free_workqueue_attrs(new_attrs);
- 	kfree(pwq_tbl);
  	return ret;
- 
- enomem_pwq:
- 	free_unbound_pwq(dfl_pwq);
- 	for_each_node(node)
- 		if (pwq_tbl && pwq_tbl[node] != dfl_pwq)
- 			free_unbound_pwq(pwq_tbl[node]);
- 	mutex_unlock(&wq_pool_mutex);
- 	put_online_cpus();
- enomem:
- 	ret = -ENOMEM;
- 	goto out_free;
  }
 -EXPORT_SYMBOL_GPL(work_busy);
  
  /**
 - * set_worker_desc - set description for the current work item
 - * @fmt: printf-style format string
 - * @...: arguments for the format string
 + * wq_update_unbound_numa - update NUMA affinity of a wq for CPU hot[un]plug
 + * @wq: the target workqueue
 + * @cpu: the CPU coming up or going down
 + * @online: whether @cpu is coming up or going down
   *
 - * This function can be called by a running work function to describe what
 - * the work item is about.  If the worker task gets dumped, this
 - * information will be printed out together to help debugging.  The
 - * description can be at most WORKER_DESC_LEN including the trailing '\0'.
 - */
 -void set_worker_desc(const char *fmt, ...)
 -{
 -	struct worker *worker = current_wq_worker();
 -	va_list args;
 -
 -	if (worker) {
 -		va_start(args, fmt);
 -		vsnprintf(worker->desc, sizeof(worker->desc), fmt, args);
 -		va_end(args);
 -		worker->desc_valid = true;
 -	}
 -}
 -
 -/**
 - * print_worker_info - print out worker information and description
 - * @log_lvl: the log level to use when printing
 - * @task: target task
 + * This function is to be called from %CPU_DOWN_PREPARE, %CPU_ONLINE and
 + * %CPU_DOWN_FAILED.  @cpu is being hot[un]plugged, update NUMA affinity of
 + * @wq accordingly.
   *
 - * If @task is a worker and currently executing a work item, print out the
 - * name of the workqueue being serviced and worker description set with
 - * set_worker_desc() by the currently executing work item.
 + * If NUMA affinity can't be adjusted due to memory allocation failure, it
 + * falls back to @wq->dfl_pwq which may not be optimal but is always
 + * correct.
   *
 - * This function can be safely called on any task as long as the
 - * task_struct itself is accessible.  While safe, this function isn't
 - * synchronized and may print out mixups or garbages of limited length.
 + * Note that when the last allowed CPU of a NUMA node goes offline for a
 + * workqueue with a cpumask spanning multiple nodes, the workers which were
 + * already executing the work items for the workqueue will lose their CPU
 + * affinity and may execute on any CPU.  This is similar to how per-cpu
 + * workqueues behave on CPU_DOWN.  If a workqueue user wants strict
 + * affinity, it's the user's responsibility to flush the work item from
 + * CPU_DOWN_PREPARE.
   */
 -void print_worker_info(const char *log_lvl, struct task_struct *task)
 +static void wq_update_unbound_numa(struct workqueue_struct *wq, int cpu,
 +				   bool online)
  {
 -	work_func_t *fn = NULL;
 -	char name[WQ_NAME_LEN] = { };
 -	char desc[WORKER_DESC_LEN] = { };
 -	struct pool_workqueue *pwq = NULL;
 -	struct workqueue_struct *wq = NULL;
 -	bool desc_valid = false;
 -	struct worker *worker;
 +	int node = cpu_to_node(cpu);
 +	int cpu_off = online ? -1 : cpu;
 +	struct pool_workqueue *old_pwq = NULL, *pwq;
 +	struct workqueue_attrs *target_attrs;
 +	cpumask_t *cpumask;
  
 -	if (!(task->flags & PF_WQ_WORKER))
 +	lockdep_assert_held(&wq_pool_mutex);
 +
 +	if (!wq_numa_enabled || !(wq->flags & WQ_UNBOUND))
  		return;
  
  	/*
* Unmerged path kernel/workqueue.c
