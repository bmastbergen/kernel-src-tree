cxgb4: Move SGE Ingress DMA state monitor code to a new routine

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Hariprasad Shenai <hariprasad@chelsio.com>
commit a3bfb6179cd1277b259f86b022f3340f3bb49cac
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/a3bfb617.failed

	Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a3bfb6179cd1277b259f86b022f3340f3bb49cac)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
#	drivers/net/ethernet/chelsio/cxgb4/sge.c
#	drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index dddf17cba9bf,1f52d9f66e41..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@@ -581,18 -641,15 +592,13 @@@ struct sge 
  	u32 fl_align;               /* response queue message alignment */
  	u32 fl_starve_thres;        /* Free List starvation threshold */
  
- 	/* State variables for detecting an SGE Ingress DMA hang */
- 	unsigned int idma_1s_thresh;/* SGE same State Counter 1s threshold */
- 	unsigned int idma_stalled[2];/* SGE synthesized stalled timers in HZ */
- 	unsigned int idma_state[2]; /* SGE IDMA Hang detect state */
- 	unsigned int idma_qid[2];   /* SGE IDMA Hung Ingress Queue ID */
- 
+ 	struct sge_idma_monitor_state idma_monitor;
  	unsigned int egr_start;
 -	unsigned int egr_sz;
  	unsigned int ingr_start;
 -	unsigned int ingr_sz;
 -	void **egr_map;    /* qid->queue egress queue map */
 -	struct sge_rspq **ingr_map; /* qid->queue ingress queue map */
 -	unsigned long *starving_fl;
 -	unsigned long *txq_maperr;
 +	void *egr_map[MAX_EGRQ];    /* qid->queue egress queue map */
 +	struct sge_rspq *ingr_map[MAX_INGQ]; /* qid->queue ingress queue map */
 +	DECLARE_BITMAP(starving_fl, MAX_EGRQ);
 +	DECLARE_BITMAP(txq_maperr, MAX_EGRQ);
  	struct timer_list rx_timer; /* refills starving FLs */
  	struct timer_list tx_timer; /* checks Tx queues */
  };
@@@ -1087,4 -1316,10 +1093,13 @@@ void t4_db_dropped(struct adapter *adap
  int t4_fwaddrspace_write(struct adapter *adap, unsigned int mbox,
  			 u32 addr, u32 val);
  void t4_sge_decode_idma_state(struct adapter *adapter, int state);
++<<<<<<< HEAD
++=======
+ void t4_free_mem(void *addr);
+ void t4_idma_monitor_init(struct adapter *adapter,
+ 			  struct sge_idma_monitor_state *idma);
+ void t4_idma_monitor(struct adapter *adapter,
+ 		     struct sge_idma_monitor_state *idma,
+ 		     int hz, int ticks);
++>>>>>>> a3bfb6179cd1 (cxgb4: Move SGE Ingress DMA state monitor code to a new routine)
  #endif /* __CXGB4_H__ */
diff --cc drivers/net/ethernet/chelsio/cxgb4/sge.c
index 4fe675295ab8,ad504d0db1ec..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@@ -2166,67 -2290,16 +2156,78 @@@ static void sge_rx_timer_cb(unsigned lo
  					set_bit(id, s->starving_fl);
  			}
  		}
+ 	/* The remainder of the SGE RX Timer Callback routine is dedicated to
+ 	 * global Master PF activities like checking for chip ingress stalls,
+ 	 * etc.
+ 	 */
+ 	if (!(adap->flags & MASTER_PF))
+ 		goto done;
  
++<<<<<<< HEAD
 +	t4_write_reg(adap, SGE_DEBUG_INDEX, 13);
 +	idma_same_state_cnt[0] = t4_read_reg(adap, SGE_DEBUG_DATA_HIGH);
 +	idma_same_state_cnt[1] = t4_read_reg(adap, SGE_DEBUG_DATA_LOW);
 +
 +	for (i = 0; i < 2; i++) {
 +		u32 debug0, debug11;
 +
 +		/* If the Ingress DMA Same State Counter ("timer") is less
 +		 * than 1s, then we can reset our synthesized Stall Timer and
 +		 * continue.  If we have previously emitted warnings about a
 +		 * potential stalled Ingress Queue, issue a note indicating
 +		 * that the Ingress Queue has resumed forward progress.
 +		 */
 +		if (idma_same_state_cnt[i] < s->idma_1s_thresh) {
 +			if (s->idma_stalled[i] >= SGE_IDMA_WARN_THRESH)
 +				CH_WARN(adap, "SGE idma%d, queue%u,resumed after %d sec\n",
 +					i, s->idma_qid[i],
 +					s->idma_stalled[i]/HZ);
 +			s->idma_stalled[i] = 0;
 +			continue;
 +		}
 +
 +		/* Synthesize an SGE Ingress DMA Same State Timer in the Hz
 +		 * domain.  The first time we get here it'll be because we
 +		 * passed the 1s Threshold; each additional time it'll be
 +		 * because the RX Timer Callback is being fired on its regular
 +		 * schedule.
 +		 *
 +		 * If the stall is below our Potential Hung Ingress Queue
 +		 * Warning Threshold, continue.
 +		 */
 +		if (s->idma_stalled[i] == 0)
 +			s->idma_stalled[i] = HZ;
 +		else
 +			s->idma_stalled[i] += RX_QCHECK_PERIOD;
 +
 +		if (s->idma_stalled[i] < SGE_IDMA_WARN_THRESH)
 +			continue;
 +
 +		/* We'll issue a warning every SGE_IDMA_WARN_REPEAT Hz */
 +		if (((s->idma_stalled[i] - HZ) % SGE_IDMA_WARN_REPEAT) != 0)
 +			continue;
 +
 +		/* Read and save the SGE IDMA State and Queue ID information.
 +		 * We do this every time in case it changes across time ...
 +		 */
 +		t4_write_reg(adap, SGE_DEBUG_INDEX, 0);
 +		debug0 = t4_read_reg(adap, SGE_DEBUG_DATA_LOW);
 +		s->idma_state[i] = (debug0 >> (i * 9)) & 0x3f;
 +
 +		t4_write_reg(adap, SGE_DEBUG_INDEX, 11);
 +		debug11 = t4_read_reg(adap, SGE_DEBUG_DATA_LOW);
 +		s->idma_qid[i] = (debug11 >> (i * 16)) & 0xffff;
 +
 +		CH_WARN(adap, "SGE idma%u, queue%u, maybe stuck state%u %dsecs (debug0=%#x, debug11=%#x)\n",
 +			i, s->idma_qid[i], s->idma_state[i],
 +			s->idma_stalled[i]/HZ, debug0, debug11);
 +		t4_sge_decode_idma_state(adap, s->idma_state[i]);
 +	}
++=======
+ 	t4_idma_monitor(adap, &s->idma_monitor, HZ, RX_QCHECK_PERIOD);
++>>>>>>> a3bfb6179cd1 (cxgb4: Move SGE Ingress DMA state monitor code to a new routine)
  
+ done:
  	mod_timer(&s->rx_timer, jiffies + RX_QCHECK_PERIOD);
  }
  
@@@ -3019,18 -3053,18 +3020,18 @@@ int t4_sge_init(struct adapter *adap
  	 * buffers and a new field which only applies to Packed Mode Free List
  	 * buffers.
  	 */
 -	sge_conm_ctrl = t4_read_reg(adap, SGE_CONM_CTRL_A);
 +	sge_conm_ctrl = t4_read_reg(adap, SGE_CONM_CTRL);
  	if (is_t4(adap->params.chip))
 -		egress_threshold = EGRTHRESHOLD_G(sge_conm_ctrl);
 +		egress_threshold = EGRTHRESHOLD_GET(sge_conm_ctrl);
  	else
 -		egress_threshold = EGRTHRESHOLDPACKING_G(sge_conm_ctrl);
 +		egress_threshold = EGRTHRESHOLDPACKING_GET(sge_conm_ctrl);
  	s->fl_starve_thres = 2*egress_threshold + 1;
  
+ 	t4_idma_monitor_init(adap, &s->idma_monitor);
+ 
  	setup_timer(&s->rx_timer, sge_rx_timer_cb, (unsigned long)adap);
  	setup_timer(&s->tx_timer, sge_tx_timer_cb, (unsigned long)adap);
- 	s->idma_1s_thresh = core_ticks_per_usec(adap) * 1000000;  /* 1 s */
- 	s->idma_stalled[0] = 0;
- 	s->idma_stalled[1] = 0;
+ 
  	spin_lock_init(&s->intrq_lock);
  
  	return 0;
diff --cc drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
index 4234b8c917e7,6164ef3e1376..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
@@@ -4289,3 -5431,416 +4289,419 @@@ int t4_port_init(struct adapter *adap, 
  	}
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  *	t4_read_cimq_cfg - read CIM queue configuration
+  *	@adap: the adapter
+  *	@base: holds the queue base addresses in bytes
+  *	@size: holds the queue sizes in bytes
+  *	@thres: holds the queue full thresholds in bytes
+  *
+  *	Returns the current configuration of the CIM queues, starting with
+  *	the IBQs, then the OBQs.
+  */
+ void t4_read_cimq_cfg(struct adapter *adap, u16 *base, u16 *size, u16 *thres)
+ {
+ 	unsigned int i, v;
+ 	int cim_num_obq = is_t4(adap->params.chip) ?
+ 				CIM_NUM_OBQ : CIM_NUM_OBQ_T5;
+ 
+ 	for (i = 0; i < CIM_NUM_IBQ; i++) {
+ 		t4_write_reg(adap, CIM_QUEUE_CONFIG_REF_A, IBQSELECT_F |
+ 			     QUENUMSELECT_V(i));
+ 		v = t4_read_reg(adap, CIM_QUEUE_CONFIG_CTRL_A);
+ 		/* value is in 256-byte units */
+ 		*base++ = CIMQBASE_G(v) * 256;
+ 		*size++ = CIMQSIZE_G(v) * 256;
+ 		*thres++ = QUEFULLTHRSH_G(v) * 8; /* 8-byte unit */
+ 	}
+ 	for (i = 0; i < cim_num_obq; i++) {
+ 		t4_write_reg(adap, CIM_QUEUE_CONFIG_REF_A, OBQSELECT_F |
+ 			     QUENUMSELECT_V(i));
+ 		v = t4_read_reg(adap, CIM_QUEUE_CONFIG_CTRL_A);
+ 		/* value is in 256-byte units */
+ 		*base++ = CIMQBASE_G(v) * 256;
+ 		*size++ = CIMQSIZE_G(v) * 256;
+ 	}
+ }
+ 
+ /**
+  *	t4_read_cim_ibq - read the contents of a CIM inbound queue
+  *	@adap: the adapter
+  *	@qid: the queue index
+  *	@data: where to store the queue contents
+  *	@n: capacity of @data in 32-bit words
+  *
+  *	Reads the contents of the selected CIM queue starting at address 0 up
+  *	to the capacity of @data.  @n must be a multiple of 4.  Returns < 0 on
+  *	error and the number of 32-bit words actually read on success.
+  */
+ int t4_read_cim_ibq(struct adapter *adap, unsigned int qid, u32 *data, size_t n)
+ {
+ 	int i, err, attempts;
+ 	unsigned int addr;
+ 	const unsigned int nwords = CIM_IBQ_SIZE * 4;
+ 
+ 	if (qid > 5 || (n & 3))
+ 		return -EINVAL;
+ 
+ 	addr = qid * nwords;
+ 	if (n > nwords)
+ 		n = nwords;
+ 
+ 	/* It might take 3-10ms before the IBQ debug read access is allowed.
+ 	 * Wait for 1 Sec with a delay of 1 usec.
+ 	 */
+ 	attempts = 1000000;
+ 
+ 	for (i = 0; i < n; i++, addr++) {
+ 		t4_write_reg(adap, CIM_IBQ_DBG_CFG_A, IBQDBGADDR_V(addr) |
+ 			     IBQDBGEN_F);
+ 		err = t4_wait_op_done(adap, CIM_IBQ_DBG_CFG_A, IBQDBGBUSY_F, 0,
+ 				      attempts, 1);
+ 		if (err)
+ 			return err;
+ 		*data++ = t4_read_reg(adap, CIM_IBQ_DBG_DATA_A);
+ 	}
+ 	t4_write_reg(adap, CIM_IBQ_DBG_CFG_A, 0);
+ 	return i;
+ }
+ 
+ /**
+  *	t4_read_cim_obq - read the contents of a CIM outbound queue
+  *	@adap: the adapter
+  *	@qid: the queue index
+  *	@data: where to store the queue contents
+  *	@n: capacity of @data in 32-bit words
+  *
+  *	Reads the contents of the selected CIM queue starting at address 0 up
+  *	to the capacity of @data.  @n must be a multiple of 4.  Returns < 0 on
+  *	error and the number of 32-bit words actually read on success.
+  */
+ int t4_read_cim_obq(struct adapter *adap, unsigned int qid, u32 *data, size_t n)
+ {
+ 	int i, err;
+ 	unsigned int addr, v, nwords;
+ 	int cim_num_obq = is_t4(adap->params.chip) ?
+ 				CIM_NUM_OBQ : CIM_NUM_OBQ_T5;
+ 
+ 	if ((qid > (cim_num_obq - 1)) || (n & 3))
+ 		return -EINVAL;
+ 
+ 	t4_write_reg(adap, CIM_QUEUE_CONFIG_REF_A, OBQSELECT_F |
+ 		     QUENUMSELECT_V(qid));
+ 	v = t4_read_reg(adap, CIM_QUEUE_CONFIG_CTRL_A);
+ 
+ 	addr = CIMQBASE_G(v) * 64;    /* muliple of 256 -> muliple of 4 */
+ 	nwords = CIMQSIZE_G(v) * 64;  /* same */
+ 	if (n > nwords)
+ 		n = nwords;
+ 
+ 	for (i = 0; i < n; i++, addr++) {
+ 		t4_write_reg(adap, CIM_OBQ_DBG_CFG_A, OBQDBGADDR_V(addr) |
+ 			     OBQDBGEN_F);
+ 		err = t4_wait_op_done(adap, CIM_OBQ_DBG_CFG_A, OBQDBGBUSY_F, 0,
+ 				      2, 1);
+ 		if (err)
+ 			return err;
+ 		*data++ = t4_read_reg(adap, CIM_OBQ_DBG_DATA_A);
+ 	}
+ 	t4_write_reg(adap, CIM_OBQ_DBG_CFG_A, 0);
+ 	return i;
+ }
+ 
+ /**
+  *	t4_cim_read - read a block from CIM internal address space
+  *	@adap: the adapter
+  *	@addr: the start address within the CIM address space
+  *	@n: number of words to read
+  *	@valp: where to store the result
+  *
+  *	Reads a block of 4-byte words from the CIM intenal address space.
+  */
+ int t4_cim_read(struct adapter *adap, unsigned int addr, unsigned int n,
+ 		unsigned int *valp)
+ {
+ 	int ret = 0;
+ 
+ 	if (t4_read_reg(adap, CIM_HOST_ACC_CTRL_A) & HOSTBUSY_F)
+ 		return -EBUSY;
+ 
+ 	for ( ; !ret && n--; addr += 4) {
+ 		t4_write_reg(adap, CIM_HOST_ACC_CTRL_A, addr);
+ 		ret = t4_wait_op_done(adap, CIM_HOST_ACC_CTRL_A, HOSTBUSY_F,
+ 				      0, 5, 2);
+ 		if (!ret)
+ 			*valp++ = t4_read_reg(adap, CIM_HOST_ACC_DATA_A);
+ 	}
+ 	return ret;
+ }
+ 
+ /**
+  *	t4_cim_write - write a block into CIM internal address space
+  *	@adap: the adapter
+  *	@addr: the start address within the CIM address space
+  *	@n: number of words to write
+  *	@valp: set of values to write
+  *
+  *	Writes a block of 4-byte words into the CIM intenal address space.
+  */
+ int t4_cim_write(struct adapter *adap, unsigned int addr, unsigned int n,
+ 		 const unsigned int *valp)
+ {
+ 	int ret = 0;
+ 
+ 	if (t4_read_reg(adap, CIM_HOST_ACC_CTRL_A) & HOSTBUSY_F)
+ 		return -EBUSY;
+ 
+ 	for ( ; !ret && n--; addr += 4) {
+ 		t4_write_reg(adap, CIM_HOST_ACC_DATA_A, *valp++);
+ 		t4_write_reg(adap, CIM_HOST_ACC_CTRL_A, addr | HOSTWRITE_F);
+ 		ret = t4_wait_op_done(adap, CIM_HOST_ACC_CTRL_A, HOSTBUSY_F,
+ 				      0, 5, 2);
+ 	}
+ 	return ret;
+ }
+ 
+ static int t4_cim_write1(struct adapter *adap, unsigned int addr,
+ 			 unsigned int val)
+ {
+ 	return t4_cim_write(adap, addr, 1, &val);
+ }
+ 
+ /**
+  *	t4_cim_read_la - read CIM LA capture buffer
+  *	@adap: the adapter
+  *	@la_buf: where to store the LA data
+  *	@wrptr: the HW write pointer within the capture buffer
+  *
+  *	Reads the contents of the CIM LA buffer with the most recent entry at
+  *	the end	of the returned data and with the entry at @wrptr first.
+  *	We try to leave the LA in the running state we find it in.
+  */
+ int t4_cim_read_la(struct adapter *adap, u32 *la_buf, unsigned int *wrptr)
+ {
+ 	int i, ret;
+ 	unsigned int cfg, val, idx;
+ 
+ 	ret = t4_cim_read(adap, UP_UP_DBG_LA_CFG_A, 1, &cfg);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (cfg & UPDBGLAEN_F) {	/* LA is running, freeze it */
+ 		ret = t4_cim_write1(adap, UP_UP_DBG_LA_CFG_A, 0);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	ret = t4_cim_read(adap, UP_UP_DBG_LA_CFG_A, 1, &val);
+ 	if (ret)
+ 		goto restart;
+ 
+ 	idx = UPDBGLAWRPTR_G(val);
+ 	if (wrptr)
+ 		*wrptr = idx;
+ 
+ 	for (i = 0; i < adap->params.cim_la_size; i++) {
+ 		ret = t4_cim_write1(adap, UP_UP_DBG_LA_CFG_A,
+ 				    UPDBGLARDPTR_V(idx) | UPDBGLARDEN_F);
+ 		if (ret)
+ 			break;
+ 		ret = t4_cim_read(adap, UP_UP_DBG_LA_CFG_A, 1, &val);
+ 		if (ret)
+ 			break;
+ 		if (val & UPDBGLARDEN_F) {
+ 			ret = -ETIMEDOUT;
+ 			break;
+ 		}
+ 		ret = t4_cim_read(adap, UP_UP_DBG_LA_DATA_A, 1, &la_buf[i]);
+ 		if (ret)
+ 			break;
+ 		idx = (idx + 1) & UPDBGLARDPTR_M;
+ 	}
+ restart:
+ 	if (cfg & UPDBGLAEN_F) {
+ 		int r = t4_cim_write1(adap, UP_UP_DBG_LA_CFG_A,
+ 				      cfg & ~UPDBGLARDEN_F);
+ 		if (!ret)
+ 			ret = r;
+ 	}
+ 	return ret;
+ }
+ 
+ /**
+  *	t4_tp_read_la - read TP LA capture buffer
+  *	@adap: the adapter
+  *	@la_buf: where to store the LA data
+  *	@wrptr: the HW write pointer within the capture buffer
+  *
+  *	Reads the contents of the TP LA buffer with the most recent entry at
+  *	the end	of the returned data and with the entry at @wrptr first.
+  *	We leave the LA in the running state we find it in.
+  */
+ void t4_tp_read_la(struct adapter *adap, u64 *la_buf, unsigned int *wrptr)
+ {
+ 	bool last_incomplete;
+ 	unsigned int i, cfg, val, idx;
+ 
+ 	cfg = t4_read_reg(adap, TP_DBG_LA_CONFIG_A) & 0xffff;
+ 	if (cfg & DBGLAENABLE_F)			/* freeze LA */
+ 		t4_write_reg(adap, TP_DBG_LA_CONFIG_A,
+ 			     adap->params.tp.la_mask | (cfg ^ DBGLAENABLE_F));
+ 
+ 	val = t4_read_reg(adap, TP_DBG_LA_CONFIG_A);
+ 	idx = DBGLAWPTR_G(val);
+ 	last_incomplete = DBGLAMODE_G(val) >= 2 && (val & DBGLAWHLF_F) == 0;
+ 	if (last_incomplete)
+ 		idx = (idx + 1) & DBGLARPTR_M;
+ 	if (wrptr)
+ 		*wrptr = idx;
+ 
+ 	val &= 0xffff;
+ 	val &= ~DBGLARPTR_V(DBGLARPTR_M);
+ 	val |= adap->params.tp.la_mask;
+ 
+ 	for (i = 0; i < TPLA_SIZE; i++) {
+ 		t4_write_reg(adap, TP_DBG_LA_CONFIG_A, DBGLARPTR_V(idx) | val);
+ 		la_buf[i] = t4_read_reg64(adap, TP_DBG_LA_DATAL_A);
+ 		idx = (idx + 1) & DBGLARPTR_M;
+ 	}
+ 
+ 	/* Wipe out last entry if it isn't valid */
+ 	if (last_incomplete)
+ 		la_buf[TPLA_SIZE - 1] = ~0ULL;
+ 
+ 	if (cfg & DBGLAENABLE_F)                    /* restore running state */
+ 		t4_write_reg(adap, TP_DBG_LA_CONFIG_A,
+ 			     cfg | adap->params.tp.la_mask);
+ }
+ 
+ /* SGE Hung Ingress DMA Warning Threshold time and Warning Repeat Rate (in
+  * seconds).  If we find one of the SGE Ingress DMA State Machines in the same
+  * state for more than the Warning Threshold then we'll issue a warning about
+  * a potential hang.  We'll repeat the warning as the SGE Ingress DMA Channel
+  * appears to be hung every Warning Repeat second till the situation clears.
+  * If the situation clears, we'll note that as well.
+  */
+ #define SGE_IDMA_WARN_THRESH 1
+ #define SGE_IDMA_WARN_REPEAT 300
+ 
+ /**
+  *	t4_idma_monitor_init - initialize SGE Ingress DMA Monitor
+  *	@adapter: the adapter
+  *	@idma: the adapter IDMA Monitor state
+  *
+  *	Initialize the state of an SGE Ingress DMA Monitor.
+  */
+ void t4_idma_monitor_init(struct adapter *adapter,
+ 			  struct sge_idma_monitor_state *idma)
+ {
+ 	/* Initialize the state variables for detecting an SGE Ingress DMA
+ 	 * hang.  The SGE has internal counters which count up on each clock
+ 	 * tick whenever the SGE finds its Ingress DMA State Engines in the
+ 	 * same state they were on the previous clock tick.  The clock used is
+ 	 * the Core Clock so we have a limit on the maximum "time" they can
+ 	 * record; typically a very small number of seconds.  For instance,
+ 	 * with a 600MHz Core Clock, we can only count up to a bit more than
+ 	 * 7s.  So we'll synthesize a larger counter in order to not run the
+ 	 * risk of having the "timers" overflow and give us the flexibility to
+ 	 * maintain a Hung SGE State Machine of our own which operates across
+ 	 * a longer time frame.
+ 	 */
+ 	idma->idma_1s_thresh = core_ticks_per_usec(adapter) * 1000000; /* 1s */
+ 	idma->idma_stalled[0] = 0;
+ 	idma->idma_stalled[1] = 0;
+ }
+ 
+ /**
+  *	t4_idma_monitor - monitor SGE Ingress DMA state
+  *	@adapter: the adapter
+  *	@idma: the adapter IDMA Monitor state
+  *	@hz: number of ticks/second
+  *	@ticks: number of ticks since the last IDMA Monitor call
+  */
+ void t4_idma_monitor(struct adapter *adapter,
+ 		     struct sge_idma_monitor_state *idma,
+ 		     int hz, int ticks)
+ {
+ 	int i, idma_same_state_cnt[2];
+ 
+ 	 /* Read the SGE Debug Ingress DMA Same State Count registers.  These
+ 	  * are counters inside the SGE which count up on each clock when the
+ 	  * SGE finds its Ingress DMA State Engines in the same states they
+ 	  * were in the previous clock.  The counters will peg out at
+ 	  * 0xffffffff without wrapping around so once they pass the 1s
+ 	  * threshold they'll stay above that till the IDMA state changes.
+ 	  */
+ 	t4_write_reg(adapter, SGE_DEBUG_INDEX_A, 13);
+ 	idma_same_state_cnt[0] = t4_read_reg(adapter, SGE_DEBUG_DATA_HIGH_A);
+ 	idma_same_state_cnt[1] = t4_read_reg(adapter, SGE_DEBUG_DATA_LOW_A);
+ 
+ 	for (i = 0; i < 2; i++) {
+ 		u32 debug0, debug11;
+ 
+ 		/* If the Ingress DMA Same State Counter ("timer") is less
+ 		 * than 1s, then we can reset our synthesized Stall Timer and
+ 		 * continue.  If we have previously emitted warnings about a
+ 		 * potential stalled Ingress Queue, issue a note indicating
+ 		 * that the Ingress Queue has resumed forward progress.
+ 		 */
+ 		if (idma_same_state_cnt[i] < idma->idma_1s_thresh) {
+ 			if (idma->idma_stalled[i] >= SGE_IDMA_WARN_THRESH * hz)
+ 				dev_warn(adapter->pdev_dev, "SGE idma%d, queue %u, "
+ 					 "resumed after %d seconds\n",
+ 					 i, idma->idma_qid[i],
+ 					 idma->idma_stalled[i] / hz);
+ 			idma->idma_stalled[i] = 0;
+ 			continue;
+ 		}
+ 
+ 		/* Synthesize an SGE Ingress DMA Same State Timer in the Hz
+ 		 * domain.  The first time we get here it'll be because we
+ 		 * passed the 1s Threshold; each additional time it'll be
+ 		 * because the RX Timer Callback is being fired on its regular
+ 		 * schedule.
+ 		 *
+ 		 * If the stall is below our Potential Hung Ingress Queue
+ 		 * Warning Threshold, continue.
+ 		 */
+ 		if (idma->idma_stalled[i] == 0) {
+ 			idma->idma_stalled[i] = hz;
+ 			idma->idma_warn[i] = 0;
+ 		} else {
+ 			idma->idma_stalled[i] += ticks;
+ 			idma->idma_warn[i] -= ticks;
+ 		}
+ 
+ 		if (idma->idma_stalled[i] < SGE_IDMA_WARN_THRESH * hz)
+ 			continue;
+ 
+ 		/* We'll issue a warning every SGE_IDMA_WARN_REPEAT seconds.
+ 		 */
+ 		if (idma->idma_warn[i] > 0)
+ 			continue;
+ 		idma->idma_warn[i] = SGE_IDMA_WARN_REPEAT * hz;
+ 
+ 		/* Read and save the SGE IDMA State and Queue ID information.
+ 		 * We do this every time in case it changes across time ...
+ 		 * can't be too careful ...
+ 		 */
+ 		t4_write_reg(adapter, SGE_DEBUG_INDEX_A, 0);
+ 		debug0 = t4_read_reg(adapter, SGE_DEBUG_DATA_LOW_A);
+ 		idma->idma_state[i] = (debug0 >> (i * 9)) & 0x3f;
+ 
+ 		t4_write_reg(adapter, SGE_DEBUG_INDEX_A, 11);
+ 		debug11 = t4_read_reg(adapter, SGE_DEBUG_DATA_LOW_A);
+ 		idma->idma_qid[i] = (debug11 >> (i * 16)) & 0xffff;
+ 
+ 		dev_warn(adapter->pdev_dev, "SGE idma%u, queue %u, potentially stuck in "
+ 			 "state %u for %d seconds (debug0=%#x, debug11=%#x)\n",
+ 			 i, idma->idma_qid[i], idma->idma_state[i],
+ 			 idma->idma_stalled[i] / hz,
+ 			 debug0, debug11);
+ 		t4_sge_decode_idma_state(adapter, idma->idma_state[i]);
+ 	}
+ }
++>>>>>>> a3bfb6179cd1 (cxgb4: Move SGE Ingress DMA state monitor code to a new routine)
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/sge.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/t4_hw.c
