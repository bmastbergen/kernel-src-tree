blk-mq: add blk_mq_init_allocated_queue and export blk_mq_register_disk

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Mike Snitzer <snitzer@redhat.com>
commit b62c21b71f08b7a4bfd025616ff1da2913a82904
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/b62c21b7.failed

Add a variant of blk_mq_init_queue that allows a previously allocated
queue to be initialized.  blk_mq_init_allocated_queue models
blk_init_allocated_queue -- which was also created for DM's use.

DM's approach to device creation requires a placeholder request_queue be
allocated for use with alloc_dev() but the decision about what type of
request_queue will be ultimately created is deferred until all component
devices referenced in the DM table are processed to determine the table
type (request-based, blk-mq request-based, or bio-based).

Also, because of DM's late finalization of the request_queue type
the call to blk_mq_register_disk() doesn't happen during alloc_dev().
Must export blk_mq_register_disk() so that DM can backfill the 'mq' dir
once the blk-mq queue is fully allocated.

	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
	Reviewed-by: Ming Lei <ming.lei@canonical.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit b62c21b71f08b7a4bfd025616ff1da2913a82904)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	include/linux/blk-mq.h
diff --cc block/blk-mq.c
index 77afc91943c8,3000121840bb..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1759,7 -1868,45 +1759,24 @@@ static void blk_mq_add_queue_tag_set(st
  	mutex_unlock(&set->tag_list_lock);
  }
  
 -/*
 - * It is the actual release handler for mq, but we do it from
 - * request queue's release handler for avoiding use-after-free
 - * and headache because q->mq_kobj shouldn't have been introduced,
 - * but we can't group ctx/kctx kobj without it.
 - */
 -void blk_mq_release(struct request_queue *q)
 -{
 -	struct blk_mq_hw_ctx *hctx;
 -	unsigned int i;
 -
 -	/* hctx kobj stays in hctx */
 -	queue_for_each_hw_ctx(q, hctx, i)
 -		kfree(hctx);
 -
 -	kfree(q->queue_hw_ctx);
 -
 -	/* ctx kobj stays in queue_ctx */
 -	free_percpu(q->queue_ctx);
 -}
 -
  struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
+ {
+ 	struct request_queue *uninit_q, *q;
+ 
+ 	uninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);
+ 	if (!uninit_q)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	q = blk_mq_init_allocated_queue(set, uninit_q);
+ 	if (IS_ERR(q))
+ 		blk_cleanup_queue(uninit_q);
+ 
+ 	return q;
+ }
+ EXPORT_SYMBOL(blk_mq_init_queue);
+ 
+ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
+ 						  struct request_queue *q)
  {
  	struct blk_mq_hw_ctx **hctxs;
  	struct blk_mq_ctx __percpu *ctx;
@@@ -1807,12 -1944,13 +1823,22 @@@
  		hctxs[i]->queue_num = i;
  	}
  
++<<<<<<< HEAD
 +	q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);
 +	if (!q)
 +		goto err_hctxs;
 +
 +	if (percpu_ref_init(&q->mq_usage_counter, blk_mq_usage_counter_release))
 +		goto err_map;
++=======
+ 	/*
+ 	 * Init percpu_ref in atomic mode so that it's faster to shutdown.
+ 	 * See blk_register_queue() for details.
+ 	 */
+ 	if (percpu_ref_init(&q->mq_usage_counter, blk_mq_usage_counter_release,
+ 			    PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
+ 		goto err_hctxs;
++>>>>>>> b62c21b71f08 (blk-mq: add blk_mq_init_allocated_queue and export blk_mq_register_disk)
  
  	setup_timer(&q->timeout, blk_mq_rq_timer, (unsigned long) q);
  	blk_queue_rq_timeout(q, 30000);
@@@ -1855,7 -1993,7 +1881,11 @@@
  	blk_mq_init_cpu_queues(q, set->nr_hw_queues);
  
  	if (blk_mq_init_hw_queues(q, set))
++<<<<<<< HEAD
 +		goto err_hw;
++=======
+ 		goto err_hctxs;
++>>>>>>> b62c21b71f08 (blk-mq: add blk_mq_init_allocated_queue and export blk_mq_register_disk)
  
  	mutex_lock(&all_q_mutex);
  	list_add_tail(&q->all_q_node, &all_q_list);
@@@ -1870,10 -2005,6 +1900,13 @@@
  
  	return q;
  
++<<<<<<< HEAD
 +err_hw_queues:
 +	blk_mq_exit_hw_queues(q, set, set->nr_hw_queues);
 +err_hw:
 +	blk_cleanup_queue(q);
++=======
++>>>>>>> b62c21b71f08 (blk-mq: add blk_mq_init_allocated_queue and export blk_mq_register_disk)
  err_hctxs:
  	kfree(map);
  	for (i = 0; i < set->nr_hw_queues; i++) {
diff --cc include/linux/blk-mq.h
index 736ba31fe3c6,9a75c88e8908..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -188,8 -156,17 +188,14 @@@ enum 
  
  	BLK_MQ_CPU_WORK_BATCH	= 8,
  };
 -#define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
 -	((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
 -		((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
 -#define BLK_ALLOC_POLICY_TO_MQ_FLAG(policy) \
 -	((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
 -		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
  
  struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
++<<<<<<< HEAD
++=======
+ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
+ 						  struct request_queue *q);
+ void blk_mq_finish_init(struct request_queue *q);
++>>>>>>> b62c21b71f08 (blk-mq: add blk_mq_init_allocated_queue and export blk_mq_register_disk)
  int blk_mq_register_disk(struct gendisk *);
  void blk_mq_unregister_disk(struct gendisk *);
  
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index e0fb3f4a628f..3f340468c6ec 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -430,6 +430,7 @@ int blk_mq_register_disk(struct gendisk *disk)
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(blk_mq_register_disk);
 
 void blk_mq_sysfs_unregister(struct request_queue *q)
 {
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blk-mq.h
