cxgb4: Add low latency socket busy_poll support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Hariprasad Shenai <hariprasad@chelsio.com>
commit 3a336cb17183b29827fdffaffb5e62f8912f5ca1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/3a336cb1.failed

cxgb_busy_poll, corresponding to ndo_busy_poll, gets called by the socket
waiting for data.

With busy_poll enabled, improvement is seen in latency numbers as observed by
collecting netperf TCP_RR numbers.
Below are latency number, with and without busy-poll, in a switched environment
for a particular msg size:
netperf command: netperf -4 -H <ip> -l 30 -t TCP_RR -- -r1,1
Latency without busy-poll: ~16.25 us
Latency with busy-poll   : ~08.79 us

Based on original work by Kumar Sanghvi <kumaras@chelsio.com>

	Signed-off-by: Hariprasad Shenai <hariprasad@chelsio.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3a336cb17183b29827fdffaffb5e62f8912f5ca1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/chelsio/cxgb4/sge.c
#	drivers/net/ethernet/chelsio/cxgb4/t4_values.h
diff --cc drivers/net/ethernet/chelsio/cxgb4/sge.c
index b5afc3f0dd91,b4b9f6048fe7..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@@ -43,8 -43,12 +43,11 @@@
  #include <linux/export.h>
  #include <net/ipv6.h>
  #include <net/tcp.h>
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ #include <net/busy_poll.h>
+ #endif /* CONFIG_NET_RX_BUSY_POLL */
  #include "cxgb4.h"
  #include "t4_regs.h"
 -#include "t4_values.h"
  #include "t4_msg.h"
  #include "t4fw_api.h"
  
@@@ -1713,8 -1723,10 +1716,9 @@@ static void do_gro(struct sge_eth_rxq *
  	skb->truesize += skb->data_len;
  	skb->ip_summed = CHECKSUM_UNNECESSARY;
  	skb_record_rx_queue(skb, rxq->rspq.idx);
+ 	skb_mark_napi_id(skb, &rxq->rspq.napi);
  	if (rxq->rspq.netdev->features & NETIF_F_RXHASH)
 -		skb_set_hash(skb, (__force u32)pkt->rsshdr.hash_val,
 -			     PKT_HASH_TYPE_L3);
 +		skb->rxhash = (__force u32)pkt->rsshdr.hash_val;
  
  	if (unlikely(pkt->vlan_ex)) {
  		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(pkt->vlan));
@@@ -1754,7 -1766,8 +1758,12 @@@ int t4_ethrx_handler(struct sge_rspq *q
  	pkt = (const struct cpl_rx_pkt *)rsp;
  	csum_ok = pkt->csum_calc && !pkt->err_vec &&
  		  (q->netdev->features & NETIF_F_RXCSUM);
++<<<<<<< HEAD
 +	if ((pkt->l2info & htonl(RXF_TCP)) &&
++=======
+ 	if ((pkt->l2info & htonl(RXF_TCP_F)) &&
+ 	    !(cxgb_poll_busy_polling(q)) &&
++>>>>>>> 3a336cb17183 (cxgb4: Add low latency socket busy_poll support)
  	    (q->netdev->features & NETIF_F_GRO) && csum_ok && !pkt->ip_frag) {
  		do_gro(rxq, si, pkt);
  		return 0;
@@@ -1996,14 -2047,20 +2042,15 @@@ static int napi_rx_handler(struct napi_
  	} else
  		params = QINTR_TIMER_IDX(7);
  
 -	val = CIDXINC_V(work_done) | SEINTARM_V(params);
 -
 -	/* If we don't have access to the new User GTS (T5+), use the old
 -	 * doorbell mechanism; otherwise use the new BAR2 mechanism.
 -	 */
 -	if (unlikely(q->bar2_addr == NULL)) {
 -		t4_write_reg(q->adap, MYPF_REG(SGE_PF_GTS_A),
 -			     val | INGRESSQID_V((u32)q->cntxt_id));
 +	val = CIDXINC(work_done) | SEINTARM(params);
 +	if (is_t4(q->adap->params.chip)) {
 +		t4_write_reg(q->adap, MYPF_REG(SGE_PF_GTS),
 +			     val | INGRESSQID((u32)q->cntxt_id));
  	} else {
 -		writel(val | INGRESSQID_V(q->bar2_qid),
 -		       q->bar2_addr + SGE_UDB_GTS);
 +		writel(val, q->adap->bar2 + q->udb + SGE_UDB_GTS);
  		wmb();
  	}
+ 	cxgb_poll_unlock_napi(q);
  	return work_done;
  }
  
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/t4_values.h
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index 067eaa5ac000..43ef9ed02b49 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@ -469,6 +469,22 @@ struct sge_rspq {                   /* state for an SGE response queue */
 	struct adapter *adap;
 	struct net_device *netdev;  /* associated net device */
 	rspq_handler_t handler;
+#ifdef CONFIG_NET_RX_BUSY_POLL
+#define CXGB_POLL_STATE_IDLE		0
+#define CXGB_POLL_STATE_NAPI		BIT(0) /* NAPI owns this poll */
+#define CXGB_POLL_STATE_POLL		BIT(1) /* poll owns this poll */
+#define CXGB_POLL_STATE_NAPI_YIELD	BIT(2) /* NAPI yielded this poll */
+#define CXGB_POLL_STATE_POLL_YIELD	BIT(3) /* poll yielded this poll */
+#define CXGB_POLL_YIELD			(CXGB_POLL_STATE_NAPI_YIELD |   \
+					 CXGB_POLL_STATE_POLL_YIELD)
+#define CXGB_POLL_LOCKED		(CXGB_POLL_STATE_NAPI |         \
+					 CXGB_POLL_STATE_POLL)
+#define CXGB_POLL_USER_PEND		(CXGB_POLL_STATE_POLL |         \
+					 CXGB_POLL_STATE_POLL_YIELD)
+	unsigned int bpoll_state;
+	spinlock_t bpoll_lock;		/* lock for busy poll */
+#endif /* CONFIG_NET_RX_BUSY_POLL */
+
 };
 
 struct sge_eth_stats {              /* Ethernet queue statistics */
@@ -866,6 +882,102 @@ static inline struct adapter *netdev2adap(const struct net_device *dev)
 	return netdev2pinfo(dev)->adapter;
 }
 
+#ifdef CONFIG_NET_RX_BUSY_POLL
+static inline void cxgb_busy_poll_init_lock(struct sge_rspq *q)
+{
+	spin_lock_init(&q->bpoll_lock);
+	q->bpoll_state = CXGB_POLL_STATE_IDLE;
+}
+
+static inline bool cxgb_poll_lock_napi(struct sge_rspq *q)
+{
+	bool rc = true;
+
+	spin_lock(&q->bpoll_lock);
+	if (q->bpoll_state & CXGB_POLL_LOCKED) {
+		q->bpoll_state |= CXGB_POLL_STATE_NAPI_YIELD;
+		rc = false;
+	} else {
+		q->bpoll_state = CXGB_POLL_STATE_NAPI;
+	}
+	spin_unlock(&q->bpoll_lock);
+	return rc;
+}
+
+static inline bool cxgb_poll_unlock_napi(struct sge_rspq *q)
+{
+	bool rc = false;
+
+	spin_lock(&q->bpoll_lock);
+	if (q->bpoll_state & CXGB_POLL_STATE_POLL_YIELD)
+		rc = true;
+	q->bpoll_state = CXGB_POLL_STATE_IDLE;
+	spin_unlock(&q->bpoll_lock);
+	return rc;
+}
+
+static inline bool cxgb_poll_lock_poll(struct sge_rspq *q)
+{
+	bool rc = true;
+
+	spin_lock_bh(&q->bpoll_lock);
+	if (q->bpoll_state & CXGB_POLL_LOCKED) {
+		q->bpoll_state |= CXGB_POLL_STATE_POLL_YIELD;
+		rc = false;
+	} else {
+		q->bpoll_state |= CXGB_POLL_STATE_POLL;
+	}
+	spin_unlock_bh(&q->bpoll_lock);
+	return rc;
+}
+
+static inline bool cxgb_poll_unlock_poll(struct sge_rspq *q)
+{
+	bool rc = false;
+
+	spin_lock_bh(&q->bpoll_lock);
+	if (q->bpoll_state & CXGB_POLL_STATE_POLL_YIELD)
+		rc = true;
+	q->bpoll_state = CXGB_POLL_STATE_IDLE;
+	spin_unlock_bh(&q->bpoll_lock);
+	return rc;
+}
+
+static inline bool cxgb_poll_busy_polling(struct sge_rspq *q)
+{
+	return q->bpoll_state & CXGB_POLL_USER_PEND;
+}
+#else
+static inline void cxgb_busy_poll_init_lock(struct sge_rspq *q)
+{
+}
+
+static inline bool cxgb_poll_lock_napi(struct sge_rspq *q)
+{
+	return true;
+}
+
+static inline bool cxgb_poll_unlock_napi(struct sge_rspq *q)
+{
+	return false;
+}
+
+static inline bool cxgb_poll_lock_poll(struct sge_rspq *q)
+{
+	return false;
+}
+
+static inline bool cxgb_poll_unlock_poll(struct sge_rspq *q)
+{
+	return false;
+}
+
+static inline bool cxgb_poll_busy_polling(struct sge_rspq *q)
+{
+	return false;
+}
+#endif /* CONFIG_NET_RX_BUSY_POLL */
+
 void t4_os_portmod_changed(const struct adapter *adap, int port_id);
 void t4_os_link_changed(struct adapter *adap, int port_id, int link_stat);
 
@@ -894,6 +1006,7 @@ irqreturn_t t4_sge_intr_msix(int irq, void *cookie);
 int t4_sge_init(struct adapter *adap);
 void t4_sge_start(struct adapter *adap);
 void t4_sge_stop(struct adapter *adap);
+int cxgb_busy_poll(struct napi_struct *napi);
 extern int dbfifo_int_thresh;
 
 #define for_each_port(adapter, iter) \
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
index eacea363ec4e..eadfb20a60af 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_main.c
@@ -997,8 +997,14 @@ static void quiesce_rx(struct adapter *adap)
 	for (i = 0; i < ARRAY_SIZE(adap->sge.ingr_map); i++) {
 		struct sge_rspq *q = adap->sge.ingr_map[i];
 
-		if (q && q->handler)
+		if (q && q->handler) {
 			napi_disable(&q->napi);
+			local_bh_disable();
+			while (!cxgb_poll_lock_napi(q))
+				mdelay(1);
+			local_bh_enable();
+		}
+
 	}
 }
 
@@ -1014,8 +1020,10 @@ static void enable_rx(struct adapter *adap)
 
 		if (!q)
 			continue;
-		if (q->handler)
+		if (q->handler) {
+			cxgb_busy_poll_init_lock(q);
 			napi_enable(&q->napi);
+		}
 		/* 0-increment GTS to start the timer and enable interrupts */
 		t4_write_reg(adap, MYPF_REG(SGE_PF_GTS),
 			     SEINTARM(q->intr_params) |
@@ -4780,6 +4788,10 @@ static const struct net_device_ops cxgb4_netdev_ops = {
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller  = cxgb_netpoll,
 #endif
+#ifdef CONFIG_NET_RX_BUSY_POLL
+	.ndo_busy_poll        = cxgb_busy_poll,
+#endif
+
 };
 
 void t4_fatal_err(struct adapter *adap)
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/sge.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/t4_values.h
