net: tcp: add per route congestion control

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] tcp: add per route congestion control (Florian Westphal) [1212624]
Rebuild_FUZZ: 93.67%
commit-author Daniel Borkmann <dborkman@redhat.com>
commit 81164413ad096bafe8ad1068f3f095a7dd081d8b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/81164413.failed

This work adds the possibility to define a per route/destination
congestion control algorithm. Generally, this opens up the possibility
for a machine with different links to enforce specific congestion
control algorithms with optimal strategies for each of them based
on their network characteristics, even transparently for a single
application listening on all links.

For our specific use case, this additionally facilitates deployment
of DCTCP, for example, applications can easily serve internal
traffic/dsts in DCTCP and external one with CUBIC. Other scenarios
would also allow for utilizing e.g. long living, low priority
background flows for certain destinations/routes while still being
able for normal traffic to utilize the default congestion control
algorithm. We also thought about a per netns setting (where different
defaults are possible), but given its actually a link specific
property, we argue that a per route/destination setting is the most
natural and flexible.

The administrator can utilize this through ip-route(8) by appending
"congctl [lock] <name>", where <name> denotes the name of a
congestion control algorithm and the optional lock parameter allows
to enforce the given algorithm so that applications in user space
would not be allowed to overwrite that algorithm for that destination.

The dst metric lookups are being done when a dst entry is already
available in order to avoid a costly lookup and still before the
algorithms are being initialized, thus overhead is very low when the
feature is not being used. While the client side would need to drop
the current reference on the module, on server side this can actually
even be avoided as we just got a flat-copied socket clone.

Joint work with Florian Westphal.

	Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Signed-off-by: Florian Westphal <fw@strlen.de>
	Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 81164413ad096bafe8ad1068f3f095a7dd081d8b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/tcp.h
#	net/ipv4/tcp_minisocks.c
diff --cc include/net/tcp.h
index 0373da38f3bc,b8fdc6bab3f3..000000000000
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@@ -450,24 -442,24 +450,44 @@@ extern const u8 *tcp_parse_md5sig_optio
   *	TCP v4 functions exported for the inet6 API
   */
  
++<<<<<<< HEAD
 +extern void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
 +extern int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
 +extern struct sock * tcp_create_openreq_child(struct sock *sk,
 +					      struct request_sock *req,
 +					      struct sk_buff *skb);
 +extern struct sock * tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 +					  struct request_sock *req,
 +					  struct dst_entry *dst);
 +extern int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
 +extern int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr,
 +			  int addr_len);
 +extern int tcp_connect(struct sock *sk);
 +extern struct sk_buff * tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 +					struct request_sock *req,
 +					struct tcp_fastopen_cookie *foc);
 +extern int tcp_disconnect(struct sock *sk, int flags);
++=======
+ void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
+ void tcp_v4_mtu_reduced(struct sock *sk);
+ int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
+ struct sock *tcp_create_openreq_child(struct sock *sk,
+ 				      struct request_sock *req,
+ 				      struct sk_buff *skb);
+ void tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst);
+ struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+ 				  struct request_sock *req,
+ 				  struct dst_entry *dst);
+ int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
+ int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);
+ int tcp_connect(struct sock *sk);
+ struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
+ 				struct request_sock *req,
+ 				struct tcp_fastopen_cookie *foc);
+ int tcp_disconnect(struct sock *sk, int flags);
++>>>>>>> 81164413ad09 (net: tcp: add per route congestion control)
  
 +void tcp_connect_init(struct sock *sk);
  void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
  int tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size);
  void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
@@@ -661,6 -632,16 +681,19 @@@ static inline u32 tcp_rto_min(struct so
  	return rto_min;
  }
  
++<<<<<<< HEAD
++=======
+ static inline u32 tcp_rto_min_us(struct sock *sk)
+ {
+ 	return jiffies_to_usecs(tcp_rto_min(sk));
+ }
+ 
+ static inline bool tcp_ca_dst_locked(const struct dst_entry *dst)
+ {
+ 	return dst_metric_locked(dst, RTAX_CC_ALGO);
+ }
+ 
++>>>>>>> 81164413ad09 (net: tcp: add per route congestion control)
  /* Compute the actual receive window we are currently advertising.
   * Rcv_nxt can be after the window if our peer push more data
   * than the offered window.
diff --cc net/ipv4/tcp_minisocks.c
index 7a436c517e44,bc9216dc9de1..000000000000
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@@ -420,11 -477,6 +446,14 @@@ struct sock *tcp_create_openreq_child(s
  		newtp->snd_cwnd = TCP_INIT_CWND;
  		newtp->snd_cwnd_cnt = 0;
  
++<<<<<<< HEAD
 +		if (newicsk->icsk_ca_ops != &tcp_init_congestion_ops &&
 +		    !try_module_get(newicsk->icsk_ca_ops->owner))
 +			newicsk->icsk_ca_ops = &tcp_init_congestion_ops;
 +
 +		tcp_set_ca_state(newsk, TCP_CA_Open);
++=======
++>>>>>>> 81164413ad09 (net: tcp: add per route congestion control)
  		tcp_init_xmit_timers(newsk);
  		__skb_queue_head_init(&newtp->out_of_order_queue);
  		newtp->write_seq = newtp->pushed_seq = treq->snt_isn + 1;
* Unmerged path include/net/tcp.h
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index 10efd403b538..d1f2c012f7ac 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -1663,6 +1663,8 @@ struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 	}
 	sk_setup_caps(newsk, dst);
 
+	tcp_ca_openreq_child(newsk, dst);
+
 	tcp_sync_mss(newsk, dst_mtu(dst));
 	newtp->advmss = dst_metric_advmss(dst);
 	if (tcp_sk(sk)->rx_opt.user_mss &&
* Unmerged path net/ipv4/tcp_minisocks.c
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index 5018c0817473..c5af56c29006 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -2831,6 +2831,25 @@ struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 }
 EXPORT_SYMBOL(tcp_make_synack);
 
+static void tcp_ca_dst_init(struct sock *sk, const struct dst_entry *dst)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	const struct tcp_congestion_ops *ca;
+	u32 ca_key = dst_metric(dst, RTAX_CC_ALGO);
+
+	if (ca_key == TCP_CA_UNSPEC)
+		return;
+
+	rcu_read_lock();
+	ca = tcp_ca_find_key(ca_key);
+	if (likely(ca && try_module_get(ca->owner))) {
+		module_put(icsk->icsk_ca_ops->owner);
+		icsk->icsk_ca_dst_locked = tcp_ca_dst_locked(dst);
+		icsk->icsk_ca_ops = ca;
+	}
+	rcu_read_unlock();
+}
+
 /* Do all connect socket setups that can be done AF independent. */
 void tcp_connect_init(struct sock *sk)
 {
@@ -2856,6 +2875,8 @@ void tcp_connect_init(struct sock *sk)
 	tcp_mtup_init(sk);
 	tcp_sync_mss(sk, dst_mtu(dst));
 
+	tcp_ca_dst_init(sk, dst);
+
 	if (!tp->window_clamp)
 		tp->window_clamp = dst_metric(dst, RTAX_WINDOW);
 	tp->advmss = dst_metric_advmss(dst);
diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c
index 715cc96a3475..775df0cdbe59 100644
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@ -1259,6 +1259,8 @@ static struct sock *tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 		inet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +
 						     newnp->opt->opt_flen);
 
+	tcp_ca_openreq_child(newsk, dst);
+
 	tcp_sync_mss(newsk, dst_mtu(dst));
 	newtp->advmss = dst_metric_advmss(dst);
 	if (tcp_sk(sk)->rx_opt.user_mss &&
