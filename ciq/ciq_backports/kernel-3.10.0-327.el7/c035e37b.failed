powerpc/powernv/ioda2: Use new helpers to do proper cleanup on PE release

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] powernv/ioda2: Use new helpers to do proper cleanup on PE release (David Gibson) [1213665]
Rebuild_FUZZ: 94.20%
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit c035e37b58c75ca216bfd1d5de3c1080ac0022b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/c035e37b.failed

The existing code programmed TVT#0 with some address and then
immediately released that memory.

This makes use of pnv_pci_ioda2_unset_window() and
pnv_pci_ioda2_set_bypass() which do correct resource release and
TVT update.

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit c035e37b58c75ca216bfd1d5de3c1080ac0022b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/powernv/pci-ioda.c
diff --cc arch/powerpc/platforms/powernv/pci-ioda.c
index 109e90d84e34,510bb9c8a9e1..000000000000
--- a/arch/powerpc/platforms/powernv/pci-ioda.c
+++ b/arch/powerpc/platforms/powernv/pci-ioda.c
@@@ -522,6 -1153,428 +522,431 @@@ static void pnv_pci_ioda_setup_PEs(void
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_PCI_IOV
+ static int pnv_pci_vf_release_m64(struct pci_dev *pdev)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	int                    i, j;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	for (i = 0; i < PCI_SRIOV_NUM_BARS; i++)
+ 		for (j = 0; j < M64_PER_IOV; j++) {
+ 			if (pdn->m64_wins[i][j] == IODA_INVALID_M64)
+ 				continue;
+ 			opal_pci_phb_mmio_enable(phb->opal_id,
+ 				OPAL_M64_WINDOW_TYPE, pdn->m64_wins[i][j], 0);
+ 			clear_bit(pdn->m64_wins[i][j], &phb->ioda.m64_bar_alloc);
+ 			pdn->m64_wins[i][j] = IODA_INVALID_M64;
+ 		}
+ 
+ 	return 0;
+ }
+ 
+ static int pnv_pci_vf_assign_m64(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	unsigned int           win;
+ 	struct resource       *res;
+ 	int                    i, j;
+ 	int64_t                rc;
+ 	int                    total_vfs;
+ 	resource_size_t        size, start;
+ 	int                    pe_num;
+ 	int                    vf_groups;
+ 	int                    vf_per_group;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 	total_vfs = pci_sriov_get_totalvfs(pdev);
+ 
+ 	/* Initialize the m64_wins to IODA_INVALID_M64 */
+ 	for (i = 0; i < PCI_SRIOV_NUM_BARS; i++)
+ 		for (j = 0; j < M64_PER_IOV; j++)
+ 			pdn->m64_wins[i][j] = IODA_INVALID_M64;
+ 
+ 	if (pdn->m64_per_iov == M64_PER_IOV) {
+ 		vf_groups = (num_vfs <= M64_PER_IOV) ? num_vfs: M64_PER_IOV;
+ 		vf_per_group = (num_vfs <= M64_PER_IOV)? 1:
+ 			roundup_pow_of_two(num_vfs) / pdn->m64_per_iov;
+ 	} else {
+ 		vf_groups = 1;
+ 		vf_per_group = 1;
+ 	}
+ 
+ 	for (i = 0; i < PCI_SRIOV_NUM_BARS; i++) {
+ 		res = &pdev->resource[i + PCI_IOV_RESOURCES];
+ 		if (!res->flags || !res->parent)
+ 			continue;
+ 
+ 		if (!pnv_pci_is_mem_pref_64(res->flags))
+ 			continue;
+ 
+ 		for (j = 0; j < vf_groups; j++) {
+ 			do {
+ 				win = find_next_zero_bit(&phb->ioda.m64_bar_alloc,
+ 						phb->ioda.m64_bar_idx + 1, 0);
+ 
+ 				if (win >= phb->ioda.m64_bar_idx + 1)
+ 					goto m64_failed;
+ 			} while (test_and_set_bit(win, &phb->ioda.m64_bar_alloc));
+ 
+ 			pdn->m64_wins[i][j] = win;
+ 
+ 			if (pdn->m64_per_iov == M64_PER_IOV) {
+ 				size = pci_iov_resource_size(pdev,
+ 							PCI_IOV_RESOURCES + i);
+ 				size = size * vf_per_group;
+ 				start = res->start + size * j;
+ 			} else {
+ 				size = resource_size(res);
+ 				start = res->start;
+ 			}
+ 
+ 			/* Map the M64 here */
+ 			if (pdn->m64_per_iov == M64_PER_IOV) {
+ 				pe_num = pdn->offset + j;
+ 				rc = opal_pci_map_pe_mmio_window(phb->opal_id,
+ 						pe_num, OPAL_M64_WINDOW_TYPE,
+ 						pdn->m64_wins[i][j], 0);
+ 			}
+ 
+ 			rc = opal_pci_set_phb_mem_window(phb->opal_id,
+ 						 OPAL_M64_WINDOW_TYPE,
+ 						 pdn->m64_wins[i][j],
+ 						 start,
+ 						 0, /* unused */
+ 						 size);
+ 
+ 
+ 			if (rc != OPAL_SUCCESS) {
+ 				dev_err(&pdev->dev, "Failed to map M64 window #%d: %lld\n",
+ 					win, rc);
+ 				goto m64_failed;
+ 			}
+ 
+ 			if (pdn->m64_per_iov == M64_PER_IOV)
+ 				rc = opal_pci_phb_mmio_enable(phb->opal_id,
+ 				     OPAL_M64_WINDOW_TYPE, pdn->m64_wins[i][j], 2);
+ 			else
+ 				rc = opal_pci_phb_mmio_enable(phb->opal_id,
+ 				     OPAL_M64_WINDOW_TYPE, pdn->m64_wins[i][j], 1);
+ 
+ 			if (rc != OPAL_SUCCESS) {
+ 				dev_err(&pdev->dev, "Failed to enable M64 window #%d: %llx\n",
+ 					win, rc);
+ 				goto m64_failed;
+ 			}
+ 		}
+ 	}
+ 	return 0;
+ 
+ m64_failed:
+ 	pnv_pci_vf_release_m64(pdev);
+ 	return -EBUSY;
+ }
+ 
+ static long pnv_pci_ioda2_unset_window(struct iommu_table_group *table_group,
+ 		int num);
+ static void pnv_pci_ioda2_set_bypass(struct pnv_ioda_pe *pe, bool enable);
+ 
+ static void pnv_pci_ioda2_release_dma_pe(struct pci_dev *dev, struct pnv_ioda_pe *pe)
+ {
+ 	struct iommu_table    *tbl;
+ 	int64_t               rc;
+ 
+ 	tbl = pe->table_group.tables[0];
+ 	rc = pnv_pci_ioda2_unset_window(&pe->table_group, 0);
+ 	if (rc)
+ 		pe_warn(pe, "OPAL error %ld release DMA window\n", rc);
+ 
+ 	pnv_pci_ioda2_set_bypass(pe, false);
+ 	if (pe->table_group.group) {
+ 		iommu_group_put(pe->table_group.group);
+ 		BUG_ON(pe->table_group.group);
+ 	}
+ 	pnv_pci_ioda2_table_free_pages(tbl);
+ 	iommu_free_table(tbl, of_node_full_name(dev->dev.of_node));
+ }
+ 
+ static void pnv_ioda_release_vf_PE(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pnv_ioda_pe    *pe, *pe_n;
+ 	struct pci_dn         *pdn;
+ 	u16                    vf_index;
+ 	int64_t                rc;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	if (!pdev->is_physfn)
+ 		return;
+ 
+ 	if (pdn->m64_per_iov == M64_PER_IOV && num_vfs > M64_PER_IOV) {
+ 		int   vf_group;
+ 		int   vf_per_group;
+ 		int   vf_index1;
+ 
+ 		vf_per_group = roundup_pow_of_two(num_vfs) / pdn->m64_per_iov;
+ 
+ 		for (vf_group = 0; vf_group < M64_PER_IOV; vf_group++)
+ 			for (vf_index = vf_group * vf_per_group;
+ 				vf_index < (vf_group + 1) * vf_per_group &&
+ 				vf_index < num_vfs;
+ 				vf_index++)
+ 				for (vf_index1 = vf_group * vf_per_group;
+ 					vf_index1 < (vf_group + 1) * vf_per_group &&
+ 					vf_index1 < num_vfs;
+ 					vf_index1++){
+ 
+ 					rc = opal_pci_set_peltv(phb->opal_id,
+ 						pdn->offset + vf_index,
+ 						pdn->offset + vf_index1,
+ 						OPAL_REMOVE_PE_FROM_DOMAIN);
+ 
+ 					if (rc)
+ 					    dev_warn(&pdev->dev, "%s: Failed to unlink same group PE#%d(%lld)\n",
+ 						__func__,
+ 						pdn->offset + vf_index1, rc);
+ 				}
+ 	}
+ 
+ 	list_for_each_entry_safe(pe, pe_n, &phb->ioda.pe_list, list) {
+ 		if (pe->parent_dev != pdev)
+ 			continue;
+ 
+ 		pnv_pci_ioda2_release_dma_pe(pdev, pe);
+ 
+ 		/* Remove from list */
+ 		mutex_lock(&phb->ioda.pe_list_mutex);
+ 		list_del(&pe->list);
+ 		mutex_unlock(&phb->ioda.pe_list_mutex);
+ 
+ 		pnv_ioda_deconfigure_pe(phb, pe);
+ 
+ 		pnv_ioda_free_pe(phb, pe->pe_number);
+ 	}
+ }
+ 
+ void pnv_pci_sriov_disable(struct pci_dev *pdev)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	struct pci_sriov      *iov;
+ 	u16 num_vfs;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 	iov = pdev->sriov;
+ 	num_vfs = pdn->num_vfs;
+ 
+ 	/* Release VF PEs */
+ 	pnv_ioda_release_vf_PE(pdev, num_vfs);
+ 
+ 	if (phb->type == PNV_PHB_IODA2) {
+ 		if (pdn->m64_per_iov == 1)
+ 			pnv_pci_vf_resource_shift(pdev, -pdn->offset);
+ 
+ 		/* Release M64 windows */
+ 		pnv_pci_vf_release_m64(pdev);
+ 
+ 		/* Release PE numbers */
+ 		bitmap_clear(phb->ioda.pe_alloc, pdn->offset, num_vfs);
+ 		pdn->offset = 0;
+ 	}
+ }
+ 
+ static void pnv_pci_ioda2_setup_dma_pe(struct pnv_phb *phb,
+ 				       struct pnv_ioda_pe *pe);
+ static void pnv_ioda_setup_vf_PE(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pnv_ioda_pe    *pe;
+ 	int                    pe_num;
+ 	u16                    vf_index;
+ 	struct pci_dn         *pdn;
+ 	int64_t                rc;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	if (!pdev->is_physfn)
+ 		return;
+ 
+ 	/* Reserve PE for each VF */
+ 	for (vf_index = 0; vf_index < num_vfs; vf_index++) {
+ 		pe_num = pdn->offset + vf_index;
+ 
+ 		pe = &phb->ioda.pe_array[pe_num];
+ 		pe->pe_number = pe_num;
+ 		pe->phb = phb;
+ 		pe->flags = PNV_IODA_PE_VF;
+ 		pe->pbus = NULL;
+ 		pe->parent_dev = pdev;
+ 		pe->tce32_seg = -1;
+ 		pe->mve_number = -1;
+ 		pe->rid = (pci_iov_virtfn_bus(pdev, vf_index) << 8) |
+ 			   pci_iov_virtfn_devfn(pdev, vf_index);
+ 
+ 		pe_info(pe, "VF %04d:%02d:%02d.%d associated with PE#%d\n",
+ 			hose->global_number, pdev->bus->number,
+ 			PCI_SLOT(pci_iov_virtfn_devfn(pdev, vf_index)),
+ 			PCI_FUNC(pci_iov_virtfn_devfn(pdev, vf_index)), pe_num);
+ 
+ 		if (pnv_ioda_configure_pe(phb, pe)) {
+ 			/* XXX What do we do here ? */
+ 			if (pe_num)
+ 				pnv_ioda_free_pe(phb, pe_num);
+ 			pe->pdev = NULL;
+ 			continue;
+ 		}
+ 
+ 		/* Put PE to the list */
+ 		mutex_lock(&phb->ioda.pe_list_mutex);
+ 		list_add_tail(&pe->list, &phb->ioda.pe_list);
+ 		mutex_unlock(&phb->ioda.pe_list_mutex);
+ 
+ 		pnv_pci_ioda2_setup_dma_pe(phb, pe);
+ 	}
+ 
+ 	if (pdn->m64_per_iov == M64_PER_IOV && num_vfs > M64_PER_IOV) {
+ 		int   vf_group;
+ 		int   vf_per_group;
+ 		int   vf_index1;
+ 
+ 		vf_per_group = roundup_pow_of_two(num_vfs) / pdn->m64_per_iov;
+ 
+ 		for (vf_group = 0; vf_group < M64_PER_IOV; vf_group++) {
+ 			for (vf_index = vf_group * vf_per_group;
+ 			     vf_index < (vf_group + 1) * vf_per_group &&
+ 			     vf_index < num_vfs;
+ 			     vf_index++) {
+ 				for (vf_index1 = vf_group * vf_per_group;
+ 				     vf_index1 < (vf_group + 1) * vf_per_group &&
+ 				     vf_index1 < num_vfs;
+ 				     vf_index1++) {
+ 
+ 					rc = opal_pci_set_peltv(phb->opal_id,
+ 						pdn->offset + vf_index,
+ 						pdn->offset + vf_index1,
+ 						OPAL_ADD_PE_TO_DOMAIN);
+ 
+ 					if (rc)
+ 					    dev_warn(&pdev->dev, "%s: Failed to link same group PE#%d(%lld)\n",
+ 						__func__,
+ 						pdn->offset + vf_index1, rc);
+ 				}
+ 			}
+ 		}
+ 	}
+ }
+ 
+ int pnv_pci_sriov_enable(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	struct pci_bus        *bus;
+ 	struct pci_controller *hose;
+ 	struct pnv_phb        *phb;
+ 	struct pci_dn         *pdn;
+ 	int                    ret;
+ 
+ 	bus = pdev->bus;
+ 	hose = pci_bus_to_host(bus);
+ 	phb = hose->private_data;
+ 	pdn = pci_get_pdn(pdev);
+ 
+ 	if (phb->type == PNV_PHB_IODA2) {
+ 		/* Calculate available PE for required VFs */
+ 		mutex_lock(&phb->ioda.pe_alloc_mutex);
+ 		pdn->offset = bitmap_find_next_zero_area(
+ 			phb->ioda.pe_alloc, phb->ioda.total_pe,
+ 			0, num_vfs, 0);
+ 		if (pdn->offset >= phb->ioda.total_pe) {
+ 			mutex_unlock(&phb->ioda.pe_alloc_mutex);
+ 			dev_info(&pdev->dev, "Failed to enable VF%d\n", num_vfs);
+ 			pdn->offset = 0;
+ 			return -EBUSY;
+ 		}
+ 		bitmap_set(phb->ioda.pe_alloc, pdn->offset, num_vfs);
+ 		pdn->num_vfs = num_vfs;
+ 		mutex_unlock(&phb->ioda.pe_alloc_mutex);
+ 
+ 		/* Assign M64 window accordingly */
+ 		ret = pnv_pci_vf_assign_m64(pdev, num_vfs);
+ 		if (ret) {
+ 			dev_info(&pdev->dev, "Not enough M64 window resources\n");
+ 			goto m64_failed;
+ 		}
+ 
+ 		/*
+ 		 * When using one M64 BAR to map one IOV BAR, we need to shift
+ 		 * the IOV BAR according to the PE# allocated to the VFs.
+ 		 * Otherwise, the PE# for the VF will conflict with others.
+ 		 */
+ 		if (pdn->m64_per_iov == 1) {
+ 			ret = pnv_pci_vf_resource_shift(pdev, pdn->offset);
+ 			if (ret)
+ 				goto m64_failed;
+ 		}
+ 	}
+ 
+ 	/* Setup VF PEs */
+ 	pnv_ioda_setup_vf_PE(pdev, num_vfs);
+ 
+ 	return 0;
+ 
+ m64_failed:
+ 	bitmap_clear(phb->ioda.pe_alloc, pdn->offset, num_vfs);
+ 	pdn->offset = 0;
+ 
+ 	return ret;
+ }
+ 
+ int pcibios_sriov_disable(struct pci_dev *pdev)
+ {
+ 	pnv_pci_sriov_disable(pdev);
+ 
+ 	/* Release PCI data */
+ 	remove_dev_pci_data(pdev);
+ 	return 0;
+ }
+ 
+ int pcibios_sriov_enable(struct pci_dev *pdev, u16 num_vfs)
+ {
+ 	/* Allocate PCI data */
+ 	add_dev_pci_data(pdev);
+ 
+ 	pnv_pci_sriov_enable(pdev, num_vfs);
+ 	return 0;
+ }
+ #endif /* CONFIG_PCI_IOV */
+ 
++>>>>>>> c035e37b58c7 (powerpc/powernv/ioda2: Use new helpers to do proper cleanup on PE release)
  static void pnv_pci_ioda_dma_dev_setup(struct pnv_phb *phb, struct pci_dev *pdev)
  {
  	struct pci_dn *pdn = pci_get_pdn(pdev);
* Unmerged path arch/powerpc/platforms/powernv/pci-ioda.c
