netns: make nsid_lock per net

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author WANG Cong <xiyou.wangcong@gmail.com>
commit de133464c9e70808d3e5a861294bc55940988178
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/de133464.failed

The spinlock is used to protect netns_ids which is per net,
so there is no need to use a global spinlock.

	Cc: Nicolas Dichtel <nicolas.dichtel@6wind.com>
	Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
	Acked-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit de133464c9e70808d3e5a861294bc55940988178)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/net_namespace.h
#	net/core/net_namespace.c
diff --cc include/net/net_namespace.h
index 385ac027c602,72eb23723294..000000000000
--- a/include/net/net_namespace.h
+++ b/include/net/net_namespace.h
@@@ -59,8 -58,10 +59,13 @@@ struct net 
  	struct list_head	exit_list;	/* Use only net_mutex */
  
  	struct user_namespace   *user_ns;	/* Owning user namespace */
++<<<<<<< HEAD
++=======
+ 	spinlock_t		nsid_lock;
+ 	struct idr		netns_ids;
++>>>>>>> de133464c9e7 (netns: make nsid_lock per net)
  
 -	struct ns_common	ns;
 +	unsigned int		proc_inum;
  
  	struct proc_dir_entry 	*proc_net;
  	struct proc_dir_entry 	*proc_net_stat;
diff --cc net/core/net_namespace.c
index 4cf6699528f0,2c2eb1b629b1..000000000000
--- a/net/core/net_namespace.c
+++ b/net/core/net_namespace.c
@@@ -144,6 -147,125 +144,128 @@@ static void ops_free_list(const struct 
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /* should be called with nsid_lock held */
+ static int alloc_netid(struct net *net, struct net *peer, int reqid)
+ {
+ 	int min = 0, max = 0;
+ 
+ 	if (reqid >= 0) {
+ 		min = reqid;
+ 		max = reqid + 1;
+ 	}
+ 
+ 	return idr_alloc(&net->netns_ids, peer, min, max, GFP_ATOMIC);
+ }
+ 
+ /* This function is used by idr_for_each(). If net is equal to peer, the
+  * function returns the id so that idr_for_each() stops. Because we cannot
+  * returns the id 0 (idr_for_each() will not stop), we return the magic value
+  * NET_ID_ZERO (-1) for it.
+  */
+ #define NET_ID_ZERO -1
+ static int net_eq_idr(int id, void *net, void *peer)
+ {
+ 	if (net_eq(net, peer))
+ 		return id ? : NET_ID_ZERO;
+ 	return 0;
+ }
+ 
+ /* Should be called with nsid_lock held. If a new id is assigned, the bool alloc
+  * is set to true, thus the caller knows that the new id must be notified via
+  * rtnl.
+  */
+ static int __peernet2id_alloc(struct net *net, struct net *peer, bool *alloc)
+ {
+ 	int id = idr_for_each(&net->netns_ids, net_eq_idr, peer);
+ 	bool alloc_it = *alloc;
+ 
+ 	*alloc = false;
+ 
+ 	/* Magic value for id 0. */
+ 	if (id == NET_ID_ZERO)
+ 		return 0;
+ 	if (id > 0)
+ 		return id;
+ 
+ 	if (alloc_it) {
+ 		id = alloc_netid(net, peer, -1);
+ 		*alloc = true;
+ 		return id >= 0 ? id : NETNSA_NSID_NOT_ASSIGNED;
+ 	}
+ 
+ 	return NETNSA_NSID_NOT_ASSIGNED;
+ }
+ 
+ /* should be called with nsid_lock held */
+ static int __peernet2id(struct net *net, struct net *peer)
+ {
+ 	bool no = false;
+ 
+ 	return __peernet2id_alloc(net, peer, &no);
+ }
+ 
+ static void rtnl_net_notifyid(struct net *net, int cmd, int id);
+ /* This function returns the id of a peer netns. If no id is assigned, one will
+  * be allocated and returned.
+  */
+ int peernet2id_alloc(struct net *net, struct net *peer)
+ {
+ 	unsigned long flags;
+ 	bool alloc;
+ 	int id;
+ 
+ 	spin_lock_irqsave(&net->nsid_lock, flags);
+ 	alloc = atomic_read(&peer->count) == 0 ? false : true;
+ 	id = __peernet2id_alloc(net, peer, &alloc);
+ 	spin_unlock_irqrestore(&net->nsid_lock, flags);
+ 	if (alloc && id >= 0)
+ 		rtnl_net_notifyid(net, RTM_NEWNSID, id);
+ 	return id;
+ }
+ EXPORT_SYMBOL(peernet2id_alloc);
+ 
+ /* This function returns, if assigned, the id of a peer netns. */
+ int peernet2id(struct net *net, struct net *peer)
+ {
+ 	unsigned long flags;
+ 	int id;
+ 
+ 	spin_lock_irqsave(&net->nsid_lock, flags);
+ 	id = __peernet2id(net, peer);
+ 	spin_unlock_irqrestore(&net->nsid_lock, flags);
+ 	return id;
+ }
+ 
+ /* This function returns true is the peer netns has an id assigned into the
+  * current netns.
+  */
+ bool peernet_has_id(struct net *net, struct net *peer)
+ {
+ 	return peernet2id(net, peer) >= 0;
+ }
+ 
+ struct net *get_net_ns_by_id(struct net *net, int id)
+ {
+ 	unsigned long flags;
+ 	struct net *peer;
+ 
+ 	if (id < 0)
+ 		return NULL;
+ 
+ 	rcu_read_lock();
+ 	spin_lock_irqsave(&net->nsid_lock, flags);
+ 	peer = idr_find(&net->netns_ids, id);
+ 	if (peer)
+ 		get_net(peer);
+ 	spin_unlock_irqrestore(&net->nsid_lock, flags);
+ 	rcu_read_unlock();
+ 
+ 	return peer;
+ }
+ 
++>>>>>>> de133464c9e7 (netns: make nsid_lock per net)
  /*
   * setup_net runs the initializers for the network namespace object.
   */
@@@ -158,10 -280,8 +280,15 @@@ static __net_init int setup_net(struct 
  	atomic_set(&net->passive, 1);
  	net->dev_base_seq = 1;
  	net->user_ns = user_ns;
++<<<<<<< HEAD
 +
 +#ifdef NETNS_REFCNT_DEBUG
 +	atomic_set(&net->use_count, 0);
 +#endif
++=======
+ 	idr_init(&net->netns_ids);
+ 	spin_lock_init(&net->nsid_lock);
++>>>>>>> de133464c9e7 (netns: make nsid_lock per net)
  
  	list_for_each_entry(ops, &pernet_list, list) {
  		error = ops_init(ops, net);
@@@ -288,6 -401,21 +415,24 @@@ static void cleanup_net(struct work_str
  	list_for_each_entry(net, &net_kill_list, cleanup_list) {
  		list_del_rcu(&net->list);
  		list_add_tail(&net->exit_list, &net_exit_list);
++<<<<<<< HEAD
++=======
+ 		for_each_net(tmp) {
+ 			int id;
+ 
+ 			spin_lock_irq(&tmp->nsid_lock);
+ 			id = __peernet2id(tmp, net);
+ 			if (id >= 0)
+ 				idr_remove(&tmp->netns_ids, id);
+ 			spin_unlock_irq(&tmp->nsid_lock);
+ 			if (id >= 0)
+ 				rtnl_net_notifyid(tmp, RTM_DELNSID, id);
+ 		}
+ 		spin_lock_irq(&net->nsid_lock);
+ 		idr_destroy(&net->netns_ids);
+ 		spin_unlock_irq(&net->nsid_lock);
+ 
++>>>>>>> de133464c9e7 (netns: make nsid_lock per net)
  	}
  	rtnl_unlock();
  
@@@ -397,6 -531,200 +542,203 @@@ static struct pernet_operations __net_i
  	.exit = net_ns_net_exit,
  };
  
++<<<<<<< HEAD
++=======
+ static struct nla_policy rtnl_net_policy[NETNSA_MAX + 1] = {
+ 	[NETNSA_NONE]		= { .type = NLA_UNSPEC },
+ 	[NETNSA_NSID]		= { .type = NLA_S32 },
+ 	[NETNSA_PID]		= { .type = NLA_U32 },
+ 	[NETNSA_FD]		= { .type = NLA_U32 },
+ };
+ 
+ static int rtnl_net_newid(struct sk_buff *skb, struct nlmsghdr *nlh)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct nlattr *tb[NETNSA_MAX + 1];
+ 	unsigned long flags;
+ 	struct net *peer;
+ 	int nsid, err;
+ 
+ 	err = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,
+ 			  rtnl_net_policy);
+ 	if (err < 0)
+ 		return err;
+ 	if (!tb[NETNSA_NSID])
+ 		return -EINVAL;
+ 	nsid = nla_get_s32(tb[NETNSA_NSID]);
+ 
+ 	if (tb[NETNSA_PID])
+ 		peer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));
+ 	else if (tb[NETNSA_FD])
+ 		peer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));
+ 	else
+ 		return -EINVAL;
+ 	if (IS_ERR(peer))
+ 		return PTR_ERR(peer);
+ 
+ 	spin_lock_irqsave(&net->nsid_lock, flags);
+ 	if (__peernet2id(net, peer) >= 0) {
+ 		spin_unlock_irqrestore(&net->nsid_lock, flags);
+ 		err = -EEXIST;
+ 		goto out;
+ 	}
+ 
+ 	err = alloc_netid(net, peer, nsid);
+ 	spin_unlock_irqrestore(&net->nsid_lock, flags);
+ 	if (err >= 0) {
+ 		rtnl_net_notifyid(net, RTM_NEWNSID, err);
+ 		err = 0;
+ 	}
+ out:
+ 	put_net(peer);
+ 	return err;
+ }
+ 
+ static int rtnl_net_get_size(void)
+ {
+ 	return NLMSG_ALIGN(sizeof(struct rtgenmsg))
+ 	       + nla_total_size(sizeof(s32)) /* NETNSA_NSID */
+ 	       ;
+ }
+ 
+ static int rtnl_net_fill(struct sk_buff *skb, u32 portid, u32 seq, int flags,
+ 			 int cmd, struct net *net, int nsid)
+ {
+ 	struct nlmsghdr *nlh;
+ 	struct rtgenmsg *rth;
+ 
+ 	nlh = nlmsg_put(skb, portid, seq, cmd, sizeof(*rth), flags);
+ 	if (!nlh)
+ 		return -EMSGSIZE;
+ 
+ 	rth = nlmsg_data(nlh);
+ 	rth->rtgen_family = AF_UNSPEC;
+ 
+ 	if (nla_put_s32(skb, NETNSA_NSID, nsid))
+ 		goto nla_put_failure;
+ 
+ 	nlmsg_end(skb, nlh);
+ 	return 0;
+ 
+ nla_put_failure:
+ 	nlmsg_cancel(skb, nlh);
+ 	return -EMSGSIZE;
+ }
+ 
+ static int rtnl_net_getid(struct sk_buff *skb, struct nlmsghdr *nlh)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct nlattr *tb[NETNSA_MAX + 1];
+ 	struct sk_buff *msg;
+ 	struct net *peer;
+ 	int err, id;
+ 
+ 	err = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,
+ 			  rtnl_net_policy);
+ 	if (err < 0)
+ 		return err;
+ 	if (tb[NETNSA_PID])
+ 		peer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));
+ 	else if (tb[NETNSA_FD])
+ 		peer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));
+ 	else
+ 		return -EINVAL;
+ 
+ 	if (IS_ERR(peer))
+ 		return PTR_ERR(peer);
+ 
+ 	msg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);
+ 	if (!msg) {
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	id = peernet2id(net, peer);
+ 	err = rtnl_net_fill(msg, NETLINK_CB(skb).portid, nlh->nlmsg_seq, 0,
+ 			    RTM_NEWNSID, net, id);
+ 	if (err < 0)
+ 		goto err_out;
+ 
+ 	err = rtnl_unicast(msg, net, NETLINK_CB(skb).portid);
+ 	goto out;
+ 
+ err_out:
+ 	nlmsg_free(msg);
+ out:
+ 	put_net(peer);
+ 	return err;
+ }
+ 
+ struct rtnl_net_dump_cb {
+ 	struct net *net;
+ 	struct sk_buff *skb;
+ 	struct netlink_callback *cb;
+ 	int idx;
+ 	int s_idx;
+ };
+ 
+ static int rtnl_net_dumpid_one(int id, void *peer, void *data)
+ {
+ 	struct rtnl_net_dump_cb *net_cb = (struct rtnl_net_dump_cb *)data;
+ 	int ret;
+ 
+ 	if (net_cb->idx < net_cb->s_idx)
+ 		goto cont;
+ 
+ 	ret = rtnl_net_fill(net_cb->skb, NETLINK_CB(net_cb->cb->skb).portid,
+ 			    net_cb->cb->nlh->nlmsg_seq, NLM_F_MULTI,
+ 			    RTM_NEWNSID, net_cb->net, id);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ cont:
+ 	net_cb->idx++;
+ 	return 0;
+ }
+ 
+ static int rtnl_net_dumpid(struct sk_buff *skb, struct netlink_callback *cb)
+ {
+ 	struct net *net = sock_net(skb->sk);
+ 	struct rtnl_net_dump_cb net_cb = {
+ 		.net = net,
+ 		.skb = skb,
+ 		.cb = cb,
+ 		.idx = 0,
+ 		.s_idx = cb->args[0],
+ 	};
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&net->nsid_lock, flags);
+ 	idr_for_each(&net->netns_ids, rtnl_net_dumpid_one, &net_cb);
+ 	spin_unlock_irqrestore(&net->nsid_lock, flags);
+ 
+ 	cb->args[0] = net_cb.idx;
+ 	return skb->len;
+ }
+ 
+ static void rtnl_net_notifyid(struct net *net, int cmd, int id)
+ {
+ 	struct sk_buff *msg;
+ 	int err = -ENOMEM;
+ 
+ 	msg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);
+ 	if (!msg)
+ 		goto out;
+ 
+ 	err = rtnl_net_fill(msg, 0, 0, 0, cmd, net, id);
+ 	if (err < 0)
+ 		goto err_out;
+ 
+ 	rtnl_notify(msg, net, 0, RTNLGRP_NSID, NULL, 0);
+ 	return;
+ 
+ err_out:
+ 	nlmsg_free(msg);
+ out:
+ 	rtnl_set_sk_err(net, RTNLGRP_NSID, err);
+ }
+ 
++>>>>>>> de133464c9e7 (netns: make nsid_lock per net)
  static int __init net_ns_init(void)
  {
  	struct net_generic *ng;
* Unmerged path include/net/net_namespace.h
* Unmerged path net/core/net_namespace.c
