mm: numa: do not trap faults on the huge zero page

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [mm] numa: do not trap faults on the huge zero page (Gustavo Duarte) [1217743]
Rebuild_FUZZ: 95.83%
commit-author Mel Gorman <mgorman@suse.de>
commit e944fd67b625c02bda4a78ddf85e413c5e401474
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/e944fd67.failed

Faults on the huge zero page are pointless and there is a BUG_ON to catch
them during fault time.  This patch reintroduces a check that avoids
marking the zero page PAGE_NONE.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Dave Jones <davej@redhat.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Kirill Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e944fd67b625c02bda4a78ddf85e413c5e401474)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mprotect.c
diff --cc mm/mprotect.c
index a182e560297e,dd599fc235c2..000000000000
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@@ -82,35 -75,31 +82,52 @@@ static unsigned long change_pte_range(s
  		oldpte = *pte;
  		if (pte_present(oldpte)) {
  			pte_t ptent;
 -
 +			bool updated = false;
 +
++<<<<<<< HEAD
 +			if (!prot_numa) {
 +				ptent = ptep_modify_prot_start(mm, addr, pte);
 +				if (pte_numa(ptent))
 +					ptent = pte_mknonnuma(ptent);
 +				ptent = pte_modify(ptent, newprot);
 +				/*
 +				 * Avoid taking write faults for pages we
 +				 * know to be dirty.
 +				 */
 +				if (dirty_accountable && pte_dirty(ptent))
 +					ptent = pte_mkwrite(ptent);
 +				ptep_modify_prot_commit(mm, addr, pte, ptent);
 +				updated = true;
 +			} else {
 +				struct page *page;
++=======
+ 			/*
+ 			 * Avoid trapping faults against the zero or KSM
+ 			 * pages. See similar comment in change_huge_pmd.
+ 			 */
+ 			if (prot_numa) {
+ 				struct page *page;
+ 
+ 				page = vm_normal_page(vma, addr, oldpte);
+ 				if (!page || PageKsm(page))
+ 					continue;
+ 			}
+ 
+ 			ptent = ptep_modify_prot_start(mm, addr, pte);
+ 			ptent = pte_modify(ptent, newprot);
++>>>>>>> e944fd67b625 (mm: numa: do not trap faults on the huge zero page)
  
 -			/* Avoid taking write faults for known dirty pages */
 -			if (dirty_accountable && pte_dirty(ptent) &&
 -					(pte_soft_dirty(ptent) ||
 -					 !(vma->vm_flags & VM_SOFTDIRTY))) {
 -				ptent = pte_mkwrite(ptent);
 +				page = vm_normal_page(vma, addr, oldpte);
 +				if (page && !PageKsm(page)) {
 +					if (!pte_numa(oldpte)) {
 +						ptep_set_numa(mm, addr, pte);
 +						updated = true;
 +					}
 +				}
  			}
 -			ptep_modify_prot_commit(mm, addr, pte, ptent);
 -			pages++;
 -		} else if (IS_ENABLED(CONFIG_MIGRATION)) {
 +			if (updated)
 +				pages++;
 +		} else if (IS_ENABLED(CONFIG_MIGRATION) && !pte_file(oldpte)) {
  			swp_entry_t entry = pte_to_swp_entry(oldpte);
  
  			if (is_write_migration_entry(entry)) {
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index dfc88290b1ac..78982c4f42b4 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1538,6 +1538,17 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 
 	if (__pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
 		pmd_t entry;
+
+		/*
+		 * Avoid trapping faults against the zero page. The read-only
+		 * data is likely to be read-cached on the local CPU and
+		 * local/remote hits to the zero page are not interesting.
+		 */
+		if (prot_numa && is_huge_zero_pmd(*pmd)) {
+			spin_unlock(ptl);
+			return 0;
+		}
+
 		ret = 1;
 		if (!prot_numa) {
 			entry = pmdp_get_and_clear(mm, addr, pmd);
diff --git a/mm/memory.c b/mm/memory.c
index 75338cb200d7..aa2086b78ff1 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3589,7 +3589,6 @@ int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		pte_unmap_unlock(ptep, ptl);
 		return 0;
 	}
-	BUG_ON(is_zero_pfn(page_to_pfn(page)));
 
 	/*
 	 * Avoid grouping on DSO/COW pages in specific and RO pages
* Unmerged path mm/mprotect.c
