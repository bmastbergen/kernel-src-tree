KVM: x86: use vcpu-specific functions to read/write/translate GFNs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [kvm] x86: use vcpu-specific functions to read/write/translate GFNs (Paolo Bonzini) [1202825]
Rebuild_FUZZ: 96.06%
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 54bf36aac520315385fe7623a5c3a698e993ceda
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/54bf36aa.failed

We need to hide SMRAM from guests not running in SMM.  Therefore,
all uses of kvm_read_guest* and kvm_write_guest* must be changed to
check whether the VCPU is in system management mode and use a
different set of memslots.  Switch from kvm_* to the newly-introduced
kvm_vcpu_*, which call into kvm_arch_vcpu_memslots_id.

	Reviewed-by: Radim Krčmář <rkrcmar@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 54bf36aac520315385fe7623a5c3a698e993ceda)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/mmu.c
index 46149ec9abf3,3814f483ac45..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -1245,19 -1289,65 +1245,70 @@@ void kvm_arch_mmu_write_protect_pt_mask
  	}
  }
  
++<<<<<<< HEAD
 +static bool rmap_write_protect(struct kvm *kvm, u64 gfn)
++=======
+ /**
+  * kvm_mmu_clear_dirty_pt_masked - clear MMU D-bit for PT level pages
+  * @kvm: kvm instance
+  * @slot: slot to clear D-bit
+  * @gfn_offset: start of the BITS_PER_LONG pages we care about
+  * @mask: indicates which pages we should clear D-bit
+  *
+  * Used for PML to re-log the dirty GPAs after userspace querying dirty_bitmap.
+  */
+ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
+ 				     struct kvm_memory_slot *slot,
+ 				     gfn_t gfn_offset, unsigned long mask)
+ {
+ 	unsigned long *rmapp;
+ 
+ 	while (mask) {
+ 		rmapp = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ 				      PT_PAGE_TABLE_LEVEL, slot);
+ 		__rmap_clear_dirty(kvm, rmapp);
+ 
+ 		/* clear the first set bit */
+ 		mask &= mask - 1;
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_clear_dirty_pt_masked);
+ 
+ /**
+  * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected
+  * PT level pages.
+  *
+  * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to
+  * enable dirty logging for them.
+  *
+  * Used when we do not need to care about huge page mappings: e.g. during dirty
+  * logging we do not have any such mappings.
+  */
+ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
+ 				struct kvm_memory_slot *slot,
+ 				gfn_t gfn_offset, unsigned long mask)
+ {
+ 	if (kvm_x86_ops->enable_log_dirty_pt_masked)
+ 		kvm_x86_ops->enable_log_dirty_pt_masked(kvm, slot, gfn_offset,
+ 				mask);
+ 	else
+ 		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+ }
+ 
+ static bool rmap_write_protect(struct kvm_vcpu *vcpu, u64 gfn)
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  {
  	struct kvm_memory_slot *slot;
  	unsigned long *rmapp;
  	int i;
  	bool write_protected = false;
  
- 	slot = gfn_to_memslot(kvm, gfn);
+ 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
  
 -	for (i = PT_PAGE_TABLE_LEVEL; i <= PT_MAX_HUGEPAGE_LEVEL; ++i) {
 +	for (i = PT_PAGE_TABLE_LEVEL;
 +	     i < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++i) {
  		rmapp = __gfn_to_rmap(gfn, i, slot);
- 		write_protected |= __rmap_write_protect(kvm, rmapp, true);
+ 		write_protected |= __rmap_write_protect(vcpu->kvm, rmapp, true);
  	}
  
  	return write_protected;
@@@ -3359,7 -3502,7 +3410,11 @@@ static int kvm_arch_setup_async_pf(stru
  	arch.direct_map = vcpu->arch.mmu.direct_map;
  	arch.cr3 = vcpu->arch.mmu.get_cr3(vcpu);
  
++<<<<<<< HEAD
 +	return kvm_setup_async_pf(vcpu, gva, gfn, &arch);
++=======
+ 	return kvm_setup_async_pf(vcpu, gva, kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  }
  
  static bool can_do_async_pf(struct kvm_vcpu *vcpu)
diff --cc arch/x86/kvm/vmx.c
index 85e82f921250,8c80b7d7343c..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -6984,10 -7323,10 +6984,15 @@@ static bool nested_vmx_exit_handled_io(
  		bitmap += (port & 0x7fff) / 8;
  
  		if (last_bitmap != bitmap)
++<<<<<<< HEAD
 +			if (kvm_read_guest(vcpu->kvm, bitmap, &b, 1))
 +				return 1;
++=======
+ 			if (kvm_vcpu_read_guest(vcpu, bitmap, &b, 1))
+ 				return true;
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  		if (b & (1 << (port & 7)))
 -			return true;
 +			return 1;
  
  		port++;
  		size--;
@@@ -7028,11 -7367,11 +7033,16 @@@ static bool nested_vmx_exit_handled_msr
  	/* Then read the msr_index'th bit from this bitmap: */
  	if (msr_index < 1024*8) {
  		unsigned char b;
++<<<<<<< HEAD
 +		if (kvm_read_guest(vcpu->kvm, bitmap + msr_index/8, &b, 1))
 +			return 1;
++=======
+ 		if (kvm_vcpu_read_guest(vcpu, bitmap + msr_index/8, &b, 1))
+ 			return true;
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  		return 1 & (b >> (msr_index & 7));
  	} else
 -		return true; /* let L1 handle the wrong parameter */
 +		return 1; /* let L1 handle the wrong parameter */
  }
  
  /*
@@@ -7247,6 -7598,241 +7257,244 @@@ static void vmx_get_exit_info(struct kv
  	*info2 = vmcs_read32(VM_EXIT_INTR_INFO);
  }
  
++<<<<<<< HEAD
++=======
+ static int vmx_enable_pml(struct vcpu_vmx *vmx)
+ {
+ 	struct page *pml_pg;
+ 	u32 exec_control;
+ 
+ 	pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);
+ 	if (!pml_pg)
+ 		return -ENOMEM;
+ 
+ 	vmx->pml_pg = pml_pg;
+ 
+ 	vmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));
+ 	vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
+ 
+ 	exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
+ 	exec_control |= SECONDARY_EXEC_ENABLE_PML;
+ 	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);
+ 
+ 	return 0;
+ }
+ 
+ static void vmx_disable_pml(struct vcpu_vmx *vmx)
+ {
+ 	u32 exec_control;
+ 
+ 	ASSERT(vmx->pml_pg);
+ 	__free_page(vmx->pml_pg);
+ 	vmx->pml_pg = NULL;
+ 
+ 	exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
+ 	exec_control &= ~SECONDARY_EXEC_ENABLE_PML;
+ 	vmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);
+ }
+ 
+ static void vmx_flush_pml_buffer(struct kvm_vcpu *vcpu)
+ {
+ 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+ 	u64 *pml_buf;
+ 	u16 pml_idx;
+ 
+ 	pml_idx = vmcs_read16(GUEST_PML_INDEX);
+ 
+ 	/* Do nothing if PML buffer is empty */
+ 	if (pml_idx == (PML_ENTITY_NUM - 1))
+ 		return;
+ 
+ 	/* PML index always points to next available PML buffer entity */
+ 	if (pml_idx >= PML_ENTITY_NUM)
+ 		pml_idx = 0;
+ 	else
+ 		pml_idx++;
+ 
+ 	pml_buf = page_address(vmx->pml_pg);
+ 	for (; pml_idx < PML_ENTITY_NUM; pml_idx++) {
+ 		u64 gpa;
+ 
+ 		gpa = pml_buf[pml_idx];
+ 		WARN_ON(gpa & (PAGE_SIZE - 1));
+ 		kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ 	}
+ 
+ 	/* reset PML index */
+ 	vmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);
+ }
+ 
+ /*
+  * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.
+  * Called before reporting dirty_bitmap to userspace.
+  */
+ static void kvm_flush_pml_buffers(struct kvm *kvm)
+ {
+ 	int i;
+ 	struct kvm_vcpu *vcpu;
+ 	/*
+ 	 * We only need to kick vcpu out of guest mode here, as PML buffer
+ 	 * is flushed at beginning of all VMEXITs, and it's obvious that only
+ 	 * vcpus running in guest are possible to have unflushed GPAs in PML
+ 	 * buffer.
+ 	 */
+ 	kvm_for_each_vcpu(i, vcpu, kvm)
+ 		kvm_vcpu_kick(vcpu);
+ }
+ 
+ static void vmx_dump_sel(char *name, uint32_t sel)
+ {
+ 	pr_err("%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
+ 	       name, vmcs_read32(sel),
+ 	       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),
+ 	       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),
+ 	       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));
+ }
+ 
+ static void vmx_dump_dtsel(char *name, uint32_t limit)
+ {
+ 	pr_err("%s                           limit=0x%08x, base=0x%016lx\n",
+ 	       name, vmcs_read32(limit),
+ 	       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));
+ }
+ 
+ static void dump_vmcs(void)
+ {
+ 	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
+ 	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
+ 	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
+ 	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
+ 	u32 secondary_exec_control = 0;
+ 	unsigned long cr4 = vmcs_readl(GUEST_CR4);
+ 	u64 efer = vmcs_readl(GUEST_IA32_EFER);
+ 	int i, n;
+ 
+ 	if (cpu_has_secondary_exec_ctrls())
+ 		secondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);
+ 
+ 	pr_err("*** Guest State ***\n");
+ 	pr_err("CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
+ 	       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),
+ 	       vmcs_readl(CR0_GUEST_HOST_MASK));
+ 	pr_err("CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\n",
+ 	       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));
+ 	pr_err("CR3 = 0x%016lx\n", vmcs_readl(GUEST_CR3));
+ 	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&
+ 	    (cr4 & X86_CR4_PAE) && !(efer & EFER_LMA))
+ 	{
+ 		pr_err("PDPTR0 = 0x%016lx  PDPTR1 = 0x%016lx\n",
+ 		       vmcs_readl(GUEST_PDPTR0), vmcs_readl(GUEST_PDPTR1));
+ 		pr_err("PDPTR2 = 0x%016lx  PDPTR3 = 0x%016lx\n",
+ 		       vmcs_readl(GUEST_PDPTR2), vmcs_readl(GUEST_PDPTR3));
+ 	}
+ 	pr_err("RSP = 0x%016lx  RIP = 0x%016lx\n",
+ 	       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
+ 	pr_err("RFLAGS=0x%08lx         DR7 = 0x%016lx\n",
+ 	       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));
+ 	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
+ 	       vmcs_readl(GUEST_SYSENTER_ESP),
+ 	       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));
+ 	vmx_dump_sel("CS:  ", GUEST_CS_SELECTOR);
+ 	vmx_dump_sel("DS:  ", GUEST_DS_SELECTOR);
+ 	vmx_dump_sel("SS:  ", GUEST_SS_SELECTOR);
+ 	vmx_dump_sel("ES:  ", GUEST_ES_SELECTOR);
+ 	vmx_dump_sel("FS:  ", GUEST_FS_SELECTOR);
+ 	vmx_dump_sel("GS:  ", GUEST_GS_SELECTOR);
+ 	vmx_dump_dtsel("GDTR:", GUEST_GDTR_LIMIT);
+ 	vmx_dump_sel("LDTR:", GUEST_LDTR_SELECTOR);
+ 	vmx_dump_dtsel("IDTR:", GUEST_IDTR_LIMIT);
+ 	vmx_dump_sel("TR:  ", GUEST_TR_SELECTOR);
+ 	if ((vmexit_ctl & (VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER)) ||
+ 	    (vmentry_ctl & (VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_IA32_EFER)))
+ 		pr_err("EFER =     0x%016llx  PAT = 0x%016lx\n",
+ 		       efer, vmcs_readl(GUEST_IA32_PAT));
+ 	pr_err("DebugCtl = 0x%016lx  DebugExceptions = 0x%016lx\n",
+ 	       vmcs_readl(GUEST_IA32_DEBUGCTL),
+ 	       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));
+ 	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)
+ 		pr_err("PerfGlobCtl = 0x%016lx\n",
+ 		       vmcs_readl(GUEST_IA32_PERF_GLOBAL_CTRL));
+ 	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)
+ 		pr_err("BndCfgS = 0x%016lx\n", vmcs_readl(GUEST_BNDCFGS));
+ 	pr_err("Interruptibility = %08x  ActivityState = %08x\n",
+ 	       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
+ 	       vmcs_read32(GUEST_ACTIVITY_STATE));
+ 	if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+ 		pr_err("InterruptStatus = %04x\n",
+ 		       vmcs_read16(GUEST_INTR_STATUS));
+ 
+ 	pr_err("*** Host State ***\n");
+ 	pr_err("RIP = 0x%016lx  RSP = 0x%016lx\n",
+ 	       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));
+ 	pr_err("CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\n",
+ 	       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),
+ 	       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),
+ 	       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),
+ 	       vmcs_read16(HOST_TR_SELECTOR));
+ 	pr_err("FSBase=%016lx GSBase=%016lx TRBase=%016lx\n",
+ 	       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),
+ 	       vmcs_readl(HOST_TR_BASE));
+ 	pr_err("GDTBase=%016lx IDTBase=%016lx\n",
+ 	       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));
+ 	pr_err("CR0=%016lx CR3=%016lx CR4=%016lx\n",
+ 	       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),
+ 	       vmcs_readl(HOST_CR4));
+ 	pr_err("Sysenter RSP=%016lx CS:RIP=%04x:%016lx\n",
+ 	       vmcs_readl(HOST_IA32_SYSENTER_ESP),
+ 	       vmcs_read32(HOST_IA32_SYSENTER_CS),
+ 	       vmcs_readl(HOST_IA32_SYSENTER_EIP));
+ 	if (vmexit_ctl & (VM_EXIT_LOAD_IA32_PAT | VM_EXIT_LOAD_IA32_EFER))
+ 		pr_err("EFER = 0x%016lx  PAT = 0x%016lx\n",
+ 		       vmcs_readl(HOST_IA32_EFER), vmcs_readl(HOST_IA32_PAT));
+ 	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)
+ 		pr_err("PerfGlobCtl = 0x%016lx\n",
+ 		       vmcs_readl(HOST_IA32_PERF_GLOBAL_CTRL));
+ 
+ 	pr_err("*** Control State ***\n");
+ 	pr_err("PinBased=%08x CPUBased=%08x SecondaryExec=%08x\n",
+ 	       pin_based_exec_ctrl, cpu_based_exec_ctrl, secondary_exec_control);
+ 	pr_err("EntryControls=%08x ExitControls=%08x\n", vmentry_ctl, vmexit_ctl);
+ 	pr_err("ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\n",
+ 	       vmcs_read32(EXCEPTION_BITMAP),
+ 	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),
+ 	       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));
+ 	pr_err("VMEntry: intr_info=%08x errcode=%08x ilen=%08x\n",
+ 	       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
+ 	       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),
+ 	       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));
+ 	pr_err("VMExit: intr_info=%08x errcode=%08x ilen=%08x\n",
+ 	       vmcs_read32(VM_EXIT_INTR_INFO),
+ 	       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
+ 	       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));
+ 	pr_err("        reason=%08x qualification=%016lx\n",
+ 	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
+ 	pr_err("IDTVectoring: info=%08x errcode=%08x\n",
+ 	       vmcs_read32(IDT_VECTORING_INFO_FIELD),
+ 	       vmcs_read32(IDT_VECTORING_ERROR_CODE));
+ 	pr_err("TSC Offset = 0x%016lx\n", vmcs_readl(TSC_OFFSET));
+ 	if (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW)
+ 		pr_err("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));
+ 	if (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)
+ 		pr_err("PostedIntrVec = 0x%02x\n", vmcs_read16(POSTED_INTR_NV));
+ 	if ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))
+ 		pr_err("EPT pointer = 0x%016lx\n", vmcs_readl(EPT_POINTER));
+ 	n = vmcs_read32(CR3_TARGET_COUNT);
+ 	for (i = 0; i + 1 < n; i += 4)
+ 		pr_err("CR3 target%u=%016lx target%u=%016lx\n",
+ 		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2),
+ 		       i + 1, vmcs_readl(CR3_TARGET_VALUE0 + i * 2 + 2));
+ 	if (i < n)
+ 		pr_err("CR3 target%u=%016lx\n",
+ 		       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2));
+ 	if (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)
+ 		pr_err("PLE Gap=%08x Window=%08x\n",
+ 		       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));
+ 	if (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)
+ 		pr_err("Virtual processor ID = 0x%04x\n",
+ 		       vmcs_read16(VIRTUAL_PROCESSOR_ID));
+ }
+ 
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  /*
   * The guest has exited.  See if we can fix it or if we need userspace
   * assistance.
@@@ -7257,6 -7843,16 +7505,19 @@@ static int vmx_handle_exit(struct kvm_v
  	u32 exit_reason = vmx->exit_reason;
  	u32 vectoring_info = vmx->idt_vectoring_info;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Flush logged GPAs PML buffer, this will make dirty_bitmap more
+ 	 * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before
+ 	 * querying dirty_bitmap, we only need to kick all vcpus out of guest
+ 	 * mode as if vcpus is in root mode, the PML buffer must has been
+ 	 * flushed already.
+ 	 */
+ 	if (enable_pml)
+ 		vmx_flush_pml_buffer(vcpu);
+ 
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  	/* If guest state is invalid, start emulating */
  	if (vmx->emulation_required)
  		return handle_invalid_guest_state(vcpu);
@@@ -8289,9 -9142,10 +8550,16 @@@ static int nested_vmx_store_msr(struct 
  	struct vmx_msr_entry e;
  
  	for (i = 0; i < count; i++) {
++<<<<<<< HEAD
 +		if (kvm_read_guest(vcpu->kvm,
 +				   gpa + i * sizeof(e),
 +				   &e, 2 * sizeof(u32))) {
++=======
+ 		struct msr_data msr_info;
+ 		if (kvm_vcpu_read_guest(vcpu,
+ 					gpa + i * sizeof(e),
+ 					&e, 2 * sizeof(u32))) {
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  			pr_warn_ratelimited(
  				"%s cannot read MSR entry (%u, 0x%08llx)\n",
  				__func__, i, gpa + i * sizeof(e));
@@@ -8309,13 -9165,13 +8577,20 @@@
  				__func__, i, e.index);
  			return -EINVAL;
  		}
++<<<<<<< HEAD
 +		if (kvm_write_guest(vcpu->kvm,
 +				    gpa + i * sizeof(e) +
 +					offsetof(struct vmx_msr_entry, value),
 +				    &e.value, sizeof(e.value))) {
++=======
+ 		if (kvm_vcpu_write_guest(vcpu,
+ 					 gpa + i * sizeof(e) +
+ 					     offsetof(struct vmx_msr_entry, value),
+ 					 &msr_info.data, sizeof(msr_info.data))) {
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  			pr_warn_ratelimited(
  				"%s cannot write MSR (%u, 0x%x, 0x%llx)\n",
 -				__func__, i, e.index, msr_info.data);
 +				__func__, i, e.index, e.value);
  			return -EINVAL;
  		}
  	}
diff --cc arch/x86/kvm/x86.c
index 44ac86d26f9f,a510f135180a..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -6067,6 -6393,233 +6067,236 @@@ static void process_nmi(struct kvm_vcp
  	kvm_make_request(KVM_REQ_EVENT, vcpu);
  }
  
++<<<<<<< HEAD
++=======
+ #define put_smstate(type, buf, offset, val)			  \
+ 	*(type *)((buf) + (offset) - 0x7e00) = val
+ 
+ static u32 process_smi_get_segment_flags(struct kvm_segment *seg)
+ {
+ 	u32 flags = 0;
+ 	flags |= seg->g       << 23;
+ 	flags |= seg->db      << 22;
+ 	flags |= seg->l       << 21;
+ 	flags |= seg->avl     << 20;
+ 	flags |= seg->present << 15;
+ 	flags |= seg->dpl     << 13;
+ 	flags |= seg->s       << 12;
+ 	flags |= seg->type    << 8;
+ 	return flags;
+ }
+ 
+ static void process_smi_save_seg_32(struct kvm_vcpu *vcpu, char *buf, int n)
+ {
+ 	struct kvm_segment seg;
+ 	int offset;
+ 
+ 	kvm_get_segment(vcpu, &seg, n);
+ 	put_smstate(u32, buf, 0x7fa8 + n * 4, seg.selector);
+ 
+ 	if (n < 3)
+ 		offset = 0x7f84 + n * 12;
+ 	else
+ 		offset = 0x7f2c + (n - 3) * 12;
+ 
+ 	put_smstate(u32, buf, offset + 8, seg.base);
+ 	put_smstate(u32, buf, offset + 4, seg.limit);
+ 	put_smstate(u32, buf, offset, process_smi_get_segment_flags(&seg));
+ }
+ 
+ static void process_smi_save_seg_64(struct kvm_vcpu *vcpu, char *buf, int n)
+ {
+ 	struct kvm_segment seg;
+ 	int offset;
+ 	u16 flags;
+ 
+ 	kvm_get_segment(vcpu, &seg, n);
+ 	offset = 0x7e00 + n * 16;
+ 
+ 	flags = process_smi_get_segment_flags(&seg) >> 8;
+ 	put_smstate(u16, buf, offset, seg.selector);
+ 	put_smstate(u16, buf, offset + 2, flags);
+ 	put_smstate(u32, buf, offset + 4, seg.limit);
+ 	put_smstate(u64, buf, offset + 8, seg.base);
+ }
+ 
+ static void process_smi_save_state_32(struct kvm_vcpu *vcpu, char *buf)
+ {
+ 	struct desc_ptr dt;
+ 	struct kvm_segment seg;
+ 	unsigned long val;
+ 	int i;
+ 
+ 	put_smstate(u32, buf, 0x7ffc, kvm_read_cr0(vcpu));
+ 	put_smstate(u32, buf, 0x7ff8, kvm_read_cr3(vcpu));
+ 	put_smstate(u32, buf, 0x7ff4, kvm_get_rflags(vcpu));
+ 	put_smstate(u32, buf, 0x7ff0, kvm_rip_read(vcpu));
+ 
+ 	for (i = 0; i < 8; i++)
+ 		put_smstate(u32, buf, 0x7fd0 + i * 4, kvm_register_read(vcpu, i));
+ 
+ 	kvm_get_dr(vcpu, 6, &val);
+ 	put_smstate(u32, buf, 0x7fcc, (u32)val);
+ 	kvm_get_dr(vcpu, 7, &val);
+ 	put_smstate(u32, buf, 0x7fc8, (u32)val);
+ 
+ 	kvm_get_segment(vcpu, &seg, VCPU_SREG_TR);
+ 	put_smstate(u32, buf, 0x7fc4, seg.selector);
+ 	put_smstate(u32, buf, 0x7f64, seg.base);
+ 	put_smstate(u32, buf, 0x7f60, seg.limit);
+ 	put_smstate(u32, buf, 0x7f5c, process_smi_get_segment_flags(&seg));
+ 
+ 	kvm_get_segment(vcpu, &seg, VCPU_SREG_LDTR);
+ 	put_smstate(u32, buf, 0x7fc0, seg.selector);
+ 	put_smstate(u32, buf, 0x7f80, seg.base);
+ 	put_smstate(u32, buf, 0x7f7c, seg.limit);
+ 	put_smstate(u32, buf, 0x7f78, process_smi_get_segment_flags(&seg));
+ 
+ 	kvm_x86_ops->get_gdt(vcpu, &dt);
+ 	put_smstate(u32, buf, 0x7f74, dt.address);
+ 	put_smstate(u32, buf, 0x7f70, dt.size);
+ 
+ 	kvm_x86_ops->get_idt(vcpu, &dt);
+ 	put_smstate(u32, buf, 0x7f58, dt.address);
+ 	put_smstate(u32, buf, 0x7f54, dt.size);
+ 
+ 	for (i = 0; i < 6; i++)
+ 		process_smi_save_seg_32(vcpu, buf, i);
+ 
+ 	put_smstate(u32, buf, 0x7f14, kvm_read_cr4(vcpu));
+ 
+ 	/* revision id */
+ 	put_smstate(u32, buf, 0x7efc, 0x00020000);
+ 	put_smstate(u32, buf, 0x7ef8, vcpu->arch.smbase);
+ }
+ 
+ static void process_smi_save_state_64(struct kvm_vcpu *vcpu, char *buf)
+ {
+ #ifdef CONFIG_X86_64
+ 	struct desc_ptr dt;
+ 	struct kvm_segment seg;
+ 	unsigned long val;
+ 	int i;
+ 
+ 	for (i = 0; i < 16; i++)
+ 		put_smstate(u64, buf, 0x7ff8 - i * 8, kvm_register_read(vcpu, i));
+ 
+ 	put_smstate(u64, buf, 0x7f78, kvm_rip_read(vcpu));
+ 	put_smstate(u32, buf, 0x7f70, kvm_get_rflags(vcpu));
+ 
+ 	kvm_get_dr(vcpu, 6, &val);
+ 	put_smstate(u64, buf, 0x7f68, val);
+ 	kvm_get_dr(vcpu, 7, &val);
+ 	put_smstate(u64, buf, 0x7f60, val);
+ 
+ 	put_smstate(u64, buf, 0x7f58, kvm_read_cr0(vcpu));
+ 	put_smstate(u64, buf, 0x7f50, kvm_read_cr3(vcpu));
+ 	put_smstate(u64, buf, 0x7f48, kvm_read_cr4(vcpu));
+ 
+ 	put_smstate(u32, buf, 0x7f00, vcpu->arch.smbase);
+ 
+ 	/* revision id */
+ 	put_smstate(u32, buf, 0x7efc, 0x00020064);
+ 
+ 	put_smstate(u64, buf, 0x7ed0, vcpu->arch.efer);
+ 
+ 	kvm_get_segment(vcpu, &seg, VCPU_SREG_TR);
+ 	put_smstate(u16, buf, 0x7e90, seg.selector);
+ 	put_smstate(u16, buf, 0x7e92, process_smi_get_segment_flags(&seg) >> 8);
+ 	put_smstate(u32, buf, 0x7e94, seg.limit);
+ 	put_smstate(u64, buf, 0x7e98, seg.base);
+ 
+ 	kvm_x86_ops->get_idt(vcpu, &dt);
+ 	put_smstate(u32, buf, 0x7e84, dt.size);
+ 	put_smstate(u64, buf, 0x7e88, dt.address);
+ 
+ 	kvm_get_segment(vcpu, &seg, VCPU_SREG_LDTR);
+ 	put_smstate(u16, buf, 0x7e70, seg.selector);
+ 	put_smstate(u16, buf, 0x7e72, process_smi_get_segment_flags(&seg) >> 8);
+ 	put_smstate(u32, buf, 0x7e74, seg.limit);
+ 	put_smstate(u64, buf, 0x7e78, seg.base);
+ 
+ 	kvm_x86_ops->get_gdt(vcpu, &dt);
+ 	put_smstate(u32, buf, 0x7e64, dt.size);
+ 	put_smstate(u64, buf, 0x7e68, dt.address);
+ 
+ 	for (i = 0; i < 6; i++)
+ 		process_smi_save_seg_64(vcpu, buf, i);
+ #else
+ 	WARN_ON_ONCE(1);
+ #endif
+ }
+ 
+ static void process_smi(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_segment cs, ds;
+ 	char buf[512];
+ 	u32 cr0;
+ 
+ 	if (is_smm(vcpu)) {
+ 		vcpu->arch.smi_pending = true;
+ 		return;
+ 	}
+ 
+ 	trace_kvm_enter_smm(vcpu->vcpu_id, vcpu->arch.smbase, true);
+ 	vcpu->arch.hflags |= HF_SMM_MASK;
+ 	memset(buf, 0, 512);
+ 	if (guest_cpuid_has_longmode(vcpu))
+ 		process_smi_save_state_64(vcpu, buf);
+ 	else
+ 		process_smi_save_state_32(vcpu, buf);
+ 
+ 	kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, buf, sizeof(buf));
+ 
+ 	if (kvm_x86_ops->get_nmi_mask(vcpu))
+ 		vcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;
+ 	else
+ 		kvm_x86_ops->set_nmi_mask(vcpu, true);
+ 
+ 	kvm_set_rflags(vcpu, X86_EFLAGS_FIXED);
+ 	kvm_rip_write(vcpu, 0x8000);
+ 
+ 	cr0 = vcpu->arch.cr0 & ~(X86_CR0_PE | X86_CR0_EM | X86_CR0_TS | X86_CR0_PG);
+ 	kvm_x86_ops->set_cr0(vcpu, cr0);
+ 	vcpu->arch.cr0 = cr0;
+ 
+ 	kvm_x86_ops->set_cr4(vcpu, 0);
+ 
+ 	__kvm_set_dr(vcpu, 7, DR7_FIXED_1);
+ 
+ 	cs.selector = (vcpu->arch.smbase >> 4) & 0xffff;
+ 	cs.base = vcpu->arch.smbase;
+ 
+ 	ds.selector = 0;
+ 	ds.base = 0;
+ 
+ 	cs.limit    = ds.limit = 0xffffffff;
+ 	cs.type     = ds.type = 0x3;
+ 	cs.dpl      = ds.dpl = 0;
+ 	cs.db       = ds.db = 0;
+ 	cs.s        = ds.s = 1;
+ 	cs.l        = ds.l = 0;
+ 	cs.g        = ds.g = 1;
+ 	cs.avl      = ds.avl = 0;
+ 	cs.present  = ds.present = 1;
+ 	cs.unusable = ds.unusable = 0;
+ 	cs.padding  = ds.padding = 0;
+ 
+ 	kvm_set_segment(vcpu, &cs, VCPU_SREG_CS);
+ 	kvm_set_segment(vcpu, &ds, VCPU_SREG_DS);
+ 	kvm_set_segment(vcpu, &ds, VCPU_SREG_ES);
+ 	kvm_set_segment(vcpu, &ds, VCPU_SREG_FS);
+ 	kvm_set_segment(vcpu, &ds, VCPU_SREG_GS);
+ 	kvm_set_segment(vcpu, &ds, VCPU_SREG_SS);
+ 
+ 	if (guest_cpuid_has_longmode(vcpu))
+ 		kvm_x86_ops->set_efer(vcpu, 0);
+ 
+ 	kvm_update_cpuid(vcpu);
+ 	kvm_mmu_reset_context(vcpu);
+ }
+ 
++>>>>>>> 54bf36aac520 (KVM: x86: use vcpu-specific functions to read/write/translate GFNs)
  static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
  {
  	u64 eoi_exit_bitmap[4];
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 1442e6fbe132..4cf6a8a0299b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -829,7 +829,7 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot);
 void kvm_mmu_zap_all(struct kvm *kvm);
-void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm);
+void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, struct kvm_memslots *slots);
 unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu_audit.c b/arch/x86/kvm/mmu_audit.c
index 19c22a49550a..269ae0ae94f6 100644
--- a/arch/x86/kvm/mmu_audit.c
+++ b/arch/x86/kvm/mmu_audit.c
@@ -114,7 +114,7 @@ static void audit_mappings(struct kvm_vcpu *vcpu, u64 *sptep, int level)
 		return;
 
 	gfn = kvm_mmu_page_get_gfn(sp, sptep - sp->spt);
-	pfn = gfn_to_pfn_atomic(vcpu->kvm, gfn);
+	pfn = kvm_vcpu_gfn_to_pfn_atomic(vcpu, gfn);
 
 	if (is_error_pfn(pfn))
 		return;
diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h
index 6e6d115fe9b5..0f67d7e24800 100644
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@ -256,7 +256,7 @@ static int FNAME(update_accessed_dirty_bits)(struct kvm_vcpu *vcpu,
 		if (ret)
 			return ret;
 
-		mark_page_dirty(vcpu->kvm, table_gfn);
+		kvm_vcpu_mark_page_dirty(vcpu, table_gfn);
 		walker->ptes[level] = pte;
 	}
 	return 0;
@@ -338,7 +338,7 @@ retry_walk:
 
 		real_gfn = gpa_to_gfn(real_gfn);
 
-		host_addr = gfn_to_hva_prot(vcpu->kvm, real_gfn,
+		host_addr = kvm_vcpu_gfn_to_hva_prot(vcpu, real_gfn,
 					    &walker->pte_writable[walker->level - 1]);
 		if (unlikely(kvm_is_error_hva(host_addr)))
 			goto error;
@@ -511,11 +511,11 @@ static bool FNAME(gpte_changed)(struct kvm_vcpu *vcpu,
 		base_gpa = pte_gpa & ~mask;
 		index = (pte_gpa - base_gpa) / sizeof(pt_element_t);
 
-		r = kvm_read_guest_atomic(vcpu->kvm, base_gpa,
+		r = kvm_vcpu_read_guest_atomic(vcpu, base_gpa,
 				gw->prefetch_ptes, sizeof(gw->prefetch_ptes));
 		curr_pte = gw->prefetch_ptes[index];
 	} else
-		r = kvm_read_guest_atomic(vcpu->kvm, pte_gpa,
+		r = kvm_vcpu_read_guest_atomic(vcpu, pte_gpa,
 				  &curr_pte, sizeof(curr_pte));
 
 	return r || curr_pte != gw->ptes[level - 1];
@@ -869,8 +869,8 @@ static void FNAME(invlpg)(struct kvm_vcpu *vcpu, gva_t gva)
 			if (!rmap_can_add(vcpu))
 				break;
 
-			if (kvm_read_guest_atomic(vcpu->kvm, pte_gpa, &gpte,
-						  sizeof(pt_element_t)))
+			if (kvm_vcpu_read_guest_atomic(vcpu, pte_gpa, &gpte,
+						       sizeof(pt_element_t)))
 				break;
 
 			FNAME(update_pte)(vcpu, sp, sptep, &gpte);
@@ -956,8 +956,8 @@ static int FNAME(sync_page)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 
 		pte_gpa = first_pte_gpa + i * sizeof(pt_element_t);
 
-		if (kvm_read_guest_atomic(vcpu->kvm, pte_gpa, &gpte,
-					  sizeof(pt_element_t)))
+		if (kvm_vcpu_read_guest_atomic(vcpu, pte_gpa, &gpte,
+					       sizeof(pt_element_t)))
 			return -EINVAL;
 
 		if (FNAME(prefetch_invalid_gpte)(vcpu, sp, &sp->spt[i], gpte)) {
@@ -970,7 +970,7 @@ static int FNAME(sync_page)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)
 		pte_access &= FNAME(gpte_access)(vcpu, gpte);
 		FNAME(protect_clean_gpte)(&pte_access, gpte);
 
-		if (sync_mmio_spte(vcpu->kvm, &sp->spt[i], gfn, pte_access,
+		if (sync_mmio_spte(vcpu, &sp->spt[i], gfn, pte_access,
 		      &nr_present))
 			continue;
 
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index e21cbdc1b064..e1419ffa3c4e 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -1955,8 +1955,8 @@ static u64 nested_svm_get_tdp_pdptr(struct kvm_vcpu *vcpu, int index)
 	u64 pdpte;
 	int ret;
 
-	ret = kvm_read_guest_page(vcpu->kvm, gpa_to_gfn(cr3), &pdpte,
-				  offset_in_page(cr3) + index * 8, 8);
+	ret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(cr3), &pdpte,
+				       offset_in_page(cr3) + index * 8, 8);
 	if (ret)
 		return 0;
 	return pdpte;
@@ -2114,7 +2114,7 @@ static void *nested_svm_map(struct vcpu_svm *svm, u64 gpa, struct page **_page)
 
 	might_sleep();
 
-	page = gfn_to_page(svm->vcpu.kvm, gpa >> PAGE_SHIFT);
+	page = kvm_vcpu_gfn_to_page(&svm->vcpu, gpa >> PAGE_SHIFT);
 	if (is_error_page(page))
 		goto error;
 
@@ -2153,7 +2153,7 @@ static int nested_svm_intercept_ioio(struct vcpu_svm *svm)
 	mask = (0xf >> (4 - size)) << start_bit;
 	val = 0;
 
-	if (kvm_read_guest(svm->vcpu.kvm, gpa, &val, iopm_len))
+	if (kvm_vcpu_read_guest(&svm->vcpu, gpa, &val, iopm_len))
 		return NESTED_EXIT_DONE;
 
 	return (val & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;
@@ -2178,7 +2178,7 @@ static int nested_svm_exit_handled_msr(struct vcpu_svm *svm)
 	/* Offset is in 32 bit units but need in 8 bit units */
 	offset *= 4;
 
-	if (kvm_read_guest(svm->vcpu.kvm, svm->nested.vmcb_msrpm + offset, &value, 4))
+	if (kvm_vcpu_read_guest(&svm->vcpu, svm->nested.vmcb_msrpm + offset, &value, 4))
 		return NESTED_EXIT_DONE;
 
 	return (value & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;
@@ -2449,7 +2449,7 @@ static bool nested_svm_vmrun_msrpm(struct vcpu_svm *svm)
 		p      = msrpm_offsets[i];
 		offset = svm->nested.vmcb_msrpm + (p * 4);
 
-		if (kvm_read_guest(svm->vcpu.kvm, offset, &value, 4))
+		if (kvm_vcpu_read_guest(&svm->vcpu, offset, &value, 4))
 			return false;
 
 		svm->nested.msrpm[p] = svm->msrpm[p] | value;
* Unmerged path arch/x86/kvm/vmx.c
* Unmerged path arch/x86/kvm/x86.c
