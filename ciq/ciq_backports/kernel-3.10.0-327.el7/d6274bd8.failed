tcp: add send_synack method to tcp_request_sock_ops

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Octavian Purdila <octavian.purdila@intel.com>
commit d6274bd8d6ea84b7b54cc1c3fde6bcb6143b104f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/d6274bd8.failed

Create a new tcp_request_sock_ops method to unify the IPv4/IPv6
signature for tcp_v[46]_send_synack. This allows us to later unify
tcp_v4_rtx_synack with tcp_v6_rtx_synack and tcp_v4_conn_request with
tcp_v4_conn_request.

	Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit d6274bd8d6ea84b7b54cc1c3fde6bcb6143b104f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/tcp.h
#	net/ipv4/tcp_ipv4.c
#	net/ipv6/tcp_ipv6.c
diff --cc include/net/tcp.h
index ba45accd7103,8c05c25018d5..000000000000
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@@ -1579,11 -1599,40 +1579,27 @@@ struct tcp_request_sock_ops 
  						  const struct request_sock *req,
  						  const struct sk_buff *skb);
  #endif
++<<<<<<< HEAD
++=======
+ 	void (*init_req)(struct request_sock *req, struct sock *sk,
+ 			 struct sk_buff *skb);
+ #ifdef CONFIG_SYN_COOKIES
+ 	__u32 (*cookie_init_seq)(struct sock *sk, const struct sk_buff *skb,
+ 				 __u16 *mss);
+ #endif
+ 	struct dst_entry *(*route_req)(struct sock *sk, struct flowi *fl,
+ 				       const struct request_sock *req,
+ 				       bool *strict);
+ 	__u32 (*init_seq)(const struct sk_buff *skb);
+ 	int (*send_synack)(struct sock *sk, struct dst_entry *dst,
+ 			   struct flowi *fl, struct request_sock *req,
+ 			   u16 queue_mapping, struct tcp_fastopen_cookie *foc);
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  };
  
 -#ifdef CONFIG_SYN_COOKIES
 -static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
 -					 struct sock *sk, struct sk_buff *skb,
 -					 __u16 *mss)
 -{
 -	return ops->cookie_init_seq(sk, skb, mss);
 -}
 -#else
 -static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
 -					 struct sock *sk, struct sk_buff *skb,
 -					 __u16 *mss)
 -{
 -	return 0;
 -}
 -#endif
 -
 -int tcpv4_offload_init(void);
 +extern int tcpv4_offload_init(void);
  
 -void tcp_v4_init(void);
 -void tcp_init(void);
 +extern void tcp_v4_init(void);
 +extern void tcp_init(void);
  
  #endif	/* _TCP_H */
diff --cc net/ipv4/tcp_ipv4.c
index 7c1eb4426934,b5945ac50876..000000000000
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@@ -813,8 -814,10 +813,9 @@@ static void tcp_v4_reqsk_send_ack(struc
   *	socket.
   */
  static int tcp_v4_send_synack(struct sock *sk, struct dst_entry *dst,
+ 			      struct flowi *fl,
  			      struct request_sock *req,
 -			      u16 queue_mapping,
 -			      struct tcp_fastopen_cookie *foc)
 +			      u16 queue_mapping)
  {
  	const struct inet_request_sock *ireq = inet_rsk(req);
  	struct flowi4 fl4;
@@@ -844,7 -847,8 +845,12 @@@
  
  static int tcp_v4_rtx_synack(struct sock *sk, struct request_sock *req)
  {
++<<<<<<< HEAD
 +	int res = tcp_v4_send_synack(sk, NULL, req, 0);
++=======
+ 	const struct  tcp_request_sock_ops *af_ops = tcp_rsk(req)->af_specific;
+ 	int res = af_ops->send_synack(sk, NULL, NULL, req, 0, NULL);
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  
  	if (!res) {
  		TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_RETRANSSEGS);
@@@ -1247,193 -1276,19 +1253,204 @@@ struct request_sock_ops tcp_request_soc
  	.syn_ack_timeout = 	tcp_syn_ack_timeout,
  };
  
 -static const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {
  #ifdef CONFIG_TCP_MD5SIG
 +static const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {
  	.md5_lookup	=	tcp_v4_reqsk_md5_lookup,
  	.calc_md5_hash	=	tcp_v4_md5_hash_skb,
++<<<<<<< HEAD
++=======
+ #endif
+ 	.init_req	=	tcp_v4_init_req,
+ #ifdef CONFIG_SYN_COOKIES
+ 	.cookie_init_seq =	cookie_v4_init_sequence,
+ #endif
+ 	.route_req	=	tcp_v4_route_req,
+ 	.init_seq	=	tcp_v4_init_sequence,
+ 	.send_synack	=	tcp_v4_send_synack,
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  };
 +#endif
 +
 +static bool tcp_fastopen_check(struct sock *sk, struct sk_buff *skb,
 +			       struct request_sock *req,
 +			       struct tcp_fastopen_cookie *foc,
 +			       struct tcp_fastopen_cookie *valid_foc)
 +{
 +	bool skip_cookie = false;
 +	struct fastopen_queue *fastopenq;
 +
 +	if (likely(!fastopen_cookie_present(foc))) {
 +		/* See include/net/tcp.h for the meaning of these knobs */
 +		if ((sysctl_tcp_fastopen & TFO_SERVER_ALWAYS) ||
 +		    ((sysctl_tcp_fastopen & TFO_SERVER_COOKIE_NOT_REQD) &&
 +		    (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq + 1)))
 +			skip_cookie = true; /* no cookie to validate */
 +		else
 +			return false;
 +	}
 +	fastopenq = inet_csk(sk)->icsk_accept_queue.fastopenq;
 +	/* A FO option is present; bump the counter. */
 +	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPFASTOPENPASSIVE);
 +
 +	/* Make sure the listener has enabled fastopen, and we don't
 +	 * exceed the max # of pending TFO requests allowed before trying
 +	 * to validating the cookie in order to avoid burning CPU cycles
 +	 * unnecessarily.
 +	 *
 +	 * XXX (TFO) - The implication of checking the max_qlen before
 +	 * processing a cookie request is that clients can't differentiate
 +	 * between qlen overflow causing Fast Open to be disabled
 +	 * temporarily vs a server not supporting Fast Open at all.
 +	 */
 +	if ((sysctl_tcp_fastopen & TFO_SERVER_ENABLE) == 0 ||
 +	    fastopenq == NULL || fastopenq->max_qlen == 0)
 +		return false;
 +
 +	if (fastopenq->qlen >= fastopenq->max_qlen) {
 +		struct request_sock *req1;
 +		spin_lock(&fastopenq->lock);
 +		req1 = fastopenq->rskq_rst_head;
 +		if ((req1 == NULL) || time_after(req1->expires, jiffies)) {
 +			spin_unlock(&fastopenq->lock);
 +			NET_INC_STATS_BH(sock_net(sk),
 +			    LINUX_MIB_TCPFASTOPENLISTENOVERFLOW);
 +			/* Avoid bumping LINUX_MIB_TCPFASTOPENPASSIVEFAIL*/
 +			foc->len = -1;
 +			return false;
 +		}
 +		fastopenq->rskq_rst_head = req1->dl_next;
 +		fastopenq->qlen--;
 +		spin_unlock(&fastopenq->lock);
 +		reqsk_free(req1);
 +	}
 +	if (skip_cookie) {
 +		tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +		return true;
 +	}
 +
 +	if (foc->len == TCP_FASTOPEN_COOKIE_SIZE) {
 +		if ((sysctl_tcp_fastopen & TFO_SERVER_COOKIE_NOT_CHKED) == 0) {
 +			tcp_fastopen_cookie_gen(ip_hdr(skb)->saddr,
 +						ip_hdr(skb)->daddr, valid_foc);
 +			if ((valid_foc->len != TCP_FASTOPEN_COOKIE_SIZE) ||
 +			    memcmp(&foc->val[0], &valid_foc->val[0],
 +			    TCP_FASTOPEN_COOKIE_SIZE) != 0)
 +				return false;
 +			valid_foc->len = -1;
 +		}
 +		/* Acknowledge the data received from the peer. */
 +		tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +		return true;
 +	} else if (foc->len == 0) { /* Client requesting a cookie */
 +		tcp_fastopen_cookie_gen(ip_hdr(skb)->saddr,
 +					ip_hdr(skb)->daddr, valid_foc);
 +		NET_INC_STATS_BH(sock_net(sk),
 +		    LINUX_MIB_TCPFASTOPENCOOKIEREQD);
 +	} else {
 +		/* Client sent a cookie with wrong size. Treat it
 +		 * the same as invalid and return a valid one.
 +		 */
 +		tcp_fastopen_cookie_gen(ip_hdr(skb)->saddr,
 +					ip_hdr(skb)->daddr, valid_foc);
 +	}
 +	return false;
 +}
 +
 +static int tcp_v4_conn_req_fastopen(struct sock *sk,
 +				    struct sk_buff *skb,
 +				    struct sk_buff *skb_synack,
 +				    struct request_sock *req)
 +{
 +	struct tcp_sock *tp = tcp_sk(sk);
 +	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
 +	const struct inet_request_sock *ireq = inet_rsk(req);
 +	struct sock *child;
 +	int err;
 +
 +	req->num_retrans = 0;
 +	req->num_timeout = 0;
 +	req->sk = NULL;
 +
 +	child = inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb, req, NULL);
 +	if (child == NULL) {
 +		NET_INC_STATS_BH(sock_net(sk),
 +				 LINUX_MIB_TCPFASTOPENPASSIVEFAIL);
 +		kfree_skb(skb_synack);
 +		return -1;
 +	}
 +	err = ip_build_and_send_pkt(skb_synack, sk, ireq->ir_loc_addr,
 +				    ireq->ir_rmt_addr, ireq->opt);
 +	err = net_xmit_eval(err);
 +	if (!err)
 +		tcp_rsk(req)->snt_synack = tcp_time_stamp;
 +	/* XXX (TFO) - is it ok to ignore error and continue? */
 +
 +	spin_lock(&queue->fastopenq->lock);
 +	queue->fastopenq->qlen++;
 +	spin_unlock(&queue->fastopenq->lock);
 +
 +	/* Initialize the child socket. Have to fix some values to take
 +	 * into account the child is a Fast Open socket and is created
 +	 * only out of the bits carried in the SYN packet.
 +	 */
 +	tp = tcp_sk(child);
 +
 +	tp->fastopen_rsk = req;
 +	/* Do a hold on the listner sk so that if the listener is being
 +	 * closed, the child that has been accepted can live on and still
 +	 * access listen_lock.
 +	 */
 +	sock_hold(sk);
 +	tcp_rsk(req)->listener = sk;
 +
 +	/* RFC1323: The window in SYN & SYN/ACK segments is never
 +	 * scaled. So correct it appropriately.
 +	 */
 +	tp->snd_wnd = ntohs(tcp_hdr(skb)->window);
 +
 +	/* Activate the retrans timer so that SYNACK can be retransmitted.
 +	 * The request socket is not added to the SYN table of the parent
 +	 * because it's been added to the accept queue directly.
 +	 */
 +	inet_csk_reset_xmit_timer(child, ICSK_TIME_RETRANS,
 +	    TCP_TIMEOUT_INIT, TCP_RTO_MAX);
 +
 +	/* Add the child socket directly into the accept queue */
 +	inet_csk_reqsk_queue_add(sk, req, child);
 +
 +	/* Now finish processing the fastopen child socket. */
 +	inet_csk(child)->icsk_af_ops->rebuild_header(child);
 +	tcp_init_congestion_control(child);
 +	tcp_mtup_init(child);
 +	tcp_init_metrics(child);
 +	tcp_init_buffer_space(child);
 +
 +	/* Queue the data carried in the SYN packet. We need to first
 +	 * bump skb's refcnt because the caller will attempt to free it.
 +	 *
 +	 * XXX (TFO) - we honor a zero-payload TFO request for now.
 +	 * (Any reason not to?)
 +	 */
 +	if (TCP_SKB_CB(skb)->end_seq == TCP_SKB_CB(skb)->seq + 1) {
 +		/* Don't queue the skb if there is no payload in SYN.
 +		 * XXX (TFO) - How about SYN+FIN?
 +		 */
 +		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +	} else {
 +		skb = skb_get(skb);
 +		skb_dst_drop(skb);
 +		__skb_pull(skb, tcp_hdr(skb)->doff * 4);
 +		skb_set_owner_r(skb, child);
 +		__skb_queue_tail(&child->sk_receive_queue, skb);
 +		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +		tp->syn_data_acked = 1;
 +	}
 +	sk->sk_data_ready(sk, 0);
 +	bh_unlock_sock(child);
 +	sock_put(child);
 +	WARN_ON(req->sk == NULL);
 +	return 0;
 +}
  
  int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
  {
@@@ -1547,42 -1395,21 +1564,52 @@@
  			goto drop_and_release;
  		}
  
 -		isn = af_ops->init_seq(skb);
 +		isn = tcp_v4_init_sequence(skb);
  	}
 -	if (!dst) {
 -		dst = af_ops->route_req(sk, (struct flowi *)&fl4, req, NULL);
 -		if (!dst)
 +	tcp_rsk(req)->snt_isn = isn;
 +
 +	if (dst == NULL) {
 +		dst = inet_csk_route_req(sk, &fl4, req);
 +		if (dst == NULL)
  			goto drop_and_free;
  	}
 +	do_fastopen = tcp_fastopen_check(sk, skb, req, &foc, &valid_foc);
 +
++<<<<<<< HEAD
 +	/* We don't call tcp_v4_send_synack() directly because we need
 +	 * to make sure a child socket can be created successfully before
 +	 * sending back synack!
 +	 *
 +	 * XXX (TFO) - Ideally one would simply call tcp_v4_send_synack()
 +	 * (or better yet, call tcp_send_synack() in the child context
 +	 * directly, but will have to fix bunch of other code first)
 +	 * after syn_recv_sock() except one will need to first fix the
 +	 * latter to remove its dependency on the current implementation
 +	 * of tcp_v4_send_synack()->tcp_select_initial_window().
 +	 */
 +	skb_synack = tcp_make_synack(sk, dst, req,
 +	    fastopen_cookie_present(&valid_foc) ? &valid_foc : NULL);
  
 +	if (skb_synack) {
 +		__tcp_v4_send_check(skb_synack, ireq->ir_loc_addr, ireq->ir_rmt_addr);
 +		skb_set_queue_mapping(skb_synack, skb_get_queue_mapping(skb));
 +	} else
 +		goto drop_and_free;
 +
 +	if (likely(!do_fastopen)) {
 +		int err;
 +		err = ip_build_and_send_pkt(skb_synack, sk, ireq->ir_loc_addr,
 +		     ireq->ir_rmt_addr, ireq->opt);
 +		err = net_xmit_eval(err);
++=======
+ 	tcp_rsk(req)->snt_isn = isn;
+ 	tcp_openreq_init_rwin(req, sk, dst);
+ 	fastopen = !want_cookie &&
+ 		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+ 	err = af_ops->send_synack(sk, dst, NULL, req,
+ 				  skb_get_queue_mapping(skb), &foc);
+ 	if (!fastopen) {
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  		if (err || want_cookie)
  			goto drop_and_free;
  
diff --cc net/ipv6/tcp_ipv6.c
index 45f11a2930c9,210b6105afed..000000000000
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@@ -472,9 -470,10 +472,9 @@@ out
  
  
  static int tcp_v6_send_synack(struct sock *sk, struct dst_entry *dst,
- 			      struct flowi6 *fl6,
+ 			      struct flowi *fl,
  			      struct request_sock *req,
 -			      u16 queue_mapping,
 -			      struct tcp_fastopen_cookie *foc)
 +			      u16 queue_mapping)
  {
  	struct inet_request_sock *ireq = inet_rsk(req);
  	struct ipv6_pinfo *np = inet6_sk(sk);
@@@ -503,10 -508,11 +504,15 @@@ done
  
  static int tcp_v6_rtx_synack(struct sock *sk, struct request_sock *req)
  {
- 	struct flowi6 fl6;
+ 	const struct tcp_request_sock_ops *af_ops = tcp_rsk(req)->af_specific;
+ 	struct flowi fl;
  	int res;
  
++<<<<<<< HEAD
 +	res = tcp_v6_send_synack(sk, NULL, &fl6, req, 0);
++=======
+ 	res = af_ops->send_synack(sk, NULL, &fl, req, 0, NULL);
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  	if (!res) {
  		TCP_INC_STATS_BH(sock_net(sk), TCP_MIB_RETRANSSEGS);
  		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);
@@@ -716,6 -722,40 +722,43 @@@ static int tcp_v6_inbound_md5_hash(stru
  }
  #endif
  
++<<<<<<< HEAD
++=======
+ static void tcp_v6_init_req(struct request_sock *req, struct sock *sk,
+ 			    struct sk_buff *skb)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 	struct ipv6_pinfo *np = inet6_sk(sk);
+ 
+ 	ireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;
+ 	ireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;
+ 
+ 	ireq->ir_iif = sk->sk_bound_dev_if;
+ 
+ 	/* So that link locals have meaning */
+ 	if (!sk->sk_bound_dev_if &&
+ 	    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)
+ 		ireq->ir_iif = inet6_iif(skb);
+ 
+ 	if (!TCP_SKB_CB(skb)->when &&
+ 	    (ipv6_opt_accepted(sk, skb) || np->rxopt.bits.rxinfo ||
+ 	     np->rxopt.bits.rxoinfo || np->rxopt.bits.rxhlim ||
+ 	     np->rxopt.bits.rxohlim || np->repflow)) {
+ 		atomic_inc(&skb->users);
+ 		ireq->pktopts = skb;
+ 	}
+ }
+ 
+ static struct dst_entry *tcp_v6_route_req(struct sock *sk, struct flowi *fl,
+ 					  const struct request_sock *req,
+ 					  bool *strict)
+ {
+ 	if (strict)
+ 		*strict = true;
+ 	return inet6_csk_route_req(sk, &fl->u.ip6, req);
+ }
+ 
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  struct request_sock_ops tcp6_request_sock_ops __read_mostly = {
  	.family		=	AF_INET6,
  	.obj_size	=	sizeof(struct tcp6_request_sock),
@@@ -723,15 -763,22 +766,26 @@@
  	.send_ack	=	tcp_v6_reqsk_send_ack,
  	.destructor	=	tcp_v6_reqsk_destructor,
  	.send_reset	=	tcp_v6_send_reset,
 -	.syn_ack_timeout =	tcp_syn_ack_timeout,
 +	.syn_ack_timeout = 	tcp_syn_ack_timeout,
  };
  
 -static const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {
  #ifdef CONFIG_TCP_MD5SIG
 +static const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {
  	.md5_lookup	=	tcp_v6_reqsk_md5_lookup,
  	.calc_md5_hash	=	tcp_v6_md5_hash_skb,
++<<<<<<< HEAD
++=======
+ #endif
+ 	.init_req	=	tcp_v6_init_req,
+ #ifdef CONFIG_SYN_COOKIES
+ 	.cookie_init_seq =	cookie_v6_init_sequence,
+ #endif
+ 	.route_req	=	tcp_v6_route_req,
+ 	.init_seq	=	tcp_v6_init_sequence,
+ 	.send_synack	=	tcp_v6_send_synack,
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  };
 +#endif
  
  static void tcp_v6_send_response(struct sk_buff *skb, u32 seq, u32 ack, u32 win,
  				 u32 tsval, u32 tsecr, int oif,
@@@ -1078,22 -1117,28 +1132,33 @@@ static int tcp_v6_conn_request(struct s
  			goto drop_and_release;
  		}
  
 -		isn = af_ops->init_seq(skb);
 -	}
 -
 -	if (!dst) {
 -		dst = af_ops->route_req(sk, (struct flowi *)&fl6, req, NULL);
 -		if (!dst)
 -			goto drop_and_free;
 +		isn = tcp_v6_init_sequence(skb);
  	}
 -
 +have_isn:
  	tcp_rsk(req)->snt_isn = isn;
++<<<<<<< HEAD
++=======
+ 	tcp_openreq_init_rwin(req, sk, dst);
+ 	fastopen = !want_cookie &&
+ 		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+ 	err = af_ops->send_synack(sk, dst, (struct flowi *)&fl6, req,
+ 				  skb_get_queue_mapping(skb), &foc);
+ 	if (!fastopen) {
+ 		if (err || want_cookie)
+ 			goto drop_and_free;
++>>>>>>> d6274bd8d6ea (tcp: add send_synack method to tcp_request_sock_ops)
  
 -		tcp_rsk(req)->listener = NULL;
 -		inet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
 -	}
 +	if (security_inet_conn_request(sk, skb, req))
 +		goto drop_and_release;
 +
 +	if (tcp_v6_send_synack(sk, dst, &fl6, req,
 +			       skb_get_queue_mapping(skb)) ||
 +	    want_cookie)
 +		goto drop_and_free;
 +
 +	tcp_rsk(req)->snt_synack = tcp_time_stamp;
 +	tcp_rsk(req)->listener = NULL;
 +	inet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
  	return 0;
  
  drop_and_release:
* Unmerged path include/net/tcp.h
* Unmerged path net/ipv4/tcp_ipv4.c
* Unmerged path net/ipv6/tcp_ipv6.c
