uprobes: Add the "enum rp_check ctx" arg to arch_uretprobe_is_alive()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Oleg Nesterov <oleg@redhat.com>
commit 86dcb702e74b8ab7d3b2d36984ef00671cea73b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/86dcb702.failed

arch/x86 doesn't care (so far), but as Pratyush Anand pointed
out other architectures might want why arch_uretprobe_is_alive()
was called and use different checks depending on the context.
Add the new argument to distinguish 2 callers.

	Tested-by: Pratyush Anand <panand@redhat.com>
	Signed-off-by: Oleg Nesterov <oleg@redhat.com>
	Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Acked-by: Anton Arapov <arapov@gmail.com>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: http://lkml.kernel.org/r/20150721134026.GA4779@redhat.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 86dcb702e74b8ab7d3b2d36984ef00671cea73b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/uprobes.c
#	include/linux/uprobes.h
#	kernel/events/uprobes.c
diff --cc arch/x86/kernel/uprobes.c
index 5d1cbfe4ae58,acf8b9010bbf..000000000000
--- a/arch/x86/kernel/uprobes.c
+++ b/arch/x86/kernel/uprobes.c
@@@ -926,3 -985,9 +926,12 @@@ arch_uretprobe_hijack_return_addr(unsig
  
  	return -1;
  }
++<<<<<<< HEAD
++=======
+ 
+ bool arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx,
+ 				struct pt_regs *regs)
+ {
+ 	return regs->sp <= ret->stack;
+ }
++>>>>>>> 86dcb702e74b (uprobes: Add the "enum rp_check ctx" arg to arch_uretprobe_is_alive())
diff --cc include/linux/uprobes.h
index 13a7f13ff0d3,c0a540239ab6..000000000000
--- a/include/linux/uprobes.h
+++ b/include/linux/uprobes.h
@@@ -75,31 -90,25 +75,41 @@@ struct uprobe_task 
  
  	struct return_instance		*return_instances;
  	unsigned int			depth;
 -};
 +	struct uprobe			*active_uprobe;
  
 -struct return_instance {
 -	struct uprobe		*uprobe;
 -	unsigned long		func;
 -	unsigned long		stack;		/* stack pointer */
 -	unsigned long		orig_ret_vaddr; /* original return address */
 -	bool			chained;	/* true, if instance is nested */
 +	unsigned long			xol_vaddr;
 +	unsigned long			vaddr;
 +};
  
 -	struct return_instance	*next;		/* keep as stack */
 +/*
 + * On a breakpoint hit, thread contests for a slot.  It frees the
 + * slot after singlestep. Currently a fixed number of slots are
 + * allocated.
 + */
 +struct xol_area {
 +	wait_queue_head_t 	wq;		/* if all slots are busy */
 +	atomic_t 		slot_count;	/* number of in-use slots */
 +	unsigned long 		*bitmap;	/* 0 = free slot */
 +	struct page 		*page;
 +
 +	/*
 +	 * We keep the vma's vm_start rather than a pointer to the vma
 +	 * itself.  The probed process or a naughty kernel module could make
 +	 * the vma go away, and we must handle that reasonably gracefully.
 +	 */
 +	unsigned long 		vaddr;		/* Page(s) of instruction slots */
  };
  
++<<<<<<< HEAD
++=======
+ enum rp_check {
+ 	RP_CHECK_CALL,
+ 	RP_CHECK_RET,
+ };
+ 
+ struct xol_area;
+ 
++>>>>>>> 86dcb702e74b (uprobes: Add the "enum rp_check ctx" arg to arch_uretprobe_is_alive())
  struct uprobes_state {
  	struct xol_area		*xol_area;
  };
@@@ -123,8 -134,19 +133,22 @@@ extern int uprobe_post_sstep_notifier(s
  extern int uprobe_pre_sstep_notifier(struct pt_regs *regs);
  extern void uprobe_notify_resume(struct pt_regs *regs);
  extern bool uprobe_deny_signal(void);
 -extern bool arch_uprobe_skip_sstep(struct arch_uprobe *aup, struct pt_regs *regs);
 +extern bool __weak arch_uprobe_skip_sstep(struct arch_uprobe *aup, struct pt_regs *regs);
  extern void uprobe_clear_state(struct mm_struct *mm);
++<<<<<<< HEAD
++=======
+ extern int  arch_uprobe_analyze_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long addr);
+ extern int  arch_uprobe_pre_xol(struct arch_uprobe *aup, struct pt_regs *regs);
+ extern int  arch_uprobe_post_xol(struct arch_uprobe *aup, struct pt_regs *regs);
+ extern bool arch_uprobe_xol_was_trapped(struct task_struct *tsk);
+ extern int  arch_uprobe_exception_notify(struct notifier_block *self, unsigned long val, void *data);
+ extern void arch_uprobe_abort_xol(struct arch_uprobe *aup, struct pt_regs *regs);
+ extern unsigned long arch_uretprobe_hijack_return_addr(unsigned long trampoline_vaddr, struct pt_regs *regs);
+ extern bool arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx, struct pt_regs *regs);
+ extern bool arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs);
+ extern void arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,
+ 					 void *src, unsigned long len);
++>>>>>>> 86dcb702e74b (uprobes: Add the "enum rp_check ctx" arg to arch_uretprobe_is_alive())
  #else /* !CONFIG_UPROBES */
  struct uprobes_state {
  };
diff --cc kernel/events/uprobes.c
index 969436d08697,df5661a44e35..000000000000
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@@ -1740,28 -1799,24 +1742,40 @@@ static void handle_trampoline(struct pt
  	if (!ri)
  		goto sigill;
  
++<<<<<<< HEAD
 +	/*
 +	 * TODO: we should throw out return_instance's invalidated by
 +	 * longjmp(), currently we assume that the probed function always
 +	 * returns.
 +	 */
 +	instruction_pointer_set(regs, ri->orig_ret_vaddr);
++=======
+ 	do {
+ 		/*
+ 		 * We should throw out the frames invalidated by longjmp().
+ 		 * If this chain is valid, then the next one should be alive
+ 		 * or NULL; the latter case means that nobody but ri->func
+ 		 * could hit this trampoline on return. TODO: sigaltstack().
+ 		 */
+ 		next = find_next_ret_chain(ri);
+ 		valid = !next || arch_uretprobe_is_alive(next, RP_CHECK_RET, regs);
++>>>>>>> 86dcb702e74b (uprobes: Add the "enum rp_check ctx" arg to arch_uretprobe_is_alive())
 +
 +	for (;;) {
 +		handle_uretprobe_chain(ri, regs);
 +
 +		chained = ri->chained;
 +		put_uprobe(ri->uprobe);
 +
 +		tmp = ri;
 +		ri = ri->next;
 +		kfree(tmp);
 +		utask->depth--;
  
 -		instruction_pointer_set(regs, ri->orig_ret_vaddr);
 -		do {
 -			if (valid)
 -				handle_uretprobe_chain(ri, regs);
 -			ri = free_ret_instance(ri);
 -			utask->depth--;
 -		} while (ri != next);
 -	} while (!valid);
 +		if (!chained)
 +			break;
 +		BUG_ON(!ri);
 +	}
  
  	utask->return_instances = ri;
  	return;
@@@ -1772,6 -1827,17 +1786,20 @@@
  
  }
  
++<<<<<<< HEAD
++=======
+ bool __weak arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs)
+ {
+ 	return false;
+ }
+ 
+ bool __weak arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx,
+ 					struct pt_regs *regs)
+ {
+ 	return true;
+ }
+ 
++>>>>>>> 86dcb702e74b (uprobes: Add the "enum rp_check ctx" arg to arch_uretprobe_is_alive())
  /*
   * Run handler and ask thread to singlestep.
   * Ensure all non-fatal signals cannot interrupt thread while it singlesteps.
* Unmerged path arch/x86/kernel/uprobes.c
* Unmerged path include/linux/uprobes.h
* Unmerged path kernel/events/uprobes.c
