percpu: make pcpu_alloc_area() capable of allocating only from populated areas

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Tejun Heo <tj@kernel.org>
commit a16037c8dfc2734c1a2c8e3ffd4766ed25f2a41d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/a16037c8.failed

Update pcpu_alloc_area() so that it can skip unpopulated areas if the
new parameter @pop_only is true.  This is implemented by a new
function, pcpu_fit_in_area(), which determines the amount of head
padding considering the alignment and populated state.

@pop_only is currently always false but this will be used to implement
atomic allocation.

	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit a16037c8dfc2734c1a2c8e3ffd4766ed25f2a41d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index cb806a43465f,e18aa143aab1..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -400,44 -400,52 +400,93 @@@ out_unlock
  }
  
  /**
++<<<<<<< HEAD
 + * pcpu_split_block - split a map block
 + * @chunk: chunk of interest
 + * @i: index of map block to split
 + * @head: head size in bytes (can be 0)
 + * @tail: tail size in bytes (can be 0)
 + *
 + * Split the @i'th map block into two or three blocks.  If @head is
 + * non-zero, @head bytes block is inserted before block @i moving it
 + * to @i+1 and reducing its size by @head bytes.
 + *
 + * If @tail is non-zero, the target block, which can be @i or @i+1
 + * depending on @head, is reduced by @tail bytes and @tail byte block
 + * is inserted after the target block.
 + *
 + * @chunk->map must have enough free slots to accommodate the split.
 + *
 + * CONTEXT:
 + * pcpu_lock.
 + */
 +static void pcpu_split_block(struct pcpu_chunk *chunk, int i,
 +			     int head, int size, int tail)
 +{
 +	int nr_extra = !!head + !!tail;
 +	int off;
 +
 +	BUG_ON(chunk->map_alloc <= chunk->map_used + nr_extra);
 +
 +	/* insert new subblocks */
 +	memmove(&chunk->map[i + nr_extra] + 1, &chunk->map[i] + 1,
 +		sizeof(chunk->map[0]) * (chunk->map_used - i));
 +	chunk->map_used += nr_extra;
 +
 +	off = chunk->map[i];
 +
 +	if (head)
 +		chunk->map[++i] = off += head;
 +	if (tail)
 +		chunk->map[++i] = off += size;
++=======
+  * pcpu_fit_in_area - try to fit the requested allocation in a candidate area
+  * @chunk: chunk the candidate area belongs to
+  * @off: the offset to the start of the candidate area
+  * @this_size: the size of the candidate area
+  * @size: the size of the target allocation
+  * @align: the alignment of the target allocation
+  * @pop_only: only allocate from already populated region
+  *
+  * We're trying to allocate @size bytes aligned at @align.  @chunk's area
+  * at @off sized @this_size is a candidate.  This function determines
+  * whether the target allocation fits in the candidate area and returns the
+  * number of bytes to pad after @off.  If the target area doesn't fit, -1
+  * is returned.
+  *
+  * If @pop_only is %true, this function only considers the already
+  * populated part of the candidate area.
+  */
+ static int pcpu_fit_in_area(struct pcpu_chunk *chunk, int off, int this_size,
+ 			    int size, int align, bool pop_only)
+ {
+ 	int cand_off = off;
+ 
+ 	while (true) {
+ 		int head = ALIGN(cand_off, align) - off;
+ 		int page_start, page_end, rs, re;
+ 
+ 		if (this_size < head + size)
+ 			return -1;
+ 
+ 		if (!pop_only)
+ 			return head;
+ 
+ 		/*
+ 		 * If the first unpopulated page is beyond the end of the
+ 		 * allocation, the whole allocation is populated;
+ 		 * otherwise, retry from the end of the unpopulated area.
+ 		 */
+ 		page_start = PFN_DOWN(head + off);
+ 		page_end = PFN_UP(head + off + size);
+ 
+ 		rs = page_start;
+ 		pcpu_next_unpop(chunk, &rs, &re, PFN_UP(off + this_size));
+ 		if (rs >= page_end)
+ 			return head;
+ 		cand_off = re * PAGE_SIZE;
+ 	}
++>>>>>>> a16037c8dfc2 (percpu: make pcpu_alloc_area() capable of allocating only from populated areas)
  }
  
  /**
@@@ -475,11 -485,11 +526,19 @@@ static int pcpu_alloc_area(struct pcpu_
  		if (off & 1)
  			continue;
  
++<<<<<<< HEAD
 +		/* extra for alignment requirement */
 +		head = ALIGN(off, align) - off;
 +
 +		this_size = (chunk->map[i + 1] & ~1) - off;
 +		if (this_size < head + size) {
++=======
+ 		this_size = (p[1] & ~1) - off;
+ 
+ 		head = pcpu_fit_in_area(chunk, off, this_size, size, align,
+ 					pop_only);
+ 		if (head < 0) {
++>>>>>>> a16037c8dfc2 (percpu: make pcpu_alloc_area() capable of allocating only from populated areas)
  			if (!seen_free) {
  				chunk->first_free = i;
  				seen_free = true;
* Unmerged path mm/percpu.c
