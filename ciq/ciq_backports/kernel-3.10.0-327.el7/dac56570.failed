KVM: PPC: Remove page table walk helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] kvm: Remove page table walk helpers (Gustavo Duarte) [1233071]
Rebuild_FUZZ: 93.33%
commit-author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
commit dac5657067919161eb3273ca787d8ae9814801e7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/dac56570.failed

This patch remove helpers which we had used only once in the code.
Limiting page table walk variants help in ensuring that we won't
end up with code walking page table with wrong assumptions.

	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit dac5657067919161eb3273ca787d8ae9814801e7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv_rm_mmu.c
#	arch/powerpc/kvm/e500_mmu_host.c
diff --cc arch/powerpc/kvm/book3s_hv_rm_mmu.c
index f1ee45edf8d1,73e083cb9f7e..000000000000
--- a/arch/powerpc/kvm/book3s_hv_rm_mmu.c
+++ b/arch/powerpc/kvm/book3s_hv_rm_mmu.c
@@@ -135,29 -131,10 +135,33 @@@ static void remove_revmap_chain(struct 
  	unlock_rmap(rmap);
  }
  
++<<<<<<< HEAD
 +static pte_t lookup_linux_pte_and_update(pgd_t *pgdir, unsigned long hva,
 +			      int writing, unsigned long *pte_sizep)
 +{
 +	pte_t *ptep;
 +	unsigned long ps = *pte_sizep;
 +	unsigned int hugepage_shift;
 +
 +	ptep = find_linux_pte_or_hugepte(pgdir, hva, &hugepage_shift);
 +	if (!ptep)
 +		return __pte(0);
 +	if (hugepage_shift)
 +		*pte_sizep = 1ul << hugepage_shift;
 +	else
 +		*pte_sizep = PAGE_SIZE;
 +	if (ps > *pte_sizep)
 +		return __pte(0);
 +	return kvmppc_read_update_linux_pte(ptep, writing, hugepage_shift);
 +}
 +
 +static inline void unlock_hpte(unsigned long *hpte, unsigned long hpte_v)
++=======
+ static inline void unlock_hpte(__be64 *hpte, unsigned long hpte_v)
++>>>>>>> dac565706791 (KVM: PPC: Remove page table walk helpers)
  {
  	asm volatile(PPC_RELEASE_BARRIER "" : : : "memory");
 -	hpte[0] = cpu_to_be64(hpte_v);
 +	hpte[0] = hpte_v;
  }
  
  long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,
@@@ -170,10 -147,10 +174,14 @@@
  	struct revmap_entry *rev;
  	unsigned long g_ptel;
  	struct kvm_memory_slot *memslot;
++<<<<<<< HEAD
 +	unsigned long *physp, pte_size;
++=======
+ 	unsigned hpage_shift;
++>>>>>>> dac565706791 (KVM: PPC: Remove page table walk helpers)
  	unsigned long is_io;
  	unsigned long *rmap;
- 	pte_t pte;
+ 	pte_t *ptep;
  	unsigned int writing;
  	unsigned long mmu_seq;
  	unsigned long rcbits;
@@@ -213,35 -187,32 +221,62 @@@
  	slot_fn = gfn - memslot->base_gfn;
  	rmap = &memslot->arch.rmap[slot_fn];
  
++<<<<<<< HEAD
 +	if (!kvm->arch.using_mmu_notifiers) {
 +		physp = memslot->arch.slot_phys;
 +		if (!physp)
 +			return H_PARAMETER;
 +		physp += slot_fn;
 +		if (realmode)
 +			physp = real_vmalloc_addr(physp);
 +		pa = *physp;
 +		if (!pa)
 +			return H_TOO_HARD;
 +		is_io = pa & (HPTE_R_I | HPTE_R_W);
 +		pte_size = PAGE_SIZE << (pa & KVMPPC_PAGE_ORDER_MASK);
 +		pa &= PAGE_MASK;
 +		pa |= gpa & ~PAGE_MASK;
 +	} else {
 +		/* Translate to host virtual address */
 +		hva = __gfn_to_hva_memslot(memslot, gfn);
 +
 +		/* Look up the Linux PTE for the backing page */
 +		pte_size = psize;
 +		pte = lookup_linux_pte_and_update(pgdir, hva, writing,
 +						  &pte_size);
 +		if (pte_present(pte) && !pte_numa(pte)) {
++=======
+ 	/* Translate to host virtual address */
+ 	hva = __gfn_to_hva_memslot(memslot, gfn);
+ 	ptep = find_linux_pte_or_hugepte(pgdir, hva, &hpage_shift);
+ 	if (ptep) {
+ 		pte_t pte;
+ 		unsigned int host_pte_size;
+ 
+ 		if (hpage_shift)
+ 			host_pte_size = 1ul << hpage_shift;
+ 		else
+ 			host_pte_size = PAGE_SIZE;
+ 		/*
+ 		 * We should always find the guest page size
+ 		 * to <= host page size, if host is using hugepage
+ 		 */
+ 		if (host_pte_size < psize)
+ 			return H_PARAMETER;
+ 
+ 		pte = kvmppc_read_update_linux_pte(ptep, writing, hpage_shift);
+ 		if (pte_present(pte) && !pte_protnone(pte)) {
++>>>>>>> dac565706791 (KVM: PPC: Remove page table walk helpers)
  			if (writing && !pte_write(pte))
  				/* make the actual HPTE be read-only */
  				ptel = hpte_make_readonly(ptel);
  			is_io = hpte_cache_bits(pte_val(pte));
  			pa = pte_pfn(pte) << PAGE_SHIFT;
++<<<<<<< HEAD
 +			pa |= hva & (pte_size - 1);
++=======
+ 			pa |= hva & (host_pte_size - 1);
++>>>>>>> dac565706791 (KVM: PPC: Remove page table walk helpers)
  			pa |= gpa & ~PAGE_MASK;
  		}
  	}
diff --cc arch/powerpc/kvm/e500_mmu_host.c
index 82aafd61162a,a1f5b0d4b1d6..000000000000
--- a/arch/powerpc/kvm/e500_mmu_host.c
+++ b/arch/powerpc/kvm/e500_mmu_host.c
@@@ -449,7 -460,29 +449,33 @@@ static inline int kvmppc_e500_shadow_ma
  		gvaddr &= ~((tsize_pages << PAGE_SHIFT) - 1);
  	}
  
++<<<<<<< HEAD
 +	kvmppc_e500_ref_setup(ref, gtlbe, pfn);
++=======
+ 	spin_lock(&kvm->mmu_lock);
+ 	if (mmu_notifier_retry(kvm, mmu_seq)) {
+ 		ret = -EAGAIN;
+ 		goto out;
+ 	}
+ 
+ 
+ 	pgdir = vcpu_e500->vcpu.arch.pgdir;
+ 	ptep = find_linux_pte_or_hugepte(pgdir, hva, NULL);
+ 	if (ptep) {
+ 		pte_t pte = READ_ONCE(*ptep);
+ 
+ 		if (pte_present(pte))
+ 			wimg = (pte_val(pte) >> PTE_WIMGE_SHIFT) &
+ 				MAS2_WIMGE_MASK;
+ 		else {
+ 			pr_err_ratelimited("%s: pte not present: gfn %lx,pfn %lx\n",
+ 					   __func__, (long)gfn, pfn);
+ 			ret = -EINVAL;
+ 			goto out;
+ 		}
+ 	}
+ 	kvmppc_e500_ref_setup(ref, gtlbe, pfn, wimg);
++>>>>>>> dac565706791 (KVM: PPC: Remove page table walk helpers)
  
  	kvmppc_e500_setup_stlbe(&vcpu_e500->vcpu, gtlbe, tsize,
  				ref, gvaddr, stlbe);
diff --git a/arch/powerpc/include/asm/pgtable.h b/arch/powerpc/include/asm/pgtable.h
index 4fbc04254e64..78784a7a2379 100644
--- a/arch/powerpc/include/asm/pgtable.h
+++ b/arch/powerpc/include/asm/pgtable.h
@@ -312,27 +312,6 @@ extern int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr,
 #endif
 pte_t *find_linux_pte_or_hugepte(pgd_t *pgdir, unsigned long ea,
 				 unsigned *shift);
-
-static inline pte_t *lookup_linux_ptep(pgd_t *pgdir, unsigned long hva,
-				     unsigned long *pte_sizep)
-{
-	pte_t *ptep;
-	unsigned long ps = *pte_sizep;
-	unsigned int shift;
-
-	ptep = find_linux_pte_or_hugepte(pgdir, hva, &shift);
-	if (!ptep)
-		return NULL;
-	if (shift)
-		*pte_sizep = 1ul << shift;
-	else
-		*pte_sizep = PAGE_SIZE;
-
-	if (ps > *pte_sizep)
-		return NULL;
-
-	return ptep;
-}
 #endif /* __ASSEMBLY__ */
 
 #endif /* __KERNEL__ */
* Unmerged path arch/powerpc/kvm/book3s_hv_rm_mmu.c
* Unmerged path arch/powerpc/kvm/e500_mmu_host.c
