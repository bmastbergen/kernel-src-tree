xfs: use generic percpu counters for free block counter

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Dave Chinner <david@fromorbit.com>
commit 0d485ada404b3614b045e574bec26aaf5d9b3c5b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/0d485ada.failed

XFS has hand-rolled per-cpu counters for the superblock since before
there was any generic implementation. The free block counter is
special in that it is used for ENOSPC detection outside transaction
contexts for for delayed allocation. This means that the counter
needs to be accurate at zero. The current per-cpu counter code jumps
through lots of hoops to ensure we never run past zero, but we don't
need to make all those jumps with the generic counter
implementation.

The generic counter implementation allows us to pass a "batch"
threshold at which the addition/subtraction to the counter value
will be folded back into global value under lock. We can use this
feature to reduce the batch size as we approach 0 in a very similar
manner to the existing counters and their rebalance algorithm. If we
use a batch size of 1 as we approach 0, then every addition and
subtraction will be done against the global value and hence allow
accurate detection of zero threshold crossing.

Hence we can replace the handrolled, accurate-at-zero counters with
generic percpu counters.

Note: this removes just enough of the icsb infrastructure to compile
without warnings. The rest will go in subsequent commits.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 0d485ada404b3614b045e574bec26aaf5d9b3c5b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/libxfs/xfs_sb.c
#	fs/xfs/xfs_fsops.c
#	fs/xfs/xfs_mount.c
#	fs/xfs/xfs_mount.h
#	fs/xfs/xfs_super.c
diff --cc fs/xfs/libxfs/xfs_sb.c
index 07f47e5690d3,31a3e972f86f..000000000000
--- a/fs/xfs/libxfs/xfs_sb.c
+++ b/fs/xfs/libxfs/xfs_sb.c
@@@ -826,42 -759,55 +826,48 @@@ xfs_initialize_perag_data
  }
  
  /*
 - * xfs_log_sb() can be used to copy arbitrary changes to the in-core superblock
 - * into the superblock buffer to be logged.  It does not provide the higher
 - * level of locking that is needed to protect the in-core superblock from
 - * concurrent access.
 + * xfs_mod_sb() can be used to copy arbitrary changes to the
 + * in-core superblock into the superblock buffer to be logged.
 + * It does not provide the higher level of locking that is
 + * needed to protect the in-core superblock from concurrent
 + * access.
   */
  void
 -xfs_log_sb(
 -	struct xfs_trans	*tp)
 +xfs_mod_sb(xfs_trans_t *tp, __int64_t fields)
  {
 -	struct xfs_mount	*mp = tp->t_mountp;
 -	struct xfs_buf		*bp = xfs_trans_getsb(tp, mp, 0);
 +	xfs_buf_t	*bp;
 +	int		first;
 +	int		last;
 +	xfs_mount_t	*mp;
 +	xfs_sb_field_t	f;
 +
++<<<<<<< HEAD
 +	ASSERT(fields);
 +	if (!fields)
 +		return;
 +	mp = tp->t_mountp;
 +	bp = xfs_trans_getsb(tp, mp, 0);
 +	first = sizeof(xfs_sb_t);
 +	last = 0;
 +
 +	/* translate/copy */
 +
 +	xfs_sb_to_disk(XFS_BUF_TO_SBP(bp), &mp->m_sb, fields);
 +
 +	/* find modified range */
 +	f = (xfs_sb_field_t)xfs_highbit64((__uint64_t)fields);
 +	ASSERT((1LL << f) & XFS_SB_MOD_BITS);
 +	last = xfs_sb_info[f + 1].offset - 1;
  
 +	f = (xfs_sb_field_t)xfs_lowbit64((__uint64_t)fields);
 +	ASSERT((1LL << f) & XFS_SB_MOD_BITS);
 +	first = xfs_sb_info[f].offset;
++=======
+ 	mp->m_sb.sb_icount = percpu_counter_sum(&mp->m_icount);
+ 	mp->m_sb.sb_ifree = percpu_counter_sum(&mp->m_ifree);
+ 	mp->m_sb.sb_fdblocks = percpu_counter_sum(&mp->m_fdblocks);
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  
 -	xfs_sb_to_disk(XFS_BUF_TO_SBP(bp), &mp->m_sb);
  	xfs_trans_buf_set_type(tp, bp, XFS_BLFT_SB_BUF);
 -	xfs_trans_log_buf(tp, bp, 0, sizeof(struct xfs_dsb));
 -}
 -
 -/*
 - * xfs_sync_sb
 - *
 - * Sync the superblock to disk.
 - *
 - * Note that the caller is responsible for checking the frozen state of the
 - * filesystem. This procedure uses the non-blocking transaction allocator and
 - * thus will allow modifications to a frozen fs. This is required because this
 - * code can be called during the process of freezing where use of the high-level
 - * allocator would deadlock.
 - */
 -int
 -xfs_sync_sb(
 -	struct xfs_mount	*mp,
 -	bool			wait)
 -{
 -	struct xfs_trans	*tp;
 -	int			error;
 -
 -	tp = _xfs_trans_alloc(mp, XFS_TRANS_SB_CHANGE, KM_SLEEP);
 -	error = xfs_trans_reserve(tp, &M_RES(mp)->tr_sb, 0, 0);
 -	if (error) {
 -		xfs_trans_cancel(tp, 0);
 -		return error;
 -	}
 -
 -	xfs_log_sb(tp);
 -	if (wait)
 -		xfs_trans_set_sync(tp);
 -	return xfs_trans_commit(tp, 0);
 +	xfs_trans_log_buf(tp, bp, first, last);
  }
diff --cc fs/xfs/xfs_fsops.c
index 5f4d1f6b208b,7ef25588062f..000000000000
--- a/fs/xfs/xfs_fsops.c
+++ b/fs/xfs/xfs_fsops.c
@@@ -648,11 -638,13 +648,18 @@@ xfs_fs_counts
  	xfs_fsop_counts_t	*cnt)
  {
  	xfs_icsb_sync_counters(mp, XFS_ICSB_LAZY_COUNT);
++<<<<<<< HEAD
++=======
+ 	cnt->allocino = percpu_counter_read_positive(&mp->m_icount);
+ 	cnt->freeino = percpu_counter_read_positive(&mp->m_ifree);
+ 	cnt->freedata = percpu_counter_read_positive(&mp->m_fdblocks) -
+ 							XFS_ALLOC_SET_ASIDE(mp);
+ 
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	spin_lock(&mp->m_sb_lock);
- 	cnt->freedata = mp->m_sb.sb_fdblocks - XFS_ALLOC_SET_ASIDE(mp);
  	cnt->freertx = mp->m_sb.sb_frextents;
 +	cnt->freeino = mp->m_sb.sb_ifree;
 +	cnt->allocino = mp->m_sb.sb_icount;
  	spin_unlock(&mp->m_sb_lock);
  	return 0;
  }
@@@ -765,9 -758,8 +773,14 @@@ out
  		 * the extra reserve blocks from the reserve.....
  		 */
  		int error;
++<<<<<<< HEAD
 +		error = xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS,
 +						 fdblks_delta, 0);
 +		if (error == ENOSPC)
++=======
+ 		error = xfs_mod_fdblocks(mp, fdblks_delta, 0);
+ 		if (error == -ENOSPC)
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  			goto retry;
  	}
  	return 0;
diff --cc fs/xfs/xfs_mount.c
index 1f0460bd27b8,767c09a5d3ff..000000000000
--- a/fs/xfs/xfs_mount.c
+++ b/fs/xfs/xfs_mount.c
@@@ -1123,19 -1096,123 +1123,121 @@@ xfs_log_sbcount(xfs_mount_t *mp
  	if (!xfs_sb_version_haslazysbcount(&mp->m_sb))
  		return 0;
  
 -	return xfs_sync_sb(mp, true);
 -}
 -
 -int
 -xfs_mod_icount(
 -	struct xfs_mount	*mp,
 -	int64_t			delta)
 -{
 -	/* deltas are +/-64, hence the large batch size of 128. */
 -	__percpu_counter_add(&mp->m_icount, delta, 128);
 -	if (percpu_counter_compare(&mp->m_icount, 0) < 0) {
 -		ASSERT(0);
 -		percpu_counter_add(&mp->m_icount, -delta);
 -		return -EINVAL;
 +	tp = _xfs_trans_alloc(mp, XFS_TRANS_SB_COUNT, KM_SLEEP);
 +	error = xfs_trans_reserve(tp, &M_RES(mp)->tr_sb, 0, 0);
 +	if (error) {
 +		xfs_trans_cancel(tp, 0);
 +		return error;
  	}
 -	return 0;
 +
 +	xfs_mod_sb(tp, XFS_SB_IFREE | XFS_SB_ICOUNT | XFS_SB_FDBLOCKS);
 +	xfs_trans_set_sync(tp);
 +	error = xfs_trans_commit(tp, 0);
 +	return error;
  }
  
++<<<<<<< HEAD
++=======
+ int
+ xfs_mod_ifree(
+ 	struct xfs_mount	*mp,
+ 	int64_t			delta)
+ {
+ 	percpu_counter_add(&mp->m_ifree, delta);
+ 	if (percpu_counter_compare(&mp->m_ifree, 0) < 0) {
+ 		ASSERT(0);
+ 		percpu_counter_add(&mp->m_ifree, -delta);
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ int
+ xfs_mod_fdblocks(
+ 	struct xfs_mount	*mp,
+ 	int64_t			delta,
+ 	bool			rsvd)
+ {
+ 	int64_t			lcounter;
+ 	long long		res_used;
+ 	s32			batch;
+ 
+ 	if (delta > 0) {
+ 		/*
+ 		 * If the reserve pool is depleted, put blocks back into it
+ 		 * first. Most of the time the pool is full.
+ 		 */
+ 		if (likely(mp->m_resblks == mp->m_resblks_avail)) {
+ 			percpu_counter_add(&mp->m_fdblocks, delta);
+ 			return 0;
+ 		}
+ 
+ 		spin_lock(&mp->m_sb_lock);
+ 		res_used = (long long)(mp->m_resblks - mp->m_resblks_avail);
+ 
+ 		if (res_used > delta) {
+ 			mp->m_resblks_avail += delta;
+ 		} else {
+ 			delta -= res_used;
+ 			mp->m_resblks_avail = mp->m_resblks;
+ 			percpu_counter_add(&mp->m_fdblocks, delta);
+ 		}
+ 		spin_unlock(&mp->m_sb_lock);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * Taking blocks away, need to be more accurate the closer we
+ 	 * are to zero.
+ 	 *
+ 	 * batch size is set to a maximum of 1024 blocks - if we are
+ 	 * allocating of freeing extents larger than this then we aren't
+ 	 * going to be hammering the counter lock so a lock per update
+ 	 * is not a problem.
+ 	 *
+ 	 * If the counter has a value of less than 2 * max batch size,
+ 	 * then make everything serialise as we are real close to
+ 	 * ENOSPC.
+ 	 */
+ #define __BATCH	1024
+ 	if (percpu_counter_compare(&mp->m_fdblocks, 2 * __BATCH) < 0)
+ 		batch = 1;
+ 	else
+ 		batch = __BATCH;
+ #undef __BATCH
+ 
+ 	__percpu_counter_add(&mp->m_fdblocks, delta, batch);
+ 	if (percpu_counter_compare(&mp->m_fdblocks,
+ 				   XFS_ALLOC_SET_ASIDE(mp)) >= 0) {
+ 		/* we had space! */
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * lock up the sb for dipping into reserves before releasing the space
+ 	 * that took us to ENOSPC.
+ 	 */
+ 	spin_lock(&mp->m_sb_lock);
+ 	percpu_counter_add(&mp->m_fdblocks, -delta);
+ 	if (!rsvd)
+ 		goto fdblocks_enospc;
+ 
+ 	lcounter = (long long)mp->m_resblks_avail + delta;
+ 	if (lcounter >= 0) {
+ 		mp->m_resblks_avail = lcounter;
+ 		spin_unlock(&mp->m_sb_lock);
+ 		return 0;
+ 	}
+ 	printk_once(KERN_WARNING
+ 		"Filesystem \"%s\": reserve blocks depleted! "
+ 		"Consider increasing reserve pool size.",
+ 		mp->m_fsname);
+ fdblocks_enospc:
+ 	spin_unlock(&mp->m_sb_lock);
+ 	return -ENOSPC;
+ }
+ 
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  /*
   * xfs_mod_incore_sb_unlocked() is a utility routine commonly used to apply
   * a delta to a specified field in the in-core superblock.  Simply
@@@ -1164,65 -1240,10 +1265,71 @@@ xfs_mod_incore_sb_unlocked
  	 */
  	switch (field) {
  	case XFS_SBS_ICOUNT:
 +		lcounter = (long long)mp->m_sb.sb_icount;
 +		lcounter += delta;
 +		if (lcounter < 0) {
 +			ASSERT(0);
 +			return XFS_ERROR(EINVAL);
 +		}
 +		mp->m_sb.sb_icount = lcounter;
 +		return 0;
  	case XFS_SBS_IFREE:
++<<<<<<< HEAD
 +		lcounter = (long long)mp->m_sb.sb_ifree;
 +		lcounter += delta;
 +		if (lcounter < 0) {
 +			ASSERT(0);
 +			return XFS_ERROR(EINVAL);
 +		}
 +		mp->m_sb.sb_ifree = lcounter;
 +		return 0;
 +	case XFS_SBS_FDBLOCKS:
 +		lcounter = (long long)
 +			mp->m_sb.sb_fdblocks - XFS_ALLOC_SET_ASIDE(mp);
 +		res_used = (long long)(mp->m_resblks - mp->m_resblks_avail);
 +
 +		if (delta > 0) {		/* Putting blocks back */
 +			if (res_used > delta) {
 +				mp->m_resblks_avail += delta;
 +			} else {
 +				rem = delta - res_used;
 +				mp->m_resblks_avail = mp->m_resblks;
 +				lcounter += rem;
 +			}
 +		} else {				/* Taking blocks away */
 +			lcounter += delta;
 +			if (lcounter >= 0) {
 +				mp->m_sb.sb_fdblocks = lcounter +
 +							XFS_ALLOC_SET_ASIDE(mp);
 +				return 0;
 +			}
 +
 +			/*
 +			 * We are out of blocks, use any available reserved
 +			 * blocks if were allowed to.
 +			 */
 +			if (!rsvd)
 +				return XFS_ERROR(ENOSPC);
 +
 +			lcounter = (long long)mp->m_resblks_avail + delta;
 +			if (lcounter >= 0) {
 +				mp->m_resblks_avail = lcounter;
 +				return 0;
 +			}
 +			printk_once(KERN_WARNING
 +				"Filesystem \"%s\": reserve blocks depleted! "
 +				"Consider increasing reserve pool size.",
 +				mp->m_fsname);
 +			return XFS_ERROR(ENOSPC);
 +		}
 +
 +		mp->m_sb.sb_fdblocks = lcounter + XFS_ALLOC_SET_ASIDE(mp);
 +		return 0;
++=======
+ 	case XFS_SBS_FDBLOCKS:
+ 		ASSERT(0);
+ 		return -EINVAL;
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	case XFS_SBS_FREXTENTS:
  		lcounter = (long long)mp->m_sb.sb_frextents;
  		lcounter += delta;
@@@ -1557,9 -1551,6 +1664,12 @@@ xfs_icsb_cpu_notify
  	case CPU_ONLINE:
  	case CPU_ONLINE_FROZEN:
  		xfs_icsb_lock(mp);
++<<<<<<< HEAD
 +		xfs_icsb_balance_counter(mp, XFS_SBS_ICOUNT, 0);
 +		xfs_icsb_balance_counter(mp, XFS_SBS_IFREE, 0);
 +		xfs_icsb_balance_counter(mp, XFS_SBS_FDBLOCKS, 0);
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  		xfs_icsb_unlock(mp);
  		break;
  	case CPU_DEAD:
@@@ -1569,19 -1560,9 +1679,25 @@@
  		 * re-enable the counters. */
  		xfs_icsb_lock(mp);
  		spin_lock(&mp->m_sb_lock);
++<<<<<<< HEAD
 +		xfs_icsb_disable_counter(mp, XFS_SBS_ICOUNT);
 +		xfs_icsb_disable_counter(mp, XFS_SBS_IFREE);
 +		xfs_icsb_disable_counter(mp, XFS_SBS_FDBLOCKS);
 +
 +		mp->m_sb.sb_icount += cntp->icsb_icount;
 +		mp->m_sb.sb_ifree += cntp->icsb_ifree;
 +		mp->m_sb.sb_fdblocks += cntp->icsb_fdblocks;
  
  		memset(cntp, 0, sizeof(xfs_icsb_cnts_t));
  
 +		xfs_icsb_balance_counter_locked(mp, XFS_SBS_ICOUNT, 0);
 +		xfs_icsb_balance_counter_locked(mp, XFS_SBS_IFREE, 0);
 +		xfs_icsb_balance_counter_locked(mp, XFS_SBS_FDBLOCKS, 0);
++=======
++
++		memset(cntp, 0, sizeof(xfs_icsb_cnts_t));
++
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  		spin_unlock(&mp->m_sb_lock);
  		xfs_icsb_unlock(mp);
  		break;
@@@ -1596,11 -1577,26 +1712,31 @@@ xfs_icsb_init_counters
  	xfs_mount_t	*mp)
  {
  	xfs_icsb_cnts_t *cntp;
 -	int		error;
  	int		i;
  
++<<<<<<< HEAD
 +	mp->m_sb_cnts = alloc_percpu(xfs_icsb_cnts_t);
 +	if (mp->m_sb_cnts == NULL)
 +		return -ENOMEM;
++=======
+ 	error = percpu_counter_init(&mp->m_icount, 0, GFP_KERNEL);
+ 	if (error)
+ 		return error;
+ 
+ 	error = percpu_counter_init(&mp->m_ifree, 0, GFP_KERNEL);
+ 	if (error)
+ 		goto free_icount;
+ 
+ 	error = percpu_counter_init(&mp->m_fdblocks, 0, GFP_KERNEL);
+ 	if (error)
+ 		goto free_ifree;
+ 
+ 	mp->m_sb_cnts = alloc_percpu(xfs_icsb_cnts_t);
+ 	if (!mp->m_sb_cnts) {
+ 		error = -ENOMEM;
+ 		goto free_fdblocks;
+ 	}
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  
  	for_each_online_cpu(i) {
  		cntp = (xfs_icsb_cnts_t *)per_cpu_ptr(mp->m_sb_cnts, i);
@@@ -1622,21 -1618,30 +1758,42 @@@
  #endif /* CONFIG_HOTPLUG_CPU */
  
  	return 0;
++<<<<<<< HEAD
++=======
+ 
+ free_fdblocks:
+ 	percpu_counter_destroy(&mp->m_fdblocks);
+ free_ifree:
+ 	percpu_counter_destroy(&mp->m_ifree);
+ free_icount:
+ 	percpu_counter_destroy(&mp->m_icount);
+ 	return error;
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  }
  
  void
  xfs_icsb_reinit_counters(
  	xfs_mount_t	*mp)
  {
++<<<<<<< HEAD
++=======
+ 	percpu_counter_set(&mp->m_icount, mp->m_sb.sb_icount);
+ 	percpu_counter_set(&mp->m_ifree, mp->m_sb.sb_ifree);
+ 	percpu_counter_set(&mp->m_fdblocks, mp->m_sb.sb_fdblocks);
+ 
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	xfs_icsb_lock(mp);
  	/*
  	 * start with all counters disabled so that the
  	 * initial balance kicks us off correctly
  	 */
  	mp->m_icsb_counters = -1;
++<<<<<<< HEAD
 +	xfs_icsb_balance_counter(mp, XFS_SBS_ICOUNT, 0);
 +	xfs_icsb_balance_counter(mp, XFS_SBS_IFREE, 0);
 +	xfs_icsb_balance_counter(mp, XFS_SBS_FDBLOCKS, 0);
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	xfs_icsb_unlock(mp);
  }
  
@@@ -1648,6 -1653,11 +1805,14 @@@ xfs_icsb_destroy_counters
  		unregister_hotcpu_notifier(&mp->m_icsb_notifier);
  		free_percpu(mp->m_sb_cnts);
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	percpu_counter_destroy(&mp->m_icount);
+ 	percpu_counter_destroy(&mp->m_ifree);
+ 	percpu_counter_destroy(&mp->m_fdblocks);
+ 
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	mutex_destroy(&mp->m_icsb_mutex);
  }
  
@@@ -1708,12 -1715,6 +1870,15 @@@ xfs_icsb_count
  	if (!(flags & XFS_ICSB_LAZY_COUNT))
  		xfs_icsb_lock_all_counters(mp);
  
++<<<<<<< HEAD
 +	for_each_online_cpu(i) {
 +		cntp = (xfs_icsb_cnts_t *)per_cpu_ptr(mp->m_sb_cnts, i);
 +		cnt->icsb_icount += cntp->icsb_icount;
 +		cnt->icsb_ifree += cntp->icsb_ifree;
 +		cnt->icsb_fdblocks += cntp->icsb_fdblocks;
 +	}
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  
  	if (!(flags & XFS_ICSB_LAZY_COUNT))
  		xfs_icsb_unlock_all_counters(mp);
@@@ -1724,7 -1725,6 +1889,10 @@@ xfs_icsb_counter_disabled
  	xfs_mount_t	*mp,
  	xfs_sb_field_t	field)
  {
++<<<<<<< HEAD
 +	ASSERT((field >= XFS_SBS_ICOUNT) && (field <= XFS_SBS_FDBLOCKS));
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	return test_bit(field, &mp->m_icsb_counters);
  }
  
@@@ -1735,8 -1735,6 +1903,11 @@@ xfs_icsb_disable_counter
  {
  	xfs_icsb_cnts_t	cnt;
  
++<<<<<<< HEAD
 +	ASSERT((field >= XFS_SBS_ICOUNT) && (field <= XFS_SBS_FDBLOCKS));
 +
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	/*
  	 * If we are already disabled, then there is nothing to do
  	 * here. We check before locking all the counters to avoid
@@@ -1754,15 -1752,6 +1925,18 @@@
  
  		xfs_icsb_count(mp, &cnt, XFS_ICSB_LAZY_COUNT);
  		switch(field) {
++<<<<<<< HEAD
 +		case XFS_SBS_ICOUNT:
 +			mp->m_sb.sb_icount = cnt.icsb_icount;
 +			break;
 +		case XFS_SBS_IFREE:
 +			mp->m_sb.sb_ifree = cnt.icsb_ifree;
 +			break;
 +		case XFS_SBS_FDBLOCKS:
 +			mp->m_sb.sb_fdblocks = cnt.icsb_fdblocks;
 +			break;
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  		default:
  			BUG();
  		}
@@@ -1778,24 -1767,11 +1952,28 @@@ xfs_icsb_enable_counter
  	uint64_t	count,
  	uint64_t	resid)
  {
- 	xfs_icsb_cnts_t	*cntp;
  	int		i;
  
++<<<<<<< HEAD
 +	ASSERT((field >= XFS_SBS_ICOUNT) && (field <= XFS_SBS_FDBLOCKS));
 +
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	xfs_icsb_lock_all_counters(mp);
  	for_each_online_cpu(i) {
- 		cntp = per_cpu_ptr(mp->m_sb_cnts, i);
  		switch (field) {
++<<<<<<< HEAD
 +		case XFS_SBS_ICOUNT:
 +			cntp->icsb_icount = count + resid;
 +			break;
 +		case XFS_SBS_IFREE:
 +			cntp->icsb_ifree = count + resid;
 +			break;
 +		case XFS_SBS_FDBLOCKS:
 +			cntp->icsb_fdblocks = count + resid;
 +			break;
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  		default:
  			BUG();
  			break;
@@@ -1814,13 -1790,6 +1992,16 @@@ xfs_icsb_sync_counters_locked
  	xfs_icsb_cnts_t	cnt;
  
  	xfs_icsb_count(mp, &cnt, flags);
++<<<<<<< HEAD
 +
 +	if (!xfs_icsb_counter_disabled(mp, XFS_SBS_ICOUNT))
 +		mp->m_sb.sb_icount = cnt.icsb_icount;
 +	if (!xfs_icsb_counter_disabled(mp, XFS_SBS_IFREE))
 +		mp->m_sb.sb_ifree = cnt.icsb_ifree;
 +	if (!xfs_icsb_counter_disabled(mp, XFS_SBS_FDBLOCKS))
 +		mp->m_sb.sb_fdblocks = cnt.icsb_fdblocks;
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  }
  
  /*
@@@ -1870,24 -1837,6 +2049,27 @@@ xfs_icsb_balance_counter_locked
  
  	/* update counters  - first CPU gets residual*/
  	switch (field) {
++<<<<<<< HEAD
 +	case XFS_SBS_ICOUNT:
 +		count = mp->m_sb.sb_icount;
 +		resid = do_div(count, weight);
 +		if (count < max(min, XFS_ICSB_INO_CNTR_REENABLE))
 +			return;
 +		break;
 +	case XFS_SBS_IFREE:
 +		count = mp->m_sb.sb_ifree;
 +		resid = do_div(count, weight);
 +		if (count < max(min, XFS_ICSB_INO_CNTR_REENABLE))
 +			return;
 +		break;
 +	case XFS_SBS_FDBLOCKS:
 +		count = mp->m_sb.sb_fdblocks;
 +		resid = do_div(count, weight);
 +		if (count < max(min, XFS_ICSB_FDBLK_CNTR_REENABLE(mp)))
 +			return;
 +		break;
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	default:
  		BUG();
  		count = resid = 0;	/* quiet, gcc */
@@@ -1936,34 -1884,9 +2117,37 @@@ again
  	}
  
  	switch (field) {
++<<<<<<< HEAD
 +	case XFS_SBS_ICOUNT:
 +		lcounter = icsbp->icsb_icount;
 +		lcounter += delta;
 +		if (unlikely(lcounter < 0))
 +			goto balance_counter;
 +		icsbp->icsb_icount = lcounter;
 +		break;
 +
 +	case XFS_SBS_IFREE:
 +		lcounter = icsbp->icsb_ifree;
 +		lcounter += delta;
 +		if (unlikely(lcounter < 0))
 +			goto balance_counter;
 +		icsbp->icsb_ifree = lcounter;
 +		break;
 +
 +	case XFS_SBS_FDBLOCKS:
 +		BUG_ON((mp->m_resblks - mp->m_resblks_avail) != 0);
 +
 +		lcounter = icsbp->icsb_fdblocks - XFS_ALLOC_SET_ASIDE(mp);
 +		lcounter += delta;
 +		if (unlikely(lcounter < 0))
 +			goto balance_counter;
 +		icsbp->icsb_fdblocks = lcounter + XFS_ALLOC_SET_ASIDE(mp);
 +		break;
++=======
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	default:
  		BUG();
- 		break;
+ 		goto balance_counter; /* be still, gcc */
  	}
  	xfs_icsb_unlock_cntr(icsbp);
  	preempt_enable();
diff --cc fs/xfs/xfs_mount.h
index 77ab2563aabb,84b745fbc787..000000000000
--- a/fs/xfs/xfs_mount.h
+++ b/fs/xfs/xfs_mount.h
@@@ -81,8 -80,13 +81,15 @@@ typedef struct xfs_mount 
  	struct super_block	*m_super;
  	xfs_tid_t		m_tid;		/* next unused tid for fs */
  	struct xfs_ail		*m_ail;		/* fs active log item list */
 -
 -	struct xfs_sb		m_sb;		/* copy of fs superblock */
 +	xfs_sb_t		m_sb;		/* copy of fs superblock */
  	spinlock_t		m_sb_lock;	/* sb counter lock */
++<<<<<<< HEAD
++=======
+ 	struct percpu_counter	m_icount;	/* allocated inodes counter */
+ 	struct percpu_counter	m_ifree;	/* free inodes counter */
+ 	struct percpu_counter	m_fdblocks;	/* free block counter */
+ 
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	struct xfs_buf		*m_sb_bp;	/* buffer for superblock */
  	char			*m_fsname;	/* filesystem name */
  	int			m_fsname_len;	/* strlen of fs name */
@@@ -399,7 -392,11 +406,15 @@@ extern void	xfs_unmountfs(xfs_mount_t *
  extern int	xfs_mod_incore_sb(xfs_mount_t *, xfs_sb_field_t, int64_t, int);
  extern int	xfs_mod_incore_sb_batch(xfs_mount_t *, xfs_mod_sb_t *,
  			uint, int);
++<<<<<<< HEAD
 +extern int	xfs_mount_log_sb(xfs_mount_t *, __int64_t);
++=======
+ extern int	xfs_mod_icount(struct xfs_mount *mp, int64_t delta);
+ extern int	xfs_mod_ifree(struct xfs_mount *mp, int64_t delta);
+ extern int	xfs_mod_fdblocks(struct xfs_mount *mp, int64_t delta,
+ 				 bool reserved);
+ extern int	xfs_mount_log_sb(xfs_mount_t *);
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  extern struct xfs_buf *xfs_getsb(xfs_mount_t *, int);
  extern int	xfs_readsb(xfs_mount_t *, int);
  extern void	xfs_freesb(xfs_mount_t *);
diff --cc fs/xfs/xfs_super.c
index a9ec2c211f1a,9ec75074026d..000000000000
--- a/fs/xfs/xfs_super.c
+++ b/fs/xfs/xfs_super.c
@@@ -1097,6 -1085,9 +1097,12 @@@ xfs_fs_statfs
  	xfs_sb_t		*sbp = &mp->m_sb;
  	struct xfs_inode	*ip = XFS_I(dentry->d_inode);
  	__uint64_t		fakeinos, id;
++<<<<<<< HEAD
++=======
+ 	__uint64_t		icount;
+ 	__uint64_t		ifree;
+ 	__uint64_t		fdblocks;
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  	xfs_extlen_t		lsize;
  	__int64_t		ffree;
  
@@@ -1108,16 -1099,21 +1114,25 @@@
  	statp->f_fsid.val[1] = (u32)(id >> 32);
  
  	xfs_icsb_sync_counters(mp, XFS_ICSB_LAZY_COUNT);
++<<<<<<< HEAD
++=======
+ 	icount = percpu_counter_sum(&mp->m_icount);
+ 	ifree = percpu_counter_sum(&mp->m_ifree);
+ 	fdblocks = percpu_counter_sum(&mp->m_fdblocks);
++>>>>>>> 0d485ada404b (xfs: use generic percpu counters for free block counter)
  
  	spin_lock(&mp->m_sb_lock);
  	statp->f_bsize = sbp->sb_blocksize;
  	lsize = sbp->sb_logstart ? sbp->sb_logblocks : 0;
  	statp->f_blocks = sbp->sb_dblocks - lsize;
- 	statp->f_bfree = statp->f_bavail =
- 				sbp->sb_fdblocks - XFS_ALLOC_SET_ASIDE(mp);
+ 	spin_unlock(&mp->m_sb_lock);
+ 
+ 	statp->f_bfree = fdblocks - XFS_ALLOC_SET_ASIDE(mp);
+ 	statp->f_bavail = statp->f_bfree;
+ 
  	fakeinos = statp->f_bfree << sbp->sb_inopblog;
 -	statp->f_files = MIN(icount + fakeinos, (__uint64_t)XFS_MAXINUMBER);
 +	statp->f_files =
 +	    MIN(sbp->sb_icount + fakeinos, (__uint64_t)XFS_MAXINUMBER);
  	if (mp->m_maxicount)
  		statp->f_files = min_t(typeof(statp->f_files),
  					statp->f_files,
@@@ -1129,10 -1125,9 +1144,9 @@@
  					sbp->sb_icount);
  
  	/* make sure statp->f_ffree does not underflow */
 -	ffree = statp->f_files - (icount - ifree);
 +	ffree = statp->f_files - (sbp->sb_icount - sbp->sb_ifree);
  	statp->f_ffree = max_t(__int64_t, ffree, 0);
  
- 	spin_unlock(&mp->m_sb_lock);
  
  	if ((ip->i_d.di_flags & XFS_DIFLAG_PROJINHERIT) &&
  	    ((mp->m_qflags & (XFS_PQUOTA_ACCT|XFS_PQUOTA_ENFD))) ==
diff --git a/fs/xfs/libxfs/xfs_bmap.c b/fs/xfs/libxfs/xfs_bmap.c
index 306dc359fdbf..fd8244b9bb69 100644
--- a/fs/xfs/libxfs/xfs_bmap.c
+++ b/fs/xfs/libxfs/xfs_bmap.c
@@ -2221,9 +2221,8 @@ xfs_bmap_add_extent_delay_real(
 		diff = (int)(temp + temp2 - startblockval(PREV.br_startblock) -
 			(bma->cur ? bma->cur->bc_private.b.allocated : 0));
 		if (diff > 0) {
-			error = xfs_icsb_modify_counters(bma->ip->i_mount,
-					XFS_SBS_FDBLOCKS,
-					-((int64_t)diff), 0);
+			error = xfs_mod_fdblocks(bma->ip->i_mount,
+						 -((int64_t)diff), false);
 			ASSERT(!error);
 			if (error)
 				goto done;
@@ -2274,9 +2273,8 @@ xfs_bmap_add_extent_delay_real(
 			temp += bma->cur->bc_private.b.allocated;
 		ASSERT(temp <= da_old);
 		if (temp < da_old)
-			xfs_icsb_modify_counters(bma->ip->i_mount,
-					XFS_SBS_FDBLOCKS,
-					(int64_t)(da_old - temp), 0);
+			xfs_mod_fdblocks(bma->ip->i_mount,
+					(int64_t)(da_old - temp), false);
 	}
 
 	/* clear out the allocated field, done with it now in any case. */
@@ -2953,8 +2951,8 @@ xfs_bmap_add_extent_hole_delay(
 	}
 	if (oldlen != newlen) {
 		ASSERT(oldlen > newlen);
-		xfs_icsb_modify_counters(ip->i_mount, XFS_SBS_FDBLOCKS,
-			(int64_t)(oldlen - newlen), 0);
+		xfs_mod_fdblocks(ip->i_mount, (int64_t)(oldlen - newlen),
+				 false);
 		/*
 		 * Nothing to do for disk quota accounting here.
 		 */
@@ -4171,15 +4169,13 @@ xfs_bmapi_reserve_delalloc(
 		error = xfs_mod_incore_sb(mp, XFS_SBS_FREXTENTS,
 					  -((int64_t)extsz), 0);
 	} else {
-		error = xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS,
-						 -((int64_t)alen), 0);
+		error = xfs_mod_fdblocks(mp, -((int64_t)alen), false);
 	}
 
 	if (error)
 		goto out_unreserve_quota;
 
-	error = xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS,
-					 -((int64_t)indlen), 0);
+	error = xfs_mod_fdblocks(mp, -((int64_t)indlen), false);
 	if (error)
 		goto out_unreserve_blocks;
 
@@ -4208,7 +4204,7 @@ out_unreserve_blocks:
 	if (rt)
 		xfs_mod_incore_sb(mp, XFS_SBS_FREXTENTS, extsz, 0);
 	else
-		xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS, alen, 0);
+		xfs_mod_fdblocks(mp, alen, false);
 out_unreserve_quota:
 	if (XFS_IS_QUOTA_ON(mp))
 		xfs_trans_unreserve_quota_nblks(NULL, ip, (long)alen, 0, rt ?
@@ -5018,10 +5014,8 @@ xfs_bmap_del_extent(
 	 * Nothing to do for disk quota accounting here.
 	 */
 	ASSERT(da_old >= da_new);
-	if (da_old > da_new) {
-		xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS,
-			(int64_t)(da_old - da_new), 0);
-	}
+	if (da_old > da_new)
+		xfs_mod_fdblocks(mp, (int64_t)(da_old - da_new), false);
 done:
 	*logflagsp = flags;
 	return error;
@@ -5295,8 +5289,8 @@ xfs_bunmapi(
 					ip, -((long)del.br_blockcount), 0,
 					XFS_QMOPT_RES_RTBLKS);
 			} else {
-				xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS,
-						(int64_t)del.br_blockcount, 0);
+				xfs_mod_fdblocks(mp, (int64_t)del.br_blockcount,
+						 false);
 				(void)xfs_trans_reserve_quota_nblks(NULL,
 					ip, -((long)del.br_blockcount), 0,
 					XFS_QMOPT_RES_REGBLKS);
* Unmerged path fs/xfs/libxfs/xfs_sb.c
* Unmerged path fs/xfs/xfs_fsops.c
diff --git a/fs/xfs/xfs_iomap.c b/fs/xfs/xfs_iomap.c
index a2c5a821da94..c03f420dc397 100644
--- a/fs/xfs/xfs_iomap.c
+++ b/fs/xfs/xfs_iomap.c
@@ -466,7 +466,7 @@ xfs_iomap_prealloc_size(
 				       alloc_blocks);
 
 	xfs_icsb_sync_counters(mp, XFS_ICSB_LAZY_COUNT);
-	freesp = mp->m_sb.sb_fdblocks;
+	freesp = percpu_counter_read_positive(&mp->m_fdblocks);
 	if (freesp < mp->m_low_space[XFS_LOWSP_5_PCNT]) {
 		shift = 2;
 		if (freesp < mp->m_low_space[XFS_LOWSP_4_PCNT])
* Unmerged path fs/xfs/xfs_mount.c
* Unmerged path fs/xfs/xfs_mount.h
* Unmerged path fs/xfs/xfs_super.c
diff --git a/fs/xfs/xfs_trans.c b/fs/xfs/xfs_trans.c
index ddcabad67a51..6b4a1fab17f1 100644
--- a/fs/xfs/xfs_trans.c
+++ b/fs/xfs/xfs_trans.c
@@ -175,7 +175,7 @@ xfs_trans_reserve(
 	uint			rtextents)
 {
 	int		error = 0;
-	int		rsvd = (tp->t_flags & XFS_TRANS_RESERVE) != 0;
+	bool		rsvd = (tp->t_flags & XFS_TRANS_RESERVE) != 0;
 
 	/* Mark this thread as being in a transaction */
 	current_set_flags_nested(&tp->t_pflags, PF_FSTRANS);
@@ -186,8 +186,7 @@ xfs_trans_reserve(
 	 * fail if the count would go below zero.
 	 */
 	if (blocks > 0) {
-		error = xfs_icsb_modify_counters(tp->t_mountp, XFS_SBS_FDBLOCKS,
-					  -((int64_t)blocks), rsvd);
+		error = xfs_mod_fdblocks(tp->t_mountp, -((int64_t)blocks), rsvd);
 		if (error != 0) {
 			current_restore_flags_nested(&tp->t_pflags, PF_FSTRANS);
 			return XFS_ERROR(ENOSPC);
@@ -270,8 +269,7 @@ undo_log:
 
 undo_blocks:
 	if (blocks > 0) {
-		xfs_icsb_modify_counters(tp->t_mountp, XFS_SBS_FDBLOCKS,
-					 (int64_t)blocks, rsvd);
+		xfs_mod_fdblocks(tp->t_mountp, -((int64_t)blocks), rsvd);
 		tp->t_blk_res = 0;
 	}
 
@@ -518,14 +516,13 @@ xfs_trans_unreserve_and_mod_sb(
 	xfs_mount_t	*mp = tp->t_mountp;
 	/* REFERENCED */
 	int		error;
-	int		rsvd;
+	bool		rsvd = (tp->t_flags & XFS_TRANS_RESERVE) != 0;
 	int64_t		blkdelta = 0;
 	int64_t		rtxdelta = 0;
 	int64_t		idelta = 0;
 	int64_t		ifreedelta = 0;
 
 	msbp = msb;
-	rsvd = (tp->t_flags & XFS_TRANS_RESERVE) != 0;
 
 	/* calculate deltas */
 	if (tp->t_blk_res > 0)
@@ -549,8 +546,7 @@ xfs_trans_unreserve_and_mod_sb(
 
 	/* apply the per-cpu counters */
 	if (blkdelta) {
-		error = xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS,
-						 blkdelta, rsvd);
+		error = xfs_mod_fdblocks(mp, blkdelta, rsvd);
 		if (error)
 			goto out;
 	}
@@ -639,7 +635,7 @@ out_undo_icount:
 		xfs_icsb_modify_counters(mp, XFS_SBS_ICOUNT, -idelta, rsvd);
 out_undo_fdblocks:
 	if (blkdelta)
-		xfs_icsb_modify_counters(mp, XFS_SBS_FDBLOCKS, -blkdelta, rsvd);
+		xfs_mod_fdblocks(mp, -blkdelta, rsvd);
 out:
 	ASSERT(error == 0);
 	return;
