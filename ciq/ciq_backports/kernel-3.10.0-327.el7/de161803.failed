net/mlx4_core: Move affinity hints to mlx4_core ownership

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [netdrv] mlx4_core: Move affinity hints to mlx4_core ownership (Amir Vadai) [1164527 1164530 1164531 1164536 1164537]
Rebuild_FUZZ: 96.36%
commit-author Ido Shamay <idos@mellanox.com>
commit de1618034ae5704f9e503a20a1c328a0e60f6b5f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/de161803.failed

Now that EQs management is in the sole responsibility of mlx4_core,
the IRQ affinity hints configuration should be in its hands as well.
request_irq is called only once by the first consumer (maybe mlx4_ib),
so mlx4_en passes the affinity mask too late. We also need to request
vectors according to the cores we want to run on.

mlx4_core distribution of IRQs to cores is straight forward,
EQ(i)->IRQ will set affinity hint to core i.
Consumers need to request EQ vectors, according to their cores
considerations (NUMA).

	Signed-off-by: Ido Shamay <idos@mellanox.com>
	Signed-off-by: Matan Barak <matanb@mellanox.com>
	Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit de1618034ae5704f9e503a20a1c328a0e60f6b5f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx4/en_cq.c
#	drivers/net/ethernet/mellanox/mlx4/eq.c
#	drivers/net/ethernet/mellanox/mlx4/main.c
#	drivers/net/ethernet/mellanox/mlx4/mlx4.h
diff --cc drivers/net/ethernet/mellanox/mlx4/en_cq.c
index 5a19a55448f8,63769df872a4..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_cq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_cq.c
@@@ -117,26 -112,21 +117,32 @@@ int mlx4_en_activate_cq(struct mlx4_en_
  	memset(cq->buf, 0, cq->buf_size);
  
  	if (cq->is_tx == RX) {
++<<<<<<< HEAD
 +		if (mdev->dev->caps.comp_pool) {
 +			if (!cq->vector) {
 +				sprintf(name, "%s-%d", priv->dev->name,
 +					cq->ring);
 +				/* Set IRQ for specific name (per ring) */
 +				if (mlx4_assign_eq(mdev->dev, name, rmap,
 +						   &cq->vector)) {
 +					cq->vector = (cq->ring + 1 + priv->port)
 +					    % mdev->dev->caps.num_comp_vectors;
 +					mlx4_warn(mdev, "Failed assigning an EQ to %s, falling back to legacy EQ's\n",
 +						  name);
 +				}
++=======
+ 		if (!mlx4_is_eq_vector_valid(mdev->dev, priv->port,
+ 					     cq->vector)) {
+ 			cq->vector = cpumask_first(priv->rx_ring[cq->ring]->affinity_mask);
++>>>>>>> de1618034ae5 (net/mlx4_core: Move affinity hints to mlx4_core ownership)
  
 -			err = mlx4_assign_eq(mdev->dev, priv->port,
 -					     &cq->vector);
 -			if (err) {
 -				mlx4_err(mdev, "Failed assigning an EQ to %s\n",
 -					 name);
 -				goto free_eq;
  			}
 -
 -			assigned_eq = true;
 +		} else {
 +			cq->vector = (cq->ring + 1 + priv->port) %
 +				mdev->dev->caps.num_comp_vectors;
  		}
  
 +#ifdef CONFIG_GENERIC_HARDIRQS
  		cq->irq_desc =
  			irq_to_desc(mlx4_eq_get_irq(mdev->dev,
  						    cq->vector));
diff --cc drivers/net/ethernet/mellanox/mlx4/eq.c
index 25bcdb02089f,11168825a9fa..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/eq.c
@@@ -1339,52 -1386,140 +1357,85 @@@ int mlx4_test_interrupts(struct mlx4_de
  }
  EXPORT_SYMBOL(mlx4_test_interrupts);
  
 -bool mlx4_is_eq_vector_valid(struct mlx4_dev *dev, u8 port, int vector)
 -{
 -	struct mlx4_priv *priv = mlx4_priv(dev);
 -
 -	vector = MLX4_CQ_TO_EQ_VECTOR(vector);
 -	if (vector < 0 || (vector >= dev->caps.num_comp_vectors + 1) ||
 -	    (vector == MLX4_EQ_ASYNC))
 -		return false;
 -
 -	return test_bit(port - 1, priv->eq_table.eq[vector].actv_ports.ports);
 -}
 -EXPORT_SYMBOL(mlx4_is_eq_vector_valid);
 -
 -u32 mlx4_get_eqs_per_port(struct mlx4_dev *dev, u8 port)
 -{
 -	struct mlx4_priv *priv = mlx4_priv(dev);
 -	unsigned int i;
 -	unsigned int sum = 0;
 -
 -	for (i = 0; i < dev->caps.num_comp_vectors + 1; i++)
 -		sum += !!test_bit(port - 1,
 -				  priv->eq_table.eq[i].actv_ports.ports);
 -
 -	return sum;
 -}
 -EXPORT_SYMBOL(mlx4_get_eqs_per_port);
 -
 -int mlx4_is_eq_shared(struct mlx4_dev *dev, int vector)
 -{
 -	struct mlx4_priv *priv = mlx4_priv(dev);
 -
 -	vector = MLX4_CQ_TO_EQ_VECTOR(vector);
 -	if (vector <= 0 || (vector >= dev->caps.num_comp_vectors + 1))
 -		return -EINVAL;
 -
 -	return !!(bitmap_weight(priv->eq_table.eq[vector].actv_ports.ports,
 -				dev->caps.num_ports) > 1);
 -}
 -EXPORT_SYMBOL(mlx4_is_eq_shared);
 -
 -struct cpu_rmap *mlx4_get_cpu_rmap(struct mlx4_dev *dev, int port)
 +int mlx4_assign_eq(struct mlx4_dev *dev, char *name, struct cpu_rmap *rmap,
 +		   int *vector)
  {
 -	return mlx4_priv(dev)->port[port].rmap;
 -}
 -EXPORT_SYMBOL(mlx4_get_cpu_rmap);
  
 -int mlx4_assign_eq(struct mlx4_dev *dev, u8 port, int *vector)
 -{
  	struct mlx4_priv *priv = mlx4_priv(dev);
 -	int err = 0, i = 0;
 -	u32 min_ref_count_val = (u32)-1;
 -	int requested_vector = MLX4_CQ_TO_EQ_VECTOR(*vector);
 -	int *prequested_vector = NULL;
 -
 +	int vec = 0, err = 0, i;
  
  	mutex_lock(&priv->msix_ctl.pool_lock);
 -	if (requested_vector < (dev->caps.num_comp_vectors + 1) &&
 -	    (requested_vector >= 0) &&
 -	    (requested_vector != MLX4_EQ_ASYNC)) {
 -		if (test_bit(port - 1,
 -			     priv->eq_table.eq[requested_vector].actv_ports.ports)) {
 -			prequested_vector = &requested_vector;
 -		} else {
 -			struct mlx4_eq *eq;
 -
 -			for (i = 1; i < port;
 -			     requested_vector += mlx4_get_eqs_per_port(dev, i++))
 -				;
 -
 -			eq = &priv->eq_table.eq[requested_vector];
 -			if (requested_vector < dev->caps.num_comp_vectors + 1 &&
 -			    test_bit(port - 1, eq->actv_ports.ports)) {
 -				prequested_vector = &requested_vector;
 +	for (i = 0; !vec && i < dev->caps.comp_pool; i++) {
 +		if (~priv->msix_ctl.pool_bm & 1ULL << i) {
 +			priv->msix_ctl.pool_bm |= 1ULL << i;
 +			vec = dev->caps.num_comp_vectors + 1 + i;
 +			snprintf(priv->eq_table.irq_names +
 +					vec * MLX4_IRQNAME_SIZE,
 +					MLX4_IRQNAME_SIZE, "%s", name);
 +#ifdef CONFIG_RFS_ACCEL
 +			if (rmap) {
 +				err = irq_cpu_rmap_add(rmap,
 +						       priv->eq_table.eq[vec].irq);
 +				if (err)
 +					mlx4_warn(dev, "Failed adding irq rmap\n");
  			}
 -		}
 -	}
 -
 -	if  (!prequested_vector) {
 -		requested_vector = -1;
 -		for (i = 0; min_ref_count_val && i < dev->caps.num_comp_vectors + 1;
 -		     i++) {
 -			struct mlx4_eq *eq = &priv->eq_table.eq[i];
 -
 -			if (min_ref_count_val > eq->ref_count &&
 -			    test_bit(port - 1, eq->actv_ports.ports)) {
 -				min_ref_count_val = eq->ref_count;
 -				requested_vector = i;
 +#endif
 +			err = request_irq(priv->eq_table.eq[vec].irq,
 +					  mlx4_msi_x_interrupt, 0,
 +					  &priv->eq_table.irq_names[vec<<5],
 +					  priv->eq_table.eq + vec);
 +			if (err) {
 +				/*zero out bit by fliping it*/
 +				priv->msix_ctl.pool_bm ^= 1 << i;
 +				vec = 0;
 +				continue;
 +				/*we dont want to break here*/
  			}
 -		}
  
++<<<<<<< HEAD
 +			eq_set_ci(&priv->eq_table.eq[vec], 1);
++=======
+ 		if (requested_vector < 0) {
+ 			err = -ENOSPC;
+ 			goto err_unlock;
+ 		}
+ 
+ 		prequested_vector = &requested_vector;
+ 	}
+ 
+ 	if (!test_bit(*prequested_vector, priv->msix_ctl.pool_bm) &&
+ 	    dev->flags & MLX4_FLAG_MSI_X) {
+ 		set_bit(*prequested_vector, priv->msix_ctl.pool_bm);
+ 		snprintf(priv->eq_table.irq_names +
+ 			 *prequested_vector * MLX4_IRQNAME_SIZE,
+ 			 MLX4_IRQNAME_SIZE, "mlx4-%d@%s",
+ 			 *prequested_vector, dev_name(&dev->persist->pdev->dev));
+ 
+ 		err = request_irq(priv->eq_table.eq[*prequested_vector].irq,
+ 				  mlx4_msi_x_interrupt, 0,
+ 				  &priv->eq_table.irq_names[*prequested_vector << 5],
+ 				  priv->eq_table.eq + *prequested_vector);
+ 
+ 		if (err) {
+ 			clear_bit(*prequested_vector, priv->msix_ctl.pool_bm);
+ 			*prequested_vector = -1;
+ 		} else {
+ #if defined(CONFIG_SMP)
+ 			mlx4_set_eq_affinity_hint(priv, *prequested_vector);
+ #endif
+ 			eq_set_ci(&priv->eq_table.eq[*prequested_vector], 1);
+ 			priv->eq_table.eq[*prequested_vector].have_irq = 1;
++>>>>>>> de1618034ae5 (net/mlx4_core: Move affinity hints to mlx4_core ownership)
  		}
  	}
 -
 -	if (!err && *prequested_vector >= 0)
 -		priv->eq_table.eq[*prequested_vector].ref_count++;
 -
 -err_unlock:
  	mutex_unlock(&priv->msix_ctl.pool_lock);
  
 -	if (!err && *prequested_vector >= 0)
 -		*vector = MLX4_EQ_TO_CQ_VECTOR(*prequested_vector);
 -	else
 +	if (vec) {
 +		*vector = vec;
 +	} else {
  		*vector = 0;
 -
 +		err = (i == dev->caps.comp_pool) ? -ENOSPC : err;
 +	}
  	return err;
  }
  EXPORT_SYMBOL(mlx4_assign_eq);
diff --cc drivers/net/ethernet/mellanox/mlx4/main.c
index 0632ee652e13,0dbd70427221..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/main.c
+++ b/drivers/net/ethernet/mellanox/mlx4/main.c
@@@ -2232,22 -2531,58 +2262,70 @@@ static void mlx4_enable_msi_x(struct ml
  		for (i = 0; i < nreq; ++i)
  			entries[i].entry = i;
  
 -		nreq = pci_enable_msix_range(dev->persist->pdev, entries, 2,
 -					     nreq);
 +		nreq = pci_enable_msix_range(dev->pdev, entries, 2, nreq);
  
 -		if (nreq < 0 || nreq < MLX4_EQ_ASYNC) {
 +		if (nreq < 0) {
  			kfree(entries);
  			goto no_msi;
 +		} else if (nreq < MSIX_LEGACY_SZ +
 +			   dev->caps.num_ports * MIN_MSIX_P_PORT) {
 +			/*Working in legacy mode , all EQ's shared*/
 +			dev->caps.comp_pool           = 0;
 +			dev->caps.num_comp_vectors = nreq - 1;
 +		} else {
 +			dev->caps.comp_pool           = nreq - MSIX_LEGACY_SZ;
 +			dev->caps.num_comp_vectors = MSIX_LEGACY_SZ - 1;
  		}
++<<<<<<< HEAD
 +		for (i = 0; i < nreq; ++i)
 +			priv->eq_table.eq[i].irq = entries[i].vector;
++=======
+ 		/* 1 is reserved for events (asyncrounous EQ) */
+ 		dev->caps.num_comp_vectors = nreq - 1;
+ 
+ 		priv->eq_table.eq[MLX4_EQ_ASYNC].irq = entries[0].vector;
+ 		bitmap_zero(priv->eq_table.eq[MLX4_EQ_ASYNC].actv_ports.ports,
+ 			    dev->caps.num_ports);
+ 
+ 		for (i = 0; i < dev->caps.num_comp_vectors + 1; i++) {
+ 			if (i == MLX4_EQ_ASYNC)
+ 				continue;
+ 
+ 			priv->eq_table.eq[i].irq =
+ 				entries[i + 1 - !!(i > MLX4_EQ_ASYNC)].vector;
+ 
+ 			if (MLX4_IS_LEGACY_EQ_MODE(dev->caps)) {
+ 				bitmap_fill(priv->eq_table.eq[i].actv_ports.ports,
+ 					    dev->caps.num_ports);
+ 				/* We don't set affinity hint when there
+ 				 * aren't enough EQs
+ 				 */
+ 			} else {
+ 				set_bit(port,
+ 					priv->eq_table.eq[i].actv_ports.ports);
+ 				if (mlx4_init_affinity_hint(dev, port + 1, i))
+ 					mlx4_warn(dev, "Couldn't init hint cpumask for EQ %d\n",
+ 						  i);
+ 			}
+ 			/* We divide the Eqs evenly between the two ports.
+ 			 * (dev->caps.num_comp_vectors / dev->caps.num_ports)
+ 			 * refers to the number of Eqs per port
+ 			 * (i.e eqs_per_port). Theoretically, we would like to
+ 			 * write something like (i + 1) % eqs_per_port == 0.
+ 			 * However, since there's an asynchronous Eq, we have
+ 			 * to skip over it by comparing this condition to
+ 			 * !!((i + 1) > MLX4_EQ_ASYNC).
+ 			 */
+ 			if ((dev->caps.num_comp_vectors > dev->caps.num_ports) &&
+ 			    ((i + 1) %
+ 			     (dev->caps.num_comp_vectors / dev->caps.num_ports)) ==
+ 			    !!((i + 1) > MLX4_EQ_ASYNC))
+ 				/* If dev->caps.num_comp_vectors < dev->caps.num_ports,
+ 				 * everything is shared anyway.
+ 				 */
+ 				port++;
+ 		}
++>>>>>>> de1618034ae5 (net/mlx4_core: Move affinity hints to mlx4_core ownership)
  
  		dev->flags |= MLX4_FLAG_MSI_X;
  
diff --cc drivers/net/ethernet/mellanox/mlx4/mlx4.h
index 693d8152c03b,f424900d23a6..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4.h
@@@ -383,6 -397,9 +383,12 @@@ struct mlx4_eq 
  	struct mlx4_buf_list   *page_list;
  	struct mlx4_mtt		mtt;
  	struct mlx4_eq_tasklet	tasklet_ctx;
++<<<<<<< HEAD
++=======
+ 	struct mlx4_active_ports actv_ports;
+ 	u32			ref_count;
+ 	cpumask_var_t		affinity_mask;
++>>>>>>> de1618034ae5 (net/mlx4_core: Move affinity hints to mlx4_core ownership)
  };
  
  struct mlx4_slave_eqe {
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_cq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/mlx4.h
