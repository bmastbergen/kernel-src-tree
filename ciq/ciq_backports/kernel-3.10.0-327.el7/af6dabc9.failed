net: drop the packet when fails to do software segmentation or header check

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] drop the packet when fails to do software segmentation or header check (Jason Wang) [1232621]
Rebuild_FUZZ: 96.55%
commit-author Jason Wang <jasowang@redhat.com>
commit af6dabc9c70ae3f307685b1f32f52d60b1bf0527
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/af6dabc9.failed

Commit cecda693a969816bac5e470e1d9c9c0ef5567bca ("net: keep original skb
which only needs header checking during software GSO") keeps the original
skb for packets that only needs header check, but it doesn't drop the
packet if software segmentation or header check were failed.

Fixes cecda693a9 ("net: keep original skb which only needs header checking during software GSO")
	Cc: Eric Dumazet <eric.dumazet@gmail.com>
	Signed-off-by: Jason Wang <jasowang@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit af6dabc9c70ae3f307685b1f32f52d60b1bf0527)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/dev.c
diff --cc net/core/dev.c
index c33def8a553a,a989f8502412..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -2510,115 -2595,144 +2510,183 @@@ netdev_features_t netif_skb_features(st
  }
  EXPORT_SYMBOL(netif_skb_features);
  
 -static int xmit_one(struct sk_buff *skb, struct net_device *dev,
 -		    struct netdev_queue *txq, bool more)
 +int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
 +			struct netdev_queue *txq)
  {
 -	unsigned int len;
 -	int rc;
 -
 -	if (!list_empty(&ptype_all))
 -		dev_queue_xmit_nit(skb, dev);
 -
 -	len = skb->len;
 -	trace_net_dev_start_xmit(skb, dev);
 -	rc = netdev_start_xmit(skb, dev, txq, more);
 -	trace_net_dev_xmit(skb, rc, dev, len);
 -
 -	return rc;
 -}
 -
 -struct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,
 -				    struct netdev_queue *txq, int *ret)
 -{
 -	struct sk_buff *skb = first;
 +	const struct net_device_ops *ops = dev->netdev_ops;
  	int rc = NETDEV_TX_OK;
 +	unsigned int skb_len;
  
 -	while (skb) {
 -		struct sk_buff *next = skb->next;
 +	if (likely(!skb->next)) {
 +		netdev_features_t features;
  
++<<<<<<< HEAD
 +		/*
 +		 * If device doesn't need skb->dst, release it right now while
 +		 * its hot in this cpu cache
++=======
+ 		skb->next = NULL;
+ 		rc = xmit_one(skb, dev, txq, next != NULL);
+ 		if (unlikely(!dev_xmit_complete(rc))) {
+ 			skb->next = next;
+ 			goto out;
+ 		}
+ 
+ 		skb = next;
+ 		if (netif_xmit_stopped(txq) && skb) {
+ 			rc = NETDEV_TX_BUSY;
+ 			break;
+ 		}
+ 	}
+ 
+ out:
+ 	*ret = rc;
+ 	return skb;
+ }
+ 
+ static struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,
+ 					  netdev_features_t features)
+ {
+ 	if (vlan_tx_tag_present(skb) &&
+ 	    !vlan_hw_offload_capable(features, skb->vlan_proto))
+ 		skb = __vlan_hwaccel_push_inside(skb);
+ 	return skb;
+ }
+ 
+ static struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	netdev_features_t features;
+ 
+ 	if (skb->next)
+ 		return skb;
+ 
+ 	features = netif_skb_features(skb);
+ 	skb = validate_xmit_vlan(skb, features);
+ 	if (unlikely(!skb))
+ 		goto out_null;
+ 
+ 	/* If encapsulation offload request, verify we are testing
+ 	 * hardware encapsulation features instead of standard
+ 	 * features for the netdev
+ 	 */
+ 	if (skb->encapsulation)
+ 		features &= dev->hw_enc_features;
+ 
+ 	if (netif_needs_gso(dev, skb, features)) {
+ 		struct sk_buff *segs;
+ 
+ 		segs = skb_gso_segment(skb, features);
+ 		if (IS_ERR(segs)) {
+ 			goto out_kfree_skb;
+ 		} else if (segs) {
+ 			consume_skb(skb);
+ 			skb = segs;
+ 		}
+ 	} else {
+ 		if (skb_needs_linearize(skb, features) &&
+ 		    __skb_linearize(skb))
+ 			goto out_kfree_skb;
+ 
+ 		/* If packet is not checksummed and device does not
+ 		 * support checksumming for this protocol, complete
+ 		 * checksumming here.
++>>>>>>> af6dabc9c70a (net: drop the packet when fails to do software segmentation or header check)
  		 */
 -		if (skb->ip_summed == CHECKSUM_PARTIAL) {
 -			if (skb->encapsulation)
 -				skb_set_inner_transport_header(skb,
 -							       skb_checksum_start_offset(skb));
 -			else
 -				skb_set_transport_header(skb,
 -							 skb_checksum_start_offset(skb));
 -			if (!(features & NETIF_F_ALL_CSUM) &&
 -			    skb_checksum_help(skb))
 -				goto out_kfree_skb;
 +		if (dev->priv_flags & IFF_XMIT_DST_RELEASE)
 +			skb_dst_drop(skb);
 +
 +		features = netif_skb_features(skb);
 +
 +		if (vlan_tx_tag_present(skb) &&
 +		    !vlan_hw_offload_capable(features, skb->vlan_proto)) {
 +			skb = __vlan_put_tag(skb, skb->vlan_proto,
 +					     vlan_tx_tag_get(skb));
 +			if (unlikely(!skb))
 +				goto out;
 +
 +			skb->vlan_tci = 0;
  		}
 -	}
  
 -	return skb;
 +		/* If encapsulation offload request, verify we are testing
 +		 * hardware encapsulation features instead of standard
 +		 * features for the netdev
 +		 */
 +		if (skb->encapsulation)
 +			features &= dev->hw_enc_features;
  
 -out_kfree_skb:
 -	kfree_skb(skb);
 -out_null:
 -	return NULL;
 -}
 +		if (netif_needs_gso(skb, features)) {
 +			if (unlikely(dev_gso_segment(skb, features)))
 +				goto out_kfree_skb;
 +			if (skb->next)
 +				goto gso;
 +		} else {
 +			if (skb_needs_linearize(skb, features) &&
 +			    __skb_linearize(skb))
 +				goto out_kfree_skb;
  
 -struct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev)
 -{
 -	struct sk_buff *next, *head = NULL, *tail;
 +			/* If packet is not checksummed and device does not
 +			 * support checksumming for this protocol, complete
 +			 * checksumming here.
 +			 */
 +			if (skb->ip_summed == CHECKSUM_PARTIAL) {
 +				if (skb->encapsulation)
 +					skb_set_inner_transport_header(skb,
 +						skb_checksum_start_offset(skb));
 +				else
 +					skb_set_transport_header(skb,
 +						skb_checksum_start_offset(skb));
 +				if (!(features & NETIF_F_ALL_CSUM) &&
 +				     skb_checksum_help(skb))
 +					goto out_kfree_skb;
 +			}
 +		}
  
 -	for (; skb != NULL; skb = next) {
 -		next = skb->next;
 -		skb->next = NULL;
 +		if (!list_empty(&ptype_all))
 +			dev_queue_xmit_nit(skb, dev);
 +
 +		skb_len = skb->len;
 +		rc = ops->ndo_start_xmit(skb, dev);
 +		trace_net_dev_xmit(skb, rc, dev, skb_len);
 +		if (rc == NETDEV_TX_OK)
 +			txq_trans_update(txq);
 +		return rc;
 +	}
  
 -		/* in case skb wont be segmented, point to itself */
 -		skb->prev = skb;
 +gso:
 +	do {
 +		struct sk_buff *nskb = skb->next;
  
 -		skb = validate_xmit_skb(skb, dev);
 -		if (!skb)
 -			continue;
 +		skb->next = nskb->next;
 +		nskb->next = NULL;
  
 -		if (!head)
 -			head = skb;
 -		else
 -			tail->next = skb;
 -		/* If skb was segmented, skb->prev points to
 -		 * the last segment. If not, it still contains skb.
 -		 */
 -		tail = skb->prev;
 +		if (!list_empty(&ptype_all))
 +			dev_queue_xmit_nit(nskb, dev);
 +
 +		skb_len = nskb->len;
 +		rc = ops->ndo_start_xmit(nskb, dev);
 +		trace_net_dev_xmit(nskb, rc, dev, skb_len);
 +		if (unlikely(rc != NETDEV_TX_OK)) {
 +			if (rc & ~NETDEV_TX_MASK)
 +				goto out_kfree_gso_skb;
 +			nskb->next = skb->next;
 +			skb->next = nskb;
 +			return rc;
 +		}
 +		txq_trans_update(txq);
 +		if (unlikely(netif_xmit_stopped(txq) && skb->next))
 +			return NETDEV_TX_BUSY;
 +	} while (skb->next);
 +
 +out_kfree_gso_skb:
 +	if (likely(skb->next == NULL)) {
 +		skb->destructor = DEV_GSO_CB(skb)->destructor;
 +		consume_skb(skb);
 +		return rc;
  	}
 -	return head;
 +out_kfree_skb:
 +	kfree_skb(skb);
 +out:
 +	return rc;
  }
  
  static void qdisc_pkt_len_init(struct sk_buff *skb)
* Unmerged path net/core/dev.c
