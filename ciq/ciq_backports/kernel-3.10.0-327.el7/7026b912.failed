tcp: fix undo on partial ack in recovery

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Yuchung Cheng <ycheng@google.com>
commit 7026b912f97d912476dff5465ed9a127be094208
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/7026b912.failed

Upon detecting spurious fast retransmit via timestamps during recovery,
use PRR to clock out new data packet instead of retransmission. Once
all retransmission are proven spurious, the sender then reverts the
cwnd reduction and congestion state to open or disorder.

The current code does the opposite: it undoes cwnd as soon as any
retransmission is spurious and continues to retransmit until all
data are acked. This nullifies the point to undo the cwnd because
the sender is still retransmistting spuriously. This patch fixes
it. The undo_ssthresh argument of tcp_undo_cwnd_reductiuon() is no
longer needed and is removed.

	Signed-off-by: Yuchung Cheng <ycheng@google.com>
	Acked-by: Neal Cardwell <ncardwell@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 7026b912f97d912476dff5465ed9a127be094208)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_input.c
diff --cc net/ipv4/tcp_input.c
index a5777ec6c24c,c35b22751982..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -2302,7 -2243,7 +2302,11 @@@ static void DBGUNDO(struct sock *sk, co
  #define DBGUNDO(x...) do { } while (0)
  #endif
  
++<<<<<<< HEAD
 +static void tcp_undo_cwr(struct sock *sk, const bool undo_ssthresh)
++=======
+ static void tcp_undo_cwnd_reduction(struct sock *sk, bool unmark_loss)
++>>>>>>> 7026b912f97d (tcp: fix undo on partial ack in recovery)
  {
  	struct tcp_sock *tp = tcp_sk(sk);
  
@@@ -2322,6 -2275,7 +2326,10 @@@
  		tp->snd_cwnd = max(tp->snd_cwnd, tp->snd_ssthresh);
  	}
  	tp->snd_cwnd_stamp = tcp_time_stamp;
++<<<<<<< HEAD
++=======
+ 	tp->undo_marker = 0;
++>>>>>>> 7026b912f97d (tcp: fix undo on partial ack in recovery)
  }
  
  static inline bool tcp_may_undo(const struct tcp_sock *tp)
@@@ -2341,7 -2295,7 +2349,11 @@@ static bool tcp_try_undo_recovery(struc
  		 * or our original transmission succeeded.
  		 */
  		DBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? "loss" : "retrans");
++<<<<<<< HEAD
 +		tcp_undo_cwr(sk, true);
++=======
+ 		tcp_undo_cwnd_reduction(sk, false);
++>>>>>>> 7026b912f97d (tcp: fix undo on partial ack in recovery)
  		if (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)
  			mib_idx = LINUX_MIB_TCPLOSSUNDO;
  		else
@@@ -2370,8 -2321,7 +2382,12 @@@ static void tcp_try_undo_dsack(struct s
  
  	if (tp->undo_marker && !tp->undo_retrans) {
  		DBGUNDO(sk, "D-SACK");
++<<<<<<< HEAD
 +		tcp_undo_cwr(sk, true);
 +		tp->undo_marker = 0;
++=======
+ 		tcp_undo_cwnd_reduction(sk, false);
++>>>>>>> 7026b912f97d (tcp: fix undo on partial ack in recovery)
  		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPDSACKUNDO);
  	}
  }
@@@ -2412,18 -2361,9 +2428,22 @@@ static bool tcp_try_undo_loss(struct so
  	struct tcp_sock *tp = tcp_sk(sk);
  
  	if (frto_undo || tcp_may_undo(tp)) {
++<<<<<<< HEAD
 +		struct sk_buff *skb;
 +		tcp_for_write_queue(skb, sk) {
 +			if (skb == tcp_send_head(sk))
 +				break;
 +			TCP_SKB_CB(skb)->sacked &= ~TCPCB_LOST;
 +		}
 +
 +		tcp_clear_all_retrans_hints(tp);
++=======
+ 		tcp_undo_cwnd_reduction(sk, true);
++>>>>>>> 7026b912f97d (tcp: fix undo on partial ack in recovery)
  
  		DBGUNDO(sk, "partial loss");
 +		tp->lost_out = 0;
 +		tcp_undo_cwr(sk, true);
  		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPLOSSUNDO);
  		if (frto_undo)
  			NET_INC_STATS_BH(sock_net(sk),
@@@ -2701,6 -2640,40 +2721,43 @@@ static void tcp_process_loss(struct soc
  	tcp_xmit_retransmit_queue(sk);
  }
  
++<<<<<<< HEAD
++=======
+ /* Undo during fast recovery after partial ACK. */
+ static bool tcp_try_undo_partial(struct sock *sk, const int acked,
+ 				 const int prior_unsacked)
+ {
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 
+ 	if (tp->undo_marker && tcp_packet_delayed(tp)) {
+ 		/* Plain luck! Hole if filled with delayed
+ 		 * packet, rather than with a retransmit.
+ 		 */
+ 		tcp_update_reordering(sk, tcp_fackets_out(tp) + acked, 1);
+ 
+ 		/* We are getting evidence that the reordering degree is higher
+ 		 * than we realized. If there are no retransmits out then we
+ 		 * can undo. Otherwise we clock out new packets but do not
+ 		 * mark more packets lost or retransmit more.
+ 		 */
+ 		if (tp->retrans_out) {
+ 			tcp_cwnd_reduction(sk, prior_unsacked, 0);
+ 			return true;
+ 		}
+ 
+ 		if (!tcp_any_retrans_done(sk))
+ 			tp->retrans_stamp = 0;
+ 
+ 		DBGUNDO(sk, "partial recovery");
+ 		tcp_undo_cwnd_reduction(sk, true);
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPPARTIALUNDO);
+ 		tcp_try_keep_open(sk);
+ 		return true;
+ 	}
+ 	return false;
+ }
+ 
++>>>>>>> 7026b912f97d (tcp: fix undo on partial ack in recovery)
  /* Process an event, which can update packets-in-flight not trivially.
   * Main goal of this function is to calculate new estimate for left_out,
   * taking into account both packets sitting in receiver's buffer and
* Unmerged path net/ipv4/tcp_input.c
