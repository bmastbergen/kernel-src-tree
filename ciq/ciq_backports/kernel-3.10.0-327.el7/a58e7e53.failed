hpsa: don't return abort request until target is complete

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Webb Scales <webbnh@hp.com>
commit a58e7e53b410c8ed05f0b1b0f37411c76b8e253f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/a58e7e53.failed

Don't return from the abort request until the target command is complete.
Mark outstanding commands which have a pending abort, and do not send them
to the host if we can avoid it.

If the current command has been aborted, do not call the SCSI command
completion routine from the I/O path: when the abort returns successfully,
the SCSI mid-layer will handle the completion implicitly.

The following race was possible in theory.

1. LLD is requested to abort a scsi command
2. scsi command completes
3. The struct CommandList associated with 2 is made available.
4. new io request to LLD to another LUN re-uses struct CommandList
5. abort handler follows scsi_cmnd->host_scribble and
   finds struct CommandList and tries to aborts it.

Now we have aborted the wrong command.

Fix by resetting the scsi_cmd field of struct CommandList
upon completion and making the abort handler check that
the scsi_cmd pointer in the CommadList struct matches the
scsi_cmnd that it has been asked to abort.

	Reviewed-by: Scott Teel <scott.teel@pmcs.com>
	Reviewed-by: Kevin Barnett <kevin.barnett@pmcs.com>
	Reviewed-by: Tomas Henzl <thenzl@redhat.com>
	Reviewed-by: Hannes Reinecke <hare@Suse.de>
	Signed-off-by: Webb Scales <webbnh@hp.com>
	Signed-off-by: Don Brace <don.brace@pmcs.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: James Bottomley <JBottomley@Odin.com>
(cherry picked from commit a58e7e53b410c8ed05f0b1b0f37411c76b8e253f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/hpsa.c
#	drivers/scsi/hpsa.h
#	drivers/scsi/hpsa_cmd.h
diff --cc drivers/scsi/hpsa.c
index f57c783f05e4,11966f90c895..000000000000
--- a/drivers/scsi/hpsa.c
+++ b/drivers/scsi/hpsa.c
@@@ -275,6 -274,33 +279,36 @@@ static inline struct ctlr_info *shost_t
  	return (struct ctlr_info *) *priv;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool hpsa_is_cmd_idle(struct CommandList *c)
+ {
+ 	return c->scsi_cmd == SCSI_CMD_IDLE;
+ }
+ 
+ /* extract sense key, asc, and ascq from sense data.  -1 means invalid. */
+ static void decode_sense_data(const u8 *sense_data, int sense_data_len,
+ 			u8 *sense_key, u8 *asc, u8 *ascq)
+ {
+ 	struct scsi_sense_hdr sshdr;
+ 	bool rc;
+ 
+ 	*sense_key = -1;
+ 	*asc = -1;
+ 	*ascq = -1;
+ 
+ 	if (sense_data_len < 1)
+ 		return;
+ 
+ 	rc = scsi_normalize_sense(sense_data, sense_data_len, &sshdr);
+ 	if (rc) {
+ 		*sense_key = sshdr.sense_key;
+ 		*asc = sshdr.asc;
+ 		*ascq = sshdr.ascq;
+ 	}
+ }
+ 
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  static int check_for_unit_attention(struct ctlr_info *h,
  	struct CommandList *c)
  {
@@@ -868,6 -968,14 +902,17 @@@ static void enqueue_cmd_and_start_io(st
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void enqueue_cmd_and_start_io(struct ctlr_info *h, struct CommandList *c)
+ {
+ 	if (unlikely(c->abort_pending))
+ 		return finish_cmd(c);
+ 
+ 	__enqueue_cmd_and_start_io(h, c, DEFAULT_REPLY_QUEUE);
+ }
+ 
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  static inline int is_hba_lunid(unsigned char scsi3addr[])
  {
  	return memcmp(scsi3addr, RAID_CTLR_LUNID, 8) == 0;
@@@ -1643,6 -1984,61 +1688,64 @@@ static int handle_ioaccel_mode2_error(s
  	return retry;	/* retry on raid path? */
  }
  
++<<<<<<< HEAD
++=======
+ static void hpsa_cmd_resolve_events(struct ctlr_info *h,
+ 		struct CommandList *c)
+ {
+ 	/*
+ 	 * Prevent the following race in the abort handler:
+ 	 *
+ 	 * 1. LLD is requested to abort a SCSI command
+ 	 * 2. The SCSI command completes
+ 	 * 3. The struct CommandList associated with step 2 is made available
+ 	 * 4. New I/O request to LLD to another LUN re-uses struct CommandList
+ 	 * 5. Abort handler follows scsi_cmnd->host_scribble and
+ 	 *    finds struct CommandList and tries to aborts it
+ 	 * Now we have aborted the wrong command.
+ 	 *
+ 	 * Clear c->scsi_cmd here so that the abort handler will know this
+ 	 * command has completed.  Then, check to see if the abort handler is
+ 	 * waiting for this command, and, if so, wake it.
+ 	 */
+ 	c->scsi_cmd = SCSI_CMD_IDLE;
+ 	mb(); /* Ensure c->scsi_cmd is set to SCSI_CMD_IDLE */
+ 	if (c->abort_pending) {
+ 		c->abort_pending = false;
+ 		wake_up_all(&h->abort_sync_wait_queue);
+ 	}
+ }
+ 
+ static void hpsa_cmd_free_and_done(struct ctlr_info *h,
+ 		struct CommandList *c, struct scsi_cmnd *cmd)
+ {
+ 	hpsa_cmd_resolve_events(h, c);
+ 	cmd_free(h, c);
+ 	cmd->scsi_done(cmd);
+ }
+ 
+ static void hpsa_retry_cmd(struct ctlr_info *h, struct CommandList *c)
+ {
+ 	INIT_WORK(&c->work, hpsa_command_resubmit_worker);
+ 	queue_work_on(raw_smp_processor_id(), h->resubmit_wq, &c->work);
+ }
+ 
+ static void hpsa_set_scsi_cmd_aborted(struct scsi_cmnd *cmd)
+ {
+ 	cmd->result = DID_ABORT << 16;
+ }
+ 
+ static void hpsa_cmd_abort_and_free(struct ctlr_info *h, struct CommandList *c,
+ 				    struct scsi_cmnd *cmd)
+ {
+ 	hpsa_set_scsi_cmd_aborted(cmd);
+ 	dev_warn(&h->pdev->dev, "CDB %16phN was aborted with status 0x%x\n",
+ 			 c->Request.CDB, c->err_info->ScsiStatus);
+ 	hpsa_cmd_resolve_events(h, c);
+ 	cmd_free(h, c);		/* FIX-ME:  change to cmd_tagged_free(h, c) */
+ }
+ 
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  static void process_ioaccel2_completion(struct ctlr_info *h,
  		struct CommandList *c, struct scsi_cmnd *cmd,
  		struct hpsa_scsi_dev_t *dev)
@@@ -1652,13 -2047,15 +1755,22 @@@
  
  	/* check for good status */
  	if (likely(c2->error_data.serv_response == 0 &&
 -			c2->error_data.status == 0))
 -		return hpsa_cmd_free_and_done(h, c, cmd);
 +			c2->error_data.status == 0)) {
 +		cmd_free(h, c);
 +		cmd->scsi_done(cmd);
 +		return;
 +	}
  
++<<<<<<< HEAD
 +	/* Any RAID offload error results in retry which will use
++=======
+ 	/* don't requeue a command which is being aborted */
+ 	if (unlikely(c->abort_pending))
+ 		return hpsa_cmd_abort_and_free(h, c, cmd);
+ 
+ 	/*
+ 	 * Any RAID offload error results in retry which will use
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  	 * the normal I/O path so the controller can handle whatever's
  	 * wrong.
  	 */
@@@ -1752,10 -2175,8 +1864,15 @@@ static void complete_scsi_command(struc
  		if (is_logical_dev_addr_mode(dev->scsi3addr)) {
  			if (ei->CommandStatus == CMD_IOACCEL_DISABLED)
  				dev->offload_enabled = 0;
++<<<<<<< HEAD
 +			cmd->result = DID_SOFT_ERROR << 16;
 +			cmd_free(h, cp);
 +			cmd->scsi_done(cmd);
 +			return;
++=======
+ 			if (!cp->abort_pending)
+ 				return hpsa_retry_cmd(h, cp);
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  		}
  	}
  
@@@ -3963,7 -4491,179 +4081,183 @@@ static int hpsa_scsi_queue_command(stru
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int do_not_scan_if_controller_locked_up(struct ctlr_info *h)
++=======
+ static void hpsa_cmd_init(struct ctlr_info *h, int index,
+ 				struct CommandList *c)
+ {
+ 	dma_addr_t cmd_dma_handle, err_dma_handle;
+ 
+ 	/* Zero out all of commandlist except the last field, refcount */
+ 	memset(c, 0, offsetof(struct CommandList, refcount));
+ 	c->Header.tag = cpu_to_le64((u64) (index << DIRECT_LOOKUP_SHIFT));
+ 	cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+ 	c->err_info = h->errinfo_pool + index;
+ 	memset(c->err_info, 0, sizeof(*c->err_info));
+ 	err_dma_handle = h->errinfo_pool_dhandle
+ 	    + index * sizeof(*c->err_info);
+ 	c->cmdindex = index;
+ 	c->busaddr = (u32) cmd_dma_handle;
+ 	c->ErrDesc.Addr = cpu_to_le64((u64) err_dma_handle);
+ 	c->ErrDesc.Len = cpu_to_le32((u32) sizeof(*c->err_info));
+ 	c->h = h;
+ 	c->scsi_cmd = SCSI_CMD_IDLE;
+ }
+ 
+ static void hpsa_preinitialize_commands(struct ctlr_info *h)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < h->nr_cmds; i++) {
+ 		struct CommandList *c = h->cmd_pool + i;
+ 
+ 		hpsa_cmd_init(h, i, c);
+ 		atomic_set(&c->refcount, 0);
+ 	}
+ }
+ 
+ static inline void hpsa_cmd_partial_init(struct ctlr_info *h, int index,
+ 				struct CommandList *c)
+ {
+ 	dma_addr_t cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+ 
+ 	memset(c->Request.CDB, 0, sizeof(c->Request.CDB));
+ 	memset(c->err_info, 0, sizeof(*c->err_info));
+ 	c->busaddr = (u32) cmd_dma_handle;
+ }
+ 
+ static int hpsa_ioaccel_submit(struct ctlr_info *h,
+ 		struct CommandList *c, struct scsi_cmnd *cmd,
+ 		unsigned char *scsi3addr)
+ {
+ 	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
+ 	int rc = IO_ACCEL_INELIGIBLE;
+ 
+ 	cmd->host_scribble = (unsigned char *) c;
+ 
+ 	if (dev->offload_enabled) {
+ 		hpsa_cmd_init(h, c->cmdindex, c);
+ 		c->cmd_type = CMD_SCSI;
+ 		c->scsi_cmd = cmd;
+ 		rc = hpsa_scsi_ioaccel_raid_map(h, c);
+ 		if (rc < 0)     /* scsi_dma_map failed. */
+ 			rc = SCSI_MLQUEUE_HOST_BUSY;
+ 	} else if (dev->hba_ioaccel_enabled) {
+ 		hpsa_cmd_init(h, c->cmdindex, c);
+ 		c->cmd_type = CMD_SCSI;
+ 		c->scsi_cmd = cmd;
+ 		rc = hpsa_scsi_ioaccel_direct_map(h, c);
+ 		if (rc < 0)     /* scsi_dma_map failed. */
+ 			rc = SCSI_MLQUEUE_HOST_BUSY;
+ 	}
+ 	return rc;
+ }
+ 
+ static void hpsa_command_resubmit_worker(struct work_struct *work)
+ {
+ 	struct scsi_cmnd *cmd;
+ 	struct hpsa_scsi_dev_t *dev;
+ 	struct CommandList *c = container_of(work, struct CommandList, work);
+ 
+ 	cmd = c->scsi_cmd;
+ 	dev = cmd->device->hostdata;
+ 	if (!dev) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		return hpsa_cmd_free_and_done(c->h, c, cmd);
+ 	}
+ 	if (c->abort_pending)
+ 		return hpsa_cmd_abort_and_free(c->h, c, cmd);
+ 	if (c->cmd_type == CMD_IOACCEL2) {
+ 		struct ctlr_info *h = c->h;
+ 		struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+ 		int rc;
+ 
+ 		if (c2->error_data.serv_response ==
+ 				IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL) {
+ 			rc = hpsa_ioaccel_submit(h, c, cmd, dev->scsi3addr);
+ 			if (rc == 0)
+ 				return;
+ 			if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+ 				/*
+ 				 * If we get here, it means dma mapping failed.
+ 				 * Try again via scsi mid layer, which will
+ 				 * then get SCSI_MLQUEUE_HOST_BUSY.
+ 				 */
+ 				cmd->result = DID_IMM_RETRY << 16;
+ 				return hpsa_cmd_free_and_done(h, c, cmd);
+ 			}
+ 			/* else, fall thru and resubmit down CISS path */
+ 		}
+ 	}
+ 	hpsa_cmd_partial_init(c->h, c->cmdindex, c);
+ 	if (hpsa_ciss_submit(c->h, c, cmd, dev->scsi3addr)) {
+ 		/*
+ 		 * If we get here, it means dma mapping failed. Try
+ 		 * again via scsi mid layer, which will then get
+ 		 * SCSI_MLQUEUE_HOST_BUSY.
+ 		 *
+ 		 * hpsa_ciss_submit will have already freed c
+ 		 * if it encountered a dma mapping failure.
+ 		 */
+ 		cmd->result = DID_IMM_RETRY << 16;
+ 		cmd->scsi_done(cmd);
+ 	}
+ }
+ 
+ /* Running in struct Scsi_Host->host_lock less mode */
+ static int hpsa_scsi_queue_command(struct Scsi_Host *sh, struct scsi_cmnd *cmd)
+ {
+ 	struct ctlr_info *h;
+ 	struct hpsa_scsi_dev_t *dev;
+ 	unsigned char scsi3addr[8];
+ 	struct CommandList *c;
+ 	int rc = 0;
+ 
+ 	/* Get the ptr to our adapter structure out of cmd->host. */
+ 	h = sdev_to_hba(cmd->device);
+ 	dev = cmd->device->hostdata;
+ 	if (!dev) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 	memcpy(scsi3addr, dev->scsi3addr, sizeof(scsi3addr));
+ 
+ 	if (unlikely(lockup_detected(h))) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 	c = cmd_alloc(h);
+ 
+ 	if (unlikely(lockup_detected(h))) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd_free(h, c);
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * Call alternate submit routine for I/O accelerated commands.
+ 	 * Retries always go down the normal I/O path.
+ 	 */
+ 	if (likely(cmd->retries == 0 &&
+ 		cmd->request->cmd_type == REQ_TYPE_FS &&
+ 		h->acciopath_status)) {
+ 		rc = hpsa_ioaccel_submit(h, c, cmd, scsi3addr);
+ 		if (rc == 0)
+ 			return 0;
+ 		if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+ 			cmd_free(h, c);
+ 			return SCSI_MLQUEUE_HOST_BUSY;
+ 		}
+ 	}
+ 	return hpsa_ciss_submit(h, c, cmd, scsi3addr);
+ }
+ 
+ static void hpsa_scan_complete(struct ctlr_info *h)
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  {
  	unsigned long flags;
  
@@@ -4280,6 -4983,48 +4574,51 @@@ static int hpsa_send_abort(struct ctlr_
  	return rc;
  }
  
++<<<<<<< HEAD
++=======
+ static void setup_ioaccel2_abort_cmd(struct CommandList *c, struct ctlr_info *h,
+ 	struct CommandList *command_to_abort, int reply_queue)
+ {
+ 	struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+ 	struct hpsa_tmf_struct *ac = (struct hpsa_tmf_struct *) c2;
+ 	struct io_accel2_cmd *c2a =
+ 		&h->ioaccel2_cmd_pool[command_to_abort->cmdindex];
+ 	struct scsi_cmnd *scmd = command_to_abort->scsi_cmd;
+ 	struct hpsa_scsi_dev_t *dev = scmd->device->hostdata;
+ 
+ 	/*
+ 	 * We're overlaying struct hpsa_tmf_struct on top of something which
+ 	 * was allocated as a struct io_accel2_cmd, so we better be sure it
+ 	 * actually fits, and doesn't overrun the error info space.
+ 	 */
+ 	BUILD_BUG_ON(sizeof(struct hpsa_tmf_struct) >
+ 			sizeof(struct io_accel2_cmd));
+ 	BUG_ON(offsetof(struct io_accel2_cmd, error_data) <
+ 			offsetof(struct hpsa_tmf_struct, error_len) +
+ 				sizeof(ac->error_len));
+ 
+ 	c->cmd_type = IOACCEL2_TMF;
+ 	c->scsi_cmd = SCSI_CMD_BUSY;
+ 
+ 	/* Adjust the DMA address to point to the accelerated command buffer */
+ 	c->busaddr = (u32) h->ioaccel2_cmd_pool_dhandle +
+ 				(c->cmdindex * sizeof(struct io_accel2_cmd));
+ 	BUG_ON(c->busaddr & 0x0000007F);
+ 
+ 	memset(ac, 0, sizeof(*c2)); /* yes this is correct */
+ 	ac->iu_type = IOACCEL2_IU_TMF_TYPE;
+ 	ac->reply_queue = reply_queue;
+ 	ac->tmf = IOACCEL2_TMF_ABORT;
+ 	ac->it_nexus = cpu_to_le32(dev->ioaccel_handle);
+ 	memset(ac->lun_id, 0, sizeof(ac->lun_id));
+ 	ac->tag = cpu_to_le64(c->cmdindex << DIRECT_LOOKUP_SHIFT);
+ 	ac->abort_tag = cpu_to_le64(le32_to_cpu(c2a->Tag));
+ 	ac->error_ptr = cpu_to_le64(c->busaddr +
+ 			offsetof(struct io_accel2_cmd, error_data));
+ 	ac->error_len = cpu_to_le32(sizeof(c2->error_data));
+ }
+ 
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  /* ioaccel2 path firmware cannot handle abort task requests.
   * Change abort requests to physical target reset, and send to the
   * address of the physical disk used for the ioaccel 2 command.
@@@ -4424,54 -5257,73 +4763,76 @@@ static int hpsa_eh_abort_handler(struc
  	/* Get SCSI command to be aborted */
  	abort = (struct CommandList *) sc->host_scribble;
  	if (abort == NULL) {
 -		/* This can happen if the command already completed. */
 -		return SUCCESS;
 -	}
 -	refcount = atomic_inc_return(&abort->refcount);
 -	if (refcount == 1) { /* Command is done already. */
 -		cmd_free(h, abort);
 -		return SUCCESS;
 -	}
 -
 -	/* Don't bother trying the abort if we know it won't work. */
 -	if (abort->cmd_type != CMD_IOACCEL2 &&
 -		abort->cmd_type != CMD_IOACCEL1 && !dev->supports_aborts) {
 -		cmd_free(h, abort);
 +		dev_err(&h->pdev->dev, "%s FAILED, Command to abort is NULL.\n",
 +				msg);
  		return FAILED;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Check that we're aborting the right command.
+ 	 * It's possible the CommandList already completed and got re-used.
+ 	 */
+ 	if (abort->scsi_cmd != sc) {
+ 		cmd_free(h, abort);
+ 		return SUCCESS;
+ 	}
+ 
+ 	abort->abort_pending = true;
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  	hpsa_get_tag(h, abort, &taglower, &tagupper);
 -	reply_queue = hpsa_extract_reply_queue(h, abort);
  	ml += sprintf(msg+ml, "Tag:0x%08x:%08x ", tagupper, taglower);
 -	as  = abort->scsi_cmd;
 +	as  = (struct scsi_cmnd *) abort->scsi_cmd;
  	if (as != NULL)
 -		ml += sprintf(msg+ml,
 -			"CDBLen: %d CDB: 0x%02x%02x... SN: 0x%lx ",
 -			as->cmd_len, as->cmnd[0], as->cmnd[1],
 -			as->serial_number);
 -	dev_warn(&h->pdev->dev, "%s BEING SENT\n", msg);
 -	hpsa_show_dev_msg(KERN_WARNING, h, dev, "Aborting command");
 -
 +		ml += sprintf(msg+ml, "Command:0x%x SN:0x%lx ",
 +			as->cmnd[0], as->serial_number);
 +	dev_dbg(&h->pdev->dev, "%s\n", msg);
 +	dev_warn(&h->pdev->dev, "Abort request on C%d:B%d:T%d:L%d\n",
 +		h->scsi_host->host_no, dev->bus, dev->target, dev->lun);
  	/*
  	 * Command is in flight, or possibly already completed
  	 * by the firmware (but not to the scsi mid layer) but we can't
  	 * distinguish which.  Send the abort down.
  	 */
 -	if (wait_for_available_abort_cmd(h)) {
 -		dev_warn(&h->pdev->dev,
 -			"%s FAILED, timeout waiting for an abort command to become available.\n",
 -			msg);
 -		cmd_free(h, abort);
 -		return FAILED;
 -	}
 -	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort, reply_queue);
 -	atomic_inc(&h->abort_cmds_available);
 -	wake_up_all(&h->abort_cmd_wait_queue);
 +	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort);
  	if (rc != 0) {
 -		dev_warn(&h->pdev->dev, "%s SENT, FAILED\n", msg);
 -		hpsa_show_dev_msg(KERN_WARNING, h, dev,
 -				"FAILED to abort command");
 -		cmd_free(h, abort);
 +		dev_dbg(&h->pdev->dev, "%s Request FAILED.\n", msg);
 +		dev_warn(&h->pdev->dev, "FAILED abort on device C%d:B%d:T%d:L%d\n",
 +			h->scsi_host->host_no,
 +			dev->bus, dev->target, dev->lun);
  		return FAILED;
  	}
++<<<<<<< HEAD
 +	dev_info(&h->pdev->dev, "%s REQUEST SUCCEEDED.\n", msg);
 +
 +	/* If the abort(s) above completed and actually aborted the
 +	 * command, then the command to be aborted should already be
 +	 * completed.  If not, wait around a bit more to see if they
 +	 * manage to complete normally.
 +	 */
 +#define ABORT_COMPLETE_WAIT_SECS 30
 +	for (i = 0; i < ABORT_COMPLETE_WAIT_SECS * 10; i++) {
 +		if (test_bit(abort->cmdindex & (BITS_PER_LONG - 1),
 +				h->cmd_pool_bits +
 +				(abort->cmdindex / BITS_PER_LONG)))
 +			msleep(100);
 +		else
 +			return SUCCESS;
 +	}
 +	dev_warn(&h->pdev->dev, "%s FAILED. Aborted command has not completed after %d seconds.\n",
 +		msg, ABORT_COMPLETE_WAIT_SECS);
 +	return FAILED;
++=======
+ 	dev_info(&h->pdev->dev, "%s SENT, SUCCESS\n", msg);
+ 	wait_event(h->abort_sync_wait_queue,
+ 		   abort->scsi_cmd != sc || lockup_detected(h));
+ 	cmd_free(h, abort);
+ 	return !lockup_detected(h) ? SUCCESS : FAILED;
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  }
  
 +
  /*
   * For operations that cannot sleep, a command block is allocated at init,
   * and managed by cmd_alloc() and cmd_free() using a simple bitmap to track
@@@ -4763,13 -5563,11 +5124,14 @@@ static int hpsa_passthru_ioctl(struct c
  			memset(buff, 0, iocommand.buf_size);
  		}
  	}
 -	c = cmd_alloc(h);
 -
 +	c = cmd_special_alloc(h);
 +	if (c == NULL) {
 +		rc = -ENOMEM;
 +		goto out_kfree;
 +	}
  	/* Fill in the command type */
  	c->cmd_type = CMD_IOCTL_PEND;
+ 	c->scsi_cmd = SCSI_CMD_BUSY;
  	/* Fill in Command Header */
  	c->Header.ReplyQueue = 0; /* unused in simple mode */
  	if (iocommand.buf_size > 0) {	/* buffer to fill */
@@@ -4898,12 -5700,10 +5260,13 @@@ static int hpsa_big_passthru_ioctl(stru
  		data_ptr += sz;
  		sg_used++;
  	}
 -	c = cmd_alloc(h);
 -
 +	c = cmd_special_alloc(h);
 +	if (c == NULL) {
 +		status = -ENOMEM;
 +		goto cleanup1;
 +	}
  	c->cmd_type = CMD_IOCTL_PEND;
+ 	c->scsi_cmd = SCSI_CMD_BUSY;
  	c->Header.ReplyQueue = 0;
  	c->Header.SGList = (u8) sg_used;
  	c->Header.SGTotal = cpu_to_le16(sg_used);
@@@ -5069,9 -5844,10 +5432,10 @@@ static int fill_cmd(struct CommandList 
  	int cmd_type)
  {
  	int pci_dir = XFER_NONE;
 -	u64 tag; /* for commands to be aborted */
 +	struct CommandList *a; /* for commands to be aborted */
  
  	c->cmd_type = CMD_IOCTL_PEND;
+ 	c->scsi_cmd = SCSI_CMD_BUSY;
  	c->Header.ReplyQueue = 0;
  	if (buff != NULL && size > 0) {
  		c->Header.SGList = 1;
@@@ -6786,10 -7635,13 +7150,15 @@@ reinit_after_soft_reset
  	       h->intr[h->intr_mode], dac ? "" : " not");
  	rc = hpsa_alloc_cmd_pool(h);
  	if (rc)
 -		goto clean4;	/* irq, pci, lockup, wq/aer/h */
 -	rc = hpsa_alloc_sg_chain_blocks(h);
 -	if (rc)
 -		goto clean5;	/* cmd, irq, pci, lockup, wq/aer/h */
 +		goto clean2_and_free_irqs;
 +	if (hpsa_allocate_sg_chain_blocks(h))
 +		goto clean4;
  	init_waitqueue_head(&h->scan_wait_queue);
++<<<<<<< HEAD
++=======
+ 	init_waitqueue_head(&h->abort_cmd_wait_queue);
+ 	init_waitqueue_head(&h->abort_sync_wait_queue);
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  	h->scan_finished = 1; /* no scan currently in progress */
  
  	pci_set_drvdata(pdev, h);
diff --cc drivers/scsi/hpsa.h
index bb1c5c5da1f2,7cb8586dc192..000000000000
--- a/drivers/scsi/hpsa.h
+++ b/drivers/scsi/hpsa.h
@@@ -236,6 -261,12 +236,15 @@@ struct ctlr_info 
  	struct list_head offline_device_list;
  	int	acciopath_status;
  	int	raid_offload_debug;
++<<<<<<< HEAD
++=======
+ 	int	needs_abort_tags_swizzled;
+ 	struct workqueue_struct *resubmit_wq;
+ 	struct workqueue_struct *rescan_ctlr_wq;
+ 	atomic_t abort_cmds_available;
+ 	wait_queue_head_t abort_cmd_wait_queue;
+ 	wait_queue_head_t abort_sync_wait_queue;
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  };
  
  struct offline_device_entry {
diff --cc drivers/scsi/hpsa_cmd.h
index 9a0de7e7efe1,f98640241ab7..000000000000
--- a/drivers/scsi/hpsa_cmd.h
+++ b/drivers/scsi/hpsa_cmd.h
@@@ -403,7 -426,22 +403,26 @@@ struct CommandList 
  	int			   cmd_type;
  	long			   cmdindex;
  	struct completion *waiting;
++<<<<<<< HEAD
 +	void   *scsi_cmd;
++=======
+ 	struct scsi_cmnd *scsi_cmd;
+ 	struct work_struct work;
+ 
+ 	/*
+ 	 * For commands using either of the two "ioaccel" paths to
+ 	 * bypass the RAID stack and go directly to the physical disk
+ 	 * phys_disk is a pointer to the hpsa_scsi_dev_t to which the
+ 	 * i/o is destined.  We need to store that here because the command
+ 	 * may potentially encounter TASK SET FULL and need to be resubmitted
+ 	 * For "normal" i/o's not using the "ioaccel" paths, phys_disk is
+ 	 * not used.
+ 	 */
+ 	struct hpsa_scsi_dev_t *phys_disk;
+ 
+ 	int abort_pending;
+ 	atomic_t refcount; /* Must be last to avoid memset in hpsa_cmd_init() */
++>>>>>>> a58e7e53b410 (hpsa: don't return abort request until target is complete)
  } __aligned(COMMANDLIST_ALIGNMENT);
  
  /* Max S/G elements in I/O accelerator command */
* Unmerged path drivers/scsi/hpsa.c
* Unmerged path drivers/scsi/hpsa.h
* Unmerged path drivers/scsi/hpsa_cmd.h
