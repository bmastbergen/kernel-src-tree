percpu: implement [__]alloc_percpu_gfp()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Tejun Heo <tj@kernel.org>
commit 5835d96e9ce4efdba8c6cefffc2f1575925456de
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/5835d96e.failed

Now that pcpu_alloc_area() can allocate only from populated areas,
it's easy to add atomic allocation support to [__]alloc_percpu().
Update pcpu_alloc() so that it accepts @gfp and skips all the blocking
operations and allocates only from the populated areas if @gfp doesn't
contain GFP_KERNEL.  New interface functions [__]alloc_percpu_gfp()
are added.

While this means that atomic allocations are possible, this isn't
complete yet as there's no mechanism to ensure that certain amount of
populated areas is kept available and atomic allocations may keep
failing under certain conditions.

	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 5835d96e9ce4efdba8c6cefffc2f1575925456de)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/percpu.c
diff --cc mm/percpu.c
index 162dd6a056ba,c52b93117dc2..000000000000
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@@ -756,7 -777,7 +753,11 @@@ static void __percpu *pcpu_alloc(size_
  			spin_lock_irqsave(&pcpu_lock, flags);
  		}
  
++<<<<<<< HEAD
 +		off = pcpu_alloc_area(chunk, size, align);
++=======
+ 		off = pcpu_alloc_area(chunk, size, align, is_atomic);
++>>>>>>> 5835d96e9ce4 (percpu: implement [__]alloc_percpu_gfp())
  		if (off >= 0)
  			goto area_found;
  
@@@ -787,7 -810,7 +790,11 @@@ restart
  				goto restart;
  			}
  
++<<<<<<< HEAD
 +			off = pcpu_alloc_area(chunk, size, align);
++=======
+ 			off = pcpu_alloc_area(chunk, size, align, is_atomic);
++>>>>>>> 5835d96e9ce4 (percpu: implement [__]alloc_percpu_gfp())
  			if (off >= 0)
  				goto area_found;
  		}
diff --git a/include/linux/percpu.h b/include/linux/percpu.h
index d56038effeef..89ef08537994 100644
--- a/include/linux/percpu.h
+++ b/include/linux/percpu.h
@@ -159,12 +159,17 @@ extern void __init setup_per_cpu_areas(void);
 #endif
 extern void __init percpu_init_late(void);
 
+extern void __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp);
 extern void __percpu *__alloc_percpu(size_t size, size_t align);
 extern void free_percpu(void __percpu *__pdata);
 extern phys_addr_t per_cpu_ptr_to_phys(void *addr);
 
-#define alloc_percpu(type)	\
-	(typeof(type) __percpu *)__alloc_percpu(sizeof(type), __alignof__(type))
+#define alloc_percpu_gfp(type, gfp)					\
+	(typeof(type) __percpu *)__alloc_percpu_gfp(sizeof(type),	\
+						__alignof__(type), gfp)
+#define alloc_percpu(type)						\
+	(typeof(type) __percpu *)__alloc_percpu(sizeof(type),		\
+						__alignof__(type))
 
 /*
  * Branching function to split up a function into a set of functions that
* Unmerged path mm/percpu.c
