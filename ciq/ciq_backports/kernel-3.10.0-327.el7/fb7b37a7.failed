tcp: add init_cookie_seq method to tcp_request_sock_ops

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Octavian Purdila <octavian.purdila@intel.com>
commit fb7b37a7f3d6f7b7ba05ee526fee96810d5b92a8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/fb7b37a7.failed

Move the specific IPv4/IPv6 cookie sequence initialization to a new
method in tcp_request_sock_ops in preparation for unifying
tcp_v4_conn_request and tcp_v6_conn_request.

	Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit fb7b37a7f3d6f7b7ba05ee526fee96810d5b92a8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/tcp.h
#	net/ipv4/tcp_ipv4.c
#	net/ipv6/tcp_ipv6.c
diff --cc include/net/tcp.h
index ba45accd7103,086d00ec6d8b..000000000000
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@@ -497,39 -491,25 +497,53 @@@ static inline u32 tcp_cookie_time(void
  	return val;
  }
  
++<<<<<<< HEAD
 +extern u32 __cookie_v4_init_sequence(const struct iphdr *iph,
 +				     const struct tcphdr *th, u16 *mssp);
 +extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
 +				     __u16 *mss);
 +#else
 +static inline __u32 cookie_v4_init_sequence(struct sock *sk,
 +					    struct sk_buff *skb,
 +					    __u16 *mss)
 +{
 +	return 0;
 +}
++=======
+ u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
+ 			      u16 *mssp);
+ __u32 cookie_v4_init_sequence(struct sock *sk, const struct sk_buff *skb,
+ 			      __u16 *mss);
++>>>>>>> fb7b37a7f3d6 (tcp: add init_cookie_seq method to tcp_request_sock_ops)
  #endif
  
 -__u32 cookie_init_timestamp(struct request_sock *req);
 -bool cookie_check_timestamp(struct tcp_options_received *opt, struct net *net,
 -			    bool *ecn_ok);
 +extern __u32 cookie_init_timestamp(struct request_sock *req);
 +extern bool cookie_check_timestamp(struct tcp_options_received *opt,
 +				struct net *net, bool *ecn_ok);
  
  /* From net/ipv6/syncookies.c */
 -int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
 -		      u32 cookie);
 -struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 +extern int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
 +			     u32 cookie);
 +extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
  #ifdef CONFIG_SYN_COOKIES
++<<<<<<< HEAD
 +extern u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
 +				     const struct tcphdr *th, u16 *mssp);
 +extern __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
 +				     __u16 *mss);
 +#else
 +static inline __u32 cookie_v6_init_sequence(struct sock *sk,
 +					    struct sk_buff *skb,
 +					    __u16 *mss)
 +{
 +	return 0;
 +}
++=======
+ u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
+ 			      const struct tcphdr *th, u16 *mssp);
+ __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
+ 			      __u16 *mss);
++>>>>>>> fb7b37a7f3d6 (tcp: add init_cookie_seq method to tcp_request_sock_ops)
  #endif
  /* tcp_output.c */
  
@@@ -1579,11 -1599,33 +1593,39 @@@ struct tcp_request_sock_ops 
  						  const struct request_sock *req,
  						  const struct sk_buff *skb);
  #endif
++<<<<<<< HEAD
 +};
 +
 +extern int tcpv4_offload_init(void);
++=======
+ 	void (*init_req)(struct request_sock *req, struct sock *sk,
+ 			 struct sk_buff *skb);
+ #ifdef CONFIG_SYN_COOKIES
+ 	__u32 (*cookie_init_seq)(struct sock *sk, const struct sk_buff *skb,
+ 				 __u16 *mss);
+ #endif
+ };
+ 
+ #ifdef CONFIG_SYN_COOKIES
+ static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
+ 					 struct sock *sk, struct sk_buff *skb,
+ 					 __u16 *mss)
+ {
+ 	return ops->cookie_init_seq(sk, skb, mss);
+ }
+ #else
+ static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
+ 					 struct sock *sk, struct sk_buff *skb,
+ 					 __u16 *mss)
+ {
+ 	return 0;
+ }
+ #endif
+ 
+ int tcpv4_offload_init(void);
++>>>>>>> fb7b37a7f3d6 (tcp: add init_cookie_seq method to tcp_request_sock_ops)
  
 -void tcp_v4_init(void);
 -void tcp_init(void);
 +extern void tcp_v4_init(void);
 +extern void tcp_init(void);
  
  #endif	/* _TCP_H */
diff --cc net/ipv4/tcp_ipv4.c
index 7c1eb4426934,8c69e44c287b..000000000000
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@@ -1247,193 -1258,16 +1247,201 @@@ struct request_sock_ops tcp_request_soc
  	.syn_ack_timeout = 	tcp_syn_ack_timeout,
  };
  
 -static const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {
  #ifdef CONFIG_TCP_MD5SIG
 +static const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {
  	.md5_lookup	=	tcp_v4_reqsk_md5_lookup,
  	.calc_md5_hash	=	tcp_v4_md5_hash_skb,
++<<<<<<< HEAD
++=======
+ #endif
+ 	.init_req	=	tcp_v4_init_req,
+ #ifdef CONFIG_SYN_COOKIES
+ 	.cookie_init_seq =	cookie_v4_init_sequence,
+ #endif
++>>>>>>> fb7b37a7f3d6 (tcp: add init_cookie_seq method to tcp_request_sock_ops)
  };
 +#endif
 +
 +static bool tcp_fastopen_check(struct sock *sk, struct sk_buff *skb,
 +			       struct request_sock *req,
 +			       struct tcp_fastopen_cookie *foc,
 +			       struct tcp_fastopen_cookie *valid_foc)
 +{
 +	bool skip_cookie = false;
 +	struct fastopen_queue *fastopenq;
 +
 +	if (likely(!fastopen_cookie_present(foc))) {
 +		/* See include/net/tcp.h for the meaning of these knobs */
 +		if ((sysctl_tcp_fastopen & TFO_SERVER_ALWAYS) ||
 +		    ((sysctl_tcp_fastopen & TFO_SERVER_COOKIE_NOT_REQD) &&
 +		    (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq + 1)))
 +			skip_cookie = true; /* no cookie to validate */
 +		else
 +			return false;
 +	}
 +	fastopenq = inet_csk(sk)->icsk_accept_queue.fastopenq;
 +	/* A FO option is present; bump the counter. */
 +	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPFASTOPENPASSIVE);
 +
 +	/* Make sure the listener has enabled fastopen, and we don't
 +	 * exceed the max # of pending TFO requests allowed before trying
 +	 * to validating the cookie in order to avoid burning CPU cycles
 +	 * unnecessarily.
 +	 *
 +	 * XXX (TFO) - The implication of checking the max_qlen before
 +	 * processing a cookie request is that clients can't differentiate
 +	 * between qlen overflow causing Fast Open to be disabled
 +	 * temporarily vs a server not supporting Fast Open at all.
 +	 */
 +	if ((sysctl_tcp_fastopen & TFO_SERVER_ENABLE) == 0 ||
 +	    fastopenq == NULL || fastopenq->max_qlen == 0)
 +		return false;
 +
 +	if (fastopenq->qlen >= fastopenq->max_qlen) {
 +		struct request_sock *req1;
 +		spin_lock(&fastopenq->lock);
 +		req1 = fastopenq->rskq_rst_head;
 +		if ((req1 == NULL) || time_after(req1->expires, jiffies)) {
 +			spin_unlock(&fastopenq->lock);
 +			NET_INC_STATS_BH(sock_net(sk),
 +			    LINUX_MIB_TCPFASTOPENLISTENOVERFLOW);
 +			/* Avoid bumping LINUX_MIB_TCPFASTOPENPASSIVEFAIL*/
 +			foc->len = -1;
 +			return false;
 +		}
 +		fastopenq->rskq_rst_head = req1->dl_next;
 +		fastopenq->qlen--;
 +		spin_unlock(&fastopenq->lock);
 +		reqsk_free(req1);
 +	}
 +	if (skip_cookie) {
 +		tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +		return true;
 +	}
 +
 +	if (foc->len == TCP_FASTOPEN_COOKIE_SIZE) {
 +		if ((sysctl_tcp_fastopen & TFO_SERVER_COOKIE_NOT_CHKED) == 0) {
 +			tcp_fastopen_cookie_gen(ip_hdr(skb)->saddr,
 +						ip_hdr(skb)->daddr, valid_foc);
 +			if ((valid_foc->len != TCP_FASTOPEN_COOKIE_SIZE) ||
 +			    memcmp(&foc->val[0], &valid_foc->val[0],
 +			    TCP_FASTOPEN_COOKIE_SIZE) != 0)
 +				return false;
 +			valid_foc->len = -1;
 +		}
 +		/* Acknowledge the data received from the peer. */
 +		tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +		return true;
 +	} else if (foc->len == 0) { /* Client requesting a cookie */
 +		tcp_fastopen_cookie_gen(ip_hdr(skb)->saddr,
 +					ip_hdr(skb)->daddr, valid_foc);
 +		NET_INC_STATS_BH(sock_net(sk),
 +		    LINUX_MIB_TCPFASTOPENCOOKIEREQD);
 +	} else {
 +		/* Client sent a cookie with wrong size. Treat it
 +		 * the same as invalid and return a valid one.
 +		 */
 +		tcp_fastopen_cookie_gen(ip_hdr(skb)->saddr,
 +					ip_hdr(skb)->daddr, valid_foc);
 +	}
 +	return false;
 +}
 +
 +static int tcp_v4_conn_req_fastopen(struct sock *sk,
 +				    struct sk_buff *skb,
 +				    struct sk_buff *skb_synack,
 +				    struct request_sock *req)
 +{
 +	struct tcp_sock *tp = tcp_sk(sk);
 +	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
 +	const struct inet_request_sock *ireq = inet_rsk(req);
 +	struct sock *child;
 +	int err;
 +
 +	req->num_retrans = 0;
 +	req->num_timeout = 0;
 +	req->sk = NULL;
 +
 +	child = inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb, req, NULL);
 +	if (child == NULL) {
 +		NET_INC_STATS_BH(sock_net(sk),
 +				 LINUX_MIB_TCPFASTOPENPASSIVEFAIL);
 +		kfree_skb(skb_synack);
 +		return -1;
 +	}
 +	err = ip_build_and_send_pkt(skb_synack, sk, ireq->ir_loc_addr,
 +				    ireq->ir_rmt_addr, ireq->opt);
 +	err = net_xmit_eval(err);
 +	if (!err)
 +		tcp_rsk(req)->snt_synack = tcp_time_stamp;
 +	/* XXX (TFO) - is it ok to ignore error and continue? */
 +
 +	spin_lock(&queue->fastopenq->lock);
 +	queue->fastopenq->qlen++;
 +	spin_unlock(&queue->fastopenq->lock);
 +
 +	/* Initialize the child socket. Have to fix some values to take
 +	 * into account the child is a Fast Open socket and is created
 +	 * only out of the bits carried in the SYN packet.
 +	 */
 +	tp = tcp_sk(child);
 +
 +	tp->fastopen_rsk = req;
 +	/* Do a hold on the listner sk so that if the listener is being
 +	 * closed, the child that has been accepted can live on and still
 +	 * access listen_lock.
 +	 */
 +	sock_hold(sk);
 +	tcp_rsk(req)->listener = sk;
 +
 +	/* RFC1323: The window in SYN & SYN/ACK segments is never
 +	 * scaled. So correct it appropriately.
 +	 */
 +	tp->snd_wnd = ntohs(tcp_hdr(skb)->window);
 +
 +	/* Activate the retrans timer so that SYNACK can be retransmitted.
 +	 * The request socket is not added to the SYN table of the parent
 +	 * because it's been added to the accept queue directly.
 +	 */
 +	inet_csk_reset_xmit_timer(child, ICSK_TIME_RETRANS,
 +	    TCP_TIMEOUT_INIT, TCP_RTO_MAX);
 +
 +	/* Add the child socket directly into the accept queue */
 +	inet_csk_reqsk_queue_add(sk, req, child);
 +
 +	/* Now finish processing the fastopen child socket. */
 +	inet_csk(child)->icsk_af_ops->rebuild_header(child);
 +	tcp_init_congestion_control(child);
 +	tcp_mtup_init(child);
 +	tcp_init_metrics(child);
 +	tcp_init_buffer_space(child);
 +
 +	/* Queue the data carried in the SYN packet. We need to first
 +	 * bump skb's refcnt because the caller will attempt to free it.
 +	 *
 +	 * XXX (TFO) - we honor a zero-payload TFO request for now.
 +	 * (Any reason not to?)
 +	 */
 +	if (TCP_SKB_CB(skb)->end_seq == TCP_SKB_CB(skb)->seq + 1) {
 +		/* Don't queue the skb if there is no payload in SYN.
 +		 * XXX (TFO) - How about SYN+FIN?
 +		 */
 +		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +	} else {
 +		skb = skb_get(skb);
 +		skb_dst_drop(skb);
 +		__skb_pull(skb, tcp_hdr(skb)->doff * 4);
 +		skb_set_owner_r(skb, child);
 +		__skb_queue_tail(&child->sk_receive_queue, skb);
 +		tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
 +		tp->syn_data_acked = 1;
 +	}
 +	sk->sk_data_ready(sk, 0);
 +	bh_unlock_sock(child);
 +	sock_put(child);
 +	WARN_ON(req->sk == NULL);
 +	return 0;
 +}
  
  int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
  {
diff --cc net/ipv6/tcp_ipv6.c
index fdc8f8ca98c1,17710cffddaa..000000000000
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@@ -723,15 -752,19 +723,23 @@@ struct request_sock_ops tcp6_request_so
  	.send_ack	=	tcp_v6_reqsk_send_ack,
  	.destructor	=	tcp_v6_reqsk_destructor,
  	.send_reset	=	tcp_v6_send_reset,
 -	.syn_ack_timeout =	tcp_syn_ack_timeout,
 +	.syn_ack_timeout = 	tcp_syn_ack_timeout,
  };
  
 -static const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {
  #ifdef CONFIG_TCP_MD5SIG
 +static const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {
  	.md5_lookup	=	tcp_v6_reqsk_md5_lookup,
  	.calc_md5_hash	=	tcp_v6_md5_hash_skb,
++<<<<<<< HEAD
++=======
+ #endif
+ 	.init_req	=	tcp_v6_init_req,
+ #ifdef CONFIG_SYN_COOKIES
+ 	.cookie_init_seq =	cookie_v6_init_sequence,
+ #endif
++>>>>>>> fb7b37a7f3d6 (tcp: add init_cookie_seq method to tcp_request_sock_ops)
  };
 +#endif
  
  static void tcp_v6_send_response(struct sk_buff *skb, u32 seq, u32 ack, u32 win,
  				 u32 tsval, u32 tsecr, int oif,
@@@ -1023,23 -1062,9 +1031,23 @@@ static int tcp_v6_conn_request(struct s
  	if (!want_cookie || tmp_opt.tstamp_ok)
  		TCP_ECN_create_request(req, skb, sock_net(sk));
  
 +	ireq->ir_iif = sk->sk_bound_dev_if;
 +
 +	/* So that link locals have meaning */
 +	if (!sk->sk_bound_dev_if &&
 +	    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)
 +		ireq->ir_iif = inet6_iif(skb);
 +
  	if (!isn) {
 +		if (ipv6_opt_accepted(sk, skb) ||
 +		    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||
 +		    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {
 +			atomic_inc(&skb->users);
 +			ireq->pktopts = skb;
 +		}
 +
  		if (want_cookie) {
- 			isn = cookie_v6_init_sequence(sk, skb, &req->mss);
+ 			isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
  			req->cookie_ts = tmp_opt.tstamp_ok;
  			goto have_isn;
  		}
* Unmerged path include/net/tcp.h
* Unmerged path net/ipv4/tcp_ipv4.c
* Unmerged path net/ipv6/tcp_ipv6.c
