net: rename vlan_tx_* helpers since "tx" is misleading there

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] vlan: rename vlan_tx_* helpers since "tx" is misleading there (Ivan Vecera) [1200759]
Rebuild_FUZZ: 95.87%
commit-author Jiri Pirko <jiri@resnulli.us>
commit df8a39defad46b83694ea6dd868d332976d62cc0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/df8a39de.failed

The same macros are used for rx as well. So rename it.

	Signed-off-by: Jiri Pirko <jiri@resnulli.us>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit df8a39defad46b83694ea6dd868d332976d62cc0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/networking/filter.txt
#	drivers/net/ethernet/amd/xgbe/xgbe-drv.c
#	drivers/net/ethernet/freescale/gianfar.c
#	drivers/net/ethernet/intel/fm10k/fm10k_main.c
#	drivers/net/ethernet/intel/fm10k/fm10k_netdev.c
#	drivers/net/ethernet/mellanox/mlx4/en_tx.c
#	drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
#	drivers/net/macvtap.c
#	drivers/net/tun.c
#	drivers/net/usb/r8152.c
#	include/linux/if_vlan.h
#	include/net/pkt_sched.h
#	include/trace/events/net.h
#	net/bridge/br_netfilter.c
#	net/bridge/br_vlan.c
#	net/core/dev.c
#	net/core/skbuff.c
#	net/ipv4/geneve.c
#	net/openvswitch/actions.c
#	net/openvswitch/flow.c
#	net/openvswitch/vport.c
#	net/packet/af_packet.c
diff --cc Documentation/networking/filter.txt
index a06b48d2f5cc,9930ecfbb465..000000000000
--- a/Documentation/networking/filter.txt
+++ b/Documentation/networking/filter.txt
@@@ -277,10 -277,11 +277,16 @@@ Possible BPF extensions are shown in th
    mark                                  skb->mark
    queue                                 skb->queue_mapping
    hatype                                skb->dev->type
 -  rxhash                                skb->hash
 +  rxhash                                skb->rxhash
    cpu                                   raw_smp_processor_id()
++<<<<<<< HEAD
 +  vlan_tci                              vlan_tx_tag_get(skb)
 +  vlan_pr                               vlan_tx_tag_present(skb)
++=======
+   vlan_tci                              skb_vlan_tag_get(skb)
+   vlan_pr                               skb_vlan_tag_present(skb)
+   rand                                  prandom_u32()
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  
  These extensions can also be prefixed with '#'.
  Examples for low-level BPF:
diff --cc drivers/net/ethernet/freescale/gianfar.c
index 2375a01715a0,93ff846e96f1..000000000000
--- a/drivers/net/ethernet/freescale/gianfar.c
+++ b/drivers/net/ethernet/freescale/gianfar.c
@@@ -2085,24 -2229,26 +2085,35 @@@ static int gfar_start_xmit(struct sk_bu
  	base = tx_queue->tx_bd_base;
  	regs = tx_queue->grp->regs;
  
++<<<<<<< HEAD
++=======
+ 	do_csum = (CHECKSUM_PARTIAL == skb->ip_summed);
+ 	do_vlan = skb_vlan_tag_present(skb);
+ 	do_tstamp = (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+ 		    priv->hwts_tx_en;
+ 
+ 	if (do_csum || do_vlan)
+ 		fcb_len = GMAC_FCB_LEN;
+ 
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  	/* check if time stamp should be generated */
 -	if (unlikely(do_tstamp))
 -		fcb_len = GMAC_FCB_LEN + GMAC_TXPAL_LEN;
 +	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP &&
 +		     priv->hwts_tx_en)) {
 +		do_tstamp = 1;
 +		fcb_length = GMAC_FCB_LEN + GMAC_TXPAL_LEN;
 +	}
  
  	/* make space for additional header when fcb is needed */
 -	if (fcb_len && unlikely(skb_headroom(skb) < fcb_len)) {
 +	if (((skb->ip_summed == CHECKSUM_PARTIAL) ||
 +	     vlan_tx_tag_present(skb) ||
 +	     unlikely(do_tstamp)) &&
 +	    (skb_headroom(skb) < fcb_length)) {
  		struct sk_buff *skb_new;
  
 -		skb_new = skb_realloc_headroom(skb, fcb_len);
 +		skb_new = skb_realloc_headroom(skb, fcb_length);
  		if (!skb_new) {
  			dev->stats.tx_errors++;
 -			dev_kfree_skb_any(skb);
 +			kfree_skb(skb);
  			return NETDEV_TX_OK;
  		}
  
diff --cc drivers/net/ethernet/mellanox/mlx4/en_tx.c
index 2ddc399b76aa,359bb1286eb5..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
@@@ -635,13 -682,14 +635,13 @@@ u16 mlx4_en_select_queue(struct net_dev
  	if (dev->num_tc)
  		return skb_tx_hash(dev, skb);
  
- 	if (vlan_tx_tag_present(skb))
- 		up = vlan_tx_tag_get(skb) >> VLAN_PRIO_SHIFT;
+ 	if (skb_vlan_tag_present(skb))
+ 		up = skb_vlan_tag_get(skb) >> VLAN_PRIO_SHIFT;
  
 -	return fallback(dev, skb) % rings_p_up + up * rings_p_up;
 +	return __netdev_pick_tx(dev, skb) % rings_p_up + up * rings_p_up;
  }
  
 -static void mlx4_bf_copy(void __iomem *dst, const void *src,
 -			 unsigned int bytecnt)
 +static void mlx4_bf_copy(void __iomem *dst, unsigned long *src, unsigned bytecnt)
  {
  	__iowrite64_copy(dst, src, bytecnt / 8);
  }
@@@ -683,34 -742,11 +683,39 @@@ netdev_tx_t mlx4_en_xmit(struct sk_buf
  		goto tx_drop;
  	}
  
++<<<<<<< HEAD
 +	tx_ind = skb->queue_mapping;
 +	ring = priv->tx_ring[tx_ind];
 +	if (vlan_tx_tag_present(skb))
 +		vlan_tag = vlan_tx_tag_get(skb);
++=======
+ 	if (skb_vlan_tag_present(skb))
+ 		vlan_tag = skb_vlan_tag_get(skb);
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  
 +	/* Check available TXBBs And 2K spare for prefetch */
 +	if (unlikely(((int)(ring->prod - ring->cons)) >
 +		     ring->size - HEADROOM - MAX_DESC_TXBBS)) {
 +		/* every full Tx ring stops queue */
 +		netif_tx_stop_queue(ring->tx_queue);
 +		ring->queue_stopped++;
 +
 +		/* If queue was emptied after the if, and before the
 +		 * stop_queue - need to wake the queue, or else it will remain
 +		 * stopped forever.
 +		 * Need a memory barrier to make sure ring->cons was not
 +		 * updated before queue was stopped.
 +		 */
 +		wmb();
  
 -	netdev_txq_bql_enqueue_prefetchw(ring->tx_queue);
 +		if (unlikely(((int)(ring->prod - ring->cons)) <=
 +			     ring->size - HEADROOM - MAX_DESC_TXBBS)) {
 +			netif_tx_wake_queue(ring->tx_queue);
 +			ring->wake_queue++;
 +		} else {
 +			return NETDEV_TX_BUSY;
 +		}
 +	}
  
  	/* Track current inflight packets for performance analysis */
  	AVG_PERF_COUNTER(priv->pstats.inflight_avg,
@@@ -880,12 -918,26 +885,30 @@@
  
  	skb_tx_timestamp(skb);
  
++<<<<<<< HEAD
 +	if (ring->bf_enabled && desc_size <= MAX_BF && !bounce && !vlan_tx_tag_present(skb)) {
 +		tx_desc->ctrl.bf_qpn |= cpu_to_be32(ring->doorbell_qpn);
++=======
+ 	/* Check available TXBBs And 2K spare for prefetch */
+ 	stop_queue = (int)(ring->prod - ring_cons) >
+ 		      ring->size - HEADROOM - MAX_DESC_TXBBS;
+ 	if (unlikely(stop_queue)) {
+ 		netif_tx_stop_queue(ring->tx_queue);
+ 		ring->queue_stopped++;
+ 	}
+ 	send_doorbell = !skb->xmit_more || netif_xmit_stopped(ring->tx_queue);
+ 
+ 	real_size = (real_size / 16) & 0x3f;
+ 
+ 	if (ring->bf_enabled && desc_size <= MAX_BF && !bounce &&
+ 	    !skb_vlan_tag_present(skb) && send_doorbell) {
+ 		tx_desc->ctrl.bf_qpn = ring->doorbell_qpn |
+ 				       cpu_to_be32(real_size);
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  
  		op_own |= htonl((bf_index & 0xffff) << 8);
 -		/* Ensure new descriptor hits memory
 -		 * before setting ownership of this descriptor to HW
 -		 */
 +		/* Ensure new descirptor hits memory
 +		* before setting ownership of this descriptor to HW */
  		wmb();
  		tx_desc->ctrl.owner_opcode = op_own;
  
@@@ -898,14 -950,51 +921,25 @@@
  
  		ring->bf.offset ^= ring->bf.buf_size;
  	} else {
++<<<<<<< HEAD
 +		/* Ensure new descirptor hits memory
 +		* before setting ownership of this descriptor to HW */
++=======
+ 		tx_desc->ctrl.vlan_tag = cpu_to_be16(vlan_tag);
+ 		tx_desc->ctrl.ins_vlan = MLX4_WQE_CTRL_INS_VLAN *
+ 			!!skb_vlan_tag_present(skb);
+ 		tx_desc->ctrl.fence_size = real_size;
+ 
+ 		/* Ensure new descriptor hits memory
+ 		 * before setting ownership of this descriptor to HW
+ 		 */
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  		wmb();
  		tx_desc->ctrl.owner_opcode = op_own;
 -		if (send_doorbell) {
 -			wmb();
 -			/* Since there is no iowrite*_native() that writes the
 -			 * value as is, without byteswapping - using the one
 -			 * the doesn't do byteswapping in the relevant arch
 -			 * endianness.
 -			 */
 -#if defined(__LITTLE_ENDIAN)
 -			iowrite32(
 -#else
 -			iowrite32be(
 -#endif
 -				  ring->doorbell_qpn,
 -				  ring->bf.uar->map + MLX4_SEND_DOORBELL);
 -		} else {
 -			ring->xmit_more++;
 -		}
 +		wmb();
 +		iowrite32be(ring->doorbell_qpn, ring->bf.uar->map + MLX4_SEND_DOORBELL);
  	}
  
 -	if (unlikely(stop_queue)) {
 -		/* If queue was emptied after the if (stop_queue) , and before
 -		 * the netif_tx_stop_queue() - need to wake the queue,
 -		 * or else it will remain stopped forever.
 -		 * Need a memory barrier to make sure ring->cons was not
 -		 * updated before queue was stopped.
 -		 */
 -		smp_rmb();
 -
 -		ring_cons = ACCESS_ONCE(ring->cons);
 -		if (unlikely(((int)(ring->prod - ring_cons)) <=
 -			     ring->size - HEADROOM - MAX_DESC_TXBBS)) {
 -			netif_tx_wake_queue(ring->tx_queue);
 -			ring->wake_queue++;
 -		}
 -	}
  	return NETDEV_TX_OK;
  
  tx_drop_unmap:
diff --cc drivers/net/macvtap.c
index c64c7701d09b,d0ed5694dd7d..000000000000
--- a/drivers/net/macvtap.c
+++ b/drivers/net/macvtap.c
@@@ -706,8 -645,13 +706,18 @@@ static int macvtap_skb_to_vnet_hdr(cons
  
  	if (skb->ip_summed == CHECKSUM_PARTIAL) {
  		vnet_hdr->flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;
++<<<<<<< HEAD
 +		vnet_hdr->csum_start = skb_checksum_start_offset(skb);
 +		vnet_hdr->csum_offset = skb->csum_offset;
++=======
+ 		if (skb_vlan_tag_present(skb))
+ 			vnet_hdr->csum_start = cpu_to_macvtap16(q,
+ 				skb_checksum_start_offset(skb) + VLAN_HLEN);
+ 		else
+ 			vnet_hdr->csum_start = cpu_to_macvtap16(q,
+ 				skb_checksum_start_offset(skb));
+ 		vnet_hdr->csum_offset = cpu_to_macvtap16(q, skb->csum_offset);
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  	} else if (skb->ip_summed == CHECKSUM_UNNECESSARY) {
  		vnet_hdr->flags = VIRTIO_NET_HDR_F_DATA_VALID;
  	} /* else everything is zero */
@@@ -890,44 -807,37 +900,48 @@@ static ssize_t macvtap_put_user(struct 
  	if (q->flags & IFF_VNET_HDR) {
  		struct virtio_net_hdr vnet_hdr;
  		vnet_hdr_len = q->vnet_hdr_sz;
 -		if (iov_iter_count(iter) < vnet_hdr_len)
 +		if ((len -= vnet_hdr_len) < 0)
  			return -EINVAL;
  
 -		macvtap_skb_to_vnet_hdr(q, skb, &vnet_hdr);
 +		ret = macvtap_skb_to_vnet_hdr(skb, &vnet_hdr);
 +		if (ret)
 +			return ret;
  
 -		if (copy_to_iter(&vnet_hdr, sizeof(vnet_hdr), iter) !=
 -		    sizeof(vnet_hdr))
 +		if (memcpy_toiovecend(iv, (void *)&vnet_hdr, 0, sizeof(vnet_hdr)))
  			return -EFAULT;
 -
 -		iov_iter_advance(iter, vnet_hdr_len - sizeof(vnet_hdr));
  	}
 -	total = vnet_hdr_len;
 -	total += skb->len;
 +	copied = vnet_hdr_len;
  
++<<<<<<< HEAD
 +	if (!vlan_tx_tag_present(skb))
 +		len = min_t(int, skb->len, len);
 +	else {
 +		int copy;
++=======
+ 	if (skb_vlan_tag_present(skb)) {
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  		struct {
  			__be16 h_vlan_proto;
  			__be16 h_vlan_TCI;
  		} veth;
  		veth.h_vlan_proto = skb->vlan_proto;
- 		veth.h_vlan_TCI = htons(vlan_tx_tag_get(skb));
+ 		veth.h_vlan_TCI = htons(skb_vlan_tag_get(skb));
  
  		vlan_offset = offsetof(struct vlan_ethhdr, h_vlan_proto);
 -		total += VLAN_HLEN;
 +		len = min_t(int, skb->len + VLAN_HLEN, len);
  
 -		ret = skb_copy_datagram_iter(skb, 0, iter, vlan_offset);
 -		if (ret || !iov_iter_count(iter))
 +		copy = min_t(int, vlan_offset, len);
 +		ret = skb_copy_datagram_const_iovec(skb, 0, iv, copied, copy);
 +		len -= copy;
 +		copied += copy;
 +		if (ret || !len)
  			goto done;
  
 -		ret = copy_to_iter(&veth, sizeof(veth), iter);
 -		if (ret != sizeof(veth) || !iov_iter_count(iter))
 +		copy = min_t(int, sizeof(veth), len);
 +		ret = memcpy_toiovecend(iv, (void *)&veth, copied, copy);
 +		len -= copy;
 +		copied += copy;
 +		if (ret || !len)
  			goto done;
  	}
  
diff --cc drivers/net/tun.c
index 6c2b390ec103,be196e89ab6c..000000000000
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@@ -1270,16 -1252,28 +1270,29 @@@ static ssize_t tun_chr_aio_write(struc
  static ssize_t tun_put_user(struct tun_struct *tun,
  			    struct tun_file *tfile,
  			    struct sk_buff *skb,
 -			    struct iov_iter *iter)
 +			    const struct iovec *iv, int len)
  {
  	struct tun_pi pi = { 0, skb->protocol };
 -	ssize_t total;
 -	int vlan_offset = 0;
 -	int vlan_hlen = 0;
 -	int vnet_hdr_sz = 0;
 +	ssize_t total = 0;
  
++<<<<<<< HEAD
 +	if (!(tun->flags & TUN_NO_PI)) {
 +		if ((len -= sizeof(pi)) < 0)
++=======
+ 	if (skb_vlan_tag_present(skb))
+ 		vlan_hlen = VLAN_HLEN;
+ 
+ 	if (tun->flags & IFF_VNET_HDR)
+ 		vnet_hdr_sz = tun->vnet_hdr_sz;
+ 
+ 	total = skb->len + vlan_hlen + vnet_hdr_sz;
+ 
+ 	if (!(tun->flags & IFF_NO_PI)) {
+ 		if (iov_iter_count(iter) < sizeof(pi))
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  			return -EINVAL;
  
 -		total += sizeof(pi);
 -		if (iov_iter_count(iter) < total) {
 +		if (len < skb->len) {
  			/* Packet will be striped */
  			pi.flags |= TUN_PKT_STRIP;
  		}
@@@ -1331,19 -1323,38 +1344,24 @@@
  			gso.flags = VIRTIO_NET_HDR_F_DATA_VALID;
  		} /* else everything is zero */
  
 -		if (copy_to_iter(&gso, sizeof(gso), iter) != sizeof(gso))
 +		if (unlikely(memcpy_toiovecend(iv, (void *)&gso, total,
 +					       sizeof(gso))))
  			return -EFAULT;
 -
 -		iov_iter_advance(iter, vnet_hdr_sz - sizeof(gso));
 +		total += tun->vnet_hdr_sz;
  	}
  
 -	if (vlan_hlen) {
 -		int ret;
 -		struct {
 -			__be16 h_vlan_proto;
 -			__be16 h_vlan_TCI;
 -		} veth;
 +	len = min_t(int, skb->len, len);
  
++<<<<<<< HEAD
 +	skb_copy_datagram_const_iovec(skb, 0, iv, total, len);
 +	total += skb->len;
++=======
+ 		veth.h_vlan_proto = skb->vlan_proto;
+ 		veth.h_vlan_TCI = htons(skb_vlan_tag_get(skb));
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  
 -		vlan_offset = offsetof(struct vlan_ethhdr, h_vlan_proto);
 -
 -		ret = skb_copy_datagram_iter(skb, 0, iter, vlan_offset);
 -		if (ret || !iov_iter_count(iter))
 -			goto done;
 -
 -		ret = copy_to_iter(&veth, sizeof(veth), iter);
 -		if (ret != sizeof(veth) || !iov_iter_count(iter))
 -			goto done;
 -	}
 -
 -	skb_copy_datagram_iter(skb, vlan_offset, iter, skb->len - vlan_offset);
 -
 -done:
  	tun->dev->stats.tx_packets++;
 -	tun->dev->stats.tx_bytes += skb->len + vlan_hlen;
 +	tun->dev->stats.tx_bytes += len;
  
  	return total;
  }
diff --cc drivers/net/usb/r8152.c
index 14e519888631,e519e6a269b9..000000000000
--- a/drivers/net/usb/r8152.c
+++ b/drivers/net/usb/r8152.c
@@@ -775,230 -1070,657 +775,359 @@@ static void read_bulk_callback(struct u
  	case -ENOENT:
  		return;	/* the urb is in unlink state */
  	case -ETIME:
 -		if (net_ratelimit())
 -			netdev_warn(netdev, "maybe reset is needed?\n");
 -		break;
 +		pr_warn_ratelimited("may be reset is needed?..\n");
 +		goto goon;
  	default:
 -		if (net_ratelimit())
 -			netdev_warn(netdev, "Rx status %d\n", status);
 -		break;
 -	}
 +		pr_warn_ratelimited("Rx status %d\n", status);
 +		goto goon;
 +	}
 +
 +	/* protect against short packets (tell me why we got some?!?) */
 +	if (urb->actual_length < sizeof(*rx_desc))
 +		goto goon;
 +
 +
 +	rx_desc = (struct rx_desc *)urb->transfer_buffer;
 +	pkt_len = le32_to_cpu(rx_desc->opts1) & RX_LEN_MASK;
 +	if (urb->actual_length < sizeof(struct rx_desc) + pkt_len)
 +		goto goon;
 +
 +	skb = netdev_alloc_skb_ip_align(netdev, pkt_len);
 +	if (!skb)
 +		goto goon;
 +
 +	memcpy(skb->data, tp->rx_skb->data + sizeof(struct rx_desc), pkt_len);
 +	skb_put(skb, pkt_len);
 +	skb->protocol = eth_type_trans(skb, netdev);
 +	netif_rx(skb);
 +	stats->rx_packets++;
 +	stats->rx_bytes += pkt_len;
 +goon:
 +	usb_fill_bulk_urb(tp->rx_urb, tp->udev, usb_rcvbulkpipe(tp->udev, 1),
 +		      tp->rx_skb->data, RTL8152_RMS + sizeof(struct rx_desc),
 +		      (usb_complete_t)read_bulk_callback, tp);
 +	result = usb_submit_urb(tp->rx_urb, GFP_ATOMIC);
 +	if (result == -ENODEV) {
 +		netif_device_detach(tp->netdev);
 +	} else if (result) {
 +		set_bit(RX_URB_FAIL, &tp->flags);
 +		goto resched;
 +	} else {
++<<<<<<< HEAD
 +		clear_bit(RX_URB_FAIL, &tp->flags);
++=======
++		struct net_device_stats *stats;
+ 
 -	r8152_submit_rx(tp, agg, GFP_ATOMIC);
++drop:
++		stats = &tp->netdev->stats;
++		stats->tx_dropped++;
++		dev_kfree_skb(skb);
++	}
+ }
+ 
 -static void write_bulk_callback(struct urb *urb)
++/* msdn_giant_send_check()
++ * According to the document of microsoft, the TCP Pseudo Header excludes the
++ * packet length for IPv6 TCP large packets.
++ */
++static int msdn_giant_send_check(struct sk_buff *skb)
+ {
 -	struct net_device_stats *stats;
 -	struct net_device *netdev;
 -	struct tx_agg *agg;
 -	struct r8152 *tp;
 -	int status = urb->status;
 -
 -	agg = urb->context;
 -	if (!agg)
 -		return;
++	const struct ipv6hdr *ipv6h;
++	struct tcphdr *th;
++	int ret;
+ 
 -	tp = agg->context;
 -	if (!tp)
 -		return;
++	ret = skb_cow_head(skb, 0);
++	if (ret)
++		return ret;
+ 
 -	netdev = tp->netdev;
 -	stats = &netdev->stats;
 -	if (status) {
 -		if (net_ratelimit())
 -			netdev_warn(netdev, "Tx status %d\n", status);
 -		stats->tx_errors += agg->skb_num;
 -	} else {
 -		stats->tx_packets += agg->skb_num;
 -		stats->tx_bytes += agg->skb_len;
 -	}
++	ipv6h = ipv6_hdr(skb);
++	th = tcp_hdr(skb);
+ 
 -	spin_lock(&tp->tx_lock);
 -	list_add_tail(&agg->list, &tp->tx_free);
 -	spin_unlock(&tp->tx_lock);
++	th->check = 0;
++	th->check = ~tcp_v6_check(0, &ipv6h->saddr, &ipv6h->daddr, 0);
+ 
 -	usb_autopm_put_interface_async(tp->intf);
++	return ret;
++}
+ 
 -	if (!netif_carrier_ok(netdev))
 -		return;
++static inline void rtl_tx_vlan_tag(struct tx_desc *desc, struct sk_buff *skb)
++{
++	if (skb_vlan_tag_present(skb)) {
++		u32 opts2;
+ 
 -	if (!test_bit(WORK_ENABLE, &tp->flags))
 -		return;
++		opts2 = TX_VLAN_TAG | swab16(skb_vlan_tag_get(skb));
++		desc->opts2 |= cpu_to_le32(opts2);
++	}
++}
+ 
 -	if (test_bit(RTL8152_UNPLUG, &tp->flags))
 -		return;
++static inline void rtl_rx_vlan_tag(struct rx_desc *desc, struct sk_buff *skb)
++{
++	u32 opts2 = le32_to_cpu(desc->opts2);
+ 
 -	if (!skb_queue_empty(&tp->tx_queue))
 -		tasklet_schedule(&tp->tl);
++	if (opts2 & RX_VLAN_TAG)
++		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
++				       swab16(opts2 & 0xffff));
+ }
+ 
 -static void intr_callback(struct urb *urb)
++static int r8152_tx_csum(struct r8152 *tp, struct tx_desc *desc,
++			 struct sk_buff *skb, u32 len, u32 transport_offset)
+ {
 -	struct r8152 *tp;
 -	__le16 *d;
 -	int status = urb->status;
 -	int res;
++	u32 mss = skb_shinfo(skb)->gso_size;
++	u32 opts1, opts2 = 0;
++	int ret = TX_CSUM_SUCCESS;
+ 
 -	tp = urb->context;
 -	if (!tp)
 -		return;
++	WARN_ON_ONCE(len > TX_LEN_MAX);
+ 
 -	if (!test_bit(WORK_ENABLE, &tp->flags))
 -		return;
++	opts1 = len | TX_FS | TX_LS;
+ 
 -	if (test_bit(RTL8152_UNPLUG, &tp->flags))
 -		return;
 -
 -	switch (status) {
 -	case 0:			/* success */
 -		break;
 -	case -ECONNRESET:	/* unlink */
 -	case -ESHUTDOWN:
 -		netif_device_detach(tp->netdev);
 -	case -ENOENT:
 -	case -EPROTO:
 -		netif_info(tp, intr, tp->netdev,
 -			   "Stop submitting intr, status %d\n", status);
 -		return;
 -	case -EOVERFLOW:
 -		netif_info(tp, intr, tp->netdev, "intr status -EOVERFLOW\n");
 -		goto resubmit;
 -	/* -EPIPE:  should clear the halt */
 -	default:
 -		netif_info(tp, intr, tp->netdev, "intr status %d\n", status);
 -		goto resubmit;
 -	}
 -
 -	d = urb->transfer_buffer;
 -	if (INTR_LINK & __le16_to_cpu(d[0])) {
 -		if (!(tp->speed & LINK_STATUS)) {
 -			set_bit(RTL8152_LINK_CHG, &tp->flags);
 -			schedule_delayed_work(&tp->schedule, 0);
 -		}
 -	} else {
 -		if (tp->speed & LINK_STATUS) {
 -			set_bit(RTL8152_LINK_CHG, &tp->flags);
 -			schedule_delayed_work(&tp->schedule, 0);
 -		}
 -	}
 -
 -resubmit:
 -	res = usb_submit_urb(urb, GFP_ATOMIC);
 -	if (res == -ENODEV) {
 -		set_bit(RTL8152_UNPLUG, &tp->flags);
 -		netif_device_detach(tp->netdev);
 -	} else if (res) {
 -		netif_err(tp, intr, tp->netdev,
 -			  "can't resubmit intr, status %d\n", res);
 -	}
 -}
 -
 -static inline void *rx_agg_align(void *data)
 -{
 -	return (void *)ALIGN((uintptr_t)data, RX_ALIGN);
 -}
 -
 -static inline void *tx_agg_align(void *data)
 -{
 -	return (void *)ALIGN((uintptr_t)data, TX_ALIGN);
 -}
 -
 -static void free_all_mem(struct r8152 *tp)
 -{
 -	int i;
 -
 -	for (i = 0; i < RTL8152_MAX_RX; i++) {
 -		usb_free_urb(tp->rx_info[i].urb);
 -		tp->rx_info[i].urb = NULL;
 -
 -		kfree(tp->rx_info[i].buffer);
 -		tp->rx_info[i].buffer = NULL;
 -		tp->rx_info[i].head = NULL;
 -	}
 -
 -	for (i = 0; i < RTL8152_MAX_TX; i++) {
 -		usb_free_urb(tp->tx_info[i].urb);
 -		tp->tx_info[i].urb = NULL;
 -
 -		kfree(tp->tx_info[i].buffer);
 -		tp->tx_info[i].buffer = NULL;
 -		tp->tx_info[i].head = NULL;
 -	}
 -
 -	usb_free_urb(tp->intr_urb);
 -	tp->intr_urb = NULL;
 -
 -	kfree(tp->intr_buff);
 -	tp->intr_buff = NULL;
 -}
 -
 -static int alloc_all_mem(struct r8152 *tp)
 -{
 -	struct net_device *netdev = tp->netdev;
 -	struct usb_interface *intf = tp->intf;
 -	struct usb_host_interface *alt = intf->cur_altsetting;
 -	struct usb_host_endpoint *ep_intr = alt->endpoint + 2;
 -	struct urb *urb;
 -	int node, i;
 -	u8 *buf;
 -
 -	node = netdev->dev.parent ? dev_to_node(netdev->dev.parent) : -1;
 -
 -	spin_lock_init(&tp->rx_lock);
 -	spin_lock_init(&tp->tx_lock);
 -	INIT_LIST_HEAD(&tp->tx_free);
 -	skb_queue_head_init(&tp->tx_queue);
 -
 -	for (i = 0; i < RTL8152_MAX_RX; i++) {
 -		buf = kmalloc_node(agg_buf_sz, GFP_KERNEL, node);
 -		if (!buf)
 -			goto err1;
 -
 -		if (buf != rx_agg_align(buf)) {
 -			kfree(buf);
 -			buf = kmalloc_node(agg_buf_sz + RX_ALIGN, GFP_KERNEL,
 -					   node);
 -			if (!buf)
 -				goto err1;
 -		}
 -
 -		urb = usb_alloc_urb(0, GFP_KERNEL);
 -		if (!urb) {
 -			kfree(buf);
 -			goto err1;
 -		}
 -
 -		INIT_LIST_HEAD(&tp->rx_info[i].list);
 -		tp->rx_info[i].context = tp;
 -		tp->rx_info[i].urb = urb;
 -		tp->rx_info[i].buffer = buf;
 -		tp->rx_info[i].head = rx_agg_align(buf);
 -	}
 -
 -	for (i = 0; i < RTL8152_MAX_TX; i++) {
 -		buf = kmalloc_node(agg_buf_sz, GFP_KERNEL, node);
 -		if (!buf)
 -			goto err1;
 -
 -		if (buf != tx_agg_align(buf)) {
 -			kfree(buf);
 -			buf = kmalloc_node(agg_buf_sz + TX_ALIGN, GFP_KERNEL,
 -					   node);
 -			if (!buf)
 -				goto err1;
 -		}
 -
 -		urb = usb_alloc_urb(0, GFP_KERNEL);
 -		if (!urb) {
 -			kfree(buf);
 -			goto err1;
 -		}
 -
 -		INIT_LIST_HEAD(&tp->tx_info[i].list);
 -		tp->tx_info[i].context = tp;
 -		tp->tx_info[i].urb = urb;
 -		tp->tx_info[i].buffer = buf;
 -		tp->tx_info[i].head = tx_agg_align(buf);
 -
 -		list_add_tail(&tp->tx_info[i].list, &tp->tx_free);
 -	}
 -
 -	tp->intr_urb = usb_alloc_urb(0, GFP_KERNEL);
 -	if (!tp->intr_urb)
 -		goto err1;
 -
 -	tp->intr_buff = kmalloc(INTBUFSIZE, GFP_KERNEL);
 -	if (!tp->intr_buff)
 -		goto err1;
 -
 -	tp->intr_interval = (int)ep_intr->desc.bInterval;
 -	usb_fill_int_urb(tp->intr_urb, tp->udev, usb_rcvintpipe(tp->udev, 3),
 -			 tp->intr_buff, INTBUFSIZE, intr_callback,
 -			 tp, tp->intr_interval);
 -
 -	return 0;
 -
 -err1:
 -	free_all_mem(tp);
 -	return -ENOMEM;
 -}
 -
 -static struct tx_agg *r8152_get_tx_agg(struct r8152 *tp)
 -{
 -	struct tx_agg *agg = NULL;
 -	unsigned long flags;
 -
 -	if (list_empty(&tp->tx_free))
 -		return NULL;
 -
 -	spin_lock_irqsave(&tp->tx_lock, flags);
 -	if (!list_empty(&tp->tx_free)) {
 -		struct list_head *cursor;
 -
 -		cursor = tp->tx_free.next;
 -		list_del_init(cursor);
 -		agg = list_entry(cursor, struct tx_agg, list);
 -	}
 -	spin_unlock_irqrestore(&tp->tx_lock, flags);
 -
 -	return agg;
 -}
 -
 -static inline __be16 get_protocol(struct sk_buff *skb)
 -{
 -	__be16 protocol;
 -
 -	if (skb->protocol == htons(ETH_P_8021Q))
 -		protocol = vlan_eth_hdr(skb)->h_vlan_encapsulated_proto;
 -	else
 -		protocol = skb->protocol;
 -
 -	return protocol;
 -}
 -
 -/* r8152_csum_workaround()
 - * The hw limites the value the transport offset. When the offset is out of the
 - * range, calculate the checksum by sw.
 - */
 -static void r8152_csum_workaround(struct r8152 *tp, struct sk_buff *skb,
 -				  struct sk_buff_head *list)
 -{
 -	if (skb_shinfo(skb)->gso_size) {
 -		netdev_features_t features = tp->netdev->features;
 -		struct sk_buff_head seg_list;
 -		struct sk_buff *segs, *nskb;
 -
 -		features &= ~(NETIF_F_SG | NETIF_F_IPV6_CSUM | NETIF_F_TSO6);
 -		segs = skb_gso_segment(skb, features);
 -		if (IS_ERR(segs) || !segs)
 -			goto drop;
 -
 -		__skb_queue_head_init(&seg_list);
 -
 -		do {
 -			nskb = segs;
 -			segs = segs->next;
 -			nskb->next = NULL;
 -			__skb_queue_tail(&seg_list, nskb);
 -		} while (segs);
 -
 -		skb_queue_splice(&seg_list, list);
 -		dev_kfree_skb(skb);
 -	} else if (skb->ip_summed == CHECKSUM_PARTIAL) {
 -		if (skb_checksum_help(skb) < 0)
 -			goto drop;
 -
 -		__skb_queue_head(list, skb);
 -	} else {
 -		struct net_device_stats *stats;
 -
 -drop:
 -		stats = &tp->netdev->stats;
 -		stats->tx_dropped++;
 -		dev_kfree_skb(skb);
 -	}
 -}
 -
 -/* msdn_giant_send_check()
 - * According to the document of microsoft, the TCP Pseudo Header excludes the
 - * packet length for IPv6 TCP large packets.
 - */
 -static int msdn_giant_send_check(struct sk_buff *skb)
 -{
 -	const struct ipv6hdr *ipv6h;
 -	struct tcphdr *th;
 -	int ret;
 -
 -	ret = skb_cow_head(skb, 0);
 -	if (ret)
 -		return ret;
 -
 -	ipv6h = ipv6_hdr(skb);
 -	th = tcp_hdr(skb);
 -
 -	th->check = 0;
 -	th->check = ~tcp_v6_check(0, &ipv6h->saddr, &ipv6h->daddr, 0);
 -
 -	return ret;
 -}
 -
 -static inline void rtl_tx_vlan_tag(struct tx_desc *desc, struct sk_buff *skb)
 -{
 -	if (skb_vlan_tag_present(skb)) {
 -		u32 opts2;
 -
 -		opts2 = TX_VLAN_TAG | swab16(skb_vlan_tag_get(skb));
 -		desc->opts2 |= cpu_to_le32(opts2);
 -	}
 -}
 -
 -static inline void rtl_rx_vlan_tag(struct rx_desc *desc, struct sk_buff *skb)
 -{
 -	u32 opts2 = le32_to_cpu(desc->opts2);
 -
 -	if (opts2 & RX_VLAN_TAG)
 -		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
 -				       swab16(opts2 & 0xffff));
 -}
 -
 -static int r8152_tx_csum(struct r8152 *tp, struct tx_desc *desc,
 -			 struct sk_buff *skb, u32 len, u32 transport_offset)
 -{
 -	u32 mss = skb_shinfo(skb)->gso_size;
 -	u32 opts1, opts2 = 0;
 -	int ret = TX_CSUM_SUCCESS;
 -
 -	WARN_ON_ONCE(len > TX_LEN_MAX);
 -
 -	opts1 = len | TX_FS | TX_LS;
 -
 -	if (mss) {
 -		if (transport_offset > GTTCPHO_MAX) {
 -			netif_warn(tp, tx_err, tp->netdev,
 -				   "Invalid transport offset 0x%x for TSO\n",
 -				   transport_offset);
 -			ret = TX_CSUM_TSO;
 -			goto unavailable;
 -		}
++	if (mss) {
++		if (transport_offset > GTTCPHO_MAX) {
++			netif_warn(tp, tx_err, tp->netdev,
++				   "Invalid transport offset 0x%x for TSO\n",
++				   transport_offset);
++			ret = TX_CSUM_TSO;
++			goto unavailable;
++		}
+ 
+ 		switch (get_protocol(skb)) {
+ 		case htons(ETH_P_IP):
+ 			opts1 |= GTSENDV4;
+ 			break;
+ 
+ 		case htons(ETH_P_IPV6):
+ 			if (msdn_giant_send_check(skb)) {
+ 				ret = TX_CSUM_TSO;
+ 				goto unavailable;
+ 			}
+ 			opts1 |= GTSENDV6;
+ 			break;
+ 
+ 		default:
+ 			WARN_ON_ONCE(1);
+ 			break;
+ 		}
+ 
+ 		opts1 |= transport_offset << GTTCPHO_SHIFT;
+ 		opts2 |= min(mss, MSS_MAX) << MSS_SHIFT;
+ 	} else if (skb->ip_summed == CHECKSUM_PARTIAL) {
+ 		u8 ip_protocol;
+ 
+ 		if (transport_offset > TCPHO_MAX) {
+ 			netif_warn(tp, tx_err, tp->netdev,
+ 				   "Invalid transport offset 0x%x\n",
+ 				   transport_offset);
+ 			ret = TX_CSUM_NONE;
+ 			goto unavailable;
+ 		}
+ 
+ 		switch (get_protocol(skb)) {
+ 		case htons(ETH_P_IP):
+ 			opts2 |= IPV4_CS;
+ 			ip_protocol = ip_hdr(skb)->protocol;
+ 			break;
+ 
+ 		case htons(ETH_P_IPV6):
+ 			opts2 |= IPV6_CS;
+ 			ip_protocol = ipv6_hdr(skb)->nexthdr;
+ 			break;
+ 
+ 		default:
+ 			ip_protocol = IPPROTO_RAW;
+ 			break;
+ 		}
+ 
+ 		if (ip_protocol == IPPROTO_TCP)
+ 			opts2 |= TCP_CS;
+ 		else if (ip_protocol == IPPROTO_UDP)
+ 			opts2 |= UDP_CS;
+ 		else
+ 			WARN_ON_ONCE(1);
+ 
+ 		opts2 |= transport_offset << TCPHO_SHIFT;
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  	}
  
 -	desc->opts2 = cpu_to_le32(opts2);
 -	desc->opts1 = cpu_to_le32(opts1);
 -
 -unavailable:
 -	return ret;
 +	return;
 +resched:
 +	tasklet_schedule(&tp->tl);
  }
  
 -static int r8152_tx_agg_fill(struct r8152 *tp, struct tx_agg *agg)
 +static void rx_fixup(unsigned long data)
  {
 -	struct sk_buff_head skb_head, *tx_queue = &tp->tx_queue;
 -	int remain, ret;
 -	u8 *tx_data;
 +	struct r8152 *tp;
 +	int status;
  
 -	__skb_queue_head_init(&skb_head);
 -	spin_lock(&tx_queue->lock);
 -	skb_queue_splice_init(tx_queue, &skb_head);
 -	spin_unlock(&tx_queue->lock);
 +	tp = (struct r8152 *)data;
 +	if (!test_bit(WORK_ENABLE, &tp->flags))
 +		return;
  
 -	tx_data = agg->head;
 -	agg->skb_num = 0;
 -	agg->skb_len = 0;
 -	remain = agg_buf_sz;
 +	status = usb_submit_urb(tp->rx_urb, GFP_ATOMIC);
 +	if (status == -ENODEV) {
 +		netif_device_detach(tp->netdev);
 +	} else if (status) {
 +		set_bit(RX_URB_FAIL, &tp->flags);
 +		goto tlsched;
 +	} else {
 +		clear_bit(RX_URB_FAIL, &tp->flags);
 +	}
  
 -	while (remain >= ETH_ZLEN + sizeof(struct tx_desc)) {
 -		struct tx_desc *tx_desc;
 -		struct sk_buff *skb;
 -		unsigned int len;
 -		u32 offset;
 +	return;
 +tlsched:
 +	tasklet_schedule(&tp->tl);
 +}
  
 -		skb = __skb_dequeue(&skb_head);
 -		if (!skb)
 -			break;
 +static void write_bulk_callback(struct urb *urb)
 +{
 +	struct r8152 *tp;
 +	int status = urb->status;
  
 -		len = skb->len + sizeof(*tx_desc);
 +	tp = urb->context;
 +	if (!tp)
 +		return;
 +	dev_kfree_skb_irq(tp->tx_skb);
 +	if (!netif_device_present(tp->netdev))
 +		return;
 +	if (status)
 +		dev_info(&urb->dev->dev, "%s: Tx status %d\n",
 +			 tp->netdev->name, status);
 +	tp->netdev->trans_start = jiffies;
 +	netif_wake_queue(tp->netdev);
 +}
  
 -		if (len > remain) {
 -			__skb_queue_head(&skb_head, skb);
 -			break;
 -		}
 +static void rtl8152_tx_timeout(struct net_device *netdev)
 +{
 +	struct r8152 *tp = netdev_priv(netdev);
 +	struct net_device_stats *stats = rtl8152_get_stats(netdev);
 +	netif_warn(tp, tx_err, netdev, "Tx timeout.\n");
 +	usb_unlink_urb(tp->tx_urb);
 +	stats->tx_errors++;
 +}
  
 -		tx_data = tx_agg_align(tx_data);
 -		tx_desc = (struct tx_desc *)tx_data;
 +static void rtl8152_set_rx_mode(struct net_device *netdev)
 +{
 +	struct r8152 *tp = netdev_priv(netdev);
  
 -		offset = (u32)skb_transport_offset(skb);
 +	if (tp->speed & LINK_STATUS)
 +		set_bit(RTL8152_SET_RX_MODE, &tp->flags);
 +}
  
 -		if (r8152_tx_csum(tp, tx_desc, skb, skb->len, offset)) {
 -			r8152_csum_workaround(tp, skb, &skb_head);
 -			continue;
 -		}
 +static void _rtl8152_set_rx_mode(struct net_device *netdev)
 +{
 +	struct r8152 *tp = netdev_priv(netdev);
 +	u32 tmp, *mc_filter;	/* Multicast hash filter */
 +	u32 ocp_data;
  
 -		rtl_tx_vlan_tag(tx_desc, skb);
 +	mc_filter = kmalloc(sizeof(u32) * 2, GFP_KERNEL);
 +	if (!mc_filter) {
 +		netif_err(tp, link, netdev, "out of memory");
 +		return;
 +	}
  
 -		tx_data += sizeof(*tx_desc);
 +	clear_bit(RTL8152_SET_RX_MODE, &tp->flags);
 +	netif_stop_queue(netdev);
 +	ocp_data = ocp_read_dword(tp, MCU_TYPE_PLA, PLA_RCR);
 +	ocp_data &= ~RCR_ACPT_ALL;
 +	ocp_data |= RCR_AB | RCR_APM;
  
 -		len = skb->len;
 -		if (skb_copy_bits(skb, 0, tx_data, len) < 0) {
 -			struct net_device_stats *stats = &tp->netdev->stats;
 +	if (netdev->flags & IFF_PROMISC) {
 +		/* Unconditionally log net taps. */
 +		netif_notice(tp, link, netdev, "Promiscuous mode enabled\n");
 +		ocp_data |= RCR_AM | RCR_AAP;
 +		mc_filter[1] = mc_filter[0] = 0xffffffff;
 +	} else if ((netdev_mc_count(netdev) > multicast_filter_limit) ||
 +		   (netdev->flags & IFF_ALLMULTI)) {
 +		/* Too many to filter perfectly -- accept all multicasts. */
 +		ocp_data |= RCR_AM;
 +		mc_filter[1] = mc_filter[0] = 0xffffffff;
 +	} else {
 +		struct netdev_hw_addr *ha;
  
 -			stats->tx_dropped++;
 -			dev_kfree_skb_any(skb);
 -			tx_data -= sizeof(*tx_desc);
 -			continue;
 +		mc_filter[1] = mc_filter[0] = 0;
 +		netdev_for_each_mc_addr(ha, netdev) {
 +			int bit_nr = ether_crc(ETH_ALEN, ha->addr) >> 26;
 +			mc_filter[bit_nr >> 5] |= 1 << (bit_nr & 31);
 +			ocp_data |= RCR_AM;
  		}
 -
 -		tx_data += len;
 -		agg->skb_len += len;
 -		agg->skb_num++;
 -
 -		dev_kfree_skb_any(skb);
 -
 -		remain = agg_buf_sz - (int)(tx_agg_align(tx_data) - agg->head);
  	}
  
 -	if (!skb_queue_empty(&skb_head)) {
 -		spin_lock(&tx_queue->lock);
 -		skb_queue_splice(&skb_head, tx_queue);
 -		spin_unlock(&tx_queue->lock);
 -	}
 -
 -	netif_tx_lock(tp->netdev);
 -
 -	if (netif_queue_stopped(tp->netdev) &&
 -	    skb_queue_len(&tp->tx_queue) < tp->tx_qlen)
 -		netif_wake_queue(tp->netdev);
 -
 -	netif_tx_unlock(tp->netdev);
 -
 -	ret = usb_autopm_get_interface_async(tp->intf);
 -	if (ret < 0)
 -		goto out_tx_fill;
 -
 -	usb_fill_bulk_urb(agg->urb, tp->udev, usb_sndbulkpipe(tp->udev, 2),
 -			  agg->head, (int)(tx_data - (u8 *)agg->head),
 -			  (usb_complete_t)write_bulk_callback, agg);
 -
 -	ret = usb_submit_urb(agg->urb, GFP_ATOMIC);
 -	if (ret < 0)
 -		usb_autopm_put_interface_async(tp->intf);
 +	tmp = mc_filter[0];
 +	mc_filter[0] = __cpu_to_le32(swab32(mc_filter[1]));
 +	mc_filter[1] = __cpu_to_le32(swab32(tmp));
  
 -out_tx_fill:
 -	return ret;
 +	pla_ocp_write(tp, PLA_MAR, BYTE_EN_DWORD, sizeof(u32) * 2, mc_filter);
 +	ocp_write_dword(tp, MCU_TYPE_PLA, PLA_RCR, ocp_data);
 +	netif_wake_queue(netdev);
 +	kfree(mc_filter);
  }
  
 -static u8 r8152_rx_csum(struct r8152 *tp, struct rx_desc *rx_desc)
 +static netdev_tx_t rtl8152_start_xmit(struct sk_buff *skb,
 +					    struct net_device *netdev)
  {
 -	u8 checksum = CHECKSUM_NONE;
 -	u32 opts2, opts3;
 -
 -	if (tp->version == RTL_VER_01)
 -		goto return_result;
 +	struct r8152 *tp = netdev_priv(netdev);
 +	struct net_device_stats *stats = rtl8152_get_stats(netdev);
 +	struct tx_desc *tx_desc;
 +	int len, res;
  
 -	opts2 = le32_to_cpu(rx_desc->opts2);
 -	opts3 = le32_to_cpu(rx_desc->opts3);
 +	netif_stop_queue(netdev);
 +	len = skb->len;
 +	if (skb_header_cloned(skb) || skb_headroom(skb) < sizeof(*tx_desc)) {
 +		struct sk_buff *tx_skb;
  
 -	if (opts2 & RD_IPV4_CS) {
 -		if (opts3 & IPF)
 -			checksum = CHECKSUM_NONE;
 -		else if ((opts2 & RD_UDP_CS) && (opts3 & UDPF))
 -			checksum = CHECKSUM_NONE;
 -		else if ((opts2 & RD_TCP_CS) && (opts3 & TCPF))
 -			checksum = CHECKSUM_NONE;
 -		else
 -			checksum = CHECKSUM_UNNECESSARY;
 -	} else if (RD_IPV6_CS) {
 -		if ((opts2 & RD_UDP_CS) && !(opts3 & UDPF))
 -			checksum = CHECKSUM_UNNECESSARY;
 -		else if ((opts2 & RD_TCP_CS) && !(opts3 & TCPF))
 -			checksum = CHECKSUM_UNNECESSARY;
 +		tx_skb = skb_copy_expand(skb, sizeof(*tx_desc), 0, GFP_ATOMIC);
 +		dev_kfree_skb_any(skb);
 +		if (!tx_skb) {
 +			stats->tx_dropped++;
 +			netif_wake_queue(netdev);
 +			return NETDEV_TX_OK;
 +		}
 +		skb = tx_skb;
 +	}
 +	tx_desc = (struct tx_desc *)skb_push(skb, sizeof(*tx_desc));
 +	memset(tx_desc, 0, sizeof(*tx_desc));
 +	tx_desc->opts1 = cpu_to_le32((len & TX_LEN_MASK) | TX_FS | TX_LS);
 +	tp->tx_skb = skb;
 +	skb_tx_timestamp(skb);
 +	usb_fill_bulk_urb(tp->tx_urb, tp->udev, usb_sndbulkpipe(tp->udev, 2),
 +			  skb->data, skb->len,
 +			  (usb_complete_t)write_bulk_callback, tp);
 +	res = usb_submit_urb(tp->tx_urb, GFP_ATOMIC);
 +	if (res) {
 +		/* Can we get/handle EPIPE here? */
 +		if (res == -ENODEV) {
 +			netif_device_detach(tp->netdev);
 +		} else {
 +			netif_warn(tp, tx_err, netdev,
 +				   "failed tx_urb %d\n", res);
 +			stats->tx_errors++;
 +			netif_start_queue(netdev);
 +		}
 +	} else {
 +		stats->tx_packets++;
 +		stats->tx_bytes += skb->len;
  	}
  
 -return_result:
 -	return checksum;
 +	return NETDEV_TX_OK;
  }
  
 -static void rx_bottom(struct r8152 *tp)
 +static void r8152b_reset_packet_filter(struct r8152 *tp)
  {
 -	unsigned long flags;
 -	struct list_head *cursor, *next, rx_queue;
 -
 -	if (list_empty(&tp->rx_done))
 -		return;
 +	u32	ocp_data;
  
 -	INIT_LIST_HEAD(&rx_queue);
 -	spin_lock_irqsave(&tp->rx_lock, flags);
 -	list_splice_init(&tp->rx_done, &rx_queue);
 -	spin_unlock_irqrestore(&tp->rx_lock, flags);
 -
 -	list_for_each_safe(cursor, next, &rx_queue) {
 -		struct rx_desc *rx_desc;
 -		struct rx_agg *agg;
 -		int len_used = 0;
 -		struct urb *urb;
 -		u8 *rx_data;
 -
 -		list_del_init(cursor);
 -
 -		agg = list_entry(cursor, struct rx_agg, list);
 -		urb = agg->urb;
 -		if (urb->actual_length < ETH_ZLEN)
 -			goto submit;
 -
 -		rx_desc = agg->head;
 -		rx_data = agg->head;
 -		len_used += sizeof(struct rx_desc);
 -
 -		while (urb->actual_length > len_used) {
 -			struct net_device *netdev = tp->netdev;
 -			struct net_device_stats *stats = &netdev->stats;
 -			unsigned int pkt_len;
 -			struct sk_buff *skb;
 -
 -			pkt_len = le32_to_cpu(rx_desc->opts1) & RX_LEN_MASK;
 -			if (pkt_len < ETH_ZLEN)
 -				break;
 +	ocp_data = ocp_read_word(tp, MCU_TYPE_PLA, PLA_FMC);
 +	ocp_data &= ~FMC_FCR_MCU_EN;
 +	ocp_write_word(tp, MCU_TYPE_PLA, PLA_FMC, ocp_data);
 +	ocp_data |= FMC_FCR_MCU_EN;
 +	ocp_write_word(tp, MCU_TYPE_PLA, PLA_FMC, ocp_data);
 +}
  
 -			len_used += pkt_len;
 -			if (urb->actual_length < len_used)
 -				break;
 -
 -			pkt_len -= CRC_SIZE;
 -			rx_data += sizeof(struct rx_desc);
 -
 -			skb = netdev_alloc_skb_ip_align(netdev, pkt_len);
 -			if (!skb) {
 -				stats->rx_dropped++;
 -				goto find_next_rx;
 -			}
 +static void rtl8152_nic_reset(struct r8152 *tp)
 +{
 +	int	i;
  
 -			skb->ip_summed = r8152_rx_csum(tp, rx_desc);
 -			memcpy(skb->data, rx_data, pkt_len);
 -			skb_put(skb, pkt_len);
 -			skb->protocol = eth_type_trans(skb, netdev);
 -			rtl_rx_vlan_tag(rx_desc, skb);
 -			netif_receive_skb(skb);
 -			stats->rx_packets++;
 -			stats->rx_bytes += pkt_len;
 -
 -find_next_rx:
 -			rx_data = rx_agg_align(rx_data + pkt_len + CRC_SIZE);
 -			rx_desc = (struct rx_desc *)rx_data;
 -			len_used = (int)(rx_data - (u8 *)agg->head);
 -			len_used += sizeof(struct rx_desc);
 -		}
 +	ocp_write_byte(tp, MCU_TYPE_PLA, PLA_CR, CR_RST);
  
 -submit:
 -		r8152_submit_rx(tp, agg, GFP_ATOMIC);
 +	for (i = 0; i < 1000; i++) {
 +		if (!(ocp_read_byte(tp, MCU_TYPE_PLA, PLA_CR) & CR_RST))
 +			break;
 +		udelay(100);
  	}
  }
  
diff --cc include/linux/if_vlan.h
index 978291b95d4e,bea465f24ebb..000000000000
--- a/include/linux/if_vlan.h
+++ b/include/linux/if_vlan.h
@@@ -78,29 -78,10 +78,29 @@@ static inline int is_vlan_dev(struct ne
          return dev->priv_flags & IFF_802_1Q_VLAN;
  }
  
- #define vlan_tx_tag_present(__skb)	((__skb)->vlan_tci & VLAN_TAG_PRESENT)
- #define vlan_tx_tag_get(__skb)		((__skb)->vlan_tci & ~VLAN_TAG_PRESENT)
- #define vlan_tx_tag_get_id(__skb)	((__skb)->vlan_tci & VLAN_VID_MASK)
+ #define skb_vlan_tag_present(__skb)	((__skb)->vlan_tci & VLAN_TAG_PRESENT)
+ #define skb_vlan_tag_get(__skb)		((__skb)->vlan_tci & ~VLAN_TAG_PRESENT)
+ #define skb_vlan_tag_get_id(__skb)	((__skb)->vlan_tci & VLAN_VID_MASK)
  
 +#if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)
 +
 +extern struct net_device *__vlan_find_dev_deep(struct net_device *real_dev,
 +					       __be16 vlan_proto, u16 vlan_id);
 +extern struct net_device *vlan_dev_real_dev(const struct net_device *dev);
 +extern u16 vlan_dev_vlan_id(const struct net_device *dev);
 +
 +/**
 + *	struct vlan_priority_tci_mapping - vlan egress priority mappings
 + *	@priority: skb priority
 + *	@vlan_qos: vlan priority: (skb->priority << 13) & 0xE000
 + *	@next: pointer to next struct
 + */
 +struct vlan_priority_tci_mapping {
 +	u32					priority;
 +	u16					vlan_qos;
 +	struct vlan_priority_tci_mapping	*next;
 +};
 +
  /**
   *	struct vlan_pcpu_stats - VLAN percpu rx/tx stats
   *	@rx_packets: number of received packets
@@@ -326,6 -364,40 +326,43 @@@ static inline struct sk_buff *__vlan_pu
  	return skb;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * __vlan_hwaccel_push_inside - pushes vlan tag to the payload
+  * @skb: skbuff to tag
+  *
+  * Pushes the VLAN tag from @skb->vlan_tci inside to the payload.
+  *
+  * Following the skb_unshare() example, in case of error, the calling function
+  * doesn't have to worry about freeing the original skb.
+  */
+ static inline struct sk_buff *__vlan_hwaccel_push_inside(struct sk_buff *skb)
+ {
+ 	skb = vlan_insert_tag_set_proto(skb, skb->vlan_proto,
+ 					skb_vlan_tag_get(skb));
+ 	if (likely(skb))
+ 		skb->vlan_tci = 0;
+ 	return skb;
+ }
+ /*
+  * vlan_hwaccel_push_inside - pushes vlan tag to the payload
+  * @skb: skbuff to tag
+  *
+  * Checks is tag is present in @skb->vlan_tci and if it is, it pushes the
+  * VLAN tag from @skb->vlan_tci inside to the payload.
+  *
+  * Following the skb_unshare() example, in case of error, the calling function
+  * doesn't have to worry about freeing the original skb.
+  */
+ static inline struct sk_buff *vlan_hwaccel_push_inside(struct sk_buff *skb)
+ {
+ 	if (skb_vlan_tag_present(skb))
+ 		skb = __vlan_hwaccel_push_inside(skb);
+ 	return skb;
+ }
+ 
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  /**
   * __vlan_hwaccel_put_tag - hardware accelerated VLAN inserting
   * @skb: skbuff to tag
diff --cc include/net/pkt_sched.h
index 59ec3cd15d68,2342bf12cb78..000000000000
--- a/include/net/pkt_sched.h
+++ b/include/net/pkt_sched.h
@@@ -113,6 -115,17 +113,20 @@@ int tc_classify_compat(struct sk_buff *
  int tc_classify(struct sk_buff *skb, const struct tcf_proto *tp,
  		struct tcf_result *res);
  
++<<<<<<< HEAD
++=======
+ static inline __be16 tc_skb_protocol(const struct sk_buff *skb)
+ {
+ 	/* We need to take extra care in case the skb came via
+ 	 * vlan accelerated path. In that case, use skb->vlan_proto
+ 	 * as the original vlan header was already stripped.
+ 	 */
+ 	if (skb_vlan_tag_present(skb))
+ 		return skb->vlan_proto;
+ 	return skb->protocol;
+ }
+ 
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  /* Calculate maximal size of packet seen by hard_start_xmit
     routine of this device.
   */
diff --cc include/trace/events/net.h
index f99645d05a8f,49cc7c3de252..000000000000
--- a/include/trace/events/net.h
+++ b/include/trace/events/net.h
@@@ -9,6 -9,64 +9,66 @@@
  #include <linux/ip.h>
  #include <linux/tracepoint.h>
  
++<<<<<<< HEAD
++=======
+ TRACE_EVENT(net_dev_start_xmit,
+ 
+ 	TP_PROTO(const struct sk_buff *skb, const struct net_device *dev),
+ 
+ 	TP_ARGS(skb, dev),
+ 
+ 	TP_STRUCT__entry(
+ 		__string(	name,			dev->name	)
+ 		__field(	u16,			queue_mapping	)
+ 		__field(	const void *,		skbaddr		)
+ 		__field(	bool,			vlan_tagged	)
+ 		__field(	u16,			vlan_proto	)
+ 		__field(	u16,			vlan_tci	)
+ 		__field(	u16,			protocol	)
+ 		__field(	u8,			ip_summed	)
+ 		__field(	unsigned int,		len		)
+ 		__field(	unsigned int,		data_len	)
+ 		__field(	int,			network_offset	)
+ 		__field(	bool,			transport_offset_valid)
+ 		__field(	int,			transport_offset)
+ 		__field(	u8,			tx_flags	)
+ 		__field(	u16,			gso_size	)
+ 		__field(	u16,			gso_segs	)
+ 		__field(	u16,			gso_type	)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__assign_str(name, dev->name);
+ 		__entry->queue_mapping = skb->queue_mapping;
+ 		__entry->skbaddr = skb;
+ 		__entry->vlan_tagged = skb_vlan_tag_present(skb);
+ 		__entry->vlan_proto = ntohs(skb->vlan_proto);
+ 		__entry->vlan_tci = skb_vlan_tag_get(skb);
+ 		__entry->protocol = ntohs(skb->protocol);
+ 		__entry->ip_summed = skb->ip_summed;
+ 		__entry->len = skb->len;
+ 		__entry->data_len = skb->data_len;
+ 		__entry->network_offset = skb_network_offset(skb);
+ 		__entry->transport_offset_valid =
+ 			skb_transport_header_was_set(skb);
+ 		__entry->transport_offset = skb_transport_offset(skb);
+ 		__entry->tx_flags = skb_shinfo(skb)->tx_flags;
+ 		__entry->gso_size = skb_shinfo(skb)->gso_size;
+ 		__entry->gso_segs = skb_shinfo(skb)->gso_segs;
+ 		__entry->gso_type = skb_shinfo(skb)->gso_type;
+ 	),
+ 
+ 	TP_printk("dev=%s queue_mapping=%u skbaddr=%p vlan_tagged=%d vlan_proto=0x%04x vlan_tci=0x%04x protocol=0x%04x ip_summed=%d len=%u data_len=%u network_offset=%d transport_offset_valid=%d transport_offset=%d tx_flags=%d gso_size=%d gso_segs=%d gso_type=%#x",
+ 		  __get_str(name), __entry->queue_mapping, __entry->skbaddr,
+ 		  __entry->vlan_tagged, __entry->vlan_proto, __entry->vlan_tci,
+ 		  __entry->protocol, __entry->ip_summed, __entry->len,
+ 		  __entry->data_len,
+ 		  __entry->network_offset, __entry->transport_offset_valid,
+ 		  __entry->transport_offset, __entry->tx_flags,
+ 		  __entry->gso_size, __entry->gso_segs, __entry->gso_type)
+ );
+ 
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  TRACE_EVENT(net_dev_xmit,
  
  	TP_PROTO(struct sk_buff *skb,
@@@ -78,6 -136,106 +138,109 @@@ DEFINE_EVENT(net_dev_template, netif_rx
  
  	TP_ARGS(skb)
  );
++<<<<<<< HEAD
++=======
+ 
+ DECLARE_EVENT_CLASS(net_dev_rx_verbose_template,
+ 
+ 	TP_PROTO(const struct sk_buff *skb),
+ 
+ 	TP_ARGS(skb),
+ 
+ 	TP_STRUCT__entry(
+ 		__string(	name,			skb->dev->name	)
+ 		__field(	unsigned int,		napi_id		)
+ 		__field(	u16,			queue_mapping	)
+ 		__field(	const void *,		skbaddr		)
+ 		__field(	bool,			vlan_tagged	)
+ 		__field(	u16,			vlan_proto	)
+ 		__field(	u16,			vlan_tci	)
+ 		__field(	u16,			protocol	)
+ 		__field(	u8,			ip_summed	)
+ 		__field(	u32,			hash		)
+ 		__field(	bool,			l4_hash		)
+ 		__field(	unsigned int,		len		)
+ 		__field(	unsigned int,		data_len	)
+ 		__field(	unsigned int,		truesize	)
+ 		__field(	bool,			mac_header_valid)
+ 		__field(	int,			mac_header	)
+ 		__field(	unsigned char,		nr_frags	)
+ 		__field(	u16,			gso_size	)
+ 		__field(	u16,			gso_type	)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__assign_str(name, skb->dev->name);
+ #ifdef CONFIG_NET_RX_BUSY_POLL
+ 		__entry->napi_id = skb->napi_id;
+ #else
+ 		__entry->napi_id = 0;
+ #endif
+ 		__entry->queue_mapping = skb->queue_mapping;
+ 		__entry->skbaddr = skb;
+ 		__entry->vlan_tagged = skb_vlan_tag_present(skb);
+ 		__entry->vlan_proto = ntohs(skb->vlan_proto);
+ 		__entry->vlan_tci = skb_vlan_tag_get(skb);
+ 		__entry->protocol = ntohs(skb->protocol);
+ 		__entry->ip_summed = skb->ip_summed;
+ 		__entry->hash = skb->hash;
+ 		__entry->l4_hash = skb->l4_hash;
+ 		__entry->len = skb->len;
+ 		__entry->data_len = skb->data_len;
+ 		__entry->truesize = skb->truesize;
+ 		__entry->mac_header_valid = skb_mac_header_was_set(skb);
+ 		__entry->mac_header = skb_mac_header(skb) - skb->data;
+ 		__entry->nr_frags = skb_shinfo(skb)->nr_frags;
+ 		__entry->gso_size = skb_shinfo(skb)->gso_size;
+ 		__entry->gso_type = skb_shinfo(skb)->gso_type;
+ 	),
+ 
+ 	TP_printk("dev=%s napi_id=%#x queue_mapping=%u skbaddr=%p vlan_tagged=%d vlan_proto=0x%04x vlan_tci=0x%04x protocol=0x%04x ip_summed=%d hash=0x%08x l4_hash=%d len=%u data_len=%u truesize=%u mac_header_valid=%d mac_header=%d nr_frags=%d gso_size=%d gso_type=%#x",
+ 		  __get_str(name), __entry->napi_id, __entry->queue_mapping,
+ 		  __entry->skbaddr, __entry->vlan_tagged, __entry->vlan_proto,
+ 		  __entry->vlan_tci, __entry->protocol, __entry->ip_summed,
+ 		  __entry->hash, __entry->l4_hash, __entry->len,
+ 		  __entry->data_len, __entry->truesize,
+ 		  __entry->mac_header_valid, __entry->mac_header,
+ 		  __entry->nr_frags, __entry->gso_size, __entry->gso_type)
+ );
+ 
+ DEFINE_EVENT(net_dev_rx_verbose_template, napi_gro_frags_entry,
+ 
+ 	TP_PROTO(const struct sk_buff *skb),
+ 
+ 	TP_ARGS(skb)
+ );
+ 
+ DEFINE_EVENT(net_dev_rx_verbose_template, napi_gro_receive_entry,
+ 
+ 	TP_PROTO(const struct sk_buff *skb),
+ 
+ 	TP_ARGS(skb)
+ );
+ 
+ DEFINE_EVENT(net_dev_rx_verbose_template, netif_receive_skb_entry,
+ 
+ 	TP_PROTO(const struct sk_buff *skb),
+ 
+ 	TP_ARGS(skb)
+ );
+ 
+ DEFINE_EVENT(net_dev_rx_verbose_template, netif_rx_entry,
+ 
+ 	TP_PROTO(const struct sk_buff *skb),
+ 
+ 	TP_ARGS(skb)
+ );
+ 
+ DEFINE_EVENT(net_dev_rx_verbose_template, netif_rx_ni_entry,
+ 
+ 	TP_PROTO(const struct sk_buff *skb),
+ 
+ 	TP_ARGS(skb)
+ );
+ 
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  #endif /* _TRACE_NET_H */
  
  /* This part must be outside protection */
diff --cc net/bridge/br_netfilter.c
index 878f008afefa,65728e0dc4ff..000000000000
--- a/net/bridge/br_netfilter.c
+++ b/net/bridge/br_netfilter.c
@@@ -532,11 -436,11 +532,16 @@@ static struct net_device *brnf_get_logi
  	struct net_device *vlan, *br;
  
  	br = bridge_parent(dev);
- 	if (brnf_pass_vlan_indev == 0 || !vlan_tx_tag_present(skb))
+ 	if (brnf_pass_vlan_indev == 0 || !skb_vlan_tag_present(skb))
  		return br;
  
++<<<<<<< HEAD
 +	vlan = __vlan_find_dev_deep(br, skb->vlan_proto,
 +				    vlan_tx_tag_get(skb) & VLAN_VID_MASK);
++=======
+ 	vlan = __vlan_find_dev_deep_rcu(br, skb->vlan_proto,
+ 				    skb_vlan_tag_get(skb) & VLAN_VID_MASK);
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  
  	return vlan ? vlan : br;
  }
diff --cc net/bridge/br_vlan.c
index 7754c6b86b7f,13013fe8db24..000000000000
--- a/net/bridge/br_vlan.c
+++ b/net/bridge/br_vlan.c
@@@ -185,9 -187,9 +185,9 @@@ bool br_allowed_ingress(struct net_brid
  	 * sent from vlan device on the bridge device, it does not have
  	 * HW accelerated vlan tag.
  	 */
- 	if (unlikely(!vlan_tx_tag_present(skb) &&
+ 	if (unlikely(!skb_vlan_tag_present(skb) &&
  		     skb->protocol == proto)) {
 -		skb = skb_vlan_untag(skb);
 +		skb = vlan_untag(skb);
  		if (unlikely(!skb))
  			return false;
  	}
@@@ -197,8 -199,8 +197,13 @@@
  		if (skb->vlan_proto != proto) {
  			/* Protocol-mismatch, empty out vlan_tci for new tag */
  			skb_push(skb, ETH_HLEN);
++<<<<<<< HEAD
 +			skb = __vlan_put_tag(skb, skb->vlan_proto,
 +					     vlan_tx_tag_get(skb));
++=======
+ 			skb = vlan_insert_tag_set_proto(skb, skb->vlan_proto,
+ 							skb_vlan_tag_get(skb));
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  			if (unlikely(!skb))
  				return false;
  
diff --cc net/core/dev.c
index 9a2bea4eeeee,1e325adc4367..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -2460,13 -2563,22 +2460,24 @@@ static netdev_features_t harmonize_feat
  
  netdev_features_t netif_skb_features(struct sk_buff *skb)
  {
 -	struct net_device *dev = skb->dev;
 -	netdev_features_t features = dev->features;
 -	u16 gso_segs = skb_shinfo(skb)->gso_segs;
  	__be16 protocol = skb->protocol;
 +	netdev_features_t features = skb->dev->features;
  
 -	if (gso_segs > dev->gso_max_segs || gso_segs < dev->gso_min_segs)
 +	if (skb_shinfo(skb)->gso_segs > skb->dev->gso_max_segs)
  		features &= ~NETIF_F_GSO_MASK;
  
++<<<<<<< HEAD
 +	if (!vlan_tx_tag_present(skb)) {
++=======
+ 	/* If encapsulation offload request, verify we are testing
+ 	 * hardware encapsulation features instead of standard
+ 	 * features for the netdev
+ 	 */
+ 	if (skb->encapsulation)
+ 		features &= dev->hw_enc_features;
+ 
+ 	if (!skb_vlan_tag_present(skb)) {
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  		if (unlikely(protocol == htons(ETH_P_8021Q) ||
  			     protocol == htons(ETH_P_8021AD))) {
  			struct vlan_ethhdr *veh = (struct vlan_ethhdr *)skb->data;
@@@ -2494,115 -2611,137 +2505,176 @@@
  }
  EXPORT_SYMBOL(netif_skb_features);
  
 -static int xmit_one(struct sk_buff *skb, struct net_device *dev,
 -		    struct netdev_queue *txq, bool more)
 -{
 -	unsigned int len;
 -	int rc;
 -
 -	if (!list_empty(&ptype_all))
 -		dev_queue_xmit_nit(skb, dev);
 -
 -	len = skb->len;
 -	trace_net_dev_start_xmit(skb, dev);
 -	rc = netdev_start_xmit(skb, dev, txq, more);
 -	trace_net_dev_xmit(skb, rc, dev, len);
 -
 -	return rc;
 -}
 -
 -struct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,
 -				    struct netdev_queue *txq, int *ret)
 +int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
 +			struct netdev_queue *txq)
  {
 -	struct sk_buff *skb = first;
 +	const struct net_device_ops *ops = dev->netdev_ops;
  	int rc = NETDEV_TX_OK;
 +	unsigned int skb_len;
  
 -	while (skb) {
 -		struct sk_buff *next = skb->next;
 +	if (likely(!skb->next)) {
 +		netdev_features_t features;
  
++<<<<<<< HEAD
 +		/*
 +		 * If device doesn't need skb->dst, release it right now while
 +		 * its hot in this cpu cache
++=======
+ 		skb->next = NULL;
+ 		rc = xmit_one(skb, dev, txq, next != NULL);
+ 		if (unlikely(!dev_xmit_complete(rc))) {
+ 			skb->next = next;
+ 			goto out;
+ 		}
+ 
+ 		skb = next;
+ 		if (netif_xmit_stopped(txq) && skb) {
+ 			rc = NETDEV_TX_BUSY;
+ 			break;
+ 		}
+ 	}
+ 
+ out:
+ 	*ret = rc;
+ 	return skb;
+ }
+ 
+ static struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,
+ 					  netdev_features_t features)
+ {
+ 	if (skb_vlan_tag_present(skb) &&
+ 	    !vlan_hw_offload_capable(features, skb->vlan_proto))
+ 		skb = __vlan_hwaccel_push_inside(skb);
+ 	return skb;
+ }
+ 
+ static struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	netdev_features_t features;
+ 
+ 	if (skb->next)
+ 		return skb;
+ 
+ 	features = netif_skb_features(skb);
+ 	skb = validate_xmit_vlan(skb, features);
+ 	if (unlikely(!skb))
+ 		goto out_null;
+ 
+ 	if (netif_needs_gso(dev, skb, features)) {
+ 		struct sk_buff *segs;
+ 
+ 		segs = skb_gso_segment(skb, features);
+ 		if (IS_ERR(segs)) {
+ 			goto out_kfree_skb;
+ 		} else if (segs) {
+ 			consume_skb(skb);
+ 			skb = segs;
+ 		}
+ 	} else {
+ 		if (skb_needs_linearize(skb, features) &&
+ 		    __skb_linearize(skb))
+ 			goto out_kfree_skb;
+ 
+ 		/* If packet is not checksummed and device does not
+ 		 * support checksumming for this protocol, complete
+ 		 * checksumming here.
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  		 */
 -		if (skb->ip_summed == CHECKSUM_PARTIAL) {
 -			if (skb->encapsulation)
 -				skb_set_inner_transport_header(skb,
 -							       skb_checksum_start_offset(skb));
 -			else
 -				skb_set_transport_header(skb,
 -							 skb_checksum_start_offset(skb));
 -			if (!(features & NETIF_F_ALL_CSUM) &&
 -			    skb_checksum_help(skb))
 -				goto out_kfree_skb;
 +		if (dev->priv_flags & IFF_XMIT_DST_RELEASE)
 +			skb_dst_drop(skb);
 +
 +		features = netif_skb_features(skb);
 +
 +		if (vlan_tx_tag_present(skb) &&
 +		    !vlan_hw_offload_capable(features, skb->vlan_proto)) {
 +			skb = __vlan_put_tag(skb, skb->vlan_proto,
 +					     vlan_tx_tag_get(skb));
 +			if (unlikely(!skb))
 +				goto out;
 +
 +			skb->vlan_tci = 0;
  		}
 -	}
  
 -	return skb;
 +		/* If encapsulation offload request, verify we are testing
 +		 * hardware encapsulation features instead of standard
 +		 * features for the netdev
 +		 */
 +		if (skb->encapsulation)
 +			features &= dev->hw_enc_features;
  
 -out_kfree_skb:
 -	kfree_skb(skb);
 -out_null:
 -	return NULL;
 -}
 +		if (netif_needs_gso(skb, features)) {
 +			if (unlikely(dev_gso_segment(skb, features)))
 +				goto out_kfree_skb;
 +			if (skb->next)
 +				goto gso;
 +		} else {
 +			if (skb_needs_linearize(skb, features) &&
 +			    __skb_linearize(skb))
 +				goto out_kfree_skb;
  
 -struct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev)
 -{
 -	struct sk_buff *next, *head = NULL, *tail;
 +			/* If packet is not checksummed and device does not
 +			 * support checksumming for this protocol, complete
 +			 * checksumming here.
 +			 */
 +			if (skb->ip_summed == CHECKSUM_PARTIAL) {
 +				if (skb->encapsulation)
 +					skb_set_inner_transport_header(skb,
 +						skb_checksum_start_offset(skb));
 +				else
 +					skb_set_transport_header(skb,
 +						skb_checksum_start_offset(skb));
 +				if (!(features & NETIF_F_ALL_CSUM) &&
 +				     skb_checksum_help(skb))
 +					goto out_kfree_skb;
 +			}
 +		}
  
 -	for (; skb != NULL; skb = next) {
 -		next = skb->next;
 -		skb->next = NULL;
 +		if (!list_empty(&ptype_all))
 +			dev_queue_xmit_nit(skb, dev);
  
 -		/* in case skb wont be segmented, point to itself */
 -		skb->prev = skb;
 +		skb_len = skb->len;
 +		rc = ops->ndo_start_xmit(skb, dev);
 +		trace_net_dev_xmit(skb, rc, dev, skb_len);
 +		if (rc == NETDEV_TX_OK)
 +			txq_trans_update(txq);
 +		return rc;
 +	}
  
 -		skb = validate_xmit_skb(skb, dev);
 -		if (!skb)
 -			continue;
 +gso:
 +	do {
 +		struct sk_buff *nskb = skb->next;
  
 -		if (!head)
 -			head = skb;
 -		else
 -			tail->next = skb;
 -		/* If skb was segmented, skb->prev points to
 -		 * the last segment. If not, it still contains skb.
 -		 */
 -		tail = skb->prev;
 +		skb->next = nskb->next;
 +		nskb->next = NULL;
 +
 +		if (!list_empty(&ptype_all))
 +			dev_queue_xmit_nit(nskb, dev);
 +
 +		skb_len = nskb->len;
 +		rc = ops->ndo_start_xmit(nskb, dev);
 +		trace_net_dev_xmit(nskb, rc, dev, skb_len);
 +		if (unlikely(rc != NETDEV_TX_OK)) {
 +			if (rc & ~NETDEV_TX_MASK)
 +				goto out_kfree_gso_skb;
 +			nskb->next = skb->next;
 +			skb->next = nskb;
 +			return rc;
 +		}
 +		txq_trans_update(txq);
 +		if (unlikely(netif_xmit_stopped(txq) && skb->next))
 +			return NETDEV_TX_BUSY;
 +	} while (skb->next);
 +
 +out_kfree_gso_skb:
 +	if (likely(skb->next == NULL)) {
 +		skb->destructor = DEV_GSO_CB(skb)->destructor;
 +		consume_skb(skb);
 +		return rc;
  	}
 -	return head;
 +out_kfree_skb:
 +	kfree_skb(skb);
 +out:
 +	return rc;
  }
  
  static void qdisc_pkt_len_init(struct sk_buff *skb)
diff --cc net/core/skbuff.c
index 13e8ed896c2a,56db472e9b86..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -3718,3 -4179,240 +3718,243 @@@ unsigned int skb_gso_transport_seglen(c
  	return thlen + shinfo->gso_size;
  }
  EXPORT_SYMBOL_GPL(skb_gso_transport_seglen);
++<<<<<<< HEAD
++=======
+ 
+ static struct sk_buff *skb_reorder_vlan_header(struct sk_buff *skb)
+ {
+ 	if (skb_cow(skb, skb_headroom(skb)) < 0) {
+ 		kfree_skb(skb);
+ 		return NULL;
+ 	}
+ 
+ 	memmove(skb->data - ETH_HLEN, skb->data - VLAN_ETH_HLEN, 2 * ETH_ALEN);
+ 	skb->mac_header += VLAN_HLEN;
+ 	return skb;
+ }
+ 
+ struct sk_buff *skb_vlan_untag(struct sk_buff *skb)
+ {
+ 	struct vlan_hdr *vhdr;
+ 	u16 vlan_tci;
+ 
+ 	if (unlikely(skb_vlan_tag_present(skb))) {
+ 		/* vlan_tci is already set-up so leave this for another time */
+ 		return skb;
+ 	}
+ 
+ 	skb = skb_share_check(skb, GFP_ATOMIC);
+ 	if (unlikely(!skb))
+ 		goto err_free;
+ 
+ 	if (unlikely(!pskb_may_pull(skb, VLAN_HLEN)))
+ 		goto err_free;
+ 
+ 	vhdr = (struct vlan_hdr *)skb->data;
+ 	vlan_tci = ntohs(vhdr->h_vlan_TCI);
+ 	__vlan_hwaccel_put_tag(skb, skb->protocol, vlan_tci);
+ 
+ 	skb_pull_rcsum(skb, VLAN_HLEN);
+ 	vlan_set_encap_proto(skb, vhdr);
+ 
+ 	skb = skb_reorder_vlan_header(skb);
+ 	if (unlikely(!skb))
+ 		goto err_free;
+ 
+ 	skb_reset_network_header(skb);
+ 	skb_reset_transport_header(skb);
+ 	skb_reset_mac_len(skb);
+ 
+ 	return skb;
+ 
+ err_free:
+ 	kfree_skb(skb);
+ 	return NULL;
+ }
+ EXPORT_SYMBOL(skb_vlan_untag);
+ 
+ int skb_ensure_writable(struct sk_buff *skb, int write_len)
+ {
+ 	if (!pskb_may_pull(skb, write_len))
+ 		return -ENOMEM;
+ 
+ 	if (!skb_cloned(skb) || skb_clone_writable(skb, write_len))
+ 		return 0;
+ 
+ 	return pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+ }
+ EXPORT_SYMBOL(skb_ensure_writable);
+ 
+ /* remove VLAN header from packet and update csum accordingly. */
+ static int __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci)
+ {
+ 	struct vlan_hdr *vhdr;
+ 	unsigned int offset = skb->data - skb_mac_header(skb);
+ 	int err;
+ 
+ 	__skb_push(skb, offset);
+ 	err = skb_ensure_writable(skb, VLAN_ETH_HLEN);
+ 	if (unlikely(err))
+ 		goto pull;
+ 
+ 	skb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);
+ 
+ 	vhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);
+ 	*vlan_tci = ntohs(vhdr->h_vlan_TCI);
+ 
+ 	memmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);
+ 	__skb_pull(skb, VLAN_HLEN);
+ 
+ 	vlan_set_encap_proto(skb, vhdr);
+ 	skb->mac_header += VLAN_HLEN;
+ 
+ 	if (skb_network_offset(skb) < ETH_HLEN)
+ 		skb_set_network_header(skb, ETH_HLEN);
+ 
+ 	skb_reset_mac_len(skb);
+ pull:
+ 	__skb_pull(skb, offset);
+ 
+ 	return err;
+ }
+ 
+ int skb_vlan_pop(struct sk_buff *skb)
+ {
+ 	u16 vlan_tci;
+ 	__be16 vlan_proto;
+ 	int err;
+ 
+ 	if (likely(skb_vlan_tag_present(skb))) {
+ 		skb->vlan_tci = 0;
+ 	} else {
+ 		if (unlikely((skb->protocol != htons(ETH_P_8021Q) &&
+ 			      skb->protocol != htons(ETH_P_8021AD)) ||
+ 			     skb->len < VLAN_ETH_HLEN))
+ 			return 0;
+ 
+ 		err = __skb_vlan_pop(skb, &vlan_tci);
+ 		if (err)
+ 			return err;
+ 	}
+ 	/* move next vlan tag to hw accel tag */
+ 	if (likely((skb->protocol != htons(ETH_P_8021Q) &&
+ 		    skb->protocol != htons(ETH_P_8021AD)) ||
+ 		   skb->len < VLAN_ETH_HLEN))
+ 		return 0;
+ 
+ 	vlan_proto = skb->protocol;
+ 	err = __skb_vlan_pop(skb, &vlan_tci);
+ 	if (unlikely(err))
+ 		return err;
+ 
+ 	__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(skb_vlan_pop);
+ 
+ int skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci)
+ {
+ 	if (skb_vlan_tag_present(skb)) {
+ 		unsigned int offset = skb->data - skb_mac_header(skb);
+ 		int err;
+ 
+ 		/* __vlan_insert_tag expect skb->data pointing to mac header.
+ 		 * So change skb->data before calling it and change back to
+ 		 * original position later
+ 		 */
+ 		__skb_push(skb, offset);
+ 		err = __vlan_insert_tag(skb, skb->vlan_proto,
+ 					skb_vlan_tag_get(skb));
+ 		if (err)
+ 			return err;
+ 		skb->protocol = skb->vlan_proto;
+ 		skb->mac_len += VLAN_HLEN;
+ 		__skb_pull(skb, offset);
+ 
+ 		if (skb->ip_summed == CHECKSUM_COMPLETE)
+ 			skb->csum = csum_add(skb->csum, csum_partial(skb->data
+ 					+ (2 * ETH_ALEN), VLAN_HLEN, 0));
+ 	}
+ 	__vlan_hwaccel_put_tag(skb, vlan_proto, vlan_tci);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(skb_vlan_push);
+ 
+ /**
+  * alloc_skb_with_frags - allocate skb with page frags
+  *
+  * @header_len: size of linear part
+  * @data_len: needed length in frags
+  * @max_page_order: max page order desired.
+  * @errcode: pointer to error code if any
+  * @gfp_mask: allocation mask
+  *
+  * This can be used to allocate a paged skb, given a maximal order for frags.
+  */
+ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,
+ 				     unsigned long data_len,
+ 				     int max_page_order,
+ 				     int *errcode,
+ 				     gfp_t gfp_mask)
+ {
+ 	int npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+ 	unsigned long chunk;
+ 	struct sk_buff *skb;
+ 	struct page *page;
+ 	gfp_t gfp_head;
+ 	int i;
+ 
+ 	*errcode = -EMSGSIZE;
+ 	/* Note this test could be relaxed, if we succeed to allocate
+ 	 * high order pages...
+ 	 */
+ 	if (npages > MAX_SKB_FRAGS)
+ 		return NULL;
+ 
+ 	gfp_head = gfp_mask;
+ 	if (gfp_head & __GFP_WAIT)
+ 		gfp_head |= __GFP_REPEAT;
+ 
+ 	*errcode = -ENOBUFS;
+ 	skb = alloc_skb(header_len, gfp_head);
+ 	if (!skb)
+ 		return NULL;
+ 
+ 	skb->truesize += npages << PAGE_SHIFT;
+ 
+ 	for (i = 0; npages > 0; i++) {
+ 		int order = max_page_order;
+ 
+ 		while (order) {
+ 			if (npages >= 1 << order) {
+ 				page = alloc_pages(gfp_mask |
+ 						   __GFP_COMP |
+ 						   __GFP_NOWARN |
+ 						   __GFP_NORETRY,
+ 						   order);
+ 				if (page)
+ 					goto fill_page;
+ 				/* Do not retry other high order allocations */
+ 				order = 1;
+ 				max_page_order = 0;
+ 			}
+ 			order--;
+ 		}
+ 		page = alloc_page(gfp_mask);
+ 		if (!page)
+ 			goto failure;
+ fill_page:
+ 		chunk = min_t(unsigned long, data_len,
+ 			      PAGE_SIZE << order);
+ 		skb_fill_page_desc(skb, i, page, 0, chunk);
+ 		data_len -= chunk;
+ 		npages -= 1 << order;
+ 	}
+ 	return skb;
+ 
+ failure:
+ 	kfree_skb(skb);
+ 	return NULL;
+ }
+ EXPORT_SYMBOL(alloc_skb_with_frags);
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
diff --cc net/openvswitch/actions.c
index e6e6d775efa0,b4cffe686126..000000000000
--- a/net/openvswitch/actions.c
+++ b/net/openvswitch/actions.c
@@@ -176,27 -207,30 +176,50 @@@ static int pop_vlan(struct sk_buff *skb
  	return 0;
  }
  
 -static int pop_vlan(struct sk_buff *skb, struct sw_flow_key *key)
 +static int push_vlan(struct sk_buff *skb, const struct ovs_action_push_vlan *vlan)
  {
 -	int err;
 +	if (unlikely(vlan_tx_tag_present(skb))) {
 +		u16 current_tag;
 +
++<<<<<<< HEAD
 +		/* push down current VLAN tag */
 +		current_tag = vlan_tx_tag_get(skb);
 +
 +		if (!__vlan_put_tag(skb, skb->vlan_proto, current_tag))
 +			return -ENOMEM;
  
 +		if (skb->ip_summed == CHECKSUM_COMPLETE)
 +			skb->csum = csum_add(skb->csum, csum_partial(skb->data
 +					+ (2 * ETH_ALEN), VLAN_HLEN, 0));
 +
 +	}
 +	__vlan_hwaccel_put_tag(skb, vlan->vlan_tpid, ntohs(vlan->vlan_tci) & ~VLAN_TAG_PRESENT);
 +	return 0;
 +}
 +
 +static int set_eth_addr(struct sk_buff *skb,
++=======
+ 	err = skb_vlan_pop(skb);
+ 	if (skb_vlan_tag_present(skb))
+ 		invalidate_flow_key(key);
+ 	else
+ 		key->eth.tci = 0;
+ 	return err;
+ }
+ 
+ static int push_vlan(struct sk_buff *skb, struct sw_flow_key *key,
+ 		     const struct ovs_action_push_vlan *vlan)
+ {
+ 	if (skb_vlan_tag_present(skb))
+ 		invalidate_flow_key(key);
+ 	else
+ 		key->eth.tci = vlan->vlan_tci;
+ 	return skb_vlan_push(skb, vlan->vlan_tpid,
+ 			     ntohs(vlan->vlan_tci) & ~VLAN_TAG_PRESENT);
+ }
+ 
+ static int set_eth_addr(struct sk_buff *skb, struct sw_flow_key *key,
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  			const struct ovs_key_ethernet *eth_key)
  {
  	int err;
diff --cc net/openvswitch/flow.c
index 1e2f2e18925b,df334fe43d7f..000000000000
--- a/net/openvswitch/flow.c
+++ b/net/openvswitch/flow.c
@@@ -68,6 -70,7 +68,10 @@@ void ovs_flow_stats_update(struct sw_fl
  {
  	struct flow_stats *stats;
  	int node = numa_node_id();
++<<<<<<< HEAD
++=======
+ 	int len = skb->len + (skb_vlan_tag_present(skb) ? VLAN_HLEN : 0);
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  
  	stats = rcu_dereference(flow->stats[node]);
  
@@@ -462,7 -471,8 +466,12 @@@ static int key_extract(struct sk_buff *
  	 * update skb->csum here.
  	 */
  
++<<<<<<< HEAD
 +	if (vlan_tx_tag_present(skb))
++=======
+ 	key->eth.tci = 0;
+ 	if (skb_vlan_tag_present(skb))
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  		key->eth.tci = htons(skb->vlan_tci);
  	else if (eth->h_proto == htons(ETH_P_8021Q))
  		if (unlikely(parse_vlan(skb, key)))
diff --cc net/openvswitch/vport.c
index 3dc860858021,464739aac0f3..000000000000
--- a/net/openvswitch/vport.c
+++ b/net/openvswitch/vport.c
@@@ -441,7 -480,8 +441,12 @@@ void ovs_vport_receive(struct vport *vp
  	stats = this_cpu_ptr(vport->percpu_stats);
  	u64_stats_update_begin(&stats->syncp);
  	stats->rx_packets++;
++<<<<<<< HEAD
 +	stats->rx_bytes += skb->len;
++=======
+ 	stats->rx_bytes += skb->len +
+ 			   (skb_vlan_tag_present(skb) ? VLAN_HLEN : 0);
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  	u64_stats_update_end(&stats->syncp);
  
  	OVS_CB(skb)->input_vport = vport;
diff --cc net/packet/af_packet.c
index 674dd1d4ef8f,d37075b0d6d5..000000000000
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@@ -922,11 -986,13 +922,18 @@@ static void prb_clear_rxhash(struct tpa
  static void prb_fill_vlan_info(struct tpacket_kbdq_core *pkc,
  			struct tpacket3_hdr *ppd)
  {
++<<<<<<< HEAD
 +	if (vlan_tx_tag_present(pkc->skb)) {
 +		ppd->hv1.tp_vlan_tci = vlan_tx_tag_get(pkc->skb);
 +		ppd->tp_status = TP_STATUS_VLAN_VALID;
++=======
+ 	if (skb_vlan_tag_present(pkc->skb)) {
+ 		ppd->hv1.tp_vlan_tci = skb_vlan_tag_get(pkc->skb);
+ 		ppd->hv1.tp_vlan_tpid = ntohs(pkc->skb->vlan_proto);
+ 		ppd->tp_status = TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  	} else {
  		ppd->hv1.tp_vlan_tci = 0;
 -		ppd->hv1.tp_vlan_tpid = 0;
  		ppd->tp_status = TP_STATUS_AVAILABLE;
  	}
  }
@@@ -1916,13 -2000,15 +1923,20 @@@ static int tpacket_rcv(struct sk_buff *
  		h.h2->tp_net = netoff;
  		h.h2->tp_sec = ts.tv_sec;
  		h.h2->tp_nsec = ts.tv_nsec;
++<<<<<<< HEAD
 +		if (vlan_tx_tag_present(skb)) {
 +			h.h2->tp_vlan_tci = vlan_tx_tag_get(skb);
 +			status |= TP_STATUS_VLAN_VALID;
++=======
+ 		if (skb_vlan_tag_present(skb)) {
+ 			h.h2->tp_vlan_tci = skb_vlan_tag_get(skb);
+ 			h.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);
+ 			status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  		} else {
  			h.h2->tp_vlan_tci = 0;
 -			h.h2->tp_vlan_tpid = 0;
  		}
 -		memset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));
 +		h.h2->tp_padding = 0;
  		hdrlen = sizeof(*h.h2);
  		break;
  	case TPACKET_V3:
@@@ -2940,13 -3010,14 +2954,20 @@@ static int packet_recvmsg(struct kiocb 
  		aux.tp_snaplen = skb->len;
  		aux.tp_mac = 0;
  		aux.tp_net = skb_network_offset(skb);
++<<<<<<< HEAD
 +		if (vlan_tx_tag_present(skb)) {
 +			aux.tp_vlan_tci = vlan_tx_tag_get(skb);
 +			aux.tp_status |= TP_STATUS_VLAN_VALID;
++=======
+ 		if (skb_vlan_tag_present(skb)) {
+ 			aux.tp_vlan_tci = skb_vlan_tag_get(skb);
+ 			aux.tp_vlan_tpid = ntohs(skb->vlan_proto);
+ 			aux.tp_status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;
++>>>>>>> df8a39defad4 (net: rename vlan_tx_* helpers since "tx" is misleading there)
  		} else {
  			aux.tp_vlan_tci = 0;
 -			aux.tp_vlan_tpid = 0;
  		}
 +		aux.tp_padding = 0;
  		put_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);
  	}
  
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-drv.c
* Unmerged path drivers/net/ethernet/intel/fm10k/fm10k_main.c
* Unmerged path drivers/net/ethernet/intel/fm10k/fm10k_netdev.c
* Unmerged path drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
* Unmerged path net/ipv4/geneve.c
* Unmerged path Documentation/networking/filter.txt
diff --git a/drivers/infiniband/hw/nes/nes_nic.c b/drivers/infiniband/hw/nes/nes_nic.c
index 49eb5111d2cd..70acda91eb2a 100644
--- a/drivers/infiniband/hw/nes/nes_nic.c
+++ b/drivers/infiniband/hw/nes/nes_nic.c
@@ -373,11 +373,11 @@ static int nes_nic_send(struct sk_buff *skb, struct net_device *netdev)
 	wqe_fragment_length = (__le16 *)&nic_sqe->wqe_words[NES_NIC_SQ_WQE_LENGTH_0_TAG_IDX];
 
 	/* setup the VLAN tag if present */
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		nes_debug(NES_DBG_NIC_TX, "%s: VLAN packet to send... VLAN = %08X\n",
-				netdev->name, vlan_tx_tag_get(skb));
+				netdev->name, skb_vlan_tag_get(skb));
 		wqe_misc = NES_NIC_SQ_WQE_TAGVALUE_ENABLE;
-		wqe_fragment_length[0] = (__force __le16) vlan_tx_tag_get(skb);
+		wqe_fragment_length[0] = (__force __le16) skb_vlan_tag_get(skb);
 	} else
 		wqe_misc = 0;
 
@@ -576,11 +576,12 @@ tso_sq_no_longer_full:
 				wqe_fragment_length =
 						(__le16 *)&nic_sqe->wqe_words[NES_NIC_SQ_WQE_LENGTH_0_TAG_IDX];
 				/* setup the VLAN tag if present */
-				if (vlan_tx_tag_present(skb)) {
+				if (skb_vlan_tag_present(skb)) {
 					nes_debug(NES_DBG_NIC_TX, "%s: VLAN packet to send... VLAN = %08X\n",
-							netdev->name, vlan_tx_tag_get(skb) );
+							netdev->name,
+						  skb_vlan_tag_get(skb));
 					wqe_misc = NES_NIC_SQ_WQE_TAGVALUE_ENABLE;
-					wqe_fragment_length[0] = (__force __le16) vlan_tx_tag_get(skb);
+					wqe_fragment_length[0] = (__force __le16) skb_vlan_tag_get(skb);
 				} else
 					wqe_misc = 0;
 
diff --git a/drivers/net/ethernet/3com/typhoon.c b/drivers/net/ethernet/3com/typhoon.c
index 144942f6372b..64f6de66fc8f 100644
--- a/drivers/net/ethernet/3com/typhoon.c
+++ b/drivers/net/ethernet/3com/typhoon.c
@@ -769,11 +769,11 @@ typhoon_start_tx(struct sk_buff *skb, struct net_device *dev)
 		first_txd->processFlags |= TYPHOON_TX_PF_IP_CHKSUM;
 	}
 
-	if(vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		first_txd->processFlags |=
 		    TYPHOON_TX_PF_INSERT_VLAN | TYPHOON_TX_PF_VLAN_PRIORITY;
 		first_txd->processFlags |=
-		    cpu_to_le32(htons(vlan_tx_tag_get(skb)) <<
+		    cpu_to_le32(htons(skb_vlan_tag_get(skb)) <<
 				TYPHOON_TX_PF_VLAN_TAG_SHIFT);
 	}
 
diff --git a/drivers/net/ethernet/alteon/acenic.c b/drivers/net/ethernet/alteon/acenic.c
index b7894f8af9d1..57548ae6e2e0 100644
--- a/drivers/net/ethernet/alteon/acenic.c
+++ b/drivers/net/ethernet/alteon/acenic.c
@@ -2443,9 +2443,9 @@ restart:
 		flagsize = (skb->len << 16) | (BD_FLG_END);
 		if (skb->ip_summed == CHECKSUM_PARTIAL)
 			flagsize |= BD_FLG_TCP_UDP_SUM;
-		if (vlan_tx_tag_present(skb)) {
+		if (skb_vlan_tag_present(skb)) {
 			flagsize |= BD_FLG_VLAN_TAG;
-			vlan_tag = vlan_tx_tag_get(skb);
+			vlan_tag = skb_vlan_tag_get(skb);
 		}
 		desc = ap->tx_ring + idx;
 		idx = (idx + 1) % ACE_TX_RING_ENTRIES(ap);
@@ -2464,9 +2464,9 @@ restart:
 		flagsize = (skb_headlen(skb) << 16);
 		if (skb->ip_summed == CHECKSUM_PARTIAL)
 			flagsize |= BD_FLG_TCP_UDP_SUM;
-		if (vlan_tx_tag_present(skb)) {
+		if (skb_vlan_tag_present(skb)) {
 			flagsize |= BD_FLG_VLAN_TAG;
-			vlan_tag = vlan_tx_tag_get(skb);
+			vlan_tag = skb_vlan_tag_get(skb);
 		}
 
 		ace_load_tx_bd(ap, ap->tx_ring + idx, mapping, flagsize, vlan_tag);
diff --git a/drivers/net/ethernet/amd/amd8111e.c b/drivers/net/ethernet/amd/amd8111e.c
index 8e6b665a6726..0216623e905a 100644
--- a/drivers/net/ethernet/amd/amd8111e.c
+++ b/drivers/net/ethernet/amd/amd8111e.c
@@ -1308,11 +1308,11 @@ static netdev_tx_t amd8111e_start_xmit(struct sk_buff *skb,
 	lp->tx_ring[tx_index].tx_flags = 0;
 
 #if AMD8111E_VLAN_TAG_USED
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		lp->tx_ring[tx_index].tag_ctrl_cmd |=
 				cpu_to_le16(TCC_VLAN_INSERT);
 		lp->tx_ring[tx_index].tag_ctrl_info =
-				cpu_to_le16(vlan_tx_tag_get(skb));
+				cpu_to_le16(skb_vlan_tag_get(skb));
 
 	}
 #endif
* Unmerged path drivers/net/ethernet/amd/xgbe/xgbe-drv.c
diff --git a/drivers/net/ethernet/atheros/atl1c/atl1c_main.c b/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
index 11cdf1d43041..531b6644ad11 100644
--- a/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
+++ b/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
@@ -2228,8 +2228,8 @@ static netdev_tx_t atl1c_xmit_frame(struct sk_buff *skb,
 		return NETDEV_TX_OK;
 	}
 
-	if (unlikely(vlan_tx_tag_present(skb))) {
-		u16 vlan = vlan_tx_tag_get(skb);
+	if (unlikely(skb_vlan_tag_present(skb))) {
+		u16 vlan = skb_vlan_tag_get(skb);
 		__le16 tag;
 
 		vlan = cpu_to_le16(vlan);
diff --git a/drivers/net/ethernet/atheros/atl1e/atl1e_main.c b/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
index c23bb02e3ed0..741d2505090b 100644
--- a/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
+++ b/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
@@ -1850,8 +1850,8 @@ static netdev_tx_t atl1e_xmit_frame(struct sk_buff *skb,
 
 	tpd = atl1e_get_tpd(adapter);
 
-	if (vlan_tx_tag_present(skb)) {
-		u16 vlan_tag = vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb)) {
+		u16 vlan_tag = skb_vlan_tag_get(skb);
 		u16 atl1e_vlan_tag;
 
 		tpd->word3 |= 1 << TPD_INS_VL_TAG_SHIFT;
diff --git a/drivers/net/ethernet/atheros/atlx/atl1.c b/drivers/net/ethernet/atheros/atlx/atl1.c
index fa0915f3999b..b5f865c097c0 100644
--- a/drivers/net/ethernet/atheros/atlx/atl1.c
+++ b/drivers/net/ethernet/atheros/atlx/atl1.c
@@ -2410,8 +2410,8 @@ static netdev_tx_t atl1_xmit_frame(struct sk_buff *skb,
 		(u16) atomic_read(&tpd_ring->next_to_use));
 	memset(ptpd, 0, sizeof(struct tx_packet_desc));
 
-	if (vlan_tx_tag_present(skb)) {
-		vlan_tag = vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb)) {
+		vlan_tag = skb_vlan_tag_get(skb);
 		vlan_tag = (vlan_tag << 4) | (vlan_tag >> 13) |
 			((vlan_tag >> 9) & 0x8);
 		ptpd->word3 |= 1 << TPD_INS_VL_TAG_SHIFT;
diff --git a/drivers/net/ethernet/atheros/atlx/atl2.c b/drivers/net/ethernet/atheros/atlx/atl2.c
index 265ce1b752ed..ce9c70f44d7e 100644
--- a/drivers/net/ethernet/atheros/atlx/atl2.c
+++ b/drivers/net/ethernet/atheros/atlx/atl2.c
@@ -888,8 +888,8 @@ static netdev_tx_t atl2_xmit_frame(struct sk_buff *skb,
 		offset = ((u32)(skb->len-copy_len + 3) & ~3);
 	}
 #ifdef NETIF_F_HW_VLAN_CTAG_TX
-	if (vlan_tx_tag_present(skb)) {
-		u16 vlan_tag = vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb)) {
+		u16 vlan_tag = skb_vlan_tag_get(skb);
 		vlan_tag = (vlan_tag << 4) |
 			(vlan_tag >> 13) |
 			((vlan_tag >> 9) & 0x8);
diff --git a/drivers/net/ethernet/broadcom/bnx2.c b/drivers/net/ethernet/broadcom/bnx2.c
index 1435da8ea183..90f7e90c85cb 100644
--- a/drivers/net/ethernet/broadcom/bnx2.c
+++ b/drivers/net/ethernet/broadcom/bnx2.c
@@ -6596,9 +6596,9 @@ bnx2_start_xmit(struct sk_buff *skb, struct net_device *dev)
 		vlan_tag_flags |= TX_BD_FLAGS_TCP_UDP_CKSUM;
 	}
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		vlan_tag_flags |=
-			(TX_BD_FLAGS_VLAN_TAG | (vlan_tx_tag_get(skb) << 16));
+			(TX_BD_FLAGS_VLAN_TAG | (skb_vlan_tag_get(skb) << 16));
 	}
 
 	if ((mss = skb_shinfo(skb)->gso_size)) {
diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
index 66717c51ac4d..1751c6344f54 100644
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
@@ -3864,9 +3864,9 @@ netdev_tx_t bnx2x_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	   "sending pkt %u @%p  next_idx %u  bd %u @%p\n",
 	   pkt_prod, tx_buf, txdata->tx_pkt_prod, bd_prod, tx_start_bd);
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		tx_start_bd->vlan_or_ethertype =
-		    cpu_to_le16(vlan_tx_tag_get(skb));
+		    cpu_to_le16(skb_vlan_tag_get(skb));
 		tx_start_bd->bd_flags.as_bitfield |=
 		    (X_ETH_OUTBAND_VLAN << ETH_TX_BD_FLAGS_VLAN_MODE_SHIFT);
 	} else {
diff --git a/drivers/net/ethernet/broadcom/tg3.c b/drivers/net/ethernet/broadcom/tg3.c
index 56e85554cab4..e0bdcb9d0ed5 100644
--- a/drivers/net/ethernet/broadcom/tg3.c
+++ b/drivers/net/ethernet/broadcom/tg3.c
@@ -7984,9 +7984,9 @@ static netdev_tx_t tg3_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	    !mss && skb->len > VLAN_ETH_FRAME_LEN)
 		base_flags |= TXD_FLAG_JMB_PKT;
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		base_flags |= TXD_FLAG_VLAN;
-		vlan = vlan_tx_tag_get(skb);
+		vlan = skb_vlan_tag_get(skb);
 	}
 
 	if ((unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) &&
diff --git a/drivers/net/ethernet/brocade/bna/bnad.c b/drivers/net/ethernet/brocade/bna/bnad.c
index be74a43e89e3..eb4927494f7a 100644
--- a/drivers/net/ethernet/brocade/bna/bnad.c
+++ b/drivers/net/ethernet/brocade/bna/bnad.c
@@ -2824,8 +2824,8 @@ bnad_txq_wi_prepare(struct bnad *bnad, struct bna_tcb *tcb,
 	u32 gso_size;
 	u16 vlan_tag = 0;
 
-	if (vlan_tx_tag_present(skb)) {
-		vlan_tag = (u16)vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb)) {
+		vlan_tag = (u16)skb_vlan_tag_get(skb);
 		flags |= (BNA_TXQ_WI_CF_INS_PRIO | BNA_TXQ_WI_CF_INS_VLAN);
 	}
 	if (test_bit(BNAD_RF_CEE_RUNNING, &bnad->run_flags)) {
diff --git a/drivers/net/ethernet/chelsio/cxgb/sge.c b/drivers/net/ethernet/chelsio/cxgb/sge.c
index 8061fb0ef7ed..922df2ea94e7 100644
--- a/drivers/net/ethernet/chelsio/cxgb/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb/sge.c
@@ -1861,9 +1861,9 @@ netdev_tx_t t1_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	}
 	cpl->iff = dev->if_port;
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		cpl->vlan_valid = 1;
-		cpl->vlan = htons(vlan_tx_tag_get(skb));
+		cpl->vlan = htons(skb_vlan_tag_get(skb));
 		st->vlan_insert++;
 	} else
 		cpl->vlan_valid = 0;
diff --git a/drivers/net/ethernet/chelsio/cxgb3/sge.c b/drivers/net/ethernet/chelsio/cxgb3/sge.c
index 674deb0435ab..03ac0b050797 100644
--- a/drivers/net/ethernet/chelsio/cxgb3/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb3/sge.c
@@ -1148,8 +1148,8 @@ static void write_tx_pkt_wr(struct adapter *adap, struct sk_buff *skb,
 	cpl->len = htonl(skb->len);
 	cntrl = V_TXPKT_INTF(pi->port_id);
 
-	if (vlan_tx_tag_present(skb))
-		cntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(vlan_tx_tag_get(skb));
+	if (skb_vlan_tag_present(skb))
+		cntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(skb_vlan_tag_get(skb));
 
 	tso_info = V_LSO_MSS(skb_shinfo(skb)->gso_size);
 	if (tso_info) {
@@ -1282,7 +1282,7 @@ netdev_tx_t t3_eth_xmit(struct sk_buff *skb, struct net_device *dev)
 		qs->port_stats[SGE_PSTAT_TX_CSUM]++;
 	if (skb_shinfo(skb)->gso_size)
 		qs->port_stats[SGE_PSTAT_TSO]++;
-	if (vlan_tx_tag_present(skb))
+	if (skb_vlan_tag_present(skb))
 		qs->port_stats[SGE_PSTAT_VLANINS]++;
 
 	/*
diff --git a/drivers/net/ethernet/chelsio/cxgb4/sge.c b/drivers/net/ethernet/chelsio/cxgb4/sge.c
index b3f79e041f0b..49247cdba8b0 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/sge.c
@@ -1147,9 +1147,9 @@ out_free:	dev_kfree_skb_any(skb);
 			cntrl = TXPKT_L4CSUM_DIS | TXPKT_IPCSUM_DIS;
 	}
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		q->vlan_ins++;
-		cntrl |= TXPKT_VLAN_VLD | TXPKT_VLAN(vlan_tx_tag_get(skb));
+		cntrl |= TXPKT_VLAN_VLD | TXPKT_VLAN(skb_vlan_tag_get(skb));
 	}
 
 	cpl->ctrl0 = htonl(TXPKT_OPCODE(CPL_TX_PKT_XT) |
diff --git a/drivers/net/ethernet/chelsio/cxgb4vf/sge.c b/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
index 4e4e43e07043..8ae234ff8d22 100644
--- a/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb4vf/sge.c
@@ -1243,9 +1243,9 @@ int t4vf_eth_xmit(struct sk_buff *skb, struct net_device *dev)
 	 * If there's a VLAN tag present, add that to the list of things to
 	 * do in this Work Request.
 	 */
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		txq->vlan_ins++;
-		cntrl |= TXPKT_VLAN_VLD | TXPKT_VLAN(vlan_tx_tag_get(skb));
+		cntrl |= TXPKT_VLAN_VLD | TXPKT_VLAN(skb_vlan_tag_get(skb));
 	}
 
 	/*
diff --git a/drivers/net/ethernet/cisco/enic/enic_main.c b/drivers/net/ethernet/cisco/enic/enic_main.c
index 87ac728034ed..b9402c48afcb 100644
--- a/drivers/net/ethernet/cisco/enic/enic_main.c
+++ b/drivers/net/ethernet/cisco/enic/enic_main.c
@@ -503,10 +503,10 @@ static inline void enic_queue_wq_skb(struct enic *enic,
 	int vlan_tag_insert = 0;
 	int loopback = 0;
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		/* VLAN tag from trunking driver */
 		vlan_tag_insert = 1;
-		vlan_tag = vlan_tx_tag_get(skb);
+		vlan_tag = skb_vlan_tag_get(skb);
 	} else if (enic->loop_enable) {
 		vlan_tag = enic->loop_tag;
 		loopback = 1;
diff --git a/drivers/net/ethernet/emulex/benet/be_main.c b/drivers/net/ethernet/emulex/benet/be_main.c
index 504f5524604a..a42ba24538e1 100644
--- a/drivers/net/ethernet/emulex/benet/be_main.c
+++ b/drivers/net/ethernet/emulex/benet/be_main.c
@@ -713,7 +713,7 @@ static inline u16 be_get_tx_vlan_tag(struct be_adapter *adapter,
 	u8 vlan_prio;
 	u16 vlan_tag;
 
-	vlan_tag = vlan_tx_tag_get(skb);
+	vlan_tag = skb_vlan_tag_get(skb);
 	vlan_prio = (vlan_tag & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
 	/* If vlan priority provided by OS is NOT in available bmap */
 	if (!(adapter->vlan_prio_bmap & (1 << vlan_prio)))
@@ -764,7 +764,7 @@ static void wrb_fill_hdr(struct be_adapter *adapter, struct be_eth_hdr_wrb *hdr,
 			SET_TX_WRB_HDR_BITS(udpcs, hdr, 1);
 	}
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		SET_TX_WRB_HDR_BITS(vlan, hdr, 1);
 		vlan_tag = be_get_tx_vlan_tag(adapter, skb);
 		SET_TX_WRB_HDR_BITS(vlan_tag, hdr, vlan_tag);
@@ -873,7 +873,7 @@ static struct sk_buff *be_insert_vlan_in_pkt(struct be_adapter *adapter,
 	if (unlikely(!skb))
 		return skb;
 
-	if (vlan_tx_tag_present(skb))
+	if (skb_vlan_tag_present(skb))
 		vlan_tag = be_get_tx_vlan_tag(adapter, skb);
 
 	if (qnq_async_evt_rcvd(adapter) && adapter->pvid) {
@@ -930,7 +930,7 @@ static bool be_ipv6_exthdr_check(struct sk_buff *skb)
 
 static int be_vlan_tag_tx_chk(struct be_adapter *adapter, struct sk_buff *skb)
 {
-	return vlan_tx_tag_present(skb) || adapter->pvid || adapter->qnq_vid;
+	return skb_vlan_tag_present(skb) || adapter->pvid || adapter->qnq_vid;
 }
 
 static int be_ipv6_tx_stall_chk(struct be_adapter *adapter, struct sk_buff *skb)
@@ -953,7 +953,7 @@ static struct sk_buff *be_lancer_xmit_workarounds(struct be_adapter *adapter,
 	eth_hdr_len = ntohs(skb->protocol) == ETH_P_8021Q ?
 						VLAN_ETH_HLEN : ETH_HLEN;
 	if (skb->len <= 60 &&
-	    (lancer_chip(adapter) || vlan_tx_tag_present(skb)) &&
+	    (lancer_chip(adapter) || skb_vlan_tag_present(skb)) &&
 	    is_ipv4_pkt(skb)) {
 		ip = (struct iphdr *)ip_hdr(skb);
 		pskb_trim(skb, eth_hdr_len + ntohs(ip->tot_len));
@@ -971,7 +971,7 @@ static struct sk_buff *be_lancer_xmit_workarounds(struct be_adapter *adapter,
 	 * Manually insert VLAN in pkt.
 	 */
 	if (skb->ip_summed != CHECKSUM_PARTIAL &&
-	    vlan_tx_tag_present(skb)) {
+	    skb_vlan_tag_present(skb)) {
 		skb = be_insert_vlan_in_pkt(adapter, skb, skip_hw_vlan);
 		if (unlikely(!skb))
 			goto err;
* Unmerged path drivers/net/ethernet/freescale/gianfar.c
diff --git a/drivers/net/ethernet/ibm/ehea/ehea_main.c b/drivers/net/ethernet/ibm/ehea/ehea_main.c
index c0f0f29d05df..85d26f15e65d 100644
--- a/drivers/net/ethernet/ibm/ehea/ehea_main.c
+++ b/drivers/net/ethernet/ibm/ehea/ehea_main.c
@@ -2053,9 +2053,9 @@ static int ehea_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	memset(swqe, 0, SWQE_HEADER_SIZE);
 	atomic_dec(&pr->swqe_avail);
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		swqe->tx_control |= EHEA_SWQE_VLAN_INSERT;
-		swqe->vlan_tag = vlan_tx_tag_get(skb);
+		swqe->vlan_tag = skb_vlan_tag_get(skb);
 	}
 
 	pr->tx_packets++;
diff --git a/drivers/net/ethernet/intel/e1000/e1000_main.c b/drivers/net/ethernet/intel/e1000/e1000_main.c
index 60bb25c97093..7b1c0d6c5286 100644
--- a/drivers/net/ethernet/intel/e1000/e1000_main.c
+++ b/drivers/net/ethernet/intel/e1000/e1000_main.c
@@ -3217,9 +3217,10 @@ static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 		return NETDEV_TX_BUSY;
 	}
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		tx_flags |= E1000_TX_FLAGS_VLAN;
-		tx_flags |= (vlan_tx_tag_get(skb) << E1000_TX_FLAGS_VLAN_SHIFT);
+		tx_flags |= (skb_vlan_tag_get(skb) <<
+			     E1000_TX_FLAGS_VLAN_SHIFT);
 	}
 
 	first = tx_ring->next_to_use;
diff --git a/drivers/net/ethernet/intel/e1000e/netdev.c b/drivers/net/ethernet/intel/e1000e/netdev.c
index 963b807e635e..8c57cec9e0d9 100644
--- a/drivers/net/ethernet/intel/e1000e/netdev.c
+++ b/drivers/net/ethernet/intel/e1000e/netdev.c
@@ -5455,8 +5455,8 @@ static int e1000_transfer_dhcp_info(struct e1000_adapter *adapter,
 	struct e1000_hw *hw = &adapter->hw;
 	u16 length, offset;
 
-	if (vlan_tx_tag_present(skb) &&
-	    !((vlan_tx_tag_get(skb) == adapter->hw.mng_cookie.vlan_id) &&
+	if (skb_vlan_tag_present(skb) &&
+	    !((skb_vlan_tag_get(skb) == adapter->hw.mng_cookie.vlan_id) &&
 	      (adapter->hw.mng_cookie.status &
 	       E1000_MNG_DHCP_COOKIE_STATUS_VLAN)))
 		return 0;
@@ -5599,9 +5599,10 @@ static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 	if (e1000_maybe_stop_tx(tx_ring, count + 2))
 		return NETDEV_TX_BUSY;
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		tx_flags |= E1000_TX_FLAGS_VLAN;
-		tx_flags |= (vlan_tx_tag_get(skb) << E1000_TX_FLAGS_VLAN_SHIFT);
+		tx_flags |= (skb_vlan_tag_get(skb) <<
+			     E1000_TX_FLAGS_VLAN_SHIFT);
 	}
 
 	first = tx_ring->next_to_use;
* Unmerged path drivers/net/ethernet/intel/fm10k/fm10k_main.c
* Unmerged path drivers/net/ethernet/intel/fm10k/fm10k_netdev.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 74ac50f7aa71..ccd414b466dc 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -1772,8 +1772,8 @@ static int i40e_tx_prepare_vlan_flags(struct sk_buff *skb,
 	u32  tx_flags = 0;
 
 	/* if we have a HW VLAN tag being added, default to the HW one */
-	if (vlan_tx_tag_present(skb)) {
-		tx_flags |= vlan_tx_tag_get(skb) << I40E_TX_FLAGS_VLAN_SHIFT;
+	if (skb_vlan_tag_present(skb)) {
+		tx_flags |= skb_vlan_tag_get(skb) << I40E_TX_FLAGS_VLAN_SHIFT;
 		tx_flags |= I40E_TX_FLAGS_HW_VLAN;
 	/* else if it is a SW VLAN, check the next protocol and store the tag */
 	} else if (protocol == htons(ETH_P_8021Q)) {
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
index 8a5eb2f8026f..61d66a908285 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
@@ -1122,8 +1122,8 @@ static int i40e_tx_prepare_vlan_flags(struct sk_buff *skb,
 	u32  tx_flags = 0;
 
 	/* if we have a HW VLAN tag being added, default to the HW one */
-	if (vlan_tx_tag_present(skb)) {
-		tx_flags |= vlan_tx_tag_get(skb) << I40E_TX_FLAGS_VLAN_SHIFT;
+	if (skb_vlan_tag_present(skb)) {
+		tx_flags |= skb_vlan_tag_get(skb) << I40E_TX_FLAGS_VLAN_SHIFT;
 		tx_flags |= I40E_TX_FLAGS_HW_VLAN;
 	/* else if it is a SW VLAN, check the next protocol and store the tag */
 	} else if (protocol == htons(ETH_P_8021Q)) {
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index cca298badddb..c790feeb7e4c 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -5029,9 +5029,9 @@ netdev_tx_t igb_xmit_frame_ring(struct sk_buff *skb,
 
 	skb_tx_timestamp(skb);
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		tx_flags |= IGB_TX_FLAGS_VLAN;
-		tx_flags |= (vlan_tx_tag_get(skb) << IGB_TX_FLAGS_VLAN_SHIFT);
+		tx_flags |= (skb_vlan_tag_get(skb) << IGB_TX_FLAGS_VLAN_SHIFT);
 	}
 
 	/* record initial flags and protocol */
diff --git a/drivers/net/ethernet/intel/igbvf/netdev.c b/drivers/net/ethernet/intel/igbvf/netdev.c
index 63c807c9b21c..ad2b4897b392 100644
--- a/drivers/net/ethernet/intel/igbvf/netdev.c
+++ b/drivers/net/ethernet/intel/igbvf/netdev.c
@@ -2234,9 +2234,10 @@ static netdev_tx_t igbvf_xmit_frame_ring_adv(struct sk_buff *skb,
 		return NETDEV_TX_BUSY;
 	}
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		tx_flags |= IGBVF_TX_FLAGS_VLAN;
-		tx_flags |= (vlan_tx_tag_get(skb) << IGBVF_TX_FLAGS_VLAN_SHIFT);
+		tx_flags |= (skb_vlan_tag_get(skb) <<
+			     IGBVF_TX_FLAGS_VLAN_SHIFT);
 	}
 
 	if (skb->protocol == htons(ETH_P_IP))
diff --git a/drivers/net/ethernet/intel/ixgb/ixgb_main.c b/drivers/net/ethernet/intel/ixgb/ixgb_main.c
index fce3e92f9d11..248df7e06084 100644
--- a/drivers/net/ethernet/intel/ixgb/ixgb_main.c
+++ b/drivers/net/ethernet/intel/ixgb/ixgb_main.c
@@ -1540,9 +1540,9 @@ ixgb_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
                      DESC_NEEDED)))
 		return NETDEV_TX_BUSY;
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		tx_flags |= IXGB_TX_FLAGS_VLAN;
-		vlan_id = vlan_tx_tag_get(skb);
+		vlan_id = skb_vlan_tag_get(skb);
 	}
 
 	first = adapter->tx_ring.next_to_use;
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index b0a004972e7a..4485adfe5be2 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -6828,8 +6828,8 @@ netdev_tx_t ixgbe_xmit_frame_ring(struct sk_buff *skb,
 	first->gso_segs = 1;
 
 	/* if we have a HW VLAN tag being added default to the HW one */
-	if (vlan_tx_tag_present(skb)) {
-		tx_flags |= vlan_tx_tag_get(skb) << IXGBE_TX_FLAGS_VLAN_SHIFT;
+	if (skb_vlan_tag_present(skb)) {
+		tx_flags |= skb_vlan_tag_get(skb) << IXGBE_TX_FLAGS_VLAN_SHIFT;
 		tx_flags |= IXGBE_TX_FLAGS_HW_VLAN;
 	/* else if it is a SW VLAN check the next protocol and store the tag */
 	} else if (protocol == htons(ETH_P_8021Q)) {
diff --git a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 30088a979d25..4b98ba681a92 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@ -3207,8 +3207,8 @@ static int ixgbevf_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
 	first->bytecount = skb->len;
 	first->gso_segs = 1;
 
-	if (vlan_tx_tag_present(skb)) {
-		tx_flags |= vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb)) {
+		tx_flags |= skb_vlan_tag_get(skb);
 		tx_flags <<= IXGBE_TX_FLAGS_VLAN_SHIFT;
 		tx_flags |= IXGBE_TX_FLAGS_VLAN;
 	}
diff --git a/drivers/net/ethernet/jme.c b/drivers/net/ethernet/jme.c
index 070a6f1a0577..e603bb625899 100644
--- a/drivers/net/ethernet/jme.c
+++ b/drivers/net/ethernet/jme.c
@@ -2131,9 +2131,9 @@ jme_tx_csum(struct jme_adapter *jme, struct sk_buff *skb, u8 *flags)
 static inline void
 jme_tx_vlan(struct sk_buff *skb, __le16 *vlan, u8 *flags)
 {
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		*flags |= TXFLAG_TAGON;
-		*vlan = cpu_to_le16(vlan_tx_tag_get(skb));
+		*vlan = cpu_to_le16(skb_vlan_tag_get(skb));
 	}
 }
 
diff --git a/drivers/net/ethernet/marvell/sky2.c b/drivers/net/ethernet/marvell/sky2.c
index d175bbd3ffd3..c816fd683d1e 100644
--- a/drivers/net/ethernet/marvell/sky2.c
+++ b/drivers/net/ethernet/marvell/sky2.c
@@ -1897,14 +1897,14 @@ static netdev_tx_t sky2_xmit_frame(struct sk_buff *skb,
 	ctrl = 0;
 
 	/* Add VLAN tag, can piggyback on LRGLEN or ADDR64 */
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		if (!le) {
 			le = get_tx_le(sky2, &slot);
 			le->addr = 0;
 			le->opcode = OP_VLAN|HW_OWNER;
 		} else
 			le->opcode |= OP_VLAN;
-		le->length = cpu_to_be16(vlan_tx_tag_get(skb));
+		le->length = cpu_to_be16(skb_vlan_tag_get(skb));
 		ctrl |= INS_VLAN;
 	}
 
@@ -2595,7 +2595,7 @@ static struct sk_buff *sky2_receive(struct net_device *dev,
 	sky2->rx_next = (sky2->rx_next + 1) % sky2->rx_pending;
 	prefetch(sky2->rx_ring + sky2->rx_next);
 
-	if (vlan_tx_tag_present(re->skb))
+	if (skb_vlan_tag_present(re->skb))
 		count -= VLAN_HLEN;	/* Account for vlan tag */
 
 	/* This chip has hardware problems that generates bogus status.
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_tx.c
diff --git a/drivers/net/ethernet/natsemi/ns83820.c b/drivers/net/ethernet/natsemi/ns83820.c
index d3b47003a575..ff90e3abc419 100644
--- a/drivers/net/ethernet/natsemi/ns83820.c
+++ b/drivers/net/ethernet/natsemi/ns83820.c
@@ -1123,12 +1123,12 @@ again:
 	}
 
 #ifdef NS83820_VLAN_ACCEL_SUPPORT
-	if(vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		/* fetch the vlan tag info out of the
 		 * ancillary data if the vlan code
 		 * is using hw vlan acceleration
 		 */
-		short tag = vlan_tx_tag_get(skb);
+		short tag = skb_vlan_tag_get(skb);
 		extsts |= (EXTSTS_VPKT | htons(tag));
 	}
 #endif
diff --git a/drivers/net/ethernet/neterion/s2io.c b/drivers/net/ethernet/neterion/s2io.c
index 51b00941302c..0975dba2dbc1 100644
--- a/drivers/net/ethernet/neterion/s2io.c
+++ b/drivers/net/ethernet/neterion/s2io.c
@@ -4050,8 +4050,8 @@ static netdev_tx_t s2io_xmit(struct sk_buff *skb, struct net_device *dev)
 	}
 
 	queue = 0;
-	if (vlan_tx_tag_present(skb))
-		vlan_tag = vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb))
+		vlan_tag = skb_vlan_tag_get(skb);
 	if (sp->config.tx_steering_type == TX_DEFAULT_STEERING) {
 		if (skb->protocol == htons(ETH_P_IP)) {
 			struct iphdr *ip;
diff --git a/drivers/net/ethernet/neterion/vxge/vxge-main.c b/drivers/net/ethernet/neterion/vxge/vxge-main.c
index cbfaed5f2f8d..7b9b89443160 100644
--- a/drivers/net/ethernet/neterion/vxge/vxge-main.c
+++ b/drivers/net/ethernet/neterion/vxge/vxge-main.c
@@ -888,8 +888,8 @@ vxge_xmit(struct sk_buff *skb, struct net_device *dev)
 		dev->name, __func__, __LINE__,
 		fifo_hw, dtr, dtr_priv);
 
-	if (vlan_tx_tag_present(skb)) {
-		u16 vlan_tag = vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb)) {
+		u16 vlan_tag = skb_vlan_tag_get(skb);
 		vxge_hw_fifo_txdl_vlan_set(dtr, vlan_tag);
 	}
 
diff --git a/drivers/net/ethernet/nvidia/forcedeth.c b/drivers/net/ethernet/nvidia/forcedeth.c
index b003fe53c8e2..6aac594a2a62 100644
--- a/drivers/net/ethernet/nvidia/forcedeth.c
+++ b/drivers/net/ethernet/nvidia/forcedeth.c
@@ -2464,9 +2464,9 @@ static netdev_tx_t nv_start_xmit_optimized(struct sk_buff *skb,
 			 NV_TX2_CHECKSUM_L3 | NV_TX2_CHECKSUM_L4 : 0;
 
 	/* vlan tag */
-	if (vlan_tx_tag_present(skb))
+	if (skb_vlan_tag_present(skb))
 		start_tx->txvlan = cpu_to_le32(NV_TX3_VLAN_TAG_PRESENT |
-					vlan_tx_tag_get(skb));
+					skb_vlan_tag_get(skb));
 	else
 		start_tx->txvlan = 0;
 
diff --git a/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c b/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c
index 081cf765b96b..147e81a38662 100644
--- a/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c
+++ b/drivers/net/ethernet/qlogic/netxen/netxen_nic_main.c
@@ -1893,9 +1893,9 @@ netxen_tso_check(struct net_device *netdev,
 		protocol = vh->h_vlan_encapsulated_proto;
 		flags = FLAGS_VLAN_TAGGED;
 
-	} else if (vlan_tx_tag_present(skb)) {
+	} else if (skb_vlan_tag_present(skb)) {
 		flags = FLAGS_VLAN_OOB;
-		vid = vlan_tx_tag_get(skb);
+		vid = skb_vlan_tag_get(skb);
 		netxen_set_tx_vlan_tci(first_desc, vid);
 		vlan_oob = 1;
 	}
diff --git a/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c b/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c
index 3f8e2b38bc0a..2519694fef85 100644
--- a/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c
+++ b/drivers/net/ethernet/qlogic/qlcnic/qlcnic_io.c
@@ -320,8 +320,8 @@ static void qlcnic_send_filter(struct qlcnic_adapter *adapter,
 		if (protocol == ETH_P_8021Q) {
 			vh = (struct vlan_ethhdr *)skb->data;
 			vlan_id = ntohs(vh->h_vlan_TCI);
-		} else if (vlan_tx_tag_present(skb)) {
-			vlan_id = vlan_tx_tag_get(skb);
+		} else if (skb_vlan_tag_present(skb)) {
+			vlan_id = skb_vlan_tag_get(skb);
 		}
 	}
 
@@ -472,9 +472,9 @@ static int qlcnic_tx_pkt(struct qlcnic_adapter *adapter,
 		flags = QLCNIC_FLAGS_VLAN_TAGGED;
 		vlan_tci = ntohs(vh->h_vlan_TCI);
 		protocol = ntohs(vh->h_vlan_encapsulated_proto);
-	} else if (vlan_tx_tag_present(skb)) {
+	} else if (skb_vlan_tag_present(skb)) {
 		flags = QLCNIC_FLAGS_VLAN_OOB;
-		vlan_tci = vlan_tx_tag_get(skb);
+		vlan_tci = skb_vlan_tag_get(skb);
 	}
 	if (unlikely(adapter->tx_pvid)) {
 		if (vlan_tci && !(adapter->flags & QLCNIC_TAGGING_ENABLED))
diff --git a/drivers/net/ethernet/qlogic/qlge/qlge_main.c b/drivers/net/ethernet/qlogic/qlge/qlge_main.c
index 1c3e1996018d..ac23cbebd961 100644
--- a/drivers/net/ethernet/qlogic/qlge/qlge_main.c
+++ b/drivers/net/ethernet/qlogic/qlge/qlge_main.c
@@ -2666,11 +2666,11 @@ static netdev_tx_t qlge_send(struct sk_buff *skb, struct net_device *ndev)
 
 	mac_iocb_ptr->frame_len = cpu_to_le16((u16) skb->len);
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		netif_printk(qdev, tx_queued, KERN_DEBUG, qdev->ndev,
-			     "Adding a vlan tag %d.\n", vlan_tx_tag_get(skb));
+			     "Adding a vlan tag %d.\n", skb_vlan_tag_get(skb));
 		mac_iocb_ptr->flags3 |= OB_MAC_IOCB_V;
-		mac_iocb_ptr->vlan_tci = cpu_to_le16(vlan_tx_tag_get(skb));
+		mac_iocb_ptr->vlan_tci = cpu_to_le16(skb_vlan_tag_get(skb));
 	}
 	tso = ql_tso(skb, (struct ob_mac_tso_iocb_req *)mac_iocb_ptr);
 	if (tso < 0) {
diff --git a/drivers/net/ethernet/realtek/8139cp.c b/drivers/net/ethernet/realtek/8139cp.c
index 064425d3178d..9e061dcdb9fd 100644
--- a/drivers/net/ethernet/realtek/8139cp.c
+++ b/drivers/net/ethernet/realtek/8139cp.c
@@ -718,8 +718,8 @@ static void cp_tx (struct cp_private *cp)
 
 static inline u32 cp_tx_vlan_tag(struct sk_buff *skb)
 {
-	return vlan_tx_tag_present(skb) ?
-		TxVlanTag | swab16(vlan_tx_tag_get(skb)) : 0x00;
+	return skb_vlan_tag_present(skb) ?
+		TxVlanTag | swab16(skb_vlan_tag_get(skb)) : 0x00;
 }
 
 static void unwind_tx_frag_mapping(struct cp_private *cp, struct sk_buff *skb,
diff --git a/drivers/net/ethernet/realtek/r8169.c b/drivers/net/ethernet/realtek/r8169.c
index 0472a23610a8..3cdf5788178b 100644
--- a/drivers/net/ethernet/realtek/r8169.c
+++ b/drivers/net/ethernet/realtek/r8169.c
@@ -2068,8 +2068,8 @@ static int rtl8169_set_features(struct net_device *dev,
 
 static inline u32 rtl8169_tx_vlan_tag(struct sk_buff *skb)
 {
-	return (vlan_tx_tag_present(skb)) ?
-		TxVlanTag | swab16(vlan_tx_tag_get(skb)) : 0x00;
+	return (skb_vlan_tag_present(skb)) ?
+		TxVlanTag | swab16(skb_vlan_tag_get(skb)) : 0x00;
 }
 
 static void rtl8169_rx_vlan_tag(struct RxDesc *desc, struct sk_buff *skb)
* Unmerged path drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
diff --git a/drivers/net/ethernet/tehuti/tehuti.c b/drivers/net/ethernet/tehuti/tehuti.c
index 571452e786d5..a0848c35251e 100644
--- a/drivers/net/ethernet/tehuti/tehuti.c
+++ b/drivers/net/ethernet/tehuti/tehuti.c
@@ -1650,9 +1650,9 @@ static netdev_tx_t bdx_tx_transmit(struct sk_buff *skb,
 		    txd_mss);
 	}
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		/*Cut VLAN ID to 12 bits */
-		txd_vlan_id = vlan_tx_tag_get(skb) & BITS_MASK(12);
+		txd_vlan_id = skb_vlan_tag_get(skb) & BITS_MASK(12);
 		txd_vtag = 1;
 	}
 
diff --git a/drivers/net/ethernet/via/via-rhine.c b/drivers/net/ethernet/via/via-rhine.c
index 75b82b6e8c54..e24941c749b5 100644
--- a/drivers/net/ethernet/via/via-rhine.c
+++ b/drivers/net/ethernet/via/via-rhine.c
@@ -1693,8 +1693,8 @@ static netdev_tx_t rhine_start_tx(struct sk_buff *skb,
 	rp->tx_ring[entry].desc_length =
 		cpu_to_le32(TXDESC | (skb->len >= ETH_ZLEN ? skb->len : ETH_ZLEN));
 
-	if (unlikely(vlan_tx_tag_present(skb))) {
-		u16 vid_pcp = vlan_tx_tag_get(skb);
+	if (unlikely(skb_vlan_tag_present(skb))) {
+		u16 vid_pcp = skb_vlan_tag_get(skb);
 
 		/* drop CFI/DEI bit, register needs VID and PCP */
 		vid_pcp = (vid_pcp & VLAN_VID_MASK) |
@@ -1715,7 +1715,7 @@ static netdev_tx_t rhine_start_tx(struct sk_buff *skb,
 
 	/* Non-x86 Todo: explicitly flush cache lines here. */
 
-	if (vlan_tx_tag_present(skb))
+	if (skb_vlan_tag_present(skb))
 		/* Tx queues are bits 7-0 (first Tx queue: bit 7) */
 		BYTE_REG_BITS_ON(1 << 7, ioaddr + TQWake);
 
diff --git a/drivers/net/ethernet/via/via-velocity.c b/drivers/net/ethernet/via/via-velocity.c
index fb6248956ee2..0205043a95dd 100644
--- a/drivers/net/ethernet/via/via-velocity.c
+++ b/drivers/net/ethernet/via/via-velocity.c
@@ -2573,8 +2573,8 @@ static netdev_tx_t velocity_xmit(struct sk_buff *skb,
 
 	td_ptr->tdesc1.cmd = TCPLS_NORMAL + (tdinfo->nskb_dma + 1) * 16;
 
-	if (vlan_tx_tag_present(skb)) {
-		td_ptr->tdesc1.vlan = cpu_to_le16(vlan_tx_tag_get(skb));
+	if (skb_vlan_tag_present(skb)) {
+		td_ptr->tdesc1.vlan = cpu_to_le16(skb_vlan_tag_get(skb));
 		td_ptr->tdesc1.TCR |= TCR0_VETAG;
 	}
 
* Unmerged path drivers/net/macvtap.c
* Unmerged path drivers/net/tun.c
* Unmerged path drivers/net/usb/r8152.c
diff --git a/drivers/net/vmxnet3/vmxnet3_drv.c b/drivers/net/vmxnet3/vmxnet3_drv.c
index acada8e7ec95..aa624f8e6426 100644
--- a/drivers/net/vmxnet3/vmxnet3_drv.c
+++ b/drivers/net/vmxnet3/vmxnet3_drv.c
@@ -1038,9 +1038,9 @@ vmxnet3_tq_xmit(struct sk_buff *skb, struct vmxnet3_tx_queue *tq,
 		le32_add_cpu(&tq->shared->txNumDeferred, 1);
 	}
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		gdesc->txd.ti = 1;
-		gdesc->txd.tci = vlan_tx_tag_get(skb);
+		gdesc->txd.tci = skb_vlan_tag_get(skb);
 	}
 
 	/* finally flips the GEN bit of the SOP desc. */
diff --git a/drivers/net/vxlan.c b/drivers/net/vxlan.c
index 1f7710467383..69f1a085e20b 100644
--- a/drivers/net/vxlan.c
+++ b/drivers/net/vxlan.c
@@ -1555,7 +1555,7 @@ static int vxlan6_xmit_skb(struct net *net, struct vxlan_sock *vs,
 
 	min_headroom = LL_RESERVED_SPACE(dst->dev) + dst->header_len
 			+ VXLAN_HLEN + sizeof(struct ipv6hdr)
-			+ (vlan_tx_tag_present(skb) ? VLAN_HLEN : 0);
+			+ (skb_vlan_tag_present(skb) ? VLAN_HLEN : 0);
 
 	/* Need space for new headers (invalidates iph ptr) */
 	err = skb_cow_head(skb, min_headroom);
@@ -1628,7 +1628,7 @@ int vxlan_xmit_skb(struct net *net, struct vxlan_sock *vs,
 
 	min_headroom = LL_RESERVED_SPACE(rt->dst.dev) + rt->dst.header_len
 			+ VXLAN_HLEN + sizeof(struct iphdr)
-			+ (vlan_tx_tag_present(skb) ? VLAN_HLEN : 0);
+			+ (skb_vlan_tag_present(skb) ? VLAN_HLEN : 0);
 
 	/* Need space for new headers (invalidates iph ptr) */
 	err = skb_cow_head(skb, min_headroom);
diff --git a/drivers/s390/net/qeth_l3_main.c b/drivers/s390/net/qeth_l3_main.c
index c8cbc6835e31..d330b5c7e568 100644
--- a/drivers/s390/net/qeth_l3_main.c
+++ b/drivers/s390/net/qeth_l3_main.c
@@ -2800,12 +2800,12 @@ static void qeth_l3_fill_header(struct qeth_card *card, struct qeth_hdr *hdr,
 	 * before we're going to overwrite this location with next hop ip.
 	 * v6 uses passthrough, v4 sets the tag in the QDIO header.
 	 */
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		if ((ipv == 4) || (card->info.type == QETH_CARD_TYPE_IQD))
 			hdr->hdr.l3.ext_flags = QETH_HDR_EXT_VLAN_FRAME;
 		else
 			hdr->hdr.l3.ext_flags = QETH_HDR_EXT_INCLUDE_VLAN_TAG;
-		hdr->hdr.l3.vlan_id = vlan_tx_tag_get(skb);
+		hdr->hdr.l3.vlan_id = skb_vlan_tag_get(skb);
 	}
 
 	hdr->hdr.l3.length = skb->len - sizeof(struct qeth_hdr);
@@ -2983,7 +2983,7 @@ static int qeth_l3_hard_start_xmit(struct sk_buff *skb, struct net_device *dev)
 			skb_pull(new_skb, ETH_HLEN);
 		}
 
-		if (ipv != 4 && vlan_tx_tag_present(new_skb)) {
+		if (ipv != 4 && skb_vlan_tag_present(new_skb)) {
 			skb_push(new_skb, VLAN_HLEN);
 			skb_copy_to_linear_data(new_skb, new_skb->data + 4, 4);
 			skb_copy_to_linear_data_offset(new_skb, 4,
@@ -2992,7 +2992,7 @@ static int qeth_l3_hard_start_xmit(struct sk_buff *skb, struct net_device *dev)
 				new_skb->data + 12, 4);
 			tag = (u16 *)(new_skb->data + 12);
 			*tag = __constant_htons(ETH_P_8021Q);
-			*(tag + 1) = htons(vlan_tx_tag_get(new_skb));
+			*(tag + 1) = htons(skb_vlan_tag_get(new_skb));
 		}
 	}
 
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index fc67228014bd..ecc725748a77 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -475,7 +475,7 @@ static int peek_head_len(struct sock *sk)
 	head = skb_peek(&sk->sk_receive_queue);
 	if (likely(head)) {
 		len = head->len;
-		if (vlan_tx_tag_present(head))
+		if (skb_vlan_tag_present(head))
 			len += VLAN_HLEN;
 	}
 
* Unmerged path include/linux/if_vlan.h
* Unmerged path include/net/pkt_sched.h
* Unmerged path include/trace/events/net.h
diff --git a/net/8021q/vlan_core.c b/net/8021q/vlan_core.c
index 6ee48aac776f..b502e3edc544 100644
--- a/net/8021q/vlan_core.c
+++ b/net/8021q/vlan_core.c
@@ -9,7 +9,7 @@ bool vlan_do_receive(struct sk_buff **skbp)
 {
 	struct sk_buff *skb = *skbp;
 	__be16 vlan_proto = skb->vlan_proto;
-	u16 vlan_id = vlan_tx_tag_get_id(skb);
+	u16 vlan_id = skb_vlan_tag_get_id(skb);
 	struct net_device *vlan_dev;
 	struct vlan_pcpu_stats *rx_stats;
 
* Unmerged path net/bridge/br_netfilter.c
diff --git a/net/bridge/br_private.h b/net/bridge/br_private.h
index fe99f0095976..f2a18cbe0a65 100644
--- a/net/bridge/br_private.h
+++ b/net/bridge/br_private.h
@@ -646,8 +646,8 @@ static inline int br_vlan_get_tag(const struct sk_buff *skb, u16 *vid)
 {
 	int err = 0;
 
-	if (vlan_tx_tag_present(skb))
-		*vid = vlan_tx_tag_get(skb) & VLAN_VID_MASK;
+	if (skb_vlan_tag_present(skb))
+		*vid = skb_vlan_tag_get(skb) & VLAN_VID_MASK;
 	else {
 		*vid = 0;
 		err = -EINVAL;
* Unmerged path net/bridge/br_vlan.c
diff --git a/net/bridge/netfilter/ebt_vlan.c b/net/bridge/netfilter/ebt_vlan.c
index eae67bf0446c..a6f3f6bee06f 100644
--- a/net/bridge/netfilter/ebt_vlan.c
+++ b/net/bridge/netfilter/ebt_vlan.c
@@ -46,8 +46,8 @@ ebt_vlan_mt(const struct sk_buff *skb, struct xt_action_param *par)
 	/* VLAN encapsulated Type/Length field, given from orig frame */
 	__be16 encap;
 
-	if (vlan_tx_tag_present(skb)) {
-		TCI = vlan_tx_tag_get(skb);
+	if (skb_vlan_tag_present(skb)) {
+		TCI = skb_vlan_tag_get(skb);
 		encap = skb->protocol;
 	} else {
 		const struct vlan_hdr *fp;
diff --git a/net/bridge/netfilter/ebtables.c b/net/bridge/netfilter/ebtables.c
index 6651a7797d46..f1c6fb79a68c 100644
--- a/net/bridge/netfilter/ebtables.c
+++ b/net/bridge/netfilter/ebtables.c
@@ -132,7 +132,7 @@ ebt_basic_match(const struct ebt_entry *e, const struct sk_buff *skb,
 	__be16 ethproto;
 	int verdict, i;
 
-	if (vlan_tx_tag_present(skb))
+	if (skb_vlan_tag_present(skb))
 		ethproto = htons(ETH_P_8021Q);
 	else
 		ethproto = h->h_proto;
* Unmerged path net/core/dev.c
diff --git a/net/core/netpoll.c b/net/core/netpoll.c
index 89e339d3631e..685a0e15626a 100644
--- a/net/core/netpoll.c
+++ b/net/core/netpoll.c
@@ -82,7 +82,7 @@ static int netpoll_start_xmit(struct sk_buff *skb, struct net_device *dev,
 
 	features = netif_skb_features(skb);
 
-	if (vlan_tx_tag_present(skb) &&
+	if (skb_vlan_tag_present(skb) &&
 	    !vlan_hw_offload_capable(features, skb->vlan_proto)) {
 		skb = __vlan_put_tag(skb, skb->vlan_proto,
 				     vlan_tx_tag_get(skb));
* Unmerged path net/core/skbuff.c
* Unmerged path net/ipv4/geneve.c
* Unmerged path net/openvswitch/actions.c
diff --git a/net/openvswitch/datapath.c b/net/openvswitch/datapath.c
index 054e47f4601f..f9c47385266e 100644
--- a/net/openvswitch/datapath.c
+++ b/net/openvswitch/datapath.c
@@ -416,7 +416,7 @@ static int queue_userspace_packet(struct datapath *dp, struct sk_buff *skb,
 	if (!dp_ifindex)
 		return -ENODEV;
 
-	if (vlan_tx_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb)) {
 		nskb = skb_clone(skb, GFP_ATOMIC);
 		if (!nskb)
 			return -ENOMEM;
* Unmerged path net/openvswitch/flow.c
diff --git a/net/openvswitch/vport-gre.c b/net/openvswitch/vport-gre.c
index 4d7d657439f7..74f91180c2d0 100644
--- a/net/openvswitch/vport-gre.c
+++ b/net/openvswitch/vport-gre.c
@@ -160,7 +160,7 @@ static int gre_tnl_send(struct vport *vport, struct sk_buff *skb)
 
 	min_headroom = LL_RESERVED_SPACE(rt->dst.dev) + rt->dst.header_len
 			+ tunnel_hlen + sizeof(struct iphdr)
-			+ (vlan_tx_tag_present(skb) ? VLAN_HLEN : 0);
+			+ (skb_vlan_tag_present(skb) ? VLAN_HLEN : 0);
 	if (skb_headroom(skb) < min_headroom || skb_header_cloned(skb)) {
 		int head_delta = SKB_DATA_ALIGN(min_headroom -
 						skb_headroom(skb) +
* Unmerged path net/openvswitch/vport.c
* Unmerged path net/packet/af_packet.c
diff --git a/net/sched/em_meta.c b/net/sched/em_meta.c
index 2c04ce77a278..22e7f0d90d10 100644
--- a/net/sched/em_meta.c
+++ b/net/sched/em_meta.c
@@ -176,7 +176,7 @@ META_COLLECTOR(int_vlan_tag)
 {
 	unsigned short tag;
 
-	tag = vlan_tx_tag_get(skb);
+	tag = skb_vlan_tag_get(skb);
 	if (!tag && __vlan_get_tag(skb, &tag))
 		*err = -1;
 	else
diff --git a/net/wireless/util.c b/net/wireless/util.c
index 728f1c0dc70d..3c5f9de09821 100644
--- a/net/wireless/util.c
+++ b/net/wireless/util.c
@@ -705,8 +705,8 @@ unsigned int cfg80211_classify8021d(struct sk_buff *skb,
 	if (skb->priority >= 256 && skb->priority <= 263)
 		return skb->priority - 256;
 
-	if (vlan_tx_tag_present(skb)) {
-		vlan_priority = (vlan_tx_tag_get(skb) & VLAN_PRIO_MASK)
+	if (skb_vlan_tag_present(skb)) {
+		vlan_priority = (skb_vlan_tag_get(skb) & VLAN_PRIO_MASK)
 			>> VLAN_PRIO_SHIFT;
 		if (vlan_priority > 0)
 			return vlan_priority;
