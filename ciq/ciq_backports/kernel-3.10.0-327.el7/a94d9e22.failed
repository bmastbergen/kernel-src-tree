ethernet/intel: Use eth_skb_pad and skb_put_padto helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Alexander Duyck <alexander.h.duyck@redhat.com>
commit a94d9e224e3c48f57559183582c6410e7acf1d8b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/a94d9e22.failed

Update the Intel Ethernet drivers to use eth_skb_pad() and skb_put_padto
instead of doing their own implementations of the function.

Also this cleans up two other spots where skb_pad was called but the length
and tail pointers were being manipulated directly instead of just having
the padding length added via __skb_put.

	Cc: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
	Signed-off-by: Alexander Duyck <alexander.h.duyck@redhat.com>
	Acked-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a94d9e224e3c48f57559183582c6410e7acf1d8b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/fm10k/fm10k_main.c
#	drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
diff --cc drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 30088a979d25,62a0d8e0f17d..000000000000
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@@ -435,41 -490,304 +435,220 @@@ static void ixgbevf_alloc_rx_buffers(st
  	struct ixgbevf_rx_buffer *bi;
  	unsigned int i = rx_ring->next_to_use;
  
 -	/* nothing to do or no valid netdev defined */
 -	if (!cleaned_count || !rx_ring->netdev)
 -		return;
 +	while (cleaned_count--) {
 +		rx_desc = IXGBEVF_RX_DESC(rx_ring, i);
 +		bi = &rx_ring->rx_buffer_info[i];
  
 -	rx_desc = IXGBEVF_RX_DESC(rx_ring, i);
 -	bi = &rx_ring->rx_buffer_info[i];
 -	i -= rx_ring->count;
 +		if (!bi->skb) {
 +			struct sk_buff *skb;
  
 -	do {
 -		if (!ixgbevf_alloc_mapped_page(rx_ring, bi))
 -			break;
 +			skb = netdev_alloc_skb_ip_align(rx_ring->netdev,
 +							rx_ring->rx_buf_len);
 +			if (!skb)
 +				goto no_buffers;
  
 -		/* Refresh the desc even if pkt_addr didn't change
 -		 * because each write-back erases this info.
 -		 */
 -		rx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);
 +			bi->skb = skb;
  
 -		rx_desc++;
 -		bi++;
 -		i++;
 -		if (unlikely(!i)) {
 -			rx_desc = IXGBEVF_RX_DESC(rx_ring, 0);
 -			bi = rx_ring->rx_buffer_info;
 -			i -= rx_ring->count;
 +			bi->dma = dma_map_single(rx_ring->dev, skb->data,
 +						 rx_ring->rx_buf_len,
 +						 DMA_FROM_DEVICE);
 +			if (dma_mapping_error(rx_ring->dev, bi->dma)) {
 +				dev_kfree_skb(skb);
 +				bi->skb = NULL;
 +				dev_err(rx_ring->dev, "Rx DMA map failed\n");
 +				break;
 +			}
  		}
 +		rx_desc->read.pkt_addr = cpu_to_le64(bi->dma);
  
 -		/* clear the hdr_addr for the next_to_use descriptor */
 -		rx_desc->read.hdr_addr = 0;
 -
 -		cleaned_count--;
 -	} while (cleaned_count);
 -
 -	i += rx_ring->count;
 -
 -	if (rx_ring->next_to_use != i) {
 -		/* record the next descriptor to use */
 -		rx_ring->next_to_use = i;
 -
 -		/* update next to alloc since we have filled the ring */
 -		rx_ring->next_to_alloc = i;
 -
 -		/* Force memory writes to complete before letting h/w
 -		 * know there are new descriptors to fetch.  (Only
 -		 * applicable for weak-ordered memory model archs,
 -		 * such as IA-64).
 -		 */
 -		wmb();
 -		ixgbevf_write_tail(rx_ring, i);
 -	}
 -}
 -
 -/* ixgbevf_pull_tail - ixgbevf specific version of skb_pull_tail
 - * @rx_ring: rx descriptor ring packet is being transacted on
 - * @skb: pointer to current skb being adjusted
 - *
 - * This function is an ixgbevf specific version of __pskb_pull_tail.  The
 - * main difference between this version and the original function is that
 - * this function can make several assumptions about the state of things
 - * that allow for significant optimizations versus the standard function.
 - * As a result we can do things like drop a frag and maintain an accurate
 - * truesize for the skb.
 - */
 -static void ixgbevf_pull_tail(struct ixgbevf_ring *rx_ring,
 -			      struct sk_buff *skb)
 -{
 -	struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];
 -	unsigned char *va;
 -	unsigned int pull_len;
 -
 -	/* it is valid to use page_address instead of kmap since we are
 -	 * working with pages allocated out of the lomem pool per
 -	 * alloc_page(GFP_ATOMIC)
 -	 */
 -	va = skb_frag_address(frag);
 -
 -	/* we need the header to contain the greater of either ETH_HLEN or
 -	 * 60 bytes if the skb->len is less than 60 for skb_pad.
 -	 */
 -	pull_len = eth_get_headlen(va, IXGBEVF_RX_HDR_SIZE);
 -
 -	/* align pull length to size of long to optimize memcpy performance */
 -	skb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));
 -
 -	/* update all of the pointers */
 -	skb_frag_size_sub(frag, pull_len);
 -	frag->page_offset += pull_len;
 -	skb->data_len -= pull_len;
 -	skb->tail += pull_len;
 -}
 -
 -/* ixgbevf_cleanup_headers - Correct corrupted or empty headers
 - * @rx_ring: rx descriptor ring packet is being transacted on
 - * @rx_desc: pointer to the EOP Rx descriptor
 - * @skb: pointer to current skb being fixed
 - *
 - * Check for corrupted packet headers caused by senders on the local L2
 - * embedded NIC switch not setting up their Tx Descriptors right.  These
 - * should be very rare.
 - *
 - * Also address the case where we are pulling data in on pages only
 - * and as such no data is present in the skb header.
 - *
 - * In addition if skb is not at least 60 bytes we need to pad it so that
 - * it is large enough to qualify as a valid Ethernet frame.
 - *
 - * Returns true if an error was encountered and skb was freed.
 - */
 -static bool ixgbevf_cleanup_headers(struct ixgbevf_ring *rx_ring,
 -				    union ixgbe_adv_rx_desc *rx_desc,
 -				    struct sk_buff *skb)
 -{
 -	/* verify that the packet does not have any known errors */
 -	if (unlikely(ixgbevf_test_staterr(rx_desc,
 -					  IXGBE_RXDADV_ERR_FRAME_ERR_MASK))) {
 -		struct net_device *netdev = rx_ring->netdev;
 -
 -		if (!(netdev->features & NETIF_F_RXALL)) {
 -			dev_kfree_skb_any(skb);
 -			return true;
 -		}
 +		i++;
 +		if (i == rx_ring->count)
 +			i = 0;
  	}
  
++<<<<<<< HEAD
 +no_buffers:
 +	rx_ring->rx_stats.alloc_rx_buff_failed++;
 +	if (rx_ring->next_to_use != i)
 +		ixgbevf_release_rx_desc(rx_ring, i);
++=======
+ 	/* place header in linear portion of buffer */
+ 	if (skb_is_nonlinear(skb))
+ 		ixgbevf_pull_tail(rx_ring, skb);
+ 
+ 	/* if eth_skb_pad returns an error the skb was freed */
+ 	if (eth_skb_pad(skb))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /* ixgbevf_reuse_rx_page - page flip buffer and store it back on the ring
+  * @rx_ring: rx descriptor ring to store buffers on
+  * @old_buff: donor buffer to have page reused
+  *
+  * Synchronizes page for reuse by the adapter
+  */
+ static void ixgbevf_reuse_rx_page(struct ixgbevf_ring *rx_ring,
+ 				  struct ixgbevf_rx_buffer *old_buff)
+ {
+ 	struct ixgbevf_rx_buffer *new_buff;
+ 	u16 nta = rx_ring->next_to_alloc;
+ 
+ 	new_buff = &rx_ring->rx_buffer_info[nta];
+ 
+ 	/* update, and store next to alloc */
+ 	nta++;
+ 	rx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;
+ 
+ 	/* transfer page from old buffer to new buffer */
+ 	new_buff->page = old_buff->page;
+ 	new_buff->dma = old_buff->dma;
+ 	new_buff->page_offset = old_buff->page_offset;
+ 
+ 	/* sync the buffer for use by the device */
+ 	dma_sync_single_range_for_device(rx_ring->dev, new_buff->dma,
+ 					 new_buff->page_offset,
+ 					 IXGBEVF_RX_BUFSZ,
+ 					 DMA_FROM_DEVICE);
+ }
+ 
+ static inline bool ixgbevf_page_is_reserved(struct page *page)
+ {
+ 	return (page_to_nid(page) != numa_mem_id()) || page->pfmemalloc;
+ }
+ 
+ /* ixgbevf_add_rx_frag - Add contents of Rx buffer to sk_buff
+  * @rx_ring: rx descriptor ring to transact packets on
+  * @rx_buffer: buffer containing page to add
+  * @rx_desc: descriptor containing length of buffer written by hardware
+  * @skb: sk_buff to place the data into
+  *
+  * This function will add the data contained in rx_buffer->page to the skb.
+  * This is done either through a direct copy if the data in the buffer is
+  * less than the skb header size, otherwise it will just attach the page as
+  * a frag to the skb.
+  *
+  * The function will then update the page offset if necessary and return
+  * true if the buffer can be reused by the adapter.
+  */
+ static bool ixgbevf_add_rx_frag(struct ixgbevf_ring *rx_ring,
+ 				struct ixgbevf_rx_buffer *rx_buffer,
+ 				union ixgbe_adv_rx_desc *rx_desc,
+ 				struct sk_buff *skb)
+ {
+ 	struct page *page = rx_buffer->page;
+ 	unsigned int size = le16_to_cpu(rx_desc->wb.upper.length);
+ #if (PAGE_SIZE < 8192)
+ 	unsigned int truesize = IXGBEVF_RX_BUFSZ;
+ #else
+ 	unsigned int truesize = ALIGN(size, L1_CACHE_BYTES);
+ #endif
+ 
+ 	if ((size <= IXGBEVF_RX_HDR_SIZE) && !skb_is_nonlinear(skb)) {
+ 		unsigned char *va = page_address(page) + rx_buffer->page_offset;
+ 
+ 		memcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));
+ 
+ 		/* page is not reserved, we can reuse buffer as is */
+ 		if (likely(!ixgbevf_page_is_reserved(page)))
+ 			return true;
+ 
+ 		/* this page cannot be reused so discard it */
+ 		put_page(page);
+ 		return false;
+ 	}
+ 
+ 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,
+ 			rx_buffer->page_offset, size, truesize);
+ 
+ 	/* avoid re-using remote pages */
+ 	if (unlikely(ixgbevf_page_is_reserved(page)))
+ 		return false;
+ 
+ #if (PAGE_SIZE < 8192)
+ 	/* if we are only owner of page we can reuse it */
+ 	if (unlikely(page_count(page) != 1))
+ 		return false;
+ 
+ 	/* flip page offset to other buffer */
+ 	rx_buffer->page_offset ^= IXGBEVF_RX_BUFSZ;
+ 
+ #else
+ 	/* move offset up to the next cache line */
+ 	rx_buffer->page_offset += truesize;
+ 
+ 	if (rx_buffer->page_offset > (PAGE_SIZE - IXGBEVF_RX_BUFSZ))
+ 		return false;
+ 
+ #endif
+ 	/* Even if we own the page, we are not allowed to use atomic_set()
+ 	 * This would break get_page_unless_zero() users.
+ 	 */
+ 	atomic_inc(&page->_count);
+ 
+ 	return true;
+ }
+ 
+ static struct sk_buff *ixgbevf_fetch_rx_buffer(struct ixgbevf_ring *rx_ring,
+ 					       union ixgbe_adv_rx_desc *rx_desc,
+ 					       struct sk_buff *skb)
+ {
+ 	struct ixgbevf_rx_buffer *rx_buffer;
+ 	struct page *page;
+ 
+ 	rx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];
+ 	page = rx_buffer->page;
+ 	prefetchw(page);
+ 
+ 	if (likely(!skb)) {
+ 		void *page_addr = page_address(page) +
+ 				  rx_buffer->page_offset;
+ 
+ 		/* prefetch first cache line of first page */
+ 		prefetch(page_addr);
+ #if L1_CACHE_BYTES < 128
+ 		prefetch(page_addr + L1_CACHE_BYTES);
+ #endif
+ 
+ 		/* allocate a skb to store the frags */
+ 		skb = netdev_alloc_skb_ip_align(rx_ring->netdev,
+ 						IXGBEVF_RX_HDR_SIZE);
+ 		if (unlikely(!skb)) {
+ 			rx_ring->rx_stats.alloc_rx_buff_failed++;
+ 			return NULL;
+ 		}
+ 
+ 		/* we will be copying header into skb->data in
+ 		 * pskb_may_pull so it is in our interest to prefetch
+ 		 * it now to avoid a possible cache miss
+ 		 */
+ 		prefetchw(skb->data);
+ 	}
+ 
+ 	/* we are reusing so sync this buffer for CPU use */
+ 	dma_sync_single_range_for_cpu(rx_ring->dev,
+ 				      rx_buffer->dma,
+ 				      rx_buffer->page_offset,
+ 				      IXGBEVF_RX_BUFSZ,
+ 				      DMA_FROM_DEVICE);
+ 
+ 	/* pull page into skb */
+ 	if (ixgbevf_add_rx_frag(rx_ring, rx_buffer, rx_desc, skb)) {
+ 		/* hand second half of page back to the ring */
+ 		ixgbevf_reuse_rx_page(rx_ring, rx_buffer);
+ 	} else {
+ 		/* we are not reusing the buffer so unmap it */
+ 		dma_unmap_page(rx_ring->dev, rx_buffer->dma,
+ 			       PAGE_SIZE, DMA_FROM_DEVICE);
+ 	}
+ 
+ 	/* clear contents of buffer_info */
+ 	rx_buffer->dma = 0;
+ 	rx_buffer->page = NULL;
+ 
+ 	return skb;
++>>>>>>> a94d9e224e3c (ethernet/intel: Use eth_skb_pad and skb_put_padto helpers)
  }
  
  static inline void ixgbevf_irq_enable_queues(struct ixgbevf_adapter *adapter,
* Unmerged path drivers/net/ethernet/intel/fm10k/fm10k_main.c
diff --git a/drivers/net/ethernet/intel/e1000/e1000_main.c b/drivers/net/ethernet/intel/e1000/e1000_main.c
index 60bb25c97093..c1c243371fef 100644
--- a/drivers/net/ethernet/intel/e1000/e1000_main.c
+++ b/drivers/net/ethernet/intel/e1000/e1000_main.c
@@ -3123,12 +3123,8 @@ static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 	 * packets may get corrupted during padding by HW.
 	 * To WA this issue, pad all small packets manually.
 	 */
-	if (skb->len < ETH_ZLEN) {
-		if (skb_pad(skb, ETH_ZLEN - skb->len))
-			return NETDEV_TX_OK;
-		skb->len = ETH_ZLEN;
-		skb_set_tail_pointer(skb, ETH_ZLEN);
-	}
+	if (eth_skb_pad(skb))
+		return NETDEV_TX_OK;
 
 	mss = skb_shinfo(skb)->gso_size;
 	/* The controller does a simple calculation to
diff --git a/drivers/net/ethernet/intel/e1000e/netdev.c b/drivers/net/ethernet/intel/e1000e/netdev.c
index d2b29b6b6641..6608d49d25c7 100644
--- a/drivers/net/ethernet/intel/e1000e/netdev.c
+++ b/drivers/net/ethernet/intel/e1000e/netdev.c
@@ -5546,12 +5546,8 @@ static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 	/* The minimum packet size with TCTL.PSP set is 17 bytes so
 	 * pad skb in order to meet this minimum size requirement
 	 */
-	if (unlikely(skb->len < 17)) {
-		if (skb_pad(skb, 17 - skb->len))
-			return NETDEV_TX_OK;
-		skb->len = 17;
-		skb_set_tail_pointer(skb, 17);
-	}
+	if (skb_put_padto(skb, 17))
+		return NETDEV_TX_OK;
 
 	mss = skb_shinfo(skb)->gso_size;
 	if (mss) {
* Unmerged path drivers/net/ethernet/intel/fm10k/fm10k_main.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 74ac50f7aa71..9a4a65c6ea74 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -2397,12 +2397,8 @@ netdev_tx_t i40e_lan_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
 	/* hardware can't handle really short frames, hardware padding works
 	 * beyond this point
 	 */
-	if (unlikely(skb->len < I40E_MIN_TX_LEN)) {
-		if (skb_pad(skb, I40E_MIN_TX_LEN - skb->len))
-			return NETDEV_TX_OK;
-		skb->len = I40E_MIN_TX_LEN;
-		skb_set_tail_pointer(skb, I40E_MIN_TX_LEN);
-	}
+	if (skb_put_padto(skb, I40E_MIN_TX_LEN))
+		return NETDEV_TX_OK;
 
 	return i40e_xmit_frame_ring(skb, tx_ring);
 }
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index 15a626c9978a..1702a651e4d6 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -5088,12 +5088,8 @@ static netdev_tx_t igb_xmit_frame(struct sk_buff *skb,
 	/* The minimum packet size with TCTL.PSP set is 17 so pad the skb
 	 * in order to meet this minimum size requirement.
 	 */
-	if (unlikely(skb->len < 17)) {
-		if (skb_pad(skb, 17 - skb->len))
-			return NETDEV_TX_OK;
-		skb->len = 17;
-		skb_set_tail_pointer(skb, 17);
-	}
+	if (skb_put_padto(skb, 17))
+		return NETDEV_TX_OK;
 
 	return igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb));
 }
@@ -6847,14 +6843,9 @@ static bool igb_cleanup_headers(struct igb_ring *rx_ring,
 	if (skb_is_nonlinear(skb))
 		igb_pull_tail(rx_ring, rx_desc, skb);
 
-	/* if skb_pad returns an error the skb was freed */
-	if (unlikely(skb->len < 60)) {
-		int pad_len = 60 - skb->len;
-
-		if (skb_pad(skb, pad_len))
-			return true;
-		__skb_put(skb, pad_len);
-	}
+	/* if eth_skb_pad returns an error the skb was freed */
+	if (eth_skb_pad(skb))
+		return true;
 
 	return false;
 }
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index b0a004972e7a..6a397bc3eab2 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -1860,14 +1860,9 @@ static bool ixgbe_cleanup_headers(struct ixgbe_ring *rx_ring,
 		return false;
 
 #endif
-	/* if skb_pad returns an error the skb was freed */
-	if (unlikely(skb->len < 60)) {
-		int pad_len = 60 - skb->len;
-
-		if (skb_pad(skb, pad_len))
-			return true;
-		__skb_put(skb, pad_len);
-	}
+	/* if eth_skb_pad returns an error the skb was freed */
+	if (eth_skb_pad(skb))
+		return true;
 
 	return false;
 }
@@ -6939,12 +6934,8 @@ static netdev_tx_t ixgbe_xmit_frame(struct sk_buff *skb,
 	 * The minimum packet size for olinfo paylen is 17 so pad the skb
 	 * in order to meet this minimum size requirement.
 	 */
-	if (unlikely(skb->len < 17)) {
-		if (skb_pad(skb, 17 - skb->len))
-			return NETDEV_TX_OK;
-		skb->len = 17;
-		skb_set_tail_pointer(skb, 17);
-	}
+	if (skb_put_padto(skb, 17))
+		return NETDEV_TX_OK;
 
 	tx_ring = adapter->tx_ring[skb->queue_mapping];
 	return ixgbe_xmit_frame_ring(skb, adapter, tx_ring);
* Unmerged path drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
