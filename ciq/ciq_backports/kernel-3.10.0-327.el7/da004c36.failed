powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] iommu: Move tce_xxx callbacks from ppc_md to iommu_table (David Gibson) [1213665]
Rebuild_FUZZ: 93.33%
commit-author Alexey Kardashevskiy <aik@ozlabs.ru>
commit da004c3600f52e4f05017f60970e5010978006bc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/da004c36.failed

This adds a iommu_table_ops struct and puts pointer to it into
the iommu_table struct. This moves tce_build/tce_free/tce_get/tce_flush
callbacks from ppc_md to the new struct where they really belong to.

This adds the requirement for @it_ops to be initialized before calling
iommu_init_table() to make sure that we do not leave any IOMMU table
with iommu_table_ops uninitialized. This is not a parameter of
iommu_init_table() though as there will be cases when iommu_init_table()
will not be called on TCE tables, for example - VFIO.

This does s/tce_build/set/, s/tce_free/clear/ and removes "tce_"
redundant prefixes.

This removes tce_xxx_rm handlers from ppc_md but does not add
them to iommu_table_ops as this will be done later if we decide to
support TCE hypercalls in real mode. This removes _vm callbacks as
only virtual mode is supported by now so this also removes @rm parameter.

For pSeries, this always uses tce_buildmulti_pSeriesLP/
tce_buildmulti_pSeriesLP. This changes multi callback to fall back to
tce_build_pSeriesLP/tce_free_pSeriesLP if FW_FEATURE_MULTITCE is not
present. The reason for this is we still have to support "multitce=off"
boot parameter in disable_multitce() and we do not want to walk through
all IOMMU tables in the system and replace "multi" callbacks with single
ones.

For powernv, this defines _ops per PHB type which are P5IOC2/IODA1/IODA2.
This makes the callbacks for them public. Later patches will extend
callbacks for IODA1/2.

No change in behaviour is expected.

	Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
	Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
	Reviewed-by: Gavin Shan <gwshan@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit da004c3600f52e4f05017f60970e5010978006bc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/include/asm/machdep.h
#	arch/powerpc/platforms/pasemi/iommu.c
#	arch/powerpc/platforms/powernv/pci.c
#	arch/powerpc/platforms/powernv/pci.h
#	arch/powerpc/platforms/pseries/iommu.c
diff --cc arch/powerpc/include/asm/machdep.h
index be90e671e0f6,952579f5e79a..000000000000
--- a/arch/powerpc/include/asm/machdep.h
+++ b/arch/powerpc/include/asm/machdep.h
@@@ -65,19 -65,6 +65,22 @@@ struct machdep_calls 
  	 * destroyed as well */
  	void		(*hpte_clear_all)(void);
  
++<<<<<<< HEAD
 +	int		(*tce_build)(struct iommu_table *tbl,
 +				     long index,
 +				     long npages,
 +				     unsigned long uaddr,
 +				     enum dma_data_direction direction,
 +				     struct dma_attrs *attrs);
 +	void		(*tce_free)(struct iommu_table *tbl,
 +				    long index,
 +				    long npages);
 +	unsigned long	(*tce_get)(struct iommu_table *tbl,
 +				    long index);
 +	void		(*tce_flush)(struct iommu_table *tbl);
 +
++=======
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  	void __iomem *	(*ioremap)(phys_addr_t addr, unsigned long size,
  				   unsigned long flags, void *caller);
  	void		(*iounmap)(volatile void __iomem *token);
diff --cc arch/powerpc/platforms/pasemi/iommu.c
index 2e576f2ae442,c929644e74a6..000000000000
--- a/arch/powerpc/platforms/pasemi/iommu.c
+++ b/arch/powerpc/platforms/pasemi/iommu.c
@@@ -248,10 -255,8 +253,15 @@@ void __init iommu_init_early_pasemi(voi
  
  	iob_init(NULL);
  
++<<<<<<< HEAD
 +	ppc_md.pci_dma_dev_setup = pci_dma_dev_setup_pasemi;
 +	ppc_md.pci_dma_bus_setup = pci_dma_bus_setup_pasemi;
 +	ppc_md.tce_build = iobmap_build;
 +	ppc_md.tce_free  = iobmap_free;
++=======
+ 	pasemi_pci_controller_ops.dma_dev_setup = pci_dma_dev_setup_pasemi;
+ 	pasemi_pci_controller_ops.dma_bus_setup = pci_dma_bus_setup_pasemi;
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  	set_pci_dma_ops(&dma_iommu_ops);
  }
  
diff --cc arch/powerpc/platforms/powernv/pci.c
index 17649771621c,2793b9d576e3..000000000000
--- a/arch/powerpc/platforms/powernv/pci.c
+++ b/arch/powerpc/platforms/powernv/pci.c
@@@ -599,9 -572,9 +599,15 @@@ struct pci_ops pnv_pci_ops = 
  	.write = pnv_pci_write_config,
  };
  
++<<<<<<< HEAD
 +static int pnv_tce_build(struct iommu_table *tbl, long index, long npages,
 +			 unsigned long uaddr, enum dma_data_direction direction,
 +			 struct dma_attrs *attrs)
++=======
+ int pnv_tce_build(struct iommu_table *tbl, long index, long npages,
+ 		unsigned long uaddr, enum dma_data_direction direction,
+ 		struct dma_attrs *attrs)
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  {
  	u64 proto_tce = iommu_direction_to_tce_perm(direction);
  	__be64 *tcep, *tces;
@@@ -619,12 -592,12 +625,20 @@@
  	 * of flags if that becomes the case
  	 */
  	if (tbl->it_type & TCE_PCI_SWINV_CREATE)
++<<<<<<< HEAD
 +		pnv_pci_ioda_tce_invalidate(tbl, tces, tcep - 1);
++=======
+ 		pnv_pci_ioda_tce_invalidate(tbl, tces, tcep - 1, false);
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void pnv_tce_free(struct iommu_table *tbl, long index, long npages)
++=======
+ void pnv_tce_free(struct iommu_table *tbl, long index, long npages)
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  {
  	__be64 *tcep, *tces;
  
@@@ -634,10 -607,10 +648,17 @@@
  		*(tcep++) = cpu_to_be64(0);
  
  	if (tbl->it_type & TCE_PCI_SWINV_FREE)
++<<<<<<< HEAD
 +		pnv_pci_ioda_tce_invalidate(tbl, tces, tcep - 1);
 +}
 +
 +static unsigned long pnv_tce_get(struct iommu_table *tbl, long index)
++=======
+ 		pnv_pci_ioda_tce_invalidate(tbl, tces, tcep - 1, false);
+ }
+ 
+ unsigned long pnv_tce_get(struct iommu_table *tbl, long index)
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  {
  	return ((u64 *)tbl->it_base)[index - tbl->it_offset];
  }
@@@ -816,19 -716,7 +837,22 @@@ void __init pnv_pci_init(void
  	pci_devs_phb_init();
  
  	/* Configure IOMMU DMA hooks */
++<<<<<<< HEAD
 +	ppc_md.pci_dma_dev_setup = pnv_pci_dma_dev_setup;
 +	ppc_md.tce_build = pnv_tce_build;
 +	ppc_md.tce_free = pnv_tce_free;
 +	ppc_md.tce_get = pnv_tce_get;
 +	ppc_md.pci_probe_mode = pnv_pci_probe_mode;
++=======
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  	set_pci_dma_ops(&dma_iommu_ops);
 +
 +	/* Configure MSIs */
 +#ifdef CONFIG_PCI_MSI
 +	ppc_md.msi_check_device = pnv_msi_check_device;
 +	ppc_md.setup_msi_irqs = pnv_setup_msi_irqs;
 +	ppc_md.teardown_msi_irqs = pnv_teardown_msi_irqs;
 +#endif
  }
  
  machine_subsys_initcall_sync(powernv, tce_iommu_bus_notifier_init);
diff --cc arch/powerpc/platforms/powernv/pci.h
index 6092ce3351f9,45a756007ec3..000000000000
--- a/arch/powerpc/platforms/powernv/pci.h
+++ b/arch/powerpc/platforms/powernv/pci.h
@@@ -181,9 -197,11 +181,17 @@@ struct pnv_phb 
  };
  
  extern struct pci_ops pnv_pci_ops;
++<<<<<<< HEAD
 +#ifdef CONFIG_EEH
 +extern struct pnv_eeh_ops ioda_eeh_ops;
 +#endif
++=======
+ extern int pnv_tce_build(struct iommu_table *tbl, long index, long npages,
+ 		unsigned long uaddr, enum dma_data_direction direction,
+ 		struct dma_attrs *attrs);
+ extern void pnv_tce_free(struct iommu_table *tbl, long index, long npages);
+ extern unsigned long pnv_tce_get(struct iommu_table *tbl, long index);
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  
  void pnv_pci_dump_phb_diag_data(struct pci_controller *hose,
  				unsigned char *log_buff);
diff --cc arch/powerpc/platforms/pseries/iommu.c
index 1edda98be625,33f3a855bfd9..000000000000
--- a/arch/powerpc/platforms/pseries/iommu.c
+++ b/arch/powerpc/platforms/pseries/iommu.c
@@@ -687,10 -714,11 +702,11 @@@ static void pci_dma_dev_setup_pSeries(s
  		tbl = kzalloc_node(sizeof(struct iommu_table), GFP_KERNEL,
  				   phb->node);
  		iommu_table_setparms(phb, dn, tbl);
+ 		tbl->it_ops = &iommu_table_pseries_ops;
  		PCI_DN(dn)->iommu_table = iommu_init_table(tbl, phb->node);
  		iommu_register_group(tbl, pci_domain_nr(phb->bus), 0);
 -		set_iommu_table_base(&dev->dev, tbl);
 -		iommu_add_device(&dev->dev);
 +		set_iommu_table_base_and_group(&dev->dev,
 +					       PCI_DN(dn)->iommu_table);
  		return;
  	}
  
@@@ -1294,24 -1332,13 +1311,34 @@@ void iommu_init_early_pSeries(void
  		return;
  
  	if (firmware_has_feature(FW_FEATURE_LPAR)) {
++<<<<<<< HEAD
 +		if (firmware_has_feature(FW_FEATURE_MULTITCE)) {
 +			ppc_md.tce_build = tce_buildmulti_pSeriesLP;
 +			ppc_md.tce_free	 = tce_freemulti_pSeriesLP;
 +		} else {
 +			ppc_md.tce_build = tce_build_pSeriesLP;
 +			ppc_md.tce_free	 = tce_free_pSeriesLP;
 +		}
 +		ppc_md.tce_get   = tce_get_pSeriesLP;
 +		ppc_md.pci_dma_bus_setup = pci_dma_bus_setup_pSeriesLP;
 +		ppc_md.pci_dma_dev_setup = pci_dma_dev_setup_pSeriesLP;
 +		ppc_md.dma_set_mask = dma_set_mask_pSeriesLP;
 +		ppc_md.dma_get_required_mask = dma_get_required_mask_pSeriesLP;
 +	} else {
 +		ppc_md.tce_build = tce_build_pSeries;
 +		ppc_md.tce_free  = tce_free_pSeries;
 +		ppc_md.tce_get   = tce_get_pseries;
 +		ppc_md.pci_dma_bus_setup = pci_dma_bus_setup_pSeries;
 +		ppc_md.pci_dma_dev_setup = pci_dma_dev_setup_pSeries;
++=======
+ 		pseries_pci_controller_ops.dma_bus_setup = pci_dma_bus_setup_pSeriesLP;
+ 		pseries_pci_controller_ops.dma_dev_setup = pci_dma_dev_setup_pSeriesLP;
+ 		ppc_md.dma_set_mask = dma_set_mask_pSeriesLP;
+ 		ppc_md.dma_get_required_mask = dma_get_required_mask_pSeriesLP;
+ 	} else {
+ 		pseries_pci_controller_ops.dma_bus_setup = pci_dma_bus_setup_pSeries;
+ 		pseries_pci_controller_ops.dma_dev_setup = pci_dma_dev_setup_pSeries;
++>>>>>>> da004c3600f5 (powerpc/iommu: Move tce_xxx callbacks from ppc_md to iommu_table)
  	}
  
  
diff --git a/arch/powerpc/include/asm/iommu.h b/arch/powerpc/include/asm/iommu.h
index 2d866433cb3d..a5687c78b0bb 100644
--- a/arch/powerpc/include/asm/iommu.h
+++ b/arch/powerpc/include/asm/iommu.h
@@ -43,6 +43,22 @@
 extern int iommu_is_off;
 extern int iommu_force_on;
 
+struct iommu_table_ops {
+	int (*set)(struct iommu_table *tbl,
+			long index, long npages,
+			unsigned long uaddr,
+			enum dma_data_direction direction,
+			struct dma_attrs *attrs);
+	void (*clear)(struct iommu_table *tbl,
+			long index, long npages);
+	unsigned long (*get)(struct iommu_table *tbl, long index);
+	void (*flush)(struct iommu_table *tbl);
+};
+
+/* These are used by VIO */
+extern struct iommu_table_ops iommu_table_lpar_multi_ops;
+extern struct iommu_table_ops iommu_table_pseries_ops;
+
 /*
  * IOMAP_MAX_ORDER defines the largest contiguous block
  * of dma space we can get.  IOMAP_MAX_ORDER = 13
@@ -77,6 +93,7 @@ struct iommu_table {
 #ifdef CONFIG_IOMMU_API
 	struct iommu_group *it_group;
 #endif
+	struct iommu_table_ops *it_ops;
 	void (*set_bypass)(struct iommu_table *tbl, bool enable);
 };
 
* Unmerged path arch/powerpc/include/asm/machdep.h
diff --git a/arch/powerpc/kernel/iommu.c b/arch/powerpc/kernel/iommu.c
index a8ba9f468d1a..12ff612ba28f 100644
--- a/arch/powerpc/kernel/iommu.c
+++ b/arch/powerpc/kernel/iommu.c
@@ -322,11 +322,11 @@ static dma_addr_t iommu_alloc(struct device *dev, struct iommu_table *tbl,
 	ret = entry << tbl->it_page_shift;	/* Set the return dma address */
 
 	/* Put the TCEs in the HW table */
-	build_fail = ppc_md.tce_build(tbl, entry, npages,
+	build_fail = tbl->it_ops->set(tbl, entry, npages,
 				      (unsigned long)page &
 				      IOMMU_PAGE_MASK(tbl), direction, attrs);
 
-	/* ppc_md.tce_build() only returns non-zero for transient errors.
+	/* tbl->it_ops->set() only returns non-zero for transient errors.
 	 * Clean up the table bitmap in this case and return
 	 * DMA_ERROR_CODE. For all other errors the functionality is
 	 * not altered.
@@ -337,8 +337,8 @@ static dma_addr_t iommu_alloc(struct device *dev, struct iommu_table *tbl,
 	}
 
 	/* Flush/invalidate TLB caches if necessary */
-	if (ppc_md.tce_flush)
-		ppc_md.tce_flush(tbl);
+	if (tbl->it_ops->flush)
+		tbl->it_ops->flush(tbl);
 
 	/* Make sure updates are seen by hardware */
 	mb();
@@ -408,7 +408,7 @@ static void __iommu_free(struct iommu_table *tbl, dma_addr_t dma_addr,
 	if (!iommu_free_check(tbl, dma_addr, npages))
 		return;
 
-	ppc_md.tce_free(tbl, entry, npages);
+	tbl->it_ops->clear(tbl, entry, npages);
 
 	spin_lock_irqsave(&(pool->lock), flags);
 	bitmap_clear(tbl->it_map, free_entry, npages);
@@ -424,8 +424,8 @@ static void iommu_free(struct iommu_table *tbl, dma_addr_t dma_addr,
 	 * not do an mb() here on purpose, it is not needed on any of
 	 * the current platforms.
 	 */
-	if (ppc_md.tce_flush)
-		ppc_md.tce_flush(tbl);
+	if (tbl->it_ops->flush)
+		tbl->it_ops->flush(tbl);
 }
 
 int iommu_map_sg(struct device *dev, struct iommu_table *tbl,
@@ -495,7 +495,7 @@ int iommu_map_sg(struct device *dev, struct iommu_table *tbl,
 			    npages, entry, dma_addr);
 
 		/* Insert into HW table */
-		build_fail = ppc_md.tce_build(tbl, entry, npages,
+		build_fail = tbl->it_ops->set(tbl, entry, npages,
 					      vaddr & IOMMU_PAGE_MASK(tbl),
 					      direction, attrs);
 		if(unlikely(build_fail))
@@ -534,8 +534,8 @@ int iommu_map_sg(struct device *dev, struct iommu_table *tbl,
 	}
 
 	/* Flush/invalidate TLB caches if necessary */
-	if (ppc_md.tce_flush)
-		ppc_md.tce_flush(tbl);
+	if (tbl->it_ops->flush)
+		tbl->it_ops->flush(tbl);
 
 	DBG("mapped %d elements:\n", outcount);
 
@@ -600,8 +600,8 @@ void iommu_unmap_sg(struct iommu_table *tbl, struct scatterlist *sglist,
 	 * do not do an mb() here, the affected platforms do not need it
 	 * when freeing.
 	 */
-	if (ppc_md.tce_flush)
-		ppc_md.tce_flush(tbl);
+	if (tbl->it_ops->flush)
+		tbl->it_ops->flush(tbl);
 }
 
 static void iommu_table_clear(struct iommu_table *tbl)
@@ -613,17 +613,17 @@ static void iommu_table_clear(struct iommu_table *tbl)
 	 */
 	if (!is_kdump_kernel() || is_fadump_active()) {
 		/* Clear the table in case firmware left allocations in it */
-		ppc_md.tce_free(tbl, tbl->it_offset, tbl->it_size);
+		tbl->it_ops->clear(tbl, tbl->it_offset, tbl->it_size);
 		return;
 	}
 
 #ifdef CONFIG_CRASH_DUMP
-	if (ppc_md.tce_get) {
+	if (tbl->it_ops->get) {
 		unsigned long index, tceval, tcecount = 0;
 
 		/* Reserve the existing mappings left by the first kernel. */
 		for (index = 0; index < tbl->it_size; index++) {
-			tceval = ppc_md.tce_get(tbl, index + tbl->it_offset);
+			tceval = tbl->it_ops->get(tbl, index + tbl->it_offset);
 			/*
 			 * Freed TCE entry contains 0x7fffffffffffffff on JS20
 			 */
@@ -657,6 +657,8 @@ struct iommu_table *iommu_init_table(struct iommu_table *tbl, int nid)
 	unsigned int i;
 	struct iommu_pool *p;
 
+	BUG_ON(!tbl->it_ops);
+
 	/* number of bytes needed for the bitmap */
 	sz = BITS_TO_LONGS(tbl->it_size) * sizeof(unsigned long);
 
@@ -936,8 +938,8 @@ EXPORT_SYMBOL_GPL(iommu_tce_direction);
 void iommu_flush_tce(struct iommu_table *tbl)
 {
 	/* Flush/invalidate TLB caches if necessary */
-	if (ppc_md.tce_flush)
-		ppc_md.tce_flush(tbl);
+	if (tbl->it_ops->flush)
+		tbl->it_ops->flush(tbl);
 
 	/* Make sure updates are seen by hardware */
 	mb();
@@ -948,7 +950,7 @@ int iommu_tce_clear_param_check(struct iommu_table *tbl,
 		unsigned long ioba, unsigned long tce_value,
 		unsigned long npages)
 {
-	/* ppc_md.tce_free() does not support any value but 0 */
+	/* tbl->it_ops->clear() does not support any value but 0 */
 	if (tce_value)
 		return -EINVAL;
 
@@ -996,9 +998,9 @@ unsigned long iommu_clear_tce(struct iommu_table *tbl, unsigned long entry)
 
 	spin_lock(&(pool->lock));
 
-	oldtce = ppc_md.tce_get(tbl, entry);
+	oldtce = tbl->it_ops->get(tbl, entry);
 	if (oldtce & (TCE_PCI_WRITE | TCE_PCI_READ))
-		ppc_md.tce_free(tbl, entry, 1);
+		tbl->it_ops->clear(tbl, entry, 1);
 	else
 		oldtce = 0;
 
@@ -1021,10 +1023,10 @@ int iommu_tce_build(struct iommu_table *tbl, unsigned long entry,
 
 	spin_lock(&(pool->lock));
 
-	oldtce = ppc_md.tce_get(tbl, entry);
+	oldtce = tbl->it_ops->get(tbl, entry);
 	/* Add new entry if it is not busy */
 	if (!(oldtce & (TCE_PCI_WRITE | TCE_PCI_READ)))
-		ret = ppc_md.tce_build(tbl, entry, 1, hwaddr, direction, NULL);
+		ret = tbl->it_ops->set(tbl, entry, 1, hwaddr, direction, NULL);
 
 	spin_unlock(&(pool->lock));
 
diff --git a/arch/powerpc/kernel/vio.c b/arch/powerpc/kernel/vio.c
index 93a59229e0f0..724e30c9eeef 100644
--- a/arch/powerpc/kernel/vio.c
+++ b/arch/powerpc/kernel/vio.c
@@ -1190,6 +1190,11 @@ static struct iommu_table *vio_build_iommu_table(struct vio_dev *dev)
 	tbl->it_type = TCE_VB;
 	tbl->it_blocksize = 16;
 
+	if (firmware_has_feature(FW_FEATURE_LPAR))
+		tbl->it_ops = &iommu_table_lpar_multi_ops;
+	else
+		tbl->it_ops = &iommu_table_pseries_ops;
+
 	return iommu_init_table(tbl, -1);
 }
 
diff --git a/arch/powerpc/platforms/cell/iommu.c b/arch/powerpc/platforms/cell/iommu.c
index 2b90ff8a93be..c24cd6d84d28 100644
--- a/arch/powerpc/platforms/cell/iommu.c
+++ b/arch/powerpc/platforms/cell/iommu.c
@@ -465,6 +465,11 @@ static inline u32 cell_iommu_get_ioid(struct device_node *np)
 	return *ioid;
 }
 
+static struct iommu_table_ops cell_iommu_ops = {
+	.set = tce_build_cell,
+	.clear = tce_free_cell
+};
+
 static struct iommu_window * __init
 cell_iommu_setup_window(struct cbe_iommu *iommu, struct device_node *np,
 			unsigned long offset, unsigned long size,
@@ -491,6 +496,7 @@ cell_iommu_setup_window(struct cbe_iommu *iommu, struct device_node *np,
 	window->table.it_offset =
 		(offset >> window->table.it_page_shift) + pte_offset;
 	window->table.it_size = size >> window->table.it_page_shift;
+	window->table.it_ops = &cell_iommu_ops;
 
 	iommu_init_table(&window->table, iommu->nid);
 
@@ -1199,8 +1205,6 @@ static int __init cell_iommu_init(void)
 	/* Setup various ppc_md. callbacks */
 	ppc_md.pci_dma_dev_setup = cell_pci_dma_dev_setup;
 	ppc_md.dma_get_required_mask = cell_dma_get_required_mask;
-	ppc_md.tce_build = tce_build_cell;
-	ppc_md.tce_free = tce_free_cell;
 
 	if (!iommu_fixed_disabled && cell_iommu_fixed_mapping_init() == 0)
 		goto bail;
* Unmerged path arch/powerpc/platforms/pasemi/iommu.c
diff --git a/arch/powerpc/platforms/powernv/pci-ioda.c b/arch/powerpc/platforms/powernv/pci-ioda.c
index 109e90d84e34..42eb7a43c107 100644
--- a/arch/powerpc/platforms/powernv/pci-ioda.c
+++ b/arch/powerpc/platforms/powernv/pci-ioda.c
@@ -628,6 +628,12 @@ static void pnv_pci_ioda1_tce_invalidate(struct iommu_table *tbl,
 	 */
 }
 
+static struct iommu_table_ops pnv_ioda1_iommu_ops = {
+	.set = pnv_tce_build,
+	.clear = pnv_tce_free,
+	.get = pnv_tce_get,
+};
+
 static void pnv_pci_ioda2_tce_invalidate(struct pnv_ioda_pe *pe,
 					 struct iommu_table *tbl,
 					 __be64 *startp, __be64 *endp)
@@ -667,6 +673,12 @@ void pnv_pci_ioda_tce_invalidate(struct iommu_table *tbl,
 		pnv_pci_ioda2_tce_invalidate(pe, tbl, startp, endp);
 }
 
+static struct iommu_table_ops pnv_ioda2_iommu_ops = {
+	.set = pnv_tce_build,
+	.clear = pnv_tce_free,
+	.get = pnv_tce_get,
+};
+
 static void pnv_pci_ioda_setup_dma_pe(struct pnv_phb *phb,
 				      struct pnv_ioda_pe *pe, unsigned int base,
 				      unsigned int segs)
@@ -741,6 +753,7 @@ static void pnv_pci_ioda_setup_dma_pe(struct pnv_phb *phb,
 				 TCE_PCI_SWINV_FREE   |
 				 TCE_PCI_SWINV_PAIR);
 	}
+	tbl->it_ops = &pnv_ioda1_iommu_ops;
 	iommu_init_table(tbl, phb->hose->node);
 	iommu_register_group(tbl, phb->hose->global_number, pe->pe_number);
 
@@ -872,6 +885,7 @@ static void pnv_pci_ioda2_setup_dma_pe(struct pnv_phb *phb,
 		tbl->it_index = (unsigned long)ioremap(be64_to_cpup(swinvp), 8);
 		tbl->it_type |= (TCE_PCI_SWINV_CREATE | TCE_PCI_SWINV_FREE);
 	}
+	tbl->it_ops = &pnv_ioda2_iommu_ops;
 	iommu_init_table(tbl, phb->hose->node);
 	iommu_register_group(tbl, phb->hose->global_number, pe->pe_number);
 
diff --git a/arch/powerpc/platforms/powernv/pci-p5ioc2.c b/arch/powerpc/platforms/powernv/pci-p5ioc2.c
index 94ce3481490b..479c4a3c05b3 100644
--- a/arch/powerpc/platforms/powernv/pci-p5ioc2.c
+++ b/arch/powerpc/platforms/powernv/pci-p5ioc2.c
@@ -83,10 +83,17 @@ static void pnv_pci_init_p5ioc2_msis(struct pnv_phb *phb)
 static void pnv_pci_init_p5ioc2_msis(struct pnv_phb *phb) { }
 #endif /* CONFIG_PCI_MSI */
 
+static struct iommu_table_ops pnv_p5ioc2_iommu_ops = {
+	.set = pnv_tce_build,
+	.clear = pnv_tce_free,
+	.get = pnv_tce_get,
+};
+
 static void pnv_pci_p5ioc2_dma_dev_setup(struct pnv_phb *phb,
 					 struct pci_dev *pdev)
 {
 	if (phb->p5ioc2.iommu_table.it_map == NULL) {
+		phb->p5ioc2.iommu_table.it_ops = &pnv_p5ioc2_iommu_ops;
 		iommu_init_table(&phb->p5ioc2.iommu_table, phb->hose->node);
 		iommu_register_group(&phb->p5ioc2.iommu_table,
 				pci_domain_nr(phb->hose->bus), phb->opal_id);
* Unmerged path arch/powerpc/platforms/powernv/pci.c
* Unmerged path arch/powerpc/platforms/powernv/pci.h
* Unmerged path arch/powerpc/platforms/pseries/iommu.c
diff --git a/arch/powerpc/sysdev/dart_iommu.c b/arch/powerpc/sysdev/dart_iommu.c
index bd968a43a48b..b74aca2d7a52 100644
--- a/arch/powerpc/sysdev/dart_iommu.c
+++ b/arch/powerpc/sysdev/dart_iommu.c
@@ -286,6 +286,12 @@ static int __init dart_init(struct device_node *dart_node)
 	return 0;
 }
 
+static struct iommu_table_ops iommu_dart_ops = {
+	.set = dart_build,
+	.clear = dart_free,
+	.flush = dart_flush,
+};
+
 static void iommu_table_dart_setup(void)
 {
 	iommu_table_dart.it_busno = 0;
@@ -297,6 +303,7 @@ static void iommu_table_dart_setup(void)
 	iommu_table_dart.it_base = (unsigned long)dart_vbase;
 	iommu_table_dart.it_index = 0;
 	iommu_table_dart.it_blocksize = 1;
+	iommu_table_dart.it_ops = &iommu_dart_ops;
 	iommu_init_table(&iommu_table_dart, -1);
 
 	/* Reserve the last page of the DART to avoid possible prefetch
@@ -385,11 +392,6 @@ void __init iommu_init_early_dart(void)
 	if (dart_init(dn) != 0)
 		goto bail;
 
-	/* Setup low level TCE operations for the core IOMMU code */
-	ppc_md.tce_build = dart_build;
-	ppc_md.tce_free  = dart_free;
-	ppc_md.tce_flush = dart_flush;
-
 	/* Setup bypass if supported */
 	if (dart_is_u4)
 		ppc_md.dma_set_mask = dart_dma_set_mask;
