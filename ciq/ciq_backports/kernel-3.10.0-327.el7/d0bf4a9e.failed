net: cleanup and document skb fclone layout

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] cleanup and document skb fclone layout (Florian Westphal) [1151756]
Rebuild_FUZZ: 93.83%
commit-author Eric Dumazet <edumazet@google.com>
commit d0bf4a9e92b9a93ffeeacbd7b6cb83e0ee3dc2ef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/d0bf4a9e.failed

Lets use a proper structure to clearly document and implement
skb fast clones.

Then, we might experiment more easily alternative layouts.

This patch adds a new skb_fclone_busy() helper, used by tcp and xfrm,
to stop leaking of implementation details.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit d0bf4a9e92b9a93ffeeacbd7b6cb83e0ee3dc2ef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/skbuff.h
#	net/core/skbuff.c
#	net/ipv4/tcp_output.c
#	net/xfrm/xfrm_policy.c
diff --cc include/linux/skbuff.h
index 67a799712ba7,d8f7d74d5a4d..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -771,6 -775,37 +771,40 @@@ static inline struct sk_buff *alloc_skb
  	return __alloc_skb(size, priority, 0, NUMA_NO_NODE);
  }
  
++<<<<<<< HEAD
++=======
+ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,
+ 				     unsigned long data_len,
+ 				     int max_page_order,
+ 				     int *errcode,
+ 				     gfp_t gfp_mask);
+ 
+ /* Layout of fast clones : [skb1][skb2][fclone_ref] */
+ struct sk_buff_fclones {
+ 	struct sk_buff	skb1;
+ 
+ 	struct sk_buff	skb2;
+ 
+ 	atomic_t	fclone_ref;
+ };
+ 
+ /**
+  *	skb_fclone_busy - check if fclone is busy
+  *	@skb: buffer
+  *
+  * Returns true is skb is a fast clone, and its clone is not freed.
+  */
+ static inline bool skb_fclone_busy(const struct sk_buff *skb)
+ {
+ 	const struct sk_buff_fclones *fclones;
+ 
+ 	fclones = container_of(skb, struct sk_buff_fclones, skb1);
+ 
+ 	return skb->fclone == SKB_FCLONE_ORIG &&
+ 	       fclones->skb2.fclone == SKB_FCLONE_CLONE;
+ }
+ 
++>>>>>>> d0bf4a9e92b9 (net: cleanup and document skb fclone layout)
  static inline struct sk_buff *alloc_skb_fclone(unsigned int size,
  					       gfp_t priority)
  {
diff --cc net/core/skbuff.c
index af49e6e94c48,a8cebb40699c..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -285,16 -257,16 +285,21 @@@ struct sk_buff *__alloc_skb(unsigned in
  	kmemcheck_annotate_variable(shinfo->destructor_arg);
  
  	if (flags & SKB_ALLOC_FCLONE) {
- 		struct sk_buff *child = skb + 1;
- 		atomic_t *fclone_ref = (atomic_t *) (child + 1);
+ 		struct sk_buff_fclones *fclones;
  
++<<<<<<< HEAD
 +		kmemcheck_annotate_bitfield(child, flags1);
 +		kmemcheck_annotate_bitfield(child, flags2);
++=======
+ 		fclones = container_of(skb, struct sk_buff_fclones, skb1);
+ 
+ 		kmemcheck_annotate_bitfield(&fclones->skb2, flags1);
++>>>>>>> d0bf4a9e92b9 (net: cleanup and document skb fclone layout)
  		skb->fclone = SKB_FCLONE_ORIG;
- 		atomic_set(fclone_ref, 1);
+ 		atomic_set(&fclones->fclone_ref, 1);
  
- 		child->fclone = SKB_FCLONE_UNAVAILABLE;
- 		child->pfmemalloc = pfmemalloc;
+ 		fclones->skb2.fclone = SKB_FCLONE_UNAVAILABLE;
+ 		fclones->skb2.pfmemalloc = pfmemalloc;
  	}
  out:
  	return skb;
diff --cc net/ipv4/tcp_output.c
index c50742a4dcce,8d4eac793700..000000000000
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@@ -2044,6 -2102,22 +2044,25 @@@ bool tcp_schedule_loss_probe(struct soc
  	return true;
  }
  
++<<<<<<< HEAD
++=======
+ /* Thanks to skb fast clones, we can detect if a prior transmit of
+  * a packet is still in a qdisc or driver queue.
+  * In this case, there is very little point doing a retransmit !
+  * Note: This is called from BH context only.
+  */
+ static bool skb_still_in_host_queue(const struct sock *sk,
+ 				    const struct sk_buff *skb)
+ {
+ 	if (unlikely(skb_fclone_busy(skb))) {
+ 		NET_INC_STATS_BH(sock_net(sk),
+ 				 LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES);
+ 		return true;
+ 	}
+ 	return false;
+ }
+ 
++>>>>>>> d0bf4a9e92b9 (net: cleanup and document skb fclone layout)
  /* When probe timeout (PTO) fires, send a new segment if one exists, else
   * retransmit the last segment.
   */
diff --cc net/xfrm/xfrm_policy.c
index f93ccd6902d4,4c4e457e7888..000000000000
--- a/net/xfrm/xfrm_policy.c
+++ b/net/xfrm/xfrm_policy.c
@@@ -1831,7 -1959,13 +1831,17 @@@ static int xdst_queue_output(struct soc
  	unsigned long sched_next;
  	struct dst_entry *dst = skb_dst(skb);
  	struct xfrm_dst *xdst = (struct xfrm_dst *) dst;
++<<<<<<< HEAD
 +	struct xfrm_policy_queue *pq = &xdst->pols[0]->polq;
++=======
+ 	struct xfrm_policy *pol = xdst->pols[0];
+ 	struct xfrm_policy_queue *pq = &pol->polq;
+ 
+ 	if (unlikely(skb_fclone_busy(skb))) {
+ 		kfree_skb(skb);
+ 		return 0;
+ 	}
++>>>>>>> d0bf4a9e92b9 (net: cleanup and document skb fclone layout)
  
  	if (pq->hold_queue.qlen > XFRM_MAX_QUEUE_LEN) {
  		kfree_skb(skb);
* Unmerged path include/linux/skbuff.h
* Unmerged path net/core/skbuff.c
* Unmerged path net/ipv4/tcp_output.c
* Unmerged path net/xfrm/xfrm_policy.c
