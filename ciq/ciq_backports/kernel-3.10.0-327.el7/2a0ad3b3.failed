perf/x86/intel: Use context switch callback to flush LBR stack

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [perf] x86/intel: Use context switch callback to flush LBR stack (Jiri Olsa) [1222189]
Rebuild_FUZZ: 95.80%
commit-author Yan, Zheng <zheng.z.yan@intel.com>
commit 2a0ad3b326a9024ba86dca4028499d31fa0c6c4d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/2a0ad3b3.failed

Previous commit introduces context switch callback, its function
overlaps with the flush branch stack callback. So we can use the
context switch callback to flush LBR stack.

This patch adds code that uses the flush branch callback to
flush the LBR stack when task is being scheduled in. The callback
is enabled only when there are events use the LBR hardware. This
patch also removes all old flush branch stack code.

	Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
	Signed-off-by: Kan Liang <kan.liang@intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Vince Weaver <vincent.weaver@maine.edu>
	Cc: eranian@google.com
	Cc: jolsa@redhat.com
Link: http://lkml.kernel.org/r/1415156173-10035-4-git-send-email-kan.liang@intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 2a0ad3b326a9024ba86dca4028499d31fa0c6c4d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event.c
#	arch/x86/kernel/cpu/perf_event.h
#	kernel/events/core.c
diff --cc arch/x86/kernel/cpu/perf_event.c
index bcdabcaa6d7a,6b1fd26a37cf..000000000000
--- a/arch/x86/kernel/cpu/perf_event.c
+++ b/arch/x86/kernel/cpu/perf_event.c
@@@ -1887,10 -1914,10 +1887,17 @@@ static const struct attribute_group *x8
  	NULL,
  };
  
++<<<<<<< HEAD
 +static void x86_pmu_flush_branch_stack(void)
 +{
 +	if (x86_pmu.flush_branch_stack)
 +		x86_pmu.flush_branch_stack();
++=======
+ static void x86_pmu_sched_task(struct perf_event_context *ctx, bool sched_in)
+ {
+ 	if (x86_pmu.sched_task)
+ 		x86_pmu.sched_task(ctx, sched_in);
++>>>>>>> 2a0ad3b326a9 (perf/x86/intel: Use context switch callback to flush LBR stack)
  }
  
  void perf_check_microcode(void)
@@@ -1919,14 -1949,18 +1926,18 @@@ static struct pmu pmu = 
  	.commit_txn		= x86_pmu_commit_txn,
  
  	.event_idx		= x86_pmu_event_idx,
++<<<<<<< HEAD
 +	.flush_branch_stack	= x86_pmu_flush_branch_stack,
++=======
+ 	.sched_task		= x86_pmu_sched_task,
++>>>>>>> 2a0ad3b326a9 (perf/x86/intel: Use context switch callback to flush LBR stack)
  };
  
 -void arch_perf_update_userpage(struct perf_event *event,
 -			       struct perf_event_mmap_page *userpg, u64 now)
 +void arch_perf_update_userpage(struct perf_event_mmap_page *userpg, u64 now)
  {
 -	struct cyc2ns_data *data;
 -
  	userpg->cap_user_time = 0;
  	userpg->cap_user_time_zero = 0;
 -	userpg->cap_user_rdpmc =
 -		!!(event->hw.flags & PERF_X86_EVENT_RDPMC_ALLOWED);
 +	userpg->cap_user_rdpmc = x86_pmu.attr_rdpmc;
  	userpg->pmc_width = x86_pmu.cntval_bits;
  
  	if (!sched_clock_stable())
diff --cc arch/x86/kernel/cpu/perf_event.h
index ee385313590e,949d0083a29e..000000000000
--- a/arch/x86/kernel/cpu/perf_event.h
+++ b/arch/x86/kernel/cpu/perf_event.h
@@@ -471,7 -472,8 +471,12 @@@ struct x86_pmu 
  	void		(*cpu_dead)(int cpu);
  
  	void		(*check_microcode)(void);
++<<<<<<< HEAD
 +	void		(*flush_branch_stack)(void);
++=======
+ 	void		(*sched_task)(struct perf_event_context *ctx,
+ 				      bool sched_in);
++>>>>>>> 2a0ad3b326a9 (perf/x86/intel: Use context switch callback to flush LBR stack)
  
  	/*
  	 * Intel Arch Perfmon v2+
diff --cc kernel/events/core.c
index ec07afe9af6e,f563ce767f93..000000000000
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@@ -153,7 -153,7 +153,11 @@@ enum event_type_t 
   */
  struct static_key_deferred perf_sched_events __read_mostly;
  static DEFINE_PER_CPU(atomic_t, perf_cgroup_events);
++<<<<<<< HEAD
 +static DEFINE_PER_CPU(atomic_t, perf_branch_stack_events);
++=======
+ static DEFINE_PER_CPU(int, perf_sched_cb_usages);
++>>>>>>> 2a0ad3b326a9 (perf/x86/intel: Use context switch callback to flush LBR stack)
  
  static atomic_t nr_mmap_events __read_mostly;
  static atomic_t nr_comm_events __read_mostly;
@@@ -1245,12 -1239,7 +1249,9 @@@ list_add_event(struct perf_event *event
  	if (is_cgroup_event(event))
  		ctx->nr_cgroups++;
  
- 	if (has_branch_stack(event))
- 		ctx->nr_branch_stack++;
- 
  	list_add_rcu(&event->event_entry, &ctx->event_list);
 +	if (!ctx->nr_events)
 +		perf_pmu_rotate_start(ctx->pmu);
  	ctx->nr_events++;
  	if (event->attr.inherit_stat)
  		ctx->nr_stat++;
@@@ -2758,72 -2799,8 +2756,14 @@@ static void perf_event_context_sched_in
  
  	perf_pmu_enable(ctx->pmu);
  	perf_ctx_unlock(cpuctx, ctx);
 +
 +	/*
 +	 * Since these rotations are per-cpu, we need to ensure the
 +	 * cpu-context we got scheduled on is actually rotating.
 +	 */
 +	perf_pmu_rotate_start(ctx->pmu);
  }
  
- /*
-  * When sampling the branck stack in system-wide, it may be necessary
-  * to flush the stack on context switch. This happens when the branch
-  * stack does not tag its entries with the pid of the current task.
-  * Otherwise it becomes impossible to associate a branch entry with a
-  * task. This ambiguity is more likely to appear when the branch stack
-  * supports priv level filtering and the user sets it to monitor only
-  * at the user level (which could be a useful measurement in system-wide
-  * mode). In that case, the risk is high of having a branch stack with
-  * branch from multiple tasks. Flushing may mean dropping the existing
-  * entries or stashing them somewhere in the PMU specific code layer.
-  *
-  * This function provides the context switch callback to the lower code
-  * layer. It is invoked ONLY when there is at least one system-wide context
-  * with at least one active event using taken branch sampling.
-  */
- static void perf_branch_stack_sched_in(struct task_struct *prev,
- 				       struct task_struct *task)
- {
- 	struct perf_cpu_context *cpuctx;
- 	struct pmu *pmu;
- 	unsigned long flags;
- 
- 	/* no need to flush branch stack if not changing task */
- 	if (prev == task)
- 		return;
- 
- 	local_irq_save(flags);
- 
- 	rcu_read_lock();
- 
- 	list_for_each_entry_rcu(pmu, &pmus, entry) {
- 		cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
- 
- 		/*
- 		 * check if the context has at least one
- 		 * event using PERF_SAMPLE_BRANCH_STACK
- 		 */
- 		if (cpuctx->ctx.nr_branch_stack > 0
- 		    && pmu->flush_branch_stack) {
- 
- 			perf_ctx_lock(cpuctx, cpuctx->task_ctx);
- 
- 			perf_pmu_disable(pmu);
- 
- 			pmu->flush_branch_stack();
- 
- 			perf_pmu_enable(pmu);
- 
- 			perf_ctx_unlock(cpuctx, cpuctx->task_ctx);
- 		}
- 	}
- 
- 	rcu_read_unlock();
- 
- 	local_irq_restore(flags);
- }
- 
  /*
   * Called from scheduler to add the events of the current task
   * with interrupts disabled.
@@@ -2853,12 -2830,11 +2793,17 @@@ void __perf_event_task_sched_in(struct 
  	 * to check if we have to switch in PMU state.
  	 * cgroup event are system-wide mode only
  	 */
 -	if (atomic_read(this_cpu_ptr(&perf_cgroup_events)))
 +	if (atomic_read(&__get_cpu_var(perf_cgroup_events)))
  		perf_cgroup_sched_in(prev, task);
  
++<<<<<<< HEAD
 +	/* check for system-wide branch_stack events */
 +	if (atomic_read(&__get_cpu_var(perf_branch_stack_events)))
 +		perf_branch_stack_sched_in(prev, task);
++=======
+ 	if (__this_cpu_read(perf_sched_cb_usages))
+ 		perf_pmu_sched_task(prev, task, true);
++>>>>>>> 2a0ad3b326a9 (perf/x86/intel: Use context switch callback to flush LBR stack)
  }
  
  static u64 perf_calculate_period(struct perf_event *event, u64 nsec, u64 count)
* Unmerged path arch/x86/kernel/cpu/perf_event.c
* Unmerged path arch/x86/kernel/cpu/perf_event.h
diff --git a/arch/x86/kernel/cpu/perf_event_intel.c b/arch/x86/kernel/cpu/perf_event_intel.c
index 00ea103db3d3..51dc4498e682 100644
--- a/arch/x86/kernel/cpu/perf_event_intel.c
+++ b/arch/x86/kernel/cpu/perf_event_intel.c
@@ -2187,18 +2187,6 @@ static void intel_pmu_cpu_dying(int cpu)
 	fini_debug_store_on_cpu(cpu);
 }
 
-static void intel_pmu_flush_branch_stack(void)
-{
-	/*
-	 * Intel LBR does not tag entries with the
-	 * PID of the current task, then we need to
-	 * flush it on ctxsw
-	 * For now, we simply reset it
-	 */
-	if (x86_pmu.lbr_nr)
-		intel_pmu_lbr_reset();
-}
-
 PMU_FORMAT_ATTR(offcore_rsp, "config1:0-63");
 
 PMU_FORMAT_ATTR(ldlat, "config1:0-15");
@@ -2250,7 +2238,7 @@ static __initconst const struct x86_pmu intel_pmu = {
 	.cpu_starting		= intel_pmu_cpu_starting,
 	.cpu_dying		= intel_pmu_cpu_dying,
 	.guest_get_msrs		= intel_guest_get_msrs,
-	.flush_branch_stack	= intel_pmu_flush_branch_stack,
+	.sched_task		= intel_pmu_lbr_sched_task,
 };
 
 static __init void intel_clovertown_quirk(void)
diff --git a/arch/x86/kernel/cpu/perf_event_intel_lbr.c b/arch/x86/kernel/cpu/perf_event_intel_lbr.c
index 5d6c8762a21d..836264511691 100644
--- a/arch/x86/kernel/cpu/perf_event_intel_lbr.c
+++ b/arch/x86/kernel/cpu/perf_event_intel_lbr.c
@@ -177,6 +177,31 @@ void intel_pmu_lbr_reset(void)
 		intel_pmu_lbr_reset_64();
 }
 
+void intel_pmu_lbr_sched_task(struct perf_event_context *ctx, bool sched_in)
+{
+	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+
+	if (!x86_pmu.lbr_nr)
+		return;
+
+	/*
+	 * When sampling the branck stack in system-wide, it may be
+	 * necessary to flush the stack on context switch. This happens
+	 * when the branch stack does not tag its entries with the pid
+	 * of the current task. Otherwise it becomes impossible to
+	 * associate a branch entry with a task. This ambiguity is more
+	 * likely to appear when the branch stack supports priv level
+	 * filtering and the user sets it to monitor only at the user
+	 * level (which could be a useful measurement in system-wide
+	 * mode). In that case, the risk is high of having a branch
+	 * stack with branch from multiple tasks.
+ 	 */
+	if (sched_in) {
+		intel_pmu_lbr_reset();
+		cpuc->lbr_context = ctx;
+	}
+}
+
 void intel_pmu_lbr_enable(struct perf_event *event)
 {
 	struct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);
@@ -195,6 +220,7 @@ void intel_pmu_lbr_enable(struct perf_event *event)
 	cpuc->br_sel = event->hw.branch_reg.reg;
 
 	cpuc->lbr_users++;
+	perf_sched_cb_inc(event->ctx->pmu);
 }
 
 void intel_pmu_lbr_disable(struct perf_event *event)
@@ -206,6 +232,7 @@ void intel_pmu_lbr_disable(struct perf_event *event)
 
 	cpuc->lbr_users--;
 	WARN_ON_ONCE(cpuc->lbr_users < 0);
+	perf_sched_cb_dec(event->ctx->pmu);
 
 	if (cpuc->enabled && !cpuc->lbr_users) {
 		__intel_pmu_lbr_disable();
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 4d40d5541619..0cd20bd0d636 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -523,7 +523,6 @@ struct perf_event_context {
 	u64				generation;
 	int				pin_count;
 	int				nr_cgroups;	 /* cgroup evts */
-	int				nr_branch_stack; /* branch_stack evt */
 	struct rcu_head			rcu_head;
 
 	struct delayed_work		orphans_remove;
* Unmerged path kernel/events/core.c
