crypto: qat - add support for cbc(aes) ablkcipher

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [crypto] qat - add support for cbc(aes) ablkcipher (Nikolay Aleksandrov) [1173791]
Rebuild_FUZZ: 91.11%
commit-author Tadeusz Struk <tadeusz.struk@intel.com>
commit 338e84f3a9740ab3582c8b6bc5a1a027794dac72
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/338e84f3.failed

Add support for cbc(aes) ablkcipher.

	Signed-off-by: Tadeusz Struk <tadeusz.struk@intel.com>
	Acked-by: Bruce W. Allan <bruce.w.allan@intel.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit 338e84f3a9740ab3582c8b6bc5a1a027794dac72)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/qat/qat_common/qat_algs.c
diff --cc drivers/crypto/qat/qat_common/qat_algs.c
index 5d93760fdd84,f32d0a58bcc0..000000000000
--- a/drivers/crypto/qat/qat_common/qat_algs.c
+++ b/drivers/crypto/qat/qat_common/qat_algs.c
@@@ -492,12 -582,10 +580,19 @@@ static int qat_alg_aead_setkey(struct c
  	if (ctx->enc_cd) {
  		/* rekeying */
  		dev = &GET_DEV(ctx->inst->accel_dev);
++<<<<<<< HEAD
 +		memset(ctx->enc_cd, 0, sizeof(struct qat_alg_cd));
 +		memset(ctx->dec_cd, 0, sizeof(struct qat_alg_cd));
 +		memset(&ctx->enc_fw_req_tmpl, 0,
 +		       sizeof(struct icp_qat_fw_la_bulk_req));
 +		memset(&ctx->dec_fw_req_tmpl, 0,
 +		       sizeof(struct icp_qat_fw_la_bulk_req));
++=======
+ 		memzero_explicit(ctx->enc_cd, sizeof(*ctx->enc_cd));
+ 		memzero_explicit(ctx->dec_cd, sizeof(*ctx->dec_cd));
+ 		memzero_explicit(&ctx->enc_fw_req, sizeof(ctx->enc_fw_req));
+ 		memzero_explicit(&ctx->dec_fw_req, sizeof(ctx->dec_fw_req));
++>>>>>>> 338e84f3a974 (crypto: qat - add support for cbc(aes) ablkcipher)
  	} else {
  		/* new key */
  		int node = get_current_node();
@@@ -843,15 -955,153 +960,153 @@@ static int qat_alg_aead_genivenc(struc
  	seq = cpu_to_be64(req->seq);
  	memcpy(req->giv + AES_BLOCK_SIZE - sizeof(uint64_t),
  	       &seq, sizeof(uint64_t));
- 	return qat_alg_enc_internal(&req->areq, req->giv, 1);
+ 	return qat_alg_aead_enc_internal(&req->areq, req->giv, 1);
  }
  
- static int qat_alg_init(struct crypto_tfm *tfm,
- 			enum icp_qat_hw_auth_algo hash, const char *hash_name)
+ static int qat_alg_ablkcipher_setkey(struct crypto_ablkcipher *tfm,
+ 				     const uint8_t *key,
+ 				     unsigned int keylen)
  {
- 	struct qat_alg_session_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct qat_alg_ablkcipher_ctx *ctx = crypto_ablkcipher_ctx(tfm);
+ 	struct device *dev;
+ 
+ 	spin_lock(&ctx->lock);
+ 	if (ctx->enc_cd) {
+ 		/* rekeying */
+ 		dev = &GET_DEV(ctx->inst->accel_dev);
+ 		memzero_explicit(ctx->enc_cd, sizeof(*ctx->enc_cd));
+ 		memzero_explicit(ctx->dec_cd, sizeof(*ctx->dec_cd));
+ 		memzero_explicit(&ctx->enc_fw_req, sizeof(ctx->enc_fw_req));
+ 		memzero_explicit(&ctx->dec_fw_req, sizeof(ctx->dec_fw_req));
+ 	} else {
+ 		/* new key */
+ 		int node = get_current_node();
+ 		struct qat_crypto_instance *inst =
+ 				qat_crypto_get_instance_node(node);
+ 		if (!inst) {
+ 			spin_unlock(&ctx->lock);
+ 			return -EINVAL;
+ 		}
+ 
+ 		dev = &GET_DEV(inst->accel_dev);
+ 		ctx->inst = inst;
+ 		ctx->enc_cd = dma_zalloc_coherent(dev, sizeof(*ctx->enc_cd),
+ 						  &ctx->enc_cd_paddr,
+ 						  GFP_ATOMIC);
+ 		if (!ctx->enc_cd) {
+ 			spin_unlock(&ctx->lock);
+ 			return -ENOMEM;
+ 		}
+ 		ctx->dec_cd = dma_zalloc_coherent(dev, sizeof(*ctx->dec_cd),
+ 						  &ctx->dec_cd_paddr,
+ 						  GFP_ATOMIC);
+ 		if (!ctx->dec_cd) {
+ 			spin_unlock(&ctx->lock);
+ 			goto out_free_enc;
+ 		}
+ 	}
+ 	spin_unlock(&ctx->lock);
+ 	if (qat_alg_ablkcipher_init_sessions(ctx, key, keylen))
+ 		goto out_free_all;
+ 
+ 	return 0;
+ 
+ out_free_all:
+ 	memzero_explicit(ctx->dec_cd, sizeof(*ctx->enc_cd));
+ 	dma_free_coherent(dev, sizeof(*ctx->enc_cd),
+ 			  ctx->dec_cd, ctx->dec_cd_paddr);
+ 	ctx->dec_cd = NULL;
+ out_free_enc:
+ 	memzero_explicit(ctx->enc_cd, sizeof(*ctx->dec_cd));
+ 	dma_free_coherent(dev, sizeof(*ctx->dec_cd),
+ 			  ctx->enc_cd, ctx->enc_cd_paddr);
+ 	ctx->enc_cd = NULL;
+ 	return -ENOMEM;
+ }
+ 
+ static int qat_alg_ablkcipher_encrypt(struct ablkcipher_request *req)
+ {
+ 	struct crypto_ablkcipher *atfm = crypto_ablkcipher_reqtfm(req);
+ 	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(atfm);
+ 	struct qat_alg_ablkcipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct qat_crypto_request *qat_req = ablkcipher_request_ctx(req);
+ 	struct icp_qat_fw_la_cipher_req_params *cipher_param;
+ 	struct icp_qat_fw_la_bulk_req *msg;
+ 	int ret, ctr = 0;
+ 
+ 	ret = qat_alg_sgl_to_bufl(ctx->inst, NULL, req->src, req->dst,
+ 				  NULL, 0, qat_req);
+ 	if (unlikely(ret))
+ 		return ret;
+ 
+ 	msg = &qat_req->req;
+ 	*msg = ctx->enc_fw_req;
+ 	qat_req->ablkcipher_ctx = ctx;
+ 	qat_req->ablkcipher_req = req;
+ 	qat_req->cb = qat_ablkcipher_alg_callback;
+ 	qat_req->req.comn_mid.opaque_data = (uint64_t)(__force long)qat_req;
+ 	qat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;
+ 	qat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;
+ 	cipher_param = (void *)&qat_req->req.serv_specif_rqpars;
+ 	cipher_param->cipher_length = req->nbytes;
+ 	cipher_param->cipher_offset = 0;
+ 	memcpy(cipher_param->u.cipher_IV_array, req->info, AES_BLOCK_SIZE);
+ 	do {
+ 		ret = adf_send_message(ctx->inst->sym_tx, (uint32_t *)msg);
+ 	} while (ret == -EAGAIN && ctr++ < 10);
+ 
+ 	if (ret == -EAGAIN) {
+ 		qat_alg_free_bufl(ctx->inst, qat_req);
+ 		return -EBUSY;
+ 	}
+ 	return -EINPROGRESS;
+ }
+ 
+ static int qat_alg_ablkcipher_decrypt(struct ablkcipher_request *req)
+ {
+ 	struct crypto_ablkcipher *atfm = crypto_ablkcipher_reqtfm(req);
+ 	struct crypto_tfm *tfm = crypto_ablkcipher_tfm(atfm);
+ 	struct qat_alg_ablkcipher_ctx *ctx = crypto_tfm_ctx(tfm);
+ 	struct qat_crypto_request *qat_req = ablkcipher_request_ctx(req);
+ 	struct icp_qat_fw_la_cipher_req_params *cipher_param;
+ 	struct icp_qat_fw_la_bulk_req *msg;
+ 	int ret, ctr = 0;
+ 
+ 	ret = qat_alg_sgl_to_bufl(ctx->inst, NULL, req->src, req->dst,
+ 				  NULL, 0, qat_req);
+ 	if (unlikely(ret))
+ 		return ret;
+ 
+ 	msg = &qat_req->req;
+ 	*msg = ctx->dec_fw_req;
+ 	qat_req->ablkcipher_ctx = ctx;
+ 	qat_req->ablkcipher_req = req;
+ 	qat_req->cb = qat_ablkcipher_alg_callback;
+ 	qat_req->req.comn_mid.opaque_data = (uint64_t)(__force long)qat_req;
+ 	qat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;
+ 	qat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;
+ 	cipher_param = (void *)&qat_req->req.serv_specif_rqpars;
+ 	cipher_param->cipher_length = req->nbytes;
+ 	cipher_param->cipher_offset = 0;
+ 	memcpy(cipher_param->u.cipher_IV_array, req->info, AES_BLOCK_SIZE);
+ 	do {
+ 		ret = adf_send_message(ctx->inst->sym_tx, (uint32_t *)msg);
+ 	} while (ret == -EAGAIN && ctr++ < 10);
+ 
+ 	if (ret == -EAGAIN) {
+ 		qat_alg_free_bufl(ctx->inst, qat_req);
+ 		return -EBUSY;
+ 	}
+ 	return -EINPROGRESS;
+ }
+ 
+ static int qat_alg_aead_init(struct crypto_tfm *tfm,
+ 			     enum icp_qat_hw_auth_algo hash,
+ 			     const char *hash_name)
+ {
+ 	struct qat_alg_aead_ctx *ctx = crypto_tfm_ctx(tfm);
  
 -	memzero_explicit(ctx, sizeof(*ctx));
 +	memset(ctx, '\0', sizeof(*ctx));
  	ctx->hash_tfm = crypto_alloc_shash(hash_name, 0, 0);
  	if (IS_ERR(ctx->hash_tfm))
  		return -EFAULT;
diff --git a/drivers/crypto/qat/qat_common/icp_qat_hw.h b/drivers/crypto/qat/qat_common/icp_qat_hw.h
index 5031f8c10d75..68f191b653b0 100644
--- a/drivers/crypto/qat/qat_common/icp_qat_hw.h
+++ b/drivers/crypto/qat/qat_common/icp_qat_hw.h
@@ -301,5 +301,5 @@ struct icp_qat_hw_cipher_aes256_f8 {
 
 struct icp_qat_hw_cipher_algo_blk {
 	struct icp_qat_hw_cipher_aes256_f8 aes;
-};
+} __aligned(64);
 #endif
* Unmerged path drivers/crypto/qat/qat_common/qat_algs.c
diff --git a/drivers/crypto/qat/qat_common/qat_crypto.h b/drivers/crypto/qat/qat_common/qat_crypto.h
index fcb323116e60..d503007b49e6 100644
--- a/drivers/crypto/qat/qat_common/qat_crypto.h
+++ b/drivers/crypto/qat/qat_common/qat_crypto.h
@@ -75,10 +75,21 @@ struct qat_crypto_request_buffs {
 	size_t sz_out;
 };
 
+struct qat_crypto_request;
+
 struct qat_crypto_request {
 	struct icp_qat_fw_la_bulk_req req;
-	struct qat_alg_session_ctx *ctx;
-	struct aead_request *areq;
+	union {
+		struct qat_alg_aead_ctx *aead_ctx;
+		struct qat_alg_ablkcipher_ctx *ablkcipher_ctx;
+	};
+	union {
+		struct aead_request *aead_req;
+		struct ablkcipher_request *ablkcipher_req;
+	};
 	struct qat_crypto_request_buffs buf;
+	void (*cb)(struct icp_qat_fw_la_resp *resp,
+		   struct qat_crypto_request *req);
 };
+
 #endif
