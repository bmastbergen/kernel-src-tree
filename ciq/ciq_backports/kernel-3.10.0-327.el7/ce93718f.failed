net: Don't keep around original SKB when we software segment GSO frames.

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] Don't keep around original SKB when we software segment GSO frames (Alexander Duyck) [1205266]
Rebuild_FUZZ: 95.65%
commit-author David S. Miller <davem@davemloft.net>
commit ce93718fb7cdbc064c3000ff59e4d3200bdfa744
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/ce93718f.failed

Just maintain the list properly by returning the head of the remaining
SKB list from dev_hard_start_xmit().

	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ce93718fb7cdbc064c3000ff59e4d3200bdfa744)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/netdevice.h
#	net/core/dev.c
diff --cc include/linux/netdevice.h
index dba59a041ff6,202c25a9aadf..000000000000
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@@ -2471,37 -2805,34 +2471,63 @@@ static inline void napi_free_frags(stru
  	napi->skb = NULL;
  }
  
 -int netdev_rx_handler_register(struct net_device *dev,
 -			       rx_handler_func_t *rx_handler,
 -			       void *rx_handler_data);
 -void netdev_rx_handler_unregister(struct net_device *dev);
 -
 +extern int netdev_rx_handler_register(struct net_device *dev,
 +				      rx_handler_func_t *rx_handler,
 +				      void *rx_handler_data);
 +extern void netdev_rx_handler_unregister(struct net_device *dev);
 +
++<<<<<<< HEAD
 +extern bool		dev_valid_name(const char *name);
 +extern int		dev_ioctl(struct net *net, unsigned int cmd, void __user *);
 +extern int		dev_ethtool(struct net *net, struct ifreq *);
 +extern unsigned int	dev_get_flags(const struct net_device *);
 +extern int		__dev_change_flags(struct net_device *, unsigned int flags);
 +extern int		dev_change_flags(struct net_device *, unsigned int);
 +void			__dev_notify_flags(struct net_device *,
 +					   unsigned int old_flags,
 +					   unsigned int gchanges);
 +extern int		dev_change_name(struct net_device *, const char *);
 +extern int		dev_set_alias(struct net_device *, const char *, size_t);
 +extern int		dev_change_net_namespace(struct net_device *,
 +						 struct net *, const char *);
 +extern int		dev_set_mtu(struct net_device *, int);
 +extern void		dev_set_group(struct net_device *, int);
 +extern int		dev_set_mac_address(struct net_device *,
 +					    struct sockaddr *);
 +extern int		dev_change_carrier(struct net_device *,
 +					   bool new_carrier);
 +extern int		dev_get_phys_port_id(struct net_device *dev,
 +					     struct netdev_phys_port_id *ppid);
 +extern int		dev_hard_start_xmit(struct sk_buff *skb,
 +					    struct net_device *dev,
 +					    struct netdev_queue *txq);
 +extern int		dev_forward_skb(struct net_device *dev,
 +					struct sk_buff *skb);
++=======
+ bool dev_valid_name(const char *name);
+ int dev_ioctl(struct net *net, unsigned int cmd, void __user *);
+ int dev_ethtool(struct net *net, struct ifreq *);
+ unsigned int dev_get_flags(const struct net_device *);
+ int __dev_change_flags(struct net_device *, unsigned int flags);
+ int dev_change_flags(struct net_device *, unsigned int);
+ void __dev_notify_flags(struct net_device *, unsigned int old_flags,
+ 			unsigned int gchanges);
+ int dev_change_name(struct net_device *, const char *);
+ int dev_set_alias(struct net_device *, const char *, size_t);
+ int dev_change_net_namespace(struct net_device *, struct net *, const char *);
+ int dev_set_mtu(struct net_device *, int);
+ void dev_set_group(struct net_device *, int);
+ int dev_set_mac_address(struct net_device *, struct sockaddr *);
+ int dev_change_carrier(struct net_device *, bool new_carrier);
+ int dev_get_phys_port_id(struct net_device *dev,
+ 			 struct netdev_phys_port_id *ppid);
+ struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev);
+ struct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
+ 				    struct netdev_queue *txq, int *ret);
+ int __dev_forward_skb(struct net_device *dev, struct sk_buff *skb);
+ int dev_forward_skb(struct net_device *dev, struct sk_buff *skb);
+ bool is_skb_forwardable(struct net_device *dev, struct sk_buff *skb);
++>>>>>>> ce93718fb7cd (net: Don't keep around original SKB when we software segment GSO frames.)
  
  extern int		netdev_budget;
  
diff --cc net/core/dev.c
index 1924c9647d47,c89da4f306b1..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -2399,51 -2485,27 +2399,75 @@@ static int illegal_highdma(struct net_d
  	return 0;
  }
  
++<<<<<<< HEAD
 +struct dev_gso_cb {
 +	void (*destructor)(struct sk_buff *skb);
 +};
 +
 +#define DEV_GSO_CB(skb) ((struct dev_gso_cb *)(skb)->cb)
 +
 +static void dev_gso_skb_destructor(struct sk_buff *skb)
 +{
 +	struct dev_gso_cb *cb;
 +
 +	kfree_skb_list(skb->next);
 +	skb->next = NULL;
 +
 +	cb = DEV_GSO_CB(skb);
 +	if (cb->destructor)
 +		cb->destructor(skb);
 +}
 +
 +/**
 + *	dev_gso_segment - Perform emulated hardware segmentation on skb.
 + *	@skb: buffer to segment
 + *	@features: device features as applicable to this skb
 + *
 + *	This function segments the given skb and stores the list of segments
 + *	in skb->next.
 + */
 +static int dev_gso_segment(struct sk_buff *skb, netdev_features_t features)
 +{
 +	struct sk_buff *segs;
 +
 +	segs = skb_gso_segment(skb, features);
 +
 +	/* Verifying header integrity only. */
 +	if (!segs)
 +		return 0;
 +
 +	if (IS_ERR(segs))
 +		return PTR_ERR(segs);
 +
 +	skb->next = segs;
 +	DEV_GSO_CB(skb)->destructor = skb->destructor;
 +	skb->destructor = dev_gso_skb_destructor;
 +
 +	return 0;
 +}
++=======
+ /* If MPLS offload request, verify we are testing hardware MPLS features
+  * instead of standard features for the netdev.
+  */
+ #ifdef CONFIG_NET_MPLS_GSO
+ static netdev_features_t net_mpls_features(struct sk_buff *skb,
+ 					   netdev_features_t features,
+ 					   __be16 type)
+ {
+ 	if (type == htons(ETH_P_MPLS_UC) || type == htons(ETH_P_MPLS_MC))
+ 		features &= skb->dev->mpls_features;
+ 
+ 	return features;
+ }
+ #else
+ static netdev_features_t net_mpls_features(struct sk_buff *skb,
+ 					   netdev_features_t features,
+ 					   __be16 type)
+ {
+ 	return features;
+ }
+ #endif
++>>>>>>> ce93718fb7cd (net: Don't keep around original SKB when we software segment GSO frames.)
  
  static netdev_features_t harmonize_features(struct sk_buff *skb,
  	netdev_features_t features)
@@@ -2485,115 -2553,135 +2509,247 @@@ netdev_features_t netif_skb_features(st
  }
  EXPORT_SYMBOL(netif_skb_features);
  
++<<<<<<< HEAD
 +int dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
 +			struct netdev_queue *txq)
 +{
 +	const struct net_device_ops *ops = dev->netdev_ops;
 +	int rc = NETDEV_TX_OK;
 +	unsigned int skb_len;
 +
 +	if (likely(!skb->next)) {
 +		netdev_features_t features;
 +
 +		/*
 +		 * If device doesn't need skb->dst, release it right now while
 +		 * its hot in this cpu cache
 +		 */
 +		if (dev->priv_flags & IFF_XMIT_DST_RELEASE)
 +			skb_dst_drop(skb);
 +
 +		features = netif_skb_features(skb);
 +
 +		if (vlan_tx_tag_present(skb) &&
 +		    !vlan_hw_offload_capable(features, skb->vlan_proto)) {
 +			skb = __vlan_put_tag(skb, skb->vlan_proto,
 +					     vlan_tx_tag_get(skb));
 +			if (unlikely(!skb))
 +				goto out;
 +
 +			skb->vlan_tci = 0;
 +		}
 +
 +		/* If encapsulation offload request, verify we are testing
 +		 * hardware encapsulation features instead of standard
 +		 * features for the netdev
 +		 */
 +		if (skb->encapsulation)
 +			features &= dev->hw_enc_features;
 +
 +		if (netif_needs_gso(skb, features)) {
 +			if (unlikely(dev_gso_segment(skb, features)))
 +				goto out_kfree_skb;
 +			if (skb->next)
 +				goto gso;
 +		} else {
 +			if (skb_needs_linearize(skb, features) &&
 +			    __skb_linearize(skb))
 +				goto out_kfree_skb;
 +
 +			/* If packet is not checksummed and device does not
 +			 * support checksumming for this protocol, complete
 +			 * checksumming here.
 +			 */
 +			if (skb->ip_summed == CHECKSUM_PARTIAL) {
 +				if (skb->encapsulation)
 +					skb_set_inner_transport_header(skb,
 +						skb_checksum_start_offset(skb));
 +				else
 +					skb_set_transport_header(skb,
 +						skb_checksum_start_offset(skb));
 +				if (!(features & NETIF_F_ALL_CSUM) &&
 +				     skb_checksum_help(skb))
 +					goto out_kfree_skb;
 +			}
 +		}
 +
 +		if (!list_empty(&ptype_all))
 +			dev_queue_xmit_nit(skb, dev);
 +
 +		skb_len = skb->len;
 +		rc = ops->ndo_start_xmit(skb, dev);
 +		trace_net_dev_xmit(skb, rc, dev, skb_len);
 +		if (rc == NETDEV_TX_OK)
 +			txq_trans_update(txq);
 +		return rc;
 +	}
 +
 +gso:
 +	do {
 +		struct sk_buff *nskb = skb->next;
 +
 +		skb->next = nskb->next;
 +		nskb->next = NULL;
 +
 +		if (!list_empty(&ptype_all))
 +			dev_queue_xmit_nit(nskb, dev);
 +
 +		skb_len = nskb->len;
 +		rc = ops->ndo_start_xmit(nskb, dev);
 +		trace_net_dev_xmit(nskb, rc, dev, skb_len);
 +		if (unlikely(rc != NETDEV_TX_OK)) {
 +			if (rc & ~NETDEV_TX_MASK)
 +				goto out_kfree_gso_skb;
 +			nskb->next = skb->next;
 +			skb->next = nskb;
 +			return rc;
 +		}
 +		txq_trans_update(txq);
 +		if (unlikely(netif_xmit_stopped(txq) && skb->next))
 +			return NETDEV_TX_BUSY;
 +	} while (skb->next);
 +
 +out_kfree_gso_skb:
 +	if (likely(skb->next == NULL)) {
 +		skb->destructor = DEV_GSO_CB(skb)->destructor;
 +		consume_skb(skb);
 +		return rc;
 +	}
 +out_kfree_skb:
 +	kfree_skb(skb);
 +out:
 +	return rc;
++=======
+ static int xmit_one(struct sk_buff *skb, struct net_device *dev,
+ 		    struct netdev_queue *txq, bool more)
+ {
+ 	unsigned int len;
+ 	int rc;
+ 
+ 	if (!list_empty(&ptype_all))
+ 		dev_queue_xmit_nit(skb, dev);
+ 
+ 	len = skb->len;
+ 	trace_net_dev_start_xmit(skb, dev);
+ 	rc = netdev_start_xmit(skb, dev, txq, more);
+ 	trace_net_dev_xmit(skb, rc, dev, len);
+ 
+ 	return rc;
+ }
+ 
+ static struct sk_buff *xmit_list(struct sk_buff *first, struct net_device *dev,
+ 				 struct netdev_queue *txq, int *ret)
+ {
+ 	struct sk_buff *skb = first;
+ 	int rc = NETDEV_TX_OK;
+ 
+ 	while (skb) {
+ 		struct sk_buff *next = skb->next;
+ 
+ 		skb->next = NULL;
+ 		rc = xmit_one(skb, dev, txq, next != NULL);
+ 		if (unlikely(!dev_xmit_complete(rc))) {
+ 			skb->next = next;
+ 			goto out;
+ 		}
+ 
+ 		skb = next;
+ 		if (netif_xmit_stopped(txq) && skb) {
+ 			rc = NETDEV_TX_BUSY;
+ 			break;
+ 		}
+ 	}
+ 
+ out:
+ 	*ret = rc;
+ 	return skb;
+ }
+ 
+ struct sk_buff *validate_xmit_vlan(struct sk_buff *skb, netdev_features_t features)
+ {
+ 	if (vlan_tx_tag_present(skb) &&
+ 	    !vlan_hw_offload_capable(features, skb->vlan_proto)) {
+ 		skb = __vlan_put_tag(skb, skb->vlan_proto,
+ 				     vlan_tx_tag_get(skb));
+ 		if (skb)
+ 			skb->vlan_tci = 0;
+ 	}
+ 	return skb;
+ }
+ 
+ struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	netdev_features_t features;
+ 
+ 	if (skb->next)
+ 		return skb;
+ 
+ 	/* If device doesn't need skb->dst, release it right now while
+ 	 * its hot in this cpu cache
+ 	 */
+ 	if (dev->priv_flags & IFF_XMIT_DST_RELEASE)
+ 		skb_dst_drop(skb);
+ 
+ 	features = netif_skb_features(skb);
+ 	skb = validate_xmit_vlan(skb, features);
+ 	if (unlikely(!skb))
+ 		goto out_null;
+ 
+ 	/* If encapsulation offload request, verify we are testing
+ 	 * hardware encapsulation features instead of standard
+ 	 * features for the netdev
+ 	 */
+ 	if (skb->encapsulation)
+ 		features &= dev->hw_enc_features;
+ 
+ 	if (netif_needs_gso(skb, features)) {
+ 		struct sk_buff *segs;
+ 
+ 		segs = skb_gso_segment(skb, features);
+ 		kfree_skb(skb);
+ 		if (IS_ERR(segs))
+ 			segs = NULL;
+ 		skb = segs;
+ 	} else {
+ 		if (skb_needs_linearize(skb, features) &&
+ 		    __skb_linearize(skb))
+ 			goto out_kfree_skb;
+ 
+ 		/* If packet is not checksummed and device does not
+ 		 * support checksumming for this protocol, complete
+ 		 * checksumming here.
+ 		 */
+ 		if (skb->ip_summed == CHECKSUM_PARTIAL) {
+ 			if (skb->encapsulation)
+ 				skb_set_inner_transport_header(skb,
+ 							       skb_checksum_start_offset(skb));
+ 			else
+ 				skb_set_transport_header(skb,
+ 							 skb_checksum_start_offset(skb));
+ 			if (!(features & NETIF_F_ALL_CSUM) &&
+ 			    skb_checksum_help(skb))
+ 				goto out_kfree_skb;
+ 		}
+ 	}
+ 
+ 	return skb;
+ 
+ out_kfree_skb:
+ 	kfree_skb(skb);
+ out_null:
+ 	return NULL;
+ }
+ 
+ struct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,
+ 				    struct netdev_queue *txq, int *ret)
+ {
+ 	if (likely(!skb->next)) {
+ 		*ret = xmit_one(skb, dev, txq, false);
+ 		return skb;
+ 	}
+ 
+ 	return xmit_list(skb, dev, txq, ret);
++>>>>>>> ce93718fb7cd (net: Don't keep around original SKB when we software segment GSO frames.)
  }
  
  static void qdisc_pkt_len_init(struct sk_buff *skb)
* Unmerged path include/linux/netdevice.h
* Unmerged path net/core/dev.c
diff --git a/net/sched/sch_generic.c b/net/sched/sch_generic.c
index 7fbf6a59446f..c249314dbf61 100644
--- a/net/sched/sch_generic.c
+++ b/net/sched/sch_generic.c
@@ -125,7 +125,7 @@ int sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,
 
 	HARD_TX_LOCK(dev, txq, smp_processor_id());
 	if (!netif_xmit_frozen_or_stopped(txq))
-		ret = dev_hard_start_xmit(skb, dev, txq);
+		skb = dev_hard_start_xmit(skb, dev, txq, &ret);
 
 	HARD_TX_UNLOCK(dev, txq);
 
