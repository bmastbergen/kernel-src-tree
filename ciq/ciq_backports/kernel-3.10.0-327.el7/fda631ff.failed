NVMe: embedded iod mask cleanup

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Chong Yuan <chong.yuan@memblaze.com>
commit fda631ffe5422424579e1649e04cc468d0215b85
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/fda631ff.failed

Remove unused mask in nvme_alloc_iod

	Signed-off-by: Chong Yuan <chong.yuan@memblaze.com>
	Reviewed-by: Wenbo Wang  <wenbo.wang@memblaze.com>
	Acked-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit fda631ffe5422424579e1649e04cc468d0215b85)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index 6783fae878e4,7ed618125c27..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -141,62 -142,123 +141,170 @@@ typedef void (*nvme_completion_fn)(stru
  struct nvme_cmd_info {
  	nvme_completion_fn fn;
  	void *ctx;
 +	unsigned long timeout;
  	int aborted;
 -	struct nvme_queue *nvmeq;
 -	struct nvme_iod iod[0];
  };
  
 -/*
 - * Max size of iod being embedded in the request payload
 +static struct nvme_cmd_info *nvme_cmd_info(struct nvme_queue *nvmeq)
 +{
 +	return (void *)&nvmeq->cmdid_data[BITS_TO_LONGS(nvmeq->q_depth)];
 +}
 +
 +static unsigned nvme_queue_extra(int depth)
 +{
 +	return DIV_ROUND_UP(depth, 8) + (depth * sizeof(struct nvme_cmd_info));
 +}
 +
 +/**
 + * alloc_cmdid() - Allocate a Command ID
 + * @nvmeq: The queue that will be used for this command
 + * @ctx: A pointer that will be passed to the handler
 + * @handler: The function to call on completion
 + *
 + * Allocate a Command ID for a queue.  The data passed in will
 + * be passed to the completion handler.  This is implemented by using
 + * the bottom two bits of the ctx pointer to store the handler ID.
 + * Passing in a pointer that's not 4-byte aligned will cause a BUG.
 + * We can change this if it becomes a problem.
 + *
 + * May be called with local interrupts disabled and the q_lock held,
 + * or with interrupts enabled and no locks held.
   */
++<<<<<<< HEAD
 +static int alloc_cmdid(struct nvme_queue *nvmeq, void *ctx,
 +				nvme_completion_fn handler, unsigned timeout)
++=======
+ #define NVME_INT_PAGES		2
+ #define NVME_INT_BYTES(dev)	(NVME_INT_PAGES * (dev)->page_size)
+ #define NVME_INT_MASK		0x01
+ 
+ /*
+  * Will slightly overestimate the number of pages needed.  This is OK
+  * as it only leads to a small amount of wasted memory for the lifetime of
+  * the I/O.
+  */
+ static int nvme_npages(unsigned size, struct nvme_dev *dev)
++>>>>>>> fda631ffe542 (NVMe: embedded iod mask cleanup)
  {
 -	unsigned nprps = DIV_ROUND_UP(size + dev->page_size, dev->page_size);
 -	return DIV_ROUND_UP(8 * nprps, PAGE_SIZE - 8);
 -}
 +	int depth = nvmeq->q_depth - 1;
 +	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
 +	int cmdid;
  
 -static unsigned int nvme_cmd_size(struct nvme_dev *dev)
 -{
 +	do {
 +		cmdid = find_first_zero_bit(nvmeq->cmdid_data, depth);
 +		if (cmdid >= depth)
 +			return -EBUSY;
 +	} while (test_and_set_bit(cmdid, nvmeq->cmdid_data));
 +
 +	info[cmdid].fn = handler;
 +	info[cmdid].ctx = ctx;
 +	info[cmdid].timeout = jiffies + timeout;
 +	info[cmdid].aborted = 0;
 +	return cmdid;
 +}
 +
 +static int alloc_cmdid_killable(struct nvme_queue *nvmeq, void *ctx,
 +				nvme_completion_fn handler, unsigned timeout)
 +{
++<<<<<<< HEAD
 +	int cmdid;
 +	wait_event_killable(nvmeq->sq_full,
 +		(cmdid = alloc_cmdid(nvmeq, ctx, handler, timeout)) >= 0);
 +	return (cmdid < 0) ? -EINTR : cmdid;
++=======
+ 	unsigned int ret = sizeof(struct nvme_cmd_info);
+ 
+ 	ret += sizeof(struct nvme_iod);
+ 	ret += sizeof(__le64 *) * nvme_npages(NVME_INT_BYTES(dev), dev);
+ 	ret += sizeof(struct scatterlist) * NVME_INT_PAGES;
+ 
+ 	return ret;
+ }
+ 
+ static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+ 				unsigned int hctx_idx)
+ {
+ 	struct nvme_dev *dev = data;
+ 	struct nvme_queue *nvmeq = dev->queues[0];
+ 
+ 	WARN_ON(nvmeq->hctx);
+ 	nvmeq->hctx = hctx;
+ 	hctx->driver_data = nvmeq;
+ 	return 0;
+ }
+ 
+ static int nvme_admin_init_request(void *data, struct request *req,
+ 				unsigned int hctx_idx, unsigned int rq_idx,
+ 				unsigned int numa_node)
+ {
+ 	struct nvme_dev *dev = data;
+ 	struct nvme_cmd_info *cmd = blk_mq_rq_to_pdu(req);
+ 	struct nvme_queue *nvmeq = dev->queues[0];
+ 
+ 	BUG_ON(!nvmeq);
+ 	cmd->nvmeq = nvmeq;
+ 	return 0;
+ }
+ 
+ static void nvme_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
+ {
+ 	struct nvme_queue *nvmeq = hctx->driver_data;
+ 
+ 	nvmeq->hctx = NULL;
+ }
+ 
+ static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+ 			  unsigned int hctx_idx)
+ {
+ 	struct nvme_dev *dev = data;
+ 	struct nvme_queue *nvmeq = dev->queues[
+ 					(hctx_idx % dev->queue_count) + 1];
+ 
+ 	if (!nvmeq->hctx)
+ 		nvmeq->hctx = hctx;
+ 
+ 	/* nvmeq queues are shared between namespaces. We assume here that
+ 	 * blk-mq map the tags so they match up with the nvme queue tags. */
+ 	WARN_ON(nvmeq->hctx->tags != hctx->tags);
+ 
+ 	hctx->driver_data = nvmeq;
+ 	return 0;
+ }
+ 
+ static int nvme_init_request(void *data, struct request *req,
+ 				unsigned int hctx_idx, unsigned int rq_idx,
+ 				unsigned int numa_node)
+ {
+ 	struct nvme_dev *dev = data;
+ 	struct nvme_cmd_info *cmd = blk_mq_rq_to_pdu(req);
+ 	struct nvme_queue *nvmeq = dev->queues[hctx_idx + 1];
+ 
+ 	BUG_ON(!nvmeq);
+ 	cmd->nvmeq = nvmeq;
+ 	return 0;
+ }
+ 
+ static void nvme_set_info(struct nvme_cmd_info *cmd, void *ctx,
+ 				nvme_completion_fn handler)
+ {
+ 	cmd->fn = handler;
+ 	cmd->ctx = ctx;
+ 	cmd->aborted = 0;
+ 	blk_mq_start_request(blk_mq_rq_from_pdu(cmd));
+ }
+ 
+ static void *iod_get_private(struct nvme_iod *iod)
+ {
+ 	return (void *) (iod->private & ~0x1UL);
+ }
+ 
+ /*
+  * If bit 0 is set, the iod is embedded in the request payload.
+  */
+ static bool iod_should_kfree(struct nvme_iod *iod)
+ {
+ 	return (iod->private & NVME_INT_MASK) == 0;
++>>>>>>> fda631ffe542 (NVMe: embedded iod mask cleanup)
  }
  
  /* Special values must be less than 0x1000 */
@@@ -396,9 -428,30 +504,33 @@@ nvme_alloc_iod(unsigned nseg, unsigned 
  	return iod;
  }
  
++<<<<<<< HEAD
++=======
+ static struct nvme_iod *nvme_alloc_iod(struct request *rq, struct nvme_dev *dev,
+ 			               gfp_t gfp)
+ {
+ 	unsigned size = !(rq->cmd_flags & REQ_DISCARD) ? blk_rq_bytes(rq) :
+                                                 sizeof(struct nvme_dsm_range);
+ 	struct nvme_iod *iod;
+ 
+ 	if (rq->nr_phys_segments <= NVME_INT_PAGES &&
+ 	    size <= NVME_INT_BYTES(dev)) {
+ 		struct nvme_cmd_info *cmd = blk_mq_rq_to_pdu(rq);
+ 
+ 		iod = cmd->iod;
+ 		iod_init(iod, size, rq->nr_phys_segments,
+ 				(unsigned long) rq | NVME_INT_MASK);
+ 		return iod;
+ 	}
+ 
+ 	return __nvme_alloc_iod(rq->nr_phys_segments, size, dev,
+ 				(unsigned long) rq, gfp);
+ }
+ 
++>>>>>>> fda631ffe542 (NVMe: embedded iod mask cleanup)
  void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod)
  {
 -	const int last_prp = dev->page_size / 8 - 1;
 +	const int last_prp = PAGE_SIZE / 8 - 1;
  	int i;
  	__le64 **list = iod_list(iod);
  	dma_addr_t prp_dma = iod->first_dma;
* Unmerged path drivers/block/nvme-core.c
