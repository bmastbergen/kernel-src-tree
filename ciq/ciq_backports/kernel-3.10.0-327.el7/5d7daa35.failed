ixgbe: improve mac filter handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Jacob Keller <jacob.e.keller@intel.com>
commit 5d7daa35b9eb14b64acd208a900e44aeeee25eca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/5d7daa35.failed

Add mac_table API based on work done for igb, which includes functions
to add and delete mac filters. This simplifies code for various entities
that use MAC filters such as VMDQ, SR-IOV, MACVLAN, and such.

	Reported-by: Mitch Williams <mitch.a.williams@intel.com>
	Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
	Tested-by: Phil Schmitt <phillip.j.schmitt@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 5d7daa35b9eb14b64acd208a900e44aeeee25eca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
#	drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 2dd08b8c8279,39a1c07258b0..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -3866,6 -3846,158 +3866,161 @@@ static void ixgbe_restore_vlan(struct i
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * ixgbe_write_mc_addr_list - write multicast addresses to MTA
+  * @netdev: network interface device structure
+  *
+  * Writes multicast address list to the MTA hash table.
+  * Returns: -ENOMEM on failure
+  *                0 on no addresses written
+  *                X on writing X addresses to MTA
+  **/
+ static int ixgbe_write_mc_addr_list(struct net_device *netdev)
+ {
+ 	struct ixgbe_adapter *adapter = netdev_priv(netdev);
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 
+ 	if (!netif_running(netdev))
+ 		return 0;
+ 
+ 	if (hw->mac.ops.update_mc_addr_list)
+ 		hw->mac.ops.update_mc_addr_list(hw, netdev);
+ 	else
+ 		return -ENOMEM;
+ 
+ #ifdef CONFIG_PCI_IOV
+ 	ixgbe_restore_vf_multicasts(adapter);
+ #endif
+ 
+ 	return netdev_mc_count(netdev);
+ }
+ 
+ #ifdef CONFIG_PCI_IOV
+ void ixgbe_full_sync_mac_table(struct ixgbe_adapter *adapter)
+ {
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 	int i;
+ 	for (i = 0; i < hw->mac.num_rar_entries; i++) {
+ 		if (adapter->mac_table[i].state & IXGBE_MAC_STATE_IN_USE)
+ 			hw->mac.ops.set_rar(hw, i, adapter->mac_table[i].addr,
+ 					    adapter->mac_table[i].queue,
+ 					    IXGBE_RAH_AV);
+ 		else
+ 			hw->mac.ops.clear_rar(hw, i);
+ 
+ 		adapter->mac_table[i].state &= ~(IXGBE_MAC_STATE_MODIFIED);
+ 	}
+ }
+ #endif
+ 
+ static void ixgbe_sync_mac_table(struct ixgbe_adapter *adapter)
+ {
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 	int i;
+ 	for (i = 0; i < hw->mac.num_rar_entries; i++) {
+ 		if (adapter->mac_table[i].state & IXGBE_MAC_STATE_MODIFIED) {
+ 			if (adapter->mac_table[i].state &
+ 			    IXGBE_MAC_STATE_IN_USE)
+ 				hw->mac.ops.set_rar(hw, i,
+ 						adapter->mac_table[i].addr,
+ 						adapter->mac_table[i].queue,
+ 						IXGBE_RAH_AV);
+ 			else
+ 				hw->mac.ops.clear_rar(hw, i);
+ 
+ 			adapter->mac_table[i].state &=
+ 						~(IXGBE_MAC_STATE_MODIFIED);
+ 		}
+ 	}
+ }
+ 
+ static void ixgbe_flush_sw_mac_table(struct ixgbe_adapter *adapter)
+ {
+ 	int i;
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 
+ 	for (i = 0; i < hw->mac.num_rar_entries; i++) {
+ 		adapter->mac_table[i].state |= IXGBE_MAC_STATE_MODIFIED;
+ 		adapter->mac_table[i].state &= ~IXGBE_MAC_STATE_IN_USE;
+ 		memset(adapter->mac_table[i].addr, 0, ETH_ALEN);
+ 		adapter->mac_table[i].queue = 0;
+ 	}
+ 	ixgbe_sync_mac_table(adapter);
+ }
+ 
+ static int ixgbe_available_rars(struct ixgbe_adapter *adapter)
+ {
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 	int i, count = 0;
+ 
+ 	for (i = 0; i < hw->mac.num_rar_entries; i++) {
+ 		if (adapter->mac_table[i].state == 0)
+ 			count++;
+ 	}
+ 	return count;
+ }
+ 
+ /* this function destroys the first RAR entry */
+ static void ixgbe_mac_set_default_filter(struct ixgbe_adapter *adapter,
+ 					 u8 *addr)
+ {
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 
+ 	memcpy(&adapter->mac_table[0].addr, addr, ETH_ALEN);
+ 	adapter->mac_table[0].queue = VMDQ_P(0);
+ 	adapter->mac_table[0].state = (IXGBE_MAC_STATE_DEFAULT |
+ 				       IXGBE_MAC_STATE_IN_USE);
+ 	hw->mac.ops.set_rar(hw, 0, adapter->mac_table[0].addr,
+ 			    adapter->mac_table[0].queue,
+ 			    IXGBE_RAH_AV);
+ }
+ 
+ int ixgbe_add_mac_filter(struct ixgbe_adapter *adapter, u8 *addr, u16 queue)
+ {
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 	int i;
+ 
+ 	if (is_zero_ether_addr(addr))
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < hw->mac.num_rar_entries; i++) {
+ 		if (adapter->mac_table[i].state & IXGBE_MAC_STATE_IN_USE)
+ 			continue;
+ 		adapter->mac_table[i].state |= (IXGBE_MAC_STATE_MODIFIED |
+ 						IXGBE_MAC_STATE_IN_USE);
+ 		ether_addr_copy(adapter->mac_table[i].addr, addr);
+ 		adapter->mac_table[i].queue = queue;
+ 		ixgbe_sync_mac_table(adapter);
+ 		return i;
+ 	}
+ 	return -ENOMEM;
+ }
+ 
+ int ixgbe_del_mac_filter(struct ixgbe_adapter *adapter, u8 *addr, u16 queue)
+ {
+ 	/* search table for addr, if found, set to 0 and sync */
+ 	int i;
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 
+ 	if (is_zero_ether_addr(addr))
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < hw->mac.num_rar_entries; i++) {
+ 		if (ether_addr_equal(addr, adapter->mac_table[i].addr) &&
+ 		    adapter->mac_table[i].queue == queue) {
+ 			adapter->mac_table[i].state |= IXGBE_MAC_STATE_MODIFIED;
+ 			adapter->mac_table[i].state &= ~IXGBE_MAC_STATE_IN_USE;
+ 			memset(adapter->mac_table[i].addr, 0, ETH_ALEN);
+ 			adapter->mac_table[i].queue = 0;
+ 			ixgbe_sync_mac_table(adapter);
+ 			return 0;
+ 		}
+ 	}
+ 	return -ENOMEM;
+ }
+ /**
++>>>>>>> 5d7daa35b9eb (ixgbe: improve mac filter handling)
   * ixgbe_write_uc_addr_list - write unicast addresses to RAR table
   * @netdev: network interface device structure
   *
@@@ -3874,19 -4006,13 +4029,20 @@@
   *                0 on no addresses written
   *                X on writing X addresses to the RAR table
   **/
- static int ixgbe_write_uc_addr_list(struct net_device *netdev)
+ static int ixgbe_write_uc_addr_list(struct net_device *netdev, int vfn)
  {
  	struct ixgbe_adapter *adapter = netdev_priv(netdev);
- 	struct ixgbe_hw *hw = &adapter->hw;
- 	unsigned int rar_entries = hw->mac.num_rar_entries - 1;
  	int count = 0;
  
++<<<<<<< HEAD
 +	/* In SR-IOV mode significantly less RAR entries are available */
 +	if (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)
 +		rar_entries = IXGBE_MAX_PF_MACVLANS - 1;
 +
++=======
++>>>>>>> 5d7daa35b9eb (ixgbe: improve mac filter handling)
  	/* return ENOMEM indicating insufficient memory for addresses */
- 	if (netdev_uc_count(netdev) > rar_entries)
+ 	if (netdev_uc_count(netdev) > ixgbe_available_rars(adapter))
  		return -ENOMEM;
  
  	if (!netdev_uc_empty(netdev)) {
@@@ -4262,6 -4374,218 +4408,221 @@@ static void ixgbe_fdir_filter_restore(s
  	spin_unlock(&adapter->fdir_perfect_lock);
  }
  
++<<<<<<< HEAD
++=======
+ static void ixgbe_macvlan_set_rx_mode(struct net_device *dev, unsigned int pool,
+ 				      struct ixgbe_adapter *adapter)
+ {
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 	u32 vmolr;
+ 
+ 	/* No unicast promiscuous support for VMDQ devices. */
+ 	vmolr = IXGBE_READ_REG(hw, IXGBE_VMOLR(pool));
+ 	vmolr |= (IXGBE_VMOLR_ROMPE | IXGBE_VMOLR_BAM | IXGBE_VMOLR_AUPE);
+ 
+ 	/* clear the affected bit */
+ 	vmolr &= ~IXGBE_VMOLR_MPE;
+ 
+ 	if (dev->flags & IFF_ALLMULTI) {
+ 		vmolr |= IXGBE_VMOLR_MPE;
+ 	} else {
+ 		vmolr |= IXGBE_VMOLR_ROMPE;
+ 		hw->mac.ops.update_mc_addr_list(hw, dev);
+ 	}
+ 	ixgbe_write_uc_addr_list(adapter->netdev, pool);
+ 	IXGBE_WRITE_REG(hw, IXGBE_VMOLR(pool), vmolr);
+ }
+ 
+ static void ixgbe_fwd_psrtype(struct ixgbe_fwd_adapter *vadapter)
+ {
+ 	struct ixgbe_adapter *adapter = vadapter->real_adapter;
+ 	int rss_i = adapter->num_rx_queues_per_pool;
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 	u16 pool = vadapter->pool;
+ 	u32 psrtype = IXGBE_PSRTYPE_TCPHDR |
+ 		      IXGBE_PSRTYPE_UDPHDR |
+ 		      IXGBE_PSRTYPE_IPV4HDR |
+ 		      IXGBE_PSRTYPE_L2HDR |
+ 		      IXGBE_PSRTYPE_IPV6HDR;
+ 
+ 	if (hw->mac.type == ixgbe_mac_82598EB)
+ 		return;
+ 
+ 	if (rss_i > 3)
+ 		psrtype |= 2 << 29;
+ 	else if (rss_i > 1)
+ 		psrtype |= 1 << 29;
+ 
+ 	IXGBE_WRITE_REG(hw, IXGBE_PSRTYPE(VMDQ_P(pool)), psrtype);
+ }
+ 
+ /**
+  * ixgbe_clean_rx_ring - Free Rx Buffers per Queue
+  * @rx_ring: ring to free buffers from
+  **/
+ static void ixgbe_clean_rx_ring(struct ixgbe_ring *rx_ring)
+ {
+ 	struct device *dev = rx_ring->dev;
+ 	unsigned long size;
+ 	u16 i;
+ 
+ 	/* ring already cleared, nothing to do */
+ 	if (!rx_ring->rx_buffer_info)
+ 		return;
+ 
+ 	/* Free all the Rx ring sk_buffs */
+ 	for (i = 0; i < rx_ring->count; i++) {
+ 		struct ixgbe_rx_buffer *rx_buffer;
+ 
+ 		rx_buffer = &rx_ring->rx_buffer_info[i];
+ 		if (rx_buffer->skb) {
+ 			struct sk_buff *skb = rx_buffer->skb;
+ 			if (IXGBE_CB(skb)->page_released) {
+ 				dma_unmap_page(dev,
+ 					       IXGBE_CB(skb)->dma,
+ 					       ixgbe_rx_bufsz(rx_ring),
+ 					       DMA_FROM_DEVICE);
+ 				IXGBE_CB(skb)->page_released = false;
+ 			}
+ 			dev_kfree_skb(skb);
+ 		}
+ 		rx_buffer->skb = NULL;
+ 		if (rx_buffer->dma)
+ 			dma_unmap_page(dev, rx_buffer->dma,
+ 				       ixgbe_rx_pg_size(rx_ring),
+ 				       DMA_FROM_DEVICE);
+ 		rx_buffer->dma = 0;
+ 		if (rx_buffer->page)
+ 			__free_pages(rx_buffer->page,
+ 				     ixgbe_rx_pg_order(rx_ring));
+ 		rx_buffer->page = NULL;
+ 	}
+ 
+ 	size = sizeof(struct ixgbe_rx_buffer) * rx_ring->count;
+ 	memset(rx_ring->rx_buffer_info, 0, size);
+ 
+ 	/* Zero out the descriptor ring */
+ 	memset(rx_ring->desc, 0, rx_ring->size);
+ 
+ 	rx_ring->next_to_alloc = 0;
+ 	rx_ring->next_to_clean = 0;
+ 	rx_ring->next_to_use = 0;
+ }
+ 
+ static void ixgbe_disable_fwd_ring(struct ixgbe_fwd_adapter *vadapter,
+ 				   struct ixgbe_ring *rx_ring)
+ {
+ 	struct ixgbe_adapter *adapter = vadapter->real_adapter;
+ 	int index = rx_ring->queue_index + vadapter->rx_base_queue;
+ 
+ 	/* shutdown specific queue receive and wait for dma to settle */
+ 	ixgbe_disable_rx_queue(adapter, rx_ring);
+ 	usleep_range(10000, 20000);
+ 	ixgbe_irq_disable_queues(adapter, ((u64)1 << index));
+ 	ixgbe_clean_rx_ring(rx_ring);
+ 	rx_ring->l2_accel_priv = NULL;
+ }
+ 
+ static int ixgbe_fwd_ring_down(struct net_device *vdev,
+ 			       struct ixgbe_fwd_adapter *accel)
+ {
+ 	struct ixgbe_adapter *adapter = accel->real_adapter;
+ 	unsigned int rxbase = accel->rx_base_queue;
+ 	unsigned int txbase = accel->tx_base_queue;
+ 	int i;
+ 
+ 	netif_tx_stop_all_queues(vdev);
+ 
+ 	for (i = 0; i < adapter->num_rx_queues_per_pool; i++) {
+ 		ixgbe_disable_fwd_ring(accel, adapter->rx_ring[rxbase + i]);
+ 		adapter->rx_ring[rxbase + i]->netdev = adapter->netdev;
+ 	}
+ 
+ 	for (i = 0; i < adapter->num_rx_queues_per_pool; i++) {
+ 		adapter->tx_ring[txbase + i]->l2_accel_priv = NULL;
+ 		adapter->tx_ring[txbase + i]->netdev = adapter->netdev;
+ 	}
+ 
+ 
+ 	return 0;
+ }
+ 
+ static int ixgbe_fwd_ring_up(struct net_device *vdev,
+ 			     struct ixgbe_fwd_adapter *accel)
+ {
+ 	struct ixgbe_adapter *adapter = accel->real_adapter;
+ 	unsigned int rxbase, txbase, queues;
+ 	int i, baseq, err = 0;
+ 
+ 	if (!test_bit(accel->pool, &adapter->fwd_bitmask))
+ 		return 0;
+ 
+ 	baseq = accel->pool * adapter->num_rx_queues_per_pool;
+ 	netdev_dbg(vdev, "pool %i:%i queues %i:%i VSI bitmask %lx\n",
+ 		   accel->pool, adapter->num_rx_pools,
+ 		   baseq, baseq + adapter->num_rx_queues_per_pool,
+ 		   adapter->fwd_bitmask);
+ 
+ 	accel->netdev = vdev;
+ 	accel->rx_base_queue = rxbase = baseq;
+ 	accel->tx_base_queue = txbase = baseq;
+ 
+ 	for (i = 0; i < adapter->num_rx_queues_per_pool; i++)
+ 		ixgbe_disable_fwd_ring(accel, adapter->rx_ring[rxbase + i]);
+ 
+ 	for (i = 0; i < adapter->num_rx_queues_per_pool; i++) {
+ 		adapter->rx_ring[rxbase + i]->netdev = vdev;
+ 		adapter->rx_ring[rxbase + i]->l2_accel_priv = accel;
+ 		ixgbe_configure_rx_ring(adapter, adapter->rx_ring[rxbase + i]);
+ 	}
+ 
+ 	for (i = 0; i < adapter->num_rx_queues_per_pool; i++) {
+ 		adapter->tx_ring[txbase + i]->netdev = vdev;
+ 		adapter->tx_ring[txbase + i]->l2_accel_priv = accel;
+ 	}
+ 
+ 	queues = min_t(unsigned int,
+ 		       adapter->num_rx_queues_per_pool, vdev->num_tx_queues);
+ 	err = netif_set_real_num_tx_queues(vdev, queues);
+ 	if (err)
+ 		goto fwd_queue_err;
+ 
+ 	err = netif_set_real_num_rx_queues(vdev, queues);
+ 	if (err)
+ 		goto fwd_queue_err;
+ 
+ 	if (is_valid_ether_addr(vdev->dev_addr))
+ 		ixgbe_add_mac_filter(adapter, vdev->dev_addr, accel->pool);
+ 
+ 	ixgbe_fwd_psrtype(accel);
+ 	ixgbe_macvlan_set_rx_mode(vdev, accel->pool, adapter);
+ 	return err;
+ fwd_queue_err:
+ 	ixgbe_fwd_ring_down(vdev, accel);
+ 	return err;
+ }
+ 
+ static void ixgbe_configure_dfwd(struct ixgbe_adapter *adapter)
+ {
+ 	struct net_device *upper;
+ 	struct list_head *iter;
+ 	int err;
+ 
+ 	netdev_for_each_all_upper_dev_rcu(adapter->netdev, upper, iter) {
+ 		if (netif_is_macvlan(upper)) {
+ 			struct macvlan_dev *dfwd = netdev_priv(upper);
+ 			struct ixgbe_fwd_adapter *vadapter = dfwd->fwd_priv;
+ 
+ 			if (dfwd->fwd_priv) {
+ 				err = ixgbe_fwd_ring_up(upper, vadapter);
+ 				if (err)
+ 					continue;
+ 			}
+ 		}
+ 	}
+ }
+ 
++>>>>>>> 5d7daa35b9eb (ixgbe: improve mac filter handling)
  static void ixgbe_configure(struct ixgbe_adapter *adapter)
  {
  	struct ixgbe_hw *hw = &adapter->hw;
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
index ac6f5c6e917f,a01417c06620..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
@@@ -360,21 -358,7 +358,25 @@@ static int ixgbe_set_vf_multicasts(stru
  	return 0;
  }
  
++<<<<<<< HEAD
 +static void ixgbe_restore_vf_macvlans(struct ixgbe_adapter *adapter)
 +{
 +	struct ixgbe_hw *hw = &adapter->hw;
 +	struct list_head *pos;
 +	struct vf_macvlans *entry;
 +
 +	list_for_each(pos, &adapter->vf_mvs.l) {
 +		entry = list_entry(pos, struct vf_macvlans, l);
 +		if (!entry->free)
 +			hw->mac.ops.set_rar(hw, entry->rar_entry,
 +					    entry->vf_macvlan,
 +					    entry->vf, IXGBE_RAH_AV);
 +	}
 +}
 +
++=======
+ #ifdef CONFIG_PCI_IOV
++>>>>>>> 5d7daa35b9eb (ixgbe: improve mac filter handling)
  void ixgbe_restore_vf_multicasts(struct ixgbe_adapter *adapter)
  {
  	struct ixgbe_hw *hw = &adapter->hw;
@@@ -397,8 -382,15 +399,8 @@@
  	}
  
  	/* Restore any VF macvlans */
- 	ixgbe_restore_vf_macvlans(adapter);
+ 	ixgbe_full_sync_mac_table(adapter);
  }
 -#endif
  
  static int ixgbe_set_vf_vlan(struct ixgbe_adapter *adapter, int add, int vid,
  			     u32 vf)
@@@ -556,11 -546,9 +557,17 @@@ static inline void ixgbe_vf_reset_event
  static int ixgbe_set_vf_mac(struct ixgbe_adapter *adapter,
  			    int vf, unsigned char *mac_addr)
  {
++<<<<<<< HEAD
 +	struct ixgbe_hw *hw = &adapter->hw;
 +	int rar_entry = hw->mac.num_rar_entries - (vf + 1);
 +
 +	memcpy(adapter->vfinfo[vf].vf_mac_addresses, mac_addr, 6);
 +	hw->mac.ops.set_rar(hw, rar_entry, mac_addr, vf, IXGBE_RAH_AV);
++=======
+ 	ixgbe_del_mac_filter(adapter, adapter->vfinfo[vf].vf_mac_addresses, vf);
+ 	memcpy(adapter->vfinfo[vf].vf_mac_addresses, mac_addr, ETH_ALEN);
+ 	ixgbe_add_mac_filter(adapter, adapter->vfinfo[vf].vf_mac_addresses, vf);
++>>>>>>> 5d7daa35b9eb (ixgbe: improve mac filter handling)
  
  	return 0;
  }
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe.h b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
index 364dbb931d92..a380262cb653 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe.h
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
@@ -155,7 +155,6 @@ struct vf_data_storage {
 struct vf_macvlans {
 	struct list_head l;
 	int vf;
-	int rar_entry;
 	bool free;
 	bool is_macvlan;
 	u8 vf_macvlan[ETH_ALEN];
@@ -595,6 +594,15 @@ static inline void ixgbe_write_tail(struct ixgbe_ring *ring, u32 value)
 #define MAX_MSIX_VECTORS_82598 18
 #define MAX_Q_VECTORS_82598 16
 
+struct ixgbe_mac_addr {
+	u8 addr[ETH_ALEN];
+	u16 queue;
+	u16 state; /* bitmask */
+};
+#define IXGBE_MAC_STATE_DEFAULT		0x1
+#define IXGBE_MAC_STATE_MODIFIED	0x2
+#define IXGBE_MAC_STATE_IN_USE		0x4
+
 #define MAX_Q_VECTORS MAX_Q_VECTORS_82599
 #define MAX_MSIX_COUNT MAX_MSIX_VECTORS_82599
 
@@ -767,6 +775,7 @@ struct ixgbe_adapter {
 
 	u32 timer_event_accumulator;
 	u32 vferr_refcount;
+	struct ixgbe_mac_addr *mac_table;
 	struct kobject *info_kobj;
 #ifdef CONFIG_IXGBE_HWMON
 	struct hwmon_buff ixgbe_hwmon_buff;
@@ -848,6 +857,13 @@ void ixgbe_update_stats(struct ixgbe_adapter *adapter);
 int ixgbe_init_interrupt_scheme(struct ixgbe_adapter *adapter);
 int ixgbe_wol_supported(struct ixgbe_adapter *adapter, u16 device_id,
 			       u16 subdevice_id);
+#ifdef CONFIG_PCI_IOV
+void ixgbe_full_sync_mac_table(struct ixgbe_adapter *adapter);
+#endif
+int ixgbe_add_mac_filter(struct ixgbe_adapter *adapter,
+			 u8 *addr, u16 queue);
+int ixgbe_del_mac_filter(struct ixgbe_adapter *adapter,
+			 u8 *addr, u16 queue);
 void ixgbe_clear_interrupt_scheme(struct ixgbe_adapter *adapter);
 netdev_tx_t ixgbe_xmit_frame_ring(struct sk_buff *, struct ixgbe_adapter *,
 				  struct ixgbe_ring *);
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_sriov.c
