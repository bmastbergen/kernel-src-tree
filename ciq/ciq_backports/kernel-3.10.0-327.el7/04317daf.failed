tcp: introduce TCP_SKB_CB(skb)->tcp_tw_isn

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 04317dafd11dd7b0ec19b85f098414abae6ed5f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/04317daf.failed

TCP_SKB_CB(skb)->when has different meaning in output and input paths.

In output path, it contains a timestamp.
In input path, it contains an ISN, chosen by tcp_timewait_state_process()

Lets add a different name to ease code comprehension.

Note that 'when' field will disappear in following patch,
as skb_mstamp already contains timestamp, the anonymous
union will promptly disappear as well.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Acked-by: Yuchung Cheng <ycheng@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 04317dafd11dd7b0ec19b85f098414abae6ed5f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_input.c
#	net/ipv6/tcp_ipv6.c
diff --cc net/ipv4/tcp_input.c
index 6394ffe428d8,9c8b9f1dcf69..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -5817,3 -5883,156 +5817,159 @@@ discard
  	return 0;
  }
  EXPORT_SYMBOL(tcp_rcv_state_process);
++<<<<<<< HEAD
++=======
+ 
+ static inline void pr_drop_req(struct request_sock *req, __u16 port, int family)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 
+ 	if (family == AF_INET)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI4/%u\n"),
+ 			       &ireq->ir_rmt_addr, port);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	else if (family == AF_INET6)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI6/%u\n"),
+ 			       &ireq->ir_v6_rmt_addr, port);
+ #endif
+ }
+ 
+ int tcp_conn_request(struct request_sock_ops *rsk_ops,
+ 		     const struct tcp_request_sock_ops *af_ops,
+ 		     struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct tcp_options_received tmp_opt;
+ 	struct request_sock *req;
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 	struct dst_entry *dst = NULL;
+ 	__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;
+ 	bool want_cookie = false, fastopen;
+ 	struct flowi fl;
+ 	struct tcp_fastopen_cookie foc = { .len = -1 };
+ 	int err;
+ 
+ 
+ 	/* TW buckets are converted to open requests without
+ 	 * limitations, they conserve resources and peer is
+ 	 * evidently real one.
+ 	 */
+ 	if ((sysctl_tcp_syncookies == 2 ||
+ 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
+ 		want_cookie = tcp_syn_flood_action(sk, skb, rsk_ops->slab_name);
+ 		if (!want_cookie)
+ 			goto drop;
+ 	}
+ 
+ 
+ 	/* Accept backlog is full. If we have already queued enough
+ 	 * of warm entries in syn queue, drop request. It is better than
+ 	 * clogging syn queue with openreqs with exponentially increasing
+ 	 * timeout.
+ 	 */
+ 	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+ 		goto drop;
+ 	}
+ 
+ 	req = inet_reqsk_alloc(rsk_ops);
+ 	if (!req)
+ 		goto drop;
+ 
+ 	tcp_rsk(req)->af_specific = af_ops;
+ 
+ 	tcp_clear_options(&tmp_opt);
+ 	tmp_opt.mss_clamp = af_ops->mss_clamp;
+ 	tmp_opt.user_mss  = tp->rx_opt.user_mss;
+ 	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
+ 
+ 	if (want_cookie && !tmp_opt.saw_tstamp)
+ 		tcp_clear_options(&tmp_opt);
+ 
+ 	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+ 	tcp_openreq_init(req, &tmp_opt, skb, sk);
+ 
+ 	af_ops->init_req(req, sk, skb);
+ 
+ 	if (security_inet_conn_request(sk, skb, req))
+ 		goto drop_and_free;
+ 
+ 	if (!want_cookie || tmp_opt.tstamp_ok)
+ 		TCP_ECN_create_request(req, skb, sock_net(sk));
+ 
+ 	if (want_cookie) {
+ 		isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
+ 		req->cookie_ts = tmp_opt.tstamp_ok;
+ 	} else if (!isn) {
+ 		/* VJ's idea. We save last timestamp seen
+ 		 * from the destination in peer table, when entering
+ 		 * state TIME-WAIT, and check against it before
+ 		 * accepting new connection request.
+ 		 *
+ 		 * If "isn" is not zero, this request hit alive
+ 		 * timewait bucket, so that all the necessary checks
+ 		 * are made in the function processing timewait state.
+ 		 */
+ 		if (tcp_death_row.sysctl_tw_recycle) {
+ 			bool strict;
+ 
+ 			dst = af_ops->route_req(sk, &fl, req, &strict);
+ 
+ 			if (dst && strict &&
+ 			    !tcp_peer_is_proven(req, dst, true,
+ 						tmp_opt.saw_tstamp)) {
+ 				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
+ 				goto drop_and_release;
+ 			}
+ 		}
+ 		/* Kill the following clause, if you dislike this way. */
+ 		else if (!sysctl_tcp_syncookies &&
+ 			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
+ 			  (sysctl_max_syn_backlog >> 2)) &&
+ 			 !tcp_peer_is_proven(req, dst, false,
+ 					     tmp_opt.saw_tstamp)) {
+ 			/* Without syncookies last quarter of
+ 			 * backlog is filled with destinations,
+ 			 * proven to be alive.
+ 			 * It means that we continue to communicate
+ 			 * to destinations, already remembered
+ 			 * to the moment of synflood.
+ 			 */
+ 			pr_drop_req(req, ntohs(tcp_hdr(skb)->source),
+ 				    rsk_ops->family);
+ 			goto drop_and_release;
+ 		}
+ 
+ 		isn = af_ops->init_seq(skb);
+ 	}
+ 	if (!dst) {
+ 		dst = af_ops->route_req(sk, &fl, req, NULL);
+ 		if (!dst)
+ 			goto drop_and_free;
+ 	}
+ 
+ 	tcp_rsk(req)->snt_isn = isn;
+ 	tcp_openreq_init_rwin(req, sk, dst);
+ 	fastopen = !want_cookie &&
+ 		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+ 	err = af_ops->send_synack(sk, dst, &fl, req,
+ 				  skb_get_queue_mapping(skb), &foc);
+ 	if (!fastopen) {
+ 		if (err || want_cookie)
+ 			goto drop_and_free;
+ 
+ 		tcp_rsk(req)->listener = NULL;
+ 		af_ops->queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+ 	}
+ 
+ 	return 0;
+ 
+ drop_and_release:
+ 	dst_release(dst);
+ drop_and_free:
+ 	reqsk_free(req);
+ drop:
+ 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tcp_conn_request);
++>>>>>>> 04317dafd11d (tcp: introduce TCP_SKB_CB(skb)->tcp_tw_isn)
diff --cc net/ipv6/tcp_ipv6.c
index 45f11a2930c9,5b3c70ff7a72..000000000000
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@@ -714,8 -708,54 +714,45 @@@ static int tcp_v6_inbound_md5_hash(stru
  	}
  	return 0;
  }
 -
 -static int tcp_v6_inbound_md5_hash(struct sock *sk, const struct sk_buff *skb)
 -{
 -	int ret;
 -
 -	rcu_read_lock();
 -	ret = __tcp_v6_inbound_md5_hash(sk, skb);
 -	rcu_read_unlock();
 -
 -	return ret;
 -}
 -
  #endif
  
++<<<<<<< HEAD
++=======
+ static void tcp_v6_init_req(struct request_sock *req, struct sock *sk,
+ 			    struct sk_buff *skb)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 	struct ipv6_pinfo *np = inet6_sk(sk);
+ 
+ 	ireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;
+ 	ireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;
+ 
+ 	ireq->ir_iif = sk->sk_bound_dev_if;
+ 
+ 	/* So that link locals have meaning */
+ 	if (!sk->sk_bound_dev_if &&
+ 	    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)
+ 		ireq->ir_iif = inet6_iif(skb);
+ 
+ 	if (!TCP_SKB_CB(skb)->tcp_tw_isn &&
+ 	    (ipv6_opt_accepted(sk, skb) || np->rxopt.bits.rxinfo ||
+ 	     np->rxopt.bits.rxoinfo || np->rxopt.bits.rxhlim ||
+ 	     np->rxopt.bits.rxohlim || np->repflow)) {
+ 		atomic_inc(&skb->users);
+ 		ireq->pktopts = skb;
+ 	}
+ }
+ 
+ static struct dst_entry *tcp_v6_route_req(struct sock *sk, struct flowi *fl,
+ 					  const struct request_sock *req,
+ 					  bool *strict)
+ {
+ 	if (strict)
+ 		*strict = true;
+ 	return inet6_csk_route_req(sk, &fl->u.ip6, req);
+ }
+ 
++>>>>>>> 04317dafd11d (tcp: introduce TCP_SKB_CB(skb)->tcp_tw_isn)
  struct request_sock_ops tcp6_request_sock_ops __read_mostly = {
  	.family		=	AF_INET6,
  	.obj_size	=	sizeof(struct tcp6_request_sock),
diff --git a/include/net/tcp.h b/include/net/tcp.h
index 81f472949625..3c445d9123f7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -716,7 +716,12 @@ struct tcp_skb_cb {
 	} header;	/* For incoming frames		*/
 	__u32		seq;		/* Starting sequence number	*/
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
-	__u32		when;		/* used to compute rtt's	*/
+	union {
+		/* used in output path */
+		__u32		when;	/* used to compute rtt's	*/
+		/* used in input path */
+		__u32		tcp_tw_isn; /* isn chosen by tcp_timewait_state_process() */
+	};
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 
 	__u8		sacked;		/* State flags for SACK/FACK.	*/
* Unmerged path net/ipv4/tcp_input.c
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index f2224d131626..40c7c013d5fa 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -1940,7 +1940,7 @@ int tcp_v4_rcv(struct sk_buff *skb)
 	TCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +
 				    skb->len - th->doff * 4);
 	TCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);
-	TCP_SKB_CB(skb)->when	 = 0;
+	TCP_SKB_CB(skb)->tcp_tw_isn = 0;
 	TCP_SKB_CB(skb)->ip_dsfield = ipv4_get_dsfield(iph);
 	TCP_SKB_CB(skb)->sacked	 = 0;
 
diff --git a/net/ipv4/tcp_minisocks.c b/net/ipv4/tcp_minisocks.c
index 7a436c517e44..239fd1cb54e2 100644
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@ -232,7 +232,7 @@ kill:
 		u32 isn = tcptw->tw_snd_nxt + 65535 + 2;
 		if (isn == 0)
 			isn++;
-		TCP_SKB_CB(skb)->when = isn;
+		TCP_SKB_CB(skb)->tcp_tw_isn = isn;
 		return TCP_TW_SYN;
 	}
 
* Unmerged path net/ipv6/tcp_ipv6.c
