perf/x86/intel: Make the HT bug workaround conditional on HT enabled

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [perf] x86/intel: Make the HT bug workaround conditional on HT enabled (Jiri Olsa) [1210494]
Rebuild_FUZZ: 96.18%
commit-author Stephane Eranian <eranian@google.com>
commit b37609c30e41264c4df4acff78abfc894499a49b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/b37609c3.failed

This patch disables the PMU HT bug when Hyperthreading (HT)
is disabled. We cannot do this test immediately when perf_events
is initialized. We need to wait until the topology information
is setup properly. As such, we register a later initcall, check
the topology and potentially disable the workaround. To do this,
we need to ensure there is no user of the PMU. At this point of
the boot, the only user is the NMI watchdog, thus we disable
it during the switch and re-enable it right after.

Having the workaround disabled when it is not needed provides
some benefits by limiting the overhead is time and space.
The workaround still ensures correct scheduling of the corrupting
memory events (0xd0, 0xd1, 0xd2) when HT is off. Those events
can only be measured on counters 0-3. Something else the current
kernel did not handle correctly.

	Signed-off-by: Stephane Eranian <eranian@google.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: bp@alien8.de
	Cc: jolsa@redhat.com
	Cc: kan.liang@intel.com
	Cc: maria.n.dimakopoulou@gmail.com
Link: http://lkml.kernel.org/r/1416251225-17721-13-git-send-email-eranian@google.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit b37609c30e41264c4df4acff78abfc894499a49b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event.h
#	arch/x86/kernel/cpu/perf_event_intel.c
diff --cc arch/x86/kernel/cpu/perf_event.h
index aaa12c2a8b1e,7250c0281f9d..000000000000
--- a/arch/x86/kernel/cpu/perf_event.h
+++ b/arch/x86/kernel/cpu/perf_event.h
@@@ -552,6 -623,8 +552,11 @@@ do {									
   */
  #define PMU_FL_NO_HT_SHARING	0x1 /* no hyper-threading resource sharing */
  #define PMU_FL_HAS_RSP_1	0x2 /* has 2 equivalent offcore_rsp regs   */
++<<<<<<< HEAD
++=======
+ #define PMU_FL_EXCL_CNTRS	0x4 /* has exclusive counter requirements  */
+ #define PMU_FL_EXCL_ENABLED	0x8 /* exclusive counter active */
++>>>>>>> b37609c30e41 (perf/x86/intel: Make the HT bug workaround conditional on HT enabled)
  
  #define EVENT_VAR(_id)  event_attr_##_id
  #define EVENT_PTR(_id) &event_attr_##_id.attr.attr
diff --cc arch/x86/kernel/cpu/perf_event_intel.c
index 8bd638d75162,6ea61a572fb0..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel.c
+++ b/arch/x86/kernel/cpu/perf_event_intel.c
@@@ -1775,11 -1872,278 +1776,112 @@@ intel_get_event_constraints(struct cpu_
  	if (c)
  		return c;
  
++<<<<<<< HEAD
 +	c = intel_shared_regs_constraints(cpuc, event);
 +	if (c)
++=======
+ 	return x86_get_event_constraints(cpuc, idx, event);
+ }
+ 
+ static void
+ intel_start_scheduling(struct cpu_hw_events *cpuc)
+ {
+ 	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
+ 	struct intel_excl_states *xl, *xlo;
+ 	int tid = cpuc->excl_thread_id;
+ 	int o_tid = 1 - tid; /* sibling thread */
+ 
+ 	/*
+ 	 * nothing needed if in group validation mode
+ 	 */
+ 	if (cpuc->is_fake || !is_ht_workaround_enabled())
+ 		return;
+ 
+ 	/*
+ 	 * no exclusion needed
+ 	 */
+ 	if (!excl_cntrs)
+ 		return;
+ 
+ 	xlo = &excl_cntrs->states[o_tid];
+ 	xl = &excl_cntrs->states[tid];
+ 
+ 	xl->sched_started = true;
+ 	xl->num_alloc_cntrs = 0;
+ 	/*
+ 	 * lock shared state until we are done scheduling
+ 	 * in stop_event_scheduling()
+ 	 * makes scheduling appear as a transaction
+ 	 */
+ 	WARN_ON_ONCE(!irqs_disabled());
+ 	raw_spin_lock(&excl_cntrs->lock);
+ 
+ 	/*
+ 	 * save initial state of sibling thread
+ 	 */
+ 	memcpy(xlo->init_state, xlo->state, sizeof(xlo->init_state));
+ }
+ 
+ static void
+ intel_stop_scheduling(struct cpu_hw_events *cpuc)
+ {
+ 	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
+ 	struct intel_excl_states *xl, *xlo;
+ 	int tid = cpuc->excl_thread_id;
+ 	int o_tid = 1 - tid; /* sibling thread */
+ 
+ 	/*
+ 	 * nothing needed if in group validation mode
+ 	 */
+ 	if (cpuc->is_fake || !is_ht_workaround_enabled())
+ 		return;
+ 	/*
+ 	 * no exclusion needed
+ 	 */
+ 	if (!excl_cntrs)
+ 		return;
+ 
+ 	xlo = &excl_cntrs->states[o_tid];
+ 	xl = &excl_cntrs->states[tid];
+ 
+ 	/*
+ 	 * make new sibling thread state visible
+ 	 */
+ 	memcpy(xlo->state, xlo->init_state, sizeof(xlo->state));
+ 
+ 	xl->sched_started = false;
+ 	/*
+ 	 * release shared state lock (acquired in intel_start_scheduling())
+ 	 */
+ 	raw_spin_unlock(&excl_cntrs->lock);
+ }
+ 
+ static struct event_constraint *
+ intel_get_excl_constraints(struct cpu_hw_events *cpuc, struct perf_event *event,
+ 			   int idx, struct event_constraint *c)
+ {
+ 	struct event_constraint *cx;
+ 	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
+ 	struct intel_excl_states *xl, *xlo;
+ 	int is_excl, i;
+ 	int tid = cpuc->excl_thread_id;
+ 	int o_tid = 1 - tid; /* alternate */
+ 
+ 	/*
+ 	 * validating a group does not require
+ 	 * enforcing cross-thread  exclusion
+ 	 */
+ 	if (cpuc->is_fake || !is_ht_workaround_enabled())
+ 		return c;
+ 
+ 	/*
+ 	 * no exclusion needed
+ 	 */
+ 	if (!excl_cntrs)
++>>>>>>> b37609c30e41 (perf/x86/intel: Make the HT bug workaround conditional on HT enabled)
  		return c;
 -	/*
 -	 * event requires exclusive counter access
 -	 * across HT threads
 -	 */
 -	is_excl = c->flags & PERF_X86_EVENT_EXCL;
 -
 -	/*
 -	 * xl = state of current HT
 -	 * xlo = state of sibling HT
 -	 */
 -	xl = &excl_cntrs->states[tid];
 -	xlo = &excl_cntrs->states[o_tid];
 -
 -	/*
 -	 * do not allow scheduling of more than max_alloc_cntrs
 -	 * which is set to half the available generic counters.
 -	 * this helps avoid counter starvation of sibling thread
 -	 * by ensuring at most half the counters cannot be in
 -	 * exclusive mode. There is not designated counters for the
 -	 * limits. Any N/2 counters can be used. This helps with
 -	 * events with specifix counter constraints
 -	 */
 -	if (xl->num_alloc_cntrs++ == xl->max_alloc_cntrs)
 -		return &emptyconstraint;
 -
 -	cx = c;
 -
 -	/*
 -	 * because we modify the constraint, we need
 -	 * to make a copy. Static constraints come
 -	 * from static const tables.
 -	 *
 -	 * only needed when constraint has not yet
 -	 * been cloned (marked dynamic)
 -	 */
 -	if (!(c->flags & PERF_X86_EVENT_DYNAMIC)) {
 -
 -		/* sanity check */
 -		if (idx < 0)
 -			return &emptyconstraint;
 -
 -		/*
 -		 * grab pre-allocated constraint entry
 -		 */
 -		cx = &cpuc->constraint_list[idx];
 -
 -		/*
 -		 * initialize dynamic constraint
 -		 * with static constraint
 -		 */
 -		memcpy(cx, c, sizeof(*cx));
  
 -		/*
 -		 * mark constraint as dynamic, so we
 -		 * can free it later on
 -		 */
 -		cx->flags |= PERF_X86_EVENT_DYNAMIC;
 -	}
 -
 -	/*
 -	 * From here on, the constraint is dynamic.
 -	 * Either it was just allocated above, or it
 -	 * was allocated during a earlier invocation
 -	 * of this function
 -	 */
 -
 -	/*
 -	 * Modify static constraint with current dynamic
 -	 * state of thread
 -	 *
 -	 * EXCLUSIVE: sibling counter measuring exclusive event
 -	 * SHARED   : sibling counter measuring non-exclusive event
 -	 * UNUSED   : sibling counter unused
 -	 */
 -	for_each_set_bit(i, cx->idxmsk, X86_PMC_IDX_MAX) {
 -		/*
 -		 * exclusive event in sibling counter
 -		 * our corresponding counter cannot be used
 -		 * regardless of our event
 -		 */
 -		if (xl->state[i] == INTEL_EXCL_EXCLUSIVE)
 -			__clear_bit(i, cx->idxmsk);
 -		/*
 -		 * if measuring an exclusive event, sibling
 -		 * measuring non-exclusive, then counter cannot
 -		 * be used
 -		 */
 -		if (is_excl && xl->state[i] == INTEL_EXCL_SHARED)
 -			__clear_bit(i, cx->idxmsk);
 -	}
 -
 -	/*
 -	 * recompute actual bit weight for scheduling algorithm
 -	 */
 -	cx->weight = hweight64(cx->idxmsk64);
 -
 -	/*
 -	 * if we return an empty mask, then switch
 -	 * back to static empty constraint to avoid
 -	 * the cost of freeing later on
 -	 */
 -	if (cx->weight == 0)
 -		cx = &emptyconstraint;
 -
 -	return cx;
 -}
 -
 -static struct event_constraint *
 -intel_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
 -			    struct perf_event *event)
 -{
 -	struct event_constraint *c1 = event->hw.constraint;
 -	struct event_constraint *c2;
 -
 -	/*
 -	 * first time only
 -	 * - static constraint: no change across incremental scheduling calls
 -	 * - dynamic constraint: handled by intel_get_excl_constraints()
 -	 */
 -	c2 = __intel_get_event_constraints(cpuc, idx, event);
 -	if (c1 && (c1->flags & PERF_X86_EVENT_DYNAMIC)) {
 -		bitmap_copy(c1->idxmsk, c2->idxmsk, X86_PMC_IDX_MAX);
 -		c1->weight = c2->weight;
 -		c2 = c1;
 -	}
 -
 -	if (cpuc->excl_cntrs)
 -		return intel_get_excl_constraints(cpuc, event, idx, c2);
 -
 -	return c2;
 -}
 -
 -static void intel_put_excl_constraints(struct cpu_hw_events *cpuc,
 -		struct perf_event *event)
 -{
 -	struct hw_perf_event *hwc = &event->hw;
 -	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
 -	struct intel_excl_states *xlo, *xl;
 -	unsigned long flags = 0; /* keep compiler happy */
 -	int tid = cpuc->excl_thread_id;
 -	int o_tid = 1 - tid;
 -
 -	/*
 -	 * nothing needed if in group validation mode
 -	 */
 -	if (cpuc->is_fake)
 -		return;
 -
 -	WARN_ON_ONCE(!excl_cntrs);
 -
 -	if (!excl_cntrs)
 -		return;
 -
 -	xl = &excl_cntrs->states[tid];
 -	xlo = &excl_cntrs->states[o_tid];
 -
 -	/*
 -	 * put_constraint may be called from x86_schedule_events()
 -	 * which already has the lock held so here make locking
 -	 * conditional
 -	 */
 -	if (!xl->sched_started)
 -		raw_spin_lock_irqsave(&excl_cntrs->lock, flags);
 -
 -	/*
 -	 * if event was actually assigned, then mark the
 -	 * counter state as unused now
 -	 */
 -	if (hwc->idx >= 0)
 -		xlo->state[hwc->idx] = INTEL_EXCL_UNUSED;
 -
 -	if (!xl->sched_started)
 -		raw_spin_unlock_irqrestore(&excl_cntrs->lock, flags);
 +	return x86_get_event_constraints(cpuc, idx, event);
  }
  
  static void
@@@ -2144,13 -2641,51 +2246,33 @@@ static void intel_pmu_cpu_starting(int 
  
  	if (x86_pmu.lbr_sel_map)
  		cpuc->lbr_sel = &cpuc->shared_regs->regs[EXTRA_REG_LBR];
 -
 -	if (x86_pmu.flags & PMU_FL_EXCL_CNTRS) {
 -		int h = x86_pmu.num_counters >> 1;
 -
 -		for_each_cpu(i, topology_thread_cpumask(cpu)) {
 -			struct intel_excl_cntrs *c;
 -
 -			c = per_cpu(cpu_hw_events, i).excl_cntrs;
 -			if (c && c->core_id == core_id) {
 -				cpuc->kfree_on_online[1] = cpuc->excl_cntrs;
 -				cpuc->excl_cntrs = c;
 -				cpuc->excl_thread_id = 1;
 -				break;
 -			}
 -		}
 -		cpuc->excl_cntrs->core_id = core_id;
 -		cpuc->excl_cntrs->refcnt++;
 -		/*
 -		 * set hard limit to half the number of generic counters
 -		 */
 -		cpuc->excl_cntrs->states[0].max_alloc_cntrs = h;
 -		cpuc->excl_cntrs->states[1].max_alloc_cntrs = h;
 -	}
  }
  
+ static void free_excl_cntrs(int cpu)
+ {
+ 	struct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);
++<<<<<<< HEAD
++	struct intel_shared_regs *pc;
++
++=======
+ 	struct intel_excl_cntrs *c;
+ 
+ 	c = cpuc->excl_cntrs;
+ 	if (c) {
+ 		if (c->core_id == -1 || --c->refcnt == 0)
+ 			kfree(c);
+ 		cpuc->excl_cntrs = NULL;
+ 		kfree(cpuc->constraint_list);
+ 		cpuc->constraint_list = NULL;
+ 	}
+ }
+ 
  static void intel_pmu_cpu_dying(int cpu)
  {
  	struct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);
  	struct intel_shared_regs *pc;
  
++>>>>>>> b37609c30e41 (perf/x86/intel: Make the HT bug workaround conditional on HT enabled)
  	pc = cpuc->shared_regs;
  	if (pc) {
  		if (pc->core_id == -1 || --pc->refcnt == 0)
* Unmerged path arch/x86/kernel/cpu/perf_event.h
* Unmerged path arch/x86/kernel/cpu/perf_event_intel.c
