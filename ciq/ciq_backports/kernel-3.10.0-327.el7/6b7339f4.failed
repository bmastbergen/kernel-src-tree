mm: avoid setting up anonymous pages into file mapping

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [mm] avoid setting up anonymous pages into file mapping (Larry Woodman) [1261582]
Rebuild_FUZZ: 96.15%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit 6b7339f4c31ad69c8e9c0b2859276e22cf72176d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/6b7339f4.failed

Reading page fault handler code I've noticed that under right
circumstances kernel would map anonymous pages into file mappings: if
the VMA doesn't have vm_ops->fault() and the VMA wasn't fully populated
on ->mmap(), kernel would handle page fault to not populated pte with
do_anonymous_page().

Let's change page fault handler to use do_anonymous_page() only on
anonymous VMA (->vm_ops == NULL) and make sure that the VMA is not
shared.

For file mappings without vm_ops->fault() or shred VMA without vm_ops,
page fault on pte_none() entry would lead to SIGBUS.

	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Acked-by: Oleg Nesterov <oleg@redhat.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Willy Tarreau <w@1wt.eu>
	Cc: stable@vger.kernel.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 6b7339f4c31ad69c8e9c0b2859276e22cf72176d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index 75338cb200d7,388dcf9aa283..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3235,78 -2670,257 +3235,82 @@@ static int do_anonymous_page(struct mm_
  
  	pte_unmap(page_table);
  
+ 	/* File mapping without ->vm_ops ? */
+ 	if (vma->vm_flags & VM_SHARED)
+ 		return VM_FAULT_SIGBUS;
+ 
  	/* Check if we need to add a guard page to the stack */
  	if (check_stack_guard_page(vma, address) < 0)
 -		return VM_FAULT_SIGSEGV;
 +		return VM_FAULT_SIGBUS;
  
  	/* Use the zero-page for reads */
 -	if (!(flags & FAULT_FLAG_WRITE) && !mm_forbids_zeropage(mm)) {
 +	if (!(flags & FAULT_FLAG_WRITE)) {
  		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
  						vma->vm_page_prot));
 -		page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
 -		if (!pte_none(*page_table))
 -			goto unlock;
 -		goto setpte;
 -	}
 -
 -	/* Allocate our own private page. */
 -	if (unlikely(anon_vma_prepare(vma)))
 -		goto oom;
 -	page = alloc_zeroed_user_highpage_movable(vma, address);
 -	if (!page)
 -		goto oom;
 -
 -	if (mem_cgroup_try_charge(page, mm, GFP_KERNEL, &memcg))
 -		goto oom_free_page;
 -
 -	/*
 -	 * The memory barrier inside __SetPageUptodate makes sure that
 -	 * preceeding stores to the page contents become visible before
 -	 * the set_pte_at() write.
 -	 */
 -	__SetPageUptodate(page);
 -
 -	entry = mk_pte(page, vma->vm_page_prot);
 -	if (vma->vm_flags & VM_WRITE)
 -		entry = pte_mkwrite(pte_mkdirty(entry));
 -
 -	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
 -	if (!pte_none(*page_table))
 -		goto release;
 -
 -	inc_mm_counter_fast(mm, MM_ANONPAGES);
 -	page_add_new_anon_rmap(page, vma, address);
 -	mem_cgroup_commit_charge(page, memcg, false);
 -	lru_cache_add_active_or_unevictable(page, vma);
 -setpte:
 -	set_pte_at(mm, address, page_table, entry);
 -
 -	/* No need to invalidate - it was non-present before */
 -	update_mmu_cache(vma, address, page_table);
 -unlock:
 -	pte_unmap_unlock(page_table, ptl);
 -	return 0;
 -release:
 -	mem_cgroup_cancel_charge(page, memcg);
 -	page_cache_release(page);
 -	goto unlock;
 -oom_free_page:
 -	page_cache_release(page);
 -oom:
 -	return VM_FAULT_OOM;
 -}
 -
 -/*
 - * The mmap_sem must have been held on entry, and may have been
 - * released depending on flags and vma->vm_ops->fault() return value.
 - * See filemap_fault() and __lock_page_retry().
 - */
 -static int __do_fault(struct vm_area_struct *vma, unsigned long address,
 -			pgoff_t pgoff, unsigned int flags,
 -			struct page *cow_page, struct page **page)
 -{
 -	struct vm_fault vmf;
 -	int ret;
 -
 -	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
 -	vmf.pgoff = pgoff;
 -	vmf.flags = flags;
 -	vmf.page = NULL;
 -	vmf.cow_page = cow_page;
 -
 -	ret = vma->vm_ops->fault(vma, &vmf);
 -	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 -		return ret;
 -	if (!vmf.page)
 -		goto out;
 -
 -	if (unlikely(PageHWPoison(vmf.page))) {
 -		if (ret & VM_FAULT_LOCKED)
 -			unlock_page(vmf.page);
 -		page_cache_release(vmf.page);
 -		return VM_FAULT_HWPOISON;
 -	}
 -
 -	if (unlikely(!(ret & VM_FAULT_LOCKED)))
 -		lock_page(vmf.page);
 -	else
 -		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
 -
 - out:
 -	*page = vmf.page;
 -	return ret;
 -}
 -
 -/**
 - * do_set_pte - setup new PTE entry for given page and add reverse page mapping.
 - *
 - * @vma: virtual memory area
 - * @address: user virtual address
 - * @page: page to map
 - * @pte: pointer to target page table entry
 - * @write: true, if new entry is writable
 - * @anon: true, if it's anonymous page
 - *
 - * Caller must hold page table lock relevant for @pte.
 - *
 - * Target users are page handler itself and implementations of
 - * vm_ops->map_pages.
 - */
 -void do_set_pte(struct vm_area_struct *vma, unsigned long address,
 -		struct page *page, pte_t *pte, bool write, bool anon)
 -{
 -	pte_t entry;
 -
 -	flush_icache_page(vma, page);
 -	entry = mk_pte(page, vma->vm_page_prot);
 -	if (write)
 -		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 -	if (anon) {
 -		inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
 -		page_add_new_anon_rmap(page, vma, address);
 -	} else {
 -		inc_mm_counter_fast(vma->vm_mm, MM_FILEPAGES);
 -		page_add_file_rmap(page);
 +		page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
 +		if (!pte_none(*page_table))
 +			goto unlock;
 +		goto setpte;
  	}
 -	set_pte_at(vma->vm_mm, address, pte, entry);
  
 -	/* no need to invalidate: a not-present page won't be cached */
 -	update_mmu_cache(vma, address, pte);
 -}
 +	/* Allocate our own private page. */
 +	if (unlikely(anon_vma_prepare(vma)))
 +		goto oom;
 +	page = alloc_zeroed_user_highpage_movable(vma, address);
 +	if (!page)
 +		goto oom;
 +	/*
 +	 * The memory barrier inside __SetPageUptodate makes sure that
 +	 * preceeding stores to the page contents become visible before
 +	 * the set_pte_at() write.
 +	 */
 +	__SetPageUptodate(page);
  
 -static unsigned long fault_around_bytes __read_mostly =
 -	rounddown_pow_of_two(65536);
 +	if (mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))
 +		goto oom_free_page;
  
 -#ifdef CONFIG_DEBUG_FS
 -static int fault_around_bytes_get(void *data, u64 *val)
 -{
 -	*val = fault_around_bytes;
 -	return 0;
 -}
 +	entry = mk_pte(page, vma->vm_page_prot);
 +	if (vma->vm_flags & VM_WRITE)
 +		entry = pte_mkwrite(pte_mkdirty(entry));
  
 -/*
 - * fault_around_pages() and fault_around_mask() expects fault_around_bytes
 - * rounded down to nearest page order. It's what do_fault_around() expects to
 - * see.
 - */
 -static int fault_around_bytes_set(void *data, u64 val)
 -{
 -	if (val / PAGE_SIZE > PTRS_PER_PTE)
 -		return -EINVAL;
 -	if (val > PAGE_SIZE)
 -		fault_around_bytes = rounddown_pow_of_two(val);
 -	else
 -		fault_around_bytes = PAGE_SIZE; /* rounddown_pow_of_two(0) is undefined */
 -	return 0;
 -}
 -DEFINE_SIMPLE_ATTRIBUTE(fault_around_bytes_fops,
 -		fault_around_bytes_get, fault_around_bytes_set, "%llu\n");
 +	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
 +	if (!pte_none(*page_table))
 +		goto release;
  
 -static int __init fault_around_debugfs(void)
 -{
 -	void *ret;
 +	inc_mm_counter_fast(mm, MM_ANONPAGES);
 +	page_add_new_anon_rmap(page, vma, address);
 +setpte:
 +	set_pte_at(mm, address, page_table, entry);
  
 -	ret = debugfs_create_file("fault_around_bytes", 0644, NULL, NULL,
 -			&fault_around_bytes_fops);
 -	if (!ret)
 -		pr_warn("Failed to create fault_around_bytes in debugfs");
 +	/* No need to invalidate - it was non-present before */
 +	update_mmu_cache(vma, address, page_table);
 +unlock:
 +	pte_unmap_unlock(page_table, ptl);
  	return 0;
 +release:
 +	mem_cgroup_uncharge_page(page);
 +	page_cache_release(page);
 +	goto unlock;
 +oom_free_page:
 +	page_cache_release(page);
 +oom:
 +	return VM_FAULT_OOM;
  }
 -late_initcall(fault_around_debugfs);
 -#endif
  
  /*
 - * do_fault_around() tries to map few pages around the fault address. The hope
 - * is that the pages will be needed soon and this will lower the number of
 - * faults to handle.
 - *
 - * It uses vm_ops->map_pages() to map the pages, which skips the page if it's
 - * not ready to be mapped: not up-to-date, locked, etc.
 + * __do_fault() tries to create a new page mapping. It aggressively
 + * tries to share with existing pages, but makes a separate copy if
 + * the FAULT_FLAG_WRITE is set in the flags parameter in order to avoid
 + * the next page fault.
   *
 - * This function is called with the page table lock taken. In the split ptlock
 - * case the page table lock only protects only those entries which belong to
 - * the page table corresponding to the fault address.
 - *
 - * This function doesn't cross the VMA boundaries, in order to call map_pages()
 - * only once.
 - *
 - * fault_around_pages() defines how many pages we'll try to map.
 - * do_fault_around() expects it to return a power of two less than or equal to
 - * PTRS_PER_PTE.
 + * As this is called only for pages that do not currently exist, we
 + * do not need to flush old virtual caches or the TLB.
   *
 - * The virtual address of the area that we map is naturally aligned to the
 - * fault_around_pages() value (and therefore to page order).  This way it's
 - * easier to guarantee that we don't cross page table boundaries.
 + * We enter with non-exclusive mmap_sem (to exclude vma changes,
 + * but allow concurrent faults), and pte neither mapped nor locked.
 + * We return with mmap_sem still held, but pte unmapped and unlocked.
   */
 -static void do_fault_around(struct vm_area_struct *vma, unsigned long address,
 -		pte_t *pte, pgoff_t pgoff, unsigned int flags)
 -{
 -	unsigned long start_addr, nr_pages, mask;
 -	pgoff_t max_pgoff;
 -	struct vm_fault vmf;
 -	int off;
 -
 -	nr_pages = READ_ONCE(fault_around_bytes) >> PAGE_SHIFT;
 -	mask = ~(nr_pages * PAGE_SIZE - 1) & PAGE_MASK;
 -
 -	start_addr = max(address & mask, vma->vm_start);
 -	off = ((address - start_addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
 -	pte -= off;
 -	pgoff -= off;
 -
 -	/*
 -	 *  max_pgoff is either end of page table or end of vma
 -	 *  or fault_around_pages() from pgoff, depending what is nearest.
 -	 */
 -	max_pgoff = pgoff - ((start_addr >> PAGE_SHIFT) & (PTRS_PER_PTE - 1)) +
 -		PTRS_PER_PTE - 1;
 -	max_pgoff = min3(max_pgoff, vma_pages(vma) + vma->vm_pgoff - 1,
 -			pgoff + nr_pages - 1);
 -
 -	/* Check if it makes any sense to call ->map_pages */
 -	while (!pte_none(*pte)) {
 -		if (++pgoff > max_pgoff)
 -			return;
 -		start_addr += PAGE_SIZE;
 -		if (start_addr >= vma->vm_end)
 -			return;
 -		pte++;
 -	}
 -
 -	vmf.virtual_address = (void __user *) start_addr;
 -	vmf.pte = pte;
 -	vmf.pgoff = pgoff;
 -	vmf.max_pgoff = max_pgoff;
 -	vmf.flags = flags;
 -	vma->vm_ops->map_pages(vma, &vmf);
 -}
 -
 -static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 +static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
  		unsigned long address, pmd_t *pmd,
  		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
  {
@@@ -3500,42 -3103,19 +3504,55 @@@ static int do_linear_fault(struct mm_st
  			- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
  
  	pte_unmap(page_table);
++<<<<<<< HEAD
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++=======
+ 	/* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
+ 	if (!vma->vm_ops->fault)
+ 		return VM_FAULT_SIGBUS;
+ 	if (!(flags & FAULT_FLAG_WRITE))
+ 		return do_read_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	if (!(vma->vm_flags & VM_SHARED))
+ 		return do_cow_fault(mm, vma, address, pmd, pgoff, flags,
+ 				orig_pte);
+ 	return do_shared_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
++>>>>>>> 6b7339f4c31a (mm: avoid setting up anonymous pages into file mapping)
 +}
 +
 +/*
 + * Fault of a previously existing named mapping. Repopulate the pte
 + * from the encoded file_pte if possible. This enables swappable
 + * nonlinear vmas.
 + *
 + * We enter with non-exclusive mmap_sem (to exclude vma changes,
 + * but allow concurrent faults), and pte mapped but not yet locked.
 + * We return with mmap_sem still held, but pte unmapped and unlocked.
 + */
 +static int do_nonlinear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 +		unsigned long address, pte_t *page_table, pmd_t *pmd,
 +		unsigned int flags, pte_t orig_pte)
 +{
 +	pgoff_t pgoff;
 +
 +	flags |= FAULT_FLAG_NONLINEAR;
 +
 +	if (!pte_unmap_same(mm, pmd, page_table, orig_pte))
 +		return 0;
 +
 +	if (unlikely(!(vma->vm_flags & VM_NONLINEAR))) {
 +		/*
 +		 * Page table corrupted: show pte and kill process.
 +		 */
 +		print_bad_pte(vma, address, orig_pte, NULL);
 +		return VM_FAULT_SIGBUS;
 +	}
 +
 +	pgoff = pte_to_pgoff(orig_pte);
 +	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
  }
  
 -static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 +int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
  				unsigned long addr, int page_nid,
  				int *flags)
  {
@@@ -3648,20 -3239,25 +3665,29 @@@ static int handle_pte_fault(struct mm_s
  	pte_t entry;
  	spinlock_t *ptl;
  
 -	/*
 -	 * some architectures can have larger ptes than wordsize,
 -	 * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and CONFIG_32BIT=y,
 -	 * so READ_ONCE or ACCESS_ONCE cannot guarantee atomic accesses.
 -	 * The code below just needs a consistent view for the ifs and
 -	 * we later double check anyway with the ptl lock held. So here
 -	 * a barrier will do.
 -	 */
  	entry = *pte;
 -	barrier();
  	if (!pte_present(entry)) {
  		if (pte_none(entry)) {
++<<<<<<< HEAD
 +			if (vma->vm_ops) {
 +				if (likely(vma->vm_ops->fault))
 +					return do_linear_fault(mm, vma, address,
 +						pte, pmd, flags, entry);
 +			}
 +			return do_anonymous_page(mm, vma, address,
 +						 pte, pmd, flags);
++=======
+ 			if (vma->vm_ops)
+ 				return do_fault(mm, vma, address, pte, pmd,
+ 						flags, entry);
+ 
+ 			return do_anonymous_page(mm, vma, address, pte, pmd,
+ 					flags);
++>>>>>>> 6b7339f4c31a (mm: avoid setting up anonymous pages into file mapping)
  		}
 +		if (pte_file(entry))
 +			return do_nonlinear_fault(mm, vma, address,
 +					pte, pmd, flags, entry);
  		return do_swap_page(mm, vma, address,
  					pte, pmd, flags, entry);
  	}
* Unmerged path mm/memory.c
