KVM: MMU: introduce slot_handle_level_range() and its helpers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [kvm] mmu: introduce slot_handle_level_range() and its helpers (Paolo Bonzini) [1202825]
Rebuild_FUZZ: 95.73%
commit-author Xiao Guangrong <guangrong.xiao@linux.intel.com>
commit 1bad2b2a3b158fbb19fef6cd563301b94b5c28b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/1bad2b2a.failed

There are several places walking all rmaps for the memslot so that
introduce common functions to cleanup the code

	Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 1bad2b2a3b158fbb19fef6cd563301b94b5c28b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 10d8f908481b,d1d072d70b7b..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -4318,9 -4454,78 +4318,83 @@@ void kvm_mmu_setup(struct kvm_vcpu *vcp
  	init_kvm_mmu(vcpu);
  }
  
++<<<<<<< HEAD
 +void kvm_mmu_slot_remove_write_access(struct kvm *kvm, int slot)
++=======
+ /* The return value indicates if tlb flush on all vcpus is needed. */
+ typedef bool (*slot_level_handler) (struct kvm *kvm, unsigned long *rmap);
+ 
+ /* The caller should hold mmu-lock before calling this function. */
+ static bool
+ slot_handle_level_range(struct kvm *kvm, struct kvm_memory_slot *memslot,
+ 			slot_level_handler fn, int start_level, int end_level,
+ 			gfn_t start_gfn, gfn_t end_gfn, bool lock_flush_tlb)
+ {
+ 	struct slot_rmap_walk_iterator iterator;
+ 	bool flush = false;
+ 
+ 	for_each_slot_rmap_range(memslot, start_level, end_level, start_gfn,
+ 			end_gfn, &iterator) {
+ 		if (iterator.rmap)
+ 			flush |= fn(kvm, iterator.rmap);
+ 
+ 		if (need_resched() || spin_needbreak(&kvm->mmu_lock)) {
+ 			if (flush && lock_flush_tlb) {
+ 				kvm_flush_remote_tlbs(kvm);
+ 				flush = false;
+ 			}
+ 			cond_resched_lock(&kvm->mmu_lock);
+ 		}
+ 	}
+ 
+ 	if (flush && lock_flush_tlb) {
+ 		kvm_flush_remote_tlbs(kvm);
+ 		flush = false;
+ 	}
+ 
+ 	return flush;
+ }
+ 
+ static bool
+ slot_handle_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
+ 		  slot_level_handler fn, int start_level, int end_level,
+ 		  bool lock_flush_tlb)
+ {
+ 	return slot_handle_level_range(kvm, memslot, fn, start_level,
+ 			end_level, memslot->base_gfn,
+ 			memslot->base_gfn + memslot->npages - 1,
+ 			lock_flush_tlb);
+ }
+ 
+ static bool
+ slot_handle_all_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
+ 		      slot_level_handler fn, bool lock_flush_tlb)
+ {
+ 	return slot_handle_level(kvm, memslot, fn, PT_PAGE_TABLE_LEVEL,
+ 				 PT_MAX_HUGEPAGE_LEVEL, lock_flush_tlb);
+ }
+ 
+ static bool
+ slot_handle_large_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
+ 			slot_level_handler fn, bool lock_flush_tlb)
+ {
+ 	return slot_handle_level(kvm, memslot, fn, PT_PAGE_TABLE_LEVEL + 1,
+ 				 PT_MAX_HUGEPAGE_LEVEL, lock_flush_tlb);
+ }
+ 
+ static bool
+ slot_handle_leaf(struct kvm *kvm, struct kvm_memory_slot *memslot,
+ 		 slot_level_handler fn, bool lock_flush_tlb)
+ {
+ 	return slot_handle_level(kvm, memslot, fn, PT_PAGE_TABLE_LEVEL,
+ 				 PT_PAGE_TABLE_LEVEL, lock_flush_tlb);
+ }
+ 
+ void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
+ 				      struct kvm_memory_slot *memslot)
++>>>>>>> 1bad2b2a3b15 (KVM: MMU: introduce slot_handle_level_range() and its helpers)
  {
 +	struct kvm_memory_slot *memslot;
  	gfn_t last_gfn;
  	int i;
  	bool flush = false;
* Unmerged path arch/x86/kvm/mmu.c
