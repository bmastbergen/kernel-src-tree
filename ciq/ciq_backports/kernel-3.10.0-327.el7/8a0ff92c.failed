hpsa: use helper routines for finishing commands

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Webb Scales <webbnh@hp.com>
commit 8a0ff92cc342e6be0f4db5183b27446796c15d91
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/8a0ff92c.failed

cleanup command completions

	Reviewed-by: Scott Teel <scott.teel@pmcs.com>
	Reviewed-by: Kevin Barnett <kevin.barnett@pmcs.com>
	Reviewed-by: Tomas Henzl <thenzl@redhat.com>
	Reviewed-by: Hannes Reinecke <hare@Suse.de>
	Signed-off-by: Webb Scales <webbnh@hp.com>
	Signed-off-by: Don Brace <don.brace@pmcs.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: James Bottomley <JBottomley@Odin.com>
(cherry picked from commit 8a0ff92cc342e6be0f4db5183b27446796c15d91)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/hpsa.c
diff --cc drivers/scsi/hpsa.c
index f57c783f05e4,4b56525a803a..000000000000
--- a/drivers/scsi/hpsa.c
+++ b/drivers/scsi/hpsa.c
@@@ -1665,24 -2005,45 +1676,66 @@@ static void process_ioaccel2_completion
  	if (is_logical_dev_addr_mode(dev->scsi3addr) &&
  		c2->error_data.serv_response ==
  			IOACCEL2_SERV_RESPONSE_FAILURE) {
++<<<<<<< HEAD
 +		dev->offload_enabled = 0;
 +		cmd->result = DID_SOFT_ERROR << 16;
 +		cmd_free(h, c);
 +		cmd->scsi_done(cmd);
 +		return;
 +	}
 +	raid_retry = handle_ioaccel_mode2_error(h, c, cmd, c2);
 +	/* If error found, disable Smart Path,
 +	 * force a retry on the standard path.
 +	 */
 +	if (raid_retry) {
 +		dev_warn(&h->pdev->dev, "%s: Retrying on standard path.\n",
 +			"HP SSD Smart Path");
 +		dev->offload_enabled = 0; /* Disable Smart Path */
 +		cmd->result = DID_SOFT_ERROR << 16;
 +	}
 +	cmd_free(h, c);
 +	cmd->scsi_done(cmd);
++=======
+ 		if (c2->error_data.status ==
+ 			IOACCEL2_STATUS_SR_IOACCEL_DISABLED)
+ 			dev->offload_enabled = 0;
+ 
+ 		return hpsa_retry_cmd(h, c);
+ 	}
+ 
+ 	if (handle_ioaccel_mode2_error(h, c, cmd, c2))
+ 		return hpsa_retry_cmd(h, c);
+ 
+ 	return hpsa_cmd_free_and_done(h, c, cmd);
+ }
+ 
+ /* Returns 0 on success, < 0 otherwise. */
+ static int hpsa_evaluate_tmf_status(struct ctlr_info *h,
+ 					struct CommandList *cp)
+ {
+ 	u8 tmf_status = cp->err_info->ScsiStatus;
+ 
+ 	switch (tmf_status) {
+ 	case CISS_TMF_COMPLETE:
+ 		/*
+ 		 * CISS_TMF_COMPLETE never happens, instead,
+ 		 * ei->CommandStatus == 0 for this case.
+ 		 */
+ 	case CISS_TMF_SUCCESS:
+ 		return 0;
+ 	case CISS_TMF_INVALID_FRAME:
+ 	case CISS_TMF_NOT_SUPPORTED:
+ 	case CISS_TMF_FAILED:
+ 	case CISS_TMF_WRONG_LUN:
+ 	case CISS_TMF_OVERLAPPED_TAG:
+ 		break;
+ 	default:
+ 		dev_warn(&h->pdev->dev, "Unknown TMF status: 0x%02x\n",
+ 				tmf_status);
+ 		break;
+ 	}
+ 	return -tmf_status;
++>>>>>>> 8a0ff92cc342 (hpsa: use helper routines for finishing commands)
  }
  
  static void complete_scsi_command(struct CommandList *cp)
@@@ -1710,27 -2073,30 +1763,49 @@@
  	cmd->result = (DID_OK << 16); 		/* host byte */
  	cmd->result |= (COMMAND_COMPLETE << 8);	/* msg byte */
  
++<<<<<<< HEAD
++=======
+ 	if (cp->cmd_type == CMD_IOACCEL2 || cp->cmd_type == CMD_IOACCEL1)
+ 		atomic_dec(&cp->phys_disk->ioaccel_cmds_out);
+ 
+ 	/*
+ 	 * We check for lockup status here as it may be set for
+ 	 * CMD_SCSI, CMD_IOACCEL1 and CMD_IOACCEL2 commands by
+ 	 * fail_all_oustanding_cmds()
+ 	 */
+ 	if (unlikely(ei->CommandStatus == CMD_CTLR_LOCKUP)) {
+ 		/* DID_NO_CONNECT will prevent a retry */
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		return hpsa_cmd_free_and_done(h, cp, cmd);
+ 	}
+ 
++>>>>>>> 8a0ff92cc342 (hpsa: use helper routines for finishing commands)
  	if (cp->cmd_type == CMD_IOACCEL2)
  		return process_ioaccel2_completion(h, cp, cmd, dev);
  
 +	cmd->result |= ei->ScsiStatus;
 +
  	scsi_set_resid(cmd, ei->ResidualCnt);
++<<<<<<< HEAD
 +	if (ei->CommandStatus == 0) {
 +		cmd_free(h, cp);
 +		cmd->scsi_done(cmd);
 +		return;
 +	}
++=======
+ 	if (ei->CommandStatus == 0)
+ 		return hpsa_cmd_free_and_done(h, cp, cmd);
++>>>>>>> 8a0ff92cc342 (hpsa: use helper routines for finishing commands)
 +
 +	/* copy the sense data */
 +	if (SCSI_SENSE_BUFFERSIZE < sizeof(ei->SenseInfo))
 +		sense_data_size = SCSI_SENSE_BUFFERSIZE;
 +	else
 +		sense_data_size = sizeof(ei->SenseInfo);
 +	if (ei->SenseLen < sense_data_size)
 +		sense_data_size = ei->SenseLen;
 +
 +	memcpy(cmd->sense_buffer, ei->SenseInfo, sense_data_size);
  
  	/* For I/O accelerator commands, copy over some fields to the normal
  	 * CISS header used below for error handling.
@@@ -1752,10 -2118,7 +1827,14 @@@
  		if (is_logical_dev_addr_mode(dev->scsi3addr)) {
  			if (ei->CommandStatus == CMD_IOACCEL_DISABLED)
  				dev->offload_enabled = 0;
++<<<<<<< HEAD
 +			cmd->result = DID_SOFT_ERROR << 16;
 +			cmd_free(h, cp);
 +			cmd->scsi_done(cmd);
 +			return;
++=======
+ 			return hpsa_retry_cmd(h, cp);
++>>>>>>> 8a0ff92cc342 (hpsa: use helper routines for finishing commands)
  		}
  	}
  
@@@ -3963,7 -4432,176 +4042,180 @@@ static int hpsa_scsi_queue_command(stru
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int do_not_scan_if_controller_locked_up(struct ctlr_info *h)
++=======
+ static void hpsa_cmd_init(struct ctlr_info *h, int index,
+ 				struct CommandList *c)
+ {
+ 	dma_addr_t cmd_dma_handle, err_dma_handle;
+ 
+ 	/* Zero out all of commandlist except the last field, refcount */
+ 	memset(c, 0, offsetof(struct CommandList, refcount));
+ 	c->Header.tag = cpu_to_le64((u64) (index << DIRECT_LOOKUP_SHIFT));
+ 	cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+ 	c->err_info = h->errinfo_pool + index;
+ 	memset(c->err_info, 0, sizeof(*c->err_info));
+ 	err_dma_handle = h->errinfo_pool_dhandle
+ 	    + index * sizeof(*c->err_info);
+ 	c->cmdindex = index;
+ 	c->busaddr = (u32) cmd_dma_handle;
+ 	c->ErrDesc.Addr = cpu_to_le64((u64) err_dma_handle);
+ 	c->ErrDesc.Len = cpu_to_le32((u32) sizeof(*c->err_info));
+ 	c->h = h;
+ }
+ 
+ static void hpsa_preinitialize_commands(struct ctlr_info *h)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < h->nr_cmds; i++) {
+ 		struct CommandList *c = h->cmd_pool + i;
+ 
+ 		hpsa_cmd_init(h, i, c);
+ 		atomic_set(&c->refcount, 0);
+ 	}
+ }
+ 
+ static inline void hpsa_cmd_partial_init(struct ctlr_info *h, int index,
+ 				struct CommandList *c)
+ {
+ 	dma_addr_t cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+ 
+ 	memset(c->Request.CDB, 0, sizeof(c->Request.CDB));
+ 	memset(c->err_info, 0, sizeof(*c->err_info));
+ 	c->busaddr = (u32) cmd_dma_handle;
+ }
+ 
+ static int hpsa_ioaccel_submit(struct ctlr_info *h,
+ 		struct CommandList *c, struct scsi_cmnd *cmd,
+ 		unsigned char *scsi3addr)
+ {
+ 	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
+ 	int rc = IO_ACCEL_INELIGIBLE;
+ 
+ 	cmd->host_scribble = (unsigned char *) c;
+ 
+ 	if (dev->offload_enabled) {
+ 		hpsa_cmd_init(h, c->cmdindex, c);
+ 		c->cmd_type = CMD_SCSI;
+ 		c->scsi_cmd = cmd;
+ 		rc = hpsa_scsi_ioaccel_raid_map(h, c);
+ 		if (rc < 0)     /* scsi_dma_map failed. */
+ 			rc = SCSI_MLQUEUE_HOST_BUSY;
+ 	} else if (dev->hba_ioaccel_enabled) {
+ 		hpsa_cmd_init(h, c->cmdindex, c);
+ 		c->cmd_type = CMD_SCSI;
+ 		c->scsi_cmd = cmd;
+ 		rc = hpsa_scsi_ioaccel_direct_map(h, c);
+ 		if (rc < 0)     /* scsi_dma_map failed. */
+ 			rc = SCSI_MLQUEUE_HOST_BUSY;
+ 	}
+ 	return rc;
+ }
+ 
+ static void hpsa_command_resubmit_worker(struct work_struct *work)
+ {
+ 	struct scsi_cmnd *cmd;
+ 	struct hpsa_scsi_dev_t *dev;
+ 	struct CommandList *c = container_of(work, struct CommandList, work);
+ 
+ 	cmd = c->scsi_cmd;
+ 	dev = cmd->device->hostdata;
+ 	if (!dev) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		return hpsa_cmd_free_and_done(c->h, c, cmd);
+ 	}
+ 	if (c->cmd_type == CMD_IOACCEL2) {
+ 		struct ctlr_info *h = c->h;
+ 		struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+ 		int rc;
+ 
+ 		if (c2->error_data.serv_response ==
+ 				IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL) {
+ 			rc = hpsa_ioaccel_submit(h, c, cmd, dev->scsi3addr);
+ 			if (rc == 0)
+ 				return;
+ 			if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+ 				/*
+ 				 * If we get here, it means dma mapping failed.
+ 				 * Try again via scsi mid layer, which will
+ 				 * then get SCSI_MLQUEUE_HOST_BUSY.
+ 				 */
+ 				cmd->result = DID_IMM_RETRY << 16;
+ 				return hpsa_cmd_free_and_done(h, c, cmd);
+ 			}
+ 			/* else, fall thru and resubmit down CISS path */
+ 		}
+ 	}
+ 	hpsa_cmd_partial_init(c->h, c->cmdindex, c);
+ 	if (hpsa_ciss_submit(c->h, c, cmd, dev->scsi3addr)) {
+ 		/*
+ 		 * If we get here, it means dma mapping failed. Try
+ 		 * again via scsi mid layer, which will then get
+ 		 * SCSI_MLQUEUE_HOST_BUSY.
+ 		 *
+ 		 * hpsa_ciss_submit will have already freed c
+ 		 * if it encountered a dma mapping failure.
+ 		 */
+ 		cmd->result = DID_IMM_RETRY << 16;
+ 		cmd->scsi_done(cmd);
+ 	}
+ }
+ 
+ /* Running in struct Scsi_Host->host_lock less mode */
+ static int hpsa_scsi_queue_command(struct Scsi_Host *sh, struct scsi_cmnd *cmd)
+ {
+ 	struct ctlr_info *h;
+ 	struct hpsa_scsi_dev_t *dev;
+ 	unsigned char scsi3addr[8];
+ 	struct CommandList *c;
+ 	int rc = 0;
+ 
+ 	/* Get the ptr to our adapter structure out of cmd->host. */
+ 	h = sdev_to_hba(cmd->device);
+ 	dev = cmd->device->hostdata;
+ 	if (!dev) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 	memcpy(scsi3addr, dev->scsi3addr, sizeof(scsi3addr));
+ 
+ 	if (unlikely(lockup_detected(h))) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 	c = cmd_alloc(h);
+ 
+ 	if (unlikely(lockup_detected(h))) {
+ 		cmd->result = DID_NO_CONNECT << 16;
+ 		cmd_free(h, c);
+ 		cmd->scsi_done(cmd);
+ 		return 0;
+ 	}
+ 
+ 	/*
+ 	 * Call alternate submit routine for I/O accelerated commands.
+ 	 * Retries always go down the normal I/O path.
+ 	 */
+ 	if (likely(cmd->retries == 0 &&
+ 		cmd->request->cmd_type == REQ_TYPE_FS &&
+ 		h->acciopath_status)) {
+ 		rc = hpsa_ioaccel_submit(h, c, cmd, scsi3addr);
+ 		if (rc == 0)
+ 			return 0;
+ 		if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+ 			cmd_free(h, c);
+ 			return SCSI_MLQUEUE_HOST_BUSY;
+ 		}
+ 	}
+ 	return hpsa_ciss_submit(h, c, cmd, scsi3addr);
+ }
+ 
+ static void hpsa_scan_complete(struct ctlr_info *h)
++>>>>>>> 8a0ff92cc342 (hpsa: use helper routines for finishing commands)
  {
  	unsigned long flags;
  
@@@ -6893,8 -7705,8 +7145,11 @@@ static void hpsa_flush_cache(struct ctl
  {
  	char *flush_buf;
  	struct CommandList *c;
 -	int rc;
  
++<<<<<<< HEAD
 +	/* Don't bother trying to flush the cache if locked up */
++=======
++>>>>>>> 8a0ff92cc342 (hpsa: use helper routines for finishing commands)
  	if (unlikely(lockup_detected(h)))
  		return;
  	flush_buf = kzalloc(4, GFP_KERNEL);
* Unmerged path drivers/scsi/hpsa.c
