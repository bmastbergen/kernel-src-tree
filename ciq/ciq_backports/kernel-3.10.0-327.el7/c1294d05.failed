userfaultfd: prevent khugepaged to merge if userfaultfd is armed

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Andrea Arcangeli <aarcange@redhat.com>
commit c1294d05de5df1ab8c93aa13c531782ede907e14
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/c1294d05.failed

If userfaultfd is armed on a certain vma we can't "fill" the holes with
zeroes or we'll break the userland on demand paging.  The holes if the
userfault is armed, are really missing information (not zeroes) that the
userland has to load from network or elsewhere.

The same issue happens for wrprotected ptes that we can't just convert
into a single writable pmd_trans_huge.

We could however in theory still merge across zeropages if only
VM_UFFD_MISSING is set (so if VM_UFFD_WP is not set)...  that could be
slightly improved but it'd be much more complex code for a tiny corner
case.

	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Acked-by: Pavel Emelyanov <xemul@parallels.com>
	Cc: Sanidhya Kashyap <sanidhya.gatech@gmail.com>
	Cc: zhang.zhanghailiang@huawei.com
	Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
	Cc: Andres Lagar-Cavilla <andreslc@google.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Peter Feiner <pfeiner@google.com>
	Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: "Huangpeng (Peter)" <peter.huangpeng@huawei.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c1294d05de5df1ab8c93aa13c531782ede907e14)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index bff17a8147f4,d38aaf9dcba6..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -2170,8 -2157,9 +2170,14 @@@ static int __collapse_huge_page_isolate
  	for (_pte = pte; _pte < pte+HPAGE_PMD_NR;
  	     _pte++, address += PAGE_SIZE) {
  		pte_t pteval = *_pte;
++<<<<<<< HEAD
 +		if (pte_none(pteval)) {
 +			if (++none <= khugepaged_max_ptes_none)
++=======
+ 		if (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {
+ 			if (!userfaultfd_armed(vma) &&
+ 			    ++none_or_zero <= khugepaged_max_ptes_none)
++>>>>>>> c1294d05de5d (userfaultfd: prevent khugepaged to merge if userfaultfd is armed)
  				continue;
  			else
  				goto out;
@@@ -2560,8 -2611,9 +2566,14 @@@ static int khugepaged_scan_pmd(struct m
  	for (_address = address, _pte = pte; _pte < pte+HPAGE_PMD_NR;
  	     _pte++, _address += PAGE_SIZE) {
  		pte_t pteval = *_pte;
++<<<<<<< HEAD
 +		if (pte_none(pteval)) {
 +			if (++none <= khugepaged_max_ptes_none)
++=======
+ 		if (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {
+ 			if (!userfaultfd_armed(vma) &&
+ 			    ++none_or_zero <= khugepaged_max_ptes_none)
++>>>>>>> c1294d05de5d (userfaultfd: prevent khugepaged to merge if userfaultfd is armed)
  				continue;
  			else
  				goto out_unmap;
* Unmerged path mm/huge_memory.c
