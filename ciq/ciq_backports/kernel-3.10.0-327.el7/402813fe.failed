KVM: PPC: Book3S HV: Fix preempted vcore list locking

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [powerpc] kvm: book3s_hv: Fix preempted vcore list locking (Laurent Vivier) [1242757]
Rebuild_FUZZ: 93.07%
commit-author Paul Mackerras <paulus@samba.org>
commit 402813fe39db66e8f3be2a1b5b62dd664e33f6b8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/402813fe.failed

When a vcore gets preempted, we put it on the preempted vcore list for
the current CPU.  The runner task then calls schedule() and comes back
some time later and takes itself off the list.  We need to be careful
to lock the list that it was put onto, which may not be the list for the
current CPU since the runner task may have moved to another CPU.

	Signed-off-by: Paul Mackerras <paulus@samba.org>
	Signed-off-by: Alexander Graf <agraf@suse.de>
(cherry picked from commit 402813fe39db66e8f3be2a1b5b62dd664e33f6b8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv.c
diff --cc arch/powerpc/kvm/book3s_hv.c
index 077343fa2903,3d022766294a..000000000000
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@@ -1676,6 -1921,278 +1676,281 @@@ static void kvmppc_start_restoring_l2_c
  	mtspr(SPRN_MPPR, mpp_addr | PPC_MPPR_FETCH_WHOLE_TABLE);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * A list of virtual cores for each physical CPU.
+  * These are vcores that could run but their runner VCPU tasks are
+  * (or may be) preempted.
+  */
+ struct preempted_vcore_list {
+ 	struct list_head	list;
+ 	spinlock_t		lock;
+ };
+ 
+ static DEFINE_PER_CPU(struct preempted_vcore_list, preempted_vcores);
+ 
+ static void init_vcore_lists(void)
+ {
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct preempted_vcore_list *lp = &per_cpu(preempted_vcores, cpu);
+ 		spin_lock_init(&lp->lock);
+ 		INIT_LIST_HEAD(&lp->list);
+ 	}
+ }
+ 
+ static void kvmppc_vcore_preempt(struct kvmppc_vcore *vc)
+ {
+ 	struct preempted_vcore_list *lp = this_cpu_ptr(&preempted_vcores);
+ 
+ 	vc->vcore_state = VCORE_PREEMPT;
+ 	vc->pcpu = smp_processor_id();
+ 	if (vc->num_threads < threads_per_subcore) {
+ 		spin_lock(&lp->lock);
+ 		list_add_tail(&vc->preempt_list, &lp->list);
+ 		spin_unlock(&lp->lock);
+ 	}
+ 
+ 	/* Start accumulating stolen time */
+ 	kvmppc_core_start_stolen(vc);
+ }
+ 
+ static void kvmppc_vcore_end_preempt(struct kvmppc_vcore *vc)
+ {
+ 	struct preempted_vcore_list *lp;
+ 
+ 	kvmppc_core_end_stolen(vc);
+ 	if (!list_empty(&vc->preempt_list)) {
+ 		lp = &per_cpu(preempted_vcores, vc->pcpu);
+ 		spin_lock(&lp->lock);
+ 		list_del_init(&vc->preempt_list);
+ 		spin_unlock(&lp->lock);
+ 	}
+ 	vc->vcore_state = VCORE_INACTIVE;
+ }
+ 
+ /*
+  * This stores information about the virtual cores currently
+  * assigned to a physical core.
+  */
+ struct core_info {
+ 	int		n_subcores;
+ 	int		max_subcore_threads;
+ 	int		total_threads;
+ 	int		subcore_threads[MAX_SUBCORES];
+ 	struct kvm	*subcore_vm[MAX_SUBCORES];
+ 	struct list_head vcs[MAX_SUBCORES];
+ };
+ 
+ /*
+  * This mapping means subcores 0 and 1 can use threads 0-3 and 4-7
+  * respectively in 2-way micro-threading (split-core) mode.
+  */
+ static int subcore_thread_map[MAX_SUBCORES] = { 0, 4, 2, 6 };
+ 
+ static void init_core_info(struct core_info *cip, struct kvmppc_vcore *vc)
+ {
+ 	int sub;
+ 
+ 	memset(cip, 0, sizeof(*cip));
+ 	cip->n_subcores = 1;
+ 	cip->max_subcore_threads = vc->num_threads;
+ 	cip->total_threads = vc->num_threads;
+ 	cip->subcore_threads[0] = vc->num_threads;
+ 	cip->subcore_vm[0] = vc->kvm;
+ 	for (sub = 0; sub < MAX_SUBCORES; ++sub)
+ 		INIT_LIST_HEAD(&cip->vcs[sub]);
+ 	list_add_tail(&vc->preempt_list, &cip->vcs[0]);
+ }
+ 
+ static bool subcore_config_ok(int n_subcores, int n_threads)
+ {
+ 	/* Can only dynamically split if unsplit to begin with */
+ 	if (n_subcores > 1 && threads_per_subcore < MAX_SMT_THREADS)
+ 		return false;
+ 	if (n_subcores > MAX_SUBCORES)
+ 		return false;
+ 	if (n_subcores > 1) {
+ 		if (!(dynamic_mt_modes & 2))
+ 			n_subcores = 4;
+ 		if (n_subcores > 2 && !(dynamic_mt_modes & 4))
+ 			return false;
+ 	}
+ 
+ 	return n_subcores * roundup_pow_of_two(n_threads) <= MAX_SMT_THREADS;
+ }
+ 
+ static void init_master_vcore(struct kvmppc_vcore *vc)
+ {
+ 	vc->master_vcore = vc;
+ 	vc->entry_exit_map = 0;
+ 	vc->in_guest = 0;
+ 	vc->napping_threads = 0;
+ 	vc->conferring_threads = 0;
+ }
+ 
+ /*
+  * See if the existing subcores can be split into 3 (or fewer) subcores
+  * of at most two threads each, so we can fit in another vcore.  This
+  * assumes there are at most two subcores and at most 6 threads in total.
+  */
+ static bool can_split_piggybacked_subcores(struct core_info *cip)
+ {
+ 	int sub, new_sub;
+ 	int large_sub = -1;
+ 	int thr;
+ 	int n_subcores = cip->n_subcores;
+ 	struct kvmppc_vcore *vc, *vcnext;
+ 	struct kvmppc_vcore *master_vc = NULL;
+ 
+ 	for (sub = 0; sub < cip->n_subcores; ++sub) {
+ 		if (cip->subcore_threads[sub] <= 2)
+ 			continue;
+ 		if (large_sub >= 0)
+ 			return false;
+ 		large_sub = sub;
+ 		vc = list_first_entry(&cip->vcs[sub], struct kvmppc_vcore,
+ 				      preempt_list);
+ 		if (vc->num_threads > 2)
+ 			return false;
+ 		n_subcores += (cip->subcore_threads[sub] - 1) >> 1;
+ 	}
+ 	if (n_subcores > 3 || large_sub < 0)
+ 		return false;
+ 
+ 	/*
+ 	 * Seems feasible, so go through and move vcores to new subcores.
+ 	 * Note that when we have two or more vcores in one subcore,
+ 	 * all those vcores must have only one thread each.
+ 	 */
+ 	new_sub = cip->n_subcores;
+ 	thr = 0;
+ 	sub = large_sub;
+ 	list_for_each_entry_safe(vc, vcnext, &cip->vcs[sub], preempt_list) {
+ 		if (thr >= 2) {
+ 			list_del(&vc->preempt_list);
+ 			list_add_tail(&vc->preempt_list, &cip->vcs[new_sub]);
+ 			/* vc->num_threads must be 1 */
+ 			if (++cip->subcore_threads[new_sub] == 1) {
+ 				cip->subcore_vm[new_sub] = vc->kvm;
+ 				init_master_vcore(vc);
+ 				master_vc = vc;
+ 				++cip->n_subcores;
+ 			} else {
+ 				vc->master_vcore = master_vc;
+ 				++new_sub;
+ 			}
+ 		}
+ 		thr += vc->num_threads;
+ 	}
+ 	cip->subcore_threads[large_sub] = 2;
+ 	cip->max_subcore_threads = 2;
+ 
+ 	return true;
+ }
+ 
+ static bool can_dynamic_split(struct kvmppc_vcore *vc, struct core_info *cip)
+ {
+ 	int n_threads = vc->num_threads;
+ 	int sub;
+ 
+ 	if (!cpu_has_feature(CPU_FTR_ARCH_207S))
+ 		return false;
+ 
+ 	if (n_threads < cip->max_subcore_threads)
+ 		n_threads = cip->max_subcore_threads;
+ 	if (subcore_config_ok(cip->n_subcores + 1, n_threads)) {
+ 		cip->max_subcore_threads = n_threads;
+ 	} else if (cip->n_subcores <= 2 && cip->total_threads <= 6 &&
+ 		   vc->num_threads <= 2) {
+ 		/*
+ 		 * We may be able to fit another subcore in by
+ 		 * splitting an existing subcore with 3 or 4
+ 		 * threads into two 2-thread subcores, or one
+ 		 * with 5 or 6 threads into three subcores.
+ 		 * We can only do this if those subcores have
+ 		 * piggybacked virtual cores.
+ 		 */
+ 		if (!can_split_piggybacked_subcores(cip))
+ 			return false;
+ 	} else {
+ 		return false;
+ 	}
+ 
+ 	sub = cip->n_subcores;
+ 	++cip->n_subcores;
+ 	cip->total_threads += vc->num_threads;
+ 	cip->subcore_threads[sub] = vc->num_threads;
+ 	cip->subcore_vm[sub] = vc->kvm;
+ 	init_master_vcore(vc);
+ 	list_del(&vc->preempt_list);
+ 	list_add_tail(&vc->preempt_list, &cip->vcs[sub]);
+ 
+ 	return true;
+ }
+ 
+ static bool can_piggyback_subcore(struct kvmppc_vcore *pvc,
+ 				  struct core_info *cip, int sub)
+ {
+ 	struct kvmppc_vcore *vc;
+ 	int n_thr;
+ 
+ 	vc = list_first_entry(&cip->vcs[sub], struct kvmppc_vcore,
+ 			      preempt_list);
+ 
+ 	/* require same VM and same per-core reg values */
+ 	if (pvc->kvm != vc->kvm ||
+ 	    pvc->tb_offset != vc->tb_offset ||
+ 	    pvc->pcr != vc->pcr ||
+ 	    pvc->lpcr != vc->lpcr)
+ 		return false;
+ 
+ 	/* P8 guest with > 1 thread per core would see wrong TIR value */
+ 	if (cpu_has_feature(CPU_FTR_ARCH_207S) &&
+ 	    (vc->num_threads > 1 || pvc->num_threads > 1))
+ 		return false;
+ 
+ 	n_thr = cip->subcore_threads[sub] + pvc->num_threads;
+ 	if (n_thr > cip->max_subcore_threads) {
+ 		if (!subcore_config_ok(cip->n_subcores, n_thr))
+ 			return false;
+ 		cip->max_subcore_threads = n_thr;
+ 	}
+ 
+ 	cip->total_threads += pvc->num_threads;
+ 	cip->subcore_threads[sub] = n_thr;
+ 	pvc->master_vcore = vc;
+ 	list_del(&pvc->preempt_list);
+ 	list_add_tail(&pvc->preempt_list, &cip->vcs[sub]);
+ 
+ 	return true;
+ }
+ 
+ /*
+  * Work out whether it is possible to piggyback the execution of
+  * vcore *pvc onto the execution of the other vcores described in *cip.
+  */
+ static bool can_piggyback(struct kvmppc_vcore *pvc, struct core_info *cip,
+ 			  int target_threads)
+ {
+ 	int sub;
+ 
+ 	if (cip->total_threads + pvc->num_threads > target_threads)
+ 		return false;
+ 	for (sub = 0; sub < cip->n_subcores; ++sub)
+ 		if (cip->subcore_threads[sub] &&
+ 		    can_piggyback_subcore(pvc, cip, sub))
+ 			return true;
+ 
+ 	if (can_dynamic_split(pvc, cip))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
++>>>>>>> 402813fe39db (KVM: PPC: Book3S HV: Fix preempted vcore list locking)
  static void prepare_threads(struct kvmppc_vcore *vc)
  {
  	struct kvm_vcpu *vcpu, *vnext;
* Unmerged path arch/powerpc/kvm/book3s_hv.c
