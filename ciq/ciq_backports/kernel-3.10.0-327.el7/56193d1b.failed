net: Add function for parsing the header length out of linear ethernet frames

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [net] flow: Add function for parsing the header length out of linear ethernet frames (Ivan Vecera) [1200759]
Rebuild_FUZZ: 95.48%
commit-author Alexander Duyck <alexander.h.duyck@intel.com>
commit 56193d1bce2b2759cb4bdcc00cd05544894a0c90
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/56193d1b.failed

This patch updates some of the flow_dissector api so that it can be used to
parse the length of ethernet buffers stored in fragments.  Most of the
changes needed were to __skb_get_poff as it needed to be updated to support
sending a linear buffer instead of a skb.

I have split __skb_get_poff into two functions, the first is skb_get_poff
and it retains the functionality of the original __skb_get_poff.  The other
function is __skb_get_poff which now works much like __skb_flow_dissect in
relation to skb_flow_dissect in that it provides the same functionality but
works with just a data buffer and hlen instead of needing an skb.

	Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
	Acked-by: Alexei Starovoitov <ast@plumgrid.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 56193d1bce2b2759cb4bdcc00cd05544894a0c90)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/etherdevice.h
#	include/linux/skbuff.h
#	include/net/flow_keys.h
#	net/core/filter.c
#	net/core/flow_dissector.c
diff --cc include/linux/etherdevice.h
index 32e2b3823810,733980fce8e3..000000000000
--- a/include/linux/etherdevice.h
+++ b/include/linux/etherdevice.h
@@@ -26,29 -26,28 +26,34 @@@
  #include <linux/netdevice.h>
  #include <linux/random.h>
  #include <asm/unaligned.h>
 -#include <asm/bitsperlong.h>
  
  #ifdef __KERNEL__
++<<<<<<< HEAD
 +extern __be16		eth_type_trans(struct sk_buff *skb, struct net_device *dev);
++=======
+ u32 eth_get_headlen(void *data, unsigned int max_len);
+ __be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev);
++>>>>>>> 56193d1bce2b (net: Add function for parsing the header length out of linear ethernet frames)
  extern const struct header_ops eth_header_ops;
  
 -int eth_header(struct sk_buff *skb, struct net_device *dev, unsigned short type,
 -	       const void *daddr, const void *saddr, unsigned len);
 -int eth_rebuild_header(struct sk_buff *skb);
 -int eth_header_parse(const struct sk_buff *skb, unsigned char *haddr);
 -int eth_header_cache(const struct neighbour *neigh, struct hh_cache *hh,
 -		     __be16 type);
 -void eth_header_cache_update(struct hh_cache *hh, const struct net_device *dev,
 -			     const unsigned char *haddr);
 -int eth_prepare_mac_addr_change(struct net_device *dev, void *p);
 -void eth_commit_mac_addr_change(struct net_device *dev, void *p);
 -int eth_mac_addr(struct net_device *dev, void *p);
 -int eth_change_mtu(struct net_device *dev, int new_mtu);
 -int eth_validate_addr(struct net_device *dev);
 -
 -struct net_device *alloc_etherdev_mqs(int sizeof_priv, unsigned int txqs,
 +extern int eth_header(struct sk_buff *skb, struct net_device *dev,
 +		      unsigned short type,
 +		      const void *daddr, const void *saddr, unsigned len);
 +extern int eth_rebuild_header(struct sk_buff *skb);
 +extern int eth_header_parse(const struct sk_buff *skb, unsigned char *haddr);
 +extern int eth_header_cache(const struct neighbour *neigh, struct hh_cache *hh, __be16 type);
 +extern void eth_header_cache_update(struct hh_cache *hh,
 +				    const struct net_device *dev,
 +				    const unsigned char *haddr);
 +extern int eth_prepare_mac_addr_change(struct net_device *dev, void *p);
 +extern void eth_commit_mac_addr_change(struct net_device *dev, void *p);
 +extern int eth_mac_addr(struct net_device *dev, void *p);
 +extern int eth_change_mtu(struct net_device *dev, int new_mtu);
 +extern int eth_validate_addr(struct net_device *dev);
 +
 +
 +
 +extern struct net_device *alloc_etherdev_mqs(int sizeof_priv, unsigned int txqs,
  					    unsigned int rxqs);
  #define alloc_etherdev(sizeof_priv) alloc_etherdev_mq(sizeof_priv, 1)
  #define alloc_etherdev_mq(sizeof_priv, count) alloc_etherdev_mqs(sizeof_priv, count, count)
diff --cc include/linux/skbuff.h
index af2c639c5324,07c9fdd0c126..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -3241,7 -3216,11 +3241,15 @@@ static inline void skb_checksum_none_as
  
  bool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off);
  
++<<<<<<< HEAD
 +u32 __skb_get_poff(const struct sk_buff *skb);
++=======
+ int skb_checksum_setup(struct sk_buff *skb, bool recalculate);
+ 
+ u32 skb_get_poff(const struct sk_buff *skb);
+ u32 __skb_get_poff(const struct sk_buff *skb, void *data,
+ 		   const struct flow_keys *keys, int hlen);
++>>>>>>> 56193d1bce2b (net: Add function for parsing the header length out of linear ethernet frames)
  
  /**
   * skb_head_is_locked - Determine if the skb->head is locked down
diff --cc include/net/flow_keys.h
index d4fec0ba6827,7ee2df083542..000000000000
--- a/include/net/flow_keys.h
+++ b/include/net/flow_keys.h
@@@ -27,5 -27,19 +27,23 @@@ struct flow_keys 
  	u8 ip_proto;
  };
  
++<<<<<<< HEAD
 +extern bool skb_flow_dissect(const struct sk_buff *skb, struct flow_keys *flow);
++=======
+ bool __skb_flow_dissect(const struct sk_buff *skb, struct flow_keys *flow,
+ 			void *data, __be16 proto, int nhoff, int hlen);
+ static inline bool skb_flow_dissect(const struct sk_buff *skb, struct flow_keys *flow)
+ {
+ 	return __skb_flow_dissect(skb, flow, NULL, 0, 0, 0);
+ }
+ __be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,
+ 			    void *data, int hlen_proto);
+ static inline __be32 skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto)
+ {
+ 	return __skb_flow_get_ports(skb, thoff, ip_proto, NULL, 0);
+ }
+ u32 flow_hash_from_keys(struct flow_keys *keys);
+ unsigned int flow_get_hlen(const unsigned char *data, unsigned int max_len,
+ 			   __be16 protocol);
++>>>>>>> 56193d1bce2b (net: Add function for parsing the header length out of linear ethernet frames)
  #endif
diff --cc net/core/filter.c
index 01bd4de9aefe,fa5b7d0f77ac..000000000000
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@@ -108,312 -87,515 +108,539 @@@ int sk_filter(struct sock *sk, struct s
  }
  EXPORT_SYMBOL(sk_filter);
  
++<<<<<<< HEAD
++=======
+ /* Helper to find the offset of pkt_type in sk_buff structure. We want
+  * to make sure its still a 3bit field starting at a byte boundary;
+  * taken from arch/x86/net/bpf_jit_comp.c.
+  */
+ #ifdef __BIG_ENDIAN_BITFIELD
+ #define PKT_TYPE_MAX	(7 << 5)
+ #else
+ #define PKT_TYPE_MAX	7
+ #endif
+ static unsigned int pkt_type_offset(void)
+ {
+ 	struct sk_buff skb_probe = { .pkt_type = ~0, };
+ 	u8 *ct = (u8 *) &skb_probe;
+ 	unsigned int off;
+ 
+ 	for (off = 0; off < sizeof(struct sk_buff); off++) {
+ 		if (ct[off] == PKT_TYPE_MAX)
+ 			return off;
+ 	}
+ 
+ 	pr_err_once("Please fix %s, as pkt_type couldn't be found!\n", __func__);
+ 	return -1;
+ }
+ 
+ static u64 __skb_get_pay_offset(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	return skb_get_poff((struct sk_buff *)(unsigned long) ctx);
+ }
+ 
+ static u64 __skb_get_nlattr(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *)(unsigned long) ctx;
+ 	struct nlattr *nla;
+ 
+ 	if (skb_is_nonlinear(skb))
+ 		return 0;
+ 
+ 	if (skb->len < sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	if (a > skb->len - sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	nla = nla_find((struct nlattr *) &skb->data[a], skb->len - a, x);
+ 	if (nla)
+ 		return (void *) nla - (void *) skb->data;
+ 
+ 	return 0;
+ }
+ 
+ static u64 __skb_get_nlattr_nest(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	struct sk_buff *skb = (struct sk_buff *)(unsigned long) ctx;
+ 	struct nlattr *nla;
+ 
+ 	if (skb_is_nonlinear(skb))
+ 		return 0;
+ 
+ 	if (skb->len < sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	if (a > skb->len - sizeof(struct nlattr))
+ 		return 0;
+ 
+ 	nla = (struct nlattr *) &skb->data[a];
+ 	if (nla->nla_len > skb->len - a)
+ 		return 0;
+ 
+ 	nla = nla_find_nested(nla, x);
+ 	if (nla)
+ 		return (void *) nla - (void *) skb->data;
+ 
+ 	return 0;
+ }
+ 
+ static u64 __get_raw_cpu_id(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	return raw_smp_processor_id();
+ }
+ 
+ /* note that this only generates 32-bit random numbers */
+ static u64 __get_random_u32(u64 ctx, u64 a, u64 x, u64 r4, u64 r5)
+ {
+ 	return prandom_u32();
+ }
+ 
+ static bool convert_bpf_extensions(struct sock_filter *fp,
+ 				   struct bpf_insn **insnp)
+ {
+ 	struct bpf_insn *insn = *insnp;
+ 
+ 	switch (fp->k) {
+ 	case SKF_AD_OFF + SKF_AD_PROTOCOL:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
+ 
+ 		/* A = *(u16 *) (CTX + offsetof(protocol)) */
+ 		*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
+ 				      offsetof(struct sk_buff, protocol));
+ 		/* A = ntohs(A) [emitting a nop or swap16] */
+ 		*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_PKTTYPE:
+ 		*insn = BPF_LDX_MEM(BPF_B, BPF_REG_A, BPF_REG_CTX,
+ 				    pkt_type_offset());
+ 		if (insn->off < 0)
+ 			return false;
+ 		insn++;
+ 		*insn = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, PKT_TYPE_MAX);
+ #ifdef __BIG_ENDIAN_BITFIELD
+ 		insn++;
+                 *insn = BPF_ALU32_IMM(BPF_RSH, BPF_REG_A, 5);
+ #endif
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_IFINDEX:
+ 	case SKF_AD_OFF + SKF_AD_HATYPE:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
+ 		BUILD_BUG_ON(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)) < 0);
+ 
+ 		*insn++ = BPF_LDX_MEM(bytes_to_bpf_size(FIELD_SIZEOF(struct sk_buff, dev)),
+ 				      BPF_REG_TMP, BPF_REG_CTX,
+ 				      offsetof(struct sk_buff, dev));
+ 		/* if (tmp != 0) goto pc + 1 */
+ 		*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_TMP, 0, 1);
+ 		*insn++ = BPF_EXIT_INSN();
+ 		if (fp->k == SKF_AD_OFF + SKF_AD_IFINDEX)
+ 			*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_TMP,
+ 					    offsetof(struct net_device, ifindex));
+ 		else
+ 			*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_TMP,
+ 					    offsetof(struct net_device, type));
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_MARK:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
+ 
+ 		*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,
+ 				    offsetof(struct sk_buff, mark));
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_RXHASH:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
+ 
+ 		*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,
+ 				    offsetof(struct sk_buff, hash));
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_QUEUE:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
+ 
+ 		*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
+ 				    offsetof(struct sk_buff, queue_mapping));
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_VLAN_TAG:
+ 	case SKF_AD_OFF + SKF_AD_VLAN_TAG_PRESENT:
+ 		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
+ 		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
+ 
+ 		/* A = *(u16 *) (CTX + offsetof(vlan_tci)) */
+ 		*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,
+ 				      offsetof(struct sk_buff, vlan_tci));
+ 		if (fp->k == SKF_AD_OFF + SKF_AD_VLAN_TAG) {
+ 			*insn = BPF_ALU32_IMM(BPF_AND, BPF_REG_A,
+ 					      ~VLAN_TAG_PRESENT);
+ 		} else {
+ 			/* A >>= 12 */
+ 			*insn++ = BPF_ALU32_IMM(BPF_RSH, BPF_REG_A, 12);
+ 			/* A &= 1 */
+ 			*insn = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 1);
+ 		}
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
+ 	case SKF_AD_OFF + SKF_AD_NLATTR:
+ 	case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
+ 	case SKF_AD_OFF + SKF_AD_CPU:
+ 	case SKF_AD_OFF + SKF_AD_RANDOM:
+ 		/* arg1 = CTX */
+ 		*insn++ = BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_CTX);
+ 		/* arg2 = A */
+ 		*insn++ = BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_A);
+ 		/* arg3 = X */
+ 		*insn++ = BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_X);
+ 		/* Emit call(arg1=CTX, arg2=A, arg3=X) */
+ 		switch (fp->k) {
+ 		case SKF_AD_OFF + SKF_AD_PAY_OFFSET:
+ 			*insn = BPF_EMIT_CALL(__skb_get_pay_offset);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_NLATTR:
+ 			*insn = BPF_EMIT_CALL(__skb_get_nlattr);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_NLATTR_NEST:
+ 			*insn = BPF_EMIT_CALL(__skb_get_nlattr_nest);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_CPU:
+ 			*insn = BPF_EMIT_CALL(__get_raw_cpu_id);
+ 			break;
+ 		case SKF_AD_OFF + SKF_AD_RANDOM:
+ 			*insn = BPF_EMIT_CALL(__get_random_u32);
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case SKF_AD_OFF + SKF_AD_ALU_XOR_X:
+ 		/* A ^= X */
+ 		*insn = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_X);
+ 		break;
+ 
+ 	default:
+ 		/* This is just a dummy call to avoid letting the compiler
+ 		 * evict __bpf_call_base() as an optimization. Placed here
+ 		 * where no-one bothers.
+ 		 */
+ 		BUG_ON(__bpf_call_base(0, 0, 0, 0, 0) != 0);
+ 		return false;
+ 	}
+ 
+ 	*insnp = insn;
+ 	return true;
+ }
+ 
++>>>>>>> 56193d1bce2b (net: Add function for parsing the header length out of linear ethernet frames)
  /**
 - *	bpf_convert_filter - convert filter program
 - *	@prog: the user passed filter program
 - *	@len: the length of the user passed filter program
 - *	@new_prog: buffer where converted program will be stored
 - *	@new_len: pointer to store length of converted program
 + *	sk_run_filter - run a filter on a socket
 + *	@skb: buffer to run the filter on
 + *	@fentry: filter to apply
   *
 - * Remap 'sock_filter' style BPF instruction set to 'sock_filter_ext' style.
 - * Conversion workflow:
 - *
 - * 1) First pass for calculating the new program length:
 - *   bpf_convert_filter(old_prog, old_len, NULL, &new_len)
 - *
 - * 2) 2nd pass to remap in two passes: 1st pass finds new
 - *    jump offsets, 2nd pass remapping:
 - *   new_prog = kmalloc(sizeof(struct bpf_insn) * new_len);
 - *   bpf_convert_filter(old_prog, old_len, new_prog, &new_len);
 - *
 - * User BPF's register A is mapped to our BPF register 6, user BPF
 - * register X is mapped to BPF register 7; frame pointer is always
 - * register 10; Context 'void *ctx' is stored in register 1, that is,
 - * for socket filters: ctx == 'struct sk_buff *', for seccomp:
 - * ctx == 'struct seccomp_data *'.
 + * Decode and apply filter instructions to the skb->data.
 + * Return length to keep, 0 for none. @skb is the data we are
 + * filtering, @filter is the array of filter instructions.
 + * Because all jumps are guaranteed to be before last instruction,
 + * and last instruction guaranteed to be a RET, we dont need to check
 + * flen. (We used to pass to this function the length of filter)
   */
 -int bpf_convert_filter(struct sock_filter *prog, int len,
 -		       struct bpf_insn *new_prog, int *new_len)
 +unsigned int sk_run_filter(const struct sk_buff *skb,
 +			   const struct sock_filter *fentry)
  {
 -	int new_flen = 0, pass = 0, target, i;
 -	struct bpf_insn *new_insn;
 -	struct sock_filter *fp;
 -	int *addrs = NULL;
 -	u8 bpf_src;
 -
 -	BUILD_BUG_ON(BPF_MEMWORDS * sizeof(u32) > MAX_BPF_STACK);
 -	BUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);
 -
 -	if (len <= 0 || len > BPF_MAXINSNS)
 -		return -EINVAL;
 -
 -	if (new_prog) {
 -		addrs = kcalloc(len, sizeof(*addrs), GFP_KERNEL);
 -		if (!addrs)
 -			return -ENOMEM;
 -	}
 -
 -do_pass:
 -	new_insn = new_prog;
 -	fp = prog;
 -
 -	if (new_insn)
 -		*new_insn = BPF_MOV64_REG(BPF_REG_CTX, BPF_REG_ARG1);
 -	new_insn++;
 -
 -	for (i = 0; i < len; fp++, i++) {
 -		struct bpf_insn tmp_insns[6] = { };
 -		struct bpf_insn *insn = tmp_insns;
 -
 -		if (addrs)
 -			addrs[i] = new_insn - new_prog;
 -
 -		switch (fp->code) {
 -		/* All arithmetic insns and skb loads map as-is. */
 -		case BPF_ALU | BPF_ADD | BPF_X:
 -		case BPF_ALU | BPF_ADD | BPF_K:
 -		case BPF_ALU | BPF_SUB | BPF_X:
 -		case BPF_ALU | BPF_SUB | BPF_K:
 -		case BPF_ALU | BPF_AND | BPF_X:
 -		case BPF_ALU | BPF_AND | BPF_K:
 -		case BPF_ALU | BPF_OR | BPF_X:
 -		case BPF_ALU | BPF_OR | BPF_K:
 -		case BPF_ALU | BPF_LSH | BPF_X:
 -		case BPF_ALU | BPF_LSH | BPF_K:
 -		case BPF_ALU | BPF_RSH | BPF_X:
 -		case BPF_ALU | BPF_RSH | BPF_K:
 -		case BPF_ALU | BPF_XOR | BPF_X:
 -		case BPF_ALU | BPF_XOR | BPF_K:
 -		case BPF_ALU | BPF_MUL | BPF_X:
 -		case BPF_ALU | BPF_MUL | BPF_K:
 -		case BPF_ALU | BPF_DIV | BPF_X:
 -		case BPF_ALU | BPF_DIV | BPF_K:
 -		case BPF_ALU | BPF_MOD | BPF_X:
 -		case BPF_ALU | BPF_MOD | BPF_K:
 -		case BPF_ALU | BPF_NEG:
 -		case BPF_LD | BPF_ABS | BPF_W:
 -		case BPF_LD | BPF_ABS | BPF_H:
 -		case BPF_LD | BPF_ABS | BPF_B:
 -		case BPF_LD | BPF_IND | BPF_W:
 -		case BPF_LD | BPF_IND | BPF_H:
 -		case BPF_LD | BPF_IND | BPF_B:
 -			/* Check for overloaded BPF extension and
 -			 * directly convert it if found, otherwise
 -			 * just move on with mapping.
 -			 */
 -			if (BPF_CLASS(fp->code) == BPF_LD &&
 -			    BPF_MODE(fp->code) == BPF_ABS &&
 -			    convert_bpf_extensions(fp, &insn))
 -				break;
 -
 -			*insn = BPF_RAW_INSN(fp->code, BPF_REG_A, BPF_REG_X, 0, fp->k);
 -			break;
 +	void *ptr;
 +	u32 A = 0;			/* Accumulator */
 +	u32 X = 0;			/* Index Register */
 +	u32 mem[BPF_MEMWORDS];		/* Scratch Memory Store */
 +	u32 tmp;
 +	int k;
  
 -		/* Jump transformation cannot use BPF block macros
 -		 * everywhere as offset calculation and target updates
 -		 * require a bit more work than the rest, i.e. jump
 -		 * opcodes map as-is, but offsets need adjustment.
 -		 */
 -
 -#define BPF_EMIT_JMP							\
 -	do {								\
 -		if (target >= len || target < 0)			\
 -			goto err;					\
 -		insn->off = addrs ? addrs[target] - addrs[i] - 1 : 0;	\
 -		/* Adjust pc relative offset for 2nd or 3rd insn. */	\
 -		insn->off -= insn - tmp_insns;				\
 -	} while (0)
 -
 -		case BPF_JMP | BPF_JA:
 -			target = i + fp->k + 1;
 -			insn->code = fp->code;
 -			BPF_EMIT_JMP;
 -			break;
 +	/*
 +	 * Process array of filter instructions.
 +	 */
 +	for (;; fentry++) {
 +#if defined(CONFIG_X86_32)
 +#define	K (fentry->k)
 +#else
 +		const u32 K = fentry->k;
 +#endif
  
 -		case BPF_JMP | BPF_JEQ | BPF_K:
 -		case BPF_JMP | BPF_JEQ | BPF_X:
 -		case BPF_JMP | BPF_JSET | BPF_K:
 -		case BPF_JMP | BPF_JSET | BPF_X:
 -		case BPF_JMP | BPF_JGT | BPF_K:
 -		case BPF_JMP | BPF_JGT | BPF_X:
 -		case BPF_JMP | BPF_JGE | BPF_K:
 -		case BPF_JMP | BPF_JGE | BPF_X:
 -			if (BPF_SRC(fp->code) == BPF_K && (int) fp->k < 0) {
 -				/* BPF immediates are signed, zero extend
 -				 * immediate into tmp register and use it
 -				 * in compare insn.
 -				 */
 -				*insn++ = BPF_MOV32_IMM(BPF_REG_TMP, fp->k);
 -
 -				insn->dst_reg = BPF_REG_A;
 -				insn->src_reg = BPF_REG_TMP;
 -				bpf_src = BPF_X;
 -			} else {
 -				insn->dst_reg = BPF_REG_A;
 -				insn->src_reg = BPF_REG_X;
 -				insn->imm = fp->k;
 -				bpf_src = BPF_SRC(fp->code);
 +		switch (fentry->code) {
 +		case BPF_S_ALU_ADD_X:
 +			A += X;
 +			continue;
 +		case BPF_S_ALU_ADD_K:
 +			A += K;
 +			continue;
 +		case BPF_S_ALU_SUB_X:
 +			A -= X;
 +			continue;
 +		case BPF_S_ALU_SUB_K:
 +			A -= K;
 +			continue;
 +		case BPF_S_ALU_MUL_X:
 +			A *= X;
 +			continue;
 +		case BPF_S_ALU_MUL_K:
 +			A *= K;
 +			continue;
 +		case BPF_S_ALU_DIV_X:
 +			if (X == 0)
 +				return 0;
 +			A /= X;
 +			continue;
 +		case BPF_S_ALU_DIV_K:
 +			A /= K;
 +			continue;
 +		case BPF_S_ALU_MOD_X:
 +			if (X == 0)
 +				return 0;
 +			A %= X;
 +			continue;
 +		case BPF_S_ALU_MOD_K:
 +			A %= K;
 +			continue;
 +		case BPF_S_ALU_AND_X:
 +			A &= X;
 +			continue;
 +		case BPF_S_ALU_AND_K:
 +			A &= K;
 +			continue;
 +		case BPF_S_ALU_OR_X:
 +			A |= X;
 +			continue;
 +		case BPF_S_ALU_OR_K:
 +			A |= K;
 +			continue;
 +		case BPF_S_ANC_ALU_XOR_X:
 +		case BPF_S_ALU_XOR_X:
 +			A ^= X;
 +			continue;
 +		case BPF_S_ALU_XOR_K:
 +			A ^= K;
 +			continue;
 +		case BPF_S_ALU_LSH_X:
 +			A <<= X;
 +			continue;
 +		case BPF_S_ALU_LSH_K:
 +			A <<= K;
 +			continue;
 +		case BPF_S_ALU_RSH_X:
 +			A >>= X;
 +			continue;
 +		case BPF_S_ALU_RSH_K:
 +			A >>= K;
 +			continue;
 +		case BPF_S_ALU_NEG:
 +			A = -A;
 +			continue;
 +		case BPF_S_JMP_JA:
 +			fentry += K;
 +			continue;
 +		case BPF_S_JMP_JGT_K:
 +			fentry += (A > K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_K:
 +			fentry += (A >= K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_K:
 +			fentry += (A == K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_K:
 +			fentry += (A & K) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGT_X:
 +			fentry += (A > X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JGE_X:
 +			fentry += (A >= X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JEQ_X:
 +			fentry += (A == X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_JMP_JSET_X:
 +			fentry += (A & X) ? fentry->jt : fentry->jf;
 +			continue;
 +		case BPF_S_LD_W_ABS:
 +			k = K;
 +load_w:
 +			ptr = load_pointer(skb, k, 4, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be32(ptr);
 +				continue;
  			}
 -
 -			/* Common case where 'jump_false' is next insn. */
 -			if (fp->jf == 0) {
 -				insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -				target = i + fp->jt + 1;
 -				BPF_EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_H_ABS:
 +			k = K;
 +load_h:
 +			ptr = load_pointer(skb, k, 2, &tmp);
 +			if (ptr != NULL) {
 +				A = get_unaligned_be16(ptr);
 +				continue;
  			}
 -
 -			/* Convert JEQ into JNE when 'jump_true' is next insn. */
 -			if (fp->jt == 0 && BPF_OP(fp->code) == BPF_JEQ) {
 -				insn->code = BPF_JMP | BPF_JNE | bpf_src;
 -				target = i + fp->jf + 1;
 -				BPF_EMIT_JMP;
 -				break;
 +			return 0;
 +		case BPF_S_LD_B_ABS:
 +			k = K;
 +load_b:
 +			ptr = load_pointer(skb, k, 1, &tmp);
 +			if (ptr != NULL) {
 +				A = *(u8 *)ptr;
 +				continue;
  			}
 +			return 0;
 +		case BPF_S_LD_W_LEN:
 +			A = skb->len;
 +			continue;
 +		case BPF_S_LDX_W_LEN:
 +			X = skb->len;
 +			continue;
 +		case BPF_S_LD_W_IND:
 +			k = X + K;
 +			goto load_w;
 +		case BPF_S_LD_H_IND:
 +			k = X + K;
 +			goto load_h;
 +		case BPF_S_LD_B_IND:
 +			k = X + K;
 +			goto load_b;
 +		case BPF_S_LDX_B_MSH:
 +			ptr = load_pointer(skb, K, 1, &tmp);
 +			if (ptr != NULL) {
 +				X = (*(u8 *)ptr & 0xf) << 2;
 +				continue;
 +			}
 +			return 0;
 +		case BPF_S_LD_IMM:
 +			A = K;
 +			continue;
 +		case BPF_S_LDX_IMM:
 +			X = K;
 +			continue;
 +		case BPF_S_LD_MEM:
 +			A = mem[K];
 +			continue;
 +		case BPF_S_LDX_MEM:
 +			X = mem[K];
 +			continue;
 +		case BPF_S_MISC_TAX:
 +			X = A;
 +			continue;
 +		case BPF_S_MISC_TXA:
 +			A = X;
 +			continue;
 +		case BPF_S_RET_K:
 +			return K;
 +		case BPF_S_RET_A:
 +			return A;
 +		case BPF_S_ST:
 +			mem[K] = A;
 +			continue;
 +		case BPF_S_STX:
 +			mem[K] = X;
 +			continue;
 +		case BPF_S_ANC_PROTOCOL:
 +			A = ntohs(skb->protocol);
 +			continue;
 +		case BPF_S_ANC_PKTTYPE:
 +			A = skb->pkt_type;
 +			continue;
 +		case BPF_S_ANC_IFINDEX:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->ifindex;
 +			continue;
 +		case BPF_S_ANC_MARK:
 +			A = skb->mark;
 +			continue;
 +		case BPF_S_ANC_QUEUE:
 +			A = skb->queue_mapping;
 +			continue;
 +		case BPF_S_ANC_HATYPE:
 +			if (!skb->dev)
 +				return 0;
 +			A = skb->dev->type;
 +			continue;
 +		case BPF_S_ANC_RXHASH:
 +			A = skb->rxhash;
 +			continue;
 +		case BPF_S_ANC_CPU:
 +			A = raw_smp_processor_id();
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG:
 +			A = vlan_tx_tag_get(skb);
 +			continue;
 +		case BPF_S_ANC_VLAN_TAG_PRESENT:
 +			A = !!vlan_tx_tag_present(skb);
 +			continue;
 +		case BPF_S_ANC_PAY_OFFSET:
 +			A = __skb_get_poff(skb);
 +			continue;
 +		case BPF_S_ANC_NLATTR: {
 +			struct nlattr *nla;
 +
 +			if (skb_is_nonlinear(skb))
 +				return 0;
 +
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
 +
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
 +
 +			nla = nla_find((struct nlattr *)&skb->data[A],
 +				       skb->len - A, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +		case BPF_S_ANC_NLATTR_NEST: {
 +			struct nlattr *nla;
  
 -			/* Other jumps are mapped into two insns: Jxx and JA. */
 -			target = i + fp->jt + 1;
 -			insn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;
 -			BPF_EMIT_JMP;
 -			insn++;
 -
 -			insn->code = BPF_JMP | BPF_JA;
 -			target = i + fp->jf + 1;
 -			BPF_EMIT_JMP;
 -			break;
 -
 -		/* ldxb 4 * ([14] & 0xf) is remaped into 6 insns. */
 -		case BPF_LDX | BPF_MSH | BPF_B:
 -			/* tmp = A */
 -			*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_A);
 -			/* A = BPF_R0 = *(u8 *) (skb->data + K) */
 -			*insn++ = BPF_LD_ABS(BPF_B, fp->k);
 -			/* A &= 0xf */
 -			*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 0xf);
 -			/* A <<= 2 */
 -			*insn++ = BPF_ALU32_IMM(BPF_LSH, BPF_REG_A, 2);
 -			/* X = A */
 -			*insn++ = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);
 -			/* A = tmp */
 -			*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_TMP);
 -			break;
 -
 -		/* RET_K, RET_A are remaped into 2 insns. */
 -		case BPF_RET | BPF_A:
 -		case BPF_RET | BPF_K:
 -			*insn++ = BPF_MOV32_RAW(BPF_RVAL(fp->code) == BPF_K ?
 -						BPF_K : BPF_X, BPF_REG_0,
 -						BPF_REG_A, fp->k);
 -			*insn = BPF_EXIT_INSN();
 -			break;
 -
 -		/* Store to stack. */
 -		case BPF_ST:
 -		case BPF_STX:
 -			*insn = BPF_STX_MEM(BPF_W, BPF_REG_FP, BPF_CLASS(fp->code) ==
 -					    BPF_ST ? BPF_REG_A : BPF_REG_X,
 -					    -(BPF_MEMWORDS - fp->k) * 4);
 -			break;
 -
 -		/* Load from stack. */
 -		case BPF_LD | BPF_MEM:
 -		case BPF_LDX | BPF_MEM:
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD  ?
 -					    BPF_REG_A : BPF_REG_X, BPF_REG_FP,
 -					    -(BPF_MEMWORDS - fp->k) * 4);
 -			break;
 -
 -		/* A = K or X = K */
 -		case BPF_LD | BPF_IMM:
 -		case BPF_LDX | BPF_IMM:
 -			*insn = BPF_MOV32_IMM(BPF_CLASS(fp->code) == BPF_LD ?
 -					      BPF_REG_A : BPF_REG_X, fp->k);
 -			break;
 -
 -		/* X = A */
 -		case BPF_MISC | BPF_TAX:
 -			*insn = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);
 -			break;
 +			if (skb_is_nonlinear(skb))
 +				return 0;
  
 -		/* A = X */
 -		case BPF_MISC | BPF_TXA:
 -			*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_X);
 -			break;
 +			if (skb->len < sizeof(struct nlattr))
 +				return 0;
  
 -		/* A = skb->len or X = skb->len */
 -		case BPF_LD | BPF_W | BPF_LEN:
 -		case BPF_LDX | BPF_W | BPF_LEN:
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD ?
 -					    BPF_REG_A : BPF_REG_X, BPF_REG_CTX,
 -					    offsetof(struct sk_buff, len));
 -			break;
 +			if (A > skb->len - sizeof(struct nlattr))
 +				return 0;
  
 -		/* Access seccomp_data fields. */
 -		case BPF_LDX | BPF_ABS | BPF_W:
 -			/* A = *(u32 *) (ctx + K) */
 -			*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX, fp->k);
 -			break;
 +			nla = (struct nlattr *)&skb->data[A];
 +			if (nla->nla_len > skb->len - A)
 +				return 0;
  
 -		/* Unkown instruction. */
 +			nla = nla_find_nested(nla, X);
 +			if (nla)
 +				A = (void *)nla - (void *)skb->data;
 +			else
 +				A = 0;
 +			continue;
 +		}
 +#ifdef CONFIG_SECCOMP_FILTER
 +		case BPF_S_ANC_SECCOMP_LD_W:
 +			A = seccomp_bpf_load(fentry->k);
 +			continue;
 +#endif
  		default:
 -			goto err;
 +			WARN_RATELIMIT(1, "Unknown code:%u jt:%u tf:%u k:%u\n",
 +				       fentry->code, fentry->jt,
 +				       fentry->jf, fentry->k);
 +			return 0;
  		}
 -
 -		insn++;
 -		if (new_prog)
 -			memcpy(new_insn, tmp_insns,
 -			       sizeof(*insn) * (insn - tmp_insns));
 -		new_insn += insn - tmp_insns;
 -	}
 -
 -	if (!new_prog) {
 -		/* Only calculating new length. */
 -		*new_len = new_insn - new_prog;
 -		return 0;
 -	}
 -
 -	pass++;
 -	if (new_flen != new_insn - new_prog) {
 -		new_flen = new_insn - new_prog;
 -		if (pass > 2)
 -			goto err;
 -		goto do_pass;
  	}
  
 -	kfree(addrs);
 -	BUG_ON(*new_len != new_flen);
  	return 0;
 -err:
 -	kfree(addrs);
 -	return -EINVAL;
  }
 +EXPORT_SYMBOL(sk_run_filter);
  
 -/* Security:
 - *
 +/*
 + * Security :
   * A BPF program is able to use 16 cells of memory to store intermediate
 - * values (check u32 mem[BPF_MEMWORDS] in sk_run_filter()).
 - *
 + * values (check u32 mem[BPF_MEMWORDS] in sk_run_filter())
   * As we dont want to clear mem[] array for each packet going through
   * sk_run_filter(), we check that filter loaded by user never try to read
   * a cell if not previously written, and we check all branches to be sure
diff --cc net/core/flow_dissector.c
index 6a5c78b76323,8560dea58803..000000000000
--- a/net/core/flow_dissector.c
+++ b/net/core/flow_dissector.c
@@@ -63,9 -117,32 +64,34 @@@ ipv6
  		flow->src = (__force __be32)ipv6_addr_hash(&iph->saddr);
  		flow->dst = (__force __be32)ipv6_addr_hash(&iph->daddr);
  		nhoff += sizeof(struct ipv6hdr);
++<<<<<<< HEAD
++=======
+ 
+ 		/* skip the flow label processing if skb is NULL.  The
+ 		 * assumption here is that if there is no skb we are not
+ 		 * looking for flow info as much as we are length.
+ 		 */
+ 		if (!skb)
+ 			break;
+ 
+ 		flow_label = ip6_flowlabel(iph);
+ 		if (flow_label) {
+ 			/* Awesome, IPv6 packet has a flow label so we can
+ 			 * use that to represent the ports without any
+ 			 * further dissection.
+ 			 */
+ 			flow->n_proto = proto;
+ 			flow->ip_proto = ip_proto;
+ 			flow->ports = flow_label;
+ 			flow->thoff = (u16)nhoff;
+ 
+ 			return true;
+ 		}
+ 
++>>>>>>> 56193d1bce2b (net: Add function for parsing the header length out of linear ethernet frames)
  		break;
  	}
 -	case htons(ETH_P_8021AD):
 -	case htons(ETH_P_8021Q): {
 +	case __constant_htons(ETH_P_8021Q): {
  		const struct vlan_hdr *vlan;
  		struct vlan_hdr _vlan;
  
@@@ -286,15 -372,19 +307,31 @@@ u32 __skb_get_poff(const struct sk_buf
  	return poff;
  }
  
++<<<<<<< HEAD
 +static inline u16 dev_cap_txqueue(struct net_device *dev, u16 queue_index)
 +{
 +	if (unlikely(queue_index >= dev->real_num_tx_queues)) {
 +		net_warn_ratelimited("%s selects TX queue %d, but real number of TX queues is %d\n",
 +				     dev->name, queue_index,
 +				     dev->real_num_tx_queues);
 +		return 0;
 +	}
 +	return queue_index;
++=======
+ /* skb_get_poff() returns the offset to the payload as far as it could
+  * be dissected. The main user is currently BPF, so that we can dynamically
+  * truncate packets without needing to push actual payload to the user
+  * space and can analyze headers only, instead.
+  */
+ u32 skb_get_poff(const struct sk_buff *skb)
+ {
+ 	struct flow_keys keys;
+ 
+ 	if (!skb_flow_dissect(skb, &keys))
+ 		return 0;
+ 
+ 	return __skb_get_poff(skb, skb->data, &keys, skb_headlen(skb));
++>>>>>>> 56193d1bce2b (net: Add function for parsing the header length out of linear ethernet frames)
  }
  
  static inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)
* Unmerged path include/linux/etherdevice.h
* Unmerged path include/linux/skbuff.h
* Unmerged path include/net/flow_keys.h
* Unmerged path net/core/filter.c
* Unmerged path net/core/flow_dissector.c
diff --git a/net/ethernet/eth.c b/net/ethernet/eth.c
index 759dfa82d546..fef8beded061 100644
--- a/net/ethernet/eth.c
+++ b/net/ethernet/eth.c
@@ -145,6 +145,33 @@ int eth_rebuild_header(struct sk_buff *skb)
 }
 EXPORT_SYMBOL(eth_rebuild_header);
 
+/**
+ * eth_get_headlen - determine the the length of header for an ethernet frame
+ * @data: pointer to start of frame
+ * @len: total length of frame
+ *
+ * Make a best effort attempt to pull the length for all of the headers for
+ * a given frame in a linear buffer.
+ */
+u32 eth_get_headlen(void *data, unsigned int len)
+{
+	const struct ethhdr *eth = (const struct ethhdr *)data;
+	struct flow_keys keys;
+
+	/* this should never happen, but better safe than sorry */
+	if (len < sizeof(*eth))
+		return len;
+
+	/* parse any remaining L2/L3 headers, check for L4 */
+	if (!__skb_flow_dissect(NULL, &keys, data,
+				eth->h_proto, sizeof(*eth), len))
+		return max_t(u32, keys.thoff, sizeof(*eth));
+
+	/* parse for any L4 headers */
+	return min_t(u32, __skb_get_poff(NULL, data, &keys, len), len);
+}
+EXPORT_SYMBOL(eth_get_headlen);
+
 /**
  * eth_type_trans - determine the packet's protocol ID.
  * @skb: received socket data
