tcp: don't allow syn packets without timestamps to pass tcp_tw_recycle logic

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Hannes Frederic Sowa <hannes@stressinduktion.org>
commit a26552afe89438eefbe097512b3187f2c7e929fd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/a26552af.failed

tcp_tw_recycle heavily relies on tcp timestamps to build a per-host
ordering of incoming connections and teardowns without the need to
hold state on a specific quadruple for TCP_TIMEWAIT_LEN, but only for
the last measured RTO. To do so, we keep the last seen timestamp in a
per-host indexed data structure and verify if the incoming timestamp
in a connection request is strictly greater than the saved one during
last connection teardown. Thus we can verify later on that no old data
packets will be accepted by the new connection.

During moving a socket to time-wait state we already verify if timestamps
where seen on a connection. Only if that was the case we let the
time-wait socket expire after the RTO, otherwise normal TCP_TIMEWAIT_LEN
will be used. But we don't verify this on incoming SYN packets. If a
connection teardown was less than TCP_PAWS_MSL seconds in the past we
cannot guarantee to not accept data packets from an old connection if
no timestamps are present. We should drop this SYN packet. This patch
closes this loophole.

Please note, this patch does not make tcp_tw_recycle in any way more
usable but only adds another safety check:
Sporadic drops of SYN packets because of reordering in the network or
in the socket backlog queues can happen. Users behing NAT trying to
connect to a tcp_tw_recycle enabled server can get caught in blackholes
and their connection requests may regullary get dropped because hosts
behind an address translator don't have synchronized tcp timestamp clocks.
tcp_tw_recycle cannot work if peers don't have tcp timestamps enabled.

In general, use of tcp_tw_recycle is disadvised.

	Cc: Eric Dumazet <eric.dumazet@gmail.com>
	Cc: Florian Westphal <fw@strlen.de>
	Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a26552afe89438eefbe097512b3187f2c7e929fd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/tcp.h
#	net/ipv4/tcp_input.c
diff --cc include/net/tcp.h
index 81f472949625,590e01a476ac..000000000000
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@@ -406,45 -403,45 +406,76 @@@ enum tcp_tw_status 
  };
  
  
++<<<<<<< HEAD
 +extern enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,
 +						     struct sk_buff *skb,
 +						     const struct tcphdr *th);
 +extern struct sock * tcp_check_req(struct sock *sk,struct sk_buff *skb,
 +				   struct request_sock *req,
 +				   struct request_sock **prev,
 +				   bool fastopen);
 +extern int tcp_child_process(struct sock *parent, struct sock *child,
 +			     struct sk_buff *skb);
 +extern void tcp_enter_loss(struct sock *sk, int how);
 +extern void tcp_clear_retrans(struct tcp_sock *tp);
 +extern void tcp_update_metrics(struct sock *sk);
 +extern void tcp_init_metrics(struct sock *sk);
 +extern void tcp_metrics_init(void);
 +extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst, bool paws_check);
 +extern bool tcp_remember_stamp(struct sock *sk);
 +extern bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
 +extern void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
 +extern void tcp_disable_fack(struct tcp_sock *tp);
 +extern void tcp_close(struct sock *sk, long timeout);
 +extern void tcp_init_sock(struct sock *sk);
 +extern unsigned int tcp_poll(struct file * file, struct socket *sock,
 +			     struct poll_table_struct *wait);
 +extern int tcp_getsockopt(struct sock *sk, int level, int optname,
++=======
+ enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,
+ 					      struct sk_buff *skb,
+ 					      const struct tcphdr *th);
+ struct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,
+ 			   struct request_sock *req, struct request_sock **prev,
+ 			   bool fastopen);
+ int tcp_child_process(struct sock *parent, struct sock *child,
+ 		      struct sk_buff *skb);
+ void tcp_enter_loss(struct sock *sk);
+ void tcp_clear_retrans(struct tcp_sock *tp);
+ void tcp_update_metrics(struct sock *sk);
+ void tcp_init_metrics(struct sock *sk);
+ void tcp_metrics_init(void);
+ bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst,
+ 			bool paws_check, bool timestamps);
+ bool tcp_remember_stamp(struct sock *sk);
+ bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
+ void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
+ void tcp_disable_fack(struct tcp_sock *tp);
+ void tcp_close(struct sock *sk, long timeout);
+ void tcp_init_sock(struct sock *sk);
+ unsigned int tcp_poll(struct file *file, struct socket *sock,
+ 		      struct poll_table_struct *wait);
+ int tcp_getsockopt(struct sock *sk, int level, int optname,
+ 		   char __user *optval, int __user *optlen);
+ int tcp_setsockopt(struct sock *sk, int level, int optname,
+ 		   char __user *optval, unsigned int optlen);
+ int compat_tcp_getsockopt(struct sock *sk, int level, int optname,
++>>>>>>> a26552afe894 (tcp: don't allow syn packets without timestamps to pass tcp_tw_recycle logic)
  			  char __user *optval, int __user *optlen);
 -int compat_tcp_setsockopt(struct sock *sk, int level, int optname,
 +extern int tcp_setsockopt(struct sock *sk, int level, int optname,
  			  char __user *optval, unsigned int optlen);
 -void tcp_set_keepalive(struct sock *sk, int val);
 -void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
 -int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 -		size_t len, int nonblock, int flags, int *addr_len);
 -void tcp_parse_options(const struct sk_buff *skb,
 -		       struct tcp_options_received *opt_rx,
 -		       int estab, struct tcp_fastopen_cookie *foc);
 -const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 +extern int compat_tcp_getsockopt(struct sock *sk, int level, int optname,
 +				 char __user *optval, int __user *optlen);
 +extern int compat_tcp_setsockopt(struct sock *sk, int level, int optname,
 +				 char __user *optval, unsigned int optlen);
 +extern void tcp_set_keepalive(struct sock *sk, int val);
 +extern void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
 +extern int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 +		       size_t len, int nonblock, int flags, int *addr_len);
 +extern void tcp_parse_options(const struct sk_buff *skb,
 +			      struct tcp_options_received *opt_rx,
 +			      int estab, struct tcp_fastopen_cookie *foc);
 +extern const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
  
  /*
   *	TCP v4 functions exported for the inet6 API
diff --cc net/ipv4/tcp_input.c
index 7bcc2fdfb73f,4f6cfbc57775..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -5819,3 -5889,156 +5819,159 @@@ discard
  	return 0;
  }
  EXPORT_SYMBOL(tcp_rcv_state_process);
++<<<<<<< HEAD
++=======
+ 
+ static inline void pr_drop_req(struct request_sock *req, __u16 port, int family)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 
+ 	if (family == AF_INET)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI4/%u\n"),
+ 			       &ireq->ir_rmt_addr, port);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	else if (family == AF_INET6)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI6/%u\n"),
+ 			       &ireq->ir_v6_rmt_addr, port);
+ #endif
+ }
+ 
+ int tcp_conn_request(struct request_sock_ops *rsk_ops,
+ 		     const struct tcp_request_sock_ops *af_ops,
+ 		     struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct tcp_options_received tmp_opt;
+ 	struct request_sock *req;
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 	struct dst_entry *dst = NULL;
+ 	__u32 isn = TCP_SKB_CB(skb)->when;
+ 	bool want_cookie = false, fastopen;
+ 	struct flowi fl;
+ 	struct tcp_fastopen_cookie foc = { .len = -1 };
+ 	int err;
+ 
+ 
+ 	/* TW buckets are converted to open requests without
+ 	 * limitations, they conserve resources and peer is
+ 	 * evidently real one.
+ 	 */
+ 	if ((sysctl_tcp_syncookies == 2 ||
+ 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
+ 		want_cookie = tcp_syn_flood_action(sk, skb, rsk_ops->slab_name);
+ 		if (!want_cookie)
+ 			goto drop;
+ 	}
+ 
+ 
+ 	/* Accept backlog is full. If we have already queued enough
+ 	 * of warm entries in syn queue, drop request. It is better than
+ 	 * clogging syn queue with openreqs with exponentially increasing
+ 	 * timeout.
+ 	 */
+ 	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+ 		goto drop;
+ 	}
+ 
+ 	req = inet_reqsk_alloc(rsk_ops);
+ 	if (!req)
+ 		goto drop;
+ 
+ 	tcp_rsk(req)->af_specific = af_ops;
+ 
+ 	tcp_clear_options(&tmp_opt);
+ 	tmp_opt.mss_clamp = af_ops->mss_clamp;
+ 	tmp_opt.user_mss  = tp->rx_opt.user_mss;
+ 	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
+ 
+ 	if (want_cookie && !tmp_opt.saw_tstamp)
+ 		tcp_clear_options(&tmp_opt);
+ 
+ 	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+ 	tcp_openreq_init(req, &tmp_opt, skb, sk);
+ 
+ 	af_ops->init_req(req, sk, skb);
+ 
+ 	if (security_inet_conn_request(sk, skb, req))
+ 		goto drop_and_free;
+ 
+ 	if (!want_cookie || tmp_opt.tstamp_ok)
+ 		TCP_ECN_create_request(req, skb, sock_net(sk));
+ 
+ 	if (want_cookie) {
+ 		isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
+ 		req->cookie_ts = tmp_opt.tstamp_ok;
+ 	} else if (!isn) {
+ 		/* VJ's idea. We save last timestamp seen
+ 		 * from the destination in peer table, when entering
+ 		 * state TIME-WAIT, and check against it before
+ 		 * accepting new connection request.
+ 		 *
+ 		 * If "isn" is not zero, this request hit alive
+ 		 * timewait bucket, so that all the necessary checks
+ 		 * are made in the function processing timewait state.
+ 		 */
+ 		if (tcp_death_row.sysctl_tw_recycle) {
+ 			bool strict;
+ 
+ 			dst = af_ops->route_req(sk, &fl, req, &strict);
+ 
+ 			if (dst && strict &&
+ 			    !tcp_peer_is_proven(req, dst, true,
+ 						tmp_opt.saw_tstamp)) {
+ 				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
+ 				goto drop_and_release;
+ 			}
+ 		}
+ 		/* Kill the following clause, if you dislike this way. */
+ 		else if (!sysctl_tcp_syncookies &&
+ 			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
+ 			  (sysctl_max_syn_backlog >> 2)) &&
+ 			 !tcp_peer_is_proven(req, dst, false,
+ 					     tmp_opt.saw_tstamp)) {
+ 			/* Without syncookies last quarter of
+ 			 * backlog is filled with destinations,
+ 			 * proven to be alive.
+ 			 * It means that we continue to communicate
+ 			 * to destinations, already remembered
+ 			 * to the moment of synflood.
+ 			 */
+ 			pr_drop_req(req, ntohs(tcp_hdr(skb)->source),
+ 				    rsk_ops->family);
+ 			goto drop_and_release;
+ 		}
+ 
+ 		isn = af_ops->init_seq(skb);
+ 	}
+ 	if (!dst) {
+ 		dst = af_ops->route_req(sk, &fl, req, NULL);
+ 		if (!dst)
+ 			goto drop_and_free;
+ 	}
+ 
+ 	tcp_rsk(req)->snt_isn = isn;
+ 	tcp_openreq_init_rwin(req, sk, dst);
+ 	fastopen = !want_cookie &&
+ 		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+ 	err = af_ops->send_synack(sk, dst, &fl, req,
+ 				  skb_get_queue_mapping(skb), &foc);
+ 	if (!fastopen) {
+ 		if (err || want_cookie)
+ 			goto drop_and_free;
+ 
+ 		tcp_rsk(req)->listener = NULL;
+ 		af_ops->queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+ 	}
+ 
+ 	return 0;
+ 
+ drop_and_release:
+ 	dst_release(dst);
+ drop_and_free:
+ 	reqsk_free(req);
+ drop:
+ 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tcp_conn_request);
++>>>>>>> a26552afe894 (tcp: don't allow syn packets without timestamps to pass tcp_tw_recycle logic)
* Unmerged path include/net/tcp.h
* Unmerged path net/ipv4/tcp_input.c
diff --git a/net/ipv4/tcp_metrics.c b/net/ipv4/tcp_metrics.c
index 8f0699bcdb4b..4389d2400816 100644
--- a/net/ipv4/tcp_metrics.c
+++ b/net/ipv4/tcp_metrics.c
@@ -563,7 +563,8 @@ reset:
 	tp->snd_cwnd_stamp = tcp_time_stamp;
 }
 
-bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst, bool paws_check)
+bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst,
+			bool paws_check, bool timestamps)
 {
 	struct tcp_metrics_block *tm;
 	bool ret;
@@ -576,7 +577,8 @@ bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst, bool pa
 	if (paws_check) {
 		if (tm &&
 		    (u32)get_seconds() - tm->tcpm_ts_stamp < TCP_PAWS_MSL &&
-		    (s32)(tm->tcpm_ts - req->ts_recent) > TCP_PAWS_WINDOW)
+		    ((s32)(tm->tcpm_ts - req->ts_recent) > TCP_PAWS_WINDOW ||
+		     !timestamps))
 			ret = false;
 		else
 			ret = true;
