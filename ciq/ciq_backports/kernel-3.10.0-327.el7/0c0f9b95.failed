NVMe: Fix potential corruption on sync commands

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Keith Busch <keith.busch@intel.com>
commit 0c0f9b95c8b710b74772edd9693fe7ab5419a75a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/0c0f9b95.failed

This makes all sync commands uninterruptible and schedules without timeout
so the controller either has to post a completion or the timeout recovery
fails the command. This fixes potential memory or data corruption from
a command timing out too early or woken by a signal. Previously any DMA
buffers mapped for that command would have been released even though we
don't know what the controller is planning to do with those addresses.

	Signed-off-by: Keith Busch <keith.busch@intel.com>
(cherry picked from commit 0c0f9b95c8b710b74772edd9693fe7ab5419a75a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index 6783fae878e4,b64bccbb78c9..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -973,13 -926,6 +973,16 @@@ static irqreturn_t nvme_irq_check(int i
  	return IRQ_WAKE_THREAD;
  }
  
++<<<<<<< HEAD
 +static void nvme_abort_command(struct nvme_queue *nvmeq, int cmdid)
 +{
 +	spin_lock_irq(&nvmeq->q_lock);
 +	cancel_cmdid(nvmeq, cmdid, NULL);
 +	spin_unlock_irq(&nvmeq->q_lock);
 +}
 +
++=======
++>>>>>>> 0c0f9b95c8b7 (NVMe: Fix potential corruption on sync commands)
  struct sync_cmd_info {
  	struct task_struct *task;
  	u32 result;
@@@ -999,47 -945,23 +1002,56 @@@ static void sync_completion(struct nvme
   * Returns 0 on success.  If the result is negative, it's a Linux error code;
   * if the result is positive, it's an NVM Express status code
   */
 -static int nvme_submit_sync_cmd(struct request *req, struct nvme_command *cmd,
 +static int nvme_submit_sync_cmd(struct nvme_dev *dev, int q_idx,
 +						struct nvme_command *cmd,
  						u32 *result, unsigned timeout)
  {
++<<<<<<< HEAD
 +	int cmdid, ret;
++=======
++>>>>>>> 0c0f9b95c8b7 (NVMe: Fix potential corruption on sync commands)
  	struct sync_cmd_info cmdinfo;
 -	struct nvme_cmd_info *cmd_rq = blk_mq_rq_to_pdu(req);
 -	struct nvme_queue *nvmeq = cmd_rq->nvmeq;
 +	struct nvme_queue *nvmeq;
 +
 +	nvmeq = lock_nvmeq(dev, q_idx);
 +	if (!nvmeq)
 +		return -ENODEV;
  
  	cmdinfo.task = current;
  	cmdinfo.status = -EINTR;
  
 -	cmd->common.command_id = req->tag;
 +	cmdid = alloc_cmdid(nvmeq, &cmdinfo, sync_completion, timeout);
 +	if (cmdid < 0) {
 +		unlock_nvmeq(nvmeq);
 +		return cmdid;
 +	}
 +	cmd->common.command_id = cmdid;
  
 -	nvme_set_info(cmd_rq, &cmdinfo, sync_completion);
++<<<<<<< HEAD
 +	set_current_state(TASK_KILLABLE);
 +	ret = nvme_submit_cmd(nvmeq, cmd);
 +	if (ret) {
 +		free_cmdid(nvmeq, cmdid, NULL);
 +		unlock_nvmeq(nvmeq);
 +		set_current_state(TASK_RUNNING);
 +		return ret;
 +	}
 +	unlock_nvmeq(nvmeq);
 +	schedule_timeout(timeout);
  
 +	if (cmdinfo.status == -EINTR) {
 +		nvmeq = lock_nvmeq(dev, q_idx);
 +		if (nvmeq) {
 +			nvme_abort_command(nvmeq, cmdid);
 +			unlock_nvmeq(nvmeq);
 +		}
 +		return -EINTR;
 +	}
++=======
+ 	set_current_state(TASK_UNINTERRUPTIBLE);
+ 	nvme_submit_cmd(nvmeq, cmd);
+ 	schedule();
++>>>>>>> 0c0f9b95c8b7 (NVMe: Fix potential corruption on sync commands)
  
  	if (result)
  		*result = cmdinfo.result;
* Unmerged path drivers/block/nvme-core.c
