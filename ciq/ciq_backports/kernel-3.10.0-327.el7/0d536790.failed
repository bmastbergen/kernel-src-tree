KVM: MMU: introduce for_each_rmap_spte()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [kvm] mmu: introduce for_each_rmap_spte() (Paolo Bonzini) [1202825]
Rebuild_FUZZ: 93.33%
commit-author Xiao Guangrong <guangrong.xiao@linux.intel.com>
commit 0d5367900a319ab8971817b0ca15a8b9f7c47e6f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/0d536790.failed

It's used to walk all the sptes on the rmap to clean up the
code

	Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 0d5367900a319ab8971817b0ca15a8b9f7c47e6f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
diff --cc arch/x86/kvm/mmu.c
index 10d8f908481b,7a1158533f89..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -1215,8 -1216,54 +1216,57 @@@ static bool __rmap_write_protect(struc
  	return flush;
  }
  
++<<<<<<< HEAD
++=======
+ static bool spte_clear_dirty(struct kvm *kvm, u64 *sptep)
+ {
+ 	u64 spte = *sptep;
+ 
+ 	rmap_printk("rmap_clear_dirty: spte %p %llx\n", sptep, *sptep);
+ 
+ 	spte &= ~shadow_dirty_mask;
+ 
+ 	return mmu_spte_update(sptep, spte);
+ }
+ 
+ static bool __rmap_clear_dirty(struct kvm *kvm, unsigned long *rmapp)
+ {
+ 	u64 *sptep;
+ 	struct rmap_iterator iter;
+ 	bool flush = false;
+ 
+ 	for_each_rmap_spte(rmapp, &iter, sptep)
+ 		flush |= spte_clear_dirty(kvm, sptep);
+ 
+ 	return flush;
+ }
+ 
+ static bool spte_set_dirty(struct kvm *kvm, u64 *sptep)
+ {
+ 	u64 spte = *sptep;
+ 
+ 	rmap_printk("rmap_set_dirty: spte %p %llx\n", sptep, *sptep);
+ 
+ 	spte |= shadow_dirty_mask;
+ 
+ 	return mmu_spte_update(sptep, spte);
+ }
+ 
+ static bool __rmap_set_dirty(struct kvm *kvm, unsigned long *rmapp)
+ {
+ 	u64 *sptep;
+ 	struct rmap_iterator iter;
+ 	bool flush = false;
+ 
+ 	for_each_rmap_spte(rmapp, &iter, sptep)
+ 		flush |= spte_set_dirty(kvm, sptep);
+ 
+ 	return flush;
+ }
+ 
++>>>>>>> 0d5367900a31 (KVM: MMU: introduce for_each_rmap_spte())
  /**
 - * kvm_mmu_write_protect_pt_masked - write protect selected PT level pages
 + * kvm_arch_mmu_write_protect_pt_masked - write protect selected PT level pages
   * @kvm: kvm instance
   * @slot: slot to protect
   * @gfn_offset: start of the BITS_PER_LONG pages we care about
@@@ -1415,30 -1508,15 +1464,31 @@@ static int kvm_age_rmapp(struct kvm *kv
  	struct rmap_iterator uninitialized_var(iter);
  	int young = 0;
  
 -	BUG_ON(!shadow_accessed_mask);
 +	/*
 +	 * In case of absence of EPT Access and Dirty Bits supports,
 +	 * emulate the accessed bit for EPT, by checking if this page has
 +	 * an EPT mapping, and clearing it if it does. On the next access,
 +	 * a new EPT mapping will be established.
 +	 * This has some overhead, but not as much as the cost of swapping
 +	 * out actively used pages or breaking up actively used hugepages.
 +	 */
 +	if (!shadow_accessed_mask) {
 +		young = kvm_unmap_rmapp(kvm, rmapp, slot, gfn, level, data);
 +		goto out;
 +	}
  
- 	for (sptep = rmap_get_first(*rmapp, &iter); sptep;
- 	     sptep = rmap_get_next(&iter)) {
- 		BUG_ON(!is_shadow_present_pte(*sptep));
- 
+ 	for_each_rmap_spte(rmapp, &iter, sptep)
  		if (*sptep & shadow_accessed_mask) {
  			young = 1;
  			clear_bit((ffs(shadow_accessed_mask) - 1),
  				 (unsigned long *)sptep);
  		}
++<<<<<<< HEAD
 +	}
 +out:
++=======
+ 
++>>>>>>> 0d5367900a31 (KVM: MMU: introduce for_each_rmap_spte())
  	trace_kvm_age_page(gfn, level, slot, young);
  	return young;
  }
@@@ -4372,6 -4458,186 +4418,189 @@@ void kvm_mmu_slot_remove_write_access(s
  		kvm_flush_remote_tlbs(kvm);
  }
  
++<<<<<<< HEAD
++=======
+ static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,
+ 		unsigned long *rmapp)
+ {
+ 	u64 *sptep;
+ 	struct rmap_iterator iter;
+ 	int need_tlb_flush = 0;
+ 	pfn_t pfn;
+ 	struct kvm_mmu_page *sp;
+ 
+ restart:
+ 	for_each_rmap_spte(rmapp, &iter, sptep) {
+ 		sp = page_header(__pa(sptep));
+ 		pfn = spte_to_pfn(*sptep);
+ 
+ 		/*
+ 		 * We cannot do huge page mapping for indirect shadow pages,
+ 		 * which are found on the last rmap (level = 1) when not using
+ 		 * tdp; such shadow pages are synced with the page table in
+ 		 * the guest, and the guest page table is using 4K page size
+ 		 * mapping if the indirect sp has level = 1.
+ 		 */
+ 		if (sp->role.direct &&
+ 			!kvm_is_reserved_pfn(pfn) &&
+ 			PageTransCompound(pfn_to_page(pfn))) {
+ 			drop_spte(kvm, sptep);
+ 			need_tlb_flush = 1;
+ 			goto restart;
+ 		}
+ 	}
+ 
+ 	return need_tlb_flush;
+ }
+ 
+ void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
+ 			struct kvm_memory_slot *memslot)
+ {
+ 	bool flush = false;
+ 	unsigned long *rmapp;
+ 	unsigned long last_index, index;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 
+ 	rmapp = memslot->arch.rmap[0];
+ 	last_index = gfn_to_index(memslot->base_gfn + memslot->npages - 1,
+ 				memslot->base_gfn, PT_PAGE_TABLE_LEVEL);
+ 
+ 	for (index = 0; index <= last_index; ++index, ++rmapp) {
+ 		if (*rmapp)
+ 			flush |= kvm_mmu_zap_collapsible_spte(kvm, rmapp);
+ 
+ 		if (need_resched() || spin_needbreak(&kvm->mmu_lock)) {
+ 			if (flush) {
+ 				kvm_flush_remote_tlbs(kvm);
+ 				flush = false;
+ 			}
+ 			cond_resched_lock(&kvm->mmu_lock);
+ 		}
+ 	}
+ 
+ 	if (flush)
+ 		kvm_flush_remote_tlbs(kvm);
+ 
+ 	spin_unlock(&kvm->mmu_lock);
+ }
+ 
+ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
+ 				   struct kvm_memory_slot *memslot)
+ {
+ 	gfn_t last_gfn;
+ 	unsigned long *rmapp;
+ 	unsigned long last_index, index;
+ 	bool flush = false;
+ 
+ 	last_gfn = memslot->base_gfn + memslot->npages - 1;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 
+ 	rmapp = memslot->arch.rmap[PT_PAGE_TABLE_LEVEL - 1];
+ 	last_index = gfn_to_index(last_gfn, memslot->base_gfn,
+ 			PT_PAGE_TABLE_LEVEL);
+ 
+ 	for (index = 0; index <= last_index; ++index, ++rmapp) {
+ 		if (*rmapp)
+ 			flush |= __rmap_clear_dirty(kvm, rmapp);
+ 
+ 		if (need_resched() || spin_needbreak(&kvm->mmu_lock))
+ 			cond_resched_lock(&kvm->mmu_lock);
+ 	}
+ 
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	lockdep_assert_held(&kvm->slots_lock);
+ 
+ 	/*
+ 	 * It's also safe to flush TLBs out of mmu lock here as currently this
+ 	 * function is only used for dirty logging, in which case flushing TLB
+ 	 * out of mmu lock also guarantees no dirty pages will be lost in
+ 	 * dirty_bitmap.
+ 	 */
+ 	if (flush)
+ 		kvm_flush_remote_tlbs(kvm);
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_slot_leaf_clear_dirty);
+ 
+ void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
+ 					struct kvm_memory_slot *memslot)
+ {
+ 	gfn_t last_gfn;
+ 	int i;
+ 	bool flush = false;
+ 
+ 	last_gfn = memslot->base_gfn + memslot->npages - 1;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 
+ 	for (i = PT_PAGE_TABLE_LEVEL + 1; /* skip rmap for 4K page */
+ 	     i < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++i) {
+ 		unsigned long *rmapp;
+ 		unsigned long last_index, index;
+ 
+ 		rmapp = memslot->arch.rmap[i - PT_PAGE_TABLE_LEVEL];
+ 		last_index = gfn_to_index(last_gfn, memslot->base_gfn, i);
+ 
+ 		for (index = 0; index <= last_index; ++index, ++rmapp) {
+ 			if (*rmapp)
+ 				flush |= __rmap_write_protect(kvm, rmapp,
+ 						false);
+ 
+ 			if (need_resched() || spin_needbreak(&kvm->mmu_lock))
+ 				cond_resched_lock(&kvm->mmu_lock);
+ 		}
+ 	}
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	/* see kvm_mmu_slot_remove_write_access */
+ 	lockdep_assert_held(&kvm->slots_lock);
+ 
+ 	if (flush)
+ 		kvm_flush_remote_tlbs(kvm);
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_slot_largepage_remove_write_access);
+ 
+ void kvm_mmu_slot_set_dirty(struct kvm *kvm,
+ 			    struct kvm_memory_slot *memslot)
+ {
+ 	gfn_t last_gfn;
+ 	int i;
+ 	bool flush = false;
+ 
+ 	last_gfn = memslot->base_gfn + memslot->npages - 1;
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 
+ 	for (i = PT_PAGE_TABLE_LEVEL;
+ 	     i < PT_PAGE_TABLE_LEVEL + KVM_NR_PAGE_SIZES; ++i) {
+ 		unsigned long *rmapp;
+ 		unsigned long last_index, index;
+ 
+ 		rmapp = memslot->arch.rmap[i - PT_PAGE_TABLE_LEVEL];
+ 		last_index = gfn_to_index(last_gfn, memslot->base_gfn, i);
+ 
+ 		for (index = 0; index <= last_index; ++index, ++rmapp) {
+ 			if (*rmapp)
+ 				flush |= __rmap_set_dirty(kvm, rmapp);
+ 
+ 			if (need_resched() || spin_needbreak(&kvm->mmu_lock))
+ 				cond_resched_lock(&kvm->mmu_lock);
+ 		}
+ 	}
+ 
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	lockdep_assert_held(&kvm->slots_lock);
+ 
+ 	/* see kvm_mmu_slot_leaf_clear_dirty */
+ 	if (flush)
+ 		kvm_flush_remote_tlbs(kvm);
+ }
+ EXPORT_SYMBOL_GPL(kvm_mmu_slot_set_dirty);
+ 
++>>>>>>> 0d5367900a31 (KVM: MMU: introduce for_each_rmap_spte())
  #define BATCH_ZAP_PAGES	10
  static void kvm_zap_obsolete_pages(struct kvm *kvm)
  {
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu_audit.c b/arch/x86/kvm/mmu_audit.c
index 1185fe7a7f47..0e46d555b9fc 100644
--- a/arch/x86/kvm/mmu_audit.c
+++ b/arch/x86/kvm/mmu_audit.c
@@ -197,13 +197,11 @@ static void audit_write_protection(struct kvm *kvm, struct kvm_mmu_page *sp)
 
 	rmapp = gfn_to_rmap(kvm, sp->gfn, PT_PAGE_TABLE_LEVEL);
 
-	for (sptep = rmap_get_first(*rmapp, &iter); sptep;
-	     sptep = rmap_get_next(&iter)) {
+	for_each_rmap_spte(rmapp, &iter, sptep)
 		if (is_writable_pte(*sptep))
 			audit_printk(kvm, "shadow page has writable "
 				     "mappings: gfn %llx role %x\n",
 				     sp->gfn, sp->role.word);
-	}
 }
 
 static void audit_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
