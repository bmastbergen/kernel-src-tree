Btrfs, replace: write dirty pages into the replace target device

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Miao Xie <miaox@cn.fujitsu.com>
commit 2c8cdd6ee4e7f637b0486c6798117e7859dee586
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/2c8cdd6e.failed

The implementation is simple:
- In order to avoid changing the code logic of btrfs_map_bio and
  RAID56, we add the stripes of the replace target devices at the
  end of the stripe array in btrfs bio, and we sort those target
  device stripes in the array. And we keep the number of the target
  device stripes in the btrfs bio.
- Except write operation on RAID56, all the other operation don't
  take the target device stripes into account.
- When we do write operation, we read the data from the common devices
  and calculate the parity. Then write the dirty data and new parity
  out, at this time, we will find the relative replace target stripes
  and wirte the relative data into it.

Note: The function that copying old data on the source device to
the target device was implemented in the past, it is similar to
the other RAID type.

	Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
(cherry picked from commit 2c8cdd6ee4e7f637b0486c6798117e7859dee586)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/btrfs/raid56.c
diff --cc fs/btrfs/raid56.c
index 6c2905ba2744,89a8486c34b3..000000000000
--- a/fs/btrfs/raid56.c
+++ b/fs/btrfs/raid56.c
@@@ -116,6 -131,9 +116,12 @@@ struct btrfs_raid_bio 
  	/* number of data stripes (no p/q) */
  	int nr_data;
  
++<<<<<<< HEAD
++=======
+ 	int real_stripes;
+ 
+ 	int stripe_npages;
++>>>>>>> 2c8cdd6ee4e7 (Btrfs, replace: write dirty pages into the replace target device)
  	/*
  	 * set if we're doing a parity rebuild
  	 * for a read from higher up, which is handled
@@@ -928,16 -983,16 +934,22 @@@ static struct btrfs_raid_bio *alloc_rbi
  {
  	struct btrfs_raid_bio *rbio;
  	int nr_data = 0;
++<<<<<<< HEAD
 +	int num_pages = rbio_nr_pages(stripe_len, bbio->num_stripes);
++=======
+ 	int real_stripes = bbio->num_stripes - bbio->num_tgtdevs;
+ 	int num_pages = rbio_nr_pages(stripe_len, real_stripes);
+ 	int stripe_npages = DIV_ROUND_UP(stripe_len, PAGE_SIZE);
++>>>>>>> 2c8cdd6ee4e7 (Btrfs, replace: write dirty pages into the replace target device)
  	void *p;
  
 -	rbio = kzalloc(sizeof(*rbio) + num_pages * sizeof(struct page *) * 2 +
 -		       DIV_ROUND_UP(stripe_npages, BITS_PER_LONG / 8),
 +	rbio = kzalloc(sizeof(*rbio) + num_pages * sizeof(struct page *) * 2,
  			GFP_NOFS);
 -	if (!rbio)
 +	if (!rbio) {
 +		kfree(raid_map);
 +		kfree(bbio);
  		return ERR_PTR(-ENOMEM);
 +	}
  
  	bio_list_init(&rbio->bio_list);
  	INIT_LIST_HEAD(&rbio->plug_list);
@@@ -949,6 -1004,8 +961,11 @@@
  	rbio->fs_info = root->fs_info;
  	rbio->stripe_len = stripe_len;
  	rbio->nr_pages = num_pages;
++<<<<<<< HEAD
++=======
+ 	rbio->real_stripes = real_stripes;
+ 	rbio->stripe_npages = stripe_npages;
++>>>>>>> 2c8cdd6ee4e7 (Btrfs, replace: write dirty pages into the replace target device)
  	rbio->faila = -1;
  	rbio->failb = -1;
  	atomic_set(&rbio->refs, 1);
@@@ -962,11 -1019,12 +979,11 @@@
  	p = rbio + 1;
  	rbio->stripe_pages = p;
  	rbio->bio_pages = p + sizeof(struct page *) * num_pages;
 -	rbio->dbitmap = p + sizeof(struct page *) * num_pages * 2;
  
- 	if (raid_map[bbio->num_stripes - 1] == RAID6_Q_STRIPE)
- 		nr_data = bbio->num_stripes - 2;
+ 	if (raid_map[real_stripes - 1] == RAID6_Q_STRIPE)
+ 		nr_data = real_stripes - 2;
  	else
- 		nr_data = bbio->num_stripes - 1;
+ 		nr_data = real_stripes - 1;
  
  	rbio->nr_data = nr_data;
  	return rbio;
@@@ -1941,10 -2043,9 +1985,9 @@@ static void raid_recover_end_io(struct 
  static int __raid56_parity_recover(struct btrfs_raid_bio *rbio)
  {
  	int bios_to_read = 0;
- 	struct btrfs_bio *bbio = rbio->bbio;
  	struct bio_list bio_list;
  	int ret;
 -	int nr_pages = DIV_ROUND_UP(rbio->stripe_len, PAGE_CACHE_SIZE);
 +	int nr_pages = (rbio->stripe_len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
  	int pagenr;
  	int stripe;
  	struct bio *bio;
@@@ -2102,3 -2206,460 +2145,463 @@@ static void read_rebuild_work(struct bt
  	rbio = container_of(work, struct btrfs_raid_bio, work);
  	__raid56_parity_recover(rbio);
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * The following code is used to scrub/replace the parity stripe
+  *
+  * Note: We need make sure all the pages that add into the scrub/replace
+  * raid bio are correct and not be changed during the scrub/replace. That
+  * is those pages just hold metadata or file data with checksum.
+  */
+ 
+ struct btrfs_raid_bio *
+ raid56_parity_alloc_scrub_rbio(struct btrfs_root *root, struct bio *bio,
+ 			       struct btrfs_bio *bbio, u64 *raid_map,
+ 			       u64 stripe_len, struct btrfs_device *scrub_dev,
+ 			       unsigned long *dbitmap, int stripe_nsectors)
+ {
+ 	struct btrfs_raid_bio *rbio;
+ 	int i;
+ 
+ 	rbio = alloc_rbio(root, bbio, raid_map, stripe_len);
+ 	if (IS_ERR(rbio))
+ 		return NULL;
+ 	bio_list_add(&rbio->bio_list, bio);
+ 	/*
+ 	 * This is a special bio which is used to hold the completion handler
+ 	 * and make the scrub rbio is similar to the other types
+ 	 */
+ 	ASSERT(!bio->bi_iter.bi_size);
+ 	rbio->operation = BTRFS_RBIO_PARITY_SCRUB;
+ 
+ 	for (i = 0; i < rbio->real_stripes; i++) {
+ 		if (bbio->stripes[i].dev == scrub_dev) {
+ 			rbio->scrubp = i;
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* Now we just support the sectorsize equals to page size */
+ 	ASSERT(root->sectorsize == PAGE_SIZE);
+ 	ASSERT(rbio->stripe_npages == stripe_nsectors);
+ 	bitmap_copy(rbio->dbitmap, dbitmap, stripe_nsectors);
+ 
+ 	return rbio;
+ }
+ 
+ void raid56_parity_add_scrub_pages(struct btrfs_raid_bio *rbio,
+ 				   struct page *page, u64 logical)
+ {
+ 	int stripe_offset;
+ 	int index;
+ 
+ 	ASSERT(logical >= rbio->raid_map[0]);
+ 	ASSERT(logical + PAGE_SIZE <= rbio->raid_map[0] +
+ 				rbio->stripe_len * rbio->nr_data);
+ 	stripe_offset = (int)(logical - rbio->raid_map[0]);
+ 	index = stripe_offset >> PAGE_CACHE_SHIFT;
+ 	rbio->bio_pages[index] = page;
+ }
+ 
+ /*
+  * We just scrub the parity that we have correct data on the same horizontal,
+  * so we needn't allocate all pages for all the stripes.
+  */
+ static int alloc_rbio_essential_pages(struct btrfs_raid_bio *rbio)
+ {
+ 	int i;
+ 	int bit;
+ 	int index;
+ 	struct page *page;
+ 
+ 	for_each_set_bit(bit, rbio->dbitmap, rbio->stripe_npages) {
+ 		for (i = 0; i < rbio->real_stripes; i++) {
+ 			index = i * rbio->stripe_npages + bit;
+ 			if (rbio->stripe_pages[index])
+ 				continue;
+ 
+ 			page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
+ 			if (!page)
+ 				return -ENOMEM;
+ 			rbio->stripe_pages[index] = page;
+ 			ClearPageUptodate(page);
+ 		}
+ 	}
+ 	return 0;
+ }
+ 
+ /*
+  * end io function used by finish_rmw.  When we finally
+  * get here, we've written a full stripe
+  */
+ static void raid_write_parity_end_io(struct bio *bio, int err)
+ {
+ 	struct btrfs_raid_bio *rbio = bio->bi_private;
+ 
+ 	if (err)
+ 		fail_bio_stripe(rbio, bio);
+ 
+ 	bio_put(bio);
+ 
+ 	if (!atomic_dec_and_test(&rbio->stripes_pending))
+ 		return;
+ 
+ 	err = 0;
+ 
+ 	if (atomic_read(&rbio->error))
+ 		err = -EIO;
+ 
+ 	rbio_orig_end_io(rbio, err, 0);
+ }
+ 
+ static noinline void finish_parity_scrub(struct btrfs_raid_bio *rbio,
+ 					 int need_check)
+ {
+ 	void *pointers[rbio->real_stripes];
+ 	int nr_data = rbio->nr_data;
+ 	int stripe;
+ 	int pagenr;
+ 	int p_stripe = -1;
+ 	int q_stripe = -1;
+ 	struct page *p_page = NULL;
+ 	struct page *q_page = NULL;
+ 	struct bio_list bio_list;
+ 	struct bio *bio;
+ 	int ret;
+ 
+ 	bio_list_init(&bio_list);
+ 
+ 	if (rbio->real_stripes - rbio->nr_data == 1) {
+ 		p_stripe = rbio->real_stripes - 1;
+ 	} else if (rbio->real_stripes - rbio->nr_data == 2) {
+ 		p_stripe = rbio->real_stripes - 2;
+ 		q_stripe = rbio->real_stripes - 1;
+ 	} else {
+ 		BUG();
+ 	}
+ 
+ 	/*
+ 	 * Because the higher layers(scrubber) are unlikely to
+ 	 * use this area of the disk again soon, so don't cache
+ 	 * it.
+ 	 */
+ 	clear_bit(RBIO_CACHE_READY_BIT, &rbio->flags);
+ 
+ 	if (!need_check)
+ 		goto writeback;
+ 
+ 	p_page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
+ 	if (!p_page)
+ 		goto cleanup;
+ 	SetPageUptodate(p_page);
+ 
+ 	if (q_stripe != -1) {
+ 		q_page = alloc_page(GFP_NOFS | __GFP_HIGHMEM);
+ 		if (!q_page) {
+ 			__free_page(p_page);
+ 			goto cleanup;
+ 		}
+ 		SetPageUptodate(q_page);
+ 	}
+ 
+ 	atomic_set(&rbio->error, 0);
+ 
+ 	for_each_set_bit(pagenr, rbio->dbitmap, rbio->stripe_npages) {
+ 		struct page *p;
+ 		void *parity;
+ 		/* first collect one page from each data stripe */
+ 		for (stripe = 0; stripe < nr_data; stripe++) {
+ 			p = page_in_rbio(rbio, stripe, pagenr, 0);
+ 			pointers[stripe] = kmap(p);
+ 		}
+ 
+ 		/* then add the parity stripe */
+ 		pointers[stripe++] = kmap(p_page);
+ 
+ 		if (q_stripe != -1) {
+ 
+ 			/*
+ 			 * raid6, add the qstripe and call the
+ 			 * library function to fill in our p/q
+ 			 */
+ 			pointers[stripe++] = kmap(q_page);
+ 
+ 			raid6_call.gen_syndrome(rbio->real_stripes, PAGE_SIZE,
+ 						pointers);
+ 		} else {
+ 			/* raid5 */
+ 			memcpy(pointers[nr_data], pointers[0], PAGE_SIZE);
+ 			run_xor(pointers + 1, nr_data - 1, PAGE_CACHE_SIZE);
+ 		}
+ 
+ 		/* Check scrubbing pairty and repair it */
+ 		p = rbio_stripe_page(rbio, rbio->scrubp, pagenr);
+ 		parity = kmap(p);
+ 		if (memcmp(parity, pointers[rbio->scrubp], PAGE_CACHE_SIZE))
+ 			memcpy(parity, pointers[rbio->scrubp], PAGE_CACHE_SIZE);
+ 		else
+ 			/* Parity is right, needn't writeback */
+ 			bitmap_clear(rbio->dbitmap, pagenr, 1);
+ 		kunmap(p);
+ 
+ 		for (stripe = 0; stripe < rbio->real_stripes; stripe++)
+ 			kunmap(page_in_rbio(rbio, stripe, pagenr, 0));
+ 	}
+ 
+ 	__free_page(p_page);
+ 	if (q_page)
+ 		__free_page(q_page);
+ 
+ writeback:
+ 	/*
+ 	 * time to start writing.  Make bios for everything from the
+ 	 * higher layers (the bio_list in our rbio) and our p/q.  Ignore
+ 	 * everything else.
+ 	 */
+ 	for_each_set_bit(pagenr, rbio->dbitmap, rbio->stripe_npages) {
+ 		struct page *page;
+ 
+ 		page = rbio_stripe_page(rbio, rbio->scrubp, pagenr);
+ 		ret = rbio_add_io_page(rbio, &bio_list,
+ 			       page, rbio->scrubp, pagenr, rbio->stripe_len);
+ 		if (ret)
+ 			goto cleanup;
+ 	}
+ 
+ 	nr_data = bio_list_size(&bio_list);
+ 	if (!nr_data) {
+ 		/* Every parity is right */
+ 		rbio_orig_end_io(rbio, 0, 0);
+ 		return;
+ 	}
+ 
+ 	atomic_set(&rbio->stripes_pending, nr_data);
+ 
+ 	while (1) {
+ 		bio = bio_list_pop(&bio_list);
+ 		if (!bio)
+ 			break;
+ 
+ 		bio->bi_private = rbio;
+ 		bio->bi_end_io = raid_write_parity_end_io;
+ 		BUG_ON(!test_bit(BIO_UPTODATE, &bio->bi_flags));
+ 		submit_bio(WRITE, bio);
+ 	}
+ 	return;
+ 
+ cleanup:
+ 	rbio_orig_end_io(rbio, -EIO, 0);
+ }
+ 
+ static inline int is_data_stripe(struct btrfs_raid_bio *rbio, int stripe)
+ {
+ 	if (stripe >= 0 && stripe < rbio->nr_data)
+ 		return 1;
+ 	return 0;
+ }
+ 
+ /*
+  * While we're doing the parity check and repair, we could have errors
+  * in reading pages off the disk.  This checks for errors and if we're
+  * not able to read the page it'll trigger parity reconstruction.  The
+  * parity scrub will be finished after we've reconstructed the failed
+  * stripes
+  */
+ static void validate_rbio_for_parity_scrub(struct btrfs_raid_bio *rbio)
+ {
+ 	if (atomic_read(&rbio->error) > rbio->bbio->max_errors)
+ 		goto cleanup;
+ 
+ 	if (rbio->faila >= 0 || rbio->failb >= 0) {
+ 		int dfail = 0, failp = -1;
+ 
+ 		if (is_data_stripe(rbio, rbio->faila))
+ 			dfail++;
+ 		else if (is_parity_stripe(rbio->faila))
+ 			failp = rbio->faila;
+ 
+ 		if (is_data_stripe(rbio, rbio->failb))
+ 			dfail++;
+ 		else if (is_parity_stripe(rbio->failb))
+ 			failp = rbio->failb;
+ 
+ 		/*
+ 		 * Because we can not use a scrubbing parity to repair
+ 		 * the data, so the capability of the repair is declined.
+ 		 * (In the case of RAID5, we can not repair anything)
+ 		 */
+ 		if (dfail > rbio->bbio->max_errors - 1)
+ 			goto cleanup;
+ 
+ 		/*
+ 		 * If all data is good, only parity is correctly, just
+ 		 * repair the parity.
+ 		 */
+ 		if (dfail == 0) {
+ 			finish_parity_scrub(rbio, 0);
+ 			return;
+ 		}
+ 
+ 		/*
+ 		 * Here means we got one corrupted data stripe and one
+ 		 * corrupted parity on RAID6, if the corrupted parity
+ 		 * is scrubbing parity, luckly, use the other one to repair
+ 		 * the data, or we can not repair the data stripe.
+ 		 */
+ 		if (failp != rbio->scrubp)
+ 			goto cleanup;
+ 
+ 		__raid_recover_end_io(rbio);
+ 	} else {
+ 		finish_parity_scrub(rbio, 1);
+ 	}
+ 	return;
+ 
+ cleanup:
+ 	rbio_orig_end_io(rbio, -EIO, 0);
+ }
+ 
+ /*
+  * end io for the read phase of the rmw cycle.  All the bios here are physical
+  * stripe bios we've read from the disk so we can recalculate the parity of the
+  * stripe.
+  *
+  * This will usually kick off finish_rmw once all the bios are read in, but it
+  * may trigger parity reconstruction if we had any errors along the way
+  */
+ static void raid56_parity_scrub_end_io(struct bio *bio, int err)
+ {
+ 	struct btrfs_raid_bio *rbio = bio->bi_private;
+ 
+ 	if (err)
+ 		fail_bio_stripe(rbio, bio);
+ 	else
+ 		set_bio_pages_uptodate(bio);
+ 
+ 	bio_put(bio);
+ 
+ 	if (!atomic_dec_and_test(&rbio->stripes_pending))
+ 		return;
+ 
+ 	/*
+ 	 * this will normally call finish_rmw to start our write
+ 	 * but if there are any failed stripes we'll reconstruct
+ 	 * from parity first
+ 	 */
+ 	validate_rbio_for_parity_scrub(rbio);
+ }
+ 
+ static void raid56_parity_scrub_stripe(struct btrfs_raid_bio *rbio)
+ {
+ 	int bios_to_read = 0;
+ 	struct bio_list bio_list;
+ 	int ret;
+ 	int pagenr;
+ 	int stripe;
+ 	struct bio *bio;
+ 
+ 	ret = alloc_rbio_essential_pages(rbio);
+ 	if (ret)
+ 		goto cleanup;
+ 
+ 	bio_list_init(&bio_list);
+ 
+ 	atomic_set(&rbio->error, 0);
+ 	/*
+ 	 * build a list of bios to read all the missing parts of this
+ 	 * stripe
+ 	 */
+ 	for (stripe = 0; stripe < rbio->real_stripes; stripe++) {
+ 		for_each_set_bit(pagenr, rbio->dbitmap, rbio->stripe_npages) {
+ 			struct page *page;
+ 			/*
+ 			 * we want to find all the pages missing from
+ 			 * the rbio and read them from the disk.  If
+ 			 * page_in_rbio finds a page in the bio list
+ 			 * we don't need to read it off the stripe.
+ 			 */
+ 			page = page_in_rbio(rbio, stripe, pagenr, 1);
+ 			if (page)
+ 				continue;
+ 
+ 			page = rbio_stripe_page(rbio, stripe, pagenr);
+ 			/*
+ 			 * the bio cache may have handed us an uptodate
+ 			 * page.  If so, be happy and use it
+ 			 */
+ 			if (PageUptodate(page))
+ 				continue;
+ 
+ 			ret = rbio_add_io_page(rbio, &bio_list, page,
+ 				       stripe, pagenr, rbio->stripe_len);
+ 			if (ret)
+ 				goto cleanup;
+ 		}
+ 	}
+ 
+ 	bios_to_read = bio_list_size(&bio_list);
+ 	if (!bios_to_read) {
+ 		/*
+ 		 * this can happen if others have merged with
+ 		 * us, it means there is nothing left to read.
+ 		 * But if there are missing devices it may not be
+ 		 * safe to do the full stripe write yet.
+ 		 */
+ 		goto finish;
+ 	}
+ 
+ 	/*
+ 	 * the bbio may be freed once we submit the last bio.  Make sure
+ 	 * not to touch it after that
+ 	 */
+ 	atomic_set(&rbio->stripes_pending, bios_to_read);
+ 	while (1) {
+ 		bio = bio_list_pop(&bio_list);
+ 		if (!bio)
+ 			break;
+ 
+ 		bio->bi_private = rbio;
+ 		bio->bi_end_io = raid56_parity_scrub_end_io;
+ 
+ 		btrfs_bio_wq_end_io(rbio->fs_info, bio,
+ 				    BTRFS_WQ_ENDIO_RAID56);
+ 
+ 		BUG_ON(!test_bit(BIO_UPTODATE, &bio->bi_flags));
+ 		submit_bio(READ, bio);
+ 	}
+ 	/* the actual write will happen once the reads are done */
+ 	return;
+ 
+ cleanup:
+ 	rbio_orig_end_io(rbio, -EIO, 0);
+ 	return;
+ 
+ finish:
+ 	validate_rbio_for_parity_scrub(rbio);
+ }
+ 
+ static void scrub_parity_work(struct btrfs_work *work)
+ {
+ 	struct btrfs_raid_bio *rbio;
+ 
+ 	rbio = container_of(work, struct btrfs_raid_bio, work);
+ 	raid56_parity_scrub_stripe(rbio);
+ }
+ 
+ static void async_scrub_parity(struct btrfs_raid_bio *rbio)
+ {
+ 	btrfs_init_work(&rbio->work, btrfs_rmw_helper,
+ 			scrub_parity_work, NULL, NULL);
+ 
+ 	btrfs_queue_work(rbio->fs_info->rmw_workers,
+ 			 &rbio->work);
+ }
+ 
+ void raid56_parity_submit_scrub_rbio(struct btrfs_raid_bio *rbio)
+ {
+ 	if (!lock_stripe_add(rbio))
+ 		async_scrub_parity(rbio);
+ }
++>>>>>>> 2c8cdd6ee4e7 (Btrfs, replace: write dirty pages into the replace target device)
* Unmerged path fs/btrfs/raid56.c
diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c
index a656dcd9e09a..19beb84a762b 100644
--- a/fs/btrfs/volumes.c
+++ b/fs/btrfs/volumes.c
@@ -4887,13 +4887,15 @@ static inline int parity_smaller(u64 a, u64 b)
 static void sort_parity_stripes(struct btrfs_bio *bbio, u64 *raid_map)
 {
 	struct btrfs_bio_stripe s;
+	int real_stripes = bbio->num_stripes - bbio->num_tgtdevs;
 	int i;
 	u64 l;
 	int again = 1;
+	int m;
 
 	while (again) {
 		again = 0;
-		for (i = 0; i < bbio->num_stripes - 1; i++) {
+		for (i = 0; i < real_stripes - 1; i++) {
 			if (parity_smaller(raid_map[i], raid_map[i+1])) {
 				s = bbio->stripes[i];
 				l = raid_map[i];
@@ -4901,6 +4903,14 @@ static void sort_parity_stripes(struct btrfs_bio *bbio, u64 *raid_map)
 				raid_map[i] = raid_map[i+1];
 				bbio->stripes[i+1] = s;
 				raid_map[i+1] = l;
+
+				if (bbio->tgtdev_map) {
+					m = bbio->tgtdev_map[i];
+					bbio->tgtdev_map[i] =
+							bbio->tgtdev_map[i + 1];
+					bbio->tgtdev_map[i + 1] = m;
+				}
+
 				again = 1;
 			}
 		}
@@ -4929,6 +4939,7 @@ static int __btrfs_map_block(struct btrfs_fs_info *fs_info, int rw,
 	int ret = 0;
 	int num_stripes;
 	int max_errors = 0;
+	int tgtdev_indexes = 0;
 	struct btrfs_bio *bbio = NULL;
 	struct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;
 	int dev_replace_is_ongoing = 0;
@@ -5238,14 +5249,19 @@ static int __btrfs_map_block(struct btrfs_fs_info *fs_info, int rw,
 			num_alloc_stripes <<= 1;
 		if (rw & REQ_GET_READ_MIRRORS)
 			num_alloc_stripes++;
+		tgtdev_indexes = num_stripes;
 	}
-	bbio = kzalloc(btrfs_bio_size(num_alloc_stripes), GFP_NOFS);
+
+	bbio = kzalloc(btrfs_bio_size(num_alloc_stripes, tgtdev_indexes),
+		       GFP_NOFS);
 	if (!bbio) {
 		kfree(raid_map);
 		ret = -ENOMEM;
 		goto out;
 	}
 	atomic_set(&bbio->error, 0);
+	if (dev_replace_is_ongoing)
+		bbio->tgtdev_map = (int *)(bbio->stripes + num_alloc_stripes);
 
 	if (rw & REQ_DISCARD) {
 		int factor = 0;
@@ -5330,6 +5346,7 @@ static int __btrfs_map_block(struct btrfs_fs_info *fs_info, int rw,
 	if (rw & (REQ_WRITE | REQ_GET_READ_MIRRORS))
 		max_errors = btrfs_chunk_max_errors(map);
 
+	tgtdev_indexes = 0;
 	if (dev_replace_is_ongoing && (rw & (REQ_WRITE | REQ_DISCARD)) &&
 	    dev_replace->tgtdev != NULL) {
 		int index_where_to_add;
@@ -5358,8 +5375,10 @@ static int __btrfs_map_block(struct btrfs_fs_info *fs_info, int rw,
 				new->physical = old->physical;
 				new->length = old->length;
 				new->dev = dev_replace->tgtdev;
+				bbio->tgtdev_map[i] = index_where_to_add;
 				index_where_to_add++;
 				max_errors++;
+				tgtdev_indexes++;
 			}
 		}
 		num_stripes = index_where_to_add;
@@ -5405,7 +5424,9 @@ static int __btrfs_map_block(struct btrfs_fs_info *fs_info, int rw,
 				tgtdev_stripe->length =
 					bbio->stripes[index_srcdev].length;
 				tgtdev_stripe->dev = dev_replace->tgtdev;
+				bbio->tgtdev_map[index_srcdev] = num_stripes;
 
+				tgtdev_indexes++;
 				num_stripes++;
 			}
 		}
@@ -5415,6 +5436,7 @@ static int __btrfs_map_block(struct btrfs_fs_info *fs_info, int rw,
 	bbio->num_stripes = num_stripes;
 	bbio->max_errors = max_errors;
 	bbio->mirror_num = mirror_num;
+	bbio->num_tgtdevs = tgtdev_indexes;
 
 	/*
 	 * this is the case that REQ_READ && dev_replace_is_ongoing &&
diff --git a/fs/btrfs/volumes.h b/fs/btrfs/volumes.h
index 39015677fd30..5dcceaff7ca8 100644
--- a/fs/btrfs/volumes.h
+++ b/fs/btrfs/volumes.h
@@ -291,7 +291,7 @@ struct btrfs_bio_stripe {
 struct btrfs_bio;
 typedef void (btrfs_bio_end_io_t) (struct btrfs_bio *bio, int err);
 
-#define BTRFS_BIO_ORIG_BIO_SUBMITTED	0x1
+#define BTRFS_BIO_ORIG_BIO_SUBMITTED	(1 << 0)
 
 struct btrfs_bio {
 	atomic_t stripes_pending;
@@ -304,6 +304,8 @@ struct btrfs_bio {
 	int max_errors;
 	int num_stripes;
 	int mirror_num;
+	int num_tgtdevs;
+	int *tgtdev_map;
 	struct btrfs_bio_stripe stripes[];
 };
 
@@ -386,8 +388,10 @@ struct btrfs_balance_control {
 int btrfs_account_dev_extents_size(struct btrfs_device *device, u64 start,
 				   u64 end, u64 *length);
 
-#define btrfs_bio_size(n) (sizeof(struct btrfs_bio) + \
-			    (sizeof(struct btrfs_bio_stripe) * (n)))
+#define btrfs_bio_size(total_stripes, real_stripes)		\
+	(sizeof(struct btrfs_bio) +				\
+	 (sizeof(struct btrfs_bio_stripe) * (total_stripes)) +	\
+	 (sizeof(int) * (real_stripes)))
 
 int btrfs_map_block(struct btrfs_fs_info *fs_info, int rw,
 		    u64 logical, u64 *length,
