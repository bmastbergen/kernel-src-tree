arch: Add lightweight memory barriers dma_rmb() and dma_wmb()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [arch] Add lightweight memory barriers dma_rmb() and dma_wmb() (Alexander Duyck) [1205268]
Rebuild_FUZZ: 94.83%
commit-author Alexander Duyck <alexander.h.duyck@redhat.com>
commit 1077fa36f23e259858caf6f269a47393a5aff523
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/1077fa36.failed

There are a number of situations where the mandatory barriers rmb() and
wmb() are used to order memory/memory operations in the device drivers
and those barriers are much heavier than they actually need to be.  For
example in the case of PowerPC wmb() calls the heavy-weight sync
instruction when for coherent memory operations all that is really needed
is an lsync or eieio instruction.

This commit adds a coherent only version of the mandatory memory barriers
rmb() and wmb().  In most cases this should result in the barrier being the
same as the SMP barriers for the SMP case, however in some cases we use a
barrier that is somewhere in between rmb() and smp_rmb().  For example on
ARM the rmb barriers break down as follows:

  Barrier   Call     Explanation
  --------- -------- ----------------------------------
  rmb()     dsb()    Data synchronization barrier - system
  dma_rmb() dmb(osh) data memory barrier - outer sharable
  smp_rmb() dmb(ish) data memory barrier - inner sharable

These new barriers are not as safe as the standard rmb() and wmb().
Specifically they do not guarantee ordering between coherent and incoherent
memories.  The primary use case for these would be to enforce ordering of
reads and writes when accessing coherent memory that is shared between the
CPU and a device.

It may also be noted that there is no dma_mb().  Most architectures don't
provide a good mechanism for performing a coherent only full barrier without
resorting to the same mechanism used in mb().  As such there isn't much to
be gained in trying to define such a function.

	Cc: Frederic Weisbecker <fweisbec@gmail.com>
	Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
	Cc: Michael Ellerman <michael@ellerman.id.au>
	Cc: Michael Neuling <mikey@neuling.org>
	Cc: Russell King <linux@arm.linux.org.uk>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ingo Molnar <mingo@kernel.org>
	Cc: David Miller <davem@davemloft.net>
	Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Acked-by: Will Deacon <will.deacon@arm.com>
	Signed-off-by: Alexander Duyck <alexander.h.duyck@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1077fa36f23e259858caf6f269a47393a5aff523)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/memory-barriers.txt
#	arch/arm/include/asm/barrier.h
#	arch/s390/include/asm/barrier.h
#	arch/sparc/include/asm/barrier_64.h
#	arch/x86/include/asm/barrier.h
#	arch/x86/um/asm/barrier.h
#	include/asm-generic/barrier.h
diff --cc Documentation/memory-barriers.txt
index fa5d8a9ae205,70a09f8a0383..000000000000
--- a/Documentation/memory-barriers.txt
+++ b/Documentation/memory-barriers.txt
@@@ -1119,26 -1633,47 +1119,70 @@@ There are some more advanced barrier fu
       operations" subsection for information on where to use these.
  
  
++<<<<<<< HEAD
 + (*) smp_mb__before_clear_bit(void);
 + (*) smp_mb__after_clear_bit(void);
 +
 +     These are for use similar to the atomic inc/dec barriers.  These are
 +     typically used for bitwise unlocking operations, so care must be taken as
 +     there are no implicit memory barriers here either.
 +
 +     Consider implementing an unlock operation of some nature by clearing a
 +     locking bit.  The clear_bit() would then need to be barriered like this:
 +
 +	smp_mb__before_clear_bit();
 +	clear_bit( ... );
 +
 +     This prevents memory operations before the clear leaking to after it.  See
 +     the subsection on "Locking Functions" with reference to UNLOCK operation
 +     implications.
 +
 +     See Documentation/atomic_ops.txt for more information.  See the "Atomic
 +     operations" subsection for information on where to use these.
 +
++=======
+  (*) dma_wmb();
+  (*) dma_rmb();
+ 
+      These are for use with consistent memory to guarantee the ordering
+      of writes or reads of shared memory accessible to both the CPU and a
+      DMA capable device.
+ 
+      For example, consider a device driver that shares memory with a device
+      and uses a descriptor status value to indicate if the descriptor belongs
+      to the device or the CPU, and a doorbell to notify it when new
+      descriptors are available:
+ 
+ 	if (desc->status != DEVICE_OWN) {
+ 		/* do not read data until we own descriptor */
+ 		dma_rmb();
+ 
+ 		/* read/modify data */
+ 		read_data = desc->data;
+ 		desc->data = write_data;
+ 
+ 		/* flush modifications before status update */
+ 		dma_wmb();
+ 
+ 		/* assign ownership */
+ 		desc->status = DEVICE_OWN;
+ 
+ 		/* force memory to sync before notifying device via MMIO */
+ 		wmb();
+ 
+ 		/* notify device of new descriptors */
+ 		writel(DESC_NOTIFY, doorbell);
+ 	}
+ 
+      The dma_rmb() allows us guarantee the device has released ownership
+      before we read the data from the descriptor, and he dma_wmb() allows
+      us to guarantee the data is written to the descriptor before the device
+      can see it now has ownership.  The wmb() is needed to guarantee that the
+      cache coherent memory writes have completed before attempting a write to
+      the cache incoherent MMIO region.
+ 
+      See Documentation/DMA-API.txt for more information on consistent memory.
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  
  MMIO WRITE BARRIER
  ------------------
diff --cc arch/arm/include/asm/barrier.h
index 8dcd9c702d90,d2f81e6b8c1c..000000000000
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@@ -42,7 -42,9 +42,13 @@@
  #elif defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
  #define mb()		do { dsb(); outer_sync(); } while (0)
  #define rmb()		dsb()
++<<<<<<< HEAD
 +#define wmb()		mb()
++=======
+ #define wmb()		do { dsb(st); outer_sync(); } while (0)
+ #define dma_rmb()	dmb(osh)
+ #define dma_wmb()	dmb(oshst)
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  #else
  #define mb()		barrier()
  #define rmb()		barrier()
diff --cc arch/s390/include/asm/barrier.h
index 19ff956b752b,8d724718ec21..000000000000
--- a/arch/s390/include/asm/barrier.h
+++ b/arch/s390/include/asm/barrier.h
@@@ -15,14 -15,17 +15,19 @@@
  
  #ifdef CONFIG_HAVE_MARCH_Z196_FEATURES
  /* Fast-BCR without checkpoint synchronization */
 -#define __ASM_BARRIER "bcr 14,0\n"
 +#define mb() do {  asm volatile("bcr 14,0" : : : "memory"); } while (0)
  #else
 -#define __ASM_BARRIER "bcr 15,0\n"
 +#define mb() do {  asm volatile("bcr 15,0" : : : "memory"); } while (0)
  #endif
  
 -#define mb() do {  asm volatile(__ASM_BARRIER : : : "memory"); } while (0)
 -
  #define rmb()				mb()
  #define wmb()				mb()
++<<<<<<< HEAD
 +#define read_barrier_depends()		do { } while(0)
++=======
+ #define dma_rmb()			rmb()
+ #define dma_wmb()			wmb()
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  #define smp_mb()			mb()
  #define smp_rmb()			rmb()
  #define smp_wmb()			wmb()
diff --cc arch/sparc/include/asm/barrier_64.h
index 95d45986f908,76648941fea7..000000000000
--- a/arch/sparc/include/asm/barrier_64.h
+++ b/arch/sparc/include/asm/barrier_64.h
@@@ -37,7 -37,9 +37,13 @@@ do {	__asm__ __volatile__("ba,pt	%%xcc
  #define rmb()	__asm__ __volatile__("":::"memory")
  #define wmb()	__asm__ __volatile__("":::"memory")
  
++<<<<<<< HEAD
 +#define read_barrier_depends()		do { } while(0)
++=======
+ #define dma_rmb()	rmb()
+ #define dma_wmb()	wmb()
+ 
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  #define set_mb(__var, __value) \
  	do { __var = __value; membar_safe("#StoreLoad"); } while(0)
  
diff --cc arch/x86/include/asm/barrier.h
index 72b200c1716e,2ab1eb33106e..000000000000
--- a/arch/x86/include/asm/barrier.h
+++ b/arch/x86/include/asm/barrier.h
@@@ -24,73 -24,17 +24,87 @@@
  #define wmb()	asm volatile("sfence" ::: "memory")
  #endif
  
++<<<<<<< HEAD
 +/**
 + * read_barrier_depends - Flush all pending reads that subsequents reads
 + * depend on.
 + *
 + * No data-dependent reads from memory-like regions are ever reordered
 + * over this barrier.  All reads preceding this primitive are guaranteed
 + * to access memory (but not necessarily other CPUs' caches) before any
 + * reads following this primitive that depend on the data return by
 + * any of the preceding reads.  This primitive is much lighter weight than
 + * rmb() on most CPUs, and is never heavier weight than is
 + * rmb().
 + *
 + * These ordering constraints are respected by both the local CPU
 + * and the compiler.
 + *
 + * Ordering is not guaranteed by anything other than these primitives,
 + * not even by data dependencies.  See the documentation for
 + * memory_barrier() for examples and URLs to more information.
 + *
 + * For example, the following code would force ordering (the initial
 + * value of "a" is zero, "b" is one, and "p" is "&a"):
 + *
 + * <programlisting>
 + *	CPU 0				CPU 1
 + *
 + *	b = 2;
 + *	memory_barrier();
 + *	p = &b;				q = p;
 + *					read_barrier_depends();
 + *					d = *q;
 + * </programlisting>
 + *
 + * because the read of "*q" depends on the read of "p" and these
 + * two reads are separated by a read_barrier_depends().  However,
 + * the following code, with the same initial values for "a" and "b":
 + *
 + * <programlisting>
 + *	CPU 0				CPU 1
 + *
 + *	a = 2;
 + *	memory_barrier();
 + *	b = 3;				y = b;
 + *					read_barrier_depends();
 + *					x = a;
 + * </programlisting>
 + *
 + * does not enforce ordering, since there is no data dependency between
 + * the read of "a" and the read of "b".  Therefore, on some CPUs, such
 + * as Alpha, "y" could be set to 3 and "x" to 0.  Use rmb()
 + * in cases like this where there are no data dependencies.
 + **/
 +
 +#define read_barrier_depends()	do { } while (0)
 +
 +#ifdef CONFIG_SMP
 +#define smp_mb()	mb()
 +#ifdef CONFIG_X86_PPRO_FENCE
 +# define smp_rmb()	rmb()
 +#else
 +# define smp_rmb()	barrier()
 +#endif
 +#ifdef CONFIG_X86_OOSTORE
 +# define smp_wmb() 	wmb()
 +#else
 +# define smp_wmb()	barrier()
 +#endif
 +#define smp_read_barrier_depends()	read_barrier_depends()
++=======
+ #ifdef CONFIG_X86_PPRO_FENCE
+ #define dma_rmb()	rmb()
+ #else
+ #define dma_rmb()	barrier()
+ #endif
+ #define dma_wmb()	barrier()
+ 
+ #ifdef CONFIG_SMP
+ #define smp_mb()	mb()
+ #define smp_rmb()	dma_rmb()
+ #define smp_wmb()	barrier()
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  #define set_mb(var, value) do { (void)xchg(&var, value); } while (0)
  #else /* !SMP */
  #define smp_mb()	barrier()
diff --cc arch/x86/um/asm/barrier.h
index 7d01b8c56c00,2d7d9a1f5b53..000000000000
--- a/arch/x86/um/asm/barrier.h
+++ b/arch/x86/um/asm/barrier.h
@@@ -29,24 -29,18 +29,38 @@@
  
  #endif /* CONFIG_X86_32 */
  
++<<<<<<< HEAD
 +#define read_barrier_depends()	do { } while (0)
++=======
+ #ifdef CONFIG_X86_PPRO_FENCE
+ #define dma_rmb()	rmb()
+ #else /* CONFIG_X86_PPRO_FENCE */
+ #define dma_rmb()	barrier()
+ #endif /* CONFIG_X86_PPRO_FENCE */
+ #define dma_wmb()	barrier()
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  
  #ifdef CONFIG_SMP
  
  #define smp_mb()	mb()
++<<<<<<< HEAD
 +#ifdef CONFIG_X86_PPRO_FENCE
 +#define smp_rmb()	rmb()
 +#else /* CONFIG_X86_PPRO_FENCE */
 +#define smp_rmb()	barrier()
 +#endif /* CONFIG_X86_PPRO_FENCE */
 +
 +#ifdef CONFIG_X86_OOSTORE
 +#define smp_wmb()	wmb()
 +#else /* CONFIG_X86_OOSTORE */
 +#define smp_wmb()	barrier()
 +#endif /* CONFIG_X86_OOSTORE */
 +
 +#define smp_read_barrier_depends()	read_barrier_depends()
++=======
+ #define smp_rmb()	dma_rmb()
+ #define smp_wmb()	barrier()
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  #define set_mb(var, value) do { (void)xchg(&var, value); } while (0)
  
  #else /* CONFIG_SMP */
diff --cc include/asm-generic/barrier.h
index f9d3f04cc413,f5c40b0fadc2..000000000000
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@@ -16,19 -17,42 +16,39 @@@
  
  #ifndef __ASSEMBLY__
  
 -#include <linux/compiler.h>
 -
 -#ifndef nop
 -#define nop()	asm volatile ("nop")
 -#endif
 +#define nop() asm volatile ("nop")
  
  /*
 - * Force strict CPU ordering. And yes, this is required on UP too when we're
 - * talking to devices.
 + * Force strict CPU ordering.
 + * And yes, this is required on UP too when we're talking
 + * to devices.
   *
 - * Fall back to compiler barriers if nothing better is provided.
 + * This implementation only contains a compiler barrier.
   */
  
 -#ifndef mb
 -#define mb()	barrier()
 -#endif
 -
 -#ifndef rmb
 +#define mb()	asm volatile ("": : :"memory")
  #define rmb()	mb()
++<<<<<<< HEAD
 +#define wmb()	asm volatile ("": : :"memory")
++=======
+ #endif
+ 
+ #ifndef wmb
+ #define wmb()	mb()
+ #endif
+ 
+ #ifndef dma_rmb
+ #define dma_rmb()	rmb()
+ #endif
+ 
+ #ifndef dma_wmb
+ #define dma_wmb()	wmb()
+ #endif
+ 
+ #ifndef read_barrier_depends
+ #define read_barrier_depends()		do { } while (0)
+ #endif
++>>>>>>> 1077fa36f23e (arch: Add lightweight memory barriers dma_rmb() and dma_wmb())
  
  #ifdef CONFIG_SMP
  #define smp_mb()	mb()
* Unmerged path Documentation/memory-barriers.txt
* Unmerged path arch/arm/include/asm/barrier.h
diff --git a/arch/arm64/include/asm/barrier.h b/arch/arm64/include/asm/barrier.h
index d4a63338a53c..376f98cc014e 100644
--- a/arch/arm64/include/asm/barrier.h
+++ b/arch/arm64/include/asm/barrier.h
@@ -31,6 +31,9 @@
 #define rmb()		asm volatile("dsb ld" : : : "memory")
 #define wmb()		asm volatile("dsb st" : : : "memory")
 
+#define dma_rmb()	dmb(oshld)
+#define dma_wmb()	dmb(oshst)
+
 #ifndef CONFIG_SMP
 #define smp_mb()	barrier()
 #define smp_rmb()	barrier()
diff --git a/arch/ia64/include/asm/barrier.h b/arch/ia64/include/asm/barrier.h
index 60576e06b6fb..b1558c2cb3a7 100644
--- a/arch/ia64/include/asm/barrier.h
+++ b/arch/ia64/include/asm/barrier.h
@@ -40,6 +40,9 @@
 #define wmb()	mb()
 #define read_barrier_depends()	do { } while(0)
 
+#define dma_rmb()	mb()
+#define dma_wmb()	mb()
+
 #ifdef CONFIG_SMP
 # define smp_mb()	mb()
 # define smp_rmb()	rmb()
diff --git a/arch/metag/include/asm/barrier.h b/arch/metag/include/asm/barrier.h
index c90bfc6bf648..dfdbfc0f145b 100644
--- a/arch/metag/include/asm/barrier.h
+++ b/arch/metag/include/asm/barrier.h
@@ -4,8 +4,6 @@
 #include <asm/metag_mem.h>
 
 #define nop()		asm volatile ("NOP")
-#define mb()		wmb()
-#define rmb()		barrier()
 
 #ifdef CONFIG_METAG_META21
 
@@ -39,11 +37,13 @@ static inline void wr_fence(void)
 
 #endif /* !CONFIG_METAG_META21 */
 
-static inline void wmb(void)
-{
-	/* flush writes through the write combiner */
-	wr_fence();
-}
+/* flush writes through the write combiner */
+#define mb()		wr_fence()
+#define rmb()		barrier()
+#define wmb()		mb()
+
+#define dma_rmb()	rmb()
+#define dma_wmb()	wmb()
 
 #define read_barrier_depends()  do { } while (0)
 
diff --git a/arch/mips/include/asm/barrier.h b/arch/mips/include/asm/barrier.h
index 314ab5532019..3625f1749909 100644
--- a/arch/mips/include/asm/barrier.h
+++ b/arch/mips/include/asm/barrier.h
@@ -127,20 +127,21 @@
 
 #include <asm/wbflush.h>
 
-#define wmb()		fast_wmb()
-#define rmb()		fast_rmb()
 #define mb()		wbflush()
 #define iob()		wbflush()
 
 #else /* !CONFIG_CPU_HAS_WB */
 
-#define wmb()		fast_wmb()
-#define rmb()		fast_rmb()
 #define mb()		fast_mb()
 #define iob()		fast_iob()
 
 #endif /* !CONFIG_CPU_HAS_WB */
 
+#define wmb()		fast_wmb()
+#define rmb()		fast_rmb()
+#define dma_wmb()	fast_wmb()
+#define dma_rmb()	fast_rmb()
+
 #if defined(CONFIG_WEAK_ORDERING) && defined(CONFIG_SMP)
 # ifdef CONFIG_CPU_CAVIUM_OCTEON
 #  define smp_mb()	__sync()
diff --git a/arch/powerpc/include/asm/barrier.h b/arch/powerpc/include/asm/barrier.h
index bab79a110c7b..8973d8d25b58 100644
--- a/arch/powerpc/include/asm/barrier.h
+++ b/arch/powerpc/include/asm/barrier.h
@@ -37,8 +37,6 @@
 
 #define set_mb(var, value)	do { var = value; mb(); } while (0)
 
-#ifdef CONFIG_SMP
-
 #ifdef __SUBARCH_HAS_LWSYNC
 #    define SMPWMB      LWSYNC
 #else
@@ -46,13 +44,18 @@
 #endif
 
 #define __lwsync()	__asm__ __volatile__ (stringify_in_c(LWSYNC) : : :"memory")
+#define dma_rmb()	__lwsync()
+#define dma_wmb()	__asm__ __volatile__ (stringify_in_c(SMPWMB) : : :"memory")
+
+#ifdef CONFIG_SMP
+#define smp_lwsync()	__lwsync()
 
 #define smp_mb()	mb()
 #define smp_rmb()	__lwsync()
 #define smp_wmb()	__asm__ __volatile__ (stringify_in_c(SMPWMB) : : :"memory")
 #define smp_read_barrier_depends()	read_barrier_depends()
 #else
-#define __lwsync()	barrier()
+#define smp_lwsync()	barrier()
 
 #define smp_mb()	barrier()
 #define smp_rmb()	barrier()
@@ -72,7 +75,7 @@
 #define smp_store_release(p, v)						\
 do {									\
 	compiletime_assert_atomic_type(*p);				\
-	__lwsync();							\
+	smp_lwsync();							\
 	ACCESS_ONCE(*p) = (v);						\
 } while (0)
 
@@ -80,7 +83,7 @@ do {									\
 ({									\
 	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
-	__lwsync();							\
+	smp_lwsync();							\
 	___p1;								\
 })
 
* Unmerged path arch/s390/include/asm/barrier.h
* Unmerged path arch/sparc/include/asm/barrier_64.h
* Unmerged path arch/x86/include/asm/barrier.h
* Unmerged path arch/x86/um/asm/barrier.h
* Unmerged path include/asm-generic/barrier.h
