x86/mm: Use PAGE_ALIGNED(x) instead of IS_ALIGNED(x, PAGE_SIZE)

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [kvm] x86: vmx: use PAGE_ALIGNED instead of IS_ALIGNED(..., PAGE_SIZE) (Bandan Das) [1209995]
Rebuild_FUZZ: 89.76%
commit-author Fanjun Kong <bh1scw@gmail.com>
commit e19d11267f0e6c8aff2d15d2dfed12365b4c9184
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/e19d1126.failed

The <linux/mm.h> already provides the PAGE_ALIGNED() macro. Let's
use this macro instead of IS_ALIGNED() and passing PAGE_SIZE directly.

No change in functionality.

[ mingo: Tweak changelog. ]

	Signed-off-by: Fanjun Kong <bh1scw@gmail.com>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20220526142038.1582839-1-bh1scw@gmail.com
(cherry picked from commit e19d11267f0e6c8aff2d15d2dfed12365b4c9184)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/init_64.c
diff --cc arch/x86/mm/init_64.c
index 79259c2e343b,8779d6be6a49..000000000000
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@@ -1010,12 -1237,15 +1010,19 @@@ remove_pagetable(unsigned long start, u
  	flush_tlb_all();
  }
  
 -void __ref vmemmap_free(unsigned long start, unsigned long end,
 -		struct vmem_altmap *altmap)
 +void __ref vmemmap_free(unsigned long start, unsigned long end)
  {
++<<<<<<< HEAD
 +	remove_pagetable(start, end, false);
++=======
+ 	VM_BUG_ON(!PAGE_ALIGNED(start));
+ 	VM_BUG_ON(!PAGE_ALIGNED(end));
+ 
+ 	remove_pagetable(start, end, false, altmap);
++>>>>>>> e19d11267f0e (x86/mm: Use PAGE_ALIGNED(x) instead of IS_ALIGNED(x, PAGE_SIZE))
  }
  
 +#ifdef CONFIG_MEMORY_HOTREMOVE
  static void __meminit
  kernel_physical_mapping_remove(unsigned long start, unsigned long end)
  {
@@@ -1341,12 -1605,21 +1348,28 @@@ int __meminit vmemmap_populate(unsigne
  {
  	int err;
  
++<<<<<<< HEAD
 +	if (cpu_has_pse)
 +		err = vmemmap_populate_hugepages(start, end, node);
 +	else
 +		err = vmemmap_populate_basepages(start, end, node);
++=======
+ 	VM_BUG_ON(!PAGE_ALIGNED(start));
+ 	VM_BUG_ON(!PAGE_ALIGNED(end));
+ 
+ 	if (end - start < PAGES_PER_SECTION * sizeof(struct page))
+ 		err = vmemmap_populate_basepages(start, end, node, NULL);
+ 	else if (boot_cpu_has(X86_FEATURE_PSE))
+ 		err = vmemmap_populate_hugepages(start, end, node, altmap);
+ 	else if (altmap) {
+ 		pr_err_once("%s: no cpu support for altmap allocations\n",
+ 				__func__);
+ 		err = -ENOMEM;
+ 	} else
+ 		err = vmemmap_populate_basepages(start, end, node, NULL);
++>>>>>>> e19d11267f0e (x86/mm: Use PAGE_ALIGNED(x) instead of IS_ALIGNED(x, PAGE_SIZE))
  	if (!err)
 -		sync_global_pgds(start, end - 1);
 +		sync_global_pgds(start, end - 1, 0);
  	return err;
  }
  
* Unmerged path arch/x86/mm/init_64.c
