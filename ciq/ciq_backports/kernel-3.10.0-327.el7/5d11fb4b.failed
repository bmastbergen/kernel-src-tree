xfs: rework zero range to prevent invalid i_size updates

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Brian Foster <bfoster@redhat.com>
commit 5d11fb4b9a1d90983452c029b5e1377af78fda49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/5d11fb4b.failed

The zero range operation is analogous to fallocate with the exception of
converting the range to zeroes. E.g., it attempts to allocate zeroed
blocks over the range specified by the caller. The XFS implementation
kills all delalloc blocks currently over the aligned range, converts the
range to allocated zero blocks (unwritten extents) and handles the
partial pages at the ends of the range by sending writes through the
pagecache.

The current implementation suffers from several problems associated with
inode size. If the aligned range covers an extending I/O, said I/O is
discarded and an inode size update from a previous write never makes it
to disk. Further, if an unaligned zero range extends beyond eof, the
page write induced for the partial end page can itself increase the
inode size, even if the zero range request is not supposed to update
i_size (via KEEP_SIZE, similar to an fallocate beyond EOF).

The latter behavior not only incorrectly increases the inode size, but
can lead to stray delalloc blocks on the inode. Typically, post-eof
preallocation blocks are either truncated on release or inode eviction
or explicitly written to by xfs_zero_eof() on natural file size
extension. If the inode size increases due to zero range, however,
associated blocks leak into the address space having never been
converted or mapped to pagecache pages. A direct I/O to such an
uncovered range cannot convert the extent via writeback and will BUG().
For example:

$ xfs_io -fc "pwrite 0 128k" -c "fzero -k 1m 54321" <file>
...
$ xfs_io -d -c "pread 128k 128k" <file>
<BUG>

If the entire delalloc extent happens to not have page coverage
whatsoever (e.g., delalloc conversion couldn't find a large enough free
space extent), even a full file writeback won't convert what's left of
the extent and we'll assert on inode eviction.

Rework xfs_zero_file_space() to avoid buffered I/O for partial pages.
Use the existing hole punch and prealloc mechanisms as primitives for
zero range. This implementation is not efficient nor ideal as we
writeback dirty data over the range and remove existing extents rather
than convert to unwrittern. The former writeback, however, is currently
the only mechanism available to ensure consistency between pagecache and
extent state. Even a pagecache truncate/delalloc punch prior to hole
punch has lead to inconsistencies due to racing with writeback.

This provides a consistent, correct implementation of zero range that
survives fsstress/fsx testing without assert failures. The
implementation can be optimized from this point forward once the
fundamental issue of pagecache and delalloc extent state consistency is
addressed.

	Signed-off-by: Brian Foster <bfoster@redhat.com>
	Reviewed-by: Dave Chinner <dchinner@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>


(cherry picked from commit 5d11fb4b9a1d90983452c029b5e1377af78fda49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_bmap_util.c
diff --cc fs/xfs/xfs_bmap_util.c
index 247d2c73079e,281002689d64..000000000000
--- a/fs/xfs/xfs_bmap_util.c
+++ b/fs/xfs/xfs_bmap_util.c
@@@ -1378,61 -1354,148 +1379,75 @@@ xfs_zero_file_space
  
  	trace_xfs_zero_file_space(ip);
  
- 	granularity = max_t(uint, 1 << mp->m_sb.sb_blocklog, PAGE_CACHE_SIZE);
+ 	blksize = 1 << mp->m_sb.sb_blocklog;
  
  	/*
- 	 * Round the range of extents we are going to convert inwards.  If the
- 	 * offset is aligned, then it doesn't get changed so we zero from the
- 	 * start of the block offset points to.
+ 	 * Punch a hole and prealloc the range. We use hole punch rather than
+ 	 * unwritten extent conversion for two reasons:
+ 	 *
+ 	 * 1.) Hole punch handles partial block zeroing for us.
+ 	 *
+ 	 * 2.) If prealloc returns ENOSPC, the file range is still zero-valued
+ 	 * by virtue of the hole punch.
  	 */
 -	error = xfs_free_file_space(ip, offset, len);
 -	if (error)
 -		goto out;
 -
 -	error = xfs_alloc_file_space(ip, round_down(offset, blksize),
 -				     round_up(offset + len, blksize) -
 -				     round_down(offset, blksize),
 -				     XFS_BMAPI_PREALLOC);
 -out:
 -	return error;
 -
 -}
 -
 -/*
 - * xfs_collapse_file_space()
 - *	This routine frees disk space and shift extent for the given file.
 - *	The first thing we do is to free data blocks in the specified range
 - *	by calling xfs_free_file_space(). It would also sync dirty data
 - *	and invalidate page cache over the region on which collapse range
 - *	is working. And Shift extent records to the left to cover a hole.
 - * RETURNS:
 - *	0 on success
 - *	errno on error
 - *
 - */
 -int
 -xfs_collapse_file_space(
 -	struct xfs_inode	*ip,
 -	xfs_off_t		offset,
 -	xfs_off_t		len)
 -{
 -	int			done = 0;
 -	struct xfs_mount	*mp = ip->i_mount;
 -	struct xfs_trans	*tp;
 -	int			error;
 -	struct xfs_bmap_free	free_list;
 -	xfs_fsblock_t		first_block;
 -	int			committed;
 -	xfs_fileoff_t		start_fsb;
 -	xfs_fileoff_t		next_fsb;
 -	xfs_fileoff_t		shift_fsb;
 -
 -	ASSERT(xfs_isilocked(ip, XFS_IOLOCK_EXCL));
 -
 -	trace_xfs_collapse_file_space(ip);
 -
 -	next_fsb = XFS_B_TO_FSB(mp, offset + len);
 -	shift_fsb = XFS_B_TO_FSB(mp, len);
 -
 -	error = xfs_free_file_space(ip, offset, len);
 -	if (error)
 -		return error;
 -
 -	/*
 -	 * Trim eofblocks to avoid shifting uninitialized post-eof preallocation
 -	 * into the accessible region of the file.
 -	 */
 -	if (xfs_can_free_eofblocks(ip, true)) {
 -		error = xfs_free_eofblocks(mp, ip, false);
 -		if (error)
 -			return error;
 -	}
++<<<<<<< HEAD
 +	start_boundary = round_up(offset, granularity);
 +	end_boundary = round_down(offset + len, granularity);
  
 -	/*
 -	 * Writeback and invalidate cache for the remainder of the file as we're
 -	 * about to shift down every extent from the collapse range to EOF. The
 -	 * free of the collapse range above might have already done some of
 -	 * this, but we shouldn't rely on it to do anything outside of the range
 -	 * that was freed.
 -	 */
 -	error = filemap_write_and_wait_range(VFS_I(ip)->i_mapping,
 -					     offset + len, -1);
 -	if (error)
 -		return error;
 -	error = invalidate_inode_pages2_range(VFS_I(ip)->i_mapping,
 -					(offset + len) >> PAGE_CACHE_SHIFT, -1);
 -	if (error)
 -		return error;
 +	ASSERT(start_boundary >= offset);
 +	ASSERT(end_boundary <= offset + len);
  
 -	while (!error && !done) {
 -		tp = xfs_trans_alloc(mp, XFS_TRANS_DIOSTRAT);
 +	if (start_boundary < end_boundary - 1) {
  		/*
 -		 * We would need to reserve permanent block for transaction.
 -		 * This will come into picture when after shifting extent into
 -		 * hole we found that adjacent extents can be merged which
 -		 * may lead to freeing of a block during record update.
 +		 * Writeback the range to ensure any inode size updates due to
 +		 * appending writes make it to disk (otherwise we could just
 +		 * punch out the delalloc blocks).
  		 */
 -		error = xfs_trans_reserve(tp, &M_RES(mp)->tr_write,
 -				XFS_DIOSTRAT_SPACE_RES(mp, 0), 0);
 -		if (error) {
 -			xfs_trans_cancel(tp, 0);
 -			break;
 -		}
 +		error = -filemap_write_and_wait_range(VFS_I(ip)->i_mapping,
 +				start_boundary, end_boundary - 1);
 +		if (error)
 +			goto out;
 +		truncate_pagecache_range(VFS_I(ip), start_boundary,
 +					 end_boundary - 1);
  
 -		xfs_ilock(ip, XFS_ILOCK_EXCL);
 -		error = xfs_trans_reserve_quota(tp, mp, ip->i_udquot,
 -				ip->i_gdquot, ip->i_pdquot,
 -				XFS_DIOSTRAT_SPACE_RES(mp, 0), 0,
 -				XFS_QMOPT_RES_REGBLKS);
 +		/* convert the blocks */
 +		error = xfs_alloc_file_space(ip, start_boundary,
 +					end_boundary - start_boundary - 1,
 +					XFS_BMAPI_PREALLOC | XFS_BMAPI_CONVERT);
  		if (error)
  			goto out;
  
 -		xfs_trans_ijoin(tp, ip, 0);
 +		/* We've handled the interior of the range, now for the edges */
 +		if (start_boundary != offset) {
 +			error = xfs_iozero(ip, offset, start_boundary - offset);
 +			if (error)
 +				goto out;
 +		}
  
 -		xfs_bmap_init(&free_list, &first_block);
 +		if (end_boundary != offset + len)
 +			error = xfs_iozero(ip, end_boundary,
 +					   offset + len - end_boundary);
  
 +	} else {
  		/*
 -		 * We are using the write transaction in which max 2 bmbt
 -		 * updates are allowed
 +		 * It's either a sub-granularity range or the range spanned lies
 +		 * partially across two adjacent blocks.
  		 */
 -		start_fsb = next_fsb;
 -		error = xfs_bmap_shift_extents(tp, ip, start_fsb, shift_fsb,
 -				&done, &next_fsb, &first_block, &free_list,
 -				XFS_BMAP_MAX_SHIFT_EXTENTS);
 -		if (error)
 -			goto out;
 -
 -		error = xfs_bmap_finish(&tp, &free_list, &committed);
 -		if (error)
 -			goto out;
 -
 -		error = xfs_trans_commit(tp, XFS_TRANS_RELEASE_LOG_RES);
 -		xfs_iunlock(ip, XFS_ILOCK_EXCL);
 +		error = xfs_iozero(ip, offset, len);
  	}
++=======
++	error = xfs_free_file_space(ip, offset, len);
++	if (error)
++		goto out;
++>>>>>>> 5d11fb4b9a1d (xfs: rework zero range to prevent invalid i_size updates)
  
 -	return error;
 -
++	error = xfs_alloc_file_space(ip, round_down(offset, blksize),
++				     round_up(offset + len, blksize) -
++				     round_down(offset, blksize),
++				     XFS_BMAPI_PREALLOC);
  out:
 -	xfs_trans_cancel(tp, XFS_TRANS_RELEASE_LOG_RES | XFS_TRANS_ABORT);
 -	xfs_iunlock(ip, XFS_ILOCK_EXCL);
  	return error;
 +
  }
  
  /*
* Unmerged path fs/xfs/xfs_bmap_util.c
