NVMe: Fix command setup on IO retry

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Keith Busch <keith.busch@intel.com>
commit 9af8785a38d4528d6675247f873b0f1ae29f3be8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/9af8785a.failed

On retry, the req->special is pointing to an already setup IOD, but we
still need to setup the command context and callback, otherwise you'll
see false twice completed errors and leak requests.

	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 9af8785a38d4528d6675247f873b0f1ae29f3be8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/block/nvme-core.c
diff --cc drivers/block/nvme-core.c
index 48e1152870d9,bcbdf832b1b0..000000000000
--- a/drivers/block/nvme-core.c
+++ b/drivers/block/nvme-core.c
@@@ -835,25 -612,35 +835,32 @@@ static int nvme_split_flush_data(struc
  	return 0;
  }
  
 -static int nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 -			 const struct blk_mq_queue_data *bd)
 +/*
 + * Called with local interrupts disabled and the q_lock held.  May not sleep.
 + */
 +static int nvme_submit_bio_queue(struct nvme_queue *nvmeq, struct nvme_ns *ns,
 +								struct bio *bio)
  {
 -	struct nvme_ns *ns = hctx->queue->queuedata;
 -	struct nvme_queue *nvmeq = hctx->driver_data;
 -	struct request *req = bd->rq;
 -	struct nvme_cmd_info *cmd = blk_mq_rq_to_pdu(req);
  	struct nvme_iod *iod;
 -	int psegs = req->nr_phys_segments;
 -	int result = BLK_MQ_RQ_QUEUE_BUSY;
 -	enum dma_data_direction dma_dir;
 -	unsigned size = !(req->cmd_flags & REQ_DISCARD) ? blk_rq_bytes(req) :
 -						sizeof(struct nvme_dsm_range);
 +	int psegs = bio_phys_segments(ns->queue, bio);
 +	int result;
  
 -	/*
 -	 * Requeued IO has already been prepped
 -	 */
 -	iod = req->special;
 -	if (iod)
 -		goto submit_iod;
 +	if ((bio->bi_rw & REQ_FLUSH) && psegs)
 +		return nvme_split_flush_data(nvmeq, bio);
  
 -	iod = nvme_alloc_iod(psegs, size, ns->dev, GFP_ATOMIC);
 +	iod = nvme_alloc_iod(psegs, bio->bi_size, GFP_ATOMIC);
  	if (!iod)
 -		return result;
 +		return -ENOMEM;
  
++<<<<<<< HEAD
 +	iod->private = bio;
 +	if (bio->bi_rw & REQ_DISCARD) {
++=======
+ 	iod->private = req;
+ 	req->special = iod;
+ 
+ 	if (req->cmd_flags & REQ_DISCARD) {
++>>>>>>> 9af8785a38d4 (NVMe: Fix command setup on IO retry)
  		void *range;
  		/*
  		 * We reuse the small pool to allocate the 16-byte range here
@@@ -870,26 -655,41 +877,47 @@@
  		iod_list(iod)[0] = (__le64 *)range;
  		iod->npages = 0;
  	} else if (psegs) {
 -		dma_dir = rq_data_dir(req) ? DMA_TO_DEVICE : DMA_FROM_DEVICE;
 -
 -		sg_init_table(iod->sg, psegs);
 -		iod->nents = blk_rq_map_sg(req->q, req, iod->sg);
 -		if (!iod->nents) {
 -			result = BLK_MQ_RQ_QUEUE_ERROR;
 -			goto finish_cmd;
 +		result = nvme_map_bio(nvmeq, iod, bio,
 +			bio_data_dir(bio) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
 +			psegs);
 +		if (result <= 0)
 +			goto free_iod;
 +		if (nvme_setup_prps(nvmeq->dev, iod, result, GFP_ATOMIC) !=
 +								result) {
 +			result = -ENOMEM;
 +			goto free_iod;
  		}
 -
 -		if (!dma_map_sg(nvmeq->q_dmadev, iod->sg, iod->nents, dma_dir))
 -			goto finish_cmd;
 -
 -		if (blk_rq_bytes(req) != nvme_setup_prps(nvmeq->dev, iod,
 -						blk_rq_bytes(req), GFP_ATOMIC))
 -			goto finish_cmd;
 +		nvme_start_io_acct(bio);
  	}
 +	if (unlikely(nvme_submit_iod(nvmeq, iod))) {
 +		if (!waitqueue_active(&nvmeq->sq_full))
 +			add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
 +		list_add_tail(&iod->node, &nvmeq->iod_bio);
 +	}
 +	return 0;
  
++<<<<<<< HEAD
 + free_iod:
++=======
+ 	blk_mq_start_request(req);
+ 
+  submit_iod:
+ 	nvme_set_info(cmd, iod, req_completion);
+ 	spin_lock_irq(&nvmeq->q_lock);
+ 	if (req->cmd_flags & REQ_DISCARD)
+ 		nvme_submit_discard(nvmeq, ns, req, iod);
+ 	else if (req->cmd_flags & REQ_FLUSH)
+ 		nvme_submit_flush(nvmeq, ns, req->tag);
+ 	else
+ 		nvme_submit_iod(nvmeq, iod, ns);
+ 
+ 	nvme_process_cq(nvmeq);
+ 	spin_unlock_irq(&nvmeq->q_lock);
+ 	return BLK_MQ_RQ_QUEUE_OK;
+ 
+  finish_cmd:
+ 	nvme_finish_cmd(nvmeq, req->tag, NULL);
++>>>>>>> 9af8785a38d4 (NVMe: Fix command setup on IO retry)
  	nvme_free_iod(nvmeq->dev, iod);
  	return result;
  }
* Unmerged path drivers/block/nvme-core.c
