userfaultfd: propagate the full address in THP faults

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author Andrea Arcangeli <aarcange@redhat.com>
commit 230c92a8797e0e717c6732de0fffdd5726c0f48f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/230c92a8.failed

The THP faults were not propagating the original fault address.  The
latest version of the API with uffd.arg.pagefault.address is supposed to
propagate the full address through THP faults.

This was not a kernel crashing bug and it wouldn't risk to corrupt user
memory, but it would cause a SIGBUS failure because the wrong page was
being copied.

For various reasons this wasn't easily reproducible in the qemu workload,
but the strestest exposed the problem immediately.

	Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Pavel Emelyanov <xemul@parallels.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 230c92a8797e0e717c6732de0fffdd5726c0f48f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index bff17a8147f4,279a818a39b1..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -701,16 -717,29 +701,23 @@@ static inline pmd_t mk_huge_pmd(struct 
  
  static int __do_huge_pmd_anonymous_page(struct mm_struct *mm,
  					struct vm_area_struct *vma,
++<<<<<<< HEAD
 +					unsigned long haddr, pmd_t *pmd,
 +					struct page *page)
++=======
+ 					unsigned long address, pmd_t *pmd,
+ 					struct page *page, gfp_t gfp,
+ 					unsigned int flags)
++>>>>>>> 230c92a8797e (userfaultfd: propagate the full address in THP faults)
  {
 -	struct mem_cgroup *memcg;
  	pgtable_t pgtable;
  	spinlock_t *ptl;
+ 	unsigned long haddr = address & HPAGE_PMD_MASK;
  
  	VM_BUG_ON_PAGE(!PageCompound(page), page);
 -
 -	if (mem_cgroup_try_charge(page, mm, gfp, &memcg)) {
 -		put_page(page);
 -		count_vm_event(THP_FAULT_FALLBACK);
 -		return VM_FAULT_FALLBACK;
 -	}
 -
  	pgtable = pte_alloc_one(mm, haddr);
 -	if (unlikely(!pgtable)) {
 -		mem_cgroup_cancel_charge(page, memcg);
 -		put_page(page);
 +	if (unlikely(!pgtable))
  		return VM_FAULT_OOM;
 -	}
  
  	clear_huge_page(page, haddr, HPAGE_PMD_NR);
  	/*
@@@ -728,8 -757,26 +735,28 @@@
  		pte_free(mm, pgtable);
  	} else {
  		pmd_t entry;
++<<<<<<< HEAD
 +		entry = mk_huge_pmd(page, vma);
++=======
+ 
+ 		/* Deliver the page fault to userland */
+ 		if (userfaultfd_missing(vma)) {
+ 			int ret;
+ 
+ 			spin_unlock(ptl);
+ 			mem_cgroup_cancel_charge(page, memcg);
+ 			put_page(page);
+ 			pte_free(mm, pgtable);
+ 			ret = handle_userfault(vma, address, flags,
+ 					       VM_UFFD_MISSING);
+ 			VM_BUG_ON(ret & VM_FAULT_FALLBACK);
+ 			return ret;
+ 		}
+ 
+ 		entry = mk_huge_pmd(page, vma->vm_page_prot);
+ 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
++>>>>>>> 230c92a8797e (userfaultfd: propagate the full address in THP faults)
  		page_add_new_anon_rmap(page, vma, haddr);
 -		mem_cgroup_commit_charge(page, memcg, false);
 -		lru_cache_add_active_or_unevictable(page, vma);
  		pgtable_trans_huge_deposit(mm, pmd, pgtable);
  		set_pmd_at(mm, haddr, pmd, entry);
  		add_mm_counter(mm, MM_ANONPAGES, HPAGE_PMD_NR);
@@@ -808,9 -837,23 +835,29 @@@ int do_huge_pmd_anonymous_page(struct m
  			return VM_FAULT_FALLBACK;
  		}
  		ptl = pmd_lock(mm, pmd);
++<<<<<<< HEAD
 +		set = set_huge_zero_page(pgtable, mm, vma, haddr, pmd,
 +				zero_page);
 +		spin_unlock(ptl);
++=======
+ 		ret = 0;
+ 		set = false;
+ 		if (pmd_none(*pmd)) {
+ 			if (userfaultfd_missing(vma)) {
+ 				spin_unlock(ptl);
+ 				ret = handle_userfault(vma, address, flags,
+ 						       VM_UFFD_MISSING);
+ 				VM_BUG_ON(ret & VM_FAULT_FALLBACK);
+ 			} else {
+ 				set_huge_zero_page(pgtable, mm, vma,
+ 						   haddr, pmd,
+ 						   zero_page);
+ 				spin_unlock(ptl);
+ 				set = true;
+ 			}
+ 		} else
+ 			spin_unlock(ptl);
++>>>>>>> 230c92a8797e (userfaultfd: propagate the full address in THP faults)
  		if (!set) {
  			pte_free(mm, pgtable);
  			put_huge_zero_page();
@@@ -823,20 -866,8 +870,25 @@@
  		count_vm_event(THP_FAULT_FALLBACK);
  		return VM_FAULT_FALLBACK;
  	}
++<<<<<<< HEAD
 +	if (unlikely(mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))) {
 +		put_page(page);
 +		count_vm_event(THP_FAULT_FALLBACK);
 +		return VM_FAULT_FALLBACK;
 +	}
 +	if (unlikely(__do_huge_pmd_anonymous_page(mm, vma, haddr, pmd, page))) {
 +		mem_cgroup_uncharge_page(page);
 +		put_page(page);
 +		count_vm_event(THP_FAULT_FALLBACK);
 +		return VM_FAULT_FALLBACK;
 +	}
 +
 +	count_vm_event(THP_FAULT_ALLOC);
 +	return 0;
++=======
+ 	return __do_huge_pmd_anonymous_page(mm, vma, address, pmd, page, gfp,
+ 					    flags);
++>>>>>>> 230c92a8797e (userfaultfd: propagate the full address in THP faults)
  }
  
  int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
* Unmerged path mm/huge_memory.c
