ipv4: minor spelling fixes

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
commit-author stephen hemminger <stephen@networkplumber.org>
commit f4e715c3254e3c0167b5d4272901a2b248b65ad2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/f4e715c3.failed

	Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit f4e715c3254e3c0167b5d4272901a2b248b65ad2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/ipv4/tcp_input.c
diff --cc net/ipv4/tcp_input.c
index ef845c96b4d4,df8b3c12a173..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -5833,3 -5846,190 +5833,193 @@@ discard
  	return 0;
  }
  EXPORT_SYMBOL(tcp_rcv_state_process);
++<<<<<<< HEAD
++=======
+ 
+ static inline void pr_drop_req(struct request_sock *req, __u16 port, int family)
+ {
+ 	struct inet_request_sock *ireq = inet_rsk(req);
+ 
+ 	if (family == AF_INET)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI4/%u\n"),
+ 			       &ireq->ir_rmt_addr, port);
+ #if IS_ENABLED(CONFIG_IPV6)
+ 	else if (family == AF_INET6)
+ 		LIMIT_NETDEBUG(KERN_DEBUG pr_fmt("drop open request from %pI6/%u\n"),
+ 			       &ireq->ir_v6_rmt_addr, port);
+ #endif
+ }
+ 
+ /* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set
+  *
+  * If we receive a SYN packet with these bits set, it means a
+  * network is playing bad games with TOS bits. In order to
+  * avoid possible false congestion notifications, we disable
+  * TCP ECN negotiation.
+  *
+  * Exception: tcp_ca wants ECN. This is required for DCTCP
+  * congestion control; it requires setting ECT on all packets,
+  * including SYN. We inverse the test in this case: If our
+  * local socket wants ECN, but peer only set ece/cwr (but not
+  * ECT in IP header) its probably a non-DCTCP aware sender.
+  */
+ static void tcp_ecn_create_request(struct request_sock *req,
+ 				   const struct sk_buff *skb,
+ 				   const struct sock *listen_sk)
+ {
+ 	const struct tcphdr *th = tcp_hdr(skb);
+ 	const struct net *net = sock_net(listen_sk);
+ 	bool th_ecn = th->ece && th->cwr;
+ 	bool ect, need_ecn;
+ 
+ 	if (!th_ecn)
+ 		return;
+ 
+ 	ect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);
+ 	need_ecn = tcp_ca_needs_ecn(listen_sk);
+ 
+ 	if (!ect && !need_ecn && net->ipv4.sysctl_tcp_ecn)
+ 		inet_rsk(req)->ecn_ok = 1;
+ 	else if (ect && need_ecn)
+ 		inet_rsk(req)->ecn_ok = 1;
+ }
+ 
+ int tcp_conn_request(struct request_sock_ops *rsk_ops,
+ 		     const struct tcp_request_sock_ops *af_ops,
+ 		     struct sock *sk, struct sk_buff *skb)
+ {
+ 	struct tcp_options_received tmp_opt;
+ 	struct request_sock *req;
+ 	struct tcp_sock *tp = tcp_sk(sk);
+ 	struct dst_entry *dst = NULL;
+ 	__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;
+ 	bool want_cookie = false, fastopen;
+ 	struct flowi fl;
+ 	struct tcp_fastopen_cookie foc = { .len = -1 };
+ 	int err;
+ 
+ 
+ 	/* TW buckets are converted to open requests without
+ 	 * limitations, they conserve resources and peer is
+ 	 * evidently real one.
+ 	 */
+ 	if ((sysctl_tcp_syncookies == 2 ||
+ 	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
+ 		want_cookie = tcp_syn_flood_action(sk, skb, rsk_ops->slab_name);
+ 		if (!want_cookie)
+ 			goto drop;
+ 	}
+ 
+ 
+ 	/* Accept backlog is full. If we have already queued enough
+ 	 * of warm entries in syn queue, drop request. It is better than
+ 	 * clogging syn queue with openreqs with exponentially increasing
+ 	 * timeout.
+ 	 */
+ 	if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {
+ 		NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+ 		goto drop;
+ 	}
+ 
+ 	req = inet_reqsk_alloc(rsk_ops);
+ 	if (!req)
+ 		goto drop;
+ 
+ 	tcp_rsk(req)->af_specific = af_ops;
+ 
+ 	tcp_clear_options(&tmp_opt);
+ 	tmp_opt.mss_clamp = af_ops->mss_clamp;
+ 	tmp_opt.user_mss  = tp->rx_opt.user_mss;
+ 	tcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);
+ 
+ 	if (want_cookie && !tmp_opt.saw_tstamp)
+ 		tcp_clear_options(&tmp_opt);
+ 
+ 	tmp_opt.tstamp_ok = tmp_opt.saw_tstamp;
+ 	tcp_openreq_init(req, &tmp_opt, skb, sk);
+ 
+ 	af_ops->init_req(req, sk, skb);
+ 
+ 	if (security_inet_conn_request(sk, skb, req))
+ 		goto drop_and_free;
+ 
+ 	if (!want_cookie || tmp_opt.tstamp_ok)
+ 		tcp_ecn_create_request(req, skb, sk);
+ 
+ 	if (want_cookie) {
+ 		isn = cookie_init_sequence(af_ops, sk, skb, &req->mss);
+ 		req->cookie_ts = tmp_opt.tstamp_ok;
+ 	} else if (!isn) {
+ 		/* VJ's idea. We save last timestamp seen
+ 		 * from the destination in peer table, when entering
+ 		 * state TIME-WAIT, and check against it before
+ 		 * accepting new connection request.
+ 		 *
+ 		 * If "isn" is not zero, this request hit alive
+ 		 * timewait bucket, so that all the necessary checks
+ 		 * are made in the function processing timewait state.
+ 		 */
+ 		if (tcp_death_row.sysctl_tw_recycle) {
+ 			bool strict;
+ 
+ 			dst = af_ops->route_req(sk, &fl, req, &strict);
+ 
+ 			if (dst && strict &&
+ 			    !tcp_peer_is_proven(req, dst, true,
+ 						tmp_opt.saw_tstamp)) {
+ 				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);
+ 				goto drop_and_release;
+ 			}
+ 		}
+ 		/* Kill the following clause, if you dislike this way. */
+ 		else if (!sysctl_tcp_syncookies &&
+ 			 (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <
+ 			  (sysctl_max_syn_backlog >> 2)) &&
+ 			 !tcp_peer_is_proven(req, dst, false,
+ 					     tmp_opt.saw_tstamp)) {
+ 			/* Without syncookies last quarter of
+ 			 * backlog is filled with destinations,
+ 			 * proven to be alive.
+ 			 * It means that we continue to communicate
+ 			 * to destinations, already remembered
+ 			 * to the moment of synflood.
+ 			 */
+ 			pr_drop_req(req, ntohs(tcp_hdr(skb)->source),
+ 				    rsk_ops->family);
+ 			goto drop_and_release;
+ 		}
+ 
+ 		isn = af_ops->init_seq(skb);
+ 	}
+ 	if (!dst) {
+ 		dst = af_ops->route_req(sk, &fl, req, NULL);
+ 		if (!dst)
+ 			goto drop_and_free;
+ 	}
+ 
+ 	tcp_rsk(req)->snt_isn = isn;
+ 	tcp_openreq_init_rwin(req, sk, dst);
+ 	fastopen = !want_cookie &&
+ 		   tcp_try_fastopen(sk, skb, req, &foc, dst);
+ 	err = af_ops->send_synack(sk, dst, &fl, req,
+ 				  skb_get_queue_mapping(skb), &foc);
+ 	if (!fastopen) {
+ 		if (err || want_cookie)
+ 			goto drop_and_free;
+ 
+ 		tcp_rsk(req)->listener = NULL;
+ 		af_ops->queue_hash_add(sk, req, TCP_TIMEOUT_INIT);
+ 	}
+ 
+ 	return 0;
+ 
+ drop_and_release:
+ 	dst_release(dst);
+ drop_and_free:
+ 	reqsk_free(req);
+ drop:
+ 	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(tcp_conn_request);
++>>>>>>> f4e715c3254e (ipv4: minor spelling fixes)
diff --git a/net/ipv4/geneve.c b/net/ipv4/geneve.c
index 065cd94c640c..91861fe77ed1 100644
--- a/net/ipv4/geneve.c
+++ b/net/ipv4/geneve.c
@@ -104,7 +104,7 @@ static void geneve_build_header(struct genevehdr *geneveh,
 	memcpy(geneveh->options, options, options_len);
 }
 
-/* Transmit a fully formated Geneve frame.
+/* Transmit a fully formatted Geneve frame.
  *
  * When calling this function. The skb->data should point
  * to the geneve header which is fully formed.
* Unmerged path net/ipv4/tcp_input.c
