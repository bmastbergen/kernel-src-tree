perf/x86/intel: Limit to half counters when the HT workaround is enabled, to avoid exclusive mode starvation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [perf] x86/intel: Limit to half counters when the HT workaround is enabled, to avoid exclusive mode starvation (Jiri Olsa) [1210494]
Rebuild_FUZZ: 97.63%
commit-author Stephane Eranian <eranian@google.com>
commit c02cdbf60b51b8d98a49185535f5d527a2965142
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/c02cdbf6.failed

This patch limits the number of counters available to each CPU when
the HT bug workaround is enabled.

This is necessary to avoid situation of counter starvation. Such can
arise from configuration where one HT thread, HT0, is using all 4 counters
with corrupting events which require exclusion the the sibling HT, HT1.

In such case, HT1 would not be able to schedule any event until HT0
is done. To mitigate this problem, this patch artificially limits
the number of counters to 2.

That way, we can gurantee that at least 2 counters are not in exclusive
mode and therefore allow the sibling thread to schedule events of the
same type (system vs. per-thread). The 2 counters are not determined
in advance. We simply set the limit to two events per HT.

This helps mitigate starvation in case of events with specific counter
constraints such a PREC_DIST.

Note that this does not elimintate the starvation is all cases. But
it is better than not having it.

(Solution suggested by Peter Zjilstra.)

	Signed-off-by: Stephane Eranian <eranian@google.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: bp@alien8.de
	Cc: jolsa@redhat.com
	Cc: kan.liang@intel.com
	Cc: maria.n.dimakopoulou@gmail.com
Link: http://lkml.kernel.org/r/1416251225-17721-11-git-send-email-eranian@google.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit c02cdbf60b51b8d98a49185535f5d527a2965142)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/perf_event.h
#	arch/x86/kernel/cpu/perf_event_intel.c
diff --cc arch/x86/kernel/cpu/perf_event.h
index aaa12c2a8b1e,176749cca98f..000000000000
--- a/arch/x86/kernel/cpu/perf_event.h
+++ b/arch/x86/kernel/cpu/perf_event.h
@@@ -121,6 -125,29 +121,32 @@@ struct intel_shared_regs 
  	unsigned                core_id;	/* per-core: core id */
  };
  
++<<<<<<< HEAD
++=======
+ enum intel_excl_state_type {
+ 	INTEL_EXCL_UNUSED    = 0, /* counter is unused */
+ 	INTEL_EXCL_SHARED    = 1, /* counter can be used by both threads */
+ 	INTEL_EXCL_EXCLUSIVE = 2, /* counter can be used by one thread only */
+ };
+ 
+ struct intel_excl_states {
+ 	enum intel_excl_state_type init_state[X86_PMC_IDX_MAX];
+ 	enum intel_excl_state_type state[X86_PMC_IDX_MAX];
+ 	int  num_alloc_cntrs;/* #counters allocated */
+ 	int  max_alloc_cntrs;/* max #counters allowed */
+ 	bool sched_started; /* true if scheduling has started */
+ };
+ 
+ struct intel_excl_cntrs {
+ 	raw_spinlock_t	lock;
+ 
+ 	struct intel_excl_states states[2];
+ 
+ 	int		refcnt;		/* per-core: #HT threads */
+ 	unsigned	core_id;	/* per-core: core id */
+ };
+ 
++>>>>>>> c02cdbf60b51 (perf/x86/intel: Limit to half counters when the HT workaround is enabled, to avoid exclusive mode starvation)
  #define MAX_LBR_ENTRIES		16
  
  enum {
diff --cc arch/x86/kernel/cpu/perf_event_intel.c
index 8bd638d75162,4187d3f4ed12..000000000000
--- a/arch/x86/kernel/cpu/perf_event_intel.c
+++ b/arch/x86/kernel/cpu/perf_event_intel.c
@@@ -1775,11 -1867,275 +1775,279 @@@ intel_get_event_constraints(struct cpu_
  	if (c)
  		return c;
  
 -	c = intel_pebs_constraints(event);
++<<<<<<< HEAD
 +	c = intel_shared_regs_constraints(cpuc, event);
  	if (c)
  		return c;
  
 +	return x86_get_event_constraints(cpuc, idx, event);
++=======
+ 	return x86_get_event_constraints(cpuc, idx, event);
+ }
+ 
+ static void
+ intel_start_scheduling(struct cpu_hw_events *cpuc)
+ {
+ 	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
+ 	struct intel_excl_states *xl, *xlo;
+ 	int tid = cpuc->excl_thread_id;
+ 	int o_tid = 1 - tid; /* sibling thread */
+ 
+ 	/*
+ 	 * nothing needed if in group validation mode
+ 	 */
+ 	if (cpuc->is_fake)
+ 		return;
+ 	/*
+ 	 * no exclusion needed
+ 	 */
+ 	if (!excl_cntrs)
+ 		return;
+ 
+ 	xlo = &excl_cntrs->states[o_tid];
+ 	xl = &excl_cntrs->states[tid];
+ 
+ 	xl->sched_started = true;
+ 	xl->num_alloc_cntrs = 0;
+ 	/*
+ 	 * lock shared state until we are done scheduling
+ 	 * in stop_event_scheduling()
+ 	 * makes scheduling appear as a transaction
+ 	 */
+ 	WARN_ON_ONCE(!irqs_disabled());
+ 	raw_spin_lock(&excl_cntrs->lock);
+ 
+ 	/*
+ 	 * save initial state of sibling thread
+ 	 */
+ 	memcpy(xlo->init_state, xlo->state, sizeof(xlo->init_state));
+ }
+ 
+ static void
+ intel_stop_scheduling(struct cpu_hw_events *cpuc)
+ {
+ 	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
+ 	struct intel_excl_states *xl, *xlo;
+ 	int tid = cpuc->excl_thread_id;
+ 	int o_tid = 1 - tid; /* sibling thread */
+ 
+ 	/*
+ 	 * nothing needed if in group validation mode
+ 	 */
+ 	if (cpuc->is_fake)
+ 		return;
+ 	/*
+ 	 * no exclusion needed
+ 	 */
+ 	if (!excl_cntrs)
+ 		return;
+ 
+ 	xlo = &excl_cntrs->states[o_tid];
+ 	xl = &excl_cntrs->states[tid];
+ 
+ 	/*
+ 	 * make new sibling thread state visible
+ 	 */
+ 	memcpy(xlo->state, xlo->init_state, sizeof(xlo->state));
+ 
+ 	xl->sched_started = false;
+ 	/*
+ 	 * release shared state lock (acquired in intel_start_scheduling())
+ 	 */
+ 	raw_spin_unlock(&excl_cntrs->lock);
+ }
+ 
+ static struct event_constraint *
+ intel_get_excl_constraints(struct cpu_hw_events *cpuc, struct perf_event *event,
+ 			   int idx, struct event_constraint *c)
+ {
+ 	struct event_constraint *cx;
+ 	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
+ 	struct intel_excl_states *xl, *xlo;
+ 	int is_excl, i;
+ 	int tid = cpuc->excl_thread_id;
+ 	int o_tid = 1 - tid; /* alternate */
+ 
+ 	/*
+ 	 * validating a group does not require
+ 	 * enforcing cross-thread  exclusion
+ 	 */
+ 	if (cpuc->is_fake)
+ 		return c;
+ 	/*
+ 	 * event requires exclusive counter access
+ 	 * across HT threads
+ 	 */
+ 	is_excl = c->flags & PERF_X86_EVENT_EXCL;
+ 
+ 	/*
+ 	 * xl = state of current HT
+ 	 * xlo = state of sibling HT
+ 	 */
+ 	xl = &excl_cntrs->states[tid];
+ 	xlo = &excl_cntrs->states[o_tid];
+ 
+ 	/*
+ 	 * do not allow scheduling of more than max_alloc_cntrs
+ 	 * which is set to half the available generic counters.
+ 	 * this helps avoid counter starvation of sibling thread
+ 	 * by ensuring at most half the counters cannot be in
+ 	 * exclusive mode. There is not designated counters for the
+ 	 * limits. Any N/2 counters can be used. This helps with
+ 	 * events with specifix counter constraints
+ 	 */
+ 	if (xl->num_alloc_cntrs++ == xl->max_alloc_cntrs)
+ 		return &emptyconstraint;
+ 
+ 	cx = c;
+ 
+ 	/*
+ 	 * because we modify the constraint, we need
+ 	 * to make a copy. Static constraints come
+ 	 * from static const tables.
+ 	 *
+ 	 * only needed when constraint has not yet
+ 	 * been cloned (marked dynamic)
+ 	 */
+ 	if (!(c->flags & PERF_X86_EVENT_DYNAMIC)) {
+ 
+ 		/* sanity check */
+ 		if (idx < 0)
+ 			return &emptyconstraint;
+ 
+ 		/*
+ 		 * grab pre-allocated constraint entry
+ 		 */
+ 		cx = &cpuc->constraint_list[idx];
+ 
+ 		/*
+ 		 * initialize dynamic constraint
+ 		 * with static constraint
+ 		 */
+ 		memcpy(cx, c, sizeof(*cx));
+ 
+ 		/*
+ 		 * mark constraint as dynamic, so we
+ 		 * can free it later on
+ 		 */
+ 		cx->flags |= PERF_X86_EVENT_DYNAMIC;
+ 	}
+ 
+ 	/*
+ 	 * From here on, the constraint is dynamic.
+ 	 * Either it was just allocated above, or it
+ 	 * was allocated during a earlier invocation
+ 	 * of this function
+ 	 */
+ 
+ 	/*
+ 	 * Modify static constraint with current dynamic
+ 	 * state of thread
+ 	 *
+ 	 * EXCLUSIVE: sibling counter measuring exclusive event
+ 	 * SHARED   : sibling counter measuring non-exclusive event
+ 	 * UNUSED   : sibling counter unused
+ 	 */
+ 	for_each_set_bit(i, cx->idxmsk, X86_PMC_IDX_MAX) {
+ 		/*
+ 		 * exclusive event in sibling counter
+ 		 * our corresponding counter cannot be used
+ 		 * regardless of our event
+ 		 */
+ 		if (xl->state[i] == INTEL_EXCL_EXCLUSIVE)
+ 			__clear_bit(i, cx->idxmsk);
+ 		/*
+ 		 * if measuring an exclusive event, sibling
+ 		 * measuring non-exclusive, then counter cannot
+ 		 * be used
+ 		 */
+ 		if (is_excl && xl->state[i] == INTEL_EXCL_SHARED)
+ 			__clear_bit(i, cx->idxmsk);
+ 	}
+ 
+ 	/*
+ 	 * recompute actual bit weight for scheduling algorithm
+ 	 */
+ 	cx->weight = hweight64(cx->idxmsk64);
+ 
+ 	/*
+ 	 * if we return an empty mask, then switch
+ 	 * back to static empty constraint to avoid
+ 	 * the cost of freeing later on
+ 	 */
+ 	if (cx->weight == 0)
+ 		cx = &emptyconstraint;
+ 
+ 	return cx;
+ }
+ 
+ static struct event_constraint *
+ intel_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
+ 			    struct perf_event *event)
+ {
+ 	struct event_constraint *c1 = event->hw.constraint;
+ 	struct event_constraint *c2;
+ 
+ 	/*
+ 	 * first time only
+ 	 * - static constraint: no change across incremental scheduling calls
+ 	 * - dynamic constraint: handled by intel_get_excl_constraints()
+ 	 */
+ 	c2 = __intel_get_event_constraints(cpuc, idx, event);
+ 	if (c1 && (c1->flags & PERF_X86_EVENT_DYNAMIC)) {
+ 		bitmap_copy(c1->idxmsk, c2->idxmsk, X86_PMC_IDX_MAX);
+ 		c1->weight = c2->weight;
+ 		c2 = c1;
+ 	}
+ 
+ 	if (cpuc->excl_cntrs)
+ 		return intel_get_excl_constraints(cpuc, event, idx, c2);
+ 
+ 	return c2;
+ }
+ 
+ static void intel_put_excl_constraints(struct cpu_hw_events *cpuc,
+ 		struct perf_event *event)
+ {
+ 	struct hw_perf_event *hwc = &event->hw;
+ 	struct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;
+ 	struct intel_excl_states *xlo, *xl;
+ 	unsigned long flags = 0; /* keep compiler happy */
+ 	int tid = cpuc->excl_thread_id;
+ 	int o_tid = 1 - tid;
+ 
+ 	/*
+ 	 * nothing needed if in group validation mode
+ 	 */
+ 	if (cpuc->is_fake)
+ 		return;
+ 
+ 	WARN_ON_ONCE(!excl_cntrs);
+ 
+ 	if (!excl_cntrs)
+ 		return;
+ 
+ 	xl = &excl_cntrs->states[tid];
+ 	xlo = &excl_cntrs->states[o_tid];
+ 
+ 	/*
+ 	 * put_constraint may be called from x86_schedule_events()
+ 	 * which already has the lock held so here make locking
+ 	 * conditional
+ 	 */
+ 	if (!xl->sched_started)
+ 		raw_spin_lock_irqsave(&excl_cntrs->lock, flags);
+ 
+ 	/*
+ 	 * if event was actually assigned, then mark the
+ 	 * counter state as unused now
+ 	 */
+ 	if (hwc->idx >= 0)
+ 		xlo->state[hwc->idx] = INTEL_EXCL_UNUSED;
+ 
+ 	if (!xl->sched_started)
+ 		raw_spin_unlock_irqrestore(&excl_cntrs->lock, flags);
++>>>>>>> c02cdbf60b51 (perf/x86/intel: Limit to half counters when the HT workaround is enabled, to avoid exclusive mode starvation)
  }
  
  static void
@@@ -2144,6 -2633,29 +2412,32 @@@ static void intel_pmu_cpu_starting(int 
  
  	if (x86_pmu.lbr_sel_map)
  		cpuc->lbr_sel = &cpuc->shared_regs->regs[EXTRA_REG_LBR];
++<<<<<<< HEAD
++=======
+ 
+ 	if (x86_pmu.flags & PMU_FL_EXCL_CNTRS) {
+ 		int h = x86_pmu.num_counters >> 1;
+ 
+ 		for_each_cpu(i, topology_thread_cpumask(cpu)) {
+ 			struct intel_excl_cntrs *c;
+ 
+ 			c = per_cpu(cpu_hw_events, i).excl_cntrs;
+ 			if (c && c->core_id == core_id) {
+ 				cpuc->kfree_on_online[1] = cpuc->excl_cntrs;
+ 				cpuc->excl_cntrs = c;
+ 				cpuc->excl_thread_id = 1;
+ 				break;
+ 			}
+ 		}
+ 		cpuc->excl_cntrs->core_id = core_id;
+ 		cpuc->excl_cntrs->refcnt++;
+ 		/*
+ 		 * set hard limit to half the number of generic counters
+ 		 */
+ 		cpuc->excl_cntrs->states[0].max_alloc_cntrs = h;
+ 		cpuc->excl_cntrs->states[1].max_alloc_cntrs = h;
+ 	}
++>>>>>>> c02cdbf60b51 (perf/x86/intel: Limit to half counters when the HT workaround is enabled, to avoid exclusive mode starvation)
  }
  
  static void intel_pmu_cpu_dying(int cpu)
* Unmerged path arch/x86/kernel/cpu/perf_event.h
* Unmerged path arch/x86/kernel/cpu/perf_event_intel.c
