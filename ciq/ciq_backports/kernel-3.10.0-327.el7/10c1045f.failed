mm: numa: avoid unnecessary TLB flushes when setting NUMA hinting entries

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [mm] numa: avoid unnecessary TLB flushes when setting NUMA hinting entries (Gustavo Duarte) [1217743]
Rebuild_FUZZ: 97.18%
commit-author Mel Gorman <mgorman@suse.de>
commit 10c1045f28e86ac90589a188f0be2d7a4347efdf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/10c1045f.failed

If a PTE or PMD is already marked NUMA when scanning to mark entries for
NUMA hinting then it is not necessary to update the entry and incur a TLB
flush penalty.  Avoid the avoidhead where possible.

	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Dave Jones <davej@redhat.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Kirill Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 10c1045f28e86ac90589a188f0be2d7a4347efdf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
#	mm/mprotect.c
diff --cc mm/huge_memory.c
index 12565764061a,fc00c8cb5a82..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1535,29 -1482,24 +1535,49 @@@ int change_huge_pmd(struct vm_area_stru
  
  	if (__pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {
  		pmd_t entry;
++<<<<<<< HEAD
 +		ret = 1;
 +		if (!prot_numa) {
 +			entry = pmdp_get_and_clear(mm, addr, pmd);
 +			if (pmd_numa(entry))
 +				entry = pmd_mknonnuma(entry);
++=======
+ 
+ 		/*
+ 		 * Avoid trapping faults against the zero page. The read-only
+ 		 * data is likely to be read-cached on the local CPU and
+ 		 * local/remote hits to the zero page are not interesting.
+ 		 */
+ 		if (prot_numa && is_huge_zero_pmd(*pmd)) {
+ 			spin_unlock(ptl);
+ 			return 0;
+ 		}
+ 
+ 		if (!prot_numa || !pmd_protnone(*pmd)) {
+ 			ret = 1;
+ 			entry = pmdp_get_and_clear_notify(mm, addr, pmd);
++>>>>>>> 10c1045f28e8 (mm: numa: avoid unnecessary TLB flushes when setting NUMA hinting entries)
  			entry = pmd_modify(entry, newprot);
  			ret = HPAGE_PMD_NR;
  			set_pmd_at(mm, addr, pmd, entry);
  			BUG_ON(pmd_write(entry));
++<<<<<<< HEAD
 +		} else {
 +			struct page *page = pmd_page(*pmd);
 +
 +			/*
 +			 * Do not trap faults against the zero page. The
 +			 * read-only data is likely to be read-cached on the
 +			 * local CPU cache and it is less useful to know about
 +			 * local vs remote hits on the zero page.
 +			 */
 +			if (!is_huge_zero_page(page) &&
 +			    !pmd_numa(*pmd)) {
 +				pmdp_set_numa(mm, addr, pmd);
 +				ret = HPAGE_PMD_NR;
 +			}
++=======
++>>>>>>> 10c1045f28e8 (mm: numa: avoid unnecessary TLB flushes when setting NUMA hinting entries)
  		}
  		spin_unlock(ptl);
  	}
diff --cc mm/mprotect.c
index a182e560297e,44727811bf4c..000000000000
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@@ -101,16 -84,26 +101,25 @@@ static unsigned long change_pte_range(s
  				struct page *page;
  
  				page = vm_normal_page(vma, addr, oldpte);
++<<<<<<< HEAD
 +				if (page && !PageKsm(page)) {
 +					if (!pte_numa(oldpte)) {
 +						ptep_set_numa(mm, addr, pte);
 +						updated = true;
 +					}
 +				}
++=======
+ 				if (!page || PageKsm(page))
+ 					continue;
+ 
+ 				/* Avoid TLB flush if possible */
+ 				if (pte_protnone(oldpte))
+ 					continue;
++>>>>>>> 10c1045f28e8 (mm: numa: avoid unnecessary TLB flushes when setting NUMA hinting entries)
  			}
 -
 -			ptent = ptep_modify_prot_start(mm, addr, pte);
 -			ptent = pte_modify(ptent, newprot);
 -
 -			/* Avoid taking write faults for known dirty pages */
 -			if (dirty_accountable && pte_dirty(ptent) &&
 -					(pte_soft_dirty(ptent) ||
 -					 !(vma->vm_flags & VM_SOFTDIRTY))) {
 -				ptent = pte_mkwrite(ptent);
 -			}
 -			ptep_modify_prot_commit(mm, addr, pte, ptent);
 -			pages++;
 -		} else if (IS_ENABLED(CONFIG_MIGRATION)) {
 +			if (updated)
 +				pages++;
 +		} else if (IS_ENABLED(CONFIG_MIGRATION) && !pte_file(oldpte)) {
  			swp_entry_t entry = pte_to_swp_entry(oldpte);
  
  			if (is_write_migration_entry(entry)) {
* Unmerged path mm/huge_memory.c
* Unmerged path mm/mprotect.c
