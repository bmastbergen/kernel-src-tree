fs/superblock: avoid locking counting inodes and dentries before reclaiming them

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-327.el7
Rebuild_CHGLOG: - [fs] superblock: avoid locking counting inodes and dentries before reclaiming them (Larry Woodman) [1178988]
Rebuild_FUZZ: 98.09%
commit-author Tim Chen <tim.c.chen@linux.intel.com>
commit d23da150a37c9fe3cc83dbaf71b3e37fd434ed52
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-327.el7/d23da150.failed

We remove the call to grab_super_passive in call to super_cache_count.
This becomes a scalability bottleneck as multiple threads are trying to do
memory reclamation, e.g.  when we are doing large amount of file read and
page cache is under pressure.  The cached objects quickly got reclaimed
down to 0 and we are aborting the cache_scan() reclaim.  But counting
creates a log jam acquiring the sb_lock.

We are holding the shrinker_rwsem which ensures the safety of call to
list_lru_count_node() and s_op->nr_cached_objects.  The shrinker is
unregistered now before ->kill_sb() so the operation is safe when we are
doing unmount.

The impact will depend heavily on the machine and the workload but for a
small machine using postmark tuned to use 4xRAM size the results were

                                  3.15.0-rc5            3.15.0-rc5
                                     vanilla         shrinker-v1r1
Ops/sec Transactions         21.00 (  0.00%)       24.00 ( 14.29%)
Ops/sec FilesCreate          39.00 (  0.00%)       44.00 ( 12.82%)
Ops/sec CreateTransact       10.00 (  0.00%)       12.00 ( 20.00%)
Ops/sec FilesDeleted       6202.00 (  0.00%)     6202.00 (  0.00%)
Ops/sec DeleteTransact       11.00 (  0.00%)       12.00 (  9.09%)
Ops/sec DataRead/MB          25.97 (  0.00%)       29.10 ( 12.05%)
Ops/sec DataWrite/MB         49.99 (  0.00%)       56.02 ( 12.06%)

ffsb running in a configuration that is meant to simulate a mail server showed

                                 3.15.0-rc5             3.15.0-rc5
                                    vanilla          shrinker-v1r1
Ops/sec readall           9402.63 (  0.00%)      9567.97 (  1.76%)
Ops/sec create            4695.45 (  0.00%)      4735.00 (  0.84%)
Ops/sec delete             173.72 (  0.00%)       179.83 (  3.52%)
Ops/sec Transactions     14271.80 (  0.00%)     14482.81 (  1.48%)
Ops/sec Read                37.00 (  0.00%)        37.60 (  1.62%)
Ops/sec Write               18.20 (  0.00%)        18.30 (  0.55%)

	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Signed-off-by: Mel Gorman <mgorman@suse.de>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Dave Chinner <david@fromorbit.com>
	Tested-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
	Cc: Bob Liu <bob.liu@oracle.com>
	Cc: Jan Kara <jack@suse.cz>
	Acked-by: Rik van Riel <riel@redhat.com>
	Cc: Al Viro <viro@zeniv.linux.org.uk>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d23da150a37c9fe3cc83dbaf71b3e37fd434ed52)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/super.c
diff --cc fs/super.c
index eedd3bd86aeb,d20d5b11dedf..000000000000
--- a/fs/super.c
+++ b/fs/super.c
@@@ -66,47 -69,67 +66,101 @@@ static int prune_super(struct shrinker 
  	 * Deadlock avoidance.  We may hold various FS locks, and we don't want
  	 * to recurse into the FS that called us in clear_inode() and friends..
  	 */
 -	if (!(sc->gfp_mask & __GFP_FS))
 -		return SHRINK_STOP;
 +	if (sc->nr_to_scan && !(sc->gfp_mask & __GFP_FS))
 +		return -1;
  
  	if (!grab_super_passive(sb))
++<<<<<<< HEAD
 +		return -1;
 +
++=======
+ 		return SHRINK_STOP;
+ 
+ 	if (sb->s_op->nr_cached_objects)
+ 		fs_objects = sb->s_op->nr_cached_objects(sb, sc->nid);
+ 
+ 	inodes = list_lru_count_node(&sb->s_inode_lru, sc->nid);
+ 	dentries = list_lru_count_node(&sb->s_dentry_lru, sc->nid);
+ 	total_objects = dentries + inodes + fs_objects + 1;
+ 
+ 	/* proportion the scan between the caches */
+ 	dentries = mult_frac(sc->nr_to_scan, dentries, total_objects);
+ 	inodes = mult_frac(sc->nr_to_scan, inodes, total_objects);
+ 
+ 	/*
+ 	 * prune the dcache first as the icache is pinned by it, then
+ 	 * prune the icache, followed by the filesystem specific caches
+ 	 */
+ 	freed = prune_dcache_sb(sb, dentries, sc->nid);
+ 	freed += prune_icache_sb(sb, inodes, sc->nid);
+ 
+ 	if (fs_objects) {
+ 		fs_objects = mult_frac(sc->nr_to_scan, fs_objects,
+ 								total_objects);
+ 		freed += sb->s_op->free_cached_objects(sb, fs_objects,
+ 						       sc->nid);
+ 	}
+ 
+ 	drop_super(sb);
+ 	return freed;
+ }
+ 
+ static unsigned long super_cache_count(struct shrinker *shrink,
+ 				       struct shrink_control *sc)
+ {
+ 	struct super_block *sb;
+ 	long	total_objects = 0;
+ 
+ 	sb = container_of(shrink, struct super_block, s_shrink);
+ 
+ 	/*
+ 	 * Don't call grab_super_passive as it is a potential
+ 	 * scalability bottleneck. The counts could get updated
+ 	 * between super_cache_count and super_cache_scan anyway.
+ 	 * Call to super_cache_count with shrinker_rwsem held
+ 	 * ensures the safety of call to list_lru_count_node() and
+ 	 * s_op->nr_cached_objects().
+ 	 */
++>>>>>>> d23da150a37c (fs/superblock: avoid locking counting inodes and dentries before reclaiming them)
  	if (sb->s_op && sb->s_op->nr_cached_objects)
 -		total_objects = sb->s_op->nr_cached_objects(sb,
 -						 sc->nid);
 -
 -	total_objects += list_lru_count_node(&sb->s_dentry_lru,
 -						 sc->nid);
 -	total_objects += list_lru_count_node(&sb->s_inode_lru,
 -						 sc->nid);
 +		fs_objects = sb->s_op->nr_cached_objects(sb);
 +
 +	total_objects = sb->s_nr_dentry_unused +
 +			sb->s_nr_inodes_unused + fs_objects + 1;
 +
++<<<<<<< HEAD
 +	if (sc->nr_to_scan) {
 +		int	dentries;
 +		int	inodes;
 +
 +		/* proportion the scan between the caches */
 +		dentries = (sc->nr_to_scan * sb->s_nr_dentry_unused) /
 +							total_objects;
 +		inodes = (sc->nr_to_scan * sb->s_nr_inodes_unused) /
 +							total_objects;
 +		if (fs_objects)
 +			fs_objects = (sc->nr_to_scan * fs_objects) /
 +							total_objects;
 +		/*
 +		 * prune the dcache first as the icache is pinned by it, then
 +		 * prune the icache, followed by the filesystem specific caches
 +		 */
 +		prune_dcache_sb(sb, dentries);
 +		prune_icache_sb(sb, inodes);
 +
 +		if (fs_objects && sb->s_op->free_cached_objects) {
 +			sb->s_op->free_cached_objects(sb, fs_objects);
 +			fs_objects = sb->s_op->nr_cached_objects(sb);
 +		}
 +		total_objects = sb->s_nr_dentry_unused +
 +				sb->s_nr_inodes_unused + fs_objects;
 +	}
  
 +	total_objects = (total_objects / 100) * sysctl_vfs_cache_pressure;
 +	drop_super(sb);
++=======
+ 	total_objects = vfs_pressure_ratio(total_objects);
++>>>>>>> d23da150a37c (fs/superblock: avoid locking counting inodes and dentries before reclaiming them)
  	return total_objects;
  }
  
* Unmerged path fs/super.c
