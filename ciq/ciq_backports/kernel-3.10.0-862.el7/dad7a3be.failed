blk-mq: pass correct hctx to blk_mq_try_issue_directly

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ming Lei <ming.lei@redhat.com>
commit dad7a3be4960e5545882a0cd8d7613af22874314
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/dad7a3be.failed

When direct issue is done on request picked up from plug list,
the hctx need to be updated with the actual hw queue, otherwise
wrong hctx is used and may hurt performance, especially when
wrong SRCU readlock is acquired/released

	Reported-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Signed-off-by: Ming Lei <ming.lei@redhat.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit dad7a3be4960e5545882a0cd8d7613af22874314)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 1b06c94aa73d,4ddfa019face..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1543,34 -1601,49 +1543,60 @@@ static void blk_sq_make_request(struct 
  		}
  
  		list_add_tail(&rq->queuelist, &plug->mq_list);
 -	} else if (plug && !blk_queue_nomerges(q)) {
 -		blk_mq_bio_to_request(rq, bio);
 +		return;
 +	}
  
 +	if (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {
  		/*
 -		 * We do limited plugging. If the bio can be merged, do that.
 -		 * Otherwise the existing request in the plug list will be
 -		 * issued. So the plug list will have one request at most
 -		 * The plug list might get flushed before this. If that happens,
 -		 * the plug list is empty, and same_queue_rq is invalid.
 +		 * For a SYNC request, send it to the hardware immediately. For
 +		 * an ASYNC request, just ensure that we run it later on. The
 +		 * latter allows for merging opportunities and more efficient
 +		 * dispatching.
  		 */
 -		if (list_empty(&plug->mq_list))
 -			same_queue_rq = NULL;
 -		if (same_queue_rq)
 -			list_del_init(&same_queue_rq->queuelist);
 -		list_add_tail(&rq->queuelist, &plug->mq_list);
 +run_queue:
 +		blk_mq_run_hw_queue(data.hctx, !is_sync || is_flush_fua);
 +	}
  
++<<<<<<< HEAD
 +	blk_mq_put_ctx(data.ctx);
++=======
+ 		blk_mq_put_ctx(data.ctx);
+ 
+ 		if (same_queue_rq) {
+ 			data.hctx = blk_mq_map_queue(q,
+ 					same_queue_rq->mq_ctx->cpu);
+ 			blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ 					&cookie);
+ 		}
+ 	} else if (q->nr_hw_queues > 1 && is_sync) {
+ 		blk_mq_put_ctx(data.ctx);
+ 		blk_mq_bio_to_request(rq, bio);
+ 		blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ 	} else if (q->elevator) {
+ 		blk_mq_put_ctx(data.ctx);
+ 		blk_mq_bio_to_request(rq, bio);
+ 		blk_mq_sched_insert_request(rq, false, true, true, true);
+ 	} else if (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {
+ 		blk_mq_put_ctx(data.ctx);
+ 		blk_mq_run_hw_queue(data.hctx, true);
+ 	} else
+ 		blk_mq_put_ctx(data.ctx);
+ 
+ 	return cookie;
++>>>>>>> dad7a3be4960 (blk-mq: pass correct hctx to blk_mq_try_issue_directly)
 +}
 +
 +/*
 + * Default mapping to a software queue, since we use one per CPU.
 + */
 +struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q, const int cpu)
 +{
 +	return q->queue_hw_ctx[q->mq_map[cpu]];
  }
 +EXPORT_SYMBOL(blk_mq_map_queue);
  
 -void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 -		     unsigned int hctx_idx)
 +static void blk_mq_free_rq_map(struct blk_mq_tag_set *set,
 +		struct blk_mq_tags *tags, unsigned int hctx_idx)
  {
  	struct page *page;
  
* Unmerged path block/blk-mq.c
