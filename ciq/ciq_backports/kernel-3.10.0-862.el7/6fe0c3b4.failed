nfp: add support for xdp_adjust_head()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 6fe0c3b43804c9803a4a31c9d2ed8c2014bc0dc3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6fe0c3b4.failed

Support prepending data from XDP.  We are already always allocating
some headroom because FW may prepend metadata to packets.
xdp_adjust_head() can be supported by making sure that headroom is
big enough for XDP.  In case FW had prepended metadata to the packet,
however, we have to move it out of the way before we call XDP.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 6fe0c3b43804c9803a4a31c9d2ed8c2014bc0dc3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/nfp_net.h
#	drivers/net/ethernet/netronome/nfp/nfp_net_common.c
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net.h
index 1826ee93d1da,4d45f4573b57..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net.h
@@@ -417,18 -425,82 +417,90 @@@ static inline bool nfp_net_fw_ver_eq(st
  	       fw_ver->minor == minor;
  }
  
++<<<<<<< HEAD
++=======
+ struct nfp_stat_pair {
+ 	u64 pkts;
+ 	u64 bytes;
+ };
+ 
+ /**
+  * struct nfp_net_dp - NFP network device datapath data structure
+  * @dev:		Backpointer to struct device
+  * @netdev:		Backpointer to net_device structure
+  * @is_vf:		Is the driver attached to a VF?
+  * @bpf_offload_skip_sw:  Offloaded BPF program will not be rerun by cls_bpf
+  * @bpf_offload_xdp:	Offloaded BPF program is XDP
+  * @chained_metadata_format:  Firemware will use new metadata format
+  * @rx_dma_dir:		Mapping direction for RX buffers
+  * @rx_dma_off:		Offset at which DMA packets (for XDP headroom)
+  * @rx_offset:		Offset in the RX buffers where packet data starts
+  * @ctrl:		Local copy of the control register/word.
+  * @fl_bufsz:		Currently configured size of the freelist buffers
+  * @xdp_prog:		Installed XDP program
+  * @tx_rings:		Array of pre-allocated TX ring structures
+  * @rx_rings:		Array of pre-allocated RX ring structures
+  * @ctrl_bar:		Pointer to mapped control BAR
+  *
+  * @txd_cnt:		Size of the TX ring in number of descriptors
+  * @rxd_cnt:		Size of the RX ring in number of descriptors
+  * @num_r_vecs:		Number of used ring vectors
+  * @num_tx_rings:	Currently configured number of TX rings
+  * @num_stack_tx_rings:	Number of TX rings used by the stack (not XDP)
+  * @num_rx_rings:	Currently configured number of RX rings
+  * @mtu:		Device MTU
+  */
+ struct nfp_net_dp {
+ 	struct device *dev;
+ 	struct net_device *netdev;
+ 
+ 	u8 is_vf:1;
+ 	u8 bpf_offload_skip_sw:1;
+ 	u8 bpf_offload_xdp:1;
+ 	u8 chained_metadata_format:1;
+ 
+ 	u8 rx_dma_dir;
+ 	u8 rx_dma_off;
+ 
+ 	u8 rx_offset;
+ 
+ 	u32 ctrl;
+ 	u32 fl_bufsz;
+ 
+ 	struct bpf_prog *xdp_prog;
+ 
+ 	struct nfp_net_tx_ring *tx_rings;
+ 	struct nfp_net_rx_ring *rx_rings;
+ 
+ 	u8 __iomem *ctrl_bar;
+ 
+ 	/* Cold data follows */
+ 
+ 	unsigned int txd_cnt;
+ 	unsigned int rxd_cnt;
+ 
+ 	unsigned int num_r_vecs;
+ 
+ 	unsigned int num_tx_rings;
+ 	unsigned int num_stack_tx_rings;
+ 	unsigned int num_rx_rings;
+ 
+ 	unsigned int mtu;
+ };
+ 
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  /**
   * struct nfp_net - NFP network device structure
 - * @dp:			Datapath structure
 - * @fw_ver:		Firmware version
 + * @pdev:               Backpointer to PCI device
 + * @netdev:             Backpointer to net_device structure
 + * @is_vf:              Is the driver attached to a VF?
 + * @fw_loaded:          Is the firmware loaded?
 + * @ctrl:               Local copy of the control register/word.
 + * @fl_bufsz:           Currently configured size of the freelist buffers
 + * @rx_offset:		Offset in the RX buffers where packet data starts
 + * @fw_ver:             Firmware version
   * @cap:                Capabilities advertised by the Firmware
   * @max_mtu:            Maximum support MTU advertised by the Firmware
 - * @rss_hfunc:		RSS selected hash function
   * @rss_cfg:            RSS configuration
   * @rss_key:            RSS secret key
   * @rss_itbl:           RSS indirection table
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index e0a7eb1db7a9,f134f1808b9a..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@@ -1036,11 -1100,12 +1036,16 @@@ nfp_net_calc_fl_bufsz(struct nfp_net *n
  	unsigned int fl_bufsz;
  
  	fl_bufsz = NFP_NET_RX_BUF_HEADROOM;
++<<<<<<< HEAD
 +	if (nn->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC)
++=======
+ 	fl_bufsz += dp->rx_dma_off;
+ 	if (dp->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC)
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  		fl_bufsz += NFP_NET_MAX_PREPEND;
  	else
 -		fl_bufsz += dp->rx_offset;
 -	fl_bufsz += ETH_HLEN + VLAN_HLEN * 2 + dp->mtu;
 +		fl_bufsz += nn->rx_offset;
 +	fl_bufsz += ETH_HLEN + VLAN_HLEN * 2 + mtu;
  
  	fl_bufsz = SKB_DATA_ALIGN(fl_bufsz);
  	fl_bufsz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
@@@ -1351,6 -1455,79 +1363,82 @@@ nfp_net_rx_drop(const struct nfp_net_d
  		dev_kfree_skb_any(skb);
  }
  
++<<<<<<< HEAD
++=======
+ static bool
+ nfp_net_tx_xdp_buf(struct nfp_net_dp *dp, struct nfp_net_rx_ring *rx_ring,
+ 		   struct nfp_net_tx_ring *tx_ring,
+ 		   struct nfp_net_rx_buf *rxbuf, unsigned int dma_off,
+ 		   unsigned int pkt_len)
+ {
+ 	struct nfp_net_tx_buf *txbuf;
+ 	struct nfp_net_tx_desc *txd;
+ 	dma_addr_t new_dma_addr;
+ 	void *new_frag;
+ 	int wr_idx;
+ 
+ 	if (unlikely(nfp_net_tx_full(tx_ring, 1))) {
+ 		nfp_net_rx_drop(dp, rx_ring->r_vec, rx_ring, rxbuf, NULL);
+ 		return false;
+ 	}
+ 
+ 	new_frag = nfp_net_napi_alloc_one(dp, &new_dma_addr);
+ 	if (unlikely(!new_frag)) {
+ 		nfp_net_rx_drop(dp, rx_ring->r_vec, rx_ring, rxbuf, NULL);
+ 		return false;
+ 	}
+ 	nfp_net_rx_give_one(dp, rx_ring, new_frag, new_dma_addr);
+ 
+ 	wr_idx = tx_ring->wr_p & (tx_ring->cnt - 1);
+ 
+ 	/* Stash the soft descriptor of the head then initialize it */
+ 	txbuf = &tx_ring->txbufs[wr_idx];
+ 	txbuf->frag = rxbuf->frag;
+ 	txbuf->dma_addr = rxbuf->dma_addr;
+ 	txbuf->fidx = -1;
+ 	txbuf->pkt_cnt = 1;
+ 	txbuf->real_len = pkt_len;
+ 
+ 	dma_sync_single_for_device(dp->dev, rxbuf->dma_addr + dma_off,
+ 				   pkt_len, DMA_BIDIRECTIONAL);
+ 
+ 	/* Build TX descriptor */
+ 	txd = &tx_ring->txds[wr_idx];
+ 	txd->offset_eop = PCIE_DESC_TX_EOP;
+ 	txd->dma_len = cpu_to_le16(pkt_len);
+ 	nfp_desc_set_dma_addr(txd, rxbuf->dma_addr + dma_off);
+ 	txd->data_len = cpu_to_le16(pkt_len);
+ 
+ 	txd->flags = 0;
+ 	txd->mss = 0;
+ 	txd->l4_offset = 0;
+ 
+ 	tx_ring->wr_p++;
+ 	tx_ring->wr_ptr_add++;
+ 	return true;
+ }
+ 
+ static int nfp_net_run_xdp(struct bpf_prog *prog, void *data, void *hard_start,
+ 			   unsigned int *off, unsigned int *len)
+ {
+ 	struct xdp_buff xdp;
+ 	void *orig_data;
+ 	int ret;
+ 
+ 	xdp.data_hard_start = hard_start;
+ 	xdp.data = data + *off;
+ 	xdp.data_end = data + *off + *len;
+ 
+ 	orig_data = xdp.data;
+ 	ret = bpf_prog_run_xdp(prog, &xdp);
+ 
+ 	*len -= xdp.data - orig_data;
+ 	*off += xdp.data - orig_data;
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  /**
   * nfp_net_rx() - receive up to @budget packets on @rx_ring
   * @rx_ring:   RX ring to receive from
@@@ -1370,8 -1550,14 +1458,13 @@@ static int nfp_net_rx(struct nfp_net_rx
  	int pkts_polled = 0;
  	int idx;
  
 -	rcu_read_lock();
 -	xdp_prog = READ_ONCE(dp->xdp_prog);
 -	true_bufsz = xdp_prog ? PAGE_SIZE : dp->fl_bufsz;
 -	tx_ring = r_vec->xdp_ring;
 -
  	while (pkts_polled < budget) {
++<<<<<<< HEAD
 +		unsigned int meta_len, data_len, data_off, pkt_len, pkt_off;
++=======
+ 		unsigned int meta_len, data_len, data_off, pkt_len;
+ 		u8 meta_prepend[NFP_NET_MAX_PREPEND];
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  		struct nfp_net_rx_buf *rxbuf;
  		struct nfp_net_rx_desc *rxd;
  		dma_addr_t new_dma_addr;
@@@ -1408,11 -1595,11 +1501,16 @@@
  		data_len = le16_to_cpu(rxd->rxd.data_len);
  		pkt_len = data_len - meta_len;
  
 -		if (dp->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC)
 -			data_off = NFP_NET_RX_BUF_HEADROOM + meta_len;
 +		if (nn->rx_offset == NFP_NET_CFG_RX_OFFSET_DYNAMIC)
 +			pkt_off = meta_len;
  		else
++<<<<<<< HEAD
 +			pkt_off = nn->rx_offset;
 +		data_off = NFP_NET_RX_BUF_HEADROOM + pkt_off;
++=======
+ 			data_off = NFP_NET_RX_BUF_HEADROOM + dp->rx_offset;
+ 		data_off += dp->rx_dma_off;
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  
  		/* Stats update */
  		u64_stats_update_begin(&r_vec->rx_sync);
@@@ -1420,34 -1607,96 +1518,108 @@@
  		r_vec->rx_bytes += pkt_len;
  		u64_stats_update_end(&r_vec->rx_sync);
  
++<<<<<<< HEAD
 +		skb = build_skb(rxbuf->frag, nn->fl_bufsz);
++=======
+ 		/* Pointer to start of metadata */
+ 		meta = rxbuf->frag + data_off - meta_len;
+ 
+ 		if (unlikely(meta_len > NFP_NET_MAX_PREPEND ||
+ 			     (dp->rx_offset && meta_len > dp->rx_offset))) {
+ 			nn_dp_warn(dp, "oversized RX packet metadata %u\n",
+ 				   meta_len);
+ 			nfp_net_rx_drop(dp, r_vec, rx_ring, rxbuf, NULL);
+ 			continue;
+ 		}
+ 
+ 		if (xdp_prog && !(rxd->rxd.flags & PCIE_DESC_RX_BPF &&
+ 				  dp->bpf_offload_xdp)) {
+ 			unsigned int dma_off;
+ 			void *hard_start;
+ 			int act;
+ 
+ 			hard_start = rxbuf->frag + NFP_NET_RX_BUF_HEADROOM;
+ 			dma_off = data_off - NFP_NET_RX_BUF_HEADROOM;
+ 			dma_sync_single_for_cpu(dp->dev, rxbuf->dma_addr,
+ 						dma_off + pkt_len,
+ 						DMA_BIDIRECTIONAL);
+ 
+ 			/* Move prepend out of the way */
+ 			if (xdp_prog->xdp_adjust_head) {
+ 				memcpy(meta_prepend, meta, meta_len);
+ 				meta = meta_prepend;
+ 			}
+ 
+ 			act = nfp_net_run_xdp(xdp_prog, rxbuf->frag, hard_start,
+ 					      &data_off, &pkt_len);
+ 			switch (act) {
+ 			case XDP_PASS:
+ 				break;
+ 			case XDP_TX:
+ 				dma_off = data_off - NFP_NET_RX_BUF_HEADROOM;
+ 				if (unlikely(!nfp_net_tx_xdp_buf(dp, rx_ring,
+ 								 tx_ring, rxbuf,
+ 								 dma_off,
+ 								 pkt_len)))
+ 					trace_xdp_exception(dp->netdev,
+ 							    xdp_prog, act);
+ 				continue;
+ 			default:
+ 				bpf_warn_invalid_xdp_action(act);
+ 			case XDP_ABORTED:
+ 				trace_xdp_exception(dp->netdev, xdp_prog, act);
+ 			case XDP_DROP:
+ 				nfp_net_rx_give_one(dp, rx_ring, rxbuf->frag,
+ 						    rxbuf->dma_addr);
+ 				continue;
+ 			}
+ 		}
+ 
+ 		skb = build_skb(rxbuf->frag, true_bufsz);
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  		if (unlikely(!skb)) {
- 			nfp_net_rx_drop(r_vec, rx_ring, rxbuf, NULL);
+ 			nfp_net_rx_drop(dp, r_vec, rx_ring, rxbuf, NULL);
  			continue;
  		}
 -		new_frag = nfp_net_napi_alloc_one(dp, &new_dma_addr);
 +
 +		nfp_net_set_hash(nn->netdev, skb, rxd);
 +
 +		new_frag = nfp_net_napi_alloc_one(nn, &new_dma_addr);
  		if (unlikely(!new_frag)) {
- 			nfp_net_rx_drop(r_vec, rx_ring, rxbuf, skb);
+ 			nfp_net_rx_drop(dp, r_vec, rx_ring, rxbuf, skb);
  			continue;
  		}
  
 -		nfp_net_dma_unmap_rx(dp, rxbuf->dma_addr);
 +		nfp_net_dma_unmap_rx(nn, rx_ring->rxbufs[idx].dma_addr,
 +				     nn->fl_bufsz, DMA_FROM_DEVICE);
  
- 		nfp_net_rx_give_one(rx_ring, new_frag, new_dma_addr);
+ 		nfp_net_rx_give_one(dp, rx_ring, new_frag, new_dma_addr);
  
  		skb_reserve(skb, data_off);
  		skb_put(skb, pkt_len);
  
++<<<<<<< HEAD
 +		nfp_net_set_hash_desc(nn->netdev, skb, rxd);
++=======
+ 		if (!dp->chained_metadata_format) {
+ 			nfp_net_set_hash_desc(dp->netdev, skb, meta, rxd);
+ 		} else if (meta_len) {
+ 			void *end;
+ 
+ 			end = nfp_net_parse_meta(dp->netdev, skb, meta,
+ 						 meta_len);
+ 			if (unlikely(end != meta + meta_len)) {
+ 				nn_dp_warn(dp, "invalid RX packet metadata\n");
+ 				nfp_net_rx_drop(dp, r_vec, rx_ring, NULL, skb);
+ 				continue;
+ 			}
+ 		}
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  
  		skb_record_rx_queue(skb, rx_ring->idx);
 -		skb->protocol = eth_type_trans(skb, dp->netdev);
 +		skb->protocol = eth_type_trans(skb, nn->netdev);
  
 -		nfp_net_rx_csum(dp, r_vec, rxd, skb);
 +		nfp_net_rx_csum(nn, r_vec, rxd, skb);
  
  		if (rxd->rxd.flags & PCIE_DESC_RX_VLAN)
  			__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
@@@ -1964,10 -2178,10 +2136,15 @@@ static int __nfp_net_set_config_and_ena
  	nn_writel(nn, NFP_NET_CFG_CTRL, new_ctrl);
  	err = nfp_net_reconfig(nn, update);
  
 -	nn->dp.ctrl = new_ctrl;
 +	nn->ctrl = new_ctrl;
  
++<<<<<<< HEAD
 +	for (r = 0; r < nn->num_rx_rings; r++)
 +		nfp_net_rx_ring_fill_freelist(&nn->rx_rings[r]);
++=======
+ 	for (r = 0; r < nn->dp.num_rx_rings; r++)
+ 		nfp_net_rx_ring_fill_freelist(&nn->dp, &nn->dp.rx_rings[r]);
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  
  	/* Since reconfiguration requests while NFP is down are ignored we
  	 * have to wipe the entire VXLAN configuration and reinitialize it.
@@@ -2595,8 -2873,91 +2772,95 @@@ static void nfp_net_del_vxlan_port(stru
  		nfp_net_set_vxlan_port(nn, idx, 0);
  }
  
++<<<<<<< HEAD
++=======
+ static int nfp_net_xdp_offload(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct tc_cls_bpf_offload cmd = {
+ 		.prog = prog,
+ 	};
+ 	int ret;
+ 
+ 	if (!nfp_net_ebpf_capable(nn))
+ 		return -EINVAL;
+ 
+ 	if (nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF) {
+ 		if (!nn->dp.bpf_offload_xdp)
+ 			return prog ? -EBUSY : 0;
+ 		cmd.command = prog ? TC_CLSBPF_REPLACE : TC_CLSBPF_DESTROY;
+ 	} else {
+ 		if (!prog)
+ 			return 0;
+ 		cmd.command = TC_CLSBPF_ADD;
+ 	}
+ 
+ 	ret = nfp_net_bpf_offload(nn, &cmd);
+ 	/* Stop offload if replace not possible */
+ 	if (ret && cmd.command == TC_CLSBPF_REPLACE)
+ 		nfp_net_xdp_offload(nn, NULL);
+ 	nn->dp.bpf_offload_xdp = prog && !ret;
+ 	return ret;
+ }
+ 
+ static int nfp_net_xdp_setup(struct nfp_net *nn, struct bpf_prog *prog)
+ {
+ 	struct bpf_prog *old_prog = nn->dp.xdp_prog;
+ 	struct nfp_net_dp *dp;
+ 	int err;
+ 
+ 	if (!prog && !nn->dp.xdp_prog)
+ 		return 0;
+ 	if (prog && nn->dp.xdp_prog) {
+ 		prog = xchg(&nn->dp.xdp_prog, prog);
+ 		bpf_prog_put(prog);
+ 		nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 		return 0;
+ 	}
+ 
+ 	dp = nfp_net_clone_dp(nn);
+ 	if (!dp)
+ 		return -ENOMEM;
+ 
+ 	dp->xdp_prog = prog;
+ 	dp->num_tx_rings += prog ? nn->dp.num_rx_rings : -nn->dp.num_rx_rings;
+ 	dp->rx_dma_dir = prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
+ 	if (prog)
+ 		dp->rx_dma_off = XDP_PACKET_HEADROOM -
+ 			(nn->dp.rx_offset ?: NFP_NET_MAX_PREPEND);
+ 	else
+ 		dp->rx_dma_off = 0;
+ 
+ 	/* We need RX reconfig to remap the buffers (BIDIR vs FROM_DEV) */
+ 	err = nfp_net_ring_reconfig(nn, dp);
+ 	if (err)
+ 		return err;
+ 
+ 	if (old_prog)
+ 		bpf_prog_put(old_prog);
+ 
+ 	nfp_net_xdp_offload(nn, nn->dp.xdp_prog);
+ 
+ 	return 0;
+ }
+ 
+ static int nfp_net_xdp(struct net_device *netdev, struct netdev_xdp *xdp)
+ {
+ 	struct nfp_net *nn = netdev_priv(netdev);
+ 
+ 	switch (xdp->command) {
+ 	case XDP_SETUP_PROG:
+ 		return nfp_net_xdp_setup(nn, xdp->prog);
+ 	case XDP_QUERY_PROG:
+ 		xdp->prog_attached = !!nn->dp.xdp_prog;
+ 		return 0;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  static const struct net_device_ops nfp_net_netdev_ops = {
 +	.ndo_size		= sizeof(struct net_device_ops),
  	.ndo_open		= nfp_net_netdev_open,
  	.ndo_stop		= nfp_net_netdev_close,
  	.ndo_start_xmit		= nfp_net_tx,
@@@ -2745,6 -3155,17 +3009,20 @@@ int nfp_net_netdev_init(struct net_devi
  	struct nfp_net *nn = netdev_priv(netdev);
  	int err;
  
++<<<<<<< HEAD
++=======
+ 	/* XDP calls for 256 byte packet headroom which wouldn't fit in a u8.
+ 	 * We, however, reuse the metadata prepend space for XDP buffers which
+ 	 * is at least 1 byte long and as long as XDP headroom doesn't increase
+ 	 * above 256 the *extra* XDP headroom will fit on 8 bits.
+ 	 */
+ 	BUILD_BUG_ON(XDP_PACKET_HEADROOM > 256);
+ 
+ 	nn->dp.chained_metadata_format = nn->fw_ver.major > 3;
+ 
+ 	nn->dp.rx_dma_dir = DMA_FROM_DEVICE;
+ 
++>>>>>>> 6fe0c3b43804 (nfp: add support for xdp_adjust_head())
  	/* Get some of the read-only fields from the BAR */
  	nn->cap = nn_readl(nn, NFP_NET_CFG_CAP);
  	nn->max_mtu = nn_readl(nn, NFP_NET_CFG_MAX_MTU);
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net.h
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_common.c
