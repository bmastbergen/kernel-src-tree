mm: rmap use pte lock not mmap_sem to set PageMlocked

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] rmap: use pte lock not mmap_sem to set PageMlocked (Andrea Arcangeli) [1450367]
Rebuild_FUZZ: 95.15%
commit-author Hugh Dickins <hughd@google.com>
commit b87537d9e2feb30f6a962f27eb32768682698d3b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b87537d9.failed

KernelThreadSanitizer (ktsan) has shown that the down_read_trylock() of
mmap_sem in try_to_unmap_one() (when going to set PageMlocked on a page
found mapped in a VM_LOCKED vma) is ineffective against races with
exit_mmap()'s munlock_vma_pages_all(), because mmap_sem is not held when
tearing down an mm.

But that's okay, those races are benign; and although we've believed for
years in that ugly down_read_trylock(), it's unsuitable for the job, and
frustrates the good intention of setting PageMlocked when it fails.

It just doesn't matter if here we read vm_flags an instant before or after
a racing mlock() or munlock() or exit_mmap() sets or clears VM_LOCKED: the
syscalls (or exit) work their way up the address space (taking pt locks
after updating vm_flags) to establish the final state.

We do still need to be careful never to mark a page Mlocked (hence
unevictable) by any race that will not be corrected shortly after.  The
page lock protects from many of the races, but not all (a page is not
necessarily locked when it's unmapped).  But the pte lock we just dropped
is good to cover the rest (and serializes even with
munlock_vma_pages_all(), so no special barriers required): now hold on to
the pte lock while calling mlock_vma_page().  Is that lock ordering safe?
Yes, that's how follow_page_pte() calls it, and how page_remove_rmap()
calls the complementary clear_page_mlock().

This fixes the following case (though not a case which anyone has
complained of), which mmap_sem did not: truncation's preliminary
unmap_mapping_range() is supposed to remove even the anonymous COWs of
filecache pages, and that might race with try_to_unmap_one() on a
VM_LOCKED vma, so that mlock_vma_page() sets PageMlocked just after
zap_pte_range() unmaps the page, causing "Bad page state (mlocked)" when
freed.  The pte lock protects against this.

You could say that it also protects against the more ordinary case, racing
with the preliminary unmapping of a filecache page itself: but in our
current tree, that's independently protected by i_mmap_rwsem; and that
race would be why "Bad page state (mlocked)" was seen before commit
48ec833b7851 ("Revert mm/memory.c: share the i_mmap_rwsem").

Vlastimil Babka points out another race which this patch protects against.
 try_to_unmap_one() might reach its mlock_vma_page() TestSetPageMlocked a
moment after munlock_vma_pages_all() did its Phase 1 TestClearPageMlocked:
leaving PageMlocked and unevictable when it should be evictable.  mmap_sem
is ineffective because exit_mmap() does not hold it; page lock ineffective
because __munlock_pagevec() only takes it afterwards, in Phase 2; pte lock
is effective because __munlock_pagevec_fill() takes it to get the page,
after VM_LOCKED was cleared from vm_flags, so visible to try_to_unmap_one.

Kirill Shutemov points out that if the compiler chooses to implement a
"vma->vm_flags &= VM_WHATEVER" or "vma->vm_flags |= VM_WHATEVER" operation
with an intermediate store of unrelated bits set, since I'm here foregoing
its usual protection by mmap_sem, try_to_unmap_one() might catch sight of
a spurious VM_LOCKED in vm_flags, and make the wrong decision.  This does
not appear to be an immediate problem, but we may want to define vm_flags
accessors in future, to guard against such a possibility.

While we're here, make a related optimization in try_to_munmap_one(): if
it's doing TTU_MUNLOCK, then there's no point at all in descending the
page tables and getting the pt lock, unless the vma is VM_LOCKED.  Yes,
that can change racily, but it can change racily even without the
optimization: it's not critical.  Far better not to waste time here.

Stopped short of separating try_to_munlock_one() from try_to_munmap_one()
on this occasion, but that's probably the sensible next step - with a
rename, given that try_to_munlock()'s business is to try to set Mlocked.

Updated the unevictable-lru Documentation, to remove its reference to mmap
semaphore, but found a few more updates needed in just that area.

	Signed-off-by: Hugh Dickins <hughd@google.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Rik van Riel <riel@redhat.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: Oleg Nesterov <oleg@redhat.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b87537d9e2feb30f6a962f27eb32768682698d3b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/vm/unevictable-lru.txt
#	mm/rmap.c
diff --cc Documentation/vm/unevictable-lru.txt
index a68db7692ee8,fa3b527086fa..000000000000
--- a/Documentation/vm/unevictable-lru.txt
+++ b/Documentation/vm/unevictable-lru.txt
@@@ -523,83 -531,20 +523,95 @@@ map
  
  try_to_unmap() is always called, by either vmscan for reclaim or for page
  migration, with the argument page locked and isolated from the LRU.  Separate
- functions handle anonymous and mapped file pages, as these types of pages have
- different reverse map mechanisms.
+ functions handle anonymous and mapped file and KSM pages, as these types of
+ pages have different reverse map lookup mechanisms, with different locking.
+ In each case, whether rmap_walk_anon() or rmap_walk_file() or rmap_walk_ksm(),
+ it will call try_to_unmap_one() for every VMA which might contain the page.
  
-  (*) try_to_unmap_anon()
+ When trying to reclaim, if try_to_unmap_one() finds the page in a VM_LOCKED
+ VMA, it will then mlock the page via mlock_vma_page() instead of unmapping it,
+ and return SWAP_MLOCK to indicate that the page is unevictable: and the scan
+ stops there.
  
++<<<<<<< HEAD
 +     To unmap anonymous pages, each VMA in the list anchored in the anon_vma
 +     must be visited - at least until a VM_LOCKED VMA is encountered.  If the
 +     page is being unmapped for migration, VM_LOCKED VMAs do not stop the
 +     process because mlocked pages are migratable.  However, for reclaim, if
 +     the page is mapped into a VM_LOCKED VMA, the scan stops.
 +
 +     try_to_unmap_anon() attempts to acquire in read mode the mmap semaphore of
 +     the mm_struct to which the VMA belongs.  If this is successful, it will
 +     mlock the page via mlock_vma_page() - we wouldn't have gotten to
 +     try_to_unmap_anon() if the page were already mlocked - and will return
 +     SWAP_MLOCK, indicating that the page is unevictable.
 +
 +     If the mmap semaphore cannot be acquired, we are not sure whether the page
 +     is really unevictable or not.  In this case, try_to_unmap_anon() will
 +     return SWAP_AGAIN.
 +
 + (*) try_to_unmap_file() - linear mappings
 +
 +     Unmapping of a mapped file page works the same as for anonymous mappings,
 +     except that the scan visits all VMAs that map the page's index/page offset
 +     in the page's mapping's reverse map priority search tree.  It also visits
 +     each VMA in the page's mapping's non-linear list, if the list is
 +     non-empty.
 +
 +     As for anonymous pages, on encountering a VM_LOCKED VMA for a mapped file
 +     page, try_to_unmap_file() will attempt to acquire the associated
 +     mm_struct's mmap semaphore to mlock the page, returning SWAP_MLOCK if this
 +     is successful, and SWAP_AGAIN, if not.
++=======
+ mlock_vma_page() is called while holding the page table's lock (in addition
+ to the page lock, and the rmap lock): to serialize against concurrent mlock or
+ munlock or munmap system calls, mm teardown (munlock_vma_pages_all), reclaim,
+ holepunching, and truncation of file pages and their anonymous COWed pages.
++>>>>>>> b87537d9e2fe (mm: rmap use pte lock not mmap_sem to set PageMlocked)
 +
 + (*) try_to_unmap_file() - non-linear mappings
 +
 +     If a page's mapping contains a non-empty non-linear mapping VMA list, then
 +     try_to_un{map|lock}() must also visit each VMA in that list to determine
 +     whether the page is mapped in a VM_LOCKED VMA.  Again, the scan must visit
 +     all VMAs in the non-linear list to ensure that the pages is not/should not
 +     be mlocked.
 +
 +     If a VM_LOCKED VMA is found in the list, the scan could terminate.
 +     However, there is no easy way to determine whether the page is actually
 +     mapped in a given VMA - either for unmapping or testing whether the
 +     VM_LOCKED VMA actually pins the page.
 +
 +     try_to_unmap_file() handles non-linear mappings by scanning a certain
 +     number of pages - a "cluster" - in each non-linear VMA associated with the
 +     page's mapping, for each file mapped page that vmscan tries to unmap.  If
 +     this happens to unmap the page we're trying to unmap, try_to_unmap() will
 +     notice this on return (page_mapcount(page) will be 0) and return
 +     SWAP_SUCCESS.  Otherwise, it will return SWAP_AGAIN, causing vmscan to
 +     recirculate this page.  We take advantage of the cluster scan in
 +     try_to_unmap_cluster() as follows:
 +
 +	For each non-linear VMA, try_to_unmap_cluster() attempts to acquire the
 +	mmap semaphore of the associated mm_struct for read without blocking.
 +
 +	If this attempt is successful and the VMA is VM_LOCKED,
 +	try_to_unmap_cluster() will retain the mmap semaphore for the scan;
 +	otherwise it drops it here.
 +
 +	Then, for each page in the cluster, if we're holding the mmap semaphore
 +	for a locked VMA, try_to_unmap_cluster() calls mlock_vma_page() to
 +	mlock the page.  This call is a no-op if the page is already locked,
 +	but will mlock any pages in the non-linear mapping that happen to be
 +	unlocked.
 +
 +	If one of the pages so mlocked is the page passed in to try_to_unmap(),
 +	try_to_unmap_cluster() will return SWAP_MLOCK, rather than the default
 +	SWAP_AGAIN.  This will allow vmscan to cull the page, rather than
 +	recirculating it on the inactive list.
 +
 +	Again, if try_to_unmap_cluster() cannot acquire the VMA's mmap sem, it
 +	returns SWAP_AGAIN, indicating that the page is mapped by a VM_LOCKED
 +	VMA, but couldn't be mlocked.
  
  
  try_to_munlock() REVERSE MAP SCAN
@@@ -615,24 -560,11 +627,30 @@@ all PTEs from the page.  For this purpo
  introduced a variant of try_to_unmap() called try_to_munlock().
  
  try_to_munlock() calls the same functions as try_to_unmap() for anonymous and
- mapped file pages with an additional argument specifying unlock versus unmap
+ mapped file and KSM pages with a flag argument specifying unlock versus unmap
  processing.  Again, these functions walk the respective reverse maps looking
++<<<<<<< HEAD
 +for VM_LOCKED VMAs.  When such a VMA is found for anonymous pages and file
 +pages mapped in linear VMAs, as in the try_to_unmap() case, the functions
 +attempt to acquire the associated mmap semaphore, mlock the page via
 +mlock_vma_page() and return SWAP_MLOCK.  This effectively undoes the
 +pre-clearing of the page's PG_mlocked done by munlock_vma_page.
 +
 +If try_to_unmap() is unable to acquire a VM_LOCKED VMA's associated mmap
 +semaphore, it will return SWAP_AGAIN.  This will allow shrink_page_list() to
 +recycle the page on the inactive list and hope that it has better luck with the
 +page next time.
++=======
+ for VM_LOCKED VMAs.  When such a VMA is found, as in the try_to_unmap() case,
+ the functions mlock the page via mlock_vma_page() and return SWAP_MLOCK.  This
+ undoes the pre-clearing of the page's PG_mlocked done by munlock_vma_page.
++>>>>>>> b87537d9e2fe (mm: rmap use pte lock not mmap_sem to set PageMlocked)
 +
 +For file pages mapped into non-linear VMAs, the try_to_munlock() logic works
 +slightly differently.  On encountering a VM_LOCKED non-linear VMA that might
 +map the page, try_to_munlock() returns SWAP_AGAIN without actually mlocking the
 +page.  munlock_vma_page() will just leave the page unlocked and let vmscan deal
 +with it - the usual fallback position.
  
  Note that try_to_munlock()'s reverse map walk must visit every VMA in a page's
  reverse map to determine that a page is NOT mapped into any VM_LOCKED VMA.
diff --cc mm/rmap.c
index bab4f8a26c59,b93fb540c525..000000000000
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@@ -1349,53 -1302,12 +1349,57 @@@ int try_to_unmap_one(struct page *page
  	pte_t pteval;
  	spinlock_t *ptl;
  	int ret = SWAP_AGAIN;
 -	enum ttu_flags flags = (enum ttu_flags)arg;
 +
 +	if ((flags & TTU_MIGRATION) && is_zone_device_page(page)) {
 +		swp_entry_t entry;
 +		pte_t swp_pte;
 +		pmd_t *pmdp;
 +
 +		if (!is_hmm_page(page))
 +			goto out;
 +
 +		pmdp = mm_find_pmd(mm, address);
 +		if (!pmdp)
 +			goto out;
 +
 +		pte = pte_offset_map_lock(mm, pmdp, address, &ptl);
 +		if (!pte)
 +			goto out;
 +
 +		pteval = ptep_get_and_clear(mm, address, pte);
 +		if (pte_present(pteval) || pte_none(pteval) || pte_file(pteval)) {
 +			set_pte_at(mm, address, pte, pteval);
 +			goto out_unmap;
 +		}
 +
 +		entry = pte_to_swp_entry(pteval);
 +		if (!is_hmm_entry(entry)) {
 +			set_pte_at(mm, address, pte, pteval);
 +			goto out_unmap;
 +		}
 +
 +		if (hmm_entry_to_page(entry) != page) {
 +			set_pte_at(mm, address, pte, pteval);
 +			goto out_unmap;
 +		}
 +
 +		/*
 +		 * Store the pfn of the page in a special migration
 +		 * pte. do_swap_page() will wait until the migration
 +		 * pte is removed and then restart fault handling.
 +		 */
 +		entry = make_migration_entry(page, 0);
 +		swp_pte = swp_entry_to_pte(entry);
 +		if (pte_soft_dirty(*pte))
 +			swp_pte = pte_swp_mksoft_dirty(swp_pte);
 +		set_pte_at(mm, address, pte, swp_pte);
 +		goto out_unmap;
 +	}
  
+ 	/* munlock has nothing to gain from examining un-locked vmas */
+ 	if ((flags & TTU_MUNLOCK) && !(vma->vm_flags & VM_LOCKED))
+ 		goto out;
+ 
  	pte = page_check_address(page, mm, address, &ptl, 0);
  	if (!pte)
  		goto out;
@@@ -1406,10 -1318,13 +1410,20 @@@
  	 * skipped over this mm) then we should reactivate it.
  	 */
  	if (!(flags & TTU_IGNORE_MLOCK)) {
++<<<<<<< HEAD
 +		if (vma->vm_flags & VM_LOCKED)
 +			goto out_mlock;
 +
 +		if (TTU_ACTION(flags) == TTU_MUNLOCK)
++=======
+ 		if (vma->vm_flags & VM_LOCKED) {
+ 			/* Holding pte lock, we do *not* need mmap_sem here */
+ 			mlock_vma_page(page);
+ 			ret = SWAP_MLOCK;
+ 			goto out_unmap;
+ 		}
+ 		if (flags & TTU_MUNLOCK)
++>>>>>>> b87537d9e2fe (mm: rmap use pte lock not mmap_sem to set PageMlocked)
  			goto out_unmap;
  	}
  	if (!(flags & TTU_IGNORE_ACCESS)) {
@@@ -1498,158 -1428,10 +1512,165 @@@
  
  out_unmap:
  	pte_unmap_unlock(pte, ptl);
++<<<<<<< HEAD
 +	if (ret != SWAP_FAIL && TTU_ACTION(flags) != TTU_MUNLOCK)
 +		mmu_notifier_invalidate_page(mm, address);
 +out:
 +	return ret;
 +
 +out_mlock:
 +	pte_unmap_unlock(pte, ptl);
 +
 +
 +	/*
 +	 * We need mmap_sem locking, Otherwise VM_LOCKED check makes
 +	 * unstable result and race. Plus, We can't wait here because
 +	 * we now hold anon_vma->rwsem or mapping->i_mmap_mutex.
 +	 * if trylock failed, the page remain in evictable lru and later
 +	 * vmscan could retry to move the page to unevictable lru if the
 +	 * page is actually mlocked.
 +	 */
 +	if (down_read_trylock(&vma->vm_mm->mmap_sem)) {
 +		if (vma->vm_flags & VM_LOCKED) {
 +			mlock_vma_page(page);
 +			ret = SWAP_MLOCK;
 +		}
 +		up_read(&vma->vm_mm->mmap_sem);
 +	}
 +	return ret;
++=======
+ 	if (ret != SWAP_FAIL && ret != SWAP_MLOCK && !(flags & TTU_MUNLOCK))
+ 		mmu_notifier_invalidate_page(mm, address);
+ out:
+ 	return ret;
++>>>>>>> b87537d9e2fe (mm: rmap use pte lock not mmap_sem to set PageMlocked)
 +}
 +
 +/*
 + * objrmap doesn't work for nonlinear VMAs because the assumption that
 + * offset-into-file correlates with offset-into-virtual-addresses does not hold.
 + * Consequently, given a particular page and its ->index, we cannot locate the
 + * ptes which are mapping that page without an exhaustive linear search.
 + *
 + * So what this code does is a mini "virtual scan" of each nonlinear VMA which
 + * maps the file to which the target page belongs.  The ->vm_private_data field
 + * holds the current cursor into that scan.  Successive searches will circulate
 + * around the vma's virtual address space.
 + *
 + * So as more replacement pressure is applied to the pages in a nonlinear VMA,
 + * more scanning pressure is placed against them as well.   Eventually pages
 + * will become fully unmapped and are eligible for eviction.
 + *
 + * For very sparsely populated VMAs this is a little inefficient - chances are
 + * there there won't be many ptes located within the scan cluster.  In this case
 + * maybe we could scan further - to the end of the pte page, perhaps.
 + *
 + * Mlocked pages:  check VM_LOCKED under mmap_sem held for read, if we can
 + * acquire it without blocking.  If vma locked, mlock the pages in the cluster,
 + * rather than unmapping them.  If we encounter the "check_page" that vmscan is
 + * trying to unmap, return SWAP_MLOCK, else default SWAP_AGAIN.
 + */
 +#define CLUSTER_SIZE	min(32*PAGE_SIZE, PMD_SIZE)
 +#define CLUSTER_MASK	(~(CLUSTER_SIZE - 1))
 +
 +static int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,
 +		struct vm_area_struct *vma, struct page *check_page)
 +{
 +	struct mm_struct *mm = vma->vm_mm;
 +	pmd_t *pmd;
 +	pte_t *pte;
 +	pte_t pteval;
 +	spinlock_t *ptl;
 +	struct page *page;
 +	unsigned long address;
 +	unsigned long mmun_start;	/* For mmu_notifiers */
 +	unsigned long mmun_end;		/* For mmu_notifiers */
 +	unsigned long end;
 +	int ret = SWAP_AGAIN;
 +	int locked_vma = 0;
 +
 +	address = (vma->vm_start + cursor) & CLUSTER_MASK;
 +	end = address + CLUSTER_SIZE;
 +	if (address < vma->vm_start)
 +		address = vma->vm_start;
 +	if (end > vma->vm_end)
 +		end = vma->vm_end;
 +
 +	pmd = mm_find_pmd(mm, address);
 +	if (!pmd)
 +		return ret;
 +
 +	mmun_start = address;
 +	mmun_end   = end;
 +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 +
 +	/*
 +	 * If we can acquire the mmap_sem for read, and vma is VM_LOCKED,
 +	 * keep the sem while scanning the cluster for mlocking pages.
 +	 */
 +	if (down_read_trylock(&vma->vm_mm->mmap_sem)) {
 +		locked_vma = (vma->vm_flags & VM_LOCKED);
 +		if (!locked_vma)
 +			up_read(&vma->vm_mm->mmap_sem); /* don't need it */
 +	}
 +
 +	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 +
 +	/* Update high watermark before we lower rss */
 +	update_hiwater_rss(mm);
 +
 +	for (; address < end; pte++, address += PAGE_SIZE) {
 +		if (!pte_present(*pte))
 +			continue;
 +		page = vm_normal_page(vma, address, *pte);
 +		BUG_ON(!page || PageAnon(page));
 +
 +		if (locked_vma) {
 +			if (page == check_page) {
 +				/* we know we have check_page locked */
 +				mlock_vma_page(page);
 +				ret = SWAP_MLOCK;
 +			} else if (trylock_page(page)) {
 +				/*
 +				 * If we can lock the page, perform mlock.
 +				 * Otherwise leave the page alone, it will be
 +				 * eventually encountered again later.
 +				 */
 +				mlock_vma_page(page);
 +				unlock_page(page);
 +			}
 +			continue;	/* don't unmap */
 +		}
 +
 +		if (ptep_clear_flush_young_notify(vma, address, pte))
 +			continue;
 +
 +		/* Nuke the page table entry. */
 +		flush_cache_page(vma, address, pte_pfn(*pte));
 +		pteval = ptep_clear_flush_notify(vma, address, pte);
 +
 +		/* If nonlinear, store the file page offset in the pte. */
 +		if (page->index != linear_page_index(vma, address)) {
 +			pte_t ptfile = pgoff_to_pte(page->index);
 +			if (pte_soft_dirty(pteval))
 +				ptfile = pte_file_mksoft_dirty(ptfile);
 +			set_pte_at(mm, address, pte, ptfile);
 +		}
 +
 +		/* Move the dirty bit to the physical page now the pte is gone. */
 +		if (pte_dirty(pteval))
 +			set_page_dirty(page);
 +
 +		page_remove_rmap(page);
 +		page_cache_release(page);
 +		dec_mm_counter(mm, mm_counter_file(page));
 +		(*mapcount)--;
 +	}
 +	pte_unmap_unlock(pte - 1, ptl);
 +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 +	if (locked_vma)
 +		up_read(&vma->vm_mm->mmap_sem);
 +	return ret;
  }
  
  bool is_vma_temporary_stack(struct vm_area_struct *vma)
* Unmerged path Documentation/vm/unevictable-lru.txt
* Unmerged path mm/rmap.c
