cpuset: add cs->effective_cpus and cs->effective_mems

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Li Zefan <lizefan@huawei.com>
commit e2b9a3d7d8f4ab2f3491b8ed2ac6af692a2269b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e2b9a3d7.failed

We're going to have separate user-configured masks and effective ones.

Eventually configured masks can only be changed by writing cpuset.cpus
and cpuset.mems, and they won't be restricted by parent cpuset. While
effective masks reflect cpu/memory hotplug and hierachical restriction,
and these are the real masks that apply to the tasks in the cpuset.

We calculate effective mask this way:
  - top cpuset's effective_mask == online_mask, otherwise
  - cpuset's effective_mask == configured_mask & parent effective_mask,
    if the result is empty, it inherits parent effective mask.

Those behavior changes are for default hierarchy only. For legacy
hierachy, effective_mask and configured_mask are the same, so we won't
break old interfaces.

This patch adds the effective masks to struct cpuset and initializes
them. The effective masks of the top cpuset is the same with configured
masks, and a child cpuset inherits its parent's effective masks.

This won't introduce behavior change.

v2:
- s/real_{mems,cpus}_allowed/effective_{mems,cpus}, suggested by Tejun.
- don't init effective masks in cpuset_css_online() if !cgroup_on_dfl.

	Signed-off-by: Li Zefan <lizefan@huawei.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit e2b9a3d7d8f4ab2f3491b8ed2ac6af692a2269b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cpuset.c
diff --cc kernel/cpuset.c
index 73b142e72050,ef0974c73b4b..000000000000
--- a/kernel/cpuset.c
+++ b/kernel/cpuset.c
@@@ -1929,14 -1876,20 +1945,20 @@@ static struct cgroup_subsys_state *cpus
  	cs->relax_domain_level = -1;
  
  	return &cs->css;
+ 
+ free_cpus:
+ 	free_cpumask_var(cs->cpus_allowed);
+ free_cs:
+ 	kfree(cs);
+ 	return ERR_PTR(-ENOMEM);
  }
  
 -static int cpuset_css_online(struct cgroup_subsys_state *css)
 +static int cpuset_css_online(struct cgroup *cgrp)
  {
 -	struct cpuset *cs = css_cs(css);
 +	struct cpuset *cs = cgroup_cs(cgrp);
  	struct cpuset *parent = parent_cs(cs);
  	struct cpuset *tmp_cs;
 -	struct cgroup_subsys_state *pos_css;
 +	struct cgroup *pos_cg;
  
  	if (!parent)
  		return 0;
@@@ -1949,9 -1902,16 +1971,20 @@@
  	if (is_spread_slab(parent))
  		set_bit(CS_SPREAD_SLAB, &cs->flags);
  
 -	cpuset_inc();
 +	number_of_cpusets++;
  
++<<<<<<< HEAD
 +	if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags))
++=======
+ 	mutex_lock(&callback_mutex);
+ 	if (cgroup_on_dfl(cs->css.cgroup)) {
+ 		cpumask_copy(cs->effective_cpus, parent->effective_cpus);
+ 		cs->effective_mems = parent->effective_mems;
+ 	}
+ 	mutex_unlock(&callback_mutex);
+ 
+ 	if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))
++>>>>>>> e2b9a3d7d8f4 (cpuset: add cs->effective_cpus and cs->effective_mems)
  		goto out_unlock;
  
  	/*
@@@ -2000,16 -1966,11 +2033,17 @@@ static void cpuset_css_offline(struct c
  	mutex_unlock(&cpuset_mutex);
  }
  
 -static void cpuset_css_free(struct cgroup_subsys_state *css)
 +/*
 + * If the cpuset being removed has its flag 'sched_load_balance'
 + * enabled, then simulate turning sched_load_balance off, which
 + * will call rebuild_sched_domains_locked().
 + */
 +
 +static void cpuset_css_free(struct cgroup *cgrp)
  {
 -	struct cpuset *cs = css_cs(css);
 +	struct cpuset *cs = cgroup_cs(cgrp);
  
+ 	free_cpumask_var(cs->effective_cpus);
  	free_cpumask_var(cs->cpus_allowed);
  	kfree(cs);
  }
* Unmerged path kernel/cpuset.c
