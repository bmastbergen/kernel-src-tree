target: Convert se_tpg->acl_node_lock to ->acl_node_mutex

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [target] Convert se_tpg->acl_node_lock to ->acl_node_mutex (Maurizio Lombardi) [1366062]
Rebuild_FUZZ: 92.45%
commit-author Nicholas Bellinger <nab@linux-iscsi.org>
commit 403edd78a2851ef95b24c0bf5151a4ab640898d7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/403edd78.failed

This patch converts se_tpg->acl_node_lock to struct mutex, so that
->acl_node_acl walkers in core_clear_lun_from_tpg() can block when
calling core_disable_device_list_for_node().

It also updates core_dev_add_lun() to hold ->acl_node_mutex when
calling core_tpg_add_node_to_devs() to build ->lun_entry_hlist
for dynamically generated se_node_acl.

	Reviewed-by: Hannes Reinecke <hare@suse.de>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Sagi Grimberg <sagig@mellanox.com>
	Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>
(cherry picked from commit 403edd78a2851ef95b24c0bf5151a4ab640898d7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/target/target_core_device.c
#	drivers/target/target_core_tpg.c
#	drivers/target/target_core_transport.c
diff --cc drivers/target/target_core_device.c
index 53dcefb982bc,1593d4965848..000000000000
--- a/drivers/target/target_core_device.c
+++ b/drivers/target/target_core_device.c
@@@ -431,30 -439,23 +431,33 @@@ void core_clear_lun_from_tpg(struct se_
  {
  	struct se_node_acl *nacl;
  	struct se_dev_entry *deve;
 +	u32 i;
  
- 	spin_lock_irq(&tpg->acl_node_lock);
+ 	mutex_lock(&tpg->acl_node_mutex);
  	list_for_each_entry(nacl, &tpg->acl_node_list, acl_list) {
- 		spin_unlock_irq(&tpg->acl_node_lock);
  
 -		mutex_lock(&nacl->lun_entry_mutex);
 -		hlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link) {
 -			struct se_lun *tmp_lun = rcu_dereference_check(deve->se_lun,
 -					lockdep_is_held(&nacl->lun_entry_mutex));
 -
 -			if (lun != tmp_lun)
 +		spin_lock_irq(&nacl->device_list_lock);
 +		for (i = 0; i < TRANSPORT_MAX_LUNS_PER_TPG; i++) {
 +			deve = nacl->device_list[i];
 +			if (lun != deve->se_lun)
  				continue;
 +			spin_unlock_irq(&nacl->device_list_lock);
 +
 +			core_disable_device_list_for_node(lun, NULL,
 +				deve->mapped_lun, TRANSPORT_LUNFLAGS_NO_ACCESS,
 +				nacl, tpg);
  
 -			core_disable_device_list_for_node(lun, deve, nacl, tpg);
 +			spin_lock_irq(&nacl->device_list_lock);
  		}
++<<<<<<< HEAD
 +		spin_unlock_irq(&nacl->device_list_lock);
 +
 +		spin_lock_irq(&tpg->acl_node_lock);
++=======
+ 		mutex_unlock(&nacl->lun_entry_mutex);
++>>>>>>> 403edd78a285 (target: Convert se_tpg->acl_node_lock to ->acl_node_mutex)
  	}
- 	spin_unlock_irq(&tpg->acl_node_lock);
+ 	mutex_unlock(&tpg->acl_node_mutex);
  }
  
  static struct se_port *core_alloc_port(struct se_device *dev)
@@@ -1215,15 -1197,13 +1219,13 @@@ struct se_lun *core_dev_add_lun
  			if (acl->dynamic_node_acl &&
  			    (!tpg->se_tpg_tfo->tpg_check_demo_mode_login_only ||
  			     !tpg->se_tpg_tfo->tpg_check_demo_mode_login_only(tpg))) {
- 				spin_unlock_irq(&tpg->acl_node_lock);
  				core_tpg_add_node_to_devs(acl, tpg);
- 				spin_lock_irq(&tpg->acl_node_lock);
  			}
  		}
- 		spin_unlock_irq(&tpg->acl_node_lock);
+ 		mutex_unlock(&tpg->acl_node_mutex);
  	}
  
 -	return 0;
 +	return lun;
  }
  
  /*      core_dev_del_lun():
diff --cc drivers/target/target_core_tpg.c
index 0696de9553d3,229e8278f4fe..000000000000
--- a/drivers/target/target_core_tpg.c
+++ b/drivers/target/target_core_tpg.c
@@@ -47,45 -47,9 +47,45 @@@ extern struct se_device *g_lun0_dev
  static DEFINE_SPINLOCK(tpg_lock);
  static LIST_HEAD(tpg_list);
  
 +/*	core_clear_initiator_node_from_tpg():
 + *
 + *
 + */
 +static void core_clear_initiator_node_from_tpg(
 +	struct se_node_acl *nacl,
 +	struct se_portal_group *tpg)
 +{
 +	int i;
 +	struct se_dev_entry *deve;
 +	struct se_lun *lun;
 +
 +	spin_lock_irq(&nacl->device_list_lock);
 +	for (i = 0; i < TRANSPORT_MAX_LUNS_PER_TPG; i++) {
 +		deve = nacl->device_list[i];
 +
 +		if (!(deve->lun_flags & TRANSPORT_LUNFLAGS_INITIATOR_ACCESS))
 +			continue;
 +
 +		if (!deve->se_lun) {
 +			pr_err("%s device entries device pointer is"
 +				" NULL, but Initiator has access.\n",
 +				tpg->se_tpg_tfo->get_fabric_name());
 +			continue;
 +		}
 +
 +		lun = deve->se_lun;
 +		spin_unlock_irq(&nacl->device_list_lock);
 +		core_disable_device_list_for_node(lun, NULL, deve->mapped_lun,
 +			TRANSPORT_LUNFLAGS_NO_ACCESS, nacl, tpg);
 +
 +		spin_lock_irq(&nacl->device_list_lock);
 +	}
 +	spin_unlock_irq(&nacl->device_list_lock);
 +}
 +
  /*	__core_tpg_get_initiator_node_acl():
   *
-  *	spin_lock_bh(&tpg->acl_node_lock); must be held when calling
+  *	mutex_lock(&tpg->acl_node_mutex); must be held when calling
   */
  struct se_node_acl *__core_tpg_get_initiator_node_acl(
  	struct se_portal_group *tpg,
@@@ -197,67 -156,63 +197,74 @@@ static int core_set_queue_depth_for_nod
  	return 0;
  }
  
 -static struct se_node_acl *target_alloc_node_acl(struct se_portal_group *tpg,
 -		const unsigned char *initiatorname)
 +void array_free(void *array, int n)
  {
 -	struct se_node_acl *acl;
 -
 -	acl = kzalloc(max(sizeof(*acl), tpg->se_tpg_tfo->node_acl_size),
 -			GFP_KERNEL);
 -	if (!acl)
 -		return NULL;
 -
 -	INIT_LIST_HEAD(&acl->acl_list);
 -	INIT_LIST_HEAD(&acl->acl_sess_list);
 -	INIT_HLIST_HEAD(&acl->lun_entry_hlist);
 -	kref_init(&acl->acl_kref);
 -	init_completion(&acl->acl_free_comp);
 -	spin_lock_init(&acl->nacl_sess_lock);
 -	mutex_init(&acl->lun_entry_mutex);
 -	atomic_set(&acl->acl_pr_ref_count, 0);
 -	if (tpg->se_tpg_tfo->tpg_get_default_depth)
 -		acl->queue_depth = tpg->se_tpg_tfo->tpg_get_default_depth(tpg);
 -	else
 -		acl->queue_depth = 1;
 -	snprintf(acl->initiatorname, TRANSPORT_IQN_LEN, "%s", initiatorname);
 -	acl->se_tpg = tpg;
 -	acl->acl_index = scsi_get_new_index(SCSI_AUTH_INTR_INDEX);
 -
 -	tpg->se_tpg_tfo->set_default_node_attributes(acl);
 +	void **a = array;
 +	int i;
  
 -	if (core_set_queue_depth_for_node(tpg, acl) < 0)
 -		goto out_free_acl;
 +	for (i = 0; i < n; i++)
 +		kfree(a[i]);
 +	kfree(a);
 +}
  
 -	return acl;
 +static void *array_zalloc(int n, size_t size, gfp_t flags)
 +{
 +	void **a;
 +	int i;
  
 -out_free_acl:
 -	kfree(acl);
 -	return NULL;
 +	a = kzalloc(n * sizeof(void*), flags);
 +	if (!a)
 +		return NULL;
 +	for (i = 0; i < n; i++) {
 +		a[i] = kzalloc(size, flags);
 +		if (!a[i]) {
 +			array_free(a, n);
 +			return NULL;
 +		}
 +	}
 +	return a;
  }
  
 -static void target_add_node_acl(struct se_node_acl *acl)
 +/*      core_create_device_list_for_node():
 + *
 + *
 + */
 +static int core_create_device_list_for_node(struct se_node_acl *nacl)
  {
 -	struct se_portal_group *tpg = acl->se_tpg;
 -
 +	struct se_dev_entry *deve;
 +	int i;
 +
++<<<<<<< HEAD
 +	nacl->device_list = array_zalloc(TRANSPORT_MAX_LUNS_PER_TPG,
 +			sizeof(struct se_dev_entry), GFP_KERNEL);
 +	if (!nacl->device_list) {
 +		pr_err("Unable to allocate memory for"
 +			" struct se_node_acl->device_list\n");
 +		return -ENOMEM;
 +	}
 +	for (i = 0; i < TRANSPORT_MAX_LUNS_PER_TPG; i++) {
 +		deve = nacl->device_list[i];
++=======
+ 	mutex_lock(&tpg->acl_node_mutex);
+ 	list_add_tail(&acl->acl_list, &tpg->acl_node_list);
+ 	tpg->num_node_acls++;
+ 	mutex_unlock(&tpg->acl_node_mutex);
++>>>>>>> 403edd78a285 (target: Convert se_tpg->acl_node_lock to ->acl_node_mutex)
  
 -	pr_debug("%s_TPG[%hu] - Added %s ACL with TCQ Depth: %d for %s"
 -		" Initiator Node: %s\n",
 -		tpg->se_tpg_tfo->get_fabric_name(),
 -		tpg->se_tpg_tfo->tpg_get_tag(tpg),
 -		acl->dynamic_node_acl ? "DYNAMIC" : "",
 -		acl->queue_depth,
 -		tpg->se_tpg_tfo->get_fabric_name(),
 -		acl->initiatorname);
 +		atomic_set(&deve->ua_count, 0);
 +		atomic_set(&deve->pr_ref_count, 0);
 +		spin_lock_init(&deve->ua_lock);
 +		INIT_LIST_HEAD(&deve->alua_port_list);
 +		INIT_LIST_HEAD(&deve->ua_list);
 +	}
 +
 +	return 0;
  }
  
 +/*	core_tpg_check_initiator_node_acl()
 + *
 + *
 + */
  struct se_node_acl *core_tpg_check_initiator_node_acl(
  	struct se_portal_group *tpg,
  	unsigned char *initiatorname)
@@@ -329,40 -251,13 +336,40 @@@ void core_tpg_wait_for_nacl_pr_ref(stru
  		cpu_relax();
  }
  
 +void core_tpg_clear_object_luns(struct se_portal_group *tpg)
 +{
 +	int i;
 +	struct se_lun *lun;
 +
 +	spin_lock(&tpg->tpg_lun_lock);
 +	for (i = 0; i < TRANSPORT_MAX_LUNS_PER_TPG; i++) {
 +		lun = tpg->tpg_lun_list[i];
 +
 +		if ((lun->lun_status != TRANSPORT_LUN_STATUS_ACTIVE) ||
 +		    (lun->lun_se_dev == NULL))
 +			continue;
 +
 +		spin_unlock(&tpg->tpg_lun_lock);
 +		core_dev_del_lun(tpg, lun);
 +		spin_lock(&tpg->tpg_lun_lock);
 +	}
 +	spin_unlock(&tpg->tpg_lun_lock);
 +}
 +EXPORT_SYMBOL(core_tpg_clear_object_luns);
 +
 +/*	core_tpg_add_initiator_node_acl():
 + *
 + *
 + */
  struct se_node_acl *core_tpg_add_initiator_node_acl(
  	struct se_portal_group *tpg,
 -	const char *initiatorname)
 +	struct se_node_acl *se_nacl,
 +	const char *initiatorname,
 +	u32 queue_depth)
  {
 -	struct se_node_acl *acl;
 +	struct se_node_acl *acl = NULL;
  
- 	spin_lock_irq(&tpg->acl_node_lock);
+ 	mutex_lock(&tpg->acl_node_mutex);
  	acl = __core_tpg_get_initiator_node_acl(tpg, initiatorname);
  	if (acl) {
  		if (acl->dynamic_node_acl) {
@@@ -370,87 -265,30 +377,92 @@@
  			pr_debug("%s_TPG[%u] - Replacing dynamic ACL"
  				" for %s\n", tpg->se_tpg_tfo->get_fabric_name(),
  				tpg->se_tpg_tfo->tpg_get_tag(tpg), initiatorname);
++<<<<<<< HEAD
 +			spin_unlock_irq(&tpg->acl_node_lock);
 +			/*
 +			 * Release the locally allocated struct se_node_acl
 +			 * because * core_tpg_add_initiator_node_acl() returned
 +			 * a pointer to an existing demo mode node ACL.
 +			 */
 +			if (se_nacl)
 +				tpg->se_tpg_tfo->tpg_release_fabric_acl(tpg,
 +							se_nacl);
 +			goto done;
++=======
+ 			mutex_unlock(&tpg->acl_node_mutex);
+ 			return acl;
++>>>>>>> 403edd78a285 (target: Convert se_tpg->acl_node_lock to ->acl_node_mutex)
  		}
  
  		pr_err("ACL entry for %s Initiator"
  			" Node %s already exists for TPG %u, ignoring"
  			" request.\n",  tpg->se_tpg_tfo->get_fabric_name(),
  			initiatorname, tpg->se_tpg_tfo->tpg_get_tag(tpg));
- 		spin_unlock_irq(&tpg->acl_node_lock);
+ 		mutex_unlock(&tpg->acl_node_mutex);
  		return ERR_PTR(-EEXIST);
  	}
- 	spin_unlock_irq(&tpg->acl_node_lock);
+ 	mutex_unlock(&tpg->acl_node_mutex);
  
 -	acl = target_alloc_node_acl(tpg, initiatorname);
 -	if (!acl)
 +	if (!se_nacl) {
 +		pr_err("struct se_node_acl pointer is NULL\n");
 +		return ERR_PTR(-EINVAL);
 +	}
 +	/*
 +	 * For v4.x logic the se_node_acl_s is hanging off a fabric
 +	 * dependent structure allocated via
 +	 * struct target_core_fabric_ops->fabric_make_nodeacl()
 +	 */
 +	acl = se_nacl;
 +
 +	INIT_LIST_HEAD(&acl->acl_list);
 +	INIT_LIST_HEAD(&acl->acl_sess_list);
 +	kref_init(&acl->acl_kref);
 +	init_completion(&acl->acl_free_comp);
 +	spin_lock_init(&acl->device_list_lock);
 +	spin_lock_init(&acl->nacl_sess_lock);
 +	atomic_set(&acl->acl_pr_ref_count, 0);
 +	acl->queue_depth = queue_depth;
 +	snprintf(acl->initiatorname, TRANSPORT_IQN_LEN, "%s", initiatorname);
 +	acl->se_tpg = tpg;
 +	acl->acl_index = scsi_get_new_index(SCSI_AUTH_INTR_INDEX);
 +
 +	tpg->se_tpg_tfo->set_default_node_attributes(acl);
 +
 +	if (core_create_device_list_for_node(acl) < 0) {
 +		tpg->se_tpg_tfo->tpg_release_fabric_acl(tpg, acl);
  		return ERR_PTR(-ENOMEM);
 +	}
 +
 +	if (core_set_queue_depth_for_node(tpg, acl) < 0) {
 +		core_free_device_list_for_node(acl, tpg);
 +		tpg->se_tpg_tfo->tpg_release_fabric_acl(tpg, acl);
 +		return ERR_PTR(-EINVAL);
 +	}
 +
 +	spin_lock_irq(&tpg->acl_node_lock);
 +	list_add_tail(&acl->acl_list, &tpg->acl_node_list);
 +	tpg->num_node_acls++;
 +	spin_unlock_irq(&tpg->acl_node_lock);
 +
 +done:
 +	pr_debug("%s_TPG[%hu] - Added ACL with TCQ Depth: %d for %s"
 +		" Initiator Node: %s\n", tpg->se_tpg_tfo->get_fabric_name(),
 +		tpg->se_tpg_tfo->tpg_get_tag(tpg), acl->queue_depth,
 +		tpg->se_tpg_tfo->get_fabric_name(), initiatorname);
  
 -	target_add_node_acl(acl);
  	return acl;
  }
 +EXPORT_SYMBOL(core_tpg_add_initiator_node_acl);
  
 -void core_tpg_del_initiator_node_acl(struct se_node_acl *acl)
 +/*	core_tpg_del_initiator_node_acl():
 + *
 + *
 + */
 +int core_tpg_del_initiator_node_acl(
 +	struct se_portal_group *tpg,
 +	struct se_node_acl *acl,
 +	int force)
  {
 -	struct se_portal_group *tpg = acl->se_tpg;
  	LIST_HEAD(sess_list);
  	struct se_session *sess, *sess_tmp;
  	unsigned long flags;
@@@ -710,16 -521,13 +722,20 @@@ int core_tpg_register
  	INIT_LIST_HEAD(&se_tpg->acl_node_list);
  	INIT_LIST_HEAD(&se_tpg->se_tpg_node);
  	INIT_LIST_HEAD(&se_tpg->tpg_sess_list);
- 	spin_lock_init(&se_tpg->acl_node_lock);
  	spin_lock_init(&se_tpg->session_lock);
++<<<<<<< HEAD
 +	spin_lock_init(&se_tpg->tpg_lun_lock);
++=======
+ 	mutex_init(&se_tpg->tpg_lun_mutex);
+ 	mutex_init(&se_tpg->acl_node_mutex);
++>>>>>>> 403edd78a285 (target: Convert se_tpg->acl_node_lock to ->acl_node_mutex)
  
 -	if (se_tpg->proto_id >= 0) {
 -		if (core_tpg_setup_virtual_lun0(se_tpg) < 0)
 +	if (se_tpg->se_tpg_type == TRANSPORT_TPG_TYPE_NORMAL) {
 +		if (core_tpg_setup_virtual_lun0(se_tpg) < 0) {
 +			array_free(se_tpg->tpg_lun_list,
 +				   TRANSPORT_MAX_LUNS_PER_TPG);
  			return -ENOMEM;
 +		}
  	}
  
  	spin_lock_bh(&tpg_lock);
@@@ -767,17 -574,15 +784,17 @@@ int core_tpg_deregister(struct se_porta
  
  		core_tpg_wait_for_nacl_pr_ref(nacl);
  		core_free_device_list_for_node(nacl, se_tpg);
 -		kfree(nacl);
 +		se_tpg->se_tpg_tfo->tpg_release_fabric_acl(se_tpg, nacl);
  
- 		spin_lock_irq(&se_tpg->acl_node_lock);
+ 		mutex_lock(&se_tpg->acl_node_mutex);
  	}
- 	spin_unlock_irq(&se_tpg->acl_node_lock);
+ 	mutex_unlock(&se_tpg->acl_node_mutex);
  
 -	if (se_tpg->proto_id >= 0)
 +	if (se_tpg->se_tpg_type == TRANSPORT_TPG_TYPE_NORMAL)
  		core_tpg_remove_lun(se_tpg, &se_tpg->tpg_virt_lun0);
  
 +	se_tpg->se_tpg_fabric_ptr = NULL;
 +	array_free(se_tpg->tpg_lun_list, TRANSPORT_MAX_LUNS_PER_TPG);
  	return 0;
  }
  EXPORT_SYMBOL(core_tpg_deregister);
diff --cc drivers/target/target_core_transport.c
index 9cc3afa0ef11,965a308e10a5..000000000000
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@@ -496,10 -495,10 +496,10 @@@ EXPORT_SYMBOL(transport_free_session)
  void transport_deregister_session(struct se_session *se_sess)
  {
  	struct se_portal_group *se_tpg = se_sess->se_tpg;
 -	const struct target_core_fabric_ops *se_tfo;
 +	struct target_core_fabric_ops *se_tfo;
  	struct se_node_acl *se_nacl;
  	unsigned long flags;
- 	bool comp_nacl = true;
+ 	bool comp_nacl = true, drop_nacl = false;
  
  	if (!se_tpg) {
  		transport_free_session(se_sess);
@@@ -524,17 -523,17 +524,27 @@@
  		if (!se_tfo->tpg_check_demo_mode_cache(se_tpg)) {
  			list_del(&se_nacl->acl_list);
  			se_tpg->num_node_acls--;
++<<<<<<< HEAD
 +			spin_unlock_irqrestore(&se_tpg->acl_node_lock, flags);
 +			core_tpg_wait_for_nacl_pr_ref(se_nacl);
 +			core_free_device_list_for_node(se_nacl, se_tpg);
 +			se_tfo->tpg_release_fabric_acl(se_tpg, se_nacl);
 +
 +			comp_nacl = false;
 +			spin_lock_irqsave(&se_tpg->acl_node_lock, flags);
++=======
+ 			drop_nacl = true;
++>>>>>>> 403edd78a285 (target: Convert se_tpg->acl_node_lock to ->acl_node_mutex)
  		}
  	}
- 	spin_unlock_irqrestore(&se_tpg->acl_node_lock, flags);
+ 	mutex_unlock(&se_tpg->acl_node_mutex);
  
+ 	if (drop_nacl) {
+ 		core_tpg_wait_for_nacl_pr_ref(se_nacl);
+ 		core_free_device_list_for_node(se_nacl, se_tpg);
+ 		kfree(se_nacl);
+ 		comp_nacl = false;
+ 	}
  	pr_debug("TARGET_CORE[%s]: Deregistered fabric_sess\n",
  		se_tpg->se_tpg_tfo->get_fabric_name());
  	/*
* Unmerged path drivers/target/target_core_device.c
diff --git a/drivers/target/target_core_pr.c b/drivers/target/target_core_pr.c
index a255afc452a9..46cf408ca399 100644
--- a/drivers/target/target_core_pr.c
+++ b/drivers/target/target_core_pr.c
@@ -1585,12 +1585,12 @@ core_scsi3_decode_spec_i_port(
 			 * from the decoded fabric module specific TransportID
 			 * at *i_str.
 			 */
-			spin_lock_irq(&tmp_tpg->acl_node_lock);
+			mutex_lock(&tmp_tpg->acl_node_mutex);
 			dest_node_acl = __core_tpg_get_initiator_node_acl(
 						tmp_tpg, i_str);
 			if (dest_node_acl)
 				atomic_inc_mb(&dest_node_acl->acl_pr_ref_count);
-			spin_unlock_irq(&tmp_tpg->acl_node_lock);
+			mutex_unlock(&tmp_tpg->acl_node_mutex);
 
 			if (!dest_node_acl) {
 				core_scsi3_tpg_undepend_item(tmp_tpg);
@@ -3311,12 +3311,12 @@ after_iport_check:
 	/*
 	 * Locate the destination struct se_node_acl from the received Transport ID
 	 */
-	spin_lock_irq(&dest_se_tpg->acl_node_lock);
+	mutex_lock(&dest_se_tpg->acl_node_mutex);
 	dest_node_acl = __core_tpg_get_initiator_node_acl(dest_se_tpg,
 				initiator_str);
 	if (dest_node_acl)
 		atomic_inc_mb(&dest_node_acl->acl_pr_ref_count);
-	spin_unlock_irq(&dest_se_tpg->acl_node_lock);
+	mutex_unlock(&dest_se_tpg->acl_node_mutex);
 
 	if (!dest_node_acl) {
 		pr_err("Unable to locate %s dest_node_acl for"
* Unmerged path drivers/target/target_core_tpg.c
* Unmerged path drivers/target/target_core_transport.c
diff --git a/drivers/target/tcm_fc/tfc_conf.c b/drivers/target/tcm_fc/tfc_conf.c
index 8f5b91a74d09..0457d374ccf1 100644
--- a/drivers/target/tcm_fc/tfc_conf.c
+++ b/drivers/target/tcm_fc/tfc_conf.c
@@ -249,7 +249,7 @@ struct ft_node_acl *ft_acl_get(struct ft_tpg *tpg, struct fc_rport_priv *rdata)
 	struct se_portal_group *se_tpg = &tpg->se_tpg;
 	struct se_node_acl *se_acl;
 
-	spin_lock_irq(&se_tpg->acl_node_lock);
+	mutex_lock(&se_tpg->acl_node_mutex);
 	list_for_each_entry(se_acl, &se_tpg->acl_node_list, acl_list) {
 		acl = container_of(se_acl, struct ft_node_acl, se_node_acl);
 		pr_debug("acl %p port_name %llx\n",
@@ -263,7 +263,7 @@ struct ft_node_acl *ft_acl_get(struct ft_tpg *tpg, struct fc_rport_priv *rdata)
 			break;
 		}
 	}
-	spin_unlock_irq(&se_tpg->acl_node_lock);
+	mutex_unlock(&se_tpg->acl_node_mutex);
 	return found;
 }
 
diff --git a/include/target/target_core_base.h b/include/target/target_core_base.h
index 7bcf92529421..a70446506b53 100644
--- a/include/target/target_core_base.h
+++ b/include/target/target_core_base.h
@@ -875,7 +875,7 @@ struct se_portal_group {
 	/* Used for PR SPEC_I_PT=1 and REGISTER_AND_MOVE */
 	atomic_t		tpg_pr_ref_count;
 	/* Spinlock for adding/removing ACLed Nodes */
-	spinlock_t		acl_node_lock;
+	struct mutex		acl_node_mutex;
 	/* Spinlock for adding/removing sessions */
 	spinlock_t		session_lock;
 	spinlock_t		tpg_lun_lock;
