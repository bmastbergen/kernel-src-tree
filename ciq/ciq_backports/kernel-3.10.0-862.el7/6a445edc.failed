x86/intel_rdt/cqm: Add RDT monitoring initialization

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] intel_rdt/cqm: Add RDT monitoring initialization (Jiri Olsa) [1457533]
Rebuild_FUZZ: 96.00%
commit-author Vikas Shivappa <vikas.shivappa@linux.intel.com>
commit 6a445edce657810992594c7b9e679219aaf78ad9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6a445edc.failed

Add common data structures for RDT resource monitoring and perform RDT
monitoring related data structure initializations which include setting
up the RMID(Resource monitoring ID) lists and event list which the
resource supports.

[ tony: some cleanup to make adding MBM easier later, remove "cqm" from
  	some names, make some data structure local to intel_rdt_monitor.c
  	static. Add copyright header]

[ tglx: Made it readable ]

	Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: ravi.v.shankar@intel.com
	Cc: fenghua.yu@intel.com
	Cc: peterz@infradead.org
	Cc: eranian@google.com
	Cc: vikas.shivappa@intel.com
	Cc: ak@linux.intel.com
	Cc: davidcc@google.com
	Cc: reinette.chatre@intel.com
Link: http://lkml.kernel.org/r/1501017287-28083-9-git-send-email-vikas.shivappa@linux.intel.com

(cherry picked from commit 6a445edce657810992594c7b9e679219aaf78ad9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/intel_rdt.h
#	arch/x86/kernel/cpu/Makefile
#	arch/x86/kernel/cpu/intel_rdt.c
diff --cc arch/x86/include/asm/intel_rdt.h
index 06f50d0ed14f,993ab9d678bc..000000000000
--- a/arch/x86/include/asm/intel_rdt.h
+++ b/arch/x86/include/asm/intel_rdt.h
@@@ -136,6 -123,98 +159,101 @@@ struct msr_param 
  	int			high;
  };
  
++<<<<<<< HEAD:arch/x86/include/asm/intel_rdt.h
++=======
+ /**
+  * struct rdt_cache - Cache allocation related data
+  * @cbm_len:		Length of the cache bit mask
+  * @min_cbm_bits:	Minimum number of consecutive bits to be set
+  * @cbm_idx_mult:	Multiplier of CBM index
+  * @cbm_idx_offset:	Offset of CBM index. CBM index is computed by:
+  *			closid * cbm_idx_multi + cbm_idx_offset
+  *			in a cache bit mask
+  */
+ struct rdt_cache {
+ 	unsigned int	cbm_len;
+ 	unsigned int	min_cbm_bits;
+ 	unsigned int	cbm_idx_mult;
+ 	unsigned int	cbm_idx_offset;
+ };
+ 
+ /**
+  * struct rdt_membw - Memory bandwidth allocation related data
+  * @max_delay:		Max throttle delay. Delay is the hardware
+  *			representation for memory bandwidth.
+  * @min_bw:		Minimum memory bandwidth percentage user can request
+  * @bw_gran:		Granularity at which the memory bandwidth is allocated
+  * @delay_linear:	True if memory B/W delay is in linear scale
+  * @mb_map:		Mapping of memory B/W percentage to memory B/W delay
+  */
+ struct rdt_membw {
+ 	u32		max_delay;
+ 	u32		min_bw;
+ 	u32		bw_gran;
+ 	u32		delay_linear;
+ 	u32		*mb_map;
+ };
+ 
+ static inline bool is_llc_occupancy_enabled(void)
+ {
+ 	return (rdt_mon_features & (1 << QOS_L3_OCCUP_EVENT_ID));
+ }
+ 
+ /**
+  * struct rdt_resource - attributes of an RDT resource
+  * @alloc_enabled:	Is allocation enabled on this machine
+  * @mon_enabled:		Is monitoring enabled for this feature
+  * @alloc_capable:	Is allocation available on this machine
+  * @mon_capable:		Is monitor feature available on this machine
+  * @name:		Name to use in "schemata" file
+  * @num_closid:		Number of CLOSIDs available
+  * @cache_level:	Which cache level defines scope of this resource
+  * @default_ctrl:	Specifies default cache cbm or memory B/W percent.
+  * @msr_base:		Base MSR address for CBMs
+  * @msr_update:		Function pointer to update QOS MSRs
+  * @data_width:		Character width of data when displaying
+  * @domains:		All domains for this resource
+  * @cache:		Cache allocation related data
+  * @info_files:		resctrl info files for the resource
+  * @nr_info_files:	Number of info files
+  * @format_str:		Per resource format string to show domain value
+  * @parse_ctrlval:	Per resource function pointer to parse control values
+  * @evt_list:			List of monitoring events
+  * @num_rmid:			Number of RMIDs available
+  * @mon_scale:			cqm counter * mon_scale = occupancy in bytes
+  */
+ struct rdt_resource {
+ 	bool			alloc_enabled;
+ 	bool			mon_enabled;
+ 	bool			alloc_capable;
+ 	bool			mon_capable;
+ 	char			*name;
+ 	int			num_closid;
+ 	int			cache_level;
+ 	u32			default_ctrl;
+ 	unsigned int		msr_base;
+ 	void (*msr_update)	(struct rdt_domain *d, struct msr_param *m,
+ 				 struct rdt_resource *r);
+ 	int			data_width;
+ 	struct list_head	domains;
+ 	struct rdt_cache	cache;
+ 	struct rdt_membw	membw;
+ 	struct rftype		*info_files;
+ 	int			nr_info_files;
+ 	const char		*format_str;
+ 	int (*parse_ctrlval)	(char *buf, struct rdt_resource *r,
+ 				 struct rdt_domain *d);
+ 	struct list_head	evt_list;
+ 	int			num_rmid;
+ 	unsigned int		mon_scale;
+ };
+ 
+ void rdt_get_cache_infofile(struct rdt_resource *r);
+ void rdt_get_mba_infofile(struct rdt_resource *r);
+ int parse_cbm(char *buf, struct rdt_resource *r, struct rdt_domain *d);
+ int parse_bw(char *buf, struct rdt_resource *r,  struct rdt_domain *d);
+ 
++>>>>>>> 6a445edce657 (x86/intel_rdt/cqm: Add RDT monitoring initialization):arch/x86/kernel/cpu/intel_rdt.h
  extern struct mutex rdtgroup_mutex;
  
  extern struct rdt_resource rdt_resources_all[];
@@@ -154,15 -234,20 +272,24 @@@ enum 
  	RDT_NUM_RESOURCES,
  };
  
 -#define for_each_alloc_capable_rdt_resource(r)				      \
 +#define for_each_capable_rdt_resource(r)				      \
  	for (r = rdt_resources_all; r < rdt_resources_all + RDT_NUM_RESOURCES;\
  	     r++)							      \
 -		if (r->alloc_capable)
 +		if (r->capable)
  
++<<<<<<< HEAD:arch/x86/include/asm/intel_rdt.h
 +#define for_each_enabled_rdt_resource(r)				      \
++=======
+ #define for_each_mon_capable_rdt_resource(r)				      \
+ 	for (r = rdt_resources_all; r < rdt_resources_all + RDT_NUM_RESOURCES;\
+ 	     r++)							      \
+ 		if (r->mon_capable)
+ 
+ #define for_each_alloc_enabled_rdt_resource(r)				      \
++>>>>>>> 6a445edce657 (x86/intel_rdt/cqm: Add RDT monitoring initialization):arch/x86/kernel/cpu/intel_rdt.h
  	for (r = rdt_resources_all; r < rdt_resources_all + RDT_NUM_RESOURCES;\
  	     r++)							      \
 -		if (r->alloc_enabled)
 +		if (r->enabled)
  
  /* CPUID.(EAX=10H, ECX=ResID=1).EAX */
  union cpuid_0x10_1_eax {
@@@ -189,44 -282,6 +316,45 @@@ ssize_t rdtgroup_schemata_write(struct 
  				char *buf, size_t nbytes, loff_t off);
  int rdtgroup_schemata_show(struct kernfs_open_file *of,
  			   struct seq_file *s, void *v);
+ int rdt_get_mon_l3_config(struct rdt_resource *r);
  
 +/*
 + * intel_rdt_sched_in() - Writes the task's CLOSid to IA32_PQR_MSR
 + *
 + * Following considerations are made so that this has minimal impact
 + * on scheduler hot path:
 + * - This will stay as no-op unless we are running on an Intel SKU
 + *   which supports resource control and we enable by mounting the
 + *   resctrl file system.
 + * - Caches the per cpu CLOSid values and does the MSR write only
 + *   when a task with a different CLOSid is scheduled in.
 + *
 + * Must be called with preemption disabled.
 + */
 +static inline void intel_rdt_sched_in(void)
 +{
 +	if (static_key_false(&rdt_enable_key)) {
 +		struct intel_pqr_state *state = this_cpu_ptr(&pqr_state);
 +		int closid;
 +
 +		/*
 +		 * If this task has a closid assigned, use it.
 +		 * Else use the closid assigned to this cpu.
 +		 */
 +		closid = current->closid;
 +		if (closid == 0)
 +			closid = this_cpu_read(cpu_closid);
 +
 +		if (closid != state->closid) {
 +			state->closid = closid;
 +			wrmsr(MSR_IA32_PQR_ASSOC, state->rmid, closid);
 +		}
 +	}
 +}
 +
 +#else
 +
 +static inline void intel_rdt_sched_in(void) {}
 +
 +#endif /* CONFIG_INTEL_RDT_A */
  #endif /* _ASM_X86_INTEL_RDT_H */
diff --cc arch/x86/kernel/cpu/Makefile
index 21c1e451a160,7584be0750c4..000000000000
--- a/arch/x86/kernel/cpu/Makefile
+++ b/arch/x86/kernel/cpu/Makefile
@@@ -27,7 -33,7 +27,11 @@@ obj-$(CONFIG_CPU_SUP_CENTAUR)		+= centa
  obj-$(CONFIG_CPU_SUP_TRANSMETA_32)	+= transmeta.o
  obj-$(CONFIG_CPU_SUP_UMC_32)		+= umc.o
  
++<<<<<<< HEAD
 +obj-$(CONFIG_INTEL_RDT_A)	+= intel_rdt.o intel_rdt_rdtgroup.o intel_rdt_schemata.o
++=======
+ obj-$(CONFIG_INTEL_RDT)	+= intel_rdt.o intel_rdt_rdtgroup.o intel_rdt_schemata.o intel_rdt_monitor.o
++>>>>>>> 6a445edce657 (x86/intel_rdt/cqm: Add RDT monitoring initialization)
  
  obj-$(CONFIG_X86_MCE)			+= mcheck/
  obj-$(CONFIG_MTRR)			+= mtrr/
diff --cc arch/x86/kernel/cpu/intel_rdt.c
index ad087dd4421e,cb520dd73bc8..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt.c
+++ b/arch/x86/kernel/cpu/intel_rdt.c
@@@ -45,42 -55,89 +45,58 @@@ DEFINE_PER_CPU_READ_MOSTLY(int, cpu_clo
   */
  int max_name_width, max_data_width;
  
++<<<<<<< HEAD
++=======
+ /*
+  * Global boolean for rdt_alloc which is true if any
+  * resource allocation is enabled.
+  */
+ bool rdt_alloc_capable;
+ 
+ static void
+ mba_wrmsr(struct rdt_domain *d, struct msr_param *m, struct rdt_resource *r);
+ static void
+ cat_wrmsr(struct rdt_domain *d, struct msr_param *m, struct rdt_resource *r);
+ 
+ #define domain_init(id) LIST_HEAD_INIT(rdt_resources_all[id].domains)
+ 
++>>>>>>> 6a445edce657 (x86/intel_rdt/cqm: Add RDT monitoring initialization)
  struct rdt_resource rdt_resources_all[] = {
 -	[RDT_RESOURCE_L3] =
  	{
 -		.name			= "L3",
 -		.domains		= domain_init(RDT_RESOURCE_L3),
 -		.msr_base		= IA32_L3_CBM_BASE,
 -		.msr_update		= cat_wrmsr,
 -		.cache_level		= 3,
 -		.cache = {
 -			.min_cbm_bits	= 1,
 -			.cbm_idx_mult	= 1,
 -			.cbm_idx_offset	= 0,
 -		},
 -		.parse_ctrlval		= parse_cbm,
 -		.format_str		= "%d=%0*x",
 +		.name		= "L3",
 +		.domains	= domain_init(RDT_RESOURCE_L3),
 +		.msr_base	= IA32_L3_CBM_BASE,
 +		.min_cbm_bits	= 1,
 +		.cache_level	= 3,
 +		.cbm_idx_multi	= 1,
 +		.cbm_idx_offset	= 0
  	},
 -	[RDT_RESOURCE_L3DATA] =
  	{
 -		.name			= "L3DATA",
 -		.domains		= domain_init(RDT_RESOURCE_L3DATA),
 -		.msr_base		= IA32_L3_CBM_BASE,
 -		.msr_update		= cat_wrmsr,
 -		.cache_level		= 3,
 -		.cache = {
 -			.min_cbm_bits	= 1,
 -			.cbm_idx_mult	= 2,
 -			.cbm_idx_offset	= 0,
 -		},
 -		.parse_ctrlval		= parse_cbm,
 -		.format_str		= "%d=%0*x",
 +		.name		= "L3DATA",
 +		.domains	= domain_init(RDT_RESOURCE_L3DATA),
 +		.msr_base	= IA32_L3_CBM_BASE,
 +		.min_cbm_bits	= 1,
 +		.cache_level	= 3,
 +		.cbm_idx_multi	= 2,
 +		.cbm_idx_offset	= 0
  	},
 -	[RDT_RESOURCE_L3CODE] =
  	{
 -		.name			= "L3CODE",
 -		.domains		= domain_init(RDT_RESOURCE_L3CODE),
 -		.msr_base		= IA32_L3_CBM_BASE,
 -		.msr_update		= cat_wrmsr,
 -		.cache_level		= 3,
 -		.cache = {
 -			.min_cbm_bits	= 1,
 -			.cbm_idx_mult	= 2,
 -			.cbm_idx_offset	= 1,
 -		},
 -		.parse_ctrlval		= parse_cbm,
 -		.format_str		= "%d=%0*x",
 +		.name		= "L3CODE",
 +		.domains	= domain_init(RDT_RESOURCE_L3CODE),
 +		.msr_base	= IA32_L3_CBM_BASE,
 +		.min_cbm_bits	= 1,
 +		.cache_level	= 3,
 +		.cbm_idx_multi	= 2,
 +		.cbm_idx_offset	= 1
  	},
 -	[RDT_RESOURCE_L2] =
  	{
 -		.name			= "L2",
 -		.domains		= domain_init(RDT_RESOURCE_L2),
 -		.msr_base		= IA32_L2_CBM_BASE,
 -		.msr_update		= cat_wrmsr,
 -		.cache_level		= 2,
 -		.cache = {
 -			.min_cbm_bits	= 1,
 -			.cbm_idx_mult	= 1,
 -			.cbm_idx_offset	= 0,
 -		},
 -		.parse_ctrlval		= parse_cbm,
 -		.format_str		= "%d=%0*x",
 -	},
 -	[RDT_RESOURCE_MBA] =
 -	{
 -		.name			= "MB",
 -		.domains		= domain_init(RDT_RESOURCE_MBA),
 -		.msr_base		= IA32_MBA_THRTL_BASE,
 -		.msr_update		= mba_wrmsr,
 -		.cache_level		= 3,
 -		.parse_ctrlval		= parse_bw,
 -		.format_str		= "%d=%*d",
 +		.name		= "L2",
 +		.domains	= domain_init(RDT_RESOURCE_L2),
 +		.msr_base	= IA32_L2_CBM_BASE,
 +		.min_cbm_bits	= 1,
 +		.cache_level	= 2,
 +		.cbm_idx_multi	= 1,
 +		.cbm_idx_offset	= 0
  	},
  };
  
@@@ -136,10 -193,58 +152,62 @@@ static inline bool cache_alloc_hsw_prob
  	return false;
  }
  
++<<<<<<< HEAD
 +static void rdt_get_config(int idx, struct rdt_resource *r)
++=======
+ /*
+  * rdt_get_mb_table() - get a mapping of bandwidth(b/w) percentage values
+  * exposed to user interface and the h/w understandable delay values.
+  *
+  * The non-linear delay values have the granularity of power of two
+  * and also the h/w does not guarantee a curve for configured delay
+  * values vs. actual b/w enforced.
+  * Hence we need a mapping that is pre calibrated so the user can
+  * express the memory b/w as a percentage value.
+  */
+ static inline bool rdt_get_mb_table(struct rdt_resource *r)
+ {
+ 	/*
+ 	 * There are no Intel SKUs as of now to support non-linear delay.
+ 	 */
+ 	pr_info("MBA b/w map not implemented for cpu:%d, model:%d",
+ 		boot_cpu_data.x86, boot_cpu_data.x86_model);
+ 
+ 	return false;
+ }
+ 
+ static bool rdt_get_mem_config(struct rdt_resource *r)
+ {
+ 	union cpuid_0x10_3_eax eax;
+ 	union cpuid_0x10_x_edx edx;
+ 	u32 ebx, ecx;
+ 
+ 	cpuid_count(0x00000010, 3, &eax.full, &ebx, &ecx, &edx.full);
+ 	r->num_closid = edx.split.cos_max + 1;
+ 	r->membw.max_delay = eax.split.max_delay + 1;
+ 	r->default_ctrl = MAX_MBA_BW;
+ 	if (ecx & MBA_IS_LINEAR) {
+ 		r->membw.delay_linear = true;
+ 		r->membw.min_bw = MAX_MBA_BW - r->membw.max_delay;
+ 		r->membw.bw_gran = MAX_MBA_BW - r->membw.max_delay;
+ 	} else {
+ 		if (!rdt_get_mb_table(r))
+ 			return false;
+ 	}
+ 	r->data_width = 3;
+ 	rdt_get_mba_infofile(r);
+ 
+ 	r->alloc_capable = true;
+ 	r->alloc_enabled = true;
+ 
+ 	return true;
+ }
+ 
+ static void rdt_get_cache_alloc_cfg(int idx, struct rdt_resource *r)
++>>>>>>> 6a445edce657 (x86/intel_rdt/cqm: Add RDT monitoring initialization)
  {
  	union cpuid_0x10_1_eax eax;
 -	union cpuid_0x10_x_edx edx;
 +	union cpuid_0x10_1_edx edx;
  	u32 ebx, ecx;
  
  	cpuid_count(0x00000010, idx, &eax.full, &ebx, &ecx, &edx.full);
@@@ -437,29 -522,58 +505,84 @@@ static void __init rdt_cpu_setup(void *
  	}
  }
  
++<<<<<<< HEAD
 +static struct notifier_block rdt_cpu_nb = {
 +	.notifier_call  = rdt_cpu_notify,
 +	.priority	= -INT_MAX,
 +};
 +
 +static int __init rdt_notifier_init(void)
++=======
+ static __init bool get_rdt_alloc_resources(void)
++>>>>>>> 6a445edce657 (x86/intel_rdt/cqm: Add RDT monitoring initialization)
  {
 -	bool ret = false;
 +	unsigned int cpu;
 +
++<<<<<<< HEAD
 +	for_each_online_cpu(cpu) {
 +		intel_rdt_online_cpu(cpu, false);
 +		/*
 +		 * RHEL7 - The upstream hotplug notification invokes the
 +		 *         callbacks on related cpus, but that's not the
 +		 *         case of the RHEL7 notification support.
 +		 *         Following call ensures we run all the msr
 +		 *         initialization setup on related cpus.
 +		 */
 +		smp_call_function_single(cpu, rdt_cpu_setup, NULL, 1);
 +	}
  
 +	__register_cpu_notifier(&rdt_cpu_nb);
 +	return 0;
++=======
+ 	if (cache_alloc_hsw_probe())
+ 		return true;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_RDT_A))
+ 		return false;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CAT_L3)) {
+ 		rdt_get_cache_alloc_cfg(1, &rdt_resources_all[RDT_RESOURCE_L3]);
+ 		if (boot_cpu_has(X86_FEATURE_CDP_L3)) {
+ 			rdt_get_cdp_l3_config(RDT_RESOURCE_L3DATA);
+ 			rdt_get_cdp_l3_config(RDT_RESOURCE_L3CODE);
+ 		}
+ 		ret = true;
+ 	}
+ 	if (boot_cpu_has(X86_FEATURE_CAT_L2)) {
+ 		/* CPUID 0x10.2 fields are same format at 0x10.1 */
+ 		rdt_get_cache_alloc_cfg(2, &rdt_resources_all[RDT_RESOURCE_L2]);
+ 		ret = true;
+ 	}
+ 
+ 	if (boot_cpu_has(X86_FEATURE_MBA)) {
+ 		if (rdt_get_mem_config(&rdt_resources_all[RDT_RESOURCE_MBA]))
+ 			ret = true;
+ 	}
+ 	return ret;
++>>>>>>> 6a445edce657 (x86/intel_rdt/cqm: Add RDT monitoring initialization)
+ }
+ 
+ static __init bool get_rdt_mon_resources(void)
+ {
+ 	if (boot_cpu_has(X86_FEATURE_CQM_OCCUP_LLC))
+ 		rdt_mon_features |= (1 << QOS_L3_OCCUP_EVENT_ID);
+ 	if (boot_cpu_has(X86_FEATURE_CQM_MBM_TOTAL))
+ 		rdt_mon_features |= (1 << QOS_L3_MBM_TOTAL_EVENT_ID);
+ 	if (boot_cpu_has(X86_FEATURE_CQM_MBM_LOCAL))
+ 		rdt_mon_features |= (1 << QOS_L3_MBM_LOCAL_EVENT_ID);
+ 
+ 	if (!rdt_mon_features)
+ 		return false;
+ 
+ 	return !rdt_get_mon_l3_config(&rdt_resources_all[RDT_RESOURCE_L3]);
+ }
+ 
+ static __init bool get_rdt_resources(void)
+ {
+ 	rdt_alloc_capable = get_rdt_alloc_resources();
+ 	rdt_mon_capable = get_rdt_mon_resources();
+ 
+ 	return (rdt_mon_capable || rdt_alloc_capable);
  }
  
  static int __init intel_rdt_late_init(void)
@@@ -483,9 -598,12 +606,12 @@@
  		return ret;
  	}
  
 -	for_each_alloc_capable_rdt_resource(r)
 +	for_each_capable_rdt_resource(r)
  		pr_info("Intel RDT %s allocation detected\n", r->name);
  
+ 	for_each_mon_capable_rdt_resource(r)
+ 		pr_info("Intel RDT %s monitoring detected\n", r->name);
+ 
  	return 0;
  }
  
* Unmerged path arch/x86/include/asm/intel_rdt.h
* Unmerged path arch/x86/kernel/cpu/Makefile
* Unmerged path arch/x86/kernel/cpu/intel_rdt.c
diff --git a/arch/x86/kernel/cpu/intel_rdt_monitor.c b/arch/x86/kernel/cpu/intel_rdt_monitor.c
new file mode 100644
index 000000000000..f6d43c3eba4e
--- /dev/null
+++ b/arch/x86/kernel/cpu/intel_rdt_monitor.c
@@ -0,0 +1,161 @@
+/*
+ * Resource Director Technology(RDT)
+ * - Monitoring code
+ *
+ * Copyright (C) 2017 Intel Corporation
+ *
+ * Author:
+ *    Vikas Shivappa <vikas.shivappa@intel.com>
+ *
+ * This replaces the cqm.c based on perf but we reuse a lot of
+ * code and datastructures originally from Peter Zijlstra and Matt Fleming.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * More information about RDT be found in the Intel (R) x86 Architecture
+ * Software Developer Manual June 2016, volume 3, section 17.17.
+ */
+
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <asm/cpu_device_id.h>
+#include "intel_rdt.h"
+
+struct rmid_entry {
+	u32				rmid;
+	struct list_head		list;
+};
+
+/**
+ * @rmid_free_lru    A least recently used list of free RMIDs
+ *     These RMIDs are guaranteed to have an occupancy less than the
+ *     threshold occupancy
+ */
+static LIST_HEAD(rmid_free_lru);
+
+/**
+ * @rmid_limbo_lru       list of currently unused but (potentially)
+ *     dirty RMIDs.
+ *     This list contains RMIDs that no one is currently using but that
+ *     may have a occupancy value > intel_cqm_threshold. User can change
+ *     the threshold occupancy value.
+ */
+static LIST_HEAD(rmid_limbo_lru);
+
+/**
+ * @rmid_entry - The entry in the limbo and free lists.
+ */
+static struct rmid_entry	*rmid_ptrs;
+
+/*
+ * Global boolean for rdt_monitor which is true if any
+ * resource monitoring is enabled.
+ */
+bool rdt_mon_capable;
+
+/*
+ * Global to indicate which monitoring events are enabled.
+ */
+unsigned int rdt_mon_features;
+
+/*
+ * This is the threshold cache occupancy at which we will consider an
+ * RMID available for re-allocation.
+ */
+unsigned int intel_cqm_threshold;
+
+static inline struct rmid_entry *__rmid_entry(u32 rmid)
+{
+	struct rmid_entry *entry;
+
+	entry = &rmid_ptrs[rmid];
+	WARN_ON(entry->rmid != rmid);
+
+	return entry;
+}
+
+static int dom_data_init(struct rdt_resource *r)
+{
+	struct rmid_entry *entry = NULL;
+	int i, nr_rmids;
+
+	nr_rmids = r->num_rmid;
+	rmid_ptrs = kcalloc(nr_rmids, sizeof(struct rmid_entry), GFP_KERNEL);
+	if (!rmid_ptrs)
+		return -ENOMEM;
+
+	for (i = 0; i < nr_rmids; i++) {
+		entry = &rmid_ptrs[i];
+		INIT_LIST_HEAD(&entry->list);
+
+		entry->rmid = i;
+		list_add_tail(&entry->list, &rmid_free_lru);
+	}
+
+	/*
+	 * RMID 0 is special and is always allocated. It's used for all
+	 * tasks that are not monitored.
+	 */
+	entry = __rmid_entry(0);
+	list_del(&entry->list);
+
+	return 0;
+}
+
+static struct mon_evt llc_occupancy_event = {
+	.name		= "llc_occupancy",
+	.evtid		= QOS_L3_OCCUP_EVENT_ID,
+};
+
+/*
+ * Initialize the event list for the resource.
+ *
+ * Note that MBM events are also part of RDT_RESOURCE_L3 resource
+ * because as per the SDM the total and local memory bandwidth
+ * are enumerated as part of L3 monitoring.
+ */
+static void l3_mon_evt_init(struct rdt_resource *r)
+{
+	INIT_LIST_HEAD(&r->evt_list);
+
+	if (is_llc_occupancy_enabled())
+		list_add_tail(&llc_occupancy_event.list, &r->evt_list);
+}
+
+int rdt_get_mon_l3_config(struct rdt_resource *r)
+{
+	int ret;
+
+	r->mon_scale = boot_cpu_data.x86_cache_occ_scale;
+	r->num_rmid = boot_cpu_data.x86_cache_max_rmid + 1;
+
+	/*
+	 * A reasonable upper limit on the max threshold is the number
+	 * of lines tagged per RMID if all RMIDs have the same number of
+	 * lines tagged in the LLC.
+	 *
+	 * For a 35MB LLC and 56 RMIDs, this is ~1.8% of the LLC.
+	 */
+	intel_cqm_threshold = boot_cpu_data.x86_cache_size * 1024 / r->num_rmid;
+
+	/* h/w works in units of "boot_cpu_data.x86_cache_occ_scale" */
+	intel_cqm_threshold /= r->mon_scale;
+
+	ret = dom_data_init(r);
+	if (ret)
+		return ret;
+
+	l3_mon_evt_init(r);
+
+	r->mon_capable = true;
+	r->mon_enabled = true;
+
+	return 0;
+}
