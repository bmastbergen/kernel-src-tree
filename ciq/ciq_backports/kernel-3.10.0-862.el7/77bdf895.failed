net/mlx5e: Xmit flow break down

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Xmit flow break down (Don Dutile) [1385325 1499362]
Rebuild_FUZZ: 93.10%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 77bdf8950b3cd17c780b4e5a2803806a9f573f51
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/77bdf895.failed

Break current mlx5e xmit flow into smaller blocks (helper functions)
in order to reuse them for IPoIB SKB transmission.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 77bdf8950b3cd17c780b4e5a2803806a9f573f51)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index f6e3d4bf0928,25185f8c3562..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -295,13 -302,141 +295,150 @@@ struct mlx5e_cq 
  	struct mlx5_frag_wq_ctrl   wq_ctrl;
  } ____cacheline_aligned_in_smp;
  
++<<<<<<< HEAD
 +struct mlx5e_rq;
 +typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
 +				       struct mlx5_cqe64 *cqe);
 +typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
 +				  u16 ix);
 +
 +typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq *rq, u16 ix);
++=======
+ struct mlx5e_tx_wqe_info {
+ 	struct sk_buff *skb;
+ 	u32 num_bytes;
+ 	u8  num_wqebbs;
+ 	u8  num_dma;
+ };
+ 
+ enum mlx5e_dma_map_type {
+ 	MLX5E_DMA_MAP_SINGLE,
+ 	MLX5E_DMA_MAP_PAGE
+ };
+ 
+ struct mlx5e_sq_dma {
+ 	dma_addr_t              addr;
+ 	u32                     size;
+ 	enum mlx5e_dma_map_type type;
+ };
+ 
+ enum {
+ 	MLX5E_SQ_STATE_ENABLED,
+ };
+ 
+ struct mlx5e_sq_wqe_info {
+ 	u8  opcode;
+ 	u8  num_wqebbs;
+ };
+ 
+ struct mlx5e_txqsq {
+ 	/* data path */
+ 
+ 	/* dirtied @completion */
+ 	u16                        cc;
+ 	u32                        dma_fifo_cc;
+ 
+ 	/* dirtied @xmit */
+ 	u16                        pc ____cacheline_aligned_in_smp;
+ 	u32                        dma_fifo_pc;
+ 	struct mlx5e_sq_stats      stats;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct mlx5e_sq_dma       *dma_fifo;
+ 		struct mlx5e_tx_wqe_info  *wqe_info;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	u32                        dma_fifo_mask;
+ 	void __iomem              *uar_map;
+ 	struct netdev_queue       *txq;
+ 	u32                        sqn;
+ 	u16                        max_inline;
+ 	u8                         min_inline_mode;
+ 	u16                        edge;
+ 	struct device             *pdev;
+ 	struct mlx5e_tstamp       *tstamp;
+ 	__be32                     mkey_be;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ 	int                        txq_ix;
+ 	u32                        rate_limit;
+ } ____cacheline_aligned_in_smp;
+ 
+ struct mlx5e_xdpsq {
+ 	/* data path */
+ 
+ 	/* dirtied @rx completion */
+ 	u16                        cc;
+ 	u16                        pc;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct mlx5e_dma_info     *di;
+ 		bool                       doorbell;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	void __iomem              *uar_map;
+ 	u32                        sqn;
+ 	struct device             *pdev;
+ 	__be32                     mkey_be;
+ 	u8                         min_inline_mode;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ } ____cacheline_aligned_in_smp;
+ 
+ struct mlx5e_icosq {
+ 	/* data path */
+ 
+ 	/* dirtied @completion */
+ 	u16                        cc;
+ 
+ 	/* dirtied @xmit */
+ 	u16                        pc ____cacheline_aligned_in_smp;
+ 	u32                        dma_fifo_pc;
+ 	u16                        prev_cc;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct mlx5e_sq_wqe_info *ico_wqe;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	void __iomem              *uar_map;
+ 	u32                        sqn;
+ 	u16                        edge;
+ 	struct device             *pdev;
+ 	__be32                     mkey_be;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ } ____cacheline_aligned_in_smp;
+ 
+ static inline bool
+ mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
+ {
+ 	return (((wq->sz_m1 & (cc - pc)) >= n) || (cc == pc));
+ }
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  
  struct mlx5e_dma_info {
  	struct page	*page;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index fc92406a15c4,2201b7ea05f4..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -880,26 -999,62 +880,51 @@@ static int mlx5e_alloc_sq_ico_db(struc
  	return 0;
  }
  
 -static int mlx5e_alloc_icosq(struct mlx5e_channel *c,
 -			     struct mlx5e_sq_param *param,
 -			     struct mlx5e_icosq *sq)
 +static void mlx5e_free_sq_txq_db(struct mlx5e_sq *sq)
  {
 -	void *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);
 -	struct mlx5_core_dev *mdev = c->mdev;
 -	int err;
 -
 -	sq->pdev      = c->pdev;
 -	sq->mkey_be   = c->mkey_be;
 -	sq->channel   = c;
 -	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
 -
 -	param->wq.db_numa_node = cpu_to_node(c->cpu);
 -	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq, &sq->wq_ctrl);
 -	if (err)
 -		return err;
 -	sq->wq.db = &sq->wq.db[MLX5_SND_DBR];
 -
 -	err = mlx5e_alloc_icosq_db(sq, cpu_to_node(c->cpu));
 -	if (err)
 -		goto err_sq_wq_destroy;
 -
 -	sq->edge = (sq->wq.sz_m1 + 1) - MLX5E_ICOSQ_MAX_WQEBBS;
 -
 -	return 0;
 -
 -err_sq_wq_destroy:
 -	mlx5_wq_destroy(&sq->wq_ctrl);
 -
 -	return err;
 +	kfree(sq->db.txq.wqe_info);
 +	kfree(sq->db.txq.dma_fifo);
 +	kfree(sq->db.txq.skb);
  }
  
++<<<<<<< HEAD
 +static int mlx5e_alloc_sq_txq_db(struct mlx5e_sq *sq, int numa)
++=======
+ static void mlx5e_free_icosq(struct mlx5e_icosq *sq)
+ {
+ 	mlx5e_free_icosq_db(sq);
+ 	mlx5_wq_destroy(&sq->wq_ctrl);
+ }
+ 
+ static void mlx5e_free_txqsq_db(struct mlx5e_txqsq *sq)
+ {
+ 	kfree(sq->db.wqe_info);
+ 	kfree(sq->db.dma_fifo);
+ }
+ 
+ static int mlx5e_alloc_txqsq_db(struct mlx5e_txqsq *sq, int numa)
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  {
  	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
  	int df_sz = wq_sz * MLX5_SEND_WQEBB_NUM_DS;
  
++<<<<<<< HEAD
 +	sq->db.txq.skb = kzalloc_node(wq_sz * sizeof(*sq->db.txq.skb),
 +				      GFP_KERNEL, numa);
 +	sq->db.txq.dma_fifo = kzalloc_node(df_sz * sizeof(*sq->db.txq.dma_fifo),
++=======
+ 	sq->db.dma_fifo = kzalloc_node(df_sz * sizeof(*sq->db.dma_fifo),
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  					   GFP_KERNEL, numa);
 -	sq->db.wqe_info = kzalloc_node(wq_sz * sizeof(*sq->db.wqe_info),
 +	sq->db.txq.wqe_info = kzalloc_node(wq_sz * sizeof(*sq->db.txq.wqe_info),
  					   GFP_KERNEL, numa);
++<<<<<<< HEAD
 +	if (!sq->db.txq.skb || !sq->db.txq.dma_fifo || !sq->db.txq.wqe_info) {
 +		mlx5e_free_sq_txq_db(sq);
++=======
+ 	if (!sq->db.dma_fifo || !sq->db.wqe_info) {
+ 		mlx5e_free_txqsq_db(sq);
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  		return -ENOMEM;
  	}
  
@@@ -1151,33 -1278,147 +1176,126 @@@ static inline void netif_tx_disable_que
  	__netif_tx_unlock_bh(txq);
  }
  
 -static void mlx5e_deactivate_txqsq(struct mlx5e_txqsq *sq)
 +static void mlx5e_close_sq(struct mlx5e_sq *sq)
  {
 -	struct mlx5e_channel *c = sq->channel;
 -
  	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
  	/* prevent netif_tx_wake_queue */
 -	napi_synchronize(&c->napi);
 +	napi_synchronize(&sq->channel->napi);
  
 -	netif_tx_disable_queue(sq->txq);
 +	if (sq->txq) {
 +		netif_tx_disable_queue(sq->txq);
  
++<<<<<<< HEAD
 +		/* last doorbell out, godspeed .. */
 +		if (mlx5e_sq_has_room_for(sq, 1)) {
 +			sq->db.txq.skb[(sq->pc & sq->wq.sz_m1)] = NULL;
 +			mlx5e_send_nop(sq, true);
 +		}
++=======
+ 	/* last doorbell out, godspeed .. */
+ 	if (mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, 1)) {
+ 		struct mlx5e_tx_wqe *nop;
+ 
+ 		sq->db.wqe_info[(sq->pc & sq->wq.sz_m1)].skb = NULL;
+ 		nop = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+ 		mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &nop->ctrl);
+ 	}
+ }
+ 
+ static void mlx5e_close_txqsq(struct mlx5e_txqsq *sq)
+ {
+ 	struct mlx5e_channel *c = sq->channel;
+ 	struct mlx5_core_dev *mdev = c->mdev;
+ 
+ 	mlx5e_destroy_sq(mdev, sq->sqn);
+ 	if (sq->rate_limit)
+ 		mlx5_rl_remove_rate(mdev, sq->rate_limit);
+ 	mlx5e_free_txqsq_descs(sq);
+ 	mlx5e_free_txqsq(sq);
+ }
+ 
+ static int mlx5e_open_icosq(struct mlx5e_channel *c,
+ 			    struct mlx5e_params *params,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_icosq *sq)
+ {
+ 	struct mlx5e_create_sq_param csp = {};
+ 	int err;
+ 
+ 	err = mlx5e_alloc_icosq(c, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = params->tx_min_inline_mode;
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_create_sq_rdy(c->mdev, param, &csp, &sq->sqn);
+ 	if (err)
+ 		goto err_free_icosq;
+ 
+ 	return 0;
+ 
+ err_free_icosq:
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	mlx5e_free_icosq(sq);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_close_icosq(struct mlx5e_icosq *sq)
+ {
+ 	struct mlx5e_channel *c = sq->channel;
+ 
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	napi_synchronize(&c->napi);
+ 
+ 	mlx5e_destroy_sq(c->mdev, sq->sqn);
+ 	mlx5e_free_icosq(sq);
+ }
+ 
+ static int mlx5e_open_xdpsq(struct mlx5e_channel *c,
+ 			    struct mlx5e_params *params,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_xdpsq *sq)
+ {
+ 	unsigned int ds_cnt = MLX5E_XDP_TX_DS_COUNT;
+ 	struct mlx5e_create_sq_param csp = {};
+ 	unsigned int inline_hdr_sz = 0;
+ 	int err;
+ 	int i;
+ 
+ 	err = mlx5e_alloc_xdpsq(c, params, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.tis_lst_sz      = 1;
+ 	csp.tisn            = c->priv->tisn[0]; /* tc = 0 */
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = sq->min_inline_mode;
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_create_sq_rdy(c->mdev, param, &csp, &sq->sqn);
+ 	if (err)
+ 		goto err_free_xdpsq;
+ 
+ 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+ 		inline_hdr_sz = MLX5E_XDP_MIN_INLINE;
+ 		ds_cnt++;
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  	}
  
 -	/* Pre initialize fixed WQE fields */
 -	for (i = 0; i < mlx5_wq_cyc_get_size(&sq->wq); i++) {
 -		struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(&sq->wq, i);
 -		struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
 -		struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
 -		struct mlx5_wqe_data_seg *dseg;
 -
 -		cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_cnt);
 -		eseg->inline_hdr.sz = cpu_to_be16(inline_hdr_sz);
 -
 -		dseg = (struct mlx5_wqe_data_seg *)cseg + (ds_cnt - 1);
 -		dseg->lkey = sq->mkey_be;
 -	}
 -
 -	return 0;
 -
 -err_free_xdpsq:
 -	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 -	mlx5e_free_xdpsq(sq);
 -
 -	return err;
 -}
 -
 -static void mlx5e_close_xdpsq(struct mlx5e_xdpsq *sq)
 -{
 -	struct mlx5e_channel *c = sq->channel;
 -
 -	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 -	napi_synchronize(&c->napi);
 -
 -	mlx5e_destroy_sq(c->mdev, sq->sqn);
 -	mlx5e_free_xdpsq_descs(sq);
 -	mlx5e_free_xdpsq(sq);
 +	mlx5e_disable_sq(sq);
 +	mlx5e_free_tx_descs(sq);
 +	mlx5e_destroy_sq(sq);
  }
  
 -static int mlx5e_alloc_cq_common(struct mlx5_core_dev *mdev,
 -				 struct mlx5e_cq_param *param,
 -				 struct mlx5e_cq *cq)
 +static int mlx5e_create_cq(struct mlx5e_channel *c,
 +			   struct mlx5e_cq_param *param,
 +			   struct mlx5e_cq *cq)
  {
 +	struct mlx5e_priv *priv = c->priv;
 +	struct mlx5_core_dev *mdev = priv->mdev;
  	struct mlx5_core_cq *mcq = &cq->mcq;
  	int eqn_not_used;
  	unsigned int irqn;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 2a270903b57d,ba664a1126cf..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -216,31 -177,9 +216,37 @@@ static inline void mlx5e_insert_vlan(vo
  	mlx5e_tx_skb_pull_inline(skb_data, skb_len, cpy2_sz);
  }
  
++<<<<<<< HEAD
 +static netdev_tx_t mlx5e_sq_xmit(struct mlx5e_sq *sq, struct sk_buff *skb)
 +{
 +	struct mlx5_wq_cyc       *wq   = &sq->wq;
 +
 +	u16 pi = sq->pc & wq->sz_m1;
 +	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 +	struct mlx5e_tx_wqe_info *wi   = &sq->db.txq.wqe_info[pi];
 +
 +	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
 +	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
 +	struct mlx5_wqe_data_seg *dseg;
 +
 +	unsigned char *skb_data = skb->data;
 +	unsigned int skb_len = skb->len;
 +	u8  opcode = MLX5_OPCODE_SEND;
 +	dma_addr_t dma_addr = 0;
 +	unsigned int num_bytes;
 +	bool bf = false;
 +	u16 headlen;
 +	u16 ds_cnt;
 +	u16 ihs;
 +	int i;
 +
 +	memset(wqe, 0, sizeof(*wqe));
 +
++=======
+ static inline void
+ mlx5e_txwqe_build_eseg_csum(struct mlx5e_txqsq *sq, struct sk_buff *skb, struct mlx5_wqe_eth_seg *eseg)
+ {
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
  		eseg->cs_flags = MLX5_ETH_WQE_L3_CSUM;
  		if (skb->encapsulation) {
@@@ -252,133 -191,173 +258,269 @@@
  		}
  	} else
  		sq->stats.csum_none++;
+ }
  
++<<<<<<< HEAD
 +	if (sq->cc != sq->prev_cc) {
 +		sq->prev_cc = sq->cc;
 +		sq->bf_budget = (sq->cc == sq->pc) ? MLX5E_SQ_BF_BUDGET : 0;
 +	}
 +
 +	if (skb_is_gso(skb)) {
 +		eseg->mss    = cpu_to_be16(skb_shinfo(skb)->gso_size);
 +		opcode       = MLX5_OPCODE_LSO;
++=======
+ static inline u16
+ mlx5e_txwqe_build_eseg_gso(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 			   struct mlx5_wqe_eth_seg *eseg, unsigned int *num_bytes)
+ {
+ 	u16 ihs;
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  
- 		if (skb->encapsulation) {
- 			ihs = skb_inner_transport_offset(skb) + inner_tcp_hdrlen(skb);
- 			sq->stats.tso_inner_packets++;
- 			sq->stats.tso_inner_bytes += skb->len - ihs;
- 		} else {
- 			ihs = skb_transport_offset(skb) + tcp_hdrlen(skb);
- 			sq->stats.tso_packets++;
- 			sq->stats.tso_bytes += skb->len - ihs;
- 		}
+ 	eseg->mss    = cpu_to_be16(skb_shinfo(skb)->gso_size);
  
- 		sq->stats.packets += skb_shinfo(skb)->gso_segs;
- 		num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
+ 	if (skb->encapsulation) {
+ 		ihs = skb_inner_transport_offset(skb) + inner_tcp_hdrlen(skb);
+ 		sq->stats.tso_inner_packets++;
+ 		sq->stats.tso_inner_bytes += skb->len - ihs;
  	} else {
++<<<<<<< HEAD
 +		bf = sq->bf_budget &&
 +		     !skb->xmit_more &&
 +		     !skb_shinfo(skb)->nr_frags;
 +		ihs = mlx5e_get_inline_hdr_size(sq, skb, bf);
 +		sq->stats.packets++;
 +		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
++=======
+ 		ihs = skb_transport_offset(skb) + tcp_hdrlen(skb);
+ 		sq->stats.tso_packets++;
+ 		sq->stats.tso_bytes += skb->len - ihs;
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  	}
  
- 	sq->stats.bytes += num_bytes;
+ 	*num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs;
+ 	return ihs;
+ }
+ 
+ static inline int
+ mlx5e_txwqe_build_dsegs(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 			unsigned char *skb_data, u16 headlen,
+ 			struct mlx5_wqe_data_seg *dseg)
+ {
+ 	dma_addr_t dma_addr = 0;
+ 	u8 num_dma          = 0;
+ 	int i;
+ 
+ 	if (headlen) {
+ 		dma_addr = dma_map_single(sq->pdev, skb_data, headlen,
+ 					  DMA_TO_DEVICE);
+ 		if (unlikely(dma_mapping_error(sq->pdev, dma_addr)))
+ 			return -ENOMEM;
+ 
+ 		dseg->addr       = cpu_to_be64(dma_addr);
+ 		dseg->lkey       = sq->mkey_be;
+ 		dseg->byte_count = cpu_to_be32(headlen);
+ 
+ 		mlx5e_dma_push(sq, dma_addr, headlen, MLX5E_DMA_MAP_SINGLE);
+ 		num_dma++;
+ 		dseg++;
+ 	}
+ 
+ 	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+ 		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
+ 		int fsz = skb_frag_size(frag);
+ 
+ 		dma_addr = skb_frag_dma_map(sq->pdev, frag, 0, fsz,
+ 				     DMA_TO_DEVICE);
+ 		if (unlikely(dma_mapping_error(sq->pdev, dma_addr)))
+ 			return -ENOMEM;
+ 
+ 		dseg->addr       = cpu_to_be64(dma_addr);
+ 		dseg->lkey       = sq->mkey_be;
+ 		dseg->byte_count = cpu_to_be32(fsz);
+ 
+ 		mlx5e_dma_push(sq, dma_addr, fsz, MLX5E_DMA_MAP_PAGE);
+ 		num_dma++;
+ 		dseg++;
+ 	}
+ 
+ 	return num_dma;
+ }
+ 
+ static inline void
+ mlx5e_txwqe_complete(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+ 		     u8 opcode, u16 ds_cnt, u32 num_bytes, u8 num_dma,
+ 		     struct mlx5e_tx_wqe_info *wi, struct mlx5_wqe_ctrl_seg *cseg)
+ {
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	u16 pi;
+ 
  	wi->num_bytes = num_bytes;
+ 	wi->num_dma = num_dma;
+ 	wi->num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
+ 	wi->skb = skb;
+ 
+ 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | opcode);
+ 	cseg->qpn_ds           = cpu_to_be32((sq->sqn << 8) | ds_cnt);
+ 
+ 	netdev_tx_sent_queue(sq->txq, num_bytes);
+ 
+ 	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+ 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+ 
+ 	sq->pc += wi->num_wqebbs;
+ 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, MLX5E_SQ_STOP_ROOM))) {
+ 		netif_tx_stop_queue(sq->txq);
+ 		sq->stats.stopped++;
+ 	}
+ 
+ 	if (!skb->xmit_more || netif_xmit_stopped(sq->txq))
+ 		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
+ 
+ 	/* fill sq edge with nops to avoid wqe wrap around */
+ 	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
+ 		sq->db.wqe_info[pi].skb = NULL;
+ 		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
+ 		sq->stats.nop++;
+ 	}
+ }
+ 
+ static netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb)
+ {
+ 	struct mlx5_wq_cyc       *wq   = &sq->wq;
+ 
+ 	u16 pi = sq->pc & wq->sz_m1;
+ 	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 	struct mlx5e_tx_wqe_info *wi   = &sq->db.wqe_info[pi];
+ 
+ 	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ 
+ 	unsigned char *skb_data = skb->data;
+ 	unsigned int skb_len = skb->len;
+ 	u8  opcode = MLX5_OPCODE_SEND;
+ 	unsigned int num_bytes;
+ 	int num_dma;
+ 	u16 headlen;
+ 	u16 ds_cnt;
+ 	u16 ihs;
+ 
+ 	memset(wqe, 0, sizeof(*wqe));
+ 
+ 	mlx5e_txwqe_build_eseg_csum(sq, skb, eseg);
+ 
+ 	if (skb_is_gso(skb)) {
+ 		opcode = MLX5_OPCODE_LSO;
+ 		ihs = mlx5e_txwqe_build_eseg_gso(sq, skb, eseg, &num_bytes);
+ 		sq->stats.packets += skb_shinfo(skb)->gso_segs;
+ 	} else {
+ 		ihs = mlx5e_calc_min_inline(sq->min_inline_mode, skb);
+ 		num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);
+ 		sq->stats.packets++;
+ 	}
+ 	sq->stats.bytes += num_bytes;
+ 	sq->stats.xmit_more += skb->xmit_more;
  
 -	ds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;
 -	if (ihs) {
 -		if (skb_vlan_tag_present(skb)) {
 -			mlx5e_insert_vlan(eseg->inline_hdr.start, skb, ihs, &skb_data, &skb_len);
 -			ihs += VLAN_HLEN;
 -		} else {
 -			memcpy(eseg->inline_hdr.start, skb_data, ihs);
 -			mlx5e_tx_skb_pull_inline(&skb_data, &skb_len, ihs);
 -		}
 -		eseg->inline_hdr.sz = cpu_to_be16(ihs);
 -		ds_cnt += DIV_ROUND_UP(ihs - sizeof(eseg->inline_hdr.start), MLX5_SEND_WQE_DS);
 -	} else if (skb_vlan_tag_present(skb)) {
 -		eseg->insert.type = cpu_to_be16(MLX5_ETH_WQE_INSERT_VLAN);
 -		eseg->insert.vlan_tci = cpu_to_be16(skb_vlan_tag_get(skb));
 +	if (skb_vlan_tag_present(skb)) {
 +		mlx5e_insert_vlan(eseg->inline_hdr_start, skb, ihs, &skb_data,
 +				  &skb_len);
 +		ihs += VLAN_HLEN;
 +	} else {
 +		memcpy(eseg->inline_hdr_start, skb_data, ihs);
 +		mlx5e_tx_skb_pull_inline(&skb_data, &skb_len, ihs);
  	}
  
++<<<<<<< HEAD
 +	eseg->inline_hdr_sz = cpu_to_be16(ihs);
 +
 +	ds_cnt  = sizeof(*wqe) / MLX5_SEND_WQE_DS;
 +	ds_cnt += DIV_ROUND_UP(ihs - sizeof(eseg->inline_hdr_start),
 +			       MLX5_SEND_WQE_DS);
 +	dseg    = (struct mlx5_wqe_data_seg *)cseg + ds_cnt;
 +
 +	wi->num_dma = 0;
 +
++=======
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  	headlen = skb_len - skb->data_len;
- 	if (headlen) {
- 		dma_addr = dma_map_single(sq->pdev, skb_data, headlen,
- 					  DMA_TO_DEVICE);
- 		if (unlikely(dma_mapping_error(sq->pdev, dma_addr)))
- 			goto dma_unmap_wqe_err;
+ 	num_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb_data, headlen,
+ 					  (struct mlx5_wqe_data_seg *)cseg + ds_cnt);
+ 	if (unlikely(num_dma < 0))
+ 		goto dma_unmap_wqe_err;
  
++<<<<<<< HEAD
 +		dseg->addr       = cpu_to_be64(dma_addr);
 +		dseg->lkey       = sq->mkey_be;
 +		dseg->byte_count = cpu_to_be32(headlen);
 +
 +		mlx5e_dma_push(sq, dma_addr, headlen, MLX5E_DMA_MAP_SINGLE);
 +		wi->num_dma++;
 +
 +		dseg++;
 +	}
 +
 +	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
 +		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
 +		int fsz = skb_frag_size(frag);
 +
 +		dma_addr = skb_frag_dma_map(sq->pdev, frag, 0, fsz,
 +					    DMA_TO_DEVICE);
 +		if (unlikely(dma_mapping_error(sq->pdev, dma_addr)))
 +			goto dma_unmap_wqe_err;
 +
 +		dseg->addr       = cpu_to_be64(dma_addr);
 +		dseg->lkey       = sq->mkey_be;
 +		dseg->byte_count = cpu_to_be32(fsz);
 +
 +		mlx5e_dma_push(sq, dma_addr, fsz, MLX5E_DMA_MAP_PAGE);
 +		wi->num_dma++;
 +
 +		dseg++;
 +	}
 +
 +	ds_cnt += wi->num_dma;
 +
 +	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | opcode);
 +	cseg->qpn_ds           = cpu_to_be32((sq->sqn << 8) | ds_cnt);
 +
 +	sq->db.txq.skb[pi] = skb;
 +
 +	wi->num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
 +	sq->pc += wi->num_wqebbs;
 +
 +	netdev_tx_sent_queue(sq->txq, wi->num_bytes);
 +
 +	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
 +		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
 +
 +	if (unlikely(!mlx5e_sq_has_room_for(sq, MLX5E_SQ_STOP_ROOM))) {
 +		netif_tx_stop_queue(sq->txq);
 +		sq->stats.stopped++;
 +	}
 +
 +	sq->stats.xmit_more += skb->xmit_more;
 +	if (!skb->xmit_more || netif_xmit_stopped(sq->txq)) {
 +		int bf_sz = 0;
 +
 +		if (bf && test_bit(MLX5E_SQ_STATE_BF_ENABLE, &sq->state))
 +			bf_sz = wi->num_wqebbs << 3;
 +
 +		cseg->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
 +		mlx5e_tx_notify_hw(sq, &wqe->ctrl, bf_sz);
 +	}
 +
 +	/* fill sq edge with nops to avoid wqe wrap around */
 +	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
 +		sq->db.txq.skb[pi] = NULL;
 +		mlx5e_send_nop(sq, false);
 +	}
++=======
+ 	mlx5e_txwqe_complete(sq, skb, opcode, ds_cnt + num_dma,
+ 			     num_bytes, num_dma, wi, cseg);
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
 +
 +	if (bf)
 +		sq->bf_budget--;
  
  	return NETDEV_TX_OK;
  
@@@ -446,8 -425,8 +588,13 @@@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *
  			last_wqe = (sqcc == wqe_counter);
  
  			ci = sqcc & sq->wq.sz_m1;
++<<<<<<< HEAD
 +			skb = sq->db.txq.skb[ci];
 +			wi = &sq->db.txq.wqe_info[ci];
++=======
+ 			wi = &sq->db.wqe_info[ci];
+ 			skb = wi->skb;
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  
  			if (unlikely(!skb)) { /* nop */
  				sqcc++;
@@@ -503,13 -482,10 +650,18 @@@ void mlx5e_free_tx_descs(struct mlx5e_s
  	u16 ci;
  	int i;
  
 +	if (sq->type != MLX5E_SQ_TXQ)
 +		return;
 +
  	while (sq->cc != sq->pc) {
  		ci = sq->cc & sq->wq.sz_m1;
++<<<<<<< HEAD
 +		skb = sq->db.txq.skb[ci];
 +		wi = &sq->db.txq.wqe_info[ci];
++=======
+ 		wi = &sq->db.wqe_info[ci];
+ 		skb = wi->skb;
++>>>>>>> 77bdf8950b3c (net/mlx5e: Xmit flow break down)
  
  		if (!skb) { /* nop */
  			sq->cc++;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
