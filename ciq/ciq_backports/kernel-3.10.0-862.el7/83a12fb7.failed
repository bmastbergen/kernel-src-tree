nvme-pci: factor out cqe handling into a dedicated routine

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [nvme] pci: factor out cqe handling into a dedicated routine (David Milburn) [1457880 1456486 1454365]
Rebuild_FUZZ: 95.50%
commit-author Sagi Grimberg <sagi@grimberg.me>
commit 83a12fb77b941a6735026e46c8ef5f4ec1204e97
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/83a12fb7.failed

Makes the code slightly more readable.

	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Keith Busch <keith.busch@intel.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 83a12fb77b941a6735026e46c8ef5f4ec1204e97)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/pci.c
diff --cc drivers/nvme/host/pci.c
index 9cfb96d101ef,26eb1743f8bc..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -592,7 -730,47 +592,51 @@@ static inline bool nvme_cqe_valid(struc
  	return (le16_to_cpu(nvmeq->cqes[head].status) & 1) == phase;
  }
  
++<<<<<<< HEAD
 +static int nvme_process_cq(struct nvme_queue *nvmeq)
++=======
+ static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
+ {
+ 	u16 head = nvmeq->cq_head;
+ 
+ 	if (likely(nvmeq->cq_vector >= 0)) {
+ 		if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
+ 						      nvmeq->dbbuf_cq_ei))
+ 			writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
+ 	}
+ }
+ 
+ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq,
+ 		struct nvme_completion *cqe)
+ {
+ 	struct request *req;
+ 
+ 	if (unlikely(cqe->command_id >= nvmeq->q_depth)) {
+ 		dev_warn(nvmeq->dev->ctrl.device,
+ 			"invalid id %d completed on queue %d\n",
+ 			cqe->command_id, le16_to_cpu(cqe->sq_id));
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * AEN requests are special as they don't time out and can
+ 	 * survive any kind of queue freeze and often don't respond to
+ 	 * aborts.  We don't even bother to allocate a struct request
+ 	 * for them but rather special case them here.
+ 	 */
+ 	if (unlikely(nvmeq->qid == 0 &&
+ 			cqe->command_id >= NVME_AQ_BLKMQ_DEPTH)) {
+ 		nvme_complete_async_event(&nvmeq->dev->ctrl,
+ 				cqe->status, &cqe->result);
+ 		return;
+ 	}
+ 
+ 	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+ 	nvme_end_request(req, cqe->status, cqe->result);
+ }
+ 
+ static void __nvme_process_cq(struct nvme_queue *nvmeq, unsigned int *tag)
++>>>>>>> 83a12fb77b94 (nvme-pci: factor out cqe handling into a dedicated routine)
  {
  	u16 head, phase;
  
@@@ -608,30 -785,10 +651,34 @@@
  			phase = !phase;
  		}
  
 -		if (tag && *tag == cqe.command_id)
 -			*tag = -1;
  
++<<<<<<< HEAD
 +		if (unlikely(cqe.command_id >= nvmeq->q_depth)) {
 +			dev_warn(nvmeq->dev->ctrl.device,
 +				"invalid id %d completed on queue %d\n",
 +				cqe.command_id, le16_to_cpu(cqe.sq_id));
 +			continue;
 +		}
 +
 +		/*
 +		 * AEN requests are special as they don't time out and can
 +		 * survive any kind of queue freeze and often don't respond to
 +		 * aborts.  We don't even bother to allocate a struct request
 +		 * for them but rather special case them here.
 +		 */
 +		if (unlikely(nvmeq->qid == 0 &&
 +				cqe.command_id >= NVME_AQ_BLKMQ_DEPTH)) {
 +			nvme_complete_async_event(&nvmeq->dev->ctrl,
 +					cqe.status, &cqe.result);
 +			continue;
 +		}
 +
 +		req = blk_mq_tag_to_rq(*nvmeq->tags, cqe.command_id);
 +		nvme_req(req)->result = cqe.result;
 +		blk_mq_complete_request(req, le16_to_cpu(cqe.status) >> 1);
++=======
+ 		nvme_handle_cqe(nvmeq, &cqe);
++>>>>>>> 83a12fb77b94 (nvme-pci: factor out cqe handling into a dedicated routine)
  	}
  
  	if (head == nvmeq->cq_head && phase == nvmeq->cq_phase)
* Unmerged path drivers/nvme/host/pci.c
