huge pagecache: mmap_sem is unlocked when truncation splits pmd

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Hugh Dickins <hughd@google.com>
commit 684283988f703811b8a05136d0d54f1c31025ad3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/68428398.failed

zap_pmd_range()'s CONFIG_DEBUG_VM !rwsem_is_locked(&mmap_sem) BUG() will
be invalid with huge pagecache, in whatever way it is implemented:
truncation of a hugely-mapped file to an unhugely-aligned size would
easily hit it.

(Although anon THP could in principle apply khugepaged to private file
mappings, which are not excluded by the MADV_HUGEPAGE restrictions, in
practice there's a vm_ops check which excludes them, so it never hits
this BUG() - there's no interface to "truncate" an anonymous mapping.)

We could complicate the test, to check i_mmap_rwsem also when there's a
vm_file; but my inclination was to make zap_pmd_range() more readable by
simply deleting this check.  A search has shown no report of the issue
in the years since commit e0897d75f0b2 ("mm, thp: print useful
information when mmap_sem is unlocked in zap_pmd_range") expanded it
from VM_BUG_ON() - though I cannot point to what commit I would say then
fixed the issue.

But there are a couple of other patches now floating around, neither yet
in the tree: let's agree to retain the check as a VM_BUG_ON_VMA(), as
Matthew Wilcox has done; but subject to a vma_is_anonymous() check, as
Kirill Shutemov has done.  And let's get this in, without waiting for
any particular huge pagecache implementation to reach the tree.

Matthew said "We can reproduce this BUG() in the current Linus tree with
DAX PMDs".

	Signed-off-by: Hugh Dickins <hughd@google.com>
	Tested-by: Matthew Wilcox <willy@linux.intel.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Andres Lagar-Cavilla <andreslc@google.com>
	Cc: Yang Shi <yang.shi@linaro.org>
	Cc: Ning Qu <quning@gmail.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Andres Lagar-Cavilla <andreslc@google.com>
	Cc: Konstantin Khlebnikov <koct9i@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 684283988f703811b8a05136d0d54f1c31025ad3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index 55ef43795115,52c218e2b724..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -1277,18 -1220,11 +1277,24 @@@ static inline unsigned long zap_pmd_ran
  	pmd = pmd_offset(pud, addr);
  	do {
  		next = pmd_addr_end(addr, end);
 -		if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
 +		if (pmd_trans_huge(*pmd)) {
  			if (next - addr != HPAGE_PMD_SIZE) {
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_VM
 +				if (!rwsem_is_locked(&tlb->mm->mmap_sem)) {
 +					pr_err("%s: mmap_sem is unlocked! addr=0x%lx end=0x%lx vma->vm_start=0x%lx vma->vm_end=0x%lx\n",
 +						__func__, addr, end,
 +						vma->vm_start,
 +						vma->vm_end);
 +					BUG();
 +				}
 +#endif
 +				split_huge_page_pmd(vma, addr, pmd);
++=======
+ 				VM_BUG_ON_VMA(vma_is_anonymous(vma) &&
+ 				    !rwsem_is_locked(&tlb->mm->mmap_sem), vma);
+ 				split_huge_pmd(vma, pmd, addr);
++>>>>>>> 684283988f70 (huge pagecache: mmap_sem is unlocked when truncation splits pmd)
  			} else if (zap_huge_pmd(tlb, vma, pmd, addr))
  				goto next;
  			/* fall through */
* Unmerged path mm/memory.c
