RDMA/SA: Fix kernel panic in CMA request handler flow

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Majd Dibbiny <majd@mellanox.com>
commit d3957b86a40612826ef935f474b31359d66cbdca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d3957b86.failed

Commit 9fdca4da4d8c (IB/SA: Split struct sa_path_rec based on IB and
ROCE specific fields) moved the service_id to be specific attribute
for IB and OPA SA Path Record, and thus wasn't assigned for RoCE.

This caused to the following kernel panic in the CMA request handler flow:

[   27.074594] BUG: unable to handle kernel NULL pointer dereference at 0000000000000008
[   27.074731] IP: __radix_tree_lookup+0x1d/0xe0
...
[   27.075356] Workqueue: ib_cm cm_work_handler [ib_cm]
[   27.075401] task: ffff88022e3b8000 task.stack: ffffc90001298000
[   27.075449] RIP: 0010:__radix_tree_lookup+0x1d/0xe0
...
[   27.075979] Call Trace:
[   27.076015]  radix_tree_lookup+0xd/0x10
[   27.076055]  cma_ps_find+0x59/0x70 [rdma_cm]
[   27.076097]  cma_id_from_event+0xd2/0x470 [rdma_cm]
[   27.076144]  ? ib_init_ah_from_path+0x39a/0x590 [ib_core]
[   27.076193]  cma_req_handler+0x25/0x480 [rdma_cm]
[   27.076237]  cm_process_work+0x25/0x120 [ib_cm]
[   27.076280]  ? cm_get_bth_pkey.isra.62+0x3c/0xa0 [ib_cm]
[   27.076350]  cm_req_handler+0xb03/0xd40 [ib_cm]
[   27.076430]  ? sched_clock_cpu+0x11/0xb0
[   27.076478]  cm_work_handler+0x194/0x1588 [ib_cm]
[   27.076525]  process_one_work+0x160/0x410
[   27.076565]  worker_thread+0x137/0x4a0
[   27.076614]  kthread+0x112/0x150
[   27.076684]  ? max_active_store+0x60/0x60
[   27.077642]  ? kthread_park+0x90/0x90
[   27.078530]  ret_from_fork+0x2c/0x40

This patch moves it back to the common SA Path Record structure
and removes the redundant setter and getter.

Tested on Connect-IB and Connect-X4 in Infiniband and RoCE respectively.

Fixes: 9fdca4da4d8c (IB/SA: Split struct sa_path_rec based on IB ands
	ROCE specific fields)
	Signed-off-by: Majd Dibbiny <majd@mellanox.com>
	Reviewed-by: Parav Pandit <parav@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit d3957b86a40612826ef935f474b31359d66cbdca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/cma.c
#	drivers/infiniband/core/sa_query.c
#	include/rdma/ib_sa.h
diff --cc drivers/infiniband/core/cma.c
index 2584e562abad,31bb82d8ecd7..000000000000
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@@ -1273,7 -1274,7 +1273,11 @@@ static int cma_save_req_info(const stru
  		memcpy(&req->local_gid, &req_param->primary_path->sgid,
  		       sizeof(req->local_gid));
  		req->has_gid	= true;
++<<<<<<< HEAD
 +		req->service_id	= req_param->primary_path->service_id;
++=======
+ 		req->service_id = req_param->primary_path->service_id;
++>>>>>>> d3957b86a406 (RDMA/SA: Fix kernel panic in CMA request handler flow)
  		req->pkey	= be16_to_cpu(req_param->primary_path->pkey);
  		if (req->pkey != req_param->bth_pkey)
  			pr_warn_ratelimited("RDMA CMA: got different BTH P_Key (0x%x) and primary path P_Key (0x%x)\n"
@@@ -1824,8 -1825,9 +1828,14 @@@ static struct rdma_id_private *cma_new_
  	struct rdma_cm_id *id;
  	struct rdma_route *rt;
  	const sa_family_t ss_family = listen_id->route.addr.src_addr.ss_family;
++<<<<<<< HEAD
 +	const __be64 service_id =
 +		      ib_event->param.req_rcvd.primary_path->service_id;
++=======
+ 	struct sa_path_rec *path = ib_event->param.req_rcvd.primary_path;
+ 	const __be64 service_id =
+ 		ib_event->param.req_rcvd.primary_path->service_id;
++>>>>>>> d3957b86a406 (RDMA/SA: Fix kernel panic in CMA request handler flow)
  	int ret;
  
  	id = rdma_create_id(listen_id->route.addr.dev_addr.net,
@@@ -2338,7 -2345,8 +2348,12 @@@ static int cma_query_ib_route(struct rd
  	path_rec.pkey = cpu_to_be16(ib_addr_get_pkey(dev_addr));
  	path_rec.numb_path = 1;
  	path_rec.reversible = 1;
++<<<<<<< HEAD
 +	path_rec.service_id = rdma_get_service_id(&id_priv->id, cma_dst_addr(id_priv));
++=======
+ 	path_rec.service_id = rdma_get_service_id(&id_priv->id,
+ 						  cma_dst_addr(id_priv));
++>>>>>>> d3957b86a406 (RDMA/SA: Fix kernel panic in CMA request handler flow)
  
  	comp_mask = IB_SA_PATH_REC_DGID | IB_SA_PATH_REC_SGID |
  		    IB_SA_PATH_REC_PKEY | IB_SA_PATH_REC_NUMB_PATH |
diff --cc drivers/infiniband/core/sa_query.c
index 17b24d8a34e1,fb7aec4047c8..000000000000
--- a/drivers/infiniband/core/sa_query.c
+++ b/drivers/infiniband/core/sa_query.c
@@@ -269,6 -288,136 +269,139 @@@ static const struct ib_field path_rec_t
  	  .size_bits    = 48 },
  };
  
++<<<<<<< HEAD
++=======
+ #define OPA_PATH_REC_FIELD(field) \
+ 	.struct_offset_bytes = \
+ 		offsetof(struct sa_path_rec, field), \
+ 	.struct_size_bytes   = \
+ 		sizeof((struct sa_path_rec *)0)->field,	\
+ 	.field_name          = "sa_path_rec:" #field
+ 
+ static const struct ib_field opa_path_rec_table[] = {
+ 	{ OPA_PATH_REC_FIELD(service_id),
+ 	  .offset_words = 0,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 64 },
+ 	{ OPA_PATH_REC_FIELD(dgid),
+ 	  .offset_words = 2,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 128 },
+ 	{ OPA_PATH_REC_FIELD(sgid),
+ 	  .offset_words = 6,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 128 },
+ 	{ OPA_PATH_REC_FIELD(opa.dlid),
+ 	  .offset_words = 10,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 32 },
+ 	{ OPA_PATH_REC_FIELD(opa.slid),
+ 	  .offset_words = 11,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 32 },
+ 	{ OPA_PATH_REC_FIELD(opa.raw_traffic),
+ 	  .offset_words = 12,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 1 },
+ 	{ RESERVED,
+ 	  .offset_words = 12,
+ 	  .offset_bits  = 1,
+ 	  .size_bits    = 3 },
+ 	{ OPA_PATH_REC_FIELD(flow_label),
+ 	  .offset_words = 12,
+ 	  .offset_bits  = 4,
+ 	  .size_bits    = 20 },
+ 	{ OPA_PATH_REC_FIELD(hop_limit),
+ 	  .offset_words = 12,
+ 	  .offset_bits  = 24,
+ 	  .size_bits    = 8 },
+ 	{ OPA_PATH_REC_FIELD(traffic_class),
+ 	  .offset_words = 13,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 8 },
+ 	{ OPA_PATH_REC_FIELD(reversible),
+ 	  .offset_words = 13,
+ 	  .offset_bits  = 8,
+ 	  .size_bits    = 1 },
+ 	{ OPA_PATH_REC_FIELD(numb_path),
+ 	  .offset_words = 13,
+ 	  .offset_bits  = 9,
+ 	  .size_bits    = 7 },
+ 	{ OPA_PATH_REC_FIELD(pkey),
+ 	  .offset_words = 13,
+ 	  .offset_bits  = 16,
+ 	  .size_bits    = 16 },
+ 	{ OPA_PATH_REC_FIELD(opa.l2_8B),
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 1 },
+ 	{ OPA_PATH_REC_FIELD(opa.l2_10B),
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 1,
+ 	  .size_bits    = 1 },
+ 	{ OPA_PATH_REC_FIELD(opa.l2_9B),
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 2,
+ 	  .size_bits    = 1 },
+ 	{ OPA_PATH_REC_FIELD(opa.l2_16B),
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 3,
+ 	  .size_bits    = 1 },
+ 	{ RESERVED,
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 4,
+ 	  .size_bits    = 2 },
+ 	{ OPA_PATH_REC_FIELD(opa.qos_type),
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 6,
+ 	  .size_bits    = 2 },
+ 	{ OPA_PATH_REC_FIELD(opa.qos_priority),
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 8,
+ 	  .size_bits    = 8 },
+ 	{ RESERVED,
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 16,
+ 	  .size_bits    = 3 },
+ 	{ OPA_PATH_REC_FIELD(sl),
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 19,
+ 	  .size_bits    = 5 },
+ 	{ RESERVED,
+ 	  .offset_words = 14,
+ 	  .offset_bits  = 24,
+ 	  .size_bits    = 8 },
+ 	{ OPA_PATH_REC_FIELD(mtu_selector),
+ 	  .offset_words = 15,
+ 	  .offset_bits  = 0,
+ 	  .size_bits    = 2 },
+ 	{ OPA_PATH_REC_FIELD(mtu),
+ 	  .offset_words = 15,
+ 	  .offset_bits  = 2,
+ 	  .size_bits    = 6 },
+ 	{ OPA_PATH_REC_FIELD(rate_selector),
+ 	  .offset_words = 15,
+ 	  .offset_bits  = 8,
+ 	  .size_bits    = 2 },
+ 	{ OPA_PATH_REC_FIELD(rate),
+ 	  .offset_words = 15,
+ 	  .offset_bits  = 10,
+ 	  .size_bits    = 6 },
+ 	{ OPA_PATH_REC_FIELD(packet_life_time_selector),
+ 	  .offset_words = 15,
+ 	  .offset_bits  = 16,
+ 	  .size_bits    = 2 },
+ 	{ OPA_PATH_REC_FIELD(packet_life_time),
+ 	  .offset_words = 15,
+ 	  .offset_bits  = 18,
+ 	  .size_bits    = 6 },
+ 	{ OPA_PATH_REC_FIELD(preference),
+ 	  .offset_words = 15,
+ 	  .offset_bits  = 24,
+ 	  .size_bits    = 8 },
+ };
+ 
++>>>>>>> d3957b86a406 (RDMA/SA: Fix kernel panic in CMA request handler flow)
  #define MCMEMBER_REC_FIELD(field) \
  	.struct_offset_bytes = offsetof(struct ib_sa_mcmember_rec, field),	\
  	.struct_size_bytes   = sizeof ((struct ib_sa_mcmember_rec *) 0)->field,	\
diff --cc include/rdma/ib_sa.h
index fd0e53219f93,355b81f4242d..000000000000
--- a/include/rdma/ib_sa.h
+++ b/include/rdma/ib_sa.h
@@@ -147,13 -150,44 +147,54 @@@ enum ib_sa_mc_join_states 
  #define IB_SA_PATH_REC_PACKET_LIFE_TIME			IB_SA_COMP_MASK(21)
  #define IB_SA_PATH_REC_PREFERENCE			IB_SA_COMP_MASK(22)
  
++<<<<<<< HEAD
 +struct ib_sa_path_rec {
 +	__be64       service_id;
 +	union ib_gid dgid;
 +	union ib_gid sgid;
 +	__be16       dlid;
 +	__be16       slid;
 +	u8           raw_traffic;
++=======
+ enum sa_path_rec_type {
+ 	SA_PATH_REC_TYPE_IB,
+ 	SA_PATH_REC_TYPE_ROCE_V1,
+ 	SA_PATH_REC_TYPE_ROCE_V2,
+ 	SA_PATH_REC_TYPE_OPA
+ };
+ 
+ struct sa_path_rec_ib {
+ 	__be16       dlid;
+ 	__be16       slid;
+ 	u8           raw_traffic;
+ };
+ 
+ struct sa_path_rec_roce {
+ 	u8           dmac[ETH_ALEN];
+ 	/* ignored in IB */
+ 	int	     ifindex;
+ 	/* ignored in IB */
+ 	struct net  *net;
+ 
+ };
+ 
+ struct sa_path_rec_opa {
+ 	__be32       dlid;
+ 	__be32       slid;
+ 	u8           raw_traffic;
+ 	u8	     l2_8B;
+ 	u8	     l2_10B;
+ 	u8	     l2_9B;
+ 	u8	     l2_16B;
+ 	u8	     qos_type;
+ 	u8	     qos_priority;
+ };
+ 
+ struct sa_path_rec {
+ 	union ib_gid dgid;
+ 	union ib_gid sgid;
+ 	__be64       service_id;
++>>>>>>> d3957b86a406 (RDMA/SA: Fix kernel panic in CMA request handler flow)
  	/* reserved */
  	__be32       flow_label;
  	u8           hop_limit;
@@@ -170,17 -204,109 +211,112 @@@
  	u8           packet_life_time_selector;
  	u8           packet_life_time;
  	u8           preference;
 -	union {
 -		struct sa_path_rec_ib ib;
 -		struct sa_path_rec_roce roce;
 -		struct sa_path_rec_opa opa;
 -	};
 -	enum sa_path_rec_type rec_type;
 +	u8           dmac[ETH_ALEN];
 +	/* ignored in IB */
 +	int	     ifindex;
 +	/* ignored in IB */
 +	struct net  *net;
 +	enum ib_gid_type gid_type;
  };
  
 -static inline enum ib_gid_type
 -		sa_conv_pathrec_to_gid_type(struct sa_path_rec *rec)
 +static inline struct net_device *ib_get_ndev_from_path(struct ib_sa_path_rec *rec)
  {
++<<<<<<< HEAD
 +	return rec->net ? dev_get_by_index(rec->net, rec->ifindex) : NULL;
++=======
+ 	switch (rec->rec_type) {
+ 	case SA_PATH_REC_TYPE_ROCE_V1:
+ 		return IB_GID_TYPE_ROCE;
+ 	case SA_PATH_REC_TYPE_ROCE_V2:
+ 		return IB_GID_TYPE_ROCE_UDP_ENCAP;
+ 	default:
+ 		return IB_GID_TYPE_IB;
+ 	}
+ }
+ 
+ static inline enum sa_path_rec_type
+ 		sa_conv_gid_to_pathrec_type(enum ib_gid_type type)
+ {
+ 	switch (type) {
+ 	case IB_GID_TYPE_ROCE:
+ 		return SA_PATH_REC_TYPE_ROCE_V1;
+ 	case IB_GID_TYPE_ROCE_UDP_ENCAP:
+ 		return SA_PATH_REC_TYPE_ROCE_V2;
+ 	default:
+ 		return SA_PATH_REC_TYPE_IB;
+ 	}
+ }
+ 
+ static inline void path_conv_opa_to_ib(struct sa_path_rec *ib,
+ 				       struct sa_path_rec *opa)
+ {
+ 	if ((be32_to_cpu(opa->opa.dlid) >=
+ 	     be16_to_cpu(IB_MULTICAST_LID_BASE)) ||
+ 	    (be32_to_cpu(opa->opa.slid) >=
+ 	     be16_to_cpu(IB_MULTICAST_LID_BASE))) {
+ 		/* Create OPA GID and zero out the LID */
+ 		ib->dgid.global.interface_id
+ 				= OPA_MAKE_ID(be32_to_cpu(opa->opa.dlid));
+ 		ib->dgid.global.subnet_prefix
+ 				= opa->dgid.global.subnet_prefix;
+ 		ib->sgid.global.interface_id
+ 				= OPA_MAKE_ID(be32_to_cpu(opa->opa.slid));
+ 		ib->dgid.global.subnet_prefix
+ 				= opa->dgid.global.subnet_prefix;
+ 		ib->ib.dlid	= 0;
+ 
+ 		ib->ib.slid	= 0;
+ 	} else {
+ 		ib->ib.dlid	= htons(ntohl(opa->opa.dlid));
+ 		ib->ib.slid	= htons(ntohl(opa->opa.slid));
+ 	}
+ 	ib->service_id		= opa->service_id;
+ 	ib->ib.raw_traffic	= opa->opa.raw_traffic;
+ }
+ 
+ static inline void path_conv_ib_to_opa(struct sa_path_rec *opa,
+ 				       struct sa_path_rec *ib)
+ {
+ 	__be32 slid, dlid;
+ 
+ 	if ((ib_is_opa_gid(&ib->sgid)) ||
+ 	    (ib_is_opa_gid(&ib->dgid))) {
+ 		slid = htonl(opa_get_lid_from_gid(&ib->sgid));
+ 		dlid = htonl(opa_get_lid_from_gid(&ib->dgid));
+ 	} else {
+ 		slid = htonl(ntohs(ib->ib.slid));
+ 		dlid = htonl(ntohs(ib->ib.dlid));
+ 	}
+ 	opa->opa.slid		= slid;
+ 	opa->opa.dlid		= dlid;
+ 	opa->service_id		= ib->service_id;
+ 	opa->opa.raw_traffic	= ib->ib.raw_traffic;
+ }
+ 
+ /* Convert from OPA to IB path record */
+ static inline void sa_convert_path_opa_to_ib(struct sa_path_rec *dest,
+ 					     struct sa_path_rec *src)
+ {
+ 	if (src->rec_type != SA_PATH_REC_TYPE_OPA)
+ 		return;
+ 
+ 	*dest = *src;
+ 	dest->rec_type = SA_PATH_REC_TYPE_IB;
+ 	path_conv_opa_to_ib(dest, src);
+ }
+ 
+ /* Convert from IB to OPA path record */
+ static inline void sa_convert_path_ib_to_opa(struct sa_path_rec *dest,
+ 					     struct sa_path_rec *src)
+ {
+ 	if (src->rec_type != SA_PATH_REC_TYPE_IB)
+ 		return;
+ 
+ 	/* Do a structure copy and overwrite the relevant fields */
+ 	*dest = *src;
+ 	dest->rec_type = SA_PATH_REC_TYPE_OPA;
+ 	path_conv_ib_to_opa(dest, src);
++>>>>>>> d3957b86a406 (RDMA/SA: Fix kernel panic in CMA request handler flow)
  }
  
  #define IB_SA_MCMEMBER_REC_MGID				IB_SA_COMP_MASK( 0)
@@@ -454,14 -580,119 +590,131 @@@ int ib_sa_guid_info_rec_query(struct ib
  			      void *context,
  			      struct ib_sa_query **sa_query);
  
++<<<<<<< HEAD
 +/* Support get SA ClassPortInfo */
 +int ib_sa_classport_info_rec_query(struct ib_sa_client *client,
 +				   struct ib_device *device, u8 port_num,
 +				   int timeout_ms, gfp_t gfp_mask,
 +				   void (*callback)(int status,
 +						    struct ib_class_port_info *resp,
 +						    void *context),
 +				   void *context,
 +				   struct ib_sa_query **sa_query);
++=======
+ bool ib_sa_sendonly_fullmem_support(struct ib_sa_client *client,
+ 				    struct ib_device *device,
+ 				    u8 port_num);
+ 
+ static inline bool sa_path_is_roce(struct sa_path_rec *rec)
+ {
+ 	return ((rec->rec_type == SA_PATH_REC_TYPE_ROCE_V1) ||
+ 		(rec->rec_type == SA_PATH_REC_TYPE_ROCE_V2));
+ }
+ 
+ static inline void sa_path_set_slid(struct sa_path_rec *rec, __be32 slid)
+ {
+ 	if (rec->rec_type == SA_PATH_REC_TYPE_IB)
+ 		rec->ib.slid = htons(ntohl(slid));
+ 	else if (rec->rec_type == SA_PATH_REC_TYPE_OPA)
+ 		rec->opa.slid = slid;
+ }
+ 
+ static inline void sa_path_set_dlid(struct sa_path_rec *rec, __be32 dlid)
+ {
+ 	if (rec->rec_type == SA_PATH_REC_TYPE_IB)
+ 		rec->ib.dlid = htons(ntohl(dlid));
+ 	else if (rec->rec_type == SA_PATH_REC_TYPE_OPA)
+ 		rec->opa.dlid = dlid;
+ }
+ 
+ static inline void sa_path_set_raw_traffic(struct sa_path_rec *rec,
+ 					   u8 raw_traffic)
+ {
+ 	if (rec->rec_type == SA_PATH_REC_TYPE_IB)
+ 		rec->ib.raw_traffic = raw_traffic;
+ 	else if (rec->rec_type == SA_PATH_REC_TYPE_OPA)
+ 		rec->opa.raw_traffic = raw_traffic;
+ }
+ 
+ static inline __be32 sa_path_get_slid(struct sa_path_rec *rec)
+ {
+ 	if (rec->rec_type == SA_PATH_REC_TYPE_IB)
+ 		return htonl(ntohs(rec->ib.slid));
+ 	else if (rec->rec_type == SA_PATH_REC_TYPE_OPA)
+ 		return rec->opa.slid;
+ 	return 0;
+ }
+ 
+ static inline __be32 sa_path_get_dlid(struct sa_path_rec *rec)
+ {
+ 	if (rec->rec_type == SA_PATH_REC_TYPE_IB)
+ 		return htonl(ntohs(rec->ib.dlid));
+ 	else if (rec->rec_type == SA_PATH_REC_TYPE_OPA)
+ 		return rec->opa.dlid;
+ 	return 0;
+ }
+ 
+ static inline u8 sa_path_get_raw_traffic(struct sa_path_rec *rec)
+ {
+ 	if (rec->rec_type == SA_PATH_REC_TYPE_IB)
+ 		return rec->ib.raw_traffic;
+ 	else if (rec->rec_type == SA_PATH_REC_TYPE_OPA)
+ 		return rec->opa.raw_traffic;
+ 	return 0;
+ }
+ 
+ static inline void sa_path_set_dmac(struct sa_path_rec *rec, u8 *dmac)
+ {
+ 	if (sa_path_is_roce(rec))
+ 		memcpy(rec->roce.dmac, dmac, ETH_ALEN);
+ }
+ 
+ static inline void sa_path_set_dmac_zero(struct sa_path_rec *rec)
+ {
+ 	if (sa_path_is_roce(rec))
+ 		eth_zero_addr(rec->roce.dmac);
+ }
+ 
+ static inline void sa_path_set_ifindex(struct sa_path_rec *rec, int ifindex)
+ {
+ 	if (sa_path_is_roce(rec))
+ 		rec->roce.ifindex = ifindex;
+ }
+ 
+ static inline void sa_path_set_ndev(struct sa_path_rec *rec, struct net *net)
+ {
+ 	if (sa_path_is_roce(rec))
+ 		rec->roce.net = net;
+ }
+ 
+ static inline u8 *sa_path_get_dmac(struct sa_path_rec *rec)
+ {
+ 	if (sa_path_is_roce(rec))
+ 		return rec->roce.dmac;
+ 	return NULL;
+ }
+ 
+ static inline int sa_path_get_ifindex(struct sa_path_rec *rec)
+ {
+ 	if (sa_path_is_roce(rec))
+ 		return rec->roce.ifindex;
+ 	return 0;
+ }
+ 
+ static inline struct net *sa_path_get_ndev(struct sa_path_rec *rec)
+ {
+ 	if (sa_path_is_roce(rec))
+ 		return rec->roce.net;
+ 	return NULL;
+ }
+ 
+ static inline struct net_device *ib_get_ndev_from_path(struct sa_path_rec *rec)
+ {
+ 	return sa_path_get_ndev(rec) ?
+ 		dev_get_by_index(sa_path_get_ndev(rec),
+ 				 sa_path_get_ifindex(rec))
+ 		: NULL;
+ }
++>>>>>>> d3957b86a406 (RDMA/SA: Fix kernel panic in CMA request handler flow)
  
  #endif /* IB_SA_H */
* Unmerged path drivers/infiniband/core/cma.c
* Unmerged path drivers/infiniband/core/sa_query.c
* Unmerged path include/rdma/ib_sa.h
