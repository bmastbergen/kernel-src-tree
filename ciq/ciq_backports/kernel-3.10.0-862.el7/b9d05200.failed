x86/mm: Insure that boot memory areas are mapped properly

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] mm: Insure that boot memory areas are mapped properly (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 96.36%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit b9d05200bc12444c7778a49c9694d8382ed06aa8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b9d05200.failed

The boot data and command line data are present in memory in a decrypted
state and are copied early in the boot process.  The early page fault
support will map these areas as encrypted, so before attempting to copy
them, add decrypted mappings so the data is accessed properly when copied.

For the initrd, encrypt this data in place. Since the future mapping of
the initrd area will be mapped as encrypted the data will be accessed
properly.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brijesh Singh <brijesh.singh@amd.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Larry Woodman <lwoodman@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Toshimitsu Kani <toshi.kani@hpe.com>
	Cc: kasan-dev@googlegroups.com
	Cc: kvm@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
	Cc: linux-doc@vger.kernel.org
	Cc: linux-efi@vger.kernel.org
	Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/bb0d430b41efefd45ee515aaf0979dcfda8b6a44.1500319216.git.thomas.lendacky@amd.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit b9d05200bc12444c7778a49c9694d8382ed06aa8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mem_encrypt.h
#	arch/x86/kernel/head64.c
#	arch/x86/kernel/setup.c
#	arch/x86/mm/kasan_init_64.c
#	arch/x86/mm/mem_encrypt.c
diff --cc arch/x86/kernel/head64.c
index 39ad3cdc4c78,0cdb53bf4c4b..000000000000
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@@ -31,11 -34,143 +31,14 @@@
  /*
   * Manage page tables very early on.
   */
++<<<<<<< HEAD
 +extern pgd_t early_level4_pgt[PTRS_PER_PGD];
++=======
++>>>>>>> b9d05200bc12 (x86/mm: Insure that boot memory areas are mapped properly)
  extern pmd_t early_dynamic_pgts[EARLY_DYNAMIC_PAGE_TABLES][PTRS_PER_PMD];
 -static unsigned int __initdata next_early_pgt;
 +static unsigned int __initdata next_early_pgt = 2;
  pmdval_t early_pmd_flags = __PAGE_KERNEL_LARGE & ~(_PAGE_GLOBAL | _PAGE_NX);
  
 -#define __head	__section(.head.text)
 -
 -static void __head *fixup_pointer(void *ptr, unsigned long physaddr)
 -{
 -	return ptr - (void *)_text + (void *)physaddr;
 -}
 -
 -unsigned long __head __startup_64(unsigned long physaddr)
 -{
 -	unsigned long load_delta, *p;
 -	unsigned long pgtable_flags;
 -	pgdval_t *pgd;
 -	p4dval_t *p4d;
 -	pudval_t *pud;
 -	pmdval_t *pmd, pmd_entry;
 -	int i;
 -
 -	/* Is the address too large? */
 -	if (physaddr >> MAX_PHYSMEM_BITS)
 -		for (;;);
 -
 -	/*
 -	 * Compute the delta between the address I am compiled to run at
 -	 * and the address I am actually running at.
 -	 */
 -	load_delta = physaddr - (unsigned long)(_text - __START_KERNEL_map);
 -
 -	/* Is the address not 2M aligned? */
 -	if (load_delta & ~PMD_PAGE_MASK)
 -		for (;;);
 -
 -	/* Activate Secure Memory Encryption (SME) if supported and enabled */
 -	sme_enable();
 -
 -	/* Include the SME encryption mask in the fixup value */
 -	load_delta += sme_get_me_mask();
 -
 -	/* Fixup the physical addresses in the page table */
 -
 -	pgd = fixup_pointer(&early_top_pgt, physaddr);
 -	pgd[pgd_index(__START_KERNEL_map)] += load_delta;
 -
 -	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
 -		p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
 -		p4d[511] += load_delta;
 -	}
 -
 -	pud = fixup_pointer(&level3_kernel_pgt, physaddr);
 -	pud[510] += load_delta;
 -	pud[511] += load_delta;
 -
 -	pmd = fixup_pointer(level2_fixmap_pgt, physaddr);
 -	pmd[506] += load_delta;
 -
 -	/*
 -	 * Set up the identity mapping for the switchover.  These
 -	 * entries should *NOT* have the global bit set!  This also
 -	 * creates a bunch of nonsense entries but that is fine --
 -	 * it avoids problems around wraparound.
 -	 */
 -
 -	pud = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
 -	pmd = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
 -	pgtable_flags = _KERNPG_TABLE_NOENC + sme_get_me_mask();
 -
 -	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
 -		p4d = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
 -
 -		i = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;
 -		pgd[i + 0] = (pgdval_t)p4d + pgtable_flags;
 -		pgd[i + 1] = (pgdval_t)p4d + pgtable_flags;
 -
 -		i = (physaddr >> P4D_SHIFT) % PTRS_PER_P4D;
 -		p4d[i + 0] = (pgdval_t)pud + pgtable_flags;
 -		p4d[i + 1] = (pgdval_t)pud + pgtable_flags;
 -	} else {
 -		i = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;
 -		pgd[i + 0] = (pgdval_t)pud + pgtable_flags;
 -		pgd[i + 1] = (pgdval_t)pud + pgtable_flags;
 -	}
 -
 -	i = (physaddr >> PUD_SHIFT) % PTRS_PER_PUD;
 -	pud[i + 0] = (pudval_t)pmd + pgtable_flags;
 -	pud[i + 1] = (pudval_t)pmd + pgtable_flags;
 -
 -	pmd_entry = __PAGE_KERNEL_LARGE_EXEC & ~_PAGE_GLOBAL;
 -	pmd_entry += sme_get_me_mask();
 -	pmd_entry +=  physaddr;
 -
 -	for (i = 0; i < DIV_ROUND_UP(_end - _text, PMD_SIZE); i++) {
 -		int idx = i + (physaddr >> PMD_SHIFT) % PTRS_PER_PMD;
 -		pmd[idx] = pmd_entry + i * PMD_SIZE;
 -	}
 -
 -	/*
 -	 * Fixup the kernel text+data virtual addresses. Note that
 -	 * we might write invalid pmds, when the kernel is relocated
 -	 * cleanup_highmap() fixes this up along with the mappings
 -	 * beyond _end.
 -	 */
 -
 -	pmd = fixup_pointer(level2_kernel_pgt, physaddr);
 -	for (i = 0; i < PTRS_PER_PMD; i++) {
 -		if (pmd[i] & _PAGE_PRESENT)
 -			pmd[i] += load_delta;
 -	}
 -
 -	/*
 -	 * Fixup phys_base - remove the memory encryption mask to obtain
 -	 * the true physical address.
 -	 */
 -	p = fixup_pointer(&phys_base, physaddr);
 -	*p += load_delta - sme_get_me_mask();
 -
 -	/* Encrypt the kernel (if SME is active) */
 -	sme_encrypt_kernel();
 -
 -	/*
 -	 * Return the SME encryption mask (if SME is active) to be used as a
 -	 * modifier for the initial pgdir entry programmed into CR3.
 -	 */
 -	return sme_get_me_mask();
 -}
 -
 -unsigned long __startup_secondary_64(void)
 -{
 -	/*
 -	 * Return the SME encryption mask (if SME is active) to be used as a
 -	 * modifier for the initial pgdir entry programmed into CR3.
 -	 */
 -	return sme_get_me_mask();
 -}
 -
  /* Wipe all early page tables except for the kernel symbol map */
  static void __init reset_early_page_tables(void)
  {
@@@ -50,16 -180,16 +53,16 @@@
  }
  
  /* Create a new PMD entry */
- int __init early_make_pgtable(unsigned long address)
+ int __init __early_make_pgtable(unsigned long address, pmdval_t pmd)
  {
  	unsigned long physaddr = address - __PAGE_OFFSET;
 +	unsigned long i;
  	pgdval_t pgd, *pgd_p;
 -	p4dval_t p4d, *p4d_p;
  	pudval_t pud, *pud_p;
- 	pmdval_t pmd, *pmd_p;
+ 	pmdval_t *pmd_p;
  
  	/* Invalid address or early pgt is done ?  */
 -	if (physaddr >= MAXMEM || read_cr3_pa() != __pa_nodebug(early_top_pgt))
 +	if (physaddr >= MAXMEM || read_cr3() != __pa(early_level4_pgt))
  		return -1;
  
  again:
@@@ -96,11 -242,9 +99,10 @@@
  		}
  
  		pmd_p = (pmdval_t *)early_dynamic_pgts[next_early_pgt++];
 -		memset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);
 +		for (i = 0; i < PTRS_PER_PMD; i++)
 +			pmd_p[i] = 0;
  		*pud_p = (pudval_t)pmd_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;
  	}
- 	pmd = (physaddr & PMD_MASK) + early_pmd_flags;
  	pmd_p[pmd_index(address)] = pmd;
  
  	return 0;
@@@ -135,9 -295,17 +153,17 @@@ static void __init copy_bootdata(char *
  		command_line = __va(cmd_line_ptr);
  		memcpy(boot_command_line, command_line, COMMAND_LINE_SIZE);
  	}
+ 
+ 	/*
+ 	 * The old boot data is no longer needed and won't be reserved,
+ 	 * freeing up that memory for use by the system. If SME is active,
+ 	 * we need to remove the mappings that were created so that the
+ 	 * memory doesn't remain mapped as decrypted.
+ 	 */
+ 	sme_unmap_bootdata(real_mode_data);
  }
  
 -asmlinkage __visible void __init x86_64_start_kernel(char * real_mode_data)
 +void __init x86_64_start_kernel(char * real_mode_data)
  {
  	int i;
  
diff --cc arch/x86/kernel/setup.c
index dcb7e8a78aab,0bfe0c1628f6..000000000000
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@@ -70,8 -69,9 +70,12 @@@
  #include <linux/crash_dump.h>
  #include <linux/tboot.h>
  #include <linux/jiffies.h>
++<<<<<<< HEAD
 +#include <linux/cpumask.h>
++=======
+ #include <linux/mem_encrypt.h>
++>>>>>>> b9d05200bc12 (x86/mm: Insure that boot memory areas are mapped properly)
  
 -#include <linux/usb/xhci-dbgp.h>
  #include <video/edid.h>
  
  #include <asm/mtrr.h>
* Unmerged path arch/x86/include/asm/mem_encrypt.h
* Unmerged path arch/x86/mm/kasan_init_64.c
* Unmerged path arch/x86/mm/mem_encrypt.c
* Unmerged path arch/x86/include/asm/mem_encrypt.h
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index ec9e20b59d06..3a0e06613eec 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -18,6 +18,9 @@
 #ifndef __ASSEMBLY__
 #include <asm/x86_init.h>
 
+extern pgd_t early_top_pgt[PTRS_PER_PGD];
+int __init __early_make_pgtable(unsigned long address, pmdval_t pmd);
+
 void ptdump_walk_pgd_level(struct seq_file *m, pgd_t *pgd);
 
 /*
* Unmerged path arch/x86/kernel/head64.c
* Unmerged path arch/x86/kernel/setup.c
* Unmerged path arch/x86/mm/kasan_init_64.c
* Unmerged path arch/x86/mm/mem_encrypt.c
