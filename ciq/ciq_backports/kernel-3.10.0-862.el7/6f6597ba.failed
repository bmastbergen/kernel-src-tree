gfs2: Protect gl->gl_object by spin lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Andreas Gruenbacher <agruenba@redhat.com>
commit 6f6597baae206c544c49ad7f1129d5adc1e9019d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6f6597ba.failed

Put all remaining accesses to gl->gl_object under the
gl->gl_lockref.lock spinlock to prevent races.

	Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
	Signed-off-by: Bob Peterson <rpeterso@redhat.com>
(cherry picked from commit 6f6597baae206c544c49ad7f1129d5adc1e9019d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/gfs2/bmap.c
#	fs/gfs2/glops.c
#	fs/gfs2/rgrp.c
diff --cc fs/gfs2/bmap.c
index ad1971d69c1a,56e2943ff994..000000000000
--- a/fs/gfs2/bmap.c
+++ b/fs/gfs2/bmap.c
@@@ -1105,41 -912,406 +1105,243 @@@ out
  	return error;
  }
  
 -/**
 - * sweep_bh_for_rgrps - find an rgrp in a meta buffer and free blocks therein
 - * @ip: inode
 - * @rg_gh: holder of resource group glock
 - * @mp: current metapath fully populated with buffers
 - * @btotal: place to keep count of total blocks freed
 - * @hgt: height we're processing
 - * @first: true if this is the first call to this function for this height
 - *
 - * We sweep a metadata buffer (provided by the metapath) for blocks we need to
 - * free, and free them all. However, we do it one rgrp at a time. If this
 - * block has references to multiple rgrps, we break it into individual
 - * transactions. This allows other processes to use the rgrps while we're
 - * focused on a single one, for better concurrency / performance.
 - * At every transaction boundary, we rewrite the inode into the journal.
 - * That way the bitmaps are kept consistent with the inode and we can recover
 - * if we're interrupted by power-outages.
 - *
 - * Returns: 0, or return code if an error occurred.
 - *          *btotal has the total number of blocks freed
 - */
 -static int sweep_bh_for_rgrps(struct gfs2_inode *ip, struct gfs2_holder *rd_gh,
 -			      const struct metapath *mp, u32 *btotal, int hgt,
 -			      bool preserve1)
 +static int trunc_dealloc(struct gfs2_inode *ip, u64 size)
  {
  	struct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);
++<<<<<<< HEAD
 +	unsigned int height = ip->i_height;
++=======
+ 	struct gfs2_rgrpd *rgd;
+ 	struct gfs2_trans *tr;
+ 	struct buffer_head *bh = mp->mp_bh[hgt];
+ 	__be64 *top, *bottom, *p;
+ 	int blks_outside_rgrp;
+ 	u64 bn, bstart, isize_blks;
+ 	s64 blen; /* needs to be s64 or gfs2_add_inode_blocks breaks */
+ 	int meta = ((hgt != ip->i_height - 1) ? 1 : 0);
+ 	int ret = 0;
+ 	bool buf_in_tr = false; /* buffer was added to transaction */
+ 
+ 	if (gfs2_metatype_check(sdp, bh,
+ 				(hgt ? GFS2_METATYPE_IN : GFS2_METATYPE_DI)))
+ 		return -EIO;
+ 
+ more_rgrps:
+ 	blks_outside_rgrp = 0;
+ 	bstart = 0;
+ 	blen = 0;
+ 	top = metapointer(hgt, mp); /* first ptr from metapath */
+ 	/* If we're keeping some data at the truncation point, we've got to
+ 	   preserve the metadata tree by adding 1 to the starting metapath. */
+ 	if (preserve1)
+ 		top++;
+ 
+ 	bottom = (__be64 *)(bh->b_data + bh->b_size);
+ 
+ 	for (p = top; p < bottom; p++) {
+ 		if (!*p)
+ 			continue;
+ 		bn = be64_to_cpu(*p);
+ 		if (gfs2_holder_initialized(rd_gh)) {
+ 			rgd = gfs2_glock2rgrp(rd_gh->gh_gl);
+ 			gfs2_assert_withdraw(sdp,
+ 				     gfs2_glock_is_locked_by_me(rd_gh->gh_gl));
+ 		} else {
+ 			rgd = gfs2_blk2rgrpd(sdp, bn, false);
+ 			ret = gfs2_glock_nq_init(rgd->rd_gl, LM_ST_EXCLUSIVE,
+ 						 0, rd_gh);
+ 			if (ret)
+ 				goto out;
+ 
+ 			/* Must be done with the rgrp glock held: */
+ 			if (gfs2_rs_active(&ip->i_res) &&
+ 			    rgd == ip->i_res.rs_rbm.rgd)
+ 				gfs2_rs_deltree(&ip->i_res);
+ 		}
+ 
+ 		if (!rgrp_contains_block(rgd, bn)) {
+ 			blks_outside_rgrp++;
+ 			continue;
+ 		}
+ 
+ 		/* The size of our transactions will be unknown until we
+ 		   actually process all the metadata blocks that relate to
+ 		   the rgrp. So we estimate. We know it can't be more than
+ 		   the dinode's i_blocks and we don't want to exceed the
+ 		   journal flush threshold, sd_log_thresh2. */
+ 		if (current->journal_info == NULL) {
+ 			unsigned int jblocks_rqsted, revokes;
+ 
+ 			jblocks_rqsted = rgd->rd_length + RES_DINODE +
+ 				RES_INDIRECT;
+ 			isize_blks = gfs2_get_inode_blocks(&ip->i_inode);
+ 			if (isize_blks > atomic_read(&sdp->sd_log_thresh2))
+ 				jblocks_rqsted +=
+ 					atomic_read(&sdp->sd_log_thresh2);
+ 			else
+ 				jblocks_rqsted += isize_blks;
+ 			revokes = jblocks_rqsted;
+ 			if (meta)
+ 				revokes += hptrs(sdp, hgt);
+ 			else if (ip->i_depth)
+ 				revokes += sdp->sd_inptrs;
+ 			ret = gfs2_trans_begin(sdp, jblocks_rqsted, revokes);
+ 			if (ret)
+ 				goto out_unlock;
+ 			down_write(&ip->i_rw_mutex);
+ 		}
+ 		/* check if we will exceed the transaction blocks requested */
+ 		tr = current->journal_info;
+ 		if (tr->tr_num_buf_new + RES_STATFS +
+ 		    RES_QUOTA >= atomic_read(&sdp->sd_log_thresh2)) {
+ 			/* We set blks_outside_rgrp to ensure the loop will
+ 			   be repeated for the same rgrp, but with a new
+ 			   transaction. */
+ 			blks_outside_rgrp++;
+ 			/* This next part is tricky. If the buffer was added
+ 			   to the transaction, we've already set some block
+ 			   pointers to 0, so we better follow through and free
+ 			   them, or we will introduce corruption (so break).
+ 			   This may be impossible, or at least rare, but I
+ 			   decided to cover the case regardless.
+ 
+ 			   If the buffer was not added to the transaction
+ 			   (this call), doing so would exceed our transaction
+ 			   size, so we need to end the transaction and start a
+ 			   new one (so goto). */
+ 
+ 			if (buf_in_tr)
+ 				break;
+ 			goto out_unlock;
+ 		}
+ 
+ 		gfs2_trans_add_meta(ip->i_gl, bh);
+ 		buf_in_tr = true;
+ 		*p = 0;
+ 		if (bstart + blen == bn) {
+ 			blen++;
+ 			continue;
+ 		}
+ 		if (bstart) {
+ 			__gfs2_free_blocks(ip, bstart, (u32)blen, meta);
+ 			(*btotal) += blen;
+ 			gfs2_add_inode_blocks(&ip->i_inode, -blen);
+ 		}
+ 		bstart = bn;
+ 		blen = 1;
+ 	}
+ 	if (bstart) {
+ 		__gfs2_free_blocks(ip, bstart, (u32)blen, meta);
+ 		(*btotal) += blen;
+ 		gfs2_add_inode_blocks(&ip->i_inode, -blen);
+ 	}
+ out_unlock:
+ 	if (!ret && blks_outside_rgrp) { /* If buffer still has non-zero blocks
+ 					    outside the rgrp we just processed,
+ 					    do it all over again. */
+ 		if (current->journal_info) {
+ 			struct buffer_head *dibh = mp->mp_bh[0];
+ 
+ 			/* Every transaction boundary, we rewrite the dinode
+ 			   to keep its di_blocks current in case of failure. */
+ 			ip->i_inode.i_mtime = ip->i_inode.i_ctime =
+ 				CURRENT_TIME;
+ 			gfs2_trans_add_meta(ip->i_gl, dibh);
+ 			gfs2_dinode_out(ip, dibh->b_data);
+ 			up_write(&ip->i_rw_mutex);
+ 			gfs2_trans_end(sdp);
+ 		}
+ 		gfs2_glock_dq_uninit(rd_gh);
+ 		cond_resched();
+ 		goto more_rgrps;
+ 	}
+ out:
+ 	return ret;
+ }
+ 
+ /**
+  * find_nonnull_ptr - find a non-null pointer given a metapath and height
+  * assumes the metapath is valid (with buffers) out to height h
+  * @mp: starting metapath
+  * @h: desired height to search
+  *
+  * Returns: true if a non-null pointer was found in the metapath buffer
+  *          false if all remaining pointers are NULL in the buffer
+  */
+ static bool find_nonnull_ptr(struct gfs2_sbd *sdp, struct metapath *mp,
+ 			     unsigned int h)
+ {
+ 	__be64 *ptr;
+ 	unsigned int ptrs = hptrs(sdp, h) - 1;
+ 
+ 	while (true) {
+ 		ptr = metapointer(h, mp);
+ 		if (*ptr) /* if we have a non-null pointer */
+ 			return true;
+ 
+ 		if (mp->mp_list[h] < ptrs)
+ 			mp->mp_list[h]++;
+ 		else
+ 			return false; /* no more pointers in this buffer */
+ 	}
+ }
+ 
+ enum dealloc_states {
+ 	DEALLOC_MP_FULL = 0,    /* Strip a metapath with all buffers read in */
+ 	DEALLOC_MP_LOWER = 1,   /* lower the metapath strip height */
+ 	DEALLOC_FILL_MP = 2,  /* Fill in the metapath to the given height. */
+ 	DEALLOC_DONE = 3,       /* process complete */
+ };
+ 
+ /**
+  * trunc_dealloc - truncate a file down to a desired size
+  * @ip: inode to truncate
+  * @newsize: The desired size of the file
+  *
+  * This function truncates a file to newsize. It works from the
+  * bottom up, and from the right to the left. In other words, it strips off
+  * the highest layer (data) before stripping any of the metadata. Doing it
+  * this way is best in case the operation is interrupted by power failure, etc.
+  * The dinode is rewritten in every transaction to guarantee integrity.
+  */
+ static int trunc_dealloc(struct gfs2_inode *ip, u64 newsize)
+ {
+ 	struct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);
+ 	struct metapath mp;
+ 	struct buffer_head *dibh, *bh;
+ 	struct gfs2_holder rd_gh;
++>>>>>>> 6f6597baae20 (gfs2: Protect gl->gl_object by spin lock)
  	u64 lblock;
 -	__u16 nbof[GFS2_MAX_META_HEIGHT]; /* new beginning of truncation */
 -	unsigned int strip_h = ip->i_height - 1;
 -	u32 btotal = 0;
 -	int ret, state;
 -	int mp_h; /* metapath buffers are read in to this height */
 -	sector_t last_ra = 0;
 -	u64 prev_bnr = 0;
 -	bool preserve1; /* need to preserve the first meta pointer? */
 -
 -	if (!newsize)
 +	struct metapath mp;
 +	int error;
 +
 +	if (!size)
  		lblock = 0;
  	else
 -		lblock = (newsize - 1) >> sdp->sd_sb.sb_bsize_shift;
 +		lblock = (size - 1) >> sdp->sd_sb.sb_bsize_shift;
  
 -	memset(&mp, 0, sizeof(mp));
  	find_metapath(sdp, lblock, &mp, ip->i_height);
 +	error = gfs2_rindex_update(sdp);
 +	if (error)
 +		return error;
  
 -	memcpy(&nbof, &mp.mp_list, sizeof(nbof));
 -
 -	ret = gfs2_meta_inode_buffer(ip, &dibh);
 -	if (ret)
 -		return ret;
 -
 -	mp.mp_bh[0] = dibh;
 -	ret = lookup_metapath(ip, &mp);
 -	if (ret == ip->i_height)
 -		state = DEALLOC_MP_FULL; /* We have a complete metapath */
 -	else
 -		state = DEALLOC_FILL_MP; /* deal with partial metapath */
 -
 -	ret = gfs2_rindex_update(sdp);
 -	if (ret)
 -		goto out_metapath;
 -
 -	ret = gfs2_quota_hold(ip, NO_UID_QUOTA_CHANGE, NO_GID_QUOTA_CHANGE);
 -	if (ret)
 -		goto out_metapath;
 -	gfs2_holder_mark_uninitialized(&rd_gh);
 -
 -	mp_h = strip_h;
 -
 -	while (state != DEALLOC_DONE) {
 -		switch (state) {
 -		/* Truncate a full metapath at the given strip height.
 -		 * Note that strip_h == mp_h in order to be in this state. */
 -		case DEALLOC_MP_FULL:
 -			if (mp_h > 0) { /* issue read-ahead on metadata */
 -				__be64 *top;
 -
 -				bh = mp.mp_bh[mp_h - 1];
 -				if (bh->b_blocknr != last_ra) {
 -					last_ra = bh->b_blocknr;
 -					top = metaptr1(mp_h - 1, &mp);
 -					gfs2_metapath_ra(ip->i_gl, bh, top);
 -				}
 -			}
 -			/* If we're truncating to a non-zero size and the mp is
 -			   at the beginning of file for the strip height, we
 -			   need to preserve the first metadata pointer. */
 -			preserve1 = (newsize &&
 -				     (mp.mp_list[mp_h] == nbof[mp_h]));
 -			bh = mp.mp_bh[mp_h];
 -			gfs2_assert_withdraw(sdp, bh);
 -			if (gfs2_assert_withdraw(sdp,
 -						 prev_bnr != bh->b_blocknr)) {
 -				printk(KERN_EMERG "GFS2: fsid=%s:inode %llu, "
 -				       "block:%llu, i_h:%u, s_h:%u, mp_h:%u\n",
 -				       sdp->sd_fsname,
 -				       (unsigned long long)ip->i_no_addr,
 -				       prev_bnr, ip->i_height, strip_h, mp_h);
 -			}
 -			prev_bnr = bh->b_blocknr;
 -			ret = sweep_bh_for_rgrps(ip, &rd_gh, &mp, &btotal,
 -						 mp_h, preserve1);
 -			/* If we hit an error or just swept dinode buffer,
 -			   just exit. */
 -			if (ret || !mp_h) {
 -				state = DEALLOC_DONE;
 -				break;
 -			}
 -			state = DEALLOC_MP_LOWER;
 -			break;
 +	error = gfs2_quota_hold(ip, NO_UID_QUOTA_CHANGE, NO_GID_QUOTA_CHANGE);
 +	if (error)
 +		return error;
  
 -		/* lower the metapath strip height */
 -		case DEALLOC_MP_LOWER:
 -			/* We're done with the current buffer, so release it,
 -			   unless it's the dinode buffer. Then back up to the
 -			   previous pointer. */
 -			if (mp_h) {
 -				brelse(mp.mp_bh[mp_h]);
 -				mp.mp_bh[mp_h] = NULL;
 -			}
 -			/* If we can't get any lower in height, we've stripped
 -			   off all we can. Next step is to back up and start
 -			   stripping the previous level of metadata. */
 -			if (mp_h == 0) {
 -				strip_h--;
 -				memcpy(&mp.mp_list, &nbof, sizeof(nbof));
 -				mp_h = strip_h;
 -				state = DEALLOC_FILL_MP;
 -				break;
 -			}
 -			mp.mp_list[mp_h] = 0;
 -			mp_h--; /* search one metadata height down */
 -			if (mp.mp_list[mp_h] >= hptrs(sdp, mp_h) - 1)
 -				break; /* loop around in the same state */
 -			mp.mp_list[mp_h]++;
 -			/* Here we've found a part of the metapath that is not
 -			 * allocated. We need to search at that height for the
 -			 * next non-null pointer. */
 -			if (find_nonnull_ptr(sdp, &mp, mp_h)) {
 -				state = DEALLOC_FILL_MP;
 -				mp_h++;
 -			}
 -			/* No more non-null pointers at this height. Back up
 -			   to the previous height and try again. */
 -			break; /* loop around in the same state */
 -
 -		/* Fill the metapath with buffers to the given height. */
 -		case DEALLOC_FILL_MP:
 -			/* Fill the buffers out to the current height. */
 -			ret = fillup_metapath(ip, &mp, mp_h);
 -			if (ret < 0)
 -				goto out;
 +	while (height--) {
 +		struct strip_mine sm;
 +		sm.sm_first = !!size;
 +		sm.sm_height = height;
  
 -			/* If buffers found for the entire strip height */
 -			if ((ret == ip->i_height) && (mp_h == strip_h)) {
 -				state = DEALLOC_MP_FULL;
 -				break;
 -			}
 -			if (ret < ip->i_height) /* We have a partial height */
 -				mp_h = ret - 1;
 -
 -			/* If we find a non-null block pointer, crawl a bit
 -			   higher up in the metapath and try again, otherwise
 -			   we need to look lower for a new starting point. */
 -			if (find_nonnull_ptr(sdp, &mp, mp_h))
 -				mp_h++;
 -			else
 -				state = DEALLOC_MP_LOWER;
 +		error = recursive_scan(ip, NULL, &mp, 0, 0, 1, &sm);
 +		if (error)
  			break;
 -		}
  	}
  
 -	if (btotal) {
 -		if (current->journal_info == NULL) {
 -			ret = gfs2_trans_begin(sdp, RES_DINODE + RES_STATFS +
 -					       RES_QUOTA, 0);
 -			if (ret)
 -				goto out;
 -			down_write(&ip->i_rw_mutex);
 -		}
 -		gfs2_statfs_change(sdp, 0, +btotal, 0);
 -		gfs2_quota_change(ip, -(s64)btotal, ip->i_inode.i_uid,
 -				  ip->i_inode.i_gid);
 -		ip->i_inode.i_mtime = ip->i_inode.i_ctime = CURRENT_TIME;
 -		gfs2_trans_add_meta(ip->i_gl, dibh);
 -		gfs2_dinode_out(ip, dibh->b_data);
 -		up_write(&ip->i_rw_mutex);
 -		gfs2_trans_end(sdp);
 -	}
 -
 -out:
 -	if (gfs2_holder_initialized(&rd_gh))
 -		gfs2_glock_dq_uninit(&rd_gh);
 -	if (current->journal_info) {
 -		up_write(&ip->i_rw_mutex);
 -		gfs2_trans_end(sdp);
 -		cond_resched();
 -	}
  	gfs2_quota_unhold(ip);
 -out_metapath:
 -	release_metapath(&mp);
 -	return ret;
 +
 +	return error;
  }
  
  static int trunc_end(struct gfs2_inode *ip)
diff --cc fs/gfs2/glops.c
index d2030314974c,5e69636d4dd3..000000000000
--- a/fs/gfs2/glops.c
+++ b/fs/gfs2/glops.c
@@@ -183,8 -182,9 +183,14 @@@ static void rgrp_go_sync(struct gfs2_gl
  
  static void rgrp_go_inval(struct gfs2_glock *gl, int flags)
  {
++<<<<<<< HEAD
 +	struct address_space *mapping = gfs2_glock2aspace(gl);
 +	struct gfs2_rgrpd *rgd = gl->gl_object;
++=======
+ 	struct gfs2_sbd *sdp = gl->gl_name.ln_sbd;
+ 	struct address_space *mapping = &sdp->sd_aspace;
+ 	struct gfs2_rgrpd *rgd = gfs2_glock2rgrp(gl);
++>>>>>>> 6f6597baae20 (gfs2: Protect gl->gl_object by spin lock)
  
  	if (rgd)
  		gfs2_rgrp_brelse(rgd);
diff --cc fs/gfs2/rgrp.c
index 1e6a443a7c59,836e38ba5d0a..000000000000
--- a/fs/gfs2/rgrp.c
+++ b/fs/gfs2/rgrp.c
@@@ -685,9 -705,7 +685,13 @@@ void gfs2_clear_rgrpd(struct gfs2_sbd *
  		rb_erase(n, &sdp->sd_rindex_tree);
  
  		if (gl) {
++<<<<<<< HEAD
 +			spin_lock(&gl->gl_spin);
 +			gl->gl_object = NULL;
 +			spin_unlock(&gl->gl_spin);
++=======
+ 			glock_set_object(gl, NULL);
++>>>>>>> 6f6597baae20 (gfs2: Protect gl->gl_object by spin lock)
  			gfs2_glock_add_to_lru(gl);
  			gfs2_glock_put(gl);
  		}
@@@ -892,7 -915,10 +896,14 @@@ static int read_rindex_entry(struct gfs
  	error = rgd_insert(rgd);
  	spin_unlock(&sdp->sd_rindex_spin);
  	if (!error) {
++<<<<<<< HEAD
 +		rgd->rd_gl->gl_object = rgd;
++=======
+ 		glock_set_object(rgd->rd_gl, rgd);
+ 		rgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;
+ 		rgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr +
+ 						    rgd->rd_length) * bsize) - 1;
++>>>>>>> 6f6597baae20 (gfs2: Protect gl->gl_object by spin lock)
  		return 0;
  	}
  
* Unmerged path fs/gfs2/bmap.c
diff --git a/fs/gfs2/dir.c b/fs/gfs2/dir.c
index 2f15db06abea..96c9c82a2678 100644
--- a/fs/gfs2/dir.c
+++ b/fs/gfs2/dir.c
@@ -2038,8 +2038,8 @@ static int leaf_dealloc(struct gfs2_inode *dip, u32 index, u32 len,
 	gfs2_rlist_alloc(&rlist, LM_ST_EXCLUSIVE);
 
 	for (x = 0; x < rlist.rl_rgrps; x++) {
-		struct gfs2_rgrpd *rgd;
-		rgd = rlist.rl_ghs[x].gh_gl->gl_object;
+		struct gfs2_rgrpd *rgd = gfs2_glock2rgrp(rlist.rl_ghs[x].gh_gl);
+
 		rg_blocks += rgd->rd_length;
 	}
 
* Unmerged path fs/gfs2/glops.c
diff --git a/fs/gfs2/incore.h b/fs/gfs2/incore.h
index 6f3cdb32bb5e..bd36329a96b8 100644
--- a/fs/gfs2/incore.h
+++ b/fs/gfs2/incore.h
@@ -835,5 +835,7 @@ static inline void gfs2_sbstats_inc(const struct gfs2_glock *gl, int which)
 	preempt_enable();
 }
 
+extern struct gfs2_rgrpd *gfs2_glock2rgrp(struct gfs2_glock *gl);
+
 #endif /* __INCORE_DOT_H__ */
 
diff --git a/fs/gfs2/inode.c b/fs/gfs2/inode.c
index 07d0d1279bc8..e9ce2a1ee405 100644
--- a/fs/gfs2/inode.c
+++ b/fs/gfs2/inode.c
@@ -202,14 +202,14 @@ struct inode *gfs2_inode_lookup(struct super_block *sb, unsigned int type,
 
 fail_refresh:
 	ip->i_iopen_gh.gh_flags |= GL_NOCACHE;
-	ip->i_iopen_gh.gh_gl->gl_object = NULL;
+	glock_set_object(ip->i_iopen_gh.gh_gl, NULL);
 	gfs2_glock_dq_uninit(&ip->i_iopen_gh);
 fail_put:
 	if (io_gl)
 		gfs2_glock_put(io_gl);
 	if (gfs2_holder_initialized(&i_gh))
 		gfs2_glock_dq_uninit(&i_gh);
-	ip->i_gl->gl_object = NULL;
+	glock_set_object(ip->i_gl, NULL);
 fail:
 	iget_failed(inode);
 	return ERR_PTR(error);
@@ -664,7 +664,7 @@ static int gfs2_create_inode(struct inode *dir, struct dentry *dentry,
 	if (error)
 		goto fail_free_inode;
 
-	ip->i_gl->gl_object = ip;
+	glock_set_object(ip->i_gl, ip);
 	error = gfs2_glock_nq_init(ip->i_gl, LM_ST_EXCLUSIVE, GL_SKIP, ghs + 1);
 	if (error)
 		goto fail_free_inode;
@@ -686,7 +686,7 @@ static int gfs2_create_inode(struct inode *dir, struct dentry *dentry,
 	if (error)
 		goto fail_gunlock2;
 
-	ip->i_iopen_gh.gh_gl->gl_object = ip;
+	glock_set_object(ip->i_iopen_gh.gh_gl, ip);
 	gfs2_glock_put(io_gl);
 	gfs2_set_iop(inode);
 	insert_inode_hash(inode);
diff --git a/fs/gfs2/lops.c b/fs/gfs2/lops.c
index 42a935df8730..d1690b19547e 100644
--- a/fs/gfs2/lops.c
+++ b/fs/gfs2/lops.c
@@ -70,7 +70,7 @@ static void maybe_release_space(struct gfs2_bufdata *bd)
 {
 	struct gfs2_glock *gl = bd->bd_gl;
 	struct gfs2_sbd *sdp = gl->gl_name.ln_sbd;
-	struct gfs2_rgrpd *rgd = gl->gl_object;
+	struct gfs2_rgrpd *rgd = gfs2_glock2rgrp(gl);
 	unsigned int index = bd->bd_bh->b_blocknr - gl->gl_name.ln_number;
 	struct gfs2_bitmap *bi = rgd->rd_bits + index;
 
* Unmerged path fs/gfs2/rgrp.c
diff --git a/fs/gfs2/super.c b/fs/gfs2/super.c
index 0557742eb825..a94b7bc27893 100644
--- a/fs/gfs2/super.c
+++ b/fs/gfs2/super.c
@@ -1062,9 +1062,12 @@ static int gfs2_statfs_slow(struct gfs2_sbd *sdp, struct gfs2_statfs_change_host
 					gfs2_holder_uninit(gh);
 					error = err;
 				} else {
-					if (!error)
-						error = statfs_slow_fill(
-							gh->gh_gl->gl_object, sc);
+					if (!error) {
+						struct gfs2_rgrpd *rgd =
+							gfs2_glock2rgrp(gh->gh_gl);
+
+						error = statfs_slow_fill(rgd, sc);
+					}
 					gfs2_glock_dq_uninit(gh);
 				}
 			}
@@ -1590,7 +1593,7 @@ out:
 	gfs2_glock_put(ip->i_gl);
 	ip->i_gl = NULL;
 	if (gfs2_holder_initialized(&ip->i_iopen_gh)) {
-		ip->i_iopen_gh.gh_gl->gl_object = NULL;
+		glock_set_object(ip->i_iopen_gh.gh_gl, NULL);
 		ip->i_iopen_gh.gh_flags |= GL_NOCACHE;
 		gfs2_glock_dq_uninit(&ip->i_iopen_gh);
 	}
diff --git a/fs/gfs2/xattr.c b/fs/gfs2/xattr.c
index 6780ed3f4094..3cf2ae7819d0 100644
--- a/fs/gfs2/xattr.c
+++ b/fs/gfs2/xattr.c
@@ -1339,8 +1339,8 @@ static int ea_dealloc_indirect(struct gfs2_inode *ip)
 	gfs2_rlist_alloc(&rlist, LM_ST_EXCLUSIVE);
 
 	for (x = 0; x < rlist.rl_rgrps; x++) {
-		struct gfs2_rgrpd *rgd;
-		rgd = rlist.rl_ghs[x].gh_gl->gl_object;
+		struct gfs2_rgrpd *rgd = gfs2_glock2rgrp(rlist.rl_ghs[x].gh_gl);
+
 		rg_blocks += rgd->rd_length;
 	}
 
