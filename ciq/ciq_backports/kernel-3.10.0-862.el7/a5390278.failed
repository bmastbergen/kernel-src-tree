mlxsw: spectrum: Add support for setting counters on nexthops

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Arkadi Sharshevsky <arkadis@mellanox.com>
commit a5390278a5eb573b76d2d28ce576b6b62c2200be
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/a5390278.failed

Add support for setting counters on nexthops based on dpipe's adjacency
table counter status. This patch also adds the ability for getting the
counter value, which will be used by the dpipe adjacency table dump
implementation in the next patches.

	Signed-off-by: Arkadi Sharshevsky <arkadis@mellanox.com>
	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit a5390278a5eb573b76d2d28ce576b6b62c2200be)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_router.h
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
index 6908726c154c,a75064a8ba80..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
@@@ -978,409 -1425,1789 +978,680 @@@ mlxsw_sp_router_neigh_entry_op4(struct 
  }
  
  static void
 -mlxsw_sp_neigh_counter_alloc(struct mlxsw_sp *mlxsw_sp,
 -			     struct mlxsw_sp_neigh_entry *neigh_entry)
 +mlxsw_sp_neigh_entry_update(struct mlxsw_sp *mlxsw_sp,
 +			    struct mlxsw_sp_neigh_entry *neigh_entry,
 +			    bool adding)
  {
 -	if (!mlxsw_sp_neigh_counter_should_alloc(mlxsw_sp, neigh_entry))
 -		return;
 -
 -	if (mlxsw_sp_flow_counter_alloc(mlxsw_sp, &neigh_entry->counter_index))
 +	if (!adding && !neigh_entry->connected)
  		return;
 -
 -	neigh_entry->counter_valid = true;
 +	neigh_entry->connected = adding;
 +	if (neigh_entry->key.n->tbl == &arp_tbl)
 +		mlxsw_sp_router_neigh_entry_op4(mlxsw_sp, neigh_entry,
 +						mlxsw_sp_rauht_op(adding));
 +	else
 +		WARN_ON_ONCE(1);
  }
  
 -static void
 -mlxsw_sp_neigh_counter_free(struct mlxsw_sp *mlxsw_sp,
 -			    struct mlxsw_sp_neigh_entry *neigh_entry)
 -{
 -	if (!neigh_entry->counter_valid)
 -		return;
 -	mlxsw_sp_flow_counter_free(mlxsw_sp,
 -				   neigh_entry->counter_index);
 -	neigh_entry->counter_valid = false;
 -}
 +struct mlxsw_sp_neigh_event_work {
 +	struct work_struct work;
 +	struct mlxsw_sp *mlxsw_sp;
 +	struct neighbour *n;
 +};
  
 -static struct mlxsw_sp_neigh_entry *
 -mlxsw_sp_neigh_entry_create(struct mlxsw_sp *mlxsw_sp, struct neighbour *n)
 +static void mlxsw_sp_router_neigh_event_work(struct work_struct *work)
  {
 +	struct mlxsw_sp_neigh_event_work *neigh_work =
 +		container_of(work, struct mlxsw_sp_neigh_event_work, work);
 +	struct mlxsw_sp *mlxsw_sp = neigh_work->mlxsw_sp;
  	struct mlxsw_sp_neigh_entry *neigh_entry;
 -	struct mlxsw_sp_rif *rif;
 -	int err;
 -
 -	rif = mlxsw_sp_rif_find_by_dev(mlxsw_sp, n->dev);
 -	if (!rif)
 -		return ERR_PTR(-EINVAL);
 +	struct neighbour *n = neigh_work->n;
 +	unsigned char ha[ETH_ALEN];
 +	bool entry_connected;
 +	u8 nud_state, dead;
  
 -	neigh_entry = mlxsw_sp_neigh_entry_alloc(mlxsw_sp, n, rif->rif_index);
 -	if (!neigh_entry)
 -		return ERR_PTR(-ENOMEM);
 +	/* If these parameters are changed after we release the lock,
 +	 * then we are guaranteed to receive another event letting us
 +	 * know about it.
 +	 */
 +	read_lock_bh(&n->lock);
 +	memcpy(ha, n->ha, ETH_ALEN);
 +	nud_state = n->nud_state;
 +	dead = n->dead;
 +	read_unlock_bh(&n->lock);
  
 -	err = mlxsw_sp_neigh_entry_insert(mlxsw_sp, neigh_entry);
 -	if (err)
 -		goto err_neigh_entry_insert;
 +	rtnl_lock();
 +	entry_connected = nud_state & NUD_VALID && !dead;
 +	neigh_entry = mlxsw_sp_neigh_entry_lookup(mlxsw_sp, n);
 +	if (!entry_connected && !neigh_entry)
 +		goto out;
 +	if (!neigh_entry) {
 +		neigh_entry = mlxsw_sp_neigh_entry_create(mlxsw_sp, n);
 +		if (IS_ERR(neigh_entry))
 +			goto out;
 +	}
  
 -	mlxsw_sp_neigh_counter_alloc(mlxsw_sp, neigh_entry);
 -	list_add(&neigh_entry->rif_list_node, &rif->neigh_list);
 +	memcpy(neigh_entry->ha, ha, ETH_ALEN);
 +	mlxsw_sp_neigh_entry_update(mlxsw_sp, neigh_entry, entry_connected);
 +	mlxsw_sp_nexthop_neigh_update(mlxsw_sp, neigh_entry, !entry_connected);
  
 -	return neigh_entry;
 +	if (!neigh_entry->connected && list_empty(&neigh_entry->nexthop_list))
 +		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
  
 -err_neigh_entry_insert:
 -	mlxsw_sp_neigh_entry_free(neigh_entry);
 -	return ERR_PTR(err);
 +out:
 +	rtnl_unlock();
 +	neigh_release(n);
 +	kfree(neigh_work);
  }
  
 -static void
 -mlxsw_sp_neigh_entry_destroy(struct mlxsw_sp *mlxsw_sp,
 -			     struct mlxsw_sp_neigh_entry *neigh_entry)
 +int mlxsw_sp_router_netevent_event(struct notifier_block *unused,
 +				   unsigned long event, void *ptr)
  {
 -	list_del(&neigh_entry->rif_list_node);
 -	mlxsw_sp_neigh_counter_free(mlxsw_sp, neigh_entry);
 -	mlxsw_sp_neigh_entry_remove(mlxsw_sp, neigh_entry);
 -	mlxsw_sp_neigh_entry_free(neigh_entry);
 -}
 +	struct mlxsw_sp_neigh_event_work *neigh_work;
 +	struct mlxsw_sp_port *mlxsw_sp_port;
 +	struct mlxsw_sp *mlxsw_sp;
 +	unsigned long interval;
 +	struct neigh_parms *p;
 +	struct neighbour *n;
  
 -static struct mlxsw_sp_neigh_entry *
 -mlxsw_sp_neigh_entry_lookup(struct mlxsw_sp *mlxsw_sp, struct neighbour *n)
 -{
 -	struct mlxsw_sp_neigh_key key;
 +	switch (event) {
 +	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
 +		p = ptr;
  
 -	key.n = n;
 -	return rhashtable_lookup_fast(&mlxsw_sp->router->neigh_ht,
 -				      &key, mlxsw_sp_neigh_ht_params);
 -}
 +		/* We don't care about changes in the default table. */
 +		if (!p->dev || p->tbl != &arp_tbl)
 +			return NOTIFY_DONE;
  
 -static void
 -mlxsw_sp_router_neighs_update_interval_init(struct mlxsw_sp *mlxsw_sp)
 -{
 -	unsigned long interval;
 +		/* We are in atomic context and can't take RTNL mutex,
 +		 * so use RCU variant to walk the device chain.
 +		 */
 +		mlxsw_sp_port = mlxsw_sp_port_lower_dev_hold(p->dev);
 +		if (!mlxsw_sp_port)
 +			return NOTIFY_DONE;
  
 -#if IS_ENABLED(CONFIG_IPV6)
 -	interval = min_t(unsigned long,
 -			 NEIGH_VAR(&arp_tbl.parms, DELAY_PROBE_TIME),
 -			 NEIGH_VAR(&nd_tbl.parms, DELAY_PROBE_TIME));
 -#else
 -	interval = NEIGH_VAR(&arp_tbl.parms, DELAY_PROBE_TIME);
 -#endif
 -	mlxsw_sp->router->neighs_update.interval = jiffies_to_msecs(interval);
 -}
 +		mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 +		interval = jiffies_to_msecs(NEIGH_VAR(p, DELAY_PROBE_TIME));
 +		mlxsw_sp->router.neighs_update.interval = interval;
  
 -static void mlxsw_sp_router_neigh_ent_ipv4_process(struct mlxsw_sp *mlxsw_sp,
 -						   char *rauhtd_pl,
 -						   int ent_index)
 -{
 -	struct net_device *dev;
 -	struct neighbour *n;
 -	__be32 dipn;
 -	u32 dip;
 -	u16 rif;
 +		mlxsw_sp_port_dev_put(mlxsw_sp_port);
 +		break;
 +	case NETEVENT_NEIGH_UPDATE:
 +		n = ptr;
  
 -	mlxsw_reg_rauhtd_ent_ipv4_unpack(rauhtd_pl, ent_index, &rif, &dip);
 +		if (n->tbl != &arp_tbl)
 +			return NOTIFY_DONE;
  
 -	if (!mlxsw_sp->router->rifs[rif]) {
 -		dev_err_ratelimited(mlxsw_sp->bus_info->dev, "Incorrect RIF in neighbour entry\n");
 -		return;
 -	}
 +		mlxsw_sp_port = mlxsw_sp_port_lower_dev_hold(n->dev);
 +		if (!mlxsw_sp_port)
 +			return NOTIFY_DONE;
  
 -	dipn = htonl(dip);
 -	dev = mlxsw_sp->router->rifs[rif]->dev;
 -	n = neigh_lookup(&arp_tbl, &dipn, dev);
 -	if (!n) {
 -		netdev_err(dev, "Failed to find matching neighbour for IP=%pI4h\n",
 -			   &dip);
 -		return;
 +		neigh_work = kzalloc(sizeof(*neigh_work), GFP_ATOMIC);
 +		if (!neigh_work) {
 +			mlxsw_sp_port_dev_put(mlxsw_sp_port);
 +			return NOTIFY_BAD;
 +		}
 +
 +		INIT_WORK(&neigh_work->work, mlxsw_sp_router_neigh_event_work);
 +		neigh_work->mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 +		neigh_work->n = n;
 +
 +		/* Take a reference to ensure the neighbour won't be
 +		 * destructed until we drop the reference in delayed
 +		 * work.
 +		 */
 +		neigh_clone(n);
 +		mlxsw_core_schedule_work(&neigh_work->work);
 +		mlxsw_sp_port_dev_put(mlxsw_sp_port);
 +		break;
  	}
  
 -	netdev_dbg(dev, "Updating neighbour with IP=%pI4h\n", &dip);
 -	neigh_event_send(n, NULL);
 -	neigh_release(n);
 +	return NOTIFY_DONE;
  }
  
 -#if IS_ENABLED(CONFIG_IPV6)
 -static void mlxsw_sp_router_neigh_ent_ipv6_process(struct mlxsw_sp *mlxsw_sp,
 -						   char *rauhtd_pl,
 -						   int rec_index)
 +static int mlxsw_sp_neigh_init(struct mlxsw_sp *mlxsw_sp)
  {
 -	struct net_device *dev;
 -	struct neighbour *n;
 -	struct in6_addr dip;
 -	u16 rif;
 -
 -	mlxsw_reg_rauhtd_ent_ipv6_unpack(rauhtd_pl, rec_index, &rif,
 -					 (char *) &dip);
 +	int err;
  
 -	if (!mlxsw_sp->router->rifs[rif]) {
 -		dev_err_ratelimited(mlxsw_sp->bus_info->dev, "Incorrect RIF in neighbour entry\n");
 -		return;
 -	}
 +	err = rhashtable_init(&mlxsw_sp->router.neigh_ht,
 +			      &mlxsw_sp_neigh_ht_params);
 +	if (err)
 +		return err;
  
 -	dev = mlxsw_sp->router->rifs[rif]->dev;
 -	n = neigh_lookup(&nd_tbl, &dip, dev);
 -	if (!n) {
 -		netdev_err(dev, "Failed to find matching neighbour for IP=%pI6c\n",
 -			   &dip);
 -		return;
 -	}
 +	/* Initialize the polling interval according to the default
 +	 * table.
 +	 */
 +	mlxsw_sp_router_neighs_update_interval_init(mlxsw_sp);
  
 -	netdev_dbg(dev, "Updating neighbour with IP=%pI6c\n", &dip);
 -	neigh_event_send(n, NULL);
 -	neigh_release(n);
 +	/* Create the delayed works for the activity_update */
 +	INIT_DELAYED_WORK(&mlxsw_sp->router.neighs_update.dw,
 +			  mlxsw_sp_router_neighs_update_work);
 +	INIT_DELAYED_WORK(&mlxsw_sp->router.nexthop_probe_dw,
 +			  mlxsw_sp_router_probe_unresolved_nexthops);
 +	mlxsw_core_schedule_dw(&mlxsw_sp->router.neighs_update.dw, 0);
 +	mlxsw_core_schedule_dw(&mlxsw_sp->router.nexthop_probe_dw, 0);
 +	return 0;
  }
 -#else
 -static void mlxsw_sp_router_neigh_ent_ipv6_process(struct mlxsw_sp *mlxsw_sp,
 -						   char *rauhtd_pl,
 -						   int rec_index)
 +
 +static void mlxsw_sp_neigh_fini(struct mlxsw_sp *mlxsw_sp)
  {
 +	cancel_delayed_work_sync(&mlxsw_sp->router.neighs_update.dw);
 +	cancel_delayed_work_sync(&mlxsw_sp->router.nexthop_probe_dw);
 +	rhashtable_destroy(&mlxsw_sp->router.neigh_ht);
  }
 -#endif
  
 -static void mlxsw_sp_router_neigh_rec_ipv4_process(struct mlxsw_sp *mlxsw_sp,
 -						   char *rauhtd_pl,
 -						   int rec_index)
 +static int mlxsw_sp_neigh_rif_flush(struct mlxsw_sp *mlxsw_sp,
 +				    const struct mlxsw_sp_rif *r)
  {
 -	u8 num_entries;
 -	int i;
 -
 -	num_entries = mlxsw_reg_rauhtd_ipv4_rec_num_entries_get(rauhtd_pl,
 -								rec_index);
 -	/* Hardware starts counting at 0, so add 1. */
 -	num_entries++;
 -
 -	/* Each record consists of several neighbour entries. */
 -	for (i = 0; i < num_entries; i++) {
 -		int ent_index;
 -
 -		ent_index = rec_index * MLXSW_REG_RAUHTD_IPV4_ENT_PER_REC + i;
 -		mlxsw_sp_router_neigh_ent_ipv4_process(mlxsw_sp, rauhtd_pl,
 -						       ent_index);
 -	}
 +	char rauht_pl[MLXSW_REG_RAUHT_LEN];
  
 +	mlxsw_reg_rauht_pack(rauht_pl, MLXSW_REG_RAUHT_OP_WRITE_DELETE_ALL,
 +			     r->rif, r->addr);
 +	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(rauht), rauht_pl);
  }
  
 -static void mlxsw_sp_router_neigh_rec_ipv6_process(struct mlxsw_sp *mlxsw_sp,
 -						   char *rauhtd_pl,
 -						   int rec_index)
 +static void mlxsw_sp_neigh_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 +					 struct mlxsw_sp_rif *r)
  {
 -	/* One record contains one entry. */
 -	mlxsw_sp_router_neigh_ent_ipv6_process(mlxsw_sp, rauhtd_pl,
 -					       rec_index);
 -}
 +	struct mlxsw_sp_neigh_entry *neigh_entry, *tmp;
  
 -static void mlxsw_sp_router_neigh_rec_process(struct mlxsw_sp *mlxsw_sp,
 -					      char *rauhtd_pl, int rec_index)
 -{
 -	switch (mlxsw_reg_rauhtd_rec_type_get(rauhtd_pl, rec_index)) {
 -	case MLXSW_REG_RAUHTD_TYPE_IPV4:
 -		mlxsw_sp_router_neigh_rec_ipv4_process(mlxsw_sp, rauhtd_pl,
 -						       rec_index);
 -		break;
 -	case MLXSW_REG_RAUHTD_TYPE_IPV6:
 -		mlxsw_sp_router_neigh_rec_ipv6_process(mlxsw_sp, rauhtd_pl,
 -						       rec_index);
 -		break;
 -	}
 +	mlxsw_sp_neigh_rif_flush(mlxsw_sp, r);
 +	list_for_each_entry_safe(neigh_entry, tmp, &r->neigh_list,
 +				 rif_list_node)
 +		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
  }
  
 -static bool mlxsw_sp_router_rauhtd_is_full(char *rauhtd_pl)
 -{
 -	u8 num_rec, last_rec_index, num_entries;
 +struct mlxsw_sp_nexthop_key {
 +	struct fib_nh *fib_nh;
 +};
  
 -	num_rec = mlxsw_reg_rauhtd_num_rec_get(rauhtd_pl);
 -	last_rec_index = num_rec - 1;
 +struct mlxsw_sp_nexthop {
 +	struct list_head neigh_list_node; /* member of neigh entry list */
 +	struct list_head rif_list_node;
 +	struct mlxsw_sp_nexthop_group *nh_grp; /* pointer back to the group
 +						* this belongs to
 +						*/
 +	struct rhash_head ht_node;
 +	struct mlxsw_sp_nexthop_key key;
 +	struct mlxsw_sp_rif *r;
 +	u8 should_offload:1, /* set indicates this neigh is connected and
 +			      * should be put to KVD linear area of this group.
 +			      */
 +	   offloaded:1, /* set in case the neigh is actually put into
 +			 * KVD linear area of this group.
 +			 */
 +	   update:1; /* set indicates that MAC of this neigh should be
 +		      * updated in HW
 +		      */
++<<<<<<< HEAD
 +	struct mlxsw_sp_neigh_entry *neigh_entry;
 +};
  
 -	if (num_rec < MLXSW_REG_RAUHTD_REC_MAX_NUM)
 -		return false;
 -	if (mlxsw_reg_rauhtd_rec_type_get(rauhtd_pl, last_rec_index) ==
 -	    MLXSW_REG_RAUHTD_TYPE_IPV6)
 -		return true;
 +struct mlxsw_sp_nexthop_group_key {
 +	struct fib_info *fi;
++=======
++	enum mlxsw_sp_nexthop_type type;
++	union {
++		struct mlxsw_sp_neigh_entry *neigh_entry;
++		struct mlxsw_sp_ipip_entry *ipip_entry;
++	};
++	unsigned int counter_index;
++	bool counter_valid;
++>>>>>>> a5390278a5eb (mlxsw: spectrum: Add support for setting counters on nexthops)
 +};
  
 -	num_entries = mlxsw_reg_rauhtd_ipv4_rec_num_entries_get(rauhtd_pl,
 -								last_rec_index);
 -	if (++num_entries == MLXSW_REG_RAUHTD_IPV4_ENT_PER_REC)
 -		return true;
 -	return false;
 -}
 +struct mlxsw_sp_nexthop_group {
 +	struct rhash_head ht_node;
 +	struct list_head fib_list; /* list of fib entries that use this group */
 +	struct mlxsw_sp_nexthop_group_key key;
 +	u8 adj_index_valid:1,
 +	   gateway:1; /* routes using the group use a gateway */
 +	u32 adj_index;
 +	u16 ecmp_size;
 +	u16 count;
 +	struct mlxsw_sp_nexthop nexthops[0];
 +#define nh_rif	nexthops[0].r
 +};
  
 -static int
 -__mlxsw_sp_router_neighs_update_rauhtd(struct mlxsw_sp *mlxsw_sp,
 -				       char *rauhtd_pl,
 -				       enum mlxsw_reg_rauhtd_type type)
++<<<<<<< HEAD
++=======
++static void mlxsw_sp_nexthop_counter_alloc(struct mlxsw_sp *mlxsw_sp,
++					   struct mlxsw_sp_nexthop *nh)
+ {
 -	int i, num_rec;
 -	int err;
++	struct devlink *devlink;
+ 
 -	/* Make sure the neighbour's netdev isn't removed in the
 -	 * process.
 -	 */
 -	rtnl_lock();
 -	do {
 -		mlxsw_reg_rauhtd_pack(rauhtd_pl, type);
 -		err = mlxsw_reg_query(mlxsw_sp->core, MLXSW_REG(rauhtd),
 -				      rauhtd_pl);
 -		if (err) {
 -			dev_err_ratelimited(mlxsw_sp->bus_info->dev, "Failed to dump neighbour talbe\n");
 -			break;
 -		}
 -		num_rec = mlxsw_reg_rauhtd_num_rec_get(rauhtd_pl);
 -		for (i = 0; i < num_rec; i++)
 -			mlxsw_sp_router_neigh_rec_process(mlxsw_sp, rauhtd_pl,
 -							  i);
 -	} while (mlxsw_sp_router_rauhtd_is_full(rauhtd_pl));
 -	rtnl_unlock();
++	devlink = priv_to_devlink(mlxsw_sp->core);
++	if (!devlink_dpipe_table_counter_enabled(devlink,
++						 MLXSW_SP_DPIPE_TABLE_NAME_ADJ))
++		return;
+ 
 -	return err;
++	if (mlxsw_sp_flow_counter_alloc(mlxsw_sp, &nh->counter_index))
++		return;
++
++	nh->counter_valid = true;
+ }
+ 
 -static int mlxsw_sp_router_neighs_update_rauhtd(struct mlxsw_sp *mlxsw_sp)
++static void mlxsw_sp_nexthop_counter_free(struct mlxsw_sp *mlxsw_sp,
++					  struct mlxsw_sp_nexthop *nh)
+ {
 -	enum mlxsw_reg_rauhtd_type type;
 -	char *rauhtd_pl;
 -	int err;
 -
 -	rauhtd_pl = kmalloc(MLXSW_REG_RAUHTD_LEN, GFP_KERNEL);
 -	if (!rauhtd_pl)
 -		return -ENOMEM;
++	if (!nh->counter_valid)
++		return;
++	mlxsw_sp_flow_counter_free(mlxsw_sp, nh->counter_index);
++	nh->counter_valid = false;
++}
+ 
 -	type = MLXSW_REG_RAUHTD_TYPE_IPV4;
 -	err = __mlxsw_sp_router_neighs_update_rauhtd(mlxsw_sp, rauhtd_pl, type);
 -	if (err)
 -		goto out;
++int mlxsw_sp_nexthop_counter_get(struct mlxsw_sp *mlxsw_sp,
++				 struct mlxsw_sp_nexthop *nh, u64 *p_counter)
++{
++	if (!nh->counter_valid)
++		return -EINVAL;
+ 
 -	type = MLXSW_REG_RAUHTD_TYPE_IPV6;
 -	err = __mlxsw_sp_router_neighs_update_rauhtd(mlxsw_sp, rauhtd_pl, type);
 -out:
 -	kfree(rauhtd_pl);
 -	return err;
++	return mlxsw_sp_flow_counter_get(mlxsw_sp, nh->counter_index,
++					 p_counter, NULL);
+ }
+ 
 -static void mlxsw_sp_router_neighs_update_nh(struct mlxsw_sp *mlxsw_sp)
++struct mlxsw_sp_nexthop *mlxsw_sp_nexthop_next(struct mlxsw_sp_router *router,
++					       struct mlxsw_sp_nexthop *nh)
+ {
 -	struct mlxsw_sp_neigh_entry *neigh_entry;
 -
 -	/* Take RTNL mutex here to prevent lists from changes */
 -	rtnl_lock();
 -	list_for_each_entry(neigh_entry, &mlxsw_sp->router->nexthop_neighs_list,
 -			    nexthop_neighs_list_node)
 -		/* If this neigh have nexthops, make the kernel think this neigh
 -		 * is active regardless of the traffic.
 -		 */
 -		neigh_event_send(neigh_entry->key.n, NULL);
 -	rtnl_unlock();
++	if (!nh) {
++		if (list_empty(&router->nexthop_list))
++			return NULL;
++		else
++			return list_first_entry(&router->nexthop_list,
++						typeof(*nh), router_list_node);
++	}
++	if (list_is_last(&nh->router_list_node, &router->nexthop_list))
++		return NULL;
++	return list_next_entry(nh, router_list_node);
+ }
+ 
 -static void
 -mlxsw_sp_router_neighs_update_work_schedule(struct mlxsw_sp *mlxsw_sp)
++bool mlxsw_sp_nexthop_offload(struct mlxsw_sp_nexthop *nh)
+ {
 -	unsigned long interval = mlxsw_sp->router->neighs_update.interval;
++	return nh->offloaded;
++}
+ 
 -	mlxsw_core_schedule_dw(&mlxsw_sp->router->neighs_update.dw,
 -			       msecs_to_jiffies(interval));
++unsigned char *mlxsw_sp_nexthop_ha(struct mlxsw_sp_nexthop *nh)
++{
++	if (!nh->offloaded)
++		return NULL;
++	return nh->neigh_entry->ha;
+ }
+ 
 -static void mlxsw_sp_router_neighs_update_work(struct work_struct *work)
++int mlxsw_sp_nexthop_indexes(struct mlxsw_sp_nexthop *nh, u32 *p_adj_index,
++			     u32 *p_adj_hash_index)
+ {
 -	struct mlxsw_sp_router *router;
 -	int err;
++	struct mlxsw_sp_nexthop_group *nh_grp = nh->nh_grp;
++	u32 adj_hash_index = 0;
++	int i;
+ 
 -	router = container_of(work, struct mlxsw_sp_router,
 -			      neighs_update.dw.work);
 -	err = mlxsw_sp_router_neighs_update_rauhtd(router->mlxsw_sp);
 -	if (err)
 -		dev_err(router->mlxsw_sp->bus_info->dev, "Could not update kernel for neigh activity");
++	if (!nh->offloaded || !nh_grp->adj_index_valid)
++		return -EINVAL;
++
++	*p_adj_index = nh_grp->adj_index;
+ 
 -	mlxsw_sp_router_neighs_update_nh(router->mlxsw_sp);
++	for (i = 0; i < nh_grp->count; i++) {
++		struct mlxsw_sp_nexthop *nh_iter = &nh_grp->nexthops[i];
++
++		if (nh_iter == nh)
++			break;
++		if (nh_iter->offloaded)
++			adj_hash_index++;
++	}
+ 
 -	mlxsw_sp_router_neighs_update_work_schedule(router->mlxsw_sp);
++	*p_adj_hash_index = adj_hash_index;
++	return 0;
+ }
+ 
 -static void mlxsw_sp_router_probe_unresolved_nexthops(struct work_struct *work)
++struct mlxsw_sp_rif *mlxsw_sp_nexthop_rif(struct mlxsw_sp_nexthop *nh)
+ {
 -	struct mlxsw_sp_neigh_entry *neigh_entry;
 -	struct mlxsw_sp_router *router;
++	return nh->rif;
++}
+ 
 -	router = container_of(work, struct mlxsw_sp_router,
 -			      nexthop_probe_dw.work);
 -	/* Iterate over nexthop neighbours, find those who are unresolved and
 -	 * send arp on them. This solves the chicken-egg problem when
 -	 * the nexthop wouldn't get offloaded until the neighbor is resolved
 -	 * but it wouldn't get resolved ever in case traffic is flowing in HW
 -	 * using different nexthop.
 -	 *
 -	 * Take RTNL mutex here to prevent lists from changes.
 -	 */
 -	rtnl_lock();
 -	list_for_each_entry(neigh_entry, &router->nexthop_neighs_list,
 -			    nexthop_neighs_list_node)
 -		if (!neigh_entry->connected)
 -			neigh_event_send(neigh_entry->key.n, NULL);
 -	rtnl_unlock();
++bool mlxsw_sp_nexthop_group_has_ipip(struct mlxsw_sp_nexthop *nh)
++{
++	struct mlxsw_sp_nexthop_group *nh_grp = nh->nh_grp;
++	int i;
+ 
 -	mlxsw_core_schedule_dw(&router->nexthop_probe_dw,
 -			       MLXSW_SP_UNRESOLVED_NH_PROBE_INTERVAL);
 -}
++	for (i = 0; i < nh_grp->count; i++) {
++		struct mlxsw_sp_nexthop *nh_iter = &nh_grp->nexthops[i];
+ 
 -static void
 -mlxsw_sp_nexthop_neigh_update(struct mlxsw_sp *mlxsw_sp,
 -			      struct mlxsw_sp_neigh_entry *neigh_entry,
 -			      bool removing);
++		if (nh_iter->type == MLXSW_SP_NEXTHOP_TYPE_IPIP)
++			return true;
++	}
++	return false;
++}
+ 
 -static enum mlxsw_reg_rauht_op mlxsw_sp_rauht_op(bool adding)
++static struct fib_info *
++mlxsw_sp_nexthop4_group_fi(const struct mlxsw_sp_nexthop_group *nh_grp)
+ {
 -	return adding ? MLXSW_REG_RAUHT_OP_WRITE_ADD :
 -			MLXSW_REG_RAUHT_OP_WRITE_DELETE;
++	return nh_grp->priv;
+ }
+ 
 -static void
 -mlxsw_sp_router_neigh_entry_op4(struct mlxsw_sp *mlxsw_sp,
 -				struct mlxsw_sp_neigh_entry *neigh_entry,
 -				enum mlxsw_reg_rauht_op op)
 -{
 -	struct neighbour *n = neigh_entry->key.n;
 -	u32 dip = ntohl(*((__be32 *) n->primary_key));
 -	char rauht_pl[MLXSW_REG_RAUHT_LEN];
 -
 -	mlxsw_reg_rauht_pack4(rauht_pl, op, neigh_entry->rif, neigh_entry->ha,
 -			      dip);
 -	if (neigh_entry->counter_valid)
 -		mlxsw_reg_rauht_pack_counter(rauht_pl,
 -					     neigh_entry->counter_index);
 -	mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(rauht), rauht_pl);
 -}
++struct mlxsw_sp_nexthop_group_cmp_arg {
++	enum mlxsw_sp_l3proto proto;
++	union {
++		struct fib_info *fi;
++		struct mlxsw_sp_fib6_entry *fib6_entry;
++	};
++};
+ 
 -static void
 -mlxsw_sp_router_neigh_entry_op6(struct mlxsw_sp *mlxsw_sp,
 -				struct mlxsw_sp_neigh_entry *neigh_entry,
 -				enum mlxsw_reg_rauht_op op)
++static bool
++mlxsw_sp_nexthop6_group_has_nexthop(const struct mlxsw_sp_nexthop_group *nh_grp,
++				    const struct in6_addr *gw, int ifindex)
+ {
 -	struct neighbour *n = neigh_entry->key.n;
 -	char rauht_pl[MLXSW_REG_RAUHT_LEN];
 -	const char *dip = n->primary_key;
++	int i;
+ 
 -	mlxsw_reg_rauht_pack6(rauht_pl, op, neigh_entry->rif, neigh_entry->ha,
 -			      dip);
 -	if (neigh_entry->counter_valid)
 -		mlxsw_reg_rauht_pack_counter(rauht_pl,
 -					     neigh_entry->counter_index);
 -	mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(rauht), rauht_pl);
 -}
++	for (i = 0; i < nh_grp->count; i++) {
++		const struct mlxsw_sp_nexthop *nh;
+ 
 -bool mlxsw_sp_neigh_ipv6_ignore(struct mlxsw_sp_neigh_entry *neigh_entry)
 -{
 -	struct neighbour *n = neigh_entry->key.n;
++		nh = &nh_grp->nexthops[i];
++		if (nh->ifindex == ifindex &&
++		    ipv6_addr_equal(gw, (struct in6_addr *) nh->gw_addr))
++			return true;
++	}
+ 
 -	/* Packets with a link-local destination address are trapped
 -	 * after LPM lookup and never reach the neighbour table, so
 -	 * there is no need to program such neighbours to the device.
 -	 */
 -	if (ipv6_addr_type((struct in6_addr *) &n->primary_key) &
 -	    IPV6_ADDR_LINKLOCAL)
 -		return true;
+ 	return false;
+ }
+ 
 -static void
 -mlxsw_sp_neigh_entry_update(struct mlxsw_sp *mlxsw_sp,
 -			    struct mlxsw_sp_neigh_entry *neigh_entry,
 -			    bool adding)
 -{
 -	if (!adding && !neigh_entry->connected)
 -		return;
 -	neigh_entry->connected = adding;
 -	if (neigh_entry->key.n->tbl->family == AF_INET) {
 -		mlxsw_sp_router_neigh_entry_op4(mlxsw_sp, neigh_entry,
 -						mlxsw_sp_rauht_op(adding));
 -	} else if (neigh_entry->key.n->tbl->family == AF_INET6) {
 -		if (mlxsw_sp_neigh_ipv6_ignore(neigh_entry))
 -			return;
 -		mlxsw_sp_router_neigh_entry_op6(mlxsw_sp, neigh_entry,
 -						mlxsw_sp_rauht_op(adding));
 -	} else {
 -		WARN_ON_ONCE(1);
 -	}
 -}
 -
 -void
 -mlxsw_sp_neigh_entry_counter_update(struct mlxsw_sp *mlxsw_sp,
 -				    struct mlxsw_sp_neigh_entry *neigh_entry,
 -				    bool adding)
++static bool
++mlxsw_sp_nexthop6_group_cmp(const struct mlxsw_sp_nexthop_group *nh_grp,
++			    const struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	if (adding)
 -		mlxsw_sp_neigh_counter_alloc(mlxsw_sp, neigh_entry);
 -	else
 -		mlxsw_sp_neigh_counter_free(mlxsw_sp, neigh_entry);
 -	mlxsw_sp_neigh_entry_update(mlxsw_sp, neigh_entry, true);
 -}
 -
 -struct mlxsw_sp_neigh_event_work {
 -	struct work_struct work;
 -	struct mlxsw_sp *mlxsw_sp;
 -	struct neighbour *n;
 -};
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
+ 
 -static void mlxsw_sp_router_neigh_event_work(struct work_struct *work)
 -{
 -	struct mlxsw_sp_neigh_event_work *neigh_work =
 -		container_of(work, struct mlxsw_sp_neigh_event_work, work);
 -	struct mlxsw_sp *mlxsw_sp = neigh_work->mlxsw_sp;
 -	struct mlxsw_sp_neigh_entry *neigh_entry;
 -	struct neighbour *n = neigh_work->n;
 -	unsigned char ha[ETH_ALEN];
 -	bool entry_connected;
 -	u8 nud_state, dead;
++	if (nh_grp->count != fib6_entry->nrt6)
++		return false;
+ 
 -	/* If these parameters are changed after we release the lock,
 -	 * then we are guaranteed to receive another event letting us
 -	 * know about it.
 -	 */
 -	read_lock_bh(&n->lock);
 -	memcpy(ha, n->ha, ETH_ALEN);
 -	nud_state = n->nud_state;
 -	dead = n->dead;
 -	read_unlock_bh(&n->lock);
++	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
++		struct in6_addr *gw;
++		int ifindex;
+ 
 -	rtnl_lock();
 -	entry_connected = nud_state & NUD_VALID && !dead;
 -	neigh_entry = mlxsw_sp_neigh_entry_lookup(mlxsw_sp, n);
 -	if (!entry_connected && !neigh_entry)
 -		goto out;
 -	if (!neigh_entry) {
 -		neigh_entry = mlxsw_sp_neigh_entry_create(mlxsw_sp, n);
 -		if (IS_ERR(neigh_entry))
 -			goto out;
++		ifindex = mlxsw_sp_rt6->rt->dst.dev->ifindex;
++		gw = &mlxsw_sp_rt6->rt->rt6i_gateway;
++		if (!mlxsw_sp_nexthop6_group_has_nexthop(nh_grp, gw, ifindex))
++			return false;
+ 	}
+ 
 -	memcpy(neigh_entry->ha, ha, ETH_ALEN);
 -	mlxsw_sp_neigh_entry_update(mlxsw_sp, neigh_entry, entry_connected);
 -	mlxsw_sp_nexthop_neigh_update(mlxsw_sp, neigh_entry, !entry_connected);
 -
 -	if (!neigh_entry->connected && list_empty(&neigh_entry->nexthop_list))
 -		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
 -
 -out:
 -	rtnl_unlock();
 -	neigh_release(n);
 -	kfree(neigh_work);
++	return true;
+ }
+ 
 -int mlxsw_sp_router_netevent_event(struct notifier_block *unused,
 -				   unsigned long event, void *ptr)
++static int
++mlxsw_sp_nexthop_group_cmp(struct rhashtable_compare_arg *arg, const void *ptr)
+ {
 -	struct mlxsw_sp_neigh_event_work *neigh_work;
 -	struct mlxsw_sp_port *mlxsw_sp_port;
 -	struct mlxsw_sp *mlxsw_sp;
 -	unsigned long interval;
 -	struct neigh_parms *p;
 -	struct neighbour *n;
 -
 -	switch (event) {
 -	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
 -		p = ptr;
 -
 -		/* We don't care about changes in the default table. */
 -		if (!p->dev || (p->tbl->family != AF_INET &&
 -				p->tbl->family != AF_INET6))
 -			return NOTIFY_DONE;
 -
 -		/* We are in atomic context and can't take RTNL mutex,
 -		 * so use RCU variant to walk the device chain.
 -		 */
 -		mlxsw_sp_port = mlxsw_sp_port_lower_dev_hold(p->dev);
 -		if (!mlxsw_sp_port)
 -			return NOTIFY_DONE;
 -
 -		mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 -		interval = jiffies_to_msecs(NEIGH_VAR(p, DELAY_PROBE_TIME));
 -		mlxsw_sp->router->neighs_update.interval = interval;
++	const struct mlxsw_sp_nexthop_group_cmp_arg *cmp_arg = arg->key;
++	const struct mlxsw_sp_nexthop_group *nh_grp = ptr;
+ 
 -		mlxsw_sp_port_dev_put(mlxsw_sp_port);
 -		break;
 -	case NETEVENT_NEIGH_UPDATE:
 -		n = ptr;
++	switch (cmp_arg->proto) {
++	case MLXSW_SP_L3_PROTO_IPV4:
++		return cmp_arg->fi != mlxsw_sp_nexthop4_group_fi(nh_grp);
++	case MLXSW_SP_L3_PROTO_IPV6:
++		return !mlxsw_sp_nexthop6_group_cmp(nh_grp,
++						    cmp_arg->fib6_entry);
++	default:
++		WARN_ON(1);
++		return 1;
++	}
++}
+ 
 -		if (n->tbl->family != AF_INET && n->tbl->family != AF_INET6)
 -			return NOTIFY_DONE;
++static int
++mlxsw_sp_nexthop_group_type(const struct mlxsw_sp_nexthop_group *nh_grp)
++{
++	return nh_grp->neigh_tbl->family;
++}
+ 
 -		mlxsw_sp_port = mlxsw_sp_port_lower_dev_hold(n->dev);
 -		if (!mlxsw_sp_port)
 -			return NOTIFY_DONE;
++static u32 mlxsw_sp_nexthop_group_hash_obj(const void *data, u32 len, u32 seed)
++{
++	const struct mlxsw_sp_nexthop_group *nh_grp = data;
++	const struct mlxsw_sp_nexthop *nh;
++	struct fib_info *fi;
++	unsigned int val;
++	int i;
+ 
 -		neigh_work = kzalloc(sizeof(*neigh_work), GFP_ATOMIC);
 -		if (!neigh_work) {
 -			mlxsw_sp_port_dev_put(mlxsw_sp_port);
 -			return NOTIFY_BAD;
++	switch (mlxsw_sp_nexthop_group_type(nh_grp)) {
++	case AF_INET:
++		fi = mlxsw_sp_nexthop4_group_fi(nh_grp);
++		return jhash(&fi, sizeof(fi), seed);
++	case AF_INET6:
++		val = nh_grp->count;
++		for (i = 0; i < nh_grp->count; i++) {
++			nh = &nh_grp->nexthops[i];
++			val ^= nh->ifindex;
+ 		}
 -
 -		INIT_WORK(&neigh_work->work, mlxsw_sp_router_neigh_event_work);
 -		neigh_work->mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 -		neigh_work->n = n;
 -
 -		/* Take a reference to ensure the neighbour won't be
 -		 * destructed until we drop the reference in delayed
 -		 * work.
 -		 */
 -		neigh_clone(n);
 -		mlxsw_core_schedule_work(&neigh_work->work);
 -		mlxsw_sp_port_dev_put(mlxsw_sp_port);
 -		break;
++		return jhash(&val, sizeof(val), seed);
++	default:
++		WARN_ON(1);
++		return 0;
+ 	}
 -
 -	return NOTIFY_DONE;
+ }
+ 
 -static int mlxsw_sp_neigh_init(struct mlxsw_sp *mlxsw_sp)
++static u32
++mlxsw_sp_nexthop6_group_hash(struct mlxsw_sp_fib6_entry *fib6_entry, u32 seed)
+ {
 -	int err;
 -
 -	err = rhashtable_init(&mlxsw_sp->router->neigh_ht,
 -			      &mlxsw_sp_neigh_ht_params);
 -	if (err)
 -		return err;
 -
 -	/* Initialize the polling interval according to the default
 -	 * table.
 -	 */
 -	mlxsw_sp_router_neighs_update_interval_init(mlxsw_sp);
++	unsigned int val = fib6_entry->nrt6;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
++	struct net_device *dev;
+ 
 -	/* Create the delayed works for the activity_update */
 -	INIT_DELAYED_WORK(&mlxsw_sp->router->neighs_update.dw,
 -			  mlxsw_sp_router_neighs_update_work);
 -	INIT_DELAYED_WORK(&mlxsw_sp->router->nexthop_probe_dw,
 -			  mlxsw_sp_router_probe_unresolved_nexthops);
 -	mlxsw_core_schedule_dw(&mlxsw_sp->router->neighs_update.dw, 0);
 -	mlxsw_core_schedule_dw(&mlxsw_sp->router->nexthop_probe_dw, 0);
 -	return 0;
 -}
++	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
++		dev = mlxsw_sp_rt6->rt->dst.dev;
++		val ^= dev->ifindex;
++	}
+ 
 -static void mlxsw_sp_neigh_fini(struct mlxsw_sp *mlxsw_sp)
 -{
 -	cancel_delayed_work_sync(&mlxsw_sp->router->neighs_update.dw);
 -	cancel_delayed_work_sync(&mlxsw_sp->router->nexthop_probe_dw);
 -	rhashtable_destroy(&mlxsw_sp->router->neigh_ht);
++	return jhash(&val, sizeof(val), seed);
+ }
+ 
 -static void mlxsw_sp_neigh_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 -					 struct mlxsw_sp_rif *rif)
++static u32
++mlxsw_sp_nexthop_group_hash(const void *data, u32 len, u32 seed)
+ {
 -	struct mlxsw_sp_neigh_entry *neigh_entry, *tmp;
++	const struct mlxsw_sp_nexthop_group_cmp_arg *cmp_arg = data;
+ 
 -	list_for_each_entry_safe(neigh_entry, tmp, &rif->neigh_list,
 -				 rif_list_node) {
 -		mlxsw_sp_neigh_entry_update(mlxsw_sp, neigh_entry, false);
 -		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
++	switch (cmp_arg->proto) {
++	case MLXSW_SP_L3_PROTO_IPV4:
++		return jhash(&cmp_arg->fi, sizeof(cmp_arg->fi), seed);
++	case MLXSW_SP_L3_PROTO_IPV6:
++		return mlxsw_sp_nexthop6_group_hash(cmp_arg->fib6_entry, seed);
++	default:
++		WARN_ON(1);
++		return 0;
+ 	}
+ }
+ 
 -enum mlxsw_sp_nexthop_type {
 -	MLXSW_SP_NEXTHOP_TYPE_ETH,
 -	MLXSW_SP_NEXTHOP_TYPE_IPIP,
++>>>>>>> a5390278a5eb (mlxsw: spectrum: Add support for setting counters on nexthops)
 +static const struct rhashtable_params mlxsw_sp_nexthop_group_ht_params = {
 +	.key_offset = offsetof(struct mlxsw_sp_nexthop_group, key),
 +	.head_offset = offsetof(struct mlxsw_sp_nexthop_group, ht_node),
 +	.key_len = sizeof(struct mlxsw_sp_nexthop_group_key),
  };
  
 -struct mlxsw_sp_nexthop_key {
 -	struct fib_nh *fib_nh;
 -};
 -
 -struct mlxsw_sp_nexthop {
 -	struct list_head neigh_list_node; /* member of neigh entry list */
 -	struct list_head rif_list_node;
 -	struct list_head router_list_node;
 -	struct mlxsw_sp_nexthop_group *nh_grp; /* pointer back to the group
 -						* this belongs to
 -						*/
 -	struct rhash_head ht_node;
 -	struct mlxsw_sp_nexthop_key key;
 -	unsigned char gw_addr[sizeof(struct in6_addr)];
 -	int ifindex;
 -	struct mlxsw_sp_rif *rif;
 -	u8 should_offload:1, /* set indicates this neigh is connected and
 -			      * should be put to KVD linear area of this group.
 -			      */
 -	   offloaded:1, /* set in case the neigh is actually put into
 -			 * KVD linear area of this group.
 -			 */
 -	   update:1; /* set indicates that MAC of this neigh should be
 -		      * updated in HW
 -		      */
 -	enum mlxsw_sp_nexthop_type type;
 -	union {
 -		struct mlxsw_sp_neigh_entry *neigh_entry;
 -		struct mlxsw_sp_ipip_entry *ipip_entry;
 -	};
 -	unsigned int counter_index;
 -	bool counter_valid;
 -};
 -
 -struct mlxsw_sp_nexthop_group {
 -	void *priv;
 -	struct rhash_head ht_node;
 -	struct list_head fib_list; /* list of fib entries that use this group */
 -	struct neigh_table *neigh_tbl;
 -	u8 adj_index_valid:1,
 -	   gateway:1; /* routes using the group use a gateway */
 -	u32 adj_index;
 -	u16 ecmp_size;
 -	u16 count;
 -	struct mlxsw_sp_nexthop nexthops[0];
 -#define nh_rif	nexthops[0].rif
 -};
 -
 -static void mlxsw_sp_nexthop_counter_alloc(struct mlxsw_sp *mlxsw_sp,
 -					   struct mlxsw_sp_nexthop *nh)
 +static int mlxsw_sp_nexthop_group_insert(struct mlxsw_sp *mlxsw_sp,
 +					 struct mlxsw_sp_nexthop_group *nh_grp)
  {
 -	struct devlink *devlink;
 -
 -	devlink = priv_to_devlink(mlxsw_sp->core);
 -	if (!devlink_dpipe_table_counter_enabled(devlink,
 -						 MLXSW_SP_DPIPE_TABLE_NAME_ADJ))
 -		return;
 -
 -	if (mlxsw_sp_flow_counter_alloc(mlxsw_sp, &nh->counter_index))
 -		return;
 -
 -	nh->counter_valid = true;
 +	return rhashtable_insert_fast(&mlxsw_sp->router.nexthop_group_ht,
 +				      &nh_grp->ht_node,
 +				      mlxsw_sp_nexthop_group_ht_params);
  }
  
 -static void mlxsw_sp_nexthop_counter_free(struct mlxsw_sp *mlxsw_sp,
 -					  struct mlxsw_sp_nexthop *nh)
 +static void mlxsw_sp_nexthop_group_remove(struct mlxsw_sp *mlxsw_sp,
 +					  struct mlxsw_sp_nexthop_group *nh_grp)
  {
 -	if (!nh->counter_valid)
 -		return;
 -	mlxsw_sp_flow_counter_free(mlxsw_sp, nh->counter_index);
 -	nh->counter_valid = false;
 +	rhashtable_remove_fast(&mlxsw_sp->router.nexthop_group_ht,
 +			       &nh_grp->ht_node,
 +			       mlxsw_sp_nexthop_group_ht_params);
  }
  
 -int mlxsw_sp_nexthop_counter_get(struct mlxsw_sp *mlxsw_sp,
 -				 struct mlxsw_sp_nexthop *nh, u64 *p_counter)
 +static struct mlxsw_sp_nexthop_group *
 +mlxsw_sp_nexthop_group_lookup(struct mlxsw_sp *mlxsw_sp,
 +			      struct mlxsw_sp_nexthop_group_key key)
  {
 -	if (!nh->counter_valid)
 -		return -EINVAL;
 -
 -	return mlxsw_sp_flow_counter_get(mlxsw_sp, nh->counter_index,
 -					 p_counter, NULL);
 +	return rhashtable_lookup_fast(&mlxsw_sp->router.nexthop_group_ht, &key,
 +				      mlxsw_sp_nexthop_group_ht_params);
  }
  
 -struct mlxsw_sp_nexthop *mlxsw_sp_nexthop_next(struct mlxsw_sp_router *router,
 -					       struct mlxsw_sp_nexthop *nh)
 +static const struct rhashtable_params mlxsw_sp_nexthop_ht_params = {
 +	.key_offset = offsetof(struct mlxsw_sp_nexthop, key),
 +	.head_offset = offsetof(struct mlxsw_sp_nexthop, ht_node),
 +	.key_len = sizeof(struct mlxsw_sp_nexthop_key),
 +};
 +
 +static int mlxsw_sp_nexthop_insert(struct mlxsw_sp *mlxsw_sp,
 +				   struct mlxsw_sp_nexthop *nh)
  {
 -	if (!nh) {
 -		if (list_empty(&router->nexthop_list))
 -			return NULL;
 -		else
 -			return list_first_entry(&router->nexthop_list,
 -						typeof(*nh), router_list_node);
 -	}
 -	if (list_is_last(&nh->router_list_node, &router->nexthop_list))
 -		return NULL;
 -	return list_next_entry(nh, router_list_node);
 +	return rhashtable_insert_fast(&mlxsw_sp->router.nexthop_ht,
 +				      &nh->ht_node, mlxsw_sp_nexthop_ht_params);
  }
  
 -bool mlxsw_sp_nexthop_offload(struct mlxsw_sp_nexthop *nh)
 +static void mlxsw_sp_nexthop_remove(struct mlxsw_sp *mlxsw_sp,
 +				    struct mlxsw_sp_nexthop *nh)
  {
 -	return nh->offloaded;
 +	rhashtable_remove_fast(&mlxsw_sp->router.nexthop_ht, &nh->ht_node,
 +			       mlxsw_sp_nexthop_ht_params);
  }
  
 -unsigned char *mlxsw_sp_nexthop_ha(struct mlxsw_sp_nexthop *nh)
 +static struct mlxsw_sp_nexthop *
 +mlxsw_sp_nexthop_lookup(struct mlxsw_sp *mlxsw_sp,
 +			struct mlxsw_sp_nexthop_key key)
  {
 -	if (!nh->offloaded)
 -		return NULL;
 -	return nh->neigh_entry->ha;
 +	return rhashtable_lookup_fast(&mlxsw_sp->router.nexthop_ht, &key,
 +				      mlxsw_sp_nexthop_ht_params);
  }
  
 -int mlxsw_sp_nexthop_indexes(struct mlxsw_sp_nexthop *nh, u32 *p_adj_index,
 -			     u32 *p_adj_hash_index)
 +static int mlxsw_sp_adj_index_mass_update_vr(struct mlxsw_sp *mlxsw_sp,
 +					     const struct mlxsw_sp_fib *fib,
 +					     u32 adj_index, u16 ecmp_size,
 +					     u32 new_adj_index,
 +					     u16 new_ecmp_size)
  {
 -	struct mlxsw_sp_nexthop_group *nh_grp = nh->nh_grp;
 -	u32 adj_hash_index = 0;
 -	int i;
 -
 -	if (!nh->offloaded || !nh_grp->adj_index_valid)
 -		return -EINVAL;
 +	char raleu_pl[MLXSW_REG_RALEU_LEN];
  
 -	*p_adj_index = nh_grp->adj_index;
 +	mlxsw_reg_raleu_pack(raleu_pl,
 +			     (enum mlxsw_reg_ralxx_protocol) fib->proto,
 +			     fib->vr->id, adj_index, ecmp_size, new_adj_index,
 +			     new_ecmp_size);
 +	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(raleu), raleu_pl);
 +}
  
 -	for (i = 0; i < nh_grp->count; i++) {
 -		struct mlxsw_sp_nexthop *nh_iter = &nh_grp->nexthops[i];
 +static int mlxsw_sp_adj_index_mass_update(struct mlxsw_sp *mlxsw_sp,
 +					  struct mlxsw_sp_nexthop_group *nh_grp,
 +					  u32 old_adj_index, u16 old_ecmp_size)
 +{
 +	struct mlxsw_sp_fib_entry *fib_entry;
 +	struct mlxsw_sp_fib *fib = NULL;
 +	int err;
  
 -		if (nh_iter == nh)
 -			break;
 -		if (nh_iter->offloaded)
 -			adj_hash_index++;
 +	list_for_each_entry(fib_entry, &nh_grp->fib_list, nexthop_group_node) {
 +		if (fib == fib_entry->fib_node->fib)
 +			continue;
 +		fib = fib_entry->fib_node->fib;
 +		err = mlxsw_sp_adj_index_mass_update_vr(mlxsw_sp, fib,
 +							old_adj_index,
 +							old_ecmp_size,
 +							nh_grp->adj_index,
 +							nh_grp->ecmp_size);
 +		if (err)
 +			return err;
  	}
 -
 -	*p_adj_hash_index = adj_hash_index;
  	return 0;
  }
  
- static int mlxsw_sp_nexthop_mac_update(struct mlxsw_sp *mlxsw_sp, u32 adj_index,
- 				       struct mlxsw_sp_nexthop *nh)
 -struct mlxsw_sp_rif *mlxsw_sp_nexthop_rif(struct mlxsw_sp_nexthop *nh)
++static int mlxsw_sp_nexthop_update(struct mlxsw_sp *mlxsw_sp, u32 adj_index,
++				   struct mlxsw_sp_nexthop *nh)
  {
 -	return nh->rif;
 +	struct mlxsw_sp_neigh_entry *neigh_entry = nh->neigh_entry;
 +	char ratr_pl[MLXSW_REG_RATR_LEN];
 +
 +	mlxsw_reg_ratr_pack(ratr_pl, MLXSW_REG_RATR_OP_WRITE_WRITE_ENTRY,
 +			    true, MLXSW_REG_RATR_TYPE_ETHERNET,
 +			    adj_index, neigh_entry->rif);
 +	mlxsw_reg_ratr_eth_entry_pack(ratr_pl, neigh_entry->ha);
++	if (nh->counter_valid)
++		mlxsw_reg_ratr_counter_pack(ratr_pl, nh->counter_index, true);
++	else
++		mlxsw_reg_ratr_counter_pack(ratr_pl, 0, false);
++
 +	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ratr), ratr_pl);
  }
  
 -bool mlxsw_sp_nexthop_group_has_ipip(struct mlxsw_sp_nexthop *nh)
 +static int
 +mlxsw_sp_nexthop_group_mac_update(struct mlxsw_sp *mlxsw_sp,
 +				  struct mlxsw_sp_nexthop_group *nh_grp,
 +				  bool reallocate)
  {
 -	struct mlxsw_sp_nexthop_group *nh_grp = nh->nh_grp;
 +	u32 adj_index = nh_grp->adj_index; /* base */
 +	struct mlxsw_sp_nexthop *nh;
  	int i;
 +	int err;
  
  	for (i = 0; i < nh_grp->count; i++) {
 -		struct mlxsw_sp_nexthop *nh_iter = &nh_grp->nexthops[i];
 +		nh = &nh_grp->nexthops[i];
  
 -		if (nh_iter->type == MLXSW_SP_NEXTHOP_TYPE_IPIP)
 -			return true;
 -	}
 -	return false;
 -}
 +		if (!nh->should_offload) {
 +			nh->offloaded = 0;
 +			continue;
 +		}
  
 -static struct fib_info *
 -mlxsw_sp_nexthop4_group_fi(const struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	return nh_grp->priv;
 +		if (nh->update || reallocate) {
++<<<<<<< HEAD
 +			err = mlxsw_sp_nexthop_mac_update(mlxsw_sp,
 +							  adj_index, nh);
++=======
++			switch (nh->type) {
++			case MLXSW_SP_NEXTHOP_TYPE_ETH:
++				err = mlxsw_sp_nexthop_update
++					    (mlxsw_sp, adj_index, nh);
++				break;
++			case MLXSW_SP_NEXTHOP_TYPE_IPIP:
++				err = mlxsw_sp_nexthop_ipip_update
++					    (mlxsw_sp, adj_index, nh);
++				break;
++			}
++>>>>>>> a5390278a5eb (mlxsw: spectrum: Add support for setting counters on nexthops)
 +			if (err)
 +				return err;
 +			nh->update = 0;
 +			nh->offloaded = 1;
 +		}
 +		adj_index++;
 +	}
 +	return 0;
  }
  
 -struct mlxsw_sp_nexthop_group_cmp_arg {
 -	enum mlxsw_sp_l3proto proto;
 -	union {
 -		struct fib_info *fi;
 -		struct mlxsw_sp_fib6_entry *fib6_entry;
 -	};
 -};
 +static int mlxsw_sp_fib_entry_update(struct mlxsw_sp *mlxsw_sp,
 +				     struct mlxsw_sp_fib_entry *fib_entry);
  
  static bool
 -mlxsw_sp_nexthop6_group_has_nexthop(const struct mlxsw_sp_nexthop_group *nh_grp,
 -				    const struct in6_addr *gw, int ifindex)
 -{
 -	int i;
 +mlxsw_sp_fib_node_entry_is_first(const struct mlxsw_sp_fib_node *fib_node,
 +				 const struct mlxsw_sp_fib_entry *fib_entry);
  
 -	for (i = 0; i < nh_grp->count; i++) {
 -		const struct mlxsw_sp_nexthop *nh;
 +static int
 +mlxsw_sp_nexthop_fib_entries_update(struct mlxsw_sp *mlxsw_sp,
 +				    struct mlxsw_sp_nexthop_group *nh_grp)
 +{
 +	struct mlxsw_sp_fib_entry *fib_entry;
 +	int err;
  
 -		nh = &nh_grp->nexthops[i];
 -		if (nh->ifindex == ifindex &&
 -		    ipv6_addr_equal(gw, (struct in6_addr *) nh->gw_addr))
 -			return true;
 +	list_for_each_entry(fib_entry, &nh_grp->fib_list, nexthop_group_node) {
 +		if (!mlxsw_sp_fib_node_entry_is_first(fib_entry->fib_node,
 +						      fib_entry))
 +			continue;
 +		err = mlxsw_sp_fib_entry_update(mlxsw_sp, fib_entry);
 +		if (err)
 +			return err;
  	}
 -
 -	return false;
 +	return 0;
  }
  
 -static bool
 -mlxsw_sp_nexthop6_group_cmp(const struct mlxsw_sp_nexthop_group *nh_grp,
 -			    const struct mlxsw_sp_fib6_entry *fib6_entry)
 -{
 -	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
 -
 -	if (nh_grp->count != fib6_entry->nrt6)
 -		return false;
 -
 -	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
 -		struct in6_addr *gw;
 -		int ifindex;
 -
 -		ifindex = mlxsw_sp_rt6->rt->dst.dev->ifindex;
 -		gw = &mlxsw_sp_rt6->rt->rt6i_gateway;
 -		if (!mlxsw_sp_nexthop6_group_has_nexthop(nh_grp, gw, ifindex))
 -			return false;
 -	}
 -
 -	return true;
 -}
 -
 -static int
 -mlxsw_sp_nexthop_group_cmp(struct rhashtable_compare_arg *arg, const void *ptr)
 -{
 -	const struct mlxsw_sp_nexthop_group_cmp_arg *cmp_arg = arg->key;
 -	const struct mlxsw_sp_nexthop_group *nh_grp = ptr;
 -
 -	switch (cmp_arg->proto) {
 -	case MLXSW_SP_L3_PROTO_IPV4:
 -		return cmp_arg->fi != mlxsw_sp_nexthop4_group_fi(nh_grp);
 -	case MLXSW_SP_L3_PROTO_IPV6:
 -		return !mlxsw_sp_nexthop6_group_cmp(nh_grp,
 -						    cmp_arg->fib6_entry);
 -	default:
 -		WARN_ON(1);
 -		return 1;
 -	}
 -}
 -
 -static int
 -mlxsw_sp_nexthop_group_type(const struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	return nh_grp->neigh_tbl->family;
 -}
 -
 -static u32 mlxsw_sp_nexthop_group_hash_obj(const void *data, u32 len, u32 seed)
 -{
 -	const struct mlxsw_sp_nexthop_group *nh_grp = data;
 -	const struct mlxsw_sp_nexthop *nh;
 -	struct fib_info *fi;
 -	unsigned int val;
 -	int i;
 -
 -	switch (mlxsw_sp_nexthop_group_type(nh_grp)) {
 -	case AF_INET:
 -		fi = mlxsw_sp_nexthop4_group_fi(nh_grp);
 -		return jhash(&fi, sizeof(fi), seed);
 -	case AF_INET6:
 -		val = nh_grp->count;
 -		for (i = 0; i < nh_grp->count; i++) {
 -			nh = &nh_grp->nexthops[i];
 -			val ^= nh->ifindex;
 -		}
 -		return jhash(&val, sizeof(val), seed);
 -	default:
 -		WARN_ON(1);
 -		return 0;
 -	}
 -}
 -
 -static u32
 -mlxsw_sp_nexthop6_group_hash(struct mlxsw_sp_fib6_entry *fib6_entry, u32 seed)
 -{
 -	unsigned int val = fib6_entry->nrt6;
 -	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
 -	struct net_device *dev;
 -
 -	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
 -		dev = mlxsw_sp_rt6->rt->dst.dev;
 -		val ^= dev->ifindex;
 -	}
 -
 -	return jhash(&val, sizeof(val), seed);
 -}
 -
 -static u32
 -mlxsw_sp_nexthop_group_hash(const void *data, u32 len, u32 seed)
 -{
 -	const struct mlxsw_sp_nexthop_group_cmp_arg *cmp_arg = data;
 -
 -	switch (cmp_arg->proto) {
 -	case MLXSW_SP_L3_PROTO_IPV4:
 -		return jhash(&cmp_arg->fi, sizeof(cmp_arg->fi), seed);
 -	case MLXSW_SP_L3_PROTO_IPV6:
 -		return mlxsw_sp_nexthop6_group_hash(cmp_arg->fib6_entry, seed);
 -	default:
 -		WARN_ON(1);
 -		return 0;
 -	}
 -}
 -
 -static const struct rhashtable_params mlxsw_sp_nexthop_group_ht_params = {
 -	.head_offset = offsetof(struct mlxsw_sp_nexthop_group, ht_node),
 -	.hashfn	     = mlxsw_sp_nexthop_group_hash,
 -	.obj_hashfn  = mlxsw_sp_nexthop_group_hash_obj,
 -	.obj_cmpfn   = mlxsw_sp_nexthop_group_cmp,
 -};
 -
 -static int mlxsw_sp_nexthop_group_insert(struct mlxsw_sp *mlxsw_sp,
 -					 struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	if (mlxsw_sp_nexthop_group_type(nh_grp) == AF_INET6 &&
 -	    !nh_grp->gateway)
 -		return 0;
 -
 -	return rhashtable_insert_fast(&mlxsw_sp->router->nexthop_group_ht,
 -				      &nh_grp->ht_node,
 -				      mlxsw_sp_nexthop_group_ht_params);
 -}
 -
 -static void mlxsw_sp_nexthop_group_remove(struct mlxsw_sp *mlxsw_sp,
 -					  struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	if (mlxsw_sp_nexthop_group_type(nh_grp) == AF_INET6 &&
 -	    !nh_grp->gateway)
 -		return;
 -
 -	rhashtable_remove_fast(&mlxsw_sp->router->nexthop_group_ht,
 -			       &nh_grp->ht_node,
 -			       mlxsw_sp_nexthop_group_ht_params);
 -}
 -
 -static struct mlxsw_sp_nexthop_group *
 -mlxsw_sp_nexthop4_group_lookup(struct mlxsw_sp *mlxsw_sp,
 -			       struct fib_info *fi)
 -{
 -	struct mlxsw_sp_nexthop_group_cmp_arg cmp_arg;
 -
 -	cmp_arg.proto = MLXSW_SP_L3_PROTO_IPV4;
 -	cmp_arg.fi = fi;
 -	return rhashtable_lookup_fast(&mlxsw_sp->router->nexthop_group_ht,
 -				      &cmp_arg,
 -				      mlxsw_sp_nexthop_group_ht_params);
 -}
 -
 -static struct mlxsw_sp_nexthop_group *
 -mlxsw_sp_nexthop6_group_lookup(struct mlxsw_sp *mlxsw_sp,
 -			       struct mlxsw_sp_fib6_entry *fib6_entry)
 -{
 -	struct mlxsw_sp_nexthop_group_cmp_arg cmp_arg;
 -
 -	cmp_arg.proto = MLXSW_SP_L3_PROTO_IPV6;
 -	cmp_arg.fib6_entry = fib6_entry;
 -	return rhashtable_lookup_fast(&mlxsw_sp->router->nexthop_group_ht,
 -				      &cmp_arg,
 -				      mlxsw_sp_nexthop_group_ht_params);
 -}
 -
 -static const struct rhashtable_params mlxsw_sp_nexthop_ht_params = {
 -	.key_offset = offsetof(struct mlxsw_sp_nexthop, key),
 -	.head_offset = offsetof(struct mlxsw_sp_nexthop, ht_node),
 -	.key_len = sizeof(struct mlxsw_sp_nexthop_key),
 -};
 -
 -static int mlxsw_sp_nexthop_insert(struct mlxsw_sp *mlxsw_sp,
 -				   struct mlxsw_sp_nexthop *nh)
 -{
 -	return rhashtable_insert_fast(&mlxsw_sp->router->nexthop_ht,
 -				      &nh->ht_node, mlxsw_sp_nexthop_ht_params);
 -}
 -
 -static void mlxsw_sp_nexthop_remove(struct mlxsw_sp *mlxsw_sp,
 -				    struct mlxsw_sp_nexthop *nh)
 -{
 -	rhashtable_remove_fast(&mlxsw_sp->router->nexthop_ht, &nh->ht_node,
 -			       mlxsw_sp_nexthop_ht_params);
 -}
 -
 -static struct mlxsw_sp_nexthop *
 -mlxsw_sp_nexthop_lookup(struct mlxsw_sp *mlxsw_sp,
 -			struct mlxsw_sp_nexthop_key key)
 -{
 -	return rhashtable_lookup_fast(&mlxsw_sp->router->nexthop_ht, &key,
 -				      mlxsw_sp_nexthop_ht_params);
 -}
 -
 -static int mlxsw_sp_adj_index_mass_update_vr(struct mlxsw_sp *mlxsw_sp,
 -					     const struct mlxsw_sp_fib *fib,
 -					     u32 adj_index, u16 ecmp_size,
 -					     u32 new_adj_index,
 -					     u16 new_ecmp_size)
 -{
 -	char raleu_pl[MLXSW_REG_RALEU_LEN];
 -
 -	mlxsw_reg_raleu_pack(raleu_pl,
 -			     (enum mlxsw_reg_ralxx_protocol) fib->proto,
 -			     fib->vr->id, adj_index, ecmp_size, new_adj_index,
 -			     new_ecmp_size);
 -	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(raleu), raleu_pl);
 -}
 -
 -static int mlxsw_sp_adj_index_mass_update(struct mlxsw_sp *mlxsw_sp,
 -					  struct mlxsw_sp_nexthop_group *nh_grp,
 -					  u32 old_adj_index, u16 old_ecmp_size)
 -{
 -	struct mlxsw_sp_fib_entry *fib_entry;
 -	struct mlxsw_sp_fib *fib = NULL;
 -	int err;
 -
 -	list_for_each_entry(fib_entry, &nh_grp->fib_list, nexthop_group_node) {
 -		if (fib == fib_entry->fib_node->fib)
 -			continue;
 -		fib = fib_entry->fib_node->fib;
 -		err = mlxsw_sp_adj_index_mass_update_vr(mlxsw_sp, fib,
 -							old_adj_index,
 -							old_ecmp_size,
 -							nh_grp->adj_index,
 -							nh_grp->ecmp_size);
 -		if (err)
 -			return err;
 -	}
 -	return 0;
 -}
 -
 -static int mlxsw_sp_nexthop_update(struct mlxsw_sp *mlxsw_sp, u32 adj_index,
 -				   struct mlxsw_sp_nexthop *nh)
 -{
 -	struct mlxsw_sp_neigh_entry *neigh_entry = nh->neigh_entry;
 -	char ratr_pl[MLXSW_REG_RATR_LEN];
 -
 -	mlxsw_reg_ratr_pack(ratr_pl, MLXSW_REG_RATR_OP_WRITE_WRITE_ENTRY,
 -			    true, MLXSW_REG_RATR_TYPE_ETHERNET,
 -			    adj_index, neigh_entry->rif);
 -	mlxsw_reg_ratr_eth_entry_pack(ratr_pl, neigh_entry->ha);
 -	if (nh->counter_valid)
 -		mlxsw_reg_ratr_counter_pack(ratr_pl, nh->counter_index, true);
 -	else
 -		mlxsw_reg_ratr_counter_pack(ratr_pl, 0, false);
 -
 -	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ratr), ratr_pl);
 -}
 -
 -static int mlxsw_sp_nexthop_ipip_update(struct mlxsw_sp *mlxsw_sp,
 -					u32 adj_index,
 -					struct mlxsw_sp_nexthop *nh)
 -{
 -	const struct mlxsw_sp_ipip_ops *ipip_ops;
 -
 -	ipip_ops = mlxsw_sp->router->ipip_ops_arr[nh->ipip_entry->ipipt];
 -	return ipip_ops->nexthop_update(mlxsw_sp, adj_index, nh->ipip_entry);
 -}
 -
 -static int
 -mlxsw_sp_nexthop_group_update(struct mlxsw_sp *mlxsw_sp,
 -			      struct mlxsw_sp_nexthop_group *nh_grp,
 -			      bool reallocate)
 -{
 -	u32 adj_index = nh_grp->adj_index; /* base */
 -	struct mlxsw_sp_nexthop *nh;
 -	int i;
 -	int err;
 -
 -	for (i = 0; i < nh_grp->count; i++) {
 -		nh = &nh_grp->nexthops[i];
 -
 -		if (!nh->should_offload) {
 -			nh->offloaded = 0;
 -			continue;
 -		}
 -
 -		if (nh->update || reallocate) {
 -			switch (nh->type) {
 -			case MLXSW_SP_NEXTHOP_TYPE_ETH:
 -				err = mlxsw_sp_nexthop_update
 -					    (mlxsw_sp, adj_index, nh);
 -				break;
 -			case MLXSW_SP_NEXTHOP_TYPE_IPIP:
 -				err = mlxsw_sp_nexthop_ipip_update
 -					    (mlxsw_sp, adj_index, nh);
 -				break;
 -			}
 -			if (err)
 -				return err;
 -			nh->update = 0;
 -			nh->offloaded = 1;
 -		}
 -		adj_index++;
 -	}
 -	return 0;
 -}
 -
 -static bool
 -mlxsw_sp_fib_node_entry_is_first(const struct mlxsw_sp_fib_node *fib_node,
 -				 const struct mlxsw_sp_fib_entry *fib_entry);
 -
 -static int
 -mlxsw_sp_nexthop_fib_entries_update(struct mlxsw_sp *mlxsw_sp,
 -				    struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	struct mlxsw_sp_fib_entry *fib_entry;
 -	int err;
 -
 -	list_for_each_entry(fib_entry, &nh_grp->fib_list, nexthop_group_node) {
 -		if (!mlxsw_sp_fib_node_entry_is_first(fib_entry->fib_node,
 -						      fib_entry))
 -			continue;
 -		err = mlxsw_sp_fib_entry_update(mlxsw_sp, fib_entry);
 -		if (err)
 -			return err;
 -	}
 -	return 0;
 -}
 -
 -static void
 -mlxsw_sp_fib_entry_offload_refresh(struct mlxsw_sp_fib_entry *fib_entry,
 -				   enum mlxsw_reg_ralue_op op, int err);
 -
 -static void
 -mlxsw_sp_nexthop_fib_entries_refresh(struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	enum mlxsw_reg_ralue_op op = MLXSW_REG_RALUE_OP_WRITE_WRITE;
 -	struct mlxsw_sp_fib_entry *fib_entry;
 -
 -	list_for_each_entry(fib_entry, &nh_grp->fib_list, nexthop_group_node) {
 -		if (!mlxsw_sp_fib_node_entry_is_first(fib_entry->fib_node,
 -						      fib_entry))
 -			continue;
 -		mlxsw_sp_fib_entry_offload_refresh(fib_entry, op, 0);
 -	}
 -}
 -
 -static void
 -mlxsw_sp_nexthop_group_refresh(struct mlxsw_sp *mlxsw_sp,
 -			       struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	struct mlxsw_sp_nexthop *nh;
 -	bool offload_change = false;
 -	u32 adj_index;
 -	u16 ecmp_size = 0;
 -	bool old_adj_index_valid;
 -	u32 old_adj_index;
 -	u16 old_ecmp_size;
 -	int i;
 -	int err;
 -
 -	if (!nh_grp->gateway) {
 -		mlxsw_sp_nexthop_fib_entries_update(mlxsw_sp, nh_grp);
 -		return;
 -	}
 -
 -	for (i = 0; i < nh_grp->count; i++) {
 -		nh = &nh_grp->nexthops[i];
 -
 -		if (nh->should_offload != nh->offloaded) {
 -			offload_change = true;
 -			if (nh->should_offload)
 -				nh->update = 1;
 -		}
 -		if (nh->should_offload)
 -			ecmp_size++;
 -	}
 -	if (!offload_change) {
 -		/* Nothing was added or removed, so no need to reallocate. Just
 -		 * update MAC on existing adjacency indexes.
 -		 */
 -		err = mlxsw_sp_nexthop_group_update(mlxsw_sp, nh_grp, false);
 -		if (err) {
 -			dev_warn(mlxsw_sp->bus_info->dev, "Failed to update neigh MAC in adjacency table.\n");
 -			goto set_trap;
 -		}
 -		return;
 -	}
 -	if (!ecmp_size)
 -		/* No neigh of this group is connected so we just set
 -		 * the trap and let everthing flow through kernel.
 -		 */
 -		goto set_trap;
 -
 -	err = mlxsw_sp_kvdl_alloc(mlxsw_sp, ecmp_size, &adj_index);
 -	if (err) {
 -		/* We ran out of KVD linear space, just set the
 -		 * trap and let everything flow through kernel.
 -		 */
 -		dev_warn(mlxsw_sp->bus_info->dev, "Failed to allocate KVD linear area for nexthop group.\n");
 -		goto set_trap;
 -	}
 -	old_adj_index_valid = nh_grp->adj_index_valid;
 -	old_adj_index = nh_grp->adj_index;
 -	old_ecmp_size = nh_grp->ecmp_size;
 -	nh_grp->adj_index_valid = 1;
 -	nh_grp->adj_index = adj_index;
 -	nh_grp->ecmp_size = ecmp_size;
 -	err = mlxsw_sp_nexthop_group_update(mlxsw_sp, nh_grp, true);
 -	if (err) {
 -		dev_warn(mlxsw_sp->bus_info->dev, "Failed to update neigh MAC in adjacency table.\n");
 -		goto set_trap;
 -	}
 -
 -	if (!old_adj_index_valid) {
 -		/* The trap was set for fib entries, so we have to call
 -		 * fib entry update to unset it and use adjacency index.
 -		 */
 -		err = mlxsw_sp_nexthop_fib_entries_update(mlxsw_sp, nh_grp);
 -		if (err) {
 -			dev_warn(mlxsw_sp->bus_info->dev, "Failed to add adjacency index to fib entries.\n");
 -			goto set_trap;
 -		}
 -		return;
 -	}
 -
 -	err = mlxsw_sp_adj_index_mass_update(mlxsw_sp, nh_grp,
 -					     old_adj_index, old_ecmp_size);
 -	mlxsw_sp_kvdl_free(mlxsw_sp, old_adj_index);
 -	if (err) {
 -		dev_warn(mlxsw_sp->bus_info->dev, "Failed to mass-update adjacency index for nexthop group.\n");
 -		goto set_trap;
 -	}
 -
 -	/* Offload state within the group changed, so update the flags. */
 -	mlxsw_sp_nexthop_fib_entries_refresh(nh_grp);
 -
 -	return;
 -
 -set_trap:
 -	old_adj_index_valid = nh_grp->adj_index_valid;
 -	nh_grp->adj_index_valid = 0;
 -	for (i = 0; i < nh_grp->count; i++) {
 -		nh = &nh_grp->nexthops[i];
 -		nh->offloaded = 0;
 -	}
 -	err = mlxsw_sp_nexthop_fib_entries_update(mlxsw_sp, nh_grp);
 -	if (err)
 -		dev_warn(mlxsw_sp->bus_info->dev, "Failed to set traps for fib entries.\n");
 -	if (old_adj_index_valid)
 -		mlxsw_sp_kvdl_free(mlxsw_sp, nh_grp->adj_index);
 -}
 -
 -static void __mlxsw_sp_nexthop_neigh_update(struct mlxsw_sp_nexthop *nh,
 -					    bool removing)
 -{
 -	if (!removing)
 -		nh->should_offload = 1;
 -	else if (nh->offloaded)
 -		nh->should_offload = 0;
 -	nh->update = 1;
 -}
 -
 -static void
 -mlxsw_sp_nexthop_neigh_update(struct mlxsw_sp *mlxsw_sp,
 -			      struct mlxsw_sp_neigh_entry *neigh_entry,
 -			      bool removing)
 -{
 -	struct mlxsw_sp_nexthop *nh;
 -
 -	list_for_each_entry(nh, &neigh_entry->nexthop_list,
 -			    neigh_list_node) {
 -		__mlxsw_sp_nexthop_neigh_update(nh, removing);
 -		mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh->nh_grp);
 -	}
 -}
 -
 -static void mlxsw_sp_nexthop_rif_init(struct mlxsw_sp_nexthop *nh,
 -				      struct mlxsw_sp_rif *rif)
 -{
 -	if (nh->rif)
 -		return;
 -
 -	nh->rif = rif;
 -	list_add(&nh->rif_list_node, &rif->nexthop_list);
 -}
 -
 -static void mlxsw_sp_nexthop_rif_fini(struct mlxsw_sp_nexthop *nh)
 -{
 -	if (!nh->rif)
 -		return;
 -
 -	list_del(&nh->rif_list_node);
 -	nh->rif = NULL;
 -}
 -
 -static int mlxsw_sp_nexthop_neigh_init(struct mlxsw_sp *mlxsw_sp,
 -				       struct mlxsw_sp_nexthop *nh)
 -{
 -	struct mlxsw_sp_neigh_entry *neigh_entry;
 -	struct neighbour *n;
 -	u8 nud_state, dead;
 -	int err;
 -
 -	if (!nh->nh_grp->gateway || nh->neigh_entry)
 -		return 0;
 -
 -	/* Take a reference of neigh here ensuring that neigh would
 -	 * not be destructed before the nexthop entry is finished.
 -	 * The reference is taken either in neigh_lookup() or
 -	 * in neigh_create() in case n is not found.
 -	 */
 -	n = neigh_lookup(nh->nh_grp->neigh_tbl, &nh->gw_addr, nh->rif->dev);
 -	if (!n) {
 -		n = neigh_create(nh->nh_grp->neigh_tbl, &nh->gw_addr,
 -				 nh->rif->dev);
 -		if (IS_ERR(n))
 -			return PTR_ERR(n);
 -		neigh_event_send(n, NULL);
 -	}
 -	neigh_entry = mlxsw_sp_neigh_entry_lookup(mlxsw_sp, n);
 -	if (!neigh_entry) {
 -		neigh_entry = mlxsw_sp_neigh_entry_create(mlxsw_sp, n);
 -		if (IS_ERR(neigh_entry)) {
 -			err = -EINVAL;
 -			goto err_neigh_entry_create;
 -		}
 -	}
 -
 -	/* If that is the first nexthop connected to that neigh, add to
 -	 * nexthop_neighs_list
 -	 */
 -	if (list_empty(&neigh_entry->nexthop_list))
 -		list_add_tail(&neigh_entry->nexthop_neighs_list_node,
 -			      &mlxsw_sp->router->nexthop_neighs_list);
 -
 -	nh->neigh_entry = neigh_entry;
 -	list_add_tail(&nh->neigh_list_node, &neigh_entry->nexthop_list);
 -	read_lock_bh(&n->lock);
 -	nud_state = n->nud_state;
 -	dead = n->dead;
 -	read_unlock_bh(&n->lock);
 -	__mlxsw_sp_nexthop_neigh_update(nh, !(nud_state & NUD_VALID && !dead));
 -
 -	return 0;
 -
 -err_neigh_entry_create:
 -	neigh_release(n);
 -	return err;
 -}
 -
 -static void mlxsw_sp_nexthop_neigh_fini(struct mlxsw_sp *mlxsw_sp,
 -					struct mlxsw_sp_nexthop *nh)
 -{
 -	struct mlxsw_sp_neigh_entry *neigh_entry = nh->neigh_entry;
 -	struct neighbour *n;
 -
 -	if (!neigh_entry)
 -		return;
 -	n = neigh_entry->key.n;
 -
 -	__mlxsw_sp_nexthop_neigh_update(nh, true);
 -	list_del(&nh->neigh_list_node);
 -	nh->neigh_entry = NULL;
 -
 -	/* If that is the last nexthop connected to that neigh, remove from
 -	 * nexthop_neighs_list
 -	 */
 -	if (list_empty(&neigh_entry->nexthop_list))
 -		list_del(&neigh_entry->nexthop_neighs_list_node);
 -
 -	if (!neigh_entry->connected && list_empty(&neigh_entry->nexthop_list))
 -		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
 -
 -	neigh_release(n);
 -}
 -
 -static bool mlxsw_sp_netdev_ipip_type(const struct mlxsw_sp *mlxsw_sp,
 -				      const struct net_device *dev,
 -				      enum mlxsw_sp_ipip_type *p_type)
 -{
 -	struct mlxsw_sp_router *router = mlxsw_sp->router;
 -	const struct mlxsw_sp_ipip_ops *ipip_ops;
 -	enum mlxsw_sp_ipip_type ipipt;
 -
 -	for (ipipt = 0; ipipt < MLXSW_SP_IPIP_TYPE_MAX; ++ipipt) {
 -		ipip_ops = router->ipip_ops_arr[ipipt];
 -		if (dev->type == ipip_ops->dev_type) {
 -			if (p_type)
 -				*p_type = ipipt;
 -			return true;
 -		}
 -	}
 -	return false;
 -}
 -
 -static int mlxsw_sp_nexthop_ipip_init(struct mlxsw_sp *mlxsw_sp,
 -				      enum mlxsw_sp_ipip_type ipipt,
 -				      struct mlxsw_sp_nexthop *nh,
 -				      struct net_device *ol_dev)
 -{
 -	if (!nh->nh_grp->gateway || nh->ipip_entry)
 -		return 0;
 -
 -	nh->ipip_entry = mlxsw_sp_ipip_entry_get(mlxsw_sp, ipipt, ol_dev);
 -	if (IS_ERR(nh->ipip_entry))
 -		return PTR_ERR(nh->ipip_entry);
 -
 -	__mlxsw_sp_nexthop_neigh_update(nh, false);
 -	return 0;
 -}
 -
 -static void mlxsw_sp_nexthop_ipip_fini(struct mlxsw_sp *mlxsw_sp,
 -				       struct mlxsw_sp_nexthop *nh)
 -{
 -	struct mlxsw_sp_ipip_entry *ipip_entry = nh->ipip_entry;
 -
 -	if (!ipip_entry)
 -		return;
 -
 -	__mlxsw_sp_nexthop_neigh_update(nh, true);
 -	mlxsw_sp_ipip_entry_put(mlxsw_sp, ipip_entry);
 -	nh->ipip_entry = NULL;
 -}
 -
 -static bool mlxsw_sp_nexthop4_ipip_type(const struct mlxsw_sp *mlxsw_sp,
 -					const struct fib_nh *fib_nh,
 -					enum mlxsw_sp_ipip_type *p_ipipt)
 -{
 -	struct net_device *dev = fib_nh->nh_dev;
 -
 -	return dev &&
 -	       fib_nh->nh_parent->fib_type == RTN_UNICAST &&
 -	       mlxsw_sp_netdev_ipip_type(mlxsw_sp, dev, p_ipipt);
 -}
 -
 -static void mlxsw_sp_nexthop_type_fini(struct mlxsw_sp *mlxsw_sp,
 -				       struct mlxsw_sp_nexthop *nh)
 -{
 -	switch (nh->type) {
 -	case MLXSW_SP_NEXTHOP_TYPE_ETH:
 -		mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
 -		mlxsw_sp_nexthop_rif_fini(nh);
 -		break;
 -	case MLXSW_SP_NEXTHOP_TYPE_IPIP:
 -		mlxsw_sp_nexthop_ipip_fini(mlxsw_sp, nh);
 -		break;
 -	}
 -}
 -
 -static int mlxsw_sp_nexthop4_type_init(struct mlxsw_sp *mlxsw_sp,
 -				       struct mlxsw_sp_nexthop *nh,
 -				       struct fib_nh *fib_nh)
 -{
 -	struct mlxsw_sp_router *router = mlxsw_sp->router;
 -	struct net_device *dev = fib_nh->nh_dev;
 -	enum mlxsw_sp_ipip_type ipipt;
 -	struct mlxsw_sp_rif *rif;
 -	int err;
 -
 -	if (mlxsw_sp_nexthop4_ipip_type(mlxsw_sp, fib_nh, &ipipt) &&
 -	    router->ipip_ops_arr[ipipt]->can_offload(mlxsw_sp, dev,
 -						     MLXSW_SP_L3_PROTO_IPV4)) {
 -		nh->type = MLXSW_SP_NEXTHOP_TYPE_IPIP;
 -		return mlxsw_sp_nexthop_ipip_init(mlxsw_sp, ipipt, nh, dev);
 -	}
 -
 -	nh->type = MLXSW_SP_NEXTHOP_TYPE_ETH;
 -	rif = mlxsw_sp_rif_find_by_dev(mlxsw_sp, dev);
 -	if (!rif)
 -		return 0;
 -
 -	mlxsw_sp_nexthop_rif_init(nh, rif);
 -	err = mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
 -	if (err)
 -		goto err_neigh_init;
 -
 -	return 0;
 -
 -err_neigh_init:
 -	mlxsw_sp_nexthop_rif_fini(nh);
 -	return err;
 -}
 -
 -static void mlxsw_sp_nexthop4_type_fini(struct mlxsw_sp *mlxsw_sp,
 -					struct mlxsw_sp_nexthop *nh)
 -{
 -	mlxsw_sp_nexthop_type_fini(mlxsw_sp, nh);
 -}
 -
 -static int mlxsw_sp_nexthop4_init(struct mlxsw_sp *mlxsw_sp,
 -				  struct mlxsw_sp_nexthop_group *nh_grp,
 -				  struct mlxsw_sp_nexthop *nh,
 -				  struct fib_nh *fib_nh)
 -{
 -	struct net_device *dev = fib_nh->nh_dev;
 -	struct in_device *in_dev;
 -	int err;
 -
 -	nh->nh_grp = nh_grp;
 -	nh->key.fib_nh = fib_nh;
 -	memcpy(&nh->gw_addr, &fib_nh->nh_gw, sizeof(fib_nh->nh_gw));
 -	err = mlxsw_sp_nexthop_insert(mlxsw_sp, nh);
 -	if (err)
 -		return err;
 -
 -	mlxsw_sp_nexthop_counter_alloc(mlxsw_sp, nh);
 -	list_add_tail(&nh->router_list_node, &mlxsw_sp->router->nexthop_list);
 -
 -	if (!dev)
 -		return 0;
 -
 -	in_dev = __in_dev_get_rtnl(dev);
 -	if (in_dev && IN_DEV_IGNORE_ROUTES_WITH_LINKDOWN(in_dev) &&
 -	    fib_nh->nh_flags & RTNH_F_LINKDOWN)
 -		return 0;
 -
 -	err = mlxsw_sp_nexthop4_type_init(mlxsw_sp, nh, fib_nh);
 -	if (err)
 -		goto err_nexthop_neigh_init;
 -
 -	return 0;
 -
 -err_nexthop_neigh_init:
 -	mlxsw_sp_nexthop_remove(mlxsw_sp, nh);
 -	return err;
 -}
 -
 -static void mlxsw_sp_nexthop4_fini(struct mlxsw_sp *mlxsw_sp,
 -				   struct mlxsw_sp_nexthop *nh)
 -{
 -	mlxsw_sp_nexthop4_type_fini(mlxsw_sp, nh);
 -	list_del(&nh->router_list_node);
 -	mlxsw_sp_nexthop_counter_free(mlxsw_sp, nh);
 -	mlxsw_sp_nexthop_remove(mlxsw_sp, nh);
 -}
 -
 -static void mlxsw_sp_nexthop4_event(struct mlxsw_sp *mlxsw_sp,
 -				    unsigned long event, struct fib_nh *fib_nh)
 -{
 -	struct mlxsw_sp_nexthop_key key;
 -	struct mlxsw_sp_nexthop *nh;
 -
 -	if (mlxsw_sp->router->aborted)
 -		return;
 -
 -	key.fib_nh = fib_nh;
 -	nh = mlxsw_sp_nexthop_lookup(mlxsw_sp, key);
 -	if (WARN_ON_ONCE(!nh))
 -		return;
 -
 -	switch (event) {
 -	case FIB_EVENT_NH_ADD:
 -		mlxsw_sp_nexthop4_type_init(mlxsw_sp, nh, fib_nh);
 -		break;
 -	case FIB_EVENT_NH_DEL:
 -		mlxsw_sp_nexthop4_type_fini(mlxsw_sp, nh);
 -		break;
 -	}
 -
 -	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh->nh_grp);
 -}
 -
 -static void mlxsw_sp_nexthop_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 -					   struct mlxsw_sp_rif *rif)
 -{
 -	struct mlxsw_sp_nexthop *nh, *tmp;
 -
 -	list_for_each_entry_safe(nh, tmp, &rif->nexthop_list, rif_list_node) {
 -		mlxsw_sp_nexthop_type_fini(mlxsw_sp, nh);
 -		mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh->nh_grp);
 -	}
 -}
 -
 -static bool mlxsw_sp_fi_is_gateway(const struct mlxsw_sp *mlxsw_sp,
 -				   const struct fib_info *fi)
 -{
 -	return fi->fib_nh->nh_scope == RT_SCOPE_LINK ||
 -	       mlxsw_sp_nexthop4_ipip_type(mlxsw_sp, fi->fib_nh, NULL);
 -}
 -
 -static struct mlxsw_sp_nexthop_group *
 -mlxsw_sp_nexthop4_group_create(struct mlxsw_sp *mlxsw_sp, struct fib_info *fi)
 -{
 -	struct mlxsw_sp_nexthop_group *nh_grp;
 -	struct mlxsw_sp_nexthop *nh;
 -	struct fib_nh *fib_nh;
 -	size_t alloc_size;
 -	int i;
 -	int err;
 -
 -	alloc_size = sizeof(*nh_grp) +
 -		     fi->fib_nhs * sizeof(struct mlxsw_sp_nexthop);
 -	nh_grp = kzalloc(alloc_size, GFP_KERNEL);
 -	if (!nh_grp)
 -		return ERR_PTR(-ENOMEM);
 -	nh_grp->priv = fi;
 -	INIT_LIST_HEAD(&nh_grp->fib_list);
 -	nh_grp->neigh_tbl = &arp_tbl;
 -
 -	nh_grp->gateway = mlxsw_sp_fi_is_gateway(mlxsw_sp, fi);
 -	nh_grp->count = fi->fib_nhs;
 -	fib_info_hold(fi);
 -	for (i = 0; i < nh_grp->count; i++) {
 -		nh = &nh_grp->nexthops[i];
 -		fib_nh = &fi->fib_nh[i];
 -		err = mlxsw_sp_nexthop4_init(mlxsw_sp, nh_grp, nh, fib_nh);
 -		if (err)
 -			goto err_nexthop4_init;
 -	}
 -	err = mlxsw_sp_nexthop_group_insert(mlxsw_sp, nh_grp);
 -	if (err)
 -		goto err_nexthop_group_insert;
 -	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
 -	return nh_grp;
 -
 -err_nexthop_group_insert:
 -err_nexthop4_init:
 -	for (i--; i >= 0; i--) {
 -		nh = &nh_grp->nexthops[i];
 -		mlxsw_sp_nexthop4_fini(mlxsw_sp, nh);
 -	}
 -	fib_info_put(fi);
 -	kfree(nh_grp);
 -	return ERR_PTR(err);
 -}
 -
 -static void
 -mlxsw_sp_nexthop4_group_destroy(struct mlxsw_sp *mlxsw_sp,
 -				struct mlxsw_sp_nexthop_group *nh_grp)
 -{
 -	struct mlxsw_sp_nexthop *nh;
 -	int i;
 -
 -	mlxsw_sp_nexthop_group_remove(mlxsw_sp, nh_grp);
 -	for (i = 0; i < nh_grp->count; i++) {
 -		nh = &nh_grp->nexthops[i];
 -		mlxsw_sp_nexthop4_fini(mlxsw_sp, nh);
 -	}
 -	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
 -	WARN_ON_ONCE(nh_grp->adj_index_valid);
 -	fib_info_put(mlxsw_sp_nexthop4_group_fi(nh_grp));
 -	kfree(nh_grp);
 -}
 -
 -static int mlxsw_sp_nexthop4_group_get(struct mlxsw_sp *mlxsw_sp,
 -				       struct mlxsw_sp_fib_entry *fib_entry,
 -				       struct fib_info *fi)
 -{
 -	struct mlxsw_sp_nexthop_group *nh_grp;
 -
 -	nh_grp = mlxsw_sp_nexthop4_group_lookup(mlxsw_sp, fi);
 -	if (!nh_grp) {
 -		nh_grp = mlxsw_sp_nexthop4_group_create(mlxsw_sp, fi);
 -		if (IS_ERR(nh_grp))
 -			return PTR_ERR(nh_grp);
 -	}
 -	list_add_tail(&fib_entry->nexthop_group_node, &nh_grp->fib_list);
 -	fib_entry->nh_group = nh_grp;
 -	return 0;
 -}
 -
 -static void mlxsw_sp_nexthop4_group_put(struct mlxsw_sp *mlxsw_sp,
 -					struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
 -
 -	list_del(&fib_entry->nexthop_group_node);
 -	if (!list_empty(&nh_grp->fib_list))
 -		return;
 -	mlxsw_sp_nexthop4_group_destroy(mlxsw_sp, nh_grp);
 -}
 -
 -static bool
 -mlxsw_sp_fib4_entry_should_offload(const struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	struct mlxsw_sp_fib4_entry *fib4_entry;
 -
 -	fib4_entry = container_of(fib_entry, struct mlxsw_sp_fib4_entry,
 -				  common);
 -	return !fib4_entry->tos;
 -}
 -
 -static bool
 -mlxsw_sp_fib_entry_should_offload(const struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	struct mlxsw_sp_nexthop_group *nh_group = fib_entry->nh_group;
 -
 -	switch (fib_entry->fib_node->fib->proto) {
 -	case MLXSW_SP_L3_PROTO_IPV4:
 -		if (!mlxsw_sp_fib4_entry_should_offload(fib_entry))
 -			return false;
 -		break;
 -	case MLXSW_SP_L3_PROTO_IPV6:
 -		break;
 -	}
 -
 -	switch (fib_entry->type) {
 -	case MLXSW_SP_FIB_ENTRY_TYPE_REMOTE:
 -		return !!nh_group->adj_index_valid;
 -	case MLXSW_SP_FIB_ENTRY_TYPE_LOCAL:
 -		return !!nh_group->nh_rif;
 -	case MLXSW_SP_FIB_ENTRY_TYPE_IPIP_DECAP:
 -		return true;
 -	default:
 -		return false;
 -	}
 -}
 -
 -static struct mlxsw_sp_nexthop *
 -mlxsw_sp_rt6_nexthop(struct mlxsw_sp_nexthop_group *nh_grp,
 -		     const struct mlxsw_sp_rt6 *mlxsw_sp_rt6)
 -{
 -	int i;
 -
 -	for (i = 0; i < nh_grp->count; i++) {
 -		struct mlxsw_sp_nexthop *nh = &nh_grp->nexthops[i];
 -		struct rt6_info *rt = mlxsw_sp_rt6->rt;
 -
 -		if (nh->rif && nh->rif->dev == rt->dst.dev &&
 -		    ipv6_addr_equal((const struct in6_addr *) &nh->gw_addr,
 -				    &rt->rt6i_gateway))
 -			return nh;
 -		continue;
 -	}
 -
 -	return NULL;
 -}
 -
 -static void
 -mlxsw_sp_fib4_entry_offload_set(struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
 -	int i;
 -
 -	if (fib_entry->type == MLXSW_SP_FIB_ENTRY_TYPE_LOCAL ||
 -	    fib_entry->type == MLXSW_SP_FIB_ENTRY_TYPE_IPIP_DECAP) {
 -		nh_grp->nexthops->key.fib_nh->nh_flags |= RTNH_F_OFFLOAD;
 -		return;
 -	}
 -
 -	for (i = 0; i < nh_grp->count; i++) {
 -		struct mlxsw_sp_nexthop *nh = &nh_grp->nexthops[i];
 -
 -		if (nh->offloaded)
 -			nh->key.fib_nh->nh_flags |= RTNH_F_OFFLOAD;
 -		else
 -			nh->key.fib_nh->nh_flags &= ~RTNH_F_OFFLOAD;
 -	}
 -}
 -
 -static void
 -mlxsw_sp_fib4_entry_offload_unset(struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
 -	int i;
 -
 -	for (i = 0; i < nh_grp->count; i++) {
 -		struct mlxsw_sp_nexthop *nh = &nh_grp->nexthops[i];
 -
 -		nh->key.fib_nh->nh_flags &= ~RTNH_F_OFFLOAD;
 -	}
 -}
 -
 -static void
 -mlxsw_sp_fib6_entry_offload_set(struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	struct mlxsw_sp_fib6_entry *fib6_entry;
 -	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
 -
 -	fib6_entry = container_of(fib_entry, struct mlxsw_sp_fib6_entry,
 -				  common);
 -
 -	if (fib_entry->type == MLXSW_SP_FIB_ENTRY_TYPE_LOCAL) {
 -		list_first_entry(&fib6_entry->rt6_list, struct mlxsw_sp_rt6,
 -				 list)->rt->rt6i_nh_flags |= RTNH_F_OFFLOAD;
 -		return;
 -	}
 -
 -	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
 -		struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
 -		struct mlxsw_sp_nexthop *nh;
 -
 -		nh = mlxsw_sp_rt6_nexthop(nh_grp, mlxsw_sp_rt6);
 -		if (nh && nh->offloaded)
 -			mlxsw_sp_rt6->rt->rt6i_nh_flags |= RTNH_F_OFFLOAD;
 -		else
 -			mlxsw_sp_rt6->rt->rt6i_nh_flags &= ~RTNH_F_OFFLOAD;
 -	}
 -}
 +static void
 +mlxsw_sp_fib_entry_offload_refresh(struct mlxsw_sp_fib_entry *fib_entry,
 +				   enum mlxsw_reg_ralue_op op, int err);
  
  static void
 -mlxsw_sp_fib6_entry_offload_unset(struct mlxsw_sp_fib_entry *fib_entry)
 +mlxsw_sp_nexthop_fib_entries_refresh(struct mlxsw_sp_nexthop_group *nh_grp)
  {
 -	struct mlxsw_sp_fib6_entry *fib6_entry;
 -	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
 -
 -	fib6_entry = container_of(fib_entry, struct mlxsw_sp_fib6_entry,
 -				  common);
 -	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
 -		struct rt6_info *rt = mlxsw_sp_rt6->rt;
 -
 -		rt->rt6i_nh_flags &= ~RTNH_F_OFFLOAD;
 -	}
 -}
 +	enum mlxsw_reg_ralue_op op = MLXSW_REG_RALUE_OP_WRITE_WRITE;
 +	struct mlxsw_sp_fib_entry *fib_entry;
  
 -static void mlxsw_sp_fib_entry_offload_set(struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	switch (fib_entry->fib_node->fib->proto) {
 -	case MLXSW_SP_L3_PROTO_IPV4:
 -		mlxsw_sp_fib4_entry_offload_set(fib_entry);
 -		break;
 -	case MLXSW_SP_L3_PROTO_IPV6:
 -		mlxsw_sp_fib6_entry_offload_set(fib_entry);
 -		break;
 +	list_for_each_entry(fib_entry, &nh_grp->fib_list, nexthop_group_node) {
 +		if (!mlxsw_sp_fib_node_entry_is_first(fib_entry->fib_node,
 +						      fib_entry))
 +			continue;
 +		mlxsw_sp_fib_entry_offload_refresh(fib_entry, op, 0);
  	}
  }
  
@@@ -1567,452 -3511,389 +1838,464 @@@ static int mlxsw_sp_nexthop_neigh_init(
  		}
  	}
  
 -	return NULL;
 -}
 +	/* If that is the first nexthop connected to that neigh, add to
 +	 * nexthop_neighs_list
 +	 */
 +	if (list_empty(&neigh_entry->nexthop_list))
 +		list_add_tail(&neigh_entry->nexthop_neighs_list_node,
 +			      &mlxsw_sp->router.nexthop_neighs_list);
  
 -static const struct rhashtable_params mlxsw_sp_fib_ht_params = {
 -	.key_offset = offsetof(struct mlxsw_sp_fib_node, key),
 -	.head_offset = offsetof(struct mlxsw_sp_fib_node, ht_node),
 -	.key_len = sizeof(struct mlxsw_sp_fib_key),
 -	.automatic_shrinking = true,
 -};
 +	nh->neigh_entry = neigh_entry;
 +	list_add_tail(&nh->neigh_list_node, &neigh_entry->nexthop_list);
 +	read_lock_bh(&n->lock);
 +	nud_state = n->nud_state;
 +	dead = n->dead;
 +	read_unlock_bh(&n->lock);
 +	__mlxsw_sp_nexthop_neigh_update(nh, !(nud_state & NUD_VALID && !dead));
  
 -static int mlxsw_sp_fib_node_insert(struct mlxsw_sp_fib *fib,
 -				    struct mlxsw_sp_fib_node *fib_node)
 -{
 -	return rhashtable_insert_fast(&fib->ht, &fib_node->ht_node,
 -				      mlxsw_sp_fib_ht_params);
 -}
 +	return 0;
  
 -static void mlxsw_sp_fib_node_remove(struct mlxsw_sp_fib *fib,
 -				     struct mlxsw_sp_fib_node *fib_node)
 -{
 -	rhashtable_remove_fast(&fib->ht, &fib_node->ht_node,
 -			       mlxsw_sp_fib_ht_params);
 +err_neigh_entry_create:
 +	neigh_release(n);
 +	return err;
  }
  
 -static struct mlxsw_sp_fib_node *
 -mlxsw_sp_fib_node_lookup(struct mlxsw_sp_fib *fib, const void *addr,
 -			 size_t addr_len, unsigned char prefix_len)
 +static void mlxsw_sp_nexthop_neigh_fini(struct mlxsw_sp *mlxsw_sp,
 +					struct mlxsw_sp_nexthop *nh)
  {
 -	struct mlxsw_sp_fib_key key;
 +	struct mlxsw_sp_neigh_entry *neigh_entry = nh->neigh_entry;
 +	struct neighbour *n;
  
 -	memset(&key, 0, sizeof(key));
 -	memcpy(key.addr, addr, addr_len);
 -	key.prefix_len = prefix_len;
 -	return rhashtable_lookup_fast(&fib->ht, &key, mlxsw_sp_fib_ht_params);
 -}
 +	if (!neigh_entry)
 +		return;
 +	n = neigh_entry->key.n;
  
 -static struct mlxsw_sp_fib_node *
 -mlxsw_sp_fib_node_create(struct mlxsw_sp_fib *fib, const void *addr,
 -			 size_t addr_len, unsigned char prefix_len)
 -{
 -	struct mlxsw_sp_fib_node *fib_node;
 +	__mlxsw_sp_nexthop_neigh_update(nh, true);
 +	list_del(&nh->neigh_list_node);
 +	nh->neigh_entry = NULL;
  
 -	fib_node = kzalloc(sizeof(*fib_node), GFP_KERNEL);
 -	if (!fib_node)
 -		return NULL;
 +	/* If that is the last nexthop connected to that neigh, remove from
 +	 * nexthop_neighs_list
 +	 */
 +	if (list_empty(&neigh_entry->nexthop_list))
 +		list_del(&neigh_entry->nexthop_neighs_list_node);
  
 -	INIT_LIST_HEAD(&fib_node->entry_list);
 -	list_add(&fib_node->list, &fib->node_list);
 -	memcpy(fib_node->key.addr, addr, addr_len);
 -	fib_node->key.prefix_len = prefix_len;
 +	if (!neigh_entry->connected && list_empty(&neigh_entry->nexthop_list))
 +		mlxsw_sp_neigh_entry_destroy(mlxsw_sp, neigh_entry);
  
 -	return fib_node;
 +	neigh_release(n);
  }
  
 -static void mlxsw_sp_fib_node_destroy(struct mlxsw_sp_fib_node *fib_node)
 +static int mlxsw_sp_nexthop_init(struct mlxsw_sp *mlxsw_sp,
 +				 struct mlxsw_sp_nexthop_group *nh_grp,
 +				 struct mlxsw_sp_nexthop *nh,
 +				 struct fib_nh *fib_nh)
  {
 -	list_del(&fib_node->list);
 -	WARN_ON(!list_empty(&fib_node->entry_list));
 -	kfree(fib_node);
 -}
 +	struct net_device *dev = fib_nh->nh_dev;
 +	struct in_device *in_dev;
 +	struct mlxsw_sp_rif *r;
 +	int err;
  
 -static bool
 -mlxsw_sp_fib_node_entry_is_first(const struct mlxsw_sp_fib_node *fib_node,
 -				 const struct mlxsw_sp_fib_entry *fib_entry)
 -{
 -	return list_first_entry(&fib_node->entry_list,
 -				struct mlxsw_sp_fib_entry, list) == fib_entry;
 -}
 +	nh->nh_grp = nh_grp;
 +	nh->key.fib_nh = fib_nh;
 +	err = mlxsw_sp_nexthop_insert(mlxsw_sp, nh);
 +	if (err)
 +		return err;
  
 -static int mlxsw_sp_fib_lpm_tree_link(struct mlxsw_sp *mlxsw_sp,
 -				      struct mlxsw_sp_fib *fib,
 -				      struct mlxsw_sp_fib_node *fib_node)
 -{
 -	struct mlxsw_sp_prefix_usage req_prefix_usage = {{ 0 } };
 -	struct mlxsw_sp_lpm_tree *lpm_tree;
 -	int err;
++<<<<<<< HEAD
++=======
++	mlxsw_sp_nexthop_counter_alloc(mlxsw_sp, nh);
++	list_add_tail(&nh->router_list_node, &mlxsw_sp->router->nexthop_list);
+ 
 -	/* Since the tree is shared between all virtual routers we must
 -	 * make sure it contains all the required prefix lengths. This
 -	 * can be computed by either adding the new prefix length to the
 -	 * existing prefix usage of a bound tree, or by aggregating the
 -	 * prefix lengths across all virtual routers and adding the new
 -	 * one as well.
 -	 */
 -	if (fib->lpm_tree)
 -		mlxsw_sp_prefix_usage_cpy(&req_prefix_usage,
 -					  &fib->lpm_tree->prefix_usage);
 -	else
 -		mlxsw_sp_vrs_prefixes(mlxsw_sp, fib->proto, &req_prefix_usage);
 -	mlxsw_sp_prefix_usage_set(&req_prefix_usage, fib_node->key.prefix_len);
++>>>>>>> a5390278a5eb (mlxsw: spectrum: Add support for setting counters on nexthops)
 +	if (!dev)
 +		return 0;
  
 -	lpm_tree = mlxsw_sp_lpm_tree_get(mlxsw_sp, &req_prefix_usage,
 -					 fib->proto);
 -	if (IS_ERR(lpm_tree))
 -		return PTR_ERR(lpm_tree);
 +	in_dev = __in_dev_get_rtnl(dev);
 +#if 0 /* RHEL does not support LINKDOWN yet - the condition is always false */
 +	if (in_dev && IN_DEV_IGNORE_ROUTES_WITH_LINKDOWN(in_dev) &&
 +	    fib_nh->nh_flags & RTNH_F_LINKDOWN)
 +		return 0;
 +#endif
  
 -	if (fib->lpm_tree && fib->lpm_tree->id == lpm_tree->id)
 +	r = mlxsw_sp_rif_find_by_dev(mlxsw_sp, dev);
 +	if (!r)
  		return 0;
 +	mlxsw_sp_nexthop_rif_init(nh, r);
  
 -	err = mlxsw_sp_vrs_lpm_tree_replace(mlxsw_sp, fib, lpm_tree);
 +	err = mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
  	if (err)
 -		return err;
 +		goto err_nexthop_neigh_init;
  
  	return 0;
 +
 +err_nexthop_neigh_init:
 +	mlxsw_sp_nexthop_rif_fini(nh);
 +	mlxsw_sp_nexthop_remove(mlxsw_sp, nh);
 +	return err;
  }
  
 -static void mlxsw_sp_fib_lpm_tree_unlink(struct mlxsw_sp *mlxsw_sp,
 -					 struct mlxsw_sp_fib *fib)
 +static void mlxsw_sp_nexthop_fini(struct mlxsw_sp *mlxsw_sp,
 +				  struct mlxsw_sp_nexthop *nh)
  {
 -	struct mlxsw_sp_prefix_usage req_prefix_usage = {{ 0 } };
 -	struct mlxsw_sp_lpm_tree *lpm_tree;
++<<<<<<< HEAD
 +	mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
 +	mlxsw_sp_nexthop_rif_fini(nh);
++=======
++	mlxsw_sp_nexthop4_type_fini(mlxsw_sp, nh);
++	list_del(&nh->router_list_node);
++	mlxsw_sp_nexthop_counter_free(mlxsw_sp, nh);
++>>>>>>> a5390278a5eb (mlxsw: spectrum: Add support for setting counters on nexthops)
 +	mlxsw_sp_nexthop_remove(mlxsw_sp, nh);
 +}
  
 -	/* Aggregate prefix lengths across all virtual routers to make
 -	 * sure we only have used prefix lengths in the LPM tree.
 -	 */
 -	mlxsw_sp_vrs_prefixes(mlxsw_sp, fib->proto, &req_prefix_usage);
 -	lpm_tree = mlxsw_sp_lpm_tree_get(mlxsw_sp, &req_prefix_usage,
 -					 fib->proto);
 -	if (IS_ERR(lpm_tree))
 -		goto err_tree_get;
 -	mlxsw_sp_vrs_lpm_tree_replace(mlxsw_sp, fib, lpm_tree);
 +static void mlxsw_sp_nexthop_event(struct mlxsw_sp *mlxsw_sp,
 +				   unsigned long event, struct fib_nh *fib_nh)
 +{
 +	struct mlxsw_sp_nexthop_key key;
 +	struct mlxsw_sp_nexthop *nh;
 +	struct mlxsw_sp_rif *r;
 +
 +	if (mlxsw_sp->router.aborted)
 +		return;
 +
 +	key.fib_nh = fib_nh;
 +	nh = mlxsw_sp_nexthop_lookup(mlxsw_sp, key);
 +	if (WARN_ON_ONCE(!nh))
 +		return;
  
 -err_tree_get:
 -	if (!mlxsw_sp_prefix_usage_none(&fib->prefix_usage))
 +	r = mlxsw_sp_rif_find_by_dev(mlxsw_sp, fib_nh->nh_dev);
 +	if (!r)
  		return;
 -	mlxsw_sp_vr_lpm_tree_unbind(mlxsw_sp, fib);
 -	mlxsw_sp_lpm_tree_put(mlxsw_sp, fib->lpm_tree);
 -	fib->lpm_tree = NULL;
 -}
  
 -static void mlxsw_sp_fib_node_prefix_inc(struct mlxsw_sp_fib_node *fib_node)
 -{
 -	unsigned char prefix_len = fib_node->key.prefix_len;
 -	struct mlxsw_sp_fib *fib = fib_node->fib;
 +	switch (event) {
 +	case FIB_EVENT_NH_ADD:
 +		mlxsw_sp_nexthop_rif_init(nh, r);
 +		mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
 +		break;
 +	case FIB_EVENT_NH_DEL:
 +		mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
 +		mlxsw_sp_nexthop_rif_fini(nh);
 +		break;
 +	}
  
 -	if (fib->prefix_ref_count[prefix_len]++ == 0)
 -		mlxsw_sp_prefix_usage_set(&fib->prefix_usage, prefix_len);
 +	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh->nh_grp);
  }
  
 -static void mlxsw_sp_fib_node_prefix_dec(struct mlxsw_sp_fib_node *fib_node)
 +static void mlxsw_sp_nexthop_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 +					   struct mlxsw_sp_rif *r)
  {
 -	unsigned char prefix_len = fib_node->key.prefix_len;
 -	struct mlxsw_sp_fib *fib = fib_node->fib;
 +	struct mlxsw_sp_nexthop *nh, *tmp;
  
 -	if (--fib->prefix_ref_count[prefix_len] == 0)
 -		mlxsw_sp_prefix_usage_clear(&fib->prefix_usage, prefix_len);
 +	list_for_each_entry_safe(nh, tmp, &r->nexthop_list, rif_list_node) {
 +		mlxsw_sp_nexthop_neigh_fini(mlxsw_sp, nh);
 +		mlxsw_sp_nexthop_rif_fini(nh);
 +		mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh->nh_grp);
 +	}
  }
  
 -static int mlxsw_sp_fib_node_init(struct mlxsw_sp *mlxsw_sp,
 -				  struct mlxsw_sp_fib_node *fib_node,
 -				  struct mlxsw_sp_fib *fib)
 +static struct mlxsw_sp_nexthop_group *
 +mlxsw_sp_nexthop_group_create(struct mlxsw_sp *mlxsw_sp, struct fib_info *fi)
  {
 +	struct mlxsw_sp_nexthop_group *nh_grp;
 +	struct mlxsw_sp_nexthop *nh;
 +	struct fib_nh *fib_nh;
 +	size_t alloc_size;
 +	int i;
  	int err;
  
 -	err = mlxsw_sp_fib_node_insert(fib, fib_node);
 -	if (err)
 -		return err;
 -	fib_node->fib = fib;
 -
 -	err = mlxsw_sp_fib_lpm_tree_link(mlxsw_sp, fib, fib_node);
 +	alloc_size = sizeof(*nh_grp) +
 +		     fi->fib_nhs * sizeof(struct mlxsw_sp_nexthop);
 +	nh_grp = kzalloc(alloc_size, GFP_KERNEL);
 +	if (!nh_grp)
 +		return ERR_PTR(-ENOMEM);
 +	INIT_LIST_HEAD(&nh_grp->fib_list);
 +	nh_grp->gateway = fi->fib_nh->nh_scope == RT_SCOPE_LINK;
 +	nh_grp->count = fi->fib_nhs;
 +	nh_grp->key.fi = fi;
 +	fib_info_hold(fi);
 +	for (i = 0; i < nh_grp->count; i++) {
 +		nh = &nh_grp->nexthops[i];
 +		fib_nh = &fi->fib_nh[i];
 +		err = mlxsw_sp_nexthop_init(mlxsw_sp, nh_grp, nh, fib_nh);
 +		if (err)
 +			goto err_nexthop_init;
 +	}
 +	err = mlxsw_sp_nexthop_group_insert(mlxsw_sp, nh_grp);
  	if (err)
 -		goto err_fib_lpm_tree_link;
 -
 -	mlxsw_sp_fib_node_prefix_inc(fib_node);
 -
 -	return 0;
 +		goto err_nexthop_group_insert;
 +	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
 +	return nh_grp;
  
 -err_fib_lpm_tree_link:
 -	fib_node->fib = NULL;
 -	mlxsw_sp_fib_node_remove(fib, fib_node);
 -	return err;
 +err_nexthop_group_insert:
 +err_nexthop_init:
 +	for (i--; i >= 0; i--) {
 +		nh = &nh_grp->nexthops[i];
 +		mlxsw_sp_nexthop_fini(mlxsw_sp, nh);
 +	}
 +	fib_info_put(nh_grp->key.fi);
 +	kfree(nh_grp);
 +	return ERR_PTR(err);
  }
  
 -static void mlxsw_sp_fib_node_fini(struct mlxsw_sp *mlxsw_sp,
 -				   struct mlxsw_sp_fib_node *fib_node)
 +static void
 +mlxsw_sp_nexthop_group_destroy(struct mlxsw_sp *mlxsw_sp,
 +			       struct mlxsw_sp_nexthop_group *nh_grp)
  {
 -	struct mlxsw_sp_fib *fib = fib_node->fib;
 +	struct mlxsw_sp_nexthop *nh;
 +	int i;
  
 -	mlxsw_sp_fib_node_prefix_dec(fib_node);
 -	mlxsw_sp_fib_lpm_tree_unlink(mlxsw_sp, fib);
 -	fib_node->fib = NULL;
 -	mlxsw_sp_fib_node_remove(fib, fib_node);
 +	mlxsw_sp_nexthop_group_remove(mlxsw_sp, nh_grp);
 +	for (i = 0; i < nh_grp->count; i++) {
 +		nh = &nh_grp->nexthops[i];
 +		mlxsw_sp_nexthop_fini(mlxsw_sp, nh);
 +	}
 +	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
 +	WARN_ON_ONCE(nh_grp->adj_index_valid);
 +	fib_info_put(nh_grp->key.fi);
 +	kfree(nh_grp);
  }
  
 -static struct mlxsw_sp_fib_node *
 -mlxsw_sp_fib_node_get(struct mlxsw_sp *mlxsw_sp, u32 tb_id, const void *addr,
 -		      size_t addr_len, unsigned char prefix_len,
 -		      enum mlxsw_sp_l3proto proto)
 +static int mlxsw_sp_nexthop_group_get(struct mlxsw_sp *mlxsw_sp,
 +				      struct mlxsw_sp_fib_entry *fib_entry,
 +				      struct fib_info *fi)
  {
 -	struct mlxsw_sp_fib_node *fib_node;
 -	struct mlxsw_sp_fib *fib;
 -	struct mlxsw_sp_vr *vr;
 -	int err;
 +	struct mlxsw_sp_nexthop_group_key key;
 +	struct mlxsw_sp_nexthop_group *nh_grp;
  
 -	vr = mlxsw_sp_vr_get(mlxsw_sp, tb_id);
 -	if (IS_ERR(vr))
 -		return ERR_CAST(vr);
 -	fib = mlxsw_sp_vr_fib(vr, proto);
 +	key.fi = fi;
 +	nh_grp = mlxsw_sp_nexthop_group_lookup(mlxsw_sp, key);
 +	if (!nh_grp) {
 +		nh_grp = mlxsw_sp_nexthop_group_create(mlxsw_sp, fi);
 +		if (IS_ERR(nh_grp))
 +			return PTR_ERR(nh_grp);
 +	}
 +	list_add_tail(&fib_entry->nexthop_group_node, &nh_grp->fib_list);
 +	fib_entry->nh_group = nh_grp;
 +	return 0;
 +}
  
 -	fib_node = mlxsw_sp_fib_node_lookup(fib, addr, addr_len, prefix_len);
 -	if (fib_node)
 -		return fib_node;
 +static void mlxsw_sp_nexthop_group_put(struct mlxsw_sp *mlxsw_sp,
 +				       struct mlxsw_sp_fib_entry *fib_entry)
 +{
 +	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
  
 -	fib_node = mlxsw_sp_fib_node_create(fib, addr, addr_len, prefix_len);
 -	if (!fib_node) {
 -		err = -ENOMEM;
 -		goto err_fib_node_create;
 -	}
 +	list_del(&fib_entry->nexthop_group_node);
 +	if (!list_empty(&nh_grp->fib_list))
 +		return;
 +	mlxsw_sp_nexthop_group_destroy(mlxsw_sp, nh_grp);
 +}
  
 -	err = mlxsw_sp_fib_node_init(mlxsw_sp, fib_node, fib);
 -	if (err)
 -		goto err_fib_node_init;
 +static bool
 +mlxsw_sp_fib_entry_should_offload(const struct mlxsw_sp_fib_entry *fib_entry)
 +{
 +	struct mlxsw_sp_nexthop_group *nh_group = fib_entry->nh_group;
  
 -	return fib_node;
 +	if (fib_entry->params.tos)
 +		return false;
  
 -err_fib_node_init:
 -	mlxsw_sp_fib_node_destroy(fib_node);
 -err_fib_node_create:
 -	mlxsw_sp_vr_put(vr);
 -	return ERR_PTR(err);
 +	switch (fib_entry->type) {
 +	case MLXSW_SP_FIB_ENTRY_TYPE_REMOTE:
 +		return !!nh_group->adj_index_valid;
 +	case MLXSW_SP_FIB_ENTRY_TYPE_LOCAL:
 +		return !!nh_group->nh_rif;
 +	default:
 +		return false;
 +	}
  }
  
 -static void mlxsw_sp_fib_node_put(struct mlxsw_sp *mlxsw_sp,
 -				  struct mlxsw_sp_fib_node *fib_node)
 +static void
 +mlxsw_sp_fib4_entry_offload_set(struct mlxsw_sp_fib_entry *fib_entry)
  {
 -	struct mlxsw_sp_vr *vr = fib_node->fib->vr;
 +	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
 +	int i;
  
 -	if (!list_empty(&fib_node->entry_list))
 +	if (fib_entry->type == MLXSW_SP_FIB_ENTRY_TYPE_LOCAL) {
 +		nh_grp->nexthops->key.fib_nh->nh_flags |= RTNH_F_OFFLOAD;
  		return;
 -	mlxsw_sp_fib_node_fini(mlxsw_sp, fib_node);
 -	mlxsw_sp_fib_node_destroy(fib_node);
 -	mlxsw_sp_vr_put(vr);
 +	}
 +
 +	for (i = 0; i < nh_grp->count; i++) {
 +		struct mlxsw_sp_nexthop *nh = &nh_grp->nexthops[i];
 +
 +		if (nh->offloaded)
 +			nh->key.fib_nh->nh_flags |= RTNH_F_OFFLOAD;
 +		else
 +			nh->key.fib_nh->nh_flags &= ~RTNH_F_OFFLOAD;
 +	}
  }
  
 -static struct mlxsw_sp_fib4_entry *
 -mlxsw_sp_fib4_node_entry_find(const struct mlxsw_sp_fib_node *fib_node,
 -			      const struct mlxsw_sp_fib4_entry *new4_entry)
 +static void
 +mlxsw_sp_fib4_entry_offload_unset(struct mlxsw_sp_fib_entry *fib_entry)
  {
 -	struct mlxsw_sp_fib4_entry *fib4_entry;
 +	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
 +	int i;
  
 -	list_for_each_entry(fib4_entry, &fib_node->entry_list, common.list) {
 -		if (fib4_entry->tb_id > new4_entry->tb_id)
 -			continue;
 -		if (fib4_entry->tb_id != new4_entry->tb_id)
 -			break;
 -		if (fib4_entry->tos > new4_entry->tos)
 -			continue;
 -		if (fib4_entry->prio >= new4_entry->prio ||
 -		    fib4_entry->tos < new4_entry->tos)
 -			return fib4_entry;
 -	}
 +	for (i = 0; i < nh_grp->count; i++) {
 +		struct mlxsw_sp_nexthop *nh = &nh_grp->nexthops[i];
  
 -	return NULL;
 +		nh->key.fib_nh->nh_flags &= ~RTNH_F_OFFLOAD;
 +	}
  }
  
 -static int
 -mlxsw_sp_fib4_node_list_append(struct mlxsw_sp_fib4_entry *fib4_entry,
 -			       struct mlxsw_sp_fib4_entry *new4_entry)
 +static void mlxsw_sp_fib_entry_offload_set(struct mlxsw_sp_fib_entry *fib_entry)
  {
 -	struct mlxsw_sp_fib_node *fib_node;
 +	fib_entry->offloaded = true;
  
 -	if (WARN_ON(!fib4_entry))
 -		return -EINVAL;
 +	switch (fib_entry->fib_node->fib->proto) {
 +	case MLXSW_SP_L3_PROTO_IPV4:
 +		mlxsw_sp_fib4_entry_offload_set(fib_entry);
 +		break;
 +	case MLXSW_SP_L3_PROTO_IPV6:
 +		WARN_ON_ONCE(1);
 +	}
 +}
  
 -	fib_node = fib4_entry->common.fib_node;
 -	list_for_each_entry_from(fib4_entry, &fib_node->entry_list,
 -				 common.list) {
 -		if (fib4_entry->tb_id != new4_entry->tb_id ||
 -		    fib4_entry->tos != new4_entry->tos ||
 -		    fib4_entry->prio != new4_entry->prio)
 -			break;
 +static void
 +mlxsw_sp_fib_entry_offload_unset(struct mlxsw_sp_fib_entry *fib_entry)
 +{
 +	switch (fib_entry->fib_node->fib->proto) {
 +	case MLXSW_SP_L3_PROTO_IPV4:
 +		mlxsw_sp_fib4_entry_offload_unset(fib_entry);
 +		break;
 +	case MLXSW_SP_L3_PROTO_IPV6:
 +		WARN_ON_ONCE(1);
  	}
  
 -	list_add_tail(&new4_entry->common.list, &fib4_entry->common.list);
 -	return 0;
 +	fib_entry->offloaded = false;
  }
  
 -static int
 -mlxsw_sp_fib4_node_list_insert(struct mlxsw_sp_fib4_entry *new4_entry,
 -			       bool replace, bool append)
 +static void
 +mlxsw_sp_fib_entry_offload_refresh(struct mlxsw_sp_fib_entry *fib_entry,
 +				   enum mlxsw_reg_ralue_op op, int err)
  {
 -	struct mlxsw_sp_fib_node *fib_node = new4_entry->common.fib_node;
 -	struct mlxsw_sp_fib4_entry *fib4_entry;
 -
 -	fib4_entry = mlxsw_sp_fib4_node_entry_find(fib_node, new4_entry);
 +	switch (op) {
 +	case MLXSW_REG_RALUE_OP_WRITE_DELETE:
 +		if (!fib_entry->offloaded)
 +			return;
 +		return mlxsw_sp_fib_entry_offload_unset(fib_entry);
 +	case MLXSW_REG_RALUE_OP_WRITE_WRITE:
 +		if (err)
 +			return;
 +		if (mlxsw_sp_fib_entry_should_offload(fib_entry) &&
 +		    !fib_entry->offloaded)
 +			mlxsw_sp_fib_entry_offload_set(fib_entry);
 +		else if (!mlxsw_sp_fib_entry_should_offload(fib_entry) &&
 +			 fib_entry->offloaded)
 +			mlxsw_sp_fib_entry_offload_unset(fib_entry);
 +		return;
 +	default:
 +		return;
 +	}
 +}
  
 -	if (append)
 -		return mlxsw_sp_fib4_node_list_append(fib4_entry, new4_entry);
 -	if (replace && WARN_ON(!fib4_entry))
 -		return -EINVAL;
 +static int mlxsw_sp_fib_entry_op4_remote(struct mlxsw_sp *mlxsw_sp,
 +					 struct mlxsw_sp_fib_entry *fib_entry,
 +					 enum mlxsw_reg_ralue_op op)
 +{
 +	char ralue_pl[MLXSW_REG_RALUE_LEN];
 +	struct mlxsw_sp_fib *fib = fib_entry->fib_node->fib;
 +	u32 *p_dip = (u32 *) fib_entry->fib_node->key.addr;
 +	enum mlxsw_reg_ralue_trap_action trap_action;
 +	u16 trap_id = 0;
 +	u32 adjacency_index = 0;
 +	u16 ecmp_size = 0;
  
 -	/* Insert new entry before replaced one, so that we can later
 -	 * remove the second.
 +	/* In case the nexthop group adjacency index is valid, use it
 +	 * with provided ECMP size. Otherwise, setup trap and pass
 +	 * traffic to kernel.
  	 */
 -	if (fib4_entry) {
 -		list_add_tail(&new4_entry->common.list,
 -			      &fib4_entry->common.list);
 +	if (mlxsw_sp_fib_entry_should_offload(fib_entry)) {
 +		trap_action = MLXSW_REG_RALUE_TRAP_ACTION_NOP;
 +		adjacency_index = fib_entry->nh_group->adj_index;
 +		ecmp_size = fib_entry->nh_group->ecmp_size;
  	} else {
 -		struct mlxsw_sp_fib4_entry *last;
 -
 -		list_for_each_entry(last, &fib_node->entry_list, common.list) {
 -			if (new4_entry->tb_id > last->tb_id)
 -				break;
 -			fib4_entry = last;
 -		}
 -
 -		if (fib4_entry)
 -			list_add(&new4_entry->common.list,
 -				 &fib4_entry->common.list);
 -		else
 -			list_add(&new4_entry->common.list,
 -				 &fib_node->entry_list);
 +		trap_action = MLXSW_REG_RALUE_TRAP_ACTION_TRAP;
 +		trap_id = MLXSW_TRAP_ID_RTR_INGRESS0;
  	}
  
 -	return 0;
 -}
 -
 -static void
 -mlxsw_sp_fib4_node_list_remove(struct mlxsw_sp_fib4_entry *fib4_entry)
 -{
 -	list_del(&fib4_entry->common.list);
 +	mlxsw_reg_ralue_pack4(ralue_pl,
 +			      (enum mlxsw_reg_ralxx_protocol) fib->proto, op,
 +			      fib->vr->id, fib_entry->fib_node->key.prefix_len,
 +			      *p_dip);
 +	mlxsw_reg_ralue_act_remote_pack(ralue_pl, trap_action, trap_id,
 +					adjacency_index, ecmp_size);
 +	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralue), ralue_pl);
  }
  
 -static int mlxsw_sp_fib_node_entry_add(struct mlxsw_sp *mlxsw_sp,
 -				       struct mlxsw_sp_fib_entry *fib_entry)
 +static int mlxsw_sp_fib_entry_op4_local(struct mlxsw_sp *mlxsw_sp,
 +					struct mlxsw_sp_fib_entry *fib_entry,
 +					enum mlxsw_reg_ralue_op op)
  {
 -	struct mlxsw_sp_fib_node *fib_node = fib_entry->fib_node;
 -
 -	if (!mlxsw_sp_fib_node_entry_is_first(fib_node, fib_entry))
 -		return 0;
 -
 -	/* To prevent packet loss, overwrite the previously offloaded
 -	 * entry.
 -	 */
 -	if (!list_is_singular(&fib_node->entry_list)) {
 -		enum mlxsw_reg_ralue_op op = MLXSW_REG_RALUE_OP_WRITE_DELETE;
 -		struct mlxsw_sp_fib_entry *n = list_next_entry(fib_entry, list);
 +	struct mlxsw_sp_rif *r = fib_entry->nh_group->nh_rif;
 +	struct mlxsw_sp_fib *fib = fib_entry->fib_node->fib;
 +	enum mlxsw_reg_ralue_trap_action trap_action;
 +	char ralue_pl[MLXSW_REG_RALUE_LEN];
 +	u32 *p_dip = (u32 *) fib_entry->fib_node->key.addr;
 +	u16 trap_id = 0;
 +	u16 rif = 0;
  
 -		mlxsw_sp_fib_entry_offload_refresh(n, op, 0);
 +	if (mlxsw_sp_fib_entry_should_offload(fib_entry)) {
 +		trap_action = MLXSW_REG_RALUE_TRAP_ACTION_NOP;
 +		rif = r->rif;
 +	} else {
 +		trap_action = MLXSW_REG_RALUE_TRAP_ACTION_TRAP;
 +		trap_id = MLXSW_TRAP_ID_RTR_INGRESS0;
  	}
  
 -	return mlxsw_sp_fib_entry_update(mlxsw_sp, fib_entry);
 +	mlxsw_reg_ralue_pack4(ralue_pl,
 +			      (enum mlxsw_reg_ralxx_protocol) fib->proto, op,
 +			      fib->vr->id, fib_entry->fib_node->key.prefix_len,
 +			      *p_dip);
 +	mlxsw_reg_ralue_act_local_pack(ralue_pl, trap_action, trap_id, rif);
 +	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralue), ralue_pl);
  }
  
 -static void mlxsw_sp_fib_node_entry_del(struct mlxsw_sp *mlxsw_sp,
 -					struct mlxsw_sp_fib_entry *fib_entry)
 +static int mlxsw_sp_fib_entry_op4_trap(struct mlxsw_sp *mlxsw_sp,
 +				       struct mlxsw_sp_fib_entry *fib_entry,
 +				       enum mlxsw_reg_ralue_op op)
  {
 -	struct mlxsw_sp_fib_node *fib_node = fib_entry->fib_node;
 -
 -	if (!mlxsw_sp_fib_node_entry_is_first(fib_node, fib_entry))
 -		return;
 +	struct mlxsw_sp_fib *fib = fib_entry->fib_node->fib;
 +	char ralue_pl[MLXSW_REG_RALUE_LEN];
 +	u32 *p_dip = (u32 *) fib_entry->fib_node->key.addr;
  
 -	/* Promote the next entry by overwriting the deleted entry */
 -	if (!list_is_singular(&fib_node->entry_list)) {
 -		struct mlxsw_sp_fib_entry *n = list_next_entry(fib_entry, list);
 -		enum mlxsw_reg_ralue_op op = MLXSW_REG_RALUE_OP_WRITE_DELETE;
 +	mlxsw_reg_ralue_pack4(ralue_pl,
 +			      (enum mlxsw_reg_ralxx_protocol) fib->proto, op,
 +			      fib->vr->id, fib_entry->fib_node->key.prefix_len,
 +			      *p_dip);
 +	mlxsw_reg_ralue_act_ip2me_pack(ralue_pl);
 +	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralue), ralue_pl);
 +}
  
 -		mlxsw_sp_fib_entry_update(mlxsw_sp, n);
 -		mlxsw_sp_fib_entry_offload_refresh(fib_entry, op, 0);
 -		return;
 +static int mlxsw_sp_fib_entry_op4(struct mlxsw_sp *mlxsw_sp,
 +				  struct mlxsw_sp_fib_entry *fib_entry,
 +				  enum mlxsw_reg_ralue_op op)
 +{
 +	switch (fib_entry->type) {
 +	case MLXSW_SP_FIB_ENTRY_TYPE_REMOTE:
 +		return mlxsw_sp_fib_entry_op4_remote(mlxsw_sp, fib_entry, op);
 +	case MLXSW_SP_FIB_ENTRY_TYPE_LOCAL:
 +		return mlxsw_sp_fib_entry_op4_local(mlxsw_sp, fib_entry, op);
 +	case MLXSW_SP_FIB_ENTRY_TYPE_TRAP:
 +		return mlxsw_sp_fib_entry_op4_trap(mlxsw_sp, fib_entry, op);
  	}
 -
 -	mlxsw_sp_fib_entry_del(mlxsw_sp, fib_entry);
 +	return -EINVAL;
  }
  
 -static int mlxsw_sp_fib4_node_entry_link(struct mlxsw_sp *mlxsw_sp,
 -					 struct mlxsw_sp_fib4_entry *fib4_entry,
 -					 bool replace, bool append)
 +static int mlxsw_sp_fib_entry_op(struct mlxsw_sp *mlxsw_sp,
 +				 struct mlxsw_sp_fib_entry *fib_entry,
 +				 enum mlxsw_reg_ralue_op op)
  {
 -	int err;
 +	int err = -EINVAL;
  
 -	err = mlxsw_sp_fib4_node_list_insert(fib4_entry, replace, append);
 -	if (err)
 +	switch (fib_entry->fib_node->fib->proto) {
 +	case MLXSW_SP_L3_PROTO_IPV4:
 +		err = mlxsw_sp_fib_entry_op4(mlxsw_sp, fib_entry, op);
 +		break;
 +	case MLXSW_SP_L3_PROTO_IPV6:
  		return err;
 -
 -	err = mlxsw_sp_fib_node_entry_add(mlxsw_sp, &fib4_entry->common);
 -	if (err)
 -		goto err_fib_node_entry_add;
 -
 -	return 0;
 -
 -err_fib_node_entry_add:
 -	mlxsw_sp_fib4_node_list_remove(fib4_entry);
 +	}
 +	mlxsw_sp_fib_entry_offload_refresh(fib_entry, op, err);
  	return err;
  }
  
@@@ -2583,233 -4659,714 +2866,669 @@@ err_fib4_entry_create
  	return err;
  }
  
 -static void mlxsw_sp_router_fib6_del(struct mlxsw_sp *mlxsw_sp,
 -				     struct rt6_info *rt)
 +static void mlxsw_sp_router_fib4_del(struct mlxsw_sp *mlxsw_sp,
 +				     struct fib_entry_notifier_info *fen_info)
  {
 -	struct mlxsw_sp_fib6_entry *fib6_entry;
++<<<<<<< HEAD
++=======
++	struct mlxsw_sp_fib4_entry *fib4_entry;
+ 	struct mlxsw_sp_fib_node *fib_node;
+ 
+ 	if (mlxsw_sp->router->aborted)
+ 		return;
+ 
 -	if (mlxsw_sp_fib6_rt_should_ignore(rt))
++	fib4_entry = mlxsw_sp_fib4_entry_lookup(mlxsw_sp, fen_info);
++	if (WARN_ON(!fib4_entry))
+ 		return;
++	fib_node = fib4_entry->common.fib_node;
+ 
 -	fib6_entry = mlxsw_sp_fib6_entry_lookup(mlxsw_sp, rt);
 -	if (WARN_ON(!fib6_entry))
 -		return;
++	mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib4_entry);
++	mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib4_entry);
++	mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
++}
+ 
 -	/* If route is part of a multipath entry, but not the last one
 -	 * removed, then only reduce its nexthop group.
++static bool mlxsw_sp_fib6_rt_should_ignore(const struct rt6_info *rt)
++{
++	/* Packets with link-local destination IP arriving to the router
++	 * are trapped to the CPU, so no need to program specific routes
++	 * for them.
+ 	 */
 -	if (!list_is_singular(&fib6_entry->rt6_list)) {
 -		mlxsw_sp_fib6_entry_nexthop_del(mlxsw_sp, fib6_entry, rt);
 -		return;
 -	}
++	if (ipv6_addr_type(&rt->rt6i_dst.addr) & IPV6_ADDR_LINKLOCAL)
++		return true;
++
++	/* Multicast routes aren't supported, so ignore them. Neighbour
++	 * Discovery packets are specifically trapped.
++	 */
++	if (ipv6_addr_type(&rt->rt6i_dst.addr) & IPV6_ADDR_MULTICAST)
++		return true;
+ 
 -	fib_node = fib6_entry->common.fib_node;
++	/* Cloned routes are irrelevant in the forwarding path. */
++	if (rt->rt6i_flags & RTF_CACHE)
++		return true;
+ 
 -	mlxsw_sp_fib6_node_entry_unlink(mlxsw_sp, fib6_entry);
 -	mlxsw_sp_fib6_entry_destroy(mlxsw_sp, fib6_entry);
 -	mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
++	return false;
+ }
+ 
 -static int __mlxsw_sp_router_set_abort_trap(struct mlxsw_sp *mlxsw_sp,
 -					    enum mlxsw_reg_ralxx_protocol proto,
 -					    u8 tree_id)
++static struct mlxsw_sp_rt6 *mlxsw_sp_rt6_create(struct rt6_info *rt)
+ {
 -	char ralta_pl[MLXSW_REG_RALTA_LEN];
 -	char ralst_pl[MLXSW_REG_RALST_LEN];
 -	int i, err;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
+ 
 -	mlxsw_reg_ralta_pack(ralta_pl, true, proto, tree_id);
 -	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralta), ralta_pl);
 -	if (err)
 -		return err;
++	mlxsw_sp_rt6 = kzalloc(sizeof(*mlxsw_sp_rt6), GFP_KERNEL);
++	if (!mlxsw_sp_rt6)
++		return ERR_PTR(-ENOMEM);
+ 
 -	mlxsw_reg_ralst_pack(ralst_pl, 0xff, tree_id);
 -	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralst), ralst_pl);
 -	if (err)
 -		return err;
++	/* In case of route replace, replaced route is deleted with
++	 * no notification. Take reference to prevent accessing freed
++	 * memory.
++	 */
++	mlxsw_sp_rt6->rt = rt;
++	rt6_hold(rt);
+ 
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 -		struct mlxsw_sp_vr *vr = &mlxsw_sp->router->vrs[i];
 -		char raltb_pl[MLXSW_REG_RALTB_LEN];
 -		char ralue_pl[MLXSW_REG_RALUE_LEN];
++	return mlxsw_sp_rt6;
++}
+ 
 -		mlxsw_reg_raltb_pack(raltb_pl, vr->id, proto, tree_id);
 -		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(raltb),
 -				      raltb_pl);
 -		if (err)
 -			return err;
++#if IS_ENABLED(CONFIG_IPV6)
++static void mlxsw_sp_rt6_release(struct rt6_info *rt)
++{
++	rt6_release(rt);
++}
++#else
++static void mlxsw_sp_rt6_release(struct rt6_info *rt)
++{
++}
++#endif
+ 
 -		mlxsw_reg_ralue_pack(ralue_pl, proto,
 -				     MLXSW_REG_RALUE_OP_WRITE_WRITE, vr->id, 0);
 -		mlxsw_reg_ralue_act_ip2me_pack(ralue_pl);
 -		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralue),
 -				      ralue_pl);
 -		if (err)
 -			return err;
++static void mlxsw_sp_rt6_destroy(struct mlxsw_sp_rt6 *mlxsw_sp_rt6)
++{
++	mlxsw_sp_rt6_release(mlxsw_sp_rt6->rt);
++	kfree(mlxsw_sp_rt6);
++}
++
++static bool mlxsw_sp_fib6_rt_can_mp(const struct rt6_info *rt)
++{
++	/* RTF_CACHE routes are ignored */
++	return (rt->rt6i_flags & (RTF_GATEWAY | RTF_ADDRCONF)) == RTF_GATEWAY;
++}
++
++static struct rt6_info *
++mlxsw_sp_fib6_entry_rt(const struct mlxsw_sp_fib6_entry *fib6_entry)
++{
++	return list_first_entry(&fib6_entry->rt6_list, struct mlxsw_sp_rt6,
++				list)->rt;
++}
++
++static struct mlxsw_sp_fib6_entry *
++mlxsw_sp_fib6_node_mp_entry_find(const struct mlxsw_sp_fib_node *fib_node,
++				 const struct rt6_info *nrt, bool replace)
++{
++	struct mlxsw_sp_fib6_entry *fib6_entry;
++
++	if (!mlxsw_sp_fib6_rt_can_mp(nrt) || replace)
++		return NULL;
++
++	list_for_each_entry(fib6_entry, &fib_node->entry_list, common.list) {
++		struct rt6_info *rt = mlxsw_sp_fib6_entry_rt(fib6_entry);
++
++		/* RT6_TABLE_LOCAL and RT6_TABLE_MAIN share the same
++		 * virtual router.
++		 */
++		if (rt->rt6i_table->tb6_id > nrt->rt6i_table->tb6_id)
++			continue;
++		if (rt->rt6i_table->tb6_id != nrt->rt6i_table->tb6_id)
++			break;
++		if (rt->rt6i_metric < nrt->rt6i_metric)
++			continue;
++		if (rt->rt6i_metric == nrt->rt6i_metric &&
++		    mlxsw_sp_fib6_rt_can_mp(rt))
++			return fib6_entry;
++		if (rt->rt6i_metric > nrt->rt6i_metric)
++			break;
+ 	}
+ 
 -	return 0;
++	return NULL;
+ }
+ 
 -static int mlxsw_sp_router_set_abort_trap(struct mlxsw_sp *mlxsw_sp)
++static struct mlxsw_sp_rt6 *
++mlxsw_sp_fib6_entry_rt_find(const struct mlxsw_sp_fib6_entry *fib6_entry,
++			    const struct rt6_info *rt)
+ {
 -	enum mlxsw_reg_ralxx_protocol proto = MLXSW_REG_RALXX_PROTOCOL_IPV4;
 -	int err;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
+ 
 -	err = __mlxsw_sp_router_set_abort_trap(mlxsw_sp, proto,
 -					       MLXSW_SP_LPM_TREE_MIN);
 -	if (err)
 -		return err;
++	list_for_each_entry(mlxsw_sp_rt6, &fib6_entry->rt6_list, list) {
++		if (mlxsw_sp_rt6->rt == rt)
++			return mlxsw_sp_rt6;
++	}
+ 
 -	proto = MLXSW_REG_RALXX_PROTOCOL_IPV6;
 -	return __mlxsw_sp_router_set_abort_trap(mlxsw_sp, proto,
 -						MLXSW_SP_LPM_TREE_MIN + 1);
++	return NULL;
+ }
+ 
 -static void mlxsw_sp_fib4_node_flush(struct mlxsw_sp *mlxsw_sp,
 -				     struct mlxsw_sp_fib_node *fib_node)
++static bool mlxsw_sp_nexthop6_ipip_type(const struct mlxsw_sp *mlxsw_sp,
++					const struct rt6_info *rt,
++					enum mlxsw_sp_ipip_type *ret)
+ {
 -	struct mlxsw_sp_fib4_entry *fib4_entry, *tmp;
++	return rt->dst.dev &&
++	       mlxsw_sp_netdev_ipip_type(mlxsw_sp, rt->dst.dev, ret);
++}
+ 
 -	list_for_each_entry_safe(fib4_entry, tmp, &fib_node->entry_list,
 -				 common.list) {
 -		bool do_break = &tmp->common.list == &fib_node->entry_list;
++static int mlxsw_sp_nexthop6_type_init(struct mlxsw_sp *mlxsw_sp,
++				       struct mlxsw_sp_nexthop_group *nh_grp,
++				       struct mlxsw_sp_nexthop *nh,
++				       const struct rt6_info *rt)
++{
++	struct mlxsw_sp_router *router = mlxsw_sp->router;
++	struct net_device *dev = rt->dst.dev;
++	enum mlxsw_sp_ipip_type ipipt;
++	struct mlxsw_sp_rif *rif;
++	int err;
+ 
 -		mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib4_entry);
 -		mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib4_entry);
 -		mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
 -		/* Break when entry list is empty and node was freed.
 -		 * Otherwise, we'll access freed memory in the next
 -		 * iteration.
 -		 */
 -		if (do_break)
 -			break;
++	if (mlxsw_sp_nexthop6_ipip_type(mlxsw_sp, rt, &ipipt) &&
++	    router->ipip_ops_arr[ipipt]->can_offload(mlxsw_sp, dev,
++						     MLXSW_SP_L3_PROTO_IPV6)) {
++		nh->type = MLXSW_SP_NEXTHOP_TYPE_IPIP;
++		return mlxsw_sp_nexthop_ipip_init(mlxsw_sp, ipipt, nh, dev);
+ 	}
 -}
+ 
 -static void mlxsw_sp_fib6_node_flush(struct mlxsw_sp *mlxsw_sp,
 -				     struct mlxsw_sp_fib_node *fib_node)
 -{
 -	struct mlxsw_sp_fib6_entry *fib6_entry, *tmp;
++	nh->type = MLXSW_SP_NEXTHOP_TYPE_ETH;
++	rif = mlxsw_sp_rif_find_by_dev(mlxsw_sp, dev);
++	if (!rif)
++		return 0;
++	mlxsw_sp_nexthop_rif_init(nh, rif);
+ 
 -	list_for_each_entry_safe(fib6_entry, tmp, &fib_node->entry_list,
 -				 common.list) {
 -		bool do_break = &tmp->common.list == &fib_node->entry_list;
++	err = mlxsw_sp_nexthop_neigh_init(mlxsw_sp, nh);
++	if (err)
++		goto err_nexthop_neigh_init;
+ 
 -		mlxsw_sp_fib6_node_entry_unlink(mlxsw_sp, fib6_entry);
 -		mlxsw_sp_fib6_entry_destroy(mlxsw_sp, fib6_entry);
 -		mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
 -		if (do_break)
 -			break;
 -	}
 -}
++	return 0;
+ 
 -static void mlxsw_sp_fib_node_flush(struct mlxsw_sp *mlxsw_sp,
 -				    struct mlxsw_sp_fib_node *fib_node)
 -{
 -	switch (fib_node->fib->proto) {
 -	case MLXSW_SP_L3_PROTO_IPV4:
 -		mlxsw_sp_fib4_node_flush(mlxsw_sp, fib_node);
 -		break;
 -	case MLXSW_SP_L3_PROTO_IPV6:
 -		mlxsw_sp_fib6_node_flush(mlxsw_sp, fib_node);
 -		break;
 -	}
++err_nexthop_neigh_init:
++	mlxsw_sp_nexthop_rif_fini(nh);
++	return err;
+ }
+ 
 -static void mlxsw_sp_vr_fib_flush(struct mlxsw_sp *mlxsw_sp,
 -				  struct mlxsw_sp_vr *vr,
 -				  enum mlxsw_sp_l3proto proto)
++static void mlxsw_sp_nexthop6_type_fini(struct mlxsw_sp *mlxsw_sp,
++					struct mlxsw_sp_nexthop *nh)
+ {
 -	struct mlxsw_sp_fib *fib = mlxsw_sp_vr_fib(vr, proto);
 -	struct mlxsw_sp_fib_node *fib_node, *tmp;
 -
 -	list_for_each_entry_safe(fib_node, tmp, &fib->node_list, list) {
 -		bool do_break = &tmp->list == &fib->node_list;
 -
 -		mlxsw_sp_fib_node_flush(mlxsw_sp, fib_node);
 -		if (do_break)
 -			break;
 -	}
++	mlxsw_sp_nexthop_type_fini(mlxsw_sp, nh);
+ }
+ 
 -static void mlxsw_sp_router_fib_flush(struct mlxsw_sp *mlxsw_sp)
++static int mlxsw_sp_nexthop6_init(struct mlxsw_sp *mlxsw_sp,
++				  struct mlxsw_sp_nexthop_group *nh_grp,
++				  struct mlxsw_sp_nexthop *nh,
++				  const struct rt6_info *rt)
+ {
 -	int i;
++	struct net_device *dev = rt->dst.dev;
+ 
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 -		struct mlxsw_sp_vr *vr = &mlxsw_sp->router->vrs[i];
++	nh->nh_grp = nh_grp;
++	memcpy(&nh->gw_addr, &rt->rt6i_gateway, sizeof(nh->gw_addr));
++	mlxsw_sp_nexthop_counter_alloc(mlxsw_sp, nh);
+ 
 -		if (!mlxsw_sp_vr_is_used(vr))
 -			continue;
 -		mlxsw_sp_vr_fib_flush(mlxsw_sp, vr, MLXSW_SP_L3_PROTO_IPV4);
++	list_add_tail(&nh->router_list_node, &mlxsw_sp->router->nexthop_list);
+ 
 -		/* If virtual router was only used for IPv4, then it's no
 -		 * longer used.
 -		 */
 -		if (!mlxsw_sp_vr_is_used(vr))
 -			continue;
 -		mlxsw_sp_vr_fib_flush(mlxsw_sp, vr, MLXSW_SP_L3_PROTO_IPV6);
 -	}
++	if (!dev)
++		return 0;
++	nh->ifindex = dev->ifindex;
++
++	return mlxsw_sp_nexthop6_type_init(mlxsw_sp, nh_grp, nh, rt);
+ }
+ 
 -static void mlxsw_sp_router_fib_abort(struct mlxsw_sp *mlxsw_sp)
++static void mlxsw_sp_nexthop6_fini(struct mlxsw_sp *mlxsw_sp,
++				   struct mlxsw_sp_nexthop *nh)
+ {
 -	int err;
 -
 -	if (mlxsw_sp->router->aborted)
 -		return;
 -	dev_warn(mlxsw_sp->bus_info->dev, "FIB abort triggered. Note that FIB entries are no longer being offloaded to this device.\n");
 -	mlxsw_sp_router_fib_flush(mlxsw_sp);
 -	mlxsw_sp->router->aborted = true;
 -	err = mlxsw_sp_router_set_abort_trap(mlxsw_sp);
 -	if (err)
 -		dev_warn(mlxsw_sp->bus_info->dev, "Failed to set abort trap.\n");
++	mlxsw_sp_nexthop6_type_fini(mlxsw_sp, nh);
++	list_del(&nh->router_list_node);
++	mlxsw_sp_nexthop_counter_free(mlxsw_sp, nh);
+ }
+ 
 -struct mlxsw_sp_fib_event_work {
 -	struct work_struct work;
 -	union {
 -		struct fib6_entry_notifier_info fen6_info;
 -		struct fib_entry_notifier_info fen_info;
 -		struct fib_rule_notifier_info fr_info;
 -		struct fib_nh_notifier_info fnh_info;
 -	};
 -	struct mlxsw_sp *mlxsw_sp;
 -	unsigned long event;
 -};
 -
 -static void mlxsw_sp_router_fib4_event_work(struct work_struct *work)
++static bool mlxsw_sp_rt6_is_gateway(const struct mlxsw_sp *mlxsw_sp,
++				    const struct rt6_info *rt)
+ {
 -	struct mlxsw_sp_fib_event_work *fib_work =
 -		container_of(work, struct mlxsw_sp_fib_event_work, work);
 -	struct mlxsw_sp *mlxsw_sp = fib_work->mlxsw_sp;
 -	struct fib_rule *rule;
 -	bool replace, append;
 -	int err;
 -
 -	/* Protect internal structures from changes */
 -	rtnl_lock();
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD:
 -		replace = fib_work->event == FIB_EVENT_ENTRY_REPLACE;
 -		append = fib_work->event == FIB_EVENT_ENTRY_APPEND;
 -		err = mlxsw_sp_router_fib4_add(mlxsw_sp, &fib_work->fen_info,
 -					       replace, append);
 -		if (err)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		fib_info_put(fib_work->fen_info.fi);
 -		break;
 -	case FIB_EVENT_ENTRY_DEL:
 -		mlxsw_sp_router_fib4_del(mlxsw_sp, &fib_work->fen_info);
 -		fib_info_put(fib_work->fen_info.fi);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		rule = fib_work->fr_info.rule;
 -		if (!fib4_rule_default(rule) && !rule->l3mdev)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		fib_rule_put(rule);
 -		break;
 -	case FIB_EVENT_NH_ADD: /* fall through */
 -	case FIB_EVENT_NH_DEL:
 -		mlxsw_sp_nexthop4_event(mlxsw_sp, fib_work->event,
 -					fib_work->fnh_info.fib_nh);
 -		fib_info_put(fib_work->fnh_info.fib_nh->nh_parent);
 -		break;
 -	}
 -	rtnl_unlock();
 -	kfree(fib_work);
++	return rt->rt6i_flags & RTF_GATEWAY ||
++	       mlxsw_sp_nexthop6_ipip_type(mlxsw_sp, rt, NULL);
+ }
+ 
 -static void mlxsw_sp_router_fib6_event_work(struct work_struct *work)
++static struct mlxsw_sp_nexthop_group *
++mlxsw_sp_nexthop6_group_create(struct mlxsw_sp *mlxsw_sp,
++			       struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	struct mlxsw_sp_fib_event_work *fib_work =
 -		container_of(work, struct mlxsw_sp_fib_event_work, work);
 -	struct mlxsw_sp *mlxsw_sp = fib_work->mlxsw_sp;
 -	struct fib_rule *rule;
 -	bool replace;
++	struct mlxsw_sp_nexthop_group *nh_grp;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
++	struct mlxsw_sp_nexthop *nh;
++	size_t alloc_size;
++	int i = 0;
+ 	int err;
+ 
 -	rtnl_lock();
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD:
 -		replace = fib_work->event == FIB_EVENT_ENTRY_REPLACE;
 -		err = mlxsw_sp_router_fib6_add(mlxsw_sp,
 -					       fib_work->fen6_info.rt, replace);
++	alloc_size = sizeof(*nh_grp) +
++		     fib6_entry->nrt6 * sizeof(struct mlxsw_sp_nexthop);
++	nh_grp = kzalloc(alloc_size, GFP_KERNEL);
++	if (!nh_grp)
++		return ERR_PTR(-ENOMEM);
++	INIT_LIST_HEAD(&nh_grp->fib_list);
++#if IS_ENABLED(CONFIG_IPV6)
++	nh_grp->neigh_tbl = &nd_tbl;
++#endif
++	mlxsw_sp_rt6 = list_first_entry(&fib6_entry->rt6_list,
++					struct mlxsw_sp_rt6, list);
++	nh_grp->gateway = mlxsw_sp_rt6_is_gateway(mlxsw_sp, mlxsw_sp_rt6->rt);
++	nh_grp->count = fib6_entry->nrt6;
++	for (i = 0; i < nh_grp->count; i++) {
++		struct rt6_info *rt = mlxsw_sp_rt6->rt;
++
++		nh = &nh_grp->nexthops[i];
++		err = mlxsw_sp_nexthop6_init(mlxsw_sp, nh_grp, nh, rt);
+ 		if (err)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		mlxsw_sp_rt6_release(fib_work->fen6_info.rt);
 -		break;
 -	case FIB_EVENT_ENTRY_DEL:
 -		mlxsw_sp_router_fib6_del(mlxsw_sp, fib_work->fen6_info.rt);
 -		mlxsw_sp_rt6_release(fib_work->fen6_info.rt);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		rule = fib_work->fr_info.rule;
 -		if (!fib6_rule_default(rule) && !rule->l3mdev)
 -			mlxsw_sp_router_fib_abort(mlxsw_sp);
 -		fib_rule_put(rule);
 -		break;
++			goto err_nexthop6_init;
++		mlxsw_sp_rt6 = list_next_entry(mlxsw_sp_rt6, list);
+ 	}
 -	rtnl_unlock();
 -	kfree(fib_work);
 -}
+ 
 -static void mlxsw_sp_router_fib4_event(struct mlxsw_sp_fib_event_work *fib_work,
 -				       struct fib_notifier_info *info)
 -{
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD: /* fall through */
 -	case FIB_EVENT_ENTRY_DEL:
 -		memcpy(&fib_work->fen_info, info, sizeof(fib_work->fen_info));
 -		/* Take referece on fib_info to prevent it from being
 -		 * freed while work is queued. Release it afterwards.
 -		 */
 -		fib_info_hold(fib_work->fen_info.fi);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		memcpy(&fib_work->fr_info, info, sizeof(fib_work->fr_info));
 -		fib_rule_get(fib_work->fr_info.rule);
 -		break;
 -	case FIB_EVENT_NH_ADD: /* fall through */
 -	case FIB_EVENT_NH_DEL:
 -		memcpy(&fib_work->fnh_info, info, sizeof(fib_work->fnh_info));
 -		fib_info_hold(fib_work->fnh_info.fib_nh->nh_parent);
 -		break;
++	err = mlxsw_sp_nexthop_group_insert(mlxsw_sp, nh_grp);
++	if (err)
++		goto err_nexthop_group_insert;
++
++	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
++	return nh_grp;
++
++err_nexthop_group_insert:
++err_nexthop6_init:
++	for (i--; i >= 0; i--) {
++		nh = &nh_grp->nexthops[i];
++		mlxsw_sp_nexthop6_fini(mlxsw_sp, nh);
+ 	}
++	kfree(nh_grp);
++	return ERR_PTR(err);
+ }
+ 
 -static void mlxsw_sp_router_fib6_event(struct mlxsw_sp_fib_event_work *fib_work,
 -				       struct fib_notifier_info *info)
++static void
++mlxsw_sp_nexthop6_group_destroy(struct mlxsw_sp *mlxsw_sp,
++				struct mlxsw_sp_nexthop_group *nh_grp)
+ {
 -	switch (fib_work->event) {
 -	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 -	case FIB_EVENT_ENTRY_ADD: /* fall through */
 -	case FIB_EVENT_ENTRY_DEL:
 -		memcpy(&fib_work->fen6_info, info, sizeof(fib_work->fen6_info));
 -		rt6_hold(fib_work->fen6_info.rt);
 -		break;
 -	case FIB_EVENT_RULE_ADD: /* fall through */
 -	case FIB_EVENT_RULE_DEL:
 -		memcpy(&fib_work->fr_info, info, sizeof(fib_work->fr_info));
 -		fib_rule_get(fib_work->fr_info.rule);
 -		break;
++	struct mlxsw_sp_nexthop *nh;
++	int i = nh_grp->count;
++
++	mlxsw_sp_nexthop_group_remove(mlxsw_sp, nh_grp);
++	for (i--; i >= 0; i--) {
++		nh = &nh_grp->nexthops[i];
++		mlxsw_sp_nexthop6_fini(mlxsw_sp, nh);
+ 	}
++	mlxsw_sp_nexthop_group_refresh(mlxsw_sp, nh_grp);
++	WARN_ON(nh_grp->adj_index_valid);
++	kfree(nh_grp);
+ }
+ 
 -/* Called with rcu_read_lock() */
 -static int mlxsw_sp_router_fib_event(struct notifier_block *nb,
 -				     unsigned long event, void *ptr)
++static int mlxsw_sp_nexthop6_group_get(struct mlxsw_sp *mlxsw_sp,
++				       struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	struct mlxsw_sp_fib_event_work *fib_work;
 -	struct fib_notifier_info *info = ptr;
 -	struct mlxsw_sp_router *router;
 -
 -	if (!net_eq(info->net, &init_net) ||
 -	    (info->family != AF_INET && info->family != AF_INET6))
 -		return NOTIFY_DONE;
 -
 -	fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
 -	if (WARN_ON(!fib_work))
 -		return NOTIFY_BAD;
 -
 -	router = container_of(nb, struct mlxsw_sp_router, fib_nb);
 -	fib_work->mlxsw_sp = router->mlxsw_sp;
 -	fib_work->event = event;
++	struct mlxsw_sp_nexthop_group *nh_grp;
+ 
 -	switch (info->family) {
 -	case AF_INET:
 -		INIT_WORK(&fib_work->work, mlxsw_sp_router_fib4_event_work);
 -		mlxsw_sp_router_fib4_event(fib_work, info);
 -		break;
 -	case AF_INET6:
 -		INIT_WORK(&fib_work->work, mlxsw_sp_router_fib6_event_work);
 -		mlxsw_sp_router_fib6_event(fib_work, info);
 -		break;
++	nh_grp = mlxsw_sp_nexthop6_group_lookup(mlxsw_sp, fib6_entry);
++	if (!nh_grp) {
++		nh_grp = mlxsw_sp_nexthop6_group_create(mlxsw_sp, fib6_entry);
++		if (IS_ERR(nh_grp))
++			return PTR_ERR(nh_grp);
+ 	}
+ 
 -	mlxsw_core_schedule_work(&fib_work->work);
++	list_add_tail(&fib6_entry->common.nexthop_group_node,
++		      &nh_grp->fib_list);
++	fib6_entry->common.nh_group = nh_grp;
+ 
 -	return NOTIFY_DONE;
++	return 0;
+ }
+ 
 -static struct mlxsw_sp_rif *
 -mlxsw_sp_rif_find_by_dev(const struct mlxsw_sp *mlxsw_sp,
 -			 const struct net_device *dev)
++static void mlxsw_sp_nexthop6_group_put(struct mlxsw_sp *mlxsw_sp,
++					struct mlxsw_sp_fib_entry *fib_entry)
+ {
 -	int i;
 -
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_RIFS); i++)
 -		if (mlxsw_sp->router->rifs[i] &&
 -		    mlxsw_sp->router->rifs[i]->dev == dev)
 -			return mlxsw_sp->router->rifs[i];
++	struct mlxsw_sp_nexthop_group *nh_grp = fib_entry->nh_group;
+ 
 -	return NULL;
++	list_del(&fib_entry->nexthop_group_node);
++	if (!list_empty(&nh_grp->fib_list))
++		return;
++	mlxsw_sp_nexthop6_group_destroy(mlxsw_sp, nh_grp);
+ }
+ 
 -static int mlxsw_sp_router_rif_disable(struct mlxsw_sp *mlxsw_sp, u16 rif)
++static int
++mlxsw_sp_nexthop6_group_update(struct mlxsw_sp *mlxsw_sp,
++			       struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	char ritr_pl[MLXSW_REG_RITR_LEN];
++	struct mlxsw_sp_nexthop_group *old_nh_grp = fib6_entry->common.nh_group;
+ 	int err;
+ 
 -	mlxsw_reg_ritr_rif_pack(ritr_pl, rif);
 -	err = mlxsw_reg_query(mlxsw_sp->core, MLXSW_REG(ritr), ritr_pl);
 -	if (WARN_ON_ONCE(err))
 -		return err;
 -
 -	mlxsw_reg_ritr_enable_set(ritr_pl, false);
 -	return mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ritr), ritr_pl);
 -}
 -
 -static void mlxsw_sp_router_rif_gone_sync(struct mlxsw_sp *mlxsw_sp,
 -					  struct mlxsw_sp_rif *rif)
 -{
 -	mlxsw_sp_router_rif_disable(mlxsw_sp, rif->rif_index);
 -	mlxsw_sp_nexthop_rif_gone_sync(mlxsw_sp, rif);
 -	mlxsw_sp_neigh_rif_gone_sync(mlxsw_sp, rif);
 -}
++	fib6_entry->common.nh_group = NULL;
++	list_del(&fib6_entry->common.nexthop_group_node);
+ 
 -static bool
 -mlxsw_sp_rif_should_config(struct mlxsw_sp_rif *rif, struct net_device *dev,
 -			   unsigned long event)
 -{
 -	struct inet6_dev *inet6_dev;
 -	bool addr_list_empty = true;
 -	struct in_device *idev;
++	err = mlxsw_sp_nexthop6_group_get(mlxsw_sp, fib6_entry);
++	if (err)
++		goto err_nexthop6_group_get;
+ 
 -	switch (event) {
 -	case NETDEV_UP:
 -		return rif == NULL;
 -	case NETDEV_DOWN:
 -		idev = __in_dev_get_rtnl(dev);
 -		if (idev && idev->ifa_list)
 -			addr_list_empty = false;
++	/* In case this entry is offloaded, then the adjacency index
++	 * currently associated with it in the device's table is that
++	 * of the old group. Start using the new one instead.
++	 */
++	err = mlxsw_sp_fib_node_entry_add(mlxsw_sp, &fib6_entry->common);
++	if (err)
++		goto err_fib_node_entry_add;
+ 
 -		inet6_dev = __in6_dev_get(dev);
 -		if (addr_list_empty && inet6_dev &&
 -		    !list_empty(&inet6_dev->addr_list))
 -			addr_list_empty = false;
++	if (list_empty(&old_nh_grp->fib_list))
++		mlxsw_sp_nexthop6_group_destroy(mlxsw_sp, old_nh_grp);
+ 
 -		if (rif && addr_list_empty &&
 -		    !netif_is_l3_slave(rif->dev))
 -			return true;
 -		/* It is possible we already removed the RIF ourselves
 -		 * if it was assigned to a netdev that is now a bridge
 -		 * or LAG slave.
 -		 */
 -		return false;
 -	}
++	return 0;
+ 
 -	return false;
++err_fib_node_entry_add:
++	mlxsw_sp_nexthop6_group_put(mlxsw_sp, &fib6_entry->common);
++err_nexthop6_group_get:
++	list_add_tail(&fib6_entry->common.nexthop_group_node,
++		      &old_nh_grp->fib_list);
++	fib6_entry->common.nh_group = old_nh_grp;
++	return err;
+ }
+ 
 -static enum mlxsw_sp_rif_type
 -mlxsw_sp_dev_rif_type(const struct mlxsw_sp *mlxsw_sp,
 -		      const struct net_device *dev)
++static int
++mlxsw_sp_fib6_entry_nexthop_add(struct mlxsw_sp *mlxsw_sp,
++				struct mlxsw_sp_fib6_entry *fib6_entry,
++				struct rt6_info *rt)
+ {
 -	enum mlxsw_sp_fid_type type;
 -
 -	if (mlxsw_sp_netdev_ipip_type(mlxsw_sp, dev, NULL))
 -		return MLXSW_SP_RIF_TYPE_IPIP_LB;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
++	int err;
+ 
 -	/* Otherwise RIF type is derived from the type of the underlying FID. */
 -	if (is_vlan_dev(dev) && netif_is_bridge_master(vlan_dev_real_dev(dev)))
 -		type = MLXSW_SP_FID_TYPE_8021Q;
 -	else if (netif_is_bridge_master(dev) && br_vlan_enabled(dev))
 -		type = MLXSW_SP_FID_TYPE_8021Q;
 -	else if (netif_is_bridge_master(dev))
 -		type = MLXSW_SP_FID_TYPE_8021D;
 -	else
 -		type = MLXSW_SP_FID_TYPE_RFID;
++	mlxsw_sp_rt6 = mlxsw_sp_rt6_create(rt);
++	if (IS_ERR(mlxsw_sp_rt6))
++		return PTR_ERR(mlxsw_sp_rt6);
+ 
 -	return mlxsw_sp_fid_type_rif_type(mlxsw_sp, type);
 -}
++	list_add_tail(&mlxsw_sp_rt6->list, &fib6_entry->rt6_list);
++	fib6_entry->nrt6++;
+ 
 -static int mlxsw_sp_rif_index_alloc(struct mlxsw_sp *mlxsw_sp, u16 *p_rif_index)
 -{
 -	int i;
++	err = mlxsw_sp_nexthop6_group_update(mlxsw_sp, fib6_entry);
++	if (err)
++		goto err_nexthop6_group_update;
+ 
 -	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_RIFS); i++) {
 -		if (!mlxsw_sp->router->rifs[i]) {
 -			*p_rif_index = i;
 -			return 0;
 -		}
 -	}
++	return 0;
+ 
 -	return -ENOBUFS;
++err_nexthop6_group_update:
++	fib6_entry->nrt6--;
++	list_del(&mlxsw_sp_rt6->list);
++	mlxsw_sp_rt6_destroy(mlxsw_sp_rt6);
++	return err;
+ }
+ 
 -static struct mlxsw_sp_rif *mlxsw_sp_rif_alloc(size_t rif_size, u16 rif_index,
 -					       u16 vr_id,
 -					       struct net_device *l3_dev)
++static void
++mlxsw_sp_fib6_entry_nexthop_del(struct mlxsw_sp *mlxsw_sp,
++				struct mlxsw_sp_fib6_entry *fib6_entry,
++				struct rt6_info *rt)
+ {
 -	struct mlxsw_sp_rif *rif;
 -
 -	rif = kzalloc(rif_size, GFP_KERNEL);
 -	if (!rif)
 -		return NULL;
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6;
+ 
 -	INIT_LIST_HEAD(&rif->nexthop_list);
 -	INIT_LIST_HEAD(&rif->neigh_list);
 -	ether_addr_copy(rif->addr, l3_dev->dev_addr);
 -	rif->mtu = l3_dev->mtu;
 -	rif->vr_id = vr_id;
 -	rif->dev = l3_dev;
 -	rif->rif_index = rif_index;
++	mlxsw_sp_rt6 = mlxsw_sp_fib6_entry_rt_find(fib6_entry, rt);
++	if (WARN_ON(!mlxsw_sp_rt6))
++		return;
+ 
 -	return rif;
++	fib6_entry->nrt6--;
++	list_del(&mlxsw_sp_rt6->list);
++	mlxsw_sp_nexthop6_group_update(mlxsw_sp, fib6_entry);
++	mlxsw_sp_rt6_destroy(mlxsw_sp_rt6);
+ }
+ 
 -struct mlxsw_sp_rif *mlxsw_sp_rif_by_index(const struct mlxsw_sp *mlxsw_sp,
 -					   u16 rif_index)
++static void mlxsw_sp_fib6_entry_type_set(struct mlxsw_sp *mlxsw_sp,
++					 struct mlxsw_sp_fib_entry *fib_entry,
++					 const struct rt6_info *rt)
+ {
 -	return mlxsw_sp->router->rifs[rif_index];
++	/* Packets hitting RTF_REJECT routes need to be discarded by the
++	 * stack. We can rely on their destination device not having a
++	 * RIF (it's the loopback device) and can thus use action type
++	 * local, which will cause them to be trapped with a lower
++	 * priority than packets that need to be locally received.
++	 */
++	if (rt->rt6i_flags & (RTF_LOCAL | RTF_ANYCAST))
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_TRAP;
++	else if (rt->rt6i_flags & RTF_REJECT)
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_LOCAL;
++	else if (mlxsw_sp_rt6_is_gateway(mlxsw_sp, rt))
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_REMOTE;
++	else
++		fib_entry->type = MLXSW_SP_FIB_ENTRY_TYPE_LOCAL;
+ }
+ 
 -u16 mlxsw_sp_rif_index(const struct mlxsw_sp_rif *rif)
++static void
++mlxsw_sp_fib6_entry_rt_destroy_all(struct mlxsw_sp_fib6_entry *fib6_entry)
+ {
 -	return rif->rif_index;
 -}
++	struct mlxsw_sp_rt6 *mlxsw_sp_rt6, *tmp;
+ 
 -u16 mlxsw_sp_ipip_lb_rif_index(const struct mlxsw_sp_rif_ipip_lb *lb_rif)
 -{
 -	return lb_rif->common.rif_index;
++	list_for_each_entry_safe(mlxsw_sp_rt6, tmp, &fib6_entry->rt6_list,
++				 list) {
++		fib6_entry->nrt6--;
++		list_del(&mlxsw_sp_rt6->list);
++		mlxsw_sp_rt6_destroy(mlxsw_sp_rt6);
++	}
+ }
+ 
 -u16 mlxsw_sp_ipip_lb_ul_vr_id(const struct mlxsw_sp_rif_ipip_lb *lb_rif)
++static struct mlxsw_sp_fib6_entry *
++mlxsw_sp_fib6_entry_create(struct mlxsw_sp *mlxsw_sp,
++			   struct mlxsw_sp_fib_node *fib_node,
++			   struct rt6_info *rt)
+ {
 -	return lb_rif->ul_vr_id;
 -}
++	struct mlxsw_sp_fib6_entry *fib6_entry;
++>>>>>>> a5390278a5eb (mlxsw: spectrum: Add support for setting counters on nexthops)
 +	struct mlxsw_sp_fib_entry *fib_entry;
 +	struct mlxsw_sp_fib_node *fib_node;
  
 -int mlxsw_sp_rif_dev_ifindex(const struct mlxsw_sp_rif *rif)
 -{
 -	return rif->dev->ifindex;
 -}
 +	if (mlxsw_sp->router.aborted)
 +		return;
  
 -const struct net_device *mlxsw_sp_rif_dev(const struct mlxsw_sp_rif *rif)
 -{
 -	return rif->dev;
 +	fib_entry = mlxsw_sp_fib4_entry_lookup(mlxsw_sp, fen_info);
 +	if (WARN_ON(!fib_entry))
 +		return;
 +	fib_node = fib_entry->fib_node;
 +
 +	mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib_entry);
 +	mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib_entry);
 +	mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
  }
  
 -static struct mlxsw_sp_rif *
 -mlxsw_sp_rif_create(struct mlxsw_sp *mlxsw_sp,
 -		    const struct mlxsw_sp_rif_params *params)
 +static int mlxsw_sp_router_set_abort_trap(struct mlxsw_sp *mlxsw_sp)
  {
 -	u32 tb_id = l3mdev_fib_table(params->dev);
 -	const struct mlxsw_sp_rif_ops *ops;
 -	struct mlxsw_sp_fid *fid = NULL;
 -	enum mlxsw_sp_rif_type type;
 -	struct mlxsw_sp_rif *rif;
 -	struct mlxsw_sp_vr *vr;
 -	u16 rif_index;
 -	int err;
 -
 -	type = mlxsw_sp_dev_rif_type(mlxsw_sp, params->dev);
 -	ops = mlxsw_sp->router->rif_ops_arr[type];
 -
 -	vr = mlxsw_sp_vr_get(mlxsw_sp, tb_id ? : RT_TABLE_MAIN);
 -	if (IS_ERR(vr))
 -		return ERR_CAST(vr);
 +	char ralta_pl[MLXSW_REG_RALTA_LEN];
 +	char ralst_pl[MLXSW_REG_RALST_LEN];
 +	int i, err;
  
 -	err = mlxsw_sp_rif_index_alloc(mlxsw_sp, &rif_index);
 +	mlxsw_reg_ralta_pack(ralta_pl, true, MLXSW_REG_RALXX_PROTOCOL_IPV4,
 +			     MLXSW_SP_LPM_TREE_MIN);
 +	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralta), ralta_pl);
  	if (err)
 -		goto err_rif_index_alloc;
 +		return err;
  
 -	rif = mlxsw_sp_rif_alloc(ops->rif_size, rif_index, vr->id, params->dev);
 -	if (!rif) {
 -		err = -ENOMEM;
 -		goto err_rif_alloc;
 -	}
 -	rif->mlxsw_sp = mlxsw_sp;
 -	rif->ops = ops;
 -
 -	if (ops->fid_get) {
 -		fid = ops->fid_get(rif);
 -		if (IS_ERR(fid)) {
 -			err = PTR_ERR(fid);
 -			goto err_fid_get;
 -		}
 -		rif->fid = fid;
 -	}
 +	mlxsw_reg_ralst_pack(ralst_pl, 0xff, MLXSW_SP_LPM_TREE_MIN);
 +	err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralst), ralst_pl);
 +	if (err)
 +		return err;
  
 -	if (ops->setup)
 -		ops->setup(rif, params);
 +	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 +		struct mlxsw_sp_vr *vr = &mlxsw_sp->router.vrs[i];
 +		char raltb_pl[MLXSW_REG_RALTB_LEN];
 +		char ralue_pl[MLXSW_REG_RALUE_LEN];
  
 -	err = ops->configure(rif);
 -	if (err)
 -		goto err_configure;
 +		if (!mlxsw_sp_vr_is_used(vr))
 +			continue;
  
 -	mlxsw_sp_rif_counters_alloc(rif);
 -	mlxsw_sp->router->rifs[rif_index] = rif;
 -	vr->rif_count++;
 +		mlxsw_reg_raltb_pack(raltb_pl, vr->id,
 +				     MLXSW_REG_RALXX_PROTOCOL_IPV4,
 +				     MLXSW_SP_LPM_TREE_MIN);
 +		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(raltb),
 +				      raltb_pl);
 +		if (err)
 +			return err;
  
 -	return rif;
 +		mlxsw_reg_ralue_pack4(ralue_pl, MLXSW_SP_L3_PROTO_IPV4,
 +				      MLXSW_REG_RALUE_OP_WRITE_WRITE, vr->id, 0,
 +				      0);
 +		mlxsw_reg_ralue_act_ip2me_pack(ralue_pl);
 +		err = mlxsw_reg_write(mlxsw_sp->core, MLXSW_REG(ralue),
 +				      ralue_pl);
 +		if (err)
 +			return err;
 +	}
  
 -err_configure:
 -	if (fid)
 -		mlxsw_sp_fid_put(fid);
 -err_fid_get:
 -	kfree(rif);
 -err_rif_alloc:
 -err_rif_index_alloc:
 -	mlxsw_sp_vr_put(vr);
 -	return ERR_PTR(err);
 +	return 0;
  }
  
 -void mlxsw_sp_rif_destroy(struct mlxsw_sp_rif *rif)
 +static void mlxsw_sp_fib4_node_flush(struct mlxsw_sp *mlxsw_sp,
 +				     struct mlxsw_sp_fib_node *fib_node)
  {
 -	const struct mlxsw_sp_rif_ops *ops = rif->ops;
 -	struct mlxsw_sp *mlxsw_sp = rif->mlxsw_sp;
 -	struct mlxsw_sp_fid *fid = rif->fid;
 -	struct mlxsw_sp_vr *vr;
 +	struct mlxsw_sp_fib_entry *fib_entry, *tmp;
  
 -	mlxsw_sp_router_rif_gone_sync(mlxsw_sp, rif);
 -	vr = &mlxsw_sp->router->vrs[rif->vr_id];
 +	list_for_each_entry_safe(fib_entry, tmp, &fib_node->entry_list, list) {
 +		bool do_break = &tmp->list == &fib_node->entry_list;
  
 -	vr->rif_count--;
 -	mlxsw_sp->router->rifs[rif->rif_index] = NULL;
 -	mlxsw_sp_rif_counters_free(rif);
 -	ops->deconfigure(rif);
 -	if (fid)
 -		/* Loopback RIFs are not associated with a FID. */
 -		mlxsw_sp_fid_put(fid);
 -	kfree(rif);
 -	mlxsw_sp_vr_put(vr);
 +		mlxsw_sp_fib4_node_entry_unlink(mlxsw_sp, fib_entry);
 +		mlxsw_sp_fib4_entry_destroy(mlxsw_sp, fib_entry);
 +		mlxsw_sp_fib_node_put(mlxsw_sp, fib_node);
 +		/* Break when entry list is empty and node was freed.
 +		 * Otherwise, we'll access freed memory in the next
 +		 * iteration.
 +		 */
 +		if (do_break)
 +			break;
 +	}
  }
  
 -static void
 -mlxsw_sp_rif_subport_params_init(struct mlxsw_sp_rif_params *params,
 -				 struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan)
 +static void mlxsw_sp_fib_node_flush(struct mlxsw_sp *mlxsw_sp,
 +				    struct mlxsw_sp_fib_node *fib_node)
  {
 -	struct mlxsw_sp_port *mlxsw_sp_port = mlxsw_sp_port_vlan->mlxsw_sp_port;
 -
 -	params->vid = mlxsw_sp_port_vlan->vid;
 -	params->lag = mlxsw_sp_port->lagged;
 -	if (params->lag)
 -		params->lag_id = mlxsw_sp_port->lag_id;
 -	else
 -		params->system_port = mlxsw_sp_port->local_port;
 +	switch (fib_node->fib->proto) {
 +	case MLXSW_SP_L3_PROTO_IPV4:
 +		mlxsw_sp_fib4_node_flush(mlxsw_sp, fib_node);
 +		break;
 +	case MLXSW_SP_L3_PROTO_IPV6:
 +		WARN_ON_ONCE(1);
 +		break;
 +	}
  }
  
 -static int
 -mlxsw_sp_port_vlan_router_join(struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan,
 -			       struct net_device *l3_dev)
 +static void mlxsw_sp_vr_fib_flush(struct mlxsw_sp *mlxsw_sp,
 +				  struct mlxsw_sp_vr *vr,
 +				  enum mlxsw_sp_l3proto proto)
  {
 -	struct mlxsw_sp_port *mlxsw_sp_port = mlxsw_sp_port_vlan->mlxsw_sp_port;
 -	struct mlxsw_sp *mlxsw_sp = mlxsw_sp_port->mlxsw_sp;
 -	u16 vid = mlxsw_sp_port_vlan->vid;
 -	struct mlxsw_sp_rif *rif;
 -	struct mlxsw_sp_fid *fid;
 -	int err;
 +	struct mlxsw_sp_fib *fib = mlxsw_sp_vr_fib(vr, proto);
 +	struct mlxsw_sp_fib_node *fib_node, *tmp;
  
 -	rif = mlxsw_sp_rif_find_by_dev(mlxsw_sp, l3_dev);
 -	if (!rif) {
 -		struct mlxsw_sp_rif_params params = {
 -			.dev = l3_dev,
 -		};
 +	list_for_each_entry_safe(fib_node, tmp, &fib->node_list, list) {
 +		bool do_break = &tmp->list == &fib->node_list;
  
 -		mlxsw_sp_rif_subport_params_init(&params, mlxsw_sp_port_vlan);
 -		rif = mlxsw_sp_rif_create(mlxsw_sp, &params);
 -		if (IS_ERR(rif))
 -			return PTR_ERR(rif);
 +		mlxsw_sp_fib_node_flush(mlxsw_sp, fib_node);
 +		if (do_break)
 +			break;
  	}
 +}
  
 -	/* FID was already created, just take a reference */
 -	fid = rif->ops->fid_get(rif);
 -	err = mlxsw_sp_fid_port_vid_map(fid, mlxsw_sp_port, vid);
 -	if (err)
 -		goto err_fid_port_vid_map;
 +static void mlxsw_sp_router_fib_flush(struct mlxsw_sp *mlxsw_sp)
 +{
 +	int i;
 +
 +	for (i = 0; i < MLXSW_CORE_RES_GET(mlxsw_sp->core, MAX_VRS); i++) {
 +		struct mlxsw_sp_vr *vr = &mlxsw_sp->router.vrs[i];
 +
 +		if (!mlxsw_sp_vr_is_used(vr))
 +			continue;
 +		mlxsw_sp_vr_fib_flush(mlxsw_sp, vr, MLXSW_SP_L3_PROTO_IPV4);
 +	}
 +}
  
 -	err = mlxsw_sp_port_vid_learning_set(mlxsw_sp_port, vid, false);
 -	if (err)
 -		goto err_port_vid_learning_set;
 +static void mlxsw_sp_router_fib4_abort(struct mlxsw_sp *mlxsw_sp)
 +{
 +	int err;
  
 -	err = mlxsw_sp_port_vid_stp_set(mlxsw_sp_port, vid,
 -					BR_STATE_FORWARDING);
 +	if (mlxsw_sp->router.aborted)
 +		return;
 +	dev_warn(mlxsw_sp->bus_info->dev, "FIB abort triggered. Note that FIB entries are no longer being offloaded to this device.\n");
 +	mlxsw_sp_router_fib_flush(mlxsw_sp);
 +	mlxsw_sp->router.aborted = true;
 +	err = mlxsw_sp_router_set_abort_trap(mlxsw_sp);
  	if (err)
 -		goto err_port_vid_stp_set;
 +		dev_warn(mlxsw_sp->bus_info->dev, "Failed to set abort trap.\n");
 +}
  
 -	mlxsw_sp_port_vlan->fid = fid;
 +struct mlxsw_sp_fib_event_work {
 +	struct work_struct work;
 +	union {
 +		struct fib_entry_notifier_info fen_info;
 +		struct fib_nh_notifier_info fnh_info;
 +	};
 +	struct mlxsw_sp *mlxsw_sp;
 +	unsigned long event;
 +};
  
 -	return 0;
 +static void mlxsw_sp_router_fib_event_work(struct work_struct *work)
 +{
 +	struct mlxsw_sp_fib_event_work *fib_work =
 +		container_of(work, struct mlxsw_sp_fib_event_work, work);
 +	struct mlxsw_sp *mlxsw_sp = fib_work->mlxsw_sp;
 +	bool replace, append;
 +	int err;
  
 -err_port_vid_stp_set:
 -	mlxsw_sp_port_vid_learning_set(mlxsw_sp_port, vid, true);
 -err_port_vid_learning_set:
 -	mlxsw_sp_fid_port_vid_unmap(fid, mlxsw_sp_port, vid);
 -err_fid_port_vid_map:
 -	mlxsw_sp_fid_put(fid);
 -	return err;
 +	/* Protect internal structures from changes */
 +	rtnl_lock();
 +	switch (fib_work->event) {
 +	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 +	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 +	case FIB_EVENT_ENTRY_ADD:
 +		replace = fib_work->event == FIB_EVENT_ENTRY_REPLACE;
 +		append = fib_work->event == FIB_EVENT_ENTRY_APPEND;
 +		err = mlxsw_sp_router_fib4_add(mlxsw_sp, &fib_work->fen_info,
 +					       replace, append);
 +		if (err)
 +			mlxsw_sp_router_fib4_abort(mlxsw_sp);
 +		fib_info_put(fib_work->fen_info.fi);
 +		break;
 +	case FIB_EVENT_ENTRY_DEL:
 +		mlxsw_sp_router_fib4_del(mlxsw_sp, &fib_work->fen_info);
 +		fib_info_put(fib_work->fen_info.fi);
 +		break;
 +	case FIB_EVENT_RULE_ADD: /* fall through */
 +	case FIB_EVENT_RULE_DEL:
 +		mlxsw_sp_router_fib4_abort(mlxsw_sp);
 +		break;
 +	case FIB_EVENT_NH_ADD: /* fall through */
 +	case FIB_EVENT_NH_DEL:
 +		mlxsw_sp_nexthop_event(mlxsw_sp, fib_work->event,
 +				       fib_work->fnh_info.fib_nh);
 +		fib_info_put(fib_work->fnh_info.fib_nh->nh_parent);
 +		break;
 +	}
 +	rtnl_unlock();
 +	kfree(fib_work);
  }
  
 -void
 -mlxsw_sp_port_vlan_router_leave(struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan)
 +/* Called with rcu_read_lock() */
 +static int mlxsw_sp_router_fib_event(struct notifier_block *nb,
 +				     unsigned long event, void *ptr)
  {
 -	struct mlxsw_sp_port *mlxsw_sp_port = mlxsw_sp_port_vlan->mlxsw_sp_port;
 -	struct mlxsw_sp_fid *fid = mlxsw_sp_port_vlan->fid;
 -	u16 vid = mlxsw_sp_port_vlan->vid;
 -
 -	if (WARN_ON(mlxsw_sp_fid_type(fid) != MLXSW_SP_FID_TYPE_RFID))
 -		return;
 +	struct mlxsw_sp *mlxsw_sp = container_of(nb, struct mlxsw_sp, fib_nb);
 +	struct mlxsw_sp_fib_event_work *fib_work;
 +	struct fib_notifier_info *info = ptr;
  
 -	mlxsw_sp_port_vlan->fid = NULL;
 -	mlxsw_sp_port_vid_stp_set(mlxsw_sp_port, vid, BR_STATE_BLOCKING);
 -	mlxsw_sp_port_vid_learning_set(mlxsw_sp_port, vid, true);
 -	mlxsw_sp_fid_port_vid_unmap(fid, mlxsw_sp_port, vid);
 -	/* If router port holds the last reference on the rFID, then the
 -	 * associated Sub-port RIF will be destroyed.
 -	 */
 -	mlxsw_sp_fid_put(fid);
 -}
 +	if (!net_eq(info->net, &init_net) ||
 +	    (info->family != AF_INET && info->family != AF_INET6))
 +		return NOTIFY_DONE;
  
 -static int mlxsw_sp_inetaddr_port_vlan_event(struct net_device *l3_dev,
 -					     struct net_device *port_dev,
 -					     unsigned long event, u16 vid)
 -{
 -	struct mlxsw_sp_port *mlxsw_sp_port = netdev_priv(port_dev);
 -	struct mlxsw_sp_port_vlan *mlxsw_sp_port_vlan;
 +	fib_work = kzalloc(sizeof(*fib_work), GFP_ATOMIC);
 +	if (WARN_ON(!fib_work))
 +		return NOTIFY_BAD;
  
 -	mlxsw_sp_port_vlan = mlxsw_sp_port_vlan_find_by_vid(mlxsw_sp_port, vid);
 -	if (WARN_ON(!mlxsw_sp_port_vlan))
 -		return -EINVAL;
 +	INIT_WORK(&fib_work->work, mlxsw_sp_router_fib_event_work);
 +	fib_work->mlxsw_sp = mlxsw_sp;
 +	fib_work->event = event;
  
  	switch (event) {
 -	case NETDEV_UP:
 -		return mlxsw_sp_port_vlan_router_join(mlxsw_sp_port_vlan,
 -						      l3_dev);
 -	case NETDEV_DOWN:
 -		mlxsw_sp_port_vlan_router_leave(mlxsw_sp_port_vlan);
 +	case FIB_EVENT_ENTRY_REPLACE: /* fall through */
 +	case FIB_EVENT_ENTRY_APPEND: /* fall through */
 +	case FIB_EVENT_ENTRY_ADD: /* fall through */
 +	case FIB_EVENT_ENTRY_DEL:
 +		memcpy(&fib_work->fen_info, ptr, sizeof(fib_work->fen_info));
 +		/* Take referece on fib_info to prevent it from being
 +		 * freed while work is queued. Release it afterwards.
 +		 */
 +		fib_info_hold(fib_work->fen_info.fi);
 +		break;
 +	case FIB_EVENT_NH_ADD: /* fall through */
 +	case FIB_EVENT_NH_DEL:
 +		memcpy(&fib_work->fnh_info, ptr, sizeof(fib_work->fnh_info));
 +		fib_info_hold(fib_work->fnh_info.fib_nh->nh_parent);
  		break;
  	}
  
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_router.h
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_router.h
