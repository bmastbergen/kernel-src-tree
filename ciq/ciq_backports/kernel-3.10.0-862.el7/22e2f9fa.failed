iommu/vt-d: Use per-cpu IOVA caching

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] vt-d: Use per-cpu IOVA caching (Jerry Snitselaar) [1499325]
Rebuild_FUZZ: 90.91%
commit-author Omer Peleg <omer@cs.technion.ac.il>
commit 22e2f9fa63b092923873fc8a52955151f4d83274
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/22e2f9fa.failed

Commit 9257b4a2 ('iommu/iova: introduce per-cpu caching to iova allocation')
introduced per-CPU IOVA caches to massively improve scalability. Use them.

	Signed-off-by: Omer Peleg <omer@cs.technion.ac.il>
[mad@cs.technion.ac.il: rebased, cleaned up and reworded the commit message]
	Signed-off-by: Adam Morrison <mad@cs.technion.ac.il>
	Reviewed-by: Shaohua Li <shli@fb.com>
	Reviewed-by: Ben Serebrin <serebrin@google.com>
[dwmw2: split out VT-d part into a separate patch]
	Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>
(cherry picked from commit 22e2f9fa63b092923873fc8a52955151f4d83274)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/intel-iommu.c
diff --cc drivers/iommu/intel-iommu.c
index ae278e3a91b2,76e833278db0..000000000000
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@@ -3265,19 -3370,19 +3265,30 @@@ static struct iova *intel_alloc_iova(st
  		 * DMA_BIT_MASK(32) and if that fails then try allocating
  		 * from higher range
  		 */
++<<<<<<< HEAD
 +		iova = alloc_iova(&domain->iovad, nrpages,
 +				  IOVA_PFN(DMA_BIT_MASK(32)), 1);
 +		if (iova)
 +			return iova;
++=======
+ 		iova_pfn = alloc_iova_fast(&domain->iovad, nrpages,
+ 					   IOVA_PFN(DMA_BIT_MASK(32)));
+ 		if (iova_pfn)
+ 			return iova_pfn;
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  	}
- 	iova = alloc_iova(&domain->iovad, nrpages, IOVA_PFN(dma_mask), 1);
- 	if (unlikely(!iova)) {
+ 	iova_pfn = alloc_iova_fast(&domain->iovad, nrpages, IOVA_PFN(dma_mask));
+ 	if (unlikely(!iova_pfn)) {
  		pr_err("Allocating %ld-page iova for %s failed",
  		       nrpages, dev_name(dev));
 -		return 0;
 +		return NULL;
  	}
  
++<<<<<<< HEAD
 +	return iova;
++=======
+ 	return iova_pfn;
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  }
  
  static struct dmar_domain *__get_valid_domain_for_dev(struct device *dev)
@@@ -3430,8 -3535,8 +3441,13 @@@ static dma_addr_t __intel_map_single(st
  	return start_paddr;
  
  error:
++<<<<<<< HEAD
 +	if (iova)
 +		__free_iova(&domain->iovad, iova);
++=======
+ 	if (iova_pfn)
+ 		free_iova_fast(&domain->iovad, iova_pfn, dma_to_mm_pfn(size));
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  	pr_err("Device %s request: %zx@%llx dir %d --- failed\n",
  		dev_name(dev), size, (unsigned long long)paddr, dir);
  	return 0;
@@@ -3476,15 -3584,14 +3492,19 @@@ static void flush_unmaps(void
  			/* On real hardware multiple invalidations are expensive */
  			if (cap_caching_mode(iommu->cap))
  				iommu_flush_iotlb_psi(iommu, domain,
 -					mm_to_dma_pfn(iova_pfn),
 -					nrpages, !freelist, 0);
 +					mm_to_dma_pfn(iova->pfn_lo),
 +					mm_to_dma_pfn(iova_size(iova)),
 +					!freelist, 0);
  			else {
 -				mask = ilog2(nrpages);
 +				mask = ilog2(mm_to_dma_pfn(iova_size(iova)));
  				iommu_flush_dev_iotlb(domain,
 -						(uint64_t)iova_pfn << PAGE_SHIFT, mask);
 +						(uint64_t)iova->pfn_lo << PAGE_SHIFT, mask);
  			}
++<<<<<<< HEAD
 +			__free_iova(&domain->iovad, iova);
++=======
+ 			free_iova_fast(&domain->iovad, iova_pfn, nrpages);
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  			if (freelist)
  				dma_free_pagelist(freelist);
  		}
@@@ -3564,12 -3689,12 +3584,16 @@@ static void intel_unmap(struct device *
  
  	if (intel_iommu_strict) {
  		iommu_flush_iotlb_psi(iommu, domain, start_pfn,
 -				      nrpages, !freelist, 0);
 +				      last_pfn - start_pfn + 1, !freelist, 0);
  		/* free iova */
++<<<<<<< HEAD
 +		__free_iova(&domain->iovad, iova);
++=======
+ 		free_iova_fast(&domain->iovad, iova_pfn, dma_to_mm_pfn(nrpages));
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  		dma_free_pagelist(freelist);
  	} else {
 -		add_unmap(domain, iova_pfn, nrpages, freelist);
 +		add_unmap(domain, iova, freelist);
  		/*
  		 * queue up the release of the unmap to save the 1/6th of the
  		 * cpu used up by the iotlb flush operation...
@@@ -3699,7 -3849,7 +3723,11 @@@ static int intel_map_sg(struct device *
  	if (unlikely(ret)) {
  		dma_pte_free_pagetable(domain, start_vpfn,
  				       start_vpfn + size - 1);
++<<<<<<< HEAD
 +		__free_iova(&domain->iovad, iova);
++=======
+ 		free_iova_fast(&domain->iovad, iova_pfn, dma_to_mm_pfn(size));
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  		return 0;
  	}
  
@@@ -4425,15 -4588,47 +4453,53 @@@ static struct notifier_block intel_iomm
  	.priority = 0
  };
  
++<<<<<<< HEAD
 +static void intel_disable_iommus(void)
++=======
+ static void free_all_cpu_cached_iovas(unsigned int cpu)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < g_num_of_iommus; i++) {
+ 		struct intel_iommu *iommu = g_iommus[i];
+ 		struct dmar_domain *domain;
+ 		u16 did;
+ 
+ 		if (!iommu)
+ 			continue;
+ 
+ 		for (did = 0; did < 0xffff; did++) {
+ 			domain = get_iommu_domain(iommu, did);
+ 
+ 			if (!domain)
+ 				continue;
+ 			free_cpu_cached_iovas(cpu, &domain->iovad);
+ 		}
+ 	}
+ }
+ 
+ static int intel_iommu_cpu_notifier(struct notifier_block *nfb,
+ 				    unsigned long action, void *v)
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  {
 -	unsigned int cpu = (unsigned long)v;
 +	struct intel_iommu *iommu = NULL;
 +	struct dmar_drhd_unit *drhd;
  
++<<<<<<< HEAD
 +	for_each_iommu(iommu, drhd)
 +		iommu_disable_translation(iommu);
++=======
+ 	switch (action) {
+ 	case CPU_DEAD:
+ 	case CPU_DEAD_FROZEN:
+ 		free_all_cpu_cached_iovas(cpu);
+ 		flush_unmaps_timeout(cpu);
+ 		break;
+ 	}
+ 	return NOTIFY_OK;
++>>>>>>> 22e2f9fa63b0 (iommu/vt-d: Use per-cpu IOVA caching)
  }
  
 -static struct notifier_block intel_iommu_cpu_nb = {
 -	.notifier_call = intel_iommu_cpu_notifier,
 -};
 -
  static ssize_t intel_iommu_show_version(struct device *dev,
  					struct device_attribute *attr,
  					char *buf)
* Unmerged path drivers/iommu/intel-iommu.c
