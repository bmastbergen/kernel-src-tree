ibmvnic: Create init and release routines for the tx pool

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Nathan Fontenot <nfont@linux.vnet.ibm.com>
commit c657e32cd0555e97ae8903f8a5a9d7c2f3579650
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c657e32c.failed

Move the initialization and the release of the tx pool to their own routines,
and update them to do validation. This also adds validation to the release
of the long term buffer.

	Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c657e32cd0555e97ae8903f8a5a9d7c2f3579650)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/ibm/ibmvnic.c
diff --cc drivers/net/ethernet/ibm/ibmvnic.c
index 6a325c61534d,a9399e96e942..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.c
+++ b/drivers/net/ethernet/ibm/ibmvnic.c
@@@ -368,11 -375,155 +371,158 @@@ static void free_rx_pool(struct ibmvnic
  	pool->rx_buff = NULL;
  }
  
- static int ibmvnic_open(struct net_device *netdev)
++<<<<<<< HEAD
++=======
+ static void release_tx_pools(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_tx_pool *tx_pool;
+ 	int i, tx_scrqs;
+ 
+ 	if (!adapter->tx_pool)
+ 		return;
+ 
+ 	tx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	for (i = 0; i < tx_scrqs; i++) {
+ 		tx_pool = &adapter->tx_pool[i];
+ 		kfree(tx_pool->tx_buff);
+ 		free_long_term_buff(adapter, &tx_pool->long_term_buff);
+ 		kfree(tx_pool->free_map);
+ 	}
+ 
+ 	kfree(adapter->tx_pool);
+ 	adapter->tx_pool = NULL;
+ }
+ 
+ static int init_tx_pools(struct net_device *netdev)
  {
  	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
  	struct device *dev = &adapter->vdev->dev;
  	struct ibmvnic_tx_pool *tx_pool;
+ 	int tx_subcrqs;
+ 	int i, j;
+ 
+ 	tx_subcrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
+ 	adapter->tx_pool = kcalloc(tx_subcrqs,
+ 				   sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);
+ 	if (!adapter->tx_pool)
+ 		return -1;
+ 
+ 	for (i = 0; i < tx_subcrqs; i++) {
+ 		tx_pool = &adapter->tx_pool[i];
+ 		tx_pool->tx_buff = kcalloc(adapter->req_tx_entries_per_subcrq,
+ 					   sizeof(struct ibmvnic_tx_buff),
+ 					   GFP_KERNEL);
+ 		if (!tx_pool->tx_buff) {
+ 			dev_err(dev, "tx pool buffer allocation failed\n");
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		if (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,
+ 					 adapter->req_tx_entries_per_subcrq *
+ 					 adapter->req_mtu)) {
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		tx_pool->free_map = kcalloc(adapter->req_tx_entries_per_subcrq,
+ 					    sizeof(int), GFP_KERNEL);
+ 		if (!tx_pool->free_map) {
+ 			release_tx_pools(adapter);
+ 			return -1;
+ 		}
+ 
+ 		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
+ 			tx_pool->free_map[j] = j;
+ 
+ 		tx_pool->consumer_index = 0;
+ 		tx_pool->producer_index = 0;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void release_bounce_buffer(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 
+ 	if (!adapter->bounce_buffer)
+ 		return;
+ 
+ 	if (!dma_mapping_error(dev, adapter->bounce_buffer_dma)) {
+ 		dma_unmap_single(dev, adapter->bounce_buffer_dma,
+ 				 adapter->bounce_buffer_size,
+ 				 DMA_BIDIRECTIONAL);
+ 		adapter->bounce_buffer_dma = DMA_ERROR_CODE;
+ 	}
+ 
+ 	kfree(adapter->bounce_buffer);
+ 	adapter->bounce_buffer = NULL;
+ }
+ 
+ static int init_bounce_buffer(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	struct device *dev = &adapter->vdev->dev;
+ 	char *buf;
+ 	int buf_sz;
+ 	dma_addr_t map_addr;
+ 
+ 	buf_sz = (netdev->mtu + ETH_HLEN - 1) / PAGE_SIZE + 1;
+ 	buf = kmalloc(adapter->bounce_buffer_size, GFP_KERNEL);
+ 	if (!buf)
+ 		return -1;
+ 
+ 	map_addr = dma_map_single(dev, buf, buf_sz, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(dev, map_addr)) {
+ 		dev_err(dev, "Couldn't map bounce buffer\n");
+ 		kfree(buf);
+ 		return -1;
+ 	}
+ 
+ 	adapter->bounce_buffer = buf;
+ 	adapter->bounce_buffer_size = buf_sz;
+ 	adapter->bounce_buffer_dma = map_addr;
+ 	return 0;
+ }
+ 
+ static int ibmvnic_login(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	unsigned long timeout = msecs_to_jiffies(30000);
+ 	struct device *dev = &adapter->vdev->dev;
+ 
+ 	do {
+ 		if (adapter->renegotiate) {
+ 			adapter->renegotiate = false;
+ 			release_sub_crqs_no_irqs(adapter);
+ 
+ 			reinit_completion(&adapter->init_done);
+ 			send_cap_queries(adapter);
+ 			if (!wait_for_completion_timeout(&adapter->init_done,
+ 							 timeout)) {
+ 				dev_err(dev, "Capabilities query timeout\n");
+ 				return -1;
+ 			}
+ 		}
+ 
+ 		reinit_completion(&adapter->init_done);
+ 		send_login(adapter);
+ 		if (!wait_for_completion_timeout(&adapter->init_done,
+ 						 timeout)) {
+ 			dev_err(dev, "Login timeout\n");
+ 			return -1;
+ 		}
+ 	} while (adapter->renegotiate);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> c657e32cd055 (ibmvnic: Create init and release routines for the tx pool)
+ static int ibmvnic_open(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	struct device *dev = &adapter->vdev->dev;
  	union ibmvnic_crq crq;
  	int rxadd_subcrqs;
  	u64 *size_array;
@@@ -411,50 -585,15 +561,54 @@@
  			goto rx_pool_alloc_failed;
  		}
  	}
- 	adapter->tx_pool =
- 	    kcalloc(tx_subcrqs, sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);
  
++<<<<<<< HEAD
 +	if (!adapter->tx_pool)
 +		goto tx_pool_arr_alloc_failed;
 +	for (i = 0; i < tx_subcrqs; i++) {
 +		tx_pool = &adapter->tx_pool[i];
 +		tx_pool->tx_buff =
 +		    kcalloc(adapter->req_tx_entries_per_subcrq,
 +			    sizeof(struct ibmvnic_tx_buff), GFP_KERNEL);
 +		if (!tx_pool->tx_buff)
 +			goto tx_pool_alloc_failed;
 +
 +		if (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,
 +					 adapter->req_tx_entries_per_subcrq *
 +					 adapter->req_mtu))
 +			goto tx_ltb_alloc_failed;
 +
 +		tx_pool->free_map =
 +		    kcalloc(adapter->req_tx_entries_per_subcrq,
 +			    sizeof(int), GFP_KERNEL);
 +		if (!tx_pool->free_map)
 +			goto tx_fm_alloc_failed;
 +
 +		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
 +			tx_pool->free_map[j] = j;
 +
 +		tx_pool->consumer_index = 0;
 +		tx_pool->producer_index = 0;
 +	}
 +	adapter->bounce_buffer_size =
 +	    (netdev->mtu + ETH_HLEN - 1) / PAGE_SIZE + 1;
 +	adapter->bounce_buffer = kmalloc(adapter->bounce_buffer_size,
 +					 GFP_KERNEL);
 +	if (!adapter->bounce_buffer)
 +		goto bounce_alloc_failed;
++=======
+ 	rc = init_tx_pools(netdev);
+ 	if (rc)
+ 		goto tx_pool_failed;
++>>>>>>> c657e32cd055 (ibmvnic: Create init and release routines for the tx pool)
  
 -	rc = init_bounce_buffer(netdev);
 -	if (rc)
 -		goto bounce_init_failed;
 -
 +	adapter->bounce_buffer_dma = dma_map_single(dev, adapter->bounce_buffer,
 +						    adapter->bounce_buffer_size,
 +						    DMA_TO_DEVICE);
 +	if (dma_mapping_error(dev, adapter->bounce_buffer_dma)) {
 +		dev_err(dev, "Couldn't map tx bounce buffer\n");
 +		goto bounce_map_failed;
 +	}
  	replenish_pools(adapter);
  
  	/* We're ready to receive frames, enable the sub-crq interrupts and
@@@ -476,25 -615,11 +630,12 @@@
  
  	return 0;
  
 -bounce_init_failed:
 +bounce_map_failed:
 +	kfree(adapter->bounce_buffer);
 +bounce_alloc_failed:
  	i = tx_subcrqs - 1;
  	kfree(adapter->tx_pool[i].free_map);
- tx_fm_alloc_failed:
- 	free_long_term_buff(adapter, &adapter->tx_pool[i].long_term_buff);
- tx_ltb_alloc_failed:
- 	kfree(adapter->tx_pool[i].tx_buff);
- tx_pool_alloc_failed:
- 	for (j = 0; j < i; j++) {
- 		kfree(adapter->tx_pool[j].tx_buff);
- 		free_long_term_buff(adapter,
- 				    &adapter->tx_pool[j].long_term_buff);
- 		kfree(adapter->tx_pool[j].free_map);
- 	}
- 	kfree(adapter->tx_pool);
- 	adapter->tx_pool = NULL;
- tx_pool_arr_alloc_failed:
+ tx_pool_failed:
  	i = rxadd_subcrqs;
  rx_pool_alloc_failed:
  	for (j = 0; j < i; j++) {
@@@ -511,6 -636,35 +652,37 @@@ alloc_napi_failed
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
++=======
+ static void ibmvnic_release_resources(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 	int rx_scrqs;
+ 	int i;
+ 
+ 	release_bounce_buffer(adapter);
+ 	release_tx_pools(adapter);
+ 
+ 	rx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
+ 	for (i = 0; i < rx_scrqs; i++) {
+ 		struct ibmvnic_rx_pool *rx_pool = &adapter->rx_pool[i];
+ 
+ 		free_rx_pool(adapter, rx_pool);
+ 		free_long_term_buff(adapter, &rx_pool->long_term_buff);
+ 	}
+ 	kfree(adapter->rx_pool);
+ 	adapter->rx_pool = NULL;
+ 
+ 	release_sub_crqs(adapter);
+ 	release_crq_queue(adapter);
+ 
+ 	if (adapter->stats_token)
+ 		dma_unmap_single(dev, adapter->stats_token,
+ 				 sizeof(struct ibmvnic_statistics),
+ 				 DMA_FROM_DEVICE);
+ }
+ 
++>>>>>>> c657e32cd055 (ibmvnic: Create init and release routines for the tx pool)
  static int ibmvnic_close(struct net_device *netdev)
  {
  	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.c
