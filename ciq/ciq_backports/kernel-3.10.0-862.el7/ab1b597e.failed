mm, devm_memremap_pages: use multi-order radix for ZONE_DEVICE lookups

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] devm_memremap_pages: use multi-order radix for ZONE_DEVICE lookups (Jeff Moyer) [1489187]
Rebuild_FUZZ: 97.06%
commit-author Dan Williams <dan.j.williams@intel.com>
commit ab1b597ee0e4208a1db227bb7b2c9512c8234b48
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ab1b597e.failed

devm_memremap_pages() records mapped ranges in pgmap_radix with an entry
per section's worth of memory (128MB).  The key for each of those
entries is a section number.

This leads to false positives when devm_memremap_pages() is passed a
section-unaligned range as lookups in the misalignment fail to return
NULL.  We can close this hole by using the pfn as the key for entries in
the tree.  The number of entries required to describe a remapped range
is reduced by leveraging multi-order entries.

In practice this approach usually yields just one entry in the tree if
the size and starting address are of the same power-of-2 alignment.
Previously we always needed nr_entries = mapping_size / 128MB.

Link: https://lists.01.org/pipermail/linux-nvdimm/2016-August/006666.html
Link: http://lkml.kernel.org/r/150215410565.39310.13767886055248249438.stgit@dwillia2-desk3.amr.corp.intel.com
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Reported-by: Toshi Kani <toshi.kani@hpe.com>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ab1b597ee0e4208a1db227bb7b2c9512c8234b48)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/memremap.c
#	mm/Kconfig
diff --cc kernel/memremap.c
index 10769b16af58,066e73c2fcc9..000000000000
--- a/kernel/memremap.c
+++ b/kernel/memremap.c
@@@ -174,58 -194,30 +174,85 @@@ struct page_map 
  	struct vmem_altmap altmap;
  };
  
++<<<<<<< HEAD
 +void get_zone_device_page(struct page *page)
 +{
 +	percpu_ref_get(page->pgmap->ref);
 +}
 +EXPORT_SYMBOL(get_zone_device_page);
 +
 +void put_zone_device_page(struct page *page)
 +{
 +	/*
 +	 * ZONE_DEVICE page refcount should never reach 0 and never be freed
 +	 * to kernel memory allocator.
 +	 */
 +	int count = page_ref_dec_return(page);
 +
 +	/*
 +	 * If refcount is 1 then page is freed and refcount is stable as nobody
 +	 * holds a reference on the page.
 +	 */
 +	if (page->pgmap->page_free && count == 1)
 +		page->pgmap->page_free(page, page->pgmap->data);
 +
 +	put_dev_pagemap(page->pgmap);
 +}
 +EXPORT_SYMBOL(put_zone_device_page);
 +
 +#if IS_ENABLED(CONFIG_HMM)
 +int hmm_entry_fault(struct vm_area_struct *vma,
 +		       unsigned long addr,
 +		       swp_entry_t entry,
 +		       unsigned flags,
 +		       pmd_t *pmdp)
 +{
 +	struct page *page = hmm_entry_to_page(entry);
 +
 +	/*
 +	 * The page_fault() callback must migrate page back to system memory
 +	 * so that CPU can access it. This might fail for various reasons
 +	 * (device issue, device was unsafely unplugged, ...). When such
 +	 * error conditions happen, the callback must return VM_FAULT_SIGBUS.
 +	 *
 +	 * Note that because memory cgroup charges are accounted to the device
 +	 * memory, this should never fail because of memory restrictions (but
 +	 * allocation of regular system page might still fail because we are
 +	 * out of memory).
 +	 *
 +	 * There is a more in-depth description of what that callback can and
 +	 * cannot do, in include/linux/memremap.h
 +	 */
 +	return page->pgmap->page_fault(vma, addr, page, flags, pmdp);
 +}
 +EXPORT_SYMBOL(hmm_entry_fault);
 +#endif /* CONFIG_HMM */
++=======
+ static unsigned long order_at(struct resource *res, unsigned long pgoff)
+ {
+ 	unsigned long phys_pgoff = PHYS_PFN(res->start) + pgoff;
+ 	unsigned long nr_pages, mask;
+ 
+ 	nr_pages = PHYS_PFN(resource_size(res));
+ 	if (nr_pages == pgoff)
+ 		return ULONG_MAX;
+ 
+ 	/*
+ 	 * What is the largest aligned power-of-2 range available from
+ 	 * this resource pgoff to the end of the resource range,
+ 	 * considering the alignment of the current pgoff?
+ 	 */
+ 	mask = phys_pgoff | rounddown_pow_of_two(nr_pages - pgoff);
+ 	if (!mask)
+ 		return ULONG_MAX;
+ 
+ 	return find_first_bit(&mask, BITS_PER_LONG);
+ }
+ 
+ #define foreach_order_pgoff(res, order, pgoff) \
+ 	for (pgoff = 0, order = order_at((res), pgoff); order < ULONG_MAX; \
+ 			pgoff += 1UL << order, order = order_at((res), pgoff))
++>>>>>>> ab1b597ee0e4 (mm, devm_memremap_pages: use multi-order radix for ZONE_DEVICE lookups)
  
  static void pgmap_radix_release(struct resource *res)
  {
@@@ -317,7 -316,9 +342,13 @@@ struct dev_pagemap *find_dev_pagemap(re
  void *devm_memremap_pages(struct device *dev, struct resource *res,
  		struct percpu_ref *ref, struct vmem_altmap *altmap)
  {
++<<<<<<< HEAD
 +	resource_size_t key, align_start, align_size, align_end;
++=======
+ 	resource_size_t align_start, align_size, align_end;
+ 	unsigned long pfn, pgoff, order;
+ 	pgprot_t pgprot = PAGE_KERNEL;
++>>>>>>> ab1b597ee0e4 (mm, devm_memremap_pages: use multi-order radix for ZONE_DEVICE lookups)
  	struct dev_pagemap *pgmap;
  	struct page_map *page_map;
  	int error, nid, is_ram;
diff --cc mm/Kconfig
index 8a6f054d0ebb,0ded10a22639..000000000000
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@@ -634,7 -677,8 +634,12 @@@ config ZONE_DEVIC
  	depends on MEMORY_HOTPLUG
  	depends on MEMORY_HOTREMOVE
  	depends on SPARSEMEM_VMEMMAP
++<<<<<<< HEAD
 +	depends on X86_64 #arch_add_memory() comprehends device memory
++=======
+ 	depends on ARCH_HAS_ZONE_DEVICE
+ 	select RADIX_TREE_MULTIORDER
++>>>>>>> ab1b597ee0e4 (mm, devm_memremap_pages: use multi-order radix for ZONE_DEVICE lookups)
  
  	help
  	  Device memory hotplug support allows for establishing pmem,
* Unmerged path kernel/memremap.c
* Unmerged path mm/Kconfig
