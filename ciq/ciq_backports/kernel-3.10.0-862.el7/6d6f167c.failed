blk-mq: put the driver tag of nxt rq before first one is requeued

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jianchao Wang <jianchao.w.wang@oracle.com>
commit 6d6f167ce74158903e7fc20dfbecf89c71aa1c00
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6d6f167c.failed

When freeing the driver tag of the next rq with an I/O scheduler
configured, we get the first entry of the list. However, this can
race with requeue of a request, and we end up getting the wrong request
from the head of the list. Free the driver tag of next rq before the
failed one is requeued in the failure branch of queue_rq callback.

	Signed-off-by: Jianchao Wang <jianchao.w.wang@oracle.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 6d6f167ce74158903e7fc20dfbecf89c71aa1c00)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index d9bfe0c6bc0e,6eacc1dea8b7..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -825,19 -964,143 +825,151 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
 -bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 -			   bool wait)
 +bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list)
  {
++<<<<<<< HEAD
 +	struct request_queue *q = hctx->queue;
 +	struct request *rq;
 +	LIST_HEAD(driver_list);
 +	struct list_head *dptr;
 +	int errors, queued, ret = BLK_MQ_RQ_QUEUE_OK;
++=======
+ 	struct blk_mq_alloc_data data = {
+ 		.q = rq->q,
+ 		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
+ 		.flags = wait ? 0 : BLK_MQ_REQ_NOWAIT,
+ 	};
+ 
+ 	might_sleep_if(wait);
+ 
+ 	if (rq->tag != -1)
+ 		goto done;
+ 
+ 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ 		data.flags |= BLK_MQ_REQ_RESERVED;
+ 
+ 	rq->tag = blk_mq_get_tag(&data);
+ 	if (rq->tag >= 0) {
+ 		if (blk_mq_tag_busy(data.hctx)) {
+ 			rq->rq_flags |= RQF_MQ_INFLIGHT;
+ 			atomic_inc(&data.hctx->nr_active);
+ 		}
+ 		data.hctx->tags->rqs[rq->tag] = rq;
+ 	}
+ 
+ done:
+ 	if (hctx)
+ 		*hctx = data.hctx;
+ 	return rq->tag != -1;
+ }
+ 
+ static void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
+ 				    struct request *rq)
+ {
+ 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ 	rq->tag = -1;
+ 
+ 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+ 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+ 		atomic_dec(&hctx->nr_active);
+ 	}
+ }
+ 
+ static void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
+ 				       struct request *rq)
+ {
+ 	if (rq->tag == -1 || rq->internal_tag == -1)
+ 		return;
+ 
+ 	__blk_mq_put_driver_tag(hctx, rq);
+ }
+ 
+ static void blk_mq_put_driver_tag(struct request *rq)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 
+ 	if (rq->tag == -1 || rq->internal_tag == -1)
+ 		return;
+ 
+ 	hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu);
+ 	__blk_mq_put_driver_tag(hctx, rq);
+ }
+ 
+ /*
+  * If we fail getting a driver tag because all the driver tags are already
+  * assigned and on the dispatch list, BUT the first entry does not have a
+  * tag, then we could deadlock. For that case, move entries with assigned
+  * driver tags to the front, leaving the set of tagged requests in the
+  * same order, and the untagged set in the same order.
+  */
+ static bool reorder_tags_to_front(struct list_head *list)
+ {
+ 	struct request *rq, *tmp, *first = NULL;
+ 
+ 	list_for_each_entry_safe_reverse(rq, tmp, list, queuelist) {
+ 		if (rq == first)
+ 			break;
+ 		if (rq->tag != -1) {
+ 			list_move(&rq->queuelist, list);
+ 			if (!first)
+ 				first = rq;
+ 		}
+ 	}
+ 
+ 	return first != NULL;
+ }
+ 
+ static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode, int flags,
+ 				void *key)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 
+ 	hctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);
+ 
+ 	list_del(&wait->entry);
+ 	clear_bit_unlock(BLK_MQ_S_TAG_WAITING, &hctx->state);
+ 	blk_mq_run_hw_queue(hctx, true);
+ 	return 1;
+ }
+ 
+ static bool blk_mq_dispatch_wait_add(struct blk_mq_hw_ctx *hctx)
+ {
+ 	struct sbq_wait_state *ws;
+ 
+ 	/*
+ 	 * The TAG_WAITING bit serves as a lock protecting hctx->dispatch_wait.
+ 	 * The thread which wins the race to grab this bit adds the hardware
+ 	 * queue to the wait queue.
+ 	 */
+ 	if (test_bit(BLK_MQ_S_TAG_WAITING, &hctx->state) ||
+ 	    test_and_set_bit_lock(BLK_MQ_S_TAG_WAITING, &hctx->state))
+ 		return false;
+ 
+ 	init_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);
+ 	ws = bt_wait_ptr(&hctx->tags->bitmap_tags, hctx);
+ 
+ 	/*
+ 	 * As soon as this returns, it's no longer safe to fiddle with
+ 	 * hctx->dispatch_wait, since a completion can wake up the wait queue
+ 	 * and unlock the bit.
+ 	 */
+ 	add_wait_queue(&ws->wait, &hctx->dispatch_wait);
+ 	return true;
+ }
+ 
+ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
+ 		bool got_budget)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	struct request *rq, *nxt;
+ 	int errors, queued;
++>>>>>>> 6d6f167ce741 (blk-mq: put the driver tag of nxt rq before first one is requeued)
  
 -	if (list_empty(list))
 -		return false;
 -
 -	WARN_ON(!list_is_singular(list) && got_budget);
 +	/*
 +	 * Start off with dptr being NULL, so we start the first request
 +	 * immediately, even if we have more pending.
 +	 */
 +	dptr = NULL;
  
  	/*
  	 * Now process all the entries, sending them to the driver.
@@@ -850,15 -1114,58 +982,41 @@@
  		list_del_init(&rq->queuelist);
  
  		bd.rq = rq;
++<<<<<<< HEAD
 +		bd.list = dptr;
 +		bd.last = list_empty(list);
 +
 +		ret = q->mq_ops->queue_rq(hctx, &bd);
 +		switch (ret) {
 +		case BLK_MQ_RQ_QUEUE_OK:
 +			queued++;
 +			break;
 +		case BLK_MQ_RQ_QUEUE_BUSY:
++=======
+ 
+ 		/*
+ 		 * Flag last if we have no more requests, or if we have more
+ 		 * but can't assign a driver tag to it.
+ 		 */
+ 		if (list_empty(list))
+ 			bd.last = true;
+ 		else {
+ 			nxt = list_first_entry(list, struct request, queuelist);
+ 			bd.last = !blk_mq_get_driver_tag(nxt, NULL, false);
+ 		}
+ 
+ 		ret = q->mq_ops->queue_rq(hctx, &bd);
+ 		if (ret == BLK_STS_RESOURCE) {
+ 			/*
+ 			 * If an I/O scheduler has been configured and we got a
+ 			 * driver tag for the next request already, free it again.
+ 			 */
+ 			if (!list_empty(list)) {
+ 				nxt = list_first_entry(list, struct request, queuelist);
+ 				blk_mq_put_driver_tag(nxt);
+ 			}
+ 			blk_mq_put_driver_tag_hctx(hctx, rq);
++>>>>>>> 6d6f167ce741 (blk-mq: put the driver tag of nxt rq before first one is requeued)
  			list_add(&rq->queuelist, list);
  			__blk_mq_requeue_request(rq);
  			break;
* Unmerged path block/blk-mq.c
