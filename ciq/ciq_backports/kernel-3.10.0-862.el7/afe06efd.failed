sched: Extend scheduler's asym packing

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Tim Chen <tim.c.chen@linux.intel.com>
commit afe06efdf07c12fd9370d5cce5383398cedf6c90
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/afe06efd.failed

We generalize the scheduler's asym packing to provide an ordering
of the cpu beyond just the cpu number.  This allows the use of the
ASYM_PACKING scheduler machinery to move loads to preferred CPU in a
sched domain. The preference is defined with the cpu priority
given by arch_asym_cpu_priority(cpu).

We also record the most preferred cpu in a sched group when
we build the cpu's capacity for fast lookup of preferred cpu
during load balancing.

Co-developed-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: linux-pm@vger.kernel.org
	Cc: jolsa@redhat.com
	Cc: rjw@rjwysocki.net
	Cc: linux-acpi@vger.kernel.org
	Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
	Cc: bp@suse.de
Link: http://lkml.kernel.org/r/0e73ae12737dfaafa46c07066cc7c5d3f1675e46.1479844244.git.tim.c.chen@linux.intel.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit afe06efdf07c12fd9370d5cce5383398cedf6c90)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
#	kernel/sched/sched.h
diff --cc kernel/sched/fair.c
index 38afc41c3538,18d9e75f1f6e..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -86,19 -89,24 +86,29 @@@ unsigned int sysctl_sched_child_runs_fi
   * This option delays the preemption effects of decoupled workloads
   * and reduces their over-scheduling. Synchronous workloads will still
   * have immediate wakeup/sleep latencies.
 - *
 - * (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds)
   */
 -unsigned int sysctl_sched_wakeup_granularity		= 1000000UL;
 -unsigned int normalized_sysctl_sched_wakeup_granularity	= 1000000UL;
 +unsigned int sysctl_sched_wakeup_granularity = 1000000UL;
 +unsigned int normalized_sysctl_sched_wakeup_granularity = 1000000UL;
 +
 +const_debug unsigned int sysctl_sched_migration_cost = 500000UL;
  
 -const_debug unsigned int sysctl_sched_migration_cost	= 500000UL;
 +/*
 + * The exponential sliding  window over which load is averaged for shares
 + * distribution.
 + * (default: 10msec)
 + */
 +unsigned int __read_mostly sysctl_sched_shares_window = 10000000UL;
  
+ #ifdef CONFIG_SMP
+ /*
+  * For asym packing, by default the lower numbered cpu has higher priority.
+  */
+ int __weak arch_asym_cpu_priority(int cpu)
+ {
+ 	return -cpu;
+ }
+ #endif
+ 
  #ifdef CONFIG_CFS_BANDWIDTH
  /*
   * Amount of runtime to allocate from global (tg) to local (per-cfs_rq) pool
@@@ -5888,16 -7394,22 +5898,23 @@@ static bool update_sd_pick_busiest(stru
  	if (!(env->sd->flags & SD_ASYM_PACKING))
  		return true;
  
 -	/* No ASYM_PACKING if target cpu is already busy */
 -	if (env->idle == CPU_NOT_IDLE)
 -		return true;
  	/*
- 	 * ASYM_PACKING needs to move all the work to the lowest
- 	 * numbered CPUs in the group, therefore mark all groups
- 	 * higher than ourself as busy.
+ 	 * ASYM_PACKING needs to move all the work to the highest
+ 	 * prority CPUs in the group, therefore mark all groups
+ 	 * of lower priority than ourself as busy.
  	 */
- 	if (sgs->sum_nr_running && env->dst_cpu < group_first_cpu(sg)) {
+ 	if (sgs->sum_nr_running &&
+ 	    sched_asym_prefer(env->dst_cpu, sg->asym_prefer_cpu)) {
  		if (!sds->busiest)
  			return true;
  
++<<<<<<< HEAD
 +		if (group_first_cpu(sds->busiest) > group_first_cpu(sg))
++=======
+ 		/* Prefer to move from lowest priority cpu's work */
+ 		if (sched_asym_prefer(sds->busiest->asym_prefer_cpu,
+ 				      sg->asym_prefer_cpu))
++>>>>>>> afe06efdf07c (sched: Extend scheduler's asym packing)
  			return true;
  	}
  
@@@ -7170,15 -8748,16 +7188,20 @@@ end
   *   - For SD_ASYM_PACKING, if the lower numbered cpu's in the scheduler
   *     domain span are idle.
   */
 -static inline bool nohz_kick_needed(struct rq *rq)
 +static inline int nohz_kick_needed(struct rq *rq, int cpu)
  {
  	unsigned long now = jiffies;
 -	struct sched_domain_shared *sds;
  	struct sched_domain *sd;
++<<<<<<< HEAD
 +	struct sched_group_power *sgp;
 +	int nr_busy;
++=======
+ 	int nr_busy, i, cpu = rq->cpu;
+ 	bool kick = false;
++>>>>>>> afe06efdf07c (sched: Extend scheduler's asym packing)
  
 -	if (unlikely(rq->idle_balance))
 -		return false;
 +	if (unlikely(idle_cpu(cpu)))
 +		return 0;
  
         /*
  	* We may be recently in ticked or tickless idle mode. At the first
@@@ -7212,18 -8804,21 +7235,34 @@@
  	}
  
  	sd = rcu_dereference(per_cpu(sd_asym, cpu));
++<<<<<<< HEAD
 +
 +	if (sd && (cpumask_first_and(nohz.idle_cpus_mask,
 +				  sched_domain_span(sd)) < cpu))
 +		goto need_kick_unlock;
 +
++=======
+ 	if (sd) {
+ 		for_each_cpu(i, sched_domain_span(sd)) {
+ 			if (i == cpu ||
+ 			    !cpumask_test_cpu(i, nohz.idle_cpus_mask))
+ 				continue;
+ 
+ 			if (sched_asym_prefer(i, cpu)) {
+ 				kick = true;
+ 				goto unlock;
+ 			}
+ 		}
+ 	}
+ unlock:
++>>>>>>> afe06efdf07c (sched: Extend scheduler's asym packing)
 +	rcu_read_unlock();
 +	return 0;
 +
 +need_kick_unlock:
  	rcu_read_unlock();
 -	return kick;
 +need_kick:
 +	return 1;
  }
  #else
  static void nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle) { }
diff --cc kernel/sched/sched.h
index 3e0ea93b25ef,7b34c7826ca5..000000000000
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@@ -803,7 -912,8 +808,12 @@@ struct sched_group 
  	atomic_t ref;
  
  	unsigned int group_weight;
++<<<<<<< HEAD
 +	struct sched_group_power *sgp;
++=======
+ 	struct sched_group_capacity *sgc;
+ 	int asym_prefer_cpu;		/* cpu of highest priority in group */
++>>>>>>> afe06efdf07c (sched: Extend scheduler's asym packing)
  
  	/*
  	 * The CPUs this group covers.
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 17d2f0bc1bf5..5784341ee09b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -962,6 +962,8 @@ static inline int cpu_numa_flags(void)
 }
 #endif
 
+extern int arch_asym_cpu_priority(int cpu);
+
 struct sched_domain_attr {
 	int relax_domain_level;
 };
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 108629661cfb..142dde4c9d69 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -7095,7 +7095,22 @@ static void init_sched_groups_power(int cpu, struct sched_domain *sd)
 	WARN_ON(!sd || !sg);
 
 	do {
+		int cpu, max_cpu = -1;
+
 		sg->group_weight = cpumask_weight(sched_group_cpus(sg));
+
+		if (!(sd->flags & SD_ASYM_PACKING))
+			goto next;
+
+		for_each_cpu(cpu, sched_group_cpus(sg)) {
+			if (max_cpu < 0)
+				max_cpu = cpu;
+			else if (sched_asym_prefer(cpu, max_cpu))
+				max_cpu = cpu;
+		}
+		sg->asym_prefer_cpu = max_cpu;
+
+next:
 		sg = sg->next;
 	} while (sg != sd->groups);
 
* Unmerged path kernel/sched/fair.c
* Unmerged path kernel/sched/sched.h
