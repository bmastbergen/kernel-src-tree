blk-mq: fix issue with shared tag queue re-running

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@kernel.dk>
commit eb619fdb2d4cb8b3d3419e9113921e87e7daf557
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/eb619fdb.failed

This patch attempts to make the case of hctx re-running on driver tag
failure more robust. Without this patch, it's pretty easy to trigger a
stall condition with shared tags. An example is using null_blk like
this:

modprobe null_blk queue_mode=2 nr_devices=4 shared_tags=1 submit_queues=1 hw_queue_depth=1

which sets up 4 devices, sharing the same tag set with a depth of 1.
Running a fio job ala:

[global]
bs=4k
rw=randread
norandommap
direct=1
ioengine=libaio
iodepth=4

[nullb0]
filename=/dev/nullb0
[nullb1]
filename=/dev/nullb1
[nullb2]
filename=/dev/nullb2
[nullb3]
filename=/dev/nullb3

will inevitably end with one or more threads being stuck waiting for a
scheduler tag. That IO is then stuck forever, until someone else
triggers a run of the queue.

Ensure that we always re-run the hardware queue, if the driver tag we
were waiting for got freed before we added our leftover request entries
back on the dispatch list.

	Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
	Tested-by: Bart Van Assche <bart.vanassche@wdc.com>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
	Reviewed-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit eb619fdb2d4cb8b3d3419e9113921e87e7daf557)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-debugfs.c
#	block/blk-mq.c
#	include/linux/blk-mq.h
diff --cc block/blk-mq.c
index d9bfe0c6bc0e,fed8165973a3..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -825,28 -966,133 +825,148 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
 -bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 -			   bool wait)
 +bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list)
  {
++<<<<<<< HEAD
 +	struct request_queue *q = hctx->queue;
 +	struct request *rq;
 +	LIST_HEAD(driver_list);
 +	struct list_head *dptr;
 +	int errors, queued, ret = BLK_MQ_RQ_QUEUE_OK;
 +
 +	/*
 +	 * Start off with dptr being NULL, so we start the first request
 +	 * immediately, even if we have more pending.
 +	 */
 +	dptr = NULL;
++=======
+ 	struct blk_mq_alloc_data data = {
+ 		.q = rq->q,
+ 		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
+ 		.flags = wait ? 0 : BLK_MQ_REQ_NOWAIT,
+ 	};
+ 
+ 	might_sleep_if(wait);
+ 
+ 	if (rq->tag != -1)
+ 		goto done;
+ 
+ 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ 		data.flags |= BLK_MQ_REQ_RESERVED;
+ 
+ 	rq->tag = blk_mq_get_tag(&data);
+ 	if (rq->tag >= 0) {
+ 		if (blk_mq_tag_busy(data.hctx)) {
+ 			rq->rq_flags |= RQF_MQ_INFLIGHT;
+ 			atomic_inc(&data.hctx->nr_active);
+ 		}
+ 		data.hctx->tags->rqs[rq->tag] = rq;
+ 	}
+ 
+ done:
+ 	if (hctx)
+ 		*hctx = data.hctx;
+ 	return rq->tag != -1;
+ }
+ 
+ static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
+ 				int flags, void *key)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 
+ 	hctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);
+ 
+ 	list_del_init(&wait->entry);
+ 	blk_mq_run_hw_queue(hctx, true);
+ 	return 1;
+ }
+ 
+ static bool blk_mq_dispatch_wait_add(struct blk_mq_hw_ctx **hctx,
+ 				     struct request *rq)
+ {
+ 	struct blk_mq_hw_ctx *this_hctx = *hctx;
+ 	wait_queue_entry_t *wait = &this_hctx->dispatch_wait;
+ 	struct sbq_wait_state *ws;
+ 
+ 	if (!list_empty_careful(&wait->entry))
+ 		return false;
+ 
+ 	spin_lock(&this_hctx->lock);
+ 	if (!list_empty(&wait->entry)) {
+ 		spin_unlock(&this_hctx->lock);
+ 		return false;
+ 	}
+ 
+ 	ws = bt_wait_ptr(&this_hctx->tags->bitmap_tags, this_hctx);
+ 	add_wait_queue(&ws->wait, wait);
+ 
+ 	/*
+ 	 * It's possible that a tag was freed in the window between the
+ 	 * allocation failure and adding the hardware queue to the wait
+ 	 * queue.
+ 	 */
+ 	if (!blk_mq_get_driver_tag(rq, hctx, false)) {
+ 		spin_unlock(&this_hctx->lock);
+ 		return false;
+ 	}
+ 
+ 	/*
+ 	 * We got a tag, remove ourselves from the wait queue to ensure
+ 	 * someone else gets the wakeup.
+ 	 */
+ 	spin_lock_irq(&ws->wait.lock);
+ 	list_del_init(&wait->entry);
+ 	spin_unlock_irq(&ws->wait.lock);
+ 	spin_unlock(&this_hctx->lock);
+ 	return true;
+ }
+ 
+ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
+ 			     bool got_budget)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 	struct request *rq, *nxt;
+ 	bool no_tag = false;
+ 	int errors, queued;
+ 
+ 	if (list_empty(list))
+ 		return false;
+ 
+ 	WARN_ON(!list_is_singular(list) && got_budget);
++>>>>>>> eb619fdb2d4c (blk-mq: fix issue with shared tag queue re-running)
  
  	/*
  	 * Now process all the entries, sending them to the driver.
  	 */
  	errors = queued = 0;
 -	do {
 +	while (!list_empty(list)) {
  		struct blk_mq_queue_data bd;
 -		blk_status_t ret;
  
  		rq = list_first_entry(list, struct request, queuelist);
++<<<<<<< HEAD
++=======
+ 		if (!blk_mq_get_driver_tag(rq, &hctx, false)) {
+ 			/*
+ 			 * The initial allocation attempt failed, so we need to
+ 			 * rerun the hardware queue when a tag is freed. The
+ 			 * waitqueue takes care of that. If the queue is run
+ 			 * before we add this entry back on the dispatch list,
+ 			 * we'll re-run it below.
+ 			 */
+ 			if (!blk_mq_dispatch_wait_add(&hctx, rq)) {
+ 				if (got_budget)
+ 					blk_mq_put_dispatch_budget(hctx);
+ 				no_tag = true;
+ 				break;
+ 			}
+ 		}
+ 
+ 		if (!got_budget && !blk_mq_get_dispatch_budget(hctx)) {
+ 			blk_mq_put_driver_tag(rq);
+ 			break;
+ 		}
+ 
++>>>>>>> eb619fdb2d4c (blk-mq: fix issue with shared tag queue re-running)
  		list_del_init(&rq->queuelist);
  
  		bd.rq = rq;
@@@ -894,15 -1144,28 +1014,36 @@@
  		spin_unlock(&hctx->lock);
  
  		/*
 -		 * If SCHED_RESTART was set by the caller of this function and
 -		 * it is no longer set that means that it was cleared by another
 -		 * thread and hence that a queue rerun is needed.
 +		 * the queue is expected stopped with BLK_MQ_RQ_QUEUE_BUSY, but
 +		 * it's possible the queue is stopped and restarted again
 +		 * before this. Queue restart will dispatch requests. And since
 +		 * requests in rq_list aren't added into hctx->dispatch yet,
 +		 * the requests in rq_list might get lost.
  		 *
++<<<<<<< HEAD
 +		 * blk_mq_run_hw_queue() already checks the STOPPED bit
 +		 **/
 +		blk_mq_run_hw_queue(hctx, true);
++=======
+ 		 * If 'no_tag' is set, that means that we failed getting
+ 		 * a driver tag with an I/O scheduler attached. If our dispatch
+ 		 * waitqueue is no longer active, ensure that we run the queue
+ 		 * AFTER adding our entries back to the list.
+ 		 *
+ 		 * If no I/O scheduler has been configured it is possible that
+ 		 * the hardware queue got stopped and restarted before requests
+ 		 * were pushed back onto the dispatch list. Rerun the queue to
+ 		 * avoid starvation. Notes:
+ 		 * - blk_mq_run_hw_queue() checks whether or not a queue has
+ 		 *   been stopped before rerunning a queue.
+ 		 * - Some but not all block drivers stop a queue before
+ 		 *   returning BLK_STS_RESOURCE. Two exceptions are scsi-mq
+ 		 *   and dm-rq.
+ 		 */
+ 		if (!blk_mq_sched_needs_restart(hctx) ||
+ 		    (no_tag && list_empty_careful(&hctx->dispatch_wait.entry)))
+ 			blk_mq_run_hw_queue(hctx, true);
++>>>>>>> eb619fdb2d4c (blk-mq: fix issue with shared tag queue re-running)
  	}
  
  	return (queued + errors) != 0;
diff --cc include/linux/blk-mq.h
index ab31251b7413,4ae987c2352c..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -38,22 -28,18 +38,27 @@@ struct blk_mq_hw_ctx 
  
  	void			*driver_data;
  
 -	struct sbitmap		ctx_map;
 +	unsigned int		nr_ctx;
 +	struct blk_mq_ctx	**ctxs;
  
 -	struct blk_mq_ctx	*dispatch_from;
++<<<<<<< HEAD
 +	RH_KABI_REPLACE(unsigned int		nr_ctx_map,
 +			atomic_t		wait_index)
  
 -	struct blk_mq_ctx	**ctxs;
 -	unsigned int		nr_ctx;
 +	RH_KABI_REPLACE(unsigned long		*ctx_map,
 +			unsigned long		*padding1)
 +
 +	RH_KABI_REPLACE(struct request		**rqs,
 +			struct request		**padding2)
  
 +	RH_KABI_REPLACE(struct list_head	page_list,
 +			struct list_head	padding3)
++=======
+ 	wait_queue_entry_t	dispatch_wait;
+ 	atomic_t		wait_index;
++>>>>>>> eb619fdb2d4c (blk-mq: fix issue with shared tag queue re-running)
  
  	struct blk_mq_tags	*tags;
 -	struct blk_mq_tags	*sched_tags;
  
  	unsigned long		queued;
  	unsigned long		run;
@@@ -208,6 -180,8 +213,11 @@@ enum 
  
  	BLK_MQ_S_STOPPED	= 0,
  	BLK_MQ_S_TAG_ACTIVE	= 1,
++<<<<<<< HEAD
++=======
+ 	BLK_MQ_S_SCHED_RESTART	= 2,
+ 	BLK_MQ_S_START_ON_RUN	= 3,
++>>>>>>> eb619fdb2d4c (blk-mq: fix issue with shared tag queue re-running)
  
  	BLK_MQ_MAX_DEPTH	= 10240,
  
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blk-mq.h
