scsi: lpfc: Adding additional stats counters for nvme.

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] lpfc: Adding additional stats counters for nvme (Dick Kennedy) [1385844 1461977 1387768]
Rebuild_FUZZ: 93.07%
commit-author James Smart <jsmart2021@gmail.com>
commit 547077a44b3b49f56c0f05c0b46c8c617dea591d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/547077a4.failed

More debug messages added for nvme statistics.

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 547077a44b3b49f56c0f05c0b46c8c617dea591d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_attr.c
#	drivers/scsi/lpfc/lpfc_debugfs.c
#	drivers/scsi/lpfc/lpfc_nvmet.c
#	drivers/scsi/lpfc/lpfc_nvmet.h
#	drivers/scsi/lpfc/lpfc_sli.c
diff --cc drivers/scsi/lpfc/lpfc_attr.c
index b0e0bd1cf345,41ec7451689b..000000000000
--- a/drivers/scsi/lpfc/lpfc_attr.c
+++ b/drivers/scsi/lpfc/lpfc_attr.c
@@@ -130,6 -140,215 +130,218 @@@ lpfc_enable_fip_show(struct device *dev
  }
  
  static ssize_t
++<<<<<<< HEAD
++=======
+ lpfc_nvme_info_show(struct device *dev, struct device_attribute *attr,
+ 		    char *buf)
+ {
+ 	struct Scsi_Host *shost = class_to_shost(dev);
+ 	struct lpfc_vport *vport = shost_priv(shost);
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	struct nvme_fc_local_port *localport;
+ 	struct lpfc_nvme_lport *lport;
+ 	struct lpfc_nvme_rport *rport;
+ 	struct nvme_fc_remote_port *nrport;
+ 	char *statep;
+ 	int len = 0;
+ 
+ 	if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME)) {
+ 		len += snprintf(buf, PAGE_SIZE, "NVME Disabled\n");
+ 		return len;
+ 	}
+ 	if (phba->nvmet_support) {
+ 		if (!phba->targetport) {
+ 			len = snprintf(buf, PAGE_SIZE,
+ 					"NVME Target: x%llx is not allocated\n",
+ 					wwn_to_u64(vport->fc_portname.u.wwn));
+ 			return len;
+ 		}
+ 		/* Port state is only one of two values for now. */
+ 		if (phba->targetport->port_id)
+ 			statep = "REGISTERED";
+ 		else
+ 			statep = "INIT";
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"NVME Target: Enabled  State %s\n",
+ 				statep);
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"%s%d WWPN x%llx WWNN x%llx DID x%06x\n",
+ 				"NVME Target: lpfc",
+ 				phba->brd_no,
+ 				wwn_to_u64(vport->fc_portname.u.wwn),
+ 				wwn_to_u64(vport->fc_nodename.u.wwn),
+ 				phba->targetport->port_id);
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"\nNVME Target: Statistics\n");
+ 		tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"LS: Rcv %08x Drop %08x Abort %08x\n",
+ 				atomic_read(&tgtp->rcv_ls_req_in),
+ 				atomic_read(&tgtp->rcv_ls_req_drop),
+ 				atomic_read(&tgtp->xmt_ls_abort));
+ 		if (atomic_read(&tgtp->rcv_ls_req_in) !=
+ 		    atomic_read(&tgtp->rcv_ls_req_out)) {
+ 			len += snprintf(buf+len, PAGE_SIZE-len,
+ 					"Rcv LS: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_ls_req_in),
+ 					atomic_read(&tgtp->rcv_ls_req_out));
+ 		}
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"LS: Xmt %08x Drop %08x Cmpl %08x Err %08x\n",
+ 				atomic_read(&tgtp->xmt_ls_rsp),
+ 				atomic_read(&tgtp->xmt_ls_drop),
+ 				atomic_read(&tgtp->xmt_ls_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_ls_rsp_error));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP: Rcv %08x Release %08x Drop %08x\n",
+ 				atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 				atomic_read(&tgtp->xmt_fcp_release),
+ 				atomic_read(&tgtp->rcv_fcp_cmd_drop));
+ 
+ 		if (atomic_read(&tgtp->rcv_fcp_cmd_in) !=
+ 		    atomic_read(&tgtp->rcv_fcp_cmd_out)) {
+ 			len += snprintf(buf+len, PAGE_SIZE-len,
+ 					"Rcv FCP: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 					atomic_read(&tgtp->rcv_fcp_cmd_out));
+ 		}
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP Rsp: RD %08x rsp %08x WR %08x rsp %08x "
+ 				"drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_read),
+ 				atomic_read(&tgtp->xmt_fcp_read_rsp),
+ 				atomic_read(&tgtp->xmt_fcp_write),
+ 				atomic_read(&tgtp->xmt_fcp_rsp),
+ 				atomic_read(&tgtp->xmt_fcp_drop));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"FCP Rsp Cmpl: %08x err %08x drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_error),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_drop));
+ 
+ 		len += snprintf(buf+len, PAGE_SIZE-len,
+ 				"ABORT: Xmt %08x Cmpl %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_abort),
+ 				atomic_read(&tgtp->xmt_fcp_abort_cmpl));
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"ABORT: Sol %08x  Usol %08x Err %08x Cmpl %08x",
+ 				atomic_read(&tgtp->xmt_abort_sol),
+ 				atomic_read(&tgtp->xmt_abort_unsol),
+ 				atomic_read(&tgtp->xmt_abort_rsp),
+ 				atomic_read(&tgtp->xmt_abort_rsp_error));
+ 
+ 		len +=  snprintf(buf+len, PAGE_SIZE-len, "\n");
+ 		return len;
+ 	}
+ 
+ 	localport = vport->localport;
+ 	if (!localport) {
+ 		len = snprintf(buf, PAGE_SIZE,
+ 				"NVME Initiator x%llx is not allocated\n",
+ 				wwn_to_u64(vport->fc_portname.u.wwn));
+ 		return len;
+ 	}
+ 	len = snprintf(buf, PAGE_SIZE, "NVME Initiator Enabled\n");
+ 
+ 	spin_lock_irq(shost->host_lock);
+ 	lport = (struct lpfc_nvme_lport *)localport->private;
+ 
+ 	/* Port state is only one of two values for now. */
+ 	if (localport->port_id)
+ 		statep = "ONLINE";
+ 	else
+ 		statep = "UNKNOWN ";
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"%s%d WWPN x%llx WWNN x%llx DID x%06x %s\n",
+ 			"NVME LPORT lpfc",
+ 			phba->brd_no,
+ 			wwn_to_u64(vport->fc_portname.u.wwn),
+ 			wwn_to_u64(vport->fc_nodename.u.wwn),
+ 			localport->port_id, statep);
+ 
+ 	list_for_each_entry(rport, &lport->rport_list, list) {
+ 		/* local short-hand pointer. */
+ 		nrport = rport->remoteport;
+ 
+ 		/* Port state is only one of two values for now. */
+ 		switch (nrport->port_state) {
+ 		case FC_OBJSTATE_ONLINE:
+ 			statep = "ONLINE";
+ 			break;
+ 		case FC_OBJSTATE_UNKNOWN:
+ 			statep = "UNKNOWN ";
+ 			break;
+ 		default:
+ 			statep = "UNSUPPORTED";
+ 			break;
+ 		}
+ 
+ 		/* Tab in to show lport ownership. */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"NVME RPORT       ");
+ 		if (phba->brd_no >= 10)
+ 			len += snprintf(buf + len, PAGE_SIZE - len, " ");
+ 
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "WWPN x%llx ",
+ 				nrport->port_name);
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "WWNN x%llx ",
+ 				nrport->node_name);
+ 		len += snprintf(buf + len, PAGE_SIZE - len, "DID x%06x ",
+ 				nrport->port_id);
+ 
+ 		switch (nrport->port_role) {
+ 		case FC_PORT_ROLE_NVME_INITIATOR:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "INITIATOR ");
+ 			break;
+ 		case FC_PORT_ROLE_NVME_TARGET:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "TARGET ");
+ 			break;
+ 		case FC_PORT_ROLE_NVME_DISCOVERY:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "DISCOVERY ");
+ 			break;
+ 		default:
+ 			len +=  snprintf(buf + len, PAGE_SIZE - len,
+ 					 "UNKNOWN_ROLE x%x",
+ 					 nrport->port_role);
+ 			break;
+ 		}
+ 		len +=  snprintf(buf + len, PAGE_SIZE - len, "%s  ", statep);
+ 		/* Terminate the string. */
+ 		len +=  snprintf(buf + len, PAGE_SIZE - len, "\n");
+ 	}
+ 	spin_unlock_irq(shost->host_lock);
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE - len, "\nNVME Statistics\n");
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"LS: Xmt %016llx Cmpl %016llx\n",
+ 			phba->fc4NvmeLsRequests,
+ 			phba->fc4NvmeLsCmpls);
+ 
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"FCP: Rd %016llx Wr %016llx IO %016llx\n",
+ 			phba->fc4NvmeInputRequests,
+ 			phba->fc4NvmeOutputRequests,
+ 			phba->fc4NvmeControlRequests);
+ 
+ 	len += snprintf(buf+len, PAGE_SIZE-len,
+ 			"    Cmpl %016llx\n", phba->fc4NvmeIoCmpls);
+ 
+ 	return len;
+ }
+ 
+ static ssize_t
++>>>>>>> 547077a44b3b (scsi: lpfc: Adding additional stats counters for nvme.)
  lpfc_bg_info_show(struct device *dev, struct device_attribute *attr,
  		  char *buf)
  {
diff --cc drivers/scsi/lpfc/lpfc_debugfs.c
index e6cf568b0f02,a41daedeb967..000000000000
--- a/drivers/scsi/lpfc/lpfc_debugfs.c
+++ b/drivers/scsi/lpfc/lpfc_debugfs.c
@@@ -611,8 -630,622 +611,531 @@@ lpfc_debugfs_nodelist_data(struct lpfc_
  		len +=  snprintf(buf+len, size-len, "\n");
  	}
  	spin_unlock_irq(shost->host_lock);
 -
 -	if (phba->nvmet_support && phba->targetport && (vport == phba->pport)) {
 -		tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
 -		len += snprintf(buf + len, size - len,
 -				"\nNVME Targetport Entry ...\n");
 -
 -		/* Port state is only one of two values for now. */
 -		if (phba->targetport->port_id)
 -			statep = "REGISTERED";
 -		else
 -			statep = "INIT";
 -		len += snprintf(buf + len, size - len,
 -				"TGT WWNN x%llx WWPN x%llx State %s\n",
 -				wwn_to_u64(vport->fc_nodename.u.wwn),
 -				wwn_to_u64(vport->fc_portname.u.wwn),
 -				statep);
 -		len += snprintf(buf + len, size - len,
 -				"    Targetport DID x%06x\n",
 -				phba->targetport->port_id);
 -		goto out_exit;
 -	}
 -
 -	len += snprintf(buf + len, size - len,
 -				"\nNVME Lport/Rport Entries ...\n");
 -
 -	localport = vport->localport;
 -	if (!localport)
 -		goto out_exit;
 -
 -	spin_lock_irq(shost->host_lock);
 -	lport = (struct lpfc_nvme_lport *)localport->private;
 -
 -	/* Port state is only one of two values for now. */
 -	if (localport->port_id)
 -		statep = "ONLINE";
 -	else
 -		statep = "UNKNOWN ";
 -
 -	len += snprintf(buf + len, size - len,
 -			"Lport DID x%06x PortState %s\n",
 -			localport->port_id, statep);
 -
 -	len += snprintf(buf + len, size - len, "\tRport List:\n");
 -	list_for_each_entry(rport, &lport->rport_list, list) {
 -		/* local short-hand pointer. */
 -		nrport = rport->remoteport;
 -
 -		/* Port state is only one of two values for now. */
 -		switch (nrport->port_state) {
 -		case FC_OBJSTATE_ONLINE:
 -			statep = "ONLINE";
 -			break;
 -		case FC_OBJSTATE_UNKNOWN:
 -			statep = "UNKNOWN ";
 -			break;
 -		default:
 -			statep = "UNSUPPORTED";
 -			break;
 -		}
 -
 -		/* Tab in to show lport ownership. */
 -		len += snprintf(buf + len, size - len,
 -				"\t%s Port ID:x%06x ",
 -				statep, nrport->port_id);
 -		len += snprintf(buf + len, size - len, "WWPN x%llx ",
 -				nrport->port_name);
 -		len += snprintf(buf + len, size - len, "WWNN x%llx ",
 -				nrport->node_name);
 -		switch (nrport->port_role) {
 -		case FC_PORT_ROLE_NVME_INITIATOR:
 -			len +=  snprintf(buf + len, size - len,
 -					 "NVME INITIATOR ");
 -			break;
 -		case FC_PORT_ROLE_NVME_TARGET:
 -			len +=  snprintf(buf + len, size - len,
 -					 "NVME TARGET ");
 -			break;
 -		case FC_PORT_ROLE_NVME_DISCOVERY:
 -			len +=  snprintf(buf + len, size - len,
 -					 "NVME DISCOVERY ");
 -			break;
 -		default:
 -			len +=  snprintf(buf + len, size - len,
 -					 "UNKNOWN ROLE x%x",
 -					 nrport->port_role);
 -			break;
 -		}
 -
 -		/* Terminate the string. */
 -		len +=  snprintf(buf + len, size - len, "\n");
 -	}
 -
 -	spin_unlock_irq(shost->host_lock);
 - out_exit:
  	return len;
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * lpfc_debugfs_nvmestat_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmestat_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	struct lpfc_nvmet_rcv_ctx *ctxp, *next_ctxp;
+ 	int len = 0;
+ 	int cnt;
+ 
+ 	if (phba->nvmet_support) {
+ 		if (!phba->targetport)
+ 			return len;
+ 		tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
+ 		len += snprintf(buf + len, size - len,
+ 				"\nNVME Targetport Statistics\n");
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"LS: Rcv %08x Drop %08x Abort %08x\n",
+ 				atomic_read(&tgtp->rcv_ls_req_in),
+ 				atomic_read(&tgtp->rcv_ls_req_drop),
+ 				atomic_read(&tgtp->xmt_ls_abort));
+ 		if (atomic_read(&tgtp->rcv_ls_req_in) !=
+ 		    atomic_read(&tgtp->rcv_ls_req_out)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Rcv LS: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_ls_req_in),
+ 					atomic_read(&tgtp->rcv_ls_req_out));
+ 		}
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"LS: Xmt %08x Drop %08x Cmpl %08x Err %08x\n",
+ 				atomic_read(&tgtp->xmt_ls_rsp),
+ 				atomic_read(&tgtp->xmt_ls_drop),
+ 				atomic_read(&tgtp->xmt_ls_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_ls_rsp_error));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP: Rcv %08x Drop %08x\n",
+ 				atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 				atomic_read(&tgtp->rcv_fcp_cmd_drop));
+ 
+ 		if (atomic_read(&tgtp->rcv_fcp_cmd_in) !=
+ 		    atomic_read(&tgtp->rcv_fcp_cmd_out)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Rcv FCP: in %08x != out %08x\n",
+ 					atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 					atomic_read(&tgtp->rcv_fcp_cmd_out));
+ 		}
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP Rsp: read %08x readrsp %08x "
+ 				"write %08x rsp %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_read),
+ 				atomic_read(&tgtp->xmt_fcp_read_rsp),
+ 				atomic_read(&tgtp->xmt_fcp_write),
+ 				atomic_read(&tgtp->xmt_fcp_rsp));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP Rsp Cmpl: %08x err %08x drop %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_rsp_cmpl),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_error),
+ 				atomic_read(&tgtp->xmt_fcp_rsp_drop));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"ABORT: Xmt %08x Cmpl %08x\n",
+ 				atomic_read(&tgtp->xmt_fcp_abort),
+ 				atomic_read(&tgtp->xmt_fcp_abort_cmpl));
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"ABORT: Sol %08x  Usol %08x Err %08x Cmpl %08x",
+ 				atomic_read(&tgtp->xmt_abort_sol),
+ 				atomic_read(&tgtp->xmt_abort_unsol),
+ 				atomic_read(&tgtp->xmt_abort_rsp),
+ 				atomic_read(&tgtp->xmt_abort_rsp_error));
+ 
+ 		len +=  snprintf(buf + len, size - len, "\n");
+ 
+ 		cnt = 0;
+ 		spin_lock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		list_for_each_entry_safe(ctxp, next_ctxp,
+ 				&phba->sli4_hba.lpfc_abts_nvmet_ctx_list,
+ 				list) {
+ 			cnt++;
+ 		}
+ 		spin_unlock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		if (cnt) {
+ 			len += snprintf(buf + len, size - len,
+ 					"ABORT: %d ctx entries\n", cnt);
+ 			spin_lock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 			list_for_each_entry_safe(ctxp, next_ctxp,
+ 				    &phba->sli4_hba.lpfc_abts_nvmet_ctx_list,
+ 				    list) {
+ 				if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ))
+ 					break;
+ 				len += snprintf(buf + len, size - len,
+ 						"Entry: oxid %x state %x "
+ 						"flag %x\n",
+ 						ctxp->oxid, ctxp->state,
+ 						ctxp->flag);
+ 			}
+ 			spin_unlock(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		}
+ 	} else {
+ 		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME))
+ 			return len;
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"\nNVME Lport Statistics\n");
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"LS: Xmt %016llx Cmpl %016llx\n",
+ 				phba->fc4NvmeLsRequests,
+ 				phba->fc4NvmeLsCmpls);
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"FCP: Rd %016llx Wr %016llx IO %016llx\n",
+ 				phba->fc4NvmeInputRequests,
+ 				phba->fc4NvmeOutputRequests,
+ 				phba->fc4NvmeControlRequests);
+ 
+ 		len += snprintf(buf + len, size - len,
+ 				"    Cmpl %016llx\n", phba->fc4NvmeIoCmpls);
+ 	}
+ 
+ 	return len;
+ }
+ 
+ 
+ /**
+  * lpfc_debugfs_nvmektime_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmektime_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	int len = 0;
+ 
+ 	if (phba->nvmet_support == 0) {
+ 		/* NVME Initiator */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"ktime %s: Total Samples: %lld\n",
+ 				(phba->ktime_on ?  "Enabled" : "Disabled"),
+ 				phba->ktime_data_samples);
+ 		if (phba->ktime_data_samples == 0)
+ 			return len;
+ 
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 1: Last NVME Cmd cmpl "
+ 			"done -to- Start of next NVME cnd (in driver)\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg1_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg1_min,
+ 			phba->ktime_seg1_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 2: Driver start of NVME cmd "
+ 			"-to- Firmware WQ doorbell\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg2_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg2_min,
+ 			phba->ktime_seg2_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 3: Firmware WQ doorbell -to- "
+ 			"MSI-X ISR cmpl\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg3_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg3_min,
+ 			phba->ktime_seg3_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Segment 4: MSI-X ISR cmpl -to- "
+ 			"NVME cmpl done\n");
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg4_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg4_min,
+ 			phba->ktime_seg4_max);
+ 		len += snprintf(
+ 			buf + len, PAGE_SIZE - len,
+ 			"Total IO avg time: %08lld\n",
+ 			div_u64(phba->ktime_seg1_total +
+ 			phba->ktime_seg2_total  +
+ 			phba->ktime_seg3_total +
+ 			phba->ktime_seg4_total,
+ 			phba->ktime_data_samples));
+ 		return len;
+ 	}
+ 
+ 	/* NVME Target */
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"ktime %s: Total Samples: %lld %lld\n",
+ 			(phba->ktime_on ? "Enabled" : "Disabled"),
+ 			phba->ktime_data_samples,
+ 			phba->ktime_status_samples);
+ 	if (phba->ktime_data_samples == 0)
+ 		return len;
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 1: MSI-X ISR Rcv cmd -to- "
+ 			"cmd pass to NVME Layer\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg1_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg1_min,
+ 			phba->ktime_seg1_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 2: cmd pass to NVME Layer- "
+ 			"-to- Driver rcv cmd OP (action)\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg2_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg2_min,
+ 			phba->ktime_seg2_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 3: Driver rcv cmd OP -to- "
+ 			"Firmware WQ doorbell: cmd\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg3_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg3_min,
+ 			phba->ktime_seg3_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 4: Firmware WQ doorbell: cmd "
+ 			"-to- MSI-X ISR for cmd cmpl\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg4_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg4_min,
+ 			phba->ktime_seg4_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 5: MSI-X ISR for cmd cmpl "
+ 			"-to- NVME layer passed cmd done\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg5_total,
+ 				phba->ktime_data_samples),
+ 			phba->ktime_seg5_min,
+ 			phba->ktime_seg5_max);
+ 
+ 	if (phba->ktime_status_samples == 0) {
+ 		len += snprintf(buf + len, PAGE_SIZE-len,
+ 				"Total: cmd received by MSI-X ISR "
+ 				"-to- cmd completed on wire\n");
+ 		len += snprintf(buf + len, PAGE_SIZE-len,
+ 				"avg:%08lld min:%08lld "
+ 				"max %08lld\n",
+ 				div_u64(phba->ktime_seg10_total,
+ 					phba->ktime_data_samples),
+ 				phba->ktime_seg10_min,
+ 				phba->ktime_seg10_max);
+ 		return len;
+ 	}
+ 
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 6: NVME layer passed cmd done "
+ 			"-to- Driver rcv rsp status OP\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg6_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg6_min,
+ 			phba->ktime_seg6_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 7: Driver rcv rsp status OP "
+ 			"-to- Firmware WQ doorbell: status\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg7_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg7_min,
+ 			phba->ktime_seg7_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 8: Firmware WQ doorbell: status"
+ 			" -to- MSI-X ISR for status cmpl\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg8_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg8_min,
+ 			phba->ktime_seg8_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Segment 9: MSI-X ISR for status cmpl  "
+ 			"-to- NVME layer passed status done\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg9_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg9_min,
+ 			phba->ktime_seg9_max);
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"Total: cmd received by MSI-X ISR -to- "
+ 			"cmd completed on wire\n");
+ 	len += snprintf(buf + len, PAGE_SIZE-len,
+ 			"avg:%08lld min:%08lld max %08lld\n",
+ 			div_u64(phba->ktime_seg10_total,
+ 				phba->ktime_status_samples),
+ 			phba->ktime_seg10_min,
+ 			phba->ktime_seg10_max);
+ 	return len;
+ }
+ 
+ /**
+  * lpfc_debugfs_nvmeio_trc_data - Dump NVME IO trace list to a buffer
+  * @phba: The phba to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME IO trace associated with @phba
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_nvmeio_trc_data(struct lpfc_hba *phba, char *buf, int size)
+ {
+ 	struct lpfc_debugfs_nvmeio_trc *dtp;
+ 	int i, state, index, skip;
+ 	int len = 0;
+ 
+ 	state = phba->nvmeio_trc_on;
+ 
+ 	index = (atomic_read(&phba->nvmeio_trc_cnt) + 1) &
+ 		(phba->nvmeio_trc_size - 1);
+ 	skip = phba->nvmeio_trc_output_idx;
+ 
+ 	len += snprintf(buf + len, size - len,
+ 			"%s IO Trace %s: next_idx %d skip %d size %d\n",
+ 			(phba->nvmet_support ? "NVME" : "NVMET"),
+ 			(state ? "Enabled" : "Disabled"),
+ 			index, skip, phba->nvmeio_trc_size);
+ 
+ 	if (!phba->nvmeio_trc || state)
+ 		return len;
+ 
+ 	/* trace MUST bhe off to continue */
+ 
+ 	for (i = index; i < phba->nvmeio_trc_size; i++) {
+ 		if (skip) {
+ 			skip--;
+ 			continue;
+ 		}
+ 		dtp = phba->nvmeio_trc + i;
+ 		phba->nvmeio_trc_output_idx++;
+ 
+ 		if (!dtp->fmt)
+ 			continue;
+ 
+ 		len +=  snprintf(buf + len, size - len, dtp->fmt,
+ 			dtp->data1, dtp->data2, dtp->data3);
+ 
+ 		if (phba->nvmeio_trc_output_idx >= phba->nvmeio_trc_size) {
+ 			phba->nvmeio_trc_output_idx = 0;
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Complete\n");
+ 			goto out;
+ 		}
+ 
+ 		if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Continue (%d of %d)\n",
+ 					phba->nvmeio_trc_output_idx,
+ 					phba->nvmeio_trc_size);
+ 			goto out;
+ 		}
+ 	}
+ 	for (i = 0; i < index; i++) {
+ 		if (skip) {
+ 			skip--;
+ 			continue;
+ 		}
+ 		dtp = phba->nvmeio_trc + i;
+ 		phba->nvmeio_trc_output_idx++;
+ 
+ 		if (!dtp->fmt)
+ 			continue;
+ 
+ 		len +=  snprintf(buf + len, size - len, dtp->fmt,
+ 			dtp->data1, dtp->data2, dtp->data3);
+ 
+ 		if (phba->nvmeio_trc_output_idx >= phba->nvmeio_trc_size) {
+ 			phba->nvmeio_trc_output_idx = 0;
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Complete\n");
+ 			goto out;
+ 		}
+ 
+ 		if (len >= (size - LPFC_DEBUG_OUT_LINE_SZ)) {
+ 			len += snprintf(buf + len, size - len,
+ 					"Trace Continue (%d of %d)\n",
+ 					phba->nvmeio_trc_output_idx,
+ 					phba->nvmeio_trc_size);
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	len += snprintf(buf + len, size - len,
+ 			"Trace Done\n");
+ out:
+ 	return len;
+ }
+ 
+ /**
+  * lpfc_debugfs_cpucheck_data - Dump target node list to a buffer
+  * @vport: The vport to gather target node info from.
+  * @buf: The buffer to dump log into.
+  * @size: The maximum amount of data to process.
+  *
+  * Description:
+  * This routine dumps the NVME statistics associated with @vport
+  *
+  * Return Value:
+  * This routine returns the amount of bytes that were dumped into @buf and will
+  * not exceed @size.
+  **/
+ static int
+ lpfc_debugfs_cpucheck_data(struct lpfc_vport *vport, char *buf, int size)
+ {
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	int i;
+ 	int len = 0;
+ 	uint32_t tot_xmt = 0;
+ 	uint32_t tot_rcv = 0;
+ 	uint32_t tot_cmpl = 0;
+ 	uint32_t tot_ccmpl = 0;
+ 
+ 	if (phba->nvmet_support == 0) {
+ 		/* NVME Initiator */
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"CPUcheck %s\n",
+ 				(phba->cpucheck_on & LPFC_CHECK_NVME_IO ?
+ 					"Enabled" : "Disabled"));
+ 		for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
+ 			if (i >= LPFC_CHECK_CPU_CNT)
+ 				break;
+ 			len += snprintf(buf + len, PAGE_SIZE - len,
+ 					"%02d: xmit x%08x cmpl x%08x\n",
+ 					i, phba->cpucheck_xmt_io[i],
+ 					phba->cpucheck_cmpl_io[i]);
+ 			tot_xmt += phba->cpucheck_xmt_io[i];
+ 			tot_cmpl += phba->cpucheck_cmpl_io[i];
+ 		}
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"tot:xmit x%08x cmpl x%08x\n",
+ 				tot_xmt, tot_cmpl);
+ 		return len;
+ 	}
+ 
+ 	/* NVME Target */
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"CPUcheck %s ",
+ 			(phba->cpucheck_on & LPFC_CHECK_NVMET_IO ?
+ 				"IO Enabled - " : "IO Disabled - "));
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"%s\n",
+ 			(phba->cpucheck_on & LPFC_CHECK_NVMET_RCV ?
+ 				"Rcv Enabled\n" : "Rcv Disabled\n"));
+ 	for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
+ 		if (i >= LPFC_CHECK_CPU_CNT)
+ 			break;
+ 		len += snprintf(buf + len, PAGE_SIZE - len,
+ 				"%02d: xmit x%08x ccmpl x%08x "
+ 				"cmpl x%08x rcv x%08x\n",
+ 				i, phba->cpucheck_xmt_io[i],
+ 				phba->cpucheck_ccmpl_io[i],
+ 				phba->cpucheck_cmpl_io[i],
+ 				phba->cpucheck_rcv_io[i]);
+ 		tot_xmt += phba->cpucheck_xmt_io[i];
+ 		tot_rcv += phba->cpucheck_rcv_io[i];
+ 		tot_cmpl += phba->cpucheck_cmpl_io[i];
+ 		tot_ccmpl += phba->cpucheck_ccmpl_io[i];
+ 	}
+ 	len += snprintf(buf + len, PAGE_SIZE - len,
+ 			"tot:xmit x%08x ccmpl x%08x cmpl x%08x rcv x%08x\n",
+ 			tot_xmt, tot_ccmpl, tot_cmpl, tot_rcv);
+ 	return len;
+ }
+ 
++>>>>>>> 547077a44b3b (scsi: lpfc: Adding additional stats counters for nvme.)
  #endif
  
  /**
@@@ -1243,6 -1896,426 +1766,429 @@@ lpfc_debugfs_dumpDataDif_release(struc
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ 
+ static int
+ lpfc_debugfs_nvmestat_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_vport *vport = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_NVMESTAT_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_nvmestat_data(vport, debug->buffer,
+ 		LPFC_NVMESTAT_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static ssize_t
+ lpfc_debugfs_nvmestat_write(struct file *file, const char __user *buf,
+ 			    size_t nbytes, loff_t *ppos)
+ {
+ 	struct lpfc_debug *debug = file->private_data;
+ 	struct lpfc_vport *vport = (struct lpfc_vport *)debug->i_private;
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	char mybuf[64];
+ 	char *pbuf;
+ 
+ 	if (!phba->targetport)
+ 		return -ENXIO;
+ 
+ 	if (nbytes > 64)
+ 		nbytes = 64;
+ 
+ 	/* Protect copy from user */
+ 	if (!access_ok(VERIFY_READ, buf, nbytes))
+ 		return -EFAULT;
+ 
+ 	memset(mybuf, 0, sizeof(mybuf));
+ 
+ 	if (copy_from_user(mybuf, buf, nbytes))
+ 		return -EFAULT;
+ 	pbuf = &mybuf[0];
+ 
+ 	tgtp = (struct lpfc_nvmet_tgtport *)phba->targetport->private;
+ 	if ((strncmp(pbuf, "reset", strlen("reset")) == 0) ||
+ 	    (strncmp(pbuf, "zero", strlen("zero")) == 0)) {
+ 		atomic_set(&tgtp->rcv_ls_req_in, 0);
+ 		atomic_set(&tgtp->rcv_ls_req_out, 0);
+ 		atomic_set(&tgtp->rcv_ls_req_drop, 0);
+ 		atomic_set(&tgtp->xmt_ls_abort, 0);
+ 		atomic_set(&tgtp->xmt_ls_abort_cmpl, 0);
+ 		atomic_set(&tgtp->xmt_ls_rsp, 0);
+ 		atomic_set(&tgtp->xmt_ls_drop, 0);
+ 		atomic_set(&tgtp->xmt_ls_rsp_error, 0);
+ 		atomic_set(&tgtp->xmt_ls_rsp_cmpl, 0);
+ 
+ 		atomic_set(&tgtp->rcv_fcp_cmd_in, 0);
+ 		atomic_set(&tgtp->rcv_fcp_cmd_out, 0);
+ 		atomic_set(&tgtp->rcv_fcp_cmd_drop, 0);
+ 		atomic_set(&tgtp->xmt_fcp_drop, 0);
+ 		atomic_set(&tgtp->xmt_fcp_read_rsp, 0);
+ 		atomic_set(&tgtp->xmt_fcp_read, 0);
+ 		atomic_set(&tgtp->xmt_fcp_write, 0);
+ 		atomic_set(&tgtp->xmt_fcp_rsp, 0);
+ 		atomic_set(&tgtp->xmt_fcp_release, 0);
+ 		atomic_set(&tgtp->xmt_fcp_rsp_cmpl, 0);
+ 		atomic_set(&tgtp->xmt_fcp_rsp_error, 0);
+ 		atomic_set(&tgtp->xmt_fcp_rsp_drop, 0);
+ 
+ 		atomic_set(&tgtp->xmt_fcp_abort, 0);
+ 		atomic_set(&tgtp->xmt_fcp_abort_cmpl, 0);
+ 		atomic_set(&tgtp->xmt_abort_sol, 0);
+ 		atomic_set(&tgtp->xmt_abort_unsol, 0);
+ 		atomic_set(&tgtp->xmt_abort_rsp, 0);
+ 		atomic_set(&tgtp->xmt_abort_rsp_error, 0);
+ 	}
+ 	return nbytes;
+ }
+ 
+ static int
+ lpfc_debugfs_nvmektime_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_vport *vport = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_NVMEKTIME_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_nvmektime_data(vport, debug->buffer,
+ 		LPFC_NVMEKTIME_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static ssize_t
+ lpfc_debugfs_nvmektime_write(struct file *file, const char __user *buf,
+ 			     size_t nbytes, loff_t *ppos)
+ {
+ 	struct lpfc_debug *debug = file->private_data;
+ 	struct lpfc_vport *vport = (struct lpfc_vport *)debug->i_private;
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	char mybuf[64];
+ 	char *pbuf;
+ 
+ 	if (nbytes > 64)
+ 		nbytes = 64;
+ 
+ 	/* Protect copy from user */
+ 	if (!access_ok(VERIFY_READ, buf, nbytes))
+ 		return -EFAULT;
+ 
+ 	memset(mybuf, 0, sizeof(mybuf));
+ 
+ 	if (copy_from_user(mybuf, buf, nbytes))
+ 		return -EFAULT;
+ 	pbuf = &mybuf[0];
+ 
+ 	if ((strncmp(pbuf, "on", sizeof("on") - 1) == 0)) {
+ 		phba->ktime_data_samples = 0;
+ 		phba->ktime_status_samples = 0;
+ 		phba->ktime_seg1_total = 0;
+ 		phba->ktime_seg1_max = 0;
+ 		phba->ktime_seg1_min = 0xffffffff;
+ 		phba->ktime_seg2_total = 0;
+ 		phba->ktime_seg2_max = 0;
+ 		phba->ktime_seg2_min = 0xffffffff;
+ 		phba->ktime_seg3_total = 0;
+ 		phba->ktime_seg3_max = 0;
+ 		phba->ktime_seg3_min = 0xffffffff;
+ 		phba->ktime_seg4_total = 0;
+ 		phba->ktime_seg4_max = 0;
+ 		phba->ktime_seg4_min = 0xffffffff;
+ 		phba->ktime_seg5_total = 0;
+ 		phba->ktime_seg5_max = 0;
+ 		phba->ktime_seg5_min = 0xffffffff;
+ 		phba->ktime_seg6_total = 0;
+ 		phba->ktime_seg6_max = 0;
+ 		phba->ktime_seg6_min = 0xffffffff;
+ 		phba->ktime_seg7_total = 0;
+ 		phba->ktime_seg7_max = 0;
+ 		phba->ktime_seg7_min = 0xffffffff;
+ 		phba->ktime_seg8_total = 0;
+ 		phba->ktime_seg8_max = 0;
+ 		phba->ktime_seg8_min = 0xffffffff;
+ 		phba->ktime_seg9_total = 0;
+ 		phba->ktime_seg9_max = 0;
+ 		phba->ktime_seg9_min = 0xffffffff;
+ 		phba->ktime_seg10_total = 0;
+ 		phba->ktime_seg10_max = 0;
+ 		phba->ktime_seg10_min = 0xffffffff;
+ 
+ 		phba->ktime_on = 1;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "off",
+ 		   sizeof("off") - 1) == 0)) {
+ 		phba->ktime_on = 0;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "zero",
+ 		   sizeof("zero") - 1) == 0)) {
+ 		phba->ktime_data_samples = 0;
+ 		phba->ktime_status_samples = 0;
+ 		phba->ktime_seg1_total = 0;
+ 		phba->ktime_seg1_max = 0;
+ 		phba->ktime_seg1_min = 0xffffffff;
+ 		phba->ktime_seg2_total = 0;
+ 		phba->ktime_seg2_max = 0;
+ 		phba->ktime_seg2_min = 0xffffffff;
+ 		phba->ktime_seg3_total = 0;
+ 		phba->ktime_seg3_max = 0;
+ 		phba->ktime_seg3_min = 0xffffffff;
+ 		phba->ktime_seg4_total = 0;
+ 		phba->ktime_seg4_max = 0;
+ 		phba->ktime_seg4_min = 0xffffffff;
+ 		phba->ktime_seg5_total = 0;
+ 		phba->ktime_seg5_max = 0;
+ 		phba->ktime_seg5_min = 0xffffffff;
+ 		phba->ktime_seg6_total = 0;
+ 		phba->ktime_seg6_max = 0;
+ 		phba->ktime_seg6_min = 0xffffffff;
+ 		phba->ktime_seg7_total = 0;
+ 		phba->ktime_seg7_max = 0;
+ 		phba->ktime_seg7_min = 0xffffffff;
+ 		phba->ktime_seg8_total = 0;
+ 		phba->ktime_seg8_max = 0;
+ 		phba->ktime_seg8_min = 0xffffffff;
+ 		phba->ktime_seg9_total = 0;
+ 		phba->ktime_seg9_max = 0;
+ 		phba->ktime_seg9_min = 0xffffffff;
+ 		phba->ktime_seg10_total = 0;
+ 		phba->ktime_seg10_max = 0;
+ 		phba->ktime_seg10_min = 0xffffffff;
+ 		return strlen(pbuf);
+ 	}
+ 	return -EINVAL;
+ }
+ 
+ static int
+ lpfc_debugfs_nvmeio_trc_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_hba *phba = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_NVMEIO_TRC_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_nvmeio_trc_data(phba, debug->buffer,
+ 		LPFC_NVMEIO_TRC_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static ssize_t
+ lpfc_debugfs_nvmeio_trc_write(struct file *file, const char __user *buf,
+ 			      size_t nbytes, loff_t *ppos)
+ {
+ 	struct lpfc_debug *debug = file->private_data;
+ 	struct lpfc_hba *phba = (struct lpfc_hba *)debug->i_private;
+ 	int i;
+ 	unsigned long sz;
+ 	char mybuf[64];
+ 	char *pbuf;
+ 
+ 	if (nbytes > 64)
+ 		nbytes = 64;
+ 
+ 	/* Protect copy from user */
+ 	if (!access_ok(VERIFY_READ, buf, nbytes))
+ 		return -EFAULT;
+ 
+ 	memset(mybuf, 0, sizeof(mybuf));
+ 
+ 	if (copy_from_user(mybuf, buf, nbytes))
+ 		return -EFAULT;
+ 	pbuf = &mybuf[0];
+ 
+ 	if ((strncmp(pbuf, "off", sizeof("off") - 1) == 0)) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0570 nvmeio_trc_off\n");
+ 		phba->nvmeio_trc_output_idx = 0;
+ 		phba->nvmeio_trc_on = 0;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "on", sizeof("on") - 1) == 0)) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0571 nvmeio_trc_on\n");
+ 		phba->nvmeio_trc_output_idx = 0;
+ 		phba->nvmeio_trc_on = 1;
+ 		return strlen(pbuf);
+ 	}
+ 
+ 	/* We must be off to allocate the trace buffer */
+ 	if (phba->nvmeio_trc_on != 0)
+ 		return -EINVAL;
+ 
+ 	/* If not on or off, the parameter is the trace buffer size */
+ 	i = kstrtoul(pbuf, 0, &sz);
+ 	if (i)
+ 		return -EINVAL;
+ 	phba->nvmeio_trc_size = (uint32_t)sz;
+ 
+ 	/* It must be a power of 2 - round down */
+ 	i = 0;
+ 	while (sz > 1) {
+ 		sz = sz >> 1;
+ 		i++;
+ 	}
+ 	sz = (1 << i);
+ 	if (phba->nvmeio_trc_size != sz)
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0572 nvmeio_trc_size changed to %ld\n",
+ 				sz);
+ 	phba->nvmeio_trc_size = (uint32_t)sz;
+ 
+ 	/* If one previously exists, free it */
+ 	kfree(phba->nvmeio_trc);
+ 
+ 	/* Allocate new trace buffer and initialize */
+ 	phba->nvmeio_trc = kmalloc((sizeof(struct lpfc_debugfs_nvmeio_trc) *
+ 				    sz), GFP_KERNEL);
+ 	if (!phba->nvmeio_trc) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"0573 Cannot create debugfs "
+ 				"nvmeio_trc buffer\n");
+ 		return -ENOMEM;
+ 	}
+ 	memset(phba->nvmeio_trc, 0,
+ 	       (sizeof(struct lpfc_debugfs_nvmeio_trc) * sz));
+ 	atomic_set(&phba->nvmeio_trc_cnt, 0);
+ 	phba->nvmeio_trc_on = 0;
+ 	phba->nvmeio_trc_output_idx = 0;
+ 
+ 	return strlen(pbuf);
+ }
+ 
+ static int
+ lpfc_debugfs_cpucheck_open(struct inode *inode, struct file *file)
+ {
+ 	struct lpfc_vport *vport = inode->i_private;
+ 	struct lpfc_debug *debug;
+ 	int rc = -ENOMEM;
+ 
+ 	debug = kmalloc(sizeof(*debug), GFP_KERNEL);
+ 	if (!debug)
+ 		goto out;
+ 
+ 	 /* Round to page boundary */
+ 	debug->buffer = kmalloc(LPFC_CPUCHECK_SIZE, GFP_KERNEL);
+ 	if (!debug->buffer) {
+ 		kfree(debug);
+ 		goto out;
+ 	}
+ 
+ 	debug->len = lpfc_debugfs_cpucheck_data(vport, debug->buffer,
+ 		LPFC_NVMEKTIME_SIZE);
+ 
+ 	debug->i_private = inode->i_private;
+ 	file->private_data = debug;
+ 
+ 	rc = 0;
+ out:
+ 	return rc;
+ }
+ 
+ static ssize_t
+ lpfc_debugfs_cpucheck_write(struct file *file, const char __user *buf,
+ 			    size_t nbytes, loff_t *ppos)
+ {
+ 	struct lpfc_debug *debug = file->private_data;
+ 	struct lpfc_vport *vport = (struct lpfc_vport *)debug->i_private;
+ 	struct lpfc_hba   *phba = vport->phba;
+ 	char mybuf[64];
+ 	char *pbuf;
+ 	int i;
+ 
+ 	if (nbytes > 64)
+ 		nbytes = 64;
+ 
+ 	/* Protect copy from user */
+ 	if (!access_ok(VERIFY_READ, buf, nbytes))
+ 		return -EFAULT;
+ 
+ 	memset(mybuf, 0, sizeof(mybuf));
+ 
+ 	if (copy_from_user(mybuf, buf, nbytes))
+ 		return -EFAULT;
+ 	pbuf = &mybuf[0];
+ 
+ 	if ((strncmp(pbuf, "on", sizeof("on") - 1) == 0)) {
+ 		if (phba->nvmet_support)
+ 			phba->cpucheck_on |= LPFC_CHECK_NVMET_IO;
+ 		else
+ 			phba->cpucheck_on |= LPFC_CHECK_NVME_IO;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "rcv",
+ 		   sizeof("rcv") - 1) == 0)) {
+ 		if (phba->nvmet_support)
+ 			phba->cpucheck_on |= LPFC_CHECK_NVMET_RCV;
+ 		else
+ 			return -EINVAL;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "off",
+ 		   sizeof("off") - 1) == 0)) {
+ 		phba->cpucheck_on = LPFC_CHECK_OFF;
+ 		return strlen(pbuf);
+ 	} else if ((strncmp(pbuf, "zero",
+ 		   sizeof("zero") - 1) == 0)) {
+ 		for (i = 0; i < phba->sli4_hba.num_present_cpu; i++) {
+ 			if (i >= LPFC_CHECK_CPU_CNT)
+ 				break;
+ 			phba->cpucheck_rcv_io[i] = 0;
+ 			phba->cpucheck_xmt_io[i] = 0;
+ 			phba->cpucheck_cmpl_io[i] = 0;
+ 			phba->cpucheck_ccmpl_io[i] = 0;
+ 		}
+ 		return strlen(pbuf);
+ 	}
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> 547077a44b3b (scsi: lpfc: Adding additional stats counters for nvme.)
  /*
   * ---------------------------------
   * iDiag debugfs file access methods
@@@ -1988,6 -3061,201 +2934,204 @@@ error_out
  	return -EINVAL;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ __lpfc_idiag_print_wq(struct lpfc_queue *qp, char *wqtype,
+ 			char *pbuffer, int len)
+ {
+ 	if (!qp)
+ 		return len;
+ 
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\t\t%s WQ info: ", wqtype);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"AssocCQID[%04d]: WQ-STAT[oflow:x%x posted:x%llx]\n",
+ 			qp->assoc_qid, qp->q_cnt_1,
+ 			(unsigned long long)qp->q_cnt_4);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\t\tWQID[%02d], QE-CNT[%04d], QE-SIZE[%04d], "
+ 			"HOST-IDX[%04d], PORT-IDX[%04d]",
+ 			qp->queue_id, qp->entry_count,
+ 			qp->entry_size, qp->host_index,
+ 			qp->hba_index);
+ 	len +=  snprintf(pbuffer + len,
+ 			LPFC_QUE_INFO_GET_BUF_SIZE - len, "\n");
+ 	return len;
+ }
+ 
+ static int
+ lpfc_idiag_wqs_for_cq(struct lpfc_hba *phba, char *wqtype, char *pbuffer,
+ 		int *len, int max_cnt, int cq_id)
+ {
+ 	struct lpfc_queue *qp;
+ 	int qidx;
+ 
+ 	for (qidx = 0; qidx < phba->cfg_fcp_io_channel; qidx++) {
+ 		qp = phba->sli4_hba.fcp_wq[qidx];
+ 		if (qp->assoc_qid != cq_id)
+ 			continue;
+ 		*len = __lpfc_idiag_print_wq(qp, wqtype, pbuffer, *len);
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 	}
+ 	for (qidx = 0; qidx < phba->cfg_nvme_io_channel; qidx++) {
+ 		qp = phba->sli4_hba.nvme_wq[qidx];
+ 		if (qp->assoc_qid != cq_id)
+ 			continue;
+ 		*len = __lpfc_idiag_print_wq(qp, wqtype, pbuffer, *len);
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ __lpfc_idiag_print_cq(struct lpfc_queue *qp, char *cqtype,
+ 			char *pbuffer, int len)
+ {
+ 	if (!qp)
+ 		return len;
+ 
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\t%s CQ info: ", cqtype);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"AssocEQID[%02d]: CQ STAT[max:x%x relw:x%x "
+ 			"xabt:x%x wq:x%llx]\n",
+ 			qp->assoc_qid, qp->q_cnt_1, qp->q_cnt_2,
+ 			qp->q_cnt_3, (unsigned long long)qp->q_cnt_4);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\tCQID[%02d], QE-CNT[%04d], QE-SIZE[%04d], "
+ 			"HOST-IDX[%04d], PORT-IDX[%04d]",
+ 			qp->queue_id, qp->entry_count,
+ 			qp->entry_size, qp->host_index,
+ 			qp->hba_index);
+ 
+ 	len +=  snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len, "\n");
+ 
+ 	return len;
+ }
+ 
+ static int
+ __lpfc_idiag_print_rqpair(struct lpfc_queue *qp, struct lpfc_queue *datqp,
+ 			char *rqtype, char *pbuffer, int len)
+ {
+ 	if (!qp || !datqp)
+ 		return len;
+ 
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\t\t%s RQ info: ", rqtype);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"AssocCQID[%02d]: RQ-STAT[nopost:x%x nobuf:x%x "
+ 			"posted:x%x rcv:x%llx]\n",
+ 			qp->assoc_qid, qp->q_cnt_1, qp->q_cnt_2,
+ 			qp->q_cnt_3, (unsigned long long)qp->q_cnt_4);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\t\tHQID[%02d], QE-CNT[%04d], QE-SIZE[%04d], "
+ 			"HOST-IDX[%04d], PORT-IDX[%04d]\n",
+ 			qp->queue_id, qp->entry_count, qp->entry_size,
+ 			qp->host_index, qp->hba_index);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\t\tDQID[%02d], QE-CNT[%04d], QE-SIZE[%04d], "
+ 			"HOST-IDX[%04d], PORT-IDX[%04d]\n",
+ 			datqp->queue_id, datqp->entry_count,
+ 			datqp->entry_size, datqp->host_index,
+ 			datqp->hba_index);
+ 	return len;
+ }
+ 
+ static int
+ lpfc_idiag_cqs_for_eq(struct lpfc_hba *phba, char *pbuffer,
+ 		int *len, int max_cnt, int eqidx, int eq_id)
+ {
+ 	struct lpfc_queue *qp;
+ 	int qidx, rc;
+ 
+ 	for (qidx = 0; qidx < phba->cfg_fcp_io_channel; qidx++) {
+ 		qp = phba->sli4_hba.fcp_cq[qidx];
+ 		if (qp->assoc_qid != eq_id)
+ 			continue;
+ 
+ 		*len = __lpfc_idiag_print_cq(qp, "FCP", pbuffer, *len);
+ 
+ 		/* Reset max counter */
+ 		qp->CQ_max_cqe = 0;
+ 
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 
+ 		rc = lpfc_idiag_wqs_for_cq(phba, "FCP", pbuffer, len,
+ 				max_cnt, qp->queue_id);
+ 		if (rc)
+ 			return 1;
+ 	}
+ 
+ 	for (qidx = 0; qidx < phba->cfg_nvme_io_channel; qidx++) {
+ 		qp = phba->sli4_hba.nvme_cq[qidx];
+ 		if (qp->assoc_qid != eq_id)
+ 			continue;
+ 
+ 		*len = __lpfc_idiag_print_cq(qp, "NVME", pbuffer, *len);
+ 
+ 		/* Reset max counter */
+ 		qp->CQ_max_cqe = 0;
+ 
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 
+ 		rc = lpfc_idiag_wqs_for_cq(phba, "NVME", pbuffer, len,
+ 				max_cnt, qp->queue_id);
+ 		if (rc)
+ 			return 1;
+ 	}
+ 
+ 	if (eqidx < phba->cfg_nvmet_mrq) {
+ 		/* NVMET CQset */
+ 		qp = phba->sli4_hba.nvmet_cqset[eqidx];
+ 		*len = __lpfc_idiag_print_cq(qp, "NVMET CQset", pbuffer, *len);
+ 
+ 		/* Reset max counter */
+ 		qp->CQ_max_cqe = 0;
+ 
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 
+ 		/* RQ header */
+ 		qp = phba->sli4_hba.nvmet_mrq_hdr[eqidx];
+ 		*len = __lpfc_idiag_print_rqpair(qp,
+ 				phba->sli4_hba.nvmet_mrq_data[eqidx],
+ 				"NVMET MRQ", pbuffer, *len);
+ 
+ 		if (*len >= max_cnt)
+ 			return 1;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ __lpfc_idiag_print_eq(struct lpfc_queue *qp, char *eqtype,
+ 			char *pbuffer, int len)
+ {
+ 	if (!qp)
+ 		return len;
+ 
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"\n%s EQ info: EQ-STAT[max:x%x noE:x%x "
+ 			"bs:x%x proc:x%llx]\n",
+ 			eqtype, qp->q_cnt_1, qp->q_cnt_2, qp->q_cnt_3,
+ 			(unsigned long long)qp->q_cnt_4);
+ 	len += snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len,
+ 			"EQID[%02d], QE-CNT[%04d], QE-SIZE[%04d], "
+ 			"HOST-IDX[%04d], PORT-IDX[%04d]",
+ 			qp->queue_id, qp->entry_count, qp->entry_size,
+ 			qp->host_index, qp->hba_index);
+ 	len +=  snprintf(pbuffer + len, LPFC_QUE_INFO_GET_BUF_SIZE - len, "\n");
+ 
+ 	return len;
+ }
+ 
++>>>>>>> 547077a44b3b (scsi: lpfc: Adding additional stats counters for nvme.)
  /**
   * lpfc_idiag_queinfo_read - idiag debugfs read queue information
   * @file: The file pointer to read from.
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index c54385fd9058,333c5094b97d..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -12130,8 -12786,10 +12131,9 @@@ static boo
  lpfc_sli4_sp_handle_rcqe(struct lpfc_hba *phba, struct lpfc_rcqe *rcqe)
  {
  	bool workposted = false;
 -	struct fc_frame_header *fc_hdr;
  	struct lpfc_queue *hrq = phba->sli4_hba.hdr_rq;
  	struct lpfc_queue *drq = phba->sli4_hba.dat_rq;
+ 	struct lpfc_nvmet_tgtport *tgtp;
  	struct hbq_dmabuf *dma_buf;
  	uint32_t status, rq_id;
  	unsigned long iflags;
@@@ -12163,7 -12820,12 +12164,8 @@@
  			goto out;
  		}
  		hrq->RQ_rcv_buf++;
+ 		hrq->RQ_buf_posted--;
  		memcpy(&dma_buf->cq_event.cqe.rcqe_cmpl, rcqe, sizeof(*rcqe));
 -
 -		/* If a NVME LS event (type 0x28), treat it as Fast path */
 -		fc_hdr = (struct fc_frame_header *)dma_buf->hbuf.virt;
 -
  		/* save off the frame for the word thread to process */
  		list_add_tail(&dma_buf->cq_event.list,
  			      &phba->sli4_hba.sp_queue_event);
@@@ -12438,7 -13133,117 +12453,121 @@@ lpfc_sli4_fp_handle_rel_wcqe(struct lpf
  }
  
  /**
++<<<<<<< HEAD
 + * lpfc_sli4_fp_handle_wcqe - Process fast-path work queue completion entry
++=======
+  * lpfc_sli4_nvmet_handle_rcqe - Process a receive-queue completion queue entry
+  * @phba: Pointer to HBA context object.
+  * @rcqe: Pointer to receive-queue completion queue entry.
+  *
+  * This routine process a receive-queue completion queue entry.
+  *
+  * Return: true if work posted to worker thread, otherwise false.
+  **/
+ static bool
+ lpfc_sli4_nvmet_handle_rcqe(struct lpfc_hba *phba, struct lpfc_queue *cq,
+ 			    struct lpfc_rcqe *rcqe)
+ {
+ 	bool workposted = false;
+ 	struct lpfc_queue *hrq;
+ 	struct lpfc_queue *drq;
+ 	struct rqb_dmabuf *dma_buf;
+ 	struct fc_frame_header *fc_hdr;
+ 	struct lpfc_nvmet_tgtport *tgtp;
+ 	uint32_t status, rq_id;
+ 	unsigned long iflags;
+ 	uint32_t fctl, idx;
+ 
+ 	if ((phba->nvmet_support == 0) ||
+ 	    (phba->sli4_hba.nvmet_cqset == NULL))
+ 		return workposted;
+ 
+ 	idx = cq->queue_id - phba->sli4_hba.nvmet_cqset[0]->queue_id;
+ 	hrq = phba->sli4_hba.nvmet_mrq_hdr[idx];
+ 	drq = phba->sli4_hba.nvmet_mrq_data[idx];
+ 
+ 	/* sanity check on queue memory */
+ 	if (unlikely(!hrq) || unlikely(!drq))
+ 		return workposted;
+ 
+ 	if (bf_get(lpfc_cqe_code, rcqe) == CQE_CODE_RECEIVE_V1)
+ 		rq_id = bf_get(lpfc_rcqe_rq_id_v1, rcqe);
+ 	else
+ 		rq_id = bf_get(lpfc_rcqe_rq_id, rcqe);
+ 
+ 	if ((phba->nvmet_support == 0) ||
+ 	    (rq_id != hrq->queue_id))
+ 		return workposted;
+ 
+ 	status = bf_get(lpfc_rcqe_status, rcqe);
+ 	switch (status) {
+ 	case FC_STATUS_RQ_BUF_LEN_EXCEEDED:
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 				"6126 Receive Frame Truncated!!\n");
+ 	case FC_STATUS_RQ_SUCCESS:
+ 		lpfc_sli4_rq_release(hrq, drq);
+ 		spin_lock_irqsave(&phba->hbalock, iflags);
+ 		dma_buf = lpfc_sli_rqbuf_get(phba, hrq);
+ 		if (!dma_buf) {
+ 			hrq->RQ_no_buf_found++;
+ 			spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 			goto out;
+ 		}
+ 		spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 		hrq->RQ_rcv_buf++;
+ 		hrq->RQ_buf_posted--;
+ 		fc_hdr = (struct fc_frame_header *)dma_buf->hbuf.virt;
+ 
+ 		/* Just some basic sanity checks on FCP Command frame */
+ 		fctl = (fc_hdr->fh_f_ctl[0] << 16 |
+ 		fc_hdr->fh_f_ctl[1] << 8 |
+ 		fc_hdr->fh_f_ctl[2]);
+ 		if (((fctl &
+ 		    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) !=
+ 		    (FC_FC_FIRST_SEQ | FC_FC_END_SEQ | FC_FC_SEQ_INIT)) ||
+ 		    (fc_hdr->fh_seq_cnt != 0)) /* 0 byte swapped is still 0 */
+ 			goto drop;
+ 
+ 		if (fc_hdr->fh_type == FC_TYPE_FCP) {
+ 			dma_buf->bytes_recv = bf_get(lpfc_rcqe_length,  rcqe);
+ 			lpfc_nvmet_unsol_fcp_event(
+ 				phba, phba->sli4_hba.els_wq->pring, dma_buf,
+ 				cq->assoc_qp->isr_timestamp);
+ 			return false;
+ 		}
+ drop:
+ 		lpfc_in_buf_free(phba, &dma_buf->dbuf);
+ 		break;
+ 	case FC_STATUS_INSUFF_BUF_FRM_DISC:
+ 		if (phba->nvmet_support) {
+ 			tgtp = phba->targetport->private;
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI | LOG_NVME,
+ 					"6401 RQE Error x%x, posted %d err_cnt "
+ 					"%d: %x %x %x\n",
+ 					status, hrq->RQ_buf_posted,
+ 					hrq->RQ_no_posted_buf,
+ 					atomic_read(&tgtp->rcv_fcp_cmd_in),
+ 					atomic_read(&tgtp->rcv_fcp_cmd_out),
+ 					atomic_read(&tgtp->xmt_fcp_release));
+ 		}
+ 		/* fallthrough */
+ 
+ 	case FC_STATUS_INSUFF_BUF_NEED_BUF:
+ 		hrq->RQ_no_posted_buf++;
+ 		/* Post more buffers if possible */
+ 		spin_lock_irqsave(&phba->hbalock, iflags);
+ 		phba->hba_flag |= HBA_POST_RECEIVE_BUFFER;
+ 		spin_unlock_irqrestore(&phba->hbalock, iflags);
+ 		workposted = true;
+ 		break;
+ 	}
+ out:
+ 	return workposted;
+ }
+ 
+ /**
+  * lpfc_sli4_fp_handle_cqe - Process fast-path work queue completion entry
++>>>>>>> 547077a44b3b (scsi: lpfc: Adding additional stats counters for nvme.)
   * @cq: Pointer to the completion queue.
   * @eqe: Pointer to fast-path completion queue entry.
   *
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc_attr.c
* Unmerged path drivers/scsi/lpfc/lpfc_debugfs.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
diff --git a/drivers/scsi/lpfc/lpfc_sli4.h b/drivers/scsi/lpfc/lpfc_sli4.h
index 10078254ebc7..d3c235847562 100644
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@ -173,7 +173,7 @@ struct lpfc_queue {
 /* defines for RQ stats */
 #define	RQ_no_posted_buf	q_cnt_1
 #define	RQ_no_buf_found		q_cnt_2
-#define	RQ_buf_trunc		q_cnt_3
+#define	RQ_buf_posted		q_cnt_3
 #define	RQ_rcv_buf		q_cnt_4
 
 	union sli4_qe qe[1];	/* array to index entries (must be last) */
