blk-mq: only apply active queue tag throttling for driver tags

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 200e86b3372b51e136a382e007b6b904b1dac7e4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/200e86b3.failed

If we have a scheduler attached, we have two sets of tags. We don't
want to apply our active queue throttling for the scheduler side
of tags, that only applies to driver tags since that's the resource
we need to dispatch an IO.

	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 200e86b3372b51e136a382e007b6b904b1dac7e4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-tag.c
#	block/blk-mq.c
diff --cc block/blk-mq-tag.c
index 7e6885bccaac,1b156ca79af6..000000000000
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@@ -144,134 -90,48 +144,153 @@@ static inline bool hctx_may_queue(struc
  	return atomic_read(&hctx->nr_active) < depth;
  }
  
++<<<<<<< HEAD
 +static int __bt_get_word(struct blk_align_bitmap *bm, unsigned int last_tag)
 +{
 +	int tag, org_last_tag = last_tag;
++=======
+ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
+ 			    struct sbitmap_queue *bt)
+ {
+ 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+ 	    !hctx_may_queue(data->hctx, bt))
+ 		return -1;
+ 	return __sbitmap_queue_get(bt);
+ }
++>>>>>>> 200e86b3372b (blk-mq: only apply active queue tag throttling for driver tags)
 +
 +	while (1) {
 +		tag = find_next_zero_bit(&bm->word, bm->depth, last_tag);
 +		if (unlikely(tag >= bm->depth)) {
 +			/*
 +			 * We started with an offset, and we didn't reset the
 +			 * offset to 0 in a failure case, so start from 0 to
 +			 * exhaust the map.
 +			 */
 +			if (org_last_tag && last_tag) {
 +				last_tag = org_last_tag = 0;
 +				continue;
 +			}
 +			return -1;
 +		}
  
 -unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 +		if (!test_and_set_bit(tag, &bm->word))
 +			break;
 +
 +		last_tag = tag + 1;
 +		if (last_tag >= bm->depth - 1)
 +			last_tag = 0;
 +	}
 +
++<<<<<<< HEAD
 +	return tag;
 +}
 +
 +/*
 + * Straight forward bitmap tag implementation, where each bit is a tag
 + * (cleared == free, and set == busy). The small twist is using per-cpu
 + * last_tag caches, which blk-mq stores in the blk_mq_ctx software queue
 + * contexts. This enables us to drastically limit the space searched,
 + * without dirtying an extra shared cacheline like we would if we stored
 + * the cache value inside the shared blk_mq_bitmap_tags structure. On top
 + * of that, each word of tags is in a separate cacheline. This means that
 + * multiple users will tend to stick to different cachelines, at least
 + * until the map is exhausted.
 + */
 +static int __bt_get(struct blk_mq_hw_ctx *hctx, struct blk_mq_bitmap_tags *bt,
 +		    unsigned int *tag_cache)
  {
 -	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 -	struct sbitmap_queue *bt;
 -	struct sbq_wait_state *ws;
 -	DEFINE_WAIT(wait);
 -	unsigned int tag_offset;
 -	int tag;
 +	unsigned int last_tag, org_last_tag;
 +	int index, i, tag;
 +
 +	if (!hctx_may_queue(hctx, bt))
 +		return -1;
 +
 +	last_tag = org_last_tag = *tag_cache;
 +	index = TAG_TO_INDEX(bt, last_tag);
  
 -	if (data->flags & BLK_MQ_REQ_RESERVED) {
 -		if (unlikely(!tags->nr_reserved_tags)) {
 -			WARN_ON_ONCE(1);
 -			return BLK_MQ_TAG_FAIL;
 +	for (i = 0; i < bt->map_nr; i++) {
 +		tag = __bt_get_word(&bt->map[index], TAG_TO_BIT(bt, last_tag));
 +		if (tag != -1) {
 +			tag += (index << bt->bits_per_word);
 +			goto done;
  		}
 -		bt = &tags->breserved_tags;
 -		tag_offset = 0;
 -	} else {
 -		bt = &tags->bitmap_tags;
 -		tag_offset = tags->nr_reserved_tags;
 +
 +		/*
 +		 * Jump to next index, and reset the last tag to be the
 +		 * first tag of that index
 +		 */
 +		index++;
 +		last_tag = (index << bt->bits_per_word);
 +
 +		if (index >= bt->map_nr) {
 +			index = 0;
 +			last_tag = 0;
 +		}
 +	}
 +
 +	*tag_cache = 0;
 +	return -1;
 +
 +	/*
 +	 * Only update the cache from the allocation path, if we ended
 +	 * up using the specific cached tag.
 +	 */
 +done:
 +	if (tag == org_last_tag) {
 +		last_tag = tag + 1;
 +		if (last_tag >= bt->depth - 1)
 +			last_tag = 0;
 +
 +		*tag_cache = last_tag;
  	}
  
 +	return tag;
 +}
 +
 +static struct bt_wait_state *bt_wait_ptr(struct blk_mq_bitmap_tags *bt,
 +					 struct blk_mq_hw_ctx *hctx)
 +{
 +	struct bt_wait_state *bs;
 +	int wait_index;
 +
 +	if (!hctx)
 +		return &bt->bs[0];
 +
 +	wait_index = atomic_read(&hctx->wait_index);
 +	bs = &bt->bs[wait_index];
 +	bt_index_atomic_inc(&hctx->wait_index);
 +	return bs;
 +}
 +
 +static int bt_get(struct blk_mq_alloc_data *data,
 +		struct blk_mq_bitmap_tags *bt,
 +		struct blk_mq_hw_ctx *hctx,
 +		unsigned int *last_tag)
 +{
 +	struct bt_wait_state *bs;
 +	DEFINE_WAIT(wait);
 +	int tag;
 +
 +	tag = __bt_get(hctx, bt, last_tag);
++=======
+ 	tag = __blk_mq_get_tag(data, bt);
++>>>>>>> 200e86b3372b (blk-mq: only apply active queue tag throttling for driver tags)
  	if (tag != -1)
 -		goto found_tag;
 +		return tag;
  
  	if (data->flags & BLK_MQ_REQ_NOWAIT)
 -		return BLK_MQ_TAG_FAIL;
 +		return -1;
  
 -	ws = bt_wait_ptr(bt, data->hctx);
 +	bs = bt_wait_ptr(bt, hctx);
  	do {
 -		prepare_to_wait(&ws->wait, &wait, TASK_UNINTERRUPTIBLE);
 +		prepare_to_wait(&bs->wait, &wait, TASK_UNINTERRUPTIBLE);
  
++<<<<<<< HEAD
 +		tag = __bt_get(hctx, bt, last_tag);
++=======
+ 		tag = __blk_mq_get_tag(data, bt);
++>>>>>>> 200e86b3372b (blk-mq: only apply active queue tag throttling for driver tags)
  		if (tag != -1)
  			break;
  
@@@ -288,7 -146,7 +307,11 @@@
  		 * Retry tag allocation after running the hardware queue,
  		 * as running the queue may also have found completions.
  		 */
++<<<<<<< HEAD
 +		tag = __bt_get(hctx, bt, last_tag);
++=======
+ 		tag = __blk_mq_get_tag(data, bt);
++>>>>>>> 200e86b3372b (blk-mq: only apply active queue tag throttling for driver tags)
  		if (tag != -1)
  			break;
  
diff --cc block/blk-mq.c
index 3b21482b7f01,dcb567642db7..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -251,15 -226,23 +251,31 @@@ __blk_mq_alloc_request(struct blk_mq_al
  
  	tag = blk_mq_get_tag(data);
  	if (tag != BLK_MQ_TAG_FAIL) {
 -		struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 +		rq = data->hctx->tags->rqs[tag];
  
 -		rq = tags->static_rqs[tag];
++<<<<<<< HEAD
 +		if (blk_mq_tag_busy(data->hctx)) {
 +			rq->cmd_flags = REQ_MQ_INFLIGHT;
 +			atomic_inc(&data->hctx->nr_active);
 +		}
  
 +		rq->tag = tag;
 +		blk_mq_rq_ctx_init(data->q, data->ctx, rq, rw);
++=======
+ 		if (data->flags & BLK_MQ_REQ_INTERNAL) {
+ 			rq->tag = -1;
+ 			rq->internal_tag = tag;
+ 		} else {
+ 			if (blk_mq_tag_busy(data->hctx)) {
+ 				rq->rq_flags = RQF_MQ_INFLIGHT;
+ 				atomic_inc(&data->hctx->nr_active);
+ 			}
+ 			rq->tag = tag;
+ 			rq->internal_tag = -1;
+ 		}
+ 
+ 		blk_mq_rq_ctx_init(data->q, data->ctx, rq, op);
++>>>>>>> 200e86b3372b (blk-mq: only apply active queue tag throttling for driver tags)
  		return rq;
  	}
  
@@@ -824,6 -846,63 +840,66 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
++<<<<<<< HEAD
++=======
+ static bool blk_mq_get_driver_tag(struct request *rq,
+ 				  struct blk_mq_hw_ctx **hctx, bool wait)
+ {
+ 	struct blk_mq_alloc_data data = {
+ 		.q = rq->q,
+ 		.ctx = rq->mq_ctx,
+ 		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
+ 		.flags = wait ? 0 : BLK_MQ_REQ_NOWAIT,
+ 	};
+ 
+ 	if (blk_mq_hctx_stopped(data.hctx))
+ 		return false;
+ 
+ 	if (rq->tag != -1) {
+ done:
+ 		if (hctx)
+ 			*hctx = data.hctx;
+ 		return true;
+ 	}
+ 
+ 	rq->tag = blk_mq_get_tag(&data);
+ 	if (rq->tag >= 0) {
+ 		if (blk_mq_tag_busy(data.hctx)) {
+ 			rq->rq_flags |= RQF_MQ_INFLIGHT;
+ 			atomic_inc(&data.hctx->nr_active);
+ 		}
+ 		data.hctx->tags->rqs[rq->tag] = rq;
+ 		goto done;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ /*
+  * If we fail getting a driver tag because all the driver tags are already
+  * assigned and on the dispatch list, BUT the first entry does not have a
+  * tag, then we could deadlock. For that case, move entries with assigned
+  * driver tags to the front, leaving the set of tagged requests in the
+  * same order, and the untagged set in the same order.
+  */
+ static bool reorder_tags_to_front(struct list_head *list)
+ {
+ 	struct request *rq, *tmp, *first = NULL;
+ 
+ 	list_for_each_entry_safe_reverse(rq, tmp, list, queuelist) {
+ 		if (rq == first)
+ 			break;
+ 		if (rq->tag != -1) {
+ 			list_move(&rq->queuelist, list);
+ 			if (!first)
+ 				first = rq;
+ 		}
+ 	}
+ 
+ 	return first != NULL;
+ }
+ 
++>>>>>>> 200e86b3372b (blk-mq: only apply active queue tag throttling for driver tags)
  bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list)
  {
  	struct request_queue *q = hctx->queue;
* Unmerged path block/blk-mq-tag.c
* Unmerged path block/blk-mq.c
