mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys (Rui Wang) [1272615]
Rebuild_FUZZ: 97.48%
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit 33a709b25a760b91184bb335cf7d7c32b8123013
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/33a709b2.failed

Today, for normal faults and page table walks, we check the VMA
and/or PTE to ensure that it is compatible with the action.  For
instance, if we get a write fault on a non-writeable VMA, we
SIGSEGV.

We try to do the same thing for protection keys.  Basically, we
try to make sure that if a user does this:

	mprotect(ptr, size, PROT_NONE);
	*ptr = foo;

they see the same effects with protection keys when they do this:

	mprotect(ptr, size, PROT_READ|PROT_WRITE);
	set_pkey(ptr, size, 4);
	wrpkru(0xffffff3f); // access disable pkey 4
	*ptr = foo;

The state to do that checking is in the VMA, but we also
sometimes have to do it on the page tables only, like when doing
a get_user_pages_fast() where we have no VMA.

We add two functions and expose them to generic code:

	arch_pte_access_permitted(pte_flags, write)
	arch_vma_access_permitted(vma, write)

These are, of course, backed up in x86 arch code with checks
against the PTE or VMA's protection key.

But, there are also cases where we do not want to respect
protection keys.  When we ptrace(), for instance, we do not want
to apply the tracer's PKRU permissions to the PTEs from the
process being traced.

	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Alexey Kardashevskiy <aik@ozlabs.ru>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Boaz Harrosh <boaz@plexistor.com>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Hansen <dave@sr71.net>
	Cc: David Gibson <david@gibson.dropbear.id.au>
	Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
	Cc: David Vrabel <david.vrabel@citrix.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
	Cc: Dominik Vogt <vogt@linux.vnet.ibm.com>
	Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jason Low <jason.low2@hp.com>
	Cc: Jerome Marchand <jmarchan@redhat.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Laurent Dufour <ldufour@linux.vnet.ibm.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
	Cc: Matthew Wilcox <willy@linux.intel.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Mikulas Patocka <mpatocka@redhat.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Sasha Levin <sasha.levin@oracle.com>
	Cc: Shachar Raindel <raindel@mellanox.com>
	Cc: Stephen Smalley <sds@tycho.nsa.gov>
	Cc: Toshi Kani <toshi.kani@hpe.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: linux-arch@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Cc: linux-mm@kvack.org
	Cc: linux-s390@vger.kernel.org
	Cc: linuxppc-dev@lists.ozlabs.org
Link: http://lkml.kernel.org/r/20160212210219.14D5D715@viggo.jf.intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 33a709b25a760b91184bb335cf7d7c32b8123013)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/mmu_context.h
#	arch/unicore32/include/asm/mmu_context.h
#	arch/x86/include/asm/pgtable.h
#	arch/x86/mm/fault.c
#	mm/gup.c
diff --cc arch/s390/include/asm/mmu_context.h
index eccdb1327fc0,2627b338382c..000000000000
--- a/arch/s390/include/asm/mmu_context.h
+++ b/arch/s390/include/asm/mmu_context.h
@@@ -108,5 -130,15 +108,19 @@@ static inline void arch_bprm_mm_init(st
  {
  }
  
++<<<<<<< HEAD
 +
++=======
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
  #endif /* __S390_MMU_CONTEXT_H */
diff --cc arch/unicore32/include/asm/mmu_context.h
index fb5e4c658f7a,3133f947ade2..000000000000
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@@ -84,4 -86,26 +84,29 @@@ static inline void arch_dup_mmap(struc
  {
  }
  
++<<<<<<< HEAD
++=======
+ static inline void arch_unmap(struct mm_struct *mm,
+ 			struct vm_area_struct *vma,
+ 			unsigned long start, unsigned long end)
+ {
+ }
+ 
+ static inline void arch_bprm_mm_init(struct mm_struct *mm,
+ 				     struct vm_area_struct *vma)
+ {
+ }
+ 
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	/* by default, allow everything */
+ 	return true;
+ }
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
  #endif
diff --cc arch/x86/include/asm/pgtable.h
index e3437b206dfd,3cbfae80abb2..000000000000
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@@ -923,6 -902,52 +923,55 @@@ static inline void update_mmu_cache_pmd
  {
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
+ static inline pte_t pte_swp_mksoft_dirty(pte_t pte)
+ {
+ 	return pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+ }
+ 
+ static inline int pte_swp_soft_dirty(pte_t pte)
+ {
+ 	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
+ }
+ 
+ static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
+ {
+ 	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
+ }
+ #endif
+ 
+ #define PKRU_AD_BIT 0x1
+ #define PKRU_WD_BIT 0x2
+ 
+ static inline bool __pkru_allows_read(u32 pkru, u16 pkey)
+ {
+ 	int pkru_pkey_bits = pkey * 2;
+ 	return !(pkru & (PKRU_AD_BIT << pkru_pkey_bits));
+ }
+ 
+ static inline bool __pkru_allows_write(u32 pkru, u16 pkey)
+ {
+ 	int pkru_pkey_bits = pkey * 2;
+ 	/*
+ 	 * Access-disable disables writes too so we need to check
+ 	 * both bits here.
+ 	 */
+ 	return !(pkru & ((PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits));
+ }
+ 
+ static inline u16 pte_flags_pkey(unsigned long pte_flags)
+ {
+ #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
+ 	/* ifdef to avoid doing 59-bit shift on 32-bit values */
+ 	return (pte_flags & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;
+ #else
+ 	return 0;
+ #endif
+ }
+ 
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
  #include <asm-generic/pgtable.h>
  #endif	/* __ASSEMBLY__ */
  
diff --cc arch/x86/mm/fault.c
index 1977abf5754c,319331afae24..000000000000
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@@ -809,14 -894,32 +809,36 @@@ __bad_area(struct pt_regs *regs, unsign
  static noinline void
  bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)
  {
 -	__bad_area(regs, error_code, address, NULL, SEGV_MAPERR);
 +	__bad_area(regs, error_code, address, SEGV_MAPERR);
  }
  
+ static inline bool bad_area_access_from_pkeys(unsigned long error_code,
+ 		struct vm_area_struct *vma)
+ {
+ 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
+ 		return false;
+ 	if (error_code & PF_PK)
+ 		return true;
+ 	return false;
+ }
+ 
  static noinline void
  bad_area_access_error(struct pt_regs *regs, unsigned long error_code,
 -		      unsigned long address, struct vm_area_struct *vma)
 +		      unsigned long address)
  {
++<<<<<<< HEAD
 +	__bad_area(regs, error_code, address, SEGV_ACCERR);
++=======
+ 	/*
+ 	 * This OSPKE check is not strictly necessary at runtime.
+ 	 * But, doing it this way allows compiler optimizations
+ 	 * if pkeys are compiled out.
+ 	 */
+ 	if (bad_area_access_from_pkeys(error_code, vma))
+ 		__bad_area(regs, error_code, address, vma, SEGV_PKUERR);
+ 	else
+ 		__bad_area(regs, error_code, address, vma, SEGV_ACCERR);
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
  }
  
  static void
diff --cc mm/gup.c
index 3166366affd5,e0f5f3574d16..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -4,9 -8,15 +4,19 @@@
  #include <linux/memremap.h>
  #include <linux/pagemap.h>
  #include <linux/rmap.h>
 -#include <linux/swap.h>
 +#include <linux/writeback.h>
 +#include <linux/mmu_notifier.h>
  #include <linux/swapops.h>
++<<<<<<< HEAD
++=======
+ 
+ #include <linux/sched.h>
+ #include <linux/rwsem.h>
+ #include <linux/hugetlb.h>
+ 
+ #include <asm/mmu_context.h>
+ #include <asm/pgtable.h>
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
  #include <asm/tlbflush.h>
  
  #include "internal.h"
@@@ -254,21 -195,259 +264,65 @@@ out
  no_page:
  	pte_unmap_unlock(ptep, ptl);
  	if (!pte_none(pte))
 -		return NULL;
 -	return no_page_table(vma, flags);
 -}
 -
 -/**
 - * follow_page_mask - look up a page descriptor from a user-virtual address
 - * @vma: vm_area_struct mapping @address
 - * @address: virtual address to look up
 - * @flags: flags modifying lookup behaviour
 - * @page_mask: on output, *page_mask is set according to the size of the page
 - *
 - * @flags can have FOLL_ flags set, defined in <linux/mm.h>
 - *
 - * Returns the mapped (struct page *), %NULL if no mapping exists, or
 - * an error pointer if there is a mapping to something not represented
 - * by a page descriptor (see also vm_normal_page()).
 - */
 -struct page *follow_page_mask(struct vm_area_struct *vma,
 -			      unsigned long address, unsigned int flags,
 -			      unsigned int *page_mask)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	spinlock_t *ptl;
 -	struct page *page;
 -	struct mm_struct *mm = vma->vm_mm;
 -
 -	*page_mask = 0;
 -
 -	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 -	if (!IS_ERR(page)) {
 -		BUG_ON(flags & FOLL_GET);
  		return page;
 -	}
 -
 -	pgd = pgd_offset(mm, address);
 -	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 -		return no_page_table(vma, flags);
 -
 -	pud = pud_offset(pgd, address);
 -	if (pud_none(*pud))
 -		return no_page_table(vma, flags);
 -	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
 -		page = follow_huge_pud(mm, address, pud, flags);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if (unlikely(pud_bad(*pud)))
 -		return no_page_table(vma, flags);
 -
 -	pmd = pmd_offset(pud, address);
 -	if (pmd_none(*pmd))
 -		return no_page_table(vma, flags);
 -	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
 -		page = follow_huge_pmd(mm, address, pmd, flags);
 -		if (page)
 -			return page;
 -		return no_page_table(vma, flags);
 -	}
 -	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
 -		return no_page_table(vma, flags);
 -	if (pmd_devmap(*pmd)) {
 -		ptl = pmd_lock(mm, pmd);
 -		page = follow_devmap_pmd(vma, address, pmd, flags);
 -		spin_unlock(ptl);
 -		if (page)
 -			return page;
 -	}
 -	if (likely(!pmd_trans_huge(*pmd)))
 -		return follow_page_pte(vma, address, pmd, flags);
 -
 -	ptl = pmd_lock(mm, pmd);
 -	if (unlikely(!pmd_trans_huge(*pmd))) {
 -		spin_unlock(ptl);
 -		return follow_page_pte(vma, address, pmd, flags);
 -	}
 -	if (flags & FOLL_SPLIT) {
 -		int ret;
 -		page = pmd_page(*pmd);
 -		if (is_huge_zero_page(page)) {
 -			spin_unlock(ptl);
 -			ret = 0;
 -			split_huge_pmd(vma, pmd, address);
 -		} else {
 -			get_page(page);
 -			spin_unlock(ptl);
 -			lock_page(page);
 -			ret = split_huge_page(page);
 -			unlock_page(page);
 -			put_page(page);
 -		}
 -
 -		return ret ? ERR_PTR(ret) :
 -			follow_page_pte(vma, address, pmd, flags);
 -	}
 -
 -	page = follow_trans_huge_pmd(vma, address, pmd, flags);
 -	spin_unlock(ptl);
 -	*page_mask = HPAGE_PMD_NR - 1;
 -	return page;
 -}
 -
 -static int get_gate_page(struct mm_struct *mm, unsigned long address,
 -		unsigned int gup_flags, struct vm_area_struct **vma,
 -		struct page **page)
 -{
 -	pgd_t *pgd;
 -	pud_t *pud;
 -	pmd_t *pmd;
 -	pte_t *pte;
 -	int ret = -EFAULT;
 -
 -	/* user gate pages are read-only */
 -	if (gup_flags & FOLL_WRITE)
 -		return -EFAULT;
 -	if (address > TASK_SIZE)
 -		pgd = pgd_offset_k(address);
 -	else
 -		pgd = pgd_offset_gate(mm, address);
 -	BUG_ON(pgd_none(*pgd));
 -	pud = pud_offset(pgd, address);
 -	BUG_ON(pud_none(*pud));
 -	pmd = pmd_offset(pud, address);
 -	if (pmd_none(*pmd))
 -		return -EFAULT;
 -	VM_BUG_ON(pmd_trans_huge(*pmd));
 -	pte = pte_offset_map(pmd, address);
 -	if (pte_none(*pte))
 -		goto unmap;
 -	*vma = get_gate_vma(mm);
 -	if (!page)
 -		goto out;
 -	*page = vm_normal_page(*vma, address, *pte);
 -	if (!*page) {
 -		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
 -			goto unmap;
 -		*page = pte_page(*pte);
 -	}
 -	get_page(*page);
 -out:
 -	ret = 0;
 -unmap:
 -	pte_unmap(pte);
 -	return ret;
 -}
 -
 -/*
 - * mmap_sem must be held on entry.  If @nonblocking != NULL and
 - * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.
 - * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
 - */
 -static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
 -		unsigned long address, unsigned int *flags, int *nonblocking)
 -{
 -	struct mm_struct *mm = vma->vm_mm;
 -	unsigned int fault_flags = 0;
 -	int ret;
 -
 -	/* mlock all present pages, but do not fault in new pages */
 -	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
 -		return -ENOENT;
 -	/* For mm_populate(), just skip the stack guard page. */
 -	if ((*flags & FOLL_POPULATE) &&
 -			(stack_guard_page_start(vma, address) ||
 -			 stack_guard_page_end(vma, address + PAGE_SIZE)))
 -		return -ENOENT;
 -	if (*flags & FOLL_WRITE)
 -		fault_flags |= FAULT_FLAG_WRITE;
 -	if (nonblocking)
 -		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
 -	if (*flags & FOLL_NOWAIT)
 -		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
 -	if (*flags & FOLL_TRIED) {
 -		VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
 -		fault_flags |= FAULT_FLAG_TRIED;
 -	}
 -
 -	ret = handle_mm_fault(mm, vma, address, fault_flags);
 -	if (ret & VM_FAULT_ERROR) {
 -		if (ret & VM_FAULT_OOM)
 -			return -ENOMEM;
 -		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
 -			return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;
 -		if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
 -			return -EFAULT;
 -		BUG();
 -	}
 -
 -	if (tsk) {
 -		if (ret & VM_FAULT_MAJOR)
 -			tsk->maj_flt++;
 -		else
 -			tsk->min_flt++;
 -	}
 -
 -	if (ret & VM_FAULT_RETRY) {
 -		if (nonblocking)
 -			*nonblocking = 0;
 -		return -EBUSY;
 -	}
  
 +no_page_table:
  	/*
 -	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
 -	 * necessary, even if maybe_mkwrite decided not to set pte_write. We
 -	 * can thus safely do subsequent page lookups as if they were reads.
 -	 * But only do so when looping for pte_write is futile: in some cases
 -	 * userspace may also be wanting to write to the gotten user page,
 -	 * which a read fault here might prevent (a readonly page might get
 -	 * reCOWed by userspace write).
 +	 * When core dumping an enormous anonymous area that nobody
 +	 * has touched so far, we don't want to allocate unnecessary pages or
 +	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
 +	 * then get_dump_page() will return NULL to leave a hole in the dump.
 +	 * But we can only make this optimization where a hole would surely
 +	 * be zero-filled if handle_mm_fault() actually did handle it.
  	 */
++<<<<<<< HEAD
 +	if ((flags & FOLL_DUMP) &&
 +	    (!vma->vm_ops || !vma->vm_ops->fault))
 +		return ERR_PTR(-EFAULT);
 +	return page;
++=======
+ 	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
+ 		*flags &= ~FOLL_WRITE;
+ 	return 0;
+ }
+ 
+ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
+ {
+ 	vm_flags_t vm_flags = vma->vm_flags;
+ 
+ 	if (vm_flags & (VM_IO | VM_PFNMAP))
+ 		return -EFAULT;
+ 
+ 	if (gup_flags & FOLL_WRITE) {
+ 		if (!(vm_flags & VM_WRITE)) {
+ 			if (!(gup_flags & FOLL_FORCE))
+ 				return -EFAULT;
+ 			/*
+ 			 * We used to let the write,force case do COW in a
+ 			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
+ 			 * set a breakpoint in a read-only mapping of an
+ 			 * executable, without corrupting the file (yet only
+ 			 * when that file had been opened for writing!).
+ 			 * Anon pages in shared mappings are surprising: now
+ 			 * just reject it.
+ 			 */
+ 			if (!is_cow_mapping(vm_flags))
+ 				return -EFAULT;
+ 		}
+ 	} else if (!(vm_flags & VM_READ)) {
+ 		if (!(gup_flags & FOLL_FORCE))
+ 			return -EFAULT;
+ 		/*
+ 		 * Is there actually any vma we can reach here which does not
+ 		 * have VM_MAYREAD set?
+ 		 */
+ 		if (!(vm_flags & VM_MAYREAD))
+ 			return -EFAULT;
+ 	}
+ 	if (!arch_vma_access_permitted(vma, (gup_flags & FOLL_WRITE)))
+ 		return -EFAULT;
+ 	return 0;
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
  }
  
  /**
@@@ -535,6 -613,24 +589,27 @@@ next_page
  }
  EXPORT_SYMBOL(__get_user_pages);
  
++<<<<<<< HEAD
++=======
+ bool vma_permits_fault(struct vm_area_struct *vma, unsigned int fault_flags)
+ {
+ 	bool write = !!(fault_flags & FAULT_FLAG_WRITE);
+ 	vm_flags_t vm_flags = write ? VM_WRITE : VM_READ;
+ 
+ 	if (!(vm_flags & vma->vm_flags))
+ 		return false;
+ 
+ 	/*
+ 	 * The architecture might have a hardware protection
+ 	 * mechanism other than read/write that can deny access
+ 	 */
+ 	if (!arch_vma_access_permitted(vma, write))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
  /*
   * fixup_user_fault() - manually resolve a user page fault
   * @tsk:	the task_struct to use for page fault accounting, or
@@@ -869,3 -1118,427 +944,430 @@@ struct page *get_dump_page(unsigned lon
  	return page;
  }
  #endif /* CONFIG_ELF_CORE */
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Generic RCU Fast GUP
+  *
+  * get_user_pages_fast attempts to pin user pages by walking the page
+  * tables directly and avoids taking locks. Thus the walker needs to be
+  * protected from page table pages being freed from under it, and should
+  * block any THP splits.
+  *
+  * One way to achieve this is to have the walker disable interrupts, and
+  * rely on IPIs from the TLB flushing code blocking before the page table
+  * pages are freed. This is unsuitable for architectures that do not need
+  * to broadcast an IPI when invalidating TLBs.
+  *
+  * Another way to achieve this is to batch up page table containing pages
+  * belonging to more than one mm_user, then rcu_sched a callback to free those
+  * pages. Disabling interrupts will allow the fast_gup walker to both block
+  * the rcu_sched callback, and an IPI that we broadcast for splitting THPs
+  * (which is a relatively rare event). The code below adopts this strategy.
+  *
+  * Before activating this code, please be aware that the following assumptions
+  * are currently made:
+  *
+  *  *) HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table is used to free
+  *      pages containing page tables.
+  *
+  *  *) ptes can be read atomically by the architecture.
+  *
+  *  *) access_ok is sufficient to validate userspace address ranges.
+  *
+  * The last two assumptions can be relaxed by the addition of helper functions.
+  *
+  * This code is based heavily on the PowerPC implementation by Nick Piggin.
+  */
+ #ifdef CONFIG_HAVE_GENERIC_RCU_GUP
+ 
+ #ifdef __HAVE_ARCH_PTE_SPECIAL
+ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	pte_t *ptep, *ptem;
+ 	int ret = 0;
+ 
+ 	ptem = ptep = pte_offset_map(&pmd, addr);
+ 	do {
+ 		/*
+ 		 * In the line below we are assuming that the pte can be read
+ 		 * atomically. If this is not the case for your architecture,
+ 		 * please wrap this in a helper function!
+ 		 *
+ 		 * for an example see gup_get_pte in arch/x86/mm/gup.c
+ 		 */
+ 		pte_t pte = READ_ONCE(*ptep);
+ 		struct page *head, *page;
+ 
+ 		/*
+ 		 * Similar to the PMD case below, NUMA hinting must take slow
+ 		 * path using the pte_protnone check.
+ 		 */
+ 		if (!pte_present(pte) || pte_special(pte) ||
+ 			pte_protnone(pte) || (write && !pte_write(pte)))
+ 			goto pte_unmap;
+ 
+ 		if (!arch_pte_access_permitted(pte, write))
+ 			goto pte_unmap;
+ 
+ 		VM_BUG_ON(!pfn_valid(pte_pfn(pte)));
+ 		page = pte_page(pte);
+ 		head = compound_head(page);
+ 
+ 		if (!page_cache_get_speculative(head))
+ 			goto pte_unmap;
+ 
+ 		if (unlikely(pte_val(pte) != pte_val(*ptep))) {
+ 			put_page(head);
+ 			goto pte_unmap;
+ 		}
+ 
+ 		VM_BUG_ON_PAGE(compound_head(page) != head, page);
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 
+ 	} while (ptep++, addr += PAGE_SIZE, addr != end);
+ 
+ 	ret = 1;
+ 
+ pte_unmap:
+ 	pte_unmap(ptem);
+ 	return ret;
+ }
+ #else
+ 
+ /*
+  * If we can't determine whether or not a pte is special, then fail immediately
+  * for ptes. Note, we can still pin HugeTLB and THP as these are guaranteed not
+  * to be special.
+  *
+  * For a futex to be placed on a THP tail page, get_futex_key requires a
+  * __get_user_pages_fast implementation that can pin pages. Thus it's still
+  * useful to have gup_huge_pmd even if we can't operate on ptes.
+  */
+ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	return 0;
+ }
+ #endif /* __HAVE_ARCH_PTE_SPECIAL */
+ 
+ static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
+ 		unsigned long end, int write, struct page **pages, int *nr)
+ {
+ 	struct page *head, *page;
+ 	int refs;
+ 
+ 	if (write && !pmd_write(orig))
+ 		return 0;
+ 
+ 	refs = 0;
+ 	head = pmd_page(orig);
+ 	page = head + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		VM_BUG_ON_PAGE(compound_head(page) != head, page);
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	if (!page_cache_add_speculative(head, refs)) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	return 1;
+ }
+ 
+ static int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,
+ 		unsigned long end, int write, struct page **pages, int *nr)
+ {
+ 	struct page *head, *page;
+ 	int refs;
+ 
+ 	if (write && !pud_write(orig))
+ 		return 0;
+ 
+ 	refs = 0;
+ 	head = pud_page(orig);
+ 	page = head + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		VM_BUG_ON_PAGE(compound_head(page) != head, page);
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	if (!page_cache_add_speculative(head, refs)) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pud_val(orig) != pud_val(*pudp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	return 1;
+ }
+ 
+ static int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,
+ 			unsigned long end, int write,
+ 			struct page **pages, int *nr)
+ {
+ 	int refs;
+ 	struct page *head, *page;
+ 
+ 	if (write && !pgd_write(orig))
+ 		return 0;
+ 
+ 	refs = 0;
+ 	head = pgd_page(orig);
+ 	page = head + ((addr & ~PGDIR_MASK) >> PAGE_SHIFT);
+ 	do {
+ 		VM_BUG_ON_PAGE(compound_head(page) != head, page);
+ 		pages[*nr] = page;
+ 		(*nr)++;
+ 		page++;
+ 		refs++;
+ 	} while (addr += PAGE_SIZE, addr != end);
+ 
+ 	if (!page_cache_add_speculative(head, refs)) {
+ 		*nr -= refs;
+ 		return 0;
+ 	}
+ 
+ 	if (unlikely(pgd_val(orig) != pgd_val(*pgdp))) {
+ 		*nr -= refs;
+ 		while (refs--)
+ 			put_page(head);
+ 		return 0;
+ 	}
+ 
+ 	return 1;
+ }
+ 
+ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,
+ 		int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pmd_t *pmdp;
+ 
+ 	pmdp = pmd_offset(&pud, addr);
+ 	do {
+ 		pmd_t pmd = READ_ONCE(*pmdp);
+ 
+ 		next = pmd_addr_end(addr, end);
+ 		if (pmd_none(pmd))
+ 			return 0;
+ 
+ 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
+ 			/*
+ 			 * NUMA hinting faults need to be handled in the GUP
+ 			 * slowpath for accounting purposes and so that they
+ 			 * can be serialised against THP migration.
+ 			 */
+ 			if (pmd_protnone(pmd))
+ 				return 0;
+ 
+ 			if (!gup_huge_pmd(pmd, pmdp, addr, next, write,
+ 				pages, nr))
+ 				return 0;
+ 
+ 		} else if (unlikely(is_hugepd(__hugepd(pmd_val(pmd))))) {
+ 			/*
+ 			 * architecture have different format for hugetlbfs
+ 			 * pmd format and THP pmd format
+ 			 */
+ 			if (!gup_huge_pd(__hugepd(pmd_val(pmd)), addr,
+ 					 PMD_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pte_range(pmd, addr, next, write, pages, nr))
+ 				return 0;
+ 	} while (pmdp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,
+ 			 int write, struct page **pages, int *nr)
+ {
+ 	unsigned long next;
+ 	pud_t *pudp;
+ 
+ 	pudp = pud_offset(&pgd, addr);
+ 	do {
+ 		pud_t pud = READ_ONCE(*pudp);
+ 
+ 		next = pud_addr_end(addr, end);
+ 		if (pud_none(pud))
+ 			return 0;
+ 		if (unlikely(pud_huge(pud))) {
+ 			if (!gup_huge_pud(pud, pudp, addr, next, write,
+ 					  pages, nr))
+ 				return 0;
+ 		} else if (unlikely(is_hugepd(__hugepd(pud_val(pud))))) {
+ 			if (!gup_huge_pd(__hugepd(pud_val(pud)), addr,
+ 					 PUD_SHIFT, next, write, pages, nr))
+ 				return 0;
+ 		} else if (!gup_pmd_range(pud, addr, next, write, pages, nr))
+ 			return 0;
+ 	} while (pudp++, addr = next, addr != end);
+ 
+ 	return 1;
+ }
+ 
+ /*
+  * Like get_user_pages_fast() except it's IRQ-safe in that it won't fall back to
+  * the regular GUP. It will only return non-negative values.
+  */
+ int __get_user_pages_fast(unsigned long start, int nr_pages, int write,
+ 			  struct page **pages)
+ {
+ 	struct mm_struct *mm = current->mm;
+ 	unsigned long addr, len, end;
+ 	unsigned long next, flags;
+ 	pgd_t *pgdp;
+ 	int nr = 0;
+ 
+ 	start &= PAGE_MASK;
+ 	addr = start;
+ 	len = (unsigned long) nr_pages << PAGE_SHIFT;
+ 	end = start + len;
+ 
+ 	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
+ 					start, len)))
+ 		return 0;
+ 
+ 	/*
+ 	 * Disable interrupts.  We use the nested form as we can already have
+ 	 * interrupts disabled by get_futex_key.
+ 	 *
+ 	 * With interrupts disabled, we block page table pages from being
+ 	 * freed from under us. See mmu_gather_tlb in asm-generic/tlb.h
+ 	 * for more details.
+ 	 *
+ 	 * We do not adopt an rcu_read_lock(.) here as we also want to
+ 	 * block IPIs that come from THPs splitting.
+ 	 */
+ 
+ 	local_irq_save(flags);
+ 	pgdp = pgd_offset(mm, addr);
+ 	do {
+ 		pgd_t pgd = READ_ONCE(*pgdp);
+ 
+ 		next = pgd_addr_end(addr, end);
+ 		if (pgd_none(pgd))
+ 			break;
+ 		if (unlikely(pgd_huge(pgd))) {
+ 			if (!gup_huge_pgd(pgd, pgdp, addr, next, write,
+ 					  pages, &nr))
+ 				break;
+ 		} else if (unlikely(is_hugepd(__hugepd(pgd_val(pgd))))) {
+ 			if (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,
+ 					 PGDIR_SHIFT, next, write, pages, &nr))
+ 				break;
+ 		} else if (!gup_pud_range(pgd, addr, next, write, pages, &nr))
+ 			break;
+ 	} while (pgdp++, addr = next, addr != end);
+ 	local_irq_restore(flags);
+ 
+ 	return nr;
+ }
+ 
+ /**
+  * get_user_pages_fast() - pin user pages in memory
+  * @start:	starting user address
+  * @nr_pages:	number of pages from start to pin
+  * @write:	whether pages will be written to
+  * @pages:	array that receives pointers to the pages pinned.
+  *		Should be at least nr_pages long.
+  *
+  * Attempt to pin user pages in memory without taking mm->mmap_sem.
+  * If not successful, it will fall back to taking the lock and
+  * calling get_user_pages().
+  *
+  * Returns number of pages pinned. This may be fewer than the number
+  * requested. If nr_pages is 0 or negative, returns 0. If no pages
+  * were pinned, returns -errno.
+  */
+ int get_user_pages_fast(unsigned long start, int nr_pages, int write,
+ 			struct page **pages)
+ {
+ 	struct mm_struct *mm = current->mm;
+ 	int nr, ret;
+ 
+ 	start &= PAGE_MASK;
+ 	nr = __get_user_pages_fast(start, nr_pages, write, pages);
+ 	ret = nr;
+ 
+ 	if (nr < nr_pages) {
+ 		/* Try to get the remaining pages with get_user_pages */
+ 		start += nr << PAGE_SHIFT;
+ 		pages += nr;
+ 
+ 		ret = get_user_pages_unlocked(current, mm, start,
+ 					      nr_pages - nr, write, 0, pages);
+ 
+ 		/* Have to be a bit careful with return values */
+ 		if (nr > 0) {
+ 			if (ret < 0)
+ 				ret = nr;
+ 			else
+ 				ret += nr;
+ 		}
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ #endif /* CONFIG_HAVE_GENERIC_RCU_GUP */
+ 
+ long get_user_pages8(struct task_struct *tsk, struct mm_struct *mm,
+ 		     unsigned long start, unsigned long nr_pages,
+ 		     int write, int force, struct page **pages,
+ 		     struct vm_area_struct **vmas)
+ {
+ 	WARN_ONCE(tsk != current, "get_user_pages() called on remote task");
+ 	WARN_ONCE(mm != current->mm, "get_user_pages() called on remote mm");
+ 
+ 	return get_user_pages6(start, nr_pages, write, force, pages, vmas);
+ }
+ EXPORT_SYMBOL(get_user_pages8);
+ 
+ long get_user_pages_locked8(struct task_struct *tsk, struct mm_struct *mm,
+ 			    unsigned long start, unsigned long nr_pages,
+ 			    int write, int force, struct page **pages, int *locked)
+ {
+ 	WARN_ONCE(tsk != current, "get_user_pages_locked() called on remote task");
+ 	WARN_ONCE(mm != current->mm, "get_user_pages_locked() called on remote mm");
+ 
+ 	return get_user_pages_locked6(start, nr_pages, write, force, pages, locked);
+ }
+ EXPORT_SYMBOL(get_user_pages_locked8);
+ 
+ long get_user_pages_unlocked7(struct task_struct *tsk, struct mm_struct *mm,
+ 				  unsigned long start, unsigned long nr_pages,
+ 				  int write, int force, struct page **pages)
+ {
+ 	WARN_ONCE(tsk != current, "get_user_pages_unlocked() called on remote task");
+ 	WARN_ONCE(mm != current->mm, "get_user_pages_unlocked() called on remote mm");
+ 
+ 	return get_user_pages_unlocked5(start, nr_pages, write, force, pages);
+ }
+ EXPORT_SYMBOL(get_user_pages_unlocked7);
+ 
++>>>>>>> 33a709b25a76 (mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys)
diff --git a/arch/powerpc/include/asm/mmu_context.h b/arch/powerpc/include/asm/mmu_context.h
index b2294769341b..4da9cb21fadf 100644
--- a/arch/powerpc/include/asm/mmu_context.h
+++ b/arch/powerpc/include/asm/mmu_context.h
@@ -165,5 +165,16 @@ static inline void arch_bprm_mm_init(struct mm_struct *mm,
 {
 }
 
+static inline bool arch_vma_access_permitted(struct vm_area_struct *vma, bool write)
+{
+	/* by default, allow everything */
+	return true;
+}
+
+static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+{
+	/* by default, allow everything */
+	return true;
+}
 #endif /* __KERNEL__ */
 #endif /* __ASM_POWERPC_MMU_CONTEXT_H */
* Unmerged path arch/s390/include/asm/mmu_context.h
* Unmerged path arch/unicore32/include/asm/mmu_context.h
diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
index a34c7d411865..0540c5ae1f04 100644
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@ -153,4 +153,53 @@ static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
 	mpx_notify_unmap(mm, vma, start, end);
 }
 
+static inline bool __pkru_allows_pkey(u16 pkey, bool write)
+{
+	u32 pkru = read_pkru();
+
+	if (!__pkru_allows_read(pkru, pkey))
+		return false;
+	if (write && !__pkru_allows_write(pkru, pkey))
+		return false;
+
+	return true;
+}
+
+/*
+ * We only want to enforce protection keys on the current process
+ * because we effectively have no access to PKRU for other
+ * processes or any way to tell *which * PKRU in a threaded
+ * process we could use.
+ *
+ * So do not enforce things if the VMA is not from the current
+ * mm, or if we are in a kernel thread.
+ */
+static inline bool vma_is_foreign(struct vm_area_struct *vma)
+{
+	if (!current->mm)
+		return true;
+	/*
+	 * Should PKRU be enforced on the access to this VMA?  If
+	 * the VMA is from another process, then PKRU has no
+	 * relevance and should not be enforced.
+	 */
+	if (current->mm != vma->vm_mm)
+		return true;
+
+	return false;
+}
+
+static inline bool arch_vma_access_permitted(struct vm_area_struct *vma, bool write)
+{
+	/* allow access if the VMA is not one from this process */
+	if (vma_is_foreign(vma))
+		return true;
+	return __pkru_allows_pkey(vma_pkey(vma), write);
+}
+
+static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+{
+	return __pkru_allows_pkey(pte_flags_pkey(pte_flags(pte)), write);
+}
+
 #endif /* _ASM_X86_MMU_CONTEXT_H */
* Unmerged path arch/x86/include/asm/pgtable.h
* Unmerged path arch/x86/mm/fault.c
diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c
index 0f64b85f7bb2..176f51c0c27e 100644
--- a/arch/x86/mm/gup.c
+++ b/arch/x86/mm/gup.c
@@ -11,6 +11,7 @@
 #include <linux/swap.h>
 #include <linux/memremap.h>
 
+#include <asm/mmu_context.h>
 #include <asm/pgtable.h>
 
 static inline pte_t gup_get_pte(pte_t *ptep)
@@ -89,6 +90,10 @@ static inline int pte_allows_gup(unsigned long pteval, int write)
 	if ((pteval & need_pte_bits) != need_pte_bits)
 		return 0;
 
+	/* Check memory protection keys permissions. */
+	if (!__pkru_allows_pkey(pte_flags_pkey(pteval), write))
+		return 0;
+
 	return 1;
 }
 
diff --git a/include/asm-generic/mm_hooks.h b/include/asm-generic/mm_hooks.h
index 866aa461efa5..c1fc5af3c384 100644
--- a/include/asm-generic/mm_hooks.h
+++ b/include/asm-generic/mm_hooks.h
@@ -26,4 +26,15 @@ static inline void arch_bprm_mm_init(struct mm_struct *mm,
 {
 }
 
+static inline bool arch_vma_access_permitted(struct vm_area_struct *vma, bool write)
+{
+	/* by default, allow everything */
+	return true;
+}
+
+static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+{
+	/* by default, allow everything */
+	return true;
+}
 #endif	/* _ASM_GENERIC_MM_HOOKS_H */
* Unmerged path mm/gup.c
diff --git a/mm/memory.c b/mm/memory.c
index 2fc5b28b6782..c93f2b500223 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -65,6 +65,7 @@
 #include <linux/dax.h>
 
 #include <asm/io.h>
+#include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
 #include <asm/tlb.h>
@@ -3272,6 +3273,9 @@ static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	pmd_t *pmd;
 	pte_t *pte;
 
+	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE))
+		return VM_FAULT_SIGSEGV;
+
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		return hugetlb_fault(mm, vma, address, flags);
 
