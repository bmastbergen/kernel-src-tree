mm: move handling of COW faults into DAX code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] move handling of COW faults into DAX code (Larry Woodman) [1457569 1383493 1457572]
Rebuild_FUZZ: 95.35%
commit-author Jan Kara <jack@suse.cz>
commit b1aa812b21084285e9f6098639be9cd5bf9e05d7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b1aa812b.failed

Move final handling of COW faults from generic code into DAX fault
handler.  That way generic code doesn't have to be aware of
peculiarities of DAX locking so remove that knowledge and make locking
functions private to fs/dax.c.

Link: http://lkml.kernel.org/r/1479460644-25076-11-git-send-email-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b1aa812b21084285e9f6098639be9cd5bf9e05d7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
#	include/linux/mm.h
#	mm/memory.c
diff --cc fs/dax.c
index 1dfecdfb6245,e83aa4077df4..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -581,11 -500,9 +582,15 @@@ static int dax_load_hole(struct address
  
  	/* This will replace locked radix tree entry with a hole page */
  	page = find_or_create_page(mapping, vmf->pgoff,
++<<<<<<< HEAD
 +		mapping_gfp_mask(mapping) | __GFP_FS | __GFP_IO | __GFP_ZERO);
 +	if (!page) {
 +		put_locked_mapping_entry(mapping, vmf->pgoff, entry);
++=======
+ 				   vmf->gfp_mask | __GFP_ZERO);
+ 	if (!page)
++>>>>>>> b1aa812b2108 (mm: move handling of COW faults into DAX code)
  		return VM_FAULT_OOM;
- 	}
  	vmf->page = page;
  	return VM_FAULT_LOCKED;
  }
@@@ -988,56 -817,438 +993,381 @@@ int __dax_zero_page_range(struct block_
  }
  EXPORT_SYMBOL_GPL(__dax_zero_page_range);
  
 -#ifdef CONFIG_FS_IOMAP
 -static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
 -{
 -	return iomap->blkno + (((pos & PAGE_MASK) - iomap->offset) >> 9);
 -}
 -
 -static loff_t
 -dax_iomap_actor(struct inode *inode, loff_t pos, loff_t length, void *data,
 -		struct iomap *iomap)
 +/**
 + * dax_zero_page_range - zero a range within a page of a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @length: The number of bytes to zero
 + * @get_block: The filesystem method used to translate file offsets to blocks
 + *
 + * This function can be called by a filesystem when it is zeroing part of a
 + * page in a DAX file.  This is intended for hole-punch operations.  If
 + * you are truncating a file, the helper function dax_truncate_page() may be
 + * more convenient.
 + */
 +int dax_zero_page_range(struct inode *inode, loff_t from, unsigned length,
 +							get_block_t get_block)
  {
 -	struct iov_iter *iter = data;
 -	loff_t end = pos + length, done = 0;
 -	ssize_t ret = 0;
 -
 -	if (iov_iter_rw(iter) == READ) {
 -		end = min(end, i_size_read(inode));
 -		if (pos >= end)
 -			return 0;
 +	struct buffer_head bh;
 +	pgoff_t index = from >> PAGE_CACHE_SHIFT;
 +	unsigned offset = from & (PAGE_CACHE_SIZE-1);
 +	int err;
  
 -		if (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)
 -			return iov_iter_zero(min(length, end - pos), iter);
 -	}
 -
 -	if (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED))
 -		return -EIO;
 -
 -	while (pos < end) {
 -		unsigned offset = pos & (PAGE_SIZE - 1);
 -		struct blk_dax_ctl dax = { 0 };
 -		ssize_t map_len;
 -
 -		dax.sector = dax_iomap_sector(iomap, pos);
 -		dax.size = (length + offset + PAGE_SIZE - 1) & PAGE_MASK;
 -		map_len = dax_map_atomic(iomap->bdev, &dax);
 -		if (map_len < 0) {
 -			ret = map_len;
 -			break;
 -		}
 -
 -		dax.addr += offset;
 -		map_len -= offset;
 -		if (map_len > end - pos)
 -			map_len = end - pos;
 -
 -		if (iov_iter_rw(iter) == WRITE)
 -			map_len = copy_from_iter_pmem(dax.addr, map_len, iter);
 -		else
 -			map_len = copy_to_iter(dax.addr, map_len, iter);
 -		dax_unmap_atomic(iomap->bdev, &dax);
 -		if (map_len <= 0) {
 -			ret = map_len ? map_len : -EFAULT;
 -			break;
 -		}
 -
 -		pos += map_len;
 -		length -= map_len;
 -		done += map_len;
 -	}
 -
 -	return done ? done : ret;
 +	/* Block boundary? Nothing to do */
 +	if (!length)
 +		return 0;
 +	if (WARN_ON_ONCE((offset + length) > PAGE_CACHE_SIZE))
 +		return -EINVAL;
 +
 +	memset(&bh, 0, sizeof(bh));
 +	bh.b_bdev = inode->i_sb->s_bdev;
 +	bh.b_size = PAGE_CACHE_SIZE;
 +	err = get_block(inode, index, &bh, 0);
 +	if (err < 0 || !buffer_written(&bh))
 +		return err;
 +
 +	return __dax_zero_page_range(bh.b_bdev, to_sector(&bh, inode),
 +			offset, length);
  }
 +EXPORT_SYMBOL_GPL(dax_zero_page_range);
  
  /**
 - * dax_iomap_rw - Perform I/O to a DAX file
 - * @iocb:	The control block for this I/O
 - * @iter:	The addresses to do I/O from or to
 - * @ops:	iomap ops passed from the file system
 + * dax_truncate_page - handle a partial page being truncated in a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @get_block: The filesystem method used to translate file offsets to blocks
   *
 - * This function performs read and write operations to directly mapped
 - * persistent memory.  The callers needs to take care of read/write exclusion
 - * and evicting any page cache pages in the region under I/O.
 + * Similar to block_truncate_page(), this function can be called by a
 + * filesystem when it is truncating a DAX file to handle the partial page.
   */
 -ssize_t
 -dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
 -		struct iomap_ops *ops)
 +int dax_truncate_page(struct inode *inode, loff_t from, get_block_t get_block)
  {
 -	struct address_space *mapping = iocb->ki_filp->f_mapping;
 -	struct inode *inode = mapping->host;
 -	loff_t pos = iocb->ki_pos, ret = 0, done = 0;
 -	unsigned flags = 0;
 -
 -	if (iov_iter_rw(iter) == WRITE)
 -		flags |= IOMAP_WRITE;
 -
 -	/*
 -	 * Yes, even DAX files can have page cache attached to them:  A zeroed
 -	 * page is inserted into the pagecache when we have to serve a write
 -	 * fault on a hole.  It should never be dirtied and can simply be
 -	 * dropped from the pagecache once we get real data for the page.
 -	 *
 -	 * XXX: This is racy against mmap, and there's nothing we can do about
 -	 * it. We'll eventually need to shift this down even further so that
 -	 * we can check if we allocated blocks over a hole first.
 -	 */
 -	if (mapping->nrpages) {
 -		ret = invalidate_inode_pages2_range(mapping,
 -				pos >> PAGE_SHIFT,
 -				(pos + iov_iter_count(iter) - 1) >> PAGE_SHIFT);
 -		WARN_ON_ONCE(ret);
 -	}
 -
 -	while (iov_iter_count(iter)) {
 -		ret = iomap_apply(inode, pos, iov_iter_count(iter), flags, ops,
 -				iter, dax_iomap_actor);
 -		if (ret <= 0)
 -			break;
 -		pos += ret;
 -		done += ret;
 -	}
 -
 -	iocb->ki_pos += done;
 -	return done ? done : ret;
 +	unsigned length = PAGE_CACHE_ALIGN(from) - from;
 +	return dax_zero_page_range(inode, from, length, get_block);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(dax_truncate_page);
++=======
+ EXPORT_SYMBOL_GPL(dax_iomap_rw);
+ 
+ /**
+  * dax_iomap_fault - handle a page fault on a DAX file
+  * @vma: The virtual memory area where the fault occurred
+  * @vmf: The description of the fault
+  * @ops: iomap ops passed from the file system
+  *
+  * When a page fault occurs, filesystems may call this helper in their fault
+  * or mkwrite handler for DAX files. Assumes the caller has done all the
+  * necessary locking for the page fault to proceed successfully.
+  */
+ int dax_iomap_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
+ 			struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	unsigned long vaddr = vmf->address;
+ 	loff_t pos = (loff_t)vmf->pgoff << PAGE_SHIFT;
+ 	sector_t sector;
+ 	struct iomap iomap = { 0 };
+ 	unsigned flags = IOMAP_FAULT;
+ 	int error, major = 0;
+ 	int vmf_ret = 0;
+ 	void *entry;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is supposed
+ 	 * to hold locks serializing us with truncate / punch hole so this is
+ 	 * a reliable test.
+ 	 */
+ 	if (pos >= i_size_read(inode))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	entry = grab_mapping_entry(mapping, vmf->pgoff, 0);
+ 	if (IS_ERR(entry)) {
+ 		error = PTR_ERR(entry);
+ 		goto out;
+ 	}
+ 
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Note that we don't bother to use iomap_apply here: DAX required
+ 	 * the file system block size to be equal the page size, which means
+ 	 * that we never have to deal with more than a single extent here.
+ 	 */
+ 	error = ops->iomap_begin(inode, pos, PAGE_SIZE, flags, &iomap);
+ 	if (error)
+ 		goto unlock_entry;
+ 	if (WARN_ON_ONCE(iomap.offset + iomap.length < pos + PAGE_SIZE)) {
+ 		error = -EIO;		/* fs corruption? */
+ 		goto finish_iomap;
+ 	}
+ 
+ 	sector = dax_iomap_sector(&iomap, pos);
+ 
+ 	if (vmf->cow_page) {
+ 		switch (iomap.type) {
+ 		case IOMAP_HOLE:
+ 		case IOMAP_UNWRITTEN:
+ 			clear_user_highpage(vmf->cow_page, vaddr);
+ 			break;
+ 		case IOMAP_MAPPED:
+ 			error = copy_user_dax(iomap.bdev, sector, PAGE_SIZE,
+ 					vmf->cow_page, vaddr);
+ 			break;
+ 		default:
+ 			WARN_ON_ONCE(1);
+ 			error = -EIO;
+ 			break;
+ 		}
+ 
+ 		if (error)
+ 			goto finish_iomap;
+ 
+ 		__SetPageUptodate(vmf->cow_page);
+ 		vmf_ret = finish_fault(vmf);
+ 		if (!vmf_ret)
+ 			vmf_ret = VM_FAULT_DONE_COW;
+ 		goto finish_iomap;
+ 	}
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		if (iomap.flags & IOMAP_F_NEW) {
+ 			count_vm_event(PGMAJFAULT);
+ 			mem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);
+ 			major = VM_FAULT_MAJOR;
+ 		}
+ 		error = dax_insert_mapping(mapping, iomap.bdev, sector,
+ 				PAGE_SIZE, &entry, vma, vmf);
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (!(vmf->flags & FAULT_FLAG_WRITE)) {
+ 			vmf_ret = dax_load_hole(mapping, entry, vmf);
+ 			break;
+ 		}
+ 		/*FALLTHRU*/
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		error = -EIO;
+ 		break;
+ 	}
+ 
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		if (error || (vmf_ret & VM_FAULT_ERROR)) {
+ 			/* keep previous error */
+ 			ops->iomap_end(inode, pos, PAGE_SIZE, 0, flags,
+ 					&iomap);
+ 		} else {
+ 			error = ops->iomap_end(inode, pos, PAGE_SIZE,
+ 					PAGE_SIZE, flags, &iomap);
+ 		}
+ 	}
+  unlock_entry:
+ 	if (vmf_ret != VM_FAULT_LOCKED || error)
+ 		put_locked_mapping_entry(mapping, vmf->pgoff, entry);
+  out:
+ 	if (error == -ENOMEM)
+ 		return VM_FAULT_OOM | major;
+ 	/* -EBUSY is fine, somebody else faulted on the same PTE */
+ 	if (error < 0 && error != -EBUSY)
+ 		return VM_FAULT_SIGBUS | major;
+ 	if (vmf_ret) {
+ 		WARN_ON_ONCE(error); /* -EBUSY from ops->iomap_end? */
+ 		return vmf_ret;
+ 	}
+ 	return VM_FAULT_NOPAGE | major;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_fault);
+ 
+ #ifdef CONFIG_FS_DAX_PMD
+ /*
+  * The 'colour' (ie low bits) within a PMD of a page offset.  This comes up
+  * more often than one might expect in the below functions.
+  */
+ #define PG_PMD_COLOUR	((PMD_SIZE >> PAGE_SHIFT) - 1)
+ 
+ static int dax_pmd_insert_mapping(struct vm_area_struct *vma, pmd_t *pmd,
+ 		struct vm_fault *vmf, unsigned long address,
+ 		struct iomap *iomap, loff_t pos, bool write, void **entryp)
+ {
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	struct block_device *bdev = iomap->bdev;
+ 	struct blk_dax_ctl dax = {
+ 		.sector = dax_iomap_sector(iomap, pos),
+ 		.size = PMD_SIZE,
+ 	};
+ 	long length = dax_map_atomic(bdev, &dax);
+ 	void *ret;
+ 
+ 	if (length < 0) /* dax_map_atomic() failed */
+ 		return VM_FAULT_FALLBACK;
+ 	if (length < PMD_SIZE)
+ 		goto unmap_fallback;
+ 	if (pfn_t_to_pfn(dax.pfn) & PG_PMD_COLOUR)
+ 		goto unmap_fallback;
+ 	if (!pfn_t_devmap(dax.pfn))
+ 		goto unmap_fallback;
+ 
+ 	dax_unmap_atomic(bdev, &dax);
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, dax.sector,
+ 			RADIX_DAX_PMD);
+ 	if (IS_ERR(ret))
+ 		return VM_FAULT_FALLBACK;
+ 	*entryp = ret;
+ 
+ 	return vmf_insert_pfn_pmd(vma, address, pmd, dax.pfn, write);
+ 
+  unmap_fallback:
+ 	dax_unmap_atomic(bdev, &dax);
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int dax_pmd_load_hole(struct vm_area_struct *vma, pmd_t *pmd,
+ 		struct vm_fault *vmf, unsigned long address,
+ 		struct iomap *iomap, void **entryp)
+ {
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = address & PMD_MASK;
+ 	struct page *zero_page;
+ 	spinlock_t *ptl;
+ 	pmd_t pmd_entry;
+ 	void *ret;
+ 
+ 	zero_page = mm_get_huge_zero_page(vma->vm_mm);
+ 
+ 	if (unlikely(!zero_page))
+ 		return VM_FAULT_FALLBACK;
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, 0,
+ 			RADIX_DAX_PMD | RADIX_DAX_HZP);
+ 	if (IS_ERR(ret))
+ 		return VM_FAULT_FALLBACK;
+ 	*entryp = ret;
+ 
+ 	ptl = pmd_lock(vma->vm_mm, pmd);
+ 	if (!pmd_none(*pmd)) {
+ 		spin_unlock(ptl);
+ 		return VM_FAULT_FALLBACK;
+ 	}
+ 
+ 	pmd_entry = mk_pmd(zero_page, vma->vm_page_prot);
+ 	pmd_entry = pmd_mkhuge(pmd_entry);
+ 	set_pmd_at(vma->vm_mm, pmd_addr, pmd, pmd_entry);
+ 	spin_unlock(ptl);
+ 	return VM_FAULT_NOPAGE;
+ }
+ 
+ int dax_iomap_pmd_fault(struct vm_area_struct *vma, unsigned long address,
+ 		pmd_t *pmd, unsigned int flags, struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = address & PMD_MASK;
+ 	bool write = flags & FAULT_FLAG_WRITE;
+ 	unsigned int iomap_flags = (write ? IOMAP_WRITE : 0) | IOMAP_FAULT;
+ 	struct inode *inode = mapping->host;
+ 	int result = VM_FAULT_FALLBACK;
+ 	struct iomap iomap = { 0 };
+ 	pgoff_t max_pgoff, pgoff;
+ 	struct vm_fault vmf;
+ 	void *entry;
+ 	loff_t pos;
+ 	int error;
+ 
+ 	/* Fall back to PTEs if we're going to COW */
+ 	if (write && !(vma->vm_flags & VM_SHARED))
+ 		goto fallback;
+ 
+ 	/* If the PMD would extend outside the VMA */
+ 	if (pmd_addr < vma->vm_start)
+ 		goto fallback;
+ 	if ((pmd_addr + PMD_SIZE) > vma->vm_end)
+ 		goto fallback;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is
+ 	 * supposed to hold locks serializing us with truncate / punch hole so
+ 	 * this is a reliable test.
+ 	 */
+ 	pgoff = linear_page_index(vma, pmd_addr);
+ 	max_pgoff = (i_size_read(inode) - 1) >> PAGE_SHIFT;
+ 
+ 	if (pgoff > max_pgoff)
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	/* If the PMD would extend beyond the file size */
+ 	if ((pgoff | PG_PMD_COLOUR) > max_pgoff)
+ 		goto fallback;
+ 
+ 	/*
+ 	 * grab_mapping_entry() will make sure we get a 2M empty entry, a DAX
+ 	 * PMD or a HZP entry.  If it can't (because a 4k page is already in
+ 	 * the tree, for instance), it will return -EEXIST and we just fall
+ 	 * back to 4k entries.
+ 	 */
+ 	entry = grab_mapping_entry(mapping, pgoff, RADIX_DAX_PMD);
+ 	if (IS_ERR(entry))
+ 		goto fallback;
+ 
+ 	/*
+ 	 * Note that we don't use iomap_apply here.  We aren't doing I/O, only
+ 	 * setting up a mapping, so really we're using iomap_begin() as a way
+ 	 * to look up our filesystem block.
+ 	 */
+ 	pos = (loff_t)pgoff << PAGE_SHIFT;
+ 	error = ops->iomap_begin(inode, pos, PMD_SIZE, iomap_flags, &iomap);
+ 	if (error)
+ 		goto unlock_entry;
+ 	if (iomap.offset + iomap.length < pos + PMD_SIZE)
+ 		goto finish_iomap;
+ 
+ 	vmf.pgoff = pgoff;
+ 	vmf.flags = flags;
+ 	vmf.gfp_mask = mapping_gfp_mask(mapping) | __GFP_IO;
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		result = dax_pmd_insert_mapping(vma, pmd, &vmf, address,
+ 				&iomap, pos, write, &entry);
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (WARN_ON_ONCE(write))
+ 			goto finish_iomap;
+ 		result = dax_pmd_load_hole(vma, pmd, &vmf, address, &iomap,
+ 				&entry);
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		break;
+ 	}
+ 
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		if (result == VM_FAULT_FALLBACK) {
+ 			ops->iomap_end(inode, pos, PMD_SIZE, 0, iomap_flags,
+ 					&iomap);
+ 		} else {
+ 			error = ops->iomap_end(inode, pos, PMD_SIZE, PMD_SIZE,
+ 					iomap_flags, &iomap);
+ 			if (error)
+ 				result = VM_FAULT_FALLBACK;
+ 		}
+ 	}
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, pgoff, entry);
+  fallback:
+ 	if (result == VM_FAULT_FALLBACK) {
+ 		split_huge_pmd(vma, pmd, address);
+ 		count_vm_event(THP_FAULT_FALLBACK);
+ 	}
+ 	return result;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_pmd_fault);
+ #endif /* CONFIG_FS_DAX_PMD */
+ #endif /* CONFIG_FS_IOMAP */
++>>>>>>> b1aa812b2108 (mm: move handling of COW faults into DAX code)
diff --cc include/linux/mm.h
index a0514d1e5d91,59a4da1742e5..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -244,12 -308,21 +244,30 @@@ struct vm_fault 
  					 * is set (which is also implied by
  					 * VM_FAULT_ERROR).
  					 */
++<<<<<<< HEAD
 +	RH_KABI_EXTEND(struct page *cow_page)	/* Handler may choose to COW */
 +	RH_KABI_EXTEND(void *entry)	/* ->fault handler can alternatively
 +					 * return locked DAX entry. In that
 +					 * case handler should return
 +					 * VM_FAULT_DAX_LOCKED and fill in
 +					 * entry here.
++=======
+ 	/* These three entries are valid only while holding ptl lock */
+ 	pte_t *pte;			/* Pointer to pte entry matching
+ 					 * the 'address'. NULL if the page
+ 					 * table hasn't been allocated.
+ 					 */
+ 	spinlock_t *ptl;		/* Page table lock.
+ 					 * Protects pte page table if 'pte'
+ 					 * is not NULL, otherwise pmd.
+ 					 */
+ 	pgtable_t prealloc_pte;		/* Pre-allocated pte page table.
+ 					 * vm_ops->map_pages() calls
+ 					 * alloc_set_pte() from atomic context.
+ 					 * do_fault_around() pre-allocates
+ 					 * page table to avoid allocation from
+ 					 * atomic context.
++>>>>>>> b1aa812b2108 (mm: move handling of COW faults into DAX code)
  					 */
  };
  
@@@ -1116,7 -1098,7 +1134,11 @@@ static inline void clear_page_pfmemallo
  #define VM_FAULT_LOCKED	0x0200	/* ->fault locked the returned page */
  #define VM_FAULT_RETRY	0x0400	/* ->fault blocked, must retry */
  #define VM_FAULT_FALLBACK 0x0800	/* huge page fault failed, fall back to small */
++<<<<<<< HEAD
 +#define VM_FAULT_DAX_LOCKED 0x1000	/* ->fault has locked DAX entry */
++=======
+ #define VM_FAULT_DONE_COW   0x1000	/* ->fault has fully handled COW */
++>>>>>>> b1aa812b2108 (mm: move handling of COW faults into DAX code)
  
  #define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */
  
diff --cc mm/memory.c
index 2fc5b28b6782,ca3b95fa5fd1..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2821,32 -2833,26 +2821,39 @@@ oom
  	return VM_FAULT_OOM;
  }
  
 -/*
 - * The mmap_sem must have been held on entry, and may have been
 - * released depending on flags and vma->vm_ops->fault() return value.
 - * See filemap_fault() and __lock_page_retry().
 - */
 -static int __do_fault(struct vm_fault *vmf)
 +static int __do_fault(struct vm_area_struct *vma, unsigned long address,
 +			pgoff_t pgoff, unsigned int flags,
 +			struct page *cow_page, struct page **page,
 +			void **entry)
  {
 -	struct vm_area_struct *vma = vmf->vma;
 +	struct vm_fault vmf;
  	int ret;
  
++<<<<<<< HEAD
 +	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
 +	vmf.pgoff = pgoff;
 +	vmf.flags = flags;
 +	vmf.page = NULL;
 +	vmf.cow_page = cow_page;
++=======
+ 	ret = vma->vm_ops->fault(vma, vmf);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY |
+ 			    VM_FAULT_DONE_COW)))
+ 		return ret;
++>>>>>>> b1aa812b2108 (mm: move handling of COW faults into DAX code)
 +
 +	ret = vma->vm_ops->fault(vma, &vmf);
 +	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 +		return ret;
 +	if (ret & VM_FAULT_DAX_LOCKED) {
 +		*entry = vmf.entry;
 +		return ret;
 +	}
  
 -	if (unlikely(PageHWPoison(vmf->page))) {
 +	if (unlikely(PageHWPoison(vmf.page))) {
  		if (ret & VM_FAULT_LOCKED)
 -			unlock_page(vmf->page);
 -		put_page(vmf->page);
 -		vmf->page = NULL;
 +			unlock_page(vmf.page);
 +		page_cache_release(vmf.page);
  		return VM_FAULT_HWPOISON;
  	}
  
@@@ -2931,39 -3270,24 +2938,49 @@@ static int do_cow_fault(struct mm_struc
  		return VM_FAULT_OOM;
  	}
  
 -	ret = __do_fault(vmf);
 +	ret = __do_fault(vma, address, pgoff, flags, new_page, &fault_page,
 +			 &fault_entry);
  	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
  		goto uncharge_out;
 -	if (ret & VM_FAULT_DONE_COW)
 -		return ret;
  
++<<<<<<< HEAD
 +	if (!(ret & VM_FAULT_DAX_LOCKED))
 +		copy_user_highpage(new_page, fault_page, address, vma);
 +	__SetPageUptodate(new_page);
 +
 +	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 +	if (unlikely(!pte_same(*pte, orig_pte))) {
 +		pte_unmap_unlock(pte, ptl);
 +		if (!(ret & VM_FAULT_DAX_LOCKED)) {
 +			unlock_page(fault_page);
 +			page_cache_release(fault_page);
 +		} else {
 +			dax_unlock_mapping_entry(vma->vm_file->f_mapping,
 +						 pgoff);
 +		}
++=======
+ 	copy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);
+ 	__SetPageUptodate(vmf->cow_page);
+ 
+ 	ret |= finish_fault(vmf);
+ 	unlock_page(vmf->page);
+ 	put_page(vmf->page);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
++>>>>>>> b1aa812b2108 (mm: move handling of COW faults into DAX code)
  		goto uncharge_out;
 +	}
 +	do_set_pte(vma, address, new_page, pte, true, true);
 +	pte_unmap_unlock(pte, ptl);
 +	if (!(ret & VM_FAULT_DAX_LOCKED)) {
 +		unlock_page(fault_page);
 +		page_cache_release(fault_page);
 +	} else {
 +		dax_unlock_mapping_entry(vma->vm_file->f_mapping, pgoff);
 +	}
  	return ret;
  uncharge_out:
 -	mem_cgroup_cancel_charge(vmf->cow_page, vmf->memcg, false);
 -	put_page(vmf->cow_page);
 +	mem_cgroup_uncharge_page(new_page);
 +	page_cache_release(new_page);
  	return ret;
  }
  
* Unmerged path fs/dax.c
diff --git a/include/linux/dax.h b/include/linux/dax.h
index 8937c7aed5cb..65d484daa341 100644
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@ -34,7 +34,6 @@ void dax_wake_mapping_entry_waiter(struct address_space *mapping,
 
 #ifdef CONFIG_FS_DAX
 struct page *read_dax_sector(struct block_device *bdev, sector_t n);
-void dax_unlock_mapping_entry(struct address_space *mapping, pgoff_t index);
 int __dax_zero_page_range(struct block_device *bdev, sector_t sector,
 		unsigned int offset, unsigned int length);
 #else
@@ -43,12 +42,6 @@ static inline struct page *read_dax_sector(struct block_device *bdev,
 {
 	return ERR_PTR(-ENXIO);
 }
-/* Shouldn't ever be called when dax is disabled. */
-static inline void dax_unlock_mapping_entry(struct address_space *mapping,
-					    pgoff_t index)
-{
-	BUG();
-}
 static inline int __dax_zero_page_range(struct block_device *bdev,
 		sector_t sector, unsigned int offset, unsigned int length)
 {
* Unmerged path include/linux/mm.h
* Unmerged path mm/memory.c
