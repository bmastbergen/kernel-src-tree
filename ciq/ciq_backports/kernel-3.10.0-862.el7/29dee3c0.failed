locking/refcounts: Out-of-line everything

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit 29dee3c03abce04cd527878ef5f9e5f91b7b83f4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/29dee3c0.failed

Linus asked to please make this real C code.

And since size then isn't an issue what so ever anymore, remove the
debug knob and make all WARN()s unconditional.

	Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: dwindsor@gmail.com
	Cc: elena.reshetova@intel.com
	Cc: gregkh@linuxfoundation.org
	Cc: ishkamiel@gmail.com
	Cc: keescook@chromium.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 29dee3c03abce04cd527878ef5f9e5f91b7b83f4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/Kconfig.debug
#	lib/Makefile
#	tools/include/linux/refcount.h
diff --cc lib/Kconfig.debug
index 87b12a011765,0dbce99d8433..000000000000
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@@ -539,6 -578,408 +539,411 @@@ config DEBUG_KMEMLEAK_DEFAULT_OF
  	  Say Y here to disable kmemleak by default. It can then be enabled
  	  on the command line via kmemleak=on.
  
++<<<<<<< HEAD
++=======
+ config DEBUG_STACK_USAGE
+ 	bool "Stack utilization instrumentation"
+ 	depends on DEBUG_KERNEL && !IA64
+ 	help
+ 	  Enables the display of the minimum amount of free stack which each
+ 	  task has ever had available in the sysrq-T and sysrq-P debug output.
+ 
+ 	  This option will slow down process creation somewhat.
+ 
+ config DEBUG_VM
+ 	bool "Debug VM"
+ 	depends on DEBUG_KERNEL
+ 	help
+ 	  Enable this to turn on extended checks in the virtual-memory system
+           that may impact performance.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VM_VMACACHE
+ 	bool "Debug VMA caching"
+ 	depends on DEBUG_VM
+ 	help
+ 	  Enable this to turn on VMA caching debug information. Doing so
+ 	  can cause significant overhead, so only enable it in non-production
+ 	  environments.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VM_RB
+ 	bool "Debug VM red-black trees"
+ 	depends on DEBUG_VM
+ 	help
+ 	  Enable VM red-black tree debugging information and extra validations.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VM_PGFLAGS
+ 	bool "Debug page-flags operations"
+ 	depends on DEBUG_VM
+ 	help
+ 	  Enables extra validation on page flags operations.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VIRTUAL
+ 	bool "Debug VM translations"
+ 	depends on DEBUG_KERNEL && X86
+ 	help
+ 	  Enable some costly sanity checks in virtual to page code. This can
+ 	  catch mistakes with virt_to_page() and friends.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_NOMMU_REGIONS
+ 	bool "Debug the global anon/private NOMMU mapping region tree"
+ 	depends on DEBUG_KERNEL && !MMU
+ 	help
+ 	  This option causes the global tree of anonymous and private mapping
+ 	  regions to be regularly checked for invalid topology.
+ 
+ config DEBUG_MEMORY_INIT
+ 	bool "Debug memory initialisation" if EXPERT
+ 	default !EXPERT
+ 	help
+ 	  Enable this for additional checks during memory initialisation.
+ 	  The sanity checks verify aspects of the VM such as the memory model
+ 	  and other information provided by the architecture. Verbose
+ 	  information will be printed at KERN_DEBUG loglevel depending
+ 	  on the mminit_loglevel= command-line option.
+ 
+ 	  If unsure, say Y
+ 
+ config MEMORY_NOTIFIER_ERROR_INJECT
+ 	tristate "Memory hotplug notifier error injection module"
+ 	depends on MEMORY_HOTPLUG_SPARSE && NOTIFIER_ERROR_INJECTION
+ 	help
+ 	  This option provides the ability to inject artificial errors to
+ 	  memory hotplug notifier chain callbacks.  It is controlled through
+ 	  debugfs interface under /sys/kernel/debug/notifier-error-inject/memory
+ 
+ 	  If the notifier call chain should be failed with some events
+ 	  notified, write the error code to "actions/<notifier event>/error".
+ 
+ 	  Example: Inject memory hotplug offline error (-12 == -ENOMEM)
+ 
+ 	  # cd /sys/kernel/debug/notifier-error-inject/memory
+ 	  # echo -12 > actions/MEM_GOING_OFFLINE/error
+ 	  # echo offline > /sys/devices/system/memory/memoryXXX/state
+ 	  bash: echo: write error: Cannot allocate memory
+ 
+ 	  To compile this code as a module, choose M here: the module will
+ 	  be called memory-notifier-error-inject.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_PER_CPU_MAPS
+ 	bool "Debug access to per_cpu maps"
+ 	depends on DEBUG_KERNEL
+ 	depends on SMP
+ 	help
+ 	  Say Y to verify that the per_cpu map being accessed has
+ 	  been set up. This adds a fair amount of code to kernel memory
+ 	  and decreases performance.
+ 
+ 	  Say N if unsure.
+ 
+ config DEBUG_HIGHMEM
+ 	bool "Highmem debugging"
+ 	depends on DEBUG_KERNEL && HIGHMEM
+ 	help
+ 	  This option enables additional error checking for high memory
+ 	  systems.  Disable for production systems.
+ 
+ config HAVE_DEBUG_STACKOVERFLOW
+ 	bool
+ 
+ config DEBUG_STACKOVERFLOW
+ 	bool "Check for stack overflows"
+ 	depends on DEBUG_KERNEL && HAVE_DEBUG_STACKOVERFLOW
+ 	---help---
+ 	  Say Y here if you want to check for overflows of kernel, IRQ
+ 	  and exception stacks (if your architecture uses them). This
+ 	  option will show detailed messages if free stack space drops
+ 	  below a certain limit.
+ 
+ 	  These kinds of bugs usually occur when call-chains in the
+ 	  kernel get too deep, especially when interrupts are
+ 	  involved.
+ 
+ 	  Use this in cases where you see apparently random memory
+ 	  corruption, especially if it appears in 'struct thread_info'
+ 
+ 	  If in doubt, say "N".
+ 
+ source "lib/Kconfig.kmemcheck"
+ 
+ source "lib/Kconfig.kasan"
+ 
+ endmenu # "Memory Debugging"
+ 
+ config ARCH_HAS_KCOV
+ 	bool
+ 	help
+ 	  KCOV does not have any arch-specific code, but currently it is enabled
+ 	  only for x86_64. KCOV requires testing on other archs, and most likely
+ 	  disabling of instrumentation for some early boot code.
+ 
+ config KCOV
+ 	bool "Code coverage for fuzzing"
+ 	depends on ARCH_HAS_KCOV
+ 	select DEBUG_FS
+ 	select GCC_PLUGINS if !COMPILE_TEST
+ 	select GCC_PLUGIN_SANCOV if !COMPILE_TEST
+ 	help
+ 	  KCOV exposes kernel code coverage information in a form suitable
+ 	  for coverage-guided fuzzing (randomized testing).
+ 
+ 	  If RANDOMIZE_BASE is enabled, PC values will not be stable across
+ 	  different machines and across reboots. If you need stable PC values,
+ 	  disable RANDOMIZE_BASE.
+ 
+ 	  For more details, see Documentation/dev-tools/kcov.rst.
+ 
+ config KCOV_INSTRUMENT_ALL
+ 	bool "Instrument all code by default"
+ 	depends on KCOV
+ 	default y if KCOV
+ 	help
+ 	  If you are doing generic system call fuzzing (like e.g. syzkaller),
+ 	  then you will want to instrument the whole kernel and you should
+ 	  say y here. If you are doing more targeted fuzzing (like e.g.
+ 	  filesystem fuzzing with AFL) then you will want to enable coverage
+ 	  for more specific subsets of files, and should say n here.
+ 
+ config DEBUG_SHIRQ
+ 	bool "Debug shared IRQ handlers"
+ 	depends on DEBUG_KERNEL
+ 	help
+ 	  Enable this to generate a spurious interrupt as soon as a shared
+ 	  interrupt handler is registered, and just before one is deregistered.
+ 	  Drivers ought to be able to handle interrupts coming in at those
+ 	  points; some don't and need to be caught.
+ 
+ menu "Debug Lockups and Hangs"
+ 
+ config LOCKUP_DETECTOR
+ 	bool "Detect Hard and Soft Lockups"
+ 	depends on DEBUG_KERNEL && !S390
+ 	help
+ 	  Say Y here to enable the kernel to act as a watchdog to detect
+ 	  hard and soft lockups.
+ 
+ 	  Softlockups are bugs that cause the kernel to loop in kernel
+ 	  mode for more than 20 seconds, without giving other tasks a
+ 	  chance to run.  The current stack trace is displayed upon
+ 	  detection and the system will stay locked up.
+ 
+ 	  Hardlockups are bugs that cause the CPU to loop in kernel mode
+ 	  for more than 10 seconds, without letting other interrupts have a
+ 	  chance to run.  The current stack trace is displayed upon detection
+ 	  and the system will stay locked up.
+ 
+ 	  The overhead should be minimal.  A periodic hrtimer runs to
+ 	  generate interrupts and kick the watchdog task every 4 seconds.
+ 	  An NMI is generated every 10 seconds or so to check for hardlockups.
+ 
+ 	  The frequency of hrtimer and NMI events and the soft and hard lockup
+ 	  thresholds can be controlled through the sysctl watchdog_thresh.
+ 
+ config HARDLOCKUP_DETECTOR
+ 	def_bool y
+ 	depends on LOCKUP_DETECTOR && !HAVE_NMI_WATCHDOG
+ 	depends on PERF_EVENTS && HAVE_PERF_EVENTS_NMI
+ 
+ config BOOTPARAM_HARDLOCKUP_PANIC
+ 	bool "Panic (Reboot) On Hard Lockups"
+ 	depends on HARDLOCKUP_DETECTOR
+ 	help
+ 	  Say Y here to enable the kernel to panic on "hard lockups",
+ 	  which are bugs that cause the kernel to loop in kernel
+ 	  mode with interrupts disabled for more than 10 seconds (configurable
+ 	  using the watchdog_thresh sysctl).
+ 
+ 	  Say N if unsure.
+ 
+ config BOOTPARAM_HARDLOCKUP_PANIC_VALUE
+ 	int
+ 	depends on HARDLOCKUP_DETECTOR
+ 	range 0 1
+ 	default 0 if !BOOTPARAM_HARDLOCKUP_PANIC
+ 	default 1 if BOOTPARAM_HARDLOCKUP_PANIC
+ 
+ config BOOTPARAM_SOFTLOCKUP_PANIC
+ 	bool "Panic (Reboot) On Soft Lockups"
+ 	depends on LOCKUP_DETECTOR
+ 	help
+ 	  Say Y here to enable the kernel to panic on "soft lockups",
+ 	  which are bugs that cause the kernel to loop in kernel
+ 	  mode for more than 20 seconds (configurable using the watchdog_thresh
+ 	  sysctl), without giving other tasks a chance to run.
+ 
+ 	  The panic can be used in combination with panic_timeout,
+ 	  to cause the system to reboot automatically after a
+ 	  lockup has been detected. This feature is useful for
+ 	  high-availability systems that have uptime guarantees and
+ 	  where a lockup must be resolved ASAP.
+ 
+ 	  Say N if unsure.
+ 
+ config BOOTPARAM_SOFTLOCKUP_PANIC_VALUE
+ 	int
+ 	depends on LOCKUP_DETECTOR
+ 	range 0 1
+ 	default 0 if !BOOTPARAM_SOFTLOCKUP_PANIC
+ 	default 1 if BOOTPARAM_SOFTLOCKUP_PANIC
+ 
+ config DETECT_HUNG_TASK
+ 	bool "Detect Hung Tasks"
+ 	depends on DEBUG_KERNEL
+ 	default LOCKUP_DETECTOR
+ 	help
+ 	  Say Y here to enable the kernel to detect "hung tasks",
+ 	  which are bugs that cause the task to be stuck in
+ 	  uninterruptible "D" state indefinitely.
+ 
+ 	  When a hung task is detected, the kernel will print the
+ 	  current stack trace (which you should report), but the
+ 	  task will stay in uninterruptible state. If lockdep is
+ 	  enabled then all held locks will also be reported. This
+ 	  feature has negligible overhead.
+ 
+ config DEFAULT_HUNG_TASK_TIMEOUT
+ 	int "Default timeout for hung task detection (in seconds)"
+ 	depends on DETECT_HUNG_TASK
+ 	default 120
+ 	help
+ 	  This option controls the default timeout (in seconds) used
+ 	  to determine when a task has become non-responsive and should
+ 	  be considered hung.
+ 
+ 	  It can be adjusted at runtime via the kernel.hung_task_timeout_secs
+ 	  sysctl or by writing a value to
+ 	  /proc/sys/kernel/hung_task_timeout_secs.
+ 
+ 	  A timeout of 0 disables the check.  The default is two minutes.
+ 	  Keeping the default should be fine in most cases.
+ 
+ config BOOTPARAM_HUNG_TASK_PANIC
+ 	bool "Panic (Reboot) On Hung Tasks"
+ 	depends on DETECT_HUNG_TASK
+ 	help
+ 	  Say Y here to enable the kernel to panic on "hung tasks",
+ 	  which are bugs that cause the kernel to leave a task stuck
+ 	  in uninterruptible "D" state.
+ 
+ 	  The panic can be used in combination with panic_timeout,
+ 	  to cause the system to reboot automatically after a
+ 	  hung task has been detected. This feature is useful for
+ 	  high-availability systems that have uptime guarantees and
+ 	  where a hung tasks must be resolved ASAP.
+ 
+ 	  Say N if unsure.
+ 
+ config BOOTPARAM_HUNG_TASK_PANIC_VALUE
+ 	int
+ 	depends on DETECT_HUNG_TASK
+ 	range 0 1
+ 	default 0 if !BOOTPARAM_HUNG_TASK_PANIC
+ 	default 1 if BOOTPARAM_HUNG_TASK_PANIC
+ 
+ config WQ_WATCHDOG
+ 	bool "Detect Workqueue Stalls"
+ 	depends on DEBUG_KERNEL
+ 	help
+ 	  Say Y here to enable stall detection on workqueues.  If a
+ 	  worker pool doesn't make forward progress on a pending work
+ 	  item for over a given amount of time, 30s by default, a
+ 	  warning message is printed along with dump of workqueue
+ 	  state.  This can be configured through kernel parameter
+ 	  "workqueue.watchdog_thresh" and its sysfs counterpart.
+ 
+ endmenu # "Debug lockups and hangs"
+ 
+ config PANIC_ON_OOPS
+ 	bool "Panic on Oops"
+ 	help
+ 	  Say Y here to enable the kernel to panic when it oopses. This
+ 	  has the same effect as setting oops=panic on the kernel command
+ 	  line.
+ 
+ 	  This feature is useful to ensure that the kernel does not do
+ 	  anything erroneous after an oops which could result in data
+ 	  corruption or other issues.
+ 
+ 	  Say N if unsure.
+ 
+ config PANIC_ON_OOPS_VALUE
+ 	int
+ 	range 0 1
+ 	default 0 if !PANIC_ON_OOPS
+ 	default 1 if PANIC_ON_OOPS
+ 
+ config PANIC_TIMEOUT
+ 	int "panic timeout"
+ 	default 0
+ 	help
+ 	  Set the timeout value (in seconds) until a reboot occurs when the
+ 	  the kernel panics. If n = 0, then we wait forever. A timeout
+ 	  value n > 0 will wait n seconds before rebooting, while a timeout
+ 	  value n < 0 will reboot immediately.
+ 
+ config SCHED_DEBUG
+ 	bool "Collect scheduler debugging info"
+ 	depends on DEBUG_KERNEL && PROC_FS
+ 	default y
+ 	help
+ 	  If you say Y here, the /proc/sched_debug file will be provided
+ 	  that can help debug the scheduler. The runtime overhead of this
+ 	  option is minimal.
+ 
+ config SCHED_INFO
+ 	bool
+ 	default n
+ 
+ config SCHEDSTATS
+ 	bool "Collect scheduler statistics"
+ 	depends on DEBUG_KERNEL && PROC_FS
+ 	select SCHED_INFO
+ 	help
+ 	  If you say Y here, additional code will be inserted into the
+ 	  scheduler and related routines to collect statistics about
+ 	  scheduler behavior and provide them in /proc/schedstat.  These
+ 	  stats may be useful for both tuning and debugging the scheduler
+ 	  If you aren't debugging the scheduler or trying to tune a specific
+ 	  application, you can say N to avoid the very slight overhead
+ 	  this adds.
+ 
+ config SCHED_STACK_END_CHECK
+ 	bool "Detect stack corruption on calls to schedule()"
+ 	depends on DEBUG_KERNEL
+ 	default n
+ 	help
+ 	  This option checks for a stack overrun on calls to schedule().
+ 	  If the stack end location is found to be over written always panic as
+ 	  the content of the corrupted region can no longer be trusted.
+ 	  This is to ensure no erroneous behaviour occurs which could result in
+ 	  data corruption or a sporadic crash at a later stage once the region
+ 	  is examined. The runtime overhead introduced is minimal.
+ 
+ config DEBUG_TIMEKEEPING
+ 	bool "Enable extra timekeeping sanity checking"
+ 	help
+ 	  This option will enable additional timekeeping sanity checks
+ 	  which may be helpful when diagnosing issues where timekeeping
+ 	  problems are suspected.
+ 
+ 	  This may include checks in the timekeeping hotpaths, so this
+ 	  option may have a (very small) performance impact to some
+ 	  workloads.
+ 
+ 	  If unsure, say N.
+ 
++>>>>>>> 29dee3c03abc (locking/refcounts: Out-of-line everything)
  config DEBUG_PREEMPT
  	bool "Debug preemptible kernel"
  	depends on DEBUG_KERNEL && PREEMPT && TRACE_IRQFLAGS_SUPPORT
diff --cc lib/Makefile
index c09f5df6637c,192e4d03caf9..000000000000
--- a/lib/Makefile
+++ b/lib/Makefile
@@@ -23,17 -31,30 +23,26 @@@ lib-$(CONFIG_HAS_DMA) += dma-noop.
  lib-y	+= kobject.o klist.o
  obj-y	+= lockref.o
  
++<<<<<<< HEAD
 +obj-y += bcd.o div64.o sort.o parser.o halfmd4.o debug_locks.o random32.o \
 +	 bust_spinlocks.o hexdump.o kasprintf.o bitmap.o scatterlist.o \
 +	 gcd.o lcm.o list_sort.o uuid.o flex_array.o iovec.o \
 +	 bsearch.o find_last_bit.o find_next_bit.o llist.o memweight.o kfifo.o \
 +	 percpu_ida.o rhashtable.o
++=======
+ obj-y += bcd.o div64.o sort.o parser.o debug_locks.o random32.o \
+ 	 bust_spinlocks.o kasprintf.o bitmap.o scatterlist.o \
+ 	 gcd.o lcm.o list_sort.o uuid.o flex_array.o iov_iter.o clz_ctz.o \
+ 	 bsearch.o find_bit.o llist.o memweight.o kfifo.o \
+ 	 percpu-refcount.o percpu_ida.o rhashtable.o reciprocal_div.o \
+ 	 once.o refcount.o
++>>>>>>> 29dee3c03abc (locking/refcounts: Out-of-line everything)
  obj-y += string_helpers.o
  obj-$(CONFIG_TEST_STRING_HELPERS) += test-string_helpers.o
 -obj-y += hexdump.o
 -obj-$(CONFIG_TEST_HEXDUMP) += test_hexdump.o
  obj-y += kstrtox.o
 -obj-$(CONFIG_TEST_BPF) += test_bpf.o
 -obj-$(CONFIG_TEST_FIRMWARE) += test_firmware.o
 -obj-$(CONFIG_TEST_HASH) += test_hash.o
 -obj-$(CONFIG_TEST_KASAN) += test_kasan.o
  obj-$(CONFIG_TEST_KSTRTOX) += test-kstrtox.o
 -obj-$(CONFIG_TEST_LKM) += test_module.o
  obj-$(CONFIG_TEST_RHASHTABLE) += test_rhashtable.o
 -obj-$(CONFIG_TEST_USER_COPY) += test_user_copy.o
 -obj-$(CONFIG_TEST_STATIC_KEYS) += test_static_keys.o
 -obj-$(CONFIG_TEST_STATIC_KEYS) += test_static_key_base.o
 -obj-$(CONFIG_TEST_PRINTF) += test_printf.o
 -obj-$(CONFIG_TEST_BITMAP) += test_bitmap.o
 -obj-$(CONFIG_TEST_UUID) += test_uuid.o
 +obj-$(CONFIG_TEST_PARMAN) += test_parman.o
  
  ifeq ($(CONFIG_DEBUG_KOBJECT),y)
  CFLAGS_kobject.o += -DDEBUG
diff --cc tools/include/linux/refcount.h
index a0177c1f55b1,0e8cfb2ce91e..000000000000
--- a/tools/include/linux/refcount.h
+++ b/tools/include/linux/refcount.h
@@@ -1,53 -1,10 +1,23 @@@
 -#ifndef _LINUX_REFCOUNT_H
 -#define _LINUX_REFCOUNT_H
 +#ifndef _TOOLS_LINUX_REFCOUNT_H
 +#define _TOOLS_LINUX_REFCOUNT_H
  
- /*
-  * Variant of atomic_t specialized for reference counts.
-  *
-  * The interface matches the atomic_t interface (to aid in porting) but only
-  * provides the few functions one should use for reference counting.
-  *
-  * It differs in that the counter saturates at UINT_MAX and will not move once
-  * there. This avoids wrapping the counter and causing 'spurious'
-  * use-after-free issues.
-  *
-  * Memory ordering rules are slightly relaxed wrt regular atomic_t functions
-  * and provide only what is strictly required for refcounts.
-  *
-  * The increments are fully relaxed; these will not provide ordering. The
-  * rationale is that whatever is used to obtain the object we're increasing the
-  * reference count on will provide the ordering. For locked data structures,
-  * its the lock acquire, for RCU/lockless data structures its the dependent
-  * load.
-  *
-  * Do note that inc_not_zero() provides a control dependency which will order
-  * future stores against the inc, this ensures we'll never modify the object
-  * if we did not in fact acquire a reference.
-  *
-  * The decrements will provide release order, such that all the prior loads and
-  * stores will be issued before, it also provides a control dependency, which
-  * will order us against the subsequent free().
-  *
-  * The control dependency is against the load of the cmpxchg (ll/sc) that
-  * succeeded. This means the stores aren't fully ordered, but this is fine
-  * because the 1->0 transition indicates no concurrency.
-  *
-  * Note that the allocator is responsible for ordering things between free()
-  * and alloc().
-  *
-  */
- 
  #include <linux/atomic.h>
++<<<<<<< HEAD:tools/include/linux/refcount.h
 +#include <linux/kernel.h>
 +
 +#ifdef NDEBUG
 +#define REFCOUNT_WARN(cond, str) (void)(cond)
 +#define __refcount_check
 +#else
 +#define REFCOUNT_WARN(cond, str) BUG_ON(cond)
 +#define __refcount_check	__must_check
 +#endif
 +
++=======
+ #include <linux/mutex.h>
+ #include <linux/spinlock.h>
+ 
++>>>>>>> 29dee3c03abc (locking/refcounts: Out-of-line everything):include/linux/refcount.h
  typedef struct refcount_struct {
  	atomic_t refs;
  } refcount_t;
@@@ -64,88 -21,21 +34,109 @@@ static inline unsigned int refcount_rea
  	return atomic_read(&r->refs);
  }
  
++<<<<<<< HEAD:tools/include/linux/refcount.h
 +/*
 + * Similar to atomic_inc_not_zero(), will saturate at UINT_MAX and WARN.
 + *
 + * Provides no memory ordering, it is assumed the caller has guaranteed the
 + * object memory to be stable (RCU, etc.). It does provide a control dependency
 + * and thereby orders future stores. See the comment on top.
 + */
 +static inline __refcount_check
 +bool refcount_inc_not_zero(refcount_t *r)
 +{
 +	unsigned int old, new, val = atomic_read(&r->refs);
 +
 +	for (;;) {
 +		new = val + 1;
 +
 +		if (!val)
 +			return false;
 +
 +		if (unlikely(!new))
 +			return true;
 +
 +		old = atomic_cmpxchg_relaxed(&r->refs, val, new);
 +		if (old == val)
 +			break;
 +
 +		val = old;
 +	}
 +
 +	REFCOUNT_WARN(new == UINT_MAX, "refcount_t: saturated; leaking memory.\n");
 +
 +	return true;
 +}
 +
 +/*
 + * Similar to atomic_inc(), will saturate at UINT_MAX and WARN.
 + *
 + * Provides no memory ordering, it is assumed the caller already has a
 + * reference on the object, will WARN when this is not so.
 + */
 +static inline void refcount_inc(refcount_t *r)
 +{
 +	REFCOUNT_WARN(!refcount_inc_not_zero(r), "refcount_t: increment on 0; use-after-free.\n");
 +}
 +
 +/*
 + * Similar to atomic_dec_and_test(), it will WARN on underflow and fail to
 + * decrement when saturated at UINT_MAX.
 + *
 + * Provides release memory ordering, such that prior loads and stores are done
 + * before, and provides a control dependency such that free() must come after.
 + * See the comment on top.
 + */
 +static inline __refcount_check
 +bool refcount_sub_and_test(unsigned int i, refcount_t *r)
 +{
 +	unsigned int old, new, val = atomic_read(&r->refs);
 +
 +	for (;;) {
 +		if (unlikely(val == UINT_MAX))
 +			return false;
 +
 +		new = val - i;
 +		if (new > val) {
 +			REFCOUNT_WARN(new > val, "refcount_t: underflow; use-after-free.\n");
 +			return false;
 +		}
 +
 +		old = atomic_cmpxchg_release(&r->refs, val, new);
 +		if (old == val)
 +			break;
 +
 +		val = old;
 +	}
 +
 +	return !new;
 +}
 +
 +static inline __refcount_check
 +bool refcount_dec_and_test(refcount_t *r)
 +{
 +	return refcount_sub_and_test(1, r);
 +}
 +
 +
 +#endif /* _ATOMIC_LINUX_REFCOUNT_H */
++=======
+ extern __must_check bool refcount_add_not_zero(unsigned int i, refcount_t *r);
+ extern void refcount_add(unsigned int i, refcount_t *r);
+ 
+ extern __must_check bool refcount_inc_not_zero(refcount_t *r);
+ extern void refcount_inc(refcount_t *r);
+ 
+ extern __must_check bool refcount_sub_and_test(unsigned int i, refcount_t *r);
+ extern void refcount_sub(unsigned int i, refcount_t *r);
+ 
+ extern __must_check bool refcount_dec_and_test(refcount_t *r);
+ extern void refcount_dec(refcount_t *r);
+ 
+ extern __must_check bool refcount_dec_if_one(refcount_t *r);
+ extern __must_check bool refcount_dec_not_one(refcount_t *r);
+ extern __must_check bool refcount_dec_and_mutex_lock(refcount_t *r, struct mutex *lock);
+ extern __must_check bool refcount_dec_and_lock(refcount_t *r, spinlock_t *lock);
+ 
+ #endif /* _LINUX_REFCOUNT_H */
++>>>>>>> 29dee3c03abc (locking/refcounts: Out-of-line everything):include/linux/refcount.h
* Unmerged path lib/Kconfig.debug
* Unmerged path lib/Makefile
diff --git a/lib/refcount.c b/lib/refcount.c
new file mode 100644
index 000000000000..1d33366189d1
--- /dev/null
+++ b/lib/refcount.c
@@ -0,0 +1,267 @@
+/*
+ * Variant of atomic_t specialized for reference counts.
+ *
+ * The interface matches the atomic_t interface (to aid in porting) but only
+ * provides the few functions one should use for reference counting.
+ *
+ * It differs in that the counter saturates at UINT_MAX and will not move once
+ * there. This avoids wrapping the counter and causing 'spurious'
+ * use-after-free issues.
+ *
+ * Memory ordering rules are slightly relaxed wrt regular atomic_t functions
+ * and provide only what is strictly required for refcounts.
+ *
+ * The increments are fully relaxed; these will not provide ordering. The
+ * rationale is that whatever is used to obtain the object we're increasing the
+ * reference count on will provide the ordering. For locked data structures,
+ * its the lock acquire, for RCU/lockless data structures its the dependent
+ * load.
+ *
+ * Do note that inc_not_zero() provides a control dependency which will order
+ * future stores against the inc, this ensures we'll never modify the object
+ * if we did not in fact acquire a reference.
+ *
+ * The decrements will provide release order, such that all the prior loads and
+ * stores will be issued before, it also provides a control dependency, which
+ * will order us against the subsequent free().
+ *
+ * The control dependency is against the load of the cmpxchg (ll/sc) that
+ * succeeded. This means the stores aren't fully ordered, but this is fine
+ * because the 1->0 transition indicates no concurrency.
+ *
+ * Note that the allocator is responsible for ordering things between free()
+ * and alloc().
+ *
+ */
+
+#include <linux/refcount.h>
+#include <linux/bug.h>
+
+bool refcount_add_not_zero(unsigned int i, refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		if (!val)
+			return false;
+
+		if (unlikely(val == UINT_MAX))
+			return true;
+
+		new = val + i;
+		if (new < val)
+			new = UINT_MAX;
+		old = atomic_cmpxchg_relaxed(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	WARN(new == UINT_MAX, "refcount_t: saturated; leaking memory.\n");
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(refcount_add_not_zero);
+
+void refcount_add(unsigned int i, refcount_t *r)
+{
+	WARN(!refcount_add_not_zero(i, r), "refcount_t: addition on 0; use-after-free.\n");
+}
+EXPORT_SYMBOL_GPL(refcount_add);
+
+/*
+ * Similar to atomic_inc_not_zero(), will saturate at UINT_MAX and WARN.
+ *
+ * Provides no memory ordering, it is assumed the caller has guaranteed the
+ * object memory to be stable (RCU, etc.). It does provide a control dependency
+ * and thereby orders future stores. See the comment on top.
+ */
+bool refcount_inc_not_zero(refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		new = val + 1;
+
+		if (!val)
+			return false;
+
+		if (unlikely(!new))
+			return true;
+
+		old = atomic_cmpxchg_relaxed(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	WARN(new == UINT_MAX, "refcount_t: saturated; leaking memory.\n");
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(refcount_inc_not_zero);
+
+/*
+ * Similar to atomic_inc(), will saturate at UINT_MAX and WARN.
+ *
+ * Provides no memory ordering, it is assumed the caller already has a
+ * reference on the object, will WARN when this is not so.
+ */
+void refcount_inc(refcount_t *r)
+{
+	WARN(!refcount_inc_not_zero(r), "refcount_t: increment on 0; use-after-free.\n");
+}
+EXPORT_SYMBOL_GPL(refcount_inc);
+
+bool refcount_sub_and_test(unsigned int i, refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		if (unlikely(val == UINT_MAX))
+			return false;
+
+		new = val - i;
+		if (new > val) {
+			WARN(new > val, "refcount_t: underflow; use-after-free.\n");
+			return false;
+		}
+
+		old = atomic_cmpxchg_release(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	return !new;
+}
+EXPORT_SYMBOL_GPL(refcount_sub_and_test);
+
+/*
+ * Similar to atomic_dec_and_test(), it will WARN on underflow and fail to
+ * decrement when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before, and provides a control dependency such that free() must come after.
+ * See the comment on top.
+ */
+bool refcount_dec_and_test(refcount_t *r)
+{
+	return refcount_sub_and_test(1, r);
+}
+EXPORT_SYMBOL_GPL(refcount_dec_and_test);
+
+/*
+ * Similar to atomic_dec(), it will WARN on underflow and fail to decrement
+ * when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before.
+ */
+
+void refcount_dec(refcount_t *r)
+{
+	WARN(refcount_dec_and_test(r), "refcount_t: decrement hit 0; leaking memory.\n");
+}
+EXPORT_SYMBOL_GPL(refcount_dec);
+
+/*
+ * No atomic_t counterpart, it attempts a 1 -> 0 transition and returns the
+ * success thereof.
+ *
+ * Like all decrement operations, it provides release memory order and provides
+ * a control dependency.
+ *
+ * It can be used like a try-delete operator; this explicit case is provided
+ * and not cmpxchg in generic, because that would allow implementing unsafe
+ * operations.
+ */
+bool refcount_dec_if_one(refcount_t *r)
+{
+	return atomic_cmpxchg_release(&r->refs, 1, 0) == 1;
+}
+EXPORT_SYMBOL_GPL(refcount_dec_if_one);
+
+/*
+ * No atomic_t counterpart, it decrements unless the value is 1, in which case
+ * it will return false.
+ *
+ * Was often done like: atomic_add_unless(&var, -1, 1)
+ */
+bool refcount_dec_not_one(refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		if (unlikely(val == UINT_MAX))
+			return true;
+
+		if (val == 1)
+			return false;
+
+		new = val - 1;
+		if (new > val) {
+			WARN(new > val, "refcount_t: underflow; use-after-free.\n");
+			return true;
+		}
+
+		old = atomic_cmpxchg_release(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(refcount_dec_not_one);
+
+/*
+ * Similar to atomic_dec_and_mutex_lock(), it will WARN on underflow and fail
+ * to decrement when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before, and provides a control dependency such that free() must come after.
+ * See the comment on top.
+ */
+bool refcount_dec_and_mutex_lock(refcount_t *r, struct mutex *lock)
+{
+	if (refcount_dec_not_one(r))
+		return false;
+
+	mutex_lock(lock);
+	if (!refcount_dec_and_test(r)) {
+		mutex_unlock(lock);
+		return false;
+	}
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(refcount_dec_and_mutex_lock);
+
+/*
+ * Similar to atomic_dec_and_lock(), it will WARN on underflow and fail to
+ * decrement when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before, and provides a control dependency such that free() must come after.
+ * See the comment on top.
+ */
+bool refcount_dec_and_lock(refcount_t *r, spinlock_t *lock)
+{
+	if (refcount_dec_not_one(r))
+		return false;
+
+	spin_lock(lock);
+	if (!refcount_dec_and_test(r)) {
+		spin_unlock(lock);
+		return false;
+	}
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(refcount_dec_and_lock);
+
* Unmerged path tools/include/linux/refcount.h
