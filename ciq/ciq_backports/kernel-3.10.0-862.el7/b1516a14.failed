iommu/amd: Implement flush queue

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] amd: Implement flush queue (Jerry Snitselaar) [1411581]
Rebuild_FUZZ: 89.66%
commit-author Joerg Roedel <jroedel@suse.de>
commit b1516a14657acf81a587e9a6e733a881625eee53
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b1516a14.failed

With the flush queue the IOMMU TLBs will not be flushed at
every dma-ops unmap operation. The unmapped ranges will be
queued and flushed at once, when the queue is full. This
makes unmapping operations a lot faster (on average) and
restores the performance of the old address allocator.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit b1516a14657acf81a587e9a6e733a881625eee53)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index 3c777dd3adaa,a8e4c5adade7..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -2554,7 -2318,13 +2614,17 @@@ static void __unmap_single(struct dma_o
  		start += PAGE_SIZE;
  	}
  
++<<<<<<< HEAD
 +	dma_ops_free_addresses(dma_dom, dma_addr, pages);
++=======
+ 	if (amd_iommu_unmap_flush) {
+ 		dma_ops_free_iova(dma_dom, dma_addr, pages);
+ 		domain_flush_tlb(&dma_dom->domain);
+ 		domain_flush_complete(&dma_dom->domain);
+ 	} else {
+ 		queue_add(dma_dom, dma_addr, pages);
+ 	}
++>>>>>>> b1516a14657a (iommu/amd: Implement flush queue)
  }
  
  /*
* Unmerged path drivers/iommu/amd_iommu.c
