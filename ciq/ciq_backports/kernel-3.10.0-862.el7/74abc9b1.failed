net: ethernet: update drivers to make both SW and HW TX timestamps

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [net] ethernet: update drivers to make both SW and HW TX timestamps (Hangbin Liu) [1421164]
Rebuild_FUZZ: 96.06%
commit-author Miroslav Lichvar <mlichvar@redhat.com>
commit 74abc9b18f446d1a9e0602a71a22e5ffe8a2cd23
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/74abc9b1.failed

Some drivers were calling the skb_tx_timestamp() function only when
a hardware timestamp was not requested. Now that applications can use
the SOF_TIMESTAMPING_OPT_TX_SWHW option to request both software and
hardware timestamps, the drivers need to be modified to unconditionally
call skb_tx_timestamp().

CC: Richard Cochran <richardcochran@gmail.com>
CC: Willem de Bruijn <willemb@google.com>
	Signed-off-by: Miroslav Lichvar <mlichvar@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 74abc9b18f446d1a9e0602a71a22e5ffe8a2cd23)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
#	drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
diff --cc drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 2b10e68c213a,27c12e732a8a..000000000000
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@@ -1763,7 -2695,245 +1763,249 @@@ static int stmmac_release(struct net_de
  }
  
  /**
++<<<<<<< HEAD
 + *  stmmac_xmit: Tx entry point of the driver
++=======
+  *  stmmac_tso_allocator - close entry point of the driver
+  *  @priv: driver private structure
+  *  @des: buffer start address
+  *  @total_len: total length to fill in descriptors
+  *  @last_segmant: condition for the last descriptor
+  *  @queue: TX queue index
+  *  Description:
+  *  This function fills descriptor and request new descriptors according to
+  *  buffer length to fill
+  */
+ static void stmmac_tso_allocator(struct stmmac_priv *priv, unsigned int des,
+ 				 int total_len, bool last_segment, u32 queue)
+ {
+ 	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+ 	struct dma_desc *desc;
+ 	u32 buff_size;
+ 	int tmp_len;
+ 
+ 	tmp_len = total_len;
+ 
+ 	while (tmp_len > 0) {
+ 		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
+ 		desc = tx_q->dma_tx + tx_q->cur_tx;
+ 
+ 		desc->des0 = cpu_to_le32(des + (total_len - tmp_len));
+ 		buff_size = tmp_len >= TSO_MAX_BUFF_SIZE ?
+ 			    TSO_MAX_BUFF_SIZE : tmp_len;
+ 
+ 		priv->hw->desc->prepare_tso_tx_desc(desc, 0, buff_size,
+ 			0, 1,
+ 			(last_segment) && (buff_size < TSO_MAX_BUFF_SIZE),
+ 			0, 0);
+ 
+ 		tmp_len -= TSO_MAX_BUFF_SIZE;
+ 	}
+ }
+ 
+ /**
+  *  stmmac_tso_xmit - Tx entry point of the driver for oversized frames (TSO)
+  *  @skb : the socket buffer
+  *  @dev : device pointer
+  *  Description: this is the transmit function that is called on TSO frames
+  *  (support available on GMAC4 and newer chips).
+  *  Diagram below show the ring programming in case of TSO frames:
+  *
+  *  First Descriptor
+  *   --------
+  *   | DES0 |---> buffer1 = L2/L3/L4 header
+  *   | DES1 |---> TCP Payload (can continue on next descr...)
+  *   | DES2 |---> buffer 1 and 2 len
+  *   | DES3 |---> must set TSE, TCP hdr len-> [22:19]. TCP payload len [17:0]
+  *   --------
+  *	|
+  *     ...
+  *	|
+  *   --------
+  *   | DES0 | --| Split TCP Payload on Buffers 1 and 2
+  *   | DES1 | --|
+  *   | DES2 | --> buffer 1 and 2 len
+  *   | DES3 |
+  *   --------
+  *
+  * mss is fixed when enable tso, so w/o programming the TDES3 ctx field.
+  */
+ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
+ {
+ 	struct dma_desc *desc, *first, *mss_desc = NULL;
+ 	struct stmmac_priv *priv = netdev_priv(dev);
+ 	int nfrags = skb_shinfo(skb)->nr_frags;
+ 	u32 queue = skb_get_queue_mapping(skb);
+ 	unsigned int first_entry, des;
+ 	struct stmmac_tx_queue *tx_q;
+ 	int tmp_pay_len = 0;
+ 	u32 pay_len, mss;
+ 	u8 proto_hdr_len;
+ 	int i;
+ 
+ 	tx_q = &priv->tx_queue[queue];
+ 
+ 	/* Compute header lengths */
+ 	proto_hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);
+ 
+ 	/* Desc availability based on threshold should be enough safe */
+ 	if (unlikely(stmmac_tx_avail(priv, queue) <
+ 		(((skb->len - proto_hdr_len) / TSO_MAX_BUFF_SIZE + 1)))) {
+ 		if (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, queue))) {
+ 			netif_tx_stop_queue(netdev_get_tx_queue(priv->dev,
+ 								queue));
+ 			/* This is a hard error, log it. */
+ 			netdev_err(priv->dev,
+ 				   "%s: Tx Ring full when queue awake\n",
+ 				   __func__);
+ 		}
+ 		return NETDEV_TX_BUSY;
+ 	}
+ 
+ 	pay_len = skb_headlen(skb) - proto_hdr_len; /* no frags */
+ 
+ 	mss = skb_shinfo(skb)->gso_size;
+ 
+ 	/* set new MSS value if needed */
+ 	if (mss != priv->mss) {
+ 		mss_desc = tx_q->dma_tx + tx_q->cur_tx;
+ 		priv->hw->desc->set_mss(mss_desc, mss);
+ 		priv->mss = mss;
+ 		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
+ 	}
+ 
+ 	if (netif_msg_tx_queued(priv)) {
+ 		pr_info("%s: tcphdrlen %d, hdr_len %d, pay_len %d, mss %d\n",
+ 			__func__, tcp_hdrlen(skb), proto_hdr_len, pay_len, mss);
+ 		pr_info("\tskb->len %d, skb->data_len %d\n", skb->len,
+ 			skb->data_len);
+ 	}
+ 
+ 	first_entry = tx_q->cur_tx;
+ 
+ 	desc = tx_q->dma_tx + first_entry;
+ 	first = desc;
+ 
+ 	/* first descriptor: fill Headers on Buf1 */
+ 	des = dma_map_single(priv->device, skb->data, skb_headlen(skb),
+ 			     DMA_TO_DEVICE);
+ 	if (dma_mapping_error(priv->device, des))
+ 		goto dma_map_err;
+ 
+ 	tx_q->tx_skbuff_dma[first_entry].buf = des;
+ 	tx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb);
+ 	tx_q->tx_skbuff[first_entry] = skb;
+ 
+ 	first->des0 = cpu_to_le32(des);
+ 
+ 	/* Fill start of payload in buff2 of first descriptor */
+ 	if (pay_len)
+ 		first->des1 = cpu_to_le32(des + proto_hdr_len);
+ 
+ 	/* If needed take extra descriptors to fill the remaining payload */
+ 	tmp_pay_len = pay_len - TSO_MAX_BUFF_SIZE;
+ 
+ 	stmmac_tso_allocator(priv, des, tmp_pay_len, (nfrags == 0), queue);
+ 
+ 	/* Prepare fragments */
+ 	for (i = 0; i < nfrags; i++) {
+ 		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+ 
+ 		des = skb_frag_dma_map(priv->device, frag, 0,
+ 				       skb_frag_size(frag),
+ 				       DMA_TO_DEVICE);
+ 		if (dma_mapping_error(priv->device, des))
+ 			goto dma_map_err;
+ 
+ 		stmmac_tso_allocator(priv, des, skb_frag_size(frag),
+ 				     (i == nfrags - 1), queue);
+ 
+ 		tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des;
+ 		tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag);
+ 		tx_q->tx_skbuff[tx_q->cur_tx] = NULL;
+ 		tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true;
+ 	}
+ 
+ 	tx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true;
+ 
+ 	tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
+ 
+ 	if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {
+ 		netif_dbg(priv, hw, priv->dev, "%s: stop transmitted packets\n",
+ 			  __func__);
+ 		netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue));
+ 	}
+ 
+ 	dev->stats.tx_bytes += skb->len;
+ 	priv->xstats.tx_tso_frames++;
+ 	priv->xstats.tx_tso_nfrags += nfrags;
+ 
+ 	/* Manage tx mitigation */
+ 	priv->tx_count_frames += nfrags + 1;
+ 	if (likely(priv->tx_coal_frames > priv->tx_count_frames)) {
+ 		mod_timer(&priv->txtimer,
+ 			  STMMAC_COAL_TIMER(priv->tx_coal_timer));
+ 	} else {
+ 		priv->tx_count_frames = 0;
+ 		priv->hw->desc->set_tx_ic(desc);
+ 		priv->xstats.tx_set_ic_bit++;
+ 	}
+ 
+ 	skb_tx_timestamp(skb);
+ 
+ 	if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+ 		     priv->hwts_tx_en)) {
+ 		/* declare that device is doing timestamping */
+ 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+ 		priv->hw->desc->enable_tx_timestamp(first);
+ 	}
+ 
+ 	/* Complete the first descriptor before granting the DMA */
+ 	priv->hw->desc->prepare_tso_tx_desc(first, 1,
+ 			proto_hdr_len,
+ 			pay_len,
+ 			1, tx_q->tx_skbuff_dma[first_entry].last_segment,
+ 			tcp_hdrlen(skb) / 4, (skb->len - proto_hdr_len));
+ 
+ 	/* If context desc is used to change MSS */
+ 	if (mss_desc)
+ 		priv->hw->desc->set_tx_owner(mss_desc);
+ 
+ 	/* The own bit must be the latest setting done when prepare the
+ 	 * descriptor and then barrier is needed to make sure that
+ 	 * all is coherent before granting the DMA engine.
+ 	 */
+ 	dma_wmb();
+ 
+ 	if (netif_msg_pktdata(priv)) {
+ 		pr_info("%s: curr=%d dirty=%d f=%d, e=%d, f_p=%p, nfrags %d\n",
+ 			__func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry,
+ 			tx_q->cur_tx, first, nfrags);
+ 
+ 		priv->hw->desc->display_ring((void *)tx_q->dma_tx, DMA_TX_SIZE,
+ 					     0);
+ 
+ 		pr_info(">>> frame to be transmitted: ");
+ 		print_pkt(skb->data, skb_headlen(skb));
+ 	}
+ 
+ 	netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len);
+ 
+ 	priv->hw->dma->set_tx_tail_ptr(priv->ioaddr, tx_q->tx_tail_addr,
+ 				       queue);
+ 
+ 	return NETDEV_TX_OK;
+ 
+ dma_map_err:
+ 	dev_err(priv->device, "Tx dma map failed\n");
+ 	dev_kfree_skb(skb);
+ 	priv->dev->stats.tx_dropped++;
+ 	return NETDEV_TX_OK;
+ }
+ 
+ /**
+  *  stmmac_xmit - Tx entry point of the driver
++>>>>>>> 74abc9b18f44 (net: ethernet: update drivers to make both SW and HW TX timestamps)
   *  @skb : the socket buffer
   *  @dev : device pointer
   *  Description : this is the tx entry point of the driver.
@@@ -1849,86 -3005,166 +2091,85 @@@ static netdev_tx_t stmmac_xmit(struct s
  	for (i = 0; i < nfrags; i++) {
  		const skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
  		int len = skb_frag_size(frag);
 -		bool last_segment = (i == (nfrags - 1));
 -
 -		entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
 -
 -		if (likely(priv->extend_desc))
 -			desc = (struct dma_desc *)(tx_q->dma_etx + entry);
 -		else
 -			desc = tx_q->dma_tx + entry;
 -
 -		des = skb_frag_dma_map(priv->device, frag, 0, len,
 -				       DMA_TO_DEVICE);
 -		if (dma_mapping_error(priv->device, des))
 -			goto dma_map_err; /* should reuse desc w/o issues */
 -
 -		tx_q->tx_skbuff[entry] = NULL;
 -
 -		tx_q->tx_skbuff_dma[entry].buf = des;
 -		if (unlikely(priv->synopsys_id >= DWMAC_CORE_4_00))
 -			desc->des0 = cpu_to_le32(des);
 -		else
 -			desc->des2 = cpu_to_le32(des);
 -
 -		tx_q->tx_skbuff_dma[entry].map_as_page = true;
 -		tx_q->tx_skbuff_dma[entry].len = len;
 -		tx_q->tx_skbuff_dma[entry].last_segment = last_segment;
 -
 -		/* Prepare the descriptor and set the own bit too */
 -		priv->hw->desc->prepare_tx_desc(desc, 0, len, csum_insertion,
 -						priv->mode, 1, last_segment,
 -						skb->len);
 -	}
 -
 -	entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
 -
 -	tx_q->cur_tx = entry;
 -
 -	if (netif_msg_pktdata(priv)) {
 -		void *tx_head;
 -
 -		netdev_dbg(priv->dev,
 -			   "%s: curr=%d dirty=%d f=%d, e=%d, first=%p, nfrags=%d",
 -			   __func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry,
 -			   entry, first, nfrags);
  
 +		entry = (++priv->cur_tx) % txsize;
  		if (priv->extend_desc)
 -			tx_head = (void *)tx_q->dma_etx;
 +			desc = (struct dma_desc *)(priv->dma_etx + entry);
  		else
 -			tx_head = (void *)tx_q->dma_tx;
 -
 -		priv->hw->desc->display_ring(tx_head, DMA_TX_SIZE, false);
 -
 -		netdev_dbg(priv->dev, ">>> frame to be transmitted: ");
 -		print_pkt(skb->data, skb->len);
 -	}
 +			desc = priv->dma_tx + entry;
  
 -	if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {
 -		netif_dbg(priv, hw, priv->dev, "%s: stop transmitted packets\n",
 -			  __func__);
 -		netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue));
 +		TX_DBG("\t[entry %d] segment len: %d\n", entry, len);
 +		desc->des2 = skb_frag_dma_map(priv->device, frag, 0, len,
 +					      DMA_TO_DEVICE);
 +		priv->tx_skbuff_dma[entry] = desc->des2;
 +		priv->tx_skbuff[entry] = NULL;
 +		priv->hw->desc->prepare_tx_desc(desc, 0, len, csum_insertion,
 +						priv->mode);
 +		wmb();
 +		priv->hw->desc->set_tx_owner(desc);
 +		wmb();
  	}
  
 -	dev->stats.tx_bytes += skb->len;
 +	/* Finalize the latest segment. */
 +	priv->hw->desc->close_tx_desc(desc);
  
 +	wmb();
  	/* According to the coalesce parameter the IC bit for the latest
 -	 * segment is reset and the timer re-started to clean the tx status.
 -	 * This approach takes care about the fragments: desc is the first
 -	 * element in case of no SG.
 +	 * segment could be reset and the timer re-started to invoke the
 +	 * stmmac_tx function. This approach takes care about the fragments.
  	 */
  	priv->tx_count_frames += nfrags + 1;
 -	if (likely(priv->tx_coal_frames > priv->tx_count_frames)) {
 +	if (priv->tx_coal_frames > priv->tx_count_frames) {
 +		priv->hw->desc->clear_tx_ic(desc);
 +		priv->xstats.tx_reset_ic_bit++;
 +		TX_DBG("\t[entry %d]: tx_count_frames %d\n", entry,
 +		       priv->tx_count_frames);
  		mod_timer(&priv->txtimer,
  			  STMMAC_COAL_TIMER(priv->tx_coal_timer));
 -	} else {
 +	} else
  		priv->tx_count_frames = 0;
 -		priv->hw->desc->set_tx_ic(desc);
 -		priv->xstats.tx_set_ic_bit++;
 -	}
  
 -	skb_tx_timestamp(skb);
 -
 -	/* Ready to fill the first descriptor and set the OWN bit w/o any
 -	 * problems because all the descriptors are actually ready to be
 -	 * passed to the DMA engine.
 -	 */
 -	if (likely(!is_jumbo)) {
 -		bool last_segment = (nfrags == 0);
 +	/* To avoid raise condition */
 +	priv->hw->desc->set_tx_owner(first);
 +	wmb();
  
 -		des = dma_map_single(priv->device, skb->data,
 -				     nopaged_len, DMA_TO_DEVICE);
 -		if (dma_mapping_error(priv->device, des))
 -			goto dma_map_err;
 +	priv->cur_tx++;
  
 -		tx_q->tx_skbuff_dma[first_entry].buf = des;
 -		if (unlikely(priv->synopsys_id >= DWMAC_CORE_4_00))
 -			first->des0 = cpu_to_le32(des);
 +#ifdef STMMAC_XMIT_DEBUG
 +	if (netif_msg_pktdata(priv)) {
 +		pr_info("%s: curr %d dirty=%d entry=%d, first=%p, nfrags=%d",
 +			__func__, (priv->cur_tx % txsize),
 +			(priv->dirty_tx % txsize), entry, first, nfrags);
 +		if (priv->extend_desc)
 +			stmmac_display_ring((void *)priv->dma_etx, txsize, 1);
  		else
 -			first->des2 = cpu_to_le32(des);
 -
 -		tx_q->tx_skbuff_dma[first_entry].len = nopaged_len;
 -		tx_q->tx_skbuff_dma[first_entry].last_segment = last_segment;
 -
 -		if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
 -			     priv->hwts_tx_en)) {
 -			/* declare that device is doing timestamping */
 -			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
 -			priv->hw->desc->enable_tx_timestamp(first);
 -		}
 -
 -		/* Prepare the first descriptor setting the OWN bit too */
 -		priv->hw->desc->prepare_tx_desc(first, 1, nopaged_len,
 -						csum_insertion, priv->mode, 1,
 -						last_segment, skb->len);
 +			stmmac_display_ring((void *)priv->dma_tx, txsize, 0);
  
 -		/* The own bit must be the latest setting done when prepare the
 -		 * descriptor and then barrier is needed to make sure that
 -		 * all is coherent before granting the DMA engine.
 -		 */
 -		dma_wmb();
 +		pr_info(">>> frame to be transmitted: ");
 +		print_pkt(skb->data, skb->len);
 +	}
 +#endif
 +	if (unlikely(stmmac_tx_avail(priv) <= (MAX_SKB_FRAGS + 1))) {
 +		TX_DBG("%s: stop transmitted packets\n", __func__);
 +		netif_stop_queue(dev);
  	}
  
 -	netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len);
 -
 -	if (priv->synopsys_id < DWMAC_CORE_4_00)
 -		priv->hw->dma->enable_dma_transmission(priv->ioaddr);
 -	else
 -		priv->hw->dma->set_tx_tail_ptr(priv->ioaddr, tx_q->tx_tail_addr,
 -					       queue);
 -
 -	return NETDEV_TX_OK;
 -
 -dma_map_err:
 -	netdev_err(priv->dev, "Tx DMA map failed\n");
 -	dev_kfree_skb(skb);
 -	priv->dev->stats.tx_dropped++;
 -	return NETDEV_TX_OK;
 -}
 -
 -static void stmmac_rx_vlan(struct net_device *dev, struct sk_buff *skb)
 -{
 -	struct ethhdr *ehdr;
 -	u16 vlanid;
 -
 -	if ((dev->features & NETIF_F_HW_VLAN_CTAG_RX) ==
 -	    NETIF_F_HW_VLAN_CTAG_RX &&
 -	    !__vlan_get_tag(skb, &vlanid)) {
 -		/* pop the vlan tag */
 -		ehdr = (struct ethhdr *)skb->data;
 -		memmove(skb->data + VLAN_HLEN, ehdr, ETH_ALEN * 2);
 -		skb_pull(skb, VLAN_HLEN);
 -		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlanid);
 +	dev->stats.tx_bytes += skb->len;
 +
 +	if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
 +		     priv->hwts_tx_en)) {
 +		/* declare that device is doing timestamping */
 +		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
 +		priv->hw->desc->enable_tx_timestamp(first);
  	}
 -}
  
- 	if (!priv->hwts_tx_en)
- 		skb_tx_timestamp(skb);
++	skb_tx_timestamp(skb);
  
 -static inline int stmmac_rx_threshold_count(struct stmmac_rx_queue *rx_q)
 -{
 -	if (rx_q->rx_zeroc_thresh < STMMAC_RX_THRESH)
 -		return 0;
 +	priv->hw->dma->enable_dma_transmission(priv->ioaddr);
 +
 +	spin_unlock(&priv->tx_lock);
  
 -	return 1;
 +	return NETDEV_TX_OK;
  }
  
  /**
* Unmerged path drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
diff --git a/drivers/net/ethernet/amd/xgbe/xgbe-drv.c b/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
index a01866542bfd..893eae84b69f 100644
--- a/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
+++ b/drivers/net/ethernet/amd/xgbe/xgbe-drv.c
@@ -1394,8 +1394,7 @@ static void xgbe_prep_tx_tstamp(struct xgbe_prv_data *pdata,
 		spin_unlock_irqrestore(&pdata->tstamp_lock, flags);
 	}
 
-	if (!XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES, PTP))
-		skb_tx_timestamp(skb);
+	skb_tx_timestamp(skb);
 }
 
 static void xgbe_prep_vlan(struct sk_buff *skb, struct xgbe_packet_data *packet)
diff --git a/drivers/net/ethernet/intel/e1000e/netdev.c b/drivers/net/ethernet/intel/e1000e/netdev.c
index 25d6279d9bfb..9f484e63735d 100644
--- a/drivers/net/ethernet/intel/e1000e/netdev.c
+++ b/drivers/net/ethernet/intel/e1000e/netdev.c
@@ -5866,10 +5866,10 @@ static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 			adapter->tx_hwtstamp_skb = skb_get(skb);
 			adapter->tx_hwtstamp_start = jiffies;
 			schedule_work(&adapter->tx_hwtstamp_work);
-		} else {
-			skb_tx_timestamp(skb);
 		}
 
+		skb_tx_timestamp(skb);
+
 		netdev_sent_queue(netdev, skb->len);
 		e1000_tx_queue(tx_ring, tx_flags, count);
 		/* Make sure there is space in the ring for the next send. */
* Unmerged path drivers/net/ethernet/samsung/sxgbe/sxgbe_main.c
* Unmerged path drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
