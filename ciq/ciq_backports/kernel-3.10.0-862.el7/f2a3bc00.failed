IB/hfi1: Protect context array set/clear with spinlock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Michael J. Ruhl <michael.j.ruhl@intel.com>
commit f2a3bc00a03c2cc9caa40c8867de973fd4e48c6a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f2a3bc00.failed

The rcd array can be accessed from user context or during interrupts.
Protecting this with a mutex isn't a good idea because the mutex should
not be used from an IRQ.

Protect the allocation and freeing of rcd array elements with a
spinlock.

	Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Reviewed-by: Sebastian Sanchez <sebastian.sanchez@intel.com>
	Signed-off-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit f2a3bc00a03c2cc9caa40c8867de973fd4e48c6a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/file_ops.c
#	drivers/infiniband/hw/hfi1/hfi.h
#	drivers/infiniband/hw/hfi1/init.c
#	drivers/infiniband/hw/hfi1/vnic_main.c
diff --cc drivers/infiniband/hw/hfi1/file_ops.c
index 8df16f5d7e38,7361366d80e4..000000000000
--- a/drivers/infiniband/hw/hfi1/file_ops.c
+++ b/drivers/infiniband/hw/hfi1/file_ops.c
@@@ -70,31 -71,41 +70,63 @@@
  /*
   * File operation functions
   */
 -static int hfi1_file_open(struct inode *inode, struct file *fp);
 -static int hfi1_file_close(struct inode *inode, struct file *fp);
 -static ssize_t hfi1_write_iter(struct kiocb *kiocb, struct iov_iter *from);
 -static unsigned int hfi1_poll(struct file *fp, struct poll_table_struct *pt);
 -static int hfi1_file_mmap(struct file *fp, struct vm_area_struct *vma);
 -
 +static int hfi1_file_open(struct inode *, struct file *);
 +static int hfi1_file_close(struct inode *, struct file *);
 +static ssize_t hfi1_aio_write(struct kiocb *, const struct iovec *,
 +			      unsigned long, loff_t);
 +static unsigned int hfi1_poll(struct file *, struct poll_table_struct *);
 +static int hfi1_file_mmap(struct file *, struct vm_area_struct *);
 +
++<<<<<<< HEAD
 +static u64 kvirt_to_phys(void *);
 +static int assign_ctxt(struct file *, struct hfi1_user_info *);
 +static int init_subctxts(struct hfi1_ctxtdata *, const struct hfi1_user_info *);
 +static int user_init(struct file *);
 +static int get_ctxt_info(struct file *, void __user *, __u32);
 +static int get_base_info(struct file *, void __user *, __u32);
 +static int setup_ctxt(struct file *);
 +static int setup_subctxt(struct hfi1_ctxtdata *);
 +static int get_user_context(struct file *, struct hfi1_user_info *, int);
 +static int find_shared_ctxt(struct file *, const struct hfi1_user_info *);
 +static int allocate_ctxt(struct file *, struct hfi1_devdata *,
 +			 struct hfi1_user_info *);
 +static unsigned int poll_urgent(struct file *, struct poll_table_struct *);
 +static unsigned int poll_next(struct file *, struct poll_table_struct *);
 +static int user_event_ack(struct hfi1_ctxtdata *, int, unsigned long);
 +static int set_ctxt_pkey(struct hfi1_ctxtdata *, unsigned, u16);
 +static int manage_rcvq(struct hfi1_ctxtdata *, unsigned, int);
 +static int vma_fault(struct vm_area_struct *, struct vm_fault *);
++=======
+ static u64 kvirt_to_phys(void *addr);
+ static int assign_ctxt(struct hfi1_filedata *fd, struct hfi1_user_info *uinfo);
+ static void init_subctxts(struct hfi1_ctxtdata *uctxt,
+ 			  const struct hfi1_user_info *uinfo);
+ static int init_user_ctxt(struct hfi1_filedata *fd,
+ 			  struct hfi1_ctxtdata *uctxt);
+ static void user_init(struct hfi1_ctxtdata *uctxt);
+ static int get_ctxt_info(struct hfi1_filedata *fd, void __user *ubase,
+ 			 __u32 len);
+ static int get_base_info(struct hfi1_filedata *fd, void __user *ubase,
+ 			 __u32 len);
+ static int setup_base_ctxt(struct hfi1_filedata *fd,
+ 			   struct hfi1_ctxtdata *uctxt);
+ static int setup_subctxt(struct hfi1_ctxtdata *uctxt);
+ 
+ static int find_sub_ctxt(struct hfi1_filedata *fd,
+ 			 const struct hfi1_user_info *uinfo);
+ static int allocate_ctxt(struct hfi1_filedata *fd, struct hfi1_devdata *dd,
+ 			 struct hfi1_user_info *uinfo,
+ 			 struct hfi1_ctxtdata **cd);
+ static void deallocate_ctxt(struct hfi1_ctxtdata *uctxt);
+ static unsigned int poll_urgent(struct file *fp, struct poll_table_struct *pt);
+ static unsigned int poll_next(struct file *fp, struct poll_table_struct *pt);
+ static int user_event_ack(struct hfi1_ctxtdata *uctxt, u16 subctxt,
+ 			  unsigned long events);
+ static int set_ctxt_pkey(struct hfi1_ctxtdata *uctxt, u16 subctxt, u16 pkey);
+ static int manage_rcvq(struct hfi1_ctxtdata *uctxt, u16 subctxt,
+ 		       int start_stop);
+ static int vma_fault(struct vm_fault *vmf);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  static long hfi1_file_ioctl(struct file *fp, unsigned int cmd,
  			    unsigned long arg);
  
@@@ -770,8 -777,11 +801,16 @@@ static int hfi1_file_close(struct inod
  			   HFI1_MAX_SHARED_CTXTS) + fdata->subctxt;
  	*ev = 0;
  
++<<<<<<< HEAD
 +	if (--uctxt->cnt) {
 +		uctxt->active_slaves &= ~(1 << fdata->subctxt);
++=======
+ 	mutex_lock(&hfi1_mutex);
+ 	__clear_bit(fdata->subctxt, uctxt->in_use_ctxts);
+ 	fdata->uctxt = NULL;
+ 	hfi1_rcd_put(uctxt); /* fdata reference */
+ 	if (!bitmap_empty(uctxt->in_use_ctxts, HFI1_MAX_SHARED_CTXTS)) {
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  		mutex_unlock(&hfi1_mutex);
  		goto done;
  	}
@@@ -842,127 -844,158 +881,226 @@@ static u64 kvirt_to_phys(void *addr
  	return paddr;
  }
  
++<<<<<<< HEAD
 +static int assign_ctxt(struct file *fp, struct hfi1_user_info *uinfo)
++=======
+ static int complete_subctxt(struct hfi1_filedata *fd)
+ {
+ 	int ret;
+ 
+ 	/*
+ 	 * sub-context info can only be set up after the base context
+ 	 * has been completed.
+ 	 */
+ 	ret = wait_event_interruptible(
+ 		fd->uctxt->wait,
+ 		!test_bit(HFI1_CTXT_BASE_UNINIT, &fd->uctxt->event_flags));
+ 
+ 	if (test_bit(HFI1_CTXT_BASE_FAILED, &fd->uctxt->event_flags))
+ 		ret = -ENOMEM;
+ 
+ 	/* The only thing a sub context needs is the user_xxx stuff */
+ 	if (!ret) {
+ 		fd->rec_cpu_num = hfi1_get_proc_affinity(fd->uctxt->numa_id);
+ 		ret = init_user_ctxt(fd, fd->uctxt);
+ 	}
+ 
+ 	if (ret) {
+ 		hfi1_rcd_put(fd->uctxt);
+ 		fd->uctxt = NULL;
+ 		mutex_lock(&hfi1_mutex);
+ 		__clear_bit(fd->subctxt, fd->uctxt->in_use_ctxts);
+ 		mutex_unlock(&hfi1_mutex);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int assign_ctxt(struct hfi1_filedata *fd, struct hfi1_user_info *uinfo)
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  {
 -	int ret;
 +	int i_minor, ret = 0;
  	unsigned int swmajor, swminor;
 -	struct hfi1_ctxtdata *uctxt = NULL;
  
  	swmajor = uinfo->userversion >> 16;
 -	if (swmajor != HFI1_USER_SWMAJOR)
 -		return -ENODEV;
 +	if (swmajor != HFI1_USER_SWMAJOR) {
 +		ret = -ENODEV;
 +		goto done;
 +	}
  
+ 	if (uinfo->subctxt_cnt > HFI1_MAX_SHARED_CTXTS)
+ 		return -EINVAL;
+ 
  	swminor = uinfo->userversion & 0xffff;
  
+ 	/*
+ 	 * Acquire the mutex to protect against multiple creations of what
+ 	 * could be a shared base context.
+ 	 */
  	mutex_lock(&hfi1_mutex);
++<<<<<<< HEAD
 +	/* First, lets check if we need to setup a shared context? */
 +	if (uinfo->subctxt_cnt) {
 +		struct hfi1_filedata *fd = fp->private_data;
 +
 +		ret = find_shared_ctxt(fp, uinfo);
 +		if (ret < 0)
 +			goto done_unlock;
 +		if (ret) {
 +			fd->rec_cpu_num =
 +				hfi1_get_proc_affinity(fd->uctxt->numa_id);
 +		}
 +	}
 +
 +	/*
 +	 * We execute the following block if we couldn't find a
 +	 * shared context or if context sharing is not required.
++=======
+ 	/*
+ 	 * Get a sub context if available  (fd->uctxt will be set).
+ 	 * ret < 0 error, 0 no context, 1 sub-context found
+ 	 */
+ 	ret = find_sub_ctxt(fd, uinfo);
+ 
+ 	/*
+ 	 * Allocate a base context if context sharing is not required or a
+ 	 * sub context wasn't found.
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	 */
 -	if (!ret)
 -		ret = allocate_ctxt(fd, fd->dd, uinfo, &uctxt);
 -
 +	if (!ret) {
 +		i_minor = iminor(file_inode(fp)) - HFI1_USER_MINOR_BASE;
 +		ret = get_user_context(fp, uinfo, i_minor);
 +	}
 +done_unlock:
  	mutex_unlock(&hfi1_mutex);
++<<<<<<< HEAD
 +done:
 +	return ret;
 +}
 +
 +static int get_user_context(struct file *fp, struct hfi1_user_info *uinfo,
 +			    int devno)
++=======
+ 
+ 	/* Depending on the context type, do the appropriate init */
+ 	switch (ret) {
+ 	case 0:
+ 		ret = setup_base_ctxt(fd, uctxt);
+ 		if (uctxt->subctxt_cnt) {
+ 			/*
+ 			 * Base context is done, notify anybody using a
+ 			 * sub-context that is waiting for this completion
+ 			 */
+ 			clear_bit(HFI1_CTXT_BASE_UNINIT, &uctxt->event_flags);
+ 			wake_up(&uctxt->wait);
+ 		}
+ 		break;
+ 	case 1:
+ 		ret = complete_subctxt(fd);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * The hfi1_mutex must be held when this function is called.  It is
+  * necessary to ensure serialized creation of shared contexts.
+  */
+ static int find_sub_ctxt(struct hfi1_filedata *fd,
+ 			 const struct hfi1_user_info *uinfo)
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  {
 -	u16 i;
 -	struct hfi1_devdata *dd = fd->dd;
 -	u16 subctxt;
 +	struct hfi1_devdata *dd = NULL;
 +	int devmax, npresent, nup;
  
++<<<<<<< HEAD
 +	devmax = hfi1_count_units(&npresent, &nup);
 +	if (!npresent)
 +		return -ENXIO;
++=======
+ 	if (!uinfo->subctxt_cnt)
+ 		return 0;
+ 
+ 	for (i = dd->first_dyn_alloc_ctxt; i < dd->num_rcv_contexts; i++) {
+ 		struct hfi1_ctxtdata *uctxt = dd->rcd[i];
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  
 -		/* Skip ctxts which are not yet open */
 -		if (!uctxt ||
 -		    bitmap_empty(uctxt->in_use_ctxts,
 -				 HFI1_MAX_SHARED_CTXTS))
 -			continue;
 -
 -		/* Skip dynamically allocted kernel contexts */
 -		if (uctxt->sc && (uctxt->sc->type == SC_KERNEL))
 -			continue;
 +	if (!nup)
 +		return -ENETDOWN;
  
 -		/* Skip ctxt if it doesn't match the requested one */
 -		if (memcmp(uctxt->uuid, uinfo->uuid,
 -			   sizeof(uctxt->uuid)) ||
 -		    uctxt->jkey != generate_jkey(current_uid()) ||
 -		    uctxt->subctxt_id != uinfo->subctxt_id ||
 -		    uctxt->subctxt_cnt != uinfo->subctxt_cnt)
 -			continue;
 +	dd = hfi1_lookup(devno);
 +	if (!dd)
 +		return -ENODEV;
 +	else if (!dd->freectxts)
 +		return -EBUSY;
  
 -		/* Verify the sharing process matches the master */
 -		if (uctxt->userversion != uinfo->userversion)
 -			return -EINVAL;
 +	return allocate_ctxt(fp, dd, uinfo);
 +}
  
 -		/* Find an unused context */
 -		subctxt = find_first_zero_bit(uctxt->in_use_ctxts,
 -					      HFI1_MAX_SHARED_CTXTS);
 -		if (subctxt >= uctxt->subctxt_cnt)
 -			return -EBUSY;
 +static int find_shared_ctxt(struct file *fp,
 +			    const struct hfi1_user_info *uinfo)
 +{
 +	int devmax, ndev, i;
 +	int ret = 0;
 +	struct hfi1_filedata *fd = fp->private_data;
  
 -		fd->uctxt = uctxt;
 -		fd->subctxt = subctxt;
 +	devmax = hfi1_count_units(NULL, NULL);
  
 -		hfi1_rcd_get(uctxt);
 -		__set_bit(fd->subctxt, uctxt->in_use_ctxts);
 +	for (ndev = 0; ndev < devmax; ndev++) {
 +		struct hfi1_devdata *dd = hfi1_lookup(ndev);
  
 -		return 1;
 +		if (!(dd && (dd->flags & HFI1_PRESENT) && dd->kregbase))
 +			continue;
 +		for (i = dd->first_dyn_alloc_ctxt;
 +		     i < dd->num_rcv_contexts; i++) {
 +			struct hfi1_ctxtdata *uctxt = dd->rcd[i];
 +
 +			/* Skip ctxts which are not yet open */
 +			if (!uctxt || !uctxt->cnt)
 +				continue;
 +
 +			/* Skip dynamically allocted kernel contexts */
 +			if (uctxt->sc && (uctxt->sc->type == SC_KERNEL))
 +				continue;
 +
 +			/* Skip ctxt if it doesn't match the requested one */
 +			if (memcmp(uctxt->uuid, uinfo->uuid,
 +				   sizeof(uctxt->uuid)) ||
 +			    uctxt->jkey != generate_jkey(current_uid()) ||
 +			    uctxt->subctxt_id != uinfo->subctxt_id ||
 +			    uctxt->subctxt_cnt != uinfo->subctxt_cnt)
 +				continue;
 +
 +			/* Verify the sharing process matches the master */
 +			if (uctxt->userversion != uinfo->userversion ||
 +			    uctxt->cnt >= uctxt->subctxt_cnt) {
 +				ret = -EINVAL;
 +				goto done;
 +			}
 +			fd->uctxt = uctxt;
 +			fd->subctxt  = uctxt->cnt++;
 +			uctxt->active_slaves |= 1 << fd->subctxt;
 +			ret = 1;
 +			goto done;
 +		}
  	}
  
 -	return 0;
 +done:
 +	return ret;
  }
  
 -static int allocate_ctxt(struct hfi1_filedata *fd, struct hfi1_devdata *dd,
 -			 struct hfi1_user_info *uinfo,
 -			 struct hfi1_ctxtdata **cd)
 +static int allocate_ctxt(struct file *fp, struct hfi1_devdata *dd,
 +			 struct hfi1_user_info *uinfo)
  {
 +	struct hfi1_filedata *fd = fp->private_data;
  	struct hfi1_ctxtdata *uctxt;
++<<<<<<< HEAD
 +	unsigned ctxt;
++=======
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	int ret, numa;
  
  	if (dd->flags & HFI1_FROZEN) {
@@@ -976,12 -1009,7 +1114,16 @@@
  		return -EIO;
  	}
  
++<<<<<<< HEAD
 +	for (ctxt = dd->first_dyn_alloc_ctxt;
 +	     ctxt < dd->num_rcv_contexts; ctxt++)
 +		if (!dd->rcd[ctxt])
 +			break;
 +
 +	if (ctxt == dd->num_rcv_contexts)
++=======
+ 	if (!dd->freectxts)
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  		return -EBUSY;
  
  	/*
@@@ -1019,20 -1045,13 +1159,30 @@@
  		goto ctxdata_free;
  
  	/*
++<<<<<<< HEAD
 +	 * Setup shared context resources if the user-level has requested
 +	 * shared contexts and this is the 'master' process.
++=======
+ 	 * Setup sub context information if the user-level has requested
+ 	 * sub contexts.
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	 * This has to be done here so the rest of the sub-contexts find the
- 	 * proper master.
+ 	 * proper base context.
  	 */
++<<<<<<< HEAD
 +	if (uinfo->subctxt_cnt && !fd->subctxt) {
 +		ret = init_subctxts(uctxt, uinfo);
 +		/*
 +		 * On error, we don't need to disable and de-allocate the
 +		 * send context because it will be done during file close
 +		 */
 +		if (ret)
 +			goto ctxdata_free;
 +	}
++=======
+ 	if (uinfo->subctxt_cnt)
+ 		init_subctxts(uctxt, uinfo);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	uctxt->userversion = uinfo->userversion;
  	uctxt->flags = hfi1_cap_mask; /* save current flag state */
  	init_waitqueue_head(&uctxt->wait);
@@@ -1053,27 -1071,27 +1203,51 @@@
  	return 0;
  
  ctxdata_free:
++<<<<<<< HEAD
 +	dd->rcd[ctxt] = NULL;
 +	hfi1_free_ctxtdata(dd, uctxt);
 +	return ret;
 +}
 +
 +static int init_subctxts(struct hfi1_ctxtdata *uctxt,
 +			 const struct hfi1_user_info *uinfo)
 +{
 +	unsigned num_subctxts;
 +
 +	num_subctxts = uinfo->subctxt_cnt;
 +	if (num_subctxts > HFI1_MAX_SHARED_CTXTS)
 +		return -EINVAL;
 +
 +	uctxt->subctxt_cnt = uinfo->subctxt_cnt;
 +	uctxt->subctxt_id = uinfo->subctxt_id;
 +	uctxt->active_slaves = 1;
 +	uctxt->redirect_seq_cnt = 1;
 +	set_bit(HFI1_CTXT_MASTER_UNINIT, &uctxt->event_flags);
 +
 +	return 0;
++=======
+ 	hfi1_free_ctxt(dd, uctxt);
+ 	return ret;
+ }
+ 
+ static void deallocate_ctxt(struct hfi1_ctxtdata *uctxt)
+ {
+ 	mutex_lock(&hfi1_mutex);
+ 	hfi1_stats.sps_ctxts--;
+ 	if (++uctxt->dd->freectxts == uctxt->dd->num_user_contexts)
+ 		aspm_enable_all(uctxt->dd);
+ 	mutex_unlock(&hfi1_mutex);
+ 
+ 	hfi1_free_ctxt(uctxt->dd, uctxt);
+ }
+ 
+ static void init_subctxts(struct hfi1_ctxtdata *uctxt,
+ 			  const struct hfi1_user_info *uinfo)
+ {
+ 	uctxt->subctxt_cnt = uinfo->subctxt_cnt;
+ 	uctxt->subctxt_id = uinfo->subctxt_id;
+ 	set_bit(HFI1_CTXT_BASE_UNINIT, &uctxt->event_flags);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  }
  
  static int setup_subctxt(struct hfi1_ctxtdata *uctxt)
@@@ -1224,54 -1245,42 +1398,82 @@@ static int setup_ctxt(struct file *fp
  	struct hfi1_devdata *dd = uctxt->dd;
  	int ret = 0;
  
 -	hfi1_init_ctxt(uctxt->sc);
 +	/*
 +	 * Context should be set up only once, including allocation and
 +	 * programming of eager buffers. This is done if context sharing
 +	 * is not requested or by the master process.
 +	 */
 +	if (!uctxt->subctxt_cnt || !fd->subctxt) {
 +		ret = hfi1_init_ctxt(uctxt->sc);
 +		if (ret)
 +			goto done;
  
 -	/* Now allocate the RcvHdr queue and eager buffers. */
 -	ret = hfi1_create_rcvhdrq(dd, uctxt);
 -	if (ret)
 -		return ret;
 +		/* Now allocate the RcvHdr queue and eager buffers. */
 +		ret = hfi1_create_rcvhdrq(dd, uctxt);
 +		if (ret)
 +			goto done;
 +		ret = hfi1_setup_eagerbufs(uctxt);
 +		if (ret)
 +			goto done;
 +		if (uctxt->subctxt_cnt && !fd->subctxt) {
 +			ret = setup_subctxt(uctxt);
 +			if (ret)
 +				goto done;
 +		}
 +	} else {
 +		ret = wait_event_interruptible(uctxt->wait, !test_bit(
 +					       HFI1_CTXT_MASTER_UNINIT,
 +					       &uctxt->event_flags));
 +		if (ret)
 +			goto done;
 +	}
  
 -	ret = hfi1_setup_eagerbufs(uctxt);
 +	ret = hfi1_user_sdma_alloc_queues(uctxt, fp);
  	if (ret)
 -		goto setup_failed;
 +		goto done;
 +	/*
 +	 * Expected receive has to be setup for all processes (including
 +	 * shared contexts). However, it has to be done after the master
 +	 * context has been fully configured as it depends on the
 +	 * eager/expected split of the RcvArray entries.
 +	 * Setting it up here ensures that the subcontexts will be waiting
 +	 * (due to the above wait_event_interruptible() until the master
 +	 * is setup.
 +	 */
 +	ret = hfi1_user_exp_rcv_init(fp);
 +	if (ret)
 +		goto done;
  
++<<<<<<< HEAD
 +	set_bit(HFI1_CTXT_SETUP_DONE, &uctxt->event_flags);
 +done:
++=======
+ 	/* If sub-contexts are enabled, do the appropriate setup */
+ 	if (uctxt->subctxt_cnt)
+ 		ret = setup_subctxt(uctxt);
+ 	if (ret)
+ 		goto setup_failed;
+ 
+ 	ret = hfi1_alloc_ctxt_rcv_groups(uctxt);
+ 	if (ret)
+ 		goto setup_failed;
+ 
+ 	ret = init_user_ctxt(fd, uctxt);
+ 	if (ret)
+ 		goto setup_failed;
+ 
+ 	user_init(uctxt);
+ 
+ 	/* Now that the context is set up, the fd can get a reference. */
+ 	fd->uctxt = uctxt;
+ 	hfi1_rcd_get(uctxt);
+ 
+ 	return 0;
+ 
+ setup_failed:
+ 	set_bit(HFI1_CTXT_BASE_FAILED, &uctxt->event_flags);
+ 	deallocate_ctxt(uctxt);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	return ret;
  }
  
diff --cc drivers/infiniband/hw/hfi1/hfi.h
index 9719cf207532,bb003ff1df96..000000000000
--- a/drivers/infiniband/hw/hfi1/hfi.h
+++ b/drivers/infiniband/hw/hfi1/hfi.h
@@@ -296,10 -273,8 +296,9 @@@ struct hfi1_ctxtdata 
  	u16 poll_type;
  	/* receive packet sequence counter */
  	u8 seq_cnt;
- 	u8 redirect_seq_cnt;
  	/* ctxt rcvhdrq head offset */
  	u32 head;
 +	u32 pkt_count;
  	/* QPs waiting for context processing */
  	struct list_head qp_wait_list;
  	/* interrupt handling */
@@@ -1283,17 -1260,20 +1282,34 @@@ void handle_linkup_change(struct hfi1_d
  
  void handle_user_interrupt(struct hfi1_ctxtdata *rcd);
  
++<<<<<<< HEAD
 +int hfi1_create_rcvhdrq(struct hfi1_devdata *, struct hfi1_ctxtdata *);
 +int hfi1_setup_eagerbufs(struct hfi1_ctxtdata *);
 +int hfi1_create_ctxts(struct hfi1_devdata *dd);
 +struct hfi1_ctxtdata *hfi1_create_ctxtdata(struct hfi1_pportdata *, u32, int);
 +void hfi1_init_pportdata(struct pci_dev *, struct hfi1_pportdata *,
 +			 struct hfi1_devdata *, u8, u8);
 +void hfi1_free_ctxtdata(struct hfi1_devdata *, struct hfi1_ctxtdata *);
 +
 +int handle_receive_interrupt(struct hfi1_ctxtdata *, int);
 +int handle_receive_interrupt_nodma_rtail(struct hfi1_ctxtdata *, int);
 +int handle_receive_interrupt_dma_rtail(struct hfi1_ctxtdata *, int);
++=======
+ int hfi1_create_rcvhdrq(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd);
+ int hfi1_setup_eagerbufs(struct hfi1_ctxtdata *rcd);
+ int hfi1_create_kctxts(struct hfi1_devdata *dd);
+ int hfi1_create_ctxtdata(struct hfi1_pportdata *ppd, int numa,
+ 			 struct hfi1_ctxtdata **rcd);
+ void hfi1_free_ctxt(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd);
+ void hfi1_init_pportdata(struct pci_dev *pdev, struct hfi1_pportdata *ppd,
+ 			 struct hfi1_devdata *dd, u8 hw_pidx, u8 port);
+ void hfi1_free_ctxtdata(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd);
+ int hfi1_rcd_put(struct hfi1_ctxtdata *rcd);
+ void hfi1_rcd_get(struct hfi1_ctxtdata *rcd);
+ int handle_receive_interrupt(struct hfi1_ctxtdata *rcd, int thread);
+ int handle_receive_interrupt_nodma_rtail(struct hfi1_ctxtdata *rcd, int thread);
+ int handle_receive_interrupt_dma_rtail(struct hfi1_ctxtdata *rcd, int thread);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  void set_all_slowpath(struct hfi1_devdata *dd);
  void hfi1_vnic_synchronize_irq(struct hfi1_devdata *dd);
  void hfi1_set_vnic_msix_info(struct hfi1_ctxtdata *rcd);
diff --cc drivers/infiniband/hw/hfi1/init.c
index 3bed53851d35,23f0bbc9c436..000000000000
--- a/drivers/infiniband/hw/hfi1/init.c
+++ b/drivers/infiniband/hw/hfi1/init.c
@@@ -124,91 -126,164 +124,236 @@@ static struct idr hfi1_unit_table
  u32 hfi1_cpulist_count;
  unsigned long *hfi1_cpulist;
  
- /*
-  * Common code for creating the receive context array.
-  */
- int hfi1_create_ctxts(struct hfi1_devdata *dd)
+ static int hfi1_create_kctxt(struct hfi1_devdata *dd,
+ 			     struct hfi1_pportdata *ppd)
  {
++<<<<<<< HEAD
 +	unsigned i;
++=======
+ 	struct hfi1_ctxtdata *rcd;
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	int ret;
  
  	/* Control context has to be always 0 */
  	BUILD_BUG_ON(HFI1_CTRL_CTXT != 0);
  
++<<<<<<< HEAD
 +	dd->rcd = kzalloc_node(dd->num_rcv_contexts * sizeof(*dd->rcd),
 +			       GFP_KERNEL, dd->node);
 +	if (!dd->rcd)
 +		goto nomem;
 +
 +	/* create one or more kernel contexts */
 +	for (i = 0; i < dd->first_dyn_alloc_ctxt; ++i) {
 +		struct hfi1_pportdata *ppd;
 +		struct hfi1_ctxtdata *rcd;
 +
 +		ppd = dd->pport + (i % dd->num_pports);
 +
 +		/* dd->rcd[i] gets assigned inside the callee */
 +		rcd = hfi1_create_ctxtdata(ppd, i, dd->node);
 +		if (!rcd) {
 +			dd_dev_err(dd,
 +				   "Unable to allocate kernel receive context, failing\n");
 +			goto nomem;
 +		}
 +		/*
 +		 * Set up the kernel context flags here and now because they
 +		 * use default values for all receive side memories.  User
 +		 * contexts will be handled as they are created.
 +		 */
 +		rcd->flags = HFI1_CAP_KGET(MULTI_PKT_EGR) |
 +			HFI1_CAP_KGET(NODROP_RHQ_FULL) |
 +			HFI1_CAP_KGET(NODROP_EGR_FULL) |
 +			HFI1_CAP_KGET(DMA_RTAIL);
 +
 +		/* Control context must use DMA_RTAIL */
 +		if (rcd->ctxt == HFI1_CTRL_CTXT)
 +			rcd->flags |= HFI1_CAP_DMA_RTAIL;
 +		rcd->seq_cnt = 1;
 +
 +		rcd->sc = sc_alloc(dd, SC_ACK, rcd->rcvhdrqentsize, dd->node);
 +		if (!rcd->sc) {
 +			dd_dev_err(dd,
 +				   "Unable to allocate kernel send context, failing\n");
 +			goto nomem;
 +		}
 +
 +		ret = hfi1_init_ctxt(rcd->sc);
 +		if (ret < 0) {
 +			dd_dev_err(dd,
 +				   "Failed to setup kernel receive context, failing\n");
 +			ret = -EFAULT;
 +			goto bail;
 +		}
++=======
+ 	ret = hfi1_create_ctxtdata(ppd, dd->node, &rcd);
+ 	if (ret < 0) {
+ 		dd_dev_err(dd, "Kernel receive context allocation failed\n");
+ 		return ret;
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	}
  
  	/*
- 	 * Initialize aspm, to be done after gen3 transition and setting up
- 	 * contexts and before enabling interrupts
+ 	 * Set up the kernel context flags here and now because they use
+ 	 * default values for all receive side memories.  User contexts will
+ 	 * be handled as they are created.
  	 */
- 	aspm_init(dd);
+ 	rcd->flags = HFI1_CAP_KGET(MULTI_PKT_EGR) |
+ 		HFI1_CAP_KGET(NODROP_RHQ_FULL) |
+ 		HFI1_CAP_KGET(NODROP_EGR_FULL) |
+ 		HFI1_CAP_KGET(DMA_RTAIL);
+ 
+ 	/* Control context must use DMA_RTAIL */
+ 	if (rcd->ctxt == HFI1_CTRL_CTXT)
+ 		rcd->flags |= HFI1_CAP_DMA_RTAIL;
+ 	rcd->seq_cnt = 1;
+ 
+ 	rcd->sc = sc_alloc(dd, SC_ACK, rcd->rcvhdrqentsize, dd->node);
+ 	if (!rcd->sc) {
+ 		dd_dev_err(dd, "Kernel send context allocation failed\n");
+ 		return -ENOMEM;
+ 	}
+ 	hfi1_init_ctxt(rcd->sc);
  
  	return 0;
++<<<<<<< HEAD
 +nomem:
 +	ret = -ENOMEM;
 +bail:
 +	if (dd->rcd) {
 +		for (i = 0; i < dd->num_rcv_contexts; ++i)
 +			hfi1_free_ctxtdata(dd, dd->rcd[i]);
 +	}
++=======
+ }
+ 
+ /*
+  * Create the receive context array and one or more kernel contexts
+  */
+ int hfi1_create_kctxts(struct hfi1_devdata *dd)
+ {
+ 	u16 i;
+ 	int ret;
+ 
+ 	dd->rcd = kzalloc_node(dd->num_rcv_contexts * sizeof(*dd->rcd),
+ 			       GFP_KERNEL, dd->node);
+ 	if (!dd->rcd)
+ 		return -ENOMEM;
+ 
+ 	for (i = 0; i < dd->first_dyn_alloc_ctxt; ++i) {
+ 		ret = hfi1_create_kctxt(dd, dd->pport);
+ 		if (ret)
+ 			goto bail;
+ 	}
+ 
+ 	return 0;
+ bail:
+ 	for (i = 0; dd->rcd && i < dd->first_dyn_alloc_ctxt; ++i)
+ 		hfi1_rcd_put(dd->rcd[i]);
+ 
+ 	/* All the contexts should be freed, free the array */
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	kfree(dd->rcd);
  	dd->rcd = NULL;
  	return ret;
  }
  
  /*
++<<<<<<< HEAD
 + * Common code for user and kernel context setup.
 + */
 +struct hfi1_ctxtdata *hfi1_create_ctxtdata(struct hfi1_pportdata *ppd, u32 ctxt,
 +					   int numa)
++=======
+  * Helper routines for the receive context reference count (rcd and uctxt)
+  */
+ static void hfi1_rcd_init(struct hfi1_ctxtdata *rcd)
+ {
+ 	kref_init(&rcd->kref);
+ }
+ 
+ /**
+  * hfi1_rcd_free - When reference is zero clean up.
+  * @kref: pointer to an initialized rcd data structure
+  *
+  */
+ static void hfi1_rcd_free(struct kref *kref)
+ {
+ 	struct hfi1_ctxtdata *rcd =
+ 		container_of(kref, struct hfi1_ctxtdata, kref);
+ 
+ 	hfi1_free_ctxtdata(rcd->dd, rcd);
+ 	kfree(rcd);
+ }
+ 
+ /**
+  * hfi1_rcd_put - decrement reference for rcd
+  * @rcd: pointer to an initialized rcd data structure
+  *
+  * Use this to put a reference after the init.
+  */
+ int hfi1_rcd_put(struct hfi1_ctxtdata *rcd)
+ {
+ 	if (rcd)
+ 		return kref_put(&rcd->kref, hfi1_rcd_free);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * hfi1_rcd_get - increment reference for rcd
+  * @rcd: pointer to an initialized rcd data structure
+  *
+  * Use this to get a reference after the init.
+  */
+ void hfi1_rcd_get(struct hfi1_ctxtdata *rcd)
+ {
+ 	kref_get(&rcd->kref);
+ }
+ 
+ /**
+  * allocate_rcd_index - allocate an rcd index from the rcd array
+  * @dd: pointer to a valid devdata structure
+  * @rcd: rcd data structure to assign
+  * @index: pointer to index that is allocated
+  *
+  * Find an empty index in the rcd array, and assign the given rcd to it.
+  * If the array is full, we are EBUSY.
+  *
+  */
+ static u16 allocate_rcd_index(struct hfi1_devdata *dd,
+ 			      struct hfi1_ctxtdata *rcd, u16 *index)
+ {
+ 	unsigned long flags;
+ 	u16 ctxt;
+ 
+ 	spin_lock_irqsave(&dd->uctxt_lock, flags);
+ 	for (ctxt = 0; ctxt < dd->num_rcv_contexts; ctxt++)
+ 		if (!dd->rcd[ctxt])
+ 			break;
+ 
+ 	if (ctxt < dd->num_rcv_contexts) {
+ 		rcd->ctxt = ctxt;
+ 		dd->rcd[ctxt] = rcd;
+ 		hfi1_rcd_init(rcd);
+ 	}
+ 	spin_unlock_irqrestore(&dd->uctxt_lock, flags);
+ 
+ 	if (ctxt >= dd->num_rcv_contexts)
+ 		return -EBUSY;
+ 
+ 	*index = ctxt;
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * Common code for user and kernel context setup.
+  */
+ int hfi1_create_ctxtdata(struct hfi1_pportdata *ppd, int numa,
+ 			 struct hfi1_ctxtdata **context)
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  {
  	struct hfi1_devdata *dd = ppd->dd;
  	struct hfi1_ctxtdata *rcd;
@@@ -225,12 -302,20 +372,23 @@@
  
  		hfi1_cdbg(PROC, "setting up context %u\n", ctxt);
  
+ 		ret = allocate_rcd_index(dd, rcd, &ctxt);
+ 		if (ret) {
+ 			*context = NULL;
+ 			kfree(rcd);
+ 			return ret;
+ 		}
+ 
  		INIT_LIST_HEAD(&rcd->qp_wait_list);
 -		hfi1_exp_tid_group_init(&rcd->tid_group_list);
 -		hfi1_exp_tid_group_init(&rcd->tid_used_list);
 -		hfi1_exp_tid_group_init(&rcd->tid_full_list);
  		rcd->ppd = ppd;
  		rcd->dd = dd;
++<<<<<<< HEAD
 +		rcd->cnt = 1;
 +		rcd->ctxt = ctxt;
 +		dd->rcd[ctxt] = rcd;
++=======
+ 		__set_bit(0, rcd->in_use_ctxts);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  		rcd->numa_id = numa;
  		rcd->rcv_array_groups = dd->rcv_entries.ngroups;
  
@@@ -333,14 -418,35 +491,38 @@@
  			if (!rcd->opstats)
  				goto bail;
  		}
++<<<<<<< HEAD
++=======
+ 
+ 		*context = rcd;
+ 		return 0;
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	}
- 	return rcd;
+ 
  bail:
- 	dd->rcd[ctxt] = NULL;
- 	kfree(rcd->egrbufs.rcvtids);
- 	kfree(rcd->egrbufs.buffers);
- 	kfree(rcd);
- 	return NULL;
+ 	*context = NULL;
+ 	hfi1_free_ctxt(dd, rcd);
+ 	return -ENOMEM;
+ }
+ 
+ /**
+  * hfi1_free_ctxt
+  * @dd: Pointer to a valid device
+  * @rcd: pointer to an initialized rcd data structure
+  *
+  * This is the "free" to match the _create_ctxtdata (alloc) function.
+  * This is the final "put" for the kref.
+  */
+ void hfi1_free_ctxt(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd)
+ {
+ 	unsigned long flags;
+ 
+ 	if (rcd) {
+ 		spin_lock_irqsave(&dd->uctxt_lock, flags);
+ 		dd->rcd[rcd->ctxt] = NULL;
+ 		spin_unlock_irqrestore(&dd->uctxt_lock, flags);
+ 		hfi1_rcd_put(rcd);
+ 	}
  }
  
  /*
diff --cc drivers/infiniband/hw/hfi1/vnic_main.c
index b1572c795c35,c91456c16cdb..000000000000
--- a/drivers/infiniband/hw/hfi1/vnic_main.c
+++ b/drivers/infiniband/hw/hfi1/vnic_main.c
@@@ -110,7 -106,6 +110,10 @@@ static int allocate_vnic_ctxt(struct hf
  			      struct hfi1_ctxtdata **vnic_ctxt)
  {
  	struct hfi1_ctxtdata *uctxt;
++<<<<<<< HEAD
 +	unsigned int ctxt;
++=======
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	int ret;
  
  	if (dd->flags & HFI1_FROZEN)
@@@ -160,11 -147,10 +155,18 @@@
  	return ret;
  bail:
  	/*
++<<<<<<< HEAD
 +	 * hfi1_free_ctxtdata() also releases send_context
 +	 * structure if uctxt->sc is not null
 +	 */
 +	dd->rcd[uctxt->ctxt] = NULL;
 +	hfi1_free_ctxtdata(dd, uctxt);
++=======
+ 	 * hfi1_free_ctxt() will call hfi1_free_ctxtdata(), which will
+ 	 * release send_context structure if uctxt->sc is not null
+ 	 */
+ 	hfi1_free_ctxt(dd, uctxt);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  	dd_dev_dbg(dd, "vnic allocation failed. rc %d\n", ret);
  	return ret;
  }
@@@ -212,7 -197,8 +213,12 @@@ static void deallocate_vnic_ctxt(struc
  	hfi1_clear_ctxt_pkey(dd, uctxt);
  
  	hfi1_stats.sps_ctxts--;
++<<<<<<< HEAD
 +	hfi1_free_ctxtdata(dd, uctxt);
++=======
+ 
+ 	hfi1_free_ctxt(dd, uctxt);
++>>>>>>> f2a3bc00a03c (IB/hfi1: Protect context array set/clear with spinlock)
  }
  
  void hfi1_vnic_setup(struct hfi1_devdata *dd)
diff --git a/drivers/infiniband/hw/hfi1/chip.c b/drivers/infiniband/hw/hfi1/chip.c
index 7ed61dba0ba4..e871da104ac8 100644
--- a/drivers/infiniband/hw/hfi1/chip.c
+++ b/drivers/infiniband/hw/hfi1/chip.c
@@ -14979,10 +14979,16 @@ struct hfi1_devdata *hfi1_init_dd(struct pci_dev *pdev,
 	if (ret)
 		goto bail_cleanup;
 
-	ret = hfi1_create_ctxts(dd);
+	ret = hfi1_create_kctxts(dd);
 	if (ret)
 		goto bail_cleanup;
 
+	/*
+	 * Initialize aspm, to be done after gen3 transition and setting up
+	 * contexts and before enabling interrupts
+	 */
+	aspm_init(dd);
+
 	dd->rcvhdrsize = DEFAULT_RCVHDRSIZE;
 	/*
 	 * rcd[0] is guaranteed to be valid by this point. Also, all
@@ -15001,7 +15007,7 @@ struct hfi1_devdata *hfi1_init_dd(struct pci_dev *pdev,
 			goto bail_cleanup;
 	}
 
-	/* use contexts created by hfi1_create_ctxts */
+	/* use contexts created by hfi1_create_kctxts */
 	ret = set_up_interrupts(dd);
 	if (ret)
 		goto bail_cleanup;
* Unmerged path drivers/infiniband/hw/hfi1/file_ops.c
* Unmerged path drivers/infiniband/hw/hfi1/hfi.h
* Unmerged path drivers/infiniband/hw/hfi1/init.c
* Unmerged path drivers/infiniband/hw/hfi1/vnic_main.c
