blk-mq: export some helpers we need to the scheduling framework

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 2c3ad667902ef6f4b60ef0a3c6f7d8c2b007769a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/2c3ad667.failed

	Signed-off-by: Jens Axboe <axboe@fb.com>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Reviewed-by: Omar Sandoval <osandov@fb.com>
(cherry picked from commit 2c3ad667902ef6f4b60ef0a3c6f7d8c2b007769a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 5b92b7659b74,9fc521755e22..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -195,12 -167,9 +195,17 @@@ bool blk_mq_can_queue(struct blk_mq_hw_
  }
  EXPORT_SYMBOL(blk_mq_can_queue);
  
++<<<<<<< HEAD
 +static void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
 +			       struct request *rq, unsigned int rw_flags)
++=======
+ void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
+ 			struct request *rq, unsigned int op)
++>>>>>>> 2c3ad667902e (blk-mq: export some helpers we need to the scheduling framework)
  {
 +	if (blk_queue_io_stat(q))
 +		rw_flags |= REQ_IO_STAT;
 +
  	INIT_LIST_HEAD(&rq->queuelist);
  	/* csd/requeue_work/fifo_time is initialized before use */
  	rq->q = q;
@@@ -240,11 -211,12 +245,17 @@@
  	rq->end_io_data = NULL;
  	rq->next_rq = NULL;
  
 -	ctx->rq_dispatched[op_is_sync(op)]++;
 +	ctx->rq_dispatched[rw_is_sync(rw_flags)]++;
  }
+ EXPORT_SYMBOL_GPL(blk_mq_rq_ctx_init);
  
++<<<<<<< HEAD
 +static struct request *
 +__blk_mq_alloc_request(struct blk_mq_alloc_data *data, int rw)
++=======
+ struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data,
+ 				       unsigned int op)
++>>>>>>> 2c3ad667902e (blk-mq: export some helpers we need to the scheduling framework)
  {
  	struct request *rq;
  	unsigned int tag;
@@@ -786,35 -804,16 +798,36 @@@ static bool blk_mq_attempt_merge(struc
   * Process software queues that have been marked busy, splicing them
   * to the for-dispatch
   */
- static void flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
+ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
  {
 -	struct flush_busy_ctx_data data = {
 -		.hctx = hctx,
 -		.list = list,
 -	};
 +	struct blk_mq_ctx *ctx;
 +	int i;
  
 -	sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
 +	for (i = 0; i < hctx->ctx_map.size; i++) {
 +		struct blk_align_bitmap *bm = &hctx->ctx_map.map[i];
 +		unsigned int off, bit;
 +
 +		if (!bm->word)
 +			continue;
 +
 +		bit = 0;
 +		off = i * hctx->ctx_map.bits_per_word;
 +		do {
 +			bit = find_next_bit(&bm->word, bm->depth, bit);
 +			if (bit >= bm->depth)
 +				break;
 +
 +			ctx = hctx->ctxs[bit + off];
 +			clear_bit(bit, &bm->word);
 +			spin_lock(&ctx->lock);
 +			list_splice_tail_init(&ctx->rq_list, list);
 +			spin_unlock(&ctx->lock);
 +
 +			bit++;
 +		} while (1);
 +	}
  }
+ EXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);
  
  static inline unsigned int queued_to_index(unsigned int queued)
  {
@@@ -1545,19 -1550,11 +1558,24 @@@ run_queue
  	}
  
  	blk_mq_put_ctx(data.ctx);
 -	return cookie;
  }
  
++<<<<<<< HEAD
 +/*
 + * Default mapping to a software queue, since we use one per CPU.
 + */
 +struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q, const int cpu)
 +{
 +	return q->queue_hw_ctx[q->mq_map[cpu]];
 +}
 +EXPORT_SYMBOL(blk_mq_map_queue);
 +
 +static void blk_mq_free_rq_map(struct blk_mq_tag_set *set,
 +		struct blk_mq_tags *tags, unsigned int hctx_idx)
++=======
+ void blk_mq_free_rq_map(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+ 			unsigned int hctx_idx)
++>>>>>>> 2c3ad667902e (blk-mq: export some helpers we need to the scheduling framework)
  {
  	struct page *page;
  
@@@ -2333,7 -2262,37 +2351,41 @@@ static int blk_mq_queue_reinit_notify(s
  		blk_mq_unfreeze_queue(q);
  
  	mutex_unlock(&all_q_mutex);
++<<<<<<< HEAD
 +	return NOTIFY_OK;
++=======
+ }
+ 
+ static int blk_mq_queue_reinit_dead(unsigned int cpu)
+ {
+ 	cpumask_copy(&cpuhp_online_new, cpu_online_mask);
+ 	blk_mq_queue_reinit_work();
+ 	return 0;
+ }
+ 
+ /*
+  * Before hotadded cpu starts handling requests, new mappings must be
+  * established.  Otherwise, these requests in hw queue might never be
+  * dispatched.
+  *
+  * For example, there is a single hw queue (hctx) and two CPU queues (ctx0
+  * for CPU0, and ctx1 for CPU1).
+  *
+  * Now CPU1 is just onlined and a request is inserted into ctx1->rq_list
+  * and set bit0 in pending bitmap as ctx1->index_hw is still zero.
+  *
+  * And then while running hw queue, blk_mq_flush_busy_ctxs() finds bit0 is set
+  * in pending bitmap and tries to retrieve requests in hctx->ctxs[0]->rq_list.
+  * But htx->ctxs[0] is a pointer to ctx0, so the request in ctx1->rq_list is
+  * ignored.
+  */
+ static int blk_mq_queue_reinit_prepare(unsigned int cpu)
+ {
+ 	cpumask_copy(&cpuhp_online_new, cpu_online_mask);
+ 	cpumask_set_cpu(cpu, &cpuhp_online_new);
+ 	blk_mq_queue_reinit_work();
+ 	return 0;
++>>>>>>> 2c3ad667902e (blk-mq: export some helpers we need to the scheduling framework)
  }
  
  static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
* Unmerged path block/blk-mq.c
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 2d50f02667c4..444d4bcaa149 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -34,6 +34,21 @@ void blk_mq_free_queue(struct request_queue *q);
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
 void blk_mq_wake_waiters(struct request_queue *q);
 bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
+void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
+
+/*
+ * Internal helpers for allocating/freeing the request map
+ */
+void blk_mq_free_rq_map(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+			unsigned int hctx_idx);
+struct blk_mq_tags *blk_mq_init_rq_map(struct blk_mq_tag_set *set,
+					unsigned int hctx_idx);
+
+/*
+ * Internal helpers for request insertion into sw queues
+ */
+void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
+				bool at_head);
 
 /*
  * CPU hotplug helpers
@@ -116,6 +131,16 @@ static inline void blk_mq_set_alloc_data(struct blk_mq_alloc_data *data,
 	data->hctx = hctx;
 }
 
+/*
+ * Internal helpers for request allocation/init/free
+ */
+void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
+			struct request *rq, unsigned int op);
+void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
+				struct request *rq);
+struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data,
+					unsigned int op);
+
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
