x86/mce: Improve memcpy_mcsafe()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] mce: Improve memcpy_mcsafe() (Jeff Moyer) [1437205]
Rebuild_FUZZ: 93.33%
commit-author Tony Luck <tony.luck@intel.com>
commit 9a6fb28a355d2609ace4dab4e6425442c647894d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/9a6fb28a.failed

Use the mcsafe_key defined in the previous patch to make decisions on which
copy function to use. We can't use the FEATURE bit any more because PCI
quirks run too late to affect the patching of code. So we use a static key.

Turn memcpy_mcsafe() into an inline function to make life easier for
callers. The assembly code that actually does the copy is now named
memcpy_mcsafe_unrolled()

	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Acked-by: Borislav Petkov <bp@suse.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Boris Petkov <bp@suse.de>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
Link: http://lkml.kernel.org/r/bfde2fc774e94f53d91b70a4321c85a0d33e7118.1472754712.git.tony.luck@intel.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 9a6fb28a355d2609ace4dab4e6425442c647894d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/pmem.h
#	arch/x86/include/asm/string_64.h
#	arch/x86/kernel/x8664_ksyms_64.c
#	arch/x86/lib/memcpy_64.S
diff --cc arch/x86/include/asm/pmem.h
index eda280cff1d3,2c1ebeb4d737..000000000000
--- a/arch/x86/include/asm/pmem.h
+++ b/arch/x86/include/asm/pmem.h
@@@ -44,19 -44,9 +44,23 @@@ static inline void arch_memcpy_to_pmem(
  		BUG();
  }
  
 -static inline int arch_memcpy_from_pmem(void *dst, const void *src, size_t n)
 +static inline int arch_memcpy_from_pmem(void *dst, const void *src,
 +		size_t n)
  {
++<<<<<<< HEAD
 +	/*
 +	 * KABI: until we can figure out how to shoe-horn mcsafe_memcpy
 +	 * into RHEL, we always perform the fallback.
 +	 */
 +#if 0
 +	if (static_cpu_has(X86_FEATURE_MCE_RECOVERY))
 +		return memcpy_mcsafe(dst, (void __force *) src, n);
 +#endif
 +	memcpy(dst, (void __force *) src, n);
 +	return 0;
++=======
+ 	return memcpy_mcsafe(dst, src, n);
++>>>>>>> 9a6fb28a355d (x86/mce: Improve memcpy_mcsafe())
  }
  
  /**
diff --cc arch/x86/include/asm/string_64.h
index 19e2c468fc2c,a164862d77e3..000000000000
--- a/arch/x86/include/asm/string_64.h
+++ b/arch/x86/include/asm/string_64.h
@@@ -63,6 -66,48 +63,51 @@@ char *strcpy(char *dest, const char *sr
  char *strcat(char *dest, const char *src);
  int strcmp(const char *cs, const char *ct);
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_KASAN) && !defined(__SANITIZE_ADDRESS__)
+ 
+ /*
+  * For files that not instrumented (e.g. mm/slub.c) we
+  * should use not instrumented version of mem* functions.
+  */
+ 
+ #undef memcpy
+ #define memcpy(dst, src, len) __memcpy(dst, src, len)
+ #define memmove(dst, src, len) __memmove(dst, src, len)
+ #define memset(s, c, n) __memset(s, c, n)
+ #endif
+ 
+ __must_check int memcpy_mcsafe_unrolled(void *dst, const void *src, size_t cnt);
+ DECLARE_STATIC_KEY_FALSE(mcsafe_key);
+ 
+ /**
+  * memcpy_mcsafe - copy memory with indication if a machine check happened
+  *
+  * @dst:	destination address
+  * @src:	source address
+  * @cnt:	number of bytes to copy
+  *
+  * Low level memory copy function that catches machine checks
+  * We only call into the "safe" function on systems that can
+  * actually do machine check recovery. Everyone else can just
+  * use memcpy().
+  *
+  * Return 0 for success, -EFAULT for fail
+  */
+ static __always_inline __must_check int
+ memcpy_mcsafe(void *dst, const void *src, size_t cnt)
+ {
+ #ifdef CONFIG_X86_MCE
+ 	if (static_branch_unlikely(&mcsafe_key))
+ 		return memcpy_mcsafe_unrolled(dst, src, cnt);
+ 	else
+ #endif
+ 		memcpy(dst, src, cnt);
+ 	return 0;
+ }
+ 
++>>>>>>> 9a6fb28a355d (x86/mce: Improve memcpy_mcsafe())
  #endif /* __KERNEL__ */
  
  #endif /* _ASM_X86_STRING_64_H */
diff --cc arch/x86/kernel/x8664_ksyms_64.c
index 72606f19c287,b2cee3d19477..000000000000
--- a/arch/x86/kernel/x8664_ksyms_64.c
+++ b/arch/x86/kernel/x8664_ksyms_64.c
@@@ -37,6 -38,8 +37,11 @@@ EXPORT_SYMBOL(__copy_user_nocache)
  EXPORT_SYMBOL(_copy_from_user);
  EXPORT_SYMBOL(_copy_to_user);
  
++<<<<<<< HEAD
++=======
+ EXPORT_SYMBOL_GPL(memcpy_mcsafe_unrolled);
+ 
++>>>>>>> 9a6fb28a355d (x86/mce: Improve memcpy_mcsafe())
  EXPORT_SYMBOL(copy_page);
  EXPORT_SYMBOL(clear_page);
  
diff --cc arch/x86/lib/memcpy_64.S
index 56313a326188,49e6ebac7e73..000000000000
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@@ -180,27 -177,121 +180,133 @@@ ENTRY(memcpy
  
  .Lend:
  	retq
++<<<<<<< HEAD
 +	CFI_ENDPROC
 +ENDPROC(memcpy)
 +ENDPROC(__memcpy)
++=======
+ ENDPROC(memcpy_orig)
+ 
+ #ifndef CONFIG_UML
+ /*
+  * memcpy_mcsafe_unrolled - memory copy with machine check exception handling
+  * Note that we only catch machine checks when reading the source addresses.
+  * Writes to target are posted and don't generate machine checks.
+  */
+ ENTRY(memcpy_mcsafe_unrolled)
+ 	cmpl $8, %edx
+ 	/* Less than 8 bytes? Go to byte copy loop */
+ 	jb .L_no_whole_words
+ 
+ 	/* Check for bad alignment of source */
+ 	testl $7, %esi
+ 	/* Already aligned */
+ 	jz .L_8byte_aligned
+ 
+ 	/* Copy one byte at a time until source is 8-byte aligned */
+ 	movl %esi, %ecx
+ 	andl $7, %ecx
+ 	subl $8, %ecx
+ 	negl %ecx
+ 	subl %ecx, %edx
+ .L_copy_leading_bytes:
+ 	movb (%rsi), %al
+ 	movb %al, (%rdi)
+ 	incq %rsi
+ 	incq %rdi
+ 	decl %ecx
+ 	jnz .L_copy_leading_bytes
+ 
+ .L_8byte_aligned:
+ 	/* Figure out how many whole cache lines (64-bytes) to copy */
+ 	movl %edx, %ecx
+ 	andl $63, %edx
+ 	shrl $6, %ecx
+ 	jz .L_no_whole_cache_lines
+ 
+ 	/* Loop copying whole cache lines */
+ .L_cache_w0: movq (%rsi), %r8
+ .L_cache_w1: movq 1*8(%rsi), %r9
+ .L_cache_w2: movq 2*8(%rsi), %r10
+ .L_cache_w3: movq 3*8(%rsi), %r11
+ 	movq %r8, (%rdi)
+ 	movq %r9, 1*8(%rdi)
+ 	movq %r10, 2*8(%rdi)
+ 	movq %r11, 3*8(%rdi)
+ .L_cache_w4: movq 4*8(%rsi), %r8
+ .L_cache_w5: movq 5*8(%rsi), %r9
+ .L_cache_w6: movq 6*8(%rsi), %r10
+ .L_cache_w7: movq 7*8(%rsi), %r11
+ 	movq %r8, 4*8(%rdi)
+ 	movq %r9, 5*8(%rdi)
+ 	movq %r10, 6*8(%rdi)
+ 	movq %r11, 7*8(%rdi)
+ 	leaq 64(%rsi), %rsi
+ 	leaq 64(%rdi), %rdi
+ 	decl %ecx
+ 	jnz .L_cache_w0
+ 
+ 	/* Are there any trailing 8-byte words? */
+ .L_no_whole_cache_lines:
+ 	movl %edx, %ecx
+ 	andl $7, %edx
+ 	shrl $3, %ecx
+ 	jz .L_no_whole_words
+ 
+ 	/* Copy trailing words */
+ .L_copy_trailing_words:
+ 	movq (%rsi), %r8
+ 	mov %r8, (%rdi)
+ 	leaq 8(%rsi), %rsi
+ 	leaq 8(%rdi), %rdi
+ 	decl %ecx
+ 	jnz .L_copy_trailing_words
+ 
+ 	/* Any trailing bytes? */
+ .L_no_whole_words:
+ 	andl %edx, %edx
+ 	jz .L_done_memcpy_trap
+ 
+ 	/* Copy trailing bytes */
+ 	movl %edx, %ecx
+ .L_copy_trailing_bytes:
+ 	movb (%rsi), %al
+ 	movb %al, (%rdi)
+ 	incq %rsi
+ 	incq %rdi
+ 	decl %ecx
+ 	jnz .L_copy_trailing_bytes
+ 
+ 	/* Copy successful. Return zero */
+ .L_done_memcpy_trap:
+ 	xorq %rax, %rax
+ 	ret
+ ENDPROC(memcpy_mcsafe_unrolled)
+ 
+ 	.section .fixup, "ax"
+ 	/* Return -EFAULT for any failure */
+ .L_memcpy_mcsafe_fail:
+ 	mov	$-EFAULT, %rax
+ 	ret
++>>>>>>> 9a6fb28a355d (x86/mce: Improve memcpy_mcsafe())
  
 +	/*
 +	 * Some CPUs are adding enhanced REP MOVSB/STOSB feature
 +	 * If the feature is supported, memcpy_c_e() is the first choice.
 +	 * If enhanced rep movsb copy is not available, use fast string copy
 +	 * memcpy_c() when possible. This is faster and code is simpler than
 +	 * original memcpy().
 +	 * Otherwise, original memcpy() is used.
 +	 * In .altinstructions section, ERMS feature is placed after REG_GOOD
 +         * feature to implement the right patch order.
 +	 *
 +	 * Replace only beginning, memcpy is used to apply alternatives,
 +	 * so it is silly to overwrite itself with nops - reboot is the
 +	 * only outcome...
 +	 */
 +	.section .altinstructions, "a"
 +	altinstruction_entry memcpy,.Lmemcpy_c,X86_FEATURE_REP_GOOD,\
 +			     .Lmemcpy_e-.Lmemcpy_c,.Lmemcpy_e-.Lmemcpy_c
 +	altinstruction_entry memcpy,.Lmemcpy_c_e,X86_FEATURE_ERMS, \
 +			     .Lmemcpy_e_e-.Lmemcpy_c_e,.Lmemcpy_e_e-.Lmemcpy_c_e
  	.previous
 -
 -	_ASM_EXTABLE_FAULT(.L_copy_leading_bytes, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w0, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w1, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w4, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w5, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w6, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w7, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_copy_trailing_words, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_copy_trailing_bytes, .L_memcpy_mcsafe_fail)
 -#endif
* Unmerged path arch/x86/include/asm/pmem.h
* Unmerged path arch/x86/include/asm/string_64.h
* Unmerged path arch/x86/kernel/x8664_ksyms_64.c
* Unmerged path arch/x86/lib/memcpy_64.S
