dax: add dax_iomap_sector() helper function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ross Zwisler <ross.zwisler@linux.intel.com>
commit 333ccc978e1e09af2690e459b6f97d8e91cc01fa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/333ccc97.failed

To be able to correctly calculate the sector from a file position and a
struct iomap there is a complex little bit of logic that currently happens
in both dax_iomap_actor() and dax_iomap_fault().  This will need to be
repeated yet again in the DAX PMD fault handler when it is added, so break
it out into a helper function.

	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 333ccc978e1e09af2690e459b6f97d8e91cc01fa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 1dfecdfb6245,77379546433e..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -1041,3 -1028,233 +1041,236 @@@ int dax_truncate_page(struct inode *ino
  	return dax_zero_page_range(inode, from, length, get_block);
  }
  EXPORT_SYMBOL_GPL(dax_truncate_page);
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_FS_IOMAP
+ static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
+ {
+ 	return iomap->blkno + (((pos & PAGE_MASK) - iomap->offset) >> 9);
+ }
+ 
+ static loff_t
+ dax_iomap_actor(struct inode *inode, loff_t pos, loff_t length, void *data,
+ 		struct iomap *iomap)
+ {
+ 	struct iov_iter *iter = data;
+ 	loff_t end = pos + length, done = 0;
+ 	ssize_t ret = 0;
+ 
+ 	if (iov_iter_rw(iter) == READ) {
+ 		end = min(end, i_size_read(inode));
+ 		if (pos >= end)
+ 			return 0;
+ 
+ 		if (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)
+ 			return iov_iter_zero(min(length, end - pos), iter);
+ 	}
+ 
+ 	if (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED))
+ 		return -EIO;
+ 
+ 	while (pos < end) {
+ 		unsigned offset = pos & (PAGE_SIZE - 1);
+ 		struct blk_dax_ctl dax = { 0 };
+ 		ssize_t map_len;
+ 
+ 		dax.sector = dax_iomap_sector(iomap, pos);
+ 		dax.size = (length + offset + PAGE_SIZE - 1) & PAGE_MASK;
+ 		map_len = dax_map_atomic(iomap->bdev, &dax);
+ 		if (map_len < 0) {
+ 			ret = map_len;
+ 			break;
+ 		}
+ 
+ 		dax.addr += offset;
+ 		map_len -= offset;
+ 		if (map_len > end - pos)
+ 			map_len = end - pos;
+ 
+ 		if (iov_iter_rw(iter) == WRITE)
+ 			map_len = copy_from_iter_pmem(dax.addr, map_len, iter);
+ 		else
+ 			map_len = copy_to_iter(dax.addr, map_len, iter);
+ 		dax_unmap_atomic(iomap->bdev, &dax);
+ 		if (map_len <= 0) {
+ 			ret = map_len ? map_len : -EFAULT;
+ 			break;
+ 		}
+ 
+ 		pos += map_len;
+ 		length -= map_len;
+ 		done += map_len;
+ 	}
+ 
+ 	return done ? done : ret;
+ }
+ 
+ /**
+  * dax_iomap_rw - Perform I/O to a DAX file
+  * @iocb:	The control block for this I/O
+  * @iter:	The addresses to do I/O from or to
+  * @ops:	iomap ops passed from the file system
+  *
+  * This function performs read and write operations to directly mapped
+  * persistent memory.  The callers needs to take care of read/write exclusion
+  * and evicting any page cache pages in the region under I/O.
+  */
+ ssize_t
+ dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
+ 		struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = iocb->ki_filp->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	loff_t pos = iocb->ki_pos, ret = 0, done = 0;
+ 	unsigned flags = 0;
+ 
+ 	if (iov_iter_rw(iter) == WRITE)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Yes, even DAX files can have page cache attached to them:  A zeroed
+ 	 * page is inserted into the pagecache when we have to serve a write
+ 	 * fault on a hole.  It should never be dirtied and can simply be
+ 	 * dropped from the pagecache once we get real data for the page.
+ 	 *
+ 	 * XXX: This is racy against mmap, and there's nothing we can do about
+ 	 * it. We'll eventually need to shift this down even further so that
+ 	 * we can check if we allocated blocks over a hole first.
+ 	 */
+ 	if (mapping->nrpages) {
+ 		ret = invalidate_inode_pages2_range(mapping,
+ 				pos >> PAGE_SHIFT,
+ 				(pos + iov_iter_count(iter) - 1) >> PAGE_SHIFT);
+ 		WARN_ON_ONCE(ret);
+ 	}
+ 
+ 	while (iov_iter_count(iter)) {
+ 		ret = iomap_apply(inode, pos, iov_iter_count(iter), flags, ops,
+ 				iter, dax_iomap_actor);
+ 		if (ret <= 0)
+ 			break;
+ 		pos += ret;
+ 		done += ret;
+ 	}
+ 
+ 	iocb->ki_pos += done;
+ 	return done ? done : ret;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_rw);
+ 
+ /**
+  * dax_iomap_fault - handle a page fault on a DAX file
+  * @vma: The virtual memory area where the fault occurred
+  * @vmf: The description of the fault
+  * @ops: iomap ops passed from the file system
+  *
+  * When a page fault occurs, filesystems may call this helper in their fault
+  * or mkwrite handler for DAX files. Assumes the caller has done all the
+  * necessary locking for the page fault to proceed successfully.
+  */
+ int dax_iomap_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
+ 			struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	unsigned long vaddr = (unsigned long)vmf->virtual_address;
+ 	loff_t pos = (loff_t)vmf->pgoff << PAGE_SHIFT;
+ 	sector_t sector;
+ 	struct iomap iomap = { 0 };
+ 	unsigned flags = 0;
+ 	int error, major = 0;
+ 	void *entry;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is supposed
+ 	 * to hold locks serializing us with truncate / punch hole so this is
+ 	 * a reliable test.
+ 	 */
+ 	if (pos >= i_size_read(inode))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	entry = grab_mapping_entry(mapping, vmf->pgoff);
+ 	if (IS_ERR(entry)) {
+ 		error = PTR_ERR(entry);
+ 		goto out;
+ 	}
+ 
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Note that we don't bother to use iomap_apply here: DAX required
+ 	 * the file system block size to be equal the page size, which means
+ 	 * that we never have to deal with more than a single extent here.
+ 	 */
+ 	error = ops->iomap_begin(inode, pos, PAGE_SIZE, flags, &iomap);
+ 	if (error)
+ 		goto unlock_entry;
+ 	if (WARN_ON_ONCE(iomap.offset + iomap.length < pos + PAGE_SIZE)) {
+ 		error = -EIO;		/* fs corruption? */
+ 		goto unlock_entry;
+ 	}
+ 
+ 	sector = dax_iomap_sector(&iomap, pos);
+ 
+ 	if (vmf->cow_page) {
+ 		switch (iomap.type) {
+ 		case IOMAP_HOLE:
+ 		case IOMAP_UNWRITTEN:
+ 			clear_user_highpage(vmf->cow_page, vaddr);
+ 			break;
+ 		case IOMAP_MAPPED:
+ 			error = copy_user_dax(iomap.bdev, sector, PAGE_SIZE,
+ 					vmf->cow_page, vaddr);
+ 			break;
+ 		default:
+ 			WARN_ON_ONCE(1);
+ 			error = -EIO;
+ 			break;
+ 		}
+ 
+ 		if (error)
+ 			goto unlock_entry;
+ 		if (!radix_tree_exceptional_entry(entry)) {
+ 			vmf->page = entry;
+ 			return VM_FAULT_LOCKED;
+ 		}
+ 		vmf->entry = entry;
+ 		return VM_FAULT_DAX_LOCKED;
+ 	}
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		if (iomap.flags & IOMAP_F_NEW) {
+ 			count_vm_event(PGMAJFAULT);
+ 			mem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);
+ 			major = VM_FAULT_MAJOR;
+ 		}
+ 		error = dax_insert_mapping(mapping, iomap.bdev, sector,
+ 				PAGE_SIZE, &entry, vma, vmf);
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (!(vmf->flags & FAULT_FLAG_WRITE))
+ 			return dax_load_hole(mapping, entry, vmf);
+ 		/*FALLTHRU*/
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		error = -EIO;
+ 		break;
+ 	}
+ 
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, vmf->pgoff, entry);
+  out:
+ 	if (error == -ENOMEM)
+ 		return VM_FAULT_OOM | major;
+ 	/* -EBUSY is fine, somebody else faulted on the same PTE */
+ 	if (error < 0 && error != -EBUSY)
+ 		return VM_FAULT_SIGBUS | major;
+ 	return VM_FAULT_NOPAGE | major;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_fault);
+ #endif /* CONFIG_FS_IOMAP */
++>>>>>>> 333ccc978e1e (dax: add dax_iomap_sector() helper function)
* Unmerged path fs/dax.c
