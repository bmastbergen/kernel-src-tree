mm, dax: change pmd_fault() to take only vmf parameter

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] change pmd_fault() to take only vmf parameter (Larry Woodman) [1457572 1457561]
Rebuild_FUZZ: 90.91%
commit-author Dave Jiang <dave.jiang@intel.com>
commit f42003917b4569a2f4f0c79c35e1e3df2859f81a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f4200391.failed

pmd_fault() and related functions really only need the vmf parameter since
the additional parameters are all included in the vmf struct.  Remove the
additional parameter and simplify pmd_fault() and friends.

Link: http://lkml.kernel.org/r/1484085142-2297-8-git-send-email-ross.zwisler@linux.intel.com
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Dave Jiang <dave.jiang@intel.com>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f42003917b4569a2f4f0c79c35e1e3df2859f81a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/dax.c
#	fs/dax.c
#	fs/ext4/file.c
#	fs/xfs/xfs_file.c
#	include/linux/dax.h
#	include/linux/mm.h
#	include/trace/events/fs_dax.h
#	mm/memory.c
diff --cc drivers/dax/dax.c
index 85c0bc93f989,18e9875f6277..000000000000
--- a/drivers/dax/dax.c
+++ b/drivers/dax/dax.c
@@@ -486,19 -472,16 +486,23 @@@ static int dax_dev_fault(struct vm_area
  	return rc;
  }
  
++<<<<<<< HEAD
 +static int __dax_dev_pmd_fault(struct dax_dev *dax_dev,
 +		struct vm_area_struct *vma, unsigned long addr, pmd_t *pmd,
 +		unsigned int flags)
++=======
+ static int __dax_dev_pmd_fault(struct dax_dev *dax_dev, struct vm_fault *vmf)
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  {
 -	unsigned long pmd_addr = vmf->address & PMD_MASK;
 +	unsigned long pmd_addr = addr & PMD_MASK;
  	struct device *dev = &dax_dev->dev;
  	struct dax_region *dax_region;
  	phys_addr_t phys;
  	pgoff_t pgoff;
  	pfn_t pfn;
 +	unsigned int fault_size = PAGE_SIZE;
  
- 	if (check_vma(dax_dev, vma, __func__))
+ 	if (check_vma(dax_dev, vmf->vma, __func__))
  		return VM_FAULT_SIGBUS;
  
  	dax_region = dax_dev->region;
@@@ -513,43 -496,33 +517,64 @@@
  		return VM_FAULT_SIGBUS;
  	}
  
++<<<<<<< HEAD
 +	if (fault_size < dax_region->align)
 +		return VM_FAULT_SIGBUS;
 +	else if (fault_size > dax_region->align)
 +		return VM_FAULT_FALLBACK;
 +
 +	/* if we are outside of the VMA */
 +	if (pmd_addr < vma->vm_start ||
 +			(pmd_addr + PMD_SIZE) > vma->vm_end)
 +		return VM_FAULT_SIGBUS;
 +
 +	pgoff = linear_page_index(vma, pmd_addr);
++=======
+ 	pgoff = linear_page_index(vmf->vma, pmd_addr);
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  	phys = pgoff_to_phys(dax_dev, pgoff, PMD_SIZE);
  	if (phys == -1) {
 -		dev_dbg(dev, "%s: phys_to_pgoff(%#lx) failed\n", __func__,
 +		dev_dbg(dev, "%s: pgoff_to_phys(%#lx) failed\n", __func__,
  				pgoff);
  		return VM_FAULT_SIGBUS;
  	}
  
  	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
  
++<<<<<<< HEAD
 +	return vmf_insert_pfn_pmd(vma, addr, pmd, pfn,
 +			flags & FAULT_FLAG_WRITE);
 +}
 +
 +static int dax_dev_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
 +		pmd_t *pmd, unsigned int flags)
++=======
+ 	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd, pfn,
+ 			vmf->flags & FAULT_FLAG_WRITE);
+ }
+ 
+ static int dax_dev_pmd_fault(struct vm_fault *vmf)
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  {
  	int rc;
- 	struct file *filp = vma->vm_file;
+ 	struct file *filp = vmf->vma->vm_file;
  	struct dax_dev *dax_dev = filp->private_data;
  
  	dev_dbg(&dax_dev->dev, "%s: %s: %s (%#lx - %#lx)\n", __func__,
++<<<<<<< HEAD
 +			current->comm, (flags & FAULT_FLAG_WRITE)
 +			? "write" : "read", vma->vm_start, vma->vm_end);
 +
 +	rcu_read_lock();
 +	rc = __dax_dev_pmd_fault(dax_dev, vma, addr, pmd, flags);
++=======
+ 			current->comm, (vmf->flags & FAULT_FLAG_WRITE)
+ 			? "write" : "read",
+ 			vmf->vma->vm_start, vmf->vma->vm_end);
+ 
+ 	rcu_read_lock();
+ 	rc = __dax_dev_pmd_fault(dax_dev, vmf);
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  	rcu_read_unlock();
  
  	return rc;
diff --cc fs/dax.c
index fa7935571d11,d800197aba34..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -989,56 -993,458 +989,399 @@@ int __dax_zero_page_range(struct block_
  }
  EXPORT_SYMBOL_GPL(__dax_zero_page_range);
  
 -static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
 -{
 -	return iomap->blkno + (((pos & PAGE_MASK) - iomap->offset) >> 9);
 -}
 -
 -static loff_t
 -dax_iomap_actor(struct inode *inode, loff_t pos, loff_t length, void *data,
 -		struct iomap *iomap)
 +/**
 + * dax_zero_page_range - zero a range within a page of a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @length: The number of bytes to zero
 + * @get_block: The filesystem method used to translate file offsets to blocks
 + *
 + * This function can be called by a filesystem when it is zeroing part of a
 + * page in a DAX file.  This is intended for hole-punch operations.  If
 + * you are truncating a file, the helper function dax_truncate_page() may be
 + * more convenient.
 + */
 +int dax_zero_page_range(struct inode *inode, loff_t from, unsigned length,
 +							get_block_t get_block)
  {
 -	struct iov_iter *iter = data;
 -	loff_t end = pos + length, done = 0;
 -	ssize_t ret = 0;
 -
 -	if (iov_iter_rw(iter) == READ) {
 -		end = min(end, i_size_read(inode));
 -		if (pos >= end)
 -			return 0;
 -
 -		if (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)
 -			return iov_iter_zero(min(length, end - pos), iter);
 -	}
 -
 -	if (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED))
 -		return -EIO;
 +	struct buffer_head bh;
 +	pgoff_t index = from >> PAGE_CACHE_SHIFT;
 +	unsigned offset = from & (PAGE_CACHE_SIZE-1);
 +	int err;
  
 -	/*
 -	 * Write can allocate block for an area which has a hole page mapped
 -	 * into page tables. We have to tear down these mappings so that data
 -	 * written by write(2) is visible in mmap.
 -	 */
 -	if ((iomap->flags & IOMAP_F_NEW) && inode->i_mapping->nrpages) {
 -		invalidate_inode_pages2_range(inode->i_mapping,
 -					      pos >> PAGE_SHIFT,
 -					      (end - 1) >> PAGE_SHIFT);
 -	}
 -
 -	while (pos < end) {
 -		unsigned offset = pos & (PAGE_SIZE - 1);
 -		struct blk_dax_ctl dax = { 0 };
 -		ssize_t map_len;
 -
 -		if (fatal_signal_pending(current)) {
 -			ret = -EINTR;
 -			break;
 -		}
 -
 -		dax.sector = dax_iomap_sector(iomap, pos);
 -		dax.size = (length + offset + PAGE_SIZE - 1) & PAGE_MASK;
 -		map_len = dax_map_atomic(iomap->bdev, &dax);
 -		if (map_len < 0) {
 -			ret = map_len;
 -			break;
 -		}
 -
 -		dax.addr += offset;
 -		map_len -= offset;
 -		if (map_len > end - pos)
 -			map_len = end - pos;
 -
 -		if (iov_iter_rw(iter) == WRITE)
 -			map_len = copy_from_iter_pmem(dax.addr, map_len, iter);
 -		else
 -			map_len = copy_to_iter(dax.addr, map_len, iter);
 -		dax_unmap_atomic(iomap->bdev, &dax);
 -		if (map_len <= 0) {
 -			ret = map_len ? map_len : -EFAULT;
 -			break;
 -		}
 -
 -		pos += map_len;
 -		length -= map_len;
 -		done += map_len;
 -	}
 -
 -	return done ? done : ret;
 +	/* Block boundary? Nothing to do */
 +	if (!length)
 +		return 0;
 +	if (WARN_ON_ONCE((offset + length) > PAGE_CACHE_SIZE))
 +		return -EINVAL;
 +
 +	memset(&bh, 0, sizeof(bh));
 +	bh.b_bdev = inode->i_sb->s_bdev;
 +	bh.b_size = PAGE_CACHE_SIZE;
 +	err = get_block(inode, index, &bh, 0);
 +	if (err < 0 || !buffer_written(&bh))
 +		return err;
 +
 +	return __dax_zero_page_range(bh.b_bdev, to_sector(&bh, inode),
 +			offset, length);
  }
 +EXPORT_SYMBOL_GPL(dax_zero_page_range);
  
  /**
 - * dax_iomap_rw - Perform I/O to a DAX file
 - * @iocb:	The control block for this I/O
 - * @iter:	The addresses to do I/O from or to
 - * @ops:	iomap ops passed from the file system
 + * dax_truncate_page - handle a partial page being truncated in a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @get_block: The filesystem method used to translate file offsets to blocks
   *
 - * This function performs read and write operations to directly mapped
 - * persistent memory.  The callers needs to take care of read/write exclusion
 - * and evicting any page cache pages in the region under I/O.
 + * Similar to block_truncate_page(), this function can be called by a
 + * filesystem when it is truncating a DAX file to handle the partial page.
   */
 -ssize_t
 -dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
 -		struct iomap_ops *ops)
 +int dax_truncate_page(struct inode *inode, loff_t from, get_block_t get_block)
  {
 -	struct address_space *mapping = iocb->ki_filp->f_mapping;
 -	struct inode *inode = mapping->host;
 -	loff_t pos = iocb->ki_pos, ret = 0, done = 0;
 -	unsigned flags = 0;
 -
 -	if (iov_iter_rw(iter) == WRITE) {
 -		lockdep_assert_held_exclusive(&inode->i_rwsem);
 -		flags |= IOMAP_WRITE;
 -	} else {
 -		lockdep_assert_held(&inode->i_rwsem);
 -	}
 -
 -	while (iov_iter_count(iter)) {
 -		ret = iomap_apply(inode, pos, iov_iter_count(iter), flags, ops,
 -				iter, dax_iomap_actor);
 -		if (ret <= 0)
 -			break;
 -		pos += ret;
 -		done += ret;
 -	}
 -
 -	iocb->ki_pos += done;
 -	return done ? done : ret;
 +	unsigned length = PAGE_CACHE_ALIGN(from) - from;
 +	return dax_zero_page_range(inode, from, length, get_block);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(dax_truncate_page);
++=======
+ EXPORT_SYMBOL_GPL(dax_iomap_rw);
+ 
+ static int dax_fault_return(int error)
+ {
+ 	if (error == 0)
+ 		return VM_FAULT_NOPAGE;
+ 	if (error == -ENOMEM)
+ 		return VM_FAULT_OOM;
+ 	return VM_FAULT_SIGBUS;
+ }
+ 
+ /**
+  * dax_iomap_fault - handle a page fault on a DAX file
+  * @vma: The virtual memory area where the fault occurred
+  * @vmf: The description of the fault
+  * @ops: iomap ops passed from the file system
+  *
+  * When a page fault occurs, filesystems may call this helper in their fault
+  * or mkwrite handler for DAX files. Assumes the caller has done all the
+  * necessary locking for the page fault to proceed successfully.
+  */
+ int dax_iomap_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
+ 			struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	unsigned long vaddr = vmf->address;
+ 	loff_t pos = (loff_t)vmf->pgoff << PAGE_SHIFT;
+ 	sector_t sector;
+ 	struct iomap iomap = { 0 };
+ 	unsigned flags = IOMAP_FAULT;
+ 	int error, major = 0;
+ 	int vmf_ret = 0;
+ 	void *entry;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is supposed
+ 	 * to hold locks serializing us with truncate / punch hole so this is
+ 	 * a reliable test.
+ 	 */
+ 	if (pos >= i_size_read(inode))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Note that we don't bother to use iomap_apply here: DAX required
+ 	 * the file system block size to be equal the page size, which means
+ 	 * that we never have to deal with more than a single extent here.
+ 	 */
+ 	error = ops->iomap_begin(inode, pos, PAGE_SIZE, flags, &iomap);
+ 	if (error)
+ 		return dax_fault_return(error);
+ 	if (WARN_ON_ONCE(iomap.offset + iomap.length < pos + PAGE_SIZE)) {
+ 		vmf_ret = dax_fault_return(-EIO);	/* fs corruption? */
+ 		goto finish_iomap;
+ 	}
+ 
+ 	entry = grab_mapping_entry(mapping, vmf->pgoff, 0);
+ 	if (IS_ERR(entry)) {
+ 		vmf_ret = dax_fault_return(PTR_ERR(entry));
+ 		goto finish_iomap;
+ 	}
+ 
+ 	sector = dax_iomap_sector(&iomap, pos);
+ 
+ 	if (vmf->cow_page) {
+ 		switch (iomap.type) {
+ 		case IOMAP_HOLE:
+ 		case IOMAP_UNWRITTEN:
+ 			clear_user_highpage(vmf->cow_page, vaddr);
+ 			break;
+ 		case IOMAP_MAPPED:
+ 			error = copy_user_dax(iomap.bdev, sector, PAGE_SIZE,
+ 					vmf->cow_page, vaddr);
+ 			break;
+ 		default:
+ 			WARN_ON_ONCE(1);
+ 			error = -EIO;
+ 			break;
+ 		}
+ 
+ 		if (error)
+ 			goto error_unlock_entry;
+ 
+ 		__SetPageUptodate(vmf->cow_page);
+ 		vmf_ret = finish_fault(vmf);
+ 		if (!vmf_ret)
+ 			vmf_ret = VM_FAULT_DONE_COW;
+ 		goto unlock_entry;
+ 	}
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		if (iomap.flags & IOMAP_F_NEW) {
+ 			count_vm_event(PGMAJFAULT);
+ 			mem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);
+ 			major = VM_FAULT_MAJOR;
+ 		}
+ 		error = dax_insert_mapping(mapping, iomap.bdev, sector,
+ 				PAGE_SIZE, &entry, vma, vmf);
+ 		/* -EBUSY is fine, somebody else faulted on the same PTE */
+ 		if (error == -EBUSY)
+ 			error = 0;
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (!(vmf->flags & FAULT_FLAG_WRITE)) {
+ 			vmf_ret = dax_load_hole(mapping, &entry, vmf);
+ 			goto unlock_entry;
+ 		}
+ 		/*FALLTHRU*/
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		error = -EIO;
+ 		break;
+ 	}
+ 
+  error_unlock_entry:
+ 	vmf_ret = dax_fault_return(error) | major;
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, vmf->pgoff, entry);
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		int copied = PAGE_SIZE;
+ 
+ 		if (vmf_ret & VM_FAULT_ERROR)
+ 			copied = 0;
+ 		/*
+ 		 * The fault is done by now and there's no way back (other
+ 		 * thread may be already happily using PTE we have installed).
+ 		 * Just ignore error from ->iomap_end since we cannot do much
+ 		 * with it.
+ 		 */
+ 		ops->iomap_end(inode, pos, PAGE_SIZE, copied, flags, &iomap);
+ 	}
+ 	return vmf_ret;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_fault);
+ 
+ #ifdef CONFIG_FS_DAX_PMD
+ /*
+  * The 'colour' (ie low bits) within a PMD of a page offset.  This comes up
+  * more often than one might expect in the below functions.
+  */
+ #define PG_PMD_COLOUR	((PMD_SIZE >> PAGE_SHIFT) - 1)
+ 
+ static int dax_pmd_insert_mapping(struct vm_fault *vmf, struct iomap *iomap,
+ 		loff_t pos, void **entryp)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	struct block_device *bdev = iomap->bdev;
+ 	struct inode *inode = mapping->host;
+ 	struct blk_dax_ctl dax = {
+ 		.sector = dax_iomap_sector(iomap, pos),
+ 		.size = PMD_SIZE,
+ 	};
+ 	long length = dax_map_atomic(bdev, &dax);
+ 	void *ret = NULL;
+ 
+ 	if (length < 0) /* dax_map_atomic() failed */
+ 		goto fallback;
+ 	if (length < PMD_SIZE)
+ 		goto unmap_fallback;
+ 	if (pfn_t_to_pfn(dax.pfn) & PG_PMD_COLOUR)
+ 		goto unmap_fallback;
+ 	if (!pfn_t_devmap(dax.pfn))
+ 		goto unmap_fallback;
+ 
+ 	dax_unmap_atomic(bdev, &dax);
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, dax.sector,
+ 			RADIX_DAX_PMD);
+ 	if (IS_ERR(ret))
+ 		goto fallback;
+ 	*entryp = ret;
+ 
+ 	trace_dax_pmd_insert_mapping(inode, vmf, length, dax.pfn, ret);
+ 	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd,
+ 			dax.pfn, vmf->flags & FAULT_FLAG_WRITE);
+ 
+  unmap_fallback:
+ 	dax_unmap_atomic(bdev, &dax);
+ fallback:
+ 	trace_dax_pmd_insert_mapping_fallback(inode, vmf, length,
+ 			dax.pfn, ret);
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int dax_pmd_load_hole(struct vm_fault *vmf, struct iomap *iomap,
+ 		void **entryp)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = vmf->address & PMD_MASK;
+ 	struct inode *inode = mapping->host;
+ 	struct page *zero_page;
+ 	void *ret = NULL;
+ 	spinlock_t *ptl;
+ 	pmd_t pmd_entry;
+ 
+ 	zero_page = mm_get_huge_zero_page(vmf->vma->vm_mm);
+ 
+ 	if (unlikely(!zero_page))
+ 		goto fallback;
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, 0,
+ 			RADIX_DAX_PMD | RADIX_DAX_HZP);
+ 	if (IS_ERR(ret))
+ 		goto fallback;
+ 	*entryp = ret;
+ 
+ 	ptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);
+ 	if (!pmd_none(*(vmf->pmd))) {
+ 		spin_unlock(ptl);
+ 		goto fallback;
+ 	}
+ 
+ 	pmd_entry = mk_pmd(zero_page, vmf->vma->vm_page_prot);
+ 	pmd_entry = pmd_mkhuge(pmd_entry);
+ 	set_pmd_at(vmf->vma->vm_mm, pmd_addr, vmf->pmd, pmd_entry);
+ 	spin_unlock(ptl);
+ 	trace_dax_pmd_load_hole(inode, vmf, zero_page, ret);
+ 	return VM_FAULT_NOPAGE;
+ 
+ fallback:
+ 	trace_dax_pmd_load_hole_fallback(inode, vmf, zero_page, ret);
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ int dax_iomap_pmd_fault(struct vm_fault *vmf, struct iomap_ops *ops)
+ {
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = vmf->address & PMD_MASK;
+ 	bool write = vmf->flags & FAULT_FLAG_WRITE;
+ 	unsigned int iomap_flags = (write ? IOMAP_WRITE : 0) | IOMAP_FAULT;
+ 	struct inode *inode = mapping->host;
+ 	int result = VM_FAULT_FALLBACK;
+ 	struct iomap iomap = { 0 };
+ 	pgoff_t max_pgoff, pgoff;
+ 	void *entry;
+ 	loff_t pos;
+ 	int error;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is
+ 	 * supposed to hold locks serializing us with truncate / punch hole so
+ 	 * this is a reliable test.
+ 	 */
+ 	pgoff = linear_page_index(vma, pmd_addr);
+ 	max_pgoff = (i_size_read(inode) - 1) >> PAGE_SHIFT;
+ 
+ 	trace_dax_pmd_fault(inode, vmf, max_pgoff, 0);
+ 
+ 	/* Fall back to PTEs if we're going to COW */
+ 	if (write && !(vma->vm_flags & VM_SHARED))
+ 		goto fallback;
+ 
+ 	/* If the PMD would extend outside the VMA */
+ 	if (pmd_addr < vma->vm_start)
+ 		goto fallback;
+ 	if ((pmd_addr + PMD_SIZE) > vma->vm_end)
+ 		goto fallback;
+ 
+ 	if (pgoff > max_pgoff) {
+ 		result = VM_FAULT_SIGBUS;
+ 		goto out;
+ 	}
+ 
+ 	/* If the PMD would extend beyond the file size */
+ 	if ((pgoff | PG_PMD_COLOUR) > max_pgoff)
+ 		goto fallback;
+ 
+ 	/*
+ 	 * Note that we don't use iomap_apply here.  We aren't doing I/O, only
+ 	 * setting up a mapping, so really we're using iomap_begin() as a way
+ 	 * to look up our filesystem block.
+ 	 */
+ 	pos = (loff_t)pgoff << PAGE_SHIFT;
+ 	error = ops->iomap_begin(inode, pos, PMD_SIZE, iomap_flags, &iomap);
+ 	if (error)
+ 		goto fallback;
+ 
+ 	if (iomap.offset + iomap.length < pos + PMD_SIZE)
+ 		goto finish_iomap;
+ 
+ 	/*
+ 	 * grab_mapping_entry() will make sure we get a 2M empty entry, a DAX
+ 	 * PMD or a HZP entry.  If it can't (because a 4k page is already in
+ 	 * the tree, for instance), it will return -EEXIST and we just fall
+ 	 * back to 4k entries.
+ 	 */
+ 	entry = grab_mapping_entry(mapping, pgoff, RADIX_DAX_PMD);
+ 	if (IS_ERR(entry))
+ 		goto finish_iomap;
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		result = dax_pmd_insert_mapping(vmf, &iomap, pos, &entry);
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (WARN_ON_ONCE(write))
+ 			goto unlock_entry;
+ 		result = dax_pmd_load_hole(vmf, &iomap, &entry);
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		break;
+ 	}
+ 
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, pgoff, entry);
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		int copied = PMD_SIZE;
+ 
+ 		if (result == VM_FAULT_FALLBACK)
+ 			copied = 0;
+ 		/*
+ 		 * The fault is done by now and there's no way back (other
+ 		 * thread may be already happily using PMD we have installed).
+ 		 * Just ignore error from ->iomap_end since we cannot do much
+ 		 * with it.
+ 		 */
+ 		ops->iomap_end(inode, pos, PMD_SIZE, copied, iomap_flags,
+ 				&iomap);
+ 	}
+  fallback:
+ 	if (result == VM_FAULT_FALLBACK) {
+ 		split_huge_pmd(vma, vmf->pmd, vmf->address);
+ 		count_vm_event(THP_FAULT_FALLBACK);
+ 	}
+ out:
+ 	trace_dax_pmd_fault_done(inode, vmf, max_pgoff, result);
+ 	return result;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_pmd_fault);
+ #endif /* CONFIG_FS_DAX_PMD */
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
diff --cc fs/ext4/file.c
index b51446198588,13021a054fc0..000000000000
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@@ -235,38 -273,23 +235,55 @@@ static int ext4_dax_fault(struct vm_are
  	return result;
  }
  
++<<<<<<< HEAD
 +static int ext4_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
 +						pmd_t *pmd, unsigned int flags)
 +{
 +	int result;
 +	handle_t *handle = NULL;
 +	struct inode *inode = file_inode(vma->vm_file);
++=======
+ static int
+ ext4_dax_pmd_fault(struct vm_fault *vmf)
+ {
+ 	int result;
+ 	struct inode *inode = file_inode(vmf->vma->vm_file);
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  	struct super_block *sb = inode->i_sb;
 -	bool write = vmf->flags & FAULT_FLAG_WRITE;
 +	bool write = flags & FAULT_FLAG_WRITE;
  
  	if (write) {
  		sb_start_pagefault(sb);
++<<<<<<< HEAD
 +		file_update_time(vma->vm_file);
 +		down_read(&EXT4_I(inode)->i_mmap_sem);
 +		handle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,
 +				ext4_chunk_trans_blocks(inode,
 +							PMD_SIZE / PAGE_SIZE));
 +	} else
 +		down_read(&EXT4_I(inode)->i_mmap_sem);
 +
 +	if (IS_ERR(handle))
 +		result = VM_FAULT_SIGBUS;
 +	else
 +		result = dax_pmd_fault(vma, addr, pmd, flags,
 +				ext4_dax_get_block);
 +
 +	if (write) {
 +		if (!IS_ERR(handle))
 +			ext4_journal_stop(handle);
 +		up_read(&EXT4_I(inode)->i_mmap_sem);
++=======
+ 		file_update_time(vmf->vma->vm_file);
+ 	}
+ 	down_read(&EXT4_I(inode)->i_mmap_sem);
+ 	result = dax_iomap_pmd_fault(vmf, &ext4_iomap_ops);
+ 	up_read(&EXT4_I(inode)->i_mmap_sem);
+ 	if (write)
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  		sb_end_pagefault(sb);
 +	} else
 +		up_read(&EXT4_I(inode)->i_mmap_sem);
  
  	return result;
  }
diff --cc fs/xfs/xfs_file.c
index d7247d5d7ddf,9d8440b07b53..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -1763,12 -1431,9 +1763,16 @@@ xfs_filemap_fault
   */
  STATIC int
  xfs_filemap_pmd_fault(
++<<<<<<< HEAD
 +	struct vm_area_struct	*vma,
 +	unsigned long		addr,
 +	pmd_t			*pmd,
 +	unsigned int		flags)
++=======
+ 	struct vm_fault		*vmf)
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  {
- 	struct inode		*inode = file_inode(vma->vm_file);
+ 	struct inode		*inode = file_inode(vmf->vma->vm_file);
  	struct xfs_inode	*ip = XFS_I(inode);
  	int			ret;
  
@@@ -1777,16 -1442,16 +1781,20 @@@
  
  	trace_xfs_filemap_pmd_fault(ip);
  
 -	if (vmf->flags & FAULT_FLAG_WRITE) {
 +	if (flags & FAULT_FLAG_WRITE) {
  		sb_start_pagefault(inode->i_sb);
- 		file_update_time(vma->vm_file);
+ 		file_update_time(vmf->vma->vm_file);
  	}
  
  	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
++<<<<<<< HEAD
 +	ret = dax_pmd_fault(vma, addr, pmd, flags, xfs_get_blocks_dax_fault);
++=======
+ 	ret = dax_iomap_pmd_fault(vmf, &xfs_iomap_ops);
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
  
 -	if (vmf->flags & FAULT_FLAG_WRITE)
 +	if (flags & FAULT_FLAG_WRITE)
  		sb_end_pagefault(inode->i_sb);
  
  	return ret;
diff --cc include/linux/dax.h
index 8937c7aed5cb,c1bd6ab5e974..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -56,8 -64,21 +56,26 @@@ static inline int __dax_zero_page_range
  }
  #endif
  
++<<<<<<< HEAD
 +static inline int dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
 +				pmd_t *pmd, unsigned int flags, get_block_t gb)
++=======
+ #ifdef CONFIG_FS_DAX_PMD
+ static inline unsigned int dax_radix_order(void *entry)
+ {
+ 	if ((unsigned long)entry & RADIX_DAX_PMD)
+ 		return PMD_SHIFT - PAGE_SHIFT;
+ 	return 0;
+ }
+ int dax_iomap_pmd_fault(struct vm_fault *vmf, struct iomap_ops *ops);
+ #else
+ static inline unsigned int dax_radix_order(void *entry)
+ {
+ 	return 0;
+ }
+ static inline int dax_iomap_pmd_fault(struct vm_fault *vmf,
+ 		struct iomap_ops *ops)
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  {
  	return VM_FAULT_FALLBACK;
  }
diff --cc include/linux/mm.h
index 3416fff96060,3787f047a098..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -261,7 -349,11 +261,13 @@@ struct vm_fault 
  struct vm_operations_struct {
  	void (*open)(struct vm_area_struct * area);
  	void (*close)(struct vm_area_struct * area);
 -	int (*mremap)(struct vm_area_struct * area);
  	int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);
++<<<<<<< HEAD
++=======
+ 	int (*pmd_fault)(struct vm_fault *vmf);
+ 	void (*map_pages)(struct vm_fault *vmf,
+ 			pgoff_t start_pgoff, pgoff_t end_pgoff);
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  
  	/* notification that a previously read-only page is about to become
  	 * writable, if an error is returned it will cause a SIGBUS */
diff --cc mm/memory.c
index 14270187456b,ececdc4a2892..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3173,24 -3469,26 +3173,43 @@@ out
  	return 0;
  }
  
 -static int create_huge_pmd(struct vm_fault *vmf)
 +static int create_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
 +			unsigned long address, pmd_t *pmd, unsigned int flags)
  {
++<<<<<<< HEAD
 +	if (!vma->vm_ops)
 +		return do_huge_pmd_anonymous_page(mm, vma, address, pmd, flags);
 +	if ((vma->vm_flags2 & VM_PMD_FAULT) && vma->vm_ops->pmd_fault)
 +		return vma->vm_ops->pmd_fault(vma, address, pmd, flags);
++=======
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return do_huge_pmd_anonymous_page(vmf);
+ 	if (vmf->vma->vm_ops->pmd_fault)
+ 		return vmf->vma->vm_ops->pmd_fault(vmf);
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  	return VM_FAULT_FALLBACK;
  }
  
 -static int wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)
 +static int wp_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
 +			unsigned long address, pmd_t *pmd, pmd_t orig_pmd,
 +			unsigned int flags)
  {
++<<<<<<< HEAD
 +	if (!vma->vm_ops)
 +		return do_huge_pmd_wp_page(mm, vma, address, pmd, orig_pmd);
 +	if ((vma->vm_flags2 & VM_PMD_FAULT) && vma->vm_ops->pmd_fault)
 +		return vma->vm_ops->pmd_fault(vma, address, pmd, flags);
++=======
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return do_huge_pmd_wp_page(vmf, orig_pmd);
+ 	if (vmf->vma->vm_ops->pmd_fault)
+ 		return vmf->vma->vm_ops->pmd_fault(vmf);
+ 
+ 	/* COW handled on pte level: split pmd */
+ 	VM_BUG_ON_VMA(vmf->vma->vm_flags & VM_SHARED, vmf->vma);
+ 	__split_huge_pmd(vmf->vma, vmf->pmd, vmf->address, false, NULL);
+ 
++>>>>>>> f42003917b45 (mm, dax: change pmd_fault() to take only vmf parameter)
  	return VM_FAULT_FALLBACK;
  }
  
* Unmerged path include/trace/events/fs_dax.h
* Unmerged path drivers/dax/dax.c
* Unmerged path fs/dax.c
* Unmerged path fs/ext4/file.c
* Unmerged path fs/xfs/xfs_file.c
* Unmerged path include/linux/dax.h
* Unmerged path include/linux/mm.h
* Unmerged path include/trace/events/fs_dax.h
* Unmerged path mm/memory.c
