xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Chuck Lever <chuck.lever@oracle.com>
commit 9590d083c1bb1419b7992609d1a0a3e3517d3893
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/9590d083.failed

Adopt the use of xprt_pin_rqst to eliminate contention between
Call-side users of rb_lock and the use of rb_lock in
rpcrdma_reply_handler.

This replaces the mechanism introduced in 431af645cf66 ("xprtrdma:
Fix client lock-up after application signal fires").

Use recv_lock to quickly find the completing rqst, pin it, then
drop the lock. At that point invalidation and pull-up of the Reply
XDR can be done. Both are often expensive operations.

Finally, take recv_lock again to signal completion to the RPC
layer. It also protects adjustment of "cwnd".

This greatly reduces the amount of time a lock is held by the
reply handler. Comparing lock_stat results shows a marked decrease
in contention on rb_lock and recv_lock.

	Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
[trond.myklebust@primarydata.com: Remove call to rpcrdma_buffer_put() from
   the "out_norqst:" path in rpcrdma_reply_handler.]
	Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>
(cherry picked from commit 9590d083c1bb1419b7992609d1a0a3e3517d3893)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprt.c
#	net/sunrpc/xprtrdma/rpc_rdma.c
diff --cc net/sunrpc/xprt.c
index e83aa477fea7,e741ec2b4d8e..000000000000
--- a/net/sunrpc/xprt.c
+++ b/net/sunrpc/xprt.c
@@@ -844,6 -844,50 +844,53 @@@ struct rpc_rqst *xprt_lookup_rqst(struc
  }
  EXPORT_SYMBOL_GPL(xprt_lookup_rqst);
  
++<<<<<<< HEAD
++=======
+ /**
+  * xprt_pin_rqst - Pin a request on the transport receive list
+  * @req: Request to pin
+  *
+  * Caller must ensure this is atomic with the call to xprt_lookup_rqst()
+  * so should be holding the xprt transport lock.
+  */
+ void xprt_pin_rqst(struct rpc_rqst *req)
+ {
+ 	set_bit(RPC_TASK_MSG_RECV, &req->rq_task->tk_runstate);
+ }
+ EXPORT_SYMBOL_GPL(xprt_pin_rqst);
+ 
+ /**
+  * xprt_unpin_rqst - Unpin a request on the transport receive list
+  * @req: Request to pin
+  *
+  * Caller should be holding the xprt transport lock.
+  */
+ void xprt_unpin_rqst(struct rpc_rqst *req)
+ {
+ 	struct rpc_task *task = req->rq_task;
+ 
+ 	clear_bit(RPC_TASK_MSG_RECV, &task->tk_runstate);
+ 	if (test_bit(RPC_TASK_MSG_RECV_WAIT, &task->tk_runstate))
+ 		wake_up_bit(&task->tk_runstate, RPC_TASK_MSG_RECV);
+ }
+ EXPORT_SYMBOL_GPL(xprt_unpin_rqst);
+ 
+ static void xprt_wait_on_pinned_rqst(struct rpc_rqst *req)
+ __must_hold(&req->rq_xprt->recv_lock)
+ {
+ 	struct rpc_task *task = req->rq_task;
+ 	
+ 	if (task && test_bit(RPC_TASK_MSG_RECV, &task->tk_runstate)) {
+ 		spin_unlock(&req->rq_xprt->recv_lock);
+ 		set_bit(RPC_TASK_MSG_RECV_WAIT, &task->tk_runstate);
+ 		wait_on_bit(&task->tk_runstate, RPC_TASK_MSG_RECV,
+ 				TASK_UNINTERRUPTIBLE);
+ 		clear_bit(RPC_TASK_MSG_RECV_WAIT, &task->tk_runstate);
+ 		spin_lock(&req->rq_xprt->recv_lock);
+ 	}
+ }
+ 
++>>>>>>> 9590d083c1bb (xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler)
  static void xprt_update_rtt(struct rpc_task *task)
  {
  	struct rpc_rqst *req = task->tk_rqstp;
diff --cc net/sunrpc/xprtrdma/rpc_rdma.c
index 42cacbf866f4,f1889f4d4803..000000000000
--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@@ -1051,16 -1223,13 +1048,19 @@@ rpcrdma_reply_handler(struct work_struc
  	struct rpcrdma_rep *rep =
  			container_of(work, struct rpcrdma_rep, rr_work);
  	struct rpcrdma_xprt *r_xprt = rep->rr_rxprt;
- 	struct rpcrdma_buffer *buf = &r_xprt->rx_buf;
  	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
  	struct xdr_stream *xdr = &rep->rr_stream;
 +	struct rpcrdma_msg *headerp;
  	struct rpcrdma_req *req;
  	struct rpc_rqst *rqst;
 -	__be32 *p, xid, vers, proc;
 +	__be32 *iptr, *p, xid, vers, proc;
 +	int rdmalen, status, rmerr;
  	unsigned long cwnd;
++<<<<<<< HEAD
 +	struct list_head mws;
++=======
+ 	int status;
++>>>>>>> 9590d083c1bb (xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler)
  
  	dprintk("RPC:       %s: incoming rep %p\n", __func__, rep);
  
@@@ -1114,99 -1273,42 +1107,111 @@@
  	 * waking the next RPC waits until this RPC has relinquished
  	 * all its Send Queue entries.
  	 */
- 	if (!list_empty(&mws))
- 		r_xprt->rx_ia.ri_ops->ro_unmap_sync(r_xprt, &mws);
+ 	if (!list_empty(&req->rl_registered)) {
+ 		rpcrdma_mark_remote_invalidation(&req->rl_registered, rep);
+ 		r_xprt->rx_ia.ri_ops->ro_unmap_sync(r_xprt,
+ 						    &req->rl_registered);
+ 	}
  
++<<<<<<< HEAD
 +	/* Perform XID lookup, reconstruction of the RPC reply, and
 +	 * RPC completion while holding the transport lock to ensure
 +	 * the rep, rqst, and rq_task pointers remain stable.
 +	 */
 +	spin_lock_bh(&xprt->transport_lock);
 +	rqst = xprt_lookup_rqst(xprt, xid);
 +	if (!rqst)
 +		goto out_norqst;
++=======
++>>>>>>> 9590d083c1bb (xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler)
  	xprt->reestablish_timeout = 0;
  	if (vers != rpcrdma_version)
  		goto out_badversion;
  
 +	/* check for expected message types */
 +	/* The order of some of these tests is important. */
  	switch (proc) {
  	case rdma_msg:
 -		status = rpcrdma_decode_msg(r_xprt, rep, rqst);
 +		/* never expect read chunks */
 +		/* never expect reply chunks (two ways to check) */
 +		if (headerp->rm_body.rm_chunks[0] != xdr_zero ||
 +		    (headerp->rm_body.rm_chunks[1] == xdr_zero &&
 +		     headerp->rm_body.rm_chunks[2] != xdr_zero))
 +			goto badheader;
 +		if (headerp->rm_body.rm_chunks[1] != xdr_zero) {
 +			/* count any expected write chunks in read reply */
 +			/* start at write chunk array count */
 +			iptr = &headerp->rm_body.rm_chunks[2];
 +			rdmalen = rpcrdma_count_chunks(rep, 1, &iptr);
 +			/* check for validity, and no reply chunk after */
 +			if (rdmalen < 0 || *iptr++ != xdr_zero)
 +				goto badheader;
 +			rep->rr_len -=
 +			    ((unsigned char *)iptr - (unsigned char *)headerp);
 +			status = rep->rr_len + rdmalen;
 +			r_xprt->rx_stats.total_rdma_reply += rdmalen;
 +			/* special case - last chunk may omit padding */
 +			if (rdmalen &= 3) {
 +				rdmalen = 4 - rdmalen;
 +				status += rdmalen;
 +			}
 +		} else {
 +			/* else ordinary inline */
 +			rdmalen = 0;
 +			iptr = (__be32 *)((unsigned char *)headerp +
 +							RPCRDMA_HDRLEN_MIN);
 +			rep->rr_len -= RPCRDMA_HDRLEN_MIN;
 +			status = rep->rr_len;
 +		}
 +
 +		r_xprt->rx_stats.fixup_copy_count +=
 +			rpcrdma_inline_fixup(rqst, (char *)iptr, rep->rr_len,
 +					     rdmalen);
  		break;
 +
  	case rdma_nomsg:
 -		status = rpcrdma_decode_nomsg(r_xprt, rep);
 +		/* never expect read or write chunks, always reply chunks */
 +		if (headerp->rm_body.rm_chunks[0] != xdr_zero ||
 +		    headerp->rm_body.rm_chunks[1] != xdr_zero ||
 +		    headerp->rm_body.rm_chunks[2] != xdr_one)
 +			goto badheader;
 +		iptr = (__be32 *)((unsigned char *)headerp +
 +							RPCRDMA_HDRLEN_MIN);
 +		rdmalen = rpcrdma_count_chunks(rep, 0, &iptr);
 +		if (rdmalen < 0)
 +			goto badheader;
 +		r_xprt->rx_stats.total_rdma_reply += rdmalen;
 +		/* Reply chunk buffer already is the reply vector - no fixup. */
 +		status = rdmalen;
  		break;
 +
  	case rdma_error:
 -		status = rpcrdma_decode_error(r_xprt, rep, rqst);
 -		break;
 +		goto out_rdmaerr;
 +
 +badheader:
  	default:
 +		dprintk("RPC: %5u %s: invalid rpcrdma reply (type %u)\n",
 +			rqst->rq_task->tk_pid, __func__,
 +			be32_to_cpu(proc));
  		status = -EIO;
 +		r_xprt->rx_stats.bad_reply_count++;
 +		break;
  	}
 -	if (status < 0)
 -		goto out_badheader;
  
  out:
+ 	spin_lock(&xprt->recv_lock);
  	cwnd = xprt->cwnd;
  	xprt->cwnd = atomic_read(&r_xprt->rx_buf.rb_credits) << RPC_CWNDSHIFT;
  	if (xprt->cwnd > cwnd)
  		xprt_release_rqst_cong(rqst->rq_task);
  
  	xprt_complete_rqst(rqst->rq_task, status);
++<<<<<<< HEAD
 +	spin_unlock_bh(&xprt->transport_lock);
++=======
+ 	xprt_unpin_rqst(rqst);
+ 	spin_unlock(&xprt->recv_lock);
++>>>>>>> 9590d083c1bb (xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler)
  	dprintk("RPC:       %s: xprt_complete_rqst(0x%p, 0x%p, %d)\n",
  		__func__, xprt, rqst, status);
  	return;
@@@ -1262,18 -1344,7 +1267,22 @@@ out_rdmaerr
   * has already been terminated.
   */
  out_norqst:
++<<<<<<< HEAD
 +	spin_unlock_bh(&xprt->transport_lock);
 +	rpcrdma_buffer_put(req);
 +	dprintk("RPC:       %s: race, no rqst left for req %p\n",
 +		__func__, req);
 +	return;
 +
 +out_shortreply:
 +	dprintk("RPC:       %s: short/invalid reply\n", __func__);
 +	goto repost;
 +
 +out_nomatch:
 +	spin_unlock(&buf->rb_lock);
++=======
+ 	spin_unlock(&xprt->recv_lock);
++>>>>>>> 9590d083c1bb (xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler)
  	dprintk("RPC:       %s: no match for incoming xid 0x%08x\n",
  		__func__, be32_to_cpu(xid));
  	goto repost;
* Unmerged path net/sunrpc/xprt.c
* Unmerged path net/sunrpc/xprtrdma/rpc_rdma.c
diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 51c02d280e2e..3e5867642c95 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -685,7 +685,6 @@ xprt_rdma_free(struct rpc_task *task)
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	rpcrdma_remove_req(&r_xprt->rx_buf, req);
 	if (!list_empty(&req->rl_registered))
 		ia->ri_ops->ro_unmap_safe(r_xprt, req, !RPC_IS_ASYNC(task));
 	rpcrdma_unmap_sges(ia, req);
diff --git a/net/sunrpc/xprtrdma/verbs.c b/net/sunrpc/xprtrdma/verbs.c
index c78fb27c20ed..11a1fbf7e59e 100644
--- a/net/sunrpc/xprtrdma/verbs.c
+++ b/net/sunrpc/xprtrdma/verbs.c
@@ -1001,7 +1001,6 @@ rpcrdma_buffer_create(struct rpcrdma_xprt *r_xprt)
 	spin_lock_init(&buf->rb_recovery_lock);
 	INIT_LIST_HEAD(&buf->rb_mws);
 	INIT_LIST_HEAD(&buf->rb_all);
-	INIT_LIST_HEAD(&buf->rb_pending);
 	INIT_LIST_HEAD(&buf->rb_stale_mrs);
 	INIT_DELAYED_WORK(&buf->rb_refresh_worker,
 			  rpcrdma_mr_refresh_worker);
diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 45dab2475c99..e26a97d2f922 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -340,7 +340,6 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
-	__be32			rl_xid;
 	unsigned int		rl_mapped_sges;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
@@ -404,7 +403,6 @@ struct rpcrdma_buffer {
 	int			rb_send_count, rb_recv_count;
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
-	struct list_head	rb_pending;
 	u32			rb_max_requests;
 	atomic_t		rb_credits;	/* most recent credit grant */
 
@@ -557,34 +555,6 @@ void rpcrdma_destroy_req(struct rpcrdma_req *);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 
-static inline void
-rpcrdma_insert_req(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
-{
-	spin_lock(&buffers->rb_lock);
-	if (list_empty(&req->rl_list))
-		list_add_tail(&req->rl_list, &buffers->rb_pending);
-	spin_unlock(&buffers->rb_lock);
-}
-
-static inline struct rpcrdma_req *
-rpcrdma_lookup_req_locked(struct rpcrdma_buffer *buffers, __be32 xid)
-{
-	struct rpcrdma_req *pos;
-
-	list_for_each_entry(pos, &buffers->rb_pending, rl_list)
-		if (pos->rl_xid == xid)
-			return pos;
-	return NULL;
-}
-
-static inline void
-rpcrdma_remove_req(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
-{
-	spin_lock(&buffers->rb_lock);
-	list_del(&req->rl_list);
-	spin_unlock(&buffers->rb_lock);
-}
-
 struct rpcrdma_mw *rpcrdma_get_mw(struct rpcrdma_xprt *);
 void rpcrdma_put_mw(struct rpcrdma_xprt *, struct rpcrdma_mw *);
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
