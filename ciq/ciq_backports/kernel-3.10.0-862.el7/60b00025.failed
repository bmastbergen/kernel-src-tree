powerpc/perf: factor out the event format field

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [powerpc] perf: factor out the event format field (Mauricio Oliveira) [1494439]
Rebuild_FUZZ: 90.70%
commit-author Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
commit 60b00025641e2921dcfba4d54b6cf7f0c5903677
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/60b00025.failed

Factor out the format field structure for PowerISA v2.07.

	Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
(cherry picked from commit 60b00025641e2921dcfba4d54b6cf7f0c5903677)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/perf/isa207-common.c
#	arch/powerpc/perf/power8-pmu.c
#	arch/powerpc/perf/power9-pmu.c
diff --cc arch/powerpc/perf/power8-pmu.c
index 4db4aa43174b,d07186382f3a..000000000000
--- a/arch/powerpc/perf/power8-pmu.c
+++ b/arch/powerpc/perf/power8-pmu.c
@@@ -30,249 -30,8 +30,254 @@@ enum 
  #define	POWER8_MMCRA_IFM2		0x0000000080000000UL
  #define	POWER8_MMCRA_IFM3		0x00000000C0000000UL
  
++<<<<<<< HEAD
 +static inline bool event_is_fab_match(u64 event)
 +{
 +	/* Only check pmc, unit and pmcxsel, ignore the edge bit (0) */
 +	event &= 0xff0fe;
 +
 +	/* PM_MRK_FAB_RSP_MATCH & PM_MRK_FAB_RSP_MATCH_CYC */
 +	return (event == 0x30056 || event == 0x4f052);
 +}
 +
 +static int power8_get_constraint(u64 event, unsigned long *maskp, unsigned long *valp)
 +{
 +	unsigned int unit, pmc, cache, ebb;
 +	unsigned long mask, value;
 +
 +	mask = value = 0;
 +
 +	if (event & ~EVENT_VALID_MASK)
 +		return -1;
 +
 +	pmc   = (event >> EVENT_PMC_SHIFT)        & EVENT_PMC_MASK;
 +	unit  = (event >> EVENT_UNIT_SHIFT)       & EVENT_UNIT_MASK;
 +	cache = (event >> EVENT_CACHE_SEL_SHIFT)  & EVENT_CACHE_SEL_MASK;
 +	ebb   = (event >> EVENT_EBB_SHIFT)        & EVENT_EBB_MASK;
 +
 +	if (pmc) {
 +		u64 base_event;
 +
 +		if (pmc > 6)
 +			return -1;
 +
 +		/* Ignore Linux defined bits when checking event below */
 +		base_event = event & ~EVENT_LINUX_MASK;
 +
 +		if (pmc >= 5 && base_event != PM_RUN_INST_CMPL &&
 +				base_event != PM_RUN_CYC)
 +			return -1;
 +
 +		mask  |= CNST_PMC_MASK(pmc);
 +		value |= CNST_PMC_VAL(pmc);
 +	}
 +
 +	if (pmc <= 4) {
 +		/*
 +		 * Add to number of counters in use. Note this includes events with
 +		 * a PMC of 0 - they still need a PMC, it's just assigned later.
 +		 * Don't count events on PMC 5 & 6, there is only one valid event
 +		 * on each of those counters, and they are handled above.
 +		 */
 +		mask  |= CNST_NC_MASK;
 +		value |= CNST_NC_VAL;
 +	}
 +
 +	if (unit >= 6 && unit <= 9) {
 +		/*
 +		 * L2/L3 events contain a cache selector field, which is
 +		 * supposed to be programmed into MMCRC. However MMCRC is only
 +		 * HV writable, and there is no API for guest kernels to modify
 +		 * it. The solution is for the hypervisor to initialise the
 +		 * field to zeroes, and for us to only ever allow events that
 +		 * have a cache selector of zero. The bank selector (bit 3) is
 +		 * irrelevant, as long as the rest of the value is 0.
 +		 */
 +		if (cache & 0x7)
 +			return -1;
 +
 +	} else if (event & EVENT_IS_L1) {
 +		mask  |= CNST_L1_QUAL_MASK;
 +		value |= CNST_L1_QUAL_VAL(cache);
 +	}
 +
 +	if (event & EVENT_IS_MARKED) {
 +		mask  |= CNST_SAMPLE_MASK;
 +		value |= CNST_SAMPLE_VAL(event >> EVENT_SAMPLE_SHIFT);
 +	}
 +
 +	/*
 +	 * Special case for PM_MRK_FAB_RSP_MATCH and PM_MRK_FAB_RSP_MATCH_CYC,
 +	 * the threshold control bits are used for the match value.
 +	 */
 +	if (event_is_fab_match(event)) {
 +		mask  |= CNST_FAB_MATCH_MASK;
 +		value |= CNST_FAB_MATCH_VAL(event >> EVENT_THR_CTL_SHIFT);
 +	} else {
 +		/*
 +		 * Check the mantissa upper two bits are not zero, unless the
 +		 * exponent is also zero. See the THRESH_CMP_MANTISSA doc.
 +		 */
 +		unsigned int cmp, exp;
 +
 +		cmp = (event >> EVENT_THR_CMP_SHIFT) & EVENT_THR_CMP_MASK;
 +		exp = cmp >> 7;
 +
 +		if (exp && (cmp & 0x60) == 0)
 +			return -1;
 +
 +		mask  |= CNST_THRESH_MASK;
 +		value |= CNST_THRESH_VAL(event >> EVENT_THRESH_SHIFT);
 +	}
 +
 +	if (!pmc && ebb)
 +		/* EBB events must specify the PMC */
 +		return -1;
 +
 +	if (event & EVENT_WANTS_BHRB) {
 +		if (!ebb)
 +			/* Only EBB events can request BHRB */
 +			return -1;
 +
 +		mask  |= CNST_IFM_MASK;
 +		value |= CNST_IFM_VAL(event >> EVENT_IFM_SHIFT);
 +	}
 +
 +	/*
 +	 * All events must agree on EBB, either all request it or none.
 +	 * EBB events are pinned & exclusive, so this should never actually
 +	 * hit, but we leave it as a fallback in case.
 +	 */
 +	mask  |= CNST_EBB_VAL(ebb);
 +	value |= CNST_EBB_MASK;
 +
 +	*maskp = mask;
 +	*valp = value;
 +
 +	return 0;
 +}
 +
 +static int power8_compute_mmcr(u64 event[], int n_ev,
 +			       unsigned int hwc[], unsigned long mmcr[],
 +			       struct perf_event *pevents[])
 +{
 +	unsigned long mmcra, mmcr1, mmcr2, unit, combine, psel, cache, val;
 +	unsigned int pmc, pmc_inuse;
 +	int i;
 +
 +	pmc_inuse = 0;
 +
 +	/* First pass to count resource use */
 +	for (i = 0; i < n_ev; ++i) {
 +		pmc = (event[i] >> EVENT_PMC_SHIFT) & EVENT_PMC_MASK;
 +		if (pmc)
 +			pmc_inuse |= 1 << pmc;
 +	}
 +
 +	/* In continous sampling mode, update SDAR on TLB miss */
 +	mmcra = MMCRA_SDAR_MODE_TLB;
 +	mmcr1 = mmcr2 = 0;
 +
 +	/* Second pass: assign PMCs, set all MMCR1 fields */
 +	for (i = 0; i < n_ev; ++i) {
 +		pmc     = (event[i] >> EVENT_PMC_SHIFT) & EVENT_PMC_MASK;
 +		unit    = (event[i] >> EVENT_UNIT_SHIFT) & EVENT_UNIT_MASK;
 +		combine = (event[i] >> EVENT_COMBINE_SHIFT) & EVENT_COMBINE_MASK;
 +		psel    =  event[i] & EVENT_PSEL_MASK;
 +
 +		if (!pmc) {
 +			for (pmc = 1; pmc <= 4; ++pmc) {
 +				if (!(pmc_inuse & (1 << pmc)))
 +					break;
 +			}
 +
 +			pmc_inuse |= 1 << pmc;
 +		}
 +
 +		if (pmc <= 4) {
 +			mmcr1 |= unit << MMCR1_UNIT_SHIFT(pmc);
 +			mmcr1 |= combine << MMCR1_COMBINE_SHIFT(pmc);
 +			mmcr1 |= psel << MMCR1_PMCSEL_SHIFT(pmc);
 +		}
 +
 +		if (event[i] & EVENT_IS_L1) {
 +			cache = event[i] >> EVENT_CACHE_SEL_SHIFT;
 +			mmcr1 |= (cache & 1) << MMCR1_IC_QUAL_SHIFT;
 +			cache >>= 1;
 +			mmcr1 |= (cache & 1) << MMCR1_DC_QUAL_SHIFT;
 +		}
 +
 +		if (event[i] & EVENT_IS_MARKED) {
 +			mmcra |= MMCRA_SAMPLE_ENABLE;
 +
 +			val = (event[i] >> EVENT_SAMPLE_SHIFT) & EVENT_SAMPLE_MASK;
 +			if (val) {
 +				mmcra |= (val &  3) << MMCRA_SAMP_MODE_SHIFT;
 +				mmcra |= (val >> 2) << MMCRA_SAMP_ELIG_SHIFT;
 +			}
 +		}
 +
 +		/*
 +		 * PM_MRK_FAB_RSP_MATCH and PM_MRK_FAB_RSP_MATCH_CYC,
 +		 * the threshold bits are used for the match value.
 +		 */
 +		if (event_is_fab_match(event[i])) {
 +			mmcr1 |= ((event[i] >> EVENT_THR_CTL_SHIFT) &
 +				  EVENT_THR_CTL_MASK) << MMCR1_FAB_SHIFT;
 +		} else {
 +			val = (event[i] >> EVENT_THR_CTL_SHIFT) & EVENT_THR_CTL_MASK;
 +			mmcra |= val << MMCRA_THR_CTL_SHIFT;
 +			val = (event[i] >> EVENT_THR_SEL_SHIFT) & EVENT_THR_SEL_MASK;
 +			mmcra |= val << MMCRA_THR_SEL_SHIFT;
 +			val = (event[i] >> EVENT_THR_CMP_SHIFT) & EVENT_THR_CMP_MASK;
 +			mmcra |= val << MMCRA_THR_CMP_SHIFT;
 +		}
 +
 +		if (event[i] & EVENT_WANTS_BHRB) {
 +			val = (event[i] >> EVENT_IFM_SHIFT) & EVENT_IFM_MASK;
 +			mmcra |= val << MMCRA_IFM_SHIFT;
 +		}
 +
 +		if (pevents[i]->attr.exclude_user)
 +			mmcr2 |= MMCR2_FCP(pmc);
 +
 +		if (pevents[i]->attr.exclude_hv)
 +			mmcr2 |= MMCR2_FCH(pmc);
 +
 +		if (pevents[i]->attr.exclude_kernel) {
 +			if (cpu_has_feature(CPU_FTR_HVMODE))
 +				mmcr2 |= MMCR2_FCH(pmc);
 +			else
 +				mmcr2 |= MMCR2_FCS(pmc);
 +		}
 +
 +		hwc[i] = pmc - 1;
 +	}
 +
 +	/* Return MMCRx values */
 +	mmcr[0] = 0;
 +
 +	/* pmc_inuse is 1-based */
 +	if (pmc_inuse & 2)
 +		mmcr[0] = MMCR0_PMC1CE;
 +
 +	if (pmc_inuse & 0x7c)
 +		mmcr[0] |= MMCR0_PMCjCE;
 +
 +	/* If we're not using PMC 5 or 6, freeze them */
 +	if (!(pmc_inuse & 0x60))
 +		mmcr[0] |= MMCR0_FC56;
 +
 +	mmcr[1] = mmcr1;
 +	mmcr[2] = mmcra;
 +	mmcr[3] = mmcr2;
 +
 +	return 0;
 +}
++=======
+ /* PowerISA v2.07 format attribute structure*/
+ extern struct attribute_group isa207_pmu_format_group;
++>>>>>>> 60b00025641e (powerpc/perf: factor out the event format field)
  
  /* Table of alternatives, sorted by column 0 */
  static const unsigned int event_alternatives[][MAX_ALT] = {
@@@ -425,42 -178,8 +430,45 @@@ static struct attribute_group power8_pm
  	.attrs = power8_events_attr,
  };
  
++<<<<<<< HEAD
 +PMU_FORMAT_ATTR(event,		"config:0-49");
 +PMU_FORMAT_ATTR(pmcxsel,	"config:0-7");
 +PMU_FORMAT_ATTR(mark,		"config:8");
 +PMU_FORMAT_ATTR(combine,	"config:11");
 +PMU_FORMAT_ATTR(unit,		"config:12-15");
 +PMU_FORMAT_ATTR(pmc,		"config:16-19");
 +PMU_FORMAT_ATTR(cache_sel,	"config:20-23");
 +PMU_FORMAT_ATTR(sample_mode,	"config:24-28");
 +PMU_FORMAT_ATTR(thresh_sel,	"config:29-31");
 +PMU_FORMAT_ATTR(thresh_stop,	"config:32-35");
 +PMU_FORMAT_ATTR(thresh_start,	"config:36-39");
 +PMU_FORMAT_ATTR(thresh_cmp,	"config:40-49");
 +
 +static struct attribute *power8_pmu_format_attr[] = {
 +	&format_attr_event.attr,
 +	&format_attr_pmcxsel.attr,
 +	&format_attr_mark.attr,
 +	&format_attr_combine.attr,
 +	&format_attr_unit.attr,
 +	&format_attr_pmc.attr,
 +	&format_attr_cache_sel.attr,
 +	&format_attr_sample_mode.attr,
 +	&format_attr_thresh_sel.attr,
 +	&format_attr_thresh_stop.attr,
 +	&format_attr_thresh_start.attr,
 +	&format_attr_thresh_cmp.attr,
 +	NULL,
 +};
 +
 +struct attribute_group power8_pmu_format_group = {
 +	.name = "format",
 +	.attrs = power8_pmu_format_attr,
 +};
 +
++=======
++>>>>>>> 60b00025641e (powerpc/perf: factor out the event format field)
  static const struct attribute_group *power8_pmu_attr_groups[] = {
- 	&power8_pmu_format_group,
+ 	&isa207_pmu_format_group,
  	&power8_pmu_events_group,
  	NULL,
  };
* Unmerged path arch/powerpc/perf/isa207-common.c
* Unmerged path arch/powerpc/perf/power9-pmu.c
* Unmerged path arch/powerpc/perf/isa207-common.c
* Unmerged path arch/powerpc/perf/power8-pmu.c
* Unmerged path arch/powerpc/perf/power9-pmu.c
