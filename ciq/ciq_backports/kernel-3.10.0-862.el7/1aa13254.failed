memcg, slab: clean up memcg cache initialization/destruction

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Vladimir Davydov <vdavydov@parallels.com>
commit 1aa13254259bdef0bca723849ab3bab308d2f0c3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1aa13254.failed

Currently, we have rather a messy function set relating to per-memcg
kmem cache initialization/destruction.

Per-memcg caches are created in memcg_create_kmem_cache().  This
function calls kmem_cache_create_memcg() to allocate and initialize a
kmem cache and then "registers" the new cache in the
memcg_params::memcg_caches array of the parent cache.

During its work-flow, kmem_cache_create_memcg() executes the following
memcg-related functions:

 - memcg_alloc_cache_params(), to initialize memcg_params of the newly
   created cache;
 - memcg_cache_list_add(), to add the new cache to the memcg_slab_caches
   list.

On the other hand, kmem_cache_destroy() called on a cache destruction
only calls memcg_release_cache(), which does all the work: it cleans the
reference to the cache in its parent's memcg_params::memcg_caches,
removes the cache from the memcg_slab_caches list, and frees
memcg_params.

Such an inconsistency between destruction and initialization paths make
the code difficult to read, so let's clean this up a bit.

This patch moves all the code relating to registration of per-memcg
caches (adding to memcg list, setting the pointer to a cache from its
parent) to the newly created memcg_register_cache() and
memcg_unregister_cache() functions making the initialization and
destruction paths look symmetrical.

	Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
	Cc: Michal Hocko <mhocko@suse.cz>
	Cc: Glauber Costa <glommer@gmail.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
	Cc: Pekka Enberg <penberg@kernel.org>
	Cc: Christoph Lameter <cl@linux.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1aa13254259bdef0bca723849ab3bab308d2f0c3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	mm/memcontrol.c
#	mm/slab_common.c
diff --cc include/linux/memcontrol.h
index 584b3a91b9fd,abd0113b6620..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -484,10 -497,11 +484,18 @@@ void __memcg_kmem_commit_charge(struct 
  void __memcg_kmem_uncharge_pages(struct page *page, int order);
  
  int memcg_cache_id(struct mem_cgroup *memcg);
++<<<<<<< HEAD
 +int memcg_register_cache(struct mem_cgroup *memcg, struct kmem_cache *s,
 +			 struct kmem_cache *root_cache);
 +void memcg_release_cache(struct kmem_cache *cachep);
 +void memcg_cache_list_add(struct mem_cgroup *memcg, struct kmem_cache *cachep);
++=======
+ int memcg_alloc_cache_params(struct mem_cgroup *memcg, struct kmem_cache *s,
+ 			     struct kmem_cache *root_cache);
+ void memcg_free_cache_params(struct kmem_cache *s);
+ void memcg_register_cache(struct kmem_cache *s);
+ void memcg_unregister_cache(struct kmem_cache *s);
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
  
  int memcg_update_cache_size(struct kmem_cache *s, int num_groups);
  void memcg_update_array_size(int num_groups);
@@@ -633,12 -647,15 +641,19 @@@ memcg_register_cache(struct mem_cgroup 
  	return 0;
  }
  
++<<<<<<< HEAD
 +static inline void memcg_release_cache(struct kmem_cache *cachep)
++=======
+ static inline void memcg_free_cache_params(struct kmem_cache *s)
  {
  }
  
- static inline void memcg_cache_list_add(struct mem_cgroup *memcg,
- 					struct kmem_cache *s)
+ static inline void memcg_register_cache(struct kmem_cache *s)
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
+ {
+ }
+ 
+ static inline void memcg_unregister_cache(struct kmem_cache *s)
  {
  }
  
diff --cc mm/memcontrol.c
index 23e6528af2de,739383cd3f70..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -3050,31 -3073,28 +3050,21 @@@ static int memcg_charge_kmem(struct mem
  	return ret;
  }
  
 -static void memcg_uncharge_kmem(struct mem_cgroup *memcg, u64 size)
 +static void memcg_uncharge_kmem(struct mem_cgroup *memcg,
 +				unsigned long nr_pages)
  {
 -	res_counter_uncharge(&memcg->res, size);
 +	page_counter_uncharge(&memcg->memory, nr_pages);
  	if (do_swap_account)
 -		res_counter_uncharge(&memcg->memsw, size);
 +		page_counter_uncharge(&memcg->memsw, nr_pages);
  
  	/* Not down to 0 */
 -	if (res_counter_uncharge(&memcg->kmem, size))
 +	if (page_counter_uncharge(&memcg->kmem, nr_pages))
  		return;
  
 -	/*
 -	 * Releases a reference taken in kmem_cgroup_css_offline in case
 -	 * this last uncharge is racing with the offlining code or it is
 -	 * outliving the memcg existence.
 -	 *
 -	 * The memory barrier imposed by test&clear is paired with the
 -	 * explicit one in memcg_kmem_mark_dead().
 -	 */
  	if (memcg_kmem_test_and_clear_dead(memcg))
 -		css_put(&memcg->css);
 +		mem_cgroup_put(memcg);
  }
  
- void memcg_cache_list_add(struct mem_cgroup *memcg, struct kmem_cache *cachep)
- {
- 	if (!memcg)
- 		return;
- 
- 	mutex_lock(&memcg->slab_caches_mutex);
- 	list_add(&cachep->memcg_params->list, &memcg->memcg_slab_caches);
- 	mutex_unlock(&memcg->slab_caches_mutex);
- }
- 
  /*
   * helper for acessing a memcg's index. It will be used as an index in the
   * child cache array in kmem_cache, and also to derive its name. This function
@@@ -3227,7 -3250,12 +3217,16 @@@ int memcg_register_cache(struct mem_cgr
  	return 0;
  }
  
++<<<<<<< HEAD
 +void memcg_release_cache(struct kmem_cache *s)
++=======
+ void memcg_free_cache_params(struct kmem_cache *s)
+ {
+ 	kfree(s->memcg_params);
+ }
+ 
+ void memcg_register_cache(struct kmem_cache *s)
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
  {
  	struct kmem_cache *root;
  	struct mem_cgroup *memcg;
@@@ -3253,9 -3301,7 +3272,13 @@@ void memcg_unregister_cache(struct kmem
  	list_del(&s->memcg_params->list);
  	mutex_unlock(&memcg->slab_caches_mutex);
  
++<<<<<<< HEAD
 +	mem_cgroup_put(memcg);
 +out:
 +	kfree(s->memcg_params);
++=======
+ 	css_put(&memcg->css);
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
  }
  
  /*
@@@ -3412,25 -3458,14 +3435,33 @@@ static struct kmem_cache *memcg_create_
  	idx = memcg_cache_id(memcg);
  
  	mutex_lock(&memcg_cache_mutex);
++<<<<<<< HEAD
 +	new_cachep = cachep->memcg_params->memcg_caches[idx];
++=======
+ 	new_cachep = cache_from_memcg_idx(cachep, idx);
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
  	if (new_cachep)
  		goto out;
  
  	new_cachep = kmem_cache_dup(memcg, cachep);
- 	if (new_cachep == NULL) {
+ 	if (new_cachep == NULL)
  		new_cachep = cachep;
++<<<<<<< HEAD
 +		goto out;
 +	}
 +
 +	mem_cgroup_get(memcg);
 +	atomic_set(&new_cachep->memcg_params->nr_pages , 0);
 +
 +	cachep->memcg_params->memcg_caches[idx] = new_cachep;
 +	/*
 +	 * the readers won't lock, make sure everybody sees the updated value,
 +	 * so they won't put stuff in the queue again for no reason
 +	 */
 +	wmb();
++=======
+ 
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
  out:
  	mutex_unlock(&memcg_cache_mutex);
  	return new_cachep;
@@@ -3512,7 -3545,6 +3543,10 @@@ static void memcg_create_cache_work_fun
  
  	cw = container_of(w, struct create_work, work);
  	memcg_create_kmem_cache(cw->memcg, cw->cachep);
++<<<<<<< HEAD
 +	/* Drop the reference gotten when we enqueued. */
++=======
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
  	css_put(&cw->memcg->css);
  	kfree(cw);
  }
diff --cc mm/slab_common.c
index e13d227ed0ab,db24ec48b946..000000000000
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@@ -209,40 -190,34 +209,52 @@@ kmem_cache_create_memcg(struct mem_cgro
  
  	s = __kmem_cache_alias(memcg, name, size, align, flags, ctor);
  	if (s)
 -		goto out_unlock;
 +		goto out_locked;
  
 -	err = -ENOMEM;
  	s = kmem_cache_zalloc(kmem_cache, GFP_KERNEL);
 -	if (!s)
 -		goto out_unlock;
 +	if (s) {
 +		s->object_size = s->size = size;
 +		s->align = calculate_alignment(flags, align, size);
 +		s->ctor = ctor;
  
 -	s->object_size = s->size = size;
 -	s->align = calculate_alignment(flags, align, size);
 -	s->ctor = ctor;
 +		if (memcg_register_cache(memcg, s, parent_cache)) {
 +			kmem_cache_free(kmem_cache, s);
 +			err = -ENOMEM;
 +			goto out_locked;
 +		}
  
 -	s->name = kstrdup(name, GFP_KERNEL);
 -	if (!s->name)
 -		goto out_free_cache;
 +		s->name = kstrdup(name, GFP_KERNEL);
 +		if (!s->name) {
 +			kmem_cache_free(kmem_cache, s);
 +			err = -ENOMEM;
 +			goto out_locked;
 +		}
  
 -	err = memcg_alloc_cache_params(memcg, s, parent_cache);
 -	if (err)
 -		goto out_free_cache;
 +		err = __kmem_cache_create(s, flags);
 +		if (!err) {
 +			s->refcount = 1;
 +			list_add(&s->list, &slab_caches);
 +			memcg_cache_list_add(memcg, s);
 +		} else {
 +			kfree(s->name);
 +			kmem_cache_free(kmem_cache, s);
 +		}
 +	} else
 +		err = -ENOMEM;
  
++<<<<<<< HEAD
 +out_locked:
++=======
+ 	err = __kmem_cache_create(s, flags);
+ 	if (err)
+ 		goto out_free_cache;
+ 
+ 	s->refcount = 1;
+ 	list_add(&s->list, &slab_caches);
+ 	memcg_register_cache(s);
+ 
+ out_unlock:
++>>>>>>> 1aa13254259b (memcg, slab: clean up memcg cache initialization/destruction)
  	mutex_unlock(&slab_mutex);
  	put_online_cpus();
  
* Unmerged path include/linux/memcontrol.h
* Unmerged path mm/memcontrol.c
* Unmerged path mm/slab_common.c
