vmbus: use rcu for per-cpu channel list

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Stephen Hemminger <stephen@networkplumber.org>
commit 8200f2085abe7f29a016381f3122000cc7b2a760
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/8200f208.failed

The per-cpu channel list is now referred to in the interrupt
routine. This is mostly safe since the host will not normally generate
an interrupt when channel is being deleted but if it did then there
would be a use after free problem.

To solve, this use RCU protection on ther per-cpu list.

Fixes: 631e63a9f346 ("vmbus: change to per channel tasklet")

	Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
	Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
	Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
(cherry picked from commit 8200f2085abe7f29a016381f3122000cc7b2a760)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/hv/channel_mgmt.c
#	drivers/hv/vmbus_drv.c
diff --cc drivers/hv/channel_mgmt.c
index 8a209b718a7d,d2cfa3eb71a2..000000000000
--- a/drivers/hv/channel_mgmt.c
+++ b/drivers/hv/channel_mgmt.c
@@@ -323,15 -349,18 +323,25 @@@ static struct vmbus_channel *alloc_chan
   */
  static void free_channel(struct vmbus_channel *channel)
  {
++<<<<<<< HEAD
 +	kfree(channel);
++=======
+ 	tasklet_kill(&channel->callback_event);
+ 
+ 	kfree_rcu(channel, rcu);
++>>>>>>> 8200f2085abe (vmbus: use rcu for per-cpu channel list)
  }
  
  static void percpu_channel_enq(void *arg)
  {
  	struct vmbus_channel *channel = arg;
 -	struct hv_per_cpu_context *hv_cpu
 -		= this_cpu_ptr(hv_context.cpu_context);
 +	int cpu = smp_processor_id();
  
++<<<<<<< HEAD
 +	list_add_tail(&channel->percpu_list, &hv_context.percpu_list[cpu]);
++=======
+ 	list_add_tail_rcu(&channel->percpu_list, &hv_cpu->chan_list);
++>>>>>>> 8200f2085abe (vmbus: use rcu for per-cpu channel list)
  }
  
  static void percpu_channel_deq(void *arg)
diff --cc drivers/hv/vmbus_drv.c
index 4aa498db4703,8370b9dc6037..000000000000
--- a/drivers/hv/vmbus_drv.c
+++ b/drivers/hv/vmbus_drv.c
@@@ -915,10 -887,87 +915,89 @@@ msg_handled
  	vmbus_signal_eom(msg, message_type);
  }
  
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Direct callback for channels using other deferred processing
+  */
+ static void vmbus_channel_isr(struct vmbus_channel *channel)
+ {
+ 	void (*callback_fn)(void *);
+ 
+ 	callback_fn = READ_ONCE(channel->onchannel_callback);
+ 	if (likely(callback_fn != NULL))
+ 		(*callback_fn)(channel->channel_callback_context);
+ }
+ 
+ /*
+  * Schedule all channels with events pending
+  */
+ static void vmbus_chan_sched(struct hv_per_cpu_context *hv_cpu)
+ {
+ 	unsigned long *recv_int_page;
+ 	u32 maxbits, relid;
+ 
+ 	if (vmbus_proto_version < VERSION_WIN8) {
+ 		maxbits = MAX_NUM_CHANNELS_SUPPORTED;
+ 		recv_int_page = vmbus_connection.recv_int_page;
+ 	} else {
+ 		/*
+ 		 * When the host is win8 and beyond, the event page
+ 		 * can be directly checked to get the id of the channel
+ 		 * that has the interrupt pending.
+ 		 */
+ 		void *page_addr = hv_cpu->synic_event_page;
+ 		union hv_synic_event_flags *event
+ 			= (union hv_synic_event_flags *)page_addr +
+ 						 VMBUS_MESSAGE_SINT;
+ 
+ 		maxbits = HV_EVENT_FLAGS_COUNT;
+ 		recv_int_page = event->flags;
+ 	}
+ 
+ 	if (unlikely(!recv_int_page))
+ 		return;
+ 
+ 	for_each_set_bit(relid, recv_int_page, maxbits) {
+ 		struct vmbus_channel *channel;
+ 
+ 		if (!sync_test_and_clear_bit(relid, recv_int_page))
+ 			continue;
+ 
+ 		/* Special case - vmbus channel protocol msg */
+ 		if (relid == 0)
+ 			continue;
+ 
+ 		rcu_read_lock();
+ 
+ 		/* Find channel based on relid */
+ 		list_for_each_entry_rcu(channel, &hv_cpu->chan_list, percpu_list) {
+ 			if (channel->offermsg.child_relid != relid)
+ 				continue;
+ 
+ 			switch (channel->callback_mode) {
+ 			case HV_CALL_ISR:
+ 				vmbus_channel_isr(channel);
+ 				break;
+ 
+ 			case HV_CALL_BATCHED:
+ 				hv_begin_read(&channel->inbound);
+ 				/* fallthrough */
+ 			case HV_CALL_DIRECT:
+ 				tasklet_schedule(&channel->callback_event);
+ 			}
+ 		}
+ 
+ 		rcu_read_unlock();
+ 	}
+ }
+ 
++>>>>>>> 8200f2085abe (vmbus: use rcu for per-cpu channel list)
  static void vmbus_isr(void)
  {
 -	struct hv_per_cpu_context *hv_cpu
 -		= this_cpu_ptr(hv_context.cpu_context);
 -	void *page_addr = hv_cpu->synic_event_page;
 +	int cpu = smp_processor_id();
 +	void *page_addr;
  	struct hv_message *msg;
  	union hv_synic_event_flags *event;
  	bool handled = false;
* Unmerged path drivers/hv/channel_mgmt.c
* Unmerged path drivers/hv/vmbus_drv.c
diff --git a/include/linux/hyperv.h b/include/linux/hyperv.h
index 2d127a5ad611..c3c199f01ed5 100644
--- a/include/linux/hyperv.h
+++ b/include/linux/hyperv.h
@@ -846,6 +846,13 @@ struct vmbus_channel {
 	 * link up channels based on their CPU affinity.
 	 */
 	struct list_head percpu_list;
+
+	/*
+	 * Defer freeing channel until after all cpu's have
+	 * gone through grace period.
+	 */
+	struct rcu_head rcu;
+
 	/*
 	 * Host signaling policy: The default policy will be
 	 * based on the ring buffer state. We will also support
