nvme-fc: decouple ns references from lldd references

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author James Smart <jsmart2021@gmail.com>
commit 158bfb8888c3f2beab747ab4f7a8197639f03e54
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/158bfb88.failed

In the lldd api, a lldd may unregister a remoteport (loss of connectivity
or driver unload) or localport (driver unload). The lldd must wait for the
remoteport_delete or localport_delete before completing its actions post
the unregister.  The xxx_deletes currently occur only when the xxxport
structure is fully freed after all references are removed. Thus the lldd
may be held hostage until an app or in-kernel entity that has a namespace
open finally closes so the namespace can be removed, the controller
removed, thus the transport objects, thus the lldd.

This patch decouples the transport and os-facing objects from the lldd
and the remoteport and localport. There is a point in all deletions
where the transport will no longer interact with the lldd on behalf of
a controller. That point centers around the association established
with the target/subsystem. It will access the lldd whenever it attempts
to create an association and while the association is active. New
associations may only be created if the remoteport is live (thus the
localport is live). It will not access the lldd after deleting the
association.

Therefore, the patch tracks the count of active controllers - those with
associations being created or that are active - on a remoteport. It also
tracks the number of remoteports that have active controllers, on a
a localport. When a remoteport is unregistered, as soon as there are no
active controllers, the lldd's remoteport_delete may be called and the
lldd may continue. Similarly, when a localport is unregistered, as soon
as there are no remoteports with active controllers, the localport_delete
callback may be made. This significantly speeds up unregistration with
the lldd.

The transport objects continue in suspended status with reconnect timers
running, and upon expiration, normal ref-counting will occur and the
objects will be freed. The transport object may still be held hostage
by the application/kernel module, but that is acceptable.

With this change, the lldd may be fully unloaded and reloaded, and
if registrations occur prior to the timeouts, the nvme controller and
namespaces will resume normally as if a link bounce.

	Signed-off-by: James Smart <james.smart@broadcom.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 158bfb8888c3f2beab747ab4f7a8197639f03e54)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/fc.c
diff --cc drivers/nvme/host/fc.c
index bbc0c7f6abc6,da9296a1eb26..000000000000
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@@ -122,13 -134,17 +123,18 @@@ struct nvme_fc_rport 
  
  	struct list_head		endp_list; /* for lport->endp_list */
  	struct list_head		ctrl_list;
 -	struct list_head		ls_req_list;
 -	struct device			*dev;	/* physical device for dma */
 -	struct nvme_fc_lport		*lport;
  	spinlock_t			lock;
  	struct kref			ref;
++<<<<<<< HEAD
++=======
+ 	atomic_t                        act_ctrl_cnt;
+ 	unsigned long			dev_loss_end;
++>>>>>>> 158bfb8888c3 (nvme-fc: decouple ns references from lldd references)
  } __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
  
 -enum nvme_fcctrl_flags {
 -	FCCTRL_TERMIO		= (1 << 0),
 +enum nvme_fcctrl_state {
 +	FCCTRL_INIT		= 0,
 +	FCCTRL_ACTIVE		= 1,
  };
  
  struct nvme_fc_ctrl {
@@@ -141,12 -155,10 +147,13 @@@
  	struct nvme_fc_rport	*rport;
  	u32			cnum;
  
+ 	bool			assoc_active;
  	u64			association_id;
  
 +	u64			cap;
 +
  	struct list_head	ctrl_list;	/* rport->ctrl_list */
 +	struct list_head	ls_req_list;
  
  	struct blk_mq_tag_set	admin_tag_set;
  	struct blk_mq_tag_set	tag_set;
@@@ -602,6 -538,282 +607,285 @@@ nvme_fc_rport_get(struct nvme_fc_rport 
  	return kref_get_unless_zero(&rport->ref);
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ nvme_fc_resume_controller(struct nvme_fc_ctrl *ctrl)
+ {
+ 	switch (ctrl->ctrl.state) {
+ 	case NVME_CTRL_NEW:
+ 	case NVME_CTRL_RECONNECTING:
+ 		/*
+ 		 * As all reconnects were suppressed, schedule a
+ 		 * connect.
+ 		 */
+ 		dev_info(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: connectivity re-established. "
+ 			"Attempting reconnect\n", ctrl->cnum);
+ 
+ 		queue_delayed_work(nvme_wq, &ctrl->connect_work, 0);
+ 		break;
+ 
+ 	case NVME_CTRL_RESETTING:
+ 		/*
+ 		 * Controller is already in the process of terminating the
+ 		 * association. No need to do anything further. The reconnect
+ 		 * step will naturally occur after the reset completes.
+ 		 */
+ 		break;
+ 
+ 	default:
+ 		/* no action to take - let it delete */
+ 		break;
+ 	}
+ }
+ 
+ static struct nvme_fc_rport *
+ nvme_fc_attach_to_suspended_rport(struct nvme_fc_lport *lport,
+ 				struct nvme_fc_port_info *pinfo)
+ {
+ 	struct nvme_fc_rport *rport;
+ 	struct nvme_fc_ctrl *ctrl;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&nvme_fc_lock, flags);
+ 
+ 	list_for_each_entry(rport, &lport->endp_list, endp_list) {
+ 		if (rport->remoteport.node_name != pinfo->node_name ||
+ 		    rport->remoteport.port_name != pinfo->port_name)
+ 			continue;
+ 
+ 		if (!nvme_fc_rport_get(rport)) {
+ 			rport = ERR_PTR(-ENOLCK);
+ 			goto out_done;
+ 		}
+ 
+ 		spin_unlock_irqrestore(&nvme_fc_lock, flags);
+ 
+ 		spin_lock_irqsave(&rport->lock, flags);
+ 
+ 		/* has it been unregistered */
+ 		if (rport->remoteport.port_state != FC_OBJSTATE_DELETED) {
+ 			/* means lldd called us twice */
+ 			spin_unlock_irqrestore(&rport->lock, flags);
+ 			nvme_fc_rport_put(rport);
+ 			return ERR_PTR(-ESTALE);
+ 		}
+ 
+ 		rport->remoteport.port_state = FC_OBJSTATE_ONLINE;
+ 		rport->dev_loss_end = 0;
+ 
+ 		/*
+ 		 * kick off a reconnect attempt on all associations to the
+ 		 * remote port. A successful reconnects will resume i/o.
+ 		 */
+ 		list_for_each_entry(ctrl, &rport->ctrl_list, ctrl_list)
+ 			nvme_fc_resume_controller(ctrl);
+ 
+ 		spin_unlock_irqrestore(&rport->lock, flags);
+ 
+ 		return rport;
+ 	}
+ 
+ 	rport = NULL;
+ 
+ out_done:
+ 	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+ 
+ 	return rport;
+ }
+ 
+ static inline void
+ __nvme_fc_set_dev_loss_tmo(struct nvme_fc_rport *rport,
+ 			struct nvme_fc_port_info *pinfo)
+ {
+ 	if (pinfo->dev_loss_tmo)
+ 		rport->remoteport.dev_loss_tmo = pinfo->dev_loss_tmo;
+ 	else
+ 		rport->remoteport.dev_loss_tmo = NVME_FC_DEFAULT_DEV_LOSS_TMO;
+ }
+ 
+ /**
+  * nvme_fc_register_remoteport - transport entry point called by an
+  *                              LLDD to register the existence of a NVME
+  *                              subsystem FC port on its fabric.
+  * @localport: pointer to the (registered) local port that the remote
+  *             subsystem port is connected to.
+  * @pinfo:     pointer to information about the port to be registered
+  * @rport_p:   pointer to a remote port pointer. Upon success, the routine
+  *             will allocate a nvme_fc_remote_port structure and place its
+  *             address in the remote port pointer. Upon failure, remote port
+  *             pointer will be set to 0.
+  *
+  * Returns:
+  * a completion status. Must be 0 upon success; a negative errno
+  * (ex: -ENXIO) upon failure.
+  */
+ int
+ nvme_fc_register_remoteport(struct nvme_fc_local_port *localport,
+ 				struct nvme_fc_port_info *pinfo,
+ 				struct nvme_fc_remote_port **portptr)
+ {
+ 	struct nvme_fc_lport *lport = localport_to_lport(localport);
+ 	struct nvme_fc_rport *newrec;
+ 	unsigned long flags;
+ 	int ret, idx;
+ 
+ 	if (!nvme_fc_lport_get(lport)) {
+ 		ret = -ESHUTDOWN;
+ 		goto out_reghost_failed;
+ 	}
+ 
+ 	/*
+ 	 * look to see if there is already a remoteport that is waiting
+ 	 * for a reconnect (within dev_loss_tmo) with the same WWN's.
+ 	 * If so, transition to it and reconnect.
+ 	 */
+ 	newrec = nvme_fc_attach_to_suspended_rport(lport, pinfo);
+ 
+ 	/* found an rport, but something about its state is bad */
+ 	if (IS_ERR(newrec)) {
+ 		ret = PTR_ERR(newrec);
+ 		goto out_lport_put;
+ 
+ 	/* found existing rport, which was resumed */
+ 	} else if (newrec) {
+ 		nvme_fc_lport_put(lport);
+ 		__nvme_fc_set_dev_loss_tmo(newrec, pinfo);
+ 		nvme_fc_signal_discovery_scan(lport, newrec);
+ 		*portptr = &newrec->remoteport;
+ 		return 0;
+ 	}
+ 
+ 	/* nothing found - allocate a new remoteport struct */
+ 
+ 	newrec = kmalloc((sizeof(*newrec) + lport->ops->remote_priv_sz),
+ 			 GFP_KERNEL);
+ 	if (!newrec) {
+ 		ret = -ENOMEM;
+ 		goto out_lport_put;
+ 	}
+ 
+ 	idx = ida_simple_get(&lport->endp_cnt, 0, 0, GFP_KERNEL);
+ 	if (idx < 0) {
+ 		ret = -ENOSPC;
+ 		goto out_kfree_rport;
+ 	}
+ 
+ 	INIT_LIST_HEAD(&newrec->endp_list);
+ 	INIT_LIST_HEAD(&newrec->ctrl_list);
+ 	INIT_LIST_HEAD(&newrec->ls_req_list);
+ 	kref_init(&newrec->ref);
+ 	atomic_set(&newrec->act_ctrl_cnt, 0);
+ 	spin_lock_init(&newrec->lock);
+ 	newrec->remoteport.localport = &lport->localport;
+ 	newrec->dev = lport->dev;
+ 	newrec->lport = lport;
+ 	newrec->remoteport.private = &newrec[1];
+ 	newrec->remoteport.port_role = pinfo->port_role;
+ 	newrec->remoteport.node_name = pinfo->node_name;
+ 	newrec->remoteport.port_name = pinfo->port_name;
+ 	newrec->remoteport.port_id = pinfo->port_id;
+ 	newrec->remoteport.port_state = FC_OBJSTATE_ONLINE;
+ 	newrec->remoteport.port_num = idx;
+ 	__nvme_fc_set_dev_loss_tmo(newrec, pinfo);
+ 
+ 	spin_lock_irqsave(&nvme_fc_lock, flags);
+ 	list_add_tail(&newrec->endp_list, &lport->endp_list);
+ 	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+ 
+ 	nvme_fc_signal_discovery_scan(lport, newrec);
+ 
+ 	*portptr = &newrec->remoteport;
+ 	return 0;
+ 
+ out_kfree_rport:
+ 	kfree(newrec);
+ out_lport_put:
+ 	nvme_fc_lport_put(lport);
+ out_reghost_failed:
+ 	*portptr = NULL;
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(nvme_fc_register_remoteport);
+ 
+ static int
+ nvme_fc_abort_lsops(struct nvme_fc_rport *rport)
+ {
+ 	struct nvmefc_ls_req_op *lsop;
+ 	unsigned long flags;
+ 
+ restart:
+ 	spin_lock_irqsave(&rport->lock, flags);
+ 
+ 	list_for_each_entry(lsop, &rport->ls_req_list, lsreq_list) {
+ 		if (!(lsop->flags & FCOP_FLAGS_TERMIO)) {
+ 			lsop->flags |= FCOP_FLAGS_TERMIO;
+ 			spin_unlock_irqrestore(&rport->lock, flags);
+ 			rport->lport->ops->ls_abort(&rport->lport->localport,
+ 						&rport->remoteport,
+ 						&lsop->ls_req);
+ 			goto restart;
+ 		}
+ 	}
+ 	spin_unlock_irqrestore(&rport->lock, flags);
+ 
+ 	return 0;
+ }
+ 
+ static void
+ nvme_fc_ctrl_connectivity_loss(struct nvme_fc_ctrl *ctrl)
+ {
+ 	dev_info(ctrl->ctrl.device,
+ 		"NVME-FC{%d}: controller connectivity lost. Awaiting "
+ 		"Reconnect", ctrl->cnum);
+ 
+ 	switch (ctrl->ctrl.state) {
+ 	case NVME_CTRL_NEW:
+ 	case NVME_CTRL_LIVE:
+ 		/*
+ 		 * Schedule a controller reset. The reset will terminate the
+ 		 * association and schedule the reconnect timer.  Reconnects
+ 		 * will be attempted until either the ctlr_loss_tmo
+ 		 * (max_retries * connect_delay) expires or the remoteport's
+ 		 * dev_loss_tmo expires.
+ 		 */
+ 		if (nvme_reset_ctrl(&ctrl->ctrl)) {
+ 			dev_warn(ctrl->ctrl.device,
+ 				"NVME-FC{%d}: Couldn't schedule reset. "
+ 				"Deleting controller.\n",
+ 				ctrl->cnum);
+ 			nvme_delete_ctrl(&ctrl->ctrl);
+ 		}
+ 		break;
+ 
+ 	case NVME_CTRL_RECONNECTING:
+ 		/*
+ 		 * The association has already been terminated and the
+ 		 * controller is attempting reconnects.  No need to do anything
+ 		 * futher.  Reconnects will be attempted until either the
+ 		 * ctlr_loss_tmo (max_retries * connect_delay) expires or the
+ 		 * remoteport's dev_loss_tmo expires.
+ 		 */
+ 		break;
+ 
+ 	case NVME_CTRL_RESETTING:
+ 		/*
+ 		 * Controller is already in the process of terminating the
+ 		 * association.  No need to do anything further. The reconnect
+ 		 * step will kick in naturally after the association is
+ 		 * terminated.
+ 		 */
+ 		break;
+ 
+ 	case NVME_CTRL_DELETING:
+ 	default:
+ 		/* no action to take - let it delete */
+ 		break;
+ 	}
+ }
+ 
++>>>>>>> 158bfb8888c3 (nvme-fc: decouple ns references from lldd references)
  /**
   * nvme_fc_unregister_remoteport - transport entry point called by an
   *                              LLDD to deregister/remove a previously
@@@ -637,7 -859,18 +921,20 @@@ nvme_fc_unregister_remoteport(struct nv
  
  	spin_unlock_irqrestore(&rport->lock, flags);
  
++<<<<<<< HEAD
++=======
+ 	nvme_fc_abort_lsops(rport);
+ 
+ 	if (atomic_read(&rport->act_ctrl_cnt) == 0)
+ 		rport->lport->ops->remoteport_delete(portptr);
+ 
+ 	/*
+ 	 * release the reference, which will allow, if all controllers
+ 	 * go away, which should only occur after dev_loss_tmo occurs,
+ 	 * for the rport to be torn down.
+ 	 */
++>>>>>>> 158bfb8888c3 (nvme-fc: decouple ns references from lldd references)
  	nvme_fc_rport_put(rport);
 -
  	return 0;
  }
  EXPORT_SYMBOL_GPL(nvme_fc_unregister_remoteport);
@@@ -2419,6 -2598,461 +2716,464 @@@ out_free_tag_set
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ nvme_fc_reinit_io_queues(struct nvme_fc_ctrl *ctrl)
+ {
+ 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+ 	unsigned int nr_io_queues;
+ 	int ret;
+ 
+ 	nr_io_queues = min(min(opts->nr_io_queues, num_online_cpus()),
+ 				ctrl->lport->ops->max_hw_queues);
+ 	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ 	if (ret) {
+ 		dev_info(ctrl->ctrl.device,
+ 			"set_queue_count failed: %d\n", ret);
+ 		return ret;
+ 	}
+ 
+ 	ctrl->ctrl.queue_count = nr_io_queues + 1;
+ 	/* check for io queues existing */
+ 	if (ctrl->ctrl.queue_count == 1)
+ 		return 0;
+ 
+ 	nvme_fc_init_io_queues(ctrl);
+ 
+ 	ret = nvme_reinit_tagset(&ctrl->ctrl, ctrl->ctrl.tagset);
+ 	if (ret)
+ 		goto out_free_io_queues;
+ 
+ 	ret = nvme_fc_create_hw_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
+ 	if (ret)
+ 		goto out_free_io_queues;
+ 
+ 	ret = nvme_fc_connect_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
+ 	if (ret)
+ 		goto out_delete_hw_queues;
+ 
+ 	blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ 
+ 	return 0;
+ 
+ out_delete_hw_queues:
+ 	nvme_fc_delete_hw_io_queues(ctrl);
+ out_free_io_queues:
+ 	nvme_fc_free_io_queues(ctrl);
+ 	return ret;
+ }
+ 
+ static void
+ nvme_fc_rport_active_on_lport(struct nvme_fc_rport *rport)
+ {
+ 	struct nvme_fc_lport *lport = rport->lport;
+ 
+ 	atomic_inc(&lport->act_rport_cnt);
+ }
+ 
+ static void
+ nvme_fc_rport_inactive_on_lport(struct nvme_fc_rport *rport)
+ {
+ 	struct nvme_fc_lport *lport = rport->lport;
+ 	u32 cnt;
+ 
+ 	cnt = atomic_dec_return(&lport->act_rport_cnt);
+ 	if (cnt == 0 && lport->localport.port_state == FC_OBJSTATE_DELETED)
+ 		lport->ops->localport_delete(&lport->localport);
+ }
+ 
+ static int
+ nvme_fc_ctlr_active_on_rport(struct nvme_fc_ctrl *ctrl)
+ {
+ 	struct nvme_fc_rport *rport = ctrl->rport;
+ 	u32 cnt;
+ 
+ 	if (ctrl->assoc_active)
+ 		return 1;
+ 
+ 	ctrl->assoc_active = true;
+ 	cnt = atomic_inc_return(&rport->act_ctrl_cnt);
+ 	if (cnt == 1)
+ 		nvme_fc_rport_active_on_lport(rport);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ nvme_fc_ctlr_inactive_on_rport(struct nvme_fc_ctrl *ctrl)
+ {
+ 	struct nvme_fc_rport *rport = ctrl->rport;
+ 	struct nvme_fc_lport *lport = rport->lport;
+ 	u32 cnt;
+ 
+ 	/* ctrl->assoc_active=false will be set independently */
+ 
+ 	cnt = atomic_dec_return(&rport->act_ctrl_cnt);
+ 	if (cnt == 0) {
+ 		if (rport->remoteport.port_state == FC_OBJSTATE_DELETED)
+ 			lport->ops->remoteport_delete(&rport->remoteport);
+ 		nvme_fc_rport_inactive_on_lport(rport);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * This routine restarts the controller on the host side, and
+  * on the link side, recreates the controller association.
+  */
+ static int
+ nvme_fc_create_association(struct nvme_fc_ctrl *ctrl)
+ {
+ 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+ 	int ret;
+ 	bool changed;
+ 
+ 	++ctrl->ctrl.nr_reconnects;
+ 
+ 	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE)
+ 		return -ENODEV;
+ 
+ 	if (nvme_fc_ctlr_active_on_rport(ctrl))
+ 		return -ENOTUNIQ;
+ 
+ 	/*
+ 	 * Create the admin queue
+ 	 */
+ 
+ 	nvme_fc_init_queue(ctrl, 0, NVME_FC_AQ_BLKMQ_DEPTH);
+ 
+ 	ret = __nvme_fc_create_hw_queue(ctrl, &ctrl->queues[0], 0,
+ 				NVME_FC_AQ_BLKMQ_DEPTH);
+ 	if (ret)
+ 		goto out_free_queue;
+ 
+ 	ret = nvme_fc_connect_admin_queue(ctrl, &ctrl->queues[0],
+ 				NVME_FC_AQ_BLKMQ_DEPTH,
+ 				(NVME_FC_AQ_BLKMQ_DEPTH / 4));
+ 	if (ret)
+ 		goto out_delete_hw_queue;
+ 
+ 	if (ctrl->ctrl.state != NVME_CTRL_NEW)
+ 		blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ 
+ 	ret = nvmf_connect_admin_queue(&ctrl->ctrl);
+ 	if (ret)
+ 		goto out_disconnect_admin_queue;
+ 
+ 	/*
+ 	 * Check controller capabilities
+ 	 *
+ 	 * todo:- add code to check if ctrl attributes changed from
+ 	 * prior connection values
+ 	 */
+ 
+ 	ret = nvmf_reg_read64(&ctrl->ctrl, NVME_REG_CAP, &ctrl->ctrl.cap);
+ 	if (ret) {
+ 		dev_err(ctrl->ctrl.device,
+ 			"prop_get NVME_REG_CAP failed\n");
+ 		goto out_disconnect_admin_queue;
+ 	}
+ 
+ 	ctrl->ctrl.sqsize =
+ 		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap) + 1, ctrl->ctrl.sqsize);
+ 
+ 	ret = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ 	if (ret)
+ 		goto out_disconnect_admin_queue;
+ 
+ 	ctrl->ctrl.max_hw_sectors =
+ 		(ctrl->lport->ops->max_sgl_segments - 1) << (PAGE_SHIFT - 9);
+ 
+ 	ret = nvme_init_identify(&ctrl->ctrl);
+ 	if (ret)
+ 		goto out_disconnect_admin_queue;
+ 
+ 	/* sanity checks */
+ 
+ 	/* FC-NVME does not have other data in the capsule */
+ 	if (ctrl->ctrl.icdoff) {
+ 		dev_err(ctrl->ctrl.device, "icdoff %d is not supported!\n",
+ 				ctrl->ctrl.icdoff);
+ 		goto out_disconnect_admin_queue;
+ 	}
+ 
+ 	/* FC-NVME supports normal SGL Data Block Descriptors */
+ 
+ 	if (opts->queue_size > ctrl->ctrl.maxcmd) {
+ 		/* warn if maxcmd is lower than queue_size */
+ 		dev_warn(ctrl->ctrl.device,
+ 			"queue_size %zu > ctrl maxcmd %u, reducing "
+ 			"to queue_size\n",
+ 			opts->queue_size, ctrl->ctrl.maxcmd);
+ 		opts->queue_size = ctrl->ctrl.maxcmd;
+ 	}
+ 
+ 	ret = nvme_fc_init_aen_ops(ctrl);
+ 	if (ret)
+ 		goto out_term_aen_ops;
+ 
+ 	/*
+ 	 * Create the io queues
+ 	 */
+ 
+ 	if (ctrl->ctrl.queue_count > 1) {
+ 		if (ctrl->ctrl.state == NVME_CTRL_NEW)
+ 			ret = nvme_fc_create_io_queues(ctrl);
+ 		else
+ 			ret = nvme_fc_reinit_io_queues(ctrl);
+ 		if (ret)
+ 			goto out_term_aen_ops;
+ 	}
+ 
+ 	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+ 
+ 	ctrl->ctrl.nr_reconnects = 0;
+ 
+ 	if (changed)
+ 		nvme_start_ctrl(&ctrl->ctrl);
+ 
+ 	return 0;	/* Success */
+ 
+ out_term_aen_ops:
+ 	nvme_fc_term_aen_ops(ctrl);
+ out_disconnect_admin_queue:
+ 	/* send a Disconnect(association) LS to fc-nvme target */
+ 	nvme_fc_xmt_disconnect_assoc(ctrl);
+ out_delete_hw_queue:
+ 	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
+ out_free_queue:
+ 	nvme_fc_free_queue(&ctrl->queues[0]);
+ 	ctrl->assoc_active = false;
+ 	nvme_fc_ctlr_inactive_on_rport(ctrl);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * This routine stops operation of the controller on the host side.
+  * On the host os stack side: Admin and IO queues are stopped,
+  *   outstanding ios on them terminated via FC ABTS.
+  * On the link side: the association is terminated.
+  */
+ static void
+ nvme_fc_delete_association(struct nvme_fc_ctrl *ctrl)
+ {
+ 	unsigned long flags;
+ 
+ 	if (!ctrl->assoc_active)
+ 		return;
+ 	ctrl->assoc_active = false;
+ 
+ 	spin_lock_irqsave(&ctrl->lock, flags);
+ 	ctrl->flags |= FCCTRL_TERMIO;
+ 	ctrl->iocnt = 0;
+ 	spin_unlock_irqrestore(&ctrl->lock, flags);
+ 
+ 	/*
+ 	 * If io queues are present, stop them and terminate all outstanding
+ 	 * ios on them. As FC allocates FC exchange for each io, the
+ 	 * transport must contact the LLDD to terminate the exchange,
+ 	 * thus releasing the FC exchange. We use blk_mq_tagset_busy_itr()
+ 	 * to tell us what io's are busy and invoke a transport routine
+ 	 * to kill them with the LLDD.  After terminating the exchange
+ 	 * the LLDD will call the transport's normal io done path, but it
+ 	 * will have an aborted status. The done path will return the
+ 	 * io requests back to the block layer as part of normal completions
+ 	 * (but with error status).
+ 	 */
+ 	if (ctrl->ctrl.queue_count > 1) {
+ 		nvme_stop_queues(&ctrl->ctrl);
+ 		blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ 				nvme_fc_terminate_exchange, &ctrl->ctrl);
+ 	}
+ 
+ 	/*
+ 	 * Other transports, which don't have link-level contexts bound
+ 	 * to sqe's, would try to gracefully shutdown the controller by
+ 	 * writing the registers for shutdown and polling (call
+ 	 * nvme_shutdown_ctrl()). Given a bunch of i/o was potentially
+ 	 * just aborted and we will wait on those contexts, and given
+ 	 * there was no indication of how live the controlelr is on the
+ 	 * link, don't send more io to create more contexts for the
+ 	 * shutdown. Let the controller fail via keepalive failure if
+ 	 * its still present.
+ 	 */
+ 
+ 	/*
+ 	 * clean up the admin queue. Same thing as above.
+ 	 * use blk_mq_tagset_busy_itr() and the transport routine to
+ 	 * terminate the exchanges.
+ 	 */
+ 	if (ctrl->ctrl.state != NVME_CTRL_NEW)
+ 		blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ 	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ 				nvme_fc_terminate_exchange, &ctrl->ctrl);
+ 
+ 	/* kill the aens as they are a separate path */
+ 	nvme_fc_abort_aen_ops(ctrl);
+ 
+ 	/* wait for all io that had to be aborted */
+ 	spin_lock_irqsave(&ctrl->lock, flags);
+ 	wait_event_lock_irq(ctrl->ioabort_wait, ctrl->iocnt == 0, ctrl->lock);
+ 	ctrl->flags &= ~FCCTRL_TERMIO;
+ 	spin_unlock_irqrestore(&ctrl->lock, flags);
+ 
+ 	nvme_fc_term_aen_ops(ctrl);
+ 
+ 	/*
+ 	 * send a Disconnect(association) LS to fc-nvme target
+ 	 * Note: could have been sent at top of process, but
+ 	 * cleaner on link traffic if after the aborts complete.
+ 	 * Note: if association doesn't exist, association_id will be 0
+ 	 */
+ 	if (ctrl->association_id)
+ 		nvme_fc_xmt_disconnect_assoc(ctrl);
+ 
+ 	if (ctrl->ctrl.tagset) {
+ 		nvme_fc_delete_hw_io_queues(ctrl);
+ 		nvme_fc_free_io_queues(ctrl);
+ 	}
+ 
+ 	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
+ 	nvme_fc_free_queue(&ctrl->queues[0]);
+ 
+ 	nvme_fc_ctlr_inactive_on_rport(ctrl);
+ }
+ 
+ static void
+ nvme_fc_delete_ctrl(struct nvme_ctrl *nctrl)
+ {
+ 	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
+ 
+ 	cancel_delayed_work_sync(&ctrl->connect_work);
+ 	/*
+ 	 * kill the association on the link side.  this will block
+ 	 * waiting for io to terminate
+ 	 */
+ 	nvme_fc_delete_association(ctrl);
+ }
+ 
+ static void
+ nvme_fc_reconnect_or_delete(struct nvme_fc_ctrl *ctrl, int status)
+ {
+ 	struct nvme_fc_rport *rport = ctrl->rport;
+ 	struct nvme_fc_remote_port *portptr = &rport->remoteport;
+ 	unsigned long recon_delay = ctrl->ctrl.opts->reconnect_delay * HZ;
+ 	bool recon = true;
+ 
+ 	if (ctrl->ctrl.state != NVME_CTRL_RECONNECTING)
+ 		return;
+ 
+ 	if (portptr->port_state == FC_OBJSTATE_ONLINE)
+ 		dev_info(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: reset: Reconnect attempt failed (%d)\n",
+ 			ctrl->cnum, status);
+ 	else if (time_after_eq(jiffies, rport->dev_loss_end))
+ 		recon = false;
+ 
+ 	if (recon && nvmf_should_reconnect(&ctrl->ctrl)) {
+ 		if (portptr->port_state == FC_OBJSTATE_ONLINE)
+ 			dev_info(ctrl->ctrl.device,
+ 				"NVME-FC{%d}: Reconnect attempt in %ld "
+ 				"seconds\n",
+ 				ctrl->cnum, recon_delay / HZ);
+ 		else if (time_after(jiffies + recon_delay, rport->dev_loss_end))
+ 			recon_delay = rport->dev_loss_end - jiffies;
+ 
+ 		queue_delayed_work(nvme_wq, &ctrl->connect_work, recon_delay);
+ 	} else {
+ 		if (portptr->port_state == FC_OBJSTATE_ONLINE)
+ 			dev_warn(ctrl->ctrl.device,
+ 				"NVME-FC{%d}: Max reconnect attempts (%d) "
+ 				"reached. Removing controller\n",
+ 				ctrl->cnum, ctrl->ctrl.nr_reconnects);
+ 		else
+ 			dev_warn(ctrl->ctrl.device,
+ 				"NVME-FC{%d}: dev_loss_tmo (%d) expired "
+ 				"while waiting for remoteport connectivity. "
+ 				"Removing controller\n", ctrl->cnum,
+ 				portptr->dev_loss_tmo);
+ 		WARN_ON(nvme_delete_ctrl(&ctrl->ctrl));
+ 	}
+ }
+ 
+ static void
+ nvme_fc_reset_ctrl_work(struct work_struct *work)
+ {
+ 	struct nvme_fc_ctrl *ctrl =
+ 		container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
+ 	int ret;
+ 
+ 	nvme_stop_ctrl(&ctrl->ctrl);
+ 
+ 	/* will block will waiting for io to terminate */
+ 	nvme_fc_delete_association(ctrl);
+ 
+ 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RECONNECTING)) {
+ 		dev_err(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: error_recovery: Couldn't change state "
+ 			"to RECONNECTING\n", ctrl->cnum);
+ 		return;
+ 	}
+ 
+ 	if (ctrl->rport->remoteport.port_state == FC_OBJSTATE_ONLINE)
+ 		ret = nvme_fc_create_association(ctrl);
+ 	else
+ 		ret = -ENOTCONN;
+ 
+ 	if (ret)
+ 		nvme_fc_reconnect_or_delete(ctrl, ret);
+ 	else
+ 		dev_info(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: controller reset complete\n",
+ 			ctrl->cnum);
+ }
+ 
+ static const struct nvme_ctrl_ops nvme_fc_ctrl_ops = {
+ 	.name			= "fc",
+ 	.module			= THIS_MODULE,
+ 	.flags			= NVME_F_FABRICS,
+ 	.reg_read32		= nvmf_reg_read32,
+ 	.reg_read64		= nvmf_reg_read64,
+ 	.reg_write32		= nvmf_reg_write32,
+ 	.free_ctrl		= nvme_fc_nvme_ctrl_freed,
+ 	.submit_async_event	= nvme_fc_submit_async_event,
+ 	.delete_ctrl		= nvme_fc_delete_ctrl,
+ 	.get_address		= nvmf_get_address,
+ 	.reinit_request		= nvme_fc_reinit_request,
+ };
+ 
+ static void
+ nvme_fc_connect_ctrl_work(struct work_struct *work)
+ {
+ 	int ret;
+ 
+ 	struct nvme_fc_ctrl *ctrl =
+ 			container_of(to_delayed_work(work),
+ 				struct nvme_fc_ctrl, connect_work);
+ 
+ 	ret = nvme_fc_create_association(ctrl);
+ 	if (ret)
+ 		nvme_fc_reconnect_or_delete(ctrl, ret);
+ 	else
+ 		dev_info(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: controller reconnect complete\n",
+ 			ctrl->cnum);
+ }
+ 
+ 
+ static const struct blk_mq_ops nvme_fc_admin_mq_ops = {
+ 	.queue_rq	= nvme_fc_queue_rq,
+ 	.complete	= nvme_fc_complete_rq,
+ 	.init_request	= nvme_fc_init_request,
+ 	.exit_request	= nvme_fc_exit_request,
+ 	.init_hctx	= nvme_fc_init_admin_hctx,
+ 	.timeout	= nvme_fc_timeout,
+ };
+ 
++>>>>>>> 158bfb8888c3 (nvme-fc: decouple ns references from lldd references)
  
  /*
   * Fails a controller request if it matches an existing controller
@@@ -2486,13 -3118,9 +3241,14 @@@ __nvme_fc_create_ctrl(struct device *de
  	ctrl->lport = lport;
  	ctrl->rport = rport;
  	ctrl->dev = lport->dev;
 +	ctrl->state = FCCTRL_INIT;
  	ctrl->cnum = idx;
+ 	ctrl->assoc_active = false;
  
 +	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
 +	if (ret)
 +		goto out_free_ida;
 +
  	get_device(ctrl->dev);
  	kref_init(&ctrl->ref);
  
* Unmerged path drivers/nvme/host/fc.c
