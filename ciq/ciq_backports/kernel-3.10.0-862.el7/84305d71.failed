RDMA/mlx5: Limit scope of get vector affinity local function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Leon Romanovsky <leon@kernel.org>
commit 84305d71d6ca9a2e6a33d9b403639f1cdeeee846
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/84305d71.failed

The mlx5_ib_get_vector_affinity() call is local to main.c file and there
is no need to be declared globally visible.

Fixes: 40b24403f33e ("mlx5: support ->get_vector_affinity")
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 84305d71d6ca9a2e6a33d9b403639f1cdeeee846)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index 278c0964f404,762ef6bf219e..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -3287,6 -3570,263 +3287,266 @@@ dealloc_counters
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static struct rdma_hw_stats *mlx5_ib_alloc_hw_stats(struct ib_device *ibdev,
+ 						    u8 port_num)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx5_ib_port *port = &dev->port[port_num - 1];
+ 
+ 	/* We support only per port stats */
+ 	if (port_num == 0)
+ 		return NULL;
+ 
+ 	return rdma_alloc_hw_stats_struct(port->cnts.names,
+ 					  port->cnts.num_q_counters +
+ 					  port->cnts.num_cong_counters,
+ 					  RDMA_HW_STATS_DEFAULT_LIFESPAN);
+ }
+ 
+ static int mlx5_ib_query_q_counters(struct mlx5_ib_dev *dev,
+ 				    struct mlx5_ib_port *port,
+ 				    struct rdma_hw_stats *stats)
+ {
+ 	int outlen = MLX5_ST_SZ_BYTES(query_q_counter_out);
+ 	void *out;
+ 	__be32 val;
+ 	int ret, i;
+ 
+ 	out = kvzalloc(outlen, GFP_KERNEL);
+ 	if (!out)
+ 		return -ENOMEM;
+ 
+ 	ret = mlx5_core_query_q_counter(dev->mdev,
+ 					port->cnts.set_id, 0,
+ 					out, outlen);
+ 	if (ret)
+ 		goto free;
+ 
+ 	for (i = 0; i < port->cnts.num_q_counters; i++) {
+ 		val = *(__be32 *)(out + port->cnts.offsets[i]);
+ 		stats->value[i] = (u64)be32_to_cpu(val);
+ 	}
+ 
+ free:
+ 	kvfree(out);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_query_cong_counters(struct mlx5_ib_dev *dev,
+ 				       struct mlx5_ib_port *port,
+ 				       struct rdma_hw_stats *stats)
+ {
+ 	int outlen = MLX5_ST_SZ_BYTES(query_cong_statistics_out);
+ 	void *out;
+ 	int ret, i;
+ 	int offset = port->cnts.num_q_counters;
+ 
+ 	out = kvzalloc(outlen, GFP_KERNEL);
+ 	if (!out)
+ 		return -ENOMEM;
+ 
+ 	ret = mlx5_cmd_query_cong_counter(dev->mdev, false, out, outlen);
+ 	if (ret)
+ 		goto free;
+ 
+ 	for (i = 0; i < port->cnts.num_cong_counters; i++) {
+ 		stats->value[i + offset] =
+ 			be64_to_cpup((__be64 *)(out +
+ 				     port->cnts.offsets[i + offset]));
+ 	}
+ 
+ free:
+ 	kvfree(out);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_get_hw_stats(struct ib_device *ibdev,
+ 				struct rdma_hw_stats *stats,
+ 				u8 port_num, int index)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx5_ib_port *port = &dev->port[port_num - 1];
+ 	int ret, num_counters;
+ 
+ 	if (!stats)
+ 		return -EINVAL;
+ 
+ 	ret = mlx5_ib_query_q_counters(dev, port, stats);
+ 	if (ret)
+ 		return ret;
+ 	num_counters = port->cnts.num_q_counters;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, cc_query_allowed)) {
+ 		ret = mlx5_ib_query_cong_counters(dev, port, stats);
+ 		if (ret)
+ 			return ret;
+ 		num_counters += port->cnts.num_cong_counters;
+ 	}
+ 
+ 	return num_counters;
+ }
+ 
+ static void mlx5_ib_free_rdma_netdev(struct net_device *netdev)
+ {
+ 	return mlx5_rdma_netdev_free(netdev);
+ }
+ 
+ static struct net_device*
+ mlx5_ib_alloc_rdma_netdev(struct ib_device *hca,
+ 			  u8 port_num,
+ 			  enum rdma_netdev_t type,
+ 			  const char *name,
+ 			  unsigned char name_assign_type,
+ 			  void (*setup)(struct net_device *))
+ {
+ 	struct net_device *netdev;
+ 	struct rdma_netdev *rn;
+ 
+ 	if (type != RDMA_NETDEV_IPOIB)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	netdev = mlx5_rdma_netdev_alloc(to_mdev(hca)->mdev, hca,
+ 					name, setup);
+ 	if (likely(!IS_ERR_OR_NULL(netdev))) {
+ 		rn = netdev_priv(netdev);
+ 		rn->free_rdma_netdev = mlx5_ib_free_rdma_netdev;
+ 	}
+ 	return netdev;
+ }
+ 
+ static void delay_drop_debugfs_cleanup(struct mlx5_ib_dev *dev)
+ {
+ 	if (!dev->delay_drop.dbg)
+ 		return;
+ 	debugfs_remove_recursive(dev->delay_drop.dbg->dir_debugfs);
+ 	kfree(dev->delay_drop.dbg);
+ 	dev->delay_drop.dbg = NULL;
+ }
+ 
+ static void cancel_delay_drop(struct mlx5_ib_dev *dev)
+ {
+ 	if (!(dev->ib_dev.attrs.raw_packet_caps & IB_RAW_PACKET_CAP_DELAY_DROP))
+ 		return;
+ 
+ 	cancel_work_sync(&dev->delay_drop.delay_drop_work);
+ 	delay_drop_debugfs_cleanup(dev);
+ }
+ 
+ static ssize_t delay_drop_timeout_read(struct file *filp, char __user *buf,
+ 				       size_t count, loff_t *pos)
+ {
+ 	struct mlx5_ib_delay_drop *delay_drop = filp->private_data;
+ 	char lbuf[20];
+ 	int len;
+ 
+ 	len = snprintf(lbuf, sizeof(lbuf), "%u\n", delay_drop->timeout);
+ 	return simple_read_from_buffer(buf, count, pos, lbuf, len);
+ }
+ 
+ static ssize_t delay_drop_timeout_write(struct file *filp, const char __user *buf,
+ 					size_t count, loff_t *pos)
+ {
+ 	struct mlx5_ib_delay_drop *delay_drop = filp->private_data;
+ 	u32 timeout;
+ 	u32 var;
+ 
+ 	if (kstrtouint_from_user(buf, count, 0, &var))
+ 		return -EFAULT;
+ 
+ 	timeout = min_t(u32, roundup(var, 100), MLX5_MAX_DELAY_DROP_TIMEOUT_MS *
+ 			1000);
+ 	if (timeout != var)
+ 		mlx5_ib_dbg(delay_drop->dev, "Round delay drop timeout to %u usec\n",
+ 			    timeout);
+ 
+ 	delay_drop->timeout = timeout;
+ 
+ 	return count;
+ }
+ 
+ static const struct file_operations fops_delay_drop_timeout = {
+ 	.owner	= THIS_MODULE,
+ 	.open	= simple_open,
+ 	.write	= delay_drop_timeout_write,
+ 	.read	= delay_drop_timeout_read,
+ };
+ 
+ static int delay_drop_debugfs_init(struct mlx5_ib_dev *dev)
+ {
+ 	struct mlx5_ib_dbg_delay_drop *dbg;
+ 
+ 	if (!mlx5_debugfs_root)
+ 		return 0;
+ 
+ 	dbg = kzalloc(sizeof(*dbg), GFP_KERNEL);
+ 	if (!dbg)
+ 		return -ENOMEM;
+ 
+ 	dbg->dir_debugfs =
+ 		debugfs_create_dir("delay_drop",
+ 				   dev->mdev->priv.dbg_root);
+ 	if (!dbg->dir_debugfs)
+ 		return -ENOMEM;
+ 
+ 	dbg->events_cnt_debugfs =
+ 		debugfs_create_atomic_t("num_timeout_events", 0400,
+ 					dbg->dir_debugfs,
+ 					&dev->delay_drop.events_cnt);
+ 	if (!dbg->events_cnt_debugfs)
+ 		goto out_debugfs;
+ 
+ 	dbg->rqs_cnt_debugfs =
+ 		debugfs_create_atomic_t("num_rqs", 0400,
+ 					dbg->dir_debugfs,
+ 					&dev->delay_drop.rqs_cnt);
+ 	if (!dbg->rqs_cnt_debugfs)
+ 		goto out_debugfs;
+ 
+ 	dbg->timeout_debugfs =
+ 		debugfs_create_file("timeout", 0600,
+ 				    dbg->dir_debugfs,
+ 				    &dev->delay_drop,
+ 				    &fops_delay_drop_timeout);
+ 	if (!dbg->timeout_debugfs)
+ 		goto out_debugfs;
+ 
+ 	dev->delay_drop.dbg = dbg;
+ 
+ 	return 0;
+ 
+ out_debugfs:
+ 	delay_drop_debugfs_cleanup(dev);
+ 	return -ENOMEM;
+ }
+ 
+ static void init_delay_drop(struct mlx5_ib_dev *dev)
+ {
+ 	if (!(dev->ib_dev.attrs.raw_packet_caps & IB_RAW_PACKET_CAP_DELAY_DROP))
+ 		return;
+ 
+ 	mutex_init(&dev->delay_drop.lock);
+ 	dev->delay_drop.dev = dev;
+ 	dev->delay_drop.activate = false;
+ 	dev->delay_drop.timeout = MLX5_MAX_DELAY_DROP_TIMEOUT_MS * 1000;
+ 	INIT_WORK(&dev->delay_drop.delay_drop_work, delay_drop_handler);
+ 	atomic_set(&dev->delay_drop.rqs_cnt, 0);
+ 	atomic_set(&dev->delay_drop.events_cnt, 0);
+ 
+ 	if (delay_drop_debugfs_init(dev))
+ 		mlx5_ib_warn(dev, "Failed to init delay drop debugfs\n");
+ }
+ 
+ static const struct cpumask *
+ mlx5_ib_get_vector_affinity(struct ib_device *ibdev, int comp_vector)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(ibdev);
+ 
+ 	return mlx5_get_vector_affinity(dev->mdev, comp_vector);
+ }
+ 
++>>>>>>> 84305d71d6ca (RDMA/mlx5: Limit scope of get vector affinity local function)
  static void *mlx5_ib_add(struct mlx5_core_dev *mdev)
  {
  	struct mlx5_ib_dev *dev;
* Unmerged path drivers/infiniband/hw/mlx5/main.c
