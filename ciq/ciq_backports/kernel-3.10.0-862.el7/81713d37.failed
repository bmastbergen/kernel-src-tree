IB/mlx5: Add implicit MR support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Artemy Kovalyov <artemyko@mellanox.com>
commit 81713d3788d2e6bc005f15ee1c59d0eb06050a6b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/81713d37.failed

Add implicit MR, covering entire user address space.
The MR is implemented as an indirect KSM MR consisting of
1GB direct MRs.
Pages and direct MRs are added/removed to MR by ODP.

	Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 81713d3788d2e6bc005f15ee1c59d0eb06050a6b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
#	drivers/infiniband/hw/mlx5/odp.c
#	include/linux/mlx5/driver.h
diff --cc drivers/infiniband/hw/mlx5/main.c
index 87c95640770e,eb8719ca500e..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -3410,21 -3583,10 +3410,27 @@@ static int __init mlx5_ib_init(void
  {
  	int err;
  
++<<<<<<< HEAD
 +	if (deprecated_prof_sel != 2)
 +		pr_warn("prof_sel is deprecated for mlx5_ib, set it for mlx5_core\n");
++=======
+ 	mlx5_ib_odp_init();
+ 
+ 	err = mlx5_register_interface(&mlx5_ib_interface);
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
 +
 +	err = mlx5_ib_odp_init();
 +	if (err)
 +		return err;
 +
 +	err = mlx5_register_interface(&mlx5_ib_interface);
 +	if (err)
 +		goto clean_odp;
 +
 +	return err;
  
 +clean_odp:
 +	mlx5_ib_odp_cleanup();
  	return err;
  }
  
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6922362fef46,3cd064b5f0bf..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -192,6 -196,14 +192,17 @@@ struct mlx5_ib_flow_db 
  #define MLX5_IB_UMR_OCTOWORD	       16
  #define MLX5_IB_UMR_XLT_ALIGNMENT      64
  
++<<<<<<< HEAD
++=======
+ #define MLX5_IB_UPD_XLT_ZAP	      BIT(0)
+ #define MLX5_IB_UPD_XLT_ENABLE	      BIT(1)
+ #define MLX5_IB_UPD_XLT_ATOMIC	      BIT(2)
+ #define MLX5_IB_UPD_XLT_ADDR	      BIT(3)
+ #define MLX5_IB_UPD_XLT_PD	      BIT(4)
+ #define MLX5_IB_UPD_XLT_ACCESS	      BIT(5)
+ #define MLX5_IB_UPD_XLT_INDIRECT      BIT(6)
+ 
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  /* Private QP creation flags to be passed in ib_qp_init_attr.create_flags.
   *
   * These flags are intended for internal use by the mlx5_ib driver, and they
@@@ -646,8 -642,9 +661,9 @@@ struct mlx5_ib_dev 
  	 * being used by a page fault handler.
  	 */
  	struct srcu_struct      mr_srcu;
+ 	u32			null_mkey;
  #endif
 -	struct mlx5_ib_flow_db	flow_db;
 +	struct mlx5_ib_flow_db  flow_db;
  	/* protect resources needed as part of reset flow */
  	spinlock_t		reset_flow_resource_lock;
  	struct list_head	qp_list;
@@@ -794,8 -793,11 +810,16 @@@ struct ib_mr *mlx5_ib_reg_user_mr(struc
  struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
  			       struct ib_udata *udata);
  int mlx5_ib_dealloc_mw(struct ib_mw *mw);
++<<<<<<< HEAD
 +int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
 +		       int npages, int zap);
++=======
+ int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
+ 		       int page_shift, int flags);
+ struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
+ 					     int access_flags);
+ void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *mr);
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
  			  u64 length, u64 virt_addr, int access_flags,
  			  struct ib_pd *pd, struct ib_udata *udata);
@@@ -874,23 -875,25 +898,34 @@@ int mlx5_ib_odp_init_one(struct mlx5_ib
  void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev);
  int __init mlx5_ib_odp_init(void);
  void mlx5_ib_odp_cleanup(void);
 +void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp);
 +void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp);
  void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
  			      unsigned long end);
+ void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
+ void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
+ 			   size_t nentries, struct mlx5_ib_mr *mr, int flags);
  #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
  static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
  {
  	return;
  }
  
 +static inline void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp)		{}
  static inline int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev) { return 0; }
- static inline void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)	{}
+ static inline void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)	    {}
  static inline int mlx5_ib_odp_init(void) { return 0; }
++<<<<<<< HEAD
 +static inline void mlx5_ib_odp_cleanup(void)				{}
 +static inline void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp) {}
 +static inline void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp)  {}
++=======
+ static inline void mlx5_ib_odp_cleanup(void)				    {}
+ static inline void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent) {}
+ static inline void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
+ 					 size_t nentries, struct mlx5_ib_mr *mr,
+ 					 int flags) {}
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  
  #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
  
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 47de9e74f883,3c1f483d003f..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -452,6 -450,42 +452,45 @@@ static void cache_work_func(struct work
  	__cache_work_func(ent);
  }
  
++<<<<<<< HEAD
++=======
+ struct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev, int entry)
+ {
+ 	struct mlx5_mr_cache *cache = &dev->cache;
+ 	struct mlx5_cache_ent *ent;
+ 	struct mlx5_ib_mr *mr;
+ 	int err;
+ 
+ 	if (entry < 0 || entry >= MAX_MR_CACHE_ENTRIES) {
+ 		mlx5_ib_err(dev, "cache entry %d is out of range\n", entry);
+ 		return NULL;
+ 	}
+ 
+ 	ent = &cache->ent[entry];
+ 	while (1) {
+ 		spin_lock_irq(&ent->lock);
+ 		if (list_empty(&ent->head)) {
+ 			spin_unlock_irq(&ent->lock);
+ 
+ 			err = add_keys(dev, entry, 1);
+ 			if (err && err != -EAGAIN)
+ 				return ERR_PTR(err);
+ 
+ 			wait_for_completion(&ent->compl);
+ 		} else {
+ 			mr = list_first_entry(&ent->head, struct mlx5_ib_mr,
+ 					      list);
+ 			list_del(&mr->list);
+ 			ent->cur--;
+ 			spin_unlock_irq(&ent->lock);
+ 			if (ent->cur < ent->limit)
+ 				queue_work(cache->wq, &ent->work);
+ 			return mr;
+ 		}
+ 	}
+ }
+ 
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  static struct mlx5_ib_mr *alloc_cached_mr(struct mlx5_ib_dev *dev, int order)
  {
  	struct mlx5_mr_cache *cache = &dev->cache;
@@@ -628,17 -662,30 +667,38 @@@ int mlx5_mr_cache_init(struct mlx5_ib_d
  		spin_lock_init(&ent->lock);
  		ent->order = i + 2;
  		ent->dev = dev;
 -		ent->limit = 0;
  
 -		init_completion(&ent->compl);
 +		if ((dev->mdev->profile->mask & MLX5_PROF_MASK_MR_CACHE) &&
 +		    (mlx5_core_is_pf(dev->mdev)))
 +			limit = dev->mdev->profile->mr_cache[i].limit;
 +		else
 +			limit = 0;
 +
  		INIT_WORK(&ent->work, cache_work_func);
  		INIT_DELAYED_WORK(&ent->dwork, delayed_cache_work_func);
 +		ent->limit = limit;
  		queue_work(cache->wq, &ent->work);
++<<<<<<< HEAD
++=======
+ 
+ 		if (i > MAX_UMR_CACHE_ENTRY) {
+ 			mlx5_odp_init_mr_cache_entry(ent);
+ 			continue;
+ 		}
+ 
+ 		if (!use_umr(dev, ent->order))
+ 			continue;
+ 
+ 		ent->page = PAGE_SHIFT;
+ 		ent->xlt = (1 << ent->order) * sizeof(struct mlx5_mtt) /
+ 			   MLX5_IB_UMR_OCTOWORD;
+ 		ent->access_mode = MLX5_MKC_ACCESS_MODE_MTT;
+ 		if ((dev->mdev->profile->mask & MLX5_PROF_MASK_MR_CACHE) &&
+ 		    mlx5_core_is_pf(dev->mdev))
+ 			ent->limit = dev->mdev->profile->mr_cache[i].limit;
+ 		else
+ 			ent->limit = 0;
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  	}
  
  	err = mlx5_mr_cache_debugfs_init(dev);
@@@ -975,25 -929,56 +1035,67 @@@ free_mr
  	return mr;
  }
  
 -static inline int populate_xlt(struct mlx5_ib_mr *mr, int idx, int npages,
 -			       void *xlt, int page_shift, size_t size,
 -			       int flags)
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index, int npages,
 +		       int zap)
  {
  	struct mlx5_ib_dev *dev = mr->dev;
 +	struct device *ddev = dev->ib_dev.dev.parent;
 +	struct umr_common *umrc = &dev->umrc;
 +	struct mlx5_ib_umr_context umr_context;
  	struct ib_umem *umem = mr->umem;
++<<<<<<< HEAD
++=======
+ 	if (flags & MLX5_IB_UPD_XLT_INDIRECT) {
+ 		mlx5_odp_populate_klm(xlt, idx, npages, mr, flags);
+ 		return npages;
+ 	}
+ 
+ 	npages = min_t(size_t, npages, ib_umem_num_pages(umem) - idx);
+ 
+ 	if (!(flags & MLX5_IB_UPD_XLT_ZAP)) {
+ 		__mlx5_ib_populate_pas(dev, umem, page_shift,
+ 				       idx, npages, xlt,
+ 				       MLX5_IB_MTT_PRESENT);
+ 		/* Clear padding after the pages
+ 		 * brought from the umem.
+ 		 */
+ 		memset(xlt + (npages * sizeof(struct mlx5_mtt)), 0,
+ 		       size - npages * sizeof(struct mlx5_mtt));
+ 	}
+ 
+ 	return npages;
+ }
+ 
+ #define MLX5_MAX_UMR_CHUNK ((1 << (MLX5_MAX_UMR_SHIFT + 4)) - \
+ 			    MLX5_UMR_MTT_ALIGNMENT)
+ #define MLX5_SPARE_UMR_CHUNK 0x10000
+ 
+ int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
+ 		       int page_shift, int flags)
+ {
+ 	struct mlx5_ib_dev *dev = mr->dev;
+ 	struct device *ddev = dev->ib_dev.dma_device;
+ 	struct mlx5_ib_ucontext *uctx = NULL;
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  	int size;
 -	void *xlt;
 +	__be64 *pas;
  	dma_addr_t dma;
 +	struct ib_send_wr *bad;
  	struct mlx5_umr_wr wr;
  	struct ib_sge sg;
  	int err = 0;
++<<<<<<< HEAD
 +	const int page_index_alignment = MLX5_UMR_MTT_ALIGNMENT /
 +					 sizeof(struct mlx5_mtt);
 +	const int page_index_mask = page_index_alignment - 1;
++=======
+ 	int desc_size = (flags & MLX5_IB_UPD_XLT_INDIRECT)
+ 			       ? sizeof(struct mlx5_klm)
+ 			       : sizeof(struct mlx5_mtt);
+ 	const int page_align = MLX5_UMR_MTT_ALIGNMENT / desc_size;
+ 	const int page_mask = page_align - 1;
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  	size_t pages_mapped = 0;
  	size_t pages_to_map = 0;
  	size_t pages_iter = 0;
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 25e3fb5efdf9,d7b12f0750e2..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -41,7 -42,139 +42,143 @@@
   * a pagefault. */
  #define MMU_NOTIFIER_TIMEOUT 1000
  
++<<<<<<< HEAD
 +struct workqueue_struct *mlx5_ib_page_fault_wq;
++=======
+ #define MLX5_IMR_MTT_BITS (30 - PAGE_SHIFT)
+ #define MLX5_IMR_MTT_SHIFT (MLX5_IMR_MTT_BITS + PAGE_SHIFT)
+ #define MLX5_IMR_MTT_ENTRIES BIT_ULL(MLX5_IMR_MTT_BITS)
+ #define MLX5_IMR_MTT_SIZE BIT_ULL(MLX5_IMR_MTT_SHIFT)
+ #define MLX5_IMR_MTT_MASK (~(MLX5_IMR_MTT_SIZE - 1))
+ 
+ #define MLX5_KSM_PAGE_SHIFT MLX5_IMR_MTT_SHIFT
+ 
+ static u64 mlx5_imr_ksm_entries;
+ 
+ static int check_parent(struct ib_umem_odp *odp,
+ 			       struct mlx5_ib_mr *parent)
+ {
+ 	struct mlx5_ib_mr *mr = odp->private;
+ 
+ 	return mr && mr->parent == parent;
+ }
+ 
+ static struct ib_umem_odp *odp_next(struct ib_umem_odp *odp)
+ {
+ 	struct mlx5_ib_mr *mr = odp->private, *parent = mr->parent;
+ 	struct ib_ucontext *ctx = odp->umem->context;
+ 	struct rb_node *rb;
+ 
+ 	down_read(&ctx->umem_rwsem);
+ 	while (1) {
+ 		rb = rb_next(&odp->interval_tree.rb);
+ 		if (!rb)
+ 			goto not_found;
+ 		odp = rb_entry(rb, struct ib_umem_odp, interval_tree.rb);
+ 		if (check_parent(odp, parent))
+ 			goto end;
+ 	}
+ not_found:
+ 	odp = NULL;
+ end:
+ 	up_read(&ctx->umem_rwsem);
+ 	return odp;
+ }
+ 
+ static struct ib_umem_odp *odp_lookup(struct ib_ucontext *ctx,
+ 				      u64 start, u64 length,
+ 				      struct mlx5_ib_mr *parent)
+ {
+ 	struct ib_umem_odp *odp;
+ 	struct rb_node *rb;
+ 
+ 	down_read(&ctx->umem_rwsem);
+ 	odp = rbt_ib_umem_lookup(&ctx->umem_tree, start, length);
+ 	if (!odp)
+ 		goto end;
+ 
+ 	while (1) {
+ 		if (check_parent(odp, parent))
+ 			goto end;
+ 		rb = rb_next(&odp->interval_tree.rb);
+ 		if (!rb)
+ 			goto not_found;
+ 		odp = rb_entry(rb, struct ib_umem_odp, interval_tree.rb);
+ 		if (ib_umem_start(odp->umem) > start + length)
+ 			goto not_found;
+ 	}
+ not_found:
+ 	odp = NULL;
+ end:
+ 	up_read(&ctx->umem_rwsem);
+ 	return odp;
+ }
+ 
+ void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
+ 			   size_t nentries, struct mlx5_ib_mr *mr, int flags)
+ {
+ 	struct ib_pd *pd = mr->ibmr.pd;
+ 	struct ib_ucontext *ctx = pd->uobject->context;
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct ib_umem_odp *odp;
+ 	unsigned long va;
+ 	int i;
+ 
+ 	if (flags & MLX5_IB_UPD_XLT_ZAP) {
+ 		for (i = 0; i < nentries; i++, pklm++) {
+ 			pklm->bcount = cpu_to_be32(MLX5_IMR_MTT_SIZE);
+ 			pklm->key = cpu_to_be32(dev->null_mkey);
+ 			pklm->va = 0;
+ 		}
+ 		return;
+ 	}
+ 
+ 	odp = odp_lookup(ctx, offset * MLX5_IMR_MTT_SIZE,
+ 			     nentries * MLX5_IMR_MTT_SIZE, mr);
+ 
+ 	for (i = 0; i < nentries; i++, pklm++) {
+ 		pklm->bcount = cpu_to_be32(MLX5_IMR_MTT_SIZE);
+ 		va = (offset + i) * MLX5_IMR_MTT_SIZE;
+ 		if (odp && odp->umem->address == va) {
+ 			struct mlx5_ib_mr *mtt = odp->private;
+ 
+ 			pklm->key = cpu_to_be32(mtt->ibmr.lkey);
+ 			odp = odp_next(odp);
+ 		} else {
+ 			pklm->key = cpu_to_be32(dev->null_mkey);
+ 		}
+ 		mlx5_ib_dbg(dev, "[%d] va %lx key %x\n",
+ 			    i, va, be32_to_cpu(pklm->key));
+ 	}
+ }
+ 
+ static void mr_leaf_free_action(struct work_struct *work)
+ {
+ 	struct ib_umem_odp *odp = container_of(work, struct ib_umem_odp, work);
+ 	int idx = ib_umem_start(odp->umem) >> MLX5_IMR_MTT_SHIFT;
+ 	struct mlx5_ib_mr *mr = odp->private, *imr = mr->parent;
+ 
+ 	mr->parent = NULL;
+ 	synchronize_srcu(&mr->dev->mr_srcu);
+ 
+ 	if (!READ_ONCE(odp->dying)) {
+ 		mr->parent = imr;
+ 		if (atomic_dec_and_test(&imr->num_leaf_free))
+ 			wake_up(&imr->q_leaf_free);
+ 		return;
+ 	}
+ 
+ 	ib_umem_release(odp->umem);
+ 	if (imr->live)
+ 		mlx5_ib_update_xlt(imr, idx, 1, 0,
+ 				   MLX5_IB_UPD_XLT_INDIRECT |
+ 				   MLX5_IB_UPD_XLT_ATOMIC);
+ 	mlx5_mr_cache_free(mr->dev, mr);
+ 
+ 	if (atomic_dec_and_test(&imr->num_leaf_free))
+ 		wake_up(&imr->q_leaf_free);
+ }
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  
  void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
  			      unsigned long end)
@@@ -141,6 -286,14 +285,17 @@@ void mlx5_ib_internal_fill_odp_caps(str
  	if (MLX5_CAP_ODP(dev->mdev, rc_odp_caps.read))
  		caps->per_transport_caps.rc_odp_caps |= IB_ODP_SUPPORT_READ;
  
++<<<<<<< HEAD
++=======
+ 	if (MLX5_CAP_ODP(dev->mdev, rc_odp_caps.atomic))
+ 		caps->per_transport_caps.rc_odp_caps |= IB_ODP_SUPPORT_ATOMIC;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, fixed_buffer_size) &&
+ 	    MLX5_CAP_GEN(dev->mdev, null_mkey) &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset))
+ 		caps->general_caps |= IB_ODP_SUPPORT_IMPLICIT;
+ 
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  	return;
  }
  
@@@ -162,40 -315,231 +317,235 @@@ static struct mlx5_ib_mr *mlx5_ib_odp_f
  	return container_of(mmkey, struct mlx5_ib_mr, mmkey);
  }
  
 -static void mlx5_ib_page_fault_resume(struct mlx5_ib_dev *dev,
 -				      struct mlx5_pagefault *pfault,
 +static void mlx5_ib_page_fault_resume(struct mlx5_ib_qp *qp,
 +				      struct mlx5_ib_pfault *pfault,
  				      int error)
  {
 -	int wq_num = pfault->event_subtype == MLX5_PFAULT_SUBTYPE_WQE ?
 -		     pfault->wqe.wq_num : pfault->token;
 +	struct mlx5_ib_dev *dev = to_mdev(qp->ibqp.pd->device);
 +	u32 qpn = qp->trans_qp.base.mqp.qpn;
  	int ret = mlx5_core_page_fault_resume(dev->mdev,
 -					      pfault->token,
 -					      wq_num,
 -					      pfault->type,
 +					      qpn,
 +					      pfault->mpfault.flags,
  					      error);
  	if (ret)
 -		mlx5_ib_err(dev, "Failed to resolve the page fault on WQ 0x%x\n",
 -			    wq_num);
 +		pr_err("Failed to resolve the page fault on QP 0x%x\n", qpn);
  }
  
+ static struct mlx5_ib_mr *implicit_mr_alloc(struct ib_pd *pd,
+ 					    struct ib_umem *umem,
+ 					    bool ksm, int access_flags)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct mlx5_ib_mr *mr;
+ 	int err;
+ 
+ 	mr = mlx5_mr_cache_alloc(dev, ksm ? MLX5_IMR_KSM_CACHE_ENTRY :
+ 					    MLX5_IMR_MTT_CACHE_ENTRY);
+ 
+ 	if (IS_ERR(mr))
+ 		return mr;
+ 
+ 	mr->ibmr.pd = pd;
+ 
+ 	mr->dev = dev;
+ 	mr->access_flags = access_flags;
+ 	mr->mmkey.iova = 0;
+ 	mr->umem = umem;
+ 
+ 	if (ksm) {
+ 		err = mlx5_ib_update_xlt(mr, 0,
+ 					 mlx5_imr_ksm_entries,
+ 					 MLX5_KSM_PAGE_SHIFT,
+ 					 MLX5_IB_UPD_XLT_INDIRECT |
+ 					 MLX5_IB_UPD_XLT_ZAP |
+ 					 MLX5_IB_UPD_XLT_ENABLE);
+ 
+ 	} else {
+ 		err = mlx5_ib_update_xlt(mr, 0,
+ 					 MLX5_IMR_MTT_ENTRIES,
+ 					 PAGE_SHIFT,
+ 					 MLX5_IB_UPD_XLT_ZAP |
+ 					 MLX5_IB_UPD_XLT_ENABLE |
+ 					 MLX5_IB_UPD_XLT_ATOMIC);
+ 	}
+ 
+ 	if (err)
+ 		goto fail;
+ 
+ 	mr->ibmr.lkey = mr->mmkey.key;
+ 	mr->ibmr.rkey = mr->mmkey.key;
+ 
+ 	mr->live = 1;
+ 
+ 	mlx5_ib_dbg(dev, "key %x dev %p mr %p\n",
+ 		    mr->mmkey.key, dev->mdev, mr);
+ 
+ 	return mr;
+ 
+ fail:
+ 	mlx5_ib_err(dev, "Failed to register MKEY %d\n", err);
+ 	mlx5_mr_cache_free(dev, mr);
+ 
+ 	return ERR_PTR(err);
+ }
+ 
+ static struct ib_umem_odp *implicit_mr_get_data(struct mlx5_ib_mr *mr,
+ 						u64 io_virt, size_t bcnt)
+ {
+ 	struct ib_ucontext *ctx = mr->ibmr.pd->uobject->context;
+ 	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.pd->device);
+ 	struct ib_umem_odp *odp, *result = NULL;
+ 	u64 addr = io_virt & MLX5_IMR_MTT_MASK;
+ 	int nentries = 0, start_idx = 0, ret;
+ 	struct mlx5_ib_mr *mtt;
+ 	struct ib_umem *umem;
+ 
+ 	mutex_lock(&mr->umem->odp_data->umem_mutex);
+ 	odp = odp_lookup(ctx, addr, 1, mr);
+ 
+ 	mlx5_ib_dbg(dev, "io_virt:%llx bcnt:%zx addr:%llx odp:%p\n",
+ 		    io_virt, bcnt, addr, odp);
+ 
+ next_mr:
+ 	if (likely(odp)) {
+ 		if (nentries)
+ 			nentries++;
+ 	} else {
+ 		umem = ib_alloc_odp_umem(ctx, addr, MLX5_IMR_MTT_SIZE);
+ 		if (IS_ERR(umem)) {
+ 			mutex_unlock(&mr->umem->odp_data->umem_mutex);
+ 			return ERR_CAST(umem);
+ 		}
+ 
+ 		mtt = implicit_mr_alloc(mr->ibmr.pd, umem, 0, mr->access_flags);
+ 		if (IS_ERR(mtt)) {
+ 			mutex_unlock(&mr->umem->odp_data->umem_mutex);
+ 			ib_umem_release(umem);
+ 			return ERR_CAST(mtt);
+ 		}
+ 
+ 		odp = umem->odp_data;
+ 		odp->private = mtt;
+ 		mtt->umem = umem;
+ 		mtt->mmkey.iova = addr;
+ 		mtt->parent = mr;
+ 		INIT_WORK(&odp->work, mr_leaf_free_action);
+ 
+ 		if (!nentries)
+ 			start_idx = addr >> MLX5_IMR_MTT_SHIFT;
+ 		nentries++;
+ 	}
+ 
+ 	odp->dying = 0;
+ 
+ 	/* Return first odp if region not covered by single one */
+ 	if (likely(!result))
+ 		result = odp;
+ 
+ 	addr += MLX5_IMR_MTT_SIZE;
+ 	if (unlikely(addr < io_virt + bcnt)) {
+ 		odp = odp_next(odp);
+ 		if (odp && odp->umem->address != addr)
+ 			odp = NULL;
+ 		goto next_mr;
+ 	}
+ 
+ 	if (unlikely(nentries)) {
+ 		ret = mlx5_ib_update_xlt(mr, start_idx, nentries, 0,
+ 					 MLX5_IB_UPD_XLT_INDIRECT |
+ 					 MLX5_IB_UPD_XLT_ATOMIC);
+ 		if (ret) {
+ 			mlx5_ib_err(dev, "Failed to update PAS\n");
+ 			result = ERR_PTR(ret);
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&mr->umem->odp_data->umem_mutex);
+ 	return result;
+ }
+ 
+ struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
+ 					     int access_flags)
+ {
+ 	struct ib_ucontext *ctx = pd->ibpd.uobject->context;
+ 	struct mlx5_ib_mr *imr;
+ 	struct ib_umem *umem;
+ 
+ 	umem = ib_umem_get(ctx, 0, 0, IB_ACCESS_ON_DEMAND, 0);
+ 	if (IS_ERR(umem))
+ 		return ERR_CAST(umem);
+ 
+ 	imr = implicit_mr_alloc(&pd->ibpd, umem, 1, access_flags);
+ 	if (IS_ERR(imr)) {
+ 		ib_umem_release(umem);
+ 		return ERR_CAST(imr);
+ 	}
+ 
+ 	imr->umem = umem;
+ 	init_waitqueue_head(&imr->q_leaf_free);
+ 	atomic_set(&imr->num_leaf_free, 0);
+ 
+ 	return imr;
+ }
+ 
+ static int mr_leaf_free(struct ib_umem *umem, u64 start,
+ 			u64 end, void *cookie)
+ {
+ 	struct mlx5_ib_mr *mr = umem->odp_data->private, *imr = cookie;
+ 
+ 	if (mr->parent != imr)
+ 		return 0;
+ 
+ 	ib_umem_odp_unmap_dma_pages(umem,
+ 				    ib_umem_start(umem),
+ 				    ib_umem_end(umem));
+ 
+ 	if (umem->odp_data->dying)
+ 		return 0;
+ 
+ 	WRITE_ONCE(umem->odp_data->dying, 1);
+ 	atomic_inc(&imr->num_leaf_free);
+ 	schedule_work(&umem->odp_data->work);
+ 
+ 	return 0;
+ }
+ 
+ void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)
+ {
+ 	struct ib_ucontext *ctx = imr->ibmr.pd->uobject->context;
+ 
+ 	down_read(&ctx->umem_rwsem);
+ 	rbt_ib_umem_for_each_in_range(&ctx->umem_tree, 0, ULLONG_MAX,
+ 				      mr_leaf_free, imr);
+ 	up_read(&ctx->umem_rwsem);
+ 
+ 	wait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));
+ }
+ 
  /*
 - * Handle a single data segment in a page-fault WQE or RDMA region.
 + * Handle a single data segment in a page-fault WQE.
   *
 - * Returns number of pages retrieved on success. The caller may continue to
 + * Returns number of pages retrieved on success. The caller will continue to
   * the next data segment.
   * Can return the following error codes:
   * -EAGAIN to designate a temporary error. The caller will abort handling the
   *  page fault and resolve it.
   * -EFAULT when there's an error mapping the requested pages. The caller will
 - *  abort the page fault handling.
 + *  abort the page fault handling and possibly move the QP to an error state.
 + * On other errors the QP should also be closed with an error.
   */
++<<<<<<< HEAD
 +static int pagefault_single_data_segment(struct mlx5_ib_qp *qp,
 +					 struct mlx5_ib_pfault *pfault,
++=======
+ static int pagefault_single_data_segment(struct mlx5_ib_dev *dev,
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  					 u32 key, u64 io_virt, size_t bcnt,
 -					 u32 *bytes_committed,
  					 u32 *bytes_mapped)
  {
 +	struct mlx5_ib_dev *mib_dev = to_mdev(qp->ibqp.pd->device);
  	int srcu_key;
- 	unsigned int current_seq;
+ 	unsigned int current_seq = 0;
  	u64 start_idx;
  	int npages = 0, ret = 0;
  	struct mlx5_ib_mr *mr;
@@@ -215,16 -562,11 +568,16 @@@
  		goto srcu_unlock;
  	}
  	if (!mr->umem->odp_data) {
- 		pr_debug("skipping non ODP MR (lkey=0x%06x) in page fault handler.\n",
- 			 key);
+ 		mlx5_ib_dbg(dev, "skipping non ODP MR (lkey=0x%06x) in page fault handler.\n",
+ 			    key);
  		if (bytes_mapped)
  			*bytes_mapped +=
 -				(bcnt - *bytes_committed);
 +				(bcnt - pfault->mpfault.bytes_committed);
 +		goto srcu_unlock;
 +	}
 +	if (mr->ibmr.pd != qp->ibqp.pd) {
 +		pr_err("Page-fault with different PDs for QP and MR.\n");
 +		ret = -EFAULT;
  		goto srcu_unlock;
  	}
  
@@@ -240,9 -575,32 +586,32 @@@
  	 * in all iterations (in iteration 2 and above,
  	 * bytes_committed == 0).
  	 */
 -	io_virt += *bytes_committed;
 -	bcnt -= *bytes_committed;
 +	io_virt += pfault->mpfault.bytes_committed;
 +	bcnt -= pfault->mpfault.bytes_committed;
  
+ 	if (!mr->umem->odp_data->page_list) {
+ 		odp = implicit_mr_get_data(mr, io_virt, bcnt);
+ 
+ 		if (IS_ERR(odp)) {
+ 			ret = PTR_ERR(odp);
+ 			goto srcu_unlock;
+ 		}
+ 		mr = odp->private;
+ 		implicit = 1;
+ 
+ 	} else {
+ 		odp = mr->umem->odp_data;
+ 	}
+ 
+ next_mr:
+ 	current_seq = READ_ONCE(odp->notifiers_seq);
+ 	/*
+ 	 * Ensure the sequence number is valid for some time before we call
+ 	 * gup.
+ 	 */
+ 	smp_rmb();
+ 
+ 	size = min_t(size_t, bcnt, ib_umem_end(odp->umem) - io_virt);
  	start_idx = (io_virt - (mr->mmkey.iova & PAGE_MASK)) >> PAGE_SHIFT;
  
  	if (mr->umem->writable)
@@@ -262,7 -622,9 +633,13 @@@
  			 * this MR, since ib_umem_odp_map_dma_pages already
  			 * checks this.
  			 */
++<<<<<<< HEAD
 +			ret = mlx5_ib_update_mtt(mr, start_idx, npages, 0);
++=======
+ 			ret = mlx5_ib_update_xlt(mr, start_idx, np,
+ 						 PAGE_SHIFT,
+ 						 MLX5_IB_UPD_XLT_ATOMIC);
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  		} else {
  			ret = -EAGAIN;
  		}
@@@ -297,8 -678,10 +693,15 @@@ srcu_unlock
  			ret = -EFAULT;
  		}
  	}
++<<<<<<< HEAD
 +	srcu_read_unlock(&mib_dev->mr_srcu, srcu_key);
 +	pfault->mpfault.bytes_committed = 0;
++=======
+ 
+ srcu_unlock_no_wait:
+ 	srcu_read_unlock(&dev->mr_srcu, srcu_key);
+ 	*bytes_committed = 0;
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  	return ret ? ret : npages;
  }
  
@@@ -587,23 -990,25 +990,36 @@@ static void mlx5_ib_mr_wqe_pfault_handl
  		goto resolve_page_fault;
  	}
  
 -	ret = pagefault_data_segments(dev, pfault, qp, wqe, wqe_end,
 -				      &bytes_mapped, &total_wqe_bytes,
 -				      !requestor);
 +	ret = pagefault_data_segments(qp, pfault, wqe, wqe_end, &bytes_mapped,
 +				      &total_wqe_bytes, !requestor);
  	if (ret == -EAGAIN) {
 -		resume_with_error = 0;
  		goto resolve_page_fault;
  	} else if (ret < 0 || total_wqe_bytes > bytes_mapped) {
++<<<<<<< HEAD
 +		mlx5_ib_err(dev, "Error getting user pages for page fault. Error: 0x%x\n",
 +			    -ret);
 +		resume_with_error = 1;
++=======
+ 		if (ret != -ENOENT)
+ 			mlx5_ib_err(dev, "PAGE FAULT error: %d. QP 0x%x. type: 0x%x\n",
+ 				    ret, pfault->wqe.wq_num, pfault->type);
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  		goto resolve_page_fault;
  	}
  
 -	resume_with_error = 0;
  resolve_page_fault:
++<<<<<<< HEAD
 +	mlx5_ib_page_fault_resume(qp, pfault, resume_with_error);
 +	mlx5_ib_dbg(dev, "PAGE FAULT completed. QP 0x%x resume_with_error=%d, flags: 0x%x\n",
 +		    qpn, resume_with_error,
 +		    pfault->mpfault.flags);
 +
++=======
+ 	mlx5_ib_page_fault_resume(dev, pfault, resume_with_error);
+ 	mlx5_ib_dbg(dev, "PAGE FAULT completed. QP 0x%x resume_with_error=%d, type: 0x%x\n",
+ 		    pfault->wqe.wq_num, resume_with_error,
+ 		    pfault->type);
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  	free_page((unsigned long)buffer);
  }
  
@@@ -669,14 -1075,14 +1085,21 @@@ static void mlx5_ib_mr_rdma_pfault_hand
  	 * work-queue is being fenced. */
  
  	if (prefetch_activated) {
 -		u32 bytes_committed = 0;
 -
 -		ret = pagefault_single_data_segment(dev, rkey, address,
 +		ret = pagefault_single_data_segment(qp, &dummy_pfault, rkey,
 +						    address,
  						    prefetch_len,
++<<<<<<< HEAD
 +						    NULL);
 +		if (ret < 0) {
 +			pr_warn("Prefetch failed (ret = %d, prefetch_activated = %d) for QPN %d, address: 0x%.16llx, length = 0x%.16x\n",
 +				ret, prefetch_activated,
 +				qp->ibqp.qp_num, address, prefetch_len);
++=======
+ 						    &bytes_committed, NULL);
+ 		if (ret < 0 && ret != -EAGAIN) {
+ 			mlx5_ib_warn(dev, "Prefetch failed. ret: %d, QP 0x%x, address: 0x%.16llx, length = 0x%.16x\n",
+ 				     ret, pfault->token, address, prefetch_len);
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  		}
  	}
  }
@@@ -701,81 -1107,33 +1124,111 @@@ void mlx5_ib_mr_pfault_handler(struct m
  	}
  }
  
++<<<<<<< HEAD
 +static void mlx5_ib_qp_pfault_action(struct work_struct *work)
 +{
 +	struct mlx5_ib_pfault *pfault = container_of(work,
 +						     struct mlx5_ib_pfault,
 +						     work);
 +	enum mlx5_ib_pagefault_context context =
 +		mlx5_ib_get_pagefault_context(&pfault->mpfault);
 +	struct mlx5_ib_qp *qp = container_of(pfault, struct mlx5_ib_qp,
 +					     pagefaults[context]);
 +	mlx5_ib_mr_pfault_handler(qp, pfault);
 +}
 +
 +void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp)
 +{
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&qp->disable_page_faults_lock, flags);
 +	qp->disable_page_faults = 1;
 +	spin_unlock_irqrestore(&qp->disable_page_faults_lock, flags);
 +
 +	/*
 +	 * Note that at this point, we are guarenteed that no more
 +	 * work queue elements will be posted to the work queue with
 +	 * the QP we are closing.
 +	 */
 +	flush_workqueue(mlx5_ib_page_fault_wq);
 +}
 +
 +void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp)
 +{
 +	unsigned long flags;
 +
 +	spin_lock_irqsave(&qp->disable_page_faults_lock, flags);
 +	qp->disable_page_faults = 0;
 +	spin_unlock_irqrestore(&qp->disable_page_faults_lock, flags);
 +}
 +
 +static void mlx5_ib_pfault_handler(struct mlx5_core_qp *qp,
 +				   struct mlx5_pagefault *pfault)
 +{
 +	/*
 +	 * Note that we will only get one fault event per QP per context
 +	 * (responder/initiator, read/write), until we resolve the page fault
 +	 * with the mlx5_ib_page_fault_resume command. Since this function is
 +	 * called from within the work element, there is no risk of missing
 +	 * events.
 +	 */
 +	struct mlx5_ib_qp *mibqp = to_mibqp(qp);
 +	enum mlx5_ib_pagefault_context context =
 +		mlx5_ib_get_pagefault_context(pfault);
 +	struct mlx5_ib_pfault *qp_pfault = &mibqp->pagefaults[context];
 +
 +	qp_pfault->mpfault = *pfault;
 +
 +	/* No need to stop interrupts here since we are in an interrupt */
 +	spin_lock(&mibqp->disable_page_faults_lock);
 +	if (!mibqp->disable_page_faults)
 +		queue_work(mlx5_ib_page_fault_wq, &qp_pfault->work);
 +	spin_unlock(&mibqp->disable_page_faults_lock);
 +}
 +
 +void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp)
 +{
 +	int i;
 +
 +	qp->disable_page_faults = 1;
 +	spin_lock_init(&qp->disable_page_faults_lock);
 +
 +	qp->trans_qp.base.mqp.pfault_handler = mlx5_ib_pfault_handler;
 +
 +	for (i = 0; i < MLX5_IB_PAGEFAULT_CONTEXTS; ++i)
 +		INIT_WORK(&qp->pagefaults[i].work, mlx5_ib_qp_pfault_action);
 +}
 +
 +int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev)
++=======
+ void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent)
+ {
+ 	if (!(ent->dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
+ 		return;
+ 
+ 	switch (ent->order - 2) {
+ 	case MLX5_IMR_MTT_CACHE_ENTRY:
+ 		ent->page = PAGE_SHIFT;
+ 		ent->xlt = MLX5_IMR_MTT_ENTRIES *
+ 			   sizeof(struct mlx5_mtt) /
+ 			   MLX5_IB_UMR_OCTOWORD;
+ 		ent->access_mode = MLX5_MKC_ACCESS_MODE_MTT;
+ 		ent->limit = 0;
+ 		break;
+ 
+ 	case MLX5_IMR_KSM_CACHE_ENTRY:
+ 		ent->page = MLX5_KSM_PAGE_SHIFT;
+ 		ent->xlt = mlx5_imr_ksm_entries *
+ 			   sizeof(struct mlx5_klm) /
+ 			   MLX5_IB_UMR_OCTOWORD;
+ 		ent->access_mode = MLX5_MKC_ACCESS_MODE_KSM;
+ 		ent->limit = 0;
+ 		break;
+ 	}
+ }
+ 
+ int mlx5_ib_odp_init_one(struct mlx5_ib_dev *dev)
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  {
  	int ret;
  
@@@ -786,22 -1152,16 +1247,30 @@@
  	return 0;
  }
  
- void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)
+ void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *dev)
+ {
+ 	cleanup_srcu_struct(&dev->mr_srcu);
+ }
+ 
+ int mlx5_ib_odp_init(void)
  {
- 	cleanup_srcu_struct(&ibdev->mr_srcu);
+ 	mlx5_imr_ksm_entries = BIT_ULL(get_order(TASK_SIZE) -
+ 				       MLX5_IMR_MTT_BITS);
+ 
+ 	return 0;
  }
  
 +int __init mlx5_ib_odp_init(void)
 +{
 +	mlx5_ib_page_fault_wq = alloc_ordered_workqueue("mlx5_ib_page_faults",
 +							WQ_MEM_RECLAIM);
 +	if (!mlx5_ib_page_fault_wq)
 +		return -ENOMEM;
 +
 +	return 0;
 +}
 +
 +void mlx5_ib_odp_cleanup(void)
 +{
 +	destroy_workqueue(mlx5_ib_page_fault_wq);
 +}
diff --cc include/linux/mlx5/driver.h
index 61d8bec93f9f,886ff2b00500..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -980,7 -1052,10 +980,14 @@@ enum 
  };
  
  enum {
++<<<<<<< HEAD
 +	MAX_MR_CACHE_ENTRIES    = 16,
++=======
+ 	MAX_UMR_CACHE_ENTRY = 20,
+ 	MLX5_IMR_MTT_CACHE_ENTRY,
+ 	MLX5_IMR_KSM_CACHE_ENTRY,
+ 	MAX_MR_CACHE_ENTRIES
++>>>>>>> 81713d3788d2 (IB/mlx5: Add implicit MR support)
  };
  
  enum {
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
* Unmerged path include/linux/mlx5/driver.h
