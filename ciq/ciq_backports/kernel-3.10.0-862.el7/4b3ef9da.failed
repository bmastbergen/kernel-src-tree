mm/swap: split swap cache into 64MB trunks

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] swap: split swap cache into 64MB trunks (Jerome Marchand) [1400689]
Rebuild_FUZZ: 96.30%
commit-author Huang, Ying <ying.huang@intel.com>
commit 4b3ef9daa4fc0bba742a79faecb17fdaaead083b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/4b3ef9da.failed

The patch is to improve the scalability of the swap out/in via using
fine grained locks for the swap cache.  In current kernel, one address
space will be used for each swap device.  And in the common
configuration, the number of the swap device is very small (one is
typical).  This causes the heavy lock contention on the radix tree of
the address space if multiple tasks swap out/in concurrently.

But in fact, there is no dependency between pages in the swap cache.  So
that, we can split the one shared address space for each swap device
into several address spaces to reduce the lock contention.  In the
patch, the shared address space is split into 64MB trunks.  64MB is
chosen to balance the memory space usage and effect of lock contention
reduction.

The size of struct address_space on x86_64 architecture is 408B, so with
the patch, 6528B more memory will be used for every 1GB swap space on
x86_64 architecture.

One address space is still shared for the swap entries in the same 64M
trunks.  To avoid lock contention for the first round of swap space
allocation, the order of the swap clusters in the initial free clusters
list is changed.  The swap space distance between the consecutive swap
clusters in the free cluster list is at least 64M.  After the first
round of allocation, the swap clusters are expected to be freed
randomly, so the lock contention should be reduced effectively.

Link: http://lkml.kernel.org/r/735bab895e64c930581ffb0a05b661e01da82bc5.1484082593.git.tim.c.chen@linux.intel.com
	Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Aaron Lu <aaron.lu@intel.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
	Cc: Huang Ying <ying.huang@intel.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Jonathan Corbet <corbet@lwn.net> escreveu:
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Shaohua Li <shli@kernel.org>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4b3ef9daa4fc0bba742a79faecb17fdaaead083b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/swap.c
#	mm/swap_state.c
#	mm/swapfile.c
diff --cc mm/swap.c
index 499f47fcf68a,aabf2e90fe32..000000000000
--- a/mm/swap.c
+++ b/mm/swap.c
@@@ -1206,15 -971,6 +1206,18 @@@ EXPORT_SYMBOL(pagevec_lookup_tag)
  void __init swap_setup(void)
  {
  	unsigned long megs = totalram_pages >> (20 - PAGE_SHIFT);
++<<<<<<< HEAD
 +#ifdef CONFIG_SWAP
 +	int i;
 +
 +	bdi_init(swapper_spaces[0].backing_dev_info);
 +	for (i = 0; i < MAX_SWAPFILES; i++) {
 +		spin_lock_init(&swapper_spaces[i].tree_lock);
 +		INIT_LIST_HEAD(&swapper_spaces[i].i_mmap_nonlinear);
 +	}
 +#endif
++=======
++>>>>>>> 4b3ef9daa4fc (mm/swap: split swap cache into 64MB trunks)
  
  	/* Use a smaller cluster for small-memory machines */
  	if (megs < 16)
diff --cc mm/swap_state.c
index 8ead62769c81,3863acd6189c..000000000000
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@@ -17,7 -17,7 +17,11 @@@
  #include <linux/blkdev.h>
  #include <linux/pagevec.h>
  #include <linux/migrate.h>
++<<<<<<< HEAD
 +#include <linux/page_cgroup.h>
++=======
+ #include <linux/vmalloc.h>
++>>>>>>> 4b3ef9daa4fc (mm/swap: split swap cache into 64MB trunks)
  
  #include <asm/pgtable.h>
  
@@@ -28,22 -28,13 +32,27 @@@
  static const struct address_space_operations swap_aops = {
  	.writepage	= swap_writepage,
  	.set_page_dirty	= swap_set_page_dirty,
 -#ifdef CONFIG_MIGRATION
  	.migratepage	= migrate_page,
 -#endif
  };
  
 +static struct backing_dev_info swap_backing_dev_info = {
 +	.name		= "swap",
 +	.capabilities	= BDI_CAP_NO_ACCT_AND_WRITEBACK | BDI_CAP_SWAP_BACKED,
 +};
 +
++<<<<<<< HEAD
 +struct address_space swapper_spaces[MAX_SWAPFILES] = {
 +	[0 ... MAX_SWAPFILES - 1] = {
 +		.page_tree	= RADIX_TREE_INIT(GFP_ATOMIC|__GFP_NOWARN),
 +		.i_mmap_writable = ATOMIC_INIT(0),
 +		.a_ops		= &swap_aops,
 +		.backing_dev_info = &swap_backing_dev_info,
 +	}
 +};
++=======
+ struct address_space *swapper_spaces[MAX_SWAPFILES];
+ static unsigned int nr_swapper_spaces[MAX_SWAPFILES];
++>>>>>>> 4b3ef9daa4fc (mm/swap: split swap cache into 64MB trunks)
  
  #define INC_CACHE_INFO(x)	do { swap_cache_info.x++; } while (0)
  
@@@ -448,5 -511,41 +472,40 @@@ struct page *swapin_readahead(swp_entry
  	blk_finish_plug(&plug);
  
  	lru_add_drain();	/* Push any new pages onto the LRU now */
 -skip:
  	return read_swap_cache_async(entry, gfp_mask, vma, addr);
  }
+ 
+ int init_swap_address_space(unsigned int type, unsigned long nr_pages)
+ {
+ 	struct address_space *spaces, *space;
+ 	unsigned int i, nr;
+ 
+ 	nr = DIV_ROUND_UP(nr_pages, SWAP_ADDRESS_SPACE_PAGES);
+ 	spaces = vzalloc(sizeof(struct address_space) * nr);
+ 	if (!spaces)
+ 		return -ENOMEM;
+ 	for (i = 0; i < nr; i++) {
+ 		space = spaces + i;
+ 		INIT_RADIX_TREE(&space->page_tree, GFP_ATOMIC|__GFP_NOWARN);
+ 		atomic_set(&space->i_mmap_writable, 0);
+ 		space->a_ops = &swap_aops;
+ 		/* swap cache doesn't use writeback related tags */
+ 		mapping_set_no_writeback_tags(space);
+ 		spin_lock_init(&space->tree_lock);
+ 	}
+ 	nr_swapper_spaces[type] = nr;
+ 	rcu_assign_pointer(swapper_spaces[type], spaces);
+ 
+ 	return 0;
+ }
+ 
+ void exit_swap_address_space(unsigned int type)
+ {
+ 	struct address_space *spaces;
+ 
+ 	spaces = swapper_spaces[type];
+ 	nr_swapper_spaces[type] = 0;
+ 	rcu_assign_pointer(swapper_spaces[type], NULL);
+ 	synchronize_rcu();
+ 	kvfree(spaces);
+ }
diff --cc mm/swapfile.c
index 44c2eac6b890,66e95eb73040..000000000000
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@@ -1691,11 -2075,16 +1691,12 @@@ SYSCALL_DEFINE1(swapoff, const char __u
  	spin_unlock(&p->lock);
  	spin_unlock(&swap_lock);
  	frontswap_invalidate_area(p->type);
 -	frontswap_map_set(p, NULL);
  	mutex_unlock(&swapon_mutex);
 -	free_percpu(p->percpu_cluster);
 -	p->percpu_cluster = NULL;
  	vfree(swap_map);
 -	vfree(cluster_info);
  	vfree(frontswap_map);
 -	/* Destroy swap account information */
 +	/* Destroy swap account informatin */
  	swap_cgroup_swapoff(p->type);
+ 	exit_swap_address_space(p->type);
  
  	inode = mapping->host;
  	if (S_ISBLK(inode->i_mode)) {
@@@ -2003,6 -2408,13 +2004,16 @@@ static unsigned long read_swap_header(s
  	return maxpages;
  }
  
++<<<<<<< HEAD
++=======
+ #define SWAP_CLUSTER_INFO_COLS						\
+ 	DIV_ROUND_UP(L1_CACHE_BYTES, sizeof(struct swap_cluster_info))
+ #define SWAP_CLUSTER_SPACE_COLS						\
+ 	DIV_ROUND_UP(SWAP_ADDRESS_SPACE_PAGES, SWAPFILE_CLUSTER)
+ #define SWAP_CLUSTER_COLS						\
+ 	max_t(unsigned int, SWAP_CLUSTER_INFO_COLS, SWAP_CLUSTER_SPACE_COLS)
+ 
++>>>>>>> 4b3ef9daa4fc (mm/swap: split swap cache into 64MB trunks)
  static int setup_swap_map_and_extents(struct swap_info_struct *p,
  					union swap_header *swap_header,
  					unsigned char *swap_map,
@@@ -2039,6 -2472,27 +2050,30 @@@
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (!cluster_info)
+ 		return nr_extents;
+ 
+ 
+ 	/*
+ 	 * Reduce false cache line sharing between cluster_info and
+ 	 * sharing same address space.
+ 	 */
+ 	for (k = 0; k < SWAP_CLUSTER_COLS; k++) {
+ 		j = (k + col) % SWAP_CLUSTER_COLS;
+ 		for (i = 0; i < DIV_ROUND_UP(nr_clusters, SWAP_CLUSTER_COLS); i++) {
+ 			idx = i * SWAP_CLUSTER_COLS + j;
+ 			if (idx >= nr_clusters)
+ 				continue;
+ 			if (cluster_count(&cluster_info[idx]))
+ 				continue;
+ 			cluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);
+ 			cluster_list_add_tail(&p->free_clusters, cluster_info,
+ 					      idx);
+ 		}
+ 	}
++>>>>>>> 4b3ef9daa4fc (mm/swap: split swap cache into 64MB trunks)
  	return nr_extents;
  }
  
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 61d41d507b58..997dd040e944 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -383,8 +383,13 @@ int generic_swapfile_activate(struct swap_info_struct *, struct file *,
 		sector_t *);
 
 /* linux/mm/swap_state.c */
-extern struct address_space swapper_spaces[];
-#define swap_address_space(entry) (&swapper_spaces[swp_type(entry)])
+/* One swap address space for each 64M swap space */
+#define SWAP_ADDRESS_SPACE_SHIFT	14
+#define SWAP_ADDRESS_SPACE_PAGES	(1 << SWAP_ADDRESS_SPACE_SHIFT)
+extern struct address_space *swapper_spaces[];
+#define swap_address_space(entry)			    \
+	(&swapper_spaces[swp_type(entry)][swp_offset(entry) \
+		>> SWAP_ADDRESS_SPACE_SHIFT])
 extern unsigned long total_swapcache_pages(void);
 extern void show_swap_cache_info(void);
 extern int add_to_swap(struct page *, struct list_head *list);
@@ -434,6 +439,8 @@ extern struct swap_info_struct *page_swap_info(struct page *);
 extern int reuse_swap_page(struct page *);
 extern int try_to_free_swap(struct page *);
 struct backing_dev_info;
+extern int init_swap_address_space(unsigned int type, unsigned long nr_pages);
+extern void exit_swap_address_space(unsigned int type);
 
 #ifdef CONFIG_MEMCG
 extern void
* Unmerged path mm/swap.c
* Unmerged path mm/swap_state.c
* Unmerged path mm/swapfile.c
