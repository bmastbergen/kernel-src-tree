ixgbe: get rid of custom busy polling code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 3ffc1af576550ec61d35668485954e49da29d168
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/3ffc1af5.failed

In linux-4.5, busy polling was implemented in core
NAPI stack, meaning that all custom implementation can
be removed from drivers.

Not only we remove lot's of code, we also remove one lock
operation in fast path, and allow GRO to do its job.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
	Acked-by: Alexander Duyck <alexander.h.duyck@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3ffc1af576550ec61d35668485954e49da29d168)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ixgbe/ixgbe.h
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe.h
index f0fd27d2f566,e83444c34cf9..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe.h
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe.h
@@@ -399,158 -391,9 +391,164 @@@ struct ixgbe_q_vector 
  	struct rcu_head rcu;	/* to avoid race with update stats on free */
  	char name[IFNAMSIZ + 9];
  
++<<<<<<< HEAD
 +#ifdef CONFIG_NET_RX_BUSY_POLL
 +	unsigned int state;
 +#define IXGBE_QV_STATE_IDLE        0
 +#define IXGBE_QV_STATE_NAPI	   1     /* NAPI owns this QV */
 +#define IXGBE_QV_STATE_POLL	   2     /* poll owns this QV */
 +#define IXGBE_QV_STATE_DISABLED	   4     /* QV is disabled */
 +#define IXGBE_QV_OWNED (IXGBE_QV_STATE_NAPI | IXGBE_QV_STATE_POLL)
 +#define IXGBE_QV_LOCKED (IXGBE_QV_OWNED | IXGBE_QV_STATE_DISABLED)
 +#define IXGBE_QV_STATE_NAPI_YIELD  8     /* NAPI yielded this QV */
 +#define IXGBE_QV_STATE_POLL_YIELD  16    /* poll yielded this QV */
 +#define IXGBE_QV_YIELD (IXGBE_QV_STATE_NAPI_YIELD | IXGBE_QV_STATE_POLL_YIELD)
 +#define IXGBE_QV_USER_PEND (IXGBE_QV_STATE_POLL | IXGBE_QV_STATE_POLL_YIELD)
 +	spinlock_t lock;
 +#endif  /* CONFIG_NET_RX_BUSY_POLL */
 +
 +	/* for dynamic allocation of rings associated with this q_vector */
 +	struct ixgbe_ring ring[0] ____cacheline_internodealigned_in_smp;
 +};
 +#ifdef CONFIG_NET_RX_BUSY_POLL
 +static inline void ixgbe_qv_init_lock(struct ixgbe_q_vector *q_vector)
 +{
 +
 +	spin_lock_init(&q_vector->lock);
 +	q_vector->state = IXGBE_QV_STATE_IDLE;
 +}
 +
 +/* called from the device poll routine to get ownership of a q_vector */
 +static inline bool ixgbe_qv_lock_napi(struct ixgbe_q_vector *q_vector)
 +{
 +	int rc = true;
 +	spin_lock_bh(&q_vector->lock);
 +	if (q_vector->state & IXGBE_QV_LOCKED) {
 +		WARN_ON(q_vector->state & IXGBE_QV_STATE_NAPI);
 +		q_vector->state |= IXGBE_QV_STATE_NAPI_YIELD;
 +		rc = false;
 +#ifdef BP_EXTENDED_STATS
 +		q_vector->tx.ring->stats.yields++;
 +#endif
 +	} else {
 +		/* we don't care if someone yielded */
 +		q_vector->state = IXGBE_QV_STATE_NAPI;
 +	}
 +	spin_unlock_bh(&q_vector->lock);
 +	return rc;
 +}
 +
 +/* returns true is someone tried to get the qv while napi had it */
 +static inline bool ixgbe_qv_unlock_napi(struct ixgbe_q_vector *q_vector)
 +{
 +	int rc = false;
 +	spin_lock_bh(&q_vector->lock);
 +	WARN_ON(q_vector->state & (IXGBE_QV_STATE_POLL |
 +			       IXGBE_QV_STATE_NAPI_YIELD));
 +
 +	if (q_vector->state & IXGBE_QV_STATE_POLL_YIELD)
 +		rc = true;
 +	/* will reset state to idle, unless QV is disabled */
 +	q_vector->state &= IXGBE_QV_STATE_DISABLED;
 +	spin_unlock_bh(&q_vector->lock);
 +	return rc;
 +}
 +
 +/* called from ixgbe_low_latency_poll() */
 +static inline bool ixgbe_qv_lock_poll(struct ixgbe_q_vector *q_vector)
 +{
 +	int rc = true;
 +	spin_lock_bh(&q_vector->lock);
 +	if ((q_vector->state & IXGBE_QV_LOCKED)) {
 +		q_vector->state |= IXGBE_QV_STATE_POLL_YIELD;
 +		rc = false;
 +#ifdef BP_EXTENDED_STATS
 +		q_vector->rx.ring->stats.yields++;
 +#endif
 +	} else {
 +		/* preserve yield marks */
 +		q_vector->state |= IXGBE_QV_STATE_POLL;
 +	}
 +	spin_unlock_bh(&q_vector->lock);
 +	return rc;
 +}
 +
 +/* returns true if someone tried to get the qv while it was locked */
 +static inline bool ixgbe_qv_unlock_poll(struct ixgbe_q_vector *q_vector)
 +{
 +	int rc = false;
 +	spin_lock_bh(&q_vector->lock);
 +	WARN_ON(q_vector->state & (IXGBE_QV_STATE_NAPI));
 +
 +	if (q_vector->state & IXGBE_QV_STATE_POLL_YIELD)
 +		rc = true;
 +	/* will reset state to idle, unless QV is disabled */
 +	q_vector->state &= IXGBE_QV_STATE_DISABLED;
 +	spin_unlock_bh(&q_vector->lock);
 +	return rc;
 +}
 +
 +/* true if a socket is polling, even if it did not get the lock */
 +static inline bool ixgbe_qv_busy_polling(struct ixgbe_q_vector *q_vector)
 +{
 +	WARN_ON(!(q_vector->state & IXGBE_QV_OWNED));
 +	return q_vector->state & IXGBE_QV_USER_PEND;
 +}
 +
 +/* false if QV is currently owned */
 +static inline bool ixgbe_qv_disable(struct ixgbe_q_vector *q_vector)
 +{
 +	int rc = true;
 +	spin_lock_bh(&q_vector->lock);
 +	if (q_vector->state & IXGBE_QV_OWNED)
 +		rc = false;
 +	q_vector->state |= IXGBE_QV_STATE_DISABLED;
 +	spin_unlock_bh(&q_vector->lock);
 +
 +	return rc;
 +}
 +
 +#else /* CONFIG_NET_RX_BUSY_POLL */
 +static inline void ixgbe_qv_init_lock(struct ixgbe_q_vector *q_vector)
 +{
 +}
 +
 +static inline bool ixgbe_qv_lock_napi(struct ixgbe_q_vector *q_vector)
 +{
 +	return true;
 +}
 +
 +static inline bool ixgbe_qv_unlock_napi(struct ixgbe_q_vector *q_vector)
 +{
 +	return false;
 +}
 +
 +static inline bool ixgbe_qv_lock_poll(struct ixgbe_q_vector *q_vector)
 +{
 +	return false;
 +}
 +
 +static inline bool ixgbe_qv_unlock_poll(struct ixgbe_q_vector *q_vector)
 +{
 +	return false;
 +}
 +
 +static inline bool ixgbe_qv_busy_polling(struct ixgbe_q_vector *q_vector)
 +{
 +	return false;
 +}
 +
 +static inline bool ixgbe_qv_disable(struct ixgbe_q_vector *q_vector)
 +{
 +	return true;
 +}
 +
 +#endif /* CONFIG_NET_RX_BUSY_POLL */
++=======
+ 	/* for dynamic allocation of rings associated with this q_vector */
+ 	struct ixgbe_ring ring[0] ____cacheline_internodealigned_in_smp;
+ };
++>>>>>>> 3ffc1af57655 (ixgbe: get rid of custom busy polling code)
  
  #ifdef CONFIG_IXGBE_HWMON
  
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe.h
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c
index 72e704cb0047..f7e48f4e2a52 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_ethtool.c
@@ -1179,12 +1179,6 @@ static void ixgbe_get_ethtool_stats(struct net_device *netdev,
 			data[i] = 0;
 			data[i+1] = 0;
 			i += 2;
-#ifdef BP_EXTENDED_STATS
-			data[i] = 0;
-			data[i+1] = 0;
-			data[i+2] = 0;
-			i += 3;
-#endif
 			continue;
 		}
 
@@ -1194,12 +1188,6 @@ static void ixgbe_get_ethtool_stats(struct net_device *netdev,
 			data[i+1] = ring->stats.bytes;
 		} while (u64_stats_fetch_retry_irq(&ring->syncp, start));
 		i += 2;
-#ifdef BP_EXTENDED_STATS
-		data[i] = ring->stats.yields;
-		data[i+1] = ring->stats.misses;
-		data[i+2] = ring->stats.cleaned;
-		i += 3;
-#endif
 	}
 	for (j = 0; j < IXGBE_NUM_RX_QUEUES; j++) {
 		ring = adapter->rx_ring[j];
@@ -1207,12 +1195,6 @@ static void ixgbe_get_ethtool_stats(struct net_device *netdev,
 			data[i] = 0;
 			data[i+1] = 0;
 			i += 2;
-#ifdef BP_EXTENDED_STATS
-			data[i] = 0;
-			data[i+1] = 0;
-			data[i+2] = 0;
-			i += 3;
-#endif
 			continue;
 		}
 
@@ -1222,12 +1204,6 @@ static void ixgbe_get_ethtool_stats(struct net_device *netdev,
 			data[i+1] = ring->stats.bytes;
 		} while (u64_stats_fetch_retry_irq(&ring->syncp, start));
 		i += 2;
-#ifdef BP_EXTENDED_STATS
-		data[i] = ring->stats.yields;
-		data[i+1] = ring->stats.misses;
-		data[i+2] = ring->stats.cleaned;
-		i += 3;
-#endif
 	}
 
 	for (j = 0; j < IXGBE_MAX_PACKET_BUFFERS; j++) {
@@ -1264,28 +1240,12 @@ static void ixgbe_get_strings(struct net_device *netdev, u32 stringset,
 			p += ETH_GSTRING_LEN;
 			sprintf(p, "tx_queue_%u_bytes", i);
 			p += ETH_GSTRING_LEN;
-#ifdef BP_EXTENDED_STATS
-			sprintf(p, "tx_queue_%u_bp_napi_yield", i);
-			p += ETH_GSTRING_LEN;
-			sprintf(p, "tx_queue_%u_bp_misses", i);
-			p += ETH_GSTRING_LEN;
-			sprintf(p, "tx_queue_%u_bp_cleaned", i);
-			p += ETH_GSTRING_LEN;
-#endif /* BP_EXTENDED_STATS */
 		}
 		for (i = 0; i < IXGBE_NUM_RX_QUEUES; i++) {
 			sprintf(p, "rx_queue_%u_packets", i);
 			p += ETH_GSTRING_LEN;
 			sprintf(p, "rx_queue_%u_bytes", i);
 			p += ETH_GSTRING_LEN;
-#ifdef BP_EXTENDED_STATS
-			sprintf(p, "rx_queue_%u_bp_poll_yield", i);
-			p += ETH_GSTRING_LEN;
-			sprintf(p, "rx_queue_%u_bp_misses", i);
-			p += ETH_GSTRING_LEN;
-			sprintf(p, "rx_queue_%u_bp_cleaned", i);
-			p += ETH_GSTRING_LEN;
-#endif /* BP_EXTENDED_STATS */
 		}
 		for (i = 0; i < IXGBE_MAX_PACKET_BUFFERS; i++) {
 			sprintf(p, "tx_pb_%u_pxon", i);
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index 0afaf44ddea2..f78f8863d58d 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -1719,11 +1719,7 @@ static void ixgbe_process_skb_fields(struct ixgbe_ring *rx_ring,
 static void ixgbe_rx_skb(struct ixgbe_q_vector *q_vector,
 			 struct sk_buff *skb)
 {
-	skb_mark_napi_id(skb, &q_vector->napi);
-	if (ixgbe_qv_busy_polling(q_vector))
-		netif_receive_skb(skb);
-	else
-		napi_gro_receive(&q_vector->napi, skb);
+	napi_gro_receive(&q_vector->napi, skb);
 }
 
 /**
@@ -2200,40 +2196,6 @@ static int ixgbe_clean_rx_irq(struct ixgbe_q_vector *q_vector,
 	return total_rx_packets;
 }
 
-#ifdef CONFIG_NET_RX_BUSY_POLL
-/* must be called with local_bh_disable()d */
-static int ixgbe_low_latency_recv(struct napi_struct *napi)
-{
-	struct ixgbe_q_vector *q_vector =
-			container_of(napi, struct ixgbe_q_vector, napi);
-	struct ixgbe_adapter *adapter = q_vector->adapter;
-	struct ixgbe_ring  *ring;
-	int found = 0;
-
-	if (test_bit(__IXGBE_DOWN, &adapter->state))
-		return LL_FLUSH_FAILED;
-
-	if (!ixgbe_qv_lock_poll(q_vector))
-		return LL_FLUSH_BUSY;
-
-	ixgbe_for_each_ring(ring, q_vector->rx) {
-		found = ixgbe_clean_rx_irq(q_vector, ring, 4);
-#ifdef BP_EXTENDED_STATS
-		if (found)
-			ring->stats.cleaned += found;
-		else
-			ring->stats.misses++;
-#endif
-		if (found)
-			break;
-	}
-
-	ixgbe_qv_unlock_poll(q_vector);
-
-	return found;
-}
-#endif	/* CONFIG_NET_RX_BUSY_POLL */
-
 /**
  * ixgbe_configure_msix - Configure MSI-X hardware
  * @adapter: board private structure
@@ -2877,8 +2839,8 @@ int ixgbe_poll(struct napi_struct *napi, int budget)
 			clean_complete = false;
 	}
 
-	/* Exit if we are called by netpoll or busy polling is active */
-	if ((budget <= 0) || !ixgbe_qv_lock_napi(q_vector))
+	/* Exit if we are called by netpoll */
+	if (budget <= 0)
 		return budget;
 
 	/* attempt to distribute budget to each queue fairly, but don't allow
@@ -2897,7 +2859,6 @@ int ixgbe_poll(struct napi_struct *napi, int budget)
 			clean_complete = false;
 	}
 
-	ixgbe_qv_unlock_napi(q_vector);
 	/* If all work not completed, return budget and keep polling */
 	if (!clean_complete)
 		return budget;
@@ -4580,23 +4541,16 @@ static void ixgbe_napi_enable_all(struct ixgbe_adapter *adapter)
 {
 	int q_idx;
 
-	for (q_idx = 0; q_idx < adapter->num_q_vectors; q_idx++) {
-		ixgbe_qv_init_lock(adapter->q_vector[q_idx]);
+	for (q_idx = 0; q_idx < adapter->num_q_vectors; q_idx++)
 		napi_enable(&adapter->q_vector[q_idx]->napi);
-	}
 }
 
 static void ixgbe_napi_disable_all(struct ixgbe_adapter *adapter)
 {
 	int q_idx;
 
-	for (q_idx = 0; q_idx < adapter->num_q_vectors; q_idx++) {
+	for (q_idx = 0; q_idx < adapter->num_q_vectors; q_idx++)
 		napi_disable(&adapter->q_vector[q_idx]->napi);
-		while (!ixgbe_qv_disable(adapter->q_vector[q_idx])) {
-			pr_info("QV %d locked\n", q_idx);
-			usleep_range(1000, 20000);
-		}
-	}
 }
 
 static void ixgbe_clear_udp_tunnel_port(struct ixgbe_adapter *adapter, u32 mask)
@@ -8864,9 +8818,6 @@ static const struct net_device_ops ixgbe_netdev_ops = {
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= ixgbe_netpoll,
 #endif
-#ifdef CONFIG_NET_RX_BUSY_POLL
-	.ndo_busy_poll		= ixgbe_low_latency_recv,
-#endif
 #ifdef IXGBE_FCOE
 	.ndo_fcoe_ddp_setup = ixgbe_fcoe_ddp_get,
 	.ndo_fcoe_ddp_target = ixgbe_fcoe_ddp_target,
