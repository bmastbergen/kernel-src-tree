refcount_t: Introduce a special purpose refcount type

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit f405df5de3170c00e5c54f8b7cf4766044a032ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f405df5d.failed

Provide refcount_t, an atomic_t like primitive built just for
refcounting.

It provides saturation semantics such that overflow becomes impossible
and thereby 'spurious' use-after-free is avoided.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit f405df5de3170c00e5c54f8b7cf4766044a032ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/Kconfig.debug
diff --cc lib/Kconfig.debug
index 87b12a011765,8f1f0e609891..000000000000
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@@ -539,6 -578,435 +539,438 @@@ config DEBUG_KMEMLEAK_DEFAULT_OF
  	  Say Y here to disable kmemleak by default. It can then be enabled
  	  on the command line via kmemleak=on.
  
++<<<<<<< HEAD
++=======
+ config DEBUG_STACK_USAGE
+ 	bool "Stack utilization instrumentation"
+ 	depends on DEBUG_KERNEL && !IA64
+ 	help
+ 	  Enables the display of the minimum amount of free stack which each
+ 	  task has ever had available in the sysrq-T and sysrq-P debug output.
+ 
+ 	  This option will slow down process creation somewhat.
+ 
+ config DEBUG_VM
+ 	bool "Debug VM"
+ 	depends on DEBUG_KERNEL
+ 	help
+ 	  Enable this to turn on extended checks in the virtual-memory system
+           that may impact performance.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VM_VMACACHE
+ 	bool "Debug VMA caching"
+ 	depends on DEBUG_VM
+ 	help
+ 	  Enable this to turn on VMA caching debug information. Doing so
+ 	  can cause significant overhead, so only enable it in non-production
+ 	  environments.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VM_RB
+ 	bool "Debug VM red-black trees"
+ 	depends on DEBUG_VM
+ 	help
+ 	  Enable VM red-black tree debugging information and extra validations.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VM_PGFLAGS
+ 	bool "Debug page-flags operations"
+ 	depends on DEBUG_VM
+ 	help
+ 	  Enables extra validation on page flags operations.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_VIRTUAL
+ 	bool "Debug VM translations"
+ 	depends on DEBUG_KERNEL && X86
+ 	help
+ 	  Enable some costly sanity checks in virtual to page code. This can
+ 	  catch mistakes with virt_to_page() and friends.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_NOMMU_REGIONS
+ 	bool "Debug the global anon/private NOMMU mapping region tree"
+ 	depends on DEBUG_KERNEL && !MMU
+ 	help
+ 	  This option causes the global tree of anonymous and private mapping
+ 	  regions to be regularly checked for invalid topology.
+ 
+ config DEBUG_MEMORY_INIT
+ 	bool "Debug memory initialisation" if EXPERT
+ 	default !EXPERT
+ 	help
+ 	  Enable this for additional checks during memory initialisation.
+ 	  The sanity checks verify aspects of the VM such as the memory model
+ 	  and other information provided by the architecture. Verbose
+ 	  information will be printed at KERN_DEBUG loglevel depending
+ 	  on the mminit_loglevel= command-line option.
+ 
+ 	  If unsure, say Y
+ 
+ config MEMORY_NOTIFIER_ERROR_INJECT
+ 	tristate "Memory hotplug notifier error injection module"
+ 	depends on MEMORY_HOTPLUG_SPARSE && NOTIFIER_ERROR_INJECTION
+ 	help
+ 	  This option provides the ability to inject artificial errors to
+ 	  memory hotplug notifier chain callbacks.  It is controlled through
+ 	  debugfs interface under /sys/kernel/debug/notifier-error-inject/memory
+ 
+ 	  If the notifier call chain should be failed with some events
+ 	  notified, write the error code to "actions/<notifier event>/error".
+ 
+ 	  Example: Inject memory hotplug offline error (-12 == -ENOMEM)
+ 
+ 	  # cd /sys/kernel/debug/notifier-error-inject/memory
+ 	  # echo -12 > actions/MEM_GOING_OFFLINE/error
+ 	  # echo offline > /sys/devices/system/memory/memoryXXX/state
+ 	  bash: echo: write error: Cannot allocate memory
+ 
+ 	  To compile this code as a module, choose M here: the module will
+ 	  be called memory-notifier-error-inject.
+ 
+ 	  If unsure, say N.
+ 
+ config DEBUG_PER_CPU_MAPS
+ 	bool "Debug access to per_cpu maps"
+ 	depends on DEBUG_KERNEL
+ 	depends on SMP
+ 	help
+ 	  Say Y to verify that the per_cpu map being accessed has
+ 	  been set up. This adds a fair amount of code to kernel memory
+ 	  and decreases performance.
+ 
+ 	  Say N if unsure.
+ 
+ config DEBUG_HIGHMEM
+ 	bool "Highmem debugging"
+ 	depends on DEBUG_KERNEL && HIGHMEM
+ 	help
+ 	  This option enables additional error checking for high memory
+ 	  systems.  Disable for production systems.
+ 
+ config HAVE_DEBUG_STACKOVERFLOW
+ 	bool
+ 
+ config DEBUG_STACKOVERFLOW
+ 	bool "Check for stack overflows"
+ 	depends on DEBUG_KERNEL && HAVE_DEBUG_STACKOVERFLOW
+ 	---help---
+ 	  Say Y here if you want to check for overflows of kernel, IRQ
+ 	  and exception stacks (if your architecture uses them). This
+ 	  option will show detailed messages if free stack space drops
+ 	  below a certain limit.
+ 
+ 	  These kinds of bugs usually occur when call-chains in the
+ 	  kernel get too deep, especially when interrupts are
+ 	  involved.
+ 
+ 	  Use this in cases where you see apparently random memory
+ 	  corruption, especially if it appears in 'struct thread_info'
+ 
+ 	  If in doubt, say "N".
+ 
+ source "lib/Kconfig.kmemcheck"
+ 
+ source "lib/Kconfig.kasan"
+ 
+ config DEBUG_REFCOUNT
+ 	bool "Verbose refcount checks"
+ 	help
+ 	  Say Y here if you want reference counters (refcount_t and kref) to
+ 	  generate WARNs on dubious usage. Without this refcount_t will still
+ 	  be a saturating counter and avoid Use-After-Free by turning it into
+ 	  a resource leak Denial-Of-Service.
+ 
+ 	  Use of this option will increase kernel text size but will alert the
+ 	  admin of potential abuse.
+ 
+ 	  If in doubt, say "N".
+ 
+ endmenu # "Memory Debugging"
+ 
+ config ARCH_HAS_KCOV
+ 	bool
+ 	help
+ 	  KCOV does not have any arch-specific code, but currently it is enabled
+ 	  only for x86_64. KCOV requires testing on other archs, and most likely
+ 	  disabling of instrumentation for some early boot code.
+ 
+ config KCOV
+ 	bool "Code coverage for fuzzing"
+ 	depends on ARCH_HAS_KCOV
+ 	select DEBUG_FS
+ 	select GCC_PLUGINS if !COMPILE_TEST
+ 	select GCC_PLUGIN_SANCOV if !COMPILE_TEST
+ 	help
+ 	  KCOV exposes kernel code coverage information in a form suitable
+ 	  for coverage-guided fuzzing (randomized testing).
+ 
+ 	  If RANDOMIZE_BASE is enabled, PC values will not be stable across
+ 	  different machines and across reboots. If you need stable PC values,
+ 	  disable RANDOMIZE_BASE.
+ 
+ 	  For more details, see Documentation/dev-tools/kcov.rst.
+ 
+ config KCOV_INSTRUMENT_ALL
+ 	bool "Instrument all code by default"
+ 	depends on KCOV
+ 	default y if KCOV
+ 	help
+ 	  If you are doing generic system call fuzzing (like e.g. syzkaller),
+ 	  then you will want to instrument the whole kernel and you should
+ 	  say y here. If you are doing more targeted fuzzing (like e.g.
+ 	  filesystem fuzzing with AFL) then you will want to enable coverage
+ 	  for more specific subsets of files, and should say n here.
+ 
+ config DEBUG_SHIRQ
+ 	bool "Debug shared IRQ handlers"
+ 	depends on DEBUG_KERNEL
+ 	help
+ 	  Enable this to generate a spurious interrupt as soon as a shared
+ 	  interrupt handler is registered, and just before one is deregistered.
+ 	  Drivers ought to be able to handle interrupts coming in at those
+ 	  points; some don't and need to be caught.
+ 
+ menu "Debug Lockups and Hangs"
+ 
+ config LOCKUP_DETECTOR
+ 	bool "Detect Hard and Soft Lockups"
+ 	depends on DEBUG_KERNEL && !S390
+ 	help
+ 	  Say Y here to enable the kernel to act as a watchdog to detect
+ 	  hard and soft lockups.
+ 
+ 	  Softlockups are bugs that cause the kernel to loop in kernel
+ 	  mode for more than 20 seconds, without giving other tasks a
+ 	  chance to run.  The current stack trace is displayed upon
+ 	  detection and the system will stay locked up.
+ 
+ 	  Hardlockups are bugs that cause the CPU to loop in kernel mode
+ 	  for more than 10 seconds, without letting other interrupts have a
+ 	  chance to run.  The current stack trace is displayed upon detection
+ 	  and the system will stay locked up.
+ 
+ 	  The overhead should be minimal.  A periodic hrtimer runs to
+ 	  generate interrupts and kick the watchdog task every 4 seconds.
+ 	  An NMI is generated every 10 seconds or so to check for hardlockups.
+ 
+ 	  The frequency of hrtimer and NMI events and the soft and hard lockup
+ 	  thresholds can be controlled through the sysctl watchdog_thresh.
+ 
+ config HARDLOCKUP_DETECTOR
+ 	def_bool y
+ 	depends on LOCKUP_DETECTOR && !HAVE_NMI_WATCHDOG
+ 	depends on PERF_EVENTS && HAVE_PERF_EVENTS_NMI
+ 
+ config BOOTPARAM_HARDLOCKUP_PANIC
+ 	bool "Panic (Reboot) On Hard Lockups"
+ 	depends on HARDLOCKUP_DETECTOR
+ 	help
+ 	  Say Y here to enable the kernel to panic on "hard lockups",
+ 	  which are bugs that cause the kernel to loop in kernel
+ 	  mode with interrupts disabled for more than 10 seconds (configurable
+ 	  using the watchdog_thresh sysctl).
+ 
+ 	  Say N if unsure.
+ 
+ config BOOTPARAM_HARDLOCKUP_PANIC_VALUE
+ 	int
+ 	depends on HARDLOCKUP_DETECTOR
+ 	range 0 1
+ 	default 0 if !BOOTPARAM_HARDLOCKUP_PANIC
+ 	default 1 if BOOTPARAM_HARDLOCKUP_PANIC
+ 
+ config BOOTPARAM_SOFTLOCKUP_PANIC
+ 	bool "Panic (Reboot) On Soft Lockups"
+ 	depends on LOCKUP_DETECTOR
+ 	help
+ 	  Say Y here to enable the kernel to panic on "soft lockups",
+ 	  which are bugs that cause the kernel to loop in kernel
+ 	  mode for more than 20 seconds (configurable using the watchdog_thresh
+ 	  sysctl), without giving other tasks a chance to run.
+ 
+ 	  The panic can be used in combination with panic_timeout,
+ 	  to cause the system to reboot automatically after a
+ 	  lockup has been detected. This feature is useful for
+ 	  high-availability systems that have uptime guarantees and
+ 	  where a lockup must be resolved ASAP.
+ 
+ 	  Say N if unsure.
+ 
+ config BOOTPARAM_SOFTLOCKUP_PANIC_VALUE
+ 	int
+ 	depends on LOCKUP_DETECTOR
+ 	range 0 1
+ 	default 0 if !BOOTPARAM_SOFTLOCKUP_PANIC
+ 	default 1 if BOOTPARAM_SOFTLOCKUP_PANIC
+ 
+ config DETECT_HUNG_TASK
+ 	bool "Detect Hung Tasks"
+ 	depends on DEBUG_KERNEL
+ 	default LOCKUP_DETECTOR
+ 	help
+ 	  Say Y here to enable the kernel to detect "hung tasks",
+ 	  which are bugs that cause the task to be stuck in
+ 	  uninterruptible "D" state indefinitely.
+ 
+ 	  When a hung task is detected, the kernel will print the
+ 	  current stack trace (which you should report), but the
+ 	  task will stay in uninterruptible state. If lockdep is
+ 	  enabled then all held locks will also be reported. This
+ 	  feature has negligible overhead.
+ 
+ config DEFAULT_HUNG_TASK_TIMEOUT
+ 	int "Default timeout for hung task detection (in seconds)"
+ 	depends on DETECT_HUNG_TASK
+ 	default 120
+ 	help
+ 	  This option controls the default timeout (in seconds) used
+ 	  to determine when a task has become non-responsive and should
+ 	  be considered hung.
+ 
+ 	  It can be adjusted at runtime via the kernel.hung_task_timeout_secs
+ 	  sysctl or by writing a value to
+ 	  /proc/sys/kernel/hung_task_timeout_secs.
+ 
+ 	  A timeout of 0 disables the check.  The default is two minutes.
+ 	  Keeping the default should be fine in most cases.
+ 
+ config BOOTPARAM_HUNG_TASK_PANIC
+ 	bool "Panic (Reboot) On Hung Tasks"
+ 	depends on DETECT_HUNG_TASK
+ 	help
+ 	  Say Y here to enable the kernel to panic on "hung tasks",
+ 	  which are bugs that cause the kernel to leave a task stuck
+ 	  in uninterruptible "D" state.
+ 
+ 	  The panic can be used in combination with panic_timeout,
+ 	  to cause the system to reboot automatically after a
+ 	  hung task has been detected. This feature is useful for
+ 	  high-availability systems that have uptime guarantees and
+ 	  where a hung tasks must be resolved ASAP.
+ 
+ 	  Say N if unsure.
+ 
+ config BOOTPARAM_HUNG_TASK_PANIC_VALUE
+ 	int
+ 	depends on DETECT_HUNG_TASK
+ 	range 0 1
+ 	default 0 if !BOOTPARAM_HUNG_TASK_PANIC
+ 	default 1 if BOOTPARAM_HUNG_TASK_PANIC
+ 
+ config WQ_WATCHDOG
+ 	bool "Detect Workqueue Stalls"
+ 	depends on DEBUG_KERNEL
+ 	help
+ 	  Say Y here to enable stall detection on workqueues.  If a
+ 	  worker pool doesn't make forward progress on a pending work
+ 	  item for over a given amount of time, 30s by default, a
+ 	  warning message is printed along with dump of workqueue
+ 	  state.  This can be configured through kernel parameter
+ 	  "workqueue.watchdog_thresh" and its sysfs counterpart.
+ 
+ endmenu # "Debug lockups and hangs"
+ 
+ config PANIC_ON_OOPS
+ 	bool "Panic on Oops"
+ 	help
+ 	  Say Y here to enable the kernel to panic when it oopses. This
+ 	  has the same effect as setting oops=panic on the kernel command
+ 	  line.
+ 
+ 	  This feature is useful to ensure that the kernel does not do
+ 	  anything erroneous after an oops which could result in data
+ 	  corruption or other issues.
+ 
+ 	  Say N if unsure.
+ 
+ config PANIC_ON_OOPS_VALUE
+ 	int
+ 	range 0 1
+ 	default 0 if !PANIC_ON_OOPS
+ 	default 1 if PANIC_ON_OOPS
+ 
+ config PANIC_TIMEOUT
+ 	int "panic timeout"
+ 	default 0
+ 	help
+ 	  Set the timeout value (in seconds) until a reboot occurs when the
+ 	  the kernel panics. If n = 0, then we wait forever. A timeout
+ 	  value n > 0 will wait n seconds before rebooting, while a timeout
+ 	  value n < 0 will reboot immediately.
+ 
+ config SCHED_DEBUG
+ 	bool "Collect scheduler debugging info"
+ 	depends on DEBUG_KERNEL && PROC_FS
+ 	default y
+ 	help
+ 	  If you say Y here, the /proc/sched_debug file will be provided
+ 	  that can help debug the scheduler. The runtime overhead of this
+ 	  option is minimal.
+ 
+ config SCHED_INFO
+ 	bool
+ 	default n
+ 
+ config SCHEDSTATS
+ 	bool "Collect scheduler statistics"
+ 	depends on DEBUG_KERNEL && PROC_FS
+ 	select SCHED_INFO
+ 	help
+ 	  If you say Y here, additional code will be inserted into the
+ 	  scheduler and related routines to collect statistics about
+ 	  scheduler behavior and provide them in /proc/schedstat.  These
+ 	  stats may be useful for both tuning and debugging the scheduler
+ 	  If you aren't debugging the scheduler or trying to tune a specific
+ 	  application, you can say N to avoid the very slight overhead
+ 	  this adds.
+ 
+ config SCHED_STACK_END_CHECK
+ 	bool "Detect stack corruption on calls to schedule()"
+ 	depends on DEBUG_KERNEL
+ 	default n
+ 	help
+ 	  This option checks for a stack overrun on calls to schedule().
+ 	  If the stack end location is found to be over written always panic as
+ 	  the content of the corrupted region can no longer be trusted.
+ 	  This is to ensure no erroneous behaviour occurs which could result in
+ 	  data corruption or a sporadic crash at a later stage once the region
+ 	  is examined. The runtime overhead introduced is minimal.
+ 
+ config DEBUG_TIMEKEEPING
+ 	bool "Enable extra timekeeping sanity checking"
+ 	help
+ 	  This option will enable additional timekeeping sanity checks
+ 	  which may be helpful when diagnosing issues where timekeeping
+ 	  problems are suspected.
+ 
+ 	  This may include checks in the timekeeping hotpaths, so this
+ 	  option may have a (very small) performance impact to some
+ 	  workloads.
+ 
+ 	  If unsure, say N.
+ 
+ config TIMER_STATS
+ 	bool "Collect kernel timers statistics"
+ 	depends on DEBUG_KERNEL && PROC_FS
+ 	help
+ 	  If you say Y here, additional code will be inserted into the
+ 	  timer routines to collect statistics about kernel timers being
+ 	  reprogrammed. The statistics can be read from /proc/timer_stats.
+ 	  The statistics collection is started by writing 1 to /proc/timer_stats,
+ 	  writing 0 stops it. This feature is useful to collect information
+ 	  about timer usage patterns in kernel and userspace. This feature
+ 	  is lightweight if enabled in the kernel config but not activated
+ 	  (it defaults to deactivated on bootup and will only be activated
+ 	  if some application like powertop activates it explicitly).
+ 
++>>>>>>> f405df5de317 (refcount_t: Introduce a special purpose refcount type)
  config DEBUG_PREEMPT
  	bool "Debug preemptible kernel"
  	depends on DEBUG_KERNEL && PREEMPT && TRACE_IRQFLAGS_SUPPORT
diff --git a/include/linux/refcount.h b/include/linux/refcount.h
new file mode 100644
index 000000000000..600aadf9cca4
--- /dev/null
+++ b/include/linux/refcount.h
@@ -0,0 +1,294 @@
+#ifndef _LINUX_REFCOUNT_H
+#define _LINUX_REFCOUNT_H
+
+/*
+ * Variant of atomic_t specialized for reference counts.
+ *
+ * The interface matches the atomic_t interface (to aid in porting) but only
+ * provides the few functions one should use for reference counting.
+ *
+ * It differs in that the counter saturates at UINT_MAX and will not move once
+ * there. This avoids wrapping the counter and causing 'spurious'
+ * use-after-free issues.
+ *
+ * Memory ordering rules are slightly relaxed wrt regular atomic_t functions
+ * and provide only what is strictly required for refcounts.
+ *
+ * The increments are fully relaxed; these will not provide ordering. The
+ * rationale is that whatever is used to obtain the object we're increasing the
+ * reference count on will provide the ordering. For locked data structures,
+ * its the lock acquire, for RCU/lockless data structures its the dependent
+ * load.
+ *
+ * Do note that inc_not_zero() provides a control dependency which will order
+ * future stores against the inc, this ensures we'll never modify the object
+ * if we did not in fact acquire a reference.
+ *
+ * The decrements will provide release order, such that all the prior loads and
+ * stores will be issued before, it also provides a control dependency, which
+ * will order us against the subsequent free().
+ *
+ * The control dependency is against the load of the cmpxchg (ll/sc) that
+ * succeeded. This means the stores aren't fully ordered, but this is fine
+ * because the 1->0 transition indicates no concurrency.
+ *
+ * Note that the allocator is responsible for ordering things between free()
+ * and alloc().
+ *
+ */
+
+#include <linux/atomic.h>
+#include <linux/bug.h>
+#include <linux/mutex.h>
+#include <linux/spinlock.h>
+
+#ifdef CONFIG_DEBUG_REFCOUNT
+#define REFCOUNT_WARN(cond, str) WARN_ON(cond)
+#define __refcount_check	__must_check
+#else
+#define REFCOUNT_WARN(cond, str) (void)(cond)
+#define __refcount_check
+#endif
+
+typedef struct refcount_struct {
+	atomic_t refs;
+} refcount_t;
+
+#define REFCOUNT_INIT(n)	{ .refs = ATOMIC_INIT(n), }
+
+static inline void refcount_set(refcount_t *r, unsigned int n)
+{
+	atomic_set(&r->refs, n);
+}
+
+static inline unsigned int refcount_read(const refcount_t *r)
+{
+	return atomic_read(&r->refs);
+}
+
+static inline __refcount_check
+bool refcount_add_not_zero(unsigned int i, refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		if (!val)
+			return false;
+
+		if (unlikely(val == UINT_MAX))
+			return true;
+
+		new = val + i;
+		if (new < val)
+			new = UINT_MAX;
+		old = atomic_cmpxchg_relaxed(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	REFCOUNT_WARN(new == UINT_MAX, "refcount_t: saturated; leaking memory.\n");
+
+	return true;
+}
+
+static inline void refcount_add(unsigned int i, refcount_t *r)
+{
+	REFCOUNT_WARN(!refcount_add_not_zero(i, r), "refcount_t: addition on 0; use-after-free.\n");
+}
+
+/*
+ * Similar to atomic_inc_not_zero(), will saturate at UINT_MAX and WARN.
+ *
+ * Provides no memory ordering, it is assumed the caller has guaranteed the
+ * object memory to be stable (RCU, etc.). It does provide a control dependency
+ * and thereby orders future stores. See the comment on top.
+ */
+static inline __refcount_check
+bool refcount_inc_not_zero(refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		new = val + 1;
+
+		if (!val)
+			return false;
+
+		if (unlikely(!new))
+			return true;
+
+		old = atomic_cmpxchg_relaxed(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	REFCOUNT_WARN(new == UINT_MAX, "refcount_t: saturated; leaking memory.\n");
+
+	return true;
+}
+
+/*
+ * Similar to atomic_inc(), will saturate at UINT_MAX and WARN.
+ *
+ * Provides no memory ordering, it is assumed the caller already has a
+ * reference on the object, will WARN when this is not so.
+ */
+static inline void refcount_inc(refcount_t *r)
+{
+	REFCOUNT_WARN(!refcount_inc_not_zero(r), "refcount_t: increment on 0; use-after-free.\n");
+}
+
+/*
+ * Similar to atomic_dec_and_test(), it will WARN on underflow and fail to
+ * decrement when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before, and provides a control dependency such that free() must come after.
+ * See the comment on top.
+ */
+static inline __refcount_check
+bool refcount_sub_and_test(unsigned int i, refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		if (unlikely(val == UINT_MAX))
+			return false;
+
+		new = val - i;
+		if (new > val) {
+			REFCOUNT_WARN(new > val, "refcount_t: underflow; use-after-free.\n");
+			return false;
+		}
+
+		old = atomic_cmpxchg_release(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	return !new;
+}
+
+static inline __refcount_check
+bool refcount_dec_and_test(refcount_t *r)
+{
+	return refcount_sub_and_test(1, r);
+}
+
+/*
+ * Similar to atomic_dec(), it will WARN on underflow and fail to decrement
+ * when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before.
+ */
+static inline
+void refcount_dec(refcount_t *r)
+{
+	REFCOUNT_WARN(refcount_dec_and_test(r), "refcount_t: decrement hit 0; leaking memory.\n");
+}
+
+/*
+ * No atomic_t counterpart, it attempts a 1 -> 0 transition and returns the
+ * success thereof.
+ *
+ * Like all decrement operations, it provides release memory order and provides
+ * a control dependency.
+ *
+ * It can be used like a try-delete operator; this explicit case is provided
+ * and not cmpxchg in generic, because that would allow implementing unsafe
+ * operations.
+ */
+static inline __refcount_check
+bool refcount_dec_if_one(refcount_t *r)
+{
+	return atomic_cmpxchg_release(&r->refs, 1, 0) == 1;
+}
+
+/*
+ * No atomic_t counterpart, it decrements unless the value is 1, in which case
+ * it will return false.
+ *
+ * Was often done like: atomic_add_unless(&var, -1, 1)
+ */
+static inline __refcount_check
+bool refcount_dec_not_one(refcount_t *r)
+{
+	unsigned int old, new, val = atomic_read(&r->refs);
+
+	for (;;) {
+		if (unlikely(val == UINT_MAX))
+			return true;
+
+		if (val == 1)
+			return false;
+
+		new = val - 1;
+		if (new > val) {
+			REFCOUNT_WARN(new > val, "refcount_t: underflow; use-after-free.\n");
+			return true;
+		}
+
+		old = atomic_cmpxchg_release(&r->refs, val, new);
+		if (old == val)
+			break;
+
+		val = old;
+	}
+
+	return true;
+}
+
+/*
+ * Similar to atomic_dec_and_mutex_lock(), it will WARN on underflow and fail
+ * to decrement when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before, and provides a control dependency such that free() must come after.
+ * See the comment on top.
+ */
+static inline __refcount_check
+bool refcount_dec_and_mutex_lock(refcount_t *r, struct mutex *lock)
+{
+	if (refcount_dec_not_one(r))
+		return false;
+
+	mutex_lock(lock);
+	if (!refcount_dec_and_test(r)) {
+		mutex_unlock(lock);
+		return false;
+	}
+
+	return true;
+}
+
+/*
+ * Similar to atomic_dec_and_lock(), it will WARN on underflow and fail to
+ * decrement when saturated at UINT_MAX.
+ *
+ * Provides release memory ordering, such that prior loads and stores are done
+ * before, and provides a control dependency such that free() must come after.
+ * See the comment on top.
+ */
+static inline __refcount_check
+bool refcount_dec_and_lock(refcount_t *r, spinlock_t *lock)
+{
+	if (refcount_dec_not_one(r))
+		return false;
+
+	spin_lock(lock);
+	if (!refcount_dec_and_test(r)) {
+		spin_unlock(lock);
+		return false;
+	}
+
+	return true;
+}
+
+#endif /* _LINUX_REFCOUNT_H */
* Unmerged path lib/Kconfig.debug
