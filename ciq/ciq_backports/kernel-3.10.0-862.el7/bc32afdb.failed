ixgbe: push cls_u32 and mqprio setup_tc processing into separate functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jiri Pirko <jiri@mellanox.com>
commit bc32afdb2b01ae7652cbb829475d270f7d813618
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/bc32afdb.failed

Let __ixgbe_setup_tc be a splitter for specific setup_tc types and push out
cls_u32 and mqprio specific codes into separate functions. Also change
the return values so they are the same as in the rest of the drivers.

	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit bc32afdb2b01ae7652cbb829475d270f7d813618)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
diff --cc drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index c0b8df7cf72a,35db198199b0..000000000000
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@@ -8564,14 -8793,486 +8564,497 @@@ int ixgbe_setup_tc(struct net_device *d
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int __ixgbe_setup_tc(struct net_device *dev, u32 handle, __be16 proto,
 +			    struct tc_to_netdev *tc)
 +{
 +	/* Only support egress tc setup for now */
 +	if (tc->type != TC_SETUP_MQPRIO)
 +		return -EINVAL;
 +
 +	return ixgbe_setup_tc(dev, tc->tc);
++=======
+ static int ixgbe_delete_clsu32(struct ixgbe_adapter *adapter,
+ 			       struct tc_cls_u32_offload *cls)
+ {
+ 	u32 hdl = cls->knode.handle;
+ 	u32 uhtid = TC_U32_USERHTID(cls->knode.handle);
+ 	u32 loc = cls->knode.handle & 0xfffff;
+ 	int err = 0, i, j;
+ 	struct ixgbe_jump_table *jump = NULL;
+ 
+ 	if (loc > IXGBE_MAX_HW_ENTRIES)
+ 		return -EINVAL;
+ 
+ 	if ((uhtid != 0x800) && (uhtid >= IXGBE_MAX_LINK_HANDLE))
+ 		return -EINVAL;
+ 
+ 	/* Clear this filter in the link data it is associated with */
+ 	if (uhtid != 0x800) {
+ 		jump = adapter->jump_tables[uhtid];
+ 		if (!jump)
+ 			return -EINVAL;
+ 		if (!test_bit(loc - 1, jump->child_loc_map))
+ 			return -EINVAL;
+ 		clear_bit(loc - 1, jump->child_loc_map);
+ 	}
+ 
+ 	/* Check if the filter being deleted is a link */
+ 	for (i = 1; i < IXGBE_MAX_LINK_HANDLE; i++) {
+ 		jump = adapter->jump_tables[i];
+ 		if (jump && jump->link_hdl == hdl) {
+ 			/* Delete filters in the hardware in the child hash
+ 			 * table associated with this link
+ 			 */
+ 			for (j = 0; j < IXGBE_MAX_HW_ENTRIES; j++) {
+ 				if (!test_bit(j, jump->child_loc_map))
+ 					continue;
+ 				spin_lock(&adapter->fdir_perfect_lock);
+ 				err = ixgbe_update_ethtool_fdir_entry(adapter,
+ 								      NULL,
+ 								      j + 1);
+ 				spin_unlock(&adapter->fdir_perfect_lock);
+ 				clear_bit(j, jump->child_loc_map);
+ 			}
+ 			/* Remove resources for this link */
+ 			kfree(jump->input);
+ 			kfree(jump->mask);
+ 			kfree(jump);
+ 			adapter->jump_tables[i] = NULL;
+ 			return err;
+ 		}
+ 	}
+ 
+ 	spin_lock(&adapter->fdir_perfect_lock);
+ 	err = ixgbe_update_ethtool_fdir_entry(adapter, NULL, loc);
+ 	spin_unlock(&adapter->fdir_perfect_lock);
+ 	return err;
+ }
+ 
+ static int ixgbe_configure_clsu32_add_hnode(struct ixgbe_adapter *adapter,
+ 					    __be16 protocol,
+ 					    struct tc_cls_u32_offload *cls)
+ {
+ 	u32 uhtid = TC_U32_USERHTID(cls->hnode.handle);
+ 
+ 	if (uhtid >= IXGBE_MAX_LINK_HANDLE)
+ 		return -EINVAL;
+ 
+ 	/* This ixgbe devices do not support hash tables at the moment
+ 	 * so abort when given hash tables.
+ 	 */
+ 	if (cls->hnode.divisor > 0)
+ 		return -EINVAL;
+ 
+ 	set_bit(uhtid - 1, &adapter->tables);
+ 	return 0;
+ }
+ 
+ static int ixgbe_configure_clsu32_del_hnode(struct ixgbe_adapter *adapter,
+ 					    struct tc_cls_u32_offload *cls)
+ {
+ 	u32 uhtid = TC_U32_USERHTID(cls->hnode.handle);
+ 
+ 	if (uhtid >= IXGBE_MAX_LINK_HANDLE)
+ 		return -EINVAL;
+ 
+ 	clear_bit(uhtid - 1, &adapter->tables);
+ 	return 0;
+ }
+ 
+ #ifdef CONFIG_NET_CLS_ACT
+ struct upper_walk_data {
+ 	struct ixgbe_adapter *adapter;
+ 	u64 action;
+ 	int ifindex;
+ 	u8 queue;
+ };
+ 
+ static int get_macvlan_queue(struct net_device *upper, void *_data)
+ {
+ 	if (netif_is_macvlan(upper)) {
+ 		struct macvlan_dev *dfwd = netdev_priv(upper);
+ 		struct ixgbe_fwd_adapter *vadapter = dfwd->fwd_priv;
+ 		struct upper_walk_data *data = _data;
+ 		struct ixgbe_adapter *adapter = data->adapter;
+ 		int ifindex = data->ifindex;
+ 
+ 		if (vadapter && vadapter->netdev->ifindex == ifindex) {
+ 			data->queue = adapter->rx_ring[vadapter->rx_base_queue]->reg_idx;
+ 			data->action = data->queue;
+ 			return 1;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int handle_redirect_action(struct ixgbe_adapter *adapter, int ifindex,
+ 				  u8 *queue, u64 *action)
+ {
+ 	unsigned int num_vfs = adapter->num_vfs, vf;
+ 	struct upper_walk_data data;
+ 	struct net_device *upper;
+ 
+ 	/* redirect to a SRIOV VF */
+ 	for (vf = 0; vf < num_vfs; ++vf) {
+ 		upper = pci_get_drvdata(adapter->vfinfo[vf].vfdev);
+ 		if (upper->ifindex == ifindex) {
+ 			if (adapter->num_rx_pools > 1)
+ 				*queue = vf * 2;
+ 			else
+ 				*queue = vf * adapter->num_rx_queues_per_pool;
+ 
+ 			*action = vf + 1;
+ 			*action <<= ETHTOOL_RX_FLOW_SPEC_RING_VF_OFF;
+ 			return 0;
+ 		}
+ 	}
+ 
+ 	/* redirect to a offloaded macvlan netdev */
+ 	data.adapter = adapter;
+ 	data.ifindex = ifindex;
+ 	data.action = 0;
+ 	data.queue = 0;
+ 	if (netdev_walk_all_upper_dev_rcu(adapter->netdev,
+ 					  get_macvlan_queue, &data)) {
+ 		*action = data.action;
+ 		*queue = data.queue;
+ 
+ 		return 0;
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ static int parse_tc_actions(struct ixgbe_adapter *adapter,
+ 			    struct tcf_exts *exts, u64 *action, u8 *queue)
+ {
+ 	const struct tc_action *a;
+ 	LIST_HEAD(actions);
+ 	int err;
+ 
+ 	if (!tcf_exts_has_actions(exts))
+ 		return -EINVAL;
+ 
+ 	tcf_exts_to_list(exts, &actions);
+ 	list_for_each_entry(a, &actions, list) {
+ 
+ 		/* Drop action */
+ 		if (is_tcf_gact_shot(a)) {
+ 			*action = IXGBE_FDIR_DROP_QUEUE;
+ 			*queue = IXGBE_FDIR_DROP_QUEUE;
+ 			return 0;
+ 		}
+ 
+ 		/* Redirect to a VF or a offloaded macvlan */
+ 		if (is_tcf_mirred_egress_redirect(a)) {
+ 			int ifindex = tcf_mirred_ifindex(a);
+ 
+ 			err = handle_redirect_action(adapter, ifindex, queue,
+ 						     action);
+ 			if (err == 0)
+ 				return err;
+ 		}
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ #else
+ static int parse_tc_actions(struct ixgbe_adapter *adapter,
+ 			    struct tcf_exts *exts, u64 *action, u8 *queue)
+ {
+ 	return -EINVAL;
+ }
+ #endif /* CONFIG_NET_CLS_ACT */
+ 
+ static int ixgbe_clsu32_build_input(struct ixgbe_fdir_filter *input,
+ 				    union ixgbe_atr_input *mask,
+ 				    struct tc_cls_u32_offload *cls,
+ 				    struct ixgbe_mat_field *field_ptr,
+ 				    struct ixgbe_nexthdr *nexthdr)
+ {
+ 	int i, j, off;
+ 	__be32 val, m;
+ 	bool found_entry = false, found_jump_field = false;
+ 
+ 	for (i = 0; i < cls->knode.sel->nkeys; i++) {
+ 		off = cls->knode.sel->keys[i].off;
+ 		val = cls->knode.sel->keys[i].val;
+ 		m = cls->knode.sel->keys[i].mask;
+ 
+ 		for (j = 0; field_ptr[j].val; j++) {
+ 			if (field_ptr[j].off == off) {
+ 				field_ptr[j].val(input, mask, val, m);
+ 				input->filter.formatted.flow_type |=
+ 					field_ptr[j].type;
+ 				found_entry = true;
+ 				break;
+ 			}
+ 		}
+ 		if (nexthdr) {
+ 			if (nexthdr->off == cls->knode.sel->keys[i].off &&
+ 			    nexthdr->val == cls->knode.sel->keys[i].val &&
+ 			    nexthdr->mask == cls->knode.sel->keys[i].mask)
+ 				found_jump_field = true;
+ 			else
+ 				continue;
+ 		}
+ 	}
+ 
+ 	if (nexthdr && !found_jump_field)
+ 		return -EINVAL;
+ 
+ 	if (!found_entry)
+ 		return 0;
+ 
+ 	mask->formatted.flow_type = IXGBE_ATR_L4TYPE_IPV6_MASK |
+ 				    IXGBE_ATR_L4TYPE_MASK;
+ 
+ 	if (input->filter.formatted.flow_type == IXGBE_ATR_FLOW_TYPE_IPV4)
+ 		mask->formatted.flow_type &= IXGBE_ATR_L4TYPE_IPV6_MASK;
+ 
+ 	return 0;
+ }
+ 
+ static int ixgbe_configure_clsu32(struct ixgbe_adapter *adapter,
+ 				  __be16 protocol,
+ 				  struct tc_cls_u32_offload *cls)
+ {
+ 	u32 loc = cls->knode.handle & 0xfffff;
+ 	struct ixgbe_hw *hw = &adapter->hw;
+ 	struct ixgbe_mat_field *field_ptr;
+ 	struct ixgbe_fdir_filter *input = NULL;
+ 	union ixgbe_atr_input *mask = NULL;
+ 	struct ixgbe_jump_table *jump = NULL;
+ 	int i, err = -EINVAL;
+ 	u8 queue;
+ 	u32 uhtid, link_uhtid;
+ 
+ 	uhtid = TC_U32_USERHTID(cls->knode.handle);
+ 	link_uhtid = TC_U32_USERHTID(cls->knode.link_handle);
+ 
+ 	/* At the moment cls_u32 jumps to network layer and skips past
+ 	 * L2 headers. The canonical method to match L2 frames is to use
+ 	 * negative values. However this is error prone at best but really
+ 	 * just broken because there is no way to "know" what sort of hdr
+ 	 * is in front of the network layer. Fix cls_u32 to support L2
+ 	 * headers when needed.
+ 	 */
+ 	if (protocol != htons(ETH_P_IP))
+ 		return err;
+ 
+ 	if (loc >= ((1024 << adapter->fdir_pballoc) - 2)) {
+ 		e_err(drv, "Location out of range\n");
+ 		return err;
+ 	}
+ 
+ 	/* cls u32 is a graph starting at root node 0x800. The driver tracks
+ 	 * links and also the fields used to advance the parser across each
+ 	 * link (e.g. nexthdr/eat parameters from 'tc'). This way we can map
+ 	 * the u32 graph onto the hardware parse graph denoted in ixgbe_model.h
+ 	 * To add support for new nodes update ixgbe_model.h parse structures
+ 	 * this function _should_ be generic try not to hardcode values here.
+ 	 */
+ 	if (uhtid == 0x800) {
+ 		field_ptr = (adapter->jump_tables[0])->mat;
+ 	} else {
+ 		if (uhtid >= IXGBE_MAX_LINK_HANDLE)
+ 			return err;
+ 		if (!adapter->jump_tables[uhtid])
+ 			return err;
+ 		field_ptr = (adapter->jump_tables[uhtid])->mat;
+ 	}
+ 
+ 	if (!field_ptr)
+ 		return err;
+ 
+ 	/* At this point we know the field_ptr is valid and need to either
+ 	 * build cls_u32 link or attach filter. Because adding a link to
+ 	 * a handle that does not exist is invalid and the same for adding
+ 	 * rules to handles that don't exist.
+ 	 */
+ 
+ 	if (link_uhtid) {
+ 		struct ixgbe_nexthdr *nexthdr = ixgbe_ipv4_jumps;
+ 
+ 		if (link_uhtid >= IXGBE_MAX_LINK_HANDLE)
+ 			return err;
+ 
+ 		if (!test_bit(link_uhtid - 1, &adapter->tables))
+ 			return err;
+ 
+ 		/* Multiple filters as links to the same hash table are not
+ 		 * supported. To add a new filter with the same next header
+ 		 * but different match/jump conditions, create a new hash table
+ 		 * and link to it.
+ 		 */
+ 		if (adapter->jump_tables[link_uhtid] &&
+ 		    (adapter->jump_tables[link_uhtid])->link_hdl) {
+ 			e_err(drv, "Link filter exists for link: %x\n",
+ 			      link_uhtid);
+ 			return err;
+ 		}
+ 
+ 		for (i = 0; nexthdr[i].jump; i++) {
+ 			if (nexthdr[i].o != cls->knode.sel->offoff ||
+ 			    nexthdr[i].s != cls->knode.sel->offshift ||
+ 			    nexthdr[i].m != cls->knode.sel->offmask)
+ 				return err;
+ 
+ 			jump = kzalloc(sizeof(*jump), GFP_KERNEL);
+ 			if (!jump)
+ 				return -ENOMEM;
+ 			input = kzalloc(sizeof(*input), GFP_KERNEL);
+ 			if (!input) {
+ 				err = -ENOMEM;
+ 				goto free_jump;
+ 			}
+ 			mask = kzalloc(sizeof(*mask), GFP_KERNEL);
+ 			if (!mask) {
+ 				err = -ENOMEM;
+ 				goto free_input;
+ 			}
+ 			jump->input = input;
+ 			jump->mask = mask;
+ 			jump->link_hdl = cls->knode.handle;
+ 
+ 			err = ixgbe_clsu32_build_input(input, mask, cls,
+ 						       field_ptr, &nexthdr[i]);
+ 			if (!err) {
+ 				jump->mat = nexthdr[i].jump;
+ 				adapter->jump_tables[link_uhtid] = jump;
+ 				break;
+ 			}
+ 		}
+ 		return 0;
+ 	}
+ 
+ 	input = kzalloc(sizeof(*input), GFP_KERNEL);
+ 	if (!input)
+ 		return -ENOMEM;
+ 	mask = kzalloc(sizeof(*mask), GFP_KERNEL);
+ 	if (!mask) {
+ 		err = -ENOMEM;
+ 		goto free_input;
+ 	}
+ 
+ 	if ((uhtid != 0x800) && (adapter->jump_tables[uhtid])) {
+ 		if ((adapter->jump_tables[uhtid])->input)
+ 			memcpy(input, (adapter->jump_tables[uhtid])->input,
+ 			       sizeof(*input));
+ 		if ((adapter->jump_tables[uhtid])->mask)
+ 			memcpy(mask, (adapter->jump_tables[uhtid])->mask,
+ 			       sizeof(*mask));
+ 
+ 		/* Lookup in all child hash tables if this location is already
+ 		 * filled with a filter
+ 		 */
+ 		for (i = 1; i < IXGBE_MAX_LINK_HANDLE; i++) {
+ 			struct ixgbe_jump_table *link = adapter->jump_tables[i];
+ 
+ 			if (link && (test_bit(loc - 1, link->child_loc_map))) {
+ 				e_err(drv, "Filter exists in location: %x\n",
+ 				      loc);
+ 				err = -EINVAL;
+ 				goto err_out;
+ 			}
+ 		}
+ 	}
+ 	err = ixgbe_clsu32_build_input(input, mask, cls, field_ptr, NULL);
+ 	if (err)
+ 		goto err_out;
+ 
+ 	err = parse_tc_actions(adapter, cls->knode.exts, &input->action,
+ 			       &queue);
+ 	if (err < 0)
+ 		goto err_out;
+ 
+ 	input->sw_idx = loc;
+ 
+ 	spin_lock(&adapter->fdir_perfect_lock);
+ 
+ 	if (hlist_empty(&adapter->fdir_filter_list)) {
+ 		memcpy(&adapter->fdir_mask, mask, sizeof(*mask));
+ 		err = ixgbe_fdir_set_input_mask_82599(hw, mask);
+ 		if (err)
+ 			goto err_out_w_lock;
+ 	} else if (memcmp(&adapter->fdir_mask, mask, sizeof(*mask))) {
+ 		err = -EINVAL;
+ 		goto err_out_w_lock;
+ 	}
+ 
+ 	ixgbe_atr_compute_perfect_hash_82599(&input->filter, mask);
+ 	err = ixgbe_fdir_write_perfect_filter_82599(hw, &input->filter,
+ 						    input->sw_idx, queue);
+ 	if (!err)
+ 		ixgbe_update_ethtool_fdir_entry(adapter, input, input->sw_idx);
+ 	spin_unlock(&adapter->fdir_perfect_lock);
+ 
+ 	if ((uhtid != 0x800) && (adapter->jump_tables[uhtid]))
+ 		set_bit(loc - 1, (adapter->jump_tables[uhtid])->child_loc_map);
+ 
+ 	kfree(mask);
+ 	return err;
+ err_out_w_lock:
+ 	spin_unlock(&adapter->fdir_perfect_lock);
+ err_out:
+ 	kfree(mask);
+ free_input:
+ 	kfree(input);
+ free_jump:
+ 	kfree(jump);
+ 	return err;
+ }
+ 
+ static int ixgbe_setup_tc_cls_u32(struct net_device *dev,
+ 				  u32 handle, u32 chain_index, __be16 proto,
+ 				  struct tc_cls_u32_offload *cls_u32)
+ {
+ 	struct ixgbe_adapter *adapter = netdev_priv(dev);
+ 
+ 	if (TC_H_MAJ(handle) != TC_H_MAJ(TC_H_INGRESS) ||
+ 	    chain_index)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (cls_u32->command) {
+ 	case TC_CLSU32_NEW_KNODE:
+ 	case TC_CLSU32_REPLACE_KNODE:
+ 		return ixgbe_configure_clsu32(adapter, proto, cls_u32);
+ 	case TC_CLSU32_DELETE_KNODE:
+ 		return ixgbe_delete_clsu32(adapter, cls_u32);
+ 	case TC_CLSU32_NEW_HNODE:
+ 	case TC_CLSU32_REPLACE_HNODE:
+ 		return ixgbe_configure_clsu32_add_hnode(adapter, proto,
+ 							cls_u32);
+ 	case TC_CLSU32_DELETE_HNODE:
+ 		return ixgbe_configure_clsu32_del_hnode(adapter, cls_u32);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ static int ixgbe_setup_tc_mqprio(struct net_device *dev,
+ 				 struct tc_mqprio_qopt *mqprio)
+ {
+ 	mqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;
+ 	return ixgbe_setup_tc(dev, mqprio->num_tc);
+ }
+ 
+ static int __ixgbe_setup_tc(struct net_device *dev, enum tc_setup_type type,
+ 			    u32 handle, u32 chain_index, __be16 proto,
+ 			    struct tc_to_netdev *tc)
+ {
+ 	switch (type) {
+ 	case TC_SETUP_CLSU32:
+ 		return ixgbe_setup_tc_cls_u32(dev, handle, chain_index, proto,
+ 					      tc->cls_u32);
+ 	case TC_SETUP_MQPRIO:
+ 		return ixgbe_setup_tc_mqprio(dev, tc->mqprio);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
++>>>>>>> bc32afdb2b01 (ixgbe: push cls_u32 and mqprio setup_tc processing into separate functions)
  }
  
  #ifdef CONFIG_PCI_IOV
* Unmerged path drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
