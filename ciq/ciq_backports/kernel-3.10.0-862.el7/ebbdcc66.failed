qed: Reset IGU CAM to default on init

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Mintz, Yuval <Yuval.Mintz@cavium.com>
commit ebbdcc669c7f9d8632d358a739d814485f8917dc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ebbdcc66.failed

The IGU CAM contains an assocaition between hardware SBs
and interrupt lines, and it can be dynamically configured
to allow more interrupts in one entity over another, specifically
for Re-distibution of SBs between a PF and its child VFs.

While we don't yet use this functionality, there are other
clients that do and as such its possible the information
passed from management firmware during initialization in
regard to the possible number of SBs doesn't accurately reflect
the current HW configuration.

The following changes are going to apply to the driver init sequence:

 a. PF is going to re-configure all entries belonging to itself and
    its child VFs in IGU CAM based on the management firmware info
    regarding the number of SBs that are supposed to exist there.

 b. PF is going to stop using the SB resource [management firmware
    provided information] for anything but the initialization.
    Instead, it would use the live-time counters it maintains for
    the numbers.

	Signed-off-by: Yuval Mintz <Yuval.Mintz@cavium.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ebbdcc669c7f9d8632d358a739d814485f8917dc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/qlogic/qed/qed_dev.c
#	drivers/net/ethernet/qlogic/qed/qed_int.c
#	drivers/net/ethernet/qlogic/qed/qed_int.h
diff --cc drivers/net/ethernet/qlogic/qed/qed_dev.c
index 1b5a3d62b9f3,939e85cc63a0..000000000000
--- a/drivers/net/ethernet/qlogic/qed/qed_dev.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_dev.c
@@@ -2066,149 -2060,34 +2069,162 @@@ static void qed_hw_set_feat(struct qed_
  	if (p_hwfn->hw_info.personality == QED_PCI_ETH_ROCE ||
  	    p_hwfn->hw_info.personality == QED_PCI_ETH) {
  		/* Start by allocating VF queues, then PF's */
- 		memset(&sb_cnt_info, 0, sizeof(sb_cnt_info));
- 		qed_int_get_num_sbs(p_hwfn, &sb_cnt_info);
  		feat_num[QED_VF_L2_QUE] = min_t(u32,
  						RESC_NUM(p_hwfn, QED_L2_QUEUE),
++<<<<<<< HEAD
 +						sb_cnt_info.sb_iov_cnt);
++=======
+ 						sb_cnt.iov_cnt);
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  		feat_num[QED_PF_L2_QUE] = min_t(u32,
- 						RESC_NUM(p_hwfn, QED_SB) -
- 						non_l2_sbs,
+ 						sb_cnt.cnt - non_l2_sbs,
  						RESC_NUM(p_hwfn,
  							 QED_L2_QUEUE) -
  						FEAT_NUM(p_hwfn,
  							 QED_VF_L2_QUE));
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (p_hwfn->hw_info.personality == QED_PCI_ISCSI)
+ 		feat_num[QED_ISCSI_CQ] = min_t(u32, sb_cnt.cnt,
+ 					       RESC_NUM(p_hwfn,
+ 							QED_CMDQS_CQS));
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  	DP_VERBOSE(p_hwfn,
  		   NETIF_MSG_PROBE,
 -		   "#PF_L2_QUEUES=%d VF_L2_QUEUES=%d #ROCE_CNQ=%d ISCSI_CQ=%d #SBS=%d\n",
 +		   "#PF_L2_QUEUES=%d VF_L2_QUEUES=%d #ROCE_CNQ=%d #SBS=%d\n",
  		   (int)FEAT_NUM(p_hwfn, QED_PF_L2_QUE),
  		   (int)FEAT_NUM(p_hwfn, QED_VF_L2_QUE),
  		   (int)FEAT_NUM(p_hwfn, QED_RDMA_CNQ),
++<<<<<<< HEAD
 +		   RESC_NUM(p_hwfn, QED_SB));
++=======
+ 		   (int)FEAT_NUM(p_hwfn, QED_ISCSI_CQ),
+ 		   (int)sb_cnt.cnt);
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  }
  
 -const char *qed_hw_get_resc_name(enum qed_resources res_id)
 +static enum resource_id_enum qed_hw_get_mfw_res_id(enum qed_resources res_id)
  {
 +	enum resource_id_enum mfw_res_id = RESOURCE_NUM_INVALID;
 +
  	switch (res_id) {
 +	case QED_SB:
 +		mfw_res_id = RESOURCE_NUM_SB_E;
 +		break;
 +	case QED_L2_QUEUE:
 +		mfw_res_id = RESOURCE_NUM_L2_QUEUE_E;
 +		break;
 +	case QED_VPORT:
 +		mfw_res_id = RESOURCE_NUM_VPORT_E;
 +		break;
 +	case QED_RSS_ENG:
 +		mfw_res_id = RESOURCE_NUM_RSS_ENGINES_E;
 +		break;
 +	case QED_PQ:
 +		mfw_res_id = RESOURCE_NUM_PQ_E;
 +		break;
 +	case QED_RL:
 +		mfw_res_id = RESOURCE_NUM_RL_E;
 +		break;
 +	case QED_MAC:
 +	case QED_VLAN:
 +		/* Each VFC resource can accommodate both a MAC and a VLAN */
 +		mfw_res_id = RESOURCE_VFC_FILTER_E;
 +		break;
 +	case QED_ILT:
 +		mfw_res_id = RESOURCE_ILT_E;
 +		break;
 +	case QED_LL2_QUEUE:
 +		mfw_res_id = RESOURCE_LL2_QUEUE_E;
 +		break;
 +	case QED_RDMA_CNQ_RAM:
 +	case QED_CMDQS_CQS:
 +		/* CNQ/CMDQS are the same resource */
 +		mfw_res_id = RESOURCE_CQS_E;
 +		break;
 +	case QED_RDMA_STATS_QUEUE:
 +		mfw_res_id = RESOURCE_RDMA_STATS_QUEUE_E;
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	return mfw_res_id;
 +}
 +
 +static u32 qed_hw_get_dflt_resc_num(struct qed_hwfn *p_hwfn,
 +				    enum qed_resources res_id)
 +{
 +	u8 num_funcs = p_hwfn->num_funcs_on_engine;
 +	bool b_ah = QED_IS_AH(p_hwfn->cdev);
 +	struct qed_sb_cnt_info sb_cnt_info;
 +	u32 dflt_resc_num = 0;
 +
 +	switch (res_id) {
 +	case QED_SB:
 +		memset(&sb_cnt_info, 0, sizeof(sb_cnt_info));
 +		qed_int_get_num_sbs(p_hwfn, &sb_cnt_info);
 +		dflt_resc_num = sb_cnt_info.sb_cnt;
 +		break;
 +	case QED_L2_QUEUE:
 +		dflt_resc_num = (b_ah ? MAX_NUM_L2_QUEUES_K2
 +				      : MAX_NUM_L2_QUEUES_BB) / num_funcs;
 +		break;
 +	case QED_VPORT:
 +		dflt_resc_num = MAX_NUM_VPORTS_BB / num_funcs;
 +		dflt_resc_num = (b_ah ? MAX_NUM_VPORTS_K2
 +				      : MAX_NUM_VPORTS_BB) / num_funcs;
 +		break;
 +	case QED_RSS_ENG:
 +		dflt_resc_num = (b_ah ? ETH_RSS_ENGINE_NUM_K2
 +				      : ETH_RSS_ENGINE_NUM_BB) / num_funcs;
 +		break;
 +	case QED_PQ:
 +		/* The granularity of the PQs is 8 */
 +		dflt_resc_num = (b_ah ? MAX_QM_TX_QUEUES_K2
 +				      : MAX_QM_TX_QUEUES_BB) / num_funcs;
 +		dflt_resc_num &= ~0x7;
 +		break;
 +	case QED_RL:
 +		dflt_resc_num = MAX_QM_GLOBAL_RLS / num_funcs;
 +		break;
 +	case QED_MAC:
 +	case QED_VLAN:
 +		/* Each VFC resource can accommodate both a MAC and a VLAN */
 +		dflt_resc_num = ETH_NUM_MAC_FILTERS / num_funcs;
 +		break;
 +	case QED_ILT:
 +		dflt_resc_num = (b_ah ? PXP_NUM_ILT_RECORDS_K2
 +				      : PXP_NUM_ILT_RECORDS_BB) / num_funcs;
 +		break;
 +	case QED_LL2_QUEUE:
 +		dflt_resc_num = MAX_NUM_LL2_RX_QUEUES / num_funcs;
 +		break;
 +	case QED_RDMA_CNQ_RAM:
 +	case QED_CMDQS_CQS:
 +		/* CNQ/CMDQS are the same resource */
 +		dflt_resc_num = NUM_OF_CMDQS_CQS / num_funcs;
 +		break;
 +	case QED_RDMA_STATS_QUEUE:
 +		dflt_resc_num = (b_ah ? RDMA_NUM_STATISTIC_COUNTERS_K2
 +				      : RDMA_NUM_STATISTIC_COUNTERS_BB) /
 +				num_funcs;
 +
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	return dflt_resc_num;
 +}
 +
 +static const char *qed_hw_get_resc_name(enum qed_resources res_id)
 +{
 +	switch (res_id) {
 +	case QED_SB:
 +		return "SB";
  	case QED_L2_QUEUE:
  		return "L2_QUEUE";
  	case QED_VPORT:
@@@ -2238,12 -2121,170 +2254,173 @@@
  	}
  }
  
 -static int
 -__qed_hw_set_soft_resc_size(struct qed_hwfn *p_hwfn,
 -			    struct qed_ptt *p_ptt,
 -			    enum qed_resources res_id,
 -			    u32 resc_max_val, u32 *p_mcp_resp)
 +static int qed_hw_set_resc_info(struct qed_hwfn *p_hwfn,
 +				enum qed_resources res_id)
  {
++<<<<<<< HEAD
 +	u32 dflt_resc_num = 0, dflt_resc_start = 0, mcp_resp, mcp_param;
 +	u32 *p_resc_num, *p_resc_start;
 +	struct resource_info resc_info;
++=======
+ 	int rc;
+ 
+ 	rc = qed_mcp_set_resc_max_val(p_hwfn, p_ptt, res_id,
+ 				      resc_max_val, p_mcp_resp);
+ 	if (rc) {
+ 		DP_NOTICE(p_hwfn,
+ 			  "MFW response failure for a max value setting of resource %d [%s]\n",
+ 			  res_id, qed_hw_get_resc_name(res_id));
+ 		return rc;
+ 	}
+ 
+ 	if (*p_mcp_resp != FW_MSG_CODE_RESOURCE_ALLOC_OK)
+ 		DP_INFO(p_hwfn,
+ 			"Failed to set the max value of resource %d [%s]. mcp_resp = 0x%08x.\n",
+ 			res_id, qed_hw_get_resc_name(res_id), *p_mcp_resp);
+ 
+ 	return 0;
+ }
+ 
+ static int
+ qed_hw_set_soft_resc_size(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)
+ {
+ 	bool b_ah = QED_IS_AH(p_hwfn->cdev);
+ 	u32 resc_max_val, mcp_resp;
+ 	u8 res_id;
+ 	int rc;
+ 
+ 	for (res_id = 0; res_id < QED_MAX_RESC; res_id++) {
+ 		switch (res_id) {
+ 		case QED_LL2_QUEUE:
+ 			resc_max_val = MAX_NUM_LL2_RX_QUEUES;
+ 			break;
+ 		case QED_RDMA_CNQ_RAM:
+ 			/* No need for a case for QED_CMDQS_CQS since
+ 			 * CNQ/CMDQS are the same resource.
+ 			 */
+ 			resc_max_val = NUM_OF_CMDQS_CQS;
+ 			break;
+ 		case QED_RDMA_STATS_QUEUE:
+ 			resc_max_val = b_ah ? RDMA_NUM_STATISTIC_COUNTERS_K2
+ 			    : RDMA_NUM_STATISTIC_COUNTERS_BB;
+ 			break;
+ 		case QED_BDQ:
+ 			resc_max_val = BDQ_NUM_RESOURCES;
+ 			break;
+ 		default:
+ 			continue;
+ 		}
+ 
+ 		rc = __qed_hw_set_soft_resc_size(p_hwfn, p_ptt, res_id,
+ 						 resc_max_val, &mcp_resp);
+ 		if (rc)
+ 			return rc;
+ 
+ 		/* There's no point to continue to the next resource if the
+ 		 * command is not supported by the MFW.
+ 		 * We do continue if the command is supported but the resource
+ 		 * is unknown to the MFW. Such a resource will be later
+ 		 * configured with the default allocation values.
+ 		 */
+ 		if (mcp_resp == FW_MSG_CODE_UNSUPPORTED)
+ 			return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static
+ int qed_hw_get_dflt_resc(struct qed_hwfn *p_hwfn,
+ 			 enum qed_resources res_id,
+ 			 u32 *p_resc_num, u32 *p_resc_start)
+ {
+ 	u8 num_funcs = p_hwfn->num_funcs_on_engine;
+ 	bool b_ah = QED_IS_AH(p_hwfn->cdev);
+ 
+ 	switch (res_id) {
+ 	case QED_L2_QUEUE:
+ 		*p_resc_num = (b_ah ? MAX_NUM_L2_QUEUES_K2 :
+ 			       MAX_NUM_L2_QUEUES_BB) / num_funcs;
+ 		break;
+ 	case QED_VPORT:
+ 		*p_resc_num = (b_ah ? MAX_NUM_VPORTS_K2 :
+ 			       MAX_NUM_VPORTS_BB) / num_funcs;
+ 		break;
+ 	case QED_RSS_ENG:
+ 		*p_resc_num = (b_ah ? ETH_RSS_ENGINE_NUM_K2 :
+ 			       ETH_RSS_ENGINE_NUM_BB) / num_funcs;
+ 		break;
+ 	case QED_PQ:
+ 		*p_resc_num = (b_ah ? MAX_QM_TX_QUEUES_K2 :
+ 			       MAX_QM_TX_QUEUES_BB) / num_funcs;
+ 		*p_resc_num &= ~0x7;	/* The granularity of the PQs is 8 */
+ 		break;
+ 	case QED_RL:
+ 		*p_resc_num = MAX_QM_GLOBAL_RLS / num_funcs;
+ 		break;
+ 	case QED_MAC:
+ 	case QED_VLAN:
+ 		/* Each VFC resource can accommodate both a MAC and a VLAN */
+ 		*p_resc_num = ETH_NUM_MAC_FILTERS / num_funcs;
+ 		break;
+ 	case QED_ILT:
+ 		*p_resc_num = (b_ah ? PXP_NUM_ILT_RECORDS_K2 :
+ 			       PXP_NUM_ILT_RECORDS_BB) / num_funcs;
+ 		break;
+ 	case QED_LL2_QUEUE:
+ 		*p_resc_num = MAX_NUM_LL2_RX_QUEUES / num_funcs;
+ 		break;
+ 	case QED_RDMA_CNQ_RAM:
+ 	case QED_CMDQS_CQS:
+ 		/* CNQ/CMDQS are the same resource */
+ 		*p_resc_num = NUM_OF_CMDQS_CQS / num_funcs;
+ 		break;
+ 	case QED_RDMA_STATS_QUEUE:
+ 		*p_resc_num = (b_ah ? RDMA_NUM_STATISTIC_COUNTERS_K2 :
+ 			       RDMA_NUM_STATISTIC_COUNTERS_BB) / num_funcs;
+ 		break;
+ 	case QED_BDQ:
+ 		if (p_hwfn->hw_info.personality != QED_PCI_ISCSI &&
+ 		    p_hwfn->hw_info.personality != QED_PCI_FCOE)
+ 			*p_resc_num = 0;
+ 		else
+ 			*p_resc_num = 1;
+ 		break;
+ 	case QED_SB:
+ 		/* Since we want its value to reflect whether MFW supports
+ 		 * the new scheme, have a default of 0.
+ 		 */
+ 		*p_resc_num = 0;
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ 	switch (res_id) {
+ 	case QED_BDQ:
+ 		if (!*p_resc_num)
+ 			*p_resc_start = 0;
+ 		else if (p_hwfn->cdev->num_ports_in_engine == 4)
+ 			*p_resc_start = p_hwfn->port_id;
+ 		else if (p_hwfn->hw_info.personality == QED_PCI_ISCSI)
+ 			*p_resc_start = p_hwfn->port_id;
+ 		else if (p_hwfn->hw_info.personality == QED_PCI_FCOE)
+ 			*p_resc_start = p_hwfn->port_id + 2;
+ 		break;
+ 	default:
+ 		*p_resc_start = *p_resc_num * p_hwfn->enabled_func_idx;
+ 		break;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int __qed_hw_set_resc_info(struct qed_hwfn *p_hwfn,
+ 				  enum qed_resources res_id)
+ {
+ 	u32 dflt_resc_num = 0, dflt_resc_start = 0;
+ 	u32 mcp_resp, *p_resc_num, *p_resc_start;
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  	int rc;
  
  	p_resc_num = &RESC_NUM(p_hwfn, res_id);
@@@ -2294,15 -2324,6 +2471,18 @@@
  		goto out;
  	}
  
++<<<<<<< HEAD
 +	/* Special handling for status blocks; Would be revised in future */
 +	if (res_id == QED_SB) {
 +		resc_info.size -= 1;
 +		resc_info.offset -= p_hwfn->enabled_func_idx;
 +	}
 +
 +	*p_resc_num = resc_info.size;
 +	*p_resc_start = resc_info.offset;
 +
++=======
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  out:
  	/* PQs have to divide by 8 [that's the HW granularity].
  	 * Reduce number so it would fit.
@@@ -2341,10 -2421,12 +2521,14 @@@ static int qed_hw_get_resc(struct qed_h
  		return -EINVAL;
  	}
  
+ 	/* This will also learn the number of SBs from MFW */
+ 	if (qed_int_igu_reset_cam(p_hwfn, p_ptt))
+ 		return -EINVAL;
+ 
  	qed_hw_set_feat(p_hwfn);
  
 +	DP_VERBOSE(p_hwfn, NETIF_MSG_PROBE,
 +		   "The numbers for each resource are:\n");
  	for (res_id = 0; res_id < QED_MAX_RESC; res_id++)
  		DP_VERBOSE(p_hwfn, NETIF_MSG_PROBE, "%s = %d start = %d\n",
  			   qed_hw_get_resc_name(res_id),
diff --cc drivers/net/ethernet/qlogic/qed/qed_int.c
index 37b56d7fa1f8,719cdbfe1695..000000000000
--- a/drivers/net/ethernet/qlogic/qed/qed_int.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_int.c
@@@ -3037,54 -1832,175 +3037,193 @@@ void qed_int_igu_init_pure_rt(struct qe
  	val &= ~IGU_REG_BLOCK_CONFIGURATION_PXP_TPH_INTERFACE_EN;
  	qed_wr(p_hwfn, p_ptt, IGU_REG_BLOCK_CONFIGURATION, val);
  
 -	for (igu_sb_id = 0;
 -	     igu_sb_id < QED_MAPPING_MEMORY_SIZE(p_hwfn->cdev); igu_sb_id++) {
 -		p_block = &p_info->entry[igu_sb_id];
 -
 -		if (!(p_block->status & QED_IGU_STATUS_VALID) ||
 -		    !p_block->is_pf ||
 -		    (p_block->status & QED_IGU_STATUS_DSB))
 -			continue;
 +	DP_VERBOSE(p_hwfn, NETIF_MSG_INTR,
 +		   "IGU cleaning SBs [%d,...,%d]\n",
 +		   igu_base_sb, igu_base_sb + igu_sb_cnt - 1);
  
 -		qed_int_igu_init_pure_rt_single(p_hwfn, p_ptt, igu_sb_id,
 +	for (sb_id = igu_base_sb; sb_id < igu_base_sb + igu_sb_cnt; sb_id++)
 +		qed_int_igu_init_pure_rt_single(p_hwfn, p_ptt, sb_id,
  						p_hwfn->hw_info.opaque_fid,
  						b_set);
 -	}
  
 -	if (b_slowpath)
 -		qed_int_igu_init_pure_rt_single(p_hwfn, p_ptt,
 -						p_info->igu_dsb_id,
 -						p_hwfn->hw_info.opaque_fid,
 -						b_set);
 +	if (!b_slowpath)
 +		return;
 +
 +	sb_id = p_hwfn->hw_info.p_igu_info->igu_dsb_id;
 +	DP_VERBOSE(p_hwfn, NETIF_MSG_INTR,
 +		   "IGU cleaning slowpath SB [%d]\n", sb_id);
 +	qed_int_igu_init_pure_rt_single(p_hwfn, p_ptt, sb_id,
 +					p_hwfn->hw_info.opaque_fid, b_set);
  }
  
++<<<<<<< HEAD
 +static u32 qed_int_igu_read_cam_block(struct qed_hwfn *p_hwfn,
 +				      struct qed_ptt *p_ptt, u16 sb_id)
++=======
+ int qed_int_igu_reset_cam(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)
+ {
+ 	struct qed_igu_info *p_info = p_hwfn->hw_info.p_igu_info;
+ 	struct qed_igu_block *p_block;
+ 	int pf_sbs, vf_sbs;
+ 	u16 igu_sb_id;
+ 	u32 val, rval;
+ 
+ 	if (!RESC_NUM(p_hwfn, QED_SB)) {
+ 		p_info->b_allow_pf_vf_change = false;
+ 	} else {
+ 		/* Use the numbers the MFW have provided -
+ 		 * don't forget MFW accounts for the default SB as well.
+ 		 */
+ 		p_info->b_allow_pf_vf_change = true;
+ 
+ 		if (p_info->usage.cnt != RESC_NUM(p_hwfn, QED_SB) - 1) {
+ 			DP_INFO(p_hwfn,
+ 				"MFW notifies of 0x%04x PF SBs; IGU indicates of only 0x%04x\n",
+ 				RESC_NUM(p_hwfn, QED_SB) - 1,
+ 				p_info->usage.cnt);
+ 			p_info->usage.cnt = RESC_NUM(p_hwfn, QED_SB) - 1;
+ 		}
+ 
+ 		if (IS_PF_SRIOV(p_hwfn)) {
+ 			u16 vfs = p_hwfn->cdev->p_iov_info->total_vfs;
+ 
+ 			if (vfs != p_info->usage.iov_cnt)
+ 				DP_VERBOSE(p_hwfn,
+ 					   NETIF_MSG_INTR,
+ 					   "0x%04x VF SBs in IGU CAM != PCI configuration 0x%04x\n",
+ 					   p_info->usage.iov_cnt, vfs);
+ 
+ 			/* At this point we know how many SBs we have totally
+ 			 * in IGU + number of PF SBs. So we can validate that
+ 			 * we'd have sufficient for VF.
+ 			 */
+ 			if (vfs > p_info->usage.free_cnt +
+ 			    p_info->usage.free_cnt_iov - p_info->usage.cnt) {
+ 				DP_NOTICE(p_hwfn,
+ 					  "Not enough SBs for VFs - 0x%04x SBs, from which %04x PFs and %04x are required\n",
+ 					  p_info->usage.free_cnt +
+ 					  p_info->usage.free_cnt_iov,
+ 					  p_info->usage.cnt, vfs);
+ 				return -EINVAL;
+ 			}
+ 
+ 			/* Currently cap the number of VFs SBs by the
+ 			 * number of VFs.
+ 			 */
+ 			p_info->usage.iov_cnt = vfs;
+ 		}
+ 	}
+ 
+ 	/* Mark all SBs as free, now in the right PF/VFs division */
+ 	p_info->usage.free_cnt = p_info->usage.cnt;
+ 	p_info->usage.free_cnt_iov = p_info->usage.iov_cnt;
+ 	p_info->usage.orig = p_info->usage.cnt;
+ 	p_info->usage.iov_orig = p_info->usage.iov_cnt;
+ 
+ 	/* We now proceed to re-configure the IGU cam to reflect the initial
+ 	 * configuration. We can start with the Default SB.
+ 	 */
+ 	pf_sbs = p_info->usage.cnt;
+ 	vf_sbs = p_info->usage.iov_cnt;
+ 
+ 	for (igu_sb_id = p_info->igu_dsb_id;
+ 	     igu_sb_id < QED_MAPPING_MEMORY_SIZE(p_hwfn->cdev); igu_sb_id++) {
+ 		p_block = &p_info->entry[igu_sb_id];
+ 		val = 0;
+ 
+ 		if (!(p_block->status & QED_IGU_STATUS_VALID))
+ 			continue;
+ 
+ 		if (p_block->status & QED_IGU_STATUS_DSB) {
+ 			p_block->function_id = p_hwfn->rel_pf_id;
+ 			p_block->is_pf = 1;
+ 			p_block->vector_number = 0;
+ 			p_block->status = QED_IGU_STATUS_VALID |
+ 					  QED_IGU_STATUS_PF |
+ 					  QED_IGU_STATUS_DSB;
+ 		} else if (pf_sbs) {
+ 			pf_sbs--;
+ 			p_block->function_id = p_hwfn->rel_pf_id;
+ 			p_block->is_pf = 1;
+ 			p_block->vector_number = p_info->usage.cnt - pf_sbs;
+ 			p_block->status = QED_IGU_STATUS_VALID |
+ 					  QED_IGU_STATUS_PF |
+ 					  QED_IGU_STATUS_FREE;
+ 		} else if (vf_sbs) {
+ 			p_block->function_id =
+ 			    p_hwfn->cdev->p_iov_info->first_vf_in_pf +
+ 			    p_info->usage.iov_cnt - vf_sbs;
+ 			p_block->is_pf = 0;
+ 			p_block->vector_number = 0;
+ 			p_block->status = QED_IGU_STATUS_VALID |
+ 					  QED_IGU_STATUS_FREE;
+ 			vf_sbs--;
+ 		} else {
+ 			p_block->function_id = 0;
+ 			p_block->is_pf = 0;
+ 			p_block->vector_number = 0;
+ 		}
+ 
+ 		SET_FIELD(val, IGU_MAPPING_LINE_FUNCTION_NUMBER,
+ 			  p_block->function_id);
+ 		SET_FIELD(val, IGU_MAPPING_LINE_PF_VALID, p_block->is_pf);
+ 		SET_FIELD(val, IGU_MAPPING_LINE_VECTOR_NUMBER,
+ 			  p_block->vector_number);
+ 
+ 		/* VF entries would be enabled when VF is initializaed */
+ 		SET_FIELD(val, IGU_MAPPING_LINE_VALID, p_block->is_pf);
+ 
+ 		rval = qed_rd(p_hwfn, p_ptt,
+ 			      IGU_REG_MAPPING_MEMORY + sizeof(u32) * igu_sb_id);
+ 
+ 		if (rval != val) {
+ 			qed_wr(p_hwfn, p_ptt,
+ 			       IGU_REG_MAPPING_MEMORY +
+ 			       sizeof(u32) * igu_sb_id, val);
+ 
+ 			DP_VERBOSE(p_hwfn,
+ 				   NETIF_MSG_INTR,
+ 				   "IGU reset: [SB 0x%04x] func_id = %d is_pf = %d vector_num = 0x%x [%08x -> %08x]\n",
+ 				   igu_sb_id,
+ 				   p_block->function_id,
+ 				   p_block->is_pf,
+ 				   p_block->vector_number, rval, val);
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void qed_int_igu_read_cam_block(struct qed_hwfn *p_hwfn,
+ 				       struct qed_ptt *p_ptt, u16 igu_sb_id)
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  {
  	u32 val = qed_rd(p_hwfn, p_ptt,
 -			 IGU_REG_MAPPING_MEMORY + sizeof(u32) * igu_sb_id);
 +			 IGU_REG_MAPPING_MEMORY + sizeof(u32) * sb_id);
  	struct qed_igu_block *p_block;
  
 -	p_block = &p_hwfn->hw_info.p_igu_info->entry[igu_sb_id];
 +	p_block = &p_hwfn->hw_info.p_igu_info->igu_map.igu_blocks[sb_id];
 +
 +	/* stop scanning when hit first invalid PF entry */
 +	if (!GET_FIELD(val, IGU_MAPPING_LINE_VALID) &&
 +	    GET_FIELD(val, IGU_MAPPING_LINE_PF_VALID))
 +		goto out;
  
  	/* Fill the block information */
 -	p_block->function_id = GET_FIELD(val, IGU_MAPPING_LINE_FUNCTION_NUMBER);
 -	p_block->is_pf = GET_FIELD(val, IGU_MAPPING_LINE_PF_VALID);
 -	p_block->vector_number = GET_FIELD(val, IGU_MAPPING_LINE_VECTOR_NUMBER);
 -	p_block->igu_sb_id = igu_sb_id;
 +	p_block->status		= QED_IGU_STATUS_VALID;
 +	p_block->function_id	= GET_FIELD(val,
 +					    IGU_MAPPING_LINE_FUNCTION_NUMBER);
 +	p_block->is_pf		= GET_FIELD(val, IGU_MAPPING_LINE_PF_VALID);
 +	p_block->vector_number	= GET_FIELD(val,
 +					    IGU_MAPPING_LINE_VECTOR_NUMBER);
 +
 +	DP_VERBOSE(p_hwfn, NETIF_MSG_INTR,
 +		   "IGU_BLOCK: [SB 0x%04x, Value in CAM 0x%08x] func_id = %d is_pf = %d vector_num = 0x%x\n",
 +		   sb_id, val, p_block->function_id,
 +		   p_block->is_pf, p_block->vector_number);
 +
 +out:
 +	return val;
  }
  
  int qed_int_igu_read_cam(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)
@@@ -3114,63 -2027,50 +3253,85 @@@
  		max_vf	= p_iov->first_vf_in_pf + p_iov->total_vfs;
  	}
  
 -	for (igu_sb_id = 0;
 -	     igu_sb_id < QED_MAPPING_MEMORY_SIZE(p_hwfn->cdev); igu_sb_id++) {
 -		/* Read current entry; Notice it might not belong to this PF */
 -		qed_int_igu_read_cam_block(p_hwfn, p_ptt, igu_sb_id);
 -		p_block = &p_igu_info->entry[igu_sb_id];
 -
 -		if ((p_block->is_pf) &&
 -		    (p_block->function_id == p_hwfn->rel_pf_id)) {
 -			p_block->status = QED_IGU_STATUS_PF |
 -					  QED_IGU_STATUS_VALID |
 -					  QED_IGU_STATUS_FREE;
 +	for (sb_id = 0; sb_id < QED_MAPPING_MEMORY_SIZE(p_hwfn->cdev);
 +	     sb_id++) {
 +		blk = &p_igu_info->igu_map.igu_blocks[sb_id];
  
 -			if (p_igu_info->igu_dsb_id != QED_SB_INVALID_IDX)
 -				p_igu_info->usage.cnt++;
 -		} else if (!(p_block->is_pf) &&
 -			   (p_block->function_id >= min_vf) &&
 -			   (p_block->function_id < max_vf)) {
 -			/* Available for VFs of this PF */
 -			p_block->status = QED_IGU_STATUS_VALID |
 -					  QED_IGU_STATUS_FREE;
 +		val	= qed_int_igu_read_cam_block(p_hwfn, p_ptt, sb_id);
  
 -			if (p_igu_info->igu_dsb_id != QED_SB_INVALID_IDX)
 -				p_igu_info->usage.iov_cnt++;
 -		}
 +		/* stop scanning when hit first invalid PF entry */
 +		if (!GET_FIELD(val, IGU_MAPPING_LINE_VALID) &&
 +		    GET_FIELD(val, IGU_MAPPING_LINE_PF_VALID))
 +			break;
  
 +		if (blk->is_pf) {
 +			if (blk->function_id == p_hwfn->rel_pf_id) {
 +				blk->status |= QED_IGU_STATUS_PF;
 +
++<<<<<<< HEAD
 +				if (blk->vector_number == 0) {
 +					if (p_igu_info->igu_dsb_id == 0xffff)
 +						p_igu_info->igu_dsb_id = sb_id;
 +				} else {
 +					if (p_igu_info->igu_base_sb ==
 +					    0xffff) {
 +						p_igu_info->igu_base_sb = sb_id;
 +					} else if (prev_sb_id != sb_id - 1) {
 +						DP_NOTICE(p_hwfn->cdev,
 +							  "consecutive igu vectors for HWFN %x broken",
 +							  p_hwfn->rel_pf_id);
 +						break;
 +					}
 +					prev_sb_id = sb_id;
 +					/* we don't count the default */
 +					(p_igu_info->igu_sb_cnt)++;
 +				}
 +			}
 +		} else {
 +			if ((blk->function_id >= min_vf) &&
 +			    (blk->function_id < max_vf)) {
 +				/* Available for VFs of this PF */
 +				if (p_igu_info->igu_base_sb_iov == 0xffff) {
 +					p_igu_info->igu_base_sb_iov = sb_id;
 +				} else if (last_iov_sb_id != sb_id - 1) {
 +					if (!val) {
 +						DP_VERBOSE(p_hwfn->cdev,
 +							   NETIF_MSG_INTR,
 +							   "First uninitialized IGU CAM entry at index 0x%04x\n",
 +							   sb_id);
 +					} else {
 +						DP_NOTICE(p_hwfn->cdev,
 +							  "Consecutive igu vectors for HWFN %x vfs is broken [jumps from %04x to %04x]\n",
 +							  p_hwfn->rel_pf_id,
 +							  last_iov_sb_id,
 +							  sb_id); }
 +					break;
 +				}
 +				blk->status |= QED_IGU_STATUS_FREE;
 +				p_hwfn->hw_info.p_igu_info->free_blks++;
 +				last_iov_sb_id = sb_id;
 +			}
++=======
+ 		/* Mark the First entry belonging to the PF or its VFs
+ 		 * as the default SB [we'll reset IGU prior to first usage].
+ 		 */
+ 		if ((p_block->status & QED_IGU_STATUS_VALID) &&
+ 		    (p_igu_info->igu_dsb_id == QED_SB_INVALID_IDX)) {
+ 			p_igu_info->igu_dsb_id = igu_sb_id;
+ 			p_block->status |= QED_IGU_STATUS_DSB;
+ 		}
+ 
+ 		/* limit number of prints by having each PF print only its
+ 		 * entries with the exception of PF0 which would print
+ 		 * everything.
+ 		 */
+ 		if ((p_block->status & QED_IGU_STATUS_VALID) ||
+ 		    (p_hwfn->abs_pf_id == 0)) {
+ 			DP_VERBOSE(p_hwfn, NETIF_MSG_INTR,
+ 				   "IGU_BLOCK: [SB 0x%04x] func_id = %d is_pf = %d vector_num = 0x%x\n",
+ 				   igu_sb_id, p_block->function_id,
+ 				   p_block->is_pf, p_block->vector_number);
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  		}
  	}
  
@@@ -3221,6 -2081,15 +3382,18 @@@
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
++=======
+ 	/* All non default SB are considered free at this point */
+ 	p_igu_info->usage.free_cnt = p_igu_info->usage.cnt;
+ 	p_igu_info->usage.free_cnt_iov = p_igu_info->usage.iov_cnt;
+ 
+ 	DP_VERBOSE(p_hwfn, NETIF_MSG_INTR,
+ 		   "igu_dsb_id=0x%x, num Free SBs - PF: %04x VF: %04x [might change after resource allocation]\n",
+ 		   p_igu_info->igu_dsb_id,
+ 		   p_igu_info->usage.cnt, p_igu_info->usage.iov_cnt);
+ 
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  	return 0;
  }
  
diff --cc drivers/net/ethernet/qlogic/qed/qed_int.h
index a8e48e14efef,5199634ed630..000000000000
--- a/drivers/net/ethernet/qlogic/qed/qed_int.h
+++ b/drivers/net/ethernet/qlogic/qed/qed_int.h
@@@ -215,16 -219,43 +215,56 @@@ struct qed_igu_map 
  };
  
  struct qed_igu_info {
++<<<<<<< HEAD
 +	struct qed_igu_map	igu_map;
 +	u16			igu_dsb_id;
 +	u16			igu_base_sb;
 +	u16			igu_base_sb_iov;
 +	u16			igu_sb_cnt;
 +	u16			igu_sb_cnt_iov;
 +	u16			free_blks;
 +};
 +
 +/* TODO Names of function may change... */
++=======
+ 	struct qed_igu_block entry[MAX_TOT_SB_PER_PATH];
+ 	u16 igu_dsb_id;
+ 
+ 	struct qed_sb_cnt_info usage;
+ 
+ 	bool b_allow_pf_vf_change;
+ };
+ 
+ /**
+  * @brief - Make sure the IGU CAM reflects the resources provided by MFW
+  *
+  * @param p_hwfn
+  * @param p_ptt
+  */
+ int qed_int_igu_reset_cam(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt);
+ 
+ /**
+  * @brief Translate the weakly-defined client sb-id into an IGU sb-id
+  *
+  * @param p_hwfn
+  * @param sb_id - user provided sb_id
+  *
+  * @return an index inside IGU CAM where the SB resides
+  */
+ u16 qed_get_igu_sb_id(struct qed_hwfn *p_hwfn, u16 sb_id);
+ 
+ /**
+  * @brief return a pointer to an unused valid SB
+  *
+  * @param p_hwfn
+  * @param b_is_pf - true iff we want a SB belonging to a PF
+  *
+  * @return point to an igu_block, NULL if none is available
+  */
+ struct qed_igu_block *qed_get_igu_free_sb(struct qed_hwfn *p_hwfn,
+ 					  bool b_is_pf);
+ 
++>>>>>>> ebbdcc669c7f (qed: Reset IGU CAM to default on init)
  void qed_int_igu_init_pure_rt(struct qed_hwfn *p_hwfn,
  			      struct qed_ptt *p_ptt,
  			      bool b_set,
* Unmerged path drivers/net/ethernet/qlogic/qed/qed_dev.c
* Unmerged path drivers/net/ethernet/qlogic/qed/qed_int.c
* Unmerged path drivers/net/ethernet/qlogic/qed/qed_int.h
