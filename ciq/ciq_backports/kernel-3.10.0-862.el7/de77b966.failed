net: introduce __skb_put_[zero, data, u8]

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [net] introduce __skb_put_[zero, data, u8] (Jiri Benc) [1497085]
Rebuild_FUZZ: 93.51%
commit-author yuan linyu <Linyu.Yuan@alcatel-sbell.com.cn>
commit de77b966ce8adcb4c58d50e2f087320d5479812a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/de77b966.failed

follow Johannes Berg, semantic patch file as below,
@@
identifier p, p2;
expression len;
expression skb;
type t, t2;
@@
(
-p = __skb_put(skb, len);
+p = __skb_put_zero(skb, len);
|
-p = (t)__skb_put(skb, len);
+p = __skb_put_zero(skb, len);
)
... when != p
(
p2 = (t2)p;
-memset(p2, 0, len);
|
-memset(p, 0, len);
)

@@
identifier p;
expression len;
expression skb;
type t;
@@
(
-t p = __skb_put(skb, len);
+t p = __skb_put_zero(skb, len);
)
... when != p
(
-memset(p, 0, len);
)

@@
type t, t2;
identifier p, p2;
expression skb;
@@
t *p;
...
(
-p = __skb_put(skb, sizeof(t));
+p = __skb_put_zero(skb, sizeof(t));
|
-p = (t *)__skb_put(skb, sizeof(t));
+p = __skb_put_zero(skb, sizeof(t));
)
... when != p
(
p2 = (t2)p;
-memset(p2, 0, sizeof(*p));
|
-memset(p, 0, sizeof(*p));
)

@@
expression skb, len;
@@
-memset(__skb_put(skb, len), 0, len);
+__skb_put_zero(skb, len);

@@
expression skb, len, data;
@@
-memcpy(__skb_put(skb, len), data, len);
+__skb_put_data(skb, data, len);

@@
expression SKB, C, S;
typedef u8;
identifier fn = {__skb_put};
fresh identifier fn2 = fn ## "_u8";
@@
- *(u8 *)fn(SKB, S) = C;
+ fn2(SKB, C);

	Signed-off-by: yuan linyu <Linyu.Yuan@alcatel-sbell.com.cn>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit de77b966ce8adcb4c58d50e2f087320d5479812a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/infiniband/hw/cxgb4/cm.c
#	drivers/infiniband/hw/cxgb4/cq.c
#	drivers/infiniband/hw/cxgb4/mem.c
#	drivers/infiniband/hw/cxgb4/qp.c
#	drivers/isdn/gigaset/asyncdata.c
#	drivers/isdn/gigaset/isocdata.c
#	drivers/net/ethernet/chelsio/cxgb3/cxgb3_main.c
#	drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
#	drivers/target/iscsi/cxgbit/cxgbit_cm.c
#	include/linux/skbuff.h
#	lib/test_bpf.c
diff --cc drivers/crypto/chelsio/chcr_algo.c
index bda117371c9f,b75b8beed68f..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -535,8 -604,7 +535,12 @@@ static struct sk_buf
  	if (!skb)
  		return ERR_PTR(-ENOMEM);
  	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
++<<<<<<< HEAD
 +	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
 +	memset(chcr_req, 0, transhdr_len);
++=======
+ 	chcr_req = __skb_put_zero(skb, transhdr_len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	chcr_req->sec_cpl.op_ivinsrtofst =
  		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2, 1);
  
@@@ -811,8 -880,7 +815,12 @@@ static struct sk_buff *create_hash_wr(s
  		return skb;
  
  	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
++<<<<<<< HEAD
 +	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
 +	memset(chcr_req, 0, transhdr_len);
++=======
+ 	chcr_req = __skb_put_zero(skb, transhdr_len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  
  	chcr_req->sec_cpl.op_ivinsrtofst =
  		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2, 0);
@@@ -1260,6 -1328,1222 +1268,1225 @@@ static void chcr_hmac_cra_exit(struct c
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int chcr_copy_assoc(struct aead_request *req,
+ 				struct chcr_aead_ctx *ctx)
+ {
+ 	SKCIPHER_REQUEST_ON_STACK(skreq, ctx->null);
+ 
+ 	skcipher_request_set_tfm(skreq, ctx->null);
+ 	skcipher_request_set_callback(skreq, aead_request_flags(req),
+ 			NULL, NULL);
+ 	skcipher_request_set_crypt(skreq, req->src, req->dst, req->assoclen,
+ 			NULL);
+ 
+ 	return crypto_skcipher_encrypt(skreq);
+ }
+ static int chcr_aead_need_fallback(struct aead_request *req, int src_nent,
+ 				   int aadmax, int wrlen,
+ 				   unsigned short op_type)
+ {
+ 	unsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));
+ 
+ 	if (((req->cryptlen - (op_type ? authsize : 0)) == 0) ||
+ 	    (req->assoclen > aadmax) ||
+ 	    (src_nent > MAX_SKB_FRAGS) ||
+ 	    (wrlen > MAX_WR_SIZE))
+ 		return 1;
+ 	return 0;
+ }
+ 
+ static int chcr_aead_fallback(struct aead_request *req, unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 
+ 	aead_request_set_tfm(subreq, aeadctx->sw_cipher);
+ 	aead_request_set_callback(subreq, req->base.flags,
+ 				  req->base.complete, req->base.data);
+ 	 aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 				 req->iv);
+ 	 aead_request_set_ad(subreq, req->assoclen);
+ 	return op_type ? crypto_aead_decrypt(subreq) :
+ 		crypto_aead_encrypt(subreq);
+ }
+ 
+ static struct sk_buff *create_authenc_wr(struct aead_request *req,
+ 					 unsigned short qid,
+ 					 int size,
+ 					 unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = crypto_aead_ivsize(tfm), dst_size = 0;
+ 	unsigned int   kctx_len = 0;
+ 	unsigned short stop_offset = 0;
+ 	unsigned int  assoclen = req->assoclen;
+ 	unsigned int  authsize = crypto_aead_authsize(tfm);
+ 	int err = -EINVAL, src_nent;
+ 	int null = 0;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 	if (aeadctx->enckey_len == 0 || (req->cryptlen == 0))
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		err = chcr_copy_assoc(req, aeadctx);
+ 		if (err)
+ 			return ERR_PTR(err);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {
+ 		null = 1;
+ 		assoclen = 0;
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("AUTHENC:Invalid Destination sg entries\n");
+ 		goto err;
+ 	}
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)
+ 		- sizeof(chcr_req->key_ctx);
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_AUTH_SG,
+ 			T6_MAX_AAD_SIZE,
+ 			transhdr_len + (sgl_len(src_nent + MIN_AUTH_SG) * 8),
+ 				op_type)) {
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb)
+ 		goto err;
+ 
+ 	/* LLD is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	/* Write WR */
+ 	chcr_req = __skb_put_zero(skb, transhdr_len);
+ 
+ 	stop_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 
+ 	/*
+ 	 * Input order	is AAD,IV and Payload. where IV should be included as
+ 	 * the part of authdata. All other fields should be filled according
+ 	 * to the hardware spec
+ 	 */
+ 	chcr_req->sec_cpl.op_ivinsrtofst =
+ 		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,
+ 				       (ivsize ? (assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1,
+ 					(stop_offset & 0x1F0) >> 4);
+ 	chcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(
+ 					stop_offset & 0xF,
+ 					null ? 0 : assoclen + ivsize + 1,
+ 					stop_offset, stop_offset);
+ 	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_CBC,
+ 					actx->auth_mode, aeadctx->hmac_ctrl,
+ 					ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					 0, 1, dst_size);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	if (op_type == CHCR_ENCRYPT_OP)
+ 		memcpy(chcr_req->key_ctx.key, aeadctx->key,
+ 		       aeadctx->enckey_len);
+ 	else
+ 		memcpy(chcr_req->key_ctx.key, actx->dec_rrkey,
+ 		       aeadctx->enckey_len);
+ 
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<
+ 					4), actx->h_iopad, kctx_len -
+ 				(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	sg_param.align = 0;
+ 	if (map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl, reqctx->dst,
+ 				  &sg_param))
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 
+ 	if (assoclen) {
+ 		/* AAD buffer in */
+ 		write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 
+ 	}
+ 	write_buffer_to_skb(skb, &frags, req->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	create_wreq(ctx, chcr_req, req, skb, kctx_len, size, 1,
+ 		   sizeof(struct cpl_rx_phys_dsgl) + dst_size);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 
+ 	return skb;
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(-EINVAL);
+ }
+ 
+ static int set_msg_len(u8 *block, unsigned int msglen, int csize)
+ {
+ 	__be32 data;
+ 
+ 	memset(block, 0, csize);
+ 	block += csize;
+ 
+ 	if (csize >= 4)
+ 		csize = 4;
+ 	else if (msglen > (unsigned int)(1 << (8 * csize)))
+ 		return -EOVERFLOW;
+ 
+ 	data = cpu_to_be32(msglen);
+ 	memcpy(block - csize, (u8 *)&data + 4 - csize, csize);
+ 
+ 	return 0;
+ }
+ 
+ static void generate_b0(struct aead_request *req,
+ 			struct chcr_aead_ctx *aeadctx,
+ 			unsigned short op_type)
+ {
+ 	unsigned int l, lp, m;
+ 	int rc;
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	u8 *b0 = reqctx->scratch_pad;
+ 
+ 	m = crypto_aead_authsize(aead);
+ 
+ 	memcpy(b0, reqctx->iv, 16);
+ 
+ 	lp = b0[0];
+ 	l = lp + 1;
+ 
+ 	/* set m, bits 3-5 */
+ 	*b0 |= (8 * ((m - 2) / 2));
+ 
+ 	/* set adata, bit 6, if associated data is used */
+ 	if (req->assoclen)
+ 		*b0 |= 64;
+ 	rc = set_msg_len(b0 + 16 - l,
+ 			 (op_type == CHCR_DECRYPT_OP) ?
+ 			 req->cryptlen - m : req->cryptlen, l);
+ }
+ 
+ static inline int crypto_ccm_check_iv(const u8 *iv)
+ {
+ 	/* 2 <= L <= 8, so 1 <= L' <= 7. */
+ 	if (iv[0] < 1 || iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int ccm_format_packet(struct aead_request *req,
+ 			     struct chcr_aead_ctx *aeadctx,
+ 			     unsigned int sub_type,
+ 			     unsigned short op_type)
+ {
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int rc = 0;
+ 
+ 	if (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		reqctx->iv[0] = 3;
+ 		memcpy(reqctx->iv + 1, &aeadctx->salt[0], 3);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 		memset(reqctx->iv + 12, 0, 4);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen - 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 16);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen);
+ 	}
+ 	generate_b0(req, aeadctx, op_type);
+ 	/* zero the ctr value */
+ 	memset(reqctx->iv + 15 - reqctx->iv[0], 0, reqctx->iv[0] + 1);
+ 	return rc;
+ }
+ 
+ static void fill_sec_cpl_for_aead(struct cpl_tx_sec_pdu *sec_cpl,
+ 				  unsigned int dst_size,
+ 				  struct aead_request *req,
+ 				  unsigned short op_type,
+ 					  struct chcr_context *chcrctx)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int cipher_mode = CHCR_SCMD_CIPHER_MODE_AES_CCM;
+ 	unsigned int mac_mode = CHCR_SCMD_AUTH_MODE_CBCMAC;
+ 	unsigned int c_id = chcrctx->dev->rx_channel_id;
+ 	unsigned int ccm_xtra;
+ 	unsigned char tag_offset = 0, auth_offset = 0;
+ 	unsigned int assoclen;
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 		assoclen = req->assoclen - 8;
+ 	else
+ 		assoclen = req->assoclen;
+ 	ccm_xtra = CCM_B0_SIZE +
+ 		((assoclen) ? CCM_AAD_FIELD_SIZE : 0);
+ 
+ 	auth_offset = req->cryptlen ?
+ 		(assoclen + ivsize + 1 + ccm_xtra) : 0;
+ 	if (op_type == CHCR_DECRYPT_OP) {
+ 		if (crypto_aead_authsize(tfm) != req->cryptlen)
+ 			tag_offset = crypto_aead_authsize(tfm);
+ 		else
+ 			auth_offset = 0;
+ 	}
+ 
+ 
+ 	sec_cpl->op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(c_id,
+ 					 2, (ivsize ?  (assoclen + 1) :  0) +
+ 					 ccm_xtra);
+ 	sec_cpl->pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen + ccm_xtra);
+ 	/* For CCM there wil be b0 always. So AAD start will be 1 always */
+ 	sec_cpl->aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					1, assoclen + ccm_xtra, assoclen
+ 					+ ivsize + 1 + ccm_xtra, 0);
+ 
+ 	sec_cpl->cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(0,
+ 					auth_offset, tag_offset,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 :
+ 					crypto_aead_authsize(tfm));
+ 	sec_cpl->seqno_numivs =  FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 : 1,
+ 					cipher_mode, mac_mode,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 
+ 	sec_cpl->ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1, 0,
+ 					1, dst_size);
+ }
+ 
+ int aead_ccm_validate_input(unsigned short op_type,
+ 			    struct aead_request *req,
+ 			    struct chcr_aead_ctx *aeadctx,
+ 			    unsigned int sub_type)
+ {
+ 	if (sub_type != CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		if (crypto_ccm_check_iv(req->iv)) {
+ 			pr_err("CCM: IV check fails\n");
+ 			return -EINVAL;
+ 		}
+ 	} else {
+ 		if (req->assoclen != 16 && req->assoclen != 20) {
+ 			pr_err("RFC4309: Invalid AAD length %d\n",
+ 			       req->assoclen);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	if (aeadctx->enckey_len == 0) {
+ 		pr_err("CCM: Encryption key not set\n");
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ unsigned int fill_aead_req_fields(struct sk_buff *skb,
+ 				  struct aead_request *req,
+ 				  struct scatterlist *src,
+ 				  unsigned int ivsize,
+ 				  struct chcr_aead_ctx *aeadctx)
+ {
+ 	unsigned int frags = 0;
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	/* b0 and aad length(if available) */
+ 
+ 	write_buffer_to_skb(skb, &frags, reqctx->scratch_pad, CCM_B0_SIZE +
+ 				(req->assoclen ?  CCM_AAD_FIELD_SIZE : 0));
+ 	if (req->assoclen) {
+ 		if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 			write_sg_to_skb(skb, &frags, req->src,
+ 					req->assoclen - 8);
+ 		else
+ 			write_sg_to_skb(skb, &frags, req->src, req->assoclen);
+ 	}
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	if (req->cryptlen)
+ 		write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 
+ 	return frags;
+ }
+ 
+ static struct sk_buff *create_aead_ccm_wr(struct aead_request *req,
+ 					  unsigned short qid,
+ 					  int size,
+ 					  unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len, ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len;
+ 	unsigned int sub_type;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int err = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	sub_type = get_aead_subtype(tfm);
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		err = chcr_copy_assoc(req, aeadctx);
+ 		if (err) {
+ 			pr_err("AAD copy to destination buffer fails\n");
+ 			return ERR_PTR(err);
+ 		}
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("CCM:Invalid Destination sg entries\n");
+ 		goto err;
+ 	}
+ 
+ 
+ 	if (aead_ccm_validate_input(op_type, req, aeadctx, sub_type))
+ 		goto err;
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) * 2;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_CCM_SG,
+ 			    T6_MAX_AAD_SIZE - 18,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_CCM_SG) * 8),
+ 			    op_type)) {
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)),  flags);
+ 
+ 	if (!skb)
+ 		goto err;
+ 
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = __skb_put_zero(skb, transhdr_len);
+ 
+ 	fill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, op_type, ctx);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 					16), aeadctx->key, aeadctx->enckey_len);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	if (ccm_format_packet(req, aeadctx, sub_type, op_type))
+ 		goto dstmap_fail;
+ 
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	sg_param.align = 0;
+ 	if (map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl, reqctx->dst,
+ 				  &sg_param))
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	frags = fill_aead_req_fields(skb, req, src, ivsize, aeadctx);
+ 	create_wreq(ctx, chcr_req, req, skb, kctx_len, 0, 1,
+ 		    sizeof(struct cpl_rx_phys_dsgl) + dst_size);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ dstmap_fail:
+ 	kfree_skb(skb);
+ 	skb = NULL;
+ err:
+ 	return ERR_PTR(-EINVAL);
+ }
+ 
+ static struct sk_buff *create_gcm_wr(struct aead_request *req,
+ 				     unsigned short qid,
+ 				     int size,
+ 				     unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len;
+ 	unsigned char tag_offset = 0;
+ 	unsigned int crypt_len = 0;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int err = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 	/* validate key size */
+ 	if (aeadctx->enckey_len == 0)
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 	if (req->src != req->dst) {
+ 		err = chcr_copy_assoc(req, aeadctx);
+ 		if (err)
+ 			return	ERR_PTR(err);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 
+ 	if (!req->cryptlen)
+ 		/* null-payload is not supported in the hardware.
+ 		 * software is sending block size
+ 		 */
+ 		crypt_len = AES_BLOCK_SIZE;
+ 	else
+ 		crypt_len = req->cryptlen;
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("GCM:Invalid Destination sg entries\n");
+ 		goto err;
+ 	}
+ 
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_GCM_SG,
+ 			    T6_MAX_AAD_SIZE,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_GCM_SG) * 8),
+ 			    op_type)) {
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb)
+ 		goto err;
+ 
+ 	/* NIC driver is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = __skb_put_zero(skb, transhdr_len);
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)
+ 		req->assoclen -= 8;
+ 
+ 	tag_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 	chcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(
+ 					ctx->dev->rx_channel_id, 2, (ivsize ?
+ 					(req->assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen =
+ 		htonl(req->assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					req->assoclen ? 1 : 0, req->assoclen,
+ 					req->assoclen + ivsize + 1, 0);
+ 		chcr_req->sec_cpl.cipherstop_lo_authinsert =
+ 			FILL_SEC_CPL_AUTHINSERT(0, req->assoclen + ivsize + 1,
+ 						tag_offset, tag_offset);
+ 		chcr_req->sec_cpl.seqno_numivs =
+ 			FILL_SEC_CPL_SCMD0_SEQNO(op_type, (op_type ==
+ 					CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_GCM,
+ 					CHCR_SCMD_AUTH_MODE_GHASH,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					0, 1, dst_size);
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 				16), GCM_CTX(aeadctx)->ghash_h, AEAD_H_SIZE);
+ 
+ 	/* prepare a 16 byte iv */
+ 	/* S   A   L  T |  IV | 0x00000001 */
+ 	if (get_aead_subtype(tfm) ==
+ 	    CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) {
+ 		memcpy(reqctx->iv, aeadctx->salt, 4);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 12);
+ 	}
+ 	*((unsigned int *)(reqctx->iv + 12)) = htonl(0x01);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	sg_param.align = 0;
+ 	if (map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl, reqctx->dst,
+ 				  &sg_param))
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 
+ 	write_sg_to_skb(skb, &frags, req->src, req->assoclen);
+ 
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	create_wreq(ctx, chcr_req, req, skb, kctx_len, size, 1,
+ 			sizeof(struct cpl_rx_phys_dsgl) + dst_size);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ 
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ 	skb = NULL;
+ err:
+ 	return skb;
+ }
+ 
+ 
+ 
+ static int chcr_aead_cra_init(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_alg *alg = crypto_aead_alg(tfm);
+ 
+ 	aeadctx->sw_cipher = crypto_alloc_aead(alg->base.cra_name, 0,
+ 					       CRYPTO_ALG_NEED_FALLBACK);
+ 	if  (IS_ERR(aeadctx->sw_cipher))
+ 		return PTR_ERR(aeadctx->sw_cipher);
+ 	crypto_aead_set_reqsize(tfm, max(sizeof(struct chcr_aead_reqctx),
+ 				 sizeof(struct aead_request) +
+ 				 crypto_aead_reqsize(aeadctx->sw_cipher)));
+ 	aeadctx->null = crypto_get_default_null_skcipher();
+ 	if (IS_ERR(aeadctx->null))
+ 		return PTR_ERR(aeadctx->null);
+ 	return chcr_device_init(ctx);
+ }
+ 
+ static void chcr_aead_cra_exit(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 
+ 	crypto_put_default_null_skcipher();
+ 	crypto_free_aead(aeadctx->sw_cipher);
+ }
+ 
+ static int chcr_authenc_null_setauthsize(struct crypto_aead *tfm,
+ 					unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NOP;
+ 	aeadctx->mayverify = VERIFY_HW;
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ static int chcr_authenc_setauthsize(struct crypto_aead *tfm,
+ 				    unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	u32 maxauth = crypto_aead_maxauthsize(tfm);
+ 
+ 	/*SHA1 authsize in ipsec is 12 instead of 10 i.e maxauthsize / 2 is not
+ 	 * true for sha1. authsize == 12 condition should be before
+ 	 * authsize == (maxauth >> 1)
+ 	 */
+ 	if (authsize == ICV_4) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_6) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_10) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_12) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_14) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == (maxauth >> 1)) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == maxauth) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ 
+ static int chcr_gcm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_13:
+ 	case ICV_15:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 		break;
+ 	default:
+ 
+ 		  crypto_tfm_set_flags((struct crypto_tfm *) tfm,
+ 			CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_4106_4309_setauthsize(struct crypto_aead *tfm,
+ 					  unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_setauthsize(struct crypto_aead *tfm,
+ 				unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_6:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_10:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_common_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	unsigned char ck_size, mk_size;
+ 	int key_ctx_size = 0;
+ 
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4)  * 2;
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		mk_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, mk_size, 0, 0,
+ 						key_ctx_size >> 4);
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 
+ 	return 0;
+ }
+ 
+ static int chcr_aead_ccm_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	int error;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &
+ 			      CRYPTO_TFM_REQ_MASK);
+ 	error = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (error)
+ 		return error;
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_aead_rfc4309_setkey(struct crypto_aead *aead, const u8 *key,
+ 				    unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	 struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 
+ 	if (keylen < 3) {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	keylen -= 3;
+ 	memcpy(aeadctx->salt, key + keylen, 3);
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_gcm_setkey(struct crypto_aead *aead, const u8 *key,
+ 			   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_gcm_ctx *gctx = GCM_CTX(aeadctx);
+ 	struct crypto_cipher *cipher;
+ 	unsigned int ck_size;
+ 	int ret = 0, key_ctx_size = 0;
+ 
+ 	aeadctx->enckey_len = 0;
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (get_aead_subtype(aead) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&
+ 	    keylen > 3) {
+ 		keylen -= 4;  /* nonce/salt is present in the last 4 bytes */
+ 		memcpy(aeadctx->salt, key + keylen, 4);
+ 	}
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		pr_err("GCM: Invalid key length %d\n", keylen);
+ 		ret = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,
+ 						CHCR_KEYCTX_MAC_KEY_SIZE_128,
+ 						0, 0,
+ 						key_ctx_size >> 4);
+ 	/* Calculate the H = CIPH(K, 0 repeated 16 times).
+ 	 * It will go in key context
+ 	 */
+ 	cipher = crypto_alloc_cipher("aes-generic", 0, 0);
+ 	if (IS_ERR(cipher)) {
+ 		aeadctx->enckey_len = 0;
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	ret = crypto_cipher_setkey(cipher, key, keylen);
+ 	if (ret) {
+ 		aeadctx->enckey_len = 0;
+ 		goto out1;
+ 	}
+ 	memset(gctx->ghash_h, 0, AEAD_H_SIZE);
+ 	crypto_cipher_encrypt_one(cipher, gctx->ghash_h, gctx->ghash_h);
+ 
+ out1:
+ 	crypto_free_cipher(cipher);
+ out:
+ 	return ret;
+ }
+ 
+ static int chcr_authenc_setkey(struct crypto_aead *authenc, const u8 *key,
+ 				   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	/* it contains auth and cipher key both*/
+ 	struct crypto_authenc_keys keys;
+ 	unsigned int bs;
+ 	unsigned int max_authsize = crypto_aead_alg(authenc)->maxauthsize;
+ 	int err = 0, i, key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 	unsigned char pad[CHCR_HASH_MAX_BLOCK_SIZE_128] = { 0 };
+ 	struct crypto_shash *base_hash = ERR_PTR(-EINVAL);
+ 	struct algo_param param;
+ 	int align;
+ 	u8 *o_ptr = NULL;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 
+ 	if (get_alg_config(&param, max_authsize)) {
+ 		pr_err("chcr : Unsupported digest size\n");
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 
+ 	/* Copy only encryption key. We use authkey to generate h(ipad) and
+ 	 * h(opad) so authkey is not needed again. authkeylen size have the
+ 	 * size of the hash digest size.
+ 	 */
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 			    aeadctx->enckey_len << 3);
+ 
+ 	base_hash  = chcr_alloc_shash(max_authsize);
+ 	if (IS_ERR(base_hash)) {
+ 		pr_err("chcr : Base driver cannot be loaded\n");
+ 		aeadctx->enckey_len = 0;
+ 		return -EINVAL;
+ 	}
+ 	{
+ 		SHASH_DESC_ON_STACK(shash, base_hash);
+ 		shash->tfm = base_hash;
+ 		shash->flags = crypto_shash_get_flags(base_hash);
+ 		bs = crypto_shash_blocksize(base_hash);
+ 		align = KEYCTX_ALIGN_PAD(max_authsize);
+ 		o_ptr =  actx->h_iopad + param.result_size + align;
+ 
+ 		if (keys.authkeylen > bs) {
+ 			err = crypto_shash_digest(shash, keys.authkey,
+ 						  keys.authkeylen,
+ 						  o_ptr);
+ 			if (err) {
+ 				pr_err("chcr : Base driver cannot be loaded\n");
+ 				goto out;
+ 			}
+ 			keys.authkeylen = max_authsize;
+ 		} else
+ 			memcpy(o_ptr, keys.authkey, keys.authkeylen);
+ 
+ 		/* Compute the ipad-digest*/
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= IPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, actx->h_iopad,
+ 					      max_authsize))
+ 			goto out;
+ 		/* Compute the opad-digest */
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= OPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, o_ptr, max_authsize))
+ 			goto out;
+ 
+ 		/* convert the ipad and opad digest to network order */
+ 		chcr_change_order(actx->h_iopad, param.result_size);
+ 		chcr_change_order(o_ptr, param.result_size);
+ 		key_ctx_len = sizeof(struct _key_ctx) +
+ 			((DIV_ROUND_UP(keys.enckeylen, 16)) << 4) +
+ 			(param.result_size + align) * 2;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, param.mk_size,
+ 						0, 1, key_ctx_len >> 4);
+ 		actx->auth_mode = param.auth_mode;
+ 		chcr_free_shash(base_hash);
+ 
+ 		return 0;
+ 	}
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	if (!IS_ERR(base_hash))
+ 		chcr_free_shash(base_hash);
+ 	return -EINVAL;
+ }
+ 
+ static int chcr_aead_digest_null_setkey(struct crypto_aead *authenc,
+ 					const u8 *key, unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct crypto_authenc_keys keys;
+ 	int err;
+ 	/* it contains auth and cipher key both*/
+ 	int key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 				    aeadctx->enckey_len << 3);
+ 	key_ctx_len =  sizeof(struct _key_ctx)
+ 		+ ((DIV_ROUND_UP(keys.enckeylen, 16)) << 4);
+ 
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY, 0,
+ 						0, key_ctx_len >> 4);
+ 	actx->auth_mode = CHCR_SCMD_AUTH_MODE_NOP;
+ 	return 0;
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	return -EINVAL;
+ }
+ static int chcr_aead_encrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 
+ 	reqctx->verify = VERIFY_HW;
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int size;
+ 
+ 	if (aeadctx->mayverify == VERIFY_SW) {
+ 		size = crypto_aead_maxauthsize(tfm);
+ 		reqctx->verify = VERIFY_SW;
+ 	} else {
+ 		size = 0;
+ 		reqctx->verify = VERIFY_HW;
+ 	}
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_op(struct aead_request *req,
+ 			  unsigned short op_type,
+ 			  int size,
+ 			  create_wr_t create_wr_fn)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx;
+ 	struct sk_buff *skb;
+ 
+ 	if (!ctx->dev) {
+ 		pr_err("chcr : %s : No crypto device.\n", __func__);
+ 		return -ENXIO;
+ 	}
+ 	u_ctx = ULD_CTX(ctx);
+ 	if (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 				   ctx->tx_qidx)) {
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
+ 			return -EBUSY;
+ 	}
+ 
+ 	/* Form a WR from req */
+ 	skb = create_wr_fn(req, u_ctx->lldi.rxq_ids[ctx->rx_qidx], size,
+ 			   op_type);
+ 
+ 	if (IS_ERR(skb) || !skb)
+ 		return PTR_ERR(skb);
+ 
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
+ 	chcr_send_wr(skb);
+ 	return -EINPROGRESS;
+ }
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  static struct chcr_alg_template driver_algs[] = {
  	/* AES-CBC */
  	{
diff --cc drivers/infiniband/hw/cxgb4/cm.c
index c5a62399c28c,e49b34c3b136..000000000000
--- a/drivers/infiniband/hw/cxgb4/cm.c
+++ b/drivers/infiniband/hw/cxgb4/cm.c
@@@ -1905,8 -1900,7 +1905,12 @@@ static int send_fw_act_open_req(struct 
  	int win;
  
  	skb = get_skb(NULL, sizeof(*req), GFP_KERNEL);
++<<<<<<< HEAD
 +	req = (struct fw_ofld_connection_wr *)__skb_put(skb, sizeof(*req));
 +	memset(req, 0, sizeof(*req));
++=======
+ 	req = __skb_put_zero(skb, sizeof(*req));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	req->op_compl = htonl(WR_OP_V(FW_OFLD_CONNECTION_WR));
  	req->len16_pkd = htonl(FW_WR_LEN16_V(DIV_ROUND_UP(sizeof(*req), 16)));
  	req->le.filter = cpu_to_be32(cxgb4_select_ntuple(
@@@ -3806,8 -3802,7 +3810,12 @@@ static void send_fw_pass_open_req(struc
  	req_skb = alloc_skb(sizeof(struct fw_ofld_connection_wr), GFP_KERNEL);
  	if (!req_skb)
  		return;
++<<<<<<< HEAD
 +	req = (struct fw_ofld_connection_wr *)__skb_put(req_skb, sizeof(*req));
 +	memset(req, 0, sizeof(*req));
++=======
+ 	req = __skb_put_zero(req_skb, sizeof(*req));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	req->op_compl = htonl(WR_OP_V(FW_OFLD_CONNECTION_WR) | FW_WR_COMPL_F);
  	req->len16_pkd = htonl(FW_WR_LEN16_V(DIV_ROUND_UP(sizeof(*req), 16)));
  	req->le.version_cpl = htonl(FW_OFLD_CONNECTION_WR_CPL_F);
diff --cc drivers/infiniband/hw/cxgb4/cq.c
index eb5e910786df,e16fcaf6b5a3..000000000000
--- a/drivers/infiniband/hw/cxgb4/cq.c
+++ b/drivers/infiniband/hw/cxgb4/cq.c
@@@ -44,8 -44,7 +44,12 @@@ static int destroy_cq(struct c4iw_rdev 
  	wr_len = sizeof *res_wr + sizeof *res;
  	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
  
++<<<<<<< HEAD
 +	res_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);
 +	memset(res_wr, 0, wr_len);
++=======
+ 	res_wr = __skb_put_zero(skb, wr_len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	res_wr->op_nres = cpu_to_be32(
  			FW_WR_OP_V(FW_RI_RES_WR) |
  			FW_RI_RES_WR_NRES_V(1) |
@@@ -114,8 -113,7 +118,12 @@@ static int create_cq(struct c4iw_rdev *
  	}
  	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
  
++<<<<<<< HEAD
 +	res_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);
 +	memset(res_wr, 0, wr_len);
++=======
+ 	res_wr = __skb_put_zero(skb, wr_len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	res_wr->op_nres = cpu_to_be32(
  			FW_WR_OP_V(FW_RI_RES_WR) |
  			FW_RI_RES_WR_NRES_V(1) |
diff --cc drivers/infiniband/hw/cxgb4/mem.c
index 39cac5db937e,5332f06b99ba..000000000000
--- a/drivers/infiniband/hw/cxgb4/mem.c
+++ b/drivers/infiniband/hw/cxgb4/mem.c
@@@ -74,8 -81,7 +74,12 @@@ static int _c4iw_write_mem_dma_aligned(
  	}
  	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
  
++<<<<<<< HEAD
 +	req = (struct ulp_mem_io *)__skb_put(skb, wr_len);
 +	memset(req, 0, wr_len);
++=======
+ 	req = __skb_put_zero(skb, wr_len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	INIT_ULPTX_WR(req, wr_len, 0, 0);
  	req->wr.wr_hi = cpu_to_be32(FW_WR_OP_V(FW_ULPTX_WR) |
  			(wait ? FW_WR_COMPL_F : 0));
@@@ -135,8 -141,7 +139,12 @@@ static int _c4iw_write_mem_inline(struc
  		}
  		set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
  
++<<<<<<< HEAD
 +		req = (struct ulp_mem_io *)__skb_put(skb, wr_len);
 +		memset(req, 0, wr_len);
++=======
+ 		req = __skb_put_zero(skb, wr_len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  		INIT_ULPTX_WR(req, wr_len, 0, 0);
  
  		if (i == (num_wqe-1)) {
diff --cc drivers/infiniband/hw/cxgb4/qp.c
index 50b8ed44737d,bfc77596acbe..000000000000
--- a/drivers/infiniband/hw/cxgb4/qp.c
+++ b/drivers/infiniband/hw/cxgb4/qp.c
@@@ -293,8 -293,7 +293,12 @@@ static int create_qp(struct c4iw_rdev *
  	}
  	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
  
++<<<<<<< HEAD
 +	res_wr = (struct fw_ri_res_wr *)__skb_put(skb, wr_len);
 +	memset(res_wr, 0, wr_len);
++=======
+ 	res_wr = __skb_put_zero(skb, wr_len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	res_wr->op_nres = cpu_to_be32(
  			FW_WR_OP_V(FW_RI_RES_WR) |
  			FW_RI_RES_WR_NRES_V(2) |
diff --cc drivers/isdn/gigaset/asyncdata.c
index c90dca5abeac,bc208557f783..000000000000
--- a/drivers/isdn/gigaset/asyncdata.c
+++ b/drivers/isdn/gigaset/asyncdata.c
@@@ -264,7 -264,7 +264,11 @@@ byte_stuff
  				/* skip remainder of packet */
  				bcs->rx_skb = skb = NULL;
  			} else {
++<<<<<<< HEAD
 +				*__skb_put(skb, 1) = c;
++=======
+ 				__skb_put_u8(skb, c);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  				fcs = crc_ccitt_byte(fcs, c);
  			}
  		}
@@@ -315,7 -315,7 +319,11 @@@ static unsigned iraw_loop(unsigned numb
  
  		/* regular data byte: append to current skb */
  		inputstate |= INS_have_data;
++<<<<<<< HEAD
 +		*__skb_put(skb, 1) = bitrev8(c);
++=======
+ 		__skb_put_u8(skb, bitrev8(c));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	}
  
  	/* pass data up */
diff --cc drivers/isdn/gigaset/isocdata.c
index bc29f1d52a2f,97e00118ccfe..000000000000
--- a/drivers/isdn/gigaset/isocdata.c
+++ b/drivers/isdn/gigaset/isocdata.c
@@@ -511,7 -511,7 +511,11 @@@ static inline void hdlc_putbyte(unsigne
  		bcs->rx_skb = NULL;
  		return;
  	}
++<<<<<<< HEAD
 +	*__skb_put(bcs->rx_skb, 1) = c;
++=======
+ 	__skb_put_u8(bcs->rx_skb, c);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  }
  
  /* hdlc_flush
diff --cc drivers/net/ethernet/chelsio/cxgb3/cxgb3_main.c
index fe449775ccc0,0bc6a4ffce30..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb3/cxgb3_main.c
+++ b/drivers/net/ethernet/chelsio/cxgb3/cxgb3_main.c
@@@ -471,8 -471,7 +471,12 @@@ static int init_tp_parity(struct adapte
  		if (!skb)
  			goto alloc_skb_fail;
  
++<<<<<<< HEAD
 +		req = (struct cpl_smt_write_req *)__skb_put(skb, sizeof(*req));
 +		memset(req, 0, sizeof(*req));
++=======
+ 		req = __skb_put_zero(skb, sizeof(*req));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  		req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
  		OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SMT_WRITE_REQ, i));
  		req->mtu_idx = NMTUS - 1;
@@@ -495,8 -494,7 +499,12 @@@
  		if (!skb)
  			goto alloc_skb_fail;
  
++<<<<<<< HEAD
 +		req = (struct cpl_l2t_write_req *)__skb_put(skb, sizeof(*req));
 +		memset(req, 0, sizeof(*req));
++=======
+ 		req = __skb_put_zero(skb, sizeof(*req));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  		req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
  		OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_L2T_WRITE_REQ, i));
  		req->params = htonl(V_L2T_W_IDX(i));
@@@ -518,8 -516,7 +526,12 @@@
  		if (!skb)
  			goto alloc_skb_fail;
  
++<<<<<<< HEAD
 +		req = (struct cpl_rte_write_req *)__skb_put(skb, sizeof(*req));
 +		memset(req, 0, sizeof(*req));
++=======
+ 		req = __skb_put_zero(skb, sizeof(*req));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  		req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
  		OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_RTE_WRITE_REQ, i));
  		req->l2t_idx = htonl(V_L2T_W_IDX(i));
@@@ -538,8 -535,7 +550,12 @@@
  	if (!skb)
  		goto alloc_skb_fail;
  
++<<<<<<< HEAD
 +	greq = (struct cpl_set_tcb_field *)__skb_put(skb, sizeof(*greq));
 +	memset(greq, 0, sizeof(*greq));
++=======
+ 	greq = __skb_put_zero(skb, sizeof(*greq));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  	greq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
  	OPCODE_TID(greq) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, 0));
  	greq->mask = cpu_to_be64(1);
diff --cc drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
index 10736738ff30,45b5853ca2f1..000000000000
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
@@@ -231,8 -231,7 +231,12 @@@ int set_filter_wr(struct adapter *adapt
  		}
  	}
  
++<<<<<<< HEAD
 +	fwr = (struct fw_filter_wr *)__skb_put(skb, sizeof(*fwr));
 +	memset(fwr, 0, sizeof(*fwr));
++=======
+ 	fwr = __skb_put_zero(skb, sizeof(*fwr));
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  
  	/* It would be nice to put most of the following in t4_hw.c but most
  	 * of the work is translating the cxgbtool ch_filter_specification
diff --cc drivers/target/iscsi/cxgbit/cxgbit_cm.c
index b875b68da953,e583dd8a418b..000000000000
--- a/drivers/target/iscsi/cxgbit/cxgbit_cm.c
+++ b/drivers/target/iscsi/cxgbit/cxgbit_cm.c
@@@ -1085,8 -1085,7 +1085,12 @@@ cxgbit_pass_accept_rpl(struct cxgbit_so
  		return;
  	}
  
++<<<<<<< HEAD
 +	rpl5 = (struct cpl_t5_pass_accept_rpl *)__skb_put(skb, len);
 +	memset(rpl5, 0, len);
++=======
+ 	rpl5 = __skb_put_zero(skb, len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  
  	INIT_TP_WR(rpl5, csk->tid);
  	OPCODE_TID(rpl5) = cpu_to_be32(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,
@@@ -1367,8 -1366,7 +1371,12 @@@ u32 cxgbit_send_tx_flowc_wr(struct cxgb
  	flowclen16 = cxgbit_tx_flowc_wr_credits(csk, &nparams, &flowclen);
  
  	skb = __skb_dequeue(&csk->skbq);
++<<<<<<< HEAD
 +	flowc = (struct fw_flowc_wr *)__skb_put(skb, flowclen);
 +	memset(flowc, 0, flowclen);
++=======
+ 	flowc = __skb_put_zero(skb, flowclen);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  
  	flowc->op_to_nparams = cpu_to_be32(FW_WR_OP_V(FW_FLOWC_WR) |
  					   FW_FLOWC_WR_NPARAMS_V(nparams));
@@@ -1439,8 -1437,7 +1447,12 @@@ int cxgbit_setup_conn_digest(struct cxg
  		return -ENOMEM;
  
  	/*  set up ulp submode */
++<<<<<<< HEAD
 +	req = (struct cpl_set_tcb_field *)__skb_put(skb, len);
 +	memset(req, 0, len);
++=======
+ 	req = __skb_put_zero(skb, len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  
  	INIT_TP_WR(req, csk->tid);
  	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, csk->tid));
@@@ -1476,8 -1473,7 +1488,12 @@@ int cxgbit_setup_conn_pgidx(struct cxgb
  	if (!skb)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	req = (struct cpl_set_tcb_field *)__skb_put(skb, len);
 +	memset(req, 0, len);
++=======
+ 	req = __skb_put_zero(skb, len);
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  
  	INIT_TP_WR(req, csk->tid);
  	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, csk->tid));
diff --cc include/linux/skbuff.h
index 28aefdf34562,a17e235639ae..000000000000
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@@ -1928,8 -1904,54 +1928,59 @@@ static inline unsigned char *__skb_put(
  	return tmp;
  }
  
++<<<<<<< HEAD
 +unsigned char *skb_push(struct sk_buff *skb, unsigned int len);
 +static inline unsigned char *__skb_push(struct sk_buff *skb, unsigned int len)
++=======
+ static inline void *__skb_put_zero(struct sk_buff *skb, unsigned int len)
+ {
+ 	void *tmp = __skb_put(skb, len);
+ 
+ 	memset(tmp, 0, len);
+ 	return tmp;
+ }
+ 
+ static inline void *__skb_put_data(struct sk_buff *skb, const void *data,
+ 				   unsigned int len)
+ {
+ 	void *tmp = __skb_put(skb, len);
+ 
+ 	memcpy(tmp, data, len);
+ 	return tmp;
+ }
+ 
+ static inline void __skb_put_u8(struct sk_buff *skb, u8 val)
+ {
+ 	*(u8 *)__skb_put(skb, 1) = val;
+ }
+ 
+ static inline void *skb_put_zero(struct sk_buff *skb, unsigned int len)
+ {
+ 	void *tmp = skb_put(skb, len);
+ 
+ 	memset(tmp, 0, len);
+ 
+ 	return tmp;
+ }
+ 
+ static inline void *skb_put_data(struct sk_buff *skb, const void *data,
+ 				 unsigned int len)
+ {
+ 	void *tmp = skb_put(skb, len);
+ 
+ 	memcpy(tmp, data, len);
+ 
+ 	return tmp;
+ }
+ 
+ static inline void skb_put_u8(struct sk_buff *skb, u8 val)
+ {
+ 	*(u8 *)skb_put(skb, 1) = val;
+ }
+ 
+ void *skb_push(struct sk_buff *skb, unsigned int len);
+ static inline void *__skb_push(struct sk_buff *skb, unsigned int len)
++>>>>>>> de77b966ce8a (net: introduce __skb_put_[zero, data, u8])
  {
  	skb->data -= len;
  	skb->len  += len;
* Unmerged path lib/test_bpf.c
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/infiniband/hw/cxgb4/cm.c
* Unmerged path drivers/infiniband/hw/cxgb4/cq.c
* Unmerged path drivers/infiniband/hw/cxgb4/mem.c
* Unmerged path drivers/infiniband/hw/cxgb4/qp.c
* Unmerged path drivers/isdn/gigaset/asyncdata.c
* Unmerged path drivers/isdn/gigaset/isocdata.c
* Unmerged path drivers/net/ethernet/chelsio/cxgb3/cxgb3_main.c
diff --git a/drivers/net/ethernet/chelsio/cxgb3/sge.c b/drivers/net/ethernet/chelsio/cxgb3/sge.c
index 90f2afb9c4bb..0835175ce75c 100644
--- a/drivers/net/ethernet/chelsio/cxgb3/sge.c
+++ b/drivers/net/ethernet/chelsio/cxgb3/sge.c
@@ -2281,7 +2281,7 @@ static int process_responses(struct adapter *adap, struct sge_qset *qs,
 			if (!skb)
 				goto no_mem;
 
-			memcpy(__skb_put(skb, AN_PKT_SIZE), r, AN_PKT_SIZE);
+			__skb_put_data(skb, r, AN_PKT_SIZE);
 			skb->data[0] = CPL_ASYNC_NOTIF;
 			rss_hi = htonl(CPL_ASYNC_NOTIF << 24);
 			q->async_notif++;
* Unmerged path drivers/net/ethernet/chelsio/cxgb4/cxgb4_filter.c
diff --git a/drivers/net/usb/int51x1.c b/drivers/net/usb/int51x1.c
index 4ff70b22c6ee..2c17badb4593 100644
--- a/drivers/net/usb/int51x1.c
+++ b/drivers/net/usb/int51x1.c
@@ -110,7 +110,7 @@ static struct sk_buff *int51x1_tx_fixup(struct usbnet *dev,
 	*len = cpu_to_le16(pack_len);
 
 	if(need_tail)
-		memset(__skb_put(skb, need_tail), 0, need_tail);
+		__skb_put_zero(skb, need_tail);
 
 	return skb;
 }
diff --git a/drivers/staging/octeon/ethernet-tx.c b/drivers/staging/octeon/ethernet-tx.c
index 5631dd9f8201..02084692ead5 100644
--- a/drivers/staging/octeon/ethernet-tx.c
+++ b/drivers/staging/octeon/ethernet-tx.c
@@ -247,8 +247,7 @@ int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev)
 				int add_bytes = 64 - skb->len;
 				if ((skb_tail_pointer(skb) + add_bytes) <=
 				    skb_end_pointer(skb))
-					memset(__skb_put(skb, add_bytes), 0,
-					       add_bytes);
+					__skb_put_zero(skb, add_bytes);
 			}
 		}
 	}
* Unmerged path drivers/target/iscsi/cxgbit/cxgbit_cm.c
* Unmerged path include/linux/skbuff.h
* Unmerged path lib/test_bpf.c
diff --git a/net/802/garp.c b/net/802/garp.c
index b38ee6dcba45..fa4e33dad2de 100644
--- a/net/802/garp.c
+++ b/net/802/garp.c
@@ -232,7 +232,7 @@ static int garp_pdu_append_end_mark(struct garp_applicant *app)
 {
 	if (skb_tailroom(app->pdu) < sizeof(u8))
 		return -1;
-	*(u8 *)__skb_put(app->pdu, sizeof(u8)) = GARP_END_MARK;
+	__skb_put_u8(app->pdu, GARP_END_MARK);
 	return 0;
 }
 
diff --git a/net/bluetooth/bnep/core.c b/net/bluetooth/bnep/core.c
index 468e4b303918..227f60664b91 100644
--- a/net/bluetooth/bnep/core.c
+++ b/net/bluetooth/bnep/core.c
@@ -374,25 +374,22 @@ static int bnep_rx_frame(struct bnep_session *s, struct sk_buff *skb)
 	/* Decompress header and construct ether frame */
 	switch (type & BNEP_TYPE_MASK) {
 	case BNEP_COMPRESSED:
-		memcpy(__skb_put(nskb, ETH_HLEN), &s->eh, ETH_HLEN);
+		__skb_put_data(nskb, &s->eh, ETH_HLEN);
 		break;
 
 	case BNEP_COMPRESSED_SRC_ONLY:
-		memcpy(__skb_put(nskb, ETH_ALEN), s->eh.h_dest, ETH_ALEN);
-		memcpy(__skb_put(nskb, ETH_ALEN), skb_mac_header(skb), ETH_ALEN);
+		__skb_put_data(nskb, s->eh.h_dest, ETH_ALEN);
+		__skb_put_data(nskb, skb_mac_header(skb), ETH_ALEN);
 		put_unaligned(s->eh.h_proto, (__be16 *) __skb_put(nskb, 2));
 		break;
 
 	case BNEP_COMPRESSED_DST_ONLY:
-		memcpy(__skb_put(nskb, ETH_ALEN), skb_mac_header(skb),
-								ETH_ALEN);
-		memcpy(__skb_put(nskb, ETH_ALEN + 2), s->eh.h_source,
-								ETH_ALEN + 2);
+		__skb_put_data(nskb, skb_mac_header(skb), ETH_ALEN);
+		__skb_put_data(nskb, s->eh.h_source, ETH_ALEN + 2);
 		break;
 
 	case BNEP_GENERAL:
-		memcpy(__skb_put(nskb, ETH_ALEN * 2), skb_mac_header(skb),
-								ETH_ALEN * 2);
+		__skb_put_data(nskb, skb_mac_header(skb), ETH_ALEN * 2);
 		put_unaligned(s->eh.h_proto, (__be16 *) __skb_put(nskb, 2));
 		break;
 	}
diff --git a/net/bluetooth/bnep/netdev.c b/net/bluetooth/bnep/netdev.c
index f4fcb4a9d5c1..2ff72cb42c68 100644
--- a/net/bluetooth/bnep/netdev.c
+++ b/net/bluetooth/bnep/netdev.c
@@ -75,16 +75,16 @@ static void bnep_net_set_mc_list(struct net_device *dev)
 		u8 start[ETH_ALEN] = { 0x01 };
 
 		/* Request all addresses */
-		memcpy(__skb_put(skb, ETH_ALEN), start, ETH_ALEN);
-		memcpy(__skb_put(skb, ETH_ALEN), dev->broadcast, ETH_ALEN);
+		__skb_put_data(skb, start, ETH_ALEN);
+		__skb_put_data(skb, dev->broadcast, ETH_ALEN);
 		r->len = htons(ETH_ALEN * 2);
 	} else {
 		struct netdev_hw_addr *ha;
 		int i, len = skb->len;
 
 		if (dev->flags & IFF_BROADCAST) {
-			memcpy(__skb_put(skb, ETH_ALEN), dev->broadcast, ETH_ALEN);
-			memcpy(__skb_put(skb, ETH_ALEN), dev->broadcast, ETH_ALEN);
+			__skb_put_data(skb, dev->broadcast, ETH_ALEN);
+			__skb_put_data(skb, dev->broadcast, ETH_ALEN);
 		}
 
 		/* FIXME: We should group addresses here. */
@@ -93,8 +93,8 @@ static void bnep_net_set_mc_list(struct net_device *dev)
 		netdev_for_each_mc_addr(ha, dev) {
 			if (i == BNEP_MAX_MULTICAST_FILTERS)
 				break;
-			memcpy(__skb_put(skb, ETH_ALEN), ha->addr, ETH_ALEN);
-			memcpy(__skb_put(skb, ETH_ALEN), ha->addr, ETH_ALEN);
+			__skb_put_data(skb, ha->addr, ETH_ALEN);
+			__skb_put_data(skb, ha->addr, ETH_ALEN);
 
 			i++;
 		}
diff --git a/net/bridge/br_stp_bpdu.c b/net/bridge/br_stp_bpdu.c
index 3017a396cdef..5a96fb4ce3c2 100644
--- a/net/bridge/br_stp_bpdu.c
+++ b/net/bridge/br_stp_bpdu.c
@@ -49,7 +49,7 @@ static void br_send_bpdu(struct net_bridge_port *p,
 	skb->priority = TC_PRIO_CONTROL;
 
 	skb_reserve(skb, LLC_RESERVE);
-	memcpy(__skb_put(skb, length), data, length);
+	__skb_put_data(skb, data, length);
 
 	llc_pdu_header_init(skb, LLC_PDU_TYPE_U, LLC_SAP_BSPAN,
 			    LLC_SAP_BSPAN, LLC_PDU_CMD);
