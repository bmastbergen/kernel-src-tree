dax: correct dax iomap code namespace

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ross Zwisler <ross.zwisler@linux.intel.com>
commit 11c59c92f44d9272db7655a462608658a6d95013
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/11c59c92.failed

The recently added DAX functions that use the new struct iomap data
structure were named iomap_dax_rw(), iomap_dax_fault() and
iomap_dax_actor().  These are actually defined in fs/dax.c, though, so
should be part of the "dax" namespace and not the "iomap" namespace.
Rename them to dax_iomap_rw(), dax_iomap_fault() and dax_iomap_actor()
respectively.

	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Suggested-by: Dave Chinner <david@fromorbit.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Dave Chinner <david@fromorbit.com>

(cherry picked from commit 11c59c92f44d9272db7655a462608658a6d95013)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
#	fs/ext2/file.c
#	fs/xfs/xfs_file.c
#	include/linux/dax.h
diff --cc fs/dax.c
index 1dfecdfb6245,fdbd7a1ec6cf..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -1041,3 -1028,229 +1041,232 @@@ int dax_truncate_page(struct inode *ino
  	return dax_zero_page_range(inode, from, length, get_block);
  }
  EXPORT_SYMBOL_GPL(dax_truncate_page);
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_FS_IOMAP
+ static loff_t
+ dax_iomap_actor(struct inode *inode, loff_t pos, loff_t length, void *data,
+ 		struct iomap *iomap)
+ {
+ 	struct iov_iter *iter = data;
+ 	loff_t end = pos + length, done = 0;
+ 	ssize_t ret = 0;
+ 
+ 	if (iov_iter_rw(iter) == READ) {
+ 		end = min(end, i_size_read(inode));
+ 		if (pos >= end)
+ 			return 0;
+ 
+ 		if (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)
+ 			return iov_iter_zero(min(length, end - pos), iter);
+ 	}
+ 
+ 	if (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED))
+ 		return -EIO;
+ 
+ 	while (pos < end) {
+ 		unsigned offset = pos & (PAGE_SIZE - 1);
+ 		struct blk_dax_ctl dax = { 0 };
+ 		ssize_t map_len;
+ 
+ 		dax.sector = iomap->blkno +
+ 			(((pos & PAGE_MASK) - iomap->offset) >> 9);
+ 		dax.size = (length + offset + PAGE_SIZE - 1) & PAGE_MASK;
+ 		map_len = dax_map_atomic(iomap->bdev, &dax);
+ 		if (map_len < 0) {
+ 			ret = map_len;
+ 			break;
+ 		}
+ 
+ 		dax.addr += offset;
+ 		map_len -= offset;
+ 		if (map_len > end - pos)
+ 			map_len = end - pos;
+ 
+ 		if (iov_iter_rw(iter) == WRITE)
+ 			map_len = copy_from_iter_pmem(dax.addr, map_len, iter);
+ 		else
+ 			map_len = copy_to_iter(dax.addr, map_len, iter);
+ 		dax_unmap_atomic(iomap->bdev, &dax);
+ 		if (map_len <= 0) {
+ 			ret = map_len ? map_len : -EFAULT;
+ 			break;
+ 		}
+ 
+ 		pos += map_len;
+ 		length -= map_len;
+ 		done += map_len;
+ 	}
+ 
+ 	return done ? done : ret;
+ }
+ 
+ /**
+  * dax_iomap_rw - Perform I/O to a DAX file
+  * @iocb:	The control block for this I/O
+  * @iter:	The addresses to do I/O from or to
+  * @ops:	iomap ops passed from the file system
+  *
+  * This function performs read and write operations to directly mapped
+  * persistent memory.  The callers needs to take care of read/write exclusion
+  * and evicting any page cache pages in the region under I/O.
+  */
+ ssize_t
+ dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
+ 		struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = iocb->ki_filp->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	loff_t pos = iocb->ki_pos, ret = 0, done = 0;
+ 	unsigned flags = 0;
+ 
+ 	if (iov_iter_rw(iter) == WRITE)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Yes, even DAX files can have page cache attached to them:  A zeroed
+ 	 * page is inserted into the pagecache when we have to serve a write
+ 	 * fault on a hole.  It should never be dirtied and can simply be
+ 	 * dropped from the pagecache once we get real data for the page.
+ 	 *
+ 	 * XXX: This is racy against mmap, and there's nothing we can do about
+ 	 * it. We'll eventually need to shift this down even further so that
+ 	 * we can check if we allocated blocks over a hole first.
+ 	 */
+ 	if (mapping->nrpages) {
+ 		ret = invalidate_inode_pages2_range(mapping,
+ 				pos >> PAGE_SHIFT,
+ 				(pos + iov_iter_count(iter) - 1) >> PAGE_SHIFT);
+ 		WARN_ON_ONCE(ret);
+ 	}
+ 
+ 	while (iov_iter_count(iter)) {
+ 		ret = iomap_apply(inode, pos, iov_iter_count(iter), flags, ops,
+ 				iter, dax_iomap_actor);
+ 		if (ret <= 0)
+ 			break;
+ 		pos += ret;
+ 		done += ret;
+ 	}
+ 
+ 	iocb->ki_pos += done;
+ 	return done ? done : ret;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_rw);
+ 
+ /**
+  * dax_iomap_fault - handle a page fault on a DAX file
+  * @vma: The virtual memory area where the fault occurred
+  * @vmf: The description of the fault
+  * @ops: iomap ops passed from the file system
+  *
+  * When a page fault occurs, filesystems may call this helper in their fault
+  * or mkwrite handler for DAX files. Assumes the caller has done all the
+  * necessary locking for the page fault to proceed successfully.
+  */
+ int dax_iomap_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
+ 			struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	unsigned long vaddr = (unsigned long)vmf->virtual_address;
+ 	loff_t pos = (loff_t)vmf->pgoff << PAGE_SHIFT;
+ 	sector_t sector;
+ 	struct iomap iomap = { 0 };
+ 	unsigned flags = 0;
+ 	int error, major = 0;
+ 	void *entry;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is supposed
+ 	 * to hold locks serializing us with truncate / punch hole so this is
+ 	 * a reliable test.
+ 	 */
+ 	if (pos >= i_size_read(inode))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	entry = grab_mapping_entry(mapping, vmf->pgoff);
+ 	if (IS_ERR(entry)) {
+ 		error = PTR_ERR(entry);
+ 		goto out;
+ 	}
+ 
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Note that we don't bother to use iomap_apply here: DAX required
+ 	 * the file system block size to be equal the page size, which means
+ 	 * that we never have to deal with more than a single extent here.
+ 	 */
+ 	error = ops->iomap_begin(inode, pos, PAGE_SIZE, flags, &iomap);
+ 	if (error)
+ 		goto unlock_entry;
+ 	if (WARN_ON_ONCE(iomap.offset + iomap.length < pos + PAGE_SIZE)) {
+ 		error = -EIO;		/* fs corruption? */
+ 		goto unlock_entry;
+ 	}
+ 
+ 	sector = iomap.blkno + (((pos & PAGE_MASK) - iomap.offset) >> 9);
+ 
+ 	if (vmf->cow_page) {
+ 		switch (iomap.type) {
+ 		case IOMAP_HOLE:
+ 		case IOMAP_UNWRITTEN:
+ 			clear_user_highpage(vmf->cow_page, vaddr);
+ 			break;
+ 		case IOMAP_MAPPED:
+ 			error = copy_user_dax(iomap.bdev, sector, PAGE_SIZE,
+ 					vmf->cow_page, vaddr);
+ 			break;
+ 		default:
+ 			WARN_ON_ONCE(1);
+ 			error = -EIO;
+ 			break;
+ 		}
+ 
+ 		if (error)
+ 			goto unlock_entry;
+ 		if (!radix_tree_exceptional_entry(entry)) {
+ 			vmf->page = entry;
+ 			return VM_FAULT_LOCKED;
+ 		}
+ 		vmf->entry = entry;
+ 		return VM_FAULT_DAX_LOCKED;
+ 	}
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		if (iomap.flags & IOMAP_F_NEW) {
+ 			count_vm_event(PGMAJFAULT);
+ 			mem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);
+ 			major = VM_FAULT_MAJOR;
+ 		}
+ 		error = dax_insert_mapping(mapping, iomap.bdev, sector,
+ 				PAGE_SIZE, &entry, vma, vmf);
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (!(vmf->flags & FAULT_FLAG_WRITE))
+ 			return dax_load_hole(mapping, entry, vmf);
+ 		/*FALLTHRU*/
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		error = -EIO;
+ 		break;
+ 	}
+ 
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, vmf->pgoff, entry);
+  out:
+ 	if (error == -ENOMEM)
+ 		return VM_FAULT_OOM | major;
+ 	/* -EBUSY is fine, somebody else faulted on the same PTE */
+ 	if (error < 0 && error != -EBUSY)
+ 		return VM_FAULT_SIGBUS | major;
+ 	return VM_FAULT_NOPAGE | major;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_fault);
+ #endif /* CONFIG_FS_IOMAP */
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
diff --cc fs/ext2/file.c
index 05a1de3b3230,b0f241528a30..000000000000
--- a/fs/ext2/file.c
+++ b/fs/ext2/file.c
@@@ -27,6 -29,52 +27,55 @@@
  #include "acl.h"
  
  #ifdef CONFIG_FS_DAX
++<<<<<<< HEAD
++=======
+ static ssize_t ext2_dax_read_iter(struct kiocb *iocb, struct iov_iter *to)
+ {
+ 	struct inode *inode = iocb->ki_filp->f_mapping->host;
+ 	ssize_t ret;
+ 
+ 	if (!iov_iter_count(to))
+ 		return 0; /* skip atime */
+ 
+ 	inode_lock_shared(inode);
+ 	ret = dax_iomap_rw(iocb, to, &ext2_iomap_ops);
+ 	inode_unlock_shared(inode);
+ 
+ 	file_accessed(iocb->ki_filp);
+ 	return ret;
+ }
+ 
+ static ssize_t ext2_dax_write_iter(struct kiocb *iocb, struct iov_iter *from)
+ {
+ 	struct file *file = iocb->ki_filp;
+ 	struct inode *inode = file->f_mapping->host;
+ 	ssize_t ret;
+ 
+ 	inode_lock(inode);
+ 	ret = generic_write_checks(iocb, from);
+ 	if (ret <= 0)
+ 		goto out_unlock;
+ 	ret = file_remove_privs(file);
+ 	if (ret)
+ 		goto out_unlock;
+ 	ret = file_update_time(file);
+ 	if (ret)
+ 		goto out_unlock;
+ 
+ 	ret = dax_iomap_rw(iocb, from, &ext2_iomap_ops);
+ 	if (ret > 0 && iocb->ki_pos > i_size_read(inode)) {
+ 		i_size_write(inode, iocb->ki_pos);
+ 		mark_inode_dirty(inode);
+ 	}
+ 
+ out_unlock:
+ 	inode_unlock(inode);
+ 	if (ret > 0)
+ 		ret = generic_write_sync(iocb, ret);
+ 	return ret;
+ }
+ 
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
  /*
   * The lock ordering for ext2 DAX fault paths is:
   *
@@@ -51,7 -99,7 +100,11 @@@ static int ext2_dax_fault(struct vm_are
  	}
  	down_read(&ei->dax_sem);
  
++<<<<<<< HEAD
 +	ret = dax_fault(vma, vmf, ext2_get_block);
++=======
+ 	ret = dax_iomap_fault(vma, vmf, &ext2_iomap_ops);
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
  
  	up_read(&ei->dax_sem);
  	if (vmf->flags & FAULT_FLAG_WRITE)
diff --cc fs/xfs/xfs_file.c
index 4da908a64865,e7f35d548cfc..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -397,11 -344,7 +397,15 @@@ xfs_file_dax_read
  		return 0; /* skip atime */
  
  	xfs_rw_ilock(ip, XFS_IOLOCK_SHARED);
++<<<<<<< HEAD
 +	ret = dax_do_io(READ, iocb, inode, iovp, pos, nr_segs,
 +			xfs_get_blocks_direct, NULL, 0);
 +	if (ret > 0) {
 +		iocb->ki_pos = pos + ret;
 +	}
++=======
+ 	ret = dax_iomap_rw(iocb, to, &xfs_iomap_ops);
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
  	xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
  
  	file_accessed(iocb->ki_filp);
@@@ -982,40 -686,20 +986,52 @@@ xfs_file_dax_write
  	if (ret)
  		goto out;
  
++<<<<<<< HEAD
 +	/*
 +	 * Yes, even DAX files can have page cache attached to them:  A zeroed
 +	 * page is inserted into the pagecache when we have to serve a write
 +	 * fault on a hole.  It should never be dirtied and can simply be
 +	 * dropped from the pagecache once we get real data for the page.
 +	 *
 +	 * XXX: This is racy against mmap, and there's nothing we can do about
 +	 * it. dax_do_io() should really do this invalidation internally as
 +	 * it will know if we've allocated over a holei for this specific IO and
 +	 * if so it needs to update the mapping tree and invalidate existing
 +	 * PTEs over the newly allocated range. Remove this invalidation when
 +	 * dax_do_io() is fixed up.
 +	 */
 +	if (mapping->nrpages) {
 +		loff_t end = iocb->ki_pos + count - 1;
 +		ret = invalidate_inode_pages2_range(mapping,
 +						iocb->ki_pos >> PAGE_SHIFT,
 +						end >> PAGE_SHIFT);
 +		WARN_ON_ONCE(ret);
++=======
+ 	pos = iocb->ki_pos;
+ 	count = iov_iter_count(from);
+ 
+ 	trace_xfs_file_dax_write(ip, count, pos);
+ 
+ 	ret = dax_iomap_rw(iocb, from, &xfs_iomap_ops);
+ 	if (ret > 0 && iocb->ki_pos > i_size_read(inode)) {
+ 		i_size_write(inode, iocb->ki_pos);
+ 		error = xfs_setfilesize(ip, pos, ret);
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
  	}
  
 +	trace_xfs_file_dax_write(ip, count, iocb->ki_pos);
 +
 +	ret = dax_do_io(WRITE, iocb, inode, iovp, pos, nr_segs,
 +			xfs_get_blocks_direct,
 +			xfs_end_io_direct_write, 0);
 +
 +	if (ret > 0) {
 +		pos += ret;
 +		iocb->ki_pos = pos;
 +	}
  out:
  	xfs_rw_iunlock(ip, iolock);
 -	return error ? error : ret;
 +	return ret;
  }
  
  STATIC ssize_t
@@@ -1711,9 -1640,9 +1727,13 @@@ xfs_filemap_page_mkwrite
  	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
  
  	if (IS_DAX(inode)) {
++<<<<<<< HEAD
 +		ret = dax_mkwrite(vma, vmf, xfs_get_blocks_dax_fault);
++=======
+ 		ret = dax_iomap_fault(vma, vmf, &xfs_iomap_ops);
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
  	} else {
 -		ret = iomap_page_mkwrite(vma, vmf, &xfs_iomap_ops);
 +		ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
  		ret = block_page_mkwrite_return(ret);
  	}
  
@@@ -1745,7 -1674,7 +1765,11 @@@ xfs_filemap_fault
  		 * changes to xfs_get_blocks_direct() to map unwritten extent
  		 * ioend for conversion on read-only mappings.
  		 */
++<<<<<<< HEAD
 +		ret = dax_fault(vma, vmf, xfs_get_blocks_dax_fault);
++=======
+ 		ret = dax_iomap_fault(vma, vmf, &xfs_iomap_ops);
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
  	} else
  		ret = filemap_fault(vma, vmf);
  	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
diff --cc include/linux/dax.h
index 8937c7aed5cb,a3dfee4cb03f..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -6,27 -6,19 +6,38 @@@
  #include <linux/radix-tree.h>
  #include <asm/pgtable.h>
  
 -struct iomap_ops;
 -
 -/* We use lowest available exceptional entry bit for locking */
 +/*
 + * We use lowest available bit in exceptional entry for locking, other two
 + * bits to determine entry type. In total 3 special bits.
 + */
 +#define RADIX_DAX_SHIFT	(RADIX_TREE_EXCEPTIONAL_SHIFT + 3)
  #define RADIX_DAX_ENTRY_LOCK (1 << RADIX_TREE_EXCEPTIONAL_SHIFT)
 +#define RADIX_DAX_PTE (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 1))
 +#define RADIX_DAX_PMD (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 2))
 +#define RADIX_DAX_TYPE_MASK (RADIX_DAX_PTE | RADIX_DAX_PMD)
 +#define RADIX_DAX_TYPE(entry) ((unsigned long)entry & RADIX_DAX_TYPE_MASK)
 +#define RADIX_DAX_SECTOR(entry) (((unsigned long)entry >> RADIX_DAX_SHIFT))
 +#define RADIX_DAX_ENTRY(sector, pmd) ((void *)((unsigned long)sector << \
 +		RADIX_DAX_SHIFT | (pmd ? RADIX_DAX_PMD : RADIX_DAX_PTE) | \
 +		RADIX_TREE_EXCEPTIONAL_ENTRY))
 +
++<<<<<<< HEAD
  
 +ssize_t dax_do_io(int rw, struct kiocb *iocb, struct inode *inode,
 +                  const struct iovec *iov, loff_t pos, unsigned long nr_segs,
 +                  get_block_t get_block, dio_iodone_t end_io, int flags);
 +int dax_zero_page_range(struct inode *, loff_t from, unsigned len, get_block_t);
 +int dax_truncate_page(struct inode *, loff_t from, get_block_t);
++=======
+ ssize_t dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
+ 		struct iomap_ops *ops);
+ ssize_t dax_do_io(struct kiocb *, struct inode *, struct iov_iter *,
+ 		  get_block_t, dio_iodone_t, int flags);
+ int dax_zero_page_range(struct inode *, loff_t from, unsigned len, get_block_t);
+ int dax_truncate_page(struct inode *, loff_t from, get_block_t);
+ int dax_iomap_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
+ 			struct iomap_ops *ops);
++>>>>>>> 11c59c92f44d (dax: correct dax iomap code namespace)
  int dax_fault(struct vm_area_struct *, struct vm_fault *, get_block_t);
  int dax_delete_mapping_entry(struct address_space *mapping, pgoff_t index);
  void dax_wake_mapping_entry_waiter(struct address_space *mapping,
* Unmerged path fs/dax.c
* Unmerged path fs/ext2/file.c
* Unmerged path fs/xfs/xfs_file.c
* Unmerged path include/linux/dax.h
