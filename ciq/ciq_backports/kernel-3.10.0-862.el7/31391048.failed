net/mlx5e: Different SQ types

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Different SQ types (Don Dutile) [1456694 1499362]
Rebuild_FUZZ: 92.59%
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 3139104861d9a8fed13f813237cc998c8272eafc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/31391048.failed

Different SQ types (tx, xdp, ico) are growing apart, we separate them
and remove unwanted parts in each one of them, to simplify data path and
utilize data cache.

Remove DB union from SQ structures since it is not needed anymore as we
now have different SQ data type for each SQ.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3139104861d9a8fed13f813237cc998c8272eafc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 16c2c2d53ebb,bace9233dc1f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -286,13 -293,140 +286,144 @@@ struct mlx5e_cq 
  	struct mlx5_frag_wq_ctrl   wq_ctrl;
  } ____cacheline_aligned_in_smp;
  
 -struct mlx5e_tx_wqe_info {
 -	u32 num_bytes;
 -	u8  num_wqebbs;
 -	u8  num_dma;
 -};
 -
 +struct mlx5e_rq;
 +typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
 +				       struct mlx5_cqe64 *cqe);
 +typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
 +				  u16 ix);
 +
++<<<<<<< HEAD
 +typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq *rq, u16 ix);
++=======
+ enum mlx5e_dma_map_type {
+ 	MLX5E_DMA_MAP_SINGLE,
+ 	MLX5E_DMA_MAP_PAGE
+ };
+ 
+ struct mlx5e_sq_dma {
+ 	dma_addr_t              addr;
+ 	u32                     size;
+ 	enum mlx5e_dma_map_type type;
+ };
+ 
+ enum {
+ 	MLX5E_SQ_STATE_ENABLED,
+ };
+ 
+ struct mlx5e_sq_wqe_info {
+ 	u8  opcode;
+ 	u8  num_wqebbs;
+ };
+ 
+ struct mlx5e_txqsq {
+ 	/* data path */
+ 
+ 	/* dirtied @completion */
+ 	u16                        cc;
+ 	u32                        dma_fifo_cc;
+ 
+ 	/* dirtied @xmit */
+ 	u16                        pc ____cacheline_aligned_in_smp;
+ 	u32                        dma_fifo_pc;
+ 	struct mlx5e_sq_stats      stats;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct sk_buff           **skb;
+ 		struct mlx5e_sq_dma       *dma_fifo;
+ 		struct mlx5e_tx_wqe_info  *wqe_info;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	u32                        dma_fifo_mask;
+ 	void __iomem              *uar_map;
+ 	struct netdev_queue       *txq;
+ 	u32                        sqn;
+ 	u16                        max_inline;
+ 	u8                         min_inline_mode;
+ 	u16                        edge;
+ 	struct device             *pdev;
+ 	struct mlx5e_tstamp       *tstamp;
+ 	__be32                     mkey_be;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ 	int                        tc;
+ 	u32                        rate_limit;
+ } ____cacheline_aligned_in_smp;
+ 
+ struct mlx5e_xdpsq {
+ 	/* data path */
+ 
+ 	/* dirtied @rx completion */
+ 	u16                        cc;
+ 	u16                        pc;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct mlx5e_dma_info     *di;
+ 		bool                       doorbell;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	void __iomem              *uar_map;
+ 	u32                        sqn;
+ 	struct device             *pdev;
+ 	__be32                     mkey_be;
+ 	u8                         min_inline_mode;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ } ____cacheline_aligned_in_smp;
+ 
+ struct mlx5e_icosq {
+ 	/* data path */
+ 
+ 	/* dirtied @completion */
+ 	u16                        cc;
+ 
+ 	/* dirtied @xmit */
+ 	u16                        pc ____cacheline_aligned_in_smp;
+ 	u32                        dma_fifo_pc;
+ 	u16                        prev_cc;
+ 
+ 	struct mlx5e_cq            cq;
+ 
+ 	/* write@xmit, read@completion */
+ 	struct {
+ 		struct mlx5e_sq_wqe_info *ico_wqe;
+ 	} db;
+ 
+ 	/* read only */
+ 	struct mlx5_wq_cyc         wq;
+ 	void __iomem              *uar_map;
+ 	u32                        sqn;
+ 	u16                        edge;
+ 	struct device             *pdev;
+ 	__be32                     mkey_be;
+ 	unsigned long              state;
+ 
+ 	/* control path */
+ 	struct mlx5_wq_ctrl        wq_ctrl;
+ 	struct mlx5e_channel      *channel;
+ } ____cacheline_aligned_in_smp;
+ 
+ static inline bool
+ mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
+ {
+ 	return (((wq->sz_m1 & (cc - pc)) >= n) || (cc == pc));
+ }
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  
  struct mlx5e_dma_info {
  	struct page	*page;
@@@ -368,6 -521,11 +499,13 @@@ struct mlx5e_rq 
  
  	struct mlx5e_rx_am     am; /* Adaptive Moderation */
  
++<<<<<<< HEAD
++=======
+ 	/* XDP */
+ 	struct bpf_prog       *xdp_prog;
+ 	struct mlx5e_xdpsq     xdpsq;
+ 
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	/* control */
  	struct mlx5_wq_ctrl    wq_ctrl;
  	u8                     wq_type;
@@@ -488,8 -544,9 +626,14 @@@ enum channel_flags 
  struct mlx5e_channel {
  	/* data path */
  	struct mlx5e_rq            rq;
++<<<<<<< HEAD
 +	struct mlx5e_sq            sq[MLX5E_MAX_NUM_TC];
 +	struct mlx5e_sq            icosq;   /* internal control operations */
++=======
+ 	struct mlx5e_txqsq         sq[MLX5E_MAX_NUM_TC];
+ 	struct mlx5e_icosq         icosq;   /* internal control operations */
+ 	bool                       xdp;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	struct napi_struct         napi;
  	struct device             *pdev;
  	struct net_device         *netdev;
@@@ -670,8 -727,9 +814,8 @@@ struct mlx5e_profile 
  
  struct mlx5e_priv {
  	/* priv data path fields - start */
- 	struct mlx5e_sq            **txq_to_sq_map;
+ 	struct mlx5e_txqsq         **txq_to_sq_map;
  	int channeltc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
 -	struct bpf_prog *xdp_prog;
  	/* priv data path fields - end */
  
  	unsigned long              state;
@@@ -720,7 -777,9 +864,13 @@@ void mlx5e_cq_error_event(struct mlx5_c
  int mlx5e_napi_poll(struct napi_struct *napi, int budget);
  bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
++<<<<<<< HEAD
 +void mlx5e_free_tx_descs(struct mlx5e_sq *sq);
++=======
+ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
+ void mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq);
+ void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  
  void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
  			bool recycle);
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index edb21d8194bc,e849a0fc2653..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -817,11 -846,12 +818,11 @@@ static int mlx5e_open_rq(struct mlx5e_c
  			 struct mlx5e_rq_param *param,
  			 struct mlx5e_rq *rq)
  {
- 	struct mlx5e_sq *sq = &c->icosq;
+ 	struct mlx5e_icosq *sq = &c->icosq;
  	u16 pi = sq->pc & sq->wq.sz_m1;
 -	struct mlx5e_tx_wqe *nopwqe;
  	int err;
  
 -	err = mlx5e_alloc_rq(c, param, rq);
 +	err = mlx5e_create_rq(c, param, rq);
  	if (err)
  		return err;
  
@@@ -839,15 -869,15 +840,20 @@@
  
  	sq->db.ico_wqe[pi].opcode     = MLX5_OPCODE_NOP;
  	sq->db.ico_wqe[pi].num_wqebbs = 1;
++<<<<<<< HEAD
 +	mlx5e_send_nop(sq, true); /* trigger mlx5e_post_rx_wqes() */
 +
++=======
+ 	nopwqe = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+ 	mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &nopwqe->ctrl);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	return 0;
  
 -err_destroy_rq:
 +err_disable_rq:
  	clear_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);
 +	mlx5e_disable_rq(rq);
 +err_destroy_rq:
  	mlx5e_destroy_rq(rq);
 -err_free_rq:
 -	mlx5e_free_rq(rq);
  
  	return err;
  }
@@@ -858,12 -888,70 +864,74 @@@ static void mlx5e_close_rq(struct mlx5e
  	napi_synchronize(&rq->channel->napi); /* prevent mlx5e_post_rx_wqes */
  	cancel_work_sync(&rq->am.work);
  
 -	mlx5e_destroy_rq(rq);
 +	mlx5e_disable_rq(rq);
  	mlx5e_free_rx_descs(rq);
++<<<<<<< HEAD
 +	mlx5e_destroy_rq(rq);
++=======
+ 	mlx5e_free_rq(rq);
  }
  
- static void mlx5e_free_sq_ico_db(struct mlx5e_sq *sq)
+ static void mlx5e_free_xdpsq_db(struct mlx5e_xdpsq *sq)
+ {
+ 	kfree(sq->db.di);
+ }
+ 
+ static int mlx5e_alloc_xdpsq_db(struct mlx5e_xdpsq *sq, int numa)
+ {
+ 	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
+ 
+ 	sq->db.di = kzalloc_node(sizeof(*sq->db.di) * wq_sz,
+ 				     GFP_KERNEL, numa);
+ 	if (!sq->db.di) {
+ 		mlx5e_free_xdpsq_db(sq);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	return 0;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
+ }
+ 
+ static int mlx5e_alloc_xdpsq(struct mlx5e_channel *c,
+ 			     struct mlx5e_sq_param *param,
+ 			     struct mlx5e_xdpsq *sq)
+ {
+ 	void *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);
+ 	struct mlx5e_priv *priv    = c->priv;
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	int err;
+ 
+ 	sq->pdev      = c->pdev;
+ 	sq->mkey_be   = c->mkey_be;
+ 	sq->channel   = c;
+ 	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
+ 	sq->min_inline_mode = param->min_inline_mode;
+ 
+ 	param->wq.db_numa_node = cpu_to_node(c->cpu);
+ 	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq, &sq->wq_ctrl);
+ 	if (err)
+ 		return err;
+ 	sq->wq.db = &sq->wq.db[MLX5_SND_DBR];
+ 
+ 	err = mlx5e_alloc_xdpsq_db(sq, cpu_to_node(c->cpu));
+ 	if (err)
+ 		goto err_sq_wq_destroy;
+ 
+ 	return 0;
+ 
+ err_sq_wq_destroy:
+ 	mlx5_wq_destroy(&sq->wq_ctrl);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_free_xdpsq(struct mlx5e_xdpsq *sq)
+ {
+ 	mlx5e_free_xdpsq_db(sq);
+ 	mlx5_wq_destroy(&sq->wq_ctrl);
+ }
+ 
+ static void mlx5e_free_icosq_db(struct mlx5e_icosq *sq)
  {
  	kfree(sq->db.ico_wqe);
  }
@@@ -880,121 -968,32 +948,141 @@@ static int mlx5e_alloc_icosq_db(struct 
  	return 0;
  }
  
- static void mlx5e_free_sq_txq_db(struct mlx5e_sq *sq)
+ static int mlx5e_alloc_icosq(struct mlx5e_channel *c,
+ 			     int tc,
+ 			     struct mlx5e_sq_param *param,
+ 			     struct mlx5e_icosq *sq)
  {
++<<<<<<< HEAD
 +	kfree(sq->db.txq.wqe_info);
 +	kfree(sq->db.txq.dma_fifo);
 +	kfree(sq->db.txq.skb);
 +}
 +
 +static int mlx5e_alloc_sq_txq_db(struct mlx5e_sq *sq, int numa)
 +{
 +	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
 +	int df_sz = wq_sz * MLX5_SEND_WQEBB_NUM_DS;
 +
 +	sq->db.txq.skb = kzalloc_node(wq_sz * sizeof(*sq->db.txq.skb),
 +				      GFP_KERNEL, numa);
 +	sq->db.txq.dma_fifo = kzalloc_node(df_sz * sizeof(*sq->db.txq.dma_fifo),
 +					   GFP_KERNEL, numa);
 +	sq->db.txq.wqe_info = kzalloc_node(wq_sz * sizeof(*sq->db.txq.wqe_info),
 +					   GFP_KERNEL, numa);
 +	if (!sq->db.txq.skb || !sq->db.txq.dma_fifo || !sq->db.txq.wqe_info) {
 +		mlx5e_free_sq_txq_db(sq);
 +		return -ENOMEM;
 +	}
 +
 +	sq->dma_fifo_mask = df_sz - 1;
 +
 +	return 0;
 +}
 +
 +static void mlx5e_free_sq_db(struct mlx5e_sq *sq)
 +{
 +	switch (sq->type) {
 +	case MLX5E_SQ_TXQ:
 +		mlx5e_free_sq_txq_db(sq);
 +		break;
 +	case MLX5E_SQ_ICO:
 +		mlx5e_free_sq_ico_db(sq);
 +		break;
 +	}
 +}
 +
 +static int mlx5e_alloc_sq_db(struct mlx5e_sq *sq, int numa)
 +{
 +	switch (sq->type) {
 +	case MLX5E_SQ_TXQ:
 +		return mlx5e_alloc_sq_txq_db(sq, numa);
 +	case MLX5E_SQ_ICO:
 +		return mlx5e_alloc_sq_ico_db(sq, numa);
 +	}
 +
 +	return 0;
 +}
 +
 +static int mlx5e_create_sq(struct mlx5e_channel *c,
 +			   int tc,
 +			   struct mlx5e_sq_param *param,
 +			   struct mlx5e_sq *sq)
 +{
 +	struct mlx5e_priv *priv = c->priv;
 +	struct mlx5_core_dev *mdev = priv->mdev;
 +
 +	void *sqc = param->sqc;
 +	void *sqc_wq = MLX5_ADDR_OF(sqc, sqc, wq);
 +	u16 sq_max_wqebbs;
++=======
+ 	void *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);
+ 	struct mlx5e_priv *priv    = c->priv;
+ 	struct mlx5_core_dev *mdev = priv->mdev;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	int err;
  
- 	sq->type      = param->type;
  	sq->pdev      = c->pdev;
- 	sq->tstamp    = &priv->tstamp;
  	sq->mkey_be   = c->mkey_be;
  	sq->channel   = c;
++<<<<<<< HEAD
 +	sq->tc        = tc;
++=======
+ 	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  
 +	err = mlx5_alloc_map_uar(mdev, &sq->uar, !!MLX5_CAP_GEN(mdev, bf));
 +	if (err)
 +		return err;
 +
 +	sq->uar_map = sq->bfreg.map;
  	param->wq.db_numa_node = cpu_to_node(c->cpu);
- 
- 	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq,
- 				 &sq->wq_ctrl);
+ 	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq, &sq->wq_ctrl);
  	if (err)
++<<<<<<< HEAD
 +		goto err_unmap_free_uar;
 +
 +	sq->wq.db       = &sq->wq.db[MLX5_SND_DBR];
 +	if (sq->uar.bf_map) {
 +		set_bit(MLX5E_SQ_STATE_BF_ENABLE, &sq->state);
 +		sq->uar_map = sq->uar.bf_map;
 +	} else {
 +		sq->uar_map = sq->uar.map;
 +	}
 +	sq->bf_buf_size = (1 << MLX5_CAP_GEN(mdev, log_bf_reg_size)) / 2;
 +	sq->max_inline  = param->max_inline;
 +	sq->min_inline_mode =
 +		MLX5_CAP_ETH(mdev, wqe_inline_mode) == MLX5_CAP_INLINE_MODE_VPORT_CONTEXT ?
 +		param->min_inline_mode : 0;
 +
 +	err = mlx5e_alloc_sq_db(sq, cpu_to_node(c->cpu));
 +	if (err)
 +		goto err_sq_wq_destroy;
 +
 +	sq_max_wqebbs = MLX5_SEND_WQE_MAX_WQEBBS;
 +	if (sq->type == MLX5E_SQ_TXQ) {
 +		int txq_ix;
 +
 +		txq_ix = c->ix + tc * priv->params.num_channels;
 +		sq->txq = netdev_get_tx_queue(priv->netdev, txq_ix);
 +		priv->txq_to_sq_map[txq_ix] = sq;
 +	}
 +
 +	if (sq->type == MLX5E_SQ_ICO)
 +		sq_max_wqebbs = MLX5E_ICOSQ_MAX_WQEBBS;
 +
 +	sq->edge      = (sq->wq.sz_m1 + 1) - sq_max_wqebbs;
 +	sq->bf_budget = MLX5E_SQ_BF_BUDGET;
++=======
+ 		return err;
+ 	sq->wq.db = &sq->wq.db[MLX5_SND_DBR];
+ 
+ 	err = mlx5e_alloc_icosq_db(sq, cpu_to_node(c->cpu));
+ 	if (err)
+ 		goto err_sq_wq_destroy;
+ 
+ 	sq->edge = (sq->wq.sz_m1 + 1) - MLX5E_ICOSQ_MAX_WQEBBS;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  
  	return 0;
  
@@@ -1007,20 -1003,103 +1095,104 @@@ err_unmap_free_uar
  	return err;
  }
  
++<<<<<<< HEAD
 +static void mlx5e_destroy_sq(struct mlx5e_sq *sq)
 +{
 +	struct mlx5e_channel *c = sq->channel;
 +	struct mlx5e_priv *priv = c->priv;
 +
 +	mlx5e_free_sq_db(sq);
++=======
+ static void mlx5e_free_icosq(struct mlx5e_icosq *sq)
+ {
+ 	mlx5e_free_icosq_db(sq);
+ 	mlx5_wq_destroy(&sq->wq_ctrl);
+ }
+ 
+ static void mlx5e_free_txqsq_db(struct mlx5e_txqsq *sq)
+ {
+ 	kfree(sq->db.wqe_info);
+ 	kfree(sq->db.dma_fifo);
+ 	kfree(sq->db.skb);
+ }
+ 
+ static int mlx5e_alloc_txqsq_db(struct mlx5e_txqsq *sq, int numa)
+ {
+ 	int wq_sz = mlx5_wq_cyc_get_size(&sq->wq);
+ 	int df_sz = wq_sz * MLX5_SEND_WQEBB_NUM_DS;
+ 
+ 	sq->db.skb = kzalloc_node(wq_sz * sizeof(*sq->db.skb),
+ 				      GFP_KERNEL, numa);
+ 	sq->db.dma_fifo = kzalloc_node(df_sz * sizeof(*sq->db.dma_fifo),
+ 					   GFP_KERNEL, numa);
+ 	sq->db.wqe_info = kzalloc_node(wq_sz * sizeof(*sq->db.wqe_info),
+ 					   GFP_KERNEL, numa);
+ 	if (!sq->db.skb || !sq->db.dma_fifo || !sq->db.wqe_info) {
+ 		mlx5e_free_txqsq_db(sq);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	sq->dma_fifo_mask = df_sz - 1;
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5e_alloc_txqsq(struct mlx5e_channel *c,
+ 			     int tc,
+ 			     struct mlx5e_sq_param *param,
+ 			     struct mlx5e_txqsq *sq)
+ {
+ 	void *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);
+ 	struct mlx5e_priv *priv    = c->priv;
+ 	struct mlx5_core_dev *mdev = priv->mdev;
+ 	int txq_ix;
+ 	int err;
+ 
+ 	sq->pdev      = c->pdev;
+ 	sq->tstamp    = &priv->tstamp;
+ 	sq->mkey_be   = c->mkey_be;
+ 	sq->channel   = c;
+ 	sq->tc        = tc;
+ 	sq->uar_map   = mdev->mlx5e_res.bfreg.map;
+ 	sq->max_inline      = param->max_inline;
+ 	sq->min_inline_mode = param->min_inline_mode;
+ 
+ 	param->wq.db_numa_node = cpu_to_node(c->cpu);
+ 	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, &sq->wq, &sq->wq_ctrl);
+ 	if (err)
+ 		return err;
+ 	sq->wq.db    = &sq->wq.db[MLX5_SND_DBR];
+ 
+ 	err = mlx5e_alloc_txqsq_db(sq, cpu_to_node(c->cpu));
+ 	if (err)
+ 		goto err_sq_wq_destroy;
+ 
+ 	txq_ix = c->ix + tc * priv->params.num_channels;
+ 	sq->txq = netdev_get_tx_queue(priv->netdev, txq_ix);
+ 	priv->txq_to_sq_map[txq_ix] = sq;
+ 
+ 	sq->edge = (sq->wq.sz_m1 + 1) - MLX5_SEND_WQE_MAX_WQEBBS;
+ 
+ 	return 0;
+ 
+ err_sq_wq_destroy:
+ 	mlx5_wq_destroy(&sq->wq_ctrl);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_free_txqsq(struct mlx5e_txqsq *sq)
+ {
+ 	mlx5e_free_txqsq_db(sq);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	mlx5_wq_destroy(&sq->wq_ctrl);
 +	mlx5_unmap_free_uar(priv->mdev, &sq->uar);
  }
  
 -struct mlx5e_create_sq_param {
 -	struct mlx5_wq_ctrl        *wq_ctrl;
 -	u32                         cqn;
 -	u32                         tisn;
 -	u8                          tis_lst_sz;
 -	u8                          min_inline_mode;
 -};
 -
 -static int mlx5e_create_sq(struct mlx5e_priv *priv,
 -			   struct mlx5e_sq_param *param,
 -			   struct mlx5e_create_sq_param *csp,
 -			   u32 *sqn)
 +static int mlx5e_enable_sq(struct mlx5e_sq *sq, struct mlx5e_sq_param *param)
  {
 +	struct mlx5e_channel *c = sq->channel;
 +	struct mlx5e_priv *priv = c->priv;
  	struct mlx5_core_dev *mdev = priv->mdev;
  
  	void *in;
@@@ -1096,50 -1181,62 +1268,103 @@@ static int mlx5e_modify_sq(struct mlx5e
  	return err;
  }
  
++<<<<<<< HEAD
 +static void mlx5e_disable_sq(struct mlx5e_sq *sq)
++=======
+ static void mlx5e_destroy_sq(struct mlx5e_priv *priv, u32 sqn)
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  {
 -	mlx5_core_destroy_sq(priv->mdev, sqn);
 +	struct mlx5e_channel *c = sq->channel;
 +	struct mlx5e_priv *priv = c->priv;
 +	struct mlx5_core_dev *mdev = priv->mdev;
 +
 +	mlx5_core_destroy_sq(mdev, sq->sqn);
 +	if (sq->rate_limit)
 +		mlx5_rl_remove_rate(mdev, sq->rate_limit);
  }
  
- static int mlx5e_open_sq(struct mlx5e_channel *c,
- 			 int tc,
- 			 struct mlx5e_sq_param *param,
- 			 struct mlx5e_sq *sq)
+ static int mlx5e_create_sq_rdy(struct mlx5e_priv *priv,
+ 			       struct mlx5e_sq_param *param,
+ 			       struct mlx5e_create_sq_param *csp,
+ 			       u32 *sqn)
  {
++<<<<<<< HEAD
 +	int err;
 +
 +	err = mlx5e_create_sq(c, tc, param, sq);
 +	if (err)
 +		return err;
 +
 +	err = mlx5e_enable_sq(sq, param);
 +	if (err)
 +		goto err_destroy_sq;
 +
 +	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 +	err = mlx5e_modify_sq(sq, MLX5_SQC_STATE_RST, MLX5_SQC_STATE_RDY,
 +			      false, 0);
 +	if (err)
 +		goto err_disable_sq;
 +
 +	if (sq->txq) {
 +		netdev_tx_reset_queue(sq->txq);
 +		netif_tx_start_queue(sq->txq);
 +	}
++=======
+ 	struct mlx5e_modify_sq_param msp = {0};
+ 	int err;
+ 
+ 	err = mlx5e_create_sq(priv, param, csp, sqn);
+ 	if (err)
+ 		return err;
+ 
+ 	msp.curr_state = MLX5_SQC_STATE_RST;
+ 	msp.next_state = MLX5_SQC_STATE_RDY;
+ 	err = mlx5e_modify_sq(priv, *sqn, &msp);
+ 	if (err)
+ 		mlx5e_destroy_sq(priv, *sqn);
+ 
+ 	return err;
+ }
+ 
+ static int mlx5e_open_txqsq(struct mlx5e_channel *c,
+ 			    int tc,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_txqsq *sq)
+ {
+ 	struct mlx5e_create_sq_param csp = {};
+ 	struct mlx5e_priv *priv = c->priv;
+ 	int err;
+ 
+ 	err = mlx5e_alloc_txqsq(c, tc, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.tisn            = priv->tisn[sq->tc];
+ 	csp.tis_lst_sz      = 1;
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = sq->min_inline_mode;
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_create_sq_rdy(c->priv, param, &csp, &sq->sqn);
+ 	if (err)
+ 		goto err_free_txqsq;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  
+ 	netdev_tx_reset_queue(sq->txq);
+ 	netif_tx_start_queue(sq->txq);
  	return 0;
  
++<<<<<<< HEAD
 +err_disable_sq:
 +	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
 +	mlx5e_disable_sq(sq);
 +err_destroy_sq:
 +	mlx5e_destroy_sq(sq);
++=======
+ err_free_txqsq:
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	mlx5e_free_txqsq(sq);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  
  	return err;
  }
@@@ -1151,30 -1248,142 +1376,151 @@@ static inline void netif_tx_disable_que
  	__netif_tx_unlock_bh(txq);
  }
  
- static void mlx5e_close_sq(struct mlx5e_sq *sq)
+ static void mlx5e_close_txqsq(struct mlx5e_txqsq *sq)
  {
 -	struct mlx5e_channel *c = sq->channel;
 -	struct mlx5e_priv *priv = c->priv;
 -	struct mlx5_core_dev *mdev = priv->mdev;
 -
  	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
  	/* prevent netif_tx_wake_queue */
 -	napi_synchronize(&c->napi);
 +	napi_synchronize(&sq->channel->napi);
  
- 	if (sq->txq) {
- 		netif_tx_disable_queue(sq->txq);
+ 	netif_tx_disable_queue(sq->txq);
  
++<<<<<<< HEAD
 +		/* last doorbell out, godspeed .. */
 +		if (mlx5e_sq_has_room_for(sq, 1)) {
 +			sq->db.txq.skb[(sq->pc & sq->wq.sz_m1)] = NULL;
 +			mlx5e_send_nop(sq, true);
 +		}
 +	}
 +
 +	mlx5e_disable_sq(sq);
 +	mlx5e_free_tx_descs(sq);
 +	mlx5e_destroy_sq(sq);
++=======
+ 	/* last doorbell out, godspeed .. */
+ 	if (mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, 1)) {
+ 		struct mlx5e_tx_wqe *nop;
+ 
+ 		sq->db.skb[(sq->pc & sq->wq.sz_m1)] = NULL;
+ 		nop = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+ 		mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &nop->ctrl);
+ 	}
+ 
+ 	mlx5e_destroy_sq(priv, sq->sqn);
+ 	if (sq->rate_limit)
+ 		mlx5_rl_remove_rate(mdev, sq->rate_limit);
+ 	mlx5e_free_txqsq_descs(sq);
+ 	mlx5e_free_txqsq(sq);
+ }
+ 
+ static int mlx5e_open_icosq(struct mlx5e_channel *c,
+ 			    int tc,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_icosq *sq)
+ {
+ 	struct mlx5e_create_sq_param csp = {};
+ 	int err;
+ 
+ 	err = mlx5e_alloc_icosq(c, tc, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = param->min_inline_mode;
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_create_sq_rdy(c->priv, param, &csp, &sq->sqn);
+ 	if (err)
+ 		goto err_free_icosq;
+ 
+ 	return 0;
+ 
+ err_free_icosq:
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	mlx5e_free_icosq(sq);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_close_icosq(struct mlx5e_icosq *sq)
+ {
+ 	struct mlx5e_channel *c = sq->channel;
+ 
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	napi_synchronize(&c->napi);
+ 
+ 	mlx5e_destroy_sq(c->priv, sq->sqn);
+ 	mlx5e_free_icosq(sq);
+ }
+ 
+ static int mlx5e_open_xdpsq(struct mlx5e_channel *c,
+ 			    struct mlx5e_sq_param *param,
+ 			    struct mlx5e_xdpsq *sq)
+ {
+ 	unsigned int ds_cnt = MLX5E_XDP_TX_DS_COUNT;
+ 	struct mlx5e_create_sq_param csp = {};
+ 	struct mlx5e_priv *priv = c->priv;
+ 	unsigned int inline_hdr_sz = 0;
+ 	int err;
+ 	int i;
+ 
+ 	err = mlx5e_alloc_xdpsq(c, param, sq);
+ 	if (err)
+ 		return err;
+ 
+ 	csp.tis_lst_sz      = 1;
+ 	csp.tisn            = priv->tisn[0]; /* tc = 0 */
+ 	csp.cqn             = sq->cq.mcq.cqn;
+ 	csp.wq_ctrl         = &sq->wq_ctrl;
+ 	csp.min_inline_mode = sq->min_inline_mode;
+ 	set_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	err = mlx5e_create_sq_rdy(c->priv, param, &csp, &sq->sqn);
+ 	if (err)
+ 		goto err_free_xdpsq;
+ 
+ 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+ 		inline_hdr_sz = MLX5E_XDP_MIN_INLINE;
+ 		ds_cnt++;
+ 	}
+ 
+ 	/* Pre initialize fixed WQE fields */
+ 	for (i = 0; i < mlx5_wq_cyc_get_size(&sq->wq); i++) {
+ 		struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(&sq->wq, i);
+ 		struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+ 		struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ 		struct mlx5_wqe_data_seg *dseg;
+ 
+ 		cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_cnt);
+ 		eseg->inline_hdr.sz = cpu_to_be16(inline_hdr_sz);
+ 
+ 		dseg = (struct mlx5_wqe_data_seg *)cseg + (ds_cnt - 1);
+ 		dseg->lkey = sq->mkey_be;
+ 	}
+ 
+ 	return 0;
+ 
+ err_free_xdpsq:
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	mlx5e_free_xdpsq(sq);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_close_xdpsq(struct mlx5e_xdpsq *sq)
+ {
+ 	struct mlx5e_channel *c = sq->channel;
+ 
+ 	clear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);
+ 	napi_synchronize(&c->napi);
+ 
+ 	mlx5e_destroy_sq(c->priv, sq->sqn);
+ 	mlx5e_free_xdpsq_descs(sq);
+ 	mlx5e_free_xdpsq(sq);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  }
  
 -static int mlx5e_alloc_cq(struct mlx5e_channel *c,
 -			  struct mlx5e_cq_param *param,
 -			  struct mlx5e_cq *cq)
 +static int mlx5e_create_cq(struct mlx5e_channel *c,
 +			   struct mlx5e_cq_param *param,
 +			   struct mlx5e_cq *cq)
  {
  	struct mlx5e_priv *priv = c->priv;
  	struct mlx5_core_dev *mdev = priv->mdev;
@@@ -1462,6 -1674,14 +1808,17 @@@ static int mlx5e_set_tx_maxrate(struct 
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
+ {
+ 	return is_kdump_kernel() ?
+ 		MLX5E_MIN_NUM_CHANNELS :
+ 		min_t(int, mdev->priv.eq_table.num_comp_vectors,
+ 		      MLX5E_MAX_NUM_CHANNELS);
+ }
+ 
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  static int mlx5e_open_channel(struct mlx5e_priv *priv, int ix,
  			      struct mlx5e_channel_param *cparam,
  			      struct mlx5e_channel **cp)
@@@ -1509,9 -1729,15 +1865,9 @@@
  	if (err)
  		goto err_close_tx_cqs;
  
 -	/* XDP SQ CQ params are same as normal TXQ sq CQ params */
 -	err = c->xdp ? mlx5e_open_cq(c, &cparam->tx_cq, &c->rq.xdpsq.cq,
 -				     priv->params.tx_cq_moderation) : 0;
 -	if (err)
 -		goto err_close_rx_cq;
 -
  	napi_enable(&c->napi);
  
- 	err = mlx5e_open_sq(c, 0, &cparam->icosq, &c->icosq);
+ 	err = mlx5e_open_icosq(c, 0, &cparam->icosq, &c->icosq);
  	if (err)
  		goto err_disable_napi;
  
@@@ -1537,6 -1768,9 +1894,12 @@@
  	*cp = c;
  
  	return 0;
++<<<<<<< HEAD
++=======
+ err_close_xdp_sq:
+ 	if (c->xdp)
+ 		mlx5e_close_xdpsq(&c->rq.xdpsq);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  
  err_close_sqs:
  	mlx5e_close_sqs(c);
@@@ -1564,9 -1802,13 +1927,14 @@@ err_napi_del
  static void mlx5e_close_channel(struct mlx5e_channel *c)
  {
  	mlx5e_close_rq(&c->rq);
++<<<<<<< HEAD
++=======
+ 	if (c->xdp)
+ 		mlx5e_close_xdpsq(&c->rq.xdpsq);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	mlx5e_close_sqs(c);
- 	mlx5e_close_sq(&c->icosq);
+ 	mlx5e_close_icosq(&c->icosq);
  	napi_disable(&c->napi);
 -	if (c->xdp)
 -		mlx5e_close_cq(&c->rq.xdpsq.cq);
  	mlx5e_close_cq(&c->rq.cq);
  	mlx5e_close_tx_cqs(c);
  	mlx5e_close_cq(&c->icosq.cq);
@@@ -1710,10 -1951,21 +2077,24 @@@ static void mlx5e_build_icosq_param(str
  
  	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
  	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(priv->mdev, reg_umr_sq));
+ }
+ 
++<<<<<<< HEAD
++=======
+ static void mlx5e_build_xdpsq_param(struct mlx5e_priv *priv,
+ 				    struct mlx5e_sq_param *param)
+ {
+ 	void *sqc = param->sqc;
+ 	void *wq = MLX5_ADDR_OF(sqc, sqc, wq);
  
- 	param->type = MLX5E_SQ_ICO;
+ 	mlx5e_build_sq_param_common(priv, param);
+ 	MLX5_SET(wq, wq, log_wq_sz,     priv->params.log_sq_size);
+ 
+ 	param->max_inline = priv->params.tx_max_inline;
+ 	param->min_inline_mode = priv->params.tx_min_inline_mode;
  }
  
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  static void mlx5e_build_channel_param(struct mlx5e_priv *priv, struct mlx5e_channel_param *cparam)
  {
  	u8 icosq_log_wq_sz = MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index e8c9b2d23033,3ecbe8c2d5e3..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -340,7 -341,7 +340,11 @@@ static inline void mlx5e_post_umr_wqe(s
  	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
  		sq->db.ico_wqe[pi].opcode = MLX5_OPCODE_NOP;
  		sq->db.ico_wqe[pi].num_wqebbs = 1;
++<<<<<<< HEAD
 +		mlx5e_send_nop(sq, false);
++=======
+ 		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	}
  
  	wqe = mlx5_wq_cyc_get_wqe(wq, pi);
@@@ -636,6 -637,120 +640,123 @@@ static inline void mlx5e_complete_rx_cq
  	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
  }
  
++<<<<<<< HEAD
++=======
+ static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
+ {
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u16 pi = (sq->pc - 1) & wq->sz_m1; /* last pi */
+ 
+ 	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 
+ 	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &wqe->ctrl);
+ }
+ 
+ static inline bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq,
+ 					struct mlx5e_dma_info *di,
+ 					const struct xdp_buff *xdp)
+ {
+ 	struct mlx5e_xdpsq       *sq   = &rq->xdpsq;
+ 	struct mlx5_wq_cyc       *wq   = &sq->wq;
+ 	u16                       pi   = sq->pc & wq->sz_m1;
+ 	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 
+ 	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 
+ 	ptrdiff_t data_offset = xdp->data - xdp->data_hard_start;
+ 	dma_addr_t dma_addr  = di->addr + data_offset;
+ 	unsigned int dma_len = xdp->data_end - xdp->data;
+ 
+ 	prefetchw(wqe);
+ 
+ 	if (unlikely(dma_len < MLX5E_XDP_MIN_INLINE ||
+ 		     MLX5E_SW2HW_MTU(rq->netdev->mtu) < dma_len)) {
+ 		rq->stats.xdp_drop++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return false;
+ 	}
+ 
+ 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, 1))) {
+ 		if (sq->db.doorbell) {
+ 			/* SQ is full, ring doorbell */
+ 			mlx5e_xmit_xdp_doorbell(sq);
+ 			sq->db.doorbell = false;
+ 		}
+ 		rq->stats.xdp_tx_full++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return false;
+ 	}
+ 
+ 	dma_sync_single_for_device(sq->pdev, dma_addr, dma_len, PCI_DMA_TODEVICE);
+ 
+ 	cseg->fm_ce_se = 0;
+ 
+ 	dseg = (struct mlx5_wqe_data_seg *)eseg + 1;
+ 
+ 	/* copy the inline part if required */
+ 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+ 		memcpy(eseg->inline_hdr.start, xdp->data, MLX5E_XDP_MIN_INLINE);
+ 		eseg->inline_hdr.sz = cpu_to_be16(MLX5E_XDP_MIN_INLINE);
+ 		dma_len  -= MLX5E_XDP_MIN_INLINE;
+ 		dma_addr += MLX5E_XDP_MIN_INLINE;
+ 		dseg++;
+ 	}
+ 
+ 	/* write the dma part */
+ 	dseg->addr       = cpu_to_be64(dma_addr);
+ 	dseg->byte_count = cpu_to_be32(dma_len);
+ 
+ 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_SEND);
+ 
+ 	sq->db.di[pi] = *di;
+ 	sq->pc++;
+ 
+ 	sq->db.doorbell = true;
+ 	rq->stats.xdp_tx++;
+ 	return true;
+ }
+ 
+ /* returns true if packet was consumed by xdp */
+ static inline int mlx5e_xdp_handle(struct mlx5e_rq *rq,
+ 				   struct mlx5e_dma_info *di,
+ 				   void *va, u16 *rx_headroom, u32 *len)
+ {
+ 	const struct bpf_prog *prog = READ_ONCE(rq->xdp_prog);
+ 	struct xdp_buff xdp;
+ 	u32 act;
+ 
+ 	if (!prog)
+ 		return false;
+ 
+ 	xdp.data = va + *rx_headroom;
+ 	xdp.data_end = xdp.data + *len;
+ 	xdp.data_hard_start = va;
+ 
+ 	act = bpf_prog_run_xdp(prog, &xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		*rx_headroom = xdp.data - xdp.data_hard_start;
+ 		*len = xdp.data_end - xdp.data;
+ 		return false;
+ 	case XDP_TX:
+ 		if (unlikely(!mlx5e_xmit_xdp_frame(rq, di, &xdp)))
+ 			trace_xdp_exception(rq->netdev, prog, act);
+ 		return true;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(rq->netdev, prog, act);
+ 	case XDP_DROP:
+ 		rq->stats.xdp_drop++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return true;
+ 	}
+ }
+ 
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  static inline
  struct sk_buff *skb_from_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
  			     u16 wqe_counter, u32 cqe_bcnt)
@@@ -819,6 -943,7 +940,10 @@@ mpwrq_cqe_out
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
  {
  	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
++<<<<<<< HEAD
++=======
+ 	struct mlx5e_xdpsq *xdpsq = &rq->xdpsq;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	int work_done = 0;
  
  	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))
@@@ -845,6 -970,11 +970,14 @@@
  		rq->handle_rx_cqe(rq, cqe);
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (xdpsq->db.doorbell) {
+ 		mlx5e_xmit_xdp_doorbell(xdpsq);
+ 		xdpsq->db.doorbell = false;
+ 	}
+ 
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	mlx5_cqwq_update_db_record(&cq->wq);
  
  	/* ensure cq space is freed before enabling more cqes */
@@@ -852,3 -982,74 +985,77 @@@
  
  	return work_done;
  }
++<<<<<<< HEAD
++=======
+ 
+ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)
+ {
+ 	struct mlx5e_xdpsq *sq;
+ 	struct mlx5e_rq *rq;
+ 	u16 sqcc;
+ 	int i;
+ 
+ 	sq = container_of(cq, struct mlx5e_xdpsq, cq);
+ 
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
+ 		return false;
+ 
+ 	rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 
+ 	/* sq->cc must be updated only after mlx5_cqwq_update_db_record(),
+ 	 * otherwise a cq overrun may occur
+ 	 */
+ 	sqcc = sq->cc;
+ 
+ 	for (i = 0; i < MLX5E_TX_CQ_POLL_BUDGET; i++) {
+ 		struct mlx5_cqe64 *cqe;
+ 		u16 wqe_counter;
+ 		bool last_wqe;
+ 
+ 		cqe = mlx5e_get_cqe(cq);
+ 		if (!cqe)
+ 			break;
+ 
+ 		mlx5_cqwq_pop(&cq->wq);
+ 
+ 		wqe_counter = be16_to_cpu(cqe->wqe_counter);
+ 
+ 		do {
+ 			struct mlx5e_dma_info *di;
+ 			u16 ci;
+ 
+ 			last_wqe = (sqcc == wqe_counter);
+ 
+ 			ci = sqcc & sq->wq.sz_m1;
+ 			di = &sq->db.di[ci];
+ 
+ 			sqcc++;
+ 			/* Recycle RX page */
+ 			mlx5e_page_release(rq, di, true);
+ 		} while (!last_wqe);
+ 	}
+ 
+ 	mlx5_cqwq_update_db_record(&cq->wq);
+ 
+ 	/* ensure cq space is freed before enabling more cqes */
+ 	wmb();
+ 
+ 	sq->cc = sqcc;
+ 	return (i == MLX5E_TX_CQ_POLL_BUDGET);
+ }
+ 
+ void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq)
+ {
+ 	struct mlx5e_rq *rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 	struct mlx5e_dma_info *di;
+ 	u16 ci;
+ 
+ 	while (sq->cc != sq->pc) {
+ 		ci = sq->cc & sq->wq.sz_m1;
+ 		di = &sq->db.di[ci];
+ 		sq->cc++;
+ 
+ 		mlx5e_page_release(rq, di, false);
+ 	}
+ }
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 2a270903b57d,20f71b55651e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -373,13 -320,11 +373,19 @@@ static netdev_tx_t mlx5e_sq_xmit(struc
  
  	/* fill sq edge with nops to avoid wqe wrap around */
  	while ((pi = (sq->pc & wq->sz_m1)) > sq->edge) {
++<<<<<<< HEAD
 +		sq->db.txq.skb[pi] = NULL;
 +		mlx5e_send_nop(sq, false);
++=======
+ 		sq->db.skb[pi] = NULL;
+ 		mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);
+ 		sq->stats.nop++;
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  	}
  
 +	if (bf)
 +		sq->bf_budget--;
 +
  	return NETDEV_TX_OK;
  
  dma_unmap_wqe_err:
@@@ -496,20 -441,17 +502,24 @@@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *
  	return (i == MLX5E_TX_CQ_POLL_BUDGET);
  }
  
++<<<<<<< HEAD
 +void mlx5e_free_tx_descs(struct mlx5e_sq *sq)
++=======
+ void mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq)
++>>>>>>> 3139104861d9 (net/mlx5e: Different SQ types)
  {
  	struct mlx5e_tx_wqe_info *wi;
  	struct sk_buff *skb;
  	u16 ci;
  	int i;
  
 +	if (sq->type != MLX5E_SQ_TXQ)
 +		return;
 +
  	while (sq->cc != sq->pc) {
  		ci = sq->cc & sq->wq.sz_m1;
- 		skb = sq->db.txq.skb[ci];
- 		wi = &sq->db.txq.wqe_info[ci];
+ 		skb = sq->db.skb[ci];
+ 		wi = &sq->db.wqe_info[ci];
  
  		if (!skb) { /* nop */
  			sq->cc++;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
index 8689b7bebc3c..2b843d7db153 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -51,7 +51,7 @@ struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq)
 
 static void mlx5e_poll_ico_cq(struct mlx5e_cq *cq)
 {
-	struct mlx5e_sq *sq = container_of(cq, struct mlx5e_sq, cq);
+	struct mlx5e_icosq *sq = container_of(cq, struct mlx5e_icosq, cq);
 	struct mlx5_wq_cyc *wq;
 	struct mlx5_cqe64 *cqe;
 	u16 sqcc;
