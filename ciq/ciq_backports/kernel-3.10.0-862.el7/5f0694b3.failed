device-dax: rename 'dax_dev' to 'dev_dax'

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 5f0694b300b9fb8409272c550418c22e0e57314a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/5f0694b3.failed

In preparation for introducing a struct dax_device type to the kernel
global type namespace, rename dax_dev to dev_dax. A 'dax_device'
instance will be a generic device-driver object for any provider of dax
functionality. A 'dev_dax' object is a device-dax-driver local /
internal instance.

	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 5f0694b300b9fb8409272c550418c22e0e57314a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/dax.c
diff --cc drivers/dax/dax.c
index 5e37741e7a4f,376fdd353aea..000000000000
--- a/drivers/dax/dax.c
+++ b/drivers/dax/dax.c
@@@ -428,23 -421,22 +428,33 @@@ static phys_addr_t pgoff_to_phys(struc
  	return -1;
  }
  
++<<<<<<< HEAD
 +static int __dax_dev_fault(struct dax_dev *dax_dev, struct vm_area_struct *vma,
 +		struct vm_fault *vmf)
 +{
 +	unsigned long vaddr = (unsigned long) vmf->virtual_address;
 +	struct device *dev = &dax_dev->dev;
++=======
+ static int __dev_dax_pte_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
+ {
+ 	struct device *dev = &dev_dax->dev;
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  	struct dax_region *dax_region;
  	int rc = VM_FAULT_SIGBUS;
  	phys_addr_t phys;
  	pfn_t pfn;
  	unsigned int fault_size = PAGE_SIZE;
  
++<<<<<<< HEAD
 +	if (check_vma(dax_dev, vma, __func__))
++=======
+ 	if (check_vma(dev_dax, vmf->vma, __func__))
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  		return VM_FAULT_SIGBUS;
  
- 	dax_region = dax_dev->region;
+ 	dax_region = dev_dax->region;
  	if (dax_region->align > PAGE_SIZE) {
 -		dev_dbg(dev, "%s: alignment (%#x) > fault size (%#x)\n",
 -			__func__, dax_region->align, fault_size);
 +		dev_dbg(dev, "%s: alignment > fault size\n", __func__);
  		return VM_FAULT_SIGBUS;
  	}
  
@@@ -470,40 -462,23 +480,51 @@@
  	return VM_FAULT_NOPAGE;
  }
  
++<<<<<<< HEAD
 +static int dax_dev_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 +{
 +	int rc;
 +	struct file *filp = vma->vm_file;
 +	struct dax_dev *dax_dev = filp->private_data;
 +
 +	dev_dbg(&dax_dev->dev, "%s: %s: %s (%#lx - %#lx)\n", __func__,
 +			current->comm, (vmf->flags & FAULT_FLAG_WRITE)
 +			? "write" : "read", vma->vm_start, vma->vm_end);
 +	rcu_read_lock();
 +	rc = __dax_dev_fault(dax_dev, vma, vmf);
 +	rcu_read_unlock();
 +
 +	return rc;
 +}
 +
 +static int __dax_dev_pmd_fault(struct dax_dev *dax_dev,
 +		struct vm_area_struct *vma, unsigned long addr, pmd_t *pmd,
 +		unsigned int flags)
 +{
 +	unsigned long pmd_addr = addr & PMD_MASK;
 +	struct device *dev = &dax_dev->dev;
++=======
+ static int __dev_dax_pmd_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
+ {
+ 	unsigned long pmd_addr = vmf->address & PMD_MASK;
+ 	struct device *dev = &dev_dax->dev;
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  	struct dax_region *dax_region;
  	phys_addr_t phys;
  	pgoff_t pgoff;
  	pfn_t pfn;
 -	unsigned int fault_size = PMD_SIZE;
 +	unsigned int fault_size = PAGE_SIZE;
  
++<<<<<<< HEAD
 +	if (check_vma(dax_dev, vma, __func__))
++=======
+ 	if (check_vma(dev_dax, vmf->vma, __func__))
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  		return VM_FAULT_SIGBUS;
  
- 	dax_region = dax_dev->region;
+ 	dax_region = dev_dax->region;
  	if (dax_region->align > PMD_SIZE) {
 -		dev_dbg(dev, "%s: alignment (%#x) > fault size (%#x)\n",
 -			__func__, dax_region->align, fault_size);
 +		dev_dbg(dev, "%s: alignment > fault size\n", __func__);
  		return VM_FAULT_SIGBUS;
  	}
  
@@@ -519,12 -494,12 +540,17 @@@
  		return VM_FAULT_FALLBACK;
  
  	/* if we are outside of the VMA */
 -	if (pmd_addr < vmf->vma->vm_start ||
 -			(pmd_addr + PMD_SIZE) > vmf->vma->vm_end)
 +	if (pmd_addr < vma->vm_start ||
 +			(pmd_addr + PMD_SIZE) > vma->vm_end)
  		return VM_FAULT_SIGBUS;
  
++<<<<<<< HEAD
 +	pgoff = linear_page_index(vma, pmd_addr);
 +	phys = pgoff_to_phys(dax_dev, pgoff, PMD_SIZE);
++=======
+ 	pgoff = linear_page_index(vmf->vma, pmd_addr);
+ 	phys = pgoff_to_phys(dev_dax, pgoff, PMD_SIZE);
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  	if (phys == -1) {
  		dev_dbg(dev, "%s: pgoff_to_phys(%#lx) failed\n", __func__,
  				pgoff);
@@@ -533,31 -508,107 +559,130 @@@
  
  	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
  
 -	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd, pfn,
 -			vmf->flags & FAULT_FLAG_WRITE);
 +	return vmf_insert_pfn_pmd(vma, addr, pmd, pfn,
 +			flags & FAULT_FLAG_WRITE);
  }
  
++<<<<<<< HEAD
 +static int dax_dev_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
 +		pmd_t *pmd, unsigned int flags)
 +{
 +	int rc;
 +	struct file *filp = vma->vm_file;
 +	struct dax_dev *dax_dev = filp->private_data;
 +
 +	dev_dbg(&dax_dev->dev, "%s: %s: %s (%#lx - %#lx)\n", __func__,
 +			current->comm, (flags & FAULT_FLAG_WRITE)
 +			? "write" : "read", vma->vm_start, vma->vm_end);
 +
 +	rcu_read_lock();
 +	rc = __dax_dev_pmd_fault(dax_dev, vma, addr, pmd, flags);
 +	rcu_read_unlock();
++=======
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static int __dev_dax_pud_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
+ {
+ 	unsigned long pud_addr = vmf->address & PUD_MASK;
+ 	struct device *dev = &dev_dax->dev;
+ 	struct dax_region *dax_region;
+ 	phys_addr_t phys;
+ 	pgoff_t pgoff;
+ 	pfn_t pfn;
+ 	unsigned int fault_size = PUD_SIZE;
+ 
+ 
+ 	if (check_vma(dev_dax, vmf->vma, __func__))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	dax_region = dev_dax->region;
+ 	if (dax_region->align > PUD_SIZE) {
+ 		dev_dbg(dev, "%s: alignment (%#x) > fault size (%#x)\n",
+ 			__func__, dax_region->align, fault_size);
+ 		return VM_FAULT_SIGBUS;
+ 	}
+ 
+ 	/* dax pud mappings require pfn_t_devmap() */
+ 	if ((dax_region->pfn_flags & (PFN_DEV|PFN_MAP)) != (PFN_DEV|PFN_MAP)) {
+ 		dev_dbg(dev, "%s: region lacks devmap flags\n", __func__);
+ 		return VM_FAULT_SIGBUS;
+ 	}
+ 
+ 	if (fault_size < dax_region->align)
+ 		return VM_FAULT_SIGBUS;
+ 	else if (fault_size > dax_region->align)
+ 		return VM_FAULT_FALLBACK;
+ 
+ 	/* if we are outside of the VMA */
+ 	if (pud_addr < vmf->vma->vm_start ||
+ 			(pud_addr + PUD_SIZE) > vmf->vma->vm_end)
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	pgoff = linear_page_index(vmf->vma, pud_addr);
+ 	phys = pgoff_to_phys(dev_dax, pgoff, PUD_SIZE);
+ 	if (phys == -1) {
+ 		dev_dbg(dev, "%s: pgoff_to_phys(%#lx) failed\n", __func__,
+ 				pgoff);
+ 		return VM_FAULT_SIGBUS;
+ 	}
+ 
+ 	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
+ 
+ 	return vmf_insert_pfn_pud(vmf->vma, vmf->address, vmf->pud, pfn,
+ 			vmf->flags & FAULT_FLAG_WRITE);
+ }
+ #else
+ static int __dev_dax_pud_fault(struct dev_dax *dev_dax, struct vm_fault *vmf)
+ {
+ 	return VM_FAULT_FALLBACK;
+ }
+ #endif /* !CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
+ 
+ static int dev_dax_huge_fault(struct vm_fault *vmf,
+ 		enum page_entry_size pe_size)
+ {
+ 	int rc, id;
+ 	struct file *filp = vmf->vma->vm_file;
+ 	struct dev_dax *dev_dax = filp->private_data;
+ 
+ 	dev_dbg(&dev_dax->dev, "%s: %s: %s (%#lx - %#lx) size = %d\n", __func__,
+ 			current->comm, (vmf->flags & FAULT_FLAG_WRITE)
+ 			? "write" : "read",
+ 			vmf->vma->vm_start, vmf->vma->vm_end, pe_size);
+ 
+ 	id = srcu_read_lock(&dax_srcu);
+ 	switch (pe_size) {
+ 	case PE_SIZE_PTE:
+ 		rc = __dev_dax_pte_fault(dev_dax, vmf);
+ 		break;
+ 	case PE_SIZE_PMD:
+ 		rc = __dev_dax_pmd_fault(dev_dax, vmf);
+ 		break;
+ 	case PE_SIZE_PUD:
+ 		rc = __dev_dax_pud_fault(dev_dax, vmf);
+ 		break;
+ 	default:
+ 		rc = VM_FAULT_SIGBUS;
+ 	}
+ 	srcu_read_unlock(&dax_srcu, id);
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  
  	return rc;
  }
  
++<<<<<<< HEAD
 +static const struct vm_operations_struct dax_dev_vm_ops = {
 +	.fault = dax_dev_fault,
 +	.pmd_fault = dax_dev_pmd_fault,
++=======
+ static int dev_dax_fault(struct vm_fault *vmf)
+ {
+ 	return dev_dax_huge_fault(vmf, PE_SIZE_PTE);
+ }
+ 
+ static const struct vm_operations_struct dax_vm_ops = {
+ 	.fault = dev_dax_fault,
+ 	.huge_fault = dev_dax_huge_fault,
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  };
  
  static int dax_mmap(struct file *filp, struct vm_area_struct *vma)
@@@ -643,35 -694,35 +768,45 @@@ static const struct file_operations dax
  	.mmap = dax_mmap,
  };
  
- static void dax_dev_release(struct device *dev)
+ static void dev_dax_release(struct device *dev)
  {
- 	struct dax_dev *dax_dev = to_dax_dev(dev);
- 	struct dax_region *dax_region = dax_dev->region;
+ 	struct dev_dax *dev_dax = to_dev_dax(dev);
+ 	struct dax_region *dax_region = dev_dax->region;
  
- 	ida_simple_remove(&dax_region->ida, dax_dev->id);
+ 	ida_simple_remove(&dax_region->ida, dev_dax->id);
  	ida_simple_remove(&dax_minor_ida, MINOR(dev->devt));
  	dax_region_put(dax_region);
- 	iput(dax_dev->inode);
- 	kfree(dax_dev);
+ 	iput(dev_dax->inode);
+ 	kfree(dev_dax);
  }
  
- static void kill_dax_dev(struct dax_dev *dax_dev)
+ static void kill_dev_dax(struct dev_dax *dev_dax)
  {
  	/*
- 	 * Note, rcu is not protecting the liveness of dax_dev, rcu is
+ 	 * Note, rcu is not protecting the liveness of dev_dax, rcu is
  	 * ensuring that any fault handlers that might have seen
++<<<<<<< HEAD
 +	 * dax_dev->alive == true, have completed.  Any fault handlers
 +	 * that start after synchronize_rcu() has started will abort
 +	 * upon seeing dax_dev->alive == false.
 +	 */
 +	dax_dev->alive = false;
 +	synchronize_rcu();
 +	unmap_mapping_range(dax_dev->inode->i_mapping, 0, 0, 1);
++=======
+ 	 * dev_dax->alive == true, have completed.  Any fault handlers
+ 	 * that start after synchronize_srcu() has started will abort
+ 	 * upon seeing dev_dax->alive == false.
+ 	 */
+ 	dev_dax->alive = false;
+ 	synchronize_srcu(&dax_srcu);
+ 	unmap_mapping_range(dev_dax->inode->i_mapping, 0, 0, 1);
++>>>>>>> 5f0694b300b9 (device-dax: rename 'dax_dev' to 'dev_dax')
  }
  
- static void unregister_dax_dev(void *dev)
+ static void unregister_dev_dax(void *dev)
  {
- 	struct dax_dev *dax_dev = to_dax_dev(dev);
+ 	struct dev_dax *dev_dax = to_dev_dax(dev);
  
  	dev_dbg(dev, "%s\n", __func__);
  
* Unmerged path drivers/dax/dax.c
diff --git a/drivers/dax/dax.h b/drivers/dax/dax.h
index ddd829ab58c0..ea176d875d60 100644
--- a/drivers/dax/dax.h
+++ b/drivers/dax/dax.h
@@ -13,13 +13,13 @@
 #ifndef __DAX_H__
 #define __DAX_H__
 struct device;
-struct dax_dev;
+struct dev_dax;
 struct resource;
 struct dax_region;
 void dax_region_put(struct dax_region *dax_region);
 struct dax_region *alloc_dax_region(struct device *parent,
 		int region_id, struct resource *res, unsigned int align,
 		void *addr, unsigned long flags);
-struct dax_dev *devm_create_dax_dev(struct dax_region *dax_region,
+struct dev_dax *devm_create_dev_dax(struct dax_region *dax_region,
 		struct resource *res, int count);
 #endif /* __DAX_H__ */
diff --git a/drivers/dax/pmem.c b/drivers/dax/pmem.c
index 033f49b31fdc..2c736fc4508b 100644
--- a/drivers/dax/pmem.c
+++ b/drivers/dax/pmem.c
@@ -61,8 +61,8 @@ static int dax_pmem_probe(struct device *dev)
 	int rc;
 	void *addr;
 	struct resource res;
-	struct dax_dev *dax_dev;
 	struct nd_pfn_sb *pfn_sb;
+	struct dev_dax *dev_dax;
 	struct dax_pmem *dax_pmem;
 	struct nd_region *nd_region;
 	struct nd_namespace_io *nsio;
@@ -130,12 +130,12 @@ static int dax_pmem_probe(struct device *dev)
 		return -ENOMEM;
 
 	/* TODO: support for subdividing a dax region... */
-	dax_dev = devm_create_dax_dev(dax_region, &res, 1);
+	dev_dax = devm_create_dev_dax(dax_region, &res, 1);
 
-	/* child dax_dev instances now own the lifetime of the dax_region */
+	/* child dev_dax instances now own the lifetime of the dax_region */
 	dax_region_put(dax_region);
 
-	return PTR_ERR_OR_ZERO(dax_dev);
+	return PTR_ERR_OR_ZERO(dev_dax);
 }
 
 static struct nd_device_driver dax_pmem_driver = {
