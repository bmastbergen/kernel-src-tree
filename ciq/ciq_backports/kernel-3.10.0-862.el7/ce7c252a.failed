SUNRPC: Add a separate spinlock to protect the RPC request receive list

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Trond Myklebust <trond.myklebust@primarydata.com>
commit ce7c252a8c741aba7c38f817b86e34361f561e42
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ce7c252a.failed

This further reduces contention with the transport_lock, and allows us
to convert to using a non-bh-safe spinlock, since the list is now never
accessed from a bh context.

	Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>
(cherry picked from commit ce7c252a8c741aba7c38f817b86e34361f561e42)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sunrpc/xprt.c
#	net/sunrpc/xprtrdma/rpc_rdma.c
#	net/sunrpc/xprtsock.c
diff --cc net/sunrpc/xprt.c
index e83aa477fea7,2af189c5ac3e..000000000000
--- a/net/sunrpc/xprt.c
+++ b/net/sunrpc/xprt.c
@@@ -844,6 -844,48 +844,51 @@@ struct rpc_rqst *xprt_lookup_rqst(struc
  }
  EXPORT_SYMBOL_GPL(xprt_lookup_rqst);
  
++<<<<<<< HEAD
++=======
+ /**
+  * xprt_pin_rqst - Pin a request on the transport receive list
+  * @req: Request to pin
+  *
+  * Caller must ensure this is atomic with the call to xprt_lookup_rqst()
+  * so should be holding the xprt transport lock.
+  */
+ void xprt_pin_rqst(struct rpc_rqst *req)
+ {
+ 	set_bit(RPC_TASK_MSG_RECV, &req->rq_task->tk_runstate);
+ }
+ 
+ /**
+  * xprt_unpin_rqst - Unpin a request on the transport receive list
+  * @req: Request to pin
+  *
+  * Caller should be holding the xprt transport lock.
+  */
+ void xprt_unpin_rqst(struct rpc_rqst *req)
+ {
+ 	struct rpc_task *task = req->rq_task;
+ 
+ 	clear_bit(RPC_TASK_MSG_RECV, &task->tk_runstate);
+ 	if (test_bit(RPC_TASK_MSG_RECV_WAIT, &task->tk_runstate))
+ 		wake_up_bit(&task->tk_runstate, RPC_TASK_MSG_RECV);
+ }
+ 
+ static void xprt_wait_on_pinned_rqst(struct rpc_rqst *req)
+ __must_hold(&req->rq_xprt->recv_lock)
+ {
+ 	struct rpc_task *task = req->rq_task;
+ 	
+ 	if (task && test_bit(RPC_TASK_MSG_RECV, &task->tk_runstate)) {
+ 		spin_unlock(&req->rq_xprt->recv_lock);
+ 		set_bit(RPC_TASK_MSG_RECV_WAIT, &task->tk_runstate);
+ 		wait_on_bit(&task->tk_runstate, RPC_TASK_MSG_RECV,
+ 				TASK_UNINTERRUPTIBLE);
+ 		clear_bit(RPC_TASK_MSG_RECV_WAIT, &task->tk_runstate);
+ 		spin_lock(&req->rq_xprt->recv_lock);
+ 	}
+ }
+ 
++>>>>>>> ce7c252a8c74 (SUNRPC: Add a separate spinlock to protect the RPC request receive list)
  static void xprt_update_rtt(struct rpc_task *task)
  {
  	struct rpc_rqst *req = task->tk_rqstp;
diff --cc net/sunrpc/xprtrdma/rpc_rdma.c
index 42cacbf866f4,dfa748a0c8de..000000000000
--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@@ -1121,8 -1051,8 +1121,13 @@@ rpcrdma_reply_handler(struct work_struc
  	 * RPC completion while holding the transport lock to ensure
  	 * the rep, rqst, and rq_task pointers remain stable.
  	 */
++<<<<<<< HEAD
 +	spin_lock_bh(&xprt->transport_lock);
 +	rqst = xprt_lookup_rqst(xprt, xid);
++=======
+ 	spin_lock(&xprt->recv_lock);
+ 	rqst = xprt_lookup_rqst(xprt, headerp->rm_xid);
++>>>>>>> ce7c252a8c74 (SUNRPC: Add a separate spinlock to protect the RPC request receive list)
  	if (!rqst)
  		goto out_norqst;
  	xprt->reestablish_timeout = 0;
diff --cc net/sunrpc/xprtsock.c
index e4ae7a2afb1e,2b918137aaa0..000000000000
--- a/net/sunrpc/xprtsock.c
+++ b/net/sunrpc/xprtsock.c
@@@ -951,6 -973,8 +951,11 @@@ static void xs_local_data_read_skb(stru
  	rovr = xprt_lookup_rqst(xprt, *xp);
  	if (!rovr)
  		goto out_unlock;
++<<<<<<< HEAD
++=======
+ 	xprt_pin_rqst(rovr);
+ 	spin_unlock(&xprt->recv_lock);
++>>>>>>> ce7c252a8c74 (SUNRPC: Add a separate spinlock to protect the RPC request receive list)
  	task = rovr->rq_task;
  
  	copied = rovr->rq_private_buf.buflen;
@@@ -959,13 -983,16 +964,21 @@@
  
  	if (xs_local_copy_to_xdr(&rovr->rq_private_buf, skb)) {
  		dprintk("RPC:       sk_buff copy failed\n");
++<<<<<<< HEAD
 +		goto out_unlock;
 +	}
 +
++=======
+ 		spin_lock(&xprt->recv_lock);
+ 		goto out_unpin;
+ 	}
+ 
+ 	spin_lock(&xprt->recv_lock);
++>>>>>>> ce7c252a8c74 (SUNRPC: Add a separate spinlock to protect the RPC request receive list)
  	xprt_complete_rqst(task, copied);
 -out_unpin:
 -	xprt_unpin_rqst(rovr);
 +
   out_unlock:
- 	spin_unlock_bh(&xprt->transport_lock);
+ 	spin_unlock(&xprt->recv_lock);
  }
  
  static void xs_local_data_receive(struct sock_xprt *transport)
@@@ -1033,6 -1059,8 +1046,11 @@@ static void xs_udp_data_read_skb(struc
  	rovr = xprt_lookup_rqst(xprt, *xp);
  	if (!rovr)
  		goto out_unlock;
++<<<<<<< HEAD
++=======
+ 	xprt_pin_rqst(rovr);
+ 	spin_unlock(&xprt->recv_lock);
++>>>>>>> ce7c252a8c74 (SUNRPC: Add a separate spinlock to protect the RPC request receive list)
  	task = rovr->rq_task;
  
  	if ((copied = rovr->rq_private_buf.buflen) > repsize)
@@@ -1040,17 -1068,22 +1058,25 @@@
  
  	/* Suck it into the iovec, verify checksum if not done by hw. */
  	if (csum_partial_copy_to_xdr(&rovr->rq_private_buf, skb)) {
++<<<<<<< HEAD
 +		UDPX_INC_STATS_BH(sk, UDP_MIB_INERRORS);
 +		goto out_unlock;
++=======
+ 		__UDPX_INC_STATS(sk, UDP_MIB_INERRORS);
+ 		spin_lock(&xprt->recv_lock);
+ 		goto out_unpin;
++>>>>>>> ce7c252a8c74 (SUNRPC: Add a separate spinlock to protect the RPC request receive list)
  	}
  
 -	__UDPX_INC_STATS(sk, UDP_MIB_INDATAGRAMS);
 +	UDPX_INC_STATS_BH(sk, UDP_MIB_INDATAGRAMS);
  
 -	spin_lock_bh(&xprt->transport_lock);
  	xprt_adjust_cwnd(xprt, task, copied);
+ 	spin_unlock_bh(&xprt->transport_lock);
+ 	spin_lock(&xprt->recv_lock);
  	xprt_complete_rqst(task, copied);
 -out_unpin:
 -	xprt_unpin_rqst(rovr);
 +
   out_unlock:
- 	spin_unlock_bh(&xprt->transport_lock);
+ 	spin_unlock(&xprt->recv_lock);
  }
  
  static void xs_udp_data_receive(struct sock_xprt *transport)
@@@ -1327,16 -1351,19 +1353,29 @@@ static inline int xs_tcp_read_reply(str
  	if (!req) {
  		dprintk("RPC:       XID %08x request not found!\n",
  				ntohl(transport->tcp_xid));
- 		spin_unlock_bh(&xprt->transport_lock);
+ 		spin_unlock(&xprt->recv_lock);
  		return -1;
  	}
++<<<<<<< HEAD
 +
 +	xs_tcp_read_common(xprt, desc, req);
 +
 +	if (!(transport->tcp_flags & TCP_RCV_COPY_DATA))
 +		xprt_complete_rqst(req->rq_task, transport->tcp_copied);
 +
 +	spin_unlock_bh(&xprt->transport_lock);
++=======
+ 	xprt_pin_rqst(req);
+ 	spin_unlock(&xprt->recv_lock);
+ 
+ 	xs_tcp_read_common(xprt, desc, req);
+ 
+ 	spin_lock(&xprt->recv_lock);
+ 	if (!(transport->tcp_flags & TCP_RCV_COPY_DATA))
+ 		xprt_complete_rqst(req->rq_task, transport->tcp_copied);
+ 	xprt_unpin_rqst(req);
+ 	spin_unlock(&xprt->recv_lock);
++>>>>>>> ce7c252a8c74 (SUNRPC: Add a separate spinlock to protect the RPC request receive list)
  	return 0;
  }
  
diff --git a/include/linux/sunrpc/xprt.h b/include/linux/sunrpc/xprt.h
index 184611330b55..699390cd3112 100644
--- a/include/linux/sunrpc/xprt.h
+++ b/include/linux/sunrpc/xprt.h
@@ -228,6 +228,7 @@ struct rpc_xprt {
 	 */
 	spinlock_t		transport_lock;	/* lock transport info */
 	spinlock_t		reserve_lock;	/* lock slot table */
+	spinlock_t		recv_lock;	/* lock receive list */
 	u32			xid;		/* Next XID value to use */
 	struct rpc_task *	snd_task;	/* Task blocked in send */
 	struct svc_xprt		*bc_xprt;	/* NFSv4.1 backchannel */
diff --git a/net/sunrpc/svcsock.c b/net/sunrpc/svcsock.c
index d7b80f384c98..39f0ba120be3 100644
--- a/net/sunrpc/svcsock.c
+++ b/net/sunrpc/svcsock.c
@@ -1003,7 +1003,7 @@ static int receive_cb_reply(struct svc_sock *svsk, struct svc_rqst *rqstp)
 
 	if (!bc_xprt)
 		return -EAGAIN;
-	spin_lock_bh(&bc_xprt->transport_lock);
+	spin_lock(&bc_xprt->recv_lock);
 	req = xprt_lookup_rqst(bc_xprt, xid);
 	if (!req)
 		goto unlock_notfound;
@@ -1021,7 +1021,7 @@ static int receive_cb_reply(struct svc_sock *svsk, struct svc_rqst *rqstp)
 	memcpy(dst->iov_base, src->iov_base, src->iov_len);
 	xprt_complete_rqst(req->rq_task, rqstp->rq_arg.len);
 	rqstp->rq_arg.len = 0;
-	spin_unlock_bh(&bc_xprt->transport_lock);
+	spin_unlock(&bc_xprt->recv_lock);
 	return 0;
 unlock_notfound:
 	printk(KERN_NOTICE
@@ -1030,7 +1030,7 @@ unlock_notfound:
 		__func__, ntohl(calldir),
 		bc_xprt, ntohl(xid));
 unlock_eagain:
-	spin_unlock_bh(&bc_xprt->transport_lock);
+	spin_unlock(&bc_xprt->recv_lock);
 	return -EAGAIN;
 }
 
* Unmerged path net/sunrpc/xprt.c
* Unmerged path net/sunrpc/xprtrdma/rpc_rdma.c
diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 73afdfc4ffec..0431a32e891f 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -53,7 +53,7 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	if (src->iov_len < 24)
 		goto out_shortreply;
 
-	spin_lock_bh(&xprt->transport_lock);
+	spin_lock(&xprt->recv_lock);
 	req = xprt_lookup_rqst(xprt, xid);
 	if (!req)
 		goto out_notfound;
@@ -70,17 +70,20 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)
 		credits = r_xprt->rx_buf.rb_bc_max_requests;
 
+	spin_lock_bh(&xprt->transport_lock);
 	cwnd = xprt->cwnd;
 	xprt->cwnd = credits << RPC_CWNDSHIFT;
 	if (xprt->cwnd > cwnd)
 		xprt_release_rqst_cong(req->rq_task);
+	spin_unlock_bh(&xprt->transport_lock);
+
 
 	ret = 0;
 	xprt_complete_rqst(req->rq_task, rcvbuf->len);
 	rcvbuf->len = 0;
 
 out_unlock:
-	spin_unlock_bh(&xprt->transport_lock);
+	spin_unlock(&xprt->recv_lock);
 out:
 	return ret;
 
* Unmerged path net/sunrpc/xprtsock.c
