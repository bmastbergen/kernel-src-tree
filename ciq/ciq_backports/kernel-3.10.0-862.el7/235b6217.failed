mm/swap: add cluster lock

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] swap: add cluster lock (Jerome Marchand) [1400689]
Rebuild_FUZZ: 93.62%
commit-author Huang, Ying <ying.huang@intel.com>
commit 235b62176712b970c815923e36b9a9cc05d4d901
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/235b6217.failed

This patch is to reduce the lock contention of swap_info_struct->lock
via using a more fine grained lock in swap_cluster_info for some swap
operations.  swap_info_struct->lock is heavily contended if multiple
processes reclaim pages simultaneously.  Because there is only one lock
for each swap device.  While in common configuration, there is only one
or several swap devices in the system.  The lock protects almost all
swap related operations.

In fact, many swap operations only access one element of
swap_info_struct->swap_map array.  And there is no dependency between
different elements of swap_info_struct->swap_map.  So a fine grained
lock can be used to allow parallel access to the different elements of
swap_info_struct->swap_map.

In this patch, a spinlock is added to swap_cluster_info to protect the
elements of swap_info_struct->swap_map in the swap cluster and the
fields of swap_cluster_info.  This reduced locking contention for
swap_info_struct->swap_map access greatly.

Because of the added spinlock, the size of swap_cluster_info increases
from 4 bytes to 8 bytes on the 64 bit and 32 bit system.  This will use
additional 4k RAM for every 1G swap space.

Because the size of swap_cluster_info is much smaller than the size of
the cache line (8 vs 64 on x86_64 architecture), there may be false
cache line sharing between spinlocks in swap_cluster_info.  To avoid the
false sharing in the first round of the swap cluster allocation, the
order of the swap clusters in the free clusters list is changed.  So
that, the swap_cluster_info sharing the same cache line will be placed
as far as possible.  After the first round of allocation, the order of
the clusters in free clusters list is expected to be random.  So the
false sharing should be not serious.

Compared with a previous implementation using bit_spin_lock, the
sequential swap out throughput improved about 3.2%.  Test was done on a
Xeon E5 v3 system.  The swap device used is a RAM simulated PMEM
(persistent memory) device.  To test the sequential swapping out, the
test case created 32 processes, which sequentially allocate and write to
the anonymous pages until the RAM and part of the swap device is used.

[ying.huang@intel.com: v5]
  Link: http://lkml.kernel.org/r/878tqeuuic.fsf_-_@yhuang-dev.intel.com
[minchan@kernel.org: initialize spinlock for swap_cluster_info]
  Link: http://lkml.kernel.org/r/1486434945-29753-1-git-send-email-minchan@kernel.org
[hughd@google.com: annotate nested locking for cluster lock]
  Link: http://lkml.kernel.org/r/alpine.LSU.2.11.1702161050540.21773@eggly.anvils
Link: http://lkml.kernel.org/r/dbb860bbd825b1aaba18988015e8963f263c3f0d.1484082593.git.tim.c.chen@linux.intel.com
	Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
	Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
	Signed-off-by: Minchan Kim <minchan@kernel.org>
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Cc: Aaron Lu <aaron.lu@intel.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
	Cc: Huang Ying <ying.huang@intel.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Jonathan Corbet <corbet@lwn.net> escreveu:
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Shaohua Li <shli@kernel.org>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 235b62176712b970c815923e36b9a9cc05d4d901)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swap.h
#	mm/swapfile.c
diff --cc include/linux/swap.h
index 61d41d507b58,279c7b84bd63..000000000000
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@@ -196,6 -166,44 +196,47 @@@ enum 
  #define SWAP_MAP_SHMEM	0xbf	/* Owned by shmem/tmpfs, in first swap_map */
  
  /*
++<<<<<<< HEAD
++=======
+  * We use this to track usage of a cluster. A cluster is a block of swap disk
+  * space with SWAPFILE_CLUSTER pages long and naturally aligns in disk. All
+  * free clusters are organized into a list. We fetch an entry from the list to
+  * get a free cluster.
+  *
+  * The data field stores next cluster if the cluster is free or cluster usage
+  * counter otherwise. The flags field determines if a cluster is free. This is
+  * protected by swap_info_struct.lock.
+  */
+ struct swap_cluster_info {
+ 	spinlock_t lock;	/*
+ 				 * Protect swap_cluster_info fields
+ 				 * and swap_info_struct->swap_map
+ 				 * elements correspond to the swap
+ 				 * cluster
+ 				 */
+ 	unsigned int data:24;
+ 	unsigned int flags:8;
+ };
+ #define CLUSTER_FLAG_FREE 1 /* This cluster is free */
+ #define CLUSTER_FLAG_NEXT_NULL 2 /* This cluster has no next cluster */
+ 
+ /*
+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from
+  * its own cluster and swapout sequentially. The purpose is to optimize swapout
+  * throughput.
+  */
+ struct percpu_cluster {
+ 	struct swap_cluster_info index; /* Current cluster index */
+ 	unsigned int next; /* Likely next allocation offset */
+ };
+ 
+ struct swap_cluster_list {
+ 	struct swap_cluster_info head;
+ 	struct swap_cluster_info tail;
+ };
+ 
+ /*
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
   * The in-memory structure used to track swap areas.
   */
  struct swap_info_struct {
diff --cc mm/swapfile.c
index 44c2eac6b890,eb71b5d9430b..000000000000
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@@ -202,6 -199,361 +202,364 @@@ static void discard_swap_cluster(struc
  #define SWAPFILE_CLUSTER	256
  #define LATENCY_LIMIT		256
  
++<<<<<<< HEAD
++=======
+ static inline void cluster_set_flag(struct swap_cluster_info *info,
+ 	unsigned int flag)
+ {
+ 	info->flags = flag;
+ }
+ 
+ static inline unsigned int cluster_count(struct swap_cluster_info *info)
+ {
+ 	return info->data;
+ }
+ 
+ static inline void cluster_set_count(struct swap_cluster_info *info,
+ 				     unsigned int c)
+ {
+ 	info->data = c;
+ }
+ 
+ static inline void cluster_set_count_flag(struct swap_cluster_info *info,
+ 					 unsigned int c, unsigned int f)
+ {
+ 	info->flags = f;
+ 	info->data = c;
+ }
+ 
+ static inline unsigned int cluster_next(struct swap_cluster_info *info)
+ {
+ 	return info->data;
+ }
+ 
+ static inline void cluster_set_next(struct swap_cluster_info *info,
+ 				    unsigned int n)
+ {
+ 	info->data = n;
+ }
+ 
+ static inline void cluster_set_next_flag(struct swap_cluster_info *info,
+ 					 unsigned int n, unsigned int f)
+ {
+ 	info->flags = f;
+ 	info->data = n;
+ }
+ 
+ static inline bool cluster_is_free(struct swap_cluster_info *info)
+ {
+ 	return info->flags & CLUSTER_FLAG_FREE;
+ }
+ 
+ static inline bool cluster_is_null(struct swap_cluster_info *info)
+ {
+ 	return info->flags & CLUSTER_FLAG_NEXT_NULL;
+ }
+ 
+ static inline void cluster_set_null(struct swap_cluster_info *info)
+ {
+ 	info->flags = CLUSTER_FLAG_NEXT_NULL;
+ 	info->data = 0;
+ }
+ 
+ static inline struct swap_cluster_info *lock_cluster(struct swap_info_struct *si,
+ 						     unsigned long offset)
+ {
+ 	struct swap_cluster_info *ci;
+ 
+ 	ci = si->cluster_info;
+ 	if (ci) {
+ 		ci += offset / SWAPFILE_CLUSTER;
+ 		spin_lock(&ci->lock);
+ 	}
+ 	return ci;
+ }
+ 
+ static inline void unlock_cluster(struct swap_cluster_info *ci)
+ {
+ 	if (ci)
+ 		spin_unlock(&ci->lock);
+ }
+ 
+ static inline struct swap_cluster_info *lock_cluster_or_swap_info(
+ 	struct swap_info_struct *si,
+ 	unsigned long offset)
+ {
+ 	struct swap_cluster_info *ci;
+ 
+ 	ci = lock_cluster(si, offset);
+ 	if (!ci)
+ 		spin_lock(&si->lock);
+ 
+ 	return ci;
+ }
+ 
+ static inline void unlock_cluster_or_swap_info(struct swap_info_struct *si,
+ 					       struct swap_cluster_info *ci)
+ {
+ 	if (ci)
+ 		unlock_cluster(ci);
+ 	else
+ 		spin_unlock(&si->lock);
+ }
+ 
+ static inline bool cluster_list_empty(struct swap_cluster_list *list)
+ {
+ 	return cluster_is_null(&list->head);
+ }
+ 
+ static inline unsigned int cluster_list_first(struct swap_cluster_list *list)
+ {
+ 	return cluster_next(&list->head);
+ }
+ 
+ static void cluster_list_init(struct swap_cluster_list *list)
+ {
+ 	cluster_set_null(&list->head);
+ 	cluster_set_null(&list->tail);
+ }
+ 
+ static void cluster_list_add_tail(struct swap_cluster_list *list,
+ 				  struct swap_cluster_info *ci,
+ 				  unsigned int idx)
+ {
+ 	if (cluster_list_empty(list)) {
+ 		cluster_set_next_flag(&list->head, idx, 0);
+ 		cluster_set_next_flag(&list->tail, idx, 0);
+ 	} else {
+ 		struct swap_cluster_info *ci_tail;
+ 		unsigned int tail = cluster_next(&list->tail);
+ 
+ 		/*
+ 		 * Nested cluster lock, but both cluster locks are
+ 		 * only acquired when we held swap_info_struct->lock
+ 		 */
+ 		ci_tail = ci + tail;
+ 		spin_lock_nested(&ci_tail->lock, SINGLE_DEPTH_NESTING);
+ 		cluster_set_next(ci_tail, idx);
+ 		unlock_cluster(ci_tail);
+ 		cluster_set_next_flag(&list->tail, idx, 0);
+ 	}
+ }
+ 
+ static unsigned int cluster_list_del_first(struct swap_cluster_list *list,
+ 					   struct swap_cluster_info *ci)
+ {
+ 	unsigned int idx;
+ 
+ 	idx = cluster_next(&list->head);
+ 	if (cluster_next(&list->tail) == idx) {
+ 		cluster_set_null(&list->head);
+ 		cluster_set_null(&list->tail);
+ 	} else
+ 		cluster_set_next_flag(&list->head,
+ 				      cluster_next(&ci[idx]), 0);
+ 
+ 	return idx;
+ }
+ 
+ /* Add a cluster to discard list and schedule it to do discard */
+ static void swap_cluster_schedule_discard(struct swap_info_struct *si,
+ 		unsigned int idx)
+ {
+ 	/*
+ 	 * If scan_swap_map() can't find a free cluster, it will check
+ 	 * si->swap_map directly. To make sure the discarding cluster isn't
+ 	 * taken by scan_swap_map(), mark the swap entries bad (occupied). It
+ 	 * will be cleared after discard
+ 	 */
+ 	memset(si->swap_map + idx * SWAPFILE_CLUSTER,
+ 			SWAP_MAP_BAD, SWAPFILE_CLUSTER);
+ 
+ 	cluster_list_add_tail(&si->discard_clusters, si->cluster_info, idx);
+ 
+ 	schedule_work(&si->discard_work);
+ }
+ 
+ /*
+  * Doing discard actually. After a cluster discard is finished, the cluster
+  * will be added to free cluster list. caller should hold si->lock.
+ */
+ static void swap_do_scheduled_discard(struct swap_info_struct *si)
+ {
+ 	struct swap_cluster_info *info, *ci;
+ 	unsigned int idx;
+ 
+ 	info = si->cluster_info;
+ 
+ 	while (!cluster_list_empty(&si->discard_clusters)) {
+ 		idx = cluster_list_del_first(&si->discard_clusters, info);
+ 		spin_unlock(&si->lock);
+ 
+ 		discard_swap_cluster(si, idx * SWAPFILE_CLUSTER,
+ 				SWAPFILE_CLUSTER);
+ 
+ 		spin_lock(&si->lock);
+ 		ci = lock_cluster(si, idx * SWAPFILE_CLUSTER);
+ 		cluster_set_flag(ci, CLUSTER_FLAG_FREE);
+ 		unlock_cluster(ci);
+ 		cluster_list_add_tail(&si->free_clusters, info, idx);
+ 		ci = lock_cluster(si, idx * SWAPFILE_CLUSTER);
+ 		memset(si->swap_map + idx * SWAPFILE_CLUSTER,
+ 				0, SWAPFILE_CLUSTER);
+ 		unlock_cluster(ci);
+ 	}
+ }
+ 
+ static void swap_discard_work(struct work_struct *work)
+ {
+ 	struct swap_info_struct *si;
+ 
+ 	si = container_of(work, struct swap_info_struct, discard_work);
+ 
+ 	spin_lock(&si->lock);
+ 	swap_do_scheduled_discard(si);
+ 	spin_unlock(&si->lock);
+ }
+ 
+ /*
+  * The cluster corresponding to page_nr will be used. The cluster will be
+  * removed from free cluster list and its usage counter will be increased.
+  */
+ static void inc_cluster_info_page(struct swap_info_struct *p,
+ 	struct swap_cluster_info *cluster_info, unsigned long page_nr)
+ {
+ 	unsigned long idx = page_nr / SWAPFILE_CLUSTER;
+ 
+ 	if (!cluster_info)
+ 		return;
+ 	if (cluster_is_free(&cluster_info[idx])) {
+ 		VM_BUG_ON(cluster_list_first(&p->free_clusters) != idx);
+ 		cluster_list_del_first(&p->free_clusters, cluster_info);
+ 		cluster_set_count_flag(&cluster_info[idx], 0, 0);
+ 	}
+ 
+ 	VM_BUG_ON(cluster_count(&cluster_info[idx]) >= SWAPFILE_CLUSTER);
+ 	cluster_set_count(&cluster_info[idx],
+ 		cluster_count(&cluster_info[idx]) + 1);
+ }
+ 
+ /*
+  * The cluster corresponding to page_nr decreases one usage. If the usage
+  * counter becomes 0, which means no page in the cluster is in using, we can
+  * optionally discard the cluster and add it to free cluster list.
+  */
+ static void dec_cluster_info_page(struct swap_info_struct *p,
+ 	struct swap_cluster_info *cluster_info, unsigned long page_nr)
+ {
+ 	unsigned long idx = page_nr / SWAPFILE_CLUSTER;
+ 
+ 	if (!cluster_info)
+ 		return;
+ 
+ 	VM_BUG_ON(cluster_count(&cluster_info[idx]) == 0);
+ 	cluster_set_count(&cluster_info[idx],
+ 		cluster_count(&cluster_info[idx]) - 1);
+ 
+ 	if (cluster_count(&cluster_info[idx]) == 0) {
+ 		/*
+ 		 * If the swap is discardable, prepare discard the cluster
+ 		 * instead of free it immediately. The cluster will be freed
+ 		 * after discard.
+ 		 */
+ 		if ((p->flags & (SWP_WRITEOK | SWP_PAGE_DISCARD)) ==
+ 				 (SWP_WRITEOK | SWP_PAGE_DISCARD)) {
+ 			swap_cluster_schedule_discard(p, idx);
+ 			return;
+ 		}
+ 
+ 		cluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);
+ 		cluster_list_add_tail(&p->free_clusters, cluster_info, idx);
+ 	}
+ }
+ 
+ /*
+  * It's possible scan_swap_map() uses a free cluster in the middle of free
+  * cluster list. Avoiding such abuse to avoid list corruption.
+  */
+ static bool
+ scan_swap_map_ssd_cluster_conflict(struct swap_info_struct *si,
+ 	unsigned long offset)
+ {
+ 	struct percpu_cluster *percpu_cluster;
+ 	bool conflict;
+ 
+ 	offset /= SWAPFILE_CLUSTER;
+ 	conflict = !cluster_list_empty(&si->free_clusters) &&
+ 		offset != cluster_list_first(&si->free_clusters) &&
+ 		cluster_is_free(&si->cluster_info[offset]);
+ 
+ 	if (!conflict)
+ 		return false;
+ 
+ 	percpu_cluster = this_cpu_ptr(si->percpu_cluster);
+ 	cluster_set_null(&percpu_cluster->index);
+ 	return true;
+ }
+ 
+ /*
+  * Try to get a swap entry from current cpu's swap entry pool (a cluster). This
+  * might involve allocating a new cluster for current CPU too.
+  */
+ static void scan_swap_map_try_ssd_cluster(struct swap_info_struct *si,
+ 	unsigned long *offset, unsigned long *scan_base)
+ {
+ 	struct percpu_cluster *cluster;
+ 	struct swap_cluster_info *ci;
+ 	bool found_free;
+ 	unsigned long tmp, max;
+ 
+ new_cluster:
+ 	cluster = this_cpu_ptr(si->percpu_cluster);
+ 	if (cluster_is_null(&cluster->index)) {
+ 		if (!cluster_list_empty(&si->free_clusters)) {
+ 			cluster->index = si->free_clusters.head;
+ 			cluster->next = cluster_next(&cluster->index) *
+ 					SWAPFILE_CLUSTER;
+ 		} else if (!cluster_list_empty(&si->discard_clusters)) {
+ 			/*
+ 			 * we don't have free cluster but have some clusters in
+ 			 * discarding, do discard now and reclaim them
+ 			 */
+ 			swap_do_scheduled_discard(si);
+ 			*scan_base = *offset = si->cluster_next;
+ 			goto new_cluster;
+ 		} else
+ 			return;
+ 	}
+ 
+ 	found_free = false;
+ 
+ 	/*
+ 	 * Other CPUs can use our cluster if they can't find a free cluster,
+ 	 * check if there is still free entry in the cluster
+ 	 */
+ 	tmp = cluster->next;
+ 	max = min_t(unsigned long, si->max,
+ 		    (cluster_next(&cluster->index) + 1) * SWAPFILE_CLUSTER);
+ 	if (tmp >= max) {
+ 		cluster_set_null(&cluster->index);
+ 		goto new_cluster;
+ 	}
+ 	ci = lock_cluster(si, tmp);
+ 	while (tmp < max) {
+ 		if (!si->swap_map[tmp]) {
+ 			found_free = true;
+ 			break;
+ 		}
+ 		tmp++;
+ 	}
+ 	unlock_cluster(ci);
+ 	if (!found_free) {
+ 		cluster_set_null(&cluster->index);
+ 		goto new_cluster;
+ 	}
+ 	cluster->next = tmp + 1;
+ 	*offset = tmp;
+ 	*scan_base = tmp;
+ }
+ 
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
  static unsigned long scan_swap_map(struct swap_info_struct *si,
  				   unsigned char usage)
  {
@@@ -338,6 -666,8 +701,11 @@@ checks
  		spin_unlock(&swap_avail_lock);
  	}
  	si->swap_map[offset] = usage;
++<<<<<<< HEAD
++=======
+ 	inc_cluster_info_page(si, si->cluster_info, offset);
+ 	unlock_cluster(ci);
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
  	si->cluster_next = offset + 1;
  	si->flags -= SWP_SCANNING;
  
@@@ -593,8 -897,15 +991,18 @@@ lock_swap_info
  	usage = count | has_cache;
  	p->swap_map[offset] = usage;
  
+ 	unlock_cluster(ci);
+ 
  	/* free if no reference */
  	if (!usage) {
++<<<<<<< HEAD
++=======
+ 		VM_BUG_ON(!swap_info_locked);
+ 		mem_cgroup_uncharge_swap(entry);
+ 		ci = lock_cluster(p, offset);
+ 		dec_cluster_info_page(p, p->cluster_info, offset);
+ 		unlock_cluster(ci);
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
  		if (offset < p->lowest_bit)
  			p->lowest_bit = offset;
  		if (offset > p->highest_bit) {
@@@ -641,18 -953,13 +1050,24 @@@ void swap_free(swp_entry_t entry
  /*
   * Called after dropping swapcache to decrease refcnt to swap entries.
   */
 -void swapcache_free(swp_entry_t entry)
 +void swapcache_free(swp_entry_t entry, struct page *page)
  {
  	struct swap_info_struct *p;
 +	unsigned char count;
  
++<<<<<<< HEAD
 +	p = swap_info_get(entry);
 +	if (p) {
 +		count = swap_entry_free(p, entry, SWAP_HAS_CACHE);
 +		if (page)
 +			mem_cgroup_uncharge_swapcache(page, entry, count != 0);
 +		spin_unlock(&p->lock);
 +	}
++=======
+ 	p = _swap_info_get(entry);
+ 	if (p)
+ 		swap_entry_free(p, entry, SWAP_HAS_CACHE, false);
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
  }
  
  /*
@@@ -676,6 -987,52 +1095,55 @@@ int page_swapcount(struct page *page
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * How many references to @entry are currently swapped out?
+  * This considers COUNT_CONTINUED so it returns exact answer.
+  */
+ int swp_swapcount(swp_entry_t entry)
+ {
+ 	int count, tmp_count, n;
+ 	struct swap_info_struct *p;
+ 	struct swap_cluster_info *ci;
+ 	struct page *page;
+ 	pgoff_t offset;
+ 	unsigned char *map;
+ 
+ 	p = _swap_info_get(entry);
+ 	if (!p)
+ 		return 0;
+ 
+ 	offset = swp_offset(entry);
+ 
+ 	ci = lock_cluster_or_swap_info(p, offset);
+ 
+ 	count = swap_count(p->swap_map[offset]);
+ 	if (!(count & COUNT_CONTINUED))
+ 		goto out;
+ 
+ 	count &= ~COUNT_CONTINUED;
+ 	n = SWAP_MAP_MAX + 1;
+ 
+ 	page = vmalloc_to_page(p->swap_map + offset);
+ 	offset &= ~PAGE_MASK;
+ 	VM_BUG_ON(page_private(page) != SWP_CONTINUED);
+ 
+ 	do {
+ 		page = list_next_entry(page, lru);
+ 		map = kmap_atomic(page);
+ 		tmp_count = map[offset];
+ 		kunmap_atomic(map);
+ 
+ 		count += (tmp_count & ~COUNT_CONTINUED) * n;
+ 		n *= (SWAP_CONT_MAX + 1);
+ 	} while (tmp_count & COUNT_CONTINUED);
+ out:
+ 	unlock_cluster_or_swap_info(p, ci);
+ 	return count;
+ }
+ 
+ /*
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
   * We can write to an anon page without COW if there are no other references
   * to it.  And as a side-effect, free up its swap: because the old content
   * on disk will never be read, and seeking back there to write new content
@@@ -751,11 -1126,11 +1219,11 @@@ int free_swap_and_cache(swp_entry_t ent
  
  	p = swap_info_get(entry);
  	if (p) {
- 		if (swap_entry_free(p, entry, 1) == SWAP_HAS_CACHE) {
+ 		if (swap_entry_free(p, entry, 1, true) == SWAP_HAS_CACHE) {
  			page = find_get_page(swap_address_space(entry),
 -					     swp_offset(entry));
 +						entry.val);
  			if (page && !trylock_page(page)) {
 -				put_page(page);
 +				page_cache_release(page);
  				page = NULL;
  			}
  		}
@@@ -2009,9 -2416,13 +2480,15 @@@ static int setup_swap_map_and_extents(s
  					unsigned long maxpages,
  					sector_t *span)
  {
- 	int i;
+ 	unsigned int j, k;
  	unsigned int nr_good_pages;
  	int nr_extents;
++<<<<<<< HEAD
++=======
+ 	unsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);
+ 	unsigned long col = p->cluster_next / SWAPFILE_CLUSTER % SWAP_CLUSTER_COLS;
+ 	unsigned long i, idx;
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
  
  	nr_good_pages = maxpages - 1;	/* omit header page */
  
@@@ -2039,6 -2467,24 +2516,27 @@@
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (!cluster_info)
+ 		return nr_extents;
+ 
+ 
+ 	/* Reduce false cache line sharing between cluster_info */
+ 	for (k = 0; k < SWAP_CLUSTER_COLS; k++) {
+ 		j = (k + col) % SWAP_CLUSTER_COLS;
+ 		for (i = 0; i < DIV_ROUND_UP(nr_clusters, SWAP_CLUSTER_COLS); i++) {
+ 			idx = i * SWAP_CLUSTER_COLS + j;
+ 			if (idx >= nr_clusters)
+ 				continue;
+ 			if (cluster_count(&cluster_info[idx]))
+ 				continue;
+ 			cluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);
+ 			cluster_list_add_tail(&p->free_clusters, cluster_info,
+ 					      idx);
+ 		}
+ 	}
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
  	return nr_extents;
  }
  
@@@ -2144,6 -2581,42 +2642,45 @@@ SYSCALL_DEFINE2(swapon, const char __us
  		goto bad_swap;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (bdi_cap_stable_pages_required(inode_to_bdi(inode)))
+ 		p->flags |= SWP_STABLE_WRITES;
+ 
+ 	if (p->bdev && blk_queue_nonrot(bdev_get_queue(p->bdev))) {
+ 		int cpu;
+ 		unsigned long ci, nr_cluster;
+ 
+ 		p->flags |= SWP_SOLIDSTATE;
+ 		/*
+ 		 * select a random position to start with to help wear leveling
+ 		 * SSD
+ 		 */
+ 		p->cluster_next = 1 + (prandom_u32() % p->highest_bit);
+ 		nr_cluster = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);
+ 
+ 		cluster_info = vzalloc(nr_cluster * sizeof(*cluster_info));
+ 		if (!cluster_info) {
+ 			error = -ENOMEM;
+ 			goto bad_swap;
+ 		}
+ 
+ 		for (ci = 0; ci < nr_cluster; ci++)
+ 			spin_lock_init(&((cluster_info + ci)->lock));
+ 
+ 		p->percpu_cluster = alloc_percpu(struct percpu_cluster);
+ 		if (!p->percpu_cluster) {
+ 			error = -ENOMEM;
+ 			goto bad_swap;
+ 		}
+ 		for_each_possible_cpu(cpu) {
+ 			struct percpu_cluster *cluster;
+ 			cluster = per_cpu_ptr(p->percpu_cluster, cpu);
+ 			cluster_set_null(&cluster->index);
+ 		}
+ 	}
+ 
++>>>>>>> 235b62176712 (mm/swap: add cluster lock)
  	error = swap_cgroup_swapon(p->type, maxpages);
  	if (error)
  		goto bad_swap;
@@@ -2296,12 -2764,22 +2834,12 @@@ static int __swap_duplicate(swp_entry_
  		goto bad_file;
  	p = swap_info[type];
  	offset = swp_offset(entry);
- 
- 	spin_lock(&p->lock);
  	if (unlikely(offset >= p->max))
- 		goto unlock_out;
+ 		goto out;
+ 
+ 	ci = lock_cluster_or_swap_info(p, offset);
  
  	count = p->swap_map[offset];
 -
 -	/*
 -	 * swapin_readahead() doesn't check if a swap entry is valid, so the
 -	 * swap entry could be SWAP_MAP_BAD. Check here with lock held.
 -	 */
 -	if (unlikely(swap_count(count) == SWAP_MAP_BAD)) {
 -		err = -ENOENT;
 -		goto unlock_out;
 -	}
 -
  	has_cache = count & SWAP_HAS_CACHE;
  	count &= ~SWAP_HAS_CACHE;
  	err = 0;
* Unmerged path include/linux/swap.h
* Unmerged path mm/swapfile.c
