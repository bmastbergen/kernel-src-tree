net: sched: cls_flow: no need to call tcf_exts_change for newly allocated struct

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [net] sched: cls_flow: no need to call tcf_exts_change for newly allocated struct (Ivan Vecera) [1445420]
Rebuild_FUZZ: 96.77%
commit-author Jiri Pirko <jiri@mellanox.com>
commit c09fc2e11ed1b7fc8cfa97fb1da544225fc32277
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c09fc2e1.failed

As the fnew struct just was allocated, so no need to use tcf_exts_change
to do atomic change, and we can just fill-up the unused exts struct
directly by tcf_exts_validate.

	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c09fc2e11ed1b7fc8cfa97fb1da544225fc32277)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_flow.c
diff --cc net/sched/cls_flow.c
index 109b18b99cbb,55e281b20140..000000000000
--- a/net/sched/cls_flow.c
+++ b/net/sched/cls_flow.c
@@@ -381,8 -388,6 +381,11 @@@ static int flow_change(struct net *net
  	struct flow_filter *fold, *fnew;
  	struct nlattr *opt = tca[TCA_OPTIONS];
  	struct nlattr *tb[TCA_FLOW_MAX + 1];
++<<<<<<< HEAD
 +	struct tcf_exts e;
 +	struct tcf_ematch_tree t;
++=======
++>>>>>>> c09fc2e11ed1 (net: sched: cls_flow: no need to call tcf_exts_change for newly allocated struct)
  	unsigned int nkeys = 0;
  	unsigned int perturb_period = 0;
  	u32 baseclass = 0;
@@@ -418,21 -423,21 +421,39 @@@
  			return -EOPNOTSUPP;
  	}
  
++<<<<<<< HEAD
 +	tcf_exts_init(&e, TCA_FLOW_ACT, TCA_FLOW_POLICE);
 +	err = tcf_exts_validate(net, tp, tb, tca[TCA_RATE], &e, ovr);
 +	if (err < 0)
 +		return err;
 +
 +	err = tcf_em_tree_validate(tp, tb[TCA_FLOW_EMATCHES], &t);
 +	if (err < 0)
 +		goto err1;
 +
 +	err = -ENOBUFS;
 +	fnew = kzalloc(sizeof(*fnew), GFP_KERNEL);
 +	if (!fnew)
 +		goto err2;
 +
 +	tcf_exts_init(&fnew->exts, TCA_FLOW_ACT, TCA_FLOW_POLICE);
++=======
+ 	fnew = kzalloc(sizeof(*fnew), GFP_KERNEL);
+ 	if (!fnew)
+ 		return -ENOBUFS;
+ 
+ 	err = tcf_em_tree_validate(tp, tb[TCA_FLOW_EMATCHES], &fnew->ematches);
+ 	if (err < 0)
+ 		goto err1;
+ 
+ 	err = tcf_exts_init(&fnew->exts, TCA_FLOW_ACT, TCA_FLOW_POLICE);
+ 	if (err < 0)
+ 		goto err2;
+ 
+ 	err = tcf_exts_validate(net, tp, tb, tca[TCA_RATE], &fnew->exts, ovr);
+ 	if (err < 0)
+ 		goto err2;
++>>>>>>> c09fc2e11ed1 (net: sched: cls_flow: no need to call tcf_exts_change for newly allocated struct)
  
  	fold = (struct flow_filter *)*arg;
  	if (fold) {
@@@ -500,9 -505,6 +521,12 @@@
  	setup_deferrable_timer(&fnew->perturb_timer, flow_perturbation,
  			       (unsigned long)fnew);
  
++<<<<<<< HEAD
 +	tcf_exts_change(tp, &fnew->exts, &e);
 +	tcf_em_tree_change(tp, &fnew->ematches, &t);
 +
++=======
++>>>>>>> c09fc2e11ed1 (net: sched: cls_flow: no need to call tcf_exts_change for newly allocated struct)
  	netif_keep_dst(qdisc_dev(tp->q));
  
  	if (tb[TCA_FLOW_KEYS]) {
@@@ -542,10 -544,10 +566,15 @@@
  	return 0;
  
  err2:
++<<<<<<< HEAD
 +	tcf_em_tree_destroy(&t);
 +	kfree(fnew);
++=======
+ 	tcf_exts_destroy(&fnew->exts);
+ 	tcf_em_tree_destroy(&fnew->ematches);
++>>>>>>> c09fc2e11ed1 (net: sched: cls_flow: no need to call tcf_exts_change for newly allocated struct)
  err1:
- 	tcf_exts_destroy(&e);
+ 	kfree(fnew);
  	return err;
  }
  
* Unmerged path net/sched/cls_flow.c
