fs: Fix page cache inconsistency when mixing buffered and AIO DIO

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [fs] Fix page cache inconsistency when mixing buffered and AIO DIO (Lukas Czerner) [1457517]
Rebuild_FUZZ: 96.83%
commit-author Lukas Czerner <lczerner@redhat.com>
commit 332391a9935da939319e473b4680e173df75afcf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/332391a9.failed

Currently when mixing buffered reads and asynchronous direct writes it
is possible to end up with the situation where we have stale data in the
page cache while the new data is already written to disk. This is
permanent until the affected pages are flushed away. Despite the fact
that mixing buffered and direct IO is ill-advised it does pose a thread
for a data integrity, is unexpected and should be fixed.

Fix this by deferring completion of asynchronous direct writes to a
process context in the case that there are mapped pages to be found in
the inode. Later before the completion in dio_complete() invalidate
the pages in question. This ensures that after the completion the pages
in the written area are either unmapped, or populated with up-to-date
data. Also do the same for the iomap case which uses
iomap_dio_complete() instead.

This has a side effect of deferring the completion to a process context
for every AIO DIO that happens on inode that has pages mapped. However
since the consensus is that this is ill-advised practice the performance
implication should not be a problem.

This was based on proposal from Jeff Moyer, thanks!

	Reviewed-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
	Signed-off-by: Lukas Czerner <lczerner@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 332391a9935da939319e473b4680e173df75afcf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/direct-io.c
#	fs/iomap.c
#	mm/filemap.c
diff --cc fs/direct-io.c
index 5682ec0743e0,62cf812ed0e5..000000000000
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@@ -271,10 -225,11 +271,11 @@@ static void dio_iodone2_helper(struct d
   * filesystems can use it to hold additional state between get_block calls and
   * dio_complete.
   */
 -static ssize_t dio_complete(struct dio *dio, ssize_t ret, bool is_async)
 +static ssize_t dio_complete(struct dio *dio, loff_t offset, ssize_t ret,
 +		bool is_async)
  {
 -	loff_t offset = dio->iocb->ki_pos;
  	ssize_t transferred = 0;
+ 	int err;
  
  	/*
  	 * AIO submission can race with bio completion to get here while
@@@ -301,18 -260,43 +302,58 @@@
  		ret = transferred;
  
  	/*
++<<<<<<< HEAD
 +	 * Red Hat only: we have to support two calling conventions for
 +	 * dio_iodone_t functions:
 +	 * 1) the original, where the routine will call aio_complete and
 +	 * 2) the new calling convention, where that is done in the
 +	 *    generic code.
 +	 * Differentiate between the two cases using an inode flag that
 +	 * gets populated for all in-tree file systems.
 +	 */
 +	if (dio->inode->i_sb->s_type->fs_flags & FS_HAS_DIO_IODONE2)
 +		dio_iodone2_helper(dio, offset, transferred, ret, is_async);
 +	else
 +		dio_iodone_helper(dio, offset, transferred, ret, is_async);
++=======
+ 	 * Try again to invalidate clean pages which might have been cached by
+ 	 * non-direct readahead, or faulted in by get_user_pages() if the source
+ 	 * of the write was an mmap'ed region of the file we're writing.  Either
+ 	 * one is a pretty crazy thing to do, so we don't support it 100%.  If
+ 	 * this invalidation fails, tough, the write still worked...
+ 	 */
+ 	if (ret > 0 && dio->op == REQ_OP_WRITE &&
+ 	    dio->inode->i_mapping->nrpages) {
+ 		err = invalidate_inode_pages2_range(dio->inode->i_mapping,
+ 					offset >> PAGE_SHIFT,
+ 					(offset + ret - 1) >> PAGE_SHIFT);
+ 		WARN_ON_ONCE(err);
+ 	}
+ 
+ 	if (dio->end_io) {
+ 
+ 		// XXX: ki_pos??
+ 		err = dio->end_io(dio->iocb, offset, ret, dio->private);
+ 		if (err)
+ 			ret = err;
+ 	}
+ 
+ 	if (!(dio->flags & DIO_SKIP_DIO_COUNT))
+ 		inode_dio_end(dio->inode);
+ 
+ 	if (is_async) {
+ 		/*
+ 		 * generic_write_sync expects ki_pos to have been updated
+ 		 * already, but the submission path only does this for
+ 		 * synchronous I/O.
+ 		 */
+ 		dio->iocb->ki_pos += transferred;
+ 
+ 		if (dio->op == REQ_OP_WRITE)
+ 			ret = generic_write_sync(dio->iocb,  transferred);
+ 		dio->iocb->ki_complete(dio->iocb, ret, 0);
+ 	}
++>>>>>>> 332391a9935d (fs: Fix page cache inconsistency when mixing buffered and AIO DIO)
  
  	kmem_cache_free(dio_cache, dio);
  	return ret;
@@@ -1263,11 -1238,19 +1317,27 @@@ do_blockdev_direct_IO(int rw, struct ki
  	 * For AIO O_(D)SYNC writes we need to defer completions to a workqueue
  	 * so that we can call ->fsync.
  	 */
++<<<<<<< HEAD
 +	if ((dio->inode->i_sb->s_type->fs_flags & FS_HAS_DIO_IODONE2) &&
 +	    dio->is_async && (rw & WRITE) &&
 +	    ((iocb->ki_filp->f_flags & O_DSYNC) ||
 +	     IS_SYNC(iocb->ki_filp->f_mapping->host))) {
 +		retval = dio_set_defer_completion(dio);
++=======
+ 	if (dio->is_async && iov_iter_rw(iter) == WRITE) {
+ 		retval = 0;
+ 		if ((iocb->ki_filp->f_flags & O_DSYNC) ||
+ 		    IS_SYNC(iocb->ki_filp->f_mapping->host))
+ 			retval = dio_set_defer_completion(dio);
+ 		else if (!dio->inode->i_sb->s_dio_done_wq) {
+ 			/*
+ 			 * In case of AIO write racing with buffered read we
+ 			 * need to defer completion. We can't decide this now,
+ 			 * however the workqueue needs to be initialized here.
+ 			 */
+ 			retval = sb_init_dio_done_wq(dio->inode->i_sb);
+ 		}
++>>>>>>> 332391a9935d (fs: Fix page cache inconsistency when mixing buffered and AIO DIO)
  		if (retval) {
  			/*
  			 * We grab i_mutex only for reads so we don't have
diff --cc fs/iomap.c
index f6bfcdf64146,8194d30bdca0..000000000000
--- a/fs/iomap.c
+++ b/fs/iomap.c
@@@ -589,3 -583,485 +589,488 @@@ int iomap_fiemap(struct inode *inode, s
  	return 0;
  }
  EXPORT_SYMBOL_GPL(iomap_fiemap);
++<<<<<<< HEAD
++=======
+ 
+ static loff_t
+ iomap_seek_hole_actor(struct inode *inode, loff_t offset, loff_t length,
+ 		      void *data, struct iomap *iomap)
+ {
+ 	switch (iomap->type) {
+ 	case IOMAP_UNWRITTEN:
+ 		offset = page_cache_seek_hole_data(inode, offset, length,
+ 						   SEEK_HOLE);
+ 		if (offset < 0)
+ 			return length;
+ 		/* fall through */
+ 	case IOMAP_HOLE:
+ 		*(loff_t *)data = offset;
+ 		return 0;
+ 	default:
+ 		return length;
+ 	}
+ }
+ 
+ loff_t
+ iomap_seek_hole(struct inode *inode, loff_t offset, const struct iomap_ops *ops)
+ {
+ 	loff_t size = i_size_read(inode);
+ 	loff_t length = size - offset;
+ 	loff_t ret;
+ 
+ 	/* Nothing to be found before or beyond the end of the file. */
+ 	if (offset < 0 || offset >= size)
+ 		return -ENXIO;
+ 
+ 	while (length > 0) {
+ 		ret = iomap_apply(inode, offset, length, IOMAP_REPORT, ops,
+ 				  &offset, iomap_seek_hole_actor);
+ 		if (ret < 0)
+ 			return ret;
+ 		if (ret == 0)
+ 			break;
+ 
+ 		offset += ret;
+ 		length -= ret;
+ 	}
+ 
+ 	return offset;
+ }
+ EXPORT_SYMBOL_GPL(iomap_seek_hole);
+ 
+ static loff_t
+ iomap_seek_data_actor(struct inode *inode, loff_t offset, loff_t length,
+ 		      void *data, struct iomap *iomap)
+ {
+ 	switch (iomap->type) {
+ 	case IOMAP_HOLE:
+ 		return length;
+ 	case IOMAP_UNWRITTEN:
+ 		offset = page_cache_seek_hole_data(inode, offset, length,
+ 						   SEEK_DATA);
+ 		if (offset < 0)
+ 			return length;
+ 		/*FALLTHRU*/
+ 	default:
+ 		*(loff_t *)data = offset;
+ 		return 0;
+ 	}
+ }
+ 
+ loff_t
+ iomap_seek_data(struct inode *inode, loff_t offset, const struct iomap_ops *ops)
+ {
+ 	loff_t size = i_size_read(inode);
+ 	loff_t length = size - offset;
+ 	loff_t ret;
+ 
+ 	/* Nothing to be found before or beyond the end of the file. */
+ 	if (offset < 0 || offset >= size)
+ 		return -ENXIO;
+ 
+ 	while (length > 0) {
+ 		ret = iomap_apply(inode, offset, length, IOMAP_REPORT, ops,
+ 				  &offset, iomap_seek_data_actor);
+ 		if (ret < 0)
+ 			return ret;
+ 		if (ret == 0)
+ 			break;
+ 
+ 		offset += ret;
+ 		length -= ret;
+ 	}
+ 
+ 	if (length <= 0)
+ 		return -ENXIO;
+ 	return offset;
+ }
+ EXPORT_SYMBOL_GPL(iomap_seek_data);
+ 
+ /*
+  * Private flags for iomap_dio, must not overlap with the public ones in
+  * iomap.h:
+  */
+ #define IOMAP_DIO_WRITE		(1 << 30)
+ #define IOMAP_DIO_DIRTY		(1 << 31)
+ 
+ struct iomap_dio {
+ 	struct kiocb		*iocb;
+ 	iomap_dio_end_io_t	*end_io;
+ 	loff_t			i_size;
+ 	loff_t			size;
+ 	atomic_t		ref;
+ 	unsigned		flags;
+ 	int			error;
+ 
+ 	union {
+ 		/* used during submission and for synchronous completion: */
+ 		struct {
+ 			struct iov_iter		*iter;
+ 			struct task_struct	*waiter;
+ 			struct request_queue	*last_queue;
+ 			blk_qc_t		cookie;
+ 		} submit;
+ 
+ 		/* used for aio completion: */
+ 		struct {
+ 			struct work_struct	work;
+ 		} aio;
+ 	};
+ };
+ 
+ static ssize_t iomap_dio_complete(struct iomap_dio *dio)
+ {
+ 	struct kiocb *iocb = dio->iocb;
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 	ssize_t ret;
+ 
+ 	/*
+ 	 * Try again to invalidate clean pages which might have been cached by
+ 	 * non-direct readahead, or faulted in by get_user_pages() if the source
+ 	 * of the write was an mmap'ed region of the file we're writing.  Either
+ 	 * one is a pretty crazy thing to do, so we don't support it 100%.  If
+ 	 * this invalidation fails, tough, the write still worked...
+ 	 */
+ 	if (!dio->error &&
+ 	    (dio->flags & IOMAP_DIO_WRITE) && inode->i_mapping->nrpages) {
+ 		ret = invalidate_inode_pages2_range(inode->i_mapping,
+ 				iocb->ki_pos >> PAGE_SHIFT,
+ 				(iocb->ki_pos + dio->size - 1) >> PAGE_SHIFT);
+ 		WARN_ON_ONCE(ret);
+ 	}
+ 
+ 	if (dio->end_io) {
+ 		ret = dio->end_io(iocb,
+ 				dio->error ? dio->error : dio->size,
+ 				dio->flags);
+ 	} else {
+ 		ret = dio->error;
+ 	}
+ 
+ 	if (likely(!ret)) {
+ 		ret = dio->size;
+ 		/* check for short read */
+ 		if (iocb->ki_pos + ret > dio->i_size &&
+ 		    !(dio->flags & IOMAP_DIO_WRITE))
+ 			ret = dio->i_size - iocb->ki_pos;
+ 		iocb->ki_pos += ret;
+ 	}
+ 
+ 	inode_dio_end(file_inode(iocb->ki_filp));
+ 	kfree(dio);
+ 
+ 	return ret;
+ }
+ 
+ static void iomap_dio_complete_work(struct work_struct *work)
+ {
+ 	struct iomap_dio *dio = container_of(work, struct iomap_dio, aio.work);
+ 	struct kiocb *iocb = dio->iocb;
+ 	bool is_write = (dio->flags & IOMAP_DIO_WRITE);
+ 	ssize_t ret;
+ 
+ 	ret = iomap_dio_complete(dio);
+ 	if (is_write && ret > 0)
+ 		ret = generic_write_sync(iocb, ret);
+ 	iocb->ki_complete(iocb, ret, 0);
+ }
+ 
+ /*
+  * Set an error in the dio if none is set yet.  We have to use cmpxchg
+  * as the submission context and the completion context(s) can race to
+  * update the error.
+  */
+ static inline void iomap_dio_set_error(struct iomap_dio *dio, int ret)
+ {
+ 	cmpxchg(&dio->error, 0, ret);
+ }
+ 
+ static void iomap_dio_bio_end_io(struct bio *bio)
+ {
+ 	struct iomap_dio *dio = bio->bi_private;
+ 	bool should_dirty = (dio->flags & IOMAP_DIO_DIRTY);
+ 
+ 	if (bio->bi_status)
+ 		iomap_dio_set_error(dio, blk_status_to_errno(bio->bi_status));
+ 
+ 	if (atomic_dec_and_test(&dio->ref)) {
+ 		if (is_sync_kiocb(dio->iocb)) {
+ 			struct task_struct *waiter = dio->submit.waiter;
+ 
+ 			WRITE_ONCE(dio->submit.waiter, NULL);
+ 			wake_up_process(waiter);
+ 		} else if (dio->flags & IOMAP_DIO_WRITE) {
+ 			struct inode *inode = file_inode(dio->iocb->ki_filp);
+ 
+ 			INIT_WORK(&dio->aio.work, iomap_dio_complete_work);
+ 			queue_work(inode->i_sb->s_dio_done_wq, &dio->aio.work);
+ 		} else {
+ 			iomap_dio_complete_work(&dio->aio.work);
+ 		}
+ 	}
+ 
+ 	if (should_dirty) {
+ 		bio_check_pages_dirty(bio);
+ 	} else {
+ 		struct bio_vec *bvec;
+ 		int i;
+ 
+ 		bio_for_each_segment_all(bvec, bio, i)
+ 			put_page(bvec->bv_page);
+ 		bio_put(bio);
+ 	}
+ }
+ 
+ static blk_qc_t
+ iomap_dio_zero(struct iomap_dio *dio, struct iomap *iomap, loff_t pos,
+ 		unsigned len)
+ {
+ 	struct page *page = ZERO_PAGE(0);
+ 	struct bio *bio;
+ 
+ 	bio = bio_alloc(GFP_KERNEL, 1);
+ 	bio_set_dev(bio, iomap->bdev);
+ 	bio->bi_iter.bi_sector =
+ 		iomap->blkno + ((pos - iomap->offset) >> 9);
+ 	bio->bi_private = dio;
+ 	bio->bi_end_io = iomap_dio_bio_end_io;
+ 
+ 	get_page(page);
+ 	if (bio_add_page(bio, page, len, 0) != len)
+ 		BUG();
+ 	bio_set_op_attrs(bio, REQ_OP_WRITE, REQ_SYNC | REQ_IDLE);
+ 
+ 	atomic_inc(&dio->ref);
+ 	return submit_bio(bio);
+ }
+ 
+ static loff_t
+ iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
+ 		void *data, struct iomap *iomap)
+ {
+ 	struct iomap_dio *dio = data;
+ 	unsigned int blkbits = blksize_bits(bdev_logical_block_size(iomap->bdev));
+ 	unsigned int fs_block_size = i_blocksize(inode), pad;
+ 	unsigned int align = iov_iter_alignment(dio->submit.iter);
+ 	struct iov_iter iter;
+ 	struct bio *bio;
+ 	bool need_zeroout = false;
+ 	int nr_pages, ret;
+ 
+ 	if ((pos | length | align) & ((1 << blkbits) - 1))
+ 		return -EINVAL;
+ 
+ 	switch (iomap->type) {
+ 	case IOMAP_HOLE:
+ 		if (WARN_ON_ONCE(dio->flags & IOMAP_DIO_WRITE))
+ 			return -EIO;
+ 		/*FALLTHRU*/
+ 	case IOMAP_UNWRITTEN:
+ 		if (!(dio->flags & IOMAP_DIO_WRITE)) {
+ 			iov_iter_zero(length, dio->submit.iter);
+ 			dio->size += length;
+ 			return length;
+ 		}
+ 		dio->flags |= IOMAP_DIO_UNWRITTEN;
+ 		need_zeroout = true;
+ 		break;
+ 	case IOMAP_MAPPED:
+ 		if (iomap->flags & IOMAP_F_SHARED)
+ 			dio->flags |= IOMAP_DIO_COW;
+ 		if (iomap->flags & IOMAP_F_NEW)
+ 			need_zeroout = true;
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		return -EIO;
+ 	}
+ 
+ 	/*
+ 	 * Operate on a partial iter trimmed to the extent we were called for.
+ 	 * We'll update the iter in the dio once we're done with this extent.
+ 	 */
+ 	iter = *dio->submit.iter;
+ 	iov_iter_truncate(&iter, length);
+ 
+ 	nr_pages = iov_iter_npages(&iter, BIO_MAX_PAGES);
+ 	if (nr_pages <= 0)
+ 		return nr_pages;
+ 
+ 	if (need_zeroout) {
+ 		/* zero out from the start of the block to the write offset */
+ 		pad = pos & (fs_block_size - 1);
+ 		if (pad)
+ 			iomap_dio_zero(dio, iomap, pos - pad, pad);
+ 	}
+ 
+ 	do {
+ 		if (dio->error)
+ 			return 0;
+ 
+ 		bio = bio_alloc(GFP_KERNEL, nr_pages);
+ 		bio_set_dev(bio, iomap->bdev);
+ 		bio->bi_iter.bi_sector =
+ 			iomap->blkno + ((pos - iomap->offset) >> 9);
+ 		bio->bi_write_hint = dio->iocb->ki_hint;
+ 		bio->bi_private = dio;
+ 		bio->bi_end_io = iomap_dio_bio_end_io;
+ 
+ 		ret = bio_iov_iter_get_pages(bio, &iter);
+ 		if (unlikely(ret)) {
+ 			bio_put(bio);
+ 			return ret;
+ 		}
+ 
+ 		if (dio->flags & IOMAP_DIO_WRITE) {
+ 			bio_set_op_attrs(bio, REQ_OP_WRITE, REQ_SYNC | REQ_IDLE);
+ 			task_io_account_write(bio->bi_iter.bi_size);
+ 		} else {
+ 			bio_set_op_attrs(bio, REQ_OP_READ, 0);
+ 			if (dio->flags & IOMAP_DIO_DIRTY)
+ 				bio_set_pages_dirty(bio);
+ 		}
+ 
+ 		dio->size += bio->bi_iter.bi_size;
+ 		pos += bio->bi_iter.bi_size;
+ 
+ 		nr_pages = iov_iter_npages(&iter, BIO_MAX_PAGES);
+ 
+ 		atomic_inc(&dio->ref);
+ 
+ 		dio->submit.last_queue = bdev_get_queue(iomap->bdev);
+ 		dio->submit.cookie = submit_bio(bio);
+ 	} while (nr_pages);
+ 
+ 	if (need_zeroout) {
+ 		/* zero out from the end of the write to the end of the block */
+ 		pad = pos & (fs_block_size - 1);
+ 		if (pad)
+ 			iomap_dio_zero(dio, iomap, pos, fs_block_size - pad);
+ 	}
+ 
+ 	iov_iter_advance(dio->submit.iter, length);
+ 	return length;
+ }
+ 
+ ssize_t
+ iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
+ 		const struct iomap_ops *ops, iomap_dio_end_io_t end_io)
+ {
+ 	struct address_space *mapping = iocb->ki_filp->f_mapping;
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 	size_t count = iov_iter_count(iter);
+ 	loff_t pos = iocb->ki_pos, start = pos;
+ 	loff_t end = iocb->ki_pos + count - 1, ret = 0;
+ 	unsigned int flags = IOMAP_DIRECT;
+ 	struct blk_plug plug;
+ 	struct iomap_dio *dio;
+ 
+ 	lockdep_assert_held(&inode->i_rwsem);
+ 
+ 	if (!count)
+ 		return 0;
+ 
+ 	dio = kmalloc(sizeof(*dio), GFP_KERNEL);
+ 	if (!dio)
+ 		return -ENOMEM;
+ 
+ 	dio->iocb = iocb;
+ 	atomic_set(&dio->ref, 1);
+ 	dio->size = 0;
+ 	dio->i_size = i_size_read(inode);
+ 	dio->end_io = end_io;
+ 	dio->error = 0;
+ 	dio->flags = 0;
+ 
+ 	dio->submit.iter = iter;
+ 	if (is_sync_kiocb(iocb)) {
+ 		dio->submit.waiter = current;
+ 		dio->submit.cookie = BLK_QC_T_NONE;
+ 		dio->submit.last_queue = NULL;
+ 	}
+ 
+ 	if (iov_iter_rw(iter) == READ) {
+ 		if (pos >= dio->i_size)
+ 			goto out_free_dio;
+ 
+ 		if (iter->type == ITER_IOVEC)
+ 			dio->flags |= IOMAP_DIO_DIRTY;
+ 	} else {
+ 		dio->flags |= IOMAP_DIO_WRITE;
+ 		flags |= IOMAP_WRITE;
+ 	}
+ 
+ 	if (iocb->ki_flags & IOCB_NOWAIT) {
+ 		if (filemap_range_has_page(mapping, start, end)) {
+ 			ret = -EAGAIN;
+ 			goto out_free_dio;
+ 		}
+ 		flags |= IOMAP_NOWAIT;
+ 	}
+ 
+ 	ret = filemap_write_and_wait_range(mapping, start, end);
+ 	if (ret)
+ 		goto out_free_dio;
+ 
+ 	ret = invalidate_inode_pages2_range(mapping,
+ 			start >> PAGE_SHIFT, end >> PAGE_SHIFT);
+ 	WARN_ON_ONCE(ret);
+ 	ret = 0;
+ 
+ 	inode_dio_begin(inode);
+ 
+ 	blk_start_plug(&plug);
+ 	do {
+ 		ret = iomap_apply(inode, pos, count, flags, ops, dio,
+ 				iomap_dio_actor);
+ 		if (ret <= 0) {
+ 			/* magic error code to fall back to buffered I/O */
+ 			if (ret == -ENOTBLK)
+ 				ret = 0;
+ 			break;
+ 		}
+ 		pos += ret;
+ 
+ 		if (iov_iter_rw(iter) == READ && pos >= dio->i_size)
+ 			break;
+ 	} while ((count = iov_iter_count(iter)) > 0);
+ 	blk_finish_plug(&plug);
+ 
+ 	if (ret < 0)
+ 		iomap_dio_set_error(dio, ret);
+ 
+ 	if (ret >= 0 && iov_iter_rw(iter) == WRITE && !is_sync_kiocb(iocb) &&
+ 			!inode->i_sb->s_dio_done_wq) {
+ 		ret = sb_init_dio_done_wq(inode->i_sb);
+ 		if (ret < 0)
+ 			iomap_dio_set_error(dio, ret);
+ 	}
+ 
+ 	if (!atomic_dec_and_test(&dio->ref)) {
+ 		if (!is_sync_kiocb(iocb))
+ 			return -EIOCBQUEUED;
+ 
+ 		for (;;) {
+ 			set_current_state(TASK_UNINTERRUPTIBLE);
+ 			if (!READ_ONCE(dio->submit.waiter))
+ 				break;
+ 
+ 			if (!(iocb->ki_flags & IOCB_HIPRI) ||
+ 			    !dio->submit.last_queue ||
+ 			    !blk_mq_poll(dio->submit.last_queue,
+ 					 dio->submit.cookie))
+ 				io_schedule();
+ 		}
+ 		__set_current_state(TASK_RUNNING);
+ 	}
+ 
+ 	ret = iomap_dio_complete(dio);
+ 
+ 	return ret;
+ 
+ out_free_dio:
+ 	kfree(dio);
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(iomap_dio_rw);
++>>>>>>> 332391a9935d (fs: Fix page cache inconsistency when mixing buffered and AIO DIO)
diff --cc mm/filemap.c
index 685e2bed3093,db250d0e0565..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -2882,11 -2926,15 +2882,22 @@@ generic_file_direct_write(struct kiocb 
  	 * we're writing.  Either one is a pretty crazy thing to do,
  	 * so we don't support it 100%.  If this invalidation
  	 * fails, tough, the write still worked...
+ 	 *
+ 	 * Most of the time we do not need this since dio_complete() will do
+ 	 * the invalidation for us. However there are some file systems that
+ 	 * do not end up with dio_complete() being called, so let's not break
+ 	 * them by removing it completely
  	 */
++<<<<<<< HEAD
 +	if (mapping->nrpages) {
 +		invalidate_inode_pages2_range(mapping,
 +					      pos >> PAGE_CACHE_SHIFT, end);
 +	}
++=======
+ 	if (mapping->nrpages)
+ 		invalidate_inode_pages2_range(mapping,
+ 					pos >> PAGE_SHIFT, end);
++>>>>>>> 332391a9935d (fs: Fix page cache inconsistency when mixing buffered and AIO DIO)
  
  	if (written > 0) {
  		pos += written;
* Unmerged path fs/direct-io.c
* Unmerged path fs/iomap.c
* Unmerged path mm/filemap.c
