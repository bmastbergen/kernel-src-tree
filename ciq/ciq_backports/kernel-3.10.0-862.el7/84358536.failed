xfs: actually report xattr extents via iomap

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Darrick J. Wong <darrick.wong@oracle.com>
commit 84358536dc355a9c8978ee425f87e116186bed16
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/84358536.failed

Apparently FIEMAP for xattrs has been broken since we switched to
the iomap backend because of an incorrect check for xattr presence.
Also fix the broken locking.

	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 84358536dc355a9c8978ee425f87e116186bed16)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_iomap.c
diff --cc fs/xfs/xfs_iomap.c
index 2f3719461cbd,b2f0901bb517..000000000000
--- a/fs/xfs/xfs_iomap.c
+++ b/fs/xfs/xfs_iomap.c
@@@ -942,28 -939,259 +942,96 @@@ error_on_bmapi_transaction
  	return error;
  }
  
 -static inline bool imap_needs_alloc(struct inode *inode,
 -		struct xfs_bmbt_irec *imap, int nimaps)
 -{
 -	return !nimaps ||
 -		imap->br_startblock == HOLESTARTBLOCK ||
 -		imap->br_startblock == DELAYSTARTBLOCK ||
 -		(IS_DAX(inode) && imap->br_state == XFS_EXT_UNWRITTEN);
 -}
 -
 -static inline bool need_excl_ilock(struct xfs_inode *ip, unsigned flags)
 -{
 -	/*
 -	 * COW writes will allocate delalloc space, so we need to make sure
 -	 * to take the lock exclusively here.
 -	 */
 -	if (xfs_is_reflink_inode(ip) && (flags & (IOMAP_WRITE | IOMAP_ZERO)))
 -		return true;
 -	if ((flags & IOMAP_DIRECT) && (flags & IOMAP_WRITE))
 -		return true;
 -	return false;
 -}
 -
 -static int
 -xfs_file_iomap_begin(
 -	struct inode		*inode,
 -	loff_t			offset,
 -	loff_t			length,
 -	unsigned		flags,
 -	struct iomap		*iomap)
 -{
 -	struct xfs_inode	*ip = XFS_I(inode);
 -	struct xfs_mount	*mp = ip->i_mount;
 -	struct xfs_bmbt_irec	imap;
 -	xfs_fileoff_t		offset_fsb, end_fsb;
 -	int			nimaps = 1, error = 0;
 -	bool			shared = false, trimmed = false;
 -	unsigned		lockmode;
 -
 -	if (XFS_FORCED_SHUTDOWN(mp))
 -		return -EIO;
 -
 -	if (((flags & (IOMAP_WRITE | IOMAP_DIRECT)) == IOMAP_WRITE) &&
 -			!IS_DAX(inode) && !xfs_get_extsz_hint(ip)) {
 -		/* Reserve delalloc blocks for regular writeback. */
 -		return xfs_file_iomap_begin_delay(inode, offset, length, flags,
 -				iomap);
 -	}
 -
 -	if (need_excl_ilock(ip, flags)) {
 -		lockmode = XFS_ILOCK_EXCL;
 -		xfs_ilock(ip, XFS_ILOCK_EXCL);
 -	} else {
 -		lockmode = xfs_ilock_data_map_shared(ip);
 -	}
 -
 -	ASSERT(offset <= mp->m_super->s_maxbytes);
 -	if ((xfs_fsize_t)offset + length > mp->m_super->s_maxbytes)
 -		length = mp->m_super->s_maxbytes - offset;
 -	offset_fsb = XFS_B_TO_FSBT(mp, offset);
 -	end_fsb = XFS_B_TO_FSB(mp, offset + length);
 -
 -	error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb, &imap,
 -			       &nimaps, 0);
 -	if (error)
 -		goto out_unlock;
 -
 -	if (flags & IOMAP_REPORT) {
 -		/* Trim the mapping to the nearest shared extent boundary. */
 -		error = xfs_reflink_trim_around_shared(ip, &imap, &shared,
 -				&trimmed);
 -		if (error)
 -			goto out_unlock;
 -	}
 -
 -	if ((flags & (IOMAP_WRITE | IOMAP_ZERO)) && xfs_is_reflink_inode(ip)) {
 -		if (flags & IOMAP_DIRECT) {
 -			/* may drop and re-acquire the ilock */
 -			error = xfs_reflink_allocate_cow(ip, &imap, &shared,
 -					&lockmode);
 -			if (error)
 -				goto out_unlock;
 -		} else {
 -			error = xfs_reflink_reserve_cow(ip, &imap, &shared);
 -			if (error)
 -				goto out_unlock;
 -		}
 -
 -		end_fsb = imap.br_startoff + imap.br_blockcount;
 -		length = XFS_FSB_TO_B(mp, end_fsb) - offset;
 -	}
 -
 -	if ((flags & IOMAP_WRITE) && imap_needs_alloc(inode, &imap, nimaps)) {
 -		/*
 -		 * We cap the maximum length we map here to MAX_WRITEBACK_PAGES
 -		 * pages to keep the chunks of work done where somewhat symmetric
 -		 * with the work writeback does. This is a completely arbitrary
 -		 * number pulled out of thin air as a best guess for initial
 -		 * testing.
 -		 *
 -		 * Note that the values needs to be less than 32-bits wide until
 -		 * the lower level functions are updated.
 -		 */
 -		length = min_t(loff_t, length, 1024 * PAGE_SIZE);
 -		/*
 -		 * xfs_iomap_write_direct() expects the shared lock. It
 -		 * is unlocked on return.
 -		 */
 -		if (lockmode == XFS_ILOCK_EXCL)
 -			xfs_ilock_demote(ip, lockmode);
 -		error = xfs_iomap_write_direct(ip, offset, length, &imap,
 -				nimaps);
 -		if (error)
 -			return error;
 -
 -		iomap->flags = IOMAP_F_NEW;
 -		trace_xfs_iomap_alloc(ip, offset, length, 0, &imap);
 -	} else {
 -		ASSERT(nimaps);
 -
 -		xfs_iunlock(ip, lockmode);
 -		trace_xfs_iomap_found(ip, offset, length, 0, &imap);
 -	}
 -
 -	xfs_bmbt_to_iomap(ip, iomap, &imap);
 -	if (shared)
 -		iomap->flags |= IOMAP_F_SHARED;
 -	return 0;
 -out_unlock:
 -	xfs_iunlock(ip, lockmode);
 -	return error;
 -}
 -
 -static int
 -xfs_file_iomap_end_delalloc(
 +void
 +xfs_bmbt_to_iomap(
  	struct xfs_inode	*ip,
 -	loff_t			offset,
 -	loff_t			length,
 -	ssize_t			written,
 -	struct iomap		*iomap)
 +	struct iomap		*iomap,
 +	struct xfs_bmbt_irec	*imap)
  {
  	struct xfs_mount	*mp = ip->i_mount;
 -	xfs_fileoff_t		start_fsb;
 -	xfs_fileoff_t		end_fsb;
 -	int			error = 0;
 -
 -	/*
 -	 * Behave as if the write failed if drop writes is enabled. Set the NEW
 -	 * flag to force delalloc cleanup.
 -	 */
 -	if (xfs_mp_drop_writes(mp)) {
 -		iomap->flags |= IOMAP_F_NEW;
 -		written = 0;
 -	}
 -
 -	/*
 -	 * start_fsb refers to the first unused block after a short write. If
 -	 * nothing was written, round offset down to point at the first block in
 -	 * the range.
 -	 */
 -	if (unlikely(!written))
 -		start_fsb = XFS_B_TO_FSBT(mp, offset);
 -	else
 -		start_fsb = XFS_B_TO_FSB(mp, offset + written);
 -	end_fsb = XFS_B_TO_FSB(mp, offset + length);
 -
 -	/*
 -	 * Trim delalloc blocks if they were allocated by this write and we
 -	 * didn't manage to write the whole range.
 -	 *
 -	 * We don't need to care about racing delalloc as we hold i_mutex
 -	 * across the reserve/allocate/unreserve calls. If there are delalloc
 -	 * blocks in the range, they are ours.
 -	 */
 -	if ((iomap->flags & IOMAP_F_NEW) && start_fsb < end_fsb) {
 -		truncate_pagecache_range(VFS_I(ip), XFS_FSB_TO_B(mp, start_fsb),
 -					 XFS_FSB_TO_B(mp, end_fsb) - 1);
  
 -		xfs_ilock(ip, XFS_ILOCK_EXCL);
 -		error = xfs_bmap_punch_delalloc_range(ip, start_fsb,
 -					       end_fsb - start_fsb);
 -		xfs_iunlock(ip, XFS_ILOCK_EXCL);
 -
 -		if (error && !XFS_FORCED_SHUTDOWN(mp)) {
 -			xfs_alert(mp, "%s: unable to clean up ino %lld",
 -				__func__, ip->i_ino);
 -			return error;
 -		}
 +	if (imap->br_startblock == HOLESTARTBLOCK) {
 +		iomap->blkno = IOMAP_NULL_BLOCK;
 +		iomap->type = IOMAP_HOLE;
 +	} else if (imap->br_startblock == DELAYSTARTBLOCK) {
 +		iomap->blkno = IOMAP_NULL_BLOCK;
 +		iomap->type = IOMAP_DELALLOC;
 +	} else {
 +		iomap->blkno = xfs_fsb_to_db(ip, imap->br_startblock);
 +		if (imap->br_state == XFS_EXT_UNWRITTEN)
 +			iomap->type = IOMAP_UNWRITTEN;
 +		else
 +			iomap->type = IOMAP_MAPPED;
  	}
 -
 -	return 0;
 +	iomap->offset = XFS_FSB_TO_B(mp, imap->br_startoff);
 +	iomap->length = XFS_FSB_TO_B(mp, imap->br_blockcount);
 +	iomap->bdev = xfs_find_bdev_for_inode(VFS_I(ip));
  }
++<<<<<<< HEAD
++=======
+ 
+ static int
+ xfs_file_iomap_end(
+ 	struct inode		*inode,
+ 	loff_t			offset,
+ 	loff_t			length,
+ 	ssize_t			written,
+ 	unsigned		flags,
+ 	struct iomap		*iomap)
+ {
+ 	if ((flags & IOMAP_WRITE) && iomap->type == IOMAP_DELALLOC)
+ 		return xfs_file_iomap_end_delalloc(XFS_I(inode), offset,
+ 				length, written, iomap);
+ 	return 0;
+ }
+ 
+ const struct iomap_ops xfs_iomap_ops = {
+ 	.iomap_begin		= xfs_file_iomap_begin,
+ 	.iomap_end		= xfs_file_iomap_end,
+ };
+ 
+ static int
+ xfs_xattr_iomap_begin(
+ 	struct inode		*inode,
+ 	loff_t			offset,
+ 	loff_t			length,
+ 	unsigned		flags,
+ 	struct iomap		*iomap)
+ {
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	xfs_fileoff_t		offset_fsb = XFS_B_TO_FSBT(mp, offset);
+ 	xfs_fileoff_t		end_fsb = XFS_B_TO_FSB(mp, offset + length);
+ 	struct xfs_bmbt_irec	imap;
+ 	int			nimaps = 1, error = 0;
+ 	unsigned		lockmode;
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		return -EIO;
+ 
+ 	lockmode = xfs_ilock_attr_map_shared(ip);
+ 
+ 	/* if there are no attribute fork or extents, return ENOENT */
+ 	if (!XFS_IFORK_Q(ip) || !ip->i_d.di_anextents) {
+ 		error = -ENOENT;
+ 		goto out_unlock;
+ 	}
+ 
+ 	ASSERT(ip->i_d.di_aformat != XFS_DINODE_FMT_LOCAL);
+ 	error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb, &imap,
+ 			       &nimaps, XFS_BMAPI_ENTIRE | XFS_BMAPI_ATTRFORK);
+ out_unlock:
+ 	xfs_iunlock(ip, lockmode);
+ 
+ 	if (!error) {
+ 		ASSERT(nimaps);
+ 		xfs_bmbt_to_iomap(ip, iomap, &imap);
+ 	}
+ 
+ 	return error;
+ }
+ 
+ const struct iomap_ops xfs_xattr_iomap_ops = {
+ 	.iomap_begin		= xfs_xattr_iomap_begin,
+ };
++>>>>>>> 84358536dc35 (xfs: actually report xattr extents via iomap)
* Unmerged path fs/xfs/xfs_iomap.c
