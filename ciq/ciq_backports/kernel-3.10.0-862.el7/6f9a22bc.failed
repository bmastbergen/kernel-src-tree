PCI/MSI: Ignore affinity if pre/post vector count is more than min_vecs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Michael Hernandez <michael.hernandez@cavium.com>
commit 6f9a22bc5775d231ab8fbe2c2f3c88e45e3e7c28
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6f9a22bc.failed

min_vecs is the minimum amount of vectors needed to operate in MSI-X mode
which may just include the vectors that don't need affinity.

Disabling affinity settings causes the qla2xxx driver scsi_add_host() to fail
when blk_mq is enabled as the blk_mq_pci_map_queues() expects affinity masks
on each vector.

Fixes: dfef358bd1be ("PCI/MSI: Don't apply affinity if there aren't enough vectors left")
	Signed-off-by: Michael Hernandez <michael.hernandez@cavium.com>
	Signed-off-by: Himanshu Madhani <himanshu.madhani@cavium.com>
	Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Cc: stable@vger.kernel.org	# v4.10+
(cherry picked from commit 6f9a22bc5775d231ab8fbe2c2f3c88e45e3e7c28)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/pci/msi.c
#	include/linux/interrupt.h
#	kernel/irq/affinity.c
diff --cc drivers/pci/msi.c
index 0a99aff13490,9e1569107cd6..000000000000
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@@ -1108,25 -1050,70 +1108,71 @@@ int pci_enable_msi_range(struct pci_de
  	nvec = pci_msi_vec_count(dev);
  	if (nvec < 0)
  		return nvec;
 -	if (nvec < minvec)
 -		return -ENOSPC;
 -
 -	if (nvec > maxvec)
 +	else if (nvec < minvec)
 +		return -EINVAL;
 +	else if (nvec > maxvec)
  		nvec = maxvec;
  
++<<<<<<< HEAD
 +	do {
 +		rc = msi_capability_init(dev, nvec);
 +		if (rc < 0) {
 +			return rc;
 +		} else if (rc > 0) {
 +			if (rc < minvec)
++=======
+ 	for (;;) {
+ 		if (affd) {
+ 			nvec = irq_calc_affinity_vectors(minvec, nvec, affd);
+ 			if (nvec < minvec)
+ 				return -ENOSPC;
+ 		}
+ 
+ 		rc = msi_capability_init(dev, nvec, affd);
+ 		if (rc == 0)
+ 			return nvec;
+ 
+ 		if (rc < 0)
+ 			return rc;
+ 		if (rc < minvec)
+ 			return -ENOSPC;
+ 
+ 		nvec = rc;
+ 	}
+ }
+ 
+ /* deprecated, don't use */
+ int pci_enable_msi(struct pci_dev *dev)
+ {
+ 	int rc = __pci_enable_msi_range(dev, 1, 1, NULL);
+ 	if (rc < 0)
+ 		return rc;
+ 	return 0;
+ }
+ EXPORT_SYMBOL(pci_enable_msi);
+ 
+ static int __pci_enable_msix_range(struct pci_dev *dev,
+ 				   struct msix_entry *entries, int minvec,
+ 				   int maxvec, const struct irq_affinity *affd)
+ {
+ 	int rc, nvec = maxvec;
+ 
+ 	if (maxvec < minvec)
+ 		return -ERANGE;
+ 
+ 	for (;;) {
+ 		if (affd) {
+ 			nvec = irq_calc_affinity_vectors(minvec, nvec, affd);
+ 			if (nvec < minvec)
++>>>>>>> 6f9a22bc5775 (PCI/MSI: Ignore affinity if pre/post vector count is more than min_vecs)
  				return -ENOSPC;
 +			nvec = rc;
  		}
 +	} while (rc);
  
 -		rc = __pci_enable_msix(dev, entries, nvec, affd);
 -		if (rc == 0)
 -			return nvec;
 -
 -		if (rc < 0)
 -			return rc;
 -		if (rc < minvec)
 -			return -ENOSPC;
 -
 -		nvec = rc;
 -	}
 +	return nvec;
  }
 +EXPORT_SYMBOL(pci_enable_msi_range);
  
  /**
   * pci_enable_msix_range - configure device's MSI-X capability structure
@@@ -1144,25 -1131,382 +1190,402 @@@
   * with new allocated MSI-X interrupts.
   **/
  int pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries,
 -		int minvec, int maxvec)
 +			       int minvec, int maxvec)
  {
 -	return __pci_enable_msix_range(dev, entries, minvec, maxvec, NULL);
 +	int nvec = maxvec;
 +	int rc;
 +
 +	if (maxvec < minvec)
 +		return -ERANGE;
 +
 +	do {
 +		rc = pci_enable_msix(dev, entries, nvec);
 +		if (rc < 0) {
 +			return rc;
 +		} else if (rc > 0) {
 +			if (rc < minvec)
 +				return -ENOSPC;
 +			nvec = rc;
 +		}
 +	} while (rc);
 +
 +	return nvec;
  }
  EXPORT_SYMBOL(pci_enable_msix_range);
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * pci_alloc_irq_vectors_affinity - allocate multiple IRQs for a device
+  * @dev:		PCI device to operate on
+  * @min_vecs:		minimum number of vectors required (must be >= 1)
+  * @max_vecs:		maximum (desired) number of vectors
+  * @flags:		flags or quirks for the allocation
+  * @affd:		optional description of the affinity requirements
+  *
+  * Allocate up to @max_vecs interrupt vectors for @dev, using MSI-X or MSI
+  * vectors if available, and fall back to a single legacy vector
+  * if neither is available.  Return the number of vectors allocated,
+  * (which might be smaller than @max_vecs) if successful, or a negative
+  * error code on error. If less than @min_vecs interrupt vectors are
+  * available for @dev the function will fail with -ENOSPC.
+  *
+  * To get the Linux IRQ number used for a vector that can be passed to
+  * request_irq() use the pci_irq_vector() helper.
+  */
+ int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
+ 				   unsigned int max_vecs, unsigned int flags,
+ 				   const struct irq_affinity *affd)
+ {
+ 	static const struct irq_affinity msi_default_affd;
+ 	int vecs = -ENOSPC;
+ 
+ 	if (flags & PCI_IRQ_AFFINITY) {
+ 		if (!affd)
+ 			affd = &msi_default_affd;
+ 	} else {
+ 		if (WARN_ON(affd))
+ 			affd = NULL;
+ 	}
+ 
+ 	if (flags & PCI_IRQ_MSIX) {
+ 		vecs = __pci_enable_msix_range(dev, NULL, min_vecs, max_vecs,
+ 				affd);
+ 		if (vecs > 0)
+ 			return vecs;
+ 	}
+ 
+ 	if (flags & PCI_IRQ_MSI) {
+ 		vecs = __pci_enable_msi_range(dev, min_vecs, max_vecs, affd);
+ 		if (vecs > 0)
+ 			return vecs;
+ 	}
+ 
+ 	/* use legacy irq if allowed */
+ 	if (flags & PCI_IRQ_LEGACY) {
+ 		if (min_vecs == 1 && dev->irq) {
+ 			pci_intx(dev, 1);
+ 			return 1;
+ 		}
+ 	}
+ 
+ 	return vecs;
+ }
+ EXPORT_SYMBOL(pci_alloc_irq_vectors_affinity);
+ 
+ /**
+  * pci_free_irq_vectors - free previously allocated IRQs for a device
+  * @dev:		PCI device to operate on
+  *
+  * Undoes the allocations and enabling in pci_alloc_irq_vectors().
+  */
+ void pci_free_irq_vectors(struct pci_dev *dev)
+ {
+ 	pci_disable_msix(dev);
+ 	pci_disable_msi(dev);
+ }
+ EXPORT_SYMBOL(pci_free_irq_vectors);
+ 
+ /**
+  * pci_irq_vector - return Linux IRQ number of a device vector
+  * @dev: PCI device to operate on
+  * @nr: device-relative interrupt vector index (0-based).
+  */
+ int pci_irq_vector(struct pci_dev *dev, unsigned int nr)
+ {
+ 	if (dev->msix_enabled) {
+ 		struct msi_desc *entry;
+ 		int i = 0;
+ 
+ 		for_each_pci_msi_entry(entry, dev) {
+ 			if (i == nr)
+ 				return entry->irq;
+ 			i++;
+ 		}
+ 		WARN_ON_ONCE(1);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (dev->msi_enabled) {
+ 		struct msi_desc *entry = first_pci_msi_entry(dev);
+ 
+ 		if (WARN_ON_ONCE(nr >= entry->nvec_used))
+ 			return -EINVAL;
+ 	} else {
+ 		if (WARN_ON_ONCE(nr > 0))
+ 			return -EINVAL;
+ 	}
+ 
+ 	return dev->irq + nr;
+ }
+ EXPORT_SYMBOL(pci_irq_vector);
+ 
+ /**
+  * pci_irq_get_affinity - return the affinity of a particular msi vector
+  * @dev:	PCI device to operate on
+  * @nr:		device-relative interrupt vector index (0-based).
+  */
+ const struct cpumask *pci_irq_get_affinity(struct pci_dev *dev, int nr)
+ {
+ 	if (dev->msix_enabled) {
+ 		struct msi_desc *entry;
+ 		int i = 0;
+ 
+ 		for_each_pci_msi_entry(entry, dev) {
+ 			if (i == nr)
+ 				return entry->affinity;
+ 			i++;
+ 		}
+ 		WARN_ON_ONCE(1);
+ 		return NULL;
+ 	} else if (dev->msi_enabled) {
+ 		struct msi_desc *entry = first_pci_msi_entry(dev);
+ 
+ 		if (WARN_ON_ONCE(!entry || !entry->affinity ||
+ 				 nr >= entry->nvec_used))
+ 			return NULL;
+ 
+ 		return &entry->affinity[nr];
+ 	} else {
+ 		return cpu_possible_mask;
+ 	}
+ }
+ EXPORT_SYMBOL(pci_irq_get_affinity);
+ 
+ /**
+  * pci_irq_get_node - return the numa node of a particular msi vector
+  * @pdev:	PCI device to operate on
+  * @vec:	device-relative interrupt vector index (0-based).
+  */
+ int pci_irq_get_node(struct pci_dev *pdev, int vec)
+ {
+ 	const struct cpumask *mask;
+ 
+ 	mask = pci_irq_get_affinity(pdev, vec);
+ 	if (mask)
+ 		return local_memory_node(cpu_to_node(cpumask_first(mask)));
+ 	return dev_to_node(&pdev->dev);
+ }
+ EXPORT_SYMBOL(pci_irq_get_node);
+ 
+ struct pci_dev *msi_desc_to_pci_dev(struct msi_desc *desc)
+ {
+ 	return to_pci_dev(desc->dev);
+ }
+ EXPORT_SYMBOL(msi_desc_to_pci_dev);
+ 
+ void *msi_desc_to_pci_sysdata(struct msi_desc *desc)
+ {
+ 	struct pci_dev *dev = msi_desc_to_pci_dev(desc);
+ 
+ 	return dev->bus->sysdata;
+ }
+ EXPORT_SYMBOL_GPL(msi_desc_to_pci_sysdata);
+ 
+ #ifdef CONFIG_PCI_MSI_IRQ_DOMAIN
+ /**
+  * pci_msi_domain_write_msg - Helper to write MSI message to PCI config space
+  * @irq_data:	Pointer to interrupt data of the MSI interrupt
+  * @msg:	Pointer to the message
+  */
+ void pci_msi_domain_write_msg(struct irq_data *irq_data, struct msi_msg *msg)
+ {
+ 	struct msi_desc *desc = irq_data_get_msi_desc(irq_data);
+ 
+ 	/*
+ 	 * For MSI-X desc->irq is always equal to irq_data->irq. For
+ 	 * MSI only the first interrupt of MULTI MSI passes the test.
+ 	 */
+ 	if (desc->irq == irq_data->irq)
+ 		__pci_write_msi_msg(desc, msg);
+ }
+ 
+ /**
+  * pci_msi_domain_calc_hwirq - Generate a unique ID for an MSI source
+  * @dev:	Pointer to the PCI device
+  * @desc:	Pointer to the msi descriptor
+  *
+  * The ID number is only used within the irqdomain.
+  */
+ irq_hw_number_t pci_msi_domain_calc_hwirq(struct pci_dev *dev,
+ 					  struct msi_desc *desc)
+ {
+ 	return (irq_hw_number_t)desc->msi_attrib.entry_nr |
+ 		PCI_DEVID(dev->bus->number, dev->devfn) << 11 |
+ 		(pci_domain_nr(dev->bus) & 0xFFFFFFFF) << 27;
+ }
+ 
+ static inline bool pci_msi_desc_is_multi_msi(struct msi_desc *desc)
+ {
+ 	return !desc->msi_attrib.is_msix && desc->nvec_used > 1;
+ }
+ 
+ /**
+  * pci_msi_domain_check_cap - Verify that @domain supports the capabilities for @dev
+  * @domain:	The interrupt domain to check
+  * @info:	The domain info for verification
+  * @dev:	The device to check
+  *
+  * Returns:
+  *  0 if the functionality is supported
+  *  1 if Multi MSI is requested, but the domain does not support it
+  *  -ENOTSUPP otherwise
+  */
+ int pci_msi_domain_check_cap(struct irq_domain *domain,
+ 			     struct msi_domain_info *info, struct device *dev)
+ {
+ 	struct msi_desc *desc = first_pci_msi_entry(to_pci_dev(dev));
+ 
+ 	/* Special handling to support __pci_enable_msi_range() */
+ 	if (pci_msi_desc_is_multi_msi(desc) &&
+ 	    !(info->flags & MSI_FLAG_MULTI_PCI_MSI))
+ 		return 1;
+ 	else if (desc->msi_attrib.is_msix && !(info->flags & MSI_FLAG_PCI_MSIX))
+ 		return -ENOTSUPP;
+ 
+ 	return 0;
+ }
+ 
+ static int pci_msi_domain_handle_error(struct irq_domain *domain,
+ 				       struct msi_desc *desc, int error)
+ {
+ 	/* Special handling to support __pci_enable_msi_range() */
+ 	if (pci_msi_desc_is_multi_msi(desc) && error == -ENOSPC)
+ 		return 1;
+ 
+ 	return error;
+ }
+ 
+ #ifdef GENERIC_MSI_DOMAIN_OPS
+ static void pci_msi_domain_set_desc(msi_alloc_info_t *arg,
+ 				    struct msi_desc *desc)
+ {
+ 	arg->desc = desc;
+ 	arg->hwirq = pci_msi_domain_calc_hwirq(msi_desc_to_pci_dev(desc),
+ 					       desc);
+ }
+ #else
+ #define pci_msi_domain_set_desc		NULL
+ #endif
+ 
+ static struct msi_domain_ops pci_msi_domain_ops_default = {
+ 	.set_desc	= pci_msi_domain_set_desc,
+ 	.msi_check	= pci_msi_domain_check_cap,
+ 	.handle_error	= pci_msi_domain_handle_error,
+ };
+ 
+ static void pci_msi_domain_update_dom_ops(struct msi_domain_info *info)
+ {
+ 	struct msi_domain_ops *ops = info->ops;
+ 
+ 	if (ops == NULL) {
+ 		info->ops = &pci_msi_domain_ops_default;
+ 	} else {
+ 		if (ops->set_desc == NULL)
+ 			ops->set_desc = pci_msi_domain_set_desc;
+ 		if (ops->msi_check == NULL)
+ 			ops->msi_check = pci_msi_domain_check_cap;
+ 		if (ops->handle_error == NULL)
+ 			ops->handle_error = pci_msi_domain_handle_error;
+ 	}
+ }
+ 
+ static void pci_msi_domain_update_chip_ops(struct msi_domain_info *info)
+ {
+ 	struct irq_chip *chip = info->chip;
+ 
+ 	BUG_ON(!chip);
+ 	if (!chip->irq_write_msi_msg)
+ 		chip->irq_write_msi_msg = pci_msi_domain_write_msg;
+ 	if (!chip->irq_mask)
+ 		chip->irq_mask = pci_msi_mask_irq;
+ 	if (!chip->irq_unmask)
+ 		chip->irq_unmask = pci_msi_unmask_irq;
+ }
+ 
+ /**
+  * pci_msi_create_irq_domain - Create a MSI interrupt domain
+  * @fwnode:	Optional fwnode of the interrupt controller
+  * @info:	MSI domain info
+  * @parent:	Parent irq domain
+  *
+  * Updates the domain and chip ops and creates a MSI interrupt domain.
+  *
+  * Returns:
+  * A domain pointer or NULL in case of failure.
+  */
+ struct irq_domain *pci_msi_create_irq_domain(struct fwnode_handle *fwnode,
+ 					     struct msi_domain_info *info,
+ 					     struct irq_domain *parent)
+ {
+ 	struct irq_domain *domain;
+ 
+ 	if (info->flags & MSI_FLAG_USE_DEF_DOM_OPS)
+ 		pci_msi_domain_update_dom_ops(info);
+ 	if (info->flags & MSI_FLAG_USE_DEF_CHIP_OPS)
+ 		pci_msi_domain_update_chip_ops(info);
+ 
+ 	info->flags |= MSI_FLAG_ACTIVATE_EARLY;
+ 
+ 	domain = msi_create_irq_domain(fwnode, info, parent);
+ 	if (!domain)
+ 		return NULL;
+ 
+ 	domain->bus_token = DOMAIN_BUS_PCI_MSI;
+ 	return domain;
+ }
+ EXPORT_SYMBOL_GPL(pci_msi_create_irq_domain);
+ 
+ static int get_msi_id_cb(struct pci_dev *pdev, u16 alias, void *data)
+ {
+ 	u32 *pa = data;
+ 
+ 	*pa = alias;
+ 	return 0;
+ }
+ /**
+  * pci_msi_domain_get_msi_rid - Get the MSI requester id (RID)
+  * @domain:	The interrupt domain
+  * @pdev:	The PCI device.
+  *
+  * The RID for a device is formed from the alias, with a firmware
+  * supplied mapping applied
+  *
+  * Returns: The RID.
+  */
+ u32 pci_msi_domain_get_msi_rid(struct irq_domain *domain, struct pci_dev *pdev)
+ {
+ 	struct device_node *of_node;
+ 	u32 rid = 0;
+ 
+ 	pci_for_each_dma_alias(pdev, get_msi_id_cb, &rid);
+ 
+ 	of_node = irq_domain_get_of_node(domain);
+ 	rid = of_node ? of_msi_map_rid(&pdev->dev, of_node, rid) :
+ 			iort_msi_map_rid(&pdev->dev, rid);
+ 
+ 	return rid;
+ }
+ 
+ /**
+  * pci_msi_get_device_domain - Get the MSI domain for a given PCI device
+  * @pdev:	The PCI device
+  *
+  * Use the firmware data to find a device-specific MSI domain
+  * (i.e. not one that is ste as a default).
+  *
+  * Returns: The coresponding MSI domain or NULL if none has been found.
+  */
+ struct irq_domain *pci_msi_get_device_domain(struct pci_dev *pdev)
+ {
+ 	struct irq_domain *dom;
+ 	u32 rid = 0;
+ 
+ 	pci_for_each_dma_alias(pdev, get_msi_id_cb, &rid);
+ 	dom = of_msi_map_get_device_domain(&pdev->dev, rid);
+ 	if (!dom)
+ 		dom = iort_get_device_domain(&pdev->dev, rid);
+ 	return dom;
+ }
+ #endif /* CONFIG_PCI_MSI_IRQ_DOMAIN */
++>>>>>>> 6f9a22bc5775 (PCI/MSI: Ignore affinity if pre/post vector count is more than min_vecs)
diff --cc include/linux/interrupt.h
index 775c38a0baff,0991f973f8ca..000000000000
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@@ -299,6 -290,9 +299,12 @@@ struct irq_affinity_notify 
  extern int
  irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify);
  
++<<<<<<< HEAD
++=======
+ struct cpumask *irq_create_affinity_masks(int nvec, const struct irq_affinity *affd);
+ int irq_calc_affinity_vectors(int minvec, int maxvec, const struct irq_affinity *affd);
+ 
++>>>>>>> 6f9a22bc5775 (PCI/MSI: Ignore affinity if pre/post vector count is more than min_vecs)
  #else /* CONFIG_SMP */
  
  static inline int irq_set_affinity(unsigned int irq, const struct cpumask *m)
@@@ -318,9 -317,27 +324,32 @@@ static inline int irq_set_affinity_hint
  {
  	return -EINVAL;
  }
 +#endif /* CONFIG_SMP && CONFIG_GENERIC_HARDIRQS */
  
++<<<<<<< HEAD
 +#ifdef CONFIG_GENERIC_HARDIRQS
++=======
+ static inline int
+ irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)
+ {
+ 	return 0;
+ }
+ 
+ static inline struct cpumask *
+ irq_create_affinity_masks(int nvec, const struct irq_affinity *affd)
+ {
+ 	return NULL;
+ }
+ 
+ static inline int
+ irq_calc_affinity_vectors(int minvec, int maxvec, const struct irq_affinity *affd)
+ {
+ 	return maxvec;
+ }
+ 
+ #endif /* CONFIG_SMP */
+ 
++>>>>>>> 6f9a22bc5775 (PCI/MSI: Ignore affinity if pre/post vector count is more than min_vecs)
  /*
   * Special lockdep variants of irq disabling/enabling.
   * These should be used for locking constructs that
* Unmerged path kernel/irq/affinity.c
* Unmerged path drivers/pci/msi.c
* Unmerged path include/linux/interrupt.h
* Unmerged path kernel/irq/affinity.c
