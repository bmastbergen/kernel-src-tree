cgroup: kill css_id

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Li Zefan <lizefan@huawei.com>
commit 2ff2a7d03bbe472ed44a8380dbdbea490d81c59d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/2ff2a7d0.failed

The only user of css_id was memcg, and it has been convered to use
cgroup->id, so kill css_id.

	Signed-off-by: Li Zefan <lizefan@huwei.com>
	Reviewed-by: Michal Hocko <mhocko@suse.cz>
	Acked-by: Tejun Heo <tj@kernel.org>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 2ff2a7d03bbe472ed44a8380dbdbea490d81c59d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/cgroup.h
#	kernel/cgroup.c
diff --cc include/linux/cgroup.h
index 366173997b9d,39c1d9469677..000000000000
--- a/include/linux/cgroup.h
+++ b/include/linux/cgroup.h
@@@ -902,37 -867,8 +894,42 @@@ int cgroup_scan_tasks(struct cgroup_sca
  int cgroup_attach_task_all(struct task_struct *from, struct task_struct *);
  int cgroup_transfer_tasks(struct cgroup *to, struct cgroup *from);
  
++<<<<<<< HEAD
 +/*
 + * CSS ID is ID for cgroup_subsys_state structs under subsys. This only works
 + * if cgroup_subsys.use_id == true. It can be used for looking up and scanning.
 + * CSS ID is assigned at cgroup allocation (create) automatically
 + * and removed when subsys calls free_css_id() function. This is because
 + * the lifetime of cgroup_subsys_state is subsys's matter.
 + *
 + * Looking up and scanning function should be called under rcu_read_lock().
 + * Taking cgroup_mutex is not necessary for following calls.
 + * But the css returned by this routine can be "not populated yet" or "being
 + * destroyed". The caller should check css and cgroup's status.
 + */
 +
 +/*
 + * Typically Called at ->destroy(), or somewhere the subsys frees
 + * cgroup_subsys_state.
 + */
 +void free_css_id(struct cgroup_subsys *ss, struct cgroup_subsys_state *css);
 +
 +/* Find a cgroup_subsys_state which has given ID */
 +
 +struct cgroup_subsys_state *css_lookup(struct cgroup_subsys *ss, int id);
 +
 +/* Returns true if root is ancestor of cg */
 +bool css_is_ancestor(struct cgroup_subsys_state *cg,
 +		     const struct cgroup_subsys_state *root);
 +
 +/* Get id and depth of css */
 +unsigned short css_id(struct cgroup_subsys_state *css);
 +unsigned short css_depth(struct cgroup_subsys_state *css);
 +struct cgroup_subsys_state *cgroup_css_from_dir(struct file *f, int id);
++=======
+ struct cgroup_subsys_state *css_from_dir(struct dentry *dentry,
+ 					 struct cgroup_subsys *ss);
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  
  #else /* !CONFIG_CGROUPS */
  
diff --cc kernel/cgroup.c
index 60085cf4072d,a5629f1df13a..000000000000
--- a/kernel/cgroup.c
+++ b/kernel/cgroup.c
@@@ -346,14 -353,13 +314,22 @@@ struct cg_cgroup_link 
   */
  
  static struct css_set init_css_set;
 -static struct cgrp_cset_link init_cgrp_cset_link;
 +static struct cg_cgroup_link init_css_set_link;
  
++<<<<<<< HEAD
 +static int cgroup_init_idr(struct cgroup_subsys *ss,
 +			   struct cgroup_subsys_state *css);
 +
 +/* css_set_lock protects the list of css_set objects, and the
 + * chain of tasks off each css_set.  Nests outside task->alloc_lock
 + * due to cgroup_iter_start() */
++=======
+ /*
+  * css_set_lock protects the list of css_set objects, and the chain of
+  * tasks off each css_set.  Nests outside task->alloc_lock due to
+  * css_task_iter_start().
+  */
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  static DEFINE_RWLOCK(css_set_lock);
  static int css_set_count;
  
@@@ -807,9 -806,6 +783,12 @@@ static struct backing_dev_info cgroup_b
  	.capabilities	= BDI_CAP_NO_ACCT_AND_WRITEBACK,
  };
  
++<<<<<<< HEAD
 +static int alloc_css_id(struct cgroup_subsys *ss,
 +			struct cgroup *parent, struct cgroup *child);
 +
++=======
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  static struct inode *cgroup_new_inode(umode_t mode, struct super_block *sb)
  {
  	struct inode *inode = new_inode(sb);
@@@ -4025,81 -4181,111 +4004,112 @@@ static struct cftype files[] = 
  };
  
  /**
 - * cgroup_populate_dir - create subsys files in a cgroup directory
 + * cgroup_populate_dir - selectively creation of files in a directory
   * @cgrp: target cgroup
 + * @base_files: true if the base files should be added
   * @subsys_mask: mask of the subsystem ids whose files should be added
 - *
 - * On failure, no file is added.
   */
 -static int cgroup_populate_dir(struct cgroup *cgrp, unsigned long subsys_mask)
 +static int cgroup_populate_dir(struct cgroup *cgrp, bool base_files,
 +			       unsigned long subsys_mask)
  {
 +	int err;
  	struct cgroup_subsys *ss;
 -	int i, ret = 0;
 +
 +	if (base_files) {
 +		err = cgroup_addrm_files(cgrp, NULL, files, true);
 +		if (err < 0)
 +			return err;
 +	}
  
  	/* process cftsets of each subsystem */
 -	for_each_subsys(ss, i) {
 +	for_each_subsys(cgrp->root, ss) {
  		struct cftype_set *set;
 -
 -		if (!test_bit(i, &subsys_mask))
 +		if (!test_bit(ss->subsys_id, &subsys_mask))
  			continue;
  
 -		list_for_each_entry(set, &ss->cftsets, node) {
 -			ret = cgroup_addrm_files(cgrp, set->cfts, true);
 -			if (ret < 0)
 -				goto err;
 -		}
 +		list_for_each_entry(set, &ss->cftsets, node)
 +			cgroup_addrm_files(cgrp, ss, set->cfts, true);
  	}
++<<<<<<< HEAD
 +
 +	/* This cgroup is ready now */
 +	for_each_subsys(cgrp->root, ss) {
 +		struct cgroup_subsys_state *css = cgrp->subsys[ss->subsys_id];
 +		/*
 +		 * Update id->css pointer and make this css visible from
 +		 * CSS ID functions. This pointer will be dereferened
 +		 * from RCU-read-side without locks.
 +		 */
 +		if (css->id)
 +			rcu_assign_pointer(css->id->css, css);
 +	}
 +
++=======
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  	return 0;
 -err:
 -	cgroup_clear_dir(cgrp, subsys_mask);
 -	return ret;
  }
  
 -/*
 - * css destruction is four-stage process.
 - *
 - * 1. Destruction starts.  Killing of the percpu_ref is initiated.
 - *    Implemented in kill_css().
 - *
 - * 2. When the percpu_ref is confirmed to be visible as killed on all CPUs
 - *    and thus css_tryget() is guaranteed to fail, the css can be offlined
 - *    by invoking offline_css().  After offlining, the base ref is put.
 - *    Implemented in css_killed_work_fn().
 - *
 - * 3. When the percpu_ref reaches zero, the only possible remaining
 - *    accessors are inside RCU read sections.  css_release() schedules the
 - *    RCU callback.
 - *
 - * 4. After the grace period, the css can be freed.  Implemented in
 - *    css_free_work_fn().
 - *
 - * It is actually hairier because both step 2 and 4 require process context
 - * and thus involve punting to css->destroy_work adding two additional
 - * steps to the already complex sequence.
 - */
 -static void css_free_work_fn(struct work_struct *work)
 +static void css_dput_fn(struct work_struct *work)
  {
  	struct cgroup_subsys_state *css =
 -		container_of(work, struct cgroup_subsys_state, destroy_work);
 -	struct cgroup *cgrp = css->cgroup;
 +		container_of(work, struct cgroup_subsys_state, dput_work);
  
 -	if (css->parent)
 -		css_put(css->parent);
 -
 -	css->ss->css_free(css);
 -	cgroup_dput(cgrp);
 +	cgroup_dput(css->cgroup);
  }
  
 -static void css_free_rcu_fn(struct rcu_head *rcu_head)
 +static void init_cgroup_css(struct cgroup_subsys_state *css,
 +			       struct cgroup_subsys *ss,
 +			       struct cgroup *cgrp)
  {
 -	struct cgroup_subsys_state *css =
 -		container_of(rcu_head, struct cgroup_subsys_state, rcu_head);
 +	css->cgroup = cgrp;
 +	atomic_set(&css->refcnt, 1);
 +	css->flags = 0;
 +	css->id = NULL;
 +	if (cgrp == dummytop)
 +		css->flags |= CSS_ROOT;
 +	BUG_ON(cgrp->subsys[ss->subsys_id]);
 +	cgrp->subsys[ss->subsys_id] = css;
  
  	/*
  	 * css holds an extra ref to @cgrp->dentry which is put on the last
 -	 * css_put().  dput() requires process context which we don't have.
 +	 * css_put().  dput() requires process context, which css_put() may
 +	 * be called without.  @css->dput_work will be used to invoke
 +	 * dput() asynchronously from css_put().
  	 */
 -	INIT_WORK(&css->destroy_work, css_free_work_fn);
 -	schedule_work(&css->destroy_work);
 +	INIT_WORK(&css->dput_work, css_dput_fn);
  }
  
 -static void css_release(struct percpu_ref *ref)
 +/* invoke ->post_create() on a new CSS and mark it online if successful */
 +static int online_css(struct cgroup_subsys *ss, struct cgroup *cgrp)
  {
++<<<<<<< HEAD
++=======
+ 	struct cgroup_subsys_state *css =
+ 		container_of(ref, struct cgroup_subsys_state, refcnt);
+ 
+ 	call_rcu(&css->rcu_head, css_free_rcu_fn);
+ }
+ 
+ static void init_css(struct cgroup_subsys_state *css, struct cgroup_subsys *ss,
+ 		     struct cgroup *cgrp)
+ {
+ 	css->cgroup = cgrp;
+ 	css->ss = ss;
+ 	css->flags = 0;
+ 
+ 	if (cgrp->parent)
+ 		css->parent = cgroup_css(cgrp->parent, ss);
+ 	else
+ 		css->flags |= CSS_ROOT;
+ 
+ 	BUG_ON(cgroup_css(cgrp, ss));
+ }
+ 
+ /* invoke ->css_online() on a new CSS and mark it online if successful */
+ static int online_css(struct cgroup_subsys_state *css)
+ {
+ 	struct cgroup_subsys *ss = css->ss;
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  	int ret = 0;
  
  	lockdep_assert_held(&cgroup_mutex);
@@@ -4201,12 -4397,13 +4211,22 @@@ static long cgroup_create(struct cgrou
  			err = PTR_ERR(css);
  			goto err_free_all;
  		}
++<<<<<<< HEAD
 +		init_cgroup_css(css, ss, cgrp);
 +		if (ss->use_id) {
 +			err = alloc_css_id(ss, parent, cgrp);
 +			if (err)
 +				goto err_free_all;
 +		}
++=======
+ 		css_ar[ss->subsys_id] = css;
+ 
+ 		err = percpu_ref_init(&css->refcnt, css_release);
+ 		if (err)
+ 			goto err_free_all;
+ 
+ 		init_css(css, ss, cgrp);
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  	}
  
  	/*
@@@ -4486,17 -4863,11 +4506,21 @@@ int __init_or_module cgroup_load_subsys
  		return PTR_ERR(css);
  	}
  
 -	list_add(&ss->sibling, &cgroup_dummy_root.subsys_list);
 -	ss->root = &cgroup_dummy_root;
 +	list_add(&ss->sibling, &rootnode.subsys_list);
 +	ss->root = &rootnode;
  
  	/* our new subsystem will be attached to the dummy hierarchy. */
++<<<<<<< HEAD
 +	init_cgroup_css(css, ss, dummytop);
 +	/* init_idr must be after init_cgroup_css because it sets css->id. */
 +	if (ss->use_id) {
 +		ret = cgroup_init_idr(ss, css);
 +		if (ret)
 +			goto err_unload;
 +	}
++=======
+ 	init_css(css, ss, cgroup_dummy_top);
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  
  	/*
  	 * Now we need to entangle the css into the existing css_sets. unlike
@@@ -4560,15 -4931,12 +4584,12 @@@ void cgroup_unload_subsys(struct cgroup
  
  	mutex_lock(&cgroup_mutex);
  
 -	offline_css(cgroup_css(cgroup_dummy_top, ss));
 +	offline_css(ss, dummytop);
  
- 	if (ss->use_id)
- 		idr_destroy(&ss->idr);
- 
  	/* deassign the subsys_id */
 -	cgroup_subsys[ss->subsys_id] = NULL;
 +	subsys[ss->subsys_id] = NULL;
  
 -	/* remove subsystem from the dummy root's list of subsystems */
 +	/* remove subsystem from rootnode's list of subsystems */
  	list_del_init(&ss->sibling);
  
  	/*
@@@ -4588,13 -4957,12 +4609,19 @@@
  	write_unlock(&css_set_lock);
  
  	/*
++<<<<<<< HEAD
 +	 * remove subsystem's css from the dummytop and free it - need to
 +	 * free before marking as null because ss->css_free needs the
 +	 * cgrp->subsys pointer to find their state. note that this also
 +	 * takes care of freeing the css_id.
++=======
+ 	 * remove subsystem's css from the cgroup_dummy_top and free it -
+ 	 * need to free before marking as null because ss->css_free needs
+ 	 * the cgrp->subsys pointer to find their state.
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
  	 */
 -	ss->css_free(cgroup_css(cgroup_dummy_top, ss));
 -	RCU_INIT_POINTER(cgroup_dummy_top->subsys[ss->subsys_id], NULL);
 +	ss->css_free(dummytop);
 +	dummytop->subsys[ss->subsys_id] = NULL;
  
  	mutex_unlock(&cgroup_mutex);
  }
@@@ -4664,18 -5027,15 +4691,16 @@@ int __init cgroup_init(void
  	if (err)
  		return err;
  
 -	for_each_builtin_subsys(ss, i) {
 +	for (i = 0; i < CGROUP_SUBSYS_COUNT; i++) {
 +		struct cgroup_subsys *ss = subsys[i];
 +
 +		/* at bootup time, we don't worry about modular subsystems */
 +		if (!ss || ss->module)
 +			continue;
  		if (!ss->early_init)
  			cgroup_init_subsys(ss);
- 		if (ss->use_id)
- 			cgroup_init_idr(ss, init_css_set.subsys[ss->subsys_id]);
  	}
  
 -	/* allocate id for the dummy hierarchy */
 -	mutex_lock(&cgroup_mutex);
 -	mutex_lock(&cgroup_root_mutex);
 -
  	/* Add init_css_set to the hash table */
  	key = css_set_hash(init_css_set.subsys);
  	hash_add(css_set_table, &init_css_set.hlist, key);
@@@ -5208,219 -5449,51 +5233,230 @@@ static int __init cgroup_disable(char *
  }
  __setup("cgroup_disable=", cgroup_disable);
  
++<<<<<<< HEAD
 +/*
 + * Functons for CSS ID.
 + */
 +
 +/*
 + *To get ID other than 0, this should be called when !cgroup_is_removed().
 + */
 +unsigned short css_id(struct cgroup_subsys_state *css)
 +{
 +	struct css_id *cssid;
 +
 +	/*
 +	 * This css_id() can return correct value when somone has refcnt
 +	 * on this or this is under rcu_read_lock(). Once css->id is allocated,
 +	 * it's unchanged until freed.
 +	 */
 +	cssid = rcu_dereference_check(css->id, css_refcnt(css));
 +
 +	if (cssid)
 +		return cssid->id;
 +	return 0;
 +}
 +EXPORT_SYMBOL_GPL(css_id);
 +
 +unsigned short css_depth(struct cgroup_subsys_state *css)
 +{
 +	struct css_id *cssid;
 +
 +	cssid = rcu_dereference_check(css->id, css_refcnt(css));
 +
 +	if (cssid)
 +		return cssid->depth;
 +	return 0;
 +}
 +EXPORT_SYMBOL_GPL(css_depth);
 +
  /**
 - * css_from_dir - get corresponding css from the dentry of a cgroup dir
 - * @dentry: directory dentry of interest
 - * @ss: subsystem of interest
 + *  css_is_ancestor - test "root" css is an ancestor of "child"
 + * @child: the css to be tested.
 + * @root: the css supporsed to be an ancestor of the child.
   *
 - * Must be called under RCU read lock.  The caller is responsible for
 - * pinning the returned css if it needs to be accessed outside the RCU
 - * critical section.
 + * Returns true if "root" is an ancestor of "child" in its hierarchy. Because
 + * this function reads css->id, the caller must hold rcu_read_lock().
 + * But, considering usual usage, the csses should be valid objects after test.
 + * Assuming that the caller will do some action to the child if this returns
 + * returns true, the caller must take "child";s reference count.
 + * If "child" is valid object and this returns true, "root" is valid, too.
   */
 -struct cgroup_subsys_state *css_from_dir(struct dentry *dentry,
 -					 struct cgroup_subsys *ss)
 +
 +bool css_is_ancestor(struct cgroup_subsys_state *child,
 +		    const struct cgroup_subsys_state *root)
  {
 -	struct cgroup *cgrp;
 +	struct css_id *child_id;
 +	struct css_id *root_id;
  
 -	WARN_ON_ONCE(!rcu_read_lock_held());
 +	child_id  = rcu_dereference(child->id);
 +	if (!child_id)
 +		return false;
 +	root_id = rcu_dereference(root->id);
 +	if (!root_id)
 +		return false;
 +	if (child_id->depth < root_id->depth)
 +		return false;
 +	if (child_id->stack[root_id->depth] != root_id->id)
 +		return false;
 +	return true;
 +}
  
 -	/* is @dentry a cgroup dir? */
 -	if (!dentry->d_inode ||
 -	    dentry->d_inode->i_op != &cgroup_dir_inode_operations)
 -		return ERR_PTR(-EBADF);
 +void free_css_id(struct cgroup_subsys *ss, struct cgroup_subsys_state *css)
 +{
 +	struct css_id *id = css->id;
 +	/* When this is called before css_id initialization, id can be NULL */
 +	if (!id)
 +		return;
 +
 +	BUG_ON(!ss->use_id);
 +
 +	rcu_assign_pointer(id->css, NULL);
 +	rcu_assign_pointer(css->id, NULL);
 +	spin_lock(&ss->id_lock);
 +	idr_remove(&ss->idr, id->id);
 +	spin_unlock(&ss->id_lock);
 +	kfree_rcu(id, rcu_head);
 +}
 +EXPORT_SYMBOL_GPL(free_css_id);
 +
 +/*
 + * This is called by init or create(). Then, calls to this function are
 + * always serialized (By cgroup_mutex() at create()).
 + */
 +
 +static struct css_id *get_new_cssid(struct cgroup_subsys *ss, int depth)
 +{
 +	struct css_id *newid;
 +	int ret, size;
 +
 +	BUG_ON(!ss->use_id);
 +
 +	size = sizeof(*newid) + sizeof(unsigned short) * (depth + 1);
 +	newid = kzalloc(size, GFP_KERNEL);
 +	if (!newid)
 +		return ERR_PTR(-ENOMEM);
 +
 +	idr_preload(GFP_KERNEL);
 +	spin_lock(&ss->id_lock);
 +	/* Don't use 0. allocates an ID of 1-65535 */
 +	ret = idr_alloc(&ss->idr, newid, 1, CSS_ID_MAX + 1, GFP_NOWAIT);
 +	spin_unlock(&ss->id_lock);
 +	idr_preload_end();
 +
 +	/* Returns error when there are no free spaces for new ID.*/
 +	if (ret < 0)
 +		goto err_out;
 +
 +	newid->id = ret;
 +	newid->depth = depth;
 +	return newid;
 +err_out:
 +	kfree(newid);
 +	return ERR_PTR(ret);
 +
 +}
 +
 +static int __init_or_module cgroup_init_idr(struct cgroup_subsys *ss,
 +					    struct cgroup_subsys_state *rootcss)
 +{
 +	struct css_id *newid;
 +
 +	spin_lock_init(&ss->id_lock);
 +	idr_init(&ss->idr);
 +
 +	newid = get_new_cssid(ss, 0);
 +	if (IS_ERR(newid))
 +		return PTR_ERR(newid);
 +
 +	newid->stack[0] = newid->id;
 +	newid->css = rootcss;
 +	rootcss->id = newid;
 +	return 0;
 +}
 +
 +static int alloc_css_id(struct cgroup_subsys *ss, struct cgroup *parent,
 +			struct cgroup *child)
 +{
 +	int subsys_id, i, depth = 0;
 +	struct cgroup_subsys_state *parent_css, *child_css;
 +	struct css_id *child_id, *parent_id;
 +
 +	subsys_id = ss->subsys_id;
 +	parent_css = parent->subsys[subsys_id];
 +	child_css = child->subsys[subsys_id];
 +	parent_id = parent_css->id;
 +	depth = parent_id->depth + 1;
 +
 +	child_id = get_new_cssid(ss, depth);
 +	if (IS_ERR(child_id))
 +		return PTR_ERR(child_id);
  
 -	cgrp = __d_cgrp(dentry);
 -	return cgroup_css(cgrp, ss) ?: ERR_PTR(-ENOENT);
 +	for (i = 0; i < depth; i++)
 +		child_id->stack[i] = parent_id->stack[i];
 +	child_id->stack[depth] = child_id->id;
 +	/*
 +	 * child_id->css pointer will be set after this cgroup is available
 +	 * see cgroup_populate_dir()
 +	 */
 +	rcu_assign_pointer(child_css->id, child_id);
 +
 +	return 0;
  }
  
  /**
 - * css_from_id - lookup css by id
 - * @id: the cgroup id
 - * @ss: cgroup subsys to be looked into
 + * css_lookup - lookup css by id
 + * @ss: cgroup subsys to be looked into.
 + * @id: the id
   *
 - * Returns the css if there's valid one with @id, otherwise returns NULL.
 - * Should be called under rcu_read_lock().
 + * Returns pointer to cgroup_subsys_state if there is valid one with id.
 + * NULL if not. Should be called under rcu_read_lock()
   */
 -struct cgroup_subsys_state *css_from_id(int id, struct cgroup_subsys *ss)
 +struct cgroup_subsys_state *css_lookup(struct cgroup_subsys *ss, int id)
 +{
 +	struct css_id *cssid = NULL;
 +
 +	BUG_ON(!ss->use_id);
 +	cssid = idr_find(&ss->idr, id);
 +
 +	if (unlikely(!cssid))
 +		return NULL;
 +
 +	return rcu_dereference(cssid->css);
 +}
 +EXPORT_SYMBOL_GPL(css_lookup);
 +
 +/*
 + * get corresponding css from file open on cgroupfs directory
++=======
++/**
++ * css_from_dir - get corresponding css from the dentry of a cgroup dir
++ * @dentry: directory dentry of interest
++ * @ss: subsystem of interest
++ *
++ * Must be called under RCU read lock.  The caller is responsible for
++ * pinning the returned css if it needs to be accessed outside the RCU
++ * critical section.
++>>>>>>> 2ff2a7d03bbe (cgroup: kill css_id)
 + */
 +struct cgroup_subsys_state *cgroup_css_from_dir(struct file *f, int id)
  {
  	struct cgroup *cgrp;
 +	struct inode *inode;
 +	struct cgroup_subsys_state *css;
  
 -	rcu_lockdep_assert(rcu_read_lock_held() ||
 -			   lockdep_is_held(&cgroup_mutex),
 -			   "css_from_id() needs proper protection");
 +	inode = file_inode(f);
 +	/* check in cgroup filesystem dir */
 +	if (inode->i_op != &cgroup_dir_inode_operations)
 +		return ERR_PTR(-EBADF);
  
 -	cgrp = idr_find(&ss->root->cgroup_idr, id);
 -	if (cgrp)
 -		return cgroup_css(cgrp, ss);
 -	return NULL;
 +	if (id < 0 || id >= CGROUP_SUBSYS_COUNT)
 +		return ERR_PTR(-EINVAL);
 +
 +	/* get cgroup */
 +	cgrp = __d_cgrp(f->f_dentry);
 +	css = cgrp->subsys[id];
 +	return css ? css : ERR_PTR(-ENOENT);
  }
  
  #ifdef CONFIG_CGROUP_DEBUG
* Unmerged path include/linux/cgroup.h
* Unmerged path kernel/cgroup.c
