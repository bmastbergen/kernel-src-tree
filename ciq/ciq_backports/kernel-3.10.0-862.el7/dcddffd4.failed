mm: do not pass mm_struct into handle_mm_fault

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] do not pass mm_struct into handle_mm_fault (Larry Woodman) [1457572 1457561]
Rebuild_FUZZ: 95.45%
commit-author Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
commit dcddffd41d3f1d3bdcc1dce3f1cd142779b6d4c1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/dcddffd4.failed

We always have vma->vm_mm around.

Link: http://lkml.kernel.org/r/1466021202-61880-8-git-send-email-kirill.shutemov@linux.intel.com
	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit dcddffd41d3f1d3bdcc1dce3f1cd142779b6d4c1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/frv/mm/fault.c
#	arch/m32r/mm/fault.c
#	arch/m68k/mm/fault.c
#	arch/nios2/mm/fault.c
#	arch/parisc/mm/fault.c
#	arch/score/mm/fault.c
#	arch/sparc/mm/fault_32.c
#	arch/sparc/mm/fault_64.c
#	drivers/iommu/amd_iommu_v2.c
#	drivers/iommu/intel-svm.c
#	mm/gup.c
#	mm/ksm.c
#	mm/memory.c
diff --cc arch/frv/mm/fault.c
index 331c1e2cfb67,614a46c413d2..000000000000
--- a/arch/frv/mm/fault.c
+++ b/arch/frv/mm/fault.c
@@@ -162,7 -164,7 +162,11 @@@ asmlinkage void do_page_fault(int datam
  	 * make sure we exit gracefully rather than endlessly redo
  	 * the fault.
  	 */
++<<<<<<< HEAD
 +	fault = handle_mm_fault(mm, vma, ear0, write ? FAULT_FLAG_WRITE : 0);
++=======
+ 	fault = handle_mm_fault(vma, ear0, flags);
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  	if (unlikely(fault & VM_FAULT_ERROR)) {
  		if (fault & VM_FAULT_OOM)
  			goto out_of_memory;
diff --cc arch/m32r/mm/fault.c
index 3cdfa9c1d091,a3785d3644c2..000000000000
--- a/arch/m32r/mm/fault.c
+++ b/arch/m32r/mm/fault.c
@@@ -194,7 -196,7 +194,11 @@@ good_area
  	 */
  	addr = (address & PAGE_MASK);
  	set_thread_fault_code(error_code);
++<<<<<<< HEAD
 +	fault = handle_mm_fault(mm, vma, addr, write ? FAULT_FLAG_WRITE : 0);
++=======
+ 	fault = handle_mm_fault(vma, addr, flags);
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  	if (unlikely(fault & VM_FAULT_ERROR)) {
  		if (fault & VM_FAULT_OOM)
  			goto out_of_memory;
diff --cc arch/m68k/mm/fault.c
index a563727806bf,bd66a0b20c6b..000000000000
--- a/arch/m68k/mm/fault.c
+++ b/arch/m68k/mm/fault.c
@@@ -140,10 -136,8 +140,15 @@@ good_area
  	 * the fault.
  	 */
  
++<<<<<<< HEAD
 +	fault = handle_mm_fault(mm, vma, address, flags);
 +#ifdef DEBUG
 +	printk("handle_mm_fault returns %d\n",fault);
 +#endif
++=======
+ 	fault = handle_mm_fault(vma, address, flags);
+ 	pr_debug("handle_mm_fault returns %d\n", fault);
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  
  	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
  		return 0;
diff --cc arch/parisc/mm/fault.c
index f247a3480e8e,163af2c31d76..000000000000
--- a/arch/parisc/mm/fault.c
+++ b/arch/parisc/mm/fault.c
@@@ -203,8 -239,7 +203,12 @@@ good_area
  	 * fault.
  	 */
  
++<<<<<<< HEAD
 +	fault = handle_mm_fault(mm, vma, address,
 +			flags | ((acc_type & VM_WRITE) ? FAULT_FLAG_WRITE : 0));
++=======
+ 	fault = handle_mm_fault(vma, address, flags);
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  
  	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
  		return;
diff --cc arch/score/mm/fault.c
index 47b600e4b2c5,995b71e4db4b..000000000000
--- a/arch/score/mm/fault.c
+++ b/arch/score/mm/fault.c
@@@ -106,7 -111,7 +106,11 @@@ survive
  	* make sure we exit gracefully rather than endlessly redo
  	* the fault.
  	*/
++<<<<<<< HEAD
 +	fault = handle_mm_fault(mm, vma, address, write);
++=======
+ 	fault = handle_mm_fault(vma, address, flags);
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  	if (unlikely(fault & VM_FAULT_ERROR)) {
  		if (fault & VM_FAULT_OOM)
  			goto out_of_memory;
diff --cc arch/sparc/mm/fault_32.c
index e98bfda205a2,4714061d6cd3..000000000000
--- a/arch/sparc/mm/fault_32.c
+++ b/arch/sparc/mm/fault_32.c
@@@ -406,7 -411,7 +406,11 @@@ good_area
  		if (!(vma->vm_flags & (VM_READ | VM_EXEC)))
  			goto bad_area;
  	}
++<<<<<<< HEAD
 +	switch (handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0)) {
++=======
+ 	switch (handle_mm_fault(vma, address, flags)) {
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  	case VM_FAULT_SIGBUS:
  	case VM_FAULT_OOM:
  		goto do_sigbus;
diff --cc arch/sparc/mm/fault_64.c
index 5062ff389e83,6c43b924a7a2..000000000000
--- a/arch/sparc/mm/fault_64.c
+++ b/arch/sparc/mm/fault_64.c
@@@ -424,11 -436,10 +424,15 @@@ good_area
  			goto bad_area;
  	}
  
++<<<<<<< HEAD
 +	flags |= ((fault_code & FAULT_CODE_WRITE) ? FAULT_FLAG_WRITE : 0);
 +	fault = handle_mm_fault(mm, vma, address, flags);
++=======
+ 	fault = handle_mm_fault(vma, address, flags);
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  
  	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 -		goto exit_exception;
 +		return;
  
  	if (unlikely(fault & VM_FAULT_ERROR)) {
  		if (fault & VM_FAULT_OOM)
diff --cc drivers/iommu/amd_iommu_v2.c
index 9b539a79cc08,fbdaf81ae925..000000000000
--- a/drivers/iommu/amd_iommu_v2.c
+++ b/drivers/iommu/amd_iommu_v2.c
@@@ -522,33 -522,30 +522,38 @@@ static void do_fault(struct work_struc
  	mm = fault->state->mm;
  	address = fault->address;
  
 -	if (fault->flags & PPR_FAULT_USER)
 -		flags |= FAULT_FLAG_USER;
 -	if (fault->flags & PPR_FAULT_WRITE)
 -		flags |= FAULT_FLAG_WRITE;
 -	flags |= FAULT_FLAG_REMOTE;
 -
  	down_read(&mm->mmap_sem);
  	vma = find_extend_vma(mm, address);
 -	if (!vma || address < vma->vm_start)
 +	if (!vma || address < vma->vm_start) {
  		/* failed to get a vma in the right range */
 +		up_read(&mm->mmap_sem);
 +		handle_fault_error(fault);
  		goto out;
 +	}
  
  	/* Check if we have the right permissions on the vma */
 -	if (access_error(vma, fault))
 +	if (access_error(vma, fault)) {
 +		up_read(&mm->mmap_sem);
 +		handle_fault_error(fault);
 +		goto out;
 +	}
 +
++<<<<<<< HEAD
 +	ret = handle_mm_fault(mm, vma, address, write);
 +	if (ret & VM_FAULT_ERROR) {
 +		/* failed to service fault */
 +		up_read(&mm->mmap_sem);
 +		handle_fault_error(fault);
  		goto out;
 +	}
  
++=======
+ 	ret = handle_mm_fault(vma, address, flags);
+ out:
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  	up_read(&mm->mmap_sem);
  
 -	if (ret & VM_FAULT_ERROR)
 -		/* failed to service fault */
 -		handle_fault_error(fault);
 -
 +out:
  	finish_pri_tag(fault->dev_state, fault->state, fault->tag);
  
  	put_pasid_state(fault->state);
diff --cc mm/gup.c
index 3166366affd5,9671e29f8ffd..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -271,6 -300,164 +271,167 @@@ no_page_table
  	return page;
  }
  
++<<<<<<< HEAD
++=======
+ static int get_gate_page(struct mm_struct *mm, unsigned long address,
+ 		unsigned int gup_flags, struct vm_area_struct **vma,
+ 		struct page **page)
+ {
+ 	pgd_t *pgd;
+ 	pud_t *pud;
+ 	pmd_t *pmd;
+ 	pte_t *pte;
+ 	int ret = -EFAULT;
+ 
+ 	/* user gate pages are read-only */
+ 	if (gup_flags & FOLL_WRITE)
+ 		return -EFAULT;
+ 	if (address > TASK_SIZE)
+ 		pgd = pgd_offset_k(address);
+ 	else
+ 		pgd = pgd_offset_gate(mm, address);
+ 	BUG_ON(pgd_none(*pgd));
+ 	pud = pud_offset(pgd, address);
+ 	BUG_ON(pud_none(*pud));
+ 	pmd = pmd_offset(pud, address);
+ 	if (pmd_none(*pmd))
+ 		return -EFAULT;
+ 	VM_BUG_ON(pmd_trans_huge(*pmd));
+ 	pte = pte_offset_map(pmd, address);
+ 	if (pte_none(*pte))
+ 		goto unmap;
+ 	*vma = get_gate_vma(mm);
+ 	if (!page)
+ 		goto out;
+ 	*page = vm_normal_page(*vma, address, *pte);
+ 	if (!*page) {
+ 		if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))
+ 			goto unmap;
+ 		*page = pte_page(*pte);
+ 	}
+ 	get_page(*page);
+ out:
+ 	ret = 0;
+ unmap:
+ 	pte_unmap(pte);
+ 	return ret;
+ }
+ 
+ /*
+  * mmap_sem must be held on entry.  If @nonblocking != NULL and
+  * *@flags does not include FOLL_NOWAIT, the mmap_sem may be released.
+  * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
+  */
+ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
+ 		unsigned long address, unsigned int *flags, int *nonblocking)
+ {
+ 	unsigned int fault_flags = 0;
+ 	int ret;
+ 
+ 	/* mlock all present pages, but do not fault in new pages */
+ 	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
+ 		return -ENOENT;
+ 	/* For mm_populate(), just skip the stack guard page. */
+ 	if ((*flags & FOLL_POPULATE) &&
+ 			(stack_guard_page_start(vma, address) ||
+ 			 stack_guard_page_end(vma, address + PAGE_SIZE)))
+ 		return -ENOENT;
+ 	if (*flags & FOLL_WRITE)
+ 		fault_flags |= FAULT_FLAG_WRITE;
+ 	if (*flags & FOLL_REMOTE)
+ 		fault_flags |= FAULT_FLAG_REMOTE;
+ 	if (nonblocking)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
+ 	if (*flags & FOLL_NOWAIT)
+ 		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
+ 	if (*flags & FOLL_TRIED) {
+ 		VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
+ 		fault_flags |= FAULT_FLAG_TRIED;
+ 	}
+ 
+ 	ret = handle_mm_fault(vma, address, fault_flags);
+ 	if (ret & VM_FAULT_ERROR) {
+ 		if (ret & VM_FAULT_OOM)
+ 			return -ENOMEM;
+ 		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
+ 			return *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;
+ 		if (ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
+ 			return -EFAULT;
+ 		BUG();
+ 	}
+ 
+ 	if (tsk) {
+ 		if (ret & VM_FAULT_MAJOR)
+ 			tsk->maj_flt++;
+ 		else
+ 			tsk->min_flt++;
+ 	}
+ 
+ 	if (ret & VM_FAULT_RETRY) {
+ 		if (nonblocking)
+ 			*nonblocking = 0;
+ 		return -EBUSY;
+ 	}
+ 
+ 	/*
+ 	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when
+ 	 * necessary, even if maybe_mkwrite decided not to set pte_write. We
+ 	 * can thus safely do subsequent page lookups as if they were reads.
+ 	 * But only do so when looping for pte_write is futile: in some cases
+ 	 * userspace may also be wanting to write to the gotten user page,
+ 	 * which a read fault here might prevent (a readonly page might get
+ 	 * reCOWed by userspace write).
+ 	 */
+ 	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
+ 		*flags &= ~FOLL_WRITE;
+ 	return 0;
+ }
+ 
+ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
+ {
+ 	vm_flags_t vm_flags = vma->vm_flags;
+ 	int write = (gup_flags & FOLL_WRITE);
+ 	int foreign = (gup_flags & FOLL_REMOTE);
+ 
+ 	if (vm_flags & (VM_IO | VM_PFNMAP))
+ 		return -EFAULT;
+ 
+ 	if (write) {
+ 		if (!(vm_flags & VM_WRITE)) {
+ 			if (!(gup_flags & FOLL_FORCE))
+ 				return -EFAULT;
+ 			/*
+ 			 * We used to let the write,force case do COW in a
+ 			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
+ 			 * set a breakpoint in a read-only mapping of an
+ 			 * executable, without corrupting the file (yet only
+ 			 * when that file had been opened for writing!).
+ 			 * Anon pages in shared mappings are surprising: now
+ 			 * just reject it.
+ 			 */
+ 			if (!is_cow_mapping(vm_flags))
+ 				return -EFAULT;
+ 		}
+ 	} else if (!(vm_flags & VM_READ)) {
+ 		if (!(gup_flags & FOLL_FORCE))
+ 			return -EFAULT;
+ 		/*
+ 		 * Is there actually any vma we can reach here which does not
+ 		 * have VM_MAYREAD set?
+ 		 */
+ 		if (!(vm_flags & VM_MAYREAD))
+ 			return -EFAULT;
+ 	}
+ 	/*
+ 	 * gups are always data accesses, not instruction
+ 	 * fetches, so execute=false here
+ 	 */
+ 	if (!arch_vma_access_permitted(vma, write, false, foreign))
+ 		return -EFAULT;
+ 	return 0;
+ }
+ 
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  /**
   * __get_user_pages() - pin user pages in memory
   * @tsk:	task_struct of target task
@@@ -573,11 -688,11 +734,16 @@@ int fixup_user_fault(struct task_struc
  	if (!vma || address < vma->vm_start)
  		return -EFAULT;
  
 -	if (!vma_permits_fault(vma, fault_flags))
 +	vm_flags = (fault_flags & FAULT_FLAG_WRITE) ? VM_WRITE : VM_READ;
 +	if (!(vm_flags & vma->vm_flags))
  		return -EFAULT;
  
++<<<<<<< HEAD
 +	ret = handle_mm_fault(mm, vma, address, fault_flags);
++=======
+ 	ret = handle_mm_fault(vma, address, fault_flags);
+ 	major |= ret & VM_FAULT_MAJOR;
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  	if (ret & VM_FAULT_ERROR) {
  		if (ret & VM_FAULT_OOM)
  			return -ENOMEM;
diff --cc mm/ksm.c
index feae14491122,73d43bafd9fb..000000000000
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@@ -441,8 -376,8 +441,13 @@@ static int break_ksm(struct vm_area_str
  		if (IS_ERR_OR_NULL(page))
  			break;
  		if (PageKsm(page))
++<<<<<<< HEAD
 +			ret = handle_mm_fault(vma->vm_mm, vma, addr,
 +							FAULT_FLAG_WRITE);
++=======
+ 			ret = handle_mm_fault(vma, addr,
+ 					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE);
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  		else
  			ret = VM_FAULT_WRITE;
  		put_page(page);
diff --cc mm/memory.c
index 14270187456b,6bf2b8564376..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3263,10 -3416,14 +3263,11 @@@ unlock
  
  /*
   * By the time we get here, we already hold the mm semaphore
 - *
 - * The mmap_sem may have been released depending on flags and our
 - * return value.  See filemap_fault() and __lock_page_or_retry().
   */
- static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
- 			     unsigned long address, unsigned int flags)
+ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
+ 		unsigned int flags)
  {
+ 	struct mm_struct *mm = vma->vm_mm;
  	pgd_t *pgd;
  	pud_t *pud;
  	pmd_t *pmd;
@@@ -3351,8 -3504,14 +3352,19 @@@
  	return handle_pte_fault(mm, vma, address, pte, pmd, flags);
  }
  
++<<<<<<< HEAD
 +int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 +		    unsigned long address, unsigned int flags)
++=======
+ /*
+  * By the time we get here, we already hold the mm semaphore
+  *
+  * The mmap_sem may have been released depending on flags and our
+  * return value.  See filemap_fault() and __lock_page_or_retry().
+  */
+ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
+ 		unsigned int flags)
++>>>>>>> dcddffd41d3f (mm: do not pass mm_struct into handle_mm_fault)
  {
  	int ret;
  
* Unmerged path arch/nios2/mm/fault.c
* Unmerged path drivers/iommu/intel-svm.c
diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c
index 0c4132dd3507..aa1346e9b717 100644
--- a/arch/alpha/mm/fault.c
+++ b/arch/alpha/mm/fault.c
@@ -147,7 +147,7 @@ retry:
 	/* If for any reason at all we couldn't handle the fault,
 	   make sure we exit gracefully rather than endlessly redo
 	   the fault.  */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
diff --git a/arch/arc/mm/fault.c b/arch/arc/mm/fault.c
index 331a0846628e..d04defca5856 100644
--- a/arch/arc/mm/fault.c
+++ b/arch/arc/mm/fault.c
@@ -126,7 +126,7 @@ survive:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	/* If Pagefault was interrupted by SIGKILL, exit page fault "early" */
 	if (unlikely(fatal_signal_pending(current))) {
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index 5dbf13f954f6..9ee0cc0d57b6 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -244,7 +244,7 @@ good_area:
 		goto out;
 	}
 
-	return handle_mm_fault(mm, vma, addr & PAGE_MASK, flags);
+	return handle_mm_fault(vma, addr & PAGE_MASK, flags);
 
 check_stack:
 	/* Don't allow expansion below FIRST_USER_ADDRESS */
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index f51d669c8ebd..d06496abfdb4 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -181,7 +181,7 @@ good_area:
 		goto out;
 	}
 
-	return handle_mm_fault(mm, vma, addr & PAGE_MASK, mm_flags);
+	return handle_mm_fault(vma, addr & PAGE_MASK, mm_flags);
 
 check_stack:
 	if (vma->vm_flags & VM_GROWSDOWN && !expand_stack(vma, addr))
diff --git a/arch/avr32/mm/fault.c b/arch/avr32/mm/fault.c
index b2f2d2d66849..583503dcfb1c 100644
--- a/arch/avr32/mm/fault.c
+++ b/arch/avr32/mm/fault.c
@@ -132,7 +132,7 @@ good_area:
 	 * sure we exit gracefully rather than endlessly redo the
 	 * fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
diff --git a/arch/cris/mm/fault.c b/arch/cris/mm/fault.c
index 73312ab6c696..4e5531bc44af 100644
--- a/arch/cris/mm/fault.c
+++ b/arch/cris/mm/fault.c
@@ -166,7 +166,7 @@ retry:
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
* Unmerged path arch/frv/mm/fault.c
diff --git a/arch/hexagon/mm/vm_fault.c b/arch/hexagon/mm/vm_fault.c
index 1bd276dbec7d..2e9e4c00b3e1 100644
--- a/arch/hexagon/mm/vm_fault.c
+++ b/arch/hexagon/mm/vm_fault.c
@@ -99,7 +99,7 @@ good_area:
 		break;
 	}
 
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c
index 6cf0341f978e..9c47b409482b 100644
--- a/arch/ia64/mm/fault.c
+++ b/arch/ia64/mm/fault.c
@@ -157,7 +157,7 @@ retry:
 	 * sure we exit gracefully rather than endlessly redo the
 	 * fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
* Unmerged path arch/m32r/mm/fault.c
* Unmerged path arch/m68k/mm/fault.c
diff --git a/arch/metag/mm/fault.c b/arch/metag/mm/fault.c
index 2c75bf7357c5..53ffb1bb380c 100644
--- a/arch/metag/mm/fault.c
+++ b/arch/metag/mm/fault.c
@@ -131,7 +131,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return 0;
diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c
index 731f739d17a1..eb965dfcc63a 100644
--- a/arch/microblaze/mm/fault.c
+++ b/arch/microblaze/mm/fault.c
@@ -213,7 +213,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index 0fead53d1c26..407f6be1b588 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -146,7 +146,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
diff --git a/arch/mn10300/mm/fault.c b/arch/mn10300/mm/fault.c
index d48a84fd7fae..d4c71aeb8cb2 100644
--- a/arch/mn10300/mm/fault.c
+++ b/arch/mn10300/mm/fault.c
@@ -252,7 +252,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
* Unmerged path arch/nios2/mm/fault.c
diff --git a/arch/openrisc/mm/fault.c b/arch/openrisc/mm/fault.c
index e2bfafce66c5..4909fa05bef5 100644
--- a/arch/openrisc/mm/fault.c
+++ b/arch/openrisc/mm/fault.c
@@ -162,7 +162,7 @@ good_area:
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
* Unmerged path arch/parisc/mm/fault.c
diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c
index ceb99aaf11f5..f8269682a2b0 100644
--- a/arch/powerpc/mm/copro_fault.c
+++ b/arch/powerpc/mm/copro_fault.c
@@ -71,7 +71,7 @@ int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,
 	}
 
 	ret = 0;
-	*flt = handle_mm_fault(mm, vma, ea, is_write ? FAULT_FLAG_WRITE : 0);
+	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0);
 	if (unlikely(*flt & VM_FAULT_ERROR)) {
 		if (*flt & VM_FAULT_OOM) {
 			ret = -ENOMEM;
diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
index a633519cb63a..b46e5c8c1450 100644
--- a/arch/powerpc/mm/fault.c
+++ b/arch/powerpc/mm/fault.c
@@ -443,7 +443,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 	if (unlikely(fault & (VM_FAULT_RETRY|VM_FAULT_ERROR))) {
 		rc = mm_fault_error(regs, address, fault);
 		if (rc >= MM_FAULT_RETURN)
diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c
index 9aa3443fbf65..6b7b798b0fe0 100644
--- a/arch/s390/mm/fault.c
+++ b/arch/s390/mm/fault.c
@@ -349,7 +349,7 @@ retry:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 	/* No reason to continue if interrupted by SIGKILL. */
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current)) {
 		fault = VM_FAULT_SIGNAL;
* Unmerged path arch/score/mm/fault.c
diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c
index 1f49c28affa9..5a61fca74adb 100644
--- a/arch/sh/mm/fault.c
+++ b/arch/sh/mm/fault.c
@@ -481,7 +481,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if (unlikely(fault & (VM_FAULT_RETRY | VM_FAULT_ERROR)))
 		if (mm_fault_error(regs, error_code, address, fault))
* Unmerged path arch/sparc/mm/fault_32.c
* Unmerged path arch/sparc/mm/fault_64.c
diff --git a/arch/tile/mm/fault.c b/arch/tile/mm/fault.c
index 3d2b81c163a6..c54a9b32c678 100644
--- a/arch/tile/mm/fault.c
+++ b/arch/tile/mm/fault.c
@@ -436,7 +436,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return 0;
diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c
index 089f3987e273..f2a84dcd6b6c 100644
--- a/arch/um/kernel/trap.c
+++ b/arch/um/kernel/trap.c
@@ -68,7 +68,7 @@ good_area:
 	do {
 		int fault;
 
-		fault = handle_mm_fault(mm, vma, address, flags);
+		fault = handle_mm_fault(vma, address, flags);
 
 		if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 			goto out_nosemaphore;
diff --git a/arch/unicore32/mm/fault.c b/arch/unicore32/mm/fault.c
index f9b5c10bccee..c54d882ab202 100644
--- a/arch/unicore32/mm/fault.c
+++ b/arch/unicore32/mm/fault.c
@@ -194,7 +194,7 @@ good_area:
 	 * If for any reason at all we couldn't handle the fault, make
 	 * sure we exit gracefully rather than endlessly redo the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, addr & PAGE_MASK, flags);
+	fault = handle_mm_fault(vma, addr & PAGE_MASK, flags);
 	return fault;
 
 check_stack:
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 7d19407891fe..9dc226ebcf5f 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1187,7 +1187,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault:
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 	major |= fault & VM_FAULT_MAJOR;
 
 	/*
diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c
index 4b7bc8db170f..f491a0ffab4f 100644
--- a/arch/xtensa/mm/fault.c
+++ b/arch/xtensa/mm/fault.c
@@ -107,7 +107,7 @@ good_area:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(mm, vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags);
 
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 		return;
* Unmerged path drivers/iommu/amd_iommu_v2.c
* Unmerged path drivers/iommu/intel-svm.c
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 3416fff96060..14a4a985d716 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1244,14 +1244,13 @@ int generic_error_remove_page(struct address_space *mapping, struct page *page);
 int invalidate_inode_page(struct page *page);
 
 #ifdef CONFIG_MMU
-extern int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
-			unsigned long address, unsigned int flags);
+extern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
+		unsigned int flags);
 extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags);
 #else
-static inline int handle_mm_fault(struct mm_struct *mm,
-			struct vm_area_struct *vma, unsigned long address,
-			unsigned int flags)
+static inline int handle_mm_fault(struct vm_area_struct *vma,
+		unsigned long address, unsigned int flags)
 {
 	/* should never happen if there's no MMU */
 	BUG();
* Unmerged path mm/gup.c
* Unmerged path mm/ksm.c
* Unmerged path mm/memory.c
