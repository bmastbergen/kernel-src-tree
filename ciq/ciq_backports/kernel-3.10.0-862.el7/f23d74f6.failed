x86/mm: Rework wbinvd, hlt operation in stop_this_cpu()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] mm: Rework wbinvd, hlt operation in stop_this_cpu() (Suravee Suthikulpanit) [1522676]
Rebuild_FUZZ: 96.23%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit f23d74f6c66c3697e032550eeef3f640391a3a7d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f23d74f6.failed

Some issues have been reported with the for loop in stop_this_cpu() that
issues the 'wbinvd; hlt' sequence.  Reverting this sequence to halt()
has been shown to resolve the issue.

However, the wbinvd is needed when running with SME.  The reason for the
wbinvd is to prevent cache flush races between encrypted and non-encrypted
entries that have the same physical address.  This can occur when
kexec'ing from memory encryption active to inactive or vice-versa.  The
important thing is to not have outside of kernel text memory references
(such as stack usage), so the usage of the native_*() functions is needed
since these expand as inline asm sequences.  So instead of reverting the
change, rework the sequence.

Move the wbinvd instruction outside of the for loop as native_wbinvd()
and make its execution conditional on X86_FEATURE_SME.  In the for loop,
change the asm 'wbinvd; hlt' sequence back to a halt sequence but use
the native_halt() call.

Fixes: bba4ed011a52 ("x86/mm, kexec: Allow kexec to be used with SME")
	Reported-by: Dave Young <dyoung@redhat.com>
	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Dave Young <dyoung@redhat.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: Tony Luck <tony.luck@intel.com>
	Cc: Yu Chen <yu.c.chen@intel.com>
	Cc: Baoquan He <bhe@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: kexec@lists.infradead.org
	Cc: ebiederm@redhat.com
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Rui Zhang <rui.zhang@intel.com>
	Cc: Arjan van de Ven <arjan@linux.intel.com>
	Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: stable@vger.kernel.org
Link: https://lkml.kernel.org/r/20180117234141.21184.44067.stgit@tlendack-t1.amdoffice.net

(cherry picked from commit f23d74f6c66c3697e032550eeef3f640391a3a7d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/process.c
diff --cc arch/x86/kernel/process.c
index 1e5497794b6a,cb368c2a22ab..000000000000
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@@ -340,19 -380,25 +340,41 @@@ void stop_this_cpu(void *dummy
  	disable_local_APIC();
  	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));
  
++<<<<<<< HEAD
 +	for (;;)
 +		halt();
 +}
 +
 +bool amd_e400_c1e_detected;
 +EXPORT_SYMBOL(amd_e400_c1e_detected);
 +
 +static cpumask_var_t amd_e400_c1e_mask;
 +
 +void amd_e400_remove_cpu(int cpu)
 +{
 +	if (amd_e400_c1e_mask != NULL)
 +		cpumask_clear_cpu(cpu, amd_e400_c1e_mask);
++=======
+ 	/*
+ 	 * Use wbinvd on processors that support SME. This provides support
+ 	 * for performing a successful kexec when going from SME inactive
+ 	 * to SME active (or vice-versa). The cache must be cleared so that
+ 	 * if there are entries with the same physical address, both with and
+ 	 * without the encryption bit, they don't race each other when flushed
+ 	 * and potentially end up with the wrong entry being committed to
+ 	 * memory.
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_SME))
+ 		native_wbinvd();
+ 	for (;;) {
+ 		/*
+ 		 * Use native_halt() so that memory contents don't change
+ 		 * (stack usage and variables) after possibly issuing the
+ 		 * native_wbinvd() above.
+ 		 */
+ 		native_halt();
+ 	}
++>>>>>>> f23d74f6c66c (x86/mm: Rework wbinvd, hlt operation in stop_this_cpu())
  }
  
  /*
* Unmerged path arch/x86/kernel/process.c
