net/mlx5e: Use kernel's mechanism to avoid missing NAPIs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Use kernel's mechanism to avoid missing NAPIs (Kamal Heib) [1456694]
Rebuild_FUZZ: 96.30%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit 7b33aaeaae1acfec01cbb0cf41fa8c07aea55609
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7b33aaea.failed

We used a channel state bit MLX5E_CHANNEL_NAPI_SCHED to make
sure no NAPI is missed when a channel's napi_schedule() is called
for completion events of the different channel's resources/rings
while NAPI is currently running.
Now, as similar mechanism is implemented in kernel,
("39e6c8208d7b net: solve a NAPI race"),
we obsolete our own implementation and rely on the return value
of napi_complete_done().

This patch removes a redundant overhead of atomic bit operations.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Cc: Eric Dumazet <edumazet@google.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 7b33aaeaae1acfec01cbb0cf41fa8c07aea55609)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 54fbcbad0e6e,7c046ae8b18e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -391,112 -570,6 +391,115 @@@ struct mlx5e_rq 
  	struct mlx5_core_mkey  umr_mkey;
  } ____cacheline_aligned_in_smp;
  
++<<<<<<< HEAD
 +struct mlx5e_umr_dma_info {
 +	__be64                *mtt;
 +	dma_addr_t             mtt_addr;
 +	struct mlx5e_dma_info  dma_info[MLX5_MPWRQ_PAGES_PER_WQE];
 +	struct mlx5e_umr_wqe   wqe;
 +};
 +
 +struct mlx5e_mpw_info {
 +	struct mlx5e_umr_dma_info umr;
 +	u16 consumed_strides;
 +	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
 +};
 +
 +struct mlx5e_tx_wqe_info {
 +	u32 num_bytes;
 +	u8  num_wqebbs;
 +	u8  num_dma;
 +};
 +
 +enum mlx5e_dma_map_type {
 +	MLX5E_DMA_MAP_SINGLE,
 +	MLX5E_DMA_MAP_PAGE
 +};
 +
 +struct mlx5e_sq_dma {
 +	dma_addr_t              addr;
 +	u32                     size;
 +	enum mlx5e_dma_map_type type;
 +};
 +
 +enum {
 +	MLX5E_SQ_STATE_ENABLED,
 +	MLX5E_SQ_STATE_BF_ENABLE,
 +};
 +
 +struct mlx5e_ico_wqe_info {
 +	u8  opcode;
 +	u8  num_wqebbs;
 +};
 +
 +enum mlx5e_sq_type {
 +	MLX5E_SQ_TXQ,
 +	MLX5E_SQ_ICO
 +};
 +
 +struct mlx5e_sq {
 +	/* data path */
 +
 +	/* dirtied @completion */
 +	u16                        cc;
 +	u32                        dma_fifo_cc;
 +
 +	/* dirtied @xmit */
 +	u16                        pc ____cacheline_aligned_in_smp;
 +	u32                        dma_fifo_pc;
 +	u16                        bf_offset;
 +	u16                        prev_cc;
 +	u8                         bf_budget;
 +	struct mlx5e_sq_stats      stats;
 +
 +	struct mlx5e_cq            cq;
 +
 +	/* pointers to per tx element info: write@xmit, read@completion */
 +	union {
 +		struct {
 +			struct sk_buff           **skb;
 +			struct mlx5e_sq_dma       *dma_fifo;
 +			struct mlx5e_tx_wqe_info  *wqe_info;
 +		} txq;
 +		struct mlx5e_ico_wqe_info *ico_wqe;
 +	} db;
 +
 +	/* read only */
 +	struct mlx5_wq_cyc         wq;
 +	u32                        dma_fifo_mask;
 +	void __iomem              *uar_map;
 +	struct netdev_queue       *txq;
 +	u32                        sqn;
 +	u16                        bf_buf_size;
 +	u16                        max_inline;
 +	u8                         min_inline_mode;
 +	u16                        edge;
 +	struct device             *pdev;
 +	struct mlx5e_tstamp       *tstamp;
 +	__be32                     mkey_be;
 +	unsigned long              state;
 +
 +	/* control path */
 +	struct mlx5_wq_ctrl        wq_ctrl;
 +	struct mlx5_uar            uar;
 +	struct mlx5e_channel      *channel;
 +	int                        tc;
 +	u32                        rate_limit;
 +	u8                         type;
 +} ____cacheline_aligned_in_smp;
 +
 +static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)
 +{
 +	return (((sq->wq.sz_m1 & (sq->cc - sq->pc)) >= n) ||
 +		(sq->cc  == sq->pc));
 +}
 +
 +enum channel_flags {
 +	MLX5E_CHANNEL_NAPI_SCHED = 1,
 +};
 +
++=======
++>>>>>>> 7b33aaeaae1a (net/mlx5e: Use kernel's mechanism to avoid missing NAPIs)
  struct mlx5e_channel {
  	/* data path */
  	struct mlx5e_rq            rq;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index d8b64a5a33e7,b2f689ec0d72..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -3129,6 -3628,115 +3129,118 @@@ static void mlx5e_tx_timeout(struct net
  		schedule_work(&priv->tx_timeout_work);
  }
  
++<<<<<<< HEAD
++=======
+ static int mlx5e_xdp_set(struct net_device *netdev, struct bpf_prog *prog)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(netdev);
+ 	struct bpf_prog *old_prog;
+ 	int err = 0;
+ 	bool reset, was_opened;
+ 	int i;
+ 
+ 	mutex_lock(&priv->state_lock);
+ 
+ 	if ((netdev->features & NETIF_F_LRO) && prog) {
+ 		netdev_warn(netdev, "can't set XDP while LRO is on, disable LRO first\n");
+ 		err = -EINVAL;
+ 		goto unlock;
+ 	}
+ 
+ 	if ((netdev->features & NETIF_F_HW_ESP) && prog) {
+ 		netdev_warn(netdev, "can't set XDP with IPSec offload\n");
+ 		err = -EINVAL;
+ 		goto unlock;
+ 	}
+ 
+ 	was_opened = test_bit(MLX5E_STATE_OPENED, &priv->state);
+ 	/* no need for full reset when exchanging programs */
+ 	reset = (!priv->channels.params.xdp_prog || !prog);
+ 
+ 	if (was_opened && reset)
+ 		mlx5e_close_locked(netdev);
+ 	if (was_opened && !reset) {
+ 		/* num_channels is invariant here, so we can take the
+ 		 * batched reference right upfront.
+ 		 */
+ 		prog = bpf_prog_add(prog, priv->channels.num);
+ 		if (IS_ERR(prog)) {
+ 			err = PTR_ERR(prog);
+ 			goto unlock;
+ 		}
+ 	}
+ 
+ 	/* exchange programs, extra prog reference we got from caller
+ 	 * as long as we don't fail from this point onwards.
+ 	 */
+ 	old_prog = xchg(&priv->channels.params.xdp_prog, prog);
+ 	if (old_prog)
+ 		bpf_prog_put(old_prog);
+ 
+ 	if (reset) /* change RQ type according to priv->xdp_prog */
+ 		mlx5e_set_rq_params(priv->mdev, &priv->channels.params);
+ 
+ 	if (was_opened && reset)
+ 		mlx5e_open_locked(netdev);
+ 
+ 	if (!test_bit(MLX5E_STATE_OPENED, &priv->state) || reset)
+ 		goto unlock;
+ 
+ 	/* exchanging programs w/o reset, we update ref counts on behalf
+ 	 * of the channels RQs here.
+ 	 */
+ 	for (i = 0; i < priv->channels.num; i++) {
+ 		struct mlx5e_channel *c = priv->channels.c[i];
+ 
+ 		clear_bit(MLX5E_RQ_STATE_ENABLED, &c->rq.state);
+ 		napi_synchronize(&c->napi);
+ 		/* prevent mlx5e_poll_rx_cq from accessing rq->xdp_prog */
+ 
+ 		old_prog = xchg(&c->rq.xdp_prog, prog);
+ 
+ 		set_bit(MLX5E_RQ_STATE_ENABLED, &c->rq.state);
+ 		/* napi_schedule in case we have missed anything */
+ 		napi_schedule(&c->napi);
+ 
+ 		if (old_prog)
+ 			bpf_prog_put(old_prog);
+ 	}
+ 
+ unlock:
+ 	mutex_unlock(&priv->state_lock);
+ 	return err;
+ }
+ 
+ static u32 mlx5e_xdp_query(struct net_device *dev)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	const struct bpf_prog *xdp_prog;
+ 	u32 prog_id = 0;
+ 
+ 	mutex_lock(&priv->state_lock);
+ 	xdp_prog = priv->channels.params.xdp_prog;
+ 	if (xdp_prog)
+ 		prog_id = xdp_prog->aux->id;
+ 	mutex_unlock(&priv->state_lock);
+ 
+ 	return prog_id;
+ }
+ 
+ static int mlx5e_xdp(struct net_device *dev, struct netdev_xdp *xdp)
+ {
+ 	switch (xdp->command) {
+ 	case XDP_SETUP_PROG:
+ 		return mlx5e_xdp_set(dev, xdp->prog);
+ 	case XDP_QUERY_PROG:
+ 		xdp->prog_id = mlx5e_xdp_query(dev);
+ 		xdp->prog_attached = !!xdp->prog_id;
+ 		return 0;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
++>>>>>>> 7b33aaeaae1a (net/mlx5e: Use kernel's mechanism to avoid missing NAPIs)
  #ifdef CONFIG_NET_POLL_CONTROLLER
  /* Fake "interrupt" called by netpoll (eg netconsole) to send skbs without
   * reenabling interrupts.
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
index a75c0268e502..5e1a9b632a21 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c
@@ -113,8 +113,6 @@ int mlx5e_napi_poll(struct napi_struct *napi, int budget)
 	int work_done;
 	int i;
 
-	clear_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags);
-
 	for (i = 0; i < c->num_tc; i++)
 		busy |= mlx5e_poll_tx_cq(&c->sq[i].cq, budget);
 
@@ -128,13 +126,8 @@ int mlx5e_napi_poll(struct napi_struct *napi, int budget)
 	if (busy)
 		return budget;
 
-	napi_complete_done(napi, work_done);
-
-	/* avoid losing completion event during/after polling cqs */
-	if (test_bit(MLX5E_CHANNEL_NAPI_SCHED, &c->flags)) {
-		napi_schedule(napi);
+	if (unlikely(!napi_complete_done(napi, work_done)))
 		return work_done;
-	}
 
 	for (i = 0; i < c->num_tc; i++)
 		mlx5e_cq_arm(&c->sq[i].cq);
@@ -153,7 +146,6 @@ void mlx5e_completion_event(struct mlx5_core_cq *mcq)
 	struct mlx5e_cq *cq = container_of(mcq, struct mlx5e_cq, mcq);
 
 	cq->event_ctr++;
-	set_bit(MLX5E_CHANNEL_NAPI_SCHED, &cq->channel->flags);
 	napi_schedule(cq->napi);
 }
 
