x86/efi: Update EFI pagetable creation to work with SME

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] efi: Update EFI pagetable creation to work with SME (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 96.23%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit 38eecccdf488e38ee93690cfe9ec1914b73f512f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/38eecccd.failed

When SME is active, pagetable entries created for EFI need to have the
encryption mask set as necessary.

When the new pagetable pages are allocated they are mapped encrypted. So,
update the efi_pgt value that will be used in CR3 to include the encryption
mask so that the PGD table can be read successfully. The pagetable mapping
as well as the kernel are also added to the pagetable mapping as encrypted.
All other EFI mappings are mapped decrypted (tables, etc.).

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brijesh Singh <brijesh.singh@amd.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Larry Woodman <lwoodman@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Toshimitsu Kani <toshi.kani@hpe.com>
	Cc: kasan-dev@googlegroups.com
	Cc: kvm@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
	Cc: linux-doc@vger.kernel.org
	Cc: linux-efi@vger.kernel.org
	Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/9a8f4c502db4a84b09e2f0a1555bb75aa8b69785.1500319216.git.thomas.lendacky@amd.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 38eecccdf488e38ee93690cfe9ec1914b73f512f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/platform/efi/efi_64.c
diff --cc arch/x86/platform/efi/efi_64.c
index 1a2483c88c8c,12e83888e5b9..000000000000
--- a/arch/x86/platform/efi/efi_64.c
+++ b/arch/x86/platform/efi/efi_64.c
@@@ -166,25 -241,107 +166,40 @@@ void efi_sync_low_kernel_mappings(void
  	if (efi_enabled(EFI_OLD_MEMMAP))
  		return;
  
 -	/*
 -	 * We can share all PGD entries apart from the one entry that
 -	 * covers the EFI runtime mapping space.
 -	 *
 -	 * Make sure the EFI runtime region mappings are guaranteed to
 -	 * only span a single PGD entry and that the entry also maps
 -	 * other important kernel regions.
 -	 */
 -	BUILD_BUG_ON(pgd_index(EFI_VA_END) != pgd_index(MODULES_END));
 -	BUILD_BUG_ON((EFI_VA_START & PGDIR_MASK) !=
 -			(EFI_VA_END & PGDIR_MASK));
 -
 -	pgd_efi = efi_pgd + pgd_index(PAGE_OFFSET);
 -	pgd_k = pgd_offset_k(PAGE_OFFSET);
 -
 -	num_entries = pgd_index(EFI_VA_END) - pgd_index(PAGE_OFFSET);
 -	memcpy(pgd_efi, pgd_k, sizeof(pgd_t) * num_entries);
 -
 -	/*
 -	 * As with PGDs, we share all P4D entries apart from the one entry
 -	 * that covers the EFI runtime mapping space.
 -	 */
 -	BUILD_BUG_ON(p4d_index(EFI_VA_END) != p4d_index(MODULES_END));
 -	BUILD_BUG_ON((EFI_VA_START & P4D_MASK) != (EFI_VA_END & P4D_MASK));
 -
 -	pgd_efi = efi_pgd + pgd_index(EFI_VA_END);
 -	pgd_k = pgd_offset_k(EFI_VA_END);
 -	p4d_efi = p4d_offset(pgd_efi, 0);
 -	p4d_k = p4d_offset(pgd_k, 0);
 -
 -	num_entries = p4d_index(EFI_VA_END);
 -	memcpy(p4d_efi, p4d_k, sizeof(p4d_t) * num_entries);
 -
 -	/*
 -	 * We share all the PUD entries apart from those that map the
 -	 * EFI regions. Copy around them.
 -	 */
 -	BUILD_BUG_ON((EFI_VA_START & ~PUD_MASK) != 0);
 -	BUILD_BUG_ON((EFI_VA_END & ~PUD_MASK) != 0);
 -
 -	p4d_efi = p4d_offset(pgd_efi, EFI_VA_END);
 -	p4d_k = p4d_offset(pgd_k, EFI_VA_END);
 -	pud_efi = pud_offset(p4d_efi, 0);
 -	pud_k = pud_offset(p4d_k, 0);
 +	num_pgds = pgd_index(MODULES_END - 1) - pgd_index(PAGE_OFFSET);
  
 -	num_entries = pud_index(EFI_VA_END);
 -	memcpy(pud_efi, pud_k, sizeof(pud_t) * num_entries);
 -
 -	pud_efi = pud_offset(p4d_efi, EFI_VA_START);
 -	pud_k = pud_offset(p4d_k, EFI_VA_START);
 -
 -	num_entries = PTRS_PER_PUD - pud_index(EFI_VA_START);
 -	memcpy(pud_efi, pud_k, sizeof(pud_t) * num_entries);
 +	memcpy(pgd + pgd_index(PAGE_OFFSET),
 +		init_mm.pgd + pgd_index(PAGE_OFFSET),
 +		sizeof(pgd_t) * num_pgds);
  }
  
 -/*
 - * Wrapper for slow_virt_to_phys() that handles NULL addresses.
 - */
 -static inline phys_addr_t
 -virt_to_phys_or_null_size(void *va, unsigned long size)
 -{
 -	bool bad_size;
 -
 -	if (!va)
 -		return 0;
 -
 -	if (virt_addr_valid(va))
 -		return virt_to_phys(va);
 -
 -	/*
 -	 * A fully aligned variable on the stack is guaranteed not to
 -	 * cross a page bounary. Try to catch strings on the stack by
 -	 * checking that 'size' is a power of two.
 -	 */
 -	bad_size = size > PAGE_SIZE || !is_power_of_2(size);
 -
 -	WARN_ON(!IS_ALIGNED((unsigned long)va, size) || bad_size);
 -
 -	return slow_virt_to_phys(va);
 -}
 -
 -#define virt_to_phys_or_null(addr)				\
 -	virt_to_phys_or_null_size((addr), sizeof(*(addr)))
 -
  int __init efi_setup_page_tables(unsigned long pa_memmap, unsigned num_pages)
  {
++<<<<<<< HEAD
 +	unsigned long text;
++=======
+ 	unsigned long pfn, text, pf;
+ 	struct page *page;
++>>>>>>> 38eecccdf488 (x86/efi: Update EFI pagetable creation to work with SME)
  	unsigned npages;
 +	struct page *page;
  	pgd_t *pgd;
  
  	if (efi_enabled(EFI_OLD_MEMMAP))
  		return 0;
  
++<<<<<<< HEAD
 +	efi_scratch.efi_pgt = (pgd_t *)(unsigned long)real_mode_header->trampoline_pgd;
 +	pgd = __va(efi_scratch.efi_pgt);
++=======
+ 	/*
+ 	 * Since the PGD is encrypted, set the encryption mask so that when
+ 	 * this value is loaded into cr3 the PGD will be decrypted during
+ 	 * the pagetable walk.
+ 	 */
+ 	efi_scratch.efi_pgt = (pgd_t *)__sme_pa(efi_pgd);
+ 	pgd = efi_pgd;
++>>>>>>> 38eecccdf488 (x86/efi: Update EFI pagetable creation to work with SME)
  
  	/*
  	 * It can happen that the physical address of new_memmap lands in memory
@@@ -192,7 -349,9 +207,13 @@@
  	 * and ident-map those pages containing the map before calling
  	 * phys_efi_set_virtual_address_map().
  	 */
++<<<<<<< HEAD
 +	if (kernel_map_pages_in_pgd(pgd, pa_memmap, pa_memmap, num_pages, _PAGE_NX)) {
++=======
+ 	pfn = pa_memmap >> PAGE_SHIFT;
+ 	pf = _PAGE_NX | _PAGE_RW | _PAGE_ENC;
+ 	if (kernel_map_pages_in_pgd(pgd, pfn, pa_memmap, num_pages, pf)) {
++>>>>>>> 38eecccdf488 (x86/efi: Update EFI pagetable creation to work with SME)
  		pr_err("Error ident-mapping new memmap (0x%lx)!\n", pa_memmap);
  		return 1;
  	}
@@@ -215,11 -390,12 +236,16 @@@
  	efi_scratch.phys_stack = virt_to_phys(page_address(page));
  	efi_scratch.phys_stack += PAGE_SIZE; /* stack grows down */
  
 -	npages = (_etext - _text) >> PAGE_SHIFT;
 +	npages = (_end - _text) >> PAGE_SHIFT;
  	text = __pa(_text);
 -	pfn = text >> PAGE_SHIFT;
  
++<<<<<<< HEAD
 +	if (kernel_map_pages_in_pgd(__va(efi_scratch.efi_pgt),
 +				    text >> PAGE_SHIFT, text, npages, 0)) {
++=======
+ 	pf = _PAGE_RW | _PAGE_ENC;
+ 	if (kernel_map_pages_in_pgd(pgd, pfn, text, npages, pf)) {
++>>>>>>> 38eecccdf488 (x86/efi: Update EFI pagetable creation to work with SME)
  		pr_err("Failed to map kernel text 1:1\n");
  		return 1;
  	}
* Unmerged path arch/x86/platform/efi/efi_64.c
