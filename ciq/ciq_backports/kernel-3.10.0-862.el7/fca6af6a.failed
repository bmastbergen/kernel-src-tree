iommu/amd: Add per-domain timer to flush per-cpu queues

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] amd: Add per-domain timer to flush per-cpu queues (Suravee Suthikulpanit) [1508644]
Rebuild_FUZZ: 94.23%
commit-author Joerg Roedel <jroedel@suse.de>
commit fca6af6a5976dfa00182232f666b4f789c98bd0c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/fca6af6a.failed

Add a timer to each dma_ops domain so that we flush unused
IOTLB entries regularily, even if the queues don't get full
all the time.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit fca6af6a5976dfa00182232f666b4f789c98bd0c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index e80343c1de99,00c1796e07bf..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -146,16 -157,43 +146,46 @@@ struct dma_ops_domain 
  	/* generic protection domain information */
  	struct protection_domain domain;
  
 -	/* IOVA RB-Tree */
 -	struct iova_domain iovad;
 +	/* size of the aperture for the mappings */
 +	unsigned long aperture_size;
  
 -	struct flush_queue __percpu *flush_queue;
 +	/* aperture index we start searching for free addresses */
 +	unsigned long next_index;
  
++<<<<<<< HEAD
 +	/* address space relevant data */
 +	struct aperture_range *aperture[APERTURE_MAX_RANGES];
++=======
+ 	/*
+ 	 * We need two counter here to be race-free wrt. IOTLB flushing and
+ 	 * adding entries to the flush queue.
+ 	 *
+ 	 * The flush_start_cnt is incremented _before_ the IOTLB flush starts.
+ 	 * New entries added to the flush ring-buffer get their 'counter' value
+ 	 * from here. This way we can make sure that entries added to the queue
+ 	 * (or other per-cpu queues of the same domain) while the TLB is about
+ 	 * to be flushed are not considered to be flushed already.
+ 	 */
+ 	atomic64_t flush_start_cnt;
+ 
+ 	/*
+ 	 * The flush_finish_cnt is incremented when an IOTLB flush is complete.
+ 	 * This value is always smaller than flush_start_cnt. The queue_add
+ 	 * function frees all IOVAs that have a counter value smaller than
+ 	 * flush_finish_cnt. This makes sure that we only free IOVAs that are
+ 	 * flushed out of the IOTLB of the domain.
+ 	 */
+ 	atomic64_t flush_finish_cnt;
+ 
+ 	/*
+ 	 * Timer to make sure we don't keep IOVAs around unflushed
+ 	 * for too long
+ 	 */
+ 	struct timer_list flush_timer;
+ 	atomic_t flush_timer_on;
++>>>>>>> fca6af6a5976 (iommu/amd: Add per-domain timer to flush per-cpu queues)
  };
  
 -static struct iova_domain reserved_iova_ranges;
 -static struct lock_class_key reserved_rbtree_key;
 -
  /****************************************************************************
   *
   * Helper functions
@@@ -1972,6 -1786,186 +2002,189 @@@ static void free_gcr3_table(struct prot
  	free_page((unsigned long)domain->gcr3_tbl);
  }
  
++<<<<<<< HEAD
++=======
+ static void dma_ops_domain_free_flush_queue(struct dma_ops_domain *dom)
+ {
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct flush_queue *queue;
+ 
+ 		queue = per_cpu_ptr(dom->flush_queue, cpu);
+ 		kfree(queue->entries);
+ 	}
+ 
+ 	free_percpu(dom->flush_queue);
+ 
+ 	dom->flush_queue = NULL;
+ }
+ 
+ static int dma_ops_domain_alloc_flush_queue(struct dma_ops_domain *dom)
+ {
+ 	int cpu;
+ 
+ 	atomic64_set(&dom->flush_start_cnt,  0);
+ 	atomic64_set(&dom->flush_finish_cnt, 0);
+ 
+ 	dom->flush_queue = alloc_percpu(struct flush_queue);
+ 	if (!dom->flush_queue)
+ 		return -ENOMEM;
+ 
+ 	/* First make sure everything is cleared */
+ 	for_each_possible_cpu(cpu) {
+ 		struct flush_queue *queue;
+ 
+ 		queue = per_cpu_ptr(dom->flush_queue, cpu);
+ 		queue->head    = 0;
+ 		queue->tail    = 0;
+ 		queue->entries = NULL;
+ 	}
+ 
+ 	/* Now start doing the allocation */
+ 	for_each_possible_cpu(cpu) {
+ 		struct flush_queue *queue;
+ 
+ 		queue = per_cpu_ptr(dom->flush_queue, cpu);
+ 		queue->entries = kzalloc(FLUSH_QUEUE_SIZE * sizeof(*queue->entries),
+ 					 GFP_KERNEL);
+ 		if (!queue->entries) {
+ 			dma_ops_domain_free_flush_queue(dom);
+ 			return -ENOMEM;
+ 		}
+ 
+ 		spin_lock_init(&queue->lock);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void dma_ops_domain_flush_tlb(struct dma_ops_domain *dom)
+ {
+ 	atomic64_inc(&dom->flush_start_cnt);
+ 	domain_flush_tlb(&dom->domain);
+ 	domain_flush_complete(&dom->domain);
+ 	atomic64_inc(&dom->flush_finish_cnt);
+ }
+ 
+ static inline bool queue_ring_full(struct flush_queue *queue)
+ {
+ 	assert_spin_locked(&queue->lock);
+ 
+ 	return (((queue->tail + 1) % FLUSH_QUEUE_SIZE) == queue->head);
+ }
+ 
+ #define queue_ring_for_each(i, q) \
+ 	for (i = (q)->head; i != (q)->tail; i = (i + 1) % FLUSH_QUEUE_SIZE)
+ 
+ static void queue_release(struct dma_ops_domain *dom,
+ 			  struct flush_queue *queue)
+ {
+ 	unsigned i;
+ 
+ 	assert_spin_locked(&queue->lock);
+ 
+ 	queue_ring_for_each(i, queue)
+ 		free_iova_fast(&dom->iovad,
+ 			       queue->entries[i].iova_pfn,
+ 			       queue->entries[i].pages);
+ 
+ 	queue->head = queue->tail = 0;
+ }
+ 
+ static inline unsigned queue_ring_add(struct flush_queue *queue)
+ {
+ 	unsigned idx = queue->tail;
+ 
+ 	assert_spin_locked(&queue->lock);
+ 	queue->tail = (idx + 1) % FLUSH_QUEUE_SIZE;
+ 
+ 	return idx;
+ }
+ 
+ static inline void queue_ring_remove_head(struct flush_queue *queue)
+ {
+ 	assert_spin_locked(&queue->lock);
+ 	queue->head = (queue->head + 1) % FLUSH_QUEUE_SIZE;
+ }
+ 
+ static void queue_ring_free_flushed(struct dma_ops_domain *dom,
+ 				    struct flush_queue *queue)
+ {
+ 	u64 counter = atomic64_read(&dom->flush_finish_cnt);
+ 	int idx;
+ 
+ 	queue_ring_for_each(idx, queue) {
+ 		/*
+ 		 * This assumes that counter values in the ring-buffer are
+ 		 * monotonously rising.
+ 		 */
+ 		if (queue->entries[idx].counter >= counter)
+ 			break;
+ 
+ 		free_iova_fast(&dom->iovad,
+ 			       queue->entries[idx].iova_pfn,
+ 			       queue->entries[idx].pages);
+ 
+ 		queue_ring_remove_head(queue);
+ 	}
+ }
+ 
+ static void queue_add(struct dma_ops_domain *dom,
+ 		      unsigned long address, unsigned long pages)
+ {
+ 	struct flush_queue *queue;
+ 	unsigned long flags;
+ 	int idx;
+ 
+ 	pages     = __roundup_pow_of_two(pages);
+ 	address >>= PAGE_SHIFT;
+ 
+ 	queue = get_cpu_ptr(dom->flush_queue);
+ 	spin_lock_irqsave(&queue->lock, flags);
+ 
+ 	queue_ring_free_flushed(dom, queue);
+ 
+ 	if (queue_ring_full(queue)) {
+ 		dma_ops_domain_flush_tlb(dom);
+ 		queue_release(dom, queue);
+ 	}
+ 
+ 	idx = queue_ring_add(queue);
+ 
+ 	queue->entries[idx].iova_pfn = address;
+ 	queue->entries[idx].pages    = pages;
+ 	queue->entries[idx].counter  = atomic64_read(&dom->flush_start_cnt);
+ 
+ 	spin_unlock_irqrestore(&queue->lock, flags);
+ 
+ 	if (atomic_cmpxchg(&dom->flush_timer_on, 0, 1) == 0)
+ 		mod_timer(&dom->flush_timer, jiffies + msecs_to_jiffies(10));
+ 
+ 	put_cpu_ptr(dom->flush_queue);
+ }
+ 
+ static void queue_flush_timeout(unsigned long data)
+ {
+ 	struct dma_ops_domain *dom = (struct dma_ops_domain *)data;
+ 	int cpu;
+ 
+ 	atomic_set(&dom->flush_timer_on, 0);
+ 
+ 	dma_ops_domain_flush_tlb(dom);
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct flush_queue *queue;
+ 		unsigned long flags;
+ 
+ 		queue = per_cpu_ptr(dom->flush_queue, cpu);
+ 		spin_lock_irqsave(&queue->lock, flags);
+ 		queue_ring_free_flushed(dom, queue);
+ 		spin_unlock_irqrestore(&queue->lock, flags);
+ 	}
+ }
+ 
++>>>>>>> fca6af6a5976 (iommu/amd: Add per-domain timer to flush per-cpu queues)
  /*
   * Free a domain, only used if something went wrong in the
   * allocation path and we need to free an already allocated page table
@@@ -1985,14 -1977,17 +2198,24 @@@ static void dma_ops_domain_free(struct 
  
  	del_domain_from_list(&dom->domain);
  
++<<<<<<< HEAD
++=======
+ 	if (timer_pending(&dom->flush_timer))
+ 		del_timer(&dom->flush_timer);
+ 
+ 	dma_ops_domain_free_flush_queue(dom);
+ 
+ 	put_iova_domain(&dom->iovad);
+ 
++>>>>>>> fca6af6a5976 (iommu/amd: Add per-domain timer to flush per-cpu queues)
  	free_pagetable(&dom->domain);
  
 -	if (dom->domain.id)
 -		domain_id_free(dom->domain.id);
 +	for (i = 0; i < APERTURE_MAX_RANGES; ++i) {
 +		if (!dom->aperture[i])
 +			continue;
 +		free_page((unsigned long)dom->aperture[i]->bitmap);
 +		kfree(dom->aperture[i]);
 +	}
  
  	kfree(dom);
  }
@@@ -2037,18 -2014,21 +2260,27 @@@ static struct dma_ops_domain *dma_ops_d
  	if (!dma_dom->domain.pt_root)
  		goto free_dma_dom;
  
 -	init_iova_domain(&dma_dom->iovad, PAGE_SIZE,
 -			 IOVA_START_PFN, DMA_32BIT_PFN);
 -
 -	/* Initialize reserved ranges */
 -	copy_reserved_iova(&reserved_iova_ranges, &dma_dom->iovad);
 +	add_domain_to_list(&dma_dom->domain);
  
 -	if (dma_ops_domain_alloc_flush_queue(dma_dom))
 +	if (alloc_new_range(dma_dom, true, GFP_KERNEL))
  		goto free_dma_dom;
  
++<<<<<<< HEAD
 +	/*
 +	 * mark the first page as allocated so we never return 0 as
 +	 * a valid dma-address. So we can use 0 as error value
 +	 */
 +	dma_dom->aperture[0]->bitmap[0] = 1;
 +	dma_dom->next_index = 0;
 +
++=======
+ 	setup_timer(&dma_dom->flush_timer, queue_flush_timeout,
+ 		    (unsigned long)dma_dom);
+ 
+ 	atomic_set(&dma_dom->flush_timer_on, 0);
+ 
+ 	add_domain_to_list(&dma_dom->domain);
++>>>>>>> fca6af6a5976 (iommu/amd: Add per-domain timer to flush per-cpu queues)
  
  	return dma_dom;
  
* Unmerged path drivers/iommu/amd_iommu.c
