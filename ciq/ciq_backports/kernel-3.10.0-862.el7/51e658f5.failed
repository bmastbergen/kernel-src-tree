IB/rdmavt, hfi1, qib: Enhance rdmavt and hfi1 to use 32 bit lids

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
commit 51e658f5dd362cc8666f3f5ec1986660e3e51047
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/51e658f5.failed

Increase lid used in hfi1 driver to 32 bits. qib continues
to use 16 bit lids.

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
	Signed-off-by: Don Hiatt <don.hiatt@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 51e658f5dd362cc8666f3f5ec1986660e3e51047)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/mad.c
#	drivers/infiniband/hw/hfi1/verbs.c
#	drivers/infiniband/hw/hfi1/verbs.h
diff --cc drivers/infiniband/hw/hfi1/mad.c
index d59426fb9866,cdcb4d021480..000000000000
--- a/drivers/infiniband/hw/hfi1/mad.c
+++ b/drivers/infiniband/hw/hfi1/mad.c
@@@ -97,7 -108,188 +97,192 @@@ void hfi1_event_pkey_change(struct hfi1
  	ib_dispatch_event(&event);
  }
  
++<<<<<<< HEAD
 +static void send_trap(struct hfi1_ibport *ibp, void *data, unsigned len)
++=======
+ /*
+  * If the port is down, clean up all pending traps.  We need to be careful
+  * with the given trap, because it may be queued.
+  */
+ static void cleanup_traps(struct hfi1_ibport *ibp, struct trap_node *trap)
+ {
+ 	struct trap_node *node, *q;
+ 	unsigned long flags;
+ 	struct list_head trap_list;
+ 	int i;
+ 
+ 	for (i = 0; i < RVT_MAX_TRAP_LISTS; i++) {
+ 		spin_lock_irqsave(&ibp->rvp.lock, flags);
+ 		list_replace_init(&ibp->rvp.trap_lists[i].list, &trap_list);
+ 		ibp->rvp.trap_lists[i].list_len = 0;
+ 		spin_unlock_irqrestore(&ibp->rvp.lock, flags);
+ 
+ 		/*
+ 		 * Remove all items from the list, freeing all the non-given
+ 		 * traps.
+ 		 */
+ 		list_for_each_entry_safe(node, q, &trap_list, list) {
+ 			list_del(&node->list);
+ 			if (node != trap)
+ 				kfree(node);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * If this wasn't on one of the lists it would not be freed.  If it
+ 	 * was on the list, it is now safe to free.
+ 	 */
+ 	kfree(trap);
+ }
+ 
+ static struct trap_node *check_and_add_trap(struct hfi1_ibport *ibp,
+ 					    struct trap_node *trap)
+ {
+ 	struct trap_node *node;
+ 	struct trap_list *trap_list;
+ 	unsigned long flags;
+ 	unsigned long timeout;
+ 	int found = 0;
+ 
+ 	/*
+ 	 * Since the retry (handle timeout) does not remove a trap request
+ 	 * from the list, all we have to do is compare the node.
+ 	 */
+ 	spin_lock_irqsave(&ibp->rvp.lock, flags);
+ 	trap_list = &ibp->rvp.trap_lists[trap->data.generic_type & 0x0F];
+ 
+ 	list_for_each_entry(node, &trap_list->list, list) {
+ 		if (node == trap) {
+ 			node->retry++;
+ 			found = 1;
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* If it is not on the list, add it, limited to RVT-MAX_TRAP_LEN. */
+ 	if (!found) {
+ 		if (trap_list->list_len < RVT_MAX_TRAP_LEN) {
+ 			trap_list->list_len++;
+ 			list_add_tail(&trap->list, &trap_list->list);
+ 		} else {
+ 			pr_warn_ratelimited("hfi1: Maximim trap limit reached for 0x%0x traps\n",
+ 					    trap->data.generic_type);
+ 			kfree(trap);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Next check to see if there is a timer pending.  If not, set it up
+ 	 * and get the first trap from the list.
+ 	 */
+ 	node = NULL;
+ 	if (!timer_pending(&ibp->rvp.trap_timer)) {
+ 		/*
+ 		 * o14-2
+ 		 * If the time out is set we have to wait until it expires
+ 		 * before the trap can be sent.
+ 		 * This should be > RVT_TRAP_TIMEOUT
+ 		 */
+ 		timeout = (RVT_TRAP_TIMEOUT *
+ 			   (1UL << ibp->rvp.subnet_timeout)) / 1000;
+ 		mod_timer(&ibp->rvp.trap_timer,
+ 			  jiffies + usecs_to_jiffies(timeout));
+ 		node = list_first_entry(&trap_list->list, struct trap_node,
+ 					list);
+ 		node->in_use = 1;
+ 	}
+ 	spin_unlock_irqrestore(&ibp->rvp.lock, flags);
+ 
+ 	return node;
+ }
+ 
+ static void subn_handle_opa_trap_repress(struct hfi1_ibport *ibp,
+ 					 struct opa_smp *smp)
+ {
+ 	struct trap_list *trap_list;
+ 	struct trap_node *trap;
+ 	unsigned long flags;
+ 	int i;
+ 
+ 	if (smp->attr_id != IB_SMP_ATTR_NOTICE)
+ 		return;
+ 
+ 	spin_lock_irqsave(&ibp->rvp.lock, flags);
+ 	for (i = 0; i < RVT_MAX_TRAP_LISTS; i++) {
+ 		trap_list = &ibp->rvp.trap_lists[i];
+ 		trap = list_first_entry_or_null(&trap_list->list,
+ 						struct trap_node, list);
+ 		if (trap && trap->tid == smp->tid) {
+ 			if (trap->in_use) {
+ 				trap->repress = 1;
+ 			} else {
+ 				trap_list->list_len--;
+ 				list_del(&trap->list);
+ 				kfree(trap);
+ 			}
+ 			break;
+ 		}
+ 	}
+ 	spin_unlock_irqrestore(&ibp->rvp.lock, flags);
+ }
+ 
+ static void hfi1_update_sm_ah_attr(struct hfi1_ibport *ibp,
+ 				   struct rdma_ah_attr *attr, u32 dlid)
+ {
+ 	rdma_ah_set_dlid(attr, dlid);
+ 	rdma_ah_set_port_num(attr, ppd_from_ibp(ibp)->port);
+ 	if (dlid >= be16_to_cpu(IB_MULTICAST_LID_BASE)) {
+ 		struct ib_global_route *grh = rdma_ah_retrieve_grh(attr);
+ 
+ 		rdma_ah_set_ah_flags(attr, IB_AH_GRH);
+ 		grh->sgid_index = 0;
+ 		grh->hop_limit = 1;
+ 		grh->dgid.global.subnet_prefix =
+ 			ibp->rvp.gid_prefix;
+ 		grh->dgid.global.interface_id = OPA_MAKE_ID(dlid);
+ 	}
+ }
+ 
+ static int hfi1_modify_qp0_ah(struct hfi1_ibport *ibp,
+ 			      struct rvt_ah *ah, u32 dlid)
+ {
+ 	struct rdma_ah_attr attr;
+ 	struct rvt_qp *qp0;
+ 	int ret = -EINVAL;
+ 
+ 	memset(&attr, 0, sizeof(attr));
+ 	attr.type = ah->ibah.type;
+ 	hfi1_update_sm_ah_attr(ibp, &attr, dlid);
+ 	rcu_read_lock();
+ 	qp0 = rcu_dereference(ibp->rvp.qp[0]);
+ 	if (qp0)
+ 		ret = rdma_modify_ah(&ah->ibah, &attr);
+ 	rcu_read_unlock();
+ 	return ret;
+ }
+ 
+ static struct ib_ah *hfi1_create_qp0_ah(struct hfi1_ibport *ibp, u32 dlid)
+ {
+ 	struct rdma_ah_attr attr;
+ 	struct ib_ah *ah = ERR_PTR(-EINVAL);
+ 	struct rvt_qp *qp0;
+ 	struct hfi1_pportdata *ppd = ppd_from_ibp(ibp);
+ 	struct hfi1_devdata *dd = dd_from_ppd(ppd);
+ 	u8 port_num = ppd->port;
+ 
+ 	memset(&attr, 0, sizeof(attr));
+ 	attr.type = rdma_ah_find_type(&dd->verbs_dev.rdi.ibdev, port_num);
+ 	hfi1_update_sm_ah_attr(ibp, &attr, dlid);
+ 	rcu_read_lock();
+ 	qp0 = rcu_dereference(ibp->rvp.qp[0]);
+ 	if (qp0)
+ 		ah = rdma_create_ah(qp0->ibqp.pd, &attr);
+ 	rcu_read_unlock();
+ 	return ah;
+ }
+ 
+ static void send_trap(struct hfi1_ibport *ibp, struct trap_node *trap)
++>>>>>>> 51e658f5dd36 (IB/rdmavt, hfi1, qib: Enhance rdmavt and hfi1 to use 32 bit lids)
  {
  	struct ib_mad_send_buf *send_buf;
  	struct ib_mad_agent *agent;
@@@ -1178,9 -1434,9 +1375,13 @@@ static int __subn_set_opa_portinfo(stru
  		spin_lock_irqsave(&ibp->rvp.lock, flags);
  		if (ibp->rvp.sm_ah) {
  			if (smlid != ibp->rvp.sm_lid)
++<<<<<<< HEAD
 +				ibp->rvp.sm_ah->attr.dlid = smlid;
++=======
+ 				hfi1_modify_qp0_ah(ibp, ibp->rvp.sm_ah, smlid);
++>>>>>>> 51e658f5dd36 (IB/rdmavt, hfi1, qib: Enhance rdmavt and hfi1 to use 32 bit lids)
  			if (msl != ibp->rvp.sm_sl)
 -				rdma_ah_set_sl(&ibp->rvp.sm_ah->attr, msl);
 +				ibp->rvp.sm_ah->attr.sl = msl;
  		}
  		spin_unlock_irqrestore(&ibp->rvp.lock, flags);
  		if (smlid != ibp->rvp.sm_lid)
diff --cc drivers/infiniband/hw/hfi1/verbs.c
index 06ac08cf82a6,83565e5f46d0..000000000000
--- a/drivers/infiniband/hw/hfi1/verbs.c
+++ b/drivers/infiniband/hw/hfi1/verbs.c
@@@ -1360,8 -1394,9 +1360,8 @@@ static int query_port(struct rvt_dev_in
  	struct hfi1_ibdev *verbs_dev = dev_from_rdi(rdi);
  	struct hfi1_devdata *dd = dd_from_dev(verbs_dev);
  	struct hfi1_pportdata *ppd = &dd->pport[port_num - 1];
- 	u16 lid = ppd->lid;
+ 	u32 lid = ppd->lid;
  
 -	/* props being zeroed by the caller, avoid zeroing it here */
  	props->lid = lid ? lid : 0;
  	props->lmc = ppd->lmc;
  	/* OPA logical states match IB logical states */
@@@ -1504,23 -1555,6 +1504,26 @@@ static void hfi1_notify_new_ah(struct i
  		ah->log_pmtu = ilog2(dd->vld[ah->vl].mtu);
  }
  
++<<<<<<< HEAD
 +struct ib_ah *hfi1_create_qp0_ah(struct hfi1_ibport *ibp, u16 dlid)
 +{
 +	struct ib_ah_attr attr;
 +	struct ib_ah *ah = ERR_PTR(-EINVAL);
 +	struct rvt_qp *qp0;
 +
 +	memset(&attr, 0, sizeof(attr));
 +	attr.dlid = dlid;
 +	attr.port_num = ppd_from_ibp(ibp)->port;
 +	rcu_read_lock();
 +	qp0 = rcu_dereference(ibp->rvp.qp[0]);
 +	if (qp0)
 +		ah = ib_create_ah(qp0->ibqp.pd, &attr);
 +	rcu_read_unlock();
 +	return ah;
 +}
 +
++=======
++>>>>>>> 51e658f5dd36 (IB/rdmavt, hfi1, qib: Enhance rdmavt and hfi1 to use 32 bit lids)
  /**
   * hfi1_get_npkeys - return the size of the PKEY table for context 0
   * @dd: the hfi1_ib device
diff --cc drivers/infiniband/hw/hfi1/verbs.h
index 76081f770f70,ab1618e32d9c..000000000000
--- a/drivers/infiniband/hw/hfi1/verbs.h
+++ b/drivers/infiniband/hw/hfi1/verbs.h
@@@ -307,15 -329,12 +307,19 @@@ void hfi1_rc_rcv(struct hfi1_packet *pa
  
  void hfi1_rc_hdrerr(
  	struct hfi1_ctxtdata *rcd,
 -	struct hfi1_packet *packet,
 +	struct ib_header *hdr,
 +	u32 rcv_flags,
  	struct rvt_qp *qp);
  
 -u8 ah_to_sc(struct ib_device *ibdev, struct rdma_ah_attr *ah_attr);
 +u8 ah_to_sc(struct ib_device *ibdev, struct ib_ah_attr *ah_attr);
 +
++<<<<<<< HEAD
 +struct ib_ah *hfi1_create_qp0_ah(struct hfi1_ibport *ibp, u16 dlid);
  
 +void hfi1_rc_send_complete(struct rvt_qp *qp, struct ib_header *hdr);
++=======
+ void hfi1_rc_send_complete(struct rvt_qp *qp, struct hfi1_opa_header *opah);
++>>>>>>> 51e658f5dd36 (IB/rdmavt, hfi1, qib: Enhance rdmavt and hfi1 to use 32 bit lids)
  
  void hfi1_ud_rcv(struct hfi1_packet *packet);
  
diff --git a/drivers/infiniband/hw/hfi1/chip.c b/drivers/infiniband/hw/hfi1/chip.c
index 7ed61dba0ba4..117753941961 100644
--- a/drivers/infiniband/hw/hfi1/chip.c
+++ b/drivers/infiniband/hw/hfi1/chip.c
@@ -10002,10 +10002,16 @@ static void set_lidlmc(struct hfi1_pportdata *ppd)
 	struct hfi1_devdata *dd = ppd->dd;
 	u32 mask = ~((1U << ppd->lmc) - 1);
 	u64 c1 = read_csr(ppd->dd, DCC_CFG_PORT_CONFIG1);
+	u32 lid;
 
+	/*
+	 * Program 0 in CSR if port lid is extended. This prevents
+	 * 9B packets being sent out for large lids.
+	 */
+	lid = (ppd->lid >= be16_to_cpu(IB_MULTICAST_LID_BASE)) ? 0 : ppd->lid;
 	c1 &= ~(DCC_CFG_PORT_CONFIG1_TARGET_DLID_SMASK
 		| DCC_CFG_PORT_CONFIG1_DLID_MASK_SMASK);
-	c1 |= ((ppd->lid & DCC_CFG_PORT_CONFIG1_TARGET_DLID_MASK)
+	c1 |= ((lid & DCC_CFG_PORT_CONFIG1_TARGET_DLID_MASK)
 			<< DCC_CFG_PORT_CONFIG1_TARGET_DLID_SHIFT) |
 	      ((mask & DCC_CFG_PORT_CONFIG1_DLID_MASK_MASK)
 			<< DCC_CFG_PORT_CONFIG1_DLID_MASK_SHIFT);
@@ -10016,7 +10022,7 @@ static void set_lidlmc(struct hfi1_pportdata *ppd)
 	 */
 	sreg = ((mask & SEND_CTXT_CHECK_SLID_MASK_MASK) <<
 			SEND_CTXT_CHECK_SLID_MASK_SHIFT) |
-	       (((ppd->lid & mask) & SEND_CTXT_CHECK_SLID_VALUE_MASK) <<
+	       (((lid & mask) & SEND_CTXT_CHECK_SLID_VALUE_MASK) <<
 			SEND_CTXT_CHECK_SLID_VALUE_SHIFT);
 
 	for (i = 0; i < dd->chip_send_contexts; i++) {
@@ -10026,7 +10032,7 @@ static void set_lidlmc(struct hfi1_pportdata *ppd)
 	}
 
 	/* Now we have to do the same thing for the sdma engines */
-	sdma_update_lmc(dd, mask, ppd->lid);
+	sdma_update_lmc(dd, mask, lid);
 }
 
 static int wait_phy_linkstate(struct hfi1_devdata *dd, u32 state, u32 msecs)
diff --git a/drivers/infiniband/hw/hfi1/hfi.h b/drivers/infiniband/hw/hfi1/hfi.h
index 9719cf207532..c2a55e240e80 100644
--- a/drivers/infiniband/hw/hfi1/hfi.h
+++ b/drivers/infiniband/hw/hfi1/hfi.h
@@ -627,7 +627,7 @@ struct hfi1_pportdata {
 	u32 ibmaxlen;
 	u32 current_egress_rate; /* units [10^6 bits/sec] */
 	/* LID programmed for this instance */
-	u16 lid;
+	u32 lid;
 	/* list of pkeys programmed; 0 if not set */
 	u16 pkeys[MAX_PKEY_VALUES];
 	u16 link_width_supported;
* Unmerged path drivers/infiniband/hw/hfi1/mad.c
* Unmerged path drivers/infiniband/hw/hfi1/verbs.c
* Unmerged path drivers/infiniband/hw/hfi1/verbs.h
diff --git a/drivers/infiniband/hw/qib/qib_mad.c b/drivers/infiniband/hw/qib/qib_mad.c
index 11242e3474f7..f0a95d3fe465 100644
--- a/drivers/infiniband/hw/qib/qib_mad.c
+++ b/drivers/infiniband/hw/qib/qib_mad.c
@@ -105,7 +105,7 @@ static void qib_send_trap(struct qib_ibport *ibp, void *data, unsigned len)
 		if (ibp->rvp.sm_lid != be16_to_cpu(IB_LID_PERMISSIVE)) {
 			struct ib_ah *ah;
 
-			ah = qib_create_qp0_ah(ibp, ibp->rvp.sm_lid);
+			ah = qib_create_qp0_ah(ibp, (u16)ibp->rvp.sm_lid);
 			if (IS_ERR(ah))
 				ret = PTR_ERR(ah);
 			else {
@@ -499,7 +499,7 @@ static int subn_get_portinfo(struct ib_smp *smp, struct ib_device *ibdev,
 		pip->mkey = ibp->rvp.mkey;
 	pip->gid_prefix = ibp->rvp.gid_prefix;
 	pip->lid = cpu_to_be16(ppd->lid);
-	pip->sm_lid = cpu_to_be16(ibp->rvp.sm_lid);
+	pip->sm_lid = cpu_to_be16((u16)ibp->rvp.sm_lid);
 	pip->cap_mask = cpu_to_be32(ibp->rvp.port_cap_flags);
 	/* pip->diag_code; */
 	pip->mkey_lease_period = cpu_to_be16(ibp->rvp.mkey_lease_period);
diff --git a/include/rdma/rdma_vt.h b/include/rdma/rdma_vt.h
index 2b925099bac5..d79bffa5174a 100644
--- a/include/rdma/rdma_vt.h
+++ b/include/rdma/rdma_vt.h
@@ -81,7 +81,7 @@ struct rvt_ibport {
 	__be16 pma_counter_select[5];
 	u16 pma_tag;
 	u16 mkey_lease_period;
-	u16 sm_lid;
+	u32 sm_lid;
 	u8 sm_sl;
 	u8 mkeyprot;
 	u8 subnet_timeout;
