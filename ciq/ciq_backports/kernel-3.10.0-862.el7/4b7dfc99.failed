net/mlx5e: Early-return on empty completion queues

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Early-return on empty completion queues (Kamal Heib) [1456694]
Rebuild_FUZZ: 95.83%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit 4b7dfc9925143eb4a55bbb97c033d6da03b29bff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/4b7dfc99.failed

NAPI context handles different kinds of completion queues
(RX, TX, and others). Hence, upon a poll trial, some of them
might be empty.
Here we early-return upon empty completion queues, as well as
full rx buffer, and save unnecessary logic and memory barriers.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit 4b7dfc9925143eb4a55bbb97c033d6da03b29bff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 6de42c8730ba,ab1213a3615e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -813,6 -984,8 +816,11 @@@ mpwrq_cqe_out
  int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
  {
  	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
++<<<<<<< HEAD
++=======
+ 	struct mlx5e_xdpsq *xdpsq;
+ 	struct mlx5_cqe64 *cqe;
++>>>>>>> 4b7dfc992514 (net/mlx5e: Early-return on empty completion queues)
  	int work_done = 0;
  
  	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))
@@@ -821,12 -994,13 +829,18 @@@
  	if (cq->decmprs_left)
  		work_done += mlx5e_decompress_cqes_cont(rq, cq, 0, budget);
  
++<<<<<<< HEAD
 +	for (; work_done < budget; work_done++) {
 +		struct mlx5_cqe64 *cqe = mlx5e_get_cqe(cq);
++=======
+ 	cqe = mlx5_cqwq_get_cqe(&cq->wq);
+ 	if (!cqe)
+ 		return 0;
++>>>>>>> 4b7dfc992514 (net/mlx5e: Early-return on empty completion queues)
  
- 		if (!cqe)
- 			break;
+ 	xdpsq = &rq->xdpsq;
  
+ 	do {
  		if (mlx5_get_cqe_format(cqe) == MLX5_COMPRESSED) {
  			work_done +=
  				mlx5e_decompress_cqes_start(rq, cq,
@@@ -837,8 -1011,13 +851,8 @@@
  		mlx5_cqwq_pop(&cq->wq);
  
  		rq->handle_rx_cqe(rq, cqe);
- 	}
+ 	} while ((++work_done < budget) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));
  
 -	if (xdpsq->db.doorbell) {
 -		mlx5e_xmit_xdp_doorbell(xdpsq);
 -		xdpsq->db.doorbell = false;
 -	}
 -
  	mlx5_cqwq_update_db_record(&cq->wq);
  
  	/* ensure cq space is freed before enabling more cqes */
@@@ -846,3 -1025,202 +860,205 @@@
  
  	return work_done;
  }
++<<<<<<< HEAD
++=======
+ 
+ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)
+ {
+ 	struct mlx5e_xdpsq *sq;
+ 	struct mlx5_cqe64 *cqe;
+ 	struct mlx5e_rq *rq;
+ 	u16 sqcc;
+ 	int i;
+ 
+ 	sq = container_of(cq, struct mlx5e_xdpsq, cq);
+ 
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
+ 		return false;
+ 
+ 	cqe = mlx5_cqwq_get_cqe(&cq->wq);
+ 	if (!cqe)
+ 		return false;
+ 
+ 	rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 
+ 	/* sq->cc must be updated only after mlx5_cqwq_update_db_record(),
+ 	 * otherwise a cq overrun may occur
+ 	 */
+ 	sqcc = sq->cc;
+ 
+ 	i = 0;
+ 	do {
+ 		u16 wqe_counter;
+ 		bool last_wqe;
+ 
+ 		mlx5_cqwq_pop(&cq->wq);
+ 
+ 		wqe_counter = be16_to_cpu(cqe->wqe_counter);
+ 
+ 		do {
+ 			struct mlx5e_dma_info *di;
+ 			u16 ci;
+ 
+ 			last_wqe = (sqcc == wqe_counter);
+ 
+ 			ci = sqcc & sq->wq.sz_m1;
+ 			di = &sq->db.di[ci];
+ 
+ 			sqcc++;
+ 			/* Recycle RX page */
+ 			mlx5e_page_release(rq, di, true);
+ 		} while (!last_wqe);
+ 	} while ((++i < MLX5E_TX_CQ_POLL_BUDGET) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));
+ 
+ 	mlx5_cqwq_update_db_record(&cq->wq);
+ 
+ 	/* ensure cq space is freed before enabling more cqes */
+ 	wmb();
+ 
+ 	sq->cc = sqcc;
+ 	return (i == MLX5E_TX_CQ_POLL_BUDGET);
+ }
+ 
+ void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq)
+ {
+ 	struct mlx5e_rq *rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 	struct mlx5e_dma_info *di;
+ 	u16 ci;
+ 
+ 	while (sq->cc != sq->pc) {
+ 		ci = sq->cc & sq->wq.sz_m1;
+ 		di = &sq->db.di[ci];
+ 		sq->cc++;
+ 
+ 		mlx5e_page_release(rq, di, false);
+ 	}
+ }
+ 
+ #ifdef CONFIG_MLX5_CORE_IPOIB
+ 
+ #define MLX5_IB_GRH_DGID_OFFSET 24
+ #define MLX5_GID_SIZE           16
+ 
+ static inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,
+ 					 struct mlx5_cqe64 *cqe,
+ 					 u32 cqe_bcnt,
+ 					 struct sk_buff *skb)
+ {
+ 	struct net_device *netdev = rq->netdev;
+ 	struct mlx5e_tstamp *tstamp = rq->tstamp;
+ 	char *pseudo_header;
+ 	u8 *dgid;
+ 	u8 g;
+ 
+ 	g = (be32_to_cpu(cqe->flags_rqpn) >> 28) & 3;
+ 	dgid = skb->data + MLX5_IB_GRH_DGID_OFFSET;
+ 	if ((!g) || dgid[0] != 0xff)
+ 		skb->pkt_type = PACKET_HOST;
+ 	else if (memcmp(dgid, netdev->broadcast + 4, MLX5_GID_SIZE) == 0)
+ 		skb->pkt_type = PACKET_BROADCAST;
+ 	else
+ 		skb->pkt_type = PACKET_MULTICAST;
+ 
+ 	/* TODO: IB/ipoib: Allow mcast packets from other VFs
+ 	 * 68996a6e760e5c74654723eeb57bf65628ae87f4
+ 	 */
+ 
+ 	skb_pull(skb, MLX5_IB_GRH_BYTES);
+ 
+ 	skb->protocol = *((__be16 *)(skb->data));
+ 
+ 	skb->ip_summed = CHECKSUM_COMPLETE;
+ 	skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
+ 
+ 	if (unlikely(mlx5e_rx_hw_stamp(tstamp)))
+ 		mlx5e_fill_hwstamp(tstamp, get_cqe_ts(cqe), skb_hwtstamps(skb));
+ 
+ 	skb_record_rx_queue(skb, rq->ix);
+ 
+ 	if (likely(netdev->features & NETIF_F_RXHASH))
+ 		mlx5e_skb_set_hash(cqe, skb);
+ 
+ 	/* 20 bytes of ipoib header and 4 for encap existing */
+ 	pseudo_header = skb_push(skb, MLX5_IPOIB_PSEUDO_LEN);
+ 	memset(pseudo_header, 0, MLX5_IPOIB_PSEUDO_LEN);
+ 	skb_reset_mac_header(skb);
+ 	skb_pull(skb, MLX5_IPOIB_HARD_LEN);
+ 
+ 	skb->dev = netdev;
+ 
+ 	rq->stats.csum_complete++;
+ 	rq->stats.packets++;
+ 	rq->stats.bytes += cqe_bcnt;
+ }
+ 
+ void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_wqe_frag_info *wi;
+ 	struct mlx5e_rx_wqe *wqe;
+ 	__be16 wqe_counter_be;
+ 	struct sk_buff *skb;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	wi             = &rq->wqe.frag_info[wqe_counter];
+ 	cqe_bcnt       = be32_to_cpu(cqe->byte_cnt);
+ 
+ 	skb = skb_from_cqe(rq, cqe, wi, cqe_bcnt);
+ 	if (!skb)
+ 		goto wq_free_wqe;
+ 
+ 	mlx5i_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ 
+ wq_free_wqe:
+ 	mlx5e_free_rx_wqe_reuse(rq, wi);
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ #endif /* CONFIG_MLX5_CORE_IPOIB */
+ 
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 
+ void mlx5e_ipsec_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_wqe_frag_info *wi;
+ 	struct mlx5e_rx_wqe *wqe;
+ 	__be16 wqe_counter_be;
+ 	struct sk_buff *skb;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	wi             = &rq->wqe.frag_info[wqe_counter];
+ 	cqe_bcnt       = be32_to_cpu(cqe->byte_cnt);
+ 
+ 	skb = skb_from_cqe(rq, cqe, wi, cqe_bcnt);
+ 	if (unlikely(!skb)) {
+ 		/* a DROP, save the page-reuse checks */
+ 		mlx5e_free_rx_wqe(rq, wi);
+ 		goto wq_ll_pop;
+ 	}
+ 	skb = mlx5e_ipsec_handle_rx_skb(rq->netdev, skb);
+ 	if (unlikely(!skb)) {
+ 		mlx5e_free_rx_wqe(rq, wi);
+ 		goto wq_ll_pop;
+ 	}
+ 
+ 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ 
+ 	mlx5e_free_rx_wqe_reuse(rq, wi);
+ wq_ll_pop:
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ #endif /* CONFIG_MLX5_EN_IPSEC */
++>>>>>>> 4b7dfc992514 (net/mlx5e: Early-return on empty completion queues)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 46d0d61188dc,80d2121643ee..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@@ -405,7 -393,8 +405,12 @@@ netdev_tx_t mlx5e_xmit(struct sk_buff *
  
  bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget)
  {
++<<<<<<< HEAD
 +	struct mlx5e_sq *sq;
++=======
+ 	struct mlx5e_txqsq *sq;
+ 	struct mlx5_cqe64 *cqe;
++>>>>>>> 4b7dfc992514 (net/mlx5e: Early-return on empty completion queues)
  	u32 dma_fifo_cc;
  	u32 nbytes;
  	u16 npkts;
@@@ -433,10 -426,6 +442,13 @@@
  		u16 wqe_counter;
  		bool last_wqe;
  
++<<<<<<< HEAD
 +		cqe = mlx5e_get_cqe(cq);
 +		if (!cqe)
 +			break;
 +
++=======
++>>>>>>> 4b7dfc992514 (net/mlx5e: Early-return on empty completion queues)
  		mlx5_cqwq_pop(&cq->wq);
  
  		wqe_counter = be16_to_cpu(cqe->wqe_counter);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
