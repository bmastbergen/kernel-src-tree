IB/mlx5: Allow future extension of libmlx5 input data

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Eli Cohen <eli@mellanox.com>
commit b037c29a8056b8e896c4e084ba7cc30d6a1f165f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b037c29a.failed

Current check requests that new fields in struct
mlx5_ib_alloc_ucontext_req_v2 that are not known to the driver be zero.
This was introduced so new libraries passing additional information to
the kernel through struct mlx5_ib_alloc_ucontext_req_v2 will be notified
by old kernels that do not support their request by failing the
operation. This schecme is problematic since it requires libmlx5 to issue
the requests with descending input size for struct
mlx5_ib_alloc_ucontext_req_v2.

To avoid this, we require that new features that will obey the following
rules:
If the feature requires one or more fields in the response and the at
least one of the fields can be encoded such that a zero value means the
kernel ignored the request then this field will provide the indication
to the library. If no response is required or if zero is a valid
response, a new field should be added that indicates to the library
whether its request was processed.

Fixes: b368d7cb8ceb ('IB/mlx5: Add hca_core_clock_offset to udata in init_ucontext')
	Signed-off-by: Eli Cohen <eli@mellanox.com>
	Reviewed-by: Matan Barak <matanb@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit b037c29a8056b8e896c4e084ba7cc30d6a1f165f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/cq.c
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/qp.c
#	include/linux/mlx5/device.h
#	include/linux/mlx5/driver.h
diff --cc drivers/infiniband/hw/mlx5/cq.c
index b3ef47c3ab73,31803b367104..000000000000
--- a/drivers/infiniband/hw/mlx5/cq.c
+++ b/drivers/infiniband/hw/mlx5/cq.c
@@@ -790,7 -788,7 +790,11 @@@ static int create_cq_user(struct mlx5_i
  	MLX5_SET(cqc, cqc, log_page_size,
  		 page_shift - MLX5_ADAPTER_PAGE_SHIFT);
  
++<<<<<<< HEAD
 +	*index = to_mucontext(context)->uuari.uars[0].index;
++=======
+ 	*index = to_mucontext(context)->bfregi.sys_pages[0];
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  
  	if (ucmd.cqe_comp_en == 1) {
  		if (unlikely((*cqe_size != 64) ||
diff --cc drivers/infiniband/hw/mlx5/main.c
index 9454bdb12197,664067239de5..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -1009,14 -1073,9 +1083,18 @@@ static struct ib_ucontext *mlx5_ib_allo
  	struct mlx5_ib_alloc_ucontext_req_v2 req = {};
  	struct mlx5_ib_alloc_ucontext_resp resp = {};
  	struct mlx5_ib_ucontext *context;
++<<<<<<< HEAD
 +	struct mlx5_uuar_info *uuari;
 +	struct mlx5_uar *uars;
 +	int gross_uuars;
 +	int num_uars;
 +	int ver;
 +	int uuarn;
++=======
+ 	struct mlx5_bfreg_info *bfregi;
+ 	int ver;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  	int err;
- 	int i;
  	size_t reqlen;
  	size_t min_req_v2 = offsetof(struct mlx5_ib_alloc_ucontext_req_v2,
  				     max_cqe_version);
@@@ -1042,27 -1102,14 +1121,38 @@@
  	if (req.flags)
  		return ERR_PTR(-EINVAL);
  
++<<<<<<< HEAD
 +	if (req.total_num_uuars > MLX5_MAX_UUARS)
 +		return ERR_PTR(-ENOMEM);
 +
 +	if (req.total_num_uuars == 0)
 +		return ERR_PTR(-EINVAL);
 +
 +	if (req.comp_mask || req.reserved0 || req.reserved1 || req.reserved2)
 +		return ERR_PTR(-EOPNOTSUPP);
 +
 +	if (reqlen > sizeof(req) &&
 +	    !ib_is_udata_cleared(udata, sizeof(req),
 +				 reqlen - sizeof(req)))
 +		return ERR_PTR(-EOPNOTSUPP);
 +
 +	req.total_num_uuars = ALIGN(req.total_num_uuars,
 +				    MLX5_NON_FP_BF_REGS_PER_PAGE);
 +	if (req.num_low_latency_uuars > req.total_num_uuars - 1)
 +		return ERR_PTR(-EINVAL);
 +
 +	num_uars = req.total_num_uuars / MLX5_NON_FP_BF_REGS_PER_PAGE;
 +	gross_uuars = num_uars * MLX5_BF_REGS_PER_PAGE;
++=======
+ 	if (req.comp_mask || req.reserved0 || req.reserved1 || req.reserved2)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	req.total_num_bfregs = ALIGN(req.total_num_bfregs,
+ 				    MLX5_NON_FP_BFREGS_PER_UAR);
+ 	if (req.num_low_latency_bfregs > req.total_num_bfregs - 1)
+ 		return ERR_PTR(-EINVAL);
+ 
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  	resp.qp_tab_size = 1 << MLX5_CAP_GEN(dev->mdev, log_max_qp);
  	if (mlx5_core_is_pf(dev->mdev) && MLX5_CAP_GEN(dev->mdev, bf))
  		resp.bf_reg_size = 1 << MLX5_CAP_GEN(dev->mdev, log_bf_reg_size);
@@@ -1082,41 -1129,34 +1172,71 @@@
  	if (!context)
  		return ERR_PTR(-ENOMEM);
  
++<<<<<<< HEAD
 +	uuari = &context->uuari;
 +	mutex_init(&uuari->lock);
 +	uars = kcalloc(num_uars, sizeof(*uars), GFP_KERNEL);
 +	if (!uars) {
++=======
+ 	lib_uar_4k = false;
+ 	bfregi = &context->bfregi;
+ 
+ 	/* updates req->total_num_bfregs */
+ 	err = calc_total_bfregs(dev, lib_uar_4k, &req, &bfregi->num_sys_pages);
+ 	if (err)
+ 		goto out_ctx;
+ 
+ 	mutex_init(&bfregi->lock);
+ 	bfregi->lib_uar_4k = lib_uar_4k;
+ 	bfregi->count = kcalloc(req.total_num_bfregs, sizeof(*bfregi->count),
+ 				GFP_KERNEL);
+ 	if (!bfregi->count) {
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  		err = -ENOMEM;
  		goto out_ctx;
  	}
  
++<<<<<<< HEAD
 +	uuari->bitmap = kcalloc(BITS_TO_LONGS(gross_uuars),
 +				sizeof(*uuari->bitmap),
 +				GFP_KERNEL);
 +	if (!uuari->bitmap) {
 +		err = -ENOMEM;
 +		goto out_uar_ctx;
 +	}
 +	/*
 +	 * clear all fast path uuars
 +	 */
 +	for (i = 0; i < gross_uuars; i++) {
 +		uuarn = i & 3;
 +		if (uuarn == 2 || uuarn == 3)
 +			set_bit(i, uuari->bitmap);
 +	}
 +
 +	uuari->count = kcalloc(gross_uuars, sizeof(*uuari->count), GFP_KERNEL);
 +	if (!uuari->count) {
 +		err = -ENOMEM;
 +		goto out_bitmap;
 +	}
 +
 +	for (i = 0; i < num_uars; i++) {
 +		err = mlx5_cmd_alloc_uar(dev->mdev, &uars[i].index);
 +		if (err)
 +			goto out_count;
 +	}
++=======
+ 	bfregi->sys_pages = kcalloc(bfregi->num_sys_pages,
+ 				    sizeof(*bfregi->sys_pages),
+ 				    GFP_KERNEL);
+ 	if (!bfregi->sys_pages) {
+ 		err = -ENOMEM;
+ 		goto out_count;
+ 	}
+ 
+ 	err = allocate_uars(dev, context);
+ 	if (err)
+ 		goto out_sys_pages;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  
  #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
  	context->ibucontext.invalidate_range = &mlx5_ib_invalidate_range;
@@@ -1166,11 -1213,10 +1286,17 @@@
  	if (err)
  		goto out_td;
  
++<<<<<<< HEAD
 +	uuari->ver = ver;
 +	uuari->num_low_latency_uuars = req.num_low_latency_uuars;
 +	uuari->uars = uars;
 +	uuari->num_uars = num_uars;
++=======
+ 	bfregi->ver = ver;
+ 	bfregi->num_low_latency_bfregs = req.num_low_latency_bfregs;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  	context->cqe_version = resp.cqe_version;
+ 	context->lib_caps = false;
  
  	return &context->ibucontext;
  
@@@ -1178,20 -1224,21 +1304,27 @@@ out_td
  	if (MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))
  		mlx5_core_dealloc_transport_domain(dev->mdev, context->tdn);
  
 -out_page:
 -	free_page(context->upd_xlt_page);
 -
  out_uars:
- 	for (i--; i >= 0; i--)
- 		mlx5_cmd_free_uar(dev->mdev, uars[i].index);
+ 	deallocate_uars(dev, context);
+ 
+ out_sys_pages:
+ 	kfree(bfregi->sys_pages);
+ 
  out_count:
 -	kfree(bfregi->count);
 +	kfree(uuari->count);
 +
++<<<<<<< HEAD
 +out_bitmap:
 +	kfree(uuari->bitmap);
  
 +out_uar_ctx:
 +	kfree(uars);
 +
++=======
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  out_ctx:
  	kfree(context);
+ 
  	return ERR_PTR(err);
  }
  
@@@ -1199,20 -1246,16 +1332,32 @@@ static int mlx5_ib_dealloc_ucontext(str
  {
  	struct mlx5_ib_ucontext *context = to_mucontext(ibcontext);
  	struct mlx5_ib_dev *dev = to_mdev(ibcontext->device);
++<<<<<<< HEAD
 +	struct mlx5_uuar_info *uuari = &context->uuari;
 +	int i;
++=======
+ 	struct mlx5_bfreg_info *bfregi;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  
+ 	bfregi = &context->bfregi;
  	if (MLX5_CAP_GEN(dev->mdev, log_max_transport_domain))
  		mlx5_core_dealloc_transport_domain(dev->mdev, context->tdn);
  
++<<<<<<< HEAD
 +	for (i = 0; i < uuari->num_uars; i++) {
 +		if (mlx5_cmd_free_uar(dev->mdev, uuari->uars[i].index))
 +			mlx5_ib_warn(dev, "failed to free UAR 0x%x\n", uuari->uars[i].index);
 +	}
 +
 +	kfree(uuari->count);
 +	kfree(uuari->bitmap);
 +	kfree(uuari->uars);
++=======
+ 	free_page(context->upd_xlt_page);
+ 	deallocate_uars(dev, context);
+ 	kfree(bfregi->sys_pages);
+ 	kfree(bfregi->count);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  	kfree(context);
  
  	return 0;
@@@ -1402,14 -1464,7 +1566,18 @@@ static int uar_mmap(struct mlx5_ib_dev 
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
 +	if (vma->vm_end - vma->vm_start != PAGE_SIZE)
 +		return -EINVAL;
 +
 +	idx = get_index(vma->vm_pgoff);
 +	if (idx >= uuari->num_uars)
 +		return -EINVAL;
 +
 +	pfn = uar_index2pfn(dev, uuari->uars[idx].index);
++=======
+ 	pfn = uar_index2pfn(dev, bfregi, idx);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  	mlx5_ib_dbg(dev, "uar idx 0x%lx, pfn %pa\n", idx, &pfn);
  
  	vma->vm_page_prot = prot;
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index dfad8ddf0c5a,e1a4b93dce6b..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -125,6 -124,11 +124,14 @@@ struct mlx5_ib_ucontext 
  	/* Transport Domain number */
  	u32			tdn;
  	struct list_head	vma_private_list;
++<<<<<<< HEAD
++=======
+ 
+ 	unsigned long		upd_xlt_page;
+ 	/* protect ODP/KSM */
+ 	struct mutex		upd_xlt_page_mutex;
+ 	u64			lib_caps;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  };
  
  static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
diff --cc drivers/infiniband/hw/mlx5/qp.c
index 65e5d668d1be,6a83fb32599d..000000000000
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@@ -480,55 -480,48 +480,99 @@@ static int first_med_uuar(void
  	return 1;
  }
  
++<<<<<<< HEAD
 +static int next_uuar(int n)
 +{
 +	n++;
 +
 +	while (((n % 4) & 2))
 +		n++;
 +
 +	return n;
 +}
 +
 +static int num_med_uuar(struct mlx5_uuar_info *uuari)
 +{
 +	int n;
 +
 +	n = uuari->num_uars * MLX5_NON_FP_BF_REGS_PER_PAGE -
 +		uuari->num_low_latency_uuars - 1;
++=======
+ enum {
+ 	/* this is the first blue flame register in the array of bfregs assigned
+ 	 * to a processes. Since we do not use it for blue flame but rather
+ 	 * regular 64 bit doorbells, we do not need a lock for maintaiing
+ 	 * "odd/even" order
+ 	 */
+ 	NUM_NON_BLUE_FLAME_BFREGS = 1,
+ };
+ 
+ static int max_bfregs(struct mlx5_ib_dev *dev, struct mlx5_bfreg_info *bfregi)
+ {
+ 	return get_num_uars(dev, bfregi) * MLX5_NON_FP_BFREGS_PER_UAR;
+ }
+ 
+ static int num_med_bfreg(struct mlx5_ib_dev *dev,
+ 			 struct mlx5_bfreg_info *bfregi)
+ {
+ 	int n;
+ 
+ 	n = max_bfregs(dev, bfregi) - bfregi->num_low_latency_bfregs -
+ 	    NUM_NON_BLUE_FLAME_BFREGS;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  
  	return n >= 0 ? n : 0;
  }
  
++<<<<<<< HEAD
 +static int max_uuari(struct mlx5_uuar_info *uuari)
 +{
 +	return uuari->num_uars * 4;
 +}
 +
 +static int first_hi_uuar(struct mlx5_uuar_info *uuari)
++=======
+ static int first_hi_bfreg(struct mlx5_ib_dev *dev,
+ 			  struct mlx5_bfreg_info *bfregi)
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  {
  	int med;
 +	int i;
 +	int t;
  
++<<<<<<< HEAD
 +	med = num_med_uuar(uuari);
 +	for (t = 0, i = first_med_uuar();; i = next_uuar(i)) {
 +		t++;
 +		if (t == med)
 +			return next_uuar(i);
 +	}
 +
 +	return 0;
 +}
 +
 +static int alloc_high_class_uuar(struct mlx5_uuar_info *uuari)
 +{
 +	int i;
 +
 +	for (i = first_hi_uuar(uuari); i < max_uuari(uuari); i = next_uuar(i)) {
 +		if (!test_bit(i, uuari->bitmap)) {
 +			set_bit(i, uuari->bitmap);
 +			uuari->count[i]++;
++=======
+ 	med = num_med_bfreg(dev, bfregi);
+ 	return ++med;
+ }
+ 
+ static int alloc_high_class_bfreg(struct mlx5_ib_dev *dev,
+ 				  struct mlx5_bfreg_info *bfregi)
+ {
+ 	int i;
+ 
+ 	for (i = first_hi_bfreg(dev, bfregi); i < max_bfregs(dev, bfregi); i++) {
+ 		if (!bfregi->count[i]) {
+ 			bfregi->count[i]++;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  			return i;
  		}
  	}
@@@ -536,87 -529,61 +580,119 @@@
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
 +static int alloc_med_class_uuar(struct mlx5_uuar_info *uuari)
++=======
+ static int alloc_med_class_bfreg(struct mlx5_ib_dev *dev,
+ 				 struct mlx5_bfreg_info *bfregi)
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  {
 -	int minidx = first_med_bfreg();
 +	int minidx = first_med_uuar();
  	int i;
  
++<<<<<<< HEAD
 +	for (i = first_med_uuar(); i < first_hi_uuar(uuari); i = next_uuar(i)) {
 +		if (uuari->count[i] < uuari->count[minidx])
++=======
+ 	for (i = first_med_bfreg(); i < first_hi_bfreg(dev, bfregi); i++) {
+ 		if (bfregi->count[i] < bfregi->count[minidx])
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  			minidx = i;
 -		if (!bfregi->count[minidx])
 -			break;
  	}
  
 -	bfregi->count[minidx]++;
 +	uuari->count[minidx]++;
  	return minidx;
  }
  
++<<<<<<< HEAD
 +static int alloc_uuar(struct mlx5_uuar_info *uuari,
 +		      enum mlx5_ib_latency_class lat)
++=======
+ static int alloc_bfreg(struct mlx5_ib_dev *dev,
+ 		       struct mlx5_bfreg_info *bfregi,
+ 		       enum mlx5_ib_latency_class lat)
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  {
 -	int bfregn = -EINVAL;
 +	int uuarn = -EINVAL;
  
 -	mutex_lock(&bfregi->lock);
 +	mutex_lock(&uuari->lock);
  	switch (lat) {
  	case MLX5_IB_LATENCY_CLASS_LOW:
 -		BUILD_BUG_ON(NUM_NON_BLUE_FLAME_BFREGS != 1);
 -		bfregn = 0;
 -		bfregi->count[bfregn]++;
 +		uuarn = 0;
 +		uuari->count[uuarn]++;
  		break;
  
  	case MLX5_IB_LATENCY_CLASS_MEDIUM:
 -		if (bfregi->ver < 2)
 -			bfregn = -ENOMEM;
 +		if (uuari->ver < 2)
 +			uuarn = -ENOMEM;
  		else
++<<<<<<< HEAD
 +			uuarn = alloc_med_class_uuar(uuari);
++=======
+ 			bfregn = alloc_med_class_bfreg(dev, bfregi);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  		break;
  
  	case MLX5_IB_LATENCY_CLASS_HIGH:
 -		if (bfregi->ver < 2)
 -			bfregn = -ENOMEM;
 +		if (uuari->ver < 2)
 +			uuarn = -ENOMEM;
  		else
++<<<<<<< HEAD
 +			uuarn = alloc_high_class_uuar(uuari);
 +		break;
 +
 +	case MLX5_IB_LATENCY_CLASS_FAST_PATH:
 +		uuarn = 2;
++=======
+ 			bfregn = alloc_high_class_bfreg(dev, bfregi);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  		break;
  	}
 -	mutex_unlock(&bfregi->lock);
 +	mutex_unlock(&uuari->lock);
 +
 +	return uuarn;
 +}
  
 -	return bfregn;
++<<<<<<< HEAD
 +static void free_med_class_uuar(struct mlx5_uuar_info *uuari, int uuarn)
 +{
 +	clear_bit(uuarn, uuari->bitmap);
 +	--uuari->count[uuarn];
  }
  
 +static void free_high_class_uuar(struct mlx5_uuar_info *uuari, int uuarn)
 +{
 +	clear_bit(uuarn, uuari->bitmap);
 +	--uuari->count[uuarn];
 +}
 +
 +static void free_uuar(struct mlx5_uuar_info *uuari, int uuarn)
 +{
 +	int nuuars = uuari->num_uars * MLX5_BF_REGS_PER_PAGE;
 +	int high_uuar = nuuars - uuari->num_low_latency_uuars;
 +
 +	mutex_lock(&uuari->lock);
 +	if (uuarn == 0) {
 +		--uuari->count[uuarn];
 +		goto out;
 +	}
 +
 +	if (uuarn < high_uuar) {
 +		free_med_class_uuar(uuari, uuarn);
 +		goto out;
 +	}
 +
 +	free_high_class_uuar(uuari, uuarn);
 +
 +out:
 +	mutex_unlock(&uuari->lock);
++=======
+ static void free_bfreg(struct mlx5_ib_dev *dev, struct mlx5_bfreg_info *bfregi, int bfregn)
+ {
+ 	mutex_lock(&bfregi->lock);
+ 	bfregi->count[bfregn]--;
+ 	mutex_unlock(&bfregi->lock);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  }
  
  static enum mlx5_qp_state to_mlx5_state(enum ib_qp_state state)
@@@ -657,9 -624,20 +733,26 @@@ static void mlx5_ib_lock_cqs(struct mlx
  static void mlx5_ib_unlock_cqs(struct mlx5_ib_cq *send_cq,
  			       struct mlx5_ib_cq *recv_cq);
  
++<<<<<<< HEAD
 +static int uuarn_to_uar_index(struct mlx5_uuar_info *uuari, int uuarn)
 +{
 +	return uuari->uars[uuarn / MLX5_BF_REGS_PER_PAGE].index;
++=======
+ static int bfregn_to_uar_index(struct mlx5_ib_dev *dev,
+ 			       struct mlx5_bfreg_info *bfregi, int bfregn)
+ {
+ 	int bfregs_per_sys_page;
+ 	int index_of_sys_page;
+ 	int offset;
+ 
+ 	bfregs_per_sys_page = get_uars_per_sys_page(dev, bfregi->lib_uar_4k) *
+ 				MLX5_NON_FP_BFREGS_PER_UAR;
+ 	index_of_sys_page = bfregn / bfregs_per_sys_page;
+ 
+ 	offset = bfregn % bfregs_per_sys_page / MLX5_NON_FP_BFREGS_PER_UAR;
+ 
+ 	return bfregi->sys_pages[index_of_sys_page] + offset;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  }
  
  static int mlx5_ib_umem_get(struct mlx5_ib_dev *dev,
@@@ -794,27 -779,27 +894,47 @@@ static int create_user_qp(struct mlx5_i
  	 */
  	if (qp->flags & MLX5_IB_QP_CROSS_CHANNEL)
  		/* In CROSS_CHANNEL CQ and QP must use the same UAR */
 -		bfregn = MLX5_CROSS_CHANNEL_BFREG;
 +		uuarn = MLX5_CROSS_CHANNEL_UUAR;
  	else {
++<<<<<<< HEAD
 +		uuarn = alloc_uuar(&context->uuari, MLX5_IB_LATENCY_CLASS_HIGH);
 +		if (uuarn < 0) {
 +			mlx5_ib_dbg(dev, "failed to allocate low latency UUAR\n");
 +			mlx5_ib_dbg(dev, "reverting to medium latency\n");
 +			uuarn = alloc_uuar(&context->uuari, MLX5_IB_LATENCY_CLASS_MEDIUM);
 +			if (uuarn < 0) {
 +				mlx5_ib_dbg(dev, "failed to allocate medium latency UUAR\n");
 +				mlx5_ib_dbg(dev, "reverting to high latency\n");
 +				uuarn = alloc_uuar(&context->uuari, MLX5_IB_LATENCY_CLASS_LOW);
 +				if (uuarn < 0) {
 +					mlx5_ib_warn(dev, "uuar allocation failed\n");
 +					return uuarn;
++=======
+ 		bfregn = alloc_bfreg(dev, &context->bfregi, MLX5_IB_LATENCY_CLASS_HIGH);
+ 		if (bfregn < 0) {
+ 			mlx5_ib_dbg(dev, "failed to allocate low latency BFREG\n");
+ 			mlx5_ib_dbg(dev, "reverting to medium latency\n");
+ 			bfregn = alloc_bfreg(dev, &context->bfregi, MLX5_IB_LATENCY_CLASS_MEDIUM);
+ 			if (bfregn < 0) {
+ 				mlx5_ib_dbg(dev, "failed to allocate medium latency BFREG\n");
+ 				mlx5_ib_dbg(dev, "reverting to high latency\n");
+ 				bfregn = alloc_bfreg(dev, &context->bfregi, MLX5_IB_LATENCY_CLASS_LOW);
+ 				if (bfregn < 0) {
+ 					mlx5_ib_warn(dev, "bfreg allocation failed\n");
+ 					return bfregn;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  				}
  			}
  		}
  	}
  
++<<<<<<< HEAD
 +	uar_index = uuarn_to_uar_index(&context->uuari, uuarn);
 +	mlx5_ib_dbg(dev, "uuarn 0x%x, uar_index 0x%x\n", uuarn, uar_index);
++=======
+ 	uar_index = bfregn_to_uar_index(dev, &context->bfregi, bfregn);
+ 	mlx5_ib_dbg(dev, "bfregn 0x%x, uar_index 0x%x\n", bfregn, uar_index);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  
  	qp->rq.offset = 0;
  	qp->sq.wqe_shift = ilog2(MLX5_SEND_WQE_BB);
@@@ -854,8 -839,8 +974,13 @@@
  	MLX5_SET(qpc, qpc, page_offset, offset);
  
  	MLX5_SET(qpc, qpc, uar_page, uar_index);
++<<<<<<< HEAD
 +	resp->uuar_index = uuarn;
 +	qp->uuarn = uuarn;
++=======
+ 	resp->bfreg_index = adjust_bfregn(dev, &context->bfregi, bfregn);
+ 	qp->bfregn = bfregn;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  
  	err = mlx5_ib_db_map_user(context, ucmd.db_addr, &qp->db);
  	if (err) {
@@@ -882,8 -867,8 +1007,13 @@@ err_umem
  	if (ubuffer->umem)
  		ib_umem_release(ubuffer->umem);
  
++<<<<<<< HEAD
 +err_uuar:
 +	free_uuar(&context->uuari, uuarn);
++=======
+ err_bfreg:
+ 	free_bfreg(dev, &context->bfregi, bfregn);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  	return err;
  }
  
@@@ -896,7 -881,7 +1026,11 @@@ static void destroy_qp_user(struct mlx5
  	mlx5_ib_db_unmap_user(context, &qp->db);
  	if (base->ubuffer.umem)
  		ib_umem_release(base->ubuffer.umem);
++<<<<<<< HEAD
 +	free_uuar(&context->uuari, qp->uuarn);
++=======
+ 	free_bfreg(dev, &context->bfregi, qp->bfregn);
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  }
  
  static int create_kernel_qp(struct mlx5_ib_dev *dev,
diff --cc include/linux/mlx5/device.h
index 258ff2c8f518,dd345e8cf6f0..000000000000
--- a/include/linux/mlx5/device.h
+++ b/include/linux/mlx5/device.h
@@@ -212,10 -212,20 +212,27 @@@ enum 
  };
  
  enum {
++<<<<<<< HEAD
 +	MLX5_BF_REGS_PER_PAGE		= 4,
 +	MLX5_MAX_UAR_PAGES		= 1 << 8,
 +	MLX5_NON_FP_BF_REGS_PER_PAGE	= 2,
 +	MLX5_MAX_UUARS	= MLX5_MAX_UAR_PAGES * MLX5_NON_FP_BF_REGS_PER_PAGE,
++=======
+ 	MLX5_ADAPTER_PAGE_SHIFT		= 12,
+ 	MLX5_ADAPTER_PAGE_SIZE		= 1 << MLX5_ADAPTER_PAGE_SHIFT,
+ };
+ 
+ enum {
+ 	MLX5_BFREGS_PER_UAR		= 4,
+ 	MLX5_MAX_UARS			= 1 << 8,
+ 	MLX5_NON_FP_BFREGS_PER_UAR	= 2,
+ 	MLX5_FP_BFREGS_PER_UAR		= MLX5_BFREGS_PER_UAR -
+ 					  MLX5_NON_FP_BFREGS_PER_UAR,
+ 	MLX5_MAX_BFREGS			= MLX5_MAX_UARS *
+ 					  MLX5_NON_FP_BFREGS_PER_UAR,
+ 	MLX5_UARS_IN_PAGE		= PAGE_SIZE / MLX5_ADAPTER_PAGE_SIZE,
+ 	MLX5_NON_FP_BFREGS_IN_PAGE	= MLX5_NON_FP_BFREGS_PER_UAR * MLX5_UARS_IN_PAGE,
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  };
  
  enum {
diff --cc include/linux/mlx5/driver.h
index 1fa4d48e24be,7e7394fef835..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -183,38 -180,28 +183,53 @@@ enum mlx5_port_status 
  	MLX5_PORT_DOWN      = 2,
  };
  
++<<<<<<< HEAD
 +struct mlx5_uuar_info {
 +	struct mlx5_uar	       *uars;
 +	int			num_uars;
 +	int			num_low_latency_uuars;
 +	unsigned long	       *bitmap;
++=======
+ enum mlx5_eq_type {
+ 	MLX5_EQ_TYPE_COMP,
+ 	MLX5_EQ_TYPE_ASYNC,
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	MLX5_EQ_TYPE_PF,
+ #endif
+ };
+ 
+ struct mlx5_bfreg_info {
+ 	u32		       *sys_pages;
+ 	int			num_low_latency_bfregs;
++>>>>>>> b037c29a8056 (IB/mlx5: Allow future extension of libmlx5 input data)
  	unsigned int	       *count;
- 	struct mlx5_bf	       *bfs;
  
  	/*
 -	 * protect bfreg allocation data structs
 +	 * protect uuar allocation data structs
  	 */
  	struct mutex		lock;
  	u32			ver;
+ 	bool			lib_uar_4k;
+ 	u32			num_sys_pages;
  };
  
 +struct mlx5_bf {
 +	void __iomem	       *reg;
 +	void __iomem	       *regreg;
 +	int			buf_size;
 +	struct mlx5_uar	       *uar;
 +	unsigned long		offset;
 +	int			need_lock;
 +	/* protect blue flame buffer selection when needed
 +	 */
 +	spinlock_t		lock;
 +
 +	/* serialize 64 bit writes when done as two 32 bit accesses
 +	 */
 +	spinlock_t		lock32;
 +	int			uuarn;
 +};
 +
  struct mlx5_cmd_first {
  	__be32		data[4];
  };
@@@ -428,15 -434,45 +443,12 @@@ struct mlx5_eq_table 
  	spinlock_t		lock;
  };
  
 -struct mlx5_uars_page {
 -	void __iomem	       *map;
 -	bool			wc;
 -	u32			index;
 -	struct list_head	list;
 -	unsigned int		bfregs;
 -	unsigned long	       *reg_bitmap; /* for non fast path bf regs */
 -	unsigned long	       *fp_bitmap;
 -	unsigned int		reg_avail;
 -	unsigned int		fp_avail;
 -	struct kref		ref_count;
 -	struct mlx5_core_dev   *mdev;
 -};
 -
 -struct mlx5_bfreg_head {
 -	/* protect blue flame registers allocations */
 -	struct mutex		lock;
 -	struct list_head	list;
 -};
 -
 -struct mlx5_bfreg_data {
 -	struct mlx5_bfreg_head	reg_head;
 -	struct mlx5_bfreg_head	wc_head;
 -};
 -
 -struct mlx5_sq_bfreg {
 -	void __iomem	       *map;
 -	struct mlx5_uars_page  *up;
 -	bool			wc;
 -	u32			index;
 -	unsigned int		offset;
 -};
 -
  struct mlx5_uar {
  	u32			index;
- 	struct list_head	bf_list;
- 	unsigned		free_bf_bmap;
- 	void __iomem	       *bf_map;
  	void __iomem	       *map;
+ 	void __iomem	       *bf_map;
  };
  
- 
  struct mlx5_core_health {
  	struct health_buffer __iomem   *health;
  	__be32 __iomem		       *health_counter;
* Unmerged path drivers/infiniband/hw/mlx5/cq.c
* Unmerged path drivers/infiniband/hw/mlx5/main.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/qp.c
* Unmerged path include/linux/mlx5/device.h
* Unmerged path include/linux/mlx5/driver.h
