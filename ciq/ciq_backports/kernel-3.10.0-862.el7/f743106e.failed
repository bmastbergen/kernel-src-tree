ibmvnic: fix dma_mapping_error call

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Desnes Augusto Nunes do Rosario <desnesn@linux.vnet.ibm.com>
commit f743106ec140589b45ecc6ff92bacf48a0e26b05
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f743106e.failed

This patch fixes the dma_mapping_error call to use the correct dma_addr
which is inside the ibmvnic_vpd struct. Moreover, it fixes an uninitialized
warning regarding a local dma_addr variable which is not used anymore.

Fixes: 4e6759be28e4 ("ibmvnic: Feature implementation of VPD for the ibmvnic driver")
	Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
	Signed-off-by: Desnes A. Nunes do Rosario <desnesn@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit f743106ec140589b45ecc6ff92bacf48a0e26b05)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/ibm/ibmvnic.c
diff --cc drivers/net/ethernet/ibm/ibmvnic.c
index 8c661442348b,1dc4aef37d3a..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.c
+++ b/drivers/net/ethernet/ibm/ibmvnic.c
@@@ -375,20 -690,300 +375,226 @@@ static void free_rx_pool(struct ibmvnic
  {
  	int i;
  
 -	if (adapter->napi_enabled)
 -		return;
 +	kfree(pool->free_map);
 +	pool->free_map = NULL;
  
 -	for (i = 0; i < adapter->req_rx_queues; i++)
 -		napi_enable(&adapter->napi[i]);
 -
 -	adapter->napi_enabled = true;
 -}
 -
 -static void ibmvnic_napi_disable(struct ibmvnic_adapter *adapter)
 -{
 -	int i;
 -
 -	if (!adapter->napi_enabled)
 +	if (!pool->rx_buff)
  		return;
  
 -	for (i = 0; i < adapter->req_rx_queues; i++) {
 -		netdev_dbg(adapter->netdev, "Disabling napi[%d]\n", i);
 -		napi_disable(&adapter->napi[i]);
 -	}
 -
 -	adapter->napi_enabled = false;
 -}
 -
 -static int ibmvnic_login(struct net_device *netdev)
 -{
 -	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 -	unsigned long timeout = msecs_to_jiffies(30000);
 -	struct device *dev = &adapter->vdev->dev;
 -	int rc;
 -
 -	do {
 -		if (adapter->renegotiate) {
 -			adapter->renegotiate = false;
 -			release_sub_crqs(adapter);
 -
 -			reinit_completion(&adapter->init_done);
 -			send_cap_queries(adapter);
 -			if (!wait_for_completion_timeout(&adapter->init_done,
 -							 timeout)) {
 -				dev_err(dev, "Capabilities query timeout\n");
 -				return -1;
 -			}
 -			rc = init_sub_crqs(adapter);
 -			if (rc) {
 -				dev_err(dev,
 -					"Initialization of SCRQ's failed\n");
 -				return -1;
 -			}
 -			rc = init_sub_crq_irqs(adapter);
 -			if (rc) {
 -				dev_err(dev,
 -					"Initialization of SCRQ's irqs failed\n");
 -				return -1;
 -			}
 -		}
 -
 -		reinit_completion(&adapter->init_done);
 -		send_login(adapter);
 -		if (!wait_for_completion_timeout(&adapter->init_done,
 -						 timeout)) {
 -			dev_err(dev, "Login timeout\n");
 -			return -1;
 -		}
 -	} while (adapter->renegotiate);
 -
 -	return 0;
 -}
 -
 -static void release_resources(struct ibmvnic_adapter *adapter)
 -{
 -	int i;
 -
 -	release_vpd_data(adapter);
 -
 -	release_tx_pools(adapter);
 -	release_rx_pools(adapter);
 -
 -	release_stats_token(adapter);
 -	release_stats_buffers(adapter);
 -	release_error_buffers(adapter);
 -
 -	if (adapter->napi) {
 -		for (i = 0; i < adapter->req_rx_queues; i++) {
 -			if (&adapter->napi[i]) {
 -				netdev_dbg(adapter->netdev,
 -					   "Releasing napi[%d]\n", i);
 -				netif_napi_del(&adapter->napi[i]);
 -			}
 +	for (i = 0; i < pool->size; i++) {
 +		if (pool->rx_buff[i].skb) {
 +			dev_kfree_skb_any(pool->rx_buff[i].skb);
 +			pool->rx_buff[i].skb = NULL;
  		}
  	}
++<<<<<<< HEAD
 +	kfree(pool->rx_buff);
 +	pool->rx_buff = NULL;
++=======
+ }
+ 
+ static int set_link_state(struct ibmvnic_adapter *adapter, u8 link_state)
+ {
+ 	struct net_device *netdev = adapter->netdev;
+ 	unsigned long timeout = msecs_to_jiffies(30000);
+ 	union ibmvnic_crq crq;
+ 	bool resend;
+ 	int rc;
+ 
+ 	netdev_dbg(netdev, "setting link state %d\n", link_state);
+ 
+ 	memset(&crq, 0, sizeof(crq));
+ 	crq.logical_link_state.first = IBMVNIC_CRQ_CMD;
+ 	crq.logical_link_state.cmd = LOGICAL_LINK_STATE;
+ 	crq.logical_link_state.link_state = link_state;
+ 
+ 	do {
+ 		resend = false;
+ 
+ 		reinit_completion(&adapter->init_done);
+ 		rc = ibmvnic_send_crq(adapter, &crq);
+ 		if (rc) {
+ 			netdev_err(netdev, "Failed to set link state\n");
+ 			return rc;
+ 		}
+ 
+ 		if (!wait_for_completion_timeout(&adapter->init_done,
+ 						 timeout)) {
+ 			netdev_err(netdev, "timeout setting link state\n");
+ 			return -1;
+ 		}
+ 
+ 		if (adapter->init_done_rc == 1) {
+ 			/* Partuial success, delay and re-send */
+ 			mdelay(1000);
+ 			resend = true;
+ 		}
+ 	} while (resend);
+ 
+ 	return 0;
+ }
+ 
+ static int set_real_num_queues(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	int rc;
+ 
+ 	netdev_dbg(netdev, "Setting real tx/rx queues (%llx/%llx)\n",
+ 		   adapter->req_tx_queues, adapter->req_rx_queues);
+ 
+ 	rc = netif_set_real_num_tx_queues(netdev, adapter->req_tx_queues);
+ 	if (rc) {
+ 		netdev_err(netdev, "failed to set the number of tx queues\n");
+ 		return rc;
+ 	}
+ 
+ 	rc = netif_set_real_num_rx_queues(netdev, adapter->req_rx_queues);
+ 	if (rc)
+ 		netdev_err(netdev, "failed to set the number of rx queues\n");
+ 
+ 	return rc;
+ }
+ 
+ static int ibmvnic_get_vpd(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 	union ibmvnic_crq crq;
+ 	int len = 0;
+ 
+ 	if (adapter->vpd->buff)
+ 		len = adapter->vpd->len;
+ 
+ 	reinit_completion(&adapter->fw_done);
+ 	crq.get_vpd_size.first = IBMVNIC_CRQ_CMD;
+ 	crq.get_vpd_size.cmd = GET_VPD_SIZE;
+ 	ibmvnic_send_crq(adapter, &crq);
+ 	wait_for_completion(&adapter->fw_done);
+ 
+ 	if (!adapter->vpd->len)
+ 		return -ENODATA;
+ 
+ 	if (!adapter->vpd->buff)
+ 		adapter->vpd->buff = kzalloc(adapter->vpd->len, GFP_KERNEL);
+ 	else if (adapter->vpd->len != len)
+ 		adapter->vpd->buff =
+ 			krealloc(adapter->vpd->buff,
+ 				 adapter->vpd->len, GFP_KERNEL);
+ 
+ 	if (!adapter->vpd->buff) {
+ 		dev_err(dev, "Could allocate VPD buffer\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	adapter->vpd->dma_addr =
+ 		dma_map_single(dev, adapter->vpd->buff, adapter->vpd->len,
+ 			       DMA_FROM_DEVICE);
+ 	if (dma_mapping_error(dev, adapter->vpd->dma_addr)) {
+ 		dev_err(dev, "Could not map VPD buffer\n");
+ 		kfree(adapter->vpd->buff);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	reinit_completion(&adapter->fw_done);
+ 	crq.get_vpd.first = IBMVNIC_CRQ_CMD;
+ 	crq.get_vpd.cmd = GET_VPD;
+ 	crq.get_vpd.ioba = cpu_to_be32(adapter->vpd->dma_addr);
+ 	crq.get_vpd.len = cpu_to_be32((u32)adapter->vpd->len);
+ 	ibmvnic_send_crq(adapter, &crq);
+ 	wait_for_completion(&adapter->fw_done);
+ 
+ 	return 0;
+ }
+ 
+ static int init_resources(struct ibmvnic_adapter *adapter)
+ {
+ 	struct net_device *netdev = adapter->netdev;
+ 	int i, rc;
+ 
+ 	rc = set_real_num_queues(netdev);
+ 	if (rc)
+ 		return rc;
+ 
+ 	rc = init_stats_buffers(adapter);
+ 	if (rc)
+ 		return rc;
+ 
+ 	rc = init_stats_token(adapter);
+ 	if (rc)
+ 		return rc;
+ 
+ 	adapter->vpd = kzalloc(sizeof(*adapter->vpd), GFP_KERNEL);
+ 	if (!adapter->vpd)
+ 		return -ENOMEM;
+ 
+ 	adapter->map_id = 1;
+ 	adapter->napi = kcalloc(adapter->req_rx_queues,
+ 				sizeof(struct napi_struct), GFP_KERNEL);
+ 	if (!adapter->napi)
+ 		return -ENOMEM;
+ 
+ 	for (i = 0; i < adapter->req_rx_queues; i++) {
+ 		netdev_dbg(netdev, "Adding napi[%d]\n", i);
+ 		netif_napi_add(netdev, &adapter->napi[i], ibmvnic_poll,
+ 			       NAPI_POLL_WEIGHT);
+ 	}
+ 
+ 	send_map_query(adapter);
+ 
+ 	rc = init_rx_pools(netdev);
+ 	if (rc)
+ 		return rc;
+ 
+ 	rc = init_tx_pools(netdev);
+ 	return rc;
+ }
+ 
+ static int __ibmvnic_open(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	enum vnic_state prev_state = adapter->state;
+ 	int i, rc;
+ 
+ 	adapter->state = VNIC_OPENING;
+ 	replenish_pools(adapter);
+ 	ibmvnic_napi_enable(adapter);
+ 
+ 	/* We're ready to receive frames, enable the sub-crq interrupts and
+ 	 * set the logical link state to up
+ 	 */
+ 	for (i = 0; i < adapter->req_rx_queues; i++) {
+ 		netdev_dbg(netdev, "Enabling rx_scrq[%d] irq\n", i);
+ 		if (prev_state == VNIC_CLOSED)
+ 			enable_irq(adapter->rx_scrq[i]->irq);
+ 		else
+ 			enable_scrq_irq(adapter, adapter->rx_scrq[i]);
+ 	}
+ 
+ 	for (i = 0; i < adapter->req_tx_queues; i++) {
+ 		netdev_dbg(netdev, "Enabling tx_scrq[%d] irq\n", i);
+ 		if (prev_state == VNIC_CLOSED)
+ 			enable_irq(adapter->tx_scrq[i]->irq);
+ 		else
+ 			enable_scrq_irq(adapter, adapter->tx_scrq[i]);
+ 	}
+ 
+ 	rc = set_link_state(adapter, IBMVNIC_LOGICAL_LNK_UP);
+ 	if (rc) {
+ 		for (i = 0; i < adapter->req_rx_queues; i++)
+ 			napi_disable(&adapter->napi[i]);
+ 		release_resources(adapter);
+ 		return rc;
+ 	}
+ 
+ 	netif_tx_start_all_queues(netdev);
+ 
+ 	if (prev_state == VNIC_CLOSED) {
+ 		for (i = 0; i < adapter->req_rx_queues; i++)
+ 			napi_schedule(&adapter->napi[i]);
+ 	}
+ 
+ 	adapter->state = VNIC_OPEN;
+ 	return rc;
++>>>>>>> f743106ec140 (ibmvnic: fix dma_mapping_error call)
  }
  
  static int ibmvnic_open(struct net_device *netdev)
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.c
