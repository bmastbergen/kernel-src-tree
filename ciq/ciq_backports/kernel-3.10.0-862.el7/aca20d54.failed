x86/mm: Add support to make use of Secure Memory Encryption

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] mm: Add support to make use of Secure Memory Encryption (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 96.49%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit aca20d5462149333ba8b24a4a352be5b7a00dfd2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/aca20d54.failed

Add support to check if SME has been enabled and if memory encryption
should be activated (checking of command line option based on the
configuration of the default state).  If memory encryption is to be
activated, then the encryption mask is set and the kernel is encrypted
"in place."

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brijesh Singh <brijesh.singh@amd.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Larry Woodman <lwoodman@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Toshimitsu Kani <toshi.kani@hpe.com>
	Cc: kasan-dev@googlegroups.com
	Cc: kvm@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
	Cc: linux-doc@vger.kernel.org
	Cc: linux-efi@vger.kernel.org
	Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/5f0da2fd4cce63f556117549e2c89c170072209f.1500319216.git.thomas.lendacky@amd.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit aca20d5462149333ba8b24a4a352be5b7a00dfd2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mem_encrypt.h
#	arch/x86/kernel/head64.c
#	arch/x86/mm/mem_encrypt.c
diff --cc arch/x86/kernel/head64.c
index 39ad3cdc4c78,925b2928f377..000000000000
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@@ -31,11 -34,144 +31,148 @@@
  /*
   * Manage page tables very early on.
   */
 +extern pgd_t early_level4_pgt[PTRS_PER_PGD];
  extern pmd_t early_dynamic_pgts[EARLY_DYNAMIC_PAGE_TABLES][PTRS_PER_PMD];
 -static unsigned int __initdata next_early_pgt;
 +static unsigned int __initdata next_early_pgt = 2;
  pmdval_t early_pmd_flags = __PAGE_KERNEL_LARGE & ~(_PAGE_GLOBAL | _PAGE_NX);
  
++<<<<<<< HEAD
++=======
+ #define __head	__section(.head.text)
+ 
+ static void __head *fixup_pointer(void *ptr, unsigned long physaddr)
+ {
+ 	return ptr - (void *)_text + (void *)physaddr;
+ }
+ 
+ unsigned long __head __startup_64(unsigned long physaddr,
+ 				  struct boot_params *bp)
+ {
+ 	unsigned long load_delta, *p;
+ 	unsigned long pgtable_flags;
+ 	pgdval_t *pgd;
+ 	p4dval_t *p4d;
+ 	pudval_t *pud;
+ 	pmdval_t *pmd, pmd_entry;
+ 	int i;
+ 
+ 	/* Is the address too large? */
+ 	if (physaddr >> MAX_PHYSMEM_BITS)
+ 		for (;;);
+ 
+ 	/*
+ 	 * Compute the delta between the address I am compiled to run at
+ 	 * and the address I am actually running at.
+ 	 */
+ 	load_delta = physaddr - (unsigned long)(_text - __START_KERNEL_map);
+ 
+ 	/* Is the address not 2M aligned? */
+ 	if (load_delta & ~PMD_PAGE_MASK)
+ 		for (;;);
+ 
+ 	/* Activate Secure Memory Encryption (SME) if supported and enabled */
+ 	sme_enable(bp);
+ 
+ 	/* Include the SME encryption mask in the fixup value */
+ 	load_delta += sme_get_me_mask();
+ 
+ 	/* Fixup the physical addresses in the page table */
+ 
+ 	pgd = fixup_pointer(&early_top_pgt, physaddr);
+ 	pgd[pgd_index(__START_KERNEL_map)] += load_delta;
+ 
+ 	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
+ 		p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
+ 		p4d[511] += load_delta;
+ 	}
+ 
+ 	pud = fixup_pointer(&level3_kernel_pgt, physaddr);
+ 	pud[510] += load_delta;
+ 	pud[511] += load_delta;
+ 
+ 	pmd = fixup_pointer(level2_fixmap_pgt, physaddr);
+ 	pmd[506] += load_delta;
+ 
+ 	/*
+ 	 * Set up the identity mapping for the switchover.  These
+ 	 * entries should *NOT* have the global bit set!  This also
+ 	 * creates a bunch of nonsense entries but that is fine --
+ 	 * it avoids problems around wraparound.
+ 	 */
+ 
+ 	pud = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
+ 	pmd = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
+ 	pgtable_flags = _KERNPG_TABLE_NOENC + sme_get_me_mask();
+ 
+ 	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
+ 		p4d = fixup_pointer(early_dynamic_pgts[next_early_pgt++], physaddr);
+ 
+ 		i = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;
+ 		pgd[i + 0] = (pgdval_t)p4d + pgtable_flags;
+ 		pgd[i + 1] = (pgdval_t)p4d + pgtable_flags;
+ 
+ 		i = (physaddr >> P4D_SHIFT) % PTRS_PER_P4D;
+ 		p4d[i + 0] = (pgdval_t)pud + pgtable_flags;
+ 		p4d[i + 1] = (pgdval_t)pud + pgtable_flags;
+ 	} else {
+ 		i = (physaddr >> PGDIR_SHIFT) % PTRS_PER_PGD;
+ 		pgd[i + 0] = (pgdval_t)pud + pgtable_flags;
+ 		pgd[i + 1] = (pgdval_t)pud + pgtable_flags;
+ 	}
+ 
+ 	i = (physaddr >> PUD_SHIFT) % PTRS_PER_PUD;
+ 	pud[i + 0] = (pudval_t)pmd + pgtable_flags;
+ 	pud[i + 1] = (pudval_t)pmd + pgtable_flags;
+ 
+ 	pmd_entry = __PAGE_KERNEL_LARGE_EXEC & ~_PAGE_GLOBAL;
+ 	pmd_entry += sme_get_me_mask();
+ 	pmd_entry +=  physaddr;
+ 
+ 	for (i = 0; i < DIV_ROUND_UP(_end - _text, PMD_SIZE); i++) {
+ 		int idx = i + (physaddr >> PMD_SHIFT) % PTRS_PER_PMD;
+ 		pmd[idx] = pmd_entry + i * PMD_SIZE;
+ 	}
+ 
+ 	/*
+ 	 * Fixup the kernel text+data virtual addresses. Note that
+ 	 * we might write invalid pmds, when the kernel is relocated
+ 	 * cleanup_highmap() fixes this up along with the mappings
+ 	 * beyond _end.
+ 	 */
+ 
+ 	pmd = fixup_pointer(level2_kernel_pgt, physaddr);
+ 	for (i = 0; i < PTRS_PER_PMD; i++) {
+ 		if (pmd[i] & _PAGE_PRESENT)
+ 			pmd[i] += load_delta;
+ 	}
+ 
+ 	/*
+ 	 * Fixup phys_base - remove the memory encryption mask to obtain
+ 	 * the true physical address.
+ 	 */
+ 	p = fixup_pointer(&phys_base, physaddr);
+ 	*p += load_delta - sme_get_me_mask();
+ 
+ 	/* Encrypt the kernel (if SME is active) */
+ 	sme_encrypt_kernel();
+ 
+ 	/*
+ 	 * Return the SME encryption mask (if SME is active) to be used as a
+ 	 * modifier for the initial pgdir entry programmed into CR3.
+ 	 */
+ 	return sme_get_me_mask();
+ }
+ 
+ unsigned long __startup_secondary_64(void)
+ {
+ 	/*
+ 	 * Return the SME encryption mask (if SME is active) to be used as a
+ 	 * modifier for the initial pgdir entry programmed into CR3.
+ 	 */
+ 	return sme_get_me_mask();
+ }
+ 
++>>>>>>> aca20d546214 (x86/mm: Add support to make use of Secure Memory Encryption)
  /* Wipe all early page tables except for the kernel symbol map */
  static void __init reset_early_page_tables(void)
  {
* Unmerged path arch/x86/include/asm/mem_encrypt.h
* Unmerged path arch/x86/mm/mem_encrypt.c
* Unmerged path arch/x86/include/asm/mem_encrypt.h
* Unmerged path arch/x86/kernel/head64.c
* Unmerged path arch/x86/mm/mem_encrypt.c
