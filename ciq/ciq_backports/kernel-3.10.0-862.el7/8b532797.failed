netvsc: allow controlling send/recv buffer size

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author stephen hemminger <stephen@networkplumber.org>
commit 8b5327975ae171ca54dfd93e6c042d1292945867
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/8b532797.failed

Control the size of the buffer areas via ethtool ring settings.
They aren't really traditional hardware rings, but host API breaks
receive and send buffer into chunks. The final size of the chunks are
controlled by the host.

The default value of send and receive buffer area for host DMA
is much larger than it needs to be. Experimentation shows that
4M receive and 1M send is sufficient.

	Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 8b5327975ae171ca54dfd93e6c042d1292945867)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/hyperv/hyperv_net.h
#	drivers/net/hyperv/netvsc.c
#	drivers/net/hyperv/netvsc_drv.c
diff --cc drivers/net/hyperv/hyperv_net.h
index a32d7f1b2505,30326373e46f..000000000000
--- a/drivers/net/hyperv/hyperv_net.h
+++ b/drivers/net/hyperv/hyperv_net.h
@@@ -146,10 -146,10 +146,12 @@@ struct hv_netvsc_packet 
  
  struct netvsc_device_info {
  	unsigned char mac_adr[ETH_ALEN];
 +	bool link_state;	/* 0 - link up, 1 - link down */
  	int  ring_size;
 +	u32  max_num_vrss_chns;
  	u32  num_chn;
+ 	u32  send_sections;
+ 	u32  recv_sections;
  };
  
  enum rndis_device_state {
@@@ -628,10 -642,9 +632,14 @@@ struct nvsp_message 
  #define NETVSC_RECEIVE_BUFFER_ID		0xcafe
  #define NETVSC_SEND_BUFFER_ID			0
  
++<<<<<<< HEAD
 +#define NETVSC_PACKET_SIZE                      4096
 +
 +#define VRSS_SEND_TAB_SIZE 16
++=======
+ #define VRSS_SEND_TAB_SIZE 16  /* must be power of 2 */
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  #define VRSS_CHANNEL_MAX 64
 -#define VRSS_CHANNEL_DEFAULT 8
  
  #define RNDIS_MAX_PKT_DEFAULT 8
  #define RNDIS_PKT_ALIGN_DEFAULT 8
@@@ -727,10 -756,10 +735,14 @@@ struct netvsc_device 
  
  	/* Receive buffer allocated by us but manages by NetVSP */
  	void *recv_buf;
- 	u32 recv_buf_size;
  	u32 recv_buf_gpadl_handle;
  	u32 recv_section_cnt;
++<<<<<<< HEAD
 +	struct nvsp_1_receive_buffer_section *recv_section;
++=======
+ 	u32 recv_section_size;
+ 	u32 recv_completion_cnt;
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  
  	/* Send buffer allocated by us */
  	void *send_buf;
diff --cc drivers/net/hyperv/netvsc.c
index 52667f48ba1c,d9d7555148eb..000000000000
--- a/drivers/net/hyperv/netvsc.c
+++ b/drivers/net/hyperv/netvsc.c
@@@ -83,7 -75,12 +83,11 @@@ static struct netvsc_device *alloc_net_
  	atomic_set(&net_device->open_cnt, 0);
  	net_device->max_pkt = RNDIS_MAX_PKT_DEFAULT;
  	net_device->pkt_align = RNDIS_PKT_ALIGN_DEFAULT;
+ 
+ 	net_device->recv_section_size = NETVSC_RECV_SECTION_SIZE;
+ 	net_device->send_section_size = NETVSC_SEND_SECTION_SIZE;
+ 
  	init_completion(&net_device->channel_init_wait);
 -	init_waitqueue_head(&net_device->subchan_open);
  
  	return net_device;
  }
@@@ -261,25 -235,40 +267,43 @@@ static void netvsc_destroy_buf(struct h
  	kfree(net_device->send_section_map);
  }
  
 -int netvsc_alloc_recv_comp_ring(struct netvsc_device *net_device, u32 q_idx)
 -{
 -	struct netvsc_channel *nvchan = &net_device->chan_table[q_idx];
 -	int node = cpu_to_node(nvchan->channel->target_cpu);
 -	size_t size;
 -
 -	size = net_device->recv_completion_cnt * sizeof(struct recv_comp_data);
 -	nvchan->mrc.slots = vzalloc_node(size, node);
 -	if (!nvchan->mrc.slots)
 -		nvchan->mrc.slots = vzalloc(size);
 -
 -	return nvchan->mrc.slots ? 0 : -ENOMEM;
 -}
 -
  static int netvsc_init_buf(struct hv_device *device,
- 			   struct netvsc_device *net_device)
+ 			   struct netvsc_device *net_device,
+ 			   const struct netvsc_device_info *device_info)
  {
++<<<<<<< HEAD
++=======
+ 	struct nvsp_1_message_send_receive_buffer_complete *resp;
+ 	struct net_device *ndev = hv_get_drvdata(device);
+ 	struct nvsp_message *init_packet;
+ 	unsigned int buf_size;
+ 	size_t map_words;
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  	int ret = 0;
 +	struct nvsp_message *init_packet;
 +	struct net_device *ndev;
 +	size_t map_words;
 +	int node;
 +
 +	ndev = hv_get_drvdata(device);
 +
 +	node = cpu_to_node(device->channel->target_cpu);
 +	net_device->recv_buf = vzalloc_node(net_device->recv_buf_size, node);
 +	if (!net_device->recv_buf)
 +		net_device->recv_buf = vzalloc(net_device->recv_buf_size);
  
++<<<<<<< HEAD
++=======
+ 	/* Get receive buffer area. */
+ 	buf_size = device_info->recv_sections * net_device->recv_section_size;
+ 	buf_size = roundup(buf_size, PAGE_SIZE);
+ 
+ 	net_device->recv_buf = vzalloc(buf_size);
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  	if (!net_device->recv_buf) {
- 		netdev_err(ndev, "unable to allocate receive "
- 			"buffer of size %d\n", net_device->recv_buf_size);
+ 		netdev_err(ndev,
+ 			   "unable to allocate receive buffer of size %u\n",
+ 			   buf_size);
  		ret = -ENOMEM;
  		goto cleanup;
  	}
@@@ -333,38 -321,35 +357,62 @@@
  	}
  
  	/* Parse the response */
 -	netdev_dbg(ndev, "Receive sections: %u sub_allocs: size %u count: %u\n",
 -		   resp->num_sections, resp->sections[0].sub_alloc_size,
 -		   resp->sections[0].num_sub_allocs);
  
++<<<<<<< HEAD
 +	net_device->recv_section_cnt = init_packet->msg.
 +		v1_msg.send_recv_buf_complete.num_sections;
 +
 +	net_device->recv_section = kmemdup(
 +		init_packet->msg.v1_msg.send_recv_buf_complete.sections,
 +		net_device->recv_section_cnt *
 +		sizeof(struct nvsp_1_receive_buffer_section),
 +		GFP_KERNEL);
 +	if (net_device->recv_section == NULL) {
 +		ret = -EINVAL;
 +		goto cleanup;
 +	}
 +
 +	/*
 +	 * For 1st release, there should only be 1 section that represents the
 +	 * entire receive buffer
 +	 */
 +	if (net_device->recv_section_cnt != 1 ||
 +	    net_device->recv_section->offset != 0) {
++=======
+ 	/* There should only be one section for the entire receive buffer */
+ 	if (resp->num_sections != 1 || resp->sections[0].offset != 0) {
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  		ret = -EINVAL;
  		goto cleanup;
  	}
  
++<<<<<<< HEAD
 +	/* Now setup the send buffer.
 +	 */
 +	net_device->send_buf = vzalloc_node(net_device->send_buf_size, node);
 +	if (!net_device->send_buf)
 +		net_device->send_buf = vzalloc(net_device->send_buf_size);
++=======
+ 	net_device->recv_section_size = resp->sections[0].sub_alloc_size;
+ 	net_device->recv_section_cnt = resp->sections[0].num_sub_allocs;
+ 
+ 	/* Setup receive completion ring */
+ 	net_device->recv_completion_cnt
+ 		= round_up(net_device->recv_section_cnt + 1,
+ 			   PAGE_SIZE / sizeof(u64));
+ 	ret = netvsc_alloc_recv_comp_ring(net_device, 0);
+ 	if (ret)
+ 		goto cleanup;
+ 
+ 	/* Now setup the send buffer. */
+ 	buf_size = device_info->send_sections * net_device->send_section_size;
+ 	buf_size = round_up(buf_size, PAGE_SIZE);
+ 
+ 	net_device->send_buf = vzalloc(buf_size);
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  	if (!net_device->send_buf) {
- 		netdev_err(ndev, "unable to allocate send "
- 			   "buffer of size %d\n", net_device->send_buf_size);
+ 		netdev_err(ndev, "unable to allocate send buffer of size %u\n",
+ 			   buf_size);
  		ret = -ENOMEM;
  		goto cleanup;
  	}
@@@ -1381,12 -1290,10 +1422,12 @@@ int netvsc_device_add(struct hv_device 
  	/* Writing nvdev pointer unlocks netvsc_send(), make sure chn_table is
  	 * populated.
  	 */
 -	rcu_assign_pointer(net_device_ctx->nvdev, net_device);
 +	wmb();
 +
 +	net_device_ctx->nvdev = net_device;
  
  	/* Connect with the NetVsp */
- 	ret = netvsc_connect_vsp(device, net_device);
+ 	ret = netvsc_connect_vsp(device, net_device, device_info);
  	if (ret != 0) {
  		netdev_err(ndev,
  			"unable to connect to NetVSP - %d\n", ret);
diff --cc drivers/net/hyperv/netvsc_drv.c
index b16b610847ea,873c83a66cc2..000000000000
--- a/drivers/net/hyperv/netvsc_drv.c
+++ b/drivers/net/hyperv/netvsc_drv.c
@@@ -43,16 -45,14 +43,21 @@@
  
  #include "hyperv_net.h"
  
- #define RING_SIZE_MIN 64
+ #define RING_SIZE_MIN		64
+ #define NETVSC_MIN_TX_SECTIONS	10
+ #define NETVSC_DEFAULT_TX	192	/* ~1M */
+ #define NETVSC_MIN_RX_SECTIONS	10	/* ~64K */
+ #define NETVSC_DEFAULT_RX	2048	/* ~4M */
+ 
  #define LINKCHANGE_INT (2 * HZ)
 -#define VF_TAKEOVER_INT (HZ / 10)
 +#define NETVSC_HW_FEATURES	(NETIF_F_RXCSUM | \
 +				 NETIF_F_SG | \
 +				 NETIF_F_TSO | \
 +				 NETIF_F_TSO6 | \
 +				 NETIF_F_HW_CSUM)
 +
 +/* Restrict GSO size to account for NVGRE */
 +#define NETVSC_GSO_MAX_SIZE	62768
  
  static int ring_size = 128;
  module_param(ring_size, int, S_IRUGO);
@@@ -739,84 -808,61 +744,118 @@@ static int netvsc_set_channels(struct n
  {
  	struct net_device_context *net_device_ctx = netdev_priv(net);
  	struct hv_device *dev = net_device_ctx->device_ctx;
 -	struct netvsc_device *nvdev = rtnl_dereference(net_device_ctx->nvdev);
 -	unsigned int orig, count = channels->combined_count;
 +	struct netvsc_device *nvdev = net_device_ctx->nvdev;
  	struct netvsc_device_info device_info;
 -	bool was_opened;
 +	u32 num_chn;
 +	u32 max_chn;
  	int ret = 0;
 +	bool recovering = false;
  
 -	/* We do not support separate count for rx, tx, or other */
 -	if (count == 0 ||
 -	    channels->rx_count || channels->tx_count || channels->other_count)
 -		return -EINVAL;
 -
 -	if (count > net->num_tx_queues || count > VRSS_CHANNEL_MAX)
 -		return -EINVAL;
 -
 -	if (!nvdev || nvdev->destroy)
 +	if (net_device_ctx->start_remove || !nvdev || nvdev->destroy)
  		return -ENODEV;
  
 -	if (nvdev->nvsp_version < NVSP_PROTOCOL_VERSION_5)
 +	num_chn = nvdev->num_chn;
 +	max_chn = min_t(u32, nvdev->max_chn, num_online_cpus());
 +
 +	if (nvdev->nvsp_version < NVSP_PROTOCOL_VERSION_5) {
 +		pr_info("vRSS unsupported before NVSP Version 5\n");
  		return -EINVAL;
++<<<<<<< HEAD
++=======
+ 
+ 	if (count > nvdev->max_chn)
+ 		return -EINVAL;
+ 
+ 	orig = nvdev->num_chn;
+ 	was_opened = rndis_filter_opened(nvdev);
+ 	if (was_opened)
+ 		rndis_filter_close(nvdev);
+ 
+ 	memset(&device_info, 0, sizeof(device_info));
+ 	device_info.num_chn = count;
+ 	device_info.ring_size = ring_size;
+ 	device_info.send_sections = nvdev->send_section_cnt;
+ 	device_info.recv_sections = nvdev->recv_section_cnt;
+ 
+ 	rndis_filter_device_remove(dev, nvdev);
+ 
+ 	nvdev = rndis_filter_device_add(dev, &device_info);
+ 	if (!IS_ERR(nvdev)) {
+ 		netif_set_real_num_tx_queues(net, nvdev->num_chn);
+ 		netif_set_real_num_rx_queues(net, nvdev->num_chn);
+ 	} else {
+ 		ret = PTR_ERR(nvdev);
+ 		device_info.num_chn = orig;
+ 		nvdev = rndis_filter_device_add(dev, &device_info);
+ 
+ 		if (IS_ERR(nvdev)) {
+ 			netdev_err(net, "restoring channel setting failed: %ld\n",
+ 				   PTR_ERR(nvdev));
+ 			return ret;
+ 		}
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  	}
  
 -	if (was_opened)
 -		rndis_filter_open(nvdev);
 +	/* We do not support rx, tx, or other */
 +	if (!channels ||
 +	    channels->rx_count ||
 +	    channels->tx_count ||
 +	    channels->other_count ||
 +	    (channels->combined_count < 1))
 +		return -EINVAL;
 +
 +	if (channels->combined_count > max_chn) {
 +		pr_info("combined channels too high, using %d\n", max_chn);
 +		channels->combined_count = max_chn;
 +	}
 +
 +	ret = netvsc_close(net);
 +	if (ret)
 +		goto out;
 +
 + do_set:
 +	net_device_ctx->start_remove = true;
 +	rndis_filter_device_remove(dev);
 +
 +	nvdev->num_chn = channels->combined_count;
  
 +	memset(&device_info, 0, sizeof(device_info));
 +	device_info.num_chn = nvdev->num_chn; /* passed to RNDIS */
 +	device_info.ring_size = ring_size;
 +	device_info.max_num_vrss_chns = max_num_vrss_chns;
 +
 +	ret = rndis_filter_device_add(dev, &device_info);
 +	if (ret) {
 +		if (recovering) {
 +			netdev_err(net, "unable to add netvsc device (ret %d)\n", ret);
 +			return ret;
 +		}
 +		goto recover;
 +	}
 +
 +	nvdev = net_device_ctx->nvdev;
 +
 +	ret = netif_set_real_num_tx_queues(net, nvdev->num_chn);
 +	if (ret) {
 +		if (recovering) {
 +			netdev_err(net, "could not set tx queue count (ret %d)\n", ret);
 +			return ret;
 +		}
 +		goto recover;
 +	}
 +
 +	ret = netif_set_real_num_rx_queues(net, nvdev->num_chn);
 +	if (ret) {
 +		if (recovering) {
 +			netdev_err(net, "could not set rx queue count (ret %d)\n", ret);
 +			return ret;
 +		}
 +		goto recover;
 +	}
 +
 + out:
 +	netvsc_open(net);
 +	net_device_ctx->start_remove = false;
  	/* We may have missed link change notifications */
  	net_device_ctx->last_reconfig = 0;
  	schedule_delayed_work(&net_device_ctx->dwork, 0);
@@@ -891,42 -928,61 +930,50 @@@ static int netvsc_set_link_ksettings(st
  static int netvsc_change_mtu(struct net_device *ndev, int mtu)
  {
  	struct net_device_context *ndevctx = netdev_priv(ndev);
 -	struct net_device *vf_netdev = rtnl_dereference(ndevctx->vf_netdev);
 -	struct netvsc_device *nvdev = rtnl_dereference(ndevctx->nvdev);
 +	struct netvsc_device *nvdev = ndevctx->nvdev;
  	struct hv_device *hdev = ndevctx->device_ctx;
 -	int orig_mtu = ndev->mtu;
  	struct netvsc_device_info device_info;
 -	bool was_opened;
 +	int limit = ETH_DATA_LEN;
 +	u32 num_chn;
  	int ret = 0;
  
 -	if (!nvdev || nvdev->destroy)
 +	if (ndevctx->start_remove || !nvdev || nvdev->destroy)
  		return -ENODEV;
  
 -	/* Change MTU of underlying VF netdev first. */
 -	if (vf_netdev) {
 -		ret = dev_set_mtu(vf_netdev, mtu);
 -		if (ret)
 -			return ret;
 -	}
 +	if (nvdev->nvsp_version >= NVSP_PROTOCOL_VERSION_2)
 +		limit = NETVSC_MTU - ETH_HLEN;
  
 -	netif_device_detach(ndev);
 -	was_opened = rndis_filter_opened(nvdev);
 -	if (was_opened)
 -		rndis_filter_close(nvdev);
 +	if (mtu < NETVSC_MTU_MIN || mtu > limit)
 +		return -EINVAL;
  
++<<<<<<< HEAD
 +	ret = netvsc_close(ndev);
 +	if (ret)
 +		goto out;
++=======
+ 	memset(&device_info, 0, sizeof(device_info));
+ 	device_info.ring_size = ring_size;
+ 	device_info.num_chn = nvdev->num_chn;
+ 	device_info.send_sections = nvdev->send_section_cnt;
+ 	device_info.recv_sections = nvdev->recv_section_cnt;
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  
 -	rndis_filter_device_remove(hdev, nvdev);
 +	num_chn = nvdev->num_chn;
  
 -	ndev->mtu = mtu;
 -
 -	nvdev = rndis_filter_device_add(hdev, &device_info);
 -	if (IS_ERR(nvdev)) {
 -		ret = PTR_ERR(nvdev);
 +	ndevctx->start_remove = true;
 +	rndis_filter_device_remove(hdev);
  
 -		/* Attempt rollback to original MTU */
 -		ndev->mtu = orig_mtu;
 -		nvdev = rndis_filter_device_add(hdev, &device_info);
 -
 -		if (vf_netdev)
 -			dev_set_mtu(vf_netdev, orig_mtu);
 -
 -		if (IS_ERR(nvdev)) {
 -			netdev_err(ndev, "restoring mtu failed: %ld\n",
 -				   PTR_ERR(nvdev));
 -			return ret;
 -		}
 -	}
 +	ndev->mtu = mtu;
  
 -	if (was_opened)
 -		rndis_filter_open(nvdev);
 +	memset(&device_info, 0, sizeof(device_info));
 +	device_info.ring_size = ring_size;
 +	device_info.num_chn = num_chn;
 +	device_info.max_num_vrss_chns = max_num_vrss_chns;
 +	rndis_filter_device_add(hdev, &device_info);
  
 -	netif_device_attach(ndev);
 +out:
 +	netvsc_open(ndev);
 +	ndevctx->start_remove = false;
  
  	/* We may have missed link change notifications */
  	schedule_delayed_work(&ndevctx->dwork, 0);
@@@ -1529,20 -1890,36 +1676,30 @@@ static int netvsc_probe(struct hv_devic
  	/* Notify the netvsc driver of the new device */
  	memset(&device_info, 0, sizeof(device_info));
  	device_info.ring_size = ring_size;
++<<<<<<< HEAD
 +	device_info.max_num_vrss_chns = max_num_vrss_chns;
 +	ret = rndis_filter_device_add(dev, &device_info);
 +	if (ret != 0) {
++=======
+ 	device_info.num_chn = VRSS_CHANNEL_DEFAULT;
+ 	device_info.send_sections = NETVSC_DEFAULT_TX;
+ 	device_info.recv_sections = NETVSC_DEFAULT_RX;
+ 
+ 	nvdev = rndis_filter_device_add(dev, &device_info);
+ 	if (IS_ERR(nvdev)) {
+ 		ret = PTR_ERR(nvdev);
++>>>>>>> 8b5327975ae1 (netvsc: allow controlling send/recv buffer size)
  		netdev_err(net, "unable to add netvsc device (ret %d)\n", ret);
 -		goto rndis_failed;
 +		netvsc_free_netdev(net);
 +		hv_set_drvdata(dev, NULL);
 +		return ret;
  	}
 -
  	memcpy(net->dev_addr, device_info.mac_adr, ETH_ALEN);
  
 -	/* hw_features computed in rndis_filter_device_add */
 -	net->features = net->hw_features |
 -		NETIF_F_HIGHDMA | NETIF_F_SG |
 -		NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;
 -	net->vlan_features = net->features;
 -
 +	nvdev = net_device_ctx->nvdev;
  	netif_set_real_num_tx_queues(net, nvdev->num_chn);
  	netif_set_real_num_rx_queues(net, nvdev->num_chn);
 -
 -	netdev_lockdep_set_classes(net);
 -
 -	/* MTU range: 68 - 1500 or 65521 */
 -	net->min_mtu = NETVSC_MTU_MIN;
 -	if (nvdev->nvsp_version >= NVSP_PROTOCOL_VERSION_2)
 -		net->max_mtu = NETVSC_MTU - ETH_HLEN;
 -	else
 -		net->max_mtu = ETH_DATA_LEN;
 +	netif_set_gso_max_size(net, NETVSC_GSO_MAX_SIZE);
  
  	ret = register_netdev(net);
  	if (ret != 0) {
* Unmerged path drivers/net/hyperv/hyperv_net.h
* Unmerged path drivers/net/hyperv/netvsc.c
* Unmerged path drivers/net/hyperv/netvsc_drv.c
