udp: under rx pressure, try to condense skbs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Eric Dumazet <edumazet@google.com>
commit c8c8b127091b758f5768f906bcdeeb88bc9951ca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c8c8b127.failed

Under UDP flood, many softirq producers try to add packets to
UDP receive queue, and one user thread is burning one cpu trying
to dequeue packets as fast as possible.

Two parts of the per packet cost are :
- copying payload from kernel space to user space,
- freeing memory pieces associated with skb.

If socket is under pressure, softirq handler(s) can try to pull in
skb->head the payload of the packet if it fits.

Meaning the softirq handler(s) can free/reuse the page fragment
immediately, instead of letting udp_recvmsg() do this hundreds of usec
later, possibly from another node.

Additional gains :
- We reduce skb->truesize and thus can store more packets per SO_RCVBUF
- We avoid cache line misses at copyout() time and consume_skb() time,
and avoid one put_page() with potential alien freeing on NUMA hosts.

This comes at the cost of a copy, bounded to available tail room, which
is usually small. (We might have to fix GRO_MAX_HEAD which looks bigger
than necessary)

This patch gave me about 5 % increase in throughput in my tests.

skb_condense() helper could probably used in other contexts.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Cc: Paolo Abeni <pabeni@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c8c8b127091b758f5768f906bcdeeb88bc9951ca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skbuff.c
diff --cc net/core/skbuff.c
index 1810381d0deb,84151cf40aeb..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -4399,3 -4695,267 +4399,270 @@@ failure
  	return NULL;
  }
  EXPORT_SYMBOL(alloc_skb_with_frags);
++<<<<<<< HEAD
++=======
+ 
+ /* carve out the first off bytes from skb when off < headlen */
+ static int pskb_carve_inside_header(struct sk_buff *skb, const u32 off,
+ 				    const int headlen, gfp_t gfp_mask)
+ {
+ 	int i;
+ 	int size = skb_end_offset(skb);
+ 	int new_hlen = headlen - off;
+ 	u8 *data;
+ 
+ 	size = SKB_DATA_ALIGN(size);
+ 
+ 	if (skb_pfmemalloc(skb))
+ 		gfp_mask |= __GFP_MEMALLOC;
+ 	data = kmalloc_reserve(size +
+ 			       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),
+ 			       gfp_mask, NUMA_NO_NODE, NULL);
+ 	if (!data)
+ 		return -ENOMEM;
+ 
+ 	size = SKB_WITH_OVERHEAD(ksize(data));
+ 
+ 	/* Copy real data, and all frags */
+ 	skb_copy_from_linear_data_offset(skb, off, data, new_hlen);
+ 	skb->len -= off;
+ 
+ 	memcpy((struct skb_shared_info *)(data + size),
+ 	       skb_shinfo(skb),
+ 	       offsetof(struct skb_shared_info,
+ 			frags[skb_shinfo(skb)->nr_frags]));
+ 	if (skb_cloned(skb)) {
+ 		/* drop the old head gracefully */
+ 		if (skb_orphan_frags(skb, gfp_mask)) {
+ 			kfree(data);
+ 			return -ENOMEM;
+ 		}
+ 		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+ 			skb_frag_ref(skb, i);
+ 		if (skb_has_frag_list(skb))
+ 			skb_clone_fraglist(skb);
+ 		skb_release_data(skb);
+ 	} else {
+ 		/* we can reuse existing recount- all we did was
+ 		 * relocate values
+ 		 */
+ 		skb_free_head(skb);
+ 	}
+ 
+ 	skb->head = data;
+ 	skb->data = data;
+ 	skb->head_frag = 0;
+ #ifdef NET_SKBUFF_DATA_USES_OFFSET
+ 	skb->end = size;
+ #else
+ 	skb->end = skb->head + size;
+ #endif
+ 	skb_set_tail_pointer(skb, skb_headlen(skb));
+ 	skb_headers_offset_update(skb, 0);
+ 	skb->cloned = 0;
+ 	skb->hdr_len = 0;
+ 	skb->nohdr = 0;
+ 	atomic_set(&skb_shinfo(skb)->dataref, 1);
+ 
+ 	return 0;
+ }
+ 
+ static int pskb_carve(struct sk_buff *skb, const u32 off, gfp_t gfp);
+ 
+ /* carve out the first eat bytes from skb's frag_list. May recurse into
+  * pskb_carve()
+  */
+ static int pskb_carve_frag_list(struct sk_buff *skb,
+ 				struct skb_shared_info *shinfo, int eat,
+ 				gfp_t gfp_mask)
+ {
+ 	struct sk_buff *list = shinfo->frag_list;
+ 	struct sk_buff *clone = NULL;
+ 	struct sk_buff *insp = NULL;
+ 
+ 	do {
+ 		if (!list) {
+ 			pr_err("Not enough bytes to eat. Want %d\n", eat);
+ 			return -EFAULT;
+ 		}
+ 		if (list->len <= eat) {
+ 			/* Eaten as whole. */
+ 			eat -= list->len;
+ 			list = list->next;
+ 			insp = list;
+ 		} else {
+ 			/* Eaten partially. */
+ 			if (skb_shared(list)) {
+ 				clone = skb_clone(list, gfp_mask);
+ 				if (!clone)
+ 					return -ENOMEM;
+ 				insp = list->next;
+ 				list = clone;
+ 			} else {
+ 				/* This may be pulled without problems. */
+ 				insp = list;
+ 			}
+ 			if (pskb_carve(list, eat, gfp_mask) < 0) {
+ 				kfree_skb(clone);
+ 				return -ENOMEM;
+ 			}
+ 			break;
+ 		}
+ 	} while (eat);
+ 
+ 	/* Free pulled out fragments. */
+ 	while ((list = shinfo->frag_list) != insp) {
+ 		shinfo->frag_list = list->next;
+ 		kfree_skb(list);
+ 	}
+ 	/* And insert new clone at head. */
+ 	if (clone) {
+ 		clone->next = list;
+ 		shinfo->frag_list = clone;
+ 	}
+ 	return 0;
+ }
+ 
+ /* carve off first len bytes from skb. Split line (off) is in the
+  * non-linear part of skb
+  */
+ static int pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,
+ 				       int pos, gfp_t gfp_mask)
+ {
+ 	int i, k = 0;
+ 	int size = skb_end_offset(skb);
+ 	u8 *data;
+ 	const int nfrags = skb_shinfo(skb)->nr_frags;
+ 	struct skb_shared_info *shinfo;
+ 
+ 	size = SKB_DATA_ALIGN(size);
+ 
+ 	if (skb_pfmemalloc(skb))
+ 		gfp_mask |= __GFP_MEMALLOC;
+ 	data = kmalloc_reserve(size +
+ 			       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),
+ 			       gfp_mask, NUMA_NO_NODE, NULL);
+ 	if (!data)
+ 		return -ENOMEM;
+ 
+ 	size = SKB_WITH_OVERHEAD(ksize(data));
+ 
+ 	memcpy((struct skb_shared_info *)(data + size),
+ 	       skb_shinfo(skb), offsetof(struct skb_shared_info,
+ 					 frags[skb_shinfo(skb)->nr_frags]));
+ 	if (skb_orphan_frags(skb, gfp_mask)) {
+ 		kfree(data);
+ 		return -ENOMEM;
+ 	}
+ 	shinfo = (struct skb_shared_info *)(data + size);
+ 	for (i = 0; i < nfrags; i++) {
+ 		int fsize = skb_frag_size(&skb_shinfo(skb)->frags[i]);
+ 
+ 		if (pos + fsize > off) {
+ 			shinfo->frags[k] = skb_shinfo(skb)->frags[i];
+ 
+ 			if (pos < off) {
+ 				/* Split frag.
+ 				 * We have two variants in this case:
+ 				 * 1. Move all the frag to the second
+ 				 *    part, if it is possible. F.e.
+ 				 *    this approach is mandatory for TUX,
+ 				 *    where splitting is expensive.
+ 				 * 2. Split is accurately. We make this.
+ 				 */
+ 				shinfo->frags[0].page_offset += off - pos;
+ 				skb_frag_size_sub(&shinfo->frags[0], off - pos);
+ 			}
+ 			skb_frag_ref(skb, i);
+ 			k++;
+ 		}
+ 		pos += fsize;
+ 	}
+ 	shinfo->nr_frags = k;
+ 	if (skb_has_frag_list(skb))
+ 		skb_clone_fraglist(skb);
+ 
+ 	if (k == 0) {
+ 		/* split line is in frag list */
+ 		pskb_carve_frag_list(skb, shinfo, off - pos, gfp_mask);
+ 	}
+ 	skb_release_data(skb);
+ 
+ 	skb->head = data;
+ 	skb->head_frag = 0;
+ 	skb->data = data;
+ #ifdef NET_SKBUFF_DATA_USES_OFFSET
+ 	skb->end = size;
+ #else
+ 	skb->end = skb->head + size;
+ #endif
+ 	skb_reset_tail_pointer(skb);
+ 	skb_headers_offset_update(skb, 0);
+ 	skb->cloned   = 0;
+ 	skb->hdr_len  = 0;
+ 	skb->nohdr    = 0;
+ 	skb->len -= off;
+ 	skb->data_len = skb->len;
+ 	atomic_set(&skb_shinfo(skb)->dataref, 1);
+ 	return 0;
+ }
+ 
+ /* remove len bytes from the beginning of the skb */
+ static int pskb_carve(struct sk_buff *skb, const u32 len, gfp_t gfp)
+ {
+ 	int headlen = skb_headlen(skb);
+ 
+ 	if (len < headlen)
+ 		return pskb_carve_inside_header(skb, len, headlen, gfp);
+ 	else
+ 		return pskb_carve_inside_nonlinear(skb, len, headlen, gfp);
+ }
+ 
+ /* Extract to_copy bytes starting at off from skb, and return this in
+  * a new skb
+  */
+ struct sk_buff *pskb_extract(struct sk_buff *skb, int off,
+ 			     int to_copy, gfp_t gfp)
+ {
+ 	struct sk_buff  *clone = skb_clone(skb, gfp);
+ 
+ 	if (!clone)
+ 		return NULL;
+ 
+ 	if (pskb_carve(clone, off, gfp) < 0 ||
+ 	    pskb_trim(clone, to_copy)) {
+ 		kfree_skb(clone);
+ 		return NULL;
+ 	}
+ 	return clone;
+ }
+ EXPORT_SYMBOL(pskb_extract);
+ 
+ /**
+  * skb_condense - try to get rid of fragments/frag_list if possible
+  * @skb: buffer
+  *
+  * Can be used to save memory before skb is added to a busy queue.
+  * If packet has bytes in frags and enough tail room in skb->head,
+  * pull all of them, so that we can free the frags right now and adjust
+  * truesize.
+  * Notes:
+  *	We do not reallocate skb->head thus can not fail.
+  *	Caller must re-evaluate skb->truesize if needed.
+  */
+ void skb_condense(struct sk_buff *skb)
+ {
+ 	if (!skb->data_len ||
+ 	    skb->data_len > skb->end - skb->tail ||
+ 	    skb_cloned(skb))
+ 		return;
+ 
+ 	/* Nice, we can free page frag(s) right now */
+ 	__pskb_pull_tail(skb, skb->data_len);
+ 
+ 	/* Now adjust skb->truesize, since __pskb_pull_tail() does
+ 	 * not do this.
+ 	 */
+ 	skb->truesize = SKB_TRUESIZE(skb_end_offset(skb));
+ }
++>>>>>>> c8c8b127091b (udp: under rx pressure, try to condense skbs)
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 28aefdf34562..f6b17727f61f 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -1974,6 +1974,8 @@ static inline int pskb_may_pull(struct sk_buff *skb, unsigned int len)
 	return __pskb_pull_tail(skb, len - skb_headlen(skb)) != NULL;
 }
 
+void skb_condense(struct sk_buff *skb);
+
 /**
  *	skb_headroom - bytes at buffer head
  *	@skb: buffer to check
* Unmerged path net/core/skbuff.c
diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index c6e2b198be79..7570be819834 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -1199,7 +1199,7 @@ int __udp_enqueue_schedule_skb(struct sock *sk, struct sk_buff *skb)
 {
 	struct sk_buff_head *list = &sk->sk_receive_queue;
 	int rmem, delta, amt, err = -ENOMEM;
-	int size = skb->truesize;
+	int size;
 
 	/* try to avoid the costly atomic add/sub pair when the receive
 	 * queue is full; always allow at least a packet
@@ -1208,6 +1208,16 @@ int __udp_enqueue_schedule_skb(struct sock *sk, struct sk_buff *skb)
 	if (rmem > sk->sk_rcvbuf)
 		goto drop;
 
+	/* Under mem pressure, it might be helpful to help udp_recvmsg()
+	 * having linear skbs :
+	 * - Reduce memory overhead and thus increase receive queue capacity
+	 * - Less cache line misses at copyout() time
+	 * - Less work at consume_skb() (less alien page frag freeing)
+	 */
+	if (rmem > (sk->sk_rcvbuf >> 1))
+		skb_condense(skb);
+	size = skb->truesize;
+
 	/* we drop only if the receive buf is full and the receive
 	 * queue contains some other skb
 	 */
