target: Drop duplicate + unused se_dev_check_wce

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [target] Drop duplicate + unused se_dev_check_wce (Maurizio Lombardi) [1366062]
Rebuild_FUZZ: 90.91%
commit-author Nicholas Bellinger <nab@linux-iscsi.org>
commit e34d366273ce17bba902975f7c1e64a24a399024
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e34d3662.failed

	Reported-by: kbuild test robot <fengguang.wu@intel.com>
	Reported-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>
(cherry picked from commit e34d366273ce17bba902975f7c1e64a24a399024)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/target/target_core_device.c
diff --cc drivers/target/target_core_device.c
index 53dcefb982bc,db531fb5e787..000000000000
--- a/drivers/target/target_core_device.c
+++ b/drivers/target/target_core_device.c
@@@ -651,360 -538,10 +651,365 @@@ static u32 se_dev_align_max_sectors(u3
  	return aligned_max_sectors;
  }
  
++<<<<<<< HEAD
 +bool se_dev_check_wce(struct se_device *dev)
 +{
 +	bool wce = false;
 +
 +	if (dev->transport->get_write_cache)
 +		wce = dev->transport->get_write_cache(dev);
 +	else if (dev->dev_attrib.emulate_write_cache > 0)
 +		wce = true;
 +
 +	return wce;
 +}
 +
 +int se_dev_set_max_unmap_lba_count(
++=======
+ int core_dev_add_lun(
+ 	struct se_portal_group *tpg,
++>>>>>>> e34d366273ce (target: Drop duplicate + unused se_dev_check_wce)
  	struct se_device *dev,
 -	struct se_lun *lun)
 +	u32 max_unmap_lba_count)
 +{
 +	dev->dev_attrib.max_unmap_lba_count = max_unmap_lba_count;
 +	pr_debug("dev[%p]: Set max_unmap_lba_count: %u\n",
 +			dev, dev->dev_attrib.max_unmap_lba_count);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_max_unmap_lba_count);
 +
 +int se_dev_set_max_unmap_block_desc_count(
 +	struct se_device *dev,
 +	u32 max_unmap_block_desc_count)
 +{
 +	dev->dev_attrib.max_unmap_block_desc_count =
 +		max_unmap_block_desc_count;
 +	pr_debug("dev[%p]: Set max_unmap_block_desc_count: %u\n",
 +			dev, dev->dev_attrib.max_unmap_block_desc_count);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_max_unmap_block_desc_count);
 +
 +int se_dev_set_unmap_granularity(
 +	struct se_device *dev,
 +	u32 unmap_granularity)
 +{
 +	dev->dev_attrib.unmap_granularity = unmap_granularity;
 +	pr_debug("dev[%p]: Set unmap_granularity: %u\n",
 +			dev, dev->dev_attrib.unmap_granularity);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_unmap_granularity);
 +
 +int se_dev_set_unmap_granularity_alignment(
 +	struct se_device *dev,
 +	u32 unmap_granularity_alignment)
 +{
 +	dev->dev_attrib.unmap_granularity_alignment = unmap_granularity_alignment;
 +	pr_debug("dev[%p]: Set unmap_granularity_alignment: %u\n",
 +			dev, dev->dev_attrib.unmap_granularity_alignment);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_unmap_granularity_alignment);
 +
 +int se_dev_set_max_write_same_len(
 +	struct se_device *dev,
 +	u32 max_write_same_len)
 +{
 +	dev->dev_attrib.max_write_same_len = max_write_same_len;
 +	pr_debug("dev[%p]: Set max_write_same_len: %u\n",
 +			dev, dev->dev_attrib.max_write_same_len);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_max_write_same_len);
 +
 +static void dev_set_t10_wwn_model_alias(struct se_device *dev)
 +{
 +	const char *configname;
 +
 +	configname = config_item_name(&dev->dev_group.cg_item);
 +	if (strlen(configname) >= 16) {
 +		pr_warn("dev[%p]: Backstore name '%s' is too long for "
 +			"INQUIRY_MODEL, truncating to 16 bytes\n", dev,
 +			configname);
 +	}
 +	snprintf(&dev->t10_wwn.model[0], 16, "%s", configname);
 +}
 +
 +int se_dev_set_emulate_model_alias(struct se_device *dev, int flag)
 +{
 +	if (dev->export_count) {
 +		pr_err("dev[%p]: Unable to change model alias"
 +			" while export_count is %d\n",
 +			dev, dev->export_count);
 +			return -EINVAL;
 +	}
 +
 +	if (flag != 0 && flag != 1) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +
 +	if (flag) {
 +		dev_set_t10_wwn_model_alias(dev);
 +	} else {
 +		strncpy(&dev->t10_wwn.model[0],
 +			dev->transport->inquiry_prod, 16);
 +	}
 +	dev->dev_attrib.emulate_model_alias = flag;
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_model_alias);
 +
 +int se_dev_set_emulate_dpo(struct se_device *dev, int flag)
 +{
 +	if (flag != 0 && flag != 1) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +
 +	if (flag) {
 +		pr_err("dpo_emulated not supported\n");
 +		return -EINVAL;
 +	}
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_dpo);
 +
 +int se_dev_set_emulate_fua_write(struct se_device *dev, int flag)
 +{
 +	if (flag != 0 && flag != 1) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +	if (flag &&
 +	    dev->transport->get_write_cache) {
 +		pr_warn("emulate_fua_write not supported for this device, ignoring\n");
 +		return 0;
 +	}
 +	if (dev->export_count) {
 +		pr_err("emulate_fua_write cannot be changed with active"
 +		       " exports: %d\n", dev->export_count);
 +		return -EINVAL;
 +	}
 +	dev->dev_attrib.emulate_fua_write = flag;
 +	pr_debug("dev[%p]: SE Device Forced Unit Access WRITEs: %d\n",
 +			dev, dev->dev_attrib.emulate_fua_write);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_fua_write);
 +
 +int se_dev_set_emulate_fua_read(struct se_device *dev, int flag)
 +{
 +	if (flag != 0 && flag != 1) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +
 +	if (flag) {
 +		pr_err("ua read emulated not supported\n");
 +		return -EINVAL;
 +	}
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_fua_read);
 +
 +int se_dev_set_emulate_write_cache(struct se_device *dev, int flag)
 +{
 +	if (flag != 0 && flag != 1) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +	if (flag &&
 +	    dev->transport->get_write_cache) {
 +		pr_err("emulate_write_cache not supported for this device\n");
 +		return -EINVAL;
 +	}
 +	if (dev->export_count) {
 +		pr_err("emulate_write_cache cannot be changed with active"
 +		       " exports: %d\n", dev->export_count);
 +		return -EINVAL;
 +	}
 +	dev->dev_attrib.emulate_write_cache = flag;
 +	pr_debug("dev[%p]: SE Device WRITE_CACHE_EMULATION flag: %d\n",
 +			dev, dev->dev_attrib.emulate_write_cache);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_write_cache);
 +
 +int se_dev_set_emulate_ua_intlck_ctrl(struct se_device *dev, int flag)
 +{
 +	if ((flag != 0) && (flag != 1) && (flag != 2)) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +
 +	if (dev->export_count) {
 +		pr_err("dev[%p]: Unable to change SE Device"
 +			" UA_INTRLCK_CTRL while export_count is %d\n",
 +			dev, dev->export_count);
 +		return -EINVAL;
 +	}
 +	dev->dev_attrib.emulate_ua_intlck_ctrl = flag;
 +	pr_debug("dev[%p]: SE Device UA_INTRLCK_CTRL flag: %d\n",
 +		dev, dev->dev_attrib.emulate_ua_intlck_ctrl);
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_ua_intlck_ctrl);
 +
 +int se_dev_set_emulate_tas(struct se_device *dev, int flag)
 +{
 +	if ((flag != 0) && (flag != 1)) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +
 +	if (dev->export_count) {
 +		pr_err("dev[%p]: Unable to change SE Device TAS while"
 +			" export_count is %d\n",
 +			dev, dev->export_count);
 +		return -EINVAL;
 +	}
 +	dev->dev_attrib.emulate_tas = flag;
 +	pr_debug("dev[%p]: SE Device TASK_ABORTED status bit: %s\n",
 +		dev, (dev->dev_attrib.emulate_tas) ? "Enabled" : "Disabled");
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_tas);
 +
 +int se_dev_set_emulate_tpu(struct se_device *dev, int flag)
 +{
 +	if ((flag != 0) && (flag != 1)) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +	/*
 +	 * We expect this value to be non-zero when generic Block Layer
 +	 * Discard supported is detected iblock_create_virtdevice().
 +	 */
 +	if (flag && !dev->dev_attrib.max_unmap_block_desc_count) {
 +		pr_err("Generic Block Discard not supported\n");
 +		return -ENOSYS;
 +	}
 +
 +	dev->dev_attrib.emulate_tpu = flag;
 +	pr_debug("dev[%p]: SE Device Thin Provisioning UNMAP bit: %d\n",
 +				dev, flag);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_tpu);
 +
 +int se_dev_set_emulate_tpws(struct se_device *dev, int flag)
 +{
 +	if ((flag != 0) && (flag != 1)) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +	/*
 +	 * We expect this value to be non-zero when generic Block Layer
 +	 * Discard supported is detected iblock_create_virtdevice().
 +	 */
 +	if (flag && !dev->dev_attrib.max_unmap_block_desc_count) {
 +		pr_err("Generic Block Discard not supported\n");
 +		return -ENOSYS;
 +	}
 +
 +	dev->dev_attrib.emulate_tpws = flag;
 +	pr_debug("dev[%p]: SE Device Thin Provisioning WRITE_SAME: %d\n",
 +				dev, flag);
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_tpws);
 +
 +int se_dev_set_emulate_caw(struct se_device *dev, int flag)
 +{
 +	if (flag != 0 && flag != 1) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +	dev->dev_attrib.emulate_caw = flag;
 +	pr_debug("dev[%p]: SE Device CompareAndWrite (AtomicTestandSet): %d\n",
 +		 dev, flag);
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_caw);
 +
 +int se_dev_set_emulate_3pc(struct se_device *dev, int flag)
 +{
 +	if (flag != 0 && flag != 1) {
 +		pr_err("Illegal value %d\n", flag);
 +		return -EINVAL;
 +	}
 +	dev->dev_attrib.emulate_3pc = flag;
 +	pr_debug("dev[%p]: SE Device 3rd Party Copy (EXTENDED_COPY): %d\n",
 +		dev, flag);
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_emulate_3pc);
 +
 +int se_dev_set_pi_prot_type(struct se_device *dev, int flag)
 +{
 +	int rc, old_prot = dev->dev_attrib.pi_prot_type;
 +
 +	if (flag != 0 && flag != 1 && flag != 2 && flag != 3) {
 +		pr_err("Illegal value %d for pi_prot_type\n", flag);
 +		return -EINVAL;
 +	}
 +	if (flag == 2) {
 +		pr_err("DIF TYPE2 protection currently not supported\n");
 +		return -ENOSYS;
 +	}
 +	if (dev->dev_attrib.hw_pi_prot_type) {
 +		pr_warn("DIF protection enabled on underlying hardware,"
 +			" ignoring\n");
 +		return 0;
 +	}
 +	if (!dev->transport->init_prot || !dev->transport->free_prot) {
 +		/* 0 is only allowed value for non-supporting backends */
 +		if (flag == 0)
 +			return 0;
 +
 +		pr_err("DIF protection not supported by backend: %s\n",
 +		       dev->transport->name);
 +		return -ENOSYS;
 +	}
 +	if (!(dev->dev_flags & DF_CONFIGURED)) {
 +		pr_err("DIF protection requires device to be configured\n");
 +		return -ENODEV;
 +	}
 +	if (dev->export_count) {
 +		pr_err("dev[%p]: Unable to change SE Device PROT type while"
 +		       " export_count is %d\n", dev, dev->export_count);
 +		return -EINVAL;
 +	}
 +
 +	dev->dev_attrib.pi_prot_type = flag;
 +
 +	if (flag && !old_prot) {
 +		rc = dev->transport->init_prot(dev);
 +		if (rc) {
 +			dev->dev_attrib.pi_prot_type = old_prot;
 +			return rc;
 +		}
 +
 +	} else if (!flag && old_prot) {
 +		dev->transport->free_prot(dev);
 +	}
 +	pr_debug("dev[%p]: SE Device Protection Type: %d\n", dev, flag);
 +
 +	return 0;
 +}
 +EXPORT_SYMBOL(se_dev_set_pi_prot_type);
 +
 +int se_dev_set_pi_prot_format(struct se_device *dev, int flag)
  {
  	int rc;
  
* Unmerged path drivers/target/target_core_device.c
