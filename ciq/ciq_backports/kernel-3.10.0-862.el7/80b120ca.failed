cpufreq: intel_pstate: Avoid transient updates of cpuinfo.max_freq

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Avoid transient updates of cpuinfo.max_freq (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 92.68%
commit-author Rafael J. Wysocki <rafael.j.wysocki@intel.com>
commit 80b120ca1a75c2df093d15936ab0591d90c99de9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/80b120ca.failed

Both intel_pstate_verify_policy() and intel_cpufreq_verify_policy()
set policy->cpuinfo.max_freq depending on the turbo status, but the
updates made by them are discarded by the core, because the policy
object passed to them by the core is temporary and cpuinfo.max_freq
from that object is not copied to the final policy object in
cpufreq_set_policy().

However, cpufreq_set_policy() passes the temporary policy object
to the ->setpolicy callback of the driver, so intel_pstate_set_policy()
actually sees the policy->cpuinfo.max_freq value updated by
intel_pstate_verify_policy() and not the final one.  It also
updates policy->max sometimes which basically has no effect after
it returns, because the core discards that update.

To avoid confusion, eliminate policy->cpuinfo.max_freq updates from
intel_pstate_verify_policy() and intel_cpufreq_verify_policy()
entirely and check the maximum frequency explicitly in
intel_pstate_update_perf_limits() instead of relying on the
transiently updated policy->cpuinfo.max_freq value.

Moreover, move the max->policy adjustment carried out in
intel_pstate_set_policy() to a separate function and call that
function from the ->verify driver callbacks to ensure that it will
actually be effective.

	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 80b120ca1a75c2df093d15936ab0591d90c99de9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index 9b85edc571c0,60544c210d75..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -1424,14 -1990,95 +1424,104 @@@ static int intel_pstate_init_cpu(unsign
  
  static unsigned int intel_pstate_get(unsigned int cpu_num)
  {
 -	struct cpudata *cpu = all_cpu_data[cpu_num];
 +	struct sample *sample;
 +	struct cpudata *cpu;
  
++<<<<<<< HEAD
 +	cpu = all_cpu_data[cpu_num];
 +	if (!cpu)
 +		return 0;
 +	sample = &cpu->sample;
 +	return sample->freq;
++=======
+ 	return cpu ? get_avg_frequency(cpu) : 0;
+ }
+ 
+ static void intel_pstate_set_update_util_hook(unsigned int cpu_num)
+ {
+ 	struct cpudata *cpu = all_cpu_data[cpu_num];
+ 
+ 	if (cpu->update_util_set)
+ 		return;
+ 
+ 	/* Prevent intel_pstate_update_util() from using stale data. */
+ 	cpu->sample.time = 0;
+ 	cpufreq_add_update_util_hook(cpu_num, &cpu->update_util,
+ 				     intel_pstate_update_util);
+ 	cpu->update_util_set = true;
+ }
+ 
+ static void intel_pstate_clear_update_util_hook(unsigned int cpu)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[cpu];
+ 
+ 	if (!cpu_data->update_util_set)
+ 		return;
+ 
+ 	cpufreq_remove_update_util_hook(cpu);
+ 	cpu_data->update_util_set = false;
+ 	synchronize_sched();
+ }
+ 
+ static int intel_pstate_get_max_freq(struct cpudata *cpu)
+ {
+ 	return global.turbo_disabled || global.no_turbo ?
+ 			cpu->pstate.max_freq : cpu->pstate.turbo_freq;
+ }
+ 
+ static void intel_pstate_update_perf_limits(struct cpufreq_policy *policy,
+ 					    struct cpudata *cpu)
+ {
+ 	struct perf_limits *limits = &cpu->perf_limits;
+ 	int max_freq = intel_pstate_get_max_freq(cpu);
+ 	int32_t max_policy_perf, min_policy_perf;
+ 
+ 	max_policy_perf = div_ext_fp(policy->max, max_freq);
+ 	max_policy_perf = clamp_t(int32_t, max_policy_perf, 0, int_ext_tofp(1));
+ 	if (policy->max == policy->min) {
+ 		min_policy_perf = max_policy_perf;
+ 	} else {
+ 		min_policy_perf = div_ext_fp(policy->min, max_freq);
+ 		min_policy_perf = clamp_t(int32_t, min_policy_perf,
+ 					  0, max_policy_perf);
+ 	}
+ 
+ 	/* Normalize user input to [min_perf, max_perf] */
+ 	if (per_cpu_limits) {
+ 		limits->min_perf = min_policy_perf;
+ 		limits->max_perf = max_policy_perf;
+ 	} else {
+ 		int32_t global_min, global_max;
+ 
+ 		/* Global limits are in percent of the maximum turbo P-state. */
+ 		global_max = percent_ext_fp(global.max_perf_pct);
+ 		global_min = percent_ext_fp(global.min_perf_pct);
+ 		if (max_freq != cpu->pstate.turbo_freq) {
+ 			int32_t turbo_factor;
+ 
+ 			turbo_factor = div_ext_fp(cpu->pstate.turbo_pstate,
+ 						  cpu->pstate.max_pstate);
+ 			global_min = mul_ext_fp(global_min, turbo_factor);
+ 			global_max = mul_ext_fp(global_max, turbo_factor);
+ 		}
+ 		global_min = clamp_t(int32_t, global_min, 0, global_max);
+ 
+ 		limits->min_perf = max(min_policy_perf, global_min);
+ 		limits->min_perf = min(limits->min_perf, max_policy_perf);
+ 		limits->max_perf = min(max_policy_perf, global_max);
+ 		limits->max_perf = max(min_policy_perf, limits->max_perf);
+ 
+ 		/* Make sure min_perf <= max_perf */
+ 		limits->min_perf = min(limits->min_perf, limits->max_perf);
+ 	}
+ 
+ 	limits->max_perf = round_up(limits->max_perf, EXT_FRAC_BITS);
+ 	limits->min_perf = round_up(limits->min_perf, EXT_FRAC_BITS);
+ 
+ 	pr_debug("cpu:%d max_perf_pct:%d min_perf_pct:%d\n", policy->cpu,
+ 		 fp_ext_toint(limits->max_perf * 100),
+ 		 fp_ext_toint(limits->min_perf * 100));
++>>>>>>> 80b120ca1a75 (cpufreq: intel_pstate: Avoid transient updates of cpuinfo.max_freq)
  }
  
  static int intel_pstate_set_policy(struct cpufreq_policy *policy)
@@@ -1442,44 -2091,28 +1532,51 @@@
  	pr_debug("set_policy cpuinfo.max %u policy->max %u\n",
  		 policy->cpuinfo.max_freq, policy->max);
  
++<<<<<<< HEAD
 +	if (policy->policy == CPUFREQ_POLICY_PERFORMANCE &&
 +	    policy->max >= policy->cpuinfo.max_freq) {
 +		pr_debug("intel_pstate: set performance\n");
 +		limits = &performance_limits;
 +		if (hwp_active)
 +			intel_pstate_hwp_set(policy->cpus);
 +		return 0;
 +	}
 +
 +	pr_debug("intel_pstate: set powersave\n");
 +	limits = &powersave_limits;
 +	limits->min_policy_pct = (policy->min * 100) / policy->cpuinfo.max_freq;
 +	limits->min_policy_pct = clamp_t(int, limits->min_policy_pct, 0 , 100);
 +	limits->max_policy_pct = DIV_ROUND_UP(policy->max * 100,
 +					      policy->cpuinfo.max_freq);
 +	limits->max_policy_pct = clamp_t(int, limits->max_policy_pct, 0 , 100);
++=======
+ 	cpu = all_cpu_data[policy->cpu];
+ 	cpu->policy = policy->policy;
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
 -
 -	intel_pstate_update_perf_limits(policy, cpu);
 -
 -	if (cpu->policy == CPUFREQ_POLICY_PERFORMANCE) {
 -		/*
 -		 * NOHZ_FULL CPUs need this as the governor callback may not
 -		 * be invoked on them.
 -		 */
 -		intel_pstate_clear_update_util_hook(policy->cpu);
 -		intel_pstate_max_within_limits(cpu);
 -	}
 -
 -	intel_pstate_set_update_util_hook(policy->cpu);
++>>>>>>> 80b120ca1a75 (cpufreq: intel_pstate: Avoid transient updates of cpuinfo.max_freq)
 +
 +	/* Normalize user input to [min_policy_pct, max_policy_pct] */
 +	limits->min_perf_pct = max(limits->min_policy_pct,
 +				   limits->min_sysfs_pct);
 +	limits->min_perf_pct = min(limits->max_policy_pct,
 +				   limits->min_perf_pct);
 +	limits->max_perf_pct = min(limits->max_policy_pct,
 +				   limits->max_sysfs_pct);
 +	limits->max_perf_pct = max(limits->min_policy_pct,
 +				   limits->max_perf_pct);
 +
 +	/* Make sure min_perf_pct <= max_perf_pct */
 +	limits->min_perf_pct = min(limits->max_perf_pct, limits->min_perf_pct);
 +
 +	limits->min_perf = div_fp(int_tofp(limits->min_perf_pct),
 +				  int_tofp(100));
 +	limits->max_perf = div_fp(int_tofp(limits->max_perf_pct),
 +				  int_tofp(100));
 +	limits->max_perf = round_up(limits->max_perf, FRAC_BITS);
  
  	if (hwp_active)
 -		intel_pstate_hwp_set(policy);
 -
 -	mutex_unlock(&intel_pstate_limits_lock);
 +		intel_pstate_hwp_set(policy->cpus);
  
  	return 0;
  }
@@@ -1487,20 -2131,10 +1595,25 @@@ static void intel_pstate_adjust_policy_
  static int intel_pstate_verify_policy(struct cpufreq_policy *policy)
  {
  	struct cpudata *cpu = all_cpu_data[policy->cpu];
 +	struct perf_limits *perf_limits;
 +
 +	if (policy->policy == CPUFREQ_POLICY_PERFORMANCE)
 +		perf_limits = &performance_limits;
 +	else
 +		perf_limits = &powersave_limits;
  
  	update_turbo_state();
++<<<<<<< HEAD
 +	policy->cpuinfo.max_freq = perf_limits->turbo_disabled ||
 +					perf_limits->no_turbo ?
 +					cpu->pstate.max_freq :
 +					cpu->pstate.turbo_freq;
 +
 +	cpufreq_verify_within_cpu_limits(policy);
++=======
+ 	cpufreq_verify_within_limits(policy, policy->cpuinfo.min_freq,
+ 				     intel_pstate_get_max_freq(cpu));
++>>>>>>> 80b120ca1a75 (cpufreq: intel_pstate: Avoid transient updates of cpuinfo.max_freq)
  
  	if (policy->policy != CPUFREQ_POLICY_POWERSAVE &&
  	    policy->policy != CPUFREQ_POLICY_PERFORMANCE)
@@@ -1574,6 -2231,209 +1689,212 @@@ static struct cpufreq_driver intel_psta
  	.name		= "intel_pstate",
  };
  
++<<<<<<< HEAD
++=======
+ static int intel_cpufreq_verify_policy(struct cpufreq_policy *policy)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 
+ 	update_turbo_state();
+ 	cpufreq_verify_within_limits(policy, policy->cpuinfo.min_freq,
+ 				     intel_pstate_get_max_freq(cpu));
+ 
+ 	intel_pstate_adjust_policy_max(policy, cpu);
+ 
+ 	intel_pstate_update_perf_limits(policy, cpu);
+ 
+ 	return 0;
+ }
+ 
+ static int intel_cpufreq_target(struct cpufreq_policy *policy,
+ 				unsigned int target_freq,
+ 				unsigned int relation)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 	struct cpufreq_freqs freqs;
+ 	int target_pstate;
+ 
+ 	update_turbo_state();
+ 
+ 	freqs.old = policy->cur;
+ 	freqs.new = target_freq;
+ 
+ 	cpufreq_freq_transition_begin(policy, &freqs);
+ 	switch (relation) {
+ 	case CPUFREQ_RELATION_L:
+ 		target_pstate = DIV_ROUND_UP(freqs.new, cpu->pstate.scaling);
+ 		break;
+ 	case CPUFREQ_RELATION_H:
+ 		target_pstate = freqs.new / cpu->pstate.scaling;
+ 		break;
+ 	default:
+ 		target_pstate = DIV_ROUND_CLOSEST(freqs.new, cpu->pstate.scaling);
+ 		break;
+ 	}
+ 	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
+ 	if (target_pstate != cpu->pstate.current_pstate) {
+ 		cpu->pstate.current_pstate = target_pstate;
+ 		wrmsrl_on_cpu(policy->cpu, MSR_IA32_PERF_CTL,
+ 			      pstate_funcs.get_val(cpu, target_pstate));
+ 	}
+ 	freqs.new = target_pstate * cpu->pstate.scaling;
+ 	cpufreq_freq_transition_end(policy, &freqs, false);
+ 
+ 	return 0;
+ }
+ 
+ static unsigned int intel_cpufreq_fast_switch(struct cpufreq_policy *policy,
+ 					      unsigned int target_freq)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 	int target_pstate;
+ 
+ 	update_turbo_state();
+ 
+ 	target_pstate = DIV_ROUND_UP(target_freq, cpu->pstate.scaling);
+ 	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
+ 	intel_pstate_update_pstate(cpu, target_pstate);
+ 	return target_pstate * cpu->pstate.scaling;
+ }
+ 
+ static int intel_cpufreq_cpu_init(struct cpufreq_policy *policy)
+ {
+ 	int ret = __intel_pstate_cpu_init(policy);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	policy->cpuinfo.transition_latency = INTEL_CPUFREQ_TRANSITION_LATENCY;
+ 	/* This reflects the intel_pstate_get_cpu_pstates() setting. */
+ 	policy->cur = policy->cpuinfo.min_freq;
+ 
+ 	return 0;
+ }
+ 
+ static struct cpufreq_driver intel_cpufreq = {
+ 	.flags		= CPUFREQ_CONST_LOOPS,
+ 	.verify		= intel_cpufreq_verify_policy,
+ 	.target		= intel_cpufreq_target,
+ 	.fast_switch	= intel_cpufreq_fast_switch,
+ 	.init		= intel_cpufreq_cpu_init,
+ 	.exit		= intel_pstate_cpu_exit,
+ 	.stop_cpu	= intel_cpufreq_stop_cpu,
+ 	.name		= "intel_cpufreq",
+ };
+ 
+ static struct cpufreq_driver *intel_pstate_driver = &intel_pstate;
+ 
+ static void intel_pstate_driver_cleanup(void)
+ {
+ 	unsigned int cpu;
+ 
+ 	get_online_cpus();
+ 	for_each_online_cpu(cpu) {
+ 		if (all_cpu_data[cpu]) {
+ 			if (intel_pstate_driver == &intel_pstate)
+ 				intel_pstate_clear_update_util_hook(cpu);
+ 
+ 			kfree(all_cpu_data[cpu]);
+ 			all_cpu_data[cpu] = NULL;
+ 		}
+ 	}
+ 	put_online_cpus();
+ }
+ 
+ static int intel_pstate_register_driver(void)
+ {
+ 	int ret;
+ 
+ 	memset(&global, 0, sizeof(global));
+ 	global.max_perf_pct = 100;
+ 
+ 	ret = cpufreq_register_driver(intel_pstate_driver);
+ 	if (ret) {
+ 		intel_pstate_driver_cleanup();
+ 		return ret;
+ 	}
+ 
+ 	global.min_perf_pct = min_perf_pct_min();
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 	driver_registered = true;
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	if (intel_pstate_driver == &intel_pstate && !hwp_active &&
+ 	    pstate_funcs.get_target_pstate != get_target_pstate_use_cpu_load)
+ 		intel_pstate_debug_expose_params();
+ 
+ 	return 0;
+ }
+ 
+ static int intel_pstate_unregister_driver(void)
+ {
+ 	if (hwp_active)
+ 		return -EBUSY;
+ 
+ 	if (intel_pstate_driver == &intel_pstate && !hwp_active &&
+ 	    pstate_funcs.get_target_pstate != get_target_pstate_use_cpu_load)
+ 		intel_pstate_debug_hide_params();
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 	driver_registered = false;
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	cpufreq_unregister_driver(intel_pstate_driver);
+ 	intel_pstate_driver_cleanup();
+ 
+ 	return 0;
+ }
+ 
+ static ssize_t intel_pstate_show_status(char *buf)
+ {
+ 	if (!driver_registered)
+ 		return sprintf(buf, "off\n");
+ 
+ 	return sprintf(buf, "%s\n", intel_pstate_driver == &intel_pstate ?
+ 					"active" : "passive");
+ }
+ 
+ static int intel_pstate_update_status(const char *buf, size_t size)
+ {
+ 	int ret;
+ 
+ 	if (size == 3 && !strncmp(buf, "off", size))
+ 		return driver_registered ?
+ 			intel_pstate_unregister_driver() : -EINVAL;
+ 
+ 	if (size == 6 && !strncmp(buf, "active", size)) {
+ 		if (driver_registered) {
+ 			if (intel_pstate_driver == &intel_pstate)
+ 				return 0;
+ 
+ 			ret = intel_pstate_unregister_driver();
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		intel_pstate_driver = &intel_pstate;
+ 		return intel_pstate_register_driver();
+ 	}
+ 
+ 	if (size == 7 && !strncmp(buf, "passive", size)) {
+ 		if (driver_registered) {
+ 			if (intel_pstate_driver != &intel_pstate)
+ 				return 0;
+ 
+ 			ret = intel_pstate_unregister_driver();
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		intel_pstate_driver = &intel_cpufreq;
+ 		return intel_pstate_register_driver();
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> 80b120ca1a75 (cpufreq: intel_pstate: Avoid transient updates of cpuinfo.max_freq)
  static int no_load __initdata;
  static int no_hwp __initdata;
  static int hwp_only __initdata;
* Unmerged path drivers/cpufreq/intel_pstate.c
