mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit 5c7fb56e5e3f7035dd798a8e1adee639f87043e5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/5c7fb56e.failed

A dax-huge-page mapping while it uses some thp helpers is ultimately not
a transparent huge page.  The distinction is especially important in the
get_user_pages() path.  pmd_devmap() is used to distinguish dax-pmds
from pmd_huge() and pmd_trans_huge() which have slightly different
semantics.

Explicitly mark the pmd_trans_huge() helpers that dax needs by adding
pmd_devmap() checks.

[kirill.shutemov@linux.intel.com: fix regression in handling mlocked pages in  __split_huge_pmd()]
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Cc: Dave Hansen <dave@sr71.net>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Matthew Wilcox <willy@linux.intel.com>
	Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5c7fb56e5e3f7035dd798a8e1adee639f87043e5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/huge_mm.h
#	include/linux/mm.h
#	mm/huge_memory.c
#	mm/memory.c
#	mm/pgtable-generic.c
diff --cc include/linux/huge_mm.h
index 2f1205c8c5a1,d39fa60bd6bf..000000000000
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@@ -105,26 -96,20 +105,32 @@@ static inline int split_huge_page(struc
  {
  	return split_huge_page_to_list(page, NULL);
  }
 -void deferred_split_huge_page(struct page *page);
 -
 -void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 -		unsigned long address);
 -
 -#define split_huge_pmd(__vma, __pmd, __address)				\
 +extern void __split_huge_page_pmd(struct vm_area_struct *vma,
 +		unsigned long address, pmd_t *pmd);
 +#define split_huge_page_pmd(__vma, __address, __pmd)			\
  	do {								\
  		pmd_t *____pmd = (__pmd);				\
++<<<<<<< HEAD
 +		if (unlikely(pmd_trans_huge(*____pmd)))			\
 +			__split_huge_page_pmd(__vma, __address,		\
 +					____pmd);			\
++=======
+ 		if (pmd_trans_huge(*____pmd)				\
+ 					|| pmd_devmap(*____pmd))	\
+ 			__split_huge_pmd(__vma, __pmd, __address);	\
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  	}  while (0)
 -
 -#if HPAGE_PMD_ORDER >= MAX_ORDER
 +#define wait_split_huge_page(__anon_vma, __pmd)				\
 +	do {								\
 +		pmd_t *____pmd = (__pmd);				\
 +		anon_vma_lock_write(__anon_vma);			\
 +		anon_vma_unlock_write(__anon_vma);			\
 +		BUG_ON(pmd_trans_splitting(*____pmd) ||			\
 +		       pmd_trans_huge(*____pmd));			\
 +	} while (0)
 +extern void split_huge_page_pmd_mm(struct mm_struct *mm, unsigned long address,
 +		pmd_t *pmd);
 +#if HPAGE_PMD_ORDER > MAX_ORDER
  #error "hugepages can't be allocated by the buddy allocator"
  #endif
  extern int hugepage_madvise(struct vm_area_struct *vma,
@@@ -133,17 -118,17 +139,22 @@@ extern void vma_adjust_trans_huge(struc
  				    unsigned long start,
  				    unsigned long end,
  				    long adjust_next);
 -extern bool __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,
 +extern int __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,
  		spinlock_t **ptl);
  /* mmap_sem must be held on entry */
 -static inline bool pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,
 +static inline int pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,
  		spinlock_t **ptl)
  {
++<<<<<<< HEAD
 +	VM_BUG_ON(!rwsem_is_locked(&vma->vm_mm->mmap_sem));
 +	if (pmd_trans_huge(*pmd))
++=======
+ 	VM_BUG_ON_VMA(!rwsem_is_locked(&vma->vm_mm->mmap_sem), vma);
+ 	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  		return __pmd_trans_huge_lock(pmd, vma, ptl);
  	else
 -		return false;
 +		return 0;
  }
  static inline int hpage_nr_pages(struct page *page)
  {
diff --cc include/linux/mm.h
index 3416fff96060,cd123272d28d..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -314,17 -329,12 +314,26 @@@ struct inode
  #define page_private(page)		((page)->private)
  #define set_page_private(page, v)	((page)->private = (v))
  
++<<<<<<< HEAD
 +/* It's valid only if the page is free path or free_list */
 +static inline void set_freepage_migratetype(struct page *page, int migratetype)
 +{
 +	page->index = migratetype;
 +}
 +
 +/* It's valid only if the page is free path or free_list */
 +static inline int get_freepage_migratetype(struct page *page)
 +{
 +	return page->index;
 +}
++=======
+ #if !defined(__HAVE_ARCH_PTE_DEVMAP) || !defined(CONFIG_TRANSPARENT_HUGEPAGE)
+ static inline int pmd_devmap(pmd_t pmd)
+ {
+ 	return 0;
+ }
+ #endif
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  
  /*
   * FIXME: take this include out, include page-flags.h in
diff --cc mm/huge_memory.c
index 329799c812d7,82bed2bec3ed..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -942,20 -1018,16 +942,33 @@@ int copy_huge_pmd(struct mm_struct *dst
  		goto out_unlock;
  	}
  
++<<<<<<< HEAD
 +	if (unlikely(pmd_trans_splitting(pmd))) {
 +		/* split huge page running from under us */
 +		spin_unlock(src_ptl);
 +		spin_unlock(dst_ptl);
 +		pte_free(dst_mm, pgtable);
 +
 +		wait_split_huge_page(vma->anon_vma, src_pmd); /* src_vma */
 +		goto out;
 +	}
 +	src_page = pmd_page(pmd);
 +	VM_BUG_ON_PAGE(!PageHead(src_page), src_page);
 +	get_page(src_page);
 +	page_dup_rmap(src_page);
 +	add_mm_counter(dst_mm, MM_ANONPAGES, HPAGE_PMD_NR);
++=======
+ 	if (pmd_trans_huge(pmd)) {
+ 		/* thp accounting separate from pmd_devmap accounting */
+ 		src_page = pmd_page(pmd);
+ 		VM_BUG_ON_PAGE(!PageHead(src_page), src_page);
+ 		get_page(src_page);
+ 		page_dup_rmap(src_page, true);
+ 		add_mm_counter(dst_mm, MM_ANONPAGES, HPAGE_PMD_NR);
+ 		atomic_long_inc(&dst_mm->nr_ptes);
+ 		pgtable_trans_huge_deposit(dst_mm, dst_pmd, pgtable);
+ 	}
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  
  	pmdp_set_wrprotect(src_mm, addr, src_pmd);
  	pmd = pmd_mkold(pmd_wrprotect(pmd));
@@@ -1648,433 -1719,41 +1659,438 @@@ int __pmd_trans_huge_lock(pmd_t *pmd, s
  		spinlock_t **ptl)
  {
  	*ptl = pmd_lock(vma->vm_mm, pmd);
++<<<<<<< HEAD
 +	if (likely(pmd_trans_huge(*pmd))) {
 +		if (unlikely(pmd_trans_splitting(*pmd))) {
 +			spin_unlock(*ptl);
 +			wait_split_huge_page(vma->anon_vma, pmd);
 +			return -1;
 +		} else {
 +			/* Thp mapped by 'pmd' is stable, so we can
 +			 * handle it as it is. */
 +			return 1;
 +		}
 +	}
++=======
+ 	if (likely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))
+ 		return true;
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  	spin_unlock(*ptl);
 -	return false;
 +	return 0;
  }
  
 -#define VM_NO_THP (VM_SPECIAL | VM_HUGETLB | VM_SHARED | VM_MAYSHARE)
 +/*
 + * This function returns whether a given @page is mapped onto the @address
 + * in the virtual space of @mm.
 + *
 + * When it's true, this function returns *pmd with holding the page table lock
 + * and passing it back to the caller via @ptl.
 + * If it's false, returns NULL without holding the page table lock.
 + */
 +pmd_t *page_check_address_pmd(struct page *page,
 +			      struct mm_struct *mm,
 +			      unsigned long address,
 +			      enum page_check_address_pmd_flag flag,
 +			      spinlock_t **ptl)
 +{
 +	pgd_t *pgd;
 +	pud_t *pud;
 +	pmd_t *pmd;
 +
 +	if (address & ~HPAGE_PMD_MASK)
 +		return NULL;
  
 -int hugepage_madvise(struct vm_area_struct *vma,
 -		     unsigned long *vm_flags, int advice)
 +	pgd = pgd_offset(mm, address);
 +	if (!pgd_present(*pgd))
 +		return NULL;
 +	pud = pud_offset(pgd, address);
 +	if (!pud_present(*pud))
 +		return NULL;
 +	pmd = pmd_offset(pud, address);
 +
 +	*ptl = pmd_lock(mm, pmd);
 +	if (!pmd_present(*pmd))
 +		goto unlock;
 +	if (pmd_page(*pmd) != page)
 +		goto unlock;
 +	/*
 +	 * split_vma() may create temporary aliased mappings. There is
 +	 * no risk as long as all huge pmd are found and have their
 +	 * splitting bit set before __split_huge_page_refcount
 +	 * runs. Finding the same huge pmd more than once during the
 +	 * same rmap walk is not a problem.
 +	 */
 +	if (flag == PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG &&
 +	    pmd_trans_splitting(*pmd))
 +		goto unlock;
 +	if (pmd_trans_huge(*pmd)) {
 +		VM_BUG_ON(flag == PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG &&
 +			  !pmd_trans_splitting(*pmd));
 +		return pmd;
 +	}
 +unlock:
 +	spin_unlock(*ptl);
 +	return NULL;
 +}
 +
 +static int __split_huge_page_splitting(struct page *page,
 +				       struct vm_area_struct *vma,
 +				       unsigned long address)
  {
 -	switch (advice) {
 -	case MADV_HUGEPAGE:
 -#ifdef CONFIG_S390
 -		/*
 -		 * qemu blindly sets MADV_HUGEPAGE on all allocations, but s390
 -		 * can't handle this properly after s390_enable_sie, so we simply
 -		 * ignore the madvise to prevent qemu from causing a SIGSEGV.
 -		 */
 -		if (mm_has_pgste(vma->vm_mm))
 -			return 0;
 -#endif
 +	struct mm_struct *mm = vma->vm_mm;
 +	spinlock_t *ptl;
 +	pmd_t *pmd;
 +	int ret = 0;
 +	/* For mmu_notifiers */
 +	const unsigned long mmun_start = address;
 +	const unsigned long mmun_end   = address + HPAGE_PMD_SIZE;
 +
 +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 +	pmd = page_check_address_pmd(page, mm, address,
 +			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &ptl);
 +	if (pmd) {
  		/*
 -		 * Be somewhat over-protective like KSM for now!
 +		 * We can't temporarily set the pmd to null in order
 +		 * to split it, the pmd must remain marked huge at all
 +		 * times or the VM won't take the pmd_trans_huge paths
 +		 * and it won't wait on the anon_vma->root->rwsem to
 +		 * serialize against split_huge_page*.
  		 */
 -		if (*vm_flags & VM_NO_THP)
 -			return -EINVAL;
 -		*vm_flags &= ~VM_NOHUGEPAGE;
 -		*vm_flags |= VM_HUGEPAGE;
 -		/*
 -		 * If the vma become good for khugepaged to scan,
 -		 * register it here without waiting a page fault that
 -		 * may not happen any time soon.
 +		pmdp_splitting_flush(vma, address, pmd);
 +		ret = 1;
 +		spin_unlock(ptl);
 +	}
 +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 +
 +	return ret;
 +}
 +
 +static void __split_huge_page_refcount(struct page *page,
 +				       struct list_head *list)
 +{
 +	int i;
 +	struct zone *zone = page_zone(page);
 +	struct lruvec *lruvec;
 +	int tail_count = 0;
 +	int mmu_gather_count;
 +
 +	/* prevent PageLRU to go away from under us, and freeze lru stats */
 +	spin_lock_irq(&zone->lru_lock);
 +	lruvec = mem_cgroup_page_lruvec(page, zone);
 +
 +	/*
 +	 * No mmu_gather_count increase can happen anymore because
 +	 * here all pmds are already pmd_trans_splitting(). No
 +	 * decrease can happen either because it's only decreased
 +	 * while holding the lru_lock. So here the mmu_gather_count is
 +	 * already stable so store it on the stack. Then it'll be
 +	 * overwritten when the page_tail->index is initialized.
 +	 */
 +	mmu_gather_count = trans_huge_mmu_gather_count(page);
 +
 +	compound_lock(page);
 +	/* complete memcg works before add pages to LRU */
 +	mem_cgroup_split_huge_fixup(page);
 +
 +	for (i = HPAGE_PMD_NR - 1; i >= 1; i--) {
 +		struct page *page_tail = page + i;
 +
 +		/* tail_page->_mapcount cannot change */
 +		BUG_ON(page_mapcount(page_tail) < 0);
 +		tail_count += page_mapcount(page_tail);
 +		/* check for overflow */
 +		BUG_ON(tail_count < 0);
 +		BUG_ON(atomic_read(&page_tail->_count) != 0);
 +		/*
 +		 * tail_page->_count is zero and not changing from
 +		 * under us. But get_page_unless_zero() may be running
 +		 * from under us on the tail_page. If we used
 +		 * atomic_set() below instead of atomic_add(), we
 +		 * would then run atomic_set() concurrently with
 +		 * get_page_unless_zero(), and atomic_set() is
 +		 * implemented in C not using locked ops. spin_unlock
 +		 * on x86 sometime uses locked ops because of PPro
 +		 * errata 66, 92, so unless somebody can guarantee
 +		 * atomic_set() here would be safe on all archs (and
 +		 * not only on x86), it's safer to use atomic_add().
 +		 */
 +		atomic_add(page_mapcount(page) + page_mapcount(page_tail) +
 +			   mmu_gather_count + 1, &page_tail->_count);
 +
 +		/* after clearing PageTail the gup refcount can be released */
 +		smp_mb();
 +
 +		/*
 +		 * retain hwpoison flag of the poisoned tail page:
 +		 *   fix for the unsuitable process killed on Guest Machine(KVM)
 +		 *   by the memory-failure.
 +		 */
 +		page_tail->flags &= ~PAGE_FLAGS_CHECK_AT_PREP | __PG_HWPOISON;
 +		page_tail->flags |= (page->flags &
 +				     ((1L << PG_referenced) |
 +				      (1L << PG_swapbacked) |
 +				      (1L << PG_mlocked) |
 +				      (1L << PG_uptodate) |
 +				      (1L << PG_active) |
 +				      (1L << PG_unevictable)));
 +		page_tail->flags |= (1L << PG_dirty);
 +
 +		/* clear PageTail before overwriting first_page */
 +		smp_wmb();
 +
 +		/*
 +		 * __split_huge_page_splitting() already set the
 +		 * splitting bit in all pmd that could map this
 +		 * hugepage, that will ensure no CPU can alter the
 +		 * mapcount on the head page. The mapcount is only
 +		 * accounted in the head page and it has to be
 +		 * transferred to all tail pages in the below code. So
 +		 * for this code to be safe, the split the mapcount
 +		 * can't change. But that doesn't mean userland can't
 +		 * keep changing and reading the page contents while
 +		 * we transfer the mapcount, so the pmd splitting
 +		 * status is achieved setting a reserved bit in the
 +		 * pmd, not by clearing the present bit.
 +		*/
 +		page_tail->_mapcount = page->_mapcount;
 +
 +		BUG_ON(page_tail->mapping);
 +		page_tail->mapping = page->mapping;
 +
 +		page_tail->index = page->index + i;
 +		page_cpupid_xchg_last(page_tail, page_cpupid_last(page));
 +
 +		BUG_ON(!PageAnon(page_tail));
 +		BUG_ON(!PageUptodate(page_tail));
 +		BUG_ON(!PageDirty(page_tail));
 +		BUG_ON(!PageSwapBacked(page_tail));
 +
 +		lru_add_page_tail(page, page_tail, lruvec, list);
 +	}
 +	atomic_sub(tail_count, &page->_count);
 +	BUG_ON(atomic_read(&page->_count) <= 0);
 +
 +	__mod_zone_page_state(zone, NR_ANON_TRANSPARENT_HUGEPAGES, -1);
 +	__mod_zone_page_state(zone, NR_ANON_PAGES, HPAGE_PMD_NR);
 +
 +	ClearPageCompound(page);
 +	compound_unlock(page);
 +	spin_unlock_irq(&zone->lru_lock);
 +
 +	for (i = 1; i < HPAGE_PMD_NR; i++) {
 +		struct page *page_tail = page + i;
 +		BUG_ON(page_count(page_tail) <= 0);
 +		/*
 +		 * Tail pages may be freed if there wasn't any mapping
 +		 * like if add_to_swap() is running on a lru page that
 +		 * had its mapping zapped. And freeing these pages
 +		 * requires taking the lru_lock so we do the put_page
 +		 * of the tail pages after the split is complete.
 +		 */
 +		put_page(page_tail);
 +	}
 +
 +	/*
 +	 * Only the head page (now become a regular page) is required
 +	 * to be pinned by the caller.
 +	 */
 +	BUG_ON(page_count(page) <= 0);
 +}
 +
 +static int __split_huge_page_map(struct page *page,
 +				 struct vm_area_struct *vma,
 +				 unsigned long address)
 +{
 +	struct mm_struct *mm = vma->vm_mm;
 +	spinlock_t *ptl;
 +	pmd_t *pmd, _pmd = {0};
 +	int ret = 0, i;
 +	pgtable_t pgtable;
 +	unsigned long haddr;
 +
 +	pmd = page_check_address_pmd(page, mm, address,
 +			PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG, &ptl);
 +	if (pmd) {
 +		pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 +		pmd_populate(mm, &_pmd, pgtable);
 +
 +		haddr = address;
 +		for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
 +			pte_t *pte, entry;
 +			BUG_ON(PageCompound(page+i));
 +			entry = mk_pte(page + i, vma->vm_page_prot);
 +			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 +			if (!pmd_write(*pmd))
 +				entry = pte_wrprotect(entry);
 +			else
 +				BUG_ON(page_mapcount(page) != 1);
 +			if (!pmd_young(*pmd))
 +				entry = pte_mkold(entry);
 +			if (pmd_numa(*pmd))
 +				entry = pte_mknuma(entry);
 +			pte = pte_offset_map(&_pmd, haddr);
 +			BUG_ON(!pte_none(*pte));
 +			set_pte_at(mm, haddr, pte, entry);
 +			pte_unmap(pte);
 +		}
 +
 +		smp_wmb(); /* make pte visible before pmd */
 +		/*
 +		 * Up to this point the pmd is present and huge and
 +		 * userland has the whole access to the hugepage
 +		 * during the split (which happens in place). If we
 +		 * overwrite the pmd with the not-huge version
 +		 * pointing to the pte here (which of course we could
 +		 * if all CPUs were bug free), userland could trigger
 +		 * a small page size TLB miss on the small sized TLB
 +		 * while the hugepage TLB entry is still established
 +		 * in the huge TLB. Some CPU doesn't like that. See
 +		 * http://support.amd.com/us/Processor_TechDocs/41322.pdf,
 +		 * Erratum 383 on page 93. Intel should be safe but is
 +		 * also warns that it's only safe if the permission
 +		 * and cache attributes of the two entries loaded in
 +		 * the two TLB is identical (which should be the case
 +		 * here). But it is generally safer to never allow
 +		 * small and huge TLB entries for the same virtual
 +		 * address to be loaded simultaneously. So instead of
 +		 * doing "pmd_populate(); flush_tlb_range();" we first
 +		 * mark the current pmd notpresent (atomically because
 +		 * here the pmd_trans_huge and pmd_trans_splitting
 +		 * must remain set at all times on the pmd until the
 +		 * split is complete for this pmd), then we flush the
 +		 * SMP TLB and finally we write the non-huge version
 +		 * of the pmd entry with pmd_populate.
  		 */
 -		if (unlikely(khugepaged_enter_vma_merge(vma, *vm_flags)))
 +		pmdp_invalidate(vma, address, pmd);
 +		pmd_populate(mm, pmd, pgtable);
 +		ret = 1;
 +		spin_unlock(ptl);
 +	}
 +
 +	return ret;
 +}
 +
 +/* must be called with anon_vma->root->rwsem held */
 +static void __split_huge_page(struct page *page,
 +			      struct anon_vma *anon_vma,
 +			      struct list_head *list)
 +{
 +	int mapcount, mapcount2;
 +	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
 +	struct anon_vma_chain *avc;
 +
 +	BUG_ON(!PageHead(page));
 +	BUG_ON(PageTail(page));
 +
 +	mapcount = 0;
 +	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
 +		struct vm_area_struct *vma = avc->vma;
 +		unsigned long addr = vma_address(page, vma);
 +		BUG_ON(is_vma_temporary_stack(vma));
 +		mapcount += __split_huge_page_splitting(page, vma, addr);
 +	}
 +	/*
 +	 * It is critical that new vmas are added to the tail of the
 +	 * anon_vma list. This guarantes that if copy_huge_pmd() runs
 +	 * and establishes a child pmd before
 +	 * __split_huge_page_splitting() freezes the parent pmd (so if
 +	 * we fail to prevent copy_huge_pmd() from running until the
 +	 * whole __split_huge_page() is complete), we will still see
 +	 * the newly established pmd of the child later during the
 +	 * walk, to be able to set it as pmd_trans_splitting too.
 +	 */
 +	if (mapcount != page_mapcount(page))
 +		printk(KERN_ERR "mapcount %d page_mapcount %d\n",
 +		       mapcount, page_mapcount(page));
 +	BUG_ON(mapcount != page_mapcount(page));
 +
 +	__split_huge_page_refcount(page, list);
 +
 +	mapcount2 = 0;
 +	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {
 +		struct vm_area_struct *vma = avc->vma;
 +		unsigned long addr = vma_address(page, vma);
 +		BUG_ON(is_vma_temporary_stack(vma));
 +		mapcount2 += __split_huge_page_map(page, vma, addr);
 +	}
 +	if (mapcount != mapcount2)
 +		printk(KERN_ERR "mapcount %d mapcount2 %d page_mapcount %d\n",
 +		       mapcount, mapcount2, page_mapcount(page));
 +	BUG_ON(mapcount != mapcount2);
 +}
 +
 +/*
 + * Split a hugepage into normal pages. This doesn't change the position of head
 + * page. If @list is null, tail pages will be added to LRU list, otherwise, to
 + * @list. Both head page and tail pages will inherit mapping, flags, and so on
 + * from the hugepage.
 + * Return 0 if the hugepage is split successfully otherwise return 1.
 + */
 +int split_huge_page_to_list(struct page *page, struct list_head *list)
 +{
 +	struct anon_vma *anon_vma;
 +	int ret = 1;
 +
 +	BUG_ON(is_huge_zero_page(page));
 +	BUG_ON(!PageAnon(page));
 +
 +	/*
 +	 * The caller does not necessarily hold an mmap_sem that would prevent
 +	 * the anon_vma disappearing so we first we take a reference to it
 +	 * and then lock the anon_vma for write. This is similar to
 +	 * page_lock_anon_vma_read except the write lock is taken to serialise
 +	 * against parallel split or collapse operations.
 +	 */
 +	anon_vma = page_get_anon_vma(page);
 +	if (!anon_vma)
 +		goto out;
 +	anon_vma_lock_write(anon_vma);
 +
 +	ret = 0;
 +	if (!PageCompound(page))
 +		goto out_unlock;
 +
 +	BUG_ON(!PageSwapBacked(page));
 +	__split_huge_page(page, anon_vma, list);
 +	count_vm_event(THP_SPLIT);
 +
 +	BUG_ON(PageCompound(page));
 +out_unlock:
 +	anon_vma_unlock_write(anon_vma);
 +	put_anon_vma(anon_vma);
 +out:
 +	return ret;
 +}
 +
 +#define VM_NO_THP (VM_SPECIAL|VM_MIXEDMAP|VM_HUGETLB|VM_SHARED|VM_MAYSHARE)
 +
 +int hugepage_madvise(struct vm_area_struct *vma,
 +		     unsigned long *vm_flags, int advice)
 +{
 +	switch (advice) {
 +	case MADV_HUGEPAGE:
 +#ifdef CONFIG_S390
 +		/*
 +		 * qemu blindly sets MADV_HUGEPAGE on all allocations, but s390
 +		 * can't handle this properly after s390_enable_sie, so we simply
 +		 * ignore the madvise to prevent qemu from causing a SIGSEGV.
 +		 */
 +		if (mm_has_pgste(vma->vm_mm))
 +			return 0;
 +#endif
 +		/*
 +		 * Be somewhat over-protective like KSM for now!
 +		 */
 +		if (*vm_flags & (VM_HUGEPAGE | VM_NO_THP))
 +			return -EINVAL;
 +		*vm_flags &= ~VM_NOHUGEPAGE;
 +		*vm_flags |= VM_HUGEPAGE;
 +		/*
 +		 * If the vma become good for khugepaged to scan,
 +		 * register it here without waiting a page fault that
 +		 * may not happen any time soon.
 +		 */
 +		if (unlikely(khugepaged_enter_vma_merge(vma)))
  			return -ENOMEM;
  		break;
  	case MADV_NOHUGEPAGE:
@@@ -2929,66 -2778,153 +2945,207 @@@ static void __split_huge_zero_page_pmd(
  	put_huge_zero_page();
  }
  
++<<<<<<< HEAD
 +void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,
 +		pmd_t *pmd)
++=======
+ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
+ 		unsigned long haddr, bool freeze)
+ {
+ 	struct mm_struct *mm = vma->vm_mm;
+ 	struct page *page;
+ 	pgtable_t pgtable;
+ 	pmd_t _pmd;
+ 	bool young, write, dirty;
+ 	int i;
+ 
+ 	VM_BUG_ON(haddr & ~HPAGE_PMD_MASK);
+ 	VM_BUG_ON_VMA(vma->vm_start > haddr, vma);
+ 	VM_BUG_ON_VMA(vma->vm_end < haddr + HPAGE_PMD_SIZE, vma);
+ 	VM_BUG_ON(!pmd_trans_huge(*pmd) && !pmd_devmap(*pmd));
+ 
+ 	count_vm_event(THP_SPLIT_PMD);
+ 
+ 	if (vma_is_dax(vma)) {
+ 		pmd_t _pmd = pmdp_huge_clear_flush_notify(vma, haddr, pmd);
+ 		if (is_huge_zero_pmd(_pmd))
+ 			put_huge_zero_page();
+ 		return;
+ 	} else if (is_huge_zero_pmd(*pmd)) {
+ 		return __split_huge_zero_page_pmd(vma, haddr, pmd);
+ 	}
+ 
+ 	page = pmd_page(*pmd);
+ 	VM_BUG_ON_PAGE(!page_count(page), page);
+ 	atomic_add(HPAGE_PMD_NR - 1, &page->_count);
+ 	write = pmd_write(*pmd);
+ 	young = pmd_young(*pmd);
+ 	dirty = pmd_dirty(*pmd);
+ 
+ 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
+ 	pmd_populate(mm, &_pmd, pgtable);
+ 
+ 	for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
+ 		pte_t entry, *pte;
+ 		/*
+ 		 * Note that NUMA hinting access restrictions are not
+ 		 * transferred to avoid any possibility of altering
+ 		 * permissions across VMAs.
+ 		 */
+ 		if (freeze) {
+ 			swp_entry_t swp_entry;
+ 			swp_entry = make_migration_entry(page + i, write);
+ 			entry = swp_entry_to_pte(swp_entry);
+ 		} else {
+ 			entry = mk_pte(page + i, vma->vm_page_prot);
+ 			entry = maybe_mkwrite(entry, vma);
+ 			if (!write)
+ 				entry = pte_wrprotect(entry);
+ 			if (!young)
+ 				entry = pte_mkold(entry);
+ 		}
+ 		if (dirty)
+ 			SetPageDirty(page + i);
+ 		pte = pte_offset_map(&_pmd, haddr);
+ 		BUG_ON(!pte_none(*pte));
+ 		set_pte_at(mm, haddr, pte, entry);
+ 		atomic_inc(&page[i]._mapcount);
+ 		pte_unmap(pte);
+ 	}
+ 
+ 	/*
+ 	 * Set PG_double_map before dropping compound_mapcount to avoid
+ 	 * false-negative page_mapped().
+ 	 */
+ 	if (compound_mapcount(page) > 1 && !TestSetPageDoubleMap(page)) {
+ 		for (i = 0; i < HPAGE_PMD_NR; i++)
+ 			atomic_inc(&page[i]._mapcount);
+ 	}
+ 
+ 	if (atomic_add_negative(-1, compound_mapcount_ptr(page))) {
+ 		/* Last compound_mapcount is gone. */
+ 		__dec_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);
+ 		if (TestClearPageDoubleMap(page)) {
+ 			/* No need in mapcount reference anymore */
+ 			for (i = 0; i < HPAGE_PMD_NR; i++)
+ 				atomic_dec(&page[i]._mapcount);
+ 		}
+ 	}
+ 
+ 	smp_wmb(); /* make pte visible before pmd */
+ 	/*
+ 	 * Up to this point the pmd is present and huge and userland has the
+ 	 * whole access to the hugepage during the split (which happens in
+ 	 * place). If we overwrite the pmd with the not-huge version pointing
+ 	 * to the pte here (which of course we could if all CPUs were bug
+ 	 * free), userland could trigger a small page size TLB miss on the
+ 	 * small sized TLB while the hugepage TLB entry is still established in
+ 	 * the huge TLB. Some CPU doesn't like that.
+ 	 * See http://support.amd.com/us/Processor_TechDocs/41322.pdf, Erratum
+ 	 * 383 on page 93. Intel should be safe but is also warns that it's
+ 	 * only safe if the permission and cache attributes of the two entries
+ 	 * loaded in the two TLB is identical (which should be the case here).
+ 	 * But it is generally safer to never allow small and huge TLB entries
+ 	 * for the same virtual address to be loaded simultaneously. So instead
+ 	 * of doing "pmd_populate(); flush_pmd_tlb_range();" we first mark the
+ 	 * current pmd notpresent (atomically because here the pmd_trans_huge
+ 	 * and pmd_trans_splitting must remain set at all times on the pmd
+ 	 * until the split is complete for this pmd), then we flush the SMP TLB
+ 	 * and finally we write the non-huge version of the pmd entry with
+ 	 * pmd_populate.
+ 	 */
+ 	pmdp_invalidate(vma, haddr, pmd);
+ 	pmd_populate(mm, pmd, pgtable);
+ 
+ 	if (freeze) {
+ 		for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
+ 			page_remove_rmap(page + i, false);
+ 			put_page(page + i);
+ 		}
+ 	}
+ }
+ 
+ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+ 		unsigned long address)
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  {
  	spinlock_t *ptl;
 -	struct mm_struct *mm = vma->vm_mm;
  	struct page *page = NULL;
 +	struct mm_struct *mm = vma->vm_mm;
  	unsigned long haddr = address & HPAGE_PMD_MASK;
 +	unsigned long mmun_start;	/* For mmu_notifiers */
 +	unsigned long mmun_end;		/* For mmu_notifiers */
  
 -	mmu_notifier_invalidate_range_start(mm, haddr, haddr + HPAGE_PMD_SIZE);
 +	BUG_ON(vma->vm_start > haddr || vma->vm_end < haddr + HPAGE_PMD_SIZE);
 +
 +	mmun_start = haddr;
 +	mmun_end   = haddr + HPAGE_PMD_SIZE;
 +again:
 +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
  	ptl = pmd_lock(mm, pmd);
++<<<<<<< HEAD
 +	if (unlikely(!pmd_trans_huge(*pmd)))
 +		goto unlock;
 +	if (vma_is_dax(vma)) {
 +		pmd_t _pmd = pmdp_clear_flush_notify(vma, haddr, pmd);
 +		if (is_huge_zero_pmd(_pmd))
 +			put_huge_zero_page();
 +	} else if (is_huge_zero_pmd(*pmd)) {
 +		__split_huge_zero_page_pmd(vma, haddr, pmd);
 +	} else {
 +		page = pmd_page(*pmd);
 +		VM_BUG_ON_PAGE(!page_count(page), page);
 +		get_page(page);
++=======
+ 	if (pmd_trans_huge(*pmd)) {
+ 		page = pmd_page(*pmd);
+ 		if (PageMlocked(page))
+ 			get_page(page);
+ 		else
+ 			page = NULL;
+ 	} else if (!pmd_devmap(*pmd))
+ 		goto out;
+ 	__split_huge_pmd_locked(vma, pmd, haddr, false);
+ out:
+ 	spin_unlock(ptl);
+ 	mmu_notifier_invalidate_range_end(mm, haddr, haddr + HPAGE_PMD_SIZE);
+ 	if (page) {
+ 		lock_page(page);
+ 		munlock_vma_page(page);
+ 		unlock_page(page);
+ 		put_page(page);
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  	}
 + unlock:
 +	spin_unlock(ptl);
 +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 +
 +	if (!page)
 +		return;
 +
 +	split_huge_page(page);
 +	put_page(page);
 +
 +	/*
 +	 * We don't always have down_write of mmap_sem here: a racing
 +	 * do_huge_pmd_wp_page() might have copied-on-write to another
 +	 * huge page before our split_huge_page() got the anon_vma lock.
 +	 */
 +	if (unlikely(pmd_trans_huge(*pmd)))
 +		goto again;
 +}
 +
 +void split_huge_page_pmd_mm(struct mm_struct *mm, unsigned long address,
 +		pmd_t *pmd)
 +{
 +	struct vm_area_struct *vma;
 +
 +	vma = find_vma(mm, address);
 +	BUG_ON(vma == NULL);
 +	split_huge_page_pmd(vma, address, pmd);
  }
  
 -static void split_huge_pmd_address(struct vm_area_struct *vma,
 +static void split_huge_page_address(struct mm_struct *mm,
  				    unsigned long address)
  {
  	pgd_t *pgd;
@@@ -3006,7 -2942,7 +3163,11 @@@
  		return;
  
  	pmd = pmd_offset(pud, address);
++<<<<<<< HEAD
 +	if (!pmd_present(*pmd))
++=======
+ 	if (!pmd_present(*pmd) || (!pmd_trans_huge(*pmd) && !pmd_devmap(*pmd)))
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  		return;
  	/*
  	 * Caller holds the mmap_sem write mode, so a huge pmd cannot
diff --cc mm/memory.c
index 14270187456b,ff17850a52d9..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3291,18 -3375,10 +3291,18 @@@ static int __handle_mm_fault(struct mm_
  		int ret;
  
  		barrier();
- 		if (pmd_trans_huge(orig_pmd)) {
+ 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
  			unsigned int dirty = flags & FAULT_FLAG_WRITE;
  
 -			if (pmd_protnone(orig_pmd))
 +			/*
 +			 * If the pmd is splitting, return and retry the
 +			 * the fault.  Alternative: wait until the split
 +			 * is done, and goto retry.
 +			 */
 +			if (pmd_trans_splitting(orig_pmd))
 +				return 0;
 +
 +			if (pmd_numa(orig_pmd))
  				return do_huge_pmd_numa_page(mm, vma, address,
  							     orig_pmd, pmd);
  
@@@ -3327,18 -3403,8 +3327,23 @@@
  	if (unlikely(pmd_none(*pmd)) &&
  	    unlikely(__pte_alloc(mm, vma, pmd, address)))
  		return VM_FAULT_OOM;
++<<<<<<< HEAD
 +	/*
 +	 * If a huge pmd materialized under us just retry later.  Use
 +	 * pmd_trans_unstable() instead of pmd_trans_huge() to ensure the pmd
 +	 * didn't become pmd_trans_huge under us and then back to pmd_none, as
 +	 * a result of MADV_DONTNEED running immediately after a huge pmd fault
 +	 * in a different thread of this mm, in turn leading to a misleading
 +	 * pmd_trans_huge() retval.  All we have to ensure is that it is a
 +	 * regular pmd that we can walk with pte_offset_map() and we can do that
 +	 * through an atomic read in C, which is what pmd_trans_unstable()
 +	 * provides.
 +	 */
 +	if (unlikely(pmd_trans_unstable(pmd)))
++=======
+ 	/* if an huge pmd materialized from under us just retry later */
+ 	if (unlikely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  		return 0;
  	/*
  	 * A regular pmd is established and it can't morph into a huge pmd
diff --cc mm/pgtable-generic.c
index d0b6afa621ef,9d4767698a1c..000000000000
--- a/mm/pgtable-generic.c
+++ b/mm/pgtable-generic.c
@@@ -102,25 -132,11 +102,31 @@@ pmd_t pmdp_clear_flush(struct vm_area_s
  {
  	pmd_t pmd;
  	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
++<<<<<<< HEAD
 +	pmd = pmdp_get_and_clear(vma->vm_mm, address, pmdp);
 +	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
++=======
+ 	VM_BUG_ON(!pmd_trans_huge(*pmdp) && !pmd_devmap(*pmdp));
+ 	pmd = pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);
+ 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
++>>>>>>> 5c7fb56e5e3f (mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd)
  	return pmd;
  }
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 +#endif
 +
 +#ifndef __HAVE_ARCH_PMDP_SPLITTING_FLUSH
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
 +			  pmd_t *pmdp)
 +{
 +	pmd_t pmd = pmd_mksplitting(*pmdp);
 +	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 +	set_pmd_at(vma->vm_mm, address, pmdp, pmd);
 +	/* tlb flush only to serialize against gup-fast */
 +	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 +}
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  #endif
  
  #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index 50bdb1a560e0..af61e00cb664 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -177,13 +177,20 @@ static inline int pmd_trans_splitting(pmd_t pmd)
 
 static inline int pmd_trans_huge(pmd_t pmd)
 {
-	return pmd_val(pmd) & _PAGE_PSE;
+	return (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;
 }
 
 static inline int has_transparent_hugepage(void)
 {
 	return cpu_has_pse;
 }
+
+#ifdef __HAVE_ARCH_PTE_DEVMAP
+static inline int pmd_devmap(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_DEVMAP);
+}
+#endif
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
* Unmerged path include/linux/huge_mm.h
* Unmerged path include/linux/mm.h
* Unmerged path mm/huge_memory.c
* Unmerged path mm/memory.c
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 12cbcc768180..bf960e9170b7 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -142,7 +142,8 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 		unsigned long this_pages;
 
 		next = pmd_addr_end(addr, end);
-		if (!pmd_trans_huge(*pmd) && pmd_none_or_clear_bad(pmd))
+		if (!pmd_trans_huge(*pmd) && !pmd_devmap(*pmd)
+				&& pmd_none_or_clear_bad(pmd))
 			continue;
 
 		/* invoke the mmu notifier if the pmd is populated */
@@ -151,7 +152,7 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 			mmu_notifier_invalidate_range_start(mm, mni_start, end);
 		}
 
-		if (pmd_trans_huge(*pmd)) {
+		if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
 			if (next - addr != HPAGE_PMD_SIZE)
 				split_huge_page_pmd(vma, addr, pmd);
 			else {
* Unmerged path mm/pgtable-generic.c
