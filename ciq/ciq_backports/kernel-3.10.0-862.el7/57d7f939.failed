s390: add no-execute support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [s390] add no-execute support (Hendrik Brueckner) [1489742]
Rebuild_FUZZ: 88.00%
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit 57d7f939e7bdd746992f5c318a78697ba837c523
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/57d7f939.failed

Bit 0x100 of a page table, segment table of region table entry
can be used to disallow code execution for the virtual addresses
associated with the entry.

There is one tricky bit, the system call to return from a signal
is part of the signal frame written to the user stack. With a
non-executable stack this would stop working. To avoid breaking
things the protection fault handler checks the opcode that caused
the fault for 0x0a77 (sys_sigreturn) and 0x0aad (sys_rt_sigreturn)
and injects a system call. This is preferable to the alternative
solution with a stub function in the vdso because it works for
vdso=off and statically linked binaries as well.

	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 57d7f939e7bdd746992f5c318a78697ba837c523)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/cacheflush.h
#	arch/s390/include/asm/pgtable.h
#	arch/s390/include/asm/setup.h
#	arch/s390/kernel/early.c
#	arch/s390/kernel/entry.S
#	arch/s390/kernel/module.c
#	arch/s390/mm/dump_pagetables.c
#	arch/s390/mm/hugetlbpage.c
#	arch/s390/mm/init.c
#	arch/s390/mm/pageattr.c
#	arch/s390/mm/pgtable.c
#	arch/s390/mm/vmem.c
diff --cc arch/s390/include/asm/cacheflush.h
index 3e20383d0921,0499334f9473..000000000000
--- a/arch/s390/include/asm/cacheflush.h
+++ b/arch/s390/include/asm/cacheflush.h
@@@ -4,13 -4,31 +4,42 @@@
  /* Caches aren't brain-dead on the s390. */
  #include <asm-generic/cacheflush.h>
  
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_PAGEALLOC
 +void kernel_map_pages(struct page *page, int numpages, int enable);
 +#endif
 +
 +int set_memory_ro(unsigned long addr, int numpages);
 +int set_memory_rw(unsigned long addr, int numpages);
 +int set_memory_nx(unsigned long addr, int numpages);
 +int set_memory_x(unsigned long addr, int numpages);
++=======
+ #define SET_MEMORY_RO	1UL
+ #define SET_MEMORY_RW	2UL
+ #define SET_MEMORY_NX	4UL
+ #define SET_MEMORY_X	8UL
+ 
+ int __set_memory(unsigned long addr, int numpages, unsigned long flags);
+ 
+ static inline int set_memory_ro(unsigned long addr, int numpages)
+ {
+ 	return __set_memory(addr, numpages, SET_MEMORY_RO);
+ }
+ 
+ static inline int set_memory_rw(unsigned long addr, int numpages)
+ {
+ 	return __set_memory(addr, numpages, SET_MEMORY_RW);
+ }
+ 
+ static inline int set_memory_nx(unsigned long addr, int numpages)
+ {
+ 	return __set_memory(addr, numpages, SET_MEMORY_NX);
+ }
+ 
+ static inline int set_memory_x(unsigned long addr, int numpages)
+ {
+ 	return __set_memory(addr, numpages, SET_MEMORY_X);
+ }
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  
  #endif /* _S390_CACHEFLUSH_H */
diff --cc arch/s390/include/asm/pgtable.h
index 3e30aad217ee,d03b60d53f99..000000000000
--- a/arch/s390/include/asm/pgtable.h
+++ b/arch/s390/include/asm/pgtable.h
@@@ -219,7 -200,7 +219,11 @@@ extern unsigned long MODULES_END
   */
  
  /* Hardware bits in the page table entry */
++<<<<<<< HEAD
 +#define _PAGE_CO	0x100		/* HW Change-bit override */
++=======
+ #define _PAGE_NOEXEC	0x100		/* HW no-execute bit  */
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  #define _PAGE_PROTECT	0x200		/* HW read-only bit  */
  #define _PAGE_INVALID	0x400		/* HW invalid bit    */
  #define _PAGE_LARGE	0x800		/* Bit to mark a large pte */
@@@ -331,6 -278,8 +335,11 @@@
  /* Bits in the region table entry */
  #define _REGION_ENTRY_ORIGIN	~0xfffUL/* region/segment table origin	    */
  #define _REGION_ENTRY_PROTECT	0x200	/* region protection bit	    */
++<<<<<<< HEAD
++=======
+ #define _REGION_ENTRY_NOEXEC	0x100	/* region no-execute bit	    */
+ #define _REGION_ENTRY_OFFSET	0xc0	/* region table offset		    */
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  #define _REGION_ENTRY_INVALID	0x20	/* invalid region table entry	    */
  #define _REGION_ENTRY_TYPE_MASK	0x0c	/* region/segment table type mask   */
  #define _REGION_ENTRY_TYPE_R1	0x0c	/* region first table type	    */
@@@ -414,10 -387,14 +424,19 @@@
  /*
   * Page protection definitions.
   */
++<<<<<<< HEAD
 +#define PAGE_NONE	__pgprot(_PAGE_PRESENT | _PAGE_INVALID)
 +#define PAGE_READ	__pgprot(_PAGE_PRESENT | _PAGE_READ | \
++=======
+ #define PAGE_NONE	__pgprot(_PAGE_PRESENT | _PAGE_INVALID | _PAGE_PROTECT)
+ #define PAGE_RO		__pgprot(_PAGE_PRESENT | _PAGE_READ | \
+ 				 _PAGE_NOEXEC  | _PAGE_INVALID | _PAGE_PROTECT)
+ #define PAGE_RX		__pgprot(_PAGE_PRESENT | _PAGE_READ | \
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  				 _PAGE_INVALID | _PAGE_PROTECT)
- #define PAGE_WRITE	__pgprot(_PAGE_PRESENT | _PAGE_READ | _PAGE_WRITE | \
+ #define PAGE_RW		__pgprot(_PAGE_PRESENT | _PAGE_READ | _PAGE_WRITE | \
+ 				 _PAGE_NOEXEC  | _PAGE_INVALID | _PAGE_PROTECT)
+ #define PAGE_RWX	__pgprot(_PAGE_PRESENT | _PAGE_READ | _PAGE_WRITE | \
  				 _PAGE_INVALID | _PAGE_PROTECT)
  
  #define PAGE_SHARED	__pgprot(_PAGE_PRESENT | _PAGE_READ | _PAGE_WRITE | \
@@@ -455,16 -434,48 +476,60 @@@
   * Segment entry (large page) protection definitions.
   */
  #define SEGMENT_NONE	__pgprot(_SEGMENT_ENTRY_INVALID | \
 +				 _SEGMENT_ENTRY_NONE)
 +#define SEGMENT_READ	__pgprot(_SEGMENT_ENTRY_INVALID | \
  				 _SEGMENT_ENTRY_PROTECT)
++<<<<<<< HEAD
 +#define SEGMENT_WRITE	__pgprot(_SEGMENT_ENTRY_INVALID)
 +
 +static inline int mm_exclusive(struct mm_struct *mm)
 +{
 +	return likely(mm == current->active_mm &&
 +		      atomic_read(&mm->context.attach_count) <= 1);
 +}
++=======
+ #define SEGMENT_RO	__pgprot(_SEGMENT_ENTRY_PROTECT | \
+ 				 _SEGMENT_ENTRY_READ | \
+ 				 _SEGMENT_ENTRY_NOEXEC)
+ #define SEGMENT_RX	__pgprot(_SEGMENT_ENTRY_PROTECT | \
+ 				 _SEGMENT_ENTRY_READ)
+ #define SEGMENT_RW	__pgprot(_SEGMENT_ENTRY_READ | \
+ 				 _SEGMENT_ENTRY_WRITE | \
+ 				 _SEGMENT_ENTRY_NOEXEC)
+ #define SEGMENT_RWX	__pgprot(_SEGMENT_ENTRY_READ | \
+ 				 _SEGMENT_ENTRY_WRITE)
+ #define SEGMENT_KERNEL	__pgprot(_SEGMENT_ENTRY |	\
+ 				 _SEGMENT_ENTRY_LARGE |	\
+ 				 _SEGMENT_ENTRY_READ |	\
+ 				 _SEGMENT_ENTRY_WRITE | \
+ 				 _SEGMENT_ENTRY_YOUNG | \
+ 				 _SEGMENT_ENTRY_DIRTY | \
+ 				 _SEGMENT_ENTRY_NOEXEC)
+ #define SEGMENT_KERNEL_RO __pgprot(_SEGMENT_ENTRY |	\
+ 				 _SEGMENT_ENTRY_LARGE |	\
+ 				 _SEGMENT_ENTRY_READ |	\
+ 				 _SEGMENT_ENTRY_YOUNG |	\
+ 				 _SEGMENT_ENTRY_PROTECT | \
+ 				 _SEGMENT_ENTRY_NOEXEC)
+ 
+ /*
+  * Region3 entry (large page) protection definitions.
+  */
+ 
+ #define REGION3_KERNEL	__pgprot(_REGION_ENTRY_TYPE_R3 | \
+ 				 _REGION3_ENTRY_LARGE |	 \
+ 				 _REGION3_ENTRY_READ |	 \
+ 				 _REGION3_ENTRY_WRITE |	 \
+ 				 _REGION3_ENTRY_YOUNG |	 \
+ 				 _REGION3_ENTRY_DIRTY | \
+ 				 _REGION_ENTRY_NOEXEC)
+ #define REGION3_KERNEL_RO __pgprot(_REGION_ENTRY_TYPE_R3 | \
+ 				   _REGION3_ENTRY_LARGE |  \
+ 				   _REGION3_ENTRY_READ |   \
+ 				   _REGION3_ENTRY_YOUNG |  \
+ 				   _REGION_ENTRY_PROTECT | \
+ 				   _REGION_ENTRY_NOEXEC)
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  
  static inline int mm_has_pgste(struct mm_struct *mm)
  {
@@@ -1260,26 -1004,56 +1325,52 @@@ static inline pte_t ptep_set_wrprotect(
  
  #define __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS
  static inline int ptep_set_access_flags(struct vm_area_struct *vma,
 -					unsigned long addr, pte_t *ptep,
 +					unsigned long address, pte_t *ptep,
  					pte_t entry, int dirty)
  {
 +	pgste_t pgste;
 +
  	if (pte_same(*ptep, entry))
  		return 0;
 -	ptep_xchg_direct(vma->vm_mm, addr, ptep, entry);
 -	return 1;
 -}
 +	if (mm_has_pgste(vma->vm_mm)) {
 +		pgste = pgste_get_lock(ptep);
 +		pgste = pgste_ipte_notify(vma->vm_mm, address, ptep, pgste);
 +	}
  
 -/*
 - * Additional functions to handle KVM guest page tables
 - */
 -void ptep_set_pte_at(struct mm_struct *mm, unsigned long addr,
 -		     pte_t *ptep, pte_t entry);
 -void ptep_set_notify(struct mm_struct *mm, unsigned long addr, pte_t *ptep);
 -void ptep_notify(struct mm_struct *mm, unsigned long addr,
 -		 pte_t *ptep, unsigned long bits);
 -int ptep_force_prot(struct mm_struct *mm, unsigned long gaddr,
 -		    pte_t *ptep, int prot, unsigned long bit);
 -void ptep_zap_unused(struct mm_struct *mm, unsigned long addr,
 -		     pte_t *ptep , int reset);
 -void ptep_zap_key(struct mm_struct *mm, unsigned long addr, pte_t *ptep);
 -int ptep_shadow_pte(struct mm_struct *mm, unsigned long saddr,
 -		    pte_t *sptep, pte_t *tptep, pte_t pte);
 -void ptep_unshadow_pte(struct mm_struct *mm, unsigned long saddr, pte_t *ptep);
 +	__ptep_ipte(address, ptep);
  
++<<<<<<< HEAD
 +	if (mm_has_pgste(vma->vm_mm)) {
 +		pgste_set_pte(ptep, entry);
 +		pgste_set_unlock(ptep, pgste);
 +	} else
++=======
+ bool test_and_clear_guest_dirty(struct mm_struct *mm, unsigned long address);
+ int set_guest_storage_key(struct mm_struct *mm, unsigned long addr,
+ 			  unsigned char key, bool nq);
+ int cond_set_guest_storage_key(struct mm_struct *mm, unsigned long addr,
+ 			       unsigned char key, unsigned char *oldkey,
+ 			       bool nq, bool mr, bool mc);
+ int reset_guest_reference_bit(struct mm_struct *mm, unsigned long addr);
+ int get_guest_storage_key(struct mm_struct *mm, unsigned long addr,
+ 			  unsigned char *key);
+ 
+ /*
+  * Certain architectures need to do special things when PTEs
+  * within a page table are directly modified.  Thus, the following
+  * hook is made available.
+  */
+ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
+ 			      pte_t *ptep, pte_t entry)
+ {
+ 	if (!MACHINE_HAS_NX)
+ 		pte_val(entry) &= ~_PAGE_NOEXEC;
+ 	if (mm_has_pgste(mm))
+ 		ptep_set_pte_at(mm, addr, ptep, entry);
+ 	else
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  		*ptep = entry;
 +	return 1;
  }
  
  /*
@@@ -1497,8 -1340,8 +1592,13 @@@ static inline int pmd_trans_splitting(p
  static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
  			      pmd_t *pmdp, pmd_t entry)
  {
++<<<<<<< HEAD
 +	if (!(pmd_val(entry) & _SEGMENT_ENTRY_INVALID) && MACHINE_HAS_EDAT1)
 +		pmd_val(entry) |= _SEGMENT_ENTRY_CO;
++=======
+ 	if (!MACHINE_HAS_NX)
+ 		pmd_val(entry) &= ~_SEGMENT_ENTRY_NOEXEC;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  	*pmdp = entry;
  }
  
diff --cc arch/s390/include/asm/setup.h
index d2f2186ab488,040a4b49ab42..000000000000
--- a/arch/s390/include/asm/setup.h
+++ b/arch/s390/include/asm/setup.h
@@@ -9,7 -10,27 +9,31 @@@
  
  
  #define PARMAREA		0x10400
++<<<<<<< HEAD
 +#define MEMORY_CHUNKS		256
++=======
+ 
+ /*
+  * Machine features detected in early.c
+  */
+ 
+ #define MACHINE_FLAG_VM		_BITUL(0)
+ #define MACHINE_FLAG_KVM	_BITUL(1)
+ #define MACHINE_FLAG_LPAR	_BITUL(2)
+ #define MACHINE_FLAG_DIAG9C	_BITUL(3)
+ #define MACHINE_FLAG_ESOP	_BITUL(4)
+ #define MACHINE_FLAG_IDTE	_BITUL(5)
+ #define MACHINE_FLAG_DIAG44	_BITUL(6)
+ #define MACHINE_FLAG_EDAT1	_BITUL(7)
+ #define MACHINE_FLAG_EDAT2	_BITUL(8)
+ #define MACHINE_FLAG_LPP	_BITUL(9)
+ #define MACHINE_FLAG_TOPOLOGY	_BITUL(10)
+ #define MACHINE_FLAG_TE		_BITUL(11)
+ #define MACHINE_FLAG_TLB_LC	_BITUL(12)
+ #define MACHINE_FLAG_VX		_BITUL(13)
+ #define MACHINE_FLAG_CAD	_BITUL(14)
+ #define MACHINE_FLAG_NX		_BITUL(15)
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  
  #define LPP_MAGIC		_BITUL(31)
  #define LPP_PFAULT_PID_MASK	_AC(0xffffffff, UL)
@@@ -106,9 -69,10 +130,14 @@@ void create_mem_hole(struct mem_chunk m
  #define MACHINE_HAS_LPP		(S390_lowcore.machine_flags & MACHINE_FLAG_LPP)
  #define MACHINE_HAS_TOPOLOGY	(S390_lowcore.machine_flags & MACHINE_FLAG_TOPOLOGY)
  #define MACHINE_HAS_TE		(S390_lowcore.machine_flags & MACHINE_FLAG_TE)
 -#define MACHINE_HAS_TLB_LC	(S390_lowcore.machine_flags & MACHINE_FLAG_TLB_LC)
 +#define MACHINE_HAS_RRBM	(S390_lowcore.machine_flags & MACHINE_FLAG_RRBM)
  #define MACHINE_HAS_VX		(S390_lowcore.machine_flags & MACHINE_FLAG_VX)
++<<<<<<< HEAD
 +#endif /* CONFIG_64BIT */
++=======
+ #define MACHINE_HAS_CAD		(S390_lowcore.machine_flags & MACHINE_FLAG_CAD)
+ #define MACHINE_HAS_NX		(S390_lowcore.machine_flags & MACHINE_FLAG_NX)
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  
  /*
   * Console mode. Override with conmode=
diff --cc arch/s390/kernel/early.c
index 205828171666,4e65c79cc5f2..000000000000
--- a/arch/s390/kernel/early.c
+++ b/arch/s390/kernel/early.c
@@@ -384,14 -348,124 +384,133 @@@ static __init void detect_machine_facil
  		S390_lowcore.machine_flags |= MACHINE_FLAG_LPP;
  	if (test_facility(50) && test_facility(73))
  		S390_lowcore.machine_flags |= MACHINE_FLAG_TE;
++<<<<<<< HEAD
 +	if (test_facility(66))
 +		S390_lowcore.machine_flags |= MACHINE_FLAG_RRBM;
++=======
+ 	if (test_facility(51))
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_TLB_LC;
+ 	if (test_facility(129)) {
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_VX;
+ 		__ctl_set_bit(0, 17);
+ 	}
+ 	if (test_facility(130)) {
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_NX;
+ 		__ctl_set_bit(0, 20);
+ 	}
+ }
+ 
+ static inline void save_vector_registers(void)
+ {
+ #ifdef CONFIG_CRASH_DUMP
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  	if (test_facility(129))
 -		save_vx_regs(boot_cpu_vector_save_area);
 +		S390_lowcore.machine_flags |= MACHINE_FLAG_VX;
  #endif
  }
  
++<<<<<<< HEAD
 +static __init void rescue_initrd(void)
++=======
+ static int __init topology_setup(char *str)
+ {
+ 	bool enabled;
+ 	int rc;
+ 
+ 	rc = kstrtobool(str, &enabled);
+ 	if (!rc && !enabled)
+ 		S390_lowcore.machine_flags &= ~MACHINE_HAS_TOPOLOGY;
+ 	return rc;
+ }
+ early_param("topology", topology_setup);
+ 
+ static int __init disable_vector_extension(char *str)
+ {
+ 	S390_lowcore.machine_flags &= ~MACHINE_FLAG_VX;
+ 	__ctl_clear_bit(0, 17);
+ 	return 1;
+ }
+ early_param("novx", disable_vector_extension);
+ 
+ static int __init noexec_setup(char *str)
+ {
+ 	bool enabled;
+ 	int rc;
+ 
+ 	rc = kstrtobool(str, &enabled);
+ 	if (!rc && !enabled) {
+ 		/* Disable no-execute support */
+ 		S390_lowcore.machine_flags &= ~MACHINE_FLAG_NX;
+ 		__ctl_clear_bit(0, 20);
+ 	}
+ 	return rc;
+ }
+ early_param("noexec", noexec_setup);
+ 
+ static int __init cad_setup(char *str)
+ {
+ 	int val;
+ 
+ 	get_option(&str, &val);
+ 	if (val && test_facility(128))
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_CAD;
+ 	return 0;
+ }
+ early_param("cad", cad_setup);
+ 
+ static int __init cad_init(void)
+ {
+ 	if (MACHINE_HAS_CAD)
+ 		/* Enable problem state CAD. */
+ 		__ctl_set_bit(2, 3);
+ 	return 0;
+ }
+ early_initcall(cad_init);
+ 
+ static __init void memmove_early(void *dst, const void *src, size_t n)
+ {
+ 	unsigned long addr;
+ 	long incr;
+ 	psw_t old;
+ 
+ 	if (!n)
+ 		return;
+ 	incr = 1;
+ 	if (dst > src) {
+ 		incr = -incr;
+ 		dst += n - 1;
+ 		src += n - 1;
+ 	}
+ 	old = S390_lowcore.program_new_psw;
+ 	S390_lowcore.program_new_psw.mask = __extract_psw();
+ 	asm volatile(
+ 		"	larl	%[addr],1f\n"
+ 		"	stg	%[addr],%[psw_pgm_addr]\n"
+ 		"0:     mvc	0(1,%[dst]),0(%[src])\n"
+ 		"	agr	%[dst],%[incr]\n"
+ 		"	agr	%[src],%[incr]\n"
+ 		"	brctg	%[n],0b\n"
+ 		"1:\n"
+ 		: [addr] "=&d" (addr),
+ 		  [psw_pgm_addr] "=Q" (S390_lowcore.program_new_psw.addr),
+ 		  [dst] "+&a" (dst), [src] "+&a" (src),  [n] "+d" (n)
+ 		: [incr] "d" (incr)
+ 		: "cc", "memory");
+ 	S390_lowcore.program_new_psw = old;
+ }
+ 
+ static __init noinline void ipl_save_parameters(void)
+ {
+ 	void *src, *dst;
+ 
+ 	src = (void *)(unsigned long) S390_lowcore.ipl_parmblock_ptr;
+ 	dst = (void *) IPL_PARMBLOCK_ORIGIN;
+ 	memmove_early(dst, src, PAGE_SIZE);
+ 	S390_lowcore.ipl_parmblock_ptr = IPL_PARMBLOCK_ORIGIN;
+ }
+ 
+ static __init noinline void rescue_initrd(void)
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  {
  #ifdef CONFIG_BLK_DEV_INITRD
  	unsigned long min_initrd_addr = (unsigned long) _end + (4UL << 20);
diff --cc arch/s390/kernel/entry.S
index 9fd2c1f12630,34ab7e8d6a76..000000000000
--- a/arch/s390/kernel/entry.S
+++ b/arch/s390/kernel/entry.S
@@@ -268,20 -375,16 +268,33 @@@ sysc_uaccess
  #
  # _TIF_SIGPENDING is set, call do_signal
  #
++<<<<<<< HEAD
 +sysc_sigpending:
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	l	%r1,BASED(.Ldo_signal)
 +	basr	%r14,%r1		# call do_signal
 +	tm	__TI_flags+3(%r12),_TIF_SYSCALL
 +	jno	sysc_return
 +	lm	%r2,%r7,__PT_R2(%r11)	# load svc arguments
 +	l	%r10,__TI_sysc_table(%r12)	# 31 bit system call table
 +	xr	%r8,%r8			# svc 0 returns -ENOSYS
 +	clc	__PT_INT_CODE+2(2,%r11),BASED(.Lnr_syscalls+2)
 +	jnl	sysc_nr_ok		# invalid svc number -> do svc 0
 +	lh	%r8,__PT_INT_CODE+2(%r11)	# load new svc number
 +	sla	%r8,2
 +	j	sysc_nr_ok		# restart svc
++=======
+ .Lsysc_sigpending:
+ 	lgr	%r2,%r11		# pass pointer to pt_regs
+ 	brasl	%r14,do_signal
+ 	TSTMSK	__PT_FLAGS(%r11),_PIF_SYSCALL
+ 	jno	.Lsysc_return
+ .Lsysc_do_syscall:
+ 	lghi	%r13,__TASK_thread
+ 	lmg	%r2,%r7,__PT_R2(%r11)	# load svc arguments
+ 	lghi	%r1,0			# svc 0 returns -ENOSYS
+ 	j	.Lsysc_do_svc
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  
  #
  # _TIF_NOTIFY_RESUME is set, call do_notify_resume
@@@ -359,47 -465,70 +372,57 @@@ ENTRY(kernel_thread_starter
  
  ENTRY(pgm_check_handler)
  	stpt	__LC_SYNC_ENTER_TIMER
 -	stmg	%r8,%r15,__LC_SAVE_AREA_SYNC
 -	lg	%r10,__LC_LAST_BREAK
 -	lg	%r12,__LC_CURRENT
 -	larl	%r13,cleanup_critical
 -	lmg	%r8,%r9,__LC_PGM_OLD_PSW
 -	tmhh	%r8,0x0001		# test problem state bit
 -	jnz	2f			# -> fault in user space
 -#if IS_ENABLED(CONFIG_KVM)
 -	# cleanup critical section for sie64a
 -	lgr	%r14,%r9
 -	slg	%r14,BASED(.Lsie_critical_start)
 -	clg	%r14,BASED(.Lsie_critical_length)
 -	jhe	0f
 -	brasl	%r14,.Lcleanup_sie
 -#endif
 -0:	tmhh	%r8,0x4000		# PER bit set in old PSW ?
 -	jnz	1f			# -> enabled, can't be a double fault
 +	stm	%r8,%r15,__LC_SAVE_AREA_SYNC
 +	l	%r12,__LC_THREAD_INFO
 +	l	%r13,__LC_SVC_NEW_PSW+4
 +	lm	%r8,%r9,__LC_PGM_OLD_PSW
 +	tmh	%r8,0x0001		# test problem state bit
 +	jnz	1f			# -> fault in user space
 +	tmh	%r8,0x4000		# PER bit set in old PSW ?
 +	jnz	0f			# -> enabled, can't be a double fault
  	tm	__LC_PGM_ILC+3,0x80	# check for per exception
 -	jnz	.Lpgm_svcper		# -> single stepped svc
 -1:	CHECK_STACK STACK_SIZE,__LC_SAVE_AREA_SYNC
 -	aghi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
 -	j	3f
 -2:	UPDATE_VTIME %r14,%r15,__LC_SYNC_ENTER_TIMER
 -	lg	%r15,__LC_KERNEL_STACK
 -	lgr	%r14,%r12
 -	aghi	%r14,__TASK_thread	# pointer to thread_struct
 -	lghi	%r13,__LC_PGM_TDB
 -	tm	__LC_PGM_ILC+2,0x02	# check for transaction abort
 -	jz	3f
 -	mvc	__THREAD_trap_tdb(256,%r14),0(%r13)
 -3:	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 -	stg	%r10,__THREAD_last_break(%r14)
 -	stmg	%r0,%r7,__PT_R0(%r11)
 -	mvc	__PT_R8(64,%r11),__LC_SAVE_AREA_SYNC
 -	stmg	%r8,%r9,__PT_PSW(%r11)
 +	jnz	pgm_svcper		# -> single stepped svc
 +0:	CHECK_STACK STACK_SIZE,__LC_SAVE_AREA_SYNC
 +	ahi	%r15,-(STACK_FRAME_OVERHEAD + __PT_SIZE)
 +	j	2f
 +1:	UPDATE_VTIME %r14,%r15,__LC_SYNC_ENTER_TIMER
 +	l	%r15,__LC_KERNEL_STACK
 +2:	la	%r11,STACK_FRAME_OVERHEAD(%r15)
 +	stm	%r0,%r7,__PT_R0(%r11)
 +	mvc	__PT_R8(32,%r11),__LC_SAVE_AREA_SYNC
 +	stm	%r8,%r9,__PT_PSW(%r11)
  	mvc	__PT_INT_CODE(4,%r11),__LC_PGM_ILC
 -	mvc	__PT_INT_PARM_LONG(8,%r11),__LC_TRANS_EXC_CODE
 -	xc	__PT_FLAGS(8,%r11),__PT_FLAGS(%r11)
 -	stg	%r10,__PT_ARGS(%r11)
 +	mvc	__PT_INT_PARM_LONG(4,%r11),__LC_TRANS_EXC_CODE
  	tm	__LC_PGM_ILC+3,0x80	# check for per exception
 -	jz	4f
 -	tmhh	%r8,0x0001		# kernel per event ?
 -	jz	.Lpgm_kprobe
 -	oi	__PT_FLAGS+7(%r11),_PIF_PER_TRAP
 -	mvc	__THREAD_per_address(8,%r14),__LC_PER_ADDRESS
 -	mvc	__THREAD_per_cause(2,%r14),__LC_PER_CODE
 -	mvc	__THREAD_per_paid(1,%r14),__LC_PER_ACCESS_ID
 -4:	REENABLE_IRQS
 -	xc	__SF_BACKCHAIN(8,%r15),__SF_BACKCHAIN(%r15)
 -	larl	%r1,pgm_check_table
 -	llgh	%r10,__PT_INT_CODE+2(%r11)
 -	nill	%r10,0x007f
 +	jz	0f
 +	l	%r1,__TI_task(%r12)
 +	tmh	%r8,0x0001		# kernel per event ?
 +	jz	pgm_kprobe
 +	oi	__TI_flags+3(%r12),_TIF_PER_TRAP
 +	mvc	__THREAD_per_address(4,%r1),__LC_PER_ADDRESS
 +	mvc	__THREAD_per_cause(2,%r1),__LC_PER_CAUSE
 +	mvc	__THREAD_per_paid(1,%r1),__LC_PER_PAID
 +0:	REENABLE_IRQS
 +	xc	__SF_BACKCHAIN(4,%r15),__SF_BACKCHAIN(%r15)
 +	l	%r1,BASED(.Ljump_table)
 +	la	%r10,0x7f
 +	n	%r10,__PT_INT_CODE(%r11)
 +	je	sysc_return
  	sll	%r10,2
 -	je	.Lpgm_return
 -	lgf	%r1,0(%r10,%r1)		# load address of handler routine
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 +	l	%r1,0(%r10,%r1)		# load address of handler routine
 +	lr	%r2,%r11		# pass pointer to pt_regs
  	basr	%r14,%r1		# branch to interrupt-handler
++<<<<<<< HEAD
 +	j	sysc_return
++=======
+ .Lpgm_return:
+ 	LOCKDEP_SYS_EXIT
+ 	tm	__PT_PSW+1(%r11),0x01	# returning to user ?
+ 	jno	.Lsysc_restore
+ 	TSTMSK	__PT_FLAGS(%r11),_PIF_SYSCALL
+ 	jo	.Lsysc_do_syscall
+ 	j	.Lsysc_tif
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  
  #
  # PER event in supervisor state, must be kprobes
diff --cc arch/s390/kernel/module.c
index b89b59158b95,1a27f307a920..000000000000
--- a/arch/s390/kernel/module.c
+++ b/arch/s390/kernel/module.c
@@@ -50,19 -45,19 +50,24 @@@ void *module_alloc(unsigned long size
  	if (PAGE_ALIGN(size) > MODULES_LEN)
  		return NULL;
  	return __vmalloc_node_range(size, 1, MODULES_VADDR, MODULES_END,
++<<<<<<< HEAD
 +				    GFP_KERNEL, PAGE_KERNEL, NUMA_NO_NODE,
++=======
+ 				    GFP_KERNEL, PAGE_KERNEL_EXEC,
+ 				    0, NUMA_NO_NODE,
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  				    __builtin_return_address(0));
  }
 +#endif
  
 -void module_arch_freeing_init(struct module *mod)
 +/* Free memory returned from module_alloc */
 +void module_free(struct module *mod, void *module_region)
  {
 -	if (is_livepatch_module(mod) &&
 -	    mod->state == MODULE_STATE_LIVE)
 -		return;
 -
 -	vfree(mod->arch.syminfo);
 -	mod->arch.syminfo = NULL;
 +	if (mod) {
 +		vfree(mod->arch.syminfo);
 +		mod->arch.syminfo = NULL;
 +	}
 +	vfree(module_region);
  }
  
  static void check_rela(Elf_Rela *rela, struct module *me)
diff --cc arch/s390/mm/dump_pagetables.c
index 46d517c3c763,5a46b1d7e578..000000000000
--- a/arch/s390/mm/dump_pagetables.c
+++ b/arch/s390/mm/dump_pagetables.c
@@@ -53,9 -49,8 +53,14 @@@ static void print_prot(struct seq_file 
  		seq_printf(m, "I\n");
  		return;
  	}
++<<<<<<< HEAD
 +	seq_printf(m, "%s", pr & _PAGE_PROTECT ? "RO " : "RW ");
 +	seq_printf(m, "%s", pr & _PAGE_CO ? "CO " : "   ");
 +	seq_putc(m, '\n');
++=======
+ 	seq_puts(m, (pr & _PAGE_PROTECT) ? "RO " : "RW ");
+ 	seq_puts(m, (pr & _PAGE_NOEXEC) ? "NX\n" : "X\n");
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  }
  
  static void note_page(struct seq_file *m, struct pg_state *st,
@@@ -146,7 -136,9 +152,13 @@@ static void walk_pmd_level(struct seq_f
  		pmd = pmd_offset(pud, addr);
  		if (!pmd_none(*pmd)) {
  			if (pmd_large(*pmd)) {
++<<<<<<< HEAD
 +				prot = pmd_val(*pmd) & _PMD_PROT_MASK;
++=======
+ 				prot = pmd_val(*pmd) &
+ 					(_SEGMENT_ENTRY_PROTECT |
+ 					 _SEGMENT_ENTRY_NOEXEC);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  				note_page(m, st, prot, 3);
  			} else
  				walk_pte_level(m, st, pmd, addr);
@@@ -174,7 -160,9 +186,13 @@@ static void walk_pud_level(struct seq_f
  		pud = pud_offset(pgd, addr);
  		if (!pud_none(*pud))
  			if (pud_large(*pud)) {
++<<<<<<< HEAD
 +				prot = pud_val(*pud) & _PUD_PROT_MASK;
++=======
+ 				prot = pud_val(*pud) &
+ 					(_REGION_ENTRY_PROTECT |
+ 					 _REGION_ENTRY_NOEXEC);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  				note_page(m, st, prot, 2);
  			} else
  				walk_pmd_level(m, st, pud, addr);
diff --cc arch/s390/mm/hugetlbpage.c
index 7b7d88b8af33,a03816227719..000000000000
--- a/arch/s390/mm/hugetlbpage.c
+++ b/arch/s390/mm/hugetlbpage.c
@@@ -34,18 -43,24 +34,39 @@@ static inline unsigned long __pte_to_rs
  	 */
  	if (pte_present(pte)) {
  		rste = pte_val(pte) & PAGE_MASK;
++<<<<<<< HEAD
 +		if (pte_val(pte) & _PAGE_INVALID)
 +			rste |= _SEGMENT_ENTRY_INVALID;
 +		none = (pte_val(pte) & _PAGE_PRESENT) &&
 +			!(pte_val(pte) & _PAGE_READ) &&
 +			!(pte_val(pte) & _PAGE_WRITE);
 +		prot = (pte_val(pte) & _PAGE_PROTECT) &&
 +			!(pte_val(pte) & _PAGE_WRITE);
 +		young = pte_val(pte) & _PAGE_YOUNG;
 +		if (none || young)
 +			rste |= _SEGMENT_ENTRY_YOUNG;
 +		if (prot || (none && young))
 +			rste |= _SEGMENT_ENTRY_PROTECT;
++=======
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_READ,
+ 				     _SEGMENT_ENTRY_READ);
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_WRITE,
+ 				     _SEGMENT_ENTRY_WRITE);
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_INVALID,
+ 				     _SEGMENT_ENTRY_INVALID);
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_PROTECT,
+ 				     _SEGMENT_ENTRY_PROTECT);
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_DIRTY,
+ 				     _SEGMENT_ENTRY_DIRTY);
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_YOUNG,
+ 				     _SEGMENT_ENTRY_YOUNG);
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_SOFT_DIRTY,
+ 				     _SEGMENT_ENTRY_SOFT_DIRTY);
+ #endif
+ 		rste |= move_set_bit(pte_val(pte), _PAGE_NOEXEC,
+ 				     _SEGMENT_ENTRY_NOEXEC);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  	} else
  		rste = _SEGMENT_ENTRY_INVALID;
  	return rste;
@@@ -62,34 -77,46 +83,57 @@@ static inline pte_t __rste_to_pte(unsig
  		present = pmd_present(__pmd(rste));
  
  	/*
 -	 * Convert encoding		pmd / pud bits	    pte bits
 -	 *				dy..R...I...wr	  lIR.uswrdy.p
 -	 * empty			00..0...1...00 -> 010.000000.0
 -	 * prot-none, clean, old	00..1...1...00 -> 111.000000.1
 -	 * prot-none, clean, young	01..1...1...00 -> 111.000001.1
 -	 * prot-none, dirty, old	10..1...1...00 -> 111.000010.1
 -	 * prot-none, dirty, young	11..1...1...00 -> 111.000011.1
 -	 * read-only, clean, old	00..1...1...01 -> 111.000100.1
 -	 * read-only, clean, young	01..1...0...01 -> 101.000101.1
 -	 * read-only, dirty, old	10..1...1...01 -> 111.000110.1
 -	 * read-only, dirty, young	11..1...0...01 -> 101.000111.1
 -	 * read-write, clean, old	00..1...1...11 -> 111.001100.1
 -	 * read-write, clean, young	01..1...0...11 -> 101.001101.1
 -	 * read-write, dirty, old	10..0...1...11 -> 110.001110.1
 -	 * read-write, dirty, young	11..0...0...11 -> 100.001111.1
 -	 * HW-bits: R read-only, I invalid
 -	 * SW-bits: p present, y young, d dirty, r read, w write, s special,
 -	 *	    u unused, l large
 +	 * Convert encoding	pmd/pud bits	  pte bits
 +	 *			..R...I...y.	.IR...wrdytp
 +	 * empty		..0...1...0. -> .10...000000
 +	 * prot-none, old	..0...1...1. -> .10...001001
 +	 * prot-none, young	..1...1...1. -> .10...001101
 +	 * read-only, old	..1...1...0. -> .11...011001
 +	 * read-only, young	..1...0...1. -> .01...011101
 +	 * read-write, old	..0...1...0. -> .10...111001
 +	 * read-write, young	..0...0...1. -> .00...111101
 +	 * Huge ptes are dirty by definition
  	 */
  	if (present) {
++<<<<<<< HEAD
 +		pte_val(pte) = _PAGE_PRESENT | _PAGE_LARGE | _PAGE_DIRTY |
 +			       (rste & PAGE_MASK);
 +		if (rste & _SEGMENT_ENTRY_INVALID)
 +			pte_val(pte) |= _PAGE_INVALID;
 +		if (pmd_prot_none(__pmd(rste))) {
 +			if (rste & _SEGMENT_ENTRY_PROTECT)
 +				pte_val(pte) |= _PAGE_YOUNG;
 +		} else {
 +			pte_val(pte) |= _PAGE_READ;
 +			if (rste & _SEGMENT_ENTRY_PROTECT)
 +				pte_val(pte) |= _PAGE_PROTECT;
 +			else
 +				pte_val(pte) |= _PAGE_WRITE;
 +			if (rste & _SEGMENT_ENTRY_YOUNG)
 +				pte_val(pte) |= _PAGE_YOUNG;
 +		}
++=======
+ 		pte_val(pte) = rste & _SEGMENT_ENTRY_ORIGIN_LARGE;
+ 		pte_val(pte) |= _PAGE_LARGE | _PAGE_PRESENT;
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_READ,
+ 					     _PAGE_READ);
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_WRITE,
+ 					     _PAGE_WRITE);
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_INVALID,
+ 					     _PAGE_INVALID);
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_PROTECT,
+ 					     _PAGE_PROTECT);
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_DIRTY,
+ 					     _PAGE_DIRTY);
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_YOUNG,
+ 					     _PAGE_YOUNG);
+ #ifdef CONFIG_MEM_SOFT_DIRTY
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_SOFT_DIRTY,
+ 					     _PAGE_DIRTY);
+ #endif
+ 		pte_val(pte) |= move_set_bit(rste, _SEGMENT_ENTRY_NOEXEC,
+ 					     _PAGE_NOEXEC);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  	} else
  		pte_val(pte) = _PAGE_INVALID;
  	return pte;
diff --cc arch/s390/mm/init.c
index 4386d239610f,ba0c8d18e10d..000000000000
--- a/arch/s390/mm/init.c
+++ b/arch/s390/mm/init.c
@@@ -172,7 -137,10 +172,14 @@@ void __init mem_init(void
  
  void free_initmem(void)
  {
++<<<<<<< HEAD
 +	free_initmem_default(0);
++=======
+ 	__set_memory((unsigned long) _sinittext,
+ 		     (_einittext - _sinittext) >> PAGE_SHIFT,
+ 		     SET_MEMORY_RW | SET_MEMORY_NX);
+ 	free_initmem_default(POISON_FREE_INITMEM);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  }
  
  #ifdef CONFIG_BLK_DEV_INITRD
diff --cc arch/s390/mm/pageattr.c
index 954cfcf38d1b,a1543b74ee00..000000000000
--- a/arch/s390/mm/pageattr.c
+++ b/arch/s390/mm/pageattr.c
@@@ -37,73 -39,281 +37,313 @@@ void __storage_key_init_range(unsigned 
  		start += PAGE_SIZE;
  	}
  }
 +#endif
  
 -#ifdef CONFIG_PROC_FS
 -atomic_long_t direct_pages_count[PG_DIRECT_MAP_MAX];
 -
 -void arch_report_meminfo(struct seq_file *m)
 +static pte_t *walk_page_table(unsigned long addr)
  {
++<<<<<<< HEAD
++=======
+ 	seq_printf(m, "DirectMap4k:    %8lu kB\n",
+ 		   atomic_long_read(&direct_pages_count[PG_DIRECT_MAP_4K]) << 2);
+ 	seq_printf(m, "DirectMap1M:    %8lu kB\n",
+ 		   atomic_long_read(&direct_pages_count[PG_DIRECT_MAP_1M]) << 10);
+ 	seq_printf(m, "DirectMap2G:    %8lu kB\n",
+ 		   atomic_long_read(&direct_pages_count[PG_DIRECT_MAP_2G]) << 21);
+ }
+ #endif /* CONFIG_PROC_FS */
+ 
+ static void pgt_set(unsigned long *old, unsigned long new, unsigned long addr,
+ 		    unsigned long dtt)
+ {
+ 	unsigned long table, mask;
+ 
+ 	mask = 0;
+ 	if (MACHINE_HAS_EDAT2) {
+ 		switch (dtt) {
+ 		case CRDTE_DTT_REGION3:
+ 			mask = ~(PTRS_PER_PUD * sizeof(pud_t) - 1);
+ 			break;
+ 		case CRDTE_DTT_SEGMENT:
+ 			mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);
+ 			break;
+ 		case CRDTE_DTT_PAGE:
+ 			mask = ~(PTRS_PER_PTE * sizeof(pte_t) - 1);
+ 			break;
+ 		}
+ 		table = (unsigned long)old & mask;
+ 		crdte(*old, new, table, dtt, addr, S390_lowcore.kernel_asce);
+ 	} else if (MACHINE_HAS_IDTE) {
+ 		cspg(old, *old, new);
+ 	} else {
+ 		csp((unsigned int *)old + 1, *old, new);
+ 	}
+ }
+ 
+ static int walk_pte_level(pmd_t *pmdp, unsigned long addr, unsigned long end,
+ 			  unsigned long flags)
+ {
+ 	pte_t *ptep, new;
+ 
+ 	ptep = pte_offset(pmdp, addr);
+ 	do {
+ 		new = *ptep;
+ 		if (pte_none(new))
+ 			return -EINVAL;
+ 		if (flags & SET_MEMORY_RO)
+ 			new = pte_wrprotect(new);
+ 		else if (flags & SET_MEMORY_RW)
+ 			new = pte_mkwrite(pte_mkdirty(new));
+ 		if ((flags & SET_MEMORY_NX) && MACHINE_HAS_NX)
+ 			pte_val(new) |= _PAGE_NOEXEC;
+ 		else if (flags & SET_MEMORY_X)
+ 			pte_val(new) &= ~_PAGE_NOEXEC;
+ 		pgt_set((unsigned long *)ptep, pte_val(new), addr, CRDTE_DTT_PAGE);
+ 		ptep++;
+ 		addr += PAGE_SIZE;
+ 		cond_resched();
+ 	} while (addr < end);
+ 	return 0;
+ }
+ 
+ static int split_pmd_page(pmd_t *pmdp, unsigned long addr)
+ {
+ 	unsigned long pte_addr, prot;
+ 	pte_t *pt_dir, *ptep;
+ 	pmd_t new;
+ 	int i, ro, nx;
+ 
+ 	pt_dir = vmem_pte_alloc();
+ 	if (!pt_dir)
+ 		return -ENOMEM;
+ 	pte_addr = pmd_pfn(*pmdp) << PAGE_SHIFT;
+ 	ro = !!(pmd_val(*pmdp) & _SEGMENT_ENTRY_PROTECT);
+ 	nx = !!(pmd_val(*pmdp) & _SEGMENT_ENTRY_NOEXEC);
+ 	prot = pgprot_val(ro ? PAGE_KERNEL_RO : PAGE_KERNEL);
+ 	if (!nx)
+ 		prot &= ~_PAGE_NOEXEC;
+ 	ptep = pt_dir;
+ 	for (i = 0; i < PTRS_PER_PTE; i++) {
+ 		pte_val(*ptep) = pte_addr | prot;
+ 		pte_addr += PAGE_SIZE;
+ 		ptep++;
+ 	}
+ 	pmd_val(new) = __pa(pt_dir) | _SEGMENT_ENTRY;
+ 	pgt_set((unsigned long *)pmdp, pmd_val(new), addr, CRDTE_DTT_SEGMENT);
+ 	update_page_count(PG_DIRECT_MAP_4K, PTRS_PER_PTE);
+ 	update_page_count(PG_DIRECT_MAP_1M, -1);
+ 	return 0;
+ }
+ 
+ static void modify_pmd_page(pmd_t *pmdp, unsigned long addr,
+ 			    unsigned long flags)
+ {
+ 	pmd_t new = *pmdp;
+ 
+ 	if (flags & SET_MEMORY_RO)
+ 		new = pmd_wrprotect(new);
+ 	else if (flags & SET_MEMORY_RW)
+ 		new = pmd_mkwrite(pmd_mkdirty(new));
+ 	if ((flags & SET_MEMORY_NX) && MACHINE_HAS_NX)
+ 		pmd_val(new) |= _SEGMENT_ENTRY_NOEXEC;
+ 	else if (flags & SET_MEMORY_X)
+ 		pmd_val(new) &= ~_SEGMENT_ENTRY_NOEXEC;
+ 	pgt_set((unsigned long *)pmdp, pmd_val(new), addr, CRDTE_DTT_SEGMENT);
+ }
+ 
+ static int walk_pmd_level(pud_t *pudp, unsigned long addr, unsigned long end,
+ 			  unsigned long flags)
+ {
+ 	unsigned long next;
+ 	pmd_t *pmdp;
+ 	int rc = 0;
+ 
+ 	pmdp = pmd_offset(pudp, addr);
+ 	do {
+ 		if (pmd_none(*pmdp))
+ 			return -EINVAL;
+ 		next = pmd_addr_end(addr, end);
+ 		if (pmd_large(*pmdp)) {
+ 			if (addr & ~PMD_MASK || addr + PMD_SIZE > next) {
+ 				rc = split_pmd_page(pmdp, addr);
+ 				if (rc)
+ 					return rc;
+ 				continue;
+ 			}
+ 			modify_pmd_page(pmdp, addr, flags);
+ 		} else {
+ 			rc = walk_pte_level(pmdp, addr, next, flags);
+ 			if (rc)
+ 				return rc;
+ 		}
+ 		pmdp++;
+ 		addr = next;
+ 		cond_resched();
+ 	} while (addr < end);
+ 	return rc;
+ }
+ 
+ static int split_pud_page(pud_t *pudp, unsigned long addr)
+ {
+ 	unsigned long pmd_addr, prot;
+ 	pmd_t *pm_dir, *pmdp;
+ 	pud_t new;
+ 	int i, ro, nx;
+ 
+ 	pm_dir = vmem_pmd_alloc();
+ 	if (!pm_dir)
+ 		return -ENOMEM;
+ 	pmd_addr = pud_pfn(*pudp) << PAGE_SHIFT;
+ 	ro = !!(pud_val(*pudp) & _REGION_ENTRY_PROTECT);
+ 	nx = !!(pud_val(*pudp) & _REGION_ENTRY_NOEXEC);
+ 	prot = pgprot_val(ro ? SEGMENT_KERNEL_RO : SEGMENT_KERNEL);
+ 	if (!nx)
+ 		prot &= ~_SEGMENT_ENTRY_NOEXEC;
+ 	pmdp = pm_dir;
+ 	for (i = 0; i < PTRS_PER_PMD; i++) {
+ 		pmd_val(*pmdp) = pmd_addr | prot;
+ 		pmd_addr += PMD_SIZE;
+ 		pmdp++;
+ 	}
+ 	pud_val(new) = __pa(pm_dir) | _REGION3_ENTRY;
+ 	pgt_set((unsigned long *)pudp, pud_val(new), addr, CRDTE_DTT_REGION3);
+ 	update_page_count(PG_DIRECT_MAP_1M, PTRS_PER_PMD);
+ 	update_page_count(PG_DIRECT_MAP_2G, -1);
+ 	return 0;
+ }
+ 
+ static void modify_pud_page(pud_t *pudp, unsigned long addr,
+ 			    unsigned long flags)
+ {
+ 	pud_t new = *pudp;
+ 
+ 	if (flags & SET_MEMORY_RO)
+ 		new = pud_wrprotect(new);
+ 	else if (flags & SET_MEMORY_RW)
+ 		new = pud_mkwrite(pud_mkdirty(new));
+ 	if ((flags & SET_MEMORY_NX) && MACHINE_HAS_NX)
+ 		pud_val(new) |= _REGION_ENTRY_NOEXEC;
+ 	else if (flags & SET_MEMORY_X)
+ 		pud_val(new) &= ~_REGION_ENTRY_NOEXEC;
+ 	pgt_set((unsigned long *)pudp, pud_val(new), addr, CRDTE_DTT_REGION3);
+ }
+ 
+ static int walk_pud_level(pgd_t *pgd, unsigned long addr, unsigned long end,
+ 			  unsigned long flags)
+ {
+ 	unsigned long next;
+ 	pud_t *pudp;
+ 	int rc = 0;
+ 
+ 	pudp = pud_offset(pgd, addr);
+ 	do {
+ 		if (pud_none(*pudp))
+ 			return -EINVAL;
+ 		next = pud_addr_end(addr, end);
+ 		if (pud_large(*pudp)) {
+ 			if (addr & ~PUD_MASK || addr + PUD_SIZE > next) {
+ 				rc = split_pud_page(pudp, addr);
+ 				if (rc)
+ 					break;
+ 				continue;
+ 			}
+ 			modify_pud_page(pudp, addr, flags);
+ 		} else {
+ 			rc = walk_pmd_level(pudp, addr, next, flags);
+ 		}
+ 		pudp++;
+ 		addr = next;
+ 		cond_resched();
+ 	} while (addr < end && !rc);
+ 	return rc;
+ }
+ 
+ static DEFINE_MUTEX(cpa_mutex);
+ 
+ static int change_page_attr(unsigned long addr, unsigned long end,
+ 			    unsigned long flags)
+ {
+ 	unsigned long next;
+ 	int rc = -EINVAL;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  	pgd_t *pgdp;
 +	pud_t *pudp;
 +	pmd_t *pmdp;
 +	pte_t *ptep;
  
 -	if (addr == end)
 -		return 0;
 -	if (end >= MODULES_END)
 -		return -EINVAL;
 -	mutex_lock(&cpa_mutex);
  	pgdp = pgd_offset_k(addr);
 -	do {
 -		if (pgd_none(*pgdp))
 +	if (pgd_none(*pgdp))
 +		return NULL;
 +	pudp = pud_offset(pgdp, addr);
 +	if (pud_none(*pudp) || pud_large(*pudp))
 +		return NULL;
 +	pmdp = pmd_offset(pudp, addr);
 +	if (pmd_none(*pmdp) || pmd_large(*pmdp))
 +		return NULL;
 +	ptep = pte_offset_kernel(pmdp, addr);
 +	if (pte_none(*ptep))
 +		return NULL;
 +	return ptep;
 +}
 +
 +static void change_page_attr(unsigned long addr, int numpages,
 +			     pte_t (*set) (pte_t))
 +{
 +	pte_t *ptep;
 +	int i;
 +
 +	for (i = 0; i < numpages; i++) {
 +		ptep = walk_page_table(addr);
 +		if (WARN_ON_ONCE(!ptep))
  			break;
++<<<<<<< HEAD
 +		*ptep = set(*ptep);
 +		addr += PAGE_SIZE;
 +	}
 +	__tlb_flush_kernel();
++=======
+ 		next = pgd_addr_end(addr, end);
+ 		rc = walk_pud_level(pgdp, addr, next, flags);
+ 		if (rc)
+ 			break;
+ 		cond_resched();
+ 	} while (pgdp++, addr = next, addr < end && !rc);
+ 	mutex_unlock(&cpa_mutex);
+ 	return rc;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  }
  
- int set_memory_ro(unsigned long addr, int numpages)
+ int __set_memory(unsigned long addr, int numpages, unsigned long flags)
  {
 -	addr &= PAGE_MASK;
 -	return change_page_attr(addr, addr + numpages * PAGE_SIZE, flags);
++<<<<<<< HEAD
 +	change_page_attr(addr, numpages, pte_wrprotect);
 +	return 0;
  }
  
 -#ifdef CONFIG_DEBUG_PAGEALLOC
 +int set_memory_rw(unsigned long addr, int numpages)
 +{
 +	change_page_attr(addr, numpages, pte_mkwrite);
 +	return 0;
 +}
  
 -static void ipte_range(pte_t *pte, unsigned long address, int nr)
 +/* not possible */
 +int set_memory_nx(unsigned long addr, int numpages)
  {
 -	int i;
 +	return 0;
 +}
  
 -	if (test_facility(13)) {
 -		__ptep_ipte_range(address, nr - 1, pte, IPTE_GLOBAL);
 -		return;
 -	}
 -	for (i = 0; i < nr; i++) {
 -		__ptep_ipte(address, pte, IPTE_GLOBAL);
 -		address += PAGE_SIZE;
 -		pte++;
 -	}
 +int set_memory_x(unsigned long addr, int numpages)
 +{
 +	return 0;
++=======
++	addr &= PAGE_MASK;
++	return change_page_attr(addr, addr + numpages * PAGE_SIZE, flags);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  }
  
 -void __kernel_map_pages(struct page *page, int numpages, int enable)
 +#ifdef CONFIG_DEBUG_PAGEALLOC
 +void kernel_map_pages(struct page *page, int numpages, int enable)
  {
  	unsigned long address;
 -	int nr, i, j;
  	pgd_t *pgd;
  	pud_t *pud;
  	pmd_t *pmd;
@@@ -116,12 -325,19 +356,26 @@@
  		pud = pud_offset(pgd, address);
  		pmd = pmd_offset(pud, address);
  		pte = pte_offset_kernel(pmd, address);
++<<<<<<< HEAD
 +		if (!enable) {
 +			__ptep_ipte(address, pte);
 +			pte_val(*pte) = _PAGE_INVALID;
 +			continue;
++=======
+ 		nr = (unsigned long)pte >> ilog2(sizeof(long));
+ 		nr = PTRS_PER_PTE - (nr & (PTRS_PER_PTE - 1));
+ 		nr = min(numpages - i, nr);
+ 		if (enable) {
+ 			for (j = 0; j < nr; j++) {
+ 				pte_val(*pte) &= ~_PAGE_INVALID;
+ 				address += PAGE_SIZE;
+ 				pte++;
+ 			}
+ 		} else {
+ 			ipte_range(pte, address, nr);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  		}
 -		i += nr;
 +		pte_val(*pte) = __pa(address);
  	}
  }
  
diff --cc arch/s390/mm/pgtable.c
index 21b9a3a53e9d,190d0c65904a..000000000000
--- a/arch/s390/mm/pgtable.c
+++ b/arch/s390/mm/pgtable.c
@@@ -24,1236 -24,749 +24,1461 @@@
  #include <asm/tlbflush.h>
  #include <asm/mmu_context.h>
  
 -static inline pte_t ptep_flush_direct(struct mm_struct *mm,
 -				      unsigned long addr, pte_t *ptep)
 +#ifndef CONFIG_64BIT
 +#define ALLOC_ORDER	1
 +#define FRAG_MASK	0x0f
 +#else
 +#define ALLOC_ORDER	2
 +#define FRAG_MASK	0x03
 +#endif
 +
 +
 +unsigned long *crst_table_alloc(struct mm_struct *mm)
  {
 -	pte_t old;
 +	struct page *page = alloc_pages(GFP_KERNEL, ALLOC_ORDER);
  
 -	old = *ptep;
 -	if (unlikely(pte_val(old) & _PAGE_INVALID))
 -		return old;
 -	atomic_inc(&mm->context.flush_count);
 -	if (MACHINE_HAS_TLB_LC &&
 -	    cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))
 -		__ptep_ipte(addr, ptep, IPTE_LOCAL);
 +	if (!page)
 +		return NULL;
 +	return (unsigned long *) page_to_phys(page);
 +}
 +
 +void crst_table_free(struct mm_struct *mm, unsigned long *table)
 +{
 +	free_pages((unsigned long) table, ALLOC_ORDER);
 +}
 +
 +#ifdef CONFIG_64BIT
 +static void __crst_table_upgrade(void *arg)
 +{
 +	struct mm_struct *mm = arg;
 +
 +	if (current->active_mm == mm)
 +		update_mm(mm, current);
 +	__tlb_flush_local();
 +}
 +
 +int crst_table_upgrade(struct mm_struct *mm)
 +{
 +	unsigned long *table, *pgd;
 +
 +	/* upgrade should only happen from 3 to 4 levels */
 +	BUG_ON(mm->context.asce_limit != (1UL << 42));
 +
 +	table = crst_table_alloc(mm);
 +	if (!table)
 +		return -ENOMEM;
 +
 +	spin_lock_bh(&mm->page_table_lock);
 +	pgd = (unsigned long *) mm->pgd;
 +	crst_table_init(table, _REGION2_ENTRY_EMPTY);
 +	pgd_populate(mm, (pgd_t *) table, (pud_t *) pgd);
 +	mm->pgd = (pgd_t *) table;
 +	mm->context.asce_limit = 1UL << 53;
 +	mm->context.asce = __pa(mm->pgd) | _ASCE_TABLE_LENGTH |
 +			   _ASCE_USER_BITS | _ASCE_TYPE_REGION2;
 +	mm->task_size = mm->context.asce_limit;
 +	spin_unlock_bh(&mm->page_table_lock);
 +
 +	on_each_cpu(__crst_table_upgrade, mm, 0);
 +	return 0;
 +}
 +
 +void crst_table_downgrade(struct mm_struct *mm)
 +{
 +	pgd_t *pgd;
 +
 +	/* downgrade should only happen from 3 to 2 levels (compat only) */
 +	BUG_ON(mm->context.asce_limit != (1UL << 42));
 +
 +	if (current->active_mm == mm)
 +		__tlb_flush_mm(mm);
 +
 +	pgd = mm->pgd;
 +	mm->pgd = (pgd_t *) (pgd_val(*pgd) & _REGION_ENTRY_ORIGIN);
 +	mm->context.asce_limit = 1UL << 31;
 +	mm->context.asce = __pa(mm->pgd) | _ASCE_TABLE_LENGTH |
 +			   _ASCE_USER_BITS | _ASCE_TYPE_SEGMENT;
 +	mm->task_size = mm->context.asce_limit;
 +	crst_table_free(mm, (unsigned long *) pgd);
 +
 +	if (current->active_mm == mm)
 +		update_mm(mm, current);
 +}
 +#endif
 +
 +#ifdef CONFIG_PGSTE
 +
 +/**
 + * gmap_alloc - allocate a guest address space
 + * @mm: pointer to the parent mm_struct
 + *
 + * Returns a guest address space structure.
 + */
 +struct gmap *gmap_alloc(struct mm_struct *mm)
 +{
 +	struct gmap *gmap;
 +	struct page *page;
 +	unsigned long *table;
 +
 +	gmap = kzalloc(sizeof(struct gmap), GFP_KERNEL);
 +	if (!gmap)
 +		goto out;
 +	INIT_LIST_HEAD(&gmap->crst_list);
 +	gmap->mm = mm;
 +	page = alloc_pages(GFP_KERNEL, ALLOC_ORDER);
 +	if (!page)
 +		goto out_free;
 +	list_add(&page->lru, &gmap->crst_list);
 +	table = (unsigned long *) page_to_phys(page);
 +	crst_table_init(table, _REGION1_ENTRY_EMPTY);
 +	gmap->table = table;
 +	gmap->asce = _ASCE_TYPE_REGION1 | _ASCE_TABLE_LENGTH |
 +		     _ASCE_USER_BITS | __pa(table);
 +	list_add(&gmap->list, &mm->context.gmap_list);
 +	return gmap;
 +
 +out_free:
 +	kfree(gmap);
 +out:
 +	return NULL;
 +}
 +EXPORT_SYMBOL_GPL(gmap_alloc);
 +
 +static int gmap_unlink_segment(struct gmap *gmap, unsigned long *table)
 +{
 +	struct gmap_pgtable *mp;
 +	struct gmap_rmap *rmap;
 +	struct page *page;
 +
 +	if (*table & _SEGMENT_ENTRY_INVALID)
 +		return 0;
 +	page = pfn_to_page(*table >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	list_for_each_entry(rmap, &mp->mapper, list) {
 +		if (rmap->entry != table)
 +			continue;
 +		list_del(&rmap->list);
 +		kfree(rmap);
 +		break;
 +	}
 +	*table = mp->vmaddr | _SEGMENT_ENTRY_INVALID | _SEGMENT_ENTRY_PROTECT;
 +	return 1;
 +}
 +
 +static void gmap_flush_tlb(struct gmap *gmap)
 +{
 +	if (MACHINE_HAS_IDTE)
 +		__tlb_flush_idte((unsigned long) gmap->table |
 +				 _ASCE_TYPE_REGION1);
  	else
 -		__ptep_ipte(addr, ptep, IPTE_GLOBAL);
 -	atomic_dec(&mm->context.flush_count);
 -	return old;
 +		__tlb_flush_global();
  }
  
 -static inline pte_t ptep_flush_lazy(struct mm_struct *mm,
 -				    unsigned long addr, pte_t *ptep)
 +/**
 + * gmap_free - free a guest address space
 + * @gmap: pointer to the guest address space structure
 + */
 +void gmap_free(struct gmap *gmap)
  {
 -	pte_t old;
 +	struct page *page, *next;
 +	unsigned long *table;
 +	int i;
  
 -	old = *ptep;
 -	if (unlikely(pte_val(old) & _PAGE_INVALID))
 -		return old;
 -	atomic_inc(&mm->context.flush_count);
 -	if (cpumask_equal(&mm->context.cpu_attach_mask,
 -			  cpumask_of(smp_processor_id()))) {
 -		pte_val(*ptep) |= _PAGE_INVALID;
 -		mm->context.flush_mm = 1;
 +
 +	/* Flush tlb. */
 +	if (MACHINE_HAS_IDTE)
 +		__tlb_flush_idte((unsigned long) gmap->table |
 +				 _ASCE_TYPE_REGION1);
 +	else
 +		__tlb_flush_global();
 +
 +	/* Free all segment & region tables. */
 +	down_read(&gmap->mm->mmap_sem);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	list_for_each_entry_safe(page, next, &gmap->crst_list, lru) {
 +		table = (unsigned long *) page_to_phys(page);
 +		if ((*table & _REGION_ENTRY_TYPE_MASK) == 0)
 +			/* Remove gmap rmap structures for segment table. */
 +			for (i = 0; i < PTRS_PER_PMD; i++, table++)
 +				gmap_unlink_segment(gmap, table);
 +		__free_pages(page, ALLOC_ORDER);
 +	}
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	list_del(&gmap->list);
 +	kfree(gmap);
 +}
 +EXPORT_SYMBOL_GPL(gmap_free);
 +
 +/**
 + * gmap_enable - switch primary space to the guest address space
 + * @gmap: pointer to the guest address space structure
 + */
 +void gmap_enable(struct gmap *gmap)
 +{
 +	S390_lowcore.gmap = (unsigned long) gmap;
 +}
 +EXPORT_SYMBOL_GPL(gmap_enable);
 +
 +/**
 + * gmap_disable - switch back to the standard primary address space
 + * @gmap: pointer to the guest address space structure
 + */
 +void gmap_disable(struct gmap *gmap)
 +{
 +	S390_lowcore.gmap = 0UL;
 +}
 +EXPORT_SYMBOL_GPL(gmap_disable);
 +
 +/*
 + * gmap_alloc_table is assumed to be called with mmap_sem held
 + */
 +static int gmap_alloc_table(struct gmap *gmap,
 +			       unsigned long *table, unsigned long init)
 +{
 +	struct page *page;
 +	unsigned long *new;
 +
 +	/* since we dont free the gmap table until gmap_free we can unlock */
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	page = alloc_pages(GFP_KERNEL, ALLOC_ORDER);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	if (!page)
 +		return -ENOMEM;
 +	new = (unsigned long *) page_to_phys(page);
 +	crst_table_init(new, init);
 +	if (*table & _REGION_ENTRY_INVALID) {
 +		list_add(&page->lru, &gmap->crst_list);
 +		*table = (unsigned long) new | _REGION_ENTRY_LENGTH |
 +			(*table & _REGION_ENTRY_TYPE_MASK);
  	} else
 -		__ptep_ipte(addr, ptep, IPTE_GLOBAL);
 -	atomic_dec(&mm->context.flush_count);
 -	return old;
 +		__free_pages(page, ALLOC_ORDER);
 +	return 0;
  }
  
 -static inline pgste_t pgste_get_lock(pte_t *ptep)
 +/**
 + * gmap_unmap_segment - unmap segment from the guest address space
 + * @gmap: pointer to the guest address space structure
 + * @addr: address in the guest address space
 + * @len: length of the memory area to unmap
 + *
 + * Returns 0 if the unmap succeded, -EINVAL if not.
 + */
 +int gmap_unmap_segment(struct gmap *gmap, unsigned long to, unsigned long len)
  {
 -	unsigned long new = 0;
 -#ifdef CONFIG_PGSTE
 -	unsigned long old;
 -
 -	asm(
 -		"	lg	%0,%2\n"
 -		"0:	lgr	%1,%0\n"
 -		"	nihh	%0,0xff7f\n"	/* clear PCL bit in old */
 -		"	oihh	%1,0x0080\n"	/* set PCL bit in new */
 -		"	csg	%0,%1,%2\n"
 -		"	jl	0b\n"
 -		: "=&d" (old), "=&d" (new), "=Q" (ptep[PTRS_PER_PTE])
 -		: "Q" (ptep[PTRS_PER_PTE]) : "cc", "memory");
 -#endif
 -	return __pgste(new);
 +	unsigned long *table;
 +	unsigned long off;
 +	int flush;
 +
 +	if ((to | len) & (PMD_SIZE - 1))
 +		return -EINVAL;
 +	if (len == 0 || to + len < to)
 +		return -EINVAL;
 +
 +	flush = 0;
 +	down_read(&gmap->mm->mmap_sem);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	for (off = 0; off < len; off += PMD_SIZE) {
 +		/* Walk the guest addr space page table */
 +		table = gmap->table + (((to + off) >> 53) & 0x7ff);
 +		if (*table & _REGION_ENTRY_INVALID)
 +			goto out;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 42) & 0x7ff);
 +		if (*table & _REGION_ENTRY_INVALID)
 +			goto out;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 31) & 0x7ff);
 +		if (*table & _REGION_ENTRY_INVALID)
 +			goto out;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 20) & 0x7ff);
 +
 +		/* Clear segment table entry in guest address space. */
 +		flush |= gmap_unlink_segment(gmap, table);
 +		*table = _SEGMENT_ENTRY_INVALID;
 +	}
 +out:
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	if (flush)
 +		gmap_flush_tlb(gmap);
 +	return 0;
  }
 +EXPORT_SYMBOL_GPL(gmap_unmap_segment);
  
 -static inline void pgste_set_unlock(pte_t *ptep, pgste_t pgste)
 +/**
 + * gmap_mmap_segment - map a segment to the guest address space
 + * @gmap: pointer to the guest address space structure
 + * @from: source address in the parent address space
 + * @to: target address in the guest address space
 + *
 + * Returns 0 if the mmap succeded, -EINVAL or -ENOMEM if not.
 + */
 +int gmap_map_segment(struct gmap *gmap, unsigned long from,
 +		     unsigned long to, unsigned long len)
  {
 -#ifdef CONFIG_PGSTE
 -	asm(
 -		"	nihh	%1,0xff7f\n"	/* clear PCL bit */
 -		"	stg	%1,%0\n"
 -		: "=Q" (ptep[PTRS_PER_PTE])
 -		: "d" (pgste_val(pgste)), "Q" (ptep[PTRS_PER_PTE])
 -		: "cc", "memory");
 -#endif
 +	unsigned long *table;
 +	unsigned long off;
 +	int flush;
 +
 +	if ((from | to | len) & (PMD_SIZE - 1))
 +		return -EINVAL;
 +	if (len == 0 || from + len > TASK_MAX_SIZE ||
 +	    from + len < from || to + len < to)
 +		return -EINVAL;
 +
 +	flush = 0;
 +	down_read(&gmap->mm->mmap_sem);
 +	spin_lock(&gmap->mm->page_table_lock);
 +	for (off = 0; off < len; off += PMD_SIZE) {
 +		/* Walk the gmap address space page table */
 +		table = gmap->table + (((to + off) >> 53) & 0x7ff);
 +		if ((*table & _REGION_ENTRY_INVALID) &&
 +		    gmap_alloc_table(gmap, table, _REGION2_ENTRY_EMPTY))
 +			goto out_unmap;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 42) & 0x7ff);
 +		if ((*table & _REGION_ENTRY_INVALID) &&
 +		    gmap_alloc_table(gmap, table, _REGION3_ENTRY_EMPTY))
 +			goto out_unmap;
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 31) & 0x7ff);
 +		if ((*table & _REGION_ENTRY_INVALID) &&
 +		    gmap_alloc_table(gmap, table, _SEGMENT_ENTRY_EMPTY))
 +			goto out_unmap;
 +		table = (unsigned long *) (*table & _REGION_ENTRY_ORIGIN);
 +		table = table + (((to + off) >> 20) & 0x7ff);
 +
 +		/* Store 'from' address in an invalid segment table entry. */
 +		flush |= gmap_unlink_segment(gmap, table);
 +		*table =  (from + off) | (_SEGMENT_ENTRY_INVALID |
 +					  _SEGMENT_ENTRY_PROTECT);
 +	}
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	if (flush)
 +		gmap_flush_tlb(gmap);
 +	return 0;
 +
 +out_unmap:
 +	spin_unlock(&gmap->mm->page_table_lock);
 +	up_read(&gmap->mm->mmap_sem);
 +	gmap_unmap_segment(gmap, to, len);
 +	return -ENOMEM;
  }
 +EXPORT_SYMBOL_GPL(gmap_map_segment);
  
 -static inline pgste_t pgste_get(pte_t *ptep)
 +static unsigned long *gmap_table_walk(unsigned long address, struct gmap *gmap)
  {
 -	unsigned long pgste = 0;
 -#ifdef CONFIG_PGSTE
 -	pgste = *(unsigned long *)(ptep + PTRS_PER_PTE);
 -#endif
 -	return __pgste(pgste);
 +	unsigned long *table;
 +
 +	table = gmap->table + ((address >> 53) & 0x7ff);
 +	if (unlikely(*table & _REGION_ENTRY_INVALID))
 +		return ERR_PTR(-EFAULT);
 +	table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +	table = table + ((address >> 42) & 0x7ff);
 +	if (unlikely(*table & _REGION_ENTRY_INVALID))
 +		return ERR_PTR(-EFAULT);
 +	table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +	table = table + ((address >> 31) & 0x7ff);
 +	if (unlikely(*table & _REGION_ENTRY_INVALID))
 +		return ERR_PTR(-EFAULT);
 +	table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +	table = table + ((address >> 20) & 0x7ff);
 +	return table;
  }
  
 -static inline void pgste_set(pte_t *ptep, pgste_t pgste)
 +/**
 + * __gmap_translate - translate a guest address to a user space address
 + * @address: guest address
 + * @gmap: pointer to guest mapping meta data structure
 + *
 + * Returns user space address which corresponds to the guest address or
 + * -EFAULT if no such mapping exists.
 + * This function does not establish potentially missing page table entries.
 + * The mmap_sem of the mm that belongs to the address space must be held
 + * when this function gets called.
 + */
 +unsigned long __gmap_translate(unsigned long address, struct gmap *gmap)
  {
 -#ifdef CONFIG_PGSTE
 -	*(pgste_t *)(ptep + PTRS_PER_PTE) = pgste;
 -#endif
 +	unsigned long *segment_ptr, vmaddr, segment;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +
 +	current->thread.gmap_addr = address;
 +	segment_ptr = gmap_table_walk(address, gmap);
 +	if (IS_ERR(segment_ptr))
 +		return PTR_ERR(segment_ptr);
 +	/* Convert the gmap address to an mm address. */
 +	segment = *segment_ptr;
 +	if (!(segment & _SEGMENT_ENTRY_INVALID)) {
 +		page = pfn_to_page(segment >> PAGE_SHIFT);
 +		mp = (struct gmap_pgtable *) page->index;
 +		return mp->vmaddr | (address & ~PMD_MASK);
 +	} else if (segment & _SEGMENT_ENTRY_PROTECT) {
 +		vmaddr = segment & _SEGMENT_ENTRY_ORIGIN;
 +		return vmaddr | (address & ~PMD_MASK);
 +	}
 +	return -EFAULT;
  }
 +EXPORT_SYMBOL_GPL(__gmap_translate);
  
 -static inline pgste_t pgste_update_all(pte_t pte, pgste_t pgste,
 -				       struct mm_struct *mm)
 +/**
 + * gmap_translate - translate a guest address to a user space address
 + * @address: guest address
 + * @gmap: pointer to guest mapping meta data structure
 + *
 + * Returns user space address which corresponds to the guest address or
 + * -EFAULT if no such mapping exists.
 + * This function does not establish potentially missing page table entries.
 + */
 +unsigned long gmap_translate(unsigned long address, struct gmap *gmap)
  {
 -#ifdef CONFIG_PGSTE
 -	unsigned long address, bits, skey;
 -
 -	if (!mm_use_skey(mm) || pte_val(pte) & _PAGE_INVALID)
 -		return pgste;
 -	address = pte_val(pte) & PAGE_MASK;
 -	skey = (unsigned long) page_get_storage_key(address);
 -	bits = skey & (_PAGE_CHANGED | _PAGE_REFERENCED);
 -	/* Transfer page changed & referenced bit to guest bits in pgste */
 -	pgste_val(pgste) |= bits << 48;		/* GR bit & GC bit */
 -	/* Copy page access key and fetch protection bit to pgste */
 -	pgste_val(pgste) &= ~(PGSTE_ACC_BITS | PGSTE_FP_BIT);
 -	pgste_val(pgste) |= (skey & (_PAGE_ACC_BITS | _PAGE_FP_BIT)) << 56;
 -#endif
 -	return pgste;
 +	unsigned long rc;
  
 +	down_read(&gmap->mm->mmap_sem);
 +	rc = __gmap_translate(address, gmap);
 +	up_read(&gmap->mm->mmap_sem);
 +	return rc;
  }
 +EXPORT_SYMBOL_GPL(gmap_translate);
  
 -static inline void pgste_set_key(pte_t *ptep, pgste_t pgste, pte_t entry,
 -				 struct mm_struct *mm)
 +static int gmap_connect_pgtable(unsigned long address, unsigned long segment,
 +				unsigned long *segment_ptr, struct gmap *gmap)
  {
 -#ifdef CONFIG_PGSTE
 -	unsigned long address;
 -	unsigned long nkey;
 +	unsigned long vmaddr;
 +	struct vm_area_struct *vma;
 +	struct gmap_pgtable *mp;
 +	struct gmap_rmap *rmap;
 +	struct mm_struct *mm;
 +	struct page *page;
 +	pgd_t *pgd;
 +	pud_t *pud;
 +	pmd_t *pmd;
 +
 +	mm = gmap->mm;
 +	vmaddr = segment & _SEGMENT_ENTRY_ORIGIN;
 +	vma = find_vma(mm, vmaddr);
 +	if (!vma || vma->vm_start > vmaddr)
 +		return -EFAULT;
 +	/* Walk the parent mm page table */
 +	pgd = pgd_offset(mm, vmaddr);
 +	pud = pud_alloc(mm, pgd, vmaddr);
 +	if (!pud)
 +		return -ENOMEM;
 +	/* large puds cannot yet be handled */
 +	if (pud_large(*pud))
 +		return -EFAULT;
 +	pmd = pmd_alloc(mm, pud, vmaddr);
 +	if (!pmd)
 +		return -ENOMEM;
 +	if (!pmd_present(*pmd) &&
 +	    __pte_alloc(mm, vma, pmd, vmaddr))
 +		return -ENOMEM;
 +	/* large pmds cannot yet be handled */
 +	if (pmd_large(*pmd))
 +		return -EFAULT;
 +	/* pmd now points to a valid segment table entry. */
 +	rmap = kmalloc(sizeof(*rmap), GFP_KERNEL|__GFP_REPEAT);
 +	if (!rmap)
 +		return -ENOMEM;
 +	/* Link gmap segment table entry location to page table. */
 +	page = pmd_page(*pmd);
 +	mp = (struct gmap_pgtable *) page->index;
 +	rmap->gmap = gmap;
 +	rmap->entry = segment_ptr;
 +	rmap->vmaddr = address & PMD_MASK;
 +	spin_lock(&mm->page_table_lock);
 +	if (*segment_ptr == segment) {
 +		list_add(&rmap->list, &mp->mapper);
 +		/* Set gmap segment table entry to page table. */
 +		*segment_ptr = pmd_val(*pmd) & PAGE_MASK;
 +		rmap = NULL;
 +	}
 +	spin_unlock(&mm->page_table_lock);
 +	kfree(rmap);
 +	return 0;
 +}
  
 -	if (!mm_use_skey(mm) || pte_val(entry) & _PAGE_INVALID)
 +static void gmap_disconnect_pgtable(struct mm_struct *mm, unsigned long *table)
 +{
 +	struct gmap_rmap *rmap, *next;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +	int flush;
 +
 +	flush = 0;
 +	spin_lock(&mm->page_table_lock);
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	list_for_each_entry_safe(rmap, next, &mp->mapper, list) {
 +		*rmap->entry = mp->vmaddr | (_SEGMENT_ENTRY_INVALID |
 +					     _SEGMENT_ENTRY_PROTECT);
 +		list_del(&rmap->list);
 +		kfree(rmap);
 +		flush = 1;
 +	}
 +	spin_unlock(&mm->page_table_lock);
 +	if (flush)
 +		__tlb_flush_global();
 +}
 +
 +/*
 + * this function is assumed to be called with mmap_sem held
 + */
 +unsigned long __gmap_fault(unsigned long address, struct gmap *gmap)
 +{
 +	unsigned long *segment_ptr, segment;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +	int rc;
 +
 +	current->thread.gmap_addr = address;
 +	segment_ptr = gmap_table_walk(address, gmap);
 +	if (IS_ERR(segment_ptr))
 +		return -EFAULT;
 +	/* Convert the gmap address to an mm address. */
 +	while (1) {
 +		segment = *segment_ptr;
 +		if (!(segment & _SEGMENT_ENTRY_INVALID)) {
 +			/* Page table is present */
 +			page = pfn_to_page(segment >> PAGE_SHIFT);
 +			mp = (struct gmap_pgtable *) page->index;
 +			return mp->vmaddr | (address & ~PMD_MASK);
 +		}
 +		if (!(segment & _SEGMENT_ENTRY_PROTECT))
 +			/* Nothing mapped in the gmap address space. */
 +			break;
 +		rc = gmap_connect_pgtable(address, segment, segment_ptr, gmap);
 +		if (rc)
 +			return rc;
 +	}
 +	return -EFAULT;
 +}
 +
 +unsigned long gmap_fault(unsigned long address, struct gmap *gmap)
 +{
 +	unsigned long rc;
 +
 +	down_read(&gmap->mm->mmap_sem);
 +	rc = __gmap_fault(address, gmap);
 +	up_read(&gmap->mm->mmap_sem);
 +
 +	return rc;
 +}
 +EXPORT_SYMBOL_GPL(gmap_fault);
 +
 +void gmap_discard(unsigned long from, unsigned long to, struct gmap *gmap)
 +{
 +
 +	unsigned long *table, address, size;
 +	struct vm_area_struct *vma;
 +	struct gmap_pgtable *mp;
 +	struct page *page;
 +
 +	down_read(&gmap->mm->mmap_sem);
 +	address = from;
 +	while (address < to) {
 +		/* Walk the gmap address space page table */
 +		table = gmap->table + ((address >> 53) & 0x7ff);
 +		if (unlikely(*table & _REGION_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + ((address >> 42) & 0x7ff);
 +		if (unlikely(*table & _REGION_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + ((address >> 31) & 0x7ff);
 +		if (unlikely(*table & _REGION_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		table = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);
 +		table = table + ((address >> 20) & 0x7ff);
 +		if (unlikely(*table & _SEGMENT_ENTRY_INVALID)) {
 +			address = (address + PMD_SIZE) & PMD_MASK;
 +			continue;
 +		}
 +		page = pfn_to_page(*table >> PAGE_SHIFT);
 +		mp = (struct gmap_pgtable *) page->index;
 +		vma = find_vma(gmap->mm, mp->vmaddr);
 +		size = min(to - address, PMD_SIZE - (address & ~PMD_MASK));
 +		zap_page_range(vma, mp->vmaddr | (address & ~PMD_MASK),
 +			       size, NULL);
 +		address = (address + PMD_SIZE) & PMD_MASK;
 +	}
 +	up_read(&gmap->mm->mmap_sem);
 +}
 +EXPORT_SYMBOL_GPL(gmap_discard);
 +
 +static LIST_HEAD(gmap_notifier_list);
 +static DEFINE_SPINLOCK(gmap_notifier_lock);
 +
 +/**
 + * gmap_register_ipte_notifier - register a pte invalidation callback
 + * @nb: pointer to the gmap notifier block
 + */
 +void gmap_register_ipte_notifier(struct gmap_notifier *nb)
 +{
 +	spin_lock(&gmap_notifier_lock);
 +	list_add(&nb->list, &gmap_notifier_list);
 +	spin_unlock(&gmap_notifier_lock);
 +}
 +EXPORT_SYMBOL_GPL(gmap_register_ipte_notifier);
 +
 +/**
 + * gmap_unregister_ipte_notifier - remove a pte invalidation callback
 + * @nb: pointer to the gmap notifier block
 + */
 +void gmap_unregister_ipte_notifier(struct gmap_notifier *nb)
 +{
 +	spin_lock(&gmap_notifier_lock);
 +	list_del_init(&nb->list);
 +	spin_unlock(&gmap_notifier_lock);
 +}
 +EXPORT_SYMBOL_GPL(gmap_unregister_ipte_notifier);
 +
 +/**
 + * gmap_ipte_notify - mark a range of ptes for invalidation notification
 + * @gmap: pointer to guest mapping meta data structure
 + * @address: virtual address in the guest address space
 + * @len: size of area
 + *
 + * Returns 0 if for each page in the given range a gmap mapping exists and
 + * the invalidation notification could be set. If the gmap mapping is missing
 + * for one or more pages -EFAULT is returned. If no memory could be allocated
 + * -ENOMEM is returned. This function establishes missing page table entries.
 + */
 +int gmap_ipte_notify(struct gmap *gmap, unsigned long start, unsigned long len)
 +{
 +	unsigned long addr;
 +	spinlock_t *ptl;
 +	pte_t *ptep, entry;
 +	pgste_t pgste;
 +	int rc = 0;
 +
 +	if ((start & ~PAGE_MASK) || (len & ~PAGE_MASK))
 +		return -EINVAL;
 +	down_read(&gmap->mm->mmap_sem);
 +	while (len) {
 +		/* Convert gmap address and connect the page tables */
 +		addr = __gmap_fault(start, gmap);
 +		if (IS_ERR_VALUE(addr)) {
 +			rc = addr;
 +			break;
 +		}
 +		/* Get the page mapped */
 +		if (fixup_user_fault(current, gmap->mm, addr, FAULT_FLAG_WRITE)) {
 +			rc = -EFAULT;
 +			break;
 +		}
 +		/* Walk the process page table, lock and get pte pointer */
 +		ptep = get_locked_pte(gmap->mm, addr, &ptl);
 +		if (unlikely(!ptep))
 +			continue;
 +		/* Set notification bit in the pgste of the pte */
 +		entry = *ptep;
 +		if ((pte_val(entry) & (_PAGE_INVALID | _PAGE_PROTECT)) == 0) {
 +			pgste = pgste_get_lock(ptep);
 +			pgste_val(pgste) |= PGSTE_IN_BIT;
 +			pgste_set_unlock(ptep, pgste);
 +			start += PAGE_SIZE;
 +			len -= PAGE_SIZE;
 +		}
 +		spin_unlock(ptl);
 +	}
 +	up_read(&gmap->mm->mmap_sem);
 +	return rc;
 +}
 +EXPORT_SYMBOL_GPL(gmap_ipte_notify);
 +
 +/**
 + * gmap_do_ipte_notify - call all invalidation callbacks for a specific pte.
 + * @mm: pointer to the process mm_struct
 + * @addr: virtual address in the process address space
 + * @pte: pointer to the page table entry
 + *
 + * This function is assumed to be called with the page table lock held
 + * for the pte to notify.
 + */
 +void gmap_do_ipte_notify(struct mm_struct *mm, unsigned long addr, pte_t *pte)
 +{
 +	unsigned long segment_offset;
 +	struct gmap_notifier *nb;
 +	struct gmap_pgtable *mp;
 +	struct gmap_rmap *rmap;
 +	struct page *page;
 +
 +	segment_offset = ((unsigned long) pte) & (255 * sizeof(pte_t));
 +	segment_offset = segment_offset * (4096 / sizeof(pte_t));
 +	page = pfn_to_page(__pa(pte) >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	spin_lock(&gmap_notifier_lock);
 +	list_for_each_entry(rmap, &mp->mapper, list) {
 +		list_for_each_entry(nb, &gmap_notifier_list, list)
 +			nb->notifier_call(rmap->gmap,
 +					  rmap->vmaddr + segment_offset);
 +	}
 +	spin_unlock(&gmap_notifier_lock);
 +}
 +
 +static inline int page_table_with_pgste(struct page *page)
 +{
 +	return atomic_read(&page->_mapcount) == 0;
 +}
 +
 +static inline unsigned long *page_table_alloc_pgste(struct mm_struct *mm,
 +						    unsigned long vmaddr)
 +{
 +	struct page *page;
 +	unsigned long *table;
 +	struct gmap_pgtable *mp;
 +
 +	page = alloc_page(GFP_KERNEL|__GFP_REPEAT);
 +	if (!page)
 +		return NULL;
 +	mp = kmalloc(sizeof(*mp), GFP_KERNEL|__GFP_REPEAT);
 +	if (!mp) {
 +		__free_page(page);
 +		return NULL;
 +	}
 +	if (!pgtable_page_ctor(page)) {
 +		kfree(mp);
 +		__free_page(page);
 +		return NULL;
 +	}
 +	mp->vmaddr = vmaddr & PMD_MASK;
 +	INIT_LIST_HEAD(&mp->mapper);
 +	page->index = (unsigned long) mp;
 +	atomic_set(&page->_mapcount, 0);
 +	table = (unsigned long *) page_to_phys(page);
 +	clear_table(table, _PAGE_INVALID, PAGE_SIZE/2);
 +	clear_table(table + PTRS_PER_PTE, PGSTE_HR_BIT | PGSTE_HC_BIT,
 +		    PAGE_SIZE/2);
 +	return table;
 +}
 +
 +static inline void page_table_free_pgste(unsigned long *table)
 +{
 +	struct page *page;
 +	struct gmap_pgtable *mp;
 +
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	mp = (struct gmap_pgtable *) page->index;
 +	BUG_ON(!list_empty(&mp->mapper));
 +	pgtable_page_dtor(page);
 +	atomic_set(&page->_mapcount, -1);
 +	kfree(mp);
 +	__free_page(page);
 +}
 +
 +int set_guest_storage_key(struct mm_struct *mm, unsigned long addr,
 +			  unsigned long key, bool nq)
 +{
 +	spinlock_t *ptl;
 +	pgste_t old, new;
 +	pte_t *ptep;
 +
 +	down_read(&mm->mmap_sem);
 +	ptep = get_locked_pte(current->mm, addr, &ptl);
 +	if (unlikely(!ptep)) {
 +		up_read(&mm->mmap_sem);
 +		return -EFAULT;
 +	}
 +
 +	new = old = pgste_get_lock(ptep);
 +	pgste_val(new) &= ~(PGSTE_GR_BIT | PGSTE_GC_BIT |
 +			    PGSTE_ACC_BITS | PGSTE_FP_BIT);
 +	pgste_val(new) |= (key & (_PAGE_CHANGED | _PAGE_REFERENCED)) << 48;
 +	pgste_val(new) |= (key & (_PAGE_ACC_BITS | _PAGE_FP_BIT)) << 56;
 +	if (!(pte_val(*ptep) & _PAGE_INVALID)) {
 +		unsigned long address, bits, skey;
 +
 +		address = pte_val(*ptep) & PAGE_MASK;
 +		skey = (unsigned long) page_get_storage_key(address);
 +		bits = skey & (_PAGE_CHANGED | _PAGE_REFERENCED);
 +		skey = key & (_PAGE_ACC_BITS | _PAGE_FP_BIT);
 +		/* Set storage key ACC and FP */
 +		page_set_storage_key(address, skey, !nq);
 +		/* Merge host changed & referenced into pgste  */
 +		pgste_val(new) |= bits << 52;
 +	}
 +	/* changing the guest storage key is considered a change of the page */
 +	if ((pgste_val(new) ^ pgste_val(old)) &
 +	    (PGSTE_ACC_BITS | PGSTE_FP_BIT | PGSTE_GR_BIT | PGSTE_GC_BIT))
 +		pgste_val(new) |= PGSTE_HC_BIT;
 +
 +	pgste_set_unlock(ptep, new);
 +	pte_unmap_unlock(*ptep, ptl);
 +	up_read(&mm->mmap_sem);
 +	return 0;
 +}
 +EXPORT_SYMBOL(set_guest_storage_key);
 +
 +#else /* CONFIG_PGSTE */
 +
 +static inline int page_table_with_pgste(struct page *page)
 +{
 +	return 0;
 +}
 +
 +static inline unsigned long *page_table_alloc_pgste(struct mm_struct *mm,
 +						    unsigned long vmaddr)
 +{
 +	return NULL;
 +}
 +
 +static inline void page_table_free_pgste(unsigned long *table)
 +{
 +}
 +
 +static inline void gmap_disconnect_pgtable(struct mm_struct *mm,
 +					   unsigned long *table)
 +{
 +}
 +
 +#endif /* CONFIG_PGSTE */
 +
 +static inline unsigned int atomic_xor_bits(atomic_t *v, unsigned int bits)
 +{
 +	unsigned int old, new;
 +
 +	do {
 +		old = atomic_read(v);
 +		new = old ^ bits;
 +	} while (atomic_cmpxchg(v, old, new) != old);
 +	return new;
 +}
 +
 +/*
 + * page table entry allocation/free routines.
 + */
 +unsigned long *page_table_alloc(struct mm_struct *mm, unsigned long vmaddr)
 +{
 +	unsigned long *uninitialized_var(table);
 +	struct page *uninitialized_var(page);
 +	unsigned int mask, bit;
 +
 +	if (mm_has_pgste(mm))
 +		return page_table_alloc_pgste(mm, vmaddr);
 +	/* Allocate fragments of a 4K page as 1K/2K page table */
 +	spin_lock_bh(&mm->context.list_lock);
 +	mask = FRAG_MASK;
 +	if (!list_empty(&mm->context.pgtable_list)) {
 +		page = list_first_entry(&mm->context.pgtable_list,
 +					struct page, lru);
 +		table = (unsigned long *) page_to_phys(page);
 +		mask = atomic_read(&page->_mapcount);
 +		mask = mask | (mask >> 4);
 +	}
 +	if ((mask & FRAG_MASK) == FRAG_MASK) {
 +		spin_unlock_bh(&mm->context.list_lock);
 +		page = alloc_page(GFP_KERNEL|__GFP_REPEAT);
 +		if (!page)
 +			return NULL;
 +		if (!pgtable_page_ctor(page)) {
 +			__free_page(page);
 +			return NULL;
 +		}
 +		atomic_set(&page->_mapcount, 1);
 +		table = (unsigned long *) page_to_phys(page);
 +		clear_table(table, _PAGE_INVALID, PAGE_SIZE);
 +		spin_lock_bh(&mm->context.list_lock);
 +		list_add(&page->lru, &mm->context.pgtable_list);
 +	} else {
 +		for (bit = 1; mask & bit; bit <<= 1)
 +			table += PTRS_PER_PTE;
 +		mask = atomic_xor_bits(&page->_mapcount, bit);
 +		if ((mask & FRAG_MASK) == FRAG_MASK)
 +			list_del(&page->lru);
 +	}
 +	spin_unlock_bh(&mm->context.list_lock);
 +	return table;
 +}
 +
 +void page_table_free(struct mm_struct *mm, unsigned long *table)
 +{
 +	struct page *page;
 +	unsigned int bit, mask;
 +
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	if (page_table_with_pgste(page)) {
 +		gmap_disconnect_pgtable(mm, table);
 +		return page_table_free_pgste(table);
 +	}
 +	/* Free 1K/2K page table fragment of a 4K page */
 +	bit = 1 << ((__pa(table) & ~PAGE_MASK)/(PTRS_PER_PTE*sizeof(pte_t)));
 +	spin_lock_bh(&mm->context.list_lock);
 +	if ((atomic_read(&page->_mapcount) & FRAG_MASK) != FRAG_MASK)
 +		list_del(&page->lru);
 +	mask = atomic_xor_bits(&page->_mapcount, bit);
 +	if (mask & FRAG_MASK)
 +		list_add(&page->lru, &mm->context.pgtable_list);
 +	spin_unlock_bh(&mm->context.list_lock);
 +	if (mask == 0) {
 +		pgtable_page_dtor(page);
 +		atomic_set(&page->_mapcount, -1);
 +		__free_page(page);
 +	}
 +}
 +
 +static void __page_table_free_rcu(void *table, unsigned bit)
 +{
 +	struct page *page;
 +
 +	if (bit == FRAG_MASK)
 +		return page_table_free_pgste(table);
 +	/* Free 1K/2K page table fragment of a 4K page */
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	if (atomic_xor_bits(&page->_mapcount, bit) == 0) {
 +		pgtable_page_dtor(page);
 +		atomic_set(&page->_mapcount, -1);
 +		__free_page(page);
 +	}
 +}
 +
 +void page_table_free_rcu(struct mmu_gather *tlb, unsigned long *table)
 +{
 +	struct mm_struct *mm;
 +	struct page *page;
 +	unsigned int bit, mask;
 +
 +	mm = tlb->mm;
 +	page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +	if (page_table_with_pgste(page)) {
 +		gmap_disconnect_pgtable(mm, table);
 +		table = (unsigned long *) (__pa(table) | FRAG_MASK);
 +		tlb_remove_table(tlb, table);
  		return;
 -	VM_BUG_ON(!(pte_val(*ptep) & _PAGE_INVALID));
 -	address = pte_val(entry) & PAGE_MASK;
 +	}
 +	bit = 1 << ((__pa(table) & ~PAGE_MASK) / (PTRS_PER_PTE*sizeof(pte_t)));
 +	spin_lock_bh(&mm->context.list_lock);
 +	if ((atomic_read(&page->_mapcount) & FRAG_MASK) != FRAG_MASK)
 +		list_del(&page->lru);
 +	mask = atomic_xor_bits(&page->_mapcount, bit | (bit << 4));
 +	if (mask & FRAG_MASK)
 +		list_add_tail(&page->lru, &mm->context.pgtable_list);
 +	spin_unlock_bh(&mm->context.list_lock);
 +	table = (unsigned long *) (__pa(table) | (bit << 4));
 +	tlb_remove_table(tlb, table);
 +}
 +
 +void __tlb_remove_table(void *_table)
 +{
 +	const unsigned long mask = (FRAG_MASK << 4) | FRAG_MASK;
 +	void *table = (void *)((unsigned long) _table & ~mask);
 +	unsigned type = (unsigned long) _table & mask;
 +
 +	if (type)
 +		__page_table_free_rcu(table, type);
 +	else
 +		free_pages((unsigned long) table, ALLOC_ORDER);
 +}
 +
 +static void tlb_remove_table_smp_sync(void *arg)
 +{
 +	/* Simply deliver the interrupt */
 +}
 +
 +static void tlb_remove_table_one(void *table)
 +{
  	/*
 -	 * Set page access key and fetch protection bit from pgste.
 -	 * The guest C/R information is still in the PGSTE, set real
 -	 * key C/R to 0.
 +	 * This isn't an RCU grace period and hence the page-tables cannot be
 +	 * assumed to be actually RCU-freed.
 +	 *
 +	 * It is however sufficient for software page-table walkers that rely
 +	 * on IRQ disabling. See the comment near struct mmu_table_batch.
  	 */
 -	nkey = (pgste_val(pgste) & (PGSTE_ACC_BITS | PGSTE_FP_BIT)) >> 56;
 -	nkey |= (pgste_val(pgste) & (PGSTE_GR_BIT | PGSTE_GC_BIT)) >> 48;
 -	page_set_storage_key(address, nkey, 0);
 -#endif
 +	smp_call_function(tlb_remove_table_smp_sync, NULL, 1);
 +	__tlb_remove_table(table);
  }
  
 -static inline pgste_t pgste_set_pte(pte_t *ptep, pgste_t pgste, pte_t entry)
 +static void tlb_remove_table_rcu(struct rcu_head *head)
  {
 -#ifdef CONFIG_PGSTE
 -	if ((pte_val(entry) & _PAGE_PRESENT) &&
 -	    (pte_val(entry) & _PAGE_WRITE) &&
 -	    !(pte_val(entry) & _PAGE_INVALID)) {
 -		if (!MACHINE_HAS_ESOP) {
 -			/*
 -			 * Without enhanced suppression-on-protection force
 -			 * the dirty bit on for all writable ptes.
 -			 */
 -			pte_val(entry) |= _PAGE_DIRTY;
 -			pte_val(entry) &= ~_PAGE_PROTECT;
 +	struct mmu_table_batch *batch;
 +	int i;
 +
 +	batch = container_of(head, struct mmu_table_batch, rcu);
 +
 +	for (i = 0; i < batch->nr; i++)
 +		__tlb_remove_table(batch->tables[i]);
 +
 +	free_page((unsigned long)batch);
 +}
 +
 +void tlb_table_flush(struct mmu_gather *tlb)
 +{
 +	struct mmu_table_batch **batch = &tlb->batch;
 +
 +	if (*batch) {
 +		__tlb_flush_mm(tlb->mm);
 +		call_rcu_sched(&(*batch)->rcu, tlb_remove_table_rcu);
 +		*batch = NULL;
 +	}
 +}
 +
 +void tlb_remove_table(struct mmu_gather *tlb, void *table)
 +{
 +	struct mmu_table_batch **batch = &tlb->batch;
 +
 +	if (*batch == NULL) {
 +		*batch = (struct mmu_table_batch *)
 +			__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
 +		if (*batch == NULL) {
 +			__tlb_flush_mm(tlb->mm);
 +			tlb_remove_table_one(table);
 +			return;
  		}
 -		if (!(pte_val(entry) & _PAGE_PROTECT))
 -			/* This pte allows write access, set user-dirty */
 -			pgste_val(pgste) |= PGSTE_UC_BIT;
 +		(*batch)->nr = 0;
  	}
 -#endif
 -	*ptep = entry;
 -	return pgste;
 +	(*batch)->tables[(*batch)->nr++] = table;
 +	if ((*batch)->nr == MAX_TABLE_BATCH)
 +		tlb_table_flush(tlb);
  }
  
++<<<<<<< HEAD
++=======
+ static inline pgste_t pgste_pte_notify(struct mm_struct *mm,
+ 				       unsigned long addr,
+ 				       pte_t *ptep, pgste_t pgste)
+ {
+ #ifdef CONFIG_PGSTE
+ 	unsigned long bits;
+ 
+ 	bits = pgste_val(pgste) & (PGSTE_IN_BIT | PGSTE_VSIE_BIT);
+ 	if (bits) {
+ 		pgste_val(pgste) ^= bits;
+ 		ptep_notify(mm, addr, ptep, bits);
+ 	}
+ #endif
+ 	return pgste;
+ }
+ 
+ static inline pgste_t ptep_xchg_start(struct mm_struct *mm,
+ 				      unsigned long addr, pte_t *ptep)
+ {
+ 	pgste_t pgste = __pgste(0);
+ 
+ 	if (mm_has_pgste(mm)) {
+ 		pgste = pgste_get_lock(ptep);
+ 		pgste = pgste_pte_notify(mm, addr, ptep, pgste);
+ 	}
+ 	return pgste;
+ }
+ 
+ static inline void ptep_xchg_commit(struct mm_struct *mm,
+ 				    unsigned long addr, pte_t *ptep,
+ 				    pgste_t pgste, pte_t old, pte_t new)
+ {
+ 	if (mm_has_pgste(mm)) {
+ 		if (pte_val(old) & _PAGE_INVALID)
+ 			pgste_set_key(ptep, pgste, new, mm);
+ 		if (pte_val(new) & _PAGE_INVALID) {
+ 			pgste = pgste_update_all(old, pgste, mm);
+ 			if ((pgste_val(pgste) & _PGSTE_GPS_USAGE_MASK) ==
+ 			    _PGSTE_GPS_USAGE_UNUSED)
+ 				pte_val(old) |= _PAGE_UNUSED;
+ 		}
+ 		pgste = pgste_set_pte(ptep, pgste, new);
+ 		pgste_set_unlock(ptep, pgste);
+ 	} else {
+ 		*ptep = new;
+ 	}
+ }
+ 
+ pte_t ptep_xchg_direct(struct mm_struct *mm, unsigned long addr,
+ 		       pte_t *ptep, pte_t new)
+ {
+ 	pgste_t pgste;
+ 	pte_t old;
+ 
+ 	preempt_disable();
+ 	pgste = ptep_xchg_start(mm, addr, ptep);
+ 	old = ptep_flush_direct(mm, addr, ptep);
+ 	ptep_xchg_commit(mm, addr, ptep, pgste, old, new);
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(ptep_xchg_direct);
+ 
+ pte_t ptep_xchg_lazy(struct mm_struct *mm, unsigned long addr,
+ 		     pte_t *ptep, pte_t new)
+ {
+ 	pgste_t pgste;
+ 	pte_t old;
+ 
+ 	preempt_disable();
+ 	pgste = ptep_xchg_start(mm, addr, ptep);
+ 	old = ptep_flush_lazy(mm, addr, ptep);
+ 	ptep_xchg_commit(mm, addr, ptep, pgste, old, new);
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(ptep_xchg_lazy);
+ 
+ pte_t ptep_modify_prot_start(struct mm_struct *mm, unsigned long addr,
+ 			     pte_t *ptep)
+ {
+ 	pgste_t pgste;
+ 	pte_t old;
+ 
+ 	preempt_disable();
+ 	pgste = ptep_xchg_start(mm, addr, ptep);
+ 	old = ptep_flush_lazy(mm, addr, ptep);
+ 	if (mm_has_pgste(mm)) {
+ 		pgste = pgste_update_all(old, pgste, mm);
+ 		pgste_set(ptep, pgste);
+ 	}
+ 	return old;
+ }
+ EXPORT_SYMBOL(ptep_modify_prot_start);
+ 
+ void ptep_modify_prot_commit(struct mm_struct *mm, unsigned long addr,
+ 			     pte_t *ptep, pte_t pte)
+ {
+ 	pgste_t pgste;
+ 
+ 	if (!MACHINE_HAS_NX)
+ 		pte_val(pte) &= ~_PAGE_NOEXEC;
+ 	if (mm_has_pgste(mm)) {
+ 		pgste = pgste_get(ptep);
+ 		pgste_set_key(ptep, pgste, pte, mm);
+ 		pgste = pgste_set_pte(ptep, pgste, pte);
+ 		pgste_set_unlock(ptep, pgste);
+ 	} else {
+ 		*ptep = pte;
+ 	}
+ 	preempt_enable();
+ }
+ EXPORT_SYMBOL(ptep_modify_prot_commit);
+ 
+ static inline pmd_t pmdp_flush_direct(struct mm_struct *mm,
+ 				      unsigned long addr, pmd_t *pmdp)
+ {
+ 	pmd_t old;
+ 
+ 	old = *pmdp;
+ 	if (pmd_val(old) & _SEGMENT_ENTRY_INVALID)
+ 		return old;
+ 	if (!MACHINE_HAS_IDTE) {
+ 		__pmdp_csp(pmdp);
+ 		return old;
+ 	}
+ 	atomic_inc(&mm->context.flush_count);
+ 	if (MACHINE_HAS_TLB_LC &&
+ 	    cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))
+ 		__pmdp_idte(addr, pmdp, IDTE_LOCAL);
+ 	else
+ 		__pmdp_idte(addr, pmdp, IDTE_GLOBAL);
+ 	atomic_dec(&mm->context.flush_count);
+ 	return old;
+ }
+ 
+ static inline pmd_t pmdp_flush_lazy(struct mm_struct *mm,
+ 				    unsigned long addr, pmd_t *pmdp)
+ {
+ 	pmd_t old;
+ 
+ 	old = *pmdp;
+ 	if (pmd_val(old) & _SEGMENT_ENTRY_INVALID)
+ 		return old;
+ 	atomic_inc(&mm->context.flush_count);
+ 	if (cpumask_equal(&mm->context.cpu_attach_mask,
+ 			  cpumask_of(smp_processor_id()))) {
+ 		pmd_val(*pmdp) |= _SEGMENT_ENTRY_INVALID;
+ 		mm->context.flush_mm = 1;
+ 	} else if (MACHINE_HAS_IDTE)
+ 		__pmdp_idte(addr, pmdp, IDTE_GLOBAL);
+ 	else
+ 		__pmdp_csp(pmdp);
+ 	atomic_dec(&mm->context.flush_count);
+ 	return old;
+ }
+ 
+ pmd_t pmdp_xchg_direct(struct mm_struct *mm, unsigned long addr,
+ 		       pmd_t *pmdp, pmd_t new)
+ {
+ 	pmd_t old;
+ 
+ 	preempt_disable();
+ 	old = pmdp_flush_direct(mm, addr, pmdp);
+ 	*pmdp = new;
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(pmdp_xchg_direct);
+ 
+ pmd_t pmdp_xchg_lazy(struct mm_struct *mm, unsigned long addr,
+ 		     pmd_t *pmdp, pmd_t new)
+ {
+ 	pmd_t old;
+ 
+ 	preempt_disable();
+ 	old = pmdp_flush_lazy(mm, addr, pmdp);
+ 	*pmdp = new;
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(pmdp_xchg_lazy);
+ 
+ static inline pud_t pudp_flush_direct(struct mm_struct *mm,
+ 				      unsigned long addr, pud_t *pudp)
+ {
+ 	pud_t old;
+ 
+ 	old = *pudp;
+ 	if (pud_val(old) & _REGION_ENTRY_INVALID)
+ 		return old;
+ 	if (!MACHINE_HAS_IDTE) {
+ 		/*
+ 		 * Invalid bit position is the same for pmd and pud, so we can
+ 		 * re-use _pmd_csp() here
+ 		 */
+ 		__pmdp_csp((pmd_t *) pudp);
+ 		return old;
+ 	}
+ 	atomic_inc(&mm->context.flush_count);
+ 	if (MACHINE_HAS_TLB_LC &&
+ 	    cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))
+ 		__pudp_idte(addr, pudp, IDTE_LOCAL);
+ 	else
+ 		__pudp_idte(addr, pudp, IDTE_GLOBAL);
+ 	atomic_dec(&mm->context.flush_count);
+ 	return old;
+ }
+ 
+ pud_t pudp_xchg_direct(struct mm_struct *mm, unsigned long addr,
+ 		       pud_t *pudp, pud_t new)
+ {
+ 	pud_t old;
+ 
+ 	preempt_disable();
+ 	old = pudp_flush_direct(mm, addr, pudp);
+ 	*pudp = new;
+ 	preempt_enable();
+ 	return old;
+ }
+ EXPORT_SYMBOL(pudp_xchg_direct);
+ 
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 -void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 -				pgtable_t pgtable)
 +static inline void thp_split_vma(struct vm_area_struct *vma)
  {
 -	struct list_head *lh = (struct list_head *) pgtable;
 +	unsigned long addr;
  
 -	assert_spin_locked(pmd_lockptr(mm, pmdp));
 -
 -	/* FIFO */
 -	if (!pmd_huge_pte(mm, pmdp))
 -		INIT_LIST_HEAD(lh);
 -	else
 -		list_add(lh, (struct list_head *) pmd_huge_pte(mm, pmdp));
 -	pmd_huge_pte(mm, pmdp) = pgtable;
 +	for (addr = vma->vm_start; addr < vma->vm_end; addr += PAGE_SIZE)
 +		follow_page(vma, addr, FOLL_SPLIT);
  }
  
 -pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)
 +static inline void thp_split_mm(struct mm_struct *mm)
  {
 -	struct list_head *lh;
 -	pgtable_t pgtable;
 -	pte_t *ptep;
 -
 -	assert_spin_locked(pmd_lockptr(mm, pmdp));
 +	struct vm_area_struct *vma;
  
 -	/* FIFO */
 -	pgtable = pmd_huge_pte(mm, pmdp);
 -	lh = (struct list_head *) pgtable;
 -	if (list_empty(lh))
 -		pmd_huge_pte(mm, pmdp) = NULL;
 -	else {
 -		pmd_huge_pte(mm, pmdp) = (pgtable_t) lh->next;
 -		list_del(lh);
 +	for (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {
 +		thp_split_vma(vma);
 +		vma->vm_flags &= ~VM_HUGEPAGE;
 +		vma->vm_flags |= VM_NOHUGEPAGE;
  	}
 -	ptep = (pte_t *) pgtable;
 -	pte_val(*ptep) = _PAGE_INVALID;
 -	ptep++;
 -	pte_val(*ptep) = _PAGE_INVALID;
 -	return pgtable;
 +	mm->def_flags |= VM_NOHUGEPAGE;
  }
 -#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 -
 -#ifdef CONFIG_PGSTE
 -void ptep_set_pte_at(struct mm_struct *mm, unsigned long addr,
 -		     pte_t *ptep, pte_t entry)
 +#else
 +static inline void thp_split_mm(struct mm_struct *mm)
  {
 -	pgste_t pgste;
 -
 -	/* the mm_has_pgste() check is done in set_pte_at() */
 -	preempt_disable();
 -	pgste = pgste_get_lock(ptep);
 -	pgste_val(pgste) &= ~_PGSTE_GPS_ZERO;
 -	pgste_set_key(ptep, pgste, entry, mm);
 -	pgste = pgste_set_pte(ptep, pgste, entry);
 -	pgste_set_unlock(ptep, pgste);
 -	preempt_enable();
 -}
 -
 -void ptep_set_notify(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
 -{
 -	pgste_t pgste;
 -
 -	preempt_disable();
 -	pgste = pgste_get_lock(ptep);
 -	pgste_val(pgste) |= PGSTE_IN_BIT;
 -	pgste_set_unlock(ptep, pgste);
 -	preempt_enable();
  }
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
  
 -/**
 - * ptep_force_prot - change access rights of a locked pte
 - * @mm: pointer to the process mm_struct
 - * @addr: virtual address in the guest address space
 - * @ptep: pointer to the page table entry
 - * @prot: indicates guest access rights: PROT_NONE, PROT_READ or PROT_WRITE
 - * @bit: pgste bit to set (e.g. for notification)
 - *
 - * Returns 0 if the access rights were changed and -EAGAIN if the current
 - * and requested access rights are incompatible.
 - */
 -int ptep_force_prot(struct mm_struct *mm, unsigned long addr,
 -		    pte_t *ptep, int prot, unsigned long bit)
 +static unsigned long page_table_realloc_pmd(struct mmu_gather *tlb,
 +				struct mm_struct *mm, pud_t *pud,
 +				unsigned long addr, unsigned long end)
  {
 -	pte_t entry;
 -	pgste_t pgste;
 -	int pte_i, pte_p;
 -
 -	pgste = pgste_get_lock(ptep);
 -	entry = *ptep;
 -	/* Check pte entry after all locks have been acquired */
 -	pte_i = pte_val(entry) & _PAGE_INVALID;
 -	pte_p = pte_val(entry) & _PAGE_PROTECT;
 -	if ((pte_i && (prot != PROT_NONE)) ||
 -	    (pte_p && (prot & PROT_WRITE))) {
 -		pgste_set_unlock(ptep, pgste);
 -		return -EAGAIN;
 -	}
 -	/* Change access rights and set pgste bit */
 -	if (prot == PROT_NONE && !pte_i) {
 -		ptep_flush_direct(mm, addr, ptep);
 -		pgste = pgste_update_all(entry, pgste, mm);
 -		pte_val(entry) |= _PAGE_INVALID;
 -	}
 -	if (prot == PROT_READ && !pte_p) {
 -		ptep_flush_direct(mm, addr, ptep);
 -		pte_val(entry) &= ~_PAGE_INVALID;
 -		pte_val(entry) |= _PAGE_PROTECT;
 -	}
 -	pgste_val(pgste) |= bit;
 -	pgste = pgste_set_pte(ptep, pgste, entry);
 -	pgste_set_unlock(ptep, pgste);
 -	return 0;
 -}
 +	unsigned long next, *table, *new;
 +	struct page *page;
 +	pmd_t *pmd;
 +
 +	pmd = pmd_offset(pud, addr);
 +	do {
 +		next = pmd_addr_end(addr, end);
 +again:
 +		if (pmd_none_or_clear_bad(pmd))
 +			continue;
 +		table = (unsigned long *) pmd_deref(*pmd);
 +		page = pfn_to_page(__pa(table) >> PAGE_SHIFT);
 +		if (page_table_with_pgste(page))
 +			continue;
 +		/* Allocate new page table with pgstes */
 +		new = page_table_alloc_pgste(mm, addr);
 +		if (!new)
 +			return -ENOMEM;
 +
 +		spin_lock(&mm->page_table_lock);
 +		if (likely((unsigned long *) pmd_deref(*pmd) == table)) {
 +			/* Nuke pmd entry pointing to the "short" page table */
 +			pmdp_flush_lazy(mm, addr, pmd);
 +			pmd_clear(pmd);
 +			/* Copy ptes from old table to new table */
 +			memcpy(new, table, PAGE_SIZE/2);
 +			clear_table(table, _PAGE_INVALID, PAGE_SIZE/2);
 +			/* Establish new table */
 +			pmd_populate(mm, pmd, (pte_t *) new);
 +			/* Free old table with rcu, there might be a walker! */
 +			page_table_free_rcu(tlb, table);
 +			new = NULL;
 +		}
 +		spin_unlock(&mm->page_table_lock);
 +		if (new) {
 +			page_table_free_pgste(new);
 +			goto again;
 +		}
 +	} while (pmd++, addr = next, addr != end);
  
 -int ptep_shadow_pte(struct mm_struct *mm, unsigned long saddr,
 -		    pte_t *sptep, pte_t *tptep, pte_t pte)
 -{
 -	pgste_t spgste, tpgste;
 -	pte_t spte, tpte;
 -	int rc = -EAGAIN;
 -
 -	if (!(pte_val(*tptep) & _PAGE_INVALID))
 -		return 0;	/* already shadowed */
 -	spgste = pgste_get_lock(sptep);
 -	spte = *sptep;
 -	if (!(pte_val(spte) & _PAGE_INVALID) &&
 -	    !((pte_val(spte) & _PAGE_PROTECT) &&
 -	      !(pte_val(pte) & _PAGE_PROTECT))) {
 -		pgste_val(spgste) |= PGSTE_VSIE_BIT;
 -		tpgste = pgste_get_lock(tptep);
 -		pte_val(tpte) = (pte_val(spte) & PAGE_MASK) |
 -				(pte_val(pte) & _PAGE_PROTECT);
 -		/* don't touch the storage key - it belongs to parent pgste */
 -		tpgste = pgste_set_pte(tptep, tpgste, tpte);
 -		pgste_set_unlock(tptep, tpgste);
 -		rc = 1;
 -	}
 -	pgste_set_unlock(sptep, spgste);
 -	return rc;
 +	return addr;
  }
  
 -void ptep_unshadow_pte(struct mm_struct *mm, unsigned long saddr, pte_t *ptep)
 +static unsigned long page_table_realloc_pud(struct mmu_gather *tlb,
 +				   struct mm_struct *mm, pgd_t *pgd,
 +				   unsigned long addr, unsigned long end)
  {
 -	pgste_t pgste;
 -
 -	pgste = pgste_get_lock(ptep);
 -	/* notifier is called by the caller */
 -	ptep_flush_direct(mm, saddr, ptep);
 -	/* don't touch the storage key - it belongs to parent pgste */
 -	pgste = pgste_set_pte(ptep, pgste, __pte(_PAGE_INVALID));
 -	pgste_set_unlock(ptep, pgste);
 +	unsigned long next;
 +	pud_t *pud;
 +
 +	pud = pud_offset(pgd, addr);
 +	do {
 +		next = pud_addr_end(addr, end);
 +		if (pud_none_or_clear_bad(pud))
 +			continue;
 +		next = page_table_realloc_pmd(tlb, mm, pud, addr, next);
 +		if (unlikely(IS_ERR_VALUE(next)))
 +			return next;
 +	} while (pud++, addr = next, addr != end);
 +
 +	return addr;
  }
  
 -static void ptep_zap_swap_entry(struct mm_struct *mm, swp_entry_t entry)
 +static unsigned long page_table_realloc(struct mmu_gather *tlb, struct mm_struct *mm,
 +					unsigned long addr, unsigned long end)
  {
 -	if (!non_swap_entry(entry))
 -		dec_mm_counter(mm, MM_SWAPENTS);
 -	else if (is_migration_entry(entry)) {
 -		struct page *page = migration_entry_to_page(entry);
 +	unsigned long next;
 +	pgd_t *pgd;
 +
 +	pgd = pgd_offset(mm, addr);
 +	do {
 +		next = pgd_addr_end(addr, end);
 +		if (pgd_none_or_clear_bad(pgd))
 +			continue;
 +		next = page_table_realloc_pud(tlb, mm, pgd, addr, next);
 +		if (unlikely(IS_ERR_VALUE(next)))
 +			return next;
 +	} while (pgd++, addr = next, addr != end);
  
 -		dec_mm_counter(mm, mm_counter(page));
 -	}
 -	free_swap_and_cache(entry);
 +	return 0;
  }
  
 -void ptep_zap_unused(struct mm_struct *mm, unsigned long addr,
 -		     pte_t *ptep, int reset)
 +/*
 + * switch on pgstes for its userspace process (for kvm)
 + */
 +int s390_enable_sie(void)
  {
 -	unsigned long pgstev;
 -	pgste_t pgste;
 -	pte_t pte;
 -
 -	/* Zap unused and logically-zero pages */
 -	preempt_disable();
 -	pgste = pgste_get_lock(ptep);
 -	pgstev = pgste_val(pgste);
 -	pte = *ptep;
 -	if (!reset && pte_swap(pte) &&
 -	    ((pgstev & _PGSTE_GPS_USAGE_MASK) == _PGSTE_GPS_USAGE_UNUSED ||
 -	     (pgstev & _PGSTE_GPS_ZERO))) {
 -		ptep_zap_swap_entry(mm, pte_to_swp_entry(pte));
 -		pte_clear(mm, addr, ptep);
 -	}
 -	if (reset)
 -		pgste_val(pgste) &= ~_PGSTE_GPS_USAGE_MASK;
 -	pgste_set_unlock(ptep, pgste);
 -	preempt_enable();
 +	struct task_struct *tsk = current;
 +	struct mm_struct *mm = tsk->mm;
 +	struct mmu_gather tlb;
 +
 +	/* Do we have pgstes? if yes, we are done */
 +	if (mm_has_pgste(tsk->mm))
 +		return 0;
 +
 +	down_write(&mm->mmap_sem);
 +	/* split thp mappings and disable thp for future mappings */
 +	thp_split_mm(mm);
 +	/* Reallocate the page tables with pgstes */
 +	tlb_gather_mmu(&tlb, mm, 0, TASK_SIZE);
 +	if (!page_table_realloc(&tlb, mm, 0, TASK_SIZE))
 +		mm->context.has_pgste = 1;
 +	tlb_finish_mmu(&tlb, 0, TASK_SIZE);
 +	up_write(&mm->mmap_sem);
 +	return mm->context.has_pgste ? 0 : -ENOMEM;
  }
 +EXPORT_SYMBOL_GPL(s390_enable_sie);
  
 -void ptep_zap_key(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +int pmdp_clear_flush_young(struct vm_area_struct *vma, unsigned long address,
 +			   pmd_t *pmdp)
  {
 -	unsigned long ptev;
 -	pgste_t pgste;
 -
 -	/* Clear storage key */
 -	preempt_disable();
 -	pgste = pgste_get_lock(ptep);
 -	pgste_val(pgste) &= ~(PGSTE_ACC_BITS | PGSTE_FP_BIT |
 -			      PGSTE_GR_BIT | PGSTE_GC_BIT);
 -	ptev = pte_val(*ptep);
 -	if (!(ptev & _PAGE_INVALID) && (ptev & _PAGE_WRITE))
 -		page_set_storage_key(ptev & PAGE_MASK, PAGE_DEFAULT_KEY, 1);
 -	pgste_set_unlock(ptep, pgste);
 -	preempt_enable();
 +	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 +	/* No need to flush TLB
 +	 * On s390 reference bits are in storage key and never in TLB */
 +	return pmdp_test_and_clear_young(vma, address, pmdp);
  }
  
 -/*
 - * Test and reset if a guest page is dirty
 - */
 -bool test_and_clear_guest_dirty(struct mm_struct *mm, unsigned long addr)
 +int pmdp_set_access_flags(struct vm_area_struct *vma,
 +			  unsigned long address, pmd_t *pmdp,
 +			  pmd_t entry, int dirty)
  {
 -	spinlock_t *ptl;
 -	pgste_t pgste;
 -	pte_t *ptep;
 -	pte_t pte;
 -	bool dirty;
 -
 -	ptep = get_locked_pte(mm, addr, &ptl);
 -	if (unlikely(!ptep))
 -		return false;
 -
 -	pgste = pgste_get_lock(ptep);
 -	dirty = !!(pgste_val(pgste) & PGSTE_UC_BIT);
 -	pgste_val(pgste) &= ~PGSTE_UC_BIT;
 -	pte = *ptep;
 -	if (dirty && (pte_val(pte) & _PAGE_PRESENT)) {
 -		pgste = pgste_pte_notify(mm, addr, ptep, pgste);
 -		__ptep_ipte(addr, ptep, IPTE_GLOBAL);
 -		if (MACHINE_HAS_ESOP || !(pte_val(pte) & _PAGE_WRITE))
 -			pte_val(pte) |= _PAGE_PROTECT;
 -		else
 -			pte_val(pte) |= _PAGE_INVALID;
 -		*ptep = pte;
 -	}
 -	pgste_set_unlock(ptep, pgste);
 +	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
  
 -	spin_unlock(ptl);
 -	return dirty;
 +	if (pmd_same(*pmdp, entry))
 +		return 0;
 +	pmdp_invalidate(vma, address, pmdp);
 +	set_pmd_at(vma->vm_mm, address, pmdp, entry);
 +	return 1;
  }
 -EXPORT_SYMBOL_GPL(test_and_clear_guest_dirty);
  
 -int set_guest_storage_key(struct mm_struct *mm, unsigned long addr,
 -			  unsigned char key, bool nq)
 +static void pmdp_splitting_flush_sync(void *arg)
  {
 -	unsigned long keyul;
 -	spinlock_t *ptl;
 -	pgste_t old, new;
 -	pte_t *ptep;
 -
 -	ptep = get_locked_pte(mm, addr, &ptl);
 -	if (unlikely(!ptep))
 -		return -EFAULT;
 -
 -	new = old = pgste_get_lock(ptep);
 -	pgste_val(new) &= ~(PGSTE_GR_BIT | PGSTE_GC_BIT |
 -			    PGSTE_ACC_BITS | PGSTE_FP_BIT);
 -	keyul = (unsigned long) key;
 -	pgste_val(new) |= (keyul & (_PAGE_CHANGED | _PAGE_REFERENCED)) << 48;
 -	pgste_val(new) |= (keyul & (_PAGE_ACC_BITS | _PAGE_FP_BIT)) << 56;
 -	if (!(pte_val(*ptep) & _PAGE_INVALID)) {
 -		unsigned long address, bits, skey;
 -
 -		address = pte_val(*ptep) & PAGE_MASK;
 -		skey = (unsigned long) page_get_storage_key(address);
 -		bits = skey & (_PAGE_CHANGED | _PAGE_REFERENCED);
 -		skey = key & (_PAGE_ACC_BITS | _PAGE_FP_BIT);
 -		/* Set storage key ACC and FP */
 -		page_set_storage_key(address, skey, !nq);
 -		/* Merge host changed & referenced into pgste  */
 -		pgste_val(new) |= bits << 52;
 -	}
 -	/* changing the guest storage key is considered a change of the page */
 -	if ((pgste_val(new) ^ pgste_val(old)) &
 -	    (PGSTE_ACC_BITS | PGSTE_FP_BIT | PGSTE_GR_BIT | PGSTE_GC_BIT))
 -		pgste_val(new) |= PGSTE_UC_BIT;
 -
 -	pgste_set_unlock(ptep, new);
 -	pte_unmap_unlock(ptep, ptl);
 -	return 0;
 +	/* Simply deliver the interrupt */
  }
 -EXPORT_SYMBOL(set_guest_storage_key);
  
 -/**
 - * Conditionally set a guest storage key (handling csske).
 - * oldkey will be updated when either mr or mc is set and a pointer is given.
 - *
 - * Returns 0 if a guests storage key update wasn't necessary, 1 if the guest
 - * storage key was updated and -EFAULT on access errors.
 - */
 -int cond_set_guest_storage_key(struct mm_struct *mm, unsigned long addr,
 -			       unsigned char key, unsigned char *oldkey,
 -			       bool nq, bool mr, bool mc)
 +void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
 +			  pmd_t *pmdp)
  {
 -	unsigned char tmp, mask = _PAGE_ACC_BITS | _PAGE_FP_BIT;
 -	int rc;
 -
 -	/* we can drop the pgste lock between getting and setting the key */
 -	if (mr | mc) {
 -		rc = get_guest_storage_key(current->mm, addr, &tmp);
 -		if (rc)
 -			return rc;
 -		if (oldkey)
 -			*oldkey = tmp;
 -		if (!mr)
 -			mask |= _PAGE_REFERENCED;
 -		if (!mc)
 -			mask |= _PAGE_CHANGED;
 -		if (!((tmp ^ key) & mask))
 -			return 0;
 +	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 +	if (!test_and_set_bit(_SEGMENT_ENTRY_SPLIT_BIT,
 +			      (unsigned long *) pmdp)) {
 +		/* need to serialize against gup-fast (IRQ disabled) */
 +		smp_call_function(pmdp_splitting_flush_sync, NULL, 1);
  	}
 -	rc = set_guest_storage_key(current->mm, addr, key, nq);
 -	return rc < 0 ? rc : 1;
  }
 -EXPORT_SYMBOL(cond_set_guest_storage_key);
  
 -/**
 - * Reset a guest reference bit (rrbe), returning the reference and changed bit.
 - *
 - * Returns < 0 in case of error, otherwise the cc to be reported to the guest.
 - */
 -int reset_guest_reference_bit(struct mm_struct *mm, unsigned long addr)
 +void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 +				pgtable_t pgtable)
  {
 -	spinlock_t *ptl;
 -	pgste_t old, new;
 -	pte_t *ptep;
 -	int cc = 0;
 -
 -	ptep = get_locked_pte(mm, addr, &ptl);
 -	if (unlikely(!ptep))
 -		return -EFAULT;
 -
 -	new = old = pgste_get_lock(ptep);
 -	/* Reset guest reference bit only */
 -	pgste_val(new) &= ~PGSTE_GR_BIT;
 +	struct list_head *lh = (struct list_head *) pgtable;
  
 -	if (!(pte_val(*ptep) & _PAGE_INVALID)) {
 -		cc = page_reset_referenced(pte_val(*ptep) & PAGE_MASK);
 -		/* Merge real referenced bit into host-set */
 -		pgste_val(new) |= ((unsigned long) cc << 53) & PGSTE_HR_BIT;
 -	}
 -	/* Reflect guest's logical view, not physical */
 -	cc |= (pgste_val(old) & (PGSTE_GR_BIT | PGSTE_GC_BIT)) >> 49;
 -	/* Changing the guest storage key is considered a change of the page */
 -	if ((pgste_val(new) ^ pgste_val(old)) & PGSTE_GR_BIT)
 -		pgste_val(new) |= PGSTE_UC_BIT;
 +	assert_spin_locked(&mm->page_table_lock);
  
 -	pgste_set_unlock(ptep, new);
 -	pte_unmap_unlock(ptep, ptl);
 -	return 0;
 +	/* FIFO */
 +	if (!pmd_huge_pte(mm, pmdp))
 +		INIT_LIST_HEAD(lh);
 +	else
 +		list_add(lh, (struct list_head *) pmd_huge_pte(mm, pmdp));
 +	pmd_huge_pte(mm, pmdp) = pgtable;
  }
 -EXPORT_SYMBOL(reset_guest_reference_bit);
  
 -int get_guest_storage_key(struct mm_struct *mm, unsigned long addr,
 -			  unsigned char *key)
 +pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)
  {
 -	spinlock_t *ptl;
 -	pgste_t pgste;
 +	struct list_head *lh;
 +	pgtable_t pgtable;
  	pte_t *ptep;
  
 -	ptep = get_locked_pte(mm, addr, &ptl);
 -	if (unlikely(!ptep))
 -		return -EFAULT;
 +	assert_spin_locked(&mm->page_table_lock);
  
 -	pgste = pgste_get_lock(ptep);
 -	*key = (pgste_val(pgste) & (PGSTE_ACC_BITS | PGSTE_FP_BIT)) >> 56;
 -	if (!(pte_val(*ptep) & _PAGE_INVALID))
 -		*key = page_get_storage_key(pte_val(*ptep) & PAGE_MASK);
 -	/* Reflect guest's logical view, not physical */
 -	*key |= (pgste_val(pgste) & (PGSTE_GR_BIT | PGSTE_GC_BIT)) >> 48;
 -	pgste_set_unlock(ptep, pgste);
 -	pte_unmap_unlock(ptep, ptl);
 -	return 0;
 +	/* FIFO */
 +	pgtable = pmd_huge_pte(mm, pmdp);
 +	lh = (struct list_head *) pgtable;
 +	if (list_empty(lh))
 +		pmd_huge_pte(mm, pmdp) = NULL;
 +	else {
 +		pmd_huge_pte(mm, pmdp) = (pgtable_t) lh->next;
 +		list_del(lh);
 +	}
 +	ptep = (pte_t *) pgtable;
 +	pte_val(*ptep) = _PAGE_INVALID;
 +	ptep++;
 +	pte_val(*ptep) = _PAGE_INVALID;
 +	return pgtable;
  }
 -EXPORT_SYMBOL(get_guest_storage_key);
 -#endif
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
diff --cc arch/s390/mm/vmem.c
index c109b6bb46f8,253046344b3c..000000000000
--- a/arch/s390/mm/vmem.c
+++ b/arch/s390/mm/vmem.c
@@@ -79,8 -77,10 +79,13 @@@ static pte_t __ref *vmem_pte_alloc(unsi
  /*
   * Add a physical memory range to the 1:1 mapping.
   */
 -static int vmem_add_mem(unsigned long start, unsigned long size)
 +static int vmem_add_mem(unsigned long start, unsigned long size, int ro)
  {
++<<<<<<< HEAD
++=======
+ 	unsigned long pgt_prot, sgt_prot, r3_prot;
+ 	unsigned long pages4k, pages1m, pages2g;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  	unsigned long end = start + size;
  	unsigned long address = start;
  	pgd_t *pg_dir;
@@@ -89,6 -89,15 +94,18 @@@
  	pte_t *pt_dir;
  	int ret = -ENOMEM;
  
++<<<<<<< HEAD
++=======
+ 	pgt_prot = pgprot_val(PAGE_KERNEL);
+ 	sgt_prot = pgprot_val(SEGMENT_KERNEL);
+ 	r3_prot = pgprot_val(REGION3_KERNEL);
+ 	if (!MACHINE_HAS_NX) {
+ 		pgt_prot &= ~_PAGE_NOEXEC;
+ 		sgt_prot &= ~_SEGMENT_ENTRY_NOEXEC;
+ 		r3_prot &= ~_REGION_ENTRY_NOEXEC;
+ 	}
+ 	pages4k = pages1m = pages2g = 0;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  	while (address < end) {
  		pg_dir = pgd_offset_k(address);
  		if (pgd_none(*pg_dir)) {
@@@ -98,16 -107,14 +115,22 @@@
  			pgd_populate(&init_mm, pg_dir, pu_dir);
  		}
  		pu_dir = pud_offset(pg_dir, address);
 +#if defined(CONFIG_64BIT) && !defined(CONFIG_DEBUG_PAGEALLOC)
  		if (MACHINE_HAS_EDAT2 && pud_none(*pu_dir) && address &&
++<<<<<<< HEAD
 +		    !(address & ~PUD_MASK) && (address + PUD_SIZE <= end)) {
 +			pud_val(*pu_dir) = __pa(address) |
 +				_REGION_ENTRY_TYPE_R3 | _REGION3_ENTRY_LARGE |
 +				(ro ? _REGION_ENTRY_PROTECT : 0);
++=======
+ 		    !(address & ~PUD_MASK) && (address + PUD_SIZE <= end) &&
+ 		     !debug_pagealloc_enabled()) {
+ 			pud_val(*pu_dir) = address | r3_prot;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  			address += PUD_SIZE;
 -			pages2g++;
  			continue;
  		}
 +#endif
  		if (pud_none(*pu_dir)) {
  			pm_dir = vmem_pmd_alloc();
  			if (!pm_dir)
@@@ -115,28 -122,25 +138,38 @@@
  			pud_populate(&init_mm, pu_dir, pm_dir);
  		}
  		pm_dir = pmd_offset(pu_dir, address);
 +#if defined(CONFIG_64BIT) && !defined(CONFIG_DEBUG_PAGEALLOC)
  		if (MACHINE_HAS_EDAT1 && pmd_none(*pm_dir) && address &&
++<<<<<<< HEAD
 +		    !(address & ~PMD_MASK) && (address + PMD_SIZE <= end)) {
 +			pmd_val(*pm_dir) = __pa(address) |
 +				_SEGMENT_ENTRY | _SEGMENT_ENTRY_LARGE |
 +				_SEGMENT_ENTRY_YOUNG |
 +				(ro ? _SEGMENT_ENTRY_PROTECT : 0);
++=======
+ 		    !(address & ~PMD_MASK) && (address + PMD_SIZE <= end) &&
+ 		    !debug_pagealloc_enabled()) {
+ 			pmd_val(*pm_dir) = address | sgt_prot;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  			address += PMD_SIZE;
 -			pages1m++;
  			continue;
  		}
 +#endif
  		if (pmd_none(*pm_dir)) {
 -			pt_dir = vmem_pte_alloc();
 +			pt_dir = vmem_pte_alloc(address);
  			if (!pt_dir)
  				goto out;
  			pmd_populate(&init_mm, pm_dir, pt_dir);
  		}
  
  		pt_dir = pte_offset_kernel(pm_dir, address);
++<<<<<<< HEAD
 +		pte_val(*pt_dir) = __pa(address) |
 +			pgprot_val(ro ? PAGE_KERNEL_RO : PAGE_KERNEL);
++=======
+ 		pte_val(*pt_dir) = address | pgt_prot;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  		address += PAGE_SIZE;
 -		pages4k++;
  	}
  	ret = 0;
  out:
@@@ -236,9 -254,7 +276,13 @@@ int __meminit vmemmap_populate(unsigne
  				new_page = vmemmap_alloc_block(PMD_SIZE, node);
  				if (!new_page)
  					goto out;
++<<<<<<< HEAD
 +				pmd_val(*pm_dir) = __pa(new_page) |
 +					_SEGMENT_ENTRY | _SEGMENT_ENTRY_LARGE |
 +					_SEGMENT_ENTRY_CO;
++=======
+ 				pmd_val(*pm_dir) = __pa(new_page) | sgt_prot;
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  				address = (address + PMD_SIZE) & PMD_MASK;
  				continue;
  			}
@@@ -254,13 -269,12 +298,12 @@@
  
  		pt_dir = pte_offset_kernel(pm_dir, address);
  		if (pte_none(*pt_dir)) {
 -			void *new_page;
 +			unsigned long new_page;
  
 -			new_page = vmemmap_alloc_block(PAGE_SIZE, node);
 +			new_page =__pa(vmem_alloc_pages(0));
  			if (!new_page)
  				goto out;
- 			pte_val(*pt_dir) =
- 				__pa(new_page) | pgprot_val(PAGE_KERNEL);
+ 			pte_val(*pt_dir) = __pa(new_page) | pgt_prot;
  		}
  		address += PAGE_SIZE;
  	}
@@@ -374,33 -386,21 +417,51 @@@ out
   */
  void __init vmem_map_init(void)
  {
++<<<<<<< HEAD
 +	unsigned long ro_start, ro_end;
 +	unsigned long start, end;
 +	int i;
 +
 +	ro_start = PFN_ALIGN((unsigned long)&_stext);
 +	ro_end = (unsigned long)&_eshared & PAGE_MASK;
 +	for (i = 0; i < MEMORY_CHUNKS; i++) {
 +		if (!memory_chunk[i].size)
 +			continue;
 +		start = memory_chunk[i].addr;
 +		end = memory_chunk[i].addr + memory_chunk[i].size;
 +		if (start >= ro_end || end <= ro_start)
 +			vmem_add_mem(start, end - start, 0);
 +		else if (start >= ro_start && end <= ro_end)
 +			vmem_add_mem(start, end - start, 1);
 +		else if (start >= ro_start) {
 +			vmem_add_mem(start, ro_end - start, 1);
 +			vmem_add_mem(ro_end, end - ro_end, 0);
 +		} else if (end < ro_end) {
 +			vmem_add_mem(start, ro_start - start, 0);
 +			vmem_add_mem(ro_start, end - ro_start, 1);
 +		} else {
 +			vmem_add_mem(start, ro_start - start, 0);
 +			vmem_add_mem(ro_start, ro_end - ro_start, 1);
 +			vmem_add_mem(ro_end, end - ro_end, 0);
 +		}
 +	}
++=======
+ 	struct memblock_region *reg;
+ 
+ 	for_each_memblock(memory, reg)
+ 		vmem_add_mem(reg->base, reg->size);
+ 	__set_memory((unsigned long) _stext,
+ 		     (_etext - _stext) >> PAGE_SHIFT,
+ 		     SET_MEMORY_RO | SET_MEMORY_X);
+ 	__set_memory((unsigned long) _etext,
+ 		     (_eshared - _etext) >> PAGE_SHIFT,
+ 		     SET_MEMORY_RO);
+ 	__set_memory((unsigned long) _sinittext,
+ 		     (_einittext - _sinittext) >> PAGE_SHIFT,
+ 		     SET_MEMORY_RO | SET_MEMORY_X);
+ 	pr_info("Write protected kernel read-only data: %luk\n",
+ 		(_eshared - _stext) >> 10);
++>>>>>>> 57d7f939e7bd (s390: add no-execute support)
  }
  
  /*
* Unmerged path arch/s390/include/asm/cacheflush.h
* Unmerged path arch/s390/include/asm/pgtable.h
* Unmerged path arch/s390/include/asm/setup.h
* Unmerged path arch/s390/kernel/early.c
* Unmerged path arch/s390/kernel/entry.S
diff --git a/arch/s390/kernel/kprobes.c b/arch/s390/kernel/kprobes.c
index 0a5b1c04e6ef..314894d8d0f6 100644
--- a/arch/s390/kernel/kprobes.c
+++ b/arch/s390/kernel/kprobes.c
@@ -42,11 +42,17 @@ DEFINE_INSN_CACHE_OPS(dmainsn);
 
 static void *alloc_dmainsn_page(void)
 {
-	return (void *)__get_free_page(GFP_KERNEL | GFP_DMA);
+	void *page;
+
+	page = (void *) __get_free_page(GFP_KERNEL | GFP_DMA);
+	if (page)
+		set_memory_x((unsigned long) page, 1);
+	return page;
 }
 
 static void free_dmainsn_page(void *page)
 {
+	set_memory_nx((unsigned long) page, 1);
 	free_page((unsigned long)page);
 }
 
* Unmerged path arch/s390/kernel/module.c
diff --git a/arch/s390/kernel/vmlinux.lds.S b/arch/s390/kernel/vmlinux.lds.S
index a6f622efba8b..9e27541df126 100644
--- a/arch/s390/kernel/vmlinux.lds.S
+++ b/arch/s390/kernel/vmlinux.lds.S
@@ -53,6 +53,7 @@ SECTIONS
 		*(.gnu.warning)
 	} :text = 0x0700
 
+	. = ALIGN(PAGE_SIZE);
 	_etext = .;		/* End of text section */
 
 	NOTES :text :note
@@ -79,7 +80,13 @@ SECTIONS
 	. = ALIGN(PAGE_SIZE);	/* Init code and data */
 	__init_begin = .;
 
-	INIT_TEXT_SECTION(PAGE_SIZE)
+	. = ALIGN(PAGE_SIZE);
+	.init.text : AT(ADDR(.init.text) - LOAD_OFFSET) {
+		VMLINUX_SYMBOL(_sinittext) = . ;
+		INIT_TEXT
+		. = ALIGN(PAGE_SIZE);
+		VMLINUX_SYMBOL(_einittext) = . ;
+	}
 
 	/*
 	 * .exit.text is discarded at runtime, not link time,
* Unmerged path arch/s390/mm/dump_pagetables.c
diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c
index 9aa3443fbf65..ec30d882cc90 100644
--- a/arch/s390/mm/fault.c
+++ b/arch/s390/mm/fault.c
@@ -214,12 +214,34 @@ static noinline void do_sigbus(struct pt_regs *regs)
 	force_sig_info(SIGBUS, &si, tsk);
 }
 
-static noinline void do_fault_error(struct pt_regs *regs, int fault)
+static noinline int signal_return(struct pt_regs *regs)
+{
+	u16 instruction;
+	int rc;
+
+	rc = __get_user(instruction, (u16 __user *) regs->psw.addr);
+	if (rc)
+		return rc;
+	if (instruction == 0x0a77) {
+		set_pt_regs_flag(regs, PIF_SYSCALL);
+		regs->int_code = 0x00040077;
+		return 0;
+	} else if (instruction == 0x0aad) {
+		set_pt_regs_flag(regs, PIF_SYSCALL);
+		regs->int_code = 0x000400ad;
+		return 0;
+	}
+	return -EACCES;
+}
+
+static noinline void do_fault_error(struct pt_regs *regs, int access, int fault)
 {
 	int si_code;
 
 	switch (fault) {
 	case VM_FAULT_BADACCESS:
+		if (access == VM_EXEC && signal_return(regs) == 0)
+			break;
 	case VM_FAULT_BADMAP:
 		/* Bad memory access. Check if it is kernel or user space. */
 		if (user_mode(regs)) {
@@ -227,7 +249,7 @@ static noinline void do_fault_error(struct pt_regs *regs, int fault)
 			si_code = (fault == VM_FAULT_BADMAP) ?
 				SEGV_MAPERR : SEGV_ACCERR;
 			do_sigsegv(regs, si_code);
-			return;
+			break;
 		}
 	case VM_FAULT_BADCONTEXT:
 		do_no_context(regs);
@@ -392,7 +414,7 @@ out:
 void __kprobes do_protection_exception(struct pt_regs *regs)
 {
 	unsigned long trans_exc_code;
-	int fault;
+	int access, fault;
 
 	trans_exc_code = regs->int_parm_long;
 	/*
@@ -411,9 +433,17 @@ void __kprobes do_protection_exception(struct pt_regs *regs)
 		do_low_address(regs);
 		return;
 	}
-	fault = do_exception(regs, VM_WRITE);
+	if (unlikely(MACHINE_HAS_NX && (trans_exc_code & 0x80))) {
+		regs->int_parm_long = (trans_exc_code & ~PAGE_MASK) |
+					(regs->psw.addr & PAGE_MASK);
+		access = VM_EXEC;
+		fault = VM_FAULT_BADACCESS;
+	} else {
+		access = VM_WRITE;
+		fault = do_exception(regs, access);
+	}
 	if (unlikely(fault))
-		do_fault_error(regs, fault);
+		do_fault_error(regs, access, fault);
 }
 
 void __kprobes do_dat_exception(struct pt_regs *regs)
@@ -423,7 +453,7 @@ void __kprobes do_dat_exception(struct pt_regs *regs)
 	access = VM_READ | VM_EXEC | VM_WRITE;
 	fault = do_exception(regs, access);
 	if (unlikely(fault))
-		do_fault_error(regs, fault);
+		do_fault_error(regs, access, fault);
 }
 
 #ifdef CONFIG_PFAULT 
* Unmerged path arch/s390/mm/hugetlbpage.c
* Unmerged path arch/s390/mm/init.c
* Unmerged path arch/s390/mm/pageattr.c
* Unmerged path arch/s390/mm/pgtable.c
* Unmerged path arch/s390/mm/vmem.c
