bnxt: Fix typo in comments

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] bnx2x: fix typos in comment (Michal Schmidt) [1479145]
Rebuild_FUZZ: 90.57%
commit-author Jiang Jian <jiangjian@cdjrlc.com>
commit c909e7ca494f397f51648048252d00d3dd61cefd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c909e7ca.failed

Remove the repeated word 'and' from comments

	Signed-off-by: Jiang Jian <jiangjian@cdjrlc.com>
Link: https://lore.kernel.org/r/20220622144526.20659-1-jiangjian@cdjrlc.com
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit c909e7ca494f397f51648048252d00d3dd61cefd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/broadcom/bnxt/bnxt.c
diff --cc drivers/net/ethernet/broadcom/bnxt/bnxt.c
index bdacd982a1af,b474a4fe4039..000000000000
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
@@@ -5581,769 -11438,914 +5581,845 @@@ int bnxt_hwrm_set_pause(struct bnxt *bp
  	return rc;
  }
  
 -static int bnxt_dbg_hwrm_ring_info_get(struct bnxt *bp, u8 ring_type,
 -				       u32 ring_id, u32 *prod, u32 *cons)
 +static void bnxt_hwrm_set_eee(struct bnxt *bp,
 +			      struct hwrm_port_phy_cfg_input *req)
  {
 -	struct hwrm_dbg_ring_info_get_output *resp;
 -	struct hwrm_dbg_ring_info_get_input *req;
 -	int rc;
 +	struct ethtool_eee *eee = &bp->eee;
  
 -	rc = hwrm_req_init(bp, req, HWRM_DBG_RING_INFO_GET);
 -	if (rc)
 -		return rc;
 +	if (eee->eee_enabled) {
 +		u16 eee_speeds;
 +		u32 flags = PORT_PHY_CFG_REQ_FLAGS_EEE_ENABLE;
  
 -	req->ring_type = ring_type;
 -	req->fw_ring_id = cpu_to_le32(ring_id);
 -	resp = hwrm_req_hold(bp, req);
 -	rc = hwrm_req_send(bp, req);
 -	if (!rc) {
 -		*prod = le32_to_cpu(resp->producer_index);
 -		*cons = le32_to_cpu(resp->consumer_index);
 +		if (eee->tx_lpi_enabled)
 +			flags |= PORT_PHY_CFG_REQ_FLAGS_EEE_TX_LPI_ENABLE;
 +		else
 +			flags |= PORT_PHY_CFG_REQ_FLAGS_EEE_TX_LPI_DISABLE;
 +
 +		req->flags |= cpu_to_le32(flags);
 +		eee_speeds = bnxt_get_fw_auto_link_speeds(eee->advertised);
 +		req->eee_link_speed_mask = cpu_to_le16(eee_speeds);
 +		req->tx_lpi_timer = cpu_to_le32(eee->tx_lpi_timer);
 +	} else {
 +		req->flags |= cpu_to_le32(PORT_PHY_CFG_REQ_FLAGS_EEE_DISABLE);
  	}
 -	hwrm_req_drop(bp, req);
 -	return rc;
  }
  
 -static void bnxt_dump_tx_sw_state(struct bnxt_napi *bnapi)
 +int bnxt_hwrm_set_link_setting(struct bnxt *bp, bool set_pause, bool set_eee)
  {
 -	struct bnxt_tx_ring_info *txr = bnapi->tx_ring;
 -	int i = bnapi->index;
 +	struct hwrm_port_phy_cfg_input req = {0};
  
 -	if (!txr)
 -		return;
 +	bnxt_hwrm_cmd_hdr_init(bp, &req, HWRM_PORT_PHY_CFG, -1, -1);
 +	if (set_pause)
 +		bnxt_hwrm_set_pause_common(bp, &req);
  
 -	netdev_info(bnapi->bp->dev, "[%d]: tx{fw_ring: %d prod: %x cons: %x}\n",
 -		    i, txr->tx_ring_struct.fw_ring_id, txr->tx_prod,
 -		    txr->tx_cons);
 +	bnxt_hwrm_set_link_common(bp, &req);
 +
 +	if (set_eee)
 +		bnxt_hwrm_set_eee(bp, &req);
 +	return hwrm_send_message(bp, &req, sizeof(req), HWRM_CMD_TIMEOUT);
  }
  
 -static void bnxt_dump_rx_sw_state(struct bnxt_napi *bnapi)
 +static int bnxt_hwrm_shutdown_link(struct bnxt *bp)
  {
 -	struct bnxt_rx_ring_info *rxr = bnapi->rx_ring;
 -	int i = bnapi->index;
 +	struct hwrm_port_phy_cfg_input req = {0};
  
 -	if (!rxr)
 -		return;
 +	if (!BNXT_SINGLE_PF(bp))
 +		return 0;
  
 -	netdev_info(bnapi->bp->dev, "[%d]: rx{fw_ring: %d prod: %x} rx_agg{fw_ring: %d agg_prod: %x sw_agg_prod: %x}\n",
 -		    i, rxr->rx_ring_struct.fw_ring_id, rxr->rx_prod,
 -		    rxr->rx_agg_ring_struct.fw_ring_id, rxr->rx_agg_prod,
 -		    rxr->rx_sw_agg_prod);
 +	if (pci_num_vf(bp->pdev))
 +		return 0;
 +
 +	bnxt_hwrm_cmd_hdr_init(bp, &req, HWRM_PORT_PHY_CFG, -1, -1);
 +	req.flags = cpu_to_le32(PORT_PHY_CFG_REQ_FLAGS_FORCE_LINK_DWN);
 +	return hwrm_send_message(bp, &req, sizeof(req), HWRM_CMD_TIMEOUT);
  }
  
 -static void bnxt_dump_cp_sw_state(struct bnxt_napi *bnapi)
 +static int bnxt_hwrm_port_led_qcaps(struct bnxt *bp)
  {
 -	struct bnxt_cp_ring_info *cpr = &bnapi->cp_ring;
 -	int i = bnapi->index;
 +	struct hwrm_port_led_qcaps_output *resp = bp->hwrm_cmd_resp_addr;
 +	struct hwrm_port_led_qcaps_input req = {0};
 +	struct bnxt_pf_info *pf = &bp->pf;
 +	int rc;
  
 -	netdev_info(bnapi->bp->dev, "[%d]: cp{fw_ring: %d raw_cons: %x}\n",
 -		    i, cpr->cp_ring_struct.fw_ring_id, cpr->cp_raw_cons);
 +	if (BNXT_VF(bp) || bp->hwrm_spec_code < 0x10601)
 +		return 0;
 +
 +	bnxt_hwrm_cmd_hdr_init(bp, &req, HWRM_PORT_LED_QCAPS, -1, -1);
 +	req.port_id = cpu_to_le16(pf->port_id);
 +	mutex_lock(&bp->hwrm_cmd_lock);
 +	rc = _hwrm_send_message(bp, &req, sizeof(req), HWRM_CMD_TIMEOUT);
 +	if (rc) {
 +		mutex_unlock(&bp->hwrm_cmd_lock);
 +		return rc;
 +	}
 +	if (resp->num_leds > 0 && resp->num_leds < BNXT_MAX_LED) {
 +		int i;
 +
 +		bp->num_leds = resp->num_leds;
 +		memcpy(bp->leds, &resp->led0_id, sizeof(bp->leds[0]) *
 +						 bp->num_leds);
 +		for (i = 0; i < bp->num_leds; i++) {
 +			struct bnxt_led_info *led = &bp->leds[i];
 +			__le16 caps = led->led_state_caps;
 +
 +			if (!led->led_group_id ||
 +			    !BNXT_LED_ALT_BLINK_CAP(caps)) {
 +				bp->num_leds = 0;
 +				break;
 +			}
 +		}
 +	}
 +	mutex_unlock(&bp->hwrm_cmd_lock);
 +	return 0;
  }
  
 -static void bnxt_dbg_dump_states(struct bnxt *bp)
 +static bool bnxt_eee_config_ok(struct bnxt *bp)
  {
 -	int i;
 -	struct bnxt_napi *bnapi;
 +	struct ethtool_eee *eee = &bp->eee;
 +	struct bnxt_link_info *link_info = &bp->link_info;
  
 -	for (i = 0; i < bp->cp_nr_rings; i++) {
 -		bnapi = bp->bnapi[i];
 -		if (netif_msg_drv(bp)) {
 -			bnxt_dump_tx_sw_state(bnapi);
 -			bnxt_dump_rx_sw_state(bnapi);
 -			bnxt_dump_cp_sw_state(bnapi);
 +	if (!(bp->flags & BNXT_FLAG_EEE_CAP))
 +		return true;
 +
 +	if (eee->eee_enabled) {
 +		u32 advertising =
 +			_bnxt_fw_to_ethtool_adv_spds(link_info->advertising, 0);
 +
 +		if (!(link_info->autoneg & BNXT_AUTONEG_SPEED)) {
 +			eee->eee_enabled = 0;
 +			return false;
 +		}
 +		if (eee->advertised & ~advertising) {
 +			eee->advertised = advertising & eee->supported;
 +			return false;
  		}
  	}
 +	return true;
  }
  
 -static int bnxt_hwrm_rx_ring_reset(struct bnxt *bp, int ring_nr)
 +static int bnxt_update_phy_setting(struct bnxt *bp)
  {
 -	struct bnxt_rx_ring_info *rxr = &bp->rx_ring[ring_nr];
 -	struct hwrm_ring_reset_input *req;
 -	struct bnxt_napi *bnapi = rxr->bnapi;
 -	struct bnxt_cp_ring_info *cpr;
 -	u16 cp_ring_id;
  	int rc;
 +	bool update_link = false;
 +	bool update_pause = false;
 +	bool update_eee = false;
 +	struct bnxt_link_info *link_info = &bp->link_info;
  
 -	rc = hwrm_req_init(bp, req, HWRM_RING_RESET);
 -	if (rc)
 +	rc = bnxt_update_link(bp, true);
 +	if (rc) {
 +		netdev_err(bp->dev, "failed to update link (rc: %x)\n",
 +			   rc);
  		return rc;
 +	}
 +	if (!BNXT_SINGLE_PF(bp))
 +		return 0;
  
 -	cpr = &bnapi->cp_ring;
 -	cp_ring_id = cpr->cp_ring_struct.fw_ring_id;
 -	req->cmpl_ring = cpu_to_le16(cp_ring_id);
 -	req->ring_type = RING_RESET_REQ_RING_TYPE_RX_RING_GRP;
 -	req->ring_id = cpu_to_le16(bp->grp_info[bnapi->index].fw_grp_id);
 -	return hwrm_req_send_silent(bp, req);
 -}
 +	if ((link_info->autoneg & BNXT_AUTONEG_FLOW_CTRL) &&
 +	    (link_info->auto_pause_setting & BNXT_LINK_PAUSE_BOTH) !=
 +	    link_info->req_flow_ctrl)
 +		update_pause = true;
 +	if (!(link_info->autoneg & BNXT_AUTONEG_FLOW_CTRL) &&
 +	    link_info->force_pause_setting != link_info->req_flow_ctrl)
 +		update_pause = true;
 +	if (!(link_info->autoneg & BNXT_AUTONEG_SPEED)) {
 +		if (BNXT_AUTO_MODE(link_info->auto_mode))
 +			update_link = true;
 +		if (link_info->req_link_speed != link_info->force_link_speed)
 +			update_link = true;
 +		if (link_info->req_duplex != link_info->duplex_setting)
 +			update_link = true;
 +	} else {
 +		if (link_info->auto_mode == BNXT_LINK_AUTO_NONE)
 +			update_link = true;
 +		if (link_info->advertising != link_info->auto_link_speeds)
 +			update_link = true;
 +	}
  
 -static void bnxt_reset_task(struct bnxt *bp, bool silent)
 -{
 -	if (!silent)
 -		bnxt_dbg_dump_states(bp);
 -	if (netif_running(bp->dev)) {
 -		int rc;
 +	/* The last close may have shutdown the link, so need to call
 +	 * PHY_CFG to bring it back up.
 +	 */
 +	if (!netif_carrier_ok(bp->dev))
 +		update_link = true;
  
 -		if (silent) {
 -			bnxt_close_nic(bp, false, false);
 -			bnxt_open_nic(bp, false, false);
 -		} else {
 -			bnxt_ulp_stop(bp);
 -			bnxt_close_nic(bp, true, false);
 -			rc = bnxt_open_nic(bp, true, false);
 -			bnxt_ulp_start(bp, rc);
 -		}
 +	if (!bnxt_eee_config_ok(bp))
 +		update_eee = true;
 +
 +	if (update_link)
 +		rc = bnxt_hwrm_set_link_setting(bp, update_pause, update_eee);
 +	else if (update_pause)
 +		rc = bnxt_hwrm_set_pause(bp);
 +	if (rc) {
 +		netdev_err(bp->dev, "failed to update phy setting (rc: %x)\n",
 +			   rc);
 +		return rc;
  	}
 +
 +	return rc;
  }
  
 -static void bnxt_tx_timeout(struct net_device *dev, unsigned int txqueue)
 +/* Common routine to pre-map certain register block to different GRC window.
 + * A PF has 16 4K windows and a VF has 4 4K windows. However, only 15 windows
 + * in PF and 3 windows in VF that can be customized to map in different
 + * register blocks.
 + */
 +static void bnxt_preset_reg_win(struct bnxt *bp)
  {
 -	struct bnxt *bp = netdev_priv(dev);
 -
 -	netdev_err(bp->dev,  "TX timeout detected, starting reset task!\n");
 -	set_bit(BNXT_RESET_TASK_SP_EVENT, &bp->sp_event);
 -	bnxt_queue_sp_work(bp);
 +	if (BNXT_PF(bp)) {
 +		/* CAG registers map to GRC window #4 */
 +		writel(BNXT_CAG_REG_BASE,
 +		       bp->bar0 + BNXT_GRCPF_REG_WINDOW_BASE_OUT + 12);
 +	}
  }
  
 -static void bnxt_fw_health_check(struct bnxt *bp)
 +static int __bnxt_open_nic(struct bnxt *bp, bool irq_re_init, bool link_re_init)
  {
 -	struct bnxt_fw_health *fw_health = bp->fw_health;
 -	u32 val;
 +	int rc = 0;
  
 -	if (!fw_health->enabled || test_bit(BNXT_STATE_IN_FW_RESET, &bp->state))
 -		return;
 +	bnxt_preset_reg_win(bp);
 +	netif_carrier_off(bp->dev);
 +	if (irq_re_init) {
 +		rc = bnxt_setup_int_mode(bp);
 +		if (rc) {
 +			netdev_err(bp->dev, "bnxt_setup_int_mode err: %x\n",
 +				   rc);
 +			return rc;
 +		}
 +	}
 +	if ((bp->flags & BNXT_FLAG_RFS) &&
 +	    !(bp->flags & BNXT_FLAG_USING_MSIX)) {
 +		/* disable RFS if falling back to INTA */
 +		bp->dev->hw_features &= ~NETIF_F_NTUPLE;
 +		bp->flags &= ~BNXT_FLAG_RFS;
 +	}
  
 -	/* Make sure it is enabled before checking the tmr_counter. */
 -	smp_rmb();
 -	if (fw_health->tmr_counter) {
 -		fw_health->tmr_counter--;
 -		return;
 +	rc = bnxt_alloc_mem(bp, irq_re_init);
 +	if (rc) {
 +		netdev_err(bp->dev, "bnxt_alloc_mem err: %x\n", rc);
 +		goto open_err_free_mem;
  	}
  
 -	val = bnxt_fw_health_readl(bp, BNXT_FW_HEARTBEAT_REG);
 -	if (val == fw_health->last_fw_heartbeat) {
 -		fw_health->arrests++;
 -		goto fw_reset;
 +	if (irq_re_init) {
 +		bnxt_init_napi(bp);
 +		rc = bnxt_request_irq(bp);
 +		if (rc) {
 +			netdev_err(bp->dev, "bnxt_request_irq err: %x\n", rc);
 +			goto open_err;
 +		}
  	}
  
 -	fw_health->last_fw_heartbeat = val;
 +	bnxt_enable_napi(bp);
  
 -	val = bnxt_fw_health_readl(bp, BNXT_FW_RESET_CNT_REG);
 -	if (val != fw_health->last_fw_reset_cnt) {
 -		fw_health->discoveries++;
 -		goto fw_reset;
 +	rc = bnxt_init_nic(bp, irq_re_init);
 +	if (rc) {
 +		netdev_err(bp->dev, "bnxt_init_nic err: %x\n", rc);
 +		goto open_err;
  	}
  
 -	fw_health->tmr_counter = fw_health->tmr_multiplier;
 -	return;
 +	if (link_re_init) {
 +		rc = bnxt_update_phy_setting(bp);
 +		if (rc)
 +			netdev_warn(bp->dev, "failed to update phy settings\n");
 +	}
  
 -fw_reset:
 -	set_bit(BNXT_FW_EXCEPTION_SP_EVENT, &bp->sp_event);
 -	bnxt_queue_sp_work(bp);
 -}
 +	if (irq_re_init)
 +		udp_tunnel_get_rx_info(bp->dev);
  
 -static void bnxt_timer(struct timer_list *t)
 -{
 -	struct bnxt *bp = from_timer(bp, t, timer);
 -	struct net_device *dev = bp->dev;
 +	set_bit(BNXT_STATE_OPEN, &bp->state);
 +	bnxt_enable_int(bp);
 +	/* Enable TX queues */
 +	bnxt_tx_enable(bp);
 +	mod_timer(&bp->timer, jiffies + bp->current_interval);
 +	/* Poll link status and check for SFP+ module status */
 +	bnxt_get_port_module_status(bp);
  
 -	if (!netif_running(dev) || !test_bit(BNXT_STATE_OPEN, &bp->state))
 -		return;
 +	return 0;
  
 -	if (atomic_read(&bp->intr_sem) != 0)
 -		goto bnxt_restart_timer;
 +open_err:
 +	bnxt_disable_napi(bp);
 +	bnxt_del_napi(bp);
 +
 +open_err_free_mem:
 +	bnxt_free_skbs(bp);
 +	bnxt_free_irq(bp);
 +	bnxt_free_mem(bp, true);
 +	return rc;
 +}
  
 -	if (bp->fw_cap & BNXT_FW_CAP_ERROR_RECOVERY)
 -		bnxt_fw_health_check(bp);
 +/* rtnl_lock held */
 +int bnxt_open_nic(struct bnxt *bp, bool irq_re_init, bool link_re_init)
 +{
 +	int rc = 0;
  
 -	if (BNXT_LINK_IS_UP(bp) && bp->stats_coal_ticks) {
 -		set_bit(BNXT_PERIODIC_STATS_SP_EVENT, &bp->sp_event);
 -		bnxt_queue_sp_work(bp);
 +	rc = __bnxt_open_nic(bp, irq_re_init, link_re_init);
 +	if (rc) {
 +		netdev_err(bp->dev, "nic open fail (rc: %x)\n", rc);
 +		dev_close(bp->dev);
  	}
 +	return rc;
 +}
 +
 +static int bnxt_open(struct net_device *dev)
 +{
 +	struct bnxt *bp = netdev_priv(dev);
  
 -	if (bnxt_tc_flower_enabled(bp)) {
 -		set_bit(BNXT_FLOW_STATS_SP_EVENT, &bp->sp_event);
 -		bnxt_queue_sp_work(bp);
++<<<<<<< HEAD
 +	return __bnxt_open_nic(bp, true, true);
++=======
++	if (test_bit(BNXT_STATE_ABORT_ERR, &bp->state)) {
++		rc = bnxt_reinit_after_abort(bp);
++		if (rc) {
++			if (rc == -EBUSY)
++				netdev_err(bp->dev, "A previous firmware reset has not completed, aborting\n");
++			else
++				netdev_err(bp->dev, "Failed to reinitialize after aborted firmware reset\n");
++			return -ENODEV;
++		}
+ 	}
+ 
 -#ifdef CONFIG_RFS_ACCEL
 -	if ((bp->flags & BNXT_FLAG_RFS) && bp->ntp_fltr_count) {
 -		set_bit(BNXT_RX_NTP_FLTR_SP_EVENT, &bp->sp_event);
 -		bnxt_queue_sp_work(bp);
 -	}
 -#endif /*CONFIG_RFS_ACCEL*/
++	rc = bnxt_hwrm_if_change(bp, true);
++	if (rc)
++		return rc;
+ 
 -	if (bp->link_info.phy_retry) {
 -		if (time_after(jiffies, bp->link_info.phy_retry_expires)) {
 -			bp->link_info.phy_retry = false;
 -			netdev_warn(bp->dev, "failed to update phy settings after maximum retries.\n");
 -		} else {
 -			set_bit(BNXT_UPDATE_PHY_SP_EVENT, &bp->sp_event);
 -			bnxt_queue_sp_work(bp);
++	rc = __bnxt_open_nic(bp, true, true);
++	if (rc) {
++		bnxt_hwrm_if_change(bp, false);
++	} else {
++		if (test_and_clear_bit(BNXT_STATE_FW_RESET_DET, &bp->state)) {
++			if (!test_bit(BNXT_STATE_IN_FW_RESET, &bp->state)) {
++				bnxt_ulp_start(bp, 0);
++				bnxt_reenable_sriov(bp);
++			}
+ 		}
++		bnxt_hwmon_open(bp);
+ 	}
+ 
 -	if (test_bit(BNXT_STATE_L2_FILTER_RETRY, &bp->state)) {
 -		set_bit(BNXT_RX_MASK_SP_EVENT, &bp->sp_event);
 -		bnxt_queue_sp_work(bp);
 -	}
 -
 -	if ((bp->flags & BNXT_FLAG_CHIP_P5) && !bp->chip_rev &&
 -	    netif_carrier_ok(dev)) {
 -		set_bit(BNXT_RING_COAL_NOW_SP_EVENT, &bp->sp_event);
 -		bnxt_queue_sp_work(bp);
 -	}
 -bnxt_restart_timer:
 -	mod_timer(&bp->timer, jiffies + bp->current_interval);
++	return rc;
+ }
+ 
 -static void bnxt_rtnl_lock_sp(struct bnxt *bp)
++static bool bnxt_drv_busy(struct bnxt *bp)
+ {
 -	/* We are called from bnxt_sp_task which has BNXT_STATE_IN_SP_TASK
 -	 * set.  If the device is being closed, bnxt_close() may be holding
 -	 * rtnl() and waiting for BNXT_STATE_IN_SP_TASK to clear.  So we
 -	 * must clear BNXT_STATE_IN_SP_TASK before holding rtnl().
 -	 */
 -	clear_bit(BNXT_STATE_IN_SP_TASK, &bp->state);
 -	rtnl_lock();
++	return (test_bit(BNXT_STATE_IN_SP_TASK, &bp->state) ||
++		test_bit(BNXT_STATE_READ_STATS, &bp->state));
+ }
+ 
 -static void bnxt_rtnl_unlock_sp(struct bnxt *bp)
 -{
 -	set_bit(BNXT_STATE_IN_SP_TASK, &bp->state);
 -	rtnl_unlock();
 -}
++static void bnxt_get_ring_stats(struct bnxt *bp,
++				struct rtnl_link_stats64 *stats);
+ 
 -/* Only called from bnxt_sp_task() */
 -static void bnxt_reset(struct bnxt *bp, bool silent)
++static void __bnxt_close_nic(struct bnxt *bp, bool irq_re_init,
++			     bool link_re_init)
+ {
 -	bnxt_rtnl_lock_sp(bp);
 -	if (test_bit(BNXT_STATE_OPEN, &bp->state))
 -		bnxt_reset_task(bp, silent);
 -	bnxt_rtnl_unlock_sp(bp);
 -}
++	/* Close the VF-reps before closing PF */
++	if (BNXT_PF(bp))
++		bnxt_vf_reps_close(bp);
+ 
 -/* Only called from bnxt_sp_task() */
 -static void bnxt_rx_ring_reset(struct bnxt *bp)
 -{
 -	int i;
++	/* Change device state to avoid TX queue wake up's */
++	bnxt_tx_disable(bp);
+ 
 -	bnxt_rtnl_lock_sp(bp);
 -	if (!test_bit(BNXT_STATE_OPEN, &bp->state)) {
 -		bnxt_rtnl_unlock_sp(bp);
 -		return;
 -	}
 -	/* Disable and flush TPA before resetting the RX ring */
 -	if (bp->flags & BNXT_FLAG_TPA)
 -		bnxt_set_tpa(bp, false);
 -	for (i = 0; i < bp->rx_nr_rings; i++) {
 -		struct bnxt_rx_ring_info *rxr = &bp->rx_ring[i];
 -		struct bnxt_cp_ring_info *cpr;
 -		int rc;
++	clear_bit(BNXT_STATE_OPEN, &bp->state);
++	smp_mb__after_atomic();
++	while (bnxt_drv_busy(bp))
++		msleep(20);
+ 
 -		if (!rxr->bnapi->in_reset)
 -			continue;
++	/* Flush rings and disable interrupts */
++	bnxt_shutdown_nic(bp, irq_re_init);
+ 
 -		rc = bnxt_hwrm_rx_ring_reset(bp, i);
 -		if (rc) {
 -			if (rc == -EINVAL || rc == -EOPNOTSUPP)
 -				netdev_info_once(bp->dev, "RX ring reset not supported by firmware, falling back to global reset\n");
 -			else
 -				netdev_warn(bp->dev, "RX ring reset failed, rc = %d, falling back to global reset\n",
 -					    rc);
 -			bnxt_reset_task(bp, true);
 -			break;
 -		}
 -		bnxt_free_one_rx_ring_skbs(bp, i);
 -		rxr->rx_prod = 0;
 -		rxr->rx_agg_prod = 0;
 -		rxr->rx_sw_agg_prod = 0;
 -		rxr->rx_next_cons = 0;
 -		rxr->bnapi->in_reset = false;
 -		bnxt_alloc_one_rx_ring(bp, i);
 -		cpr = &rxr->bnapi->cp_ring;
 -		cpr->sw_stats.rx.rx_resets++;
 -		if (bp->flags & BNXT_FLAG_AGG_RINGS)
 -			bnxt_db_write(bp, &rxr->rx_agg_db, rxr->rx_agg_prod);
 -		bnxt_db_write(bp, &rxr->rx_db, rxr->rx_prod);
 -	}
 -	if (bp->flags & BNXT_FLAG_TPA)
 -		bnxt_set_tpa(bp, true);
 -	bnxt_rtnl_unlock_sp(bp);
 -}
++	/* TODO CHIMP_FW: Link/PHY related cleanup if (link_re_init) */
+ 
 -static void bnxt_fw_reset_close(struct bnxt *bp)
 -{
 -	bnxt_ulp_stop(bp);
 -	/* When firmware is in fatal state, quiesce device and disable
 -	 * bus master to prevent any potential bad DMAs before freeing
 -	 * kernel memory.
 -	 */
 -	if (test_bit(BNXT_STATE_FW_FATAL_COND, &bp->state)) {
 -		u16 val = 0;
 -
 -		pci_read_config_word(bp->pdev, PCI_SUBSYSTEM_ID, &val);
 -		if (val == 0xffff)
 -			bp->fw_reset_min_dsecs = 0;
 -		bnxt_tx_disable(bp);
 -		bnxt_disable_napi(bp);
 -		bnxt_disable_int_sync(bp);
++	bnxt_debug_dev_exit(bp);
++	bnxt_disable_napi(bp);
++	del_timer_sync(&bp->timer);
++	bnxt_free_skbs(bp);
++
++	/* Save ring stats before shutdown */
++	if (bp->bnapi && irq_re_init)
++		bnxt_get_ring_stats(bp, &bp->net_stats_prev);
++	if (irq_re_init) {
+ 		bnxt_free_irq(bp);
 -		bnxt_clear_int_mode(bp);
 -		pci_disable_device(bp->pdev);
++		bnxt_del_napi(bp);
+ 	}
 -	__bnxt_close_nic(bp, true, false);
 -	bnxt_vf_reps_free(bp);
 -	bnxt_clear_int_mode(bp);
 -	bnxt_hwrm_func_drv_unrgtr(bp);
 -	if (pci_is_enabled(bp->pdev))
 -		pci_disable_device(bp->pdev);
 -	bnxt_free_ctx_mem(bp);
 -	kfree(bp->ctx);
 -	bp->ctx = NULL;
++	bnxt_free_mem(bp, irq_re_init);
++>>>>>>> c909e7ca494f (bnxt: Fix typo in comments)
  }
  
 -static bool is_bnxt_fw_ok(struct bnxt *bp)
 +int bnxt_close_nic(struct bnxt *bp, bool irq_re_init, bool link_re_init)
  {
 -	struct bnxt_fw_health *fw_health = bp->fw_health;
 -	bool no_heartbeat = false, has_reset = false;
 -	u32 val;
 -
 -	val = bnxt_fw_health_readl(bp, BNXT_FW_HEARTBEAT_REG);
 -	if (val == fw_health->last_fw_heartbeat)
 -		no_heartbeat = true;
 +	int rc = 0;
  
 -	val = bnxt_fw_health_readl(bp, BNXT_FW_RESET_CNT_REG);
 -	if (val != fw_health->last_fw_reset_cnt)
 -		has_reset = true;
 +#ifdef CONFIG_BNXT_SRIOV
 +	if (bp->sriov_cfg) {
 +		rc = wait_event_interruptible_timeout(bp->sriov_cfg_wait,
 +						      !bp->sriov_cfg,
 +						      BNXT_SRIOV_CFG_WAIT_TMO);
 +		if (rc)
 +			netdev_warn(bp->dev, "timeout waiting for SRIOV config operation to complete!\n");
 +	}
 +#endif
 +	/* Change device state to avoid TX queue wake up's */
 +	bnxt_tx_disable(bp);
  
 -	if (!no_heartbeat && has_reset)
 -		return true;
 +	clear_bit(BNXT_STATE_OPEN, &bp->state);
 +	smp_mb__after_atomic();
 +	while (test_bit(BNXT_STATE_IN_SP_TASK, &bp->state))
 +		msleep(20);
  
 -	return false;
 -}
 +	/* Flush rings and and disable interrupts */
 +	bnxt_shutdown_nic(bp, irq_re_init);
  
 -/* rtnl_lock is acquired before calling this function */
 -static void bnxt_force_fw_reset(struct bnxt *bp)
 -{
 -	struct bnxt_fw_health *fw_health = bp->fw_health;
 -	struct bnxt_ptp_cfg *ptp = bp->ptp_cfg;
 -	u32 wait_dsecs;
 +	/* TODO CHIMP_FW: Link/PHY related cleanup if (link_re_init) */
  
 -	if (!test_bit(BNXT_STATE_OPEN, &bp->state) ||
 -	    test_bit(BNXT_STATE_IN_FW_RESET, &bp->state))
 -		return;
 +	bnxt_disable_napi(bp);
 +	del_timer_sync(&bp->timer);
 +	bnxt_free_skbs(bp);
  
 -	if (ptp) {
 -		spin_lock_bh(&ptp->ptp_lock);
 -		set_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
 -		spin_unlock_bh(&ptp->ptp_lock);
 -	} else {
 -		set_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
 -	}
 -	bnxt_fw_reset_close(bp);
 -	wait_dsecs = fw_health->master_func_wait_dsecs;
 -	if (fw_health->primary) {
 -		if (fw_health->flags & ERROR_RECOVERY_QCFG_RESP_FLAGS_CO_CPU)
 -			wait_dsecs = 0;
 -		bp->fw_reset_state = BNXT_FW_RESET_STATE_RESET_FW;
 -	} else {
 -		bp->fw_reset_timestamp = jiffies + wait_dsecs * HZ / 10;
 -		wait_dsecs = fw_health->normal_func_wait_dsecs;
 -		bp->fw_reset_state = BNXT_FW_RESET_STATE_ENABLE_DEV;
 +	if (irq_re_init) {
 +		bnxt_free_irq(bp);
 +		bnxt_del_napi(bp);
  	}
 -
 -	bp->fw_reset_min_dsecs = fw_health->post_reset_wait_dsecs;
 -	bp->fw_reset_max_dsecs = fw_health->post_reset_max_wait_dsecs;
 -	bnxt_queue_fw_reset_work(bp, wait_dsecs * HZ / 10);
 +	bnxt_free_mem(bp, irq_re_init);
 +	return rc;
  }
  
 -void bnxt_fw_exception(struct bnxt *bp)
 +static int bnxt_close(struct net_device *dev)
  {
 -	netdev_warn(bp->dev, "Detected firmware fatal condition, initiating reset\n");
 -	set_bit(BNXT_STATE_FW_FATAL_COND, &bp->state);
 -	bnxt_rtnl_lock_sp(bp);
 -	bnxt_force_fw_reset(bp);
 -	bnxt_rtnl_unlock_sp(bp);
 +	struct bnxt *bp = netdev_priv(dev);
 +
 +	bnxt_close_nic(bp, true, true);
 +	bnxt_hwrm_shutdown_link(bp);
 +	return 0;
  }
  
 -/* Returns the number of registered VFs, or 1 if VF configuration is pending, or
 - * < 0 on error.
 - */
 -static int bnxt_get_registered_vfs(struct bnxt *bp)
 +/* rtnl_lock held */
 +static int bnxt_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
  {
 -#ifdef CONFIG_BNXT_SRIOV
 -	int rc;
 +	switch (cmd) {
 +	case SIOCGMIIPHY:
 +		/* fallthru */
 +	case SIOCGMIIREG: {
 +		if (!netif_running(dev))
 +			return -EAGAIN;
  
 -	if (!BNXT_PF(bp))
  		return 0;
 -
 -	rc = bnxt_hwrm_func_qcfg(bp);
 -	if (rc) {
 -		netdev_err(bp->dev, "func_qcfg cmd failed, rc = %d\n", rc);
 -		return rc;
  	}
 -	if (bp->pf.registered_vfs)
 -		return bp->pf.registered_vfs;
 -	if (bp->sriov_cfg)
 -		return 1;
 -#endif
 -	return 0;
 -}
  
 -void bnxt_fw_reset(struct bnxt *bp)
 -{
 -	bnxt_rtnl_lock_sp(bp);
 -	if (test_bit(BNXT_STATE_OPEN, &bp->state) &&
 -	    !test_bit(BNXT_STATE_IN_FW_RESET, &bp->state)) {
 -		struct bnxt_ptp_cfg *ptp = bp->ptp_cfg;
 -		int n = 0, tmo;
 -
 -		if (ptp) {
 -			spin_lock_bh(&ptp->ptp_lock);
 -			set_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
 -			spin_unlock_bh(&ptp->ptp_lock);
 -		} else {
 -			set_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
 -		}
 -		if (bp->pf.active_vfs &&
 -		    !test_bit(BNXT_STATE_FW_FATAL_COND, &bp->state))
 -			n = bnxt_get_registered_vfs(bp);
 -		if (n < 0) {
 -			netdev_err(bp->dev, "Firmware reset aborted, rc = %d\n",
 -				   n);
 -			clear_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
 -			dev_close(bp->dev);
 -			goto fw_reset_exit;
 -		} else if (n > 0) {
 -			u16 vf_tmo_dsecs = n * 10;
 -
 -			if (bp->fw_reset_max_dsecs < vf_tmo_dsecs)
 -				bp->fw_reset_max_dsecs = vf_tmo_dsecs;
 -			bp->fw_reset_state =
 -				BNXT_FW_RESET_STATE_POLL_VF;
 -			bnxt_queue_fw_reset_work(bp, HZ / 10);
 -			goto fw_reset_exit;
 -		}
 -		bnxt_fw_reset_close(bp);
 -		if (bp->fw_cap & BNXT_FW_CAP_ERR_RECOVER_RELOAD) {
 -			bp->fw_reset_state = BNXT_FW_RESET_STATE_POLL_FW_DOWN;
 -			tmo = HZ / 10;
 -		} else {
 -			bp->fw_reset_state = BNXT_FW_RESET_STATE_ENABLE_DEV;
 -			tmo = bp->fw_reset_min_dsecs * HZ / 10;
 -		}
 -		bnxt_queue_fw_reset_work(bp, tmo);
 +	case SIOCSMIIREG:
 +		if (!netif_running(dev))
 +			return -EAGAIN;
 +
 +		return 0;
 +
 +	default:
 +		/* do nothing */
 +		break;
  	}
 -fw_reset_exit:
 -	bnxt_rtnl_unlock_sp(bp);
 +	return -EOPNOTSUPP;
  }
  
 -static void bnxt_chk_missed_irq(struct bnxt *bp)
 +static struct rtnl_link_stats64 *
 +bnxt_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
  {
 -	int i;
 +	u32 i;
 +	struct bnxt *bp = netdev_priv(dev);
  
 -	if (!(bp->flags & BNXT_FLAG_CHIP_P5))
 -		return;
 +	if (!bp->bnapi)
 +		return stats;
  
 +	/* TODO check if we need to synchronize with bnxt_close path */
  	for (i = 0; i < bp->cp_nr_rings; i++) {
  		struct bnxt_napi *bnapi = bp->bnapi[i];
 -		struct bnxt_cp_ring_info *cpr;
 -		u32 fw_ring_id;
 -		int j;
 +		struct bnxt_cp_ring_info *cpr = &bnapi->cp_ring;
 +		struct ctx_hw_stats *hw_stats = cpr->hw_stats;
  
 -		if (!bnapi)
 -			continue;
 +		stats->rx_packets += le64_to_cpu(hw_stats->rx_ucast_pkts);
 +		stats->rx_packets += le64_to_cpu(hw_stats->rx_mcast_pkts);
 +		stats->rx_packets += le64_to_cpu(hw_stats->rx_bcast_pkts);
  
 -		cpr = &bnapi->cp_ring;
 -		for (j = 0; j < 2; j++) {
 -			struct bnxt_cp_ring_info *cpr2 = cpr->cp_ring_arr[j];
 -			u32 val[2];
 +		stats->tx_packets += le64_to_cpu(hw_stats->tx_ucast_pkts);
 +		stats->tx_packets += le64_to_cpu(hw_stats->tx_mcast_pkts);
 +		stats->tx_packets += le64_to_cpu(hw_stats->tx_bcast_pkts);
  
 -			if (!cpr2 || cpr2->has_more_work ||
 -			    !bnxt_has_work(bp, cpr2))
 -				continue;
 +		stats->rx_bytes += le64_to_cpu(hw_stats->rx_ucast_bytes);
 +		stats->rx_bytes += le64_to_cpu(hw_stats->rx_mcast_bytes);
 +		stats->rx_bytes += le64_to_cpu(hw_stats->rx_bcast_bytes);
  
 -			if (cpr2->cp_raw_cons != cpr2->last_cp_raw_cons) {
 -				cpr2->last_cp_raw_cons = cpr2->cp_raw_cons;
 -				continue;
 -			}
 -			fw_ring_id = cpr2->cp_ring_struct.fw_ring_id;
 -			bnxt_dbg_hwrm_ring_info_get(bp,
 -				DBG_RING_INFO_GET_REQ_RING_TYPE_L2_CMPL,
 -				fw_ring_id, &val[0], &val[1]);
 -			cpr->sw_stats.cmn.missed_irqs++;
 -		}
 +		stats->tx_bytes += le64_to_cpu(hw_stats->tx_ucast_bytes);
 +		stats->tx_bytes += le64_to_cpu(hw_stats->tx_mcast_bytes);
 +		stats->tx_bytes += le64_to_cpu(hw_stats->tx_bcast_bytes);
 +
 +		stats->rx_missed_errors +=
 +			le64_to_cpu(hw_stats->rx_discard_pkts);
 +
 +		stats->multicast += le64_to_cpu(hw_stats->rx_mcast_pkts);
 +
 +		stats->tx_dropped += le64_to_cpu(hw_stats->tx_drop_pkts);
  	}
 -}
  
 -static void bnxt_cfg_ntp_filters(struct bnxt *);
 +	if (bp->flags & BNXT_FLAG_PORT_STATS) {
 +		struct rx_port_stats *rx = bp->hw_rx_port_stats;
 +		struct tx_port_stats *tx = bp->hw_tx_port_stats;
 +
 +		stats->rx_crc_errors = le64_to_cpu(rx->rx_fcs_err_frames);
 +		stats->rx_frame_errors = le64_to_cpu(rx->rx_align_err_frames);
 +		stats->rx_length_errors = le64_to_cpu(rx->rx_undrsz_frames) +
 +					  le64_to_cpu(rx->rx_ovrsz_frames) +
 +					  le64_to_cpu(rx->rx_runt_frames);
 +		stats->rx_errors = le64_to_cpu(rx->rx_false_carrier_frames) +
 +				   le64_to_cpu(rx->rx_jbr_frames);
 +		stats->collisions = le64_to_cpu(tx->tx_total_collisions);
 +		stats->tx_fifo_errors = le64_to_cpu(tx->tx_fifo_underruns);
 +		stats->tx_errors = le64_to_cpu(tx->tx_err);
 +	}
 +
 +	return stats;
 +}
  
 -static void bnxt_init_ethtool_link_settings(struct bnxt *bp)
 +static bool bnxt_mc_list_updated(struct bnxt *bp, u32 *rx_mask)
  {
 -	struct bnxt_link_info *link_info = &bp->link_info;
 +	struct net_device *dev = bp->dev;
 +	struct bnxt_vnic_info *vnic = &bp->vnic_info[0];
 +	struct netdev_hw_addr *ha;
 +	u8 *haddr;
 +	int mc_count = 0;
 +	bool update = false;
 +	int off = 0;
  
 -	if (BNXT_AUTO_MODE(link_info->auto_mode)) {
 -		link_info->autoneg = BNXT_AUTONEG_SPEED;
 -		if (bp->hwrm_spec_code >= 0x10201) {
 -			if (link_info->auto_pause_setting &
 -			    PORT_PHY_CFG_REQ_AUTO_PAUSE_AUTONEG_PAUSE)
 -				link_info->autoneg |= BNXT_AUTONEG_FLOW_CTRL;
 -		} else {
 -			link_info->autoneg |= BNXT_AUTONEG_FLOW_CTRL;
 +	netdev_for_each_mc_addr(ha, dev) {
 +		if (mc_count >= BNXT_MAX_MC_ADDRS) {
 +			*rx_mask |= CFA_L2_SET_RX_MASK_REQ_MASK_ALL_MCAST;
 +			vnic->mc_list_count = 0;
 +			return false;
  		}
 -		link_info->advertising = link_info->auto_link_speeds;
 -		link_info->advertising_pam4 = link_info->auto_pam4_link_speeds;
 -	} else {
 -		link_info->req_link_speed = link_info->force_link_speed;
 -		link_info->req_signal_mode = BNXT_SIG_MODE_NRZ;
 -		if (link_info->force_pam4_link_speed) {
 -			link_info->req_link_speed =
 -				link_info->force_pam4_link_speed;
 -			link_info->req_signal_mode = BNXT_SIG_MODE_PAM4;
 +		haddr = ha->addr;
 +		if (!ether_addr_equal(haddr, vnic->mc_list + off)) {
 +			memcpy(vnic->mc_list + off, haddr, ETH_ALEN);
 +			update = true;
  		}
 -		link_info->req_duplex = link_info->duplex_setting;
 +		off += ETH_ALEN;
 +		mc_count++;
  	}
 -	if (link_info->autoneg & BNXT_AUTONEG_FLOW_CTRL)
 -		link_info->req_flow_ctrl =
 -			link_info->auto_pause_setting & BNXT_LINK_PAUSE_BOTH;
 -	else
 -		link_info->req_flow_ctrl = link_info->force_pause_setting;
 +	if (mc_count)
 +		*rx_mask |= CFA_L2_SET_RX_MASK_REQ_MASK_MCAST;
 +
 +	if (mc_count != vnic->mc_list_count) {
 +		vnic->mc_list_count = mc_count;
 +		update = true;
 +	}
 +	return update;
  }
  
 -static void bnxt_fw_echo_reply(struct bnxt *bp)
 +static bool bnxt_uc_list_updated(struct bnxt *bp)
  {
 -	struct bnxt_fw_health *fw_health = bp->fw_health;
 -	struct hwrm_func_echo_response_input *req;
 -	int rc;
 +	struct net_device *dev = bp->dev;
 +	struct bnxt_vnic_info *vnic = &bp->vnic_info[0];
 +	struct netdev_hw_addr *ha;
 +	int off = 0;
  
 -	rc = hwrm_req_init(bp, req, HWRM_FUNC_ECHO_RESPONSE);
 -	if (rc)
 -		return;
 -	req->event_data1 = cpu_to_le32(fw_health->echo_req_data1);
 -	req->event_data2 = cpu_to_le32(fw_health->echo_req_data2);
 -	hwrm_req_send(bp, req);
 +	if (netdev_uc_count(dev) != (vnic->uc_filter_count - 1))
 +		return true;
 +
 +	netdev_for_each_uc_addr(ha, dev) {
 +		if (!ether_addr_equal(ha->addr, vnic->uc_list + off))
 +			return true;
 +
 +		off += ETH_ALEN;
 +	}
 +	return false;
  }
  
 -static void bnxt_sp_task(struct work_struct *work)
 +static void bnxt_set_rx_mode(struct net_device *dev)
  {
 -	struct bnxt *bp = container_of(work, struct bnxt, sp_task);
 +	struct bnxt *bp = netdev_priv(dev);
 +	struct bnxt_vnic_info *vnic = &bp->vnic_info[0];
 +	u32 mask = vnic->rx_mask;
 +	bool mc_update = false;
 +	bool uc_update;
  
 -	set_bit(BNXT_STATE_IN_SP_TASK, &bp->state);
 -	smp_mb__after_atomic();
 -	if (!test_bit(BNXT_STATE_OPEN, &bp->state)) {
 -		clear_bit(BNXT_STATE_IN_SP_TASK, &bp->state);
 +	if (!netif_running(dev))
  		return;
 -	}
  
 -	if (test_and_clear_bit(BNXT_RX_MASK_SP_EVENT, &bp->sp_event))
 -		bnxt_cfg_rx_mode(bp);
 +	mask &= ~(CFA_L2_SET_RX_MASK_REQ_MASK_PROMISCUOUS |
 +		  CFA_L2_SET_RX_MASK_REQ_MASK_MCAST |
 +		  CFA_L2_SET_RX_MASK_REQ_MASK_ALL_MCAST);
  
 -	if (test_and_clear_bit(BNXT_RX_NTP_FLTR_SP_EVENT, &bp->sp_event))
 -		bnxt_cfg_ntp_filters(bp);
 -	if (test_and_clear_bit(BNXT_HWRM_EXEC_FWD_REQ_SP_EVENT, &bp->sp_event))
 -		bnxt_hwrm_exec_fwd_req(bp);
 -	if (test_and_clear_bit(BNXT_PERIODIC_STATS_SP_EVENT, &bp->sp_event)) {
 -		bnxt_hwrm_port_qstats(bp, 0);
 -		bnxt_hwrm_port_qstats_ext(bp, 0);
 -		bnxt_accumulate_all_stats(bp);
 -	}
 +	if ((dev->flags & IFF_PROMISC) && bnxt_promisc_ok(bp))
 +		mask |= CFA_L2_SET_RX_MASK_REQ_MASK_PROMISCUOUS;
  
 -	if (test_and_clear_bit(BNXT_LINK_CHNG_SP_EVENT, &bp->sp_event)) {
 -		int rc;
 +	uc_update = bnxt_uc_list_updated(bp);
  
 -		mutex_lock(&bp->link_lock);
 -		if (test_and_clear_bit(BNXT_LINK_SPEED_CHNG_SP_EVENT,
 -				       &bp->sp_event))
 -			bnxt_hwrm_phy_qcaps(bp);
 +	if (dev->flags & IFF_ALLMULTI) {
 +		mask |= CFA_L2_SET_RX_MASK_REQ_MASK_ALL_MCAST;
 +		vnic->mc_list_count = 0;
 +	} else {
 +		mc_update = bnxt_mc_list_updated(bp, &mask);
 +	}
  
 -		rc = bnxt_update_link(bp, true);
 -		if (rc)
 -			netdev_err(bp->dev, "SP task can't update link (rc: %x)\n",
 -				   rc);
 +	if (mask != vnic->rx_mask || uc_update || mc_update) {
 +		vnic->rx_mask = mask;
  
 -		if (test_and_clear_bit(BNXT_LINK_CFG_CHANGE_SP_EVENT,
 -				       &bp->sp_event))
 -			bnxt_init_ethtool_link_settings(bp);
 -		mutex_unlock(&bp->link_lock);
 +		set_bit(BNXT_RX_MASK_SP_EVENT, &bp->sp_event);
 +		schedule_work(&bp->sp_task);
  	}
 -	if (test_and_clear_bit(BNXT_UPDATE_PHY_SP_EVENT, &bp->sp_event)) {
 -		int rc;
 +}
  
 -		mutex_lock(&bp->link_lock);
 -		rc = bnxt_update_phy_setting(bp);
 -		mutex_unlock(&bp->link_lock);
 -		if (rc) {
 -			netdev_warn(bp->dev, "update phy settings retry failed\n");
 -		} else {
 -			bp->link_info.phy_retry = false;
 -			netdev_info(bp->dev, "update phy settings retry succeeded\n");
 -		}
 -	}
 -	if (test_and_clear_bit(BNXT_HWRM_PORT_MODULE_SP_EVENT, &bp->sp_event)) {
 -		mutex_lock(&bp->link_lock);
 -		bnxt_get_port_module_status(bp);
 -		mutex_unlock(&bp->link_lock);
 -	}
 +static int bnxt_cfg_rx_mode(struct bnxt *bp)
 +{
 +	struct net_device *dev = bp->dev;
 +	struct bnxt_vnic_info *vnic = &bp->vnic_info[0];
 +	struct netdev_hw_addr *ha;
 +	int i, off = 0, rc;
 +	bool uc_update;
  
 -	if (test_and_clear_bit(BNXT_FLOW_STATS_SP_EVENT, &bp->sp_event))
 -		bnxt_tc_flow_stats_work(bp);
 +	netif_addr_lock_bh(dev);
 +	uc_update = bnxt_uc_list_updated(bp);
 +	netif_addr_unlock_bh(dev);
  
 -	if (test_and_clear_bit(BNXT_RING_COAL_NOW_SP_EVENT, &bp->sp_event))
 -		bnxt_chk_missed_irq(bp);
 +	if (!uc_update)
 +		goto skip_uc;
  
 -	if (test_and_clear_bit(BNXT_FW_ECHO_REQUEST_SP_EVENT, &bp->sp_event))
 -		bnxt_fw_echo_reply(bp);
 +	mutex_lock(&bp->hwrm_cmd_lock);
 +	for (i = 1; i < vnic->uc_filter_count; i++) {
 +		struct hwrm_cfa_l2_filter_free_input req = {0};
  
 -	/* These functions below will clear BNXT_STATE_IN_SP_TASK.  They
 -	 * must be the last functions to be called before exiting.
 -	 */
 -	if (test_and_clear_bit(BNXT_RESET_TASK_SP_EVENT, &bp->sp_event))
 -		bnxt_reset(bp, false);
 +		bnxt_hwrm_cmd_hdr_init(bp, &req, HWRM_CFA_L2_FILTER_FREE, -1,
 +				       -1);
  
 -	if (test_and_clear_bit(BNXT_RESET_TASK_SILENT_SP_EVENT, &bp->sp_event))
 -		bnxt_reset(bp, true);
 +		req.l2_filter_id = vnic->fw_l2_filter_id[i];
  
 -	if (test_and_clear_bit(BNXT_RST_RING_SP_EVENT, &bp->sp_event))
 -		bnxt_rx_ring_reset(bp);
 +		rc = _hwrm_send_message(bp, &req, sizeof(req),
 +					HWRM_CMD_TIMEOUT);
 +	}
 +	mutex_unlock(&bp->hwrm_cmd_lock);
  
 -	if (test_and_clear_bit(BNXT_FW_RESET_NOTIFY_SP_EVENT, &bp->sp_event)) {
 -		if (test_bit(BNXT_STATE_FW_FATAL_COND, &bp->state) ||
 -		    test_bit(BNXT_STATE_FW_NON_FATAL_COND, &bp->state))
 -			bnxt_devlink_health_fw_report(bp);
 -		else
 -			bnxt_fw_reset(bp);
 +	vnic->uc_filter_count = 1;
 +
 +	netif_addr_lock_bh(dev);
 +	if (netdev_uc_count(dev) > (BNXT_MAX_UC_ADDRS - 1)) {
 +		vnic->rx_mask |= CFA_L2_SET_RX_MASK_REQ_MASK_PROMISCUOUS;
 +	} else {
 +		netdev_for_each_uc_addr(ha, dev) {
 +			memcpy(vnic->uc_list + off, ha->addr, ETH_ALEN);
 +			off += ETH_ALEN;
 +			vnic->uc_filter_count++;
 +		}
  	}
 +	netif_addr_unlock_bh(dev);
  
 -	if (test_and_clear_bit(BNXT_FW_EXCEPTION_SP_EVENT, &bp->sp_event)) {
 -		if (!is_bnxt_fw_ok(bp))
 -			bnxt_devlink_health_fw_report(bp);
 +	for (i = 1, off = 0; i < vnic->uc_filter_count; i++, off += ETH_ALEN) {
 +		rc = bnxt_hwrm_set_vnic_filter(bp, 0, i, vnic->uc_list + off);
 +		if (rc) {
 +			netdev_err(bp->dev, "HWRM vnic filter failure rc: %x\n",
 +				   rc);
 +			vnic->uc_filter_count = i;
 +			return rc;
 +		}
  	}
  
 -	smp_mb__before_atomic();
 -	clear_bit(BNXT_STATE_IN_SP_TASK, &bp->state);
 +skip_uc:
 +	rc = bnxt_hwrm_cfa_l2_set_rx_mask(bp, 0);
 +	if (rc)
 +		netdev_err(bp->dev, "HWRM cfa l2 rx mask failure rc: %x\n",
 +			   rc);
 +
 +	return rc;
  }
  
 -/* Under rtnl_lock */
 -int bnxt_check_rings(struct bnxt *bp, int tx, int rx, bool sh, int tcs,
 -		     int tx_xdp)
 +/* If the chip and firmware supports RFS */
 +static bool bnxt_rfs_supported(struct bnxt *bp)
  {
 -	int max_rx, max_tx, tx_sets = 1;
 -	int tx_rings_needed, stats;
 -	int rx_rings = rx;
 -	int cp, vnics, rc;
 -
 -	if (tcs)
 -		tx_sets = tcs;
 -
 -	rc = bnxt_get_max_rings(bp, &max_rx, &max_tx, sh);
 -	if (rc)
 -		return rc;
 +	if (BNXT_PF(bp) && !BNXT_CHIP_TYPE_NITRO_A0(bp))
 +		return true;
 +	if (bp->flags & BNXT_FLAG_NEW_RSS_CAP)
 +		return true;
 +	return false;
 +}
  
 -	if (max_rx < rx)
 -		return -ENOMEM;
 +/* If runtime conditions support RFS */
 +static bool bnxt_rfs_capable(struct bnxt *bp)
 +{
 +#ifdef CONFIG_RFS_ACCEL
 +	int vnics, max_vnics, max_rss_ctxs;
  
 -	tx_rings_needed = tx * tx_sets + tx_xdp;
 -	if (max_tx < tx_rings_needed)
 -		return -ENOMEM;
 +	if (!(bp->flags & BNXT_FLAG_MSIX_CAP))
 +		return false;
  
 -	vnics = 1;
 -	if ((bp->flags & (BNXT_FLAG_RFS | BNXT_FLAG_CHIP_P5)) == BNXT_FLAG_RFS)
 -		vnics += rx_rings;
 +	vnics = 1 + bp->rx_nr_rings;
 +	max_vnics = bnxt_get_max_func_vnics(bp);
 +	max_rss_ctxs = bnxt_get_max_func_rss_ctxs(bp);
  
 -	if (bp->flags & BNXT_FLAG_AGG_RINGS)
 -		rx_rings <<= 1;
 -	cp = sh ? max_t(int, tx_rings_needed, rx) : tx_rings_needed + rx;
 -	stats = cp;
 -	if (BNXT_NEW_RM(bp)) {
 -		cp += bnxt_get_ulp_msix_num(bp);
 -		stats += bnxt_get_ulp_stat_ctxs(bp);
 +	/* RSS contexts not a limiting factor */
 +	if (bp->flags & BNXT_FLAG_NEW_RSS_CAP)
 +		max_rss_ctxs = max_vnics;
 +	if (vnics > max_vnics || vnics > max_rss_ctxs) {
 +		netdev_warn(bp->dev,
 +			    "Not enough resources to support NTUPLE filters, enough resources for up to %d rx rings\n",
 +			    min(max_rss_ctxs - 1, max_vnics - 1));
 +		return false;
  	}
 -	return bnxt_hwrm_check_rings(bp, tx_rings_needed, rx_rings, rx, cp,
 -				     stats, vnics);
 +
 +	return true;
 +#else
 +	return false;
 +#endif
  }
  
 -static void bnxt_unmap_bars(struct bnxt *bp, struct pci_dev *pdev)
 +static netdev_features_t bnxt_fix_features(struct net_device *dev,
 +					   netdev_features_t features)
  {
 -	if (bp->bar2) {
 -		pci_iounmap(pdev, bp->bar2);
 -		bp->bar2 = NULL;
 -	}
 +	struct bnxt *bp = netdev_priv(dev);
  
 -	if (bp->bar1) {
 -		pci_iounmap(pdev, bp->bar1);
 -		bp->bar1 = NULL;
 -	}
 +	if ((features & NETIF_F_NTUPLE) && !bnxt_rfs_capable(bp))
 +		features &= ~NETIF_F_NTUPLE;
  
 -	if (bp->bar0) {
 -		pci_iounmap(pdev, bp->bar0);
 -		bp->bar0 = NULL;
 +	/* Both CTAG and STAG VLAN accelaration on the RX side have to be
 +	 * turned on or off together.
 +	 */
 +	if ((features & (NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_STAG_RX)) !=
 +	    (NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_STAG_RX)) {
 +		if (dev->features & NETIF_F_HW_VLAN_CTAG_RX)
 +			features &= ~(NETIF_F_HW_VLAN_CTAG_RX |
 +				      NETIF_F_HW_VLAN_STAG_RX);
 +		else
 +			features |= NETIF_F_HW_VLAN_CTAG_RX |
 +				    NETIF_F_HW_VLAN_STAG_RX;
  	}
 +#ifdef CONFIG_BNXT_SRIOV
 +	if (BNXT_VF(bp)) {
 +		if (bp->vf.vlan) {
 +			features &= ~(NETIF_F_HW_VLAN_CTAG_RX |
 +				      NETIF_F_HW_VLAN_STAG_RX);
 +		}
 +	}
 +#endif
 +	return features;
  }
  
 -static void bnxt_cleanup_pci(struct bnxt *bp)
 -{
 -	bnxt_unmap_bars(bp, bp->pdev);
 -	pci_release_regions(bp->pdev);
 -	if (pci_is_enabled(bp->pdev))
 -		pci_disable_device(bp->pdev);
 -}
 -
 -static void bnxt_init_dflt_coal(struct bnxt *bp)
 +static int bnxt_set_features(struct net_device *dev, netdev_features_t features)
  {
 -	struct bnxt_coal_cap *coal_cap = &bp->coal_cap;
 -	struct bnxt_coal *coal;
 -	u16 flags = 0;
 +	struct bnxt *bp = netdev_priv(dev);
 +	u32 flags = bp->flags;
 +	u32 changes;
 +	int rc = 0;
 +	bool re_init = false;
 +	bool update_tpa = false;
  
 -	if (coal_cap->cmpl_params &
 -	    RING_AGGINT_QCAPS_RESP_CMPL_PARAMS_TIMER_RESET)
 -		flags |= RING_CMPL_RING_CFG_AGGINT_PARAMS_REQ_FLAGS_TIMER_RESET;
 +	flags &= ~BNXT_FLAG_ALL_CONFIG_FEATS;
 +	if ((features & NETIF_F_GRO) && !BNXT_CHIP_TYPE_NITRO_A0(bp))
 +		flags |= BNXT_FLAG_GRO;
 +	if (features & NETIF_F_LRO)
 +		flags |= BNXT_FLAG_LRO;
  
 -	/* Tick values in micro seconds.
 -	 * 1 coal_buf x bufs_per_record = 1 completion record.
 -	 */
 -	coal = &bp->rx_coal;
 -	coal->coal_ticks = 10;
 -	coal->coal_bufs = 30;
 -	coal->coal_ticks_irq = 1;
 -	coal->coal_bufs_irq = 2;
 -	coal->idle_thresh = 50;
 -	coal->bufs_per_record = 2;
 -	coal->budget = 64;		/* NAPI budget */
 -	coal->flags = flags;
 -
 -	coal = &bp->tx_coal;
 -	coal->coal_ticks = 28;
 -	coal->coal_bufs = 30;
 -	coal->coal_ticks_irq = 2;
 -	coal->coal_bufs_irq = 2;
 -	coal->bufs_per_record = 1;
 -	coal->flags = flags;
 +	if (bp->flags & BNXT_FLAG_NO_AGG_RINGS)
 +		flags &= ~BNXT_FLAG_TPA;
  
 -	bp->stats_coal_ticks = BNXT_DEF_STATS_COAL_TICKS;
 -}
 +	if (features & NETIF_F_HW_VLAN_CTAG_RX)
 +		flags |= BNXT_FLAG_STRIP_VLAN;
  
 -static int bnxt_fw_init_one_p1(struct bnxt *bp)
 -{
 -	int rc;
 +	if (features & NETIF_F_NTUPLE)
 +		flags |= BNXT_FLAG_RFS;
  
 -	bp->fw_cap = 0;
 -	rc = bnxt_hwrm_ver_get(bp);
 -	bnxt_try_map_fw_health_reg(bp);
 -	if (rc) {
 -		rc = bnxt_try_recover_fw(bp);
 -		if (rc)
 -			return rc;
 -		rc = bnxt_hwrm_ver_get(bp);
 -		if (rc)
 -			return rc;
 +	changes = flags ^ bp->flags;
 +	if (changes & BNXT_FLAG_TPA) {
 +		update_tpa = true;
 +		if ((bp->flags & BNXT_FLAG_TPA) == 0 ||
 +		    (flags & BNXT_FLAG_TPA) == 0)
 +			re_init = true;
  	}
  
 -	bnxt_nvm_cfg_ver_get(bp);
 +	if (changes & ~BNXT_FLAG_TPA)
 +		re_init = true;
  
 -	rc = bnxt_hwrm_func_reset(bp);
 -	if (rc)
 -		return -ENODEV;
 +	if (flags != bp->flags) {
 +		u32 old_flags = bp->flags;
  
 -	bnxt_hwrm_fw_set_time(bp);
 -	return 0;
 -}
 +		bp->flags = flags;
  
 -static int bnxt_fw_init_one_p2(struct bnxt *bp)
 -{
 -	int rc;
 +		if (!test_bit(BNXT_STATE_OPEN, &bp->state)) {
 +			if (update_tpa)
 +				bnxt_set_ring_params(bp);
 +			return rc;
 +		}
  
 -	/* Get the MAX capabilities for this function */
 -	rc = bnxt_hwrm_func_qcaps(bp);
 -	if (rc) {
 -		netdev_err(bp->dev, "hwrm query capability failure rc: %x\n",
 -			   rc);
 -		return -ENODEV;
 +		if (re_init) {
 +			bnxt_close_nic(bp, false, false);
 +			if (update_tpa)
 +				bnxt_set_ring_params(bp);
 +
 +			return bnxt_open_nic(bp, false, false);
 +		}
 +		if (update_tpa) {
 +			rc = bnxt_set_tpa(bp,
 +					  (flags & BNXT_FLAG_TPA) ?
 +					  true : false);
 +			if (rc)
 +				bp->flags = old_flags;
 +		}
  	}
 +	return rc;
 +}
  
 -	rc = bnxt_hwrm_cfa_adv_flow_mgnt_qcaps(bp);
 -	if (rc)
 -		netdev_warn(bp->dev, "hwrm query adv flow mgnt failure rc: %d\n",
 -			    rc);
 -
 -	if (bnxt_alloc_fw_health(bp)) {
 -		netdev_warn(bp->dev, "no memory for firmware error recovery\n");
 -	} else {
 -		rc = bnxt_hwrm_error_recovery_qcfg(bp);
 -		if (rc)
 -			netdev_warn(bp->dev, "hwrm query error recovery failure rc: %d\n",
 -				    rc);
 -	}
 +static void bnxt_dump_tx_sw_state(struct bnxt_napi *bnapi)
 +{
 +	struct bnxt_tx_ring_info *txr = bnapi->tx_ring;
 +	int i = bnapi->index;
  
 -	rc = bnxt_hwrm_func_drv_rgtr(bp, NULL, 0, false);
 -	if (rc)
 -		return -ENODEV;
 +	if (!txr)
 +		return;
  
 -	bnxt_hwrm_func_qcfg(bp);
 -	bnxt_hwrm_vnic_qcaps(bp);
 -	bnxt_hwrm_port_led_qcaps(bp);
 -	bnxt_ethtool_init(bp);
 -	bnxt_dcb_init(bp);
 -	return 0;
 +	netdev_info(bnapi->bp->dev, "[%d]: tx{fw_ring: %d prod: %x cons: %x}\n",
 +		    i, txr->tx_ring_struct.fw_ring_id, txr->tx_prod,
 +		    txr->tx_cons);
  }
  
 -static void bnxt_set_dflt_rss_hash_type(struct bnxt *bp)
 +static void bnxt_dump_rx_sw_state(struct bnxt_napi *bnapi)
  {
 -	bp->flags &= ~BNXT_FLAG_UDP_RSS_CAP;
 -	bp->rss_hash_cfg = VNIC_RSS_CFG_REQ_HASH_TYPE_IPV4 |
 -			   VNIC_RSS_CFG_REQ_HASH_TYPE_TCP_IPV4 |
 -			   VNIC_RSS_CFG_REQ_HASH_TYPE_IPV6 |
 -			   VNIC_RSS_CFG_REQ_HASH_TYPE_TCP_IPV6;
 -	if (BNXT_CHIP_P4_PLUS(bp) && bp->hwrm_spec_code >= 0x10501) {
 -		bp->flags |= BNXT_FLAG_UDP_RSS_CAP;
 -		bp->rss_hash_cfg |= VNIC_RSS_CFG_REQ_HASH_TYPE_UDP_IPV4 |
 -				    VNIC_RSS_CFG_REQ_HASH_TYPE_UDP_IPV6;
 -	}
 -}
 +	struct bnxt_rx_ring_info *rxr = bnapi->rx_ring;
 +	int i = bnapi->index;
  
 -static void bnxt_set_dflt_rfs(struct bnxt *bp)
 -{
 -	struct net_device *dev = bp->dev;
 +	if (!rxr)
 +		return;
  
 -	dev->hw_features &= ~NETIF_F_NTUPLE;
 -	dev->features &= ~NETIF_F_NTUPLE;
 -	bp->flags &= ~BNXT_FLAG_RFS;
 -	if (bnxt_rfs_supported(bp)) {
 -		dev->hw_features |= NETIF_F_NTUPLE;
 -		if (bnxt_rfs_capable(bp)) {
 -			bp->flags |= BNXT_FLAG_RFS;
 -			dev->features |= NETIF_F_NTUPLE;
 -		}
 -	}
 +	netdev_info(bnapi->bp->dev, "[%d]: rx{fw_ring: %d prod: %x} rx_agg{fw_ring: %d agg_prod: %x sw_agg_prod: %x}\n",
 +		    i, rxr->rx_ring_struct.fw_ring_id, rxr->rx_prod,
 +		    rxr->rx_agg_ring_struct.fw_ring_id, rxr->rx_agg_prod,
 +		    rxr->rx_sw_agg_prod);
  }
  
 -static void bnxt_fw_init_one_p3(struct bnxt *bp)
 +static void bnxt_dump_cp_sw_state(struct bnxt_napi *bnapi)
  {
 -	struct pci_dev *pdev = bp->pdev;
 -
 -	bnxt_set_dflt_rss_hash_type(bp);
 -	bnxt_set_dflt_rfs(bp);
 -
 -	bnxt_get_wol_settings(bp);
 -	if (bp->flags & BNXT_FLAG_WOL_CAP)
 -		device_set_wakeup_enable(&pdev->dev, bp->wol);
 -	else
 -		device_set_wakeup_capable(&pdev->dev, false);
 +	struct bnxt_cp_ring_info *cpr = &bnapi->cp_ring;
 +	int i = bnapi->index;
  
 -	bnxt_hwrm_set_cache_line_size(bp, cache_line_size());
 -	bnxt_hwrm_coal_params_qcaps(bp);
 +	netdev_info(bnapi->bp->dev, "[%d]: cp{fw_ring: %d raw_cons: %x}\n",
 +		    i, cpr->cp_ring_struct.fw_ring_id, cpr->cp_raw_cons);
  }
  
 -static int bnxt_probe_phy(struct bnxt *bp, bool fw_dflt);
 -
 -int bnxt_fw_init_one(struct bnxt *bp)
 +static void bnxt_dbg_dump_states(struct bnxt *bp)
  {
 -	int rc;
 +	int i;
 +	struct bnxt_napi *bnapi;
  
 -	rc = bnxt_fw_init_one_p1(bp);
 -	if (rc) {
 -		netdev_err(bp->dev, "Firmware init phase 1 failed\n");
 -		return rc;
 -	}
 -	rc = bnxt_fw_init_one_p2(bp);
 -	if (rc) {
 -		netdev_err(bp->dev, "Firmware init phase 2 failed\n");
 -		return rc;
 +	for (i = 0; i < bp->cp_nr_rings; i++) {
 +		bnapi = bp->bnapi[i];
 +		if (netif_msg_drv(bp)) {
 +			bnxt_dump_tx_sw_state(bnapi);
 +			bnxt_dump_rx_sw_state(bnapi);
 +			bnxt_dump_cp_sw_state(bnapi);
 +		}
  	}
 -	rc = bnxt_probe_phy(bp, false);
 -	if (rc)
 -		return rc;
 -	rc = bnxt_approve_mac(bp, bp->dev->dev_addr, false);
 -	if (rc)
 -		return rc;
 -
 -	bnxt_fw_init_one_p3(bp);
 -	return 0;
  }
  
 -static void bnxt_fw_reset_writel(struct bnxt *bp, int reg_idx)
 +static void bnxt_reset_task(struct bnxt *bp, bool silent)
  {
 -	struct bnxt_fw_health *fw_health = bp->fw_health;
 -	u32 reg = fw_health->fw_reset_seq_regs[reg_idx];
 -	u32 val = fw_health->fw_reset_seq_vals[reg_idx];
 -	u32 reg_type, reg_off, delay_msecs;
 +	if (!silent)
 +		bnxt_dbg_dump_states(bp);
 +	if (netif_running(bp->dev)) {
 +		int rc;
  
 -	delay_msecs = fw_health->fw_reset_seq_delay_msec[reg_idx];
 -	reg_type = BNXT_FW_HEALTH_REG_TYPE(reg);
 -	reg_off = BNXT_FW_HEALTH_REG_OFF(reg);
 -	switch (reg_type) {
 -	case BNXT_FW_HEALTH_REG_TYPE_CFG:
 -		pci_write_config_dword(bp->pdev, reg_off, val);
 -		break;
 -	case BNXT_FW_HEALTH_REG_TYPE_GRC:
 -		writel(reg_off & BNXT_GRC_BASE_MASK,
 -		       bp->bar0 + BNXT_GRCPF_REG_WINDOW_BASE_OUT + 4);
 -		reg_off = (reg_off & BNXT_GRC_OFFSET_MASK) + 0x2000;
 -		fallthrough;
 -	case BNXT_FW_HEALTH_REG_TYPE_BAR0:
 -		writel(val, bp->bar0 + reg_off);
 -		break;
 -	case BNXT_FW_HEALTH_REG_TYPE_BAR1:
 -		writel(val, bp->bar1 + reg_off);
 -		break;
 -	}
 -	if (delay_msecs) {
 -		pci_read_config_dword(bp->pdev, 0, &val);
 -		msleep(delay_msecs);
 +		if (!silent)
 +			bnxt_ulp_stop(bp);
 +		bnxt_close_nic(bp, false, false);
 +		rc = bnxt_open_nic(bp, false, false);
 +		if (!silent && !rc)
 +			bnxt_ulp_start(bp);
  	}
  }
  
* Unmerged path drivers/net/ethernet/broadcom/bnxt/bnxt.c
