blk-mq: ensure that bd->last is always set correctly

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 113285b473824922498d07d7f82459507b9792eb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/113285b4.failed

When drivers are called with a request in blk-mq, blk-mq flags the
state such that the driver knows if this is the last request in
this call chain or not. The driver can then use that information
to defer kicking off IO until bd->last is true. However, with blk-mq
and scheduling, we need to allocate a driver tag for a request before
it can be issued. If we fail to allocate such a tag, we could end up
in the situation where the last request issued did not have
bd->last == true set. This can then cause a driver hang.

This fixes a hang with virtio-blk, which uses bd->last as a hint
on whether to kick the queue or not.

	Reported-by: Chris Mason <clm@fb.com>
	Tested-by: Chris Mason <clm@fb.com>
	Reviewed-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 113285b473824922498d07d7f82459507b9792eb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index d9bfe0c6bc0e,e797607dab89..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -825,6 -844,132 +825,135 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
++<<<<<<< HEAD
++=======
+ bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
+ 			   bool wait)
+ {
+ 	struct blk_mq_alloc_data data = {
+ 		.q = rq->q,
+ 		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
+ 		.flags = wait ? 0 : BLK_MQ_REQ_NOWAIT,
+ 	};
+ 
+ 	if (rq->tag != -1) {
+ done:
+ 		if (hctx)
+ 			*hctx = data.hctx;
+ 		return true;
+ 	}
+ 
+ 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ 		data.flags |= BLK_MQ_REQ_RESERVED;
+ 
+ 	rq->tag = blk_mq_get_tag(&data);
+ 	if (rq->tag >= 0) {
+ 		if (blk_mq_tag_busy(data.hctx)) {
+ 			rq->rq_flags |= RQF_MQ_INFLIGHT;
+ 			atomic_inc(&data.hctx->nr_active);
+ 		}
+ 		data.hctx->tags->rqs[rq->tag] = rq;
+ 		goto done;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
+ 				    struct request *rq)
+ {
+ 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ 	rq->tag = -1;
+ 
+ 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+ 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+ 		atomic_dec(&hctx->nr_active);
+ 	}
+ }
+ 
+ static void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
+ 				       struct request *rq)
+ {
+ 	if (rq->tag == -1 || rq->internal_tag == -1)
+ 		return;
+ 
+ 	__blk_mq_put_driver_tag(hctx, rq);
+ }
+ 
+ static void blk_mq_put_driver_tag(struct request *rq)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 
+ 	if (rq->tag == -1 || rq->internal_tag == -1)
+ 		return;
+ 
+ 	hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu);
+ 	__blk_mq_put_driver_tag(hctx, rq);
+ }
+ 
+ /*
+  * If we fail getting a driver tag because all the driver tags are already
+  * assigned and on the dispatch list, BUT the first entry does not have a
+  * tag, then we could deadlock. For that case, move entries with assigned
+  * driver tags to the front, leaving the set of tagged requests in the
+  * same order, and the untagged set in the same order.
+  */
+ static bool reorder_tags_to_front(struct list_head *list)
+ {
+ 	struct request *rq, *tmp, *first = NULL;
+ 
+ 	list_for_each_entry_safe_reverse(rq, tmp, list, queuelist) {
+ 		if (rq == first)
+ 			break;
+ 		if (rq->tag != -1) {
+ 			list_move(&rq->queuelist, list);
+ 			if (!first)
+ 				first = rq;
+ 		}
+ 	}
+ 
+ 	return first != NULL;
+ }
+ 
+ static int blk_mq_dispatch_wake(wait_queue_t *wait, unsigned mode, int flags,
+ 				void *key)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 
+ 	hctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);
+ 
+ 	list_del(&wait->task_list);
+ 	clear_bit_unlock(BLK_MQ_S_TAG_WAITING, &hctx->state);
+ 	blk_mq_run_hw_queue(hctx, true);
+ 	return 1;
+ }
+ 
+ static bool blk_mq_dispatch_wait_add(struct blk_mq_hw_ctx *hctx)
+ {
+ 	struct sbq_wait_state *ws;
+ 
+ 	/*
+ 	 * The TAG_WAITING bit serves as a lock protecting hctx->dispatch_wait.
+ 	 * The thread which wins the race to grab this bit adds the hardware
+ 	 * queue to the wait queue.
+ 	 */
+ 	if (test_bit(BLK_MQ_S_TAG_WAITING, &hctx->state) ||
+ 	    test_and_set_bit_lock(BLK_MQ_S_TAG_WAITING, &hctx->state))
+ 		return false;
+ 
+ 	init_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);
+ 	ws = bt_wait_ptr(&hctx->tags->bitmap_tags, hctx);
+ 
+ 	/*
+ 	 * As soon as this returns, it's no longer safe to fiddle with
+ 	 * hctx->dispatch_wait, since a completion can wake up the wait queue
+ 	 * and unlock the bit.
+ 	 */
+ 	add_wait_queue(&ws->wait, &hctx->dispatch_wait);
+ 	return true;
+ }
+ 
++>>>>>>> 113285b47382 (blk-mq: ensure that bd->last is always set correctly)
  bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list)
  {
  	struct request_queue *q = hctx->queue;
@@@ -859,6 -1037,7 +1000,10 @@@
  			queued++;
  			break;
  		case BLK_MQ_RQ_QUEUE_BUSY:
++<<<<<<< HEAD
++=======
+ 			blk_mq_put_driver_tag_hctx(hctx, rq);
++>>>>>>> 113285b47382 (blk-mq: ensure that bd->last is always set correctly)
  			list_add(&rq->queuelist, list);
  			__blk_mq_requeue_request(rq);
  			break;
@@@ -889,8 -1067,15 +1034,15 @@@
  	 * that is where we will continue on next queue run.
  	 */
  	if (!list_empty(list)) {
+ 		/*
+ 		 * If we got a driver tag for the next request already,
+ 		 * free it again.
+ 		 */
+ 		rq = list_first_entry(list, struct request, queuelist);
+ 		blk_mq_put_driver_tag(rq);
+ 
  		spin_lock(&hctx->lock);
 -		list_splice_init(list, &hctx->dispatch);
 +		list_splice(list, &hctx->dispatch);
  		spin_unlock(&hctx->lock);
  
  		/*
* Unmerged path block/blk-mq.c
