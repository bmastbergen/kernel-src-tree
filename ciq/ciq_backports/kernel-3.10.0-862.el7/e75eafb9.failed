genirq/msi: Switch to new irq spreading infrastructure

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Thomas Gleixner <tglx@linutronix.de>
commit e75eafb9b0395c338230b0eef2cc92ca8d20dee2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e75eafb9.failed

Switch MSI over to the new spreading code. If a pci device contains a valid
pointer to a cpumask, then this mask is used for spreading otherwise the
online cpu mask is used. This allows a driver to restrict the spread to a
subset of CPUs, e.g. cpus on a particular node.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: axboe@fb.com
	Cc: keith.busch@intel.com
	Cc: agordeev@redhat.com
	Cc: linux-block@vger.kernel.org
Link: http://lkml.kernel.org/r/1473862739-15032-4-git-send-email-hch@lst.de
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit e75eafb9b0395c338230b0eef2cc92ca8d20dee2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/pci/msi.c
#	kernel/irq/irqdesc.c
diff --cc drivers/pci/msi.c
index 3127b0433131,06100dde0e86..000000000000
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@@ -542,15 -549,23 +542,27 @@@ error_attrs
  	return ret;
  }
  
- static struct msi_desc *msi_setup_entry(struct pci_dev *dev, int nvec)
+ static struct msi_desc *
+ msi_setup_entry(struct pci_dev *dev, int nvec, bool affinity)
  {
- 	u16 control;
+ 	struct cpumask *masks = NULL;
  	struct msi_desc *entry;
+ 	u16 control;
+ 
+ 	if (affinity) {
+ 		masks = irq_create_affinity_masks(dev->irq_affinity, nvec);
+ 		if (!masks)
+ 			pr_err("Unable to allocate affinity masks, ignoring\n");
+ 	}
  
  	/* MSI Entry Initialization */
++<<<<<<< HEAD
 +	entry = alloc_msi_entry(dev);
++=======
+ 	entry = alloc_msi_entry(&dev->dev, nvec, masks);
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
  	if (!entry)
- 		return NULL;
+ 		goto out;
  
  	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);
  
@@@ -559,10 -574,8 +571,13 @@@
  	entry->msi_attrib.entry_nr	= 0;
  	entry->msi_attrib.maskbit	= !!(control & PCI_MSI_FLAGS_MASKBIT);
  	entry->msi_attrib.default_irq	= dev->irq;	/* Save IOAPIC IRQ */
 -	entry->msi_attrib.multi_cap	= (control & PCI_MSI_FLAGS_QMASK) >> 1;
 +	entry->msi_attrib.pos		= dev->msi_cap;
 +	entry->multi_cap		= (control & PCI_MSI_FLAGS_QMASK) >> 1;
  	entry->msi_attrib.multiple	= ilog2(__roundup_pow_of_two(nvec));
++<<<<<<< HEAD
 +	entry->nvec_used		= nvec;
++=======
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
  
  	if (control & PCI_MSI_FLAGS_64BIT)
  		entry->mask_pos = dev->msi_cap + PCI_MSI_MASK_64;
@@@ -672,13 -687,21 +689,29 @@@ static void __iomem *msix_map_region(st
  }
  
  static int msix_setup_entries(struct pci_dev *dev, void __iomem *base,
- 			      struct msix_entry *entries, int nvec)
+ 			      struct msix_entry *entries, int nvec,
+ 			      bool affinity)
  {
++<<<<<<< HEAD
 +	struct msi_desc *entry;
 +	int i;
 +
 +	for (i = 0; i < nvec; i++) {
 +		entry = alloc_msi_entry(dev);
++=======
+ 	struct cpumask *curmsk, *masks = NULL;
+ 	struct msi_desc *entry;
+ 	int ret, i;
+ 
+ 	if (affinity) {
+ 		masks = irq_create_affinity_masks(dev->irq_affinity, nvec);
+ 		if (!masks)
+ 			pr_err("Unable to allocate affinity masks, ignoring\n");
+ 	}
+ 
+ 	for (i = 0, curmsk = masks; i < nvec; i++) {
+ 		entry = alloc_msi_entry(&dev->dev, 1, curmsk);
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
  		if (!entry) {
  			if (!i)
  				iounmap(base);
@@@ -690,14 -714,20 +724,23 @@@
  
  		entry->msi_attrib.is_msix	= 1;
  		entry->msi_attrib.is_64		= 1;
 -		if (entries)
 -			entry->msi_attrib.entry_nr = entries[i].entry;
 -		else
 -			entry->msi_attrib.entry_nr = i;
 +		entry->msi_attrib.entry_nr	= entries[i].entry;
  		entry->msi_attrib.default_irq	= dev->irq;
  		entry->mask_base		= base;
++<<<<<<< HEAD
 +		entry->nvec_used		= 1;
 +
 +		list_add_tail(&entry->list, &dev->msi_list);
- 	}
++=======
  
+ 		list_add_tail(&entry->list, dev_to_msi_list(&dev->dev));
+ 		if (masks)
+ 			curmsk++;
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
+ 	}
+ 	ret = 0;
+ out:
+ 	kfree(masks);
  	return 0;
  }
  
@@@ -968,22 -952,8 +1011,27 @@@ int pci_msix_vec_count(struct pci_dev *
  }
  EXPORT_SYMBOL(pci_msix_vec_count);
  
++<<<<<<< HEAD
 +/**
 + * pci_enable_msix - configure device's MSI-X capability structure
 + * @dev: pointer to the pci_dev data structure of MSI-X device function
 + * @entries: pointer to an array of MSI-X entries
 + * @nvec: number of MSI-X irqs requested for allocation by device driver
 + *
 + * Setup the MSI-X capability structure of device function with the number
 + * of requested irqs upon its software driver call to request for
 + * MSI-X mode enabled on its hardware device function. A return of zero
 + * indicates the successful configuration of MSI-X capability structure
 + * with new allocated MSI-X irqs. A return of < 0 indicates a failure.
 + * Or a return of > 0 indicates that driver request is exceeding the number
 + * of irqs or MSI-X vectors available. Driver should use the returned value to
 + * re-send its request.
 + **/
 +int pci_enable_msix(struct pci_dev *dev, struct msix_entry *entries, int nvec)
++=======
+ static int __pci_enable_msix(struct pci_dev *dev, struct msix_entry *entries,
+ 			     int nvec, bool affinity)
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
  {
  	int nr_entries;
  	int i, j;
@@@ -1066,25 -1055,10 +1134,26 @@@ int pci_msi_enabled(void
  }
  EXPORT_SYMBOL(pci_msi_enabled);
  
 -static int __pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec,
 -		unsigned int flags)
 +void pci_msi_init_pci_dev(struct pci_dev *dev)
 +{
 +	INIT_LIST_HEAD(&dev->msi_list);
 +}
 +
 +/**
 + * pci_enable_msi_range - configure device's MSI capability structure
 + * @dev: device to configure
 + * @minvec: minimal number of interrupts to configure
 + * @maxvec: maximum number of interrupts to configure
 + *
 + * This function tries to allocate a maximum possible number of interrupts in a
 + * range between @minvec and @maxvec. It returns a negative errno if an error
 + * occurs. If it succeeds, it returns the actual number of interrupts allocated
 + * and updates the @dev's irq member to the lowest new interrupt number;
 + * the other interrupt numbers allocated to this device are consecutive.
 + **/
 +int pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec)
  {
+ 	bool affinity = flags & PCI_IRQ_AFFINITY;
  	int nvec;
  	int rc;
  
@@@ -1106,26 -1080,82 +1175,76 @@@
  	nvec = pci_msi_vec_count(dev);
  	if (nvec < 0)
  		return nvec;
 -	if (nvec < minvec)
 +	else if (nvec < minvec)
  		return -EINVAL;
 -
 -	if (nvec > maxvec)
 +	else if (nvec > maxvec)
  		nvec = maxvec;
  
++<<<<<<< HEAD
 +	do {
 +		rc = msi_capability_init(dev, nvec);
 +		if (rc < 0) {
++=======
+ 	for (;;) {
+ 		if (affinity) {
+ 			nvec = irq_calc_affinity_vectors(dev->irq_affinity,
+ 					nvec);
+ 			if (nvec < minvec)
+ 				return -ENOSPC;
+ 		}
+ 
+ 		rc = msi_capability_init(dev, nvec, affinity);
+ 		if (rc == 0)
+ 			return nvec;
+ 
+ 		if (rc < 0)
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
  			return rc;
 -		if (rc < minvec)
 -			return -ENOSPC;
 -
 -		nvec = rc;
 -	}
 -}
 +		} else if (rc > 0) {
 +			if (rc < minvec)
 +				return -ENOSPC;
 +			nvec = rc;
 +		}
 +	} while (rc);
  
 -/**
 - * pci_enable_msi_range - configure device's MSI capability structure
 - * @dev: device to configure
 - * @minvec: minimal number of interrupts to configure
 - * @maxvec: maximum number of interrupts to configure
 - *
 - * This function tries to allocate a maximum possible number of interrupts in a
 - * range between @minvec and @maxvec. It returns a negative errno if an error
 - * occurs. If it succeeds, it returns the actual number of interrupts allocated
 - * and updates the @dev's irq member to the lowest new interrupt number;
 - * the other interrupt numbers allocated to this device are consecutive.
 - **/
 -int pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec)
 -{
 -	return __pci_enable_msi_range(dev, minvec, maxvec, 0);
 +	return nvec;
  }
  EXPORT_SYMBOL(pci_enable_msi_range);
  
++<<<<<<< HEAD
++=======
+ static int __pci_enable_msix_range(struct pci_dev *dev,
+ 		struct msix_entry *entries, int minvec, int maxvec,
+ 		unsigned int flags)
+ {
+ 	bool affinity = flags & PCI_IRQ_AFFINITY;
+ 	int rc, nvec = maxvec;
+ 
+ 	if (maxvec < minvec)
+ 		return -ERANGE;
+ 
+ 	for (;;) {
+ 		if (affinity) {
+ 			nvec = irq_calc_affinity_vectors(dev->irq_affinity,
+ 					nvec);
+ 			if (nvec < minvec)
+ 				return -ENOSPC;
+ 		}
+ 
+ 		rc = __pci_enable_msix(dev, entries, nvec, affinity);
+ 		if (rc == 0)
+ 			return nvec;
+ 
+ 		if (rc < 0)
+ 			return rc;
+ 		if (rc < minvec)
+ 			return -ENOSPC;
+ 
+ 		nvec = rc;
+ 	}
+ }
+ 
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
  /**
   * pci_enable_msix_range - configure device's MSI-X capability structure
   * @dev: pointer to the pci_dev data structure of MSI-X device function
diff --cc kernel/irq/irqdesc.c
index 0674e54847c2,5a5a685aba33..000000000000
--- a/kernel/irq/irqdesc.c
+++ b/kernel/irq/irqdesc.c
@@@ -199,13 -231,31 +199,37 @@@ static void free_desc(unsigned int irq
  }
  
  static int alloc_descs(unsigned int start, unsigned int cnt, int node,
 -		       const struct cpumask *affinity, struct module *owner)
 +		       struct module *owner)
  {
 -	const struct cpumask *mask = NULL;
  	struct irq_desc *desc;
++<<<<<<< HEAD
 +	int i;
 +
 +	for (i = 0; i < cnt; i++) {
 +		desc = alloc_desc(start + i, node, owner);
++=======
+ 	unsigned int flags;
+ 	int i;
+ 
+ 	/* Validate affinity mask(s) */
+ 	if (affinity) {
+ 		for (i = 0, mask = affinity; i < cnt; i++, mask++) {
+ 			if (cpumask_empty(mask))
+ 				return -EINVAL;
+ 		}
+ 	}
+ 
+ 	flags = affinity ? IRQD_AFFINITY_MANAGED : 0;
+ 	mask = NULL;
+ 
+ 	for (i = 0; i < cnt; i++) {
+ 		if (affinity) {
+ 			node = cpu_to_node(cpumask_first(affinity));
+ 			mask = affinity;
+ 			affinity++;
+ 		}
+ 		desc = alloc_desc(start + i, node, flags, mask, owner);
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
  		if (!desc)
  			goto err;
  		mutex_lock(&sparse_irq_lock);
@@@ -368,6 -480,9 +392,12 @@@ EXPORT_SYMBOL_GPL(irq_free_descs)
   * @cnt:	Number of consecutive irqs to allocate.
   * @node:	Preferred node on which the irq descriptor should be allocated
   * @owner:	Owning module (can be NULL)
++<<<<<<< HEAD
++=======
+  * @affinity:	Optional pointer to an affinity mask array of size @cnt which
+  *		hints where the irq descriptors should be allocated and which
+  *		default affinities to use
++>>>>>>> e75eafb9b039 (genirq/msi: Switch to new irq spreading infrastructure)
   *
   * Returns the first irq number or error code
   */
* Unmerged path drivers/pci/msi.c
* Unmerged path kernel/irq/irqdesc.c
