x86/retpoline: Add initial retpoline support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] retpoline: Add initial retpoline support (Josh Poimboeuf) [1535644]
Rebuild_FUZZ: 95.24%
commit-author David Woodhouse <dwmw@amazon.co.uk>
commit 76b043848fd22dbf7f8bf3a1452f8c70d557b860
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/76b04384.failed

Enable the use of -mindirect-branch=thunk-extern in newer GCC, and provide
the corresponding thunks. Provide assembler macros for invoking the thunks
in the same way that GCC does, from native and inline assembler.

This adds X86_FEATURE_RETPOLINE and sets it by default on all CPUs. In
some circumstances, IBRS microcode features may be used instead, and the
retpoline can be disabled.

On AMD CPUs if lfence is serialising, the retpoline can be dramatically
simplified to a simple "lfence; jmp *\reg". A future patch, after it has
been verified that lfence really is serialising in all circumstances, can
enable this by setting the X86_FEATURE_RETPOLINE_AMD feature bit in addition
to X86_FEATURE_RETPOLINE.

Do not align the retpoline in the altinstr section, because there is no
guarantee that it stays aligned when it's copied over the oldinstr during
alternative patching.

[ Andi Kleen: Rename the macros, add CONFIG_RETPOLINE option, export thunks]
[ tglx: Put actual function CALL/JMP in front of the macros, convert to
  	symbolic labels ]
[ dwmw2: Convert back to numeric labels, merge objtool fixes ]

	Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Arjan van de Ven <arjan@linux.intel.com>
	Acked-by: Ingo Molnar <mingo@kernel.org>
	Cc: gnomes@lxorguk.ukuu.org.uk
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Andi Kleen <ak@linux.intel.com>
	Cc: Josh Poimboeuf <jpoimboe@redhat.com>
	Cc: thomas.lendacky@amd.com
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Jiri Kosina <jikos@kernel.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Kees Cook <keescook@google.com>
	Cc: Tim Chen <tim.c.chen@linux.intel.com>
	Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org>
	Cc: Paul Turner <pjt@google.com>
Link: https://lkml.kernel.org/r/1515707194-20531-4-git-send-email-dwmw@amazon.co.uk

(cherry picked from commit 76b043848fd22dbf7f8bf3a1452f8c70d557b860)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/include/asm/asm-prototypes.h
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/kernel/cpu/common.c
diff --cc arch/x86/Kconfig
index 0912e61d8b57,d1819161cc6c..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -358,8 -429,21 +358,26 @@@ config GOLDFIS
         def_bool y
         depends on X86_GOLDFISH
  
++<<<<<<< HEAD
 +config INTEL_RDT_A
 +	bool "Intel Resource Director Technology Allocation support"
++=======
+ config RETPOLINE
+ 	bool "Avoid speculative indirect branches in kernel"
+ 	default y
+ 	help
+ 	  Compile kernel with the retpoline compiler options to guard against
+ 	  kernel-to-user data leaks by avoiding speculative indirect
+ 	  branches. Requires a compiler with -mindirect-branch=thunk-extern
+ 	  support for full protection. The kernel may run slower.
+ 
+ 	  Without compiler support, at least indirect branches in assembler
+ 	  code are eliminated. Since this includes the syscall entry path,
+ 	  it is not entirely pointless.
+ 
+ config INTEL_RDT
+ 	bool "Intel Resource Director Technology support"
++>>>>>>> 76b043848fd2 (x86/retpoline: Add initial retpoline support)
  	default n
  	depends on X86 && CPU_SUP_INTEL
  	select KERNFS
diff --cc arch/x86/kernel/cpu/common.c
index 3eec1147ce1e,7a671d1ae3cb..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -808,29 -877,47 +808,37 @@@ static void __init early_identify_cpu(s
  	memset(&c->x86_capability, 0, sizeof c->x86_capability);
  	c->extended_cpuid_level = 0;
  
 -	/* cyrix could have cpuid enabled via c_identify()*/
 -	if (have_cpuid_p()) {
 -		cpu_detect(c);
 -		get_cpu_vendor(c);
 -		get_cpu_cap(c);
 -		setup_force_cpu_cap(X86_FEATURE_CPUID);
 -
 -		if (this_cpu->c_early_init)
 -			this_cpu->c_early_init(c);
 -
 -		c->cpu_index = 0;
 -		filter_cpuid_features(c, false);
 -
 -		if (this_cpu->c_bsp_init)
 -			this_cpu->c_bsp_init(c);
 -	} else {
 +	if (!have_cpuid_p())
  		identify_cpu_without_cpuid(c);
 -		setup_clear_cpu_cap(X86_FEATURE_CPUID);
 -	}
  
 -	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 +	/* cyrix could have cpuid enabled via c_identify()*/
 +	if (!have_cpuid_p())
 +		return;
  
 -	if (c->x86_vendor != X86_VENDOR_AMD)
 -		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 +	cpu_detect(c);
  
 -	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);
 -	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
 +	get_cpu_vendor(c);
  
++<<<<<<< HEAD
 +	get_cpu_cap(c);
++=======
+ #ifdef CONFIG_RETPOLINE
+ 	setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
+ #endif
+ 
+ 	fpu__init_system(c);
++>>>>>>> 76b043848fd2 (x86/retpoline: Add initial retpoline support)
  
 -#ifdef CONFIG_X86_32
 -	/*
 -	 * Regardless of whether PCID is enumerated, the SDM says
 -	 * that it can't be enabled in 32-bit mode.
 -	 */
 -	setup_clear_cpu_cap(X86_FEATURE_PCID);
 -#endif
 +	get_model_name(c); /* RHEL7: get default name for unsupported check */
 +
 +	if (this_cpu->c_early_init)
 +		this_cpu->c_early_init(c);
 +
 +	c->cpu_index = 0;
 +	filter_cpuid_features(c, false);
 +
 +	if (this_cpu->c_bsp_init)
 +		this_cpu->c_bsp_init(c);
  }
  
  void __init early_cpu_init(void)
* Unmerged path arch/x86/include/asm/asm-prototypes.h
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 30f91aa8b548..3ee903edb7b3 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -157,6 +157,16 @@ KBUILD_CFLAGS += $(call cc-option,-mno-avx,)
 KBUILD_CFLAGS += $(mflags-y)
 KBUILD_AFLAGS += $(mflags-y)
 
+# Avoid indirect branches in kernel to deal with Spectre
+ifdef CONFIG_RETPOLINE
+    RETPOLINE_CFLAGS += $(call cc-option,-mindirect-branch=thunk-extern -mindirect-branch-register)
+    ifneq ($(RETPOLINE_CFLAGS),)
+        KBUILD_CFLAGS += $(RETPOLINE_CFLAGS) -DRETPOLINE
+    else
+        $(warning CONFIG_RETPOLINE=y, but not supported by the compiler. Toolchain update recommended.)
+    endif
+endif
+
 archscripts: scripts_basic
 	$(Q)$(MAKE) $(build)=arch/x86/tools relocs
 
* Unmerged path arch/x86/include/asm/asm-prototypes.h
* Unmerged path arch/x86/include/asm/cpufeatures.h
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
new file mode 100644
index 000000000000..e20e92ef2ca8
--- /dev/null
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -0,0 +1,128 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#ifndef __NOSPEC_BRANCH_H__
+#define __NOSPEC_BRANCH_H__
+
+#include <asm/alternative.h>
+#include <asm/alternative-asm.h>
+#include <asm/cpufeatures.h>
+
+#ifdef __ASSEMBLY__
+
+/*
+ * This should be used immediately before a retpoline alternative.  It tells
+ * objtool where the retpolines are so that it can make sense of the control
+ * flow by just reading the original instruction(s) and ignoring the
+ * alternatives.
+ */
+.macro ANNOTATE_NOSPEC_ALTERNATIVE
+	.Lannotate_\@:
+	.pushsection .discard.nospec
+	.long .Lannotate_\@ - .
+	.popsection
+.endm
+
+/*
+ * These are the bare retpoline primitives for indirect jmp and call.
+ * Do not use these directly; they only exist to make the ALTERNATIVE
+ * invocation below less ugly.
+ */
+.macro RETPOLINE_JMP reg:req
+	call	.Ldo_rop_\@
+.Lspec_trap_\@:
+	pause
+	jmp	.Lspec_trap_\@
+.Ldo_rop_\@:
+	mov	\reg, (%_ASM_SP)
+	ret
+.endm
+
+/*
+ * This is a wrapper around RETPOLINE_JMP so the called function in reg
+ * returns to the instruction after the macro.
+ */
+.macro RETPOLINE_CALL reg:req
+	jmp	.Ldo_call_\@
+.Ldo_retpoline_jmp_\@:
+	RETPOLINE_JMP \reg
+.Ldo_call_\@:
+	call	.Ldo_retpoline_jmp_\@
+.endm
+
+/*
+ * JMP_NOSPEC and CALL_NOSPEC macros can be used instead of a simple
+ * indirect jmp/call which may be susceptible to the Spectre variant 2
+ * attack.
+ */
+.macro JMP_NOSPEC reg:req
+#ifdef CONFIG_RETPOLINE
+	ANNOTATE_NOSPEC_ALTERNATIVE
+	ALTERNATIVE_2 __stringify(jmp *\reg),				\
+		__stringify(RETPOLINE_JMP \reg), X86_FEATURE_RETPOLINE,	\
+		__stringify(lfence; jmp *\reg), X86_FEATURE_RETPOLINE_AMD
+#else
+	jmp	*\reg
+#endif
+.endm
+
+.macro CALL_NOSPEC reg:req
+#ifdef CONFIG_RETPOLINE
+	ANNOTATE_NOSPEC_ALTERNATIVE
+	ALTERNATIVE_2 __stringify(call *\reg),				\
+		__stringify(RETPOLINE_CALL \reg), X86_FEATURE_RETPOLINE,\
+		__stringify(lfence; call *\reg), X86_FEATURE_RETPOLINE_AMD
+#else
+	call	*\reg
+#endif
+.endm
+
+#else /* __ASSEMBLY__ */
+
+#define ANNOTATE_NOSPEC_ALTERNATIVE				\
+	"999:\n\t"						\
+	".pushsection .discard.nospec\n\t"			\
+	".long 999b - .\n\t"					\
+	".popsection\n\t"
+
+#if defined(CONFIG_X86_64) && defined(RETPOLINE)
+
+/*
+ * Since the inline asm uses the %V modifier which is only in newer GCC,
+ * the 64-bit one is dependent on RETPOLINE not CONFIG_RETPOLINE.
+ */
+# define CALL_NOSPEC						\
+	ANNOTATE_NOSPEC_ALTERNATIVE				\
+	ALTERNATIVE(						\
+	"call *%[thunk_target]\n",				\
+	"call __x86_indirect_thunk_%V[thunk_target]\n",		\
+	X86_FEATURE_RETPOLINE)
+# define THUNK_TARGET(addr) [thunk_target] "r" (addr)
+
+#elif defined(CONFIG_X86_32) && defined(CONFIG_RETPOLINE)
+/*
+ * For i386 we use the original ret-equivalent retpoline, because
+ * otherwise we'll run out of registers. We don't care about CET
+ * here, anyway.
+ */
+# define CALL_NOSPEC ALTERNATIVE("call *%[thunk_target]\n",	\
+	"       jmp    904f;\n"					\
+	"       .align 16\n"					\
+	"901:	call   903f;\n"					\
+	"902:	pause;\n"					\
+	"       jmp    902b;\n"					\
+	"       .align 16\n"					\
+	"903:	addl   $4, %%esp;\n"				\
+	"       pushl  %[thunk_target];\n"			\
+	"       ret;\n"						\
+	"       .align 16\n"					\
+	"904:	call   901b;\n",				\
+	X86_FEATURE_RETPOLINE)
+
+# define THUNK_TARGET(addr) [thunk_target] "rm" (addr)
+#else /* No retpoline */
+# define CALL_NOSPEC "call *%[thunk_target]\n"
+# define THUNK_TARGET(addr) [thunk_target] "rm" (addr)
+#endif
+
+#endif /* __ASSEMBLY__ */
+#endif /* __NOSPEC_BRANCH_H__ */
* Unmerged path arch/x86/kernel/cpu/common.c
diff --git a/arch/x86/lib/Makefile b/arch/x86/lib/Makefile
index 502faf0d9880..4f04e54d4a0d 100644
--- a/arch/x86/lib/Makefile
+++ b/arch/x86/lib/Makefile
@@ -33,6 +33,7 @@ lib-$(CONFIG_SMP) += rwlock.o
 lib-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem.o
 lib-$(CONFIG_INSTRUCTION_DECODER) += insn.o inat.o
 lib-$(CONFIG_RANDOMIZE_BASE) += kaslr.o
+lib-$(CONFIG_RETPOLINE) += retpoline.o
 
 obj-y += msr.o msr-reg.o msr-reg-export.o
 
diff --git a/arch/x86/lib/retpoline.S b/arch/x86/lib/retpoline.S
new file mode 100644
index 000000000000..cb45c6cb465f
--- /dev/null
+++ b/arch/x86/lib/retpoline.S
@@ -0,0 +1,48 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#include <linux/stringify.h>
+#include <linux/linkage.h>
+#include <asm/dwarf2.h>
+#include <asm/cpufeatures.h>
+#include <asm/alternative-asm.h>
+#include <asm/export.h>
+#include <asm/nospec-branch.h>
+
+.macro THUNK reg
+	.section .text.__x86.indirect_thunk.\reg
+
+ENTRY(__x86_indirect_thunk_\reg)
+	CFI_STARTPROC
+	JMP_NOSPEC %\reg
+	CFI_ENDPROC
+ENDPROC(__x86_indirect_thunk_\reg)
+.endm
+
+/*
+ * Despite being an assembler file we can't just use .irp here
+ * because __KSYM_DEPS__ only uses the C preprocessor and would
+ * only see one instance of "__x86_indirect_thunk_\reg" rather
+ * than one per register with the correct names. So we do it
+ * the simple and nasty way...
+ */
+#define EXPORT_THUNK(reg) EXPORT_SYMBOL(__x86_indirect_thunk_ ## reg)
+#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)
+
+GENERATE_THUNK(_ASM_AX)
+GENERATE_THUNK(_ASM_BX)
+GENERATE_THUNK(_ASM_CX)
+GENERATE_THUNK(_ASM_DX)
+GENERATE_THUNK(_ASM_SI)
+GENERATE_THUNK(_ASM_DI)
+GENERATE_THUNK(_ASM_BP)
+GENERATE_THUNK(_ASM_SP)
+#ifdef CONFIG_64BIT
+GENERATE_THUNK(r8)
+GENERATE_THUNK(r9)
+GENERATE_THUNK(r10)
+GENERATE_THUNK(r11)
+GENERATE_THUNK(r12)
+GENERATE_THUNK(r13)
+GENERATE_THUNK(r14)
+GENERATE_THUNK(r15)
+#endif
