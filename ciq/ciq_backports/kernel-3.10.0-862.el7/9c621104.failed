blk-mq-sched: don't run the queue async from blk_mq_try_issue_directly()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 9c62110454b088b4914ffe375c2dbc19643eac34
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/9c621104.failed

If we have scheduling enabled, we jump directly to insert-and-run.
That's fine, but we run the queue async and we don't pass in information
on whether we can block from this context or not. Fixup both these
cases.

	Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Reviewed-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 9c62110454b088b4914ffe375c2dbc19643eac34)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 3e6f9b3d2b64,a4546f060e80..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1303,56 -1426,18 +1303,61 @@@ insert_rq
  	}
  }
  
 -static blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx, struct request *rq)
 +struct blk_map_ctx {
 +	struct blk_mq_hw_ctx *hctx;
 +	struct blk_mq_ctx *ctx;
 +};
 +
 +static struct request *blk_mq_map_request(struct request_queue *q,
 +					  struct bio *bio,
 +					  struct blk_map_ctx *data)
  {
 -	if (rq->tag != -1)
 -		return blk_tag_to_qc_t(rq->tag, hctx->queue_num, false);
 +	struct blk_mq_hw_ctx *hctx;
 +	struct blk_mq_ctx *ctx;
 +	struct request *rq;
 +	int rw = bio_data_dir(bio);
 +	struct blk_mq_alloc_data alloc_data;
  
 -	return blk_tag_to_qc_t(rq->internal_tag, hctx->queue_num, true);
 -}
 +	blk_queue_enter_live(q);
 +	ctx = blk_mq_get_ctx(q);
 +	hctx = q->mq_ops->map_queue(q, ctx->cpu);
  
 -static void blk_mq_try_issue_directly(struct request *rq, blk_qc_t *cookie,
 -				      bool may_sleep)
 +	if (rw_is_sync(bio->bi_rw))
 +		rw |= REQ_SYNC;
 +
 +	trace_block_getrq(q, bio, rw);
 +	blk_mq_set_alloc_data(&alloc_data, q, BLK_MQ_REQ_NOWAIT, ctx, hctx);
 +	rq = __blk_mq_alloc_request(&alloc_data, rw);
 +	if (unlikely(!rq)) {
 +		__blk_mq_run_hw_queue(hctx);
 +		blk_mq_put_ctx(ctx);
 +		trace_block_sleeprq(q, bio, rw);
 +
 +		ctx = blk_mq_get_ctx(q);
 +		hctx = q->mq_ops->map_queue(q, ctx->cpu);
 +		blk_mq_set_alloc_data(&alloc_data, q, 0, ctx, hctx);
 +		rq = __blk_mq_alloc_request(&alloc_data, rw);
 +		ctx = alloc_data.ctx;
 +		hctx = alloc_data.hctx;
 +	}
 +
 +	hctx->queued++;
 +	data->hctx = hctx;
 +	data->ctx = ctx;
 +	return rq;
 +}
 +
++<<<<<<< HEAD
 +static void blk_mq_try_issue_directly(struct request *rq)
++=======
++static void blk_mq_try_issue_directly(struct request *rq, blk_qc_t *cookie,
++				      bool may_sleep)
++>>>>>>> 9c62110454b0 (blk-mq-sched: don't run the queue async from blk_mq_try_issue_directly())
  {
 +	int ret;
  	struct request_queue *q = rq->q;
 +	struct blk_mq_hw_ctx *hctx = q->mq_ops->map_queue(q,
 +			rq->mq_ctx->cpu);
  	struct blk_mq_queue_data bd = {
  		.rq = rq,
  		.list = NULL,
@@@ -1380,7 -1476,7 +1385,11 @@@
  	}
  
  insert:
++<<<<<<< HEAD
 +	blk_mq_insert_request(rq, false, true, true);
++=======
+ 	blk_mq_sched_insert_request(rq, false, true, false, may_sleep);
++>>>>>>> 9c62110454b0 (blk-mq-sched: don't run the queue async from blk_mq_try_issue_directly())
  }
  
  /*
@@@ -1455,16 -1570,24 +1464,24 @@@ static void blk_mq_make_request(struct 
  
  		if (!(data.hctx->flags & BLK_MQ_F_BLOCKING)) {
  			rcu_read_lock();
++<<<<<<< HEAD
 +			blk_mq_try_issue_directly(old_rq);
 +			rcu_read_unlock();
 +		} else {
 +			srcu_idx = srcu_read_lock(&data.hctx->queue_rq_srcu);
 +			blk_mq_try_issue_directly(old_rq);
++=======
+ 			blk_mq_try_issue_directly(old_rq, &cookie, false);
+ 			rcu_read_unlock();
+ 		} else {
+ 			srcu_idx = srcu_read_lock(&data.hctx->queue_rq_srcu);
+ 			blk_mq_try_issue_directly(old_rq, &cookie, true);
++>>>>>>> 9c62110454b0 (blk-mq-sched: don't run the queue async from blk_mq_try_issue_directly())
  			srcu_read_unlock(&data.hctx->queue_rq_srcu, srcu_idx);
  		}
 -		goto done;
 +		return;
  	}
  
 -	if (q->elevator) {
 -elv_insert:
 -		blk_mq_put_ctx(data.ctx);
 -		blk_mq_bio_to_request(rq, bio);
 -		blk_mq_sched_insert_request(rq, false, true,
 -						!is_sync || is_flush_fua, true);
 -		goto done;
 -	}
  	if (!blk_mq_merge_queue_io(data.hctx, data.ctx, rq, bio)) {
  		/*
  		 * For a SYNC request, send it to the hardware immediately. For
* Unmerged path block/blk-mq.c
