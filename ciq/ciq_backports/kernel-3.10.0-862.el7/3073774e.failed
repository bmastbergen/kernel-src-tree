KVM: PPC: Book3S HV: Drop prepare_done from struct kvm_resize_hpt

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Serhii Popovych <spopovyc@redhat.com>
commit 3073774e638ef18d222465fe92bfc8fccb90d288
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/3073774e.failed

Currently the kvm_resize_hpt structure has two fields relevant to the
state of an ongoing resize: 'prepare_done', which indicates whether
the worker thread has completed or not, and 'error' which indicates
whether it was successful or not.

Since the success/failure isn't known until completion, this is
confusingly redundant.  This patch consolidates the information into
just the 'error' value: -EBUSY indicates the worked is still in
progress, other negative values indicate (completed) failure, 0
indicates successful completion.

As a bonus this reduces size of struct kvm_resize_hpt by
__alignof__(struct kvm_hpt_info) and saves few bytes of code.

While there correct comment in struct kvm_resize_hpt which references
a non-existent semaphore (leftover from an early draft).

Assert with WARN_ON() in case of HPT allocation thread work runs more
than once for resize request or resize_hpt_allocate() returns -EBUSY
that is treated specially.

Change comparison against zero to make checkpatch.pl happy.

	Cc: stable@vger.kernel.org # v4.10+
	Signed-off-by: Serhii Popovych <spopovyc@redhat.com>
[dwg: Changed BUG_ON()s to WARN_ON()s and altered commit message for
 clarity]
	Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
	Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
(cherry picked from commit 3073774e638ef18d222465fe92bfc8fccb90d288)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_64_mmu_hv.c
diff --cc arch/powerpc/kvm/book3s_64_mmu_hv.c
index 283e37e10f56,f5f2c6bf5856..000000000000
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@@ -47,37 -57,49 +47,62 @@@
  static long kvmppc_virtmode_do_h_enter(struct kvm *kvm, unsigned long flags,
  				long pte_index, unsigned long pteh,
  				unsigned long ptel, unsigned long *pte_idx_ret);
 +static void kvmppc_rmap_reset(struct kvm *kvm);
  
++<<<<<<< HEAD
 +long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)
++=======
+ struct kvm_resize_hpt {
+ 	/* These fields read-only after init */
+ 	struct kvm *kvm;
+ 	struct work_struct work;
+ 	u32 order;
+ 
+ 	/* These fields protected by kvm->lock */
+ 
+ 	/* Possible values and their usage:
+ 	 *  <0     an error occurred during allocation,
+ 	 *  -EBUSY allocation is in the progress,
+ 	 *  0      allocation made successfuly.
+ 	 */
+ 	int error;
+ 
+ 	/* Private to the work thread, until error != -EBUSY,
+ 	 * then protected by kvm->lock.
+ 	 */
+ 	struct kvm_hpt_info hpt;
+ };
+ 
+ int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order)
++>>>>>>> 3073774e638e (KVM: PPC: Book3S HV: Drop prepare_done from struct kvm_resize_hpt)
  {
  	unsigned long hpt = 0;
 -	int cma = 0;
 -	struct page *page = NULL;
  	struct revmap_entry *rev;
 -	unsigned long npte;
 +	struct page *page = NULL;
 +	long order = KVM_DEFAULT_HPT_ORDER;
  
 -	if ((order < PPC_MIN_HPT_ORDER) || (order > PPC_MAX_HPT_ORDER))
 -		return -EINVAL;
 +	if (htab_orderp) {
 +		order = *htab_orderp;
 +		if (order < PPC_MIN_HPT_ORDER)
 +			order = PPC_MIN_HPT_ORDER;
 +	}
  
 -	page = kvm_alloc_hpt_cma(1ul << (order - PAGE_SHIFT));
 +	kvm->arch.hpt_cma_alloc = 0;
 +	VM_BUG_ON(order < KVM_CMA_CHUNK_ORDER);
 +	page = kvm_alloc_hpt(1 << (order - PAGE_SHIFT));
  	if (page) {
  		hpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));
 -		memset((void *)hpt, 0, (1ul << order));
 -		cma = 1;
 +		kvm->arch.hpt_cma_alloc = 1;
  	}
  
 -	if (!hpt)
 -		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_RETRY_MAYFAIL
 -				       |__GFP_NOWARN, order - PAGE_SHIFT);
 +	/* Lastly try successively smaller sizes from the page allocator */
 +	/* Only do this if userspace didn't specify a size via ioctl */
 +	while (!hpt && order > PPC_MIN_HPT_ORDER && !htab_orderp) {
 +		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT|
 +				       __GFP_NOWARN, order - PAGE_SHIFT);
 +		if (!hpt)
 +			--order;
 +	}
  
  	if (!hpt)
  		return -ENOMEM;
@@@ -1119,6 -1216,372 +1144,375 @@@ void kvmppc_unpin_guest_page(struct kv
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * HPT resizing
+  */
+ static int resize_hpt_allocate(struct kvm_resize_hpt *resize)
+ {
+ 	int rc;
+ 
+ 	rc = kvmppc_allocate_hpt(&resize->hpt, resize->order);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_allocate(): HPT @ 0x%lx\n",
+ 			 resize->hpt.virt);
+ 
+ 	return 0;
+ }
+ 
+ static unsigned long resize_hpt_rehash_hpte(struct kvm_resize_hpt *resize,
+ 					    unsigned long idx)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	struct kvm_hpt_info *old = &kvm->arch.hpt;
+ 	struct kvm_hpt_info *new = &resize->hpt;
+ 	unsigned long old_hash_mask = (1ULL << (old->order - 7)) - 1;
+ 	unsigned long new_hash_mask = (1ULL << (new->order - 7)) - 1;
+ 	__be64 *hptep, *new_hptep;
+ 	unsigned long vpte, rpte, guest_rpte;
+ 	int ret;
+ 	struct revmap_entry *rev;
+ 	unsigned long apsize, avpn, pteg, hash;
+ 	unsigned long new_idx, new_pteg, replace_vpte;
+ 	int pshift;
+ 
+ 	hptep = (__be64 *)(old->virt + (idx << 4));
+ 
+ 	/* Guest is stopped, so new HPTEs can't be added or faulted
+ 	 * in, only unmapped or altered by host actions.  So, it's
+ 	 * safe to check this before we take the HPTE lock */
+ 	vpte = be64_to_cpu(hptep[0]);
+ 	if (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))
+ 		return 0; /* nothing to do */
+ 
+ 	while (!try_lock_hpte(hptep, HPTE_V_HVLOCK))
+ 		cpu_relax();
+ 
+ 	vpte = be64_to_cpu(hptep[0]);
+ 
+ 	ret = 0;
+ 	if (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))
+ 		/* Nothing to do */
+ 		goto out;
+ 
+ 	/* Unmap */
+ 	rev = &old->rev[idx];
+ 	guest_rpte = rev->guest_rpte;
+ 
+ 	ret = -EIO;
+ 	apsize = kvmppc_actual_pgsz(vpte, guest_rpte);
+ 	if (!apsize)
+ 		goto out;
+ 
+ 	if (vpte & HPTE_V_VALID) {
+ 		unsigned long gfn = hpte_rpn(guest_rpte, apsize);
+ 		int srcu_idx = srcu_read_lock(&kvm->srcu);
+ 		struct kvm_memory_slot *memslot =
+ 			__gfn_to_memslot(kvm_memslots(kvm), gfn);
+ 
+ 		if (memslot) {
+ 			unsigned long *rmapp;
+ 			rmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];
+ 
+ 			lock_rmap(rmapp);
+ 			kvmppc_unmap_hpte(kvm, idx, memslot, rmapp, gfn);
+ 			unlock_rmap(rmapp);
+ 		}
+ 
+ 		srcu_read_unlock(&kvm->srcu, srcu_idx);
+ 	}
+ 
+ 	/* Reload PTE after unmap */
+ 	vpte = be64_to_cpu(hptep[0]);
+ 
+ 	BUG_ON(vpte & HPTE_V_VALID);
+ 	BUG_ON(!(vpte & HPTE_V_ABSENT));
+ 
+ 	ret = 0;
+ 	if (!(vpte & HPTE_V_BOLTED))
+ 		goto out;
+ 
+ 	rpte = be64_to_cpu(hptep[1]);
+ 	pshift = kvmppc_hpte_base_page_shift(vpte, rpte);
+ 	avpn = HPTE_V_AVPN_VAL(vpte) & ~(((1ul << pshift) - 1) >> 23);
+ 	pteg = idx / HPTES_PER_GROUP;
+ 	if (vpte & HPTE_V_SECONDARY)
+ 		pteg = ~pteg;
+ 
+ 	if (!(vpte & HPTE_V_1TB_SEG)) {
+ 		unsigned long offset, vsid;
+ 
+ 		/* We only have 28 - 23 bits of offset in avpn */
+ 		offset = (avpn & 0x1f) << 23;
+ 		vsid = avpn >> 5;
+ 		/* We can find more bits from the pteg value */
+ 		if (pshift < 23)
+ 			offset |= ((vsid ^ pteg) & old_hash_mask) << pshift;
+ 
+ 		hash = vsid ^ (offset >> pshift);
+ 	} else {
+ 		unsigned long offset, vsid;
+ 
+ 		/* We only have 40 - 23 bits of seg_off in avpn */
+ 		offset = (avpn & 0x1ffff) << 23;
+ 		vsid = avpn >> 17;
+ 		if (pshift < 23)
+ 			offset |= ((vsid ^ (vsid << 25) ^ pteg) & old_hash_mask) << pshift;
+ 
+ 		hash = vsid ^ (vsid << 25) ^ (offset >> pshift);
+ 	}
+ 
+ 	new_pteg = hash & new_hash_mask;
+ 	if (vpte & HPTE_V_SECONDARY) {
+ 		BUG_ON(~pteg != (hash & old_hash_mask));
+ 		new_pteg = ~new_pteg;
+ 	} else {
+ 		BUG_ON(pteg != (hash & old_hash_mask));
+ 	}
+ 
+ 	new_idx = new_pteg * HPTES_PER_GROUP + (idx % HPTES_PER_GROUP);
+ 	new_hptep = (__be64 *)(new->virt + (new_idx << 4));
+ 
+ 	replace_vpte = be64_to_cpu(new_hptep[0]);
+ 
+ 	if (replace_vpte & (HPTE_V_VALID | HPTE_V_ABSENT)) {
+ 		BUG_ON(new->order >= old->order);
+ 
+ 		if (replace_vpte & HPTE_V_BOLTED) {
+ 			if (vpte & HPTE_V_BOLTED)
+ 				/* Bolted collision, nothing we can do */
+ 				ret = -ENOSPC;
+ 			/* Discard the new HPTE */
+ 			goto out;
+ 		}
+ 
+ 		/* Discard the previous HPTE */
+ 	}
+ 
+ 	new_hptep[1] = cpu_to_be64(rpte);
+ 	new->rev[new_idx].guest_rpte = guest_rpte;
+ 	/* No need for a barrier, since new HPT isn't active */
+ 	new_hptep[0] = cpu_to_be64(vpte);
+ 	unlock_hpte(new_hptep, vpte);
+ 
+ out:
+ 	unlock_hpte(hptep, vpte);
+ 	return ret;
+ }
+ 
+ static int resize_hpt_rehash(struct kvm_resize_hpt *resize)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	unsigned  long i;
+ 	int rc;
+ 
+ 	/*
+ 	 * resize_hpt_rehash_hpte() doesn't handle the new-format HPTEs
+ 	 * that POWER9 uses, and could well hit a BUG_ON on POWER9.
+ 	 */
+ 	if (cpu_has_feature(CPU_FTR_ARCH_300))
+ 		return -EIO;
+ 	for (i = 0; i < kvmppc_hpt_npte(&kvm->arch.hpt); i++) {
+ 		rc = resize_hpt_rehash_hpte(resize, i);
+ 		if (rc != 0)
+ 			return rc;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void resize_hpt_pivot(struct kvm_resize_hpt *resize)
+ {
+ 	struct kvm *kvm = resize->kvm;
+ 	struct kvm_hpt_info hpt_tmp;
+ 
+ 	/* Exchange the pending tables in the resize structure with
+ 	 * the active tables */
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_pivot()\n");
+ 
+ 	spin_lock(&kvm->mmu_lock);
+ 	asm volatile("ptesync" : : : "memory");
+ 
+ 	hpt_tmp = kvm->arch.hpt;
+ 	kvmppc_set_hpt(kvm, &resize->hpt);
+ 	resize->hpt = hpt_tmp;
+ 
+ 	spin_unlock(&kvm->mmu_lock);
+ 
+ 	synchronize_srcu_expedited(&kvm->srcu);
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_pivot() done\n");
+ }
+ 
+ static void resize_hpt_release(struct kvm *kvm, struct kvm_resize_hpt *resize)
+ {
+ 	BUG_ON(kvm->arch.resize_hpt != resize);
+ 
+ 	if (!resize)
+ 		return;
+ 
+ 	if (resize->hpt.virt)
+ 		kvmppc_free_hpt(&resize->hpt);
+ 
+ 	kvm->arch.resize_hpt = NULL;
+ 	kfree(resize);
+ }
+ 
+ static void resize_hpt_prepare_work(struct work_struct *work)
+ {
+ 	struct kvm_resize_hpt *resize = container_of(work,
+ 						     struct kvm_resize_hpt,
+ 						     work);
+ 	struct kvm *kvm = resize->kvm;
+ 	int err;
+ 
+ 	if (WARN_ON(resize->error != -EBUSY))
+ 		return;
+ 
+ 	resize_hpt_debug(resize, "resize_hpt_prepare_work(): order = %d\n",
+ 			 resize->order);
+ 
+ 	err = resize_hpt_allocate(resize);
+ 
+ 	/* We have strict assumption about -EBUSY
+ 	 * when preparing for HPT resize.
+ 	 */
+ 	if (WARN_ON(err == -EBUSY))
+ 		err = -EINPROGRESS;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize->error = err;
+ 
+ 	mutex_unlock(&kvm->lock);
+ }
+ 
+ long kvm_vm_ioctl_resize_hpt_prepare(struct kvm *kvm,
+ 				     struct kvm_ppc_resize_hpt *rhpt)
+ {
+ 	unsigned long flags = rhpt->flags;
+ 	unsigned long shift = rhpt->shift;
+ 	struct kvm_resize_hpt *resize;
+ 	int ret;
+ 
+ 	if (flags != 0 || kvm_is_radix(kvm))
+ 		return -EINVAL;
+ 
+ 	if (shift && ((shift < 18) || (shift > 46)))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize = kvm->arch.resize_hpt;
+ 
+ 	if (resize) {
+ 		if (resize->order == shift) {
+ 			/* Suitable resize in progress? */
+ 			ret = resize->error;
+ 			if (ret == -EBUSY)
+ 				ret = 100; /* estimated time in ms */
+ 			else if (ret)
+ 				resize_hpt_release(kvm, resize);
+ 
+ 			goto out;
+ 		}
+ 
+ 		/* not suitable, cancel it */
+ 		resize_hpt_release(kvm, resize);
+ 	}
+ 
+ 	ret = 0;
+ 	if (!shift)
+ 		goto out; /* nothing to do */
+ 
+ 	/* start new resize */
+ 
+ 	resize = kzalloc(sizeof(*resize), GFP_KERNEL);
+ 	if (!resize) {
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	resize->error = -EBUSY;
+ 	resize->order = shift;
+ 	resize->kvm = kvm;
+ 	INIT_WORK(&resize->work, resize_hpt_prepare_work);
+ 	kvm->arch.resize_hpt = resize;
+ 
+ 	schedule_work(&resize->work);
+ 
+ 	ret = 100; /* estimated time in ms */
+ 
+ out:
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ static void resize_hpt_boot_vcpu(void *opaque)
+ {
+ 	/* Nothing to do, just force a KVM exit */
+ }
+ 
+ long kvm_vm_ioctl_resize_hpt_commit(struct kvm *kvm,
+ 				    struct kvm_ppc_resize_hpt *rhpt)
+ {
+ 	unsigned long flags = rhpt->flags;
+ 	unsigned long shift = rhpt->shift;
+ 	struct kvm_resize_hpt *resize;
+ 	long ret;
+ 
+ 	if (flags != 0 || kvm_is_radix(kvm))
+ 		return -EINVAL;
+ 
+ 	if (shift && ((shift < 18) || (shift > 46)))
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	resize = kvm->arch.resize_hpt;
+ 
+ 	/* This shouldn't be possible */
+ 	ret = -EIO;
+ 	if (WARN_ON(!kvm->arch.mmu_ready))
+ 		goto out_no_hpt;
+ 
+ 	/* Stop VCPUs from running while we mess with the HPT */
+ 	kvm->arch.mmu_ready = 0;
+ 	smp_mb();
+ 
+ 	/* Boot all CPUs out of the guest so they re-read
+ 	 * mmu_ready */
+ 	on_each_cpu(resize_hpt_boot_vcpu, NULL, 1);
+ 
+ 	ret = -ENXIO;
+ 	if (!resize || (resize->order != shift))
+ 		goto out;
+ 
+ 	ret = resize->error;
+ 	if (ret)
+ 		goto out;
+ 
+ 	ret = resize_hpt_rehash(resize);
+ 	if (ret)
+ 		goto out;
+ 
+ 	resize_hpt_pivot(resize);
+ 
+ out:
+ 	/* Let VCPUs run again */
+ 	kvm->arch.mmu_ready = 1;
+ 	smp_mb();
+ out_no_hpt:
+ 	resize_hpt_release(kvm, resize);
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ /*
++>>>>>>> 3073774e638e (KVM: PPC: Book3S HV: Drop prepare_done from struct kvm_resize_hpt)
   * Functions for reading and writing the hash table via reads and
   * writes on a file descriptor.
   *
* Unmerged path arch/powerpc/kvm/book3s_64_mmu_hv.c
