dax: clear dirty entry tags on cache flush

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jan Kara <jack@suse.cz>
commit 4b4bb46d00b386e1c972890dc5785a7966eaa9c0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/4b4bb46d.failed

Currently we never clear dirty tags in DAX mappings and thus address
ranges to flush accumulate.  Now that we have locking of radix tree
entries, we have all the locking necessary to reliably clear the radix
tree dirty tag when flushing caches for corresponding address range.
Similarly to page_mkclean() we also have to write-protect pages to get a
page fault when the page is next written to so that we can mark the
entry dirty again.

Link: http://lkml.kernel.org/r/1479460644-25076-21-git-send-email-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 4b4bb46d00b386e1c972890dc5785a7966eaa9c0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 1dfecdfb6245,a8732fbed381..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -32,6 -31,9 +32,12 @@@
  #include <linux/vmstat.h>
  #include <linux/pfn_t.h>
  #include <linux/sizes.h>
++<<<<<<< HEAD
++=======
+ #include <linux/mmu_notifier.h>
+ #include <linux/iomap.h>
+ #include "internal.h"
++>>>>>>> 4b4bb46d00b3 (dax: clear dirty entry tags on cache flush)
  
  /* We choose 4096 entries - same as per-zone page wait tables */
  #define DAX_WAIT_TABLE_BITS 12
@@@ -673,12 -610,64 +679,65 @@@ static void *dax_insert_mapping_entry(s
  		if (mapping->a_ops->freepage)
  			mapping->a_ops->freepage(entry);
  		unlock_page(entry);
 -		put_page(entry);
 +		page_cache_release(entry);
  	}
  	return new_entry;
 +
  }
  
+ static inline unsigned long
+ pgoff_address(pgoff_t pgoff, struct vm_area_struct *vma)
+ {
+ 	unsigned long address;
+ 
+ 	address = vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);
+ 	VM_BUG_ON_VMA(address < vma->vm_start || address >= vma->vm_end, vma);
+ 	return address;
+ }
+ 
+ /* Walk all mappings of a given index of a file and writeprotect them */
+ static void dax_mapping_entry_mkclean(struct address_space *mapping,
+ 				      pgoff_t index, unsigned long pfn)
+ {
+ 	struct vm_area_struct *vma;
+ 	pte_t *ptep;
+ 	pte_t pte;
+ 	spinlock_t *ptl;
+ 	bool changed;
+ 
+ 	i_mmap_lock_read(mapping);
+ 	vma_interval_tree_foreach(vma, &mapping->i_mmap, index, index) {
+ 		unsigned long address;
+ 
+ 		cond_resched();
+ 
+ 		if (!(vma->vm_flags & VM_SHARED))
+ 			continue;
+ 
+ 		address = pgoff_address(index, vma);
+ 		changed = false;
+ 		if (follow_pte(vma->vm_mm, address, &ptep, &ptl))
+ 			continue;
+ 		if (pfn != pte_pfn(*ptep))
+ 			goto unlock;
+ 		if (!pte_dirty(*ptep) && !pte_write(*ptep))
+ 			goto unlock;
+ 
+ 		flush_cache_page(vma, address, pfn);
+ 		pte = ptep_clear_flush(vma, address, ptep);
+ 		pte = pte_wrprotect(pte);
+ 		pte = pte_mkclean(pte);
+ 		set_pte_at(vma->vm_mm, address, ptep, pte);
+ 		changed = true;
+ unlock:
+ 		pte_unmap_unlock(ptep, ptl);
+ 
+ 		if (changed)
+ 			mmu_notifier_invalidate_page(vma->vm_mm, address);
+ 	}
+ 	i_mmap_unlock_read(mapping);
+ }
+ 
  static int dax_writeback_one(struct block_device *bdev,
  		struct address_space *mapping, pgoff_t index, void *entry)
  {
@@@ -726,10 -741,16 +785,22 @@@
  		goto unmap;
  	}
  
+ 	dax_mapping_entry_mkclean(mapping, index, pfn_t_to_pfn(dax.pfn));
  	wb_cache_pmem(dax.addr, dax.size);
++<<<<<<< HEAD
 +
 +	spin_lock_irq(&mapping->tree_lock);
 +	radix_tree_tag_clear(page_tree, index, PAGECACHE_TAG_TOWRITE);
++=======
+ 	/*
+ 	 * After we have flushed the cache, we can clear the dirty tag. There
+ 	 * cannot be new dirty data in the pfn after the flush has completed as
+ 	 * the pfn mappings are writeprotected and fault waits for mapping
+ 	 * entry lock.
+ 	 */
+ 	spin_lock_irq(&mapping->tree_lock);
+ 	radix_tree_tag_clear(page_tree, index, PAGECACHE_TAG_DIRTY);
++>>>>>>> 4b4bb46d00b3 (dax: clear dirty entry tags on cache flush)
  	spin_unlock_irq(&mapping->tree_lock);
   unmap:
  	dax_unmap_atomic(bdev, &dax);
* Unmerged path fs/dax.c
