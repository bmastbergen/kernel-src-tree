dm raid: fix incorrect sync_ratio when degraded

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [md] dm-raid: fix incorrect sync_ratio when degraded (Mike Snitzer) [1547979]
Rebuild_FUZZ: 97.87%
commit-author Jonathan Brassow <jbrassow@redhat.com>
commit da1e148803e0b98961599b0295418bb7a8fc79f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/da1e1488.failed

Upstream commit 4102d9de6d375 ("dm raid: fix rs_get_progress()
synchronization state/ratio") in combination with commit 7c29744ecce
("dm raid: simplify rs_get_progress()") introduced a regression by
incorrectly reporting a sync_ratio of 0 for degraded raid sets.  This
caused lvm2 to fail to repair raid legs automatically.

Fix by identifying the degraded state by checking the MD_RECOVERY_INTR
flag and returning mddev->recovery_cp in case it is set.

MD sets recovery = [ MD_RECOVERY_RECOVER MD_RECOVERY_INTR
MD_RECOVERY_NEEDED ] when a RAID member fails.  It then shuts down any
sync thread that is running and leaves us with all MD_RECOVERY_* flags
cleared.  The bug occurs if a status is requested in the short time it
takes to shut down any sync thread and clear the flags, because we were
keying in on the MD_RECOVERY_NEEDED - understanding it to be the initial
phase of a “recover” sync thread.  However, this is an incorrect
interpretation if MD_RECOVERY_INTR is also set.

This also explains why the bug only happened when automatic repair was
enabled and not a normal ‘manual’ method.  It is impossible to react
quick enough to hit the problematic window without it being automated.

Fix passes automatic repair tests.

Fixes: 7c29744ecce ("dm raid: simplify rs_get_progress()")
	Signed-off-by: Jonathan Brassow <jbrassow@redhat.com>
	Signed-off-by: Heinz Mauelshagen <heinzm@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit da1e148803e0b98961599b0295418bb7a8fc79f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-raid.c
diff --cc drivers/md/dm-raid.c
index 94db8c87956d,c1d1034ff7b7..000000000000
--- a/drivers/md/dm-raid.c
+++ b/drivers/md/dm-raid.c
@@@ -3297,39 -3405,55 +3297,49 @@@ static sector_t rs_get_progress(struct 
  
  	if (rs_is_raid0(rs)) {
  		r = resync_max_sectors;
 -		set_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);
 +		*array_in_sync = true;
  
  	} else {
++<<<<<<< HEAD
 +		r = mddev->reshape_position;
++=======
+ 		if (!test_bit(MD_RECOVERY_INTR, &recovery) &&
+ 		    (test_bit(MD_RECOVERY_NEEDED, &recovery) ||
+ 		     test_bit(MD_RECOVERY_RESHAPE, &recovery) ||
+ 		     test_bit(MD_RECOVERY_RUNNING, &recovery)))
+ 			r = mddev->curr_resync_completed;
+ 		else
+ 			r = mddev->recovery_cp;
++>>>>>>> da1e148803e0 (dm raid: fix incorrect sync_ratio when degraded)
 +
 +		/* Reshape is relative to the array size */
 +		if (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) ||
 +		    r != MaxSector) {
 +			if (r == MaxSector) {
 +				*array_in_sync = true;
 +				r = resync_max_sectors;
 +			} else {
 +				/* Got to reverse on backward reshape */
 +				if (mddev->reshape_backwards)
 +					r = mddev->array_sectors - r;
  
 -		if (r >= resync_max_sectors &&
 -		    (!test_bit(MD_RECOVERY_REQUESTED, &recovery) ||
 -		     (!test_bit(MD_RECOVERY_FROZEN, &recovery) &&
 -		      !test_bit(MD_RECOVERY_NEEDED, &recovery) &&
 -		      !test_bit(MD_RECOVERY_RUNNING, &recovery)))) {
 -			/*
 -			 * Sync complete.
 -			 */
 -			/* In case we have finished recovering, the array is in sync. */
 -			if (test_bit(MD_RECOVERY_RECOVER, &recovery))
 -				set_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);
 -
 -		} else if (test_bit(MD_RECOVERY_RECOVER, &recovery)) {
 -			/*
 -			 * In case we are recovering, the array is not in sync
 -			 * and health chars should show the recovering legs.
 -			 */
 -			;
 +				/* Devide by # of data stripes */
 +				sector_div(r, mddev_data_stripes(rs));
 +			}
  
 -		} else if (test_bit(MD_RECOVERY_SYNC, &recovery) &&
 -			   !test_bit(MD_RECOVERY_REQUESTED, &recovery)) {
 -			/*
 -			 * If "resync" is occurring, the raid set
 -			 * is or may be out of sync hence the health
 -			 * characters shall be 'a'.
 -			 */
 -			set_bit(RT_FLAG_RS_RESYNCING, &rs->runtime_flags);
 +		/* Sync is relative to the component device size */
 +		} else if (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))
 +			r = curr_resync_completed;
 +		else
 +			r = recovery_cp;
  
 -		} else if (test_bit(MD_RECOVERY_RESHAPE, &recovery) &&
 -			   !test_bit(MD_RECOVERY_REQUESTED, &recovery)) {
 +		if (r == MaxSector) {
  			/*
 -			 * If "reshape" is occurring, the raid set
 -			 * is or may be out of sync hence the health
 -			 * characters shall be 'a'.
 +			 * Sync complete.
  			 */
 -			set_bit(RT_FLAG_RS_RESYNCING, &rs->runtime_flags);
 -
 -		} else if (test_bit(MD_RECOVERY_REQUESTED, &recovery)) {
 +			*array_in_sync = true;
 +			r = resync_max_sectors;
 +		} else if (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {
  			/*
  			 * If "check" or "repair" is occurring, the raid set has
  			 * undergone an initial sync and the health characters
* Unmerged path drivers/md/dm-raid.c
