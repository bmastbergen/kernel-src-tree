IB/hf1: User context locking is inconsistent

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Michael J. Ruhl <michael.j.ruhl@intel.com>
commit d295dbeb2a0c93364444e76b3bb30f587a823e0e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d295dbeb.failed

There is a mixture of mutex and spinlocks to protect receive context
(rcd/uctxt) information.  This is not used consistently.

Use the mutex to protect device receive context information only.
Use the spinlock to protect sub context information only.

Protect access to items in the rcd array with a spinlock and
reference count.

Remove spinlock around dd->rcd array cleanup.  Since interrupts are
disabled and cleaned up before this point, this lock is not useful.

	Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Reviewed-by: Sebastian Sanchez <sebastian.sanchez@intel.com>
	Signed-off-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit d295dbeb2a0c93364444e76b3bb30f587a823e0e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/aspm.h
#	drivers/infiniband/hw/hfi1/chip.c
#	drivers/infiniband/hw/hfi1/driver.c
#	drivers/infiniband/hw/hfi1/file_ops.c
#	drivers/infiniband/hw/hfi1/hfi.h
#	drivers/infiniband/hw/hfi1/init.c
#	drivers/infiniband/hw/hfi1/trace_rx.h
#	drivers/infiniband/hw/hfi1/vnic_main.c
diff --cc drivers/infiniband/hw/hfi1/aspm.h
index 794e6814a531,522b40ed9937..000000000000
--- a/drivers/infiniband/hw/hfi1/aspm.h
+++ b/drivers/infiniband/hw/hfi1/aspm.h
@@@ -237,14 -237,17 +237,17 @@@ static inline void aspm_disable_all(str
  {
  	struct hfi1_ctxtdata *rcd;
  	unsigned long flags;
 -	u16 i;
 +	unsigned i;
  
  	for (i = 0; i < dd->first_dyn_alloc_ctxt; i++) {
- 		rcd = dd->rcd[i];
- 		del_timer_sync(&rcd->aspm_timer);
- 		spin_lock_irqsave(&rcd->aspm_lock, flags);
- 		rcd->aspm_intr_enable = false;
- 		spin_unlock_irqrestore(&rcd->aspm_lock, flags);
+ 		rcd = hfi1_rcd_get_by_index(dd, i);
+ 		if (rcd) {
+ 			del_timer_sync(&rcd->aspm_timer);
+ 			spin_lock_irqsave(&rcd->aspm_lock, flags);
+ 			rcd->aspm_intr_enable = false;
+ 			spin_unlock_irqrestore(&rcd->aspm_lock, flags);
+ 			hfi1_rcd_put(rcd);
+ 		}
  	}
  
  	aspm_disable(dd);
@@@ -284,7 -290,8 +290,12 @@@ static inline void aspm_ctx_init(struc
  
  static inline void aspm_init(struct hfi1_devdata *dd)
  {
++<<<<<<< HEAD
 +	unsigned i;
++=======
+ 	struct hfi1_ctxtdata *rcd;
+ 	u16 i;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	spin_lock_init(&dd->aspm_lock);
  	dd->aspm_supported = aspm_hw_l1_supported(dd);
diff --cc drivers/infiniband/hw/hfi1/chip.c
index 7ed61dba0ba4,1446c16bc8a8..000000000000
--- a/drivers/infiniband/hw/hfi1/chip.c
+++ b/drivers/infiniband/hw/hfi1/chip.c
@@@ -6741,8 -6791,11 +6742,16 @@@ static void rxe_freeze(struct hfi1_devd
  	clear_rcvctrl(dd, RCV_CTRL_RCV_PORT_ENABLE_SMASK);
  
  	/* disable all receive contexts */
++<<<<<<< HEAD
 +	for (i = 0; i < dd->num_rcv_contexts; i++)
 +		hfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS, i);
++=======
+ 	for (i = 0; i < dd->num_rcv_contexts; i++) {
+ 		rcd = hfi1_rcd_get_by_index(dd, i);
+ 		hfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS, rcd);
+ 		hfi1_rcd_put(rcd);
+ 	}
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  }
  
  /*
@@@ -6754,21 -6807,24 +6763,32 @@@
  static void rxe_kernel_unfreeze(struct hfi1_devdata *dd)
  {
  	u32 rcvmask;
++<<<<<<< HEAD
 +	int i;
++=======
+ 	u16 i;
+ 	struct hfi1_ctxtdata *rcd;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	/* enable all kernel contexts */
  	for (i = 0; i < dd->num_rcv_contexts; i++) {
- 		struct hfi1_ctxtdata *rcd = dd->rcd[i];
+ 		rcd = hfi1_rcd_get_by_index(dd, i);
  
  		/* Ensure all non-user contexts(including vnic) are enabled */
- 		if (!rcd || !rcd->sc || (rcd->sc->type == SC_USER))
+ 		if (!rcd || !rcd->sc || (rcd->sc->type == SC_USER)) {
+ 			hfi1_rcd_put(rcd);
  			continue;
- 
+ 		}
  		rcvmask = HFI1_RCVCTRL_CTXT_ENB;
  		/* HFI1_RCVCTRL_TAILUPD_[ENB|DIS] needs to be set explicitly */
 -		rcvmask |= HFI1_CAP_KGET_MASK(rcd->flags, DMA_RTAIL) ?
 +		rcvmask |= HFI1_CAP_KGET_MASK(dd->rcd[i]->flags, DMA_RTAIL) ?
  			HFI1_RCVCTRL_TAILUPD_ENB : HFI1_RCVCTRL_TAILUPD_DIS;
++<<<<<<< HEAD
 +		hfi1_rcvctrl(dd, rcvmask, i);
++=======
+ 		hfi1_rcvctrl(dd, rcvmask, rcd);
+ 		hfi1_rcd_put(rcd);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	}
  
  	/* enable port */
diff --cc drivers/infiniband/hw/hfi1/driver.c
index e4e01e0fe2d8,14f2a00c13c2..000000000000
--- a/drivers/infiniband/hw/hfi1/driver.c
+++ b/drivers/infiniband/hw/hfi1/driver.c
@@@ -873,9 -837,10 +873,14 @@@ bail
  	return last;
  }
  
 -static inline void set_nodma_rtail(struct hfi1_devdata *dd, u16 ctxt)
 +static inline void set_nodma_rtail(struct hfi1_devdata *dd, u8 ctxt)
  {
++<<<<<<< HEAD
 +	int i;
++=======
+ 	struct hfi1_ctxtdata *rcd;
+ 	u16 i;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	/*
  	 * For dynamically allocated kernel contexts (like vnic) switch
@@@ -888,14 -857,19 +897,23 @@@
  		return;
  	}
  
- 	for (i = HFI1_CTRL_CTXT + 1; i < dd->first_dyn_alloc_ctxt; i++)
- 		dd->rcd[i]->do_interrupt =
- 			&handle_receive_interrupt_nodma_rtail;
+ 	for (i = HFI1_CTRL_CTXT + 1; i < dd->first_dyn_alloc_ctxt; i++) {
+ 		rcd = hfi1_rcd_get_by_index(dd, i);
+ 		if (rcd)
+ 			rcd->do_interrupt =
+ 				&handle_receive_interrupt_nodma_rtail;
+ 		hfi1_rcd_put(rcd);
+ 	}
  }
  
 -static inline void set_dma_rtail(struct hfi1_devdata *dd, u16 ctxt)
 +static inline void set_dma_rtail(struct hfi1_devdata *dd, u8 ctxt)
  {
++<<<<<<< HEAD
 +	int i;
++=======
+ 	struct hfi1_ctxtdata *rcd;
+ 	u16 i;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	/*
  	 * For dynamically allocated kernel contexts (like vnic) switch
@@@ -915,7 -897,8 +941,12 @@@
  
  void set_all_slowpath(struct hfi1_devdata *dd)
  {
++<<<<<<< HEAD
 +	int i;
++=======
+ 	struct hfi1_ctxtdata *rcd;
+ 	u16 i;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	/* HFI1_CTRL_CTXT must always use the slow path interrupt handler */
  	for (i = HFI1_CTRL_CTXT + 1; i < dd->num_rcv_contexts; i++) {
@@@ -1099,7 -1090,8 +1133,12 @@@ void receive_interrupt_work(struct work
  	struct hfi1_pportdata *ppd = container_of(work, struct hfi1_pportdata,
  						  linkstate_active_work);
  	struct hfi1_devdata *dd = ppd->dd;
++<<<<<<< HEAD
 +	int i;
++=======
+ 	struct hfi1_ctxtdata *rcd;
+ 	u16 i;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	/* Received non-SC15 packet implies neighbor_normal */
  	ppd->neighbor_normal = 1;
@@@ -1300,10 -1296,9 +1343,13 @@@ void hfi1_start_led_override(struct hfi
   */
  int hfi1_reset_device(int unit)
  {
++<<<<<<< HEAD
 +	int ret, i;
++=======
+ 	int ret;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	struct hfi1_devdata *dd = hfi1_lookup(unit);
  	struct hfi1_pportdata *ppd;
- 	unsigned long flags;
  	int pidx;
  
  	if (!dd) {
@@@ -1321,13 -1316,11 +1367,19 @@@
  		goto bail;
  	}
  
- 	spin_lock_irqsave(&dd->uctxt_lock, flags);
+ 	/* If there are any user/vnic contexts, we cannot reset */
+ 	mutex_lock(&hfi1_mutex);
  	if (dd->rcd)
++<<<<<<< HEAD
 +		for (i = dd->first_dyn_alloc_ctxt;
 +		     i < dd->num_rcv_contexts; i++) {
 +			if (!dd->rcd[i] || !dd->rcd[i]->cnt)
 +				continue;
 +			spin_unlock_irqrestore(&dd->uctxt_lock, flags);
++=======
+ 		if (hfi1_stats.sps_ctxts) {
+ 			mutex_unlock(&hfi1_mutex);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  			ret = -EBUSY;
  			goto bail;
  		}
diff --cc drivers/infiniband/hw/hfi1/file_ops.c
index 8df16f5d7e38,ab8eb2bf48d8..000000000000
--- a/drivers/infiniband/hw/hfi1/file_ops.c
+++ b/drivers/infiniband/hw/hfi1/file_ops.c
@@@ -749,8 -757,7 +749,12 @@@ static int hfi1_file_close(struct inod
  	if (!uctxt)
  		goto done;
  
++<<<<<<< HEAD
 +	hfi1_cdbg(PROC, "freeing ctxt %u:%u", uctxt->ctxt, fdata->subctxt);
 +	mutex_lock(&hfi1_mutex);
++=======
+ 	hfi1_cdbg(PROC, "closing ctxt %u:%u", uctxt->ctxt, fdata->subctxt);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	flush_wc();
  	/* drain user sdma queue */
@@@ -770,13 -784,14 +781,20 @@@
  			   HFI1_MAX_SHARED_CTXTS) + fdata->subctxt;
  	*ev = 0;
  
++<<<<<<< HEAD
 +	if (--uctxt->cnt) {
 +		uctxt->active_slaves &= ~(1 << fdata->subctxt);
 +		mutex_unlock(&hfi1_mutex);
++=======
+ 	spin_lock_irqsave(&dd->uctxt_lock, flags);
+ 	__clear_bit(fdata->subctxt, uctxt->in_use_ctxts);
+ 	if (!bitmap_empty(uctxt->in_use_ctxts, HFI1_MAX_SHARED_CTXTS)) {
+ 		spin_unlock_irqrestore(&dd->uctxt_lock, flags);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  		goto done;
  	}
+ 	spin_unlock_irqrestore(&dd->uctxt_lock, flags);
  
- 	spin_lock_irqsave(&dd->uctxt_lock, flags);
  	/*
  	 * Disable receive context and interrupt available, reset all
  	 * RcvCtxtCtrl bits to default values.
@@@ -787,34 -802,24 +805,41 @@@
  		     HFI1_RCVCTRL_TAILUPD_DIS |
  		     HFI1_RCVCTRL_ONE_PKT_EGR_DIS |
  		     HFI1_RCVCTRL_NO_RHQ_DROP_DIS |
 -		     HFI1_RCVCTRL_NO_EGR_DROP_DIS, uctxt);
 +		     HFI1_RCVCTRL_NO_EGR_DROP_DIS, uctxt->ctxt);
  	/* Clear the context's J_KEY */
 -	hfi1_clear_ctxt_jkey(dd, uctxt);
 +	hfi1_clear_ctxt_jkey(dd, uctxt->ctxt);
  	/*
 -	 * If a send context is allocated, reset context integrity
 -	 * checks to default and disable the send context.
 +	 * Reset context integrity checks to default.
 +	 * (writes to CSRs probably belong in chip.c)
  	 */
++<<<<<<< HEAD
 +	write_kctxt_csr(dd, uctxt->sc->hw_context, SEND_CTXT_CHECK_ENABLE,
 +			hfi1_pkt_default_send_ctxt_mask(dd, uctxt->sc->type));
 +	sc_disable(uctxt->sc);
 +	spin_unlock_irqrestore(&dd->uctxt_lock, flags);
++=======
+ 	if (uctxt->sc) {
+ 		set_pio_integrity(uctxt->sc);
+ 		sc_disable(uctxt->sc);
+ 	}
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
 -	hfi1_free_ctxt_rcv_groups(uctxt);
 +	dd->rcd[uctxt->ctxt] = NULL;
 +
 +	hfi1_user_exp_rcv_grp_free(uctxt);
  	hfi1_clear_ctxt_pkey(dd, uctxt);
  
 +	uctxt->rcvwait_to = 0;
 +	uctxt->piowait_to = 0;
 +	uctxt->rcvnowait = 0;
 +	uctxt->pionowait = 0;
  	uctxt->event_flags = 0;
  
 -	deallocate_ctxt(uctxt);
 +	hfi1_stats.sps_ctxts--;
 +	if (++dd->freectxts == dd->num_user_contexts)
 +		aspm_enable_all(dd);
 +	mutex_unlock(&hfi1_mutex);
 +	hfi1_free_ctxtdata(dd, uctxt);
  done:
  	mmdrop(fdata->mm);
  	kobject_put(&dd->kobj);
@@@ -842,127 -847,211 +867,303 @@@ static u64 kvirt_to_phys(void *addr
  	return paddr;
  }
  
++<<<<<<< HEAD
 +static int assign_ctxt(struct file *fp, struct hfi1_user_info *uinfo)
 +{
 +	int i_minor, ret = 0;
++=======
+ /**
+  * complete_subctxt
+  * @fd: valid filedata pointer
+  *
+  * Sub-context info can only be set up after the base context
+  * has been completed.  This is indicated by the clearing of the
+  * HFI1_CTXT_BASE_UINIT bit.
+  *
+  * Wait for the bit to be cleared, and then complete the subcontext
+  * initialization.
+  *
+  */
+ static int complete_subctxt(struct hfi1_filedata *fd)
+ {
+ 	int ret;
+ 	unsigned long flags;
+ 
+ 	/*
+ 	 * sub-context info can only be set up after the base context
+ 	 * has been completed.
+ 	 */
+ 	ret = wait_event_interruptible(
+ 		fd->uctxt->wait,
+ 		!test_bit(HFI1_CTXT_BASE_UNINIT, &fd->uctxt->event_flags));
+ 
+ 	if (test_bit(HFI1_CTXT_BASE_FAILED, &fd->uctxt->event_flags))
+ 		ret = -ENOMEM;
+ 
+ 	/* Finish the sub-context init */
+ 	if (!ret) {
+ 		fd->rec_cpu_num = hfi1_get_proc_affinity(fd->uctxt->numa_id);
+ 		ret = init_user_ctxt(fd, fd->uctxt);
+ 	}
+ 
+ 	if (ret) {
+ 		hfi1_rcd_put(fd->uctxt);
+ 		fd->uctxt = NULL;
+ 		spin_lock_irqsave(&fd->dd->uctxt_lock, flags);
+ 		__clear_bit(fd->subctxt, fd->uctxt->in_use_ctxts);
+ 		spin_unlock_irqrestore(&fd->dd->uctxt_lock, flags);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int assign_ctxt(struct hfi1_filedata *fd, struct hfi1_user_info *uinfo)
+ {
+ 	int ret;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	unsigned int swmajor, swminor;
 -	struct hfi1_ctxtdata *uctxt = NULL;
  
  	swmajor = uinfo->userversion >> 16;
 -	if (swmajor != HFI1_USER_SWMAJOR)
 -		return -ENODEV;
 -
 -	if (uinfo->subctxt_cnt > HFI1_MAX_SHARED_CTXTS)
 -		return -EINVAL;
 +	if (swmajor != HFI1_USER_SWMAJOR) {
 +		ret = -ENODEV;
 +		goto done;
 +	}
  
  	swminor = uinfo->userversion & 0xffff;
  
 -	/*
 -	 * Acquire the mutex to protect against multiple creations of what
 -	 * could be a shared base context.
 -	 */
  	mutex_lock(&hfi1_mutex);
 -	/*
 -	 * Get a sub context if available  (fd->uctxt will be set).
 -	 * ret < 0 error, 0 no context, 1 sub-context found
 -	 */
 -	ret = find_sub_ctxt(fd, uinfo);
 -
 +	/* First, lets check if we need to setup a shared context? */
 +	if (uinfo->subctxt_cnt) {
 +		struct hfi1_filedata *fd = fp->private_data;
 +
++<<<<<<< HEAD
 +		ret = find_shared_ctxt(fp, uinfo);
 +		if (ret < 0)
 +			goto done_unlock;
 +		if (ret) {
 +			fd->rec_cpu_num =
 +				hfi1_get_proc_affinity(fd->uctxt->numa_id);
++=======
+ 	/*
+ 	 * Allocate a base context if context sharing is not required or a
+ 	 * sub context wasn't found.
+ 	 */
+ 	if (!ret)
+ 		ret = allocate_ctxt(fd, fd->dd, uinfo, &uctxt);
+ 
+ 	mutex_unlock(&hfi1_mutex);
+ 
+ 	/* Depending on the context type, finish the appropriate init */
+ 	switch (ret) {
+ 	case 0:
+ 		ret = setup_base_ctxt(fd, uctxt);
+ 		if (uctxt->subctxt_cnt) {
+ 			/*
+ 			 * Base context is done (successfully or not), notify
+ 			 * anybody using a sub-context that is waiting for
+ 			 * this completion.
+ 			 */
+ 			clear_bit(HFI1_CTXT_BASE_UNINIT, &uctxt->event_flags);
+ 			wake_up(&uctxt->wait);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  		}
 -		break;
 -	case 1:
 -		ret = complete_subctxt(fd);
 -		break;
 -	default:
 -		break;
  	}
  
 +	/*
 +	 * We execute the following block if we couldn't find a
 +	 * shared context or if context sharing is not required.
 +	 */
 +	if (!ret) {
 +		i_minor = iminor(file_inode(fp)) - HFI1_USER_MINOR_BASE;
 +		ret = get_user_context(fp, uinfo, i_minor);
 +	}
 +done_unlock:
 +	mutex_unlock(&hfi1_mutex);
 +done:
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int get_user_context(struct file *fp, struct hfi1_user_info *uinfo,
 +			    int devno)
 +{
 +	struct hfi1_devdata *dd = NULL;
 +	int devmax, npresent, nup;
++=======
+ /**
+  * match_ctxt
+  * @fd: valid filedata pointer
+  * @uinfo: user info to compare base context with
+  * @uctxt: context to compare uinfo to.
+  *
+  * Compare the given context with the given information to see if it
+  * can be used for a sub context.
+  */
+ static int match_ctxt(struct hfi1_filedata *fd,
+ 		      const struct hfi1_user_info *uinfo,
+ 		      struct hfi1_ctxtdata *uctxt)
+ {
+ 	struct hfi1_devdata *dd = fd->dd;
+ 	unsigned long flags;
+ 	u16 subctxt;
+ 
+ 	/* Skip dynamically allocated kernel contexts */
+ 	if (uctxt->sc && (uctxt->sc->type == SC_KERNEL))
+ 		return 0;
+ 
+ 	/* Skip ctxt if it doesn't match the requested one */
+ 	if (memcmp(uctxt->uuid, uinfo->uuid, sizeof(uctxt->uuid)) ||
+ 	    uctxt->jkey != generate_jkey(current_uid()) ||
+ 	    uctxt->subctxt_id != uinfo->subctxt_id ||
+ 	    uctxt->subctxt_cnt != uinfo->subctxt_cnt)
+ 		return 0;
+ 
+ 	/* Verify the sharing process matches the base */
+ 	if (uctxt->userversion != uinfo->userversion)
+ 		return -EINVAL;
+ 
+ 	/* Find an unused sub context */
+ 	spin_lock_irqsave(&dd->uctxt_lock, flags);
+ 	if (bitmap_empty(uctxt->in_use_ctxts, HFI1_MAX_SHARED_CTXTS)) {
+ 		/* context is being closed, do not use */
+ 		spin_unlock_irqrestore(&dd->uctxt_lock, flags);
+ 		return 0;
+ 	}
+ 
+ 	subctxt = find_first_zero_bit(uctxt->in_use_ctxts,
+ 				      HFI1_MAX_SHARED_CTXTS);
+ 	if (subctxt >= uctxt->subctxt_cnt) {
+ 		spin_unlock_irqrestore(&dd->uctxt_lock, flags);
+ 		return -EBUSY;
+ 	}
+ 
+ 	fd->subctxt = subctxt;
+ 	__set_bit(fd->subctxt, uctxt->in_use_ctxts);
+ 	spin_unlock_irqrestore(&dd->uctxt_lock, flags);
+ 
+ 	fd->uctxt = uctxt;
+ 	hfi1_rcd_get(uctxt);
+ 
+ 	return 1;
+ }
+ 
+ /**
+  * find_sub_ctxt
+  * @fd: valid filedata pointer
+  * @uinfo: matching info to use to find a possible context to share.
+  *
+  * The hfi1_mutex must be held when this function is called.  It is
+  * necessary to ensure serialized creation of shared contexts.
+  *
+  * Return:
+  *    0      No sub-context found
+  *    1      Subcontext found and allocated
+  *    errno  EINVAL (incorrect parameters)
+  *           EBUSY (all sub contexts in use)
+  */
+ static int find_sub_ctxt(struct hfi1_filedata *fd,
+ 			 const struct hfi1_user_info *uinfo)
+ {
+ 	struct hfi1_ctxtdata *uctxt;
+ 	struct hfi1_devdata *dd = fd->dd;
+ 	u16 i;
+ 	int ret;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
 -	if (!uinfo->subctxt_cnt)
 -		return 0;
 +	devmax = hfi1_count_units(&npresent, &nup);
 +	if (!npresent)
 +		return -ENXIO;
 +
++<<<<<<< HEAD
 +	if (!nup)
 +		return -ENETDOWN;
  
 +	dd = hfi1_lookup(devno);
 +	if (!dd)
 +		return -ENODEV;
 +	else if (!dd->freectxts)
 +		return -EBUSY;
 +
 +	return allocate_ctxt(fp, dd, uinfo);
 +}
 +
 +static int find_shared_ctxt(struct file *fp,
 +			    const struct hfi1_user_info *uinfo)
++=======
+ 	for (i = dd->first_dyn_alloc_ctxt; i < dd->num_rcv_contexts; i++) {
+ 		uctxt = hfi1_rcd_get_by_index(dd, i);
+ 		if (uctxt) {
+ 			ret = match_ctxt(fd, uinfo, uctxt);
+ 			hfi1_rcd_put(uctxt);
+ 			/* value of != 0 will return */
+ 			if (ret)
+ 				return ret;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int allocate_ctxt(struct hfi1_filedata *fd, struct hfi1_devdata *dd,
+ 			 struct hfi1_user_info *uinfo,
+ 			 struct hfi1_ctxtdata **rcd)
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  {
 +	int devmax, ndev, i;
 +	int ret = 0;
 +	struct hfi1_filedata *fd = fp->private_data;
 +
 +	devmax = hfi1_count_units(NULL, NULL);
 +
 +	for (ndev = 0; ndev < devmax; ndev++) {
 +		struct hfi1_devdata *dd = hfi1_lookup(ndev);
 +
 +		if (!(dd && (dd->flags & HFI1_PRESENT) && dd->kregbase))
 +			continue;
 +		for (i = dd->first_dyn_alloc_ctxt;
 +		     i < dd->num_rcv_contexts; i++) {
 +			struct hfi1_ctxtdata *uctxt = dd->rcd[i];
 +
 +			/* Skip ctxts which are not yet open */
 +			if (!uctxt || !uctxt->cnt)
 +				continue;
 +
 +			/* Skip dynamically allocted kernel contexts */
 +			if (uctxt->sc && (uctxt->sc->type == SC_KERNEL))
 +				continue;
 +
 +			/* Skip ctxt if it doesn't match the requested one */
 +			if (memcmp(uctxt->uuid, uinfo->uuid,
 +				   sizeof(uctxt->uuid)) ||
 +			    uctxt->jkey != generate_jkey(current_uid()) ||
 +			    uctxt->subctxt_id != uinfo->subctxt_id ||
 +			    uctxt->subctxt_cnt != uinfo->subctxt_cnt)
 +				continue;
 +
 +			/* Verify the sharing process matches the master */
 +			if (uctxt->userversion != uinfo->userversion ||
 +			    uctxt->cnt >= uctxt->subctxt_cnt) {
 +				ret = -EINVAL;
 +				goto done;
 +			}
 +			fd->uctxt = uctxt;
 +			fd->subctxt  = uctxt->cnt++;
 +			uctxt->active_slaves |= 1 << fd->subctxt;
 +			ret = 1;
 +			goto done;
 +		}
 +	}
 +
 +done:
 +	return ret;
 +}
 +
 +static int allocate_ctxt(struct file *fp, struct hfi1_devdata *dd,
 +			 struct hfi1_user_info *uinfo)
 +{
 +	struct hfi1_filedata *fd = fp->private_data;
  	struct hfi1_ctxtdata *uctxt;
 +	unsigned ctxt;
  	int ret, numa;
  
  	if (dd->flags & HFI1_FROZEN) {
@@@ -1048,32 -1121,33 +1249,46 @@@
  	 */
  	if (dd->freectxts-- == dd->num_user_contexts)
  		aspm_disable_all(dd);
++<<<<<<< HEAD
 +	fd->uctxt = uctxt;
++=======
+ 
+ 	*rcd = uctxt;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  	return 0;
  
  ctxdata_free:
++<<<<<<< HEAD
 +	dd->rcd[ctxt] = NULL;
 +	hfi1_free_ctxtdata(dd, uctxt);
++=======
+ 	hfi1_free_ctxt(uctxt);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	return ret;
  }
  
 -static void deallocate_ctxt(struct hfi1_ctxtdata *uctxt)
 +static int init_subctxts(struct hfi1_ctxtdata *uctxt,
 +			 const struct hfi1_user_info *uinfo)
  {
 -	mutex_lock(&hfi1_mutex);
 -	hfi1_stats.sps_ctxts--;
 -	if (++uctxt->dd->freectxts == uctxt->dd->num_user_contexts)
 -		aspm_enable_all(uctxt->dd);
 -	mutex_unlock(&hfi1_mutex);
 +	unsigned num_subctxts;
  
++<<<<<<< HEAD
 +	num_subctxts = uinfo->subctxt_cnt;
 +	if (num_subctxts > HFI1_MAX_SHARED_CTXTS)
 +		return -EINVAL;
++=======
+ 	hfi1_free_ctxt(uctxt);
+ }
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
 -static void init_subctxts(struct hfi1_ctxtdata *uctxt,
 -			  const struct hfi1_user_info *uinfo)
 -{
  	uctxt->subctxt_cnt = uinfo->subctxt_cnt;
  	uctxt->subctxt_id = uinfo->subctxt_id;
 -	set_bit(HFI1_CTXT_BASE_UNINIT, &uctxt->event_flags);
 +	uctxt->active_slaves = 1;
 +	uctxt->redirect_seq_cnt = 1;
 +	set_bit(HFI1_CTXT_MASTER_UNINIT, &uctxt->event_flags);
 +
 +	return 0;
  }
  
  static int setup_subctxt(struct hfi1_ctxtdata *uctxt)
@@@ -1224,54 -1301,44 +1439,84 @@@ static int setup_ctxt(struct file *fp
  	struct hfi1_devdata *dd = uctxt->dd;
  	int ret = 0;
  
 -	hfi1_init_ctxt(uctxt->sc);
 +	/*
 +	 * Context should be set up only once, including allocation and
 +	 * programming of eager buffers. This is done if context sharing
 +	 * is not requested or by the master process.
 +	 */
 +	if (!uctxt->subctxt_cnt || !fd->subctxt) {
 +		ret = hfi1_init_ctxt(uctxt->sc);
 +		if (ret)
 +			goto done;
  
 -	/* Now allocate the RcvHdr queue and eager buffers. */
 -	ret = hfi1_create_rcvhdrq(dd, uctxt);
 -	if (ret)
 -		return ret;
 +		/* Now allocate the RcvHdr queue and eager buffers. */
 +		ret = hfi1_create_rcvhdrq(dd, uctxt);
 +		if (ret)
 +			goto done;
 +		ret = hfi1_setup_eagerbufs(uctxt);
 +		if (ret)
 +			goto done;
 +		if (uctxt->subctxt_cnt && !fd->subctxt) {
 +			ret = setup_subctxt(uctxt);
 +			if (ret)
 +				goto done;
 +		}
 +	} else {
 +		ret = wait_event_interruptible(uctxt->wait, !test_bit(
 +					       HFI1_CTXT_MASTER_UNINIT,
 +					       &uctxt->event_flags));
 +		if (ret)
 +			goto done;
 +	}
  
 -	ret = hfi1_setup_eagerbufs(uctxt);
 +	ret = hfi1_user_sdma_alloc_queues(uctxt, fp);
  	if (ret)
 -		goto setup_failed;
 +		goto done;
 +	/*
 +	 * Expected receive has to be setup for all processes (including
 +	 * shared contexts). However, it has to be done after the master
 +	 * context has been fully configured as it depends on the
 +	 * eager/expected split of the RcvArray entries.
 +	 * Setting it up here ensures that the subcontexts will be waiting
 +	 * (due to the above wait_event_interruptible() until the master
 +	 * is setup.
 +	 */
 +	ret = hfi1_user_exp_rcv_init(fp);
 +	if (ret)
 +		goto done;
  
++<<<<<<< HEAD
 +	set_bit(HFI1_CTXT_SETUP_DONE, &uctxt->event_flags);
 +done:
++=======
+ 	/* If sub-contexts are enabled, do the appropriate setup */
+ 	if (uctxt->subctxt_cnt)
+ 		ret = setup_subctxt(uctxt);
+ 	if (ret)
+ 		goto setup_failed;
+ 
+ 	ret = hfi1_alloc_ctxt_rcv_groups(uctxt);
+ 	if (ret)
+ 		goto setup_failed;
+ 
+ 	ret = init_user_ctxt(fd, uctxt);
+ 	if (ret)
+ 		goto setup_failed;
+ 
+ 	user_init(uctxt);
+ 
+ 	/* Now that the context is set up, the fd can get a reference. */
+ 	fd->uctxt = uctxt;
+ 	hfi1_rcd_get(uctxt);
+ 
+ 	return 0;
+ 
+ setup_failed:
+ 	/* Set the failed bit so sub-context init can do the right thing */
+ 	set_bit(HFI1_CTXT_BASE_FAILED, &uctxt->event_flags);
+ 	deallocate_ctxt(uctxt);
+ 
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	return ret;
  }
  
@@@ -1407,19 -1474,14 +1652,20 @@@ int hfi1_set_uevent_bits(struct hfi1_pp
  {
  	struct hfi1_ctxtdata *uctxt;
  	struct hfi1_devdata *dd = ppd->dd;
++<<<<<<< HEAD
 +	unsigned ctxt;
 +	int ret = 0;
 +	unsigned long flags;
++=======
+ 	u16 ctxt;
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
- 	if (!dd->events) {
- 		ret = -EINVAL;
- 		goto done;
- 	}
+ 	if (!dd->events)
+ 		return -EINVAL;
  
- 	spin_lock_irqsave(&dd->uctxt_lock, flags);
  	for (ctxt = dd->first_dyn_alloc_ctxt; ctxt < dd->num_rcv_contexts;
  	     ctxt++) {
- 		uctxt = dd->rcd[ctxt];
+ 		uctxt = hfi1_rcd_get_by_index(dd, ctxt);
  		if (uctxt) {
  			unsigned long *evs = dd->events +
  				(uctxt->ctxt - dd->first_dyn_alloc_ctxt) *
diff --cc drivers/infiniband/hw/hfi1/hfi.h
index 9719cf207532,fa9160f68bb7..000000000000
--- a/drivers/infiniband/hw/hfi1/hfi.h
+++ b/drivers/infiniband/hw/hfi1/hfi.h
@@@ -1283,17 -1259,21 +1282,35 @@@ void handle_linkup_change(struct hfi1_d
  
  void handle_user_interrupt(struct hfi1_ctxtdata *rcd);
  
++<<<<<<< HEAD
 +int hfi1_create_rcvhdrq(struct hfi1_devdata *, struct hfi1_ctxtdata *);
 +int hfi1_setup_eagerbufs(struct hfi1_ctxtdata *);
 +int hfi1_create_ctxts(struct hfi1_devdata *dd);
 +struct hfi1_ctxtdata *hfi1_create_ctxtdata(struct hfi1_pportdata *, u32, int);
 +void hfi1_init_pportdata(struct pci_dev *, struct hfi1_pportdata *,
 +			 struct hfi1_devdata *, u8, u8);
 +void hfi1_free_ctxtdata(struct hfi1_devdata *, struct hfi1_ctxtdata *);
 +
 +int handle_receive_interrupt(struct hfi1_ctxtdata *, int);
 +int handle_receive_interrupt_nodma_rtail(struct hfi1_ctxtdata *, int);
 +int handle_receive_interrupt_dma_rtail(struct hfi1_ctxtdata *, int);
++=======
+ int hfi1_create_rcvhdrq(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd);
+ int hfi1_setup_eagerbufs(struct hfi1_ctxtdata *rcd);
+ int hfi1_create_kctxts(struct hfi1_devdata *dd);
+ int hfi1_create_ctxtdata(struct hfi1_pportdata *ppd, int numa,
+ 			 struct hfi1_ctxtdata **rcd);
+ void hfi1_free_ctxt(struct hfi1_ctxtdata *rcd);
+ void hfi1_init_pportdata(struct pci_dev *pdev, struct hfi1_pportdata *ppd,
+ 			 struct hfi1_devdata *dd, u8 hw_pidx, u8 port);
+ void hfi1_free_ctxtdata(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd);
+ int hfi1_rcd_put(struct hfi1_ctxtdata *rcd);
+ void hfi1_rcd_get(struct hfi1_ctxtdata *rcd);
+ struct hfi1_ctxtdata *hfi1_rcd_get_by_index(struct hfi1_devdata *dd, u16 ctxt);
+ int handle_receive_interrupt(struct hfi1_ctxtdata *rcd, int thread);
+ int handle_receive_interrupt_nodma_rtail(struct hfi1_ctxtdata *rcd, int thread);
+ int handle_receive_interrupt_dma_rtail(struct hfi1_ctxtdata *rcd, int thread);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  void set_all_slowpath(struct hfi1_devdata *dd);
  void hfi1_vnic_synchronize_irq(struct hfi1_devdata *dd);
  void hfi1_set_vnic_msix_info(struct hfi1_ctxtdata *rcd);
diff --cc drivers/infiniband/hw/hfi1/init.c
index 3bed53851d35,fba77001c3a7..000000000000
--- a/drivers/infiniband/hw/hfi1/init.c
+++ b/drivers/infiniband/hw/hfi1/init.c
@@@ -138,77 -138,186 +138,205 @@@ int hfi1_create_ctxts(struct hfi1_devda
  	dd->rcd = kzalloc_node(dd->num_rcv_contexts * sizeof(*dd->rcd),
  			       GFP_KERNEL, dd->node);
  	if (!dd->rcd)
 -		return -ENOMEM;
 +		goto nomem;
  
 +	/* create one or more kernel contexts */
  	for (i = 0; i < dd->first_dyn_alloc_ctxt; ++i) {
 -		ret = hfi1_create_kctxt(dd, dd->pport);
 -		if (ret)
 +		struct hfi1_pportdata *ppd;
 +		struct hfi1_ctxtdata *rcd;
 +
 +		ppd = dd->pport + (i % dd->num_pports);
 +
 +		/* dd->rcd[i] gets assigned inside the callee */
 +		rcd = hfi1_create_ctxtdata(ppd, i, dd->node);
 +		if (!rcd) {
 +			dd_dev_err(dd,
 +				   "Unable to allocate kernel receive context, failing\n");
 +			goto nomem;
 +		}
 +		/*
 +		 * Set up the kernel context flags here and now because they
 +		 * use default values for all receive side memories.  User
 +		 * contexts will be handled as they are created.
 +		 */
 +		rcd->flags = HFI1_CAP_KGET(MULTI_PKT_EGR) |
 +			HFI1_CAP_KGET(NODROP_RHQ_FULL) |
 +			HFI1_CAP_KGET(NODROP_EGR_FULL) |
 +			HFI1_CAP_KGET(DMA_RTAIL);
 +
 +		/* Control context must use DMA_RTAIL */
 +		if (rcd->ctxt == HFI1_CTRL_CTXT)
 +			rcd->flags |= HFI1_CAP_DMA_RTAIL;
 +		rcd->seq_cnt = 1;
 +
 +		rcd->sc = sc_alloc(dd, SC_ACK, rcd->rcvhdrqentsize, dd->node);
 +		if (!rcd->sc) {
 +			dd_dev_err(dd,
 +				   "Unable to allocate kernel send context, failing\n");
 +			goto nomem;
 +		}
 +
 +		ret = hfi1_init_ctxt(rcd->sc);
 +		if (ret < 0) {
 +			dd_dev_err(dd,
 +				   "Failed to setup kernel receive context, failing\n");
 +			ret = -EFAULT;
  			goto bail;
 +		}
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Initialize aspm, to be done after gen3 transition and setting up
 +	 * contexts and before enabling interrupts
 +	 */
 +	aspm_init(dd);
++=======
+ 	return 0;
+ bail:
+ 	for (i = 0; dd->rcd && i < dd->first_dyn_alloc_ctxt; ++i)
+ 		hfi1_free_ctxt(dd->rcd[i]);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
 -	/* All the contexts should be freed, free the array */
 +	return 0;
 +nomem:
 +	ret = -ENOMEM;
 +bail:
 +	if (dd->rcd) {
 +		for (i = 0; i < dd->num_rcv_contexts; ++i)
 +			hfi1_free_ctxtdata(dd, dd->rcd[i]);
 +	}
  	kfree(dd->rcd);
  	dd->rcd = NULL;
  	return ret;
  }
  
  /*
++<<<<<<< HEAD
 + * Common code for user and kernel context setup.
++=======
+  * Helper routines for the receive context reference count (rcd and uctxt).
+  */
+ static void hfi1_rcd_init(struct hfi1_ctxtdata *rcd)
+ {
+ 	kref_init(&rcd->kref);
+ }
+ 
+ /**
+  * hfi1_rcd_free - When reference is zero clean up.
+  * @kref: pointer to an initialized rcd data structure
+  *
+  */
+ static void hfi1_rcd_free(struct kref *kref)
+ {
+ 	unsigned long flags;
+ 	struct hfi1_ctxtdata *rcd =
+ 		container_of(kref, struct hfi1_ctxtdata, kref);
+ 
+ 	hfi1_free_ctxtdata(rcd->dd, rcd);
+ 
+ 	spin_lock_irqsave(&rcd->dd->uctxt_lock, flags);
+ 	rcd->dd->rcd[rcd->ctxt] = NULL;
+ 	spin_unlock_irqrestore(&rcd->dd->uctxt_lock, flags);
+ 
+ 	kfree(rcd);
+ }
+ 
+ /**
+  * hfi1_rcd_put - decrement reference for rcd
+  * @rcd: pointer to an initialized rcd data structure
+  *
+  * Use this to put a reference after the init.
+  */
+ int hfi1_rcd_put(struct hfi1_ctxtdata *rcd)
+ {
+ 	if (rcd)
+ 		return kref_put(&rcd->kref, hfi1_rcd_free);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * hfi1_rcd_get - increment reference for rcd
+  * @rcd: pointer to an initialized rcd data structure
+  *
+  * Use this to get a reference after the init.
+  */
+ void hfi1_rcd_get(struct hfi1_ctxtdata *rcd)
+ {
+ 	kref_get(&rcd->kref);
+ }
+ 
+ /**
+  * allocate_rcd_index - allocate an rcd index from the rcd array
+  * @dd: pointer to a valid devdata structure
+  * @rcd: rcd data structure to assign
+  * @index: pointer to index that is allocated
+  *
+  * Find an empty index in the rcd array, and assign the given rcd to it.
+  * If the array is full, we are EBUSY.
+  *
+  */
+ static int allocate_rcd_index(struct hfi1_devdata *dd,
+ 			      struct hfi1_ctxtdata *rcd, u16 *index)
+ {
+ 	unsigned long flags;
+ 	u16 ctxt;
+ 
+ 	spin_lock_irqsave(&dd->uctxt_lock, flags);
+ 	for (ctxt = 0; ctxt < dd->num_rcv_contexts; ctxt++)
+ 		if (!dd->rcd[ctxt])
+ 			break;
+ 
+ 	if (ctxt < dd->num_rcv_contexts) {
+ 		rcd->ctxt = ctxt;
+ 		dd->rcd[ctxt] = rcd;
+ 		hfi1_rcd_init(rcd);
+ 	}
+ 	spin_unlock_irqrestore(&dd->uctxt_lock, flags);
+ 
+ 	if (ctxt >= dd->num_rcv_contexts)
+ 		return -EBUSY;
+ 
+ 	*index = ctxt;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * hfi1_rcd_get_by_index
+  * @dd: pointer to a valid devdata structure
+  * @ctxt: the index of an possilbe rcd
+  *
+  * We need to protect access to the rcd array.  If access is needed to
+  * one or more index, get the protecting spinlock and then increment the
+  * kref.
+  *
+  * The caller is responsible for making the _put().
+  *
+  */
+ struct hfi1_ctxtdata *hfi1_rcd_get_by_index(struct hfi1_devdata *dd, u16 ctxt)
+ {
+ 	unsigned long flags;
+ 	struct hfi1_ctxtdata *rcd = NULL;
+ 
+ 	spin_lock_irqsave(&dd->uctxt_lock, flags);
+ 	if (dd->rcd[ctxt]) {
+ 		rcd = dd->rcd[ctxt];
+ 		hfi1_rcd_get(rcd);
+ 	}
+ 	spin_unlock_irqrestore(&dd->uctxt_lock, flags);
+ 
+ 	return rcd;
+ }
+ 
+ /*
+  * Common code for user and kernel context create and setup.
+  * NOTE: the initial kref is done here (hf1_rcd_init()).
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
   */
 -int hfi1_create_ctxtdata(struct hfi1_pportdata *ppd, int numa,
 -			 struct hfi1_ctxtdata **context)
 +struct hfi1_ctxtdata *hfi1_create_ctxtdata(struct hfi1_pportdata *ppd, u32 ctxt,
 +					   int numa)
  {
  	struct hfi1_devdata *dd = ppd->dd;
  	struct hfi1_ctxtdata *rcd;
@@@ -222,15 -331,23 +350,24 @@@
  	rcd = kzalloc_node(sizeof(*rcd), GFP_KERNEL, numa);
  	if (rcd) {
  		u32 rcvtids, max_entries;
 -		u16 ctxt;
 -		int ret;
  
++<<<<<<< HEAD
 +		hfi1_cdbg(PROC, "setting up context %u\n", ctxt);
++=======
+ 		ret = allocate_rcd_index(dd, rcd, &ctxt);
+ 		if (ret) {
+ 			*context = NULL;
+ 			kfree(rcd);
+ 			return ret;
+ 		}
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  
  		INIT_LIST_HEAD(&rcd->qp_wait_list);
 -		hfi1_exp_tid_group_init(&rcd->tid_group_list);
 -		hfi1_exp_tid_group_init(&rcd->tid_used_list);
 -		hfi1_exp_tid_group_init(&rcd->tid_full_list);
  		rcd->ppd = ppd;
  		rcd->dd = dd;
 -		__set_bit(0, rcd->in_use_ctxts);
 +		rcd->cnt = 1;
 +		rcd->ctxt = ctxt;
 +		dd->rcd[ctxt] = rcd;
  		rcd->numa_id = numa;
  		rcd->rcv_array_groups = dd->rcv_entries.ngroups;
  
@@@ -333,14 -452,30 +472,35 @@@
  			if (!rcd->opstats)
  				goto bail;
  		}
 -
 -		*context = rcd;
 -		return 0;
  	}
 -
 +	return rcd;
  bail:
++<<<<<<< HEAD
 +	dd->rcd[ctxt] = NULL;
 +	kfree(rcd->egrbufs.rcvtids);
 +	kfree(rcd->egrbufs.buffers);
 +	kfree(rcd);
 +	return NULL;
++=======
+ 	*context = NULL;
+ 	hfi1_free_ctxt(rcd);
+ 	return -ENOMEM;
+ }
+ 
+ /**
+  * hfi1_free_ctxt
+  * @rcd: pointer to an initialized rcd data structure
+  *
+  * This wrapper is the free function that matches hfi1_create_ctxtdata().
+  * When a context is done being used (kernel or user), this function is called
+  * for the "final" put to match the kref init from hf1i_create_ctxtdata().
+  * Other users of the context do a get/put sequence to make sure that the
+  * structure isn't removed while in use.
+  */
+ void hfi1_free_ctxt(struct hfi1_ctxtdata *rcd)
+ {
+ 	hfi1_rcd_put(rcd);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  }
  
  /*
@@@ -569,10 -704,13 +729,18 @@@ static int init_after_reset(struct hfi1
  	 * pioavail updates while we re-initialize.  This is mostly
  	 * for the driver data structures, not chip registers.
  	 */
- 	for (i = 0; i < dd->num_rcv_contexts; i++)
+ 	for (i = 0; i < dd->num_rcv_contexts; i++) {
+ 		rcd = hfi1_rcd_get_by_index(dd, i);
  		hfi1_rcvctrl(dd, HFI1_RCVCTRL_CTXT_DIS |
++<<<<<<< HEAD
 +				  HFI1_RCVCTRL_INTRAVAIL_DIS |
 +				  HFI1_RCVCTRL_TAILUPD_DIS, i);
++=======
+ 			     HFI1_RCVCTRL_INTRAVAIL_DIS |
+ 			     HFI1_RCVCTRL_TAILUPD_DIS, rcd);
+ 		hfi1_rcd_put(rcd);
+ 	}
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	pio_send_control(dd, PSC_GLOBAL_DISABLE);
  	for (i = 0; i < dd->num_send_contexts; i++)
  		sc_disable(dd->send_contexts[i].sc);
@@@ -582,8 -720,9 +750,9 @@@
  
  static void enable_chip(struct hfi1_devdata *dd)
  {
+ 	struct hfi1_ctxtdata *rcd;
  	u32 rcvmask;
 -	u16 i;
 +	u32 i;
  
  	/* enable PIO send */
  	pio_send_control(dd, PSC_GLOBAL_ENABLE);
@@@ -593,17 -732,21 +762,26 @@@
  	 * Other ctxts done as user opens and initializes them.
  	 */
  	for (i = 0; i < dd->first_dyn_alloc_ctxt; ++i) {
+ 		rcd = hfi1_rcd_get_by_index(dd, i);
+ 		if (!rcd)
+ 			continue;
  		rcvmask = HFI1_RCVCTRL_CTXT_ENB | HFI1_RCVCTRL_INTRAVAIL_ENB;
- 		rcvmask |= HFI1_CAP_KGET_MASK(dd->rcd[i]->flags, DMA_RTAIL) ?
+ 		rcvmask |= HFI1_CAP_KGET_MASK(rcd->flags, DMA_RTAIL) ?
  			HFI1_RCVCTRL_TAILUPD_ENB : HFI1_RCVCTRL_TAILUPD_DIS;
- 		if (!HFI1_CAP_KGET_MASK(dd->rcd[i]->flags, MULTI_PKT_EGR))
+ 		if (!HFI1_CAP_KGET_MASK(rcd->flags, MULTI_PKT_EGR))
  			rcvmask |= HFI1_RCVCTRL_ONE_PKT_EGR_ENB;
- 		if (HFI1_CAP_KGET_MASK(dd->rcd[i]->flags, NODROP_RHQ_FULL))
+ 		if (HFI1_CAP_KGET_MASK(rcd->flags, NODROP_RHQ_FULL))
  			rcvmask |= HFI1_RCVCTRL_NO_RHQ_DROP_ENB;
- 		if (HFI1_CAP_KGET_MASK(dd->rcd[i]->flags, NODROP_EGR_FULL))
+ 		if (HFI1_CAP_KGET_MASK(rcd->flags, NODROP_EGR_FULL))
  			rcvmask |= HFI1_RCVCTRL_NO_EGR_DROP_ENB;
++<<<<<<< HEAD
 +		hfi1_rcvctrl(dd, rcvmask, i);
 +		sc_enable(dd->rcd[i]->sc);
++=======
+ 		hfi1_rcvctrl(dd, rcvmask, rcd);
+ 		sc_enable(rcd->sc);
+ 		hfi1_rcd_put(rcd);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	}
  }
  
@@@ -898,12 -1044,15 +1078,22 @@@ static void shutdown_device(struct hfi1
  
  	for (pidx = 0; pidx < dd->num_pports; ++pidx) {
  		ppd = dd->pport + pidx;
- 		for (i = 0; i < dd->num_rcv_contexts; i++)
+ 		for (i = 0; i < dd->num_rcv_contexts; i++) {
+ 			rcd = hfi1_rcd_get_by_index(dd, i);
  			hfi1_rcvctrl(dd, HFI1_RCVCTRL_TAILUPD_DIS |
++<<<<<<< HEAD
 +					  HFI1_RCVCTRL_CTXT_DIS |
 +					  HFI1_RCVCTRL_INTRAVAIL_DIS |
 +					  HFI1_RCVCTRL_PKEY_DIS |
 +					  HFI1_RCVCTRL_ONE_PKT_EGR_DIS, i);
++=======
+ 				     HFI1_RCVCTRL_CTXT_DIS |
+ 				     HFI1_RCVCTRL_INTRAVAIL_DIS |
+ 				     HFI1_RCVCTRL_PKEY_DIS |
+ 				     HFI1_RCVCTRL_ONE_PKT_EGR_DIS, rcd);
+ 			hfi1_rcd_put(rcd);
+ 		}
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  		/*
  		 * Gracefully stop all sends allowing any in progress to
  		 * trickle out first.
@@@ -1383,16 -1523,22 +1559,26 @@@ static void cleanup_device_data(struct 
  		dd->rcvhdrtail_dummy_kvaddr = NULL;
  	}
  
- 	for (ctxt = 0; tmp && ctxt < dd->num_rcv_contexts; ctxt++) {
- 		struct hfi1_ctxtdata *rcd = tmp[ctxt];
+ 	/*
+ 	 * Free any resources still in use (usually just kernel contexts)
+ 	 * at unload; we do for ctxtcnt, because that's what we allocate.
+ 	 */
+ 	for (ctxt = 0; dd->rcd && ctxt < dd->num_rcv_contexts; ctxt++) {
+ 		struct hfi1_ctxtdata *rcd = dd->rcd[ctxt];
  
- 		tmp[ctxt] = NULL; /* debugging paranoia */
  		if (rcd) {
  			hfi1_clear_tids(rcd);
++<<<<<<< HEAD
 +			hfi1_free_ctxtdata(dd, rcd);
++=======
+ 			hfi1_free_ctxt(rcd);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  		}
  	}
- 	kfree(tmp);
+ 
+ 	kfree(dd->rcd);
+ 	dd->rcd = NULL;
+ 
  	free_pio_map(dd);
  	/* must follow rcv context free - need to remove rcv's hooks */
  	for (ctxt = 0; ctxt < dd->num_send_contexts; ctxt++)
diff --cc drivers/infiniband/hw/hfi1/trace_rx.h
index 84929578cfe6,f9909d240dcc..000000000000
--- a/drivers/infiniband/hw/hfi1/trace_rx.h
+++ b/drivers/infiniband/hw/hfi1/trace_rx.h
@@@ -114,8 -114,8 +114,13 @@@ TRACE_EVENT(hfi1_rcvhdr
  );
  
  TRACE_EVENT(hfi1_receive_interrupt,
++<<<<<<< HEAD
 +	    TP_PROTO(struct hfi1_devdata *dd, u32 ctxt),
 +	    TP_ARGS(dd, ctxt),
++=======
+ 	    TP_PROTO(struct hfi1_devdata *dd, struct hfi1_ctxtdata *rcd),
+ 	    TP_ARGS(dd, rcd),
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	    TP_STRUCT__entry(DD_DEV_ENTRY(dd)
  			     __field(u32, ctxt)
  			     __field(u8, slow_path)
diff --cc drivers/infiniband/hw/hfi1/vnic_main.c
index b1572c795c35,2917a238a343..000000000000
--- a/drivers/infiniband/hw/hfi1/vnic_main.c
+++ b/drivers/infiniband/hw/hfi1/vnic_main.c
@@@ -159,12 -146,7 +159,16 @@@ static int allocate_vnic_ctxt(struct hf
  
  	return ret;
  bail:
++<<<<<<< HEAD
 +	/*
 +	 * hfi1_free_ctxtdata() also releases send_context
 +	 * structure if uctxt->sc is not null
 +	 */
 +	dd->rcd[uctxt->ctxt] = NULL;
 +	hfi1_free_ctxtdata(dd, uctxt);
++=======
+ 	hfi1_free_ctxt(uctxt);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  	dd_dev_dbg(dd, "vnic allocation failed. rc %d\n", ret);
  	return ret;
  }
@@@ -203,16 -182,15 +204,20 @@@ static void deallocate_vnic_ctxt(struc
  	sc_disable(uctxt->sc);
  
  	dd->send_contexts[uctxt->sc->sw_index].type = SC_USER;
- 	spin_unlock_irqrestore(&dd->uctxt_lock, flags);
  
 +	dd->rcd[uctxt->ctxt] = NULL;
  	uctxt->event_flags = 0;
  
  	hfi1_clear_tids(uctxt);
  	hfi1_clear_ctxt_pkey(dd, uctxt);
  
  	hfi1_stats.sps_ctxts--;
++<<<<<<< HEAD
 +	hfi1_free_ctxtdata(dd, uctxt);
++=======
+ 
+ 	hfi1_free_ctxt(uctxt);
++>>>>>>> d295dbeb2a0c (IB/hf1: User context locking is inconsistent)
  }
  
  void hfi1_vnic_setup(struct hfi1_devdata *dd)
* Unmerged path drivers/infiniband/hw/hfi1/aspm.h
* Unmerged path drivers/infiniband/hw/hfi1/chip.c
diff --git a/drivers/infiniband/hw/hfi1/debugfs.c b/drivers/infiniband/hw/hfi1/debugfs.c
index a261ad44a2b2..62b6a1eab8ec 100644
--- a/drivers/infiniband/hw/hfi1/debugfs.c
+++ b/drivers/infiniband/hw/hfi1/debugfs.c
@@ -221,12 +221,15 @@ static int _opcode_stats_seq_show(struct seq_file *s, void *v)
 	u64 n_packets = 0, n_bytes = 0;
 	struct hfi1_ibdev *ibd = (struct hfi1_ibdev *)s->private;
 	struct hfi1_devdata *dd = dd_from_dev(ibd);
+	struct hfi1_ctxtdata *rcd;
 
 	for (j = 0; j < dd->first_dyn_alloc_ctxt; j++) {
-		if (!dd->rcd[j])
-			continue;
-		n_packets += dd->rcd[j]->opstats->stats[i].n_packets;
-		n_bytes += dd->rcd[j]->opstats->stats[i].n_bytes;
+		rcd = hfi1_rcd_get_by_index(dd, j);
+		if (rcd) {
+			n_packets += rcd->opstats->stats[i].n_packets;
+			n_bytes += rcd->opstats->stats[i].n_bytes;
+		}
+		hfi1_rcd_put(rcd);
 	}
 	if (!n_packets && !n_bytes)
 		return SEQ_SKIP;
@@ -279,6 +282,7 @@ static int _ctx_stats_seq_show(struct seq_file *s, void *v)
 	u64 n_packets = 0;
 	struct hfi1_ibdev *ibd = (struct hfi1_ibdev *)s->private;
 	struct hfi1_devdata *dd = dd_from_dev(ibd);
+	struct hfi1_ctxtdata *rcd;
 
 	if (v == SEQ_START_TOKEN) {
 		seq_puts(s, "Ctx:npkts\n");
@@ -288,11 +292,14 @@ static int _ctx_stats_seq_show(struct seq_file *s, void *v)
 	spos = v;
 	i = *spos;
 
-	if (!dd->rcd[i])
+	rcd = hfi1_rcd_get_by_index(dd, i);
+	if (!rcd)
 		return SEQ_SKIP;
 
-	for (j = 0; j < ARRAY_SIZE(dd->rcd[i]->opstats->stats); j++)
-		n_packets += dd->rcd[i]->opstats->stats[j].n_packets;
+	for (j = 0; j < ARRAY_SIZE(rcd->opstats->stats); j++)
+		n_packets += rcd->opstats->stats[j].n_packets;
+
+	hfi1_rcd_put(rcd);
 
 	if (!n_packets)
 		return SEQ_SKIP;
@@ -1146,12 +1153,15 @@ static int _fault_stats_seq_show(struct seq_file *s, void *v)
 	u64 n_packets = 0, n_bytes = 0;
 	struct hfi1_ibdev *ibd = (struct hfi1_ibdev *)s->private;
 	struct hfi1_devdata *dd = dd_from_dev(ibd);
+	struct hfi1_ctxtdata *rcd;
 
 	for (j = 0; j < dd->first_dyn_alloc_ctxt; j++) {
-		if (!dd->rcd[j])
-			continue;
-		n_packets += dd->rcd[j]->opstats->stats[i].n_packets;
-		n_bytes += dd->rcd[j]->opstats->stats[i].n_bytes;
+		rcd = hfi1_rcd_get_by_index(dd, j);
+		if (rcd) {
+			n_packets += rcd->opstats->stats[i].n_packets;
+			n_bytes += rcd->opstats->stats[i].n_bytes;
+		}
+		hfi1_rcd_put(rcd);
 	}
 	if (!n_packets && !n_bytes)
 		return SEQ_SKIP;
* Unmerged path drivers/infiniband/hw/hfi1/driver.c
* Unmerged path drivers/infiniband/hw/hfi1/file_ops.c
* Unmerged path drivers/infiniband/hw/hfi1/hfi.h
* Unmerged path drivers/infiniband/hw/hfi1/init.c
* Unmerged path drivers/infiniband/hw/hfi1/trace_rx.h
* Unmerged path drivers/infiniband/hw/hfi1/vnic_main.c
