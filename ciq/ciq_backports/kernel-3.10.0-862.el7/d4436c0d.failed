cpufreq: intel_pstate: Fix ratio setting for min_perf_pct

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Fix ratio setting for min_perf_pct (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 91.43%
commit-author Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
commit d4436c0dba8d4d780588179a2e192a867d266a10
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d4436c0d.failed

When the minimum performance limit percentage is set to the power-up
default, it is possible that minimum performance ratio is off by one.

In the set_policy() callback the minimum ratio is calculated by
applying global.min_perf_pct to turbo_ratio and rounding up, but the
power-up default global.min_perf_pct is already rounded up to the
next percent in min_perf_pct_min().  That results in two round up
operations, so for the default min_perf_pct one of them is not
required.

It is better to remove rounding up in min_perf_pct_min() as this
matches the displayed min_perf_pct prior to commit c5a2ee7dde89
(cpufreq: intel_pstate: Active mode P-state limits rework) in 4.12.

For example on a platform with max turbo ratio of 37 and minimum
ratio of 10, the min_perf_pct resulted in 28 with the above commit.
Before this commit it was 27 and it will be the same after this
change.

Fixes: 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
	Reported-by: Artem Bityutskiy <artem.bityutskiy@linux.intel.com>
	Signed-off-by: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit d4436c0dba8d4d780588179a2e192a867d266a10)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index c8ed8841fcf2,2386d7036e90..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -541,40 -566,348 +541,46 @@@ static inline void update_turbo_state(v
  		 cpu->pstate.max_pstate == cpu->pstate.turbo_pstate);
  }
  
 -static int min_perf_pct_min(void)
 +static void intel_pstate_hwp_set(const struct cpumask *cpumask)
  {
 -	struct cpudata *cpu = all_cpu_data[0];
 -	int turbo_pstate = cpu->pstate.turbo_pstate;
 +	int min, hw_min, max, hw_max, cpu, range, adj_range;
 +	u64 value, cap;
  
++<<<<<<< HEAD
 +	for_each_cpu(cpu, cpumask) {
 +		rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
 +		hw_min = HWP_LOWEST_PERF(cap);
 +		if (limits->no_turbo)
 +			hw_max = HWP_GUARANTEED_PERF(cap);
 +		else
 +			hw_max = HWP_HIGHEST_PERF(cap);
 +		range = hw_max - hw_min;
++=======
+ 	return turbo_pstate ?
+ 		(cpu->pstate.min_pstate * 100 / turbo_pstate) : 0;
+ }
++>>>>>>> d4436c0dba8d (cpufreq: intel_pstate: Fix ratio setting for min_perf_pct)
  
 -static s16 intel_pstate_get_epb(struct cpudata *cpu_data)
 -{
 -	u64 epb;
 -	int ret;
 -
 -	if (!static_cpu_has(X86_FEATURE_EPB))
 -		return -ENXIO;
 -
 -	ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
 -	if (ret)
 -		return (s16)ret;
 -
 -	return (s16)(epb & 0x0f);
 -}
 -
 -static s16 intel_pstate_get_epp(struct cpudata *cpu_data, u64 hwp_req_data)
 -{
 -	s16 epp;
 -
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		/*
 -		 * When hwp_req_data is 0, means that caller didn't read
 -		 * MSR_HWP_REQUEST, so need to read and get EPP.
 -		 */
 -		if (!hwp_req_data) {
 -			epp = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST,
 -					    &hwp_req_data);
 -			if (epp)
 -				return epp;
 -		}
 -		epp = (hwp_req_data >> 24) & 0xff;
 -	} else {
 -		/* When there is no EPP present, HWP uses EPB settings */
 -		epp = intel_pstate_get_epb(cpu_data);
 -	}
 -
 -	return epp;
 -}
 -
 -static int intel_pstate_set_epb(int cpu, s16 pref)
 -{
 -	u64 epb;
 -	int ret;
 -
 -	if (!static_cpu_has(X86_FEATURE_EPB))
 -		return -ENXIO;
 -
 -	ret = rdmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
 -	if (ret)
 -		return ret;
 -
 -	epb = (epb & ~0x0f) | pref;
 -	wrmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, epb);
 -
 -	return 0;
 -}
 -
 -/*
 - * EPP/EPB display strings corresponding to EPP index in the
 - * energy_perf_strings[]
 - *	index		String
 - *-------------------------------------
 - *	0		default
 - *	1		performance
 - *	2		balance_performance
 - *	3		balance_power
 - *	4		power
 - */
 -static const char * const energy_perf_strings[] = {
 -	"default",
 -	"performance",
 -	"balance_performance",
 -	"balance_power",
 -	"power",
 -	NULL
 -};
 -
 -static int intel_pstate_get_energy_pref_index(struct cpudata *cpu_data)
 -{
 -	s16 epp;
 -	int index = -EINVAL;
 -
 -	epp = intel_pstate_get_epp(cpu_data, 0);
 -	if (epp < 0)
 -		return epp;
 -
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		/*
 -		 * Range:
 -		 *	0x00-0x3F	:	Performance
 -		 *	0x40-0x7F	:	Balance performance
 -		 *	0x80-0xBF	:	Balance power
 -		 *	0xC0-0xFF	:	Power
 -		 * The EPP is a 8 bit value, but our ranges restrict the
 -		 * value which can be set. Here only using top two bits
 -		 * effectively.
 -		 */
 -		index = (epp >> 6) + 1;
 -	} else if (static_cpu_has(X86_FEATURE_EPB)) {
 -		/*
 -		 * Range:
 -		 *	0x00-0x03	:	Performance
 -		 *	0x04-0x07	:	Balance performance
 -		 *	0x08-0x0B	:	Balance power
 -		 *	0x0C-0x0F	:	Power
 -		 * The EPB is a 4 bit value, but our ranges restrict the
 -		 * value which can be set. Here only using top two bits
 -		 * effectively.
 -		 */
 -		index = (epp >> 2) + 1;
 -	}
 -
 -	return index;
 -}
 -
 -static int intel_pstate_set_energy_pref_index(struct cpudata *cpu_data,
 -					      int pref_index)
 -{
 -	int epp = -EINVAL;
 -	int ret;
 -
 -	if (!pref_index)
 -		epp = cpu_data->epp_default;
 -
 -	mutex_lock(&intel_pstate_limits_lock);
 -
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		u64 value;
 -
 -		ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, &value);
 -		if (ret)
 -			goto return_pref;
 -
 -		value &= ~GENMASK_ULL(31, 24);
 -
 -		/*
 -		 * If epp is not default, convert from index into
 -		 * energy_perf_strings to epp value, by shifting 6
 -		 * bits left to use only top two bits in epp.
 -		 * The resultant epp need to shifted by 24 bits to
 -		 * epp position in MSR_HWP_REQUEST.
 -		 */
 -		if (epp == -EINVAL)
 -			epp = (pref_index - 1) << 6;
 -
 -		value |= (u64)epp << 24;
 -		ret = wrmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, value);
 -	} else {
 -		if (epp == -EINVAL)
 -			epp = (pref_index - 1) << 2;
 -		ret = intel_pstate_set_epb(cpu_data->cpu, epp);
 -	}
 -return_pref:
 -	mutex_unlock(&intel_pstate_limits_lock);
 -
 -	return ret;
 -}
 -
 -static ssize_t show_energy_performance_available_preferences(
 -				struct cpufreq_policy *policy, char *buf)
 -{
 -	int i = 0;
 -	int ret = 0;
 -
 -	while (energy_perf_strings[i] != NULL)
 -		ret += sprintf(&buf[ret], "%s ", energy_perf_strings[i++]);
 -
 -	ret += sprintf(&buf[ret], "\n");
 -
 -	return ret;
 -}
 -
 -cpufreq_freq_attr_ro(energy_performance_available_preferences);
 -
 -static ssize_t store_energy_performance_preference(
 -		struct cpufreq_policy *policy, const char *buf, size_t count)
 -{
 -	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
 -	char str_preference[21];
 -	int ret, i = 0;
 -
 -	ret = sscanf(buf, "%20s", str_preference);
 -	if (ret != 1)
 -		return -EINVAL;
 -
 -	while (energy_perf_strings[i] != NULL) {
 -		if (!strcmp(str_preference, energy_perf_strings[i])) {
 -			intel_pstate_set_energy_pref_index(cpu_data, i);
 -			return count;
 -		}
 -		++i;
 -	}
 -
 -	return -EINVAL;
 -}
 -
 -static ssize_t show_energy_performance_preference(
 -				struct cpufreq_policy *policy, char *buf)
 -{
 -	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
 -	int preference;
 -
 -	preference = intel_pstate_get_energy_pref_index(cpu_data);
 -	if (preference < 0)
 -		return preference;
 -
 -	return  sprintf(buf, "%s\n", energy_perf_strings[preference]);
 -}
 -
 -cpufreq_freq_attr_rw(energy_performance_preference);
 -
 -static struct freq_attr *hwp_cpufreq_attrs[] = {
 -	&energy_performance_preference,
 -	&energy_performance_available_preferences,
 -	NULL,
 -};
 -
 -static void intel_pstate_get_hwp_max(unsigned int cpu, int *phy_max,
 -				     int *current_max)
 -{
 -	u64 cap;
 -
 -	rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
 -	if (global.no_turbo)
 -		*current_max = HWP_GUARANTEED_PERF(cap);
 -	else
 -		*current_max = HWP_HIGHEST_PERF(cap);
 -
 -	*phy_max = HWP_HIGHEST_PERF(cap);
 -}
 -
 -static void intel_pstate_hwp_set(unsigned int cpu)
 -{
 -	struct cpudata *cpu_data = all_cpu_data[cpu];
 -	int max, min;
 -	u64 value;
 -	s16 epp;
 -
 -	max = cpu_data->max_perf_ratio;
 -	min = cpu_data->min_perf_ratio;
 -
 -	if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE)
 -		min = max;
 -
 -	rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
 -
 -	value &= ~HWP_MIN_PERF(~0L);
 -	value |= HWP_MIN_PERF(min);
 -
 -	value &= ~HWP_MAX_PERF(~0L);
 -	value |= HWP_MAX_PERF(max);
 -
 -	if (cpu_data->epp_policy == cpu_data->policy)
 -		goto skip_epp;
 -
 -	cpu_data->epp_policy = cpu_data->policy;
 +		rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
 +		adj_range = limits->min_perf_pct * range / 100;
 +		min = hw_min + adj_range;
 +		value &= ~HWP_MIN_PERF(~0L);
 +		value |= HWP_MIN_PERF(min);
  
 -	if (cpu_data->epp_saved >= 0) {
 -		epp = cpu_data->epp_saved;
 -		cpu_data->epp_saved = -EINVAL;
 -		goto update_epp;
 -	}
 -
 -	if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE) {
 -		epp = intel_pstate_get_epp(cpu_data, value);
 -		cpu_data->epp_powersave = epp;
 -		/* If EPP read was failed, then don't try to write */
 -		if (epp < 0)
 -			goto skip_epp;
 +		adj_range = limits->max_perf_pct * range / 100;
 +		max = hw_min + adj_range;
  
 -		epp = 0;
 -	} else {
 -		/* skip setting EPP, when saved value is invalid */
 -		if (cpu_data->epp_powersave < 0)
 -			goto skip_epp;
 -
 -		/*
 -		 * No need to restore EPP when it is not zero. This
 -		 * means:
 -		 *  - Policy is not changed
 -		 *  - user has manually changed
 -		 *  - Error reading EPB
 -		 */
 -		epp = intel_pstate_get_epp(cpu_data, value);
 -		if (epp)
 -			goto skip_epp;
 -
 -		epp = cpu_data->epp_powersave;
 -	}
 -update_epp:
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		value &= ~GENMASK_ULL(31, 24);
 -		value |= (u64)epp << 24;
 -	} else {
 -		intel_pstate_set_epb(cpu, epp);
 +		value &= ~HWP_MAX_PERF(~0L);
 +		value |= HWP_MAX_PERF(max);
 +		wrmsrl_on_cpu(cpu, MSR_HWP_REQUEST, value);
  	}
 -skip_epp:
 -	wrmsrl_on_cpu(cpu, MSR_HWP_REQUEST, value);
 -}
 -
 -static int intel_pstate_hwp_save_state(struct cpufreq_policy *policy)
 -{
 -	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
 -
 -	if (!hwp_active)
 -		return 0;
 -
 -	cpu_data->epp_saved = intel_pstate_get_epp(cpu_data, 0);
 -
 -	return 0;
 -}
 -
 -static int intel_pstate_resume(struct cpufreq_policy *policy)
 -{
 -	if (!hwp_active)
 -		return 0;
 -
 -	mutex_lock(&intel_pstate_limits_lock);
 -
 -	all_cpu_data[policy->cpu]->epp_policy = 0;
 -	intel_pstate_hwp_set(policy->cpu);
 -
 -	mutex_unlock(&intel_pstate_limits_lock);
 -
 -	return 0;
  }
  
 -static void intel_pstate_update_policies(void)
 +static void intel_pstate_hwp_set_online_cpus(void)
  {
 -	int cpu;
 -
 -	for_each_possible_cpu(cpu)
 -		cpufreq_update_policy(cpu);
 +	get_online_cpus();
 +	intel_pstate_hwp_set(cpu_online_mask);
 +	put_online_cpus();
  }
  
  /************************** debugfs begin ************************/
* Unmerged path drivers/cpufreq/intel_pstate.c
