mm: replace FAULT_FLAG_SIZE with parameter to huge_fault

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] replace FAULT_FLAG_SIZE with parameter to huge_fault (Larry Woodman) [1457572 1457561]
Rebuild_FUZZ: 96.30%
commit-author Dave Jiang <dave.jiang@intel.com>
commit c791ace1e747371658237f0d30234fef56c39669
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c791ace1.failed

Since the introduction of FAULT_FLAG_SIZE to the vm_fault flag, it has
been somewhat painful with getting the flags set and removed at the
correct locations.  More than one kernel oops was introduced due to
difficulties of getting the placement correctly.

Remove the flag values and introduce an input parameter to huge_fault
that indicates the size of the page entry.  This makes the code easier
to trace and should avoid the issues we see with the fault flags where
removal of the flag was necessary in the fallback paths.

Link: http://lkml.kernel.org/r/148615748258.43180.1690152053774975329.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
	Tested-by: Dan Williams <dan.j.williams@intel.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
	Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Nilesh Choudhury <nilesh.choudhury@oracle.com>
	Cc: Ingo Molnar <mingo@elte.hu>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c791ace1e747371658237f0d30234fef56c39669)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/dax.c
#	fs/dax.c
#	fs/ext2/file.c
#	fs/ext4/file.c
#	fs/xfs/xfs_file.c
#	include/linux/dax.h
#	include/linux/mm.h
#	mm/memory.c
diff --cc drivers/dax/dax.c
index 85c0bc93f989,b75c77254fdb..000000000000
--- a/drivers/dax/dax.c
+++ b/drivers/dax/dax.c
@@@ -533,31 -489,94 +533,106 @@@ static int __dax_dev_pmd_fault(struct d
  
  	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
  
 -	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd, pfn,
 -			vmf->flags & FAULT_FLAG_WRITE);
 +	return vmf_insert_pfn_pmd(vma, addr, pmd, pfn,
 +			flags & FAULT_FLAG_WRITE);
  }
  
++<<<<<<< HEAD
 +static int dax_dev_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
 +		pmd_t *pmd, unsigned int flags)
++=======
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static int __dax_dev_pud_fault(struct dax_dev *dax_dev, struct vm_fault *vmf)
+ {
+ 	unsigned long pud_addr = vmf->address & PUD_MASK;
+ 	struct device *dev = &dax_dev->dev;
+ 	struct dax_region *dax_region;
+ 	phys_addr_t phys;
+ 	pgoff_t pgoff;
+ 	pfn_t pfn;
+ 
+ 	if (check_vma(dax_dev, vmf->vma, __func__))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	dax_region = dax_dev->region;
+ 	if (dax_region->align > PUD_SIZE) {
+ 		dev_dbg(dev, "%s: alignment > fault size\n", __func__);
+ 		return VM_FAULT_SIGBUS;
+ 	}
+ 
+ 	/* dax pud mappings require pfn_t_devmap() */
+ 	if ((dax_region->pfn_flags & (PFN_DEV|PFN_MAP)) != (PFN_DEV|PFN_MAP)) {
+ 		dev_dbg(dev, "%s: alignment > fault size\n", __func__);
+ 		return VM_FAULT_SIGBUS;
+ 	}
+ 
+ 	pgoff = linear_page_index(vmf->vma, pud_addr);
+ 	phys = pgoff_to_phys(dax_dev, pgoff, PUD_SIZE);
+ 	if (phys == -1) {
+ 		dev_dbg(dev, "%s: phys_to_pgoff(%#lx) failed\n", __func__,
+ 				pgoff);
+ 		return VM_FAULT_SIGBUS;
+ 	}
+ 
+ 	pfn = phys_to_pfn_t(phys, dax_region->pfn_flags);
+ 
+ 	return vmf_insert_pfn_pud(vmf->vma, vmf->address, vmf->pud, pfn,
+ 			vmf->flags & FAULT_FLAG_WRITE);
+ }
+ #else
+ static int __dax_dev_pud_fault(struct dax_dev *dax_dev, struct vm_fault *vmf)
+ {
+ 	return VM_FAULT_FALLBACK;
+ }
+ #endif /* !CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */
+ 
+ static int dax_dev_huge_fault(struct vm_fault *vmf,
+ 		enum page_entry_size pe_size)
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  {
  	int rc;
 -	struct file *filp = vmf->vma->vm_file;
 +	struct file *filp = vma->vm_file;
  	struct dax_dev *dax_dev = filp->private_data;
  
  	dev_dbg(&dax_dev->dev, "%s: %s: %s (%#lx - %#lx)\n", __func__,
 -			current->comm, (vmf->flags & FAULT_FLAG_WRITE)
 -			? "write" : "read",
 -			vmf->vma->vm_start, vmf->vma->vm_end);
 +			current->comm, (flags & FAULT_FLAG_WRITE)
 +			? "write" : "read", vma->vm_start, vma->vm_end);
  
  	rcu_read_lock();
++<<<<<<< HEAD
 +	rc = __dax_dev_pmd_fault(dax_dev, vma, addr, pmd, flags);
++=======
+ 	switch (pe_size) {
+ 	case PE_SIZE_PTE:
+ 		rc = __dax_dev_pte_fault(dax_dev, vmf);
+ 		break;
+ 	case PE_SIZE_PMD:
+ 		rc = __dax_dev_pmd_fault(dax_dev, vmf);
+ 		break;
+ 	case PE_SIZE_PUD:
+ 		rc = __dax_dev_pud_fault(dax_dev, vmf);
+ 		break;
+ 	default:
+ 		return VM_FAULT_FALLBACK;
+ 	}
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  	rcu_read_unlock();
  
  	return rc;
  }
  
+ static int dax_dev_fault(struct vm_fault *vmf)
+ {
+ 	return dax_dev_huge_fault(vmf, PE_SIZE_PTE);
+ }
+ 
  static const struct vm_operations_struct dax_dev_vm_ops = {
  	.fault = dax_dev_fault,
++<<<<<<< HEAD
 +	.pmd_fault = dax_dev_pmd_fault,
++=======
+ 	.huge_fault = dax_dev_huge_fault,
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  };
  
  static int dax_mmap(struct file *filp, struct vm_area_struct *vma)
diff --cc fs/dax.c
index fa7935571d11,5ae8b71ebadc..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -989,56 -992,476 +989,417 @@@ int __dax_zero_page_range(struct block_
  }
  EXPORT_SYMBOL_GPL(__dax_zero_page_range);
  
 -static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
 -{
 -	return iomap->blkno + (((pos & PAGE_MASK) - iomap->offset) >> 9);
 -}
 -
 -static loff_t
 -dax_iomap_actor(struct inode *inode, loff_t pos, loff_t length, void *data,
 -		struct iomap *iomap)
 +/**
 + * dax_zero_page_range - zero a range within a page of a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @length: The number of bytes to zero
 + * @get_block: The filesystem method used to translate file offsets to blocks
 + *
 + * This function can be called by a filesystem when it is zeroing part of a
 + * page in a DAX file.  This is intended for hole-punch operations.  If
 + * you are truncating a file, the helper function dax_truncate_page() may be
 + * more convenient.
 + */
 +int dax_zero_page_range(struct inode *inode, loff_t from, unsigned length,
 +							get_block_t get_block)
  {
 -	struct iov_iter *iter = data;
 -	loff_t end = pos + length, done = 0;
 -	ssize_t ret = 0;
 -
 -	if (iov_iter_rw(iter) == READ) {
 -		end = min(end, i_size_read(inode));
 -		if (pos >= end)
 -			return 0;
 -
 -		if (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)
 -			return iov_iter_zero(min(length, end - pos), iter);
 -	}
 -
 -	if (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED))
 -		return -EIO;
 +	struct buffer_head bh;
 +	pgoff_t index = from >> PAGE_CACHE_SHIFT;
 +	unsigned offset = from & (PAGE_CACHE_SIZE-1);
 +	int err;
  
 -	/*
 -	 * Write can allocate block for an area which has a hole page mapped
 -	 * into page tables. We have to tear down these mappings so that data
 -	 * written by write(2) is visible in mmap.
 -	 */
 -	if ((iomap->flags & IOMAP_F_NEW) && inode->i_mapping->nrpages) {
 -		invalidate_inode_pages2_range(inode->i_mapping,
 -					      pos >> PAGE_SHIFT,
 -					      (end - 1) >> PAGE_SHIFT);
 -	}
 -
 -	while (pos < end) {
 -		unsigned offset = pos & (PAGE_SIZE - 1);
 -		struct blk_dax_ctl dax = { 0 };
 -		ssize_t map_len;
 -
 -		if (fatal_signal_pending(current)) {
 -			ret = -EINTR;
 -			break;
 -		}
 -
 -		dax.sector = dax_iomap_sector(iomap, pos);
 -		dax.size = (length + offset + PAGE_SIZE - 1) & PAGE_MASK;
 -		map_len = dax_map_atomic(iomap->bdev, &dax);
 -		if (map_len < 0) {
 -			ret = map_len;
 -			break;
 -		}
 -
 -		dax.addr += offset;
 -		map_len -= offset;
 -		if (map_len > end - pos)
 -			map_len = end - pos;
 -
 -		if (iov_iter_rw(iter) == WRITE)
 -			map_len = copy_from_iter_pmem(dax.addr, map_len, iter);
 -		else
 -			map_len = copy_to_iter(dax.addr, map_len, iter);
 -		dax_unmap_atomic(iomap->bdev, &dax);
 -		if (map_len <= 0) {
 -			ret = map_len ? map_len : -EFAULT;
 -			break;
 -		}
 -
 -		pos += map_len;
 -		length -= map_len;
 -		done += map_len;
 -	}
 -
 -	return done ? done : ret;
 +	/* Block boundary? Nothing to do */
 +	if (!length)
 +		return 0;
 +	if (WARN_ON_ONCE((offset + length) > PAGE_CACHE_SIZE))
 +		return -EINVAL;
 +
 +	memset(&bh, 0, sizeof(bh));
 +	bh.b_bdev = inode->i_sb->s_bdev;
 +	bh.b_size = PAGE_CACHE_SIZE;
 +	err = get_block(inode, index, &bh, 0);
 +	if (err < 0 || !buffer_written(&bh))
 +		return err;
 +
 +	return __dax_zero_page_range(bh.b_bdev, to_sector(&bh, inode),
 +			offset, length);
  }
 +EXPORT_SYMBOL_GPL(dax_zero_page_range);
  
  /**
 - * dax_iomap_rw - Perform I/O to a DAX file
 - * @iocb:	The control block for this I/O
 - * @iter:	The addresses to do I/O from or to
 - * @ops:	iomap ops passed from the file system
 + * dax_truncate_page - handle a partial page being truncated in a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @get_block: The filesystem method used to translate file offsets to blocks
   *
 - * This function performs read and write operations to directly mapped
 - * persistent memory.  The callers needs to take care of read/write exclusion
 - * and evicting any page cache pages in the region under I/O.
 + * Similar to block_truncate_page(), this function can be called by a
 + * filesystem when it is truncating a DAX file to handle the partial page.
   */
 -ssize_t
 -dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
 -		const struct iomap_ops *ops)
 +int dax_truncate_page(struct inode *inode, loff_t from, get_block_t get_block)
  {
 -	struct address_space *mapping = iocb->ki_filp->f_mapping;
 -	struct inode *inode = mapping->host;
 -	loff_t pos = iocb->ki_pos, ret = 0, done = 0;
 -	unsigned flags = 0;
 -
 -	if (iov_iter_rw(iter) == WRITE) {
 -		lockdep_assert_held_exclusive(&inode->i_rwsem);
 -		flags |= IOMAP_WRITE;
 -	} else {
 -		lockdep_assert_held(&inode->i_rwsem);
 -	}
 -
 -	while (iov_iter_count(iter)) {
 -		ret = iomap_apply(inode, pos, iov_iter_count(iter), flags, ops,
 -				iter, dax_iomap_actor);
 -		if (ret <= 0)
 -			break;
 -		pos += ret;
 -		done += ret;
 -	}
 -
 -	iocb->ki_pos += done;
 -	return done ? done : ret;
 +	unsigned length = PAGE_CACHE_ALIGN(from) - from;
 +	return dax_zero_page_range(inode, from, length, get_block);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(dax_truncate_page);
++=======
+ EXPORT_SYMBOL_GPL(dax_iomap_rw);
+ 
+ static int dax_fault_return(int error)
+ {
+ 	if (error == 0)
+ 		return VM_FAULT_NOPAGE;
+ 	if (error == -ENOMEM)
+ 		return VM_FAULT_OOM;
+ 	return VM_FAULT_SIGBUS;
+ }
+ 
+ static int dax_iomap_pte_fault(struct vm_fault *vmf,
+ 			       const struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	unsigned long vaddr = vmf->address;
+ 	loff_t pos = (loff_t)vmf->pgoff << PAGE_SHIFT;
+ 	sector_t sector;
+ 	struct iomap iomap = { 0 };
+ 	unsigned flags = IOMAP_FAULT;
+ 	int error, major = 0;
+ 	int vmf_ret = 0;
+ 	void *entry;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is supposed
+ 	 * to hold locks serializing us with truncate / punch hole so this is
+ 	 * a reliable test.
+ 	 */
+ 	if (pos >= i_size_read(inode))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Note that we don't bother to use iomap_apply here: DAX required
+ 	 * the file system block size to be equal the page size, which means
+ 	 * that we never have to deal with more than a single extent here.
+ 	 */
+ 	error = ops->iomap_begin(inode, pos, PAGE_SIZE, flags, &iomap);
+ 	if (error)
+ 		return dax_fault_return(error);
+ 	if (WARN_ON_ONCE(iomap.offset + iomap.length < pos + PAGE_SIZE)) {
+ 		vmf_ret = dax_fault_return(-EIO);	/* fs corruption? */
+ 		goto finish_iomap;
+ 	}
+ 
+ 	entry = grab_mapping_entry(mapping, vmf->pgoff, 0);
+ 	if (IS_ERR(entry)) {
+ 		vmf_ret = dax_fault_return(PTR_ERR(entry));
+ 		goto finish_iomap;
+ 	}
+ 
+ 	sector = dax_iomap_sector(&iomap, pos);
+ 
+ 	if (vmf->cow_page) {
+ 		switch (iomap.type) {
+ 		case IOMAP_HOLE:
+ 		case IOMAP_UNWRITTEN:
+ 			clear_user_highpage(vmf->cow_page, vaddr);
+ 			break;
+ 		case IOMAP_MAPPED:
+ 			error = copy_user_dax(iomap.bdev, sector, PAGE_SIZE,
+ 					vmf->cow_page, vaddr);
+ 			break;
+ 		default:
+ 			WARN_ON_ONCE(1);
+ 			error = -EIO;
+ 			break;
+ 		}
+ 
+ 		if (error)
+ 			goto error_unlock_entry;
+ 
+ 		__SetPageUptodate(vmf->cow_page);
+ 		vmf_ret = finish_fault(vmf);
+ 		if (!vmf_ret)
+ 			vmf_ret = VM_FAULT_DONE_COW;
+ 		goto unlock_entry;
+ 	}
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		if (iomap.flags & IOMAP_F_NEW) {
+ 			count_vm_event(PGMAJFAULT);
+ 			mem_cgroup_count_vm_event(vmf->vma->vm_mm, PGMAJFAULT);
+ 			major = VM_FAULT_MAJOR;
+ 		}
+ 		error = dax_insert_mapping(mapping, iomap.bdev, sector,
+ 				PAGE_SIZE, &entry, vmf->vma, vmf);
+ 		/* -EBUSY is fine, somebody else faulted on the same PTE */
+ 		if (error == -EBUSY)
+ 			error = 0;
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (!(vmf->flags & FAULT_FLAG_WRITE)) {
+ 			vmf_ret = dax_load_hole(mapping, &entry, vmf);
+ 			goto unlock_entry;
+ 		}
+ 		/*FALLTHRU*/
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		error = -EIO;
+ 		break;
+ 	}
+ 
+  error_unlock_entry:
+ 	vmf_ret = dax_fault_return(error) | major;
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, vmf->pgoff, entry);
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		int copied = PAGE_SIZE;
+ 
+ 		if (vmf_ret & VM_FAULT_ERROR)
+ 			copied = 0;
+ 		/*
+ 		 * The fault is done by now and there's no way back (other
+ 		 * thread may be already happily using PTE we have installed).
+ 		 * Just ignore error from ->iomap_end since we cannot do much
+ 		 * with it.
+ 		 */
+ 		ops->iomap_end(inode, pos, PAGE_SIZE, copied, flags, &iomap);
+ 	}
+ 	return vmf_ret;
+ }
+ 
+ #ifdef CONFIG_FS_DAX_PMD
+ /*
+  * The 'colour' (ie low bits) within a PMD of a page offset.  This comes up
+  * more often than one might expect in the below functions.
+  */
+ #define PG_PMD_COLOUR	((PMD_SIZE >> PAGE_SHIFT) - 1)
+ 
+ static int dax_pmd_insert_mapping(struct vm_fault *vmf, struct iomap *iomap,
+ 		loff_t pos, void **entryp)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	struct block_device *bdev = iomap->bdev;
+ 	struct inode *inode = mapping->host;
+ 	struct blk_dax_ctl dax = {
+ 		.sector = dax_iomap_sector(iomap, pos),
+ 		.size = PMD_SIZE,
+ 	};
+ 	long length = dax_map_atomic(bdev, &dax);
+ 	void *ret = NULL;
+ 
+ 	if (length < 0) /* dax_map_atomic() failed */
+ 		goto fallback;
+ 	if (length < PMD_SIZE)
+ 		goto unmap_fallback;
+ 	if (pfn_t_to_pfn(dax.pfn) & PG_PMD_COLOUR)
+ 		goto unmap_fallback;
+ 	if (!pfn_t_devmap(dax.pfn))
+ 		goto unmap_fallback;
+ 
+ 	dax_unmap_atomic(bdev, &dax);
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, dax.sector,
+ 			RADIX_DAX_PMD);
+ 	if (IS_ERR(ret))
+ 		goto fallback;
+ 	*entryp = ret;
+ 
+ 	trace_dax_pmd_insert_mapping(inode, vmf, length, dax.pfn, ret);
+ 	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd,
+ 			dax.pfn, vmf->flags & FAULT_FLAG_WRITE);
+ 
+  unmap_fallback:
+ 	dax_unmap_atomic(bdev, &dax);
+ fallback:
+ 	trace_dax_pmd_insert_mapping_fallback(inode, vmf, length,
+ 			dax.pfn, ret);
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int dax_pmd_load_hole(struct vm_fault *vmf, struct iomap *iomap,
+ 		void **entryp)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = vmf->address & PMD_MASK;
+ 	struct inode *inode = mapping->host;
+ 	struct page *zero_page;
+ 	void *ret = NULL;
+ 	spinlock_t *ptl;
+ 	pmd_t pmd_entry;
+ 
+ 	zero_page = mm_get_huge_zero_page(vmf->vma->vm_mm);
+ 
+ 	if (unlikely(!zero_page))
+ 		goto fallback;
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, 0,
+ 			RADIX_DAX_PMD | RADIX_DAX_HZP);
+ 	if (IS_ERR(ret))
+ 		goto fallback;
+ 	*entryp = ret;
+ 
+ 	ptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);
+ 	if (!pmd_none(*(vmf->pmd))) {
+ 		spin_unlock(ptl);
+ 		goto fallback;
+ 	}
+ 
+ 	pmd_entry = mk_pmd(zero_page, vmf->vma->vm_page_prot);
+ 	pmd_entry = pmd_mkhuge(pmd_entry);
+ 	set_pmd_at(vmf->vma->vm_mm, pmd_addr, vmf->pmd, pmd_entry);
+ 	spin_unlock(ptl);
+ 	trace_dax_pmd_load_hole(inode, vmf, zero_page, ret);
+ 	return VM_FAULT_NOPAGE;
+ 
+ fallback:
+ 	trace_dax_pmd_load_hole_fallback(inode, vmf, zero_page, ret);
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int dax_iomap_pmd_fault(struct vm_fault *vmf,
+ 			       const struct iomap_ops *ops)
+ {
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = vmf->address & PMD_MASK;
+ 	bool write = vmf->flags & FAULT_FLAG_WRITE;
+ 	unsigned int iomap_flags = (write ? IOMAP_WRITE : 0) | IOMAP_FAULT;
+ 	struct inode *inode = mapping->host;
+ 	int result = VM_FAULT_FALLBACK;
+ 	struct iomap iomap = { 0 };
+ 	pgoff_t max_pgoff, pgoff;
+ 	void *entry;
+ 	loff_t pos;
+ 	int error;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is
+ 	 * supposed to hold locks serializing us with truncate / punch hole so
+ 	 * this is a reliable test.
+ 	 */
+ 	pgoff = linear_page_index(vma, pmd_addr);
+ 	max_pgoff = (i_size_read(inode) - 1) >> PAGE_SHIFT;
+ 
+ 	trace_dax_pmd_fault(inode, vmf, max_pgoff, 0);
+ 
+ 	/* Fall back to PTEs if we're going to COW */
+ 	if (write && !(vma->vm_flags & VM_SHARED))
+ 		goto fallback;
+ 
+ 	/* If the PMD would extend outside the VMA */
+ 	if (pmd_addr < vma->vm_start)
+ 		goto fallback;
+ 	if ((pmd_addr + PMD_SIZE) > vma->vm_end)
+ 		goto fallback;
+ 
+ 	if (pgoff > max_pgoff) {
+ 		result = VM_FAULT_SIGBUS;
+ 		goto out;
+ 	}
+ 
+ 	/* If the PMD would extend beyond the file size */
+ 	if ((pgoff | PG_PMD_COLOUR) > max_pgoff)
+ 		goto fallback;
+ 
+ 	/*
+ 	 * Note that we don't use iomap_apply here.  We aren't doing I/O, only
+ 	 * setting up a mapping, so really we're using iomap_begin() as a way
+ 	 * to look up our filesystem block.
+ 	 */
+ 	pos = (loff_t)pgoff << PAGE_SHIFT;
+ 	error = ops->iomap_begin(inode, pos, PMD_SIZE, iomap_flags, &iomap);
+ 	if (error)
+ 		goto fallback;
+ 
+ 	if (iomap.offset + iomap.length < pos + PMD_SIZE)
+ 		goto finish_iomap;
+ 
+ 	/*
+ 	 * grab_mapping_entry() will make sure we get a 2M empty entry, a DAX
+ 	 * PMD or a HZP entry.  If it can't (because a 4k page is already in
+ 	 * the tree, for instance), it will return -EEXIST and we just fall
+ 	 * back to 4k entries.
+ 	 */
+ 	entry = grab_mapping_entry(mapping, pgoff, RADIX_DAX_PMD);
+ 	if (IS_ERR(entry))
+ 		goto finish_iomap;
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		result = dax_pmd_insert_mapping(vmf, &iomap, pos, &entry);
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (WARN_ON_ONCE(write))
+ 			goto unlock_entry;
+ 		result = dax_pmd_load_hole(vmf, &iomap, &entry);
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		break;
+ 	}
+ 
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, pgoff, entry);
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		int copied = PMD_SIZE;
+ 
+ 		if (result == VM_FAULT_FALLBACK)
+ 			copied = 0;
+ 		/*
+ 		 * The fault is done by now and there's no way back (other
+ 		 * thread may be already happily using PMD we have installed).
+ 		 * Just ignore error from ->iomap_end since we cannot do much
+ 		 * with it.
+ 		 */
+ 		ops->iomap_end(inode, pos, PMD_SIZE, copied, iomap_flags,
+ 				&iomap);
+ 	}
+  fallback:
+ 	if (result == VM_FAULT_FALLBACK) {
+ 		split_huge_pmd(vma, vmf->pmd, vmf->address);
+ 		count_vm_event(THP_FAULT_FALLBACK);
+ 	}
+ out:
+ 	trace_dax_pmd_fault_done(inode, vmf, max_pgoff, result);
+ 	return result;
+ }
+ #else
+ static int dax_iomap_pmd_fault(struct vm_fault *vmf, struct iomap_ops *ops)
+ {
+ 	return VM_FAULT_FALLBACK;
+ }
+ #endif /* CONFIG_FS_DAX_PMD */
+ 
+ /**
+  * dax_iomap_fault - handle a page fault on a DAX file
+  * @vmf: The description of the fault
+  * @ops: iomap ops passed from the file system
+  *
+  * When a page fault occurs, filesystems may call this helper in
+  * their fault handler for DAX files. dax_iomap_fault() assumes the caller
+  * has done all the necessary locking for page fault to proceed
+  * successfully.
+  */
+ int dax_iomap_fault(struct vm_fault *vmf, enum page_entry_size pe_size,
+ 		    const struct iomap_ops *ops)
+ {
+ 	switch (pe_size) {
+ 	case PE_SIZE_PTE:
+ 		return dax_iomap_pte_fault(vmf, ops);
+ 	case PE_SIZE_PMD:
+ 		return dax_iomap_pmd_fault(vmf, ops);
+ 	default:
+ 		return VM_FAULT_FALLBACK;
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_fault);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
diff --cc fs/ext2/file.c
index 67c839401773,b21891a6bfca..000000000000
--- a/fs/ext2/file.c
+++ b/fs/ext2/file.c
@@@ -51,7 -99,7 +51,11 @@@ static int ext2_dax_fault(struct vm_are
  	}
  	down_read(&ei->dax_sem);
  
++<<<<<<< HEAD
 +	ret = dax_fault(vma, vmf, ext2_get_block);
++=======
+ 	ret = dax_iomap_fault(vmf, PE_SIZE_PTE, &ext2_iomap_ops);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  
  	up_read(&ei->dax_sem);
  	if (vmf->flags & FAULT_FLAG_WRITE)
diff --cc fs/ext4/file.c
index b51446198588,8210c1f43556..000000000000
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@@ -202,71 -253,23 +202,85 @@@ ext4_file_write(struct kiocb *iocb, con
  }
  
  #ifdef CONFIG_FS_DAX
++<<<<<<< HEAD
 +static int ext4_dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
++=======
+ static int ext4_dax_huge_fault(struct vm_fault *vmf,
+ 		enum page_entry_size pe_size)
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  {
  	int result;
 -	struct inode *inode = file_inode(vmf->vma->vm_file);
 +	handle_t *handle = NULL;
 +	struct inode *inode = file_inode(vma->vm_file);
  	struct super_block *sb = inode->i_sb;
  	bool write = vmf->flags & FAULT_FLAG_WRITE;
  
  	if (write) {
  		sb_start_pagefault(sb);
++<<<<<<< HEAD
 +		file_update_time(vma->vm_file);
 +		down_read(&EXT4_I(inode)->i_mmap_sem);
 +		handle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,
 +						EXT4_DATA_TRANS_BLOCKS(sb));
 +	} else
 +		down_read(&EXT4_I(inode)->i_mmap_sem);
 +
 +	if (IS_ERR(handle))
 +		result = VM_FAULT_SIGBUS;
 +	else
 +		result = dax_fault(vma, vmf, ext4_dax_get_block);
 +
 +	if (write) {
 +		if (!IS_ERR(handle))
 +			ext4_journal_stop(handle);
 +		up_read(&EXT4_I(inode)->i_mmap_sem);
++=======
+ 		file_update_time(vmf->vma->vm_file);
+ 	}
+ 	down_read(&EXT4_I(inode)->i_mmap_sem);
+ 	result = dax_iomap_fault(vmf, pe_size, &ext4_iomap_ops);
+ 	up_read(&EXT4_I(inode)->i_mmap_sem);
+ 	if (write)
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  		sb_end_pagefault(sb);
 +	} else
 +		up_read(&EXT4_I(inode)->i_mmap_sem);
 +
 +	return result;
 +}
 +
 +static int ext4_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,
 +						pmd_t *pmd, unsigned int flags)
 +{
 +	int result;
 +	handle_t *handle = NULL;
 +	struct inode *inode = file_inode(vma->vm_file);
 +	struct super_block *sb = inode->i_sb;
 +	bool write = flags & FAULT_FLAG_WRITE;
 +
 +	if (write) {
 +		sb_start_pagefault(sb);
 +		file_update_time(vma->vm_file);
 +		down_read(&EXT4_I(inode)->i_mmap_sem);
 +		handle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,
 +				ext4_chunk_trans_blocks(inode,
 +							PMD_SIZE / PAGE_SIZE));
 +	} else
 +		down_read(&EXT4_I(inode)->i_mmap_sem);
 +
 +	if (IS_ERR(handle))
 +		result = VM_FAULT_SIGBUS;
 +	else
 +		result = dax_pmd_fault(vma, addr, pmd, flags,
 +				ext4_dax_get_block);
 +
 +	if (write) {
 +		if (!IS_ERR(handle))
 +			ext4_journal_stop(handle);
 +		up_read(&EXT4_I(inode)->i_mmap_sem);
 +		sb_end_pagefault(sb);
 +	} else
 +		up_read(&EXT4_I(inode)->i_mmap_sem);
  
  	return result;
  }
@@@ -304,10 -311,9 +323,14 @@@ static int ext4_dax_pfn_mkwrite(struct 
  
  static const struct vm_operations_struct ext4_dax_vm_ops = {
  	.fault		= ext4_dax_fault,
++<<<<<<< HEAD
 +	.pmd_fault	= ext4_dax_pmd_fault,
++=======
+ 	.huge_fault	= ext4_dax_huge_fault,
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  	.page_mkwrite	= ext4_dax_fault,
  	.pfn_mkwrite	= ext4_dax_pfn_mkwrite,
 +	.remap_pages	= generic_file_remap_pages,
  };
  #else
  #define ext4_dax_vm_ops	ext4_file_vm_ops
diff --cc fs/xfs/xfs_file.c
index d7247d5d7ddf,a50eca676670..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -1712,9 -1391,9 +1712,13 @@@ xfs_filemap_page_mkwrite
  	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
  
  	if (IS_DAX(inode)) {
++<<<<<<< HEAD
 +		ret = dax_mkwrite(vma, vmf, xfs_get_blocks_dax_fault);
++=======
+ 		ret = dax_iomap_fault(vmf, PE_SIZE_PTE, &xfs_iomap_ops);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  	} else {
 -		ret = iomap_page_mkwrite(vmf, &xfs_iomap_ops);
 +		ret = __block_page_mkwrite(vma, vmf, xfs_get_blocks);
  		ret = block_page_mkwrite_return(ret);
  	}
  
@@@ -1736,19 -1414,13 +1740,26 @@@ xfs_filemap_fault
  
  	/* DAX can shortcut the normal fault path on write faults! */
  	if ((vmf->flags & FAULT_FLAG_WRITE) && IS_DAX(inode))
 -		return xfs_filemap_page_mkwrite(vmf);
 +		return xfs_filemap_page_mkwrite(vma, vmf);
  
  	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
++<<<<<<< HEAD
 +	if (IS_DAX(inode)) {
 +		/*
 +		 * we do not want to trigger unwritten extent conversion on read
 +		 * faults - that is unnecessary overhead and would also require
 +		 * changes to xfs_get_blocks_direct() to map unwritten extent
 +		 * ioend for conversion on read-only mappings.
 +		 */
 +		ret = dax_fault(vma, vmf, xfs_get_blocks_dax_fault);
 +	} else
 +		ret = filemap_fault(vma, vmf);
++=======
+ 	if (IS_DAX(inode))
+ 		ret = dax_iomap_fault(vmf, PE_SIZE_PTE, &xfs_iomap_ops);
+ 	else
+ 		ret = filemap_fault(vmf);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
  
  	return ret;
@@@ -1762,13 -1434,11 +1773,19 @@@
   * occuring.
   */
  STATIC int
++<<<<<<< HEAD
 +xfs_filemap_pmd_fault(
 +	struct vm_area_struct	*vma,
 +	unsigned long		addr,
 +	pmd_t			*pmd,
 +	unsigned int		flags)
++=======
+ xfs_filemap_huge_fault(
+ 	struct vm_fault		*vmf,
+ 	enum page_entry_size	pe_size)
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  {
 -	struct inode		*inode = file_inode(vmf->vma->vm_file);
 +	struct inode		*inode = file_inode(vma->vm_file);
  	struct xfs_inode	*ip = XFS_I(inode);
  	int			ret;
  
@@@ -1783,10 -1453,10 +1800,14 @@@
  	}
  
  	xfs_ilock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
++<<<<<<< HEAD
 +	ret = dax_pmd_fault(vma, addr, pmd, flags, xfs_get_blocks_dax_fault);
++=======
+ 	ret = dax_iomap_fault(vmf, pe_size, &xfs_iomap_ops);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  	xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
  
 -	if (vmf->flags & FAULT_FLAG_WRITE)
 +	if (flags & FAULT_FLAG_WRITE)
  		sb_end_pagefault(inode->i_sb);
  
  	return ret;
diff --cc include/linux/dax.h
index 8937c7aed5cb,d8a3dc042e1c..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -6,29 -6,44 +6,43 @@@
  #include <linux/radix-tree.h>
  #include <asm/pgtable.h>
  
 -struct iomap_ops;
 -
  /*
 - * We use lowest available bit in exceptional entry for locking, one bit for
 - * the entry size (PMD) and two more to tell us if the entry is a huge zero
 - * page (HZP) or an empty entry that is just used for locking.  In total four
 - * special bits.
 - *
 - * If the PMD bit isn't set the entry has size PAGE_SIZE, and if the HZP and
 - * EMPTY bits aren't set the entry is a normal DAX entry with a filesystem
 - * block allocation.
 + * We use lowest available bit in exceptional entry for locking, other two
 + * bits to determine entry type. In total 3 special bits.
   */
 -#define RADIX_DAX_SHIFT	(RADIX_TREE_EXCEPTIONAL_SHIFT + 4)
 +#define RADIX_DAX_SHIFT	(RADIX_TREE_EXCEPTIONAL_SHIFT + 3)
  #define RADIX_DAX_ENTRY_LOCK (1 << RADIX_TREE_EXCEPTIONAL_SHIFT)
 -#define RADIX_DAX_PMD (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 1))
 -#define RADIX_DAX_HZP (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 2))
 -#define RADIX_DAX_EMPTY (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 3))
 +#define RADIX_DAX_PTE (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 1))
 +#define RADIX_DAX_PMD (1 << (RADIX_TREE_EXCEPTIONAL_SHIFT + 2))
 +#define RADIX_DAX_TYPE_MASK (RADIX_DAX_PTE | RADIX_DAX_PMD)
 +#define RADIX_DAX_TYPE(entry) ((unsigned long)entry & RADIX_DAX_TYPE_MASK)
 +#define RADIX_DAX_SECTOR(entry) (((unsigned long)entry >> RADIX_DAX_SHIFT))
 +#define RADIX_DAX_ENTRY(sector, pmd) ((void *)((unsigned long)sector << \
 +		RADIX_DAX_SHIFT | (pmd ? RADIX_DAX_PMD : RADIX_DAX_PTE) | \
 +		RADIX_TREE_EXCEPTIONAL_ENTRY))
  
 -static inline unsigned long dax_radix_sector(void *entry)
 -{
 -	return (unsigned long)entry >> RADIX_DAX_SHIFT;
 -}
  
++<<<<<<< HEAD
 +ssize_t dax_do_io(int rw, struct kiocb *iocb, struct inode *inode,
 +                  const struct iovec *iov, loff_t pos, unsigned long nr_segs,
 +                  get_block_t get_block, dio_iodone_t end_io, int flags);
 +int dax_zero_page_range(struct inode *, loff_t from, unsigned len, get_block_t);
 +int dax_truncate_page(struct inode *, loff_t from, get_block_t);
 +int dax_fault(struct vm_area_struct *, struct vm_fault *, get_block_t);
++=======
+ static inline void *dax_radix_locked_entry(sector_t sector, unsigned long flags)
+ {
+ 	return (void *)(RADIX_TREE_EXCEPTIONAL_ENTRY | flags |
+ 			((unsigned long)sector << RADIX_DAX_SHIFT) |
+ 			RADIX_DAX_ENTRY_LOCK);
+ }
+ 
+ ssize_t dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
+ 		const struct iomap_ops *ops);
+ int dax_iomap_fault(struct vm_fault *vmf, enum page_entry_size pe_size,
+ 		    const struct iomap_ops *ops);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  int dax_delete_mapping_entry(struct address_space *mapping, pgoff_t index);
 -int dax_invalidate_mapping_entry(struct address_space *mapping, pgoff_t index);
 -int dax_invalidate_mapping_entry_sync(struct address_space *mapping,
 -				      pgoff_t index);
  void dax_wake_mapping_entry_waiter(struct address_space *mapping,
  		pgoff_t index, void *entry, bool wake_all);
  
diff --cc include/linux/mm.h
index 3416fff96060,c65aa43b5712..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -218,13 -276,25 +218,35 @@@ extern unsigned int kobjsize(const voi
  extern pgprot_t protection_map[16];
  
  #define FAULT_FLAG_WRITE	0x01	/* Fault was a write access */
++<<<<<<< HEAD
 +#define FAULT_FLAG_NONLINEAR	0x02	/* Fault was via a nonlinear mapping */
 +#define FAULT_FLAG_MKWRITE	0x04	/* Fault was mkwrite of existing pte */
 +#define FAULT_FLAG_ALLOW_RETRY	0x08	/* Retry fault if blocking */
 +#define FAULT_FLAG_RETRY_NOWAIT	0x10	/* Don't drop mmap_sem and wait when retrying */
 +#define FAULT_FLAG_KILLABLE	0x20	/* The fault task is in SIGKILL killable region */
 +#define FAULT_FLAG_TRIED	0x40	/* second try */
 +#define FAULT_FLAG_USER		0x80	/* The fault originated in userspace */
++=======
+ #define FAULT_FLAG_MKWRITE	0x02	/* Fault was mkwrite of existing pte */
+ #define FAULT_FLAG_ALLOW_RETRY	0x04	/* Retry fault if blocking */
+ #define FAULT_FLAG_RETRY_NOWAIT	0x08	/* Don't drop mmap_sem and wait when retrying */
+ #define FAULT_FLAG_KILLABLE	0x10	/* The fault task is in SIGKILL killable region */
+ #define FAULT_FLAG_TRIED	0x20	/* Second try */
+ #define FAULT_FLAG_USER		0x40	/* The fault originated in userspace */
+ #define FAULT_FLAG_REMOTE	0x80	/* faulting for non current tsk/mm */
+ #define FAULT_FLAG_INSTRUCTION  0x100	/* The fault was during an instruction fetch */
+ 
+ #define FAULT_FLAG_TRACE \
+ 	{ FAULT_FLAG_WRITE,		"WRITE" }, \
+ 	{ FAULT_FLAG_MKWRITE,		"MKWRITE" }, \
+ 	{ FAULT_FLAG_ALLOW_RETRY,	"ALLOW_RETRY" }, \
+ 	{ FAULT_FLAG_RETRY_NOWAIT,	"RETRY_NOWAIT" }, \
+ 	{ FAULT_FLAG_KILLABLE,		"KILLABLE" }, \
+ 	{ FAULT_FLAG_TRIED,		"TRIED" }, \
+ 	{ FAULT_FLAG_USER,		"USER" }, \
+ 	{ FAULT_FLAG_REMOTE,		"REMOTE" }, \
+ 	{ FAULT_FLAG_INSTRUCTION,	"INSTRUCTION" }
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  
  /*
   * vm_fault is filled by the the pagefault handler and passed to the vma's
@@@ -261,7 -359,11 +290,15 @@@ enum page_entry_size 
  struct vm_operations_struct {
  	void (*open)(struct vm_area_struct * area);
  	void (*close)(struct vm_area_struct * area);
++<<<<<<< HEAD
 +	int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);
++=======
+ 	int (*mremap)(struct vm_area_struct * area);
+ 	int (*fault)(struct vm_fault *vmf);
+ 	int (*huge_fault)(struct vm_fault *vmf, enum page_entry_size pe_size);
+ 	void (*map_pages)(struct vm_fault *vmf,
+ 			pgoff_t start_pgoff, pgoff_t end_pgoff);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  
  	/* notification that a previously read-only page is about to become
  	 * writable, if an error is returned it will cause a SIGBUS */
diff --cc mm/memory.c
index 14270187456b,6040b74d02a2..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -3173,24 -3484,55 +3173,72 @@@ out
  	return 0;
  }
  
 -static int create_huge_pmd(struct vm_fault *vmf)
 +static int create_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
 +			unsigned long address, pmd_t *pmd, unsigned int flags)
  {
++<<<<<<< HEAD
 +	if (!vma->vm_ops)
 +		return do_huge_pmd_anonymous_page(mm, vma, address, pmd, flags);
 +	if ((vma->vm_flags2 & VM_PMD_FAULT) && vma->vm_ops->pmd_fault)
 +		return vma->vm_ops->pmd_fault(vma, address, pmd, flags);
++=======
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return do_huge_pmd_anonymous_page(vmf);
+ 	if (vmf->vma->vm_ops->huge_fault)
+ 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  	return VM_FAULT_FALLBACK;
  }
  
 -static int wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)
 +static int wp_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
 +			unsigned long address, pmd_t *pmd, pmd_t orig_pmd,
 +			unsigned int flags)
  {
++<<<<<<< HEAD
 +	if (!vma->vm_ops)
 +		return do_huge_pmd_wp_page(mm, vma, address, pmd, orig_pmd);
 +	if ((vma->vm_flags2 & VM_PMD_FAULT) && vma->vm_ops->pmd_fault)
 +		return vma->vm_ops->pmd_fault(vma, address, pmd, flags);
++=======
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return do_huge_pmd_wp_page(vmf, orig_pmd);
+ 	if (vmf->vma->vm_ops->huge_fault)
+ 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
+ 
+ 	/* COW handled on pte level: split pmd */
+ 	VM_BUG_ON_VMA(vmf->vma->vm_flags & VM_SHARED, vmf->vma);
+ 	__split_huge_pmd(vmf->vma, vmf->pmd, vmf->address, false, NULL);
+ 
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static inline bool vma_is_accessible(struct vm_area_struct *vma)
+ {
+ 	return vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE);
+ }
+ 
+ static int create_huge_pud(struct vm_fault *vmf)
+ {
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	/* No support for anonymous transparent PUD pages yet */
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return VM_FAULT_FALLBACK;
+ 	if (vmf->vma->vm_ops->huge_fault)
+ 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PUD);
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
+ {
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	/* No support for anonymous transparent PUD pages yet */
+ 	if (vma_is_anonymous(vmf->vma))
+ 		return VM_FAULT_FALLBACK;
+ 	if (vmf->vma->vm_ops->huge_fault)
+ 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PUD);
+ #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  	return VM_FAULT_FALLBACK;
  }
  
@@@ -3263,52 -3635,44 +3311,61 @@@ unlock
  
  /*
   * By the time we get here, we already hold the mm semaphore
 - *
 - * The mmap_sem may have been released depending on flags and our
 - * return value.  See filemap_fault() and __lock_page_or_retry().
   */
 -static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 -		unsigned int flags)
 +static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 +			     unsigned long address, unsigned int flags)
  {
 -	struct vm_fault vmf = {
 -		.vma = vma,
 -		.address = address & PAGE_MASK,
 -		.flags = flags,
 -		.pgoff = linear_page_index(vma, address),
 -		.gfp_mask = __get_fault_gfp_mask(vma),
 -	};
 -	struct mm_struct *mm = vma->vm_mm;
  	pgd_t *pgd;
 -	int ret;
 +	pud_t *pud;
 +	pmd_t *pmd;
 +	pte_t *pte;
  
 -	pgd = pgd_offset(mm, address);
 +	if (unlikely(is_vm_hugetlb_page(vma)))
 +		return hugetlb_fault(mm, vma, address, flags);
  
 -	vmf.pud = pud_alloc(mm, pgd, address);
 -	if (!vmf.pud)
 +	pgd = pgd_offset(mm, address);
 +	pud = pud_alloc(mm, pgd, address);
 +	if (!pud)
 +		return VM_FAULT_OOM;
++<<<<<<< HEAD
 +	pmd = pmd_alloc(mm, pud, address);
 +	if (!pmd)
  		return VM_FAULT_OOM;
 +	if (pmd_none(*pmd) && transparent_hugepage_enabled(vma)) {
 +		int ret = create_huge_pmd(mm, vma, address, pmd, flags);
++=======
+ 	if (pud_none(*vmf.pud) && transparent_hugepage_enabled(vma)) {
+ 		ret = create_huge_pud(&vmf);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  		if (!(ret & VM_FAULT_FALLBACK))
  			return ret;
  	} else {
 -		pud_t orig_pud = *vmf.pud;
 +		pmd_t orig_pmd = *pmd;
 +		int ret;
  
  		barrier();
 -		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {
 +		if (pmd_trans_huge(orig_pmd)) {
  			unsigned int dirty = flags & FAULT_FLAG_WRITE;
  
++<<<<<<< HEAD
 +			/*
 +			 * If the pmd is splitting, return and retry the
 +			 * the fault.  Alternative: wait until the split
 +			 * is done, and goto retry.
 +			 */
 +			if (pmd_trans_splitting(orig_pmd))
 +				return 0;
 +
 +			if (pmd_numa(orig_pmd))
 +				return do_huge_pmd_numa_page(mm, vma, address,
 +							     orig_pmd, pmd);
++=======
+ 			/* NUMA case for anonymous PUDs would go here */
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  
 -			if (dirty && !pud_write(orig_pud)) {
 -				ret = wp_huge_pud(&vmf, orig_pud);
 +			if (dirty && !pmd_write(orig_pmd)) {
 +				ret = wp_huge_pmd(mm, vma, address, pmd,
 +							orig_pmd, flags);
  				if (!(ret & VM_FAULT_FALLBACK))
  					return ret;
  			} else {
@@@ -3319,40 -3682,44 +3376,68 @@@
  		}
  	}
  
 -	vmf.pmd = pmd_alloc(mm, vmf.pud, address);
 -	if (!vmf.pmd)
 +	/*
 +	 * Use __pte_alloc instead of pte_alloc_map, because we can't
 +	 * run pte_offset_map on the pmd, if an huge pmd could
 +	 * materialize from under us from a different thread.
 +	 */
 +	if (unlikely(pmd_none(*pmd)) &&
 +	    unlikely(__pte_alloc(mm, vma, pmd, address)))
  		return VM_FAULT_OOM;
++<<<<<<< HEAD
 +	/*
 +	 * If a huge pmd materialized under us just retry later.  Use
 +	 * pmd_trans_unstable() instead of pmd_trans_huge() to ensure the pmd
 +	 * didn't become pmd_trans_huge under us and then back to pmd_none, as
 +	 * a result of MADV_DONTNEED running immediately after a huge pmd fault
 +	 * in a different thread of this mm, in turn leading to a misleading
 +	 * pmd_trans_huge() retval.  All we have to ensure is that it is a
 +	 * regular pmd that we can walk with pte_offset_map() and we can do that
 +	 * through an atomic read in C, which is what pmd_trans_unstable()
 +	 * provides.
 +	 */
 +	if (unlikely(pmd_trans_unstable(pmd)))
 +		return 0;
 +	/*
 +	 * A regular pmd is established and it can't morph into a huge pmd
 +	 * from under us anymore at this point because we hold the mmap_sem
 +	 * read mode and khugepaged takes it in write mode. So now it's
 +	 * safe to run pte_offset_map().
 +	 */
 +	pte = pte_offset_map(pmd, address);
 +
 +	return handle_pte_fault(mm, vma, address, pte, pmd, flags);
++=======
+ 	if (pmd_none(*vmf.pmd) && transparent_hugepage_enabled(vma)) {
+ 		ret = create_huge_pmd(&vmf);
+ 		if (!(ret & VM_FAULT_FALLBACK))
+ 			return ret;
+ 	} else {
+ 		pmd_t orig_pmd = *vmf.pmd;
+ 
+ 		barrier();
+ 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
+ 			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
+ 				return do_huge_pmd_numa_page(&vmf, orig_pmd);
+ 
+ 			if ((vmf.flags & FAULT_FLAG_WRITE) &&
+ 					!pmd_write(orig_pmd)) {
+ 				ret = wp_huge_pmd(&vmf, orig_pmd);
+ 				if (!(ret & VM_FAULT_FALLBACK))
+ 					return ret;
+ 			} else {
+ 				huge_pmd_set_accessed(&vmf, orig_pmd);
+ 				return 0;
+ 			}
+ 		}
+ 	}
+ 
+ 	return handle_pte_fault(&vmf);
++>>>>>>> c791ace1e747 (mm: replace FAULT_FLAG_SIZE with parameter to huge_fault)
  }
  
 -/*
 - * By the time we get here, we already hold the mm semaphore
 - *
 - * The mmap_sem may have been released depending on flags and our
 - * return value.  See filemap_fault() and __lock_page_or_retry().
 - */
 -int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 -		unsigned int flags)
 +int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 +		    unsigned long address, unsigned int flags)
  {
  	int ret;
  
* Unmerged path drivers/dax/dax.c
* Unmerged path fs/dax.c
* Unmerged path fs/ext2/file.c
* Unmerged path fs/ext4/file.c
* Unmerged path fs/xfs/xfs_file.c
* Unmerged path include/linux/dax.h
* Unmerged path include/linux/mm.h
* Unmerged path mm/memory.c
