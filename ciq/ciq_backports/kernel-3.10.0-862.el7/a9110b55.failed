x86/intel_rdt: Modify the intel_pqr_state for better performance

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] intel_rdt: Modify the intel_pqr_state for better performance (Jiri Olsa) [1457533]
Rebuild_FUZZ: 96.77%
commit-author Vikas Shivappa <vikas.shivappa@linux.intel.com>
commit a9110b552d44fedbd1221eb0e5bde81da32d9350
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/a9110b55.failed

Currently we have pqr_state and rdt_default_state which store the cached
CLOSID/RMIDs and the user configured cpu default values respectively. We
touch both of these during context switch. Put all of them in one
structure so that we can spare a cache line.

	Reported-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: ravi.v.shankar@intel.com
	Cc: tony.luck@intel.com
	Cc: fenghua.yu@intel.com
	Cc: peterz@infradead.org
	Cc: eranian@google.com
	Cc: sai.praneeth.prakhya@intel.com
	Cc: ak@linux.intel.com
	Cc: davidcc@google.com
Link: http://lkml.kernel.org/r/1502304395-7166-3-git-send-email-vikas.shivappa@linux.intel.com

(cherry picked from commit a9110b552d44fedbd1221eb0e5bde81da32d9350)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/intel_rdt_sched.h
#	arch/x86/kernel/cpu/intel_rdt.c
#	arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
diff --cc arch/x86/kernel/cpu/intel_rdt.c
index ad087dd4421e,97c8d8321e04..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt.c
+++ b/arch/x86/kernel/cpu/intel_rdt.c
@@@ -35,10 -39,14 +35,13 @@@
  /* Mutex to protect rdtgroup access. */
  DEFINE_MUTEX(rdtgroup_mutex);
  
 -/*
 - * The cached intel_pqr_state is strictly per CPU and can never be
 - * updated from a remote CPU. Functions which modify the state
 - * are called with interrupts disabled and no preemption, which
 - * is sufficient for the protection.
 - */
 -DEFINE_PER_CPU(struct intel_pqr_state, pqr_state);
 +DEFINE_PER_CPU_READ_MOSTLY(int, cpu_closid);
  
++<<<<<<< HEAD
 +#define domain_init(id) LIST_HEAD_INIT(rdt_resources_all[id].domains)
 +
++=======
++>>>>>>> a9110b552d44 (x86/intel_rdt: Modify the intel_pqr_state for better performance)
  /*
   * Used to store the max resource name width and max resource data width
   * to display the schemata in a tabular format
@@@ -357,12 -548,14 +360,20 @@@ static void clear_closid(int cpu
  {
  	struct intel_pqr_state *state = this_cpu_ptr(&pqr_state);
  
++<<<<<<< HEAD
 +	per_cpu(cpu_closid, cpu) = 0;
 +	state->closid = 0;
 +	wrmsr(MSR_IA32_PQR_ASSOC, state->rmid, 0);
++=======
+ 	state->default_closid = 0;
+ 	state->default_rmid = 0;
+ 	state->cur_closid = 0;
+ 	state->cur_rmid = 0;
+ 	wrmsr(IA32_PQR_ASSOC, 0, 0);
++>>>>>>> a9110b552d44 (x86/intel_rdt: Modify the intel_pqr_state for better performance)
  }
  
 -static int intel_rdt_online_cpu(unsigned int cpu)
 +static int intel_rdt_online_cpu(unsigned int cpu, bool notifier)
  {
  	struct rdt_resource *r;
  
diff --cc arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
index 1c3603d97e9d,86a69794d7e4..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
+++ b/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
@@@ -202,13 -194,18 +202,23 @@@ static int rdtgroup_cpus_show(struct ke
  /*
   * This is safe against intel_rdt_sched_in() called from __switch_to()
   * because __switch_to() is executed with interrupts disabled. A local call
 - * from update_closid_rmid() is proteced against __switch_to() because
 + * from rdt_update_closid() is proteced against __switch_to() because
   * preemption is disabled.
   */
 -static void update_cpu_closid_rmid(void *info)
 +static void rdt_update_cpu_closid(void *closid)
  {
++<<<<<<< HEAD
 +	if (closid)
 +		this_cpu_write(cpu_closid, *(int *)closid);
++=======
+ 	struct rdtgroup *r = info;
+ 
+ 	if (r) {
+ 		this_cpu_write(pqr_state.default_closid, r->closid);
+ 		this_cpu_write(pqr_state.default_rmid, r->mon.rmid);
+ 	}
+ 
++>>>>>>> a9110b552d44 (x86/intel_rdt: Modify the intel_pqr_state for better performance)
  	/*
  	 * We cannot unconditionally write the MSR because the current
  	 * executing task might have its own closid selected. Just reuse
@@@ -1009,11 -1681,137 +1019,139 @@@ out_unlock
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * We allow creating mon groups only with in a directory called "mon_groups"
+  * which is present in every ctrl_mon group. Check if this is a valid
+  * "mon_groups" directory.
+  *
+  * 1. The directory should be named "mon_groups".
+  * 2. The mon group itself should "not" be named "mon_groups".
+  *   This makes sure "mon_groups" directory always has a ctrl_mon group
+  *   as parent.
+  */
+ static bool is_mon_groups(struct kernfs_node *kn, const char *name)
+ {
+ 	return (!strcmp(kn->name, "mon_groups") &&
+ 		strcmp(name, "mon_groups"));
+ }
+ 
+ static int rdtgroup_mkdir(struct kernfs_node *parent_kn, const char *name,
+ 			  umode_t mode)
+ {
+ 	/* Do not accept '\n' to avoid unparsable situation. */
+ 	if (strchr(name, '\n'))
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * If the parent directory is the root directory and RDT
+ 	 * allocation is supported, add a control and monitoring
+ 	 * subdirectory
+ 	 */
+ 	if (rdt_alloc_capable && parent_kn == rdtgroup_default.kn)
+ 		return rdtgroup_mkdir_ctrl_mon(parent_kn, parent_kn, name, mode);
+ 
+ 	/*
+ 	 * If RDT monitoring is supported and the parent directory is a valid
+ 	 * "mon_groups" directory, add a monitoring subdirectory.
+ 	 */
+ 	if (rdt_mon_capable && is_mon_groups(parent_kn, name))
+ 		return rdtgroup_mkdir_mon(parent_kn, parent_kn->parent, name, mode);
+ 
+ 	return -EPERM;
+ }
+ 
+ static int rdtgroup_rmdir_mon(struct kernfs_node *kn, struct rdtgroup *rdtgrp,
+ 			      cpumask_var_t tmpmask)
+ {
+ 	struct rdtgroup *prdtgrp = rdtgrp->mon.parent;
+ 	int cpu;
+ 
+ 	/* Give any tasks back to the parent group */
+ 	rdt_move_group_tasks(rdtgrp, prdtgrp, tmpmask);
+ 
+ 	/* Update per cpu rmid of the moved CPUs first */
+ 	for_each_cpu(cpu, &rdtgrp->cpu_mask)
+ 		per_cpu(pqr_state.default_rmid, cpu) = prdtgrp->mon.rmid;
+ 	/*
+ 	 * Update the MSR on moved CPUs and CPUs which have moved
+ 	 * task running on them.
+ 	 */
+ 	cpumask_or(tmpmask, tmpmask, &rdtgrp->cpu_mask);
+ 	update_closid_rmid(tmpmask, NULL);
+ 
+ 	rdtgrp->flags = RDT_DELETED;
+ 	free_rmid(rdtgrp->mon.rmid);
+ 
+ 	/*
+ 	 * Remove the rdtgrp from the parent ctrl_mon group's list
+ 	 */
+ 	WARN_ON(list_empty(&prdtgrp->mon.crdtgrp_list));
+ 	list_del(&rdtgrp->mon.crdtgrp_list);
+ 
+ 	/*
+ 	 * one extra hold on this, will drop when we kfree(rdtgrp)
+ 	 * in rdtgroup_kn_unlock()
+ 	 */
+ 	kernfs_get(kn);
+ 	kernfs_remove(rdtgrp->kn);
+ 
+ 	return 0;
+ }
+ 
+ static int rdtgroup_rmdir_ctrl(struct kernfs_node *kn, struct rdtgroup *rdtgrp,
+ 			       cpumask_var_t tmpmask)
+ {
+ 	int cpu;
+ 
+ 	/* Give any tasks back to the default group */
+ 	rdt_move_group_tasks(rdtgrp, &rdtgroup_default, tmpmask);
+ 
+ 	/* Give any CPUs back to the default group */
+ 	cpumask_or(&rdtgroup_default.cpu_mask,
+ 		   &rdtgroup_default.cpu_mask, &rdtgrp->cpu_mask);
+ 
+ 	/* Update per cpu closid and rmid of the moved CPUs first */
+ 	for_each_cpu(cpu, &rdtgrp->cpu_mask) {
+ 		per_cpu(pqr_state.default_closid, cpu) = rdtgroup_default.closid;
+ 		per_cpu(pqr_state.default_rmid, cpu) = rdtgroup_default.mon.rmid;
+ 	}
+ 
+ 	/*
+ 	 * Update the MSR on moved CPUs and CPUs which have moved
+ 	 * task running on them.
+ 	 */
+ 	cpumask_or(tmpmask, tmpmask, &rdtgrp->cpu_mask);
+ 	update_closid_rmid(tmpmask, NULL);
+ 
+ 	rdtgrp->flags = RDT_DELETED;
+ 	closid_free(rdtgrp->closid);
+ 	free_rmid(rdtgrp->mon.rmid);
+ 
+ 	/*
+ 	 * Free all the child monitor group rmids.
+ 	 */
+ 	free_all_child_rdtgrp(rdtgrp);
+ 
+ 	list_del(&rdtgrp->rdtgroup_list);
+ 
+ 	/*
+ 	 * one extra hold on this, will drop when we kfree(rdtgrp)
+ 	 * in rdtgroup_kn_unlock()
+ 	 */
+ 	kernfs_get(kn);
+ 	kernfs_remove(rdtgrp->kn);
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> a9110b552d44 (x86/intel_rdt: Modify the intel_pqr_state for better performance)
  static int rdtgroup_rmdir(struct kernfs_node *kn)
  {
 -	struct kernfs_node *parent_kn = kn->parent;
 +	int ret, cpu, closid = rdtgroup_default.closid;
  	struct rdtgroup *rdtgrp;
  	cpumask_var_t tmpmask;
 -	int ret = 0;
  
  	if (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))
  		return -ENOMEM;
* Unmerged path arch/x86/include/asm/intel_rdt_sched.h
* Unmerged path arch/x86/include/asm/intel_rdt_sched.h
* Unmerged path arch/x86/kernel/cpu/intel_rdt.c
* Unmerged path arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
