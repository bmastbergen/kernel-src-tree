x86: kvm: mmu: make spte mmio mask more explicit

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] kvm: mmu: make spte mmio mask more explicit (Paolo Bonzini) [1469685]
Rebuild_FUZZ: 94.51%
commit-author Peter Feiner <pfeiner@google.com>
commit dcdca5fed5f6ef2521f927ba3b5cd6b328054be1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/dcdca5fe.failed

Specify both a mask (i.e., bits to consider) and a value (i.e.,
pattern of bits that indicates a special PTE) for mmio SPTEs. On
Intel, this lets us pack even more information into the
(SPTE_SPECIAL_MASK | EPT_VMX_RWX_MASK) mask we use for access
tracking liberating all (SPTE_SPECIAL_MASK | (non-misconfigured-RWX))
values.

	Signed-off-by: Peter Feiner <pfeiner@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit dcdca5fed5f6ef2521f927ba3b5cd6b328054be1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu.c
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/mmu.c
index 588adec09b8c,10b3cfc7b411..000000000000
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -175,14 -183,36 +175,21 @@@ static u64 __read_mostly shadow_user_ma
  static u64 __read_mostly shadow_accessed_mask;
  static u64 __read_mostly shadow_dirty_mask;
  static u64 __read_mostly shadow_mmio_mask;
+ static u64 __read_mostly shadow_mmio_value;
  static u64 __read_mostly shadow_present_mask;
  
 -/*
 - * The mask/value to distinguish a PTE that has been marked not-present for
 - * access tracking purposes.
 - * The mask would be either 0 if access tracking is disabled, or
 - * SPTE_SPECIAL_MASK|VMX_EPT_RWX_MASK if access tracking is enabled.
 - */
 -static u64 __read_mostly shadow_acc_track_mask;
 -static const u64 shadow_acc_track_value = SPTE_SPECIAL_MASK;
 -
 -/*
 - * The mask/shift to use for saving the original R/X bits when marking the PTE
 - * as not-present for access tracking purposes. We do not save the W bit as the
 - * PTEs being access tracked also need to be dirty tracked, so the W bit will be
 - * restored only when a write is attempted to the page.
 - */
 -static const u64 shadow_acc_track_saved_bits_mask = PT64_EPT_READABLE_MASK |
 -						    PT64_EPT_EXECUTABLE_MASK;
 -static const u64 shadow_acc_track_saved_bits_shift = PT64_SECOND_AVAIL_BITS_SHIFT;
 -
  static void mmu_spte_set(u64 *sptep, u64 spte);
  static void mmu_free_roots(struct kvm_vcpu *vcpu);
  
- void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask)
+ void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask, u64 mmio_value)
  {
++<<<<<<< HEAD
 +	shadow_mmio_mask = mmio_mask;
++=======
+ 	BUG_ON((mmio_mask & mmio_value) != mmio_value);
+ 	shadow_mmio_value = mmio_value | SPTE_SPECIAL_MASK;
+ 	shadow_mmio_mask = mmio_mask | SPTE_SPECIAL_MASK;
++>>>>>>> dcdca5fed5f6 (x86: kvm: mmu: make spte mmio mask more explicit)
  }
  EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
  
diff --cc arch/x86/kvm/vmx.c
index 8426c9cb3bfe,b102a864e61f..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -4832,9 -5162,8 +4832,13 @@@ static void ept_set_mmio_spte_mask(void
  	/*
  	 * EPT Misconfigurations can be generated if the value of bits 2:0
  	 * of an EPT paging-structure entry is 110b (write/execute).
 +	 * Also, special bit (62) is set to quickly identify mmio spte.
  	 */
++<<<<<<< HEAD
 +	kvm_mmu_set_mmio_spte_mask(SPTE_SPECIAL_MASK |
++=======
+ 	kvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,
++>>>>>>> dcdca5fed5f6 (x86: kvm: mmu: make spte mmio mask more explicit)
  				   VMX_EPT_MISCONFIG_WX_VALUE);
  }
  
* Unmerged path arch/x86/kvm/mmu.c
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 67af5790b4c5..c996ccd4fea9 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -51,7 +51,7 @@ static inline u64 rsvd_bits(int s, int e)
 	return ((1ULL << (e - s + 1)) - 1) << s;
 }
 
-void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask);
+void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask, u64 mmio_value);
 
 void
 reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu, struct kvm_mmu *context);
* Unmerged path arch/x86/kvm/vmx.c
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 99e230533b87..7c9503601453 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5828,7 +5828,7 @@ static void kvm_set_mmio_spte_mask(void)
 		mask &= ~1ull;
 #endif
 
-	kvm_mmu_set_mmio_spte_mask(mask);
+	kvm_mmu_set_mmio_spte_mask(mask, mask);
 }
 
 #ifdef CONFIG_X86_64
