block: Introduce blk_get_request_flags()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [block] Introduce blk_get_request_flags() (Ming Lei) [1491296]
Rebuild_FUZZ: 90.41%
commit-author Bart Van Assche <bart.vanassche@wdc.com>
commit 6a15674d1e90917f1723a814e2e8c949000440f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6a15674d.failed

A side effect of this patch is that the GFP mask that is passed to
several allocation functions in the legacy block layer is changed
from GFP_KERNEL into __GFP_DIRECT_RECLAIM.

	Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Tested-by: Martin Steigerwald <martin@lichtvoll.de>
	Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Ming Lei <ming.lei@redhat.com>
	Cc: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 6a15674d1e90917f1723a814e2e8c949000440f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	include/linux/blkdev.h
diff --cc block/blk-core.c
index df55e6267498,0f7093dfc010..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -1038,9 -1158,9 +1038,9 @@@ static struct io_context *rq_ioc(struc
  /**
   * __get_request - get a free request
   * @rl: request list to allocate from
 - * @op: operation and flags
 + * @rw_flags: RW and SYNC flags
   * @bio: bio to allocate request for (can be %NULL)
-  * @gfp_mask: allocation mask
+  * @flags: BLQ_MQ_REQ_* flags
   *
   * Get a free request from @q.  This function may fail under memory
   * pressure or if @q is dead.
@@@ -1049,16 -1169,21 +1049,29 @@@
   * Returns ERR_PTR on failure, with @q->queue_lock held.
   * Returns request pointer on success, with @q->queue_lock *not held*.
   */
++<<<<<<< HEAD
 +static struct request *__get_request(struct request_list *rl, int rw_flags,
 +				     struct bio *bio, gfp_t gfp_mask)
++=======
+ static struct request *__get_request(struct request_list *rl, unsigned int op,
+ 				     struct bio *bio, unsigned int flags)
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  {
  	struct request_queue *q = rl->q;
  	struct request *rq;
  	struct elevator_type *et = q->elevator->type;
  	struct io_context *ioc = rq_ioc(bio);
  	struct io_cq *icq = NULL;
 -	const bool is_sync = op_is_sync(op);
 +	const bool is_sync = rw_is_sync(rw_flags) != 0;
  	int may_queue;
++<<<<<<< HEAD
++=======
+ 	gfp_t gfp_mask = flags & BLK_MQ_REQ_NOWAIT ? GFP_ATOMIC :
+ 			 __GFP_DIRECT_RECLAIM;
+ 	req_flags_t rq_flags = RQF_ALLOCED;
+ 
+ 	lockdep_assert_held(q->queue_lock);
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  
  	if (unlikely(blk_queue_dying(q)))
  		return ERR_PTR(-ENODEV);
@@@ -1215,32 -1339,40 +1228,50 @@@ rq_starved
  /**
   * get_request - get a free request
   * @q: request_queue to allocate request from
 - * @op: operation and flags
 + * @rw_flags: RW and SYNC flags
   * @bio: bio to allocate request for (can be %NULL)
-  * @gfp_mask: allocation mask
+  * @flags: BLK_MQ_REQ_* flags.
   *
 - * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,
 - * this function keeps retrying under memory pressure and fails iff @q is dead.
 + * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this
 + * function keeps retrying under memory pressure and fails iff @q is dead.
   *
   * Must be called with @q->queue_lock held and,
   * Returns ERR_PTR on failure, with @q->queue_lock held.
   * Returns request pointer on success, with @q->queue_lock *not held*.
   */
++<<<<<<< HEAD
 +static struct request *get_request(struct request_queue *q, int rw_flags,
 +				   struct bio *bio, gfp_t gfp_mask)
++=======
+ static struct request *get_request(struct request_queue *q, unsigned int op,
+ 				   struct bio *bio, unsigned int flags)
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  {
 -	const bool is_sync = op_is_sync(op);
 +	const bool is_sync = rw_is_sync(rw_flags) != 0;
  	DEFINE_WAIT(wait);
  	struct request_list *rl;
  	struct request *rq;
  
 -	lockdep_assert_held(q->queue_lock);
 -	WARN_ON_ONCE(q->mq_ops);
 -
  	rl = blk_get_rl(q, bio);	/* transferred to @rq on success */
  retry:
++<<<<<<< HEAD
 +	rq = __get_request(rl, rw_flags, bio, gfp_mask);
 +	if (!IS_ERR(rq))
 +		return rq;
 +
 +	if (!(gfp_mask & __GFP_WAIT) || unlikely(blk_queue_dying(q))) {
++=======
+ 	rq = __get_request(rl, op, bio, flags);
+ 	if (!IS_ERR(rq))
+ 		return rq;
+ 
+ 	if (op & REQ_NOWAIT) {
+ 		blk_put_rl(rl);
+ 		return ERR_PTR(-EAGAIN);
+ 	}
+ 
+ 	if ((flags & BLK_MQ_REQ_NOWAIT) || unlikely(blk_queue_dying(q))) {
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  		blk_put_rl(rl);
  		return rq;
  	}
@@@ -1267,33 -1399,72 +1298,85 @@@
  	goto retry;
  }
  
++<<<<<<< HEAD
 +static struct request *blk_old_get_request(struct request_queue *q, int rw,
 +		gfp_t gfp_mask)
 +{
 +	struct request *rq;
++=======
+ /* flags: BLK_MQ_REQ_PREEMPT and/or BLK_MQ_REQ_NOWAIT. */
+ static struct request *blk_old_get_request(struct request_queue *q,
+ 					   unsigned int op, unsigned int flags)
+ {
+ 	struct request *rq;
+ 	gfp_t gfp_mask = flags & BLK_MQ_REQ_NOWAIT ? GFP_ATOMIC :
+ 			 __GFP_DIRECT_RECLAIM;
+ 	int ret = 0;
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  
 -	WARN_ON_ONCE(q->mq_ops);
 +	BUG_ON(rw != READ && rw != WRITE);
  
  	/* create ioc upfront */
  	create_io_context(gfp_mask, q->node);
  
 -	ret = blk_queue_enter(q, !(gfp_mask & __GFP_DIRECT_RECLAIM) ||
 -			      (op & REQ_NOWAIT));
 -	if (ret)
 -		return ERR_PTR(ret);
  	spin_lock_irq(q->queue_lock);
++<<<<<<< HEAD
 +	rq = get_request(q, rw, NULL, gfp_mask);
 +	if (IS_ERR(rq))
++=======
+ 	rq = get_request(q, op, NULL, flags);
+ 	if (IS_ERR(rq)) {
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  		spin_unlock_irq(q->queue_lock);
 -		blk_queue_exit(q);
 -		return rq;
 -	}
 -
  	/* q->queue_lock is unlocked at this point */
 -	rq->__data_len = 0;
 -	rq->__sector = (sector_t) -1;
 -	rq->bio = rq->biotail = NULL;
 +
  	return rq;
  }
  
++<<<<<<< HEAD
 +struct request *blk_get_request(struct request_queue *q, int rw, gfp_t gfp_mask)
 +{
 +	if (q->mq_ops)
 +		return blk_mq_alloc_request(q, rw,
 +			(gfp_mask & __GFP_WAIT) ?
 +				0 : BLK_MQ_REQ_NOWAIT);
 +	else
 +		return blk_old_get_request(q, rw, gfp_mask);
++=======
+ /**
+  * blk_get_request_flags - allocate a request
+  * @q: request queue to allocate a request for
+  * @op: operation (REQ_OP_*) and REQ_* flags, e.g. REQ_SYNC.
+  * @flags: BLK_MQ_REQ_* flags, e.g. BLK_MQ_REQ_NOWAIT.
+  */
+ struct request *blk_get_request_flags(struct request_queue *q, unsigned int op,
+ 				      unsigned int flags)
+ {
+ 	struct request *req;
+ 
+ 	WARN_ON_ONCE(op & REQ_NOWAIT);
+ 	WARN_ON_ONCE(flags & ~BLK_MQ_REQ_NOWAIT);
+ 
+ 	if (q->mq_ops) {
+ 		req = blk_mq_alloc_request(q, op, flags);
+ 		if (!IS_ERR(req) && q->mq_ops->initialize_rq_fn)
+ 			q->mq_ops->initialize_rq_fn(req);
+ 	} else {
+ 		req = blk_old_get_request(q, op, flags);
+ 		if (!IS_ERR(req) && q->initialize_rq_fn)
+ 			q->initialize_rq_fn(req);
+ 	}
+ 
+ 	return req;
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
+ }
+ EXPORT_SYMBOL(blk_get_request_flags);
+ 
+ struct request *blk_get_request(struct request_queue *q, unsigned int op,
+ 				gfp_t gfp_mask)
+ {
+ 	return blk_get_request_flags(q, op, gfp_mask & __GFP_DIRECT_RECLAIM ?
+ 				     0 : BLK_MQ_REQ_NOWAIT);
  }
  EXPORT_SYMBOL(blk_get_request);
  
@@@ -1744,9 -1890,16 +1827,14 @@@ get_rq
  	 * Grab a free request. This is might sleep but can not fail.
  	 * Returns with the queue unlocked.
  	 */
++<<<<<<< HEAD
 +	req = get_request(q, rw_flags, bio, GFP_NOIO);
++=======
+ 	blk_queue_enter_live(q);
+ 	req = get_request(q, bio->bi_opf, bio, 0);
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  	if (IS_ERR(req)) {
 -		blk_queue_exit(q);
 -		__wbt_done(q->rq_wb, wb_acct);
 -		if (PTR_ERR(req) == -ENOMEM)
 -			bio->bi_status = BLK_STS_RESOURCE;
 -		else
 -			bio->bi_status = BLK_STS_IOERR;
 -		bio_endio(bio);
 +		bio_endio(bio, PTR_ERR(req));	/* @q is dead */
  		goto out_unlock;
  	}
  
diff --cc include/linux/blkdev.h
index 2d7fe01dd7d2,1af5ddd0f631..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -851,30 -919,20 +851,38 @@@ static inline void rq_flush_dcache_page
  }
  #endif
  
 +#ifdef CONFIG_PRINTK
 +#define vfs_msg(sb, level, fmt, ...)				\
 +	__vfs_msg(sb, level, fmt, ##__VA_ARGS__)
 +#else
 +#define vfs_msg(sb, level, fmt, ...)				\
 +do {								\
 +	no_printk(fmt, ##__VA_ARGS__);				\
 +	__vfs_msg(sb, "", " ");					\
 +} while (0)
 +#endif
 +
  extern int blk_register_queue(struct gendisk *disk);
  extern void blk_unregister_queue(struct gendisk *disk);
 -extern blk_qc_t generic_make_request(struct bio *bio);
 -extern blk_qc_t direct_make_request(struct bio *bio);
 +extern void generic_make_request(struct bio *bio);
  extern void blk_rq_init(struct request_queue *q, struct request *rq);
 -extern void blk_init_request_from_bio(struct request *req, struct bio *bio);
  extern void blk_put_request(struct request *);
  extern void __blk_put_request(struct request_queue *, struct request *);
++<<<<<<< HEAD
 +extern struct request *blk_get_request(struct request_queue *, int, gfp_t);
 +extern struct request *blk_make_request(struct request_queue *, struct bio *,
 +					gfp_t);
 +extern void blk_rq_set_block_pc(struct request *);
++=======
+ extern struct request *blk_get_request_flags(struct request_queue *,
+ 					     unsigned int op,
+ 					     unsigned int flags);
+ extern struct request *blk_get_request(struct request_queue *, unsigned int op,
+ 				       gfp_t gfp_mask);
++>>>>>>> 6a15674d1e90 (block: Introduce blk_get_request_flags())
  extern void blk_requeue_request(struct request_queue *, struct request *);
 +extern void blk_add_request_payload(struct request *rq, struct page *page,
 +		int offset, unsigned int len);
  extern int blk_lld_busy(struct request_queue *q);
  extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
  			     struct bio_set *bs, gfp_t gfp_mask,
* Unmerged path block/blk-core.c
* Unmerged path include/linux/blkdev.h
