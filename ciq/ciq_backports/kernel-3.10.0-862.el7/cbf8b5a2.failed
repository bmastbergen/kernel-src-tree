x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] mm, x86/mce: Fix return type/value for memcpy_mcsafe() (Jeff Moyer) [1437205]
Rebuild_FUZZ: 96.43%
commit-author Tony Luck <tony.luck@intel.com>
commit cbf8b5a2b649a501758291cb4d4ba1e5711771ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/cbf8b5a2.failed

Returning a 'bool' was very unpopular. Doubly so because the
code was just wrong (returning zero for true, one for false;
great for shell programming, not so good for C).

Change return type to "int". Keep zero as the success indicator
because it matches other similar code and people may be more
comfortable writing:

	if (memcpy_mcsafe(to, from, count)) {
		printk("Sad panda, copy failed\n");
		...
	}

Make the failure return value -EFAULT for now.

Reported by: Mika Penttil√§ <mika.penttila@nextfour.com>
	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: mika.penttila@nextfour.com
Fixes: 92b0729c34ca ("x86/mm, x86/mce: Add memcpy_mcsafe()")
Link: http://lkml.kernel.org/r/695f14233fa7a54fcac4406c706d7fec228e3f4c.1457993040.git.tony.luck@intel.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit cbf8b5a2b649a501758291cb4d4ba1e5711771ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/string_64.h
#	arch/x86/lib/memcpy_64.S
diff --cc arch/x86/include/asm/string_64.h
index 19e2c468fc2c,90dbbd9666d4..000000000000
--- a/arch/x86/include/asm/string_64.h
+++ b/arch/x86/include/asm/string_64.h
@@@ -63,6 -65,32 +63,35 @@@ char *strcpy(char *dest, const char *sr
  char *strcat(char *dest, const char *src);
  int strcmp(const char *cs, const char *ct);
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_KASAN) && !defined(__SANITIZE_ADDRESS__)
+ 
+ /*
+  * For files that not instrumented (e.g. mm/slub.c) we
+  * should use not instrumented version of mem* functions.
+  */
+ 
+ #undef memcpy
+ #define memcpy(dst, src, len) __memcpy(dst, src, len)
+ #define memmove(dst, src, len) __memmove(dst, src, len)
+ #define memset(s, c, n) __memset(s, c, n)
+ #endif
+ 
+ /**
+  * memcpy_mcsafe - copy memory with indication if a machine check happened
+  *
+  * @dst:	destination address
+  * @src:	source address
+  * @cnt:	number of bytes to copy
+  *
+  * Low level memory copy function that catches machine checks
+  *
+  * Return 0 for success, -EFAULT for fail
+  */
+ int memcpy_mcsafe(void *dst, const void *src, size_t cnt);
+ 
++>>>>>>> cbf8b5a2b649 (x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe())
  #endif /* __KERNEL__ */
  
  #endif /* _ASM_X86_STRING_64_H */
diff --cc arch/x86/lib/memcpy_64.S
index 56313a326188,2ec0b0abbfaa..000000000000
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@@ -1,11 -1,19 +1,16 @@@
  /* Copyright 2002 Andi Kleen */
  
  #include <linux/linkage.h>
++<<<<<<< HEAD
 +
 +#include <asm/cpufeature.h>
 +#include <asm/dwarf2.h>
++=======
+ #include <asm/errno.h>
+ #include <asm/cpufeatures.h>
++>>>>>>> cbf8b5a2b649 (x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe())
  #include <asm/alternative-asm.h>
  
 -/*
 - * We build a jump to memcpy_orig by default which gets NOPped out on
 - * the majority of x86 CPUs which set REP_GOOD. In addition, CPUs which
 - * have the enhanced REP MOVSB/STOSB feature (ERMS), change those NOPs
 - * to a jmp to memcpy_erms which does the REP; MOVSB mem copy.
 - */
 -
 -.weak memcpy
 -
  /*
   * memcpy - Copy a memory block.
   *
@@@ -180,27 -177,121 +185,133 @@@ ENTRY(memcpy
  
  .Lend:
  	retq
++<<<<<<< HEAD
 +	CFI_ENDPROC
 +ENDPROC(memcpy)
 +ENDPROC(__memcpy)
++=======
+ ENDPROC(memcpy_orig)
+ 
+ #ifndef CONFIG_UML
+ /*
+  * memcpy_mcsafe - memory copy with machine check exception handling
+  * Note that we only catch machine checks when reading the source addresses.
+  * Writes to target are posted and don't generate machine checks.
+  */
+ ENTRY(memcpy_mcsafe)
+ 	cmpl $8, %edx
+ 	/* Less than 8 bytes? Go to byte copy loop */
+ 	jb .L_no_whole_words
+ 
+ 	/* Check for bad alignment of source */
+ 	testl $7, %esi
+ 	/* Already aligned */
+ 	jz .L_8byte_aligned
+ 
+ 	/* Copy one byte at a time until source is 8-byte aligned */
+ 	movl %esi, %ecx
+ 	andl $7, %ecx
+ 	subl $8, %ecx
+ 	negl %ecx
+ 	subl %ecx, %edx
+ .L_copy_leading_bytes:
+ 	movb (%rsi), %al
+ 	movb %al, (%rdi)
+ 	incq %rsi
+ 	incq %rdi
+ 	decl %ecx
+ 	jnz .L_copy_leading_bytes
+ 
+ .L_8byte_aligned:
+ 	/* Figure out how many whole cache lines (64-bytes) to copy */
+ 	movl %edx, %ecx
+ 	andl $63, %edx
+ 	shrl $6, %ecx
+ 	jz .L_no_whole_cache_lines
+ 
+ 	/* Loop copying whole cache lines */
+ .L_cache_w0: movq (%rsi), %r8
+ .L_cache_w1: movq 1*8(%rsi), %r9
+ .L_cache_w2: movq 2*8(%rsi), %r10
+ .L_cache_w3: movq 3*8(%rsi), %r11
+ 	movq %r8, (%rdi)
+ 	movq %r9, 1*8(%rdi)
+ 	movq %r10, 2*8(%rdi)
+ 	movq %r11, 3*8(%rdi)
+ .L_cache_w4: movq 4*8(%rsi), %r8
+ .L_cache_w5: movq 5*8(%rsi), %r9
+ .L_cache_w6: movq 6*8(%rsi), %r10
+ .L_cache_w7: movq 7*8(%rsi), %r11
+ 	movq %r8, 4*8(%rdi)
+ 	movq %r9, 5*8(%rdi)
+ 	movq %r10, 6*8(%rdi)
+ 	movq %r11, 7*8(%rdi)
+ 	leaq 64(%rsi), %rsi
+ 	leaq 64(%rdi), %rdi
+ 	decl %ecx
+ 	jnz .L_cache_w0
+ 
+ 	/* Are there any trailing 8-byte words? */
+ .L_no_whole_cache_lines:
+ 	movl %edx, %ecx
+ 	andl $7, %edx
+ 	shrl $3, %ecx
+ 	jz .L_no_whole_words
+ 
+ 	/* Copy trailing words */
+ .L_copy_trailing_words:
+ 	movq (%rsi), %r8
+ 	mov %r8, (%rdi)
+ 	leaq 8(%rsi), %rsi
+ 	leaq 8(%rdi), %rdi
+ 	decl %ecx
+ 	jnz .L_copy_trailing_words
+ 
+ 	/* Any trailing bytes? */
+ .L_no_whole_words:
+ 	andl %edx, %edx
+ 	jz .L_done_memcpy_trap
+ 
+ 	/* Copy trailing bytes */
+ 	movl %edx, %ecx
+ .L_copy_trailing_bytes:
+ 	movb (%rsi), %al
+ 	movb %al, (%rdi)
+ 	incq %rsi
+ 	incq %rdi
+ 	decl %ecx
+ 	jnz .L_copy_trailing_bytes
+ 
+ 	/* Copy successful. Return zero */
+ .L_done_memcpy_trap:
+ 	xorq %rax, %rax
+ 	ret
+ ENDPROC(memcpy_mcsafe)
+ 
+ 	.section .fixup, "ax"
+ 	/* Return -EFAULT for any failure */
+ .L_memcpy_mcsafe_fail:
+ 	mov	$-EFAULT, %rax
+ 	ret
++>>>>>>> cbf8b5a2b649 (x86/mm, x86/mce: Fix return type/value for memcpy_mcsafe())
  
 +	/*
 +	 * Some CPUs are adding enhanced REP MOVSB/STOSB feature
 +	 * If the feature is supported, memcpy_c_e() is the first choice.
 +	 * If enhanced rep movsb copy is not available, use fast string copy
 +	 * memcpy_c() when possible. This is faster and code is simpler than
 +	 * original memcpy().
 +	 * Otherwise, original memcpy() is used.
 +	 * In .altinstructions section, ERMS feature is placed after REG_GOOD
 +         * feature to implement the right patch order.
 +	 *
 +	 * Replace only beginning, memcpy is used to apply alternatives,
 +	 * so it is silly to overwrite itself with nops - reboot is the
 +	 * only outcome...
 +	 */
 +	.section .altinstructions, "a"
 +	altinstruction_entry memcpy,.Lmemcpy_c,X86_FEATURE_REP_GOOD,\
 +			     .Lmemcpy_e-.Lmemcpy_c,.Lmemcpy_e-.Lmemcpy_c
 +	altinstruction_entry memcpy,.Lmemcpy_c_e,X86_FEATURE_ERMS, \
 +			     .Lmemcpy_e_e-.Lmemcpy_c_e,.Lmemcpy_e_e-.Lmemcpy_c_e
  	.previous
 -
 -	_ASM_EXTABLE_FAULT(.L_copy_leading_bytes, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w0, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w1, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w4, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w5, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w6, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_cache_w7, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_copy_trailing_words, .L_memcpy_mcsafe_fail)
 -	_ASM_EXTABLE_FAULT(.L_copy_trailing_bytes, .L_memcpy_mcsafe_fail)
 -#endif
* Unmerged path arch/x86/include/asm/string_64.h
* Unmerged path arch/x86/lib/memcpy_64.S
