blk-mq-sched: Allocate sched reserved tags as specified in the original queue tagset

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Sagi Grimberg <sagi@grimberg.me>
commit 415b806de5576b656f3ff94366589af9a161d0c8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/415b806d.failed

	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>

Modified by me to also check at driver tag allocation time if the
original request was reserved, so we can be sure to allocate a
properly reserved tag at that point in time, too.

	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 415b806de5576b656f3ff94366589af9a161d0c8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-sched.c
#	block/blk-mq-tag.c
#	block/blk-mq.c
diff --cc block/blk-mq-tag.c
index 7e6885bccaac,e48bc2c72615..000000000000
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@@ -297,165 -158,111 +297,169 @@@ static int bt_get(struct blk_mq_alloc_d
  		io_schedule();
  
  		data->ctx = blk_mq_get_ctx(data->q);
 -		data->hctx = blk_mq_map_queue(data->q, data->ctx->cpu);
 -		tags = blk_mq_tags_from_data(data);
 -		if (data->flags & BLK_MQ_REQ_RESERVED)
 -			bt = &tags->breserved_tags;
 -		else
 -			bt = &tags->bitmap_tags;
 -
 -		finish_wait(&ws->wait, &wait);
 -		ws = bt_wait_ptr(bt, data->hctx);
 +		data->hctx = data->q->mq_ops->map_queue(data->q,
 +				data->ctx->cpu);
 +		if (data->flags & BLK_MQ_REQ_RESERVED) {
 +			bt = &data->hctx->tags->breserved_tags;
 +		} else {
 +			last_tag = &data->ctx->last_tag;
 +			hctx = data->hctx;
 +			bt = &hctx->tags->bitmap_tags;
 +		}
 +		finish_wait(&bs->wait, &wait);
 +		bs = bt_wait_ptr(bt, hctx);
  	} while (1);
  
 -	if (drop_ctx && data->ctx)
 -		blk_mq_put_ctx(data->ctx);
 +	finish_wait(&bs->wait, &wait);
 +	return tag;
 +}
 +
 +static unsigned int __blk_mq_get_tag(struct blk_mq_alloc_data *data)
 +{
++<<<<<<< HEAD
 +	int tag;
  
 -	finish_wait(&ws->wait, &wait);
 +	tag = bt_get(data, &data->hctx->tags->bitmap_tags, data->hctx,
 +			&data->ctx->last_tag);
 +	if (tag >= 0)
 +		return tag + data->hctx->tags->nr_reserved_tags;
  
 -found_tag:
 -	return tag + tag_offset;
 +	return BLK_MQ_TAG_FAIL;
  }
  
 -void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 -		    struct blk_mq_ctx *ctx, unsigned int tag)
 +static unsigned int __blk_mq_get_reserved_tag(struct blk_mq_alloc_data *data)
  {
 -	if (!blk_mq_tag_is_reserved(tags, tag)) {
 -		const int real_tag = tag - tags->nr_reserved_tags;
 +	int tag, zero = 0;
  
 -		BUG_ON(real_tag >= tags->nr_tags);
 -		sbitmap_queue_clear(&tags->bitmap_tags, real_tag, ctx->cpu);
 -	} else {
 -		BUG_ON(tag >= tags->nr_reserved_tags);
 -		sbitmap_queue_clear(&tags->breserved_tags, tag, ctx->cpu);
 +	if (unlikely(!data->hctx->tags->nr_reserved_tags)) {
 +		WARN_ON_ONCE(1);
 +		return BLK_MQ_TAG_FAIL;
  	}
 +
 +	tag = bt_get(data, &data->hctx->tags->breserved_tags, NULL, &zero);
 +	if (tag < 0)
 +		return BLK_MQ_TAG_FAIL;
 +
 +	return tag;
  }
  
 -struct bt_iter_data {
 -	struct blk_mq_hw_ctx *hctx;
 -	busy_iter_fn *fn;
 -	void *data;
 -	bool reserved;
 -};
 +unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 +{
 +	if (data->flags & BLK_MQ_REQ_RESERVED)
 +		return __blk_mq_get_reserved_tag(data);
 +	return __blk_mq_get_tag(data);
 +}
  
 -static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 +static struct bt_wait_state *bt_wake_ptr(struct blk_mq_bitmap_tags *bt)
  {
 -	struct bt_iter_data *iter_data = data;
 -	struct blk_mq_hw_ctx *hctx = iter_data->hctx;
 -	struct blk_mq_tags *tags = hctx->tags;
 -	bool reserved = iter_data->reserved;
 -	struct request *rq;
 +	int i, wake_index;
  
 -	if (!reserved)
 -		bitnr += tags->nr_reserved_tags;
 -	rq = tags->rqs[bitnr];
 +	wake_index = atomic_read(&bt->wake_index);
 +	for (i = 0; i < BT_WAIT_QUEUES; i++) {
 +		struct bt_wait_state *bs = &bt->bs[wake_index];
  
 -	if (rq->q == hctx->queue)
 -		iter_data->fn(hctx, rq, iter_data->data, reserved);
 -	return true;
 +		if (waitqueue_active(&bs->wait)) {
 +			int o = atomic_read(&bt->wake_index);
 +			if (wake_index != o)
 +				atomic_cmpxchg(&bt->wake_index, o, wake_index);
 +
 +			return bs;
 +		}
 +
 +		wake_index = bt_index_inc(wake_index);
 +	}
 +
 +	return NULL;
  }
  
 -static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 -			busy_iter_fn *fn, void *data, bool reserved)
 +static void bt_clear_tag(struct blk_mq_bitmap_tags *bt, unsigned int tag)
  {
 -	struct bt_iter_data iter_data = {
 -		.hctx = hctx,
 -		.fn = fn,
 -		.data = data,
 -		.reserved = reserved,
 -	};
 +	const int index = TAG_TO_INDEX(bt, tag);
 +	struct bt_wait_state *bs;
 +	int wait_cnt;
  
 -	sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
 +	clear_bit(TAG_TO_BIT(bt, tag), &bt->map[index].word);
 +
 +	/* Ensure that the wait list checks occur after clear_bit(). */
 +	smp_mb();
 +
 +	bs = bt_wake_ptr(bt);
 +	if (!bs)
 +		return;
 +
 +	wait_cnt = atomic_dec_return(&bs->wait_cnt);
 +	if (unlikely(wait_cnt < 0))
 +		wait_cnt = atomic_inc_return(&bs->wait_cnt);
 +	if (wait_cnt == 0) {
 +		atomic_add(bt->wake_cnt, &bs->wait_cnt);
 +		bt_index_atomic_inc(&bt->wake_index);
 +		wake_up(&bs->wait);
 +	}
  }
  
 -struct bt_tags_iter_data {
 -	struct blk_mq_tags *tags;
 -	busy_tag_iter_fn *fn;
 -	void *data;
 -	bool reserved;
 -};
 +void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, unsigned int tag,
 +		    unsigned int *last_tag)
 +{
 +	struct blk_mq_tags *tags = hctx->tags;
  
 -static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 +	if (tag >= tags->nr_reserved_tags) {
++=======
++	if (!blk_mq_tag_is_reserved(tags, tag)) {
++>>>>>>> 415b806de557 (blk-mq-sched: Allocate sched reserved tags as specified in the original queue tagset)
 +		const int real_tag = tag - tags->nr_reserved_tags;
 +
 +		BUG_ON(real_tag >= tags->nr_tags);
 +		bt_clear_tag(&tags->bitmap_tags, real_tag);
 +		*last_tag = real_tag;
 +	} else {
 +		BUG_ON(tag >= tags->nr_reserved_tags);
 +		bt_clear_tag(&tags->breserved_tags, tag);
 +	}
 +}
 +
 +static void bt_for_each(struct blk_mq_hw_ctx *hctx,
 +		struct blk_mq_bitmap_tags *bt, unsigned int off,
 +		busy_iter_fn *fn, void *data, bool reserved)
  {
 -	struct bt_tags_iter_data *iter_data = data;
 -	struct blk_mq_tags *tags = iter_data->tags;
 -	bool reserved = iter_data->reserved;
  	struct request *rq;
 +	int bit, i;
  
 -	if (!reserved)
 -		bitnr += tags->nr_reserved_tags;
 -	rq = tags->rqs[bitnr];
 +	for (i = 0; i < bt->map_nr; i++) {
 +		struct blk_align_bitmap *bm = &bt->map[i];
  
 -	iter_data->fn(rq, iter_data->data, reserved);
 -	return true;
 +		for (bit = find_first_bit(&bm->word, bm->depth);
 +		     bit < bm->depth;
 +		     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {
 +			rq = hctx->tags->rqs[off + bit];
 +			if (rq->q == hctx->queue)
 +				fn(hctx, rq, data, reserved);
 +		}
 +
 +		off += (1 << bt->bits_per_word);
 +	}
  }
  
 -static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 -			     busy_tag_iter_fn *fn, void *data, bool reserved)
 +static void bt_tags_for_each(struct blk_mq_tags *tags,
 +		struct blk_mq_bitmap_tags *bt, unsigned int off,
 +		busy_tag_iter_fn *fn, void *data, bool reserved)
  {
 -	struct bt_tags_iter_data iter_data = {
 -		.tags = tags,
 -		.fn = fn,
 -		.data = data,
 -		.reserved = reserved,
 -	};
 +	struct request *rq;
 +	int bit, i;
  
 -	if (tags->rqs)
 -		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 +	if (!tags->rqs)
 +		return;
 +	for (i = 0; i < bt->map_nr; i++) {
 +		struct blk_align_bitmap *bm = &bt->map[i];
 +
 +		for (bit = find_first_bit(&bm->word, bm->depth);
 +		     bit < bm->depth;
 +		     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {
 +			rq = tags->rqs[off + bit];
 +			fn(rq, data, reserved);
 +		}
 +
 +		off += (1 << bt->bits_per_word);
 +	}
  }
  
  static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
diff --cc block/blk-mq.c
index 3e6f9b3d2b64,4df5fb42c74f..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -824,6 -836,115 +824,118 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
++<<<<<<< HEAD
++=======
+ bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
+ 			   bool wait)
+ {
+ 	struct blk_mq_alloc_data data = {
+ 		.q = rq->q,
+ 		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
+ 		.flags = wait ? 0 : BLK_MQ_REQ_NOWAIT,
+ 	};
+ 
+ 	if (rq->tag != -1) {
+ done:
+ 		if (hctx)
+ 			*hctx = data.hctx;
+ 		return true;
+ 	}
+ 
+ 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ 		data.flags |= BLK_MQ_REQ_RESERVED;
+ 
+ 	rq->tag = blk_mq_get_tag(&data);
+ 	if (rq->tag >= 0) {
+ 		if (blk_mq_tag_busy(data.hctx)) {
+ 			rq->rq_flags |= RQF_MQ_INFLIGHT;
+ 			atomic_inc(&data.hctx->nr_active);
+ 		}
+ 		data.hctx->tags->rqs[rq->tag] = rq;
+ 		goto done;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
+ 				  struct request *rq)
+ {
+ 	if (rq->tag == -1 || rq->internal_tag == -1)
+ 		return;
+ 
+ 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ 	rq->tag = -1;
+ 
+ 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+ 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+ 		atomic_dec(&hctx->nr_active);
+ 	}
+ }
+ 
+ /*
+  * If we fail getting a driver tag because all the driver tags are already
+  * assigned and on the dispatch list, BUT the first entry does not have a
+  * tag, then we could deadlock. For that case, move entries with assigned
+  * driver tags to the front, leaving the set of tagged requests in the
+  * same order, and the untagged set in the same order.
+  */
+ static bool reorder_tags_to_front(struct list_head *list)
+ {
+ 	struct request *rq, *tmp, *first = NULL;
+ 
+ 	list_for_each_entry_safe_reverse(rq, tmp, list, queuelist) {
+ 		if (rq == first)
+ 			break;
+ 		if (rq->tag != -1) {
+ 			list_move(&rq->queuelist, list);
+ 			if (!first)
+ 				first = rq;
+ 		}
+ 	}
+ 
+ 	return first != NULL;
+ }
+ 
+ static int blk_mq_dispatch_wake(wait_queue_t *wait, unsigned mode, int flags,
+ 				void *key)
+ {
+ 	struct blk_mq_hw_ctx *hctx;
+ 
+ 	hctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);
+ 
+ 	list_del(&wait->task_list);
+ 	clear_bit_unlock(BLK_MQ_S_TAG_WAITING, &hctx->state);
+ 	blk_mq_run_hw_queue(hctx, true);
+ 	return 1;
+ }
+ 
+ static bool blk_mq_dispatch_wait_add(struct blk_mq_hw_ctx *hctx)
+ {
+ 	struct sbq_wait_state *ws;
+ 
+ 	/*
+ 	 * The TAG_WAITING bit serves as a lock protecting hctx->dispatch_wait.
+ 	 * The thread which wins the race to grab this bit adds the hardware
+ 	 * queue to the wait queue.
+ 	 */
+ 	if (test_bit(BLK_MQ_S_TAG_WAITING, &hctx->state) ||
+ 	    test_and_set_bit_lock(BLK_MQ_S_TAG_WAITING, &hctx->state))
+ 		return false;
+ 
+ 	init_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);
+ 	ws = bt_wait_ptr(&hctx->tags->bitmap_tags, hctx);
+ 
+ 	/*
+ 	 * As soon as this returns, it's no longer safe to fiddle with
+ 	 * hctx->dispatch_wait, since a completion can wake up the wait queue
+ 	 * and unlock the bit.
+ 	 */
+ 	add_wait_queue(&ws->wait, &hctx->dispatch_wait);
+ 	return true;
+ }
+ 
++>>>>>>> 415b806de557 (blk-mq-sched: Allocate sched reserved tags as specified in the original queue tagset)
  bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list)
  {
  	struct request_queue *q = hctx->queue;
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-tag.c
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 5cdeb865c8ff..c31448c08b81 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -102,4 +102,10 @@ static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 	hctx->tags->rqs[tag] = rq;
 }
 
+static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
+					  unsigned int tag)
+{
+	return tag < tags->nr_reserved_tags;
+}
+
 #endif
* Unmerged path block/blk-mq.c
