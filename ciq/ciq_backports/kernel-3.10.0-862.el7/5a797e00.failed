blk-mq: don't lose flags passed in to blk_mq_alloc_request()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 5a797e00dc93593c9915841779881b15d9856237
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/5a797e00.failed

If we come in from blk_mq_alloc_requst() with NOWAIT set in flags,
we must ensure that we don't later overwrite that in
blk_mq_sched_get_request(). Initialize alloc_data->flags before
passing it in.

	Reported-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 5a797e00dc93593c9915841779881b15d9856237)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-sched.c
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 3b21482b7f01,84d13b5cafd0..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -269,10 -253,8 +269,14 @@@ __blk_mq_alloc_request(struct blk_mq_al
  struct request *blk_mq_alloc_request(struct request_queue *q, int rw,
  		unsigned int flags)
  {
++<<<<<<< HEAD
 +	struct blk_mq_ctx *ctx;
 +	struct blk_mq_hw_ctx *hctx;
++=======
+ 	struct blk_mq_alloc_data alloc_data = { .flags = flags };
++>>>>>>> 5a797e00dc93 (blk-mq: don't lose flags passed in to blk_mq_alloc_request())
  	struct request *rq;
 +	struct blk_mq_alloc_data alloc_data;
  	int ret;
  
  	ret = blk_queue_enter(q, flags & BLK_MQ_REQ_NOWAIT);
@@@ -1388,11 -1378,11 +1392,17 @@@ insert
   * but will attempt to bypass the hctx queueing if we can go straight to
   * hardware for SYNC IO.
   */
 -static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 +static void blk_mq_make_request(struct request_queue *q, struct bio *bio)
  {
++<<<<<<< HEAD
 +	const int is_sync = rw_is_sync(bio->bi_rw);
 +	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
 +	struct blk_map_ctx data;
++=======
+ 	const int is_sync = op_is_sync(bio->bi_opf);
+ 	const int is_flush_fua = bio->bi_opf & (REQ_PREFLUSH | REQ_FUA);
+ 	struct blk_mq_alloc_data data = { .flags = 0 };
++>>>>>>> 5a797e00dc93 (blk-mq: don't lose flags passed in to blk_mq_alloc_request())
  	struct request *rq;
  	unsigned int request_count = 0, srcu_idx;
  	struct blk_plug *plug;
@@@ -1482,14 -1498,16 +1492,18 @@@ run_queue
   * Single hardware queue variant. This will attempt to use any per-process
   * plug for merging and IO deferral.
   */
 -static blk_qc_t blk_sq_make_request(struct request_queue *q, struct bio *bio)
 +static void blk_sq_make_request(struct request_queue *q, struct bio *bio)
  {
 -	const int is_sync = op_is_sync(bio->bi_opf);
 -	const int is_flush_fua = bio->bi_opf & (REQ_PREFLUSH | REQ_FUA);
 +	const int is_sync = rw_is_sync(bio->bi_rw);
 +	const int is_flush_fua = bio->bi_rw & (REQ_FLUSH | REQ_FUA);
  	struct blk_plug *plug;
  	unsigned int request_count = 0;
++<<<<<<< HEAD
 +	struct blk_map_ctx data;
++=======
+ 	struct blk_mq_alloc_data data = { .flags = 0 };
++>>>>>>> 5a797e00dc93 (blk-mq: don't lose flags passed in to blk_mq_alloc_request())
  	struct request *rq;
 -	blk_qc_t cookie;
 -	unsigned int wb_acct;
  
  	blk_queue_bounce(q, &bio);
  
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq.c
