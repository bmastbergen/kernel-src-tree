blk-mq: get rid of manual run of queue with __blk_mq_run_hw_queue()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 841bac2c87fc21c3ecf3bc3354855921735aeec1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/841bac2c.failed

Two cases:

1) blk_mq_alloc_request() needlessly re-runs the queue, after
   calling into the tag allocation without NOWAIT set. We don't
   need to do that.

2) blk_mq_map_request() should just use blk_mq_run_hw_queue() with
   the async flag set to false.

	Signed-off-by: Jens Axboe <axboe@fb.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 841bac2c87fc21c3ecf3bc3354855921735aeec1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 49418900af65,c29700010b5c..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -282,19 -226,9 +280,24 @@@ struct request *blk_mq_alloc_request(st
  	ctx = blk_mq_get_ctx(q);
  	hctx = q->mq_ops->map_queue(q, ctx->cpu);
  	blk_mq_set_alloc_data(&alloc_data, q, flags, ctx, hctx);
++<<<<<<< HEAD
 +
 +	rq = __blk_mq_alloc_request(&alloc_data, rw);
 +	if (!rq && !(flags & BLK_MQ_REQ_NOWAIT)) {
 +		__blk_mq_run_hw_queue(hctx);
 +		blk_mq_put_ctx(ctx);
 +
 +		ctx = blk_mq_get_ctx(q);
 +		hctx = q->mq_ops->map_queue(q, ctx->cpu);
 +		blk_mq_set_alloc_data(&alloc_data, q, flags, ctx, hctx);
 +		rq =  __blk_mq_alloc_request(&alloc_data, rw);
 +		ctx = alloc_data.ctx;
 +	}
++=======
+ 	rq = __blk_mq_alloc_request(&alloc_data, rw, 0);
++>>>>>>> 841bac2c87fc (blk-mq: get rid of manual run of queue with __blk_mq_run_hw_queue())
  	blk_mq_put_ctx(ctx);
+ 
  	if (!rq) {
  		blk_queue_exit(q);
  		return ERR_PTR(-EWOULDBLOCK);
@@@ -1313,16 -1206,16 +1316,16 @@@ static struct request *blk_mq_map_reque
  	ctx = blk_mq_get_ctx(q);
  	hctx = q->mq_ops->map_queue(q, ctx->cpu);
  
 -	if (rw_is_sync(bio_op(bio), bio->bi_opf))
 -		op_flags |= REQ_SYNC;
 +	if (rw_is_sync(bio->bi_rw))
 +		rw |= REQ_SYNC;
  
 -	trace_block_getrq(q, bio, op);
 +	trace_block_getrq(q, bio, rw);
  	blk_mq_set_alloc_data(&alloc_data, q, BLK_MQ_REQ_NOWAIT, ctx, hctx);
 -	rq = __blk_mq_alloc_request(&alloc_data, op, op_flags);
 +	rq = __blk_mq_alloc_request(&alloc_data, rw);
  	if (unlikely(!rq)) {
- 		__blk_mq_run_hw_queue(hctx);
+ 		blk_mq_run_hw_queue(hctx, false);
  		blk_mq_put_ctx(ctx);
 -		trace_block_sleeprq(q, bio, op);
 +		trace_block_sleeprq(q, bio, rw);
  
  		ctx = blk_mq_get_ctx(q);
  		hctx = q->mq_ops->map_queue(q, ctx->cpu);
* Unmerged path block/blk-mq.c
