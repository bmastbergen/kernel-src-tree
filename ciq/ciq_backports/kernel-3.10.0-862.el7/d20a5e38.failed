xfs: report zeroed or not correctly in xfs_zero_range()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Eryu Guan <eguan@redhat.com>
commit d20a5e3851969fa685f118a80e4df670255a4e8d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d20a5e38.failed

The 'did_zero' param of xfs_zero_range() was not passed to
iomap_zero_range() correctly. This was introduced by commit
7bb41db3ea16 ("xfs: handle 64-bit length in xfs_iozero"), and found
by code inspection.

	Signed-off-by: Eryu Guan <eguan@redhat.com>
	Reviewed-by: Carlos Maiolino <cmaiolino@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
(cherry picked from commit d20a5e3851969fa685f118a80e4df670255a4e8d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_file.c
index 4da908a64865,350b6d43ba23..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -47,95 -48,17 +47,99 @@@
  static const struct vm_operations_struct xfs_file_vm_ops;
  
  /*
 - * Clear the specified ranges to zero through either the pagecache or DAX.
 - * Holes and unwritten extents will be left as-is as they already are zeroed.
 + * Locking primitives for read and write IO paths to ensure we consistently use
 + * and order the inode->i_mutex, ip->i_lock and ip->i_iolock.
   */
 -int
 -xfs_zero_range(
 +static inline void
 +xfs_rw_ilock(
 +	struct xfs_inode	*ip,
 +	int			type)
 +{
 +	if (type & XFS_IOLOCK_EXCL)
 +		mutex_lock(&VFS_I(ip)->i_mutex);
 +	xfs_ilock(ip, type);
 +}
 +
 +static inline void
 +xfs_rw_iunlock(
 +	struct xfs_inode	*ip,
 +	int			type)
 +{
 +	xfs_iunlock(ip, type);
 +	if (type & XFS_IOLOCK_EXCL)
 +		mutex_unlock(&VFS_I(ip)->i_mutex);
 +}
 +
 +static inline void
 +xfs_rw_ilock_demote(
  	struct xfs_inode	*ip,
 -	xfs_off_t		pos,
 -	xfs_off_t		count,
 -	bool			*did_zero)
 +	int			type)
 +{
 +	xfs_ilock_demote(ip, type);
 +	if (type & XFS_IOLOCK_EXCL)
 +		mutex_unlock(&VFS_I(ip)->i_mutex);
 +}
 +
 +/*
 + * xfs_iozero clears the specified range supplied via the page cache (except in
 + * the DAX case). Writes through the page cache will allocate blocks over holes,
 + * though the callers usually map the holes first and avoid them. If a block is
 + * not completely zeroed, then it will be read from disk before being partially
 + * zeroed.
 + *
 + * In the DAX case, we can just directly write to the underlying pages. This
 + * will not allocate blocks, but will avoid holes and unwritten extents and so
 + * not do unnecessary work.
 + */
 +int
 +xfs_iozero(
 +	struct xfs_inode	*ip,	/* inode			*/
 +	loff_t			pos,	/* offset in file		*/
 +	size_t			count)	/* size of data to zero		*/
  {
++<<<<<<< HEAD
 +	struct page		*page;
 +	struct address_space	*mapping;
 +	int			status = 0;
 +
 +
 +	mapping = VFS_I(ip)->i_mapping;
 +	do {
 +		unsigned offset, bytes;
 +		void *fsdata;
 +
 +		offset = (pos & (PAGE_CACHE_SIZE -1)); /* Within page */
 +		bytes = PAGE_CACHE_SIZE - offset;
 +		if (bytes > count)
 +			bytes = count;
 +
 +		if (IS_DAX(VFS_I(ip))) {
 +			status = dax_zero_page_range(VFS_I(ip), pos, bytes,
 +						     xfs_get_blocks_direct);
 +			if (status)
 +				break;
 +		} else {
 +			status = pagecache_write_begin(NULL, mapping, pos, bytes,
 +						AOP_FLAG_UNINTERRUPTIBLE,
 +						&page, &fsdata);
 +			if (status)
 +				break;
 +
 +			zero_user(page, offset, bytes);
 +
 +			status = pagecache_write_end(NULL, mapping, pos, bytes,
 +						bytes, page, fsdata);
 +			WARN_ON(status <= 0); /* can't return less than zero! */
 +			status = 0;
 +		}
 +		pos += bytes;
 +		count -= bytes;
 +	} while (count);
 +
 +	return status;
++=======
+ 	return iomap_zero_range(VFS_I(ip), pos, count, did_zero, &xfs_iomap_ops);
++>>>>>>> d20a5e385196 (xfs: report zeroed or not correctly in xfs_zero_range())
  }
  
  int
* Unmerged path fs/xfs/xfs_file.c
