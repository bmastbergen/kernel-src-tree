md: Runtime support for multiple ppls

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [md] Runtime support for multiple ppls (Nigel Croxon) [1455932]
Rebuild_FUZZ: 94.29%
commit-author Pawel Baldysiak <pawel.baldysiak@intel.com>
commit ddc088238cd6988bb4ac3776f403d7ff9d3c7a63
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ddc08823.failed

Increase PPL area to 1MB and use it as circular buffer to store PPL. The
entry with highest generation number is the latest one. If PPL to be
written is larger then space left in a buffer, rewind the buffer to the
start (don't wrap it).

	Signed-off-by: Pawel Baldysiak <pawel.baldysiak@intel.com>
	Signed-off-by: Artur Paszkiewicz <artur.paszkiewicz@intel.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit ddc088238cd6988bb4ac3776f403d7ff9d3c7a63)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/md.c
#	drivers/md/md.h
#	drivers/md/raid0.c
#	drivers/md/raid1.c
#	drivers/md/raid5-ppl.c
#	include/uapi/linux/raid/md_p.h
diff --cc drivers/md/md.c
index 4c75b0e4a2a4,a7876237de10..000000000000
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@@ -1655,10 -1656,15 +1656,20 @@@ static int super_1_validate(struct mdde
  		if (le32_to_cpu(sb->feature_map) & MD_FEATURE_JOURNAL)
  			set_bit(MD_HAS_JOURNAL, &mddev->flags);
  
- 		if (le32_to_cpu(sb->feature_map) & MD_FEATURE_PPL) {
+ 		if (le32_to_cpu(sb->feature_map) &
+ 		    (MD_FEATURE_PPL | MD_FEATURE_MULTIPLE_PPLS)) {
  			if (le32_to_cpu(sb->feature_map) &
++<<<<<<< HEAD
 +				(MD_FEATURE_BITMAP_OFFSET | MD_FEATURE_JOURNAL))
 +					return -EINVAL;
++=======
+ 			    (MD_FEATURE_BITMAP_OFFSET | MD_FEATURE_JOURNAL))
+ 				return -EINVAL;
+ 			if ((le32_to_cpu(sb->feature_map) & MD_FEATURE_PPL) &&
+ 			    (le32_to_cpu(sb->feature_map) &
+ 					    MD_FEATURE_MULTIPLE_PPLS))
+ 				return -EINVAL;
++>>>>>>> ddc088238cd6 (md: Runtime support for multiple ppls)
  			set_bit(MD_HAS_PPL, &mddev->flags);
  		}
  	} else if (mddev->pers == NULL) {
diff --cc drivers/md/md.h
index 0d13bf88f41f,d4bdfa5c223b..000000000000
--- a/drivers/md/md.h
+++ b/drivers/md/md.h
@@@ -220,6 -221,24 +220,27 @@@ extern int rdev_clear_badblocks(struct 
  				int is_new);
  struct md_cluster_info;
  
++<<<<<<< HEAD
++=======
+ /* change UNSUPPORTED_MDDEV_FLAGS for each array type if new flag is added */
+ enum mddev_flags {
+ 	MD_ARRAY_FIRST_USE,	/* First use of array, needs initialization */
+ 	MD_CLOSING,		/* If set, we are closing the array, do not open
+ 				 * it then */
+ 	MD_JOURNAL_CLEAN,	/* A raid with journal is already clean */
+ 	MD_HAS_JOURNAL,		/* The raid array has journal feature set */
+ 	MD_CLUSTER_RESYNC_LOCKED, /* cluster raid only, which means node
+ 				   * already took resync lock, need to
+ 				   * release the lock */
+ 	MD_FAILFAST_SUPPORTED,	/* Using MD_FAILFAST on metadata writes is
+ 				 * supported as calls to md_error() will
+ 				 * never cause the array to become failed.
+ 				 */
+ 	MD_HAS_PPL,		/* The raid array has PPL feature set */
+ 	MD_HAS_MULTIPLE_PPLS,	/* The raid array has multiple PPLs feature set */
+ };
+ 
++>>>>>>> ddc088238cd6 (md: Runtime support for multiple ppls)
  enum mddev_sb_flags {
  	MD_SB_CHANGE_DEVS,		/* Some device status has changed */
  	MD_SB_CHANGE_CLEAN,	/* transition to or from 'clean' */
diff --cc drivers/md/raid0.c
index 5d0952d819f8,fd5e8e5efbef..000000000000
--- a/drivers/md/raid0.c
+++ b/drivers/md/raid0.c
@@@ -25,10 -26,12 +25,19 @@@
  #include "raid0.h"
  #include "raid5.h"
  
++<<<<<<< HEAD
 +static bool devices_discard_performance = false;
 +module_param(devices_discard_performance, bool, 0644);
 +MODULE_PARM_DESC(devices_discard_performance,
 +		 "Set to Y if all devices in each array handles discard requests at proper speed");
++=======
+ #define UNSUPPORTED_MDDEV_FLAGS		\
+ 	((1L << MD_HAS_JOURNAL) |	\
+ 	 (1L << MD_JOURNAL_CLEAN) |	\
+ 	 (1L << MD_FAILFAST_SUPPORTED) |\
+ 	 (1L << MD_HAS_PPL) |		\
+ 	 (1L << MD_HAS_MULTIPLE_PPLS))
++>>>>>>> ddc088238cd6 (md: Runtime support for multiple ppls)
  
  static int raid0_congested(struct mddev *mddev, int bits)
  {
diff --cc drivers/md/raid1.c
index 481b2b2701df,1f5bd9475dc1..000000000000
--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@@ -41,6 -45,12 +41,15 @@@
  #include "raid1.h"
  #include "bitmap.h"
  
++<<<<<<< HEAD
++=======
+ #define UNSUPPORTED_MDDEV_FLAGS		\
+ 	((1L << MD_HAS_JOURNAL) |	\
+ 	 (1L << MD_JOURNAL_CLEAN) |	\
+ 	 (1L << MD_HAS_PPL) |		\
+ 	 (1L << MD_HAS_MULTIPLE_PPLS))
+ 
++>>>>>>> ddc088238cd6 (md: Runtime support for multiple ppls)
  /*
   * Number of guaranteed r1bios in case of extreme VM load:
   */
diff --cc drivers/md/raid5-ppl.c
index fd4455879af2,b313f17a6260..000000000000
--- a/drivers/md/raid5-ppl.c
+++ b/drivers/md/raid5-ppl.c
@@@ -459,12 -456,25 +464,29 @@@ static void ppl_submit_iounit(struct pp
  	pplhdr->entries_count = cpu_to_le32(io->entries_count);
  	pplhdr->checksum = cpu_to_le32(~crc32c_le(~0, pplhdr, PPL_HEADER_SIZE));
  
+ 	/* Rewind the buffer if current PPL is larger then remaining space */
+ 	if (log->use_multippl &&
+ 	    log->rdev->ppl.sector + log->rdev->ppl.size - log->next_io_sector <
+ 	    (PPL_HEADER_SIZE + io->pp_size) >> 9)
+ 		log->next_io_sector = log->rdev->ppl.sector;
+ 
+ 
  	bio->bi_end_io = ppl_log_endio;
 -	bio->bi_opf = REQ_OP_WRITE | REQ_FUA;
 +	bio->bi_rw = WRITE_FUA;
  	bio->bi_bdev = log->rdev->bdev;
++<<<<<<< HEAD
 +	bio->bi_sector = log->rdev->ppl.sector;
++=======
+ 	bio->bi_iter.bi_sector = log->next_io_sector;
++>>>>>>> ddc088238cd6 (md: Runtime support for multiple ppls)
  	bio_add_page(bio, io->header_page, PAGE_SIZE, 0);
  
+ 	pr_debug("%s: log->current_io_sector: %llu\n", __func__,
+ 	    (unsigned long long)log->next_io_sector);
+ 
+ 	if (log->use_multippl)
+ 		log->next_io_sector += (PPL_HEADER_SIZE + io->pp_size) >> 9;
+ 
  	list_for_each_entry(sh, &io->stripe_list, log_list) {
  		/* entries for full stripe writes have no partial parity */
  		if (test_bit(STRIPE_FULL_WRITE, &sh->state))
@@@ -1200,8 -1229,9 +1239,9 @@@ int ppl_init_log(struct r5conf *conf
  				goto err;
  
  			q = bdev_get_queue(rdev->bdev);
 -			if (test_bit(QUEUE_FLAG_WC, &q->queue_flags))
 +			if (q->flush_flags)
  				need_cache_flush = true;
+ 			ppl_init_child_log(log, rdev);
  		}
  	}
  
diff --cc include/uapi/linux/raid/md_p.h
index 1edd4e808717,b9197976b660..000000000000
--- a/include/uapi/linux/raid/md_p.h
+++ b/include/uapi/linux/raid/md_p.h
@@@ -316,8 -324,10 +316,13 @@@ struct mdp_superblock_1 
  #define	MD_FEATURE_RECOVERY_BITMAP	128 /* recovery that is happening
  					     * is guided by bitmap.
  					     */
++<<<<<<< HEAD
++=======
+ #define	MD_FEATURE_CLUSTERED		256 /* clustered MD */
++>>>>>>> ddc088238cd6 (md: Runtime support for multiple ppls)
  #define	MD_FEATURE_JOURNAL		512 /* support write cache */
  #define	MD_FEATURE_PPL			1024 /* support PPL */
+ #define	MD_FEATURE_MULTIPLE_PPLS	2048 /* support for multiple PPLs */
  #define	MD_FEATURE_ALL			(MD_FEATURE_BITMAP_OFFSET	\
  					|MD_FEATURE_RECOVERY_OFFSET	\
  					|MD_FEATURE_RESHAPE_ACTIVE	\
@@@ -326,8 -336,10 +331,9 @@@
  					|MD_FEATURE_RESHAPE_BACKWARDS	\
  					|MD_FEATURE_NEW_OFFSET		\
  					|MD_FEATURE_RECOVERY_BITMAP	\
 -					|MD_FEATURE_CLUSTERED		\
  					|MD_FEATURE_JOURNAL		\
  					|MD_FEATURE_PPL			\
+ 					|MD_FEATURE_MULTIPLE_PPLS	\
  					)
  
  struct r5l_payload_header {
* Unmerged path drivers/md/md.c
* Unmerged path drivers/md/md.h
* Unmerged path drivers/md/raid0.c
* Unmerged path drivers/md/raid1.c
* Unmerged path drivers/md/raid5-ppl.c
diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index 91b644b32dcc..b12b04f40231 100644
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@ -7062,6 +7062,7 @@ static int raid5_run(struct mddev *mddev)
 		pr_warn("md/raid:%s: using journal device and PPL not allowed - disabling PPL\n",
 			mdname(mddev));
 		clear_bit(MD_HAS_PPL, &mddev->flags);
+		clear_bit(MD_HAS_MULTIPLE_PPLS, &mddev->flags);
 	}
 
 	if (mddev->private == NULL)
* Unmerged path include/uapi/linux/raid/md_p.h
