net/mlx5e: Act on delay probe time updates

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Act on delay probe time updates (Don Dutile) [1456687 1499362]
Rebuild_FUZZ: 95.00%
commit-author Hadar Hen Zion <hadarh@mellanox.com>
commit a2fa1fe5ad13e7f11b82291fc08bdc654fac741e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/a2fa1fe5.failed

The user can change delay_first_probe_time parameter through sysctl.
Listen to NETEVENT_DELAY_PROBE_TIME_UPDATE notifications and update the
intervals for updating the neighbours 'used' value periodic task and
for flow HW counters query periodic task.
Both of the intervals will be update only in case the new delay prob
time value is lower the current interval.

Since the driver saves only one min interval value and not per device,
the users will be able to set lower interval value for updating
neighbour 'used' value periodic task but they won't be able to schedule
a higher interval for this periodic task.
The used interval for scheduling neighbour 'used' value periodic task is
the minimal delay prob time parameter ever seen by the driver.

	Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
	Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit a2fa1fe5ad13e7f11b82291fc08bdc654fac741e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index abcb1976163d,79462c0368a0..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@@ -224,18 -227,382 +224,380 @@@ void mlx5e_remove_sqs_fwd_rules(struct 
  	mlx5_eswitch_sqs2vport_stop(esw, rep);
  }
  
 -static void mlx5e_rep_neigh_update_init_interval(struct mlx5e_rep_priv *rpriv)
 +void mlx5e_nic_rep_unload(struct mlx5_eswitch *esw,
 +			  struct mlx5_eswitch_rep *rep)
  {
 -#if IS_ENABLED(CONFIG_IPV6)
 -	unsigned long ipv6_interval = NEIGH_VAR(&ipv6_stub->nd_tbl->parms,
 -						DELAY_PROBE_TIME);
 -#else
 -	unsigned long ipv6_interval = ~0UL;
 -#endif
 -	unsigned long ipv4_interval = NEIGH_VAR(&arp_tbl.parms,
 -						DELAY_PROBE_TIME);
 -	struct net_device *netdev = rpriv->rep->netdev;
 +	struct net_device *netdev = rep->netdev;
  	struct mlx5e_priv *priv = netdev_priv(netdev);
  
 -	rpriv->neigh_update.min_interval = min_t(unsigned long, ipv6_interval, ipv4_interval);
 -	mlx5_fc_update_sampling_interval(priv->mdev, rpriv->neigh_update.min_interval);
 -}
 +	if (test_bit(MLX5E_STATE_OPENED, &priv->state))
 +		mlx5e_remove_sqs_fwd_rules(priv);
  
++<<<<<<< HEAD
 +	/* clean (and re-init) existing uplink offloaded TC rules */
 +	mlx5e_tc_cleanup(priv);
 +	mlx5e_tc_init(priv);
++=======
+ void mlx5e_rep_queue_neigh_stats_work(struct mlx5e_priv *priv)
+ {
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+ 
+ 	mlx5_fc_queue_stats_work(priv->mdev,
+ 				 &neigh_update->neigh_stats_work,
+ 				 neigh_update->min_interval);
+ }
+ 
+ static void mlx5e_rep_neigh_stats_work(struct work_struct *work)
+ {
+ 	struct mlx5e_rep_priv *rpriv = container_of(work, struct mlx5e_rep_priv,
+ 						    neigh_update.neigh_stats_work.work);
+ 	struct net_device *netdev = rpriv->rep->netdev;
+ 	struct mlx5e_priv *priv = netdev_priv(netdev);
+ 	struct mlx5e_neigh_hash_entry *nhe;
+ 
+ 	rtnl_lock();
+ 	if (!list_empty(&rpriv->neigh_update.neigh_list))
+ 		mlx5e_rep_queue_neigh_stats_work(priv);
+ 
+ 	list_for_each_entry(nhe, &rpriv->neigh_update.neigh_list, neigh_list)
+ 		mlx5e_tc_update_neigh_used_value(nhe);
+ 
+ 	rtnl_unlock();
+ }
+ 
+ static void mlx5e_rep_neigh_entry_hold(struct mlx5e_neigh_hash_entry *nhe)
+ {
+ 	refcount_inc(&nhe->refcnt);
+ }
+ 
+ static void mlx5e_rep_neigh_entry_release(struct mlx5e_neigh_hash_entry *nhe)
+ {
+ 	if (refcount_dec_and_test(&nhe->refcnt))
+ 		kfree(nhe);
+ }
+ 
+ static void mlx5e_rep_update_flows(struct mlx5e_priv *priv,
+ 				   struct mlx5e_encap_entry *e,
+ 				   bool neigh_connected,
+ 				   unsigned char ha[ETH_ALEN])
+ {
+ 	struct ethhdr *eth = (struct ethhdr *)e->encap_header;
+ 
+ 	ASSERT_RTNL();
+ 
+ 	if ((!neigh_connected && (e->flags & MLX5_ENCAP_ENTRY_VALID)) ||
+ 	    !ether_addr_equal(e->h_dest, ha))
+ 		mlx5e_tc_encap_flows_del(priv, e);
+ 
+ 	if (neigh_connected && !(e->flags & MLX5_ENCAP_ENTRY_VALID)) {
+ 		ether_addr_copy(e->h_dest, ha);
+ 		ether_addr_copy(eth->h_dest, ha);
+ 
+ 		mlx5e_tc_encap_flows_add(priv, e);
+ 	}
+ }
+ 
+ static void mlx5e_rep_neigh_update(struct work_struct *work)
+ {
+ 	struct mlx5e_neigh_hash_entry *nhe =
+ 		container_of(work, struct mlx5e_neigh_hash_entry, neigh_update_work);
+ 	struct neighbour *n = nhe->n;
+ 	struct mlx5e_encap_entry *e;
+ 	unsigned char ha[ETH_ALEN];
+ 	struct mlx5e_priv *priv;
+ 	bool neigh_connected;
+ 	bool encap_connected;
+ 	u8 nud_state, dead;
+ 
+ 	rtnl_lock();
+ 
+ 	/* If these parameters are changed after we release the lock,
+ 	 * we'll receive another event letting us know about it.
+ 	 * We use this lock to avoid inconsistency between the neigh validity
+ 	 * and it's hw address.
+ 	 */
+ 	read_lock_bh(&n->lock);
+ 	memcpy(ha, n->ha, ETH_ALEN);
+ 	nud_state = n->nud_state;
+ 	dead = n->dead;
+ 	read_unlock_bh(&n->lock);
+ 
+ 	neigh_connected = (nud_state & NUD_VALID) && !dead;
+ 
+ 	list_for_each_entry(e, &nhe->encap_list, encap_list) {
+ 		encap_connected = !!(e->flags & MLX5_ENCAP_ENTRY_VALID);
+ 		priv = netdev_priv(e->out_dev);
+ 
+ 		if (encap_connected != neigh_connected ||
+ 		    !ether_addr_equal(e->h_dest, ha))
+ 			mlx5e_rep_update_flows(priv, e, neigh_connected, ha);
+ 	}
+ 	mlx5e_rep_neigh_entry_release(nhe);
+ 	rtnl_unlock();
+ 	neigh_release(n);
+ }
+ 
+ static struct mlx5e_neigh_hash_entry *
+ mlx5e_rep_neigh_entry_lookup(struct mlx5e_priv *priv,
+ 			     struct mlx5e_neigh *m_neigh);
+ 
+ static int mlx5e_rep_netevent_event(struct notifier_block *nb,
+ 				    unsigned long event, void *ptr)
+ {
+ 	struct mlx5e_rep_priv *rpriv = container_of(nb, struct mlx5e_rep_priv,
+ 						    neigh_update.netevent_nb);
+ 	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+ 	struct net_device *netdev = rpriv->rep->netdev;
+ 	struct mlx5e_priv *priv = netdev_priv(netdev);
+ 	struct mlx5e_neigh_hash_entry *nhe = NULL;
+ 	struct mlx5e_neigh m_neigh = {};
+ 	struct neigh_parms *p;
+ 	struct neighbour *n;
+ 	bool found = false;
+ 
+ 	switch (event) {
+ 	case NETEVENT_NEIGH_UPDATE:
+ 		n = ptr;
+ #if IS_ENABLED(CONFIG_IPV6)
+ 		if (n->tbl != ipv6_stub->nd_tbl && n->tbl != &arp_tbl)
+ #else
+ 		if (n->tbl != &arp_tbl)
+ #endif
+ 			return NOTIFY_DONE;
+ 
+ 		m_neigh.dev = n->dev;
+ 		m_neigh.family = n->ops->family;
+ 		memcpy(&m_neigh.dst_ip, n->primary_key, n->tbl->key_len);
+ 
+ 		/* We are in atomic context and can't take RTNL mutex, so use
+ 		 * spin_lock_bh to lookup the neigh table. bh is used since
+ 		 * netevent can be called from a softirq context.
+ 		 */
+ 		spin_lock_bh(&neigh_update->encap_lock);
+ 		nhe = mlx5e_rep_neigh_entry_lookup(priv, &m_neigh);
+ 		if (!nhe) {
+ 			spin_unlock_bh(&neigh_update->encap_lock);
+ 			return NOTIFY_DONE;
+ 		}
+ 
+ 		/* This assignment is valid as long as the the neigh reference
+ 		 * is taken
+ 		 */
+ 		nhe->n = n;
+ 
+ 		/* Take a reference to ensure the neighbour and mlx5 encap
+ 		 * entry won't be destructed until we drop the reference in
+ 		 * delayed work.
+ 		 */
+ 		neigh_hold(n);
+ 		mlx5e_rep_neigh_entry_hold(nhe);
+ 
+ 		if (!queue_work(priv->wq, &nhe->neigh_update_work)) {
+ 			mlx5e_rep_neigh_entry_release(nhe);
+ 			neigh_release(n);
+ 		}
+ 		spin_unlock_bh(&neigh_update->encap_lock);
+ 		break;
+ 
+ 	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
+ 		p = ptr;
+ 
+ 		/* We check the device is present since we don't care about
+ 		 * changes in the default table, we only care about changes
+ 		 * done per device delay prob time parameter.
+ 		 */
+ #if IS_ENABLED(CONFIG_IPV6)
+ 		if (!p->dev || (p->tbl != ipv6_stub->nd_tbl && p->tbl != &arp_tbl))
+ #else
+ 		if (!p->dev || p->tbl != &arp_tbl)
+ #endif
+ 			return NOTIFY_DONE;
+ 
+ 		/* We are in atomic context and can't take RTNL mutex,
+ 		 * so use spin_lock_bh to walk the neigh list and look for
+ 		 * the relevant device. bh is used since netevent can be
+ 		 * called from a softirq context.
+ 		 */
+ 		spin_lock_bh(&neigh_update->encap_lock);
+ 		list_for_each_entry(nhe, &neigh_update->neigh_list, neigh_list) {
+ 			if (p->dev == nhe->m_neigh.dev) {
+ 				found = true;
+ 				break;
+ 			}
+ 		}
+ 		spin_unlock_bh(&neigh_update->encap_lock);
+ 		if (!found)
+ 			return NOTIFY_DONE;
+ 
+ 		neigh_update->min_interval = min_t(unsigned long,
+ 						   NEIGH_VAR(p, DELAY_PROBE_TIME),
+ 						   neigh_update->min_interval);
+ 		mlx5_fc_update_sampling_interval(priv->mdev,
+ 						 neigh_update->min_interval);
+ 		break;
+ 	}
+ 	return NOTIFY_DONE;
+ }
+ 
+ static const struct rhashtable_params mlx5e_neigh_ht_params = {
+ 	.head_offset = offsetof(struct mlx5e_neigh_hash_entry, rhash_node),
+ 	.key_offset = offsetof(struct mlx5e_neigh_hash_entry, m_neigh),
+ 	.key_len = sizeof(struct mlx5e_neigh),
+ 	.automatic_shrinking = true,
+ };
+ 
+ static int mlx5e_rep_neigh_init(struct mlx5e_rep_priv *rpriv)
+ {
+ 	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+ 	int err;
+ 
+ 	err = rhashtable_init(&neigh_update->neigh_ht, &mlx5e_neigh_ht_params);
+ 	if (err)
+ 		return err;
+ 
+ 	INIT_LIST_HEAD(&neigh_update->neigh_list);
+ 	spin_lock_init(&neigh_update->encap_lock);
+ 	INIT_DELAYED_WORK(&neigh_update->neigh_stats_work,
+ 			  mlx5e_rep_neigh_stats_work);
+ 	mlx5e_rep_neigh_update_init_interval(rpriv);
+ 
+ 	rpriv->neigh_update.netevent_nb.notifier_call = mlx5e_rep_netevent_event;
+ 	err = register_netevent_notifier(&rpriv->neigh_update.netevent_nb);
+ 	if (err)
+ 		goto out_err;
+ 	return 0;
+ 
+ out_err:
+ 	rhashtable_destroy(&neigh_update->neigh_ht);
+ 	return err;
+ }
+ 
+ static void mlx5e_rep_neigh_cleanup(struct mlx5e_rep_priv *rpriv)
+ {
+ 	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+ 	struct mlx5e_priv *priv = netdev_priv(rpriv->rep->netdev);
+ 
+ 	unregister_netevent_notifier(&neigh_update->netevent_nb);
+ 
+ 	flush_workqueue(priv->wq); /* flush neigh update works */
+ 
+ 	cancel_delayed_work_sync(&rpriv->neigh_update.neigh_stats_work);
+ 
+ 	rhashtable_destroy(&neigh_update->neigh_ht);
+ }
+ 
+ static int mlx5e_rep_neigh_entry_insert(struct mlx5e_priv *priv,
+ 					struct mlx5e_neigh_hash_entry *nhe)
+ {
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 	int err;
+ 
+ 	err = rhashtable_insert_fast(&rpriv->neigh_update.neigh_ht,
+ 				     &nhe->rhash_node,
+ 				     mlx5e_neigh_ht_params);
+ 	if (err)
+ 		return err;
+ 
+ 	list_add(&nhe->neigh_list, &rpriv->neigh_update.neigh_list);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5e_rep_neigh_entry_remove(struct mlx5e_priv *priv,
+ 					 struct mlx5e_neigh_hash_entry *nhe)
+ {
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 
+ 	spin_lock_bh(&rpriv->neigh_update.encap_lock);
+ 
+ 	list_del(&nhe->neigh_list);
+ 
+ 	rhashtable_remove_fast(&rpriv->neigh_update.neigh_ht,
+ 			       &nhe->rhash_node,
+ 			       mlx5e_neigh_ht_params);
+ 	spin_unlock_bh(&rpriv->neigh_update.encap_lock);
+ }
+ 
+ /* This function must only be called under RTNL lock or under the
+  * representor's encap_lock in case RTNL mutex can't be held.
+  */
+ static struct mlx5e_neigh_hash_entry *
+ mlx5e_rep_neigh_entry_lookup(struct mlx5e_priv *priv,
+ 			     struct mlx5e_neigh *m_neigh)
+ {
+ 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
+ 	struct mlx5e_neigh_update_table *neigh_update = &rpriv->neigh_update;
+ 
+ 	return rhashtable_lookup_fast(&neigh_update->neigh_ht, m_neigh,
+ 				      mlx5e_neigh_ht_params);
+ }
+ 
+ static int mlx5e_rep_neigh_entry_create(struct mlx5e_priv *priv,
+ 					struct mlx5e_encap_entry *e,
+ 					struct mlx5e_neigh_hash_entry **nhe)
+ {
+ 	int err;
+ 
+ 	*nhe = kzalloc(sizeof(**nhe), GFP_KERNEL);
+ 	if (!*nhe)
+ 		return -ENOMEM;
+ 
+ 	memcpy(&(*nhe)->m_neigh, &e->m_neigh, sizeof(e->m_neigh));
+ 	INIT_WORK(&(*nhe)->neigh_update_work, mlx5e_rep_neigh_update);
+ 	INIT_LIST_HEAD(&(*nhe)->encap_list);
+ 	refcount_set(&(*nhe)->refcnt, 1);
+ 
+ 	err = mlx5e_rep_neigh_entry_insert(priv, *nhe);
+ 	if (err)
+ 		goto out_free;
+ 	return 0;
+ 
+ out_free:
+ 	kfree(*nhe);
+ 	return err;
+ }
+ 
+ static void mlx5e_rep_neigh_entry_destroy(struct mlx5e_priv *priv,
+ 					  struct mlx5e_neigh_hash_entry *nhe)
+ {
+ 	/* The neigh hash entry must be removed from the hash table regardless
+ 	 * of the reference count value, so it won't be found by the next
+ 	 * neigh notification call. The neigh hash entry reference count is
+ 	 * incremented only during creation and neigh notification calls and
+ 	 * protects from freeing the nhe struct.
+ 	 */
+ 	mlx5e_rep_neigh_entry_remove(priv, nhe);
+ 	mlx5e_rep_neigh_entry_release(nhe);
+ }
+ 
+ int mlx5e_rep_encap_entry_attach(struct mlx5e_priv *priv,
+ 				 struct mlx5e_encap_entry *e)
+ {
+ 	struct mlx5e_neigh_hash_entry *nhe;
+ 	int err;
+ 
+ 	nhe = mlx5e_rep_neigh_entry_lookup(priv, &e->m_neigh);
+ 	if (!nhe) {
+ 		err = mlx5e_rep_neigh_entry_create(priv, e, &nhe);
+ 		if (err)
+ 			return err;
+ 	}
+ 	list_add(&e->encap_list, &nhe->encap_list);
+ 	return 0;
+ }
+ 
+ void mlx5e_rep_encap_entry_detach(struct mlx5e_priv *priv,
+ 				  struct mlx5e_encap_entry *e)
+ {
+ 	struct mlx5e_neigh_hash_entry *nhe;
+ 
+ 	list_del(&e->encap_list);
+ 	nhe = mlx5e_rep_neigh_entry_lookup(priv, &e->m_neigh);
+ 
+ 	if (list_empty(&nhe->encap_list))
+ 		mlx5e_rep_neigh_entry_destroy(priv, nhe);
++>>>>>>> a2fa1fe5ad13 (net/mlx5e: Act on delay probe time updates)
  }
  
  static int mlx5e_rep_open(struct net_device *dev)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
