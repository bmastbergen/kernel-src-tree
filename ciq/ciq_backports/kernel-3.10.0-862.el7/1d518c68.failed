KVM: LAPIC: Fix reentrancy issues with preempt notifiers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Wanpeng Li <wanpeng.li@hotmail.com>
commit 1d518c6820daf4e00d29adfba980aee05f605f0f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1d518c68.failed

Preempt can occur in the preemption timer expiration handler:

          CPU0                    CPU1

  preemption timer vmexit
  handle_preemption_timer(vCPU0)
    kvm_lapic_expired_hv_timer
      hv_timer_is_use == true
  sched_out
                           sched_in
                           kvm_arch_vcpu_load
                             kvm_lapic_restart_hv_timer
                               restart_apic_timer
                                 start_hv_timer
                                   already-expired timer or sw timer triggerd in the window
                                 start_sw_timer
                                   cancel_hv_timer
                           /* back in kvm_lapic_expired_hv_timer */
                           cancel_hv_timer
                             WARN_ON(!apic->lapic_timer.hv_timer_in_use);  ==> Oops

This can be reproduced if CONFIG_PREEMPT is enabled.

------------[ cut here ]------------
 WARNING: CPU: 4 PID: 2972 at /home/kernel/linux/arch/x86/kvm//lapic.c:1563 kvm_lapic_expired_hv_timer+0x9e/0xb0 [kvm]
 CPU: 4 PID: 2972 Comm: qemu-system-x86 Tainted: G           OE   4.13.0-rc2+ #16
 RIP: 0010:kvm_lapic_expired_hv_timer+0x9e/0xb0 [kvm]
Call Trace:
  handle_preemption_timer+0xe/0x20 [kvm_intel]
  vmx_handle_exit+0xb8/0xd70 [kvm_intel]
  kvm_arch_vcpu_ioctl_run+0xdd1/0x1be0 [kvm]
  ? kvm_arch_vcpu_load+0x47/0x230 [kvm]
  ? kvm_arch_vcpu_load+0x62/0x230 [kvm]
  kvm_vcpu_ioctl+0x340/0x700 [kvm]
  ? kvm_vcpu_ioctl+0x340/0x700 [kvm]
  ? __fget+0xfc/0x210
  do_vfs_ioctl+0xa4/0x6a0
  ? __fget+0x11d/0x210
  SyS_ioctl+0x79/0x90
  do_syscall_64+0x81/0x220
  entry_SYSCALL64_slow_path+0x25/0x25
 ------------[ cut here ]------------
 WARNING: CPU: 4 PID: 2972 at /home/kernel/linux/arch/x86/kvm//lapic.c:1498 cancel_hv_timer.isra.40+0x4f/0x60 [kvm]
 CPU: 4 PID: 2972 Comm: qemu-system-x86 Tainted: G        W  OE   4.13.0-rc2+ #16
 RIP: 0010:cancel_hv_timer.isra.40+0x4f/0x60 [kvm]
Call Trace:
  kvm_lapic_expired_hv_timer+0x3e/0xb0 [kvm]
  handle_preemption_timer+0xe/0x20 [kvm_intel]
  vmx_handle_exit+0xb8/0xd70 [kvm_intel]
  kvm_arch_vcpu_ioctl_run+0xdd1/0x1be0 [kvm]
  ? kvm_arch_vcpu_load+0x47/0x230 [kvm]
  ? kvm_arch_vcpu_load+0x62/0x230 [kvm]
  kvm_vcpu_ioctl+0x340/0x700 [kvm]
  ? kvm_vcpu_ioctl+0x340/0x700 [kvm]
  ? __fget+0xfc/0x210
  do_vfs_ioctl+0xa4/0x6a0
  ? __fget+0x11d/0x210
  SyS_ioctl+0x79/0x90
  do_syscall_64+0x81/0x220
  entry_SYSCALL64_slow_path+0x25/0x25

This patch fixes it by making the caller of cancel_hv_timer, start_hv_timer
and start_sw_timer be in preemption-disabled regions, which trivially
avoid any reentrancy issue with preempt notifier.

	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
[Add more WARNs. - Paolo]
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 1d518c6820daf4e00d29adfba980aee05f605f0f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/lapic.c
diff --cc arch/x86/kvm/lapic.c
index 0e8704ed991e,589dcc117086..000000000000
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@@ -1373,53 -1411,211 +1373,252 @@@ static void start_sw_tscdeadline(struc
  	local_irq_restore(flags);
  }
  
++<<<<<<< HEAD
++=======
+ static void start_sw_period(struct kvm_lapic *apic)
+ {
+ 	if (!apic->lapic_timer.period)
+ 		return;
+ 
+ 	if (apic_lvtt_oneshot(apic) &&
+ 	    ktime_after(ktime_get(),
+ 			apic->lapic_timer.target_expiration)) {
+ 		apic_timer_expired(apic);
+ 		return;
+ 	}
+ 
+ 	hrtimer_start(&apic->lapic_timer.timer,
+ 		apic->lapic_timer.target_expiration,
+ 		HRTIMER_MODE_ABS_PINNED);
+ }
+ 
+ static bool set_target_expiration(struct kvm_lapic *apic)
+ {
+ 	ktime_t now;
+ 	u64 tscl = rdtsc();
+ 
+ 	now = ktime_get();
+ 	apic->lapic_timer.period = (u64)kvm_lapic_get_reg(apic, APIC_TMICT)
+ 		* APIC_BUS_CYCLE_NS * apic->divide_count;
+ 
+ 	if (!apic->lapic_timer.period)
+ 		return false;
+ 
+ 	/*
+ 	 * Do not allow the guest to program periodic timers with small
+ 	 * interval, since the hrtimers are not throttled by the host
+ 	 * scheduler.
+ 	 */
+ 	if (apic_lvtt_period(apic)) {
+ 		s64 min_period = min_timer_period_us * 1000LL;
+ 
+ 		if (apic->lapic_timer.period < min_period) {
+ 			pr_info_ratelimited(
+ 			    "kvm: vcpu %i: requested %lld ns "
+ 			    "lapic timer period limited to %lld ns\n",
+ 			    apic->vcpu->vcpu_id,
+ 			    apic->lapic_timer.period, min_period);
+ 			apic->lapic_timer.period = min_period;
+ 		}
+ 	}
+ 
+ 	apic_debug("%s: bus cycle is %" PRId64 "ns, now 0x%016"
+ 		   PRIx64 ", "
+ 		   "timer initial count 0x%x, period %lldns, "
+ 		   "expire @ 0x%016" PRIx64 ".\n", __func__,
+ 		   APIC_BUS_CYCLE_NS, ktime_to_ns(now),
+ 		   kvm_lapic_get_reg(apic, APIC_TMICT),
+ 		   apic->lapic_timer.period,
+ 		   ktime_to_ns(ktime_add_ns(now,
+ 				apic->lapic_timer.period)));
+ 
+ 	apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ 		nsec_to_cycles(apic->vcpu, apic->lapic_timer.period);
+ 	apic->lapic_timer.target_expiration = ktime_add_ns(now, apic->lapic_timer.period);
+ 
+ 	return true;
+ }
+ 
+ static void advance_periodic_target_expiration(struct kvm_lapic *apic)
+ {
+ 	apic->lapic_timer.tscdeadline +=
+ 		nsec_to_cycles(apic->vcpu, apic->lapic_timer.period);
+ 	apic->lapic_timer.target_expiration =
+ 		ktime_add_ns(apic->lapic_timer.target_expiration,
+ 				apic->lapic_timer.period);
+ }
+ 
+ bool kvm_lapic_hv_timer_in_use(struct kvm_vcpu *vcpu)
+ {
+ 	if (!lapic_in_kernel(vcpu))
+ 		return false;
+ 
+ 	return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_hv_timer_in_use);
+ 
+ static void cancel_hv_timer(struct kvm_lapic *apic)
+ {
+ 	WARN_ON(preemptible());
+ 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+ 	kvm_x86_ops->cancel_hv_timer(apic->vcpu);
+ 	apic->lapic_timer.hv_timer_in_use = false;
+ }
+ 
+ static bool start_hv_timer(struct kvm_lapic *apic)
+ {
+ 	struct kvm_timer *ktimer = &apic->lapic_timer;
+ 	int r;
+ 
+ 	WARN_ON(preemptible());
+ 	if (!kvm_x86_ops->set_hv_timer)
+ 		return false;
+ 
+ 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
+ 		return false;
+ 
+ 	r = kvm_x86_ops->set_hv_timer(apic->vcpu, ktimer->tscdeadline);
+ 	if (r < 0)
+ 		return false;
+ 
+ 	ktimer->hv_timer_in_use = true;
+ 	hrtimer_cancel(&ktimer->timer);
+ 
+ 	/*
+ 	 * Also recheck ktimer->pending, in case the sw timer triggered in
+ 	 * the window.  For periodic timer, leave the hv timer running for
+ 	 * simplicity, and the deadline will be recomputed on the next vmexit.
+ 	 */
+ 	if (!apic_lvtt_period(apic) && (r || atomic_read(&ktimer->pending))) {
+ 		if (r)
+ 			apic_timer_expired(apic);
+ 		return false;
+ 	}
+ 
+ 	trace_kvm_hv_timer_state(apic->vcpu->vcpu_id, true);
+ 	return true;
+ }
+ 
+ static void start_sw_timer(struct kvm_lapic *apic)
+ {
+ 	struct kvm_timer *ktimer = &apic->lapic_timer;
+ 
+ 	WARN_ON(preemptible());
+ 	if (apic->lapic_timer.hv_timer_in_use)
+ 		cancel_hv_timer(apic);
+ 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
+ 		return;
+ 
+ 	if (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))
+ 		start_sw_period(apic);
+ 	else if (apic_lvtt_tscdeadline(apic))
+ 		start_sw_tscdeadline(apic);
+ 	trace_kvm_hv_timer_state(apic->vcpu->vcpu_id, false);
+ }
+ 
+ static void restart_apic_timer(struct kvm_lapic *apic)
+ {
+ 	preempt_disable();
+ 	if (!start_hv_timer(apic))
+ 		start_sw_timer(apic);
+ 	preempt_enable();
+ }
+ 
+ void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	preempt_disable();
+ 	/* If the preempt notifier has already run, it also called apic_timer_expired */
+ 	if (!apic->lapic_timer.hv_timer_in_use)
+ 		goto out;
+ 	WARN_ON(swait_active(&vcpu->wq));
+ 	cancel_hv_timer(apic);
+ 	apic_timer_expired(apic);
+ 
+ 	if (apic_lvtt_period(apic) && apic->lapic_timer.period) {
+ 		advance_periodic_target_expiration(apic);
+ 		restart_apic_timer(apic);
+ 	}
+ out:
+ 	preempt_enable();
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);
+ 
+ void kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	restart_apic_timer(vcpu->arch.apic);
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_hv_timer);
+ 
+ void kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	preempt_disable();
+ 	/* Possibly the TSC deadline timer is not enabled yet */
+ 	if (apic->lapic_timer.hv_timer_in_use)
+ 		start_sw_timer(apic);
+ 	preempt_enable();
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_sw_timer);
+ 
+ void kvm_lapic_restart_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+ 	restart_apic_timer(apic);
+ }
+ 
++>>>>>>> 1d518c6820da (KVM: LAPIC: Fix reentrancy issues with preempt notifiers)
  static void start_apic_timer(struct kvm_lapic *apic)
  {
 +	ktime_t now;
  	atomic_set(&apic->lapic_timer.pending, 0);
  
 -	if ((apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))
 -	    && !set_target_expiration(apic))
 -		return;
 +	if (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic)) {
 +		/* lapic timer in oneshot or periodic mode */
 +		now = apic->lapic_timer.timer.base->get_time();
 +		apic->lapic_timer.period = (u64)kvm_lapic_get_reg(apic, APIC_TMICT)
 +			    * APIC_BUS_CYCLE_NS * apic->divide_count;
  
 -	restart_apic_timer(apic);
 +		if (!apic->lapic_timer.period)
 +			return;
 +		/*
 +		 * Do not allow the guest to program periodic timers with small
 +		 * interval, since the hrtimers are not throttled by the host
 +		 * scheduler.
 +		 */
 +		if (apic_lvtt_period(apic)) {
 +			s64 min_period = min_timer_period_us * 1000LL;
 +
 +			if (apic->lapic_timer.period < min_period) {
 +				pr_info_ratelimited(
 +				    "kvm: vcpu %i: requested %lld ns "
 +				    "lapic timer period limited to %lld ns\n",
 +				    apic->vcpu->vcpu_id,
 +				    apic->lapic_timer.period, min_period);
 +				apic->lapic_timer.period = min_period;
 +			}
 +		}
 +
 +		hrtimer_start(&apic->lapic_timer.timer,
 +			      ktime_add_ns(now, apic->lapic_timer.period),
 +			      HRTIMER_MODE_ABS_PINNED);
 +
 +		apic_debug("%s: bus cycle is %" PRId64 "ns, now 0x%016"
 +			   PRIx64 ", "
 +			   "timer initial count 0x%x, period %lldns, "
 +			   "expire @ 0x%016" PRIx64 ".\n", __func__,
 +			   APIC_BUS_CYCLE_NS, ktime_to_ns(now),
 +			   kvm_lapic_get_reg(apic, APIC_TMICT),
 +			   apic->lapic_timer.period,
 +			   ktime_to_ns(ktime_add_ns(now,
 +					apic->lapic_timer.period)));
 +	} else if (apic_lvtt_tscdeadline(apic)) {
 +		start_sw_tscdeadline(apic);
 +	}
  }
  
  static void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)
* Unmerged path arch/x86/kvm/lapic.c
