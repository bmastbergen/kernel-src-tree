x86/intel_rdt: Move special case code for Haswell to a quirk function

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] intel_rdt: Move special case code for Haswell to a quirk function (Jiri Olsa) [1486121]
Rebuild_FUZZ: 97.01%
commit-author Tony Luck <tony.luck@intel.com>
commit 0576113a387e0c8a5d9e24b4cd62605d1c9c0db8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/0576113a.failed

No functional change, but lay the ground work for other per-model
quirks.

	Signed-off-by: Tony Luck <tony.luck@intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Fenghua" <fenghua.yu@intel.com>
	Cc: Ravi V" <ravi.v.shankar@intel.com>
	Cc: "Peter Zijlstra" <peterz@infradead.org>
	Cc: "Stephane Eranian" <eranian@google.com>
	Cc: "Andi Kleen" <ak@linux.intel.com>
	Cc: "David Carrillo-Cisneros" <davidcc@google.com>
	Cc: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Link: http://lkml.kernel.org/r/f195a83751b5f8b1d8a78bd3c1914300c8fa3142.1503512900.git.tony.luck@intel.com

(cherry picked from commit 0576113a387e0c8a5d9e24b4cd62605d1c9c0db8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/intel_rdt.c
diff --cc arch/x86/kernel/cpu/intel_rdt.c
index ad087dd4421e,25514cd454b3..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt.c
+++ b/arch/x86/kernel/cpu/intel_rdt.c
@@@ -107,39 -172,81 +107,43 @@@ static int cbm_idx(struct rdt_resource 
   * is always 20 on hsw server parts. The minimum cache bitmask length
   * allowed for HSW server is always 2 bits. Hardcode all of them.
   */
- static inline bool cache_alloc_hsw_probe(void)
+ static inline void cache_alloc_hsw_probe(void)
  {
- 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
- 	    boot_cpu_data.x86 == 6 &&
- 	    boot_cpu_data.x86_model == INTEL_FAM6_HASWELL_X) {
- 		struct rdt_resource *r  = &rdt_resources_all[RDT_RESOURCE_L3];
- 		u32 l, h, max_cbm = BIT_MASK(20) - 1;
+ 	struct rdt_resource *r  = &rdt_resources_all[RDT_RESOURCE_L3];
+ 	u32 l, h, max_cbm = BIT_MASK(20) - 1;
  
- 		if (wrmsr_safe(IA32_L3_CBM_BASE, max_cbm, 0))
- 			return false;
- 		rdmsr(IA32_L3_CBM_BASE, l, h);
+ 	if (wrmsr_safe(IA32_L3_CBM_BASE, max_cbm, 0))
+ 		return;
+ 	rdmsr(IA32_L3_CBM_BASE, l, h);
  
- 		/* If all the bits were set in MSR, return success */
- 		if (l != max_cbm)
- 			return false;
+ 	/* If all the bits were set in MSR, return success */
+ 	if (l != max_cbm)
+ 		return;
  
++<<<<<<< HEAD
 +		r->num_closid = 4;
 +		r->cbm_len = 20;
 +		r->max_cbm = max_cbm;
 +		r->min_cbm_bits = 2;
 +		r->capable = true;
 +		r->enabled = true;
- 
- 		return true;
- 	}
- 
- 	return false;
++=======
+ 	r->num_closid = 4;
+ 	r->default_ctrl = max_cbm;
+ 	r->cache.cbm_len = 20;
+ 	r->cache.shareable_bits = 0xc0000;
+ 	r->cache.min_cbm_bits = 2;
+ 	r->alloc_capable = true;
+ 	r->alloc_enabled = true;
++>>>>>>> 0576113a387e (x86/intel_rdt: Move special case code for Haswell to a quirk function)
+ 
+ 	rdt_alloc_capable = true;
  }
  
 -/*
 - * rdt_get_mb_table() - get a mapping of bandwidth(b/w) percentage values
 - * exposed to user interface and the h/w understandable delay values.
 - *
 - * The non-linear delay values have the granularity of power of two
 - * and also the h/w does not guarantee a curve for configured delay
 - * values vs. actual b/w enforced.
 - * Hence we need a mapping that is pre calibrated so the user can
 - * express the memory b/w as a percentage value.
 - */
 -static inline bool rdt_get_mb_table(struct rdt_resource *r)
 -{
 -	/*
 -	 * There are no Intel SKUs as of now to support non-linear delay.
 -	 */
 -	pr_info("MBA b/w map not implemented for cpu:%d, model:%d",
 -		boot_cpu_data.x86, boot_cpu_data.x86_model);
 -
 -	return false;
 -}
 -
 -static bool rdt_get_mem_config(struct rdt_resource *r)
 -{
 -	union cpuid_0x10_3_eax eax;
 -	union cpuid_0x10_x_edx edx;
 -	u32 ebx, ecx;
 -
 -	cpuid_count(0x00000010, 3, &eax.full, &ebx, &ecx, &edx.full);
 -	r->num_closid = edx.split.cos_max + 1;
 -	r->membw.max_delay = eax.split.max_delay + 1;
 -	r->default_ctrl = MAX_MBA_BW;
 -	if (ecx & MBA_IS_LINEAR) {
 -		r->membw.delay_linear = true;
 -		r->membw.min_bw = MAX_MBA_BW - r->membw.max_delay;
 -		r->membw.bw_gran = MAX_MBA_BW - r->membw.max_delay;
 -	} else {
 -		if (!rdt_get_mb_table(r))
 -			return false;
 -	}
 -	r->data_width = 3;
 -
 -	r->alloc_capable = true;
 -	r->alloc_enabled = true;
 -
 -	return true;
 -}
 -
 -static void rdt_get_cache_alloc_cfg(int idx, struct rdt_resource *r)
 +static void rdt_get_config(int idx, struct rdt_resource *r)
  {
  	union cpuid_0x10_1_eax eax;
 -	union cpuid_0x10_x_edx edx;
 +	union cpuid_0x10_1_edx edx;
  	u32 ebx, ecx;
  
  	cpuid_count(0x00000010, idx, &eax.full, &ebx, &ecx, &edx.full);
@@@ -398,68 -618,87 +402,108 @@@ static int intel_rdt_offline_cpu(unsign
  	return 0;
  }
  
 -/*
 - * Choose a width for the resource name and resource data based on the
 - * resource that has widest name and cbm.
 - */
 -static __init void rdt_init_padding(void)
 +static int
 +rdt_cpu_notify(struct notifier_block *self, unsigned long action, void *hcpu)
  {
 -	struct rdt_resource *r;
 -	int cl;
 +       unsigned int cpu = (long)hcpu;
  
 -	for_each_alloc_capable_rdt_resource(r) {
 -		cl = strlen(r->name);
 -		if (cl > max_name_width)
 -			max_name_width = cl;
 +       switch (action & ~CPU_TASKS_FROZEN) {
  
 -		if (r->data_width > max_data_width)
 -			max_data_width = r->data_width;
 -	}
 +       case CPU_ONLINE:
 +       case CPU_DOWN_FAILED:
 +               intel_rdt_online_cpu(cpu, true);
 +               break;
 +
 +       case CPU_UP_CANCELED:
 +       case CPU_DOWN_PREPARE:
 +               intel_rdt_offline_cpu(cpu);
 +               break;
 +       default:
 +               break;
 +       }
 +
 +       return NOTIFY_OK;
  }
  
 +static void __init rdt_cpu_setup(void *dummy)
 +{
 +	struct rdt_resource *r;
 +	int i;
 +
 +	clear_closid(smp_processor_id());
 +
 +	for_each_capable_rdt_resource(r) {
 +		for (i = 0; i < r->num_closid; i++) {
 +			int idx = cbm_idx(r, i);
 +
++<<<<<<< HEAD
 +			wrmsrl(r->msr_base + idx, r->max_cbm);
++=======
+ static __init bool get_rdt_alloc_resources(void)
+ {
+ 	bool ret = false;
+ 
+ 	if (rdt_alloc_capable)
+ 		return true;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_RDT_A))
+ 		return false;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CAT_L3)) {
+ 		rdt_get_cache_alloc_cfg(1, &rdt_resources_all[RDT_RESOURCE_L3]);
+ 		if (boot_cpu_has(X86_FEATURE_CDP_L3)) {
+ 			rdt_get_cdp_l3_config(RDT_RESOURCE_L3DATA);
+ 			rdt_get_cdp_l3_config(RDT_RESOURCE_L3CODE);
++>>>>>>> 0576113a387e (x86/intel_rdt: Move special case code for Haswell to a quirk function)
  		}
 -		ret = true;
  	}
 -	if (boot_cpu_has(X86_FEATURE_CAT_L2)) {
 -		/* CPUID 0x10.2 fields are same format at 0x10.1 */
 -		rdt_get_cache_alloc_cfg(2, &rdt_resources_all[RDT_RESOURCE_L2]);
 -		ret = true;
 -	}
 -
 -	if (boot_cpu_has(X86_FEATURE_MBA)) {
 -		if (rdt_get_mem_config(&rdt_resources_all[RDT_RESOURCE_MBA]))
 -			ret = true;
 -	}
 -	return ret;
  }
  
 -static __init bool get_rdt_mon_resources(void)
 +static struct notifier_block rdt_cpu_nb = {
 +	.notifier_call  = rdt_cpu_notify,
 +	.priority	= -INT_MAX,
 +};
 +
 +static int __init rdt_notifier_init(void)
  {
 -	if (boot_cpu_has(X86_FEATURE_CQM_OCCUP_LLC))
 -		rdt_mon_features |= (1 << QOS_L3_OCCUP_EVENT_ID);
 -	if (boot_cpu_has(X86_FEATURE_CQM_MBM_TOTAL))
 -		rdt_mon_features |= (1 << QOS_L3_MBM_TOTAL_EVENT_ID);
 -	if (boot_cpu_has(X86_FEATURE_CQM_MBM_LOCAL))
 -		rdt_mon_features |= (1 << QOS_L3_MBM_LOCAL_EVENT_ID);
 -
 -	if (!rdt_mon_features)
 -		return false;
 +	unsigned int cpu;
 +
 +	for_each_online_cpu(cpu) {
 +		intel_rdt_online_cpu(cpu, false);
 +		/*
 +		 * RHEL7 - The upstream hotplug notification invokes the
 +		 *         callbacks on related cpus, but that's not the
 +		 *         case of the RHEL7 notification support.
 +		 *         Following call ensures we run all the msr
 +		 *         initialization setup on related cpus.
 +		 */
 +		smp_call_function_single(cpu, rdt_cpu_setup, NULL, 1);
 +	}
  
++<<<<<<< HEAD
 +	__register_cpu_notifier(&rdt_cpu_nb);
 +	return 0;
++=======
+ 	return !rdt_get_mon_l3_config(&rdt_resources_all[RDT_RESOURCE_L3]);
+ }
+ 
+ static __init void rdt_quirks(void)
+ {
+ 	switch (boot_cpu_data.x86_model) {
+ 	case INTEL_FAM6_HASWELL_X:
+ 		cache_alloc_hsw_probe();
+ 		break;
+ 	}
+ }
+ 
+ static __init bool get_rdt_resources(void)
+ {
+ 	rdt_quirks();
+ 	rdt_alloc_capable = get_rdt_alloc_resources();
+ 	rdt_mon_capable = get_rdt_mon_resources();
+ 
+ 	return (rdt_mon_capable || rdt_alloc_capable);
++>>>>>>> 0576113a387e (x86/intel_rdt: Move special case code for Haswell to a quirk function)
  }
  
  static int __init intel_rdt_late_init(void)
* Unmerged path arch/x86/kernel/cpu/intel_rdt.c
