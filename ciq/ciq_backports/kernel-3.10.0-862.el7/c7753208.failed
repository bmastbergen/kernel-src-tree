x86, swiotlb: Add memory encryption support

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] swiotlb: Add memory encryption support (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 93.83%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit c7753208a94c73d5beb1e4bd843081d6dc7d4678
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c7753208.failed

Since DMA addresses will effectively look like 48-bit addresses when the
memory encryption mask is set, SWIOTLB is needed if the DMA mask of the
device performing the DMA does not support 48-bits. SWIOTLB will be
initialized to create decrypted bounce buffers for use by these devices.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brijesh Singh <brijesh.singh@amd.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Larry Woodman <lwoodman@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Toshimitsu Kani <toshi.kani@hpe.com>
	Cc: kasan-dev@googlegroups.com
	Cc: kvm@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
	Cc: linux-doc@vger.kernel.org
	Cc: linux-efi@vger.kernel.org
	Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/aa2d29b78ae7d508db8881e46a3215231b9327a7.1500319216.git.thomas.lendacky@amd.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit c7753208a94c73d5beb1e4bd843081d6dc7d4678)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mem_encrypt.h
#	arch/x86/kernel/pci-dma.c
#	arch/x86/mm/mem_encrypt.c
#	lib/swiotlb.c
diff --cc arch/x86/kernel/pci-dma.c
index fdf8cd04ed07,0accc2404b92..000000000000
--- a/arch/x86/kernel/pci-dma.c
+++ b/arch/x86/kernel/pci-dma.c
@@@ -101,11 -90,15 +101,23 @@@ void *dma_generic_alloc_coherent(struc
  again:
  	page = NULL;
  	/* CMA can be used only in the context which permits sleeping */
++<<<<<<< HEAD
 +	if (flag & __GFP_WAIT) {
 +		page = dma_alloc_from_contiguous(dev, count, get_order(size));
 +		if (page && page_to_phys(page) + size > dma_mask) {
 +			dma_release_from_contiguous(dev, page, count);
 +			page = NULL;
++=======
+ 	if (gfpflags_allow_blocking(flag)) {
+ 		page = dma_alloc_from_contiguous(dev, count, get_order(size),
+ 						 flag);
+ 		if (page) {
+ 			addr = phys_to_dma(dev, page_to_phys(page));
+ 			if (addr + size > dma_mask) {
+ 				dma_release_from_contiguous(dev, page, count);
+ 				page = NULL;
+ 			}
++>>>>>>> c7753208a94c (x86, swiotlb: Add memory encryption support)
  		}
  	}
  	/* fallback */
diff --cc lib/swiotlb.c
index 542ff4e0a8d4,04ac91acf193..000000000000
--- a/lib/swiotlb.c
+++ b/lib/swiotlb.c
@@@ -29,6 -29,8 +29,11 @@@
  #include <linux/ctype.h>
  #include <linux/highmem.h>
  #include <linux/gfp.h>
++<<<<<<< HEAD
++=======
+ #include <linux/scatterlist.h>
+ #include <linux/mem_encrypt.h>
++>>>>>>> c7753208a94c (x86, swiotlb: Add memory encryption support)
  
  #include <asm/io.h>
  #include <asm/dma.h>
@@@ -524,12 -607,21 +566,24 @@@ EXPORT_SYMBOL_GPL(swiotlb_tbl_map_singl
   * Allocates bounce buffer and returns its kernel virtual address.
   */
  
 -static phys_addr_t
 -map_single(struct device *hwdev, phys_addr_t phys, size_t size,
 -	   enum dma_data_direction dir, unsigned long attrs)
 +phys_addr_t map_single(struct device *hwdev, phys_addr_t phys, size_t size,
 +		       enum dma_data_direction dir)
  {
 -	dma_addr_t start_dma_addr;
 +	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);
  
++<<<<<<< HEAD
 +	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size, dir);
++=======
+ 	if (swiotlb_force == SWIOTLB_NO_FORCE) {
+ 		dev_warn_ratelimited(hwdev, "Cannot do DMA to address %pa\n",
+ 				     &phys);
+ 		return SWIOTLB_MAP_ERROR;
+ 	}
+ 
+ 	start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);
+ 	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size,
+ 				      dir, attrs);
++>>>>>>> c7753208a94c (x86, swiotlb: Add memory encryption support)
  }
  
  /*
@@@ -635,12 -734,13 +689,12 @@@ swiotlb_alloc_coherent(struct device *h
  		 * GFP_DMA memory; fall back on map_single(), which
  		 * will grab memory from the lowest available address range.
  		 */
 -		phys_addr_t paddr = map_single(hwdev, 0, size,
 -					       DMA_FROM_DEVICE, 0);
 +		phys_addr_t paddr = map_single(hwdev, 0, size, DMA_FROM_DEVICE);
  		if (paddr == SWIOTLB_MAP_ERROR)
 -			goto err_warn;
 +			return NULL;
  
  		ret = phys_to_virt(paddr);
- 		dev_addr = phys_to_dma(hwdev, paddr);
+ 		dev_addr = swiotlb_phys_to_dma(hwdev, paddr);
  
  		/* Confirm address can be DMA'd by device */
  		if (dev_addr + size - 1 > dma_mask) {
@@@ -723,25 -841,28 +777,32 @@@ dma_addr_t swiotlb_map_page(struct devi
  	 * we can safely return the device addr and not worry about bounce
  	 * buffering it.
  	 */
 -	if (dma_capable(dev, dev_addr, size) && swiotlb_force != SWIOTLB_FORCE)
 +	if (dma_capable(dev, dev_addr, size) && !swiotlb_force)
  		return dev_addr;
  
 -	trace_swiotlb_bounced(dev, dev_addr, size, swiotlb_force);
 -
  	/* Oh well, have to allocate and map a bounce buffer. */
 -	map = map_single(dev, phys, size, dir, attrs);
 +	map = map_single(dev, phys, size, dir);
  	if (map == SWIOTLB_MAP_ERROR) {
  		swiotlb_full(dev, size, dir, 1);
- 		return phys_to_dma(dev, io_tlb_overflow_buffer);
+ 		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);
  	}
  
- 	dev_addr = phys_to_dma(dev, map);
+ 	dev_addr = swiotlb_phys_to_dma(dev, map);
  
  	/* Ensure that the address returned is DMA'ble */
 -	if (dma_capable(dev, dev_addr, size))
 -		return dev_addr;
 +	if (!dma_capable(dev, dev_addr, size)) {
 +		swiotlb_tbl_unmap_single(dev, map, size, dir);
 +		return phys_to_dma(dev, io_tlb_overflow_buffer);
 +	}
  
++<<<<<<< HEAD
 +	return dev_addr;
++=======
+ 	attrs |= DMA_ATTR_SKIP_CPU_SYNC;
+ 	swiotlb_tbl_unmap_single(dev, map, size, dir, attrs);
+ 
+ 	return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);
++>>>>>>> c7753208a94c (x86, swiotlb: Add memory encryption support)
  }
  EXPORT_SYMBOL_GPL(swiotlb_map_page);
  
@@@ -868,15 -990,16 +929,15 @@@ swiotlb_map_sg_attrs(struct device *hwd
  				/* Don't panic here, we expect map_sg users
  				   to do proper error handling. */
  				swiotlb_full(hwdev, sg->length, dir, 0);
 -				attrs |= DMA_ATTR_SKIP_CPU_SYNC;
  				swiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,
  						       attrs);
 -				sg_dma_len(sgl) = 0;
 +				sgl[0].dma_length = 0;
  				return 0;
  			}
- 			sg->dma_address = phys_to_dma(hwdev, map);
+ 			sg->dma_address = swiotlb_phys_to_dma(hwdev, map);
  		} else
  			sg->dma_address = dev_addr;
 -		sg_dma_len(sg) = sg->length;
 +		sg->dma_length = sg->length;
  	}
  	return nelems;
  }
* Unmerged path arch/x86/include/asm/mem_encrypt.h
* Unmerged path arch/x86/mm/mem_encrypt.c
diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h
index 1f5b7287d1ad..7dd0799052b2 100644
--- a/arch/x86/include/asm/dma-mapping.h
+++ b/arch/x86/include/asm/dma-mapping.h
@@ -14,6 +14,7 @@
 #include <asm/swiotlb.h>
 #include <asm-generic/dma-coherent.h>
 #include <linux/dma-contiguous.h>
+#include <linux/mem_encrypt.h>
 
 #ifdef CONFIG_ISA
 # define ISA_DMA_BIT_MASK DMA_BIT_MASK(24)
@@ -84,12 +85,12 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
-	return paddr;
+	return __sme_set(paddr);
 }
 
 static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 {
-	return daddr;
+	return __sme_clr(daddr);
 }
 #endif /* CONFIG_X86_DMA_REMAP */
 
* Unmerged path arch/x86/include/asm/mem_encrypt.h
* Unmerged path arch/x86/kernel/pci-dma.c
diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c
index da15918d1c81..ca2b820ca9f4 100644
--- a/arch/x86/kernel/pci-nommu.c
+++ b/arch/x86/kernel/pci-nommu.c
@@ -30,7 +30,7 @@ static dma_addr_t nommu_map_page(struct device *dev, struct page *page,
 				 enum dma_data_direction dir,
 				 struct dma_attrs *attrs)
 {
-	dma_addr_t bus = page_to_phys(page) + offset;
+	dma_addr_t bus = phys_to_dma(dev, page_to_phys(page)) + offset;
 	WARN_ON(size == 0);
 	if (!check_addr("map_single", dev, bus, size))
 		return DMA_ERROR_CODE;
diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c
index 48f97556cf8c..4853440ea92b 100644
--- a/arch/x86/kernel/pci-swiotlb.c
+++ b/arch/x86/kernel/pci-swiotlb.c
@@ -6,12 +6,14 @@
 #include <linux/swiotlb.h>
 #include <linux/bootmem.h>
 #include <linux/dma-mapping.h>
+#include <linux/mem_encrypt.h>
 
 #include <asm/iommu.h>
 #include <asm/swiotlb.h>
 #include <asm/dma.h>
 #include <asm/xen/swiotlb-xen.h>
 #include <asm/iommu_table.h>
+
 int swiotlb __read_mostly;
 
 static void *x86_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
@@ -71,8 +73,8 @@ IOMMU_INIT_FINISH(pci_swiotlb_detect_override,
 		  pci_swiotlb_late_init);
 
 /*
- * if 4GB or more detected (and iommu=off not set) return 1
- * and set swiotlb to 1.
+ * If 4GB or more detected (and iommu=off not set) or if SME is active
+ * then set swiotlb to 1 and return 1.
  */
 int __init pci_swiotlb_detect_4gb(void)
 {
@@ -81,6 +83,15 @@ int __init pci_swiotlb_detect_4gb(void)
 	if (!no_iommu && max_possible_pfn > MAX_DMA32_PFN)
 		swiotlb = 1;
 #endif
+
+	/*
+	 * If SME is active then swiotlb will be set to 1 so that bounce
+	 * buffers are allocated and used for devices that do not support
+	 * the addressing range required for the encryption mask.
+	 */
+	if (sme_active())
+		swiotlb = 1;
+
 	return swiotlb;
 }
 IOMMU_INIT(pci_swiotlb_detect_4gb,
* Unmerged path arch/x86/mm/mem_encrypt.c
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index a5ffd32642fd..16c296a12560 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -27,6 +27,7 @@ int swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose);
 extern unsigned long swiotlb_nr_tbl(void);
 unsigned long swiotlb_size_or_default(void);
 extern int swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs);
+extern void __init swiotlb_update_mem_attributes(void);
 
 /*
  * Enumeration for sync targets
diff --git a/init/main.c b/init/main.c
index d7bc4431b530..5cf0c7273f4d 100644
--- a/init/main.c
+++ b/init/main.c
@@ -462,6 +462,8 @@ void __init __weak thread_info_cache_init(void)
 }
 #endif
 
+void __init __weak mem_encrypt_init(void) { }
+
 /*
  * Set up kernel memory allocators
  */
@@ -595,6 +597,14 @@ asmlinkage void __init start_kernel(void)
 	 */
 	locking_selftest();
 
+	/*
+	 * This needs to be called before any devices perform DMA
+	 * operations that might use the SWIOTLB bounce buffers. It will
+	 * mark the bounce buffers as decrypted so that their usage will
+	 * not cause "plain-text" data to be decrypted when accessed.
+	 */
+	mem_encrypt_init();
+
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (initrd_start && !initrd_below_start_ok &&
 	    page_to_pfn(virt_to_page((void *)initrd_start)) < min_low_pfn) {
* Unmerged path lib/swiotlb.c
