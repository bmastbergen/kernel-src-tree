x86/tsc: Remove the TSC_ADJUST clamp

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] tsc: Remove the TSC_ADJUST clamp (Frank Ramsay) [1526066]
Rebuild_FUZZ: 94.12%
commit-author Peter Zijlstra <peterz@infradead.org>
commit 855615eee9b1989cac8ec5eaae4562db081a239b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/855615ee.failed

Now that all affected platforms have a microcode update; and we check
this and disable TSC_DEADLINE and print a microcode revision update
error if its too old, we can remove the TSC_ADJUST clamp.

This should help with systems where the second socket runs ahead of
the first socket and needs a negative adjustment. In this case we'd
hit the 0 clamp and give up for not achieving synchronization.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Cc: kevin.b.stanton@intel.com
Link: http://lkml.kernel.org/r/20170531155306.100950003@infradead.org
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 855615eee9b1989cac8ec5eaae4562db081a239b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/tsc_sync.c
diff --cc arch/x86/kernel/tsc_sync.c
index 68270eb35526,7842371bc9e4..000000000000
--- a/arch/x86/kernel/tsc_sync.c
+++ b/arch/x86/kernel/tsc_sync.c
@@@ -58,10 -58,33 +58,36 @@@ void tsc_verify_tsc_adjust(void
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void tsc_sanitize_first_cpu(struct tsc_adjust *cur, s64 bootval,
+ 				   unsigned int cpu, bool bootcpu)
+ {
+ 	/*
+ 	 * First online CPU in a package stores the boot value in the
+ 	 * adjustment value. This value might change later via the sync
+ 	 * mechanism. If that fails we still can yell about boot values not
+ 	 * being consistent.
+ 	 *
+ 	 * On the boot cpu we just force set the ADJUST value to 0 if it's
+ 	 * non zero. We don't do that on non boot cpus because physical
+ 	 * hotplug should have set the ADJUST register to a value > 0 so
+ 	 * the TSC is in sync with the already running cpus.
+ 	 */
+ 	if (bootcpu && bootval != 0) {
+ 		pr_warn(FW_BUG "TSC ADJUST: CPU%u: %lld force to 0\n", cpu,
+ 			bootval);
+ 		wrmsrl(MSR_IA32_TSC_ADJUST, 0);
+ 		bootval = 0;
+ 	}
+ 	cur->adjusted = bootval;
+ }
+ 
++>>>>>>> 855615eee9b1 (x86/tsc: Remove the TSC_ADJUST clamp)
  #ifndef CONFIG_SMP
 -bool __init tsc_store_and_check_tsc_adjust(bool bootcpu)
 +bool __init tsc_store_and_check_tsc_adjust(void)
  {
 -	struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
 +	struct tsc_adjust *ref, *cur = this_cpu_ptr(&tsc_adjust);
  	s64 bootval;
  
  	if (!boot_cpu_has(X86_FEATURE_TSC_ADJUST))
* Unmerged path arch/x86/kernel/tsc_sync.c
