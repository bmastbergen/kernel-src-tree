xfs: handle 64-bit length in xfs_iozero

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 7bb41db3ea160ea55cc46af07e45f7cb1e2968ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7bb41db3.failed

We'll want to use this code for large offsets now that we're
skipping holes and unwritten extents efficiently.  Also rename it to
xfs_zero_range to be a bit more descriptive, and tell the caller if
we actually did any zeroing.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Dave Chinner <dchinner@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>


(cherry picked from commit 7bb41db3ea160ea55cc46af07e45f7cb1e2968ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_file.c
index 4da908a64865,294e5f423028..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -81,61 -81,17 +81,69 @@@ xfs_rw_ilock_demote
  }
  
  /*
 - * Clear the specified ranges to zero through either the pagecache or DAX.
 - * Holes and unwritten extents will be left as-is as they already are zeroed.
 + * xfs_iozero clears the specified range supplied via the page cache (except in
 + * the DAX case). Writes through the page cache will allocate blocks over holes,
 + * though the callers usually map the holes first and avoid them. If a block is
 + * not completely zeroed, then it will be read from disk before being partially
 + * zeroed.
 + *
 + * In the DAX case, we can just directly write to the underlying pages. This
 + * will not allocate blocks, but will avoid holes and unwritten extents and so
 + * not do unnecessary work.
   */
  int
++<<<<<<< HEAD
 +xfs_iozero(
 +	struct xfs_inode	*ip,	/* inode			*/
 +	loff_t			pos,	/* offset in file		*/
 +	size_t			count)	/* size of data to zero		*/
++=======
+ xfs_zero_range(
+ 	struct xfs_inode	*ip,
+ 	xfs_off_t		pos,
+ 	xfs_off_t		count,
+ 	bool			*did_zero)
++>>>>>>> 7bb41db3ea16 (xfs: handle 64-bit length in xfs_iozero)
  {
 -	return iomap_zero_range(VFS_I(ip), pos, count, NULL, &xfs_iomap_ops);
 +	struct page		*page;
 +	struct address_space	*mapping;
 +	int			status = 0;
 +
 +
 +	mapping = VFS_I(ip)->i_mapping;
 +	do {
 +		unsigned offset, bytes;
 +		void *fsdata;
 +
 +		offset = (pos & (PAGE_CACHE_SIZE -1)); /* Within page */
 +		bytes = PAGE_CACHE_SIZE - offset;
 +		if (bytes > count)
 +			bytes = count;
 +
 +		if (IS_DAX(VFS_I(ip))) {
 +			status = dax_zero_page_range(VFS_I(ip), pos, bytes,
 +						     xfs_get_blocks_direct);
 +			if (status)
 +				break;
 +		} else {
 +			status = pagecache_write_begin(NULL, mapping, pos, bytes,
 +						AOP_FLAG_UNINTERRUPTIBLE,
 +						&page, &fsdata);
 +			if (status)
 +				break;
 +
 +			zero_user(page, offset, bytes);
 +
 +			status = pagecache_write_end(NULL, mapping, pos, bytes,
 +						bytes, page, fsdata);
 +			WARN_ON(status <= 0); /* can't return less than zero! */
 +			status = 0;
 +		}
 +		pos += bytes;
 +		count -= bytes;
 +	} while (count);
 +
 +	return status;
  }
  
  int
* Unmerged path fs/xfs/xfs_file.c
diff --git a/fs/xfs/xfs_inode.h b/fs/xfs/xfs_inode.h
index afefb072b335..55d40c753a00 100644
--- a/fs/xfs/xfs_inode.h
+++ b/fs/xfs/xfs_inode.h
@@ -436,7 +436,8 @@ int	xfs_update_prealloc_flags(struct xfs_inode *ip,
 				  enum xfs_prealloc_flags flags);
 int	xfs_zero_eof(struct xfs_inode *ip, xfs_off_t offset,
 		     xfs_fsize_t isize, bool *did_zeroing);
-int	xfs_iozero(struct xfs_inode *ip, loff_t pos, size_t count);
+int	xfs_zero_range(struct xfs_inode *ip, xfs_off_t pos, xfs_off_t count,
+		bool *did_zero);
 loff_t	__xfs_seek_hole_data(struct inode *inode, loff_t start,
 			     loff_t eof, int whence);
 
