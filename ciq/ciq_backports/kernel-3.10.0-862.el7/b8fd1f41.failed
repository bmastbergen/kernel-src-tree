crypto: chcr - Add ctr mode and process large sg entries for cipher

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [crypto] chcr - Add ctr mode and process large sg entries for cipher (Arjun Vynipadath) [1458315]
Rebuild_FUZZ: 93.65%
commit-author Harsh Jain <harsh@chelsio.com>
commit b8fd1f4170e7e8bda45d7bcc750e909c859ec714
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b8fd1f41.failed

It send multiple WRs to H/W to handle large sg lists. Adds ctr(aes)
and rfc(ctr(aes)) modes.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit b8fd1f4170e7e8bda45d7bcc750e909c859ec714)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/chelsio/chcr_algo.h
#	drivers/crypto/chelsio/chcr_core.c
#	drivers/crypto/chelsio/chcr_crypto.h
diff --cc drivers/crypto/chelsio/chcr_algo.c
index bda117371c9f,03b817f185dd..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -54,6 -54,14 +54,17 @@@
  #include <crypto/algapi.h>
  #include <crypto/hash.h>
  #include <crypto/sha.h>
++<<<<<<< HEAD
++=======
+ #include <crypto/authenc.h>
+ #include <crypto/ctr.h>
+ #include <crypto/gf128mul.h>
+ #include <crypto/internal/aead.h>
+ #include <crypto/null.h>
+ #include <crypto/internal/skcipher.h>
+ #include <crypto/aead.h>
+ #include <crypto/scatterwalk.h>
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  #include <crypto/internal/hash.h>
  
  #include "t4fw_api.h"
@@@ -108,22 -156,26 +119,44 @@@ int chcr_handle_resp(struct crypto_asyn
  	unsigned int digestsize, updated_digestsize;
  
  	switch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {
++<<<<<<< HEAD
 +	case CRYPTO_ALG_TYPE_BLKCIPHER:
 +		ctx_req.req.ablk_req = (struct ablkcipher_request *)req;
 +		ctx_req.ctx.ablk_ctx =
 +			ablkcipher_request_ctx(ctx_req.req.ablk_req);
 +		if (!error_status) {
 +			fw6_pld = (struct cpl_fw6_pld *)input;
 +			memcpy(ctx_req.req.ablk_req->info, &fw6_pld->data[2],
 +			       AES_BLOCK_SIZE);
 +		}
 +		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.req.ablk_req->dst,
 +			     ctx_req.ctx.ablk_ctx->dst_nents, DMA_FROM_DEVICE);
 +		if (ctx_req.ctx.ablk_ctx->skb) {
 +			kfree_skb(ctx_req.ctx.ablk_ctx->skb);
 +			ctx_req.ctx.ablk_ctx->skb = NULL;
 +		}
++=======
+ 	case CRYPTO_ALG_TYPE_AEAD:
+ 		ctx_req.req.aead_req = aead_request_cast(req);
+ 		ctx_req.ctx.reqctx = aead_request_ctx(ctx_req.req.aead_req);
+ 		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.ctx.reqctx->dst,
+ 			     ctx_req.ctx.reqctx->dst_nents, DMA_FROM_DEVICE);
+ 		if (ctx_req.ctx.reqctx->skb) {
+ 			kfree_skb(ctx_req.ctx.reqctx->skb);
+ 			ctx_req.ctx.reqctx->skb = NULL;
+ 		}
+ 		if (ctx_req.ctx.reqctx->verify == VERIFY_SW) {
+ 			chcr_verify_tag(ctx_req.req.aead_req, input,
+ 					&err);
+ 			ctx_req.ctx.reqctx->verify = VERIFY_HW;
+ 		}
+ 		ctx_req.req.aead_req->base.complete(req, err);
+ 		break;
+ 
+ 	case CRYPTO_ALG_TYPE_ABLKCIPHER:
+ 		 err = chcr_handle_cipher_resp(ablkcipher_request_cast(req),
+ 					       input, err);
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  		break;
  
  	case CRYPTO_ALG_TYPE_AHASH:
@@@ -152,9 -204,10 +185,10 @@@
  			       sizeof(struct cpl_fw6_pld),
  			       updated_digestsize);
  		}
+ 		ctx_req.req.ahash_req->base.complete(req, err);
  		break;
  	}
 -	return err;
 +	return 0;
  }
  
  /*
@@@ -579,10 -699,11 +671,16 @@@ static struct sk_buff *create_cipher_wr
  		goto map_fail1;
  
  	skb_set_transport_header(skb, transhdr_len);
- 	memcpy(reqctx->iv, req->info, ivsize);
  	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
++<<<<<<< HEAD
 +	write_sg_to_skb(skb, &frags, req->src, req->nbytes);
 +	create_wreq(ctx, chcr_req, req, skb, kctx_len, 0, phys_dsgl);
++=======
+ 	write_sg_to_skb(skb, &frags, wrparam->srcsg, wrparam->bytes);
+ 	create_wreq(ctx, chcr_req, &(wrparam->req->base), skb, kctx_len, 0, 1,
+ 			sizeof(struct cpl_rx_phys_dsgl) + phys_dsgl,
+ 			ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC);
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  	reqctx->skb = skb;
  	skb_get(skb);
  	return skb;
@@@ -852,8 -1394,8 +1371,13 @@@ static struct sk_buff *create_hash_wr(s
  	if (param->sg_len != 0)
  		write_sg_to_skb(skb, &frags, req->src, param->sg_len);
  
++<<<<<<< HEAD
 +	create_wreq(ctx, chcr_req, req, skb, kctx_len, hash_size_in_response,
 +		    0);
++=======
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len,
+ 		    hash_size_in_response, 0, DUMMY_BYTES, 0);
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  	req_ctx->skb = skb;
  	skb_get(skb);
  	return skb;
@@@ -1260,6 -1803,1235 +1785,1238 @@@ static void chcr_hmac_cra_exit(struct c
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int chcr_copy_assoc(struct aead_request *req,
+ 				struct chcr_aead_ctx *ctx)
+ {
+ 	SKCIPHER_REQUEST_ON_STACK(skreq, ctx->null);
+ 
+ 	skcipher_request_set_tfm(skreq, ctx->null);
+ 	skcipher_request_set_callback(skreq, aead_request_flags(req),
+ 			NULL, NULL);
+ 	skcipher_request_set_crypt(skreq, req->src, req->dst, req->assoclen,
+ 			NULL);
+ 
+ 	return crypto_skcipher_encrypt(skreq);
+ }
+ static int chcr_aead_need_fallback(struct aead_request *req, int src_nent,
+ 				   int aadmax, int wrlen,
+ 				   unsigned short op_type)
+ {
+ 	unsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));
+ 
+ 	if (((req->cryptlen - (op_type ? authsize : 0)) == 0) ||
+ 	    (req->assoclen > aadmax) ||
+ 	    (src_nent > MAX_SKB_FRAGS) ||
+ 	    (wrlen > MAX_WR_SIZE))
+ 		return 1;
+ 	return 0;
+ }
+ 
+ static int chcr_aead_fallback(struct aead_request *req, unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 
+ 	aead_request_set_tfm(subreq, aeadctx->sw_cipher);
+ 	aead_request_set_callback(subreq, req->base.flags,
+ 				  req->base.complete, req->base.data);
+ 	 aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 				 req->iv);
+ 	 aead_request_set_ad(subreq, req->assoclen);
+ 	return op_type ? crypto_aead_decrypt(subreq) :
+ 		crypto_aead_encrypt(subreq);
+ }
+ 
+ static struct sk_buff *create_authenc_wr(struct aead_request *req,
+ 					 unsigned short qid,
+ 					 int size,
+ 					 unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = crypto_aead_ivsize(tfm), dst_size = 0;
+ 	unsigned int   kctx_len = 0;
+ 	unsigned short stop_offset = 0;
+ 	unsigned int  assoclen = req->assoclen;
+ 	unsigned int  authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	int null = 0;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 	if (aeadctx->enckey_len == 0 || (req->cryptlen == 0))
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error)
+ 			return ERR_PTR(error);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {
+ 		null = 1;
+ 		assoclen = 0;
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("AUTHENC:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)
+ 		- sizeof(chcr_req->key_ctx);
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_AUTH_SG,
+ 			T6_MAX_AAD_SIZE,
+ 			transhdr_len + (sgl_len(src_nent + MIN_AUTH_SG) * 8),
+ 				op_type)) {
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	/* LLD is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	/* Write WR */
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	stop_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 
+ 	/*
+ 	 * Input order	is AAD,IV and Payload. where IV should be included as
+ 	 * the part of authdata. All other fields should be filled according
+ 	 * to the hardware spec
+ 	 */
+ 	chcr_req->sec_cpl.op_ivinsrtofst =
+ 		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,
+ 				       (ivsize ? (assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1,
+ 					(stop_offset & 0x1F0) >> 4);
+ 	chcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(
+ 					stop_offset & 0xF,
+ 					null ? 0 : assoclen + ivsize + 1,
+ 					stop_offset, stop_offset);
+ 	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_CBC,
+ 					actx->auth_mode, aeadctx->hmac_ctrl,
+ 					ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					 0, 1, dst_size);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	if (op_type == CHCR_ENCRYPT_OP)
+ 		memcpy(chcr_req->key_ctx.key, aeadctx->key,
+ 		       aeadctx->enckey_len);
+ 	else
+ 		memcpy(chcr_req->key_ctx.key, actx->dec_rrkey,
+ 		       aeadctx->enckey_len);
+ 
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<
+ 					4), actx->h_iopad, kctx_len -
+ 				(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 					reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 
+ 	if (assoclen) {
+ 		/* AAD buffer in */
+ 		write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 
+ 	}
+ 	write_buffer_to_skb(skb, &frags, req->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
+ 		   sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 
+ 	return skb;
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(error);
+ }
+ 
+ static int set_msg_len(u8 *block, unsigned int msglen, int csize)
+ {
+ 	__be32 data;
+ 
+ 	memset(block, 0, csize);
+ 	block += csize;
+ 
+ 	if (csize >= 4)
+ 		csize = 4;
+ 	else if (msglen > (unsigned int)(1 << (8 * csize)))
+ 		return -EOVERFLOW;
+ 
+ 	data = cpu_to_be32(msglen);
+ 	memcpy(block - csize, (u8 *)&data + 4 - csize, csize);
+ 
+ 	return 0;
+ }
+ 
+ static void generate_b0(struct aead_request *req,
+ 			struct chcr_aead_ctx *aeadctx,
+ 			unsigned short op_type)
+ {
+ 	unsigned int l, lp, m;
+ 	int rc;
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	u8 *b0 = reqctx->scratch_pad;
+ 
+ 	m = crypto_aead_authsize(aead);
+ 
+ 	memcpy(b0, reqctx->iv, 16);
+ 
+ 	lp = b0[0];
+ 	l = lp + 1;
+ 
+ 	/* set m, bits 3-5 */
+ 	*b0 |= (8 * ((m - 2) / 2));
+ 
+ 	/* set adata, bit 6, if associated data is used */
+ 	if (req->assoclen)
+ 		*b0 |= 64;
+ 	rc = set_msg_len(b0 + 16 - l,
+ 			 (op_type == CHCR_DECRYPT_OP) ?
+ 			 req->cryptlen - m : req->cryptlen, l);
+ }
+ 
+ static inline int crypto_ccm_check_iv(const u8 *iv)
+ {
+ 	/* 2 <= L <= 8, so 1 <= L' <= 7. */
+ 	if (iv[0] < 1 || iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int ccm_format_packet(struct aead_request *req,
+ 			     struct chcr_aead_ctx *aeadctx,
+ 			     unsigned int sub_type,
+ 			     unsigned short op_type)
+ {
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int rc = 0;
+ 
+ 	if (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		reqctx->iv[0] = 3;
+ 		memcpy(reqctx->iv + 1, &aeadctx->salt[0], 3);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 		memset(reqctx->iv + 12, 0, 4);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen - 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 16);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen);
+ 	}
+ 	generate_b0(req, aeadctx, op_type);
+ 	/* zero the ctr value */
+ 	memset(reqctx->iv + 15 - reqctx->iv[0], 0, reqctx->iv[0] + 1);
+ 	return rc;
+ }
+ 
+ static void fill_sec_cpl_for_aead(struct cpl_tx_sec_pdu *sec_cpl,
+ 				  unsigned int dst_size,
+ 				  struct aead_request *req,
+ 				  unsigned short op_type,
+ 					  struct chcr_context *chcrctx)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int cipher_mode = CHCR_SCMD_CIPHER_MODE_AES_CCM;
+ 	unsigned int mac_mode = CHCR_SCMD_AUTH_MODE_CBCMAC;
+ 	unsigned int c_id = chcrctx->dev->rx_channel_id;
+ 	unsigned int ccm_xtra;
+ 	unsigned char tag_offset = 0, auth_offset = 0;
+ 	unsigned int assoclen;
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 		assoclen = req->assoclen - 8;
+ 	else
+ 		assoclen = req->assoclen;
+ 	ccm_xtra = CCM_B0_SIZE +
+ 		((assoclen) ? CCM_AAD_FIELD_SIZE : 0);
+ 
+ 	auth_offset = req->cryptlen ?
+ 		(assoclen + ivsize + 1 + ccm_xtra) : 0;
+ 	if (op_type == CHCR_DECRYPT_OP) {
+ 		if (crypto_aead_authsize(tfm) != req->cryptlen)
+ 			tag_offset = crypto_aead_authsize(tfm);
+ 		else
+ 			auth_offset = 0;
+ 	}
+ 
+ 
+ 	sec_cpl->op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(c_id,
+ 					 2, (ivsize ?  (assoclen + 1) :  0) +
+ 					 ccm_xtra);
+ 	sec_cpl->pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen + ccm_xtra);
+ 	/* For CCM there wil be b0 always. So AAD start will be 1 always */
+ 	sec_cpl->aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					1, assoclen + ccm_xtra, assoclen
+ 					+ ivsize + 1 + ccm_xtra, 0);
+ 
+ 	sec_cpl->cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(0,
+ 					auth_offset, tag_offset,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 :
+ 					crypto_aead_authsize(tfm));
+ 	sec_cpl->seqno_numivs =  FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 : 1,
+ 					cipher_mode, mac_mode,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 
+ 	sec_cpl->ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1, 0,
+ 					1, dst_size);
+ }
+ 
+ int aead_ccm_validate_input(unsigned short op_type,
+ 			    struct aead_request *req,
+ 			    struct chcr_aead_ctx *aeadctx,
+ 			    unsigned int sub_type)
+ {
+ 	if (sub_type != CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		if (crypto_ccm_check_iv(req->iv)) {
+ 			pr_err("CCM: IV check fails\n");
+ 			return -EINVAL;
+ 		}
+ 	} else {
+ 		if (req->assoclen != 16 && req->assoclen != 20) {
+ 			pr_err("RFC4309: Invalid AAD length %d\n",
+ 			       req->assoclen);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	if (aeadctx->enckey_len == 0) {
+ 		pr_err("CCM: Encryption key not set\n");
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ unsigned int fill_aead_req_fields(struct sk_buff *skb,
+ 				  struct aead_request *req,
+ 				  struct scatterlist *src,
+ 				  unsigned int ivsize,
+ 				  struct chcr_aead_ctx *aeadctx)
+ {
+ 	unsigned int frags = 0;
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	/* b0 and aad length(if available) */
+ 
+ 	write_buffer_to_skb(skb, &frags, reqctx->scratch_pad, CCM_B0_SIZE +
+ 				(req->assoclen ?  CCM_AAD_FIELD_SIZE : 0));
+ 	if (req->assoclen) {
+ 		if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 			write_sg_to_skb(skb, &frags, req->src,
+ 					req->assoclen - 8);
+ 		else
+ 			write_sg_to_skb(skb, &frags, req->src, req->assoclen);
+ 	}
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	if (req->cryptlen)
+ 		write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 
+ 	return frags;
+ }
+ 
+ static struct sk_buff *create_aead_ccm_wr(struct aead_request *req,
+ 					  unsigned short qid,
+ 					  int size,
+ 					  unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len, ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len;
+ 	unsigned int sub_type;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	sub_type = get_aead_subtype(tfm);
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error) {
+ 			pr_err("AAD copy to destination buffer fails\n");
+ 			return ERR_PTR(error);
+ 		}
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("CCM:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 	error = aead_ccm_validate_input(op_type, req, aeadctx, sub_type);
+ 	if (error)
+ 		goto err;
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) * 2;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_CCM_SG,
+ 			    T6_MAX_AAD_SIZE - 18,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_CCM_SG) * 8),
+ 			    op_type)) {
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)),  flags);
+ 
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	fill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, op_type, ctx);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 					16), aeadctx->key, aeadctx->enckey_len);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	error = ccm_format_packet(req, aeadctx, sub_type, op_type);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 				 reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	frags = fill_aead_req_fields(skb, req, src, ivsize, aeadctx);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, 0, 1,
+ 		    sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ dstmap_fail:
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(error);
+ }
+ 
+ static struct sk_buff *create_gcm_wr(struct aead_request *req,
+ 				     unsigned short qid,
+ 				     int size,
+ 				     unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len, assoclen = req->assoclen;
+ 	unsigned char tag_offset = 0;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 
+ 	/* validate key size */
+ 	if (aeadctx->enckey_len == 0)
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, assoclen);
+ 	reqctx->dst = src;
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error)
+ 			return	ERR_PTR(error);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       assoclen);
+ 	}
+ 
+ 
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("GCM:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_GCM_SG,
+ 			    T6_MAX_AAD_SIZE,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_GCM_SG) * 8),
+ 			    op_type)) {
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	/* NIC driver is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)
+ 		assoclen = req->assoclen - 8;
+ 
+ 	tag_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 	chcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(
+ 					ctx->dev->rx_channel_id, 2, (ivsize ?
+ 					(assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1, 0);
+ 		chcr_req->sec_cpl.cipherstop_lo_authinsert =
+ 			FILL_SEC_CPL_AUTHINSERT(0, assoclen + ivsize + 1,
+ 						tag_offset, tag_offset);
+ 		chcr_req->sec_cpl.seqno_numivs =
+ 			FILL_SEC_CPL_SCMD0_SEQNO(op_type, (op_type ==
+ 					CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_GCM,
+ 					CHCR_SCMD_AUTH_MODE_GHASH,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					0, 1, dst_size);
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 				16), GCM_CTX(aeadctx)->ghash_h, AEAD_H_SIZE);
+ 
+ 	/* prepare a 16 byte iv */
+ 	/* S   A   L  T |  IV | 0x00000001 */
+ 	if (get_aead_subtype(tfm) ==
+ 	    CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) {
+ 		memcpy(reqctx->iv, aeadctx->salt, 4);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 12);
+ 	}
+ 	*((unsigned int *)(reqctx->iv + 12)) = htonl(0x01);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 					  reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
+ 			sizeof(struct cpl_rx_phys_dsgl) + dst_size,
+ 			reqctx->verify);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ 
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(error);
+ }
+ 
+ 
+ 
+ static int chcr_aead_cra_init(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_alg *alg = crypto_aead_alg(tfm);
+ 
+ 	aeadctx->sw_cipher = crypto_alloc_aead(alg->base.cra_name, 0,
+ 					       CRYPTO_ALG_NEED_FALLBACK |
+ 					       CRYPTO_ALG_ASYNC);
+ 	if  (IS_ERR(aeadctx->sw_cipher))
+ 		return PTR_ERR(aeadctx->sw_cipher);
+ 	crypto_aead_set_reqsize(tfm, max(sizeof(struct chcr_aead_reqctx),
+ 				 sizeof(struct aead_request) +
+ 				 crypto_aead_reqsize(aeadctx->sw_cipher)));
+ 	aeadctx->null = crypto_get_default_null_skcipher();
+ 	if (IS_ERR(aeadctx->null))
+ 		return PTR_ERR(aeadctx->null);
+ 	return chcr_device_init(ctx);
+ }
+ 
+ static void chcr_aead_cra_exit(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 
+ 	crypto_put_default_null_skcipher();
+ 	crypto_free_aead(aeadctx->sw_cipher);
+ }
+ 
+ static int chcr_authenc_null_setauthsize(struct crypto_aead *tfm,
+ 					unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NOP;
+ 	aeadctx->mayverify = VERIFY_HW;
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ static int chcr_authenc_setauthsize(struct crypto_aead *tfm,
+ 				    unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	u32 maxauth = crypto_aead_maxauthsize(tfm);
+ 
+ 	/*SHA1 authsize in ipsec is 12 instead of 10 i.e maxauthsize / 2 is not
+ 	 * true for sha1. authsize == 12 condition should be before
+ 	 * authsize == (maxauth >> 1)
+ 	 */
+ 	if (authsize == ICV_4) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_6) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_10) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_12) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_14) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == (maxauth >> 1)) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == maxauth) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ 
+ static int chcr_gcm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_13:
+ 	case ICV_15:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 		break;
+ 	default:
+ 
+ 		  crypto_tfm_set_flags((struct crypto_tfm *) tfm,
+ 			CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_4106_4309_setauthsize(struct crypto_aead *tfm,
+ 					  unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_setauthsize(struct crypto_aead *tfm,
+ 				unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_6:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_10:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_common_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	unsigned char ck_size, mk_size;
+ 	int key_ctx_size = 0;
+ 
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4)  * 2;
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		mk_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, mk_size, 0, 0,
+ 						key_ctx_size >> 4);
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 
+ 	return 0;
+ }
+ 
+ static int chcr_aead_ccm_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	int error;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &
+ 			      CRYPTO_TFM_REQ_MASK);
+ 	error = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (error)
+ 		return error;
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_aead_rfc4309_setkey(struct crypto_aead *aead, const u8 *key,
+ 				    unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	int error;
+ 
+ 	if (keylen < 3) {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &
+ 			      CRYPTO_TFM_REQ_MASK);
+ 	error = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (error)
+ 		return error;
+ 	keylen -= 3;
+ 	memcpy(aeadctx->salt, key + keylen, 3);
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_gcm_setkey(struct crypto_aead *aead, const u8 *key,
+ 			   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_gcm_ctx *gctx = GCM_CTX(aeadctx);
+ 	struct crypto_cipher *cipher;
+ 	unsigned int ck_size;
+ 	int ret = 0, key_ctx_size = 0;
+ 
+ 	aeadctx->enckey_len = 0;
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (get_aead_subtype(aead) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&
+ 	    keylen > 3) {
+ 		keylen -= 4;  /* nonce/salt is present in the last 4 bytes */
+ 		memcpy(aeadctx->salt, key + keylen, 4);
+ 	}
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		pr_err("GCM: Invalid key length %d\n", keylen);
+ 		ret = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,
+ 						CHCR_KEYCTX_MAC_KEY_SIZE_128,
+ 						0, 0,
+ 						key_ctx_size >> 4);
+ 	/* Calculate the H = CIPH(K, 0 repeated 16 times).
+ 	 * It will go in key context
+ 	 */
+ 	cipher = crypto_alloc_cipher("aes-generic", 0, 0);
+ 	if (IS_ERR(cipher)) {
+ 		aeadctx->enckey_len = 0;
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	ret = crypto_cipher_setkey(cipher, key, keylen);
+ 	if (ret) {
+ 		aeadctx->enckey_len = 0;
+ 		goto out1;
+ 	}
+ 	memset(gctx->ghash_h, 0, AEAD_H_SIZE);
+ 	crypto_cipher_encrypt_one(cipher, gctx->ghash_h, gctx->ghash_h);
+ 
+ out1:
+ 	crypto_free_cipher(cipher);
+ out:
+ 	return ret;
+ }
+ 
+ static int chcr_authenc_setkey(struct crypto_aead *authenc, const u8 *key,
+ 				   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	/* it contains auth and cipher key both*/
+ 	struct crypto_authenc_keys keys;
+ 	unsigned int bs;
+ 	unsigned int max_authsize = crypto_aead_alg(authenc)->maxauthsize;
+ 	int err = 0, i, key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 	unsigned char pad[CHCR_HASH_MAX_BLOCK_SIZE_128] = { 0 };
+ 	struct crypto_shash *base_hash = ERR_PTR(-EINVAL);
+ 	struct algo_param param;
+ 	int align;
+ 	u8 *o_ptr = NULL;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 
+ 	if (get_alg_config(&param, max_authsize)) {
+ 		pr_err("chcr : Unsupported digest size\n");
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 
+ 	/* Copy only encryption key. We use authkey to generate h(ipad) and
+ 	 * h(opad) so authkey is not needed again. authkeylen size have the
+ 	 * size of the hash digest size.
+ 	 */
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 			    aeadctx->enckey_len << 3);
+ 
+ 	base_hash  = chcr_alloc_shash(max_authsize);
+ 	if (IS_ERR(base_hash)) {
+ 		pr_err("chcr : Base driver cannot be loaded\n");
+ 		aeadctx->enckey_len = 0;
+ 		return -EINVAL;
+ 	}
+ 	{
+ 		SHASH_DESC_ON_STACK(shash, base_hash);
+ 		shash->tfm = base_hash;
+ 		shash->flags = crypto_shash_get_flags(base_hash);
+ 		bs = crypto_shash_blocksize(base_hash);
+ 		align = KEYCTX_ALIGN_PAD(max_authsize);
+ 		o_ptr =  actx->h_iopad + param.result_size + align;
+ 
+ 		if (keys.authkeylen > bs) {
+ 			err = crypto_shash_digest(shash, keys.authkey,
+ 						  keys.authkeylen,
+ 						  o_ptr);
+ 			if (err) {
+ 				pr_err("chcr : Base driver cannot be loaded\n");
+ 				goto out;
+ 			}
+ 			keys.authkeylen = max_authsize;
+ 		} else
+ 			memcpy(o_ptr, keys.authkey, keys.authkeylen);
+ 
+ 		/* Compute the ipad-digest*/
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= IPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, actx->h_iopad,
+ 					      max_authsize))
+ 			goto out;
+ 		/* Compute the opad-digest */
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= OPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, o_ptr, max_authsize))
+ 			goto out;
+ 
+ 		/* convert the ipad and opad digest to network order */
+ 		chcr_change_order(actx->h_iopad, param.result_size);
+ 		chcr_change_order(o_ptr, param.result_size);
+ 		key_ctx_len = sizeof(struct _key_ctx) +
+ 			((DIV_ROUND_UP(keys.enckeylen, 16)) << 4) +
+ 			(param.result_size + align) * 2;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, param.mk_size,
+ 						0, 1, key_ctx_len >> 4);
+ 		actx->auth_mode = param.auth_mode;
+ 		chcr_free_shash(base_hash);
+ 
+ 		return 0;
+ 	}
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	if (!IS_ERR(base_hash))
+ 		chcr_free_shash(base_hash);
+ 	return -EINVAL;
+ }
+ 
+ static int chcr_aead_digest_null_setkey(struct crypto_aead *authenc,
+ 					const u8 *key, unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct crypto_authenc_keys keys;
+ 	int err;
+ 	/* it contains auth and cipher key both*/
+ 	int key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 				    aeadctx->enckey_len << 3);
+ 	key_ctx_len =  sizeof(struct _key_ctx)
+ 		+ ((DIV_ROUND_UP(keys.enckeylen, 16)) << 4);
+ 
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY, 0,
+ 						0, key_ctx_len >> 4);
+ 	actx->auth_mode = CHCR_SCMD_AUTH_MODE_NOP;
+ 	return 0;
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	return -EINVAL;
+ }
+ static int chcr_aead_encrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 
+ 	reqctx->verify = VERIFY_HW;
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int size;
+ 
+ 	if (aeadctx->mayverify == VERIFY_SW) {
+ 		size = crypto_aead_maxauthsize(tfm);
+ 		reqctx->verify = VERIFY_SW;
+ 	} else {
+ 		size = 0;
+ 		reqctx->verify = VERIFY_HW;
+ 	}
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_op(struct aead_request *req,
+ 			  unsigned short op_type,
+ 			  int size,
+ 			  create_wr_t create_wr_fn)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx;
+ 	struct sk_buff *skb;
+ 
+ 	if (!ctx->dev) {
+ 		pr_err("chcr : %s : No crypto device.\n", __func__);
+ 		return -ENXIO;
+ 	}
+ 	u_ctx = ULD_CTX(ctx);
+ 	if (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 				   ctx->tx_qidx)) {
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
+ 			return -EBUSY;
+ 	}
+ 
+ 	/* Form a WR from req */
+ 	skb = create_wr_fn(req, u_ctx->lldi.rxq_ids[ctx->rx_qidx], size,
+ 			   op_type);
+ 
+ 	if (IS_ERR(skb) || !skb)
+ 		return PTR_ERR(skb);
+ 
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
+ 	chcr_send_wr(skb);
+ 	return -EINPROGRESS;
+ }
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  static struct chcr_alg_template driver_algs[] = {
  	/* AES-CBC */
  	{
@@@ -1267,18 -3039,10 +3024,17 @@@
  		.is_registered = 0,
  		.alg.crypto = {
  			.cra_name		= "cbc(aes)",
++<<<<<<< HEAD
 +			.cra_driver_name	= "cbc(aes-chcr)",
 +			.cra_priority		= CHCR_CRA_PRIORITY,
 +			.cra_flags		= CRYPTO_ALG_TYPE_BLKCIPHER |
 +				CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_driver_name	= "cbc-aes-chcr",
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  			.cra_blocksize		= AES_BLOCK_SIZE,
- 			.cra_ctxsize		= sizeof(struct chcr_context)
- 				+ sizeof(struct ablk_ctx),
- 			.cra_alignmask		= 0,
- 			.cra_type		= &crypto_ablkcipher_type,
- 			.cra_module		= THIS_MODULE,
  			.cra_init		= chcr_cra_init,
- 			.cra_exit		= NULL,
+ 			.cra_exit		= chcr_cra_exit,
  			.cra_u.ablkcipher	= {
  				.min_keysize	= AES_MIN_KEY_SIZE,
  				.max_keysize	= AES_MAX_KEY_SIZE,
@@@ -1294,20 -3058,11 +3050,18 @@@
  		.is_registered = 0,
  		.alg.crypto =   {
  			.cra_name		= "xts(aes)",
++<<<<<<< HEAD
 +			.cra_driver_name	= "xts(aes-chcr)",
 +			.cra_priority		= CHCR_CRA_PRIORITY,
 +			.cra_flags		= CRYPTO_ALG_TYPE_BLKCIPHER |
 +				CRYPTO_ALG_ASYNC,
++=======
+ 			.cra_driver_name	= "xts-aes-chcr",
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  			.cra_blocksize		= AES_BLOCK_SIZE,
- 			.cra_ctxsize		= sizeof(struct chcr_context) +
- 				sizeof(struct ablk_ctx),
- 			.cra_alignmask		= 0,
- 			.cra_type		= &crypto_ablkcipher_type,
- 			.cra_module		= THIS_MODULE,
  			.cra_init		= chcr_cra_init,
  			.cra_exit		= NULL,
- 			.cra_u = {
- 				.ablkcipher = {
+ 			.cra_u .ablkcipher = {
  					.min_keysize	= 2 * AES_MIN_KEY_SIZE,
  					.max_keysize	= 2 * AES_MAX_KEY_SIZE,
  					.ivsize		= AES_BLOCK_SIZE,
diff --cc drivers/crypto/chelsio/chcr_algo.h
index e09cf2fab4f7,583008de51a3..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.h
+++ b/drivers/crypto/chelsio/chcr_algo.h
@@@ -218,6 -218,27 +218,30 @@@
  
  #define MAX_NK 8
  #define CRYPTO_MAX_IMM_TX_PKT_LEN 256
++<<<<<<< HEAD
++=======
+ #define MAX_WR_SIZE			512
+ #define ROUND_16(bytes)		((bytes) & 0xFFFFFFF0)
+ #define MAX_DSGL_ENT			32
+ #define MAX_DIGEST_SKB_SGE	(MAX_SKB_FRAGS - 2)
+ #define MIN_CIPHER_SG			1 /* IV */
+ #define MIN_AUTH_SG			2 /*IV + AAD*/
+ #define MIN_GCM_SG			2 /* IV + AAD*/
+ #define MIN_DIGEST_SG			1 /*Partial Buffer*/
+ #define MIN_CCM_SG			3 /*IV+AAD+B0*/
+ #define SPACE_LEFT(len) \
+ 	((MAX_WR_SIZE - WR_MIN_LEN - (len)))
+ 
+ unsigned int sgl_ent_len[] = {0, 0, 16, 24, 40,
+ 				48, 64, 72, 88,
+ 				96, 112, 120, 136,
+ 				144, 160, 168, 184,
+ 				192};
+ unsigned int dsgl_ent_len[] = {0, 32, 32, 48, 48, 64, 64, 80, 80,
+ 				112, 112, 128, 128, 144, 144, 160, 160,
+ 				192, 192, 208, 208, 224, 224, 240, 240,
+ 				272, 272, 288, 288, 304, 304, 320, 320};
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  
  struct algo_param {
  	unsigned int auth_mode;
diff --cc drivers/crypto/chelsio/chcr_core.c
index 91853c46c047,a6c938a913c7..000000000000
--- a/drivers/crypto/chelsio/chcr_core.c
+++ b/drivers/crypto/chelsio/chcr_core.c
@@@ -114,10 -114,7 +114,14 @@@ static int cpl_fw6_pld_handler(struct c
  	}
  	/* call completion callback with failure status */
  	if (req) {
++<<<<<<< HEAD
 +		if (!chcr_handle_resp(req, input, error_status))
 +			req->complete(req, error_status);
 +		else
 +			return -EINVAL;
++=======
+ 		error_status = chcr_handle_resp(req, input, error_status);
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  	} else {
  		pr_err("Incorrect request address from the firmware\n");
  		return -EFAULT;
diff --cc drivers/crypto/chelsio/chcr_crypto.h
index c00250405824,b1b794435503..000000000000
--- a/drivers/crypto/chelsio/chcr_crypto.h
+++ b/drivers/crypto/chelsio/chcr_crypto.h
@@@ -106,8 -128,20 +106,21 @@@
  #define IV_IMMEDIATE            1
  #define IV_DSGL			2
  
 -#define AEAD_H_SIZE             16
 -
  #define CRYPTO_ALG_SUB_TYPE_MASK            0x0f000000
  #define CRYPTO_ALG_SUB_TYPE_HASH_HMAC       0x01000000
++<<<<<<< HEAD
++=======
+ #define CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106    0x02000000
+ #define CRYPTO_ALG_SUB_TYPE_AEAD_GCM	    0x03000000
+ #define CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC    0x04000000
+ #define CRYPTO_ALG_SUB_TYPE_AEAD_CCM        0x05000000
+ #define CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309    0x06000000
+ #define CRYPTO_ALG_SUB_TYPE_AEAD_NULL       0x07000000
+ #define CRYPTO_ALG_SUB_TYPE_CTR             0x08000000
+ #define CRYPTO_ALG_SUB_TYPE_CTR_RFC3686     0x09000000
+ #define CRYPTO_ALG_SUB_TYPE_XTS		    0x0a000000
+ #define CRYPTO_ALG_SUB_TYPE_CBC		    0x0b000000
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  #define CRYPTO_ALG_TYPE_HMAC (CRYPTO_ALG_TYPE_AHASH |\
  			      CRYPTO_ALG_SUB_TYPE_HASH_HMAC)
  
@@@ -120,12 -153,54 +133,14 @@@
  /* Aligned to 128 bit boundary */
  
  struct ablk_ctx {
+ 	struct crypto_skcipher *sw_cipher;
  	__be32 key_ctx_hdr;
  	unsigned int enckey_len;
- 	u8 key[CHCR_AES_MAX_KEY_LEN];
  	unsigned char ciph_mode;
+ 	u8 key[CHCR_AES_MAX_KEY_LEN];
+ 	u8 nonce[4];
  	u8 rrkey[AES_MAX_KEY_SIZE];
  };
 -struct chcr_aead_reqctx {
 -	struct	sk_buff	*skb;
 -	struct scatterlist *dst;
 -	struct scatterlist srcffwd[2];
 -	struct scatterlist dstffwd[2];
 -	short int dst_nents;
 -	u16 verify;
 -	u8 iv[CHCR_MAX_CRYPTO_IV_LEN];
 -	unsigned char scratch_pad[MAX_SCRATCH_PAD_SIZE];
 -};
 -
 -struct chcr_gcm_ctx {
 -	u8 ghash_h[AEAD_H_SIZE];
 -};
 -
 -struct chcr_authenc_ctx {
 -	u8 dec_rrkey[AES_MAX_KEY_SIZE];
 -	u8 h_iopad[2 * CHCR_HASH_MAX_DIGEST_SIZE];
 -	unsigned char auth_mode;
 -};
 -
 -struct __aead_ctx {
 -	struct chcr_gcm_ctx gcm[0];
 -	struct chcr_authenc_ctx authenc[0];
 -};
 -
 -
 -
 -struct chcr_aead_ctx {
 -	__be32 key_ctx_hdr;
 -	unsigned int enckey_len;
 -	struct crypto_skcipher *null;
 -	struct crypto_aead *sw_cipher;
 -	u8 salt[MAX_SALT];
 -	u8 key[CHCR_AES_MAX_KEY_LEN];
 -	u16 hmac_ctrl;
 -	u16 mayverify;
 -	struct	__aead_ctx ctx[0];
 -};
 -
  
  
  struct hmac_ctx {
@@@ -191,9 -277,16 +213,19 @@@ struct sge_opaque_hdr 
  	dma_addr_t addr[MAX_SKB_FRAGS + 1];
  };
  
 -typedef struct sk_buff *(*create_wr_t)(struct aead_request *req,
 +typedef struct sk_buff *(*create_wr_t)(struct crypto_async_request *req,
 +				       struct chcr_context *ctx,
  				       unsigned short qid,
 -				       int size,
  				       unsigned short op_type);
  
++<<<<<<< HEAD
++=======
+ static int chcr_aead_op(struct aead_request *req_base,
+ 			  unsigned short op_type,
+ 			  int size,
+ 			  create_wr_t create_wr_fn);
+ static inline int get_aead_subtype(struct crypto_aead *aead);
+ static int chcr_handle_cipher_resp(struct ablkcipher_request *req,
+ 				   unsigned char *input, int err);
++>>>>>>> b8fd1f4170e7 (crypto: chcr - Add ctr mode and process large sg entries for cipher)
  #endif /* __CHCR_CRYPTO_H__ */
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/chelsio/chcr_algo.h
* Unmerged path drivers/crypto/chelsio/chcr_core.c
diff --git a/drivers/crypto/chelsio/chcr_core.h b/drivers/crypto/chelsio/chcr_core.h
index 0f308c89c023..2a4c9ea2f3d4 100644
--- a/drivers/crypto/chelsio/chcr_core.h
+++ b/drivers/crypto/chelsio/chcr_core.h
@@ -53,6 +53,9 @@
 #define MAC_ERROR_BIT		0
 #define CHK_MAC_ERR_BIT(x)	(((x) >> MAC_ERROR_BIT) & 1)
 #define MAX_SALT                4
+#define WR_MIN_LEN (sizeof(struct chcr_wr) + \
+		    sizeof(struct cpl_rx_phys_dsgl) + \
+		    sizeof(struct ulptx_sgl))
 
 #define padap(dev) pci_get_drvdata(dev->u_ctx->lldi.pdev)
 
* Unmerged path drivers/crypto/chelsio/chcr_crypto.h
