net/mlx5e: Reorganize struct mlx5e_rq

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Reorganize struct mlx5e_rq (Kamal Heib) [1456694]
Rebuild_FUZZ: 94.29%
commit-author Tariq Toukan <tariqt@mellanox.com>
commit b45d8b50b8264cac7b2f1245ca04a2f009038ac7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b45d8b50.failed

Bring fast-path fields together, and combine RX WQE mutual
exclusive fields into a union.

Page-reuse and XDP are mutually exclusive and cannot be used at
the same time.
Use a union to combine their footprints.

	Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit b45d8b50b8264cac7b2f1245ca04a2f009038ac7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 819a88f4cbad,d964db286c95..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -352,7 -524,14 +352,18 @@@ struct mlx5e_rq 
  	struct mlx5_wq_ll      wq;
  
  	union {
++<<<<<<< HEAD
 +		struct mlx5e_dma_info *dma_info;
++=======
+ 		struct {
+ 			struct mlx5e_wqe_frag_info *frag_info;
+ 			u32 frag_sz;	/* max possible skb frag_sz */
+ 			union {
+ 				bool page_reuse;
+ 				bool xdp_xmit;
+ 			};
+ 		} wqe;
++>>>>>>> b45d8b50b826 (net/mlx5e: Reorganize struct mlx5e_rq)
  		struct {
  			struct mlx5e_mpw_info *info;
  			void                  *mtt_no_align;
@@@ -359,10 -540,11 +372,14 @@@
  		} mpwqe;
  	};
  	struct {
- 		u8             page_order;
  		u32            wqe_sz;    /* wqe data buffer size */
++<<<<<<< HEAD
++=======
+ 		u16            headroom;
+ 		u8             page_order;
+ 		u8             map_dir;   /* dma map direction */
++>>>>>>> b45d8b50b826 (net/mlx5e: Reorganize struct mlx5e_rq)
  	} buff;
- 	__be32                 mkey_be;
  
  	struct device         *pdev;
  	struct net_device     *netdev;
@@@ -380,14 -562,17 +397,13 @@@
  
  	struct mlx5e_rx_am     am; /* Adaptive Moderation */
  
 -	/* XDP */
 -	struct bpf_prog       *xdp_prog;
 -	struct mlx5e_xdpsq     xdpsq;
 -
  	/* control */
  	struct mlx5_wq_ctrl    wq_ctrl;
+ 	__be32                 mkey_be;
  	u8                     wq_type;
- 	u32                    mpwqe_stride_sz;
- 	u32                    mpwqe_num_strides;
  	u32                    rqn;
  	struct mlx5e_channel  *channel;
 -	struct mlx5_core_dev  *mdev;
 +	struct mlx5e_priv     *priv;
  	struct mlx5_core_mkey  umr_mkey;
  } ____cacheline_aligned_in_smp;
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 582cfccdbde3,94761d0e1b33..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -563,32 -577,51 +563,65 @@@ static int mlx5e_create_rq(struct mlx5e
  
  	wq_sz = mlx5_wq_ll_get_size(&rq->wq);
  
 -	rq->wq_type = params->rq_wq_type;
 +	rq->wq_type = priv->params.rq_wq_type;
  	rq->pdev    = c->pdev;
  	rq->netdev  = c->netdev;
 -	rq->tstamp  = c->tstamp;
 +	rq->tstamp  = &priv->tstamp;
  	rq->channel = c;
  	rq->ix      = c->ix;
 -	rq->mdev    = mdev;
 +	rq->priv    = c->priv;
  
++<<<<<<< HEAD
 +	switch (priv->params.rq_wq_type) {
++=======
+ 	rq->xdp_prog = params->xdp_prog ? bpf_prog_inc(params->xdp_prog) : NULL;
+ 	if (IS_ERR(rq->xdp_prog)) {
+ 		err = PTR_ERR(rq->xdp_prog);
+ 		rq->xdp_prog = NULL;
+ 		goto err_rq_wq_destroy;
+ 	}
+ 
+ 	rq->buff.map_dir = rq->xdp_prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;
+ 	rq->buff.headroom = params->rq_headroom;
+ 
+ 	switch (rq->wq_type) {
++>>>>>>> b45d8b50b826 (net/mlx5e: Reorganize struct mlx5e_rq)
  	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
 +		if (mlx5e_is_vf_vport_rep(priv)) {
 +			err = -EINVAL;
 +			goto err_rq_wq_destroy;
 +		}
  
 +		rq->handle_rx_cqe = mlx5e_handle_rx_cqe_mpwrq;
  		rq->alloc_wqe = mlx5e_alloc_rx_mpwqe;
  		rq->dealloc_wqe = mlx5e_dealloc_rx_mpwqe;
  
++<<<<<<< HEAD
 +		rq->mpwqe_stride_sz = BIT(priv->params.mpwqe_log_stride_sz);
 +		rq->mpwqe_num_strides = BIT(priv->params.mpwqe_log_num_strides);
++=======
+ 		rq->handle_rx_cqe = c->priv->profile->rx_handlers.handle_rx_cqe_mpwqe;
+ #ifdef CONFIG_MLX5_EN_IPSEC
+ 		if (MLX5_IPSEC_DEV(mdev)) {
+ 			err = -EINVAL;
+ 			netdev_err(c->netdev, "MPWQE RQ with IPSec offload not supported\n");
+ 			goto err_rq_wq_destroy;
+ 		}
+ #endif
+ 		if (!rq->handle_rx_cqe) {
+ 			err = -EINVAL;
+ 			netdev_err(c->netdev, "RX handler of MPWQE RQ is not set, err %d\n", err);
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		rq->mpwqe.stride_sz = BIT(params->mpwqe_log_stride_sz);
+ 		rq->mpwqe.num_strides = BIT(params->mpwqe_log_num_strides);
++>>>>>>> b45d8b50b826 (net/mlx5e: Reorganize struct mlx5e_rq)
  
- 		rq->buff.wqe_sz = rq->mpwqe_stride_sz * rq->mpwqe_num_strides;
+ 		rq->buff.wqe_sz = rq->mpwqe.stride_sz * rq->mpwqe.num_strides;
  		byte_count = rq->buff.wqe_sz;
  
 -		err = mlx5e_create_rq_umr_mkey(mdev, rq);
 +		err = mlx5e_create_rq_umr_mkey(rq);
  		if (err)
  			goto err_rq_wq_destroy;
  		rq->mkey_be = cpu_to_be32(rq->umr_mkey.key);
@@@ -619,12 -665,8 +652,17 @@@
  		byte_count = rq->buff.wqe_sz;
  
  		/* calc the required page order */
++<<<<<<< HEAD
 +		frag_sz = MLX5_RX_HEADROOM +
 +			  byte_count /* packet data */ +
 +			  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 +		frag_sz = SKB_DATA_ALIGN(frag_sz);
 +
 +		npages = DIV_ROUND_UP(frag_sz, PAGE_SIZE);
++=======
+ 		rq->wqe.frag_sz = MLX5_SKB_FRAG_SZ(rq->buff.headroom + byte_count);
+ 		npages = DIV_ROUND_UP(rq->wqe.frag_sz, PAGE_SIZE);
++>>>>>>> b45d8b50b826 (net/mlx5e: Reorganize struct mlx5e_rq)
  		rq->buff.page_order = order_base_2(npages);
  
  		byte_count |= MLX5_HW_START_PADDING;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 95ed8ecaf735,1b50f1e7e48a..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -258,14 -244,26 +258,18 @@@ void mlx5e_page_release(struct mlx5e_r
  	put_page(dma_info->page);
  }
  
 -static inline bool mlx5e_page_reuse(struct mlx5e_rq *rq,
 -				    struct mlx5e_wqe_frag_info *wi)
 -{
 -	return rq->wqe.page_reuse && wi->di.page &&
 -		(wi->offset + rq->wqe.frag_sz <= RQ_PAGE_SIZE(rq)) &&
 -		!mlx5e_page_is_reserved(wi->di.page);
 -}
 -
  int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix)
  {
 -	struct mlx5e_wqe_frag_info *wi = &rq->wqe.frag_info[ix];
 +	struct mlx5e_dma_info *di = &rq->dma_info[ix];
  
 -	/* check if page exists, hence can be reused */
 -	if (!wi->di.page) {
 -		if (unlikely(mlx5e_page_alloc_mapped(rq, &wi->di)))
 -			return -ENOMEM;
 -		wi->offset = 0;
 -	}
 +	if (unlikely(mlx5e_page_alloc_mapped(rq, di)))
 +		return -ENOMEM;
  
++<<<<<<< HEAD
 +	wqe->data.addr = cpu_to_be64(di->addr + MLX5_RX_HEADROOM);
++=======
+ 	wqe->data.addr = cpu_to_be64(wi->di.addr + wi->offset + rq->buff.headroom);
++>>>>>>> b45d8b50b826 (net/mlx5e: Reorganize struct mlx5e_rq)
  	return 0;
  }
  
@@@ -637,23 -654,143 +641,32 @@@ static inline void mlx5e_complete_rx_cq
  	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
  }
  
 -static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 -{
 -	struct mlx5_wq_cyc *wq = &sq->wq;
 -	struct mlx5e_tx_wqe *wqe;
 -	u16 pi = (sq->pc - 1) & wq->sz_m1; /* last pi */
 -
 -	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 -
 -	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &wqe->ctrl);
 -}
 -
 -static inline bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq,
 -					struct mlx5e_dma_info *di,
 -					const struct xdp_buff *xdp)
 -{
 -	struct mlx5e_xdpsq       *sq   = &rq->xdpsq;
 -	struct mlx5_wq_cyc       *wq   = &sq->wq;
 -	u16                       pi   = sq->pc & wq->sz_m1;
 -	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 -
 -	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
 -	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
 -	struct mlx5_wqe_data_seg *dseg;
 -
 -	ptrdiff_t data_offset = xdp->data - xdp->data_hard_start;
 -	dma_addr_t dma_addr  = di->addr + data_offset;
 -	unsigned int dma_len = xdp->data_end - xdp->data;
 -
 -	prefetchw(wqe);
 -
 -	if (unlikely(dma_len < MLX5E_XDP_MIN_INLINE ||
 -		     MLX5E_SW2HW_MTU(rq->channel->priv, rq->netdev->mtu) < dma_len)) {
 -		rq->stats.xdp_drop++;
 -		return false;
 -	}
 -
 -	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, 1))) {
 -		if (sq->db.doorbell) {
 -			/* SQ is full, ring doorbell */
 -			mlx5e_xmit_xdp_doorbell(sq);
 -			sq->db.doorbell = false;
 -		}
 -		rq->stats.xdp_tx_full++;
 -		return false;
 -	}
 -
 -	dma_sync_single_for_device(sq->pdev, dma_addr, dma_len, PCI_DMA_TODEVICE);
 -
 -	cseg->fm_ce_se = 0;
 -
 -	dseg = (struct mlx5_wqe_data_seg *)eseg + 1;
 -
 -	/* copy the inline part if required */
 -	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
 -		memcpy(eseg->inline_hdr.start, xdp->data, MLX5E_XDP_MIN_INLINE);
 -		eseg->inline_hdr.sz = cpu_to_be16(MLX5E_XDP_MIN_INLINE);
 -		dma_len  -= MLX5E_XDP_MIN_INLINE;
 -		dma_addr += MLX5E_XDP_MIN_INLINE;
 -		dseg++;
 -	}
 -
 -	/* write the dma part */
 -	dseg->addr       = cpu_to_be64(dma_addr);
 -	dseg->byte_count = cpu_to_be32(dma_len);
 -
 -	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_SEND);
 -
 -	/* move page to reference to sq responsibility,
 -	 * and mark so it's not put back in page-cache.
 -	 */
 -	rq->wqe.xdp_xmit = true;
 -	sq->db.di[pi] = *di;
 -	sq->pc++;
 -
 -	sq->db.doorbell = true;
 -
 -	rq->stats.xdp_tx++;
 -	return true;
 -}
 -
 -/* returns true if packet was consumed by xdp */
 -static inline int mlx5e_xdp_handle(struct mlx5e_rq *rq,
 -				   struct mlx5e_dma_info *di,
 -				   void *va, u16 *rx_headroom, u32 *len)
 -{
 -	const struct bpf_prog *prog = READ_ONCE(rq->xdp_prog);
 -	struct xdp_buff xdp;
 -	u32 act;
 -
 -	if (!prog)
 -		return false;
 -
 -	xdp.data = va + *rx_headroom;
 -	xdp.data_end = xdp.data + *len;
 -	xdp.data_hard_start = va;
 -
 -	act = bpf_prog_run_xdp(prog, &xdp);
 -	switch (act) {
 -	case XDP_PASS:
 -		*rx_headroom = xdp.data - xdp.data_hard_start;
 -		*len = xdp.data_end - xdp.data;
 -		return false;
 -	case XDP_TX:
 -		if (unlikely(!mlx5e_xmit_xdp_frame(rq, di, &xdp)))
 -			trace_xdp_exception(rq->netdev, prog, act);
 -		return true;
 -	default:
 -		bpf_warn_invalid_xdp_action(act);
 -	case XDP_ABORTED:
 -		trace_xdp_exception(rq->netdev, prog, act);
 -	case XDP_DROP:
 -		rq->stats.xdp_drop++;
 -		return true;
 -	}
 -}
 -
  static inline
  struct sk_buff *skb_from_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 -			     struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt)
 +			     u16 wqe_counter, u32 cqe_bcnt)
  {
++<<<<<<< HEAD
 +	struct mlx5e_dma_info *di;
 +	struct sk_buff *skb;
 +	void *va;
++=======
+ 	struct mlx5e_dma_info *di = &wi->di;
+ 	u16 rx_headroom = rq->buff.headroom;
+ 	struct sk_buff *skb;
+ 	void *va, *data;
+ 	bool consumed;
+ 	u32 frag_size;
++>>>>>>> b45d8b50b826 (net/mlx5e: Reorganize struct mlx5e_rq)
  
 -	va             = page_address(di->page) + wi->offset;
 -	data           = va + rx_headroom;
 -	frag_size      = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);
 +	di             = &rq->dma_info[wqe_counter];
 +	va             = page_address(di->page);
  
  	dma_sync_single_range_for_cpu(rq->pdev,
 -				      di->addr + wi->offset,
 -				      0, frag_size,
 +				      di->addr,
 +				      MLX5_RX_HEADROOM,
 +				      rq->buff.wqe_sz,
  				      DMA_FROM_DEVICE);
 -	prefetch(data);
 -	wi->offset += frag_size;
 +	prefetch(va + MLX5_RX_HEADROOM);
  
  	if (unlikely((cqe->op_own >> 4) != MLX5_CQE_RESP_SEND)) {
  		rq->stats.wqe_err++;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
