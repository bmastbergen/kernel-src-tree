x86/mm/pat: Don't implicitly allow _PAGE_RW in kernel_map_pages_in_pgd()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] mm/pat: Don't implicitly allow _PAGE_RW in kernel_map_pages_in_pgd() (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 97.14%
commit-author Sai Praneeth <sai.praneeth.prakhya@intel.com>
commit 15f003d20782a4079e078d16df57081ebd1fc150
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/15f003d2.failed

As part of the preparation for the EFI_MEMORY_RO flag added in the UEFI
2.5 specification, we need the ability to map pages in kernel page
tables without _PAGE_RW being set.

Modify kernel_map_pages_in_pgd() to require its callers to pass _PAGE_RW
if the pages need to be mapped read/write. Otherwise, we'll map the
pages as read-only.

	Signed-off-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
	Signed-off-by: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: Andy Lutomirski <luto@amacapital.net>
	Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Lee, Chun-Yi <jlee@suse.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Luis R. Rodriguez <mcgrof@suse.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ravi Shankar <ravi.v.shankar@intel.com>
	Cc: Ricardo Neri <ricardo.neri@intel.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Toshi Kani <toshi.kani@hp.com>
	Cc: linux-efi@vger.kernel.org
Link: http://lkml.kernel.org/r/1455712566-16727-12-git-send-email-matt@codeblueprint.co.uk
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 15f003d20782a4079e078d16df57081ebd1fc150)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/platform/efi/efi_64.c
diff --cc arch/x86/platform/efi/efi_64.c
index 1a2483c88c8c,b0965b27e47f..000000000000
--- a/arch/x86/platform/efi/efi_64.c
+++ b/arch/x86/platform/efi/efi_64.c
@@@ -192,7 -232,8 +192,12 @@@ int __init efi_setup_page_tables(unsign
  	 * and ident-map those pages containing the map before calling
  	 * phys_efi_set_virtual_address_map().
  	 */
++<<<<<<< HEAD
 +	if (kernel_map_pages_in_pgd(pgd, pa_memmap, pa_memmap, num_pages, _PAGE_NX)) {
++=======
+ 	pfn = pa_memmap >> PAGE_SHIFT;
+ 	if (kernel_map_pages_in_pgd(pgd, pfn, pa_memmap, num_pages, _PAGE_NX | _PAGE_RW)) {
++>>>>>>> 15f003d20782 (x86/mm/pat: Don't implicitly allow _PAGE_RW in kernel_map_pages_in_pgd())
  		pr_err("Error ident-mapping new memmap (0x%lx)!\n", pa_memmap);
  		return 1;
  	}
@@@ -208,6 -249,25 +213,28 @@@
  	if (!IS_ENABLED(CONFIG_EFI_MIXED))
  		return 0;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Map all of RAM so that we can access arguments in the 1:1
+ 	 * mapping when making EFI runtime calls.
+ 	 */
+ 	for_each_efi_memory_desc(&memmap, md) {
+ 		if (md->type != EFI_CONVENTIONAL_MEMORY &&
+ 		    md->type != EFI_LOADER_DATA &&
+ 		    md->type != EFI_LOADER_CODE)
+ 			continue;
+ 
+ 		pfn = md->phys_addr >> PAGE_SHIFT;
+ 		npages = md->num_pages;
+ 
+ 		if (kernel_map_pages_in_pgd(pgd, pfn, md->phys_addr, npages, _PAGE_RW)) {
+ 			pr_err("Failed to map 1:1 memory\n");
+ 			return 1;
+ 		}
+ 	}
+ 
++>>>>>>> 15f003d20782 (x86/mm/pat: Don't implicitly allow _PAGE_RW in kernel_map_pages_in_pgd())
  	page = alloc_page(GFP_KERNEL|__GFP_DMA32);
  	if (!page)
  		panic("Unable to allocate EFI runtime stack < 4GB\n");
@@@ -217,9 -277,9 +244,13 @@@
  
  	npages = (_end - _text) >> PAGE_SHIFT;
  	text = __pa(_text);
 -	pfn = text >> PAGE_SHIFT;
  
++<<<<<<< HEAD
 +	if (kernel_map_pages_in_pgd(__va(efi_scratch.efi_pgt),
 +				    text >> PAGE_SHIFT, text, npages, 0)) {
++=======
+ 	if (kernel_map_pages_in_pgd(pgd, pfn, text, npages, _PAGE_RW)) {
++>>>>>>> 15f003d20782 (x86/mm/pat: Don't implicitly allow _PAGE_RW in kernel_map_pages_in_pgd())
  		pr_err("Failed to map kernel text 1:1\n");
  		return 1;
  	}
@@@ -235,13 -294,15 +266,19 @@@ void __init efi_cleanup_page_tables(uns
  
  static void __init __map_region(efi_memory_desc_t *md, u64 va)
  {
++<<<<<<< HEAD
 +	pgd_t *pgd = (pgd_t *)__va(real_mode_header->trampoline_pgd);
 +	unsigned long pf = 0;
++=======
+ 	unsigned long flags = _PAGE_RW;
+ 	unsigned long pfn;
+ 	pgd_t *pgd = efi_pgd;
++>>>>>>> 15f003d20782 (x86/mm/pat: Don't implicitly allow _PAGE_RW in kernel_map_pages_in_pgd())
  
  	if (!(md->attribute & EFI_MEMORY_WB))
 -		flags |= _PAGE_PCD;
 +		pf |= _PAGE_PCD;
  
 -	pfn = md->phys_addr >> PAGE_SHIFT;
 -	if (kernel_map_pages_in_pgd(pgd, pfn, va, md->num_pages, flags))
 +	if (kernel_map_pages_in_pgd(pgd, md->phys_addr, va, md->num_pages, pf))
  		pr_warn("Error mapping PA 0x%llx -> VA 0x%llx!\n",
  			   md->phys_addr, va);
  }
diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c
index 3fe832369c84..e4f0304793b3 100644
--- a/arch/x86/mm/pageattr.c
+++ b/arch/x86/mm/pageattr.c
@@ -1874,6 +1874,9 @@ int kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,
 	if (!(page_flags & _PAGE_NX))
 		cpa.mask_clr = __pgprot(_PAGE_NX);
 
+	if (!(page_flags & _PAGE_RW))
+		cpa.mask_clr = __pgprot(_PAGE_RW);
+
 	cpa.mask_set = __pgprot(_PAGE_PRESENT | page_flags);
 
 	retval = __change_page_attr_set_clr(&cpa, 0);
* Unmerged path arch/x86/platform/efi/efi_64.c
