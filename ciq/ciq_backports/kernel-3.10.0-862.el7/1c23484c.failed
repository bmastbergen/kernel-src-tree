dm mpath: do not lock up a CPU with requeuing activity

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Bart Van Assche <bart.vanassche@wdc.com>
commit 1c23484c355ec360ca2f37914f8a4802c6baeead
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1c23484c.failed

When using the block layer in single queue mode, get_request()
returns ERR_PTR(-EAGAIN) if the queue is dying and the REQ_NOWAIT
flag has been passed to get_request(). Avoid that the kernel
reports soft lockup complaints in this case due to continuous
requeuing activity.

Fixes: 7083abbbf ("dm mpath: avoid that path removal can trigger an infinite loop")
	Cc: stable@vger.kernel.org
	Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
	Tested-by: Laurence Oberman <loberman@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 1c23484c355ec360ca2f37914f8a4802c6baeead)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-mpath.c
diff --cc drivers/md/dm-mpath.c
index 745e8192f19b,d24e4b05f5da..000000000000
--- a/drivers/md/dm-mpath.c
+++ b/drivers/md/dm-mpath.c
@@@ -574,33 -494,23 +574,46 @@@ static int __multipath_map(struct dm_ta
  	mpio->nr_bytes = nr_bytes;
  
  	bdev = pgpath->path.dev->bdev;
++<<<<<<< HEAD
 +
 +	if (clone) {
 +		/*
 +		 * Old request-based interface: allocated clone is passed in.
 +		 * Used by: .request_fn stacked on .request_fn path(s).
 +		 */
 +		clone->q = bdev_get_queue(bdev);
 +		clone->rq_disk = bdev->bd_disk;
 +		clone->cmd_flags |= REQ_FAILFAST_TRANSPORT;
 +	} else {
 +		/*
 +		 * blk-mq request-based interface; used by both:
 +		 * .request_fn stacked on blk-mq path(s) and
 +		 * blk-mq stacked on blk-mq path(s).
 +		 */
 +		clone = blk_mq_alloc_request(bdev_get_queue(bdev),
 +					     rq_data_dir(rq), BLK_MQ_REQ_NOWAIT);
 +		if (IS_ERR(clone)) {
 +			/* EBUSY, ENODEV or EWOULDBLOCK: requeue */
 +			clear_request_fn_mpio(m, map_context);
 +			return DM_MAPIO_DELAY_REQUEUE;
++=======
+ 	q = bdev_get_queue(bdev);
+ 	clone = blk_get_request(q, rq->cmd_flags | REQ_NOMERGE, GFP_ATOMIC);
+ 	if (IS_ERR(clone)) {
+ 		/* EBUSY, ENODEV or EWOULDBLOCK: requeue */
+ 		bool queue_dying = blk_queue_dying(q);
+ 		DMERR_LIMIT("blk_get_request() returned %ld%s - requeuing",
+ 			    PTR_ERR(clone), queue_dying ? " (path offline)" : "");
+ 		if (queue_dying) {
+ 			atomic_inc(&m->pg_init_in_progress);
+ 			activate_or_offline_path(pgpath);
++>>>>>>> 1c23484c355e (dm mpath: do not lock up a CPU with requeuing activity)
  		}
 -		return DM_MAPIO_DELAY_REQUEUE;
 +		clone->bio = clone->biotail = NULL;
 +		clone->rq_disk = bdev->bd_disk;
 +		clone->cmd_flags |= REQ_FAILFAST_TRANSPORT;
 +		*__clone = clone;
  	}
 -	clone->bio = clone->biotail = NULL;
 -	clone->rq_disk = bdev->bd_disk;
 -	clone->cmd_flags |= REQ_FAILFAST_TRANSPORT;
 -	*__clone = clone;
  
  	if (pgpath->pg->ps.type->start_io)
  		pgpath->pg->ps.type->start_io(&pgpath->pg->ps,
* Unmerged path drivers/md/dm-mpath.c
