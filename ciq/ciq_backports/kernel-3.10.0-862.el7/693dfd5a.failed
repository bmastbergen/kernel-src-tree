IB/mlx5: Enable IPoIB acceleration

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5: Enable IPoIB acceleration (Don Dutile) [1456694 1499362]
Rebuild_FUZZ: 95.38%
commit-author Erez Shitrit <erezsh@mellanox.com>
commit 693dfd5a3f19efc44acf3a57217c0480e414f8ee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/693dfd5a.failed

Enable mlx5 IPoIB acceleration by declaring
mlx5_ib_{alloc,free}_rdma_netdev and assigning the mlx5
IPoIB rdma_netdev callbacks.

In addition, this patch brings in sync mlx5's IPoIB parts for net and IB
trees. As a precaution, we disabled IPoIB acceleration by default (in
the mlx5_core Kconfig file).

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 693dfd5a3f19efc44acf3a57217c0480e414f8ee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
#	drivers/net/ethernet/mellanox/mlx5/core/ipoib.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index 9a7070605698,d45772da0963..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -3237,6 -3431,125 +3237,128 @@@ dealloc_counters
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static struct rdma_hw_stats *mlx5_ib_alloc_hw_stats(struct ib_device *ibdev,
+ 						    u8 port_num)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx5_ib_port *port = &dev->port[port_num - 1];
+ 
+ 	/* We support only per port stats */
+ 	if (port_num == 0)
+ 		return NULL;
+ 
+ 	return rdma_alloc_hw_stats_struct(port->cnts.names,
+ 					  port->cnts.num_q_counters +
+ 					  port->cnts.num_cong_counters,
+ 					  RDMA_HW_STATS_DEFAULT_LIFESPAN);
+ }
+ 
+ static int mlx5_ib_query_q_counters(struct mlx5_ib_dev *dev,
+ 				    struct mlx5_ib_port *port,
+ 				    struct rdma_hw_stats *stats)
+ {
+ 	int outlen = MLX5_ST_SZ_BYTES(query_q_counter_out);
+ 	void *out;
+ 	__be32 val;
+ 	int ret, i;
+ 
+ 	out = mlx5_vzalloc(outlen);
+ 	if (!out)
+ 		return -ENOMEM;
+ 
+ 	ret = mlx5_core_query_q_counter(dev->mdev,
+ 					port->cnts.set_id, 0,
+ 					out, outlen);
+ 	if (ret)
+ 		goto free;
+ 
+ 	for (i = 0; i < port->cnts.num_q_counters; i++) {
+ 		val = *(__be32 *)(out + port->cnts.offsets[i]);
+ 		stats->value[i] = (u64)be32_to_cpu(val);
+ 	}
+ 
+ free:
+ 	kvfree(out);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_query_cong_counters(struct mlx5_ib_dev *dev,
+ 				       struct mlx5_ib_port *port,
+ 				       struct rdma_hw_stats *stats)
+ {
+ 	int outlen = MLX5_ST_SZ_BYTES(query_cong_statistics_out);
+ 	void *out;
+ 	int ret, i;
+ 	int offset = port->cnts.num_q_counters;
+ 
+ 	out = mlx5_vzalloc(outlen);
+ 	if (!out)
+ 		return -ENOMEM;
+ 
+ 	ret = mlx5_cmd_query_cong_counter(dev->mdev, false, out, outlen);
+ 	if (ret)
+ 		goto free;
+ 
+ 	for (i = 0; i < port->cnts.num_cong_counters; i++) {
+ 		stats->value[i + offset] =
+ 			be64_to_cpup((__be64 *)(out +
+ 				     port->cnts.offsets[i + offset]));
+ 	}
+ 
+ free:
+ 	kvfree(out);
+ 	return ret;
+ }
+ 
+ static int mlx5_ib_get_hw_stats(struct ib_device *ibdev,
+ 				struct rdma_hw_stats *stats,
+ 				u8 port_num, int index)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(ibdev);
+ 	struct mlx5_ib_port *port = &dev->port[port_num - 1];
+ 	int ret, num_counters;
+ 
+ 	if (!stats)
+ 		return -EINVAL;
+ 
+ 	ret = mlx5_ib_query_q_counters(dev, port, stats);
+ 	if (ret)
+ 		return ret;
+ 	num_counters = port->cnts.num_q_counters;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, cc_query_allowed)) {
+ 		ret = mlx5_ib_query_cong_counters(dev, port, stats);
+ 		if (ret)
+ 			return ret;
+ 		num_counters += port->cnts.num_cong_counters;
+ 	}
+ 
+ 	return num_counters;
+ }
+ 
+ static struct net_device*
+ mlx5_ib_alloc_rdma_netdev(struct ib_device *hca,
+ 			  u8 port_num,
+ 			  enum rdma_netdev_t type,
+ 			  const char *name,
+ 			  unsigned char name_assign_type,
+ 			  void (*setup)(struct net_device *))
+ {
+ 	if (type != RDMA_NETDEV_IPOIB)
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	return mlx5_rdma_netdev_alloc(to_mdev(hca)->mdev, hca,
+ 				      name, setup);
+ }
+ 
+ static void mlx5_ib_free_rdma_netdev(struct net_device *netdev)
+ {
+ 	return mlx5_rdma_netdev_free(netdev);
+ }
+ 
++>>>>>>> 693dfd5a3f19 (IB/mlx5: Enable IPoIB acceleration)
  static void *mlx5_ib_add(struct mlx5_core_dev *mdev)
  {
  	struct mlx5_ib_dev *dev;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/ipoib.c
index 4180820bcac0,019c230da498..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/ipoib.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/ipoib.c
@@@ -89,10 -286,125 +90,130 @@@ static const struct mlx5e_profile mlx5i
  	.max_tc		   = MLX5I_MAX_NUM_TC,
  };
  
++<<<<<<< HEAD
 +/* IPoIB RDMA netdev callbacks */
 +
 +int mlx5i_xmit(struct net_device *dev, struct sk_buff *skb,
 +	       struct ib_ah *address, u32 dqpn, u32 dqkey)
++=======
+ /* mlx5i netdev NDos */
+ 
+ static int mlx5i_dev_init(struct net_device *dev)
+ {
+ 	struct mlx5e_priv    *priv   = mlx5i_epriv(dev);
+ 	struct mlx5i_priv    *ipriv  = priv->ppriv;
+ 
+ 	/* Set dev address using underlay QP */
+ 	dev->dev_addr[1] = (ipriv->qp.qpn >> 16) & 0xff;
+ 	dev->dev_addr[2] = (ipriv->qp.qpn >>  8) & 0xff;
+ 	dev->dev_addr[3] = (ipriv->qp.qpn) & 0xff;
+ 
+ 	return 0;
+ }
+ 
+ static void mlx5i_dev_cleanup(struct net_device *dev)
+ {
+ 	struct mlx5e_priv    *priv   = mlx5i_epriv(dev);
+ 	struct mlx5_core_dev *mdev   = priv->mdev;
+ 	struct mlx5i_priv    *ipriv  = priv->ppriv;
+ 	struct mlx5_qp_context context;
+ 
+ 	/* detach qp from flow-steering by reset it */
+ 	mlx5_core_qp_modify(mdev, MLX5_CMD_OP_2RST_QP, 0, &context, &ipriv->qp);
+ }
+ 
+ static int mlx5i_open(struct net_device *netdev)
+ {
+ 	struct mlx5e_priv *priv = mlx5i_epriv(netdev);
+ 	int err;
+ 
+ 	mutex_lock(&priv->state_lock);
+ 
+ 	set_bit(MLX5E_STATE_OPENED, &priv->state);
+ 
+ 	err = mlx5e_open_channels(priv, &priv->channels);
+ 	if (err)
+ 		goto err_clear_state_opened_flag;
+ 
+ 	mlx5e_refresh_tirs(priv, false);
+ 	mlx5e_activate_priv_channels(priv);
+ 	mutex_unlock(&priv->state_lock);
+ 	return 0;
+ 
+ err_clear_state_opened_flag:
+ 	clear_bit(MLX5E_STATE_OPENED, &priv->state);
+ 	mutex_unlock(&priv->state_lock);
+ 	return err;
+ }
+ 
+ static int mlx5i_close(struct net_device *netdev)
+ {
+ 	struct mlx5e_priv *priv = mlx5i_epriv(netdev);
+ 
+ 	/* May already be CLOSED in case a previous configuration operation
+ 	 * (e.g RX/TX queue size change) that involves close&open failed.
+ 	 */
+ 	mutex_lock(&priv->state_lock);
+ 
+ 	if (!test_bit(MLX5E_STATE_OPENED, &priv->state))
+ 		goto unlock;
+ 
+ 	clear_bit(MLX5E_STATE_OPENED, &priv->state);
+ 
+ 	netif_carrier_off(priv->netdev);
+ 	mlx5e_deactivate_priv_channels(priv);
+ 	mlx5e_close_channels(&priv->channels);
+ unlock:
+ 	mutex_unlock(&priv->state_lock);
+ 	return 0;
+ }
+ 
+ /* IPoIB RDMA netdev callbacks */
+ static int mlx5i_attach_mcast(struct net_device *netdev, struct ib_device *hca,
+ 			      union ib_gid *gid, u16 lid, int set_qkey,
+ 			      u32 qkey)
+ {
+ 	struct mlx5e_priv    *epriv = mlx5i_epriv(netdev);
+ 	struct mlx5_core_dev *mdev  = epriv->mdev;
+ 	struct mlx5i_priv    *ipriv = epriv->ppriv;
+ 	int err;
+ 
+ 	mlx5_core_dbg(mdev, "attaching QPN 0x%x, MGID %pI6\n", ipriv->qp.qpn, gid->raw);
+ 	err = mlx5_core_attach_mcg(mdev, gid, ipriv->qp.qpn);
+ 	if (err)
+ 		mlx5_core_warn(mdev, "failed attaching QPN 0x%x, MGID %pI6\n",
+ 			       ipriv->qp.qpn, gid->raw);
+ 
+ 	if (set_qkey) {
+ 		mlx5_core_dbg(mdev, "%s setting qkey 0x%x\n",
+ 			      netdev->name, qkey);
+ 		ipriv->qkey = qkey;
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int mlx5i_detach_mcast(struct net_device *netdev, struct ib_device *hca,
+ 			      union ib_gid *gid, u16 lid)
+ {
+ 	struct mlx5e_priv    *epriv = mlx5i_epriv(netdev);
+ 	struct mlx5_core_dev *mdev  = epriv->mdev;
+ 	struct mlx5i_priv    *ipriv = epriv->ppriv;
+ 	int err;
+ 
+ 	mlx5_core_dbg(mdev, "detaching QPN 0x%x, MGID %pI6\n", ipriv->qp.qpn, gid->raw);
+ 
+ 	err = mlx5_core_detach_mcg(mdev, gid, ipriv->qp.qpn);
+ 	if (err)
+ 		mlx5_core_dbg(mdev, "failed dettaching QPN 0x%x, MGID %pI6\n",
+ 			      ipriv->qp.qpn, gid->raw);
+ 
+ 	return err;
+ }
+ 
+ static int mlx5i_xmit(struct net_device *dev, struct sk_buff *skb,
+ 		      struct ib_ah *address, u32 dqpn)
++>>>>>>> 693dfd5a3f19 (IB/mlx5: Enable IPoIB acceleration)
  {
  	struct mlx5e_priv *epriv = mlx5i_epriv(dev);
  	struct mlx5e_txqsq *sq   = epriv->txq2sq[skb_get_queue_mapping(skb)];
* Unmerged path drivers/infiniband/hw/mlx5/main.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/Kconfig b/drivers/net/ethernet/mellanox/mlx5/core/Kconfig
index f02f05f1f135..de3de3a534af 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/Kconfig
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Kconfig
@@ -34,6 +34,6 @@ config MLX5_CORE_EN_DCB
 config MLX5_CORE_IPOIB
 	bool "Mellanox Technologies ConnectX-4 IPoIB offloads support"
 	depends on MLX5_CORE_EN
-	default y
+	default n
 	---help---
 	  MLX5 IPoIB offloads & acceleration support.
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/ipoib.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/ipoib.h b/drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
index 89bca182464c..176a5313e62d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
@@ -40,7 +40,9 @@
 
 /* ipoib rdma netdev's private data structure */
 struct mlx5i_priv {
+	struct rdma_netdev rn; /* keep this first */
 	struct mlx5_core_qp qp;
+	u32    qkey;
 	char  *mlx5e_priv[0];
 };
 
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index 61d8bec93f9f..ec6efce6b401 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -1010,6 +1010,25 @@ int mlx5_cmd_destroy_vport_lag(struct mlx5_core_dev *dev);
 bool mlx5_lag_is_active(struct mlx5_core_dev *dev);
 struct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev);
 
+#ifndef CONFIG_MLX5_CORE_IPOIB
+static inline
+struct net_device *mlx5_rdma_netdev_alloc(struct mlx5_core_dev *mdev,
+					  struct ib_device *ibdev,
+					  const char *name,
+					  void (*setup)(struct net_device *))
+{
+	return ERR_PTR(-EOPNOTSUPP);
+}
+
+static inline void mlx5_rdma_netdev_free(struct net_device *netdev) {}
+#else
+struct net_device *mlx5_rdma_netdev_alloc(struct mlx5_core_dev *mdev,
+					  struct ib_device *ibdev,
+					  const char *name,
+					  void (*setup)(struct net_device *));
+void mlx5_rdma_netdev_free(struct net_device *netdev);
+#endif /* CONFIG_MLX5_CORE_IPOIB */
+
 struct mlx5_profile {
 	u64	mask;
 	u8	log_max_qp;
