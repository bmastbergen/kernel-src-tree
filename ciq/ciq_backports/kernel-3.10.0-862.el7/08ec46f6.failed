scsi: hpsa: remove abort handler

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] hpsa: remove abort handler (Joseph Szczypek) [1404073]
Rebuild_FUZZ: 89.66%
commit-author Don Brace <don.brace@microsemi.com>
commit 08ec46f673369f65dfc6fc52fb7fadb39776e81f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/08ec46f6.failed

- simplify the driver
- there are a lot of quirky racy conditions not handled
- causes more aborts/resets when the number of commands to be aborted is
  large, such as in multi-path fail-overs.
- has been turned off in our internal driver since 8/31/2015

	Reviewed-by: Scott Benesh <scott.benesh@microsemi.com>
	Reviewed-by: Scott Teel <scott.teel@microsemi.com>
	Reviewed-by: Kevin Barnett <kevin.barnett@microsemi.com>
	Signed-off-by: Don Brace <don.brace@microsemi.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 08ec46f673369f65dfc6fc52fb7fadb39776e81f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/hpsa.c
diff --cc drivers/scsi/hpsa.c
index badbc6cc3256,9a631e399169..000000000000
--- a/drivers/scsi/hpsa.c
+++ b/drivers/scsi/hpsa.c
@@@ -255,11 -255,9 +255,10 @@@ static int hpsa_scsi_queue_command(stru
  static void hpsa_scan_start(struct Scsi_Host *);
  static int hpsa_scan_finished(struct Scsi_Host *sh,
  	unsigned long elapsed_time);
 -static int hpsa_change_queue_depth(struct scsi_device *sdev, int qdepth);
 +static int hpsa_change_queue_depth(struct scsi_device *sdev,
 +	int qdepth, int reason);
  
  static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd);
- static int hpsa_eh_abort_handler(struct scsi_cmnd *scsicmd);
  static int hpsa_slave_alloc(struct scsi_device *sdev);
  static int hpsa_slave_configure(struct scsi_device *sdev);
  static void hpsa_slave_destroy(struct scsi_device *sdev);
@@@ -6007,433 -5838,6 +5858,436 @@@ return_reset_status
  	return rc;
  }
  
++<<<<<<< HEAD
 +static void swizzle_abort_tag(u8 *tag)
 +{
 +	u8 original_tag[8];
 +
 +	memcpy(original_tag, tag, 8);
 +	tag[0] = original_tag[3];
 +	tag[1] = original_tag[2];
 +	tag[2] = original_tag[1];
 +	tag[3] = original_tag[0];
 +	tag[4] = original_tag[7];
 +	tag[5] = original_tag[6];
 +	tag[6] = original_tag[5];
 +	tag[7] = original_tag[4];
 +}
 +
 +static void hpsa_get_tag(struct ctlr_info *h,
 +	struct CommandList *c, __le32 *taglower, __le32 *tagupper)
 +{
 +	u64 tag;
 +	if (c->cmd_type == CMD_IOACCEL1) {
 +		struct io_accel1_cmd *cm1 = (struct io_accel1_cmd *)
 +			&h->ioaccel_cmd_pool[c->cmdindex];
 +		tag = le64_to_cpu(cm1->tag);
 +		*tagupper = cpu_to_le32(tag >> 32);
 +		*taglower = cpu_to_le32(tag);
 +		return;
 +	}
 +	if (c->cmd_type == CMD_IOACCEL2) {
 +		struct io_accel2_cmd *cm2 = (struct io_accel2_cmd *)
 +			&h->ioaccel2_cmd_pool[c->cmdindex];
 +		/* upper tag not used in ioaccel2 mode */
 +		memset(tagupper, 0, sizeof(*tagupper));
 +		*taglower = cm2->Tag;
 +		return;
 +	}
 +	tag = le64_to_cpu(c->Header.tag);
 +	*tagupper = cpu_to_le32(tag >> 32);
 +	*taglower = cpu_to_le32(tag);
 +}
 +
 +static int hpsa_send_abort(struct ctlr_info *h, unsigned char *scsi3addr,
 +	struct CommandList *abort, int reply_queue)
 +{
 +	int rc = IO_OK;
 +	struct CommandList *c;
 +	struct ErrorInfo *ei;
 +	__le32 tagupper, taglower;
 +
 +	c = cmd_alloc(h);
 +
 +	/* fill_cmd can't fail here, no buffer to map */
 +	(void) fill_cmd(c, HPSA_ABORT_MSG, h, &abort->Header.tag,
 +		0, 0, scsi3addr, TYPE_MSG);
 +	if (h->needs_abort_tags_swizzled)
 +		swizzle_abort_tag(&c->Request.CDB[4]);
 +	(void) hpsa_scsi_do_simple_cmd(h, c, reply_queue, DEFAULT_TIMEOUT);
 +	hpsa_get_tag(h, abort, &taglower, &tagupper);
 +	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: do_simple_cmd(abort) completed.\n",
 +		__func__, tagupper, taglower);
 +	/* no unmap needed here because no data xfer. */
 +
 +	ei = c->err_info;
 +	switch (ei->CommandStatus) {
 +	case CMD_SUCCESS:
 +		break;
 +	case CMD_TMF_STATUS:
 +		rc = hpsa_evaluate_tmf_status(h, c);
 +		break;
 +	case CMD_UNABORTABLE: /* Very common, don't make noise. */
 +		rc = -1;
 +		break;
 +	default:
 +		dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: interpreting error.\n",
 +			__func__, tagupper, taglower);
 +		hpsa_scsi_interpret_error(h, c);
 +		rc = -1;
 +		break;
 +	}
 +	cmd_free(h, c);
 +	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: Finished.\n",
 +		__func__, tagupper, taglower);
 +	return rc;
 +}
 +
 +static void setup_ioaccel2_abort_cmd(struct CommandList *c, struct ctlr_info *h,
 +	struct CommandList *command_to_abort, int reply_queue)
 +{
 +	struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
 +	struct hpsa_tmf_struct *ac = (struct hpsa_tmf_struct *) c2;
 +	struct io_accel2_cmd *c2a =
 +		&h->ioaccel2_cmd_pool[command_to_abort->cmdindex];
 +	struct scsi_cmnd *scmd = command_to_abort->scsi_cmd;
 +	struct hpsa_scsi_dev_t *dev = scmd->device->hostdata;
 +
 +	if (!dev)
 +		return;
 +
 +	/*
 +	 * We're overlaying struct hpsa_tmf_struct on top of something which
 +	 * was allocated as a struct io_accel2_cmd, so we better be sure it
 +	 * actually fits, and doesn't overrun the error info space.
 +	 */
 +	BUILD_BUG_ON(sizeof(struct hpsa_tmf_struct) >
 +			sizeof(struct io_accel2_cmd));
 +	BUG_ON(offsetof(struct io_accel2_cmd, error_data) <
 +			offsetof(struct hpsa_tmf_struct, error_len) +
 +				sizeof(ac->error_len));
 +
 +	c->cmd_type = IOACCEL2_TMF;
 +	c->scsi_cmd = SCSI_CMD_BUSY;
 +
 +	/* Adjust the DMA address to point to the accelerated command buffer */
 +	c->busaddr = (u32) h->ioaccel2_cmd_pool_dhandle +
 +				(c->cmdindex * sizeof(struct io_accel2_cmd));
 +	BUG_ON(c->busaddr & 0x0000007F);
 +
 +	memset(ac, 0, sizeof(*c2)); /* yes this is correct */
 +	ac->iu_type = IOACCEL2_IU_TMF_TYPE;
 +	ac->reply_queue = reply_queue;
 +	ac->tmf = IOACCEL2_TMF_ABORT;
 +	ac->it_nexus = cpu_to_le32(dev->ioaccel_handle);
 +	memset(ac->lun_id, 0, sizeof(ac->lun_id));
 +	ac->tag = cpu_to_le64(c->cmdindex << DIRECT_LOOKUP_SHIFT);
 +	ac->abort_tag = cpu_to_le64(le32_to_cpu(c2a->Tag));
 +	ac->error_ptr = cpu_to_le64(c->busaddr +
 +			offsetof(struct io_accel2_cmd, error_data));
 +	ac->error_len = cpu_to_le32(sizeof(c2->error_data));
 +}
 +
 +/* ioaccel2 path firmware cannot handle abort task requests.
 + * Change abort requests to physical target reset, and send to the
 + * address of the physical disk used for the ioaccel 2 command.
 + * Return 0 on success (IO_OK)
 + *	 -1 on failure
 + */
 +
 +static int hpsa_send_reset_as_abort_ioaccel2(struct ctlr_info *h,
 +	unsigned char *scsi3addr, struct CommandList *abort, int reply_queue)
 +{
 +	int rc = IO_OK;
 +	struct scsi_cmnd *scmd; /* scsi command within request being aborted */
 +	struct hpsa_scsi_dev_t *dev; /* device to which scsi cmd was sent */
 +	unsigned char phys_scsi3addr[8]; /* addr of phys disk with volume */
 +	unsigned char *psa = &phys_scsi3addr[0];
 +
 +	/* Get a pointer to the hpsa logical device. */
 +	scmd = abort->scsi_cmd;
 +	dev = (struct hpsa_scsi_dev_t *)(scmd->device->hostdata);
 +	if (dev == NULL) {
 +		dev_warn(&h->pdev->dev,
 +			"Cannot abort: no device pointer for command.\n");
 +			return -1; /* not abortable */
 +	}
 +
 +	if (h->raid_offload_debug > 0)
 +		dev_info(&h->pdev->dev,
 +			"scsi %d:%d:%d:%d %s scsi3addr 0x%8phN\n",
 +			h->scsi_host->host_no, dev->bus, dev->target, dev->lun,
 +			"Reset as abort", scsi3addr);
 +
 +	if (!dev->offload_enabled) {
 +		dev_warn(&h->pdev->dev,
 +			"Can't abort: device is not operating in HP SSD Smart Path mode.\n");
 +		return -1; /* not abortable */
 +	}
 +
 +	/* Incoming scsi3addr is logical addr. We need physical disk addr. */
 +	if (!hpsa_get_pdisk_of_ioaccel2(h, abort, psa)) {
 +		dev_warn(&h->pdev->dev, "Can't abort: Failed lookup of physical address.\n");
 +		return -1; /* not abortable */
 +	}
 +
 +	/* send the reset */
 +	if (h->raid_offload_debug > 0)
 +		dev_info(&h->pdev->dev,
 +			"Reset as abort: Resetting physical device at scsi3addr 0x%8phN\n",
 +			psa);
 +	rc = hpsa_do_reset(h, dev, psa, HPSA_PHYS_TARGET_RESET, reply_queue);
 +	if (rc != 0) {
 +		dev_warn(&h->pdev->dev,
 +			"Reset as abort: Failed on physical device at scsi3addr 0x%8phN\n",
 +			psa);
 +		return rc; /* failed to reset */
 +	}
 +
 +	/* wait for device to recover */
 +	if (wait_for_device_to_become_ready(h, psa, reply_queue) != 0) {
 +		dev_warn(&h->pdev->dev,
 +			"Reset as abort: Failed: Device never recovered from reset: 0x%8phN\n",
 +			psa);
 +		return -1;  /* failed to recover */
 +	}
 +
 +	/* device recovered */
 +	dev_info(&h->pdev->dev,
 +		"Reset as abort: Device recovered from reset: scsi3addr 0x%8phN\n",
 +		psa);
 +
 +	return rc; /* success */
 +}
 +
 +static int hpsa_send_abort_ioaccel2(struct ctlr_info *h,
 +	struct CommandList *abort, int reply_queue)
 +{
 +	int rc = IO_OK;
 +	struct CommandList *c;
 +	__le32 taglower, tagupper;
 +	struct hpsa_scsi_dev_t *dev;
 +	struct io_accel2_cmd *c2;
 +
 +	dev = abort->scsi_cmd->device->hostdata;
 +	if (!dev)
 +		return -1;
 +
 +	if (!dev->offload_enabled && !dev->hba_ioaccel_enabled)
 +		return -1;
 +
 +	c = cmd_alloc(h);
 +	setup_ioaccel2_abort_cmd(c, h, abort, reply_queue);
 +	c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
 +	(void) hpsa_scsi_do_simple_cmd(h, c, reply_queue, DEFAULT_TIMEOUT);
 +	hpsa_get_tag(h, abort, &taglower, &tagupper);
 +	dev_dbg(&h->pdev->dev,
 +		"%s: Tag:0x%08x:%08x: do_simple_cmd(ioaccel2 abort) completed.\n",
 +		__func__, tagupper, taglower);
 +	/* no unmap needed here because no data xfer. */
 +
 +	dev_dbg(&h->pdev->dev,
 +		"%s: Tag:0x%08x:%08x: abort service response = 0x%02x.\n",
 +		__func__, tagupper, taglower, c2->error_data.serv_response);
 +	switch (c2->error_data.serv_response) {
 +	case IOACCEL2_SERV_RESPONSE_TMF_COMPLETE:
 +	case IOACCEL2_SERV_RESPONSE_TMF_SUCCESS:
 +		rc = 0;
 +		break;
 +	case IOACCEL2_SERV_RESPONSE_TMF_REJECTED:
 +	case IOACCEL2_SERV_RESPONSE_FAILURE:
 +	case IOACCEL2_SERV_RESPONSE_TMF_WRONG_LUN:
 +		rc = -1;
 +		break;
 +	default:
 +		dev_warn(&h->pdev->dev,
 +			"%s: Tag:0x%08x:%08x: unknown abort service response 0x%02x\n",
 +			__func__, tagupper, taglower,
 +			c2->error_data.serv_response);
 +		rc = -1;
 +	}
 +	cmd_free(h, c);
 +	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: Finished.\n", __func__,
 +		tagupper, taglower);
 +	return rc;
 +}
 +
 +static int hpsa_send_abort_both_ways(struct ctlr_info *h,
 +	struct hpsa_scsi_dev_t *dev, struct CommandList *abort, int reply_queue)
 +{
 +	/*
 +	 * ioccelerator mode 2 commands should be aborted via the
 +	 * accelerated path, since RAID path is unaware of these commands,
 +	 * but not all underlying firmware can handle abort TMF.
 +	 * Change abort to physical device reset when abort TMF is unsupported.
 +	 */
 +	if (abort->cmd_type == CMD_IOACCEL2) {
 +		if ((HPSATMF_IOACCEL_ENABLED & h->TMFSupportFlags) ||
 +			dev->physical_device)
 +			return hpsa_send_abort_ioaccel2(h, abort,
 +						reply_queue);
 +		else
 +			return hpsa_send_reset_as_abort_ioaccel2(h,
 +							dev->scsi3addr,
 +							abort, reply_queue);
 +	}
 +	return hpsa_send_abort(h, dev->scsi3addr, abort, reply_queue);
 +}
 +
 +/* Find out which reply queue a command was meant to return on */
 +static int hpsa_extract_reply_queue(struct ctlr_info *h,
 +					struct CommandList *c)
 +{
 +	if (c->cmd_type == CMD_IOACCEL2)
 +		return h->ioaccel2_cmd_pool[c->cmdindex].reply_queue;
 +	return c->Header.ReplyQueue;
 +}
 +
 +/*
 + * Limit concurrency of abort commands to prevent
 + * over-subscription of commands
 + */
 +static inline int wait_for_available_abort_cmd(struct ctlr_info *h)
 +{
 +#define ABORT_CMD_WAIT_MSECS 5000
 +	return !wait_event_timeout(h->abort_cmd_wait_queue,
 +			atomic_dec_if_positive(&h->abort_cmds_available) >= 0,
 +			msecs_to_jiffies(ABORT_CMD_WAIT_MSECS));
 +}
 +
 +/* Send an abort for the specified command.
 + *	If the device and controller support it,
 + *		send a task abort request.
 + */
 +static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 +{
 +
 +	int rc;
 +	struct ctlr_info *h;
 +	struct hpsa_scsi_dev_t *dev;
 +	struct CommandList *abort; /* pointer to command to be aborted */
 +	struct scsi_cmnd *as;	/* ptr to scsi cmd inside aborted command. */
 +	char msg[256];		/* For debug messaging. */
 +	int ml = 0;
 +	__le32 tagupper, taglower;
 +	int refcount, reply_queue;
 +
 +	if (sc == NULL)
 +		return FAILED;
 +
 +	if (sc->device == NULL)
 +		return FAILED;
 +
 +	/* Find the controller of the command to be aborted */
 +	h = sdev_to_hba(sc->device);
 +	if (h == NULL)
 +		return FAILED;
 +
 +	/* Find the device of the command to be aborted */
 +	dev = sc->device->hostdata;
 +	if (!dev) {
 +		dev_err(&h->pdev->dev, "%s FAILED, Device lookup failed.\n",
 +				msg);
 +		return FAILED;
 +	}
 +
 +	/* If controller locked up, we can guarantee command won't complete */
 +	if (lockup_detected(h)) {
 +		hpsa_show_dev_msg(KERN_WARNING, h, dev,
 +					"ABORT FAILED, lockup detected");
 +		return FAILED;
 +	}
 +
 +	/* This is a good time to check if controller lockup has occurred */
 +	if (detect_controller_lockup(h)) {
 +		hpsa_show_dev_msg(KERN_WARNING, h, dev,
 +					"ABORT FAILED, new lockup detected");
 +		return FAILED;
 +	}
 +
 +	/* Check that controller supports some kind of task abort */
 +	if (!(HPSATMF_PHYS_TASK_ABORT & h->TMFSupportFlags) &&
 +		!(HPSATMF_LOG_TASK_ABORT & h->TMFSupportFlags))
 +		return FAILED;
 +
 +	memset(msg, 0, sizeof(msg));
 +	ml += sprintf(msg+ml, "scsi %d:%d:%d:%llu %s %p",
 +		h->scsi_host->host_no, sc->device->channel,
 +		sc->device->id, (u64) sc->device->lun,
 +		"Aborting command", sc);
 +
 +	/* Get SCSI command to be aborted */
 +	abort = (struct CommandList *) sc->host_scribble;
 +	if (abort == NULL) {
 +		/* This can happen if the command already completed. */
 +		return SUCCESS;
 +	}
 +	refcount = atomic_inc_return(&abort->refcount);
 +	if (refcount == 1) { /* Command is done already. */
 +		cmd_free(h, abort);
 +		return SUCCESS;
 +	}
 +
 +	/* Don't bother trying the abort if we know it won't work. */
 +	if (abort->cmd_type != CMD_IOACCEL2 &&
 +		abort->cmd_type != CMD_IOACCEL1 && !dev->supports_aborts) {
 +		cmd_free(h, abort);
 +		return FAILED;
 +	}
 +
 +	/*
 +	 * Check that we're aborting the right command.
 +	 * It's possible the CommandList already completed and got re-used.
 +	 */
 +	if (abort->scsi_cmd != sc) {
 +		cmd_free(h, abort);
 +		return SUCCESS;
 +	}
 +
 +	abort->abort_pending = true;
 +	hpsa_get_tag(h, abort, &taglower, &tagupper);
 +	reply_queue = hpsa_extract_reply_queue(h, abort);
 +	ml += sprintf(msg+ml, "Tag:0x%08x:%08x ", tagupper, taglower);
 +	as  = abort->scsi_cmd;
 +	if (as != NULL)
 +		ml += sprintf(msg+ml,
 +			"CDBLen: %d CDB: 0x%02x%02x... SN: 0x%lx ",
 +			as->cmd_len, as->cmnd[0], as->cmnd[1],
 +			as->serial_number);
 +	dev_warn(&h->pdev->dev, "%s BEING SENT\n", msg);
 +	hpsa_show_dev_msg(KERN_WARNING, h, dev, "Aborting command");
 +
 +	/*
 +	 * Command is in flight, or possibly already completed
 +	 * by the firmware (but not to the scsi mid layer) but we can't
 +	 * distinguish which.  Send the abort down.
 +	 */
 +	if (wait_for_available_abort_cmd(h)) {
 +		dev_warn(&h->pdev->dev,
 +			"%s FAILED, timeout waiting for an abort command to become available.\n",
 +			msg);
 +		cmd_free(h, abort);
 +		return FAILED;
 +	}
 +	rc = hpsa_send_abort_both_ways(h, dev, abort, reply_queue);
 +	atomic_inc(&h->abort_cmds_available);
 +	wake_up_all(&h->abort_cmd_wait_queue);
 +	if (rc != 0) {
 +		dev_warn(&h->pdev->dev, "%s SENT, FAILED\n", msg);
 +		hpsa_show_dev_msg(KERN_WARNING, h, dev,
 +				"FAILED to abort command");
 +		cmd_free(h, abort);
 +		return FAILED;
 +	}
 +	dev_info(&h->pdev->dev, "%s SENT, SUCCESS\n", msg);
 +	wait_event(h->event_sync_wait_queue,
 +		   abort->scsi_cmd != sc || lockup_detected(h));
 +	cmd_free(h, abort);
 +	return !lockup_detected(h) ? SUCCESS : FAILED;
 +}
 +
++=======
++>>>>>>> 08ec46f67336 (scsi: hpsa: remove abort handler)
  /*
   * For operations with an associated SCSI command, a command block is allocated
   * at init, and managed by cmd_tagged_alloc() and cmd_tagged_free() using the
* Unmerged path drivers/scsi/hpsa.c
diff --git a/drivers/scsi/hpsa.h b/drivers/scsi/hpsa.h
index 5f3e37b66199..6b5ffe5a7015 100644
--- a/drivers/scsi/hpsa.h
+++ b/drivers/scsi/hpsa.h
@@ -299,7 +299,6 @@ struct ctlr_info {
 	struct workqueue_struct *resubmit_wq;
 	struct workqueue_struct *rescan_ctlr_wq;
 	atomic_t abort_cmds_available;
-	wait_queue_head_t abort_cmd_wait_queue;
 	wait_queue_head_t event_sync_wait_queue;
 	struct mutex reset_mutex;
 	u8 reset_in_progress;
