xfs: don't take the IOLOCK exclusive for direct I/O page invalidation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 0ee7a3f6b5b2f22bb69bfc6c60d0ea0777003098
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/0ee7a3f6.failed

XFS historically took the iolock exclusive when invalidating pages
before direct I/O operations to protect against writeback starvations.

But this writeback starvation issues has been fixed a long time ago
in the core writeback code, and all other file systems manage to do
without the exclusive lock.  Convert XFS over to avoid the exclusive
lock in this case, and also move to range invalidations like done
by the other file systems.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Carlos Maiolino <cmaiolino@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>


(cherry picked from commit 0ee7a3f6b5b2f22bb69bfc6c60d0ea0777003098)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
diff --cc fs/xfs/xfs_file.c
index d8d977d141f3,0dc9971d3c84..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -293,7 -248,9 +293,13 @@@ xfs_file_dio_aio_read
  	struct inode		*inode = mapping->host;
  	struct xfs_inode	*ip = XFS_I(inode);
  	loff_t			isize = i_size_read(inode);
++<<<<<<< HEAD
 +	size_t			size = 0;
++=======
+ 	size_t			count = iov_iter_count(to);
+ 	loff_t			end = iocb->ki_pos + count - 1;
+ 	struct iov_iter		data;
++>>>>>>> 0ee7a3f6b5b2 (xfs: don't take the IOLOCK exclusive for direct I/O page invalidation)
  	struct xfs_buftarg	*target;
  	ssize_t			ret = 0;
  
@@@ -321,57 -273,33 +327,30 @@@
  
  	file_accessed(iocb->ki_filp);
  
- 	/*
- 	 * Locking is a bit tricky here. If we take an exclusive lock for direct
- 	 * IO, we effectively serialise all new concurrent read IO to this file
- 	 * and block it behind IO that is currently in progress because IO in
- 	 * progress holds the IO lock shared. We only need to hold the lock
- 	 * exclusive to blow away the page cache, so only take lock exclusively
- 	 * if the page cache needs invalidation. This allows the normal direct
- 	 * IO case of no page cache pages to proceeed concurrently without
- 	 * serialisation.
- 	 */
  	xfs_rw_ilock(ip, XFS_IOLOCK_SHARED);
  	if (mapping->nrpages) {
- 		xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
- 		xfs_rw_ilock(ip, XFS_IOLOCK_EXCL);
+ 		ret = filemap_write_and_wait_range(mapping, iocb->ki_pos, end);
+ 		if (ret)
+ 			goto out_unlock;
  
  		/*
- 		 * The generic dio code only flushes the range of the particular
- 		 * I/O. Because we take an exclusive lock here, this whole
- 		 * sequence is considerably more expensive for us. This has a
- 		 * noticeable performance impact for any file with cached pages,
- 		 * even when outside of the range of the particular I/O.
- 		 *
- 		 * Hence, amortize the cost of the lock against a full file
- 		 * flush and reduce the chances of repeated iolock cycles going
- 		 * forward.
+ 		 * Invalidate whole pages. This can return an error if we fail
+ 		 * to invalidate a page, but this should never happen on XFS.
+ 		 * Warn if it does fail.
  		 */
- 		if (mapping->nrpages) {
- 			ret = filemap_write_and_wait(mapping);
- 			if (ret) {
- 				xfs_rw_iunlock(ip, XFS_IOLOCK_EXCL);
- 				return ret;
- 			}
- 
- 			/*
- 			 * Invalidate whole pages. This can return an error if
- 			 * we fail to invalidate a page, but this should never
- 			 * happen on XFS. Warn if it does fail.
- 			 */
- 			ret = invalidate_inode_pages2(mapping);
- 			WARN_ON_ONCE(ret);
- 			ret = 0;
- 		}
- 		xfs_rw_ilock_demote(ip, XFS_IOLOCK_EXCL);
+ 		ret = invalidate_inode_pages2_range(mapping,
+ 				iocb->ki_pos >> PAGE_SHIFT, end >> PAGE_SHIFT);
+ 		WARN_ON_ONCE(ret);
+ 		ret = 0;
  	}
 -
 -	data = *to;
 -	ret = __blockdev_direct_IO(iocb, inode, target->bt_bdev, &data,
 -			xfs_get_blocks_direct, NULL, NULL, 0);
 -	if (ret >= 0) {
 -		iocb->ki_pos += ret;
 -		iov_iter_advance(to, ret);
 +	ret = __blockdev_direct_IO(READ, iocb, inode, target->bt_bdev,
 +			iovp, pos, nr_segs, xfs_get_blocks_direct, NULL, NULL, 0);
 +	if (ret > 0) {
 +		iocb->ki_pos = pos + ret;
  	}
- 	xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
  
+ out_unlock:
+ 	xfs_rw_iunlock(ip, XFS_IOLOCK_SHARED);
  	return ret;
  }
  
@@@ -865,48 -541,37 +844,50 @@@ xfs_file_dio_aio_write
  					mp->m_rtdev_targp : mp->m_ddev_targp;
  
  	/* DIO must be aligned to device logical sector size */
 -	if ((iocb->ki_pos | count) & target->bt_logical_sectormask)
 +	if ((pos | count) & target->bt_logical_sectormask)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	/* "unaligned" here means not aligned to a filesystem block */
 +	if ((pos & mp->m_blockmask) || ((pos + count) & mp->m_blockmask))
- 		unaligned_io = 1;
- 
++=======
  	/*
- 	 * We don't need to take an exclusive lock unless there page cache needs
- 	 * to be invalidated or unaligned IO is being executed. We don't need to
- 	 * consider the EOF extension case here because
- 	 * xfs_file_aio_write_checks() will relock the inode as necessary for
- 	 * EOF zeroing cases and fill out the new inode size as appropriate.
+ 	 * Don't take the exclusive iolock here unless the I/O is unaligned to
+ 	 * the file system block size.  We don't need to consider the EOF
+ 	 * extension case here because xfs_file_aio_write_checks() will relock
+ 	 * the inode as necessary for EOF zeroing cases and fill out the new
+ 	 * inode size as appropriate.
  	 */
- 	if (unaligned_io || mapping->nrpages)
+ 	if ((iocb->ki_pos & mp->m_blockmask) ||
+ 	    ((iocb->ki_pos + count) & mp->m_blockmask)) {
++>>>>>>> 0ee7a3f6b5b2 (xfs: don't take the IOLOCK exclusive for direct I/O page invalidation)
+ 		unaligned_io = 1;
  		iolock = XFS_IOLOCK_EXCL;
- 	else
+ 	} else {
  		iolock = XFS_IOLOCK_SHARED;
- 	xfs_rw_ilock(ip, iolock);
- 
- 	/*
- 	 * Recheck if there are cached pages that need invalidate after we got
- 	 * the iolock to protect against other threads adding new pages while
- 	 * we were waiting for the iolock.
- 	 */
- 	if (mapping->nrpages && iolock == XFS_IOLOCK_SHARED) {
- 		xfs_rw_iunlock(ip, iolock);
- 		iolock = XFS_IOLOCK_EXCL;
- 		xfs_rw_ilock(ip, iolock);
  	}
  
++<<<<<<< HEAD
 +	ret = xfs_file_aio_write_checks(file, &pos, &count, &iolock);
++=======
+ 	xfs_rw_ilock(ip, iolock);
+ 
+ 	ret = xfs_file_aio_write_checks(iocb, from, &iolock);
++>>>>>>> 0ee7a3f6b5b2 (xfs: don't take the IOLOCK exclusive for direct I/O page invalidation)
  	if (ret)
  		goto out;
 -	count = iov_iter_count(from);
 -	end = iocb->ki_pos + count - 1;
  
++<<<<<<< HEAD
 +	/*
 +	 * See xfs_file_aio_read() for why we do a full-file flush here.
 +	 */
++=======
++>>>>>>> 0ee7a3f6b5b2 (xfs: don't take the IOLOCK exclusive for direct I/O page invalidation)
  	if (mapping->nrpages) {
- 		ret = filemap_write_and_wait(VFS_I(ip)->i_mapping);
+ 		ret = filemap_write_and_wait_range(mapping, iocb->ki_pos, end);
  		if (ret)
  			goto out;
+ 
  		/*
  		 * Invalidate whole pages. This can return an error if we fail
  		 * to invalidate a page, but this should never happen on XFS.
* Unmerged path fs/xfs/xfs_file.c
