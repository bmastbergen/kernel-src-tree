IB/mlx5: Enable UMR for MRs created with reg_create

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ilya Lesokhin <ilyal@mellanox.com>
commit 8b7ff7f3b301de52924cb2cf3fed47b181893116
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/8b7ff7f3.failed

This patch is the first step in decoupling UMR usage and
allocation from the MR cache. The only functional change
in this patch is to enables UMR for MRs created with
reg_create.

This change fixes a bug where ODP memory regions that
were not allocated from the MR cache did not have UMR
enabled.

	Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 8b7ff7f3b301de52924cb2cf3fed47b181893116)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
#	include/linux/mlx5/driver.h
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 41cc30b2cbb2,bc87016021e3..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -46,14 -46,10 +46,19 @@@ enum 
  };
  
  #define MLX5_UMR_ALIGN 2048
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +static __be64 mlx5_ib_update_mtt_emergency_buffer[
 +		MLX5_UMR_MTT_MIN_CHUNK_SIZE/sizeof(__be64)]
 +	__aligned(MLX5_UMR_ALIGN);
 +static DEFINE_MUTEX(mlx5_ib_update_mtt_emergency_buffer_mutex);
 +#endif
  
  static int clean_mr(struct mlx5_ib_mr *mr);
++<<<<<<< HEAD
++=======
+ static int mr_cache_max_order(struct mlx5_ib_dev *dev);
+ static int unreg_umr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr);
++>>>>>>> 8b7ff7f3b301 (IB/mlx5: Enable UMR for MRs created with reg_create)
  
  static int destroy_mkey(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
  {
@@@ -461,7 -496,8 +466,12 @@@ static struct mlx5_ib_mr *alloc_cached_
  	int i;
  
  	c = order2idx(dev, order);
++<<<<<<< HEAD
 +	if (c < 0 || c >= MAX_MR_CACHE_ENTRIES) {
++=======
+ 	last_umr_cache_entry = order2idx(dev, mr_cache_max_order(dev));
+ 	if (c < 0 || c > last_umr_cache_entry) {
++>>>>>>> 8b7ff7f3b301 (IB/mlx5: Enable UMR for MRs created with reg_create)
  		mlx5_ib_warn(dev, "order %d, cache index %d\n", order, c);
  		return NULL;
  	}
@@@ -633,17 -669,30 +643,38 @@@ int mlx5_mr_cache_init(struct mlx5_ib_d
  		spin_lock_init(&ent->lock);
  		ent->order = i + 2;
  		ent->dev = dev;
 -		ent->limit = 0;
  
 -		init_completion(&ent->compl);
 +		if ((dev->mdev->profile->mask & MLX5_PROF_MASK_MR_CACHE) &&
 +		    (mlx5_core_is_pf(dev->mdev)))
 +			limit = dev->mdev->profile->mr_cache[i].limit;
 +		else
 +			limit = 0;
 +
  		INIT_WORK(&ent->work, cache_work_func);
  		INIT_DELAYED_WORK(&ent->dwork, delayed_cache_work_func);
 +		ent->limit = limit;
  		queue_work(cache->wq, &ent->work);
++<<<<<<< HEAD
++=======
+ 
+ 		if (i > MR_CACHE_LAST_STD_ENTRY) {
+ 			mlx5_odp_init_mr_cache_entry(ent);
+ 			continue;
+ 		}
+ 
+ 		if (ent->order > mr_cache_max_order(dev))
+ 			continue;
+ 
+ 		ent->page = PAGE_SHIFT;
+ 		ent->xlt = (1 << ent->order) * sizeof(struct mlx5_mtt) /
+ 			   MLX5_IB_UMR_OCTOWORD;
+ 		ent->access_mode = MLX5_MKC_ACCESS_MODE_MTT;
+ 		if ((dev->mdev->profile->mask & MLX5_PROF_MASK_MR_CACHE) &&
+ 		    mlx5_core_is_pf(dev->mdev))
+ 			ent->limit = dev->mdev->profile->mr_cache[i].limit;
+ 		else
+ 			ent->limit = 0;
++>>>>>>> 8b7ff7f3b301 (IB/mlx5: Enable UMR for MRs created with reg_create)
  	}
  
  	err = mlx5_mr_cache_debugfs_init(dev);
@@@ -769,98 -818,13 +800,108 @@@ static int get_octo_len(u64 addr, u64 l
  	return (npages + 1) / 2;
  }
  
++<<<<<<< HEAD
 +static int use_umr(int order)
 +{
 +	return order <= MLX5_MAX_UMR_SHIFT;
 +}
 +
 +static int dma_map_mr_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 +			  int npages, int page_shift, int *size,
 +			  __be64 **mr_pas, dma_addr_t *dma)
 +{
 +	__be64 *pas;
 +	struct device *ddev = dev->ib_dev.dev.parent;
 +
 +	/*
 +	 * UMR copies MTTs in units of MLX5_UMR_MTT_ALIGNMENT bytes.
 +	 * To avoid copying garbage after the pas array, we allocate
 +	 * a little more.
 +	 */
 +	*size = ALIGN(sizeof(struct mlx5_mtt) * npages, MLX5_UMR_MTT_ALIGNMENT);
 +	*mr_pas = kmalloc(*size + MLX5_UMR_ALIGN - 1, GFP_KERNEL);
 +	if (!(*mr_pas))
 +		return -ENOMEM;
 +
 +	pas = PTR_ALIGN(*mr_pas, MLX5_UMR_ALIGN);
 +	mlx5_ib_populate_pas(dev, umem, page_shift, pas, MLX5_IB_MTT_PRESENT);
 +	/* Clear padding after the actual pages. */
 +	memset(pas + npages, 0, *size - npages * sizeof(struct mlx5_mtt));
 +
 +	*dma = dma_map_single(ddev, pas, *size, DMA_TO_DEVICE);
 +	if (dma_mapping_error(ddev, *dma)) {
 +		kfree(*mr_pas);
 +		return -ENOMEM;
 +	}
 +
 +	return 0;
 +}
 +
 +static void prep_umr_wqe_common(struct ib_pd *pd, struct ib_send_wr *wr,
 +				struct ib_sge *sg, u64 dma, int n, u32 key,
 +				int page_shift)
 +{
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct mlx5_umr_wr *umrwr = umr_wr(wr);
 +
 +	sg->addr = dma;
 +	sg->length = ALIGN(sizeof(struct mlx5_mtt) * n,
 +			   MLX5_IB_UMR_XLT_ALIGNMENT);
 +	sg->lkey = dev->umrc.pd->local_dma_lkey;
 +
 +	wr->next = NULL;
 +	wr->sg_list = sg;
 +	if (n)
 +		wr->num_sge = 1;
 +	else
 +		wr->num_sge = 0;
 +
 +	wr->opcode = MLX5_IB_WR_UMR;
 +
 +	umrwr->xlt_size = sg->length;
 +	umrwr->page_shift = page_shift;
 +	umrwr->mkey = key;
 +}
 +
 +static void prep_umr_reg_wqe(struct ib_pd *pd, struct ib_send_wr *wr,
 +			     struct ib_sge *sg, u64 dma, int n, u32 key,
 +			     int page_shift, u64 virt_addr, u64 len,
 +			     int access_flags)
 +{
 +	struct mlx5_umr_wr *umrwr = umr_wr(wr);
 +
 +	prep_umr_wqe_common(pd, wr, sg, dma, n, key, page_shift);
 +
 +	wr->send_flags = MLX5_IB_SEND_UMR_ENABLE_MR |
 +			 MLX5_IB_SEND_UMR_UPDATE_TRANSLATION |
 +			 MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
 +
 +	umrwr->virt_addr = virt_addr;
 +	umrwr->length = len;
 +	umrwr->access_flags = access_flags;
 +	umrwr->pd = pd;
 +}
 +
 +static void prep_umr_unreg_wqe(struct mlx5_ib_dev *dev,
 +			       struct ib_send_wr *wr, u32 key)
 +{
 +	struct mlx5_umr_wr *umrwr = umr_wr(wr);
 +
 +	wr->send_flags = MLX5_IB_SEND_UMR_DISABLE_MR |
 +			 MLX5_IB_SEND_UMR_FAIL_IF_FREE;
 +	wr->opcode = MLX5_IB_WR_UMR;
 +	umrwr->mkey = key;
 +}
 +
++=======
+ static int mr_cache_max_order(struct mlx5_ib_dev *dev)
+ {
+ 	if (MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset))
+ 		return MR_CACHE_LAST_STD_ENTRY + 2;
+ 	return MLX5_MAX_UMR_SHIFT;
+ }
+ 
++>>>>>>> 8b7ff7f3b301 (IB/mlx5: Enable UMR for MRs created with reg_create)
  static int mr_umem_get(struct ib_pd *pd, u64 start, u64 length,
  		       int access_flags, struct ib_umem **umem,
  		       int *npages, int *page_shift, int *ncont,
@@@ -1224,7 -1226,7 +1266,11 @@@ struct ib_mr *mlx5_ib_reg_user_mr(struc
          if (err < 0)
  		return ERR_PTR(err);
  
++<<<<<<< HEAD
 +	if (use_umr(order)) {
++=======
+ 	if (order <= mr_cache_max_order(dev)) {
++>>>>>>> 8b7ff7f3b301 (IB/mlx5: Enable UMR for MRs created with reg_create)
  		mr = reg_umr(pd, umem, virt_addr, length, ncont, page_shift,
  			     order, access_flags);
  		if (PTR_ERR(mr) == -EAGAIN) {
@@@ -1535,15 -1482,10 +1581,15 @@@ static int clean_mr(struct mlx5_ib_mr *
  			return err;
  		}
  	} else {
 -		mlx5_mr_cache_free(dev, mr);
 +		err = unreg_umr(dev, mr);
 +		if (err) {
 +			mlx5_ib_warn(dev, "failed unregister\n");
 +			return err;
 +		}
 +		free_cached_mr(dev, mr);
  	}
  
- 	if (!umred)
+ 	if (!allocated_from_cache)
  		kfree(mr);
  
  	return 0;
diff --cc include/linux/mlx5/driver.h
index b0e9c76df334,99d88624ad07..000000000000
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@@ -984,7 -1093,10 +984,14 @@@ enum 
  };
  
  enum {
++<<<<<<< HEAD
 +	MAX_MR_CACHE_ENTRIES    = 16,
++=======
+ 	MR_CACHE_LAST_STD_ENTRY = 20,
+ 	MLX5_IMR_MTT_CACHE_ENTRY,
+ 	MLX5_IMR_KSM_CACHE_ENTRY,
+ 	MAX_MR_CACHE_ENTRIES
++>>>>>>> 8b7ff7f3b301 (IB/mlx5: Enable UMR for MRs created with reg_create)
  };
  
  enum {
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 83418c5208ab..be716836788b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -515,7 +515,7 @@ struct mlx5_ib_mr {
 	struct mlx5_shared_mr_info	*smr_info;
 	struct list_head	list;
 	int			order;
-	int			umred;
+	bool			allocated_from_cache;
 	int			npages;
 	struct mlx5_ib_dev     *dev;
 	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path include/linux/mlx5/driver.h
