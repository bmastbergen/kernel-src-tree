mm: allow full handling of COW faults in ->fault handlers

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] allow full handling of COW faults in ->fault handlers (Larry Woodman) [1457569 1383493 1457572]
Rebuild_FUZZ: 96.36%
commit-author Jan Kara <jack@suse.cz>
commit 3917048d4572b9cabf6f8f5ad395eb693717367c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/3917048d.failed

Patch series "dax: Clear dirty bits after flushing caches", v5.

Patchset to clear dirty bits from radix tree of DAX inodes when caches
for corresponding pfns have been flushed.  In principle, these patches
enable handlers to easily update PTEs and do other work necessary to
finish the fault without duplicating the functionality present in the
generic code.  I'd like to thank Kirill and Ross for reviews of the
series!

This patch (of 20):

To allow full handling of COW faults add memcg field to struct vm_fault
and a return value of ->fault() handler meaning that COW fault is fully
handled and memcg charge must not be canceled.  This will allow us to
remove knowledge about special DAX locking from the generic fault code.

Link: http://lkml.kernel.org/r/1479460644-25076-9-git-send-email-jack@suse.cz
	Signed-off-by: Jan Kara <jack@suse.cz>
	Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3917048d4572b9cabf6f8f5ad395eb693717367c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	mm/memory.c
diff --cc include/linux/mm.h
index a0514d1e5d91,6e25f4916d6f..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -231,14 -286,23 +231,19 @@@ extern pgprot_t protection_map[16]
   * ->fault function. The vma's ->fault is responsible for returning a bitmask
   * of VM_FAULT_xxx flags that give details about how the fault was handled.
   *
 - * MM layer fills up gfp_mask for page allocations but fault handler might
 - * alter it if its implementation requires a different allocation context.
 - *
 - * pgoff should be used in favour of virtual_address, if possible.
 + * pgoff should be used in favour of virtual_address, if possible. If pgoff
 + * is used, one may implement ->remap_pages to get nonlinear mapping support.
   */
  struct vm_fault {
 -	struct vm_area_struct *vma;	/* Target VMA */
  	unsigned int flags;		/* FAULT_FLAG_xxx flags */
 -	gfp_t gfp_mask;			/* gfp mask to be used for allocations */
  	pgoff_t pgoff;			/* Logical page offset based on vma */
 -	unsigned long address;		/* Faulting virtual address */
 -	pmd_t *pmd;			/* Pointer to pmd entry matching
 -					 * the 'address' */
 -	pte_t orig_pte;			/* Value of PTE at the time of fault */
 +	void __user *virtual_address;	/* Faulting virtual address */
  
++<<<<<<< HEAD
++=======
+ 	struct page *cow_page;		/* Page handler may use for COW fault */
+ 	struct mem_cgroup *memcg;	/* Cgroup cow_page belongs to */
++>>>>>>> 3917048d4572 (mm: allow full handling of COW faults in ->fault handlers)
  	struct page *page;		/* ->fault handlers should return a
  					 * page here, unless VM_FAULT_NOPAGE
  					 * is set (which is also implied by
diff --cc mm/memory.c
index 2fc5b28b6782,02504cd4ca0e..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -2821,32 -2833,26 +2821,38 @@@ oom
  	return VM_FAULT_OOM;
  }
  
 -/*
 - * The mmap_sem must have been held on entry, and may have been
 - * released depending on flags and vma->vm_ops->fault() return value.
 - * See filemap_fault() and __lock_page_retry().
 - */
 -static int __do_fault(struct vm_fault *vmf)
 +static int __do_fault(struct vm_area_struct *vma, unsigned long address,
 +			pgoff_t pgoff, unsigned int flags,
 +			struct page *cow_page, struct page **page,
 +			void **entry)
  {
 -	struct vm_area_struct *vma = vmf->vma;
 +	struct vm_fault vmf;
  	int ret;
  
++<<<<<<< HEAD
 +	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
 +	vmf.pgoff = pgoff;
 +	vmf.flags = flags;
 +	vmf.page = NULL;
 +	vmf.cow_page = cow_page;
 +
 +	ret = vma->vm_ops->fault(vma, &vmf);
 +	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 +		return ret;
 +	if (ret & VM_FAULT_DAX_LOCKED) {
 +		*entry = vmf.entry;
++=======
+ 	ret = vma->vm_ops->fault(vma, vmf);
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY |
+ 			    VM_FAULT_DAX_LOCKED | VM_FAULT_DONE_COW)))
++>>>>>>> 3917048d4572 (mm: allow full handling of COW faults in ->fault handlers)
  		return ret;
 +	}
  
 -	if (unlikely(PageHWPoison(vmf->page))) {
 +	if (unlikely(PageHWPoison(vmf.page))) {
  		if (ret & VM_FAULT_LOCKED)
 -			unlock_page(vmf->page);
 -		put_page(vmf->page);
 -		vmf->page = NULL;
 +			unlock_page(vmf.page);
 +		page_cache_release(vmf.page);
  		return VM_FAULT_HWPOISON;
  	}
  
@@@ -2909,61 -3222,49 +2915,91 @@@ static int do_read_fault(struct mm_stru
  	return ret;
  }
  
 -static int do_cow_fault(struct vm_fault *vmf)
 +static int do_cow_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 +		unsigned long address, pmd_t *pmd,
 +		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
  {
++<<<<<<< HEAD
 +	struct page *fault_page, *new_page;
 +	void *fault_entry;
 +	spinlock_t *ptl;
 +	pte_t *pte;
++=======
+ 	struct vm_area_struct *vma = vmf->vma;
++>>>>>>> 3917048d4572 (mm: allow full handling of COW faults in ->fault handlers)
  	int ret;
  
  	if (unlikely(anon_vma_prepare(vma)))
  		return VM_FAULT_OOM;
  
 -	vmf->cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);
 -	if (!vmf->cow_page)
 +	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
 +	if (!new_page)
  		return VM_FAULT_OOM;
  
++<<<<<<< HEAD
 +	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL)) {
 +		page_cache_release(new_page);
++=======
+ 	if (mem_cgroup_try_charge(vmf->cow_page, vma->vm_mm, GFP_KERNEL,
+ 				&vmf->memcg, false)) {
+ 		put_page(vmf->cow_page);
++>>>>>>> 3917048d4572 (mm: allow full handling of COW faults in ->fault handlers)
  		return VM_FAULT_OOM;
  	}
  
 -	ret = __do_fault(vmf);
 +	ret = __do_fault(vma, address, pgoff, flags, new_page, &fault_page,
 +			 &fault_entry);
  	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
  		goto uncharge_out;
+ 	if (ret & VM_FAULT_DONE_COW)
+ 		return ret;
  
  	if (!(ret & VM_FAULT_DAX_LOCKED))
 -		copy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);
 -	__SetPageUptodate(vmf->cow_page);
 +		copy_user_highpage(new_page, fault_page, address, vma);
 +	__SetPageUptodate(new_page);
  
++<<<<<<< HEAD
 +	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 +	if (unlikely(!pte_same(*pte, orig_pte))) {
 +		pte_unmap_unlock(pte, ptl);
 +		if (!(ret & VM_FAULT_DAX_LOCKED)) {
 +			unlock_page(fault_page);
 +			page_cache_release(fault_page);
 +		} else {
 +			dax_unlock_mapping_entry(vma->vm_file->f_mapping,
 +						 pgoff);
 +		}
++=======
+ 	ret |= alloc_set_pte(vmf, vmf->memcg, vmf->cow_page);
+ 	if (vmf->pte)
+ 		pte_unmap_unlock(vmf->pte, vmf->ptl);
+ 	if (!(ret & VM_FAULT_DAX_LOCKED)) {
+ 		unlock_page(vmf->page);
+ 		put_page(vmf->page);
+ 	} else {
+ 		dax_unlock_mapping_entry(vma->vm_file->f_mapping, vmf->pgoff);
+ 	}
+ 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
++>>>>>>> 3917048d4572 (mm: allow full handling of COW faults in ->fault handlers)
  		goto uncharge_out;
 +	}
 +	do_set_pte(vma, address, new_page, pte, true, true);
 +	pte_unmap_unlock(pte, ptl);
 +	if (!(ret & VM_FAULT_DAX_LOCKED)) {
 +		unlock_page(fault_page);
 +		page_cache_release(fault_page);
 +	} else {
 +		dax_unlock_mapping_entry(vma->vm_file->f_mapping, pgoff);
 +	}
  	return ret;
  uncharge_out:
++<<<<<<< HEAD
 +	mem_cgroup_uncharge_page(new_page);
 +	page_cache_release(new_page);
++=======
+ 	mem_cgroup_cancel_charge(vmf->cow_page, vmf->memcg, false);
+ 	put_page(vmf->cow_page);
++>>>>>>> 3917048d4572 (mm: allow full handling of COW faults in ->fault handlers)
  	return ret;
  }
  
* Unmerged path include/linux/mm.h
* Unmerged path mm/memory.c
