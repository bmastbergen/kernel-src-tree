dma: add calls for dma_map_page_attrs and dma_unmap_page_attrs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Alexander Duyck <alexander.h.duyck@intel.com>
commit 0495c3d367944e4af053983ff3cdf256b567b053
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/0495c3d3.failed

Add support for mapping and unmapping a page with attributes.

The primary use for this is currently to allow for us to pass the
DMA_ATTR_SKIP_CPU_SYNC attribute when mapping and unmapping a page.  On
some architectures such as ARM the synchronization has significant
overhead and if we are already taking care of the sync_for_cpu and
sync_for_device from the driver there isn't much need to handle this in
the map/unmap calls as well.

Link: http://lkml.kernel.org/r/20161110113601.76501.46095.stgit@ahduyck-blue-test.jf.intel.com
	Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0495c3d367944e4af053983ff3cdf256b567b053)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/dma-mapping.h
diff --cc include/linux/dma-mapping.h
index fe4bb5f6dd29,10c5a17b1f51..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -88,7 -148,418 +88,405 @@@ static inline int is_device_dma_capable
  #ifdef CONFIG_HAS_DMA
  #include <asm/dma-mapping.h>
  #else
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-broken.h>
++=======
+ /*
+  * Define the dma api to allow compilation but not linking of
+  * dma dependent code.  Code that depends on the dma-mapping
+  * API needs to set 'depends on HAS_DMA' in its Kconfig
+  */
+ extern struct dma_map_ops bad_dma_ops;
+ static inline struct dma_map_ops *get_dma_ops(struct device *dev)
+ {
+ 	return &bad_dma_ops;
+ }
+ #endif
+ 
+ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
+ 					      size_t size,
+ 					      enum dma_data_direction dir,
+ 					      unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 	dma_addr_t addr;
+ 
+ 	kmemcheck_mark_initialized(ptr, size);
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	addr = ops->map_page(dev, virt_to_page(ptr),
+ 			     offset_in_page(ptr), size,
+ 			     dir, attrs);
+ 	debug_dma_map_page(dev, virt_to_page(ptr),
+ 			   offset_in_page(ptr), size,
+ 			   dir, addr, true);
+ 	return addr;
+ }
+ 
+ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
+ 					  size_t size,
+ 					  enum dma_data_direction dir,
+ 					  unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->unmap_page)
+ 		ops->unmap_page(dev, addr, size, dir, attrs);
+ 	debug_dma_unmap_page(dev, addr, size, dir, true);
+ }
+ 
+ /*
+  * dma_maps_sg_attrs returns 0 on error and > 0 on success.
+  * It should never return a value < 0.
+  */
+ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
+ 				   int nents, enum dma_data_direction dir,
+ 				   unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 	int i, ents;
+ 	struct scatterlist *s;
+ 
+ 	for_each_sg(sg, s, nents, i)
+ 		kmemcheck_mark_initialized(sg_virt(s), s->length);
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	ents = ops->map_sg(dev, sg, nents, dir, attrs);
+ 	BUG_ON(ents < 0);
+ 	debug_dma_map_sg(dev, sg, nents, ents, dir);
+ 
+ 	return ents;
+ }
+ 
+ static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,
+ 				      int nents, enum dma_data_direction dir,
+ 				      unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	debug_dma_unmap_sg(dev, sg, nents, dir);
+ 	if (ops->unmap_sg)
+ 		ops->unmap_sg(dev, sg, nents, dir, attrs);
+ }
+ 
+ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
+ 					    struct page *page,
+ 					    size_t offset, size_t size,
+ 					    enum dma_data_direction dir,
+ 					    unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 	dma_addr_t addr;
+ 
+ 	kmemcheck_mark_initialized(page_address(page) + offset, size);
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	addr = ops->map_page(dev, page, offset, size, dir, attrs);
+ 	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
+ 
+ 	return addr;
+ }
+ 
+ static inline void dma_unmap_page_attrs(struct device *dev,
+ 					dma_addr_t addr, size_t size,
+ 					enum dma_data_direction dir,
+ 					unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->unmap_page)
+ 		ops->unmap_page(dev, addr, size, dir, attrs);
+ 	debug_dma_unmap_page(dev, addr, size, dir, false);
+ }
+ 
+ static inline dma_addr_t dma_map_resource(struct device *dev,
+ 					  phys_addr_t phys_addr,
+ 					  size_t size,
+ 					  enum dma_data_direction dir,
+ 					  unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 	dma_addr_t addr;
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 
+ 	/* Don't allow RAM to be mapped */
+ 	BUG_ON(pfn_valid(PHYS_PFN(phys_addr)));
+ 
+ 	addr = phys_addr;
+ 	if (ops->map_resource)
+ 		addr = ops->map_resource(dev, phys_addr, size, dir, attrs);
+ 
+ 	debug_dma_map_resource(dev, phys_addr, size, dir, addr);
+ 
+ 	return addr;
+ }
+ 
+ static inline void dma_unmap_resource(struct device *dev, dma_addr_t addr,
+ 				      size_t size, enum dma_data_direction dir,
+ 				      unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->unmap_resource)
+ 		ops->unmap_resource(dev, addr, size, dir, attrs);
+ 	debug_dma_unmap_resource(dev, addr, size, dir);
+ }
+ 
+ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
+ 					   size_t size,
+ 					   enum dma_data_direction dir)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->sync_single_for_cpu)
+ 		ops->sync_single_for_cpu(dev, addr, size, dir);
+ 	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
+ }
+ 
+ static inline void dma_sync_single_for_device(struct device *dev,
+ 					      dma_addr_t addr, size_t size,
+ 					      enum dma_data_direction dir)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->sync_single_for_device)
+ 		ops->sync_single_for_device(dev, addr, size, dir);
+ 	debug_dma_sync_single_for_device(dev, addr, size, dir);
+ }
+ 
+ static inline void dma_sync_single_range_for_cpu(struct device *dev,
+ 						 dma_addr_t addr,
+ 						 unsigned long offset,
+ 						 size_t size,
+ 						 enum dma_data_direction dir)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->sync_single_for_cpu)
+ 		ops->sync_single_for_cpu(dev, addr + offset, size, dir);
+ 	debug_dma_sync_single_range_for_cpu(dev, addr, offset, size, dir);
+ }
+ 
+ static inline void dma_sync_single_range_for_device(struct device *dev,
+ 						    dma_addr_t addr,
+ 						    unsigned long offset,
+ 						    size_t size,
+ 						    enum dma_data_direction dir)
+ {
+ 	const struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->sync_single_for_device)
+ 		ops->sync_single_for_device(dev, addr + offset, size, dir);
+ 	debug_dma_sync_single_range_for_device(dev, addr, offset, size, dir);
+ }
+ 
+ static inline void
+ dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
+ 		    int nelems, enum dma_data_direction dir)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->sync_sg_for_cpu)
+ 		ops->sync_sg_for_cpu(dev, sg, nelems, dir);
+ 	debug_dma_sync_sg_for_cpu(dev, sg, nelems, dir);
+ }
+ 
+ static inline void
+ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
+ 		       int nelems, enum dma_data_direction dir)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!valid_dma_direction(dir));
+ 	if (ops->sync_sg_for_device)
+ 		ops->sync_sg_for_device(dev, sg, nelems, dir);
+ 	debug_dma_sync_sg_for_device(dev, sg, nelems, dir);
+ 
+ }
+ 
+ #define dma_map_single(d, a, s, r) dma_map_single_attrs(d, a, s, r, 0)
+ #define dma_unmap_single(d, a, s, r) dma_unmap_single_attrs(d, a, s, r, 0)
+ #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
+ #define dma_unmap_sg(d, s, n, r) dma_unmap_sg_attrs(d, s, n, r, 0)
+ #define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
+ #define dma_unmap_page(d, a, s, r) dma_unmap_page_attrs(d, a, s, r, 0)
+ 
+ extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
+ 			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
+ 
+ void *dma_common_contiguous_remap(struct page *page, size_t size,
+ 			unsigned long vm_flags,
+ 			pgprot_t prot, const void *caller);
+ 
+ void *dma_common_pages_remap(struct page **pages, size_t size,
+ 			unsigned long vm_flags, pgprot_t prot,
+ 			const void *caller);
+ void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags);
+ 
+ /**
+  * dma_mmap_attrs - map a coherent DMA allocation into user space
+  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+  * @vma: vm_area_struct describing requested user mapping
+  * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
+  * @handle: device-view address returned from dma_alloc_attrs
+  * @size: size of memory originally requested in dma_alloc_attrs
+  * @attrs: attributes of mapping properties requested in dma_alloc_attrs
+  *
+  * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
+  * into user space.  The coherent DMA buffer must not be freed by the
+  * driver until the user space mapping has been released.
+  */
+ static inline int
+ dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
+ 	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 	BUG_ON(!ops);
+ 	if (ops->mmap)
+ 		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+ 	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size);
+ }
+ 
+ #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
+ 
+ int
+ dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
+ 		       void *cpu_addr, dma_addr_t dma_addr, size_t size);
+ 
+ static inline int
+ dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
+ 		      dma_addr_t dma_addr, size_t size,
+ 		      unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 	BUG_ON(!ops);
+ 	if (ops->get_sgtable)
+ 		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
+ 					attrs);
+ 	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size);
+ }
+ 
+ #define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
+ 
+ #ifndef arch_dma_alloc_attrs
+ #define arch_dma_alloc_attrs(dev, flag)	(true)
+ #endif
+ 
+ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
+ 				       dma_addr_t *dma_handle, gfp_t flag,
+ 				       unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 	void *cpu_addr;
+ 
+ 	BUG_ON(!ops);
+ 
+ 	if (dma_alloc_from_coherent(dev, size, dma_handle, &cpu_addr))
+ 		return cpu_addr;
+ 
+ 	if (!arch_dma_alloc_attrs(&dev, &flag))
+ 		return NULL;
+ 	if (!ops->alloc)
+ 		return NULL;
+ 
+ 	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
+ 	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
+ 	return cpu_addr;
+ }
+ 
+ static inline void dma_free_attrs(struct device *dev, size_t size,
+ 				     void *cpu_addr, dma_addr_t dma_handle,
+ 				     unsigned long attrs)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	BUG_ON(!ops);
+ 	WARN_ON(irqs_disabled());
+ 
+ 	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))
+ 		return;
+ 
+ 	if (!ops->free || !cpu_addr)
+ 		return;
+ 
+ 	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
+ 	ops->free(dev, size, cpu_addr, dma_handle, attrs);
+ }
+ 
+ static inline void *dma_alloc_coherent(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t flag)
+ {
+ 	return dma_alloc_attrs(dev, size, dma_handle, flag, 0);
+ }
+ 
+ static inline void dma_free_coherent(struct device *dev, size_t size,
+ 		void *cpu_addr, dma_addr_t dma_handle)
+ {
+ 	return dma_free_attrs(dev, size, cpu_addr, dma_handle, 0);
+ }
+ 
+ static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
+ 		dma_addr_t *dma_handle, gfp_t gfp)
+ {
+ 	return dma_alloc_attrs(dev, size, dma_handle, gfp,
+ 			       DMA_ATTR_NON_CONSISTENT);
+ }
+ 
+ static inline void dma_free_noncoherent(struct device *dev, size_t size,
+ 		void *cpu_addr, dma_addr_t dma_handle)
+ {
+ 	dma_free_attrs(dev, size, cpu_addr, dma_handle,
+ 		       DMA_ATTR_NON_CONSISTENT);
+ }
+ 
+ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+ {
+ 	debug_dma_mapping_error(dev, dma_addr);
+ 
+ 	if (get_dma_ops(dev)->mapping_error)
+ 		return get_dma_ops(dev)->mapping_error(dev, dma_addr);
+ 
+ #ifdef DMA_ERROR_CODE
+ 	return dma_addr == DMA_ERROR_CODE;
+ #else
+ 	return 0;
+ #endif
+ }
+ 
+ #ifndef HAVE_ARCH_DMA_SUPPORTED
+ static inline int dma_supported(struct device *dev, u64 mask)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (!ops)
+ 		return 0;
+ 	if (!ops->dma_supported)
+ 		return 1;
+ 	return ops->dma_supported(dev, mask);
+ }
+ #endif
+ 
+ #ifndef HAVE_ARCH_DMA_SET_MASK
+ static inline int dma_set_mask(struct device *dev, u64 mask)
+ {
+ 	struct dma_map_ops *ops = get_dma_ops(dev);
+ 
+ 	if (ops->set_dma_mask)
+ 		return ops->set_dma_mask(dev, mask);
+ 
+ 	if (!dev->dma_mask || !dma_supported(dev, mask))
+ 		return -EIO;
+ 	*dev->dma_mask = mask;
+ 	return 0;
+ }
++>>>>>>> 0495c3d36794 (dma: add calls for dma_map_page_attrs and dma_unmap_page_attrs)
  #endif
  
  static inline u64 dma_get_mask(struct device *dev)
* Unmerged path include/linux/dma-mapping.h
