scsi: lpfc: Limit amount of work processed in IRQ

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] lpfc: Limit amount of work processed in IRQ (Dick Kennedy) [1385844 1461977 1387768]
Rebuild_FUZZ: 93.48%
commit-author Dick Kennedy <dick.kennedy@broadcom.com>
commit e3e2863def0262782aec6745bb4c0a86b3dfdd3b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e3e2863d.failed

Various oops being seen on being in the ISR too long and cpu lockups,
when under heavy load.

The amount of work being posted off of completion queues kept the ISR
running almost all the time

Correct the issue by limiting the amount of work per iteration.

[mkp: typo]

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit e3e2863def0262782aec6745bb4c0a86b3dfdd3b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/lpfc/lpfc_sli4.h
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index afe166ddbf5a,d731979e4c7c..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -66,14 -74,19 +66,14 @@@ static struct lpfc_iocbq *lpfc_sli4_els
  							 struct lpfc_iocbq *);
  static void lpfc_sli4_send_seq_to_ulp(struct lpfc_vport *,
  				      struct hbq_dmabuf *);
 -static void lpfc_sli4_handle_mds_loopback(struct lpfc_vport *vport,
 -					  struct hbq_dmabuf *dmabuf);
 -static int lpfc_sli4_fp_handle_cqe(struct lpfc_hba *, struct lpfc_queue *,
 +static int lpfc_sli4_fp_handle_wcqe(struct lpfc_hba *, struct lpfc_queue *,
  				    struct lpfc_cqe *);
 -static int lpfc_sli4_post_sgl_list(struct lpfc_hba *, struct list_head *,
 +static int lpfc_sli4_post_els_sgl_list(struct lpfc_hba *, struct list_head *,
  				       int);
- static void lpfc_sli4_hba_handle_eqe(struct lpfc_hba *, struct lpfc_eqe *,
- 			uint32_t);
+ static int lpfc_sli4_hba_handle_eqe(struct lpfc_hba *phba,
+ 				    struct lpfc_eqe *eqe, uint32_t qidx);
  static bool lpfc_sli4_mbox_completions_pending(struct lpfc_hba *phba);
  static bool lpfc_sli4_process_missed_mbox_completions(struct lpfc_hba *phba);
 -static int lpfc_sli4_abort_nvme_io(struct lpfc_hba *phba,
 -				   struct lpfc_sli_ring *pring,
 -				   struct lpfc_iocbq *cmdiocb);
  
  static IOCB_t *
  lpfc_get_iocb_from_iocbq(struct lpfc_iocbq *iocbq)
@@@ -12288,9 -13034,12 +12288,9 @@@ lpfc_sli4_sp_handle_eqe(struct lpfc_hb
  			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
  					"0365 Slow-path CQ identifier "
  					"(%d) does not exist\n", cqid);
- 		return;
+ 		return 0;
  	}
  
 -	/* Save EQ associated with this CQ */
 -	cq->assoc_qp = speq;
 -
  	/* Process all the entries to the CQ */
  	switch (cq->type) {
  	case LPFC_MCQ:
@@@ -12529,41 -13417,59 +12531,78 @@@ lpfc_sli4_hba_handle_eqe(struct lpfc_hb
  	/* Get the reference to the corresponding CQ */
  	cqid = bf_get_le32(lpfc_eqe_resource_id, eqe);
  
++<<<<<<< HEAD
 +	/* Check if this is a Slow path event */
 +	if (unlikely(cqid != phba->sli4_hba.fcp_cq_map[qidx])) {
 +		lpfc_sli4_sp_handle_eqe(phba, eqe,
 +			phba->sli4_hba.hba_eq[qidx]);
 +		return;
 +	}
 +
 +	if (unlikely(!phba->sli4_hba.fcp_cq)) {
 +		lpfc_printf_log(phba, KERN_WARNING, LOG_SLI,
 +				"3146 Fast-path completion queues "
 +				"does not exist\n");
 +		return;
 +	}
 +	cq = phba->sli4_hba.fcp_cq[qidx];
 +	if (unlikely(!cq)) {
 +		if (phba->sli.sli_flag & LPFC_SLI_ACTIVE)
 +			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
 +					"0367 Fast-path completion queue "
 +					"(%d) does not exist\n", qidx);
 +		return;
++=======
+ 	if (phba->cfg_nvmet_mrq && phba->sli4_hba.nvmet_cqset) {
+ 		id = phba->sli4_hba.nvmet_cqset[0]->queue_id;
+ 		if ((cqid >= id) && (cqid < (id + phba->cfg_nvmet_mrq))) {
+ 			/* Process NVMET unsol rcv */
+ 			cq = phba->sli4_hba.nvmet_cqset[cqid - id];
+ 			goto  process_cq;
+ 		}
+ 	}
+ 
+ 	if (phba->sli4_hba.nvme_cq_map &&
+ 	    (cqid == phba->sli4_hba.nvme_cq_map[qidx])) {
+ 		/* Process NVME / NVMET command completion */
+ 		cq = phba->sli4_hba.nvme_cq[qidx];
+ 		goto  process_cq;
+ 	}
+ 
+ 	if (phba->sli4_hba.fcp_cq_map &&
+ 	    (cqid == phba->sli4_hba.fcp_cq_map[qidx])) {
+ 		/* Process FCP command completion */
+ 		cq = phba->sli4_hba.fcp_cq[qidx];
+ 		goto  process_cq;
+ 	}
+ 
+ 	if (phba->sli4_hba.nvmels_cq &&
+ 	    (cqid == phba->sli4_hba.nvmels_cq->queue_id)) {
+ 		/* Process NVME unsol rcv */
+ 		cq = phba->sli4_hba.nvmels_cq;
+ 	}
+ 
+ 	/* Otherwise this is a Slow path event */
+ 	if (cq == NULL) {
+ 		ecount = lpfc_sli4_sp_handle_eqe(phba, eqe,
+ 						 phba->sli4_hba.hba_eq[qidx]);
+ 		return ecount;
++>>>>>>> e3e2863def02 (scsi: lpfc: Limit amount of work processed in IRQ)
  	}
  
 -process_cq:
  	if (unlikely(cqid != cq->queue_id)) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
  				"0368 Miss-matched fast-path completion "
  				"queue identifier: eqcqid=%d, fcpcqid=%d\n",
  				cqid, cq->queue_id);
- 		return;
+ 		return 0;
  	}
  
 -	/* Save EQ associated with this CQ */
 -	cq->assoc_qp = phba->sli4_hba.hba_eq[qidx];
 -
  	/* Process all the entries to the CQ */
  	while ((cqe = lpfc_sli4_cq_get(cq))) {
 -		workposted |= lpfc_sli4_fp_handle_cqe(phba, cq, cqe);
 +		workposted |= lpfc_sli4_fp_handle_wcqe(phba, cq, cqe);
  		if (!(++ecount % cq->entry_repost))
 -			break;
 +			lpfc_sli4_cq_release(cq, LPFC_QUEUE_NOARM);
  	}
  
  	/* Track the max number of CQEs processed in 1 EQ */
@@@ -12802,12 -13711,13 +12843,17 @@@ lpfc_sli4_hba_intr_handler(int irq, voi
  	struct lpfc_eqe *eqe;
  	unsigned long iflag;
  	int ecount = 0;
++<<<<<<< HEAD
 +	int fcp_eqidx;
++=======
+ 	int ccount = 0;
+ 	int hba_eqidx;
++>>>>>>> e3e2863def02 (scsi: lpfc: Limit amount of work processed in IRQ)
  
  	/* Get the driver's phba structure from the dev_id */
 -	hba_eq_hdl = (struct lpfc_hba_eq_hdl *)dev_id;
 -	phba = hba_eq_hdl->phba;
 -	hba_eqidx = hba_eq_hdl->idx;
 +	fcp_eq_hdl = (struct lpfc_fcp_eq_hdl *)dev_id;
 +	phba = fcp_eq_hdl->phba;
 +	fcp_eqidx = fcp_eq_hdl->idx;
  
  	if (unlikely(!phba))
  		return IRQ_NONE;
@@@ -12849,9 -13763,10 +12895,16 @@@
  		if (eqe == NULL)
  			break;
  
++<<<<<<< HEAD
 +		lpfc_sli4_hba_handle_eqe(phba, eqe, fcp_eqidx);
 +		if (!(++ecount % fpeq->entry_repost))
 +			lpfc_sli4_eq_release(fpeq, LPFC_QUEUE_NOARM);
++=======
+ 		ccount += lpfc_sli4_hba_handle_eqe(phba, eqe, hba_eqidx);
+ 		if (!(++ecount % fpeq->entry_repost) ||
+ 		    ccount > LPFC_MAX_ISR_CQE)
+ 			break;
++>>>>>>> e3e2863def02 (scsi: lpfc: Limit amount of work processed in IRQ)
  		fpeq->EQ_processed++;
  	}
  
diff --cc drivers/scsi/lpfc/lpfc_sli4.h
index 10078254ebc7,540454b03587..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@@ -134,10 -152,16 +134,19 @@@ struct lpfc_queue 
  	uint32_t entry_count;	/* Number of entries to support on the queue */
  	uint32_t entry_size;	/* Size of each queue entry. */
  	uint32_t entry_repost;	/* Count of entries before doorbell is rung */
++<<<<<<< HEAD
 +#define LPFC_QUEUE_MIN_REPOST	8
++=======
+ #define LPFC_EQ_REPOST		8
+ #define LPFC_MQ_REPOST		8
+ #define LPFC_CQ_REPOST		64
+ #define LPFC_RQ_REPOST		64
+ #define LPFC_MAX_ISR_CQE	64
+ #define LPFC_RELEASE_NOTIFICATION_INTERVAL	32  /* For WQs */
++>>>>>>> e3e2863def02 (scsi: lpfc: Limit amount of work processed in IRQ)
  	uint32_t queue_id;	/* Queue ID assigned by the hardware */
  	uint32_t assoc_qid;     /* Queue ID associated with, for CQ/WQ/MQ */
 +	struct list_head page_list;
  	uint32_t page_count;	/* Number of pages allocated for this queue */
  	uint32_t host_index;	/* The host's index for putting or getting */
  	uint32_t hba_index;	/* The last known hba index for get or put */
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli4.h
