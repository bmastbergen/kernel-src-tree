swap: make cluster allocation per-cpu

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Shaohua Li <shli@kernel.org>
commit ebc2a1a69111eadfeda8487e577f1a5d42ef0dae
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ebc2a1a6.failed

swap cluster allocation is to get better request merge to improve
performance.  But the cluster is shared globally, if multiple tasks are
doing swap, this will cause interleave disk access.  While multiple tasks
swap is quite common, for example, each numa node has a kswapd thread
doing swap and multiple threads/processes doing direct page reclaim.

ioscheduler can't help too much here, because tasks don't send swapout IO
down to block layer in the meantime.  Block layer does merge some IOs, but
a lot not, depending on how many tasks are doing swapout concurrently.  In
practice, I've seen a lot of small size IO in swapout workloads.

We makes the cluster allocation per-cpu here.  The interleave disk access
issue goes away.  All tasks swapout to their own cluster, so swapout will
become sequential, which can be easily merged to big size IO.  If one CPU
can't get its per-cpu cluster (for example, there is no free cluster
anymore in the swap), it will fallback to scan swap_map.  The CPU can
still continue swap.  We don't need recycle free swap entries of other
CPUs.

In my test (swap to a 2-disk raid0 partition), this improves around 10%
swapout throughput, and request size is increased significantly.

How does this impact swap readahead is uncertain though.  On one side,
page reclaim always isolates and swaps several adjancent pages, this will
make page reclaim write the pages sequentially and benefit readahead.  On
the other side, several CPU write pages interleave means the pages don't
live _sequentially_ but relatively _near_.  In the per-cpu allocation
case, if adjancent pages are written by different cpus, they will live
relatively _far_.  So how this impacts swap readahead depends on how many
pages page reclaim isolates and swaps one time.  If the number is big,
this patch will benefit swap readahead.  Of course, this is about
sequential access pattern.  The patch has no impact for random access
pattern, because the new cluster allocation algorithm is just for SSD.

Alternative solution is organizing swap layout to be per-mm instead of
this per-cpu approach.  In the per-mm layout, we allocate a disk range for
each mm, so pages of one mm live in swap disk adjacently.  per-mm layout
has potential issues of lock contention if multiple reclaimers are swap
pages from one mm.  For a sequential workload, per-mm layout is better to
implement swap readahead, because pages from the mm are adjacent in disk.
But per-cpu layout isn't very bad in this workload, as page reclaim always
isolates and swaps several pages one time, such pages will still live in
disk sequentially and readahead can utilize this.  For a random workload,
per-mm layout isn't beneficial of request merge, because it's quite
possible pages from different mm are swapout in the meantime and IO can't
be merged in per-mm layout.  while with per-cpu layout we can merge
requests from any mm.  Considering random workload is more popular in
workloads with swap (and per-cpu approach isn't too bad for sequential
workload too), I'm choosing per-cpu layout.

[akpm@linux-foundation.org: coding-style fixes]
	Signed-off-by: Shaohua Li <shli@fusionio.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Minchan Kim <minchan@kernel.org>
	Cc: Kyungmin Park <kmpark@infradead.org>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Rafael Aquini <aquini@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ebc2a1a69111eadfeda8487e577f1a5d42ef0dae)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swap.h
#	mm/swapfile.c
diff --cc include/linux/swap.h
index 61d41d507b58,24db9142e93b..000000000000
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@@ -196,6 -182,33 +196,36 @@@ enum 
  #define SWAP_MAP_SHMEM	0xbf	/* Owned by shmem/tmpfs, in first swap_map */
  
  /*
++<<<<<<< HEAD
++=======
+  * We use this to track usage of a cluster. A cluster is a block of swap disk
+  * space with SWAPFILE_CLUSTER pages long and naturally aligns in disk. All
+  * free clusters are organized into a list. We fetch an entry from the list to
+  * get a free cluster.
+  *
+  * The data field stores next cluster if the cluster is free or cluster usage
+  * counter otherwise. The flags field determines if a cluster is free. This is
+  * protected by swap_info_struct.lock.
+  */
+ struct swap_cluster_info {
+ 	unsigned int data:24;
+ 	unsigned int flags:8;
+ };
+ #define CLUSTER_FLAG_FREE 1 /* This cluster is free */
+ #define CLUSTER_FLAG_NEXT_NULL 2 /* This cluster has no next cluster */
+ 
+ /*
+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from
+  * its own cluster and swapout sequentially. The purpose is to optimize swapout
+  * throughput.
+  */
+ struct percpu_cluster {
+ 	struct swap_cluster_info index; /* Current cluster index */
+ 	unsigned int next; /* Likely next allocation offset */
+ };
+ 
+ /*
++>>>>>>> ebc2a1a69111 (swap: make cluster allocation per-cpu)
   * The in-memory structure used to track swap areas.
   */
  struct swap_info_struct {
@@@ -211,8 -227,7 +241,12 @@@
  	unsigned int inuse_pages;	/* number of those currently in use */
  	unsigned int cluster_next;	/* likely index for next allocation */
  	unsigned int cluster_nr;	/* countdown to next cluster search */
++<<<<<<< HEAD
 +	unsigned int lowest_alloc;	/* while preparing discard cluster */
 +	unsigned int highest_alloc;	/* while preparing discard cluster */
++=======
+ 	struct percpu_cluster __percpu *percpu_cluster; /* per cpu's swap location */
++>>>>>>> ebc2a1a69111 (swap: make cluster allocation per-cpu)
  	struct swap_extent *curr_swap_extent;
  	struct swap_extent first_swap_extent;
  	struct block_device *bdev;	/* swap device or bdev of swap file */
diff --cc mm/swapfile.c
index 44c2eac6b890,3963fc24fcc1..000000000000
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@@ -202,6 -178,294 +202,297 @@@ static void discard_swap_cluster(struc
  #define SWAPFILE_CLUSTER	256
  #define LATENCY_LIMIT		256
  
++<<<<<<< HEAD
++=======
+ static inline void cluster_set_flag(struct swap_cluster_info *info,
+ 	unsigned int flag)
+ {
+ 	info->flags = flag;
+ }
+ 
+ static inline unsigned int cluster_count(struct swap_cluster_info *info)
+ {
+ 	return info->data;
+ }
+ 
+ static inline void cluster_set_count(struct swap_cluster_info *info,
+ 				     unsigned int c)
+ {
+ 	info->data = c;
+ }
+ 
+ static inline void cluster_set_count_flag(struct swap_cluster_info *info,
+ 					 unsigned int c, unsigned int f)
+ {
+ 	info->flags = f;
+ 	info->data = c;
+ }
+ 
+ static inline unsigned int cluster_next(struct swap_cluster_info *info)
+ {
+ 	return info->data;
+ }
+ 
+ static inline void cluster_set_next(struct swap_cluster_info *info,
+ 				    unsigned int n)
+ {
+ 	info->data = n;
+ }
+ 
+ static inline void cluster_set_next_flag(struct swap_cluster_info *info,
+ 					 unsigned int n, unsigned int f)
+ {
+ 	info->flags = f;
+ 	info->data = n;
+ }
+ 
+ static inline bool cluster_is_free(struct swap_cluster_info *info)
+ {
+ 	return info->flags & CLUSTER_FLAG_FREE;
+ }
+ 
+ static inline bool cluster_is_null(struct swap_cluster_info *info)
+ {
+ 	return info->flags & CLUSTER_FLAG_NEXT_NULL;
+ }
+ 
+ static inline void cluster_set_null(struct swap_cluster_info *info)
+ {
+ 	info->flags = CLUSTER_FLAG_NEXT_NULL;
+ 	info->data = 0;
+ }
+ 
+ /* Add a cluster to discard list and schedule it to do discard */
+ static void swap_cluster_schedule_discard(struct swap_info_struct *si,
+ 		unsigned int idx)
+ {
+ 	/*
+ 	 * If scan_swap_map() can't find a free cluster, it will check
+ 	 * si->swap_map directly. To make sure the discarding cluster isn't
+ 	 * taken by scan_swap_map(), mark the swap entries bad (occupied). It
+ 	 * will be cleared after discard
+ 	 */
+ 	memset(si->swap_map + idx * SWAPFILE_CLUSTER,
+ 			SWAP_MAP_BAD, SWAPFILE_CLUSTER);
+ 
+ 	if (cluster_is_null(&si->discard_cluster_head)) {
+ 		cluster_set_next_flag(&si->discard_cluster_head,
+ 						idx, 0);
+ 		cluster_set_next_flag(&si->discard_cluster_tail,
+ 						idx, 0);
+ 	} else {
+ 		unsigned int tail = cluster_next(&si->discard_cluster_tail);
+ 		cluster_set_next(&si->cluster_info[tail], idx);
+ 		cluster_set_next_flag(&si->discard_cluster_tail,
+ 						idx, 0);
+ 	}
+ 
+ 	schedule_work(&si->discard_work);
+ }
+ 
+ /*
+  * Doing discard actually. After a cluster discard is finished, the cluster
+  * will be added to free cluster list. caller should hold si->lock.
+ */
+ static void swap_do_scheduled_discard(struct swap_info_struct *si)
+ {
+ 	struct swap_cluster_info *info;
+ 	unsigned int idx;
+ 
+ 	info = si->cluster_info;
+ 
+ 	while (!cluster_is_null(&si->discard_cluster_head)) {
+ 		idx = cluster_next(&si->discard_cluster_head);
+ 
+ 		cluster_set_next_flag(&si->discard_cluster_head,
+ 						cluster_next(&info[idx]), 0);
+ 		if (cluster_next(&si->discard_cluster_tail) == idx) {
+ 			cluster_set_null(&si->discard_cluster_head);
+ 			cluster_set_null(&si->discard_cluster_tail);
+ 		}
+ 		spin_unlock(&si->lock);
+ 
+ 		discard_swap_cluster(si, idx * SWAPFILE_CLUSTER,
+ 				SWAPFILE_CLUSTER);
+ 
+ 		spin_lock(&si->lock);
+ 		cluster_set_flag(&info[idx], CLUSTER_FLAG_FREE);
+ 		if (cluster_is_null(&si->free_cluster_head)) {
+ 			cluster_set_next_flag(&si->free_cluster_head,
+ 						idx, 0);
+ 			cluster_set_next_flag(&si->free_cluster_tail,
+ 						idx, 0);
+ 		} else {
+ 			unsigned int tail;
+ 
+ 			tail = cluster_next(&si->free_cluster_tail);
+ 			cluster_set_next(&info[tail], idx);
+ 			cluster_set_next_flag(&si->free_cluster_tail,
+ 						idx, 0);
+ 		}
+ 		memset(si->swap_map + idx * SWAPFILE_CLUSTER,
+ 				0, SWAPFILE_CLUSTER);
+ 	}
+ }
+ 
+ static void swap_discard_work(struct work_struct *work)
+ {
+ 	struct swap_info_struct *si;
+ 
+ 	si = container_of(work, struct swap_info_struct, discard_work);
+ 
+ 	spin_lock(&si->lock);
+ 	swap_do_scheduled_discard(si);
+ 	spin_unlock(&si->lock);
+ }
+ 
+ /*
+  * The cluster corresponding to page_nr will be used. The cluster will be
+  * removed from free cluster list and its usage counter will be increased.
+  */
+ static void inc_cluster_info_page(struct swap_info_struct *p,
+ 	struct swap_cluster_info *cluster_info, unsigned long page_nr)
+ {
+ 	unsigned long idx = page_nr / SWAPFILE_CLUSTER;
+ 
+ 	if (!cluster_info)
+ 		return;
+ 	if (cluster_is_free(&cluster_info[idx])) {
+ 		VM_BUG_ON(cluster_next(&p->free_cluster_head) != idx);
+ 		cluster_set_next_flag(&p->free_cluster_head,
+ 			cluster_next(&cluster_info[idx]), 0);
+ 		if (cluster_next(&p->free_cluster_tail) == idx) {
+ 			cluster_set_null(&p->free_cluster_tail);
+ 			cluster_set_null(&p->free_cluster_head);
+ 		}
+ 		cluster_set_count_flag(&cluster_info[idx], 0, 0);
+ 	}
+ 
+ 	VM_BUG_ON(cluster_count(&cluster_info[idx]) >= SWAPFILE_CLUSTER);
+ 	cluster_set_count(&cluster_info[idx],
+ 		cluster_count(&cluster_info[idx]) + 1);
+ }
+ 
+ /*
+  * The cluster corresponding to page_nr decreases one usage. If the usage
+  * counter becomes 0, which means no page in the cluster is in using, we can
+  * optionally discard the cluster and add it to free cluster list.
+  */
+ static void dec_cluster_info_page(struct swap_info_struct *p,
+ 	struct swap_cluster_info *cluster_info, unsigned long page_nr)
+ {
+ 	unsigned long idx = page_nr / SWAPFILE_CLUSTER;
+ 
+ 	if (!cluster_info)
+ 		return;
+ 
+ 	VM_BUG_ON(cluster_count(&cluster_info[idx]) == 0);
+ 	cluster_set_count(&cluster_info[idx],
+ 		cluster_count(&cluster_info[idx]) - 1);
+ 
+ 	if (cluster_count(&cluster_info[idx]) == 0) {
+ 		/*
+ 		 * If the swap is discardable, prepare discard the cluster
+ 		 * instead of free it immediately. The cluster will be freed
+ 		 * after discard.
+ 		 */
+ 		if ((p->flags & (SWP_WRITEOK | SWP_PAGE_DISCARD)) ==
+ 				 (SWP_WRITEOK | SWP_PAGE_DISCARD)) {
+ 			swap_cluster_schedule_discard(p, idx);
+ 			return;
+ 		}
+ 
+ 		cluster_set_flag(&cluster_info[idx], CLUSTER_FLAG_FREE);
+ 		if (cluster_is_null(&p->free_cluster_head)) {
+ 			cluster_set_next_flag(&p->free_cluster_head, idx, 0);
+ 			cluster_set_next_flag(&p->free_cluster_tail, idx, 0);
+ 		} else {
+ 			unsigned int tail = cluster_next(&p->free_cluster_tail);
+ 			cluster_set_next(&cluster_info[tail], idx);
+ 			cluster_set_next_flag(&p->free_cluster_tail, idx, 0);
+ 		}
+ 	}
+ }
+ 
+ /*
+  * It's possible scan_swap_map() uses a free cluster in the middle of free
+  * cluster list. Avoiding such abuse to avoid list corruption.
+  */
+ static bool
+ scan_swap_map_ssd_cluster_conflict(struct swap_info_struct *si,
+ 	unsigned long offset)
+ {
+ 	struct percpu_cluster *percpu_cluster;
+ 	bool conflict;
+ 
+ 	offset /= SWAPFILE_CLUSTER;
+ 	conflict = !cluster_is_null(&si->free_cluster_head) &&
+ 		offset != cluster_next(&si->free_cluster_head) &&
+ 		cluster_is_free(&si->cluster_info[offset]);
+ 
+ 	if (!conflict)
+ 		return false;
+ 
+ 	percpu_cluster = this_cpu_ptr(si->percpu_cluster);
+ 	cluster_set_null(&percpu_cluster->index);
+ 	return true;
+ }
+ 
+ /*
+  * Try to get a swap entry from current cpu's swap entry pool (a cluster). This
+  * might involve allocating a new cluster for current CPU too.
+  */
+ static void scan_swap_map_try_ssd_cluster(struct swap_info_struct *si,
+ 	unsigned long *offset, unsigned long *scan_base)
+ {
+ 	struct percpu_cluster *cluster;
+ 	bool found_free;
+ 	unsigned long tmp;
+ 
+ new_cluster:
+ 	cluster = this_cpu_ptr(si->percpu_cluster);
+ 	if (cluster_is_null(&cluster->index)) {
+ 		if (!cluster_is_null(&si->free_cluster_head)) {
+ 			cluster->index = si->free_cluster_head;
+ 			cluster->next = cluster_next(&cluster->index) *
+ 					SWAPFILE_CLUSTER;
+ 		} else if (!cluster_is_null(&si->discard_cluster_head)) {
+ 			/*
+ 			 * we don't have free cluster but have some clusters in
+ 			 * discarding, do discard now and reclaim them
+ 			 */
+ 			swap_do_scheduled_discard(si);
+ 			*scan_base = *offset = si->cluster_next;
+ 			goto new_cluster;
+ 		} else
+ 			return;
+ 	}
+ 
+ 	found_free = false;
+ 
+ 	/*
+ 	 * Other CPUs can use our cluster if they can't find a free cluster,
+ 	 * check if there is still free entry in the cluster
+ 	 */
+ 	tmp = cluster->next;
+ 	while (tmp < si->max && tmp < (cluster_next(&cluster->index) + 1) *
+ 	       SWAPFILE_CLUSTER) {
+ 		if (!si->swap_map[tmp]) {
+ 			found_free = true;
+ 			break;
+ 		}
+ 		tmp++;
+ 	}
+ 	if (!found_free) {
+ 		cluster_set_null(&cluster->index);
+ 		goto new_cluster;
+ 	}
+ 	cluster->next = tmp + 1;
+ 	*offset = tmp;
+ 	*scan_base = tmp;
+ }
+ 
++>>>>>>> ebc2a1a69111 (swap: make cluster allocation per-cpu)
  static unsigned long scan_swap_map(struct swap_info_struct *si,
  				   unsigned char usage)
  {
@@@ -230,19 -499,7 +527,23 @@@
  			si->cluster_nr = SWAPFILE_CLUSTER - 1;
  			goto checks;
  		}
++<<<<<<< HEAD
 +		if (si->flags & SWP_PAGE_DISCARD) {
 +			/*
 +			 * Start range check on racing allocations, in case
 +			 * they overlap the cluster we eventually decide on
 +			 * (we scan without swap_lock to allow preemption).
 +			 * It's hardly conceivable that cluster_nr could be
 +			 * wrapped during our scan, but don't depend on it.
 +			 */
 +			if (si->lowest_alloc)
 +				goto checks;
 +			si->lowest_alloc = si->max;
 +			si->highest_alloc = 0;
 +		}
++=======
+ 
++>>>>>>> ebc2a1a69111 (swap: make cluster allocation per-cpu)
  		spin_unlock(&si->lock);
  
  		/*
@@@ -303,6 -557,10 +604,13 @@@
  	}
  
  checks:
++<<<<<<< HEAD
++=======
+ 	if (si->cluster_info) {
+ 		while (scan_swap_map_ssd_cluster_conflict(si, offset))
+ 			scan_swap_map_try_ssd_cluster(si, &offset, &scan_base);
+ 	}
++>>>>>>> ebc2a1a69111 (swap: make cluster allocation per-cpu)
  	if (!(si->flags & SWP_WRITEOK))
  		goto no_page;
  	if (!si->highest_bit)
@@@ -1690,12 -1925,15 +1998,14 @@@ SYSCALL_DEFINE1(swapoff, const char __u
  	frontswap_map_set(p, NULL);
  	spin_unlock(&p->lock);
  	spin_unlock(&swap_lock);
 -	frontswap_invalidate_area(type);
 +	frontswap_invalidate_area(p->type);
  	mutex_unlock(&swapon_mutex);
+ 	free_percpu(p->percpu_cluster);
+ 	p->percpu_cluster = NULL;
  	vfree(swap_map);
 -	vfree(cluster_info);
  	vfree(frontswap_map);
  	/* Destroy swap account informatin */
 -	swap_cgroup_swapoff(type);
 +	swap_cgroup_swapoff(p->type);
  
  	inode = mapping->host;
  	if (S_ISBLK(inode->i_mode)) {
@@@ -2143,6 -2434,31 +2453,34 @@@ SYSCALL_DEFINE2(swapon, const char __us
  		error = -ENOMEM;
  		goto bad_swap;
  	}
++<<<<<<< HEAD
++=======
+ 	if (p->bdev && blk_queue_nonrot(bdev_get_queue(p->bdev))) {
+ 		p->flags |= SWP_SOLIDSTATE;
+ 		/*
+ 		 * select a random position to start with to help wear leveling
+ 		 * SSD
+ 		 */
+ 		p->cluster_next = 1 + (prandom_u32() % p->highest_bit);
+ 
+ 		cluster_info = vzalloc(DIV_ROUND_UP(maxpages,
+ 			SWAPFILE_CLUSTER) * sizeof(*cluster_info));
+ 		if (!cluster_info) {
+ 			error = -ENOMEM;
+ 			goto bad_swap;
+ 		}
+ 		p->percpu_cluster = alloc_percpu(struct percpu_cluster);
+ 		if (!p->percpu_cluster) {
+ 			error = -ENOMEM;
+ 			goto bad_swap;
+ 		}
+ 		for_each_possible_cpu(i) {
+ 			struct percpu_cluster *cluster;
+ 			cluster = per_cpu_ptr(p->percpu_cluster, i);
+ 			cluster_set_null(&cluster->index);
+ 		}
+ 	}
++>>>>>>> ebc2a1a69111 (swap: make cluster allocation per-cpu)
  
  	error = swap_cgroup_swapon(p->type, maxpages);
  	if (error)
* Unmerged path include/linux/swap.h
* Unmerged path mm/swapfile.c
