mm: Implement new pkey_mprotect() system call

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] Implement new pkey_mprotect() system call (Rui Wang) [1272615]
Rebuild_FUZZ: 95.35%
commit-author Dave Hansen <dave.hansen@linux.intel.com>
commit 7d06d9c9bd813fc956b9c7bffc1b9724009983eb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7d06d9c9.failed

pkey_mprotect() is just like mprotect, except it also takes a
protection key as an argument.  On systems that do not support
protection keys, it still works, but requires that key=0.
Otherwise it does exactly what mprotect does.

I expect it to get used like this, if you want to guarantee that
any mapping you create can *never* be accessed without the right
protection keys set up.

	int real_prot = PROT_READ|PROT_WRITE;
	pkey = pkey_alloc(0, PKEY_DENY_ACCESS);
	ptr = mmap(NULL, PAGE_SIZE, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
	ret = pkey_mprotect(ptr, PAGE_SIZE, real_prot, pkey);

This way, there is *no* window where the mapping is accessible
since it was always either PROT_NONE or had a protection key set
that denied all access.

We settled on 'unsigned long' for the type of the key here.  We
only need 4 bits on x86 today, but I figured that other
architectures might need some more space.

Semantically, we have a bit of a problem if we combine this
syscall with our previously-introduced execute-only support:
What do we do when we mix execute-only pkey use with
pkey_mprotect() use?  For instance:

	pkey_mprotect(ptr, PAGE_SIZE, PROT_WRITE, 6); // set pkey=6
	mprotect(ptr, PAGE_SIZE, PROT_EXEC);  // set pkey=X_ONLY_PKEY?
	mprotect(ptr, PAGE_SIZE, PROT_WRITE); // is pkey=6 again?

To solve that, we make the plain-mprotect()-initiated execute-only
support only apply to VMAs that have the default protection key (0)
set on them.

Proposed semantics:
1. protection key 0 is special and represents the default,
   "unassigned" protection key.  It is always allocated.
2. mprotect() never affects a mapping's pkey_mprotect()-assigned
   protection key. A protection key of 0 (even if set explicitly)
   represents an unassigned protection key.
   2a. mprotect(PROT_EXEC) on a mapping with an assigned protection
       key may or may not result in a mapping with execute-only
       properties.  pkey_mprotect() plus pkey_set() on all threads
       should be used to _guarantee_ execute-only semantics if this
       is not a strong enough semantic.
3. mprotect(PROT_EXEC) may result in an "execute-only" mapping. The
   kernel will internally attempt to allocate and dedicate a
   protection key for the purpose of execute-only mappings.  This
   may not be possible in cases where there are no free protection
   keys available.  It can also happen, of course, in situations
   where there is no hardware support for protection keys.

	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Cc: linux-arch@vger.kernel.org
	Cc: Dave Hansen <dave@sr71.net>
	Cc: arnd@arndb.de
	Cc: linux-api@vger.kernel.org
	Cc: linux-mm@kvack.org
	Cc: luto@kernel.org
	Cc: akpm@linux-foundation.org
	Cc: torvalds@linux-foundation.org
Link: http://lkml.kernel.org/r/20160729163012.3DDD36C4@viggo.jf.intel.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 7d06d9c9bd813fc956b9c7bffc1b9724009983eb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/mmu_context.h
#	arch/x86/include/asm/pkeys.h
#	include/linux/pkeys.h
#	mm/mprotect.c
diff --cc arch/x86/include/asm/mmu_context.h
index a34c7d411865,af0251fc85ed..000000000000
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@@ -3,6 -3,11 +3,14 @@@
  
  #include <asm/desc.h>
  #include <linux/atomic.h>
++<<<<<<< HEAD
++=======
+ #include <linux/mm_types.h>
+ #include <linux/pkeys.h>
+ 
+ #include <trace/events/tlb.h>
+ 
++>>>>>>> 7d06d9c9bd81 (mm: Implement new pkey_mprotect() system call)
  #include <asm/pgalloc.h>
  #include <asm/tlbflush.h>
  #include <asm/paravirt.h>
@@@ -150,7 -175,93 +158,97 @@@ static inline void arch_bprm_mm_init(st
  static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
  			      unsigned long start, unsigned long end)
  {
++<<<<<<< HEAD
 +	mpx_notify_unmap(mm, vma, start, end);
++=======
+ 	/*
+ 	 * mpx_notify_unmap() goes and reads a rarely-hot
+ 	 * cacheline in the mm_struct.  That can be expensive
+ 	 * enough to be seen in profiles.
+ 	 *
+ 	 * The mpx_notify_unmap() call and its contents have been
+ 	 * observed to affect munmap() performance on hardware
+ 	 * where MPX is not present.
+ 	 *
+ 	 * The unlikely() optimizes for the fast case: no MPX
+ 	 * in the CPU, or no MPX use in the process.  Even if
+ 	 * we get this wrong (in the unlikely event that MPX
+ 	 * is widely enabled on some system) the overhead of
+ 	 * MPX itself (reading bounds tables) is expected to
+ 	 * overwhelm the overhead of getting this unlikely()
+ 	 * consistently wrong.
+ 	 */
+ 	if (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))
+ 		mpx_notify_unmap(mm, vma, start, end);
+ }
+ 
+ #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
+ static inline int vma_pkey(struct vm_area_struct *vma)
+ {
+ 	unsigned long vma_pkey_mask = VM_PKEY_BIT0 | VM_PKEY_BIT1 |
+ 				      VM_PKEY_BIT2 | VM_PKEY_BIT3;
+ 
+ 	return (vma->vm_flags & vma_pkey_mask) >> VM_PKEY_SHIFT;
+ }
+ #else
+ static inline int vma_pkey(struct vm_area_struct *vma)
+ {
+ 	return 0;
+ }
+ #endif
+ 
+ static inline bool __pkru_allows_pkey(u16 pkey, bool write)
+ {
+ 	u32 pkru = read_pkru();
+ 
+ 	if (!__pkru_allows_read(pkru, pkey))
+ 		return false;
+ 	if (write && !__pkru_allows_write(pkru, pkey))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * We only want to enforce protection keys on the current process
+  * because we effectively have no access to PKRU for other
+  * processes or any way to tell *which * PKRU in a threaded
+  * process we could use.
+  *
+  * So do not enforce things if the VMA is not from the current
+  * mm, or if we are in a kernel thread.
+  */
+ static inline bool vma_is_foreign(struct vm_area_struct *vma)
+ {
+ 	if (!current->mm)
+ 		return true;
+ 	/*
+ 	 * Should PKRU be enforced on the access to this VMA?  If
+ 	 * the VMA is from another process, then PKRU has no
+ 	 * relevance and should not be enforced.
+ 	 */
+ 	if (current->mm != vma->vm_mm)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+ 		bool write, bool execute, bool foreign)
+ {
+ 	/* pkeys never affect instruction fetches */
+ 	if (execute)
+ 		return true;
+ 	/* allow access if the VMA is not one from this process */
+ 	if (foreign || vma_is_foreign(vma))
+ 		return true;
+ 	return __pkru_allows_pkey(vma_pkey(vma), write);
+ }
+ 
+ static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+ {
+ 	return __pkru_allows_pkey(pte_flags_pkey(pte_flags(pte)), write);
++>>>>>>> 7d06d9c9bd81 (mm: Implement new pkey_mprotect() system call)
  }
  
  #endif /* _ASM_X86_MMU_CONTEXT_H */
diff --cc mm/mprotect.c
index 12cbcc768180,dd3f40a2935f..000000000000
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@@ -332,13 -352,25 +332,28 @@@ fail
  	return error;
  }
  
- SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
- 		unsigned long, prot)
+ /*
+  * pkey==-1 when doing a legacy mprotect()
+  */
+ static int do_mprotect_pkey(unsigned long start, size_t len,
+ 		unsigned long prot, int pkey)
  {
 -	unsigned long nstart, end, tmp, reqprot;
 +	unsigned long vm_flags, nstart, end, tmp, reqprot;
  	struct vm_area_struct *vma, *prev;
  	int error = -EINVAL;
  	const int grows = prot & (PROT_GROWSDOWN|PROT_GROWSUP);
++<<<<<<< HEAD
++=======
+ 	const bool rier = (current->personality & READ_IMPLIES_EXEC) &&
+ 				(prot & PROT_READ);
+ 	/*
+ 	 * A temporary safety check since we are not validating
+ 	 * the pkey before we introduce the allocation code.
+ 	 */
+ 	if (pkey != -1)
+ 		return -EINVAL;
+ 
++>>>>>>> 7d06d9c9bd81 (mm: Implement new pkey_mprotect() system call)
  	prot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);
  	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can't be both */
  		return -EINVAL;
@@@ -392,10 -418,16 +407,23 @@@
  
  	for (nstart = start ; ; ) {
  		unsigned long newflags;
++<<<<<<< HEAD
 +
 +		/* Here we know that vma->vm_start <= nstart < vma->vm_end. */
 +
 +		newflags = vm_flags;
++=======
+ 		int new_vma_pkey;
+ 
+ 		/* Here we know that vma->vm_start <= nstart < vma->vm_end. */
+ 
+ 		/* Does the application expect PROT_READ to imply PROT_EXEC */
+ 		if (rier && (vma->vm_flags & VM_MAYEXEC))
+ 			prot |= PROT_EXEC;
+ 
+ 		new_vma_pkey = arch_override_mprotect_pkey(vma, prot, pkey);
+ 		newflags = calc_vm_prot_bits(prot, new_vma_pkey);
++>>>>>>> 7d06d9c9bd81 (mm: Implement new pkey_mprotect() system call)
  		newflags |= (vma->vm_flags & ~(VM_READ | VM_WRITE | VM_EXEC));
  
  		/* newflags >> 4 shift VM_MAY% in place of VM_% */
* Unmerged path arch/x86/include/asm/pkeys.h
* Unmerged path include/linux/pkeys.h
* Unmerged path arch/x86/include/asm/mmu_context.h
* Unmerged path arch/x86/include/asm/pkeys.h
* Unmerged path include/linux/pkeys.h
* Unmerged path mm/mprotect.c
