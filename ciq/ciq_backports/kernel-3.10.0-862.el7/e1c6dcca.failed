ethernet/broadcom: use core min/max MTU checking

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jarod Wilson <jarod@redhat.com>
commit e1c6dccaf3af291488fbad155d7ee6bc29db262a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e1c6dcca.failed

tg3: min_mtu 60, max_mtu 9000/1500

bnxt: min_mtu 60, max_mtu 9000

bnx2x: min_mtu 46, max_mtu 9600
- Fix up ETH_OVREHEAD -> ETH_OVERHEAD while we're in here, remove
  duplicated defines from bnx2x_link.c.

bnx2: min_mtu 46, max_mtu 9000
- Use more standard ETH_* defines while we're at it.

bcm63xx_enet: min_mtu 46, max_mtu 2028
- compute_hw_mtu was made largely pointless, and thus merged back into
  bcm_enet_change_mtu.

b44: min_mtu 60, max_mtu 1500

CC: netdev@vger.kernel.org
CC: Michael Chan <michael.chan@broadcom.com>
CC: Sony Chacko <sony.chacko@qlogic.com>
CC: Ariel Elior <ariel.elior@qlogic.com>
CC: Dept-HSGLinuxNICDev@qlogic.com
CC: Siva Reddy Kallam <siva.kallam@broadcom.com>
CC: Prashant Sreedharan <prashant@broadcom.com>
	Signed-off-by: Jarod Wilson <jarod@redhat.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit e1c6dccaf3af291488fbad155d7ee6bc29db262a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/broadcom/bcm63xx_enet.c
#	drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
#	drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
diff --cc drivers/net/ethernet/broadcom/bcm63xx_enet.c
index 1f7233bbb0e3,7e513cacb57a..000000000000
--- a/drivers/net/ethernet/broadcom/bcm63xx_enet.c
+++ b/drivers/net/ethernet/broadcom/bcm63xx_enet.c
@@@ -1530,23 -1648,8 +1529,14 @@@ static int bcm_enet_change_mtu(struct n
  	 * it's appended
  	 */
  	priv->rx_skb_size = ALIGN(actual_mtu + ETH_FCS_LEN,
++<<<<<<< HEAD
 +				  BCMENET_DMA_MAXBURST * 4);
 +	return 0;
 +}
++=======
+ 				  priv->dma_maxburst * 4);
++>>>>>>> e1c6dccaf3af (ethernet/broadcom: use core min/max MTU checking)
  
- /*
-  * adjust mtu, can't be called while device is running
-  */
- static int bcm_enet_change_mtu(struct net_device *dev, int new_mtu)
- {
- 	int ret;
- 
- 	if (netif_running(dev))
- 		return -EBUSY;
- 
- 	ret = compute_hw_mtu(netdev_priv(dev), new_mtu);
- 	if (ret)
- 		return ret;
  	dev->mtu = new_mtu;
  	return 0;
  }
@@@ -1637,7 -1736,10 +1627,14 @@@ static int bcm_enet_probe(struct platfo
  		return -ENOMEM;
  	priv = netdev_priv(dev);
  
++<<<<<<< HEAD
 +	ret = compute_hw_mtu(priv, dev->mtu);
++=======
+ 	priv->enet_is_sw = false;
+ 	priv->dma_maxburst = BCMENET_DMA_MAXBURST;
+ 
+ 	ret = bcm_enet_change_mtu(dev, dev->mtu);
++>>>>>>> e1c6dccaf3af (ethernet/broadcom: use core min/max MTU checking)
  	if (ret)
  		goto out;
  
@@@ -1772,7 -1871,10 +1769,14 @@@
  	dev->netdev_ops = &bcm_enet_ops;
  	netif_napi_add(dev, &priv->napi, bcm_enet_poll, 16);
  
++<<<<<<< HEAD
 +	SET_ETHTOOL_OPS(dev, &bcm_enet_ethtool_ops);
++=======
+ 	dev->ethtool_ops = &bcm_enet_ethtool_ops;
+ 	/* MTU range: 46 - 2028 */
+ 	dev->min_mtu = ETH_ZLEN - ETH_HLEN;
+ 	dev->max_mtu = BCMENET_MAX_MTU - VLAN_ETH_HLEN;
++>>>>>>> e1c6dccaf3af (ethernet/broadcom: use core min/max MTU checking)
  	SET_NETDEV_DEV(dev, &pdev->dev);
  
  	ret = register_netdev(dev);
@@@ -1862,8 -1963,868 +1866,871 @@@ struct platform_driver bcm63xx_enet_dri
  };
  
  /*
 - * switch mii access callbacks
 + * reserve & remap memory space shared between all macs
   */
++<<<<<<< HEAD
++=======
+ static int bcmenet_sw_mdio_read(struct bcm_enet_priv *priv,
+ 				int ext, int phy_id, int location)
+ {
+ 	u32 reg;
+ 	int ret;
+ 
+ 	spin_lock_bh(&priv->enetsw_mdio_lock);
+ 	enetsw_writel(priv, 0, ENETSW_MDIOC_REG);
+ 
+ 	reg = ENETSW_MDIOC_RD_MASK |
+ 		(phy_id << ENETSW_MDIOC_PHYID_SHIFT) |
+ 		(location << ENETSW_MDIOC_REG_SHIFT);
+ 
+ 	if (ext)
+ 		reg |= ENETSW_MDIOC_EXT_MASK;
+ 
+ 	enetsw_writel(priv, reg, ENETSW_MDIOC_REG);
+ 	udelay(50);
+ 	ret = enetsw_readw(priv, ENETSW_MDIOD_REG);
+ 	spin_unlock_bh(&priv->enetsw_mdio_lock);
+ 	return ret;
+ }
+ 
+ static void bcmenet_sw_mdio_write(struct bcm_enet_priv *priv,
+ 				 int ext, int phy_id, int location,
+ 				 uint16_t data)
+ {
+ 	u32 reg;
+ 
+ 	spin_lock_bh(&priv->enetsw_mdio_lock);
+ 	enetsw_writel(priv, 0, ENETSW_MDIOC_REG);
+ 
+ 	reg = ENETSW_MDIOC_WR_MASK |
+ 		(phy_id << ENETSW_MDIOC_PHYID_SHIFT) |
+ 		(location << ENETSW_MDIOC_REG_SHIFT);
+ 
+ 	if (ext)
+ 		reg |= ENETSW_MDIOC_EXT_MASK;
+ 
+ 	reg |= data;
+ 
+ 	enetsw_writel(priv, reg, ENETSW_MDIOC_REG);
+ 	udelay(50);
+ 	spin_unlock_bh(&priv->enetsw_mdio_lock);
+ }
+ 
+ static inline int bcm_enet_port_is_rgmii(int portid)
+ {
+ 	return portid >= ENETSW_RGMII_PORT0;
+ }
+ 
+ /*
+  * enet sw PHY polling
+  */
+ static void swphy_poll_timer(unsigned long data)
+ {
+ 	struct bcm_enet_priv *priv = (struct bcm_enet_priv *)data;
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < priv->num_ports; i++) {
+ 		struct bcm63xx_enetsw_port *port;
+ 		int val, j, up, advertise, lpa, speed, duplex, media;
+ 		int external_phy = bcm_enet_port_is_rgmii(i);
+ 		u8 override;
+ 
+ 		port = &priv->used_ports[i];
+ 		if (!port->used)
+ 			continue;
+ 
+ 		if (port->bypass_link)
+ 			continue;
+ 
+ 		/* dummy read to clear */
+ 		for (j = 0; j < 2; j++)
+ 			val = bcmenet_sw_mdio_read(priv, external_phy,
+ 						   port->phy_id, MII_BMSR);
+ 
+ 		if (val == 0xffff)
+ 			continue;
+ 
+ 		up = (val & BMSR_LSTATUS) ? 1 : 0;
+ 		if (!(up ^ priv->sw_port_link[i]))
+ 			continue;
+ 
+ 		priv->sw_port_link[i] = up;
+ 
+ 		/* link changed */
+ 		if (!up) {
+ 			dev_info(&priv->pdev->dev, "link DOWN on %s\n",
+ 				 port->name);
+ 			enetsw_writeb(priv, ENETSW_PORTOV_ENABLE_MASK,
+ 				      ENETSW_PORTOV_REG(i));
+ 			enetsw_writeb(priv, ENETSW_PTCTRL_RXDIS_MASK |
+ 				      ENETSW_PTCTRL_TXDIS_MASK,
+ 				      ENETSW_PTCTRL_REG(i));
+ 			continue;
+ 		}
+ 
+ 		advertise = bcmenet_sw_mdio_read(priv, external_phy,
+ 						 port->phy_id, MII_ADVERTISE);
+ 
+ 		lpa = bcmenet_sw_mdio_read(priv, external_phy, port->phy_id,
+ 					   MII_LPA);
+ 
+ 		/* figure out media and duplex from advertise and LPA values */
+ 		media = mii_nway_result(lpa & advertise);
+ 		duplex = (media & ADVERTISE_FULL) ? 1 : 0;
+ 
+ 		if (media & (ADVERTISE_100FULL | ADVERTISE_100HALF))
+ 			speed = 100;
+ 		else
+ 			speed = 10;
+ 
+ 		if (val & BMSR_ESTATEN) {
+ 			advertise = bcmenet_sw_mdio_read(priv, external_phy,
+ 						port->phy_id, MII_CTRL1000);
+ 
+ 			lpa = bcmenet_sw_mdio_read(priv, external_phy,
+ 						port->phy_id, MII_STAT1000);
+ 
+ 			if (advertise & (ADVERTISE_1000FULL | ADVERTISE_1000HALF)
+ 					&& lpa & (LPA_1000FULL | LPA_1000HALF)) {
+ 				speed = 1000;
+ 				duplex = (lpa & LPA_1000FULL);
+ 			}
+ 		}
+ 
+ 		dev_info(&priv->pdev->dev,
+ 			 "link UP on %s, %dMbps, %s-duplex\n",
+ 			 port->name, speed, duplex ? "full" : "half");
+ 
+ 		override = ENETSW_PORTOV_ENABLE_MASK |
+ 			ENETSW_PORTOV_LINKUP_MASK;
+ 
+ 		if (speed == 1000)
+ 			override |= ENETSW_IMPOV_1000_MASK;
+ 		else if (speed == 100)
+ 			override |= ENETSW_IMPOV_100_MASK;
+ 		if (duplex)
+ 			override |= ENETSW_IMPOV_FDX_MASK;
+ 
+ 		enetsw_writeb(priv, override, ENETSW_PORTOV_REG(i));
+ 		enetsw_writeb(priv, 0, ENETSW_PTCTRL_REG(i));
+ 	}
+ 
+ 	priv->swphy_poll.expires = jiffies + HZ;
+ 	add_timer(&priv->swphy_poll);
+ }
+ 
+ /*
+  * open callback, allocate dma rings & buffers and start rx operation
+  */
+ static int bcm_enetsw_open(struct net_device *dev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct device *kdev;
+ 	int i, ret;
+ 	unsigned int size;
+ 	void *p;
+ 	u32 val;
+ 
+ 	priv = netdev_priv(dev);
+ 	kdev = &priv->pdev->dev;
+ 
+ 	/* mask all interrupts and request them */
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);
+ 
+ 	ret = request_irq(priv->irq_rx, bcm_enet_isr_dma,
+ 			  0, dev->name, dev);
+ 	if (ret)
+ 		goto out_freeirq;
+ 
+ 	if (priv->irq_tx != -1) {
+ 		ret = request_irq(priv->irq_tx, bcm_enet_isr_dma,
+ 				  0, dev->name, dev);
+ 		if (ret)
+ 			goto out_freeirq_rx;
+ 	}
+ 
+ 	/* allocate rx dma ring */
+ 	size = priv->rx_ring_size * sizeof(struct bcm_enet_desc);
+ 	p = dma_alloc_coherent(kdev, size, &priv->rx_desc_dma, GFP_KERNEL);
+ 	if (!p) {
+ 		dev_err(kdev, "cannot allocate rx ring %u\n", size);
+ 		ret = -ENOMEM;
+ 		goto out_freeirq_tx;
+ 	}
+ 
+ 	memset(p, 0, size);
+ 	priv->rx_desc_alloc_size = size;
+ 	priv->rx_desc_cpu = p;
+ 
+ 	/* allocate tx dma ring */
+ 	size = priv->tx_ring_size * sizeof(struct bcm_enet_desc);
+ 	p = dma_alloc_coherent(kdev, size, &priv->tx_desc_dma, GFP_KERNEL);
+ 	if (!p) {
+ 		dev_err(kdev, "cannot allocate tx ring\n");
+ 		ret = -ENOMEM;
+ 		goto out_free_rx_ring;
+ 	}
+ 
+ 	memset(p, 0, size);
+ 	priv->tx_desc_alloc_size = size;
+ 	priv->tx_desc_cpu = p;
+ 
+ 	priv->tx_skb = kzalloc(sizeof(struct sk_buff *) * priv->tx_ring_size,
+ 			       GFP_KERNEL);
+ 	if (!priv->tx_skb) {
+ 		dev_err(kdev, "cannot allocate rx skb queue\n");
+ 		ret = -ENOMEM;
+ 		goto out_free_tx_ring;
+ 	}
+ 
+ 	priv->tx_desc_count = priv->tx_ring_size;
+ 	priv->tx_dirty_desc = 0;
+ 	priv->tx_curr_desc = 0;
+ 	spin_lock_init(&priv->tx_lock);
+ 
+ 	/* init & fill rx ring with skbs */
+ 	priv->rx_skb = kzalloc(sizeof(struct sk_buff *) * priv->rx_ring_size,
+ 			       GFP_KERNEL);
+ 	if (!priv->rx_skb) {
+ 		dev_err(kdev, "cannot allocate rx skb queue\n");
+ 		ret = -ENOMEM;
+ 		goto out_free_tx_skb;
+ 	}
+ 
+ 	priv->rx_desc_count = 0;
+ 	priv->rx_dirty_desc = 0;
+ 	priv->rx_curr_desc = 0;
+ 
+ 	/* disable all ports */
+ 	for (i = 0; i < priv->num_ports; i++) {
+ 		enetsw_writeb(priv, ENETSW_PORTOV_ENABLE_MASK,
+ 			      ENETSW_PORTOV_REG(i));
+ 		enetsw_writeb(priv, ENETSW_PTCTRL_RXDIS_MASK |
+ 			      ENETSW_PTCTRL_TXDIS_MASK,
+ 			      ENETSW_PTCTRL_REG(i));
+ 
+ 		priv->sw_port_link[i] = 0;
+ 	}
+ 
+ 	/* reset mib */
+ 	val = enetsw_readb(priv, ENETSW_GMCR_REG);
+ 	val |= ENETSW_GMCR_RST_MIB_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_GMCR_REG);
+ 	mdelay(1);
+ 	val &= ~ENETSW_GMCR_RST_MIB_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_GMCR_REG);
+ 	mdelay(1);
+ 
+ 	/* force CPU port state */
+ 	val = enetsw_readb(priv, ENETSW_IMPOV_REG);
+ 	val |= ENETSW_IMPOV_FORCE_MASK | ENETSW_IMPOV_LINKUP_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_IMPOV_REG);
+ 
+ 	/* enable switch forward engine */
+ 	val = enetsw_readb(priv, ENETSW_SWMODE_REG);
+ 	val |= ENETSW_SWMODE_FWD_EN_MASK;
+ 	enetsw_writeb(priv, val, ENETSW_SWMODE_REG);
+ 
+ 	/* enable jumbo on all ports */
+ 	enetsw_writel(priv, 0x1ff, ENETSW_JMBCTL_PORT_REG);
+ 	enetsw_writew(priv, 9728, ENETSW_JMBCTL_MAXSIZE_REG);
+ 
+ 	/* initialize flow control buffer allocation */
+ 	enet_dma_writel(priv, ENETDMA_BUFALLOC_FORCE_MASK | 0,
+ 			ENETDMA_BUFALLOC_REG(priv->rx_chan));
+ 
+ 	if (bcm_enet_refill_rx(dev)) {
+ 		dev_err(kdev, "cannot allocate rx skb queue\n");
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	/* write rx & tx ring addresses */
+ 	enet_dmas_writel(priv, priv->rx_desc_dma,
+ 			 ENETDMAS_RSTART_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, priv->tx_desc_dma,
+ 			 ENETDMAS_RSTART_REG, priv->tx_chan);
+ 
+ 	/* clear remaining state ram for rx & tx channel */
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM2_REG, priv->tx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM3_REG, priv->tx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->rx_chan);
+ 	enet_dmas_writel(priv, 0, ENETDMAS_SRAM4_REG, priv->tx_chan);
+ 
+ 	/* set dma maximum burst len */
+ 	enet_dmac_writel(priv, priv->dma_maxburst,
+ 			 ENETDMAC_MAXBURST, priv->rx_chan);
+ 	enet_dmac_writel(priv, priv->dma_maxburst,
+ 			 ENETDMAC_MAXBURST, priv->tx_chan);
+ 
+ 	/* set flow control low/high threshold to 1/3 / 2/3 */
+ 	val = priv->rx_ring_size / 3;
+ 	enet_dma_writel(priv, val, ENETDMA_FLOWCL_REG(priv->rx_chan));
+ 	val = (priv->rx_ring_size * 2) / 3;
+ 	enet_dma_writel(priv, val, ENETDMA_FLOWCH_REG(priv->rx_chan));
+ 
+ 	/* all set, enable mac and interrupts, start dma engine and
+ 	 * kick rx dma channel
+ 	 */
+ 	wmb();
+ 	enet_dma_writel(priv, ENETDMA_CFG_EN_MASK, ENETDMA_CFG_REG);
+ 	enet_dmac_writel(priv, ENETDMAC_CHANCFG_EN_MASK,
+ 			 ENETDMAC_CHANCFG, priv->rx_chan);
+ 
+ 	/* watch "packet transferred" interrupt in rx and tx */
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IR, priv->rx_chan);
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IR, priv->tx_chan);
+ 
+ 	/* make sure we enable napi before rx interrupt  */
+ 	napi_enable(&priv->napi);
+ 
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IRMASK, priv->rx_chan);
+ 	enet_dmac_writel(priv, ENETDMAC_IR_PKTDONE_MASK,
+ 			 ENETDMAC_IRMASK, priv->tx_chan);
+ 
+ 	netif_carrier_on(dev);
+ 	netif_start_queue(dev);
+ 
+ 	/* apply override config for bypass_link ports here. */
+ 	for (i = 0; i < priv->num_ports; i++) {
+ 		struct bcm63xx_enetsw_port *port;
+ 		u8 override;
+ 		port = &priv->used_ports[i];
+ 		if (!port->used)
+ 			continue;
+ 
+ 		if (!port->bypass_link)
+ 			continue;
+ 
+ 		override = ENETSW_PORTOV_ENABLE_MASK |
+ 			ENETSW_PORTOV_LINKUP_MASK;
+ 
+ 		switch (port->force_speed) {
+ 		case 1000:
+ 			override |= ENETSW_IMPOV_1000_MASK;
+ 			break;
+ 		case 100:
+ 			override |= ENETSW_IMPOV_100_MASK;
+ 			break;
+ 		case 10:
+ 			break;
+ 		default:
+ 			pr_warn("invalid forced speed on port %s: assume 10\n",
+ 			       port->name);
+ 			break;
+ 		}
+ 
+ 		if (port->force_duplex_full)
+ 			override |= ENETSW_IMPOV_FDX_MASK;
+ 
+ 
+ 		enetsw_writeb(priv, override, ENETSW_PORTOV_REG(i));
+ 		enetsw_writeb(priv, 0, ENETSW_PTCTRL_REG(i));
+ 	}
+ 
+ 	/* start phy polling timer */
+ 	init_timer(&priv->swphy_poll);
+ 	priv->swphy_poll.function = swphy_poll_timer;
+ 	priv->swphy_poll.data = (unsigned long)priv;
+ 	priv->swphy_poll.expires = jiffies;
+ 	add_timer(&priv->swphy_poll);
+ 	return 0;
+ 
+ out:
+ 	for (i = 0; i < priv->rx_ring_size; i++) {
+ 		struct bcm_enet_desc *desc;
+ 
+ 		if (!priv->rx_skb[i])
+ 			continue;
+ 
+ 		desc = &priv->rx_desc_cpu[i];
+ 		dma_unmap_single(kdev, desc->address, priv->rx_skb_size,
+ 				 DMA_FROM_DEVICE);
+ 		kfree_skb(priv->rx_skb[i]);
+ 	}
+ 	kfree(priv->rx_skb);
+ 
+ out_free_tx_skb:
+ 	kfree(priv->tx_skb);
+ 
+ out_free_tx_ring:
+ 	dma_free_coherent(kdev, priv->tx_desc_alloc_size,
+ 			  priv->tx_desc_cpu, priv->tx_desc_dma);
+ 
+ out_free_rx_ring:
+ 	dma_free_coherent(kdev, priv->rx_desc_alloc_size,
+ 			  priv->rx_desc_cpu, priv->rx_desc_dma);
+ 
+ out_freeirq_tx:
+ 	if (priv->irq_tx != -1)
+ 		free_irq(priv->irq_tx, dev);
+ 
+ out_freeirq_rx:
+ 	free_irq(priv->irq_rx, dev);
+ 
+ out_freeirq:
+ 	return ret;
+ }
+ 
+ /* stop callback */
+ static int bcm_enetsw_stop(struct net_device *dev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct device *kdev;
+ 	int i;
+ 
+ 	priv = netdev_priv(dev);
+ 	kdev = &priv->pdev->dev;
+ 
+ 	del_timer_sync(&priv->swphy_poll);
+ 	netif_stop_queue(dev);
+ 	napi_disable(&priv->napi);
+ 	del_timer_sync(&priv->rx_timeout);
+ 
+ 	/* mask all interrupts */
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->rx_chan);
+ 	enet_dmac_writel(priv, 0, ENETDMAC_IRMASK, priv->tx_chan);
+ 
+ 	/* disable dma & mac */
+ 	bcm_enet_disable_dma(priv, priv->tx_chan);
+ 	bcm_enet_disable_dma(priv, priv->rx_chan);
+ 
+ 	/* force reclaim of all tx buffers */
+ 	bcm_enet_tx_reclaim(dev, 1);
+ 
+ 	/* free the rx skb ring */
+ 	for (i = 0; i < priv->rx_ring_size; i++) {
+ 		struct bcm_enet_desc *desc;
+ 
+ 		if (!priv->rx_skb[i])
+ 			continue;
+ 
+ 		desc = &priv->rx_desc_cpu[i];
+ 		dma_unmap_single(kdev, desc->address, priv->rx_skb_size,
+ 				 DMA_FROM_DEVICE);
+ 		kfree_skb(priv->rx_skb[i]);
+ 	}
+ 
+ 	/* free remaining allocated memory */
+ 	kfree(priv->rx_skb);
+ 	kfree(priv->tx_skb);
+ 	dma_free_coherent(kdev, priv->rx_desc_alloc_size,
+ 			  priv->rx_desc_cpu, priv->rx_desc_dma);
+ 	dma_free_coherent(kdev, priv->tx_desc_alloc_size,
+ 			  priv->tx_desc_cpu, priv->tx_desc_dma);
+ 	if (priv->irq_tx != -1)
+ 		free_irq(priv->irq_tx, dev);
+ 	free_irq(priv->irq_rx, dev);
+ 
+ 	return 0;
+ }
+ 
+ /* try to sort out phy external status by walking the used_port field
+  * in the bcm_enet_priv structure. in case the phy address is not
+  * assigned to any physical port on the switch, assume it is external
+  * (and yell at the user).
+  */
+ static int bcm_enetsw_phy_is_external(struct bcm_enet_priv *priv, int phy_id)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < priv->num_ports; ++i) {
+ 		if (!priv->used_ports[i].used)
+ 			continue;
+ 		if (priv->used_ports[i].phy_id == phy_id)
+ 			return bcm_enet_port_is_rgmii(i);
+ 	}
+ 
+ 	printk_once(KERN_WARNING  "bcm63xx_enet: could not find a used port with phy_id %i, assuming phy is external\n",
+ 		    phy_id);
+ 	return 1;
+ }
+ 
+ /* can't use bcmenet_sw_mdio_read directly as we need to sort out
+  * external/internal status of the given phy_id first.
+  */
+ static int bcm_enetsw_mii_mdio_read(struct net_device *dev, int phy_id,
+ 				    int location)
+ {
+ 	struct bcm_enet_priv *priv;
+ 
+ 	priv = netdev_priv(dev);
+ 	return bcmenet_sw_mdio_read(priv,
+ 				    bcm_enetsw_phy_is_external(priv, phy_id),
+ 				    phy_id, location);
+ }
+ 
+ /* can't use bcmenet_sw_mdio_write directly as we need to sort out
+  * external/internal status of the given phy_id first.
+  */
+ static void bcm_enetsw_mii_mdio_write(struct net_device *dev, int phy_id,
+ 				      int location,
+ 				      int val)
+ {
+ 	struct bcm_enet_priv *priv;
+ 
+ 	priv = netdev_priv(dev);
+ 	bcmenet_sw_mdio_write(priv, bcm_enetsw_phy_is_external(priv, phy_id),
+ 			      phy_id, location, val);
+ }
+ 
+ static int bcm_enetsw_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+ {
+ 	struct mii_if_info mii;
+ 
+ 	mii.dev = dev;
+ 	mii.mdio_read = bcm_enetsw_mii_mdio_read;
+ 	mii.mdio_write = bcm_enetsw_mii_mdio_write;
+ 	mii.phy_id = 0;
+ 	mii.phy_id_mask = 0x3f;
+ 	mii.reg_num_mask = 0x1f;
+ 	return generic_mii_ioctl(&mii, if_mii(rq), cmd, NULL);
+ 
+ }
+ 
+ static const struct net_device_ops bcm_enetsw_ops = {
+ 	.ndo_open		= bcm_enetsw_open,
+ 	.ndo_stop		= bcm_enetsw_stop,
+ 	.ndo_start_xmit		= bcm_enet_start_xmit,
+ 	.ndo_change_mtu		= bcm_enet_change_mtu,
+ 	.ndo_do_ioctl		= bcm_enetsw_ioctl,
+ };
+ 
+ 
+ static const struct bcm_enet_stats bcm_enetsw_gstrings_stats[] = {
+ 	{ "rx_packets", DEV_STAT(rx_packets), -1 },
+ 	{ "tx_packets",	DEV_STAT(tx_packets), -1 },
+ 	{ "rx_bytes", DEV_STAT(rx_bytes), -1 },
+ 	{ "tx_bytes", DEV_STAT(tx_bytes), -1 },
+ 	{ "rx_errors", DEV_STAT(rx_errors), -1 },
+ 	{ "tx_errors", DEV_STAT(tx_errors), -1 },
+ 	{ "rx_dropped",	DEV_STAT(rx_dropped), -1 },
+ 	{ "tx_dropped",	DEV_STAT(tx_dropped), -1 },
+ 
+ 	{ "tx_good_octets", GEN_STAT(mib.tx_gd_octets), ETHSW_MIB_RX_GD_OCT },
+ 	{ "tx_unicast", GEN_STAT(mib.tx_unicast), ETHSW_MIB_RX_BRDCAST },
+ 	{ "tx_broadcast", GEN_STAT(mib.tx_brdcast), ETHSW_MIB_RX_BRDCAST },
+ 	{ "tx_multicast", GEN_STAT(mib.tx_mult), ETHSW_MIB_RX_MULT },
+ 	{ "tx_64_octets", GEN_STAT(mib.tx_64), ETHSW_MIB_RX_64 },
+ 	{ "tx_65_127_oct", GEN_STAT(mib.tx_65_127), ETHSW_MIB_RX_65_127 },
+ 	{ "tx_128_255_oct", GEN_STAT(mib.tx_128_255), ETHSW_MIB_RX_128_255 },
+ 	{ "tx_256_511_oct", GEN_STAT(mib.tx_256_511), ETHSW_MIB_RX_256_511 },
+ 	{ "tx_512_1023_oct", GEN_STAT(mib.tx_512_1023), ETHSW_MIB_RX_512_1023},
+ 	{ "tx_1024_1522_oct", GEN_STAT(mib.tx_1024_max),
+ 	  ETHSW_MIB_RX_1024_1522 },
+ 	{ "tx_1523_2047_oct", GEN_STAT(mib.tx_1523_2047),
+ 	  ETHSW_MIB_RX_1523_2047 },
+ 	{ "tx_2048_4095_oct", GEN_STAT(mib.tx_2048_4095),
+ 	  ETHSW_MIB_RX_2048_4095 },
+ 	{ "tx_4096_8191_oct", GEN_STAT(mib.tx_4096_8191),
+ 	  ETHSW_MIB_RX_4096_8191 },
+ 	{ "tx_8192_9728_oct", GEN_STAT(mib.tx_8192_9728),
+ 	  ETHSW_MIB_RX_8192_9728 },
+ 	{ "tx_oversize", GEN_STAT(mib.tx_ovr), ETHSW_MIB_RX_OVR },
+ 	{ "tx_oversize_drop", GEN_STAT(mib.tx_ovr), ETHSW_MIB_RX_OVR_DISC },
+ 	{ "tx_dropped",	GEN_STAT(mib.tx_drop), ETHSW_MIB_RX_DROP },
+ 	{ "tx_undersize", GEN_STAT(mib.tx_underrun), ETHSW_MIB_RX_UND },
+ 	{ "tx_pause", GEN_STAT(mib.tx_pause), ETHSW_MIB_RX_PAUSE },
+ 
+ 	{ "rx_good_octets", GEN_STAT(mib.rx_gd_octets), ETHSW_MIB_TX_ALL_OCT },
+ 	{ "rx_broadcast", GEN_STAT(mib.rx_brdcast), ETHSW_MIB_TX_BRDCAST },
+ 	{ "rx_multicast", GEN_STAT(mib.rx_mult), ETHSW_MIB_TX_MULT },
+ 	{ "rx_unicast", GEN_STAT(mib.rx_unicast), ETHSW_MIB_TX_MULT },
+ 	{ "rx_pause", GEN_STAT(mib.rx_pause), ETHSW_MIB_TX_PAUSE },
+ 	{ "rx_dropped", GEN_STAT(mib.rx_drop), ETHSW_MIB_TX_DROP_PKTS },
+ 
+ };
+ 
+ #define BCM_ENETSW_STATS_LEN	\
+ 	(sizeof(bcm_enetsw_gstrings_stats) / sizeof(struct bcm_enet_stats))
+ 
+ static void bcm_enetsw_get_strings(struct net_device *netdev,
+ 				   u32 stringset, u8 *data)
+ {
+ 	int i;
+ 
+ 	switch (stringset) {
+ 	case ETH_SS_STATS:
+ 		for (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {
+ 			memcpy(data + i * ETH_GSTRING_LEN,
+ 			       bcm_enetsw_gstrings_stats[i].stat_string,
+ 			       ETH_GSTRING_LEN);
+ 		}
+ 		break;
+ 	}
+ }
+ 
+ static int bcm_enetsw_get_sset_count(struct net_device *netdev,
+ 				     int string_set)
+ {
+ 	switch (string_set) {
+ 	case ETH_SS_STATS:
+ 		return BCM_ENETSW_STATS_LEN;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
+ static void bcm_enetsw_get_drvinfo(struct net_device *netdev,
+ 				   struct ethtool_drvinfo *drvinfo)
+ {
+ 	strncpy(drvinfo->driver, bcm_enet_driver_name, 32);
+ 	strncpy(drvinfo->version, bcm_enet_driver_version, 32);
+ 	strncpy(drvinfo->fw_version, "N/A", 32);
+ 	strncpy(drvinfo->bus_info, "bcm63xx", 32);
+ }
+ 
+ static void bcm_enetsw_get_ethtool_stats(struct net_device *netdev,
+ 					 struct ethtool_stats *stats,
+ 					 u64 *data)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	int i;
+ 
+ 	priv = netdev_priv(netdev);
+ 
+ 	for (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {
+ 		const struct bcm_enet_stats *s;
+ 		u32 lo, hi;
+ 		char *p;
+ 		int reg;
+ 
+ 		s = &bcm_enetsw_gstrings_stats[i];
+ 
+ 		reg = s->mib_reg;
+ 		if (reg == -1)
+ 			continue;
+ 
+ 		lo = enetsw_readl(priv, ENETSW_MIB_REG(reg));
+ 		p = (char *)priv + s->stat_offset;
+ 
+ 		if (s->sizeof_stat == sizeof(u64)) {
+ 			hi = enetsw_readl(priv, ENETSW_MIB_REG(reg + 1));
+ 			*(u64 *)p = ((u64)hi << 32 | lo);
+ 		} else {
+ 			*(u32 *)p = lo;
+ 		}
+ 	}
+ 
+ 	for (i = 0; i < BCM_ENETSW_STATS_LEN; i++) {
+ 		const struct bcm_enet_stats *s;
+ 		char *p;
+ 
+ 		s = &bcm_enetsw_gstrings_stats[i];
+ 
+ 		if (s->mib_reg == -1)
+ 			p = (char *)&netdev->stats + s->stat_offset;
+ 		else
+ 			p = (char *)priv + s->stat_offset;
+ 
+ 		data[i] = (s->sizeof_stat == sizeof(u64)) ?
+ 			*(u64 *)p : *(u32 *)p;
+ 	}
+ }
+ 
+ static void bcm_enetsw_get_ringparam(struct net_device *dev,
+ 				     struct ethtool_ringparam *ering)
+ {
+ 	struct bcm_enet_priv *priv;
+ 
+ 	priv = netdev_priv(dev);
+ 
+ 	/* rx/tx ring is actually only limited by memory */
+ 	ering->rx_max_pending = 8192;
+ 	ering->tx_max_pending = 8192;
+ 	ering->rx_mini_max_pending = 0;
+ 	ering->rx_jumbo_max_pending = 0;
+ 	ering->rx_pending = priv->rx_ring_size;
+ 	ering->tx_pending = priv->tx_ring_size;
+ }
+ 
+ static int bcm_enetsw_set_ringparam(struct net_device *dev,
+ 				    struct ethtool_ringparam *ering)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	int was_running;
+ 
+ 	priv = netdev_priv(dev);
+ 
+ 	was_running = 0;
+ 	if (netif_running(dev)) {
+ 		bcm_enetsw_stop(dev);
+ 		was_running = 1;
+ 	}
+ 
+ 	priv->rx_ring_size = ering->rx_pending;
+ 	priv->tx_ring_size = ering->tx_pending;
+ 
+ 	if (was_running) {
+ 		int err;
+ 
+ 		err = bcm_enetsw_open(dev);
+ 		if (err)
+ 			dev_close(dev);
+ 	}
+ 	return 0;
+ }
+ 
+ static struct ethtool_ops bcm_enetsw_ethtool_ops = {
+ 	.get_strings		= bcm_enetsw_get_strings,
+ 	.get_sset_count		= bcm_enetsw_get_sset_count,
+ 	.get_ethtool_stats      = bcm_enetsw_get_ethtool_stats,
+ 	.get_drvinfo		= bcm_enetsw_get_drvinfo,
+ 	.get_ringparam		= bcm_enetsw_get_ringparam,
+ 	.set_ringparam		= bcm_enetsw_set_ringparam,
+ };
+ 
+ /* allocate netdevice, request register memory and register device. */
+ static int bcm_enetsw_probe(struct platform_device *pdev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct net_device *dev;
+ 	struct bcm63xx_enetsw_platform_data *pd;
+ 	struct resource *res_mem;
+ 	int ret, irq_rx, irq_tx;
+ 
+ 	/* stop if shared driver failed, assume driver->probe will be
+ 	 * called in the same order we register devices (correct ?)
+ 	 */
+ 	if (!bcm_enet_shared_base[0])
+ 		return -ENODEV;
+ 
+ 	res_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+ 	irq_rx = platform_get_irq(pdev, 0);
+ 	irq_tx = platform_get_irq(pdev, 1);
+ 	if (!res_mem || irq_rx < 0)
+ 		return -ENODEV;
+ 
+ 	ret = 0;
+ 	dev = alloc_etherdev(sizeof(*priv));
+ 	if (!dev)
+ 		return -ENOMEM;
+ 	priv = netdev_priv(dev);
+ 	memset(priv, 0, sizeof(*priv));
+ 
+ 	/* initialize default and fetch platform data */
+ 	priv->enet_is_sw = true;
+ 	priv->irq_rx = irq_rx;
+ 	priv->irq_tx = irq_tx;
+ 	priv->rx_ring_size = BCMENET_DEF_RX_DESC;
+ 	priv->tx_ring_size = BCMENET_DEF_TX_DESC;
+ 	priv->dma_maxburst = BCMENETSW_DMA_MAXBURST;
+ 
+ 	pd = dev_get_platdata(&pdev->dev);
+ 	if (pd) {
+ 		memcpy(dev->dev_addr, pd->mac_addr, ETH_ALEN);
+ 		memcpy(priv->used_ports, pd->used_ports,
+ 		       sizeof(pd->used_ports));
+ 		priv->num_ports = pd->num_ports;
+ 		priv->dma_has_sram = pd->dma_has_sram;
+ 		priv->dma_chan_en_mask = pd->dma_chan_en_mask;
+ 		priv->dma_chan_int_mask = pd->dma_chan_int_mask;
+ 		priv->dma_chan_width = pd->dma_chan_width;
+ 	}
+ 
+ 	ret = bcm_enet_change_mtu(dev, dev->mtu);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (!request_mem_region(res_mem->start, resource_size(res_mem),
+ 				"bcm63xx_enetsw")) {
+ 		ret = -EBUSY;
+ 		goto out;
+ 	}
+ 
+ 	priv->base = ioremap(res_mem->start, resource_size(res_mem));
+ 	if (priv->base == NULL) {
+ 		ret = -ENOMEM;
+ 		goto out_release_mem;
+ 	}
+ 
+ 	priv->mac_clk = clk_get(&pdev->dev, "enetsw");
+ 	if (IS_ERR(priv->mac_clk)) {
+ 		ret = PTR_ERR(priv->mac_clk);
+ 		goto out_unmap;
+ 	}
+ 	clk_enable(priv->mac_clk);
+ 
+ 	priv->rx_chan = 0;
+ 	priv->tx_chan = 1;
+ 	spin_lock_init(&priv->rx_lock);
+ 
+ 	/* init rx timeout (used for oom) */
+ 	init_timer(&priv->rx_timeout);
+ 	priv->rx_timeout.function = bcm_enet_refill_rx_timer;
+ 	priv->rx_timeout.data = (unsigned long)dev;
+ 
+ 	/* register netdevice */
+ 	dev->netdev_ops = &bcm_enetsw_ops;
+ 	netif_napi_add(dev, &priv->napi, bcm_enet_poll, 16);
+ 	dev->ethtool_ops = &bcm_enetsw_ethtool_ops;
+ 	SET_NETDEV_DEV(dev, &pdev->dev);
+ 
+ 	spin_lock_init(&priv->enetsw_mdio_lock);
+ 
+ 	ret = register_netdev(dev);
+ 	if (ret)
+ 		goto out_put_clk;
+ 
+ 	netif_carrier_off(dev);
+ 	platform_set_drvdata(pdev, dev);
+ 	priv->pdev = pdev;
+ 	priv->net_dev = dev;
+ 
+ 	return 0;
+ 
+ out_put_clk:
+ 	clk_put(priv->mac_clk);
+ 
+ out_unmap:
+ 	iounmap(priv->base);
+ 
+ out_release_mem:
+ 	release_mem_region(res_mem->start, resource_size(res_mem));
+ out:
+ 	free_netdev(dev);
+ 	return ret;
+ }
+ 
+ 
+ /* exit func, stops hardware and unregisters netdevice */
+ static int bcm_enetsw_remove(struct platform_device *pdev)
+ {
+ 	struct bcm_enet_priv *priv;
+ 	struct net_device *dev;
+ 	struct resource *res;
+ 
+ 	/* stop netdevice */
+ 	dev = platform_get_drvdata(pdev);
+ 	priv = netdev_priv(dev);
+ 	unregister_netdev(dev);
+ 
+ 	/* release device resources */
+ 	iounmap(priv->base);
+ 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+ 	release_mem_region(res->start, resource_size(res));
+ 
+ 	free_netdev(dev);
+ 	return 0;
+ }
+ 
+ struct platform_driver bcm63xx_enetsw_driver = {
+ 	.probe	= bcm_enetsw_probe,
+ 	.remove	= bcm_enetsw_remove,
+ 	.driver	= {
+ 		.name	= "bcm63xx_enetsw",
+ 		.owner  = THIS_MODULE,
+ 	},
+ };
+ 
+ /* reserve & remap memory space shared between all macs */
++>>>>>>> e1c6dccaf3af (ethernet/broadcom: use core min/max MTU checking)
  static int bcm_enet_shared_probe(struct platform_device *pdev)
  {
  	struct resource *res;
diff --cc drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
index 4810103f310d,ed42c1009685..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
@@@ -4862,12 -4855,6 +4862,15 @@@ int bnx2x_change_mtu(struct net_device 
  		return -EAGAIN;
  	}
  
++<<<<<<< HEAD
 +	if ((new_mtu > ETH_MAX_JUMBO_PACKET_SIZE) ||
 +	    (new_mtu < ETH_MIN_PACKET_SIZE)) {
 +		BNX2X_ERR("Can't support requested MTU size\n");
 +		return -EINVAL;
 +	}
 +
++=======
++>>>>>>> e1c6dccaf3af (ethernet/broadcom: use core min/max MTU checking)
  	/* This does not race with packet allocation
  	 * because the actual alloc size is
  	 * only updated as part of load
diff --cc drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
index b177ab176bdf,67b6180bdbf6..000000000000
--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
@@@ -13316,11 -13314,9 +13316,17 @@@ static int bnx2x_init_dev(struct bnx2x 
  	dev->dcbnl_ops = &bnx2x_dcbnl_ops;
  #endif
  
++<<<<<<< HEAD
 +#if 0  /* Not in RHEL 7.4. mtu range check remains in bnx2x_change_mtu(). */
 +	/* MTU range, 46 - 9600 */
 +	dev->min_mtu = ETH_MIN_PACKET_SIZE;
 +	dev->max_mtu = ETH_MAX_JUMBO_PACKET_SIZE;
 +#endif
++=======
+ 	/* MTU range, 46 - 9600 */
+ 	dev->min_mtu = ETH_MIN_PACKET_SIZE;
+ 	dev->max_mtu = ETH_MAX_JUMBO_PACKET_SIZE;
++>>>>>>> e1c6dccaf3af (ethernet/broadcom: use core min/max MTU checking)
  
  	/* get_port_hwinfo() will set prtad and mmds properly */
  	bp->mdio.prtad = MDIO_PRTAD_NONE;
diff --git a/drivers/net/ethernet/broadcom/b44.c b/drivers/net/ethernet/broadcom/b44.c
index c1327990fdf7..f35f3a63ff4a 100644
--- a/drivers/net/ethernet/broadcom/b44.c
+++ b/drivers/net/ethernet/broadcom/b44.c
@@ -57,8 +57,8 @@
 #define B44_TX_TIMEOUT			(5 * HZ)
 
 /* hardware minimum and maximum for a single frame's data payload */
-#define B44_MIN_MTU			60
-#define B44_MAX_MTU			1500
+#define B44_MIN_MTU			ETH_ZLEN
+#define B44_MAX_MTU			ETH_DATA_LEN
 
 #define B44_RX_RING_SIZE		512
 #define B44_DEF_RX_RING_PENDING		200
@@ -1035,9 +1035,6 @@ static int b44_change_mtu(struct net_device *dev, int new_mtu)
 {
 	struct b44 *bp = netdev_priv(dev);
 
-	if (new_mtu < B44_MIN_MTU || new_mtu > B44_MAX_MTU)
-		return -EINVAL;
-
 	if (!netif_running(dev)) {
 		/* We'll just catch it later when the
 		 * device is up'd.
@@ -2173,6 +2170,8 @@ static int b44_init_one(struct ssb_device *sdev,
 	dev->netdev_ops = &b44_netdev_ops;
 	netif_napi_add(dev, &bp->napi, b44_poll, 64);
 	dev->watchdog_timeo = B44_TX_TIMEOUT;
+	dev->min_mtu = B44_MIN_MTU;
+	dev->max_mtu = B44_MAX_MTU;
 	dev->irq = sdev->irq;
 	SET_ETHTOOL_OPS(dev, &b44_ethtool_ops);
 
* Unmerged path drivers/net/ethernet/broadcom/bcm63xx_enet.c
diff --git a/drivers/net/ethernet/broadcom/bnx2.c b/drivers/net/ethernet/broadcom/bnx2.c
index 4a33f8b93cb7..d5d1026be4b7 100644
--- a/drivers/net/ethernet/broadcom/bnx2.c
+++ b/drivers/net/ethernet/broadcom/bnx2.c
@@ -2301,7 +2301,7 @@ bnx2_init_5706s_phy(struct bnx2 *bp, int reset_phy)
 	if (BNX2_CHIP(bp) == BNX2_CHIP_5706)
 		BNX2_WR(bp, BNX2_MISC_GP_HW_CTL0, 0x300);
 
-	if (bp->dev->mtu > 1500) {
+	if (bp->dev->mtu > ETH_DATA_LEN) {
 		u32 val;
 
 		/* Set extended packet length bit */
@@ -2355,7 +2355,7 @@ bnx2_init_copper_phy(struct bnx2 *bp, int reset_phy)
 		bnx2_write_phy(bp, MII_BNX2_DSP_RW_PORT, val);
 	}
 
-	if (bp->dev->mtu > 1500) {
+	if (bp->dev->mtu > ETH_DATA_LEN) {
 		/* Set extended packet length bit */
 		bnx2_write_phy(bp, 0x18, 0x7);
 		bnx2_read_phy(bp, 0x18, &val);
@@ -5000,12 +5000,12 @@ bnx2_init_chip(struct bnx2 *bp)
 	/* Program the MTU.  Also include 4 bytes for CRC32. */
 	mtu = bp->dev->mtu;
 	val = mtu + ETH_HLEN + ETH_FCS_LEN;
-	if (val > (MAX_ETHERNET_PACKET_SIZE + 4))
+	if (val > (MAX_ETHERNET_PACKET_SIZE + ETH_HLEN + 4))
 		val |= BNX2_EMAC_RX_MTU_SIZE_JUMBO_ENA;
 	BNX2_WR(bp, BNX2_EMAC_RX_MTU_SIZE, val);
 
-	if (mtu < 1500)
-		mtu = 1500;
+	if (mtu < ETH_DATA_LEN)
+		mtu = ETH_DATA_LEN;
 
 	bnx2_reg_wr_ind(bp, BNX2_RBUF_CONFIG, BNX2_RBUF_CONFIG_VAL(mtu));
 	bnx2_reg_wr_ind(bp, BNX2_RBUF_CONFIG2, BNX2_RBUF_CONFIG2_VAL(mtu));
@@ -7924,10 +7924,6 @@ bnx2_change_mtu(struct net_device *dev, int new_mtu)
 {
 	struct bnx2 *bp = netdev_priv(dev);
 
-	if (((new_mtu + ETH_HLEN) > MAX_ETHERNET_JUMBO_PACKET_SIZE) ||
-		((new_mtu + ETH_HLEN) < MIN_ETHERNET_PACKET_SIZE))
-		return -EINVAL;
-
 	dev->mtu = new_mtu;
 	return bnx2_change_ring_size(bp, bp->rx_ring_size, bp->tx_ring_size,
 				     false);
@@ -8620,6 +8616,8 @@ bnx2_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
 	dev->hw_features |= NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;
 	dev->features |= dev->hw_features;
 	dev->priv_flags |= IFF_UNICAST_FLT;
+	dev->min_mtu = MIN_ETHERNET_PACKET_SIZE;
+	dev->max_mtu = MAX_ETHERNET_JUMBO_PACKET_SIZE;
 
 	if (!(bp->flags & BNX2_FLAG_CAN_KEEP_VLAN))
 		dev->hw_features &= ~NETIF_F_HW_VLAN_CTAG_RX;
diff --git a/drivers/net/ethernet/broadcom/bnx2.h b/drivers/net/ethernet/broadcom/bnx2.h
index 380234d72b95..a09ec47461c9 100644
--- a/drivers/net/ethernet/broadcom/bnx2.h
+++ b/drivers/net/ethernet/broadcom/bnx2.h
@@ -6530,9 +6530,9 @@ struct l2_fhdr {
 #define MII_BNX2_AER_AER_AN_MMD			   0x3800
 #define MII_BNX2_BLK_ADDR_COMBO_IEEEB0		 0xffe0
 
-#define MIN_ETHERNET_PACKET_SIZE	60
-#define MAX_ETHERNET_PACKET_SIZE	1514
-#define MAX_ETHERNET_JUMBO_PACKET_SIZE	9014
+#define MIN_ETHERNET_PACKET_SIZE	(ETH_ZLEN - ETH_HLEN)
+#define MAX_ETHERNET_PACKET_SIZE	ETH_DATA_LEN
+#define MAX_ETHERNET_JUMBO_PACKET_SIZE	9000
 
 #define BNX2_RX_COPY_THRESH		128
 
* Unmerged path drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c
* Unmerged path drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c
diff --git a/drivers/net/ethernet/broadcom/bnxt/bnxt.c b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
index bdacd982a1af..e7ab1532d5d1 100644
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt.c
@@ -6648,9 +6648,6 @@ static int bnxt_change_mtu(struct net_device *dev, int new_mtu)
 {
 	struct bnxt *bp = netdev_priv(dev);
 
-	if (new_mtu < 60 || new_mtu > 9500)
-		return -EINVAL;
-
 	if (netif_running(dev))
 		bnxt_close_nic(bp, false, false);
 
@@ -7292,6 +7289,10 @@ static int bnxt_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
 	dev->features |= dev->hw_features | NETIF_F_HIGHDMA;
 	dev->priv_flags |= IFF_UNICAST_FLT;
 
+	/* MTU range: 60 - 9500 */
+	dev->min_mtu = ETH_ZLEN;
+	dev->max_mtu = 9500;
+
 #ifdef CONFIG_BNXT_SRIOV
 	init_waitqueue_head(&bp->sriov_cfg_wait);
 #endif
diff --git a/drivers/net/ethernet/broadcom/tg3.c b/drivers/net/ethernet/broadcom/tg3.c
index 5e522998a58a..38a74e50892d 100644
--- a/drivers/net/ethernet/broadcom/tg3.c
+++ b/drivers/net/ethernet/broadcom/tg3.c
@@ -124,7 +124,7 @@ static inline void _tg3_flag_clear(enum TG3_FLAGS flag, unsigned long *bits)
 #define TG3_TX_TIMEOUT			(5 * HZ)
 
 /* hardware minimum and maximum for a single frame's data payload */
-#define TG3_MIN_MTU			60
+#define TG3_MIN_MTU			ETH_ZLEN
 #define TG3_MAX_MTU(tp)	\
 	(tg3_flag(tp, JUMBO_CAPABLE) ? 9000 : 1500)
 
@@ -14187,9 +14187,6 @@ static int tg3_change_mtu(struct net_device *dev, int new_mtu)
 	int err;
 	bool reset_phy = false;
 
-	if (new_mtu < TG3_MIN_MTU || new_mtu > TG3_MAX_MTU(tp))
-		return -EINVAL;
-
 	if (!netif_running(dev)) {
 		/* We'll just catch it later when the
 		 * device is up'd.
@@ -17787,6 +17784,10 @@ static int tg3_init_one(struct pci_dev *pdev,
 	dev->hw_features |= features;
 	dev->priv_flags |= IFF_UNICAST_FLT;
 
+	/* MTU range: 60 - 9000 or 1500, depending on hardware */
+	dev->min_mtu = TG3_MIN_MTU;
+	dev->max_mtu = TG3_MAX_MTU(tp);
+
 	if (tg3_chip_rev_id(tp) == CHIPREV_ID_5705_A1 &&
 	    !tg3_flag(tp, TSO_CAPABLE) &&
 	    !(tr32(TG3PCI_PCISTATE) & PCISTATE_BUS_SPEED_HIGH)) {
