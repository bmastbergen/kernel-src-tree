blk-mq: use true instead of 1 for blk_mq_queue_data.last

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Omar Sandoval <osandov@fb.com>
commit d945a365a0686a37618503622954f8dc169b8bca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d945a365.failed

Trivial cleanup.

	Signed-off-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit d945a365a0686a37618503622954f8dc169b8bca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 1b06c94aa73d,9bdfeed59d9d..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1304,64 -1448,33 +1304,68 @@@ insert_rq
  	}
  }
  
 -static blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx, struct request *rq)
 -{
 -	if (rq->tag != -1)
 -		return blk_tag_to_qc_t(rq->tag, hctx->queue_num, false);
 -
 -	return blk_tag_to_qc_t(rq->internal_tag, hctx->queue_num, true);
 -}
 +struct blk_map_ctx {
 +	struct blk_mq_hw_ctx *hctx;
 +	struct blk_mq_ctx *ctx;
 +};
  
 -static void __blk_mq_try_issue_directly(struct request *rq, blk_qc_t *cookie,
 -				      bool may_sleep)
 +static struct request *blk_mq_map_request(struct request_queue *q,
 +					  struct bio *bio,
 +					  struct blk_map_ctx *data)
  {
 -	struct request_queue *q = rq->q;
 -	struct blk_mq_queue_data bd = {
 -		.rq = rq,
 -		.last = true,
 -	};
  	struct blk_mq_hw_ctx *hctx;
 -	blk_qc_t new_cookie;
 -	int ret;
 +	struct blk_mq_ctx *ctx;
 +	struct request *rq;
 +	int rw = bio_data_dir(bio);
 +	struct blk_mq_alloc_data alloc_data;
  
 -	if (q->elevator)
 -		goto insert;
 +	blk_queue_enter_live(q);
 +	ctx = blk_mq_get_ctx(q);
 +	hctx = q->mq_ops->map_queue(q, ctx->cpu);
  
 -	if (!blk_mq_get_driver_tag(rq, &hctx, false))
 -		goto insert;
 +	if (rw_is_sync(bio->bi_rw))
 +		rw |= REQ_SYNC;
 +
 +	trace_block_getrq(q, bio, rw);
 +	blk_mq_set_alloc_data(&alloc_data, q, BLK_MQ_REQ_NOWAIT, ctx, hctx);
 +	rq = __blk_mq_alloc_request(&alloc_data, rw);
 +	if (unlikely(!rq)) {
 +		__blk_mq_run_hw_queue(hctx);
 +		blk_mq_put_ctx(ctx);
 +		trace_block_sleeprq(q, bio, rw);
 +
 +		ctx = blk_mq_get_ctx(q);
 +		hctx = q->mq_ops->map_queue(q, ctx->cpu);
 +		blk_mq_set_alloc_data(&alloc_data, q, 0, ctx, hctx);
 +		rq = __blk_mq_alloc_request(&alloc_data, rw);
 +		ctx = alloc_data.ctx;
 +		hctx = alloc_data.hctx;
 +	}
  
 -	new_cookie = request_to_qc_t(hctx, rq);
 +	hctx->queued++;
 +	data->hctx = hctx;
 +	data->ctx = ctx;
 +	return rq;
 +}
 +
 +static void blk_mq_try_issue_directly(struct request *rq)
 +{
 +	int ret;
 +	struct request_queue *q = rq->q;
 +	struct blk_mq_hw_ctx *hctx = q->mq_ops->map_queue(q,
 +			rq->mq_ctx->cpu);
 +	struct blk_mq_queue_data bd = {
 +		.rq = rq,
++<<<<<<< HEAD
 +		.list = NULL,
 +		.last = 1
++=======
++		.last = true,
++>>>>>>> d945a365a068 (blk-mq: use true instead of 1 for blk_mq_queue_data.last)
 +	};
 +
 +	if (blk_mq_hctx_stopped(hctx))
 +		goto insert;
  
  	/*
  	 * For OK queue, we are done. For error, kill it. Any other
* Unmerged path block/blk-mq.c
