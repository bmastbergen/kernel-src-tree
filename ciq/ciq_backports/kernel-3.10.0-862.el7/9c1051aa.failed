blk-mq: untangle debugfs and sysfs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Omar Sandoval <osandov@fb.com>
commit 9c1051aacde828073dbbab5e8e59c0fc802efa9a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/9c1051aa.failed

Originally, I tied debugfs registration/unregistration together with
sysfs. There's no reason to do this, and it's getting in the way of
letting schedulers define their own debugfs attributes. Instead, tie the
debugfs registration to the lifetime of the structures themselves.

The saner lifetimes mean we can also get rid of the extra mq directory
and move everything one level up. I.e., nvme0n1/mq/hctx0/tags is now
just nvme0n1/hctx0/tags.

	Signed-off-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 9c1051aacde828073dbbab5e8e59c0fc802efa9a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	block/blk-mq-debugfs.c
#	block/blk-mq-debugfs.h
#	block/blk-mq-sysfs.c
#	block/blk-mq.c
#	block/blk-sysfs.c
#	include/linux/blk-mq.h
#	include/linux/blkdev.h
diff --cc block/blk-core.c
index b368719f5971,c580b0138a7f..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -37,8 -39,13 +37,17 @@@
  #include <trace/events/block.h>
  
  #include "blk.h"
 +#include "blk-cgroup.h"
  #include "blk-mq.h"
++<<<<<<< HEAD
++=======
+ #include "blk-mq-sched.h"
+ #include "blk-wbt.h"
+ 
+ #ifdef CONFIG_DEBUG_FS
+ struct dentry *blk_debugfs_root;
+ #endif
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  
  EXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_remap);
  EXPORT_TRACEPOINT_SYMBOL_GPL(block_rq_remap);
diff --cc block/blk-mq-sysfs.c
index 44799e473f05,79969c3c234f..000000000000
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@@ -375,22 -251,15 +375,25 @@@ static int blk_mq_register_hctx(struct 
  static void __blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
  {
  	struct blk_mq_hw_ctx *hctx;
 -	int i;
 -
 -	lockdep_assert_held(&q->sysfs_lock);
 +	struct blk_mq_ctx *ctx;
 +	int i, j;
  
 -	queue_for_each_hw_ctx(q, hctx, i)
 +	queue_for_each_hw_ctx(q, hctx, i) {
  		blk_mq_unregister_hctx(hctx);
  
++<<<<<<< HEAD
 +		hctx_for_each_ctx(hctx, ctx, j)
 +			kobject_put(&ctx->kobj);
 +
 +		kobject_put(&hctx->kobj);
 +	}
 +
++=======
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  	kobject_uevent(&q->mq_kobj, KOBJ_REMOVE);
  	kobject_del(&q->mq_kobj);
 +	kobject_put(&q->mq_kobj);
 +
  	kobject_put(&dev->kobj);
  
  	q->mq_sysfs_init_done = false;
@@@ -439,15 -319,31 +442,37 @@@ int blk_mq_register_dev(struct device *
  	queue_for_each_hw_ctx(q, hctx, i) {
  		ret = blk_mq_register_hctx(hctx);
  		if (ret)
 -			goto unreg;
 +			break;
  	}
  
 -	q->mq_sysfs_init_done = true;
 -
 +	if (ret)
 +		__blk_mq_unregister_dev(dev, q);
 +	else
 +		q->mq_sysfs_init_done = true;
  out:
++<<<<<<< HEAD
 +	blk_mq_enable_hotplug();
++=======
+ 	return ret;
+ 
+ unreg:
+ 	while (--i >= 0)
+ 		blk_mq_unregister_hctx(q->queue_hw_ctx[i]);
+ 
+ 	kobject_uevent(&q->mq_kobj, KOBJ_REMOVE);
+ 	kobject_del(&q->mq_kobj);
+ 	kobject_put(&dev->kobj);
+ 	return ret;
+ }
+ 
+ int blk_mq_register_dev(struct device *dev, struct request_queue *q)
+ {
+ 	int ret;
+ 
+ 	mutex_lock(&q->sysfs_lock);
+ 	ret = __blk_mq_register_dev(dev, q);
+ 	mutex_unlock(&q->sysfs_lock);
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  
  	return ret;
  }
@@@ -458,8 -354,9 +483,12 @@@ void blk_mq_sysfs_unregister(struct req
  	struct blk_mq_hw_ctx *hctx;
  	int i;
  
 -	mutex_lock(&q->sysfs_lock);
  	if (!q->mq_sysfs_init_done)
++<<<<<<< HEAD
 +		return;
++=======
+ 		goto unlock;
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  
  	queue_for_each_hw_ctx(q, hctx, i)
  		blk_mq_unregister_hctx(hctx);
@@@ -470,8 -370,9 +499,12 @@@ int blk_mq_sysfs_register(struct reques
  	struct blk_mq_hw_ctx *hctx;
  	int i, ret = 0;
  
 -	mutex_lock(&q->sysfs_lock);
  	if (!q->mq_sysfs_init_done)
++<<<<<<< HEAD
 +		return ret;
++=======
+ 		goto unlock;
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  
  	queue_for_each_hw_ctx(q, hctx, i) {
  		ret = blk_mq_register_hctx(hctx);
diff --cc block/blk-mq.c
index d9bfe0c6bc0e,5d4ce7eb8dbf..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -28,7 -31,11 +28,8 @@@
  #include <linux/blk-mq.h>
  #include "blk.h"
  #include "blk-mq.h"
+ #include "blk-mq-debugfs.h"
  #include "blk-mq-tag.h"
 -#include "blk-stat.h"
 -#include "blk-wbt.h"
 -#include "blk-mq-sched.h"
  
  static DEFINE_MUTEX(all_q_mutex);
  static LIST_HEAD(all_q_list);
@@@ -1774,7 -1863,7 +1775,11 @@@ static void blk_mq_exit_hctx(struct req
  		struct blk_mq_tag_set *set,
  		struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
  {
++<<<<<<< HEAD
 +	unsigned flush_start_tag = set->queue_depth;
++=======
+ 	blk_mq_debugfs_unregister_hctx(hctx);
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  
  	blk_mq_tag_idle(hctx);
  
@@@ -2281,52 -2402,21 +2289,53 @@@ static void blk_mq_queue_reinit(struct 
  	blk_mq_map_swqueue(q, online_mask);
  
  	blk_mq_sysfs_register(q);
+ 	blk_mq_debugfs_register_hctxs(q);
  }
  
 -/*
 - * New online cpumask which is going to be set in this hotplug event.
 - * Declare this cpumasks as global as cpu-hotplug operation is invoked
 - * one-by-one and dynamically allocating this could result in a failure.
 - */
 -static struct cpumask cpuhp_online_new;
 -
 -static void blk_mq_queue_reinit_work(void)
 +static int blk_mq_queue_reinit_notify(struct notifier_block *nb,
 +				      unsigned long action, void *hcpu)
  {
  	struct request_queue *q;
 +	int cpu = (unsigned long)hcpu;
 +	/*
 +	 * New online cpumask which is going to be set in this hotplug event.
 +	 * Declare this cpumasks as global as cpu-hotplug operation is invoked
 +	 * one-by-one and dynamically allocating this could result in a failure.
 +	 */
 +	static struct cpumask online_new;
 +
 +	/*
 +	 * Before hotadded cpu starts handling requests, new mappings must
 +	 * be established.  Otherwise, these requests in hw queue might
 +	 * never be dispatched.
 +	 *
 +	 * For example, there is a single hw queue (hctx) and two CPU queues
 +	 * (ctx0 for CPU0, and ctx1 for CPU1).
 +	 *
 +	 * Now CPU1 is just onlined and a request is inserted into
 +	 * ctx1->rq_list and set bit0 in pending bitmap as ctx1->index_hw is
 +	 * still zero.
 +	 *
 +	 * And then while running hw queue, flush_busy_ctxs() finds bit0 is
 +	 * set in pending bitmap and tries to retrieve requests in
 +	 * hctx->ctxs[0]->rq_list.  But htx->ctxs[0] is a pointer to ctx0,
 +	 * so the request in ctx1->rq_list is ignored.
 +	 */
 +	switch (action & ~CPU_TASKS_FROZEN) {
 +	case CPU_DEAD:
 +	case CPU_UP_CANCELED:
 +		cpumask_copy(&online_new, cpu_online_mask);
 +		break;
 +	case CPU_UP_PREPARE:
 +		cpumask_copy(&online_new, cpu_online_mask);
 +		cpumask_set_cpu(cpu, &online_new);
 +		break;
 +	default:
 +		return NOTIFY_OK;
 +	}
  
  	mutex_lock(&all_q_mutex);
 +
  	/*
  	 * We need to freeze and reinit all existing queues.  Freezing
  	 * involves synchronous wait for an RCU grace period and doing it
diff --cc block/blk-sysfs.c
index 28651fcb101c,504fee940052..000000000000
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@@ -613,13 -881,22 +613,21 @@@ int blk_register_queue(struct gendisk *
  	ret = kobject_add(&q->kobj, kobject_get(&dev->kobj), "%s", "queue");
  	if (ret < 0) {
  		blk_trace_remove_sysfs(dev);
 -		goto unlock;
 +		return ret;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (q->mq_ops)
+ 		__blk_mq_register_dev(dev, q);
+ 
+ 	blk_mq_debugfs_register(q);
+ 
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  	kobject_uevent(&q->kobj, KOBJ_ADD);
  
 -	wbt_enable_default(q);
 -
 -	blk_throtl_register_queue(q);
 +	if (q->mq_ops)
 +		blk_mq_register_dev(dev, q);
  
  	if (q->request_fn || (q->mq_ops && q->elevator)) {
  		ret = elv_register_queue(q);
diff --cc include/linux/blk-mq.h
index ab31251b7413,de8ed9aaa156..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -57,43 -43,29 +57,53 @@@ struct blk_mq_hw_ctx 
  
  	unsigned long		queued;
  	unsigned long		run;
 -#define BLK_MQ_MAX_DISPATCH_ORDER	7
 +#define BLK_MQ_MAX_DISPATCH_ORDER	10
  	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
  
 +	unsigned int		queue_depth;	/* DEPRECATED: RHEL kABI padding, repurpose? */
  	unsigned int		numa_node;
 -	unsigned int		queue_num;
 -
 -	atomic_t		nr_active;
 +	RH_KABI_DEPRECATE(unsigned int, cmd_size)
  
 -	struct hlist_node	cpuhp_dead;
 +	struct blk_mq_cpu_notifier	cpu_notifier;
  	struct kobject		kobj;
  
++<<<<<<< HEAD
 +	RH_KABI_EXTEND(struct delayed_work	run_work)
 +	RH_KABI_EXTEND(cpumask_var_t		cpumask)
 +	RH_KABI_EXTEND(int			next_cpu)
 +	RH_KABI_EXTEND(int			next_cpu_batch)
 +
 +	RH_KABI_EXTEND(struct blk_mq_ctxmap	ctx_map)
 +
 +	RH_KABI_EXTEND(atomic_t		nr_active)
 +
 +	RH_KABI_EXTEND(struct blk_flush_queue	*fq)
 +	RH_KABI_EXTEND(struct srcu_struct	queue_rq_srcu)
++=======
+ 	unsigned long		poll_considered;
+ 	unsigned long		poll_invoked;
+ 	unsigned long		poll_success;
+ 
+ #ifdef CONFIG_BLK_DEBUG_FS
+ 	struct dentry		*debugfs_dir;
+ #endif
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
  };
  
 +#ifdef __GENKSYMS__
 +struct blk_mq_reg {
 +	struct blk_mq_ops	*ops;
 +	unsigned int		nr_hw_queues;
 +	unsigned int		queue_depth;	/* max hw supported */
 +	unsigned int		reserved_tags;
 +	unsigned int		cmd_size;	/* per-request extra data */
 +	int			numa_node;
 +	unsigned int		timeout;
 +	unsigned int		flags;		/* BLK_MQ_F_* */
 +};
 +#else
  struct blk_mq_tag_set {
 -	unsigned int		*mq_map;
 -	const struct blk_mq_ops	*ops;
 +	struct blk_mq_ops	*ops;
  	unsigned int		nr_hw_queues;
  	unsigned int		queue_depth;	/* max hw supported */
  	unsigned int		reserved_tags;
diff --cc include/linux/blkdev.h
index ba3405333171,b49a79a29e58..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -526,24 -570,21 +526,30 @@@ struct request_queue 
  #endif
  	struct rcu_head		rcu_head;
  	wait_queue_head_t	mq_freeze_wq;
 -	struct percpu_ref	q_usage_counter;
 +	RH_KABI_DEPRECATE(struct percpu_counter, mq_usage_counter)
  	struct list_head	all_q_node;
  
 -	struct blk_mq_tag_set	*tag_set;
 -	struct list_head	tag_set_list;
 -	struct bio_set		*bio_split;
 +	RH_KABI_EXTEND(unprep_rq_fn		*unprep_rq_fn)
  
++<<<<<<< HEAD
 +	RH_KABI_EXTEND(struct blk_mq_tag_set	*tag_set)
 +	RH_KABI_EXTEND(struct list_head		tag_set_list)
++=======
+ #ifdef CONFIG_BLK_DEBUG_FS
+ 	struct dentry		*debugfs_dir;
+ #endif
 -
 -	bool			mq_sysfs_init_done;
 -
 -	size_t			cmd_size;
 -	void			*rq_alloc_data;
++>>>>>>> 9c1051aacde8 (blk-mq: untangle debugfs and sysfs)
 +
 +	RH_KABI_EXTEND(struct list_head		requeue_list)
 +	RH_KABI_EXTEND(spinlock_t			requeue_lock)
 +	/* requeue_work's type is changed from 'work_struct' to 'delayed_work' below */
 +	RH_KABI_EXTEND(struct work_struct	rh_reserved_requeue_work)
 +	RH_KABI_EXTEND(atomic_t				mq_freeze_depth)
 +	RH_KABI_EXTEND(struct blk_flush_queue   *fq)
 +	RH_KABI_EXTEND(struct percpu_ref	q_usage_counter)
 +	RH_KABI_EXTEND(bool			mq_sysfs_init_done)
 +	RH_KABI_EXTEND(struct work_struct	timeout_work)
 +	RH_KABI_EXTEND(struct delayed_work	requeue_work)
  };
  
  #define QUEUE_FLAG_QUEUED	1	/* uses generic tag queueing */
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path block/blk-mq-debugfs.h
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq-debugfs.c
* Unmerged path block/blk-mq-debugfs.h
* Unmerged path block/blk-mq-sysfs.c
* Unmerged path block/blk-mq.c
* Unmerged path block/blk-sysfs.c
* Unmerged path include/linux/blk-mq.h
* Unmerged path include/linux/blkdev.h
