cpufreq: intel_pstate: Do not walk policy->cpus

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Do not walk policy->cpus (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 89.41%
commit-author Rafael J. Wysocki <rafael.j.wysocki@intel.com>
commit 2bfc4cbb5fd3848669f1b95fea793f63d8e77fa0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/2bfc4cbb.failed

intel_pstate_hwp_set() is the only function walking policy->cpus
in intel_pstate.  The rest of the code simply assumes one CPU per
policy, including the initialization code.

Therefore it doesn't make sense for intel_pstate_hwp_set() to
walk policy->cpus as it is guaranteed to have only one bit set
for policy->cpu.

For this reason, rearrange intel_pstate_hwp_set() to take the CPU
number as the argument and drop the loop over policy->cpus from it.

	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 2bfc4cbb5fd3848669f1b95fea793f63d8e77fa0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index b6f8db18a31a,5236701958d0..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -541,40 -567,340 +541,374 @@@ static inline void update_turbo_state(v
  		 cpu->pstate.max_pstate == cpu->pstate.turbo_pstate);
  }
  
 -static int min_perf_pct_min(void)
 +static void intel_pstate_hwp_set(const struct cpumask *cpumask)
  {
++<<<<<<< HEAD
 +	int min, hw_min, max, hw_max, cpu, range, adj_range;
++=======
+ 	struct cpudata *cpu = all_cpu_data[0];
+ 
+ 	return DIV_ROUND_UP(cpu->pstate.min_pstate * 100,
+ 			    cpu->pstate.turbo_pstate);
+ }
+ 
+ static s16 intel_pstate_get_epb(struct cpudata *cpu_data)
+ {
+ 	u64 epb;
+ 	int ret;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_EPB))
+ 		return -ENXIO;
+ 
+ 	ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
+ 	if (ret)
+ 		return (s16)ret;
+ 
+ 	return (s16)(epb & 0x0f);
+ }
+ 
+ static s16 intel_pstate_get_epp(struct cpudata *cpu_data, u64 hwp_req_data)
+ {
+ 	s16 epp;
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		/*
+ 		 * When hwp_req_data is 0, means that caller didn't read
+ 		 * MSR_HWP_REQUEST, so need to read and get EPP.
+ 		 */
+ 		if (!hwp_req_data) {
+ 			epp = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST,
+ 					    &hwp_req_data);
+ 			if (epp)
+ 				return epp;
+ 		}
+ 		epp = (hwp_req_data >> 24) & 0xff;
+ 	} else {
+ 		/* When there is no EPP present, HWP uses EPB settings */
+ 		epp = intel_pstate_get_epb(cpu_data);
+ 	}
+ 
+ 	return epp;
+ }
+ 
+ static int intel_pstate_set_epb(int cpu, s16 pref)
+ {
+ 	u64 epb;
+ 	int ret;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_EPB))
+ 		return -ENXIO;
+ 
+ 	ret = rdmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
+ 	if (ret)
+ 		return ret;
+ 
+ 	epb = (epb & ~0x0f) | pref;
+ 	wrmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, epb);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * EPP/EPB display strings corresponding to EPP index in the
+  * energy_perf_strings[]
+  *	index		String
+  *-------------------------------------
+  *	0		default
+  *	1		performance
+  *	2		balance_performance
+  *	3		balance_power
+  *	4		power
+  */
+ static const char * const energy_perf_strings[] = {
+ 	"default",
+ 	"performance",
+ 	"balance_performance",
+ 	"balance_power",
+ 	"power",
+ 	NULL
+ };
+ 
+ static int intel_pstate_get_energy_pref_index(struct cpudata *cpu_data)
+ {
+ 	s16 epp;
+ 	int index = -EINVAL;
+ 
+ 	epp = intel_pstate_get_epp(cpu_data, 0);
+ 	if (epp < 0)
+ 		return epp;
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		/*
+ 		 * Range:
+ 		 *	0x00-0x3F	:	Performance
+ 		 *	0x40-0x7F	:	Balance performance
+ 		 *	0x80-0xBF	:	Balance power
+ 		 *	0xC0-0xFF	:	Power
+ 		 * The EPP is a 8 bit value, but our ranges restrict the
+ 		 * value which can be set. Here only using top two bits
+ 		 * effectively.
+ 		 */
+ 		index = (epp >> 6) + 1;
+ 	} else if (static_cpu_has(X86_FEATURE_EPB)) {
+ 		/*
+ 		 * Range:
+ 		 *	0x00-0x03	:	Performance
+ 		 *	0x04-0x07	:	Balance performance
+ 		 *	0x08-0x0B	:	Balance power
+ 		 *	0x0C-0x0F	:	Power
+ 		 * The EPB is a 4 bit value, but our ranges restrict the
+ 		 * value which can be set. Here only using top two bits
+ 		 * effectively.
+ 		 */
+ 		index = (epp >> 2) + 1;
+ 	}
+ 
+ 	return index;
+ }
+ 
+ static int intel_pstate_set_energy_pref_index(struct cpudata *cpu_data,
+ 					      int pref_index)
+ {
+ 	int epp = -EINVAL;
+ 	int ret;
+ 
+ 	if (!pref_index)
+ 		epp = cpu_data->epp_default;
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		u64 value;
+ 
+ 		ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, &value);
+ 		if (ret)
+ 			goto return_pref;
+ 
+ 		value &= ~GENMASK_ULL(31, 24);
+ 
+ 		/*
+ 		 * If epp is not default, convert from index into
+ 		 * energy_perf_strings to epp value, by shifting 6
+ 		 * bits left to use only top two bits in epp.
+ 		 * The resultant epp need to shifted by 24 bits to
+ 		 * epp position in MSR_HWP_REQUEST.
+ 		 */
+ 		if (epp == -EINVAL)
+ 			epp = (pref_index - 1) << 6;
+ 
+ 		value |= (u64)epp << 24;
+ 		ret = wrmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, value);
+ 	} else {
+ 		if (epp == -EINVAL)
+ 			epp = (pref_index - 1) << 2;
+ 		ret = intel_pstate_set_epb(cpu_data->cpu, epp);
+ 	}
+ return_pref:
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t show_energy_performance_available_preferences(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	int i = 0;
+ 	int ret = 0;
+ 
+ 	while (energy_perf_strings[i] != NULL)
+ 		ret += sprintf(&buf[ret], "%s ", energy_perf_strings[i++]);
+ 
+ 	ret += sprintf(&buf[ret], "\n");
+ 
+ 	return ret;
+ }
+ 
+ cpufreq_freq_attr_ro(energy_performance_available_preferences);
+ 
+ static ssize_t store_energy_performance_preference(
+ 		struct cpufreq_policy *policy, const char *buf, size_t count)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 	char str_preference[21];
+ 	int ret, i = 0;
+ 
+ 	ret = sscanf(buf, "%20s", str_preference);
+ 	if (ret != 1)
+ 		return -EINVAL;
+ 
+ 	while (energy_perf_strings[i] != NULL) {
+ 		if (!strcmp(str_preference, energy_perf_strings[i])) {
+ 			intel_pstate_set_energy_pref_index(cpu_data, i);
+ 			return count;
+ 		}
+ 		++i;
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ static ssize_t show_energy_performance_preference(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 	int preference;
+ 
+ 	preference = intel_pstate_get_energy_pref_index(cpu_data);
+ 	if (preference < 0)
+ 		return preference;
+ 
+ 	return  sprintf(buf, "%s\n", energy_perf_strings[preference]);
+ }
+ 
+ cpufreq_freq_attr_rw(energy_performance_preference);
+ 
+ static struct freq_attr *hwp_cpufreq_attrs[] = {
+ 	&energy_performance_preference,
+ 	&energy_performance_available_preferences,
+ 	NULL,
+ };
+ 
+ static void intel_pstate_hwp_set(unsigned int cpu)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[cpu];
+ 	int min, hw_min, max, hw_max;
++>>>>>>> 2bfc4cbb5fd3 (cpufreq: intel_pstate: Do not walk policy->cpus)
  	u64 value, cap;
+ 	s16 epp;
  
++<<<<<<< HEAD
 +	for_each_cpu(cpu, cpumask) {
 +		rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
 +		hw_min = HWP_LOWEST_PERF(cap);
 +		if (limits->no_turbo)
 +			hw_max = HWP_GUARANTEED_PERF(cap);
 +		else
 +			hw_max = HWP_HIGHEST_PERF(cap);
 +		range = hw_max - hw_min;
 +
 +		rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
 +		adj_range = limits->min_perf_pct * range / 100;
 +		min = hw_min + adj_range;
 +		value &= ~HWP_MIN_PERF(~0L);
 +		value |= HWP_MIN_PERF(min);
 +
 +		adj_range = limits->max_perf_pct * range / 100;
 +		max = hw_min + adj_range;
 +
 +		value &= ~HWP_MAX_PERF(~0L);
 +		value |= HWP_MAX_PERF(max);
 +		wrmsrl_on_cpu(cpu, MSR_HWP_REQUEST, value);
++=======
+ 	rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
+ 	hw_min = HWP_LOWEST_PERF(cap);
+ 	if (global.no_turbo)
+ 		hw_max = HWP_GUARANTEED_PERF(cap);
+ 	else
+ 		hw_max = HWP_HIGHEST_PERF(cap);
+ 
+ 	max = fp_ext_toint(hw_max * cpu_data->max_perf);
+ 	if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE)
+ 		min = max;
+ 	else
+ 		min = fp_ext_toint(hw_max * cpu_data->min_perf);
+ 
+ 	rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
+ 
+ 	value &= ~HWP_MIN_PERF(~0L);
+ 	value |= HWP_MIN_PERF(min);
+ 
+ 	value &= ~HWP_MAX_PERF(~0L);
+ 	value |= HWP_MAX_PERF(max);
+ 
+ 	if (cpu_data->epp_policy == cpu_data->policy)
+ 		goto skip_epp;
+ 
+ 	cpu_data->epp_policy = cpu_data->policy;
+ 
+ 	if (cpu_data->epp_saved >= 0) {
+ 		epp = cpu_data->epp_saved;
+ 		cpu_data->epp_saved = -EINVAL;
+ 		goto update_epp;
  	}
+ 
+ 	if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE) {
+ 		epp = intel_pstate_get_epp(cpu_data, value);
+ 		cpu_data->epp_powersave = epp;
+ 		/* If EPP read was failed, then don't try to write */
+ 		if (epp < 0)
+ 			goto skip_epp;
+ 
+ 		epp = 0;
+ 	} else {
+ 		/* skip setting EPP, when saved value is invalid */
+ 		if (cpu_data->epp_powersave < 0)
+ 			goto skip_epp;
+ 
+ 		/*
+ 		 * No need to restore EPP when it is not zero. This
+ 		 * means:
+ 		 *  - Policy is not changed
+ 		 *  - user has manually changed
+ 		 *  - Error reading EPB
+ 		 */
+ 		epp = intel_pstate_get_epp(cpu_data, value);
+ 		if (epp)
+ 			goto skip_epp;
+ 
+ 		epp = cpu_data->epp_powersave;
++>>>>>>> 2bfc4cbb5fd3 (cpufreq: intel_pstate: Do not walk policy->cpus)
+ 	}
+ update_epp:
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		value &= ~GENMASK_ULL(31, 24);
+ 		value |= (u64)epp << 24;
+ 	} else {
+ 		intel_pstate_set_epb(cpu, epp);
+ 	}
+ skip_epp:
+ 	wrmsrl_on_cpu(cpu, MSR_HWP_REQUEST, value);
  }
  
 -static int intel_pstate_hwp_save_state(struct cpufreq_policy *policy)
 +static void intel_pstate_hwp_set_online_cpus(void)
  {
++<<<<<<< HEAD
 +	get_online_cpus();
 +	intel_pstate_hwp_set(cpu_online_mask);
 +	put_online_cpus();
++=======
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 
+ 	if (!hwp_active)
+ 		return 0;
+ 
+ 	cpu_data->epp_saved = intel_pstate_get_epp(cpu_data, 0);
+ 
+ 	return 0;
+ }
+ 
+ static int intel_pstate_resume(struct cpufreq_policy *policy)
+ {
+ 	if (!hwp_active)
+ 		return 0;
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 
+ 	all_cpu_data[policy->cpu]->epp_policy = 0;
+ 	intel_pstate_hwp_set(policy->cpu);
+ 
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	return 0;
+ }
+ 
+ static void intel_pstate_update_policies(void)
+ {
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu)
+ 		cpufreq_update_policy(cpu);
++>>>>>>> 2bfc4cbb5fd3 (cpufreq: intel_pstate: Do not walk policy->cpus)
  }
  
  /************************** debugfs begin ************************/
@@@ -1437,44 -2034,28 +1771,50 @@@ static int intel_pstate_set_policy(stru
  	pr_debug("set_policy cpuinfo.max %u policy->max %u\n",
  		 policy->cpuinfo.max_freq, policy->max);
  
 -	cpu = all_cpu_data[policy->cpu];
 -	cpu->policy = policy->policy;
 -
 -	mutex_lock(&intel_pstate_limits_lock);
 -
 -	intel_pstate_update_perf_limits(policy, cpu);
 -
 -	if (cpu->policy == CPUFREQ_POLICY_PERFORMANCE) {
 -		/*
 -		 * NOHZ_FULL CPUs need this as the governor callback may not
 -		 * be invoked on them.
 -		 */
 -		intel_pstate_clear_update_util_hook(policy->cpu);
 -		intel_pstate_max_within_limits(cpu);
 +	if (policy->policy == CPUFREQ_POLICY_PERFORMANCE &&
 +	    policy->max >= policy->cpuinfo.max_freq) {
 +		pr_debug("intel_pstate: set performance\n");
 +		limits = &performance_limits;
 +		if (hwp_active)
 +			intel_pstate_hwp_set(policy->cpus);
 +		return 0;
  	}
  
 -	intel_pstate_set_update_util_hook(policy->cpu);
 +	pr_debug("intel_pstate: set powersave\n");
 +	limits = &powersave_limits;
 +	limits->min_policy_pct = (policy->min * 100) / policy->cpuinfo.max_freq;
 +	limits->min_policy_pct = clamp_t(int, limits->min_policy_pct, 0 , 100);
 +	limits->max_policy_pct = DIV_ROUND_UP(policy->max * 100,
 +					      policy->cpuinfo.max_freq);
 +	limits->max_policy_pct = clamp_t(int, limits->max_policy_pct, 0 , 100);
 +
 +	/* Normalize user input to [min_policy_pct, max_policy_pct] */
 +	limits->min_perf_pct = max(limits->min_policy_pct,
 +				   limits->min_sysfs_pct);
 +	limits->min_perf_pct = min(limits->max_policy_pct,
 +				   limits->min_perf_pct);
 +	limits->max_perf_pct = min(limits->max_policy_pct,
 +				   limits->max_sysfs_pct);
 +	limits->max_perf_pct = max(limits->min_policy_pct,
 +				   limits->max_perf_pct);
 +
 +	/* Make sure min_perf_pct <= max_perf_pct */
 +	limits->min_perf_pct = min(limits->max_perf_pct, limits->min_perf_pct);
 +
 +	limits->min_perf = div_fp(int_tofp(limits->min_perf_pct),
 +				  int_tofp(100));
 +	limits->max_perf = div_fp(int_tofp(limits->max_perf_pct),
 +				  int_tofp(100));
 +	limits->max_perf = round_up(limits->max_perf, FRAC_BITS);
  
  	if (hwp_active)
++<<<<<<< HEAD
 +		intel_pstate_hwp_set(policy->cpus);
++=======
+ 		intel_pstate_hwp_set(policy->cpu);
+ 
+ 	mutex_unlock(&intel_pstate_limits_lock);
++>>>>>>> 2bfc4cbb5fd3 (cpufreq: intel_pstate: Do not walk policy->cpus)
  
  	return 0;
  }
* Unmerged path drivers/cpufreq/intel_pstate.c
