block: add scalable completion tracking of requests

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [block] add scalable completion tracking of requests (Ming Lei) [1458104]
Rebuild_FUZZ: 92.63%
commit-author Jens Axboe <axboe@fb.com>
commit cf43e6be865a582ba66ee4747ae27a0513f6bba1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/cf43e6be.failed

For legacy block, we simply track them in the request queue. For
blk-mq, we track them on a per-sw queue basis, which we can then
sum up through the hardware queues and finally to a per device
state.

The stats are tracked in, roughly, 0.1s interval windows.

Add sysfs files to display the stats.

The feature is off by default, to avoid any extra overhead. In-kernel
users of it can turn it on by setting QUEUE_FLAG_STATS in the queue
flags. We currently don't turn it on if someone just reads any of
the stats files, that is something we could add as well.

	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit cf43e6be865a582ba66ee4747ae27a0513f6bba1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/Makefile
#	block/blk-core.c
#	block/blk-mq-sysfs.c
#	block/blk-mq.h
#	block/blk-sysfs.c
#	include/linux/blkdev.h
diff --cc block/Makefile
index 93c61e8f29e6,2528c596f7ec..000000000000
--- a/block/Makefile
+++ b/block/Makefile
@@@ -2,14 -2,15 +2,21 @@@
  # Makefile for the kernel block layer
  #
  
 -obj-$(CONFIG_BLOCK) := bio.o elevator.o blk-core.o blk-tag.o blk-sysfs.o \
 +obj-$(CONFIG_BLOCK) := elevator.o blk-core.o blk-tag.o blk-sysfs.o \
  			blk-flush.o blk-settings.o blk-ioc.o blk-map.o \
  			blk-exec.o blk-merge.o blk-softirq.o blk-timeout.o \
++<<<<<<< HEAD
 +			blk-lib.o blk-mq.o blk-mq-tag.o \
 +			blk-mq-sysfs.o blk-mq-cpu.o blk-mq-cpumap.o ioctl.o \
 +			genhd.o scsi_ioctl.o partition-generic.o partitions/ \
 +			badblocks.o
++=======
+ 			blk-lib.o blk-mq.o blk-mq-tag.o blk-stat.o \
+ 			blk-mq-sysfs.o blk-mq-cpumap.o ioctl.o \
+ 			genhd.o scsi_ioctl.o partition-generic.o ioprio.o \
+ 			badblocks.o partitions/
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  
 -obj-$(CONFIG_BOUNCE)	+= bounce.o
  obj-$(CONFIG_BLK_DEV_BSG)	+= bsg.o
  obj-$(CONFIG_BLK_DEV_BSGLIB)	+= bsg-lib.o
  obj-$(CONFIG_BLK_CGROUP)	+= blk-cgroup.o
diff --cc block/blk-core.c
index 844f81639307,216372b01624..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -2714,8 -2688,13 +2719,18 @@@ EXPORT_SYMBOL_GPL(blk_unprep_request)
   */
  void blk_finish_request(struct request *req, int error)
  {
++<<<<<<< HEAD
 +	if (blk_rq_tagged(req))
 +		blk_queue_end_tag(req->q, req);
++=======
+ 	struct request_queue *q = req->q;
+ 
+ 	if (req->rq_flags & RQF_STATS)
+ 		blk_stat_add(&q->rq_stats[rq_data_dir(req)], req);
+ 
+ 	if (req->rq_flags & RQF_QUEUED)
+ 		blk_queue_end_tag(q, req);
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  
  	BUG_ON(blk_queued_rq(req));
  
diff --cc block/blk-mq-sysfs.c
index 08941faf0f9a,eacd3af72099..000000000000
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@@ -297,6 -353,16 +338,19 @@@ static struct blk_mq_hw_ctx_sysfs_entr
  	.attr = {.name = "cpu_list", .mode = S_IRUGO },
  	.show = blk_mq_hw_sysfs_cpus_show,
  };
++<<<<<<< HEAD
++=======
+ static struct blk_mq_hw_ctx_sysfs_entry blk_mq_hw_sysfs_poll = {
+ 	.attr = {.name = "io_poll", .mode = S_IWUSR | S_IRUGO },
+ 	.show = blk_mq_hw_sysfs_poll_show,
+ 	.store = blk_mq_hw_sysfs_poll_store,
+ };
+ static struct blk_mq_hw_ctx_sysfs_entry blk_mq_hw_sysfs_stat = {
+ 	.attr = {.name = "stats", .mode = S_IRUGO | S_IWUSR },
+ 	.show = blk_mq_hw_sysfs_stat_show,
+ 	.store = blk_mq_hw_sysfs_stat_store,
+ };
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  
  static struct attribute *default_hw_ctx_attrs[] = {
  	&blk_mq_hw_sysfs_queued.attr,
@@@ -306,6 -372,8 +360,11 @@@
  	&blk_mq_hw_sysfs_tags.attr,
  	&blk_mq_hw_sysfs_cpus.attr,
  	&blk_mq_hw_sysfs_active.attr,
++<<<<<<< HEAD
++=======
+ 	&blk_mq_hw_sysfs_poll.attr,
+ 	&blk_mq_hw_sysfs_stat.attr,
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  	NULL,
  };
  
diff --cc block/blk-mq.h
index 2d50f02667c4,b444370ae05b..000000000000
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@@ -1,7 -1,7 +1,11 @@@
  #ifndef INT_BLK_MQ_H
  #define INT_BLK_MQ_H
  
++<<<<<<< HEAD
 +#include <linux/rh_kabi.h>
++=======
+ #include "blk-stat.h"
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  
  struct blk_mq_tag_set;
  
diff --cc block/blk-sysfs.c
index 91f42f273aad,9cdb7247727a..000000000000
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@@ -317,6 -336,91 +317,94 @@@ queue_rq_affinity_store(struct request_
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t queue_poll_show(struct request_queue *q, char *page)
+ {
+ 	return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+ }
+ 
+ static ssize_t queue_poll_store(struct request_queue *q, const char *page,
+ 				size_t count)
+ {
+ 	unsigned long poll_on;
+ 	ssize_t ret;
+ 
+ 	if (!q->mq_ops || !q->mq_ops->poll)
+ 		return -EINVAL;
+ 
+ 	ret = queue_var_store(&poll_on, page, count);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	spin_lock_irq(q->queue_lock);
+ 	if (poll_on)
+ 		queue_flag_set(QUEUE_FLAG_POLL, q);
+ 	else
+ 		queue_flag_clear(QUEUE_FLAG_POLL, q);
+ 	spin_unlock_irq(q->queue_lock);
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t queue_wc_show(struct request_queue *q, char *page)
+ {
+ 	if (test_bit(QUEUE_FLAG_WC, &q->queue_flags))
+ 		return sprintf(page, "write back\n");
+ 
+ 	return sprintf(page, "write through\n");
+ }
+ 
+ static ssize_t queue_wc_store(struct request_queue *q, const char *page,
+ 			      size_t count)
+ {
+ 	int set = -1;
+ 
+ 	if (!strncmp(page, "write back", 10))
+ 		set = 1;
+ 	else if (!strncmp(page, "write through", 13) ||
+ 		 !strncmp(page, "none", 4))
+ 		set = 0;
+ 
+ 	if (set == -1)
+ 		return -EINVAL;
+ 
+ 	spin_lock_irq(q->queue_lock);
+ 	if (set)
+ 		queue_flag_set(QUEUE_FLAG_WC, q);
+ 	else
+ 		queue_flag_clear(QUEUE_FLAG_WC, q);
+ 	spin_unlock_irq(q->queue_lock);
+ 
+ 	return count;
+ }
+ 
+ static ssize_t queue_dax_show(struct request_queue *q, char *page)
+ {
+ 	return queue_var_show(blk_queue_dax(q), page);
+ }
+ 
+ static ssize_t print_stat(char *page, struct blk_rq_stat *stat, const char *pre)
+ {
+ 	return sprintf(page, "%s samples=%llu, mean=%lld, min=%lld, max=%lld\n",
+ 			pre, (long long) stat->nr_samples,
+ 			(long long) stat->mean, (long long) stat->min,
+ 			(long long) stat->max);
+ }
+ 
+ static ssize_t queue_stats_show(struct request_queue *q, char *page)
+ {
+ 	struct blk_rq_stat stat[2];
+ 	ssize_t ret;
+ 
+ 	blk_queue_stat_get(q, stat);
+ 
+ 	ret = print_stat(page, &stat[BLK_STAT_READ], "read :");
+ 	ret += print_stat(page + ret, &stat[BLK_STAT_WRITE], "write:");
+ 	return ret;
+ }
+ 
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  static struct queue_sysfs_entry queue_requests_entry = {
  	.attr = {.name = "nr_requests", .mode = S_IRUGO | S_IWUSR },
  	.show = queue_requests_show,
@@@ -442,6 -556,28 +530,31 @@@ static struct queue_sysfs_entry queue_r
  	.store = queue_store_random,
  };
  
++<<<<<<< HEAD
++=======
+ static struct queue_sysfs_entry queue_poll_entry = {
+ 	.attr = {.name = "io_poll", .mode = S_IRUGO | S_IWUSR },
+ 	.show = queue_poll_show,
+ 	.store = queue_poll_store,
+ };
+ 
+ static struct queue_sysfs_entry queue_wc_entry = {
+ 	.attr = {.name = "write_cache", .mode = S_IRUGO | S_IWUSR },
+ 	.show = queue_wc_show,
+ 	.store = queue_wc_store,
+ };
+ 
+ static struct queue_sysfs_entry queue_dax_entry = {
+ 	.attr = {.name = "dax", .mode = S_IRUGO },
+ 	.show = queue_dax_show,
+ };
+ 
+ static struct queue_sysfs_entry queue_stats_entry = {
+ 	.attr = {.name = "stats", .mode = S_IRUGO },
+ 	.show = queue_stats_show,
+ };
+ 
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  static struct attribute *default_attrs[] = {
  	&queue_requests_entry.attr,
  	&queue_ra_entry.attr,
@@@ -466,6 -604,10 +579,13 @@@
  	&queue_rq_affinity_entry.attr,
  	&queue_iostats_entry.attr,
  	&queue_random_entry.attr,
++<<<<<<< HEAD
++=======
+ 	&queue_poll_entry.attr,
+ 	&queue_wc_entry.attr,
+ 	&queue_dax_entry.attr,
+ 	&queue_stats_entry.attr,
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  	NULL,
  };
  
diff --cc include/linux/blkdev.h
index ba3405333171,303723a2e5b8..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -79,24 -75,55 +79,73 @@@ struct request_list 
  enum rq_cmd_type_bits {
  	REQ_TYPE_FS		= 1,	/* fs request */
  	REQ_TYPE_BLOCK_PC,		/* scsi command */
 -	REQ_TYPE_DRV_PRIV,		/* driver defined types from here */
 +	REQ_TYPE_SENSE,			/* sense request */
 +	REQ_TYPE_PM_SUSPEND,		/* suspend request */
 +	REQ_TYPE_PM_RESUME,		/* resume request */
 +	REQ_TYPE_PM_SHUTDOWN,		/* shutdown request */
 +#ifdef __GENKSYMS__
 +	REQ_TYPE_SPECIAL,		/* driver defined type */
 +#else
 +	REQ_TYPE_DRV_PRIV,		/* driver defined type */
 +#endif
 +	/*
 +	 * for ATA/ATAPI devices. this really doesn't belong here, ide should
 +	 * use REQ_TYPE_DRV_PRIV and use rq->cmd[0] with the range of driver
 +	 * private REQ_LB opcodes to differentiate what type of request this is
 +	 */
 +	REQ_TYPE_ATA_TASKFILE,
 +	REQ_TYPE_ATA_PC,
  };
  
++<<<<<<< HEAD
++=======
+ /*
+  * request flags */
+ typedef __u32 __bitwise req_flags_t;
+ 
+ /* elevator knows about this request */
+ #define RQF_SORTED		((__force req_flags_t)(1 << 0))
+ /* drive already may have started this one */
+ #define RQF_STARTED		((__force req_flags_t)(1 << 1))
+ /* uses tagged queueing */
+ #define RQF_QUEUED		((__force req_flags_t)(1 << 2))
+ /* may not be passed by ioscheduler */
+ #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
+ /* request for flush sequence */
+ #define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
+ /* merge of different types, fail separately */
+ #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
+ /* track inflight for MQ */
+ #define RQF_MQ_INFLIGHT		((__force req_flags_t)(1 << 6))
+ /* don't call prep for this one */
+ #define RQF_DONTPREP		((__force req_flags_t)(1 << 7))
+ /* set for "ide_preempt" requests and also for requests for which the SCSI
+    "quiesce" state must be ignored. */
+ #define RQF_PREEMPT		((__force req_flags_t)(1 << 8))
+ /* contains copies of user pages */
+ #define RQF_COPY_USER		((__force req_flags_t)(1 << 9))
+ /* vaguely specified driver internal error.  Ignored by the block layer */
+ #define RQF_FAILED		((__force req_flags_t)(1 << 10))
+ /* don't warn about errors */
+ #define RQF_QUIET		((__force req_flags_t)(1 << 11))
+ /* elevator private data attached */
+ #define RQF_ELVPRIV		((__force req_flags_t)(1 << 12))
+ /* account I/O stat */
+ #define RQF_IO_STAT		((__force req_flags_t)(1 << 13))
+ /* request came from our alloc pool */
+ #define RQF_ALLOCED		((__force req_flags_t)(1 << 14))
+ /* runtime pm request */
+ #define RQF_PM			((__force req_flags_t)(1 << 15))
+ /* on IO scheduler merge hash */
+ #define RQF_HASHED		((__force req_flags_t)(1 << 16))
+ /* IO stats tracking on */
+ #define RQF_STATS		((__force req_flags_t)(1 << 17))
+ 
+ /* flags that prevent us from merging requests: */
+ #define RQF_NOMERGE_FLAGS \
+ 	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ)
+ 
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  #define BLK_MAX_CDB	16
  
  /*
@@@ -567,10 -585,13 +620,20 @@@ struct request_queue 
  #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
  #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
  #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
++<<<<<<< HEAD
 +#define QUEUE_FLAG_UNPRIV_SGIO 21	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NO_SG_MERGE 22	/* don't attempt to merge SG segments*/
 +#define QUEUE_FLAG_SG_GAPS     23	/* queue doesn't support SG gaps */
 +#define QUEUE_FLAG_DAX         24	/* device supports DAX */
++=======
+ #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
+ #define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
+ #define QUEUE_FLAG_WC	       23	/* Write back caching */
+ #define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
+ #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
+ #define QUEUE_FLAG_DAX         26	/* device supports DAX */
+ #define QUEUE_FLAG_STATS       27	/* track rq completion times */
++>>>>>>> cf43e6be865a (block: add scalable completion tracking of requests)
  
  #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
  				 (1 << QUEUE_FLAG_STACKABLE)	|	\
* Unmerged path block/Makefile
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq-sysfs.c
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 1b06c94aa73d..5b45178a2f86 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -29,6 +29,7 @@
 #include "blk.h"
 #include "blk-mq.h"
 #include "blk-mq-tag.h"
+#include "blk-stat.h"
 
 static DEFINE_MUTEX(all_q_mutex);
 static LIST_HEAD(all_q_list);
@@ -443,10 +444,27 @@ static void blk_mq_ipi_complete_request(struct request *rq)
 	put_cpu();
 }
 
+static void blk_mq_stat_add(struct request *rq)
+{
+	if (rq->rq_flags & RQF_STATS) {
+		/*
+		 * We could rq->mq_ctx here, but there's less of a risk
+		 * of races if we have the completion event add the stats
+		 * to the local software queue.
+		 */
+		struct blk_mq_ctx *ctx;
+
+		ctx = __blk_mq_get_ctx(rq->q, raw_smp_processor_id());
+		blk_stat_add(&ctx->stat[rq_data_dir(rq)], rq);
+	}
+}
+
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 
+	blk_mq_stat_add(rq);
+
 	if (!q->softirq_done_fn)
 		blk_mq_end_request(rq, rq->errors);
 	else
@@ -490,6 +508,11 @@ void blk_mq_start_request(struct request *rq)
 	if (unlikely(blk_bidi_rq(rq)))
 		rq->next_rq->resid_len = blk_rq_bytes(rq->next_rq);
 
+	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+		blk_stat_set_issue_time(&rq->issue_stat);
+		rq->rq_flags |= RQF_STATS;
+	}
+
 	blk_add_timer(rq);
 
 	/*
@@ -1902,6 +1925,8 @@ static void blk_mq_init_cpu_queues(struct request_queue *q,
 		spin_lock_init(&__ctx->lock);
 		INIT_LIST_HEAD(&__ctx->rq_list);
 		__ctx->queue = q;
+		blk_stat_init(&__ctx->stat[BLK_STAT_READ]);
+		blk_stat_init(&__ctx->stat[BLK_STAT_WRITE]);
 
 		/* If the cpu isn't online, the cpu is mapped to first hctx */
 		if (!cpu_online(i))
* Unmerged path block/blk-mq.h
diff --git a/block/blk-stat.c b/block/blk-stat.c
new file mode 100644
index 000000000000..688c958367ee
--- /dev/null
+++ b/block/blk-stat.c
@@ -0,0 +1,248 @@
+/*
+ * Block stat tracking code
+ *
+ * Copyright (C) 2016 Jens Axboe
+ */
+#include <linux/kernel.h>
+#include <linux/blk-mq.h>
+
+#include "blk-stat.h"
+#include "blk-mq.h"
+
+static void blk_stat_flush_batch(struct blk_rq_stat *stat)
+{
+	const s32 nr_batch = READ_ONCE(stat->nr_batch);
+	const s32 nr_samples = READ_ONCE(stat->nr_batch);
+
+	if (!nr_batch)
+		return;
+	if (!nr_samples)
+		stat->mean = div64_s64(stat->batch, nr_batch);
+	else {
+		stat->mean = div64_s64((stat->mean * nr_samples) +
+					stat->batch,
+					nr_batch + nr_samples);
+	}
+
+	stat->nr_samples += nr_batch;
+	stat->nr_batch = stat->batch = 0;
+}
+
+static void blk_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
+{
+	if (!src->nr_samples)
+		return;
+
+	blk_stat_flush_batch(src);
+
+	dst->min = min(dst->min, src->min);
+	dst->max = max(dst->max, src->max);
+
+	if (!dst->nr_samples)
+		dst->mean = src->mean;
+	else {
+		dst->mean = div64_s64((src->mean * src->nr_samples) +
+					(dst->mean * dst->nr_samples),
+					dst->nr_samples + src->nr_samples);
+	}
+	dst->nr_samples += src->nr_samples;
+}
+
+static void blk_mq_stat_get(struct request_queue *q, struct blk_rq_stat *dst)
+{
+	struct blk_mq_hw_ctx *hctx;
+	struct blk_mq_ctx *ctx;
+	uint64_t latest = 0;
+	int i, j, nr;
+
+	blk_stat_init(&dst[BLK_STAT_READ]);
+	blk_stat_init(&dst[BLK_STAT_WRITE]);
+
+	nr = 0;
+	do {
+		uint64_t newest = 0;
+
+		queue_for_each_hw_ctx(q, hctx, i) {
+			hctx_for_each_ctx(hctx, ctx, j) {
+				if (!ctx->stat[BLK_STAT_READ].nr_samples &&
+				    !ctx->stat[BLK_STAT_WRITE].nr_samples)
+					continue;
+				if (ctx->stat[BLK_STAT_READ].time > newest)
+					newest = ctx->stat[BLK_STAT_READ].time;
+				if (ctx->stat[BLK_STAT_WRITE].time > newest)
+					newest = ctx->stat[BLK_STAT_WRITE].time;
+			}
+		}
+
+		/*
+		 * No samples
+		 */
+		if (!newest)
+			break;
+
+		if (newest > latest)
+			latest = newest;
+
+		queue_for_each_hw_ctx(q, hctx, i) {
+			hctx_for_each_ctx(hctx, ctx, j) {
+				if (ctx->stat[BLK_STAT_READ].time == newest) {
+					blk_stat_sum(&dst[BLK_STAT_READ],
+						     &ctx->stat[BLK_STAT_READ]);
+					nr++;
+				}
+				if (ctx->stat[BLK_STAT_WRITE].time == newest) {
+					blk_stat_sum(&dst[BLK_STAT_WRITE],
+						     &ctx->stat[BLK_STAT_WRITE]);
+					nr++;
+				}
+			}
+		}
+		/*
+		 * If we race on finding an entry, just loop back again.
+		 * Should be very rare.
+		 */
+	} while (!nr);
+
+	dst[BLK_STAT_READ].time = dst[BLK_STAT_WRITE].time = latest;
+}
+
+void blk_queue_stat_get(struct request_queue *q, struct blk_rq_stat *dst)
+{
+	if (q->mq_ops)
+		blk_mq_stat_get(q, dst);
+	else {
+		memcpy(&dst[BLK_STAT_READ], &q->rq_stats[BLK_STAT_READ],
+				sizeof(struct blk_rq_stat));
+		memcpy(&dst[BLK_STAT_WRITE], &q->rq_stats[BLK_STAT_WRITE],
+				sizeof(struct blk_rq_stat));
+	}
+}
+
+void blk_hctx_stat_get(struct blk_mq_hw_ctx *hctx, struct blk_rq_stat *dst)
+{
+	struct blk_mq_ctx *ctx;
+	unsigned int i, nr;
+
+	nr = 0;
+	do {
+		uint64_t newest = 0;
+
+		hctx_for_each_ctx(hctx, ctx, i) {
+			if (!ctx->stat[BLK_STAT_READ].nr_samples &&
+			    !ctx->stat[BLK_STAT_WRITE].nr_samples)
+				continue;
+
+			if (ctx->stat[BLK_STAT_READ].time > newest)
+				newest = ctx->stat[BLK_STAT_READ].time;
+			if (ctx->stat[BLK_STAT_WRITE].time > newest)
+				newest = ctx->stat[BLK_STAT_WRITE].time;
+		}
+
+		if (!newest)
+			break;
+
+		hctx_for_each_ctx(hctx, ctx, i) {
+			if (ctx->stat[BLK_STAT_READ].time == newest) {
+				blk_stat_sum(&dst[BLK_STAT_READ],
+						&ctx->stat[BLK_STAT_READ]);
+				nr++;
+			}
+			if (ctx->stat[BLK_STAT_WRITE].time == newest) {
+				blk_stat_sum(&dst[BLK_STAT_WRITE],
+						&ctx->stat[BLK_STAT_WRITE]);
+				nr++;
+			}
+		}
+		/*
+		 * If we race on finding an entry, just loop back again.
+		 * Should be very rare, as the window is only updated
+		 * occasionally
+		 */
+	} while (!nr);
+}
+
+static void __blk_stat_init(struct blk_rq_stat *stat, s64 time_now)
+{
+	stat->min = -1ULL;
+	stat->max = stat->nr_samples = stat->mean = 0;
+	stat->batch = stat->nr_batch = 0;
+	stat->time = time_now & BLK_STAT_NSEC_MASK;
+}
+
+void blk_stat_init(struct blk_rq_stat *stat)
+{
+	__blk_stat_init(stat, ktime_to_ns(ktime_get()));
+}
+
+static bool __blk_stat_is_current(struct blk_rq_stat *stat, s64 now)
+{
+	return (now & BLK_STAT_NSEC_MASK) == (stat->time & BLK_STAT_NSEC_MASK);
+}
+
+bool blk_stat_is_current(struct blk_rq_stat *stat)
+{
+	return __blk_stat_is_current(stat, ktime_to_ns(ktime_get()));
+}
+
+void blk_stat_add(struct blk_rq_stat *stat, struct request *rq)
+{
+	s64 now, value;
+
+	now = __blk_stat_time(ktime_to_ns(ktime_get()));
+	if (now < blk_stat_time(&rq->issue_stat))
+		return;
+
+	if (!__blk_stat_is_current(stat, now))
+		__blk_stat_init(stat, now);
+
+	value = now - blk_stat_time(&rq->issue_stat);
+	if (value > stat->max)
+		stat->max = value;
+	if (value < stat->min)
+		stat->min = value;
+
+	if (stat->batch + value < stat->batch ||
+	    stat->nr_batch + 1 == BLK_RQ_STAT_BATCH)
+		blk_stat_flush_batch(stat);
+
+	stat->batch += value;
+	stat->nr_batch++;
+}
+
+void blk_stat_clear(struct request_queue *q)
+{
+	if (q->mq_ops) {
+		struct blk_mq_hw_ctx *hctx;
+		struct blk_mq_ctx *ctx;
+		int i, j;
+
+		queue_for_each_hw_ctx(q, hctx, i) {
+			hctx_for_each_ctx(hctx, ctx, j) {
+				blk_stat_init(&ctx->stat[BLK_STAT_READ]);
+				blk_stat_init(&ctx->stat[BLK_STAT_WRITE]);
+			}
+		}
+	} else {
+		blk_stat_init(&q->rq_stats[BLK_STAT_READ]);
+		blk_stat_init(&q->rq_stats[BLK_STAT_WRITE]);
+	}
+}
+
+void blk_stat_set_issue_time(struct blk_issue_stat *stat)
+{
+	stat->time = (stat->time & BLK_STAT_MASK) |
+			(ktime_to_ns(ktime_get()) & BLK_STAT_TIME_MASK);
+}
+
+/*
+ * Enable stat tracking, return whether it was enabled
+ */
+bool blk_stat_enable(struct request_queue *q)
+{
+	if (!test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+		set_bit(QUEUE_FLAG_STATS, &q->queue_flags);
+		return false;
+	}
+
+	return true;
+}
diff --git a/block/blk-stat.h b/block/blk-stat.h
new file mode 100644
index 000000000000..a2050a0a5314
--- /dev/null
+++ b/block/blk-stat.h
@@ -0,0 +1,42 @@
+#ifndef BLK_STAT_H
+#define BLK_STAT_H
+
+/*
+ * ~0.13s window as a power-of-2 (2^27 nsecs)
+ */
+#define BLK_STAT_NSEC		134217728ULL
+#define BLK_STAT_NSEC_MASK	~(BLK_STAT_NSEC - 1)
+
+/*
+ * Upper 3 bits can be used elsewhere
+ */
+#define BLK_STAT_RES_BITS	3
+#define BLK_STAT_SHIFT		(64 - BLK_STAT_RES_BITS)
+#define BLK_STAT_TIME_MASK	((1ULL << BLK_STAT_SHIFT) - 1)
+#define BLK_STAT_MASK		~BLK_STAT_TIME_MASK
+
+enum {
+	BLK_STAT_READ	= 0,
+	BLK_STAT_WRITE,
+};
+
+void blk_stat_add(struct blk_rq_stat *, struct request *);
+void blk_hctx_stat_get(struct blk_mq_hw_ctx *, struct blk_rq_stat *);
+void blk_queue_stat_get(struct request_queue *, struct blk_rq_stat *);
+void blk_stat_clear(struct request_queue *);
+void blk_stat_init(struct blk_rq_stat *);
+bool blk_stat_is_current(struct blk_rq_stat *);
+void blk_stat_set_issue_time(struct blk_issue_stat *);
+bool blk_stat_enable(struct request_queue *);
+
+static inline u64 __blk_stat_time(u64 time)
+{
+	return time & BLK_STAT_TIME_MASK;
+}
+
+static inline u64 blk_stat_time(struct blk_issue_stat *stat)
+{
+	return __blk_stat_time(stat->time);
+}
+
+#endif
* Unmerged path block/blk-sysfs.c
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 822005a81879..9b6362962bd6 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -288,4 +288,20 @@ static inline int op_from_rq_bits(u64 flags)
 		return REQ_OP_READ;
 }
 
+struct blk_issue_stat {
+	u64 time;
+};
+
+#define BLK_RQ_STAT_BATCH	64
+
+struct blk_rq_stat {
+	s64 mean;
+	u64 min;
+	u64 max;
+	s32 nr_samples;
+	s32 nr_batch;
+	u64 batch;
+	s64 time;
+};
+
 #endif /* __LINUX_BLK_TYPES_H */
* Unmerged path include/linux/blkdev.h
