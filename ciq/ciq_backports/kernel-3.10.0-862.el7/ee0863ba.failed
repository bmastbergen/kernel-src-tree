chcr - Add debug counters

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Harsh Jain <harsh@chelsio.com>
commit ee0863ba118d37609a795a15c08e9acf6809c038
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ee0863ba.failed

Count types of operation done by HW.

	Signed-off-by: Harsh Jain <harsh@chelsio.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit ee0863ba118d37609a795a15c08e9acf6809c038)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/crypto/chelsio/chcr_algo.c
#	drivers/crypto/chelsio/chcr_core.c
diff --cc drivers/crypto/chelsio/chcr_algo.c
index bda117371c9f,2f388af232ee..000000000000
--- a/drivers/crypto/chelsio/chcr_algo.c
+++ b/drivers/crypto/chelsio/chcr_algo.c
@@@ -105,25 -153,30 +105,26 @@@ int chcr_handle_resp(struct crypto_asyn
  	struct chcr_context *ctx = crypto_tfm_ctx(tfm);
  	struct uld_ctx *u_ctx = ULD_CTX(ctx);
  	struct chcr_req_ctx ctx_req;
 +	struct cpl_fw6_pld *fw6_pld;
  	unsigned int digestsize, updated_digestsize;
+ 	struct adapter *adap = padap(ctx->dev);
  
  	switch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {
 -	case CRYPTO_ALG_TYPE_AEAD:
 -		ctx_req.req.aead_req = aead_request_cast(req);
 -		ctx_req.ctx.reqctx = aead_request_ctx(ctx_req.req.aead_req);
 -		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.ctx.reqctx->dst,
 -			     ctx_req.ctx.reqctx->dst_nents, DMA_FROM_DEVICE);
 -		if (ctx_req.ctx.reqctx->skb) {
 -			kfree_skb(ctx_req.ctx.reqctx->skb);
 -			ctx_req.ctx.reqctx->skb = NULL;
 +	case CRYPTO_ALG_TYPE_BLKCIPHER:
 +		ctx_req.req.ablk_req = (struct ablkcipher_request *)req;
 +		ctx_req.ctx.ablk_ctx =
 +			ablkcipher_request_ctx(ctx_req.req.ablk_req);
 +		if (!error_status) {
 +			fw6_pld = (struct cpl_fw6_pld *)input;
 +			memcpy(ctx_req.req.ablk_req->info, &fw6_pld->data[2],
 +			       AES_BLOCK_SIZE);
  		}
 -		if (ctx_req.ctx.reqctx->verify == VERIFY_SW) {
 -			chcr_verify_tag(ctx_req.req.aead_req, input,
 -					&err);
 -			ctx_req.ctx.reqctx->verify = VERIFY_HW;
 +		dma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.req.ablk_req->dst,
 +			     ctx_req.ctx.ablk_ctx->dst_nents, DMA_FROM_DEVICE);
 +		if (ctx_req.ctx.ablk_ctx->skb) {
 +			kfree_skb(ctx_req.ctx.ablk_ctx->skb);
 +			ctx_req.ctx.ablk_ctx->skb = NULL;
  		}
 -		ctx_req.req.aead_req->base.complete(req, err);
 -		break;
 -
 -	case CRYPTO_ALG_TYPE_ABLKCIPHER:
 -		 err = chcr_handle_cipher_resp(ablkcipher_request_cast(req),
 -					       input, err);
  		break;
  
  	case CRYPTO_ALG_TYPE_AHASH:
@@@ -152,9 -205,11 +153,14 @@@
  			       sizeof(struct cpl_fw6_pld),
  			       updated_digestsize);
  		}
 -		ctx_req.req.ahash_req->base.complete(req, err);
  		break;
  	}
++<<<<<<< HEAD
 +	return 0;
++=======
+ 	atomic_inc(&adap->chcr_stats.complete);
+ 	return err;
++>>>>>>> ee0863ba118d (chcr - Add debug counters)
  }
  
  /*
@@@ -506,26 -633,15 +512,34 @@@ static struct sk_buf
  	struct sk_buff *skb = NULL;
  	struct chcr_wr *chcr_req;
  	struct cpl_rx_phys_dsgl *phys_cpl;
 -	struct chcr_blkcipher_req_ctx *reqctx =
 -		ablkcipher_request_ctx(wrparam->req);
 +	struct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);
  	struct phys_sge_parm sg_param;
  	unsigned int frags = 0, transhdr_len, phys_dsgl;
++<<<<<<< HEAD
 +	unsigned int ivsize = crypto_ablkcipher_ivsize(tfm), kctx_len;
 +	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
 +			GFP_ATOMIC;
 +
 +	if (!req->info)
 +		return ERR_PTR(-EINVAL);
 +	reqctx->dst_nents = sg_nents_for_len(req->dst, req->nbytes);
 +	if (reqctx->dst_nents <= 0) {
 +		pr_err("AES:Invalid Destination sg lists\n");
 +		return ERR_PTR(-EINVAL);
 +	}
 +	if ((ablkctx->enckey_len == 0) || (ivsize > AES_BLOCK_SIZE) ||
 +	    (req->nbytes <= 0) || (req->nbytes % AES_BLOCK_SIZE)) {
 +		pr_err("AES: Invalid value of Key Len %d nbytes %d IV Len %d\n",
 +		       ablkctx->enckey_len, req->nbytes, ivsize);
 +		return ERR_PTR(-EINVAL);
 +	}
++=======
+ 	int error;
+ 	unsigned int ivsize = AES_BLOCK_SIZE, kctx_len;
+ 	gfp_t flags = wrparam->req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?
+ 			GFP_KERNEL : GFP_ATOMIC;
+ 	struct adapter *adap = padap(ctx->dev);
++>>>>>>> ee0863ba118d (chcr - Add debug counters)
  
  	phys_dsgl = get_space_for_phys_dsgl(reqctx->dst_nents);
  
@@@ -579,10 -702,12 +593,18 @@@
  		goto map_fail1;
  
  	skb_set_transport_header(skb, transhdr_len);
 +	memcpy(reqctx->iv, req->info, ivsize);
  	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
++<<<<<<< HEAD
 +	write_sg_to_skb(skb, &frags, req->src, req->nbytes);
 +	create_wreq(ctx, chcr_req, req, skb, kctx_len, 0, phys_dsgl);
++=======
+ 	write_sg_to_skb(skb, &frags, wrparam->srcsg, wrparam->bytes);
+ 	atomic_inc(&adap->chcr_stats.cipher_rqst);
+ 	create_wreq(ctx, chcr_req, &(wrparam->req->base), skb, kctx_len, 0, 1,
+ 			sizeof(struct cpl_rx_phys_dsgl) + phys_dsgl,
+ 			ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC);
++>>>>>>> ee0863ba118d (chcr - Add debug counters)
  	reqctx->skb = skb;
  	skb_get(skb);
  	return skb;
@@@ -851,9 -1398,9 +874,15 @@@ static struct sk_buff *create_hash_wr(s
  				    param->bfr_len);
  	if (param->sg_len != 0)
  		write_sg_to_skb(skb, &frags, req->src, param->sg_len);
++<<<<<<< HEAD
 +
 +	create_wreq(ctx, chcr_req, req, skb, kctx_len, hash_size_in_response,
 +		    0);
++=======
+ 	atomic_inc(&adap->chcr_stats.digest_rqst);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len,
+ 		    hash_size_in_response, 0, DUMMY_BYTES, 0);
++>>>>>>> ee0863ba118d (chcr - Add debug counters)
  	req_ctx->skb = skb;
  	skb_get(skb);
  	return skb;
@@@ -1260,6 -1808,1244 +1289,1247 @@@ static void chcr_hmac_cra_exit(struct c
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static int chcr_copy_assoc(struct aead_request *req,
+ 				struct chcr_aead_ctx *ctx)
+ {
+ 	SKCIPHER_REQUEST_ON_STACK(skreq, ctx->null);
+ 
+ 	skcipher_request_set_tfm(skreq, ctx->null);
+ 	skcipher_request_set_callback(skreq, aead_request_flags(req),
+ 			NULL, NULL);
+ 	skcipher_request_set_crypt(skreq, req->src, req->dst, req->assoclen,
+ 			NULL);
+ 
+ 	return crypto_skcipher_encrypt(skreq);
+ }
+ static int chcr_aead_need_fallback(struct aead_request *req, int src_nent,
+ 				   int aadmax, int wrlen,
+ 				   unsigned short op_type)
+ {
+ 	unsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));
+ 
+ 	if (((req->cryptlen - (op_type ? authsize : 0)) == 0) ||
+ 	    (req->assoclen > aadmax) ||
+ 	    (src_nent > MAX_SKB_FRAGS) ||
+ 	    (wrlen > MAX_WR_SIZE))
+ 		return 1;
+ 	return 0;
+ }
+ 
+ static int chcr_aead_fallback(struct aead_request *req, unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_request *subreq = aead_request_ctx(req);
+ 
+ 	aead_request_set_tfm(subreq, aeadctx->sw_cipher);
+ 	aead_request_set_callback(subreq, req->base.flags,
+ 				  req->base.complete, req->base.data);
+ 	 aead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,
+ 				 req->iv);
+ 	 aead_request_set_ad(subreq, req->assoclen);
+ 	return op_type ? crypto_aead_decrypt(subreq) :
+ 		crypto_aead_encrypt(subreq);
+ }
+ 
+ static struct sk_buff *create_authenc_wr(struct aead_request *req,
+ 					 unsigned short qid,
+ 					 int size,
+ 					 unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = crypto_aead_ivsize(tfm), dst_size = 0;
+ 	unsigned int   kctx_len = 0;
+ 	unsigned short stop_offset = 0;
+ 	unsigned int  assoclen = req->assoclen;
+ 	unsigned int  authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	int null = 0;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 	struct adapter *adap = padap(ctx->dev);
+ 
+ 	if (aeadctx->enckey_len == 0 || (req->cryptlen == 0))
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error)
+ 			return ERR_PTR(error);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {
+ 		null = 1;
+ 		assoclen = 0;
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("AUTHENC:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)
+ 		- sizeof(chcr_req->key_ctx);
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_AUTH_SG,
+ 			T6_MAX_AAD_SIZE,
+ 			transhdr_len + (sgl_len(src_nent + MIN_AUTH_SG) * 8),
+ 				op_type)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	/* LLD is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	/* Write WR */
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	stop_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 
+ 	/*
+ 	 * Input order	is AAD,IV and Payload. where IV should be included as
+ 	 * the part of authdata. All other fields should be filled according
+ 	 * to the hardware spec
+ 	 */
+ 	chcr_req->sec_cpl.op_ivinsrtofst =
+ 		FILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,
+ 				       (ivsize ? (assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1,
+ 					(stop_offset & 0x1F0) >> 4);
+ 	chcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(
+ 					stop_offset & 0xF,
+ 					null ? 0 : assoclen + ivsize + 1,
+ 					stop_offset, stop_offset);
+ 	chcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_CBC,
+ 					actx->auth_mode, aeadctx->hmac_ctrl,
+ 					ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					 0, 1, dst_size);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	if (op_type == CHCR_ENCRYPT_OP)
+ 		memcpy(chcr_req->key_ctx.key, aeadctx->key,
+ 		       aeadctx->enckey_len);
+ 	else
+ 		memcpy(chcr_req->key_ctx.key, actx->dec_rrkey,
+ 		       aeadctx->enckey_len);
+ 
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<
+ 					4), actx->h_iopad, kctx_len -
+ 				(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 					reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 
+ 	if (assoclen) {
+ 		/* AAD buffer in */
+ 		write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 
+ 	}
+ 	write_buffer_to_skb(skb, &frags, req->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	atomic_inc(&adap->chcr_stats.cipher_rqst);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
+ 		   sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 
+ 	return skb;
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(error);
+ }
+ 
+ static int set_msg_len(u8 *block, unsigned int msglen, int csize)
+ {
+ 	__be32 data;
+ 
+ 	memset(block, 0, csize);
+ 	block += csize;
+ 
+ 	if (csize >= 4)
+ 		csize = 4;
+ 	else if (msglen > (unsigned int)(1 << (8 * csize)))
+ 		return -EOVERFLOW;
+ 
+ 	data = cpu_to_be32(msglen);
+ 	memcpy(block - csize, (u8 *)&data + 4 - csize, csize);
+ 
+ 	return 0;
+ }
+ 
+ static void generate_b0(struct aead_request *req,
+ 			struct chcr_aead_ctx *aeadctx,
+ 			unsigned short op_type)
+ {
+ 	unsigned int l, lp, m;
+ 	int rc;
+ 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	u8 *b0 = reqctx->scratch_pad;
+ 
+ 	m = crypto_aead_authsize(aead);
+ 
+ 	memcpy(b0, reqctx->iv, 16);
+ 
+ 	lp = b0[0];
+ 	l = lp + 1;
+ 
+ 	/* set m, bits 3-5 */
+ 	*b0 |= (8 * ((m - 2) / 2));
+ 
+ 	/* set adata, bit 6, if associated data is used */
+ 	if (req->assoclen)
+ 		*b0 |= 64;
+ 	rc = set_msg_len(b0 + 16 - l,
+ 			 (op_type == CHCR_DECRYPT_OP) ?
+ 			 req->cryptlen - m : req->cryptlen, l);
+ }
+ 
+ static inline int crypto_ccm_check_iv(const u8 *iv)
+ {
+ 	/* 2 <= L <= 8, so 1 <= L' <= 7. */
+ 	if (iv[0] < 1 || iv[0] > 7)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int ccm_format_packet(struct aead_request *req,
+ 			     struct chcr_aead_ctx *aeadctx,
+ 			     unsigned int sub_type,
+ 			     unsigned short op_type)
+ {
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int rc = 0;
+ 
+ 	if (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		reqctx->iv[0] = 3;
+ 		memcpy(reqctx->iv + 1, &aeadctx->salt[0], 3);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 		memset(reqctx->iv + 12, 0, 4);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen - 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 16);
+ 		*((unsigned short *)(reqctx->scratch_pad + 16)) =
+ 			htons(req->assoclen);
+ 	}
+ 	generate_b0(req, aeadctx, op_type);
+ 	/* zero the ctr value */
+ 	memset(reqctx->iv + 15 - reqctx->iv[0], 0, reqctx->iv[0] + 1);
+ 	return rc;
+ }
+ 
+ static void fill_sec_cpl_for_aead(struct cpl_tx_sec_pdu *sec_cpl,
+ 				  unsigned int dst_size,
+ 				  struct aead_request *req,
+ 				  unsigned short op_type,
+ 					  struct chcr_context *chcrctx)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int cipher_mode = CHCR_SCMD_CIPHER_MODE_AES_CCM;
+ 	unsigned int mac_mode = CHCR_SCMD_AUTH_MODE_CBCMAC;
+ 	unsigned int c_id = chcrctx->dev->rx_channel_id;
+ 	unsigned int ccm_xtra;
+ 	unsigned char tag_offset = 0, auth_offset = 0;
+ 	unsigned int assoclen;
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 		assoclen = req->assoclen - 8;
+ 	else
+ 		assoclen = req->assoclen;
+ 	ccm_xtra = CCM_B0_SIZE +
+ 		((assoclen) ? CCM_AAD_FIELD_SIZE : 0);
+ 
+ 	auth_offset = req->cryptlen ?
+ 		(assoclen + ivsize + 1 + ccm_xtra) : 0;
+ 	if (op_type == CHCR_DECRYPT_OP) {
+ 		if (crypto_aead_authsize(tfm) != req->cryptlen)
+ 			tag_offset = crypto_aead_authsize(tfm);
+ 		else
+ 			auth_offset = 0;
+ 	}
+ 
+ 
+ 	sec_cpl->op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(c_id,
+ 					 2, (ivsize ?  (assoclen + 1) :  0) +
+ 					 ccm_xtra);
+ 	sec_cpl->pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen + ccm_xtra);
+ 	/* For CCM there wil be b0 always. So AAD start will be 1 always */
+ 	sec_cpl->aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					1, assoclen + ccm_xtra, assoclen
+ 					+ ivsize + 1 + ccm_xtra, 0);
+ 
+ 	sec_cpl->cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(0,
+ 					auth_offset, tag_offset,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 :
+ 					crypto_aead_authsize(tfm));
+ 	sec_cpl->seqno_numivs =  FILL_SEC_CPL_SCMD0_SEQNO(op_type,
+ 					(op_type == CHCR_ENCRYPT_OP) ? 0 : 1,
+ 					cipher_mode, mac_mode,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 
+ 	sec_cpl->ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1, 0,
+ 					1, dst_size);
+ }
+ 
+ int aead_ccm_validate_input(unsigned short op_type,
+ 			    struct aead_request *req,
+ 			    struct chcr_aead_ctx *aeadctx,
+ 			    unsigned int sub_type)
+ {
+ 	if (sub_type != CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {
+ 		if (crypto_ccm_check_iv(req->iv)) {
+ 			pr_err("CCM: IV check fails\n");
+ 			return -EINVAL;
+ 		}
+ 	} else {
+ 		if (req->assoclen != 16 && req->assoclen != 20) {
+ 			pr_err("RFC4309: Invalid AAD length %d\n",
+ 			       req->assoclen);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 	if (aeadctx->enckey_len == 0) {
+ 		pr_err("CCM: Encryption key not set\n");
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ unsigned int fill_aead_req_fields(struct sk_buff *skb,
+ 				  struct aead_request *req,
+ 				  struct scatterlist *src,
+ 				  unsigned int ivsize,
+ 				  struct chcr_aead_ctx *aeadctx)
+ {
+ 	unsigned int frags = 0;
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	/* b0 and aad length(if available) */
+ 
+ 	write_buffer_to_skb(skb, &frags, reqctx->scratch_pad, CCM_B0_SIZE +
+ 				(req->assoclen ?  CCM_AAD_FIELD_SIZE : 0));
+ 	if (req->assoclen) {
+ 		if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)
+ 			write_sg_to_skb(skb, &frags, req->src,
+ 					req->assoclen - 8);
+ 		else
+ 			write_sg_to_skb(skb, &frags, req->src, req->assoclen);
+ 	}
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	if (req->cryptlen)
+ 		write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 
+ 	return frags;
+ }
+ 
+ static struct sk_buff *create_aead_ccm_wr(struct aead_request *req,
+ 					  unsigned short qid,
+ 					  int size,
+ 					  unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len, ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len;
+ 	unsigned int sub_type;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 	struct adapter *adap = padap(ctx->dev);
+ 
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	sub_type = get_aead_subtype(tfm);
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);
+ 	reqctx->dst = src;
+ 
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error) {
+ 			pr_err("AAD copy to destination buffer fails\n");
+ 			return ERR_PTR(error);
+ 		}
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       req->assoclen);
+ 	}
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("CCM:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 	error = aead_ccm_validate_input(op_type, req, aeadctx, sub_type);
+ 	if (error)
+ 		goto err;
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) * 2;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_CCM_SG,
+ 			    T6_MAX_AAD_SIZE - 18,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_CCM_SG) * 8),
+ 			    op_type)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)),  flags);
+ 
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *) __skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	fill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, op_type, ctx);
+ 
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 					16), aeadctx->key, aeadctx->enckey_len);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	error = ccm_format_packet(req, aeadctx, sub_type, op_type);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 				 reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	frags = fill_aead_req_fields(skb, req, src, ivsize, aeadctx);
+ 	atomic_inc(&adap->chcr_stats.aead_rqst);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, 0, 1,
+ 		    sizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ dstmap_fail:
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(error);
+ }
+ 
+ static struct sk_buff *create_gcm_wr(struct aead_request *req,
+ 				     unsigned short qid,
+ 				     int size,
+ 				     unsigned short op_type)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx = ULD_CTX(ctx);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_aead_reqctx  *reqctx = aead_request_ctx(req);
+ 	struct sk_buff *skb = NULL;
+ 	struct chcr_wr *chcr_req;
+ 	struct cpl_rx_phys_dsgl *phys_cpl;
+ 	struct phys_sge_parm sg_param;
+ 	struct scatterlist *src;
+ 	unsigned int frags = 0, transhdr_len;
+ 	unsigned int ivsize = AES_BLOCK_SIZE;
+ 	unsigned int dst_size = 0, kctx_len, assoclen = req->assoclen;
+ 	unsigned char tag_offset = 0;
+ 	unsigned int authsize = crypto_aead_authsize(tfm);
+ 	int error = -EINVAL, src_nent;
+ 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :
+ 		GFP_ATOMIC;
+ 	struct adapter *adap = padap(ctx->dev);
+ 
+ 	/* validate key size */
+ 	if (aeadctx->enckey_len == 0)
+ 		goto err;
+ 
+ 	if (op_type && req->cryptlen < crypto_aead_authsize(tfm))
+ 		goto err;
+ 	src_nent = sg_nents_for_len(req->src, assoclen + req->cryptlen);
+ 	if (src_nent < 0)
+ 		goto err;
+ 
+ 	src = scatterwalk_ffwd(reqctx->srcffwd, req->src, assoclen);
+ 	reqctx->dst = src;
+ 	if (req->src != req->dst) {
+ 		error = chcr_copy_assoc(req, aeadctx);
+ 		if (error)
+ 			return	ERR_PTR(error);
+ 		reqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, req->dst,
+ 					       assoclen);
+ 	}
+ 
+ 
+ 	reqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +
+ 					     (op_type ? -authsize : authsize));
+ 	if (reqctx->dst_nents < 0) {
+ 		pr_err("GCM:Invalid Destination sg entries\n");
+ 		error = -EINVAL;
+ 		goto err;
+ 	}
+ 
+ 
+ 	dst_size = get_space_for_phys_dsgl(reqctx->dst_nents);
+ 	kctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 	transhdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);
+ 	if (chcr_aead_need_fallback(req, src_nent + MIN_GCM_SG,
+ 			    T6_MAX_AAD_SIZE,
+ 			    transhdr_len + (sgl_len(src_nent + MIN_GCM_SG) * 8),
+ 			    op_type)) {
+ 		atomic_inc(&adap->chcr_stats.fallback);
+ 		return ERR_PTR(chcr_aead_fallback(req, op_type));
+ 	}
+ 	skb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);
+ 	if (!skb) {
+ 		error = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+ 	/* NIC driver is going to write the sge hdr. */
+ 	skb_reserve(skb, sizeof(struct sge_opaque_hdr));
+ 
+ 	chcr_req = (struct chcr_wr *)__skb_put(skb, transhdr_len);
+ 	memset(chcr_req, 0, transhdr_len);
+ 
+ 	if (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)
+ 		assoclen = req->assoclen - 8;
+ 
+ 	tag_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;
+ 	chcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(
+ 					ctx->dev->rx_channel_id, 2, (ivsize ?
+ 					(assoclen + 1) : 0));
+ 	chcr_req->sec_cpl.pldlen =
+ 		htonl(assoclen + ivsize + req->cryptlen);
+ 	chcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(
+ 					assoclen ? 1 : 0, assoclen,
+ 					assoclen + ivsize + 1, 0);
+ 		chcr_req->sec_cpl.cipherstop_lo_authinsert =
+ 			FILL_SEC_CPL_AUTHINSERT(0, assoclen + ivsize + 1,
+ 						tag_offset, tag_offset);
+ 		chcr_req->sec_cpl.seqno_numivs =
+ 			FILL_SEC_CPL_SCMD0_SEQNO(op_type, (op_type ==
+ 					CHCR_ENCRYPT_OP) ? 1 : 0,
+ 					CHCR_SCMD_CIPHER_MODE_AES_GCM,
+ 					CHCR_SCMD_AUTH_MODE_GHASH,
+ 					aeadctx->hmac_ctrl, ivsize >> 1);
+ 	chcr_req->sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,
+ 					0, 1, dst_size);
+ 	chcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;
+ 	memcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);
+ 	memcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *
+ 				16), GCM_CTX(aeadctx)->ghash_h, AEAD_H_SIZE);
+ 
+ 	/* prepare a 16 byte iv */
+ 	/* S   A   L  T |  IV | 0x00000001 */
+ 	if (get_aead_subtype(tfm) ==
+ 	    CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) {
+ 		memcpy(reqctx->iv, aeadctx->salt, 4);
+ 		memcpy(reqctx->iv + 4, req->iv, 8);
+ 	} else {
+ 		memcpy(reqctx->iv, req->iv, 12);
+ 	}
+ 	*((unsigned int *)(reqctx->iv + 12)) = htonl(0x01);
+ 
+ 	phys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);
+ 	sg_param.nents = reqctx->dst_nents;
+ 	sg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);
+ 	sg_param.qid = qid;
+ 	error = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,
+ 					  reqctx->dst, &sg_param);
+ 	if (error)
+ 		goto dstmap_fail;
+ 
+ 	skb_set_transport_header(skb, transhdr_len);
+ 	write_sg_to_skb(skb, &frags, req->src, assoclen);
+ 	write_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);
+ 	write_sg_to_skb(skb, &frags, src, req->cryptlen);
+ 	atomic_inc(&adap->chcr_stats.aead_rqst);
+ 	create_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,
+ 			sizeof(struct cpl_rx_phys_dsgl) + dst_size,
+ 			reqctx->verify);
+ 	reqctx->skb = skb;
+ 	skb_get(skb);
+ 	return skb;
+ 
+ dstmap_fail:
+ 	/* ivmap_fail: */
+ 	kfree_skb(skb);
+ err:
+ 	return ERR_PTR(error);
+ }
+ 
+ 
+ 
+ static int chcr_aead_cra_init(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct aead_alg *alg = crypto_aead_alg(tfm);
+ 
+ 	aeadctx->sw_cipher = crypto_alloc_aead(alg->base.cra_name, 0,
+ 					       CRYPTO_ALG_NEED_FALLBACK |
+ 					       CRYPTO_ALG_ASYNC);
+ 	if  (IS_ERR(aeadctx->sw_cipher))
+ 		return PTR_ERR(aeadctx->sw_cipher);
+ 	crypto_aead_set_reqsize(tfm, max(sizeof(struct chcr_aead_reqctx),
+ 				 sizeof(struct aead_request) +
+ 				 crypto_aead_reqsize(aeadctx->sw_cipher)));
+ 	aeadctx->null = crypto_get_default_null_skcipher();
+ 	if (IS_ERR(aeadctx->null))
+ 		return PTR_ERR(aeadctx->null);
+ 	return chcr_device_init(ctx);
+ }
+ 
+ static void chcr_aead_cra_exit(struct crypto_aead *tfm)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 
+ 	crypto_put_default_null_skcipher();
+ 	crypto_free_aead(aeadctx->sw_cipher);
+ }
+ 
+ static int chcr_authenc_null_setauthsize(struct crypto_aead *tfm,
+ 					unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NOP;
+ 	aeadctx->mayverify = VERIFY_HW;
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ static int chcr_authenc_setauthsize(struct crypto_aead *tfm,
+ 				    unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	u32 maxauth = crypto_aead_maxauthsize(tfm);
+ 
+ 	/*SHA1 authsize in ipsec is 12 instead of 10 i.e maxauthsize / 2 is not
+ 	 * true for sha1. authsize == 12 condition should be before
+ 	 * authsize == (maxauth >> 1)
+ 	 */
+ 	if (authsize == ICV_4) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_6) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_10) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_12) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == ICV_14) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == (maxauth >> 1)) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else if (authsize == maxauth) {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 	} else {
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ 
+ static int chcr_gcm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		 aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		 aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_13:
+ 	case ICV_15:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_SW;
+ 		break;
+ 	default:
+ 
+ 		  crypto_tfm_set_flags((struct crypto_tfm *) tfm,
+ 			CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_4106_4309_setauthsize(struct crypto_aead *tfm,
+ 					  unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_setauthsize(struct crypto_aead *tfm,
+ 				unsigned int authsize)
+ {
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 
+ 	switch (authsize) {
+ 	case ICV_4:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_6:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_8:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_10:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_12:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_14:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	case ICV_16:
+ 		aeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;
+ 		aeadctx->mayverify = VERIFY_HW;
+ 		break;
+ 	default:
+ 		crypto_tfm_set_flags((struct crypto_tfm *)tfm,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		return -EINVAL;
+ 	}
+ 	return crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);
+ }
+ 
+ static int chcr_ccm_common_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	unsigned char ck_size, mk_size;
+ 	int key_ctx_size = 0;
+ 
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4)  * 2;
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		mk_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 		mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, mk_size, 0, 0,
+ 						key_ctx_size >> 4);
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 
+ 	return 0;
+ }
+ 
+ static int chcr_aead_ccm_setkey(struct crypto_aead *aead,
+ 				const u8 *key,
+ 				unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	int error;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &
+ 			      CRYPTO_TFM_REQ_MASK);
+ 	error = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (error)
+ 		return error;
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_aead_rfc4309_setkey(struct crypto_aead *aead, const u8 *key,
+ 				    unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	int error;
+ 
+ 	if (keylen < 3) {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		aeadctx->enckey_len = 0;
+ 		return	-EINVAL;
+ 	}
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &
+ 			      CRYPTO_TFM_REQ_MASK);
+ 	error = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (error)
+ 		return error;
+ 	keylen -= 3;
+ 	memcpy(aeadctx->salt, key + keylen, 3);
+ 	return chcr_ccm_common_setkey(aead, key, keylen);
+ }
+ 
+ static int chcr_gcm_setkey(struct crypto_aead *aead, const u8 *key,
+ 			   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(aead);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_gcm_ctx *gctx = GCM_CTX(aeadctx);
+ 	struct crypto_cipher *cipher;
+ 	unsigned int ck_size;
+ 	int ret = 0, key_ctx_size = 0;
+ 
+ 	aeadctx->enckey_len = 0;
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	ret = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &
+ 			      CRYPTO_TFM_RES_MASK);
+ 	if (ret)
+ 		goto out;
+ 
+ 	if (get_aead_subtype(aead) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&
+ 	    keylen > 3) {
+ 		keylen -= 4;  /* nonce/salt is present in the last 4 bytes */
+ 		memcpy(aeadctx->salt, key + keylen, 4);
+ 	}
+ 	if (keylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		crypto_tfm_set_flags((struct crypto_tfm *)aead,
+ 				     CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		pr_err("GCM: Invalid key length %d\n", keylen);
+ 		ret = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	memcpy(aeadctx->key, key, keylen);
+ 	aeadctx->enckey_len = keylen;
+ 	key_ctx_size = sizeof(struct _key_ctx) +
+ 		((DIV_ROUND_UP(keylen, 16)) << 4) +
+ 		AEAD_H_SIZE;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,
+ 						CHCR_KEYCTX_MAC_KEY_SIZE_128,
+ 						0, 0,
+ 						key_ctx_size >> 4);
+ 	/* Calculate the H = CIPH(K, 0 repeated 16 times).
+ 	 * It will go in key context
+ 	 */
+ 	cipher = crypto_alloc_cipher("aes-generic", 0, 0);
+ 	if (IS_ERR(cipher)) {
+ 		aeadctx->enckey_len = 0;
+ 		ret = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	ret = crypto_cipher_setkey(cipher, key, keylen);
+ 	if (ret) {
+ 		aeadctx->enckey_len = 0;
+ 		goto out1;
+ 	}
+ 	memset(gctx->ghash_h, 0, AEAD_H_SIZE);
+ 	crypto_cipher_encrypt_one(cipher, gctx->ghash_h, gctx->ghash_h);
+ 
+ out1:
+ 	crypto_free_cipher(cipher);
+ out:
+ 	return ret;
+ }
+ 
+ static int chcr_authenc_setkey(struct crypto_aead *authenc, const u8 *key,
+ 				   unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	/* it contains auth and cipher key both*/
+ 	struct crypto_authenc_keys keys;
+ 	unsigned int bs;
+ 	unsigned int max_authsize = crypto_aead_alg(authenc)->maxauthsize;
+ 	int err = 0, i, key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 	unsigned char pad[CHCR_HASH_MAX_BLOCK_SIZE_128] = { 0 };
+ 	struct crypto_shash *base_hash = ERR_PTR(-EINVAL);
+ 	struct algo_param param;
+ 	int align;
+ 	u8 *o_ptr = NULL;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 
+ 	if (get_alg_config(&param, max_authsize)) {
+ 		pr_err("chcr : Unsupported digest size\n");
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 
+ 	/* Copy only encryption key. We use authkey to generate h(ipad) and
+ 	 * h(opad) so authkey is not needed again. authkeylen size have the
+ 	 * size of the hash digest size.
+ 	 */
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 			    aeadctx->enckey_len << 3);
+ 
+ 	base_hash  = chcr_alloc_shash(max_authsize);
+ 	if (IS_ERR(base_hash)) {
+ 		pr_err("chcr : Base driver cannot be loaded\n");
+ 		aeadctx->enckey_len = 0;
+ 		return -EINVAL;
+ 	}
+ 	{
+ 		SHASH_DESC_ON_STACK(shash, base_hash);
+ 		shash->tfm = base_hash;
+ 		shash->flags = crypto_shash_get_flags(base_hash);
+ 		bs = crypto_shash_blocksize(base_hash);
+ 		align = KEYCTX_ALIGN_PAD(max_authsize);
+ 		o_ptr =  actx->h_iopad + param.result_size + align;
+ 
+ 		if (keys.authkeylen > bs) {
+ 			err = crypto_shash_digest(shash, keys.authkey,
+ 						  keys.authkeylen,
+ 						  o_ptr);
+ 			if (err) {
+ 				pr_err("chcr : Base driver cannot be loaded\n");
+ 				goto out;
+ 			}
+ 			keys.authkeylen = max_authsize;
+ 		} else
+ 			memcpy(o_ptr, keys.authkey, keys.authkeylen);
+ 
+ 		/* Compute the ipad-digest*/
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= IPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, actx->h_iopad,
+ 					      max_authsize))
+ 			goto out;
+ 		/* Compute the opad-digest */
+ 		memset(pad + keys.authkeylen, 0, bs - keys.authkeylen);
+ 		memcpy(pad, o_ptr, keys.authkeylen);
+ 		for (i = 0; i < bs >> 2; i++)
+ 			*((unsigned int *)pad + i) ^= OPAD_DATA;
+ 
+ 		if (chcr_compute_partial_hash(shash, pad, o_ptr, max_authsize))
+ 			goto out;
+ 
+ 		/* convert the ipad and opad digest to network order */
+ 		chcr_change_order(actx->h_iopad, param.result_size);
+ 		chcr_change_order(o_ptr, param.result_size);
+ 		key_ctx_len = sizeof(struct _key_ctx) +
+ 			((DIV_ROUND_UP(keys.enckeylen, 16)) << 4) +
+ 			(param.result_size + align) * 2;
+ 		aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, param.mk_size,
+ 						0, 1, key_ctx_len >> 4);
+ 		actx->auth_mode = param.auth_mode;
+ 		chcr_free_shash(base_hash);
+ 
+ 		return 0;
+ 	}
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	if (!IS_ERR(base_hash))
+ 		chcr_free_shash(base_hash);
+ 	return -EINVAL;
+ }
+ 
+ static int chcr_aead_digest_null_setkey(struct crypto_aead *authenc,
+ 					const u8 *key, unsigned int keylen)
+ {
+ 	struct chcr_context *ctx = crypto_aead_ctx(authenc);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);
+ 	struct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);
+ 	struct crypto_authenc_keys keys;
+ 	int err;
+ 	/* it contains auth and cipher key both*/
+ 	int key_ctx_len = 0;
+ 	unsigned char ck_size = 0;
+ 
+ 	crypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);
+ 	crypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)
+ 			      & CRYPTO_TFM_REQ_MASK);
+ 	err = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);
+ 	crypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);
+ 	crypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)
+ 			      & CRYPTO_TFM_RES_MASK);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {
+ 		crypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);
+ 		goto out;
+ 	}
+ 	if (keys.enckeylen == AES_KEYSIZE_128) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_192) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;
+ 	} else if (keys.enckeylen == AES_KEYSIZE_256) {
+ 		ck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;
+ 	} else {
+ 		pr_err("chcr : Unsupported cipher key\n");
+ 		goto out;
+ 	}
+ 	memcpy(aeadctx->key, keys.enckey, keys.enckeylen);
+ 	aeadctx->enckey_len = keys.enckeylen;
+ 	get_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,
+ 				    aeadctx->enckey_len << 3);
+ 	key_ctx_len =  sizeof(struct _key_ctx)
+ 		+ ((DIV_ROUND_UP(keys.enckeylen, 16)) << 4);
+ 
+ 	aeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY, 0,
+ 						0, key_ctx_len >> 4);
+ 	actx->auth_mode = CHCR_SCMD_AUTH_MODE_NOP;
+ 	return 0;
+ out:
+ 	aeadctx->enckey_len = 0;
+ 	return -EINVAL;
+ }
+ static int chcr_aead_encrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 
+ 	reqctx->verify = VERIFY_HW;
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_decrypt(struct aead_request *req)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));
+ 	struct chcr_aead_reqctx *reqctx = aead_request_ctx(req);
+ 	int size;
+ 
+ 	if (aeadctx->mayverify == VERIFY_SW) {
+ 		size = crypto_aead_maxauthsize(tfm);
+ 		reqctx->verify = VERIFY_SW;
+ 	} else {
+ 		size = 0;
+ 		reqctx->verify = VERIFY_HW;
+ 	}
+ 
+ 	switch (get_aead_subtype(tfm)) {
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_NULL:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_authenc_wr);
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_CCM:
+ 	case CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_aead_ccm_wr);
+ 	default:
+ 		return chcr_aead_op(req, CHCR_DECRYPT_OP, size,
+ 				    create_gcm_wr);
+ 	}
+ }
+ 
+ static int chcr_aead_op(struct aead_request *req,
+ 			  unsigned short op_type,
+ 			  int size,
+ 			  create_wr_t create_wr_fn)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	struct chcr_context *ctx = crypto_aead_ctx(tfm);
+ 	struct uld_ctx *u_ctx;
+ 	struct sk_buff *skb;
+ 
+ 	if (!ctx->dev) {
+ 		pr_err("chcr : %s : No crypto device.\n", __func__);
+ 		return -ENXIO;
+ 	}
+ 	u_ctx = ULD_CTX(ctx);
+ 	if (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],
+ 				   ctx->tx_qidx)) {
+ 		if (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))
+ 			return -EBUSY;
+ 	}
+ 
+ 	/* Form a WR from req */
+ 	skb = create_wr_fn(req, u_ctx->lldi.rxq_ids[ctx->rx_qidx], size,
+ 			   op_type);
+ 
+ 	if (IS_ERR(skb) || !skb)
+ 		return PTR_ERR(skb);
+ 
+ 	skb->dev = u_ctx->lldi.ports[0];
+ 	set_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);
+ 	chcr_send_wr(skb);
+ 	return -EINPROGRESS;
+ }
++>>>>>>> ee0863ba118d (chcr - Add debug counters)
  static struct chcr_alg_template driver_algs[] = {
  	/* AES-CBC */
  	{
diff --cc drivers/crypto/chelsio/chcr_core.c
index 91853c46c047,5ae659af6a90..000000000000
--- a/drivers/crypto/chelsio/chcr_core.c
+++ b/drivers/crypto/chelsio/chcr_core.c
@@@ -110,7 -111,8 +111,12 @@@ static int cpl_fw6_pld_handler(struct c
  	if (ack_err_status) {
  		if (CHK_MAC_ERR_BIT(ack_err_status) ||
  		    CHK_PAD_ERR_BIT(ack_err_status))
++<<<<<<< HEAD
 +			error_status = -EINVAL;
++=======
+ 			error_status = -EBADMSG;
+ 		atomic_inc(&adap->chcr_stats.error);
++>>>>>>> ee0863ba118d (chcr - Add debug counters)
  	}
  	/* call completion callback with failure status */
  	if (req) {
* Unmerged path drivers/crypto/chelsio/chcr_algo.c
* Unmerged path drivers/crypto/chelsio/chcr_core.c
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
index 696bf43a44ea..17e8296dd132 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4.h
@@ -880,6 +880,7 @@ struct adapter {
 
 	/* TC u32 offload */
 	struct cxgb4_tc_u32_table *tc_u32;
+	struct chcr_stats_debug chcr_stats;
 };
 
 /* Support for "sched-class" command to allow a TX Scheduling Class to be
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
index bc7c596dc8e9..25b5c2f5f2f6 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_debugfs.c
@@ -3086,6 +3086,40 @@ static const struct file_operations meminfo_fops = {
 	.llseek  = seq_lseek,
 	.release = single_release,
 };
+
+static int chcr_show(struct seq_file *seq, void *v)
+{
+	struct adapter *adap = seq->private;
+
+	seq_puts(seq, "Chelsio Crypto Accelerator Stats \n");
+	seq_printf(seq, "Cipher Ops: %10u \n",
+		   atomic_read(&adap->chcr_stats.cipher_rqst));
+	seq_printf(seq, "Digest Ops: %10u \n",
+		   atomic_read(&adap->chcr_stats.digest_rqst));
+	seq_printf(seq, "Aead Ops: %10u \n",
+		   atomic_read(&adap->chcr_stats.aead_rqst));
+	seq_printf(seq, "Completion: %10u \n",
+		   atomic_read(&adap->chcr_stats.complete));
+	seq_printf(seq, "Error: %10u \n",
+		   atomic_read(&adap->chcr_stats.error));
+	seq_printf(seq, "Fallback: %10u \n",
+		   atomic_read(&adap->chcr_stats.fallback));
+	return 0;
+}
+
+
+static int chcr_stats_open(struct inode *inode, struct file *file)
+{
+        return single_open(file, chcr_show, inode->i_private);
+}
+
+static const struct file_operations chcr_stats_debugfs_fops = {
+        .owner   = THIS_MODULE,
+        .open    = chcr_stats_open,
+        .read    = seq_read,
+        .llseek  = seq_lseek,
+        .release = single_release,
+};
 /* Add an array of Debug FS files.
  */
 void add_debugfs_files(struct adapter *adap,
@@ -3160,6 +3194,7 @@ int t4_setup_debugfs(struct adapter *adap)
 		{ "tids", &tid_info_debugfs_fops, S_IRUSR, 0},
 		{ "blocked_fl", &blocked_fl_fops, S_IRUSR | S_IWUSR, 0 },
 		{ "meminfo", &meminfo_fops, S_IRUSR, 0 },
+		{ "crypto", &chcr_stats_debugfs_fops, S_IRUSR, 0 },
 	};
 
 	/* Debug FS nodes common to all T5 and later adapters.
diff --git a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
index ce0d9fbf0648..94a85010e78a 100644
--- a/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
+++ b/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.h
@@ -285,6 +285,15 @@ struct cxgb4_virt_res {                      /* virtualized HW resources */
 	unsigned int ncrypto_fc;
 };
 
+struct chcr_stats_debug {
+	atomic_t cipher_rqst;
+	atomic_t digest_rqst;
+	atomic_t aead_rqst;
+	atomic_t complete;
+	atomic_t error;
+	atomic_t fallback;
+};
+
 #define OCQ_WIN_OFFSET(pdev, vres) \
 	(pci_resource_len((pdev), 2) - roundup_pow_of_two((vres)->ocq.size))
 
