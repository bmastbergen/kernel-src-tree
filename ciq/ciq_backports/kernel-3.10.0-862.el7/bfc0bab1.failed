scsi: cxlflash: Support multiple hardware queues

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] cxlflash: Support multiple hardware queues (Gustavo Duarte) [1456494]
Rebuild_FUZZ: 93.33%
commit-author Uma Krishnan <ukrishn@linux.vnet.ibm.com>
commit bfc0bab172cabf3bb25c48c4c521b317ff4a909d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/bfc0bab1.failed

Introduce multiple hardware queues to improve legacy I/O path performance.
Each hardware queue is comprised of a master context and associated I/O
resources. The hardware queues are initially implemented as a static array
embedded in the AFU. This will be transitioned to a dynamic allocation in a
later series to improve the memory footprint of the driver.

	Signed-off-by: Uma Krishnan <ukrishn@linux.vnet.ibm.com>
	Acked-by: Matthew R. Ochs <mrochs@linux.vnet.ibm.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit bfc0bab172cabf3bb25c48c4c521b317ff4a909d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/cxlflash/common.h
#	drivers/scsi/cxlflash/main.c
#	drivers/scsi/cxlflash/superpipe.c
diff --cc drivers/scsi/cxlflash/common.h
index 811927d91c5c,b5858ae1deae..000000000000
--- a/drivers/scsi/cxlflash/common.h
+++ b/drivers/scsi/cxlflash/common.h
@@@ -53,6 -57,12 +53,15 @@@ extern const struct file_operations cxl
  /* RRQ for master issued cmds */
  #define NUM_RRQ_ENTRY                   CXLFLASH_MAX_CMDS
  
++<<<<<<< HEAD
++=======
+ /* SQ for master issued cmds */
+ #define NUM_SQ_ENTRY			CXLFLASH_MAX_CMDS
+ 
+ #define CXLFLASH_NUM_HWQS		1
+ #define PRIMARY_HWQ			0
+ 
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  
  static inline void check_sizes(void)
  {
@@@ -132,13 -140,13 +140,14 @@@ struct cxlflash_cfg 
  struct afu_cmd {
  	struct sisl_ioarcb rcb;	/* IOARCB (cache line aligned) */
  	struct sisl_ioasa sa;	/* IOASA must follow IOARCB */
 -	struct afu *parent;
 -	struct scsi_cmnd *scp;
 +	spinlock_t slock;
  	struct completion cevent;
 -	struct list_head queue;
 +	struct afu *parent;
 +	int slot;
 +	atomic_t free;
  
  	u8 cmd_tmf:1;
+ 	u32 hwq_index;
  
  	/* As per the SISLITE spec the IOARCB EA has to be 16-byte aligned.
  	 * However for performance reasons the IOARCB/IOASA should be
@@@ -146,43 -154,100 +155,126 @@@
  	 */
  } __aligned(cache_line_size());
  
++<<<<<<< HEAD
 +struct afu {
++=======
+ static inline struct afu_cmd *sc_to_afuc(struct scsi_cmnd *sc)
+ {
+ 	return PTR_ALIGN(scsi_cmd_priv(sc), __alignof__(struct afu_cmd));
+ }
+ 
+ static inline struct afu_cmd *sc_to_afucz(struct scsi_cmnd *sc)
+ {
+ 	struct afu_cmd *afuc = sc_to_afuc(sc);
+ 
+ 	memset(afuc, 0, sizeof(*afuc));
+ 	return afuc;
+ }
+ 
+ struct hwq {
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	/* Stuff requiring alignment go first. */
 -	struct sisl_ioarcb sq[NUM_SQ_ENTRY];		/* 16K SQ */
 -	u64 rrq_entry[NUM_RRQ_ENTRY];			/* 2K RRQ */
 +
 +	u64 rrq_entry[NUM_RRQ_ENTRY];	/* 2K RRQ */
 +	/*
 +	 * Command & data for AFU commands.
 +	 */
 +	struct afu_cmd cmd[CXLFLASH_NUM_CMDS];
  
  	/* Beware of alignment till here. Preferably introduce new
  	 * fields after this point
  	 */
++<<<<<<< HEAD
 +
 +	/* AFU HW */
++=======
+ 	struct afu *afu;
+ 	struct cxl_context *ctx;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	struct cxl_ioctl_start_work work;
- 	struct cxlflash_afu_map __iomem *afu_map;	/* entire MMIO map */
  	struct sisl_host_map __iomem *host_map;		/* MC host map */
  	struct sisl_ctrl_map __iomem *ctrl_map;		/* MC control map */
- 
  	ctx_hndl_t ctx_hndl;	/* master's context handle */
++<<<<<<< HEAD
++=======
+ 	u32 index;		/* Index of this hwq */
+ 
+ 	atomic_t hsq_credits;
+ 	spinlock_t hsq_slock;
+ 	struct sisl_ioarcb *hsq_start;
+ 	struct sisl_ioarcb *hsq_end;
+ 	struct sisl_ioarcb *hsq_curr;
+ 	spinlock_t hrrq_slock;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	u64 *hrrq_start;
  	u64 *hrrq_end;
  	u64 *hrrq_curr;
  	bool toggle;
++<<<<<<< HEAD
++=======
+ 
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	s64 room;
  	spinlock_t rrin_slock; /* Lock to rrin queuing and cmd_room updates */
+ 
+ 	struct irq_poll irqpoll;
+ } __aligned(cache_line_size());
+ 
+ struct afu {
+ 	struct hwq hwqs[CXLFLASH_NUM_HWQS];
+ 	int (*send_cmd)(struct afu *, struct afu_cmd *);
+ 	void (*context_reset)(struct afu_cmd *);
+ 
+ 	/* AFU HW */
+ 	struct cxlflash_afu_map __iomem *afu_map;	/* entire MMIO map */
+ 
+ 	atomic_t cmds_active;	/* Number of currently active AFU commands */
  	u64 hb;
 +	u32 cmd_couts;		/* Number of command checkouts */
  	u32 internal_lun;	/* User-desired LUN mode for this AFU */
  
  	char version[16];
  	u64 interface_version;
  
++<<<<<<< HEAD
++=======
+ 	u32 irqpoll_weight;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	struct cxlflash_cfg *parent; /* Pointer back to parent cxlflash_cfg */
- 
  };
  
++<<<<<<< HEAD
++=======
+ static inline struct hwq *get_hwq(struct afu *afu, u32 index)
+ {
+ 	WARN_ON(index >= CXLFLASH_NUM_HWQS);
+ 
+ 	return &afu->hwqs[index];
+ }
+ 
+ static inline bool afu_is_irqpoll_enabled(struct afu *afu)
+ {
+ 	return !!afu->irqpoll_weight;
+ }
+ 
+ static inline bool afu_is_cmd_mode(struct afu *afu, u64 cmd_mode)
+ {
+ 	u64 afu_cap = afu->interface_version >> SISL_INTVER_CAP_SHIFT;
+ 
+ 	return afu_cap & cmd_mode;
+ }
+ 
+ static inline bool afu_is_sq_cmd_mode(struct afu *afu)
+ {
+ 	return afu_is_cmd_mode(afu, SISL_INTVER_CAP_SQ_CMD_MODE);
+ }
+ 
+ static inline bool afu_is_ioarrin_cmd_mode(struct afu *afu)
+ {
+ 	return afu_is_cmd_mode(afu, SISL_INTVER_CAP_IOARRIN_CMD_MODE);
+ }
+ 
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  static inline u64 lun_to_lunid(u64 lun)
  {
  	__be64 lun_id;
diff --cc drivers/scsi/cxlflash/main.c
index c68badcfa77f,5d068696eee4..000000000000
--- a/drivers/scsi/cxlflash/main.c
+++ b/drivers/scsi/cxlflash/main.c
@@@ -294,7 -217,31 +294,35 @@@ static void context_reset(struct afu_cm
  }
  
  /**
++<<<<<<< HEAD
 + * send_cmd() - sends an AFU command
++=======
+  * context_reset_ioarrin() - reset command owner context via IOARRIN register
+  * @cmd:	AFU command that timed out.
+  */
+ static void context_reset_ioarrin(struct afu_cmd *cmd)
+ {
+ 	struct afu *afu = cmd->parent;
+ 	struct hwq *hwq = get_hwq(afu, cmd->hwq_index);
+ 
+ 	context_reset(cmd, &hwq->host_map->ioarrin);
+ }
+ 
+ /**
+  * context_reset_sq() - reset command owner context w/ SQ Context Reset register
+  * @cmd:	AFU command that timed out.
+  */
+ static void context_reset_sq(struct afu_cmd *cmd)
+ {
+ 	struct afu *afu = cmd->parent;
+ 	struct hwq *hwq = get_hwq(afu, cmd->hwq_index);
+ 
+ 	context_reset(cmd, &hwq->host_map->sq_ctx_reset);
+ }
+ 
+ /**
+  * send_cmd_ioarrin() - sends an AFU command via IOARRIN register
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
   * @afu:	AFU associated with the host.
   * @cmd:	AFU command to send.
   *
@@@ -324,14 -272,58 +353,64 @@@ static int send_cmd(struct afu *afu, st
  			rc = SCSI_MLQUEUE_HOST_BUSY;
  			goto out;
  		}
- 		afu->room = room - 1;
+ 		hwq->room = room - 1;
  	}
  
- 	writeq_be((u64)&cmd->rcb, &afu->host_map->ioarrin);
+ 	writeq_be((u64)&cmd->rcb, &hwq->host_map->ioarrin);
  out:
++<<<<<<< HEAD
 +	spin_unlock_irqrestore(&afu->rrin_slock, lock_flags);
 +	pr_devel("%s: cmd=%p len=%d ea=%p rc=%d\n", __func__, cmd,
 +		 cmd->rcb.data_len, (void *)cmd->rcb.data_ea, rc);
++=======
+ 	spin_unlock_irqrestore(&hwq->rrin_slock, lock_flags);
+ 	dev_dbg(dev, "%s: cmd=%p len=%u ea=%016llx rc=%d\n", __func__,
+ 		cmd, cmd->rcb.data_len, cmd->rcb.data_ea, rc);
+ 	return rc;
+ }
+ 
+ /**
+  * send_cmd_sq() - sends an AFU command via SQ ring
+  * @afu:	AFU associated with the host.
+  * @cmd:	AFU command to send.
+  *
+  * Return:
+  *	0 on success, SCSI_MLQUEUE_HOST_BUSY on failure
+  */
+ static int send_cmd_sq(struct afu *afu, struct afu_cmd *cmd)
+ {
+ 	struct cxlflash_cfg *cfg = afu->parent;
+ 	struct device *dev = &cfg->dev->dev;
+ 	struct hwq *hwq = get_hwq(afu, cmd->hwq_index);
+ 	int rc = 0;
+ 	int newval;
+ 	ulong lock_flags;
+ 
+ 	newval = atomic_dec_if_positive(&hwq->hsq_credits);
+ 	if (newval <= 0) {
+ 		rc = SCSI_MLQUEUE_HOST_BUSY;
+ 		goto out;
+ 	}
+ 
+ 	cmd->rcb.ioasa = &cmd->sa;
+ 
+ 	spin_lock_irqsave(&hwq->hsq_slock, lock_flags);
+ 
+ 	*hwq->hsq_curr = cmd->rcb;
+ 	if (hwq->hsq_curr < hwq->hsq_end)
+ 		hwq->hsq_curr++;
+ 	else
+ 		hwq->hsq_curr = hwq->hsq_start;
+ 	writeq_be((u64)hwq->hsq_curr, &hwq->host_map->sq_tail);
+ 
+ 	spin_unlock_irqrestore(&hwq->hsq_slock, lock_flags);
+ out:
+ 	dev_dbg(dev, "%s: cmd=%p len=%u ea=%016llx ioasa=%p rc=%d curr=%p "
+ 	       "head=%016llx tail=%016llx\n", __func__, cmd, cmd->rcb.data_len,
+ 	       cmd->rcb.data_ea, cmd->rcb.ioasa, rc, hwq->hsq_curr,
+ 	       readq_be(&hwq->host_map->sq_head),
+ 	       readq_be(&hwq->host_map->sq_tail));
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	return rc;
  }
  
@@@ -366,13 -368,10 +445,14 @@@ static void wait_resp(struct afu *afu, 
   */
  static int send_tmf(struct afu *afu, struct scsi_cmnd *scp, u64 tmfcmd)
  {
 -	struct cxlflash_cfg *cfg = shost_priv(scp->device->host);
 -	struct afu_cmd *cmd = sc_to_afucz(scp);
 +	struct afu_cmd *cmd;
 +
 +	u32 port_sel = scp->device->channel + 1;
 +	short lflag = 0;
 +	struct Scsi_Host *host = scp->device->host;
 +	struct cxlflash_cfg *cfg = (struct cxlflash_cfg *)host->hostdata;
  	struct device *dev = &cfg->dev->dev;
+ 	struct hwq *hwq = get_hwq(afu, PRIMARY_HWQ);
  	ulong lock_flags;
  	int rc = 0;
  	ulong to;
@@@ -391,28 -383,24 +471,39 @@@
  						  !cfg->tmf_active,
  						  cfg->tmf_slock);
  	cfg->tmf_active = true;
 +	cmd->cmd_tmf = true;
  	spin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);
  
++<<<<<<< HEAD
 +	cmd->rcb.ctx_id = afu->ctx_hndl;
 +	cmd->rcb.port_sel = port_sel;
++=======
+ 	cmd->scp = scp;
+ 	cmd->parent = afu;
+ 	cmd->cmd_tmf = true;
+ 	cmd->hwq_index = hwq->index;
+ 
+ 	cmd->rcb.ctx_id = hwq->ctx_hndl;
+ 	cmd->rcb.msi = SISL_MSI_RRQ_UPDATED;
+ 	cmd->rcb.port_sel = CHAN2PORTMASK(scp->device->channel);
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	cmd->rcb.lun_id = lun_to_lunid(scp->device->lun);
 +
 +	lflag = SISL_REQ_FLAGS_TMF_CMD;
 +
  	cmd->rcb.req_flags = (SISL_REQ_FLAGS_PORT_LUN_ID |
 -			      SISL_REQ_FLAGS_SUP_UNDERRUN |
 -			      SISL_REQ_FLAGS_TMF_CMD);
 +			      SISL_REQ_FLAGS_SUP_UNDERRUN | lflag);
 +
 +	/* Stash the scp in the reserved field, for reuse during interrupt */
 +	cmd->rcb.scp = scp;
 +
 +	/* Copy the CDB from the cmd passed in */
  	memcpy(cmd->rcb.cdb, &tmfcmd, sizeof(tmfcmd));
  
 -	rc = afu->send_cmd(afu, cmd);
 +	/* Send the command */
 +	rc = send_cmd(afu, cmd);
  	if (unlikely(rc)) {
 +		cmd_checkin(cmd);
  		spin_lock_irqsave(&cfg->tmf_slock, lock_flags);
  		cfg->tmf_active = false;
  		spin_unlock_irqrestore(&cfg->tmf_slock, lock_flags);
@@@ -455,19 -443,18 +546,26 @@@ static const char *cxlflash_driver_info
   */
  static int cxlflash_queuecommand(struct Scsi_Host *host, struct scsi_cmnd *scp)
  {
 -	struct cxlflash_cfg *cfg = shost_priv(host);
 +	struct cxlflash_cfg *cfg = (struct cxlflash_cfg *)host->hostdata;
  	struct afu *afu = cfg->afu;
  	struct device *dev = &cfg->dev->dev;
++<<<<<<< HEAD
 +	struct afu_cmd *cmd;
 +	u32 port_sel = scp->device->channel + 1;
 +	int nseg, i, ncount;
 +	struct scatterlist *sg;
++=======
+ 	struct afu_cmd *cmd = sc_to_afucz(scp);
+ 	struct scatterlist *sg = scsi_sglist(scp);
+ 	struct hwq *hwq = get_hwq(afu, PRIMARY_HWQ);
+ 	u16 req_flags = SISL_REQ_FLAGS_SUP_UNDERRUN;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	ulong lock_flags;
 +	short lflag = 0;
  	int rc = 0;
  
 -	dev_dbg_ratelimited(dev, "%s: (scp=%p) %d/%d/%d/%llu "
 -			    "cdb=(%08x-%08x-%08x-%08x)\n",
 +	dev_dbg_ratelimited(dev, "%s: (scp=%p) %d/%d/%d/%d "
 +			    "cdb=(%08X-%08X-%08X-%08X)\n",
  			    __func__, scp, host->host_no, scp->device->channel,
  			    scp->device->id, scp->device->lun,
  			    get_unaligned_be32(&((u32 *)scp->cmnd)[0]),
@@@ -502,15 -491,18 +600,25 @@@
  		break;
  	}
  
 -	if (likely(sg)) {
 -		cmd->rcb.data_len = sg->length;
 -		cmd->rcb.data_ea = (uintptr_t)sg_virt(sg);
 +	cmd = cmd_checkout(afu);
 +	if (unlikely(!cmd)) {
 +		dev_err(dev, "%s: could not get a free command\n", __func__);
 +		rc = SCSI_MLQUEUE_HOST_BUSY;
 +		goto out;
  	}
  
++<<<<<<< HEAD
 +	cmd->rcb.ctx_id = afu->ctx_hndl;
 +	cmd->rcb.port_sel = port_sel;
++=======
+ 	cmd->scp = scp;
+ 	cmd->parent = afu;
+ 	cmd->hwq_index = hwq->index;
+ 
+ 	cmd->rcb.ctx_id = hwq->ctx_hndl;
+ 	cmd->rcb.msi = SISL_MSI_RRQ_UPDATED;
+ 	cmd->rcb.port_sel = CHAN2PORTMASK(scp->device->channel);
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	cmd->rcb.lun_id = lun_to_lunid(scp->device->lun);
  
  	if (scp->sc_data_direction == DMA_TO_DEVICE)
@@@ -599,18 -555,22 +707,35 @@@ static void free_mem(struct cxlflash_cf
   */
  static void stop_afu(struct cxlflash_cfg *cfg)
  {
 +	int i;
  	struct afu *afu = cfg->afu;
++<<<<<<< HEAD
 +	struct afu_cmd *cmd;
++=======
+ 	struct hwq *hwq;
+ 	int i;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  
  	cancel_work_sync(&cfg->work_q);
  
  	if (likely(afu)) {
++<<<<<<< HEAD
 +		for (i = 0; i < CXLFLASH_NUM_CMDS; i++) {
 +			cmd = &afu->cmd[i];
 +			complete(&cmd->cevent);
 +			if (!atomic_read(&cmd->free))
 +				cmd_checkin(cmd);
++=======
+ 		while (atomic_read(&afu->cmds_active))
+ 			ssleep(1);
+ 
+ 		if (afu_is_irqpoll_enabled(afu)) {
+ 			for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 				hwq = get_hwq(afu, i);
+ 
+ 				irq_poll_disable(&hwq->irqpoll);
+ 			}
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  		}
  
  		if (likely(afu->afu_map)) {
@@@ -684,6 -664,9 +829,12 @@@ static void term_mc(struct cxlflash_cf
   */
  static void term_afu(struct cxlflash_cfg *cfg)
  {
++<<<<<<< HEAD
++=======
+ 	struct device *dev = &cfg->dev->dev;
+ 	int k;
+ 
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	/*
  	 * Tear down is carefully orchestrated to ensure
  	 * no interrupts can come in when the problem state
@@@ -697,9 -682,10 +850,10 @@@
  	if (cfg->afu)
  		stop_afu(cfg);
  
- 	term_mc(cfg);
+ 	for (k = CXLFLASH_NUM_HWQS - 1; k >= 0; k--)
+ 		term_mc(cfg, k);
  
 -	dev_dbg(dev, "%s: returning\n", __func__);
 +	pr_debug("%s: returning\n", __func__);
  }
  
  /**
@@@ -1126,7 -1064,10 +1280,8 @@@ static const struct asyc_intr_info *fin
   */
  static void afu_err_intr_init(struct afu *afu)
  {
 -	struct cxlflash_cfg *cfg = afu->parent;
 -	__be64 __iomem *fc_port_regs;
  	int i;
+ 	struct hwq *hwq = get_hwq(afu, PRIMARY_HWQ);
  	u64 reg;
  
  	/* global async interrupts: AFU clears afu_ctrl on context exit
@@@ -1186,7 -1133,9 +1345,13 @@@
   */
  static irqreturn_t cxlflash_sync_err_irq(int irq, void *data)
  {
++<<<<<<< HEAD
 +	struct afu *afu = (struct afu *)data;
++=======
+ 	struct hwq *hwq = (struct hwq *)data;
+ 	struct cxlflash_cfg *cfg = hwq->afu->parent;
+ 	struct device *dev = &cfg->dev->dev;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	u64 reg;
  	u64 reg_unmasked;
  
@@@ -1199,34 -1148,39 +1364,45 @@@
  		goto cxlflash_sync_err_irq_exit;
  	}
  
 -	dev_err(dev, "%s: unexpected interrupt, intr_status=%016llx\n",
 -		__func__, reg);
 +	pr_err("%s: %llX: unexpected interrupt, intr_status %016llX\n",
 +	       __func__, (u64)afu, reg);
  
- 	writeq_be(reg_unmasked, &afu->host_map->intr_clear);
+ 	writeq_be(reg_unmasked, &hwq->host_map->intr_clear);
  
  cxlflash_sync_err_irq_exit:
 +	pr_debug("%s: returning rc=%d\n", __func__, IRQ_HANDLED);
  	return IRQ_HANDLED;
  }
  
  /**
 - * process_hrrq() - process the read-response queue
 - * @afu:	AFU associated with the host.
 - * @doneq:	Queue of commands harvested from the RRQ.
 - * @budget:	Threshold of RRQ entries to process.
 - *
 - * This routine must be called holding the disabled RRQ spin lock.
 + * cxlflash_rrq_irq() - interrupt handler for read-response queue (normal path)
 + * @irq:	Interrupt number.
 + * @data:	Private data provided at interrupt registration, the AFU.
   *
 - * Return: The number of entries processed.
 + * Return: Always return IRQ_HANDLED.
   */
++<<<<<<< HEAD
 +static irqreturn_t cxlflash_rrq_irq(int irq, void *data)
 +{
 +	struct afu *afu = (struct afu *)data;
 +	struct afu_cmd *cmd;
 +	bool toggle = afu->toggle;
++=======
+ static int process_hrrq(struct hwq *hwq, struct list_head *doneq, int budget)
+ {
+ 	struct afu *afu = hwq->afu;
+ 	struct afu_cmd *cmd;
+ 	struct sisl_ioasa *ioasa;
+ 	struct sisl_ioarcb *ioarcb;
+ 	bool toggle = hwq->toggle;
+ 	int num_hrrq = 0;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	u64 entry,
- 	    *hrrq_start = afu->hrrq_start,
- 	    *hrrq_end = afu->hrrq_end,
- 	    *hrrq_curr = afu->hrrq_curr;
+ 	    *hrrq_start = hwq->hrrq_start,
+ 	    *hrrq_end = hwq->hrrq_end,
+ 	    *hrrq_curr = hwq->hrrq_curr;
  
 -	/* Process ready RRQ entries up to the specified budget (if any) */
 +	/* Process however many RRQ entries that are ready */
  	while (true) {
  		entry = *hrrq_curr;
  
@@@ -1243,11 -1206,92 +1419,98 @@@
  			hrrq_curr = hrrq_start;
  			toggle ^= SISL_RESP_HANDLE_T_BIT;
  		}
++<<<<<<< HEAD
++=======
+ 
+ 		atomic_inc(&hwq->hsq_credits);
+ 		num_hrrq++;
+ 
+ 		if (budget > 0 && num_hrrq >= budget)
+ 			break;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
+ 	}
+ 
+ 	hwq->hrrq_curr = hrrq_curr;
+ 	hwq->toggle = toggle;
+ 
++<<<<<<< HEAD
++=======
+ 	return num_hrrq;
+ }
+ 
+ /**
+  * process_cmd_doneq() - process a queue of harvested RRQ commands
+  * @doneq:	Queue of completed commands.
+  *
+  * Note that upon return the queue can no longer be trusted.
+  */
+ static void process_cmd_doneq(struct list_head *doneq)
+ {
+ 	struct afu_cmd *cmd, *tmp;
+ 
+ 	WARN_ON(list_empty(doneq));
+ 
+ 	list_for_each_entry_safe(cmd, tmp, doneq, queue)
+ 		cmd_complete(cmd);
+ }
+ 
+ /**
+  * cxlflash_irqpoll() - process a queue of harvested RRQ commands
+  * @irqpoll:	IRQ poll structure associated with queue to poll.
+  * @budget:	Threshold of RRQ entries to process per poll.
+  *
+  * Return: The number of entries processed.
+  */
+ static int cxlflash_irqpoll(struct irq_poll *irqpoll, int budget)
+ {
+ 	struct hwq *hwq = container_of(irqpoll, struct hwq, irqpoll);
+ 	unsigned long hrrq_flags;
+ 	LIST_HEAD(doneq);
+ 	int num_entries = 0;
+ 
+ 	spin_lock_irqsave(&hwq->hrrq_slock, hrrq_flags);
+ 
+ 	num_entries = process_hrrq(hwq, &doneq, budget);
+ 	if (num_entries < budget)
+ 		irq_poll_complete(irqpoll);
+ 
+ 	spin_unlock_irqrestore(&hwq->hrrq_slock, hrrq_flags);
+ 
+ 	process_cmd_doneq(&doneq);
+ 	return num_entries;
+ }
+ 
+ /**
+  * cxlflash_rrq_irq() - interrupt handler for read-response queue (normal path)
+  * @irq:	Interrupt number.
+  * @data:	Private data provided at interrupt registration, the AFU.
+  *
+  * Return: IRQ_HANDLED or IRQ_NONE when no ready entries found.
+  */
+ static irqreturn_t cxlflash_rrq_irq(int irq, void *data)
+ {
+ 	struct hwq *hwq = (struct hwq *)data;
+ 	struct afu *afu = hwq->afu;
+ 	unsigned long hrrq_flags;
+ 	LIST_HEAD(doneq);
+ 	int num_entries = 0;
+ 
+ 	spin_lock_irqsave(&hwq->hrrq_slock, hrrq_flags);
+ 
+ 	if (afu_is_irqpoll_enabled(afu)) {
+ 		irq_poll_sched(&hwq->irqpoll);
+ 		spin_unlock_irqrestore(&hwq->hrrq_slock, hrrq_flags);
+ 		return IRQ_HANDLED;
  	}
  
- 	afu->hrrq_curr = hrrq_curr;
- 	afu->toggle = toggle;
+ 	num_entries = process_hrrq(hwq, &doneq, -1);
+ 	spin_unlock_irqrestore(&hwq->hrrq_slock, hrrq_flags);
+ 
+ 	if (num_entries == 0)
+ 		return IRQ_NONE;
  
+ 	process_cmd_doneq(&doneq);
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	return IRQ_HANDLED;
  }
  
@@@ -1260,15 -1333,17 +1523,16 @@@
   */
  static irqreturn_t cxlflash_async_err_irq(int irq, void *data)
  {
- 	struct afu *afu = (struct afu *)data;
+ 	struct hwq *hwq = (struct hwq *)data;
+ 	struct afu *afu = hwq->afu;
  	struct cxlflash_cfg *cfg = afu->parent;
  	struct device *dev = &cfg->dev->dev;
 +	u64 reg_unmasked;
  	const struct asyc_intr_info *info;
  	struct sisl_global_map __iomem *global = &afu->afu_map->global;
 -	__be64 __iomem *fc_port_regs;
 -	u64 reg_unmasked;
  	u64 reg;
 -	u64 bit;
  	u8 port;
 +	int i;
  
  	reg = readq_be(&global->regs.aintr_status);
  	reg_unmasked = (reg & SISL_ASTATUS_UNMASK);
@@@ -1338,15 -1421,17 +1603,20 @@@ out
   *
   * Return: A success or failure value from CXL services.
   */
- static int start_context(struct cxlflash_cfg *cfg)
+ static int start_context(struct cxlflash_cfg *cfg, u32 index)
  {
++<<<<<<< HEAD
++=======
+ 	struct device *dev = &cfg->dev->dev;
+ 	struct hwq *hwq = get_hwq(cfg->afu, index);
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	int rc = 0;
  
- 	rc = cxl_start_context(cfg->mcctx,
- 			       cfg->afu->work.work_element_descriptor,
+ 	rc = cxl_start_context(hwq->ctx,
+ 			       hwq->work.work_element_descriptor,
  			       NULL);
  
 -	dev_dbg(dev, "%s: returning rc=%d\n", __func__, rc);
 +	pr_debug("%s: returning rc=%d\n", __func__, rc);
  	return rc;
  }
  
@@@ -1463,19 -1550,16 +1734,27 @@@ static void init_pcr(struct cxlflash_cf
  		writeq_be(0, &ctrl_map->ctx_cap);
  	}
  
- 	/* Copy frequently used fields into afu */
- 	afu->ctx_hndl = (u16) cxl_process_element(cfg->mcctx);
- 	afu->host_map = &afu->afu_map->hosts[afu->ctx_hndl].host;
- 	afu->ctrl_map = &afu->afu_map->ctrls[afu->ctx_hndl].ctrl;
+ 	/* Copy frequently used fields into hwq */
+ 	for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 		hwq = get_hwq(afu, i);
  
++<<<<<<< HEAD
 +	/* Program the Endian Control for the master context */
 +	writeq_be(SISL_ENDIAN_CTRL, &afu->host_map->endian_ctrl);
 +
 +	/* Initialize cmd fields that never change */
 +	for (i = 0; i < CXLFLASH_NUM_CMDS; i++) {
 +		afu->cmd[i].rcb.ctx_id = afu->ctx_hndl;
 +		afu->cmd[i].rcb.msi = SISL_MSI_RRQ_UPDATED;
 +		afu->cmd[i].rcb.rrq = 0x0;
++=======
+ 		hwq->ctx_hndl = (u16) cxl_process_element(hwq->ctx);
+ 		hwq->host_map = &afu->afu_map->hosts[hwq->ctx_hndl].host;
+ 		hwq->ctrl_map = &afu->afu_map->ctrls[hwq->ctx_hndl].ctrl;
+ 
+ 		/* Program the Endian Control for the master context */
+ 		writeq_be(SISL_ENDIAN_CTRL, &hwq->host_map->endian_ctrl);
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	}
  }
  
@@@ -1487,7 -1571,10 +1766,14 @@@ static int init_global(struct cxlflash_
  {
  	struct afu *afu = cfg->afu;
  	struct device *dev = &cfg->dev->dev;
++<<<<<<< HEAD
 +	u64 wwpn[NUM_FC_PORTS];	/* wwpn of AFU ports */
++=======
+ 	struct hwq *hwq;
+ 	struct sisl_host_map __iomem *hmap;
+ 	__be64 __iomem *fc_port_regs;
+ 	u64 wwpn[MAX_FC_PORTS];	/* wwpn of AFU ports */
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	int i = 0, num_ports = 0;
  	int rc = 0;
  	u64 reg;
@@@ -1498,12 -1585,20 +1784,29 @@@
  		goto out;
  	}
  
++<<<<<<< HEAD
 +	pr_debug("%s: wwpn0=0x%llX wwpn1=0x%llX\n", __func__, wwpn[0], wwpn[1]);
 +
 +	/* Set up RRQ in AFU for master issued cmds */
 +	writeq_be((u64) afu->hrrq_start, &afu->host_map->rrq_start);
 +	writeq_be((u64) afu->hrrq_end, &afu->host_map->rrq_end);
 +
++=======
+ 	/* Set up RRQ and SQ in HWQ for master issued cmds */
+ 	for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 		hwq = get_hwq(afu, i);
+ 		hmap = hwq->host_map;
+ 
+ 		writeq_be((u64) hwq->hrrq_start, &hmap->rrq_start);
+ 		writeq_be((u64) hwq->hrrq_end, &hmap->rrq_end);
+ 
+ 		if (afu_is_sq_cmd_mode(afu)) {
+ 			writeq_be((u64)hwq->hsq_start, &hmap->sq_start);
+ 			writeq_be((u64)hwq->hsq_end, &hmap->sq_end);
+ 		}
+ 	}
+ 
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	/* AFU configuration */
  	reg = readq_be(&afu->afu_map->global.regs.afu_config);
  	reg |= SISL_AFUCONF_AR_ALL|SISL_AFUCONF_ENDIAN;
@@@ -1546,14 -1640,17 +1849,18 @@@
  	/* Set up master's own CTX_CAP to allow real mode, host translation */
  	/* tables, afu cmds and read/write GSCSI cmds. */
  	/* First, unlock ctx_cap write by reading mbox */
- 	(void)readq_be(&afu->ctrl_map->mbox_r);	/* unlock ctx_cap */
- 	writeq_be((SISL_CTX_CAP_REAL_MODE | SISL_CTX_CAP_HOST_XLATE |
- 		   SISL_CTX_CAP_READ_CMD | SISL_CTX_CAP_WRITE_CMD |
- 		   SISL_CTX_CAP_AFU_CMD | SISL_CTX_CAP_GSCSI_CMD),
- 		  &afu->ctrl_map->ctx_cap);
+ 	for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 		hwq = get_hwq(afu, i);
+ 
+ 		(void)readq_be(&hwq->ctrl_map->mbox_r);	/* unlock ctx_cap */
+ 		writeq_be((SISL_CTX_CAP_REAL_MODE | SISL_CTX_CAP_HOST_XLATE |
+ 			SISL_CTX_CAP_READ_CMD | SISL_CTX_CAP_WRITE_CMD |
+ 			SISL_CTX_CAP_AFU_CMD | SISL_CTX_CAP_GSCSI_CMD),
+ 			&hwq->ctrl_map->ctx_cap);
+ 	}
  	/* Initialize heartbeat */
  	afu->hb = readq_be(&afu->afu_map->global.regs.afu_hb);
 +
  out:
  	return rc;
  }
@@@ -1565,29 -1662,44 +1872,69 @@@
  static int start_afu(struct cxlflash_cfg *cfg)
  {
  	struct afu *afu = cfg->afu;
++<<<<<<< HEAD
 +	struct afu_cmd *cmd;
 +
 +	int i = 0;
++=======
+ 	struct device *dev = &cfg->dev->dev;
+ 	struct hwq *hwq;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	int rc = 0;
+ 	int i;
  
 +	for (i = 0; i < CXLFLASH_NUM_CMDS; i++) {
 +		cmd = &afu->cmd[i];
 +
 +		init_completion(&cmd->cevent);
 +		spin_lock_init(&cmd->slock);
 +		cmd->parent = afu;
 +	}
 +
  	init_pcr(cfg);
  
++<<<<<<< HEAD
 +	/* After an AFU reset, RRQ entries are stale, clear them */
 +	memset(&afu->rrq_entry, 0, sizeof(afu->rrq_entry));
 +
 +	/* Initialize RRQ pointers */
 +	afu->hrrq_start = &afu->rrq_entry[0];
 +	afu->hrrq_end = &afu->rrq_entry[NUM_RRQ_ENTRY - 1];
 +	afu->hrrq_curr = afu->hrrq_start;
 +	afu->toggle = 1;
++=======
+ 	/* Initialize each HWQ */
+ 	for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 		hwq = get_hwq(afu, i);
+ 
+ 		/* After an AFU reset, RRQ entries are stale, clear them */
+ 		memset(&hwq->rrq_entry, 0, sizeof(hwq->rrq_entry));
+ 
+ 		/* Initialize RRQ pointers */
+ 		hwq->hrrq_start = &hwq->rrq_entry[0];
+ 		hwq->hrrq_end = &hwq->rrq_entry[NUM_RRQ_ENTRY - 1];
+ 		hwq->hrrq_curr = hwq->hrrq_start;
+ 		hwq->toggle = 1;
+ 		spin_lock_init(&hwq->hrrq_slock);
+ 
+ 		/* Initialize SQ */
+ 		if (afu_is_sq_cmd_mode(afu)) {
+ 			memset(&hwq->sq, 0, sizeof(hwq->sq));
+ 			hwq->hsq_start = &hwq->sq[0];
+ 			hwq->hsq_end = &hwq->sq[NUM_SQ_ENTRY - 1];
+ 			hwq->hsq_curr = hwq->hsq_start;
+ 
+ 			spin_lock_init(&hwq->hsq_slock);
+ 			atomic_set(&hwq->hsq_credits, NUM_SQ_ENTRY - 1);
+ 		}
+ 
+ 		/* Initialize IRQ poll */
+ 		if (afu_is_irqpoll_enabled(afu))
+ 			irq_poll_init(&hwq->irqpoll, afu->irqpoll_weight,
+ 				      cxlflash_irqpoll);
+ 
+ 	}
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  
  	rc = init_global(cfg);
  
@@@ -1602,44 -1715,47 +1950,50 @@@
   * Return: 0 on success, -errno on failure
   */
  static enum undo_level init_intr(struct cxlflash_cfg *cfg,
- 				 struct cxl_context *ctx)
+ 				 struct hwq *hwq)
  {
- 	struct afu *afu = cfg->afu;
  	struct device *dev = &cfg->dev->dev;
+ 	struct cxl_context *ctx = hwq->ctx;
  	int rc = 0;
  	enum undo_level level = UNDO_NOOP;
+ 	bool is_primary_hwq = (hwq->index == PRIMARY_HWQ);
+ 	int num_irqs = is_primary_hwq ? 3 : 2;
  
- 	rc = cxl_allocate_afu_irqs(ctx, 3);
+ 	rc = cxl_allocate_afu_irqs(ctx, num_irqs);
  	if (unlikely(rc)) {
 -		dev_err(dev, "%s: allocate_afu_irqs failed rc=%d\n",
 +		dev_err(dev, "%s: call to allocate_afu_irqs failed rc=%d!\n",
  			__func__, rc);
  		level = UNDO_NOOP;
  		goto out;
  	}
  
- 	rc = cxl_map_afu_irq(ctx, 1, cxlflash_sync_err_irq, afu,
+ 	rc = cxl_map_afu_irq(ctx, 1, cxlflash_sync_err_irq, hwq,
  			     "SISL_MSI_SYNC_ERROR");
  	if (unlikely(rc <= 0)) {
 -		dev_err(dev, "%s: SISL_MSI_SYNC_ERROR map failed\n", __func__);
 +		dev_err(dev, "%s: IRQ 1 (SISL_MSI_SYNC_ERROR) map failed!\n",
 +			__func__);
  		level = FREE_IRQ;
  		goto out;
  	}
  
- 	rc = cxl_map_afu_irq(ctx, 2, cxlflash_rrq_irq, afu,
+ 	rc = cxl_map_afu_irq(ctx, 2, cxlflash_rrq_irq, hwq,
  			     "SISL_MSI_RRQ_UPDATED");
  	if (unlikely(rc <= 0)) {
 -		dev_err(dev, "%s: SISL_MSI_RRQ_UPDATED map failed\n", __func__);
 +		dev_err(dev, "%s: IRQ 2 (SISL_MSI_RRQ_UPDATED) map failed!\n",
 +			__func__);
  		level = UNMAP_ONE;
  		goto out;
  	}
  
- 	rc = cxl_map_afu_irq(ctx, 3, cxlflash_async_err_irq, afu,
+ 	/* SISL_MSI_ASYNC_ERROR is setup only for the primary HWQ */
+ 	if (!is_primary_hwq)
+ 		goto out;
+ 
+ 	rc = cxl_map_afu_irq(ctx, 3, cxlflash_async_err_irq, hwq,
  			     "SISL_MSI_ASYNC_ERROR");
  	if (unlikely(rc <= 0)) {
 -		dev_err(dev, "%s: SISL_MSI_ASYNC_ERROR map failed\n", __func__);
 +		dev_err(dev, "%s: IRQ 3 (SISL_MSI_ASYNC_ERROR) map failed!\n",
 +			__func__);
  		level = UNMAP_TWO;
  		goto out;
  	}
@@@ -1670,19 -1796,20 +2034,35 @@@ static int init_mc(struct cxlflash_cfg 
  	/* Set it up as a master with the CXL */
  	cxl_set_master(ctx);
  
++<<<<<<< HEAD
 +	/* During initialization reset the AFU to start from a clean slate */
 +	rc = cxl_afu_reset(cfg->mcctx);
 +	if (unlikely(rc)) {
 +		dev_err(dev, "%s: initial AFU reset failed rc=%d\n",
 +			__func__, rc);
 +		goto ret;
++=======
+ 	/* Reset AFU when initializing primary context */
+ 	if (index == PRIMARY_HWQ) {
+ 		rc = cxl_afu_reset(ctx);
+ 		if (unlikely(rc)) {
+ 			dev_err(dev, "%s: AFU reset failed rc=%d\n",
+ 				      __func__, rc);
+ 			goto err1;
+ 		}
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	}
  
- 	level = init_intr(cfg, ctx);
+ 	level = init_intr(cfg, hwq);
  	if (unlikely(level)) {
++<<<<<<< HEAD
 +		dev_err(dev, "%s: setting up interrupts failed rc=%d\n",
 +			__func__, rc);
 +		goto out;
++=======
+ 		dev_err(dev, "%s: interrupt init failed rc=%d\n", __func__, rc);
+ 		goto err2;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	}
  
  	/* This performs the equivalent of the CXL_IOCTL_START_WORK.
@@@ -1693,16 -1820,54 +2073,26 @@@
  	if (unlikely(rc)) {
  		dev_err(dev, "%s: start context failed rc=%d\n", __func__, rc);
  		level = UNMAP_THREE;
- 		goto out;
+ 		goto err2;
  	}
++<<<<<<< HEAD
 +ret:
 +	pr_debug("%s: returning rc=%d\n", __func__, rc);
- 	return rc;
++=======
+ 
  out:
- 	term_intr(cfg, level);
- 	goto ret;
+ 	dev_dbg(dev, "%s: returning rc=%d\n", __func__, rc);
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
+ 	return rc;
+ err2:
+ 	term_intr(cfg, level, index);
+ 	if (index != PRIMARY_HWQ)
+ 		cxl_release_context(ctx);
+ err1:
+ 	hwq->ctx = NULL;
+ 	goto out;
  }
  
 -/**
 - * get_num_afu_ports() - determines and configures the number of AFU ports
 - * @cfg:	Internal structure associated with the host.
 - *
 - * This routine determines the number of AFU ports by converting the global
 - * port selection mask. The converted value is only valid following an AFU
 - * reset (explicit or power-on). This routine must be invoked shortly after
 - * mapping as other routines are dependent on the number of ports during the
 - * initialization sequence.
 - *
 - * To support legacy AFUs that might not have reflected an initial global
 - * port mask (value read is 0), default to the number of ports originally
 - * supported by the cxlflash driver (2) before hardware with other port
 - * offerings was introduced.
 - */
 -static void get_num_afu_ports(struct cxlflash_cfg *cfg)
 -{
 -	struct afu *afu = cfg->afu;
 -	struct device *dev = &cfg->dev->dev;
 -	u64 port_mask;
 -	int num_fc_ports = LEGACY_FC_PORTS;
 -
 -	port_mask = readq_be(&afu->afu_map->global.regs.afu_port_sel);
 -	if (port_mask != 0ULL)
 -		num_fc_ports = min(ilog2(port_mask) + 1, MAX_FC_PORTS);
 -
 -	dev_dbg(dev, "%s: port_mask=%016llx num_fc_ports=%d\n",
 -		__func__, port_mask, num_fc_ports);
 -
 -	cfg->num_fc_ports = num_fc_ports;
 -	cfg->host->max_channel = PORTNUM2CHAN(num_fc_ports);
 -}
 -
  /**
   * init_afu() - setup as master context and start AFU
   * @cfg:	Internal structure associated with the host.
@@@ -1721,17 -1888,20 +2113,28 @@@ static int init_afu(struct cxlflash_cf
  
  	cxl_perst_reloads_same_image(cfg->cxl_afu, true);
  
++<<<<<<< HEAD
 +	rc = init_mc(cfg);
 +	if (rc) {
 +		dev_err(dev, "%s: call to init_mc failed, rc=%d!\n",
 +			__func__, rc);
 +		goto out;
++=======
+ 	for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 		rc = init_mc(cfg, i);
+ 		if (rc) {
+ 			dev_err(dev, "%s: init_mc failed rc=%d index=%d\n",
+ 				__func__, rc, i);
+ 			goto err1;
+ 		}
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	}
  
- 	/* Map the entire MMIO space of the AFU */
- 	afu->afu_map = cxl_psa_map(cfg->mcctx);
+ 	/* Map the entire MMIO space of the AFU using the first context */
+ 	hwq = get_hwq(afu, PRIMARY_HWQ);
+ 	afu->afu_map = cxl_psa_map(hwq->ctx);
  	if (!afu->afu_map) {
 -		dev_err(dev, "%s: cxl_psa_map failed\n", __func__);
 +		dev_err(dev, "%s: call to cxl_psa_map failed!\n", __func__);
  		rc = -ENOMEM;
  		goto err1;
  	}
@@@ -1822,18 -2010,14 +2232,19 @@@ int cxlflash_afu_sync(struct afu *afu, 
  
  	cmd = (struct afu_cmd *)PTR_ALIGN(buf, __alignof__(*cmd));
  	init_completion(&cmd->cevent);
 +	spin_lock_init(&cmd->slock);
  	cmd->parent = afu;
+ 	cmd->hwq_index = hwq->index;
  
 -	dev_dbg(dev, "%s: afu=%p cmd=%p %d\n", __func__, afu, cmd, ctx_hndl_u);
 +	pr_debug("%s: afu=%p cmd=%p %d\n", __func__, afu, cmd, ctx_hndl_u);
  
  	cmd->rcb.req_flags = SISL_REQ_FLAGS_AFU_CMD;
- 	cmd->rcb.ctx_id = afu->ctx_hndl;
+ 	cmd->rcb.ctx_id = hwq->ctx_hndl;
  	cmd->rcb.msi = SISL_MSI_RRQ_UPDATED;
 +	cmd->rcb.port_sel = 0x0;	/* NA */
 +	cmd->rcb.lun_id = 0x0;	/* NA */
 +	cmd->rcb.data_len = 0x0;
 +	cmd->rcb.data_ea = 0x0;
  	cmd->rcb.timeout = MC_AFU_SYNC_TIMEOUT;
  
  	cmd->rcb.cdb[0] = 0xC0;	/* AFU Sync */
@@@ -2230,11 -2451,123 +2641,75 @@@ static ssize_t port1_lun_table_show(str
  				    struct device_attribute *attr,
  				    char *buf)
  {
 -	struct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));
 -
 -	return cxlflash_show_port_lun_table(1, cfg, buf);
 -}
 -
 -/**
 - * port2_lun_table_show() - presents the current LUN table of port 2
 - * @dev:	Generic device associated with the host owning the port.
 - * @attr:	Device attribute representing the port.
 - * @buf:	Buffer of length PAGE_SIZE to report back port status in ASCII.
 - *
 - * Return: The size of the ASCII string returned in @buf.
 - */
 -static ssize_t port2_lun_table_show(struct device *dev,
 -				    struct device_attribute *attr,
 -				    char *buf)
 -{
 -	struct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));
 -
 -	return cxlflash_show_port_lun_table(2, cfg, buf);
 -}
 -
 -/**
 - * port3_lun_table_show() - presents the current LUN table of port 3
 - * @dev:	Generic device associated with the host owning the port.
 - * @attr:	Device attribute representing the port.
 - * @buf:	Buffer of length PAGE_SIZE to report back port status in ASCII.
 - *
 - * Return: The size of the ASCII string returned in @buf.
 - */
 -static ssize_t port3_lun_table_show(struct device *dev,
 -				    struct device_attribute *attr,
 -				    char *buf)
 -{
 -	struct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));
 -
 -	return cxlflash_show_port_lun_table(3, cfg, buf);
 -}
 -
 -/**
 - * irqpoll_weight_show() - presents the current IRQ poll weight for the host
 - * @dev:	Generic device associated with the host.
 - * @attr:	Device attribute representing the IRQ poll weight.
 - * @buf:	Buffer of length PAGE_SIZE to report back the current IRQ poll
 - *		weight in ASCII.
 - *
 - * An IRQ poll weight of 0 indicates polling is disabled.
 - *
 - * Return: The size of the ASCII string returned in @buf.
 - */
 -static ssize_t irqpoll_weight_show(struct device *dev,
 -				   struct device_attribute *attr, char *buf)
 -{
 -	struct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));
 +	struct Scsi_Host *shost = class_to_shost(dev);
 +	struct cxlflash_cfg *cfg = (struct cxlflash_cfg *)shost->hostdata;
  	struct afu *afu = cfg->afu;
  
++<<<<<<< HEAD
 +	return cxlflash_show_port_lun_table(1, afu, buf);
++=======
+ 	return scnprintf(buf, PAGE_SIZE, "%u\n", afu->irqpoll_weight);
+ }
+ 
+ /**
+  * irqpoll_weight_store() - sets the current IRQ poll weight for the host
+  * @dev:	Generic device associated with the host.
+  * @attr:	Device attribute representing the IRQ poll weight.
+  * @buf:	Buffer of length PAGE_SIZE containing the desired IRQ poll
+  *		weight in ASCII.
+  * @count:	Length of data resizing in @buf.
+  *
+  * An IRQ poll weight of 0 indicates polling is disabled.
+  *
+  * Return: The size of the ASCII string returned in @buf.
+  */
+ static ssize_t irqpoll_weight_store(struct device *dev,
+ 				    struct device_attribute *attr,
+ 				    const char *buf, size_t count)
+ {
+ 	struct cxlflash_cfg *cfg = shost_priv(class_to_shost(dev));
+ 	struct device *cfgdev = &cfg->dev->dev;
+ 	struct afu *afu = cfg->afu;
+ 	struct hwq *hwq;
+ 	u32 weight;
+ 	int rc, i;
+ 
+ 	rc = kstrtouint(buf, 10, &weight);
+ 	if (rc)
+ 		return -EINVAL;
+ 
+ 	if (weight > 256) {
+ 		dev_info(cfgdev,
+ 			 "Invalid IRQ poll weight. It must be 256 or less.\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (weight == afu->irqpoll_weight) {
+ 		dev_info(cfgdev,
+ 			 "Current IRQ poll weight has the same weight.\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (afu_is_irqpoll_enabled(afu)) {
+ 		for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 			hwq = get_hwq(afu, i);
+ 
+ 			irq_poll_disable(&hwq->irqpoll);
+ 		}
+ 	}
+ 
+ 	afu->irqpoll_weight = weight;
+ 
+ 	if (weight > 0) {
+ 		for (i = 0; i < CXLFLASH_NUM_HWQS; i++) {
+ 			hwq = get_hwq(afu, i);
+ 
+ 			irq_poll_init(&hwq->irqpoll, weight, cxlflash_irqpoll);
+ 		}
+ 	}
+ 
+ 	return count;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  }
  
  /**
diff --cc drivers/scsi/cxlflash/superpipe.c
index a4dd3ca7c750,fe9f17a6268b..000000000000
--- a/drivers/scsi/cxlflash/superpipe.c
+++ b/drivers/scsi/cxlflash/superpipe.c
@@@ -1617,6 -1627,8 +1618,11 @@@ static int cxlflash_afu_recover(struct 
  	struct afu *afu = cfg->afu;
  	struct ctx_info *ctxi = NULL;
  	struct mutex *mutex = &cfg->ctx_recovery_mutex;
++<<<<<<< HEAD
++=======
+ 	struct hwq *hwq = get_hwq(afu, PRIMARY_HWQ);
+ 	u64 flags;
++>>>>>>> bfc0bab172ca (scsi: cxlflash: Support multiple hardware queues)
  	u64 ctxid = DECODE_CTXID(recover->context_id),
  	    rctxid = recover->context_id;
  	long reg;
* Unmerged path drivers/scsi/cxlflash/common.h
* Unmerged path drivers/scsi/cxlflash/main.c
* Unmerged path drivers/scsi/cxlflash/superpipe.c
