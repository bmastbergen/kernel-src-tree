xfs: simplify xfs_file_iomap_begin

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Christoph Hellwig <hch@lst.de>
commit b95a21271b30544a9fb992269d79ed1e1978e023
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b95a2127.failed

We'll never get nimap == 0 for a successful return from xfs_bmapi_read,
so don't try to handle it.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Dave Chinner <dchinner@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>


(cherry picked from commit b95a21271b30544a9fb992269d79ed1e1978e023)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_iomap.c
#	fs/xfs/xfs_trace.h
diff --cc fs/xfs/xfs_iomap.c
index 2f3719461cbd,1cce760399a2..000000000000
--- a/fs/xfs/xfs_iomap.c
+++ b/fs/xfs/xfs_iomap.c
@@@ -967,3 -968,141 +967,144 @@@ xfs_bmbt_to_iomap
  	iomap->length = XFS_FSB_TO_B(mp, imap->br_blockcount);
  	iomap->bdev = xfs_find_bdev_for_inode(VFS_I(ip));
  }
++<<<<<<< HEAD
++=======
+ 
+ static inline bool imap_needs_alloc(struct xfs_bmbt_irec *imap, int nimaps)
+ {
+ 	return !nimaps ||
+ 		imap->br_startblock == HOLESTARTBLOCK ||
+ 		imap->br_startblock == DELAYSTARTBLOCK;
+ }
+ 
+ static int
+ xfs_file_iomap_begin(
+ 	struct inode		*inode,
+ 	loff_t			offset,
+ 	loff_t			length,
+ 	unsigned		flags,
+ 	struct iomap		*iomap)
+ {
+ 	struct xfs_inode	*ip = XFS_I(inode);
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_bmbt_irec	imap;
+ 	xfs_fileoff_t		offset_fsb, end_fsb;
+ 	int			nimaps = 1, error = 0;
+ 
+ 	if (XFS_FORCED_SHUTDOWN(mp))
+ 		return -EIO;
+ 
+ 	xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 
+ 	ASSERT(offset <= mp->m_super->s_maxbytes);
+ 	if ((xfs_fsize_t)offset + length > mp->m_super->s_maxbytes)
+ 		length = mp->m_super->s_maxbytes - offset;
+ 	offset_fsb = XFS_B_TO_FSBT(mp, offset);
+ 	end_fsb = XFS_B_TO_FSB(mp, offset + length);
+ 
+ 	error = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb, &imap,
+ 			       &nimaps, XFS_BMAPI_ENTIRE);
+ 	if (error) {
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 		return error;
+ 	}
+ 
+ 	if ((flags & IOMAP_WRITE) && imap_needs_alloc(&imap, nimaps)) {
+ 		/*
+ 		 * We cap the maximum length we map here to MAX_WRITEBACK_PAGES
+ 		 * pages to keep the chunks of work done where somewhat symmetric
+ 		 * with the work writeback does. This is a completely arbitrary
+ 		 * number pulled out of thin air as a best guess for initial
+ 		 * testing.
+ 		 *
+ 		 * Note that the values needs to be less than 32-bits wide until
+ 		 * the lower level functions are updated.
+ 		 */
+ 		length = min_t(loff_t, length, 1024 * PAGE_SIZE);
+ 		if (xfs_get_extsz_hint(ip)) {
+ 			/*
+ 			 * xfs_iomap_write_direct() expects the shared lock. It
+ 			 * is unlocked on return.
+ 			 */
+ 			xfs_ilock_demote(ip, XFS_ILOCK_EXCL);
+ 			error = xfs_iomap_write_direct(ip, offset, length, &imap,
+ 					nimaps);
+ 		} else {
+ 			error = xfs_iomap_write_delay(ip, offset, length, &imap);
+ 			xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 		}
+ 
+ 		if (error)
+ 			return error;
+ 
+ 		trace_xfs_iomap_alloc(ip, offset, length, 0, &imap);
+ 	} else {
+ 		ASSERT(nimaps);
+ 
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 		trace_xfs_iomap_found(ip, offset, length, 0, &imap);
+ 	}
+ 
+ 	xfs_bmbt_to_iomap(ip, iomap, &imap);
+ 	return 0;
+ }
+ 
+ static int
+ xfs_file_iomap_end_delalloc(
+ 	struct xfs_inode	*ip,
+ 	loff_t			offset,
+ 	loff_t			length,
+ 	ssize_t			written)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	xfs_fileoff_t		start_fsb;
+ 	xfs_fileoff_t		end_fsb;
+ 	int			error = 0;
+ 
+ 	start_fsb = XFS_B_TO_FSB(mp, offset + written);
+ 	end_fsb = XFS_B_TO_FSB(mp, offset + length);
+ 
+ 	/*
+ 	 * Trim back delalloc blocks if we didn't manage to write the whole
+ 	 * range reserved.
+ 	 *
+ 	 * We don't need to care about racing delalloc as we hold i_mutex
+ 	 * across the reserve/allocate/unreserve calls. If there are delalloc
+ 	 * blocks in the range, they are ours.
+ 	 */
+ 	if (start_fsb < end_fsb) {
+ 		xfs_ilock(ip, XFS_ILOCK_EXCL);
+ 		error = xfs_bmap_punch_delalloc_range(ip, start_fsb,
+ 					       end_fsb - start_fsb);
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 
+ 		if (error && !XFS_FORCED_SHUTDOWN(mp)) {
+ 			xfs_alert(mp, "%s: unable to clean up ino %lld",
+ 				__func__, ip->i_ino);
+ 			return error;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ xfs_file_iomap_end(
+ 	struct inode		*inode,
+ 	loff_t			offset,
+ 	loff_t			length,
+ 	ssize_t			written,
+ 	unsigned		flags,
+ 	struct iomap		*iomap)
+ {
+ 	if ((flags & IOMAP_WRITE) && iomap->type == IOMAP_DELALLOC)
+ 		return xfs_file_iomap_end_delalloc(XFS_I(inode), offset,
+ 				length, written);
+ 	return 0;
+ }
+ 
+ struct iomap_ops xfs_iomap_ops = {
+ 	.iomap_begin		= xfs_file_iomap_begin,
+ 	.iomap_end		= xfs_file_iomap_end,
+ };
++>>>>>>> b95a21271b30 (xfs: simplify xfs_file_iomap_begin)
diff --cc fs/xfs/xfs_trace.h
index ed6f8b2400d2,7e88bec3f359..000000000000
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@@ -1293,11 -1295,9 +1293,17 @@@ DEFINE_IOMAP_EVENT(xfs_map_blocks_found
  DEFINE_IOMAP_EVENT(xfs_map_blocks_alloc);
  DEFINE_IOMAP_EVENT(xfs_get_blocks_found);
  DEFINE_IOMAP_EVENT(xfs_get_blocks_alloc);
++<<<<<<< HEAD
 +DEFINE_IOMAP_EVENT(xfs_gbmap_direct);
 +DEFINE_IOMAP_EVENT(xfs_gbmap_direct_new);
 +DEFINE_IOMAP_EVENT(xfs_gbmap_direct_update);
 +DEFINE_IOMAP_EVENT(xfs_gbmap_direct_none);
 +DEFINE_IOMAP_EVENT(xfs_gbmap_direct_endio);
++=======
+ DEFINE_IOMAP_EVENT(xfs_get_blocks_map_direct);
+ DEFINE_IOMAP_EVENT(xfs_iomap_alloc);
+ DEFINE_IOMAP_EVENT(xfs_iomap_found);
++>>>>>>> b95a21271b30 (xfs: simplify xfs_file_iomap_begin)
  
  DECLARE_EVENT_CLASS(xfs_simple_io_class,
  	TP_PROTO(struct xfs_inode *ip, xfs_off_t offset, ssize_t count),
* Unmerged path fs/xfs/xfs_iomap.c
* Unmerged path fs/xfs/xfs_trace.h
