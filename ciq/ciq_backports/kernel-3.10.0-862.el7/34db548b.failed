mlx4: add page recycling in receive path

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Eric Dumazet <edumazet@google.com>
commit 34db548bfb9580f33d9a7faecafe4da61a4428a3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/34db548b.failed

Same technique than some Intel drivers, for arches where PAGE_SIZE = 4096

In most cases, pages are reused because they were consumed
before we could loop around the RX ring.

This brings back performance, and is even better,
a single TCP flow reaches 30Gbit on my hosts.

v2: added full memset() in mlx4_en_free_frag(), as Tariq found it was needed
if we switch to large MTU, as priv->log_rx_info can dynamically be changed.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Acked-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 34db548bfb9580f33d9a7faecafe4da61a4428a3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx4/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx4/en_rx.c
index 984f22166c89,5edd0cf2999c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
@@@ -48,172 -50,58 +48,206 @@@
  
  #include "mlx4_en.h"
  
++<<<<<<< HEAD
 +static int mlx4_alloc_pages(struct mlx4_en_priv *priv,
 +			    struct mlx4_en_rx_alloc *page_alloc,
 +			    const struct mlx4_en_frag_info *frag_info,
 +			    gfp_t _gfp)
++=======
+ static int mlx4_alloc_page(struct mlx4_en_priv *priv,
+ 			   struct mlx4_en_rx_alloc *frag,
+ 			   gfp_t gfp)
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  {
 +	int order;
  	struct page *page;
  	dma_addr_t dma;
  
 -	page = alloc_page(gfp);
 -	if (unlikely(!page))
 -		return -ENOMEM;
 -	dma = dma_map_page(priv->ddev, page, 0, PAGE_SIZE, priv->dma_dir);
 +	for (order = frag_info->order; ;) {
 +		gfp_t gfp = _gfp;
 +
 +		if (order)
 +			gfp |= __GFP_COMP | __GFP_NOWARN | __GFP_NOMEMALLOC;
 +		page = alloc_pages(gfp, order);
 +		if (likely(page))
 +			break;
 +		if (--order < 0 ||
 +		    ((PAGE_SIZE << order) < frag_info->frag_size))
 +			return -ENOMEM;
 +	}
 +	dma = dma_map_page(priv->ddev, page, 0, PAGE_SIZE << order,
 +			   frag_info->dma_dir);
  	if (unlikely(dma_mapping_error(priv->ddev, dma))) {
- 		put_page(page);
+ 		__free_page(page);
  		return -ENOMEM;
  	}
++<<<<<<< HEAD
 +	page_alloc->page_size = PAGE_SIZE << order;
 +	page_alloc->page = page;
 +	page_alloc->dma = dma;
 +	page_alloc->page_offset = 0;
 +	/* Not doing get_page() for each frag is a big win
 +	 * on asymetric workloads. Note we can not use atomic_set().
 +	 */
 +	page_ref_add(page, page_alloc->page_size / frag_info->frag_stride - 1);
++=======
+ 	frag->page = page;
+ 	frag->dma = dma;
+ 	frag->page_offset = priv->rx_headroom;
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  	return 0;
  }
  
  static int mlx4_en_alloc_frags(struct mlx4_en_priv *priv,
  			       struct mlx4_en_rx_desc *rx_desc,
  			       struct mlx4_en_rx_alloc *frags,
- 			       struct mlx4_en_rx_alloc *ring_alloc,
  			       gfp_t gfp)
  {
++<<<<<<< HEAD
 +	struct mlx4_en_rx_alloc page_alloc[MLX4_EN_MAX_RX_FRAGS];
 +	const struct mlx4_en_frag_info *frag_info;
 +	struct page *page;
 +	dma_addr_t dma;
 +	int i;
 +
 +	for (i = 0; i < priv->num_frags; i++) {
 +		frag_info = &priv->frag_info[i];
 +		page_alloc[i] = ring_alloc[i];
 +		page_alloc[i].page_offset += frag_info->frag_stride;
 +
 +		if (page_alloc[i].page_offset + frag_info->frag_stride <=
 +		    ring_alloc[i].page_size)
 +			continue;
 +
 +		if (unlikely(mlx4_alloc_pages(priv, &page_alloc[i],
 +					      frag_info, gfp)))
 +			goto out;
 +	}
 +
 +	for (i = 0; i < priv->num_frags; i++) {
 +		frags[i] = ring_alloc[i];
 +		dma = ring_alloc[i].dma + ring_alloc[i].page_offset;
 +		ring_alloc[i] = page_alloc[i];
 +		rx_desc->data[i].addr = cpu_to_be64(dma);
 +	}
 +
 +	return 0;
 +
 +out:
 +	while (i--) {
 +		if (page_alloc[i].page != ring_alloc[i].page) {
 +			dma_unmap_page(priv->ddev, page_alloc[i].dma,
 +				page_alloc[i].page_size,
 +				priv->frag_info[i].dma_dir);
 +			page = page_alloc[i].page;
 +			/* Revert changes done by mlx4_alloc_pages */
 +			page_ref_sub(page, page_alloc[i].page_size /
 +					   priv->frag_info[i].frag_stride - 1);
 +			put_page(page);
 +		}
 +	}
 +	return -ENOMEM;
 +}
 +
 +static void mlx4_en_free_frag(struct mlx4_en_priv *priv,
 +			      struct mlx4_en_rx_alloc *frags,
 +			      int i)
 +{
 +	const struct mlx4_en_frag_info *frag_info = &priv->frag_info[i];
 +	u32 next_frag_end = frags[i].page_offset + 2 * frag_info->frag_stride;
 +
 +	if (next_frag_end > frags[i].page_size)
 +		dma_unmap_page(priv->ddev, frags[i].dma, frags[i].page_size,
 +			       frag_info->dma_dir);
 +
 +	if (frags[i].page)
 +		put_page(frags[i].page);
 +}
 +
 +static int mlx4_en_init_allocator(struct mlx4_en_priv *priv,
 +				  struct mlx4_en_rx_ring *ring)
 +{
 +	int i;
 +	struct mlx4_en_rx_alloc *page_alloc;
 +
 +	for (i = 0; i < priv->num_frags; i++) {
 +		const struct mlx4_en_frag_info *frag_info = &priv->frag_info[i];
 +
 +		if (mlx4_alloc_pages(priv, &ring->page_alloc[i],
 +				     frag_info, GFP_KERNEL | __GFP_COLD))
 +			goto out;
 +
 +		en_dbg(DRV, priv, "  frag %d allocator: - size:%d frags:%d\n",
 +		       i, ring->page_alloc[i].page_size,
 +		       page_ref_count(ring->page_alloc[i].page));
 +	}
 +	return 0;
 +
 +out:
 +	while (i--) {
 +		struct page *page;
 +
 +		page_alloc = &ring->page_alloc[i];
 +		dma_unmap_page(priv->ddev, page_alloc->dma,
 +			       page_alloc->page_size,
 +			       priv->frag_info[i].dma_dir);
 +		page = page_alloc->page;
 +		/* Revert changes done by mlx4_alloc_pages */
 +		page_ref_sub(page, page_alloc->page_size /
 +				   priv->frag_info[i].frag_stride - 1);
 +		put_page(page);
 +		page_alloc->page = NULL;
 +	}
 +	return -ENOMEM;
++=======
+ 	int i;
+ 
+ 	for (i = 0; i < priv->num_frags; i++, frags++) {
+ 		if (!frags->page && mlx4_alloc_page(priv, frags, gfp))
+ 			return -ENOMEM;
+ 		rx_desc->data[i].addr = cpu_to_be64(frags->dma +
+ 						    frags->page_offset);
+ 	}
+ 	return 0;
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  }
  
- static void mlx4_en_destroy_allocator(struct mlx4_en_priv *priv,
- 				      struct mlx4_en_rx_ring *ring)
+ static void mlx4_en_free_frag(const struct mlx4_en_priv *priv,
+ 			      struct mlx4_en_rx_alloc *frag)
  {
++<<<<<<< HEAD
 +	struct mlx4_en_rx_alloc *page_alloc;
 +	int i;
 +
 +	for (i = 0; i < priv->num_frags; i++) {
 +		const struct mlx4_en_frag_info *frag_info = &priv->frag_info[i];
 +
 +		page_alloc = &ring->page_alloc[i];
 +		en_dbg(DRV, priv, "Freeing allocator:%d count:%d\n",
 +		       i, page_count(page_alloc->page));
 +
 +		dma_unmap_page(priv->ddev, page_alloc->dma,
 +				page_alloc->page_size, frag_info->dma_dir);
 +		while (page_alloc->page_offset + frag_info->frag_stride <
 +		       page_alloc->page_size) {
 +			put_page(page_alloc->page);
 +			page_alloc->page_offset += frag_info->frag_stride;
 +		}
 +		page_alloc->page = NULL;
++=======
+ 	if (frag->page) {
+ 		dma_unmap_page(priv->ddev, frag->dma,
+ 			       PAGE_SIZE, priv->dma_dir);
+ 		__free_page(frag->page);
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  	}
+ 	/* We need to clear all fields, otherwise a change of priv->log_rx_info
+ 	 * could lead to see garbage later in frag->page.
+ 	 */
+ 	memset(frag, 0, sizeof(*frag));
  }
  
- static void mlx4_en_init_rx_desc(struct mlx4_en_priv *priv,
+ static void mlx4_en_init_rx_desc(const struct mlx4_en_priv *priv,
  				 struct mlx4_en_rx_ring *ring, int index)
  {
  	struct mlx4_en_rx_desc *rx_desc = ring->buf + ring->stride * index;
@@@ -247,8 -135,15 +281,20 @@@ static int mlx4_en_prepare_rx_desc(stru
  					(index << priv->log_rx_info);
  
  	if (ring->page_cache.index > 0) {
++<<<<<<< HEAD
 +		frags[0] = ring->page_cache.buf[--ring->page_cache.index];
 +		rx_desc->data[0].addr = cpu_to_be64(frags[0].dma);
++=======
+ 		/* XDP uses a single page per frame */
+ 		if (!frags->page) {
+ 			ring->page_cache.index--;
+ 			frags->page = ring->page_cache.buf[ring->page_cache.index].page;
+ 			frags->dma  = ring->page_cache.buf[ring->page_cache.index].dma;
+ 		}
+ 		frags->page_offset = XDP_PACKET_HEADROOM;
+ 		rx_desc->data[0].addr = cpu_to_be64(frags->dma +
+ 						    XDP_PACKET_HEADROOM);
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  		return 0;
  	}
  
@@@ -577,34 -465,54 +610,82 @@@ static int mlx4_en_complete_rx_desc(str
  				    struct sk_buff *skb,
  				    int length)
  {
++<<<<<<< HEAD
 +	struct skb_frag_struct *skb_frags_rx = skb_shinfo(skb)->frags;
 +	struct mlx4_en_frag_info *frag_info;
 +	int nr;
++=======
+ 	const struct mlx4_en_frag_info *frag_info = priv->frag_info;
+ 	unsigned int truesize = 0;
+ 	int nr, frag_size;
+ 	struct page *page;
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  	dma_addr_t dma;
+ 	bool release;
  
  	/* Collect used fragments while replacing them in the HW descriptors */
++<<<<<<< HEAD
 +	for (nr = 0; nr < priv->num_frags; nr++) {
 +		frag_info = &priv->frag_info[nr];
 +		if (length <= frag_info->frag_prefix_size)
 +			break;
 +		if (unlikely(!frags[nr].page))
++=======
+ 	for (nr = 0;; frags++) {
+ 		frag_size = min_t(int, length, frag_info->frag_size);
+ 
+ 		page = frags->page;
+ 		if (unlikely(!page))
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  			goto fail;
  
- 		dma = be64_to_cpu(rx_desc->data[nr].addr);
- 		dma_sync_single_for_cpu(priv->ddev, dma, frag_info->frag_size,
- 					DMA_FROM_DEVICE);
+ 		dma = frags->dma;
+ 		dma_sync_single_range_for_cpu(priv->ddev, dma, frags->page_offset,
+ 					      frag_size, priv->dma_dir);
  
++<<<<<<< HEAD
 +		__skb_fill_page_desc(skb, nr, frags[nr].page,
 +				     frags[nr].page_offset,
 +				     frag_info->frag_size);
 +
 +		skb->truesize += frag_info->frag_stride;
 +		frags[nr].page = NULL;
 +	}
 +	/* Adjust size of last fragment to match actual length */
 +	if (nr > 0)
 +		skb_frag_size_set(&skb_frags_rx[nr - 1],
 +			length - priv->frag_info[nr - 1].frag_prefix_size);
++=======
+ 		__skb_fill_page_desc(skb, nr, page, frags->page_offset,
+ 				     frag_size);
+ 
+ 		truesize += frag_info->frag_stride;
+ 		if (frag_info->frag_stride == PAGE_SIZE / 2) {
+ 			frags->page_offset ^= PAGE_SIZE / 2;
+ 			release = page_count(page) != 1 ||
+ 				  page_is_pfmemalloc(page) ||
+ 				  page_to_nid(page) != numa_mem_id();
+ 		} else {
+ 			u32 sz_align = ALIGN(frag_size, SMP_CACHE_BYTES);
+ 
+ 			frags->page_offset += sz_align;
+ 			release = frags->page_offset + frag_info->frag_size > PAGE_SIZE;
+ 		}
+ 		if (release) {
+ 			dma_unmap_page(priv->ddev, dma, PAGE_SIZE, priv->dma_dir);
+ 			frags->page = NULL;
+ 		} else {
+ 			page_ref_inc(page);
+ 		}
+ 
+ 		nr++;
+ 		length -= frag_size;
+ 		if (!length)
+ 			break;
+ 		frag_info++;
+ 	}
+ 	skb->truesize += truesize;
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  	return nr;
  
  fail:
@@@ -886,10 -791,61 +968,66 @@@ int mlx4_en_process_rx_cq(struct net_de
  		 */
  		length = be32_to_cpu(cqe->byte_cnt);
  		length -= ring->fcs_del;
++<<<<<<< HEAD
++=======
+ 		l2_tunnel = (dev->hw_enc_features & NETIF_F_RXCSUM) &&
+ 			(cqe->vlan_my_qpn & cpu_to_be32(MLX4_CQE_L2_TUNNEL));
+ 
+ 		/* A bpf program gets first chance to drop the packet. It may
+ 		 * read bytes but not past the end of the frag.
+ 		 */
+ 		if (xdp_prog) {
+ 			struct xdp_buff xdp;
+ 			dma_addr_t dma;
+ 			void *orig_data;
+ 			u32 act;
+ 
+ 			dma = be64_to_cpu(rx_desc->data[0].addr);
+ 			dma_sync_single_for_cpu(priv->ddev, dma,
+ 						priv->frag_info[0].frag_size,
+ 						DMA_FROM_DEVICE);
+ 
+ 			xdp.data_hard_start = page_address(frags[0].page);
+ 			xdp.data = xdp.data_hard_start + frags[0].page_offset;
+ 			xdp.data_end = xdp.data + length;
+ 			orig_data = xdp.data;
+ 
+ 			act = bpf_prog_run_xdp(xdp_prog, &xdp);
+ 
+ 			if (xdp.data != orig_data) {
+ 				length = xdp.data_end - xdp.data;
+ 				frags[0].page_offset = xdp.data -
+ 					xdp.data_hard_start;
+ 			}
+ 
+ 			switch (act) {
+ 			case XDP_PASS:
+ 				break;
+ 			case XDP_TX:
+ 				if (likely(!mlx4_en_xmit_frame(ring, frags, dev,
+ 							length, cq->ring,
+ 							&doorbell_pending))) {
+ 					frags[0].page = NULL;
+ 					goto next;
+ 				}
+ 				trace_xdp_exception(dev, xdp_prog, act);
+ 				goto xdp_drop_no_cnt; /* Drop on xmit failure */
+ 			default:
+ 				bpf_warn_invalid_xdp_action(act);
+ 			case XDP_ABORTED:
+ 				trace_xdp_exception(dev, xdp_prog, act);
+ 			case XDP_DROP:
+ 				ring->xdp_drop++;
+ xdp_drop_no_cnt:
+ 				goto next;
+ 			}
+ 		}
+ 
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  		ring->bytes += length;
  		ring->packets++;
 +		l2_tunnel = (dev->hw_enc_features & NETIF_F_RXCSUM) &&
 +			(cqe->vlan_my_qpn & cpu_to_be32(MLX4_CQE_L2_TUNNEL));
  
  		if (likely(dev->features & NETIF_F_RXCSUM)) {
  			if (cqe->status & cpu_to_be16(MLX4_CQE_STATUS_TCP |
@@@ -1039,9 -994,6 +1176,12 @@@
  
  		napi_gro_receive(&cq->napi, skb);
  next:
++<<<<<<< HEAD
 +		for (nr = 0; nr < priv->num_frags; nr++)
 +			mlx4_en_free_frag(priv, frags, nr);
 +
++=======
++>>>>>>> 34db548bfb95 (mlx4: add page recycling in receive path)
  		++cq->mcq.cons_index;
  		index = (cq->mcq.cons_index) & ring->size_mask;
  		cqe = mlx4_en_get_cqe(cq->buf, index, priv->cqe_size) + factor;
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_rx.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h b/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
index d8f46d99701e..a5dedccf9862 100644
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
@@ -336,7 +336,6 @@ struct mlx4_en_rx_desc {
 
 struct mlx4_en_rx_ring {
 	struct mlx4_hwq_resources wqres;
-	struct mlx4_en_rx_alloc page_alloc[MLX4_EN_MAX_RX_FRAGS];
 	u32 size ;	/* number of Rx descs*/
 	u32 actual_size;
 	u32 size_mask;
