blk-mq: don't fail allocating driver tag for stopped hw queue

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jens Axboe <axboe@fb.com>
commit 12d70958a2e8d587acaa51dafd5d6620e00b7543
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/12d70958.failed

We rely on blk_mq_get_driver_tag() not failing if 'wait' is true,
but it currently fails in that case if the queue happens to be
stopped at the time of the call.

We don't need to check for stopped here, it's just assigning
the tag. If the queue is stopped, we'll handle it when
attempting to run the queue.

This fixes a stall/crash on flush intensive workloads, where
we proceed to process a flush that doesn't have a valid tag
assigned.

	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 12d70958a2e8d587acaa51dafd5d6620e00b7543)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index 3b21482b7f01,489076e7ae15..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -824,6 -847,74 +824,77 @@@ static inline unsigned int queued_to_in
  	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
  }
  
++<<<<<<< HEAD
++=======
+ bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
+ 			   bool wait)
+ {
+ 	struct blk_mq_alloc_data data = {
+ 		.q = rq->q,
+ 		.hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu),
+ 		.flags = wait ? 0 : BLK_MQ_REQ_NOWAIT,
+ 	};
+ 
+ 	if (rq->tag != -1) {
+ done:
+ 		if (hctx)
+ 			*hctx = data.hctx;
+ 		return true;
+ 	}
+ 
+ 	rq->tag = blk_mq_get_tag(&data);
+ 	if (rq->tag >= 0) {
+ 		if (blk_mq_tag_busy(data.hctx)) {
+ 			rq->rq_flags |= RQF_MQ_INFLIGHT;
+ 			atomic_inc(&data.hctx->nr_active);
+ 		}
+ 		data.hctx->tags->rqs[rq->tag] = rq;
+ 		goto done;
+ 	}
+ 
+ 	return false;
+ }
+ 
+ static void blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
+ 				  struct request *rq)
+ {
+ 	if (rq->tag == -1 || rq->internal_tag == -1)
+ 		return;
+ 
+ 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ 	rq->tag = -1;
+ 
+ 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+ 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+ 		atomic_dec(&hctx->nr_active);
+ 	}
+ }
+ 
+ /*
+  * If we fail getting a driver tag because all the driver tags are already
+  * assigned and on the dispatch list, BUT the first entry does not have a
+  * tag, then we could deadlock. For that case, move entries with assigned
+  * driver tags to the front, leaving the set of tagged requests in the
+  * same order, and the untagged set in the same order.
+  */
+ static bool reorder_tags_to_front(struct list_head *list)
+ {
+ 	struct request *rq, *tmp, *first = NULL;
+ 
+ 	list_for_each_entry_safe_reverse(rq, tmp, list, queuelist) {
+ 		if (rq == first)
+ 			break;
+ 		if (rq->tag != -1) {
+ 			list_move(&rq->queuelist, list);
+ 			if (!first)
+ 				first = rq;
+ 		}
+ 	}
+ 
+ 	return first != NULL;
+ }
+ 
++>>>>>>> 12d70958a2e8 (blk-mq: don't fail allocating driver tag for stopped hw queue)
  bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list)
  {
  	struct request_queue *q = hctx->queue;
* Unmerged path block/blk-mq.c
