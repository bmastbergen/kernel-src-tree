IB/mlx5: Expose Q counters groups only if they are supported by FW

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Kamal Heib <kamalh@mellanox.com>
commit 7c16f47779498650e9f11a395f8d63accedf35a3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7c16f477.failed

This patch modify the Q counters implementation, so each one of the
three Q counters groups will be exposed by the driver only if they are
supported by the firmware.

	Signed-off-by: Kamal Heib <kamalh@mellanox.com>
	Reviewed-by: Mark Bloch <markb@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 7c16f47779498650e9f11a395f8d63accedf35a3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/main.c
diff --cc drivers/infiniband/hw/mlx5/main.c
index 87c95640770e,f8fe98d22965..000000000000
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@@ -3013,32 -3072,142 +3013,171 @@@ static void mlx5_disable_eth(struct mlx
  		mlx5_nic_vport_disable_roce(dev->mdev);
  }
  
++<<<<<<< HEAD
 +static const char * const names[] = {
 +	"rx_write_requests",
 +	"rx_read_requests",
 +	"rx_atomic_requests",
 +	"out_of_buffer",
 +	"out_of_sequence",
 +	"duplicate_request",
 +	"rnr_nak_retry_err",
 +	"packet_seq_err",
 +	"implied_nak_seq_err",
 +	"local_ack_timeout_err",
 +};
 +
 +static const size_t stats_offsets[] = {
 +	MLX5_BYTE_OFF(query_q_counter_out, rx_write_requests),
 +	MLX5_BYTE_OFF(query_q_counter_out, rx_read_requests),
 +	MLX5_BYTE_OFF(query_q_counter_out, rx_atomic_requests),
 +	MLX5_BYTE_OFF(query_q_counter_out, out_of_buffer),
 +	MLX5_BYTE_OFF(query_q_counter_out, out_of_sequence),
 +	MLX5_BYTE_OFF(query_q_counter_out, duplicate_request),
 +	MLX5_BYTE_OFF(query_q_counter_out, rnr_nak_retry_err),
 +	MLX5_BYTE_OFF(query_q_counter_out, packet_seq_err),
 +	MLX5_BYTE_OFF(query_q_counter_out, implied_nak_seq_err),
 +	MLX5_BYTE_OFF(query_q_counter_out, local_ack_timeout_err),
 +};
 +
++=======
+ struct mlx5_ib_q_counter {
+ 	const char *name;
+ 	size_t offset;
+ };
+ 
+ #define INIT_Q_COUNTER(_name)		\
+ 	{ .name = #_name, .offset = MLX5_BYTE_OFF(query_q_counter_out, _name)}
+ 
+ static const struct mlx5_ib_q_counter basic_q_cnts[] = {
+ 	INIT_Q_COUNTER(rx_write_requests),
+ 	INIT_Q_COUNTER(rx_read_requests),
+ 	INIT_Q_COUNTER(rx_atomic_requests),
+ 	INIT_Q_COUNTER(out_of_buffer),
+ };
+ 
+ static const struct mlx5_ib_q_counter out_of_seq_q_cnts[] = {
+ 	INIT_Q_COUNTER(out_of_sequence),
+ };
+ 
+ static const struct mlx5_ib_q_counter retrans_q_cnts[] = {
+ 	INIT_Q_COUNTER(duplicate_request),
+ 	INIT_Q_COUNTER(rnr_nak_retry_err),
+ 	INIT_Q_COUNTER(packet_seq_err),
+ 	INIT_Q_COUNTER(implied_nak_seq_err),
+ 	INIT_Q_COUNTER(local_ack_timeout_err),
+ };
+ 
+ static void mlx5_ib_dealloc_q_counters(struct mlx5_ib_dev *dev)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < dev->num_ports; i++) {
+ 		mlx5_core_dealloc_q_counter(dev->mdev,
+ 					    dev->port[i].q_cnts.set_id);
+ 		kfree(dev->port[i].q_cnts.names);
+ 		kfree(dev->port[i].q_cnts.offsets);
+ 	}
+ }
+ 
+ static int __mlx5_ib_alloc_q_counters(struct mlx5_ib_dev *dev,
+ 				      const char ***names,
+ 				      size_t **offsets,
+ 				      u32 *num)
+ {
+ 	u32 num_counters;
+ 
+ 	num_counters = ARRAY_SIZE(basic_q_cnts);
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, out_of_seq_cnt))
+ 		num_counters += ARRAY_SIZE(out_of_seq_q_cnts);
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, retransmission_q_counters))
+ 		num_counters += ARRAY_SIZE(retrans_q_cnts);
+ 
+ 	*names = kcalloc(num_counters, sizeof(**names), GFP_KERNEL);
+ 	if (!*names)
+ 		return -ENOMEM;
+ 
+ 	*offsets = kcalloc(num_counters, sizeof(**offsets), GFP_KERNEL);
+ 	if (!*offsets)
+ 		goto err_names;
+ 
+ 	*num = num_counters;
+ 
+ 	return 0;
+ 
+ err_names:
+ 	kfree(*names);
+ 	return -ENOMEM;
+ }
+ 
+ static void mlx5_ib_fill_q_counters(struct mlx5_ib_dev *dev,
+ 				    const char **names,
+ 				    size_t *offsets)
+ {
+ 	int i;
+ 	int j = 0;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(basic_q_cnts); i++, j++) {
+ 		names[j] = basic_q_cnts[i].name;
+ 		offsets[j] = basic_q_cnts[i].offset;
+ 	}
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, out_of_seq_cnt)) {
+ 		for (i = 0; i < ARRAY_SIZE(out_of_seq_q_cnts); i++, j++) {
+ 			names[j] = out_of_seq_q_cnts[i].name;
+ 			offsets[j] = out_of_seq_q_cnts[i].offset;
+ 		}
+ 	}
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, retransmission_q_counters)) {
+ 		for (i = 0; i < ARRAY_SIZE(retrans_q_cnts); i++, j++) {
+ 			names[j] = retrans_q_cnts[i].name;
+ 			offsets[j] = retrans_q_cnts[i].offset;
+ 		}
+ 	}
+ }
+ 
+ static int mlx5_ib_alloc_q_counters(struct mlx5_ib_dev *dev)
+ {
+ 	int i;
+ 	int ret;
+ 
+ 	for (i = 0; i < dev->num_ports; i++) {
+ 		struct mlx5_ib_port *port = &dev->port[i];
+ 
+ 		ret = mlx5_core_alloc_q_counter(dev->mdev,
+ 						&port->q_cnts.set_id);
+ 		if (ret) {
+ 			mlx5_ib_warn(dev,
+ 				     "couldn't allocate queue counter for port %d, err %d\n",
+ 				     i + 1, ret);
+ 			goto dealloc_counters;
+ 		}
+ 
+ 		ret = __mlx5_ib_alloc_q_counters(dev,
+ 						 &port->q_cnts.names,
+ 						 &port->q_cnts.offsets,
+ 						 &port->q_cnts.num_counters);
+ 		if (ret)
+ 			goto dealloc_counters;
+ 
+ 		mlx5_ib_fill_q_counters(dev, port->q_cnts.names,
+ 					port->q_cnts.offsets);
+ 	}
+ 
+ 	return 0;
+ 
+ dealloc_counters:
+ 	while (--i >= 0)
+ 		mlx5_core_dealloc_q_counter(dev->mdev,
+ 					    dev->port[i].q_cnts.set_id);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 7c16f4777949 (IB/mlx5: Expose Q counters groups only if they are supported by FW)
  static struct rdma_hw_stats *mlx5_ib_alloc_hw_stats(struct ib_device *ibdev,
  						    u8 port_num)
  {
@@@ -3076,50 -3248,16 +3218,51 @@@ static int mlx5_ib_get_hw_stats(struct 
  	if (ret)
  		goto free;
  
- 	for (i = 0; i < ARRAY_SIZE(names); i++) {
- 		val = *(__be32 *)(out + stats_offsets[i]);
+ 	for (i = 0; i < port->q_cnts.num_counters; i++) {
+ 		val = *(__be32 *)(out + port->q_cnts.offsets[i]);
  		stats->value[i] = (u64)be32_to_cpu(val);
  	}
+ 
  free:
  	kvfree(out);
- 	return ARRAY_SIZE(names);
+ 	return port->q_cnts.num_counters;
  }
  
 +static void mlx5_ib_dealloc_q_counters(struct mlx5_ib_dev *dev)
 +{
 +	unsigned int i;
 +
 +	for (i = 0; i < dev->num_ports; i++)
 +		mlx5_core_dealloc_q_counter(dev->mdev,
 +					    dev->port[i].q_cnt_id);
 +}
 +
 +static int mlx5_ib_alloc_q_counters(struct mlx5_ib_dev *dev)
 +{
 +	int i;
 +	int ret;
 +
 +	for (i = 0; i < dev->num_ports; i++) {
 +		ret = mlx5_core_alloc_q_counter(dev->mdev,
 +						&dev->port[i].q_cnt_id);
 +		if (ret) {
 +			mlx5_ib_warn(dev,
 +				     "couldn't allocate queue counter for port %d, err %d\n",
 +				     i + 1, ret);
 +			goto dealloc_counters;
 +		}
 +	}
 +
 +	return 0;
 +
 +dealloc_counters:
 +	while (--i >= 0)
 +		mlx5_core_dealloc_q_counter(dev->mdev,
 +					    dev->port[i].q_cnt_id);
 +
 +	return ret;
 +}
 +
  static void *mlx5_ib_add(struct mlx5_core_dev *mdev)
  {
  	struct mlx5_ib_dev *dev;
@@@ -3271,7 -3407,7 +3414,11 @@@
  			(1ull << IB_USER_VERBS_CMD_DEALLOC_MW);
  	}
  
++<<<<<<< HEAD
 +	if (MLX5_CAP_GEN(dev->mdev, out_of_seq_cnt)) {
++=======
+ 	if (MLX5_CAP_GEN(dev->mdev, max_qp_cnt)) {
++>>>>>>> 7c16f4777949 (IB/mlx5: Expose Q counters groups only if they are supported by FW)
  		dev->ib_dev.get_hw_stats	= mlx5_ib_get_hw_stats;
  		dev->ib_dev.alloc_hw_stats	= mlx5_ib_alloc_hw_stats;
  	}
* Unmerged path drivers/infiniband/hw/mlx5/main.c
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6922362fef46..4e7b6e75aa7f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -605,8 +605,15 @@ struct mlx5_ib_resources {
 	struct mutex	mutex;
 };
 
+struct mlx5_ib_q_counters {
+	const char **names;
+	size_t *offsets;
+	u32 num_counters;
+	u16 set_id;
+};
+
 struct mlx5_ib_port {
-	u16 q_cnt_id;
+	struct mlx5_ib_q_counters q_cnts;
 };
 
 struct mlx5_roce {
diff --git a/drivers/infiniband/hw/mlx5/qp.c b/drivers/infiniband/hw/mlx5/qp.c
index 49c23de28351..fea9a51a8b77 100644
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -2826,7 +2826,7 @@ static int __mlx5_ib_modify_qp(struct ib_qp *ibqp,
 			       qp->port) - 1;
 		mibport = &dev->port[port_num];
 		context->qp_counter_set_usr_page |=
-			cpu_to_be32((u32)(mibport->q_cnt_id) << 24);
+			cpu_to_be32((u32)(mibport->q_cnts.set_id) << 24);
 	}
 
 	if (!ibqp->uobject && cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT)
@@ -2864,7 +2864,7 @@ static int __mlx5_ib_modify_qp(struct ib_qp *ibqp,
 
 		raw_qp_param.operation = op;
 		if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT) {
-			raw_qp_param.rq_q_ctr_id = mibport->q_cnt_id;
+			raw_qp_param.rq_q_ctr_id = mibport->q_cnts.set_id;
 			raw_qp_param.set_mask |= MLX5_RAW_QP_MOD_SET_RQ_Q_CTR_ID;
 		}
 
