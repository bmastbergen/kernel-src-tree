nfp: map all queue controllers at once

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jakub Kicinski <jakub.kicinski@netronome.com>
commit 73e253f0e5d7557650159ecfac5b2653b6d02cf0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/73e253f0.failed

RX and TX queue controllers are interleaved.  Instead of creating
two mappings which map the same area at slightly different offset,
create only one mapping.  Always map all queue controllers to simplify
the code and allow reusing the mapping for non-data vNICs.

	Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 73e253f0e5d7557650159ecfac5b2653b6d02cf0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/netronome/nfp/nfp_main.h
#	drivers/net/ethernet/netronome/nfp/nfp_net_main.c
diff --cc drivers/net/ethernet/netronome/nfp/nfp_main.h
index 1ac430fbaa18,66b1e1490805..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_main.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_main.h
@@@ -57,10 -61,10 +57,17 @@@ struct nfp_eth_table
   * struct nfp_pf - NFP PF-specific device structure
   * @pdev:		Backpointer to PCI device
   * @cpp:		Pointer to the CPP handle
++<<<<<<< HEAD
 + * @ctrl_area:		Pointer to the CPP area for the control BAR
 + * @tx_area:		Pointer to the CPP area for the TX queues
 + * @rx_area:		Pointer to the CPP area for the FL/RX queues
 + * @irq_entries:	Array of MSI-X entries for all ports
++=======
+  * @app:		Pointer to the APP handle
+  * @data_vnic_bar:	Pointer to the CPP area for the data vNICs' BARs
+  * @qc_area:		Pointer to the CPP area for the queues
+  * @irq_entries:	Array of MSI-X entries for all vNICs
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
   * @limit_vfs:		Number of VFs supported by firmware (~0 for PCI limit)
   * @num_vfs:		Number of SR-IOV VFs enabled
   * @fw_loaded:		Is the firmware loaded?
@@@ -76,9 -84,10 +83,16 @@@ struct nfp_pf 
  
  	struct nfp_cpp *cpp;
  
++<<<<<<< HEAD
 +	struct nfp_cpp_area *ctrl_area;
 +	struct nfp_cpp_area *tx_area;
 +	struct nfp_cpp_area *rx_area;
++=======
+ 	struct nfp_app *app;
+ 
+ 	struct nfp_cpp_area *data_vnic_bar;
+ 	struct nfp_cpp_area *qc_area;
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  
  	struct msix_entry *irq_entries;
  
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_main.c
index acc8cfe284f3,2a3b6deae607..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_main.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_main.c
@@@ -207,68 -212,59 +207,85 @@@ static unsigned int nfp_net_pf_get_num_
  	return val;
  }
  
++<<<<<<< HEAD
 +static unsigned int
 +nfp_net_pf_total_qcs(struct nfp_pf *pf, void __iomem *ctrl_bar,
 +		     unsigned int stride, u32 start_off, u32 num_off)
 +{
 +	unsigned int i, min_qc, max_qc;
 +
 +	min_qc = readl(ctrl_bar + start_off);
 +	max_qc = min_qc;
 +
 +	for (i = 0; i < pf->num_ports; i++) {
 +		/* To make our lives simpler only accept configuration where
 +		 * queues are allocated to PFs in order (queues of PFn all have
 +		 * indexes lower than PFn+1).
 +		 */
 +		if (max_qc > readl(ctrl_bar + start_off))
 +			return 0;
 +
 +		max_qc = readl(ctrl_bar + start_off);
 +		max_qc += readl(ctrl_bar + num_off) * stride;
 +		ctrl_bar += NFP_PF_CSR_SLICE_SIZE;
 +	}
 +
 +	return max_qc - min_qc;
 +}
 +
 +static u8 __iomem *nfp_net_pf_map_ctrl_bar(struct nfp_pf *pf)
++=======
+ static int nfp_net_pf_get_num_ports(struct nfp_pf *pf)
+ {
+ 	return nfp_net_pf_rtsym_read_optional(pf, "nfd_cfg_pf%u_num_ports", 1);
+ }
+ 
+ static int nfp_net_pf_get_app_id(struct nfp_pf *pf)
+ {
+ 	return nfp_net_pf_rtsym_read_optional(pf, "_pf%u_net_app_id",
+ 					      NFP_APP_CORE_NIC);
+ }
+ 
+ static u8 __iomem *
+ nfp_net_pf_map_rtsym(struct nfp_pf *pf, const char *name, const char *sym_fmt,
+ 		     unsigned int min_size, struct nfp_cpp_area **area)
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  {
 -	const struct nfp_rtsym *sym;
 +	const struct nfp_rtsym *ctrl_sym;
 +	u8 __iomem *ctrl_bar;
  	char pf_symbol[256];
 -	u8 __iomem *mem;
  
 -	snprintf(pf_symbol, sizeof(pf_symbol), sym_fmt,
 +	snprintf(pf_symbol, sizeof(pf_symbol), "_pf%u_net_bar0",
  		 nfp_cppcore_pcie_unit(pf->cpp));
  
 -	sym = nfp_rtsym_lookup(pf->cpp, pf_symbol);
 -	if (!sym) {
 -		nfp_err(pf->cpp, "Failed to find PF symbol %s\n", pf_symbol);
 -		return (u8 __iomem *)ERR_PTR(-ENOENT);
 +	ctrl_sym = nfp_rtsym_lookup(pf->cpp, pf_symbol);
 +	if (!ctrl_sym) {
 +		dev_err(&pf->pdev->dev,
 +			"Failed to find PF BAR0 symbol %s\n", pf_symbol);
 +		return NULL;
  	}
  
 -	if (sym->size < min_size) {
 -		nfp_err(pf->cpp, "PF symbol %s too small\n", pf_symbol);
 -		return (u8 __iomem *)ERR_PTR(-EINVAL);
 +	if (ctrl_sym->size < pf->num_ports * NFP_PF_CSR_SLICE_SIZE) {
 +		dev_err(&pf->pdev->dev,
 +			"PF BAR0 too small to contain %d ports\n",
 +			pf->num_ports);
 +		return NULL;
  	}
  
 -	mem = nfp_net_map_area(pf->cpp, name, sym->domain, sym->target,
 -			       sym->addr, sym->size, area);
 -	if (IS_ERR(mem)) {
 -		nfp_err(pf->cpp, "Failed to map PF symbol %s: %ld\n",
 -			pf_symbol, PTR_ERR(mem));
 -		return mem;
 +	ctrl_bar = nfp_net_map_area(pf->cpp, "net.ctrl",
 +				    ctrl_sym->domain, ctrl_sym->target,
 +				    ctrl_sym->addr, ctrl_sym->size,
 +				    &pf->ctrl_area);
 +	if (IS_ERR(ctrl_bar)) {
 +		dev_err(&pf->pdev->dev, "Failed to map PF BAR0: %ld\n",
 +			PTR_ERR(ctrl_bar));
 +		return NULL;
  	}
  
 -	return mem;
 -}
 -
 -static void nfp_net_pf_free_vnic(struct nfp_pf *pf, struct nfp_net *nn)
 -{
 -	nfp_port_free(nn->port);
 -	list_del(&nn->vnic_list);
 -	pf->num_vnics--;
 -	nfp_net_free(nn);
 +	return ctrl_bar;
  }
  
 -static void nfp_net_pf_free_vnics(struct nfp_pf *pf)
 +static void nfp_net_pf_free_netdevs(struct nfp_pf *pf)
  {
  	struct nfp_net *nn;
  
@@@ -281,13 -275,17 +298,22 @@@
  }
  
  static struct nfp_net *
++<<<<<<< HEAD
 +nfp_net_pf_alloc_port_netdev(struct nfp_pf *pf, void __iomem *ctrl_bar,
 +			     void __iomem *tx_bar, void __iomem *rx_bar,
 +			     int stride, struct nfp_net_fw_version *fw_ver)
++=======
+ nfp_net_pf_alloc_vnic(struct nfp_pf *pf, bool needs_netdev,
+ 		      void __iomem *ctrl_bar, void __iomem *qc_bar,
+ 		      int stride, struct nfp_net_fw_version *fw_ver,
+ 		      unsigned int eth_id)
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  {
- 	u32 n_tx_rings, n_rx_rings;
+ 	u32 tx_base, rx_base, n_tx_rings, n_rx_rings;
  	struct nfp_net *nn;
 -	int err;
  
+ 	tx_base = readl(ctrl_bar + NFP_NET_CFG_START_TXQ);
+ 	rx_base = readl(ctrl_bar + NFP_NET_CFG_START_RXQ);
  	n_tx_rings = readl(ctrl_bar + NFP_NET_CFG_MAX_TXRINGS);
  	n_rx_rings = readl(ctrl_bar + NFP_NET_CFG_MAX_RXRINGS);
  
@@@ -296,12 -294,12 +322,19 @@@
  	if (IS_ERR(nn))
  		return nn;
  
 -	nn->app = pf->app;
 +	nn->cpp = pf->cpp;
  	nn->fw_ver = *fw_ver;
++<<<<<<< HEAD
 +	nn->ctrl_bar = ctrl_bar;
 +	nn->tx_bar = tx_bar;
 +	nn->rx_bar = rx_bar;
 +	nn->is_vf = 0;
++=======
+ 	nn->dp.ctrl_bar = ctrl_bar;
+ 	nn->tx_bar = qc_bar + tx_base * NFP_QCP_QUEUE_ADDR_SZ;
+ 	nn->rx_bar = qc_bar + rx_base * NFP_QCP_QUEUE_ADDR_SZ;
+ 	nn->dp.is_vf = 0;
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  	nn->stride_rx = stride;
  	nn->stride_tx = stride;
  
@@@ -335,28 -344,22 +368,39 @@@ nfp_net_pf_init_port_netdev(struct nfp_
  }
  
  static int
++<<<<<<< HEAD
 +nfp_net_pf_alloc_netdevs(struct nfp_pf *pf, void __iomem *ctrl_bar,
 +			 void __iomem *tx_bar, void __iomem *rx_bar,
 +			 int stride, struct nfp_net_fw_version *fw_ver)
++=======
+ nfp_net_pf_alloc_vnics(struct nfp_pf *pf, void __iomem *ctrl_bar,
+ 		       void __iomem *qc_bar, int stride,
+ 		       struct nfp_net_fw_version *fw_ver)
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  {
- 	u32 prev_tx_base, prev_rx_base, tgt_tx_base, tgt_rx_base;
  	struct nfp_net *nn;
  	unsigned int i;
  	int err;
  
++<<<<<<< HEAD
 +	prev_tx_base = readl(ctrl_bar + NFP_NET_CFG_START_TXQ);
 +	prev_rx_base = readl(ctrl_bar + NFP_NET_CFG_START_RXQ);
 +
 +	for (i = 0; i < pf->num_ports; i++) {
 +		tgt_tx_base = readl(ctrl_bar + NFP_NET_CFG_START_TXQ);
 +		tgt_rx_base = readl(ctrl_bar + NFP_NET_CFG_START_RXQ);
 +		tx_bar += (tgt_tx_base - prev_tx_base) * NFP_QCP_QUEUE_ADDR_SZ;
 +		rx_bar += (tgt_rx_base - prev_rx_base) * NFP_QCP_QUEUE_ADDR_SZ;
 +		prev_tx_base = tgt_tx_base;
 +		prev_rx_base = tgt_rx_base;
 +
 +		nn = nfp_net_pf_alloc_port_netdev(pf, ctrl_bar, tx_bar, rx_bar,
 +						  stride, fw_ver);
++=======
+ 	for (i = 0; i < pf->max_data_vnics; i++) {
+ 		nn = nfp_net_pf_alloc_vnic(pf, true, ctrl_bar, qc_bar,
+ 					   stride, fw_ver, i);
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  		if (IS_ERR(nn)) {
  			err = PTR_ERR(nn);
  			goto err_free_prev;
@@@ -373,19 -384,26 +417,30 @@@ err_free_prev
  	return err;
  }
  
 -static void nfp_net_pf_clean_vnic(struct nfp_pf *pf, struct nfp_net *nn)
 -{
 -	if (nn->port)
 -		nfp_devlink_port_unregister(nn->port);
 -	nfp_net_debugfs_dir_clean(&nn->debugfs_dir);
 -	nfp_net_clean(nn);
 -	nfp_app_vnic_clean(pf->app, nn);
 -}
 -
  static int
++<<<<<<< HEAD
 +nfp_net_pf_spawn_netdevs(struct nfp_pf *pf,
 +			 void __iomem *ctrl_bar, void __iomem *tx_bar,
 +			 void __iomem *rx_bar, int stride,
 +			 struct nfp_net_fw_version *fw_ver)
++=======
+ nfp_net_pf_spawn_vnics(struct nfp_pf *pf,
+ 		       void __iomem *ctrl_bar, void __iomem *qc_bar, int stride,
+ 		       struct nfp_net_fw_version *fw_ver)
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  {
 -	unsigned int id, wanted_irqs, num_irqs, vnics_left, irqs_left;
 +	unsigned int id, wanted_irqs, num_irqs, ports_left, irqs_left;
  	struct nfp_net *nn;
  	int err;
  
++<<<<<<< HEAD
 +	/* Allocate the netdevs and do basic init */
 +	err = nfp_net_pf_alloc_netdevs(pf, ctrl_bar, tx_bar, rx_bar,
 +				       stride, fw_ver);
++=======
+ 	/* Allocate the vnics and do basic init */
+ 	err = nfp_net_pf_alloc_vnics(pf, ctrl_bar, qc_bar, stride, fw_ver);
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  	if (err)
  		return err;
  
@@@ -454,72 -495,125 +509,79 @@@ static void nfp_net_pci_remove_finish(s
  	nfp_net_irqs_disable(pf->pdev);
  	kfree(pf->irq_entries);
  
++<<<<<<< HEAD
 +	nfp_cpp_area_release_free(pf->rx_area);
 +	nfp_cpp_area_release_free(pf->tx_area);
 +	nfp_cpp_area_release_free(pf->ctrl_area);
++=======
+ 	nfp_net_pf_app_clean(pf);
+ 
+ 	nfp_cpp_area_release_free(pf->qc_area);
+ 	nfp_cpp_area_release_free(pf->data_vnic_bar);
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  }
  
 -static int
 -nfp_net_eth_port_update(struct nfp_cpp *cpp, struct nfp_port *port,
 -			struct nfp_eth_table *eth_table)
 -{
 -	struct nfp_eth_table_port *eth_port;
 -
 -	ASSERT_RTNL();
 -
 -	eth_port = nfp_net_find_port(eth_table, port->eth_id);
 -	if (!eth_port) {
 -		set_bit(NFP_PORT_CHANGED, &port->flags);
 -		nfp_warn(cpp, "Warning: port #%d not present after reconfig\n",
 -			 port->eth_id);
 -		return -EIO;
 -	}
 -	if (eth_port->override_changed) {
 -		nfp_warn(cpp, "Port #%d config changed, unregistering. Reboot required before port will be operational again.\n", port->eth_id);
 -		port->type = NFP_PORT_INVALID;
 -	}
 -
 -	memcpy(port->eth_port, eth_port, sizeof(*eth_port));
 -
 -	return 0;
 -}
 -
 -int nfp_net_refresh_port_table_sync(struct nfp_pf *pf)
 +static void nfp_net_refresh_netdevs(struct work_struct *work)
  {
 -	struct nfp_eth_table *eth_table;
 +	struct nfp_pf *pf = container_of(work, struct nfp_pf,
 +					 port_refresh_work);
  	struct nfp_net *nn, *next;
 -	struct nfp_port *port;
  
 -	lockdep_assert_held(&pf->lock);
 +	mutex_lock(&pf->port_lock);
  
  	/* Check for nfp_net_pci_remove() racing against us */
 -	if (list_empty(&pf->vnics))
 -		return 0;
 -
 -	/* Update state of all ports */
 -	rtnl_lock();
 -	list_for_each_entry(port, &pf->ports, port_list)
 -		clear_bit(NFP_PORT_CHANGED, &port->flags);
 -
 -	eth_table = nfp_eth_read_ports(pf->cpp);
 -	if (!eth_table) {
 -		list_for_each_entry(port, &pf->ports, port_list)
 -			if (__nfp_port_get_eth_port(port))
 -				set_bit(NFP_PORT_CHANGED, &port->flags);
 -		rtnl_unlock();
 -		nfp_err(pf->cpp, "Error refreshing port config!\n");
 -		return -EIO;
 -	}
 +	if (list_empty(&pf->ports))
 +		goto out;
  
 -	list_for_each_entry(port, &pf->ports, port_list)
 -		if (__nfp_port_get_eth_port(port))
 -			nfp_net_eth_port_update(pf->cpp, port, eth_table);
 -	rtnl_unlock();
 +	list_for_each_entry_safe(nn, next, &pf->ports, port_list) {
 +		if (!nn->eth_port) {
 +			nfp_warn(pf->cpp, "Warning: port not present after reconfig\n");
 +			continue;
 +		}
 +		if (!nn->eth_port->override_changed)
 +			continue;
  
 -	kfree(eth_table);
 +		nn_warn(nn, "Port config changed, unregistering. Reboot required before port will be operational again.\n");
  
 -	/* Shoot off the ports which became invalid */
 -	list_for_each_entry_safe(nn, next, &pf->vnics, vnic_list) {
 -		if (!nn->port || nn->port->type != NFP_PORT_INVALID)
 -			continue;
 +		nfp_net_debugfs_dir_clean(&nn->debugfs_dir);
 +		nfp_net_netdev_clean(nn->dp.netdev);
  
 -		nfp_net_pf_clean_vnic(pf, nn);
 -		nfp_net_pf_free_vnic(pf, nn);
 +		list_del(&nn->port_list);
 +		pf->num_netdevs--;
 +		nfp_net_netdev_free(nn);
  	}
  
 -	if (list_empty(&pf->vnics))
 +	if (list_empty(&pf->ports))
  		nfp_net_pci_remove_finish(pf);
 -
 -	return 0;
 -}
 -
 -static void nfp_net_refresh_vnics(struct work_struct *work)
 -{
 -	struct nfp_pf *pf = container_of(work, struct nfp_pf,
 -					 port_refresh_work);
 -
 -	mutex_lock(&pf->lock);
 -	nfp_net_refresh_port_table_sync(pf);
 -	mutex_unlock(&pf->lock);
 +out:
 +	mutex_unlock(&pf->port_lock);
  }
  
 -void nfp_net_refresh_port_table(struct nfp_port *port)
 +void nfp_net_refresh_port_config(struct nfp_net *nn)
  {
 -	struct nfp_pf *pf = port->app->pf;
 -
 -	set_bit(NFP_PORT_CHANGED, &port->flags);
 +	struct nfp_pf *pf = pci_get_drvdata(nn->pdev);
 +	struct nfp_eth_table *old_table;
  
 -	schedule_work(&pf->port_refresh_work);
 -}
 +	ASSERT_RTNL();
  
 -int nfp_net_refresh_eth_port(struct nfp_port *port)
 -{
 -	struct nfp_cpp *cpp = port->app->cpp;
 -	struct nfp_eth_table *eth_table;
 -	int ret;
 +	old_table = pf->eth_tbl;
  
 -	clear_bit(NFP_PORT_CHANGED, &port->flags);
 +	list_for_each_entry(nn, &pf->ports, port_list)
 +		nfp_net_link_changed_read_clear(nn);
  
 -	eth_table = nfp_eth_read_ports(cpp);
 -	if (!eth_table) {
 -		set_bit(NFP_PORT_CHANGED, &port->flags);
 -		nfp_err(cpp, "Error refreshing port state table!\n");
 -		return -EIO;
 +	pf->eth_tbl = nfp_eth_read_ports(pf->cpp);
 +	if (!pf->eth_tbl) {
 +		pf->eth_tbl = old_table;
 +		nfp_err(pf->cpp, "Error refreshing port config!\n");
 +		return;
  	}
  
 -	ret = nfp_net_eth_port_update(cpp, port, eth_table);
 +	list_for_each_entry(nn, &pf->ports, port_list)
 +		nn->eth_port = nfp_net_find_port(pf, nn->eth_port->eth_index);
  
 -	kfree(eth_table);
 +	kfree(old_table);
  
 -	return ret;
 +	schedule_work(&pf->port_refresh_work);
  }
  
  /*
@@@ -527,11 -621,9 +589,17 @@@
   */
  int nfp_net_pci_probe(struct nfp_pf *pf)
  {
++<<<<<<< HEAD
 +	u8 __iomem *ctrl_bar, *tx_bar, *rx_bar;
 +	u32 total_tx_qcs, total_rx_qcs;
 +	struct nfp_net_fw_version fw_ver;
 +	u32 tx_area_sz, rx_area_sz;
 +	u32 start_q;
++=======
+ 	struct nfp_net_fw_version fw_ver;
+ 	u8 __iomem *ctrl_bar, *qc_bar;
+ 	u32 ctrl_bar_sz;
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  	int stride;
  	int err;
  
@@@ -593,35 -688,13 +657,45 @@@
  		goto err_ctrl_unmap;
  	}
  
++<<<<<<< HEAD
 +	tx_area_sz = NFP_QCP_QUEUE_ADDR_SZ * total_tx_qcs;
 +	rx_area_sz = NFP_QCP_QUEUE_ADDR_SZ * total_rx_qcs;
 +
 +	/* Map TX queues */
 +	start_q = readl(ctrl_bar + NFP_NET_CFG_START_TXQ);
 +	tx_bar = nfp_net_map_area(pf->cpp, "net.tx", 0, 0,
 +				  NFP_PCIE_QUEUE(start_q),
 +				  tx_area_sz, &pf->tx_area);
 +	if (IS_ERR(tx_bar)) {
 +		nfp_err(pf->cpp, "Failed to map TX area.\n");
 +		err = PTR_ERR(tx_bar);
 +		goto err_ctrl_unmap;
 +	}
 +
 +	/* Map RX queues */
 +	start_q = readl(ctrl_bar + NFP_NET_CFG_START_RXQ);
 +	rx_bar = nfp_net_map_area(pf->cpp, "net.rx", 0, 0,
 +				  NFP_PCIE_QUEUE(start_q),
 +				  rx_area_sz, &pf->rx_area);
 +	if (IS_ERR(rx_bar)) {
 +		nfp_err(pf->cpp, "Failed to map RX area.\n");
 +		err = PTR_ERR(rx_bar);
 +		goto err_unmap_tx;
 +	}
 +
 +	pf->ddir = nfp_net_debugfs_device_add(pf->pdev);
 +
 +	err = nfp_net_pf_spawn_netdevs(pf, ctrl_bar, tx_bar, rx_bar,
 +				       stride, &fw_ver);
++=======
+ 	err = nfp_net_pf_app_init(pf);
+ 	if (err)
+ 		goto err_unmap_qc;
+ 
+ 	pf->ddir = nfp_net_debugfs_device_add(pf->pdev);
+ 
+ 	err = nfp_net_pf_spawn_vnics(pf, ctrl_bar, qc_bar, stride, &fw_ver);
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  	if (err)
  		goto err_clean_ddir;
  
@@@ -631,13 -704,13 +705,19 @@@
  
  err_clean_ddir:
  	nfp_net_debugfs_dir_clean(&pf->ddir);
++<<<<<<< HEAD
 +	nfp_cpp_area_release_free(pf->rx_area);
 +err_unmap_tx:
 +	nfp_cpp_area_release_free(pf->tx_area);
++=======
+ 	nfp_net_pf_app_clean(pf);
+ err_unmap_qc:
+ 	nfp_cpp_area_release_free(pf->qc_area);
++>>>>>>> 73e253f0e5d7 (nfp: map all queue controllers at once)
  err_ctrl_unmap:
 -	nfp_cpp_area_release_free(pf->data_vnic_bar);
 +	nfp_cpp_area_release_free(pf->ctrl_area);
  err_unlock:
 -	mutex_unlock(&pf->lock);
 +	mutex_unlock(&pf->port_lock);
  	return err;
  }
  
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_main.h
diff --git a/drivers/net/ethernet/netronome/nfp/nfp_net.h b/drivers/net/ethernet/netronome/nfp/nfp_net.h
index 600c79f39fe0..cb31a1383724 100644
--- a/drivers/net/ethernet/netronome/nfp/nfp_net.h
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net.h
@@ -623,6 +623,7 @@ static inline void nn_pci_flush(struct nfp_net *nn)
  * either add to a pointer or to read the pointer value.
  */
 #define NFP_QCP_QUEUE_ADDR_SZ			0x800
+#define NFP_QCP_QUEUE_AREA_SZ			0x80000
 #define NFP_QCP_QUEUE_OFF(_x)			((_x) * NFP_QCP_QUEUE_ADDR_SZ)
 #define NFP_QCP_QUEUE_ADD_RPTR			0x0000
 #define NFP_QCP_QUEUE_ADD_WPTR			0x0004
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_main.c
