iommu/amd: Flush iova queue before releasing dma_ops_domain

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] amd: Flush iova queue before releasing dma_ops_domain (Jerry Snitselaar) [1411581]
Rebuild_FUZZ: 94.64%
commit-author Joerg Roedel <jroedel@suse.de>
commit 281e8ccbff172899a60579773e72ad63d58b3770
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/281e8ccb.failed

Before a dma_ops_domain can be freed, we need to make sure
it is not longer referenced by the flush queue. So empty the
queue before a dma_ops_domain can be freed.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 281e8ccbff172899a60579773e72ad63d58b3770)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index c8c3026772fe,d13a18633dce..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -2408,6 -2123,92 +2408,95 @@@ static struct iommu_group *amd_iommu_de
   *
   *****************************************************************************/
  
++<<<<<<< HEAD
++=======
+ static void __queue_flush(struct flush_queue *queue)
+ {
+ 	struct protection_domain *domain;
+ 	unsigned long flags;
+ 	int idx;
+ 
+ 	/* First flush TLB of all known domains */
+ 	spin_lock_irqsave(&amd_iommu_pd_lock, flags);
+ 	list_for_each_entry(domain, &amd_iommu_pd_list, list)
+ 		domain_flush_tlb(domain);
+ 	spin_unlock_irqrestore(&amd_iommu_pd_lock, flags);
+ 
+ 	/* Wait until flushes have completed */
+ 	domain_flush_complete(NULL);
+ 
+ 	for (idx = 0; idx < queue->next; ++idx) {
+ 		struct flush_queue_entry *entry;
+ 
+ 		entry = queue->entries + idx;
+ 
+ 		free_iova_fast(&entry->dma_dom->iovad,
+ 				entry->iova_pfn,
+ 				entry->pages);
+ 
+ 		/* Not really necessary, just to make sure we catch any bugs */
+ 		entry->dma_dom = NULL;
+ 	}
+ 
+ 	queue->next = 0;
+ }
+ 
+ static void queue_flush_all(void)
+ {
+ 	int cpu;
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct flush_queue *queue;
+ 		unsigned long flags;
+ 
+ 		queue = per_cpu_ptr(&flush_queue, cpu);
+ 		spin_lock_irqsave(&queue->lock, flags);
+ 		if (queue->next > 0)
+ 			__queue_flush(queue);
+ 		spin_unlock_irqrestore(&queue->lock, flags);
+ 	}
+ }
+ 
+ static void queue_flush_timeout(unsigned long unsused)
+ {
+ 	atomic_set(&queue_timer_on, 0);
+ 	queue_flush_all();
+ }
+ 
+ static void queue_add(struct dma_ops_domain *dma_dom,
+ 		      unsigned long address, unsigned long pages)
+ {
+ 	struct flush_queue_entry *entry;
+ 	struct flush_queue *queue;
+ 	unsigned long flags;
+ 	int idx;
+ 
+ 	pages     = __roundup_pow_of_two(pages);
+ 	address >>= PAGE_SHIFT;
+ 
+ 	queue = get_cpu_ptr(&flush_queue);
+ 	spin_lock_irqsave(&queue->lock, flags);
+ 
+ 	if (queue->next == FLUSH_QUEUE_SIZE)
+ 		__queue_flush(queue);
+ 
+ 	idx   = queue->next++;
+ 	entry = queue->entries + idx;
+ 
+ 	entry->iova_pfn = address;
+ 	entry->pages    = pages;
+ 	entry->dma_dom  = dma_dom;
+ 
+ 	spin_unlock_irqrestore(&queue->lock, flags);
+ 
+ 	if (atomic_cmpxchg(&queue_timer_on, 0, 1) == 0)
+ 		mod_timer(&queue_timer, jiffies + msecs_to_jiffies(10));
+ 
+ 	put_cpu_ptr(&flush_queue);
+ }
+ 
+ 
++>>>>>>> 281e8ccbff17 (iommu/amd: Flush iova queue before releasing dma_ops_domain)
  /*
   * In the dma_ops path we only have the struct device. This function
   * finds the corresponding IOMMU, the protection domain and the
* Unmerged path drivers/iommu/amd_iommu.c
