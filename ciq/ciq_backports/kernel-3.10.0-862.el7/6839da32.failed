net: sched: cls_bpf: no need to call tcf_exts_change for newly allocated struct

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [net] sched: cls_bpf: no need to call tcf_exts_change for newly allocated struct (Ivan Vecera) [1445420]
Rebuild_FUZZ: 96.73%
commit-author Jiri Pirko <jiri@mellanox.com>
commit 6839da326dfcb98bf8020d42b416636471f1e462
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6839da32.failed

As the prog struct was allocated right before cls_bpf_set_parms call,
no need to use tcf_exts_change to do atomic change, and we can just
fill-up the unused exts struct directly by tcf_exts_validate.

	Signed-off-by: Jiri Pirko <jiri@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 6839da326dfcb98bf8020d42b416636471f1e462)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_bpf.c
diff --cc net/sched/cls_bpf.c
index c13fb5505297,cf248c3137ad..000000000000
--- a/net/sched/cls_bpf.c
+++ b/net/sched/cls_bpf.c
@@@ -193,27 -334,100 +193,110 @@@ static int cls_bpf_modify_existing(stru
  
  	memcpy(bpf_ops, nla_data(tb[TCA_BPF_OPS]), bpf_size);
  
 -	fprog_tmp.len = bpf_num_ops;
 -	fprog_tmp.filter = bpf_ops;
 +	tmp.len = bpf_len;
 +	tmp.filter = (struct sock_filter __user *) bpf_ops;
  
 -	ret = bpf_prog_create(&fp, &fprog_tmp);
 -	if (ret < 0) {
 -		kfree(bpf_ops);
 -		return ret;
 -	}
 +	ret = sk_unattached_filter_create(&fp, &tmp);
 +	if (ret)
 +		goto errout_free;
  
 +	prog->bpf_len = bpf_len;
  	prog->bpf_ops = bpf_ops;
 -	prog->bpf_num_ops = bpf_num_ops;
 -	prog->bpf_name = NULL;
  	prog->filter = fp;
 +	prog->res.classid = classid;
 +
++<<<<<<< HEAD
 +	tcf_bind_filter(tp, &prog->res, base);
 +	tcf_exts_change(tp, &prog->exts, &exts);
  
 +	return 0;
 +errout_free:
 +	kfree(bpf_ops);
 +errout:
 +	tcf_exts_destroy(&exts);
 +	return ret;
++=======
+ 	return 0;
+ }
+ 
+ static int cls_bpf_prog_from_efd(struct nlattr **tb, struct cls_bpf_prog *prog,
+ 				 const struct tcf_proto *tp)
+ {
+ 	struct bpf_prog *fp;
+ 	char *name = NULL;
+ 	u32 bpf_fd;
+ 
+ 	bpf_fd = nla_get_u32(tb[TCA_BPF_FD]);
+ 
+ 	fp = bpf_prog_get_type(bpf_fd, BPF_PROG_TYPE_SCHED_CLS);
+ 	if (IS_ERR(fp))
+ 		return PTR_ERR(fp);
+ 
+ 	if (tb[TCA_BPF_NAME]) {
+ 		name = nla_memdup(tb[TCA_BPF_NAME], GFP_KERNEL);
+ 		if (!name) {
+ 			bpf_prog_put(fp);
+ 			return -ENOMEM;
+ 		}
+ 	}
+ 
+ 	prog->bpf_ops = NULL;
+ 	prog->bpf_name = name;
+ 	prog->filter = fp;
+ 
+ 	if (fp->dst_needed && !(tp->q->flags & TCQ_F_INGRESS))
+ 		netif_keep_dst(qdisc_dev(tp->q));
+ 
+ 	return 0;
+ }
+ 
+ static int cls_bpf_set_parms(struct net *net, struct tcf_proto *tp,
+ 			     struct cls_bpf_prog *prog, unsigned long base,
+ 			     struct nlattr **tb, struct nlattr *est, bool ovr)
+ {
+ 	bool is_bpf, is_ebpf, have_exts = false;
+ 	u32 gen_flags = 0;
+ 	int ret;
+ 
+ 	is_bpf = tb[TCA_BPF_OPS_LEN] && tb[TCA_BPF_OPS];
+ 	is_ebpf = tb[TCA_BPF_FD];
+ 	if ((!is_bpf && !is_ebpf) || (is_bpf && is_ebpf))
+ 		return -EINVAL;
+ 
+ 	ret = tcf_exts_validate(net, tp, tb, est, &prog->exts, ovr);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (tb[TCA_BPF_FLAGS]) {
+ 		u32 bpf_flags = nla_get_u32(tb[TCA_BPF_FLAGS]);
+ 
+ 		if (bpf_flags & ~TCA_BPF_FLAG_ACT_DIRECT)
+ 			return -EINVAL;
+ 
+ 		have_exts = bpf_flags & TCA_BPF_FLAG_ACT_DIRECT;
+ 	}
+ 	if (tb[TCA_BPF_FLAGS_GEN]) {
+ 		gen_flags = nla_get_u32(tb[TCA_BPF_FLAGS_GEN]);
+ 		if (gen_flags & ~CLS_BPF_SUPPORTED_GEN_FLAGS ||
+ 		    !tc_flags_valid(gen_flags))
+ 			return -EINVAL;
+ 	}
+ 
+ 	prog->exts_integrated = have_exts;
+ 	prog->gen_flags = gen_flags;
+ 
+ 	ret = is_bpf ? cls_bpf_prog_from_ops(tb, prog) :
+ 		       cls_bpf_prog_from_efd(tb, prog, tp);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (tb[TCA_BPF_CLASSID]) {
+ 		prog->res.classid = nla_get_u32(tb[TCA_BPF_CLASSID]);
+ 		tcf_bind_filter(tp, &prog->res, base);
+ 	}
+ 
+ 	return 0;
++>>>>>>> 6839da326dfc (net: sched: cls_bpf: no need to call tcf_exts_change for newly allocated struct)
  }
  
  static u32 cls_bpf_grab_new_handle(struct tcf_proto *tp,
* Unmerged path net/sched/cls_bpf.c
