md/md0: optimize raid0 discard handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [md] md0: optimize raid0 discard handling (Nigel Croxon) [1455932]
Rebuild_FUZZ: 96.00%
commit-author Shaohua Li <shli@fb.com>
commit 29efc390b9462582ae95eb9a0b8cd17ab956afc0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/29efc390.failed

There are complaints that raid0 discard handling is slow. Currently we
divide discard request into chunks and dispatch to underlayer disks. The
block layer will do merge to form big requests. This causes a lot of
request split/merge and uses significant CPU time.

A simple idea is to calculate the range for each raid disk for an IO
request and send a discard request to raid disks, which will avoid the
split/merge completely. Previously Coly tried the approach, but the
implementation was too complex because of raid0 zones. This patch always
split bio in zone boundary and handle bio within one zone. It simplifies
the implementation a lot.

	Reviewed-by: NeilBrown <neilb@suse.com>
	Acked-by: Coly Li <colyli@suse.de>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit 29efc390b9462582ae95eb9a0b8cd17ab956afc0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/raid0.c
diff --cc drivers/md/raid0.c
index 5d0952d819f8,d6c0bc76e837..000000000000
--- a/drivers/md/raid0.c
+++ b/drivers/md/raid0.c
@@@ -438,7 -384,8 +438,12 @@@ static int raid0_run(struct mddev *mdde
  
  		blk_queue_max_hw_sectors(mddev->queue, mddev->chunk_sectors);
  		blk_queue_max_write_same_sectors(mddev->queue, mddev->chunk_sectors);
++<<<<<<< HEAD
 +		blk_queue_max_discard_sectors(mddev->queue, mddev->chunk_sectors);
++=======
+ 		blk_queue_max_write_zeroes_sectors(mddev->queue, mddev->chunk_sectors);
+ 		blk_queue_max_discard_sectors(mddev->queue, UINT_MAX);
++>>>>>>> 29efc390b946 (md/md0: optimize raid0 discard handling)
  
  		blk_queue_io_min(mddev->queue, mddev->chunk_sectors << 9);
  		blk_queue_io_opt(mddev->queue,
@@@ -527,66 -459,146 +532,179 @@@ static inline int is_io_in_chunk_bounda
  	}
  }
  
++<<<<<<< HEAD
 +static bool raid0_make_request(struct mddev *mddev, struct bio *bio)
++=======
+ static void raid0_handle_discard(struct mddev *mddev, struct bio *bio)
+ {
+ 	struct r0conf *conf = mddev->private;
+ 	struct strip_zone *zone;
+ 	sector_t start = bio->bi_iter.bi_sector;
+ 	sector_t end;
+ 	unsigned int stripe_size;
+ 	sector_t first_stripe_index, last_stripe_index;
+ 	sector_t start_disk_offset;
+ 	unsigned int start_disk_index;
+ 	sector_t end_disk_offset;
+ 	unsigned int end_disk_index;
+ 	unsigned int disk;
+ 
+ 	zone = find_zone(conf, &start);
+ 
+ 	if (bio_end_sector(bio) > zone->zone_end) {
+ 		struct bio *split = bio_split(bio,
+ 			zone->zone_end - bio->bi_iter.bi_sector, GFP_NOIO,
+ 			mddev->bio_set);
+ 		bio_chain(split, bio);
+ 		generic_make_request(bio);
+ 		bio = split;
+ 		end = zone->zone_end;
+ 	} else
+ 		end = bio_end_sector(bio);
+ 
+ 	if (zone != conf->strip_zone)
+ 		end = end - zone[-1].zone_end;
+ 
+ 	/* Now start and end is the offset in zone */
+ 	stripe_size = zone->nb_dev * mddev->chunk_sectors;
+ 
+ 	first_stripe_index = start;
+ 	sector_div(first_stripe_index, stripe_size);
+ 	last_stripe_index = end;
+ 	sector_div(last_stripe_index, stripe_size);
+ 
+ 	start_disk_index = (int)(start - first_stripe_index * stripe_size) /
+ 		mddev->chunk_sectors;
+ 	start_disk_offset = ((int)(start - first_stripe_index * stripe_size) %
+ 		mddev->chunk_sectors) +
+ 		first_stripe_index * mddev->chunk_sectors;
+ 	end_disk_index = (int)(end - last_stripe_index * stripe_size) /
+ 		mddev->chunk_sectors;
+ 	end_disk_offset = ((int)(end - last_stripe_index * stripe_size) %
+ 		mddev->chunk_sectors) +
+ 		last_stripe_index * mddev->chunk_sectors;
+ 
+ 	for (disk = 0; disk < zone->nb_dev; disk++) {
+ 		sector_t dev_start, dev_end;
+ 		struct bio *discard_bio = NULL;
+ 		struct md_rdev *rdev;
+ 
+ 		if (disk < start_disk_index)
+ 			dev_start = (first_stripe_index + 1) *
+ 				mddev->chunk_sectors;
+ 		else if (disk > start_disk_index)
+ 			dev_start = first_stripe_index * mddev->chunk_sectors;
+ 		else
+ 			dev_start = start_disk_offset;
+ 
+ 		if (disk < end_disk_index)
+ 			dev_end = (last_stripe_index + 1) * mddev->chunk_sectors;
+ 		else if (disk > end_disk_index)
+ 			dev_end = last_stripe_index * mddev->chunk_sectors;
+ 		else
+ 			dev_end = end_disk_offset;
+ 
+ 		if (dev_end <= dev_start)
+ 			continue;
+ 
+ 		rdev = conf->devlist[(zone - conf->strip_zone) *
+ 			conf->strip_zone[0].nb_dev + disk];
+ 		if (__blkdev_issue_discard(rdev->bdev,
+ 			dev_start + zone->dev_start + rdev->data_offset,
+ 			dev_end - dev_start, GFP_NOIO, 0, &discard_bio) ||
+ 		    !discard_bio)
+ 			continue;
+ 		bio_chain(discard_bio, bio);
+ 		if (mddev->gendisk)
+ 			trace_block_bio_remap(bdev_get_queue(rdev->bdev),
+ 				discard_bio, disk_devt(mddev->gendisk),
+ 				bio->bi_iter.bi_sector);
+ 		generic_make_request(discard_bio);
+ 	}
+ 	bio_endio(bio);
+ }
+ 
+ static void raid0_make_request(struct mddev *mddev, struct bio *bio)
++>>>>>>> 29efc390b946 (md/md0: optimize raid0 discard handling)
  {
 +	unsigned int chunk_sects;
 +	sector_t sector_offset;
  	struct strip_zone *zone;
  	struct md_rdev *tmp_dev;
 -	sector_t bio_sector;
 -	sector_t sector;
 -	unsigned chunk_sects;
 -	unsigned sectors;
  
 -	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
  		md_flush_request(mddev, bio);
 -		return;
 +		return true;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (unlikely((bio_op(bio) == REQ_OP_DISCARD))) {
+ 		raid0_handle_discard(mddev, bio);
+ 		return;
+ 	}
+ 
+ 	bio_sector = bio->bi_iter.bi_sector;
+ 	sector = bio_sector;
++>>>>>>> 29efc390b946 (md/md0: optimize raid0 discard handling)
  	chunk_sects = mddev->chunk_sectors;
 -
 -	sectors = chunk_sects -
 -		(likely(is_power_of_2(chunk_sects))
 -		 ? (sector & (chunk_sects-1))
 -		 : sector_div(sector, chunk_sects));
 -
 -	/* Restore due to sector_div */
 -	sector = bio_sector;
 -
 -	if (sectors < bio_sectors(bio)) {
 -		struct bio *split = bio_split(bio, sectors, GFP_NOIO, mddev->bio_set);
 -		bio_chain(split, bio);
 -		generic_make_request(bio);
 -		bio = split;
 +	if (unlikely(!is_io_in_chunk_boundary(mddev, chunk_sects, bio))) {
 +		sector_t sector = bio->bi_sector;
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		if (likely(is_power_of_2(chunk_sects)))
 +			bp = bio_split(bio, chunk_sects - (sector &
 +							   (chunk_sects-1)));
 +		else
 +			bp = bio_split(bio, chunk_sects -
 +				       sector_div(sector, chunk_sects));
 +		raid0_make_request(mddev, &bp->bio1);
 +		raid0_make_request(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return true;
  	}
  
 -	zone = find_zone(mddev->private, &sector);
 -	tmp_dev = map_sector(mddev, zone, sector, &sector);
 +	sector_offset = bio->bi_sector;
 +	zone = find_zone(mddev->private, &sector_offset);
 +	tmp_dev = map_sector(mddev, zone, bio->bi_sector,
 +			     &sector_offset);
  	bio->bi_bdev = tmp_dev->bdev;
 -	bio->bi_iter.bi_sector = sector + zone->dev_start +
 +	bio->bi_sector = sector_offset + zone->dev_start +
  		tmp_dev->data_offset;
  
++<<<<<<< HEAD
 +	if (unlikely((bio->bi_rw & REQ_DISCARD) &&
 +		     !blk_queue_discard(bdev_get_queue(bio->bi_bdev)))) {
 +		/* Just ignore it */
 +		bio_endio(bio, 0);
 +		return true;
 +	}
 +
 +	generic_make_request(bio);
 +	return true;
 +
 +bad_map:
 +	printk("md/raid0:%s: make_request bug: can't convert block across chunks"
 +	       " or bigger than %dk %llu %d\n",
 +	       mdname(mddev), chunk_sects / 2,
 +	       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
 +
 +	bio_io_error(bio);
 +	return true;
++=======
+ 	if (mddev->gendisk)
+ 		trace_block_bio_remap(bdev_get_queue(bio->bi_bdev),
+ 				      bio, disk_devt(mddev->gendisk),
+ 				      bio_sector);
+ 	mddev_check_writesame(mddev, bio);
+ 	mddev_check_write_zeroes(mddev, bio);
+ 	generic_make_request(bio);
++>>>>>>> 29efc390b946 (md/md0: optimize raid0 discard handling)
  }
  
  static void raid0_status(struct seq_file *seq, struct mddev *mddev)
* Unmerged path drivers/md/raid0.c
