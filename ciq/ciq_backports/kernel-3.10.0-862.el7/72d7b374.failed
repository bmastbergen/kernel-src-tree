KVM: x86: introduce ISA specific smi_allowed callback

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ladi Prosek <lprosek@redhat.com>
commit 72d7b374b14d67e973bce476e4a75552478cc42d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/72d7b374.failed

Similar to NMI, there may be ISA specific reasons why an SMI cannot be
injected into the guest. This commit adds a new smi_allowed callback to
be implemented in following commits.

	Signed-off-by: Ladi Prosek <lprosek@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 72d7b374b14d67e973bce476e4a75552478cc42d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/svm.c
#	arch/x86/kvm/vmx.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/include/asm/kvm_host.h
index ab2d8132f390,411ddbbaeabf..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -966,7 -1057,14 +966,14 @@@ struct kvm_x86_ops 
  			      uint32_t guest_irq, bool set);
  	void (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);
  
 -	int (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc);
 -	void (*cancel_hv_timer)(struct kvm_vcpu *vcpu);
 -
  	void (*setup_mce)(struct kvm_vcpu *vcpu);
++<<<<<<< HEAD
++=======
+ 
+ 	int (*smi_allowed)(struct kvm_vcpu *vcpu);
+ 	int (*pre_enter_smm)(struct kvm_vcpu *vcpu, char *smstate);
+ 	int (*pre_leave_smm)(struct kvm_vcpu *vcpu, u64 smbase);
++>>>>>>> 72d7b374b14d (KVM: x86: introduce ISA specific smi_allowed callback)
  };
  
  struct kvm_arch_async_pf {
diff --cc arch/x86/kvm/svm.c
index ba38eb017cb6,e3c61a32249d..000000000000
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@@ -5224,7 -5395,30 +5224,34 @@@ static inline void avic_post_state_rest
  	avic_handle_ldr_update(vcpu);
  }
  
++<<<<<<< HEAD
 +static struct kvm_x86_ops svm_x86_ops = {
++=======
+ static void svm_setup_mce(struct kvm_vcpu *vcpu)
+ {
+ 	/* [63:9] are reserved. */
+ 	vcpu->arch.mcg_cap &= 0x1ff;
+ }
+ 
+ static int svm_smi_allowed(struct kvm_vcpu *vcpu)
+ {
+ 	return 1;
+ }
+ 
+ static int svm_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static int svm_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
++>>>>>>> 72d7b374b14d (KVM: x86: introduce ISA specific smi_allowed callback)
  	.cpu_has_kvm_support = has_svm,
  	.disabled_by_bios = is_disabled,
  	.hardware_setup = svm_hardware_setup,
@@@ -5338,6 -5527,11 +5365,14 @@@
  	.pmu_ops = &amd_pmu_ops,
  	.deliver_posted_interrupt = svm_deliver_avic_intr,
  	.update_pi_irte = svm_update_pi_irte,
++<<<<<<< HEAD
++=======
+ 	.setup_mce = svm_setup_mce,
+ 
+ 	.smi_allowed = svm_smi_allowed,
+ 	.pre_enter_smm = svm_pre_enter_smm,
+ 	.pre_leave_smm = svm_pre_leave_smm,
++>>>>>>> 72d7b374b14d (KVM: x86: introduce ISA specific smi_allowed callback)
  };
  
  static int __init svm_init(void)
diff --cc arch/x86/kvm/vmx.c
index 72c1c1c3db0f,156ecbaad1e6..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -11014,7 -11916,24 +11014,28 @@@ static void vmx_setup_mce(struct kvm_vc
  			~FEATURE_CONTROL_LMCE;
  }
  
++<<<<<<< HEAD
 +static struct kvm_x86_ops vmx_x86_ops = {
++=======
+ static int vmx_smi_allowed(struct kvm_vcpu *vcpu)
+ {
+ 	return 1;
+ }
+ 
+ static int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)
+ {
+ 	/* TODO: Implement */
+ 	return 0;
+ }
+ 
+ static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
++>>>>>>> 72d7b374b14d (KVM: x86: introduce ISA specific smi_allowed callback)
  	.cpu_has_kvm_support = cpu_has_kvm_support,
  	.disabled_by_bios = vmx_disabled_by_bios,
  	.hardware_setup = hardware_setup,
@@@ -11136,7 -12052,16 +11157,14 @@@
  
  	.update_pi_irte = vmx_update_pi_irte,
  
 -#ifdef CONFIG_X86_64
 -	.set_hv_timer = vmx_set_hv_timer,
 -	.cancel_hv_timer = vmx_cancel_hv_timer,
 -#endif
 -
  	.setup_mce = vmx_setup_mce,
++<<<<<<< HEAD
++=======
+ 
+ 	.smi_allowed = vmx_smi_allowed,
+ 	.pre_enter_smm = vmx_pre_enter_smm,
+ 	.pre_leave_smm = vmx_pre_leave_smm,
++>>>>>>> 72d7b374b14d (KVM: x86: introduce ISA specific smi_allowed callback)
  };
  
  static int __init vmx_init(void)
diff --cc arch/x86/kvm/x86.c
index 99e230533b87,693bf8d01128..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -6155,31 -6437,8 +6155,36 @@@ static int inject_pending_event(struct 
  			kvm_update_dr7(vcpu);
  		}
  
++<<<<<<< HEAD
 +		kvm_x86_ops->queue_exception(vcpu, vcpu->arch.exception.nr,
 +					  vcpu->arch.exception.has_error_code,
 +					  vcpu->arch.exception.error_code,
 +					  vcpu->arch.exception.reinject);
 +		return 0;
 +	}
 +
 +	if (vcpu->arch.nmi_injected) {
 +		kvm_x86_ops->set_nmi(vcpu);
 +		return 0;
 +	}
 +
 +	if (vcpu->arch.interrupt.pending) {
 +		kvm_x86_ops->set_irq(vcpu);
 +		return 0;
 +	}
 +
 +	if (is_guest_mode(vcpu) && kvm_x86_ops->check_nested_events) {
 +		r = kvm_x86_ops->check_nested_events(vcpu, req_int_win);
 +		if (r != 0)
 +			return r;
 +	}
 +
 +	/* try to inject new event if pending */
 +	if (vcpu->arch.smi_pending && !is_smm(vcpu)) {
++=======
+ 		kvm_x86_ops->queue_exception(vcpu);
+ 	} else if (vcpu->arch.smi_pending && !is_smm(vcpu) && kvm_x86_ops->smi_allowed(vcpu)) {
++>>>>>>> 72d7b374b14d (KVM: x86: introduce ISA specific smi_allowed callback)
  		vcpu->arch.smi_pending = false;
  		enter_smm(vcpu);
  	} else if (vcpu->arch.nmi_pending && kvm_x86_ops->nmi_allowed(vcpu)) {
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/svm.c
* Unmerged path arch/x86/kvm/vmx.c
* Unmerged path arch/x86/kvm/x86.c
