nvme: split nvme_uninit_ctrl into stop and uninit

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [nvme] split nvme_uninit_ctrl into stop and uninit (David Milburn) [1457880 1456486 1454365]
Rebuild_FUZZ: 93.48%
commit-author Sagi Grimberg <sagi@grimberg.me>
commit d09f2b45f346f0a9e5e1b5fcea531b1b393671dc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d09f2b45.failed

Usually before we teardown the controller we want to:
1. complete/cancel any ctrl inflight works
2. remove ctrl namespaces (only for removal though, resets
   shouldn't remove any namespaces).

but we do not want to destroy the controller device as
we might use it for logging during the teardown stage.

This patch adds nvme_start_ctrl() which queues inflight
controller works (aen, ns scan, queue start and keep-alive
if kato is set) and nvme_stop_ctrl() which cancels the works
namespace removal is left to the callers to handle.

Move nvme_uninit_ctrl after we are done with the
controller device.

	Reviewed-by: Keith Busch <keith.busch@intel.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
(cherry picked from commit d09f2b45f346f0a9e5e1b5fcea531b1b393671dc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/nvme/host/fc.c
#	drivers/nvme/host/pci.c
#	drivers/nvme/host/rdma.c
#	drivers/nvme/target/loop.c
diff --cc drivers/nvme/host/fc.c
index bff7f964238e,d666ada39a9b..000000000000
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@@ -2256,9 -2243,423 +2255,426 @@@ out_free_tag_set
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ nvme_fc_reinit_io_queues(struct nvme_fc_ctrl *ctrl)
+ {
+ 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+ 	unsigned int nr_io_queues;
+ 	int ret;
+ 
+ 	nr_io_queues = min(min(opts->nr_io_queues, num_online_cpus()),
+ 				ctrl->lport->ops->max_hw_queues);
+ 	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ 	if (ret) {
+ 		dev_info(ctrl->ctrl.device,
+ 			"set_queue_count failed: %d\n", ret);
+ 		return ret;
+ 	}
+ 
+ 	ctrl->ctrl.queue_count = nr_io_queues + 1;
+ 	/* check for io queues existing */
+ 	if (ctrl->ctrl.queue_count == 1)
+ 		return 0;
+ 
+ 	nvme_fc_init_io_queues(ctrl);
+ 
+ 	ret = blk_mq_reinit_tagset(&ctrl->tag_set);
+ 	if (ret)
+ 		goto out_free_io_queues;
+ 
+ 	ret = nvme_fc_create_hw_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
+ 	if (ret)
+ 		goto out_free_io_queues;
+ 
+ 	ret = nvme_fc_connect_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
+ 	if (ret)
+ 		goto out_delete_hw_queues;
+ 
+ 	blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ 
+ 	return 0;
+ 
+ out_delete_hw_queues:
+ 	nvme_fc_delete_hw_io_queues(ctrl);
+ out_free_io_queues:
+ 	nvme_fc_free_io_queues(ctrl);
+ 	return ret;
+ }
+ 
+ /*
+  * This routine restarts the controller on the host side, and
+  * on the link side, recreates the controller association.
+  */
+ static int
+ nvme_fc_create_association(struct nvme_fc_ctrl *ctrl)
+ {
+ 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+ 	u32 segs;
+ 	int ret;
+ 	bool changed;
+ 
+ 	++ctrl->ctrl.nr_reconnects;
+ 
+ 	/*
+ 	 * Create the admin queue
+ 	 */
+ 
+ 	nvme_fc_init_queue(ctrl, 0, NVME_FC_AQ_BLKMQ_DEPTH);
+ 
+ 	ret = __nvme_fc_create_hw_queue(ctrl, &ctrl->queues[0], 0,
+ 				NVME_FC_AQ_BLKMQ_DEPTH);
+ 	if (ret)
+ 		goto out_free_queue;
+ 
+ 	ret = nvme_fc_connect_admin_queue(ctrl, &ctrl->queues[0],
+ 				NVME_FC_AQ_BLKMQ_DEPTH,
+ 				(NVME_FC_AQ_BLKMQ_DEPTH / 4));
+ 	if (ret)
+ 		goto out_delete_hw_queue;
+ 
+ 	if (ctrl->ctrl.state != NVME_CTRL_NEW)
+ 		blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ 
+ 	ret = nvmf_connect_admin_queue(&ctrl->ctrl);
+ 	if (ret)
+ 		goto out_disconnect_admin_queue;
+ 
+ 	/*
+ 	 * Check controller capabilities
+ 	 *
+ 	 * todo:- add code to check if ctrl attributes changed from
+ 	 * prior connection values
+ 	 */
+ 
+ 	ret = nvmf_reg_read64(&ctrl->ctrl, NVME_REG_CAP, &ctrl->ctrl.cap);
+ 	if (ret) {
+ 		dev_err(ctrl->ctrl.device,
+ 			"prop_get NVME_REG_CAP failed\n");
+ 		goto out_disconnect_admin_queue;
+ 	}
+ 
+ 	ctrl->ctrl.sqsize =
+ 		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap) + 1, ctrl->ctrl.sqsize);
+ 
+ 	ret = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ 	if (ret)
+ 		goto out_disconnect_admin_queue;
+ 
+ 	segs = min_t(u32, NVME_FC_MAX_SEGMENTS,
+ 			ctrl->lport->ops->max_sgl_segments);
+ 	ctrl->ctrl.max_hw_sectors = (segs - 1) << (PAGE_SHIFT - 9);
+ 
+ 	ret = nvme_init_identify(&ctrl->ctrl);
+ 	if (ret)
+ 		goto out_disconnect_admin_queue;
+ 
+ 	/* sanity checks */
+ 
+ 	/* FC-NVME does not have other data in the capsule */
+ 	if (ctrl->ctrl.icdoff) {
+ 		dev_err(ctrl->ctrl.device, "icdoff %d is not supported!\n",
+ 				ctrl->ctrl.icdoff);
+ 		goto out_disconnect_admin_queue;
+ 	}
+ 
+ 	/* FC-NVME supports normal SGL Data Block Descriptors */
+ 
+ 	if (opts->queue_size > ctrl->ctrl.maxcmd) {
+ 		/* warn if maxcmd is lower than queue_size */
+ 		dev_warn(ctrl->ctrl.device,
+ 			"queue_size %zu > ctrl maxcmd %u, reducing "
+ 			"to queue_size\n",
+ 			opts->queue_size, ctrl->ctrl.maxcmd);
+ 		opts->queue_size = ctrl->ctrl.maxcmd;
+ 	}
+ 
+ 	ret = nvme_fc_init_aen_ops(ctrl);
+ 	if (ret)
+ 		goto out_term_aen_ops;
+ 
+ 	/*
+ 	 * Create the io queues
+ 	 */
+ 
+ 	if (ctrl->ctrl.queue_count > 1) {
+ 		if (ctrl->ctrl.state == NVME_CTRL_NEW)
+ 			ret = nvme_fc_create_io_queues(ctrl);
+ 		else
+ 			ret = nvme_fc_reinit_io_queues(ctrl);
+ 		if (ret)
+ 			goto out_term_aen_ops;
+ 	}
+ 
+ 	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+ 	WARN_ON_ONCE(!changed);
+ 
+ 	ctrl->ctrl.nr_reconnects = 0;
+ 
+ 	nvme_start_ctrl(&ctrl->ctrl);
+ 
+ 	return 0;	/* Success */
+ 
+ out_term_aen_ops:
+ 	nvme_fc_term_aen_ops(ctrl);
+ out_disconnect_admin_queue:
+ 	/* send a Disconnect(association) LS to fc-nvme target */
+ 	nvme_fc_xmt_disconnect_assoc(ctrl);
+ out_delete_hw_queue:
+ 	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
+ out_free_queue:
+ 	nvme_fc_free_queue(&ctrl->queues[0]);
+ 
+ 	return ret;
+ }
+ 
+ /*
+  * This routine stops operation of the controller on the host side.
+  * On the host os stack side: Admin and IO queues are stopped,
+  *   outstanding ios on them terminated via FC ABTS.
+  * On the link side: the association is terminated.
+  */
+ static void
+ nvme_fc_delete_association(struct nvme_fc_ctrl *ctrl)
+ {
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&ctrl->lock, flags);
+ 	ctrl->flags |= FCCTRL_TERMIO;
+ 	ctrl->iocnt = 0;
+ 	spin_unlock_irqrestore(&ctrl->lock, flags);
+ 
+ 	/*
+ 	 * If io queues are present, stop them and terminate all outstanding
+ 	 * ios on them. As FC allocates FC exchange for each io, the
+ 	 * transport must contact the LLDD to terminate the exchange,
+ 	 * thus releasing the FC exchange. We use blk_mq_tagset_busy_itr()
+ 	 * to tell us what io's are busy and invoke a transport routine
+ 	 * to kill them with the LLDD.  After terminating the exchange
+ 	 * the LLDD will call the transport's normal io done path, but it
+ 	 * will have an aborted status. The done path will return the
+ 	 * io requests back to the block layer as part of normal completions
+ 	 * (but with error status).
+ 	 */
+ 	if (ctrl->ctrl.queue_count > 1) {
+ 		nvme_stop_queues(&ctrl->ctrl);
+ 		blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ 				nvme_fc_terminate_exchange, &ctrl->ctrl);
+ 	}
+ 
+ 	/*
+ 	 * Other transports, which don't have link-level contexts bound
+ 	 * to sqe's, would try to gracefully shutdown the controller by
+ 	 * writing the registers for shutdown and polling (call
+ 	 * nvme_shutdown_ctrl()). Given a bunch of i/o was potentially
+ 	 * just aborted and we will wait on those contexts, and given
+ 	 * there was no indication of how live the controlelr is on the
+ 	 * link, don't send more io to create more contexts for the
+ 	 * shutdown. Let the controller fail via keepalive failure if
+ 	 * its still present.
+ 	 */
+ 
+ 	/*
+ 	 * clean up the admin queue. Same thing as above.
+ 	 * use blk_mq_tagset_busy_itr() and the transport routine to
+ 	 * terminate the exchanges.
+ 	 */
+ 	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ 	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ 				nvme_fc_terminate_exchange, &ctrl->ctrl);
+ 
+ 	/* kill the aens as they are a separate path */
+ 	nvme_fc_abort_aen_ops(ctrl);
+ 
+ 	/* wait for all io that had to be aborted */
+ 	spin_lock_irqsave(&ctrl->lock, flags);
+ 	wait_event_lock_irq(ctrl->ioabort_wait, ctrl->iocnt == 0, ctrl->lock);
+ 	ctrl->flags &= ~FCCTRL_TERMIO;
+ 	spin_unlock_irqrestore(&ctrl->lock, flags);
+ 
+ 	nvme_fc_term_aen_ops(ctrl);
+ 
+ 	/*
+ 	 * send a Disconnect(association) LS to fc-nvme target
+ 	 * Note: could have been sent at top of process, but
+ 	 * cleaner on link traffic if after the aborts complete.
+ 	 * Note: if association doesn't exist, association_id will be 0
+ 	 */
+ 	if (ctrl->association_id)
+ 		nvme_fc_xmt_disconnect_assoc(ctrl);
+ 
+ 	if (ctrl->ctrl.tagset) {
+ 		nvme_fc_delete_hw_io_queues(ctrl);
+ 		nvme_fc_free_io_queues(ctrl);
+ 	}
+ 
+ 	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
+ 	nvme_fc_free_queue(&ctrl->queues[0]);
+ }
+ 
+ static void
+ nvme_fc_delete_ctrl_work(struct work_struct *work)
+ {
+ 	struct nvme_fc_ctrl *ctrl =
+ 		container_of(work, struct nvme_fc_ctrl, delete_work);
+ 
+ 	cancel_work_sync(&ctrl->ctrl.reset_work);
+ 	cancel_delayed_work_sync(&ctrl->connect_work);
+ 	nvme_stop_ctrl(&ctrl->ctrl);
+ 	nvme_remove_namespaces(&ctrl->ctrl);
+ 	/*
+ 	 * kill the association on the link side.  this will block
+ 	 * waiting for io to terminate
+ 	 */
+ 	nvme_fc_delete_association(ctrl);
+ 
+ 	/*
+ 	 * tear down the controller
+ 	 * After the last reference on the nvme ctrl is removed,
+ 	 * the transport nvme_fc_nvme_ctrl_freed() callback will be
+ 	 * invoked. From there, the transport will tear down it's
+ 	 * logical queues and association.
+ 	 */
+ 	nvme_uninit_ctrl(&ctrl->ctrl);
+ 
+ 	nvme_put_ctrl(&ctrl->ctrl);
+ }
+ 
+ static bool
+ __nvme_fc_schedule_delete_work(struct nvme_fc_ctrl *ctrl)
+ {
+ 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_DELETING))
+ 		return true;
+ 
+ 	if (!queue_work(nvme_wq, &ctrl->delete_work))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static int
+ __nvme_fc_del_ctrl(struct nvme_fc_ctrl *ctrl)
+ {
+ 	return __nvme_fc_schedule_delete_work(ctrl) ? -EBUSY : 0;
+ }
+ 
+ /*
+  * Request from nvme core layer to delete the controller
+  */
+ static int
+ nvme_fc_del_nvme_ctrl(struct nvme_ctrl *nctrl)
+ {
+ 	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
+ 	int ret;
+ 
+ 	if (!kref_get_unless_zero(&ctrl->ctrl.kref))
+ 		return -EBUSY;
+ 
+ 	ret = __nvme_fc_del_ctrl(ctrl);
+ 
+ 	if (!ret)
+ 		flush_workqueue(nvme_wq);
+ 
+ 	nvme_put_ctrl(&ctrl->ctrl);
+ 
+ 	return ret;
+ }
+ 
+ static void
+ nvme_fc_reconnect_or_delete(struct nvme_fc_ctrl *ctrl, int status)
+ {
+ 	/* If we are resetting/deleting then do nothing */
+ 	if (ctrl->ctrl.state != NVME_CTRL_RECONNECTING) {
+ 		WARN_ON_ONCE(ctrl->ctrl.state == NVME_CTRL_NEW ||
+ 			ctrl->ctrl.state == NVME_CTRL_LIVE);
+ 		return;
+ 	}
+ 
+ 	dev_info(ctrl->ctrl.device,
+ 		"NVME-FC{%d}: reset: Reconnect attempt failed (%d)\n",
+ 		ctrl->cnum, status);
+ 
+ 	if (nvmf_should_reconnect(&ctrl->ctrl)) {
+ 		dev_info(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: Reconnect attempt in %d seconds.\n",
+ 			ctrl->cnum, ctrl->ctrl.opts->reconnect_delay);
+ 		queue_delayed_work(nvme_wq, &ctrl->connect_work,
+ 				ctrl->ctrl.opts->reconnect_delay * HZ);
+ 	} else {
+ 		dev_warn(ctrl->ctrl.device,
+ 				"NVME-FC{%d}: Max reconnect attempts (%d) "
+ 				"reached. Removing controller\n",
+ 				ctrl->cnum, ctrl->ctrl.nr_reconnects);
+ 		WARN_ON(__nvme_fc_schedule_delete_work(ctrl));
+ 	}
+ }
+ 
+ static void
+ nvme_fc_reset_ctrl_work(struct work_struct *work)
+ {
+ 	struct nvme_fc_ctrl *ctrl =
+ 		container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
+ 	int ret;
+ 
+ 	nvme_stop_ctrl(&ctrl->ctrl);
+ 	/* will block will waiting for io to terminate */
+ 	nvme_fc_delete_association(ctrl);
+ 
+ 	ret = nvme_fc_create_association(ctrl);
+ 	if (ret)
+ 		nvme_fc_reconnect_or_delete(ctrl, ret);
+ 	else
+ 		dev_info(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: controller reset complete\n", ctrl->cnum);
+ }
+ 
+ static const struct nvme_ctrl_ops nvme_fc_ctrl_ops = {
+ 	.name			= "fc",
+ 	.module			= THIS_MODULE,
+ 	.flags			= NVME_F_FABRICS,
+ 	.reg_read32		= nvmf_reg_read32,
+ 	.reg_read64		= nvmf_reg_read64,
+ 	.reg_write32		= nvmf_reg_write32,
+ 	.free_ctrl		= nvme_fc_nvme_ctrl_freed,
+ 	.submit_async_event	= nvme_fc_submit_async_event,
+ 	.delete_ctrl		= nvme_fc_del_nvme_ctrl,
+ 	.get_address		= nvmf_get_address,
+ };
+ 
+ static void
+ nvme_fc_connect_ctrl_work(struct work_struct *work)
+ {
+ 	int ret;
+ 
+ 	struct nvme_fc_ctrl *ctrl =
+ 			container_of(to_delayed_work(work),
+ 				struct nvme_fc_ctrl, connect_work);
+ 
+ 	ret = nvme_fc_create_association(ctrl);
+ 	if (ret)
+ 		nvme_fc_reconnect_or_delete(ctrl, ret);
+ 	else
+ 		dev_info(ctrl->ctrl.device,
+ 			"NVME-FC{%d}: controller reconnect complete\n",
+ 			ctrl->cnum);
+ }
+ 
+ 
+ static const struct blk_mq_ops nvme_fc_admin_mq_ops = {
+ 	.queue_rq	= nvme_fc_queue_rq,
+ 	.complete	= nvme_fc_complete_rq,
+ 	.init_request	= nvme_fc_init_request,
+ 	.exit_request	= nvme_fc_exit_request,
+ 	.reinit_request	= nvme_fc_reinit_request,
+ 	.init_hctx	= nvme_fc_init_admin_hctx,
+ 	.timeout	= nvme_fc_timeout,
+ };
+ 
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  
  static struct nvme_ctrl *
 -nvme_fc_init_ctrl(struct device *dev, struct nvmf_ctrl_options *opts,
 +__nvme_fc_create_ctrl(struct device *dev, struct nvmf_ctrl_options *opts,
  	struct nvme_fc_lport *lport, struct nvme_fc_rport *rport)
  {
  	struct nvme_fc_ctrl *ctrl;
diff --cc drivers/nvme/host/pci.c
index bdee6de6e811,882ed3677117..000000000000
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@@ -1713,17 -2135,6 +1713,20 @@@ static void nvme_reset_work(struct work
  		goto out;
  
  	/*
++<<<<<<< HEAD
 +	 * A controller that can not execute IO typically requires user
 +	 * intervention to correct. For such degraded controllers, the driver
 +	 * should not submit commands the user did not request, so skip
 +	 * registering for asynchronous event notification on this condition.
 +	 */
 +	if (dev->online_queues > 1)
 +		nvme_queue_async_events(&dev->ctrl);
 +
 +	mod_timer(&dev->watchdog_timer, round_jiffies(jiffies + HZ));
 +
 +	/*
++=======
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  	 * Keep the controller around but remove all namespaces if we don't have
  	 * any working I/O queue.
  	 */
@@@ -1925,12 -2330,14 +1927,19 @@@ static void nvme_remove(struct pci_dev 
  		nvme_dev_disable(dev, false);
  	}
  
++<<<<<<< HEAD
 +	flush_work(&dev->reset_work);
 +	nvme_uninit_ctrl(&dev->ctrl);
++=======
+ 	flush_work(&dev->ctrl.reset_work);
+ 	nvme_stop_ctrl(&dev->ctrl);
+ 	nvme_remove_namespaces(&dev->ctrl);
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  	nvme_dev_disable(dev, true);
 -	nvme_free_host_mem(dev);
 +	flush_work(&dev->reset_work);
  	nvme_dev_remove_admin(dev);
  	nvme_free_queues(dev, 0);
+ 	nvme_uninit_ctrl(&dev->ctrl);
  	nvme_release_prp_pools(dev);
  	nvme_dev_unmap(dev);
  	nvme_put_ctrl(&dev->ctrl);
diff --cc drivers/nvme/host/rdma.c
index c867e649495e,ccdbd99d5a1c..000000000000
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@@ -757,9 -732,7 +757,13 @@@ static void nvme_rdma_reconnect_ctrl_wo
  	if (ret)
  		goto requeue;
  
++<<<<<<< HEAD
 +	nvme_start_keep_alive(&ctrl->ctrl);
 +
 +	if (ctrl->queue_count > 1) {
++=======
+ 	if (ctrl->ctrl.queue_count > 1) {
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  		ret = nvme_rdma_init_io_queues(ctrl);
  		if (ret)
  			goto requeue;
@@@ -774,12 -747,9 +778,16 @@@
  
  	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
  	WARN_ON_ONCE(!changed);
 -	ctrl->ctrl.nr_reconnects = 0;
 +	ctrl->ctrl.opts->nr_reconnects = 0;
  
++<<<<<<< HEAD
 +	if (ctrl->queue_count > 1) {
 +		nvme_queue_scan(&ctrl->ctrl);
 +		nvme_queue_async_events(&ctrl->ctrl);
 +	}
++=======
+ 	nvme_start_ctrl(&ctrl->ctrl);
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  
  	dev_info(ctrl->ctrl.device, "Successfully reconnected\n");
  
@@@ -797,17 -767,17 +805,17 @@@ static void nvme_rdma_error_recovery_wo
  			struct nvme_rdma_ctrl, err_work);
  	int i;
  
- 	nvme_stop_keep_alive(&ctrl->ctrl);
+ 	nvme_stop_ctrl(&ctrl->ctrl);
  
 -	for (i = 0; i < ctrl->ctrl.queue_count; i++)
 +	for (i = 0; i < ctrl->queue_count; i++)
  		clear_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[i].flags);
  
 -	if (ctrl->ctrl.queue_count > 1)
 +	if (ctrl->queue_count > 1)
  		nvme_stop_queues(&ctrl->ctrl);
 -	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
 +	blk_mq_stop_hw_queues(ctrl->ctrl.admin_q);
  
  	/* We must take care of fastfail/requeue all our inflight requests */
 -	if (ctrl->ctrl.queue_count > 1)
 +	if (ctrl->queue_count > 1)
  		blk_mq_tagset_busy_iter(&ctrl->tag_set,
  					nvme_cancel_request, &ctrl->ctrl);
  	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
@@@ -1764,11 -1734,7 +1772,15 @@@ static void nvme_rdma_reset_ctrl_work(s
  	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
  	WARN_ON_ONCE(!changed);
  
++<<<<<<< HEAD
 +	if (ctrl->queue_count > 1) {
 +		nvme_start_queues(&ctrl->ctrl);
 +		nvme_queue_scan(&ctrl->ctrl);
 +		nvme_queue_async_events(&ctrl->ctrl);
 +	}
++=======
+ 	nvme_start_ctrl(&ctrl->ctrl);
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  
  	return;
  
@@@ -1983,10 -1922,7 +1995,14 @@@ static struct nvme_ctrl *nvme_rdma_crea
  	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
  	mutex_unlock(&nvme_rdma_ctrl_mutex);
  
++<<<<<<< HEAD
 +	if (opts->nr_io_queues) {
 +		nvme_queue_scan(&ctrl->ctrl);
 +		nvme_queue_async_events(&ctrl->ctrl);
 +	}
++=======
+ 	nvme_start_ctrl(&ctrl->ctrl);
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  
  	return &ctrl->ctrl;
  
diff --cc drivers/nvme/target/loop.c
index eed36630d359,717ed7ddb2f6..000000000000
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@@ -452,9 -420,7 +450,13 @@@ out_free_sq
  
  static void nvme_loop_shutdown_ctrl(struct nvme_loop_ctrl *ctrl)
  {
++<<<<<<< HEAD
 +	nvme_stop_keep_alive(&ctrl->ctrl);
 +
 +	if (ctrl->queue_count > 1) {
++=======
+ 	if (ctrl->ctrl.queue_count > 1) {
++>>>>>>> d09f2b45f346 (nvme: split nvme_uninit_ctrl into stop and uninit)
  		nvme_stop_queues(&ctrl->ctrl);
  		blk_mq_tagset_busy_iter(&ctrl->tag_set,
  					nvme_cancel_request, &ctrl->ctrl);
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index f3660f9b6cc7..864e036c15e7 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -1922,12 +1922,29 @@ static void nvme_release_instance(struct nvme_ctrl *ctrl)
 	spin_unlock(&dev_list_lock);
 }
 
-void nvme_uninit_ctrl(struct nvme_ctrl *ctrl)
+void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
 {
+	nvme_stop_keep_alive(ctrl);
 	flush_work(&ctrl->async_event_work);
 	flush_work(&ctrl->scan_work);
-	nvme_remove_namespaces(ctrl);
+}
+EXPORT_SYMBOL_GPL(nvme_stop_ctrl);
+
+void nvme_start_ctrl(struct nvme_ctrl *ctrl)
+{
+	if (ctrl->kato)
+		nvme_start_keep_alive(ctrl);
+
+	if (ctrl->queue_count > 1) {
+		nvme_queue_scan(ctrl);
+		nvme_queue_async_events(ctrl);
+		nvme_start_queues(ctrl);
+	}
+}
+EXPORT_SYMBOL_GPL(nvme_start_ctrl);
 
+void nvme_uninit_ctrl(struct nvme_ctrl *ctrl)
+{
 	device_destroy(nvme_class, MKDEV(nvme_char_major, ctrl->instance));
 
 	spin_lock(&dev_list_lock);
* Unmerged path drivers/nvme/host/fc.c
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 4755f2ec8a17..2a49ef9364c1 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -255,6 +255,8 @@ int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl);
 int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 		const struct nvme_ctrl_ops *ops, unsigned long quirks);
 void nvme_uninit_ctrl(struct nvme_ctrl *ctrl);
+void nvme_start_ctrl(struct nvme_ctrl *ctrl);
+void nvme_stop_ctrl(struct nvme_ctrl *ctrl);
 void nvme_put_ctrl(struct nvme_ctrl *ctrl);
 int nvme_init_identify(struct nvme_ctrl *ctrl);
 
* Unmerged path drivers/nvme/host/pci.c
* Unmerged path drivers/nvme/host/rdma.c
* Unmerged path drivers/nvme/target/loop.c
