cpuset: Allow v2 behavior in v1 cgroup

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Waiman Long <longman@redhat.com>
commit b8d1b8ee93df8ffbabbeadd65d39853cfad6d698
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b8d1b8ee.failed

Cpuset v2 has some useful behaviors that are not present in v1 because
of backward compatibility concern. One of that is the restoration of
the original cpu and memory node mask after a hot removal and addition
event sequence.

This patch makes the cpuset controller to check the
CGRP_ROOT_CPUSET_V2_MODE flag and use the v2 behavior if it is set.

	Signed-off-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit b8d1b8ee93df8ffbabbeadd65d39853cfad6d698)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cpuset.c
diff --cc kernel/cpuset.c
index 8b2647fd13fb,f3539a41c49d..000000000000
--- a/kernel/cpuset.c
+++ b/kernel/cpuset.c
@@@ -452,9 -496,9 +462,13 @@@ static int validate_change(const struc
  
  	par = parent_cs(cur);
  
 -	/* On legacy hiearchy, we must be a subset of our parent cpuset. */
 +	/* We must be a subset of our parent cpuset */
  	ret = -EACCES;
++<<<<<<< HEAD:kernel/cpuset.c
 +	if (!is_cpuset_subset(trial, par))
++=======
+ 	if (!is_in_v2_mode() && !is_cpuset_subset(trial, par))
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  		goto out;
  
  	/*
@@@ -866,27 -888,51 +880,64 @@@ static void update_tasks_cpumask(struc
   *
   * Called with cpuset_mutex held
   */
 -static void update_cpumasks_hier(struct cpuset *cs, struct cpumask *new_cpus)
 +static void update_tasks_cpumask_hier(struct cpuset *root_cs,
 +				      bool update_root, struct ptr_heap *heap)
  {
  	struct cpuset *cp;
 -	struct cgroup_subsys_state *pos_css;
 -	bool need_rebuild_sched_domains = false;
 +	struct cgroup *pos_cgrp;
 +
 +	if (update_root)
 +		update_tasks_cpumask(root_cs, heap);
  
  	rcu_read_lock();
++<<<<<<< HEAD:kernel/cpuset.c
 +	cpuset_for_each_descendant_pre(cp, pos_cgrp, root_cs) {
 +		/* skip the whole subtree if @cp have some CPU */
 +		if (!cpumask_empty(cp->cpus_allowed)) {
 +			pos_cgrp = cgroup_rightmost_descendant(pos_cgrp);
++=======
+ 	cpuset_for_each_descendant_pre(cp, pos_css, cs) {
+ 		struct cpuset *parent = parent_cs(cp);
+ 
+ 		cpumask_and(new_cpus, cp->cpus_allowed, parent->effective_cpus);
+ 
+ 		/*
+ 		 * If it becomes empty, inherit the effective mask of the
+ 		 * parent, which is guaranteed to have some CPUs.
+ 		 */
+ 		if (is_in_v2_mode() && cpumask_empty(new_cpus))
+ 			cpumask_copy(new_cpus, parent->effective_cpus);
+ 
+ 		/* Skip the whole subtree if the cpumask remains the same. */
+ 		if (cpumask_equal(new_cpus, cp->effective_cpus)) {
+ 			pos_css = css_rightmost_descendant(pos_css);
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  			continue;
  		}
 -
 -		if (!css_tryget_online(&cp->css))
 +		if (!css_tryget(&cp->css))
  			continue;
  		rcu_read_unlock();
  
++<<<<<<< HEAD:kernel/cpuset.c
 +		update_tasks_cpumask(cp, heap);
++=======
+ 		spin_lock_irq(&callback_lock);
+ 		cpumask_copy(cp->effective_cpus, new_cpus);
+ 		spin_unlock_irq(&callback_lock);
+ 
+ 		WARN_ON(!is_in_v2_mode() &&
+ 			!cpumask_equal(cp->cpus_allowed, cp->effective_cpus));
+ 
+ 		update_tasks_cpumask(cp);
+ 
+ 		/*
+ 		 * If the effective cpumask of any non-empty cpuset is changed,
+ 		 * we need to rebuild sched domains.
+ 		 */
+ 		if (!cpumask_empty(cp->cpus_allowed) &&
+ 		    is_sched_load_balance(cp))
+ 			need_rebuild_sched_domains = true;
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  
  		rcu_read_lock();
  		css_put(&cp->css);
@@@ -1121,27 -1142,42 +1172,56 @@@ static void update_tasks_nodemask(struc
   *
   * Called with cpuset_mutex held
   */
 -static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)
 +static void update_tasks_nodemask_hier(struct cpuset *root_cs,
 +				       bool update_root, struct ptr_heap *heap)
  {
  	struct cpuset *cp;
 -	struct cgroup_subsys_state *pos_css;
 +	struct cgroup *pos_cgrp;
 +
 +	if (update_root)
 +		update_tasks_nodemask(root_cs, heap);
  
  	rcu_read_lock();
++<<<<<<< HEAD:kernel/cpuset.c
 +	cpuset_for_each_descendant_pre(cp, pos_cgrp, root_cs) {
 +		/* skip the whole subtree if @cp have some CPU */
 +		if (!nodes_empty(cp->mems_allowed)) {
 +			pos_cgrp = cgroup_rightmost_descendant(pos_cgrp);
++=======
+ 	cpuset_for_each_descendant_pre(cp, pos_css, cs) {
+ 		struct cpuset *parent = parent_cs(cp);
+ 
+ 		nodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);
+ 
+ 		/*
+ 		 * If it becomes empty, inherit the effective mask of the
+ 		 * parent, which is guaranteed to have some MEMs.
+ 		 */
+ 		if (is_in_v2_mode() && nodes_empty(*new_mems))
+ 			*new_mems = parent->effective_mems;
+ 
+ 		/* Skip the whole subtree if the nodemask remains the same. */
+ 		if (nodes_equal(*new_mems, cp->effective_mems)) {
+ 			pos_css = css_rightmost_descendant(pos_css);
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  			continue;
  		}
 -
 -		if (!css_tryget_online(&cp->css))
 +		if (!css_tryget(&cp->css))
  			continue;
  		rcu_read_unlock();
  
++<<<<<<< HEAD:kernel/cpuset.c
 +		update_tasks_nodemask(cp, heap);
++=======
+ 		spin_lock_irq(&callback_lock);
+ 		cp->effective_mems = *new_mems;
+ 		spin_unlock_irq(&callback_lock);
+ 
+ 		WARN_ON(!is_in_v2_mode() &&
+ 			!nodes_equal(cp->mems_allowed, cp->effective_mems));
+ 
+ 		update_tasks_nodemask(cp);
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  
  		rcu_read_lock();
  		css_put(&cp->css);
@@@ -1449,14 -1458,15 +1529,18 @@@ static int cpuset_can_attach(struct cgr
  	struct task_struct *task;
  	int ret;
  
 -	/* used later by cpuset_attach() */
 -	cpuset_attach_old_cs = task_cs(cgroup_taskset_first(tset, &css));
 -	cs = css_cs(css);
 -
  	mutex_lock(&cpuset_mutex);
  
 -	/* allow moving tasks into an empty cpuset if on default hierarchy */
 +	/*
 +	 * We allow to move tasks into an empty cpuset if sane_behavior
 +	 * flag is set.
 +	 */
  	ret = -ENOSPC;
++<<<<<<< HEAD:kernel/cpuset.c
 +	if (!cgroup_sane_behavior(cgrp) &&
++=======
+ 	if (!is_in_v2_mode() &&
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  	    (cpumask_empty(cs->cpus_allowed) || nodes_empty(cs->mems_allowed)))
  		goto out_unlock;
  
@@@ -1970,9 -1981,16 +2054,20 @@@ static int cpuset_css_online(struct cgr
  	if (is_spread_slab(parent))
  		set_bit(CS_SPREAD_SLAB, &cs->flags);
  
 -	cpuset_inc();
 +	number_of_cpusets++;
  
++<<<<<<< HEAD:kernel/cpuset.c
 +	if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags))
++=======
+ 	spin_lock_irq(&callback_lock);
+ 	if (is_in_v2_mode()) {
+ 		cpumask_copy(cs->effective_cpus, parent->effective_cpus);
+ 		cs->effective_mems = parent->effective_mems;
+ 	}
+ 	spin_unlock_irq(&callback_lock);
+ 
+ 	if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  		goto out_unlock;
  
  	/*
@@@ -2035,18 -2056,51 +2130,66 @@@ static void cpuset_css_free(struct cgro
  	kfree(cs);
  }
  
++<<<<<<< HEAD:kernel/cpuset.c
 +struct cgroup_subsys cpuset_subsys = {
 +	.name = "cpuset",
 +	.css_alloc = cpuset_css_alloc,
 +	.css_online = cpuset_css_online,
 +	.css_offline = cpuset_css_offline,
 +	.css_free = cpuset_css_free,
 +	.can_attach = cpuset_can_attach,
 +	.cancel_attach = cpuset_cancel_attach,
 +	.attach = cpuset_attach,
 +	.subsys_id = cpuset_subsys_id,
 +	.base_cftypes = files,
 +	.early_init = 1,
++=======
+ static void cpuset_bind(struct cgroup_subsys_state *root_css)
+ {
+ 	mutex_lock(&cpuset_mutex);
+ 	spin_lock_irq(&callback_lock);
+ 
+ 	if (is_in_v2_mode()) {
+ 		cpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);
+ 		top_cpuset.mems_allowed = node_possible_map;
+ 	} else {
+ 		cpumask_copy(top_cpuset.cpus_allowed,
+ 			     top_cpuset.effective_cpus);
+ 		top_cpuset.mems_allowed = top_cpuset.effective_mems;
+ 	}
+ 
+ 	spin_unlock_irq(&callback_lock);
+ 	mutex_unlock(&cpuset_mutex);
+ }
+ 
+ /*
+  * Make sure the new task conform to the current state of its parent,
+  * which could have been changed by cpuset just after it inherits the
+  * state from the parent and before it sits on the cgroup's task list.
+  */
+ static void cpuset_fork(struct task_struct *task)
+ {
+ 	if (task_css_is_root(task, cpuset_cgrp_id))
+ 		return;
+ 
+ 	set_cpus_allowed_ptr(task, &current->cpus_allowed);
+ 	task->mems_allowed = current->mems_allowed;
+ }
+ 
+ struct cgroup_subsys cpuset_cgrp_subsys = {
+ 	.css_alloc	= cpuset_css_alloc,
+ 	.css_online	= cpuset_css_online,
+ 	.css_offline	= cpuset_css_offline,
+ 	.css_free	= cpuset_css_free,
+ 	.can_attach	= cpuset_can_attach,
+ 	.cancel_attach	= cpuset_cancel_attach,
+ 	.attach		= cpuset_attach,
+ 	.post_attach	= cpuset_post_attach,
+ 	.bind		= cpuset_bind,
+ 	.fork		= cpuset_fork,
+ 	.legacy_cftypes	= files,
+ 	.early_init	= true,
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  };
  
  /**
@@@ -2137,49 -2249,20 +2280,58 @@@ retry
  		goto retry;
  	}
  
 -	cpumask_and(&new_cpus, cs->cpus_allowed, parent_cs(cs)->effective_cpus);
 -	nodes_and(new_mems, cs->mems_allowed, parent_cs(cs)->effective_mems);
 +	cpumask_andnot(&off_cpus, cs->cpus_allowed, top_cpuset.cpus_allowed);
 +	nodes_andnot(off_mems, cs->mems_allowed, top_cpuset.mems_allowed);
 +
 +	mutex_lock(&callback_mutex);
 +	cpumask_andnot(cs->cpus_allowed, cs->cpus_allowed, &off_cpus);
 +	cpumask_andnot(cs->effective_cpus, cs->effective_cpus, &off_cpus);
 +	mutex_unlock(&callback_mutex);
  
 -	cpus_updated = !cpumask_equal(&new_cpus, cs->effective_cpus);
 -	mems_updated = !nodes_equal(new_mems, cs->effective_mems);
++<<<<<<< HEAD:kernel/cpuset.c
 +	/*
 +	 * If sane_behavior flag is set, we need to update tasks' cpumask
 +	 * for empty cpuset to take on ancestor's cpumask.
 +	 */
 +	if ((sane && cpumask_empty(cs->cpus_allowed)) ||
 +	    !cpumask_empty(&off_cpus))
 +		update_tasks_cpumask(cs, NULL);
 +
 +	mutex_lock(&callback_mutex);
 +	nodes_andnot(cs->mems_allowed, cs->mems_allowed, off_mems);
 +	nodes_andnot(cs->effective_mems, cs->effective_mems, off_mems);
 +	mutex_unlock(&callback_mutex);
 +
 +	/*
 +	 * If sane_behavior flag is set, we need to update tasks' nodemask
 +	 * for empty cpuset to take on ancestor's nodemask.
 +	 */
 +	if ((sane && nodes_empty(cs->mems_allowed)) ||
 +	    !nodes_empty(off_mems))
 +		update_tasks_nodemask(cs, NULL);
  
 +	is_empty = cpumask_empty(cs->cpus_allowed) ||
 +		nodes_empty(cs->mems_allowed);
++=======
+ 	if (is_in_v2_mode())
+ 		hotplug_update_tasks(cs, &new_cpus, &new_mems,
+ 				     cpus_updated, mems_updated);
+ 	else
+ 		hotplug_update_tasks_legacy(cs, &new_cpus, &new_mems,
+ 					    cpus_updated, mems_updated);
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  
  	mutex_unlock(&cpuset_mutex);
 +
 +	/*
 +	 * If sane_behavior flag is set, we'll keep tasks in empty cpusets.
 +	 *
 +	 * Otherwise move tasks to the nearest ancestor with execution
 +	 * resources.  This is full cgroup operation which will
 +	 * also call back into cpuset.  Should be done outside any lock.
 +	 */
 +	if (!sane && is_empty)
 +		remove_tasks_in_empty_cpuset(cs);
  }
  
  /**
@@@ -2203,6 -2286,7 +2355,10 @@@ static void cpuset_hotplug_workfn(struc
  	static cpumask_t new_cpus;
  	static nodemask_t new_mems;
  	bool cpus_updated, mems_updated;
++<<<<<<< HEAD:kernel/cpuset.c
++=======
+ 	bool on_dfl = is_in_v2_mode();
++>>>>>>> b8d1b8ee93df (cpuset: Allow v2 behavior in v1 cgroup):kernel/cgroup/cpuset.c
  
  	mutex_lock(&cpuset_mutex);
  
* Unmerged path kernel/cpuset.c
