IB/mlx4: Use optimal numbers of MTT entries

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Guy Levi <guyle@mellanox.com>
commit 9901abf583683e58f95f822da63cd0e32e7b2f0a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/9901abf5.failed

Optimize the device performance by assigning multiple physical pages,
which are contiguous, to a single MTT. As a result, the number of MTTs
is reduced and in turn save cache misses of MTTs.

	Signed-off-by: Guy Levi <guyle@mellanox.com>
	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 9901abf583683e58f95f822da63cd0e32e7b2f0a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx4/mr.c
diff --cc drivers/infiniband/hw/mlx4/mr.c
index 433bcdbdd680,8f408a02f699..000000000000
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@@ -101,30 -197,48 +197,68 @@@ int mlx4_ib_umem_write_mtt(struct mlx4_
  	if (!pages)
  		return -ENOMEM;
  
- 	i = n = 0;
+ 	mtt_shift = mtt->page_shift;
+ 	mtt_size = 1ULL << mtt_shift;
  
++<<<<<<< HEAD
 +	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {
 +		len = sg_dma_len(sg) >> mtt->page_shift;
 +		for (k = 0; k < len; ++k) {
 +			pages[i++] = sg_dma_address(sg) +
 +				umem->page_size * k;
 +			/*
 +			 * Be friendly to mlx4_write_mtt() and
 +			 * pass it chunks of appropriate size.
 +			 */
 +			if (i == PAGE_SIZE / sizeof (u64)) {
 +				err = mlx4_write_mtt(dev->dev, mtt, n,
 +						     i, pages);
 +				if (err)
 +					goto out;
 +				n += i;
 +				i = 0;
 +			}
++=======
+ 	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, i) {
+ 		if (cur_start_addr + len == sg_dma_address(sg)) {
+ 			/* still the same block */
+ 			len += sg_dma_len(sg);
+ 			continue;
++>>>>>>> 9901abf58368 (IB/mlx4: Use optimal numbers of MTT entries)
  		}
+ 		/*
+ 		 * A new block is started ...
+ 		 * If len is malaligned, write an extra mtt entry to cover the
+ 		 * misaligned area (round up the division)
+ 		 */
+ 		err = mlx4_ib_umem_write_mtt_block(dev, mtt, mtt_size,
+ 						   mtt_shift, len,
+ 						   cur_start_addr,
+ 						   pages, &start_index,
+ 						   &npages);
+ 		if (err)
+ 			goto out;
+ 
+ 		cur_start_addr = sg_dma_address(sg);
+ 		len = sg_dma_len(sg);
+ 	}
+ 
+ 	/* Handle the last block */
+ 	if (len > 0) {
+ 		/*
+ 		 * If len is malaligned, write an extra mtt entry to cover
+ 		 * the misaligned area (round up the division)
+ 		 */
+ 		err = mlx4_ib_umem_write_mtt_block(dev, mtt, mtt_size,
+ 						   mtt_shift, len,
+ 						   cur_start_addr, pages,
+ 						   &start_index, &npages);
+ 		if (err)
+ 			goto out;
  	}
  
- 	if (i)
- 		err = mlx4_write_mtt(dev->dev, mtt, n, i, pages);
+ 	if (npages)
+ 		err = mlx4_write_mtt(dev->dev, mtt, start_index, npages, pages);
  
  out:
  	free_page((unsigned long) pages);
@@@ -155,7 -392,7 +412,11 @@@ struct ib_mr *mlx4_ib_reg_user_mr(struc
  	}
  
  	n = ib_umem_page_count(mr->umem);
++<<<<<<< HEAD
 +	shift = ilog2(mr->umem->page_size);
++=======
+ 	shift = mlx4_ib_umem_calc_optimal_mtt_size(mr->umem, start, &n);
++>>>>>>> 9901abf58368 (IB/mlx4: Use optimal numbers of MTT entries)
  
  	err = mlx4_mr_alloc(dev->dev, to_mpd(pd)->pdn, virt_addr, length,
  			    convert_access(access_flags), n, shift, &mr->mmr);
* Unmerged path drivers/infiniband/hw/mlx4/mr.c
