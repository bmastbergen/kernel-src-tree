md: fix deadlock between mddev_suspend() and md_write_start()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [md] fix deadlock between mddev_suspend() and md_write_start() (Nigel Croxon) [1506338]
Rebuild_FUZZ: 96.61%
commit-author NeilBrown <neilb@suse.com>
commit cc27b0c78c79680d128dbac79de0d40556d041bb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/cc27b0c7.failed

If mddev_suspend() races with md_write_start() we can deadlock
with mddev_suspend() waiting for the request that is currently
in md_write_start() to complete the ->make_request() call,
and md_write_start() waiting for the metadata to be updated
to mark the array as 'dirty'.
As metadata updates done by md_check_recovery() only happen then
the mddev_lock() can be claimed, and as mddev_suspend() is often
called with the lock held, these threads wait indefinitely for each
other.

We fix this by having md_write_start() abort if mddev_suspend()
is happening, and ->make_request() aborts if md_write_start()
aborted.
md_make_request() can detect this abort, decrease the ->active_io
count, and wait for mddev_suspend().

	Reported-by: Nix <nix@esperi.org.uk>
Fix: 68866e425be2(MD: no sync IO while suspended)
	Cc: stable@vger.kernel.org
	Signed-off-by: NeilBrown <neilb@suse.com>
	Signed-off-by: Shaohua Li <shli@fb.com>
(cherry picked from commit cc27b0c78c79680d128dbac79de0d40556d041bb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/faulty.c
#	drivers/md/linear.c
#	drivers/md/md.c
#	drivers/md/md.h
#	drivers/md/raid0.c
#	drivers/md/raid1.c
#	drivers/md/raid10.c
#	drivers/md/raid5.c
diff --cc drivers/md/faulty.c
index 619851d3b837,06a64d5d8c6c..000000000000
--- a/drivers/md/faulty.c
+++ b/drivers/md/faulty.c
@@@ -181,7 -181,7 +181,11 @@@ static bool faulty_make_request(struct 
  			/* special case - don't decrement, don't generic_make_request,
  			 * just fail immediately
  			 */
++<<<<<<< HEAD
 +			bio_endio(bio, -EIO);
++=======
+ 			bio_io_error(bio);
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  			return true;
  		}
  
diff --cc drivers/md/linear.c
index 2d3f4228a31e,5f1eb9189542..000000000000
--- a/drivers/md/linear.c
+++ b/drivers/md/linear.c
@@@ -266,50 -257,51 +266,63 @@@ static bool linear_make_request(struct 
  		return true;
  	}
  
 -	tmp_dev = which_dev(mddev, bio_sector);
 +	tmp_dev = which_dev(mddev, bio->bi_sector);
  	start_sector = tmp_dev->end_sector - tmp_dev->rdev->sectors;
 -	end_sector = tmp_dev->end_sector;
 -	data_offset = tmp_dev->rdev->data_offset;
 -
 -	if (unlikely(bio_sector >= end_sector ||
 -		     bio_sector < start_sector))
 -		goto out_of_bounds;
 -
 -	if (unlikely(bio_end_sector(bio) > end_sector)) {
 -		/* This bio crosses a device boundary, so we have to split it */
 -		struct bio *split = bio_split(bio, end_sector - bio_sector,
 -					      GFP_NOIO, mddev->bio_set);
 -		bio_chain(split, bio);
 -		generic_make_request(bio);
 -		bio = split;
 -	}
  
 +
 +	if (unlikely(bio->bi_sector >= (tmp_dev->end_sector)
 +		     || (bio->bi_sector < start_sector))) {
 +		char b[BDEVNAME_SIZE];
 +
 +		pr_err("md/linear:%s: make_request: Sector %llu out of bounds on dev %s: %llu sectors, offset %llu\n",
 +		       mdname(mddev),
 +		       (unsigned long long)bio->bi_sector,
 +		       bdevname(tmp_dev->rdev->bdev, b),
 +		       (unsigned long long)tmp_dev->rdev->sectors,
 +		       (unsigned long long)start_sector);
 +		bio_io_error(bio);
 +		return true;
 +	}
 +	if (unlikely(bio_end_sector(bio) > tmp_dev->end_sector)) {
 +		/* This bio crosses a device boundary, so we have to
 +		 * split it.
 +		 */
 +		struct bio_pair *bp;
 +		sector_t end_sector = tmp_dev->end_sector;
 +
 +		bp = bio_split(bio, end_sector - bio->bi_sector);
 +
 +		linear_make_request(mddev, &bp->bio1);
 +		linear_make_request(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return true;
 +	}
 +		    
  	bio->bi_bdev = tmp_dev->rdev->bdev;
 -	bio->bi_iter.bi_sector = bio->bi_iter.bi_sector -
 -		start_sector + data_offset;
 +	bio->bi_sector = bio->bi_sector - start_sector
 +		+ tmp_dev->rdev->data_offset;
  
 -	if (unlikely((bio_op(bio) == REQ_OP_DISCARD) &&
 +	if (unlikely((bio->bi_rw & REQ_DISCARD) &&
  		     !blk_queue_discard(bdev_get_queue(bio->bi_bdev)))) {
  		/* Just ignore it */
 -		bio_endio(bio);
 -	} else {
 -		if (mddev->gendisk)
 -			trace_block_bio_remap(bdev_get_queue(bio->bi_bdev),
 -					      bio, disk_devt(mddev->gendisk),
 -					      bio_sector);
 -		mddev_check_writesame(mddev, bio);
 -		mddev_check_write_zeroes(mddev, bio);
 -		generic_make_request(bio);
 +		bio_endio(bio, 0);
 +		return true;
  	}
++<<<<<<< HEAD
 +
 +	generic_make_request(bio);
++=======
+ 	return true;
+ 
+ out_of_bounds:
+ 	pr_err("md/linear:%s: make_request: Sector %llu out of bounds on dev %s: %llu sectors, offset %llu\n",
+ 	       mdname(mddev),
+ 	       (unsigned long long)bio->bi_iter.bi_sector,
+ 	       bdevname(tmp_dev->rdev->bdev, b),
+ 	       (unsigned long long)tmp_dev->rdev->sectors,
+ 	       (unsigned long long)start_sector);
+ 	bio_io_error(bio);
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  	return true;
  }
  
diff --cc drivers/md/md.c
index 4c75b0e4a2a4,d7847014821a..000000000000
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@@ -257,14 -269,15 +257,17 @@@ static void md_make_request(struct requ
  
  	if (mddev == NULL || mddev->pers == NULL) {
  		bio_io_error(bio);
 -		return BLK_QC_T_NONE;
 +		return;
  	}
  	if (mddev->ro == 1 && unlikely(rw == WRITE)) {
 -		if (bio_sectors(bio) != 0)
 -			bio->bi_error = -EROFS;
 -		bio_endio(bio);
 -		return BLK_QC_T_NONE;
 +		bio_endio(bio, bio_sectors(bio) == 0 ? 0 : -EROFS);
 +		return;
  	}
  check_suspended:
++<<<<<<< HEAD
 +	smp_rmb(); /* Ensure implications of  'active' are visible */
++=======
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  	rcu_read_lock();
  	if (mddev->suspended) {
  		DEFINE_WAIT(__wait);
@@@ -287,6 -300,8 +290,11 @@@
  	 * go away inside make_request
  	 */
  	sectors = bio_sectors(bio);
++<<<<<<< HEAD
++=======
+ 	/* bio could be mergeable after passing to underlayer */
+ 	bio->bi_opf &= ~REQ_NOMERGE;
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  	if (!mddev->pers->make_request(mddev, bio)) {
  		atomic_dec(&mddev->active_io);
  		wake_up(&mddev->sb_wait);
@@@ -7711,7 -7996,7 +7719,11 @@@ bool md_write_start(struct mddev *mddev
  	wait_event(mddev->sb_wait,
  		   !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags) && !mddev->suspended);
  	if (test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags)) {
++<<<<<<< HEAD
 +		atomic_dec(&mddev->writes_pending);
++=======
+ 		percpu_ref_put(&mddev->writes_pending);
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  		return false;
  	}
  	return true;
diff --cc drivers/md/md.h
index 0d13bf88f41f,63d342d560b8..000000000000
--- a/drivers/md/md.h
+++ b/drivers/md/md.h
@@@ -649,7 -648,9 +649,13 @@@ extern void md_unregister_thread(struc
  extern void md_wakeup_thread(struct md_thread *thread);
  extern void md_check_recovery(struct mddev *mddev);
  extern void md_reap_sync_thread(struct mddev *mddev);
++<<<<<<< HEAD
 +extern bool md_write_start(struct mddev *mddev, struct bio *bi);
++=======
+ extern int mddev_init_writes_pending(struct mddev *mddev);
+ extern bool md_write_start(struct mddev *mddev, struct bio *bi);
+ extern void md_write_inc(struct mddev *mddev, struct bio *bi);
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  extern void md_write_end(struct mddev *mddev);
  extern void md_done_sync(struct mddev *mddev, int blocks, int ok);
  extern void md_error(struct mddev *mddev, struct md_rdev *rdev);
diff --cc drivers/md/raid0.c
index 5d0952d819f8,94d9ae9b0fd0..000000000000
--- a/drivers/md/raid0.c
+++ b/drivers/md/raid0.c
@@@ -527,66 -459,147 +527,171 @@@ static inline int is_io_in_chunk_bounda
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void raid0_handle_discard(struct mddev *mddev, struct bio *bio)
+ {
+ 	struct r0conf *conf = mddev->private;
+ 	struct strip_zone *zone;
+ 	sector_t start = bio->bi_iter.bi_sector;
+ 	sector_t end;
+ 	unsigned int stripe_size;
+ 	sector_t first_stripe_index, last_stripe_index;
+ 	sector_t start_disk_offset;
+ 	unsigned int start_disk_index;
+ 	sector_t end_disk_offset;
+ 	unsigned int end_disk_index;
+ 	unsigned int disk;
+ 
+ 	zone = find_zone(conf, &start);
+ 
+ 	if (bio_end_sector(bio) > zone->zone_end) {
+ 		struct bio *split = bio_split(bio,
+ 			zone->zone_end - bio->bi_iter.bi_sector, GFP_NOIO,
+ 			mddev->bio_set);
+ 		bio_chain(split, bio);
+ 		generic_make_request(bio);
+ 		bio = split;
+ 		end = zone->zone_end;
+ 	} else
+ 		end = bio_end_sector(bio);
+ 
+ 	if (zone != conf->strip_zone)
+ 		end = end - zone[-1].zone_end;
+ 
+ 	/* Now start and end is the offset in zone */
+ 	stripe_size = zone->nb_dev * mddev->chunk_sectors;
+ 
+ 	first_stripe_index = start;
+ 	sector_div(first_stripe_index, stripe_size);
+ 	last_stripe_index = end;
+ 	sector_div(last_stripe_index, stripe_size);
+ 
+ 	start_disk_index = (int)(start - first_stripe_index * stripe_size) /
+ 		mddev->chunk_sectors;
+ 	start_disk_offset = ((int)(start - first_stripe_index * stripe_size) %
+ 		mddev->chunk_sectors) +
+ 		first_stripe_index * mddev->chunk_sectors;
+ 	end_disk_index = (int)(end - last_stripe_index * stripe_size) /
+ 		mddev->chunk_sectors;
+ 	end_disk_offset = ((int)(end - last_stripe_index * stripe_size) %
+ 		mddev->chunk_sectors) +
+ 		last_stripe_index * mddev->chunk_sectors;
+ 
+ 	for (disk = 0; disk < zone->nb_dev; disk++) {
+ 		sector_t dev_start, dev_end;
+ 		struct bio *discard_bio = NULL;
+ 		struct md_rdev *rdev;
+ 
+ 		if (disk < start_disk_index)
+ 			dev_start = (first_stripe_index + 1) *
+ 				mddev->chunk_sectors;
+ 		else if (disk > start_disk_index)
+ 			dev_start = first_stripe_index * mddev->chunk_sectors;
+ 		else
+ 			dev_start = start_disk_offset;
+ 
+ 		if (disk < end_disk_index)
+ 			dev_end = (last_stripe_index + 1) * mddev->chunk_sectors;
+ 		else if (disk > end_disk_index)
+ 			dev_end = last_stripe_index * mddev->chunk_sectors;
+ 		else
+ 			dev_end = end_disk_offset;
+ 
+ 		if (dev_end <= dev_start)
+ 			continue;
+ 
+ 		rdev = conf->devlist[(zone - conf->strip_zone) *
+ 			conf->strip_zone[0].nb_dev + disk];
+ 		if (__blkdev_issue_discard(rdev->bdev,
+ 			dev_start + zone->dev_start + rdev->data_offset,
+ 			dev_end - dev_start, GFP_NOIO, 0, &discard_bio) ||
+ 		    !discard_bio)
+ 			continue;
+ 		bio_chain(discard_bio, bio);
+ 		if (mddev->gendisk)
+ 			trace_block_bio_remap(bdev_get_queue(rdev->bdev),
+ 				discard_bio, disk_devt(mddev->gendisk),
+ 				bio->bi_iter.bi_sector);
+ 		generic_make_request(discard_bio);
+ 	}
+ 	bio_endio(bio);
+ }
+ 
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  static bool raid0_make_request(struct mddev *mddev, struct bio *bio)
  {
 +	unsigned int chunk_sects;
 +	sector_t sector_offset;
  	struct strip_zone *zone;
  	struct md_rdev *tmp_dev;
 -	sector_t bio_sector;
 -	sector_t sector;
 -	unsigned chunk_sects;
 -	unsigned sectors;
  
 -	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
  		md_flush_request(mddev, bio);
  		return true;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (unlikely((bio_op(bio) == REQ_OP_DISCARD))) {
+ 		raid0_handle_discard(mddev, bio);
+ 		return true;
+ 	}
+ 
+ 	bio_sector = bio->bi_iter.bi_sector;
+ 	sector = bio_sector;
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  	chunk_sects = mddev->chunk_sectors;
 -
 -	sectors = chunk_sects -
 -		(likely(is_power_of_2(chunk_sects))
 -		 ? (sector & (chunk_sects-1))
 -		 : sector_div(sector, chunk_sects));
 -
 -	/* Restore due to sector_div */
 -	sector = bio_sector;
 -
 -	if (sectors < bio_sectors(bio)) {
 -		struct bio *split = bio_split(bio, sectors, GFP_NOIO, mddev->bio_set);
 -		bio_chain(split, bio);
 -		generic_make_request(bio);
 -		bio = split;
 +	if (unlikely(!is_io_in_chunk_boundary(mddev, chunk_sects, bio))) {
 +		sector_t sector = bio->bi_sector;
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		if (likely(is_power_of_2(chunk_sects)))
 +			bp = bio_split(bio, chunk_sects - (sector &
 +							   (chunk_sects-1)));
 +		else
 +			bp = bio_split(bio, chunk_sects -
 +				       sector_div(sector, chunk_sects));
 +		raid0_make_request(mddev, &bp->bio1);
 +		raid0_make_request(mddev, &bp->bio2);
 +		bio_pair_release(bp);
 +		return true;
  	}
  
 -	zone = find_zone(mddev->private, &sector);
 -	tmp_dev = map_sector(mddev, zone, sector, &sector);
 +	sector_offset = bio->bi_sector;
 +	zone = find_zone(mddev->private, &sector_offset);
 +	tmp_dev = map_sector(mddev, zone, bio->bi_sector,
 +			     &sector_offset);
  	bio->bi_bdev = tmp_dev->bdev;
 -	bio->bi_iter.bi_sector = sector + zone->dev_start +
 +	bio->bi_sector = sector_offset + zone->dev_start +
  		tmp_dev->data_offset;
  
 -	if (mddev->gendisk)
 -		trace_block_bio_remap(bdev_get_queue(bio->bi_bdev),
 -				      bio, disk_devt(mddev->gendisk),
 -				      bio_sector);
 -	mddev_check_writesame(mddev, bio);
 -	mddev_check_write_zeroes(mddev, bio);
 +	if (unlikely((bio->bi_rw & REQ_DISCARD) &&
 +		     !blk_queue_discard(bdev_get_queue(bio->bi_bdev)))) {
 +		/* Just ignore it */
 +		bio_endio(bio, 0);
 +		return true;
 +	}
 +
  	generic_make_request(bio);
  	return true;
++<<<<<<< HEAD
 +
 +bad_map:
 +	printk("md/raid0:%s: make_request bug: can't convert block across chunks"
 +	       " or bigger than %dk %llu %d\n",
 +	       mdname(mddev), chunk_sects / 2,
 +	       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
 +
 +	bio_io_error(bio);
 +	return true;
++=======
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  }
  
  static void raid0_status(struct seq_file *seq, struct mddev *mddev)
diff --cc drivers/md/raid1.c
index 481b2b2701df,c71739b87ab7..000000000000
--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@@ -1528,30 -1559,22 +1528,39 @@@ static bool raid1_make_request(struct m
  	}
  
  	/*
 -	 * There is a limit to the maximum size, but
 -	 * the read/write handler might find a lower limit
 -	 * due to bad blocks.  To avoid multiple splits,
 -	 * we pass the maximum number of sectors down
 -	 * and let the lower level perform the split.
 +	 * make_request() can abort the operation when read-ahead is being
 +	 * used and no empty request is available.
 +	 *
 +	 */
 +	r1_bio = alloc_r1bio(mddev, bio, 0);
 +
 +	/*
 +	 * We might need to issue multiple reads to different devices if there
 +	 * are bad blocks around, so we keep track of the number of reads in
 +	 * bio->bi_phys_segments.  If this is 0, there is only one r1_bio and
 +	 * no locking will be needed when requests complete.  If it is
 +	 * non-zero, then it is the number of not-completed requests.
  	 */
 -	sectors = align_to_barrier_unit_end(
 -		bio->bi_iter.bi_sector, bio_sectors(bio));
 +	bio->bi_phys_segments = 0;
 +	clear_bit(BIO_SEG_VALID, &bio->bi_flags);
  
  	if (bio_data_dir(bio) == READ)
++<<<<<<< HEAD
 +		raid1_read_request(mddev, bio, r1_bio);
 +	else {
 +		if (!md_write_start(mddev, bio))
 +			return false;
 +		raid1_write_request(mddev, bio, r1_bio);
 +	}
 +
++=======
+ 		raid1_read_request(mddev, bio, sectors, NULL);
+ 	else {
+ 		if (!md_write_start(mddev,bio))
+ 			return false;
+ 		raid1_write_request(mddev, bio, sectors);
+ 	}
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  	return true;
  }
  
diff --cc drivers/md/raid10.c
index 9928c2ec9859,52acffa7a06a..000000000000
--- a/drivers/md/raid10.c
+++ b/drivers/md/raid10.c
@@@ -1160,85 -1111,198 +1160,88 @@@ static void raid10_unplug(struct blk_pl
  	kfree(plug);
  }
  
 -static void raid10_read_request(struct mddev *mddev, struct bio *bio,
 -				struct r10bio *r10_bio)
 +static bool raid10_make_request(struct mddev *mddev, struct bio * bio)
  {
  	struct r10conf *conf = mddev->private;
 +	struct r10bio *r10_bio;
  	struct bio *read_bio;
 -	const int op = bio_op(bio);
 -	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
 +	int i;
 +	sector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);
 +	int chunk_sects = chunk_mask + 1;
 +	const int rw = bio_data_dir(bio);
 +	const unsigned long do_sync = (bio->bi_rw & REQ_SYNC);
 +	const unsigned long do_fua = (bio->bi_rw & REQ_FUA);
 +	const unsigned long do_discard = (bio->bi_rw
 +					  & (REQ_DISCARD | REQ_SECURE));
 +	const unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);
 +	unsigned long flags;
 +	struct md_rdev *blocked_rdev;
 +	struct blk_plug_cb *cb;
 +	struct raid10_plug_cb *plug = NULL;
 +	int sectors_handled;
  	int max_sectors;
 -	sector_t sectors;
 -	struct md_rdev *rdev;
 -	char b[BDEVNAME_SIZE];
 -	int slot = r10_bio->read_slot;
 -	struct md_rdev *err_rdev = NULL;
 -	gfp_t gfp = GFP_NOIO;
 -
 -	if (r10_bio->devs[slot].rdev) {
 -		/*
 -		 * This is an error retry, but we cannot
 -		 * safely dereference the rdev in the r10_bio,
 -		 * we must use the one in conf.
 -		 * If it has already been disconnected (unlikely)
 -		 * we lose the device name in error messages.
 -		 */
 -		int disk;
 -		/*
 -		 * As we are blocking raid10, it is a little safer to
 -		 * use __GFP_HIGH.
 -		 */
 -		gfp = GFP_NOIO | __GFP_HIGH;
 -
 -		rcu_read_lock();
 -		disk = r10_bio->devs[slot].devnum;
 -		err_rdev = rcu_dereference(conf->mirrors[disk].rdev);
 -		if (err_rdev)
 -			bdevname(err_rdev->bdev, b);
 -		else {
 -			strcpy(b, "???");
 -			/* This never gets dereferenced */
 -			err_rdev = r10_bio->devs[slot].rdev;
 -		}
 -		rcu_read_unlock();
 -	}
 -	/*
 -	 * Register the new request and wait if the reconstruction
 -	 * thread has put up a bar for new requests.
 -	 * Continue immediately if no resync is active currently.
 -	 */
 -	wait_barrier(conf);
 -
 -	sectors = r10_bio->sectors;
 -	while (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&
 -	    bio->bi_iter.bi_sector < conf->reshape_progress &&
 -	    bio->bi_iter.bi_sector + sectors > conf->reshape_progress) {
 -		/*
 -		 * IO spans the reshape position.  Need to wait for reshape to
 -		 * pass
 -		 */
 -		raid10_log(conf->mddev, "wait reshape");
 -		allow_barrier(conf);
 -		wait_event(conf->wait_barrier,
 -			   conf->reshape_progress <= bio->bi_iter.bi_sector ||
 -			   conf->reshape_progress >= bio->bi_iter.bi_sector +
 -			   sectors);
 -		wait_barrier(conf);
 -	}
 +	int sectors;
  
 -	rdev = read_balance(conf, r10_bio, &max_sectors);
 -	if (!rdev) {
 -		if (err_rdev) {
 -			pr_crit_ratelimited("md/raid10:%s: %s: unrecoverable I/O read error for block %llu\n",
 -					    mdname(mddev), b,
 -					    (unsigned long long)r10_bio->sector);
 -		}
 -		raid_end_bio_io(r10_bio);
 -		return;
 -	}
 -	if (err_rdev)
 -		pr_err_ratelimited("md/raid10:%s: %s: redirecting sector %llu to another mirror\n",
 -				   mdname(mddev),
 -				   bdevname(rdev->bdev, b),
 -				   (unsigned long long)r10_bio->sector);
 -	if (max_sectors < bio_sectors(bio)) {
 -		struct bio *split = bio_split(bio, max_sectors,
 -					      gfp, conf->bio_split);
 -		bio_chain(split, bio);
 -		generic_make_request(bio);
 -		bio = split;
 -		r10_bio->master_bio = bio;
 -		r10_bio->sectors = max_sectors;
++<<<<<<< HEAD
 +	if (unlikely(bio->bi_rw & REQ_FLUSH)) {
 +		md_flush_request(mddev, bio);
 +		return true;
  	}
 -	slot = r10_bio->read_slot;
 -
 -	read_bio = bio_clone_fast(bio, gfp, mddev->bio_set);
  
 -	r10_bio->devs[slot].bio = read_bio;
 -	r10_bio->devs[slot].rdev = rdev;
  
 -	read_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +
 -		choose_data_offset(r10_bio, rdev);
 -	read_bio->bi_bdev = rdev->bdev;
 -	read_bio->bi_end_io = raid10_end_read_request;
 -	bio_set_op_attrs(read_bio, op, do_sync);
 -	if (test_bit(FailFast, &rdev->flags) &&
 -	    test_bit(R10BIO_FailFast, &r10_bio->state))
 -	        read_bio->bi_opf |= MD_FAILFAST;
 -	read_bio->bi_private = r10_bio;
 +	if (!md_write_start(mddev, bio))
 +		return false;
  
 -	if (mddev->gendisk)
 -	        trace_block_bio_remap(bdev_get_queue(read_bio->bi_bdev),
 -	                              read_bio, disk_devt(mddev->gendisk),
 -	                              r10_bio->sector);
 -	generic_make_request(read_bio);
 -	return;
 -}
 +	/* If this request crosses a chunk boundary, we need to
 +	 * split it.  This will only happen for 1 PAGE (or less) requests.
 +	 */
 +	if (unlikely((bio->bi_sector & chunk_mask) + bio_sectors(bio)
 +		     > chunk_sects
 +		     && (conf->geo.near_copies < conf->geo.raid_disks
 +			 || conf->prev.near_copies < conf->prev.raid_disks))) {
 +		struct bio_pair *bp;
 +		/* Sanity check -- queue functions should prevent this happening */
 +		if (bio_segments(bio) > 1)
 +			goto bad_map;
 +		/* This is a one page bio that upper layers
 +		 * refuse to split for us, so we need to split it.
 +		 */
 +		bp = bio_split(bio,
 +			       chunk_sects - (bio->bi_sector & (chunk_sects - 1)) );
 +
 +		/* Each of these 'make_request' calls will call 'wait_barrier'.
 +		 * If the first succeeds but the second blocks due to the resync
 +		 * thread raising the barrier, we will deadlock because the
 +		 * IO to the underlying device will be queued in generic_make_request
 +		 * and will never complete, so will never reduce nr_pending.
 +		 * So increment nr_waiting here so no new raise_barriers will
 +		 * succeed, and so the second wait_barrier cannot block.
 +		 */
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting++;
 +		spin_unlock_irq(&conf->resync_lock);
  
 -static void raid10_write_one_disk(struct mddev *mddev, struct r10bio *r10_bio,
 -				  struct bio *bio, bool replacement,
 -				  int n_copy)
 -{
 -	const int op = bio_op(bio);
 -	const unsigned long do_sync = (bio->bi_opf & REQ_SYNC);
 -	const unsigned long do_fua = (bio->bi_opf & REQ_FUA);
 -	unsigned long flags;
 -	struct blk_plug_cb *cb;
 -	struct raid10_plug_cb *plug = NULL;
 -	struct r10conf *conf = mddev->private;
 -	struct md_rdev *rdev;
 -	int devnum = r10_bio->devs[n_copy].devnum;
 -	struct bio *mbio;
 -
 -	if (replacement) {
 -		rdev = conf->mirrors[devnum].replacement;
 -		if (rdev == NULL) {
 -			/* Replacement just got moved to main 'rdev' */
 -			smp_mb();
 -			rdev = conf->mirrors[devnum].rdev;
 -		}
 -	} else
 -		rdev = conf->mirrors[devnum].rdev;
 +		raid10_make_request(mddev, &bp->bio1);
 +		raid10_make_request(mddev, &bp->bio2);
  
 -	mbio = bio_clone_fast(bio, GFP_NOIO, mddev->bio_set);
 -	if (replacement)
 -		r10_bio->devs[n_copy].repl_bio = mbio;
 -	else
 -		r10_bio->devs[n_copy].bio = mbio;
 -
 -	mbio->bi_iter.bi_sector	= (r10_bio->devs[n_copy].addr +
 -				   choose_data_offset(r10_bio, rdev));
 -	mbio->bi_bdev = rdev->bdev;
 -	mbio->bi_end_io	= raid10_end_write_request;
 -	bio_set_op_attrs(mbio, op, do_sync | do_fua);
 -	if (!replacement && test_bit(FailFast,
 -				     &conf->mirrors[devnum].rdev->flags)
 -			 && enough(conf, devnum))
 -		mbio->bi_opf |= MD_FAILFAST;
 -	mbio->bi_private = r10_bio;
 -
 -	if (conf->mddev->gendisk)
 -		trace_block_bio_remap(bdev_get_queue(mbio->bi_bdev),
 -				      mbio, disk_devt(conf->mddev->gendisk),
 -				      r10_bio->sector);
 -	/* flush_pending_writes() needs access to the rdev so...*/
 -	mbio->bi_bdev = (void *)rdev;
 +		spin_lock_irq(&conf->resync_lock);
 +		conf->nr_waiting--;
 +		wake_up(&conf->wait_barrier);
 +		spin_unlock_irq(&conf->resync_lock);
  
 -	atomic_inc(&r10_bio->remaining);
 +		bio_pair_release(bp);
 +		return true;
 +	bad_map:
 +		printk("md/raid10:%s: make_request bug: can't convert block across chunks"
 +		       " or bigger than %dk %llu %d\n", mdname(mddev), chunk_sects/2,
 +		       (unsigned long long)bio->bi_sector, bio_sectors(bio) / 2);
  
 -	cb = blk_check_plugged(raid10_unplug, mddev, sizeof(*plug));
 -	if (cb)
 -		plug = container_of(cb, struct raid10_plug_cb, cb);
 -	else
 -		plug = NULL;
 -	if (plug) {
 -		bio_list_add(&plug->pending, mbio);
 -		plug->pending_cnt++;
 -	} else {
 -		spin_lock_irqsave(&conf->device_lock, flags);
 -		bio_list_add(&conf->pending_bio_list, mbio);
 -		conf->pending_count++;
 -		spin_unlock_irqrestore(&conf->device_lock, flags);
 -		md_wakeup_thread(mddev->thread);
 +		bio_io_error(bio);
 +		return true;
  	}
 -}
 -
 -static void raid10_write_request(struct mddev *mddev, struct bio *bio,
 -				 struct r10bio *r10_bio)
 -{
 -	struct r10conf *conf = mddev->private;
 -	int i;
 -	struct md_rdev *blocked_rdev;
 -	sector_t sectors;
 -	int max_sectors;
  
++=======
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  	/*
  	 * Register the new request and wait if the reconstruction
  	 * thread has put up a bar for new requests.
@@@ -1518,100 -1494,63 +1521,154 @@@ retry_write
  	bitmap_startwrite(mddev->bitmap, r10_bio->sector, r10_bio->sectors, 0);
  
  	for (i = 0; i < conf->copies; i++) {
 -		if (r10_bio->devs[i].bio)
 -			raid10_write_one_disk(mddev, r10_bio, bio, false, i);
 -		if (r10_bio->devs[i].repl_bio)
 -			raid10_write_one_disk(mddev, r10_bio, bio, true, i);
 +		struct bio *mbio;
 +		int d = r10_bio->devs[i].devnum;
 +		if (r10_bio->devs[i].bio) {
 +			struct md_rdev *rdev = conf->mirrors[d].rdev;
 +			mbio = bio_clone_mddev(bio, GFP_NOIO, mddev);
 +			bio_trim(mbio, r10_bio->sector - bio->bi_sector,
 +				 max_sectors);
 +			r10_bio->devs[i].bio = mbio;
 +
 +			mbio->bi_sector	= (r10_bio->devs[i].addr+
 +					   choose_data_offset(r10_bio,
 +							      rdev));
 +			mbio->bi_bdev = rdev->bdev;
 +			mbio->bi_end_io	= raid10_end_write_request;
 +			mbio->bi_rw =
 +				WRITE | do_sync | do_fua | do_discard | do_same;
 +			if (test_bit(FailFast, &conf->mirrors[d].rdev->flags) &&
 +			    enough(conf, d))
 +				mbio->bi_rw |= MD_FAILFAST;
 +			mbio->bi_private = r10_bio;
 +
 +			atomic_inc(&r10_bio->remaining);
 +
 +			cb = blk_check_plugged(raid10_unplug, mddev,
 +					       sizeof(*plug));
 +			if (cb)
 +				plug = container_of(cb, struct raid10_plug_cb,
 +						    cb);
 +			else
 +				plug = NULL;
 +			spin_lock_irqsave(&conf->device_lock, flags);
 +			if (plug) {
 +				bio_list_add(&plug->pending, mbio);
 +				plug->pending_cnt++;
 +			} else {
 +				bio_list_add(&conf->pending_bio_list, mbio);
 +				conf->pending_count++;
 +			}
 +			spin_unlock_irqrestore(&conf->device_lock, flags);
 +			if (!plug)
 +				md_wakeup_thread(mddev->thread);
 +		}
 +
 +		if (r10_bio->devs[i].repl_bio) {
 +			struct md_rdev *rdev = conf->mirrors[d].replacement;
 +			if (rdev == NULL) {
 +				/* Replacement just got moved to main 'rdev' */
 +				smp_mb();
 +				rdev = conf->mirrors[d].rdev;
 +			}
 +			mbio = bio_clone_mddev(bio, GFP_NOIO, mddev);
 +			bio_trim(mbio, r10_bio->sector - bio->bi_sector,
 +				 max_sectors);
 +			r10_bio->devs[i].repl_bio = mbio;
 +
 +			mbio->bi_sector	= (r10_bio->devs[i].addr +
 +					   choose_data_offset(
 +						   r10_bio, rdev));
 +			mbio->bi_bdev = rdev->bdev;
 +			mbio->bi_end_io	= raid10_end_write_request;
 +			mbio->bi_rw =
 +				WRITE | do_sync | do_fua | do_discard | do_same;
 +			mbio->bi_private = r10_bio;
 +
 +			atomic_inc(&r10_bio->remaining);
 +			spin_lock_irqsave(&conf->device_lock, flags);
 +			bio_list_add(&conf->pending_bio_list, mbio);
 +			conf->pending_count++;
 +			spin_unlock_irqrestore(&conf->device_lock, flags);
 +			if (!mddev_check_plugged(mddev))
 +				md_wakeup_thread(mddev->thread);
 +		}
 +	}
 +
 +	/* Don't remove the bias on 'remaining' (one_write_done) until
 +	 * after checking if we need to go around again.
 +	 */
 +
 +	if (sectors_handled < bio_sectors(bio)) {
 +		one_write_done(r10_bio);
 +		/* We need another r10_bio.  It has already been counted
 +		 * in bio->bi_phys_segments.
 +		 */
 +		r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
 +
 +		r10_bio->master_bio = bio;
 +		r10_bio->sectors = bio_sectors(bio) - sectors_handled;
 +
 +		r10_bio->mddev = mddev;
 +		r10_bio->sector = bio->bi_sector + sectors_handled;
 +		r10_bio->state = 0;
 +		goto retry_write;
  	}
  	one_write_done(r10_bio);
++<<<<<<< HEAD
++=======
+ }
+ 
+ static void __make_request(struct mddev *mddev, struct bio *bio, int sectors)
+ {
+ 	struct r10conf *conf = mddev->private;
+ 	struct r10bio *r10_bio;
+ 
+ 	r10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);
+ 
+ 	r10_bio->master_bio = bio;
+ 	r10_bio->sectors = sectors;
+ 
+ 	r10_bio->mddev = mddev;
+ 	r10_bio->sector = bio->bi_iter.bi_sector;
+ 	r10_bio->state = 0;
+ 	memset(r10_bio->devs, 0, sizeof(r10_bio->devs[0]) * conf->copies);
+ 
+ 	if (bio_data_dir(bio) == READ)
+ 		raid10_read_request(mddev, bio, r10_bio);
+ 	else
+ 		raid10_write_request(mddev, bio, r10_bio);
+ }
+ 
+ static bool raid10_make_request(struct mddev *mddev, struct bio *bio)
+ {
+ 	struct r10conf *conf = mddev->private;
+ 	sector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);
+ 	int chunk_sects = chunk_mask + 1;
+ 	int sectors = bio_sectors(bio);
+ 
+ 	if (unlikely(bio->bi_opf & REQ_PREFLUSH)) {
+ 		md_flush_request(mddev, bio);
+ 		return true;
+ 	}
+ 
+ 	if (!md_write_start(mddev, bio))
+ 		return false;
+ 
+ 	/*
+ 	 * If this request crosses a chunk boundary, we need to split
+ 	 * it.
+ 	 */
+ 	if (unlikely((bio->bi_iter.bi_sector & chunk_mask) +
+ 		     sectors > chunk_sects
+ 		     && (conf->geo.near_copies < conf->geo.raid_disks
+ 			 || conf->prev.near_copies <
+ 			 conf->prev.raid_disks)))
+ 		sectors = chunk_sects -
+ 			(bio->bi_iter.bi_sector &
+ 			 (chunk_sects - 1));
+ 	__make_request(mddev, bio, sectors);
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  
  	/* In case raid10d snuck in to freeze_array */
  	wake_up(&conf->wait_barrier);
diff --cc drivers/md/raid5.c
index aa86463a5e50,b218a42fd702..000000000000
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@@ -5281,11 -5475,10 +5281,14 @@@ static void make_discard_request(struc
  		/* Skip discard while reshape is happening */
  		return;
  
 -	logical_sector = bi->bi_iter.bi_sector & ~((sector_t)STRIPE_SECTORS-1);
 -	last_sector = bi->bi_iter.bi_sector + (bi->bi_iter.bi_size>>9);
 +	logical_sector = bi->bi_sector & ~((sector_t)STRIPE_SECTORS-1);
 +	last_sector = bi->bi_sector + (bi->bi_size>>9);
  
  	bi->bi_next = NULL;
++<<<<<<< HEAD
 +	bi->bi_phys_segments = 1; /* over-loaded to count active stripes */
++=======
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  
  	stripe_sectors = conf->chunk_sectors *
  		(conf->raid_disks - conf->max_degraded);
@@@ -5354,9 -5548,7 +5357,13 @@@
  		release_stripe_plug(mddev, sh);
  	}
  
++<<<<<<< HEAD
 +	remaining = raid5_dec_bi_active_stripes(bi);
 +	if (remaining == 0)
 +		bio_endio(bi, 0);
++=======
+ 	bio_endio(bi);
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  }
  
  static bool raid5_make_request(struct mddev *mddev, struct bio * bi)
@@@ -5381,32 -5573,36 +5388,45 @@@
  			return true;
  		}
  		/* ret == -EAGAIN, fallback */
 -		/*
 -		 * if r5l_handle_flush_request() didn't clear REQ_PREFLUSH,
 -		 * we need to flush journal device
 -		 */
 -		do_flush = bi->bi_opf & REQ_PREFLUSH;
  	}
  
+ 	if (!md_write_start(mddev, bi))
+ 		return false;
  	/*
  	 * If array is degraded, better not do chunk aligned read because
  	 * later we might have to read it again in order to reconstruct
  	 * data on failed drives.
  	 */
  	if (rw == READ && mddev->degraded == 0 &&
++<<<<<<< HEAD
 +	    !r5c_is_writeback(conf->log) &&
 +	     mddev->reshape_position == MaxSector &&
 +	     chunk_aligned_read(mddev,bi))
 +		return true;
++=======
+ 	    mddev->reshape_position == MaxSector) {
+ 		bi = chunk_aligned_read(mddev, bi);
+ 		if (!bi)
+ 			return true;
+ 	}
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  
 -	if (unlikely(bio_op(bi) == REQ_OP_DISCARD)) {
 +	if (!md_write_start(mddev, bi))
 +		return false;
 +
 +	if (unlikely(bi->bi_rw & REQ_DISCARD)) {
  		make_discard_request(mddev, bi);
  		md_write_end(mddev);
  		return true;
  	}
  
 -	logical_sector = bi->bi_iter.bi_sector & ~((sector_t)STRIPE_SECTORS-1);
 +	logical_sector = bi->bi_sector & ~((sector_t)STRIPE_SECTORS-1);
  	last_sector = bio_end_sector(bi);
  	bi->bi_next = NULL;
++<<<<<<< HEAD
 +	bi->bi_phys_segments = 1;	/* over-loaded to count active stripes */
++=======
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  
  	prepare_to_wait(&conf->wait_for_overlap, &w, TASK_UNINTERRUPTIBLE);
  	for (;logical_sector < last_sector; logical_sector += STRIPE_SECTORS) {
@@@ -5538,16 -5737,9 +5558,22 @@@
  	}
  	finish_wait(&conf->wait_for_overlap, &w);
  
++<<<<<<< HEAD
 +	remaining = raid5_dec_bi_active_stripes(bi);
 +	if (remaining == 0) {
 +
 +		if ( rw == WRITE )
 +			md_write_end(mddev);
 +
 +		trace_block_bio_complete(bdev_get_queue(bi->bi_bdev),
 +					 bi, 0);
 +		bio_endio(bi, 0);
 +	}
++=======
+ 	if (rw == WRITE)
+ 		md_write_end(mddev);
+ 	bio_endio(bi);
++>>>>>>> cc27b0c78c79 (md: fix deadlock between mddev_suspend() and md_write_start())
  	return true;
  }
  
* Unmerged path drivers/md/faulty.c
* Unmerged path drivers/md/linear.c
* Unmerged path drivers/md/md.c
* Unmerged path drivers/md/md.h
* Unmerged path drivers/md/raid0.c
* Unmerged path drivers/md/raid1.c
* Unmerged path drivers/md/raid10.c
* Unmerged path drivers/md/raid5.c
