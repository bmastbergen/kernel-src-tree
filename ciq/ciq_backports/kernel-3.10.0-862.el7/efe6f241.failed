iommu/amd: Enable ga_log_intr when enabling guest_mode

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] amd: Enable ga_log_intr when enabling guest_mode (Jerry Snitselaar) [1411581]
Rebuild_FUZZ: 94.12%
commit-author Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
commit efe6f241602cb61466895f6816b8ea6b90f04d4e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/efe6f241.failed

IRTE[GALogIntr] bit should set when enabling guest_mode, which enables
IOMMU to generate entry in GALog when IRTE[IsRun] is not set, and send
an interrupt to notify IOMMU driver.

	Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
	Cc: Joerg Roedel <jroedel@suse.de>
	Cc: stable@vger.kernel.org # v4.9+
Fixes: d98de49a53e48 ('iommu/amd: Enable vAPIC interrupt remapping mode by default')
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit efe6f241602cb61466895f6816b8ea6b90f04d4e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
diff --cc drivers/iommu/amd_iommu.c
index aaf5f1c5dc10,354cbd6392cd..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -4054,12 -4200,389 +4054,397 @@@ struct irq_remap_ops amd_iommu_irq_ops 
  	.disable		= amd_iommu_disable,
  	.reenable		= amd_iommu_reenable,
  	.enable_faulting	= amd_iommu_enable_faulting,
 -	.get_ir_irq_domain	= get_ir_irq_domain,
 -	.get_irq_domain		= get_irq_domain,
 +	.setup_ioapic_entry	= setup_ioapic_entry,
 +	.set_affinity		= set_affinity,
 +	.free_irq		= free_irq,
 +	.compose_msi_msg	= compose_msi_msg,
 +	.msi_alloc_irq		= msi_alloc_irq,
 +	.msi_setup_irq		= msi_setup_irq,
 +	.alloc_hpet_msi		= alloc_hpet_msi,
  };
++<<<<<<< HEAD
++=======
+ 
+ static void irq_remapping_prepare_irte(struct amd_ir_data *data,
+ 				       struct irq_cfg *irq_cfg,
+ 				       struct irq_alloc_info *info,
+ 				       int devid, int index, int sub_handle)
+ {
+ 	struct irq_2_irte *irte_info = &data->irq_2_irte;
+ 	struct msi_msg *msg = &data->msi_entry;
+ 	struct IO_APIC_route_entry *entry;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[devid];
+ 
+ 	if (!iommu)
+ 		return;
+ 
+ 	data->irq_2_irte.devid = devid;
+ 	data->irq_2_irte.index = index + sub_handle;
+ 	iommu->irte_ops->prepare(data->entry, apic->irq_delivery_mode,
+ 				 apic->irq_dest_mode, irq_cfg->vector,
+ 				 irq_cfg->dest_apicid, devid);
+ 
+ 	switch (info->type) {
+ 	case X86_IRQ_ALLOC_TYPE_IOAPIC:
+ 		/* Setup IOAPIC entry */
+ 		entry = info->ioapic_entry;
+ 		info->ioapic_entry = NULL;
+ 		memset(entry, 0, sizeof(*entry));
+ 		entry->vector        = index;
+ 		entry->mask          = 0;
+ 		entry->trigger       = info->ioapic_trigger;
+ 		entry->polarity      = info->ioapic_polarity;
+ 		/* Mask level triggered irqs. */
+ 		if (info->ioapic_trigger)
+ 			entry->mask = 1;
+ 		break;
+ 
+ 	case X86_IRQ_ALLOC_TYPE_HPET:
+ 	case X86_IRQ_ALLOC_TYPE_MSI:
+ 	case X86_IRQ_ALLOC_TYPE_MSIX:
+ 		msg->address_hi = MSI_ADDR_BASE_HI;
+ 		msg->address_lo = MSI_ADDR_BASE_LO;
+ 		msg->data = irte_info->index;
+ 		break;
+ 
+ 	default:
+ 		BUG_ON(1);
+ 		break;
+ 	}
+ }
+ 
+ struct amd_irte_ops irte_32_ops = {
+ 	.prepare = irte_prepare,
+ 	.activate = irte_activate,
+ 	.deactivate = irte_deactivate,
+ 	.set_affinity = irte_set_affinity,
+ 	.set_allocated = irte_set_allocated,
+ 	.is_allocated = irte_is_allocated,
+ 	.clear_allocated = irte_clear_allocated,
+ };
+ 
+ struct amd_irte_ops irte_128_ops = {
+ 	.prepare = irte_ga_prepare,
+ 	.activate = irte_ga_activate,
+ 	.deactivate = irte_ga_deactivate,
+ 	.set_affinity = irte_ga_set_affinity,
+ 	.set_allocated = irte_ga_set_allocated,
+ 	.is_allocated = irte_ga_is_allocated,
+ 	.clear_allocated = irte_ga_clear_allocated,
+ };
+ 
+ static int irq_remapping_alloc(struct irq_domain *domain, unsigned int virq,
+ 			       unsigned int nr_irqs, void *arg)
+ {
+ 	struct irq_alloc_info *info = arg;
+ 	struct irq_data *irq_data;
+ 	struct amd_ir_data *data = NULL;
+ 	struct irq_cfg *cfg;
+ 	int i, ret, devid;
+ 	int index = -1;
+ 
+ 	if (!info)
+ 		return -EINVAL;
+ 	if (nr_irqs > 1 && info->type != X86_IRQ_ALLOC_TYPE_MSI &&
+ 	    info->type != X86_IRQ_ALLOC_TYPE_MSIX)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * With IRQ remapping enabled, don't need contiguous CPU vectors
+ 	 * to support multiple MSI interrupts.
+ 	 */
+ 	if (info->type == X86_IRQ_ALLOC_TYPE_MSI)
+ 		info->flags &= ~X86_IRQ_ALLOC_CONTIGUOUS_VECTORS;
+ 
+ 	devid = get_devid(info);
+ 	if (devid < 0)
+ 		return -EINVAL;
+ 
+ 	ret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, arg);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (info->type == X86_IRQ_ALLOC_TYPE_IOAPIC) {
+ 		if (get_irq_table(devid, true))
+ 			index = info->ioapic_pin;
+ 		else
+ 			ret = -ENOMEM;
+ 	} else {
+ 		index = alloc_irq_index(devid, nr_irqs);
+ 	}
+ 	if (index < 0) {
+ 		pr_warn("Failed to allocate IRTE\n");
+ 		ret = index;
+ 		goto out_free_parent;
+ 	}
+ 
+ 	for (i = 0; i < nr_irqs; i++) {
+ 		irq_data = irq_domain_get_irq_data(domain, virq + i);
+ 		cfg = irqd_cfg(irq_data);
+ 		if (!irq_data || !cfg) {
+ 			ret = -EINVAL;
+ 			goto out_free_data;
+ 		}
+ 
+ 		ret = -ENOMEM;
+ 		data = kzalloc(sizeof(*data), GFP_KERNEL);
+ 		if (!data)
+ 			goto out_free_data;
+ 
+ 		if (!AMD_IOMMU_GUEST_IR_GA(amd_iommu_guest_ir))
+ 			data->entry = kzalloc(sizeof(union irte), GFP_KERNEL);
+ 		else
+ 			data->entry = kzalloc(sizeof(struct irte_ga),
+ 						     GFP_KERNEL);
+ 		if (!data->entry) {
+ 			kfree(data);
+ 			goto out_free_data;
+ 		}
+ 
+ 		irq_data->hwirq = (devid << 16) + i;
+ 		irq_data->chip_data = data;
+ 		irq_data->chip = &amd_ir_chip;
+ 		irq_remapping_prepare_irte(data, cfg, info, devid, index, i);
+ 		irq_set_status_flags(virq + i, IRQ_MOVE_PCNTXT);
+ 	}
+ 
+ 	return 0;
+ 
+ out_free_data:
+ 	for (i--; i >= 0; i--) {
+ 		irq_data = irq_domain_get_irq_data(domain, virq + i);
+ 		if (irq_data)
+ 			kfree(irq_data->chip_data);
+ 	}
+ 	for (i = 0; i < nr_irqs; i++)
+ 		free_irte(devid, index + i);
+ out_free_parent:
+ 	irq_domain_free_irqs_common(domain, virq, nr_irqs);
+ 	return ret;
+ }
+ 
+ static void irq_remapping_free(struct irq_domain *domain, unsigned int virq,
+ 			       unsigned int nr_irqs)
+ {
+ 	struct irq_2_irte *irte_info;
+ 	struct irq_data *irq_data;
+ 	struct amd_ir_data *data;
+ 	int i;
+ 
+ 	for (i = 0; i < nr_irqs; i++) {
+ 		irq_data = irq_domain_get_irq_data(domain, virq  + i);
+ 		if (irq_data && irq_data->chip_data) {
+ 			data = irq_data->chip_data;
+ 			irte_info = &data->irq_2_irte;
+ 			free_irte(irte_info->devid, irte_info->index);
+ 			kfree(data->entry);
+ 			kfree(data);
+ 		}
+ 	}
+ 	irq_domain_free_irqs_common(domain, virq, nr_irqs);
+ }
+ 
+ static void irq_remapping_activate(struct irq_domain *domain,
+ 				   struct irq_data *irq_data)
+ {
+ 	struct amd_ir_data *data = irq_data->chip_data;
+ 	struct irq_2_irte *irte_info = &data->irq_2_irte;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[irte_info->devid];
+ 
+ 	if (iommu)
+ 		iommu->irte_ops->activate(data->entry, irte_info->devid,
+ 					  irte_info->index);
+ }
+ 
+ static void irq_remapping_deactivate(struct irq_domain *domain,
+ 				     struct irq_data *irq_data)
+ {
+ 	struct amd_ir_data *data = irq_data->chip_data;
+ 	struct irq_2_irte *irte_info = &data->irq_2_irte;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[irte_info->devid];
+ 
+ 	if (iommu)
+ 		iommu->irte_ops->deactivate(data->entry, irte_info->devid,
+ 					    irte_info->index);
+ }
+ 
+ static const struct irq_domain_ops amd_ir_domain_ops = {
+ 	.alloc = irq_remapping_alloc,
+ 	.free = irq_remapping_free,
+ 	.activate = irq_remapping_activate,
+ 	.deactivate = irq_remapping_deactivate,
+ };
+ 
+ static int amd_ir_set_vcpu_affinity(struct irq_data *data, void *vcpu_info)
+ {
+ 	struct amd_iommu *iommu;
+ 	struct amd_iommu_pi_data *pi_data = vcpu_info;
+ 	struct vcpu_data *vcpu_pi_info = pi_data->vcpu_data;
+ 	struct amd_ir_data *ir_data = data->chip_data;
+ 	struct irte_ga *irte = (struct irte_ga *) ir_data->entry;
+ 	struct irq_2_irte *irte_info = &ir_data->irq_2_irte;
+ 	struct iommu_dev_data *dev_data = search_dev_data(irte_info->devid);
+ 
+ 	/* Note:
+ 	 * This device has never been set up for guest mode.
+ 	 * we should not modify the IRTE
+ 	 */
+ 	if (!dev_data || !dev_data->use_vapic)
+ 		return 0;
+ 
+ 	pi_data->ir_data = ir_data;
+ 
+ 	/* Note:
+ 	 * SVM tries to set up for VAPIC mode, but we are in
+ 	 * legacy mode. So, we force legacy mode instead.
+ 	 */
+ 	if (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir)) {
+ 		pr_debug("AMD-Vi: %s: Fall back to using intr legacy remap\n",
+ 			 __func__);
+ 		pi_data->is_guest_mode = false;
+ 	}
+ 
+ 	iommu = amd_iommu_rlookup_table[irte_info->devid];
+ 	if (iommu == NULL)
+ 		return -EINVAL;
+ 
+ 	pi_data->prev_ga_tag = ir_data->cached_ga_tag;
+ 	if (pi_data->is_guest_mode) {
+ 		/* Setting */
+ 		irte->hi.fields.ga_root_ptr = (pi_data->base >> 12);
+ 		irte->hi.fields.vector = vcpu_pi_info->vector;
+ 		irte->lo.fields_vapic.ga_log_intr = 1;
+ 		irte->lo.fields_vapic.guest_mode = 1;
+ 		irte->lo.fields_vapic.ga_tag = pi_data->ga_tag;
+ 
+ 		ir_data->cached_ga_tag = pi_data->ga_tag;
+ 	} else {
+ 		/* Un-Setting */
+ 		struct irq_cfg *cfg = irqd_cfg(data);
+ 
+ 		irte->hi.val = 0;
+ 		irte->lo.val = 0;
+ 		irte->hi.fields.vector = cfg->vector;
+ 		irte->lo.fields_remap.guest_mode = 0;
+ 		irte->lo.fields_remap.destination = cfg->dest_apicid;
+ 		irte->lo.fields_remap.int_type = apic->irq_delivery_mode;
+ 		irte->lo.fields_remap.dm = apic->irq_dest_mode;
+ 
+ 		/*
+ 		 * This communicates the ga_tag back to the caller
+ 		 * so that it can do all the necessary clean up.
+ 		 */
+ 		ir_data->cached_ga_tag = 0;
+ 	}
+ 
+ 	return modify_irte_ga(irte_info->devid, irte_info->index, irte, ir_data);
+ }
+ 
+ static int amd_ir_set_affinity(struct irq_data *data,
+ 			       const struct cpumask *mask, bool force)
+ {
+ 	struct amd_ir_data *ir_data = data->chip_data;
+ 	struct irq_2_irte *irte_info = &ir_data->irq_2_irte;
+ 	struct irq_cfg *cfg = irqd_cfg(data);
+ 	struct irq_data *parent = data->parent_data;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[irte_info->devid];
+ 	int ret;
+ 
+ 	if (!iommu)
+ 		return -ENODEV;
+ 
+ 	ret = parent->chip->irq_set_affinity(parent, mask, force);
+ 	if (ret < 0 || ret == IRQ_SET_MASK_OK_DONE)
+ 		return ret;
+ 
+ 	/*
+ 	 * Atomically updates the IRTE with the new destination, vector
+ 	 * and flushes the interrupt entry cache.
+ 	 */
+ 	iommu->irte_ops->set_affinity(ir_data->entry, irte_info->devid,
+ 			    irte_info->index, cfg->vector, cfg->dest_apicid);
+ 
+ 	/*
+ 	 * After this point, all the interrupts will start arriving
+ 	 * at the new destination. So, time to cleanup the previous
+ 	 * vector allocation.
+ 	 */
+ 	send_cleanup_vector(cfg);
+ 
+ 	return IRQ_SET_MASK_OK_DONE;
+ }
+ 
+ static void ir_compose_msi_msg(struct irq_data *irq_data, struct msi_msg *msg)
+ {
+ 	struct amd_ir_data *ir_data = irq_data->chip_data;
+ 
+ 	*msg = ir_data->msi_entry;
+ }
+ 
+ static struct irq_chip amd_ir_chip = {
+ 	.name			= "AMD-IR",
+ 	.irq_ack		= ir_ack_apic_edge,
+ 	.irq_set_affinity	= amd_ir_set_affinity,
+ 	.irq_set_vcpu_affinity	= amd_ir_set_vcpu_affinity,
+ 	.irq_compose_msi_msg	= ir_compose_msi_msg,
+ };
+ 
+ int amd_iommu_create_irq_domain(struct amd_iommu *iommu)
+ {
+ 	struct fwnode_handle *fn;
+ 
+ 	fn = irq_domain_alloc_named_id_fwnode("AMD-IR", iommu->index);
+ 	if (!fn)
+ 		return -ENOMEM;
+ 	iommu->ir_domain = irq_domain_create_tree(fn, &amd_ir_domain_ops, iommu);
+ 	irq_domain_free_fwnode(fn);
+ 	if (!iommu->ir_domain)
+ 		return -ENOMEM;
+ 
+ 	iommu->ir_domain->parent = arch_get_ir_parent_domain();
+ 	iommu->msi_domain = arch_create_remap_msi_irq_domain(iommu->ir_domain,
+ 							     "AMD-IR-MSI",
+ 							     iommu->index);
+ 	return 0;
+ }
+ 
+ int amd_iommu_update_ga(int cpu, bool is_run, void *data)
+ {
+ 	unsigned long flags;
+ 	struct amd_iommu *iommu;
+ 	struct irq_remap_table *irt;
+ 	struct amd_ir_data *ir_data = (struct amd_ir_data *)data;
+ 	int devid = ir_data->irq_2_irte.devid;
+ 	struct irte_ga *entry = (struct irte_ga *) ir_data->entry;
+ 	struct irte_ga *ref = (struct irte_ga *) ir_data->ref;
+ 
+ 	if (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir) ||
+ 	    !ref || !entry || !entry->lo.fields_vapic.guest_mode)
+ 		return 0;
+ 
+ 	iommu = amd_iommu_rlookup_table[devid];
+ 	if (!iommu)
+ 		return -ENODEV;
+ 
+ 	irt = get_irq_table(devid, false);
+ 	if (!irt)
+ 		return -ENODEV;
+ 
+ 	spin_lock_irqsave(&irt->lock, flags);
+ 
+ 	if (ref->lo.fields_vapic.guest_mode) {
+ 		if (cpu >= 0)
+ 			ref->lo.fields_vapic.destination = cpu;
+ 		ref->lo.fields_vapic.is_run = is_run;
+ 		barrier();
+ 	}
+ 
+ 	spin_unlock_irqrestore(&irt->lock, flags);
+ 
+ 	iommu_flush_irt(iommu, devid);
+ 	iommu_completion_wait(iommu);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(amd_iommu_update_ga);
++>>>>>>> efe6f241602c (iommu/amd: Enable ga_log_intr when enabling guest_mode)
  #endif
* Unmerged path drivers/iommu/amd_iommu.c
