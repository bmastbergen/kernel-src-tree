iommu: Remove pci.h include from trace/events/iommu.h

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Joerg Roedel <jroedel@suse.de>
commit 461a6946b1f93f6720577fb06aa78e8cbd9291c9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/461a6946.failed

The include file does not need any PCI specifics, so remove
that include. Also fix the places that relied on it.

	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 461a6946b1f93f6720577fb06aa78e8cbd9291c9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/fsl_pamu.h
#	drivers/iommu/rockchip-iommu.c
#	drivers/iommu/tegra-smmu.c
#	include/linux/dma-iommu.h
diff --cc drivers/iommu/tegra-smmu.c
index f6f120e25409,eeb19f560a05..000000000000
--- a/drivers/iommu/tegra-smmu.c
+++ b/drivers/iommu/tegra-smmu.c
@@@ -1,675 -1,422 +1,679 @@@
  /*
 - * Copyright (C) 2011-2014 NVIDIA CORPORATION.  All rights reserved.
 + * IOMMU API for SMMU in Tegra30
   *
 - * This program is free software; you can redistribute it and/or modify
 - * it under the terms of the GNU General Public License version 2 as
 - * published by the Free Software Foundation.
 + * Copyright (c) 2011-2013, NVIDIA CORPORATION.  All rights reserved.
 + *
 + * This program is free software; you can redistribute it and/or modify it
 + * under the terms and conditions of the GNU General Public License,
 + * version 2, as published by the Free Software Foundation.
 + *
 + * This program is distributed in the hope it will be useful, but WITHOUT
 + * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 + * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
 + * more details.
 + *
 + * You should have received a copy of the GNU General Public License along with
 + * this program; if not, write to the Free Software Foundation, Inc.,
 + * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
   */
  
 -#include <linux/bitops.h>
 -#include <linux/debugfs.h>
 +#define pr_fmt(fmt)	"%s(): " fmt, __func__
 +
  #include <linux/err.h>
 -#include <linux/iommu.h>
 -#include <linux/kernel.h>
 -#include <linux/of.h>
 -#include <linux/of_device.h>
 +#include <linux/module.h>
  #include <linux/platform_device.h>
 +#include <linux/spinlock.h>
  #include <linux/slab.h>
++<<<<<<< HEAD
 +#include <linux/vmalloc.h>
 +#include <linux/mm.h>
 +#include <linux/pagemap.h>
 +#include <linux/device.h>
 +#include <linux/sched.h>
 +#include <linux/iommu.h>
 +#include <linux/io.h>
 +#include <linux/of.h>
 +#include <linux/of_iommu.h>
 +#include <linux/debugfs.h>
 +#include <linux/seq_file.h>
 +#include <linux/tegra-ahb.h>
++=======
+ #include <linux/dma-mapping.h>
 -
 -#include <soc/tegra/ahb.h>
 -#include <soc/tegra/mc.h>
 -
 -struct tegra_smmu {
 -	void __iomem *regs;
 -	struct device *dev;
 -
 -	struct tegra_mc *mc;
 -	const struct tegra_smmu_soc *soc;
 -
 -	unsigned long pfn_mask;
 -	unsigned long tlb_mask;
 -
 -	unsigned long *asids;
 -	struct mutex lock;
 -
 -	struct list_head list;
 -
 -	struct dentry *debugfs;
++>>>>>>> 461a6946b1f9 (iommu: Remove pci.h include from trace/events/iommu.h)
 +
 +#include <asm/page.h>
 +#include <asm/cacheflush.h>
 +
 +enum smmu_hwgrp {
 +	HWGRP_AFI,
 +	HWGRP_AVPC,
 +	HWGRP_DC,
 +	HWGRP_DCB,
 +	HWGRP_EPP,
 +	HWGRP_G2,
 +	HWGRP_HC,
 +	HWGRP_HDA,
 +	HWGRP_ISP,
 +	HWGRP_MPE,
 +	HWGRP_NV,
 +	HWGRP_NV2,
 +	HWGRP_PPCS,
 +	HWGRP_SATA,
 +	HWGRP_VDE,
 +	HWGRP_VI,
 +
 +	HWGRP_COUNT,
 +
 +	HWGRP_END = ~0,
  };
  
 -struct tegra_smmu_as {
 -	struct iommu_domain domain;
 -	struct tegra_smmu *smmu;
 -	unsigned int use_count;
 -	u32 *count;
 -	struct page **pts;
 -	struct page *pd;
 -	dma_addr_t pd_dma;
 -	unsigned id;
 -	u32 attr;
 +#define HWG_AFI		(1 << HWGRP_AFI)
 +#define HWG_AVPC	(1 << HWGRP_AVPC)
 +#define HWG_DC		(1 << HWGRP_DC)
 +#define HWG_DCB		(1 << HWGRP_DCB)
 +#define HWG_EPP		(1 << HWGRP_EPP)
 +#define HWG_G2		(1 << HWGRP_G2)
 +#define HWG_HC		(1 << HWGRP_HC)
 +#define HWG_HDA		(1 << HWGRP_HDA)
 +#define HWG_ISP		(1 << HWGRP_ISP)
 +#define HWG_MPE		(1 << HWGRP_MPE)
 +#define HWG_NV		(1 << HWGRP_NV)
 +#define HWG_NV2		(1 << HWGRP_NV2)
 +#define HWG_PPCS	(1 << HWGRP_PPCS)
 +#define HWG_SATA	(1 << HWGRP_SATA)
 +#define HWG_VDE		(1 << HWGRP_VDE)
 +#define HWG_VI		(1 << HWGRP_VI)
 +
 +/* bitmap of the page sizes currently supported */
 +#define SMMU_IOMMU_PGSIZES	(SZ_4K)
 +
 +#define SMMU_CONFIG				0x10
 +#define SMMU_CONFIG_DISABLE			0
 +#define SMMU_CONFIG_ENABLE			1
 +
 +/* REVISIT: To support multiple MCs */
 +enum {
 +	_MC = 0,
  };
  
 -static struct tegra_smmu_as *to_smmu_as(struct iommu_domain *dom)
 -{
 -	return container_of(dom, struct tegra_smmu_as, domain);
 -}
 -
 -static inline void smmu_writel(struct tegra_smmu *smmu, u32 value,
 -			       unsigned long offset)
 -{
 -	writel(value, smmu->regs + offset);
 -}
 -
 -static inline u32 smmu_readl(struct tegra_smmu *smmu, unsigned long offset)
 -{
 -	return readl(smmu->regs + offset);
 -}
 -
 -#define SMMU_CONFIG 0x010
 -#define  SMMU_CONFIG_ENABLE (1 << 0)
 -
 -#define SMMU_TLB_CONFIG 0x14
 -#define  SMMU_TLB_CONFIG_HIT_UNDER_MISS (1 << 29)
 -#define  SMMU_TLB_CONFIG_ROUND_ROBIN_ARBITRATION (1 << 28)
 -#define  SMMU_TLB_CONFIG_ACTIVE_LINES(smmu) \
 -	((smmu)->soc->num_tlb_lines & (smmu)->tlb_mask)
 -
 -#define SMMU_PTC_CONFIG 0x18
 -#define  SMMU_PTC_CONFIG_ENABLE (1 << 29)
 -#define  SMMU_PTC_CONFIG_REQ_LIMIT(x) (((x) & 0x0f) << 24)
 -#define  SMMU_PTC_CONFIG_INDEX_MAP(x) ((x) & 0x3f)
 -
 -#define SMMU_PTB_ASID 0x01c
 -#define  SMMU_PTB_ASID_VALUE(x) ((x) & 0x7f)
 -
 -#define SMMU_PTB_DATA 0x020
 -#define  SMMU_PTB_DATA_VALUE(dma, attr) ((dma) >> 12 | (attr))
 +enum {
 +	_TLB = 0,
 +	_PTC,
 +};
  
 -#define SMMU_MK_PDE(dma, attr) ((dma) >> SMMU_PTE_SHIFT | (attr))
 +#define SMMU_CACHE_CONFIG_BASE			0x14
 +#define __SMMU_CACHE_CONFIG(mc, cache)		(SMMU_CACHE_CONFIG_BASE + 4 * cache)
 +#define SMMU_CACHE_CONFIG(cache)		__SMMU_CACHE_CONFIG(_MC, cache)
 +
 +#define SMMU_CACHE_CONFIG_STATS_SHIFT		31
 +#define SMMU_CACHE_CONFIG_STATS_ENABLE		(1 << SMMU_CACHE_CONFIG_STATS_SHIFT)
 +#define SMMU_CACHE_CONFIG_STATS_TEST_SHIFT	30
 +#define SMMU_CACHE_CONFIG_STATS_TEST		(1 << SMMU_CACHE_CONFIG_STATS_TEST_SHIFT)
 +
 +#define SMMU_TLB_CONFIG_HIT_UNDER_MISS__ENABLE	(1 << 29)
 +#define SMMU_TLB_CONFIG_ACTIVE_LINES__VALUE	0x10
 +#define SMMU_TLB_CONFIG_RESET_VAL		0x20000010
 +
 +#define SMMU_PTC_CONFIG_CACHE__ENABLE		(1 << 29)
 +#define SMMU_PTC_CONFIG_INDEX_MAP__PATTERN	0x3f
 +#define SMMU_PTC_CONFIG_RESET_VAL		0x2000003f
 +
 +#define SMMU_PTB_ASID				0x1c
 +#define SMMU_PTB_ASID_CURRENT_SHIFT		0
 +
 +#define SMMU_PTB_DATA				0x20
 +#define SMMU_PTB_DATA_RESET_VAL			0
 +#define SMMU_PTB_DATA_ASID_NONSECURE_SHIFT	29
 +#define SMMU_PTB_DATA_ASID_WRITABLE_SHIFT	30
 +#define SMMU_PTB_DATA_ASID_READABLE_SHIFT	31
 +
 +#define SMMU_TLB_FLUSH				0x30
 +#define SMMU_TLB_FLUSH_VA_MATCH_ALL		0
 +#define SMMU_TLB_FLUSH_VA_MATCH_SECTION		2
 +#define SMMU_TLB_FLUSH_VA_MATCH_GROUP		3
 +#define SMMU_TLB_FLUSH_ASID_SHIFT		29
 +#define SMMU_TLB_FLUSH_ASID_MATCH_DISABLE	0
 +#define SMMU_TLB_FLUSH_ASID_MATCH_ENABLE	1
 +#define SMMU_TLB_FLUSH_ASID_MATCH_SHIFT		31
 +
 +#define SMMU_PTC_FLUSH				0x34
 +#define SMMU_PTC_FLUSH_TYPE_ALL			0
 +#define SMMU_PTC_FLUSH_TYPE_ADR			1
 +#define SMMU_PTC_FLUSH_ADR_SHIFT		4
 +
 +#define SMMU_ASID_SECURITY			0x38
 +
 +#define SMMU_STATS_CACHE_COUNT_BASE		0x1f0
 +
 +#define SMMU_STATS_CACHE_COUNT(mc, cache, hitmiss)		\
 +	(SMMU_STATS_CACHE_COUNT_BASE + 8 * cache + 4 * hitmiss)
 +
 +#define SMMU_TRANSLATION_ENABLE_0		0x228
 +#define SMMU_TRANSLATION_ENABLE_1		0x22c
 +#define SMMU_TRANSLATION_ENABLE_2		0x230
 +
 +#define SMMU_AFI_ASID	0x238   /* PCIE */
 +#define SMMU_AVPC_ASID	0x23c   /* AVP */
 +#define SMMU_DC_ASID	0x240   /* Display controller */
 +#define SMMU_DCB_ASID	0x244   /* Display controller B */
 +#define SMMU_EPP_ASID	0x248   /* Encoder pre-processor */
 +#define SMMU_G2_ASID	0x24c   /* 2D engine */
 +#define SMMU_HC_ASID	0x250   /* Host1x */
 +#define SMMU_HDA_ASID	0x254   /* High-def audio */
 +#define SMMU_ISP_ASID	0x258   /* Image signal processor */
 +#define SMMU_MPE_ASID	0x264   /* MPEG encoder */
 +#define SMMU_NV_ASID	0x268   /* (3D) */
 +#define SMMU_NV2_ASID	0x26c   /* (3D) */
 +#define SMMU_PPCS_ASID	0x270   /* AHB */
 +#define SMMU_SATA_ASID	0x278   /* SATA */
 +#define SMMU_VDE_ASID	0x27c   /* Video decoder */
 +#define SMMU_VI_ASID	0x280   /* Video input */
 +
 +#define SMMU_PDE_NEXT_SHIFT		28
 +
 +#define SMMU_TLB_FLUSH_VA_SECTION__MASK		0xffc00000
 +#define SMMU_TLB_FLUSH_VA_SECTION__SHIFT	12 /* right shift */
 +#define SMMU_TLB_FLUSH_VA_GROUP__MASK		0xffffc000
 +#define SMMU_TLB_FLUSH_VA_GROUP__SHIFT		12 /* right shift */
 +#define SMMU_TLB_FLUSH_VA(iova, which)	\
 +	((((iova) & SMMU_TLB_FLUSH_VA_##which##__MASK) >> \
 +		SMMU_TLB_FLUSH_VA_##which##__SHIFT) |	\
 +	SMMU_TLB_FLUSH_VA_MATCH_##which)
 +#define SMMU_PTB_ASID_CUR(n)	\
 +		((n) << SMMU_PTB_ASID_CURRENT_SHIFT)
 +#define SMMU_TLB_FLUSH_ASID_MATCH_disable		\
 +		(SMMU_TLB_FLUSH_ASID_MATCH_DISABLE <<	\
 +			SMMU_TLB_FLUSH_ASID_MATCH_SHIFT)
 +#define SMMU_TLB_FLUSH_ASID_MATCH__ENABLE		\
 +		(SMMU_TLB_FLUSH_ASID_MATCH_ENABLE <<	\
 +			SMMU_TLB_FLUSH_ASID_MATCH_SHIFT)
 +
 +#define SMMU_PAGE_SHIFT 12
 +#define SMMU_PAGE_SIZE	(1 << SMMU_PAGE_SHIFT)
 +#define SMMU_PAGE_MASK	((1 << SMMU_PAGE_SHIFT) - 1)
 +
 +#define SMMU_PDIR_COUNT	1024
 +#define SMMU_PDIR_SIZE	(sizeof(unsigned long) * SMMU_PDIR_COUNT)
 +#define SMMU_PTBL_COUNT	1024
 +#define SMMU_PTBL_SIZE	(sizeof(unsigned long) * SMMU_PTBL_COUNT)
 +#define SMMU_PDIR_SHIFT	12
 +#define SMMU_PDE_SHIFT	12
 +#define SMMU_PTE_SHIFT	12
 +#define SMMU_PFN_MASK	0x000fffff
 +
 +#define SMMU_ADDR_TO_PFN(addr)	((addr) >> 12)
 +#define SMMU_ADDR_TO_PDN(addr)	((addr) >> 22)
 +#define SMMU_PDN_TO_ADDR(pdn)	((pdn) << 22)
 +
 +#define _READABLE	(1 << SMMU_PTB_DATA_ASID_READABLE_SHIFT)
 +#define _WRITABLE	(1 << SMMU_PTB_DATA_ASID_WRITABLE_SHIFT)
 +#define _NONSECURE	(1 << SMMU_PTB_DATA_ASID_NONSECURE_SHIFT)
 +#define _PDE_NEXT	(1 << SMMU_PDE_NEXT_SHIFT)
 +#define _MASK_ATTR	(_READABLE | _WRITABLE | _NONSECURE)
 +
 +#define _PDIR_ATTR	(_READABLE | _WRITABLE | _NONSECURE)
 +
 +#define _PDE_ATTR	(_READABLE | _WRITABLE | _NONSECURE)
 +#define _PDE_ATTR_N	(_PDE_ATTR | _PDE_NEXT)
 +#define _PDE_VACANT(pdn)	(((pdn) << 10) | _PDE_ATTR)
 +
 +#define _PTE_ATTR	(_READABLE | _WRITABLE | _NONSECURE)
 +#define _PTE_VACANT(addr)	(((addr) >> SMMU_PAGE_SHIFT) | _PTE_ATTR)
 +
 +#define SMMU_MK_PDIR(page, attr)	\
 +		((page_to_phys(page) >> SMMU_PDIR_SHIFT) | (attr))
 +#define SMMU_MK_PDE(page, attr)		\
 +		(unsigned long)((page_to_phys(page) >> SMMU_PDE_SHIFT) | (attr))
 +#define SMMU_EX_PTBL_PAGE(pde)		\
 +		pfn_to_page((unsigned long)(pde) & SMMU_PFN_MASK)
 +#define SMMU_PFN_TO_PTE(pfn, attr)	(unsigned long)((pfn) | (attr))
 +
 +#define SMMU_ASID_ENABLE(asid)	((asid) | (1 << 31))
 +#define SMMU_ASID_DISABLE	0
 +#define SMMU_ASID_ASID(n)	((n) & ~SMMU_ASID_ENABLE(0))
 +
 +#define NUM_SMMU_REG_BANKS	3
 +
 +#define smmu_client_enable_hwgrp(c, m)	smmu_client_set_hwgrp(c, m, 1)
 +#define smmu_client_disable_hwgrp(c)	smmu_client_set_hwgrp(c, 0, 0)
 +#define __smmu_client_enable_hwgrp(c, m) __smmu_client_set_hwgrp(c, m, 1)
 +#define __smmu_client_disable_hwgrp(c)	__smmu_client_set_hwgrp(c, 0, 0)
 +
 +#define HWGRP_INIT(client) [HWGRP_##client] = SMMU_##client##_ASID
 +
 +static const u32 smmu_hwgrp_asid_reg[] = {
 +	HWGRP_INIT(AFI),
 +	HWGRP_INIT(AVPC),
 +	HWGRP_INIT(DC),
 +	HWGRP_INIT(DCB),
 +	HWGRP_INIT(EPP),
 +	HWGRP_INIT(G2),
 +	HWGRP_INIT(HC),
 +	HWGRP_INIT(HDA),
 +	HWGRP_INIT(ISP),
 +	HWGRP_INIT(MPE),
 +	HWGRP_INIT(NV),
 +	HWGRP_INIT(NV2),
 +	HWGRP_INIT(PPCS),
 +	HWGRP_INIT(SATA),
 +	HWGRP_INIT(VDE),
 +	HWGRP_INIT(VI),
 +};
 +#define HWGRP_ASID_REG(x) (smmu_hwgrp_asid_reg[x])
  
 -#define SMMU_TLB_FLUSH 0x030
 -#define  SMMU_TLB_FLUSH_VA_MATCH_ALL     (0 << 0)
 -#define  SMMU_TLB_FLUSH_VA_MATCH_SECTION (2 << 0)
 -#define  SMMU_TLB_FLUSH_VA_MATCH_GROUP   (3 << 0)
 -#define  SMMU_TLB_FLUSH_ASID(x)          (((x) & 0x7f) << 24)
 -#define  SMMU_TLB_FLUSH_VA_SECTION(addr) ((((addr) & 0xffc00000) >> 12) | \
 -					  SMMU_TLB_FLUSH_VA_MATCH_SECTION)
 -#define  SMMU_TLB_FLUSH_VA_GROUP(addr)   ((((addr) & 0xffffc000) >> 12) | \
 -					  SMMU_TLB_FLUSH_VA_MATCH_GROUP)
 -#define  SMMU_TLB_FLUSH_ASID_MATCH       (1 << 31)
 +/*
 + * Per client for address space
 + */
 +struct smmu_client {
 +	struct device		*dev;
 +	struct list_head	list;
 +	struct smmu_as		*as;
 +	u32			hwgrp;
 +};
  
 -#define SMMU_PTC_FLUSH 0x034
 -#define  SMMU_PTC_FLUSH_TYPE_ALL (0 << 0)
 -#define  SMMU_PTC_FLUSH_TYPE_ADR (1 << 0)
 +/*
 + * Per address space
 + */
 +struct smmu_as {
 +	struct smmu_device	*smmu;	/* back pointer to container */
 +	unsigned int		asid;
 +	spinlock_t		lock;	/* for pagetable */
 +	struct page		*pdir_page;
 +	unsigned long		pdir_attr;
 +	unsigned long		pde_attr;
 +	unsigned long		pte_attr;
 +	unsigned int		*pte_count;
 +
 +	struct list_head	client;
 +	spinlock_t		client_lock; /* for client list */
 +};
  
 -#define SMMU_PTC_FLUSH_HI 0x9b8
 -#define  SMMU_PTC_FLUSH_HI_MASK 0x3
 +struct smmu_debugfs_info {
 +	struct smmu_device *smmu;
 +	int mc;
 +	int cache;
 +};
  
 -/* per-SWGROUP SMMU_*_ASID register */
 -#define SMMU_ASID_ENABLE (1 << 31)
 -#define SMMU_ASID_MASK 0x7f
 -#define SMMU_ASID_VALUE(x) ((x) & SMMU_ASID_MASK)
 +/*
 + * Per SMMU device - IOMMU device
 + */
 +struct smmu_device {
 +	void __iomem	*regbase;	/* register offset base */
 +	void __iomem	**regs;		/* register block start address array */
 +	void __iomem	**rege;		/* register block end address array */
 +	int		nregs;		/* number of register blocks */
 +
 +	unsigned long	iovmm_base;	/* remappable base address */
 +	unsigned long	page_count;	/* total remappable size */
 +	spinlock_t	lock;
 +	char		*name;
 +	struct device	*dev;
 +	struct page *avp_vector_page;	/* dummy page shared by all AS's */
  
 -/* page table definitions */
 -#define SMMU_NUM_PDE 1024
 -#define SMMU_NUM_PTE 1024
 +	/*
 +	 * Register image savers for suspend/resume
 +	 */
 +	unsigned long translation_enable_0;
 +	unsigned long translation_enable_1;
 +	unsigned long translation_enable_2;
 +	unsigned long asid_security;
  
 -#define SMMU_SIZE_PD (SMMU_NUM_PDE * 4)
 -#define SMMU_SIZE_PT (SMMU_NUM_PTE * 4)
 +	struct dentry *debugfs_root;
 +	struct smmu_debugfs_info *debugfs_info;
  
 -#define SMMU_PDE_SHIFT 22
 -#define SMMU_PTE_SHIFT 12
 +	struct device_node *ahb;
  
 -#define SMMU_PD_READABLE	(1 << 31)
 -#define SMMU_PD_WRITABLE	(1 << 30)
 -#define SMMU_PD_NONSECURE	(1 << 29)
 +	int		num_as;
 +	struct smmu_as	as[0];		/* Run-time allocated array */
 +};
  
 -#define SMMU_PDE_READABLE	(1 << 31)
 -#define SMMU_PDE_WRITABLE	(1 << 30)
 -#define SMMU_PDE_NONSECURE	(1 << 29)
 -#define SMMU_PDE_NEXT		(1 << 28)
 +static struct smmu_device *smmu_handle; /* unique for a system */
  
 -#define SMMU_PTE_READABLE	(1 << 31)
 -#define SMMU_PTE_WRITABLE	(1 << 30)
 -#define SMMU_PTE_NONSECURE	(1 << 29)
 +/*
 + *	SMMU register accessors
 + */
 +static bool inline smmu_valid_reg(struct smmu_device *smmu,
 +				  void __iomem *addr)
 +{
 +	int i;
  
 -#define SMMU_PDE_ATTR		(SMMU_PDE_READABLE | SMMU_PDE_WRITABLE | \
 -				 SMMU_PDE_NONSECURE)
 -#define SMMU_PTE_ATTR		(SMMU_PTE_READABLE | SMMU_PTE_WRITABLE | \
 -				 SMMU_PTE_NONSECURE)
 +	for (i = 0; i < smmu->nregs; i++) {
 +		if (addr < smmu->regs[i])
 +			break;
 +		if (addr <= smmu->rege[i])
 +			return true;
 +	}
  
 -static unsigned int iova_pd_index(unsigned long iova)
 -{
 -	return (iova >> SMMU_PDE_SHIFT) & (SMMU_NUM_PDE - 1);
 +	return false;
  }
  
 -static unsigned int iova_pt_index(unsigned long iova)
 +static inline u32 smmu_read(struct smmu_device *smmu, size_t offs)
  {
 -	return (iova >> SMMU_PTE_SHIFT) & (SMMU_NUM_PTE - 1);
 -}
 +	void __iomem *addr = smmu->regbase + offs;
  
 -static bool smmu_dma_addr_valid(struct tegra_smmu *smmu, dma_addr_t addr)
 -{
 -	addr >>= 12;
 -	return (addr & smmu->pfn_mask) == addr;
 -}
 +	BUG_ON(!smmu_valid_reg(smmu, addr));
  
 -static dma_addr_t smmu_pde_to_dma(u32 pde)
 -{
 -	return pde << 12;
 +	return readl(addr);
  }
  
 -static void smmu_flush_ptc_all(struct tegra_smmu *smmu)
 +static inline void smmu_write(struct smmu_device *smmu, u32 val, size_t offs)
  {
 -	smmu_writel(smmu, SMMU_PTC_FLUSH_TYPE_ALL, SMMU_PTC_FLUSH);
 -}
 +	void __iomem *addr = smmu->regbase + offs;
  
 -static inline void smmu_flush_ptc(struct tegra_smmu *smmu, dma_addr_t dma,
 -				  unsigned long offset)
 -{
 -	u32 value;
 +	BUG_ON(!smmu_valid_reg(smmu, addr));
  
 -	offset &= ~(smmu->mc->soc->atom_size - 1);
 +	writel(val, addr);
 +}
  
 -	if (smmu->mc->soc->num_address_bits > 32) {
 -#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
 -		value = (dma >> 32) & SMMU_PTC_FLUSH_HI_MASK;
 -#else
 -		value = 0;
 -#endif
 -		smmu_writel(smmu, value, SMMU_PTC_FLUSH_HI);
 -	}
 +#define VA_PAGE_TO_PA(va, page)	\
 +	(page_to_phys(page) + ((unsigned long)(va) & ~PAGE_MASK))
  
 -	value = (dma + offset) | SMMU_PTC_FLUSH_TYPE_ADR;
 -	smmu_writel(smmu, value, SMMU_PTC_FLUSH);
 -}
 +#define FLUSH_CPU_DCACHE(va, page, size)	\
 +	do {	\
 +		unsigned long _pa_ = VA_PAGE_TO_PA(va, page);		\
 +		__cpuc_flush_dcache_area((void *)(va), (size_t)(size));	\
 +		outer_flush_range(_pa_, _pa_+(size_t)(size));		\
 +	} while (0)
  
 -static inline void smmu_flush_tlb(struct tegra_smmu *smmu)
 -{
 -	smmu_writel(smmu, SMMU_TLB_FLUSH_VA_MATCH_ALL, SMMU_TLB_FLUSH);
 -}
 +/*
 + * Any interaction between any block on PPSB and a block on APB or AHB
 + * must have these read-back barriers to ensure the APB/AHB bus
 + * transaction is complete before initiating activity on the PPSB
 + * block.
 + */
 +#define FLUSH_SMMU_REGS(smmu)	smmu_read(smmu, SMMU_CONFIG)
 +
 +#define smmu_client_hwgrp(c) (u32)((c)->dev->platform_data)
  
 -static inline void smmu_flush_tlb_asid(struct tegra_smmu *smmu,
 -				       unsigned long asid)
 +static int __smmu_client_set_hwgrp(struct smmu_client *c,
 +				   unsigned long map, int on)
  {
 -	u32 value;
 +	int i;
 +	struct smmu_as *as = c->as;
 +	u32 val, offs, mask = SMMU_ASID_ENABLE(as->asid);
 +	struct smmu_device *smmu = as->smmu;
 +
 +	WARN_ON(!on && map);
 +	if (on && !map)
 +		return -EINVAL;
 +	if (!on)
 +		map = smmu_client_hwgrp(c);
 +
 +	for_each_set_bit(i, &map, HWGRP_COUNT) {
 +		offs = HWGRP_ASID_REG(i);
 +		val = smmu_read(smmu, offs);
 +		if (on) {
 +			if (WARN_ON(val & mask))
 +				goto err_hw_busy;
 +			val |= mask;
 +		} else {
 +			WARN_ON((val & mask) == mask);
 +			val &= ~mask;
 +		}
 +		smmu_write(smmu, val, offs);
 +	}
 +	FLUSH_SMMU_REGS(smmu);
 +	c->hwgrp = map;
 +	return 0;
  
 -	value = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |
 -		SMMU_TLB_FLUSH_VA_MATCH_ALL;
 -	smmu_writel(smmu, value, SMMU_TLB_FLUSH);
 +err_hw_busy:
 +	for_each_set_bit(i, &map, HWGRP_COUNT) {
 +		offs = HWGRP_ASID_REG(i);
 +		val = smmu_read(smmu, offs);
 +		val &= ~mask;
 +		smmu_write(smmu, val, offs);
 +	}
 +	return -EBUSY;
  }
  
 -static inline void smmu_flush_tlb_section(struct tegra_smmu *smmu,
 -					  unsigned long asid,
 -					  unsigned long iova)
 +static int smmu_client_set_hwgrp(struct smmu_client *c, u32 map, int on)
  {
 -	u32 value;
 -
 -	value = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |
 -		SMMU_TLB_FLUSH_VA_SECTION(iova);
 -	smmu_writel(smmu, value, SMMU_TLB_FLUSH);
 +	u32 val;
 +	unsigned long flags;
 +	struct smmu_as *as = c->as;
 +	struct smmu_device *smmu = as->smmu;
 +
 +	spin_lock_irqsave(&smmu->lock, flags);
 +	val = __smmu_client_set_hwgrp(c, map, on);
 +	spin_unlock_irqrestore(&smmu->lock, flags);
 +	return val;
  }
  
 -static inline void smmu_flush_tlb_group(struct tegra_smmu *smmu,
 -					unsigned long asid,
 -					unsigned long iova)
 +/*
 + * Flush all TLB entries and all PTC entries
 + * Caller must lock smmu
 + */
 +static void smmu_flush_regs(struct smmu_device *smmu, int enable)
  {
 -	u32 value;
 +	u32 val;
  
 -	value = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |
 -		SMMU_TLB_FLUSH_VA_GROUP(iova);
 -	smmu_writel(smmu, value, SMMU_TLB_FLUSH);
 -}
 +	smmu_write(smmu, SMMU_PTC_FLUSH_TYPE_ALL, SMMU_PTC_FLUSH);
 +	FLUSH_SMMU_REGS(smmu);
 +	val = SMMU_TLB_FLUSH_VA_MATCH_ALL |
 +		SMMU_TLB_FLUSH_ASID_MATCH_disable;
 +	smmu_write(smmu, val, SMMU_TLB_FLUSH);
  
 -static inline void smmu_flush(struct tegra_smmu *smmu)
 -{
 -	smmu_readl(smmu, SMMU_CONFIG);
 +	if (enable)
 +		smmu_write(smmu, SMMU_CONFIG_ENABLE, SMMU_CONFIG);
 +	FLUSH_SMMU_REGS(smmu);
  }
  
 -static int tegra_smmu_alloc_asid(struct tegra_smmu *smmu, unsigned int *idp)
 +static int smmu_setup_regs(struct smmu_device *smmu)
  {
 -	unsigned long id;
 +	int i;
 +	u32 val;
  
 -	mutex_lock(&smmu->lock);
 +	for (i = 0; i < smmu->num_as; i++) {
 +		struct smmu_as *as = &smmu->as[i];
 +		struct smmu_client *c;
  
 -	id = find_first_zero_bit(smmu->asids, smmu->soc->num_asids);
 -	if (id >= smmu->soc->num_asids) {
 -		mutex_unlock(&smmu->lock);
 -		return -ENOSPC;
 +		smmu_write(smmu, SMMU_PTB_ASID_CUR(as->asid), SMMU_PTB_ASID);
 +		val = as->pdir_page ?
 +			SMMU_MK_PDIR(as->pdir_page, as->pdir_attr) :
 +			SMMU_PTB_DATA_RESET_VAL;
 +		smmu_write(smmu, val, SMMU_PTB_DATA);
 +
 +		list_for_each_entry(c, &as->client, list)
 +			__smmu_client_set_hwgrp(c, c->hwgrp, 1);
  	}
  
 -	set_bit(id, smmu->asids);
 -	*idp = id;
 +	smmu_write(smmu, smmu->translation_enable_0, SMMU_TRANSLATION_ENABLE_0);
 +	smmu_write(smmu, smmu->translation_enable_1, SMMU_TRANSLATION_ENABLE_1);
 +	smmu_write(smmu, smmu->translation_enable_2, SMMU_TRANSLATION_ENABLE_2);
 +	smmu_write(smmu, smmu->asid_security, SMMU_ASID_SECURITY);
 +	smmu_write(smmu, SMMU_TLB_CONFIG_RESET_VAL, SMMU_CACHE_CONFIG(_TLB));
 +	smmu_write(smmu, SMMU_PTC_CONFIG_RESET_VAL, SMMU_CACHE_CONFIG(_PTC));
  
 -	mutex_unlock(&smmu->lock);
 -	return 0;
 -}
 +	smmu_flush_regs(smmu, 1);
  
 -static void tegra_smmu_free_asid(struct tegra_smmu *smmu, unsigned int id)
 -{
 -	mutex_lock(&smmu->lock);
 -	clear_bit(id, smmu->asids);
 -	mutex_unlock(&smmu->lock);
 +	return tegra_ahb_enable_smmu(smmu->ahb);
  }
  
 -static bool tegra_smmu_capable(enum iommu_cap cap)
 +static void flush_ptc_and_tlb(struct smmu_device *smmu,
 +		      struct smmu_as *as, dma_addr_t iova,
 +		      unsigned long *pte, struct page *page, int is_pde)
  {
 -	return false;
 +	u32 val;
 +	unsigned long tlb_flush_va = is_pde
 +		?  SMMU_TLB_FLUSH_VA(iova, SECTION)
 +		:  SMMU_TLB_FLUSH_VA(iova, GROUP);
 +
 +	val = SMMU_PTC_FLUSH_TYPE_ADR | VA_PAGE_TO_PA(pte, page);
 +	smmu_write(smmu, val, SMMU_PTC_FLUSH);
 +	FLUSH_SMMU_REGS(smmu);
 +	val = tlb_flush_va |
 +		SMMU_TLB_FLUSH_ASID_MATCH__ENABLE |
 +		(as->asid << SMMU_TLB_FLUSH_ASID_SHIFT);
 +	smmu_write(smmu, val, SMMU_TLB_FLUSH);
 +	FLUSH_SMMU_REGS(smmu);
  }
  
 -static struct iommu_domain *tegra_smmu_domain_alloc(unsigned type)
 +static void free_ptbl(struct smmu_as *as, dma_addr_t iova)
  {
 -	struct tegra_smmu_as *as;
 -
 -	if (type != IOMMU_DOMAIN_UNMANAGED)
 -		return NULL;
 -
 -	as = kzalloc(sizeof(*as), GFP_KERNEL);
 -	if (!as)
 -		return NULL;
 -
 -	as->attr = SMMU_PD_READABLE | SMMU_PD_WRITABLE | SMMU_PD_NONSECURE;
 -
 -	as->pd = alloc_page(GFP_KERNEL | __GFP_DMA | __GFP_ZERO);
 -	if (!as->pd) {
 -		kfree(as);
 -		return NULL;
 +	unsigned long pdn = SMMU_ADDR_TO_PDN(iova);
 +	unsigned long *pdir = (unsigned long *)page_address(as->pdir_page);
 +
 +	if (pdir[pdn] != _PDE_VACANT(pdn)) {
 +		dev_dbg(as->smmu->dev, "pdn: %lx\n", pdn);
 +
 +		ClearPageReserved(SMMU_EX_PTBL_PAGE(pdir[pdn]));
 +		__free_page(SMMU_EX_PTBL_PAGE(pdir[pdn]));
 +		pdir[pdn] = _PDE_VACANT(pdn);
 +		FLUSH_CPU_DCACHE(&pdir[pdn], as->pdir_page, sizeof pdir[pdn]);
 +		flush_ptc_and_tlb(as->smmu, as, iova, &pdir[pdn],
 +				  as->pdir_page, 1);
  	}
 -
 -	as->count = kcalloc(SMMU_NUM_PDE, sizeof(u32), GFP_KERNEL);
 -	if (!as->count) {
 -		__free_page(as->pd);
 -		kfree(as);
 -		return NULL;
 -	}
 -
 -	as->pts = kcalloc(SMMU_NUM_PDE, sizeof(*as->pts), GFP_KERNEL);
 -	if (!as->pts) {
 -		kfree(as->count);
 -		__free_page(as->pd);
 -		kfree(as);
 -		return NULL;
 -	}
 -
 -	/* setup aperture */
 -	as->domain.geometry.aperture_start = 0;
 -	as->domain.geometry.aperture_end = 0xffffffff;
 -	as->domain.geometry.force_aperture = true;
 -
 -	return &as->domain;
  }
  
 -static void tegra_smmu_domain_free(struct iommu_domain *domain)
 +static void free_pdir(struct smmu_as *as)
  {
 -	struct tegra_smmu_as *as = to_smmu_as(domain);
 +	unsigned addr;
 +	int count;
 +	struct device *dev = as->smmu->dev;
  
 -	/* TODO: free page directory and page tables */
 +	if (!as->pdir_page)
 +		return;
  
 -	kfree(as);
 +	addr = as->smmu->iovmm_base;
 +	count = as->smmu->page_count;
 +	while (count-- > 0) {
 +		free_ptbl(as, addr);
 +		addr += SMMU_PAGE_SIZE * SMMU_PTBL_COUNT;
 +	}
 +	ClearPageReserved(as->pdir_page);
 +	__free_page(as->pdir_page);
 +	as->pdir_page = NULL;
 +	devm_kfree(dev, as->pte_count);
 +	as->pte_count = NULL;
  }
  
 -static const struct tegra_smmu_swgroup *
 -tegra_smmu_find_swgroup(struct tegra_smmu *smmu, unsigned int swgroup)
 +/*
 + * Maps PTBL for given iova and returns the PTE address
 + * Caller must unmap the mapped PTBL returned in *ptbl_page_p
 + */
 +static unsigned long *locate_pte(struct smmu_as *as,
 +				 dma_addr_t iova, bool allocate,
 +				 struct page **ptbl_page_p,
 +				 unsigned int **count)
  {
 -	const struct tegra_smmu_swgroup *group = NULL;
 -	unsigned int i;
 +	unsigned long ptn = SMMU_ADDR_TO_PFN(iova);
 +	unsigned long pdn = SMMU_ADDR_TO_PDN(iova);
 +	unsigned long *pdir = page_address(as->pdir_page);
 +	unsigned long *ptbl;
 +
 +	if (pdir[pdn] != _PDE_VACANT(pdn)) {
 +		/* Mapped entry table already exists */
 +		*ptbl_page_p = SMMU_EX_PTBL_PAGE(pdir[pdn]);
 +		ptbl = page_address(*ptbl_page_p);
 +	} else if (!allocate) {
 +		return NULL;
 +	} else {
 +		int pn;
 +		unsigned long addr = SMMU_PDN_TO_ADDR(pdn);
  
 -	for (i = 0; i < smmu->soc->num_swgroups; i++) {
 -		if (smmu->soc->swgroups[i].swgroup == swgroup) {
 -			group = &smmu->soc->swgroups[i];
 -			break;
 +		/* Vacant - allocate a new page table */
 +		dev_dbg(as->smmu->dev, "New PTBL pdn: %lx\n", pdn);
 +
 +		*ptbl_page_p = alloc_page(GFP_ATOMIC);
 +		if (!*ptbl_page_p) {
 +			dev_err(as->smmu->dev,
 +				"failed to allocate smmu_device page table\n");
 +			return NULL;
 +		}
 +		SetPageReserved(*ptbl_page_p);
 +		ptbl = (unsigned long *)page_address(*ptbl_page_p);
 +		for (pn = 0; pn < SMMU_PTBL_COUNT;
 +		     pn++, addr += SMMU_PAGE_SIZE) {
 +			ptbl[pn] = _PTE_VACANT(addr);
  		}
 +		FLUSH_CPU_DCACHE(ptbl, *ptbl_page_p, SMMU_PTBL_SIZE);
 +		pdir[pdn] = SMMU_MK_PDE(*ptbl_page_p,
 +					as->pde_attr | _PDE_NEXT);
 +		FLUSH_CPU_DCACHE(&pdir[pdn], as->pdir_page, sizeof pdir[pdn]);
 +		flush_ptc_and_tlb(as->smmu, as, iova, &pdir[pdn],
 +				  as->pdir_page, 1);
  	}
 +	*count = &as->pte_count[pdn];
  
 -	return group;
 +	return &ptbl[ptn % SMMU_PTBL_COUNT];
  }
  
 -static void tegra_smmu_enable(struct tegra_smmu *smmu, unsigned int swgroup,
 -			      unsigned int asid)
 +#ifdef CONFIG_SMMU_SIG_DEBUG
 +static void put_signature(struct smmu_as *as,
 +			  dma_addr_t iova, unsigned long pfn)
  {
 -	const struct tegra_smmu_swgroup *group;
 -	unsigned int i;
 -	u32 value;
 -
 -	for (i = 0; i < smmu->soc->num_clients; i++) {
 -		const struct tegra_mc_client *client = &smmu->soc->clients[i];
 -
 -		if (client->swgroup != swgroup)
 -			continue;
 +	struct page *page;
 +	unsigned long *vaddr;
  
 -		value = smmu_readl(smmu, client->smmu.reg);
 -		value |= BIT(client->smmu.bit);
 -		smmu_writel(smmu, value, client->smmu.reg);
 -	}
 +	page = pfn_to_page(pfn);
 +	vaddr = page_address(page);
 +	if (!vaddr)
 +		return;
  
 -	group = tegra_smmu_find_swgroup(smmu, swgroup);
 -	if (group) {
 -		value = smmu_readl(smmu, group->reg);
 -		value &= ~SMMU_ASID_MASK;
 -		value |= SMMU_ASID_VALUE(asid);
 -		value |= SMMU_ASID_ENABLE;
 -		smmu_writel(smmu, value, group->reg);
 -	}
 +	vaddr[0] = iova;
 +	vaddr[1] = pfn << PAGE_SHIFT;
 +	FLUSH_CPU_DCACHE(vaddr, page, sizeof(vaddr[0]) * 2);
  }
 -
 -static void tegra_smmu_disable(struct tegra_smmu *smmu, unsigned int swgroup,
 -			       unsigned int asid)
 +#else
 +static inline void put_signature(struct smmu_as *as,
 +				 unsigned long addr, unsigned long pfn)
  {
 -	const struct tegra_smmu_swgroup *group;
 -	unsigned int i;
 -	u32 value;
 -
 -	group = tegra_smmu_find_swgroup(smmu, swgroup);
 -	if (group) {
 -		value = smmu_readl(smmu, group->reg);
 -		value &= ~SMMU_ASID_MASK;
 -		value |= SMMU_ASID_VALUE(asid);
 -		value &= ~SMMU_ASID_ENABLE;
 -		smmu_writel(smmu, value, group->reg);
 -	}
 -
 -	for (i = 0; i < smmu->soc->num_clients; i++) {
 -		const struct tegra_mc_client *client = &smmu->soc->clients[i];
 -
 -		if (client->swgroup != swgroup)
 -			continue;
 -
 -		value = smmu_readl(smmu, client->smmu.reg);
 -		value &= ~BIT(client->smmu.bit);
 -		smmu_writel(smmu, value, client->smmu.reg);
 -	}
  }
 +#endif
  
 -static int tegra_smmu_as_prepare(struct tegra_smmu *smmu,
 -				 struct tegra_smmu_as *as)
 +/*
 + * Caller must not hold as->lock
 + */
 +static int alloc_pdir(struct smmu_as *as)
  {
 -	u32 value;
 -	int err;
 +	unsigned long *pdir, flags;
 +	int pdn, err = 0;
 +	u32 val;
 +	struct smmu_device *smmu = as->smmu;
 +	struct page *page;
 +	unsigned int *cnt;
  
 -	if (as->use_count > 0) {
 -		as->use_count++;
 -		return 0;
 -	}
 +	/*
 +	 * do the allocation, then grab as->lock
 +	 */
 +	cnt = devm_kzalloc(smmu->dev,
 +			   sizeof(cnt[0]) * SMMU_PDIR_COUNT,
 +			   GFP_KERNEL);
 +	page = alloc_page(GFP_KERNEL | __GFP_DMA);
  
 -	as->pd_dma = dma_map_page(smmu->dev, as->pd, 0, SMMU_SIZE_PD,
 -				  DMA_TO_DEVICE);
 -	if (dma_mapping_error(smmu->dev, as->pd_dma))
 -		return -ENOMEM;
 +	spin_lock_irqsave(&as->lock, flags);
  
 -	/* We can't handle 64-bit DMA addresses */
 -	if (!smmu_dma_addr_valid(smmu, as->pd_dma)) {
 +	if (as->pdir_page) {
 +		/* We raced, free the redundant */
 +		err = -EAGAIN;
 +		goto err_out;
 +	}
 +
 +	if (!page || !cnt) {
 +		dev_err(smmu->dev, "failed to allocate at %s\n", __func__);
  		err = -ENOMEM;
 -		goto err_unmap;
 +		goto err_out;
  	}
  
 -	err = tegra_smmu_alloc_asid(smmu, &as->id);
 -	if (err < 0)
 -		goto err_unmap;
 +	as->pdir_page = page;
 +	as->pte_count = cnt;
  
 -	smmu_flush_ptc(smmu, as->pd_dma, 0);
 -	smmu_flush_tlb_asid(smmu, as->id);
 +	SetPageReserved(as->pdir_page);
 +	pdir = page_address(as->pdir_page);
  
 -	smmu_writel(smmu, as->id & 0x7f, SMMU_PTB_ASID);
 -	value = SMMU_PTB_DATA_VALUE(as->pd_dma, as->attr);
 -	smmu_writel(smmu, value, SMMU_PTB_DATA);
 -	smmu_flush(smmu);
 +	for (pdn = 0; pdn < SMMU_PDIR_COUNT; pdn++)
 +		pdir[pdn] = _PDE_VACANT(pdn);
 +	FLUSH_CPU_DCACHE(pdir, as->pdir_page, SMMU_PDIR_SIZE);
 +	val = SMMU_PTC_FLUSH_TYPE_ADR | VA_PAGE_TO_PA(pdir, as->pdir_page);
 +	smmu_write(smmu, val, SMMU_PTC_FLUSH);
 +	FLUSH_SMMU_REGS(as->smmu);
 +	val = SMMU_TLB_FLUSH_VA_MATCH_ALL |
 +		SMMU_TLB_FLUSH_ASID_MATCH__ENABLE |
 +		(as->asid << SMMU_TLB_FLUSH_ASID_SHIFT);
 +	smmu_write(smmu, val, SMMU_TLB_FLUSH);
 +	FLUSH_SMMU_REGS(as->smmu);
  
 -	as->smmu = smmu;
 -	as->use_count++;
 +	spin_unlock_irqrestore(&as->lock, flags);
  
  	return 0;
  
* Unmerged path drivers/iommu/fsl_pamu.h
* Unmerged path drivers/iommu/rockchip-iommu.c
* Unmerged path include/linux/dma-iommu.h
diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c
index 4bd7579ec9e6..331082fd6a4e 100644
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@ -23,6 +23,7 @@
 #include <linux/dma-mapping.h>
 #include <linux/vmalloc.h>
 #include <linux/swiotlb.h>
+#include <linux/pci.h>
 
 #include <asm/cacheflush.h>
 
diff --git a/drivers/infiniband/hw/qedr/main.c b/drivers/infiniband/hw/qedr/main.c
index ef11e770f822..6a72095d6c7a 100644
--- a/drivers/infiniband/hw/qedr/main.c
+++ b/drivers/infiniband/hw/qedr/main.c
@@ -35,6 +35,7 @@
 #include <rdma/ib_user_verbs.h>
 #include <linux/netdevice.h>
 #include <linux/iommu.h>
+#include <linux/pci.h>
 #include <net/addrconf.h>
 #include <linux/qed/qede_roce.h>
 #include <linux/qed/qed_chain.h>
* Unmerged path drivers/iommu/fsl_pamu.h
* Unmerged path drivers/iommu/rockchip-iommu.c
* Unmerged path drivers/iommu/tegra-smmu.c
* Unmerged path include/linux/dma-iommu.h
diff --git a/include/trace/events/iommu.h b/include/trace/events/iommu.h
index 2c7befb10f13..99254ed89212 100644
--- a/include/trace/events/iommu.h
+++ b/include/trace/events/iommu.h
@@ -11,7 +11,6 @@
 #define _TRACE_IOMMU_H
 
 #include <linux/tracepoint.h>
-#include <linux/pci.h>
 
 struct device;
 
