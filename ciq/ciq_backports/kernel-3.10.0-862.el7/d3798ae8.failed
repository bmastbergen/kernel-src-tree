mm: filemap: don't plant shadow entries without radix tree node

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [mm] filemap: don't plant shadow entries without radix tree node (Waiman Long) [1509891]
Rebuild_FUZZ: 96.72%
commit-author Johannes Weiner <hannes@cmpxchg.org>
commit d3798ae8c6f3767c726403c2ca6ecc317752c9dd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/d3798ae8.failed

When the underflow checks were added to workingset_node_shadow_dec(),
they triggered immediately:

  kernel BUG at ./include/linux/swap.h:276!
  invalid opcode: 0000 [#1] SMP
  Modules linked in: isofs usb_storage fuse xt_CHECKSUM ipt_MASQUERADE nf_nat_masquerade_ipv4 tun nf_conntrack_netbios_ns nf_conntrack_broadcast ip6t_REJECT nf_reject_ipv6
   soundcore wmi acpi_als pinctrl_sunrisepoint kfifo_buf tpm_tis industrialio acpi_pad pinctrl_intel tpm_tis_core tpm nfsd auth_rpcgss nfs_acl lockd grace sunrpc dm_crypt
  CPU: 0 PID: 20929 Comm: blkid Not tainted 4.8.0-rc8-00087-gbe67d60ba944 #1
  Hardware name: System manufacturer System Product Name/Z170-K, BIOS 1803 05/06/2016
  task: ffff8faa93ecd940 task.stack: ffff8faa7f478000
  RIP: page_cache_tree_insert+0xf1/0x100
  Call Trace:
    __add_to_page_cache_locked+0x12e/0x270
    add_to_page_cache_lru+0x4e/0xe0
    mpage_readpages+0x112/0x1d0
    blkdev_readpages+0x1d/0x20
    __do_page_cache_readahead+0x1ad/0x290
    force_page_cache_readahead+0xaa/0x100
    page_cache_sync_readahead+0x3f/0x50
    generic_file_read_iter+0x5af/0x740
    blkdev_read_iter+0x35/0x40
    __vfs_read+0xe1/0x130
    vfs_read+0x96/0x130
    SyS_read+0x55/0xc0
    entry_SYSCALL_64_fastpath+0x13/0x8f
  Code: 03 00 48 8b 5d d8 65 48 33 1c 25 28 00 00 00 44 89 e8 75 19 48 83 c4 18 5b 41 5c 41 5d 41 5e 5d c3 0f 0b 41 bd ef ff ff ff eb d7 <0f> 0b e8 88 68 ef ff 0f 1f 84 00
  RIP  page_cache_tree_insert+0xf1/0x100

This is a long-standing bug in the way shadow entries are accounted in
the radix tree nodes. The shrinker needs to know when radix tree nodes
contain only shadow entries, no pages, so node->count is split in half
to count shadows in the upper bits and pages in the lower bits.

Unfortunately, the radix tree implementation doesn't know of this and
assumes all entries are in node->count. When there is a shadow entry
directly in root->rnode and the tree is later extended, the radix tree
implementation will copy that entry into the new node and and bump its
node->count, i.e. increases the page count bits. Once the shadow gets
removed and we subtract from the upper counter, node->count underflows
and triggers the warning. Afterwards, without node->count reaching 0
again, the radix tree node is leaked.

Limit shadow entries to when we have actual radix tree nodes and can
count them properly. That means we lose the ability to detect refaults
from files that had only the first page faulted in at eviction time.

Fixes: 449dd6984d0e ("mm: keep page cache radix tree nodes in check")
	Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
Reported-and-tested-by: Linus Torvalds <torvalds@linux-foundation.org>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Cc: Andrew Morton <akpm@linux-foundation.org>
	Cc: stable@vger.kernel.org
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d3798ae8c6f3767c726403c2ca6ecc317752c9dd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/filemap.c
diff --cc mm/filemap.c
index 685e2bed3093,96b9e9c30630..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -108,52 -110,129 +108,116 @@@
   *   ->tasklist_lock            (memory_failure, collect_procs_ao)
   */
  
 -static int page_cache_tree_insert(struct address_space *mapping,
 -				  struct page *page, void **shadowp)
 +static void page_cache_tree_delete(struct address_space *mapping,
 +				   struct page *page, void *shadow)
  {
++<<<<<<< HEAD
  	struct radix_tree_node *node;
 -	void **slot;
 -	int error;
++=======
++	int i, nr = PageHuge(page) ? 1 : hpage_nr_pages(page);
++>>>>>>> d3798ae8c6f3 (mm: filemap: don't plant shadow entries without radix tree node)
  
 -	error = __radix_tree_create(&mapping->page_tree, page->index, 0,
 -				    &node, &slot);
 -	if (error)
 -		return error;
 -	if (*slot) {
 -		void *p;
 +	VM_BUG_ON(!PageLocked(page));
  
 -		p = radix_tree_deref_slot_protected(slot, &mapping->tree_lock);
 -		if (!radix_tree_exceptional_entry(p))
 -			return -EEXIST;
 +	node = radix_tree_replace_clear_tags(&mapping->page_tree, page->index,
 +								shadow);
  
 -		mapping->nrexceptional--;
 -		if (!dax_mapping(mapping)) {
 -			if (shadowp)
 -				*shadowp = p;
 -			if (node)
 -				workingset_node_shadows_dec(node);
 -		} else {
 -			/* DAX can replace empty locked entry with a hole */
 -			WARN_ON_ONCE(p !=
 -				(void *)(RADIX_TREE_EXCEPTIONAL_ENTRY |
 -					 RADIX_DAX_ENTRY_LOCK));
 -			/* DAX accounts exceptional entries as normal pages */
 -			if (node)
 -				workingset_node_pages_dec(node);
 -			/* Wakeup waiters for exceptional entry lock */
 -			dax_wake_mapping_entry_waiter(mapping, page->index,
 -						      false);
 -		}
 -	}
 -	radix_tree_replace_slot(slot, page);
 -	mapping->nrpages++;
 -	if (node) {
 -		workingset_node_pages_inc(node);
++<<<<<<< HEAD
 +	if (shadow) {
 +		mapping->nrexceptional++;
  		/*
 -		 * Don't track node that contains actual pages.
 -		 *
 -		 * Avoid acquiring the list_lru lock if already
 -		 * untracked.  The list_empty() test is safe as
 -		 * node->private_list is protected by
 -		 * mapping->tree_lock.
 +		 * Make sure the nrexceptional update is committed before
 +		 * the nrpages update so that final truncate racing
 +		 * with reclaim does not see both counters 0 at the
 +		 * same time and miss a shadow entry.
  		 */
 -		if (!list_empty(&node->private_list))
 -			list_lru_del(&workingset_shadow_nodes,
 -				     &node->private_list);
 +		smp_wmb();
  	}
 -	return 0;
 -}
 +	mapping->nrpages--;
  
 -static void page_cache_tree_delete(struct address_space *mapping,
 -				   struct page *page, void *shadow)
 -{
 -	int i, nr = PageHuge(page) ? 1 : hpage_nr_pages(page);
 +	if (!node)
 +		return;
  
 -	VM_BUG_ON_PAGE(!PageLocked(page), page);
 -	VM_BUG_ON_PAGE(PageTail(page), page);
 -	VM_BUG_ON_PAGE(nr != 1 && shadow, page);
 +	workingset_node_pages_dec(node);
 +	if (shadow)
 +		workingset_node_shadows_inc(node);
 +	else
 +		if (__radix_tree_delete_node(&mapping->page_tree, node))
 +			return;
  
 +	/*
 +	 * Track node that only contains shadow entries. DAX mappings contain
 +	 * no shadow entries and may contain other exceptional entries so skip
 +	 * those.
 +	 *
 +	 * Avoid acquiring the list_lru lock if already tracked.  The
 +	 * list_empty() test is safe as node->private_list is
 +	 * protected by mapping->tree_lock.
 +	 */
 +	if (!dax_mapping(mapping) && !workingset_node_pages(node) &&
 +	    list_empty(&node->private_list)) {
 +		node->private_data = mapping;
 +		workingset_remember_node(node);
++=======
+ 	for (i = 0; i < nr; i++) {
+ 		struct radix_tree_node *node;
+ 		void **slot;
+ 
+ 		__radix_tree_lookup(&mapping->page_tree, page->index + i,
+ 				    &node, &slot);
+ 
+ 		radix_tree_clear_tags(&mapping->page_tree, node, slot);
+ 
+ 		if (!node) {
+ 			VM_BUG_ON_PAGE(nr != 1, page);
+ 			/*
+ 			 * We need a node to properly account shadow
+ 			 * entries. Don't plant any without. XXX
+ 			 */
+ 			shadow = NULL;
+ 		}
+ 
+ 		radix_tree_replace_slot(slot, shadow);
+ 
+ 		if (!node)
+ 			break;
+ 
+ 		workingset_node_pages_dec(node);
+ 		if (shadow)
+ 			workingset_node_shadows_inc(node);
+ 		else
+ 			if (__radix_tree_delete_node(&mapping->page_tree, node))
+ 				continue;
+ 
+ 		/*
+ 		 * Track node that only contains shadow entries. DAX mappings
+ 		 * contain no shadow entries and may contain other exceptional
+ 		 * entries so skip those.
+ 		 *
+ 		 * Avoid acquiring the list_lru lock if already tracked.
+ 		 * The list_empty() test is safe as node->private_list is
+ 		 * protected by mapping->tree_lock.
+ 		 */
+ 		if (!dax_mapping(mapping) && !workingset_node_pages(node) &&
+ 				list_empty(&node->private_list)) {
+ 			node->private_data = mapping;
+ 			list_lru_add(&workingset_shadow_nodes,
+ 					&node->private_list);
+ 		}
++>>>>>>> d3798ae8c6f3 (mm: filemap: don't plant shadow entries without radix tree node)
+ 	}
+ 
+ 	if (shadow) {
+ 		mapping->nrexceptional += nr;
+ 		/*
+ 		 * Make sure the nrexceptional update is committed before
+ 		 * the nrpages update so that final truncate racing
+ 		 * with reclaim does not see both counters 0 at the
+ 		 * same time and miss a shadow entry.
+ 		 */
+ 		smp_wmb();
  	}
+ 	mapping->nrpages -= nr;
  }
  
  /*
diff --git a/include/linux/radix-tree.h b/include/linux/radix-tree.h
index 497bd1a32159..c18fa774dbe6 100644
--- a/include/linux/radix-tree.h
+++ b/include/linux/radix-tree.h
@@ -279,9 +279,9 @@ bool __radix_tree_delete_node(struct radix_tree_root *root,
 			      struct radix_tree_node *node);
 void *radix_tree_delete_item(struct radix_tree_root *, unsigned long, void *);
 void *radix_tree_delete(struct radix_tree_root *, unsigned long);
-struct radix_tree_node *radix_tree_replace_clear_tags(
-				struct radix_tree_root *root,
-				unsigned long index, void *entry);
+void radix_tree_clear_tags(struct radix_tree_root *root,
+			   struct radix_tree_node *node,
+			   void **slot);
 unsigned int radix_tree_gang_lookup(struct radix_tree_root *root,
 			void **results, unsigned long first_index,
 			unsigned int max_items);
diff --git a/lib/radix-tree.c b/lib/radix-tree.c
index 31edcb4c659f..2d80eff80bbb 100644
--- a/lib/radix-tree.c
+++ b/lib/radix-tree.c
@@ -1567,15 +1567,10 @@ void *radix_tree_delete(struct radix_tree_root *root, unsigned long index)
 }
 EXPORT_SYMBOL(radix_tree_delete);
 
-struct radix_tree_node *radix_tree_replace_clear_tags(
-			struct radix_tree_root *root,
-			unsigned long index, void *entry)
+void radix_tree_clear_tags(struct radix_tree_root *root,
+			   struct radix_tree_node *node,
+			   void **slot)
 {
-	struct radix_tree_node *node;
-	void **slot;
-
-	__radix_tree_lookup(root, index, &node, &slot);
-
 	if (node) {
 		unsigned int tag, offset = get_slot_offset(node, slot);
 		for (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++)
@@ -1584,9 +1579,6 @@ struct radix_tree_node *radix_tree_replace_clear_tags(
 		/* Clear root node tags */
 		root->gfp_mask &= __GFP_BITS_MASK;
 	}
-
-	radix_tree_replace_slot(slot, entry);
-	return node;
 }
 
 /**
* Unmerged path mm/filemap.c
