scsi: lpfc: Move CQ processing to a soft IRQ

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] lpfc: Move CQ processing to a soft IRQ (Dick Kennedy) [1385844 1461977 1387768]
Rebuild_FUZZ: 92.68%
commit-author Dick Kennedy <dick.kennedy@broadcom.com>
commit f485c18db27734b37003bf2fafd364234e763633
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f485c18d.failed

Under heavy target nvme load duration, the lpfc irq handler is
encountering cpu lockup warnings.

Convert the driver to a shortened ISR handler which identifies the
interrupting condition then schedules a workq thread to process the
completion queue the interrupt was for. This moves all the real work
into the workq element.

As nvmet_fc upcalls are no longer in ISR context, don't set the feature
flags

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit f485c18db27734b37003bf2fafd364234e763633)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc.h
#	drivers/scsi/lpfc/lpfc_nvmet.c
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/lpfc/lpfc_sli4.h
diff --cc drivers/scsi/lpfc/lpfc.h
index 61c6751c0584,231302273257..000000000000
--- a/drivers/scsi/lpfc/lpfc.h
+++ b/drivers/scsi/lpfc/lpfc.h
@@@ -20,6 -22,8 +20,11 @@@
   *******************************************************************/
  
  #include <scsi/scsi_host.h>
++<<<<<<< HEAD
++=======
+ #include <linux/ktime.h>
+ #include <linux/workqueue.h>
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  
  #if defined(CONFIG_DEBUG_FS) && !defined(CONFIG_SCSI_LPFC_DEBUG_FS)
  #define CONFIG_SCSI_LPFC_DEBUG_FS
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index afe166ddbf5a,2893d4fb9654..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -66,14 -74,19 +66,19 @@@ static struct lpfc_iocbq *lpfc_sli4_els
  							 struct lpfc_iocbq *);
  static void lpfc_sli4_send_seq_to_ulp(struct lpfc_vport *,
  				      struct hbq_dmabuf *);
 -static void lpfc_sli4_handle_mds_loopback(struct lpfc_vport *vport,
 -					  struct hbq_dmabuf *dmabuf);
 -static int lpfc_sli4_fp_handle_cqe(struct lpfc_hba *, struct lpfc_queue *,
 +static int lpfc_sli4_fp_handle_wcqe(struct lpfc_hba *, struct lpfc_queue *,
  				    struct lpfc_cqe *);
 -static int lpfc_sli4_post_sgl_list(struct lpfc_hba *, struct list_head *,
 +static int lpfc_sli4_post_els_sgl_list(struct lpfc_hba *, struct list_head *,
  				       int);
++<<<<<<< HEAD
 +static void lpfc_sli4_hba_handle_eqe(struct lpfc_hba *, struct lpfc_eqe *,
 +			uint32_t);
++=======
+ static void lpfc_sli4_hba_handle_eqe(struct lpfc_hba *phba,
+ 				     struct lpfc_eqe *eqe, uint32_t qidx);
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  static bool lpfc_sli4_mbox_completions_pending(struct lpfc_hba *phba);
  static bool lpfc_sli4_process_missed_mbox_completions(struct lpfc_hba *phba);
 -static int lpfc_sli4_abort_nvme_io(struct lpfc_hba *phba,
 -				   struct lpfc_sli_ring *pring,
 -				   struct lpfc_iocbq *cmdiocb);
  
  static IOCB_t *
  lpfc_get_iocb_from_iocbq(struct lpfc_iocbq *iocbq)
@@@ -12291,26 -13046,66 +12293,72 @@@ lpfc_sli4_sp_handle_eqe(struct lpfc_hb
  		return;
  	}
  
++<<<<<<< HEAD
++=======
+ 	/* Save EQ associated with this CQ */
+ 	cq->assoc_qp = speq;
+ 
+ 	if (!queue_work(phba->wq, &cq->spwork))
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 				"0390 Cannot schedule soft IRQ "
+ 				"for CQ eqcqid=%d, cqid=%d on CPU %d\n",
+ 				cqid, cq->queue_id, smp_processor_id());
+ }
+ 
+ /**
+  * lpfc_sli4_sp_process_cq - Process a slow-path event queue entry
+  * @phba: Pointer to HBA context object.
+  *
+  * This routine process a event queue entry from the slow-path event queue.
+  * It will check the MajorCode and MinorCode to determine this is for a
+  * completion event on a completion queue, if not, an error shall be logged
+  * and just return. Otherwise, it will get to the corresponding completion
+  * queue and process all the entries on that completion queue, rearm the
+  * completion queue, and then return.
+  *
+  **/
+ static void
+ lpfc_sli4_sp_process_cq(struct work_struct *work)
+ {
+ 	struct lpfc_queue *cq =
+ 		container_of(work, struct lpfc_queue, spwork);
+ 	struct lpfc_hba *phba = cq->phba;
+ 	struct lpfc_cqe *cqe;
+ 	bool workposted = false;
+ 	int ccount = 0;
+ 
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  	/* Process all the entries to the CQ */
  	switch (cq->type) {
  	case LPFC_MCQ:
  		while ((cqe = lpfc_sli4_cq_get(cq))) {
  			workposted |= lpfc_sli4_sp_handle_mcqe(phba, cqe);
++<<<<<<< HEAD
 +			if (!(++ecount % cq->entry_repost))
 +				lpfc_sli4_cq_release(cq, LPFC_QUEUE_NOARM);
++=======
+ 			if (!(++ccount % cq->entry_repost))
+ 				break;
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  			cq->CQ_mbox++;
  		}
  		break;
  	case LPFC_WCQ:
  		while ((cqe = lpfc_sli4_cq_get(cq))) {
 -			if (cq->subtype == LPFC_FCP ||
 -			    cq->subtype == LPFC_NVME) {
 -#ifdef CONFIG_SCSI_LPFC_DEBUG_FS
 -				if (phba->ktime_on)
 -					cq->isr_timestamp = ktime_get_ns();
 -				else
 -					cq->isr_timestamp = 0;
 -#endif
 -				workposted |= lpfc_sli4_fp_handle_cqe(phba, cq,
 +			if (cq->subtype == LPFC_FCP)
 +				workposted |= lpfc_sli4_fp_handle_wcqe(phba, cq,
  								       cqe);
 -			} else {
 +			else
  				workposted |= lpfc_sli4_sp_handle_cqe(phba, cq,
  								      cqe);
++<<<<<<< HEAD
 +			if (!(++ecount % cq->entry_repost))
 +				lpfc_sli4_cq_release(cq, LPFC_QUEUE_NOARM);
++=======
+ 			}
+ 			if (!(++ccount % cq->entry_repost))
+ 				break;
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  		}
  
  		/* Track the max number of CQEs processed in 1 EQ */
@@@ -12511,11 -13440,8 +12559,16 @@@ static voi
  lpfc_sli4_hba_handle_eqe(struct lpfc_hba *phba, struct lpfc_eqe *eqe,
  			uint32_t qidx)
  {
++<<<<<<< HEAD
 +	struct lpfc_queue *cq;
 +	struct lpfc_cqe *cqe;
 +	bool workposted = false;
 +	uint16_t cqid;
 +	int ecount = 0;
++=======
+ 	struct lpfc_queue *cq = NULL;
+ 	uint16_t cqid, id;
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  
  	if (unlikely(bf_get_le32(lpfc_eqe_major_code, eqe) != 0)) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
@@@ -12529,28 -13455,42 +12582,51 @@@
  	/* Get the reference to the corresponding CQ */
  	cqid = bf_get_le32(lpfc_eqe_resource_id, eqe);
  
 -	if (phba->cfg_nvmet_mrq && phba->sli4_hba.nvmet_cqset) {
 -		id = phba->sli4_hba.nvmet_cqset[0]->queue_id;
 -		if ((cqid >= id) && (cqid < (id + phba->cfg_nvmet_mrq))) {
 -			/* Process NVMET unsol rcv */
 -			cq = phba->sli4_hba.nvmet_cqset[cqid - id];
 -			goto  process_cq;
 -		}
 +	/* Check if this is a Slow path event */
 +	if (unlikely(cqid != phba->sli4_hba.fcp_cq_map[qidx])) {
 +		lpfc_sli4_sp_handle_eqe(phba, eqe,
 +			phba->sli4_hba.hba_eq[qidx]);
 +		return;
  	}
  
 -	if (phba->sli4_hba.nvme_cq_map &&
 -	    (cqid == phba->sli4_hba.nvme_cq_map[qidx])) {
 -		/* Process NVME / NVMET command completion */
 -		cq = phba->sli4_hba.nvme_cq[qidx];
 -		goto  process_cq;
 +	if (unlikely(!phba->sli4_hba.fcp_cq)) {
 +		lpfc_printf_log(phba, KERN_WARNING, LOG_SLI,
 +				"3146 Fast-path completion queues "
 +				"does not exist\n");
 +		return;
 +	}
 +	cq = phba->sli4_hba.fcp_cq[qidx];
 +	if (unlikely(!cq)) {
 +		if (phba->sli.sli_flag & LPFC_SLI_ACTIVE)
 +			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
 +					"0367 Fast-path completion queue "
 +					"(%d) does not exist\n", qidx);
 +		return;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (phba->sli4_hba.fcp_cq_map &&
+ 	    (cqid == phba->sli4_hba.fcp_cq_map[qidx])) {
+ 		/* Process FCP command completion */
+ 		cq = phba->sli4_hba.fcp_cq[qidx];
+ 		goto  process_cq;
+ 	}
+ 
+ 	if (phba->sli4_hba.nvmels_cq &&
+ 	    (cqid == phba->sli4_hba.nvmels_cq->queue_id)) {
+ 		/* Process NVME unsol rcv */
+ 		cq = phba->sli4_hba.nvmels_cq;
+ 	}
+ 
+ 	/* Otherwise this is a Slow path event */
+ 	if (cq == NULL) {
+ 		lpfc_sli4_sp_handle_eqe(phba, eqe, phba->sli4_hba.hba_eq[qidx]);
+ 		return;
+ 	}
+ 
+ process_cq:
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  	if (unlikely(cqid != cq->queue_id)) {
  		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
  				"0368 Miss-matched fast-path completion "
@@@ -12559,19 -13499,58 +12635,71 @@@
  		return;
  	}
  
++<<<<<<< HEAD
 +	/* Process all the entries to the CQ */
 +	while ((cqe = lpfc_sli4_cq_get(cq))) {
 +		workposted |= lpfc_sli4_fp_handle_wcqe(phba, cq, cqe);
 +		if (!(++ecount % cq->entry_repost))
 +			lpfc_sli4_cq_release(cq, LPFC_QUEUE_NOARM);
 +	}
 +
 +	/* Track the max number of CQEs processed in 1 EQ */
 +	if (ecount > cq->CQ_max_cqe)
 +		cq->CQ_max_cqe = ecount;
++=======
+ 	/* Save EQ associated with this CQ */
+ 	cq->assoc_qp = phba->sli4_hba.hba_eq[qidx];
+ 
+ 	if (!queue_work(phba->wq, &cq->irqwork))
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 				"0363 Cannot schedule soft IRQ "
+ 				"for CQ eqcqid=%d, cqid=%d on CPU %d\n",
+ 				cqid, cq->queue_id, smp_processor_id());
+ }
+ 
+ /**
+  * lpfc_sli4_hba_process_cq - Process a fast-path event queue entry
+  * @phba: Pointer to HBA context object.
+  * @eqe: Pointer to fast-path event queue entry.
+  *
+  * This routine process a event queue entry from the fast-path event queue.
+  * It will check the MajorCode and MinorCode to determine this is for a
+  * completion event on a completion queue, if not, an error shall be logged
+  * and just return. Otherwise, it will get to the corresponding completion
+  * queue and process all the entries on the completion queue, rearm the
+  * completion queue, and then return.
+  **/
+ static void
+ lpfc_sli4_hba_process_cq(struct work_struct *work)
+ {
+ 	struct lpfc_queue *cq =
+ 		container_of(work, struct lpfc_queue, irqwork);
+ 	struct lpfc_hba *phba = cq->phba;
+ 	struct lpfc_cqe *cqe;
+ 	bool workposted = false;
+ 	int ccount = 0;
+ 
+ 	/* Process all the entries to the CQ */
+ 	while ((cqe = lpfc_sli4_cq_get(cq))) {
+ #ifdef CONFIG_SCSI_LPFC_DEBUG_FS
+ 		if (phba->ktime_on)
+ 			cq->isr_timestamp = ktime_get_ns();
+ 		else
+ 			cq->isr_timestamp = 0;
+ #endif
+ 		workposted |= lpfc_sli4_fp_handle_cqe(phba, cq, cqe);
+ 		if (!(++ccount % cq->entry_repost))
+ 			break;
+ 	}
+ 
+ 	/* Track the max number of CQEs processed in 1 EQ */
+ 	if (ccount > cq->CQ_max_cqe)
+ 		cq->CQ_max_cqe = ccount;
+ 	cq->assoc_qp->EQ_cqe_cnt += ccount;
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  
  	/* Catch the no cq entry condition */
- 	if (unlikely(ecount == 0))
+ 	if (unlikely(ccount == 0))
  		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
  				"0369 No entry from fast-path completion "
  				"queue fcpcqid=%d\n", cq->queue_id);
@@@ -12653,29 -13629,12 +12778,28 @@@ lpfc_sli4_fof_handle_eqe(struct lpfc_hb
  	/* Save EQ associated with this CQ */
  	cq->assoc_qp = phba->sli4_hba.fof_eq;
  
++<<<<<<< HEAD
 +	/* Process all the entries to the OAS CQ */
 +	while ((cqe = lpfc_sli4_cq_get(cq))) {
 +		workposted |= lpfc_sli4_fp_handle_wcqe(phba, cq, cqe);
 +		if (!(++ecount % cq->entry_repost))
 +			lpfc_sli4_cq_release(cq, LPFC_QUEUE_NOARM);
 +	}
 +
 +	/* Track the max number of CQEs processed in 1 EQ */
 +	if (ecount > cq->CQ_max_cqe)
 +		cq->CQ_max_cqe = ecount;
 +
 +	/* Catch the no cq entry condition */
 +	if (unlikely(ecount == 0))
++=======
+ 	/* CQ work will be processed on CPU affinitized to this IRQ */
+ 	if (!queue_work(phba->wq, &cq->irqwork))
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  		lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
- 				"9153 No entry from fast-path completion "
- 				"queue fcpcqid=%d\n", cq->queue_id);
- 
- 	/* In any case, flash and re-arm the CQ */
- 	lpfc_sli4_cq_release(cq, LPFC_QUEUE_REARM);
- 
- 	/* wake up worker thread if there are works to be done */
- 	if (workposted)
- 		lpfc_worker_wake_up(phba);
+ 				"0367 Cannot schedule soft IRQ "
+ 				"for CQ eqcqid=%d, cqid=%d on CPU %d\n",
+ 				cqid, cq->queue_id, smp_processor_id());
  }
  
  /**
@@@ -12802,12 -13760,12 +12926,16 @@@ lpfc_sli4_hba_intr_handler(int irq, voi
  	struct lpfc_eqe *eqe;
  	unsigned long iflag;
  	int ecount = 0;
++<<<<<<< HEAD
 +	int fcp_eqidx;
++=======
+ 	int hba_eqidx;
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  
  	/* Get the driver's phba structure from the dev_id */
 -	hba_eq_hdl = (struct lpfc_hba_eq_hdl *)dev_id;
 -	phba = hba_eq_hdl->phba;
 -	hba_eqidx = hba_eq_hdl->idx;
 +	fcp_eq_hdl = (struct lpfc_fcp_eq_hdl *)dev_id;
 +	phba = fcp_eq_hdl->phba;
 +	fcp_eqidx = fcp_eq_hdl->idx;
  
  	if (unlikely(!phba))
  		return IRQ_NONE;
@@@ -12846,12 -13803,9 +12974,17 @@@
  	 * Process all the event on FCP fast-path EQ
  	 */
  	while ((eqe = lpfc_sli4_eq_get(fpeq))) {
++<<<<<<< HEAD
 +		if (eqe == NULL)
++=======
+ 		lpfc_sli4_hba_handle_eqe(phba, eqe, hba_eqidx);
+ 		if (!(++ecount % fpeq->entry_repost))
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  			break;
 +
 +		lpfc_sli4_hba_handle_eqe(phba, eqe, fcp_eqidx);
 +		if (!(++ecount % fpeq->entry_repost))
 +			lpfc_sli4_eq_release(fpeq, LPFC_QUEUE_NOARM);
  		fpeq->EQ_processed++;
  	}
  
@@@ -13018,16 -13986,11 +13151,22 @@@ lpfc_sli4_queue_alloc(struct lpfc_hba *
  	}
  	queue->entry_size = entry_size;
  	queue->entry_count = entry_count;
++<<<<<<< HEAD
++=======
+ 	queue->phba = phba;
+ 	INIT_WORK(&queue->irqwork, lpfc_sli4_hba_process_cq);
+ 	INIT_WORK(&queue->spwork, lpfc_sli4_sp_process_cq);
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  
 -	/* entry_repost will be set during q creation */
 +	/*
 +	 * entry_repost is calculated based on the number of entries in the
 +	 * queue. This works out except for RQs. If buffers are NOT initially
 +	 * posted for every RQE, entry_repost should be adjusted accordingly.
 +	 */
 +	queue->entry_repost = (entry_count >> 3);
 +	if (queue->entry_repost < LPFC_QUEUE_MIN_REPOST)
 +		queue->entry_repost = LPFC_QUEUE_MIN_REPOST;
 +	queue->phba = phba;
  
  	return queue;
  out_fail:
diff --cc drivers/scsi/lpfc/lpfc_sli4.h
index 10078254ebc7,13b8f4d4da34..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@@ -134,10 -152,15 +134,18 @@@ struct lpfc_queue 
  	uint32_t entry_count;	/* Number of entries to support on the queue */
  	uint32_t entry_size;	/* Size of each queue entry. */
  	uint32_t entry_repost;	/* Count of entries before doorbell is rung */
++<<<<<<< HEAD
 +#define LPFC_QUEUE_MIN_REPOST	8
++=======
+ #define LPFC_EQ_REPOST		8
+ #define LPFC_MQ_REPOST		8
+ #define LPFC_CQ_REPOST		64
+ #define LPFC_RQ_REPOST		64
+ #define LPFC_RELEASE_NOTIFICATION_INTERVAL	32  /* For WQs */
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  	uint32_t queue_id;	/* Queue ID assigned by the hardware */
  	uint32_t assoc_qid;     /* Queue ID associated with, for CQ/WQ/MQ */
 +	struct list_head page_list;
  	uint32_t page_count;	/* Number of pages allocated for this queue */
  	uint32_t host_index;	/* The host's index for putting or getting */
  	uint32_t hba_index;	/* The last known hba index for get or put */
@@@ -173,9 -198,14 +181,17 @@@
  /* defines for RQ stats */
  #define	RQ_no_posted_buf	q_cnt_1
  #define	RQ_no_buf_found		q_cnt_2
 -#define	RQ_buf_posted		q_cnt_3
 +#define	RQ_buf_trunc		q_cnt_3
  #define	RQ_rcv_buf		q_cnt_4
  
++<<<<<<< HEAD
++=======
+ 	struct work_struct irqwork;
+ 	struct work_struct spwork;
+ 
+ 	uint64_t isr_timestamp;
+ 	struct lpfc_queue *assoc_qp;
++>>>>>>> f485c18db277 (scsi: lpfc: Move CQ processing to a soft IRQ)
  	union sli4_qe qe[1];	/* array to index entries (must be last) */
  };
  
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc.h
diff --git a/drivers/scsi/lpfc/lpfc_init.c b/drivers/scsi/lpfc/lpfc_init.c
index 178fd4fd8506..4643394c6a30 100644
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@ -3071,6 +3071,9 @@ lpfc_offline_prep(struct lpfc_hba *phba, int mbx_action)
 	lpfc_destroy_vport_work_array(phba, vports);
 
 	lpfc_sli_mbox_sys_shutdown(phba, mbx_action);
+
+	if (phba->wq)
+		flush_workqueue(phba->wq);
 }
 
 /**
@@ -3745,6 +3748,9 @@ void
 lpfc_stop_port(struct lpfc_hba *phba)
 {
 	phba->lpfc_stop_port(phba);
+
+	if (phba->wq)
+		flush_workqueue(phba->wq);
 }
 
 /**
@@ -5885,6 +5891,9 @@ lpfc_setup_driver_resource_phase2(struct lpfc_hba *phba)
 		return error;
 	}
 
+	/* workqueue for deferred irq use */
+	phba->wq = alloc_workqueue("lpfc_wq", WQ_MEM_RECLAIM, 0);
+
 	return 0;
 }
 
@@ -5899,6 +5908,12 @@ lpfc_setup_driver_resource_phase2(struct lpfc_hba *phba)
 static void
 lpfc_unset_driver_resource_phase2(struct lpfc_hba *phba)
 {
+	if (phba->wq) {
+		flush_workqueue(phba->wq);
+		destroy_workqueue(phba->wq);
+		phba->wq = NULL;
+	}
+
 	/* Stop kernel worker thread */
 	kthread_stop(phba->worker_thread);
 }
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli4.h
