filesystem-dax: convert to dax_direct_access()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Dan Williams <dan.j.williams@intel.com>
commit cccbce67158290537cc671cbd4c1564876485a65
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/cccbce67.failed

Now that a dax_device is plumbed through all dax-capable drivers we can
switch from block_device_operations to dax_operations for invoking
->direct_access.

This also lets us kill off some usages of struct blk_dax_ctl on the way
to its eventual removal.

	Suggested-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit cccbce67158290537cc671cbd4c1564876485a65)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
#	include/linux/dax.h
diff --cc fs/dax.c
index fa7935571d11,ce9dc9c3e829..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -49,234 -55,25 +49,238 @@@ static int __init init_dax_wait_table(v
  }
  fs_initcall(init_dax_wait_table);
  
++<<<<<<< HEAD
 +static long dax_map_atomic(struct block_device *bdev, struct blk_dax_ctl *dax)
 +{
 +	struct request_queue *q = bdev->bd_queue;
 +	long rc = -EIO;
 +
 +	dax->addr = ERR_PTR(-EIO);
 +	if (blk_queue_enter(q, true) != 0)
 +		return rc;
 +
 +	rc = bdev_direct_access(bdev, dax);
 +	if (rc < 0) {
 +		dax->addr = ERR_PTR(rc);
 +		blk_queue_exit(q);
 +		return rc;
 +	}
 +	return rc;
 +}
 +
 +static void dax_unmap_atomic(struct block_device *bdev,
 +		const struct blk_dax_ctl *dax)
 +{
 +	if (IS_ERR(dax->addr))
 +		return;
 +	blk_queue_exit(bdev->bd_queue);
 +}
 +
 +struct page *read_dax_sector(struct block_device *bdev, sector_t n)
++=======
+ static int dax_is_pmd_entry(void *entry)
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  {
 -	return (unsigned long)entry & RADIX_DAX_PMD;
 +	struct page *page = alloc_pages(GFP_KERNEL, 0);
 +	struct blk_dax_ctl dax = {
 +		.size = PAGE_SIZE,
 +		.sector = n & ~((((int) PAGE_SIZE) / 512) - 1),
 +	};
 +	long rc;
 +
 +	if (!page)
 +		return ERR_PTR(-ENOMEM);
 +
 +	rc = dax_map_atomic(bdev, &dax);
 +	if (rc < 0)
 +		return ERR_PTR(rc);
 +	memcpy_from_pmem(page_address(page), dax.addr, PAGE_SIZE);
 +	dax_unmap_atomic(bdev, &dax);
 +	return page;
  }
  
 -static int dax_is_pte_entry(void *entry)
 +static bool buffer_written(struct buffer_head *bh)
  {
 -	return !((unsigned long)entry & RADIX_DAX_PMD);
 +	return buffer_mapped(bh) && !buffer_unwritten(bh);
  }
  
 -static int dax_is_zero_entry(void *entry)
 +static int zero_toiovecend_partial(const struct iovec *iov, int offset, int len)
  {
 -	return (unsigned long)entry & RADIX_DAX_HZP;
 +	int zero, orig_len = len;
 +	for (; len > 0; ++iov) {
 +		/* Skip over the finished iovecs */
 +		if (unlikely(offset >= iov->iov_len)) {
 +			offset -= iov->iov_len;
 +			continue;
 +		}
 +		zero = min_t(unsigned int, iov->iov_len - offset, len);
 +		if (clear_user(iov->iov_base + offset, zero))
 +			return orig_len - len;
 +		offset = 0;
 +		len -= zero;
 +	}
 +
 +	return orig_len - len;
 +}
 +
 +
 +static sector_t to_sector(const struct buffer_head *bh,
 +		const struct inode *inode)
 +{
 +	sector_t sector = bh->b_blocknr << (inode->i_blkbits - 9);
 +
 +	return sector;
 +}
 +
 +static ssize_t dax_io(int rw, struct inode *inode, const struct iovec *iov,
 +			loff_t start, loff_t end, get_block_t get_block,
 +			struct buffer_head *bh)
 +{
 +	loff_t pos = start, max = start, bh_max = start;
 +	bool hole = false;
 +	struct block_device *bdev = NULL;
 +	int rc;
 +	long map_len = 0;
 +	struct blk_dax_ctl dax = {
 +		.addr = ERR_PTR(-EIO),
 +	};
 +	unsigned blkbits = inode->i_blkbits;
 +	sector_t file_blks = (i_size_read(inode) + (1 << blkbits) - 1)
 +								>> blkbits;
 +
 +	rw &= RW_MASK;
 +	if (rw == READ)
 +		end = min(end, i_size_read(inode));
 +
 +	while (pos < end) {
 +		size_t len;
 +		if (pos == max) {
 +			long page = pos >> PAGE_SHIFT;
 +			sector_t block = page << (PAGE_SHIFT - blkbits);
 +			unsigned first = pos - (block << blkbits);
 +			long size;
 +
 +			if (pos == bh_max) {
 +				bh->b_size = PAGE_ALIGN(end - pos);
 +				bh->b_state = 0;
 +				rc = get_block(inode, block, bh, rw == WRITE);
 +				if (rc)
 +					break;
 +				bh_max = pos - first + bh->b_size;
 +				bdev = bh->b_bdev;
 +				/*
 +				 * We allow uninitialized buffers for writes
 +				 * beyond EOF as those cannot race with faults
 +				 */
 +				WARN_ON_ONCE(
 +					(buffer_new(bh) && block < file_blks) ||
 +					(rw == WRITE && buffer_unwritten(bh)));
 +			} else {
 +				unsigned done = bh->b_size -
 +						(bh_max - (pos - first));
 +				bh->b_blocknr += done >> blkbits;
 +				bh->b_size -= done;
 +			}
 +
 +			hole = (rw == READ) && !buffer_written(bh);
 +			if (hole) {
 +				size = bh->b_size - first;
 +			} else {
 +				dax_unmap_atomic(bdev, &dax);
 +				dax.sector = to_sector(bh, inode);
 +				dax.size = bh->b_size;
 +				map_len = dax_map_atomic(bdev, &dax);
 +				if (map_len < 0) {
 +					rc = map_len;
 +					break;
 +				}
 +				dax.addr += first;
 +				size = map_len - first;
 +			}
 +			/*
 +			 * pos + size is one past the last offset for IO,
 +			 * so pos + size can overflow loff_t at extreme offsets.
 +			 * Cast to u64 to catch this and get the true minimum.
 +			 */
 +			max = min_t(u64, pos + size, end);
 +		}
 +
 +		if (rw == WRITE)
 +			len = memcpy_fromiovecend_partial_nocache(
 +				(void __force *)dax.addr, iov, pos - start,
 +				max - pos);
 +		else if (!hole)
 +			len = memcpy_toiovecend_partial(iov,
 +				(void __force *)dax.addr, pos - start,
 +				max - pos);
 +		else
 +			len = zero_toiovecend_partial(iov, pos - start,
 +				max - pos);
 +
 +		if (!len) {
 +			rc = -EFAULT;
 +			break;
 +		}
 +
 +		pos += len;
 +		if (!IS_ERR(dax.addr))
 +			dax.addr += len;
 +	}
 +
 +	dax_unmap_atomic(bdev, &dax);
 +
 +	return (pos == start) ? rc : pos - start;
  }
  
 -static int dax_is_empty_entry(void *entry)
 +/**
 + * dax_do_io - Perform I/O to a DAX file
 + * @rw: READ to read or WRITE to write
 + * @iocb: The control block for this I/O
 + * @inode: The file which the I/O is directed at
 + * @iter: The addresses to do I/O from or to
 + * @pos: The file offset where the I/O starts
 + * @get_block: The filesystem method used to translate file offsets to blocks
 + * @end_io: A filesystem callback for I/O completion
 + * @flags: See below
 + *
 + * This function uses the same locking scheme as do_blockdev_direct_IO:
 + * If @flags has DIO_LOCKING set, we assume that the i_mutex is held by the
 + * caller for writes.  For reads, we take and release the i_mutex ourselves.
 + * If DIO_LOCKING is not set, the filesystem takes care of its own locking.
 + * As with do_blockdev_direct_IO(), we increment i_dio_count while the I/O
 + * is in progress.
 + */
 +ssize_t dax_do_io(int rw, struct kiocb *iocb, struct inode *inode,
 +		  const struct iovec *iov, loff_t pos, unsigned long nr_segs,
 +		  get_block_t get_block, dio_iodone_t end_io, int flags)
  {
 -	return (unsigned long)entry & RADIX_DAX_EMPTY;
 +	struct buffer_head bh;
 +	ssize_t retval = -EINVAL;
 +	loff_t end = pos + iov_length(iov, nr_segs);
 +
 +	memset(&bh, 0, sizeof(bh));
 +	bh.b_bdev = inode->i_sb->s_bdev;
 +
 +	if ((flags & DIO_LOCKING) && (rw == READ))
 +		mutex_lock(&inode->i_mutex);
 +
 +	/* Protects against truncate */
 +	if (!(flags & DIO_SKIP_DIO_COUNT))
 +		inode_dio_begin(inode);
 +
 +	retval = dax_io(rw, inode, iov, pos, end, get_block, &bh);
 +
 +	if ((flags & DIO_LOCKING) && (rw == READ))
 +		mutex_unlock(&inode->i_mutex);
 +
 +	if ((retval > 0) && end_io)
 +		end_io(iocb, pos, retval, bh.b_private, retval, false);
 +
 +	if (!(flags & DIO_SKIP_DIO_COUNT))
 +		inode_dio_end(inode);
 +	return retval;
  }
 +EXPORT_SYMBOL_GPL(dax_do_io);
  
  /*
   * DAX radix tree locking
@@@ -581,30 -511,46 +585,39 @@@ static int dax_load_hole(struct address
  
  	/* This will replace locked radix tree entry with a hole page */
  	page = find_or_create_page(mapping, vmf->pgoff,
 -				   vmf->gfp_mask | __GFP_ZERO);
 -	if (!page)
 +		mapping_gfp_mask(mapping) | __GFP_FS | __GFP_IO | __GFP_ZERO);
 +	if (!page) {
 +		put_locked_mapping_entry(mapping, vmf->pgoff, entry);
  		return VM_FAULT_OOM;
 - out:
 -	vmf->page = page;
 -	ret = finish_fault(vmf);
 -	vmf->page = NULL;
 -	*entry = page;
 -	if (!ret) {
 -		/* Grab reference for PTE that is now referencing the page */
 -		get_page(page);
 -		return VM_FAULT_NOPAGE;
  	}
 -	return ret;
 +	vmf->page = page;
 +	return VM_FAULT_LOCKED;
  }
  
- static int copy_user_dax(struct block_device *bdev, sector_t sector, size_t size,
- 		struct page *to, unsigned long vaddr)
+ static int copy_user_dax(struct block_device *bdev, struct dax_device *dax_dev,
+ 		sector_t sector, size_t size, struct page *to,
+ 		unsigned long vaddr)
  {
- 	struct blk_dax_ctl dax = {
- 		.sector = sector,
- 		.size = size,
- 	};
- 	void *vto;
+ 	void *vto, *kaddr;
+ 	pgoff_t pgoff;
+ 	pfn_t pfn;
+ 	long rc;
+ 	int id;
  
- 	if (dax_map_atomic(bdev, &dax) < 0)
- 		return PTR_ERR(dax.addr);
+ 	rc = bdev_dax_pgoff(bdev, sector, size, &pgoff);
+ 	if (rc)
+ 		return rc;
+ 
+ 	id = dax_read_lock();
+ 	rc = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
+ 	if (rc < 0) {
+ 		dax_read_unlock(id);
+ 		return rc;
+ 	}
  	vto = kmap_atomic(to);
- 	copy_user_page(vto, (void __force *)dax.addr, vaddr, to);
+ 	copy_user_page(vto, (void __force *)kaddr, vaddr, to);
  	kunmap_atomic(vto);
- 	dax_unmap_atomic(bdev, &dax);
+ 	dax_read_unlock(id);
  	return 0;
  }
  
@@@ -680,62 -651,170 +693,113 @@@ static void *dax_insert_mapping_entry(s
  }
  
  static int dax_writeback_one(struct block_device *bdev,
- 		struct address_space *mapping, pgoff_t index, void *entry)
+ 		struct dax_device *dax_dev, struct address_space *mapping,
+ 		pgoff_t index, void *entry)
  {
  	struct radix_tree_root *page_tree = &mapping->page_tree;
++<<<<<<< HEAD
 +	int type = RADIX_DAX_TYPE(entry);
 +	struct radix_tree_node *node;
 +	struct blk_dax_ctl dax;
 +	void **slot;
 +	int ret = 0;
++=======
+ 	void *entry2, **slot, *kaddr;
+ 	long ret = 0, id;
+ 	sector_t sector;
+ 	pgoff_t pgoff;
+ 	size_t size;
+ 	pfn_t pfn;
 -
 -	/*
 -	 * A page got tagged dirty in DAX mapping? Something is seriously
 -	 * wrong.
 -	 */
 -	if (WARN_ON(!radix_tree_exceptional_entry(entry)))
 -		return -EIO;
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  
  	spin_lock_irq(&mapping->tree_lock);
 -	entry2 = get_unlocked_mapping_entry(mapping, index, &slot);
 -	/* Entry got punched out / reallocated? */
 -	if (!entry2 || !radix_tree_exceptional_entry(entry2))
 -		goto put_unlocked;
  	/*
 -	 * Entry got reallocated elsewhere? No need to writeback. We have to
 -	 * compare sectors as we must not bail out due to difference in lockbit
 -	 * or entry type.
 +	 * Regular page slots are stabilized by the page lock even
 +	 * without the tree itself locked.  These unlocked entries
 +	 * need verification under the tree lock.
  	 */
 -	if (dax_radix_sector(entry2) != dax_radix_sector(entry))
 -		goto put_unlocked;
 -	if (WARN_ON_ONCE(dax_is_empty_entry(entry) ||
 -				dax_is_zero_entry(entry))) {
 +	if (!__radix_tree_lookup(page_tree, index, &node, &slot))
 +		goto unlock;
 +	if (*slot != entry)
 +		goto unlock;
 +
 +	/* another fsync thread may have already written back this entry */
 +	if (!radix_tree_tag_get(page_tree, index, PAGECACHE_TAG_TOWRITE))
 +		goto unlock;
 +
 +	if (WARN_ON_ONCE(type != RADIX_DAX_PTE && type != RADIX_DAX_PMD)) {
  		ret = -EIO;
 -		goto put_unlocked;
 +		goto unlock;
  	}
  
 -	/* Another fsync thread may have already written back this entry */
 -	if (!radix_tree_tag_get(page_tree, index, PAGECACHE_TAG_TOWRITE))
 -		goto put_unlocked;
 -	/* Lock the entry to serialize with page faults */
 -	entry = lock_slot(mapping, slot);
 -	/*
 -	 * We can clear the tag now but we have to be careful so that concurrent
 -	 * dax_writeback_one() calls for the same index cannot finish before we
 -	 * actually flush the caches. This is achieved as the calls will look
 -	 * at the entry only under tree_lock and once they do that they will
 -	 * see the entry locked and wait for it to unlock.
 -	 */
 -	radix_tree_tag_clear(page_tree, index, PAGECACHE_TAG_TOWRITE);
 +	dax.sector = RADIX_DAX_SECTOR(entry);
 +	dax.size = (type == RADIX_DAX_PMD ? PMD_SIZE : PAGE_SIZE);
  	spin_unlock_irq(&mapping->tree_lock);
  
  	/*
++<<<<<<< HEAD
 +	 * We cannot hold tree_lock while calling dax_map_atomic() because it
 +	 * eventually calls cond_resched().
 +	 */
 +	ret = dax_map_atomic(bdev, &dax);
 +	if (ret < 0)
 +		return ret;
++=======
+ 	 * Even if dax_writeback_mapping_range() was given a wbc->range_start
+ 	 * in the middle of a PMD, the 'index' we are given will be aligned to
+ 	 * the start index of the PMD, as will the sector we pull from
+ 	 * 'entry'.  This allows us to flush for PMD_SIZE and not have to
+ 	 * worry about partial PMD writebacks.
+ 	 */
+ 	sector = dax_radix_sector(entry);
+ 	size = PAGE_SIZE << dax_radix_order(entry);
+ 
+ 	id = dax_read_lock();
+ 	ret = bdev_dax_pgoff(bdev, sector, size, &pgoff);
+ 	if (ret)
+ 		goto dax_unlock;
+ 
+ 	/*
+ 	 * dax_direct_access() may sleep, so cannot hold tree_lock over
+ 	 * its invocation.
+ 	 */
+ 	ret = dax_direct_access(dax_dev, pgoff, size / PAGE_SIZE, &kaddr, &pfn);
+ 	if (ret < 0)
+ 		goto dax_unlock;
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  
- 	if (WARN_ON_ONCE(ret < dax.size)) {
+ 	if (WARN_ON_ONCE(ret < size / PAGE_SIZE)) {
  		ret = -EIO;
- 		goto unmap;
+ 		goto dax_unlock;
  	}
  
++<<<<<<< HEAD
 +	wb_cache_pmem(dax.addr, dax.size);
 +
++=======
+ 	dax_mapping_entry_mkclean(mapping, index, pfn_t_to_pfn(pfn));
+ 	wb_cache_pmem(kaddr, size);
+ 	/*
+ 	 * After we have flushed the cache, we can clear the dirty tag. There
+ 	 * cannot be new dirty data in the pfn after the flush has completed as
+ 	 * the pfn mappings are writeprotected and fault waits for mapping
+ 	 * entry lock.
+ 	 */
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  	spin_lock_irq(&mapping->tree_lock);
 -	radix_tree_tag_clear(page_tree, index, PAGECACHE_TAG_DIRTY);
 +	radix_tree_tag_clear(page_tree, index, PAGECACHE_TAG_TOWRITE);
  	spin_unlock_irq(&mapping->tree_lock);
++<<<<<<< HEAD
 + unmap:
 +	dax_unmap_atomic(bdev, &dax);
++=======
+  dax_unlock:
+ 	dax_read_unlock(id);
+ 	put_locked_mapping_entry(mapping, index, entry);
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  	return ret;
  
 - put_unlocked:
 -	put_unlocked_mapping_entry(mapping, index, entry2);
 + unlock:
  	spin_unlock_irq(&mapping->tree_lock);
  	return ret;
  }
@@@ -749,8 -828,9 +813,9 @@@ int dax_writeback_mapping_range(struct 
  		struct block_device *bdev, struct writeback_control *wbc)
  {
  	struct inode *inode = mapping->host;
 -	pgoff_t start_index, end_index;
 +	pgoff_t start_index, end_index, pmd_index;
  	pgoff_t indices[PAGEVEC_SIZE];
+ 	struct dax_device *dax_dev;
  	struct pagevec pvec;
  	bool done = false;
  	int i, ret = 0;
@@@ -762,17 -841,12 +827,26 @@@
  	if (!mapping->nrexceptional || wbc->sync_mode != WB_SYNC_ALL)
  		return 0;
  
++<<<<<<< HEAD
 +	start_index = wbc->range_start >> PAGE_CACHE_SHIFT;
 +	end_index = wbc->range_end >> PAGE_CACHE_SHIFT;
 +	pmd_index = DAX_PMD_INDEX(start_index);
 +
 +	rcu_read_lock();
 +	entry = radix_tree_lookup(&mapping->page_tree, pmd_index);
 +	rcu_read_unlock();
 +
 +	/* see if the start of our range is covered by a PMD entry */
 +	if (entry && RADIX_DAX_TYPE(entry) == RADIX_DAX_PMD)
 +		start_index = pmd_index;
++=======
+ 	dax_dev = dax_get_by_host(bdev->bd_disk->disk_name);
+ 	if (!dax_dev)
+ 		return -EIO;
+ 
+ 	start_index = wbc->range_start >> PAGE_SHIFT;
+ 	end_index = wbc->range_end >> PAGE_SHIFT;
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  
  	tag_pages_for_writeback(mapping, start_index, end_index);
  
@@@ -791,150 -865,60 +865,174 @@@
  				break;
  			}
  
- 			ret = dax_writeback_one(bdev, mapping, indices[i],
- 					pvec.pages[i]);
- 			if (ret < 0)
+ 			ret = dax_writeback_one(bdev, dax_dev, mapping,
+ 					indices[i], pvec.pages[i]);
+ 			if (ret < 0) {
+ 				put_dax(dax_dev);
  				return ret;
+ 			}
  		}
 +		start_index = indices[pvec.nr - 1] + 1;
  	}
+ 	put_dax(dax_dev);
  	return 0;
  }
  EXPORT_SYMBOL_GPL(dax_writeback_mapping_range);
  
  static int dax_insert_mapping(struct address_space *mapping,
- 		struct block_device *bdev, sector_t sector, size_t size,
- 		void **entryp, struct vm_area_struct *vma, struct vm_fault *vmf)
+ 		struct block_device *bdev, struct dax_device *dax_dev,
+ 		sector_t sector, size_t size, void **entryp,
+ 		struct vm_area_struct *vma, struct vm_fault *vmf)
  {
++<<<<<<< HEAD
 +	unsigned long vaddr = (unsigned long)vmf->virtual_address;
 +	struct blk_dax_ctl dax = {
 +		.sector = sector,
 +		.size = size,
 +	};
 +	void *ret;
++=======
+ 	unsigned long vaddr = vmf->address;
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  	void *entry = *entryp;
+ 	void *ret, *kaddr;
+ 	pgoff_t pgoff;
+ 	int id, rc;
+ 	pfn_t pfn;
  
- 	if (dax_map_atomic(bdev, &dax) < 0)
- 		return PTR_ERR(dax.addr);
- 	dax_unmap_atomic(bdev, &dax);
+ 	rc = bdev_dax_pgoff(bdev, sector, size, &pgoff);
+ 	if (rc)
+ 		return rc;
  
++<<<<<<< HEAD
 +	ret = dax_insert_mapping_entry(mapping, vmf, entry, dax.sector);
++=======
+ 	id = dax_read_lock();
+ 	rc = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
+ 	if (rc < 0) {
+ 		dax_read_unlock(id);
+ 		return rc;
+ 	}
+ 	dax_read_unlock(id);
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, entry, sector, 0);
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  	if (IS_ERR(ret))
  		return PTR_ERR(ret);
  	*entryp = ret;
  
- 	return vm_insert_mixed(vma, vaddr, dax.pfn);
+ 	return vm_insert_mixed(vma, vaddr, pfn);
  }
  
 +/**
 + * dax_fault - handle a page fault on a DAX file
 + * @vma: The virtual memory area where the fault occurred
 + * @vmf: The description of the fault
 + * @get_block: The filesystem method used to translate file offsets to blocks
 + *
 + * When a page fault occurs, filesystems may call this helper in their
 + * fault handler for DAX files. dax_fault() assumes the caller has done all
 + * the necessary locking for the page fault to proceed successfully.
 + */
 +int dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf,
 +			get_block_t get_block)
 +{
 +	struct file *file = vma->vm_file;
 +	struct address_space *mapping = file->f_mapping;
 +	struct inode *inode = mapping->host;
 +	void *entry;
 +	struct buffer_head bh;
 +	unsigned long vaddr = (unsigned long)vmf->virtual_address;
 +	unsigned blkbits = inode->i_blkbits;
 +	sector_t block;
 +	pgoff_t size;
 +	int error;
 +	int major = 0;
 +
 +	/*
 +	 * Check whether offset isn't beyond end of file now. Caller is supposed
 +	 * to hold locks serializing us with truncate / punch hole so this is
 +	 * a reliable test.
 +	 */
 +	size = (i_size_read(inode) + PAGE_SIZE - 1) >> PAGE_SHIFT;
 +	if (vmf->pgoff >= size)
 +		return VM_FAULT_SIGBUS;
 +
 +	memset(&bh, 0, sizeof(bh));
 +	block = (sector_t)vmf->pgoff << (PAGE_SHIFT - blkbits);
 +	bh.b_bdev = inode->i_sb->s_bdev;
 +	bh.b_size = PAGE_SIZE;
 +
 +	entry = grab_mapping_entry(mapping, vmf->pgoff);
 +	if (IS_ERR(entry)) {
 +		error = PTR_ERR(entry);
 +		goto out;
 +	}
 +
 +	error = get_block(inode, block, &bh, 0);
 +	if (!error && (bh.b_size < PAGE_SIZE))
 +		error = -EIO;		/* fs corruption? */
 +	if (error)
 +		goto unlock_entry;
 +
 +	if (vmf->cow_page) {
 +		struct page *new_page = vmf->cow_page;
 +		if (buffer_written(&bh))
 +			error = copy_user_dax(bh.b_bdev, to_sector(&bh, inode),
 +					bh.b_size, new_page, vaddr);
 +		else
 +			clear_user_highpage(new_page, vaddr);
 +		if (error)
 +			goto unlock_entry;
 +		if (!radix_tree_exceptional_entry(entry)) {
 +			vmf->page = entry;
 +			return VM_FAULT_LOCKED;
 +		}
 +		vmf->entry = entry;
 +		return VM_FAULT_DAX_LOCKED;
 +	}
 +
 +	if (!buffer_mapped(&bh)) {
 +		if (vmf->flags & FAULT_FLAG_WRITE) {
 +			error = get_block(inode, block, &bh, 1);
 +			count_vm_event(PGMAJFAULT);
 +			mem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);
 +			major = VM_FAULT_MAJOR;
 +			if (!error && (bh.b_size < PAGE_SIZE))
 +				error = -EIO;
 +			if (error)
 +				goto unlock_entry;
 +		} else {
 +			return dax_load_hole(mapping, entry, vmf);
 +		}
 +	}
 +
 +	/* Filesystem should not return unwritten buffers to us! */
 +	WARN_ON_ONCE(buffer_unwritten(&bh) || buffer_new(&bh));
 +	error = dax_insert_mapping(mapping, bh.b_bdev, to_sector(&bh, inode),
 +			bh.b_size, &entry, vma, vmf);
 + unlock_entry:
 +	put_locked_mapping_entry(mapping, vmf->pgoff, entry);
 + out:
 +	if (error == -ENOMEM)
 +		return VM_FAULT_OOM | major;
 +	/* -EBUSY is fine, somebody else faulted on the same PTE */
 +	if ((error < 0) && (error != -EBUSY))
 +		return VM_FAULT_SIGBUS | major;
 +	return VM_FAULT_NOPAGE | major;
 +}
 +EXPORT_SYMBOL_GPL(dax_fault);
 +
  /**
   * dax_pfn_mkwrite - handle first write to DAX page
 + * @vma: The virtual memory area where the fault occurred
   * @vmf: The description of the fault
   */
 -int dax_pfn_mkwrite(struct vm_fault *vmf)
 +int dax_pfn_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
  {
 -	struct file *file = vmf->vma->vm_file;
 +	struct file *file = vma->vm_file;
  	struct address_space *mapping = file->f_mapping;
 -	void *entry, **slot;
 +	void *entry;
  	pgoff_t index = vmf->pgoff;
  
  	spin_lock_irq(&mapping->tree_lock);
@@@ -962,83 -956,529 +1060,563 @@@ static bool dax_range_is_aligned(struc
  	return true;
  }
  
- int __dax_zero_page_range(struct block_device *bdev, sector_t sector,
- 		unsigned int offset, unsigned int length)
+ int __dax_zero_page_range(struct block_device *bdev,
+ 		struct dax_device *dax_dev, sector_t sector,
+ 		unsigned int offset, unsigned int size)
  {
++<<<<<<< HEAD
 +	struct blk_dax_ctl dax = {
 +		.sector		= sector,
 +		.size		= PAGE_CACHE_SIZE,
 +	};
 +
 +	if (dax_map_atomic(bdev, &dax) < 0)
 +		return PTR_ERR(dax.addr);
 +	clear_pmem(dax.addr + offset, length);
 +	dax_unmap_atomic(bdev, &dax);
 +	if (dax_range_is_aligned(bdev, offset, length)) {
 +		sector_t start_sector = dax.sector + (offset >> 9);
 +
 +		return blkdev_issue_zeroout(bdev, start_sector,
 +				length >> 9, GFP_NOFS);
++=======
+ 	if (dax_range_is_aligned(bdev, offset, size)) {
+ 		sector_t start_sector = sector + (offset >> 9);
+ 
+ 		return blkdev_issue_zeroout(bdev, start_sector,
+ 				size >> 9, GFP_NOFS, true);
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  	} else {
- 		if (dax_map_atomic(bdev, &dax) < 0)
- 			return PTR_ERR(dax.addr);
- 		clear_pmem(dax.addr + offset, length);
- 		dax_unmap_atomic(bdev, &dax);
+ 		pgoff_t pgoff;
+ 		long rc, id;
+ 		void *kaddr;
+ 		pfn_t pfn;
+ 
+ 		rc = bdev_dax_pgoff(bdev, sector, size, &pgoff);
+ 		if (rc)
+ 			return rc;
+ 
+ 		id = dax_read_lock();
+ 		rc = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr,
+ 				&pfn);
+ 		if (rc < 0) {
+ 			dax_read_unlock(id);
+ 			return rc;
+ 		}
+ 		clear_pmem(kaddr + offset, size);
+ 		dax_read_unlock(id);
  	}
  	return 0;
  }
  EXPORT_SYMBOL_GPL(__dax_zero_page_range);
  
 -static sector_t dax_iomap_sector(struct iomap *iomap, loff_t pos)
 +/**
 + * dax_zero_page_range - zero a range within a page of a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @length: The number of bytes to zero
 + * @get_block: The filesystem method used to translate file offsets to blocks
 + *
 + * This function can be called by a filesystem when it is zeroing part of a
 + * page in a DAX file.  This is intended for hole-punch operations.  If
 + * you are truncating a file, the helper function dax_truncate_page() may be
 + * more convenient.
 + */
 +int dax_zero_page_range(struct inode *inode, loff_t from, unsigned length,
 +							get_block_t get_block)
  {
++<<<<<<< HEAD
 +	struct buffer_head bh;
 +	pgoff_t index = from >> PAGE_CACHE_SHIFT;
 +	unsigned offset = from & (PAGE_CACHE_SIZE-1);
 +	int err;
 +
 +	/* Block boundary? Nothing to do */
 +	if (!length)
 +		return 0;
 +	if (WARN_ON_ONCE((offset + length) > PAGE_CACHE_SIZE))
 +		return -EINVAL;
 +
 +	memset(&bh, 0, sizeof(bh));
 +	bh.b_bdev = inode->i_sb->s_bdev;
 +	bh.b_size = PAGE_CACHE_SIZE;
 +	err = get_block(inode, index, &bh, 0);
 +	if (err < 0 || !buffer_written(&bh))
 +		return err;
 +
 +	return __dax_zero_page_range(bh.b_bdev, to_sector(&bh, inode),
 +			offset, length);
++=======
+ 	return iomap->blkno + (((pos & PAGE_MASK) - iomap->offset) >> 9);
+ }
+ 
+ static loff_t
+ dax_iomap_actor(struct inode *inode, loff_t pos, loff_t length, void *data,
+ 		struct iomap *iomap)
+ {
+ 	struct block_device *bdev = iomap->bdev;
+ 	struct dax_device *dax_dev = iomap->dax_dev;
+ 	struct iov_iter *iter = data;
+ 	loff_t end = pos + length, done = 0;
+ 	ssize_t ret = 0;
+ 	int id;
+ 
+ 	if (iov_iter_rw(iter) == READ) {
+ 		end = min(end, i_size_read(inode));
+ 		if (pos >= end)
+ 			return 0;
+ 
+ 		if (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)
+ 			return iov_iter_zero(min(length, end - pos), iter);
+ 	}
+ 
+ 	if (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED))
+ 		return -EIO;
+ 
+ 	/*
+ 	 * Write can allocate block for an area which has a hole page mapped
+ 	 * into page tables. We have to tear down these mappings so that data
+ 	 * written by write(2) is visible in mmap.
+ 	 */
+ 	if ((iomap->flags & IOMAP_F_NEW) && inode->i_mapping->nrpages) {
+ 		invalidate_inode_pages2_range(inode->i_mapping,
+ 					      pos >> PAGE_SHIFT,
+ 					      (end - 1) >> PAGE_SHIFT);
+ 	}
+ 
+ 	id = dax_read_lock();
+ 	while (pos < end) {
+ 		unsigned offset = pos & (PAGE_SIZE - 1);
+ 		const size_t size = ALIGN(length + offset, PAGE_SIZE);
+ 		const sector_t sector = dax_iomap_sector(iomap, pos);
+ 		ssize_t map_len;
+ 		pgoff_t pgoff;
+ 		void *kaddr;
+ 		pfn_t pfn;
+ 
+ 		if (fatal_signal_pending(current)) {
+ 			ret = -EINTR;
+ 			break;
+ 		}
+ 
+ 		ret = bdev_dax_pgoff(bdev, sector, size, &pgoff);
+ 		if (ret)
+ 			break;
+ 
+ 		map_len = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size),
+ 				&kaddr, &pfn);
+ 		if (map_len < 0) {
+ 			ret = map_len;
+ 			break;
+ 		}
+ 
+ 		map_len = PFN_PHYS(map_len);
+ 		kaddr += offset;
+ 		map_len -= offset;
+ 		if (map_len > end - pos)
+ 			map_len = end - pos;
+ 
+ 		if (iov_iter_rw(iter) == WRITE)
+ 			map_len = copy_from_iter_pmem(kaddr, map_len, iter);
+ 		else
+ 			map_len = copy_to_iter(kaddr, map_len, iter);
+ 		if (map_len <= 0) {
+ 			ret = map_len ? map_len : -EFAULT;
+ 			break;
+ 		}
+ 
+ 		pos += map_len;
+ 		length -= map_len;
+ 		done += map_len;
+ 	}
+ 	dax_read_unlock(id);
+ 
+ 	return done ? done : ret;
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  }
 +EXPORT_SYMBOL_GPL(dax_zero_page_range);
  
  /**
 - * dax_iomap_rw - Perform I/O to a DAX file
 - * @iocb:	The control block for this I/O
 - * @iter:	The addresses to do I/O from or to
 - * @ops:	iomap ops passed from the file system
 + * dax_truncate_page - handle a partial page being truncated in a DAX file
 + * @inode: The file being truncated
 + * @from: The file offset that is being truncated to
 + * @get_block: The filesystem method used to translate file offsets to blocks
   *
 - * This function performs read and write operations to directly mapped
 - * persistent memory.  The callers needs to take care of read/write exclusion
 - * and evicting any page cache pages in the region under I/O.
 + * Similar to block_truncate_page(), this function can be called by a
 + * filesystem when it is truncating a DAX file to handle the partial page.
   */
 -ssize_t
 -dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
 -		const struct iomap_ops *ops)
 +int dax_truncate_page(struct inode *inode, loff_t from, get_block_t get_block)
  {
 -	struct address_space *mapping = iocb->ki_filp->f_mapping;
 -	struct inode *inode = mapping->host;
 -	loff_t pos = iocb->ki_pos, ret = 0, done = 0;
 -	unsigned flags = 0;
 -
 -	if (iov_iter_rw(iter) == WRITE) {
 -		lockdep_assert_held_exclusive(&inode->i_rwsem);
 -		flags |= IOMAP_WRITE;
 -	} else {
 -		lockdep_assert_held(&inode->i_rwsem);
 -	}
 -
 -	while (iov_iter_count(iter)) {
 -		ret = iomap_apply(inode, pos, iov_iter_count(iter), flags, ops,
 -				iter, dax_iomap_actor);
 -		if (ret <= 0)
 -			break;
 -		pos += ret;
 -		done += ret;
 -	}
 -
 -	iocb->ki_pos += done;
 -	return done ? done : ret;
 +	unsigned length = PAGE_CACHE_ALIGN(from) - from;
 +	return dax_zero_page_range(inode, from, length, get_block);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(dax_truncate_page);
++=======
+ EXPORT_SYMBOL_GPL(dax_iomap_rw);
+ 
+ static int dax_fault_return(int error)
+ {
+ 	if (error == 0)
+ 		return VM_FAULT_NOPAGE;
+ 	if (error == -ENOMEM)
+ 		return VM_FAULT_OOM;
+ 	return VM_FAULT_SIGBUS;
+ }
+ 
+ static int dax_iomap_pte_fault(struct vm_fault *vmf,
+ 			       const struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	struct inode *inode = mapping->host;
+ 	unsigned long vaddr = vmf->address;
+ 	loff_t pos = (loff_t)vmf->pgoff << PAGE_SHIFT;
+ 	sector_t sector;
+ 	struct iomap iomap = { 0 };
+ 	unsigned flags = IOMAP_FAULT;
+ 	int error, major = 0;
+ 	int vmf_ret = 0;
+ 	void *entry;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is supposed
+ 	 * to hold locks serializing us with truncate / punch hole so this is
+ 	 * a reliable test.
+ 	 */
+ 	if (pos >= i_size_read(inode))
+ 		return VM_FAULT_SIGBUS;
+ 
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)
+ 		flags |= IOMAP_WRITE;
+ 
+ 	/*
+ 	 * Note that we don't bother to use iomap_apply here: DAX required
+ 	 * the file system block size to be equal the page size, which means
+ 	 * that we never have to deal with more than a single extent here.
+ 	 */
+ 	error = ops->iomap_begin(inode, pos, PAGE_SIZE, flags, &iomap);
+ 	if (error)
+ 		return dax_fault_return(error);
+ 	if (WARN_ON_ONCE(iomap.offset + iomap.length < pos + PAGE_SIZE)) {
+ 		vmf_ret = dax_fault_return(-EIO);	/* fs corruption? */
+ 		goto finish_iomap;
+ 	}
+ 
+ 	entry = grab_mapping_entry(mapping, vmf->pgoff, 0);
+ 	if (IS_ERR(entry)) {
+ 		vmf_ret = dax_fault_return(PTR_ERR(entry));
+ 		goto finish_iomap;
+ 	}
+ 
+ 	sector = dax_iomap_sector(&iomap, pos);
+ 
+ 	if (vmf->cow_page) {
+ 		switch (iomap.type) {
+ 		case IOMAP_HOLE:
+ 		case IOMAP_UNWRITTEN:
+ 			clear_user_highpage(vmf->cow_page, vaddr);
+ 			break;
+ 		case IOMAP_MAPPED:
+ 			error = copy_user_dax(iomap.bdev, iomap.dax_dev,
+ 					sector, PAGE_SIZE, vmf->cow_page, vaddr);
+ 			break;
+ 		default:
+ 			WARN_ON_ONCE(1);
+ 			error = -EIO;
+ 			break;
+ 		}
+ 
+ 		if (error)
+ 			goto error_unlock_entry;
+ 
+ 		__SetPageUptodate(vmf->cow_page);
+ 		vmf_ret = finish_fault(vmf);
+ 		if (!vmf_ret)
+ 			vmf_ret = VM_FAULT_DONE_COW;
+ 		goto unlock_entry;
+ 	}
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		if (iomap.flags & IOMAP_F_NEW) {
+ 			count_vm_event(PGMAJFAULT);
+ 			mem_cgroup_count_vm_event(vmf->vma->vm_mm, PGMAJFAULT);
+ 			major = VM_FAULT_MAJOR;
+ 		}
+ 		error = dax_insert_mapping(mapping, iomap.bdev, iomap.dax_dev,
+ 				sector, PAGE_SIZE, &entry, vmf->vma, vmf);
+ 		/* -EBUSY is fine, somebody else faulted on the same PTE */
+ 		if (error == -EBUSY)
+ 			error = 0;
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (!(vmf->flags & FAULT_FLAG_WRITE)) {
+ 			vmf_ret = dax_load_hole(mapping, &entry, vmf);
+ 			goto unlock_entry;
+ 		}
+ 		/*FALLTHRU*/
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		error = -EIO;
+ 		break;
+ 	}
+ 
+  error_unlock_entry:
+ 	vmf_ret = dax_fault_return(error) | major;
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, vmf->pgoff, entry);
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		int copied = PAGE_SIZE;
+ 
+ 		if (vmf_ret & VM_FAULT_ERROR)
+ 			copied = 0;
+ 		/*
+ 		 * The fault is done by now and there's no way back (other
+ 		 * thread may be already happily using PTE we have installed).
+ 		 * Just ignore error from ->iomap_end since we cannot do much
+ 		 * with it.
+ 		 */
+ 		ops->iomap_end(inode, pos, PAGE_SIZE, copied, flags, &iomap);
+ 	}
+ 	return vmf_ret;
+ }
+ 
+ #ifdef CONFIG_FS_DAX_PMD
+ /*
+  * The 'colour' (ie low bits) within a PMD of a page offset.  This comes up
+  * more often than one might expect in the below functions.
+  */
+ #define PG_PMD_COLOUR	((PMD_SIZE >> PAGE_SHIFT) - 1)
+ 
+ static int dax_pmd_insert_mapping(struct vm_fault *vmf, struct iomap *iomap,
+ 		loff_t pos, void **entryp)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	const sector_t sector = dax_iomap_sector(iomap, pos);
+ 	struct dax_device *dax_dev = iomap->dax_dev;
+ 	struct block_device *bdev = iomap->bdev;
+ 	struct inode *inode = mapping->host;
+ 	const size_t size = PMD_SIZE;
+ 	void *ret = NULL, *kaddr;
+ 	long length = 0;
+ 	pgoff_t pgoff;
+ 	pfn_t pfn;
+ 	int id;
+ 
+ 	if (bdev_dax_pgoff(bdev, sector, size, &pgoff) != 0)
+ 		goto fallback;
+ 
+ 	id = dax_read_lock();
+ 	length = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
+ 	if (length < 0)
+ 		goto unlock_fallback;
+ 	length = PFN_PHYS(length);
+ 
+ 	if (length < size)
+ 		goto unlock_fallback;
+ 	if (pfn_t_to_pfn(pfn) & PG_PMD_COLOUR)
+ 		goto unlock_fallback;
+ 	if (!pfn_t_devmap(pfn))
+ 		goto unlock_fallback;
+ 	dax_read_unlock(id);
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, sector,
+ 			RADIX_DAX_PMD);
+ 	if (IS_ERR(ret))
+ 		goto fallback;
+ 	*entryp = ret;
+ 
+ 	trace_dax_pmd_insert_mapping(inode, vmf, length, pfn, ret);
+ 	return vmf_insert_pfn_pmd(vmf->vma, vmf->address, vmf->pmd,
+ 			pfn, vmf->flags & FAULT_FLAG_WRITE);
+ 
+ unlock_fallback:
+ 	dax_read_unlock(id);
+ fallback:
+ 	trace_dax_pmd_insert_mapping_fallback(inode, vmf, length, pfn, ret);
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int dax_pmd_load_hole(struct vm_fault *vmf, struct iomap *iomap,
+ 		void **entryp)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = vmf->address & PMD_MASK;
+ 	struct inode *inode = mapping->host;
+ 	struct page *zero_page;
+ 	void *ret = NULL;
+ 	spinlock_t *ptl;
+ 	pmd_t pmd_entry;
+ 
+ 	zero_page = mm_get_huge_zero_page(vmf->vma->vm_mm);
+ 
+ 	if (unlikely(!zero_page))
+ 		goto fallback;
+ 
+ 	ret = dax_insert_mapping_entry(mapping, vmf, *entryp, 0,
+ 			RADIX_DAX_PMD | RADIX_DAX_HZP);
+ 	if (IS_ERR(ret))
+ 		goto fallback;
+ 	*entryp = ret;
+ 
+ 	ptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);
+ 	if (!pmd_none(*(vmf->pmd))) {
+ 		spin_unlock(ptl);
+ 		goto fallback;
+ 	}
+ 
+ 	pmd_entry = mk_pmd(zero_page, vmf->vma->vm_page_prot);
+ 	pmd_entry = pmd_mkhuge(pmd_entry);
+ 	set_pmd_at(vmf->vma->vm_mm, pmd_addr, vmf->pmd, pmd_entry);
+ 	spin_unlock(ptl);
+ 	trace_dax_pmd_load_hole(inode, vmf, zero_page, ret);
+ 	return VM_FAULT_NOPAGE;
+ 
+ fallback:
+ 	trace_dax_pmd_load_hole_fallback(inode, vmf, zero_page, ret);
+ 	return VM_FAULT_FALLBACK;
+ }
+ 
+ static int dax_iomap_pmd_fault(struct vm_fault *vmf,
+ 			       const struct iomap_ops *ops)
+ {
+ 	struct vm_area_struct *vma = vmf->vma;
+ 	struct address_space *mapping = vma->vm_file->f_mapping;
+ 	unsigned long pmd_addr = vmf->address & PMD_MASK;
+ 	bool write = vmf->flags & FAULT_FLAG_WRITE;
+ 	unsigned int iomap_flags = (write ? IOMAP_WRITE : 0) | IOMAP_FAULT;
+ 	struct inode *inode = mapping->host;
+ 	int result = VM_FAULT_FALLBACK;
+ 	struct iomap iomap = { 0 };
+ 	pgoff_t max_pgoff, pgoff;
+ 	void *entry;
+ 	loff_t pos;
+ 	int error;
+ 
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is
+ 	 * supposed to hold locks serializing us with truncate / punch hole so
+ 	 * this is a reliable test.
+ 	 */
+ 	pgoff = linear_page_index(vma, pmd_addr);
+ 	max_pgoff = (i_size_read(inode) - 1) >> PAGE_SHIFT;
+ 
+ 	trace_dax_pmd_fault(inode, vmf, max_pgoff, 0);
+ 
+ 	/* Fall back to PTEs if we're going to COW */
+ 	if (write && !(vma->vm_flags & VM_SHARED))
+ 		goto fallback;
+ 
+ 	/* If the PMD would extend outside the VMA */
+ 	if (pmd_addr < vma->vm_start)
+ 		goto fallback;
+ 	if ((pmd_addr + PMD_SIZE) > vma->vm_end)
+ 		goto fallback;
+ 
+ 	if (pgoff > max_pgoff) {
+ 		result = VM_FAULT_SIGBUS;
+ 		goto out;
+ 	}
+ 
+ 	/* If the PMD would extend beyond the file size */
+ 	if ((pgoff | PG_PMD_COLOUR) > max_pgoff)
+ 		goto fallback;
+ 
+ 	/*
+ 	 * Note that we don't use iomap_apply here.  We aren't doing I/O, only
+ 	 * setting up a mapping, so really we're using iomap_begin() as a way
+ 	 * to look up our filesystem block.
+ 	 */
+ 	pos = (loff_t)pgoff << PAGE_SHIFT;
+ 	error = ops->iomap_begin(inode, pos, PMD_SIZE, iomap_flags, &iomap);
+ 	if (error)
+ 		goto fallback;
+ 
+ 	if (iomap.offset + iomap.length < pos + PMD_SIZE)
+ 		goto finish_iomap;
+ 
+ 	/*
+ 	 * grab_mapping_entry() will make sure we get a 2M empty entry, a DAX
+ 	 * PMD or a HZP entry.  If it can't (because a 4k page is already in
+ 	 * the tree, for instance), it will return -EEXIST and we just fall
+ 	 * back to 4k entries.
+ 	 */
+ 	entry = grab_mapping_entry(mapping, pgoff, RADIX_DAX_PMD);
+ 	if (IS_ERR(entry))
+ 		goto finish_iomap;
+ 
+ 	switch (iomap.type) {
+ 	case IOMAP_MAPPED:
+ 		result = dax_pmd_insert_mapping(vmf, &iomap, pos, &entry);
+ 		break;
+ 	case IOMAP_UNWRITTEN:
+ 	case IOMAP_HOLE:
+ 		if (WARN_ON_ONCE(write))
+ 			goto unlock_entry;
+ 		result = dax_pmd_load_hole(vmf, &iomap, &entry);
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		break;
+ 	}
+ 
+  unlock_entry:
+ 	put_locked_mapping_entry(mapping, pgoff, entry);
+  finish_iomap:
+ 	if (ops->iomap_end) {
+ 		int copied = PMD_SIZE;
+ 
+ 		if (result == VM_FAULT_FALLBACK)
+ 			copied = 0;
+ 		/*
+ 		 * The fault is done by now and there's no way back (other
+ 		 * thread may be already happily using PMD we have installed).
+ 		 * Just ignore error from ->iomap_end since we cannot do much
+ 		 * with it.
+ 		 */
+ 		ops->iomap_end(inode, pos, PMD_SIZE, copied, iomap_flags,
+ 				&iomap);
+ 	}
+  fallback:
+ 	if (result == VM_FAULT_FALLBACK) {
+ 		split_huge_pmd(vma, vmf->pmd, vmf->address);
+ 		count_vm_event(THP_FAULT_FALLBACK);
+ 	}
+ out:
+ 	trace_dax_pmd_fault_done(inode, vmf, max_pgoff, result);
+ 	return result;
+ }
+ #else
+ static int dax_iomap_pmd_fault(struct vm_fault *vmf,
+ 			       const struct iomap_ops *ops)
+ {
+ 	return VM_FAULT_FALLBACK;
+ }
+ #endif /* CONFIG_FS_DAX_PMD */
+ 
+ /**
+  * dax_iomap_fault - handle a page fault on a DAX file
+  * @vmf: The description of the fault
+  * @ops: iomap ops passed from the file system
+  *
+  * When a page fault occurs, filesystems may call this helper in
+  * their fault handler for DAX files. dax_iomap_fault() assumes the caller
+  * has done all the necessary locking for page fault to proceed
+  * successfully.
+  */
+ int dax_iomap_fault(struct vm_fault *vmf, enum page_entry_size pe_size,
+ 		    const struct iomap_ops *ops)
+ {
+ 	switch (pe_size) {
+ 	case PE_SIZE_PTE:
+ 		return dax_iomap_pte_fault(vmf, ops);
+ 	case PE_SIZE_PMD:
+ 		return dax_iomap_pmd_fault(vmf, ops);
+ 	default:
+ 		return VM_FAULT_FALLBACK;
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_fault);
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
diff --cc include/linux/dax.h
index 8937c7aed5cb,d3158e74a59e..000000000000
--- a/include/linux/dax.h
+++ b/include/linux/dax.h
@@@ -33,24 -70,13 +33,30 @@@ void dax_wake_mapping_entry_waiter(stru
  		pgoff_t index, void *entry, bool wake_all);
  
  #ifdef CONFIG_FS_DAX
++<<<<<<< HEAD
 +struct page *read_dax_sector(struct block_device *bdev, sector_t n);
 +void dax_unlock_mapping_entry(struct address_space *mapping, pgoff_t index);
 +int __dax_zero_page_range(struct block_device *bdev, sector_t sector,
++=======
+ int __dax_zero_page_range(struct block_device *bdev,
+ 		struct dax_device *dax_dev, sector_t sector,
++>>>>>>> cccbce671582 (filesystem-dax: convert to dax_direct_access())
  		unsigned int offset, unsigned int length);
  #else
 +static inline struct page *read_dax_sector(struct block_device *bdev,
 +		sector_t n)
 +{
 +	return ERR_PTR(-ENXIO);
 +}
 +/* Shouldn't ever be called when dax is disabled. */
 +static inline void dax_unlock_mapping_entry(struct address_space *mapping,
 +					    pgoff_t index)
 +{
 +	BUG();
 +}
  static inline int __dax_zero_page_range(struct block_device *bdev,
- 		sector_t sector, unsigned int offset, unsigned int length)
+ 		struct dax_device *dax_dev, sector_t sector,
+ 		unsigned int offset, unsigned int length)
  {
  	return -ENXIO;
  }
* Unmerged path fs/dax.c
diff --git a/fs/iomap.c b/fs/iomap.c
index f6bfcdf64146..4624db523e24 100644
--- a/fs/iomap.c
+++ b/fs/iomap.c
@@ -360,7 +360,8 @@ static int iomap_dax_zero(loff_t pos, unsigned offset, unsigned bytes,
 	sector_t sector = iomap->blkno +
 		(((pos & ~(PAGE_SIZE - 1)) - iomap->offset) >> 9);
 
-	return __dax_zero_page_range(iomap->bdev, sector, offset, bytes);
+	return __dax_zero_page_range(iomap->bdev, iomap->dax_dev, sector,
+			offset, bytes);
 }
 
 static loff_t
* Unmerged path include/linux/dax.h
