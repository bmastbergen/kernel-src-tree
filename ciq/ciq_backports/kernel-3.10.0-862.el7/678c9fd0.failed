dax: add tracepoints to dax_load_hole()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ross Zwisler <ross.zwisler@linux.intel.com>
commit 678c9fd0430a1431bd9901c76f41e04fcb3eac87
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/678c9fd0.failed

Add tracepoints to dax_load_hole(), following the same logging conventions
as the rest of DAX.

Here is the logging generated by a PTE read from a hole:

  read-1075  [002] ....
    62.362108: dax_pte_fault: dev 259:0 ino 0x1003 shared ALLOW_RETRY|KILLABLE|USER address 0x10480000 pgoff 0x280

  read-1075  [002] ....
    62.362140: dax_load_hole: dev 259:0 ino 0x1003 shared ALLOW_RETRY|KILLABLE|USER address 0x10480000 pgoff 0x280 NOPAGE

  read-1075  [002] ....
    62.362141: dax_pte_fault_done: dev 259:0 ino 0x1003 shared ALLOW_RETRY|KILLABLE|USER address 0x10480000 pgoff 0x280 NOPAGE

Link: http://lkml.kernel.org/r/20170221195116.13278-4-ross.zwisler@linux.intel.com
	Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
	Reviewed-by: Jan Kara <jack@suse.cz>
	Cc: Alexander Viro <viro@zeniv.linux.org.uk>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Matthew Wilcox <mawilcox@microsoft.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 678c9fd0430a1431bd9901c76f41e04fcb3eac87)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
#	include/trace/events/fs_dax.h
diff --cc fs/dax.c
index 1dfecdfb6245,36fafff2e82f..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -568,43 -506,66 +568,72 @@@ int dax_delete_mapping_entry(struct add
   * otherwise it will simply fall out of the page cache under memory
   * pressure without ever having been dirtied.
   */
 -static int dax_load_hole(struct address_space *mapping, void **entry,
 +static int dax_load_hole(struct address_space *mapping, void *entry,
  			 struct vm_fault *vmf)
  {
+ 	struct inode *inode = mapping->host;
  	struct page *page;
 -	int ret;
  
  	/* Hole page already exists? Return it...  */
++<<<<<<< HEAD
 +	if (!radix_tree_exceptional_entry(entry)) {
 +		vmf->page = entry;
 +		return VM_FAULT_LOCKED;
++=======
+ 	if (!radix_tree_exceptional_entry(*entry)) {
+ 		page = *entry;
+ 		goto finish_fault;
++>>>>>>> 678c9fd0430a (dax: add tracepoints to dax_load_hole())
  	}
  
  	/* This will replace locked radix tree entry with a hole page */
  	page = find_or_create_page(mapping, vmf->pgoff,
++<<<<<<< HEAD
 +		mapping_gfp_mask(mapping) | __GFP_FS | __GFP_IO | __GFP_ZERO);
 +	if (!page) {
 +		put_locked_mapping_entry(mapping, vmf->pgoff, entry);
 +		return VM_FAULT_OOM;
 +	}
 +	vmf->page = page;
 +	return VM_FAULT_LOCKED;
++=======
+ 				   vmf->gfp_mask | __GFP_ZERO);
+ 	if (!page) {
+ 		ret = VM_FAULT_OOM;
+ 		goto out;
+ 	}
+ 
+ finish_fault:
+ 	vmf->page = page;
+ 	ret = finish_fault(vmf);
+ 	vmf->page = NULL;
+ 	*entry = page;
+ 	if (!ret) {
+ 		/* Grab reference for PTE that is now referencing the page */
+ 		get_page(page);
+ 		ret = VM_FAULT_NOPAGE;
+ 	}
+ out:
+ 	trace_dax_load_hole(inode, vmf, ret);
+ 	return ret;
++>>>>>>> 678c9fd0430a (dax: add tracepoints to dax_load_hole())
  }
  
 -static int copy_user_dax(struct block_device *bdev, struct dax_device *dax_dev,
 -		sector_t sector, size_t size, struct page *to,
 -		unsigned long vaddr)
 +static int copy_user_dax(struct block_device *bdev, sector_t sector, size_t size,
 +		struct page *to, unsigned long vaddr)
  {
 -	void *vto, *kaddr;
 -	pgoff_t pgoff;
 -	pfn_t pfn;
 -	long rc;
 -	int id;
 -
 -	rc = bdev_dax_pgoff(bdev, sector, size, &pgoff);
 -	if (rc)
 -		return rc;
 -
 -	id = dax_read_lock();
 -	rc = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size), &kaddr, &pfn);
 -	if (rc < 0) {
 -		dax_read_unlock(id);
 -		return rc;
 -	}
 +	struct blk_dax_ctl dax = {
 +		.sector = sector,
 +		.size = size,
 +	};
 +	void *vto;
 +
 +	if (dax_map_atomic(bdev, &dax) < 0)
 +		return PTR_ERR(dax.addr);
  	vto = kmap_atomic(to);
 -	copy_user_page(vto, (void __force *)kaddr, vaddr, to);
 +	copy_user_page(vto, (void __force *)dax.addr, vaddr, to);
  	kunmap_atomic(vto);
 -	dax_read_unlock(id);
 +	dax_unmap_atomic(bdev, &dax);
  	return 0;
  }
  
* Unmerged path include/trace/events/fs_dax.h
* Unmerged path fs/dax.c
* Unmerged path include/trace/events/fs_dax.h
