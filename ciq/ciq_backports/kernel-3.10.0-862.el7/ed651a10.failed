ibmvnic: Updated reset handling

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Nathan Fontenot <nfont@linux.vnet.ibm.com>
commit ed651a10875f13135a5f59c1bae4d51b377b3925
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ed651a10.failed

The ibmvnic driver has multiple handlers for resetting the driver
depending on the reason the reset is needed (failover, lpm,
fatal erors,...). All of the reset handlers do essentially the same
thing, this patch moves this work to a common reset handler.

By doing this we also allow the driver to better handle situations
where we can get a reset while handling a reset.

The updated reset handling works by adding a reset work item to the
list of resets and then scheduling work to perform the reset. This
step is necessary because we can receive a reset in interrupt context
and we want to handle the reset out of interrupt context.

	Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit ed651a10875f13135a5f59c1bae4d51b377b3925)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/ibm/ibmvnic.c
#	drivers/net/ethernet/ibm/ibmvnic.h
diff --cc drivers/net/ethernet/ibm/ibmvnic.c
index 8b65bfc1f714,a7c7a94c9d63..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.c
+++ b/drivers/net/ethernet/ibm/ibmvnic.c
@@@ -203,45 -191,13 +203,53 @@@ static void free_long_term_buff(struct 
  {
  	struct device *dev = &adapter->vdev->dev;
  
++<<<<<<< HEAD
 +	dma_free_coherent(dev, ltb->size, ltb->buff, ltb->addr);
 +	if (!adapter->failover)
++=======
+ 	if (!ltb->buff)
+ 		return;
+ 
+ 	if (adapter->reset_reason != VNIC_RESET_FAILOVER &&
+ 	    adapter->reset_reason != VNIC_RESET_MOBILITY)
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  		send_request_unmap(adapter, ltb->map_id);
 -	dma_free_coherent(dev, ltb->size, ltb->buff, ltb->addr);
 +}
 +
 +static int alloc_rx_pool(struct ibmvnic_adapter *adapter,
 +			 struct ibmvnic_rx_pool *pool)
 +{
 +	struct device *dev = &adapter->vdev->dev;
 +	int i;
 +
 +	pool->free_map = kcalloc(pool->size, sizeof(int), GFP_KERNEL);
 +	if (!pool->free_map)
 +		return -ENOMEM;
 +
 +	pool->rx_buff = kcalloc(pool->size, sizeof(struct ibmvnic_rx_buff),
 +				GFP_KERNEL);
 +
 +	if (!pool->rx_buff) {
 +		dev_err(dev, "Couldn't alloc rx buffers\n");
 +		kfree(pool->free_map);
 +		return -ENOMEM;
 +	}
 +
 +	if (alloc_long_term_buff(adapter, &pool->long_term_buff,
 +				 pool->size * pool->buff_size)) {
 +		kfree(pool->free_map);
 +		kfree(pool->rx_buff);
 +		return -ENOMEM;
 +	}
 +
 +	for (i = 0; i < pool->size; ++i)
 +		pool->free_map[i] = i;
 +
 +	atomic_set(&pool->available, 0);
 +	pool->next_alloc = 0;
 +	pool->next_free = 0;
 +
 +	return 0;
  }
  
  static void replenish_rx_pool(struct ibmvnic_adapter *adapter,
@@@ -442,74 -497,241 +447,283 @@@ static int ibmvnic_open(struct net_devi
  		tx_pool->consumer_index = 0;
  		tx_pool->producer_index = 0;
  	}
 -
 -	return 0;
 -}
 -
 -static void release_error_buffers(struct ibmvnic_adapter *adapter)
 -{
 -	struct device *dev = &adapter->vdev->dev;
 -	struct ibmvnic_error_buff *error_buff, *tmp;
 -	unsigned long flags;
 -
 -	spin_lock_irqsave(&adapter->error_list_lock, flags);
 -	list_for_each_entry_safe(error_buff, tmp, &adapter->errors, list) {
 -		list_del(&error_buff->list);
 -		dma_unmap_single(dev, error_buff->dma, error_buff->len,
 -				 DMA_FROM_DEVICE);
 -		kfree(error_buff->buff);
 -		kfree(error_buff);
 +	adapter->bounce_buffer_size =
 +	    (netdev->mtu + ETH_HLEN - 1) / PAGE_SIZE + 1;
 +	adapter->bounce_buffer = kmalloc(adapter->bounce_buffer_size,
 +					 GFP_KERNEL);
 +	if (!adapter->bounce_buffer)
 +		goto bounce_alloc_failed;
 +
 +	adapter->bounce_buffer_dma = dma_map_single(dev, adapter->bounce_buffer,
 +						    adapter->bounce_buffer_size,
 +						    DMA_TO_DEVICE);
 +	if (dma_mapping_error(dev, adapter->bounce_buffer_dma)) {
 +		dev_err(dev, "Couldn't map tx bounce buffer\n");
 +		goto bounce_map_failed;
  	}
++<<<<<<< HEAD
++=======
+ 	spin_unlock_irqrestore(&adapter->error_list_lock, flags);
+ }
+ 
+ static int ibmvnic_login(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	unsigned long timeout = msecs_to_jiffies(30000);
+ 	struct device *dev = &adapter->vdev->dev;
+ 
+ 	do {
+ 		if (adapter->renegotiate) {
+ 			adapter->renegotiate = false;
+ 			release_sub_crqs(adapter);
+ 
+ 			reinit_completion(&adapter->init_done);
+ 			send_cap_queries(adapter);
+ 			if (!wait_for_completion_timeout(&adapter->init_done,
+ 							 timeout)) {
+ 				dev_err(dev, "Capabilities query timeout\n");
+ 				return -1;
+ 			}
+ 		}
+ 
+ 		reinit_completion(&adapter->init_done);
+ 		send_login(adapter);
+ 		if (!wait_for_completion_timeout(&adapter->init_done,
+ 						 timeout)) {
+ 			dev_err(dev, "Login timeout\n");
+ 			return -1;
+ 		}
+ 	} while (adapter->renegotiate);
+ 
+ 	return 0;
+ }
+ 
+ static void release_resources(struct ibmvnic_adapter *adapter)
+ {
+ 	release_tx_pools(adapter);
+ 	release_rx_pools(adapter);
+ 
+ 	release_stats_token(adapter);
+ 	release_error_buffers(adapter);
+ }
+ 
+ static int set_link_state(struct ibmvnic_adapter *adapter, u8 link_state)
+ {
+ 	struct net_device *netdev = adapter->netdev;
+ 	unsigned long timeout = msecs_to_jiffies(30000);
+ 	union ibmvnic_crq crq;
+ 	bool resend;
+ 	int rc;
+ 
+ 	netdev_err(netdev, "setting link state %d\n", link_state);
+ 	memset(&crq, 0, sizeof(crq));
+ 	crq.logical_link_state.first = IBMVNIC_CRQ_CMD;
+ 	crq.logical_link_state.cmd = LOGICAL_LINK_STATE;
+ 	crq.logical_link_state.link_state = link_state;
+ 
+ 	do {
+ 		resend = false;
+ 
+ 		reinit_completion(&adapter->init_done);
+ 		rc = ibmvnic_send_crq(adapter, &crq);
+ 		if (rc) {
+ 			netdev_err(netdev, "Failed to set link state\n");
+ 			return rc;
+ 		}
+ 
+ 		if (!wait_for_completion_timeout(&adapter->init_done,
+ 						 timeout)) {
+ 			netdev_err(netdev, "timeout setting link state\n");
+ 			return -1;
+ 		}
+ 
+ 		if (adapter->init_done_rc == 1) {
+ 			/* Partuial success, delay and re-send */
+ 			mdelay(1000);
+ 			resend = true;
+ 		}
+ 	} while (resend);
+ 
+ 	return 0;
+ }
+ 
+ static int set_real_num_queues(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	int rc;
+ 
+ 	rc = netif_set_real_num_tx_queues(netdev, adapter->req_tx_queues);
+ 	if (rc) {
+ 		netdev_err(netdev, "failed to set the number of tx queues\n");
+ 		return rc;
+ 	}
+ 
+ 	rc = netif_set_real_num_rx_queues(netdev, adapter->req_rx_queues);
+ 	if (rc)
+ 		netdev_err(netdev, "failed to set the number of rx queues\n");
+ 
+ 	return rc;
+ }
+ 
+ static int init_resources(struct ibmvnic_adapter *adapter)
+ {
+ 	struct net_device *netdev = adapter->netdev;
+ 	int i, rc;
+ 
+ 	rc = set_real_num_queues(netdev);
+ 	if (rc)
+ 		return rc;
+ 
+ 	rc = init_sub_crq_irqs(adapter);
+ 	if (rc) {
+ 		netdev_err(netdev, "failed to initialize sub crq irqs\n");
+ 		return -1;
+ 	}
+ 
+ 	rc = init_stats_token(adapter);
+ 	if (rc)
+ 		return rc;
+ 
+ 	adapter->map_id = 1;
+ 	adapter->napi = kcalloc(adapter->req_rx_queues,
+ 				sizeof(struct napi_struct), GFP_KERNEL);
+ 	if (!adapter->napi)
+ 		return -ENOMEM;
+ 
+ 	for (i = 0; i < adapter->req_rx_queues; i++) {
+ 		netif_napi_add(netdev, &adapter->napi[i], ibmvnic_poll,
+ 			       NAPI_POLL_WEIGHT);
+ 	}
+ 
+ 	send_map_query(adapter);
+ 
+ 	rc = init_rx_pools(netdev);
+ 	if (rc)
+ 		return rc;
+ 
+ 	rc = init_tx_pools(netdev);
+ 	return rc;
+ }
+ 
+ static int __ibmvnic_open(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	enum vnic_state prev_state = adapter->state;
+ 	int i, rc;
+ 
+ 	adapter->state = VNIC_OPENING;
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  	replenish_pools(adapter);
  
 -	for (i = 0; i < adapter->req_rx_queues; i++)
 -		napi_enable(&adapter->napi[i]);
 -
  	/* We're ready to receive frames, enable the sub-crq interrupts and
  	 * set the logical link state to up
  	 */
- 	for (i = 0; i < adapter->req_rx_queues; i++)
- 		enable_scrq_irq(adapter, adapter->rx_scrq[i]);
+ 	for (i = 0; i < adapter->req_rx_queues; i++) {
+ 		if (prev_state == VNIC_CLOSED)
+ 			enable_irq(adapter->rx_scrq[i]->irq);
+ 		else
+ 			enable_scrq_irq(adapter, adapter->rx_scrq[i]);
+ 	}
  
- 	for (i = 0; i < adapter->req_tx_queues; i++)
- 		enable_scrq_irq(adapter, adapter->tx_scrq[i]);
+ 	for (i = 0; i < adapter->req_tx_queues; i++) {
+ 		if (prev_state == VNIC_CLOSED)
+ 			enable_irq(adapter->tx_scrq[i]->irq);
+ 		else
+ 			enable_scrq_irq(adapter, adapter->tx_scrq[i]);
+ 	}
  
++<<<<<<< HEAD
 +	memset(&crq, 0, sizeof(crq));
 +	crq.logical_link_state.first = IBMVNIC_CRQ_CMD;
 +	crq.logical_link_state.cmd = LOGICAL_LINK_STATE;
 +	crq.logical_link_state.link_state = IBMVNIC_LOGICAL_LNK_UP;
 +	ibmvnic_send_crq(adapter, &crq);
 +
 +	netif_tx_start_all_queues(netdev);
 +
 +	return 0;
 +
 +bounce_map_failed:
 +	kfree(adapter->bounce_buffer);
 +bounce_alloc_failed:
 +	i = tx_subcrqs - 1;
 +	kfree(adapter->tx_pool[i].free_map);
 +tx_fm_alloc_failed:
 +	free_long_term_buff(adapter, &adapter->tx_pool[i].long_term_buff);
 +tx_ltb_alloc_failed:
 +	kfree(adapter->tx_pool[i].tx_buff);
 +tx_pool_alloc_failed:
 +	for (j = 0; j < i; j++) {
 +		kfree(adapter->tx_pool[j].tx_buff);
 +		free_long_term_buff(adapter,
 +				    &adapter->tx_pool[j].long_term_buff);
 +		kfree(adapter->tx_pool[j].free_map);
 +	}
 +	kfree(adapter->tx_pool);
 +	adapter->tx_pool = NULL;
 +tx_pool_arr_alloc_failed:
 +	i = rxadd_subcrqs;
 +rx_pool_alloc_failed:
 +	for (j = 0; j < i; j++) {
 +		free_rx_pool(adapter, &adapter->rx_pool[j]);
 +		free_long_term_buff(adapter,
 +				    &adapter->rx_pool[j].long_term_buff);
 +	}
 +	kfree(adapter->rx_pool);
 +	adapter->rx_pool = NULL;
 +rx_pool_arr_alloc_failed:
 +	for (i = 0; i < adapter->req_rx_queues; i++)
 +		napi_disable(&adapter->napi[i]);
 +alloc_napi_failed:
 +	return -ENOMEM;
++=======
+ 	rc = set_link_state(adapter, IBMVNIC_LOGICAL_LNK_UP);
+ 	if (rc) {
+ 		for (i = 0; i < adapter->req_rx_queues; i++)
+ 			napi_disable(&adapter->napi[i]);
+ 		release_resources(adapter);
+ 		return rc;
+ 	}
+ 
+ 	netif_tx_start_all_queues(netdev);
+ 
+ 	if (prev_state == VNIC_CLOSED) {
+ 		for (i = 0; i < adapter->req_rx_queues; i++)
+ 			napi_schedule(&adapter->napi[i]);
+ 	}
+ 
+ 	adapter->state = VNIC_OPEN;
+ 	return rc;
+ }
+ 
+ static int ibmvnic_open(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	int rc;
+ 
+ 	mutex_lock(&adapter->reset_lock);
+ 
+ 	if (adapter->state != VNIC_CLOSED) {
+ 		rc = ibmvnic_login(netdev);
+ 		if (rc) {
+ 			mutex_unlock(&adapter->reset_lock);
+ 			return rc;
+ 		}
+ 
+ 		rc = init_resources(adapter);
+ 		if (rc) {
+ 			netdev_err(netdev, "failed to initialize resources\n");
+ 			release_resources(adapter);
+ 			mutex_unlock(&adapter->reset_lock);
+ 			return rc;
+ 		}
+ 	}
+ 
+ 	rc = __ibmvnic_open(netdev);
+ 	mutex_unlock(&adapter->reset_lock);
+ 
+ 	return rc;
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  }
  
  static void disable_sub_crqs(struct ibmvnic_adapter *adapter)
@@@ -529,14 -751,14 +743,19 @@@
  	}
  }
  
- static int ibmvnic_close(struct net_device *netdev)
+ static int __ibmvnic_close(struct net_device *netdev)
  {
  	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 -	int rc = 0;
 +	struct device *dev = &adapter->vdev->dev;
 +	union ibmvnic_crq crq;
  	int i;
  
++<<<<<<< HEAD
 +	adapter->closing = true;
++=======
+ 	adapter->state = VNIC_CLOSING;
+ 	netif_tx_stop_all_queues(netdev);
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  	disable_sub_crqs(adapter);
  
  	if (adapter->napi) {
@@@ -544,49 -766,22 +763,68 @@@
  			napi_disable(&adapter->napi[i]);
  	}
  
++<<<<<<< HEAD
 +	if (!adapter->failover)
 +		netif_tx_stop_all_queues(netdev);
 +
 +	if (adapter->bounce_buffer) {
 +		if (!dma_mapping_error(dev, adapter->bounce_buffer_dma)) {
 +			dma_unmap_single(&adapter->vdev->dev,
 +					 adapter->bounce_buffer_dma,
 +					 adapter->bounce_buffer_size,
 +					 DMA_BIDIRECTIONAL);
 +			adapter->bounce_buffer_dma = DMA_ERROR_CODE;
 +		}
 +		kfree(adapter->bounce_buffer);
 +		adapter->bounce_buffer = NULL;
 +	}
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.logical_link_state.first = IBMVNIC_CRQ_CMD;
 +	crq.logical_link_state.cmd = LOGICAL_LINK_STATE;
 +	crq.logical_link_state.link_state = IBMVNIC_LOGICAL_LNK_DN;
 +	ibmvnic_send_crq(adapter, &crq);
 +
 +	for (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
 +	     i++) {
 +		kfree(adapter->tx_pool[i].tx_buff);
 +		free_long_term_buff(adapter,
 +				    &adapter->tx_pool[i].long_term_buff);
 +		kfree(adapter->tx_pool[i].free_map);
 +	}
 +	kfree(adapter->tx_pool);
 +	adapter->tx_pool = NULL;
 +
 +	for (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
 +	     i++) {
 +		free_rx_pool(adapter, &adapter->rx_pool[i]);
 +		free_long_term_buff(adapter,
 +				    &adapter->rx_pool[i].long_term_buff);
 +	}
 +	kfree(adapter->rx_pool);
 +	adapter->rx_pool = NULL;
 +
 +	adapter->closing = false;
 +
 +	return 0;
++=======
+ 	rc = set_link_state(adapter, IBMVNIC_LOGICAL_LNK_DN);
+ 
+ 	adapter->state = VNIC_CLOSED;
+ 	return rc;
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
+ }
+ 
+ static int ibmvnic_close(struct net_device *netdev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
+ 	int rc;
+ 
+ 	mutex_lock(&adapter->reset_lock);
+ 	rc = __ibmvnic_close(netdev);
+ 	mutex_unlock(&adapter->reset_lock);
+ 
+ 	return rc;
  }
  
  /**
@@@ -944,29 -1137,185 +1182,199 @@@ static int ibmvnic_set_mac(struct net_d
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int ibmvnic_change_mtu(struct net_device *netdev, int new_mtu)
 +{
 +	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 +
 +	if (new_mtu > adapter->req_mtu || new_mtu < adapter->min_mtu)
 +		return -EINVAL;
 +
 +	netdev->mtu = new_mtu;
 +	return 0;
 +}
 +
- static void ibmvnic_tx_timeout(struct net_device *dev)
++=======
+ /**
+  * do_reset returns zero if we are able to keep processing reset events, or
+  * non-zero if we hit a fatal error and must halt.
+  */
+ static int do_reset(struct ibmvnic_adapter *adapter,
+ 		    struct ibmvnic_rwi *rwi, u32 reset_state)
  {
- 	struct ibmvnic_adapter *adapter = netdev_priv(dev);
- 	int rc;
+ 	struct net_device *netdev = adapter->netdev;
+ 	int i, rc;
  
- 	/* Adapter timed out, resetting it */
+ 	netif_carrier_off(netdev);
+ 	adapter->reset_reason = rwi->reset_reason;
+ 
+ 	if (rwi->reset_reason == VNIC_RESET_MOBILITY) {
+ 		rc = ibmvnic_reenable_crq_queue(adapter);
+ 		if (rc)
+ 			return 0;
+ 	}
+ 
+ 	rc = __ibmvnic_close(netdev);
+ 	if (rc)
+ 		return rc;
+ 
+ 	/* remove the closed state so when we call open it appears
+ 	 * we are coming from the probed state.
+ 	 */
+ 	adapter->state = VNIC_PROBED;
+ 
+ 	release_resources(adapter);
  	release_sub_crqs(adapter);
- 	rc = ibmvnic_reset_crq(adapter);
+ 	release_crq_queue(adapter);
+ 
+ 	rc = ibmvnic_init(adapter);
  	if (rc)
- 		dev_err(&adapter->vdev->dev, "Adapter timeout, reset failed\n");
- 	else
- 		ibmvnic_send_crq_init(adapter);
+ 		return 0;
+ 
+ 	/* If the adapter was in PROBE state prior to the reset, exit here. */
+ 	if (reset_state == VNIC_PROBED)
+ 		return 0;
+ 
+ 	rc = ibmvnic_login(netdev);
+ 	if (rc) {
+ 		adapter->state = VNIC_PROBED;
+ 		return 0;
+ 	}
+ 
+ 	rtnl_lock();
+ 	rc = init_resources(adapter);
+ 	rtnl_unlock();
+ 	if (rc)
+ 		return rc;
+ 
+ 	if (reset_state == VNIC_CLOSED)
+ 		return 0;
+ 
+ 	rc = __ibmvnic_open(netdev);
+ 	if (rc) {
+ 		if (list_empty(&adapter->rwi_list))
+ 			adapter->state = VNIC_CLOSED;
+ 		else
+ 			adapter->state = reset_state;
+ 
+ 		return 0;
+ 	}
+ 
+ 	netif_carrier_on(netdev);
+ 
+ 	/* kick napi */
+ 	for (i = 0; i < adapter->req_rx_queues; i++)
+ 		napi_schedule(&adapter->napi[i]);
+ 
+ 	return 0;
+ }
+ 
+ static struct ibmvnic_rwi *get_next_rwi(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_rwi *rwi;
+ 
+ 	mutex_lock(&adapter->rwi_lock);
+ 
+ 	if (!list_empty(&adapter->rwi_list)) {
+ 		rwi = list_first_entry(&adapter->rwi_list, struct ibmvnic_rwi,
+ 				       list);
+ 		list_del(&rwi->list);
+ 	} else {
+ 		rwi = NULL;
+ 	}
+ 
+ 	mutex_unlock(&adapter->rwi_lock);
+ 	return rwi;
+ }
+ 
+ static void free_all_rwi(struct ibmvnic_adapter *adapter)
+ {
+ 	struct ibmvnic_rwi *rwi;
+ 
+ 	rwi = get_next_rwi(adapter);
+ 	while (rwi) {
+ 		kfree(rwi);
+ 		rwi = get_next_rwi(adapter);
+ 	}
+ }
+ 
+ static void __ibmvnic_reset(struct work_struct *work)
+ {
+ 	struct ibmvnic_rwi *rwi;
+ 	struct ibmvnic_adapter *adapter;
+ 	struct net_device *netdev;
+ 	u32 reset_state;
+ 	int rc;
+ 
+ 	adapter = container_of(work, struct ibmvnic_adapter, ibmvnic_reset);
+ 	netdev = adapter->netdev;
+ 
+ 	mutex_lock(&adapter->reset_lock);
+ 	adapter->resetting = true;
+ 	reset_state = adapter->state;
+ 
+ 	rwi = get_next_rwi(adapter);
+ 	while (rwi) {
+ 		rc = do_reset(adapter, rwi, reset_state);
+ 		kfree(rwi);
+ 		if (rc)
+ 			break;
+ 
+ 		rwi = get_next_rwi(adapter);
+ 	}
+ 
+ 	if (rc) {
+ 		free_all_rwi(adapter);
+ 		return;
+ 	}
+ 
+ 	adapter->resetting = false;
+ 	mutex_unlock(&adapter->reset_lock);
+ }
+ 
+ static void ibmvnic_reset(struct ibmvnic_adapter *adapter,
+ 			  enum ibmvnic_reset_reason reason)
+ {
+ 	struct ibmvnic_rwi *rwi, *tmp;
+ 	struct net_device *netdev = adapter->netdev;
+ 	struct list_head *entry;
+ 
+ 	if (adapter->state == VNIC_REMOVING ||
+ 	    adapter->state == VNIC_REMOVED) {
+ 		netdev_dbg(netdev, "Adapter removing, skipping reset\n");
+ 		return;
+ 	}
+ 
+ 	mutex_lock(&adapter->rwi_lock);
+ 
+ 	list_for_each(entry, &adapter->rwi_list) {
+ 		tmp = list_entry(entry, struct ibmvnic_rwi, list);
+ 		if (tmp->reset_reason == reason) {
+ 			netdev_err(netdev, "Matching reset found, skipping\n");
+ 			mutex_unlock(&adapter->rwi_lock);
+ 			return;
+ 		}
+ 	}
+ 
+ 	rwi = kzalloc(sizeof(*rwi), GFP_KERNEL);
+ 	if (!rwi) {
+ 		mutex_unlock(&adapter->rwi_lock);
+ 		ibmvnic_close(netdev);
+ 		return;
+ 	}
+ 
+ 	rwi->reset_reason = reason;
+ 	list_add_tail(&rwi->list, &adapter->rwi_list);
+ 	mutex_unlock(&adapter->rwi_lock);
+ 	schedule_work(&adapter->ibmvnic_reset);
+ }
+ 
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
+ static void ibmvnic_tx_timeout(struct net_device *dev)
+ {
+ 	struct ibmvnic_adapter *adapter = netdev_priv(dev);
+ 
+ 	ibmvnic_reset(adapter, VNIC_RESET_TIMEOUT);
  }
  
  static void remove_buff_from_pool(struct ibmvnic_adapter *adapter,
@@@ -2398,20 -2652,51 +2794,50 @@@ static void handle_error_indication(uni
  	list_add_tail(&error_buff->list, &adapter->errors);
  	spin_unlock_irqrestore(&adapter->error_list_lock, flags);
  
 -	memset(&crq, 0, sizeof(crq));
 -	crq.request_error_info.first = IBMVNIC_CRQ_CMD;
 -	crq.request_error_info.cmd = REQUEST_ERROR_INFO;
 -	crq.request_error_info.ioba = cpu_to_be32(error_buff->dma);
 -	crq.request_error_info.len = cpu_to_be32(detail_len);
 -	crq.request_error_info.error_id = err_crq->error_indication.error_id;
 +	memset(&new_crq, 0, sizeof(new_crq));
 +	new_crq.request_error_info.first = IBMVNIC_CRQ_CMD;
 +	new_crq.request_error_info.cmd = REQUEST_ERROR_INFO;
 +	new_crq.request_error_info.ioba = cpu_to_be32(error_buff->dma);
 +	new_crq.request_error_info.len = cpu_to_be32(detail_len);
 +	new_crq.request_error_info.error_id = crq->error_indication.error_id;
  
 -	rc = ibmvnic_send_crq(adapter, &crq);
 -	if (rc) {
 -		netdev_err(netdev, "failed to request error information\n");
 -		goto err_info_fail;
 -	}
 +	memcpy(&inflight_cmd->crq, &crq, sizeof(crq));
  
 -	if (!wait_for_completion_timeout(&adapter->init_done, timeout)) {
 -		netdev_err(netdev, "timeout waiting for error information\n");
 -		goto err_info_fail;
 -	}
 +	spin_lock_irqsave(&adapter->inflight_lock, flags);
 +	list_add_tail(&inflight_cmd->list, &adapter->inflight);
 +	spin_unlock_irqrestore(&adapter->inflight_lock, flags);
  
++<<<<<<< HEAD
 +	ibmvnic_send_crq(adapter, &new_crq);
++=======
+ 	return;
+ 
+ err_info_fail:
+ 	spin_lock_irqsave(&adapter->error_list_lock, flags);
+ 	list_del(&error_buff->list);
+ 	spin_unlock_irqrestore(&adapter->error_list_lock, flags);
+ 
+ 	kfree(error_buff->buff);
+ 	kfree(error_buff);
+ }
+ 
+ static void handle_error_indication(union ibmvnic_crq *crq,
+ 				    struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 
+ 	dev_err(dev, "Firmware reports %serror id %x, cause %d\n",
+ 		crq->error_indication.flags
+ 			& IBMVNIC_FATAL_ERROR ? "FATAL " : "",
+ 		be32_to_cpu(crq->error_indication.error_id),
+ 		be16_to_cpu(crq->error_indication.error_cause));
+ 
+ 	if (be32_to_cpu(crq->error_indication.error_id))
+ 		request_error_information(adapter, crq);
+ 
+ 	if (crq->error_indication.flags & IBMVNIC_FATAL_ERROR)
+ 		ibmvnic_reset(adapter, VNIC_RESET_FATAL);
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  }
  
  static void handle_change_mac_rsp(union ibmvnic_crq *crq,
@@@ -2800,547 -3081,10 +3226,550 @@@ static void handle_query_cap_rsp(union 
  out:
  	if (atomic_read(&adapter->running_cap_crqs) == 0) {
  		adapter->wait_capability = false;
 -		ibmvnic_send_req_caps(adapter, 0);
 +		init_sub_crqs(adapter, 0);
 +		/* We're done querying the capabilities, initialize sub-crqs */
 +	}
 +}
 +
++<<<<<<< HEAD
 +static void handle_control_ras_rsp(union ibmvnic_crq *crq,
 +				   struct ibmvnic_adapter *adapter)
 +{
 +	u8 correlator = crq->control_ras_rsp.correlator;
 +	struct device *dev = &adapter->vdev->dev;
 +	bool found = false;
 +	int i;
 +
 +	if (crq->control_ras_rsp.rc.code) {
 +		dev_warn(dev, "Control ras failed rc=%d\n",
 +			 crq->control_ras_rsp.rc.code);
 +		return;
 +	}
 +
 +	for (i = 0; i < adapter->ras_comp_num; i++) {
 +		if (adapter->ras_comps[i].correlator == correlator) {
 +			found = true;
 +			break;
 +		}
 +	}
 +
 +	if (!found) {
 +		dev_warn(dev, "Correlator not found on control_ras_rsp\n");
 +		return;
 +	}
 +
 +	switch (crq->control_ras_rsp.op) {
 +	case IBMVNIC_TRACE_LEVEL:
 +		adapter->ras_comps[i].trace_level = crq->control_ras.level;
 +		break;
 +	case IBMVNIC_ERROR_LEVEL:
 +		adapter->ras_comps[i].error_check_level =
 +		    crq->control_ras.level;
 +		break;
 +	case IBMVNIC_TRACE_PAUSE:
 +		adapter->ras_comp_int[i].paused = 1;
 +		break;
 +	case IBMVNIC_TRACE_RESUME:
 +		adapter->ras_comp_int[i].paused = 0;
 +		break;
 +	case IBMVNIC_TRACE_ON:
 +		adapter->ras_comps[i].trace_on = 1;
 +		break;
 +	case IBMVNIC_TRACE_OFF:
 +		adapter->ras_comps[i].trace_on = 0;
 +		break;
 +	case IBMVNIC_CHG_TRACE_BUFF_SZ:
 +		/* trace_buff_sz is 3 bytes, stuff it into an int */
 +		((u8 *)(&adapter->ras_comps[i].trace_buff_size))[0] = 0;
 +		((u8 *)(&adapter->ras_comps[i].trace_buff_size))[1] =
 +		    crq->control_ras_rsp.trace_buff_sz[0];
 +		((u8 *)(&adapter->ras_comps[i].trace_buff_size))[2] =
 +		    crq->control_ras_rsp.trace_buff_sz[1];
 +		((u8 *)(&adapter->ras_comps[i].trace_buff_size))[3] =
 +		    crq->control_ras_rsp.trace_buff_sz[2];
 +		break;
 +	default:
 +		dev_err(dev, "invalid op %d on control_ras_rsp",
 +			crq->control_ras_rsp.op);
  	}
  }
  
 +static ssize_t trace_read(struct file *file, char __user *user_buf, size_t len,
 +			  loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	struct device *dev = &adapter->vdev->dev;
 +	struct ibmvnic_fw_trace_entry *trace;
 +	int num = ras_comp_int->num;
 +	union ibmvnic_crq crq;
 +	dma_addr_t trace_tok;
 +
 +	if (*ppos >= be32_to_cpu(adapter->ras_comps[num].trace_buff_size))
 +		return 0;
 +
 +	trace =
 +	    dma_alloc_coherent(dev,
 +			       be32_to_cpu(adapter->ras_comps[num].
 +					   trace_buff_size), &trace_tok,
 +			       GFP_KERNEL);
 +	if (!trace) {
 +		dev_err(dev, "Couldn't alloc trace buffer\n");
 +		return 0;
 +	}
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.collect_fw_trace.first = IBMVNIC_CRQ_CMD;
 +	crq.collect_fw_trace.cmd = COLLECT_FW_TRACE;
 +	crq.collect_fw_trace.correlator = adapter->ras_comps[num].correlator;
 +	crq.collect_fw_trace.ioba = cpu_to_be32(trace_tok);
 +	crq.collect_fw_trace.len = adapter->ras_comps[num].trace_buff_size;
 +
 +	init_completion(&adapter->fw_done);
 +	ibmvnic_send_crq(adapter, &crq);
 +	wait_for_completion(&adapter->fw_done);
 +
 +	if (*ppos + len > be32_to_cpu(adapter->ras_comps[num].trace_buff_size))
 +		len =
 +		    be32_to_cpu(adapter->ras_comps[num].trace_buff_size) -
 +		    *ppos;
 +
 +	copy_to_user(user_buf, &((u8 *)trace)[*ppos], len);
 +
 +	dma_free_coherent(dev,
 +			  be32_to_cpu(adapter->ras_comps[num].trace_buff_size),
 +			  trace, trace_tok);
 +	*ppos += len;
 +	return len;
 +}
 +
 +static const struct file_operations trace_ops = {
 +	.owner		= THIS_MODULE,
 +	.open		= simple_open,
 +	.read		= trace_read,
 +};
 +
 +static ssize_t paused_read(struct file *file, char __user *user_buf, size_t len,
 +			   loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	char buff[5]; /*  1 or 0 plus \n and \0 */
 +	int size;
 +
 +	size = sprintf(buff, "%d\n", adapter->ras_comp_int[num].paused);
 +
 +	if (*ppos >= size)
 +		return 0;
 +
 +	copy_to_user(user_buf, buff, size);
 +	*ppos += size;
 +	return size;
 +}
 +
 +static ssize_t paused_write(struct file *file, const char __user *user_buf,
 +			    size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	union ibmvnic_crq crq;
 +	unsigned long val;
 +	char buff[9]; /* decimal max int plus \n and \0 */
 +
 +	copy_from_user(buff, user_buf, sizeof(buff));
 +	val = kstrtoul(buff, 10, NULL);
 +
 +	adapter->ras_comp_int[num].paused = val ? 1 : 0;
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.control_ras.first = IBMVNIC_CRQ_CMD;
 +	crq.control_ras.cmd = CONTROL_RAS;
 +	crq.control_ras.correlator = adapter->ras_comps[num].correlator;
 +	crq.control_ras.op = val ? IBMVNIC_TRACE_PAUSE : IBMVNIC_TRACE_RESUME;
 +	ibmvnic_send_crq(adapter, &crq);
 +
 +	return len;
 +}
 +
 +static const struct file_operations paused_ops = {
 +	.owner		= THIS_MODULE,
 +	.open		= simple_open,
 +	.read		= paused_read,
 +	.write		= paused_write,
 +};
 +
 +static ssize_t tracing_read(struct file *file, char __user *user_buf,
 +			    size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	char buff[5]; /*  1 or 0 plus \n and \0 */
 +	int size;
 +
 +	size = sprintf(buff, "%d\n", adapter->ras_comps[num].trace_on);
 +
 +	if (*ppos >= size)
 +		return 0;
 +
 +	copy_to_user(user_buf, buff, size);
 +	*ppos += size;
 +	return size;
 +}
 +
 +static ssize_t tracing_write(struct file *file, const char __user *user_buf,
 +			     size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	union ibmvnic_crq crq;
 +	unsigned long val;
 +	char buff[9]; /* decimal max int plus \n and \0 */
 +
 +	copy_from_user(buff, user_buf, sizeof(buff));
 +	val = kstrtoul(buff, 10, NULL);
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.control_ras.first = IBMVNIC_CRQ_CMD;
 +	crq.control_ras.cmd = CONTROL_RAS;
 +	crq.control_ras.correlator = adapter->ras_comps[num].correlator;
 +	crq.control_ras.op = val ? IBMVNIC_TRACE_ON : IBMVNIC_TRACE_OFF;
 +
 +	return len;
 +}
 +
 +static const struct file_operations tracing_ops = {
 +	.owner		= THIS_MODULE,
 +	.open		= simple_open,
 +	.read		= tracing_read,
 +	.write		= tracing_write,
 +};
 +
 +static ssize_t error_level_read(struct file *file, char __user *user_buf,
 +				size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	char buff[5]; /* decimal max char plus \n and \0 */
 +	int size;
 +
 +	size = sprintf(buff, "%d\n", adapter->ras_comps[num].error_check_level);
 +
 +	if (*ppos >= size)
 +		return 0;
 +
 +	copy_to_user(user_buf, buff, size);
 +	*ppos += size;
 +	return size;
 +}
 +
 +static ssize_t error_level_write(struct file *file, const char __user *user_buf,
 +				 size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	union ibmvnic_crq crq;
 +	unsigned long val;
 +	char buff[9]; /* decimal max int plus \n and \0 */
 +
 +	copy_from_user(buff, user_buf, sizeof(buff));
 +	val = kstrtoul(buff, 10, NULL);
 +
 +	if (val > 9)
 +		val = 9;
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.control_ras.first = IBMVNIC_CRQ_CMD;
 +	crq.control_ras.cmd = CONTROL_RAS;
 +	crq.control_ras.correlator = adapter->ras_comps[num].correlator;
 +	crq.control_ras.op = IBMVNIC_ERROR_LEVEL;
 +	crq.control_ras.level = val;
 +	ibmvnic_send_crq(adapter, &crq);
 +
 +	return len;
 +}
 +
 +static const struct file_operations error_level_ops = {
 +	.owner		= THIS_MODULE,
 +	.open		= simple_open,
 +	.read		= error_level_read,
 +	.write		= error_level_write,
 +};
 +
 +static ssize_t trace_level_read(struct file *file, char __user *user_buf,
 +				size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	char buff[5]; /* decimal max char plus \n and \0 */
 +	int size;
 +
 +	size = sprintf(buff, "%d\n", adapter->ras_comps[num].trace_level);
 +	if (*ppos >= size)
 +		return 0;
 +
 +	copy_to_user(user_buf, buff, size);
 +	*ppos += size;
 +	return size;
 +}
 +
 +static ssize_t trace_level_write(struct file *file, const char __user *user_buf,
 +				 size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	union ibmvnic_crq crq;
 +	unsigned long val;
 +	char buff[9]; /* decimal max int plus \n and \0 */
 +
 +	copy_from_user(buff, user_buf, sizeof(buff));
 +	val = kstrtoul(buff, 10, NULL);
 +	if (val > 9)
 +		val = 9;
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.control_ras.first = IBMVNIC_CRQ_CMD;
 +	crq.control_ras.cmd = CONTROL_RAS;
 +	crq.control_ras.correlator =
 +	    adapter->ras_comps[ras_comp_int->num].correlator;
 +	crq.control_ras.op = IBMVNIC_TRACE_LEVEL;
 +	crq.control_ras.level = val;
 +	ibmvnic_send_crq(adapter, &crq);
 +
 +	return len;
 +}
 +
 +static const struct file_operations trace_level_ops = {
 +	.owner		= THIS_MODULE,
 +	.open		= simple_open,
 +	.read		= trace_level_read,
 +	.write		= trace_level_write,
 +};
 +
 +static ssize_t trace_buff_size_read(struct file *file, char __user *user_buf,
 +				    size_t len, loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	int num = ras_comp_int->num;
 +	char buff[9]; /* decimal max int plus \n and \0 */
 +	int size;
 +
 +	size = sprintf(buff, "%d\n", adapter->ras_comps[num].trace_buff_size);
 +	if (*ppos >= size)
 +		return 0;
 +
 +	copy_to_user(user_buf, buff, size);
 +	*ppos += size;
 +	return size;
 +}
 +
 +static ssize_t trace_buff_size_write(struct file *file,
 +				     const char __user *user_buf, size_t len,
 +				     loff_t *ppos)
 +{
 +	struct ibmvnic_fw_comp_internal *ras_comp_int = file->private_data;
 +	struct ibmvnic_adapter *adapter = ras_comp_int->adapter;
 +	union ibmvnic_crq crq;
 +	unsigned long val;
 +	char buff[9]; /* decimal max int plus \n and \0 */
 +
 +	copy_from_user(buff, user_buf, sizeof(buff));
 +	val = kstrtoul(buff, 10, NULL);
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.control_ras.first = IBMVNIC_CRQ_CMD;
 +	crq.control_ras.cmd = CONTROL_RAS;
 +	crq.control_ras.correlator =
 +	    adapter->ras_comps[ras_comp_int->num].correlator;
 +	crq.control_ras.op = IBMVNIC_CHG_TRACE_BUFF_SZ;
 +	/* trace_buff_sz is 3 bytes, stuff an int into it */
 +	crq.control_ras.trace_buff_sz[0] = ((u8 *)(&val))[5];
 +	crq.control_ras.trace_buff_sz[1] = ((u8 *)(&val))[6];
 +	crq.control_ras.trace_buff_sz[2] = ((u8 *)(&val))[7];
 +	ibmvnic_send_crq(adapter, &crq);
 +
 +	return len;
 +}
 +
 +static const struct file_operations trace_size_ops = {
 +	.owner		= THIS_MODULE,
 +	.open		= simple_open,
 +	.read		= trace_buff_size_read,
 +	.write		= trace_buff_size_write,
 +};
 +
 +static void handle_request_ras_comps_rsp(union ibmvnic_crq *crq,
 +					 struct ibmvnic_adapter *adapter)
 +{
 +	struct device *dev = &adapter->vdev->dev;
 +	struct dentry *dir_ent;
 +	struct dentry *ent;
 +	int i;
 +
 +	debugfs_remove_recursive(adapter->ras_comps_ent);
 +
 +	adapter->ras_comps_ent = debugfs_create_dir("ras_comps",
 +						    adapter->debugfs_dir);
 +	if (!adapter->ras_comps_ent || IS_ERR(adapter->ras_comps_ent)) {
 +		dev_info(dev, "debugfs create ras_comps dir failed\n");
 +		return;
 +	}
 +
 +	for (i = 0; i < adapter->ras_comp_num; i++) {
 +		dir_ent = debugfs_create_dir(adapter->ras_comps[i].name,
 +					     adapter->ras_comps_ent);
 +		if (!dir_ent || IS_ERR(dir_ent)) {
 +			dev_info(dev, "debugfs create %s dir failed\n",
 +				 adapter->ras_comps[i].name);
 +			continue;
 +		}
 +
 +		adapter->ras_comp_int[i].adapter = adapter;
 +		adapter->ras_comp_int[i].num = i;
 +		adapter->ras_comp_int[i].desc_blob.data =
 +		    &adapter->ras_comps[i].description;
 +		adapter->ras_comp_int[i].desc_blob.size =
 +		    sizeof(adapter->ras_comps[i].description);
 +
 +		/* Don't need to remember the dentry's because the debugfs dir
 +		 * gets removed recursively
 +		 */
 +		ent = debugfs_create_blob("description", S_IRUGO, dir_ent,
 +					  &adapter->ras_comp_int[i].desc_blob);
 +		ent = debugfs_create_file("trace_buf_size", S_IRUGO | S_IWUSR,
 +					  dir_ent, &adapter->ras_comp_int[i],
 +					  &trace_size_ops);
 +		ent = debugfs_create_file("trace_level",
 +					  S_IRUGO |
 +					  (adapter->ras_comps[i].trace_level !=
 +					   0xFF  ? S_IWUSR : 0),
 +					   dir_ent, &adapter->ras_comp_int[i],
 +					   &trace_level_ops);
 +		ent = debugfs_create_file("error_level",
 +					  S_IRUGO |
 +					  (adapter->
 +					   ras_comps[i].error_check_level !=
 +					   0xFF ? S_IWUSR : 0),
 +					  dir_ent, &adapter->ras_comp_int[i],
 +					  &trace_level_ops);
 +		ent = debugfs_create_file("tracing", S_IRUGO | S_IWUSR,
 +					  dir_ent, &adapter->ras_comp_int[i],
 +					  &tracing_ops);
 +		ent = debugfs_create_file("paused", S_IRUGO | S_IWUSR,
 +					  dir_ent, &adapter->ras_comp_int[i],
 +					  &paused_ops);
 +		ent = debugfs_create_file("trace", S_IRUGO, dir_ent,
 +					  &adapter->ras_comp_int[i],
 +					  &trace_ops);
 +	}
 +}
 +
 +static void handle_request_ras_comp_num_rsp(union ibmvnic_crq *crq,
 +					    struct ibmvnic_adapter *adapter)
 +{
 +	int len = adapter->ras_comp_num * sizeof(struct ibmvnic_fw_component);
 +	struct device *dev = &adapter->vdev->dev;
 +	union ibmvnic_crq newcrq;
 +
 +	adapter->ras_comps = dma_alloc_coherent(dev, len,
 +						&adapter->ras_comps_tok,
 +						GFP_KERNEL);
 +	if (!adapter->ras_comps) {
 +		if (!firmware_has_feature(FW_FEATURE_CMO))
 +			dev_err(dev, "Couldn't alloc fw comps buffer\n");
 +		return;
 +	}
 +
 +	adapter->ras_comp_int = kmalloc(adapter->ras_comp_num *
 +					sizeof(struct ibmvnic_fw_comp_internal),
 +					GFP_KERNEL);
 +	if (!adapter->ras_comp_int)
 +		dma_free_coherent(dev, len, adapter->ras_comps,
 +				  adapter->ras_comps_tok);
 +
 +	memset(&newcrq, 0, sizeof(newcrq));
 +	newcrq.request_ras_comps.first = IBMVNIC_CRQ_CMD;
 +	newcrq.request_ras_comps.cmd = REQUEST_RAS_COMPS;
 +	newcrq.request_ras_comps.ioba = cpu_to_be32(adapter->ras_comps_tok);
 +	newcrq.request_ras_comps.len = cpu_to_be32(len);
 +	ibmvnic_send_crq(adapter, &newcrq);
 +}
 +
 +static void ibmvnic_free_inflight(struct ibmvnic_adapter *adapter)
 +{
 +	struct ibmvnic_inflight_cmd *inflight_cmd, *tmp1;
 +	struct device *dev = &adapter->vdev->dev;
 +	struct ibmvnic_error_buff *error_buff, *tmp2;
 +	unsigned long flags;
 +	unsigned long flags2;
 +
 +	spin_lock_irqsave(&adapter->inflight_lock, flags);
 +	list_for_each_entry_safe(inflight_cmd, tmp1, &adapter->inflight, list) {
 +		switch (inflight_cmd->crq.generic.cmd) {
 +		case LOGIN:
 +			dma_unmap_single(dev, adapter->login_buf_token,
 +					 adapter->login_buf_sz,
 +					 DMA_BIDIRECTIONAL);
 +			dma_unmap_single(dev, adapter->login_rsp_buf_token,
 +					 adapter->login_rsp_buf_sz,
 +					 DMA_BIDIRECTIONAL);
 +			kfree(adapter->login_rsp_buf);
 +			kfree(adapter->login_buf);
 +			break;
 +		case REQUEST_DUMP:
 +			complete(&adapter->fw_done);
 +			break;
 +		case REQUEST_ERROR_INFO:
 +			spin_lock_irqsave(&adapter->error_list_lock, flags2);
 +			list_for_each_entry_safe(error_buff, tmp2,
 +						 &adapter->errors, list) {
 +				dma_unmap_single(dev, error_buff->dma,
 +						 error_buff->len,
 +						 DMA_FROM_DEVICE);
 +				kfree(error_buff->buff);
 +				list_del(&error_buff->list);
 +				kfree(error_buff);
 +			}
 +			spin_unlock_irqrestore(&adapter->error_list_lock,
 +					       flags2);
 +			break;
 +		}
 +		list_del(&inflight_cmd->list);
 +		kfree(inflight_cmd);
 +	}
 +	spin_unlock_irqrestore(&adapter->inflight_lock, flags);
 +}
 +
 +static void ibmvnic_xport_event(struct work_struct *work)
 +{
 +	struct ibmvnic_adapter *adapter = container_of(work,
 +						       struct ibmvnic_adapter,
 +						       ibmvnic_xport);
 +	struct device *dev = &adapter->vdev->dev;
 +	long rc;
 +
 +	ibmvnic_free_inflight(adapter);
 +	release_sub_crqs(adapter);
 +	if (adapter->migrated) {
 +		rc = ibmvnic_reenable_crq_queue(adapter);
 +		if (rc)
 +			dev_err(dev, "Error after enable rc=%ld\n", rc);
 +		adapter->migrated = false;
 +		rc = ibmvnic_send_crq_init(adapter);
 +		if (rc)
 +			dev_err(dev, "Error sending init rc=%ld\n", rc);
 +	}
 +}
 +
++=======
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  static void ibmvnic_handle_crq(union ibmvnic_crq *crq,
  			       struct ibmvnic_adapter *adapter)
  {
@@@ -3681,121 -3403,34 +4103,151 @@@ map_failed
  	return retrc;
  }
  
++<<<<<<< HEAD
 +/* debugfs for dump */
 +static int ibmvnic_dump_show(struct seq_file *seq, void *v)
 +{
 +	struct net_device *netdev = seq->private;
 +	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 +	struct device *dev = &adapter->vdev->dev;
 +	union ibmvnic_crq crq;
 +
 +	memset(&crq, 0, sizeof(crq));
 +	crq.request_dump_size.first = IBMVNIC_CRQ_CMD;
 +	crq.request_dump_size.cmd = REQUEST_DUMP_SIZE;
 +
 +	init_completion(&adapter->fw_done);
 +	ibmvnic_send_crq(adapter, &crq);
 +	wait_for_completion(&adapter->fw_done);
 +
 +	seq_write(seq, adapter->dump_data, adapter->dump_data_size);
 +
 +	dma_unmap_single(dev, adapter->dump_data_token, adapter->dump_data_size,
 +			 DMA_BIDIRECTIONAL);
 +
 +	kfree(adapter->dump_data);
 +
 +	return 0;
 +}
 +
 +static int ibmvnic_dump_open(struct inode *inode, struct file *file)
 +{
 +	return single_open(file, ibmvnic_dump_show, inode->i_private);
 +}
 +
 +static const struct file_operations ibmvnic_dump_ops = {
 +	.owner          = THIS_MODULE,
 +	.open           = ibmvnic_dump_open,
 +	.read           = seq_read,
 +	.llseek         = seq_lseek,
 +	.release        = single_release,
 +};
 +
 +static void handle_crq_init_rsp(struct work_struct *work)
 +{
 +	struct ibmvnic_adapter *adapter = container_of(work,
 +						       struct ibmvnic_adapter,
 +						       vnic_crq_init);
 +	struct device *dev = &adapter->vdev->dev;
 +	struct net_device *netdev = adapter->netdev;
 +	unsigned long timeout = msecs_to_jiffies(30000);
 +	bool restart = false;
 +	int rc;
 +
 +	if (adapter->failover) {
 +		release_sub_crqs(adapter);
 +		if (netif_running(netdev)) {
 +			netif_tx_disable(netdev);
 +			ibmvnic_close(netdev);
 +			restart = true;
 +		}
 +	}
 +
 +	reinit_completion(&adapter->init_done);
 +	send_version_xchg(adapter);
 +	if (!wait_for_completion_timeout(&adapter->init_done, timeout)) {
 +		dev_err(dev, "Passive init timeout\n");
 +		goto task_failed;
 +	}
 +
 +	do {
 +		if (adapter->renegotiate) {
 +			adapter->renegotiate = false;
 +			release_sub_crqs_no_irqs(adapter);
 +
 +			reinit_completion(&adapter->init_done);
 +			send_cap_queries(adapter);
 +			if (!wait_for_completion_timeout(&adapter->init_done,
 +							 timeout)) {
 +				dev_err(dev, "Passive init timeout\n");
 +				goto task_failed;
 +			}
 +		}
 +	} while (adapter->renegotiate);
 +	rc = init_sub_crq_irqs(adapter);
 +
 +	if (rc)
 +		goto task_failed;
 +
 +	netdev->real_num_tx_queues = adapter->req_tx_queues;
 +	netdev->mtu = adapter->req_mtu;
 +
 +	if (adapter->failover) {
 +		adapter->failover = false;
 +		if (restart) {
 +			rc = ibmvnic_open(netdev);
 +			if (rc)
 +				goto restart_failed;
 +		}
 +		netif_carrier_on(netdev);
 +		return;
 +	}
 +
 +	rc = register_netdev(netdev);
 +	if (rc) {
 +		dev_err(dev,
 +			"failed to register netdev rc=%d\n", rc);
 +		goto register_failed;
 +	}
 +	dev_info(dev, "ibmvnic registered\n");
 +
 +	return;
 +
 +restart_failed:
 +	dev_err(dev, "Failed to restart ibmvnic, rc=%d\n", rc);
 +register_failed:
 +	release_sub_crqs(adapter);
 +task_failed:
 +	dev_err(dev, "Passive initialization was not successful\n");
++=======
+ static int ibmvnic_init(struct ibmvnic_adapter *adapter)
+ {
+ 	struct device *dev = &adapter->vdev->dev;
+ 	unsigned long timeout = msecs_to_jiffies(30000);
+ 	int rc;
+ 
+ 	rc = init_crq_queue(adapter);
+ 	if (rc) {
+ 		dev_err(dev, "Couldn't initialize crq. rc=%d\n", rc);
+ 		return rc;
+ 	}
+ 
+ 	init_completion(&adapter->init_done);
+ 	ibmvnic_send_crq_init(adapter);
+ 	if (!wait_for_completion_timeout(&adapter->init_done, timeout)) {
+ 		dev_err(dev, "Initialization sequence timed out\n");
+ 		release_crq_queue(adapter);
+ 		return -1;
+ 	}
+ 
+ 	rc = init_sub_crqs(adapter);
+ 	if (rc) {
+ 		dev_err(dev, "Initialization of sub crqs failed\n");
+ 		release_crq_queue(adapter);
+ 	}
+ 
+ 	return rc;
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  }
  
  static int ibmvnic_probe(struct vio_dev *dev, const struct vio_device_id *id)
@@@ -3838,76 -3470,24 +4289,86 @@@
  	netdev->ethtool_ops = &ibmvnic_ethtool_ops;
  	SET_NETDEV_DEV(netdev, &dev->dev);
  
- 	INIT_WORK(&adapter->vnic_crq_init, handle_crq_init_rsp);
- 	INIT_WORK(&adapter->ibmvnic_xport, ibmvnic_xport_event);
- 
  	spin_lock_init(&adapter->stats_lock);
  
++<<<<<<< HEAD
 +	rc = ibmvnic_init_crq_queue(adapter);
++=======
+ 	INIT_LIST_HEAD(&adapter->errors);
+ 	spin_lock_init(&adapter->error_list_lock);
+ 
+ 	INIT_WORK(&adapter->ibmvnic_reset, __ibmvnic_reset);
+ 	INIT_LIST_HEAD(&adapter->rwi_list);
+ 	mutex_init(&adapter->reset_lock);
+ 	mutex_init(&adapter->rwi_lock);
+ 	adapter->resetting = false;
+ 
+ 	rc = ibmvnic_init(adapter);
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  	if (rc) {
 -		free_netdev(netdev);
 -		return rc;
 +		dev_err(&dev->dev, "Couldn't initialize crq. rc=%d\n", rc);
 +		goto free_netdev;
 +	}
 +
 +	INIT_LIST_HEAD(&adapter->errors);
 +	INIT_LIST_HEAD(&adapter->inflight);
 +	spin_lock_init(&adapter->error_list_lock);
 +	spin_lock_init(&adapter->inflight_lock);
 +
 +	adapter->stats_token = dma_map_single(&dev->dev, &adapter->stats,
 +					      sizeof(struct ibmvnic_statistics),
 +					      DMA_FROM_DEVICE);
 +	if (dma_mapping_error(&dev->dev, adapter->stats_token)) {
 +		if (!firmware_has_feature(FW_FEATURE_CMO))
 +			dev_err(&dev->dev, "Couldn't map stats buffer\n");
 +		rc = -ENOMEM;
 +		goto free_crq;
  	}
  
 -	netdev->mtu = adapter->req_mtu - ETH_HLEN;
 +	snprintf(buf, sizeof(buf), "ibmvnic_%x", dev->unit_address);
 +	ent = debugfs_create_dir(buf, NULL);
 +	if (!ent || IS_ERR(ent)) {
 +		dev_info(&dev->dev, "debugfs create directory failed\n");
 +		adapter->debugfs_dir = NULL;
 +	} else {
 +		adapter->debugfs_dir = ent;
 +		ent = debugfs_create_file("dump", S_IRUGO, adapter->debugfs_dir,
 +					  netdev, &ibmvnic_dump_ops);
 +		if (!ent || IS_ERR(ent)) {
 +			dev_info(&dev->dev,
 +				 "debugfs create dump file failed\n");
 +			adapter->debugfs_dump = NULL;
 +		} else {
 +			adapter->debugfs_dump = ent;
 +		}
 +	}
 +
 +	init_completion(&adapter->init_done);
 +	ibmvnic_send_crq_init(adapter);
 +	if (!wait_for_completion_timeout(&adapter->init_done, timeout))
 +		return 0;
 +
 +	do {
 +		if (adapter->renegotiate) {
 +			adapter->renegotiate = false;
 +			release_sub_crqs_no_irqs(adapter);
 +
 +			reinit_completion(&adapter->init_done);
 +			send_cap_queries(adapter);
 +			if (!wait_for_completion_timeout(&adapter->init_done,
 +							 timeout))
 +				return 0;
 +		}
 +	} while (adapter->renegotiate);
 +
 +	rc = init_sub_crq_irqs(adapter);
 +	if (rc) {
 +		dev_err(&dev->dev, "failed to initialize sub crq irqs\n");
 +		goto free_debugfs;
 +	}
 +
 +	netdev->real_num_tx_queues = adapter->req_tx_queues;
 +	netdev->mtu = adapter->req_mtu;
  
  	rc = register_netdev(netdev);
  	if (rc) {
@@@ -3935,26 -3506,17 +4396,28 @@@ static int ibmvnic_remove(struct vio_de
  	struct net_device *netdev = dev_get_drvdata(&dev->dev);
  	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
  
 -	adapter->state = VNIC_REMOVING;
  	unregister_netdev(netdev);
+ 	mutex_lock(&adapter->reset_lock);
  
 -	release_resources(adapter);
  	release_sub_crqs(adapter);
 -	release_crq_queue(adapter);
  
 -	adapter->state = VNIC_REMOVED;
 +	ibmvnic_release_crq_queue(adapter);
 +
 +	if (adapter->debugfs_dir && !IS_ERR(adapter->debugfs_dir))
 +		debugfs_remove_recursive(adapter->debugfs_dir);
 +
 +	dma_unmap_single(&dev->dev, adapter->stats_token,
 +			 sizeof(struct ibmvnic_statistics), DMA_FROM_DEVICE);
 +
 +	if (adapter->ras_comps)
 +		dma_free_coherent(&dev->dev,
 +				  adapter->ras_comp_num *
 +				  sizeof(struct ibmvnic_fw_component),
 +				  adapter->ras_comps, adapter->ras_comps_tok);
 +
 +	kfree(adapter->ras_comp_int);
  
+ 	mutex_unlock(&adapter->reset_lock);
  	free_netdev(netdev);
  	dev_set_drvdata(&dev->dev, NULL);
  
diff --cc drivers/net/ethernet/ibm/ibmvnic.h
index 91a20189cdae,4702b48cfa44..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.h
+++ b/drivers/net/ethernet/ibm/ibmvnic.h
@@@ -926,18 -913,25 +926,28 @@@ struct ibmvnic_error_buff 
  	__be32 error_id;
  };
  
 -enum vnic_state {VNIC_PROBING = 1,
 -		 VNIC_PROBED,
 -		 VNIC_OPENING,
 -		 VNIC_OPEN,
 -		 VNIC_CLOSING,
 -		 VNIC_CLOSED,
 -		 VNIC_REMOVING,
 -		 VNIC_REMOVED};
 +struct ibmvnic_fw_comp_internal {
 +	struct ibmvnic_adapter *adapter;
 +	int num;
 +	struct debugfs_blob_wrapper desc_blob;
 +	int paused;
 +};
 +
 +struct ibmvnic_inflight_cmd {
 +	union ibmvnic_crq crq;
 +	struct list_head list;
 +};
  
+ enum ibmvnic_reset_reason {VNIC_RESET_FAILOVER = 1,
+ 			   VNIC_RESET_MOBILITY,
+ 			   VNIC_RESET_FATAL,
+ 			   VNIC_RESET_TIMEOUT};
+ 
+ struct ibmvnic_rwi {
+ 	enum ibmvnic_reset_reason reset_reason;
+ 	struct list_head list;
+ };
+ 
  struct ibmvnic_adapter {
  	struct vio_dev *vdev;
  	struct net_device *netdev;
@@@ -947,11 -941,7 +957,10 @@@
  	dma_addr_t ip_offload_tok;
  	struct ibmvnic_control_ip_offload_buffer ip_offload_ctrl;
  	dma_addr_t ip_offload_ctrl_tok;
- 	bool migrated;
  	u32 msg_enable;
 +	void *bounce_buffer;
 +	int bounce_buffer_size;
 +	dma_addr_t bounce_buffer_dma;
  
  	/* Statistics */
  	struct ibmvnic_statistics stats;
@@@ -1049,8 -1024,11 +1058,15 @@@
  	__be64 tx_rx_desc_req;
  	u8 map_id;
  
- 	struct work_struct vnic_crq_init;
- 	struct work_struct ibmvnic_xport;
  	struct tasklet_struct tasklet;
++<<<<<<< HEAD
 +	bool failover;
++=======
+ 	enum vnic_state state;
+ 	enum ibmvnic_reset_reason reset_reason;
+ 	struct mutex reset_lock, rwi_lock;
+ 	struct list_head rwi_list;
+ 	struct work_struct ibmvnic_reset;
+ 	bool resetting;
++>>>>>>> ed651a10875f (ibmvnic: Updated reset handling)
  };
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.c
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.h
