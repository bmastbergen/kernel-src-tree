i40e/i40evf: Add support for using order 1 pages with a 3K buffer

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Alexander Duyck <alexander.h.duyck@intel.com>
commit 98efd69493b9d4b02353a552af8ffaaf30de8af4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/98efd694.failed

There are situations where adding padding to the front and back of an Rx
buffer will require that we add additional padding.  Specifically if
NET_IP_ALIGN is non-zero, or the MTU size is larger than 7.5K we would need
to use 2K buffers which leaves us with no room for the padding.

To preemptively address these cases I am adding support for 3K buffers to
the Rx path so that we can provide the additional padding needed in the
event of NET_IP_ALIGN being non-zero or a cache line being greater than 64.

Change-ID: I938bc1ba611285428df39a613cd66f98e60b55c7
	Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
	Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
	Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
(cherry picked from commit 98efd69493b9d4b02353a552af8ffaaf30de8af4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/i40e/i40e_main.c
#	drivers/net/ethernet/intel/i40e/i40e_txrx.h
#	drivers/net/ethernet/intel/i40evf/i40e_txrx.h
#	drivers/net/ethernet/intel/i40evf/i40evf_main.c
diff --cc drivers/net/ethernet/intel/i40e/i40e_main.c
index c5dd4cebbb6d,97489d69029a..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_main.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_main.c
@@@ -3173,27 -3075,20 +3173,42 @@@ static int i40e_vsi_configure_rx(struc
  	int err = 0;
  	u16 i;
  
++<<<<<<< HEAD
 +	if (vsi->netdev && (vsi->netdev->mtu > ETH_DATA_LEN))
 +		vsi->max_frame = vsi->netdev->mtu + ETH_HLEN
 +			       + ETH_FCS_LEN + VLAN_HLEN;
 +	else
 +		vsi->max_frame = I40E_RXBUFFER_2048;
 +
 +	vsi->rx_buf_len = I40E_RXBUFFER_2048;
 +
 +#ifdef I40E_FCOE
 +	/* setup rx buffer for FCoE */
 +	if ((vsi->type == I40E_VSI_FCOE) &&
 +	    (vsi->back->flags & I40E_FLAG_FCOE_ENABLED)) {
 +		vsi->rx_buf_len = I40E_RXBUFFER_3072;
 +		vsi->max_frame = I40E_RXBUFFER_3072;
++=======
+ 	if (!vsi->netdev || (vsi->back->flags & I40E_FLAG_LEGACY_RX)) {
+ 		vsi->max_frame = I40E_MAX_RXBUFFER;
+ 		vsi->rx_buf_len = I40E_RXBUFFER_2048;
+ #if (PAGE_SIZE < 8192)
+ 	} else if (vsi->netdev->mtu <= ETH_DATA_LEN) {
+ 		vsi->max_frame = I40E_RXBUFFER_1536 - NET_IP_ALIGN;
+ 		vsi->rx_buf_len = I40E_RXBUFFER_1536 - NET_IP_ALIGN;
+ #endif
+ 	} else {
+ 		vsi->max_frame = I40E_MAX_RXBUFFER;
+ 		vsi->rx_buf_len = (PAGE_SIZE < 8192) ? I40E_RXBUFFER_3072 :
+ 						       I40E_RXBUFFER_2048;
++>>>>>>> 98efd69493b9 (i40e/i40evf: Add support for using order 1 pages with a 3K buffer)
  	}
  
 +#endif /* I40E_FCOE */
 +	/* round up for the chip's needs */
 +	vsi->rx_buf_len = ALIGN(vsi->rx_buf_len,
 +				BIT_ULL(I40E_RXQ_CTX_DBUFF_SHIFT));
 +
  	/* set up individual rings */
  	for (i = 0; i < vsi->num_queue_pairs && !err; i++)
  		err = i40e_configure_rx_ring(vsi->rx_rings[i]);
diff --cc drivers/net/ethernet/intel/i40e/i40e_txrx.h
index 94a734a0a3cf,2f618539a436..000000000000
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.h
@@@ -117,10 -117,9 +117,14 @@@ enum i40e_dyn_idx_t 
  
  /* Supported Rx Buffer Sizes (a multiple of 128) */
  #define I40E_RXBUFFER_256   256
 -#define I40E_RXBUFFER_1536  1536  /* 128B aligned standard Ethernet frame */
  #define I40E_RXBUFFER_2048  2048
++<<<<<<< HEAD
 +#define I40E_RXBUFFER_3072  3072   /* For FCoE MTU of 2158 */
 +#define I40E_RXBUFFER_4096  4096
 +#define I40E_RXBUFFER_8192  8192
++=======
+ #define I40E_RXBUFFER_3072  3072  /* Used for large frames w/ padding */
++>>>>>>> 98efd69493b9 (i40e/i40evf: Add support for using order 1 pages with a 3K buffer)
  #define I40E_MAX_RXBUFFER   9728  /* largest size for single descriptor */
  
  /* NOTE: netdev_alloc_skb reserves up to 64 bytes, NET_IP_ALIGN means we
diff --cc drivers/net/ethernet/intel/i40evf/i40e_txrx.h
index 8b6000110ab2,dc82f65267ec..000000000000
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.h
@@@ -104,10 -104,9 +104,14 @@@ enum i40e_dyn_idx_t 
  
  /* Supported Rx Buffer Sizes (a multiple of 128) */
  #define I40E_RXBUFFER_256   256
 -#define I40E_RXBUFFER_1536  1536  /* 128B aligned standard Ethernet frame */
  #define I40E_RXBUFFER_2048  2048
++<<<<<<< HEAD
 +#define I40E_RXBUFFER_3072  3072   /* For FCoE MTU of 2158 */
 +#define I40E_RXBUFFER_4096  4096
 +#define I40E_RXBUFFER_8192  8192
++=======
+ #define I40E_RXBUFFER_3072  3072  /* Used for large frames w/ padding */
++>>>>>>> 98efd69493b9 (i40e/i40evf: Add support for using order 1 pages with a 3K buffer)
  #define I40E_MAX_RXBUFFER   9728  /* largest size for single descriptor */
  
  /* NOTE: netdev_alloc_skb reserves up to 64 bytes, NET_IP_ALIGN means we
diff --cc drivers/net/ethernet/intel/i40evf/i40evf_main.c
index 4df10e9d6551,7d00abae6104..000000000000
--- a/drivers/net/ethernet/intel/i40evf/i40evf_main.c
+++ b/drivers/net/ethernet/intel/i40evf/i40evf_main.c
@@@ -689,9 -689,29 +689,30 @@@ static void i40evf_configure_rx(struct 
  	struct i40e_hw *hw = &adapter->hw;
  	int i;
  
++<<<<<<< HEAD
++=======
+ 	/* Legacy Rx will always default to a 2048 buffer size. */
+ #if (PAGE_SIZE < 8192)
+ 	if (!(adapter->flags & I40EVF_FLAG_LEGACY_RX)) {
+ 		/* For jumbo frames on systems with 4K pages we have to use
+ 		 * an order 1 page, so we might as well increase the size
+ 		 * of our Rx buffer to make better use of the available space
+ 		 */
+ 		rx_buf_len = I40E_RXBUFFER_3072;
+ 
+ 		/* We use a 1536 buffer size for configurations with
+ 		 * standard Ethernet mtu.  On x86 this gives us enough room
+ 		 * for shared info and 192 bytes of padding.
+ 		 */
+ 		if (netdev->mtu <= ETH_DATA_LEN)
+ 			rx_buf_len = I40E_RXBUFFER_1536 - NET_IP_ALIGN;
+ 	}
+ #endif
+ 
++>>>>>>> 98efd69493b9 (i40e/i40evf: Add support for using order 1 pages with a 3K buffer)
  	for (i = 0; i < adapter->num_active_queues; i++) {
  		adapter->rx_rings[i].tail = hw->hw_addr + I40E_QRX_TAIL1(i);
 -		adapter->rx_rings[i].rx_buf_len = rx_buf_len;
 +		adapter->rx_rings[i].rx_buf_len = I40EVF_RXBUFFER_2048;
  	}
  }
  
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_main.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 8a84caf5ca7b..02a370a18f35 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -1145,14 +1145,15 @@ void i40e_clean_rx_ring(struct i40e_ring *rx_ring)
 		dma_sync_single_range_for_cpu(rx_ring->dev,
 					      rx_bi->dma,
 					      rx_bi->page_offset,
-					      I40E_RXBUFFER_2048,
+					      rx_ring->rx_buf_len,
 					      DMA_FROM_DEVICE);
 
 		/* free resources associated with mapping */
 		dma_unmap_page_attrs(rx_ring->dev, rx_bi->dma,
-				     PAGE_SIZE,
+				     i40e_rx_pg_size(rx_ring),
 				     DMA_FROM_DEVICE,
 				     I40E_RX_DMA_ATTR);
+
 		__page_frag_cache_drain(rx_bi->page, rx_bi->pagecnt_bias);
 
 		rx_bi->page = NULL;
@@ -1274,7 +1275,7 @@ static bool i40e_alloc_mapped_page(struct i40e_ring *rx_ring,
 	}
 
 	/* alloc new page for storage */
-	page = dev_alloc_page();
+	page = dev_alloc_pages(i40e_rx_pg_order(rx_ring));
 	if (unlikely(!page)) {
 		rx_ring->rx_stats.alloc_page_failed++;
 		return false;
@@ -1282,7 +1283,7 @@ static bool i40e_alloc_mapped_page(struct i40e_ring *rx_ring,
 
 	/* map page for use */
 	dma = dma_map_page_attrs(rx_ring->dev, page, 0,
-				 PAGE_SIZE,
+				 i40e_rx_pg_size(rx_ring),
 				 DMA_FROM_DEVICE,
 				 I40E_RX_DMA_ATTR);
 
@@ -1290,7 +1291,7 @@ static bool i40e_alloc_mapped_page(struct i40e_ring *rx_ring,
 	 * there isn't much point in holding memory we can't use
 	 */
 	if (dma_mapping_error(rx_ring->dev, dma)) {
-		__free_pages(page, 0);
+		__free_pages(page, i40e_rx_pg_order(rx_ring));
 		rx_ring->rx_stats.alloc_page_failed++;
 		return false;
 	}
@@ -1350,7 +1351,7 @@ bool i40e_alloc_rx_buffers(struct i40e_ring *rx_ring, u16 cleaned_count)
 		/* sync the buffer for use by the device */
 		dma_sync_single_range_for_device(rx_ring->dev, bi->dma,
 						 bi->page_offset,
-						 I40E_RXBUFFER_2048,
+						 rx_ring->rx_buf_len,
 						 DMA_FROM_DEVICE);
 
 		/* Refresh the desc even if buffer_addrs didn't change
@@ -1652,9 +1653,6 @@ static inline bool i40e_page_is_reusable(struct page *page)
  **/
 static bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer)
 {
-#if (PAGE_SIZE >= 8192)
-	unsigned int last_offset = PAGE_SIZE - I40E_RXBUFFER_2048;
-#endif
 	unsigned int pagecnt_bias = rx_buffer->pagecnt_bias;
 	struct page *page = rx_buffer->page;
 
@@ -1667,7 +1665,9 @@ static bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer)
 	if (unlikely((page_count(page) - pagecnt_bias) > 1))
 		return false;
 #else
-	if (rx_buffer->page_offset > last_offset)
+#define I40E_LAST_OFFSET \
+	(SKB_WITH_OVERHEAD(PAGE_SIZE) - I40E_RXBUFFER_2048)
+	if (rx_buffer->page_offset > I40E_LAST_OFFSET)
 		return false;
 #endif
 
@@ -1701,7 +1701,7 @@ static void i40e_add_rx_frag(struct i40e_ring *rx_ring,
 			     unsigned int size)
 {
 #if (PAGE_SIZE < 8192)
-	unsigned int truesize = I40E_RXBUFFER_2048;
+	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
 #else
 	unsigned int truesize = SKB_DATA_ALIGN(size);
 #endif
@@ -1762,7 +1762,7 @@ static struct sk_buff *i40e_construct_skb(struct i40e_ring *rx_ring,
 {
 	void *va = page_address(rx_buffer->page) + rx_buffer->page_offset;
 #if (PAGE_SIZE < 8192)
-	unsigned int truesize = I40E_RXBUFFER_2048;
+	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
 #else
 	unsigned int truesize = SKB_DATA_ALIGN(size);
 #endif
@@ -1828,7 +1828,8 @@ static void i40e_put_rx_buffer(struct i40e_ring *rx_ring,
 		rx_ring->rx_stats.page_reuse_count++;
 	} else {
 		/* we are not reusing the buffer so unmap it */
-		dma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma, PAGE_SIZE,
+		dma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,
+				     i40e_rx_pg_size(rx_ring),
 				     DMA_FROM_DEVICE, I40E_RX_DMA_ATTR);
 		__page_frag_cache_drain(rx_buffer->page,
 					rx_buffer->pagecnt_bias);
* Unmerged path drivers/net/ethernet/intel/i40e/i40e_txrx.h
diff --git a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
index ece8df17d661..6a4c63d5d0a5 100644
--- a/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40evf/i40e_txrx.c
@@ -509,14 +509,15 @@ void i40evf_clean_rx_ring(struct i40e_ring *rx_ring)
 		dma_sync_single_range_for_cpu(rx_ring->dev,
 					      rx_bi->dma,
 					      rx_bi->page_offset,
-					      I40E_RXBUFFER_2048,
+					      rx_ring->rx_buf_len,
 					      DMA_FROM_DEVICE);
 
 		/* free resources associated with mapping */
 		dma_unmap_page_attrs(rx_ring->dev, rx_bi->dma,
-				     PAGE_SIZE,
+				     i40e_rx_pg_size(rx_ring),
 				     DMA_FROM_DEVICE,
 				     I40E_RX_DMA_ATTR);
+
 		__page_frag_cache_drain(rx_bi->page, rx_bi->pagecnt_bias);
 
 		rx_bi->page = NULL;
@@ -638,7 +639,7 @@ static bool i40e_alloc_mapped_page(struct i40e_ring *rx_ring,
 	}
 
 	/* alloc new page for storage */
-	page = dev_alloc_page();
+	page = dev_alloc_pages(i40e_rx_pg_order(rx_ring));
 	if (unlikely(!page)) {
 		rx_ring->rx_stats.alloc_page_failed++;
 		return false;
@@ -646,7 +647,7 @@ static bool i40e_alloc_mapped_page(struct i40e_ring *rx_ring,
 
 	/* map page for use */
 	dma = dma_map_page_attrs(rx_ring->dev, page, 0,
-				 PAGE_SIZE,
+				 i40e_rx_pg_size(rx_ring),
 				 DMA_FROM_DEVICE,
 				 I40E_RX_DMA_ATTR);
 
@@ -654,7 +655,7 @@ static bool i40e_alloc_mapped_page(struct i40e_ring *rx_ring,
 	 * there isn't much point in holding memory we can't use
 	 */
 	if (dma_mapping_error(rx_ring->dev, dma)) {
-		__free_pages(page, 0);
+		__free_pages(page, i40e_rx_pg_order(rx_ring));
 		rx_ring->rx_stats.alloc_page_failed++;
 		return false;
 	}
@@ -714,7 +715,7 @@ bool i40evf_alloc_rx_buffers(struct i40e_ring *rx_ring, u16 cleaned_count)
 		/* sync the buffer for use by the device */
 		dma_sync_single_range_for_device(rx_ring->dev, bi->dma,
 						 bi->page_offset,
-						 I40E_RXBUFFER_2048,
+						 rx_ring->rx_buf_len,
 						 DMA_FROM_DEVICE);
 
 		/* Refresh the desc even if buffer_addrs didn't change
@@ -1006,9 +1007,6 @@ static inline bool i40e_page_is_reusable(struct page *page)
  **/
 static bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer)
 {
-#if (PAGE_SIZE >= 8192)
-	unsigned int last_offset = PAGE_SIZE - I40E_RXBUFFER_2048;
-#endif
 	unsigned int pagecnt_bias = rx_buffer->pagecnt_bias;
 	struct page *page = rx_buffer->page;
 
@@ -1021,7 +1019,9 @@ static bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer)
 	if (unlikely((page_count(page) - pagecnt_bias) > 1))
 		return false;
 #else
-	if (rx_buffer->page_offset > last_offset)
+#define I40E_LAST_OFFSET \
+	(SKB_WITH_OVERHEAD(PAGE_SIZE) - I40E_RXBUFFER_2048)
+	if (rx_buffer->page_offset > I40E_LAST_OFFSET)
 		return false;
 #endif
 
@@ -1055,7 +1055,7 @@ static void i40e_add_rx_frag(struct i40e_ring *rx_ring,
 			     unsigned int size)
 {
 #if (PAGE_SIZE < 8192)
-	unsigned int truesize = I40E_RXBUFFER_2048;
+	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
 #else
 	unsigned int truesize = SKB_DATA_ALIGN(size);
 #endif
@@ -1116,7 +1116,7 @@ static struct sk_buff *i40e_construct_skb(struct i40e_ring *rx_ring,
 {
 	void *va = page_address(rx_buffer->page) + rx_buffer->page_offset;
 #if (PAGE_SIZE < 8192)
-	unsigned int truesize = I40E_RXBUFFER_2048;
+	unsigned int truesize = i40e_rx_pg_size(rx_ring) / 2;
 #else
 	unsigned int truesize = SKB_DATA_ALIGN(size);
 #endif
@@ -1182,7 +1182,8 @@ static void i40e_put_rx_buffer(struct i40e_ring *rx_ring,
 		rx_ring->rx_stats.page_reuse_count++;
 	} else {
 		/* we are not reusing the buffer so unmap it */
-		dma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma, PAGE_SIZE,
+		dma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,
+				     i40e_rx_pg_size(rx_ring),
 				     DMA_FROM_DEVICE, I40E_RX_DMA_ATTR);
 		__page_frag_cache_drain(rx_buffer->page,
 					rx_buffer->pagecnt_bias);
* Unmerged path drivers/net/ethernet/intel/i40evf/i40e_txrx.h
* Unmerged path drivers/net/ethernet/intel/i40evf/i40evf_main.c
