net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [netdrv] mlx5e: Use hard_mtu as part of the mlx5e_priv struct (Don Dutile) [1499363 1385325]
Rebuild_FUZZ: 96.30%
commit-author Erez Shitrit <erezsh@mellanox.com>
commit c139dbfddd2c7848550ed06345060aa87701e818
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/c139dbfd.failed

The mtu extra space that kept for the HW is specific for each link type,
and it is different in mlx5e and mlx5i modules.
Now it is kept in the priv structures, set by the mlx5e/mlx5i driver
accordingly.

	Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
(cherry picked from commit c139dbfddd2c7848550ed06345060aa87701e818)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
#	drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
#	drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index efce1dc691aa,318ef7f0292f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -52,6 -52,11 +52,14 @@@
  
  #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
  
++<<<<<<< HEAD
++=======
+ #define MLX5E_ETH_HARD_MTU (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN)
+ 
+ #define MLX5E_HW2SW_MTU(priv, hwmtu) ((hwmtu) - ((priv)->hard_mtu))
+ #define MLX5E_SW2HW_MTU(priv, swmtu) ((swmtu) + ((priv)->hard_mtu))
+ 
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
  #define MLX5E_MAX_NUM_TC	8
  
  #define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index e2c448703ea8,de1e936fc2be..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -607,9 -625,17 +607,23 @@@ static int mlx5e_create_rq(struct mlx5e
  		rq->alloc_wqe = mlx5e_alloc_rx_wqe;
  		rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
  
++<<<<<<< HEAD
 +		rq->buff.wqe_sz = (priv->params.lro_en) ?
 +				priv->params.lro_wqe_sz :
 +				MLX5E_SW2HW_MTU(priv->netdev->mtu);
++=======
+ 		rq->handle_rx_cqe = c->priv->profile->rx_handlers.handle_rx_cqe;
+ 		if (!rq->handle_rx_cqe) {
+ 			kfree(rq->dma_info);
+ 			err = -EINVAL;
+ 			netdev_err(c->netdev, "RX handler of RQ is not set, err %d\n", err);
+ 			goto err_rq_wq_destroy;
+ 		}
+ 
+ 		rq->buff.wqe_sz = params->lro_en  ?
+ 				params->lro_wqe_sz :
+ 				MLX5E_SW2HW_MTU(c->priv, c->netdev->mtu);
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
  		byte_count = rq->buff.wqe_sz;
  
  		/* calc the required page order */
@@@ -2152,12 -2490,12 +2166,12 @@@ static void mlx5e_query_mtu(struct mlx5
  	if (err || !hw_mtu) /* fallback to port oper mtu */
  		mlx5_query_port_oper_mtu(mdev, &hw_mtu, 1);
  
- 	*mtu = MLX5E_HW2SW_MTU(hw_mtu);
+ 	*mtu = MLX5E_HW2SW_MTU(priv, hw_mtu);
  }
  
 -static int mlx5e_set_dev_port_mtu(struct mlx5e_priv *priv)
 +static int mlx5e_set_dev_port_mtu(struct net_device *netdev)
  {
 -	struct net_device *netdev = priv->netdev;
 +	struct mlx5e_priv *priv = netdev_priv(netdev);
  	u16 mtu;
  	int err;
  
@@@ -3309,69 -3843,83 +3323,143 @@@ u32 mlx5e_choose_lro_timeout(struct mlx
  	return MLX5_CAP_ETH(mdev, lro_timer_supported_periods[i]);
  }
  
++<<<<<<< HEAD
++=======
+ void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
+ 			    struct mlx5e_params *params,
+ 			    u16 max_channels)
+ {
+ 	u8 cq_period_mode = 0;
+ 	u32 link_speed = 0;
+ 	u32 pci_bw = 0;
+ 
+ 	params->num_channels = max_channels;
+ 	params->num_tc       = 1;
+ 
+ 	mlx5e_get_max_linkspeed(mdev, &link_speed);
+ 	mlx5e_get_pci_bw(mdev, &pci_bw);
+ 	mlx5_core_dbg(mdev, "Max link speed = %d, PCI BW = %d\n",
+ 		      link_speed, pci_bw);
+ 
+ 	/* SQ */
+ 	params->log_sq_size = is_kdump_kernel() ?
+ 		MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE :
+ 		MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
+ 
+ 	/* set CQE compression */
+ 	params->rx_cqe_compress_def = false;
+ 	if (MLX5_CAP_GEN(mdev, cqe_compression) &&
+ 	    MLX5_CAP_GEN(mdev, vport_group_manager))
+ 		params->rx_cqe_compress_def = cqe_compress_heuristic(link_speed, pci_bw);
+ 
+ 	MLX5E_SET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS, params->rx_cqe_compress_def);
+ 
+ 	/* RQ */
+ 	mlx5e_set_rq_params(mdev, params);
+ 
+ 	/* HW LRO */
+ 
+ 	/* TODO: && MLX5_CAP_ETH(mdev, lro_cap) */
+ 	if (params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ)
+ 		params->lro_en = hw_lro_heuristic(link_speed, pci_bw);
+ 	params->lro_timeout = mlx5e_choose_lro_timeout(mdev, MLX5E_DEFAULT_LRO_TIMEOUT);
+ 
+ 	/* CQ moderation params */
+ 	cq_period_mode = MLX5_CAP_GEN(mdev, cq_period_start_from_cqe) ?
+ 			MLX5_CQ_PERIOD_MODE_START_FROM_CQE :
+ 			MLX5_CQ_PERIOD_MODE_START_FROM_EQE;
+ 	params->rx_am_enabled = MLX5_CAP_GEN(mdev, cq_moderation);
+ 	mlx5e_set_rx_cq_mode_params(params, cq_period_mode);
+ 
+ 	params->tx_cq_moderation.usec = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC;
+ 	params->tx_cq_moderation.pkts = MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS;
+ 
+ 	/* TX inline */
+ 	params->tx_max_inline = mlx5e_get_max_inline_cap(mdev);
+ 	mlx5_query_min_inline(mdev, &params->tx_min_inline_mode);
+ 	if (params->tx_min_inline_mode == MLX5_INLINE_MODE_NONE &&
+ 	    !MLX5_CAP_ETH(mdev, wqe_vlan_insert))
+ 		params->tx_min_inline_mode = MLX5_INLINE_MODE_L2;
+ 
+ 	/* RSS */
+ 	params->rss_hfunc = ETH_RSS_HASH_XOR;
+ 	netdev_rss_key_fill(params->toeplitz_hash_key, sizeof(params->toeplitz_hash_key));
+ 	mlx5e_build_default_indir_rqt(mdev, params->indirection_rqt,
+ 				      MLX5E_INDIR_RQT_SIZE, max_channels);
+ }
+ 
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
  static void mlx5e_build_nic_netdev_priv(struct mlx5_core_dev *mdev,
  					struct net_device *netdev,
  					const struct mlx5e_profile *profile,
  					void *ppriv)
  {
  	struct mlx5e_priv *priv = netdev_priv(netdev);
 -
 +	u32 link_speed = 0;
 +	u32 pci_bw = 0;
 +	u8 cq_period_mode = MLX5_CAP_GEN(mdev, cq_period_start_from_cqe) ?
 +					 MLX5_CQ_PERIOD_MODE_START_FROM_CQE :
 +					 MLX5_CQ_PERIOD_MODE_START_FROM_EQE;
 +
++<<<<<<< HEAD
 +	priv->mdev                         = mdev;
 +	priv->netdev                       = netdev;
 +	priv->params.num_channels          = profile->max_nch(mdev);
 +	priv->profile                      = profile;
 +	priv->ppriv                        = ppriv;
++=======
+ 	priv->mdev        = mdev;
+ 	priv->netdev      = netdev;
+ 	priv->profile     = profile;
+ 	priv->ppriv       = ppriv;
+ 	priv->hard_mtu = MLX5E_ETH_HARD_MTU;
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
 +
 +	priv->params.lro_timeout =
 +		mlx5e_choose_lro_timeout(mdev, MLX5E_DEFAULT_LRO_TIMEOUT);
 +
 +	priv->params.log_sq_size = MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
 +
 +	/* set CQE compression */
 +	priv->params.rx_cqe_compress_def = false;
 +	if (MLX5_CAP_GEN(mdev, cqe_compression) &&
 +	    MLX5_CAP_GEN(mdev, vport_group_manager)) {
 +		mlx5e_get_max_linkspeed(mdev, &link_speed);
 +		mlx5e_get_pci_bw(mdev, &pci_bw);
 +		mlx5_core_dbg(mdev, "Max link speed = %d, PCI BW = %d\n",
 +			      link_speed, pci_bw);
 +		priv->params.rx_cqe_compress_def =
 +			cqe_compress_heuristic(link_speed, pci_bw);
 +	}
 +
 +	MLX5E_SET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_COMPRESS,
 +			priv->params.rx_cqe_compress_def);
 +
 +	mlx5e_set_rq_priv_params(priv);
 +	if (priv->params.rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ)
 +		priv->params.lro_en = true;
 +
 +	priv->params.rx_am_enabled = MLX5_CAP_GEN(mdev, cq_moderation);
 +	mlx5e_set_rx_cq_mode_params(&priv->params, cq_period_mode);
 +
 +	priv->params.tx_cq_moderation.usec =
 +		MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC;
 +	priv->params.tx_cq_moderation.pkts =
 +		MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS;
 +	priv->params.tx_max_inline         = mlx5e_get_max_inline_cap(mdev);
 +	mlx5_query_min_inline(mdev, &priv->params.tx_min_inline_mode);
 +	priv->params.num_tc                = 1;
 +	priv->params.rss_hfunc             = ETH_RSS_HASH_XOR;
 +
 +	netdev_rss_key_fill(priv->params.toeplitz_hash_key,
 +			    sizeof(priv->params.toeplitz_hash_key));
  
 -	mlx5e_build_nic_params(mdev, &priv->channels.params, profile->max_nch(mdev));
 +	mlx5e_build_default_indir_rqt(mdev, priv->params.indirection_rqt,
 +				      MLX5E_INDIR_RQT_SIZE, profile->max_nch(mdev));
 +
 +	/* Initialize pflags */
 +	MLX5E_SET_PFLAG(priv, MLX5E_PFLAG_RX_CQE_BASED_MODER,
 +			priv->params.rx_cq_period_mode == MLX5_CQ_PERIOD_MODE_START_FROM_CQE);
  
  	mutex_init(&priv->state_lock);
  
@@@ -3613,8 -4156,15 +3701,20 @@@ static void mlx5e_nic_enable(struct mlx
  {
  	struct net_device *netdev = priv->netdev;
  	struct mlx5_core_dev *mdev = priv->mdev;
++<<<<<<< HEAD
 +	struct mlx5_eswitch *esw = mdev->priv.eswitch;
 +	struct mlx5_eswitch_rep rep;
++=======
+ 	u16 max_mtu;
+ 
+ 	mlx5e_init_l2_addr(priv);
+ 
+ 	/* MTU range: 68 - hw-specific max */
+ 	netdev->min_mtu = ETH_MIN_MTU;
+ 	mlx5_query_port_max_mtu(priv->mdev, &max_mtu, 1);
+ 	netdev->max_mtu = MLX5E_HW2SW_MTU(priv, max_mtu);
+ 	mlx5e_set_dev_port_mtu(priv);
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
  
  	mlx5_lag_add(mdev, netdev);
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
index abcb1976163d,76cb6ad6e00b..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
@@@ -462,7 -821,22 +462,26 @@@ static void mlx5e_init_rep(struct mlx5_
  			   const struct mlx5e_profile *profile,
  			   void *ppriv)
  {
++<<<<<<< HEAD
 +	mlx5e_build_rep_netdev_priv(mdev, netdev, profile, ppriv);
++=======
+ 	struct mlx5e_priv *priv = netdev_priv(netdev);
+ 
+ 	priv->mdev                         = mdev;
+ 	priv->netdev                       = netdev;
+ 	priv->profile                      = profile;
+ 	priv->ppriv                        = ppriv;
+ 
+ 	mutex_init(&priv->state_lock);
+ 
+ 	INIT_DELAYED_WORK(&priv->update_stats_work, mlx5e_update_stats_work);
+ 
+ 	priv->channels.params.num_channels = profile->max_nch(mdev);
+ 
+ 	priv->hard_mtu = MLX5E_ETH_HARD_MTU;
+ 
+ 	mlx5e_build_rep_params(mdev, &priv->channels.params);
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
  	mlx5e_build_rep_netdev(netdev);
  }
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 8b4b5a3808c1,da42d55939ed..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -637,6 -617,120 +637,123 @@@ static inline void mlx5e_complete_rx_cq
  	mlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);
  }
  
++<<<<<<< HEAD
++=======
+ static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
+ {
+ 	struct mlx5_wq_cyc *wq = &sq->wq;
+ 	struct mlx5e_tx_wqe *wqe;
+ 	u16 pi = (sq->pc - 1) & wq->sz_m1; /* last pi */
+ 
+ 	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 
+ 	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &wqe->ctrl);
+ }
+ 
+ static inline bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq,
+ 					struct mlx5e_dma_info *di,
+ 					const struct xdp_buff *xdp)
+ {
+ 	struct mlx5e_xdpsq       *sq   = &rq->xdpsq;
+ 	struct mlx5_wq_cyc       *wq   = &sq->wq;
+ 	u16                       pi   = sq->pc & wq->sz_m1;
+ 	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+ 
+ 	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+ 	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+ 	struct mlx5_wqe_data_seg *dseg;
+ 
+ 	ptrdiff_t data_offset = xdp->data - xdp->data_hard_start;
+ 	dma_addr_t dma_addr  = di->addr + data_offset;
+ 	unsigned int dma_len = xdp->data_end - xdp->data;
+ 
+ 	prefetchw(wqe);
+ 
+ 	if (unlikely(dma_len < MLX5E_XDP_MIN_INLINE ||
+ 		     MLX5E_SW2HW_MTU(rq->channel->priv, rq->netdev->mtu) < dma_len)) {
+ 		rq->stats.xdp_drop++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return false;
+ 	}
+ 
+ 	if (unlikely(!mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, 1))) {
+ 		if (sq->db.doorbell) {
+ 			/* SQ is full, ring doorbell */
+ 			mlx5e_xmit_xdp_doorbell(sq);
+ 			sq->db.doorbell = false;
+ 		}
+ 		rq->stats.xdp_tx_full++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return false;
+ 	}
+ 
+ 	dma_sync_single_for_device(sq->pdev, dma_addr, dma_len, PCI_DMA_TODEVICE);
+ 
+ 	cseg->fm_ce_se = 0;
+ 
+ 	dseg = (struct mlx5_wqe_data_seg *)eseg + 1;
+ 
+ 	/* copy the inline part if required */
+ 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {
+ 		memcpy(eseg->inline_hdr.start, xdp->data, MLX5E_XDP_MIN_INLINE);
+ 		eseg->inline_hdr.sz = cpu_to_be16(MLX5E_XDP_MIN_INLINE);
+ 		dma_len  -= MLX5E_XDP_MIN_INLINE;
+ 		dma_addr += MLX5E_XDP_MIN_INLINE;
+ 		dseg++;
+ 	}
+ 
+ 	/* write the dma part */
+ 	dseg->addr       = cpu_to_be64(dma_addr);
+ 	dseg->byte_count = cpu_to_be32(dma_len);
+ 
+ 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_SEND);
+ 
+ 	sq->db.di[pi] = *di;
+ 	sq->pc++;
+ 
+ 	sq->db.doorbell = true;
+ 	rq->stats.xdp_tx++;
+ 	return true;
+ }
+ 
+ /* returns true if packet was consumed by xdp */
+ static inline int mlx5e_xdp_handle(struct mlx5e_rq *rq,
+ 				   struct mlx5e_dma_info *di,
+ 				   void *va, u16 *rx_headroom, u32 *len)
+ {
+ 	const struct bpf_prog *prog = READ_ONCE(rq->xdp_prog);
+ 	struct xdp_buff xdp;
+ 	u32 act;
+ 
+ 	if (!prog)
+ 		return false;
+ 
+ 	xdp.data = va + *rx_headroom;
+ 	xdp.data_end = xdp.data + *len;
+ 	xdp.data_hard_start = va;
+ 
+ 	act = bpf_prog_run_xdp(prog, &xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		*rx_headroom = xdp.data - xdp.data_hard_start;
+ 		*len = xdp.data_end - xdp.data;
+ 		return false;
+ 	case XDP_TX:
+ 		if (unlikely(!mlx5e_xmit_xdp_frame(rq, di, &xdp)))
+ 			trace_xdp_exception(rq->netdev, prog, act);
+ 		return true;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(act);
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(rq->netdev, prog, act);
+ 	case XDP_DROP:
+ 		rq->stats.xdp_drop++;
+ 		mlx5e_page_release(rq, di, true);
+ 		return true;
+ 	}
+ }
+ 
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
  static inline
  struct sk_buff *skb_from_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
  			     u16 wqe_counter, u32 cqe_bcnt)
@@@ -853,3 -963,154 +970,157 @@@ int mlx5e_poll_rx_cq(struct mlx5e_cq *c
  
  	return work_done;
  }
++<<<<<<< HEAD
++=======
+ 
+ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)
+ {
+ 	struct mlx5e_xdpsq *sq;
+ 	struct mlx5e_rq *rq;
+ 	u16 sqcc;
+ 	int i;
+ 
+ 	sq = container_of(cq, struct mlx5e_xdpsq, cq);
+ 
+ 	if (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))
+ 		return false;
+ 
+ 	rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 
+ 	/* sq->cc must be updated only after mlx5_cqwq_update_db_record(),
+ 	 * otherwise a cq overrun may occur
+ 	 */
+ 	sqcc = sq->cc;
+ 
+ 	for (i = 0; i < MLX5E_TX_CQ_POLL_BUDGET; i++) {
+ 		struct mlx5_cqe64 *cqe;
+ 		u16 wqe_counter;
+ 		bool last_wqe;
+ 
+ 		cqe = mlx5e_get_cqe(cq);
+ 		if (!cqe)
+ 			break;
+ 
+ 		mlx5_cqwq_pop(&cq->wq);
+ 
+ 		wqe_counter = be16_to_cpu(cqe->wqe_counter);
+ 
+ 		do {
+ 			struct mlx5e_dma_info *di;
+ 			u16 ci;
+ 
+ 			last_wqe = (sqcc == wqe_counter);
+ 
+ 			ci = sqcc & sq->wq.sz_m1;
+ 			di = &sq->db.di[ci];
+ 
+ 			sqcc++;
+ 			/* Recycle RX page */
+ 			mlx5e_page_release(rq, di, true);
+ 		} while (!last_wqe);
+ 	}
+ 
+ 	mlx5_cqwq_update_db_record(&cq->wq);
+ 
+ 	/* ensure cq space is freed before enabling more cqes */
+ 	wmb();
+ 
+ 	sq->cc = sqcc;
+ 	return (i == MLX5E_TX_CQ_POLL_BUDGET);
+ }
+ 
+ void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq)
+ {
+ 	struct mlx5e_rq *rq = container_of(sq, struct mlx5e_rq, xdpsq);
+ 	struct mlx5e_dma_info *di;
+ 	u16 ci;
+ 
+ 	while (sq->cc != sq->pc) {
+ 		ci = sq->cc & sq->wq.sz_m1;
+ 		di = &sq->db.di[ci];
+ 		sq->cc++;
+ 
+ 		mlx5e_page_release(rq, di, false);
+ 	}
+ }
+ 
+ #ifdef CONFIG_MLX5_CORE_IPOIB
+ 
+ #define MLX5_IB_GRH_DGID_OFFSET 24
+ #define MLX5_GID_SIZE           16
+ 
+ static inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,
+ 					 struct mlx5_cqe64 *cqe,
+ 					 u32 cqe_bcnt,
+ 					 struct sk_buff *skb)
+ {
+ 	struct net_device *netdev = rq->netdev;
+ 	char *pseudo_header;
+ 	u8 *dgid;
+ 	u8 g;
+ 
+ 	g = (be32_to_cpu(cqe->flags_rqpn) >> 28) & 3;
+ 	dgid = skb->data + MLX5_IB_GRH_DGID_OFFSET;
+ 	if ((!g) || dgid[0] != 0xff)
+ 		skb->pkt_type = PACKET_HOST;
+ 	else if (memcmp(dgid, netdev->broadcast + 4, MLX5_GID_SIZE) == 0)
+ 		skb->pkt_type = PACKET_BROADCAST;
+ 	else
+ 		skb->pkt_type = PACKET_MULTICAST;
+ 
+ 	/* TODO: IB/ipoib: Allow mcast packets from other VFs
+ 	 * 68996a6e760e5c74654723eeb57bf65628ae87f4
+ 	 */
+ 
+ 	skb_pull(skb, MLX5_IB_GRH_BYTES);
+ 
+ 	skb->protocol = *((__be16 *)(skb->data));
+ 
+ 	skb->ip_summed = CHECKSUM_COMPLETE;
+ 	skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
+ 
+ 	skb_record_rx_queue(skb, rq->ix);
+ 
+ 	if (likely(netdev->features & NETIF_F_RXHASH))
+ 		mlx5e_skb_set_hash(cqe, skb);
+ 
+ 	/* 20 bytes of ipoib header and 4 for encap existing */
+ 	pseudo_header = skb_push(skb, MLX5_IPOIB_PSEUDO_LEN);
+ 	memset(pseudo_header, 0, MLX5_IPOIB_PSEUDO_LEN);
+ 	skb_reset_mac_header(skb);
+ 	skb_pull(skb, MLX5_IPOIB_HARD_LEN);
+ 
+ 	skb->dev = netdev;
+ 
+ 	rq->stats.csum_complete++;
+ 	rq->stats.packets++;
+ 	rq->stats.bytes += cqe_bcnt;
+ }
+ 
+ void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+ {
+ 	struct mlx5e_rx_wqe *wqe;
+ 	__be16 wqe_counter_be;
+ 	struct sk_buff *skb;
+ 	u16 wqe_counter;
+ 	u32 cqe_bcnt;
+ 
+ 	wqe_counter_be = cqe->wqe_counter;
+ 	wqe_counter    = be16_to_cpu(wqe_counter_be);
+ 	wqe            = mlx5_wq_ll_get_wqe(&rq->wq, wqe_counter);
+ 	cqe_bcnt       = be32_to_cpu(cqe->byte_cnt);
+ 
+ 	skb = skb_from_cqe(rq, cqe, wqe_counter, cqe_bcnt);
+ 	if (!skb)
+ 		goto wq_ll_pop;
+ 
+ 	mlx5i_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ 
+ wq_ll_pop:
+ 	mlx5_wq_ll_pop(&rq->wq, wqe_counter_be,
+ 		       &wqe->next.next_wqe_index);
+ }
+ 
+ #endif /* CONFIG_MLX5_CORE_IPOIB */
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
index 89bca182464c,a0f405f520f7..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
@@@ -38,9 -38,18 +38,19 @@@
  
  #define MLX5I_MAX_NUM_TC 1
  
++<<<<<<< HEAD:drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
++=======
+ extern const struct ethtool_ops mlx5i_ethtool_ops;
+ 
+ #define MLX5_IB_GRH_BYTES       40
+ #define MLX5_IPOIB_ENCAP_LEN    4
+ #define MLX5_IPOIB_PSEUDO_LEN   20
+ #define MLX5_IPOIB_HARD_LEN     (MLX5_IPOIB_PSEUDO_LEN + MLX5_IPOIB_ENCAP_LEN)
+ 
++>>>>>>> c139dbfddd2c (net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct):drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.h
  /* ipoib rdma netdev's private data structure */
  struct mlx5i_priv {
 -	struct rdma_netdev rn; /* keep this first */
  	struct mlx5_core_qp qp;
 -	u32    qkey;
  	char  *mlx5e_priv[0];
  };
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rep.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/ipoib.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/ipoib/ipoib.c
