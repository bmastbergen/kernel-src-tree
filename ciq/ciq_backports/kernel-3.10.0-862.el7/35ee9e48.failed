KVM: lapic: reorganize start_hv_timer

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 35ee9e48b9df6c7751e8f759a4deec5aed1463c3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/35ee9e48.failed

There are many cases in which the hv timer must be canceled.  Split out
a new function to avoid duplication.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 35ee9e48b9df6c7751e8f759a4deec5aed1463c3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/lapic.c
diff --cc arch/x86/kvm/lapic.c
index 0e8704ed991e,b6689dcae1da..000000000000
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@@ -1373,9 -1411,182 +1373,186 @@@ static void start_sw_tscdeadline(struc
  	local_irq_restore(flags);
  }
  
++<<<<<<< HEAD
++=======
+ static void start_sw_period(struct kvm_lapic *apic)
+ {
+ 	if (!apic->lapic_timer.period)
+ 		return;
+ 
+ 	if (apic_lvtt_oneshot(apic) &&
+ 	    ktime_after(ktime_get(),
+ 			apic->lapic_timer.target_expiration)) {
+ 		apic_timer_expired(apic);
+ 		return;
+ 	}
+ 
+ 	hrtimer_start(&apic->lapic_timer.timer,
+ 		apic->lapic_timer.target_expiration,
+ 		HRTIMER_MODE_ABS_PINNED);
+ }
+ 
+ static bool set_target_expiration(struct kvm_lapic *apic)
+ {
+ 	ktime_t now;
+ 	u64 tscl = rdtsc();
+ 
+ 	now = ktime_get();
+ 	apic->lapic_timer.period = (u64)kvm_lapic_get_reg(apic, APIC_TMICT)
+ 		* APIC_BUS_CYCLE_NS * apic->divide_count;
+ 
+ 	if (!apic->lapic_timer.period)
+ 		return false;
+ 
+ 	/*
+ 	 * Do not allow the guest to program periodic timers with small
+ 	 * interval, since the hrtimers are not throttled by the host
+ 	 * scheduler.
+ 	 */
+ 	if (apic_lvtt_period(apic)) {
+ 		s64 min_period = min_timer_period_us * 1000LL;
+ 
+ 		if (apic->lapic_timer.period < min_period) {
+ 			pr_info_ratelimited(
+ 			    "kvm: vcpu %i: requested %lld ns "
+ 			    "lapic timer period limited to %lld ns\n",
+ 			    apic->vcpu->vcpu_id,
+ 			    apic->lapic_timer.period, min_period);
+ 			apic->lapic_timer.period = min_period;
+ 		}
+ 	}
+ 
+ 	apic_debug("%s: bus cycle is %" PRId64 "ns, now 0x%016"
+ 		   PRIx64 ", "
+ 		   "timer initial count 0x%x, period %lldns, "
+ 		   "expire @ 0x%016" PRIx64 ".\n", __func__,
+ 		   APIC_BUS_CYCLE_NS, ktime_to_ns(now),
+ 		   kvm_lapic_get_reg(apic, APIC_TMICT),
+ 		   apic->lapic_timer.period,
+ 		   ktime_to_ns(ktime_add_ns(now,
+ 				apic->lapic_timer.period)));
+ 
+ 	apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ 		nsec_to_cycles(apic->vcpu, apic->lapic_timer.period);
+ 	apic->lapic_timer.target_expiration = ktime_add_ns(now, apic->lapic_timer.period);
+ 
+ 	return true;
+ }
+ 
+ static void advance_periodic_target_expiration(struct kvm_lapic *apic)
+ {
+ 	apic->lapic_timer.tscdeadline +=
+ 		nsec_to_cycles(apic->vcpu, apic->lapic_timer.period);
+ 	apic->lapic_timer.target_expiration =
+ 		ktime_add_ns(apic->lapic_timer.target_expiration,
+ 				apic->lapic_timer.period);
+ }
+ 
+ bool kvm_lapic_hv_timer_in_use(struct kvm_vcpu *vcpu)
+ {
+ 	if (!lapic_in_kernel(vcpu))
+ 		return false;
+ 
+ 	return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_hv_timer_in_use);
+ 
+ static void cancel_hv_timer(struct kvm_lapic *apic)
+ {
+ 	preempt_disable();
+ 	kvm_x86_ops->cancel_hv_timer(apic->vcpu);
+ 	apic->lapic_timer.hv_timer_in_use = false;
+ 	preempt_enable();
+ }
+ 
+ static bool __start_hv_timer(struct kvm_lapic *apic)
+ {
+ 	struct kvm_timer *ktimer = &apic->lapic_timer;
+ 	int r;
+ 
+ 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
+ 		return false;
+ 
+ 	r = kvm_x86_ops->set_hv_timer(apic->vcpu, ktimer->tscdeadline);
+ 	if (r < 0)
+ 		return false;
+ 
+ 	ktimer->hv_timer_in_use = true;
+ 	hrtimer_cancel(&ktimer->timer);
+ 
+ 	/*
+ 	 * Also recheck ktimer->pending, in case the sw timer triggered in
+ 	 * the window.  For periodic timer, leave the hv timer running for
+ 	 * simplicity, and the deadline will be recomputed on the next vmexit.
+ 	 */
+ 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
+ 		return false;
+ 	return true;
+ }
+ 
+ static bool start_hv_timer(struct kvm_lapic *apic)
+ {
+ 	if (!__start_hv_timer(apic)) {
+ 		if (apic->lapic_timer.hv_timer_in_use)
+ 			cancel_hv_timer(apic);
+ 	}
+ 
+ 	trace_kvm_hv_timer_state(apic->vcpu->vcpu_id,
+ 			apic->lapic_timer.hv_timer_in_use);
+ 	return apic->lapic_timer.hv_timer_in_use;
+ }
+ 
+ void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+ 	WARN_ON(swait_active(&vcpu->wq));
+ 	cancel_hv_timer(apic);
+ 	apic_timer_expired(apic);
+ 
+ 	if (apic_lvtt_period(apic) && apic->lapic_timer.period) {
+ 		advance_periodic_target_expiration(apic);
+ 		if (!start_hv_timer(apic))
+ 			start_sw_period(apic);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);
+ 
+ void kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	WARN_ON(apic->lapic_timer.hv_timer_in_use);
+ 
+ 	start_hv_timer(apic);
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_hv_timer);
+ 
+ void kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_lapic *apic = vcpu->arch.apic;
+ 
+ 	/* Possibly the TSC deadline timer is not enabled yet */
+ 	if (!apic->lapic_timer.hv_timer_in_use)
+ 		return;
+ 
+ 	cancel_hv_timer(apic);
+ 
+ 	if (atomic_read(&apic->lapic_timer.pending))
+ 		return;
+ 
+ 	if (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))
+ 		start_sw_period(apic);
+ 	else if (apic_lvtt_tscdeadline(apic))
+ 		start_sw_tscdeadline(apic);
+ }
+ EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_sw_timer);
+ 
++>>>>>>> 35ee9e48b9df (KVM: lapic: reorganize start_hv_timer)
  static void start_apic_timer(struct kvm_lapic *apic)
  {
 +	ktime_t now;
  	atomic_set(&apic->lapic_timer.pending, 0);
  
  	if (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic)) {
* Unmerged path arch/x86/kvm/lapic.c
