xfs: use iomap infrastructure for DAX zeroing

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Christoph Hellwig <hch@lst.de>
commit 459f0fbc2a827da37bbfaf3cae8da4ad8884da12
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/459f0fbc.failed

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Dave Chinner <dchinner@redhat.com>
	Signed-off-by: Dave Chinner <david@fromorbit.com>


(cherry picked from commit 459f0fbc2a827da37bbfaf3cae8da4ad8884da12)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_file.c
#	fs/xfs/xfs_iops.c
diff --cc fs/xfs/xfs_file.c
index 4da908a64865,090a90f0d02c..000000000000
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@@ -81,61 -81,16 +81,74 @@@ xfs_rw_ilock_demote
  }
  
  /*
++<<<<<<< HEAD
 + * xfs_iozero clears the specified range supplied via the page cache (except in
 + * the DAX case). Writes through the page cache will allocate blocks over holes,
 + * though the callers usually map the holes first and avoid them. If a block is
 + * not completely zeroed, then it will be read from disk before being partially
 + * zeroed.
 + *
 + * In the DAX case, we can just directly write to the underlying pages. This
 + * will not allocate blocks, but will avoid holes and unwritten extents and so
 + * not do unnecessary work.
 + */
 +int
 +xfs_iozero(
 +	struct xfs_inode	*ip,	/* inode			*/
 +	loff_t			pos,	/* offset in file		*/
 +	size_t			count)	/* size of data to zero		*/
 +{
 +	struct page		*page;
 +	struct address_space	*mapping;
 +	int			status = 0;
 +
 +
 +	mapping = VFS_I(ip)->i_mapping;
 +	do {
 +		unsigned offset, bytes;
 +		void *fsdata;
 +
 +		offset = (pos & (PAGE_CACHE_SIZE -1)); /* Within page */
 +		bytes = PAGE_CACHE_SIZE - offset;
 +		if (bytes > count)
 +			bytes = count;
 +
 +		if (IS_DAX(VFS_I(ip))) {
 +			status = dax_zero_page_range(VFS_I(ip), pos, bytes,
 +						     xfs_get_blocks_direct);
 +			if (status)
 +				break;
 +		} else {
 +			status = pagecache_write_begin(NULL, mapping, pos, bytes,
 +						AOP_FLAG_UNINTERRUPTIBLE,
 +						&page, &fsdata);
 +			if (status)
 +				break;
 +
 +			zero_user(page, offset, bytes);
 +
 +			status = pagecache_write_end(NULL, mapping, pos, bytes,
 +						bytes, page, fsdata);
 +			WARN_ON(status <= 0); /* can't return less than zero! */
 +			status = 0;
 +		}
 +		pos += bytes;
 +		count -= bytes;
 +	} while (count);
 +
 +	return status;
++=======
+  * Clear the specified ranges to zero through either the pagecache or DAX.
+  * Holes and unwritten extents will be left as-is as they already are zeroed.
+  */
+ int
+ xfs_iozero(
+ 	struct xfs_inode	*ip,
+ 	loff_t			pos,
+ 	size_t			count)
+ {
+ 	return iomap_zero_range(VFS_I(ip), pos, count, NULL, &xfs_iomap_ops);
++>>>>>>> 459f0fbc2a82 (xfs: use iomap infrastructure for DAX zeroing)
  }
  
  int
diff --cc fs/xfs/xfs_iops.c
index 4c6e1d5b88f5,ab820f84ed50..000000000000
--- a/fs/xfs/xfs_iops.c
+++ b/fs/xfs/xfs_iops.c
@@@ -802,13 -819,8 +802,18 @@@ xfs_setattr_size
  	if (newsize > oldsize) {
  		error = xfs_zero_eof(ip, newsize, oldsize, &did_zeroing);
  	} else {
++<<<<<<< HEAD
 +		if (IS_DAX(inode)) {
 +			error = dax_truncate_page(inode, newsize,
 +					xfs_get_blocks_direct);
 +		} else {
 +			error = block_truncate_page(inode->i_mapping, newsize,
 +					xfs_get_blocks);
 +		}
++=======
+ 		error = iomap_truncate_page(inode, newsize, &did_zeroing,
+ 				&xfs_iomap_ops);
++>>>>>>> 459f0fbc2a82 (xfs: use iomap infrastructure for DAX zeroing)
  	}
  
  	if (error)
* Unmerged path fs/xfs/xfs_file.c
* Unmerged path fs/xfs/xfs_iops.c
