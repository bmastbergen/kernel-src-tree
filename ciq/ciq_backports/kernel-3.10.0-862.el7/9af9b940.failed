x86/cpu/AMD: Handle SME reduction in physical address size

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] cpu/amd: Handle SME reduction in physical address size (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 96.43%
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit 9af9b94068fb1ea3206a700fc222075966fbef14
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/9af9b940.failed

When System Memory Encryption (SME) is enabled, the physical address
space is reduced. Adjust the x86_phys_bits value to reflect this
reduction.

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Borislav Petkov <bp@suse.de>
	Cc: Alexander Potapenko <glider@google.com>
	Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brijesh Singh <brijesh.singh@amd.com>
	Cc: Dave Young <dyoung@redhat.com>
	Cc: Dmitry Vyukov <dvyukov@google.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Cc: Larry Woodman <lwoodman@redhat.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Matt Fleming <matt@codeblueprint.co.uk>
	Cc: Michael S. Tsirkin <mst@redhat.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Radim Krčmář <rkrcmar@redhat.com>
	Cc: Rik van Riel <riel@redhat.com>
	Cc: Toshimitsu Kani <toshi.kani@hpe.com>
	Cc: kasan-dev@googlegroups.com
	Cc: kvm@vger.kernel.org
	Cc: linux-arch@vger.kernel.org
	Cc: linux-doc@vger.kernel.org
	Cc: linux-efi@vger.kernel.org
	Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/593c037a3cad85ba92f3d061ffa7462e9ce3531d.1500319216.git.thomas.lendacky@amd.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 9af9b94068fb1ea3206a700fc222075966fbef14)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/amd.c
diff --cc arch/x86/kernel/cpu/amd.c
index 485733876ab9,4d87950fde30..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -545,26 -578,87 +545,70 @@@ static void early_init_amd(struct cpuin
  	 * can safely set X86_FEATURE_EXTD_APICID unconditionally for families
  	 * after 16h.
  	 */
 -	if (boot_cpu_has(X86_FEATURE_APIC)) {
 -		if (c->x86 > 0x16)
 +	if (cpu_has_apic && c->x86 > 0x16) {
 +		set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
 +	} else if (cpu_has_apic && c->x86 >= 0xf) {
 +		/* check CPU config space for extended APIC ID */
 +		unsigned int val;
 +		val = read_pci_config(0, 24, 0, 0x68);
 +		if ((val & ((1 << 17) | (1 << 18))) == ((1 << 17) | (1 << 18)))
  			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
 -		else if (c->x86 >= 0xf) {
 -			/* check CPU config space for extended APIC ID */
 -			unsigned int val;
 -
 -			val = read_pci_config(0, 24, 0, 0x68);
 -			if ((val >> 17 & 0x3) == 0x3)
 -				set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
 -		}
  	}
  #endif
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * This is only needed to tell the kernel whether to use VMCALL
+ 	 * and VMMCALL.  VMMCALL is never executed except under virt, so
+ 	 * we can set it unconditionally.
+ 	 */
+ 	set_cpu_cap(c, X86_FEATURE_VMMCALL);
+ 
+ 	/* F16h erratum 793, CVE-2013-6885 */
+ 	if (c->x86 == 0x16 && c->x86_model <= 0xf)
+ 		msr_set_bit(MSR_AMD64_LS_CFG, 15);
+ 
+ 	/*
+ 	 * Check whether the machine is affected by erratum 400. This is
+ 	 * used to select the proper idle routine and to enable the check
+ 	 * whether the machine is affected in arch_post_acpi_init(), which
+ 	 * sets the X86_BUG_AMD_APIC_C1E bug depending on the MSR check.
+ 	 */
+ 	if (cpu_has_amd_erratum(c, amd_erratum_400))
+ 		set_cpu_bug(c, X86_BUG_AMD_E400);
+ 
+ 	/*
+ 	 * BIOS support is required for SME. If BIOS has enabled SME then
+ 	 * adjust x86_phys_bits by the SME physical address space reduction
+ 	 * value. If BIOS has not enabled SME then don't advertise the
+ 	 * feature (set in scattered.c). Also, since the SME support requires
+ 	 * long mode, don't advertise the feature under CONFIG_X86_32.
+ 	 */
+ 	if (cpu_has(c, X86_FEATURE_SME)) {
+ 		u64 msr;
+ 
+ 		/* Check if SME is enabled */
+ 		rdmsrl(MSR_K8_SYSCFG, msr);
+ 		if (msr & MSR_K8_SYSCFG_MEM_ENCRYPT) {
+ 			c->x86_phys_bits -= (cpuid_ebx(0x8000001f) >> 6) & 0x3f;
+ 			if (IS_ENABLED(CONFIG_X86_32))
+ 				clear_cpu_cap(c, X86_FEATURE_SME);
+ 		} else {
+ 			clear_cpu_cap(c, X86_FEATURE_SME);
+ 		}
+ 	}
++>>>>>>> 9af9b94068fb (x86/cpu/AMD: Handle SME reduction in physical address size)
  }
  
 -static void init_amd_k8(struct cpuinfo_x86 *c)
 -{
 -	u32 level;
 -	u64 value;
 -
 -	/* On C+ stepping K8 rep microcode works well for copy/memset */
 -	level = cpuid_eax(1);
 -	if ((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)
 -		set_cpu_cap(c, X86_FEATURE_REP_GOOD);
 -
 -	/*
 -	 * Some BIOSes incorrectly force this feature, but only K8 revision D
 -	 * (model = 0x14) and later actually support it.
 -	 * (AMD Erratum #110, docId: 25759).
 -	 */
 -	if (c->x86_model < 0x14 && cpu_has(c, X86_FEATURE_LAHF_LM)) {
 -		clear_cpu_cap(c, X86_FEATURE_LAHF_LM);
 -		if (!rdmsrl_amd_safe(0xc001100d, &value)) {
 -			value &= ~BIT_64(32);
 -			wrmsrl_amd_safe(0xc001100d, value);
 -		}
 -	}
 +static const int amd_erratum_383[];
 +static const int amd_erratum_400[];
 +static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum);
  
 -	if (!c->x86_model_id[0])
 -		strcpy(c->x86_model_id, "Hammer");
 +static void init_amd(struct cpuinfo_x86 *c)
 +{
 +	u32 dummy;
 +	unsigned long long value;
  
  #ifdef CONFIG_SMP
  	/*
* Unmerged path arch/x86/kernel/cpu/amd.c
