treewide: Move dma_ops from struct dev_archdata into struct device

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Bart Van Assche <bart.vanassche@sandisk.com>
commit 5657933dbb6e25feaf5d8df8c88f96cdade693a3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/5657933d.failed

Some but not all architectures provide set_dma_ops(). Move dma_ops
from struct dev_archdata into struct device such that it becomes
possible on all architectures to configure dma_ops per device.

	Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Cc: David Woodhouse <dwmw2@infradead.org>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: linux-arch@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Cc: Russell King <linux@armlinux.org.uk>
	Cc: x86@kernel.org
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 5657933dbb6e25feaf5d8df8c88f96cdade693a3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/include/asm/device.h
#	arch/arm64/include/asm/device.h
#	arch/arm64/include/asm/dma-mapping.h
#	arch/arm64/mm/dma-mapping.c
#	arch/m32r/include/asm/device.h
#	arch/m32r/include/asm/dma-mapping.h
#	arch/mips/include/asm/device.h
#	arch/powerpc/include/asm/device.h
#	arch/powerpc/kernel/dma.c
#	arch/powerpc/platforms/pasemi/iommu.c
#	arch/s390/include/asm/device.h
#	arch/s390/include/asm/dma-mapping.h
#	arch/s390/pci/pci.c
#	arch/tile/include/asm/device.h
#	arch/x86/include/asm/device.h
#	arch/xtensa/include/asm/device.h
#	arch/xtensa/include/asm/dma-mapping.h
#	drivers/misc/mic/bus/mic_bus.c
#	drivers/misc/mic/bus/scif_bus.c
#	drivers/misc/mic/bus/vop_bus.c
diff --cc arch/arm/include/asm/device.h
index dc662fca9230,220ba207be91..000000000000
--- a/arch/arm/include/asm/device.h
+++ b/arch/arm/include/asm/device.h
@@@ -7,7 -7,6 +7,10 @@@
  #define ASMARM_DEVICE_H
  
  struct dev_archdata {
++<<<<<<< HEAD
 +	struct dma_map_ops	*dma_ops;
++=======
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  #ifdef CONFIG_DMABOUNCE
  	struct dmabounce_device_info *dmabounce;
  #endif
diff --cc arch/arm64/include/asm/device.h
index 0d8453c755a8,73d5bab015eb..000000000000
--- a/arch/arm64/include/asm/device.h
+++ b/arch/arm64/include/asm/device.h
@@@ -17,7 -17,10 +17,14 @@@
  #define __ASM_DEVICE_H
  
  struct dev_archdata {
++<<<<<<< HEAD
 +	struct dma_map_ops *dma_ops;
++=======
+ #ifdef CONFIG_IOMMU_API
+ 	void *iommu;			/* private IOMMU data */
+ #endif
+ 	bool dma_coherent;
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  };
  
  struct pdev_archdata {
diff --cc arch/arm64/include/asm/dma-mapping.h
index 994776894198,58ae36cc3b60..000000000000
--- a/arch/arm64/include/asm/dma-mapping.h
+++ b/arch/arm64/include/asm/dma-mapping.h
@@@ -21,52 -21,61 +21,69 @@@
  #include <linux/types.h>
  #include <linux/vmalloc.h>
  
 -#include <xen/xen.h>
 -#include <asm/xen/hypervisor.h>
 +#include <asm-generic/dma-coherent.h>
  
 -#define DMA_ERROR_CODE	(~(dma_addr_t)0)
 -extern const struct dma_map_ops dummy_dma_ops;
 +#define ARCH_HAS_DMA_GET_REQUIRED_MASK
  
 -static inline const struct dma_map_ops *__generic_dma_ops(struct device *dev)
 +extern struct dma_map_ops *dma_ops;
 +
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
  {
++<<<<<<< HEAD
 +	if (unlikely(!dev) || !dev->archdata.dma_ops)
 +		return dma_ops;
++=======
+ 	if (dev && dev->dma_ops)
+ 		return dev->dma_ops;
+ 
+ 	/*
+ 	 * We expect no ISA devices, and all other DMA masters are expected to
+ 	 * have someone call arch_setup_dma_ops at device creation time.
+ 	 */
+ 	return &dummy_dma_ops;
+ }
+ 
+ static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
+ {
+ 	if (xen_initial_domain())
+ 		return xen_dma_ops;
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  	else
 -		return __generic_dma_ops(dev);
 +		return dev->archdata.dma_ops;
  }
  
 -void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
 -			const struct iommu_ops *iommu, bool coherent);
 -#define arch_setup_dma_ops	arch_setup_dma_ops
 +#include <asm-generic/dma-mapping-common.h>
  
 -#ifdef CONFIG_IOMMU_DMA
 -void arch_teardown_dma_ops(struct device *dev);
 -#define arch_teardown_dma_ops	arch_teardown_dma_ops
 -#endif
 +static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 +{
 +	return (dma_addr_t)paddr;
 +}
  
 -/* do not use this function in a driver */
 -static inline bool is_device_dma_coherent(struct device *dev)
 +static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
  {
 -	if (!dev)
 -		return false;
 -	return dev->archdata.dma_coherent;
 +	return (phys_addr_t)dev_addr;
  }
  
 -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dev_addr)
  {
 -	dma_addr_t dev_addr = (dma_addr_t)paddr;
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	debug_dma_mapping_error(dev, dev_addr);
 +	return ops->mapping_error(dev, dev_addr);
 +}
  
 -	return dev_addr - ((dma_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	return ops->dma_supported(dev, mask);
  }
  
 -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
 +static inline int dma_set_mask(struct device *dev, u64 mask)
  {
 -	phys_addr_t paddr = (phys_addr_t)dev_addr;
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +	*dev->dma_mask = mask;
  
 -	return paddr + ((phys_addr_t)dev->dma_pfn_offset << PAGE_SHIFT);
 +	return 0;
  }
  
  static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
diff --cc arch/arm64/mm/dma-mapping.c
index 4bd7579ec9e6,dbab4c6c084b..000000000000
--- a/arch/arm64/mm/dma-mapping.c
+++ b/arch/arm64/mm/dma-mapping.c
@@@ -75,3 -540,424 +75,427 @@@ static int __init dma_debug_do_init(voi
  	return 0;
  }
  fs_initcall(dma_debug_do_init);
++<<<<<<< HEAD
++=======
+ 
+ 
+ #ifdef CONFIG_IOMMU_DMA
+ #include <linux/dma-iommu.h>
+ #include <linux/platform_device.h>
+ #include <linux/amba/bus.h>
+ 
+ /* Thankfully, all cache ops are by VA so we can ignore phys here */
+ static void flush_page(struct device *dev, const void *virt, phys_addr_t phys)
+ {
+ 	__dma_flush_area(virt, PAGE_SIZE);
+ }
+ 
+ static void *__iommu_alloc_attrs(struct device *dev, size_t size,
+ 				 dma_addr_t *handle, gfp_t gfp,
+ 				 unsigned long attrs)
+ {
+ 	bool coherent = is_device_dma_coherent(dev);
+ 	int ioprot = dma_direction_to_prot(DMA_BIDIRECTIONAL, coherent);
+ 	size_t iosize = size;
+ 	void *addr;
+ 
+ 	if (WARN(!dev, "cannot create IOMMU mapping for unknown device\n"))
+ 		return NULL;
+ 
+ 	size = PAGE_ALIGN(size);
+ 
+ 	/*
+ 	 * Some drivers rely on this, and we probably don't want the
+ 	 * possibility of stale kernel data being read by devices anyway.
+ 	 */
+ 	gfp |= __GFP_ZERO;
+ 
+ 	if (gfpflags_allow_blocking(gfp)) {
+ 		struct page **pages;
+ 		pgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL, coherent);
+ 
+ 		pages = iommu_dma_alloc(dev, iosize, gfp, attrs, ioprot,
+ 					handle, flush_page);
+ 		if (!pages)
+ 			return NULL;
+ 
+ 		addr = dma_common_pages_remap(pages, size, VM_USERMAP, prot,
+ 					      __builtin_return_address(0));
+ 		if (!addr)
+ 			iommu_dma_free(dev, pages, iosize, handle);
+ 	} else {
+ 		struct page *page;
+ 		/*
+ 		 * In atomic context we can't remap anything, so we'll only
+ 		 * get the virtually contiguous buffer we need by way of a
+ 		 * physically contiguous allocation.
+ 		 */
+ 		if (coherent) {
+ 			page = alloc_pages(gfp, get_order(size));
+ 			addr = page ? page_address(page) : NULL;
+ 		} else {
+ 			addr = __alloc_from_pool(size, &page, gfp);
+ 		}
+ 		if (!addr)
+ 			return NULL;
+ 
+ 		*handle = iommu_dma_map_page(dev, page, 0, iosize, ioprot);
+ 		if (iommu_dma_mapping_error(dev, *handle)) {
+ 			if (coherent)
+ 				__free_pages(page, get_order(size));
+ 			else
+ 				__free_from_pool(addr, size);
+ 			addr = NULL;
+ 		}
+ 	}
+ 	return addr;
+ }
+ 
+ static void __iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+ 			       dma_addr_t handle, unsigned long attrs)
+ {
+ 	size_t iosize = size;
+ 
+ 	size = PAGE_ALIGN(size);
+ 	/*
+ 	 * @cpu_addr will be one of 3 things depending on how it was allocated:
+ 	 * - A remapped array of pages from iommu_dma_alloc(), for all
+ 	 *   non-atomic allocations.
+ 	 * - A non-cacheable alias from the atomic pool, for atomic
+ 	 *   allocations by non-coherent devices.
+ 	 * - A normal lowmem address, for atomic allocations by
+ 	 *   coherent devices.
+ 	 * Hence how dodgy the below logic looks...
+ 	 */
+ 	if (__in_atomic_pool(cpu_addr, size)) {
+ 		iommu_dma_unmap_page(dev, handle, iosize, 0, 0);
+ 		__free_from_pool(cpu_addr, size);
+ 	} else if (is_vmalloc_addr(cpu_addr)){
+ 		struct vm_struct *area = find_vm_area(cpu_addr);
+ 
+ 		if (WARN_ON(!area || !area->pages))
+ 			return;
+ 		iommu_dma_free(dev, area->pages, iosize, &handle);
+ 		dma_common_free_remap(cpu_addr, size, VM_USERMAP);
+ 	} else {
+ 		iommu_dma_unmap_page(dev, handle, iosize, 0, 0);
+ 		__free_pages(virt_to_page(cpu_addr), get_order(size));
+ 	}
+ }
+ 
+ static int __iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+ 			      void *cpu_addr, dma_addr_t dma_addr, size_t size,
+ 			      unsigned long attrs)
+ {
+ 	struct vm_struct *area;
+ 	int ret;
+ 
+ 	vma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot,
+ 					     is_device_dma_coherent(dev));
+ 
+ 	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))
+ 		return ret;
+ 
+ 	area = find_vm_area(cpu_addr);
+ 	if (WARN_ON(!area || !area->pages))
+ 		return -ENXIO;
+ 
+ 	return iommu_dma_mmap(area->pages, size, vma);
+ }
+ 
+ static int __iommu_get_sgtable(struct device *dev, struct sg_table *sgt,
+ 			       void *cpu_addr, dma_addr_t dma_addr,
+ 			       size_t size, unsigned long attrs)
+ {
+ 	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	struct vm_struct *area = find_vm_area(cpu_addr);
+ 
+ 	if (WARN_ON(!area || !area->pages))
+ 		return -ENXIO;
+ 
+ 	return sg_alloc_table_from_pages(sgt, area->pages, count, 0, size,
+ 					 GFP_KERNEL);
+ }
+ 
+ static void __iommu_sync_single_for_cpu(struct device *dev,
+ 					dma_addr_t dev_addr, size_t size,
+ 					enum dma_data_direction dir)
+ {
+ 	phys_addr_t phys;
+ 
+ 	if (is_device_dma_coherent(dev))
+ 		return;
+ 
+ 	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);
+ 	__dma_unmap_area(phys_to_virt(phys), size, dir);
+ }
+ 
+ static void __iommu_sync_single_for_device(struct device *dev,
+ 					   dma_addr_t dev_addr, size_t size,
+ 					   enum dma_data_direction dir)
+ {
+ 	phys_addr_t phys;
+ 
+ 	if (is_device_dma_coherent(dev))
+ 		return;
+ 
+ 	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);
+ 	__dma_map_area(phys_to_virt(phys), size, dir);
+ }
+ 
+ static dma_addr_t __iommu_map_page(struct device *dev, struct page *page,
+ 				   unsigned long offset, size_t size,
+ 				   enum dma_data_direction dir,
+ 				   unsigned long attrs)
+ {
+ 	bool coherent = is_device_dma_coherent(dev);
+ 	int prot = dma_direction_to_prot(dir, coherent);
+ 	dma_addr_t dev_addr = iommu_dma_map_page(dev, page, offset, size, prot);
+ 
+ 	if (!iommu_dma_mapping_error(dev, dev_addr) &&
+ 	    (attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
+ 		__iommu_sync_single_for_device(dev, dev_addr, size, dir);
+ 
+ 	return dev_addr;
+ }
+ 
+ static void __iommu_unmap_page(struct device *dev, dma_addr_t dev_addr,
+ 			       size_t size, enum dma_data_direction dir,
+ 			       unsigned long attrs)
+ {
+ 	if ((attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
+ 		__iommu_sync_single_for_cpu(dev, dev_addr, size, dir);
+ 
+ 	iommu_dma_unmap_page(dev, dev_addr, size, dir, attrs);
+ }
+ 
+ static void __iommu_sync_sg_for_cpu(struct device *dev,
+ 				    struct scatterlist *sgl, int nelems,
+ 				    enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	if (is_device_dma_coherent(dev))
+ 		return;
+ 
+ 	for_each_sg(sgl, sg, nelems, i)
+ 		__dma_unmap_area(sg_virt(sg), sg->length, dir);
+ }
+ 
+ static void __iommu_sync_sg_for_device(struct device *dev,
+ 				       struct scatterlist *sgl, int nelems,
+ 				       enum dma_data_direction dir)
+ {
+ 	struct scatterlist *sg;
+ 	int i;
+ 
+ 	if (is_device_dma_coherent(dev))
+ 		return;
+ 
+ 	for_each_sg(sgl, sg, nelems, i)
+ 		__dma_map_area(sg_virt(sg), sg->length, dir);
+ }
+ 
+ static int __iommu_map_sg_attrs(struct device *dev, struct scatterlist *sgl,
+ 				int nelems, enum dma_data_direction dir,
+ 				unsigned long attrs)
+ {
+ 	bool coherent = is_device_dma_coherent(dev);
+ 
+ 	if ((attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
+ 		__iommu_sync_sg_for_device(dev, sgl, nelems, dir);
+ 
+ 	return iommu_dma_map_sg(dev, sgl, nelems,
+ 			dma_direction_to_prot(dir, coherent));
+ }
+ 
+ static void __iommu_unmap_sg_attrs(struct device *dev,
+ 				   struct scatterlist *sgl, int nelems,
+ 				   enum dma_data_direction dir,
+ 				   unsigned long attrs)
+ {
+ 	if ((attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
+ 		__iommu_sync_sg_for_cpu(dev, sgl, nelems, dir);
+ 
+ 	iommu_dma_unmap_sg(dev, sgl, nelems, dir, attrs);
+ }
+ 
+ static const struct dma_map_ops iommu_dma_ops = {
+ 	.alloc = __iommu_alloc_attrs,
+ 	.free = __iommu_free_attrs,
+ 	.mmap = __iommu_mmap_attrs,
+ 	.get_sgtable = __iommu_get_sgtable,
+ 	.map_page = __iommu_map_page,
+ 	.unmap_page = __iommu_unmap_page,
+ 	.map_sg = __iommu_map_sg_attrs,
+ 	.unmap_sg = __iommu_unmap_sg_attrs,
+ 	.sync_single_for_cpu = __iommu_sync_single_for_cpu,
+ 	.sync_single_for_device = __iommu_sync_single_for_device,
+ 	.sync_sg_for_cpu = __iommu_sync_sg_for_cpu,
+ 	.sync_sg_for_device = __iommu_sync_sg_for_device,
+ 	.map_resource = iommu_dma_map_resource,
+ 	.unmap_resource = iommu_dma_unmap_resource,
+ 	.dma_supported = iommu_dma_supported,
+ 	.mapping_error = iommu_dma_mapping_error,
+ };
+ 
+ /*
+  * TODO: Right now __iommu_setup_dma_ops() gets called too early to do
+  * everything it needs to - the device is only partially created and the
+  * IOMMU driver hasn't seen it yet, so it can't have a group. Thus we
+  * need this delayed attachment dance. Once IOMMU probe ordering is sorted
+  * to move the arch_setup_dma_ops() call later, all the notifier bits below
+  * become unnecessary, and will go away.
+  */
+ struct iommu_dma_notifier_data {
+ 	struct list_head list;
+ 	struct device *dev;
+ 	const struct iommu_ops *ops;
+ 	u64 dma_base;
+ 	u64 size;
+ };
+ static LIST_HEAD(iommu_dma_masters);
+ static DEFINE_MUTEX(iommu_dma_notifier_lock);
+ 
+ static bool do_iommu_attach(struct device *dev, const struct iommu_ops *ops,
+ 			   u64 dma_base, u64 size)
+ {
+ 	struct iommu_domain *domain = iommu_get_domain_for_dev(dev);
+ 
+ 	/*
+ 	 * If the IOMMU driver has the DMA domain support that we require,
+ 	 * then the IOMMU core will have already configured a group for this
+ 	 * device, and allocated the default domain for that group.
+ 	 */
+ 	if (!domain || iommu_dma_init_domain(domain, dma_base, size, dev)) {
+ 		pr_warn("Failed to set up IOMMU for device %s; retaining platform DMA ops\n",
+ 			dev_name(dev));
+ 		return false;
+ 	}
+ 
+ 	dev->dma_ops = &iommu_dma_ops;
+ 	return true;
+ }
+ 
+ static void queue_iommu_attach(struct device *dev, const struct iommu_ops *ops,
+ 			      u64 dma_base, u64 size)
+ {
+ 	struct iommu_dma_notifier_data *iommudata;
+ 
+ 	iommudata = kzalloc(sizeof(*iommudata), GFP_KERNEL);
+ 	if (!iommudata)
+ 		return;
+ 
+ 	iommudata->dev = dev;
+ 	iommudata->ops = ops;
+ 	iommudata->dma_base = dma_base;
+ 	iommudata->size = size;
+ 
+ 	mutex_lock(&iommu_dma_notifier_lock);
+ 	list_add(&iommudata->list, &iommu_dma_masters);
+ 	mutex_unlock(&iommu_dma_notifier_lock);
+ }
+ 
+ static int __iommu_attach_notifier(struct notifier_block *nb,
+ 				   unsigned long action, void *data)
+ {
+ 	struct iommu_dma_notifier_data *master, *tmp;
+ 
+ 	if (action != BUS_NOTIFY_BIND_DRIVER)
+ 		return 0;
+ 
+ 	mutex_lock(&iommu_dma_notifier_lock);
+ 	list_for_each_entry_safe(master, tmp, &iommu_dma_masters, list) {
+ 		if (data == master->dev && do_iommu_attach(master->dev,
+ 				master->ops, master->dma_base, master->size)) {
+ 			list_del(&master->list);
+ 			kfree(master);
+ 			break;
+ 		}
+ 	}
+ 	mutex_unlock(&iommu_dma_notifier_lock);
+ 	return 0;
+ }
+ 
+ static int __init register_iommu_dma_ops_notifier(struct bus_type *bus)
+ {
+ 	struct notifier_block *nb = kzalloc(sizeof(*nb), GFP_KERNEL);
+ 	int ret;
+ 
+ 	if (!nb)
+ 		return -ENOMEM;
+ 
+ 	nb->notifier_call = __iommu_attach_notifier;
+ 
+ 	ret = bus_register_notifier(bus, nb);
+ 	if (ret) {
+ 		pr_warn("Failed to register DMA domain notifier; IOMMU DMA ops unavailable on bus '%s'\n",
+ 			bus->name);
+ 		kfree(nb);
+ 	}
+ 	return ret;
+ }
+ 
+ static int __init __iommu_dma_init(void)
+ {
+ 	int ret;
+ 
+ 	ret = iommu_dma_init();
+ 	if (!ret)
+ 		ret = register_iommu_dma_ops_notifier(&platform_bus_type);
+ 	if (!ret)
+ 		ret = register_iommu_dma_ops_notifier(&amba_bustype);
+ #ifdef CONFIG_PCI
+ 	if (!ret)
+ 		ret = register_iommu_dma_ops_notifier(&pci_bus_type);
+ #endif
+ 	return ret;
+ }
+ arch_initcall(__iommu_dma_init);
+ 
+ static void __iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+ 				  const struct iommu_ops *ops)
+ {
+ 	struct iommu_group *group;
+ 
+ 	if (!ops)
+ 		return;
+ 	/*
+ 	 * TODO: As a concession to the future, we're ready to handle being
+ 	 * called both early and late (i.e. after bus_add_device). Once all
+ 	 * the platform bus code is reworked to call us late and the notifier
+ 	 * junk above goes away, move the body of do_iommu_attach here.
+ 	 */
+ 	group = iommu_group_get(dev);
+ 	if (group) {
+ 		do_iommu_attach(dev, ops, dma_base, size);
+ 		iommu_group_put(group);
+ 	} else {
+ 		queue_iommu_attach(dev, ops, dma_base, size);
+ 	}
+ }
+ 
+ void arch_teardown_dma_ops(struct device *dev)
+ {
+ 	dev->dma_ops = NULL;
+ }
+ 
+ #else
+ 
+ static void __iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+ 				  const struct iommu_ops *iommu)
+ { }
+ 
+ #endif  /* CONFIG_IOMMU_DMA */
+ 
+ void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+ 			const struct iommu_ops *iommu, bool coherent)
+ {
+ 	if (!dev->dma_ops)
+ 		dev->dma_ops = &swiotlb_dma_ops;
+ 
+ 	dev->archdata.dma_coherent = coherent;
+ 	__iommu_setup_dma_ops(dev, dma_base, size, iommu);
+ }
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
diff --cc arch/m32r/include/asm/device.h
index d8f9872b0e2d,5203fc87f080..000000000000
--- a/arch/m32r/include/asm/device.h
+++ b/arch/m32r/include/asm/device.h
@@@ -3,5 -3,8 +3,10 @@@
   *
   * This file is released under the GPLv2
   */
++<<<<<<< HEAD
 +#include <asm-generic/device.h>
++=======
+ struct dev_archdata {
+ };
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  
 -struct pdev_archdata {
 -};
diff --cc arch/mips/include/asm/device.h
index c94fafba9e62,6aa796f1081a..000000000000
--- a/arch/mips/include/asm/device.h
+++ b/arch/mips/include/asm/device.h
@@@ -6,11 -6,11 +6,16 @@@
  #ifndef _ASM_MIPS_DEVICE_H
  #define _ASM_MIPS_DEVICE_H
  
- struct dma_map_ops;
- 
  struct dev_archdata {
++<<<<<<< HEAD
 +	/* DMA operations on that device */
 +	struct dma_map_ops *dma_ops;
++=======
+ #ifdef CONFIG_DMA_PERDEV_COHERENT
+ 	/* Non-zero if DMA is coherent with CPU caches */
+ 	bool dma_coherent;
+ #endif
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  };
  
  struct pdev_archdata {
diff --cc arch/powerpc/include/asm/device.h
index 7992730ccfab,0245bfcaac32..000000000000
--- a/arch/powerpc/include/asm/device.h
+++ b/arch/powerpc/include/asm/device.h
@@@ -6,11 -6,19 +6,22 @@@
  #ifndef _ASM_POWERPC_DEVICE_H
  #define _ASM_POWERPC_DEVICE_H
  
- struct dma_map_ops;
  struct device_node;
 -#ifdef CONFIG_PPC64
 -struct pci_dn;
  struct iommu_table;
++<<<<<<< HEAD
 +
 +struct dev_arch_dmadata {
++=======
+ #endif
+ 
+ /*
+  * Arch extensions to struct device.
+  *
+  * When adding fields, consider macio_add_one_device in
+  * drivers/macintosh/macio_asic.c
+  */
+ struct dev_archdata {
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  	/*
  	 * These two used to be a union. However, with the hybrid ops we need
  	 * both so here we store both a DMA offset for direct mappings and
diff --cc arch/powerpc/kernel/dma.c
index 326685910e30,41c749586bd2..000000000000
--- a/arch/powerpc/kernel/dma.c
+++ b/arch/powerpc/kernel/dma.c
@@@ -26,6 -27,18 +26,21 @@@
   * default the offset is PCI_DRAM_OFFSET.
   */
  
++<<<<<<< HEAD
++=======
+ static u64 __maybe_unused get_pfn_limit(struct device *dev)
+ {
+ 	u64 pfn = (dev->coherent_dma_mask >> PAGE_SHIFT) + 1;
+ 	struct dev_archdata __maybe_unused *sd = &dev->archdata;
+ 
+ #ifdef CONFIG_SWIOTLB
+ 	if (sd->max_direct_dma_addr && dev->dma_ops == &swiotlb_dma_ops)
+ 		pfn = min_t(u64, pfn, sd->max_direct_dma_addr >> PAGE_SHIFT);
+ #endif
+ 
+ 	return pfn;
+ }
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  
  static int dma_direct_dma_supported(struct device *dev, u64 mask)
  {
diff --cc arch/powerpc/platforms/pasemi/iommu.c
index c929644e74a6,7fec04de27fc..000000000000
--- a/arch/powerpc/platforms/pasemi/iommu.c
+++ b/arch/powerpc/platforms/pasemi/iommu.c
@@@ -186,7 -186,12 +186,16 @@@ static void pci_dma_dev_setup_pasemi(st
  	 */
  	if (dev->vendor == 0x1959 && dev->device == 0xa007 &&
  	    !firmware_has_feature(FW_FEATURE_LPAR)) {
++<<<<<<< HEAD
 +		dev->dev.archdata.dma_ops = &dma_direct_ops;
++=======
+ 		dev->dev.dma_ops = &dma_direct_ops;
+ 		/*
+ 		 * Set the coherent DMA mask to prevent the iommu
+ 		 * being used unnecessarily
+ 		 */
+ 		dev->dev.coherent_dma_mask = DMA_BIT_MASK(44);
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  		return;
  	}
  #endif
diff --cc arch/s390/include/asm/device.h
index d8f9872b0e2d,5203fc87f080..000000000000
--- a/arch/s390/include/asm/device.h
+++ b/arch/s390/include/asm/device.h
@@@ -3,5 -3,8 +3,10 @@@
   *
   * This file is released under the GPLv2
   */
++<<<<<<< HEAD
 +#include <asm-generic/device.h>
++=======
+ struct dev_archdata {
+ };
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  
 -struct pdev_archdata {
 -};
diff --cc arch/s390/include/asm/dma-mapping.h
index 3f2d5669375a,a872027d0c1b..000000000000
--- a/arch/s390/include/asm/dma-mapping.h
+++ b/arch/s390/include/asm/dma-mapping.h
@@@ -11,12 -10,12 +11,17 @@@
  
  #define DMA_ERROR_CODE		(~(dma_addr_t) 0x0)
  
 -extern const struct dma_map_ops s390_pci_dma_ops;
 +extern struct dma_map_ops s390_pci_dma_ops;
  
 -static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 +static inline struct dma_map_ops *get_dma_ops(struct device *dev)
  {
++<<<<<<< HEAD
 +	if (dev && dev->device_rh->dma_ops)
 +		return dev->device_rh->dma_ops;
++=======
+ 	if (dev && dev->dma_ops)
+ 		return dev->dma_ops;
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  	return &dma_noop_ops;
  }
  
diff --cc arch/s390/pci/pci.c
index 833b01424c9d,82abef8b8574..000000000000
--- a/arch/s390/pci/pci.c
+++ b/arch/s390/pci/pci.c
@@@ -754,13 -641,9 +754,17 @@@ int pcibios_add_device(struct pci_dev *
  	int i;
  
  	pdev->dev.groups = zpci_attr_groups;
++<<<<<<< HEAD
++=======
+ 	pdev->dev.dma_ops = &s390_pci_dma_ops;
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  	zpci_map_resources(pdev);
  
 +	if (!pdev->dev.device_rh)
 +		device_rh_alloc(&pdev->dev);
 +
 +	pdev->dev.device_rh->dma_ops = &s390_pci_dma_ops;
 +
  	for (i = 0; i < PCI_BAR_COUNT; i++) {
  		res = &pdev->resource[i];
  		if (res->parent || !res->flags)
diff --cc arch/tile/include/asm/device.h
index 5182705bd056,1cf45422a0df..000000000000
--- a/arch/tile/include/asm/device.h
+++ b/arch/tile/include/asm/device.h
@@@ -17,9 -17,6 +17,12 @@@
  #define _ASM_TILE_DEVICE_H
  
  struct dev_archdata {
++<<<<<<< HEAD
 +	/* DMA operations on that device */
 +        struct dma_map_ops	*dma_ops;
 +
++=======
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  	/* Offset of the DMA address from the PA. */
  	dma_addr_t		dma_offset;
  
diff --cc arch/x86/include/asm/device.h
index 684ed6c3aa67,1b3ef26e77df..000000000000
--- a/arch/x86/include/asm/device.h
+++ b/arch/x86/include/asm/device.h
@@@ -2,9 -2,6 +2,12 @@@
  #define _ASM_X86_DEVICE_H
  
  struct dev_archdata {
++<<<<<<< HEAD
 +#ifdef CONFIG_X86_DEV_DMA_OPS
 +	struct dma_map_ops *dma_ops;
 +#endif
++=======
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
  #if defined(CONFIG_INTEL_IOMMU) || defined(CONFIG_AMD_IOMMU)
  	void *iommu; /* hook for IOMMU specific extension */
  #endif
diff --cc arch/xtensa/include/asm/dma-mapping.h
index 172a02a6ad14,9eecfc3c5dc4..000000000000
--- a/arch/xtensa/include/asm/dma-mapping.h
+++ b/arch/xtensa/include/asm/dma-mapping.h
@@@ -18,171 -18,27 +18,178 @@@
  
  #define DMA_ERROR_CODE		(~(dma_addr_t)0x0)
  
 -extern const struct dma_map_ops xtensa_dma_map_ops;
 +/*
 + * DMA-consistent mapping functions.
 + */
 +
 +extern void *consistent_alloc(int, size_t, dma_addr_t, unsigned long);
 +extern void consistent_free(void*, size_t, dma_addr_t);
 +extern void consistent_sync(void*, size_t, int);
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			   dma_addr_t *dma_handle, gfp_t flag);
 +
 +void dma_free_coherent(struct device *dev, size_t size,
 +			 void *vaddr, dma_addr_t dma_handle);
  
 -static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
  {
++<<<<<<< HEAD
 +	BUG_ON(direction == DMA_NONE);
 +	consistent_sync(ptr, size, direction);
 +	return virt_to_phys(ptr);
++=======
+ 	if (dev && dev->dma_ops)
+ 		return dev->dma_ops;
+ 	else
+ 		return &xtensa_dma_map_ops;
++>>>>>>> 5657933dbb6e (treewide: Move dma_ops from struct dev_archdata into struct device)
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	BUG_ON(direction == DMA_NONE);
 +
 +	for (i = 0; i < nents; i++, sg++ ) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +		consistent_sync(sg_virt(sg), sg->length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return (dma_addr_t)(page_to_pfn(page)) * PAGE_SIZE + offset;
  }
  
 -void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 -		    enum dma_data_direction direction);
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +		enum dma_data_direction direction)
 +{
 +	consistent_sync((void *)bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +		           size_t size, enum dma_data_direction direction)
 +{
 +	consistent_sync((void *)bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +
 +	consistent_sync((void *)bus_to_virt(dma_handle)+offset,size,direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +
 +	consistent_sync((void *)bus_to_virt(dma_handle)+offset,size,direction);
 +}
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction dir)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		consistent_sync(sg_virt(sg), sg->length, dir);
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction dir)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		consistent_sync(sg_virt(sg), sg->length, dir);
 +}
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int
 +dma_supported(struct device *dev, u64 mask)
 +{
 +	return 1;
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if(!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
 +
 +static inline void
 +dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	consistent_sync(vaddr, size, direction);
 +}
  
 -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 +/* Not supported for now */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
  {
 -	return (dma_addr_t)paddr;
 +	return -EINVAL;
  }
  
 -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
  {
 -	return (phys_addr_t)daddr;
 +	return -EINVAL;
  }
  
  #endif	/* _XTENSA_DMA_MAPPING_H */
* Unmerged path arch/m32r/include/asm/dma-mapping.h
* Unmerged path arch/xtensa/include/asm/device.h
* Unmerged path drivers/misc/mic/bus/mic_bus.c
* Unmerged path drivers/misc/mic/bus/scif_bus.c
* Unmerged path drivers/misc/mic/bus/vop_bus.c
* Unmerged path arch/arm/include/asm/device.h
diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 5b579b951503..bd75f54bba53 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -17,15 +17,15 @@ extern struct dma_map_ops arm_coherent_dma_ops;
 
 static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 {
-	if (dev && dev->archdata.dma_ops)
-		return dev->archdata.dma_ops;
+	if (dev && dev->dma_ops)
+		return dev->dma_ops;
 	return &arm_dma_ops;
 }
 
 static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 {
 	BUG_ON(!dev);
-	dev->archdata.dma_ops = ops;
+	dev->dma_ops = ops;
 }
 
 #include <asm-generic/dma-mapping-common.h>
* Unmerged path arch/arm64/include/asm/device.h
* Unmerged path arch/arm64/include/asm/dma-mapping.h
* Unmerged path arch/arm64/mm/dma-mapping.c
* Unmerged path arch/m32r/include/asm/device.h
* Unmerged path arch/m32r/include/asm/dma-mapping.h
* Unmerged path arch/mips/include/asm/device.h
diff --git a/arch/mips/include/asm/dma-mapping.h b/arch/mips/include/asm/dma-mapping.h
index 84238c574d5e..3a64d902f0dd 100644
--- a/arch/mips/include/asm/dma-mapping.h
+++ b/arch/mips/include/asm/dma-mapping.h
@@ -14,8 +14,8 @@ extern struct dma_map_ops *mips_dma_map_ops;
 
 static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 {
-	if (dev && dev->archdata.dma_ops)
-		return dev->archdata.dma_ops;
+	if (dev && dev->dma_ops)
+		return dev->dma_ops;
 	else
 		return mips_dma_map_ops;
 }
diff --git a/arch/mips/pci/pci-octeon.c b/arch/mips/pci/pci-octeon.c
index 95c2ea815cac..1ee73f6c5c09 100644
--- a/arch/mips/pci/pci-octeon.c
+++ b/arch/mips/pci/pci-octeon.c
@@ -167,7 +167,7 @@ int pcibios_plat_dev_init(struct pci_dev *dev)
 		pci_write_config_dword(dev, pos + PCI_ERR_ROOT_STATUS, dconfig);
 	}
 
-	dev->dev.archdata.dma_ops = octeon_pci_dma_map_ops;
+	dev->dev.dma_ops = octeon_pci_dma_map_ops;
 
 	return 0;
 }
* Unmerged path arch/powerpc/include/asm/device.h
diff --git a/arch/powerpc/include/asm/dma-mapping.h b/arch/powerpc/include/asm/dma-mapping.h
index c6f39b7c9f40..20d0d9827678 100644
--- a/arch/powerpc/include/asm/dma-mapping.h
+++ b/arch/powerpc/include/asm/dma-mapping.h
@@ -87,12 +87,12 @@ static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 	if (unlikely(dev == NULL))
 		return NULL;
 
-	return dev->archdata.dma_ops;
+	return dev->dma_ops;
 }
 
 static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 {
-	dev->archdata.dma_ops = ops;
+	dev->dma_ops = ops;
 }
 
 /*
* Unmerged path arch/powerpc/kernel/dma.c
diff --git a/arch/powerpc/kernel/ibmebus.c b/arch/powerpc/kernel/ibmebus.c
index 8220baa46faf..70731b57b74a 100644
--- a/arch/powerpc/kernel/ibmebus.c
+++ b/arch/powerpc/kernel/ibmebus.c
@@ -169,7 +169,7 @@ static int ibmebus_create_device(struct device_node *dn)
 		return -ENOMEM;
 
 	dev->dev.bus = &ibmebus_bus_type;
-	dev->dev.archdata.dma_ops = &ibmebus_dma_ops;
+	dev->dev.dma_ops = &ibmebus_dma_ops;
 
 	ret = of_device_add(dev);
 	if (ret)
diff --git a/arch/powerpc/platforms/cell/iommu.c b/arch/powerpc/platforms/cell/iommu.c
index e1e308110cd8..369347cf2853 100644
--- a/arch/powerpc/platforms/cell/iommu.c
+++ b/arch/powerpc/platforms/cell/iommu.c
@@ -692,7 +692,7 @@ static int cell_of_bus_notify(struct notifier_block *nb, unsigned long action,
 		return 0;
 
 	/* We use the PCI DMA ops */
-	dev->archdata.dma_ops = get_pci_dma_ops();
+	dev->dma_ops = get_pci_dma_ops();
 
 	cell_dma_dev_setup(dev);
 
* Unmerged path arch/powerpc/platforms/pasemi/iommu.c
diff --git a/arch/powerpc/platforms/pasemi/setup.c b/arch/powerpc/platforms/pasemi/setup.c
index 8c54de6d8ec4..76ba348e7a43 100644
--- a/arch/powerpc/platforms/pasemi/setup.c
+++ b/arch/powerpc/platforms/pasemi/setup.c
@@ -368,7 +368,7 @@ static int pcmcia_notify(struct notifier_block *nb, unsigned long action,
 		return 0;
 
 	/* We use the direct ops for localbus */
-	dev->archdata.dma_ops = &dma_direct_ops;
+	dev->dma_ops = &dma_direct_ops;
 
 	return 0;
 }
diff --git a/arch/powerpc/platforms/ps3/system-bus.c b/arch/powerpc/platforms/ps3/system-bus.c
index 5606fe36faf2..45d9872c63ed 100644
--- a/arch/powerpc/platforms/ps3/system-bus.c
+++ b/arch/powerpc/platforms/ps3/system-bus.c
@@ -756,11 +756,11 @@ int ps3_system_bus_device_register(struct ps3_system_bus_device *dev)
 
 	switch (dev->dev_type) {
 	case PS3_DEVICE_TYPE_IOC0:
-		dev->core.archdata.dma_ops = &ps3_ioc0_dma_ops;
+		dev->core.dma_ops = &ps3_ioc0_dma_ops;
 		dev_set_name(&dev->core, "ioc0_%02x", ++dev_ioc0_count);
 		break;
 	case PS3_DEVICE_TYPE_SB:
-		dev->core.archdata.dma_ops = &ps3_sb_dma_ops;
+		dev->core.dma_ops = &ps3_sb_dma_ops;
 		dev_set_name(&dev->core, "sb_%02x", ++dev_sb_count);
 
 		break;
* Unmerged path arch/s390/include/asm/device.h
* Unmerged path arch/s390/include/asm/dma-mapping.h
* Unmerged path arch/s390/pci/pci.c
* Unmerged path arch/tile/include/asm/device.h
diff --git a/arch/tile/include/asm/dma-mapping.h b/arch/tile/include/asm/dma-mapping.h
index f2ff191376b4..b01bb570e118 100644
--- a/arch/tile/include/asm/dma-mapping.h
+++ b/arch/tile/include/asm/dma-mapping.h
@@ -26,8 +26,8 @@ extern struct dma_map_ops *gx_legacy_pci_dma_map_ops;
 
 static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 {
-	if (dev && dev->archdata.dma_ops)
-		return dev->archdata.dma_ops;
+	if (dev && dev->dma_ops)
+		return dev->dma_ops;
 	else
 		return tile_dma_map_ops;
 }
@@ -58,7 +58,7 @@ static inline void dma_mark_clean(void *addr, size_t size) {}
 
 static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 {
-	dev->archdata.dma_ops = ops;
+	dev->dma_ops = ops;
 }
 
 static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
* Unmerged path arch/x86/include/asm/device.h
diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h
index 1f5b7287d1ad..d31efba77ac8 100644
--- a/arch/x86/include/asm/dma-mapping.h
+++ b/arch/x86/include/asm/dma-mapping.h
@@ -34,10 +34,10 @@ static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 #ifndef CONFIG_X86_DEV_DMA_OPS
 	return dma_ops;
 #else
-	if (unlikely(!dev) || !dev->archdata.dma_ops)
+	if (unlikely(!dev) || !dev->dma_ops)
 		return dma_ops;
 	else
-		return dev->archdata.dma_ops;
+		return dev->dma_ops;
 #endif
 }
 
diff --git a/arch/x86/kernel/pci-calgary_64.c b/arch/x86/kernel/pci-calgary_64.c
index 299d49302e7d..603933ff8e9b 100644
--- a/arch/x86/kernel/pci-calgary_64.c
+++ b/arch/x86/kernel/pci-calgary_64.c
@@ -1177,7 +1177,7 @@ static int __init calgary_init(void)
 		tbl = find_iommu_table(&dev->dev);
 
 		if (translation_enabled(tbl))
-			dev->dev.archdata.dma_ops = &calgary_dma_ops;
+			dev->dev.dma_ops = &calgary_dma_ops;
 	}
 
 	return ret;
@@ -1201,7 +1201,7 @@ error:
 		calgary_disable_translation(dev);
 		calgary_free_bus(dev);
 		pci_dev_put(dev); /* Undo calgary_init_one()'s pci_dev_get() */
-		dev->dev.archdata.dma_ops = NULL;
+		dev->dev.dma_ops = NULL;
 	} while (1);
 
 	return ret;
diff --git a/arch/x86/pci/common.c b/arch/x86/pci/common.c
index edc1ade5bddb..0b21f3c497b9 100644
--- a/arch/x86/pci/common.c
+++ b/arch/x86/pci/common.c
@@ -677,7 +677,7 @@ static void set_dma_domain_ops(struct pci_dev *pdev)
 	spin_lock(&dma_domain_list_lock);
 	list_for_each_entry(domain, &dma_domain_list, node) {
 		if (pci_domain_nr(pdev->bus) == domain->domain_nr) {
-			pdev->dev.archdata.dma_ops = domain->dma_ops;
+			pdev->dev.dma_ops = domain->dma_ops;
 			break;
 		}
 	}
diff --git a/arch/x86/pci/sta2x11-fixup.c b/arch/x86/pci/sta2x11-fixup.c
index 9d8a509c9730..8284c6d3a6c5 100644
--- a/arch/x86/pci/sta2x11-fixup.c
+++ b/arch/x86/pci/sta2x11-fixup.c
@@ -205,7 +205,7 @@ static void sta2x11_setup_pdev(struct pci_dev *pdev)
 		return;
 	pci_set_consistent_dma_mask(pdev, STA2X11_AMBA_SIZE - 1);
 	pci_set_dma_mask(pdev, STA2X11_AMBA_SIZE - 1);
-	pdev->dev.archdata.dma_ops = &sta2x11_dma_ops;
+	pdev->dev.dma_ops = &sta2x11_dma_ops;
 
 	/* We must enable all devices as master, for audio DMA to work */
 	pci_set_master(pdev);
@@ -225,7 +225,7 @@ bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 {
 	struct sta2x11_mapping *map;
 
-	if (dev->archdata.dma_ops != &sta2x11_dma_ops) {
+	if (dev->dma_ops != &sta2x11_dma_ops) {
 		if (!dev->dma_mask)
 			return false;
 		return addr + size - 1 <= *dev->dma_mask;
@@ -249,7 +249,7 @@ bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
  */
 dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
-	if (dev->archdata.dma_ops != &sta2x11_dma_ops)
+	if (dev->dma_ops != &sta2x11_dma_ops)
 		return paddr;
 	return p2a(paddr, to_pci_dev(dev));
 }
@@ -261,7 +261,7 @@ dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
  */
 phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 {
-	if (dev->archdata.dma_ops != &sta2x11_dma_ops)
+	if (dev->dma_ops != &sta2x11_dma_ops)
 		return daddr;
 	return a2p(daddr, to_pci_dev(dev));
 }
* Unmerged path arch/xtensa/include/asm/device.h
* Unmerged path arch/xtensa/include/asm/dma-mapping.h
diff --git a/drivers/infiniband/ulp/srpt/ib_srpt.c b/drivers/infiniband/ulp/srpt/ib_srpt.c
index 72146d165e84..d63788268cca 100644
--- a/drivers/infiniband/ulp/srpt/ib_srpt.c
+++ b/drivers/infiniband/ulp/srpt/ib_srpt.c
@@ -2505,7 +2505,7 @@ static void srpt_add_one(struct ib_device *device)
 	int i;
 
 	pr_debug("device = %p, device->dma_ops = %p\n", device,
-		 device->dma_ops);
+		 device->dma_device->dma_ops);
 
 	sdev = kzalloc(sizeof(*sdev), GFP_KERNEL);
 	if (!sdev)
diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c
index 7e1e7da7c05d..04d3093b1614 100644
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@ -447,7 +447,7 @@ static void iommu_uninit_device(struct device *dev)
 	dev_data->alias_data = NULL;
 
 	/* Remove dma-ops */
-	dev->archdata.dma_ops = NULL;
+	dev->dma_ops = NULL;
 
 	/*
 	 * We keep dev_data around for unplugged devices and reuse it when the
@@ -2266,7 +2266,7 @@ static int amd_iommu_add_device(struct device *dev)
 				dev_name(dev));
 
 		iommu_ignore_device(dev);
-		dev->archdata.dma_ops = &nommu_dma_ops;
+		dev->dma_ops = &nommu_dma_ops;
 		goto out;
 	}
 	init_iommu_group(dev);
@@ -2283,7 +2283,7 @@ static int amd_iommu_add_device(struct device *dev)
 	if (domain->type == IOMMU_DOMAIN_IDENTITY)
 		dev_data->passthrough = true;
 	else
-		dev->archdata.dma_ops = &amd_iommu_dma_ops;
+		dev->dma_ops = &amd_iommu_dma_ops;
 
 out:
 	iommu_completion_wait(iommu);
* Unmerged path drivers/misc/mic/bus/mic_bus.c
* Unmerged path drivers/misc/mic/bus/scif_bus.c
* Unmerged path drivers/misc/mic/bus/vop_bus.c
diff --git a/include/linux/device.h b/include/linux/device.h
index 29fb0497afba..961bfe96de10 100644
--- a/include/linux/device.h
+++ b/include/linux/device.h
@@ -783,6 +783,7 @@ struct device {
 #ifdef CONFIG_NUMA
 	int		numa_node;	/* NUMA node this device is close to */
 #endif
+	const struct dma_map_ops *dma_ops;
 	u64		*dma_mask;	/* dma mask (if dma'able device) */
 	u64		coherent_dma_mask;/* Like dma_mask, but for
 					     alloc_coherent mappings as
