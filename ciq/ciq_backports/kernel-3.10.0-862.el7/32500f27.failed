IB/hfi1: Move structure and MACRO definitions in user_sdma.c to user_sdma.h

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Harish Chegondi <harish.chegondi@intel.com>
commit 32500f2763906602725006f1dc833c8ea28dfd07
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/32500f27.failed

Clean up user_sdma.c by moving the structure and MACRO definitions into
the header file user_sdma.h

	Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 32500f2763906602725006f1dc833c8ea28dfd07)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hfi1/user_sdma.c
#	drivers/infiniband/hw/hfi1/user_sdma.h
diff --cc drivers/infiniband/hw/hfi1/user_sdma.c
index b5260e7006cc,dacb0fce49c6..000000000000
--- a/drivers/infiniband/hw/hfi1/user_sdma.c
+++ b/drivers/infiniband/hw/hfi1/user_sdma.c
@@@ -74,235 -74,29 +74,261 @@@ static uint hfi1_sdma_comp_ring_size = 
  module_param_named(sdma_comp_size, hfi1_sdma_comp_ring_size, uint, S_IRUGO);
  MODULE_PARM_DESC(sdma_comp_size, "Size of User SDMA completion ring. Default: 128");
  
++<<<<<<< HEAD
 +/* The maximum number of Data io vectors per message/request */
 +#define MAX_VECTORS_PER_REQ 8
 +/*
 + * Maximum number of packet to send from each message/request
 + * before moving to the next one.
 + */
 +#define MAX_PKTS_PER_QUEUE 16
 +
 +#define num_pages(x) (1 + ((((x) - 1) & PAGE_MASK) >> PAGE_SHIFT))
 +
 +#define req_opcode(x) \
 +	(((x) >> HFI1_SDMA_REQ_OPCODE_SHIFT) & HFI1_SDMA_REQ_OPCODE_MASK)
 +#define req_version(x) \
 +	(((x) >> HFI1_SDMA_REQ_VERSION_SHIFT) & HFI1_SDMA_REQ_OPCODE_MASK)
 +#define req_iovcnt(x) \
 +	(((x) >> HFI1_SDMA_REQ_IOVCNT_SHIFT) & HFI1_SDMA_REQ_IOVCNT_MASK)
 +
 +/* Number of BTH.PSN bits used for sequence number in expected rcvs */
 +#define BTH_SEQ_MASK 0x7ffull
 +
 +/*
 + * Define fields in the KDETH header so we can update the header
 + * template.
 + */
 +#define KDETH_OFFSET_SHIFT        0
 +#define KDETH_OFFSET_MASK         0x7fff
 +#define KDETH_OM_SHIFT            15
 +#define KDETH_OM_MASK             0x1
 +#define KDETH_TID_SHIFT           16
 +#define KDETH_TID_MASK            0x3ff
 +#define KDETH_TIDCTRL_SHIFT       26
 +#define KDETH_TIDCTRL_MASK        0x3
 +#define KDETH_INTR_SHIFT          28
 +#define KDETH_INTR_MASK           0x1
 +#define KDETH_SH_SHIFT            29
 +#define KDETH_SH_MASK             0x1
 +#define KDETH_HCRC_UPPER_SHIFT    16
 +#define KDETH_HCRC_UPPER_MASK     0xff
 +#define KDETH_HCRC_LOWER_SHIFT    24
 +#define KDETH_HCRC_LOWER_MASK     0xff
 +
 +#define AHG_KDETH_INTR_SHIFT 12
 +#define AHG_KDETH_SH_SHIFT   13
 +#define AHG_KDETH_ARRAY_SIZE  9
 +
 +#define PBC2LRH(x) ((((x) & 0xfff) << 2) - 4)
 +#define LRH2PBC(x) ((((x) >> 2) + 1) & 0xfff)
 +
 +#define KDETH_GET(val, field)						\
 +	(((le32_to_cpu((val))) >> KDETH_##field##_SHIFT) & KDETH_##field##_MASK)
 +#define KDETH_SET(dw, field, val) do {					\
 +		u32 dwval = le32_to_cpu(dw);				\
 +		dwval &= ~(KDETH_##field##_MASK << KDETH_##field##_SHIFT); \
 +		dwval |= (((val) & KDETH_##field##_MASK) << \
 +			  KDETH_##field##_SHIFT);			\
 +		dw = cpu_to_le32(dwval);				\
 +	} while (0)
 +
 +#define AHG_HEADER_SET(arr, idx, dw, bit, width, value)			\
 +	do {								\
 +		if ((idx) < ARRAY_SIZE((arr)))				\
 +			(arr)[(idx++)] = sdma_build_ahg_descriptor(	\
 +				(__force u16)(value), (dw), (bit),	\
 +							(width));	\
 +		else							\
 +			return -ERANGE;					\
 +	} while (0)
 +
 +/* KDETH OM multipliers and switch over point */
 +#define KDETH_OM_SMALL     4
 +#define KDETH_OM_SMALL_SHIFT     2
 +#define KDETH_OM_LARGE     64
 +#define KDETH_OM_LARGE_SHIFT     6
 +#define KDETH_OM_MAX_SIZE  (1 << ((KDETH_OM_LARGE / KDETH_OM_SMALL) + 1))
 +
 +/* Tx request flag bits */
 +#define TXREQ_FLAGS_REQ_ACK   BIT(0)      /* Set the ACK bit in the header */
 +#define TXREQ_FLAGS_REQ_DISABLE_SH BIT(1) /* Disable header suppression */
 +
 +#define SDMA_PKT_Q_INACTIVE BIT(0)
 +#define SDMA_PKT_Q_ACTIVE   BIT(1)
 +#define SDMA_PKT_Q_DEFERRED BIT(2)
 +
 +/*
 + * Maximum retry attempts to submit a TX request
 + * before putting the process to sleep.
 + */
 +#define MAX_DEFER_RETRY_COUNT 1
 +
 +static unsigned initial_pkt_count = 8;
 +
 +#define SDMA_IOWAIT_TIMEOUT 1000 /* in milliseconds */
 +
 +struct sdma_mmu_node;
 +
 +struct user_sdma_iovec {
 +	struct list_head list;
 +	struct iovec iov;
 +	/* number of pages in this vector */
 +	unsigned npages;
 +	/* array of pinned pages for this vector */
 +	struct page **pages;
 +	/*
 +	 * offset into the virtual address space of the vector at
 +	 * which we last left off.
 +	 */
 +	u64 offset;
 +	struct sdma_mmu_node *node;
 +};
 +
 +#define SDMA_CACHE_NODE_EVICT 0
 +
 +struct sdma_mmu_node {
 +	struct mmu_rb_node rb;
 +	struct hfi1_user_sdma_pkt_q *pq;
 +	atomic_t refcount;
 +	struct page **pages;
 +	unsigned npages;
 +};
 +
 +/* evict operation argument */
 +struct evict_data {
 +	u32 cleared;	/* count evicted so far */
 +	u32 target;	/* target count to evict */
 +};
 +
 +struct user_sdma_request {
 +	/* This is the original header from user space */
 +	struct hfi1_pkt_header hdr;
 +
 +	/* Read mostly fields */
 +	struct hfi1_user_sdma_pkt_q *pq ____cacheline_aligned_in_smp;
 +	struct hfi1_user_sdma_comp_q *cq;
 +	/*
 +	 * Pointer to the SDMA engine for this request.
 +	 * Since different request could be on different VLs,
 +	 * each request will need it's own engine pointer.
 +	 */
 +	struct sdma_engine *sde;
 +	struct sdma_req_info info;
 +	/* TID array values copied from the tid_iov vector */
 +	u32 *tids;
 +	/* total length of the data in the request */
 +	u32 data_len;
 +	/* number of elements copied to the tids array */
 +	u16 n_tids;
 +	/*
 +	 * We copy the iovs for this request (based on
 +	 * info.iovcnt). These are only the data vectors
 +	 */
 +	u8 data_iovs;
 +	s8 ahg_idx;
 +
 +	/* Writeable fields shared with interrupt */
 +	u64 seqcomp ____cacheline_aligned_in_smp;
 +	u64 seqsubmitted;
 +	/* status of the last txreq completed */
 +	int status;
 +
 +	/* Send side fields */
 +	struct list_head txps ____cacheline_aligned_in_smp;
 +	u64 seqnum;
 +	/*
 +	 * KDETH.OFFSET (TID) field
 +	 * The offset can cover multiple packets, depending on the
 +	 * size of the TID entry.
 +	 */
 +	u32 tidoffset;
 +	/*
 +	 * KDETH.Offset (Eager) field
 +	 * We need to remember the initial value so the headers
 +	 * can be updated properly.
 +	 */
 +	u32 koffset;
 +	u32 sent;
 +	/* TID index copied from the tid_iov vector */
 +	u16 tididx;
 +	/* progress index moving along the iovs array */
 +	u8 iov_idx;
 +	u8 done;
 +	u8 has_error;
 +
 +	struct user_sdma_iovec iovs[MAX_VECTORS_PER_REQ];
 +} ____cacheline_aligned_in_smp;
 +
 +/*
 + * A single txreq could span up to 3 physical pages when the MTU
 + * is sufficiently large (> 4K). Each of the IOV pointers also
 + * needs it's own set of flags so the vector has been handled
 + * independently of each other.
 + */
 +struct user_sdma_txreq {
 +	/* Packet header for the txreq */
 +	struct hfi1_pkt_header hdr;
 +	struct sdma_txreq txreq;
 +	struct list_head list;
 +	struct user_sdma_request *req;
 +	u16 flags;
 +	unsigned busycount;
 +	u64 seqnum;
 +};
 +
 +#define SDMA_DBG(req, fmt, ...)				     \
 +	hfi1_cdbg(SDMA, "[%u:%u:%u:%u] " fmt, (req)->pq->dd->unit, \
 +		 (req)->pq->ctxt, (req)->pq->subctxt, (req)->info.comp_idx, \
 +		 ##__VA_ARGS__)
 +#define SDMA_Q_DBG(pq, fmt, ...)			 \
 +	hfi1_cdbg(SDMA, "[%u:%u:%u] " fmt, (pq)->dd->unit, (pq)->ctxt, \
 +		 (pq)->subctxt, ##__VA_ARGS__)
 +
 +static int user_sdma_send_pkts(struct user_sdma_request *, unsigned);
 +static int num_user_pages(const struct iovec *);
 +static void user_sdma_txreq_cb(struct sdma_txreq *, int);
 +static inline void pq_update(struct hfi1_user_sdma_pkt_q *);
 +static void user_sdma_free_request(struct user_sdma_request *, bool);
 +static int pin_vector_pages(struct user_sdma_request *,
 +			    struct user_sdma_iovec *);
 +static void unpin_vector_pages(struct mm_struct *, struct page **, unsigned,
 +			       unsigned);
 +static int check_header_template(struct user_sdma_request *,
 +				 struct hfi1_pkt_header *, u32, u32);
 +static int set_txreq_header(struct user_sdma_request *,
 +			    struct user_sdma_txreq *, u32);
 +static int set_txreq_header_ahg(struct user_sdma_request *,
 +				struct user_sdma_txreq *, u32);
 +static inline void set_comp_state(struct hfi1_user_sdma_pkt_q *,
 +				  struct hfi1_user_sdma_comp_q *,
 +				  u16, enum hfi1_sdma_comp_state, int);
 +static inline u32 set_pkt_bth_psn(__be32, u8, u32);
++=======
+ static unsigned initial_pkt_count = 8;
+ 
+ static int user_sdma_send_pkts(struct user_sdma_request *req,
+ 			       unsigned maxpkts);
+ static void user_sdma_txreq_cb(struct sdma_txreq *txreq, int status);
+ static inline void pq_update(struct hfi1_user_sdma_pkt_q *pq);
+ static void user_sdma_free_request(struct user_sdma_request *req, bool unpin);
+ static int pin_vector_pages(struct user_sdma_request *req,
+ 			    struct user_sdma_iovec *iovec);
+ static void unpin_vector_pages(struct mm_struct *mm, struct page **pages,
+ 			       unsigned start, unsigned npages);
+ static int check_header_template(struct user_sdma_request *req,
+ 				 struct hfi1_pkt_header *hdr, u32 lrhlen,
+ 				 u32 datalen);
+ static int set_txreq_header(struct user_sdma_request *req,
+ 			    struct user_sdma_txreq *tx, u32 datalen);
+ static int set_txreq_header_ahg(struct user_sdma_request *req,
+ 				struct user_sdma_txreq *tx, u32 len);
+ static inline void set_comp_state(struct hfi1_user_sdma_pkt_q *pq,
+ 				  struct hfi1_user_sdma_comp_q *cq,
+ 				  u16 idx, enum hfi1_sdma_comp_state state,
+ 				  int ret);
+ static inline u32 set_pkt_bth_psn(__be32 bthpsn, u8 expct, u32 frags);
++>>>>>>> 32500f276390 (IB/hfi1: Move structure and MACRO definitions in user_sdma.c to user_sdma.h)
  static inline u32 get_lrh_len(struct hfi1_pkt_header, u32 len);
  
  static int defer_packet_queue(
diff --cc drivers/infiniband/hw/hfi1/user_sdma.h
index 39001714f551,6c10484e972f..000000000000
--- a/drivers/infiniband/hw/hfi1/user_sdma.h
+++ b/drivers/infiniband/hw/hfi1/user_sdma.h
@@@ -78,7 -140,117 +139,124 @@@ struct hfi1_user_sdma_comp_q 
  	struct hfi1_sdma_comp_entry *comps;
  };
  
++<<<<<<< HEAD
 +int hfi1_user_sdma_alloc_queues(struct hfi1_ctxtdata *, struct file *);
 +int hfi1_user_sdma_free_queues(struct hfi1_filedata *);
 +int hfi1_user_sdma_process_request(struct file *, struct iovec *, unsigned long,
 +				   unsigned long *);
++=======
+ struct sdma_mmu_node {
+ 	struct mmu_rb_node rb;
+ 	struct hfi1_user_sdma_pkt_q *pq;
+ 	atomic_t refcount;
+ 	struct page **pages;
+ 	unsigned int npages;
+ };
+ 
+ struct user_sdma_iovec {
+ 	struct list_head list;
+ 	struct iovec iov;
+ 	/* number of pages in this vector */
+ 	unsigned int npages;
+ 	/* array of pinned pages for this vector */
+ 	struct page **pages;
+ 	/*
+ 	 * offset into the virtual address space of the vector at
+ 	 * which we last left off.
+ 	 */
+ 	u64 offset;
+ 	struct sdma_mmu_node *node;
+ };
+ 
+ /* evict operation argument */
+ struct evict_data {
+ 	u32 cleared;	/* count evicted so far */
+ 	u32 target;	/* target count to evict */
+ };
+ 
+ struct user_sdma_request {
+ 	/* This is the original header from user space */
+ 	struct hfi1_pkt_header hdr;
+ 
+ 	/* Read mostly fields */
+ 	struct hfi1_user_sdma_pkt_q *pq ____cacheline_aligned_in_smp;
+ 	struct hfi1_user_sdma_comp_q *cq;
+ 	/*
+ 	 * Pointer to the SDMA engine for this request.
+ 	 * Since different request could be on different VLs,
+ 	 * each request will need it's own engine pointer.
+ 	 */
+ 	struct sdma_engine *sde;
+ 	struct sdma_req_info info;
+ 	/* TID array values copied from the tid_iov vector */
+ 	u32 *tids;
+ 	/* total length of the data in the request */
+ 	u32 data_len;
+ 	/* number of elements copied to the tids array */
+ 	u16 n_tids;
+ 	/*
+ 	 * We copy the iovs for this request (based on
+ 	 * info.iovcnt). These are only the data vectors
+ 	 */
+ 	u8 data_iovs;
+ 	s8 ahg_idx;
+ 
+ 	/* Writeable fields shared with interrupt */
+ 	u64 seqcomp ____cacheline_aligned_in_smp;
+ 	u64 seqsubmitted;
+ 	/* status of the last txreq completed */
+ 	int status;
+ 
+ 	/* Send side fields */
+ 	struct list_head txps ____cacheline_aligned_in_smp;
+ 	u64 seqnum;
+ 	/*
+ 	 * KDETH.OFFSET (TID) field
+ 	 * The offset can cover multiple packets, depending on the
+ 	 * size of the TID entry.
+ 	 */
+ 	u32 tidoffset;
+ 	/*
+ 	 * KDETH.Offset (Eager) field
+ 	 * We need to remember the initial value so the headers
+ 	 * can be updated properly.
+ 	 */
+ 	u32 koffset;
+ 	u32 sent;
+ 	/* TID index copied from the tid_iov vector */
+ 	u16 tididx;
+ 	/* progress index moving along the iovs array */
+ 	u8 iov_idx;
+ 	u8 done;
+ 	u8 has_error;
+ 
+ 	struct user_sdma_iovec iovs[MAX_VECTORS_PER_REQ];
+ } ____cacheline_aligned_in_smp;
+ 
+ /*
+  * A single txreq could span up to 3 physical pages when the MTU
+  * is sufficiently large (> 4K). Each of the IOV pointers also
+  * needs it's own set of flags so the vector has been handled
+  * independently of each other.
+  */
+ struct user_sdma_txreq {
+ 	/* Packet header for the txreq */
+ 	struct hfi1_pkt_header hdr;
+ 	struct sdma_txreq txreq;
+ 	struct list_head list;
+ 	struct user_sdma_request *req;
+ 	u16 flags;
+ 	unsigned int busycount;
+ 	u64 seqnum;
+ };
+ 
+ int hfi1_user_sdma_alloc_queues(struct hfi1_ctxtdata *uctxt,
+ 				struct hfi1_filedata *fd);
+ int hfi1_user_sdma_free_queues(struct hfi1_filedata *fd,
+ 			       struct hfi1_ctxtdata *uctxt);
+ int hfi1_user_sdma_process_request(struct hfi1_filedata *fd,
+ 				   struct iovec *iovec, unsigned long dim,
+ 				   unsigned long *count);
+ 
+ #endif /* _HFI1_USER_SDMA_H */
++>>>>>>> 32500f276390 (IB/hfi1: Move structure and MACRO definitions in user_sdma.c to user_sdma.h)
* Unmerged path drivers/infiniband/hw/hfi1/user_sdma.c
* Unmerged path drivers/infiniband/hw/hfi1/user_sdma.h
