netvsc: optimize receive completions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author stephen hemminger <stephen@networkplumber.org>
commit 7426b1a51803ba2d368177363a134b98b0a8d1c0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7426b1a5.failed

Optimize how receive completion ring are managed.
   * Allocate only as many slots as needed for all buffers from host
   * Allocate before setting up sub channel for better error detection
   * Don't need to keep copy of initial receive section message
   * Precompute the watermark for when receive flushing is needed
   * Replace division with conditional test
   * Replace atomic per-device variable with per-channel check.
   * Handle corner case where receive completion send
     fails if ring buffer to host is full.

	Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 7426b1a51803ba2d368177363a134b98b0a8d1c0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/hyperv/hyperv_net.h
#	drivers/net/hyperv/netvsc.c
#	drivers/net/hyperv/rndis_filter.c
diff --cc drivers/net/hyperv/hyperv_net.h
index a32d7f1b2505,f2cef5aaed1f..000000000000
--- a/drivers/net/hyperv/hyperv_net.h
+++ b/drivers/net/hyperv/hyperv_net.h
@@@ -180,12 -182,16 +180,20 @@@ struct rndis_device 
  /* Interface */
  struct rndis_message;
  struct netvsc_device;
++<<<<<<< HEAD
 +int netvsc_device_add(struct hv_device *device, void *additional_info);
++=======
+ struct net_device_context;
+ 
+ struct netvsc_device *netvsc_device_add(struct hv_device *device,
+ 					const struct netvsc_device_info *info);
+ int netvsc_alloc_recv_comp_ring(struct netvsc_device *net_device, u32 q_idx);
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  void netvsc_device_remove(struct hv_device *device);
 -int netvsc_send(struct net_device_context *ndc,
 +int netvsc_send(struct hv_device *device,
  		struct hv_netvsc_packet *packet,
  		struct rndis_message *rndis_msg,
 -		struct hv_page_buffer *page_buffer,
 +		struct hv_page_buffer **page_buffer,
  		struct sk_buff *skb);
  void netvsc_linkstatus_callback(struct hv_device *device_obj,
  				struct rndis_message *resp);
@@@ -767,23 -776,12 +772,26 @@@ struct netvsc_device 
  	u32 max_pkt; /* max number of pkt in one send, e.g. 8 */
  	u32 pkt_align; /* alignment bytes, e.g. 8 */
  
++<<<<<<< HEAD
 +	struct multi_recv_comp mrc[VRSS_CHANNEL_MAX];
 +	atomic_t num_outstanding_recvs;
 +
++=======
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  	atomic_t open_cnt;
 +};
  
 -	struct netvsc_channel chan_table[VRSS_CHANNEL_MAX];
 +static inline struct netvsc_device *
 +net_device_to_netvsc_device(struct net_device *ndev)
 +{
 +	return ((struct net_device_context *)netdev_priv(ndev))->nvdev;
 +}
  
 -	struct rcu_head rcu;
 -};
 +static inline struct netvsc_device *
 +hv_device_to_netvsc_device(struct hv_device *device)
 +{
 +	return net_device_to_netvsc_device(hv_get_drvdata(device));
 +}
  
  /* NdisInitialize message */
  struct rndis_initialize_request {
diff --cc drivers/net/hyperv/netvsc.c
index 52667f48ba1c,4c709b454d34..000000000000
--- a/drivers/net/hyperv/netvsc.c
+++ b/drivers/net/hyperv/netvsc.c
@@@ -69,15 -72,6 +69,18 @@@ static struct netvsc_device *alloc_net_
  	if (!net_device)
  		return NULL;
  
++<<<<<<< HEAD
 +	net_device->cb_buffer = kzalloc(NETVSC_PACKET_SIZE, GFP_KERNEL);
 +	if (!net_device->cb_buffer) {
 +		kfree(net_device);
 +		return NULL;
 +	}
 +
 +	net_device->mrc[0].buf = vzalloc(NETVSC_RECVSLOT_MAX *
 +					 sizeof(struct recv_comp_data));
 +
++=======
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  	init_waitqueue_head(&net_device->wait_drain);
  	net_device->destroy = false;
  	atomic_set(&net_device->open_cnt, 0);
@@@ -93,9 -87,10 +96,13 @@@ static void free_netvsc_device(struct n
  	int i;
  
  	for (i = 0; i < VRSS_CHANNEL_MAX; i++)
++<<<<<<< HEAD
 +		vfree(nvdev->mrc[i].buf);
++=======
+ 		vfree(nvdev->chan_table[i].mrc.slots);
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  
 +	kfree(nvdev->cb_buffer);
  	kfree(nvdev);
  }
  
@@@ -981,136 -955,102 +995,198 @@@ send_now
  	return ret;
  }
  
- static int netvsc_send_recv_completion(struct vmbus_channel *channel,
- 				       u64 transaction_id, u32 status)
+ /* Send pending recv completions */
+ static int send_recv_completions(struct netvsc_channel *nvchan)
  {
- 	struct nvsp_message recvcompMessage;
+ 	struct netvsc_device *nvdev = nvchan->net_device;
+ 	struct multi_recv_comp *mrc = &nvchan->mrc;
+ 	struct recv_comp_msg {
+ 		struct nvsp_message_header hdr;
+ 		u32 status;
+ 	}  __packed;
+ 	struct recv_comp_msg msg = {
+ 		.hdr.msg_type = NVSP_MSG1_TYPE_SEND_RNDIS_PKT_COMPLETE,
+ 	};
  	int ret;
  
- 	recvcompMessage.hdr.msg_type =
- 				NVSP_MSG1_TYPE_SEND_RNDIS_PKT_COMPLETE;
+ 	while (mrc->first != mrc->next) {
+ 		const struct recv_comp_data *rcd
+ 			= mrc->slots + mrc->first;
  
- 	recvcompMessage.msg.v1_msg.send_rndis_pkt_complete.status = status;
+ 		msg.status = rcd->status;
+ 		ret = vmbus_sendpacket(nvchan->channel, &msg, sizeof(msg),
+ 				       rcd->tid, VM_PKT_COMP, 0);
+ 		if (unlikely(ret))
+ 			return ret;
  
- 	/* Send the completion */
- 	ret = vmbus_sendpacket(channel, &recvcompMessage,
- 			       sizeof(struct nvsp_message_header) + sizeof(u32),
- 			       transaction_id, VM_PKT_COMP, 0);
+ 		if (++mrc->first == nvdev->recv_completion_cnt)
+ 			mrc->first = 0;
+ 	}
  
- 	return ret;
+ 	/* receive completion ring has been emptied */
+ 	if (unlikely(nvdev->destroy))
+ 		wake_up(&nvdev->wait_drain);
+ 
+ 	return 0;
  }
  
- static inline void count_recv_comp_slot(struct netvsc_device *nvdev, u16 q_idx,
- 					u32 *filled, u32 *avail)
+ /* Count how many receive completions are outstanding */
+ static void recv_comp_slot_avail(const struct netvsc_device *nvdev,
+ 				 const struct multi_recv_comp *mrc,
+ 				 u32 *filled, u32 *avail)
  {
++<<<<<<< HEAD
 +	u32 first = nvdev->mrc[q_idx].first;
 +	u32 next = nvdev->mrc[q_idx].next;
++=======
+ 	u32 count = nvdev->recv_completion_cnt;
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  
- 	*filled = (first > next) ? NETVSC_RECVSLOT_MAX - first + next :
- 		  next - first;
+ 	if (mrc->next >= mrc->first)
+ 		*filled = mrc->next - mrc->first;
+ 	else
+ 		*filled = (count - mrc->first) + mrc->next;
  
- 	*avail = NETVSC_RECVSLOT_MAX - *filled - 1;
+ 	*avail = count - *filled - 1;
  }
  
- /* Read the first filled slot, no change to index */
- static inline struct recv_comp_data *read_recv_comp_slot(struct netvsc_device
- 							 *nvdev, u16 q_idx)
+ /* Add receive complete to ring to send to host. */
+ static void enq_receive_complete(struct net_device *ndev,
+ 				 struct netvsc_device *nvdev, u16 q_idx,
+ 				 u64 tid, u32 status)
  {
++<<<<<<< HEAD
 +	u32 filled, avail;
 +
 +	if (!nvdev->mrc[q_idx].buf)
 +		return NULL;
 +
 +	count_recv_comp_slot(nvdev, q_idx, &filled, &avail);
 +	if (!filled)
 +		return NULL;
 +
 +	return nvdev->mrc[q_idx].buf + nvdev->mrc[q_idx].first *
 +	       sizeof(struct recv_comp_data);
 +}
 +
 +/* Put the first filled slot back to available pool */
 +static inline void put_recv_comp_slot(struct netvsc_device *nvdev, u16 q_idx)
 +{
 +	int num_recv;
 +
 +	nvdev->mrc[q_idx].first = (nvdev->mrc[q_idx].first + 1) %
 +				  NETVSC_RECVSLOT_MAX;
 +
 +	num_recv = atomic_dec_return(&nvdev->num_outstanding_recvs);
 +
 +	if (nvdev->destroy && num_recv == 0)
 +		wake_up(&nvdev->wait_drain);
 +}
 +
 +/* Check and send pending recv completions */
 +static void netvsc_chk_recv_comp(struct netvsc_device *nvdev,
 +				 struct vmbus_channel *channel, u16 q_idx)
 +{
 +	struct recv_comp_data *rcd;
 +	int ret;
 +
 +	while (true) {
 +		rcd = read_recv_comp_slot(nvdev, q_idx);
 +		if (!rcd)
 +			break;
 +
 +		ret = netvsc_send_recv_completion(channel, rcd->tid,
 +						  rcd->status);
 +		if (ret)
 +			break;
 +
 +		put_recv_comp_slot(nvdev, q_idx);
++=======
+ 	struct netvsc_channel *nvchan = &nvdev->chan_table[q_idx];
+ 	struct multi_recv_comp *mrc = &nvchan->mrc;
+ 	struct recv_comp_data *rcd;
+ 	u32 filled, avail;
+ 
+ 	recv_comp_slot_avail(nvdev, mrc, &filled, &avail);
+ 
+ 	if (unlikely(filled > NAPI_POLL_WEIGHT)) {
+ 		send_recv_completions(nvchan);
+ 		recv_comp_slot_avail(nvdev, mrc, &filled, &avail);
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  	}
- }
  
- #define NETVSC_RCD_WATERMARK 80
+ 	if (unlikely(!avail)) {
+ 		netdev_err(ndev, "Recv_comp full buf q:%hd, tid:%llx\n",
+ 			   q_idx, tid);
+ 		return;
+ 	}
  
++<<<<<<< HEAD
 +/* Get next available slot */
 +static inline struct recv_comp_data *get_recv_comp_slot(
 +	struct netvsc_device *nvdev, struct vmbus_channel *channel, u16 q_idx)
 +{
 +	u32 filled, avail, next;
 +	struct recv_comp_data *rcd;
 +
 +	if (!nvdev->recv_section)
 +		return NULL;
 +
 +	if (!nvdev->mrc[q_idx].buf)
 +		return NULL;
 +
 +	if (atomic_read(&nvdev->num_outstanding_recvs) >
 +	    nvdev->recv_section->num_sub_allocs * NETVSC_RCD_WATERMARK / 100)
 +		netvsc_chk_recv_comp(nvdev, channel, q_idx);
 +
 +	count_recv_comp_slot(nvdev, q_idx, &filled, &avail);
 +	if (!avail)
 +		return NULL;
 +
 +	next = nvdev->mrc[q_idx].next;
 +	rcd = nvdev->mrc[q_idx].buf + next * sizeof(struct recv_comp_data);
 +	nvdev->mrc[q_idx].next = (next + 1) % NETVSC_RECVSLOT_MAX;
 +
 +	atomic_inc(&nvdev->num_outstanding_recvs);
 +
 +	return rcd;
 +}
 +
 +static void netvsc_receive(struct net_device *ndev,
 +		   struct netvsc_device *net_device,
 +		   struct net_device_context *net_device_ctx,
 +		   struct hv_device *device,
 +		   struct vmbus_channel *channel,
 +		   struct vmtransfer_page_packet_header *vmxferpage_packet,
 +		   struct nvsp_message *nvsp)
++=======
+ 	rcd = mrc->slots + mrc->next;
+ 	rcd->tid = tid;
+ 	rcd->status = status;
+ 
+ 	if (++mrc->next == nvdev->recv_completion_cnt)
+ 		mrc->next = 0;
+ }
+ 
+ static int netvsc_receive(struct net_device *ndev,
+ 			  struct netvsc_device *net_device,
+ 			  struct net_device_context *net_device_ctx,
+ 			  struct hv_device *device,
+ 			  struct vmbus_channel *channel,
+ 			  const struct vmpacket_descriptor *desc,
+ 			  struct nvsp_message *nvsp)
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  {
 -	const struct vmtransfer_page_packet_header *vmxferpage_packet
 -		= container_of(desc, const struct vmtransfer_page_packet_header, d);
 -	u16 q_idx = channel->offermsg.offer.sub_channel_index;
  	char *recv_buf = net_device->recv_buf;
  	u32 status = NVSP_STAT_SUCCESS;
  	int i;
  	int count = 0;
++<<<<<<< HEAD
 +	int ret;
 +	struct recv_comp_data *rcd;
 +	u16 q_idx = channel->offermsg.offer.sub_channel_index;
++=======
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  
  	/* Make sure this is a valid nvsp packet */
  	if (unlikely(nvsp->hdr.msg_type != NVSP_MSG1_TYPE_SEND_RNDIS_PKT)) {
@@@ -1141,26 -1081,10 +1217,33 @@@
  					      channel, data, buflen);
  	}
  
++<<<<<<< HEAD
 +	if (!net_device->mrc[q_idx].buf) {
 +		ret = netvsc_send_recv_completion(channel,
 +						  vmxferpage_packet->d.trans_id,
 +						  status);
 +		if (ret)
 +			netdev_err(ndev, "Recv_comp q:%hd, tid:%llx, err:%d\n",
 +				   q_idx, vmxferpage_packet->d.trans_id, ret);
 +		return;
 +	}
 +
 +	rcd = get_recv_comp_slot(net_device, channel, q_idx);
 +
 +	if (!rcd) {
 +		netdev_err(ndev, "Recv_comp full buf q:%hd, tid:%llx\n",
 +			   q_idx, vmxferpage_packet->d.trans_id);
 +		return;
 +	}
 +
 +	rcd->tid = vmxferpage_packet->d.trans_id;
 +	rcd->status = status;
++=======
+ 	enq_receive_complete(ndev, net_device, q_idx,
+ 			     vmxferpage_packet->d.trans_id, status);
+ 
+ 	return count;
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  }
  
  static void netvsc_send_table(struct hv_device *hdev,
@@@ -1240,101 -1157,79 +1323,155 @@@ static void netvsc_process_raw_pkt(stru
  
  	default:
  		netdev_err(ndev, "unhandled packet type %d, tid %llx\n",
 -			   desc->type, desc->trans_id);
 +			   desc->type, request_id);
  		break;
  	}
 -
 -	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static struct hv_device *netvsc_channel_to_device(struct vmbus_channel *channel)
+ {
+ 	struct vmbus_channel *primary = channel->primary_channel;
+ 
+ 	return primary ? primary->device_obj : channel->device_obj;
+ }
+ 
+ /* Network processing softirq
+  * Process data in incoming ring buffer from host
+  * Stops when ring is empty or budget is met or exceeded.
+  */
+ int netvsc_poll(struct napi_struct *napi, int budget)
+ {
+ 	struct netvsc_channel *nvchan
+ 		= container_of(napi, struct netvsc_channel, napi);
+ 	struct netvsc_device *net_device = nvchan->net_device;
+ 	struct vmbus_channel *channel = nvchan->channel;
+ 	struct hv_device *device = netvsc_channel_to_device(channel);
+ 	struct net_device *ndev = hv_get_drvdata(device);
+ 	int work_done = 0;
+ 
+ 	/* If starting a new interval */
+ 	if (!nvchan->desc)
+ 		nvchan->desc = hv_pkt_iter_first(channel);
+ 
+ 	while (nvchan->desc && work_done < budget) {
+ 		work_done += netvsc_process_raw_pkt(device, channel, net_device,
+ 						    ndev, nvchan->desc, budget);
+ 		nvchan->desc = hv_pkt_iter_next(channel, nvchan->desc);
+ 	}
+ 
+ 	/* If send of  pending receive completions suceeded
+ 	 *   and did not exhaust NAPI budget
+ 	 *   and not doing busy poll
+ 	 * then reschedule if more data has arrived from host
+ 	 */
+ 	if (send_recv_completions(nvchan) == 0 &&
+ 	    work_done < budget &&
+ 	    napi_complete_done(napi, work_done) &&
+ 	    hv_end_read(&channel->inbound)) {
+ 		hv_begin_read(&channel->inbound);
+ 		napi_reschedule(napi);
+ 	}
+ 
+ 	/* Driver may overshoot since multiple packets per descriptor */
+ 	return min(work_done, budget);
+ }
+ 
+ /* Call back when data is available in host ring buffer.
+  * Processing is deferred until network softirq (NAPI)
+  */
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  void netvsc_channel_cb(void *context)
  {
 -	struct netvsc_channel *nvchan = context;
 -	struct vmbus_channel *channel = nvchan->channel;
 -	struct hv_ring_buffer_info *rbi = &channel->inbound;
 +	int ret;
 +	struct vmbus_channel *channel = (struct vmbus_channel *)context;
 +	u16 q_idx = channel->offermsg.offer.sub_channel_index;
 +	struct hv_device *device;
 +	struct netvsc_device *net_device;
 +	u32 bytes_recvd;
 +	u64 request_id;
 +	struct vmpacket_descriptor *desc;
 +	unsigned char *buffer;
 +	int bufferlen = NETVSC_PACKET_SIZE;
 +	struct net_device *ndev;
 +	bool need_to_commit = false;
 +
 +	if (channel->primary_channel != NULL)
 +		device = channel->primary_channel->device_obj;
 +	else
 +		device = channel->device_obj;
  
 -	/* preload first vmpacket descriptor */
 -	prefetch(hv_get_ring_buffer(rbi) + rbi->priv_read_index);
 +	net_device = get_inbound_net_device(device);
 +	if (!net_device)
 +		return;
 +	ndev = hv_get_drvdata(device);
 +	buffer = get_per_channel_state(channel);
 +
 +	/* commit_rd_index() -> hv_signal_on_read() needs this. */
 +	init_cached_read_index(channel);
 +
 +	do {
 +		desc = get_next_pkt_raw(channel);
 +		if (desc != NULL) {
 +			netvsc_process_raw_pkt(device,
 +					       channel,
 +					       net_device,
 +					       ndev,
 +					       desc->trans_id,
 +					       desc);
 +
 +			put_pkt_raw(channel, desc);
 +			need_to_commit = true;
 +			continue;
 +		}
 +		if (need_to_commit) {
 +			need_to_commit = false;
 +			commit_rd_index(channel);
 +		}
 +
 +		ret = vmbus_recvpacket_raw(channel, buffer, bufferlen,
 +					   &bytes_recvd, &request_id);
 +		if (ret == 0) {
 +			if (bytes_recvd > 0) {
 +				desc = (struct vmpacket_descriptor *)buffer;
 +				netvsc_process_raw_pkt(device,
 +						       channel,
 +						       net_device,
 +						       ndev,
 +						       request_id,
 +						       desc);
 +			} else {
 +				/*
 +				 * We are done for this pass.
 +				 */
 +				break;
 +			}
 +
 +		} else if (ret == -ENOBUFS) {
 +			if (bufferlen > NETVSC_PACKET_SIZE)
 +				kfree(buffer);
 +			/* Handle large packet */
 +			buffer = kmalloc(bytes_recvd, GFP_ATOMIC);
 +			if (buffer == NULL) {
 +				/* Try again next time around */
 +				netdev_err(ndev,
 +					   "unable to allocate buffer of size "
 +					   "(%d)!!\n", bytes_recvd);
 +				break;
 +			}
 +
 +			bufferlen = bytes_recvd;
 +		}
  
 -	if (napi_schedule_prep(&nvchan->napi)) {
 -		/* disable interupts from host */
 -		hv_begin_read(rbi);
 +		init_cached_read_index(channel);
  
 -		__napi_schedule(&nvchan->napi);
 -	}
 +	} while (1);
 +
 +	if (bufferlen > NETVSC_PACKET_SIZE)
 +		kfree(buffer);
 +
 +	netvsc_chk_recv_comp(net_device, channel, q_idx);
  }
  
  /*
diff --cc drivers/net/hyperv/rndis_filter.c
index b6bc44dd1f92,44165fe328a4..000000000000
--- a/drivers/net/hyperv/rndis_filter.c
+++ b/drivers/net/hyperv/rndis_filter.c
@@@ -831,6 -924,23 +831,26 @@@ cleanup
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static bool netvsc_device_idle(const struct netvsc_device *nvdev)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < nvdev->num_chn; i++) {
+ 		const struct netvsc_channel *nvchan = &nvdev->chan_table[i];
+ 
+ 		if (nvchan->mrc.first != nvchan->mrc.next)
+ 			return false;
+ 
+ 		if (atomic_read(&nvchan->queue_sends) > 0)
+ 			return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  static void rndis_filter_halt_device(struct rndis_device *dev)
  {
  	struct rndis_request *request;
@@@ -906,19 -1017,20 +926,23 @@@ static void netvsc_sc_open(struct vmbus
  {
  	struct net_device *ndev =
  		hv_get_drvdata(new_sc->primary_channel->device_obj);
 -	struct net_device_context *ndev_ctx = netdev_priv(ndev);
 -	struct netvsc_device *nvscdev;
 +	struct netvsc_device *nvscdev = net_device_to_netvsc_device(ndev);
  	u16 chn_index = new_sc->offermsg.offer.sub_channel_index;
 -	struct netvsc_channel *nvchan;
  	int ret;
 +	unsigned long flags;
  
 -	/* This is safe because this callback only happens when
 -	 * new device is being setup and waiting on the channel_init_wait.
 -	 */
 -	nvscdev = rcu_dereference_raw(ndev_ctx->nvdev);
 -	if (!nvscdev || chn_index >= nvscdev->num_chn)
 +	if (chn_index >= nvscdev->num_chn)
  		return;
  
++<<<<<<< HEAD
 +	set_per_channel_state(new_sc, nvscdev->sub_cb_buf + (chn_index - 1) *
 +			      NETVSC_PACKET_SIZE);
 +
 +	nvscdev->mrc[chn_index].buf = vzalloc(NETVSC_RECVSLOT_MAX *
 +					      sizeof(struct recv_comp_data));
++=======
+ 	nvchan = nvscdev->chan_table + chn_index;
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  
  	/* Because the device uses NAPI, all the interrupt batching and
  	 * control is done via Net softirq, not the channel handling
@@@ -1070,19 -1216,20 +1094,32 @@@ int rndis_filter_device_add(struct hv_d
  		rndis_device->ind_table[i] = ethtool_rxfh_indir_default(i,
  							net_device->num_chn);
  
 -	num_rss_qs = net_device->num_chn - 1;
 -	if (num_rss_qs == 0)
 -		return net_device;
 +	net_device->num_sc_offered = num_rss_qs;
 +
 +	if (net_device->num_chn == 1)
 +		goto out;
  
 +	net_device->sub_cb_buf = vzalloc((net_device->num_chn - 1) *
 +					 NETVSC_PACKET_SIZE);
 +	if (!net_device->sub_cb_buf) {
 +		net_device->num_chn = 1;
 +		dev_info(&dev->device, "No memory for subchannels.\n");
 +		goto out;
 +	}
 +
++<<<<<<< HEAD
++=======
+ 	for (i = 1; i < net_device->num_chn; i++) {
+ 		ret = netvsc_alloc_recv_comp_ring(net_device, i);
+ 		if (ret) {
+ 			while (--i != 0)
+ 				vfree(net_device->chan_table[i].mrc.slots);
+ 			goto out;
+ 		}
+ 	}
+ 
+ 	refcount_set(&net_device->sc_offered, num_rss_qs);
++>>>>>>> 7426b1a51803 (netvsc: optimize receive completions)
  	vmbus_set_sc_create_callback(dev->channel, netvsc_sc_open);
  
  	init_packet = &net_device->channel_init_pkt;
* Unmerged path drivers/net/hyperv/hyperv_net.h
* Unmerged path drivers/net/hyperv/netvsc.c
* Unmerged path drivers/net/hyperv/rndis_filter.c
