cpufreq: intel_pstate: Remove max/min fractions to limit performance

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Remove max/min fractions to limit performance (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 92.91%
commit-author Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
commit 1a4fe38add8be45eb3873ad756561b9baf6bcaef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1a4fe38a.failed

In the current model the max/min perf limits are a fraction of current
user space limits to the allowed max_freq or 100% for global limits.
This results in wrong ratio limits calculation because of rounding
issues for some user space limits.

Initially we tried to solve this issue by issue by having more shift
bits to increase precision. Still there are isolated cases where we still
have error.

This can be avoided by using ratios all together. Since the way we get
cpuinfo.max_freq is by multiplying scaling factor to max ratio, we can
easily keep the max/min ratios in terms of ratios and not fractions.

For example:
if the max ratio = 36
cpuinfo.max_freq = 36 * 100000 = 3600000

Suppose user space sets a limit of 1200000, then we can calculate
max ratio limit as
= 36 * 1200000 / 3600000
= 12
This will be correct for any user limits.

The other advantage is that, we don't need to do any calculation in the
fast path as ratio limit is already calculated via set_policy() callback.

	Signed-off-by: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 1a4fe38add8be45eb3873ad756561b9baf6bcaef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index b6f8db18a31a,be0290a1a5e2..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -175,8 -231,18 +175,13 @@@ struct _pid 
   * @prev_cummulative_iowait: IO Wait time difference from last and
   *			current sample
   * @sample:		Storage for storing last Sample data
++<<<<<<< HEAD
++=======
+  * @min_perf_ratio:	Minimum capacity in terms of PERF or HWP ratios
+  * @max_perf_ratio:	Maximum capacity in terms of PERF or HWP ratios
++>>>>>>> 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
   * @acpi_perf_data:	Stores ACPI perf information read from _PSS
   * @valid_pss_table:	Set to true for valid ACPI _PSS entries found
 - * @epp_powersave:	Last saved HWP energy performance preference
 - *			(EPP) or energy performance bias (EPB),
 - *			when policy switched to performance
 - * @epp_policy:		Last saved policy used to set EPP/EPB
 - * @epp_default:	Power on default HWP energy performance
 - *			preference/bias
 - * @epp_saved:		Saved EPP/EPB during system suspend or CPU offline
 - *			operation
   *
   * This structure stores per CPU instance data for all CPUs.
   */
@@@ -195,6 -264,8 +200,11 @@@ struct cpudata 
  	u64	prev_tsc;
  	u64	prev_cummulative_iowait;
  	struct sample sample;
++<<<<<<< HEAD
++=======
+ 	int32_t	min_perf_ratio;
+ 	int32_t	max_perf_ratio;
++>>>>>>> 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
  #ifdef CONFIG_ACPI
  	struct acpi_processor_performance acpi_perf_data;
  	bool valid_pss_table;
@@@ -541,40 -566,348 +551,293 @@@ static inline void update_turbo_state(v
  		 cpu->pstate.max_pstate == cpu->pstate.turbo_pstate);
  }
  
 -static int min_perf_pct_min(void)
 +static void intel_pstate_hwp_set(const struct cpumask *cpumask)
  {
++<<<<<<< HEAD
 +	int min, hw_min, max, hw_max, cpu, range, adj_range;
 +	u64 value, cap;
 +
 +	for_each_cpu(cpu, cpumask) {
 +		rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
 +		hw_min = HWP_LOWEST_PERF(cap);
 +		if (limits->no_turbo)
 +			hw_max = HWP_GUARANTEED_PERF(cap);
 +		else
 +			hw_max = HWP_HIGHEST_PERF(cap);
 +		range = hw_max - hw_min;
 +
 +		rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
 +		adj_range = limits->min_perf_pct * range / 100;
 +		min = hw_min + adj_range;
 +		value &= ~HWP_MIN_PERF(~0L);
 +		value |= HWP_MIN_PERF(min);
++=======
+ 	struct cpudata *cpu = all_cpu_data[0];
+ 	int turbo_pstate = cpu->pstate.turbo_pstate;
+ 
+ 	return turbo_pstate ?
+ 		DIV_ROUND_UP(cpu->pstate.min_pstate * 100, turbo_pstate) : 0;
+ }
+ 
+ static s16 intel_pstate_get_epb(struct cpudata *cpu_data)
+ {
+ 	u64 epb;
+ 	int ret;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_EPB))
+ 		return -ENXIO;
+ 
+ 	ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
+ 	if (ret)
+ 		return (s16)ret;
+ 
+ 	return (s16)(epb & 0x0f);
+ }
+ 
+ static s16 intel_pstate_get_epp(struct cpudata *cpu_data, u64 hwp_req_data)
+ {
+ 	s16 epp;
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		/*
+ 		 * When hwp_req_data is 0, means that caller didn't read
+ 		 * MSR_HWP_REQUEST, so need to read and get EPP.
+ 		 */
+ 		if (!hwp_req_data) {
+ 			epp = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST,
+ 					    &hwp_req_data);
+ 			if (epp)
+ 				return epp;
+ 		}
+ 		epp = (hwp_req_data >> 24) & 0xff;
+ 	} else {
+ 		/* When there is no EPP present, HWP uses EPB settings */
+ 		epp = intel_pstate_get_epb(cpu_data);
+ 	}
+ 
+ 	return epp;
+ }
+ 
+ static int intel_pstate_set_epb(int cpu, s16 pref)
+ {
+ 	u64 epb;
+ 	int ret;
+ 
+ 	if (!static_cpu_has(X86_FEATURE_EPB))
+ 		return -ENXIO;
+ 
+ 	ret = rdmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, &epb);
+ 	if (ret)
+ 		return ret;
+ 
+ 	epb = (epb & ~0x0f) | pref;
+ 	wrmsrl_on_cpu(cpu, MSR_IA32_ENERGY_PERF_BIAS, epb);
+ 
+ 	return 0;
+ }
+ 
+ /*
+  * EPP/EPB display strings corresponding to EPP index in the
+  * energy_perf_strings[]
+  *	index		String
+  *-------------------------------------
+  *	0		default
+  *	1		performance
+  *	2		balance_performance
+  *	3		balance_power
+  *	4		power
+  */
+ static const char * const energy_perf_strings[] = {
+ 	"default",
+ 	"performance",
+ 	"balance_performance",
+ 	"balance_power",
+ 	"power",
+ 	NULL
+ };
+ 
+ static int intel_pstate_get_energy_pref_index(struct cpudata *cpu_data)
+ {
+ 	s16 epp;
+ 	int index = -EINVAL;
+ 
+ 	epp = intel_pstate_get_epp(cpu_data, 0);
+ 	if (epp < 0)
+ 		return epp;
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		/*
+ 		 * Range:
+ 		 *	0x00-0x3F	:	Performance
+ 		 *	0x40-0x7F	:	Balance performance
+ 		 *	0x80-0xBF	:	Balance power
+ 		 *	0xC0-0xFF	:	Power
+ 		 * The EPP is a 8 bit value, but our ranges restrict the
+ 		 * value which can be set. Here only using top two bits
+ 		 * effectively.
+ 		 */
+ 		index = (epp >> 6) + 1;
+ 	} else if (static_cpu_has(X86_FEATURE_EPB)) {
+ 		/*
+ 		 * Range:
+ 		 *	0x00-0x03	:	Performance
+ 		 *	0x04-0x07	:	Balance performance
+ 		 *	0x08-0x0B	:	Balance power
+ 		 *	0x0C-0x0F	:	Power
+ 		 * The EPB is a 4 bit value, but our ranges restrict the
+ 		 * value which can be set. Here only using top two bits
+ 		 * effectively.
+ 		 */
+ 		index = (epp >> 2) + 1;
+ 	}
+ 
+ 	return index;
+ }
+ 
+ static int intel_pstate_set_energy_pref_index(struct cpudata *cpu_data,
+ 					      int pref_index)
+ {
+ 	int epp = -EINVAL;
+ 	int ret;
+ 
+ 	if (!pref_index)
+ 		epp = cpu_data->epp_default;
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 
+ 	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
+ 		u64 value;
+ 
+ 		ret = rdmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, &value);
+ 		if (ret)
+ 			goto return_pref;
+ 
+ 		value &= ~GENMASK_ULL(31, 24);
+ 
+ 		/*
+ 		 * If epp is not default, convert from index into
+ 		 * energy_perf_strings to epp value, by shifting 6
+ 		 * bits left to use only top two bits in epp.
+ 		 * The resultant epp need to shifted by 24 bits to
+ 		 * epp position in MSR_HWP_REQUEST.
+ 		 */
+ 		if (epp == -EINVAL)
+ 			epp = (pref_index - 1) << 6;
+ 
+ 		value |= (u64)epp << 24;
+ 		ret = wrmsrl_on_cpu(cpu_data->cpu, MSR_HWP_REQUEST, value);
+ 	} else {
+ 		if (epp == -EINVAL)
+ 			epp = (pref_index - 1) << 2;
+ 		ret = intel_pstate_set_epb(cpu_data->cpu, epp);
+ 	}
+ return_pref:
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t show_energy_performance_available_preferences(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	int i = 0;
+ 	int ret = 0;
+ 
+ 	while (energy_perf_strings[i] != NULL)
+ 		ret += sprintf(&buf[ret], "%s ", energy_perf_strings[i++]);
+ 
+ 	ret += sprintf(&buf[ret], "\n");
+ 
+ 	return ret;
+ }
+ 
+ cpufreq_freq_attr_ro(energy_performance_available_preferences);
+ 
+ static ssize_t store_energy_performance_preference(
+ 		struct cpufreq_policy *policy, const char *buf, size_t count)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 	char str_preference[21];
+ 	int ret, i = 0;
+ 
+ 	ret = sscanf(buf, "%20s", str_preference);
+ 	if (ret != 1)
+ 		return -EINVAL;
+ 
+ 	while (energy_perf_strings[i] != NULL) {
+ 		if (!strcmp(str_preference, energy_perf_strings[i])) {
+ 			intel_pstate_set_energy_pref_index(cpu_data, i);
+ 			return count;
+ 		}
+ 		++i;
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ static ssize_t show_energy_performance_preference(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
+ 	int preference;
+ 
+ 	preference = intel_pstate_get_energy_pref_index(cpu_data);
+ 	if (preference < 0)
+ 		return preference;
+ 
+ 	return  sprintf(buf, "%s\n", energy_perf_strings[preference]);
+ }
+ 
+ cpufreq_freq_attr_rw(energy_performance_preference);
+ 
+ static struct freq_attr *hwp_cpufreq_attrs[] = {
+ 	&energy_performance_preference,
+ 	&energy_performance_available_preferences,
+ 	NULL,
+ };
+ 
+ static void intel_pstate_get_hwp_max(unsigned int cpu, int *phy_max,
+ 				     int *current_max)
+ {
+ 	u64 cap;
+ 
+ 	rdmsrl_on_cpu(cpu, MSR_HWP_CAPABILITIES, &cap);
+ 	if (global.no_turbo)
+ 		*current_max = HWP_GUARANTEED_PERF(cap);
+ 	else
+ 		*current_max = HWP_HIGHEST_PERF(cap);
+ 
+ 	*phy_max = HWP_HIGHEST_PERF(cap);
+ }
+ 
+ static void intel_pstate_hwp_set(unsigned int cpu)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[cpu];
+ 	int max, min;
+ 	u64 value;
+ 	s16 epp;
+ 
+ 	max = cpu_data->max_perf_ratio;
+ 	min = cpu_data->min_perf_ratio;
+ 
+ 	if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE)
+ 		min = max;
++>>>>>>> 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
  
 -	rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &value);
 +		adj_range = limits->max_perf_pct * range / 100;
 +		max = hw_min + adj_range;
  
 -	value &= ~HWP_MIN_PERF(~0L);
 -	value |= HWP_MIN_PERF(min);
 -
 -	value &= ~HWP_MAX_PERF(~0L);
 -	value |= HWP_MAX_PERF(max);
 -
 -	if (cpu_data->epp_policy == cpu_data->policy)
 -		goto skip_epp;
 -
 -	cpu_data->epp_policy = cpu_data->policy;
 -
 -	if (cpu_data->epp_saved >= 0) {
 -		epp = cpu_data->epp_saved;
 -		cpu_data->epp_saved = -EINVAL;
 -		goto update_epp;
 -	}
 -
 -	if (cpu_data->policy == CPUFREQ_POLICY_PERFORMANCE) {
 -		epp = intel_pstate_get_epp(cpu_data, value);
 -		cpu_data->epp_powersave = epp;
 -		/* If EPP read was failed, then don't try to write */
 -		if (epp < 0)
 -			goto skip_epp;
 -
 -		epp = 0;
 -	} else {
 -		/* skip setting EPP, when saved value is invalid */
 -		if (cpu_data->epp_powersave < 0)
 -			goto skip_epp;
 -
 -		/*
 -		 * No need to restore EPP when it is not zero. This
 -		 * means:
 -		 *  - Policy is not changed
 -		 *  - user has manually changed
 -		 *  - Error reading EPB
 -		 */
 -		epp = intel_pstate_get_epp(cpu_data, value);
 -		if (epp)
 -			goto skip_epp;
 -
 -		epp = cpu_data->epp_powersave;
 -	}
 -update_epp:
 -	if (static_cpu_has(X86_FEATURE_HWP_EPP)) {
 -		value &= ~GENMASK_ULL(31, 24);
 -		value |= (u64)epp << 24;
 -	} else {
 -		intel_pstate_set_epb(cpu, epp);
 +		value &= ~HWP_MAX_PERF(~0L);
 +		value |= HWP_MAX_PERF(max);
 +		wrmsrl_on_cpu(cpu, MSR_HWP_REQUEST, value);
  	}
 -skip_epp:
 -	wrmsrl_on_cpu(cpu, MSR_HWP_REQUEST, value);
 -}
 -
 -static int intel_pstate_hwp_save_state(struct cpufreq_policy *policy)
 -{
 -	struct cpudata *cpu_data = all_cpu_data[policy->cpu];
 -
 -	if (!hwp_active)
 -		return 0;
 -
 -	cpu_data->epp_saved = intel_pstate_get_epp(cpu_data, 0);
 -
 -	return 0;
  }
  
 -static int intel_pstate_resume(struct cpufreq_policy *policy)
 +static void intel_pstate_hwp_set_online_cpus(void)
  {
 -	if (!hwp_active)
 -		return 0;
 -
 -	mutex_lock(&intel_pstate_limits_lock);
 -
 -	all_cpu_data[policy->cpu]->epp_policy = 0;
 -	intel_pstate_hwp_set(policy->cpu);
 -
 -	mutex_unlock(&intel_pstate_limits_lock);
 -
 -	return 0;
 -}
 -
 -static void intel_pstate_update_policies(void)
 -{
 -	int cpu;
 -
 -	for_each_possible_cpu(cpu)
 -		cpufreq_update_policy(cpu);
 +	get_online_cpus();
 +	intel_pstate_hwp_set(cpu_online_mask);
 +	put_online_cpus();
  }
  
  /************************** debugfs begin ************************/
@@@ -1024,107 -1503,38 +1287,113 @@@ static int knl_get_turbo_pstate(void
  	return ret;
  }
  
 -static int intel_pstate_get_base_pstate(struct cpudata *cpu)
 -{
 -	return global.no_turbo || global.turbo_disabled ?
 -			cpu->pstate.max_pstate : cpu->pstate.turbo_pstate;
 -}
 +static struct cpu_defaults core_params = {
 +	.pid_policy = {
 +		.sample_rate_ms = 10,
 +		.deadband = 0,
 +		.setpoint = 97,
 +		.p_gain_pct = 20,
 +		.d_gain_pct = 0,
 +		.i_gain_pct = 0,
 +	},
 +	.funcs = {
 +		.get_max = core_get_max_pstate,
 +		.get_max_physical = core_get_max_pstate_physical,
 +		.get_min = core_get_min_pstate,
 +		.get_turbo = core_get_turbo_pstate,
 +		.get_scaling = core_get_scaling,
 +		.set = core_set_pstate,
 +		.get_target_pstate = get_target_pstate_use_performance,
 +	},
 +};
  
 -static void intel_pstate_set_pstate(struct cpudata *cpu, int pstate)
 +static struct cpu_defaults atom_params = {
 +	.pid_policy = {
 +		.sample_rate_ms = 10,
 +		.deadband = 0,
 +		.setpoint = 60,
 +		.p_gain_pct = 14,
 +		.d_gain_pct = 0,
 +		.i_gain_pct = 4,
 +	},
 +	.funcs = {
 +		.get_max = atom_get_max_pstate,
 +		.get_max_physical = atom_get_max_pstate,
 +		.get_min = atom_get_min_pstate,
 +		.get_turbo = atom_get_turbo_pstate,
 +		.set = atom_set_pstate,
 +		.get_scaling = atom_get_scaling,
 +		.get_vid = atom_get_vid,
 +		.get_target_pstate = get_target_pstate_use_cpu_load,
 +	},
 +};
 +
 +static struct cpu_defaults knl_params = {
 +	.pid_policy = {
 +		.sample_rate_ms = 10,
 +		.deadband = 0,
 +		.setpoint = 97,
 +		.p_gain_pct = 20,
 +		.d_gain_pct = 0,
 +		.i_gain_pct = 0,
 +	},
 +	.funcs = {
 +		.get_max = core_get_max_pstate,
 +		.get_max_physical = core_get_max_pstate_physical,
 +		.get_min = core_get_min_pstate,
 +		.get_turbo = knl_get_turbo_pstate,
 +		.get_scaling = core_get_scaling,
 +		.set = core_set_pstate,
 +		.get_target_pstate = get_target_pstate_use_performance,
 +	},
 +};
 +
 +static void intel_pstate_get_min_max(struct cpudata *cpu, int *min, int *max)
  {
 -	trace_cpu_frequency(pstate * cpu->pstate.scaling, cpu->cpu);
 -	cpu->pstate.current_pstate = pstate;
 +	int max_perf = cpu->pstate.turbo_pstate;
 +	int max_perf_adj;
 +	int min_perf;
 +
 +	if (limits->no_turbo || limits->turbo_disabled)
 +		max_perf = cpu->pstate.max_pstate;
 +
  	/*
 -	 * Generally, there is no guarantee that this code will always run on
 -	 * the CPU being updated, so force the register update to run on the
 -	 * right CPU.
 +	 * performance can be limited by user through sysfs, by cpufreq
 +	 * policy, or by cpu specific default values determined through
 +	 * experimentation.
  	 */
 -	wrmsrl_on_cpu(cpu->cpu, MSR_IA32_PERF_CTL,
 -		      pstate_funcs.get_val(cpu, pstate));
 -}
 +	max_perf_adj = fp_toint(max_perf * limits->max_perf);
 +	*max = clamp_t(int, max_perf_adj,
 +			cpu->pstate.min_pstate, cpu->pstate.turbo_pstate);
  
 -static void intel_pstate_set_min_pstate(struct cpudata *cpu)
 -{
 -	intel_pstate_set_pstate(cpu, cpu->pstate.min_pstate);
 +	min_perf = fp_toint(max_perf * limits->min_perf);
 +	*min = clamp_t(int, min_perf, cpu->pstate.min_pstate, max_perf);
  }
  
 -static void intel_pstate_max_within_limits(struct cpudata *cpu)
 +static void intel_pstate_set_pstate(struct cpudata *cpu, int pstate)
  {
 -	int pstate;
 +	int max_perf, min_perf;
  
  	update_turbo_state();
++<<<<<<< HEAD
 +
 +	intel_pstate_get_min_max(cpu, &min_perf, &max_perf);
 +
 +	pstate = clamp_t(int, pstate, min_perf, max_perf);
 +
 +	if (pstate == cpu->pstate.current_pstate)
 +		return;
 +
 +	trace_cpu_frequency(pstate * cpu->pstate.scaling, cpu->cpu);
 +
 +	cpu->pstate.current_pstate = pstate;
 +
 +	pstate_funcs.set(cpu, pstate);
++=======
+ 	pstate = intel_pstate_get_base_pstate(cpu);
+ 	pstate = max(cpu->pstate.min_pstate, cpu->max_perf_ratio);
+ 	intel_pstate_set_pstate(cpu, pstate);
++>>>>>>> 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
  }
  
  static void intel_pstate_get_cpu_pstates(struct cpudata *cpu)
@@@ -1286,26 -1687,45 +1555,49 @@@ static inline int32_t get_target_pstate
  	} else {
  		sample_ratio = div_fp(100 * cpu->sample.mperf, cpu->sample.tsc);
  		if (sample_ratio < int_tofp(1))
 -			perf_scaled = 0;
 +			core_busy = 0;
  	}
  
 -	cpu->sample.busy_scaled = perf_scaled;
 -	return cpu->pstate.current_pstate - pid_calc(&cpu->pid, perf_scaled);
 +	cpu->sample.busy_scaled = core_busy;
 +	return cpu->pstate.current_pstate - pid_calc(&cpu->pid, core_busy);
  }
  
 -static int intel_pstate_prepare_request(struct cpudata *cpu, int pstate)
 +static inline void intel_pstate_adjust_busy_pstate(struct cpudata *cpu)
  {
++<<<<<<< HEAD
 +	int from, target_pstate;
++=======
+ 	int max_pstate = intel_pstate_get_base_pstate(cpu);
+ 	int min_pstate;
+ 
+ 	min_pstate = max(cpu->pstate.min_pstate, cpu->min_perf_ratio);
+ 	max_pstate = max(min_pstate, cpu->max_perf_ratio);
+ 	return clamp_t(int, pstate, min_pstate, max_pstate);
+ }
+ 
+ static void intel_pstate_update_pstate(struct cpudata *cpu, int pstate)
+ {
+ 	if (pstate == cpu->pstate.current_pstate)
+ 		return;
+ 
+ 	cpu->pstate.current_pstate = pstate;
+ 	wrmsrl(MSR_IA32_PERF_CTL, pstate_funcs.get_val(cpu, pstate));
+ }
+ 
+ static void intel_pstate_adjust_pstate(struct cpudata *cpu, int target_pstate)
+ {
+ 	int from = cpu->pstate.current_pstate;
++>>>>>>> 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
  	struct sample *sample;
  
 -	update_turbo_state();
 +	from = cpu->pstate.current_pstate;
 +
 +	target_pstate = pstate_funcs.get_target_pstate(cpu);
  
 -	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
 -	trace_cpu_frequency(target_pstate * cpu->pstate.scaling, cpu->cpu);
 -	intel_pstate_update_pstate(cpu, target_pstate);
 +	intel_pstate_set_pstate(cpu, target_pstate);
  
  	sample = &cpu->sample;
 -	trace_pstate_sample(mul_ext_fp(100, sample->core_avg_perf),
 +	trace_pstate_sample(fp_toint(sample->core_pct_busy),
  		fp_toint(sample->busy_scaled),
  		from,
  		cpu->pstate.current_pstate,
@@@ -1419,14 -1928,103 +1711,112 @@@ static int intel_pstate_init_cpu(unsign
  
  static unsigned int intel_pstate_get(unsigned int cpu_num)
  {
 -	struct cpudata *cpu = all_cpu_data[cpu_num];
 +	struct sample *sample;
 +	struct cpudata *cpu;
  
++<<<<<<< HEAD
 +	cpu = all_cpu_data[cpu_num];
 +	if (!cpu)
 +		return 0;
 +	sample = &cpu->sample;
 +	return sample->freq;
++=======
+ 	return cpu ? get_avg_frequency(cpu) : 0;
+ }
+ 
+ static void intel_pstate_set_update_util_hook(unsigned int cpu_num)
+ {
+ 	struct cpudata *cpu = all_cpu_data[cpu_num];
+ 
+ 	if (cpu->update_util_set)
+ 		return;
+ 
+ 	/* Prevent intel_pstate_update_util() from using stale data. */
+ 	cpu->sample.time = 0;
+ 	cpufreq_add_update_util_hook(cpu_num, &cpu->update_util,
+ 				     pstate_funcs.update_util);
+ 	cpu->update_util_set = true;
+ }
+ 
+ static void intel_pstate_clear_update_util_hook(unsigned int cpu)
+ {
+ 	struct cpudata *cpu_data = all_cpu_data[cpu];
+ 
+ 	if (!cpu_data->update_util_set)
+ 		return;
+ 
+ 	cpufreq_remove_update_util_hook(cpu);
+ 	cpu_data->update_util_set = false;
+ 	synchronize_sched();
+ }
+ 
+ static int intel_pstate_get_max_freq(struct cpudata *cpu)
+ {
+ 	return global.turbo_disabled || global.no_turbo ?
+ 			cpu->pstate.max_freq : cpu->pstate.turbo_freq;
+ }
+ 
+ static void intel_pstate_update_perf_limits(struct cpufreq_policy *policy,
+ 					    struct cpudata *cpu)
+ {
+ 	int max_freq = intel_pstate_get_max_freq(cpu);
+ 	int32_t max_policy_perf, min_policy_perf;
+ 	int max_state, turbo_max;
+ 
+ 	/*
+ 	 * HWP needs some special consideration, because on BDX the
+ 	 * HWP_REQUEST uses abstract value to represent performance
+ 	 * rather than pure ratios.
+ 	 */
+ 	if (hwp_active) {
+ 		intel_pstate_get_hwp_max(cpu->cpu, &turbo_max, &max_state);
+ 	} else {
+ 		max_state = intel_pstate_get_base_pstate(cpu);
+ 		turbo_max = cpu->pstate.turbo_pstate;
+ 	}
+ 
+ 	max_policy_perf = max_state * policy->max / max_freq;
+ 	if (policy->max == policy->min) {
+ 		min_policy_perf = max_policy_perf;
+ 	} else {
+ 		min_policy_perf = max_state * policy->min / max_freq;
+ 		min_policy_perf = clamp_t(int32_t, min_policy_perf,
+ 					  0, max_policy_perf);
+ 	}
+ 
+ 	pr_debug("cpu:%d max_state %d min_policy_perf:%d max_policy_perf:%d\n",
+ 		 policy->cpu, max_state,
+ 		 min_policy_perf, max_policy_perf);
+ 
+ 	/* Normalize user input to [min_perf, max_perf] */
+ 	if (per_cpu_limits) {
+ 		cpu->min_perf_ratio = min_policy_perf;
+ 		cpu->max_perf_ratio = max_policy_perf;
+ 	} else {
+ 		int32_t global_min, global_max;
+ 
+ 		/* Global limits are in percent of the maximum turbo P-state. */
+ 		global_max = DIV_ROUND_UP(turbo_max * global.max_perf_pct, 100);
+ 		global_min = DIV_ROUND_UP(turbo_max * global.min_perf_pct, 100);
+ 		global_min = clamp_t(int32_t, global_min, 0, global_max);
+ 
+ 		pr_debug("cpu:%d global_min:%d global_max:%d\n", policy->cpu,
+ 			 global_min, global_max);
+ 
+ 		cpu->min_perf_ratio = max(min_policy_perf, global_min);
+ 		cpu->min_perf_ratio = min(cpu->min_perf_ratio, max_policy_perf);
+ 		cpu->max_perf_ratio = min(max_policy_perf, global_max);
+ 		cpu->max_perf_ratio = max(min_policy_perf, cpu->max_perf_ratio);
+ 
+ 		/* Make sure min_perf <= max_perf */
+ 		cpu->min_perf_ratio = min(cpu->min_perf_ratio,
+ 					  cpu->max_perf_ratio);
+ 
+ 	}
+ 	pr_debug("cpu:%d max_perf_ratio:%d min_perf_ratio:%d\n", policy->cpu,
+ 		 cpu->max_perf_ratio,
+ 		 cpu->min_perf_ratio);
++>>>>>>> 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
  }
  
  static int intel_pstate_set_policy(struct cpufreq_policy *policy)
@@@ -1529,10 -2127,8 +1919,15 @@@ static int intel_pstate_cpu_init(struc
  
  	cpu = all_cpu_data[policy->cpu];
  
++<<<<<<< HEAD
 +	if (limits->min_perf_pct == 100 && limits->max_perf_pct == 100)
 +		policy->policy = CPUFREQ_POLICY_PERFORMANCE;
 +	else
 +		policy->policy = CPUFREQ_POLICY_POWERSAVE;
++=======
+ 	cpu->max_perf_ratio = 0xFF;
+ 	cpu->min_perf_ratio = 0;
++>>>>>>> 1a4fe38add8b (cpufreq: intel_pstate: Remove max/min fractions to limit performance)
  
  	policy->min = cpu->pstate.min_pstate * cpu->pstate.scaling;
  	policy->max = cpu->pstate.turbo_pstate * cpu->pstate.scaling;
* Unmerged path drivers/cpufreq/intel_pstate.c
