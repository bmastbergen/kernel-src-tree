ibmvnic: Cleanup failure path in ibmvnic_open

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Nathan Fontenot <nfont@linux.vnet.ibm.com>
commit 1b8955ee5f6c1575c09b44c8253883394c78bef7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1b8955ee.failed

Now that ibmvnic_release_resources will clean up all of our resources
properly, even if they were not allocated, we can just call this
for failues in ibmvnic_open.

This patch also moves the ibmvnic_release_resources() routine up
in the file to avoid creating a forward declaration ad re-names it to
drop the ibmvnic prefix.

	Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1b8955ee5f6c1575c09b44c8253883394c78bef7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/ibm/ibmvnic.c
diff --cc drivers/net/ethernet/ibm/ibmvnic.c
index 6a325c61534d,7ba43cfadf3a..000000000000
--- a/drivers/net/ethernet/ibm/ibmvnic.c
+++ b/drivers/net/ethernet/ibm/ibmvnic.c
@@@ -347,45 -302,324 +347,85 @@@ static void replenish_pools(struct ibmv
  	}
  }
  
 -static void release_stats_token(struct ibmvnic_adapter *adapter)
 -{
 -	struct device *dev = &adapter->vdev->dev;
 -
 -	if (!adapter->stats_token)
 -		return;
 -
 -	dma_unmap_single(dev, adapter->stats_token,
 -			 sizeof(struct ibmvnic_statistics),
 -			 DMA_FROM_DEVICE);
 -	adapter->stats_token = 0;
 -}
 -
 -static int init_stats_token(struct ibmvnic_adapter *adapter)
 -{
 -	struct device *dev = &adapter->vdev->dev;
 -	dma_addr_t stok;
 -
 -	stok = dma_map_single(dev, &adapter->stats,
 -			      sizeof(struct ibmvnic_statistics),
 -			      DMA_FROM_DEVICE);
 -	if (dma_mapping_error(dev, stok)) {
 -		dev_err(dev, "Couldn't map stats buffer\n");
 -		return -1;
 -	}
 -
 -	adapter->stats_token = stok;
 -	return 0;
 -}
 -
 -static void release_rx_pools(struct ibmvnic_adapter *adapter)
 -{
 -	struct ibmvnic_rx_pool *rx_pool;
 -	int rx_scrqs;
 -	int i, j;
 -
 -	if (!adapter->rx_pool)
 -		return;
 -
 -	rx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
 -	for (i = 0; i < rx_scrqs; i++) {
 -		rx_pool = &adapter->rx_pool[i];
 -
 -		kfree(rx_pool->free_map);
 -		free_long_term_buff(adapter, &rx_pool->long_term_buff);
 -
 -		if (!rx_pool->rx_buff)
 -		continue;
 -
 -		for (j = 0; j < rx_pool->size; j++) {
 -			if (rx_pool->rx_buff[j].skb) {
 -				dev_kfree_skb_any(rx_pool->rx_buff[i].skb);
 -				rx_pool->rx_buff[i].skb = NULL;
 -			}
 -		}
 -
 -		kfree(rx_pool->rx_buff);
 -	}
 -
 -	kfree(adapter->rx_pool);
 -	adapter->rx_pool = NULL;
 -}
 -
 -static int init_rx_pools(struct net_device *netdev)
 +static void free_rx_pool(struct ibmvnic_adapter *adapter,
 +			 struct ibmvnic_rx_pool *pool)
  {
 -	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 -	struct device *dev = &adapter->vdev->dev;
 -	struct ibmvnic_rx_pool *rx_pool;
 -	int rxadd_subcrqs;
 -	u64 *size_array;
 -	int i, j;
 -
 -	rxadd_subcrqs =
 -		be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
 -	size_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +
 -		be32_to_cpu(adapter->login_rsp_buf->off_rxadd_buff_size));
 -
 -	adapter->rx_pool = kcalloc(rxadd_subcrqs,
 -				   sizeof(struct ibmvnic_rx_pool),
 -				   GFP_KERNEL);
 -	if (!adapter->rx_pool) {
 -		dev_err(dev, "Failed to allocate rx pools\n");
 -		return -1;
 -	}
 -
 -	for (i = 0; i < rxadd_subcrqs; i++) {
 -		rx_pool = &adapter->rx_pool[i];
 -
 -		netdev_dbg(adapter->netdev,
 -			   "Initializing rx_pool %d, %lld buffs, %lld bytes each\n",
 -			   i, adapter->req_rx_add_entries_per_subcrq,
 -			   be64_to_cpu(size_array[i]));
 -
 -		rx_pool->size = adapter->req_rx_add_entries_per_subcrq;
 -		rx_pool->index = i;
 -		rx_pool->buff_size = be64_to_cpu(size_array[i]);
 -		rx_pool->active = 1;
 -
 -		rx_pool->free_map = kcalloc(rx_pool->size, sizeof(int),
 -					    GFP_KERNEL);
 -		if (!rx_pool->free_map) {
 -			release_rx_pools(adapter);
 -			return -1;
 -		}
 -
 -		rx_pool->rx_buff = kcalloc(rx_pool->size,
 -					   sizeof(struct ibmvnic_rx_buff),
 -					   GFP_KERNEL);
 -		if (!rx_pool->rx_buff) {
 -			dev_err(dev, "Couldn't alloc rx buffers\n");
 -			release_rx_pools(adapter);
 -			return -1;
 -		}
 -
 -		if (alloc_long_term_buff(adapter, &rx_pool->long_term_buff,
 -					 rx_pool->size * rx_pool->buff_size)) {
 -			release_rx_pools(adapter);
 -			return -1;
 -		}
 -
 -		for (j = 0; j < rx_pool->size; ++j)
 -			rx_pool->free_map[j] = j;
 -
 -		atomic_set(&rx_pool->available, 0);
 -		rx_pool->next_alloc = 0;
 -		rx_pool->next_free = 0;
 -	}
 -
 -	return 0;
 -}
 +	int i;
  
 -static void release_tx_pools(struct ibmvnic_adapter *adapter)
 -{
 -	struct ibmvnic_tx_pool *tx_pool;
 -	int i, tx_scrqs;
 +	kfree(pool->free_map);
 +	pool->free_map = NULL;
  
 -	if (!adapter->tx_pool)
 +	if (!pool->rx_buff)
  		return;
  
 -	tx_scrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
 -	for (i = 0; i < tx_scrqs; i++) {
 -		tx_pool = &adapter->tx_pool[i];
 -		kfree(tx_pool->tx_buff);
 -		free_long_term_buff(adapter, &tx_pool->long_term_buff);
 -		kfree(tx_pool->free_map);
 -	}
 -
 -	kfree(adapter->tx_pool);
 -	adapter->tx_pool = NULL;
 -}
 -
 -static int init_tx_pools(struct net_device *netdev)
 -{
 -	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 -	struct device *dev = &adapter->vdev->dev;
 -	struct ibmvnic_tx_pool *tx_pool;
 -	int tx_subcrqs;
 -	int i, j;
 -
 -	tx_subcrqs = be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
 -	adapter->tx_pool = kcalloc(tx_subcrqs,
 -				   sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);
 -	if (!adapter->tx_pool)
 -		return -1;
 -
 -	for (i = 0; i < tx_subcrqs; i++) {
 -		tx_pool = &adapter->tx_pool[i];
 -		tx_pool->tx_buff = kcalloc(adapter->req_tx_entries_per_subcrq,
 -					   sizeof(struct ibmvnic_tx_buff),
 -					   GFP_KERNEL);
 -		if (!tx_pool->tx_buff) {
 -			dev_err(dev, "tx pool buffer allocation failed\n");
 -			release_tx_pools(adapter);
 -			return -1;
 -		}
 -
 -		if (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,
 -					 adapter->req_tx_entries_per_subcrq *
 -					 adapter->req_mtu)) {
 -			release_tx_pools(adapter);
 -			return -1;
 -		}
 -
 -		tx_pool->free_map = kcalloc(adapter->req_tx_entries_per_subcrq,
 -					    sizeof(int), GFP_KERNEL);
 -		if (!tx_pool->free_map) {
 -			release_tx_pools(adapter);
 -			return -1;
 +	for (i = 0; i < pool->size; i++) {
 +		if (pool->rx_buff[i].skb) {
 +			dev_kfree_skb_any(pool->rx_buff[i].skb);
 +			pool->rx_buff[i].skb = NULL;
  		}
 -
 -		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
 -			tx_pool->free_map[j] = j;
 -
 -		tx_pool->consumer_index = 0;
 -		tx_pool->producer_index = 0;
 -	}
 -
 -	return 0;
 -}
 -
 -static void release_bounce_buffer(struct ibmvnic_adapter *adapter)
 -{
 -	struct device *dev = &adapter->vdev->dev;
 -
 -	if (!adapter->bounce_buffer)
 -		return;
 -
 -	if (!dma_mapping_error(dev, adapter->bounce_buffer_dma)) {
 -		dma_unmap_single(dev, adapter->bounce_buffer_dma,
 -				 adapter->bounce_buffer_size,
 -				 DMA_BIDIRECTIONAL);
 -		adapter->bounce_buffer_dma = DMA_ERROR_CODE;
 -	}
 -
 -	kfree(adapter->bounce_buffer);
 -	adapter->bounce_buffer = NULL;
 -}
 -
 -static int init_bounce_buffer(struct net_device *netdev)
 -{
 -	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 -	struct device *dev = &adapter->vdev->dev;
 -	char *buf;
 -	int buf_sz;
 -	dma_addr_t map_addr;
 -
 -	buf_sz = (netdev->mtu + ETH_HLEN - 1) / PAGE_SIZE + 1;
 -	buf = kmalloc(adapter->bounce_buffer_size, GFP_KERNEL);
 -	if (!buf)
 -		return -1;
 -
 -	map_addr = dma_map_single(dev, buf, buf_sz, DMA_TO_DEVICE);
 -	if (dma_mapping_error(dev, map_addr)) {
 -		dev_err(dev, "Couldn't map bounce buffer\n");
 -		kfree(buf);
 -		return -1;
  	}
 -
 -	adapter->bounce_buffer = buf;
 -	adapter->bounce_buffer_size = buf_sz;
 -	adapter->bounce_buffer_dma = map_addr;
 -	return 0;
 -}
 -
 -static int ibmvnic_login(struct net_device *netdev)
 -{
 -	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
 -	unsigned long timeout = msecs_to_jiffies(30000);
 -	struct device *dev = &adapter->vdev->dev;
 -
 -	do {
 -		if (adapter->renegotiate) {
 -			adapter->renegotiate = false;
 -			release_sub_crqs(adapter);
 -
 -			reinit_completion(&adapter->init_done);
 -			send_cap_queries(adapter);
 -			if (!wait_for_completion_timeout(&adapter->init_done,
 -							 timeout)) {
 -				dev_err(dev, "Capabilities query timeout\n");
 -				return -1;
 -			}
 -		}
 -
 -		reinit_completion(&adapter->init_done);
 -		send_login(adapter);
 -		if (!wait_for_completion_timeout(&adapter->init_done,
 -						 timeout)) {
 -			dev_err(dev, "Login timeout\n");
 -			return -1;
 -		}
 -	} while (adapter->renegotiate);
 -
 -	return 0;
 +	kfree(pool->rx_buff);
 +	pool->rx_buff = NULL;
  }
  
+ static void release_resources(struct ibmvnic_adapter *adapter)
+ {
+ 	release_bounce_buffer(adapter);
+ 	release_tx_pools(adapter);
+ 	release_rx_pools(adapter);
+ 
+ 	release_sub_crqs(adapter);
+ 	release_crq_queue(adapter);
+ 
+ 	release_stats_token(adapter);
+ }
+ 
  static int ibmvnic_open(struct net_device *netdev)
  {
  	struct ibmvnic_adapter *adapter = netdev_priv(netdev);
  	struct device *dev = &adapter->vdev->dev;
 +	struct ibmvnic_tx_pool *tx_pool;
  	union ibmvnic_crq crq;
++<<<<<<< HEAD
 +	int rxadd_subcrqs;
 +	u64 *size_array;
 +	int tx_subcrqs;
 +	int i, j;
 +
 +	rxadd_subcrqs =
 +	    be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
 +	tx_subcrqs =
 +	    be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
 +	size_array = (u64 *)((u8 *)(adapter->login_rsp_buf) +
 +				  be32_to_cpu(adapter->login_rsp_buf->
 +					      off_rxadd_buff_size));
++=======
+ 	int rc = 0;
+ 	int i;
+ 
+ 	if (adapter->is_closed) {
+ 		rc = ibmvnic_init(adapter);
+ 		if (rc)
+ 			return rc;
+ 	}
+ 
+ 	rc = ibmvnic_login(netdev);
+ 	if (rc)
+ 		return rc;
+ 
+ 	rc = netif_set_real_num_tx_queues(netdev, adapter->req_tx_queues);
+ 	if (rc) {
+ 		dev_err(dev, "failed to set the number of tx queues\n");
+ 		return -1;
+ 	}
+ 
+ 	rc = init_sub_crq_irqs(adapter);
+ 	if (rc) {
+ 		dev_err(dev, "failed to initialize sub crq irqs\n");
+ 		return -1;
+ 	}
+ 
++>>>>>>> 1b8955ee5f6c (ibmvnic: Cleanup failure path in ibmvnic_open)
  	adapter->map_id = 1;
  	adapter->napi = kcalloc(adapter->req_rx_queues,
  				sizeof(struct napi_struct), GFP_KERNEL);
@@@ -396,65 -630,21 +436,79 @@@
  			       NAPI_POLL_WEIGHT);
  		napi_enable(&adapter->napi[i]);
  	}
 +	adapter->rx_pool =
 +	    kcalloc(rxadd_subcrqs, sizeof(struct ibmvnic_rx_pool), GFP_KERNEL);
  
 +	if (!adapter->rx_pool)
 +		goto rx_pool_arr_alloc_failed;
  	send_map_query(adapter);
 +	for (i = 0; i < rxadd_subcrqs; i++) {
 +		init_rx_pool(adapter, &adapter->rx_pool[i],
 +			     adapter->req_rx_add_entries_per_subcrq, i,
 +			     be64_to_cpu(size_array[i]), 1);
 +		if (alloc_rx_pool(adapter, &adapter->rx_pool[i])) {
 +			dev_err(dev, "Couldn't alloc rx pool\n");
 +			goto rx_pool_alloc_failed;
 +		}
 +	}
 +	adapter->tx_pool =
 +	    kcalloc(tx_subcrqs, sizeof(struct ibmvnic_tx_pool), GFP_KERNEL);
 +
++<<<<<<< HEAD
 +	if (!adapter->tx_pool)
 +		goto tx_pool_arr_alloc_failed;
 +	for (i = 0; i < tx_subcrqs; i++) {
 +		tx_pool = &adapter->tx_pool[i];
 +		tx_pool->tx_buff =
 +		    kcalloc(adapter->req_tx_entries_per_subcrq,
 +			    sizeof(struct ibmvnic_tx_buff), GFP_KERNEL);
 +		if (!tx_pool->tx_buff)
 +			goto tx_pool_alloc_failed;
  
 +		if (alloc_long_term_buff(adapter, &tx_pool->long_term_buff,
 +					 adapter->req_tx_entries_per_subcrq *
 +					 adapter->req_mtu))
 +			goto tx_ltb_alloc_failed;
 +
 +		tx_pool->free_map =
 +		    kcalloc(adapter->req_tx_entries_per_subcrq,
 +			    sizeof(int), GFP_KERNEL);
 +		if (!tx_pool->free_map)
 +			goto tx_fm_alloc_failed;
++=======
+ 	rc = init_rx_pools(netdev);
+ 	if (rc)
+ 		goto ibmvnic_open_fail;
+ 
+ 	rc = init_tx_pools(netdev);
+ 	if (rc)
+ 		goto ibmvnic_open_fail;
+ 
+ 	rc = init_bounce_buffer(netdev);
+ 	if (rc)
+ 		goto ibmvnic_open_fail;
++>>>>>>> 1b8955ee5f6c (ibmvnic: Cleanup failure path in ibmvnic_open)
 +
 +		for (j = 0; j < adapter->req_tx_entries_per_subcrq; j++)
 +			tx_pool->free_map[j] = j;
 +
 +		tx_pool->consumer_index = 0;
 +		tx_pool->producer_index = 0;
 +	}
 +	adapter->bounce_buffer_size =
 +	    (netdev->mtu + ETH_HLEN - 1) / PAGE_SIZE + 1;
 +	adapter->bounce_buffer = kmalloc(adapter->bounce_buffer_size,
 +					 GFP_KERNEL);
 +	if (!adapter->bounce_buffer)
 +		goto bounce_alloc_failed;
  
 +	adapter->bounce_buffer_dma = dma_map_single(dev, adapter->bounce_buffer,
 +						    adapter->bounce_buffer_size,
 +						    DMA_TO_DEVICE);
 +	if (dma_mapping_error(dev, adapter->bounce_buffer_dma)) {
 +		dev_err(dev, "Couldn't map tx bounce buffer\n");
 +		goto bounce_map_failed;
 +	}
  	replenish_pools(adapter);
  
  	/* We're ready to receive frames, enable the sub-crq interrupts and
@@@ -476,38 -666,11 +530,45 @@@
  
  	return 0;
  
++<<<<<<< HEAD
 +bounce_map_failed:
 +	kfree(adapter->bounce_buffer);
 +bounce_alloc_failed:
 +	i = tx_subcrqs - 1;
 +	kfree(adapter->tx_pool[i].free_map);
 +tx_fm_alloc_failed:
 +	free_long_term_buff(adapter, &adapter->tx_pool[i].long_term_buff);
 +tx_ltb_alloc_failed:
 +	kfree(adapter->tx_pool[i].tx_buff);
 +tx_pool_alloc_failed:
 +	for (j = 0; j < i; j++) {
 +		kfree(adapter->tx_pool[j].tx_buff);
 +		free_long_term_buff(adapter,
 +				    &adapter->tx_pool[j].long_term_buff);
 +		kfree(adapter->tx_pool[j].free_map);
 +	}
 +	kfree(adapter->tx_pool);
 +	adapter->tx_pool = NULL;
 +tx_pool_arr_alloc_failed:
 +	i = rxadd_subcrqs;
 +rx_pool_alloc_failed:
 +	for (j = 0; j < i; j++) {
 +		free_rx_pool(adapter, &adapter->rx_pool[j]);
 +		free_long_term_buff(adapter,
 +				    &adapter->rx_pool[j].long_term_buff);
 +	}
 +	kfree(adapter->rx_pool);
 +	adapter->rx_pool = NULL;
 +rx_pool_arr_alloc_failed:
 +	for (i = 0; i < adapter->req_rx_queues; i++)
 +		napi_disable(&adapter->napi[i]);
 +alloc_napi_failed:
++=======
+ ibmvnic_open_fail:
+ 	for (i = 0; i < adapter->req_rx_queues; i++)
+ 		napi_disable(&adapter->napi[i]);
+ 	release_resources(adapter);
++>>>>>>> 1b8955ee5f6c (ibmvnic: Cleanup failure path in ibmvnic_open)
  	return -ENOMEM;
  }
  
@@@ -544,27 -694,10 +605,31 @@@ static int ibmvnic_close(struct net_dev
  	crq.logical_link_state.link_state = IBMVNIC_LOGICAL_LNK_DN;
  	ibmvnic_send_crq(adapter, &crq);
  
++<<<<<<< HEAD
 +	for (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_txsubm_subcrqs);
 +	     i++) {
 +		kfree(adapter->tx_pool[i].tx_buff);
 +		free_long_term_buff(adapter,
 +				    &adapter->tx_pool[i].long_term_buff);
 +		kfree(adapter->tx_pool[i].free_map);
 +	}
 +	kfree(adapter->tx_pool);
 +	adapter->tx_pool = NULL;
 +
 +	for (i = 0; i < be32_to_cpu(adapter->login_rsp_buf->num_rxadd_subcrqs);
 +	     i++) {
 +		free_rx_pool(adapter, &adapter->rx_pool[i]);
 +		free_long_term_buff(adapter,
 +				    &adapter->rx_pool[i].long_term_buff);
 +	}
 +	kfree(adapter->rx_pool);
 +	adapter->rx_pool = NULL;
++=======
+ 	release_resources(adapter);
++>>>>>>> 1b8955ee5f6c (ibmvnic: Cleanup failure path in ibmvnic_open)
  
 -	adapter->is_closed = true;
  	adapter->closing = false;
 +
  	return 0;
  }
  
* Unmerged path drivers/net/ethernet/ibm/ibmvnic.c
