s390/crypto: cpacf function detection

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [s390] crypto: cpacf function detection (Hendrik Brueckner) [1380349]
Rebuild_FUZZ: 92.75%
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit 69c0e360f990c2dc737681f40a361195066cef02
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/69c0e360.failed

The CPACF code makes some assumptions about the availablity of hardware
support. E.g. if the machine supports KM(AES-256) without chaining it is
assumed that KMC(AES-256) with chaining is available as well. For the
existing CPUs this is true but the architecturally correct way is to
check each CPACF functions on its own. This is what the query function
of each instructions is all about.

	Reviewed-by: Harald Freudenberger <freude@linux.vnet.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 69c0e360f990c2dc737681f40a361195066cef02)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/crypto/aes_s390.c
#	arch/s390/crypto/des_s390.c
#	arch/s390/crypto/ghash_s390.c
#	arch/s390/crypto/prng.c
#	arch/s390/crypto/sha1_s390.c
#	arch/s390/crypto/sha256_s390.c
#	arch/s390/crypto/sha512_s390.c
#	arch/s390/include/asm/cpacf.h
#	arch/s390/kvm/kvm-s390.c
diff --cc arch/s390/crypto/aes_s390.c
index 25567fe0d72c,f4ad96ebb7e9..000000000000
--- a/arch/s390/crypto/aes_s390.c
+++ b/arch/s390/crypto/aes_s390.c
@@@ -27,15 -28,13 +27,12 @@@
  #include <linux/init.h>
  #include <linux/spinlock.h>
  #include <crypto/xts.h>
 -#include <asm/cpacf.h>
 +#include "crypt_s390.h"
  
- #define AES_KEYLEN_128		1
- #define AES_KEYLEN_192		2
- #define AES_KEYLEN_256		4
- 
  static u8 *ctrblk;
  static DEFINE_SPINLOCK(ctrblk_lock);
- static char keylen_flag;
+ 
+ static cpacf_mask_t km_functions, kmc_functions, kmctr_functions;
  
  struct s390_aes_ctx {
  	u8 key[AES_MAX_KEY_SIZE];
@@@ -59,39 -57,11 +56,12 @@@ struct pcc_param 
  struct s390_xts_ctx {
  	u8 key[32];
  	u8 pcc_key[32];
 +	long enc;
 +	long dec;
  	int key_len;
 -	unsigned long fc;
 -	struct crypto_skcipher *fallback;
 +	struct crypto_blkcipher *fallback;
  };
  
- /*
-  * Check if the key_len is supported by the HW.
-  * Returns 0 if it is, a positive number if it is not and software fallback is
-  * required or a negative number in case the key size is not valid
-  */
- static int need_fallback(unsigned int key_len)
- {
- 	switch (key_len) {
- 	case 16:
- 		if (!(keylen_flag & AES_KEYLEN_128))
- 			return 1;
- 		break;
- 	case 24:
- 		if (!(keylen_flag & AES_KEYLEN_192))
- 			return 1;
- 		break;
- 	case 32:
- 		if (!(keylen_flag & AES_KEYLEN_256))
- 			return 1;
- 		break;
- 	default:
- 		return -1;
- 		break;
- 	}
- 	return 0;
- }
- 
  static int setkey_fallback_cip(struct crypto_tfm *tfm, const u8 *in_key,
  		unsigned int key_len)
  {
@@@ -135,52 -104,25 +104,61 @@@ static int aes_set_key(struct crypto_tf
  
  static void aes_encrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)
  {
 -	struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);
 +	const struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);
  
- 	if (unlikely(need_fallback(sctx->key_len))) {
+ 	if (unlikely(!sctx->fc)) {
  		crypto_cipher_encrypt_one(sctx->fallback.cip, out, in);
  		return;
  	}
++<<<<<<< HEAD
 +
 +	switch (sctx->key_len) {
 +	case 16:
 +		crypt_s390_km(KM_AES_128_ENCRYPT, &sctx->key, out, in,
 +			      AES_BLOCK_SIZE);
 +		break;
 +	case 24:
 +		crypt_s390_km(KM_AES_192_ENCRYPT, &sctx->key, out, in,
 +			      AES_BLOCK_SIZE);
 +		break;
 +	case 32:
 +		crypt_s390_km(KM_AES_256_ENCRYPT, &sctx->key, out, in,
 +			      AES_BLOCK_SIZE);
 +		break;
 +	}
++=======
+ 	cpacf_km(sctx->fc, &sctx->key, out, in, AES_BLOCK_SIZE);
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  }
  
  static void aes_decrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)
  {
 -	struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);
 +	const struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);
  
- 	if (unlikely(need_fallback(sctx->key_len))) {
+ 	if (unlikely(!sctx->fc)) {
  		crypto_cipher_decrypt_one(sctx->fallback.cip, out, in);
  		return;
  	}
++<<<<<<< HEAD
 +
 +	switch (sctx->key_len) {
 +	case 16:
 +		crypt_s390_km(KM_AES_128_DECRYPT, &sctx->key, out, in,
 +			      AES_BLOCK_SIZE);
 +		break;
 +	case 24:
 +		crypt_s390_km(KM_AES_192_DECRYPT, &sctx->key, out, in,
 +			      AES_BLOCK_SIZE);
 +		break;
 +	case 32:
 +		crypt_s390_km(KM_AES_256_DECRYPT, &sctx->key, out, in,
 +			      AES_BLOCK_SIZE);
 +		break;
 +	}
++=======
+ 	cpacf_km(sctx->fc | CPACF_DECRYPT,
+ 		 &sctx->key, out, in, AES_BLOCK_SIZE);
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  }
  
  static int fallback_init_cip(struct crypto_tfm *tfm)
@@@ -287,30 -231,21 +265,40 @@@ static int ecb_aes_set_key(struct crypt
  			   unsigned int key_len)
  {
  	struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);
- 	int ret;
+ 	unsigned long fc;
  
- 	ret = need_fallback(key_len);
- 	if (ret > 0) {
- 		sctx->key_len = key_len;
+ 	/* Pick the correct function code based on the key length */
+ 	fc = (key_len == 16) ? CPACF_KM_AES_128 :
+ 	     (key_len == 24) ? CPACF_KM_AES_192 :
+ 	     (key_len == 32) ? CPACF_KM_AES_256 : 0;
+ 
+ 	/* Check if the function code is available */
+ 	sctx->fc = (fc && cpacf_test_func(&km_functions, fc)) ? fc : 0;
+ 	if (!sctx->fc)
  		return setkey_fallback_blk(tfm, in_key, key_len);
- 	}
  
++<<<<<<< HEAD
 +	switch (key_len) {
 +	case 16:
 +		sctx->enc = KM_AES_128_ENCRYPT;
 +		sctx->dec = KM_AES_128_DECRYPT;
 +		break;
 +	case 24:
 +		sctx->enc = KM_AES_192_ENCRYPT;
 +		sctx->dec = KM_AES_192_DECRYPT;
 +		break;
 +	case 32:
 +		sctx->enc = KM_AES_256_ENCRYPT;
 +		sctx->dec = KM_AES_256_DECRYPT;
 +		break;
 +	}
 +
 +	return aes_set_key(tfm, in_key, key_len);
++=======
+ 	sctx->key_len = key_len;
+ 	memcpy(sctx->key, in_key, key_len);
+ 	return 0;
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  }
  
  static int ecb_aes_crypt(struct blkcipher_desc *desc, long func, void *param,
@@@ -416,30 -349,21 +404,40 @@@ static int cbc_aes_set_key(struct crypt
  			   unsigned int key_len)
  {
  	struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);
- 	int ret;
+ 	unsigned long fc;
+ 
+ 	/* Pick the correct function code based on the key length */
+ 	fc = (key_len == 16) ? CPACF_KMC_AES_128 :
+ 	     (key_len == 24) ? CPACF_KMC_AES_192 :
+ 	     (key_len == 32) ? CPACF_KMC_AES_256 : 0;
  
- 	ret = need_fallback(key_len);
- 	if (ret > 0) {
- 		sctx->key_len = key_len;
+ 	/* Check if the function code is available */
+ 	sctx->fc = (fc && cpacf_test_func(&kmc_functions, fc)) ? fc : 0;
+ 	if (!sctx->fc)
  		return setkey_fallback_blk(tfm, in_key, key_len);
- 	}
  
++<<<<<<< HEAD
 +	switch (key_len) {
 +	case 16:
 +		sctx->enc = KMC_AES_128_ENCRYPT;
 +		sctx->dec = KMC_AES_128_DECRYPT;
 +		break;
 +	case 24:
 +		sctx->enc = KMC_AES_192_ENCRYPT;
 +		sctx->dec = KMC_AES_192_DECRYPT;
 +		break;
 +	case 32:
 +		sctx->enc = KMC_AES_256_ENCRYPT;
 +		sctx->dec = KMC_AES_256_DECRYPT;
 +		break;
 +	}
 +
 +	return aes_set_key(tfm, in_key, key_len);
++=======
+ 	sctx->key_len = key_len;
+ 	memcpy(sctx->key, in_key, key_len);
+ 	return 0;
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  }
  
  static int cbc_aes_crypt(struct blkcipher_desc *desc, long func,
@@@ -593,29 -519,20 +591,45 @@@ static int xts_aes_set_key(struct crypt
  	if (err)
  		return err;
  
++<<<<<<< HEAD
 +	switch (key_len) {
 +	case 32:
 +		xts_ctx->enc = KM_XTS_128_ENCRYPT;
 +		xts_ctx->dec = KM_XTS_128_DECRYPT;
 +		memcpy(xts_ctx->key + 16, in_key, 16);
 +		memcpy(xts_ctx->pcc_key + 16, in_key + 16, 16);
 +		break;
 +	case 48:
 +		xts_ctx->enc = 0;
 +		xts_ctx->dec = 0;
 +		xts_fallback_setkey(tfm, in_key, key_len);
 +		break;
 +	case 64:
 +		xts_ctx->enc = KM_XTS_256_ENCRYPT;
 +		xts_ctx->dec = KM_XTS_256_DECRYPT;
 +		memcpy(xts_ctx->key, in_key, 32);
 +		memcpy(xts_ctx->pcc_key, in_key + 32, 32);
 +		break;
 +	default:
 +		*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
 +		return -EINVAL;
 +	}
++=======
+ 	/* Pick the correct function code based on the key length */
+ 	fc = (key_len == 32) ? CPACF_KM_XTS_128 :
+ 	     (key_len == 64) ? CPACF_KM_XTS_256 : 0;
+ 
+ 	/* Check if the function code is available */
+ 	xts_ctx->fc = (fc && cpacf_test_func(&km_functions, fc)) ? fc : 0;
+ 	if (!xts_ctx->fc)
+ 		return xts_fallback_setkey(tfm, in_key, key_len);
+ 
+ 	/* Split the XTS key into the two subkeys */
+ 	key_len = key_len / 2;
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  	xts_ctx->key_len = key_len;
+ 	memcpy(xts_ctx->key, in_key, key_len);
+ 	memcpy(xts_ctx->pcc_key, in_key + key_len, key_len);
  	return 0;
  }
  
@@@ -641,12 -558,11 +655,18 @@@ static int xts_aes_crypt(struct blkciph
  	memset(pcc_param.bit, 0, sizeof(pcc_param.bit));
  	memset(pcc_param.xts, 0, sizeof(pcc_param.xts));
  	memcpy(pcc_param.tweak, walk->iv, sizeof(pcc_param.tweak));
++<<<<<<< HEAD
 +	memcpy(pcc_param.key, xts_ctx->pcc_key, 32);
 +	ret = crypt_s390_pcc(func, &pcc_param.key[offset]);
 +	if (ret < 0)
 +		return -EIO;
++=======
+ 	memcpy(pcc_param.key + offset, xts_ctx->pcc_key, xts_ctx->key_len);
+ 	/* remove decipher modifier bit from 'func' and call PCC */
+ 	cpacf_pcc(func & 0x7f, &pcc_param.key[offset]);
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  
- 	memcpy(xts_param.key, xts_ctx->key, 32);
+ 	memcpy(xts_param.key + offset, xts_ctx->key, xts_ctx->key_len);
  	memcpy(xts_param.init, pcc_param.xts, 16);
  	do {
  		/* only use complete blocks */
@@@ -747,23 -659,21 +767,38 @@@ static int ctr_aes_set_key(struct crypt
  			   unsigned int key_len)
  {
  	struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);
+ 	unsigned long fc;
  
++<<<<<<< HEAD
 +	switch (key_len) {
 +	case 16:
 +		sctx->enc = KMCTR_AES_128_ENCRYPT;
 +		sctx->dec = KMCTR_AES_128_DECRYPT;
 +		break;
 +	case 24:
 +		sctx->enc = KMCTR_AES_192_ENCRYPT;
 +		sctx->dec = KMCTR_AES_192_DECRYPT;
 +		break;
 +	case 32:
 +		sctx->enc = KMCTR_AES_256_ENCRYPT;
 +		sctx->dec = KMCTR_AES_256_DECRYPT;
 +		break;
 +	}
++=======
+ 	/* Pick the correct function code based on the key length */
+ 	fc = (key_len == 16) ? CPACF_KMCTR_AES_128 :
+ 	     (key_len == 24) ? CPACF_KMCTR_AES_192 :
+ 	     (key_len == 32) ? CPACF_KMCTR_AES_256 : 0;
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
+ 
+ 	/* Check if the function code is available */
+ 	sctx->fc = (fc && cpacf_test_func(&kmctr_functions, fc)) ? fc : 0;
+ 	if (!sctx->fc)
+ 		return setkey_fallback_blk(tfm, in_key, key_len);
  
- 	return aes_set_key(tfm, in_key, key_len);
+ 	sctx->key_len = key_len;
+ 	memcpy(sctx->key, in_key, key_len);
+ 	return 0;
  }
  
  static unsigned int __ctrblk_init(u8 *ctrptr, unsigned int nbytes)
@@@ -856,8 -757,11 +891,11 @@@ static int ctr_aes_encrypt(struct blkci
  	struct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);
  	struct blkcipher_walk walk;
  
+ 	if (unlikely(!sctx->fc))
+ 		return fallback_blk_enc(desc, dst, src, nbytes);
+ 
  	blkcipher_walk_init(&walk, dst, src, nbytes);
 -	return ctr_aes_crypt(desc, sctx->fc, sctx, &walk);
 +	return ctr_aes_crypt(desc, sctx->enc, sctx, &walk);
  }
  
  static int ctr_aes_decrypt(struct blkcipher_desc *desc,
@@@ -867,15 -771,19 +905,24 @@@
  	struct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);
  	struct blkcipher_walk walk;
  
+ 	if (unlikely(!sctx->fc))
+ 		return fallback_blk_dec(desc, dst, src, nbytes);
+ 
  	blkcipher_walk_init(&walk, dst, src, nbytes);
 -	return ctr_aes_crypt(desc, sctx->fc | CPACF_DECRYPT, sctx, &walk);
 +	return ctr_aes_crypt(desc, sctx->dec, sctx, &walk);
  }
  
  static struct crypto_alg ctr_aes_alg = {
  	.cra_name		=	"ctr(aes)",
  	.cra_driver_name	=	"ctr-aes-s390",
++<<<<<<< HEAD
 +	.cra_priority		=	CRYPT_S390_COMPOSITE_PRIORITY,
 +	.cra_flags		=	CRYPTO_ALG_TYPE_BLKCIPHER,
++=======
+ 	.cra_priority		=	400,	/* combo: aes + ctr */
+ 	.cra_flags		=	CRYPTO_ALG_TYPE_BLKCIPHER |
+ 					CRYPTO_ALG_NEED_FALLBACK,
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  	.cra_blocksize		=	1,
  	.cra_ctxsize		=	sizeof(struct s390_aes_ctx),
  	.cra_type		=	&crypto_blkcipher_type,
@@@ -898,49 -827,40 +947,79 @@@ static int __init aes_s390_init(void
  {
  	int ret;
  
++<<<<<<< HEAD
 +	if (crypt_s390_func_available(KM_AES_128_ENCRYPT, CRYPT_S390_MSA))
 +		keylen_flag |= AES_KEYLEN_128;
 +	if (crypt_s390_func_available(KM_AES_192_ENCRYPT, CRYPT_S390_MSA))
 +		keylen_flag |= AES_KEYLEN_192;
 +	if (crypt_s390_func_available(KM_AES_256_ENCRYPT, CRYPT_S390_MSA))
 +		keylen_flag |= AES_KEYLEN_256;
++=======
+ 	/* Query available functions for KM, KMC and KMCTR */
+ 	cpacf_query(CPACF_KM, &km_functions);
+ 	cpacf_query(CPACF_KMC, &kmc_functions);
+ 	cpacf_query(CPACF_KMCTR, &kmctr_functions);
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
+ 
+ 	if (cpacf_test_func(&km_functions, CPACF_KM_AES_128) ||
+ 	    cpacf_test_func(&km_functions, CPACF_KM_AES_192) ||
+ 	    cpacf_test_func(&km_functions, CPACF_KM_AES_256)) {
+ 		ret = aes_s390_register_alg(&aes_alg);
+ 		if (ret)
+ 			goto out_err;
+ 		ret = aes_s390_register_alg(&ecb_aes_alg);
+ 		if (ret)
+ 			goto out_err;
+ 	}
  
- 	if (!keylen_flag)
- 		return -EOPNOTSUPP;
- 
- 	/* z9 109 and z9 BC/EC only support 128 bit key length */
- 	if (keylen_flag == AES_KEYLEN_128)
- 		pr_info("AES hardware acceleration is only available for"
- 			" 128-bit keys\n");
+ 	if (cpacf_test_func(&kmc_functions, CPACF_KMC_AES_128) ||
+ 	    cpacf_test_func(&kmc_functions, CPACF_KMC_AES_192) ||
+ 	    cpacf_test_func(&kmc_functions, CPACF_KMC_AES_256)) {
+ 		ret = aes_s390_register_alg(&cbc_aes_alg);
+ 		if (ret)
+ 			goto out_err;
+ 	}
  
++<<<<<<< HEAD
 +	ret = crypto_register_alg(&aes_alg);
 +	if (ret)
 +		goto aes_err;
 +
 +	ret = crypto_register_alg(&ecb_aes_alg);
 +	if (ret)
 +		goto ecb_aes_err;
 +
 +	ret = crypto_register_alg(&cbc_aes_alg);
 +	if (ret)
 +		goto cbc_aes_err;
 +
 +	if (crypt_s390_func_available(KM_XTS_128_ENCRYPT,
 +			CRYPT_S390_MSA | CRYPT_S390_MSA4) &&
 +	    crypt_s390_func_available(KM_XTS_256_ENCRYPT,
 +			CRYPT_S390_MSA | CRYPT_S390_MSA4)) {
 +		ret = crypto_register_alg(&xts_aes_alg);
++=======
+ 	if (cpacf_test_func(&km_functions, CPACF_KM_XTS_128) ||
+ 	    cpacf_test_func(&km_functions, CPACF_KM_XTS_256)) {
+ 		ret = aes_s390_register_alg(&xts_aes_alg);
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		if (ret)
 -			goto out_err;
 +			goto xts_aes_err;
 +		xts_aes_alg_reg = 1;
  	}
  
++<<<<<<< HEAD
 +	if (crypt_s390_func_available(KMCTR_AES_128_ENCRYPT,
 +				CRYPT_S390_MSA | CRYPT_S390_MSA4) &&
 +	    crypt_s390_func_available(KMCTR_AES_192_ENCRYPT,
 +				CRYPT_S390_MSA | CRYPT_S390_MSA4) &&
 +	    crypt_s390_func_available(KMCTR_AES_256_ENCRYPT,
 +				CRYPT_S390_MSA | CRYPT_S390_MSA4)) {
++=======
+ 	if (cpacf_test_func(&kmctr_functions, CPACF_KMCTR_AES_128) ||
+ 	    cpacf_test_func(&kmctr_functions, CPACF_KMCTR_AES_192) ||
+ 	    cpacf_test_func(&kmctr_functions, CPACF_KMCTR_AES_256)) {
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		ctrblk = (u8 *) __get_free_page(GFP_KERNEL);
  		if (!ctrblk) {
  			ret = -ENOMEM;
diff --cc arch/s390/crypto/des_s390.c
index a89feffb22b5,965587eefc39..000000000000
--- a/arch/s390/crypto/des_s390.c
+++ b/arch/s390/crypto/des_s390.c
@@@ -240,13 -240,12 +242,20 @@@ static int des3_setkey(struct crypto_tf
  		       unsigned int key_len)
  {
  	struct s390_des_ctx *ctx = crypto_tfm_ctx(tfm);
- 	u32 *flags = &tfm->crt_flags;
  
++<<<<<<< HEAD
 +	if (!(memcmp(key, &key[DES_KEY_SIZE], DES_KEY_SIZE) &&
 +	    memcmp(&key[DES_KEY_SIZE], &key[DES_KEY_SIZE * 2],
 +		   DES_KEY_SIZE)) &&
 +	    (*flags & CRYPTO_TFM_REQ_WEAK_KEY)) {
 +		*flags |= CRYPTO_TFM_RES_WEAK_KEY;
++=======
+ 	if (!(crypto_memneq(key, &key[DES_KEY_SIZE], DES_KEY_SIZE) &&
+ 	    crypto_memneq(&key[DES_KEY_SIZE], &key[DES_KEY_SIZE * 2],
+ 			  DES_KEY_SIZE)) &&
+ 	    (tfm->crt_flags & CRYPTO_TFM_REQ_WEAK_KEY)) {
+ 		tfm->crt_flags |= CRYPTO_TFM_RES_WEAK_KEY;
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		return -EINVAL;
  	}
  	memcpy(ctx->key, key, key_len);
@@@ -539,84 -555,65 +548,135 @@@ static int __init des_s390_init(void
  {
  	int ret;
  
++<<<<<<< HEAD
 +	if (!crypt_s390_func_available(KM_DEA_ENCRYPT, CRYPT_S390_MSA) ||
 +	    !crypt_s390_func_available(KM_TDEA_192_ENCRYPT, CRYPT_S390_MSA))
 +		return -EOPNOTSUPP;
 +
 +	ret = crypto_register_alg(&des_alg);
 +	if (ret)
 +		goto des_err;
 +	ret = crypto_register_alg(&ecb_des_alg);
 +	if (ret)
 +		goto ecb_des_err;
 +	ret = crypto_register_alg(&cbc_des_alg);
 +	if (ret)
 +		goto cbc_des_err;
 +	ret = crypto_register_alg(&des3_alg);
 +	if (ret)
 +		goto des3_err;
 +	ret = crypto_register_alg(&ecb_des3_alg);
 +	if (ret)
 +		goto ecb_des3_err;
 +	ret = crypto_register_alg(&cbc_des3_alg);
 +	if (ret)
 +		goto cbc_des3_err;
 +
 +	if (crypt_s390_func_available(KMCTR_DEA_ENCRYPT,
 +			CRYPT_S390_MSA | CRYPT_S390_MSA4) &&
 +	    crypt_s390_func_available(KMCTR_TDEA_192_ENCRYPT,
 +			CRYPT_S390_MSA | CRYPT_S390_MSA4)) {
 +		ret = crypto_register_alg(&ctr_des_alg);
 +		if (ret)
 +			goto ctr_des_err;
 +		ret = crypto_register_alg(&ctr_des3_alg);
 +		if (ret)
 +			goto ctr_des3_err;
++=======
+ 	/* Query available functions for KM, KMC and KMCTR */
+ 	cpacf_query(CPACF_KM, &km_functions);
+ 	cpacf_query(CPACF_KMC, &kmc_functions);
+ 	cpacf_query(CPACF_KMCTR, &kmctr_functions);
+ 
+ 	if (cpacf_test_func(&km_functions, CPACF_KM_DEA)) {
+ 		ret = des_s390_register_alg(&des_alg);
+ 		if (ret)
+ 			goto out_err;
+ 		ret = des_s390_register_alg(&ecb_des_alg);
+ 		if (ret)
+ 			goto out_err;
+ 	}
+ 	if (cpacf_test_func(&kmc_functions, CPACF_KMC_DEA)) {
+ 		ret = des_s390_register_alg(&cbc_des_alg);
+ 		if (ret)
+ 			goto out_err;
+ 	}
+ 	if (cpacf_test_func(&km_functions, CPACF_KM_TDEA_192)) {
+ 		ret = des_s390_register_alg(&des3_alg);
+ 		if (ret)
+ 			goto out_err;
+ 		ret = des_s390_register_alg(&ecb_des3_alg);
+ 		if (ret)
+ 			goto out_err;
+ 	}
+ 	if (cpacf_test_func(&kmc_functions, CPACF_KMC_TDEA_192)) {
+ 		ret = des_s390_register_alg(&cbc_des3_alg);
+ 		if (ret)
+ 			goto out_err;
+ 	}
+ 
+ 	if (cpacf_test_func(&kmctr_functions, CPACF_KMCTR_DEA) ||
+ 	    cpacf_test_func(&kmctr_functions, CPACF_KMCTR_TDEA_192)) {
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		ctrblk = (u8 *) __get_free_page(GFP_KERNEL);
  		if (!ctrblk) {
  			ret = -ENOMEM;
 -			goto out_err;
 +			goto ctr_mem_err;
  		}
++<<<<<<< HEAD
++=======
+ 	}
+ 
+ 	if (cpacf_test_func(&kmctr_functions, CPACF_KMCTR_DEA)) {
+ 		ret = des_s390_register_alg(&ctr_des_alg);
+ 		if (ret)
+ 			goto out_err;
+ 	}
+ 	if (cpacf_test_func(&kmctr_functions, CPACF_KMCTR_TDEA_192)) {
+ 		ret = des_s390_register_alg(&ctr_des3_alg);
+ 		if (ret)
+ 			goto out_err;
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  	}
 -
 -	return 0;
 -out_err:
 -	des_s390_exit();
 +out:
  	return ret;
 +
 +ctr_mem_err:
 +	crypto_unregister_alg(&ctr_des3_alg);
 +ctr_des3_err:
 +	crypto_unregister_alg(&ctr_des_alg);
 +ctr_des_err:
 +	crypto_unregister_alg(&cbc_des3_alg);
 +cbc_des3_err:
 +	crypto_unregister_alg(&ecb_des3_alg);
 +ecb_des3_err:
 +	crypto_unregister_alg(&des3_alg);
 +des3_err:
 +	crypto_unregister_alg(&cbc_des_alg);
 +cbc_des_err:
 +	crypto_unregister_alg(&ecb_des_alg);
 +ecb_des_err:
 +	crypto_unregister_alg(&des_alg);
 +des_err:
 +	goto out;
 +}
 +
 +static void __exit des_s390_exit(void)
 +{
 +	if (ctrblk) {
 +		crypto_unregister_alg(&ctr_des_alg);
 +		crypto_unregister_alg(&ctr_des3_alg);
 +		free_page((unsigned long) ctrblk);
 +	}
 +	crypto_unregister_alg(&cbc_des3_alg);
 +	crypto_unregister_alg(&ecb_des3_alg);
 +	crypto_unregister_alg(&des3_alg);
 +	crypto_unregister_alg(&cbc_des_alg);
 +	crypto_unregister_alg(&ecb_des_alg);
 +	crypto_unregister_alg(&des_alg);
  }
  
 -module_cpu_feature_match(MSA, des_s390_init);
 +module_init(des_s390_init);
  module_exit(des_s390_exit);
  
  MODULE_ALIAS_CRYPTO("des");
diff --cc arch/s390/crypto/ghash_s390.c
index b258110da952,564616d48d8b..000000000000
--- a/arch/s390/crypto/ghash_s390.c
+++ b/arch/s390/crypto/ghash_s390.c
@@@ -146,8 -136,7 +146,12 @@@ static struct shash_alg ghash_alg = 
  
  static int __init ghash_mod_init(void)
  {
++<<<<<<< HEAD
 +	if (!crypt_s390_func_available(KIMD_GHASH,
 +				       CRYPT_S390_MSA | CRYPT_S390_MSA4))
++=======
+ 	if (!cpacf_query_func(CPACF_KIMD, CPACF_KIMD_GHASH))
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		return -EOPNOTSUPP;
  
  	return crypto_register_shash(&ghash_alg);
diff --cc arch/s390/crypto/prng.c
index 9d5192c94963,79e3a1f6313a..000000000000
--- a/arch/s390/crypto/prng.c
+++ b/arch/s390/crypto/prng.c
@@@ -812,14 -757,13 +812,22 @@@ static int __init prng_init(void
  	int ret;
  
  	/* check if the CPU has a PRNG */
++<<<<<<< HEAD
 +	if (!crypt_s390_func_available(KMC_PRNG, CRYPT_S390_MSA))
++=======
+ 	if (!cpacf_query_func(CPACF_KMC, CPACF_KMC_PRNG))
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		return -EOPNOTSUPP;
  
  	/* choose prng mode */
  	if (prng_mode != PRNG_MODE_TDES) {
  		/* check for MSA5 support for PPNO operations */
++<<<<<<< HEAD
 +		if (!crypt_s390_func_available(PPNO_SHA512_DRNG_GEN,
 +					       CRYPT_S390_MSA5)) {
++=======
+ 		if (!cpacf_query_func(CPACF_PPNO, CPACF_PPNO_SHA512_DRNG_GEN)) {
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  			if (prng_mode == PRNG_MODE_SHA512) {
  				pr_err("The prng module cannot "
  				       "start in SHA-512 mode\n");
diff --cc arch/s390/crypto/sha1_s390.c
index 5b2bee323694,c7de53d8da75..000000000000
--- a/arch/s390/crypto/sha1_s390.c
+++ b/arch/s390/crypto/sha1_s390.c
@@@ -90,7 -91,7 +90,11 @@@ static struct shash_alg alg = 
  
  static int __init sha1_s390_init(void)
  {
++<<<<<<< HEAD
 +	if (!crypt_s390_func_available(KIMD_SHA_1, CRYPT_S390_MSA))
++=======
+ 	if (!cpacf_query_func(CPACF_KIMD, CPACF_KIMD_SHA_1))
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		return -EOPNOTSUPP;
  	return crypto_register_shash(&alg);
  }
diff --cc arch/s390/crypto/sha256_s390.c
index b74ff158108c,53c277999a28..000000000000
--- a/arch/s390/crypto/sha256_s390.c
+++ b/arch/s390/crypto/sha256_s390.c
@@@ -122,7 -123,7 +122,11 @@@ static int __init sha256_s390_init(void
  {
  	int ret;
  
++<<<<<<< HEAD
 +	if (!crypt_s390_func_available(KIMD_SHA_256, CRYPT_S390_MSA))
++=======
+ 	if (!cpacf_query_func(CPACF_KIMD, CPACF_KIMD_SHA_256))
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		return -EOPNOTSUPP;
  	ret = crypto_register_shash(&sha256_alg);
  	if (ret < 0)
diff --cc arch/s390/crypto/sha512_s390.c
index 0c36989ba182,2f4caa1ef123..000000000000
--- a/arch/s390/crypto/sha512_s390.c
+++ b/arch/s390/crypto/sha512_s390.c
@@@ -132,7 -133,7 +132,11 @@@ static int __init init(void
  {
  	int ret;
  
++<<<<<<< HEAD
 +	if (!crypt_s390_func_available(KIMD_SHA_512, CRYPT_S390_MSA))
++=======
+ 	if (!cpacf_query_func(CPACF_KIMD, CPACF_KIMD_SHA_512))
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  		return -EOPNOTSUPP;
  	if ((ret = crypto_register_shash(&sha512_alg)) < 0)
  		goto out;
diff --cc arch/s390/kvm/kvm-s390.c
index 9b2d6973d202,d6e7e527f0bf..000000000000
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@@ -118,11 -203,118 +118,118 @@@ int kvm_arch_hardware_setup(void
  
  void kvm_arch_hardware_unsetup(void)
  {
 -	gmap_unregister_pte_notifier(&gmap_notifier);
 -	gmap_unregister_pte_notifier(&vsie_gmap_notifier);
 -	atomic_notifier_chain_unregister(&s390_epoch_delta_notifier,
 -					 &kvm_clock_notifier);
 +	gmap_unregister_ipte_notifier(&gmap_notifier);
  }
  
 -static void allow_cpu_feat(unsigned long nr)
 +void kvm_arch_check_processor_compat(void *rtn)
  {
++<<<<<<< HEAD
++=======
+ 	set_bit_inv(nr, kvm_s390_available_cpu_feat);
+ }
+ 
+ static inline int plo_test_bit(unsigned char nr)
+ {
+ 	register unsigned long r0 asm("0") = (unsigned long) nr | 0x100;
+ 	int cc = 3; /* subfunction not available */
+ 
+ 	asm volatile(
+ 		/* Parameter registers are ignored for "test bit" */
+ 		"	plo	0,0,0,0(0)\n"
+ 		"	ipm	%0\n"
+ 		"	srl	%0,28\n"
+ 		: "=d" (cc)
+ 		: "d" (r0)
+ 		: "cc");
+ 	return cc == 0;
+ }
+ 
+ static void kvm_s390_cpu_feat_init(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < 256; ++i) {
+ 		if (plo_test_bit(i))
+ 			kvm_s390_available_subfunc.plo[i >> 3] |= 0x80 >> (i & 7);
+ 	}
+ 
+ 	if (test_facility(28)) /* TOD-clock steering */
+ 		ptff(kvm_s390_available_subfunc.ptff,
+ 		     sizeof(kvm_s390_available_subfunc.ptff),
+ 		     PTFF_QAF);
+ 
+ 	if (test_facility(17)) { /* MSA */
+ 		__cpacf_query(CPACF_KMAC, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.kmac);
+ 		__cpacf_query(CPACF_KMC, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.kmc);
+ 		__cpacf_query(CPACF_KM, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.km);
+ 		__cpacf_query(CPACF_KIMD, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.kimd);
+ 		__cpacf_query(CPACF_KLMD, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.klmd);
+ 	}
+ 	if (test_facility(76)) /* MSA3 */
+ 		__cpacf_query(CPACF_PCKMO, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.pckmo);
+ 	if (test_facility(77)) { /* MSA4 */
+ 		__cpacf_query(CPACF_KMCTR, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.kmctr);
+ 		__cpacf_query(CPACF_KMF, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.kmf);
+ 		__cpacf_query(CPACF_KMO, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.kmo);
+ 		__cpacf_query(CPACF_PCC, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.pcc);
+ 	}
+ 	if (test_facility(57)) /* MSA5 */
+ 		__cpacf_query(CPACF_PPNO, (cpacf_mask_t *)
+ 			      kvm_s390_available_subfunc.ppno);
+ 
+ 	if (MACHINE_HAS_ESOP)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_ESOP);
+ 	/*
+ 	 * We need SIE support, ESOP (PROT_READ protection for gmap_shadow),
+ 	 * 64bit SCAO (SCA passthrough) and IDTE (for gmap_shadow unshadowing).
+ 	 */
+ 	if (!sclp.has_sief2 || !MACHINE_HAS_ESOP || !sclp.has_64bscao ||
+ 	    !test_facility(3) || !nested)
+ 		return;
+ 	allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIEF2);
+ 	if (sclp.has_64bscao)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_64BSCAO);
+ 	if (sclp.has_siif)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_SIIF);
+ 	if (sclp.has_gpere)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GPERE);
+ 	if (sclp.has_gsls)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_GSLS);
+ 	if (sclp.has_ib)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IB);
+ 	if (sclp.has_cei)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_CEI);
+ 	if (sclp.has_ibs)
+ 		allow_cpu_feat(KVM_S390_VM_CPU_FEAT_IBS);
+ 	/*
+ 	 * KVM_S390_VM_CPU_FEAT_SKEY: Wrong shadow of PTE.I bits will make
+ 	 * all skey handling functions read/set the skey from the PGSTE
+ 	 * instead of the real storage key.
+ 	 *
+ 	 * KVM_S390_VM_CPU_FEAT_CMMA: Wrong shadow of PTE.I bits will make
+ 	 * pages being detected as preserved although they are resident.
+ 	 *
+ 	 * KVM_S390_VM_CPU_FEAT_PFMFI: Wrong shadow of PTE.I bits will
+ 	 * have the same effect as for KVM_S390_VM_CPU_FEAT_SKEY.
+ 	 *
+ 	 * For KVM_S390_VM_CPU_FEAT_SKEY, KVM_S390_VM_CPU_FEAT_CMMA and
+ 	 * KVM_S390_VM_CPU_FEAT_PFMFI, all PTE.I and PGSTE bits have to be
+ 	 * correctly shadowed. We can do that for the PGSTE but not for PTE.I.
+ 	 *
+ 	 * KVM_S390_VM_CPU_FEAT_SIGPIF: Wrong SCB addresses in the SCA. We
+ 	 * cannot easily shadow the SCA because of the ipte lock.
+ 	 */
++>>>>>>> 69c0e360f990 (s390/crypto: cpacf function detection)
  }
  
  int kvm_arch_init(void *opaque)
* Unmerged path arch/s390/include/asm/cpacf.h
* Unmerged path arch/s390/crypto/aes_s390.c
* Unmerged path arch/s390/crypto/des_s390.c
* Unmerged path arch/s390/crypto/ghash_s390.c
* Unmerged path arch/s390/crypto/prng.c
* Unmerged path arch/s390/crypto/sha1_s390.c
* Unmerged path arch/s390/crypto/sha256_s390.c
* Unmerged path arch/s390/crypto/sha512_s390.c
* Unmerged path arch/s390/include/asm/cpacf.h
* Unmerged path arch/s390/kvm/kvm-s390.c
