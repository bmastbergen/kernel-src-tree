s390/zcrypt: use spin_lock_bh for all queue locks and unlocks.

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [s390] zcrypt: use spin_lock_bh for all queue locks and unlocks (Hendrik Brueckner) [1380349]
Rebuild_FUZZ: 94.92%
commit-author Harald Freudenberger <freude@linux.vnet.ibm.com>
commit 7fbe5c0f2af3ab82fe6880af557e98a10d711370
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/7fbe5c0f.failed

During tests the Kernel complained about inconsistend lock state:
inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-W} usage.
Now all the queue locks use spin_lock_bh/spin_unlock_bh.

	Signed-off-by: Harald Freudenberger <freude@linux.vnet.ibm.com>
	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 7fbe5c0f2af3ab82fe6880af557e98a10d711370)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/s390/crypto/zcrypt_api.c
diff --cc drivers/s390/crypto/zcrypt_api.c
index 5282fcf3e2f5,926169c3d9b9..000000000000
--- a/drivers/s390/crypto/zcrypt_api.c
+++ b/drivers/s390/crypto/zcrypt_api.c
@@@ -733,62 -663,92 +733,131 @@@ static void zcrypt_status_mask(char sta
  
  static void zcrypt_qdepth_mask(char qdepth[AP_DEVICES])
  {
 -	struct zcrypt_card *zc;
 -	struct zcrypt_queue *zq;
 +	struct zcrypt_device *zdev;
  
  	memset(qdepth, 0, sizeof(char)	* AP_DEVICES);
++<<<<<<< HEAD
 +	spin_lock_bh(&zcrypt_device_lock);
 +	list_for_each_entry(zdev, &zcrypt_device_list, list) {
 +		spin_lock(&zdev->ap_dev->lock);
 +		qdepth[AP_QID_DEVICE(zdev->ap_dev->qid)] =
 +			zdev->ap_dev->pendingq_count +
 +			zdev->ap_dev->requestq_count;
 +		spin_unlock(&zdev->ap_dev->lock);
 +	}
 +	spin_unlock_bh(&zcrypt_device_lock);
++=======
+ 	spin_lock(&zcrypt_list_lock);
+ 	local_bh_disable();
+ 	for_each_zcrypt_card(zc) {
+ 		for_each_zcrypt_queue(zq, zc) {
+ 			if (AP_QID_QUEUE(zq->queue->qid) != ap_domain_index)
+ 				continue;
+ 			spin_lock(&zq->queue->lock);
+ 			qdepth[AP_QID_CARD(zq->queue->qid)] =
+ 				zq->queue->pendingq_count +
+ 				zq->queue->requestq_count;
+ 			spin_unlock(&zq->queue->lock);
+ 		}
+ 	}
+ 	local_bh_enable();
+ 	spin_unlock(&zcrypt_list_lock);
++>>>>>>> 7fbe5c0f2af3 (s390/zcrypt: use spin_lock_bh for all queue locks and unlocks.)
  }
  
  static void zcrypt_perdev_reqcnt(int reqcnt[AP_DEVICES])
  {
 -	struct zcrypt_card *zc;
 -	struct zcrypt_queue *zq;
 +	struct zcrypt_device *zdev;
  
  	memset(reqcnt, 0, sizeof(int) * AP_DEVICES);
++<<<<<<< HEAD
 +	spin_lock_bh(&zcrypt_device_lock);
 +	list_for_each_entry(zdev, &zcrypt_device_list, list) {
 +		spin_lock(&zdev->ap_dev->lock);
 +		reqcnt[AP_QID_DEVICE(zdev->ap_dev->qid)] =
 +			zdev->ap_dev->total_request_count;
 +		spin_unlock(&zdev->ap_dev->lock);
 +	}
 +	spin_unlock_bh(&zcrypt_device_lock);
++=======
+ 	spin_lock(&zcrypt_list_lock);
+ 	local_bh_disable();
+ 	for_each_zcrypt_card(zc) {
+ 		for_each_zcrypt_queue(zq, zc) {
+ 			if (AP_QID_QUEUE(zq->queue->qid) != ap_domain_index)
+ 				continue;
+ 			spin_lock(&zq->queue->lock);
+ 			reqcnt[AP_QID_CARD(zq->queue->qid)] =
+ 				zq->queue->total_request_count;
+ 			spin_unlock(&zq->queue->lock);
+ 		}
+ 	}
+ 	local_bh_enable();
+ 	spin_unlock(&zcrypt_list_lock);
++>>>>>>> 7fbe5c0f2af3 (s390/zcrypt: use spin_lock_bh for all queue locks and unlocks.)
  }
  
  static int zcrypt_pendingq_count(void)
  {
 -	struct zcrypt_card *zc;
 -	struct zcrypt_queue *zq;
 -	int pendingq_count;
 -
 +	struct zcrypt_device *zdev;
 +	int pendingq_count = 0;
 +
++<<<<<<< HEAD
 +	spin_lock_bh(&zcrypt_device_lock);
 +	list_for_each_entry(zdev, &zcrypt_device_list, list) {
 +		spin_lock(&zdev->ap_dev->lock);
 +		pendingq_count += zdev->ap_dev->pendingq_count;
 +		spin_unlock(&zdev->ap_dev->lock);
 +	}
 +	spin_unlock_bh(&zcrypt_device_lock);
++=======
+ 	pendingq_count = 0;
+ 	spin_lock(&zcrypt_list_lock);
+ 	local_bh_disable();
+ 	for_each_zcrypt_card(zc) {
+ 		for_each_zcrypt_queue(zq, zc) {
+ 			if (AP_QID_QUEUE(zq->queue->qid) != ap_domain_index)
+ 				continue;
+ 			spin_lock(&zq->queue->lock);
+ 			pendingq_count += zq->queue->pendingq_count;
+ 			spin_unlock(&zq->queue->lock);
+ 		}
+ 	}
+ 	local_bh_enable();
+ 	spin_unlock(&zcrypt_list_lock);
++>>>>>>> 7fbe5c0f2af3 (s390/zcrypt: use spin_lock_bh for all queue locks and unlocks.)
  	return pendingq_count;
  }
  
  static int zcrypt_requestq_count(void)
  {
 -	struct zcrypt_card *zc;
 -	struct zcrypt_queue *zq;
 -	int requestq_count;
 -
 +	struct zcrypt_device *zdev;
 +	int requestq_count = 0;
 +
++<<<<<<< HEAD
 +	spin_lock_bh(&zcrypt_device_lock);
 +	list_for_each_entry(zdev, &zcrypt_device_list, list) {
 +		spin_lock(&zdev->ap_dev->lock);
 +		requestq_count += zdev->ap_dev->requestq_count;
 +		spin_unlock(&zdev->ap_dev->lock);
 +	}
 +	spin_unlock_bh(&zcrypt_device_lock);
++=======
+ 	requestq_count = 0;
+ 	spin_lock(&zcrypt_list_lock);
+ 	local_bh_disable();
+ 	for_each_zcrypt_card(zc) {
+ 		for_each_zcrypt_queue(zq, zc) {
+ 			if (AP_QID_QUEUE(zq->queue->qid) != ap_domain_index)
+ 				continue;
+ 			spin_lock(&zq->queue->lock);
+ 			requestq_count += zq->queue->requestq_count;
+ 			spin_unlock(&zq->queue->lock);
+ 		}
+ 	}
+ 	local_bh_enable();
+ 	spin_unlock(&zcrypt_list_lock);
++>>>>>>> 7fbe5c0f2af3 (s390/zcrypt: use spin_lock_bh for all queue locks and unlocks.)
  	return requestq_count;
  }
  
* Unmerged path drivers/s390/crypto/zcrypt_api.c
