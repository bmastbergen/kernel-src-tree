x86: Use new cache mode type in memtype related functions

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] Use new cache mode type in memtype related functions (Suravee Suthikulpanit) [1361287]
Rebuild_FUZZ: 95.41%
commit-author Juergen Gross <jgross@suse.com>
commit e00c8cc93c1ac01ecd5049929a50fb47b62bb041
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e00c8cc9.failed

Instead of directly using the cache mode bits in the pte switch to
using the cache mode type.

Based-on-patch-by: Stefan Bader <stefan.bader@canonical.com>
	Signed-off-by: Juergen Gross <jgross@suse.com>
	Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: stefan.bader@canonical.com
	Cc: xen-devel@lists.xensource.com
	Cc: konrad.wilk@oracle.com
	Cc: ville.syrjala@linux.intel.com
	Cc: david.vrabel@citrix.com
	Cc: jbeulich@suse.com
	Cc: toshi.kani@hp.com
	Cc: plagnioj@jcrosoft.com
	Cc: tomi.valkeinen@ti.com
	Cc: bhelgaas@google.com
Link: http://lkml.kernel.org/r/1415019724-4317-14-git-send-email-jgross@suse.com
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
(cherry picked from commit e00c8cc93c1ac01ecd5049929a50fb47b62bb041)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/pat.c
diff --cc arch/x86/mm/pat.c
index 25b8aa9d4620,ef75f3f89810..000000000000
--- a/arch/x86/mm/pat.c
+++ b/arch/x86/mm/pat.c
@@@ -146,14 -146,14 +147,19 @@@ static unsigned long pat_x_mtrr_type(u6
  	 * Look for MTRR hint to get the effective type in case where PAT
  	 * request is for WB.
  	 */
++<<<<<<< HEAD
 +	if (req_type == _PAGE_CACHE_WB) {
 +		u8 mtrr_type, uniform;
++=======
+ 	if (req_type == _PAGE_CACHE_MODE_WB) {
+ 		u8 mtrr_type;
++>>>>>>> e00c8cc93c1a (x86: Use new cache mode type in memtype related functions)
  
 -		mtrr_type = mtrr_type_lookup(start, end);
 +		mtrr_type = mtrr_type_lookup(start, end, &uniform);
  		if (mtrr_type != MTRR_TYPE_WRBACK)
- 			return _PAGE_CACHE_UC_MINUS;
+ 			return _PAGE_CACHE_MODE_UC_MINUS;
  
- 		return _PAGE_CACHE_WB;
+ 		return _PAGE_CACHE_MODE_WB;
  	}
  
  	return req_type;
@@@ -615,17 -612,17 +618,22 @@@ static int reserve_pfn_range(u64 paddr
  		if (!pat_enabled)
  			return 0;
  
++<<<<<<< HEAD
 +		flags = lookup_memtype(paddr);
 +		if (want_flags != flags) {
++=======
+ 		pcm = lookup_memtype(paddr);
+ 		if (want_pcm != pcm) {
++>>>>>>> e00c8cc93c1a (x86: Use new cache mode type in memtype related functions)
  			printk(KERN_WARNING "%s:%d map pfn RAM range req %s for [mem %#010Lx-%#010Lx], got %s\n",
  				current->comm, current->pid,
- 				cattr_name(want_flags),
+ 				cattr_name(want_pcm),
  				(unsigned long long)paddr,
  				(unsigned long long)(paddr + size - 1),
- 				cattr_name(flags));
+ 				cattr_name(pcm));
  			*vma_prot = __pgprot((pgprot_val(*vma_prot) &
- 					      (~_PAGE_CACHE_MASK)) |
- 					     flags);
+ 					     (~_PAGE_CACHE_MASK)) |
+ 					     cachemode2protval(pcm));
  		}
  		return 0;
  	}
diff --git a/arch/x86/include/asm/cacheflush.h b/arch/x86/include/asm/cacheflush.h
index 445ea1815f9d..a21acde55d8e 100644
--- a/arch/x86/include/asm/cacheflush.h
+++ b/arch/x86/include/asm/cacheflush.h
@@ -10,10 +10,10 @@
 /*
  * X86 PAT uses page flags WC and Uncached together to keep track of
  * memory type of pages that have backing page struct. X86 PAT supports 3
- * different memory types, _PAGE_CACHE_WB, _PAGE_CACHE_WC and
- * _PAGE_CACHE_UC_MINUS and fourth state where page's memory type has not
+ * different memory types, _PAGE_CACHE_MODE_WB, _PAGE_CACHE_MODE_WC and
+ * _PAGE_CACHE_MODE_UC_MINUS and fourth state where page's memory type has not
  * been changed from its default (value of -1 used to denote this).
- * Note we do not support _PAGE_CACHE_UC here.
+ * Note we do not support _PAGE_CACHE_MODE_UC here.
  */
 
 #define _PGMT_DEFAULT		0
@@ -23,36 +23,40 @@
 #define _PGMT_MASK		(1UL << PG_uncached | 1UL << PG_arch_1)
 #define _PGMT_CLEAR_MASK	(~_PGMT_MASK)
 
-static inline unsigned long get_page_memtype(struct page *pg)
+static inline enum page_cache_mode get_page_memtype(struct page *pg)
 {
 	unsigned long pg_flags = pg->flags & _PGMT_MASK;
 
 	if (pg_flags == _PGMT_DEFAULT)
 		return -1;
 	else if (pg_flags == _PGMT_WC)
-		return _PAGE_CACHE_WC;
+		return _PAGE_CACHE_MODE_WC;
 	else if (pg_flags == _PGMT_UC_MINUS)
-		return _PAGE_CACHE_UC_MINUS;
+		return _PAGE_CACHE_MODE_UC_MINUS;
 	else
-		return _PAGE_CACHE_WB;
+		return _PAGE_CACHE_MODE_WB;
 }
 
-static inline void set_page_memtype(struct page *pg, unsigned long memtype)
+static inline void set_page_memtype(struct page *pg,
+				    enum page_cache_mode memtype)
 {
-	unsigned long memtype_flags = _PGMT_DEFAULT;
+	unsigned long memtype_flags;
 	unsigned long old_flags;
 	unsigned long new_flags;
 
 	switch (memtype) {
-	case _PAGE_CACHE_WC:
+	case _PAGE_CACHE_MODE_WC:
 		memtype_flags = _PGMT_WC;
 		break;
-	case _PAGE_CACHE_UC_MINUS:
+	case _PAGE_CACHE_MODE_UC_MINUS:
 		memtype_flags = _PGMT_UC_MINUS;
 		break;
-	case _PAGE_CACHE_WB:
+	case _PAGE_CACHE_MODE_WB:
 		memtype_flags = _PGMT_WB;
 		break;
+	default:
+		memtype_flags = _PGMT_DEFAULT;
+		break;
 	}
 
 	do {
@@ -61,8 +65,14 @@ static inline void set_page_memtype(struct page *pg, unsigned long memtype)
 	} while (cmpxchg(&pg->flags, old_flags, new_flags) != old_flags);
 }
 #else
-static inline unsigned long get_page_memtype(struct page *pg) { return -1; }
-static inline void set_page_memtype(struct page *pg, unsigned long memtype) { }
+static inline enum page_cache_mode get_page_memtype(struct page *pg)
+{
+	return -1;
+}
+static inline void set_page_memtype(struct page *pg,
+				    enum page_cache_mode memtype)
+{
+}
 #endif
 
 /*
diff --git a/arch/x86/include/asm/pat.h b/arch/x86/include/asm/pat.h
index d35ee2d976ca..150407a7234d 100644
--- a/arch/x86/include/asm/pat.h
+++ b/arch/x86/include/asm/pat.h
@@ -13,7 +13,7 @@ static const int pat_enabled;
 extern void pat_init(void);
 
 extern int reserve_memtype(u64 start, u64 end,
-		unsigned long req_type, unsigned long *ret_type);
+		enum page_cache_mode req_pcm, enum page_cache_mode *ret_pcm);
 extern int free_memtype(u64 start, u64 end);
 
 extern int kernel_map_sync_memtype(u64 base, unsigned long size,
diff --git a/arch/x86/mm/ioremap.c b/arch/x86/mm/ioremap.c
index bb79b07b43a9..54257be267ec 100644
--- a/arch/x86/mm/ioremap.c
+++ b/arch/x86/mm/ioremap.c
@@ -86,7 +86,6 @@ static void __iomem *__ioremap_caller(resource_size_t phys_addr,
 	const unsigned long unaligned_size = size;
 	struct vm_struct *area;
 	enum page_cache_mode new_pcm;
-	unsigned long new_prot_val;
 	pgprot_t prot;
 	int retval;
 	void __iomem *ret_addr;
@@ -123,14 +122,12 @@ static void __iomem *__ioremap_caller(resource_size_t phys_addr,
 	size = PAGE_ALIGN(last_addr+1) - phys_addr;
 
 	retval = reserve_memtype(phys_addr, (u64)phys_addr + size,
-				 cachemode2protval(pcm), &new_prot_val);
+						pcm, &new_pcm);
 	if (retval) {
 		printk(KERN_ERR "ioremap reserve_memtype failed %d\n", retval);
 		return NULL;
 	}
 
-	new_pcm = pgprot2cachemode(__pgprot(new_prot_val));
-
 	if (pcm != new_pcm) {
 		if (!is_new_memtype_allowed(phys_addr, size, pcm, new_pcm)) {
 			printk(KERN_ERR
diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c
index 3fe832369c84..1eb4395e3911 100644
--- a/arch/x86/mm/pageattr.c
+++ b/arch/x86/mm/pageattr.c
@@ -1462,7 +1462,7 @@ int set_memory_uc(unsigned long addr, int numpages)
 	 * for now UC MINUS. see comments in ioremap_nocache()
 	 */
 	ret = reserve_memtype(__pa(addr), __pa(addr) + numpages * PAGE_SIZE,
-			    _PAGE_CACHE_UC_MINUS, NULL);
+			      _PAGE_CACHE_MODE_UC_MINUS, NULL);
 	if (ret)
 		goto out_err;
 
@@ -1551,7 +1551,7 @@ int set_memory_wc(unsigned long addr, int numpages)
 		return set_memory_uc(addr, numpages);
 
 	ret = reserve_memtype(__pa(addr), __pa(addr) + numpages * PAGE_SIZE,
-		_PAGE_CACHE_WC, NULL);
+		_PAGE_CACHE_MODE_WC, NULL);
 	if (ret)
 		goto out_err;
 
* Unmerged path arch/x86/mm/pat.c
diff --git a/arch/x86/mm/pat_internal.h b/arch/x86/mm/pat_internal.h
index 77e5ba153fac..f6411620305d 100644
--- a/arch/x86/mm/pat_internal.h
+++ b/arch/x86/mm/pat_internal.h
@@ -10,30 +10,32 @@ struct memtype {
 	u64			start;
 	u64			end;
 	u64			subtree_max_end;
-	unsigned long		type;
+	enum page_cache_mode	type;
 	struct rb_node		rb;
 };
 
-static inline char *cattr_name(unsigned long flags)
+static inline char *cattr_name(enum page_cache_mode pcm)
 {
-	switch (flags & _PAGE_CACHE_MASK) {
-	case _PAGE_CACHE_UC:		return "uncached";
-	case _PAGE_CACHE_UC_MINUS:	return "uncached-minus";
-	case _PAGE_CACHE_WB:		return "write-back";
-	case _PAGE_CACHE_WC:		return "write-combining";
-	default:			return "broken";
+	switch (pcm) {
+	case _PAGE_CACHE_MODE_UC:		return "uncached";
+	case _PAGE_CACHE_MODE_UC_MINUS:		return "uncached-minus";
+	case _PAGE_CACHE_MODE_WB:		return "write-back";
+	case _PAGE_CACHE_MODE_WC:		return "write-combining";
+	case _PAGE_CACHE_MODE_WT:		return "write-through";
+	case _PAGE_CACHE_MODE_WP:		return "write-protected";
+	default:				return "broken";
 	}
 }
 
 #ifdef CONFIG_X86_PAT
 extern int rbt_memtype_check_insert(struct memtype *new,
-					unsigned long *new_type);
+					enum page_cache_mode *new_type);
 extern struct memtype *rbt_memtype_erase(u64 start, u64 end);
 extern struct memtype *rbt_memtype_lookup(u64 addr);
 extern int rbt_memtype_copy_nth_element(struct memtype *out, loff_t pos);
 #else
 static inline int rbt_memtype_check_insert(struct memtype *new,
-					unsigned long *new_type)
+					enum page_cache_mode *new_type)
 { return 0; }
 static inline struct memtype *rbt_memtype_erase(u64 start, u64 end)
 { return NULL; }
diff --git a/arch/x86/mm/pat_rbtree.c b/arch/x86/mm/pat_rbtree.c
index d07ba2453f27..5db980b4ae4b 100644
--- a/arch/x86/mm/pat_rbtree.c
+++ b/arch/x86/mm/pat_rbtree.c
@@ -132,11 +132,12 @@ static struct memtype *memtype_rb_match(struct rb_root *root,
 
 static int memtype_rb_check_conflict(struct rb_root *root,
 				u64 start, u64 end,
-				unsigned long reqtype, unsigned long *newtype)
+				enum page_cache_mode reqtype,
+				enum page_cache_mode *newtype)
 {
 	struct rb_node *node;
 	struct memtype *match;
-	int found_type = reqtype;
+	enum page_cache_mode found_type = reqtype;
 
 	match = memtype_rb_lowest_match(&memtype_rbroot, start, end);
 	if (match == NULL)
@@ -197,7 +198,8 @@ static void memtype_rb_insert(struct rb_root *root, struct memtype *newdata)
 	rb_insert_augmented(&newdata->rb, root, &memtype_rb_augment_cb);
 }
 
-int rbt_memtype_check_insert(struct memtype *new, unsigned long *ret_type)
+int rbt_memtype_check_insert(struct memtype *new,
+			     enum page_cache_mode *ret_type)
 {
 	int err = 0;
 
