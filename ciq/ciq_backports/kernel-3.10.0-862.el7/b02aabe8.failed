cpufreq: intel_pstate: Eliminate intel_pstate_get_min_max()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Eliminate intel_pstate_get_min_max() (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 91.74%
commit-author Rafael J. Wysocki <rafael.j.wysocki@intel.com>
commit b02aabe8ab9757a7dd5aa50e201a6d970f7e7a2f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b02aabe8.failed

Some computations in intel_pstate_get_min_max() are not necessary
and one of its two callers doesn't even use the full result.

First off, the fixed-point value of cpu->max_perf represents a
non-negative number between 0 and 1 inclusive and cpu->min_perf
cannot be greater than cpu->max_perf.  It is not necessary to check
those conditions every time the numbers in question are used.

Moreover, since intel_pstate_max_within_limits() only needs the
upper boundary, it doesn't make sense to compute the lower one in
there and returning min and max from intel_pstate_get_min_max()
via pointers doesn't look particularly nice.

For the above reasons, drop intel_pstate_get_min_max(), add a helper
to get the base P-state for min/max computations and carry out them
directly in the previous callers of intel_pstate_get_min_max().

	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit b02aabe8ab9757a7dd5aa50e201a6d970f7e7a2f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index b6f8db18a31a,b62daf5a4ee8..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -1024,107 -1496,39 +1024,142 @@@ static int knl_get_turbo_pstate(void
  	return ret;
  }
  
++<<<<<<< HEAD
 +static struct cpu_defaults core_params = {
 +	.pid_policy = {
 +		.sample_rate_ms = 10,
 +		.deadband = 0,
 +		.setpoint = 97,
 +		.p_gain_pct = 20,
 +		.d_gain_pct = 0,
 +		.i_gain_pct = 0,
 +	},
 +	.funcs = {
 +		.get_max = core_get_max_pstate,
 +		.get_max_physical = core_get_max_pstate_physical,
 +		.get_min = core_get_min_pstate,
 +		.get_turbo = core_get_turbo_pstate,
 +		.get_scaling = core_get_scaling,
 +		.set = core_set_pstate,
 +		.get_target_pstate = get_target_pstate_use_performance,
 +	},
 +};
 +
 +static struct cpu_defaults atom_params = {
 +	.pid_policy = {
 +		.sample_rate_ms = 10,
 +		.deadband = 0,
 +		.setpoint = 60,
 +		.p_gain_pct = 14,
 +		.d_gain_pct = 0,
 +		.i_gain_pct = 4,
 +	},
 +	.funcs = {
 +		.get_max = atom_get_max_pstate,
 +		.get_max_physical = atom_get_max_pstate,
 +		.get_min = atom_get_min_pstate,
 +		.get_turbo = atom_get_turbo_pstate,
 +		.set = atom_set_pstate,
 +		.get_scaling = atom_get_scaling,
 +		.get_vid = atom_get_vid,
 +		.get_target_pstate = get_target_pstate_use_cpu_load,
 +	},
 +};
 +
 +static struct cpu_defaults knl_params = {
 +	.pid_policy = {
 +		.sample_rate_ms = 10,
 +		.deadband = 0,
 +		.setpoint = 97,
 +		.p_gain_pct = 20,
 +		.d_gain_pct = 0,
 +		.i_gain_pct = 0,
 +	},
 +	.funcs = {
 +		.get_max = core_get_max_pstate,
 +		.get_max_physical = core_get_max_pstate_physical,
 +		.get_min = core_get_min_pstate,
 +		.get_turbo = knl_get_turbo_pstate,
 +		.get_scaling = core_get_scaling,
 +		.set = core_set_pstate,
 +		.get_target_pstate = get_target_pstate_use_performance,
 +	},
 +};
 +
 +static void intel_pstate_get_min_max(struct cpudata *cpu, int *min, int *max)
 +{
 +	int max_perf = cpu->pstate.turbo_pstate;
 +	int max_perf_adj;
 +	int min_perf;
 +
 +	if (limits->no_turbo || limits->turbo_disabled)
 +		max_perf = cpu->pstate.max_pstate;
 +
 +	/*
 +	 * performance can be limited by user through sysfs, by cpufreq
 +	 * policy, or by cpu specific default values determined through
 +	 * experimentation.
 +	 */
 +	max_perf_adj = fp_toint(max_perf * limits->max_perf);
 +	*max = clamp_t(int, max_perf_adj,
 +			cpu->pstate.min_pstate, cpu->pstate.turbo_pstate);
 +
 +	min_perf = fp_toint(max_perf * limits->min_perf);
 +	*min = clamp_t(int, min_perf, cpu->pstate.min_pstate, max_perf);
++=======
+ static int intel_pstate_get_base_pstate(struct cpudata *cpu)
+ {
+ 	return global.no_turbo || global.turbo_disabled ?
+ 			cpu->pstate.max_pstate : cpu->pstate.turbo_pstate;
++>>>>>>> b02aabe8ab97 (cpufreq: intel_pstate: Eliminate intel_pstate_get_min_max())
  }
  
  static void intel_pstate_set_pstate(struct cpudata *cpu, int pstate)
  {
++<<<<<<< HEAD
 +	int max_perf, min_perf;
 +
 +	update_turbo_state();
 +
 +	intel_pstate_get_min_max(cpu, &min_perf, &max_perf);
 +
 +	pstate = clamp_t(int, pstate, min_perf, max_perf);
 +
 +	if (pstate == cpu->pstate.current_pstate)
 +		return;
 +
 +	trace_cpu_frequency(pstate * cpu->pstate.scaling, cpu->cpu);
 +
 +	cpu->pstate.current_pstate = pstate;
 +
 +	pstate_funcs.set(cpu, pstate);
++=======
+ 	trace_cpu_frequency(pstate * cpu->pstate.scaling, cpu->cpu);
+ 	cpu->pstate.current_pstate = pstate;
+ 	/*
+ 	 * Generally, there is no guarantee that this code will always run on
+ 	 * the CPU being updated, so force the register update to run on the
+ 	 * right CPU.
+ 	 */
+ 	wrmsrl_on_cpu(cpu->cpu, MSR_IA32_PERF_CTL,
+ 		      pstate_funcs.get_val(cpu, pstate));
+ }
+ 
+ static void intel_pstate_set_min_pstate(struct cpudata *cpu)
+ {
+ 	intel_pstate_set_pstate(cpu, cpu->pstate.min_pstate);
+ }
+ 
+ static void intel_pstate_max_within_limits(struct cpudata *cpu)
+ {
+ 	int pstate;
+ 
+ 	update_turbo_state();
+ 	pstate = intel_pstate_get_base_pstate(cpu);
+ 	pstate = max(cpu->pstate.min_pstate,
+ 		     fp_ext_toint(pstate * cpu->max_perf));
+ 	intel_pstate_set_pstate(cpu, pstate);
++>>>>>>> b02aabe8ab97 (cpufreq: intel_pstate: Eliminate intel_pstate_get_min_max())
  }
  
  static void intel_pstate_get_cpu_pstates(struct cpudata *cpu)
@@@ -1286,26 -1681,46 +1321,50 @@@ static inline int32_t get_target_pstate
  	} else {
  		sample_ratio = div_fp(100 * cpu->sample.mperf, cpu->sample.tsc);
  		if (sample_ratio < int_tofp(1))
 -			perf_scaled = 0;
 +			core_busy = 0;
  	}
  
 -	cpu->sample.busy_scaled = perf_scaled;
 -	return cpu->pstate.current_pstate - pid_calc(&cpu->pid, perf_scaled);
 +	cpu->sample.busy_scaled = core_busy;
 +	return cpu->pstate.current_pstate - pid_calc(&cpu->pid, core_busy);
  }
  
 -static int intel_pstate_prepare_request(struct cpudata *cpu, int pstate)
 +static inline void intel_pstate_adjust_busy_pstate(struct cpudata *cpu)
  {
++<<<<<<< HEAD
 +	int from, target_pstate;
++=======
+ 	int max_pstate = intel_pstate_get_base_pstate(cpu);
+ 	int min_pstate;
+ 
+ 	min_pstate = max(cpu->pstate.min_pstate,
+ 			 fp_ext_toint(max_pstate * cpu->min_perf));
+ 	max_pstate = max(min_pstate, fp_ext_toint(max_pstate * cpu->max_perf));
+ 	return clamp_t(int, pstate, min_pstate, max_pstate);
+ }
+ 
+ static void intel_pstate_update_pstate(struct cpudata *cpu, int pstate)
+ {
+ 	if (pstate == cpu->pstate.current_pstate)
+ 		return;
+ 
+ 	cpu->pstate.current_pstate = pstate;
+ 	wrmsrl(MSR_IA32_PERF_CTL, pstate_funcs.get_val(cpu, pstate));
+ }
+ 
+ static void intel_pstate_adjust_pstate(struct cpudata *cpu, int target_pstate)
+ {
+ 	int from = cpu->pstate.current_pstate;
++>>>>>>> b02aabe8ab97 (cpufreq: intel_pstate: Eliminate intel_pstate_get_min_max())
  	struct sample *sample;
  
 -	update_turbo_state();
 +	from = cpu->pstate.current_pstate;
  
 -	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
 -	trace_cpu_frequency(target_pstate * cpu->pstate.scaling, cpu->cpu);
 -	intel_pstate_update_pstate(cpu, target_pstate);
 +	target_pstate = pstate_funcs.get_target_pstate(cpu);
 +
 +	intel_pstate_set_pstate(cpu, target_pstate);
  
  	sample = &cpu->sample;
 -	trace_pstate_sample(mul_ext_fp(100, sample->core_avg_perf),
 +	trace_pstate_sample(fp_toint(sample->core_pct_busy),
  		fp_toint(sample->busy_scaled),
  		from,
  		cpu->pstate.current_pstate,
* Unmerged path drivers/cpufreq/intel_pstate.c
