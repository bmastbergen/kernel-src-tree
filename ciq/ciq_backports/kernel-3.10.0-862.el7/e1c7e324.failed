dma-mapping: always provide the dma_map_ops based implementation

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Christoph Hellwig <hch@lst.de>
commit e1c7e324539ada3b2b13ca2898bcb4948a9ef9db
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/e1c7e324.failed

Move the generic implementation to <linux/dma-mapping.h> now that all
architectures support it and remove the HAVE_DMA_ATTR Kconfig symbol now
that everyone supports them.

[valentinrothberg@gmail.com: remove leftovers in Kconfig]
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Cc: "David S. Miller" <davem@davemloft.net>
	Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
	Cc: Chris Metcalf <cmetcalf@ezchip.com>
	Cc: David Howells <dhowells@redhat.com>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
	Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
	Cc: Helge Deller <deller@gmx.de>
	Cc: James Hogan <james.hogan@imgtec.com>
	Cc: Jesper Nilsson <jesper.nilsson@axis.com>
	Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
	Cc: Ley Foon Tan <lftan@altera.com>
	Cc: Mark Salter <msalter@redhat.com>
	Cc: Mikael Starvik <starvik@axis.com>
	Cc: Steven Miao <realmz6@gmail.com>
	Cc: Vineet Gupta <vgupta@synopsys.com>
	Cc: Christian Borntraeger <borntraeger@de.ibm.com>
	Cc: Joerg Roedel <jroedel@suse.de>
	Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
	Signed-off-by: Valentin Rothberg <valentinrothberg@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit e1c7e324539ada3b2b13ca2898bcb4948a9ef9db)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/alpha/Kconfig
#	arch/alpha/include/asm/dma-mapping.h
#	arch/arc/Kconfig
#	arch/arc/include/asm/dma-mapping.h
#	arch/arm/include/asm/dma-mapping.h
#	arch/arm64/Kconfig
#	arch/avr32/include/asm/dma-mapping.h
#	arch/blackfin/include/asm/dma-mapping.h
#	arch/c6x/Kconfig
#	arch/c6x/include/asm/dma-mapping.h
#	arch/cris/include/asm/dma-mapping.h
#	arch/frv/Kconfig
#	arch/frv/include/asm/dma-mapping.h
#	arch/h8300/Kconfig
#	arch/h8300/include/asm/dma-mapping.h
#	arch/ia64/Kconfig
#	arch/ia64/include/asm/dma-mapping.h
#	arch/m68k/include/asm/dma-mapping.h
#	arch/metag/include/asm/dma-mapping.h
#	arch/microblaze/Kconfig
#	arch/mips/Kconfig
#	arch/mips/include/asm/dma-mapping.h
#	arch/mn10300/Kconfig
#	arch/mn10300/include/asm/dma-mapping.h
#	arch/nios2/Kconfig
#	arch/openrisc/Kconfig
#	arch/openrisc/include/asm/dma-mapping.h
#	arch/parisc/Kconfig
#	arch/parisc/include/asm/dma-mapping.h
#	arch/powerpc/include/asm/dma-mapping.h
#	arch/s390/Kconfig
#	arch/s390/include/asm/dma-mapping.h
#	arch/sh/include/asm/dma-mapping.h
#	arch/sparc/Kconfig
#	arch/sparc/include/asm/dma-mapping.h
#	arch/tile/Kconfig
#	arch/tile/include/asm/dma-mapping.h
#	arch/unicore32/Kconfig
#	arch/unicore32/include/asm/dma-mapping.h
#	arch/x86/Kconfig
#	arch/x86/include/asm/dma-mapping.h
#	arch/xtensa/Kconfig
#	arch/xtensa/include/asm/dma-mapping.h
#	drivers/gpu/drm/imx/Kconfig
#	drivers/gpu/drm/rcar-du/Kconfig
#	drivers/gpu/drm/sti/Kconfig
#	drivers/gpu/drm/vc4/Kconfig
#	drivers/media/platform/Kconfig
#	include/asm-generic/dma-mapping-common.h
#	include/linux/dma-mapping.h
diff --cc arch/alpha/Kconfig
index 837a1f2d8b96,9d8a85801ed1..000000000000
--- a/arch/alpha/Kconfig
+++ b/arch/alpha/Kconfig
@@@ -6,8 -9,6 +6,11 @@@ config ALPH
  	select HAVE_OPROFILE
  	select HAVE_PCSPKR_PLATFORM
  	select HAVE_PERF_EVENTS
++<<<<<<< HEAD
 +	select HAVE_DMA_ATTRS
 +	select HAVE_GENERIC_HARDIRQS
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select VIRT_TO_BUS
  	select GENERIC_IRQ_PROBE
  	select AUTO_IRQ_AFFINITY if SMP
diff --cc arch/alpha/include/asm/dma-mapping.h
index dfa32f061320,3c3451f58ff4..000000000000
--- a/arch/alpha/include/asm/dma-mapping.h
+++ b/arch/alpha/include/asm/dma-mapping.h
@@@ -10,44 -10,6 +10,47 @@@ static inline struct dma_map_ops *get_d
  	return dma_ops;
  }
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-common.h>
 +
 +#define dma_alloc_coherent(d,s,h,f)	dma_alloc_attrs(d,s,h,f,NULL)
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				    dma_addr_t *dma_handle, gfp_t gfp,
 +				    struct dma_attrs *attrs)
 +{
 +	return get_dma_ops(dev)->alloc(dev, size, dma_handle, gfp, attrs);
 +}
 +
 +#define dma_free_coherent(d,s,c,h) dma_free_attrs(d,s,c,h,NULL)
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				  void *vaddr, dma_addr_t dma_handle,
 +				  struct dma_attrs *attrs)
 +{
 +	get_dma_ops(dev)->free(dev, size, vaddr, dma_handle, attrs);
 +}
 +
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return get_dma_ops(dev)->mapping_error(dev, dma_addr);
 +}
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	return get_dma_ops(dev)->dma_supported(dev, mask);
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	return get_dma_ops(dev)->set_dma_mask(dev, mask);
 +}
 +
 +#define dma_alloc_noncoherent(d, s, h, f)	dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h)	dma_free_coherent(d, s, v, h)
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #define dma_cache_sync(dev, va, size, dir)		  ((void)0)
  
  #endif	/* _ALPHA_DMA_MAPPING_H */
diff --cc arch/arc/Kconfig
index 5917099470ea,76dde9db7934..000000000000
--- a/arch/arc/Kconfig
+++ b/arch/arc/Kconfig
@@@ -34,6 -37,13 +34,16 @@@ config AR
  	select OF
  	select OF_EARLY_FLATTREE
  	select PERF_USE_VMALLOC
++<<<<<<< HEAD
++=======
+ 	select HAVE_DEBUG_STACKOVERFLOW
+ 
+ config TRACE_IRQFLAGS_SUPPORT
+ 	def_bool y
+ 
+ config LOCKDEP_SUPPORT
+ 	def_bool y
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  config SCHED_OMIT_FRAME_POINTER
  	def_bool y
diff --cc arch/arc/include/asm/dma-mapping.h
index 45b8e0cea176,660205414f1d..000000000000
--- a/arch/arc/include/asm/dma-mapping.h
+++ b/arch/arc/include/asm/dma-mapping.h
@@@ -11,211 -11,11 +11,214 @@@
  #ifndef ASM_ARC_DMA_MAPPING_H
  #define ASM_ARC_DMA_MAPPING_H
  
 -extern struct dma_map_ops arc_dma_ops;
 +#include <asm-generic/dma-coherent.h>
 +#include <asm/cacheflush.h>
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +#ifndef CONFIG_ARC_PLAT_NEEDS_CPU_TO_DMA
 +/*
 + * dma_map_* API take cpu addresses, which is kernel logical address in the
 + * untranslated address space (0x8000_0000) based. The dma address (bus addr)
 + * ideally needs to be 0x0000_0000 based hence these glue routines.
 + * However given that intermediate bus bridges can ignore the high bit, we can
 + * do with these routines being no-ops.
 + * If a platform/device comes up which sriclty requires 0 based bus addr
 + * (e.g. AHB-PCI bridge on Angel4 board), then it can provide it's own versions
 + */
 +#define plat_dma_addr_to_kernel(dev, addr) ((unsigned long)(addr))
 +#define plat_kernel_addr_to_dma(dev, ptr) ((dma_addr_t)(ptr))
 +
 +#else
 +#include <plat/dma_addr.h>
 +#endif
 +
 +void *dma_alloc_noncoherent(struct device *dev, size_t size,
 +			    dma_addr_t *dma_handle, gfp_t gfp);
 +
 +void dma_free_noncoherent(struct device *dev, size_t size, void *vaddr,
 +			  dma_addr_t dma_handle);
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			 dma_addr_t *dma_handle, gfp_t gfp);
 +
 +void dma_free_coherent(struct device *dev, size_t size, void *kvaddr,
 +		       dma_addr_t dma_handle);
 +
 +/* drivers/base/dma-mapping.c */
 +extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +extern int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size);
 +
 +#define dma_mmap_coherent(d, v, c, h, s) dma_common_mmap(d, v, c, h, s)
 +#define dma_get_sgtable(d, t, v, h, s) dma_common_get_sgtable(d, t, v, h, s)
 +
 +/*
 + * streaming DMA Mapping API...
 + * CPU accesses page via normal paddr, thus needs to explicitly made
 + * consistent before each use
 + */
 +
 +static inline void __inline_dma_cache_sync(unsigned long paddr, size_t size,
 +					   enum dma_data_direction dir)
 +{
 +	switch (dir) {
 +	case DMA_FROM_DEVICE:
 +		dma_cache_inv(paddr, size);
 +		break;
 +	case DMA_TO_DEVICE:
 +		dma_cache_wback(paddr, size);
 +		break;
 +	case DMA_BIDIRECTIONAL:
 +		dma_cache_wback_inv(paddr, size);
 +		break;
 +	default:
 +		pr_err("Invalid DMA dir [%d] for OP @ %lx\n", dir, paddr);
 +	}
 +}
 +
++<<<<<<< HEAD
 +void __arc_dma_cache_sync(unsigned long paddr, size_t size,
 +			  enum dma_data_direction dir);
 +
 +#define _dma_cache_sync(addr, sz, dir)			\
 +do {							\
 +	if (__builtin_constant_p(dir))			\
 +		__inline_dma_cache_sync(addr, sz, dir);	\
 +	else						\
 +		__arc_dma_cache_sync(addr, sz, dir);	\
 +}							\
 +while (0);
 +
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 +	       enum dma_data_direction dir)
 +{
 +	_dma_cache_sync((unsigned long)cpu_addr, size, dir);
 +	return plat_kernel_addr_to_dma(dev, cpu_addr);
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr,
 +		 size_t size, enum dma_data_direction dir)
 +{
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page,
 +	     unsigned long offset, size_t size,
 +	     enum dma_data_direction dir)
 +{
 +	unsigned long paddr = page_to_phys(page) + offset;
 +	return dma_map_single(dev, (void *)paddr, size, dir);
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_handle,
 +	       size_t size, enum dma_data_direction dir)
 +{
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg,
 +	   int nents, enum dma_data_direction dir)
 +{
 +	struct scatterlist *s;
 +	int i;
 +
 +	for_each_sg(sg, s, nents, i)
 +		s->dma_address = dma_map_page(dev, sg_page(s), s->offset,
 +					       s->length, dir);
 +
 +	return nents;
 +}
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg,
 +	     int nents, enum dma_data_direction dir)
 +{
 +	struct scatterlist *s;
 +	int i;
 +
 +	for_each_sg(sg, s, nents, i)
 +		dma_unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			size_t size, enum dma_data_direction dir)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle), size,
 +			DMA_FROM_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +			   size_t size, enum dma_data_direction dir)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle), size,
 +			DMA_TO_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle) + offset,
 +			size, DMA_FROM_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +	_dma_cache_sync(plat_dma_addr_to_kernel(dev, dma_handle) + offset,
 +			size, DMA_TO_DEVICE);
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction dir)
 +{
 +	int i;
 +
 +	for (i = 0; i < nelems; i++, sg++)
 +		_dma_cache_sync((unsigned int)sg_virt(sg), sg->length, dir);
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		       enum dma_data_direction dir)
 +{
 +	int i;
 +
 +	for (i = 0; i < nelems; i++, sg++)
 +		_dma_cache_sync((unsigned int)sg_virt(sg), sg->length, dir);
 +}
 +
 +static inline int dma_supported(struct device *dev, u64 dma_mask)
  {
 -	return &arc_dma_ops;
 +	/* Support 32 bit DMA mask exclusively */
 +	return dma_mask == DMA_BIT_MASK(32);
 +}
 +
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = dma_mask;
 +
 +	return 0;
  }
  
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif
diff --cc arch/arm/include/asm/dma-mapping.h
index 5b579b951503,6ad1ceda62a5..000000000000
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@@ -28,12 -38,8 +28,17 @@@ static inline void set_dma_ops(struct d
  	dev->archdata.dma_ops = ops;
  }
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	return get_dma_ops(dev)->set_dma_mask(dev, mask);
 +}
++=======
+ #define HAVE_ARCH_DMA_SUPPORTED 1
+ extern int dma_supported(struct device *dev, u64 mask);
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  #ifdef __arch_page_to_dma
  #error Please update to __arch_pfn_to_dma
diff --cc arch/arm64/Kconfig
index 56b3f6d447ae,8cc62289a63e..000000000000
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@@ -20,13 -59,30 +20,22 @@@ config ARM6
  	select HAVE_DEBUG_BUGVERBOSE
  	select HAVE_DEBUG_KMEMLEAK
  	select HAVE_DMA_API_DEBUG
++<<<<<<< HEAD
 +	select HAVE_DMA_ATTRS
++=======
+ 	select HAVE_DMA_CONTIGUOUS
+ 	select HAVE_DYNAMIC_FTRACE
+ 	select HAVE_EFFICIENT_UNALIGNED_ACCESS
+ 	select HAVE_FTRACE_MCOUNT_RECORD
+ 	select HAVE_FUNCTION_TRACER
+ 	select HAVE_FUNCTION_GRAPH_TRACER
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select HAVE_GENERIC_DMA_COHERENT
 +	select HAVE_GENERIC_HARDIRQS
  	select HAVE_HW_BREAKPOINT if PERF_EVENTS
 -	select HAVE_IRQ_TIME_ACCOUNTING
  	select HAVE_MEMBLOCK
 -	select HAVE_PATA_PLATFORM
  	select HAVE_PERF_EVENTS
 -	select HAVE_PERF_REGS
 -	select HAVE_PERF_USER_STACK_DUMP
 -	select HAVE_RCU_TABLE_FREE
 -	select HAVE_SYSCALL_TRACEPOINTS
 -	select IOMMU_DMA if IOMMU_SUPPORT
  	select IRQ_DOMAIN
 -	select IRQ_FORCED_THREADING
  	select MODULES_USE_ELF_RELA
  	select NO_BOOTMEM
  	select OF
diff --cc arch/avr32/include/asm/dma-mapping.h
index b3d18f9f3e8d,1115f2a645d1..000000000000
--- a/arch/avr32/include/asm/dma-mapping.h
+++ b/arch/avr32/include/asm/dma-mapping.h
@@@ -11,339 -4,11 +11,342 @@@
  extern void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
  	int direction);
  
 -extern struct dma_map_ops avr32_dma_ops;
 +/*
 + * Return whether the given device DMA address mask can be supported
 + * properly.  For example, if your device can only drive the low 24-bits
 + * during bus mastering, then you would pass 0x00ffffff as the mask
 + * to this function.
 + */
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	/* Fix when needed. I really don't know of any limitations */
 +	return 1;
 +}
 +
++<<<<<<< HEAD
 +static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = dma_mask;
 +	return 0;
 +}
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +/*
 + * dma_map_single can't fail as it is implemented now.
 + */
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t addr)
  {
 -	return &avr32_dma_ops;
 +	return 0;
  }
  
 +/**
 + * dma_alloc_coherent - allocate consistent memory for DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: required memory size
 + * @handle: bus-specific DMA address
 + *
 + * Allocate some uncached, unbuffered memory for a device for
 + * performing DMA.  This function allocates pages, and will
 + * return the CPU-viewed address, and sets @handle to be the
 + * device-viewed address.
 + */
 +extern void *dma_alloc_coherent(struct device *dev, size_t size,
 +				dma_addr_t *handle, gfp_t gfp);
 +
 +/**
 + * dma_free_coherent - free memory allocated by dma_alloc_coherent
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: size of memory originally requested in dma_alloc_coherent
 + * @cpu_addr: CPU-view address returned from dma_alloc_coherent
 + * @handle: device-view address returned from dma_alloc_coherent
 + *
 + * Free (and unmap) a DMA buffer previously allocated by
 + * dma_alloc_coherent().
 + *
 + * References to memory and mappings associated with cpu_addr/handle
 + * during and after this call executing are illegal.
 + */
 +extern void dma_free_coherent(struct device *dev, size_t size,
 +			      void *cpu_addr, dma_addr_t handle);
 +
 +/**
 + * dma_alloc_writecombine - allocate write-combining memory for DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: required memory size
 + * @handle: bus-specific DMA address
 + *
 + * Allocate some uncached, buffered memory for a device for
 + * performing DMA.  This function allocates pages, and will
 + * return the CPU-viewed address, and sets @handle to be the
 + * device-viewed address.
 + */
 +extern void *dma_alloc_writecombine(struct device *dev, size_t size,
 +				    dma_addr_t *handle, gfp_t gfp);
 +
 +/**
 + * dma_free_coherent - free memory allocated by dma_alloc_writecombine
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @size: size of memory originally requested in dma_alloc_writecombine
 + * @cpu_addr: CPU-view address returned from dma_alloc_writecombine
 + * @handle: device-view address returned from dma_alloc_writecombine
 + *
 + * Free (and unmap) a DMA buffer previously allocated by
 + * dma_alloc_writecombine().
 + *
 + * References to memory and mappings associated with cpu_addr/handle
 + * during and after this call executing are illegal.
 + */
 +extern void dma_free_writecombine(struct device *dev, size_t size,
 +				  void *cpu_addr, dma_addr_t handle);
 +
 +/**
 + * dma_map_single - map a single buffer for streaming DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @cpu_addr: CPU direct mapped address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Ensure that any data held in the cache is appropriately discarded
 + * or written back.
 + *
 + * The device owns this memory once this call has completed.  The CPU
 + * can regain ownership by calling dma_unmap_single() or dma_sync_single().
 + */
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	dma_cache_sync(dev, cpu_addr, size, direction);
 +	return virt_to_bus(cpu_addr);
 +}
 +
 +/**
 + * dma_unmap_single - unmap a single buffer previously mapped
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @handle: DMA address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Unmap a single streaming mode DMA translation.  The handle and size
 + * must match what was provided in the previous dma_map_single() call.
 + * All other usages are undefined.
 + *
 + * After this call, reads by the CPU to the buffer are guaranteed to see
 + * whatever the device wrote there.
 + */
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +
 +}
 +
 +/**
 + * dma_map_page - map a portion of a page for streaming DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @page: page that buffer resides in
 + * @offset: offset into page for start of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Ensure that any data held in the cache is appropriately discarded
 + * or written back.
 + *
 + * The device owns this memory once this call has completed.  The CPU
 + * can regain ownership by calling dma_unmap_page() or dma_sync_single().
 + */
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page,
 +	     unsigned long offset, size_t size,
 +	     enum dma_data_direction direction)
 +{
 +	return dma_map_single(dev, page_address(page) + offset,
 +			      size, direction);
 +}
 +
 +/**
 + * dma_unmap_page - unmap a buffer previously mapped through dma_map_page()
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @handle: DMA address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Unmap a single streaming mode DMA translation.  The handle and size
 + * must match what was provided in the previous dma_map_single() call.
 + * All other usages are undefined.
 + *
 + * After this call, reads by the CPU to the buffer are guaranteed to see
 + * whatever the device wrote there.
 + */
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	dma_unmap_single(dev, dma_address, size, direction);
 +}
 +
 +/**
 + * dma_map_sg - map a set of SG buffers for streaming mode DMA
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @sg: list of buffers
 + * @nents: number of buffers to map
 + * @dir: DMA transfer direction
 + *
 + * Map a set of buffers described by scatterlist in streaming
 + * mode for DMA.  This is the scatter-gather version of the
 + * above pci_map_single interface.  Here the scatter gather list
 + * elements are each tagged with the appropriate dma address
 + * and length.  They are obtained via sg_dma_{address,length}(SG).
 + *
 + * NOTE: An implementation may be able to use a smaller number of
 + *       DMA address/length pairs than there are SG table elements.
 + *       (for example via virtual mapping capabilities)
 + *       The routine returns the number of addr/length pairs actually
 + *       used, at most nents.
 + *
 + * Device ownership issues as mentioned above for pci_map_single are
 + * the same here.
 + */
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	for (i = 0; i < nents; i++) {
 +		char *virt;
 +
 +		sg[i].dma_address = page_to_bus(sg_page(&sg[i])) + sg[i].offset;
 +		virt = sg_virt(&sg[i]);
 +		dma_cache_sync(dev, virt, sg[i].length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +/**
 + * dma_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @sg: list of buffers
 + * @nents: number of buffers to map
 + * @dir: DMA transfer direction
 + *
 + * Unmap a set of streaming mode DMA translations.
 + * Again, CPU read rules concerning calls here are the same as for
 + * pci_unmap_single() above.
 + */
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +
 +}
 +
 +/**
 + * dma_sync_single_for_cpu
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @handle: DMA address of buffer
 + * @size: size of buffer to map
 + * @dir: DMA transfer direction
 + *
 + * Make physical memory consistent for a single streaming mode DMA
 + * translation after a transfer.
 + *
 + * If you perform a dma_map_single() but wish to interrogate the
 + * buffer using the cpu, yet do not wish to teardown the DMA mapping,
 + * you must call this function before doing so.  At the next point you
 + * give the DMA address back to the card, you must first perform a
 + * dma_sync_single_for_device, and then the device again owns the
 + * buffer.
 + */
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			size_t size, enum dma_data_direction direction)
 +{
 +	/*
 +	 * No need to do anything since the CPU isn't supposed to
 +	 * touch this memory after we flushed it at mapping- or
 +	 * sync-for-device time.
 +	 */
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +			   size_t size, enum dma_data_direction direction)
 +{
 +	dma_cache_sync(dev, bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
 +{
 +	/* just sync everything, that's all the pci API can do */
 +	dma_sync_single_for_cpu(dev, dma_handle, offset+size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +	/* just sync everything, that's all the pci API can do */
 +	dma_sync_single_for_device(dev, dma_handle, offset+size, direction);
 +}
 +
 +/**
 + * dma_sync_sg_for_cpu
 + * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 + * @sg: list of buffers
 + * @nents: number of buffers to map
 + * @dir: DMA transfer direction
 + *
 + * Make physical memory consistent for a set of streaming
 + * mode DMA translations after a transfer.
 + *
 + * The same as dma_sync_single_for_* but for a scatter-gather list,
 + * same rules and usage.
 + */
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 +		    int nents, enum dma_data_direction direction)
 +{
 +	/*
 +	 * No need to do anything since the CPU isn't supposed to
 +	 * touch this memory after we flushed it at mapping- or
 +	 * sync-for-device time.
 +	 */
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 +		       int nents, enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	for (i = 0; i < nents; i++) {
 +		dma_cache_sync(dev, sg_virt(&sg[i]), sg[i].length, direction);
 +	}
 +}
 +
 +/* Now for the API extensions over the pci_ one */
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +/* drivers/base/dma-mapping.c */
 +extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +extern int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size);
 +
 +#define dma_mmap_coherent(d, v, c, h, s) dma_common_mmap(d, v, c, h, s)
 +#define dma_get_sgtable(d, t, v, h, s) dma_common_get_sgtable(d, t, v, h, s)
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif /* __ASM_AVR32_DMA_MAPPING_H */
diff --cc arch/blackfin/include/asm/dma-mapping.h
index 054d9ec57d9d,3490570aaa82..000000000000
--- a/arch/blackfin/include/asm/dma-mapping.h
+++ b/arch/blackfin/include/asm/dma-mapping.h
@@@ -66,102 -36,11 +66,105 @@@ _dma_sync(dma_addr_t addr, size_t size
  		__dma_sync(addr, size, dir);
  }
  
 -extern struct dma_map_ops bfin_dma_ops;
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction dir)
 +{
 +	_dma_sync((dma_addr_t)ptr, size, dir);
 +	return (dma_addr_t) ptr;
 +}
 +
++<<<<<<< HEAD
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page,
 +	     unsigned long offset, size_t size,
 +	     enum dma_data_direction dir)
 +{
 +	return dma_map_single(dev, page_address(page) + offset, size, dir);
 +}
 +
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction dir)
 +{
 +	BUG_ON(!valid_dma_direction(dir));
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 +	       enum dma_data_direction dir)
 +{
 +	dma_unmap_single(dev, dma_addr, size, dir);
 +}
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +extern int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +		      enum dma_data_direction dir);
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg,
 +	     int nhwentries, enum dma_data_direction dir)
  {
 -	return &bfin_dma_ops;
 +	BUG_ON(!valid_dma_direction(dir));
  }
  
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction dir)
 +{
 +	BUG_ON(!valid_dma_direction(dir));
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction dir)
 +{
 +	_dma_sync(handle + offset, size, dir);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
 +			enum dma_data_direction dir)
 +{
 +	dma_sync_single_range_for_cpu(dev, handle, 0, size, dir);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
 +			   enum dma_data_direction dir)
 +{
 +	dma_sync_single_range_for_device(dev, handle, 0, size, dir);
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
 +		    enum dma_data_direction dir)
 +{
 +	BUG_ON(!valid_dma_direction(dir));
 +}
 +
 +extern void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 +		       int nents, enum dma_data_direction dir);
 +
 +static inline void
 +dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 +	       enum dma_data_direction dir)
 +{
 +	_dma_sync((dma_addr_t)vaddr, size, dir);
 +}
 +
 +/* drivers/base/dma-mapping.c */
 +extern int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 +			   void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +extern int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size);
 +
 +#define dma_mmap_coherent(d, v, c, h, s) dma_common_mmap(d, v, c, h, s)
 +#define dma_get_sgtable(d, t, v, h, s) dma_common_get_sgtable(d, t, v, h, s)
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif				/* _BLACKFIN_DMA_MAPPING_H */
diff --cc arch/c6x/Kconfig
index f6a3648f5ec3,79049d432d3c..000000000000
--- a/arch/c6x/Kconfig
+++ b/arch/c6x/Kconfig
@@@ -18,6 -17,7 +18,10 @@@ config C6
  	select OF_EARLY_FLATTREE
  	select GENERIC_CLOCKEVENTS
  	select MODULES_USE_ELF_RELA
++<<<<<<< HEAD
++=======
+ 	select ARCH_NO_COHERENT_DMA_MMAP
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  config MMU
  	def_bool n
diff --cc arch/c6x/include/asm/dma-mapping.h
index 88bd0d899bdb,6b5cd7b0cf32..000000000000
--- a/arch/c6x/include/asm/dma-mapping.h
+++ b/arch/c6x/include/asm/dma-mapping.h
@@@ -30,78 -15,19 +30,81 @@@ static inline int dma_set_mask(struct d
  /*
   * DMA errors are defined by all-bits-set in the DMA address.
   */
 -#define DMA_ERROR_CODE ~0
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	debug_dma_mapping_error(dev, dma_addr);
 +	return dma_addr == ~0;
 +}
 +
++<<<<<<< HEAD
 +extern dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 +				 size_t size, enum dma_data_direction dir);
 +
 +extern void dma_unmap_single(struct device *dev, dma_addr_t handle,
 +			     size_t size, enum dma_data_direction dir);
 +
 +extern int dma_map_sg(struct device *dev, struct scatterlist *sglist,
 +		      int nents, enum dma_data_direction direction);
 +
 +extern void dma_unmap_sg(struct device *dev, struct scatterlist *sglist,
 +			 int nents, enum dma_data_direction direction);
 +
 +static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 +				      unsigned long offset, size_t size,
 +				      enum dma_data_direction dir)
 +{
 +	dma_addr_t handle;
  
 -extern struct dma_map_ops c6x_dma_ops;
 +	handle = dma_map_single(dev, page_address(page) + offset, size, dir);
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +	debug_dma_map_page(dev, page, offset, size, dir, handle, false);
 +
 +	return handle;
 +}
 +
 +static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
 +		size_t size, enum dma_data_direction dir)
  {
 -	return &c6x_dma_ops;
 +	dma_unmap_single(dev, handle, size, dir);
 +
 +	debug_dma_unmap_page(dev, handle, size, dir, false);
  }
  
 +extern void dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,
 +				    size_t size, enum dma_data_direction dir);
 +
 +extern void dma_sync_single_for_device(struct device *dev, dma_addr_t handle,
 +				       size_t size,
 +				       enum dma_data_direction dir);
 +
 +extern void dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 +				int nents, enum dma_data_direction dir);
 +
 +extern void dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 +				   int nents, enum dma_data_direction dir);
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  extern void coherent_mem_init(u32 start, u32 size);
 -void *c6x_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 -		gfp_t gfp, struct dma_attrs *attrs);
 -void c6x_dma_free(struct device *dev, size_t size, void *vaddr,
 -		dma_addr_t dma_handle, struct dma_attrs *attrs);
 +extern void *dma_alloc_coherent(struct device *, size_t, dma_addr_t *, gfp_t);
 +extern void dma_free_coherent(struct device *, size_t, void *, dma_addr_t);
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent((d), (s), (h), (f))
 +#define dma_free_noncoherent(d, s, v, h)  dma_free_coherent((d), (s), (v), (h))
 +
 +/* Not supported for now */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
 +{
 +	return -EINVAL;
 +}
 +
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
 +{
 +	return -EINVAL;
 +}
  
  #endif	/* _ASM_C6X_DMA_MAPPING_H */
diff --cc arch/cris/include/asm/dma-mapping.h
index 2f0f654f1b44,5a370178a0e9..000000000000
--- a/arch/cris/include/asm/dma-mapping.h
+++ b/arch/cris/include/asm/dma-mapping.h
@@@ -3,155 -1,21 +3,158 @@@
  #ifndef _ASM_CRIS_DMA_MAPPING_H
  #define _ASM_CRIS_DMA_MAPPING_H
  
 +#include <linux/mm.h>
 +#include <linux/kernel.h>
 +
 +#include <asm/cache.h>
 +#include <asm/io.h>
 +#include <asm/scatterlist.h>
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
  #ifdef CONFIG_PCI
 -extern struct dma_map_ops v32_dma_ops;
 +#include <asm-generic/dma-coherent.h>
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			   dma_addr_t *dma_handle, gfp_t flag);
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +void dma_free_coherent(struct device *dev, size_t size,
 +			 void *vaddr, dma_addr_t dma_handle);
 +#else
 +static inline void *
 +dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,
 +                   gfp_t flag)
  {
 -	return &v32_dma_ops;
 +        BUG();
 +        return NULL;
  }
 -#else
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +
 +static inline void
 +dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
 +                    dma_addr_t dma_handle)
  {
 -	BUG();
 -	return NULL;
 +        BUG();
  }
  #endif
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return virt_to_phys(ptr);
 +}
 +
++<<<<<<< HEAD
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	printk("Map sg\n");
 +	return nents;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return page_to_phys(page) + offset;
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +			enum dma_data_direction direction)
 +{
 +}
  
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle, size_t size,
 +			enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction direction)
 +{
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction direction)
 +{
 +}
 +
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int
 +dma_supported(struct device *dev, u64 mask)
 +{
 +        /*
 +         * we fall back to GFP_DMA when the mask isn't all 1s,
 +         * so we can't guarantee allocations that must be
 +         * within a tighter range than GFP_DMA..
 +         */
 +        if(mask < 0x00ffffff)
 +                return 0;
 +
 +	return 1;
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if(!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  static inline void
  dma_cache_sync(struct device *dev, void *vaddr, size_t size,
  	       enum dma_data_direction direction)
diff --cc arch/frv/Kconfig
index 2ce731f9aa4d,eefd9a4ed156..000000000000
--- a/arch/frv/Kconfig
+++ b/arch/frv/Kconfig
@@@ -14,6 -13,9 +14,11 @@@ config FR
  	select ARCH_WANT_IPC_PARSE_VERSION
  	select OLD_SIGSUSPEND3
  	select OLD_SIGACTION
++<<<<<<< HEAD
++=======
+ 	select HAVE_DEBUG_STACKOVERFLOW
+ 	select ARCH_NO_COHERENT_DMA_MMAP
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  config ZONE_DMA
  	bool
diff --cc arch/frv/include/asm/dma-mapping.h
index 1746a2b8e6e7,9a82bfa4303b..000000000000
--- a/arch/frv/include/asm/dma-mapping.h
+++ b/arch/frv/include/asm/dma-mapping.h
@@@ -132,19 -21,4 +132,22 @@@ void dma_cache_sync(struct device *dev
  	flush_write_buffers();
  }
  
++<<<<<<< HEAD
 +/* Not supported for now */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
 +{
 +	return -EINVAL;
 +}
 +
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
 +{
 +	return -EINVAL;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif  /* _ASM_DMA_MAPPING_H */
diff --cc arch/h8300/Kconfig
index 303e4f9a79d1,8c7c82586da0..000000000000
--- a/arch/h8300/Kconfig
+++ b/arch/h8300/Kconfig
@@@ -6,75 -3,32 +6,88 @@@ config H830
  	select GENERIC_ATOMIC64
  	select HAVE_UID16
  	select VIRT_TO_BUS
 +	select ARCH_WANT_IPC_PARSE_VERSION
  	select GENERIC_IRQ_SHOW
 -	select FRAME_POINTER
  	select GENERIC_CPU_DEVICES
  	select MODULES_USE_ELF_RELA
++<<<<<<< HEAD
 +	select OLD_SIGSUSPEND3
 +	select OLD_SIGACTION
 +	select HAVE_UNDERSCORE_SYMBOL_PREFIX
 +
 +config MMU
 +	bool
 +	default n
 +
 +config SWAP
 +	bool
 +	default n
 +
 +config ZONE_DMA
 +	bool
 +	default y
 +
 +config FPU
 +	bool
 +	default n
++=======
+ 	select GENERIC_CLOCKEVENTS
+ 	select CLKDEV_LOOKUP
+ 	select COMMON_CLK
+ 	select ARCH_WANT_FRAME_POINTERS
+ 	select OF
+ 	select OF_IRQ
+ 	select OF_EARLY_FLATTREE
+ 	select HAVE_MEMBLOCK
+ 	select CLKSRC_OF
+ 	select H8300_TMR8
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  config RWSEM_GENERIC_SPINLOCK
 -	def_bool y
 +	bool
 +	default y
 +
 +config RWSEM_XCHGADD_ALGORITHM
 +	bool
 +	default n
 +
 +config ARCH_HAS_ILOG2_U32
 +	bool
 +	default n
 +
 +config ARCH_HAS_ILOG2_U64
 +	bool
 +	default n
  
  config GENERIC_HWEIGHT
 +	bool
 +	default y
 +
 +config GENERIC_CALIBRATE_DELAY
 +	bool
 +	default y
 +
 +config GENERIC_BUG
 +        bool
 +        depends on BUG
 +
 +config TIME_LOW_RES
 +	bool
 +	default y
 +
 +config NO_IOPORT
  	def_bool y
  
 -config NO_IOPORT_MAP
 +config NO_DMA
  	def_bool y
  
 -config GENERIC_CSUM
 -        def_bool y
 +config ISA
 +	bool
 +	default y
 +
 +config PCI
 +	bool
 +	default n
  
  config HZ
  	int
diff --cc arch/ia64/Kconfig
index cc595fb7224a,fb0515eb639b..000000000000
--- a/arch/ia64/Kconfig
+++ b/arch/ia64/Kconfig
@@@ -18,10 -25,9 +18,14 @@@ config IA6
  	select HAVE_FTRACE_MCOUNT_RECORD
  	select HAVE_DYNAMIC_FTRACE if (!ITANIUM)
  	select HAVE_FUNCTION_TRACER
++<<<<<<< HEAD
 +	select HAVE_DMA_ATTRS
++=======
+ 	select TTY
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select HAVE_ARCH_TRACEHOOK
  	select HAVE_DMA_API_DEBUG
 +	select HAVE_GENERIC_HARDIRQS
  	select HAVE_MEMBLOCK
  	select HAVE_MEMBLOCK_NODE_MAP
  	select HAVE_VIRT_CPU_ACCOUNTING
diff --cc arch/ia64/include/asm/dma-mapping.h
index cf3ab7e784b5,d472805edfa9..000000000000
--- a/arch/ia64/include/asm/dma-mapping.h
+++ b/arch/ia64/include/asm/dma-mapping.h
@@@ -23,60 -23,8 +23,63 @@@ extern void machvec_dma_sync_single(str
  extern void machvec_dma_sync_sg(struct device *, struct scatterlist *, int,
  				enum dma_data_direction);
  
 +#define dma_alloc_coherent(d,s,h,f)	dma_alloc_attrs(d,s,h,f,NULL)
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				    dma_addr_t *daddr, gfp_t gfp,
 +				    struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	void *caddr;
 +
 +	caddr = ops->alloc(dev, size, daddr, gfp, attrs);
 +	debug_dma_alloc_coherent(dev, size, *daddr, caddr);
 +	return caddr;
 +}
 +
 +#define dma_free_coherent(d,s,c,h) dma_free_attrs(d,s,c,h,NULL)
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				  void *caddr, dma_addr_t daddr,
 +				  struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	debug_dma_free_coherent(dev, size, caddr, daddr);
 +	ops->free(dev, size, caddr, daddr, attrs);
 +}
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
  #define get_dma_ops(dev) platform_dma_get_ops(dev)
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t daddr)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	debug_dma_mapping_error(dev, daddr);
 +	return ops->mapping_error(dev, daddr);
 +}
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = platform_dma_get_ops(dev);
 +	return ops->dma_supported(dev, mask);
 +}
 +
 +static inline int
 +dma_set_mask (struct device *dev, u64 mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +	*dev->dma_mask = mask;
 +	return 0;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
  {
  	if (!dev->dma_mask)
diff --cc arch/m68k/include/asm/dma-mapping.h
index 05aa53594d49,96c536194287..000000000000
--- a/arch/m68k/include/asm/dma-mapping.h
+++ b/arch/m68k/include/asm/dma-mapping.h
@@@ -1,51 -1,13 +1,54 @@@
  #ifndef _M68K_DMA_MAPPING_H
  #define _M68K_DMA_MAPPING_H
  
 -extern struct dma_map_ops m68k_dma_ops;
 +#include <asm/cache.h>
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +struct scatterlist;
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	return 1;
 +}
 +
++<<<<<<< HEAD
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	return 0;
 +}
 +
 +extern void *dma_alloc_coherent(struct device *, size_t,
 +				dma_addr_t *, gfp_t);
 +extern void dma_free_coherent(struct device *, size_t,
 +			      void *, dma_addr_t);
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				    dma_addr_t *dma_handle, gfp_t flag,
 +				    struct dma_attrs *attrs)
 +{
 +	/* attrs is not supported and ignored */
 +	return dma_alloc_coherent(dev, size, dma_handle, flag);
 +}
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				  void *cpu_addr, dma_addr_t dma_handle,
 +				  struct dma_attrs *attrs)
  {
 -        return &m68k_dma_ops;
 +	/* attrs is not supported and ignored */
 +	dma_free_coherent(dev, size, cpu_addr, dma_handle);
  }
  
 +static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
 +					  dma_addr_t *handle, gfp_t flag)
 +{
 +	return dma_alloc_coherent(dev, size, handle, flag);
 +}
 +static inline void dma_free_noncoherent(struct device *dev, size_t size,
 +					void *addr, dma_addr_t handle)
 +{
 +	dma_free_coherent(dev, size, addr, handle);
 +}
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
  				  enum dma_data_direction dir)
  {
diff --cc arch/metag/include/asm/dma-mapping.h
index 14b23efd9b7a,27af5d479ce6..000000000000
--- a/arch/metag/include/asm/dma-mapping.h
+++ b/arch/metag/include/asm/dma-mapping.h
@@@ -1,175 -1,13 +1,178 @@@
  #ifndef _ASM_METAG_DMA_MAPPING_H
  #define _ASM_METAG_DMA_MAPPING_H
  
 -extern struct dma_map_ops metag_dma_ops;
 +#include <linux/mm.h>
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +#include <asm/cache.h>
 +#include <asm/io.h>
 +#include <linux/scatterlist.h>
 +#include <asm/bug.h>
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			 dma_addr_t *dma_handle, gfp_t flag);
 +
 +void dma_free_coherent(struct device *dev, size_t size,
 +		       void *vaddr, dma_addr_t dma_handle);
 +
 +void dma_sync_for_device(void *vaddr, size_t size, int dma_direction);
 +void dma_sync_for_cpu(void *vaddr, size_t size, int dma_direction);
 +
 +int dma_mmap_coherent(struct device *dev, struct vm_area_struct *vma,
 +		      void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +
 +int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
 +			  void *cpu_addr, dma_addr_t dma_addr, size_t size);
 +
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	WARN_ON(size == 0);
 +	dma_sync_for_device(ptr, size, direction);
 +	return virt_to_phys(ptr);
 +}
 +
++<<<<<<< HEAD
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	dma_sync_for_cpu(phys_to_virt(dma_addr), size, direction);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sglist, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	BUG_ON(!valid_dma_direction(direction));
 +	WARN_ON(nents == 0 || sglist[0].length == 0);
 +
 +	for_each_sg(sglist, sg, nents, i) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +		dma_sync_for_device(sg_virt(sg), sg->length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	dma_sync_for_device((void *)(page_to_phys(page) + offset), size,
 +			    direction);
 +	return page_to_phys(page) + offset;
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(!valid_dma_direction(direction));
 +	dma_sync_for_cpu(phys_to_virt(dma_address), size, direction);
 +}
 +
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sglist, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	struct scatterlist *sg;
 +	int i;
 +
 +	BUG_ON(!valid_dma_direction(direction));
 +	WARN_ON(nhwentries == 0 || sglist[0].length == 0);
 +
 +	for_each_sg(sglist, sg, nhwentries, i) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +		dma_sync_for_cpu(sg_virt(sg), sg->length, direction);
 +	}
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +			enum dma_data_direction direction)
 +{
 +	dma_sync_for_cpu(phys_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +			   size_t size, enum dma_data_direction direction)
 +{
 +	dma_sync_for_device(phys_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +			      unsigned long offset, size_t size,
 +			      enum dma_data_direction direction)
  {
 -	return &metag_dma_ops;
 +	dma_sync_for_cpu(phys_to_virt(dma_handle)+offset, size,
 +			 direction);
  }
  
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +				 unsigned long offset, size_t size,
 +				 enum dma_data_direction direction)
 +{
 +	dma_sync_for_device(phys_to_virt(dma_handle)+offset, size,
 +			    direction);
 +}
 +
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		    enum dma_data_direction direction)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		dma_sync_for_cpu(sg_virt(sg), sg->length, direction);
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		       enum dma_data_direction direction)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		dma_sync_for_device(sg_virt(sg), sg->length, direction);
 +}
 +
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +#define dma_supported(dev, mask)        (1)
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  /*
   * dma_alloc_noncoherent() returns non-cacheable memory, so there's no need to
   * do any flushing here.
diff --cc arch/microblaze/Kconfig
index 4fab52294d98,53b69deceb99..000000000000
--- a/arch/microblaze/Kconfig
+++ b/arch/microblaze/Kconfig
@@@ -23,12 -15,23 +23,27 @@@ config MICROBLAZ
  	select GENERIC_IRQ_PROBE
  	select GENERIC_IRQ_SHOW
  	select GENERIC_PCI_IOMAP
++<<<<<<< HEAD
 +	select GENERIC_CPU_DEVICES
 +	select GENERIC_ATOMIC64
 +	select GENERIC_CLOCKEVENTS
 +	select GENERIC_IDLE_POLL_SETUP
++=======
+ 	select GENERIC_SCHED_CLOCK
+ 	select HAVE_ARCH_KGDB
+ 	select HAVE_DEBUG_KMEMLEAK
+ 	select HAVE_DMA_API_DEBUG
+ 	select HAVE_DYNAMIC_FTRACE
+ 	select HAVE_FTRACE_MCOUNT_RECORD
+ 	select HAVE_FUNCTION_GRAPH_TRACER
+ 	select HAVE_FUNCTION_TRACER
+ 	select HAVE_MEMBLOCK
+ 	select HAVE_MEMBLOCK_NODE_MAP
+ 	select HAVE_OPROFILE
+ 	select IRQ_DOMAIN
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select MODULES_USE_ELF_RELA
 -	select OF
 -	select OF_EARLY_FLATTREE
 -	select TRACING_SUPPORT
 -	select VIRT_TO_BUS
 +	select CLONE_BACKWARDS3
  
  config SWAP
  	def_bool n
diff --cc arch/mips/Kconfig
index e53e2b40d695,fbf3f6670b69..000000000000
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@@ -16,15 -23,16 +16,19 @@@ config MIP
  	select HAVE_FUNCTION_GRAPH_TRACER
  	select HAVE_KPROBES
  	select HAVE_KRETPROBES
 -	select HAVE_SYSCALL_TRACEPOINTS
  	select HAVE_DEBUG_KMEMLEAK
 -	select HAVE_SYSCALL_TRACEPOINTS
 -	select ARCH_HAS_ELF_RANDOMIZE
 +	select ARCH_BINFMT_ELF_RANDOMIZE_PIE
  	select HAVE_ARCH_TRANSPARENT_HUGEPAGE if CPU_SUPPORTS_HUGEPAGES && 64BIT
 -	select RTC_LIB if !MACH_LOONGSON64
 +	select RTC_LIB if !MACH_LOONGSON
  	select GENERIC_ATOMIC64 if !64BIT
  	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
++<<<<<<< HEAD
 +	select HAVE_DMA_ATTRS
++=======
+ 	select HAVE_DMA_CONTIGUOUS
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select HAVE_DMA_API_DEBUG
 +	select HAVE_GENERIC_HARDIRQS
  	select GENERIC_IRQ_PROBE
  	select GENERIC_IRQ_SHOW
  	select GENERIC_PCI_IOMAP
diff --cc arch/mips/include/asm/dma-mapping.h
index 84238c574d5e,12fa79e2f1b4..000000000000
--- a/arch/mips/include/asm/dma-mapping.h
+++ b/arch/mips/include/asm/dma-mapping.h
@@@ -30,33 -29,6 +30,36 @@@ static inline bool dma_capable(struct d
  
  static inline void dma_mark_clean(void *addr, size_t size) {}
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	return ops->dma_supported(dev, mask);
 +}
 +
 +static inline int dma_mapping_error(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	debug_dma_mapping_error(dev, mask);
 +	return ops->mapping_error(dev, mask);
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if(!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  extern void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
  	       enum dma_data_direction direction);
  
diff --cc arch/mn10300/Kconfig
index 428da175d073,10607f0d2bcd..000000000000
--- a/arch/mn10300/Kconfig
+++ b/arch/mn10300/Kconfig
@@@ -13,6 -13,8 +13,11 @@@ config MN1030
  	select MODULES_USE_ELF_RELA
  	select OLD_SIGSUSPEND3
  	select OLD_SIGACTION
++<<<<<<< HEAD
++=======
+ 	select HAVE_DEBUG_STACKOVERFLOW
+ 	select ARCH_NO_COHERENT_DMA_MMAP
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  config AM33_2
  	def_bool n
diff --cc arch/mn10300/include/asm/dma-mapping.h
index a18abfc558eb,1dcd44757f32..000000000000
--- a/arch/mn10300/include/asm/dma-mapping.h
+++ b/arch/mn10300/include/asm/dma-mapping.h
@@@ -168,19 -28,4 +168,22 @@@ void dma_cache_sync(void *vaddr, size_
  	mn10300_dcache_flush_inv();
  }
  
++<<<<<<< HEAD
 +/* Not supported for now */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
 +{
 +	return -EINVAL;
 +}
 +
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
 +{
 +	return -EINVAL;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif
diff --cc arch/openrisc/Kconfig
index 1072bfd18c50,e118c02cc79a..000000000000
--- a/arch/openrisc/Kconfig
+++ b/arch/openrisc/Kconfig
@@@ -26,12 -29,6 +26,15 @@@ config OPENRIS
  config MMU
  	def_bool y
  
++<<<<<<< HEAD
 +config HAVE_DMA_ATTRS
 +	def_bool y
 +
 +config UID16
 +	def_bool y
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  config RWSEM_GENERIC_SPINLOCK
  	def_bool y
  
diff --cc arch/openrisc/include/asm/dma-mapping.h
index fab8628e1b6e,1f260bccb368..000000000000
--- a/arch/openrisc/include/asm/dma-mapping.h
+++ b/arch/openrisc/include/asm/dma-mapping.h
@@@ -93,18 -42,4 +93,21 @@@ static inline int dma_supported(struct 
  	return dma_mask == DMA_BIT_MASK(32);
  }
  
++<<<<<<< HEAD
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 +{
 +	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = dma_mask;
 +
 +	return 0;
 +}
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif	/* __ASM_OPENRISC_DMA_MAPPING_H */
diff --cc arch/parisc/Kconfig
index 6507dabdd5dd,14f655cf542e..000000000000
--- a/arch/parisc/Kconfig
+++ b/arch/parisc/Kconfig
@@@ -27,6 -27,9 +27,12 @@@ config PARIS
  	select MODULES_USE_ELF_RELA
  	select CLONE_BACKWARDS
  	select TTY # Needed for pdc_cons.c
++<<<<<<< HEAD
++=======
+ 	select HAVE_DEBUG_STACKOVERFLOW
+ 	select HAVE_ARCH_AUDITSYSCALL
+ 	select ARCH_NO_COHERENT_DMA_MMAP
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  	help
  	  The PA-RISC microprocessor is designed by Hewlett-Packard and used
diff --cc arch/parisc/include/asm/dma-mapping.h
index d0eae5f2bd87,16e024602737..000000000000
--- a/arch/parisc/include/asm/dma-mapping.h
+++ b/arch/parisc/include/asm/dma-mapping.h
@@@ -238,22 -83,4 +238,25 @@@ struct parisc_device
  void * sba_get_iommu(struct parisc_device *dev);
  #endif
  
++<<<<<<< HEAD
 +/* At the moment, we panic on error for IOMMU resource exaustion */
 +#define dma_mapping_error(dev, x)	0
 +
 +/* This API cannot be supported on PA-RISC */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
 +{
 +	return -EINVAL;
 +}
 +
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
 +{
 +	return -EINVAL;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif
diff --cc arch/powerpc/include/asm/dma-mapping.h
index c6f39b7c9f40,77816acd4fd9..000000000000
--- a/arch/powerpc/include/asm/dma-mapping.h
+++ b/arch/powerpc/include/asm/dma-mapping.h
@@@ -120,20 -122,9 +120,26 @@@ static inline void set_dma_offset(struc
  /* this will be removed soon */
  #define flush_write_buffers()
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *dma_ops = get_dma_ops(dev);
 +
 +	if (unlikely(dma_ops == NULL))
 +		return 0;
 +	if (dma_ops->dma_supported == NULL)
 +		return 1;
 +	return dma_ops->dma_supported(dev, mask);
 +}
 +
 +extern int dma_set_mask(struct device *dev, u64 dma_mask);
++=======
+ #define HAVE_ARCH_DMA_SET_MASK 1
+ extern int dma_set_mask(struct device *dev, u64 dma_mask);
+ 
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  extern int __dma_set_mask(struct device *dev, u64 dma_mask);
  extern u64 __dma_get_required_mask(struct device *dev);
  
diff --cc arch/s390/Kconfig
index 03147cc01035,3be9c832dec1..000000000000
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@@ -530,9 -579,8 +530,12 @@@ config QDI
  
  menuconfig PCI
  	bool "PCI support"
++<<<<<<< HEAD
 +	depends on 64BIT
 +	select HAVE_DMA_ATTRS
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select PCI_MSI
 -	select IOMMU_SUPPORT
  	help
  	  Enable PCI support.
  
diff --cc arch/s390/include/asm/dma-mapping.h
index 3f2d5669375a,e64bfcb9702f..000000000000
--- a/arch/s390/include/asm/dma-mapping.h
+++ b/arch/s390/include/asm/dma-mapping.h
@@@ -27,20 -23,6 +27,23 @@@ static inline void dma_cache_sync(struc
  {
  }
  
++<<<<<<< HEAD
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *dma_ops = get_dma_ops(dev);
 +
 +	if (dma_ops->dma_supported == NULL)
 +		return 1;
 +	return dma_ops->dma_supported(dev, mask);
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
  {
  	if (!dev->dma_mask)
diff --cc arch/sh/include/asm/dma-mapping.h
index b437f2c780b8,e11cf0c8206b..000000000000
--- a/arch/sh/include/asm/dma-mapping.h
+++ b/arch/sh/include/asm/dma-mapping.h
@@@ -9,32 -9,7 +9,36 @@@ static inline struct dma_map_ops *get_d
  	return dma_ops;
  }
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-coherent.h>
 +#include <asm-generic/dma-mapping-common.h>
 +
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	if (ops->dma_supported)
 +		return ops->dma_supported(dev, mask);
 +
 +	return 1;
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +	if (ops->set_dma_mask)
 +		return ops->set_dma_mask(dev, mask);
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
++=======
+ #define DMA_ERROR_CODE 0
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
  		    enum dma_data_direction dir);
diff --cc arch/sparc/Kconfig
index 9ac9f1666339,57ffaf285c2f..000000000000
--- a/arch/sparc/Kconfig
+++ b/arch/sparc/Kconfig
@@@ -23,13 -25,11 +23,17 @@@ config SPAR
  	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
  	select RTC_CLASS
  	select RTC_DRV_M48T59
++<<<<<<< HEAD
 +	select HAVE_DMA_ATTRS
++=======
+ 	select RTC_SYSTOHC
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select HAVE_DMA_API_DEBUG
 -	select HAVE_ARCH_JUMP_LABEL if SPARC64
 +	select HAVE_ARCH_JUMP_LABEL
 +	select HAVE_GENERIC_HARDIRQS
  	select GENERIC_IRQ_SHOW
  	select ARCH_WANT_IPC_PARSE_VERSION
 +	select USE_GENERIC_SMP_HELPERS if SMP
  	select GENERIC_PCI_IOMAP
  	select HAVE_NMI_WATCHDOG if SPARC64
  	select HAVE_BPF_JIT
diff --cc arch/sparc/include/asm/dma-mapping.h
index 05fe53f5346e,1180ae254154..000000000000
--- a/arch/sparc/include/asm/dma-mapping.h
+++ b/arch/sparc/include/asm/dma-mapping.h
@@@ -29,51 -37,4 +29,54 @@@ static inline struct dma_map_ops *get_d
  	return dma_ops;
  }
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-common.h>
 +
 +#define dma_alloc_coherent(d,s,h,f)	dma_alloc_attrs(d,s,h,f,NULL)
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				    dma_addr_t *dma_handle, gfp_t flag,
 +				    struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	void *cpu_addr;
 +
 +	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
 +	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
 +	return cpu_addr;
 +}
 +
 +#define dma_free_coherent(d,s,c,h) dma_free_attrs(d,s,c,h,NULL)
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				  void *cpu_addr, dma_addr_t dma_handle,
 +				  struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +
 +	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
 +	ops->free(dev, size, cpu_addr, dma_handle, attrs);
 +}
 +
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	debug_dma_mapping_error(dev, dma_addr);
 +	return (dma_addr == DMA_ERROR_CODE);
 +}
 +
 +static inline int dma_set_mask(struct device *dev, u64 mask)
 +{
 +#ifdef CONFIG_PCI
 +	if (dev->bus == &pci_bus_type) {
 +		if (!dev->dma_mask || !dma_supported(dev, mask))
 +			return -EINVAL;
 +		*dev->dma_mask = mask;
 +		return 0;
 +	}
 +#endif
 +	return -EINVAL;
 +}
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  #endif
diff --cc arch/tile/Kconfig
index 3aa37669ff8c,de4a4fff9323..000000000000
--- a/arch/tile/Kconfig
+++ b/arch/tile/Kconfig
@@@ -3,7 -3,8 +3,12 @@@
  
  config TILE
  	def_bool y
++<<<<<<< HEAD
 +	select HAVE_DMA_ATTRS
++=======
+ 	select HAVE_PERF_EVENTS
+ 	select USE_PMC if PERF_EVENTS
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select HAVE_DMA_API_DEBUG
  	select HAVE_KVM if !TILEGX
  	select GENERIC_FIND_FIRST_BIT
diff --cc arch/tile/include/asm/dma-mapping.h
index f2ff191376b4,01ceb4a895b0..000000000000
--- a/arch/tile/include/asm/dma-mapping.h
+++ b/arch/tile/include/asm/dma-mapping.h
@@@ -69,69 -72,8 +69,74 @@@ static inline bool dma_capable(struct d
  	return addr + size - 1 <= *dev->dma_mask;
  }
  
++<<<<<<< HEAD
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	debug_dma_mapping_error(dev, dma_addr);
 +	return get_dma_ops(dev)->mapping_error(dev, dma_addr);
 +}
 +
 +static inline int
 +dma_supported(struct device *dev, u64 mask)
 +{
 +	return get_dma_ops(dev)->dma_supported(dev, mask);
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *dma_ops = get_dma_ops(dev);
 +
 +	/* Handle legacy PCI devices with limited memory addressability. */
 +	if ((dma_ops == gx_pci_dma_map_ops) && (mask <= DMA_BIT_MASK(32))) {
 +		set_dma_ops(dev, gx_legacy_pci_dma_map_ops);
 +		set_dma_offset(dev, 0);
 +		if (mask > dev->archdata.max_direct_dma_addr)
 +			mask = dev->archdata.max_direct_dma_addr;
 +	}
 +
 +	if (!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
 +
 +static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 +				    dma_addr_t *dma_handle, gfp_t flag,
 +				    struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *dma_ops = get_dma_ops(dev);
 +	void *cpu_addr;
 +
 +	cpu_addr = dma_ops->alloc(dev, size, dma_handle, flag, attrs);
 +
 +	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
 +
 +	return cpu_addr;
 +}
 +
 +static inline void dma_free_attrs(struct device *dev, size_t size,
 +				  void *cpu_addr, dma_addr_t dma_handle,
 +				  struct dma_attrs *attrs)
 +{
 +	struct dma_map_ops *dma_ops = get_dma_ops(dev);
 +
 +	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
 +
 +	dma_ops->free(dev, size, cpu_addr, dma_handle, attrs);
 +}
 +
 +#define dma_alloc_coherent(d, s, h, f) dma_alloc_attrs(d, s, h, f, NULL)
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_attrs(d, s, h, f, NULL)
 +#define dma_free_coherent(d, s, v, h) dma_free_attrs(d, s, v, h, NULL)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_attrs(d, s, v, h, NULL)
++=======
+ #define HAVE_ARCH_DMA_SET_MASK 1
+ int dma_set_mask(struct device *dev, u64 mask);
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  /*
   * dma_alloc_noncoherent() is #defined to return coherent memory,
diff --cc arch/unicore32/Kconfig
index 41bcc0013442,e5602ee9c610..000000000000
--- a/arch/unicore32/Kconfig
+++ b/arch/unicore32/Kconfig
@@@ -1,9 -1,10 +1,12 @@@
  config UNICORE32
  	def_bool y
 -	select ARCH_HAS_DEVMEM_IS_ALLOWED
 -	select ARCH_MIGHT_HAVE_PC_PARPORT
 -	select ARCH_MIGHT_HAVE_PC_SERIO
  	select HAVE_MEMBLOCK
  	select HAVE_GENERIC_DMA_COHERENT
++<<<<<<< HEAD
 +	select HAVE_GENERIC_HARDIRQS
 +	select HAVE_DMA_ATTRS
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select HAVE_KERNEL_GZIP
  	select HAVE_KERNEL_BZIP2
  	select GENERIC_ATOMIC64
diff --cc arch/unicore32/include/asm/dma-mapping.h
index 366460a81796,4749854afd03..000000000000
--- a/arch/unicore32/include/asm/dma-mapping.h
+++ b/arch/unicore32/include/asm/dma-mapping.h
@@@ -30,28 -28,6 +30,31 @@@ static inline struct dma_map_ops *get_d
  	return &swiotlb_dma_map_ops;
  }
  
++<<<<<<< HEAD
 +static inline int dma_supported(struct device *dev, u64 mask)
 +{
 +	struct dma_map_ops *dma_ops = get_dma_ops(dev);
 +
 +	if (unlikely(dma_ops == NULL))
 +		return 0;
 +
 +	return dma_ops->dma_supported(dev, mask);
 +}
 +
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	struct dma_map_ops *dma_ops = get_dma_ops(dev);
 +
 +	if (dma_ops->mapping_error)
 +		return dma_ops->mapping_error(dev, dma_addr);
 +
 +	return 0;
 +}
 +
 +#include <asm-generic/dma-mapping-common.h>
 +
++=======
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
  {
  	if (dev && dev->dma_mask)
diff --cc arch/x86/Kconfig
index c83b7a921031,89159a6fa503..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -21,60 -17,122 +21,144 @@@ config X86_6
  ### Arch settings
  config X86
  	def_bool y
 -	select ACPI_LEGACY_TABLES_LOOKUP	if ACPI
 -	select ACPI_SYSTEM_POWER_STATES_SUPPORT	if ACPI
 -	select ANON_INODES
 -	select ARCH_CLOCKSOURCE_DATA
 -	select ARCH_DISCARD_MEMBLOCK
 -	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
  	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
 -	select ARCH_HAS_DEVMEM_IS_ALLOWED
 -	select ARCH_HAS_ELF_RANDOMIZE
 -	select ARCH_HAS_FAST_MULTIPLIER
 -	select ARCH_HAS_GCOV_PROFILE_ALL
  	select ARCH_HAS_PMEM_API		if X86_64
  	select ARCH_HAS_MMIO_FLUSH
++<<<<<<< HEAD
 +	select HAVE_AOUT if X86_32
 +	select HAVE_UNSTABLE_SCHED_CLOCK
 +	select ARCH_SUPPORTS_NUMA_BALANCING
 +	select ARCH_SUPPORTS_INT128 if X86_64
 +	select ARCH_WANTS_PROT_NUMA_PROT_NONE
++=======
+ 	select ARCH_HAS_SG_CHAIN
+ 	select ARCH_HAS_UBSAN_SANITIZE_ALL
+ 	select ARCH_HAVE_NMI_SAFE_CMPXCHG
+ 	select ARCH_MIGHT_HAVE_ACPI_PDC		if ACPI
+ 	select ARCH_MIGHT_HAVE_PC_PARPORT
+ 	select ARCH_MIGHT_HAVE_PC_SERIO
+ 	select ARCH_SUPPORTS_ATOMIC_RMW
+ 	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
+ 	select ARCH_SUPPORTS_INT128		if X86_64
+ 	select ARCH_SUPPORTS_NUMA_BALANCING	if X86_64
+ 	select ARCH_USE_BUILTIN_BSWAP
+ 	select ARCH_USE_CMPXCHG_LOCKREF		if X86_64
+ 	select ARCH_USE_QUEUED_RWLOCKS
+ 	select ARCH_USE_QUEUED_SPINLOCKS
+ 	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
+ 	select ARCH_WANTS_DYNAMIC_TASK_STRUCT
+ 	select ARCH_WANT_FRAME_POINTERS
+ 	select ARCH_WANT_IPC_PARSE_VERSION	if X86_32
+ 	select ARCH_WANT_OPTIONAL_GPIOLIB
+ 	select BUILDTIME_EXTABLE_SORT
+ 	select CLKEVT_I8253
+ 	select CLKSRC_I8253			if X86_32
+ 	select CLOCKSOURCE_VALIDATE_LAST_CYCLE
+ 	select CLOCKSOURCE_WATCHDOG
+ 	select CLONE_BACKWARDS			if X86_32
+ 	select COMPAT_OLD_SIGACTION		if IA32_EMULATION
+ 	select DCACHE_WORD_ACCESS
+ 	select EDAC_ATOMIC_SCRUB
+ 	select EDAC_SUPPORT
+ 	select GENERIC_CLOCKEVENTS
+ 	select GENERIC_CLOCKEVENTS_BROADCAST	if X86_64 || (X86_32 && X86_LOCAL_APIC)
+ 	select GENERIC_CLOCKEVENTS_MIN_ADJUST
+ 	select GENERIC_CMOS_UPDATE
+ 	select GENERIC_CPU_AUTOPROBE
+ 	select GENERIC_EARLY_IOREMAP
+ 	select GENERIC_FIND_FIRST_BIT
+ 	select GENERIC_IOMAP
+ 	select GENERIC_IRQ_PROBE
+ 	select GENERIC_IRQ_SHOW
+ 	select GENERIC_PENDING_IRQ		if SMP
+ 	select GENERIC_SMP_IDLE_THREAD
+ 	select GENERIC_STRNCPY_FROM_USER
+ 	select GENERIC_STRNLEN_USER
+ 	select GENERIC_TIME_VSYSCALL
+ 	select HAVE_ACPI_APEI			if ACPI
+ 	select HAVE_ACPI_APEI_NMI		if ACPI
+ 	select HAVE_ALIGNED_STRUCT_PAGE		if SLUB
+ 	select HAVE_AOUT			if X86_32
+ 	select HAVE_ARCH_AUDITSYSCALL
+ 	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
+ 	select HAVE_ARCH_JUMP_LABEL
+ 	select HAVE_ARCH_KASAN			if X86_64 && SPARSEMEM_VMEMMAP
+ 	select HAVE_ARCH_KGDB
+ 	select HAVE_ARCH_KMEMCHECK
+ 	select HAVE_ARCH_MMAP_RND_BITS		if MMU
+ 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
+ 	select HAVE_ARCH_SECCOMP_FILTER
+ 	select HAVE_ARCH_SOFT_DIRTY		if X86_64
+ 	select HAVE_ARCH_TRACEHOOK
+ 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
+ 	select HAVE_BPF_JIT			if X86_64
+ 	select HAVE_CC_STACKPROTECTOR
+ 	select HAVE_CMPXCHG_DOUBLE
+ 	select HAVE_CMPXCHG_LOCAL
+ 	select HAVE_CONTEXT_TRACKING		if X86_64
+ 	select HAVE_COPY_THREAD_TLS
+ 	select HAVE_C_RECORDMCOUNT
+ 	select HAVE_DEBUG_KMEMLEAK
+ 	select HAVE_DEBUG_STACKOVERFLOW
+ 	select HAVE_DMA_API_DEBUG
+ 	select HAVE_DMA_CONTIGUOUS
+ 	select HAVE_DYNAMIC_FTRACE
+ 	select HAVE_DYNAMIC_FTRACE_WITH_REGS
+ 	select HAVE_EFFICIENT_UNALIGNED_ACCESS
+ 	select HAVE_FENTRY			if X86_64
+ 	select HAVE_FTRACE_MCOUNT_RECORD
+ 	select HAVE_FUNCTION_GRAPH_FP_TEST
+ 	select HAVE_FUNCTION_GRAPH_TRACER
+ 	select HAVE_FUNCTION_TRACER
+ 	select HAVE_GENERIC_DMA_COHERENT	if X86_32
+ 	select HAVE_HW_BREAKPOINT
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select HAVE_IDE
 +	select HAVE_OPROFILE
 +	select HAVE_PCSPKR_PLATFORM
 +	select HAVE_PERF_EVENTS
  	select HAVE_IOREMAP_PROT
 -	select HAVE_IRQ_EXIT_ON_IRQ_STACK	if X86_64
 -	select HAVE_IRQ_TIME_ACCOUNTING
 -	select HAVE_KERNEL_BZIP2
 -	select HAVE_KERNEL_GZIP
 -	select HAVE_KERNEL_LZ4
 -	select HAVE_KERNEL_LZMA
 -	select HAVE_KERNEL_LZO
 -	select HAVE_KERNEL_XZ
  	select HAVE_KPROBES
 -	select HAVE_KPROBES_ON_FTRACE
 -	select HAVE_KRETPROBES
 -	select HAVE_KVM
 -	select HAVE_LIVEPATCH			if X86_64
  	select HAVE_MEMBLOCK
  	select HAVE_MEMBLOCK_NODE_MAP
 -	select HAVE_MIXED_BREAKPOINTS_REGS
 -	select HAVE_OPROFILE
 +	select ARCH_DISCARD_MEMBLOCK
 +	select ARCH_SUPPORTS_DEFERRED_STRUCT_PAGE_INIT
 +	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH if SMP
 +	select ARCH_WANT_OPTIONAL_GPIOLIB
 +	select ARCH_WANT_FRAME_POINTERS
 +	select HAVE_DMA_ATTRS
 +	select HAVE_DMA_CONTIGUOUS if !SWIOTLB
 +	select HAVE_KRETPROBES
  	select HAVE_OPTPROBES
 -	select HAVE_PCSPKR_PLATFORM
 -	select HAVE_PERF_EVENTS
 +	select HAVE_KPROBES_ON_FTRACE
 +	select HAVE_FTRACE_MCOUNT_RECORD
 +	select HAVE_FENTRY if X86_64
 +	select HAVE_ARCH_MMAP_RND_BITS		if MMU
 +	select HAVE_ARCH_MMAP_RND_COMPAT_BITS	if MMU && COMPAT
 +	select HAVE_C_RECORDMCOUNT
 +	select HAVE_DYNAMIC_FTRACE
 +	select HAVE_DYNAMIC_FTRACE_WITH_REGS
 +	select HAVE_FUNCTION_TRACER
 +	select HAVE_FUNCTION_GRAPH_TRACER
 +	select HAVE_FUNCTION_GRAPH_FP_TEST
 +	select HAVE_SYSCALL_TRACEPOINTS
 +	select SYSCTL_EXCEPTION_TRACE
 +	select HAVE_KVM
 +	select HAVE_ARCH_KGDB
 +	select HAVE_ARCH_TRACEHOOK
 +	select HAVE_GENERIC_DMA_COHERENT if X86_32
 +	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 +	select USER_STACKTRACE_SUPPORT
 +	select HAVE_REGS_AND_STACK_ACCESS_API
 +	select HAVE_DMA_API_DEBUG
 +	select HAVE_KERNEL_GZIP
 +	select HAVE_KERNEL_BZIP2
 +	select HAVE_KERNEL_LZMA
 +	select HAVE_KERNEL_XZ
 +	select HAVE_KERNEL_LZO
 +	select HAVE_HW_BREAKPOINT
 +	select HAVE_MIXED_BREAKPOINTS_REGS
 +	select PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
diff --cc arch/x86/include/asm/dma-mapping.h
index 1f5b7287d1ad,3a27b93e6261..000000000000
--- a/arch/x86/include/asm/dma-mapping.h
+++ b/arch/x86/include/asm/dma-mapping.h
@@@ -41,24 -40,11 +41,32 @@@ static inline struct dma_map_ops *get_d
  #endif
  }
  
++<<<<<<< HEAD
 +#include <asm-generic/dma-mapping-common.h>
 +
 +/* Make sure we keep the same behaviour */
 +static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	struct dma_map_ops *ops = get_dma_ops(dev);
 +	debug_dma_mapping_error(dev, dma_addr);
 +	if (ops->mapping_error)
 +		return ops->mapping_error(dev, dma_addr);
 +
 +	return (dma_addr == DMA_ERROR_CODE);
 +}
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +extern int dma_supported(struct device *hwdev, u64 mask);
 +extern int dma_set_mask(struct device *dev, u64 mask);
++=======
+ bool arch_dma_alloc_attrs(struct device **dev, gfp_t *gfp);
+ #define arch_dma_alloc_attrs arch_dma_alloc_attrs
+ 
+ #define HAVE_ARCH_DMA_SUPPORTED 1
+ extern int dma_supported(struct device *hwdev, u64 mask);
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  extern void *dma_generic_alloc_coherent(struct device *dev, size_t size,
  					dma_addr_t *dma_addr, gfp_t flag,
diff --cc arch/xtensa/Kconfig
index 0a1b95f81a32,e9df1567d778..000000000000
--- a/arch/xtensa/Kconfig
+++ b/arch/xtensa/Kconfig
@@@ -4,19 -4,26 +4,34 @@@ config ZONE_DM
  config XTENSA
  	def_bool y
  	select ARCH_WANT_FRAME_POINTERS
 +	select HAVE_IDE
 +	select GENERIC_ATOMIC64
 +	select HAVE_GENERIC_HARDIRQS
 +	select VIRT_TO_BUS
 +	select GENERIC_IRQ_SHOW
 +	select GENERIC_CPU_DEVICES
 +	select MODULES_USE_ELF_RELA
 +	select GENERIC_PCI_IOMAP
  	select ARCH_WANT_IPC_PARSE_VERSION
  	select ARCH_WANT_OPTIONAL_GPIOLIB
 -	select BUILDTIME_EXTABLE_SORT
  	select CLONE_BACKWARDS
++<<<<<<< HEAD
++=======
+ 	select COMMON_CLK
+ 	select GENERIC_ATOMIC64
+ 	select GENERIC_CLOCKEVENTS
+ 	select GENERIC_IRQ_SHOW
+ 	select GENERIC_PCI_IOMAP
+ 	select GENERIC_SCHED_CLOCK
+ 	select HAVE_DMA_API_DEBUG
+ 	select HAVE_FUNCTION_TRACER
+ 	select HAVE_FUTEX_CMPXCHG if !MMU
+ 	select HAVE_IRQ_TIME_ACCOUNTING
+ 	select HAVE_OPROFILE
+ 	select HAVE_PERF_EVENTS
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  	select IRQ_DOMAIN
 -	select MODULES_USE_ELF_RELA
 -	select PERF_USE_VMALLOC
 -	select VIRT_TO_BUS
 +	select HAVE_OPROFILE
  	help
  	  Xtensa processors are 32-bit RISC machines designed by Tensilica
  	  primarily for embedded systems.  These processors are both
diff --cc arch/xtensa/include/asm/dma-mapping.h
index 172a02a6ad14,87b7a7dfbcf3..000000000000
--- a/arch/xtensa/include/asm/dma-mapping.h
+++ b/arch/xtensa/include/asm/dma-mapping.h
@@@ -18,171 -20,27 +18,178 @@@
  
  #define DMA_ERROR_CODE		(~(dma_addr_t)0x0)
  
 -extern struct dma_map_ops xtensa_dma_map_ops;
 +/*
 + * DMA-consistent mapping functions.
 + */
 +
 +extern void *consistent_alloc(int, size_t, dma_addr_t, unsigned long);
 +extern void consistent_free(void*, size_t, dma_addr_t);
 +extern void consistent_sync(void*, size_t, int);
 +
 +#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
 +#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
 +
 +void *dma_alloc_coherent(struct device *dev, size_t size,
 +			   dma_addr_t *dma_handle, gfp_t flag);
  
 -static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 +void dma_free_coherent(struct device *dev, size_t size,
 +			 void *vaddr, dma_addr_t dma_handle);
 +
 +static inline dma_addr_t
 +dma_map_single(struct device *dev, void *ptr, size_t size,
 +	       enum dma_data_direction direction)
  {
 -	if (dev && dev->archdata.dma_ops)
 -		return dev->archdata.dma_ops;
 -	else
 -		return &xtensa_dma_map_ops;
 +	BUG_ON(direction == DMA_NONE);
 +	consistent_sync(ptr, size, direction);
 +	return virt_to_phys(ptr);
  }
  
++<<<<<<< HEAD
 +static inline void
 +dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 +		 enum dma_data_direction direction)
++=======
+ void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
+ 		    enum dma_data_direction direction);
+ 
+ static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline int
 +dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 +	   enum dma_data_direction direction)
 +{
 +	int i;
 +
 +	BUG_ON(direction == DMA_NONE);
 +
 +	for (i = 0; i < nents; i++, sg++ ) {
 +		BUG_ON(!sg_page(sg));
 +
 +		sg->dma_address = sg_phys(sg);
 +		consistent_sync(sg_virt(sg), sg->length, direction);
 +	}
 +
 +	return nents;
 +}
 +
 +static inline dma_addr_t
 +dma_map_page(struct device *dev, struct page *page, unsigned long offset,
 +	     size_t size, enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +	return (dma_addr_t)(page_to_pfn(page)) * PAGE_SIZE + offset;
 +}
 +
 +static inline void
 +dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +
 +static inline void
 +dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
 +	     enum dma_data_direction direction)
 +{
 +	BUG_ON(direction == DMA_NONE);
 +}
 +
 +static inline void
 +dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
 +		enum dma_data_direction direction)
 +{
 +	consistent_sync((void *)bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_for_device(struct device *dev, dma_addr_t dma_handle,
 +		           size_t size, enum dma_data_direction direction)
 +{
 +	consistent_sync((void *)bus_to_virt(dma_handle), size, direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +
 +	consistent_sync((void *)bus_to_virt(dma_handle)+offset,size,direction);
 +}
 +
 +static inline void
 +dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_handle,
 +		      unsigned long offset, size_t size,
 +		      enum dma_data_direction direction)
 +{
 +
 +	consistent_sync((void *)bus_to_virt(dma_handle)+offset,size,direction);
 +}
 +static inline void
 +dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction dir)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		consistent_sync(sg_virt(sg), sg->length, dir);
 +}
 +
 +static inline void
 +dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nelems,
 +		 enum dma_data_direction dir)
 +{
 +	int i;
 +	for (i = 0; i < nelems; i++, sg++)
 +		consistent_sync(sg_virt(sg), sg->length, dir);
 +}
 +static inline int
 +dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 +{
 +	return 0;
 +}
 +
 +static inline int
 +dma_supported(struct device *dev, u64 mask)
 +{
 +	return 1;
 +}
 +
 +static inline int
 +dma_set_mask(struct device *dev, u64 mask)
 +{
 +	if(!dev->dma_mask || !dma_supported(dev, mask))
 +		return -EIO;
 +
 +	*dev->dma_mask = mask;
 +
 +	return 0;
 +}
 +
 +static inline void
 +dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 +	       enum dma_data_direction direction)
 +{
 +	consistent_sync(vaddr, size, direction);
 +}
 +
 +/* Not supported for now */
 +static inline int dma_mmap_coherent(struct device *dev,
 +				    struct vm_area_struct *vma, void *cpu_addr,
 +				    dma_addr_t dma_addr, size_t size)
  {
 -	return (dma_addr_t)paddr;
 +	return -EINVAL;
  }
  
 -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 +static inline int dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 +				  void *cpu_addr, dma_addr_t dma_addr,
 +				  size_t size)
  {
 -	return (phys_addr_t)daddr;
 +	return -EINVAL;
  }
  
  #endif	/* _XTENSA_DMA_MAPPING_H */
diff --cc drivers/media/platform/Kconfig
index 0494d2769fd7,526359447ff9..000000000000
--- a/drivers/media/platform/Kconfig
+++ b/drivers/media/platform/Kconfig
@@@ -201,9 -212,18 +201,21 @@@ config VIDEO_SAMSUNG_EXYNOS_GS
  	help
  	  This is a v4l2 driver for Samsung EXYNOS5 SoC G-Scaler.
  
++<<<<<<< HEAD
++=======
+ config VIDEO_STI_BDISP
+ 	tristate "STMicroelectronics BDISP 2D blitter driver"
+ 	depends on VIDEO_DEV && VIDEO_V4L2
+ 	depends on ARCH_STI || COMPILE_TEST
+ 	select VIDEOBUF2_DMA_CONTIG
+ 	select V4L2_MEM2MEM_DEV
+ 	help
+ 	  This v4l2 mem2mem driver is a 2D blitter for STMicroelectronics SoC.
+ 
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  config VIDEO_SH_VEU
  	tristate "SuperH VEU mem2mem video processing driver"
 -	depends on VIDEO_DEV && VIDEO_V4L2 && HAS_DMA
 +	depends on VIDEO_DEV && VIDEO_V4L2 && GENERIC_HARDIRQS
  	select VIDEOBUF2_DMA_CONTIG
  	select V4L2_MEM2MEM_DEV
  	help
diff --cc include/linux/dma-mapping.h
index eb18e776e914,cc0517b71c5e..000000000000
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@@ -251,22 -619,31 +611,50 @@@ static inline void dmam_release_declare
  }
  #endif /* ARCH_HAS_DMA_DECLARE_COHERENT_MEMORY */
  
++<<<<<<< HEAD
 +#ifndef CONFIG_HAVE_DMA_ATTRS
 +struct dma_attrs;
 +
 +#define dma_map_single_attrs(dev, cpu_addr, size, dir, attrs) \
 +	dma_map_single(dev, cpu_addr, size, dir)
 +
 +#define dma_unmap_single_attrs(dev, dma_addr, size, dir, attrs) \
 +	dma_unmap_single(dev, dma_addr, size, dir)
 +
 +#define dma_map_sg_attrs(dev, sgl, nents, dir, attrs) \
 +	dma_map_sg(dev, sgl, nents, dir)
 +
 +#define dma_unmap_sg_attrs(dev, sgl, nents, dir, attrs) \
 +	dma_unmap_sg(dev, sgl, nents, dir)
 +
 +#endif /* CONFIG_HAVE_DMA_ATTRS */
++=======
+ static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
+ 					   dma_addr_t *dma_addr, gfp_t gfp)
+ {
+ 	DEFINE_DMA_ATTRS(attrs);
+ 	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+ 	return dma_alloc_attrs(dev, size, dma_addr, gfp, &attrs);
+ }
+ 
+ static inline void dma_free_writecombine(struct device *dev, size_t size,
+ 					 void *cpu_addr, dma_addr_t dma_addr)
+ {
+ 	DEFINE_DMA_ATTRS(attrs);
+ 	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+ 	return dma_free_attrs(dev, size, cpu_addr, dma_addr, &attrs);
+ }
+ 
+ static inline int dma_mmap_writecombine(struct device *dev,
+ 					struct vm_area_struct *vma,
+ 					void *cpu_addr, dma_addr_t dma_addr,
+ 					size_t size)
+ {
+ 	DEFINE_DMA_ATTRS(attrs);
+ 	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+ 	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size, &attrs);
+ }
++>>>>>>> e1c7e324539a (dma-mapping: always provide the dma_map_ops based implementation)
  
  #ifdef CONFIG_NEED_DMA_MAP_STATE
  #define DEFINE_DMA_UNMAP_ADDR(ADDR_NAME)        dma_addr_t ADDR_NAME
* Unmerged path arch/h8300/include/asm/dma-mapping.h
* Unmerged path arch/nios2/Kconfig
* Unmerged path drivers/gpu/drm/imx/Kconfig
* Unmerged path drivers/gpu/drm/rcar-du/Kconfig
* Unmerged path drivers/gpu/drm/sti/Kconfig
* Unmerged path drivers/gpu/drm/vc4/Kconfig
* Unmerged path include/asm-generic/dma-mapping-common.h
diff --git a/Documentation/DMA-API-HOWTO.txt b/Documentation/DMA-API-HOWTO.txt
index 3403fb9b7e68..dddaba7f44b3 100644
--- a/Documentation/DMA-API-HOWTO.txt
+++ b/Documentation/DMA-API-HOWTO.txt
@@ -946,16 +946,6 @@ to "Closing".
    alignment constraints (e.g. the alignment constraints about 64-bit
    objects).
 
-3) Supporting multiple types of IOMMUs
-
-   If your architecture needs to support multiple types of IOMMUs, you
-   can use include/linux/asm-generic/dma-mapping-common.h. It's a
-   library to support the DMA API with multiple types of IOMMUs. Lots
-   of architectures (x86, powerpc, sh, alpha, ia64, microblaze and
-   sparc) use it. Choose one to see how it can be used. If you need to
-   support multiple types of IOMMUs in a single system, the example of
-   x86 or powerpc helps.
-
 			   Closing
 
 This document, and the API itself, would not be in its current
diff --git a/arch/Kconfig b/arch/Kconfig
index 4fa0b0631a54..b2927ad75153 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -208,9 +208,6 @@ config HAVE_NMI_WATCHDOG
 config HAVE_ARCH_TRACEHOOK
 	bool
 
-config HAVE_DMA_ATTRS
-	bool
-
 config HAVE_DMA_CONTIGUOUS
 	bool
 
* Unmerged path arch/alpha/Kconfig
* Unmerged path arch/alpha/include/asm/dma-mapping.h
* Unmerged path arch/arc/Kconfig
* Unmerged path arch/arc/include/asm/dma-mapping.h
diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index d1387fc4470a..fbb42b6d698d 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -28,7 +28,6 @@ config ARM
 	select HAVE_CC_STACKPROTECTOR
 	select HAVE_DEBUG_KMEMLEAK
 	select HAVE_DMA_API_DEBUG
-	select HAVE_DMA_ATTRS
 	select HAVE_DMA_CONTIGUOUS if MMU
 	select HAVE_DYNAMIC_FTRACE if (!XIP_KERNEL)
 	select HAVE_FTRACE_MCOUNT_RECORD if (!XIP_KERNEL)
* Unmerged path arch/arm/include/asm/dma-mapping.h
* Unmerged path arch/arm64/Kconfig
diff --git a/arch/arm64/include/asm/dma-mapping.h b/arch/arm64/include/asm/dma-mapping.h
index 994776894198..3fe88d735f97 100644
--- a/arch/arm64/include/asm/dma-mapping.h
+++ b/arch/arm64/include/asm/dma-mapping.h
@@ -35,8 +35,6 @@ static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 		return dev->archdata.dma_ops;
 }
 
-#include <asm-generic/dma-mapping-common.h>
-
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
 	return (dma_addr_t)paddr;
* Unmerged path arch/avr32/include/asm/dma-mapping.h
* Unmerged path arch/blackfin/include/asm/dma-mapping.h
* Unmerged path arch/c6x/Kconfig
* Unmerged path arch/c6x/include/asm/dma-mapping.h
* Unmerged path arch/cris/include/asm/dma-mapping.h
* Unmerged path arch/frv/Kconfig
* Unmerged path arch/frv/include/asm/dma-mapping.h
* Unmerged path arch/h8300/Kconfig
* Unmerged path arch/h8300/include/asm/dma-mapping.h
diff --git a/arch/hexagon/include/asm/dma-mapping.h b/arch/hexagon/include/asm/dma-mapping.h
index 85e9935660cb..8676b019752b 100644
--- a/arch/hexagon/include/asm/dma-mapping.h
+++ b/arch/hexagon/include/asm/dma-mapping.h
@@ -52,8 +52,6 @@ extern int dma_is_consistent(struct device *dev, dma_addr_t dma_handle);
 extern void dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 			   enum dma_data_direction direction);
 
-#include <asm-generic/dma-mapping-common.h>
-
 static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 {
 	if (!dev->dma_mask)
* Unmerged path arch/ia64/Kconfig
* Unmerged path arch/ia64/include/asm/dma-mapping.h
* Unmerged path arch/m68k/include/asm/dma-mapping.h
* Unmerged path arch/metag/include/asm/dma-mapping.h
* Unmerged path arch/microblaze/Kconfig
diff --git a/arch/microblaze/include/asm/dma-mapping.h b/arch/microblaze/include/asm/dma-mapping.h
index 46460f1c49c4..3f5d3acac87d 100644
--- a/arch/microblaze/include/asm/dma-mapping.h
+++ b/arch/microblaze/include/asm/dma-mapping.h
@@ -93,8 +93,6 @@ static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 	return 0;
 }
 
-#include <asm-generic/dma-mapping-common.h>
-
 static inline void __dma_sync(unsigned long paddr,
 			      size_t size, enum dma_data_direction direction)
 {
* Unmerged path arch/mips/Kconfig
* Unmerged path arch/mips/include/asm/dma-mapping.h
* Unmerged path arch/mn10300/Kconfig
* Unmerged path arch/mn10300/include/asm/dma-mapping.h
* Unmerged path arch/nios2/Kconfig
* Unmerged path arch/openrisc/Kconfig
* Unmerged path arch/openrisc/include/asm/dma-mapping.h
* Unmerged path arch/parisc/Kconfig
* Unmerged path arch/parisc/include/asm/dma-mapping.h
diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig
index cae2cf0c3760..049dc232c6b9 100644
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@ -149,7 +149,6 @@ config PPC
 	select HAVE_ARCH_TRACEHOOK
 	select HAVE_MEMBLOCK
 	select HAVE_MEMBLOCK_NODE_MAP
-	select HAVE_DMA_ATTRS
 	select HAVE_DMA_API_DEBUG
 	select USE_GENERIC_SMP_HELPERS if SMP
 	select HAVE_OPROFILE
* Unmerged path arch/powerpc/include/asm/dma-mapping.h
* Unmerged path arch/s390/Kconfig
* Unmerged path arch/s390/include/asm/dma-mapping.h
diff --git a/arch/sh/Kconfig b/arch/sh/Kconfig
index ff90bc498893..ef6ad107ee7d 100644
--- a/arch/sh/Kconfig
+++ b/arch/sh/Kconfig
@@ -10,7 +10,6 @@ config SUPERH
 	select HAVE_GENERIC_DMA_COHERENT
 	select HAVE_ARCH_TRACEHOOK
 	select HAVE_DMA_API_DEBUG
-	select HAVE_DMA_ATTRS
 	select HAVE_PERF_EVENTS
 	select HAVE_DEBUG_BUGVERBOSE
 	select ARCH_HAVE_CUSTOM_GPIO_H
* Unmerged path arch/sh/include/asm/dma-mapping.h
* Unmerged path arch/sparc/Kconfig
* Unmerged path arch/sparc/include/asm/dma-mapping.h
* Unmerged path arch/tile/Kconfig
* Unmerged path arch/tile/include/asm/dma-mapping.h
* Unmerged path arch/unicore32/Kconfig
* Unmerged path arch/unicore32/include/asm/dma-mapping.h
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/include/asm/dma-mapping.h
* Unmerged path arch/xtensa/Kconfig
* Unmerged path arch/xtensa/include/asm/dma-mapping.h
* Unmerged path drivers/gpu/drm/imx/Kconfig
* Unmerged path drivers/gpu/drm/rcar-du/Kconfig
diff --git a/drivers/gpu/drm/shmobile/Kconfig b/drivers/gpu/drm/shmobile/Kconfig
index b9202aa6f8ab..8d17d00ddb4b 100644
--- a/drivers/gpu/drm/shmobile/Kconfig
+++ b/drivers/gpu/drm/shmobile/Kconfig
@@ -1,6 +1,6 @@
 config DRM_SHMOBILE
 	tristate "DRM Support for SH Mobile"
-	depends on DRM && ARM && HAVE_DMA_ATTRS
+	depends on DRM && ARM
 	depends on ARCH_SHMOBILE || COMPILE_TEST
 	depends on FB_SH_MOBILE_MERAM || !FB_SH_MOBILE_MERAM
 	select BACKLIGHT_CLASS_DEVICE
* Unmerged path drivers/gpu/drm/sti/Kconfig
* Unmerged path drivers/gpu/drm/vc4/Kconfig
* Unmerged path drivers/media/platform/Kconfig
diff --git a/include/asm-generic/dma-mapping-broken.h b/include/asm-generic/dma-mapping-broken.h
deleted file mode 100644
index 6c32af918c2f..000000000000
--- a/include/asm-generic/dma-mapping-broken.h
+++ /dev/null
@@ -1,95 +0,0 @@
-#ifndef _ASM_GENERIC_DMA_MAPPING_H
-#define _ASM_GENERIC_DMA_MAPPING_H
-
-/* define the dma api to allow compilation but not linking of
- * dma dependent code.  Code that depends on the dma-mapping
- * API needs to set 'depends on HAS_DMA' in its Kconfig
- */
-
-struct scatterlist;
-
-extern void *
-dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,
-		   gfp_t flag);
-
-extern void
-dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
-		    dma_addr_t dma_handle);
-
-static inline void *dma_alloc_attrs(struct device *dev, size_t size,
-				    dma_addr_t *dma_handle, gfp_t flag,
-				    struct dma_attrs *attrs)
-{
-	/* attrs is not supported and ignored */
-	return dma_alloc_coherent(dev, size, dma_handle, flag);
-}
-
-static inline void dma_free_attrs(struct device *dev, size_t size,
-				  void *cpu_addr, dma_addr_t dma_handle,
-				  struct dma_attrs *attrs)
-{
-	/* attrs is not supported and ignored */
-	dma_free_coherent(dev, size, cpu_addr, dma_handle);
-}
-
-#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
-#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
-
-extern dma_addr_t
-dma_map_single(struct device *dev, void *ptr, size_t size,
-	       enum dma_data_direction direction);
-
-extern void
-dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
-		 enum dma_data_direction direction);
-
-extern int
-dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
-	   enum dma_data_direction direction);
-
-extern void
-dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
-	     enum dma_data_direction direction);
-
-extern dma_addr_t
-dma_map_page(struct device *dev, struct page *page, unsigned long offset,
-	     size_t size, enum dma_data_direction direction);
-
-extern void
-dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
-	       enum dma_data_direction direction);
-
-extern void
-dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle, size_t size,
-			enum dma_data_direction direction);
-
-extern void
-dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_handle,
-			      unsigned long offset, size_t size,
-			      enum dma_data_direction direction);
-
-extern void
-dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nelems,
-		    enum dma_data_direction direction);
-
-#define dma_sync_single_for_device dma_sync_single_for_cpu
-#define dma_sync_single_range_for_device dma_sync_single_range_for_cpu
-#define dma_sync_sg_for_device dma_sync_sg_for_cpu
-
-extern int
-dma_mapping_error(struct device *dev, dma_addr_t dma_addr);
-
-extern int
-dma_supported(struct device *dev, u64 mask);
-
-extern int
-dma_set_mask(struct device *dev, u64 mask);
-
-extern int
-dma_get_cache_alignment(void);
-
-extern void
-dma_cache_sync(struct device *dev, void *vaddr, size_t size,
-	       enum dma_data_direction direction);
-
-#endif /* _ASM_GENERIC_DMA_MAPPING_H */
* Unmerged path include/asm-generic/dma-mapping-common.h
diff --git a/include/linux/dma-attrs.h b/include/linux/dma-attrs.h
index c8e1831d7572..99c0be00b47c 100644
--- a/include/linux/dma-attrs.h
+++ b/include/linux/dma-attrs.h
@@ -41,7 +41,6 @@ static inline void init_dma_attrs(struct dma_attrs *attrs)
 	bitmap_zero(attrs->flags, __DMA_ATTRS_LONGS);
 }
 
-#ifdef CONFIG_HAVE_DMA_ATTRS
 /**
  * dma_set_attr - set a specific attribute
  * @attr: attribute to set
@@ -67,14 +66,5 @@ static inline int dma_get_attr(enum dma_attr attr, struct dma_attrs *attrs)
 	BUG_ON(attr >= DMA_ATTR_MAX);
 	return test_bit(attr, attrs->flags);
 }
-#else /* !CONFIG_HAVE_DMA_ATTRS */
-static inline void dma_set_attr(enum dma_attr attr, struct dma_attrs *attrs)
-{
-}
 
-static inline int dma_get_attr(enum dma_attr attr, struct dma_attrs *attrs)
-{
-	return 0;
-}
-#endif /* CONFIG_HAVE_DMA_ATTRS */
 #endif /* _DMA_ATTR_H */
* Unmerged path include/linux/dma-mapping.h
