qed/qede: Add setter APIs support for RX flow classification

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Chopra, Manish <Manish.Chopra@cavium.com>
commit 3f2a2b8b7aaadd731e688a23cbd23f7eb085c7fb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/3f2a2b8b.failed

This patch adds support for adding and deleting rx flow
classification rules. Using this user can classify RX flow
constituting of TCP/UDP 4-tuples [src_ip/dst_ip and src_port/dst_port]
to be steered on a given RX queue

	Signed-off-by: Manish Chopra <manish.chopra@cavium.com>
	Signed-off-by: Yuval Mintz <yuval.mintz@cavium.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 3f2a2b8b7aaadd731e688a23cbd23f7eb085c7fb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/qlogic/qed/qed_main.c
#	drivers/net/ethernet/qlogic/qede/qede.h
#	drivers/net/ethernet/qlogic/qede/qede_filter.c
#	drivers/net/ethernet/qlogic/qede/qede_main.c
diff --cc drivers/net/ethernet/qlogic/qed/qed_main.c
index 545e79f7504c,1bddf9372fc9..000000000000
--- a/drivers/net/ethernet/qlogic/qed/qed_main.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_main.c
@@@ -911,6 -954,7 +911,10 @@@ static int qed_slowpath_start(struct qe
  	struct qed_tunnel_info tunn_info;
  	const u8 *data = NULL;
  	struct qed_hwfn *hwfn;
++<<<<<<< HEAD
++=======
+ 	struct qed_ptt *p_ptt;
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  	int rc = -EINVAL;
  
  	if (qed_iov_wq_start(cdev))
@@@ -925,6 -969,17 +929,20 @@@
  				  QED_FW_FILE_NAME);
  			goto err;
  		}
++<<<<<<< HEAD
++=======
+ 
+ 		if (cdev->num_hwfns == 1) {
+ 			p_ptt = qed_ptt_acquire(QED_LEADING_HWFN(cdev));
+ 			if (p_ptt) {
+ 				QED_LEADING_HWFN(cdev)->p_arfs_ptt = p_ptt;
+ 			} else {
+ 				DP_NOTICE(cdev,
+ 					  "Failed to acquire PTT for aRFS\n");
+ 				goto err;
+ 			}
+ 		}
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  	}
  
  	cdev->rx_coalesce_usecs = QED_DEFAULT_RX_USECS;
@@@ -1017,6 -1087,11 +1035,14 @@@ err
  	if (IS_PF(cdev))
  		release_firmware(cdev->firmware);
  
++<<<<<<< HEAD
++=======
+ 	if (IS_PF(cdev) && (cdev->num_hwfns == 1) &&
+ 	    QED_LEADING_HWFN(cdev)->p_arfs_ptt)
+ 		qed_ptt_release(QED_LEADING_HWFN(cdev),
+ 				QED_LEADING_HWFN(cdev)->p_arfs_ptt);
+ 
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  	qed_iov_wq_stop(cdev, false);
  
  	return rc;
@@@ -1030,6 -1105,9 +1056,12 @@@ static int qed_slowpath_stop(struct qed
  	qed_ll2_dealloc_if(cdev);
  
  	if (IS_PF(cdev)) {
++<<<<<<< HEAD
++=======
+ 		if (cdev->num_hwfns == 1)
+ 			qed_ptt_release(QED_LEADING_HWFN(cdev),
+ 					QED_LEADING_HWFN(cdev)->p_arfs_ptt);
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  		qed_free_stream_mem(cdev);
  		if (IS_QED_ETH_IF(cdev))
  			qed_sriov_disable(cdev, true);
diff --cc drivers/net/ethernet/qlogic/qede/qede.h
index 01e3fd8091c0,adb700512baa..000000000000
--- a/drivers/net/ethernet/qlogic/qede/qede.h
+++ b/drivers/net/ethernet/qlogic/qede/qede.h
@@@ -406,8 -443,25 +406,30 @@@ struct qede_fastpath 
  #define QEDE_TUNN_CSUM_UNNECESSARY	BIT(2)
  
  #define QEDE_SP_RX_MODE			1
++<<<<<<< HEAD
 +#define QEDE_SP_VXLAN_PORT_CONFIG	2
 +#define QEDE_SP_GENEVE_PORT_CONFIG	3
++=======
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ int qede_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
+ 		       u16 rxq_index, u32 flow_id);
+ #define QEDE_SP_ARFS_CONFIG	4
+ #define QEDE_SP_TASK_POLL_DELAY	(5 * HZ)
+ #endif
+ 
+ void qede_process_arfs_filters(struct qede_dev *edev, bool free_fltr);
+ void qede_poll_for_freeing_arfs_filters(struct qede_dev *edev);
+ void qede_arfs_filter_op(void *dev, void *filter, u8 fw_rc);
+ void qede_free_arfs(struct qede_dev *edev);
+ int qede_alloc_arfs(struct qede_dev *edev);
+ int qede_add_cls_rule(struct qede_dev *edev, struct ethtool_rxnfc *info);
+ int qede_del_cls_rule(struct qede_dev *edev, struct ethtool_rxnfc *info);
+ int qede_get_cls_rule_entry(struct qede_dev *edev, struct ethtool_rxnfc *cmd);
+ int qede_get_cls_rule_all(struct qede_dev *edev, struct ethtool_rxnfc *info,
+ 			  u32 *rule_locs);
+ int qede_get_arfs_filter_count(struct qede_dev *edev);
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  
  struct qede_reload_args {
  	void (*func)(struct qede_dev *edev, struct qede_reload_args *args);
diff --cc drivers/net/ethernet/qlogic/qede/qede_filter.c
index 8c7b25fc9fd0,f79e36e4060a..000000000000
--- a/drivers/net/ethernet/qlogic/qede/qede_filter.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_filter.c
@@@ -38,6 -38,496 +38,499 @@@
  #include <linux/qed/qed_if.h>
  #include "qede.h"
  
++<<<<<<< HEAD
++=======
+ struct qede_arfs_tuple {
+ 	union {
+ 		__be32 src_ipv4;
+ 		struct in6_addr src_ipv6;
+ 	};
+ 	union {
+ 		__be32 dst_ipv4;
+ 		struct in6_addr dst_ipv6;
+ 	};
+ 	__be16  src_port;
+ 	__be16  dst_port;
+ 	__be16  eth_proto;
+ 	u8      ip_proto;
+ };
+ 
+ struct qede_arfs_fltr_node {
+ #define QEDE_FLTR_VALID	 0
+ 	unsigned long state;
+ 
+ 	/* pointer to aRFS packet buffer */
+ 	void *data;
+ 
+ 	/* dma map address of aRFS packet buffer */
+ 	dma_addr_t mapping;
+ 
+ 	/* length of aRFS packet buffer */
+ 	int buf_len;
+ 
+ 	/* tuples to hold from aRFS packet buffer */
+ 	struct qede_arfs_tuple tuple;
+ 
+ 	u32 flow_id;
+ 	u16 sw_id;
+ 	u16 rxq_id;
+ 	u16 next_rxq_id;
+ 	bool filter_op;
+ 	bool used;
+ 	u8 fw_rc;
+ 	struct hlist_node node;
+ };
+ 
+ struct qede_arfs {
+ #define QEDE_ARFS_BUCKET_HEAD(edev, idx) (&(edev)->arfs->arfs_hl_head[idx])
+ #define QEDE_ARFS_POLL_COUNT	100
+ #define QEDE_RFS_FLW_BITSHIFT	(4)
+ #define QEDE_RFS_FLW_MASK	((1 << QEDE_RFS_FLW_BITSHIFT) - 1)
+ 	struct hlist_head	arfs_hl_head[1 << QEDE_RFS_FLW_BITSHIFT];
+ 
+ 	/* lock for filter list access */
+ 	spinlock_t		arfs_list_lock;
+ 	unsigned long		*arfs_fltr_bmap;
+ 	int			filter_count;
+ 	bool			enable;
+ };
+ 
+ static void qede_configure_arfs_fltr(struct qede_dev *edev,
+ 				     struct qede_arfs_fltr_node *n,
+ 				     u16 rxq_id, bool add_fltr)
+ {
+ 	const struct qed_eth_ops *op = edev->ops;
+ 
+ 	if (n->used)
+ 		return;
+ 
+ 	DP_VERBOSE(edev, NETIF_MSG_RX_STATUS,
+ 		   "%s arfs filter flow_id=%d, sw_id=%d, src_port=%d, dst_port=%d, rxq=%d\n",
+ 		   add_fltr ? "Adding" : "Deleting",
+ 		   n->flow_id, n->sw_id, ntohs(n->tuple.src_port),
+ 		   ntohs(n->tuple.dst_port), rxq_id);
+ 
+ 	n->used = true;
+ 	n->filter_op = add_fltr;
+ 	op->ntuple_filter_config(edev->cdev, n, n->mapping, n->buf_len, 0,
+ 				 rxq_id, add_fltr);
+ }
+ 
+ static void
+ qede_free_arfs_filter(struct qede_dev *edev,  struct qede_arfs_fltr_node *fltr)
+ {
+ 	kfree(fltr->data);
+ 	clear_bit(fltr->sw_id, edev->arfs->arfs_fltr_bmap);
+ 	kfree(fltr);
+ }
+ 
+ static int
+ qede_enqueue_fltr_and_config_searcher(struct qede_dev *edev,
+ 				      struct qede_arfs_fltr_node *fltr,
+ 				      u16 bucket_idx)
+ {
+ 	fltr->mapping = dma_map_single(&edev->pdev->dev, fltr->data,
+ 				       fltr->buf_len, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(&edev->pdev->dev, fltr->mapping)) {
+ 		DP_NOTICE(edev, "Failed to map DMA memory for rule\n");
+ 		qede_free_arfs_filter(edev, fltr);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	INIT_HLIST_NODE(&fltr->node);
+ 	hlist_add_head(&fltr->node,
+ 		       QEDE_ARFS_BUCKET_HEAD(edev, bucket_idx));
+ 	edev->arfs->filter_count++;
+ 
+ 	if (edev->arfs->filter_count == 1 && !edev->arfs->enable) {
+ 		edev->ops->configure_arfs_searcher(edev->cdev, true);
+ 		edev->arfs->enable = true;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void
+ qede_dequeue_fltr_and_config_searcher(struct qede_dev *edev,
+ 				      struct qede_arfs_fltr_node *fltr)
+ {
+ 	hlist_del(&fltr->node);
+ 	dma_unmap_single(&edev->pdev->dev, fltr->mapping,
+ 			 fltr->buf_len, DMA_TO_DEVICE);
+ 
+ 	qede_free_arfs_filter(edev, fltr);
+ 	edev->arfs->filter_count--;
+ 
+ 	if (!edev->arfs->filter_count && edev->arfs->enable) {
+ 		edev->arfs->enable = false;
+ 		edev->ops->configure_arfs_searcher(edev->cdev, false);
+ 	}
+ }
+ 
+ void qede_arfs_filter_op(void *dev, void *filter, u8 fw_rc)
+ {
+ 	struct qede_arfs_fltr_node *fltr = filter;
+ 	struct qede_dev *edev = dev;
+ 
+ 	fltr->fw_rc = fw_rc;
+ 
+ 	if (fw_rc) {
+ 		DP_NOTICE(edev,
+ 			  "Failed arfs filter configuration fw_rc=%d, flow_id=%d, sw_id=%d, src_port=%d, dst_port=%d, rxq=%d\n",
+ 			  fw_rc, fltr->flow_id, fltr->sw_id,
+ 			  ntohs(fltr->tuple.src_port),
+ 			  ntohs(fltr->tuple.dst_port), fltr->rxq_id);
+ 
+ 		spin_lock_bh(&edev->arfs->arfs_list_lock);
+ 
+ 		fltr->used = false;
+ 		clear_bit(QEDE_FLTR_VALID, &fltr->state);
+ 
+ 		spin_unlock_bh(&edev->arfs->arfs_list_lock);
+ 		return;
+ 	}
+ 
+ 	spin_lock_bh(&edev->arfs->arfs_list_lock);
+ 
+ 	fltr->used = false;
+ 
+ 	if (fltr->filter_op) {
+ 		set_bit(QEDE_FLTR_VALID, &fltr->state);
+ 		if (fltr->rxq_id != fltr->next_rxq_id)
+ 			qede_configure_arfs_fltr(edev, fltr, fltr->rxq_id,
+ 						 false);
+ 	} else {
+ 		clear_bit(QEDE_FLTR_VALID, &fltr->state);
+ 		if (fltr->rxq_id != fltr->next_rxq_id) {
+ 			fltr->rxq_id = fltr->next_rxq_id;
+ 			qede_configure_arfs_fltr(edev, fltr,
+ 						 fltr->rxq_id, true);
+ 		}
+ 	}
+ 
+ 	spin_unlock_bh(&edev->arfs->arfs_list_lock);
+ }
+ 
+ /* Should be called while qede_lock is held */
+ void qede_process_arfs_filters(struct qede_dev *edev, bool free_fltr)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i <= QEDE_RFS_FLW_MASK; i++) {
+ 		struct hlist_node *temp;
+ 		struct hlist_head *head;
+ 		struct qede_arfs_fltr_node *fltr;
+ 
+ 		head = &edev->arfs->arfs_hl_head[i];
+ 
+ 		hlist_for_each_entry_safe(fltr, temp, head, node) {
+ 			bool del = false;
+ 
+ 			if (edev->state != QEDE_STATE_OPEN)
+ 				del = true;
+ 
+ 			spin_lock_bh(&edev->arfs->arfs_list_lock);
+ 
+ 			if ((!test_bit(QEDE_FLTR_VALID, &fltr->state) &&
+ 			     !fltr->used) || free_fltr) {
+ 				qede_dequeue_fltr_and_config_searcher(edev,
+ 								      fltr);
+ 			} else {
+ 				bool flow_exp = false;
+ #ifdef CONFIG_RFS_ACCEL
+ 				flow_exp = rps_may_expire_flow(edev->ndev,
+ 							       fltr->rxq_id,
+ 							       fltr->flow_id,
+ 							       fltr->sw_id);
+ #endif
+ 				if ((flow_exp || del) && !free_fltr)
+ 					qede_configure_arfs_fltr(edev, fltr,
+ 								 fltr->rxq_id,
+ 								 false);
+ 			}
+ 
+ 			spin_unlock_bh(&edev->arfs->arfs_list_lock);
+ 		}
+ 	}
+ 
+ 	spin_lock_bh(&edev->arfs->arfs_list_lock);
+ 
+ 	if (!edev->arfs->filter_count) {
+ 		if (edev->arfs->enable) {
+ 			edev->arfs->enable = false;
+ 			edev->ops->configure_arfs_searcher(edev->cdev, false);
+ 		}
+ #ifdef CONFIG_RFS_ACCEL
+ 	} else {
+ 		set_bit(QEDE_SP_ARFS_CONFIG, &edev->sp_flags);
+ 		schedule_delayed_work(&edev->sp_task,
+ 				      QEDE_SP_TASK_POLL_DELAY);
+ #endif
+ 	}
+ 
+ 	spin_unlock_bh(&edev->arfs->arfs_list_lock);
+ }
+ 
+ /* This function waits until all aRFS filters get deleted and freed.
+  * On timeout it frees all filters forcefully.
+  */
+ void qede_poll_for_freeing_arfs_filters(struct qede_dev *edev)
+ {
+ 	int count = QEDE_ARFS_POLL_COUNT;
+ 
+ 	while (count) {
+ 		qede_process_arfs_filters(edev, false);
+ 
+ 		if (!edev->arfs->filter_count)
+ 			break;
+ 
+ 		msleep(100);
+ 		count--;
+ 	}
+ 
+ 	if (!count) {
+ 		DP_NOTICE(edev, "Timeout in polling for arfs filter free\n");
+ 
+ 		/* Something is terribly wrong, free forcefully */
+ 		qede_process_arfs_filters(edev, true);
+ 	}
+ }
+ 
+ int qede_alloc_arfs(struct qede_dev *edev)
+ {
+ 	int i;
+ 
+ 	edev->arfs = vzalloc(sizeof(*edev->arfs));
+ 	if (!edev->arfs)
+ 		return -ENOMEM;
+ 
+ 	spin_lock_init(&edev->arfs->arfs_list_lock);
+ 
+ 	for (i = 0; i <= QEDE_RFS_FLW_MASK; i++)
+ 		INIT_HLIST_HEAD(QEDE_ARFS_BUCKET_HEAD(edev, i));
+ 
+ 	edev->arfs->arfs_fltr_bmap = vzalloc(BITS_TO_LONGS(QEDE_RFS_MAX_FLTR) *
+ 					     sizeof(long));
+ 	if (!edev->arfs->arfs_fltr_bmap) {
+ 		vfree(edev->arfs);
+ 		edev->arfs = NULL;
+ 		return -ENOMEM;
+ 	}
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	edev->ndev->rx_cpu_rmap = alloc_irq_cpu_rmap(QEDE_RSS_COUNT(edev));
+ 	if (!edev->ndev->rx_cpu_rmap) {
+ 		vfree(edev->arfs->arfs_fltr_bmap);
+ 		edev->arfs->arfs_fltr_bmap = NULL;
+ 		vfree(edev->arfs);
+ 		edev->arfs = NULL;
+ 		return -ENOMEM;
+ 	}
+ #endif
+ 	return 0;
+ }
+ 
+ void qede_free_arfs(struct qede_dev *edev)
+ {
+ 	if (!edev->arfs)
+ 		return;
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (edev->ndev->rx_cpu_rmap)
+ 		free_irq_cpu_rmap(edev->ndev->rx_cpu_rmap);
+ 
+ 	edev->ndev->rx_cpu_rmap = NULL;
+ #endif
+ 	vfree(edev->arfs->arfs_fltr_bmap);
+ 	edev->arfs->arfs_fltr_bmap = NULL;
+ 	vfree(edev->arfs);
+ 	edev->arfs = NULL;
+ }
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ static bool qede_compare_ip_addr(struct qede_arfs_fltr_node *tpos,
+ 				 const struct sk_buff *skb)
+ {
+ 	if (skb->protocol == htons(ETH_P_IP)) {
+ 		if (tpos->tuple.src_ipv4 == ip_hdr(skb)->saddr &&
+ 		    tpos->tuple.dst_ipv4 == ip_hdr(skb)->daddr)
+ 			return true;
+ 		else
+ 			return false;
+ 	} else {
+ 		struct in6_addr *src = &tpos->tuple.src_ipv6;
+ 		u8 size = sizeof(struct in6_addr);
+ 
+ 		if (!memcmp(src, &ipv6_hdr(skb)->saddr, size) &&
+ 		    !memcmp(&tpos->tuple.dst_ipv6, &ipv6_hdr(skb)->daddr, size))
+ 			return true;
+ 		else
+ 			return false;
+ 	}
+ }
+ 
+ static struct qede_arfs_fltr_node *
+ qede_arfs_htbl_key_search(struct hlist_head *h, const struct sk_buff *skb,
+ 			  __be16 src_port, __be16 dst_port, u8 ip_proto)
+ {
+ 	struct qede_arfs_fltr_node *tpos;
+ 
+ 	hlist_for_each_entry(tpos, h, node)
+ 		if (tpos->tuple.ip_proto == ip_proto &&
+ 		    tpos->tuple.eth_proto == skb->protocol &&
+ 		    qede_compare_ip_addr(tpos, skb) &&
+ 		    tpos->tuple.src_port == src_port &&
+ 		    tpos->tuple.dst_port == dst_port)
+ 			return tpos;
+ 
+ 	return NULL;
+ }
+ 
+ static struct qede_arfs_fltr_node *
+ qede_alloc_filter(struct qede_dev *edev, int min_hlen)
+ {
+ 	struct qede_arfs_fltr_node *n;
+ 	int bit_id;
+ 
+ 	bit_id = find_first_zero_bit(edev->arfs->arfs_fltr_bmap,
+ 				     QEDE_RFS_MAX_FLTR);
+ 
+ 	if (bit_id >= QEDE_RFS_MAX_FLTR)
+ 		return NULL;
+ 
+ 	n = kzalloc(sizeof(*n), GFP_ATOMIC);
+ 	if (!n)
+ 		return NULL;
+ 
+ 	n->data = kzalloc(min_hlen, GFP_ATOMIC);
+ 	if (!n->data) {
+ 		kfree(n);
+ 		return NULL;
+ 	}
+ 
+ 	n->sw_id = (u16)bit_id;
+ 	set_bit(bit_id, edev->arfs->arfs_fltr_bmap);
+ 	return n;
+ }
+ 
+ int qede_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
+ 		       u16 rxq_index, u32 flow_id)
+ {
+ 	struct qede_dev *edev = netdev_priv(dev);
+ 	struct qede_arfs_fltr_node *n;
+ 	int min_hlen, rc, tp_offset;
+ 	struct ethhdr *eth;
+ 	__be16 *ports;
+ 	u16 tbl_idx;
+ 	u8 ip_proto;
+ 
+ 	if (skb->encapsulation)
+ 		return -EPROTONOSUPPORT;
+ 
+ 	if (skb->protocol != htons(ETH_P_IP) &&
+ 	    skb->protocol != htons(ETH_P_IPV6))
+ 		return -EPROTONOSUPPORT;
+ 
+ 	if (skb->protocol == htons(ETH_P_IP)) {
+ 		ip_proto = ip_hdr(skb)->protocol;
+ 		tp_offset = sizeof(struct iphdr);
+ 	} else {
+ 		ip_proto = ipv6_hdr(skb)->nexthdr;
+ 		tp_offset = sizeof(struct ipv6hdr);
+ 	}
+ 
+ 	if (ip_proto != IPPROTO_TCP && ip_proto != IPPROTO_UDP)
+ 		return -EPROTONOSUPPORT;
+ 
+ 	ports = (__be16 *)(skb->data + tp_offset);
+ 	tbl_idx = skb_get_hash_raw(skb) & QEDE_RFS_FLW_MASK;
+ 
+ 	spin_lock_bh(&edev->arfs->arfs_list_lock);
+ 
+ 	n = qede_arfs_htbl_key_search(QEDE_ARFS_BUCKET_HEAD(edev, tbl_idx),
+ 				      skb, ports[0], ports[1], ip_proto);
+ 	if (n) {
+ 		/* Filter match */
+ 		n->next_rxq_id = rxq_index;
+ 
+ 		if (test_bit(QEDE_FLTR_VALID, &n->state)) {
+ 			if (n->rxq_id != rxq_index)
+ 				qede_configure_arfs_fltr(edev, n, n->rxq_id,
+ 							 false);
+ 		} else {
+ 			if (!n->used) {
+ 				n->rxq_id = rxq_index;
+ 				qede_configure_arfs_fltr(edev, n, n->rxq_id,
+ 							 true);
+ 			}
+ 		}
+ 
+ 		rc = n->sw_id;
+ 		goto ret_unlock;
+ 	}
+ 
+ 	min_hlen = ETH_HLEN + skb_headlen(skb);
+ 
+ 	n = qede_alloc_filter(edev, min_hlen);
+ 	if (!n) {
+ 		rc = -ENOMEM;
+ 		goto ret_unlock;
+ 	}
+ 
+ 	n->buf_len = min_hlen;
+ 	n->rxq_id = rxq_index;
+ 	n->next_rxq_id = rxq_index;
+ 	n->tuple.src_port = ports[0];
+ 	n->tuple.dst_port = ports[1];
+ 	n->flow_id = flow_id;
+ 
+ 	if (skb->protocol == htons(ETH_P_IP)) {
+ 		n->tuple.src_ipv4 = ip_hdr(skb)->saddr;
+ 		n->tuple.dst_ipv4 = ip_hdr(skb)->daddr;
+ 	} else {
+ 		memcpy(&n->tuple.src_ipv6, &ipv6_hdr(skb)->saddr,
+ 		       sizeof(struct in6_addr));
+ 		memcpy(&n->tuple.dst_ipv6, &ipv6_hdr(skb)->daddr,
+ 		       sizeof(struct in6_addr));
+ 	}
+ 
+ 	eth = (struct ethhdr *)n->data;
+ 	eth->h_proto = skb->protocol;
+ 	n->tuple.eth_proto = skb->protocol;
+ 	n->tuple.ip_proto = ip_proto;
+ 	memcpy(n->data + ETH_HLEN, skb->data, skb_headlen(skb));
+ 
+ 	rc = qede_enqueue_fltr_and_config_searcher(edev, n, tbl_idx);
+ 	if (rc)
+ 		goto ret_unlock;
+ 
+ 	qede_configure_arfs_fltr(edev, n, n->rxq_id, true);
+ 
+ 	spin_unlock_bh(&edev->arfs->arfs_list_lock);
+ 
+ 	set_bit(QEDE_SP_ARFS_CONFIG, &edev->sp_flags);
+ 	schedule_delayed_work(&edev->sp_task, 0);
+ 
+ 	return n->sw_id;
+ 
+ ret_unlock:
+ 	spin_unlock_bh(&edev->arfs->arfs_list_lock);
+ 	return rc;
+ }
+ #endif
+ 
+ void qede_udp_ports_update(void *dev, u16 vxlan_port, u16 geneve_port)
+ {
+ 	struct qede_dev *edev = dev;
+ 
+ 	if (edev->vxlan_dst_port != vxlan_port)
+ 		edev->vxlan_dst_port = 0;
+ 
+ 	if (edev->geneve_dst_port != geneve_port)
+ 		edev->geneve_dst_port = 0;
+ }
+ 
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  void qede_force_mac(void *dev, u8 *mac, bool forced)
  {
  	struct qede_dev *edev = dev;
@@@ -717,3 -1300,371 +1210,374 @@@ void qede_config_rx_mode(struct net_dev
  out:
  	kfree(uc_macs);
  }
++<<<<<<< HEAD
++=======
+ 
+ static struct qede_arfs_fltr_node *
+ qede_get_arfs_fltr_by_loc(struct hlist_head *head, u32 location)
+ {
+ 	struct qede_arfs_fltr_node *fltr;
+ 
+ 	hlist_for_each_entry(fltr, head, node)
+ 		if (location == fltr->sw_id)
+ 			return fltr;
+ 
+ 	return NULL;
+ }
+ 
+ static bool
+ qede_compare_user_flow_ips(struct qede_arfs_fltr_node *tpos,
+ 			   struct ethtool_rx_flow_spec *fsp,
+ 			   __be16 proto)
+ {
+ 	if (proto == htons(ETH_P_IP)) {
+ 		struct ethtool_tcpip4_spec *ip;
+ 
+ 		ip = &fsp->h_u.tcp_ip4_spec;
+ 
+ 		if (tpos->tuple.src_ipv4 == ip->ip4src &&
+ 		    tpos->tuple.dst_ipv4 == ip->ip4dst)
+ 			return true;
+ 		else
+ 			return false;
+ 	} else {
+ 		struct ethtool_tcpip6_spec *ip6;
+ 		struct in6_addr *src;
+ 
+ 		ip6 = &fsp->h_u.tcp_ip6_spec;
+ 		src = &tpos->tuple.src_ipv6;
+ 
+ 		if (!memcmp(src, &ip6->ip6src, sizeof(struct in6_addr)) &&
+ 		    !memcmp(&tpos->tuple.dst_ipv6, &ip6->ip6dst,
+ 			    sizeof(struct in6_addr)))
+ 			return true;
+ 		else
+ 			return false;
+ 	}
+ 	return false;
+ }
+ 
+ int qede_get_cls_rule_all(struct qede_dev *edev, struct ethtool_rxnfc *info,
+ 			  u32 *rule_locs)
+ {
+ 	struct qede_arfs_fltr_node *fltr;
+ 	struct hlist_head *head;
+ 	int cnt = 0, rc = 0;
+ 
+ 	info->data = QEDE_RFS_MAX_FLTR;
+ 
+ 	__qede_lock(edev);
+ 
+ 	if (!edev->arfs) {
+ 		rc = -EPERM;
+ 		goto unlock;
+ 	}
+ 
+ 	head = QEDE_ARFS_BUCKET_HEAD(edev, 0);
+ 
+ 	hlist_for_each_entry(fltr, head, node) {
+ 		if (cnt == info->rule_cnt) {
+ 			rc = -EMSGSIZE;
+ 			goto unlock;
+ 		}
+ 
+ 		rule_locs[cnt] = fltr->sw_id;
+ 		cnt++;
+ 	}
+ 
+ 	info->rule_cnt = cnt;
+ 
+ unlock:
+ 	__qede_unlock(edev);
+ 	return rc;
+ }
+ 
+ int qede_get_cls_rule_entry(struct qede_dev *edev, struct ethtool_rxnfc *cmd)
+ {
+ 	struct ethtool_rx_flow_spec *fsp = &cmd->fs;
+ 	struct qede_arfs_fltr_node *fltr = NULL;
+ 	int rc = 0;
+ 
+ 	cmd->data = QEDE_RFS_MAX_FLTR;
+ 
+ 	__qede_lock(edev);
+ 
+ 	if (!edev->arfs) {
+ 		rc = -EPERM;
+ 		goto unlock;
+ 	}
+ 
+ 	fltr = qede_get_arfs_fltr_by_loc(QEDE_ARFS_BUCKET_HEAD(edev, 0),
+ 					 fsp->location);
+ 	if (!fltr) {
+ 		DP_NOTICE(edev, "Rule not found - location=0x%x\n",
+ 			  fsp->location);
+ 		rc = -EINVAL;
+ 		goto unlock;
+ 	}
+ 
+ 	if (fltr->tuple.eth_proto == htons(ETH_P_IP)) {
+ 		if (fltr->tuple.ip_proto == IPPROTO_TCP)
+ 			fsp->flow_type = TCP_V4_FLOW;
+ 		else
+ 			fsp->flow_type = UDP_V4_FLOW;
+ 
+ 		fsp->h_u.tcp_ip4_spec.psrc = fltr->tuple.src_port;
+ 		fsp->h_u.tcp_ip4_spec.pdst = fltr->tuple.dst_port;
+ 		fsp->h_u.tcp_ip4_spec.ip4src = fltr->tuple.src_ipv4;
+ 		fsp->h_u.tcp_ip4_spec.ip4dst = fltr->tuple.dst_ipv4;
+ 	} else {
+ 		if (fltr->tuple.ip_proto == IPPROTO_TCP)
+ 			fsp->flow_type = TCP_V6_FLOW;
+ 		else
+ 			fsp->flow_type = UDP_V6_FLOW;
+ 		fsp->h_u.tcp_ip6_spec.psrc = fltr->tuple.src_port;
+ 		fsp->h_u.tcp_ip6_spec.pdst = fltr->tuple.dst_port;
+ 		memcpy(&fsp->h_u.tcp_ip6_spec.ip6src,
+ 		       &fltr->tuple.src_ipv6, sizeof(struct in6_addr));
+ 		memcpy(&fsp->h_u.tcp_ip6_spec.ip6dst,
+ 		       &fltr->tuple.dst_ipv6, sizeof(struct in6_addr));
+ 	}
+ 
+ 	fsp->ring_cookie = fltr->rxq_id;
+ 
+ unlock:
+ 	__qede_unlock(edev);
+ 	return rc;
+ }
+ 
+ static int
+ qede_validate_and_check_flow_exist(struct qede_dev *edev,
+ 				   struct ethtool_rx_flow_spec *fsp,
+ 				   int *min_hlen)
+ {
+ 	__be16 src_port = 0x0, dst_port = 0x0;
+ 	struct qede_arfs_fltr_node *fltr;
+ 	struct hlist_node *temp;
+ 	struct hlist_head *head;
+ 	__be16 eth_proto;
+ 	u8 ip_proto;
+ 
+ 	if (fsp->location >= QEDE_RFS_MAX_FLTR ||
+ 	    fsp->ring_cookie >= QEDE_RSS_COUNT(edev))
+ 		return -EINVAL;
+ 
+ 	if (fsp->flow_type == TCP_V4_FLOW) {
+ 		*min_hlen += sizeof(struct iphdr) +
+ 				sizeof(struct tcphdr);
+ 		eth_proto = htons(ETH_P_IP);
+ 		ip_proto = IPPROTO_TCP;
+ 	} else if (fsp->flow_type == UDP_V4_FLOW) {
+ 		*min_hlen += sizeof(struct iphdr) +
+ 				sizeof(struct udphdr);
+ 		eth_proto = htons(ETH_P_IP);
+ 		ip_proto = IPPROTO_UDP;
+ 	} else if (fsp->flow_type == TCP_V6_FLOW) {
+ 		*min_hlen += sizeof(struct ipv6hdr) +
+ 				sizeof(struct tcphdr);
+ 		eth_proto = htons(ETH_P_IPV6);
+ 		ip_proto = IPPROTO_TCP;
+ 	} else if (fsp->flow_type == UDP_V6_FLOW) {
+ 		*min_hlen += sizeof(struct ipv6hdr) +
+ 				sizeof(struct udphdr);
+ 		eth_proto = htons(ETH_P_IPV6);
+ 		ip_proto = IPPROTO_UDP;
+ 	} else {
+ 		DP_NOTICE(edev, "Unsupported flow type = 0x%x\n",
+ 			  fsp->flow_type);
+ 		return -EPROTONOSUPPORT;
+ 	}
+ 
+ 	if (eth_proto == htons(ETH_P_IP)) {
+ 		src_port = fsp->h_u.tcp_ip4_spec.psrc;
+ 		dst_port = fsp->h_u.tcp_ip4_spec.pdst;
+ 	} else {
+ 		src_port = fsp->h_u.tcp_ip6_spec.psrc;
+ 		dst_port = fsp->h_u.tcp_ip6_spec.pdst;
+ 	}
+ 
+ 	head = QEDE_ARFS_BUCKET_HEAD(edev, 0);
+ 	hlist_for_each_entry_safe(fltr, temp, head, node) {
+ 		if ((fltr->tuple.ip_proto == ip_proto &&
+ 		     fltr->tuple.eth_proto == eth_proto &&
+ 		     qede_compare_user_flow_ips(fltr, fsp, eth_proto) &&
+ 		     fltr->tuple.src_port == src_port &&
+ 		     fltr->tuple.dst_port == dst_port) ||
+ 		    fltr->sw_id == fsp->location)
+ 			return -EEXIST;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ qede_poll_arfs_filter_config(struct qede_dev *edev,
+ 			     struct qede_arfs_fltr_node *fltr)
+ {
+ 	int count = QEDE_ARFS_POLL_COUNT;
+ 
+ 	while (fltr->used && count) {
+ 		msleep(20);
+ 		count--;
+ 	}
+ 
+ 	if (count == 0 || fltr->fw_rc) {
+ 		qede_dequeue_fltr_and_config_searcher(edev, fltr);
+ 		return -EIO;
+ 	}
+ 
+ 	return fltr->fw_rc;
+ }
+ 
+ int qede_add_cls_rule(struct qede_dev *edev, struct ethtool_rxnfc *info)
+ {
+ 	struct ethtool_rx_flow_spec *fsp = &info->fs;
+ 	struct qede_arfs_fltr_node *n;
+ 	int min_hlen = ETH_HLEN, rc;
+ 	struct ethhdr *eth;
+ 	struct iphdr *ip;
+ 	__be16 *ports;
+ 
+ 	__qede_lock(edev);
+ 
+ 	if (!edev->arfs) {
+ 		rc = -EPERM;
+ 		goto unlock;
+ 	}
+ 
+ 	rc = qede_validate_and_check_flow_exist(edev, fsp, &min_hlen);
+ 	if (rc)
+ 		goto unlock;
+ 
+ 	n = kzalloc(sizeof(*n), GFP_KERNEL);
+ 	if (!n) {
+ 		rc = -ENOMEM;
+ 		goto unlock;
+ 	}
+ 
+ 	n->data = kzalloc(min_hlen, GFP_KERNEL);
+ 	if (!n->data) {
+ 		kfree(n);
+ 		rc = -ENOMEM;
+ 		goto unlock;
+ 	}
+ 
+ 	n->sw_id = fsp->location;
+ 	set_bit(n->sw_id, edev->arfs->arfs_fltr_bmap);
+ 	n->buf_len = min_hlen;
+ 	n->rxq_id = fsp->ring_cookie;
+ 	n->next_rxq_id = n->rxq_id;
+ 	eth = (struct ethhdr *)n->data;
+ 
+ 	if (info->fs.flow_type == TCP_V4_FLOW ||
+ 	    info->fs.flow_type == UDP_V4_FLOW) {
+ 		ports = (__be16 *)(n->data + ETH_HLEN +
+ 					sizeof(struct iphdr));
+ 		eth->h_proto = htons(ETH_P_IP);
+ 		n->tuple.eth_proto = htons(ETH_P_IP);
+ 		n->tuple.src_ipv4 = info->fs.h_u.tcp_ip4_spec.ip4src;
+ 		n->tuple.dst_ipv4 = info->fs.h_u.tcp_ip4_spec.ip4dst;
+ 		n->tuple.src_port = info->fs.h_u.tcp_ip4_spec.psrc;
+ 		n->tuple.dst_port = info->fs.h_u.tcp_ip4_spec.pdst;
+ 		ports[0] = n->tuple.src_port;
+ 		ports[1] = n->tuple.dst_port;
+ 		ip = (struct iphdr *)(n->data + ETH_HLEN);
+ 		ip->saddr = info->fs.h_u.tcp_ip4_spec.ip4src;
+ 		ip->daddr = info->fs.h_u.tcp_ip4_spec.ip4dst;
+ 		ip->version = 0x4;
+ 		ip->ihl = 0x5;
+ 
+ 		if (info->fs.flow_type == TCP_V4_FLOW) {
+ 			n->tuple.ip_proto = IPPROTO_TCP;
+ 			ip->protocol = IPPROTO_TCP;
+ 		} else {
+ 			n->tuple.ip_proto = IPPROTO_UDP;
+ 			ip->protocol = IPPROTO_UDP;
+ 		}
+ 		ip->tot_len = cpu_to_be16(min_hlen - ETH_HLEN);
+ 	} else {
+ 		struct ipv6hdr *ip6;
+ 
+ 		ip6 = (struct ipv6hdr *)(n->data + ETH_HLEN);
+ 		ports = (__be16 *)(n->data + ETH_HLEN +
+ 					sizeof(struct ipv6hdr));
+ 		eth->h_proto = htons(ETH_P_IPV6);
+ 		n->tuple.eth_proto = htons(ETH_P_IPV6);
+ 		memcpy(&n->tuple.src_ipv6, &info->fs.h_u.tcp_ip6_spec.ip6src,
+ 		       sizeof(struct in6_addr));
+ 		memcpy(&n->tuple.dst_ipv6, &info->fs.h_u.tcp_ip6_spec.ip6dst,
+ 		       sizeof(struct in6_addr));
+ 		n->tuple.src_port = info->fs.h_u.tcp_ip6_spec.psrc;
+ 		n->tuple.dst_port = info->fs.h_u.tcp_ip6_spec.pdst;
+ 		ports[0] = n->tuple.src_port;
+ 		ports[1] = n->tuple.dst_port;
+ 		memcpy(&ip6->saddr, &n->tuple.src_ipv6,
+ 		       sizeof(struct in6_addr));
+ 		memcpy(&ip6->daddr, &n->tuple.dst_ipv6,
+ 		       sizeof(struct in6_addr));
+ 		ip6->version = 0x6;
+ 
+ 		if (info->fs.flow_type == TCP_V6_FLOW) {
+ 			n->tuple.ip_proto = IPPROTO_TCP;
+ 			ip6->nexthdr = NEXTHDR_TCP;
+ 			ip6->payload_len = cpu_to_be16(sizeof(struct tcphdr));
+ 		} else {
+ 			n->tuple.ip_proto = IPPROTO_UDP;
+ 			ip6->nexthdr = NEXTHDR_UDP;
+ 			ip6->payload_len = cpu_to_be16(sizeof(struct udphdr));
+ 		}
+ 	}
+ 
+ 	rc = qede_enqueue_fltr_and_config_searcher(edev, n, 0);
+ 	if (rc)
+ 		goto unlock;
+ 
+ 	qede_configure_arfs_fltr(edev, n, n->rxq_id, true);
+ 	rc = qede_poll_arfs_filter_config(edev, n);
+ unlock:
+ 	__qede_unlock(edev);
+ 	return rc;
+ }
+ 
+ int qede_del_cls_rule(struct qede_dev *edev, struct ethtool_rxnfc *info)
+ {
+ 	struct ethtool_rx_flow_spec *fsp = &info->fs;
+ 	struct qede_arfs_fltr_node *fltr = NULL;
+ 	int rc = -EPERM;
+ 
+ 	__qede_lock(edev);
+ 	if (!edev->arfs)
+ 		goto unlock;
+ 
+ 	fltr = qede_get_arfs_fltr_by_loc(QEDE_ARFS_BUCKET_HEAD(edev, 0),
+ 					 fsp->location);
+ 	if (!fltr)
+ 		goto unlock;
+ 
+ 	qede_configure_arfs_fltr(edev, fltr, fltr->rxq_id, false);
+ 
+ 	rc = qede_poll_arfs_filter_config(edev, fltr);
+ 	if (rc == 0)
+ 		qede_dequeue_fltr_and_config_searcher(edev, fltr);
+ 
+ unlock:
+ 	__qede_unlock(edev);
+ 	return rc;
+ }
+ 
+ int qede_get_arfs_filter_count(struct qede_dev *edev)
+ {
+ 	int count = 0;
+ 
+ 	__qede_lock(edev);
+ 
+ 	if (!edev->arfs)
+ 		goto unlock;
+ 
+ 	count = edev->arfs->filter_count;
+ 
+ unlock:
+ 	__qede_unlock(edev);
+ 	return count;
+ }
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
diff --cc drivers/net/ethernet/qlogic/qede/qede_main.c
index 9490909958e8,e5ee9f274a71..000000000000
--- a/drivers/net/ethernet/qlogic/qede/qede_main.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_main.c
@@@ -802,9 -864,16 +802,20 @@@ static void qede_update_pf_params(struc
  {
  	struct qed_pf_params pf_params;
  
 -	/* 64 rx + 64 tx + 64 XDP */
 +	/* 64 rx + 64 tx */
  	memset(&pf_params, 0, sizeof(struct qed_pf_params));
++<<<<<<< HEAD
 +	pf_params.eth_pf_params.num_cons = (MAX_SB_PER_PF_MIMD - 1) * 2;
++=======
+ 	pf_params.eth_pf_params.num_cons = (MAX_SB_PER_PF_MIMD - 1) * 3;
+ 
+ 	/* Same for VFs - make sure they'll have sufficient connections
+ 	 * to support XDP Tx queues.
+ 	 */
+ 	pf_params.eth_pf_params.num_vf_cons = 48;
+ 
+ 	pf_params.eth_pf_params.num_arfs_filters = QEDE_RFS_MAX_FLTR;
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  	qed_ops->common->update_pf_params(cdev, &pf_params);
  }
  
@@@ -1829,6 -1983,11 +1840,14 @@@ static void qede_unload(struct qede_de
  	qede_vlan_mark_nonconfigured(edev);
  	edev->ops->fastpath_stop(edev->cdev);
  
++<<<<<<< HEAD
++=======
+ 	if (!IS_VF(edev) && edev->dev_info.common.num_hwfns == 1) {
+ 		qede_poll_for_freeing_arfs_filters(edev);
+ 		qede_free_arfs(edev);
+ 	}
+ 
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  	/* Release the interrupts */
  	qede_sync_free_irqs(edev);
  	edev->ops->common->set_fp_int(edev->cdev, 0);
@@@ -1881,6 -2039,12 +1900,15 @@@ static int qede_load(struct qede_dev *e
  	if (rc)
  		goto err2;
  
++<<<<<<< HEAD
++=======
+ 	if (!IS_VF(edev) && edev->dev_info.common.num_hwfns == 1) {
+ 		rc = qede_alloc_arfs(edev);
+ 		if (rc)
+ 			DP_NOTICE(edev, "aRFS memory allocation failed\n");
+ 	}
+ 
++>>>>>>> 3f2a2b8b7aaa (qed/qede: Add setter APIs support for RX flow classification)
  	qede_napi_add_enable(edev);
  	DP_INFO(edev, "Napi added and enabled\n");
  
* Unmerged path drivers/net/ethernet/qlogic/qed/qed_main.c
* Unmerged path drivers/net/ethernet/qlogic/qede/qede.h
diff --git a/drivers/net/ethernet/qlogic/qede/qede_ethtool.c b/drivers/net/ethernet/qlogic/qede/qede_ethtool.c
index 57cf7fefa31b..e74ee3efacaa 100644
--- a/drivers/net/ethernet/qlogic/qede/qede_ethtool.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_ethtool.c
@@ -1154,14 +1154,24 @@ static int qede_set_rss_flags(struct qede_dev *edev, struct ethtool_rxnfc *info)
 static int qede_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info)
 {
 	struct qede_dev *edev = netdev_priv(dev);
+	int rc;
 
 	switch (info->cmd) {
 	case ETHTOOL_SRXFH:
-		return qede_set_rss_flags(edev, info);
+		rc = qede_set_rss_flags(edev, info);
+		break;
+	case ETHTOOL_SRXCLSRLINS:
+		rc = qede_add_cls_rule(edev, info);
+		break;
+	case ETHTOOL_SRXCLSRLDEL:
+		rc = qede_del_cls_rule(edev, info);
+		break;
 	default:
 		DP_INFO(edev, "Command parameters not supported\n");
-		return -EOPNOTSUPP;
+		rc = -EOPNOTSUPP;
 	}
+
+	return rc;
 }
 
 static u32 qede_get_rxfh_indir_size(struct net_device *dev)
* Unmerged path drivers/net/ethernet/qlogic/qede/qede_filter.c
* Unmerged path drivers/net/ethernet/qlogic/qede/qede_main.c
