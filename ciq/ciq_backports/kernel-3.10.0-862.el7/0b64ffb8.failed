xen/pvh*: Support > 32 VCPUs at domain restore

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Ankur Arora <ankur.a.arora@oracle.com>
commit 0b64ffb8db4e310f77a01079ca752d946a8526b5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/0b64ffb8.failed

When Xen restores a PVHVM or PVH guest, its shared_info only holds
up to 32 CPUs. The hypercall VCPUOP_register_vcpu_info allows
us to setup per-page areas for VCPUs. This means we can boot
PVH* guests with more than 32 VCPUs. During restore the per-cpu
structure is allocated freshly by the hypervisor (vcpu_info_mfn is
set to INVALID_MFN) so that the newly restored guest can make a
VCPUOP_register_vcpu_info hypercall.

However, we end up triggering this condition in Xen:
/* Run this command on yourself or on other offline VCPUS. */
 if ( (v != current) && !test_bit(_VPF_down, &v->pause_flags) )

which means we are unable to setup the per-cpu VCPU structures
for running VCPUS. The Linux PV code paths makes this work by
iterating over cpu_possible in xen_vcpu_restore() with:

 1) is target CPU up (VCPUOP_is_up hypercall?)
 2) if yes, then VCPUOP_down to pause it
 3) VCPUOP_register_vcpu_info
 4) if it was down, then VCPUOP_up to bring it back up

With Xen commit 192df6f9122d ("xen/x86: allow HVM guests to use
hypercalls to bring up vCPUs") this is available for non-PV guests.
As such first check if VCPUOP_is_up is actually possible before
trying this dance.

As most of this dance code is done already in xen_vcpu_restore()
let's make it callable on PV, PVH and PVHVM.

Based-on-patch-by: Konrad Wilk <konrad.wilk@oracle.com>
	Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Signed-off-by: Ankur Arora <ankur.a.arora@oracle.com>
	Signed-off-by: Juergen Gross <jgross@suse.com>
(cherry picked from commit 0b64ffb8db4e310f77a01079ca752d946a8526b5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/xen/enlighten.c
#	arch/x86/xen/enlighten_hvm.c
#	arch/x86/xen/smp_hvm.c
#	arch/x86/xen/suspend_hvm.c
diff --cc arch/x86/xen/enlighten.c
index 5df0cb46e645,276cc21619ec..000000000000
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@@ -158,22 -79,82 +158,65 @@@ struct shared_info *HYPERVISOR_shared_i
   *
   * 0: not available, 1: available
   */
 -int xen_have_vcpu_info_placement = 1;
 -
 -static int xen_cpu_up_online(unsigned int cpu)
 -{
 -	xen_init_lock_cpu(cpu);
 -	return 0;
 -}
 -
 -int xen_cpuhp_setup(int (*cpu_up_prepare_cb)(unsigned int),
 -		    int (*cpu_dead_cb)(unsigned int))
 -{
 -	int rc;
 -
 -	rc = cpuhp_setup_state_nocalls(CPUHP_XEN_PREPARE,
 -				       "x86/xen/hvm_guest:prepare",
 -				       cpu_up_prepare_cb, cpu_dead_cb);
 -	if (rc >= 0) {
 -		rc = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
 -					       "x86/xen/hvm_guest:online",
 -					       xen_cpu_up_online, NULL);
 -		if (rc < 0)
 -			cpuhp_remove_state_nocalls(CPUHP_XEN_PREPARE);
 -	}
 +static int have_vcpu_info_placement = 1;
  
 -	return rc >= 0 ? 0 : rc;
 -}
 +struct tls_descs {
 +	struct desc_struct desc[3];
 +};
  
+ static void xen_vcpu_setup_restore(int cpu)
+ {
+ 	/* Any per_cpu(xen_vcpu) is stale, so reset it */
+ 	xen_vcpu_info_reset(cpu);
+ 
+ 	/*
+ 	 * For PVH and PVHVM, setup online VCPUs only. The rest will
+ 	 * be handled by hotplug.
+ 	 */
+ 	if (xen_pv_domain() ||
+ 	    (xen_hvm_domain() && cpu_online(cpu))) {
+ 		xen_vcpu_setup(cpu);
+ 	}
+ }
+ 
  /*
 - * On restore, set the vcpu placement up again.
 - * If it fails, then we're in a bad state, since
 - * we can't back out from using it...
 + * Updating the 3 TLS descriptors in the GDT on every task switch is
 + * surprisingly expensive so we avoid updating them if they haven't
 + * changed.  Since Xen writes different descriptors than the one
 + * passed in the update_descriptor hypercall we keep shadow copies to
 + * compare against.
   */
 -void xen_vcpu_restore(void)
 -{
 -	int cpu;
 +static DEFINE_PER_CPU(struct tls_descs, shadow_tls_desc);
  
++<<<<<<< HEAD
 +#define XEN_HVM_CPUID_VCPU_ID_PRESENT  (1u << 3)
++=======
+ 	for_each_possible_cpu(cpu) {
+ 		bool other_cpu = (cpu != smp_processor_id());
+ 		bool is_up;
+ 
+ 		if (xen_vcpu_nr(cpu) == XEN_VCPU_ID_INVALID)
+ 			continue;
+ 
+ 		/* Only Xen 4.5 and higher support this. */
+ 		is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up,
+ 					   xen_vcpu_nr(cpu), NULL) > 0;
+ 
+ 		if (other_cpu && is_up &&
+ 		    HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(cpu), NULL))
+ 			BUG();
+ 
+ 		if (xen_pv_domain() || xen_feature(XENFEAT_hvm_safe_pvclock))
+ 			xen_setup_runstate_info(cpu);
+ 
+ 		xen_vcpu_setup_restore(cpu);
+ 
+ 		if (other_cpu && is_up &&
+ 		    HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL))
+ 			BUG();
+ 	}
+ }
++>>>>>>> 0b64ffb8db4e (xen/pvh*: Support > 32 VCPUs at domain restore)
  
  static void clamp_max_cpus(void)
  {
@@@ -206,280 -198,78 +249,315 @@@ void xen_vcpu_setup(int cpu
  		if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
  			return;
  	}
 +	if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS)
 +		per_cpu(xen_vcpu, cpu) =
 +			&HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
  
++<<<<<<< HEAD
 +	if (!have_vcpu_info_placement) {
++=======
+ 	if (xen_have_vcpu_info_placement) {
+ 		vcpup = &per_cpu(xen_vcpu_info, cpu);
+ 		info.mfn = arbitrary_virt_to_mfn(vcpup);
+ 		info.offset = offset_in_page(vcpup);
+ 
+ 		/*
+ 		 * Check to see if the hypervisor will put the vcpu_info
+ 		 * structure where we want it, which allows direct access via
+ 		 * a percpu-variable.
+ 		 * N.B. This hypercall can _only_ be called once per CPU.
+ 		 * Subsequent calls will error out with -EINVAL. This is due to
+ 		 * the fact that hypervisor has no unregister variant and this
+ 		 * hypercall does not allow to over-write info.mfn and
+ 		 * info.offset.
+ 		 */
+ 		err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
+ 					 xen_vcpu_nr(cpu), &info);
+ 
+ 		if (err) {
+ 			pr_warn_once("register_vcpu_info failed: cpu=%d err=%d\n",
+ 				     cpu, err);
+ 			xen_have_vcpu_info_placement = 0;
+ 		} else {
+ 			/*
+ 			 * This cpu is using the registered vcpu info, even if
+ 			 * later ones fail to.
+ 			 */
+ 			per_cpu(xen_vcpu, cpu) = vcpup;
+ 		}
+ 	}
+ 
+ 	if (!xen_have_vcpu_info_placement) {
++>>>>>>> 0b64ffb8db4e (xen/pvh*: Support > 32 VCPUs at domain restore)
  		if (cpu >= MAX_VIRT_CPUS)
  			clamp_max_cpus();
- 		return;
+ 		xen_vcpu_info_reset(cpu);
  	}
 +
 +	vcpup = &per_cpu(xen_vcpu_info, cpu);
 +	info.mfn = arbitrary_virt_to_mfn(vcpup);
 +	info.offset = offset_in_page(vcpup);
 +
 +	/* Check to see if the hypervisor will put the vcpu_info
 +	   structure where we want it, which allows direct access via
 +	   a percpu-variable.
 +	   N.B. This hypercall can _only_ be called once per CPU. Subsequent
 +	   calls will error out with -EINVAL. This is due to the fact that
 +	   hypervisor has no unregister variant and this hypercall does not
 +	   allow to over-write info.mfn and info.offset.
 +	 */
 +	err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, xen_vcpu_nr(cpu),
 +				 &info);
 +
 +	if (err) {
 +		printk(KERN_DEBUG "register_vcpu_info failed: err=%d\n", err);
 +		have_vcpu_info_placement = 0;
 +		clamp_max_cpus();
 +	} else {
 +		/* This cpu is using the registered vcpu info, even if
 +		   later ones fail to. */
 +		per_cpu(xen_vcpu, cpu) = vcpup;
 +	}
  }
  
 -void xen_reboot(int reason)
 +/*
 + * On restore, set the vcpu placement up again.
 + * If it fails, then we're in a bad state, since
 + * we can't back out from using it...
 + */
 +void xen_vcpu_restore(void)
  {
 -	struct sched_shutdown r = { .reason = reason };
  	int cpu;
  
 -	for_each_online_cpu(cpu)
 -		xen_pmu_finish(cpu);
 +	for_each_possible_cpu(cpu) {
 +		bool other_cpu = (cpu != smp_processor_id());
 +		bool is_up = HYPERVISOR_vcpu_op(VCPUOP_is_up, xen_vcpu_nr(cpu),
 +						NULL);
  
 -	if (HYPERVISOR_sched_op(SCHEDOP_shutdown, &r))
 -		BUG();
 +		if (other_cpu && is_up &&
 +		    HYPERVISOR_vcpu_op(VCPUOP_down, xen_vcpu_nr(cpu), NULL))
 +			BUG();
 +
 +		xen_setup_runstate_info(cpu);
 +
 +		if (have_vcpu_info_placement)
 +			xen_vcpu_setup(cpu);
 +
 +		if (other_cpu && is_up &&
 +		    HYPERVISOR_vcpu_op(VCPUOP_up, xen_vcpu_nr(cpu), NULL))
 +			BUG();
 +	}
  }
  
 -void xen_emergency_restart(void)
 +static void __init xen_banner(void)
  {
 -	xen_reboot(SHUTDOWN_reboot);
 +	unsigned version = HYPERVISOR_xen_version(XENVER_version, NULL);
 +	struct xen_extraversion extra;
 +	HYPERVISOR_xen_version(XENVER_extraversion, &extra);
 +
 +	printk(KERN_INFO "Booting paravirtualized kernel on %s\n",
 +	       pv_info.name);
 +	printk(KERN_INFO "Xen version: %d.%d%s%s\n",
 +	       version >> 16, version & 0xffff, extra.extraversion,
 +	       xen_feature(XENFEAT_mmu_pt_update_preserve_ad) ? " (preserve-AD)" : "");
  }
 +/* Check if running on Xen version (major, minor) or later */
 +bool
 +xen_running_on_version_or_later(unsigned int major, unsigned int minor)
 +{
 +	unsigned int version;
  
 -static int
 -xen_panic_event(struct notifier_block *this, unsigned long event, void *ptr)
 +	if (!xen_domain())
 +		return false;
 +
 +	version = HYPERVISOR_xen_version(XENVER_version, NULL);
 +	if ((((version >> 16) == major) && ((version & 0xffff) >= minor)) ||
 +		((version >> 16) > major))
 +		return true;
 +	return false;
 +}
 +
 +#define CPUID_THERM_POWER_LEAF 6
 +#define APERFMPERF_PRESENT 0
 +
 +static __read_mostly unsigned int cpuid_leaf1_edx_mask = ~0;
 +static __read_mostly unsigned int cpuid_leaf1_ecx_mask = ~0;
 +
 +static __read_mostly unsigned int cpuid_leaf1_ecx_set_mask;
 +static __read_mostly unsigned int cpuid_leaf5_ecx_val;
 +static __read_mostly unsigned int cpuid_leaf5_edx_val;
 +
 +static void xen_cpuid(unsigned int *ax, unsigned int *bx,
 +		      unsigned int *cx, unsigned int *dx)
  {
 -	if (!kexec_crash_loaded())
 -		xen_reboot(SHUTDOWN_crash);
 -	return NOTIFY_DONE;
 +	unsigned maskebx = ~0;
 +	unsigned maskecx = ~0;
 +	unsigned maskedx = ~0;
 +	unsigned setecx = 0;
 +	/*
 +	 * Mask out inconvenient features, to try and disable as many
 +	 * unsupported kernel subsystems as possible.
 +	 */
 +	switch (*ax) {
 +	case 1:
 +		maskecx = cpuid_leaf1_ecx_mask;
 +		setecx = cpuid_leaf1_ecx_set_mask;
 +		maskedx = cpuid_leaf1_edx_mask;
 +		break;
 +
 +	case CPUID_MWAIT_LEAF:
 +		/* Synthesize the values.. */
 +		*ax = 0;
 +		*bx = 0;
 +		*cx = cpuid_leaf5_ecx_val;
 +		*dx = cpuid_leaf5_edx_val;
 +		return;
 +
 +	case CPUID_THERM_POWER_LEAF:
 +		/* Disabling APERFMPERF for kernel usage */
 +		maskecx = ~(1 << APERFMPERF_PRESENT);
 +		break;
 +
 +	case 0xb:
 +		/* Suppress extended topology stuff */
 +		maskebx = 0;
 +		break;
 +	}
 +
 +	asm(XEN_EMULATE_PREFIX "cpuid"
 +		: "=a" (*ax),
 +		  "=b" (*bx),
 +		  "=c" (*cx),
 +		  "=d" (*dx)
 +		: "0" (*ax), "2" (*cx));
 +
 +	*bx &= maskebx;
 +	*cx &= maskecx;
 +	*cx |= setecx;
 +	*dx &= maskedx;
  }
 +STACK_FRAME_NON_STANDARD(xen_cpuid); /* XEN_EMULATE_PREFIX */
  
 -static struct notifier_block xen_panic_block = {
 -	.notifier_call = xen_panic_event,
 -	.priority = INT_MIN
 -};
 +static bool __init xen_check_mwait(void)
 +{
 +#ifdef CONFIG_ACPI
 +	struct xen_platform_op op = {
 +		.cmd			= XENPF_set_processor_pminfo,
 +		.u.set_pminfo.id	= -1,
 +		.u.set_pminfo.type	= XEN_PM_PDC,
 +	};
 +	uint32_t buf[3];
 +	unsigned int ax, bx, cx, dx;
 +	unsigned int mwait_mask;
  
 -int xen_panic_handler_init(void)
 +	/* We need to determine whether it is OK to expose the MWAIT
 +	 * capability to the kernel to harvest deeper than C3 states from ACPI
 +	 * _CST using the processor_harvest_xen.c module. For this to work, we
 +	 * need to gather the MWAIT_LEAF values (which the cstate.c code
 +	 * checks against). The hypervisor won't expose the MWAIT flag because
 +	 * it would break backwards compatibility; so we will find out directly
 +	 * from the hardware and hypercall.
 +	 */
 +	if (!xen_initial_domain())
 +		return false;
 +
 +	/*
 +	 * When running under platform earlier than Xen4.2, do not expose
 +	 * mwait, to avoid the risk of loading native acpi pad driver
 +	 */
 +	if (!xen_running_on_version_or_later(4, 2))
 +		return false;
 +
 +	ax = 1;
 +	cx = 0;
 +
 +	native_cpuid(&ax, &bx, &cx, &dx);
 +
 +	mwait_mask = (1 << (X86_FEATURE_EST % 32)) |
 +		     (1 << (X86_FEATURE_MWAIT % 32));
 +
 +	if ((cx & mwait_mask) != mwait_mask)
 +		return false;
 +
 +	/* We need to emulate the MWAIT_LEAF and for that we need both
 +	 * ecx and edx. The hypercall provides only partial information.
 +	 */
 +
 +	ax = CPUID_MWAIT_LEAF;
 +	bx = 0;
 +	cx = 0;
 +	dx = 0;
 +
 +	native_cpuid(&ax, &bx, &cx, &dx);
 +
 +	/* Ask the Hypervisor whether to clear ACPI_PDC_C_C2C3_FFH. If so,
 +	 * don't expose MWAIT_LEAF and let ACPI pick the IOPORT version of C3.
 +	 */
 +	buf[0] = ACPI_PDC_REVISION_ID;
 +	buf[1] = 1;
 +	buf[2] = (ACPI_PDC_C_CAPABILITY_SMP | ACPI_PDC_EST_CAPABILITY_SWSMP);
 +
 +	set_xen_guest_handle(op.u.set_pminfo.pdc, buf);
 +
 +	if ((HYPERVISOR_dom0_op(&op) == 0) &&
 +	    (buf[2] & (ACPI_PDC_C_C1_FFH | ACPI_PDC_C_C2C3_FFH))) {
 +		cpuid_leaf5_ecx_val = cx;
 +		cpuid_leaf5_edx_val = dx;
 +	}
 +	return true;
 +#else
 +	return false;
 +#endif
 +}
 +static void __init xen_init_cpuid_mask(void)
 +{
 +	unsigned int ax, bx, cx, dx;
 +	unsigned int xsave_mask;
 +
 +	cpuid_leaf1_edx_mask =
 +		~((1 << X86_FEATURE_MTRR) |  /* disable MTRR */
 +		  (1 << X86_FEATURE_ACC));   /* thermal monitoring */
 +
 +	if (!xen_initial_domain())
 +		cpuid_leaf1_edx_mask &=
 +			~((1 << X86_FEATURE_APIC) |  /* disable local APIC */
 +			  (1 << X86_FEATURE_ACPI));  /* disable ACPI */
 +
 +	cpuid_leaf1_ecx_mask &= ~(1 << (X86_FEATURE_X2APIC % 32));
 +
 +	ax = 1;
 +	cx = 0;
 +	xen_cpuid(&ax, &bx, &cx, &dx);
 +
 +	xsave_mask =
 +		(1 << (X86_FEATURE_XSAVE % 32)) |
 +		(1 << (X86_FEATURE_OSXSAVE % 32));
 +
 +	/* Xen will set CR4.OSXSAVE if supported and not disabled by force */
 +	if ((cx & xsave_mask) != xsave_mask)
 +		cpuid_leaf1_ecx_mask &= ~xsave_mask; /* disable XSAVE & OSXSAVE */
 +	if (xen_check_mwait())
 +		cpuid_leaf1_ecx_set_mask = (1 << (X86_FEATURE_MWAIT % 32));
 +}
 +
 +static void xen_set_debugreg(int reg, unsigned long val)
 +{
 +	HYPERVISOR_set_debugreg(reg, val);
 +}
 +
 +static unsigned long xen_get_debugreg(int reg)
 +{
 +	return HYPERVISOR_get_debugreg(reg);
 +}
 +
 +static void xen_end_context_switch(struct task_struct *next)
 +{
 +	xen_mc_flush();
 +	paravirt_end_context_switch(next);
 +}
 +
 +static unsigned long xen_store_tr(void)
  {
 -	atomic_notifier_chain_register(&panic_notifier_list, &xen_panic_block);
  	return 0;
  }
  
* Unmerged path arch/x86/xen/enlighten_hvm.c
* Unmerged path arch/x86/xen/smp_hvm.c
* Unmerged path arch/x86/xen/suspend_hvm.c
* Unmerged path arch/x86/xen/enlighten.c
* Unmerged path arch/x86/xen/enlighten_hvm.c
* Unmerged path arch/x86/xen/smp_hvm.c
* Unmerged path arch/x86/xen/suspend_hvm.c
diff --git a/include/xen/xen-ops.h b/include/xen/xen-ops.h
index df2713dcb471..209af7ab3556 100644
--- a/include/xen/xen-ops.h
+++ b/include/xen/xen-ops.h
@@ -12,6 +12,8 @@ static inline uint32_t xen_vcpu_nr(int cpu)
 	return per_cpu(xen_vcpu_id, cpu);
 }
 
+#define XEN_VCPU_ID_INVALID U32_MAX
+
 void xen_arch_pre_suspend(void);
 void xen_arch_post_suspend(int suspend_cancelled);
 void xen_arch_hvm_post_suspend(int suspend_cancelled);
