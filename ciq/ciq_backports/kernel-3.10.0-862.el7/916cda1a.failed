s390: add a system call for guarded storage

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [s390] add a system call for guarded storage (Hendrik Brueckner) [1375261]
Rebuild_FUZZ: 92.50%
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit 916cda1aa1b412d7cf2991c3af7479544942d121
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/916cda1a.failed

This adds a new system call to enable the use of guarded storage for
user space processes. The system call takes two arguments, a command
and pointer to a guarded storage control block:

    s390_guarded_storage(int command, struct gs_cb *gs_cb);

The second argument is relevant only for the GS_SET_BC_CB command.

The commands in detail:

0 - GS_ENABLE
    Enable the guarded storage facility for the current task. The
    initial content of the guarded storage control block will be
    all zeros. After the enablement the user space code can use
    load-guarded-storage-controls instruction (LGSC) to load an
    arbitrary control block. While a task is enabled the kernel
    will save and restore the current content of the guarded
    storage registers on context switch.
1 - GS_DISABLE
    Disables the use of the guarded storage facility for the current
    task. The kernel will cease to save and restore the content of
    the guarded storage registers, the task specific content of
    these registers is lost.
2 - GS_SET_BC_CB
    Set a broadcast guarded storage control block. This is called
    per thread and stores a specific guarded storage control block
    in the task struct of the current task. This control block will
    be used for the broadcast event GS_BROADCAST.
3 - GS_CLEAR_BC_CB
    Clears the broadcast guarded storage control block. The guarded-
    storage control block is removed from the task struct that was
    established by GS_SET_BC_CB.
4 - GS_BROADCAST
    Sends a broadcast to all thread siblings of the current task.
    Every sibling that has established a broadcast guarded storage
    control block will load this control block and will be enabled
    for guarded storage. The broadcast guarded storage control block
    is used up, a second broadcast without a refresh of the stored
    control block with GS_SET_BC_CB will not have any effect.

	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit 916cda1aa1b412d7cf2991c3af7479544942d121)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/elf.h
#	arch/s390/include/asm/nmi.h
#	arch/s390/include/asm/processor.h
#	arch/s390/include/asm/setup.h
#	arch/s390/include/asm/switch_to.h
#	arch/s390/include/asm/thread_info.h
#	arch/s390/include/uapi/asm/Kbuild
#	arch/s390/include/uapi/asm/unistd.h
#	arch/s390/kernel/Makefile
#	arch/s390/kernel/asm-offsets.c
#	arch/s390/kernel/compat_wrapper.c
#	arch/s390/kernel/early.c
#	arch/s390/kernel/entry.S
#	arch/s390/kernel/entry.h
#	arch/s390/kernel/machine_kexec.c
#	arch/s390/kernel/nmi.c
#	arch/s390/kernel/process.c
#	arch/s390/kernel/processor.c
#	arch/s390/kernel/ptrace.c
#	arch/s390/kernel/setup.c
#	arch/s390/kernel/smp.c
#	arch/s390/kernel/syscalls.S
#	arch/s390/kvm/interrupt.c
diff --cc arch/s390/include/asm/elf.h
index a0d33f39d65a,e8f623041769..000000000000
--- a/arch/s390/include/asm/elf.h
+++ b/arch/s390/include/asm/elf.h
@@@ -103,6 -103,9 +103,12 @@@
  #define HWCAP_S390_HIGH_GPRS	512
  #define HWCAP_S390_TE		1024
  #define HWCAP_S390_VXRS		2048
++<<<<<<< HEAD
++=======
+ #define HWCAP_S390_VXRS_BCD	4096
+ #define HWCAP_S390_VXRS_EXT	8192
+ #define HWCAP_S390_GS		16384
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  /* Internal bits, not exposed via elf */
  #define HWCAP_INT_SIE		1UL
diff --cc arch/s390/include/asm/nmi.h
index 3027a5a72b74,e3e8895f5d3e..000000000000
--- a/arch/s390/include/asm/nmi.h
+++ b/arch/s390/include/asm/nmi.h
@@@ -11,51 -11,72 +11,117 @@@
  #ifndef _ASM_S390_NMI_H
  #define _ASM_S390_NMI_H
  
 -#include <linux/const.h>
  #include <linux/types.h>
  
++<<<<<<< HEAD
 +struct mci {
 +	__u32 sd :  1; /* 00 system damage */
 +	__u32 pd :  1; /* 01 instruction-processing damage */
 +	__u32 sr :  1; /* 02 system recovery */
 +	__u32	 :  1; /* 03 */
 +	__u32 cd :  1; /* 04 timing-facility damage */
 +	__u32 ed :  1; /* 05 external damage */
 +	__u32	 :  1; /* 06 */
 +	__u32 dg :  1; /* 07 degradation */
 +	__u32 w  :  1; /* 08 warning pending */
 +	__u32 cp :  1; /* 09 channel-report pending */
 +	__u32 sp :  1; /* 10 service-processor damage */
 +	__u32 ck :  1; /* 11 channel-subsystem damage */
 +	__u32	 :  2; /* 12-13 */
 +	__u32 b  :  1; /* 14 backed up */
 +	__u32	 :  1; /* 15 */
 +	__u32 se :  1; /* 16 storage error uncorrected */
 +	__u32 sc :  1; /* 17 storage error corrected */
 +	__u32 ke :  1; /* 18 storage-key error uncorrected */
 +	__u32 ds :  1; /* 19 storage degradation */
 +	__u32 wp :  1; /* 20 psw mwp validity */
 +	__u32 ms :  1; /* 21 psw mask and key validity */
 +	__u32 pm :  1; /* 22 psw program mask and cc validity */
 +	__u32 ia :  1; /* 23 psw instruction address validity */
 +	__u32 fa :  1; /* 24 failing storage address validity */
 +	__u32 vr :  1; /* 25 vector register validity */
 +	__u32 ec :  1; /* 26 external damage code validity */
 +	__u32 fp :  1; /* 27 floating point register validity */
 +	__u32 gr :  1; /* 28 general register validity */
 +	__u32 cr :  1; /* 29 control register validity */
 +	__u32	 :  1; /* 30 */
 +	__u32 st :  1; /* 31 storage logical validity */
 +	__u32 ie :  1; /* 32 indirect storage error */
 +	__u32 ar :  1; /* 33 access register validity */
 +	__u32 da :  1; /* 34 delayed access exception */
 +	__u32	 :  7; /* 35-41 */
 +	__u32 pr :  1; /* 42 tod programmable register validity */
 +	__u32 fc :  1; /* 43 fp control register validity */
 +	__u32 ap :  1; /* 44 ancillary report */
 +	__u32	 :  1; /* 45 */
 +	__u32 ct :  1; /* 46 cpu timer validity */
 +	__u32 cc :  1; /* 47 clock comparator validity */
 +	__u32	 : 16; /* 47-63 */
++=======
+ #define MCCK_CODE_SYSTEM_DAMAGE		_BITUL(63)
+ #define MCCK_CODE_CPU_TIMER_VALID	_BITUL(63 - 46)
+ #define MCCK_CODE_PSW_MWP_VALID		_BITUL(63 - 20)
+ #define MCCK_CODE_PSW_IA_VALID		_BITUL(63 - 23)
+ 
+ #ifndef __ASSEMBLY__
+ 
+ union mci {
+ 	unsigned long val;
+ 	struct {
+ 		u64 sd :  1; /* 00 system damage */
+ 		u64 pd :  1; /* 01 instruction-processing damage */
+ 		u64 sr :  1; /* 02 system recovery */
+ 		u64    :  1; /* 03 */
+ 		u64 cd :  1; /* 04 timing-facility damage */
+ 		u64 ed :  1; /* 05 external damage */
+ 		u64    :  1; /* 06 */
+ 		u64 dg :  1; /* 07 degradation */
+ 		u64 w  :  1; /* 08 warning pending */
+ 		u64 cp :  1; /* 09 channel-report pending */
+ 		u64 sp :  1; /* 10 service-processor damage */
+ 		u64 ck :  1; /* 11 channel-subsystem damage */
+ 		u64    :  2; /* 12-13 */
+ 		u64 b  :  1; /* 14 backed up */
+ 		u64    :  1; /* 15 */
+ 		u64 se :  1; /* 16 storage error uncorrected */
+ 		u64 sc :  1; /* 17 storage error corrected */
+ 		u64 ke :  1; /* 18 storage-key error uncorrected */
+ 		u64 ds :  1; /* 19 storage degradation */
+ 		u64 wp :  1; /* 20 psw mwp validity */
+ 		u64 ms :  1; /* 21 psw mask and key validity */
+ 		u64 pm :  1; /* 22 psw program mask and cc validity */
+ 		u64 ia :  1; /* 23 psw instruction address validity */
+ 		u64 fa :  1; /* 24 failing storage address validity */
+ 		u64 vr :  1; /* 25 vector register validity */
+ 		u64 ec :  1; /* 26 external damage code validity */
+ 		u64 fp :  1; /* 27 floating point register validity */
+ 		u64 gr :  1; /* 28 general register validity */
+ 		u64 cr :  1; /* 29 control register validity */
+ 		u64    :  1; /* 30 */
+ 		u64 st :  1; /* 31 storage logical validity */
+ 		u64 ie :  1; /* 32 indirect storage error */
+ 		u64 ar :  1; /* 33 access register validity */
+ 		u64 da :  1; /* 34 delayed access exception */
+ 		u64    :  1; /* 35 */
+ 		u64 gs :  1; /* 36 guarded storage registers */
+ 		u64    :  5; /* 37-41 */
+ 		u64 pr :  1; /* 42 tod programmable register validity */
+ 		u64 fc :  1; /* 43 fp control register validity */
+ 		u64 ap :  1; /* 44 ancillary report */
+ 		u64    :  1; /* 45 */
+ 		u64 ct :  1; /* 46 cpu timer validity */
+ 		u64 cc :  1; /* 47 clock comparator validity */
+ 		u64    : 16; /* 47-63 */
+ 	};
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
+ };
+ 
+ #define MCESA_ORIGIN_MASK	(~0x3ffUL)
+ #define MCESA_LC_MASK		(0xfUL)
+ 
+ struct mcesa {
+ 	u8 vector_save_area[1024];
+ 	u8 guarded_storage_save_area[32];
  };
  
  struct pt_regs;
diff --cc arch/s390/include/asm/processor.h
index ce3a4381002e,cc101f9371cb..000000000000
--- a/arch/s390/include/asm/processor.h
+++ b/arch/s390/include/asm/processor.h
@@@ -89,11 -135,14 +89,16 @@@ struct thread_struct 
  	struct list_head list;
  	/* cpu runtime instrumentation */
  	struct runtime_instr_cb *ri_cb;
++<<<<<<< HEAD
 +	int ri_signum;
 +#ifdef CONFIG_64BIT
++=======
+ 	struct gs_cb *gs_cb;		/* Current guarded storage cb */
+ 	struct gs_cb *gs_bc_cb;		/* Broadcast guarded storage cb */
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	unsigned char trap_tdb[256];	/* Transaction abort diagnose block */
 -	/*
 -	 * Warning: 'fpu' is dynamically-sized. It *MUST* be at
 -	 * the end.
 -	 */
 -	struct fpu fpu;			/* FP and VX register save area */
 +	RH_KABI_EXTEND(__vector128 *vxrs) /* Vector register save area */
 +#endif
  };
  
  /* Flag to disable transactions. */
diff --cc arch/s390/include/asm/setup.h
index d2f2186ab488,383bd8358a8c..000000000000
--- a/arch/s390/include/asm/setup.h
+++ b/arch/s390/include/asm/setup.h
@@@ -9,7 -10,28 +9,32 @@@
  
  
  #define PARMAREA		0x10400
++<<<<<<< HEAD
 +#define MEMORY_CHUNKS		256
++=======
+ 
+ /*
+  * Machine features detected in early.c
+  */
+ 
+ #define MACHINE_FLAG_VM		_BITUL(0)
+ #define MACHINE_FLAG_KVM	_BITUL(1)
+ #define MACHINE_FLAG_LPAR	_BITUL(2)
+ #define MACHINE_FLAG_DIAG9C	_BITUL(3)
+ #define MACHINE_FLAG_ESOP	_BITUL(4)
+ #define MACHINE_FLAG_IDTE	_BITUL(5)
+ #define MACHINE_FLAG_DIAG44	_BITUL(6)
+ #define MACHINE_FLAG_EDAT1	_BITUL(7)
+ #define MACHINE_FLAG_EDAT2	_BITUL(8)
+ #define MACHINE_FLAG_LPP	_BITUL(9)
+ #define MACHINE_FLAG_TOPOLOGY	_BITUL(10)
+ #define MACHINE_FLAG_TE		_BITUL(11)
+ #define MACHINE_FLAG_TLB_LC	_BITUL(12)
+ #define MACHINE_FLAG_VX		_BITUL(13)
+ #define MACHINE_FLAG_CAD	_BITUL(14)
+ #define MACHINE_FLAG_NX		_BITUL(15)
+ #define MACHINE_FLAG_GS		_BITUL(16)
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  #define LPP_MAGIC		_BITUL(31)
  #define LPP_PFAULT_PID_MASK	_AC(0xffffffff, UL)
@@@ -106,9 -67,11 +131,15 @@@ void create_mem_hole(struct mem_chunk m
  #define MACHINE_HAS_LPP		(S390_lowcore.machine_flags & MACHINE_FLAG_LPP)
  #define MACHINE_HAS_TOPOLOGY	(S390_lowcore.machine_flags & MACHINE_FLAG_TOPOLOGY)
  #define MACHINE_HAS_TE		(S390_lowcore.machine_flags & MACHINE_FLAG_TE)
 -#define MACHINE_HAS_TLB_LC	(S390_lowcore.machine_flags & MACHINE_FLAG_TLB_LC)
 +#define MACHINE_HAS_RRBM	(S390_lowcore.machine_flags & MACHINE_FLAG_RRBM)
  #define MACHINE_HAS_VX		(S390_lowcore.machine_flags & MACHINE_FLAG_VX)
++<<<<<<< HEAD
 +#endif /* CONFIG_64BIT */
++=======
+ #define MACHINE_HAS_CAD		(S390_lowcore.machine_flags & MACHINE_FLAG_CAD)
+ #define MACHINE_HAS_NX		(S390_lowcore.machine_flags & MACHINE_FLAG_NX)
+ #define MACHINE_HAS_GS		(S390_lowcore.machine_flags & MACHINE_FLAG_GS)
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  /*
   * Console mode. Override with conmode=
diff --cc arch/s390/include/asm/switch_to.h
index 95f2cd9c0cc5,f6c2b5814ab0..000000000000
--- a/arch/s390/include/asm/switch_to.h
+++ b/arch/s390/include/asm/switch_to.h
@@@ -8,6 -8,9 +8,12 @@@
  #define __ASM_SWITCH_TO_H
  
  #include <linux/thread_info.h>
++<<<<<<< HEAD
++=======
+ #include <asm/fpu/api.h>
+ #include <asm/ptrace.h>
+ #include <asm/guarded_storage.h>
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  extern struct task_struct *__switch_to(void *, void *);
  extern void update_cr_regs(struct task_struct *task);
@@@ -169,24 -31,19 +175,26 @@@ static inline void restore_access_regs(
  
  #define switch_to(prev,next,last) do {					\
  	if (prev->mm) {							\
 -		save_fpu_regs();					\
 +		save_fp_ctl(&prev->thread.fp_regs.fpc);			\
 +		save_fp_vx_regs(prev);					\
  		save_access_regs(&prev->thread.acrs[0]);		\
  		save_ri_cb(prev->thread.ri_cb);				\
+ 		save_gs_cb(prev->thread.gs_cb);				\
  	}								\
  	if (next->mm) {							\
  		update_cr_regs(next);					\
 -		set_cpu_flag(CIF_FPU);					\
 +		restore_fp_ctl(&next->thread.fp_regs.fpc);		\
 +		restore_fp_vx_regs(next);				\
  		restore_access_regs(&next->thread.acrs[0]);		\
  		restore_ri_cb(next->thread.ri_cb, prev->thread.ri_cb);	\
+ 		restore_gs_cb(next->thread.gs_cb);			\
  	}								\
  	prev = __switch_to(prev,next);					\
 +	update_primary_asce(current);					\
 +} while (0)
 +
 +#define finish_arch_switch(prev) do {					     \
 +	set_fs(current->thread.mm_segment);				     \
  } while (0)
  
  #endif /* __ASM_SWITCH_TO_H */
diff --cc arch/s390/include/asm/thread_info.h
index a04ff06ef83d,f36e6e2b73f0..000000000000
--- a/arch/s390/include/asm/thread_info.h
+++ b/arch/s390/include/asm/thread_info.h
@@@ -77,45 -51,32 +77,74 @@@ static inline struct thread_info *curre
  /*
   * thread information flags bit numbers
   */
++<<<<<<< HEAD
 +#define TIF_SYSCALL		0	/* inside a system call */
 +#define TIF_NOTIFY_RESUME	1	/* callback before returning to user */
 +#define TIF_SIGPENDING		2	/* signal pending */
 +#define TIF_NEED_RESCHED	3	/* rescheduling necessary */
 +#define TIF_UPROBE		4	/* breakpointed or single-stepping */
 +#define TIF_ASCE		5	/* primary asce needs fixup / uaccess */
 +#define TIF_PER_TRAP		6	/* deliver sigtrap on return to user */
 +#define TIF_MCCK_PENDING	7	/* machine check handling is pending */
++=======
+ #define TIF_NOTIFY_RESUME	0	/* callback before returning to user */
+ #define TIF_SIGPENDING		1	/* signal pending */
+ #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
+ #define TIF_UPROBE		3	/* breakpointed or single-stepping */
+ #define TIF_GUARDED_STORAGE	4	/* load guarded storage control block */
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  #define TIF_SYSCALL_TRACE	8	/* syscall trace active */
  #define TIF_SYSCALL_AUDIT	9	/* syscall auditing active */
  #define TIF_SECCOMP		10	/* secure computing */
  #define TIF_SYSCALL_TRACEPOINT	11	/* syscall tracepoint instrumentation */
++<<<<<<< HEAD
 +#define TIF_31BIT		17	/* 32bit process */
 +#define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 +#define TIF_RESTORE_SIGMASK	19	/* restore signal mask in do_signal() */
 +#define TIF_SINGLE_STEP		20	/* This task is single stepped */
 +#define TIF_UPROBE_SINGLESTEP	21	/* This task is uprobe single stepped */
 +
 +#define _TIF_SYSCALL		(1<<TIF_SYSCALL)
 +#define _TIF_NOTIFY_RESUME	(1<<TIF_NOTIFY_RESUME)
 +#define _TIF_SIGPENDING		(1<<TIF_SIGPENDING)
 +#define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 +#define _TIF_ASCE		(1<<TIF_ASCE)
 +#define _TIF_PER_TRAP		(1<<TIF_PER_TRAP)
 +#define _TIF_MCCK_PENDING	(1<<TIF_MCCK_PENDING)
 +#define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
 +#define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
 +#define _TIF_SECCOMP		(1<<TIF_SECCOMP)
 +#define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
 +#define _TIF_UPROBE		(1<<TIF_UPROBE)
 +#define _TIF_31BIT		(1<<TIF_31BIT)
 +#define _TIF_SINGLE_STEP	(1<<TIF_SINGLE_STEP)
 +
 +#ifdef CONFIG_64BIT
 +#define is_32bit_task()		(test_thread_flag(TIF_31BIT))
 +#else
 +#define is_32bit_task()		(1)
 +#endif
 +
 +#define PREEMPT_ACTIVE		0x4000000
++=======
+ #define TIF_31BIT		16	/* 32bit process */
+ #define TIF_MEMDIE		17	/* is terminating due to OOM killer */
+ #define TIF_RESTORE_SIGMASK	18	/* restore signal mask in do_signal() */
+ #define TIF_SINGLE_STEP		19	/* This task is single stepped */
+ #define TIF_BLOCK_STEP		20	/* This task is block stepped */
+ #define TIF_UPROBE_SINGLESTEP	21	/* This task is uprobe single stepped */
+ 
+ #define _TIF_NOTIFY_RESUME	_BITUL(TIF_NOTIFY_RESUME)
+ #define _TIF_SIGPENDING		_BITUL(TIF_SIGPENDING)
+ #define _TIF_NEED_RESCHED	_BITUL(TIF_NEED_RESCHED)
+ #define _TIF_SYSCALL_TRACE	_BITUL(TIF_SYSCALL_TRACE)
+ #define _TIF_SYSCALL_AUDIT	_BITUL(TIF_SYSCALL_AUDIT)
+ #define _TIF_SECCOMP		_BITUL(TIF_SECCOMP)
+ #define _TIF_SYSCALL_TRACEPOINT	_BITUL(TIF_SYSCALL_TRACEPOINT)
+ #define _TIF_UPROBE		_BITUL(TIF_UPROBE)
+ #define _TIF_31BIT		_BITUL(TIF_31BIT)
+ #define _TIF_SINGLE_STEP	_BITUL(TIF_SINGLE_STEP)
+ #define _TIF_GUARDED_STORAGE	_BITUL(TIF_GUARDED_STORAGE)
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  #endif /* _ASM_THREAD_INFO_H */
diff --cc arch/s390/include/uapi/asm/Kbuild
index 6a9a9eb645f5,86b761e583e3..000000000000
--- a/arch/s390/include/uapi/asm/Kbuild
+++ b/arch/s390/include/uapi/asm/Kbuild
@@@ -11,6 -12,8 +11,11 @@@ header-y += dasd.
  header-y += debug.h
  header-y += errno.h
  header-y += fcntl.h
++<<<<<<< HEAD
++=======
+ header-y += guarded_storage.h
+ header-y += hypfs.h
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  header-y += ioctl.h
  header-y += ioctls.h
  header-y += ipcbuf.h
diff --cc arch/s390/include/uapi/asm/unistd.h
index 4e8ebbbe2c05,ea42290e7d51..000000000000
--- a/arch/s390/include/uapi/asm/unistd.h
+++ b/arch/s390/include/uapi/asm/unistd.h
@@@ -280,17 -280,42 +280,25 @@@
  #define __NR_s390_runtime_instr 342
  #define __NR_kcmp		343
  #define __NR_finit_module	344
 -#define __NR_sched_setattr	345
 -#define __NR_sched_getattr	346
 -#define __NR_renameat2		347
 -#define __NR_seccomp		348
 +#define __NR_sched_setattr 345
 +#define __NR_sched_getattr 346
 +/* Number 347 is reserved for sys_renameat2 */
 +/* Number 348 is reserved for sys_seccomp */
  #define __NR_getrandom		349
 -#define __NR_memfd_create	350
 -#define __NR_bpf		351
 +/* Number 350 is reserved for sys_memfd_create */
 +/* Number 351 is reserved for sys_bpf */
  #define __NR_s390_pci_mmio_write	352
  #define __NR_s390_pci_mmio_read		353
 -#define __NR_execveat		354
 -#define __NR_userfaultfd	355
 -#define __NR_membarrier		356
 -#define __NR_recvmmsg		357
 -#define __NR_sendmmsg		358
 -#define __NR_socket		359
 -#define __NR_socketpair		360
 -#define __NR_bind		361
 -#define __NR_connect		362
 -#define __NR_listen		363
 -#define __NR_accept4		364
 -#define __NR_getsockopt		365
 -#define __NR_setsockopt		366
 -#define __NR_getsockname	367
 -#define __NR_getpeername	368
 -#define __NR_sendto		369
 -#define __NR_sendmsg		370
 -#define __NR_recvfrom		371
 -#define __NR_recvmsg		372
 -#define __NR_shutdown		373
 -#define __NR_mlock2		374
  #define __NR_copy_file_range	375
++<<<<<<< HEAD
 +#define NR_syscalls 376
++=======
+ #define __NR_preadv2		376
+ #define __NR_pwritev2		377
+ #define __NR_s390_guarded_storage	378
+ #define __NR_statx		379
+ #define NR_syscalls 380
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  /* 
   * There are some system calls that are not present on 64 bit, some
diff --cc arch/s390/kernel/Makefile
index b1d2cc34f579,aa5adbdaf200..000000000000
--- a/arch/s390/kernel/Makefile
+++ b/arch/s390/kernel/Makefile
@@@ -26,35 -51,29 +26,40 @@@ CFLAGS_dumpstack.o	+= -fno-optimize-sib
  #
  CFLAGS_ptrace.o		+= -DUTS_MACHINE='"$(UTS_MACHINE)"'
  
 -CFLAGS_sysinfo.o	+= -w
 +CFLAGS_sysinfo.o += -Iinclude/math-emu -Iarch/s390/math-emu -w
  
 -obj-y	:= traps.o time.o process.o base.o early.o setup.o idle.o vtime.o
 +obj-y	:= bitmap.o traps.o time.o process.o base.o early.o setup.o vtime.o
  obj-y	+= processor.o sys_s390.o ptrace.o signal.o cpcmd.o ebcdic.o nmi.o
 -obj-y	+= debug.o irq.o ipl.o dis.o diag.o vdso.o als.o
 +obj-y	+= debug.o irq.o ipl.o dis.o diag.o sclp.o vdso.o
  obj-y	+= sysinfo.o jump_label.o lgr.o os_info.o machine_kexec.o pgm_check.o
++<<<<<<< HEAD
 +obj-y	+= dumpstack.o
++=======
+ obj-y	+= runtime_instr.o cache.o fpu.o dumpstack.o guarded_storage.o
+ obj-y	+= entry.o reipl.o relocate_kernel.o
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
 +
 +obj-y	+= $(if $(CONFIG_64BIT),entry64.o,entry.o)
 +obj-y	+= $(if $(CONFIG_64BIT),reipl64.o,reipl.o)
 +obj-y	+= $(if $(CONFIG_64BIT),relocate_kernel64.o,relocate_kernel.o)
  
 -extra-y				+= head.o head64.o vmlinux.lds
 +extra-y				+= head.o vmlinux.lds
 +extra-y				+= $(if $(CONFIG_64BIT),head64.o,head31.o)
  
 -obj-$(CONFIG_MODULES)		+= module.o
 +obj-$(CONFIG_MODULES)		+= s390_ksyms.o module.o
  obj-$(CONFIG_SMP)		+= smp.o
 -obj-$(CONFIG_SCHED_TOPOLOGY)	+= topology.o
 -obj-$(CONFIG_HIBERNATION)	+= suspend.o swsusp.o
 +obj-$(CONFIG_SCHED_BOOK)	+= topology.o
 +obj-$(CONFIG_HIBERNATION)	+= suspend.o swsusp_asm64.o
  obj-$(CONFIG_AUDIT)		+= audit.o
  compat-obj-$(CONFIG_AUDIT)	+= compat_audit.o
 -obj-$(CONFIG_COMPAT)		+= compat_linux.o compat_signal.o
 -obj-$(CONFIG_COMPAT)		+= compat_wrapper.o $(compat-obj-y)
 -obj-$(CONFIG_EARLY_PRINTK)	+= early_printk.o
 +obj-$(CONFIG_COMPAT)		+= compat_linux.o compat_signal.o \
 +					compat_wrapper.o compat_exec_domain.o \
 +					$(compat-obj-y)
 +
  obj-$(CONFIG_STACKTRACE)	+= stacktrace.o
  obj-$(CONFIG_KPROBES)		+= kprobes.o
 -obj-$(CONFIG_FUNCTION_TRACER)	+= mcount.o ftrace.o
 +obj-$(CONFIG_FUNCTION_TRACER)	+= $(if $(CONFIG_64BIT),mcount64.o,mcount.o)
 +obj-$(CONFIG_FUNCTION_TRACER)	+= ftrace.o
  obj-$(CONFIG_CRASH_DUMP)	+= crash_dump.o
  obj-$(CONFIG_UPROBES)		+= uprobes.o
  
diff --cc arch/s390/kernel/asm-offsets.c
index 3484ac363969,6bb29633e1f1..000000000000
--- a/arch/s390/kernel/asm-offsets.c
+++ b/arch/s390/kernel/asm-offsets.c
@@@ -71,106 -84,114 +71,201 @@@ int main(void
  	/* constants used by the vdso */
  	DEFINE(__CLOCK_REALTIME, CLOCK_REALTIME);
  	DEFINE(__CLOCK_MONOTONIC, CLOCK_MONOTONIC);
 -	DEFINE(__CLOCK_REALTIME_COARSE, CLOCK_REALTIME_COARSE);
 -	DEFINE(__CLOCK_MONOTONIC_COARSE, CLOCK_MONOTONIC_COARSE);
  	DEFINE(__CLOCK_THREAD_CPUTIME_ID, CLOCK_THREAD_CPUTIME_ID);
  	DEFINE(__CLOCK_REALTIME_RES, MONOTONIC_RES_NSEC);
 -	DEFINE(__CLOCK_COARSE_RES, LOW_RES_NSEC);
  	BLANK();
  	/* idle data offsets */
 -	OFFSET(__CLOCK_IDLE_ENTER, s390_idle_data, clock_idle_enter);
 -	OFFSET(__CLOCK_IDLE_EXIT, s390_idle_data, clock_idle_exit);
 -	OFFSET(__TIMER_IDLE_ENTER, s390_idle_data, timer_idle_enter);
 -	OFFSET(__TIMER_IDLE_EXIT, s390_idle_data, timer_idle_exit);
 +	DEFINE(__CLOCK_IDLE_ENTER, offsetof(struct s390_idle_data, clock_idle_enter));
 +	DEFINE(__CLOCK_IDLE_EXIT, offsetof(struct s390_idle_data, clock_idle_exit));
 +	DEFINE(__TIMER_IDLE_ENTER, offsetof(struct s390_idle_data, timer_idle_enter));
 +	DEFINE(__TIMER_IDLE_EXIT, offsetof(struct s390_idle_data, timer_idle_exit));
 +	/* lowcore offsets */
 +	DEFINE(__LC_EXT_PARAMS, offsetof(struct _lowcore, ext_params));
 +	DEFINE(__LC_EXT_CPU_ADDR, offsetof(struct _lowcore, ext_cpu_addr));
 +	DEFINE(__LC_EXT_INT_CODE, offsetof(struct _lowcore, ext_int_code));
 +	DEFINE(__LC_SVC_ILC, offsetof(struct _lowcore, svc_ilc));
 +	DEFINE(__LC_SVC_INT_CODE, offsetof(struct _lowcore, svc_code));
 +	DEFINE(__LC_PGM_ILC, offsetof(struct _lowcore, pgm_ilc));
 +	DEFINE(__LC_PGM_INT_CODE, offsetof(struct _lowcore, pgm_code));
 +	DEFINE(__LC_TRANS_EXC_CODE, offsetof(struct _lowcore, trans_exc_code));
 +	DEFINE(__LC_PER_CAUSE, offsetof(struct _lowcore, per_perc_atmid));
 +	DEFINE(__LC_PER_ADDRESS, offsetof(struct _lowcore, per_address));
 +	DEFINE(__LC_PER_PAID, offsetof(struct _lowcore, per_access_id));
 +	DEFINE(__LC_AR_MODE_ID, offsetof(struct _lowcore, ar_access_id));
 +	DEFINE(__LC_SUBCHANNEL_ID, offsetof(struct _lowcore, subchannel_id));
 +	DEFINE(__LC_SUBCHANNEL_NR, offsetof(struct _lowcore, subchannel_nr));
 +	DEFINE(__LC_IO_INT_PARM, offsetof(struct _lowcore, io_int_parm));
 +	DEFINE(__LC_IO_INT_WORD, offsetof(struct _lowcore, io_int_word));
 +	DEFINE(__LC_STFL_FAC_LIST, offsetof(struct _lowcore, stfl_fac_list));
 +	DEFINE(__LC_MCCK_CODE, offsetof(struct _lowcore, mcck_interruption_code));
 +	DEFINE(__LC_RST_OLD_PSW, offsetof(struct _lowcore, restart_old_psw));
 +	DEFINE(__LC_EXT_OLD_PSW, offsetof(struct _lowcore, external_old_psw));
 +	DEFINE(__LC_SVC_OLD_PSW, offsetof(struct _lowcore, svc_old_psw));
 +	DEFINE(__LC_PGM_OLD_PSW, offsetof(struct _lowcore, program_old_psw));
 +	DEFINE(__LC_MCK_OLD_PSW, offsetof(struct _lowcore, mcck_old_psw));
 +	DEFINE(__LC_IO_OLD_PSW, offsetof(struct _lowcore, io_old_psw));
 +	DEFINE(__LC_RST_NEW_PSW, offsetof(struct _lowcore, restart_psw));
 +	DEFINE(__LC_EXT_NEW_PSW, offsetof(struct _lowcore, external_new_psw));
 +	DEFINE(__LC_SVC_NEW_PSW, offsetof(struct _lowcore, svc_new_psw));
 +	DEFINE(__LC_PGM_NEW_PSW, offsetof(struct _lowcore, program_new_psw));
 +	DEFINE(__LC_MCK_NEW_PSW, offsetof(struct _lowcore, mcck_new_psw));
 +	DEFINE(__LC_IO_NEW_PSW, offsetof(struct _lowcore, io_new_psw));
  	BLANK();
++<<<<<<< HEAD
 +	DEFINE(__LC_SAVE_AREA_SYNC, offsetof(struct _lowcore, save_area_sync));
 +	DEFINE(__LC_SAVE_AREA_ASYNC, offsetof(struct _lowcore, save_area_async));
 +	DEFINE(__LC_SAVE_AREA_RESTART, offsetof(struct _lowcore, save_area_restart));
 +	DEFINE(__LC_RETURN_PSW, offsetof(struct _lowcore, return_psw));
 +	DEFINE(__LC_RETURN_MCCK_PSW, offsetof(struct _lowcore, return_mcck_psw));
 +	DEFINE(__LC_SYNC_ENTER_TIMER, offsetof(struct _lowcore, sync_enter_timer));
 +	DEFINE(__LC_ASYNC_ENTER_TIMER, offsetof(struct _lowcore, async_enter_timer));
 +	DEFINE(__LC_MCCK_ENTER_TIMER, offsetof(struct _lowcore, mcck_enter_timer));
 +	DEFINE(__LC_EXIT_TIMER, offsetof(struct _lowcore, exit_timer));
 +	DEFINE(__LC_USER_TIMER, offsetof(struct _lowcore, user_timer));
 +	DEFINE(__LC_SYSTEM_TIMER, offsetof(struct _lowcore, system_timer));
 +	DEFINE(__LC_STEAL_TIMER, offsetof(struct _lowcore, steal_timer));
 +	DEFINE(__LC_LAST_UPDATE_TIMER, offsetof(struct _lowcore, last_update_timer));
 +	DEFINE(__LC_LAST_UPDATE_CLOCK, offsetof(struct _lowcore, last_update_clock));
 +	DEFINE(__LC_CURRENT, offsetof(struct _lowcore, current_task));
 +#ifdef CONFIG_64BIT
 +	DEFINE(__LC_LPP, offsetof(struct _lowcore, lpp));
 +#endif
 +	DEFINE(__LC_CURRENT_PID, offsetof(struct _lowcore, current_pid));
 +	DEFINE(__LC_THREAD_INFO, offsetof(struct _lowcore, thread_info));
 +	DEFINE(__LC_KERNEL_STACK, offsetof(struct _lowcore, kernel_stack));
 +	DEFINE(__LC_ASYNC_STACK, offsetof(struct _lowcore, async_stack));
 +	DEFINE(__LC_PANIC_STACK, offsetof(struct _lowcore, panic_stack));
 +	DEFINE(__LC_RESTART_STACK, offsetof(struct _lowcore, restart_stack));
 +	DEFINE(__LC_RESTART_FN, offsetof(struct _lowcore, restart_fn));
 +	DEFINE(__LC_RESTART_DATA, offsetof(struct _lowcore, restart_data));
 +	DEFINE(__LC_RESTART_SOURCE, offsetof(struct _lowcore, restart_source));
 +	DEFINE(__LC_KERNEL_ASCE, offsetof(struct _lowcore, kernel_asce));
 +	DEFINE(__LC_USER_ASCE, offsetof(struct _lowcore, user_asce));
 +	DEFINE(__LC_INT_CLOCK, offsetof(struct _lowcore, int_clock));
 +	DEFINE(__LC_MCCK_CLOCK, offsetof(struct _lowcore, mcck_clock));
 +	DEFINE(__LC_MACHINE_FLAGS, offsetof(struct _lowcore, machine_flags));
 +	DEFINE(__LC_FTRACE_FUNC, offsetof(struct _lowcore, ftrace_func));
 +	DEFINE(__LC_IRB, offsetof(struct _lowcore, irb));
 +	DEFINE(__LC_DUMP_REIPL, offsetof(struct _lowcore, ipib));
++=======
+ 	/* hardware defined lowcore locations 0x000 - 0x1ff */
+ 	OFFSET(__LC_EXT_PARAMS, lowcore, ext_params);
+ 	OFFSET(__LC_EXT_CPU_ADDR, lowcore, ext_cpu_addr);
+ 	OFFSET(__LC_EXT_INT_CODE, lowcore, ext_int_code);
+ 	OFFSET(__LC_SVC_ILC, lowcore, svc_ilc);
+ 	OFFSET(__LC_SVC_INT_CODE, lowcore, svc_code);
+ 	OFFSET(__LC_PGM_ILC, lowcore, pgm_ilc);
+ 	OFFSET(__LC_PGM_INT_CODE, lowcore, pgm_code);
+ 	OFFSET(__LC_DATA_EXC_CODE, lowcore, data_exc_code);
+ 	OFFSET(__LC_MON_CLASS_NR, lowcore, mon_class_num);
+ 	OFFSET(__LC_PER_CODE, lowcore, per_code);
+ 	OFFSET(__LC_PER_ATMID, lowcore, per_atmid);
+ 	OFFSET(__LC_PER_ADDRESS, lowcore, per_address);
+ 	OFFSET(__LC_EXC_ACCESS_ID, lowcore, exc_access_id);
+ 	OFFSET(__LC_PER_ACCESS_ID, lowcore, per_access_id);
+ 	OFFSET(__LC_OP_ACCESS_ID, lowcore, op_access_id);
+ 	OFFSET(__LC_AR_MODE_ID, lowcore, ar_mode_id);
+ 	OFFSET(__LC_TRANS_EXC_CODE, lowcore, trans_exc_code);
+ 	OFFSET(__LC_MON_CODE, lowcore, monitor_code);
+ 	OFFSET(__LC_SUBCHANNEL_ID, lowcore, subchannel_id);
+ 	OFFSET(__LC_SUBCHANNEL_NR, lowcore, subchannel_nr);
+ 	OFFSET(__LC_IO_INT_PARM, lowcore, io_int_parm);
+ 	OFFSET(__LC_IO_INT_WORD, lowcore, io_int_word);
+ 	OFFSET(__LC_STFL_FAC_LIST, lowcore, stfl_fac_list);
+ 	OFFSET(__LC_STFLE_FAC_LIST, lowcore, stfle_fac_list);
+ 	OFFSET(__LC_MCCK_CODE, lowcore, mcck_interruption_code);
+ 	OFFSET(__LC_EXT_DAMAGE_CODE, lowcore, external_damage_code);
+ 	OFFSET(__LC_MCCK_FAIL_STOR_ADDR, lowcore, failing_storage_address);
+ 	OFFSET(__LC_LAST_BREAK, lowcore, breaking_event_addr);
+ 	OFFSET(__LC_RST_OLD_PSW, lowcore, restart_old_psw);
+ 	OFFSET(__LC_EXT_OLD_PSW, lowcore, external_old_psw);
+ 	OFFSET(__LC_SVC_OLD_PSW, lowcore, svc_old_psw);
+ 	OFFSET(__LC_PGM_OLD_PSW, lowcore, program_old_psw);
+ 	OFFSET(__LC_MCK_OLD_PSW, lowcore, mcck_old_psw);
+ 	OFFSET(__LC_IO_OLD_PSW, lowcore, io_old_psw);
+ 	OFFSET(__LC_RST_NEW_PSW, lowcore, restart_psw);
+ 	OFFSET(__LC_EXT_NEW_PSW, lowcore, external_new_psw);
+ 	OFFSET(__LC_SVC_NEW_PSW, lowcore, svc_new_psw);
+ 	OFFSET(__LC_PGM_NEW_PSW, lowcore, program_new_psw);
+ 	OFFSET(__LC_MCK_NEW_PSW, lowcore, mcck_new_psw);
+ 	OFFSET(__LC_IO_NEW_PSW, lowcore, io_new_psw);
+ 	/* software defined lowcore locations 0x200 - 0xdff*/
+ 	OFFSET(__LC_SAVE_AREA_SYNC, lowcore, save_area_sync);
+ 	OFFSET(__LC_SAVE_AREA_ASYNC, lowcore, save_area_async);
+ 	OFFSET(__LC_SAVE_AREA_RESTART, lowcore, save_area_restart);
+ 	OFFSET(__LC_CPU_FLAGS, lowcore, cpu_flags);
+ 	OFFSET(__LC_RETURN_PSW, lowcore, return_psw);
+ 	OFFSET(__LC_RETURN_MCCK_PSW, lowcore, return_mcck_psw);
+ 	OFFSET(__LC_SYNC_ENTER_TIMER, lowcore, sync_enter_timer);
+ 	OFFSET(__LC_ASYNC_ENTER_TIMER, lowcore, async_enter_timer);
+ 	OFFSET(__LC_MCCK_ENTER_TIMER, lowcore, mcck_enter_timer);
+ 	OFFSET(__LC_EXIT_TIMER, lowcore, exit_timer);
+ 	OFFSET(__LC_USER_TIMER, lowcore, user_timer);
+ 	OFFSET(__LC_SYSTEM_TIMER, lowcore, system_timer);
+ 	OFFSET(__LC_STEAL_TIMER, lowcore, steal_timer);
+ 	OFFSET(__LC_LAST_UPDATE_TIMER, lowcore, last_update_timer);
+ 	OFFSET(__LC_LAST_UPDATE_CLOCK, lowcore, last_update_clock);
+ 	OFFSET(__LC_INT_CLOCK, lowcore, int_clock);
+ 	OFFSET(__LC_MCCK_CLOCK, lowcore, mcck_clock);
+ 	OFFSET(__LC_CURRENT, lowcore, current_task);
+ 	OFFSET(__LC_KERNEL_STACK, lowcore, kernel_stack);
+ 	OFFSET(__LC_ASYNC_STACK, lowcore, async_stack);
+ 	OFFSET(__LC_PANIC_STACK, lowcore, panic_stack);
+ 	OFFSET(__LC_RESTART_STACK, lowcore, restart_stack);
+ 	OFFSET(__LC_RESTART_FN, lowcore, restart_fn);
+ 	OFFSET(__LC_RESTART_DATA, lowcore, restart_data);
+ 	OFFSET(__LC_RESTART_SOURCE, lowcore, restart_source);
+ 	OFFSET(__LC_USER_ASCE, lowcore, user_asce);
+ 	OFFSET(__LC_LPP, lowcore, lpp);
+ 	OFFSET(__LC_CURRENT_PID, lowcore, current_pid);
+ 	OFFSET(__LC_PERCPU_OFFSET, lowcore, percpu_offset);
+ 	OFFSET(__LC_VDSO_PER_CPU, lowcore, vdso_per_cpu_data);
+ 	OFFSET(__LC_MACHINE_FLAGS, lowcore, machine_flags);
+ 	OFFSET(__LC_PREEMPT_COUNT, lowcore, preempt_count);
+ 	OFFSET(__LC_GMAP, lowcore, gmap);
+ 	OFFSET(__LC_PASTE, lowcore, paste);
+ 	/* software defined ABI-relevant lowcore locations 0xe00 - 0xe20 */
+ 	OFFSET(__LC_DUMP_REIPL, lowcore, ipib);
+ 	/* hardware defined lowcore locations 0x1000 - 0x18ff */
+ 	OFFSET(__LC_MCESAD, lowcore, mcesad);
+ 	OFFSET(__LC_EXT_PARAMS2, lowcore, ext_params2);
+ 	OFFSET(__LC_FPREGS_SAVE_AREA, lowcore, floating_pt_save_area);
+ 	OFFSET(__LC_GPREGS_SAVE_AREA, lowcore, gpregs_save_area);
+ 	OFFSET(__LC_PSW_SAVE_AREA, lowcore, psw_save_area);
+ 	OFFSET(__LC_PREFIX_SAVE_AREA, lowcore, prefixreg_save_area);
+ 	OFFSET(__LC_FP_CREG_SAVE_AREA, lowcore, fpt_creg_save_area);
+ 	OFFSET(__LC_TOD_PROGREG_SAVE_AREA, lowcore, tod_progreg_save_area);
+ 	OFFSET(__LC_CPU_TIMER_SAVE_AREA, lowcore, cpu_timer_save_area);
+ 	OFFSET(__LC_CLOCK_COMP_SAVE_AREA, lowcore, clock_comp_save_area);
+ 	OFFSET(__LC_AREGS_SAVE_AREA, lowcore, access_regs_save_area);
+ 	OFFSET(__LC_CREGS_SAVE_AREA, lowcore, cregs_save_area);
+ 	OFFSET(__LC_PGM_TDB, lowcore, pgm_tdb);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	BLANK();
 -	/* gmap/sie offsets */
 -	OFFSET(__GMAP_ASCE, gmap, asce);
 -	OFFSET(__SIE_PROG0C, kvm_s390_sie_block, prog0c);
 -	OFFSET(__SIE_PROG20, kvm_s390_sie_block, prog20);
 +	DEFINE(__LC_CPU_TIMER_SAVE_AREA, offsetof(struct _lowcore, cpu_timer_save_area));
 +	DEFINE(__LC_CLOCK_COMP_SAVE_AREA, offsetof(struct _lowcore, clock_comp_save_area));
 +	DEFINE(__LC_PSW_SAVE_AREA, offsetof(struct _lowcore, psw_save_area));
 +	DEFINE(__LC_PREFIX_SAVE_AREA, offsetof(struct _lowcore, prefixreg_save_area));
 +	DEFINE(__LC_AREGS_SAVE_AREA, offsetof(struct _lowcore, access_regs_save_area));
 +	DEFINE(__LC_FPREGS_SAVE_AREA, offsetof(struct _lowcore, floating_pt_save_area));
 +	DEFINE(__LC_GPREGS_SAVE_AREA, offsetof(struct _lowcore, gpregs_save_area));
 +	DEFINE(__LC_CREGS_SAVE_AREA, offsetof(struct _lowcore, cregs_save_area));
 +#ifdef CONFIG_32BIT
 +	DEFINE(SAVE_AREA_BASE, offsetof(struct _lowcore, extended_save_area_addr));
 +#else /* CONFIG_32BIT */
 +	DEFINE(__LC_EXT_PARAMS2, offsetof(struct _lowcore, ext_params2));
 +	DEFINE(SAVE_AREA_BASE, offsetof(struct _lowcore, floating_pt_save_area));
 +	DEFINE(__LC_PASTE, offsetof(struct _lowcore, paste));
 +	DEFINE(__LC_FP_CREG_SAVE_AREA, offsetof(struct _lowcore, fpt_creg_save_area));
 +	DEFINE(__LC_LAST_BREAK, offsetof(struct _lowcore, breaking_event_addr));
 +	DEFINE(__LC_PERCPU_OFFSET, offsetof(struct _lowcore, percpu_offset));
 +	DEFINE(__LC_VDSO_PER_CPU, offsetof(struct _lowcore, vdso_per_cpu_data));
 +	DEFINE(__LC_GMAP, offsetof(struct _lowcore, gmap));
 +	DEFINE(__LC_PGM_TDB, offsetof(struct _lowcore, pgm_tdb));
 +	DEFINE(__THREAD_trap_tdb, offsetof(struct task_struct, thread.trap_tdb));
 +	DEFINE(__GMAP_ASCE, offsetof(struct gmap, asce));
 +	DEFINE(__SIE_PROG0C, offsetof(struct kvm_s390_sie_block, prog0c));
 +	DEFINE(__SIE_PROG20, offsetof(struct kvm_s390_sie_block, prog20));
 +#endif /* CONFIG_32BIT */
  	return 0;
  }
diff --cc arch/s390/kernel/early.c
index 205828171666,95298a41076f..000000000000
--- a/arch/s390/kernel/early.c
+++ b/arch/s390/kernel/early.c
@@@ -384,10 -348,25 +384,30 @@@ static __init void detect_machine_facil
  		S390_lowcore.machine_flags |= MACHINE_FLAG_LPP;
  	if (test_facility(50) && test_facility(73))
  		S390_lowcore.machine_flags |= MACHINE_FLAG_TE;
++<<<<<<< HEAD
 +	if (test_facility(66))
 +		S390_lowcore.machine_flags |= MACHINE_FLAG_RRBM;
++=======
+ 	if (test_facility(51))
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_TLB_LC;
+ 	if (test_facility(129)) {
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_VX;
+ 		__ctl_set_bit(0, 17);
+ 	}
+ 	if (test_facility(130)) {
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_NX;
+ 		__ctl_set_bit(0, 20);
+ 	}
+ 	if (test_facility(133))
+ 		S390_lowcore.machine_flags |= MACHINE_FLAG_GS;
+ }
+ 
+ static inline void save_vector_registers(void)
+ {
+ #ifdef CONFIG_CRASH_DUMP
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	if (test_facility(129))
 -		save_vx_regs(boot_cpu_vector_save_area);
 +		S390_lowcore.machine_flags |= MACHINE_FLAG_VX;
  #endif
  }
  
diff --cc arch/s390/kernel/entry.S
index 9fd2c1f12630,fa8b8f28e08b..000000000000
--- a/arch/s390/kernel/entry.S
+++ b/arch/s390/kernel/entry.S
@@@ -18,36 -19,42 +18,48 @@@
  #include <asm/unistd.h>
  #include <asm/page.h>
  #include <asm/sigp.h>
 -#include <asm/irq.h>
 -#include <asm/vx-insn.h>
 -#include <asm/setup.h>
 -#include <asm/nmi.h>
 -#include <asm/export.h>
  
  __PT_R0      =	__PT_GPRS
 -__PT_R1      =	__PT_GPRS + 8
 -__PT_R2      =	__PT_GPRS + 16
 -__PT_R3      =	__PT_GPRS + 24
 -__PT_R4      =	__PT_GPRS + 32
 -__PT_R5      =	__PT_GPRS + 40
 -__PT_R6      =	__PT_GPRS + 48
 -__PT_R7      =	__PT_GPRS + 56
 -__PT_R8      =	__PT_GPRS + 64
 -__PT_R9      =	__PT_GPRS + 72
 -__PT_R10     =	__PT_GPRS + 80
 -__PT_R11     =	__PT_GPRS + 88
 -__PT_R12     =	__PT_GPRS + 96
 -__PT_R13     =	__PT_GPRS + 104
 -__PT_R14     =	__PT_GPRS + 112
 -__PT_R15     =	__PT_GPRS + 120
 -
 -STACK_SHIFT = PAGE_SHIFT + THREAD_SIZE_ORDER
 +__PT_R1      =	__PT_GPRS + 4
 +__PT_R2      =	__PT_GPRS + 8
 +__PT_R3      =	__PT_GPRS + 12
 +__PT_R4      =	__PT_GPRS + 16
 +__PT_R5      =	__PT_GPRS + 20
 +__PT_R6      =	__PT_GPRS + 24
 +__PT_R7      =	__PT_GPRS + 28
 +__PT_R8      =	__PT_GPRS + 32
 +__PT_R9      =	__PT_GPRS + 36
 +__PT_R10     =	__PT_GPRS + 40
 +__PT_R11     =	__PT_GPRS + 44
 +__PT_R12     =	__PT_GPRS + 48
 +__PT_R13     =	__PT_GPRS + 524
 +__PT_R14     =	__PT_GPRS + 56
 +__PT_R15     =	__PT_GPRS + 60
 +
 +_TIF_WORK_SVC = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 +		 _TIF_MCCK_PENDING | _TIF_PER_TRAP | _TIF_ASCE)
 +_TIF_WORK_INT = (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
 +		 _TIF_MCCK_PENDING | _TIF_ASCE)
 +_TIF_TRACE    = (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | \
 +		 _TIF_SYSCALL_TRACEPOINT)
 +
 +STACK_SHIFT = PAGE_SHIFT + THREAD_ORDER
  STACK_SIZE  = 1 << STACK_SHIFT
 -STACK_INIT = STACK_SIZE - STACK_FRAME_OVERHEAD - __PT_SIZE
 +STACK_INIT  = STACK_SIZE - STACK_FRAME_OVERHEAD - __PT_SIZE
  
++<<<<<<< HEAD
 +#define BASED(name) name-system_call(%r13)
++=======
+ _TIF_WORK	= (_TIF_SIGPENDING | _TIF_NOTIFY_RESUME | _TIF_NEED_RESCHED | \
+ 		   _TIF_UPROBE | _TIF_GUARDED_STORAGE)
+ _TIF_TRACE	= (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | _TIF_SECCOMP | \
+ 		   _TIF_SYSCALL_TRACEPOINT)
+ _CIF_WORK	= (_CIF_MCCK_PENDING | _CIF_ASCE_PRIMARY | \
+ 		   _CIF_ASCE_SECONDARY | _CIF_FPU)
+ _PIF_WORK	= (_PIF_PER_TRAP)
+ 
+ #define BASED(name) name-cleanup_critical(%r13)
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  	.macro	TRACE_IRQS_ON
  #ifdef CONFIG_TRACE_IRQFLAGS
@@@ -226,20 -323,28 +238,45 @@@ sysc_done
  #
  # One of the work bits is on. Find out which one.
  #
++<<<<<<< HEAD
 +sysc_work:
 +	tm	__TI_flags+3(%r12),_TIF_MCCK_PENDING
 +	jo	sysc_mcck_pending
 +	tm	__TI_flags+3(%r12),_TIF_NEED_RESCHED
 +	jo	sysc_reschedule
 +	tm	__TI_flags+3(%r12),_TIF_PER_TRAP
 +	jo	sysc_singlestep
 +	tm	__TI_flags+3(%r12),_TIF_SIGPENDING
 +	jo	sysc_sigpending
 +	tm	__TI_flags+3(%r12),_TIF_NOTIFY_RESUME
 +	jo	sysc_notify_resume
 +	tm	__TI_flags+3(%r12),_TIF_ASCE
 +	jo	sysc_uaccess
 +	j	sysc_return		# beware of critical section cleanup
++=======
+ .Lsysc_work:
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_MCCK_PENDING
+ 	jo	.Lsysc_mcck_pending
+ 	TSTMSK	__TI_flags(%r12),_TIF_NEED_RESCHED
+ 	jo	.Lsysc_reschedule
+ #ifdef CONFIG_UPROBES
+ 	TSTMSK	__TI_flags(%r12),_TIF_UPROBE
+ 	jo	.Lsysc_uprobe_notify
+ #endif
+ 	TSTMSK	__TI_flags(%r12),_TIF_GUARDED_STORAGE
+ 	jo	.Lsysc_guarded_storage
+ 	TSTMSK	__PT_FLAGS(%r11),_PIF_PER_TRAP
+ 	jo	.Lsysc_singlestep
+ 	TSTMSK	__TI_flags(%r12),_TIF_SIGPENDING
+ 	jo	.Lsysc_sigpending
+ 	TSTMSK	__TI_flags(%r12),_TIF_NOTIFY_RESUME
+ 	jo	.Lsysc_notify_resume
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU
+ 	jo	.Lsysc_vxrs
+ 	TSTMSK	__LC_CPU_FLAGS,(_CIF_ASCE_PRIMARY|_CIF_ASCE_SECONDARY)
+ 	jnz	.Lsysc_asce
+ 	j	.Lsysc_return		# beware of critical section cleanup
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  #
  # _TIF_NEED_RESCHED is set, call schedule
@@@ -286,21 -395,37 +323,47 @@@ sysc_sigpending
  #
  # _TIF_NOTIFY_RESUME is set, call do_notify_resume
  #
 -.Lsysc_notify_resume:
 -	lgr	%r2,%r11		# pass pointer to pt_regs
 -	larl	%r14,.Lsysc_return
 -	jg	do_notify_resume
 +sysc_notify_resume:
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	l	%r1,BASED(.Ldo_notify_resume)
 +	la	%r14,BASED(sysc_return)
 +	br	%r1			# call do_notify_resume
  
  #
 -# _TIF_UPROBE is set, call uprobe_notify_resume
 +# _TIF_PER_TRAP is set, call do_per_trap
  #
++<<<<<<< HEAD
 +sysc_singlestep:
 +	ni	__TI_flags+3(%r12),255-_TIF_PER_TRAP
 +	lr	%r2,%r11		# pass pointer to pt_regs
 +	l	%r1,BASED(.Ldo_per_trap)
 +	la	%r14,BASED(sysc_return)
 +	br	%r1			# call do_per_trap
++=======
+ #ifdef CONFIG_UPROBES
+ .Lsysc_uprobe_notify:
+ 	lgr	%r2,%r11		# pass pointer to pt_regs
+ 	larl	%r14,.Lsysc_return
+ 	jg	uprobe_notify_resume
+ #endif
+ 
+ #
+ # _TIF_GUARDED_STORAGE is set, call guarded_storage_load
+ #
+ .Lsysc_guarded_storage:
+ 	lgr	%r2,%r11		# pass pointer to pt_regs
+ 	larl	%r14,.Lsysc_return
+ 	jg	gs_load_bc_cb
+ 
+ #
+ # _PIF_PER_TRAP is set, call do_per_trap
+ #
+ .Lsysc_singlestep:
+ 	ni	__PT_FLAGS+7(%r11),255-_PIF_PER_TRAP
+ 	lgr	%r2,%r11		# pass pointer to pt_regs
+ 	larl	%r14,.Lsysc_return
+ 	jg	do_per_trap
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  #
  # call tracehook_report_syscall_entry/tracehook_report_syscall_exit before
@@@ -503,40 -663,63 +566,71 @@@ io_work_user
  
  #
  # One of the work bits is on. Find out which one.
 +# Checked are: _TIF_SIGPENDING, _TIF_NOTIFY_RESUME, _TIF_NEED_RESCHED
 +#		and _TIF_MCCK_PENDING
  #
++<<<<<<< HEAD
 +io_work_tif:
 +	tm	__TI_flags+3(%r12),_TIF_MCCK_PENDING
 +	jo	io_mcck_pending
 +	tm	__TI_flags+3(%r12),_TIF_NEED_RESCHED
 +	jo	io_reschedule
 +	tm	__TI_flags+3(%r12),_TIF_SIGPENDING
 +	jo	io_sigpending
 +	tm	__TI_flags+3(%r12),_TIF_NOTIFY_RESUME
 +	jo	io_notify_resume
 +	tm	__TI_flags+3(%r12),_TIF_ASCE
 +	jo	io_uaccess
 +	j	io_return		# beware of critical section cleanup
++=======
+ .Lio_work_tif:
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_MCCK_PENDING
+ 	jo	.Lio_mcck_pending
+ 	TSTMSK	__TI_flags(%r12),_TIF_NEED_RESCHED
+ 	jo	.Lio_reschedule
+ 	TSTMSK	__TI_flags(%r12),_TIF_SIGPENDING
+ 	jo	.Lio_sigpending
+ 	TSTMSK	__TI_flags(%r12),_TIF_NOTIFY_RESUME
+ 	jo	.Lio_notify_resume
+ 	TSTMSK	__TI_flags(%r12),_TIF_GUARDED_STORAGE
+ 	jo	.Lio_guarded_storage
+ 	TSTMSK	__LC_CPU_FLAGS,_CIF_FPU
+ 	jo	.Lio_vxrs
+ 	TSTMSK	__LC_CPU_FLAGS,(_CIF_ASCE_PRIMARY|_CIF_ASCE_SECONDARY)
+ 	jnz	.Lio_asce
+ 	j	.Lio_return		# beware of critical section cleanup
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  #
 -# _CIF_MCCK_PENDING is set, call handler
 +# _TIF_MCCK_PENDING is set, call handler
  #
 -.Lio_mcck_pending:
 -	# TRACE_IRQS_ON already done at .Lio_return
 -	brasl	%r14,s390_handle_mcck	# TIF bit will be cleared by handler
 +io_mcck_pending:
 +	# TRACE_IRQS_ON already done at io_return
 +	l	%r1,BASED(.Lhandle_mcck)
 +	basr	%r14,%r1		# TIF bit will be cleared by handler
  	TRACE_IRQS_OFF
 -	j	.Lio_return
 -
 -#
 -# _CIF_ASCE_PRIMARY and/or CIF_ASCE_SECONDARY set, load user space asce
 -#
 -.Lio_asce:
 -	ni	__LC_CPU_FLAGS+7,255-_CIF_ASCE_PRIMARY
 -	lctlg	%c1,%c1,__LC_USER_ASCE		# load primary asce
 -	TSTMSK	__LC_CPU_FLAGS,_CIF_ASCE_SECONDARY
 -	jz	.Lio_return
 -	larl	%r14,.Lio_return
 -	jg	set_fs_fixup
 +	j	io_return
  
  #
 -# CIF_FPU is set, restore floating-point controls and floating-point registers.
 +# _TIF_ASCE is set, load user space asce
  #
 -.Lio_vxrs:
 -	larl	%r14,.Lio_return
 -	jg	load_fpu_regs
 +io_uaccess:
 +	ni	__TI_flags+3(%r12),255-_TIF_ASCE
 +	lctl	%c1,%c1,__LC_USER_ASCE	# load primary asce
 +	j	io_return
  
+ #
+ # _TIF_GUARDED_STORAGE is set, call guarded_storage_load
+ #
+ .Lio_guarded_storage:
+ 	# TRACE_IRQS_ON already done at .Lio_return
+ 	ssm	__LC_SVC_NEW_PSW	# reenable interrupts
+ 	lgr	%r2,%r11		# pass pointer to pt_regs
+ 	brasl	%r14,gs_load_bc_cb
+ 	ssm	__LC_PGM_NEW_PSW	# disable I/O and ext. interrupts
+ 	TRACE_IRQS_OFF
+ 	j	.Lio_return
+ 
  #
  # _TIF_NEED_RESCHED is set, call schedule
  #
diff --cc arch/s390/kernel/entry.h
index 32c5e9733fb4,dbf5f7e18246..000000000000
--- a/arch/s390/kernel/entry.h
+++ b/arch/s390/kernel/entry.h
@@@ -68,20 -69,19 +68,34 @@@ struct s390_mmap_arg_struct
  struct fadvise64_64_args;
  struct old_sigaction;
  
 -long sys_rt_sigreturn(void);
 +long sys_mmap2(struct s390_mmap_arg_struct __user  *arg);
 +long sys_s390_ipc(uint call, int first, unsigned long second,
 +	     unsigned long third, void __user *ptr);
 +long sys_s390_personality(unsigned int personality);
++<<<<<<< HEAD
 +long sys_s390_fadvise64(int fd, u32 offset_high, u32 offset_low,
 +		    size_t len, int advice);
 +long sys_s390_fadvise64_64(struct fadvise64_64_args __user *args);
 +long sys_s390_fallocate(int fd, int mode, loff_t offset, u32 len_high,
 +			u32 len_low);
  long sys_sigreturn(void);
 +long sys_rt_sigreturn(void);
 +long sys32_sigreturn(void);
 +long sys32_rt_sigreturn(void);
  
 -long sys_s390_personality(unsigned int personality);
 +long sys_s390_pci_mmio_write(unsigned long, const void __user *, size_t);
 +long sys_s390_pci_mmio_read(unsigned long, void __user *, size_t);
++=======
+ long sys_s390_runtime_instr(int command, int signum);
+ long sys_s390_guarded_storage(int command, struct gs_cb __user *);
+ long sys_s390_pci_mmio_write(unsigned long, const void __user *, size_t);
+ long sys_s390_pci_mmio_read(unsigned long, void __user *, size_t);
+ 
+ DECLARE_PER_CPU(u64, mt_cycles[8]);
+ 
+ void verify_facilities(void);
+ void gs_load_bc_cb(struct pt_regs *regs);
+ void set_fs_fixup(void);
+ 
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  #endif /* _ENTRY_H */
diff --cc arch/s390/kernel/machine_kexec.c
index cf910480d58c,db5658daf994..000000000000
--- a/arch/s390/kernel/machine_kexec.c
+++ b/arch/s390/kernel/machine_kexec.c
@@@ -24,8 -24,10 +24,9 @@@
  #include <asm/diag.h>
  #include <asm/elf.h>
  #include <asm/asm-offsets.h>
 -#include <asm/cacheflush.h>
  #include <asm/os_info.h>
  #include <asm/switch_to.h>
+ #include <asm/nmi.h>
  
  typedef void (*relocate_kernel_t)(kimage_entry_t *, unsigned long);
  
@@@ -109,10 -72,71 +110,58 @@@ arch_initcall(machine_kdump_pm_init)
   */
  static void __do_machine_kdump(void *image)
  {
 -	int (*start_kdump)(int);
 -	unsigned long prefix;
 -
 -	/* store_status() saved the prefix register to lowcore */
 -	prefix = (unsigned long) S390_lowcore.prefixreg_save_area;
 -
 -	/* Now do the reset  */
 -	s390_reset_system();
 -
 -	/*
 -	 * Copy dump CPU store status info to absolute zero.
 -	 * This need to be done *after* s390_reset_system set the
 -	 * prefix register of this CPU to zero
 -	 */
 -	memcpy((void *) __LC_FPREGS_SAVE_AREA,
 -	       (void *)(prefix + __LC_FPREGS_SAVE_AREA), 512);
 +	int (*start_kdump)(int) = (void *)((struct kimage *) image)->start;
  
  	__load_psw_mask(PSW_MASK_BASE | PSW_DEFAULT_KEY | PSW_MASK_EA | PSW_MASK_BA);
 -	start_kdump = (void *)((struct kimage *) image)->start;
  	start_kdump(1);
++<<<<<<< HEAD
++=======
+ 
+ 	/* Die if start_kdump returns */
+ 	disabled_wait((unsigned long) __builtin_return_address(0));
+ }
+ 
+ /*
+  * Start kdump: create a LGR log entry, store status of all CPUs and
+  * branch to __do_machine_kdump.
+  */
+ static noinline void __machine_kdump(void *image)
+ {
+ 	struct mcesa *mcesa;
+ 	unsigned long cr2_old, cr2_new;
+ 	int this_cpu, cpu;
+ 
+ 	lgr_info_log();
+ 	/* Get status of the other CPUs */
+ 	this_cpu = smp_find_processor_id(stap());
+ 	for_each_online_cpu(cpu) {
+ 		if (cpu == this_cpu)
+ 			continue;
+ 		if (smp_store_status(cpu))
+ 			continue;
+ 	}
+ 	/* Store status of the boot CPU */
+ 	mcesa = (struct mcesa *)(S390_lowcore.mcesad & MCESA_ORIGIN_MASK);
+ 	if (MACHINE_HAS_VX)
+ 		save_vx_regs((__vector128 *) mcesa->vector_save_area);
+ 	if (MACHINE_HAS_GS) {
+ 		__ctl_store(cr2_old, 2, 2);
+ 		cr2_new = cr2_old | (1UL << 4);
+ 		__ctl_load(cr2_new, 2, 2);
+ 		save_gs_cb((struct gs_cb *) mcesa->guarded_storage_save_area);
+ 		__ctl_load(cr2_old, 2, 2);
+ 	}
+ 	/*
+ 	 * To create a good backchain for this CPU in the dump store_status
+ 	 * is passed the address of a function. The address is saved into
+ 	 * the PSW save area of the boot CPU and the function is invoked as
+ 	 * a tail call of store_status. The backchain in the dump will look
+ 	 * like this:
+ 	 *   restart_int_handler ->  __machine_kexec -> __do_machine_kdump
+ 	 * The call to store_status() will not return.
+ 	 */
+ 	store_status(__do_machine_kdump, image);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  }
  #endif
  
diff --cc arch/s390/kernel/nmi.c
index d7caa4ca8577,985589523970..000000000000
--- a/arch/s390/kernel/nmi.c
+++ b/arch/s390/kernel/nmi.c
@@@ -99,53 -105,70 +99,64 @@@ static int notrace s390_revalidate_regi
  {
  	int kill_task;
  	u64 zero;
++<<<<<<< HEAD
 +	void *fpt_save_area, *fpt_creg_save_area;
++=======
+ 	void *fpt_save_area;
+ 	struct mcesa *mcesa;
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
  	kill_task = 0;
  	zero = 0;
  
 -	if (!mci.gr) {
 +	if (!mci->gr) {
  		/*
  		 * General purpose registers couldn't be restored and have
 -		 * unknown contents. Stop system or terminate process.
 +		 * unknown contents. Process needs to be terminated.
  		 */
 -		if (!umode)
 -			s390_handle_damage();
  		kill_task = 1;
  	}
 -	/* Validate control registers */
 -	if (!mci.cr) {
 +	if (!mci->fp) {
  		/*
 -		 * Control registers have unknown contents.
 -		 * Can't recover and therefore stopping machine.
 +		 * Floating point registers can't be restored and
 +		 * therefore the process needs to be terminated.
  		 */
 -		s390_handle_damage();
 -	} else {
 -		asm volatile(
 -			"	lctlg	0,15,0(%0)\n"
 -			"	ptlb\n"
 -			: : "a" (&S390_lowcore.cregs_save_area) : "memory");
 -	}
 -	if (!mci.fp) {
 -		/*
 -		 * Floating point registers can't be restored. If the
 -		 * kernel currently uses floating point registers the
 -		 * system is stopped. If the process has its floating
 -		 * pointer registers loaded it is terminated.
 -		 * Otherwise just revalidate the registers.
 -		 */
 -		if (S390_lowcore.fpu_flags & KERNEL_VXR_V0V7)
 -			s390_handle_damage();
 -		if (!test_cpu_flag(CIF_FPU))
 -			kill_task = 1;
 +		kill_task = 1;
  	}
 -	fpt_save_area = &S390_lowcore.floating_pt_save_area;
 -	if (!mci.fc) {
 -		/*
 -		 * Floating point control register can't be restored.
 -		 * If the kernel currently uses the floating pointer
 -		 * registers and needs the FPC register the system is
 -		 * stopped. If the process has its floating pointer
 -		 * registers loaded it is terminated. Otherwiese the
 -		 * FPC is just revalidated.
 -		 */
 -		if (S390_lowcore.fpu_flags & KERNEL_FPC)
 -			s390_handle_damage();
 -		asm volatile("lfpc %0" : : "Q" (zero));
 -		if (!test_cpu_flag(CIF_FPU))
 +#ifndef CONFIG_64BIT
 +	asm volatile(
 +		"	ld	0,0(%0)\n"
 +		"	ld	2,8(%0)\n"
 +		"	ld	4,16(%0)\n"
 +		"	ld	6,24(%0)"
 +		: : "a" (&S390_lowcore.floating_pt_save_area));
 +#endif
 +
 +	if (MACHINE_HAS_IEEE) {
 +#ifdef CONFIG_64BIT
 +		fpt_save_area = &S390_lowcore.floating_pt_save_area;
 +		fpt_creg_save_area = &S390_lowcore.fpt_creg_save_area;
 +#else
 +		fpt_save_area = (void *) S390_lowcore.extended_save_area_addr;
 +		fpt_creg_save_area = fpt_save_area + 128;
 +#endif
 +		if (!mci->fc) {
 +			/*
 +			 * Floating point control register can't be restored.
 +			 * Task will be terminated.
 +			 */
 +			asm volatile("lfpc 0(%0)" : : "a" (&zero), "m" (zero));
  			kill_task = 1;
 -	} else {
 -		asm volatile("lfpc %0"
 -			     : : "Q" (S390_lowcore.fpt_creg_save_area));
 -	}
  
 +		} else
 +			asm volatile("lfpc 0(%0)" : : "a" (fpt_creg_save_area));
 +
++<<<<<<< HEAD
++=======
+ 	mcesa = (struct mcesa *)(S390_lowcore.mcesad & MCESA_ORIGIN_MASK);
+ 	if (!MACHINE_HAS_VX) {
+ 		/* Validate floating point registers */
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  		asm volatile(
  			"	ld	0,0(%0)\n"
  			"	ld	1,8(%0)\n"
@@@ -181,12 -207,15 +192,21 @@@
  		cr0.val = S390_lowcore.cregs_save_area[0];
  		cr0.afp = cr0.vx = 1;
  		__ctl_load(cr0.val, 0, 0);
++<<<<<<< HEAD
 +		restore_vx_regs((__vector128 *)
 +				&S390_lowcore.vector_save_area);
++=======
+ 		asm volatile(
+ 			"	la	1,%0\n"
+ 			"	.word	0xe70f,0x1000,0x0036\n"	/* vlm 0,15,0(1) */
+ 			"	.word	0xe70f,0x1100,0x0c36\n"	/* vlm 16,31,256(1) */
+ 			: : "Q" (*(struct vx_array *) mcesa->vector_save_area)
+ 			: "1");
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  		__ctl_load(S390_lowcore.cregs_save_area[0], 0, 0);
  	}
 -	/* Validate access registers */
 +#endif
 +	/* Revalidate access registers */
  	asm volatile(
  		"	lam	0,15,0(%0)"
  		: : "a" (&S390_lowcore.access_regs_save_area));
@@@ -197,34 -226,28 +217,49 @@@
  		 */
  		kill_task = 1;
  	}
++<<<<<<< HEAD
 +	/* Revalidate control registers */
 +	if (!mci->cr) {
 +		/*
 +		 * Control registers have unknown contents.
 +		 * Can't recover and therefore stopping machine.
 +		 */
 +		s390_handle_damage("invalid control registers.");
 +	} else {
 +#ifdef CONFIG_64BIT
 +		asm volatile(
 +			"	lctlg	0,15,0(%0)"
 +			: : "a" (&S390_lowcore.cregs_save_area));
 +#else
 +		asm volatile(
 +			"	lctl	0,15,0(%0)"
 +			: : "a" (&S390_lowcore.cregs_save_area));
 +#endif
++=======
+ 	/* Validate guarded storage registers */
+ 	if (MACHINE_HAS_GS && (S390_lowcore.cregs_save_area[2] & (1UL << 4))) {
+ 		if (!mci.gs)
+ 			/*
+ 			 * Guarded storage register can't be restored and
+ 			 * the current processes uses guarded storage.
+ 			 * It has to be terminated.
+ 			 */
+ 			kill_task = 1;
+ 		else
+ 			load_gs_cb((struct gs_cb *)
+ 				   mcesa->guarded_storage_save_area);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	}
  	/*
 -	 * We don't even try to validate the TOD register, since we simply
 +	 * We don't even try to revalidate the TOD register, since we simply
  	 * can't write something sensible into that register.
  	 */
 +#ifdef CONFIG_64BIT
  	/*
 -	 * See if we can validate the TOD programmable register with its
 +	 * See if we can revalidate the TOD programmable register with its
  	 * old contents (should be zero) otherwise set it to zero.
  	 */
 -	if (!mci.pr)
 +	if (!mci->pr)
  		asm volatile(
  			"	sr	0,0\n"
  			"	sckpf"
diff --cc arch/s390/kernel/process.c
index 148b517d0534,999d7154bbdc..000000000000
--- a/arch/s390/kernel/process.c
+++ b/arch/s390/kernel/process.c
@@@ -90,9 -71,12 +90,16 @@@ extern void __kprobes kernel_thread_sta
  /*
   * Free current thread data structures etc..
   */
 -void exit_thread(struct task_struct *tsk)
 +void exit_thread(void)
  {
++<<<<<<< HEAD
 +	exit_thread_runtime_instr();
++=======
+ 	if (tsk == current) {
+ 		exit_thread_runtime_instr();
+ 		exit_thread_gs();
+ 	}
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  }
  
  void flush_thread(void)
@@@ -172,29 -160,13 +179,32 @@@ int copy_thread(unsigned long clone_fla
  
  	/* Don't copy runtime instrumentation info */
  	p->thread.ri_cb = NULL;
 +	p->thread.ri_signum = 0;
  	frame->childregs.psw.mask &= ~PSW_MASK_RI;
+ 	/* Don't copy guarded storage control block */
+ 	p->thread.gs_cb = NULL;
+ 	p->thread.gs_bc_cb = NULL;
  
 +#ifndef CONFIG_64BIT
 +	/*
 +	 * save fprs to current->thread.fp_regs to merge them with
 +	 * the emulated registers and then copy the result to the child.
 +	 */
 +	save_fp_ctl(&current->thread.fp_regs.fpc);
 +	save_fp_regs(current->thread.fp_regs.fprs);
 +	memcpy(&p->thread.fp_regs, &current->thread.fp_regs,
 +	       sizeof(s390_fp_regs));
 +	/* Set a new TLS ?  */
 +	if (clone_flags & CLONE_SETTLS)
 +		p->thread.acrs[0] = frame->childregs.gprs[6];
 +#else /* CONFIG_64BIT */
 +	/* Save the fpu registers to new thread structure. */
 +	save_fp_ctl(&p->thread.fp_regs.fpc);
 +	save_fp_regs(p->thread.fp_regs.fprs);
 +	p->thread.fp_regs.pad = 0;
  	/* Set a new TLS ?  */
  	if (clone_flags & CLONE_SETTLS) {
 +		unsigned long tls = frame->childregs.gprs[6];
  		if (is_compat_task()) {
  			p->thread.acrs[0] = (unsigned int)tls;
  		} else {
diff --cc arch/s390/kernel/processor.c
index 13721d0de9ed,c73709869447..000000000000
--- a/arch/s390/kernel/processor.c
+++ b/arch/s390/kernel/processor.c
@@@ -39,7 -80,60 +39,64 @@@ void cpu_init(void
  	current->active_mm = &init_mm;
  	BUG_ON(current->mm);
  	enter_lazy_tlb(&init_mm, current);
++<<<<<<< HEAD
 +	memset(idle, 0, sizeof(*idle));
++=======
+ }
+ 
+ /*
+  * cpu_have_feature - Test CPU features on module initialization
+  */
+ int cpu_have_feature(unsigned int num)
+ {
+ 	return elf_hwcap & (1UL << num);
+ }
+ EXPORT_SYMBOL(cpu_have_feature);
+ 
+ static void show_cpu_summary(struct seq_file *m, void *v)
+ {
+ 	static const char *hwcap_str[] = {
+ 		"esan3", "zarch", "stfle", "msa", "ldisp", "eimm", "dfp",
+ 		"edat", "etf3eh", "highgprs", "te", "vx", "vxd", "vxe", "gs"
+ 	};
+ 	static const char * const int_hwcap_str[] = {
+ 		"sie"
+ 	};
+ 	int i, cpu;
+ 
+ 	seq_printf(m, "vendor_id       : IBM/S390\n"
+ 		   "# processors    : %i\n"
+ 		   "bogomips per cpu: %lu.%02lu\n",
+ 		   num_online_cpus(), loops_per_jiffy/(500000/HZ),
+ 		   (loops_per_jiffy/(5000/HZ))%100);
+ 	seq_printf(m, "max thread id   : %d\n", smp_cpu_mtid);
+ 	seq_puts(m, "features\t: ");
+ 	for (i = 0; i < ARRAY_SIZE(hwcap_str); i++)
+ 		if (hwcap_str[i] && (elf_hwcap & (1UL << i)))
+ 			seq_printf(m, "%s ", hwcap_str[i]);
+ 	for (i = 0; i < ARRAY_SIZE(int_hwcap_str); i++)
+ 		if (int_hwcap_str[i] && (int_hwcap & (1UL << i)))
+ 			seq_printf(m, "%s ", int_hwcap_str[i]);
+ 	seq_puts(m, "\n");
+ 	show_cacheinfo(m);
+ 	for_each_online_cpu(cpu) {
+ 		struct cpuid *id = &per_cpu(cpu_info.cpu_id, cpu);
+ 
+ 		seq_printf(m, "processor %d: "
+ 			   "version = %02X,  "
+ 			   "identification = %06X,  "
+ 			   "machine = %04X\n",
+ 			   cpu, id->version, id->ident, id->machine);
+ 	}
+ }
+ 
+ static void show_cpu_mhz(struct seq_file *m, unsigned long n)
+ {
+ 	struct cpu_info *c = per_cpu_ptr(&cpu_info, n);
+ 
+ 	seq_printf(m, "cpu MHz dynamic : %d\n", c->cpu_mhz_dynamic);
+ 	seq_printf(m, "cpu MHz static  : %d\n", c->cpu_mhz_static);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  }
  
  /*
diff --cc arch/s390/kernel/ptrace.c
index 490e6f6a7017,c933e255b5d5..000000000000
--- a/arch/s390/kernel/ptrace.c
+++ b/arch/s390/kernel/ptrace.c
@@@ -43,44 -44,42 +43,82 @@@ void update_cr_regs(struct task_struct 
  	struct pt_regs *regs = task_pt_regs(task);
  	struct thread_struct *thread = &task->thread;
  	struct per_regs old, new;
+ 	unsigned long cr0_old, cr0_new;
+ 	unsigned long cr2_old, cr2_new;
+ 	int cr0_changed, cr2_changed;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_64BIT
 +	/* Take care of the enable/disable of transactional execution. */
 +	if (MACHINE_HAS_TE || MACHINE_HAS_VX) {
 +		unsigned long cr, cr_new;
 +
 +		__ctl_store(cr, 0, 0);
 +		cr_new = cr;
 +		if (MACHINE_HAS_TE) {
 +			/* Set or clear transaction execution TXC bit 8. */
 +			cr_new |= (1UL << 55);
 +			if (task->thread.per_flags & PER_FLAG_NO_TE)
 +				cr_new &= ~(1UL << 55);
 +		}
 +		if (MACHINE_HAS_VX) {
 +			/* Enable/disable of vector extension */
 +			cr_new &= ~(1UL << 17);
 +			if (task->thread.vxrs)
 +				cr_new |= (1UL << 17);
 +		}
 +		if (cr_new != cr)
 +			__ctl_load(cr_new, 0, 0);
 +		if (MACHINE_HAS_TE) {
 +			/* Set/clear transaction execution TDC bits 62/63. */
 +			__ctl_store(cr, 2, 2);
 +			cr_new = cr & ~3UL;
 +			if (task->thread.per_flags & PER_FLAG_TE_ABORT_RAND) {
 +				if (task->thread.per_flags &
 +				    PER_FLAG_TE_ABORT_RAND_TEND)
 +					cr_new |= 1UL;
 +				else
 +					cr_new |= 2UL;
 +			}
 +			if (cr_new != cr)
 +				__ctl_load(cr_new, 2, 2);
 +		}
 +	}
 +#endif
++=======
+ 	__ctl_store(cr0_old, 0, 0);
+ 	__ctl_store(cr2_old, 2, 2);
+ 	cr0_new = cr0_old;
+ 	cr2_new = cr2_old;
+ 	/* Take care of the enable/disable of transactional execution. */
+ 	if (MACHINE_HAS_TE) {
+ 		/* Set or clear transaction execution TXC bit 8. */
+ 		cr0_new |= (1UL << 55);
+ 		if (task->thread.per_flags & PER_FLAG_NO_TE)
+ 			cr0_new &= ~(1UL << 55);
+ 		/* Set or clear transaction execution TDC bits 62 and 63. */
+ 		cr2_new &= ~3UL;
+ 		if (task->thread.per_flags & PER_FLAG_TE_ABORT_RAND) {
+ 			if (task->thread.per_flags & PER_FLAG_TE_ABORT_RAND_TEND)
+ 				cr2_new |= 1UL;
+ 			else
+ 				cr2_new |= 2UL;
+ 		}
+ 	}
+ 	/* Take care of enable/disable of guarded storage. */
+ 	if (MACHINE_HAS_GS) {
+ 		cr2_new &= ~(1UL << 4);
+ 		if (task->thread.gs_cb)
+ 			cr2_new |= (1UL << 4);
+ 	}
+ 	/* Load control register 0/2 iff changed */
+ 	cr0_changed = cr0_new != cr0_old;
+ 	cr2_changed = cr2_new != cr2_old;
+ 	if (cr0_changed)
+ 		__ctl_load(cr0_new, 0, 0);
+ 	if (cr2_changed)
+ 		__ctl_load(cr2_new, 2, 2);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	/* Copy user specified PER registers */
  	new.control = thread->per_user.control;
  	new.start = thread->per_user.start;
@@@ -1259,7 -1236,14 +1327,18 @@@ static const struct user_regset s390_re
  		.get = s390_vxrs_high_get,
  		.set = s390_vxrs_high_set,
  	},
++<<<<<<< HEAD
 +#endif
++=======
+ 	{
+ 		.core_note_type = NT_S390_GS_CB,
+ 		.n = sizeof(struct gs_cb) / sizeof(__u64),
+ 		.size = sizeof(__u64),
+ 		.align = sizeof(__u64),
+ 		.get = s390_gs_cb_get,
+ 		.set = s390_gs_cb_set,
+ 	},
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  };
  
  static const struct user_regset_view user_s390_view = {
diff --cc arch/s390/kernel/setup.c
index adf01b023e73,3ae756c0db3d..000000000000
--- a/arch/s390/kernel/setup.c
+++ b/arch/s390/kernel/setup.c
@@@ -337,19 -338,17 +337,31 @@@ static void __init setup_lowcore(void
  	lc->stfl_fac_list = S390_lowcore.stfl_fac_list;
  	memcpy(lc->stfle_fac_list, S390_lowcore.stfle_fac_list,
  	       MAX_FACILITY_BIT/8);
++<<<<<<< HEAD
 +#ifndef CONFIG_64BIT
 +	if (MACHINE_HAS_IEEE) {
 +		lc->extended_save_area_addr = (__u32)
 +			__alloc_bootmem_low(PAGE_SIZE, PAGE_SIZE, 0);
 +		/* enable extended save area */
 +		__ctl_set_bit(14, 29);
 +	}
 +#else
 +	if (MACHINE_HAS_VX)
 +		lc->vector_save_area_addr =
 +			(unsigned long) &lc->vector_save_area;
++=======
+ 	if (MACHINE_HAS_VX || MACHINE_HAS_GS) {
+ 		unsigned long bits, size;
+ 
+ 		bits = MACHINE_HAS_GS ? 11 : 10;
+ 		size = 1UL << bits;
+ 		lc->mcesad = (__u64) memblock_virt_alloc(size, size);
+ 		if (MACHINE_HAS_GS)
+ 			lc->mcesad |= bits;
+ 	}
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	lc->vdso_per_cpu_data = (unsigned long) &lc->paste[0];
 +#endif
  	lc->sync_enter_timer = S390_lowcore.sync_enter_timer;
  	lc->async_enter_timer = S390_lowcore.async_enter_timer;
  	lc->exit_timer = S390_lowcore.exit_timer;
@@@ -928,25 -773,30 +940,31 @@@ static void __init setup_hwcaps(void
  		elf_hwcap |= HWCAP_S390_TE;
  
  	/*
 -	 * Vector extension HWCAP_S390_VXRS is bit 11. The Vector extension
 -	 * can be disabled with the "novx" parameter. Use MACHINE_HAS_VX
 -	 * instead of facility bit 129.
 +	 * Vector extension HWCAP_S390_VXRS is bit 11.
  	 */
 -	if (MACHINE_HAS_VX) {
 +	if (test_facility(129))
  		elf_hwcap |= HWCAP_S390_VXRS;
 -		if (test_facility(134))
 -			elf_hwcap |= HWCAP_S390_VXRS_EXT;
 -		if (test_facility(135))
 -			elf_hwcap |= HWCAP_S390_VXRS_BCD;
 -	}
 +#endif
  
+ 	/*
+ 	 * Guarded storage support HWCAP_S390_GS is bit 12.
+ 	 */
+ 	if (MACHINE_HAS_GS)
+ 		elf_hwcap |= HWCAP_S390_GS;
+ 
  	get_cpu_id(&cpu_id);
 -	add_device_randomness(&cpu_id, sizeof(cpu_id));
  	switch (cpu_id.machine) {
 +	case 0x9672:
 +#if !defined(CONFIG_64BIT)
 +	default:	/* Use "g5" as default for 31 bit kernels. */
 +#endif
 +		strcpy(elf_platform, "g5");
 +		break;
  	case 0x2064:
  	case 0x2066:
 +#if defined(CONFIG_64BIT)
  	default:	/* Use "z900" as default for 64 bit kernels. */
 +#endif
  		strcpy(elf_platform, "z900");
  		break;
  	case 0x2084:
diff --cc arch/s390/kernel/smp.c
index dab9d6b15a1c,286bcee800f4..000000000000
--- a/arch/s390/kernel/smp.c
+++ b/arch/s390/kernel/smp.c
@@@ -45,6 -50,8 +45,11 @@@
  #include <asm/debug.h>
  #include <asm/os_info.h>
  #include <asm/sigp.h>
++<<<<<<< HEAD
++=======
+ #include <asm/idle.h>
+ #include <asm/nmi.h>
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  #include "entry.h"
  
  enum {
@@@ -70,9 -76,11 +75,11 @@@ struct pcpu 
  	u16 address;			/* physical cpu address */
  };
  
 -static u8 boot_core_type;
 +static u8 boot_cpu_type;
  static struct pcpu pcpu_devices[NR_CPUS];
  
+ static struct kmem_cache *pcpu_mcesa_cache;
+ 
  unsigned int smp_cpu_mt_shift;
  EXPORT_SYMBOL(smp_cpu_mt_shift);
  
@@@ -189,47 -184,57 +196,85 @@@ static void pcpu_ec_call(struct pcpu *p
  	pcpu_sigp_retry(pcpu, order, 0);
  }
  
 -#define ASYNC_FRAME_OFFSET (ASYNC_SIZE - STACK_FRAME_OVERHEAD - __PT_SIZE)
 -#define PANIC_FRAME_OFFSET (PAGE_SIZE - STACK_FRAME_OVERHEAD - __PT_SIZE)
 -
  static int pcpu_alloc_lowcore(struct pcpu *pcpu, int cpu)
  {
++<<<<<<< HEAD
 +	struct _lowcore *lc;
++=======
+ 	unsigned long async_stack, panic_stack;
+ 	unsigned long mcesa_origin, mcesa_bits;
+ 	struct lowcore *lc;
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  
+ 	mcesa_origin = mcesa_bits = 0;
  	if (pcpu != &pcpu_devices[0]) {
 -		pcpu->lowcore =	(struct lowcore *)
 +		pcpu->lowcore =	(struct _lowcore *)
  			__get_free_pages(GFP_KERNEL | GFP_DMA, LC_ORDER);
 -		async_stack = __get_free_pages(GFP_KERNEL, ASYNC_ORDER);
 -		panic_stack = __get_free_page(GFP_KERNEL);
 -		if (!pcpu->lowcore || !panic_stack || !async_stack)
 +		pcpu->async_stack = __get_free_pages(GFP_KERNEL, ASYNC_ORDER);
 +		pcpu->panic_stack = __get_free_page(GFP_KERNEL);
 +		if (!pcpu->lowcore || !pcpu->panic_stack || !pcpu->async_stack)
  			goto out;
++<<<<<<< HEAD
++=======
+ 		if (MACHINE_HAS_VX || MACHINE_HAS_GS) {
+ 			mcesa_origin = (unsigned long)
+ 				kmem_cache_alloc(pcpu_mcesa_cache, GFP_KERNEL);
+ 			if (!mcesa_origin)
+ 				goto out;
+ 			mcesa_bits = MACHINE_HAS_GS ? 11 : 0;
+ 		}
+ 	} else {
+ 		async_stack = pcpu->lowcore->async_stack - ASYNC_FRAME_OFFSET;
+ 		panic_stack = pcpu->lowcore->panic_stack - PANIC_FRAME_OFFSET;
+ 		mcesa_origin = pcpu->lowcore->mcesad & MCESA_ORIGIN_MASK;
+ 		mcesa_bits = pcpu->lowcore->mcesad & MCESA_LC_MASK;
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	}
  	lc = pcpu->lowcore;
  	memcpy(lc, &S390_lowcore, 512);
  	memset((char *) lc + 512, 0, sizeof(*lc) - 512);
++<<<<<<< HEAD
 +	lc->async_stack = pcpu->async_stack + ASYNC_SIZE
 +		- STACK_FRAME_OVERHEAD - sizeof(struct pt_regs);
 +	lc->panic_stack = pcpu->panic_stack + PAGE_SIZE
 +		- STACK_FRAME_OVERHEAD - sizeof(struct pt_regs);
 +	lc->cpu_nr = cpu;
 +	lc->spinlock_lockval = arch_spin_lockval(cpu);
 +#ifndef CONFIG_64BIT
 +	if (MACHINE_HAS_IEEE) {
 +		lc->extended_save_area_addr = get_zeroed_page(GFP_KERNEL);
 +		if (!lc->extended_save_area_addr)
 +			goto out;
 +	}
 +#else
 +	if (MACHINE_HAS_VX)
 +		lc->vector_save_area_addr =
 +			(unsigned long) &lc->vector_save_area;
++=======
+ 	lc->async_stack = async_stack + ASYNC_FRAME_OFFSET;
+ 	lc->panic_stack = panic_stack + PANIC_FRAME_OFFSET;
+ 	lc->mcesad = mcesa_origin | mcesa_bits;
+ 	lc->cpu_nr = cpu;
+ 	lc->spinlock_lockval = arch_spin_lockval(cpu);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	if (vdso_alloc_per_cpu(lc))
  		goto out;
 +#endif
  	lowcore_ptr[cpu] = lc;
  	pcpu_sigp_retry(pcpu, SIGP_SET_PREFIX, (u32)(unsigned long) lc);
  	return 0;
  out:
  	if (pcpu != &pcpu_devices[0]) {
++<<<<<<< HEAD
 +		free_page(pcpu->panic_stack);
 +		free_pages(pcpu->async_stack, ASYNC_ORDER);
++=======
+ 		if (mcesa_origin)
+ 			kmem_cache_free(pcpu_mcesa_cache,
+ 					(void *) mcesa_origin);
+ 		free_page(panic_stack);
+ 		free_pages(async_stack, ASYNC_ORDER);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  		free_pages((unsigned long) pcpu->lowcore, LC_ORDER);
  	}
  	return -ENOMEM;
@@@ -239,23 -244,20 +284,37 @@@
  
  static void pcpu_free_lowcore(struct pcpu *pcpu)
  {
+ 	unsigned long mcesa_origin;
+ 
  	pcpu_sigp_retry(pcpu, SIGP_SET_PREFIX, 0);
  	lowcore_ptr[pcpu - pcpu_devices] = NULL;
 +#ifndef CONFIG_64BIT
 +	if (MACHINE_HAS_IEEE) {
 +		struct _lowcore *lc = pcpu->lowcore;
 +
 +		free_page((unsigned long) lc->extended_save_area_addr);
 +		lc->extended_save_area_addr = 0;
 +	}
 +#else
  	vdso_free_per_cpu(pcpu->lowcore);
++<<<<<<< HEAD
 +#endif
 +	if (pcpu != &pcpu_devices[0]) {
 +		free_page(pcpu->panic_stack);
 +		free_pages(pcpu->async_stack, ASYNC_ORDER);
 +		free_pages((unsigned long) pcpu->lowcore, LC_ORDER);
 +	}
++=======
+ 	if (pcpu == &pcpu_devices[0])
+ 		return;
+ 	if (MACHINE_HAS_VX || MACHINE_HAS_GS) {
+ 		mcesa_origin = pcpu->lowcore->mcesad & MCESA_ORIGIN_MASK;
+ 		kmem_cache_free(pcpu_mcesa_cache, (void *) mcesa_origin);
+ 	}
+ 	free_page(pcpu->lowcore->panic_stack-PANIC_FRAME_OFFSET);
+ 	free_pages(pcpu->lowcore->async_stack-ASYNC_FRAME_OFFSET, ASYNC_ORDER);
+ 	free_pages((unsigned long) pcpu->lowcore, LC_ORDER);
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  }
  
  #endif /* CONFIG_HOTPLUG_CPU */
@@@ -592,39 -560,26 +651,55 @@@ void smp_ctl_clear_bit(int cr, int bit
  }
  EXPORT_SYMBOL(smp_ctl_clear_bit);
  
 -#ifdef CONFIG_CRASH_DUMP
 +#if defined(CONFIG_ZFCPDUMP) || defined(CONFIG_CRASH_DUMP)
  
 -int smp_store_status(int cpu)
 +static inline void __smp_store_cpu_state(int cpu, u16 address, int is_boot_cpu)
  {
 -	struct pcpu *pcpu = pcpu_devices + cpu;
 -	unsigned long pa;
 -
 +	void *lc = pcpu_devices[0].lowcore;
 +	struct save_area_ext *sa_ext;
 +	unsigned long vx_sa;
 +
++<<<<<<< HEAD
 +	sa_ext = dump_save_area_create(cpu);
 +	if (!sa_ext)
 +		panic("could not allocate memory for save area\n");
 +#ifdef CONFIG_CRASH_DUMP
 +	if (is_boot_cpu) {
 +		/* Copy the registers of the boot CPU. */
 +		copy_oldmem_page(1, (void *) &sa_ext->sa, sizeof(sa_ext->sa),
 +				 SAVE_AREA_BASE - PAGE_SIZE, 0);
 +		if (MACHINE_HAS_VX)
 +			save_vx_regs_safe(sa_ext->vx_regs);
 +		return;
 +	}
 +#endif
 +	/* Get the registers of a non-boot cpu. */
 +	__pcpu_sigp_relax(address, SIGP_STOP_AND_STORE_STATUS, 0, NULL);
 +	memcpy_real(&sa_ext->sa, lc + SAVE_AREA_BASE, sizeof(sa_ext->sa));
 +	if (!MACHINE_HAS_VX)
 +		return;
 +	/* Get the VX registers */
 +	vx_sa = __get_free_page(GFP_KERNEL);
 +	if (!vx_sa)
 +		panic("could not allocate memory for VX save area\n");
 +	__pcpu_sigp_relax(address, SIGP_STORE_ADDITIONAL_STATUS, vx_sa, NULL);
 +	memcpy(sa_ext->vx_regs, (void *) vx_sa, sizeof(sa_ext->vx_regs));
 +	free_page(vx_sa);
++=======
+ 	pa = __pa(&pcpu->lowcore->floating_pt_save_area);
+ 	if (__pcpu_sigp_relax(pcpu->address, SIGP_STORE_STATUS_AT_ADDRESS,
+ 			      pa) != SIGP_CC_ORDER_CODE_ACCEPTED)
+ 		return -EIO;
+ 	if (!MACHINE_HAS_VX && !MACHINE_HAS_GS)
+ 		return 0;
+ 	pa = __pa(pcpu->lowcore->mcesad & MCESA_ORIGIN_MASK);
+ 	if (MACHINE_HAS_GS)
+ 		pa |= pcpu->lowcore->mcesad & MCESA_LC_MASK;
+ 	if (__pcpu_sigp_relax(pcpu->address, SIGP_STORE_ADDITIONAL_STATUS,
+ 			      pa) != SIGP_CC_ORDER_CODE_ACCEPTED)
+ 		return -EIO;
+ 	return 0;
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  }
  
  /*
@@@ -958,7 -928,14 +1035,18 @@@ void __init smp_prepare_cpus(unsigned i
  	/* request the 0x1202 external call external interrupt */
  	if (register_external_irq(EXT_IRQ_EXTERNAL_CALL, do_ext_call_interrupt))
  		panic("Couldn't request external interrupt 0x1202");
++<<<<<<< HEAD
 +	smp_detect_cpus();
++=======
+ 	/* create slab cache for the machine-check-extended-save-areas */
+ 	if (MACHINE_HAS_VX || MACHINE_HAS_GS) {
+ 		size = 1UL << (MACHINE_HAS_GS ? 11 : 10);
+ 		pcpu_mcesa_cache = kmem_cache_create("nmi_save_areas",
+ 						     size, size, 0, NULL);
+ 		if (!pcpu_mcesa_cache)
+ 			panic("Couldn't create nmi save area cache");
+ 	}
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  }
  
  void __init smp_prepare_boot_cpu(void)
diff --cc arch/s390/kernel/syscalls.S
index 1dc392c977aa,54fce7b065de..000000000000
--- a/arch/s390/kernel/syscalls.S
+++ b/arch/s390/kernel/syscalls.S
@@@ -1,386 -1,390 +1,451 @@@
  /*
   * definitions for sys_call_table, each line represents an
 - * entry in the table in the form
 - * SYSCALL(64 bit syscall, 31 bit emulated syscall)
 + * entry in the table in the form 
 + * SYSCALL(31 bit syscall, 64 bit syscall, 31 bit emulated syscall)
   *
 - * this file is meant to be included from entry.S
 + * this file is meant to be included from entry.S and entry64.S
   */
  
 -#define NI_SYSCALL SYSCALL(sys_ni_syscall,sys_ni_syscall)
 +#define NI_SYSCALL SYSCALL(sys_ni_syscall,sys_ni_syscall,sys_ni_syscall)
  
 -NI_SYSCALL						/* 0 */
 -SYSCALL(sys_exit,sys_exit)
 -SYSCALL(sys_fork,sys_fork)
 -SYSCALL(sys_read,compat_sys_s390_read)
 -SYSCALL(sys_write,compat_sys_s390_write)
 -SYSCALL(sys_open,compat_sys_open)			/* 5 */
 -SYSCALL(sys_close,sys_close)
 -SYSCALL(sys_restart_syscall,sys_restart_syscall)
 -SYSCALL(sys_creat,compat_sys_creat)
 -SYSCALL(sys_link,compat_sys_link)
 -SYSCALL(sys_unlink,compat_sys_unlink)			/* 10 */
 -SYSCALL(sys_execve,compat_sys_execve)
 -SYSCALL(sys_chdir,compat_sys_chdir)
 -SYSCALL(sys_ni_syscall,compat_sys_time)			/* old time syscall */
 -SYSCALL(sys_mknod,compat_sys_mknod)
 -SYSCALL(sys_chmod,compat_sys_chmod)			/* 15 */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_lchown16)	/* old lchown16 syscall*/
 -NI_SYSCALL						/* old break syscall holder */
 -NI_SYSCALL						/* old stat syscall holder */
 -SYSCALL(sys_lseek,compat_sys_lseek)
 -SYSCALL(sys_getpid,sys_getpid)				/* 20 */
 -SYSCALL(sys_mount,compat_sys_mount)
 -SYSCALL(sys_oldumount,compat_sys_oldumount)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setuid16)	/* old setuid16 syscall*/
 -SYSCALL(sys_ni_syscall,compat_sys_s390_getuid16)	/* old getuid16 syscall*/
 -SYSCALL(sys_ni_syscall,compat_sys_stime)		/* 25 old stime syscall */
 -SYSCALL(sys_ptrace,compat_sys_ptrace)
 -SYSCALL(sys_alarm,sys_alarm)
 -NI_SYSCALL						/* old fstat syscall */
 -SYSCALL(sys_pause,sys_pause)
 -SYSCALL(sys_utime,compat_sys_utime)			/* 30 */
 -NI_SYSCALL						/* old stty syscall */
 -NI_SYSCALL						/* old gtty syscall */
 -SYSCALL(sys_access,compat_sys_access)
 -SYSCALL(sys_nice,sys_nice)
 -NI_SYSCALL						/* 35 old ftime syscall */
 -SYSCALL(sys_sync,sys_sync)
 -SYSCALL(sys_kill,sys_kill)
 -SYSCALL(sys_rename,compat_sys_rename)
 -SYSCALL(sys_mkdir,compat_sys_mkdir)
 -SYSCALL(sys_rmdir,compat_sys_rmdir)			/* 40 */
 -SYSCALL(sys_dup,sys_dup)
 -SYSCALL(sys_pipe,compat_sys_pipe)
 -SYSCALL(sys_times,compat_sys_times)
 -NI_SYSCALL						/* old prof syscall */
 -SYSCALL(sys_brk,compat_sys_brk)				/* 45 */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setgid16)	/* old setgid16 syscall*/
 -SYSCALL(sys_ni_syscall,compat_sys_s390_getgid16)	/* old getgid16 syscall*/
 -SYSCALL(sys_signal,compat_sys_signal)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_geteuid16)	/* old geteuid16 syscall */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_getegid16)	/* 50 old getegid16 syscall */
 -SYSCALL(sys_acct,compat_sys_acct)
 -SYSCALL(sys_umount,compat_sys_umount)
 -NI_SYSCALL						/* old lock syscall */
 -SYSCALL(sys_ioctl,compat_sys_ioctl)
 -SYSCALL(sys_fcntl,compat_sys_fcntl)			/* 55 */
 -NI_SYSCALL						/* intel mpx syscall */
 -SYSCALL(sys_setpgid,sys_setpgid)
 -NI_SYSCALL						/* old ulimit syscall */
 -NI_SYSCALL						/* old uname syscall */
 -SYSCALL(sys_umask,sys_umask)				/* 60 */
 -SYSCALL(sys_chroot,compat_sys_chroot)
 -SYSCALL(sys_ustat,compat_sys_ustat)
 -SYSCALL(sys_dup2,sys_dup2)
 -SYSCALL(sys_getppid,sys_getppid)
 -SYSCALL(sys_getpgrp,sys_getpgrp)			/* 65 */
 -SYSCALL(sys_setsid,sys_setsid)
 -SYSCALL(sys_sigaction,compat_sys_sigaction)
 -NI_SYSCALL						/* old sgetmask syscall*/
 -NI_SYSCALL						/* old ssetmask syscall*/
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setreuid16)	/* old setreuid16 syscall */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setregid16)	/* old setregid16 syscall */
 -SYSCALL(sys_sigsuspend,compat_sys_sigsuspend)
 -SYSCALL(sys_sigpending,compat_sys_sigpending)
 -SYSCALL(sys_sethostname,compat_sys_sethostname)
 -SYSCALL(sys_setrlimit,compat_sys_setrlimit)		/* 75 */
 -SYSCALL(sys_getrlimit,compat_sys_old_getrlimit)
 -SYSCALL(sys_getrusage,compat_sys_getrusage)
 -SYSCALL(sys_gettimeofday,compat_sys_gettimeofday)
 -SYSCALL(sys_settimeofday,compat_sys_settimeofday)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_getgroups16)	/* 80 old getgroups16 syscall */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setgroups16)	/* old setgroups16 syscall */
 -NI_SYSCALL						/* old select syscall */
 -SYSCALL(sys_symlink,compat_sys_symlink)
 -NI_SYSCALL						/* old lstat syscall */
 -SYSCALL(sys_readlink,compat_sys_readlink)		/* 85 */
 -SYSCALL(sys_uselib,compat_sys_uselib)
 -SYSCALL(sys_swapon,compat_sys_swapon)
 -SYSCALL(sys_reboot,compat_sys_reboot)
 -SYSCALL(sys_ni_syscall,compat_sys_old_readdir)		/* old readdir syscall */
 -SYSCALL(sys_old_mmap,compat_sys_s390_old_mmap)		/* 90 */
 -SYSCALL(sys_munmap,compat_sys_munmap)
 -SYSCALL(sys_truncate,compat_sys_truncate)
 -SYSCALL(sys_ftruncate,compat_sys_ftruncate)
 -SYSCALL(sys_fchmod,sys_fchmod)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_fchown16)	/* 95 old fchown16 syscall*/
 -SYSCALL(sys_getpriority,sys_getpriority)
 -SYSCALL(sys_setpriority,sys_setpriority)
 -NI_SYSCALL						/* old profil syscall */
 -SYSCALL(sys_statfs,compat_sys_statfs)
 -SYSCALL(sys_fstatfs,compat_sys_fstatfs)			/* 100 */
 -NI_SYSCALL						/* ioperm for i386 */
 -SYSCALL(sys_socketcall,compat_sys_socketcall)
 -SYSCALL(sys_syslog,compat_sys_syslog)
 -SYSCALL(sys_setitimer,compat_sys_setitimer)
 -SYSCALL(sys_getitimer,compat_sys_getitimer)		/* 105 */
 -SYSCALL(sys_newstat,compat_sys_newstat)
 -SYSCALL(sys_newlstat,compat_sys_newlstat)
 -SYSCALL(sys_newfstat,compat_sys_newfstat)
 -NI_SYSCALL						/* old uname syscall */
 -SYSCALL(sys_lookup_dcookie,compat_sys_lookup_dcookie)	/* 110 */
 -SYSCALL(sys_vhangup,sys_vhangup)
 -NI_SYSCALL						/* old "idle" system call */
 -NI_SYSCALL						/* vm86old for i386 */
 -SYSCALL(sys_wait4,compat_sys_wait4)
 -SYSCALL(sys_swapoff,compat_sys_swapoff)			/* 115 */
 -SYSCALL(sys_sysinfo,compat_sys_sysinfo)
 -SYSCALL(sys_s390_ipc,compat_sys_s390_ipc)
 -SYSCALL(sys_fsync,sys_fsync)
 -SYSCALL(sys_sigreturn,compat_sys_sigreturn)
 -SYSCALL(sys_clone,compat_sys_clone)			/* 120 */
 -SYSCALL(sys_setdomainname,compat_sys_setdomainname)
 -SYSCALL(sys_newuname,compat_sys_newuname)
 -NI_SYSCALL						/* modify_ldt for i386 */
 -SYSCALL(sys_adjtimex,compat_sys_adjtimex)
 -SYSCALL(sys_mprotect,compat_sys_mprotect)		/* 125 */
 -SYSCALL(sys_sigprocmask,compat_sys_sigprocmask)
 -NI_SYSCALL						/* old "create module" */
 -SYSCALL(sys_init_module,compat_sys_init_module)
 -SYSCALL(sys_delete_module,compat_sys_delete_module)
 -NI_SYSCALL						/* 130: old get_kernel_syms */
 -SYSCALL(sys_quotactl,compat_sys_quotactl)
 -SYSCALL(sys_getpgid,sys_getpgid)
 -SYSCALL(sys_fchdir,sys_fchdir)
 -SYSCALL(sys_bdflush,compat_sys_bdflush)
 -SYSCALL(sys_sysfs,compat_sys_sysfs)			/* 135 */
 -SYSCALL(sys_s390_personality,sys_s390_personality)
 -NI_SYSCALL						/* for afs_syscall */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setfsuid16)	/* old setfsuid16 syscall */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setfsgid16)	/* old setfsgid16 syscall */
 -SYSCALL(sys_llseek,compat_sys_llseek)			/* 140 */
 -SYSCALL(sys_getdents,compat_sys_getdents)
 -SYSCALL(sys_select,compat_sys_select)
 -SYSCALL(sys_flock,sys_flock)
 -SYSCALL(sys_msync,compat_sys_msync)
 -SYSCALL(sys_readv,compat_sys_readv)			/* 145 */
 -SYSCALL(sys_writev,compat_sys_writev)
 -SYSCALL(sys_getsid,sys_getsid)
 -SYSCALL(sys_fdatasync,sys_fdatasync)
 -SYSCALL(sys_sysctl,compat_sys_sysctl)
 -SYSCALL(sys_mlock,compat_sys_mlock)			/* 150 */
 -SYSCALL(sys_munlock,compat_sys_munlock)
 -SYSCALL(sys_mlockall,sys_mlockall)
 -SYSCALL(sys_munlockall,sys_munlockall)
 -SYSCALL(sys_sched_setparam,compat_sys_sched_setparam)
 -SYSCALL(sys_sched_getparam,compat_sys_sched_getparam)	/* 155 */
 -SYSCALL(sys_sched_setscheduler,compat_sys_sched_setscheduler)
 -SYSCALL(sys_sched_getscheduler,sys_sched_getscheduler)
 -SYSCALL(sys_sched_yield,sys_sched_yield)
 -SYSCALL(sys_sched_get_priority_max,sys_sched_get_priority_max)
 -SYSCALL(sys_sched_get_priority_min,sys_sched_get_priority_min)	/* 160 */
 -SYSCALL(sys_sched_rr_get_interval,compat_sys_sched_rr_get_interval)
 -SYSCALL(sys_nanosleep,compat_sys_nanosleep)
 -SYSCALL(sys_mremap,compat_sys_mremap)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setresuid16)	/* old setresuid16 syscall */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_getresuid16)	/* 165 old getresuid16 syscall */
 -NI_SYSCALL						/* for vm86 */
 -NI_SYSCALL						/* old sys_query_module */
 -SYSCALL(sys_poll,compat_sys_poll)
 -NI_SYSCALL						/* old nfsservctl */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_setresgid16)	/* 170 old setresgid16 syscall */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_getresgid16)	/* old getresgid16 syscall */
 -SYSCALL(sys_prctl,compat_sys_prctl)
 -SYSCALL(sys_rt_sigreturn,compat_sys_rt_sigreturn)
 -SYSCALL(sys_rt_sigaction,compat_sys_rt_sigaction)
 -SYSCALL(sys_rt_sigprocmask,compat_sys_rt_sigprocmask)	/* 175 */
 -SYSCALL(sys_rt_sigpending,compat_sys_rt_sigpending)
 -SYSCALL(sys_rt_sigtimedwait,compat_sys_rt_sigtimedwait)
 -SYSCALL(sys_rt_sigqueueinfo,compat_sys_rt_sigqueueinfo)
 -SYSCALL(sys_rt_sigsuspend,compat_sys_rt_sigsuspend)
 -SYSCALL(sys_pread64,compat_sys_s390_pread64)		/* 180 */
 -SYSCALL(sys_pwrite64,compat_sys_s390_pwrite64)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_chown16)		/* old chown16 syscall */
 -SYSCALL(sys_getcwd,compat_sys_getcwd)
 -SYSCALL(sys_capget,compat_sys_capget)
 -SYSCALL(sys_capset,compat_sys_capset)			/* 185 */
 -SYSCALL(sys_sigaltstack,compat_sys_sigaltstack)
 -SYSCALL(sys_sendfile64,compat_sys_sendfile)
 -NI_SYSCALL						/* streams1 */
 -NI_SYSCALL						/* streams2 */
 -SYSCALL(sys_vfork,sys_vfork)				/* 190 */
 -SYSCALL(sys_getrlimit,compat_sys_getrlimit)
 -SYSCALL(sys_mmap2,compat_sys_s390_mmap2)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_truncate64)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_ftruncate64)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_stat64)		/* 195 */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_lstat64)
 -SYSCALL(sys_ni_syscall,compat_sys_s390_fstat64)
 -SYSCALL(sys_lchown,compat_sys_lchown)
 -SYSCALL(sys_getuid,sys_getuid)
 -SYSCALL(sys_getgid,sys_getgid)				/* 200 */
 -SYSCALL(sys_geteuid,sys_geteuid)
 -SYSCALL(sys_getegid,sys_getegid)
 -SYSCALL(sys_setreuid,sys_setreuid)
 -SYSCALL(sys_setregid,sys_setregid)
 -SYSCALL(sys_getgroups,compat_sys_getgroups)		/* 205 */
 -SYSCALL(sys_setgroups,compat_sys_setgroups)
 -SYSCALL(sys_fchown,sys_fchown)
 -SYSCALL(sys_setresuid,sys_setresuid)
 -SYSCALL(sys_getresuid,compat_sys_getresuid)
 -SYSCALL(sys_setresgid,sys_setresgid)			/* 210 */
 -SYSCALL(sys_getresgid,compat_sys_getresgid)
 -SYSCALL(sys_chown,compat_sys_chown)
 -SYSCALL(sys_setuid,sys_setuid)
 -SYSCALL(sys_setgid,sys_setgid)
 -SYSCALL(sys_setfsuid,sys_setfsuid)			/* 215 */
 -SYSCALL(sys_setfsgid,sys_setfsgid)
 -SYSCALL(sys_pivot_root,compat_sys_pivot_root)
 -SYSCALL(sys_mincore,compat_sys_mincore)
 -SYSCALL(sys_madvise,compat_sys_madvise)
 -SYSCALL(sys_getdents64,compat_sys_getdents64)		/* 220 */
 -SYSCALL(sys_ni_syscall,compat_sys_fcntl64)
 -SYSCALL(sys_readahead,compat_sys_s390_readahead)
 -SYSCALL(sys_ni_syscall,compat_sys_sendfile64)
 -SYSCALL(sys_setxattr,compat_sys_setxattr)
 -SYSCALL(sys_lsetxattr,compat_sys_lsetxattr)		/* 225 */
 -SYSCALL(sys_fsetxattr,compat_sys_fsetxattr)
 -SYSCALL(sys_getxattr,compat_sys_getxattr)
 -SYSCALL(sys_lgetxattr,compat_sys_lgetxattr)
 -SYSCALL(sys_fgetxattr,compat_sys_fgetxattr)
 -SYSCALL(sys_listxattr,compat_sys_listxattr)		/* 230 */
 -SYSCALL(sys_llistxattr,compat_sys_llistxattr)
 -SYSCALL(sys_flistxattr,compat_sys_flistxattr)
 -SYSCALL(sys_removexattr,compat_sys_removexattr)
 -SYSCALL(sys_lremovexattr,compat_sys_lremovexattr)
 -SYSCALL(sys_fremovexattr,compat_sys_fremovexattr)	/* 235 */
 -SYSCALL(sys_gettid,sys_gettid)
 -SYSCALL(sys_tkill,sys_tkill)
 -SYSCALL(sys_futex,compat_sys_futex)
 -SYSCALL(sys_sched_setaffinity,compat_sys_sched_setaffinity)
 -SYSCALL(sys_sched_getaffinity,compat_sys_sched_getaffinity)	/* 240 */
 -SYSCALL(sys_tgkill,sys_tgkill)
 -NI_SYSCALL						/* reserved for TUX */
 -SYSCALL(sys_io_setup,compat_sys_io_setup)
 -SYSCALL(sys_io_destroy,compat_sys_io_destroy)
 -SYSCALL(sys_io_getevents,compat_sys_io_getevents)	/* 245 */
 -SYSCALL(sys_io_submit,compat_sys_io_submit)
 -SYSCALL(sys_io_cancel,compat_sys_io_cancel)
 -SYSCALL(sys_exit_group,sys_exit_group)
 -SYSCALL(sys_epoll_create,sys_epoll_create)
 -SYSCALL(sys_epoll_ctl,compat_sys_epoll_ctl)		/* 250 */
 -SYSCALL(sys_epoll_wait,compat_sys_epoll_wait)
 -SYSCALL(sys_set_tid_address,compat_sys_set_tid_address)
 -SYSCALL(sys_fadvise64_64,compat_sys_s390_fadvise64)
 -SYSCALL(sys_timer_create,compat_sys_timer_create)
 -SYSCALL(sys_timer_settime,compat_sys_timer_settime)	/* 255 */
 -SYSCALL(sys_timer_gettime,compat_sys_timer_gettime)
 -SYSCALL(sys_timer_getoverrun,sys_timer_getoverrun)
 -SYSCALL(sys_timer_delete,sys_timer_delete)
 -SYSCALL(sys_clock_settime,compat_sys_clock_settime)
 -SYSCALL(sys_clock_gettime,compat_sys_clock_gettime)	/* 260 */
 -SYSCALL(sys_clock_getres,compat_sys_clock_getres)
 -SYSCALL(sys_clock_nanosleep,compat_sys_clock_nanosleep)
 -NI_SYSCALL						/* reserved for vserver */
 -SYSCALL(sys_ni_syscall,compat_sys_s390_fadvise64_64)
 -SYSCALL(sys_statfs64,compat_sys_statfs64)
 -SYSCALL(sys_fstatfs64,compat_sys_fstatfs64)
 -SYSCALL(sys_remap_file_pages,compat_sys_remap_file_pages)
 -SYSCALL(sys_mbind,compat_sys_mbind)
 -SYSCALL(sys_get_mempolicy,compat_sys_get_mempolicy)
 -SYSCALL(sys_set_mempolicy,compat_sys_set_mempolicy)
 -SYSCALL(sys_mq_open,compat_sys_mq_open)
 -SYSCALL(sys_mq_unlink,compat_sys_mq_unlink)
 -SYSCALL(sys_mq_timedsend,compat_sys_mq_timedsend)
 -SYSCALL(sys_mq_timedreceive,compat_sys_mq_timedreceive)
 -SYSCALL(sys_mq_notify,compat_sys_mq_notify)		/* 275 */
 -SYSCALL(sys_mq_getsetattr,compat_sys_mq_getsetattr)
 -SYSCALL(sys_kexec_load,compat_sys_kexec_load)
 -SYSCALL(sys_add_key,compat_sys_add_key)
 -SYSCALL(sys_request_key,compat_sys_request_key)
 -SYSCALL(sys_keyctl,compat_sys_keyctl)			/* 280 */
 -SYSCALL(sys_waitid,compat_sys_waitid)
 -SYSCALL(sys_ioprio_set,sys_ioprio_set)
 -SYSCALL(sys_ioprio_get,sys_ioprio_get)
 -SYSCALL(sys_inotify_init,sys_inotify_init)
 -SYSCALL(sys_inotify_add_watch,compat_sys_inotify_add_watch)	/* 285 */
 -SYSCALL(sys_inotify_rm_watch,sys_inotify_rm_watch)
 -SYSCALL(sys_migrate_pages,compat_sys_migrate_pages)
 -SYSCALL(sys_openat,compat_sys_openat)
 -SYSCALL(sys_mkdirat,compat_sys_mkdirat)
 -SYSCALL(sys_mknodat,compat_sys_mknodat)			/* 290 */
 -SYSCALL(sys_fchownat,compat_sys_fchownat)
 -SYSCALL(sys_futimesat,compat_sys_futimesat)
 -SYSCALL(sys_newfstatat,compat_sys_s390_fstatat64)
 -SYSCALL(sys_unlinkat,compat_sys_unlinkat)
 -SYSCALL(sys_renameat,compat_sys_renameat)		/* 295 */
 -SYSCALL(sys_linkat,compat_sys_linkat)
 -SYSCALL(sys_symlinkat,compat_sys_symlinkat)
 -SYSCALL(sys_readlinkat,compat_sys_readlinkat)
 -SYSCALL(sys_fchmodat,compat_sys_fchmodat)
 -SYSCALL(sys_faccessat,compat_sys_faccessat)		/* 300 */
 -SYSCALL(sys_pselect6,compat_sys_pselect6)
 -SYSCALL(sys_ppoll,compat_sys_ppoll)
 -SYSCALL(sys_unshare,compat_sys_unshare)
 -SYSCALL(sys_set_robust_list,compat_sys_set_robust_list)
 -SYSCALL(sys_get_robust_list,compat_sys_get_robust_list)
 -SYSCALL(sys_splice,compat_sys_splice)
 -SYSCALL(sys_sync_file_range,compat_sys_s390_sync_file_range)
 -SYSCALL(sys_tee,compat_sys_tee)
 -SYSCALL(sys_vmsplice,compat_sys_vmsplice)
 -SYSCALL(sys_move_pages,compat_sys_move_pages)
 -SYSCALL(sys_getcpu,compat_sys_getcpu)
 -SYSCALL(sys_epoll_pwait,compat_sys_epoll_pwait)
 -SYSCALL(sys_utimes,compat_sys_utimes)
 -SYSCALL(sys_fallocate,compat_sys_s390_fallocate)
 -SYSCALL(sys_utimensat,compat_sys_utimensat)		/* 315 */
 -SYSCALL(sys_signalfd,compat_sys_signalfd)
 +NI_SYSCALL							/* 0 */
 +SYSCALL(sys_exit,sys_exit,sys32_exit_wrapper)
 +SYSCALL(sys_fork,sys_fork,sys_fork)
 +SYSCALL(sys_read,sys_read,sys32_read_wrapper)
 +SYSCALL(sys_write,sys_write,sys32_write_wrapper)
 +SYSCALL(sys_open,sys_open,compat_sys_open)			/* 5 */
 +SYSCALL(sys_close,sys_close,sys32_close_wrapper)
 +SYSCALL(sys_restart_syscall,sys_restart_syscall,sys_restart_syscall)
 +SYSCALL(sys_creat,sys_creat,sys32_creat_wrapper)
 +SYSCALL(sys_link,sys_link,sys32_link_wrapper)
 +SYSCALL(sys_unlink,sys_unlink,sys32_unlink_wrapper)		/* 10 */
 +SYSCALL(sys_execve,sys_execve,sys32_execve_wrapper)
 +SYSCALL(sys_chdir,sys_chdir,sys32_chdir_wrapper)
 +SYSCALL(sys_time,sys_ni_syscall,sys32_time_wrapper)		/* old time syscall */
 +SYSCALL(sys_mknod,sys_mknod,sys32_mknod_wrapper)
 +SYSCALL(sys_chmod,sys_chmod,sys32_chmod_wrapper)		/* 15 */
 +SYSCALL(sys_lchown16,sys_ni_syscall,sys32_lchown16_wrapper)	/* old lchown16 syscall*/
 +NI_SYSCALL							/* old break syscall holder */
 +NI_SYSCALL							/* old stat syscall holder */
 +SYSCALL(sys_lseek,sys_lseek,compat_sys_lseek)
 +SYSCALL(sys_getpid,sys_getpid,sys_getpid)			/* 20 */
 +SYSCALL(sys_mount,sys_mount,sys32_mount_wrapper)
 +SYSCALL(sys_oldumount,sys_oldumount,sys32_oldumount_wrapper)
 +SYSCALL(sys_setuid16,sys_ni_syscall,sys32_setuid16_wrapper)	/* old setuid16 syscall*/
 +SYSCALL(sys_getuid16,sys_ni_syscall,sys32_getuid16)		/* old getuid16 syscall*/
 +SYSCALL(sys_stime,sys_ni_syscall,sys32_stime_wrapper)		/* 25 old stime syscall */
 +SYSCALL(sys_ptrace,sys_ptrace,sys32_ptrace_wrapper)
 +SYSCALL(sys_alarm,sys_alarm,sys32_alarm_wrapper)
 +NI_SYSCALL							/* old fstat syscall */
 +SYSCALL(sys_pause,sys_pause,sys_pause)
 +SYSCALL(sys_utime,sys_utime,compat_sys_utime_wrapper)		/* 30 */
 +NI_SYSCALL							/* old stty syscall */
 +NI_SYSCALL							/* old gtty syscall */
 +SYSCALL(sys_access,sys_access,sys32_access_wrapper)
 +SYSCALL(sys_nice,sys_nice,sys32_nice_wrapper)
 +NI_SYSCALL							/* 35 old ftime syscall */
 +SYSCALL(sys_sync,sys_sync,sys_sync)
 +SYSCALL(sys_kill,sys_kill,sys32_kill_wrapper)
 +SYSCALL(sys_rename,sys_rename,sys32_rename_wrapper)
 +SYSCALL(sys_mkdir,sys_mkdir,sys32_mkdir_wrapper)
 +SYSCALL(sys_rmdir,sys_rmdir,sys32_rmdir_wrapper)		/* 40 */
 +SYSCALL(sys_dup,sys_dup,sys32_dup_wrapper)
 +SYSCALL(sys_pipe,sys_pipe,sys32_pipe_wrapper)
 +SYSCALL(sys_times,sys_times,compat_sys_times_wrapper)
 +NI_SYSCALL							/* old prof syscall */
 +SYSCALL(sys_brk,sys_brk,sys32_brk_wrapper)			/* 45 */
 +SYSCALL(sys_setgid16,sys_ni_syscall,sys32_setgid16_wrapper)	/* old setgid16 syscall*/
 +SYSCALL(sys_getgid16,sys_ni_syscall,sys32_getgid16)		/* old getgid16 syscall*/
 +SYSCALL(sys_signal,sys_signal,sys32_signal_wrapper)
 +SYSCALL(sys_geteuid16,sys_ni_syscall,sys32_geteuid16)		/* old geteuid16 syscall */
 +SYSCALL(sys_getegid16,sys_ni_syscall,sys32_getegid16)		/* 50 old getegid16 syscall */
 +SYSCALL(sys_acct,sys_acct,sys32_acct_wrapper)
 +SYSCALL(sys_umount,sys_umount,sys32_umount_wrapper)
 +NI_SYSCALL							/* old lock syscall */
 +SYSCALL(sys_ioctl,sys_ioctl,compat_sys_ioctl_wrapper)
 +SYSCALL(sys_fcntl,sys_fcntl,compat_sys_fcntl_wrapper)		/* 55 */
 +NI_SYSCALL							/* intel mpx syscall */
 +SYSCALL(sys_setpgid,sys_setpgid,sys32_setpgid_wrapper)
 +NI_SYSCALL							/* old ulimit syscall */
 +NI_SYSCALL							/* old uname syscall */
 +SYSCALL(sys_umask,sys_umask,sys32_umask_wrapper)		/* 60 */
 +SYSCALL(sys_chroot,sys_chroot,sys32_chroot_wrapper)
 +SYSCALL(sys_ustat,sys_ustat,sys32_ustat_wrapper)
 +SYSCALL(sys_dup2,sys_dup2,sys32_dup2_wrapper)
 +SYSCALL(sys_getppid,sys_getppid,sys_getppid)
 +SYSCALL(sys_getpgrp,sys_getpgrp,sys_getpgrp)			/* 65 */
 +SYSCALL(sys_setsid,sys_setsid,sys_setsid)
 +SYSCALL(sys_sigaction,sys_sigaction,compat_sys_sigaction)
 +NI_SYSCALL							/* old sgetmask syscall*/
 +NI_SYSCALL							/* old ssetmask syscall*/
 +SYSCALL(sys_setreuid16,sys_ni_syscall,sys32_setreuid16_wrapper)	/* old setreuid16 syscall */
 +SYSCALL(sys_setregid16,sys_ni_syscall,sys32_setregid16_wrapper)	/* old setregid16 syscall */
 +SYSCALL(sys_sigsuspend,sys_sigsuspend,sys_sigsuspend_wrapper)
 +SYSCALL(sys_sigpending,sys_sigpending,compat_sys_sigpending_wrapper)
 +SYSCALL(sys_sethostname,sys_sethostname,sys32_sethostname_wrapper)
 +SYSCALL(sys_setrlimit,sys_setrlimit,compat_sys_setrlimit_wrapper)	/* 75 */
 +SYSCALL(sys_old_getrlimit,sys_getrlimit,compat_sys_old_getrlimit_wrapper)
 +SYSCALL(sys_getrusage,sys_getrusage,compat_sys_getrusage)
 +SYSCALL(sys_gettimeofday,sys_gettimeofday,compat_sys_gettimeofday_wrapper)
 +SYSCALL(sys_settimeofday,sys_settimeofday,compat_sys_settimeofday_wrapper)
 +SYSCALL(sys_getgroups16,sys_ni_syscall,sys32_getgroups16_wrapper)	/* 80 old getgroups16 syscall */
 +SYSCALL(sys_setgroups16,sys_ni_syscall,sys32_setgroups16_wrapper)	/* old setgroups16 syscall */
 +NI_SYSCALL							/* old select syscall */
 +SYSCALL(sys_symlink,sys_symlink,sys32_symlink_wrapper)
 +NI_SYSCALL							/* old lstat syscall */
 +SYSCALL(sys_readlink,sys_readlink,sys32_readlink_wrapper)	/* 85 */
 +SYSCALL(sys_uselib,sys_uselib,sys32_uselib_wrapper)
 +SYSCALL(sys_swapon,sys_swapon,sys32_swapon_wrapper)
 +SYSCALL(sys_reboot,sys_reboot,sys32_reboot_wrapper)
 +SYSCALL(sys_ni_syscall,sys_ni_syscall,old32_readdir_wrapper)	/* old readdir syscall */
 +SYSCALL(sys_old_mmap,sys_old_mmap,old32_mmap_wrapper)		/* 90 */
 +SYSCALL(sys_munmap,sys_munmap,sys32_munmap_wrapper)
 +SYSCALL(sys_truncate,sys_truncate,compat_sys_truncate)
 +SYSCALL(sys_ftruncate,sys_ftruncate,compat_sys_ftruncate)
 +SYSCALL(sys_fchmod,sys_fchmod,sys32_fchmod_wrapper)
 +SYSCALL(sys_fchown16,sys_ni_syscall,sys32_fchown16_wrapper)	/* 95 old fchown16 syscall*/
 +SYSCALL(sys_getpriority,sys_getpriority,sys32_getpriority_wrapper)
 +SYSCALL(sys_setpriority,sys_setpriority,sys32_setpriority_wrapper)
 +NI_SYSCALL							/* old profil syscall */
 +SYSCALL(sys_statfs,sys_statfs,compat_sys_statfs_wrapper)
 +SYSCALL(sys_fstatfs,sys_fstatfs,compat_sys_fstatfs_wrapper)	/* 100 */
 +NI_SYSCALL							/* ioperm for i386 */
 +SYSCALL(sys_socketcall,sys_socketcall,compat_sys_socketcall_wrapper)
 +SYSCALL(sys_syslog,sys_syslog,sys32_syslog_wrapper)
 +SYSCALL(sys_setitimer,sys_setitimer,compat_sys_setitimer)
 +SYSCALL(sys_getitimer,sys_getitimer,compat_sys_getitimer)	/* 105 */
 +SYSCALL(sys_newstat,sys_newstat,compat_sys_newstat_wrapper)
 +SYSCALL(sys_newlstat,sys_newlstat,compat_sys_newlstat_wrapper)
 +SYSCALL(sys_newfstat,sys_newfstat,compat_sys_newfstat_wrapper)
 +NI_SYSCALL							/* old uname syscall */
 +SYSCALL(sys_lookup_dcookie,sys_lookup_dcookie,compat_sys_lookup_dcookie)	/* 110 */
 +SYSCALL(sys_vhangup,sys_vhangup,sys_vhangup)
 +NI_SYSCALL							/* old "idle" system call */
 +NI_SYSCALL							/* vm86old for i386 */
 +SYSCALL(sys_wait4,sys_wait4,compat_sys_wait4)
 +SYSCALL(sys_swapoff,sys_swapoff,sys32_swapoff_wrapper)		/* 115 */
 +SYSCALL(sys_sysinfo,sys_sysinfo,compat_sys_sysinfo_wrapper)
 +SYSCALL(sys_s390_ipc,sys_s390_ipc,compat_sys_s390_ipc)
 +SYSCALL(sys_fsync,sys_fsync,sys32_fsync_wrapper)
 +SYSCALL(sys_sigreturn,sys_sigreturn,sys32_sigreturn)
 +SYSCALL(sys_clone,sys_clone,sys_clone_wrapper)			/* 120 */
 +SYSCALL(sys_setdomainname,sys_setdomainname,sys32_setdomainname_wrapper)
 +SYSCALL(sys_newuname,sys_newuname,sys32_newuname_wrapper)
 +NI_SYSCALL							/* modify_ldt for i386 */
 +SYSCALL(sys_adjtimex,sys_adjtimex,compat_sys_adjtimex_wrapper)
 +SYSCALL(sys_mprotect,sys_mprotect,sys32_mprotect_wrapper)	/* 125 */
 +SYSCALL(sys_sigprocmask,sys_sigprocmask,compat_sys_sigprocmask)
 +NI_SYSCALL							/* old "create module" */
 +SYSCALL(sys_init_module,sys_init_module,sys_init_module_wrapper)
 +SYSCALL(sys_delete_module,sys_delete_module,sys_delete_module_wrapper)
 +NI_SYSCALL							/* 130: old get_kernel_syms */
 +SYSCALL(sys_quotactl,sys_quotactl,sys32_quotactl_wrapper)
 +SYSCALL(sys_getpgid,sys_getpgid,sys32_getpgid_wrapper)
 +SYSCALL(sys_fchdir,sys_fchdir,sys32_fchdir_wrapper)
 +SYSCALL(sys_bdflush,sys_bdflush,sys32_bdflush_wrapper)
 +SYSCALL(sys_sysfs,sys_sysfs,sys32_sysfs_wrapper)		/* 135 */
 +SYSCALL(sys_personality,sys_s390_personality,sys32_personality_wrapper)
 +NI_SYSCALL							/* for afs_syscall */
 +SYSCALL(sys_setfsuid16,sys_ni_syscall,sys32_setfsuid16_wrapper)	/* old setfsuid16 syscall */
 +SYSCALL(sys_setfsgid16,sys_ni_syscall,sys32_setfsgid16_wrapper)	/* old setfsgid16 syscall */
 +SYSCALL(sys_llseek,sys_llseek,sys32_llseek_wrapper)		/* 140 */
 +SYSCALL(sys_getdents,sys_getdents,sys32_getdents_wrapper)
 +SYSCALL(sys_select,sys_select,compat_sys_select_wrapper)
 +SYSCALL(sys_flock,sys_flock,sys32_flock_wrapper)
 +SYSCALL(sys_msync,sys_msync,sys32_msync_wrapper)
 +SYSCALL(sys_readv,sys_readv,compat_sys_readv_wrapper)		/* 145 */
 +SYSCALL(sys_writev,sys_writev,compat_sys_writev_wrapper)
 +SYSCALL(sys_getsid,sys_getsid,sys32_getsid_wrapper)
 +SYSCALL(sys_fdatasync,sys_fdatasync,sys32_fdatasync_wrapper)
 +SYSCALL(sys_sysctl,sys_sysctl,compat_sys_sysctl)
 +SYSCALL(sys_mlock,sys_mlock,sys32_mlock_wrapper)		/* 150 */
 +SYSCALL(sys_munlock,sys_munlock,sys32_munlock_wrapper)
 +SYSCALL(sys_mlockall,sys_mlockall,sys32_mlockall_wrapper)
 +SYSCALL(sys_munlockall,sys_munlockall,sys_munlockall)
 +SYSCALL(sys_sched_setparam,sys_sched_setparam,sys32_sched_setparam_wrapper)
 +SYSCALL(sys_sched_getparam,sys_sched_getparam,sys32_sched_getparam_wrapper)	/* 155 */
 +SYSCALL(sys_sched_setscheduler,sys_sched_setscheduler,sys32_sched_setscheduler_wrapper)
 +SYSCALL(sys_sched_getscheduler,sys_sched_getscheduler,sys32_sched_getscheduler_wrapper)
 +SYSCALL(sys_sched_yield,sys_sched_yield,sys_sched_yield)
 +SYSCALL(sys_sched_get_priority_max,sys_sched_get_priority_max,sys32_sched_get_priority_max_wrapper)
 +SYSCALL(sys_sched_get_priority_min,sys_sched_get_priority_min,sys32_sched_get_priority_min_wrapper)	/* 160 */
 +SYSCALL(sys_sched_rr_get_interval,sys_sched_rr_get_interval,compat_sys_sched_rr_get_interval)
 +SYSCALL(sys_nanosleep,sys_nanosleep,compat_sys_nanosleep_wrapper)
 +SYSCALL(sys_mremap,sys_mremap,sys32_mremap_wrapper)
 +SYSCALL(sys_setresuid16,sys_ni_syscall,sys32_setresuid16_wrapper)	/* old setresuid16 syscall */
 +SYSCALL(sys_getresuid16,sys_ni_syscall,sys32_getresuid16_wrapper)	/* 165 old getresuid16 syscall */
 +NI_SYSCALL							/* for vm86 */
 +NI_SYSCALL							/* old sys_query_module */
 +SYSCALL(sys_poll,sys_poll,sys32_poll_wrapper)
 +NI_SYSCALL							/* old nfsservctl */
 +SYSCALL(sys_setresgid16,sys_ni_syscall,sys32_setresgid16_wrapper)	/* 170 old setresgid16 syscall */
 +SYSCALL(sys_getresgid16,sys_ni_syscall,sys32_getresgid16_wrapper)	/* old getresgid16 syscall */
 +SYSCALL(sys_prctl,sys_prctl,sys32_prctl_wrapper)
 +SYSCALL(sys_rt_sigreturn,sys_rt_sigreturn,sys32_rt_sigreturn)
 +SYSCALL(sys_rt_sigaction,sys_rt_sigaction,compat_sys_rt_sigaction)
 +SYSCALL(sys_rt_sigprocmask,sys_rt_sigprocmask,compat_sys_rt_sigprocmask) /* 175 */
 +SYSCALL(sys_rt_sigpending,sys_rt_sigpending,compat_sys_rt_sigpending)
 +SYSCALL(sys_rt_sigtimedwait,sys_rt_sigtimedwait,compat_sys_rt_sigtimedwait)
 +SYSCALL(sys_rt_sigqueueinfo,sys_rt_sigqueueinfo,compat_sys_rt_sigqueueinfo)
 +SYSCALL(sys_rt_sigsuspend,sys_rt_sigsuspend,compat_sys_rt_sigsuspend)
 +SYSCALL(sys_pread64,sys_pread64,sys32_pread64_wrapper)		/* 180 */
 +SYSCALL(sys_pwrite64,sys_pwrite64,sys32_pwrite64_wrapper)
 +SYSCALL(sys_chown16,sys_ni_syscall,sys32_chown16_wrapper)	/* old chown16 syscall */
 +SYSCALL(sys_getcwd,sys_getcwd,sys32_getcwd_wrapper)
 +SYSCALL(sys_capget,sys_capget,sys32_capget_wrapper)
 +SYSCALL(sys_capset,sys_capset,sys32_capset_wrapper)		/* 185 */
 +SYSCALL(sys_sigaltstack,sys_sigaltstack,compat_sys_sigaltstack)
 +SYSCALL(sys_sendfile,sys_sendfile64,compat_sys_sendfile)
 +NI_SYSCALL							/* streams1 */
 +NI_SYSCALL							/* streams2 */
 +SYSCALL(sys_vfork,sys_vfork,sys_vfork)				/* 190 */
 +SYSCALL(sys_getrlimit,sys_getrlimit,compat_sys_getrlimit_wrapper)
 +SYSCALL(sys_mmap2,sys_mmap2,sys32_mmap2_wrapper)
 +SYSCALL(sys_truncate64,sys_ni_syscall,sys32_truncate64_wrapper)
 +SYSCALL(sys_ftruncate64,sys_ni_syscall,sys32_ftruncate64_wrapper)
 +SYSCALL(sys_stat64,sys_ni_syscall,sys32_stat64_wrapper)		/* 195 */
 +SYSCALL(sys_lstat64,sys_ni_syscall,sys32_lstat64_wrapper)
 +SYSCALL(sys_fstat64,sys_ni_syscall,sys32_fstat64_wrapper)
 +SYSCALL(sys_lchown,sys_lchown,sys32_lchown_wrapper)
 +SYSCALL(sys_getuid,sys_getuid,sys_getuid)
 +SYSCALL(sys_getgid,sys_getgid,sys_getgid)			/* 200 */
 +SYSCALL(sys_geteuid,sys_geteuid,sys_geteuid)
 +SYSCALL(sys_getegid,sys_getegid,sys_getegid)
 +SYSCALL(sys_setreuid,sys_setreuid,sys32_setreuid_wrapper)
 +SYSCALL(sys_setregid,sys_setregid,sys32_setregid_wrapper)
 +SYSCALL(sys_getgroups,sys_getgroups,sys32_getgroups_wrapper)	/* 205 */
 +SYSCALL(sys_setgroups,sys_setgroups,sys32_setgroups_wrapper)
 +SYSCALL(sys_fchown,sys_fchown,sys32_fchown_wrapper)
 +SYSCALL(sys_setresuid,sys_setresuid,sys32_setresuid_wrapper)
 +SYSCALL(sys_getresuid,sys_getresuid,sys32_getresuid_wrapper)
 +SYSCALL(sys_setresgid,sys_setresgid,sys32_setresgid_wrapper)	/* 210 */
 +SYSCALL(sys_getresgid,sys_getresgid,sys32_getresgid_wrapper)
 +SYSCALL(sys_chown,sys_chown,sys32_chown_wrapper)
 +SYSCALL(sys_setuid,sys_setuid,sys32_setuid_wrapper)
 +SYSCALL(sys_setgid,sys_setgid,sys32_setgid_wrapper)
 +SYSCALL(sys_setfsuid,sys_setfsuid,sys32_setfsuid_wrapper)	/* 215 */
 +SYSCALL(sys_setfsgid,sys_setfsgid,sys32_setfsgid_wrapper)
 +SYSCALL(sys_pivot_root,sys_pivot_root,sys32_pivot_root_wrapper)
 +SYSCALL(sys_mincore,sys_mincore,sys32_mincore_wrapper)
 +SYSCALL(sys_madvise,sys_madvise,sys32_madvise_wrapper)
 +SYSCALL(sys_getdents64,sys_getdents64,sys32_getdents64_wrapper)	/* 220 */
 +SYSCALL(sys_fcntl64,sys_ni_syscall,compat_sys_fcntl64_wrapper)
 +SYSCALL(sys_readahead,sys_readahead,sys32_readahead_wrapper)
 +SYSCALL(sys_sendfile64,sys_ni_syscall,compat_sys_sendfile64)
 +SYSCALL(sys_setxattr,sys_setxattr,sys32_setxattr_wrapper)
 +SYSCALL(sys_lsetxattr,sys_lsetxattr,sys32_lsetxattr_wrapper)	/* 225 */
 +SYSCALL(sys_fsetxattr,sys_fsetxattr,sys32_fsetxattr_wrapper)
 +SYSCALL(sys_getxattr,sys_getxattr,sys32_getxattr_wrapper)
 +SYSCALL(sys_lgetxattr,sys_lgetxattr,sys32_lgetxattr_wrapper)
 +SYSCALL(sys_fgetxattr,sys_fgetxattr,sys32_fgetxattr_wrapper)
 +SYSCALL(sys_listxattr,sys_listxattr,sys32_listxattr_wrapper)	/* 230 */
 +SYSCALL(sys_llistxattr,sys_llistxattr,sys32_llistxattr_wrapper)
 +SYSCALL(sys_flistxattr,sys_flistxattr,sys32_flistxattr_wrapper)
 +SYSCALL(sys_removexattr,sys_removexattr,sys32_removexattr_wrapper)
 +SYSCALL(sys_lremovexattr,sys_lremovexattr,sys32_lremovexattr_wrapper)
 +SYSCALL(sys_fremovexattr,sys_fremovexattr,sys32_fremovexattr_wrapper)	/* 235 */
 +SYSCALL(sys_gettid,sys_gettid,sys_gettid)
 +SYSCALL(sys_tkill,sys_tkill,sys_tkill_wrapper)
 +SYSCALL(sys_futex,sys_futex,compat_sys_futex)
 +SYSCALL(sys_sched_setaffinity,sys_sched_setaffinity,sys32_sched_setaffinity_wrapper)
 +SYSCALL(sys_sched_getaffinity,sys_sched_getaffinity,sys32_sched_getaffinity_wrapper)	/* 240 */
 +SYSCALL(sys_tgkill,sys_tgkill,sys_tgkill_wrapper)
 +NI_SYSCALL							/* reserved for TUX */
 +SYSCALL(sys_io_setup,sys_io_setup,sys32_io_setup_wrapper)
 +SYSCALL(sys_io_destroy,sys_io_destroy,sys32_io_destroy_wrapper)
 +SYSCALL(sys_io_getevents,sys_io_getevents,sys32_io_getevents_wrapper)	/* 245 */
 +SYSCALL(sys_io_submit,sys_io_submit,sys32_io_submit_wrapper)
 +SYSCALL(sys_io_cancel,sys_io_cancel,sys32_io_cancel_wrapper)
 +SYSCALL(sys_exit_group,sys_exit_group,sys32_exit_group_wrapper)
 +SYSCALL(sys_epoll_create,sys_epoll_create,sys_epoll_create_wrapper)
 +SYSCALL(sys_epoll_ctl,sys_epoll_ctl,sys_epoll_ctl_wrapper)	/* 250 */
 +SYSCALL(sys_epoll_wait,sys_epoll_wait,sys_epoll_wait_wrapper)
 +SYSCALL(sys_set_tid_address,sys_set_tid_address,sys32_set_tid_address_wrapper)
 +SYSCALL(sys_s390_fadvise64,sys_fadvise64_64,sys32_fadvise64_wrapper)
 +SYSCALL(sys_timer_create,sys_timer_create,sys32_timer_create_wrapper)
 +SYSCALL(sys_timer_settime,sys_timer_settime,sys32_timer_settime_wrapper)	/* 255 */
 +SYSCALL(sys_timer_gettime,sys_timer_gettime,sys32_timer_gettime_wrapper)
 +SYSCALL(sys_timer_getoverrun,sys_timer_getoverrun,sys32_timer_getoverrun_wrapper)
 +SYSCALL(sys_timer_delete,sys_timer_delete,sys32_timer_delete_wrapper)
 +SYSCALL(sys_clock_settime,sys_clock_settime,sys32_clock_settime_wrapper)
 +SYSCALL(sys_clock_gettime,sys_clock_gettime,sys32_clock_gettime_wrapper)	/* 260 */
 +SYSCALL(sys_clock_getres,sys_clock_getres,sys32_clock_getres_wrapper)
 +SYSCALL(sys_clock_nanosleep,sys_clock_nanosleep,sys32_clock_nanosleep_wrapper)
 +NI_SYSCALL							/* reserved for vserver */
 +SYSCALL(sys_s390_fadvise64_64,sys_ni_syscall,sys32_fadvise64_64_wrapper)
 +SYSCALL(sys_statfs64,sys_statfs64,compat_sys_statfs64_wrapper)
 +SYSCALL(sys_fstatfs64,sys_fstatfs64,compat_sys_fstatfs64_wrapper)
 +SYSCALL(sys_remap_file_pages,sys_remap_file_pages,sys32_remap_file_pages_wrapper)
 +NI_SYSCALL							/* 268 sys_mbind */
 +NI_SYSCALL							/* 269 sys_get_mempolicy */
 +NI_SYSCALL							/* 270 sys_set_mempolicy */
 +SYSCALL(sys_mq_open,sys_mq_open,compat_sys_mq_open_wrapper)
 +SYSCALL(sys_mq_unlink,sys_mq_unlink,sys32_mq_unlink_wrapper)
 +SYSCALL(sys_mq_timedsend,sys_mq_timedsend,compat_sys_mq_timedsend_wrapper)
 +SYSCALL(sys_mq_timedreceive,sys_mq_timedreceive,compat_sys_mq_timedreceive_wrapper)
 +SYSCALL(sys_mq_notify,sys_mq_notify,compat_sys_mq_notify_wrapper) /* 275 */
 +SYSCALL(sys_mq_getsetattr,sys_mq_getsetattr,compat_sys_mq_getsetattr_wrapper)
 +SYSCALL(sys_kexec_load,sys_kexec_load,compat_sys_kexec_load_wrapper)
 +SYSCALL(sys_add_key,sys_add_key,compat_sys_add_key_wrapper)
 +SYSCALL(sys_request_key,sys_request_key,compat_sys_request_key_wrapper)
 +SYSCALL(sys_keyctl,sys_keyctl,compat_sys_keyctl_wrapper)		/* 280 */
 +SYSCALL(sys_waitid,sys_waitid,compat_sys_waitid)
 +SYSCALL(sys_ioprio_set,sys_ioprio_set,sys_ioprio_set_wrapper)
 +SYSCALL(sys_ioprio_get,sys_ioprio_get,sys_ioprio_get_wrapper)
 +SYSCALL(sys_inotify_init,sys_inotify_init,sys_inotify_init)
 +SYSCALL(sys_inotify_add_watch,sys_inotify_add_watch,sys_inotify_add_watch_wrapper)	/* 285 */
 +SYSCALL(sys_inotify_rm_watch,sys_inotify_rm_watch,sys_inotify_rm_watch_wrapper)
 +NI_SYSCALL							/* 287 sys_migrate_pages */
 +SYSCALL(sys_openat,sys_openat,compat_sys_openat)
 +SYSCALL(sys_mkdirat,sys_mkdirat,sys_mkdirat_wrapper)
 +SYSCALL(sys_mknodat,sys_mknodat,sys_mknodat_wrapper)	/* 290 */
 +SYSCALL(sys_fchownat,sys_fchownat,sys_fchownat_wrapper)
 +SYSCALL(sys_futimesat,sys_futimesat,compat_sys_futimesat_wrapper)
 +SYSCALL(sys_fstatat64,sys_newfstatat,sys32_fstatat64_wrapper)
 +SYSCALL(sys_unlinkat,sys_unlinkat,sys_unlinkat_wrapper)
 +SYSCALL(sys_renameat,sys_renameat,sys_renameat_wrapper)	/* 295 */
 +SYSCALL(sys_linkat,sys_linkat,sys_linkat_wrapper)
 +SYSCALL(sys_symlinkat,sys_symlinkat,sys_symlinkat_wrapper)
 +SYSCALL(sys_readlinkat,sys_readlinkat,sys_readlinkat_wrapper)
 +SYSCALL(sys_fchmodat,sys_fchmodat,sys_fchmodat_wrapper)
 +SYSCALL(sys_faccessat,sys_faccessat,sys_faccessat_wrapper)	/* 300 */
 +SYSCALL(sys_pselect6,sys_pselect6,compat_sys_pselect6_wrapper)
 +SYSCALL(sys_ppoll,sys_ppoll,compat_sys_ppoll_wrapper)
 +SYSCALL(sys_unshare,sys_unshare,sys_unshare_wrapper)
 +SYSCALL(sys_set_robust_list,sys_set_robust_list,compat_sys_set_robust_list)
 +SYSCALL(sys_get_robust_list,sys_get_robust_list,compat_sys_get_robust_list)
 +SYSCALL(sys_splice,sys_splice,sys_splice_wrapper)
 +SYSCALL(sys_sync_file_range,sys_sync_file_range,sys_sync_file_range_wrapper)
 +SYSCALL(sys_tee,sys_tee,sys_tee_wrapper)
 +SYSCALL(sys_vmsplice,sys_vmsplice,compat_sys_vmsplice)
 +NI_SYSCALL							/* 310 sys_move_pages */
 +SYSCALL(sys_getcpu,sys_getcpu,sys_getcpu_wrapper)
 +SYSCALL(sys_epoll_pwait,sys_epoll_pwait,compat_sys_epoll_pwait)
 +SYSCALL(sys_utimes,sys_utimes,compat_sys_utimes_wrapper)
 +SYSCALL(sys_s390_fallocate,sys_fallocate,sys_fallocate_wrapper)
 +SYSCALL(sys_utimensat,sys_utimensat,compat_sys_utimensat_wrapper)	/* 315 */
 +SYSCALL(sys_signalfd,sys_signalfd,compat_sys_signalfd)
  NI_SYSCALL						/* 317 old sys_timer_fd */
++<<<<<<< HEAD
 +SYSCALL(sys_eventfd,sys_eventfd,sys_eventfd_wrapper)
 +SYSCALL(sys_timerfd_create,sys_timerfd_create,sys_timerfd_create_wrapper)
 +SYSCALL(sys_timerfd_settime,sys_timerfd_settime,compat_sys_timerfd_settime) /* 320 */
 +SYSCALL(sys_timerfd_gettime,sys_timerfd_gettime,compat_sys_timerfd_gettime)
 +SYSCALL(sys_signalfd4,sys_signalfd4,compat_sys_signalfd4)
 +SYSCALL(sys_eventfd2,sys_eventfd2,sys_eventfd2_wrapper)
 +SYSCALL(sys_inotify_init1,sys_inotify_init1,sys_inotify_init1_wrapper)
 +SYSCALL(sys_pipe2,sys_pipe2,sys_pipe2_wrapper) /* 325 */
 +SYSCALL(sys_dup3,sys_dup3,sys_dup3_wrapper)
 +SYSCALL(sys_epoll_create1,sys_epoll_create1,sys_epoll_create1_wrapper)
 +SYSCALL(sys_preadv,sys_preadv,compat_sys_preadv)
 +SYSCALL(sys_pwritev,sys_pwritev,compat_sys_pwritev)
 +SYSCALL(sys_rt_tgsigqueueinfo,sys_rt_tgsigqueueinfo,compat_sys_rt_tgsigqueueinfo) /* 330 */
 +SYSCALL(sys_perf_event_open,sys_perf_event_open,sys_perf_event_open_wrapper)
 +SYSCALL(sys_fanotify_init,sys_fanotify_init,sys_fanotify_init_wrapper)
 +SYSCALL(sys_fanotify_mark,sys_fanotify_mark,compat_sys_fanotify_mark)
 +SYSCALL(sys_prlimit64,sys_prlimit64,sys_prlimit64_wrapper)
 +SYSCALL(sys_name_to_handle_at,sys_name_to_handle_at,sys_name_to_handle_at_wrapper) /* 335 */
 +SYSCALL(sys_open_by_handle_at,sys_open_by_handle_at,compat_sys_open_by_handle_at)
 +SYSCALL(sys_clock_adjtime,sys_clock_adjtime,compat_sys_clock_adjtime_wrapper)
 +SYSCALL(sys_syncfs,sys_syncfs,sys_syncfs_wrapper)
 +SYSCALL(sys_setns,sys_setns,sys_setns_wrapper)
 +SYSCALL(sys_process_vm_readv,sys_process_vm_readv,compat_sys_process_vm_readv_wrapper) /* 340 */
 +SYSCALL(sys_process_vm_writev,sys_process_vm_writev,compat_sys_process_vm_writev_wrapper)
 +SYSCALL(sys_ni_syscall,sys_s390_runtime_instr,sys_s390_runtime_instr_wrapper)
 +SYSCALL(sys_kcmp,sys_kcmp,sys_kcmp_wrapper)
 +SYSCALL(sys_finit_module,sys_finit_module,sys_finit_module_wrapper)
 +SYSCALL(sys_sched_setattr,sys_sched_setattr,sys_sched_setattr_wrapper) /* 345 */
 +SYSCALL(sys_sched_getattr,sys_sched_getattr,sys_sched_getattr_wrapper)
 +NI_SYSCALL					/* 347 sys_renameat2 */
 +NI_SYSCALL					/* 348 sys_seccomp */
 +SYSCALL(sys_getrandom,sys_getrandom,sys_getrandom_wrapper)
 +NI_SYSCALL					/* 350 sys_memfd_create */
 +NI_SYSCALL					/* 351 sys_bpf */
 +SYSCALL(sys_ni_syscall,sys_s390_pci_mmio_write,sys_s390_pci_mmio_write_wrapper)
 +SYSCALL(sys_ni_syscall,sys_s390_pci_mmio_read,sys_s390_pci_mmio_read_wrapper)
 +NI_SYSCALL					/* 354 sys_execveat */
 +NI_SYSCALL					/* 355 sys_userfaultfd */
 +NI_SYSCALL					/* 356 sys_membarrier */
 +NI_SYSCALL					/* 357 sys_recvmmsg */
 +NI_SYSCALL					/* 358 sys_sendmmsg */
 +NI_SYSCALL					/* 359 sys_socket */
 +NI_SYSCALL					/* 360 sys_socketpair */
 +NI_SYSCALL					/* 361 sys_bind */
 +NI_SYSCALL					/* 362 sys_connect */
 +NI_SYSCALL					/* 363 sys_listen */
 +NI_SYSCALL					/* 364 sys_accept4 */
 +NI_SYSCALL					/* 365 sys_getsockopt */
 +NI_SYSCALL					/* 366 sys_setsockopt */
 +NI_SYSCALL					/* 367 sys_getsockname */
 +NI_SYSCALL					/* 368 sys_getpeername */
 +NI_SYSCALL					/* 369 sys_sendto */
 +NI_SYSCALL					/* 370 sys_sendmsg */
 +NI_SYSCALL					/* 371 sys_recvfrom */
 +NI_SYSCALL					/* 372 sys_recvmsg */
 +NI_SYSCALL					/* 373 sys_shutdown */
 +NI_SYSCALL					/* 374 sys_mlock2 */
 +SYSCALL(sys_copy_file_range,sys_copy_file_range,sys_copy_file_range_wrapper) /* 375 */
++=======
+ SYSCALL(sys_eventfd,sys_eventfd)
+ SYSCALL(sys_timerfd_create,sys_timerfd_create)
+ SYSCALL(sys_timerfd_settime,compat_sys_timerfd_settime) /* 320 */
+ SYSCALL(sys_timerfd_gettime,compat_sys_timerfd_gettime)
+ SYSCALL(sys_signalfd4,compat_sys_signalfd4)
+ SYSCALL(sys_eventfd2,sys_eventfd2)
+ SYSCALL(sys_inotify_init1,sys_inotify_init1)
+ SYSCALL(sys_pipe2,compat_sys_pipe2)			/* 325 */
+ SYSCALL(sys_dup3,sys_dup3)
+ SYSCALL(sys_epoll_create1,sys_epoll_create1)
+ SYSCALL(sys_preadv,compat_sys_preadv)
+ SYSCALL(sys_pwritev,compat_sys_pwritev)
+ SYSCALL(sys_rt_tgsigqueueinfo,compat_sys_rt_tgsigqueueinfo) /* 330 */
+ SYSCALL(sys_perf_event_open,compat_sys_perf_event_open)
+ SYSCALL(sys_fanotify_init,sys_fanotify_init)
+ SYSCALL(sys_fanotify_mark,compat_sys_fanotify_mark)
+ SYSCALL(sys_prlimit64,compat_sys_prlimit64)
+ SYSCALL(sys_name_to_handle_at,compat_sys_name_to_handle_at) /* 335 */
+ SYSCALL(sys_open_by_handle_at,compat_sys_open_by_handle_at)
+ SYSCALL(sys_clock_adjtime,compat_sys_clock_adjtime)
+ SYSCALL(sys_syncfs,sys_syncfs)
+ SYSCALL(sys_setns,sys_setns)
+ SYSCALL(sys_process_vm_readv,compat_sys_process_vm_readv) /* 340 */
+ SYSCALL(sys_process_vm_writev,compat_sys_process_vm_writev)
+ SYSCALL(sys_s390_runtime_instr,sys_s390_runtime_instr)
+ SYSCALL(sys_kcmp,compat_sys_kcmp)
+ SYSCALL(sys_finit_module,compat_sys_finit_module)
+ SYSCALL(sys_sched_setattr,compat_sys_sched_setattr)	/* 345 */
+ SYSCALL(sys_sched_getattr,compat_sys_sched_getattr)
+ SYSCALL(sys_renameat2,compat_sys_renameat2)
+ SYSCALL(sys_seccomp,compat_sys_seccomp)
+ SYSCALL(sys_getrandom,compat_sys_getrandom)
+ SYSCALL(sys_memfd_create,compat_sys_memfd_create)	/* 350 */
+ SYSCALL(sys_bpf,compat_sys_bpf)
+ SYSCALL(sys_s390_pci_mmio_write,compat_sys_s390_pci_mmio_write)
+ SYSCALL(sys_s390_pci_mmio_read,compat_sys_s390_pci_mmio_read)
+ SYSCALL(sys_execveat,compat_sys_execveat)
+ SYSCALL(sys_userfaultfd,sys_userfaultfd)		/* 355 */
+ SYSCALL(sys_membarrier,sys_membarrier)
+ SYSCALL(sys_recvmmsg,compat_sys_recvmmsg)
+ SYSCALL(sys_sendmmsg,compat_sys_sendmmsg)
+ SYSCALL(sys_socket,sys_socket)
+ SYSCALL(sys_socketpair,compat_sys_socketpair)		/* 360 */
+ SYSCALL(sys_bind,sys_bind)
+ SYSCALL(sys_connect,sys_connect)
+ SYSCALL(sys_listen,sys_listen)
+ SYSCALL(sys_accept4,sys_accept4)
+ SYSCALL(sys_getsockopt,compat_sys_getsockopt)		/* 365 */
+ SYSCALL(sys_setsockopt,compat_sys_setsockopt)
+ SYSCALL(sys_getsockname,compat_sys_getsockname)
+ SYSCALL(sys_getpeername,compat_sys_getpeername)
+ SYSCALL(sys_sendto,compat_sys_sendto)
+ SYSCALL(sys_sendmsg,compat_sys_sendmsg)			/* 370 */
+ SYSCALL(sys_recvfrom,compat_sys_recvfrom)
+ SYSCALL(sys_recvmsg,compat_sys_recvmsg)
+ SYSCALL(sys_shutdown,sys_shutdown)
+ SYSCALL(sys_mlock2,compat_sys_mlock2)
+ SYSCALL(sys_copy_file_range,compat_sys_copy_file_range) /* 375 */
+ SYSCALL(sys_preadv2,compat_sys_preadv2)
+ SYSCALL(sys_pwritev2,compat_sys_pwritev2)
+ SYSCALL(sys_s390_guarded_storage,compat_sys_s390_guarded_storage) /* 378 */
+ SYSCALL(sys_statx,compat_sys_statx)
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
diff --cc arch/s390/kvm/interrupt.c
index 7f1f7ac5cf7f,169558dc7daf..000000000000
--- a/arch/s390/kvm/interrupt.c
+++ b/arch/s390/kvm/interrupt.c
@@@ -140,481 -292,1684 +140,585 @@@ static void __reset_intercept_indicator
  
  static void __set_cpuflag(struct kvm_vcpu *vcpu, u32 flag)
  {
 -	atomic_or(flag, &vcpu->arch.sie_block->cpuflags);
 -}
 -
 -static void set_intercept_indicators_io(struct kvm_vcpu *vcpu)
 -{
 -	if (!(pending_irqs(vcpu) & IRQ_PEND_IO_MASK))
 -		return;
 -	else if (psw_ioint_disabled(vcpu))
 -		__set_cpuflag(vcpu, CPUSTAT_IO_INT);
 -	else
 -		vcpu->arch.sie_block->lctl |= LCTL_CR6;
 -}
 -
 -static void set_intercept_indicators_ext(struct kvm_vcpu *vcpu)
 -{
 -	if (!(pending_irqs(vcpu) & IRQ_PEND_EXT_MASK))
 -		return;
 -	if (psw_extint_disabled(vcpu))
 -		__set_cpuflag(vcpu, CPUSTAT_EXT_INT);
 -	else
 -		vcpu->arch.sie_block->lctl |= LCTL_CR0;
 -}
 -
 -static void set_intercept_indicators_mchk(struct kvm_vcpu *vcpu)
 -{
 -	if (!(pending_irqs(vcpu) & IRQ_PEND_MCHK_MASK))
 -		return;
 -	if (psw_mchk_disabled(vcpu))
 -		vcpu->arch.sie_block->ictl |= ICTL_LPSW;
 -	else
 -		vcpu->arch.sie_block->lctl |= LCTL_CR14;
 +	atomic_set_mask(flag, &vcpu->arch.sie_block->cpuflags);
  }
  
 -static void set_intercept_indicators_stop(struct kvm_vcpu *vcpu)
 +static void __set_intercept_indicator(struct kvm_vcpu *vcpu,
 +				      struct kvm_s390_interrupt_info *inti)
  {
 -	if (kvm_s390_is_stop_irq_pending(vcpu))
 +	switch (inti->type) {
 +	case KVM_S390_INT_EXTERNAL_CALL:
 +	case KVM_S390_INT_EMERGENCY:
 +	case KVM_S390_INT_SERVICE:
 +	case KVM_S390_INT_VIRTIO:
 +		if (psw_extint_disabled(vcpu))
 +			__set_cpuflag(vcpu, CPUSTAT_EXT_INT);
 +		else
 +			vcpu->arch.sie_block->lctl |= LCTL_CR0;
 +		break;
 +	case KVM_S390_SIGP_STOP:
  		__set_cpuflag(vcpu, CPUSTAT_STOP_INT);
++<<<<<<< HEAD
 +		break;
 +	case KVM_S390_MCHK:
 +		if (psw_mchk_disabled(vcpu))
 +			vcpu->arch.sie_block->ictl |= ICTL_LPSW;
 +		else
 +			vcpu->arch.sie_block->lctl |= LCTL_CR14;
 +		break;
 +	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 +		if (psw_ioint_disabled(vcpu))
 +			__set_cpuflag(vcpu, CPUSTAT_IO_INT);
 +		else
 +			vcpu->arch.sie_block->lctl |= LCTL_CR6;
 +		break;
 +	default:
 +		BUG();
++=======
+ }
+ 
+ /* Set interception request for non-deliverable interrupts */
+ static void set_intercept_indicators(struct kvm_vcpu *vcpu)
+ {
+ 	set_intercept_indicators_io(vcpu);
+ 	set_intercept_indicators_ext(vcpu);
+ 	set_intercept_indicators_mchk(vcpu);
+ 	set_intercept_indicators_stop(vcpu);
+ }
+ 
+ static int __must_check __deliver_cpu_timer(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
+ 	int rc;
+ 
+ 	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, KVM_S390_INT_CPU_TIMER,
+ 					 0, 0);
+ 
+ 	rc  = put_guest_lc(vcpu, EXT_IRQ_CPU_TIMER,
+ 			   (u16 *)__LC_EXT_INT_CODE);
+ 	rc |= put_guest_lc(vcpu, 0, (u16 *)__LC_EXT_CPU_ADDR);
+ 	rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
+ 			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
+ 	rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW,
+ 			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
+ 	clear_bit(IRQ_PEND_EXT_CPU_TIMER, &li->pending_irqs);
+ 	return rc ? -EFAULT : 0;
+ }
+ 
+ static int __must_check __deliver_ckc(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
+ 	int rc;
+ 
+ 	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, KVM_S390_INT_CLOCK_COMP,
+ 					 0, 0);
+ 
+ 	rc  = put_guest_lc(vcpu, EXT_IRQ_CLK_COMP,
+ 			   (u16 __user *)__LC_EXT_INT_CODE);
+ 	rc |= put_guest_lc(vcpu, 0, (u16 *)__LC_EXT_CPU_ADDR);
+ 	rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
+ 			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
+ 	rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW,
+ 			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
+ 	clear_bit(IRQ_PEND_EXT_CLOCK_COMP, &li->pending_irqs);
+ 	return rc ? -EFAULT : 0;
+ }
+ 
+ static int __must_check __deliver_pfault_init(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
+ 	struct kvm_s390_ext_info ext;
+ 	int rc;
+ 
+ 	spin_lock(&li->lock);
+ 	ext = li->irq.ext;
+ 	clear_bit(IRQ_PEND_PFAULT_INIT, &li->pending_irqs);
+ 	li->irq.ext.ext_params2 = 0;
+ 	spin_unlock(&li->lock);
+ 
+ 	VCPU_EVENT(vcpu, 4, "deliver: pfault init token 0x%llx",
+ 		   ext.ext_params2);
+ 	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id,
+ 					 KVM_S390_INT_PFAULT_INIT,
+ 					 0, ext.ext_params2);
+ 
+ 	rc  = put_guest_lc(vcpu, EXT_IRQ_CP_SERVICE, (u16 *) __LC_EXT_INT_CODE);
+ 	rc |= put_guest_lc(vcpu, PFAULT_INIT, (u16 *) __LC_EXT_CPU_ADDR);
+ 	rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
+ 			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
+ 	rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW,
+ 			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
+ 	rc |= put_guest_lc(vcpu, ext.ext_params2, (u64 *) __LC_EXT_PARAMS2);
+ 	return rc ? -EFAULT : 0;
+ }
+ 
+ static int __write_machine_check(struct kvm_vcpu *vcpu,
+ 				 struct kvm_s390_mchk_info *mchk)
+ {
+ 	unsigned long ext_sa_addr;
+ 	freg_t fprs[NUM_FPRS];
+ 	union mci mci;
+ 	int rc;
+ 
+ 	mci.val = mchk->mcic;
+ 	/* take care of lazy register loading */
+ 	save_fpu_regs();
+ 	save_access_regs(vcpu->run->s.regs.acrs);
+ 
+ 	/* Extended save area */
+ 	rc = read_guest_lc(vcpu, __LC_MCESAD, &ext_sa_addr,
+ 			   sizeof(unsigned long));
+ 	/* Only bits 0-53 are used for address formation */
+ 	ext_sa_addr &= ~0x3ffUL;
+ 	if (!rc && mci.vr && ext_sa_addr && test_kvm_facility(vcpu->kvm, 129)) {
+ 		if (write_guest_abs(vcpu, ext_sa_addr, vcpu->run->s.regs.vrs,
+ 				    512))
+ 			mci.vr = 0;
+ 	} else {
+ 		mci.vr = 0;
++>>>>>>> 916cda1aa1b4 (s390: add a system call for guarded storage)
  	}
 -
 -	/* General interruption information */
 -	rc |= put_guest_lc(vcpu, 1, (u8 __user *) __LC_AR_MODE_ID);
 -	rc |= write_guest_lc(vcpu, __LC_MCK_OLD_PSW,
 -			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= read_guest_lc(vcpu, __LC_MCK_NEW_PSW,
 -			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= put_guest_lc(vcpu, mci.val, (u64 __user *) __LC_MCCK_CODE);
 -
 -	/* Register-save areas */
 -	if (MACHINE_HAS_VX) {
 -		convert_vx_to_fp(fprs, (__vector128 *) vcpu->run->s.regs.vrs);
 -		rc |= write_guest_lc(vcpu, __LC_FPREGS_SAVE_AREA, fprs, 128);
 -	} else {
 -		rc |= write_guest_lc(vcpu, __LC_FPREGS_SAVE_AREA,
 -				     vcpu->run->s.regs.fprs, 128);
 -	}
 -	rc |= write_guest_lc(vcpu, __LC_GPREGS_SAVE_AREA,
 -			     vcpu->run->s.regs.gprs, 128);
 -	rc |= put_guest_lc(vcpu, current->thread.fpu.fpc,
 -			   (u32 __user *) __LC_FP_CREG_SAVE_AREA);
 -	rc |= put_guest_lc(vcpu, vcpu->arch.sie_block->todpr,
 -			   (u32 __user *) __LC_TOD_PROGREG_SAVE_AREA);
 -	rc |= put_guest_lc(vcpu, kvm_s390_get_cpu_timer(vcpu),
 -			   (u64 __user *) __LC_CPU_TIMER_SAVE_AREA);
 -	rc |= put_guest_lc(vcpu, vcpu->arch.sie_block->ckc >> 8,
 -			   (u64 __user *) __LC_CLOCK_COMP_SAVE_AREA);
 -	rc |= write_guest_lc(vcpu, __LC_AREGS_SAVE_AREA,
 -			     &vcpu->run->s.regs.acrs, 64);
 -	rc |= write_guest_lc(vcpu, __LC_CREGS_SAVE_AREA,
 -			     &vcpu->arch.sie_block->gcr, 128);
 -
 -	/* Extended interruption information */
 -	rc |= put_guest_lc(vcpu, mchk->ext_damage_code,
 -			   (u32 __user *) __LC_EXT_DAMAGE_CODE);
 -	rc |= put_guest_lc(vcpu, mchk->failing_storage_address,
 -			   (u64 __user *) __LC_MCCK_FAIL_STOR_ADDR);
 -	rc |= write_guest_lc(vcpu, __LC_PSW_SAVE_AREA, &mchk->fixed_logout,
 -			     sizeof(mchk->fixed_logout));
 -	return rc ? -EFAULT : 0;
  }
  
 -static int __must_check __deliver_machine_check(struct kvm_vcpu *vcpu)
 +static void __do_deliver_interrupt(struct kvm_vcpu *vcpu,
 +				   struct kvm_s390_interrupt_info *inti)
  {
 -	struct kvm_s390_float_interrupt *fi = &vcpu->kvm->arch.float_int;
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_mchk_info mchk = {};
 -	int deliver = 0;
 +	const unsigned short table[] = { 2, 4, 4, 6 };
  	int rc = 0;
  
 -	spin_lock(&fi->lock);
 -	spin_lock(&li->lock);
 -	if (test_bit(IRQ_PEND_MCHK_EX, &li->pending_irqs) ||
 -	    test_bit(IRQ_PEND_MCHK_REP, &li->pending_irqs)) {
 -		/*
 -		 * If there was an exigent machine check pending, then any
 -		 * repressible machine checks that might have been pending
 -		 * are indicated along with it, so always clear bits for
 -		 * repressible and exigent interrupts
 -		 */
 -		mchk = li->irq.mchk;
 -		clear_bit(IRQ_PEND_MCHK_EX, &li->pending_irqs);
 -		clear_bit(IRQ_PEND_MCHK_REP, &li->pending_irqs);
 -		memset(&li->irq.mchk, 0, sizeof(mchk));
 -		deliver = 1;
 +	switch (inti->type) {
 +	case KVM_S390_INT_EMERGENCY:
 +		VCPU_EVENT(vcpu, 4, "%s", "interrupt: sigp emerg");
 +		vcpu->stat.deliver_emergency_signal++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 inti->emerg.code, 0);
 +		rc  = put_guest(vcpu, 0x1201, (u16 __user *)__LC_EXT_INT_CODE);
 +		rc |= put_guest(vcpu, inti->emerg.code,
 +				(u16 __user *)__LC_EXT_CPU_ADDR);
 +		rc |= copy_to_guest(vcpu, __LC_EXT_OLD_PSW,
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      __LC_EXT_NEW_PSW, sizeof(psw_t));
 +		break;
 +	case KVM_S390_INT_EXTERNAL_CALL:
 +		VCPU_EVENT(vcpu, 4, "%s", "interrupt: sigp ext call");
 +		vcpu->stat.deliver_external_call++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 inti->extcall.code, 0);
 +		rc  = put_guest(vcpu, 0x1202, (u16 __user *)__LC_EXT_INT_CODE);
 +		rc |= put_guest(vcpu, inti->extcall.code,
 +				(u16 __user *)__LC_EXT_CPU_ADDR);
 +		rc |= copy_to_guest(vcpu, __LC_EXT_OLD_PSW,
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      __LC_EXT_NEW_PSW, sizeof(psw_t));
 +		break;
 +	case KVM_S390_INT_SERVICE:
 +		VCPU_EVENT(vcpu, 4, "interrupt: sclp parm:%x",
 +			   inti->ext.ext_params);
 +		vcpu->stat.deliver_service_signal++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 inti->ext.ext_params, 0);
 +		rc  = put_guest(vcpu, 0x2401, (u16 __user *)__LC_EXT_INT_CODE);
 +		rc |= copy_to_guest(vcpu, __LC_EXT_OLD_PSW,
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      __LC_EXT_NEW_PSW, sizeof(psw_t));
 +		rc |= put_guest(vcpu, inti->ext.ext_params,
 +				(u32 __user *)__LC_EXT_PARAMS);
 +		break;
 +	case KVM_S390_INT_VIRTIO:
 +		VCPU_EVENT(vcpu, 4, "interrupt: virtio parm:%x,parm64:%llx",
 +			   inti->ext.ext_params, inti->ext.ext_params2);
 +		vcpu->stat.deliver_virtio_interrupt++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 inti->ext.ext_params,
 +						 inti->ext.ext_params2);
 +		rc  = put_guest(vcpu, 0x2603, (u16 __user *)__LC_EXT_INT_CODE);
 +		rc |= put_guest(vcpu, 0x0d00, (u16 __user *)__LC_EXT_CPU_ADDR);
 +		rc |= copy_to_guest(vcpu, __LC_EXT_OLD_PSW,
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      __LC_EXT_NEW_PSW, sizeof(psw_t));
 +		rc |= put_guest(vcpu, inti->ext.ext_params,
 +				(u32 __user *)__LC_EXT_PARAMS);
 +		rc |= put_guest(vcpu, inti->ext.ext_params2,
 +				(u64 __user *)__LC_EXT_PARAMS2);
 +		break;
 +	case KVM_S390_SIGP_STOP:
 +		VCPU_EVENT(vcpu, 4, "%s", "interrupt: cpu stop");
 +		vcpu->stat.deliver_stop_signal++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 0, 0);
 +		__set_intercept_indicator(vcpu, inti);
 +		break;
 +
 +	case KVM_S390_SIGP_SET_PREFIX:
 +		VCPU_EVENT(vcpu, 4, "interrupt: set prefix to %x",
 +			   inti->prefix.address);
 +		vcpu->stat.deliver_prefix_signal++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 inti->prefix.address, 0);
 +		kvm_s390_set_prefix(vcpu, inti->prefix.address);
 +		break;
 +
 +	case KVM_S390_RESTART:
 +		VCPU_EVENT(vcpu, 4, "%s", "interrupt: cpu restart");
 +		vcpu->stat.deliver_restart_signal++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 0, 0);
 +		rc  = copy_to_guest(vcpu,
 +				    offsetof(struct _lowcore, restart_old_psw),
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      offsetof(struct _lowcore, restart_psw),
 +				      sizeof(psw_t));
 +		atomic_clear_mask(CPUSTAT_STOPPED, &vcpu->arch.sie_block->cpuflags);
 +		break;
 +	case KVM_S390_PROGRAM_INT:
 +		VCPU_EVENT(vcpu, 4, "interrupt: pgm check code:%x, ilc:%x",
 +			   inti->pgm.code,
 +			   table[vcpu->arch.sie_block->ipa >> 14]);
 +		vcpu->stat.deliver_program_int++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 inti->pgm.code, 0);
 +		rc  = put_guest(vcpu, inti->pgm.code, (u16 __user *)__LC_PGM_INT_CODE);
 +		rc |= put_guest(vcpu, table[vcpu->arch.sie_block->ipa >> 14],
 +				(u16 __user *)__LC_PGM_ILC);
 +		rc |= copy_to_guest(vcpu, __LC_PGM_OLD_PSW,
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      __LC_PGM_NEW_PSW, sizeof(psw_t));
 +		break;
 +
 +	case KVM_S390_MCHK:
 +		VCPU_EVENT(vcpu, 4, "interrupt: machine check mcic=%llx",
 +			   inti->mchk.mcic);
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 inti->mchk.cr14,
 +						 inti->mchk.mcic);
 +		rc  = kvm_s390_vcpu_store_status(vcpu,
 +						 KVM_S390_STORE_STATUS_PREFIXED);
 +		rc |= put_guest(vcpu, inti->mchk.mcic, (u64 __user *) __LC_MCCK_CODE);
 +		rc |= copy_to_guest(vcpu, __LC_MCK_OLD_PSW,
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      __LC_MCK_NEW_PSW, sizeof(psw_t));
 +		break;
 +
 +	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 +	{
 +		__u32 param0 = ((__u32)inti->io.subchannel_id << 16) |
 +			inti->io.subchannel_nr;
 +		__u64 param1 = ((__u64)inti->io.io_int_parm << 32) |
 +			inti->io.io_int_word;
 +		VCPU_EVENT(vcpu, 4, "interrupt: I/O %llx", inti->type);
 +		vcpu->stat.deliver_io_int++;
 +		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, inti->type,
 +						 param0, param1);
 +		rc  = put_guest(vcpu, inti->io.subchannel_id,
 +				(u16 __user *) __LC_SUBCHANNEL_ID);
 +		rc |= put_guest(vcpu, inti->io.subchannel_nr,
 +				(u16 __user *) __LC_SUBCHANNEL_NR);
 +		rc |= put_guest(vcpu, inti->io.io_int_parm,
 +				(u32 __user *) __LC_IO_INT_PARM);
 +		rc |= put_guest(vcpu, inti->io.io_int_word,
 +				(u32 __user *) __LC_IO_INT_WORD);
 +		rc |= copy_to_guest(vcpu, __LC_IO_OLD_PSW,
 +				    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 +		rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +				      __LC_IO_NEW_PSW, sizeof(psw_t));
 +		break;
  	}
 -	/*
 -	 * We indicate floating repressible conditions along with
 -	 * other pending conditions. Channel Report Pending and Channel
 -	 * Subsystem damage are the only two and and are indicated by
 -	 * bits in mcic and masked in cr14.
 -	 */
 -	if (test_and_clear_bit(IRQ_PEND_MCHK_REP, &fi->pending_irqs)) {
 -		mchk.mcic |= fi->mchk.mcic;
 -		mchk.cr14 |= fi->mchk.cr14;
 -		memset(&fi->mchk, 0, sizeof(mchk));
 -		deliver = 1;
 +	default:
 +		BUG();
  	}
 -	spin_unlock(&li->lock);
 -	spin_unlock(&fi->lock);
 -
 -	if (deliver) {
 -		VCPU_EVENT(vcpu, 3, "deliver: machine check mcic 0x%llx",
 -			   mchk.mcic);
 -		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id,
 -						 KVM_S390_MCHK,
 -						 mchk.cr14, mchk.mcic);
 -		rc = __write_machine_check(vcpu, &mchk);
 +	if (rc) {
 +		printk("kvm: The guest lowcore is not mapped during interrupt "
 +		       "delivery, killing userspace\n");
 +		do_exit(SIGKILL);
  	}
 -	return rc;
  }
  
 -static int __must_check __deliver_restart(struct kvm_vcpu *vcpu)
 +static int __try_deliver_ckc_interrupt(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
  	int rc;
  
 -	VCPU_EVENT(vcpu, 3, "%s", "deliver: cpu restart");
 -	vcpu->stat.deliver_restart_signal++;
 -	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, KVM_S390_RESTART, 0, 0);
 -
 -	rc  = write_guest_lc(vcpu,
 -			     offsetof(struct lowcore, restart_old_psw),
 -			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= read_guest_lc(vcpu, offsetof(struct lowcore, restart_psw),
 +	if (psw_extint_disabled(vcpu))
 +		return 0;
 +	if (!(vcpu->arch.sie_block->gcr[0] & 0x800ul))
 +		return 0;
 +	rc  = put_guest(vcpu, 0x1004, (u16 __user *)__LC_EXT_INT_CODE);
 +	rc |= copy_to_guest(vcpu, __LC_EXT_OLD_PSW,
  			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	clear_bit(IRQ_PEND_RESTART, &li->pending_irqs);
 -	return rc ? -EFAULT : 0;
 +	rc |= copy_from_guest(vcpu, &vcpu->arch.sie_block->gpsw,
 +			      __LC_EXT_NEW_PSW, sizeof(psw_t));
 +	if (rc) {
 +		printk("kvm: The guest lowcore is not mapped during interrupt "
 +			"delivery, killing userspace\n");
 +		do_exit(SIGKILL);
 +	}
 +	return 1;
  }
  
 -static int __must_check __deliver_set_prefix(struct kvm_vcpu *vcpu)
 +static int kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu)
  {
  	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_prefix_info prefix;
 +	struct kvm_s390_float_interrupt *fi = vcpu->arch.local_int.float_int;
 +	struct kvm_s390_interrupt_info  *inti;
 +	int rc = 0;
  
 -	spin_lock(&li->lock);
 -	prefix = li->irq.prefix;
 -	li->irq.prefix.address = 0;
 -	clear_bit(IRQ_PEND_SET_PREFIX, &li->pending_irqs);
 -	spin_unlock(&li->lock);
 +	if (atomic_read(&li->active)) {
 +		spin_lock_bh(&li->lock);
 +		list_for_each_entry(inti, &li->list, list)
 +			if (__interrupt_is_deliverable(vcpu, inti)) {
 +				rc = 1;
 +				break;
 +			}
 +		spin_unlock_bh(&li->lock);
 +	}
  
 -	vcpu->stat.deliver_prefix_signal++;
 -	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id,
 -					 KVM_S390_SIGP_SET_PREFIX,
 -					 prefix.address, 0);
 +	if ((!rc) && atomic_read(&fi->active)) {
 +		spin_lock(&fi->lock);
 +		list_for_each_entry(inti, &fi->list, list)
 +			if (__interrupt_is_deliverable(vcpu, inti)) {
 +				rc = 1;
 +				break;
 +			}
 +		spin_unlock(&fi->lock);
 +	}
  
 -	kvm_s390_set_prefix(vcpu, prefix.address);
 -	return 0;
 +	if ((!rc) && (vcpu->arch.sie_block->ckc <
 +		get_tod_clock_fast() + vcpu->arch.sie_block->epoch)) {
 +		if ((!psw_extint_disabled(vcpu)) &&
 +			(vcpu->arch.sie_block->gcr[0] & 0x800ul))
 +			rc = 1;
 +	}
 +
 +	return rc;
  }
  
 -static int __must_check __deliver_emergency_signal(struct kvm_vcpu *vcpu)
 +int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	int rc;
 -	int cpu_addr;
 -
 -	spin_lock(&li->lock);
 -	cpu_addr = find_first_bit(li->sigp_emerg_pending, KVM_MAX_VCPUS);
 -	clear_bit(cpu_addr, li->sigp_emerg_pending);
 -	if (bitmap_empty(li->sigp_emerg_pending, KVM_MAX_VCPUS))
 -		clear_bit(IRQ_PEND_EXT_EMERGENCY, &li->pending_irqs);
 -	spin_unlock(&li->lock);
 -
 -	VCPU_EVENT(vcpu, 4, "%s", "deliver: sigp emerg");
 -	vcpu->stat.deliver_emergency_signal++;
 -	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, KVM_S390_INT_EMERGENCY,
 -					 cpu_addr, 0);
 -
 -	rc  = put_guest_lc(vcpu, EXT_IRQ_EMERGENCY_SIG,
 -			   (u16 *)__LC_EXT_INT_CODE);
 -	rc |= put_guest_lc(vcpu, cpu_addr, (u16 *)__LC_EXT_CPU_ADDR);
 -	rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
 -			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW,
 -			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	return rc ? -EFAULT : 0;
 +	return 0;
  }
  
 -static int __must_check __deliver_external_call(struct kvm_vcpu *vcpu)
 +int kvm_s390_handle_wait(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_extcall_info extcall;
 -	int rc;
 -
 -	spin_lock(&li->lock);
 -	extcall = li->irq.extcall;
 -	li->irq.extcall.code = 0;
 -	clear_bit(IRQ_PEND_EXT_EXTERNAL, &li->pending_irqs);
 -	spin_unlock(&li->lock);
 +	u64 now, sltime;
 +	DECLARE_WAITQUEUE(wait, current);
  
 -	VCPU_EVENT(vcpu, 4, "%s", "deliver: sigp ext call");
 -	vcpu->stat.deliver_external_call++;
 -	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id,
 -					 KVM_S390_INT_EXTERNAL_CALL,
 -					 extcall.code, 0);
 +	vcpu->stat.exit_wait_state++;
 +	if (kvm_cpu_has_interrupt(vcpu))
 +		return 0;
  
 -	rc  = put_guest_lc(vcpu, EXT_IRQ_EXTERNAL_CALL,
 -			   (u16 *)__LC_EXT_INT_CODE);
 -	rc |= put_guest_lc(vcpu, extcall.code, (u16 *)__LC_EXT_CPU_ADDR);
 -	rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
 -			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW, &vcpu->arch.sie_block->gpsw,
 -			    sizeof(psw_t));
 -	return rc ? -EFAULT : 0;
 -}
 +	__set_cpu_idle(vcpu);
 +	spin_lock_bh(&vcpu->arch.local_int.lock);
 +	vcpu->arch.local_int.timer_due = 0;
 +	spin_unlock_bh(&vcpu->arch.local_int.lock);
  
 -static int __must_check __deliver_prog(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_pgm_info pgm_info;
 -	int rc = 0, nullifying = false;
 -	u16 ilen;
 +	if (psw_interrupts_disabled(vcpu)) {
 +		VCPU_EVENT(vcpu, 3, "%s", "disabled wait");
 +		__unset_cpu_idle(vcpu);
 +		return -EOPNOTSUPP; /* disabled wait */
 +	}
  
 -	spin_lock(&li->lock);
 -	pgm_info = li->irq.pgm;
 -	clear_bit(IRQ_PEND_PROG, &li->pending_irqs);
 -	memset(&li->irq.pgm, 0, sizeof(pgm_info));
 -	spin_unlock(&li->lock);
 +	if (psw_extint_disabled(vcpu) ||
 +	    (!(vcpu->arch.sie_block->gcr[0] & 0x800ul))) {
 +		VCPU_EVENT(vcpu, 3, "%s", "enabled wait w/o timer");
 +		goto no_timer;
 +	}
  
 -	ilen = pgm_info.flags & KVM_S390_PGM_FLAGS_ILC_MASK;
 -	VCPU_EVENT(vcpu, 3, "deliver: program irq code 0x%x, ilen:%d",
 -		   pgm_info.code, ilen);
 -	vcpu->stat.deliver_program_int++;
 -	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, KVM_S390_PROGRAM_INT,
 -					 pgm_info.code, 0);
 -
 -	switch (pgm_info.code & ~PGM_PER) {
 -	case PGM_AFX_TRANSLATION:
 -	case PGM_ASX_TRANSLATION:
 -	case PGM_EX_TRANSLATION:
 -	case PGM_LFX_TRANSLATION:
 -	case PGM_LSTE_SEQUENCE:
 -	case PGM_LSX_TRANSLATION:
 -	case PGM_LX_TRANSLATION:
 -	case PGM_PRIMARY_AUTHORITY:
 -	case PGM_SECONDARY_AUTHORITY:
 -		nullifying = true;
 -		/* fall through */
 -	case PGM_SPACE_SWITCH:
 -		rc = put_guest_lc(vcpu, pgm_info.trans_exc_code,
 -				  (u64 *)__LC_TRANS_EXC_CODE);
 -		break;
 -	case PGM_ALEN_TRANSLATION:
 -	case PGM_ALE_SEQUENCE:
 -	case PGM_ASTE_INSTANCE:
 -	case PGM_ASTE_SEQUENCE:
 -	case PGM_ASTE_VALIDITY:
 -	case PGM_EXTENDED_AUTHORITY:
 -		rc = put_guest_lc(vcpu, pgm_info.exc_access_id,
 -				  (u8 *)__LC_EXC_ACCESS_ID);
 -		nullifying = true;
 -		break;
 -	case PGM_ASCE_TYPE:
 -	case PGM_PAGE_TRANSLATION:
 -	case PGM_REGION_FIRST_TRANS:
 -	case PGM_REGION_SECOND_TRANS:
 -	case PGM_REGION_THIRD_TRANS:
 -	case PGM_SEGMENT_TRANSLATION:
 -		rc = put_guest_lc(vcpu, pgm_info.trans_exc_code,
 -				  (u64 *)__LC_TRANS_EXC_CODE);
 -		rc |= put_guest_lc(vcpu, pgm_info.exc_access_id,
 -				   (u8 *)__LC_EXC_ACCESS_ID);
 -		rc |= put_guest_lc(vcpu, pgm_info.op_access_id,
 -				   (u8 *)__LC_OP_ACCESS_ID);
 -		nullifying = true;
 -		break;
 -	case PGM_MONITOR:
 -		rc = put_guest_lc(vcpu, pgm_info.mon_class_nr,
 -				  (u16 *)__LC_MON_CLASS_NR);
 -		rc |= put_guest_lc(vcpu, pgm_info.mon_code,
 -				   (u64 *)__LC_MON_CODE);
 -		break;
 -	case PGM_VECTOR_PROCESSING:
 -	case PGM_DATA:
 -		rc = put_guest_lc(vcpu, pgm_info.data_exc_code,
 -				  (u32 *)__LC_DATA_EXC_CODE);
 -		break;
 -	case PGM_PROTECTION:
 -		rc = put_guest_lc(vcpu, pgm_info.trans_exc_code,
 -				  (u64 *)__LC_TRANS_EXC_CODE);
 -		rc |= put_guest_lc(vcpu, pgm_info.exc_access_id,
 -				   (u8 *)__LC_EXC_ACCESS_ID);
 -		break;
 -	case PGM_STACK_FULL:
 -	case PGM_STACK_EMPTY:
 -	case PGM_STACK_SPECIFICATION:
 -	case PGM_STACK_TYPE:
 -	case PGM_STACK_OPERATION:
 -	case PGM_TRACE_TABEL:
 -	case PGM_CRYPTO_OPERATION:
 -		nullifying = true;
 -		break;
 +	now = get_tod_clock_fast() + vcpu->arch.sie_block->epoch;
 +	if (vcpu->arch.sie_block->ckc < now) {
 +		__unset_cpu_idle(vcpu);
 +		return 0;
  	}
  
 -	if (pgm_info.code & PGM_PER) {
 -		rc |= put_guest_lc(vcpu, pgm_info.per_code,
 -				   (u8 *) __LC_PER_CODE);
 -		rc |= put_guest_lc(vcpu, pgm_info.per_atmid,
 -				   (u8 *)__LC_PER_ATMID);
 -		rc |= put_guest_lc(vcpu, pgm_info.per_address,
 -				   (u64 *) __LC_PER_ADDRESS);
 -		rc |= put_guest_lc(vcpu, pgm_info.per_access_id,
 -				   (u8 *) __LC_PER_ACCESS_ID);
 +	sltime = tod_to_ns(vcpu->arch.sie_block->ckc - now);
 +
 +	hrtimer_start(&vcpu->arch.ckc_timer, ktime_set (0, sltime) , HRTIMER_MODE_REL);
 +	VCPU_EVENT(vcpu, 5, "enabled wait via clock comparator: %llx ns", sltime);
 +no_timer:
 +	spin_lock(&vcpu->arch.local_int.float_int->lock);
 +	spin_lock_bh(&vcpu->arch.local_int.lock);
 +	add_wait_queue(&vcpu->wq, &wait);
 +	while (list_empty(&vcpu->arch.local_int.list) &&
 +		list_empty(&vcpu->arch.local_int.float_int->list) &&
 +		(!vcpu->arch.local_int.timer_due) &&
 +		!signal_pending(current)) {
 +		set_current_state(TASK_INTERRUPTIBLE);
 +		spin_unlock_bh(&vcpu->arch.local_int.lock);
 +		spin_unlock(&vcpu->arch.local_int.float_int->lock);
 +		schedule();
 +		spin_lock(&vcpu->arch.local_int.float_int->lock);
 +		spin_lock_bh(&vcpu->arch.local_int.lock);
  	}
 +	__unset_cpu_idle(vcpu);
 +	__set_current_state(TASK_RUNNING);
 +	remove_wait_queue(&vcpu->wq, &wait);
 +	spin_unlock_bh(&vcpu->arch.local_int.lock);
 +	spin_unlock(&vcpu->arch.local_int.float_int->lock);
 +	hrtimer_try_to_cancel(&vcpu->arch.ckc_timer);
 +	return 0;
 +}
  
 -	if (nullifying && !(pgm_info.flags & KVM_S390_PGM_FLAGS_NO_REWIND))
 -		kvm_s390_rewind_psw(vcpu, ilen);
 +void kvm_s390_tasklet(unsigned long parm)
 +{
 +	struct kvm_vcpu *vcpu = (struct kvm_vcpu *) parm;
  
 -	/* bit 1+2 of the target are the ilc, so we can directly use ilen */
 -	rc |= put_guest_lc(vcpu, ilen, (u16 *) __LC_PGM_ILC);
 -	rc |= put_guest_lc(vcpu, vcpu->arch.sie_block->gbea,
 -				 (u64 *) __LC_LAST_BREAK);
 -	rc |= put_guest_lc(vcpu, pgm_info.code,
 -			   (u16 *)__LC_PGM_INT_CODE);
 -	rc |= write_guest_lc(vcpu, __LC_PGM_OLD_PSW,
 -			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= read_guest_lc(vcpu, __LC_PGM_NEW_PSW,
 -			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	return rc ? -EFAULT : 0;
 +	spin_lock(&vcpu->arch.local_int.lock);
 +	vcpu->arch.local_int.timer_due = 1;
 +	if (waitqueue_active(&vcpu->wq))
 +		wake_up_interruptible(&vcpu->wq);
 +	spin_unlock(&vcpu->arch.local_int.lock);
  }
  
 -static int __must_check __deliver_service(struct kvm_vcpu *vcpu)
 +/*
 + * low level hrtimer wake routine. Because this runs in hardirq context
 + * we schedule a tasklet to do the real work.
 + */
 +enum hrtimer_restart kvm_s390_idle_wakeup(struct hrtimer *timer)
  {
 -	struct kvm_s390_float_interrupt *fi = &vcpu->kvm->arch.float_int;
 -	struct kvm_s390_ext_info ext;
 -	int rc = 0;
 +	struct kvm_vcpu *vcpu;
  
 -	spin_lock(&fi->lock);
 -	if (!(test_bit(IRQ_PEND_EXT_SERVICE, &fi->pending_irqs))) {
 -		spin_unlock(&fi->lock);
 -		return 0;
 -	}
 -	ext = fi->srv_signal;
 -	memset(&fi->srv_signal, 0, sizeof(ext));
 -	clear_bit(IRQ_PEND_EXT_SERVICE, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 +	vcpu = container_of(timer, struct kvm_vcpu, arch.ckc_timer);
 +	tasklet_schedule(&vcpu->arch.tasklet);
  
 -	VCPU_EVENT(vcpu, 4, "deliver: sclp parameter 0x%x",
 -		   ext.ext_params);
 -	vcpu->stat.deliver_service_signal++;
 -	trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id, KVM_S390_INT_SERVICE,
 -					 ext.ext_params, 0);
 +	return HRTIMER_NORESTART;
 +}
  
 -	rc  = put_guest_lc(vcpu, EXT_IRQ_SERVICE_SIG, (u16 *)__LC_EXT_INT_CODE);
 -	rc |= put_guest_lc(vcpu, 0, (u16 *)__LC_EXT_CPU_ADDR);
 -	rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
 -			     &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW,
 -			    &vcpu->arch.sie_block->gpsw, sizeof(psw_t));
 -	rc |= put_guest_lc(vcpu, ext.ext_params,
 -			   (u32 *)__LC_EXT_PARAMS);
 -
 -	return rc ? -EFAULT : 0;
 -}
 -
 -static int __must_check __deliver_pfault_done(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_float_interrupt *fi = &vcpu->kvm->arch.float_int;
 -	struct kvm_s390_interrupt_info *inti;
 -	int rc = 0;
 -
 -	spin_lock(&fi->lock);
 -	inti = list_first_entry_or_null(&fi->lists[FIRQ_LIST_PFAULT],
 -					struct kvm_s390_interrupt_info,
 -					list);
 -	if (inti) {
 -		list_del(&inti->list);
 -		fi->counters[FIRQ_CNTR_PFAULT] -= 1;
 -	}
 -	if (list_empty(&fi->lists[FIRQ_LIST_PFAULT]))
 -		clear_bit(IRQ_PEND_PFAULT_DONE, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 -
 -	if (inti) {
 -		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id,
 -						 KVM_S390_INT_PFAULT_DONE, 0,
 -						 inti->ext.ext_params2);
 -		VCPU_EVENT(vcpu, 4, "deliver: pfault done token 0x%llx",
 -			   inti->ext.ext_params2);
 -
 -		rc  = put_guest_lc(vcpu, EXT_IRQ_CP_SERVICE,
 -				(u16 *)__LC_EXT_INT_CODE);
 -		rc |= put_guest_lc(vcpu, PFAULT_DONE,
 -				(u16 *)__LC_EXT_CPU_ADDR);
 -		rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
 -				&vcpu->arch.sie_block->gpsw,
 -				sizeof(psw_t));
 -		rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW,
 -				&vcpu->arch.sie_block->gpsw,
 -				sizeof(psw_t));
 -		rc |= put_guest_lc(vcpu, inti->ext.ext_params2,
 -				(u64 *)__LC_EXT_PARAMS2);
 -		kfree(inti);
 -	}
 -	return rc ? -EFAULT : 0;
 -}
 -
 -static int __must_check __deliver_virtio(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_float_interrupt *fi = &vcpu->kvm->arch.float_int;
 -	struct kvm_s390_interrupt_info *inti;
 -	int rc = 0;
 -
 -	spin_lock(&fi->lock);
 -	inti = list_first_entry_or_null(&fi->lists[FIRQ_LIST_VIRTIO],
 -					struct kvm_s390_interrupt_info,
 -					list);
 -	if (inti) {
 -		VCPU_EVENT(vcpu, 4,
 -			   "deliver: virtio parm: 0x%x,parm64: 0x%llx",
 -			   inti->ext.ext_params, inti->ext.ext_params2);
 -		vcpu->stat.deliver_virtio_interrupt++;
 -		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id,
 -				inti->type,
 -				inti->ext.ext_params,
 -				inti->ext.ext_params2);
 -		list_del(&inti->list);
 -		fi->counters[FIRQ_CNTR_VIRTIO] -= 1;
 -	}
 -	if (list_empty(&fi->lists[FIRQ_LIST_VIRTIO]))
 -		clear_bit(IRQ_PEND_VIRTIO, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 -
 -	if (inti) {
 -		rc  = put_guest_lc(vcpu, EXT_IRQ_CP_SERVICE,
 -				(u16 *)__LC_EXT_INT_CODE);
 -		rc |= put_guest_lc(vcpu, VIRTIO_PARAM,
 -				(u16 *)__LC_EXT_CPU_ADDR);
 -		rc |= write_guest_lc(vcpu, __LC_EXT_OLD_PSW,
 -				&vcpu->arch.sie_block->gpsw,
 -				sizeof(psw_t));
 -		rc |= read_guest_lc(vcpu, __LC_EXT_NEW_PSW,
 -				&vcpu->arch.sie_block->gpsw,
 -				sizeof(psw_t));
 -		rc |= put_guest_lc(vcpu, inti->ext.ext_params,
 -				(u32 *)__LC_EXT_PARAMS);
 -		rc |= put_guest_lc(vcpu, inti->ext.ext_params2,
 -				(u64 *)__LC_EXT_PARAMS2);
 -		kfree(inti);
 -	}
 -	return rc ? -EFAULT : 0;
 -}
 -
 -static int __must_check __deliver_io(struct kvm_vcpu *vcpu,
 -				     unsigned long irq_type)
 -{
 -	struct list_head *isc_list;
 -	struct kvm_s390_float_interrupt *fi;
 -	struct kvm_s390_interrupt_info *inti = NULL;
 -	int rc = 0;
 -
 -	fi = &vcpu->kvm->arch.float_int;
 -
 -	spin_lock(&fi->lock);
 -	isc_list = &fi->lists[irq_type - IRQ_PEND_IO_ISC_0];
 -	inti = list_first_entry_or_null(isc_list,
 -					struct kvm_s390_interrupt_info,
 -					list);
 -	if (inti) {
 -		if (inti->type & KVM_S390_INT_IO_AI_MASK)
 -			VCPU_EVENT(vcpu, 4, "%s", "deliver: I/O (AI)");
 -		else
 -			VCPU_EVENT(vcpu, 4, "deliver: I/O %x ss %x schid %04x",
 -			inti->io.subchannel_id >> 8,
 -			inti->io.subchannel_id >> 1 & 0x3,
 -			inti->io.subchannel_nr);
 -
 -		vcpu->stat.deliver_io_int++;
 -		trace_kvm_s390_deliver_interrupt(vcpu->vcpu_id,
 -				inti->type,
 -				((__u32)inti->io.subchannel_id << 16) |
 -				inti->io.subchannel_nr,
 -				((__u64)inti->io.io_int_parm << 32) |
 -				inti->io.io_int_word);
 -		list_del(&inti->list);
 -		fi->counters[FIRQ_CNTR_IO] -= 1;
 -	}
 -	if (list_empty(isc_list))
 -		clear_bit(irq_type, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 -
 -	if (inti) {
 -		rc  = put_guest_lc(vcpu, inti->io.subchannel_id,
 -				(u16 *)__LC_SUBCHANNEL_ID);
 -		rc |= put_guest_lc(vcpu, inti->io.subchannel_nr,
 -				(u16 *)__LC_SUBCHANNEL_NR);
 -		rc |= put_guest_lc(vcpu, inti->io.io_int_parm,
 -				(u32 *)__LC_IO_INT_PARM);
 -		rc |= put_guest_lc(vcpu, inti->io.io_int_word,
 -				(u32 *)__LC_IO_INT_WORD);
 -		rc |= write_guest_lc(vcpu, __LC_IO_OLD_PSW,
 -				&vcpu->arch.sie_block->gpsw,
 -				sizeof(psw_t));
 -		rc |= read_guest_lc(vcpu, __LC_IO_NEW_PSW,
 -				&vcpu->arch.sie_block->gpsw,
 -				sizeof(psw_t));
 -		kfree(inti);
 -	}
 -
 -	return rc ? -EFAULT : 0;
 -}
 -
 -typedef int (*deliver_irq_t)(struct kvm_vcpu *vcpu);
 -
 -static const deliver_irq_t deliver_irq_funcs[] = {
 -	[IRQ_PEND_MCHK_EX]        = __deliver_machine_check,
 -	[IRQ_PEND_MCHK_REP]       = __deliver_machine_check,
 -	[IRQ_PEND_PROG]           = __deliver_prog,
 -	[IRQ_PEND_EXT_EMERGENCY]  = __deliver_emergency_signal,
 -	[IRQ_PEND_EXT_EXTERNAL]   = __deliver_external_call,
 -	[IRQ_PEND_EXT_CLOCK_COMP] = __deliver_ckc,
 -	[IRQ_PEND_EXT_CPU_TIMER]  = __deliver_cpu_timer,
 -	[IRQ_PEND_RESTART]        = __deliver_restart,
 -	[IRQ_PEND_SET_PREFIX]     = __deliver_set_prefix,
 -	[IRQ_PEND_PFAULT_INIT]    = __deliver_pfault_init,
 -	[IRQ_PEND_EXT_SERVICE]    = __deliver_service,
 -	[IRQ_PEND_PFAULT_DONE]    = __deliver_pfault_done,
 -	[IRQ_PEND_VIRTIO]         = __deliver_virtio,
 -};
 -
 -/* Check whether an external call is pending (deliverable or not) */
 -int kvm_s390_ext_call_pending(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	if (!sclp.has_sigpif)
 -		return test_bit(IRQ_PEND_EXT_EXTERNAL, &li->pending_irqs);
 -
 -	return sca_ext_call_pending(vcpu, NULL);
 -}
 -
 -int kvm_s390_vcpu_has_irq(struct kvm_vcpu *vcpu, int exclude_stop)
 -{
 -	if (deliverable_irqs(vcpu))
 -		return 1;
 -
 -	if (kvm_cpu_has_pending_timer(vcpu))
 -		return 1;
 -
 -	/* external call pending and deliverable */
 -	if (kvm_s390_ext_call_pending(vcpu) &&
 -	    !psw_extint_disabled(vcpu) &&
 -	    (vcpu->arch.sie_block->gcr[0] & 0x2000ul))
 -		return 1;
 -
 -	if (!exclude_stop && kvm_s390_is_stop_irq_pending(vcpu))
 -		return 1;
 -	return 0;
 -}
 -
 -int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 -{
 -	return ckc_irq_pending(vcpu) || cpu_timer_irq_pending(vcpu);
 -}
 -
 -static u64 __calculate_sltime(struct kvm_vcpu *vcpu)
 -{
 -	u64 now, cputm, sltime = 0;
 -
 -	if (ckc_interrupts_enabled(vcpu)) {
 -		now = kvm_s390_get_tod_clock_fast(vcpu->kvm);
 -		sltime = tod_to_ns(vcpu->arch.sie_block->ckc - now);
 -		/* already expired or overflow? */
 -		if (!sltime || vcpu->arch.sie_block->ckc <= now)
 -			return 0;
 -		if (cpu_timer_interrupts_enabled(vcpu)) {
 -			cputm = kvm_s390_get_cpu_timer(vcpu);
 -			/* already expired? */
 -			if (cputm >> 63)
 -				return 0;
 -			return min(sltime, tod_to_ns(cputm));
 -		}
 -	} else if (cpu_timer_interrupts_enabled(vcpu)) {
 -		sltime = kvm_s390_get_cpu_timer(vcpu);
 -		/* already expired? */
 -		if (sltime >> 63)
 -			return 0;
 -	}
 -	return sltime;
 -}
 -
 -int kvm_s390_handle_wait(struct kvm_vcpu *vcpu)
 -{
 -	u64 sltime;
 -
 -	vcpu->stat.exit_wait_state++;
 -
 -	/* fast path */
 -	if (kvm_arch_vcpu_runnable(vcpu))
 -		return 0;
 -
 -	if (psw_interrupts_disabled(vcpu)) {
 -		VCPU_EVENT(vcpu, 3, "%s", "disabled wait");
 -		return -EOPNOTSUPP; /* disabled wait */
 -	}
 -
 -	if (!ckc_interrupts_enabled(vcpu) &&
 -	    !cpu_timer_interrupts_enabled(vcpu)) {
 -		VCPU_EVENT(vcpu, 3, "%s", "enabled wait w/o timer");
 -		__set_cpu_idle(vcpu);
 -		goto no_timer;
 -	}
 -
 -	sltime = __calculate_sltime(vcpu);
 -	if (!sltime)
 -		return 0;
 -
 -	__set_cpu_idle(vcpu);
 -	hrtimer_start(&vcpu->arch.ckc_timer, sltime, HRTIMER_MODE_REL);
 -	VCPU_EVENT(vcpu, 4, "enabled wait: %llu ns", sltime);
 -no_timer:
 -	srcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);
 -	kvm_vcpu_block(vcpu);
 -	__unset_cpu_idle(vcpu);
 -	vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
 -
 -	hrtimer_cancel(&vcpu->arch.ckc_timer);
 -	return 0;
 -}
 -
 -void kvm_s390_vcpu_wakeup(struct kvm_vcpu *vcpu)
 -{
 -	/*
 -	 * We cannot move this into the if, as the CPU might be already
 -	 * in kvm_vcpu_block without having the waitqueue set (polling)
 -	 */
 -	vcpu->valid_wakeup = true;
 -	if (swait_active(&vcpu->wq)) {
 -		/*
 -		 * The vcpu gave up the cpu voluntarily, mark it as a good
 -		 * yield-candidate.
 -		 */
 -		vcpu->preempted = true;
 -		swake_up(&vcpu->wq);
 -		vcpu->stat.halt_wakeup++;
 -	}
 -	/*
 -	 * The VCPU might not be sleeping but is executing the VSIE. Let's
 -	 * kick it, so it leaves the SIE to process the request.
 -	 */
 -	kvm_s390_vsie_kick(vcpu);
 -}
 -
 -enum hrtimer_restart kvm_s390_idle_wakeup(struct hrtimer *timer)
 -{
 -	struct kvm_vcpu *vcpu;
 -	u64 sltime;
 -
 -	vcpu = container_of(timer, struct kvm_vcpu, arch.ckc_timer);
 -	sltime = __calculate_sltime(vcpu);
 -
 -	/*
 -	 * If the monotonic clock runs faster than the tod clock we might be
 -	 * woken up too early and have to go back to sleep to avoid deadlocks.
 -	 */
 -	if (sltime && hrtimer_forward_now(timer, ns_to_ktime(sltime)))
 -		return HRTIMER_RESTART;
 -	kvm_s390_vcpu_wakeup(vcpu);
 -	return HRTIMER_NORESTART;
 -}
 -
 -void kvm_s390_clear_local_irqs(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	spin_lock(&li->lock);
 -	li->pending_irqs = 0;
 -	bitmap_zero(li->sigp_emerg_pending, KVM_MAX_VCPUS);
 -	memset(&li->irq, 0, sizeof(li->irq));
 -	spin_unlock(&li->lock);
 -
 -	sca_clear_ext_call(vcpu);
 -}
 -
 -int __must_check kvm_s390_deliver_pending_interrupts(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	deliver_irq_t func;
 -	int rc = 0;
 -	unsigned long irq_type;
 -	unsigned long irqs;
 -
 -	__reset_intercept_indicators(vcpu);
 -
 -	/* pending ckc conditions might have been invalidated */
 -	clear_bit(IRQ_PEND_EXT_CLOCK_COMP, &li->pending_irqs);
 -	if (ckc_irq_pending(vcpu))
 -		set_bit(IRQ_PEND_EXT_CLOCK_COMP, &li->pending_irqs);
 -
 -	/* pending cpu timer conditions might have been invalidated */
 -	clear_bit(IRQ_PEND_EXT_CPU_TIMER, &li->pending_irqs);
 -	if (cpu_timer_irq_pending(vcpu))
 -		set_bit(IRQ_PEND_EXT_CPU_TIMER, &li->pending_irqs);
 -
 -	while ((irqs = deliverable_irqs(vcpu)) && !rc) {
 -		/* bits are in the order of interrupt priority */
 -		irq_type = find_first_bit(&irqs, IRQ_PEND_COUNT);
 -		if (is_ioirq(irq_type)) {
 -			rc = __deliver_io(vcpu, irq_type);
 -		} else {
 -			func = deliver_irq_funcs[irq_type];
 -			if (!func) {
 -				WARN_ON_ONCE(func == NULL);
 -				clear_bit(irq_type, &li->pending_irqs);
 -				continue;
 -			}
 -			rc = func(vcpu);
 -		}
 -	}
 -
 -	set_intercept_indicators(vcpu);
 -
 -	return rc;
 -}
 -
 -static int __inject_prog(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	VCPU_EVENT(vcpu, 3, "inject: program irq code 0x%x", irq->u.pgm.code);
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_PROGRAM_INT,
 -				   irq->u.pgm.code, 0);
 -
 -	if (!(irq->u.pgm.flags & KVM_S390_PGM_FLAGS_ILC_VALID)) {
 -		/* auto detection if no valid ILC was given */
 -		irq->u.pgm.flags &= ~KVM_S390_PGM_FLAGS_ILC_MASK;
 -		irq->u.pgm.flags |= kvm_s390_get_ilen(vcpu);
 -		irq->u.pgm.flags |= KVM_S390_PGM_FLAGS_ILC_VALID;
 -	}
 -
 -	if (irq->u.pgm.code == PGM_PER) {
 -		li->irq.pgm.code |= PGM_PER;
 -		li->irq.pgm.flags = irq->u.pgm.flags;
 -		/* only modify PER related information */
 -		li->irq.pgm.per_address = irq->u.pgm.per_address;
 -		li->irq.pgm.per_code = irq->u.pgm.per_code;
 -		li->irq.pgm.per_atmid = irq->u.pgm.per_atmid;
 -		li->irq.pgm.per_access_id = irq->u.pgm.per_access_id;
 -	} else if (!(irq->u.pgm.code & PGM_PER)) {
 -		li->irq.pgm.code = (li->irq.pgm.code & PGM_PER) |
 -				   irq->u.pgm.code;
 -		li->irq.pgm.flags = irq->u.pgm.flags;
 -		/* only modify non-PER information */
 -		li->irq.pgm.trans_exc_code = irq->u.pgm.trans_exc_code;
 -		li->irq.pgm.mon_code = irq->u.pgm.mon_code;
 -		li->irq.pgm.data_exc_code = irq->u.pgm.data_exc_code;
 -		li->irq.pgm.mon_class_nr = irq->u.pgm.mon_class_nr;
 -		li->irq.pgm.exc_access_id = irq->u.pgm.exc_access_id;
 -		li->irq.pgm.op_access_id = irq->u.pgm.op_access_id;
 -	} else {
 -		li->irq.pgm = irq->u.pgm;
 -	}
 -	set_bit(IRQ_PEND_PROG, &li->pending_irqs);
 -	return 0;
 -}
 -
 -static int __inject_pfault_init(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	VCPU_EVENT(vcpu, 4, "inject: pfault init parameter block at 0x%llx",
 -		   irq->u.ext.ext_params2);
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_INT_PFAULT_INIT,
 -				   irq->u.ext.ext_params,
 -				   irq->u.ext.ext_params2);
 -
 -	li->irq.ext = irq->u.ext;
 -	set_bit(IRQ_PEND_PFAULT_INIT, &li->pending_irqs);
 -	atomic_or(CPUSTAT_EXT_INT, li->cpuflags);
 -	return 0;
 -}
 -
 -static int __inject_extcall(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_extcall_info *extcall = &li->irq.extcall;
 -	uint16_t src_id = irq->u.extcall.code;
 -
 -	VCPU_EVENT(vcpu, 4, "inject: external call source-cpu:%u",
 -		   src_id);
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_INT_EXTERNAL_CALL,
 -				   src_id, 0);
 -
 -	/* sending vcpu invalid */
 -	if (kvm_get_vcpu_by_id(vcpu->kvm, src_id) == NULL)
 -		return -EINVAL;
 -
 -	if (sclp.has_sigpif)
 -		return sca_inject_ext_call(vcpu, src_id);
 -
 -	if (test_and_set_bit(IRQ_PEND_EXT_EXTERNAL, &li->pending_irqs))
 -		return -EBUSY;
 -	*extcall = irq->u.extcall;
 -	atomic_or(CPUSTAT_EXT_INT, li->cpuflags);
 -	return 0;
 -}
 -
 -static int __inject_set_prefix(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_prefix_info *prefix = &li->irq.prefix;
 -
 -	VCPU_EVENT(vcpu, 3, "inject: set prefix to %x",
 -		   irq->u.prefix.address);
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_SIGP_SET_PREFIX,
 -				   irq->u.prefix.address, 0);
 -
 -	if (!is_vcpu_stopped(vcpu))
 -		return -EBUSY;
 -
 -	*prefix = irq->u.prefix;
 -	set_bit(IRQ_PEND_SET_PREFIX, &li->pending_irqs);
 -	return 0;
 -}
 -
 -#define KVM_S390_STOP_SUPP_FLAGS (KVM_S390_STOP_FLAG_STORE_STATUS)
 -static int __inject_sigp_stop(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_stop_info *stop = &li->irq.stop;
 -	int rc = 0;
 -
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_SIGP_STOP, 0, 0);
 -
 -	if (irq->u.stop.flags & ~KVM_S390_STOP_SUPP_FLAGS)
 -		return -EINVAL;
 -
 -	if (is_vcpu_stopped(vcpu)) {
 -		if (irq->u.stop.flags & KVM_S390_STOP_FLAG_STORE_STATUS)
 -			rc = kvm_s390_store_status_unloaded(vcpu,
 -						KVM_S390_STORE_STATUS_NOADDR);
 -		return rc;
 -	}
 -
 -	if (test_and_set_bit(IRQ_PEND_SIGP_STOP, &li->pending_irqs))
 -		return -EBUSY;
 -	stop->flags = irq->u.stop.flags;
 -	__set_cpuflag(vcpu, CPUSTAT_STOP_INT);
 -	return 0;
 -}
 -
 -static int __inject_sigp_restart(struct kvm_vcpu *vcpu,
 -				 struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	VCPU_EVENT(vcpu, 3, "%s", "inject: restart int");
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_RESTART, 0, 0);
 -
 -	set_bit(IRQ_PEND_RESTART, &li->pending_irqs);
 -	return 0;
 -}
 -
 -static int __inject_sigp_emergency(struct kvm_vcpu *vcpu,
 -				   struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	VCPU_EVENT(vcpu, 4, "inject: emergency from cpu %u",
 -		   irq->u.emerg.code);
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_INT_EMERGENCY,
 -				   irq->u.emerg.code, 0);
 -
 -	/* sending vcpu invalid */
 -	if (kvm_get_vcpu_by_id(vcpu->kvm, irq->u.emerg.code) == NULL)
 -		return -EINVAL;
 -
 -	set_bit(irq->u.emerg.code, li->sigp_emerg_pending);
 -	set_bit(IRQ_PEND_EXT_EMERGENCY, &li->pending_irqs);
 -	atomic_or(CPUSTAT_EXT_INT, li->cpuflags);
 -	return 0;
 -}
 -
 -static int __inject_mchk(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	struct kvm_s390_mchk_info *mchk = &li->irq.mchk;
 -
 -	VCPU_EVENT(vcpu, 3, "inject: machine check mcic 0x%llx",
 -		   irq->u.mchk.mcic);
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_MCHK, 0,
 -				   irq->u.mchk.mcic);
 -
 -	/*
 -	 * Because repressible machine checks can be indicated along with
 -	 * exigent machine checks (PoP, Chapter 11, Interruption action)
 -	 * we need to combine cr14, mcic and external damage code.
 -	 * Failing storage address and the logout area should not be or'ed
 -	 * together, we just indicate the last occurrence of the corresponding
 -	 * machine check
 -	 */
 -	mchk->cr14 |= irq->u.mchk.cr14;
 -	mchk->mcic |= irq->u.mchk.mcic;
 -	mchk->ext_damage_code |= irq->u.mchk.ext_damage_code;
 -	mchk->failing_storage_address = irq->u.mchk.failing_storage_address;
 -	memcpy(&mchk->fixed_logout, &irq->u.mchk.fixed_logout,
 -	       sizeof(mchk->fixed_logout));
 -	if (mchk->mcic & MCHK_EX_MASK)
 -		set_bit(IRQ_PEND_MCHK_EX, &li->pending_irqs);
 -	else if (mchk->mcic & MCHK_REP_MASK)
 -		set_bit(IRQ_PEND_MCHK_REP,  &li->pending_irqs);
 -	return 0;
 -}
 -
 -static int __inject_ckc(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	VCPU_EVENT(vcpu, 3, "%s", "inject: clock comparator external");
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_INT_CLOCK_COMP,
 -				   0, 0);
 -
 -	set_bit(IRQ_PEND_EXT_CLOCK_COMP, &li->pending_irqs);
 -	atomic_or(CPUSTAT_EXT_INT, li->cpuflags);
 -	return 0;
 -}
 -
 -static int __inject_cpu_timer(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	VCPU_EVENT(vcpu, 3, "%s", "inject: cpu timer external");
 -	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, KVM_S390_INT_CPU_TIMER,
 -				   0, 0);
 -
 -	set_bit(IRQ_PEND_EXT_CPU_TIMER, &li->pending_irqs);
 -	atomic_or(CPUSTAT_EXT_INT, li->cpuflags);
 -	return 0;
 -}
 -
 -static struct kvm_s390_interrupt_info *get_io_int(struct kvm *kvm,
 -						  int isc, u32 schid)
 -{
 -	struct kvm_s390_float_interrupt *fi = &kvm->arch.float_int;
 -	struct list_head *isc_list = &fi->lists[FIRQ_LIST_IO_ISC_0 + isc];
 -	struct kvm_s390_interrupt_info *iter;
 -	u16 id = (schid & 0xffff0000U) >> 16;
 -	u16 nr = schid & 0x0000ffffU;
 -
 -	spin_lock(&fi->lock);
 -	list_for_each_entry(iter, isc_list, list) {
 -		if (schid && (id != iter->io.subchannel_id ||
 -			      nr != iter->io.subchannel_nr))
 -			continue;
 -		/* found an appropriate entry */
 -		list_del_init(&iter->list);
 -		fi->counters[FIRQ_CNTR_IO] -= 1;
 -		if (list_empty(isc_list))
 -			clear_bit(IRQ_PEND_IO_ISC_0 + isc, &fi->pending_irqs);
 -		spin_unlock(&fi->lock);
 -		return iter;
 -	}
 -	spin_unlock(&fi->lock);
 -	return NULL;
 -}
 -
 -/*
 - * Dequeue and return an I/O interrupt matching any of the interruption
 - * subclasses as designated by the isc mask in cr6 and the schid (if != 0).
 - */
 -struct kvm_s390_interrupt_info *kvm_s390_get_io_int(struct kvm *kvm,
 -						    u64 isc_mask, u32 schid)
 -{
 -	struct kvm_s390_interrupt_info *inti = NULL;
 -	int isc;
 -
 -	for (isc = 0; isc <= MAX_ISC && !inti; isc++) {
 -		if (isc_mask & isc_to_isc_bits(isc))
 -			inti = get_io_int(kvm, isc, schid);
 -	}
 -	return inti;
 -}
 -
 -#define SCCB_MASK 0xFFFFFFF8
 -#define SCCB_EVENT_PENDING 0x3
 -
 -static int __inject_service(struct kvm *kvm,
 -			     struct kvm_s390_interrupt_info *inti)
 -{
 -	struct kvm_s390_float_interrupt *fi = &kvm->arch.float_int;
 -
 -	spin_lock(&fi->lock);
 -	fi->srv_signal.ext_params |= inti->ext.ext_params & SCCB_EVENT_PENDING;
 -	/*
 -	 * Early versions of the QEMU s390 bios will inject several
 -	 * service interrupts after another without handling a
 -	 * condition code indicating busy.
 -	 * We will silently ignore those superfluous sccb values.
 -	 * A future version of QEMU will take care of serialization
 -	 * of servc requests
 -	 */
 -	if (fi->srv_signal.ext_params & SCCB_MASK)
 -		goto out;
 -	fi->srv_signal.ext_params |= inti->ext.ext_params & SCCB_MASK;
 -	set_bit(IRQ_PEND_EXT_SERVICE, &fi->pending_irqs);
 -out:
 -	spin_unlock(&fi->lock);
 -	kfree(inti);
 -	return 0;
 -}
 -
 -static int __inject_virtio(struct kvm *kvm,
 -			    struct kvm_s390_interrupt_info *inti)
 -{
 -	struct kvm_s390_float_interrupt *fi = &kvm->arch.float_int;
 -
 -	spin_lock(&fi->lock);
 -	if (fi->counters[FIRQ_CNTR_VIRTIO] >= KVM_S390_MAX_VIRTIO_IRQS) {
 -		spin_unlock(&fi->lock);
 -		return -EBUSY;
 -	}
 -	fi->counters[FIRQ_CNTR_VIRTIO] += 1;
 -	list_add_tail(&inti->list, &fi->lists[FIRQ_LIST_VIRTIO]);
 -	set_bit(IRQ_PEND_VIRTIO, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 -	return 0;
 -}
 -
 -static int __inject_pfault_done(struct kvm *kvm,
 -				 struct kvm_s390_interrupt_info *inti)
 -{
 -	struct kvm_s390_float_interrupt *fi = &kvm->arch.float_int;
 -
 -	spin_lock(&fi->lock);
 -	if (fi->counters[FIRQ_CNTR_PFAULT] >=
 -		(ASYNC_PF_PER_VCPU * KVM_MAX_VCPUS)) {
 -		spin_unlock(&fi->lock);
 -		return -EBUSY;
 -	}
 -	fi->counters[FIRQ_CNTR_PFAULT] += 1;
 -	list_add_tail(&inti->list, &fi->lists[FIRQ_LIST_PFAULT]);
 -	set_bit(IRQ_PEND_PFAULT_DONE, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 -	return 0;
 -}
 -
 -#define CR_PENDING_SUBCLASS 28
 -static int __inject_float_mchk(struct kvm *kvm,
 -				struct kvm_s390_interrupt_info *inti)
 -{
 -	struct kvm_s390_float_interrupt *fi = &kvm->arch.float_int;
 -
 -	spin_lock(&fi->lock);
 -	fi->mchk.cr14 |= inti->mchk.cr14 & (1UL << CR_PENDING_SUBCLASS);
 -	fi->mchk.mcic |= inti->mchk.mcic;
 -	set_bit(IRQ_PEND_MCHK_REP, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 -	kfree(inti);
 -	return 0;
 -}
 -
 -static int __inject_io(struct kvm *kvm, struct kvm_s390_interrupt_info *inti)
 -{
 -	struct kvm_s390_float_interrupt *fi;
 -	struct list_head *list;
 -	int isc;
 -
 -	fi = &kvm->arch.float_int;
 -	spin_lock(&fi->lock);
 -	if (fi->counters[FIRQ_CNTR_IO] >= KVM_S390_MAX_FLOAT_IRQS) {
 -		spin_unlock(&fi->lock);
 -		return -EBUSY;
 -	}
 -	fi->counters[FIRQ_CNTR_IO] += 1;
 -
 -	if (inti->type & KVM_S390_INT_IO_AI_MASK)
 -		VM_EVENT(kvm, 4, "%s", "inject: I/O (AI)");
 -	else
 -		VM_EVENT(kvm, 4, "inject: I/O %x ss %x schid %04x",
 -			inti->io.subchannel_id >> 8,
 -			inti->io.subchannel_id >> 1 & 0x3,
 -			inti->io.subchannel_nr);
 -	isc = int_word_to_isc(inti->io.io_int_word);
 -	list = &fi->lists[FIRQ_LIST_IO_ISC_0 + isc];
 -	list_add_tail(&inti->list, list);
 -	set_bit(IRQ_PEND_IO_ISC_0 + isc, &fi->pending_irqs);
 -	spin_unlock(&fi->lock);
 -	return 0;
 -}
 -
 -/*
 - * Find a destination VCPU for a floating irq and kick it.
 - */
 -static void __floating_irq_kick(struct kvm *kvm, u64 type)
 -{
 -	struct kvm_s390_float_interrupt *fi = &kvm->arch.float_int;
 -	struct kvm_s390_local_interrupt *li;
 -	struct kvm_vcpu *dst_vcpu;
 -	int sigcpu, online_vcpus, nr_tries = 0;
 -
 -	online_vcpus = atomic_read(&kvm->online_vcpus);
 -	if (!online_vcpus)
 -		return;
 -
 -	/* find idle VCPUs first, then round robin */
 -	sigcpu = find_first_bit(fi->idle_mask, online_vcpus);
 -	if (sigcpu == online_vcpus) {
 -		do {
 -			sigcpu = fi->next_rr_cpu;
 -			fi->next_rr_cpu = (fi->next_rr_cpu + 1) % online_vcpus;
 -			/* avoid endless loops if all vcpus are stopped */
 -			if (nr_tries++ >= online_vcpus)
 -				return;
 -		} while (is_vcpu_stopped(kvm_get_vcpu(kvm, sigcpu)));
 -	}
 -	dst_vcpu = kvm_get_vcpu(kvm, sigcpu);
 -
 -	/* make the VCPU drop out of the SIE, or wake it up if sleeping */
 -	li = &dst_vcpu->arch.local_int;
 -	spin_lock(&li->lock);
 -	switch (type) {
 -	case KVM_S390_MCHK:
 -		atomic_or(CPUSTAT_STOP_INT, li->cpuflags);
 -		break;
 -	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 -		atomic_or(CPUSTAT_IO_INT, li->cpuflags);
 -		break;
 -	default:
 -		atomic_or(CPUSTAT_EXT_INT, li->cpuflags);
 -		break;
 -	}
 -	spin_unlock(&li->lock);
 -	kvm_s390_vcpu_wakeup(dst_vcpu);
 -}
 -
 -static int __inject_vm(struct kvm *kvm, struct kvm_s390_interrupt_info *inti)
 -{
 -	u64 type = READ_ONCE(inti->type);
 -	int rc;
 -
 -	switch (type) {
 -	case KVM_S390_MCHK:
 -		rc = __inject_float_mchk(kvm, inti);
 -		break;
 -	case KVM_S390_INT_VIRTIO:
 -		rc = __inject_virtio(kvm, inti);
 -		break;
 -	case KVM_S390_INT_SERVICE:
 -		rc = __inject_service(kvm, inti);
 -		break;
 -	case KVM_S390_INT_PFAULT_DONE:
 -		rc = __inject_pfault_done(kvm, inti);
 -		break;
 -	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 -		rc = __inject_io(kvm, inti);
 -		break;
 -	default:
 -		rc = -EINVAL;
 -	}
 -	if (rc)
 -		return rc;
 -
 -	__floating_irq_kick(kvm, type);
 -	return 0;
 -}
 -
 -int kvm_s390_inject_vm(struct kvm *kvm,
 -		       struct kvm_s390_interrupt *s390int)
 -{
 -	struct kvm_s390_interrupt_info *inti;
 -	int rc;
 -
 -	inti = kzalloc(sizeof(*inti), GFP_KERNEL);
 -	if (!inti)
 -		return -ENOMEM;
 -
 -	inti->type = s390int->type;
 -	switch (inti->type) {
 -	case KVM_S390_INT_VIRTIO:
 -		VM_EVENT(kvm, 5, "inject: virtio parm:%x,parm64:%llx",
 -			 s390int->parm, s390int->parm64);
 -		inti->ext.ext_params = s390int->parm;
 -		inti->ext.ext_params2 = s390int->parm64;
 -		break;
 -	case KVM_S390_INT_SERVICE:
 -		VM_EVENT(kvm, 4, "inject: sclp parm:%x", s390int->parm);
 -		inti->ext.ext_params = s390int->parm;
 -		break;
 -	case KVM_S390_INT_PFAULT_DONE:
 -		inti->ext.ext_params2 = s390int->parm64;
 -		break;
 -	case KVM_S390_MCHK:
 -		VM_EVENT(kvm, 3, "inject: machine check mcic 0x%llx",
 -			 s390int->parm64);
 -		inti->mchk.cr14 = s390int->parm; /* upper bits are not used */
 -		inti->mchk.mcic = s390int->parm64;
 -		break;
 -	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 -		inti->io.subchannel_id = s390int->parm >> 16;
 -		inti->io.subchannel_nr = s390int->parm & 0x0000ffffu;
 -		inti->io.io_int_parm = s390int->parm64 >> 32;
 -		inti->io.io_int_word = s390int->parm64 & 0x00000000ffffffffull;
 -		break;
 -	default:
 -		kfree(inti);
 -		return -EINVAL;
 -	}
 -	trace_kvm_s390_inject_vm(s390int->type, s390int->parm, s390int->parm64,
 -				 2);
 -
 -	rc = __inject_vm(kvm, inti);
 -	if (rc)
 -		kfree(inti);
 -	return rc;
 -}
 -
 -int kvm_s390_reinject_io_int(struct kvm *kvm,
 -			      struct kvm_s390_interrupt_info *inti)
 -{
 -	return __inject_vm(kvm, inti);
 -}
 -
 -int s390int_to_s390irq(struct kvm_s390_interrupt *s390int,
 -		       struct kvm_s390_irq *irq)
 -{
 -	irq->type = s390int->type;
 -	switch (irq->type) {
 -	case KVM_S390_PROGRAM_INT:
 -		if (s390int->parm & 0xffff0000)
 -			return -EINVAL;
 -		irq->u.pgm.code = s390int->parm;
 -		break;
 -	case KVM_S390_SIGP_SET_PREFIX:
 -		irq->u.prefix.address = s390int->parm;
 -		break;
 -	case KVM_S390_SIGP_STOP:
 -		irq->u.stop.flags = s390int->parm;
 -		break;
 -	case KVM_S390_INT_EXTERNAL_CALL:
 -		if (s390int->parm & 0xffff0000)
 -			return -EINVAL;
 -		irq->u.extcall.code = s390int->parm;
 -		break;
 -	case KVM_S390_INT_EMERGENCY:
 -		if (s390int->parm & 0xffff0000)
 -			return -EINVAL;
 -		irq->u.emerg.code = s390int->parm;
 -		break;
 -	case KVM_S390_MCHK:
 -		irq->u.mchk.mcic = s390int->parm64;
 -		break;
 -	}
 -	return 0;
 -}
 -
 -int kvm_s390_is_stop_irq_pending(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	return test_bit(IRQ_PEND_SIGP_STOP, &li->pending_irqs);
 -}
 -
 -void kvm_s390_clear_stop_irq(struct kvm_vcpu *vcpu)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -
 -	spin_lock(&li->lock);
 -	li->irq.stop.flags = 0;
 -	clear_bit(IRQ_PEND_SIGP_STOP, &li->pending_irqs);
 -	spin_unlock(&li->lock);
 -}
 -
 -static int do_inject_vcpu(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	int rc;
 -
 -	switch (irq->type) {
 -	case KVM_S390_PROGRAM_INT:
 -		rc = __inject_prog(vcpu, irq);
 -		break;
 -	case KVM_S390_SIGP_SET_PREFIX:
 -		rc = __inject_set_prefix(vcpu, irq);
 -		break;
 -	case KVM_S390_SIGP_STOP:
 -		rc = __inject_sigp_stop(vcpu, irq);
 -		break;
 -	case KVM_S390_RESTART:
 -		rc = __inject_sigp_restart(vcpu, irq);
 -		break;
 -	case KVM_S390_INT_CLOCK_COMP:
 -		rc = __inject_ckc(vcpu);
 -		break;
 -	case KVM_S390_INT_CPU_TIMER:
 -		rc = __inject_cpu_timer(vcpu);
 -		break;
 -	case KVM_S390_INT_EXTERNAL_CALL:
 -		rc = __inject_extcall(vcpu, irq);
 -		break;
 -	case KVM_S390_INT_EMERGENCY:
 -		rc = __inject_sigp_emergency(vcpu, irq);
 -		break;
 -	case KVM_S390_MCHK:
 -		rc = __inject_mchk(vcpu, irq);
 -		break;
 -	case KVM_S390_INT_PFAULT_INIT:
 -		rc = __inject_pfault_init(vcpu, irq);
 -		break;
 -	case KVM_S390_INT_VIRTIO:
 -	case KVM_S390_INT_SERVICE:
 -	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 -	default:
 -		rc = -EINVAL;
 -	}
 -
 -	return rc;
 -}
 -
 -int kvm_s390_inject_vcpu(struct kvm_vcpu *vcpu, struct kvm_s390_irq *irq)
 -{
 -	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 -	int rc;
 -
 -	spin_lock(&li->lock);
 -	rc = do_inject_vcpu(vcpu, irq);
 -	spin_unlock(&li->lock);
 -	if (!rc)
 -		kvm_s390_vcpu_wakeup(vcpu);
 -	return rc;
 -}
 -
 -static inline void clear_irq_list(struct list_head *_list)
 -{
 -	struct kvm_s390_interrupt_info *inti, *n;
 -
 -	list_for_each_entry_safe(inti, n, _list, list) {
 -		list_del(&inti->list);
 -		kfree(inti);
 -	}
 -}
 -
 -static void inti_to_irq(struct kvm_s390_interrupt_info *inti,
 -		       struct kvm_s390_irq *irq)
 -{
 -	irq->type = inti->type;
 -	switch (inti->type) {
 -	case KVM_S390_INT_PFAULT_INIT:
 -	case KVM_S390_INT_PFAULT_DONE:
 -	case KVM_S390_INT_VIRTIO:
 -		irq->u.ext = inti->ext;
 -		break;
 -	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 -		irq->u.io = inti->io;
 -		break;
 -	}
 -}
 -
 -void kvm_s390_clear_float_irqs(struct kvm *kvm)
 -{
 -	struct kvm_s390_float_interrupt *fi = &kvm->arch.float_int;
 -	int i;
 -
 -	spin_lock(&fi->lock);
 -	fi->pending_irqs = 0;
 -	memset(&fi->srv_signal, 0, sizeof(fi->srv_signal));
 -	memset(&fi->mchk, 0, sizeof(fi->mchk));
 -	for (i = 0; i < FIRQ_LIST_COUNT; i++)
 -		clear_irq_list(&fi->lists[i]);
 -	for (i = 0; i < FIRQ_MAX_COUNT; i++)
 -		fi->counters[i] = 0;
 -	spin_unlock(&fi->lock);
 -};
 -
 -static int get_all_floating_irqs(struct kvm *kvm, u8 __user *usrbuf, u64 len)
 -{
 -	struct kvm_s390_interrupt_info *inti;
 -	struct kvm_s390_float_interrupt *fi;
 -	struct kvm_s390_irq *buf;
 -	struct kvm_s390_irq *irq;
 -	int max_irqs;
 -	int ret = 0;
 -	int n = 0;
 -	int i;
 -
 -	if (len > KVM_S390_FLIC_MAX_BUFFER || len == 0)
 -		return -EINVAL;
 -
 -	/*
 -	 * We are already using -ENOMEM to signal
 -	 * userspace it may retry with a bigger buffer,
 -	 * so we need to use something else for this case
 -	 */
 -	buf = vzalloc(len);
 -	if (!buf)
 -		return -ENOBUFS;
 -
 -	max_irqs = len / sizeof(struct kvm_s390_irq);
 -
 -	fi = &kvm->arch.float_int;
 -	spin_lock(&fi->lock);
 -	for (i = 0; i < FIRQ_LIST_COUNT; i++) {
 -		list_for_each_entry(inti, &fi->lists[i], list) {
 -			if (n == max_irqs) {
 -				/* signal userspace to try again */
 -				ret = -ENOMEM;
 -				goto out;
 -			}
 -			inti_to_irq(inti, &buf[n]);
 -			n++;
 -		}
 -	}
 -	if (test_bit(IRQ_PEND_EXT_SERVICE, &fi->pending_irqs)) {
 -		if (n == max_irqs) {
 -			/* signal userspace to try again */
 -			ret = -ENOMEM;
 -			goto out;
 -		}
 -		irq = (struct kvm_s390_irq *) &buf[n];
 -		irq->type = KVM_S390_INT_SERVICE;
 -		irq->u.ext = fi->srv_signal;
 -		n++;
 -	}
 -	if (test_bit(IRQ_PEND_MCHK_REP, &fi->pending_irqs)) {
 -		if (n == max_irqs) {
 -				/* signal userspace to try again */
 -				ret = -ENOMEM;
 -				goto out;
 -		}
 -		irq = (struct kvm_s390_irq *) &buf[n];
 -		irq->type = KVM_S390_MCHK;
 -		irq->u.mchk = fi->mchk;
 -		n++;
 -}
 -
 -out:
 -	spin_unlock(&fi->lock);
 -	if (!ret && n > 0) {
 -		if (copy_to_user(usrbuf, buf, sizeof(struct kvm_s390_irq) * n))
 -			ret = -EFAULT;
 -	}
 -	vfree(buf);
 -
 -	return ret < 0 ? ret : n;
 -}
 -
 -static int flic_get_attr(struct kvm_device *dev, struct kvm_device_attr *attr)
 -{
 -	int r;
 -
 -	switch (attr->group) {
 -	case KVM_DEV_FLIC_GET_ALL_IRQS:
 -		r = get_all_floating_irqs(dev->kvm, (u8 __user *) attr->addr,
 -					  attr->attr);
 -		break;
 -	default:
 -		r = -EINVAL;
 -	}
 -
 -	return r;
 -}
 -
 -static inline int copy_irq_from_user(struct kvm_s390_interrupt_info *inti,
 -				     u64 addr)
 +void kvm_s390_deliver_pending_interrupts(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_s390_irq __user *uptr = (struct kvm_s390_irq __user *) addr;
 -	void *target = NULL;
 -	void __user *source;
 -	u64 size;
 -
 -	if (get_user(inti->type, (u64 __user *)addr))
 -		return -EFAULT;
 +	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 +	struct kvm_s390_float_interrupt *fi = vcpu->arch.local_int.float_int;
 +	struct kvm_s390_interrupt_info  *n, *inti = NULL;
 +	int deliver;
  
 -	switch (inti->type) {
 -	case KVM_S390_INT_PFAULT_INIT:
 -	case KVM_S390_INT_PFAULT_DONE:
 -	case KVM_S390_INT_VIRTIO:
 -	case KVM_S390_INT_SERVICE:
 -		target = (void *) &inti->ext;
 -		source = &uptr->u.ext;
 -		size = sizeof(inti->ext);
 -		break;
 -	case KVM_S390_INT_IO_MIN...KVM_S390_INT_IO_MAX:
 -		target = (void *) &inti->io;
 -		source = &uptr->u.io;
 -		size = sizeof(inti->io);
 -		break;
 -	case KVM_S390_MCHK:
 -		target = (void *) &inti->mchk;
 -		source = &uptr->u.mchk;
 -		size = sizeof(inti->mchk);
 -		break;
 -	default:
 -		return -EINVAL;
 +	__reset_intercept_indicators(vcpu);
 +	if (atomic_read(&li->active)) {
 +		do {
 +			deliver = 0;
 +			spin_lock_bh(&li->lock);
 +			list_for_each_entry_safe(inti, n, &li->list, list) {
 +				if (__interrupt_is_deliverable(vcpu, inti)) {
 +					list_del(&inti->list);
 +					deliver = 1;
 +					break;
 +				}
 +				__set_intercept_indicator(vcpu, inti);
 +			}
 +			if (list_empty(&li->list))
 +				atomic_set(&li->active, 0);
 +			spin_unlock_bh(&li->lock);
 +			if (deliver) {
 +				__do_deliver_interrupt(vcpu, inti);
 +				kfree(inti);
 +			}
 +		} while (deliver);
  	}
  
 -	if (copy_from_user(target, source, size))
 -		return -EFAULT;
 +	if ((vcpu->arch.sie_block->ckc <
 +		get_tod_clock_fast() + vcpu->arch.sie_block->epoch))
 +		__try_deliver_ckc_interrupt(vcpu);
  
 -	return 0;
 +	if (atomic_read(&fi->active)) {
 +		do {
 +			deliver = 0;
 +			spin_lock(&fi->lock);
 +			list_for_each_entry_safe(inti, n, &fi->list, list) {
 +				if (__interrupt_is_deliverable(vcpu, inti)) {
 +					list_del(&inti->list);
 +					deliver = 1;
 +					break;
 +				}
 +				__set_intercept_indicator(vcpu, inti);
 +			}
 +			if (list_empty(&fi->list))
 +				atomic_set(&fi->active, 0);
 +			spin_unlock(&fi->lock);
 +			if (deliver) {
 +				__do_deliver_interrupt(vcpu, inti);
 +				kfree(inti);
 +			}
 +		} while (deliver);
 +	}
  }
  
 -static int enqueue_floating_irq(struct kvm_device *dev,
 -				struct kvm_device_attr *attr)
 +void kvm_s390_deliver_pending_machine_checks(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_s390_interrupt_info *inti = NULL;
 -	int r = 0;
 -	int len = attr->attr;
 -
 -	if (len % sizeof(struct kvm_s390_irq) != 0)
 -		return -EINVAL;
 -	else if (len > KVM_S390_FLIC_MAX_BUFFER)
 -		return -EINVAL;
 -
 -	while (len >= sizeof(struct kvm_s390_irq)) {
 -		inti = kzalloc(sizeof(*inti), GFP_KERNEL);
 -		if (!inti)
 -			return -ENOMEM;
 +	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 +	struct kvm_s390_float_interrupt *fi = vcpu->arch.local_int.float_int;
 +	struct kvm_s390_interrupt_info  *n, *inti = NULL;
 +	int deliver;
  
 -		r = copy_irq_from_user(inti, attr->addr);
 -		if (r) {
 -			kfree(inti);
 -			return r;
 -		}
 -		r = __inject_vm(dev->kvm, inti);
 -		if (r) {
 -			kfree(inti);
 -			return r;
 -		}
 -		len -= sizeof(struct kvm_s390_irq);
 -		attr->addr += sizeof(struct kvm_s390_irq);
 +	__reset_intercept_indicators(vcpu);
 +	if (atomic_read(&li->active)) {
 +		do {
 +			deliver = 0;
 +			spin_lock_bh(&li->lock);
 +			list_for_each_entry_safe(inti, n, &li->list, list) {
 +				if ((inti->type == KVM_S390_MCHK) &&
 +				    __interrupt_is_deliverable(vcpu, inti)) {
 +					list_del(&inti->list);
 +					deliver = 1;
 +					break;
 +				}
 +				__set_intercept_indicator(vcpu, inti);
 +			}
 +			if (list_empty(&li->list))
 +				atomic_set(&li->active, 0);
 +			spin_unlock_bh(&li->lock);
 +			if (deliver) {
 +				__do_deliver_interrupt(vcpu, inti);
 +				kfree(inti);
 +			}
 +		} while (deliver);
  	}
  
 -	return r;
 -}
 -
 -static struct s390_io_adapter *get_io_adapter(struct kvm *kvm, unsigned int id)
 -{
 -	if (id >= MAX_S390_IO_ADAPTERS)
 -		return NULL;
 -	return kvm->arch.adapters[id];
 +	if (atomic_read(&fi->active)) {
 +		do {
 +			deliver = 0;
 +			spin_lock(&fi->lock);
 +			list_for_each_entry_safe(inti, n, &fi->list, list) {
 +				if ((inti->type == KVM_S390_MCHK) &&
 +				    __interrupt_is_deliverable(vcpu, inti)) {
 +					list_del(&inti->list);
 +					deliver = 1;
 +					break;
 +				}
 +				__set_intercept_indicator(vcpu, inti);
 +			}
 +			if (list_empty(&fi->list))
 +				atomic_set(&fi->active, 0);
 +			spin_unlock(&fi->lock);
 +			if (deliver) {
 +				__do_deliver_interrupt(vcpu, inti);
 +				kfree(inti);
 +			}
 +		} while (deliver);
 +	}
  }
  
 -static int register_io_adapter(struct kvm_device *dev,
 -			       struct kvm_device_attr *attr)
 +int kvm_s390_inject_program_int(struct kvm_vcpu *vcpu, u16 code)
  {
 -	struct s390_io_adapter *adapter;
 -	struct kvm_s390_io_adapter adapter_info;
 -
 -	if (copy_from_user(&adapter_info,
 -			   (void __user *)attr->addr, sizeof(adapter_info)))
 -		return -EFAULT;
 -
 -	if ((adapter_info.id >= MAX_S390_IO_ADAPTERS) ||
 -	    (dev->kvm->arch.adapters[adapter_info.id] != NULL))
 -		return -EINVAL;
 +	struct kvm_s390_local_interrupt *li = &vcpu->arch.local_int;
 +	struct kvm_s390_interrupt_info *inti;
  
 -	adapter = kzalloc(sizeof(*adapter), GFP_KERNEL);
 -	if (!adapter)
 +	inti = kzalloc(sizeof(*inti), GFP_KERNEL);
 +	if (!inti)
  		return -ENOMEM;
  
 -	INIT_LIST_HEAD(&adapter->maps);
 -	init_rwsem(&adapter->maps_lock);
 -	atomic_set(&adapter->nr_maps, 0);
 -	adapter->id = adapter_info.id;
 -	adapter->isc = adapter_info.isc;
 -	adapter->maskable = adapter_info.maskable;
 -	adapter->masked = false;
 -	adapter->swap = adapter_info.swap;
 -	dev->kvm->arch.adapters[adapter->id] = adapter;
 +	inti->type = KVM_S390_PROGRAM_INT;
 +	inti->pgm.code = code;
  
 +	VCPU_EVENT(vcpu, 3, "inject: program check %d (from kernel)", code);
 +	trace_kvm_s390_inject_vcpu(vcpu->vcpu_id, inti->type, code, 0, 1);
 +	spin_lock_bh(&li->lock);
 +	list_add(&inti->list, &li->list);
 +	atomic_set(&li->active, 1);
 +	BUG_ON(waitqueue_active(li->wq));
 +	spin_unlock_bh(&li->lock);
  	return 0;
  }
  
* Unmerged path arch/s390/kernel/compat_wrapper.c
* Unmerged path arch/s390/include/asm/elf.h
diff --git a/arch/s390/include/asm/lowcore.h b/arch/s390/include/asm/lowcore.h
index 46d885094acd..f7f58dd58e1e 100644
--- a/arch/s390/include/asm/lowcore.h
+++ b/arch/s390/include/asm/lowcore.h
@@ -330,8 +330,8 @@ struct _lowcore {
 	__u64	stfle_fac_list[32];		/* 0x0f00 */
 	__u8	pad_0x1000[0x11b0-0x1000];	/* 0x1000 */
 
-	/* Pointer to vector register save area */
-	__u64	vector_save_area_addr;		/* 0x11b0 */
+	/* Pointer to the machine check extended save area */
+	__u64	mcesad;				/* 0x11b0 */
 
 	/* 64 bit extparam used for pfault/diag 250: defined by architecture */
 	__u64	ext_params2;			/* 0x11B8 */
@@ -355,10 +355,7 @@ struct _lowcore {
 
 	/* Transaction abort diagnostic block */
 	__u8	pgm_tdb[256];			/* 0x1800 */
-	__u8	pad_0x1900[0x1c00-0x1900];	/* 0x1900 */
-
-	/* Software defined save area for vector registers */
-	__u8	vector_save_area[1024];		/* 0x1c00 */
+	__u8	pad_0x1900[0x2000-0x1900];	/* 0x1900 */
 } __packed;
 
 #endif /* CONFIG_32BIT */
* Unmerged path arch/s390/include/asm/nmi.h
* Unmerged path arch/s390/include/asm/processor.h
* Unmerged path arch/s390/include/asm/setup.h
* Unmerged path arch/s390/include/asm/switch_to.h
* Unmerged path arch/s390/include/asm/thread_info.h
* Unmerged path arch/s390/include/uapi/asm/Kbuild
diff --git a/arch/s390/include/uapi/asm/guarded_storage.h b/arch/s390/include/uapi/asm/guarded_storage.h
new file mode 100644
index 000000000000..852850e8e17e
--- /dev/null
+++ b/arch/s390/include/uapi/asm/guarded_storage.h
@@ -0,0 +1,77 @@
+#ifndef _GUARDED_STORAGE_H
+#define _GUARDED_STORAGE_H
+
+#include <linux/types.h>
+
+struct gs_cb {
+	__u64 reserved;
+	__u64 gsd;
+	__u64 gssm;
+	__u64 gs_epl_a;
+};
+
+struct gs_epl {
+	__u8 pad1;
+	union {
+		__u8 gs_eam;
+		struct {
+			__u8	: 6;
+			__u8 e	: 1;
+			__u8 b	: 1;
+		};
+	};
+	union {
+		__u8 gs_eci;
+		struct {
+			__u8 tx	: 1;
+			__u8 cx	: 1;
+			__u8	: 5;
+			__u8 in	: 1;
+		};
+	};
+	union {
+		__u8 gs_eai;
+		struct {
+			__u8	: 1;
+			__u8 t	: 1;
+			__u8 as	: 2;
+			__u8 ar	: 4;
+		};
+	};
+	__u32 pad2;
+	__u64 gs_eha;
+	__u64 gs_eia;
+	__u64 gs_eoa;
+	__u64 gs_eir;
+	__u64 gs_era;
+};
+
+#define GS_ENABLE	0
+#define	GS_DISABLE	1
+#define GS_SET_BC_CB	2
+#define GS_CLEAR_BC_CB	3
+#define GS_BROADCAST	4
+
+static inline void load_gs_cb(struct gs_cb *gs_cb)
+{
+	asm volatile(".insn rxy,0xe3000000004d,0,%0" : : "Q" (*gs_cb));
+}
+
+static inline void store_gs_cb(struct gs_cb *gs_cb)
+{
+	asm volatile(".insn rxy,0xe30000000049,0,%0" : : "Q" (*gs_cb));
+}
+
+static inline void save_gs_cb(struct gs_cb *gs_cb)
+{
+	if (gs_cb)
+		store_gs_cb(gs_cb);
+}
+
+static inline void restore_gs_cb(struct gs_cb *gs_cb)
+{
+	if (gs_cb)
+		load_gs_cb(gs_cb);
+}
+
+#endif /* _GUARDED_STORAGE_H */
* Unmerged path arch/s390/include/uapi/asm/unistd.h
* Unmerged path arch/s390/kernel/Makefile
* Unmerged path arch/s390/kernel/asm-offsets.c
* Unmerged path arch/s390/kernel/compat_wrapper.c
* Unmerged path arch/s390/kernel/early.c
* Unmerged path arch/s390/kernel/entry.S
* Unmerged path arch/s390/kernel/entry.h
diff --git a/arch/s390/kernel/guarded_storage.c b/arch/s390/kernel/guarded_storage.c
new file mode 100644
index 000000000000..6f064745c3b1
--- /dev/null
+++ b/arch/s390/kernel/guarded_storage.c
@@ -0,0 +1,128 @@
+/*
+ * Copyright IBM Corp. 2016
+ * Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+#include <linux/signal.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <asm/guarded_storage.h>
+#include "entry.h"
+
+void exit_thread_gs(void)
+{
+	kfree(current->thread.gs_cb);
+	kfree(current->thread.gs_bc_cb);
+	current->thread.gs_cb = current->thread.gs_bc_cb = NULL;
+}
+
+static int gs_enable(void)
+{
+	struct gs_cb *gs_cb;
+
+	if (!current->thread.gs_cb) {
+		gs_cb = kzalloc(sizeof(*gs_cb), GFP_KERNEL);
+		if (!gs_cb)
+			return -ENOMEM;
+		gs_cb->gsd = 25;
+		preempt_disable();
+		__ctl_set_bit(2, 4);
+		load_gs_cb(gs_cb);
+		current->thread.gs_cb = gs_cb;
+		preempt_enable();
+	}
+	return 0;
+}
+
+static int gs_disable(void)
+{
+	if (current->thread.gs_cb) {
+		preempt_disable();
+		kfree(current->thread.gs_cb);
+		current->thread.gs_cb = NULL;
+		__ctl_clear_bit(2, 4);
+		preempt_enable();
+	}
+	return 0;
+}
+
+static int gs_set_bc_cb(struct gs_cb __user *u_gs_cb)
+{
+	struct gs_cb *gs_cb;
+
+	gs_cb = current->thread.gs_bc_cb;
+	if (!gs_cb) {
+		gs_cb = kzalloc(sizeof(*gs_cb), GFP_KERNEL);
+		if (!gs_cb)
+			return -ENOMEM;
+		current->thread.gs_bc_cb = gs_cb;
+	}
+	if (copy_from_user(gs_cb, u_gs_cb, sizeof(*gs_cb)))
+		return -EFAULT;
+	return 0;
+}
+
+static int gs_clear_bc_cb(void)
+{
+	struct gs_cb *gs_cb;
+
+	gs_cb = current->thread.gs_bc_cb;
+	current->thread.gs_bc_cb = NULL;
+	kfree(gs_cb);
+	return 0;
+}
+
+void gs_load_bc_cb(struct pt_regs *regs)
+{
+	struct gs_cb *gs_cb;
+
+	preempt_disable();
+	clear_thread_flag(TIF_GUARDED_STORAGE);
+	gs_cb = current->thread.gs_bc_cb;
+	if (gs_cb) {
+		kfree(current->thread.gs_cb);
+		current->thread.gs_bc_cb = NULL;
+		__ctl_set_bit(2, 4);
+		load_gs_cb(gs_cb);
+		current->thread.gs_cb = gs_cb;
+	}
+	preempt_enable();
+}
+
+static int gs_broadcast(void)
+{
+	struct task_struct *sibling;
+
+	read_lock(&tasklist_lock);
+	for_each_thread(current, sibling) {
+		if (!sibling->thread.gs_bc_cb)
+			continue;
+		if (test_and_set_tsk_thread_flag(sibling, TIF_GUARDED_STORAGE))
+			kick_process(sibling);
+	}
+	read_unlock(&tasklist_lock);
+	return 0;
+}
+
+SYSCALL_DEFINE2(s390_guarded_storage, int, command,
+		struct gs_cb __user *, gs_cb)
+{
+	if (!MACHINE_HAS_GS)
+		return -EOPNOTSUPP;
+	switch (command) {
+	case GS_ENABLE:
+		return gs_enable();
+	case GS_DISABLE:
+		return gs_disable();
+	case GS_SET_BC_CB:
+		return gs_set_bc_cb(gs_cb);
+	case GS_CLEAR_BC_CB:
+		return gs_clear_bc_cb();
+	case GS_BROADCAST:
+		return gs_broadcast();
+	default:
+		return -EINVAL;
+	}
+}
* Unmerged path arch/s390/kernel/machine_kexec.c
* Unmerged path arch/s390/kernel/nmi.c
* Unmerged path arch/s390/kernel/process.c
* Unmerged path arch/s390/kernel/processor.c
* Unmerged path arch/s390/kernel/ptrace.c
* Unmerged path arch/s390/kernel/setup.c
* Unmerged path arch/s390/kernel/smp.c
* Unmerged path arch/s390/kernel/syscalls.S
* Unmerged path arch/s390/kvm/interrupt.c
diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
index 0d8d5e022e25..4203eaa312ea 100644
--- a/include/uapi/linux/elf.h
+++ b/include/uapi/linux/elf.h
@@ -406,6 +406,7 @@ typedef struct elf64_shdr {
 #define NT_S390_TDB	0x308		/* s390 transaction diagnostic block */
 #define NT_S390_VXRS_LOW	0x309	/* s390 vector registers 0-15 upper half */
 #define NT_S390_VXRS_HIGH	0x30a	/* s390 vector registers 16-31 */
+#define NT_S390_GS_CB	0x30b		/* s390 guarded storage registers */
 #define NT_ARM_VFP	0x400		/* ARM VFP/NEON registers */
 #define NT_ARM_TLS	0x401		/* ARM TLS register */
 #define NT_ARM_HW_BREAK	0x402		/* ARM hardware breakpoint registers */
