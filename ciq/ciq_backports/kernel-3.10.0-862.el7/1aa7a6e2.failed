intel_pstate: Clean up get_target_pstate_use_performance()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Rafael J. Wysocki <rafael.j.wysocki@intel.com>
commit 1aa7a6e2b8105f22a5f7d6def281f776459c95ba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1aa7a6e2.failed

The comments and the core_busy variable name in
get_target_pstate_use_performance() are totally confusing,
so modify them to reflect what's going on.

The results of the computations should be the same as before.

	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 1aa7a6e2b8105f22a5f7d6def281f776459c95ba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index 275fb0708cd1,b76a98dd9988..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -1231,52 -1259,55 +1231,70 @@@ static inline int32_t get_target_pstate
  
  static inline int32_t get_target_pstate_use_performance(struct cpudata *cpu)
  {
++<<<<<<< HEAD
 +	int32_t core_busy, max_pstate, current_pstate, sample_ratio;
 +	s64 duration_us;
 +	u32 sample_time;
 +
 +	intel_pstate_calc_busy(cpu);
++=======
+ 	int32_t perf_scaled, max_pstate, current_pstate, sample_ratio;
+ 	u64 duration_ns;
++>>>>>>> 1aa7a6e2b810 (intel_pstate: Clean up get_target_pstate_use_performance())
  
  	/*
- 	 * core_busy is the ratio of actual performance to max
- 	 * max_pstate is the max non turbo pstate available
- 	 * current_pstate was the pstate that was requested during
- 	 * 	the last sample period.
- 	 *
- 	 * We normalize core_busy, which was our actual percent
- 	 * performance to what we requested during the last sample
- 	 * period. The result will be a percentage of busy at a
- 	 * specified pstate.
+ 	 * perf_scaled is the average performance during the last sampling
+ 	 * period scaled by the ratio of the maximum P-state to the P-state
+ 	 * requested last time (in percent).  That measures the system's
+ 	 * response to the previous P-state selection.
  	 */
++<<<<<<< HEAD
 +	core_busy = cpu->sample.core_pct_busy;
 +	max_pstate = int_tofp(cpu->pstate.max_pstate_physical);
 +	current_pstate = int_tofp(cpu->pstate.current_pstate);
 +	core_busy = mul_fp(core_busy, div_fp(max_pstate, current_pstate));
 +
 +	/*
 +	 * Since we have a deferred timer, it will not fire unless
 +	 * we are in C0.  So, determine if the actual elapsed time
 +	 * is significantly greater (3x) than our sample interval.  If it
 +	 * is, then we were idle for a long enough period of time
 +	 * to adjust our busyness.
 +	 */
 +	sample_time = pid_params.sample_rate_ms  * USEC_PER_MSEC;
 +	duration_us = ktime_us_delta(cpu->sample.time,
 +				     cpu->last_sample_time);
 +	if (duration_us > sample_time * 3) {
 +		sample_ratio = div_fp(int_tofp(sample_time),
 +				      int_tofp(duration_us));
 +		core_busy = mul_fp(core_busy, sample_ratio);
++=======
+ 	max_pstate = cpu->pstate.max_pstate_physical;
+ 	current_pstate = cpu->pstate.current_pstate;
+ 	perf_scaled = mul_ext_fp(cpu->sample.core_avg_perf,
+ 			       div_fp(100 * max_pstate, current_pstate));
+ 
+ 	/*
+ 	 * Since our utilization update callback will not run unless we are
+ 	 * in C0, check if the actual elapsed time is significantly greater (3x)
+ 	 * than our sample interval.  If it is, then we were idle for a long
+ 	 * enough period of time to adjust our performance metric.
+ 	 */
+ 	duration_ns = cpu->sample.time - cpu->last_sample_time;
+ 	if ((s64)duration_ns > pid_params.sample_rate_ns * 3) {
+ 		sample_ratio = div_fp(pid_params.sample_rate_ns, duration_ns);
+ 		perf_scaled = mul_fp(perf_scaled, sample_ratio);
++>>>>>>> 1aa7a6e2b810 (intel_pstate: Clean up get_target_pstate_use_performance())
  	} else {
  		sample_ratio = div_fp(100 * cpu->sample.mperf, cpu->sample.tsc);
  		if (sample_ratio < int_tofp(1))
- 			core_busy = 0;
+ 			perf_scaled = 0;
  	}
  
- 	cpu->sample.busy_scaled = core_busy;
- 	return cpu->pstate.current_pstate - pid_calc(&cpu->pid, core_busy);
+ 	cpu->sample.busy_scaled = perf_scaled;
+ 	return cpu->pstate.current_pstate - pid_calc(&cpu->pid, perf_scaled);
  }
  
 -static inline void intel_pstate_update_pstate(struct cpudata *cpu, int pstate)
 -{
 -	int max_perf, min_perf;
 -
 -	update_turbo_state();
 -
 -	intel_pstate_get_min_max(cpu, &min_perf, &max_perf);
 -	pstate = clamp_t(int, pstate, min_perf, max_perf);
 -	if (pstate == cpu->pstate.current_pstate)
 -		return;
 -
 -	intel_pstate_record_pstate(cpu, pstate);
 -	wrmsrl(MSR_IA32_PERF_CTL, pstate_funcs.get_val(cpu, pstate));
 -}
 -
  static inline void intel_pstate_adjust_busy_pstate(struct cpudata *cpu)
  {
  	int from, target_pstate;
* Unmerged path drivers/cpufreq/intel_pstate.c
