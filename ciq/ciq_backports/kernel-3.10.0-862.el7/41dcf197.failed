dm raid: fix incorrect status output at the end of a "recover" process

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Jonathan Brassow <jbrassow@redhat.com>
commit 41dcf197ad5373a7dd0a4b6572aec2e3ec6a0e49
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/41dcf197.failed

There are three important fields that indicate the overall health and
status of an array: dev_health, sync_ratio, and sync_action.  They tell
us the condition of the devices in the array, and the degree to which
the array is synchronized.

This commit fixes a condition that is reported incorrectly.  When a member
of the array is being rebuilt or a new device is added, the "recover"
process is used to synchronize it with the rest of the array.  When the
process is complete, but the sync thread hasn't yet been reaped, it is
possible for the state of MD to be:
 mddev->recovery = [ MD_RECOVERY_RUNNING MD_RECOVERY_RECOVER MD_RECOVERY_DONE ]
 curr_resync_completed = <max dev size> (but not MaxSector)
 and all rdevs to be In_sync.
This causes the 'array_in_sync' output parameter that is passed to
rs_get_progress() to be computed incorrectly and reported as 'false' --
or not in-sync.  This in turn causes the dev_health status characters to
be reported as all 'a', rather than the proper 'A'.

This can cause erroneous output for several seconds at a time when tools
will want to be checking the condition due to events that are raised at
the end of a sync process.  Fix this by properly calculating the
'array_in_sync' return parameter in rs_get_progress().

Also, remove an unnecessary intermediate 'recovery_cp' variable in
rs_get_progress().

	Signed-off-by: Jonathan Brassow <jbrassow@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit 41dcf197ad5373a7dd0a4b6572aec2e3ec6a0e49)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/device-mapper/dm-raid.txt
#	drivers/md/dm-raid.c
diff --cc Documentation/device-mapper/dm-raid.txt
index e1238bbb8846,32df07e29f68..000000000000
--- a/Documentation/device-mapper/dm-raid.txt
+++ b/Documentation/device-mapper/dm-raid.txt
@@@ -334,4 -342,6 +334,10 @@@ Version Histor
  1.10.1  Fix data corruption on reshape request
  1.11.0  Fix table line argument order
  	(wrong raid10_copies/raid10_format sequence)
++<<<<<<< HEAD
 +1.12.0  MD deadlock fixes
++=======
+ 1.11.1  Add raid4/5/6 journal write-back support via journal_mode option
+ 1.12.1  fix for MD deadlock between mddev_suspend() and md_write_start() available
+ 1.13.0  Fix dev_health status at end of "recover" (was 'a', now 'A')
++>>>>>>> 41dcf197ad53 (dm raid: fix incorrect status output at the end of a "recover" process)
diff --cc drivers/md/dm-raid.c
index 5deab4629dd8,43094ea89e37..000000000000
--- a/drivers/md/dm-raid.c
+++ b/drivers/md/dm-raid.c
@@@ -3810,27 -3891,9 +3811,31 @@@ static void raid_resume(struct dm_targe
  		mddev_resume(mddev);
  }
  
 +static int raid_merge(struct dm_target *ti, struct bvec_merge_data *bvm,
 +		      struct bio_vec *biovec, int max_size)
 +{
 +	struct raid_set *rs = ti->private;
 +	struct md_personality *pers = rs->md.pers;
 +
 +	if (pers && pers->mergeable_bvec)
 +		return min(max_size, pers->mergeable_bvec(&rs->md, bvm, biovec));
 +
 +	/*
 +	 * In case we can't request the personality because
 +	 * the raid set is not running yet
 +	 *
 +	 * -> return safe minimum
 +	 */
 +	return rs->md.chunk_sectors;
 +}
 +
  static struct target_type raid_target = {
  	.name = "raid",
++<<<<<<< HEAD
 +	.version = {1, 12, 0},
++=======
+ 	.version = {1, 13, 0},
++>>>>>>> 41dcf197ad53 (dm raid: fix incorrect status output at the end of a "recover" process)
  	.module = THIS_MODULE,
  	.ctr = raid_ctr,
  	.dtr = raid_dtr,
* Unmerged path Documentation/device-mapper/dm-raid.txt
* Unmerged path drivers/md/dm-raid.c
