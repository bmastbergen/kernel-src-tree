block: directly insert blk-mq request from blk_insert_cloned_request()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [block] directly insert blk-mq request from blk_insert_cloned_request() (Ming Lei) [1471956]
Rebuild_FUZZ: 94.74%
commit-author Jens Axboe <axboe@kernel.dk>
commit 157f377beb710e84bd8bc7a3c4475c0674ebebd7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/157f377b.failed

A NULL pointer crash was reported for the case of having the BFQ IO
scheduler attached to the underlying blk-mq paths of a DM multipath
device.  The crash occured in blk_mq_sched_insert_request()'s call to
e->type->ops.mq.insert_requests().

Paolo Valente correctly summarized why the crash occured with:
"the call chain (dm_mq_queue_rq -> map_request -> setup_clone ->
blk_rq_prep_clone) creates a cloned request without invoking
e->type->ops.mq.prepare_request for the target elevator e.  The cloned
request is therefore not initialized for the scheduler, but it is
however inserted into the scheduler by blk_mq_sched_insert_request."

All said, a request-based DM multipath device's IO scheduler should be
the only one used -- when the original requests are issued to the
underlying paths as cloned requests they are inserted directly in the
underlying dispatch queue(s) rather than through an additional elevator.

But commit bd166ef18 ("blk-mq-sched: add framework for MQ capable IO
schedulers") switched blk_insert_cloned_request() from using
blk_mq_insert_request() to blk_mq_sched_insert_request().  Which
incorrectly added elevator machinery into a call chain that isn't
supposed to have any.

To fix this introduce a blk-mq private blk_mq_request_bypass_insert()
that blk_insert_cloned_request() calls to insert the request without
involving any elevator that may be attached to the cloned request's
request_queue.

Fixes: bd166ef183c2 ("blk-mq-sched: add framework for MQ capable IO schedulers")
	Cc: stable@vger.kernel.org
	Reported-by: Bart Van Assche <Bart.VanAssche@wdc.com>
	Tested-by: Mike Snitzer <snitzer@redhat.com>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 157f377beb710e84bd8bc7a3c4475c0674ebebd7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	block/blk-mq.c
#	block/blk-mq.h
diff --cc block/blk-core.c
index df55e6267498,aebe676225e6..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -2193,8 -2342,13 +2193,18 @@@ int blk_insert_cloned_request(struct re
  	if (q->mq_ops) {
  		if (blk_queue_io_stat(q))
  			blk_account_io_start(rq, true);
++<<<<<<< HEAD
 +		blk_mq_insert_request(rq, false, true, false);
 +		return 0;
++=======
+ 		/*
+ 		 * Since we have a scheduler attached on the top device,
+ 		 * bypass a potential scheduler on the bottom device for
+ 		 * insert.
+ 		 */
+ 		blk_mq_request_bypass_insert(rq);
+ 		return BLK_STS_OK;
++>>>>>>> 157f377beb71 (block: directly insert blk-mq request from blk_insert_cloned_request())
  	}
  
  	spin_lock_irqsave(q->queue_lock, flags);
diff --cc block/blk-mq.c
index d9bfe0c6bc0e,98a18609755e..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1160,36 -1401,26 +1160,57 @@@ static void __blk_mq_insert_request(str
  	blk_mq_hctx_mark_pending(hctx, ctx);
  }
  
++<<<<<<< HEAD
 +void blk_mq_insert_request(struct request *rq, bool at_head, bool run_queue,
 +			   bool async)
 +{
 +	struct blk_mq_ctx *ctx = rq->mq_ctx;
 +	struct request_queue *q = rq->q;
 +	struct blk_mq_hw_ctx *hctx;
 +
 +	hctx = q->mq_ops->map_queue(q, ctx->cpu);
 +
 +	spin_lock(&ctx->lock);
 +	__blk_mq_insert_request(hctx, rq, at_head);
 +	spin_unlock(&ctx->lock);
 +
 +	if (run_queue)
 +		blk_mq_run_hw_queue(hctx, async);
 +}
 +
 +static void blk_mq_insert_requests(struct request_queue *q,
 +				     struct blk_mq_ctx *ctx,
 +				     struct list_head *list,
 +				     int depth,
 +				     bool from_schedule)
++=======
+ /*
+  * Should only be used carefully, when the caller knows we want to
+  * bypass a potential IO scheduler on the target device.
+  */
+ void blk_mq_request_bypass_insert(struct request *rq)
+ {
+ 	struct blk_mq_ctx *ctx = rq->mq_ctx;
+ 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(rq->q, ctx->cpu);
+ 
+ 	spin_lock(&hctx->lock);
+ 	list_add_tail(&rq->queuelist, &hctx->dispatch);
+ 	spin_unlock(&hctx->lock);
+ 
+ 	blk_mq_run_hw_queue(hctx, false);
+ }
+ 
+ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
+ 			    struct list_head *list)
++>>>>>>> 157f377beb71 (block: directly insert blk-mq request from blk_insert_cloned_request())
  
  {
 +	struct blk_mq_hw_ctx *hctx;
 +
 +	trace_block_unplug(q, depth, !from_schedule);
 +
 +	hctx = q->mq_ops->map_queue(q, ctx->cpu);
 +
  	/*
  	 * preemption doesn't flush plug list, so it's possible ctx->cpu is
  	 * offline now
diff --cc block/blk-mq.h
index 2d50f02667c4,ef15b3414da5..000000000000
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@@ -33,20 -30,33 +33,41 @@@ void blk_mq_freeze_queue(struct request
  void blk_mq_free_queue(struct request_queue *q);
  int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);
  void blk_mq_wake_waiters(struct request_queue *q);
 -bool blk_mq_dispatch_rq_list(struct request_queue *, struct list_head *);
 -void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);
 -bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx);
 -bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 -				bool wait);
 +bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *, struct list_head *);
  
  /*
 - * Internal helpers for allocating/freeing the request map
 + * CPU hotplug helpers
   */
++<<<<<<< HEAD
 +struct blk_mq_cpu_notifier;
 +void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
 +			      int (*fn)(void *, unsigned long, unsigned int),
 +			      void *data);
 +void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
 +void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier);
 +void blk_mq_cpu_init(void);
 +void blk_mq_enable_hotplug(void);
 +void blk_mq_disable_hotplug(void);
++=======
+ void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+ 		     unsigned int hctx_idx);
+ void blk_mq_free_rq_map(struct blk_mq_tags *tags);
+ struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
+ 					unsigned int hctx_idx,
+ 					unsigned int nr_tags,
+ 					unsigned int reserved_tags);
+ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
+ 		     unsigned int hctx_idx, unsigned int depth);
+ 
+ /*
+  * Internal helpers for request insertion into sw queues
+  */
+ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
+ 				bool at_head);
+ void blk_mq_request_bypass_insert(struct request *rq);
+ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
+ 				struct list_head *list);
++>>>>>>> 157f377beb71 (block: directly insert blk-mq request from blk_insert_cloned_request())
  
  /*
   * CPU -> queue mappings
* Unmerged path block/blk-core.c
* Unmerged path block/blk-mq.c
* Unmerged path block/blk-mq.h
