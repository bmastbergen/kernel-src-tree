PCI/MSI: Propagate IRQ affinity description through the MSI code

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [pci] msi: Propagate IRQ affinity description through the MSI code (Ming Lei) [1389540]
Rebuild_FUZZ: 96.77%
commit-author Christoph Hellwig <hch@lst.de>
commit 61e1c5905290efe48bacda5e342d4af4cb1b923b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/61e1c590.failed

No API change yet, just pass it down all the way from
pci_alloc_irq_vectors() to the core MSI code.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
	Acked-by: Bjorn Helgaas <bhelgaas@google.com>
	Acked-by: Jens Axboe <axboe@kernel.dk>
	Cc: linux-block@vger.kernel.org
	Cc: linux-pci@vger.kernel.org
Link: http://lkml.kernel.org/r/1478654107-7384-5-git-send-email-hch@lst.de
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

(cherry picked from commit 61e1c5905290efe48bacda5e342d4af4cb1b923b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/pci/msi.c
diff --cc drivers/pci/msi.c
index a0b55b8fc002,512f388a74f2..000000000000
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@@ -542,15 -550,23 +542,30 @@@ error_attrs
  	return ret;
  }
  
++<<<<<<< HEAD
 +static struct msi_desc *msi_setup_entry(struct pci_dev *dev, int nvec)
 +{
 +	u16 control;
 +	struct msi_desc *entry;
++=======
+ static struct msi_desc *
+ msi_setup_entry(struct pci_dev *dev, int nvec, const struct irq_affinity *affd)
+ {
+ 	struct cpumask *masks = NULL;
+ 	struct msi_desc *entry;
+ 	u16 control;
+ 
+ 	if (affd) {
+ 		masks = irq_create_affinity_masks(nvec, affd);
+ 		if (!masks)
+ 			pr_err("Unable to allocate affinity masks, ignoring\n");
+ 	}
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  
  	/* MSI Entry Initialization */
 -	entry = alloc_msi_entry(&dev->dev, nvec, masks);
 +	entry = alloc_msi_entry(dev);
  	if (!entry)
 -		goto out;
 +		return NULL;
  
  	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);
  
@@@ -602,7 -618,8 +617,12 @@@ static int msi_verify_entries(struct pc
   * an error, and a positive return value indicates the number of interrupts
   * which could have been allocated.
   */
++<<<<<<< HEAD
 +static int msi_capability_init(struct pci_dev *dev, int nvec)
++=======
+ static int msi_capability_init(struct pci_dev *dev, int nvec,
+ 			       const struct irq_affinity *affd)
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  {
  	struct msi_desc *entry;
  	int ret;
@@@ -610,7 -627,7 +630,11 @@@
  
  	pci_msi_set_enable(dev, 0);	/* Disable MSI during set up */
  
++<<<<<<< HEAD
 +	entry = msi_setup_entry(dev, nvec);
++=======
+ 	entry = msi_setup_entry(dev, nvec, affd);
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  	if (!entry)
  		return -ENOMEM;
  
@@@ -673,13 -690,21 +697,31 @@@ static void __iomem *msix_map_region(st
  }
  
  static int msix_setup_entries(struct pci_dev *dev, void __iomem *base,
++<<<<<<< HEAD
 +			      struct msix_entry *entries, int nvec)
 +{
++=======
+ 			      struct msix_entry *entries, int nvec,
+ 			      const struct irq_affinity *affd)
+ {
+ 	struct cpumask *curmsk, *masks = NULL;
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  	struct msi_desc *entry;
 -	int ret, i;
 +	int i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < nvec; i++) {
 +		entry = alloc_msi_entry(dev);
++=======
+ 	if (affd) {
+ 		masks = irq_create_affinity_masks(nvec, affd);
+ 		if (!masks)
+ 			pr_err("Unable to allocate affinity masks, ignoring\n");
+ 	}
+ 
+ 	for (i = 0, curmsk = masks; i < nvec; i++) {
+ 		entry = alloc_msi_entry(&dev->dev, 1, curmsk);
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  		if (!entry) {
  			if (!i)
  				iounmap(base);
@@@ -731,8 -760,8 +773,13 @@@ static void msix_program_entries(struc
   * single MSI-X irq. A return of zero indicates the successful setup of
   * requested MSI-X entries with allocated irqs or non-zero for otherwise.
   **/
++<<<<<<< HEAD
 +static int msix_capability_init(struct pci_dev *dev,
 +				struct msix_entry *entries, int nvec)
++=======
+ static int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,
+ 				int nvec, const struct irq_affinity *affd)
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  {
  	int ret;
  	u16 control;
@@@ -747,7 -776,7 +794,11 @@@
  	if (!base)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	ret = msix_setup_entries(dev, base, entries, nvec);
++=======
+ 	ret = msix_setup_entries(dev, base, entries, nvec, affd);
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  	if (ret)
  		return ret;
  
@@@ -970,6 -956,42 +1021,45 @@@ int pci_msix_vec_count(struct pci_dev *
  }
  EXPORT_SYMBOL(pci_msix_vec_count);
  
++<<<<<<< HEAD
++=======
+ static int __pci_enable_msix(struct pci_dev *dev, struct msix_entry *entries,
+ 			     int nvec, const struct irq_affinity *affd)
+ {
+ 	int nr_entries;
+ 	int i, j;
+ 
+ 	if (!pci_msi_supported(dev, nvec))
+ 		return -EINVAL;
+ 
+ 	nr_entries = pci_msix_vec_count(dev);
+ 	if (nr_entries < 0)
+ 		return nr_entries;
+ 	if (nvec > nr_entries)
+ 		return nr_entries;
+ 
+ 	if (entries) {
+ 		/* Check for any invalid entries */
+ 		for (i = 0; i < nvec; i++) {
+ 			if (entries[i].entry >= nr_entries)
+ 				return -EINVAL;		/* invalid entry */
+ 			for (j = i + 1; j < nvec; j++) {
+ 				if (entries[i].entry == entries[j].entry)
+ 					return -EINVAL;	/* duplicate entry */
+ 			}
+ 		}
+ 	}
+ 	WARN_ON(!!dev->msix_enabled);
+ 
+ 	/* Check whether driver already requested for MSI irq */
+ 	if (dev->msi_enabled) {
+ 		dev_info(&dev->dev, "can't enable MSI-X (MSI IRQ already assigned)\n");
+ 		return -EINVAL;
+ 	}
+ 	return msix_capability_init(dev, entries, nvec, affd);
+ }
+ 
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  /**
   * pci_enable_msix - configure device's MSI-X capability structure
   * @dev: pointer to the pci_dev data structure of MSI-X device function
@@@ -987,38 -1009,7 +1077,42 @@@
   **/
  int pci_enable_msix(struct pci_dev *dev, struct msix_entry *entries, int nvec)
  {
++<<<<<<< HEAD
 +	int nr_entries;
 +	int i, j;
 +
 +	if (!pci_msi_supported(dev, nvec))
 +		return -EINVAL;
 +
 +	if (!entries)
 +		return -EINVAL;
 +
 +	nr_entries = pci_msix_vec_count(dev);
 +	if (nr_entries < 0)
 +		return nr_entries;
 +	if (nvec > nr_entries)
 +		return nr_entries;
 +
 +	/* Check for any invalid entries */
 +	for (i = 0; i < nvec; i++) {
 +		if (entries[i].entry >= nr_entries)
 +			return -EINVAL;		/* invalid entry */
 +		for (j = i + 1; j < nvec; j++) {
 +			if (entries[i].entry == entries[j].entry)
 +				return -EINVAL;	/* duplicate entry */
 +		}
 +	}
 +	WARN_ON(!!dev->msix_enabled);
 +
 +	/* Check whether driver already requested for MSI irq */
 +	if (dev->msi_enabled) {
 +		dev_info(&dev->dev, "can't enable MSI-X (MSI IRQ already assigned)\n");
 +		return -EINVAL;
 +	}
 +	return msix_capability_init(dev, entries, nvec);
++=======
+ 	return __pci_enable_msix(dev, entries, nvec, NULL);
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  }
  EXPORT_SYMBOL(pci_enable_msix);
  
@@@ -1068,24 -1059,8 +1162,29 @@@ int pci_msi_enabled(void
  }
  EXPORT_SYMBOL(pci_msi_enabled);
  
++<<<<<<< HEAD
 +void pci_msi_init_pci_dev(struct pci_dev *dev)
 +{
 +	INIT_LIST_HEAD(&dev->msi_list);
 +}
 +
 +/**
 + * pci_enable_msi_range - configure device's MSI capability structure
 + * @dev: device to configure
 + * @minvec: minimal number of interrupts to configure
 + * @maxvec: maximum number of interrupts to configure
 + *
 + * This function tries to allocate a maximum possible number of interrupts in a
 + * range between @minvec and @maxvec. It returns a negative errno if an error
 + * occurs. If it succeeds, it returns the actual number of interrupts allocated
 + * and updates the @dev's irq member to the lowest new interrupt number;
 + * the other interrupt numbers allocated to this device are consecutive.
 + **/
 +int pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec)
++=======
+ static int __pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec,
+ 				  const struct irq_affinity *affd)
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  {
  	int nvec;
  	int rc;
@@@ -1108,26 -1083,79 +1207,95 @@@
  	nvec = pci_msi_vec_count(dev);
  	if (nvec < 0)
  		return nvec;
 -	if (nvec < minvec)
 +	else if (nvec < minvec)
  		return -EINVAL;
 -
 -	if (nvec > maxvec)
 +	else if (nvec > maxvec)
  		nvec = maxvec;
  
++<<<<<<< HEAD
 +	do {
 +		rc = msi_capability_init(dev, nvec);
 +		if (rc < 0) {
++=======
+ 	for (;;) {
+ 		if (affd) {
+ 			nvec = irq_calc_affinity_vectors(nvec, affd);
+ 			if (nvec < minvec)
+ 				return -ENOSPC;
+ 		}
+ 
+ 		rc = msi_capability_init(dev, nvec, affd);
+ 		if (rc == 0)
+ 			return nvec;
+ 
+ 		if (rc < 0)
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  			return rc;
 -		if (rc < minvec)
 -			return -ENOSPC;
 +		} else if (rc > 0) {
 +			if (rc < minvec)
 +				return -ENOSPC;
 +			nvec = rc;
 +		}
 +	} while (rc);
  
++<<<<<<< HEAD
 +	return nvec;
 +}
 +EXPORT_SYMBOL(pci_enable_msi_range);
 +
++=======
+ 		nvec = rc;
+ 	}
+ }
+ 
+ /**
+  * pci_enable_msi_range - configure device's MSI capability structure
+  * @dev: device to configure
+  * @minvec: minimal number of interrupts to configure
+  * @maxvec: maximum number of interrupts to configure
+  *
+  * This function tries to allocate a maximum possible number of interrupts in a
+  * range between @minvec and @maxvec. It returns a negative errno if an error
+  * occurs. If it succeeds, it returns the actual number of interrupts allocated
+  * and updates the @dev's irq member to the lowest new interrupt number;
+  * the other interrupt numbers allocated to this device are consecutive.
+  **/
+ int pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec)
+ {
+ 	return __pci_enable_msi_range(dev, minvec, maxvec, NULL);
+ }
+ EXPORT_SYMBOL(pci_enable_msi_range);
+ 
+ static int __pci_enable_msix_range(struct pci_dev *dev,
+ 				   struct msix_entry *entries, int minvec,
+ 				   int maxvec, const struct irq_affinity *affd)
+ {
+ 	int rc, nvec = maxvec;
+ 
+ 	if (maxvec < minvec)
+ 		return -ERANGE;
+ 
+ 	for (;;) {
+ 		if (affd) {
+ 			nvec = irq_calc_affinity_vectors(nvec, affd);
+ 			if (nvec < minvec)
+ 				return -ENOSPC;
+ 		}
+ 
+ 		rc = __pci_enable_msix(dev, entries, nvec, affd);
+ 		if (rc == 0)
+ 			return nvec;
+ 
+ 		if (rc < 0)
+ 			return rc;
+ 		if (rc < minvec)
+ 			return -ENOSPC;
+ 
+ 		nvec = rc;
+ 	}
+ }
+ 
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
  /**
   * pci_enable_msix_range - configure device's MSI-X capability structure
   * @dev: pointer to the pci_dev data structure of MSI-X device function
@@@ -1144,25 -1172,410 +1312,433 @@@
   * with new allocated MSI-X interrupts.
   **/
  int pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries,
 -		int minvec, int maxvec)
 +			       int minvec, int maxvec)
  {
++<<<<<<< HEAD
 +	int nvec = maxvec;
 +	int rc;
 +
 +	if (maxvec < minvec)
 +		return -ERANGE;
 +
 +	do {
 +		rc = pci_enable_msix(dev, entries, nvec);
 +		if (rc < 0) {
 +			return rc;
 +		} else if (rc > 0) {
 +			if (rc < minvec)
 +				return -ENOSPC;
 +			nvec = rc;
 +		}
 +	} while (rc);
 +
 +	return nvec;
 +}
 +EXPORT_SYMBOL(pci_enable_msix_range);
++=======
+ 	return __pci_enable_msix_range(dev, entries, minvec, maxvec, NULL);
+ }
+ EXPORT_SYMBOL(pci_enable_msix_range);
+ 
+ /**
+  * pci_alloc_irq_vectors - allocate multiple IRQs for a device
+  * @dev:		PCI device to operate on
+  * @min_vecs:		minimum number of vectors required (must be >= 1)
+  * @max_vecs:		maximum (desired) number of vectors
+  * @flags:		flags or quirks for the allocation
+  *
+  * Allocate up to @max_vecs interrupt vectors for @dev, using MSI-X or MSI
+  * vectors if available, and fall back to a single legacy vector
+  * if neither is available.  Return the number of vectors allocated,
+  * (which might be smaller than @max_vecs) if successful, or a negative
+  * error code on error. If less than @min_vecs interrupt vectors are
+  * available for @dev the function will fail with -ENOSPC.
+  *
+  * To get the Linux IRQ number used for a vector that can be passed to
+  * request_irq() use the pci_irq_vector() helper.
+  */
+ int pci_alloc_irq_vectors(struct pci_dev *dev, unsigned int min_vecs,
+ 		unsigned int max_vecs, unsigned int flags)
+ {
+ 	static const struct irq_affinity msi_default_affd;
+ 	const struct irq_affinity *affd = NULL;
+ 	int vecs = -ENOSPC;
+ 
+ 	if (flags & PCI_IRQ_AFFINITY)
+ 		affd = &msi_default_affd;
+ 
+ 	if (flags & PCI_IRQ_MSIX) {
+ 		vecs = __pci_enable_msix_range(dev, NULL, min_vecs, max_vecs,
+ 				affd);
+ 		if (vecs > 0)
+ 			return vecs;
+ 	}
+ 
+ 	if (flags & PCI_IRQ_MSI) {
+ 		vecs = __pci_enable_msi_range(dev, min_vecs, max_vecs, affd);
+ 		if (vecs > 0)
+ 			return vecs;
+ 	}
+ 
+ 	/* use legacy irq if allowed */
+ 	if ((flags & PCI_IRQ_LEGACY) && min_vecs == 1) {
+ 		pci_intx(dev, 1);
+ 		return 1;
+ 	}
+ 
+ 	return vecs;
+ }
+ EXPORT_SYMBOL(pci_alloc_irq_vectors);
+ 
+ /**
+  * pci_free_irq_vectors - free previously allocated IRQs for a device
+  * @dev:		PCI device to operate on
+  *
+  * Undoes the allocations and enabling in pci_alloc_irq_vectors().
+  */
+ void pci_free_irq_vectors(struct pci_dev *dev)
+ {
+ 	pci_disable_msix(dev);
+ 	pci_disable_msi(dev);
+ }
+ EXPORT_SYMBOL(pci_free_irq_vectors);
+ 
+ /**
+  * pci_irq_vector - return Linux IRQ number of a device vector
+  * @dev: PCI device to operate on
+  * @nr: device-relative interrupt vector index (0-based).
+  */
+ int pci_irq_vector(struct pci_dev *dev, unsigned int nr)
+ {
+ 	if (dev->msix_enabled) {
+ 		struct msi_desc *entry;
+ 		int i = 0;
+ 
+ 		for_each_pci_msi_entry(entry, dev) {
+ 			if (i == nr)
+ 				return entry->irq;
+ 			i++;
+ 		}
+ 		WARN_ON_ONCE(1);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (dev->msi_enabled) {
+ 		struct msi_desc *entry = first_pci_msi_entry(dev);
+ 
+ 		if (WARN_ON_ONCE(nr >= entry->nvec_used))
+ 			return -EINVAL;
+ 	} else {
+ 		if (WARN_ON_ONCE(nr > 0))
+ 			return -EINVAL;
+ 	}
+ 
+ 	return dev->irq + nr;
+ }
+ EXPORT_SYMBOL(pci_irq_vector);
+ 
+ /**
+  * pci_irq_get_affinity - return the affinity of a particular msi vector
+  * @dev:	PCI device to operate on
+  * @nr:		device-relative interrupt vector index (0-based).
+  */
+ const struct cpumask *pci_irq_get_affinity(struct pci_dev *dev, int nr)
+ {
+ 	if (dev->msix_enabled) {
+ 		struct msi_desc *entry;
+ 		int i = 0;
+ 
+ 		for_each_pci_msi_entry(entry, dev) {
+ 			if (i == nr)
+ 				return entry->affinity;
+ 			i++;
+ 		}
+ 		WARN_ON_ONCE(1);
+ 		return NULL;
+ 	} else if (dev->msi_enabled) {
+ 		struct msi_desc *entry = first_pci_msi_entry(dev);
+ 
+ 		if (WARN_ON_ONCE(!entry || nr >= entry->nvec_used))
+ 			return NULL;
+ 
+ 		return &entry->affinity[nr];
+ 	} else {
+ 		return cpu_possible_mask;
+ 	}
+ }
+ EXPORT_SYMBOL(pci_irq_get_affinity);
+ 
+ struct pci_dev *msi_desc_to_pci_dev(struct msi_desc *desc)
+ {
+ 	return to_pci_dev(desc->dev);
+ }
+ EXPORT_SYMBOL(msi_desc_to_pci_dev);
+ 
+ void *msi_desc_to_pci_sysdata(struct msi_desc *desc)
+ {
+ 	struct pci_dev *dev = msi_desc_to_pci_dev(desc);
+ 
+ 	return dev->bus->sysdata;
+ }
+ EXPORT_SYMBOL_GPL(msi_desc_to_pci_sysdata);
+ 
+ #ifdef CONFIG_PCI_MSI_IRQ_DOMAIN
+ /**
+  * pci_msi_domain_write_msg - Helper to write MSI message to PCI config space
+  * @irq_data:	Pointer to interrupt data of the MSI interrupt
+  * @msg:	Pointer to the message
+  */
+ void pci_msi_domain_write_msg(struct irq_data *irq_data, struct msi_msg *msg)
+ {
+ 	struct msi_desc *desc = irq_data_get_msi_desc(irq_data);
+ 
+ 	/*
+ 	 * For MSI-X desc->irq is always equal to irq_data->irq. For
+ 	 * MSI only the first interrupt of MULTI MSI passes the test.
+ 	 */
+ 	if (desc->irq == irq_data->irq)
+ 		__pci_write_msi_msg(desc, msg);
+ }
+ 
+ /**
+  * pci_msi_domain_calc_hwirq - Generate a unique ID for an MSI source
+  * @dev:	Pointer to the PCI device
+  * @desc:	Pointer to the msi descriptor
+  *
+  * The ID number is only used within the irqdomain.
+  */
+ irq_hw_number_t pci_msi_domain_calc_hwirq(struct pci_dev *dev,
+ 					  struct msi_desc *desc)
+ {
+ 	return (irq_hw_number_t)desc->msi_attrib.entry_nr |
+ 		PCI_DEVID(dev->bus->number, dev->devfn) << 11 |
+ 		(pci_domain_nr(dev->bus) & 0xFFFFFFFF) << 27;
+ }
+ 
+ static inline bool pci_msi_desc_is_multi_msi(struct msi_desc *desc)
+ {
+ 	return !desc->msi_attrib.is_msix && desc->nvec_used > 1;
+ }
+ 
+ /**
+  * pci_msi_domain_check_cap - Verify that @domain supports the capabilities for @dev
+  * @domain:	The interrupt domain to check
+  * @info:	The domain info for verification
+  * @dev:	The device to check
+  *
+  * Returns:
+  *  0 if the functionality is supported
+  *  1 if Multi MSI is requested, but the domain does not support it
+  *  -ENOTSUPP otherwise
+  */
+ int pci_msi_domain_check_cap(struct irq_domain *domain,
+ 			     struct msi_domain_info *info, struct device *dev)
+ {
+ 	struct msi_desc *desc = first_pci_msi_entry(to_pci_dev(dev));
+ 
+ 	/* Special handling to support pci_enable_msi_range() */
+ 	if (pci_msi_desc_is_multi_msi(desc) &&
+ 	    !(info->flags & MSI_FLAG_MULTI_PCI_MSI))
+ 		return 1;
+ 	else if (desc->msi_attrib.is_msix && !(info->flags & MSI_FLAG_PCI_MSIX))
+ 		return -ENOTSUPP;
+ 
+ 	return 0;
+ }
+ 
+ static int pci_msi_domain_handle_error(struct irq_domain *domain,
+ 				       struct msi_desc *desc, int error)
+ {
+ 	/* Special handling to support pci_enable_msi_range() */
+ 	if (pci_msi_desc_is_multi_msi(desc) && error == -ENOSPC)
+ 		return 1;
+ 
+ 	return error;
+ }
+ 
+ #ifdef GENERIC_MSI_DOMAIN_OPS
+ static void pci_msi_domain_set_desc(msi_alloc_info_t *arg,
+ 				    struct msi_desc *desc)
+ {
+ 	arg->desc = desc;
+ 	arg->hwirq = pci_msi_domain_calc_hwirq(msi_desc_to_pci_dev(desc),
+ 					       desc);
+ }
+ #else
+ #define pci_msi_domain_set_desc		NULL
+ #endif
+ 
+ static struct msi_domain_ops pci_msi_domain_ops_default = {
+ 	.set_desc	= pci_msi_domain_set_desc,
+ 	.msi_check	= pci_msi_domain_check_cap,
+ 	.handle_error	= pci_msi_domain_handle_error,
+ };
+ 
+ static void pci_msi_domain_update_dom_ops(struct msi_domain_info *info)
+ {
+ 	struct msi_domain_ops *ops = info->ops;
+ 
+ 	if (ops == NULL) {
+ 		info->ops = &pci_msi_domain_ops_default;
+ 	} else {
+ 		if (ops->set_desc == NULL)
+ 			ops->set_desc = pci_msi_domain_set_desc;
+ 		if (ops->msi_check == NULL)
+ 			ops->msi_check = pci_msi_domain_check_cap;
+ 		if (ops->handle_error == NULL)
+ 			ops->handle_error = pci_msi_domain_handle_error;
+ 	}
+ }
+ 
+ static void pci_msi_domain_update_chip_ops(struct msi_domain_info *info)
+ {
+ 	struct irq_chip *chip = info->chip;
+ 
+ 	BUG_ON(!chip);
+ 	if (!chip->irq_write_msi_msg)
+ 		chip->irq_write_msi_msg = pci_msi_domain_write_msg;
+ 	if (!chip->irq_mask)
+ 		chip->irq_mask = pci_msi_mask_irq;
+ 	if (!chip->irq_unmask)
+ 		chip->irq_unmask = pci_msi_unmask_irq;
+ }
+ 
+ /**
+  * pci_msi_create_irq_domain - Create a MSI interrupt domain
+  * @fwnode:	Optional fwnode of the interrupt controller
+  * @info:	MSI domain info
+  * @parent:	Parent irq domain
+  *
+  * Updates the domain and chip ops and creates a MSI interrupt domain.
+  *
+  * Returns:
+  * A domain pointer or NULL in case of failure.
+  */
+ struct irq_domain *pci_msi_create_irq_domain(struct fwnode_handle *fwnode,
+ 					     struct msi_domain_info *info,
+ 					     struct irq_domain *parent)
+ {
+ 	struct irq_domain *domain;
+ 
+ 	if (info->flags & MSI_FLAG_USE_DEF_DOM_OPS)
+ 		pci_msi_domain_update_dom_ops(info);
+ 	if (info->flags & MSI_FLAG_USE_DEF_CHIP_OPS)
+ 		pci_msi_domain_update_chip_ops(info);
+ 
+ 	info->flags |= MSI_FLAG_ACTIVATE_EARLY;
+ 
+ 	domain = msi_create_irq_domain(fwnode, info, parent);
+ 	if (!domain)
+ 		return NULL;
+ 
+ 	domain->bus_token = DOMAIN_BUS_PCI_MSI;
+ 	return domain;
+ }
+ EXPORT_SYMBOL_GPL(pci_msi_create_irq_domain);
+ 
+ /**
+  * pci_msi_domain_alloc_irqs - Allocate interrupts for @dev in @domain
+  * @domain:	The interrupt domain to allocate from
+  * @dev:	The device for which to allocate
+  * @nvec:	The number of interrupts to allocate
+  * @type:	Unused to allow simpler migration from the arch_XXX interfaces
+  *
+  * Returns:
+  * A virtual interrupt number or an error code in case of failure
+  */
+ int pci_msi_domain_alloc_irqs(struct irq_domain *domain, struct pci_dev *dev,
+ 			      int nvec, int type)
+ {
+ 	return msi_domain_alloc_irqs(domain, &dev->dev, nvec);
+ }
+ 
+ /**
+  * pci_msi_domain_free_irqs - Free interrupts for @dev in @domain
+  * @domain:	The interrupt domain
+  * @dev:	The device for which to free interrupts
+  */
+ void pci_msi_domain_free_irqs(struct irq_domain *domain, struct pci_dev *dev)
+ {
+ 	msi_domain_free_irqs(domain, &dev->dev);
+ }
+ 
+ /**
+  * pci_msi_create_default_irq_domain - Create a default MSI interrupt domain
+  * @fwnode:	Optional fwnode of the interrupt controller
+  * @info:	MSI domain info
+  * @parent:	Parent irq domain
+  *
+  * Returns: A domain pointer or NULL in case of failure. If successful
+  * the default PCI/MSI irqdomain pointer is updated.
+  */
+ struct irq_domain *pci_msi_create_default_irq_domain(struct fwnode_handle *fwnode,
+ 		struct msi_domain_info *info, struct irq_domain *parent)
+ {
+ 	struct irq_domain *domain;
+ 
+ 	mutex_lock(&pci_msi_domain_lock);
+ 	if (pci_msi_default_domain) {
+ 		pr_err("PCI: default irq domain for PCI MSI has already been created.\n");
+ 		domain = NULL;
+ 	} else {
+ 		domain = pci_msi_create_irq_domain(fwnode, info, parent);
+ 		pci_msi_default_domain = domain;
+ 	}
+ 	mutex_unlock(&pci_msi_domain_lock);
+ 
+ 	return domain;
+ }
+ 
+ static int get_msi_id_cb(struct pci_dev *pdev, u16 alias, void *data)
+ {
+ 	u32 *pa = data;
+ 
+ 	*pa = alias;
+ 	return 0;
+ }
+ /**
+  * pci_msi_domain_get_msi_rid - Get the MSI requester id (RID)
+  * @domain:	The interrupt domain
+  * @pdev:	The PCI device.
+  *
+  * The RID for a device is formed from the alias, with a firmware
+  * supplied mapping applied
+  *
+  * Returns: The RID.
+  */
+ u32 pci_msi_domain_get_msi_rid(struct irq_domain *domain, struct pci_dev *pdev)
+ {
+ 	struct device_node *of_node;
+ 	u32 rid = 0;
+ 
+ 	pci_for_each_dma_alias(pdev, get_msi_id_cb, &rid);
+ 
+ 	of_node = irq_domain_get_of_node(domain);
+ 	rid = of_node ? of_msi_map_rid(&pdev->dev, of_node, rid) :
+ 			iort_msi_map_rid(&pdev->dev, rid);
+ 
+ 	return rid;
+ }
+ 
+ /**
+  * pci_msi_get_device_domain - Get the MSI domain for a given PCI device
+  * @pdev:	The PCI device
+  *
+  * Use the firmware data to find a device-specific MSI domain
+  * (i.e. not one that is ste as a default).
+  *
+  * Returns: The coresponding MSI domain or NULL if none has been found.
+  */
+ struct irq_domain *pci_msi_get_device_domain(struct pci_dev *pdev)
+ {
+ 	struct irq_domain *dom;
+ 	u32 rid = 0;
+ 
+ 	pci_for_each_dma_alias(pdev, get_msi_id_cb, &rid);
+ 	dom = of_msi_map_get_device_domain(&pdev->dev, rid);
+ 	if (!dom)
+ 		dom = iort_get_device_domain(&pdev->dev, rid);
+ 	return dom;
+ }
+ #endif /* CONFIG_PCI_MSI_IRQ_DOMAIN */
++>>>>>>> 61e1c5905290 (PCI/MSI: Propagate IRQ affinity description through the MSI code)
* Unmerged path drivers/pci/msi.c
