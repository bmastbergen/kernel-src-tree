scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [scsi] lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context (Dick Kennedy) [1385844 1461977 1387768]
Rebuild_FUZZ: 96.10%
commit-author James Smart <jsmart2021@gmail.com>
commit 6c621a2229b084da0d926967f84b059a10c26ede
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6c621a22.failed

Currently IO resources are mapped 1 to 1 with RQ buffers posted

Added logic to separate RQE buffers from IO op resources
(sgl/iocbq/context). During initialization, the driver will determine
how many SGLs it will allocate for NVMET (based on what the firmware
reports) and associate a NVMET IOCBq and NVMET context structure with
each one.

Now that hdr/data buffers are immediately reposted back to the RQ, 512
RQEs for each MRQ is sufficient. Also, since NVMET data buffers are now
128 bytes, lpfc_nvmet_mrq_post is not necessary anymore as we will
always post the max (512) buffers per NVMET MRQ.

	Signed-off-by: Dick Kennedy <dick.kennedy@broadcom.com>
	Signed-off-by: James Smart <james.smart@broadcom.com>
	Reviewed-by: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 6c621a2229b084da0d926967f84b059a10c26ede)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/lpfc/lpfc.h
#	drivers/scsi/lpfc/lpfc_attr.c
#	drivers/scsi/lpfc/lpfc_crtn.h
#	drivers/scsi/lpfc/lpfc_init.c
#	drivers/scsi/lpfc/lpfc_mem.c
#	drivers/scsi/lpfc/lpfc_nvmet.c
#	drivers/scsi/lpfc/lpfc_nvmet.h
#	drivers/scsi/lpfc/lpfc_sli.c
#	drivers/scsi/lpfc/lpfc_sli4.h
diff --cc drivers/scsi/lpfc/lpfc.h
index 61c6751c0584,72641b1d3ab8..000000000000
--- a/drivers/scsi/lpfc/lpfc.h
+++ b/drivers/scsi/lpfc/lpfc.h
@@@ -136,6 -162,16 +143,19 @@@ struct hbq_dmabuf 
  	uint32_t tag;
  	struct lpfc_cq_event cq_event;
  	unsigned long time_stamp;
++<<<<<<< HEAD
++=======
+ 	void *context;
+ };
+ 
+ struct rqb_dmabuf {
+ 	struct lpfc_dmabuf hbuf;
+ 	struct lpfc_dmabuf dbuf;
+ 	uint16_t total_size;
+ 	uint16_t bytes_recv;
+ 	struct lpfc_queue *hrq;	  /* ptr to associated Header RQ */
+ 	struct lpfc_queue *drq;	  /* ptr to associated Data RQ */
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  };
  
  /* Priority bit.  Set value to exceed low water mark in lpfc_mem. */
@@@ -729,8 -777,16 +749,18 @@@ struct lpfc_hba 
  	uint32_t cfg_fcp_imax;
  	uint32_t cfg_fcp_cpu_map;
  	uint32_t cfg_fcp_io_channel;
++<<<<<<< HEAD
++=======
+ 	uint32_t cfg_suppress_rsp;
+ 	uint32_t cfg_nvme_oas;
+ 	uint32_t cfg_nvme_io_channel;
+ 	uint32_t cfg_nvmet_mrq;
+ 	uint32_t cfg_enable_nvmet;
+ 	uint32_t cfg_nvme_enable_fb;
+ 	uint32_t cfg_nvmet_fb_size;
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	uint32_t cfg_total_seg_cnt;
  	uint32_t cfg_sg_seg_cnt;
 -	uint32_t cfg_nvme_seg_cnt;
  	uint32_t cfg_sg_dma_buf_size;
  	uint64_t cfg_soft_wwnn;
  	uint64_t cfg_soft_wwpn;
diff --cc drivers/scsi/lpfc/lpfc_attr.c
index b0e0bd1cf345,65264582915a..000000000000
--- a/drivers/scsi/lpfc/lpfc_attr.c
+++ b/drivers/scsi/lpfc/lpfc_attr.c
@@@ -3040,6 -3297,51 +3040,54 @@@ static DEVICE_ATTR(lpfc_devloss_tmo, S_
  		   lpfc_devloss_tmo_show, lpfc_devloss_tmo_store);
  
  /*
++<<<<<<< HEAD
++=======
+  * lpfc_suppress_rsp: Enable suppress rsp feature is firmware supports it
+  * lpfc_suppress_rsp = 0  Disable
+  * lpfc_suppress_rsp = 1  Enable (default)
+  *
+  */
+ LPFC_ATTR_R(suppress_rsp, 1, 0, 1,
+ 	    "Enable suppress rsp feature is firmware supports it");
+ 
+ /*
+  * lpfc_nvmet_mrq: Specify number of RQ pairs for processing NVMET cmds
+  * lpfc_nvmet_mrq = 1  use a single RQ pair
+  * lpfc_nvmet_mrq >= 2  use specified RQ pairs for MRQ
+  *
+  */
+ LPFC_ATTR_R(nvmet_mrq,
+ 	    1, 1, 16,
+ 	    "Specify number of RQ pairs for processing NVMET cmds");
+ 
+ /*
+  * lpfc_enable_fc4_type: Defines what FC4 types are supported.
+  * Supported Values:  1 - register just FCP
+  *                    3 - register both FCP and NVME
+  * Supported values are [1,3]. Default value is 1
+  */
+ LPFC_ATTR_R(enable_fc4_type, LPFC_ENABLE_FCP,
+ 	    LPFC_ENABLE_FCP, LPFC_ENABLE_BOTH,
+ 	    "Define fc4 type to register with fabric.");
+ 
+ /*
+  * lpfc_xri_split: Defines the division of XRI resources between SCSI and NVME
+  * This parameter is only used if:
+  *     lpfc_enable_fc4_type is 3 - register both FCP and NVME and
+  *     port is not configured for NVMET.
+  *
+  * ELS/CT always get 10% of XRIs, up to a maximum of 250
+  * The remaining XRIs get split up based on lpfc_xri_split per port:
+  *
+  * Supported Values are in percentages
+  * the xri_split value is the percentage the SCSI port will get. The remaining
+  * percentage will go to NVME.
+  */
+ LPFC_ATTR_R(xri_split, 50, 10, 90,
+ 	    "Division of XRI resources between SCSI and NVME");
+ 
+ /*
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  # lpfc_log_verbose: Only turn this flag on if you are willing to risk being
  # deluged with LOTS of information.
  # You can set a bit mask to record specific types of verbose messages:
@@@ -4763,6 -5146,12 +4811,14 @@@ struct device_attribute *lpfc_hba_attrs
  	&dev_attr_lpfc_fcp_imax,
  	&dev_attr_lpfc_fcp_cpu_map,
  	&dev_attr_lpfc_fcp_io_channel,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_lpfc_suppress_rsp,
+ 	&dev_attr_lpfc_nvme_io_channel,
+ 	&dev_attr_lpfc_nvmet_mrq,
+ 	&dev_attr_lpfc_nvme_enable_fb,
+ 	&dev_attr_lpfc_nvmet_fb_size,
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	&dev_attr_lpfc_enable_bg,
  	&dev_attr_lpfc_soft_wwnn,
  	&dev_attr_lpfc_soft_wwpn,
@@@ -5793,6 -6185,43 +5849,46 @@@ lpfc_get_cfgparam(struct lpfc_hba *phba
  		phba->cfg_poll = 0;
  	else
  		phba->cfg_poll = lpfc_poll;
++<<<<<<< HEAD
++=======
+ 	lpfc_suppress_rsp_init(phba, lpfc_suppress_rsp);
+ 
+ 	lpfc_enable_fc4_type_init(phba, lpfc_enable_fc4_type);
+ 	lpfc_nvmet_mrq_init(phba, lpfc_nvmet_mrq);
+ 
+ 	/* Initialize first burst. Target vs Initiator are different. */
+ 	lpfc_nvme_enable_fb_init(phba, lpfc_nvme_enable_fb);
+ 	lpfc_nvmet_fb_size_init(phba, lpfc_nvmet_fb_size);
+ 	lpfc_fcp_io_channel_init(phba, lpfc_fcp_io_channel);
+ 	lpfc_nvme_io_channel_init(phba, lpfc_nvme_io_channel);
+ 
+ 	if (phba->sli_rev != LPFC_SLI_REV4) {
+ 		/* NVME only supported on SLI4 */
+ 		phba->nvmet_support = 0;
+ 		phba->cfg_enable_fc4_type = LPFC_ENABLE_FCP;
+ 	} else {
+ 		/* We MUST have FCP support */
+ 		if (!(phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP))
+ 			phba->cfg_enable_fc4_type |= LPFC_ENABLE_FCP;
+ 	}
+ 
+ 	/* A value of 0 means use the number of CPUs found in the system */
+ 	if (phba->cfg_fcp_io_channel == 0)
+ 		phba->cfg_fcp_io_channel = phba->sli4_hba.num_present_cpu;
+ 	if (phba->cfg_nvme_io_channel == 0)
+ 		phba->cfg_nvme_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_enable_fc4_type == LPFC_ENABLE_NVME)
+ 		phba->cfg_fcp_io_channel = 0;
+ 
+ 	if (phba->cfg_enable_fc4_type == LPFC_ENABLE_FCP)
+ 		phba->cfg_nvme_io_channel = 0;
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->cfg_nvme_io_channel)
+ 		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
+ 	else
+ 		phba->io_channel_irqs = phba->cfg_nvme_io_channel;
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  
  	phba->cfg_soft_wwnn = 0L;
  	phba->cfg_soft_wwpn = 0L;
@@@ -5812,6 -6242,59 +5908,62 @@@
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * lpfc_nvme_mod_param_dep - Adjust module parameter value based on
+  * dependencies between protocols and roles.
+  * @phba: lpfc_hba pointer.
+  **/
+ void
+ lpfc_nvme_mod_param_dep(struct lpfc_hba *phba)
+ {
+ 	if (phba->cfg_nvme_io_channel > phba->sli4_hba.num_present_cpu)
+ 		phba->cfg_nvme_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->sli4_hba.num_present_cpu)
+ 		phba->cfg_fcp_io_channel = phba->sli4_hba.num_present_cpu;
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME &&
+ 	    phba->nvmet_support) {
+ 		phba->cfg_enable_fc4_type &= ~LPFC_ENABLE_FCP;
+ 		phba->cfg_fcp_io_channel = 0;
+ 
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_NVME_DISC,
+ 				"6013 %s x%x fb_size x%x, fb_max x%x\n",
+ 				"NVME Target PRLI ACC enable_fb ",
+ 				phba->cfg_nvme_enable_fb,
+ 				phba->cfg_nvmet_fb_size,
+ 				LPFC_NVMET_FB_SZ_MAX);
+ 
+ 		if (phba->cfg_nvme_enable_fb == 0)
+ 			phba->cfg_nvmet_fb_size = 0;
+ 		else {
+ 			if (phba->cfg_nvmet_fb_size > LPFC_NVMET_FB_SZ_MAX)
+ 				phba->cfg_nvmet_fb_size = LPFC_NVMET_FB_SZ_MAX;
+ 		}
+ 
+ 		/* Adjust lpfc_nvmet_mrq to avoid running out of WQE slots */
+ 		if (phba->cfg_nvmet_mrq > phba->cfg_nvme_io_channel) {
+ 			phba->cfg_nvmet_mrq = phba->cfg_nvme_io_channel;
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_NVME_DISC,
+ 					"6018 Adjust lpfc_nvmet_mrq to %d\n",
+ 					phba->cfg_nvmet_mrq);
+ 		}
+ 	} else {
+ 		/* Not NVME Target mode.  Turn off Target parameters. */
+ 		phba->nvmet_support = 0;
+ 		phba->cfg_nvmet_mrq = 0;
+ 		phba->cfg_nvmet_fb_size = 0;
+ 	}
+ 
+ 	if (phba->cfg_fcp_io_channel > phba->cfg_nvme_io_channel)
+ 		phba->io_channel_irqs = phba->cfg_fcp_io_channel;
+ 	else
+ 		phba->io_channel_irqs = phba->cfg_nvme_io_channel;
+ }
+ 
+ /**
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
   * lpfc_get_vport_cfgparam - Used during port create, init the vport structure
   * @vport: lpfc_vport pointer.
   **/
diff --cc drivers/scsi/lpfc/lpfc_crtn.h
index 5c660beb66e2,cc95abd130b4..000000000000
--- a/drivers/scsi/lpfc/lpfc_crtn.h
+++ b/drivers/scsi/lpfc/lpfc_crtn.h
@@@ -231,8 -246,17 +233,22 @@@ struct hbq_dmabuf *lpfc_els_hbq_alloc(s
  void lpfc_els_hbq_free(struct lpfc_hba *, struct hbq_dmabuf *);
  struct hbq_dmabuf *lpfc_sli4_rb_alloc(struct lpfc_hba *);
  void lpfc_sli4_rb_free(struct lpfc_hba *, struct hbq_dmabuf *);
++<<<<<<< HEAD
 +void lpfc_sli4_build_dflt_fcf_record(struct lpfc_hba *, struct fcf_record *,
 +			uint16_t);
++=======
+ struct rqb_dmabuf *lpfc_sli4_nvmet_alloc(struct lpfc_hba *phba);
+ void lpfc_sli4_nvmet_free(struct lpfc_hba *phba, struct rqb_dmabuf *dmab);
+ void lpfc_nvmet_ctxbuf_post(struct lpfc_hba *phba,
+ 			    struct lpfc_nvmet_ctxbuf *ctxp);
+ int lpfc_nvmet_rcv_unsol_abort(struct lpfc_vport *vport,
+ 			       struct fc_frame_header *fc_hdr);
+ void lpfc_sli4_build_dflt_fcf_record(struct lpfc_hba *, struct fcf_record *,
+ 			uint16_t);
+ int lpfc_sli4_rq_put(struct lpfc_queue *hq, struct lpfc_queue *dq,
+ 		     struct lpfc_rqe *hrqe, struct lpfc_rqe *drqe);
+ int lpfc_free_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *hq);
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  void lpfc_unregister_fcf(struct lpfc_hba *);
  void lpfc_unregister_fcf_rescan(struct lpfc_hba *);
  void lpfc_unregister_unused_fcf(struct lpfc_hba *);
diff --cc drivers/scsi/lpfc/lpfc_init.c
index 4d8c754a14fe,86b0b26dfeea..000000000000
--- a/drivers/scsi/lpfc/lpfc_init.c
+++ b/drivers/scsi/lpfc/lpfc_init.c
@@@ -1055,6 -1088,21 +1055,24 @@@ lpfc_hba_down_post_s4(struct lpfc_hba *
  	list_splice(&aborts, &phba->lpfc_scsi_buf_list_put);
  	spin_unlock_irqrestore(&phba->scsi_buf_list_put_lock, iflag);
  
++<<<<<<< HEAD
++=======
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {
+ 		list_for_each_entry_safe(psb, psb_next, &nvme_aborts, list) {
+ 			psb->pCmd = NULL;
+ 			psb->status = IOSTAT_SUCCESS;
+ 		}
+ 		spin_lock_irqsave(&phba->nvme_buf_list_put_lock, iflag);
+ 		list_splice(&nvme_aborts, &phba->lpfc_nvme_buf_list_put);
+ 		spin_unlock_irqrestore(&phba->nvme_buf_list_put_lock, iflag);
+ 
+ 		list_for_each_entry_safe(ctxp, ctxp_next, &nvmet_aborts, list) {
+ 			ctxp->flag &= ~(LPFC_NVMET_XBUSY | LPFC_NVMET_ABORT_OP);
+ 			lpfc_nvmet_ctxbuf_post(phba, ctxp->ctxbuf);
+ 		}
+ 	}
+ 
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	lpfc_sli4_free_sp_events(phba);
  	return 0;
  }
@@@ -3271,6 -3357,161 +3289,164 @@@ lpfc_sli4_xri_sgl_update(struct lpfc_hb
  		sglq_entry->sli4_lxritag = lxri;
  		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
  	}
++<<<<<<< HEAD
++=======
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_els_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_nvmet_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_sglq *sglq_entry = NULL, *sglq_entry_next = NULL;
+ 	uint16_t i, lxri, xri_cnt, els_xri_cnt;
+ 	uint16_t nvmet_xri_cnt;
+ 	LIST_HEAD(nvmet_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's nvmet xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 
+ 	/* For NVMET, ALL remaining XRIs are dedicated for IO processing */
+ 	nvmet_xri_cnt = phba->sli4_hba.max_cfg_param.max_xri - els_xri_cnt;
+ 
+ 	if (nvmet_xri_cnt > phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* els xri-sgl expanded */
+ 		xri_cnt = nvmet_xri_cnt - phba->sli4_hba.nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6302 NVMET xri-sgl cnt grew from %d to %d\n",
+ 				phba->sli4_hba.nvmet_xri_cnt, nvmet_xri_cnt);
+ 		/* allocate the additional nvmet sgls */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			sglq_entry = kzalloc(sizeof(struct lpfc_sglq),
+ 					     GFP_KERNEL);
+ 			if (sglq_entry == NULL) {
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6303 Failure to allocate an "
+ 						"NVMET sgl entry:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->buff_type = NVMET_BUFF_TYPE;
+ 			sglq_entry->virt = lpfc_nvmet_buf_alloc(phba, 0,
+ 							   &sglq_entry->phys);
+ 			if (sglq_entry->virt == NULL) {
+ 				kfree(sglq_entry);
+ 				lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 						"6304 Failure to allocate an "
+ 						"NVMET buf:%d\n", i);
+ 				rc = -ENOMEM;
+ 				goto out_free_mem;
+ 			}
+ 			sglq_entry->sgl = sglq_entry->virt;
+ 			memset(sglq_entry->sgl, 0,
+ 			       phba->cfg_sg_dma_buf_size);
+ 			sglq_entry->state = SGL_FREED;
+ 			list_add_tail(&sglq_entry->list, &nvmet_sgl_list);
+ 		}
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else if (nvmet_xri_cnt < phba->sli4_hba.nvmet_xri_cnt) {
+ 		/* nvmet xri-sgl shrunk */
+ 		xri_cnt = phba->sli4_hba.nvmet_xri_cnt - nvmet_xri_cnt;
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6305 NVMET xri-sgl count decreased from "
+ 				"%d to %d\n", phba->sli4_hba.nvmet_xri_cnt,
+ 				nvmet_xri_cnt);
+ 		spin_lock_irq(&phba->hbalock);
+ 		spin_lock(&phba->sli4_hba.sgl_list_lock);
+ 		list_splice_init(&phba->sli4_hba.lpfc_nvmet_sgl_list,
+ 				 &nvmet_sgl_list);
+ 		/* release extra nvmet sgls from list */
+ 		for (i = 0; i < xri_cnt; i++) {
+ 			list_remove_head(&nvmet_sgl_list,
+ 					 sglq_entry, struct lpfc_sglq, list);
+ 			if (sglq_entry) {
+ 				lpfc_nvmet_buf_free(phba, sglq_entry->virt,
+ 						    sglq_entry->phys);
+ 				kfree(sglq_entry);
+ 			}
+ 		}
+ 		list_splice_init(&nvmet_sgl_list,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list);
+ 		spin_unlock(&phba->sli4_hba.sgl_list_lock);
+ 		spin_unlock_irq(&phba->hbalock);
+ 	} else
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_SLI,
+ 				"6306 NVMET xri-sgl count unchanged: %d\n",
+ 				nvmet_xri_cnt);
+ 	phba->sli4_hba.nvmet_xri_cnt = nvmet_xri_cnt;
+ 
+ 	/* update xris to nvmet sgls on the list */
+ 	sglq_entry = NULL;
+ 	sglq_entry_next = NULL;
+ 	list_for_each_entry_safe(sglq_entry, sglq_entry_next,
+ 				 &phba->sli4_hba.lpfc_nvmet_sgl_list, list) {
+ 		lxri = lpfc_sli4_next_xritag(phba);
+ 		if (lxri == NO_XRI) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_SLI,
+ 					"6307 Failed to allocate xri for "
+ 					"NVMET sgl\n");
+ 			rc = -ENOMEM;
+ 			goto out_free_mem;
+ 		}
+ 		sglq_entry->sli4_lxritag = lxri;
+ 		sglq_entry->sli4_xritag = phba->sli4_hba.xri_ids[lxri];
+ 	}
+ 	return 0;
+ 
+ out_free_mem:
+ 	lpfc_free_nvmet_sgl_list(phba);
+ 	return rc;
+ }
+ 
+ /**
+  * lpfc_sli4_scsi_sgl_update - update xri-sgl sizing and mapping
+  * @phba: pointer to lpfc hba data structure.
+  *
+  * This routine first calculates the sizes of the current els and allocated
+  * scsi sgl lists, and then goes through all sgls to updates the physical
+  * XRIs assigned due to port function reset. During port initialization, the
+  * current els and allocated scsi sgl lists are 0s.
+  *
+  * Return codes
+  *   0 - successful (for now, it always returns 0)
+  **/
+ int
+ lpfc_sli4_scsi_sgl_update(struct lpfc_hba *phba)
+ {
+ 	struct lpfc_scsi_buf *psb, *psb_next;
+ 	uint16_t i, lxri, els_xri_cnt, scsi_xri_cnt;
+ 	LIST_HEAD(scsi_sgl_list);
+ 	int rc;
+ 
+ 	/*
+ 	 * update on pci function's els xri-sgl list
+ 	 */
+ 	els_xri_cnt = lpfc_sli4_get_els_iocb_cnt(phba);
+ 	phba->total_scsi_bufs = 0;
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  
  	/*
  	 * update on pci function's allocated scsi xri-sgl list
@@@ -5433,11 -5813,26 +5609,31 @@@ lpfc_sli4_driver_resource_setup(struct 
  	/*
  	 * Initialize the SLI Layer to run with lpfc SLI4 HBAs.
  	 */
++<<<<<<< HEAD
 +	/* Initialize the Abort scsi buffer list used by driver */
 +	spin_lock_init(&phba->sli4_hba.abts_scsi_buf_list_lock);
 +	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
++=======
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
+ 		/* Initialize the Abort scsi buffer list used by driver */
+ 		spin_lock_init(&phba->sli4_hba.abts_scsi_buf_list_lock);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_scsi_buf_list);
+ 	}
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) {
+ 		/* Initialize the Abort nvme buffer list used by driver */
+ 		spin_lock_init(&phba->sli4_hba.abts_nvme_buf_list_lock);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_nvme_buf_list);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_abts_nvmet_ctx_list);
+ 		INIT_LIST_HEAD(&phba->sli4_hba.lpfc_nvmet_ctx_list);
+ 
+ 		/* Fast-path XRI aborted CQ Event work queue list */
+ 		INIT_LIST_HEAD(&phba->sli4_hba.sp_nvme_xri_aborted_work_queue);
+ 	}
+ 
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	/* This abort list used by worker thread */
 -	spin_lock_init(&phba->sli4_hba.sgl_list_lock);
 -	spin_lock_init(&phba->sli4_hba.nvmet_io_lock);
 +	spin_lock_init(&phba->sli4_hba.abts_sgl_list_lock);
  
  	/*
  	 * Initialize driver internal slow-path work queues
@@@ -7616,86 -8253,153 +7812,127 @@@ lpfc_sli4_queue_destroy(struct lpfc_hb
  	if (phba->cfg_fof)
  		lpfc_fof_queue_destroy(phba);
  
 -	/* Release HBA eqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.hba_eq, phba->io_channel_irqs);
 +	if (phba->sli4_hba.hba_eq != NULL) {
 +		/* Release HBA event queue */
 +		for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++) {
 +			if (phba->sli4_hba.hba_eq[idx] != NULL) {
 +				lpfc_sli4_queue_free(
 +					phba->sli4_hba.hba_eq[idx]);
 +				phba->sli4_hba.hba_eq[idx] = NULL;
 +			}
 +		}
 +		kfree(phba->sli4_hba.hba_eq);
 +		phba->sli4_hba.hba_eq = NULL;
 +	}
  
 -	/* Release FCP cqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.fcp_cq,
 -				 phba->cfg_fcp_io_channel);
 +	if (phba->sli4_hba.fcp_cq != NULL) {
 +		/* Release FCP completion queue */
 +		for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++) {
 +			if (phba->sli4_hba.fcp_cq[idx] != NULL) {
 +				lpfc_sli4_queue_free(
 +					phba->sli4_hba.fcp_cq[idx]);
 +				phba->sli4_hba.fcp_cq[idx] = NULL;
 +			}
 +		}
 +		kfree(phba->sli4_hba.fcp_cq);
 +		phba->sli4_hba.fcp_cq = NULL;
 +	}
  
 -	/* Release FCP wqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.fcp_wq,
 -				 phba->cfg_fcp_io_channel);
 +	if (phba->sli4_hba.fcp_wq != NULL) {
 +		/* Release FCP work queue */
 +		for (idx = 0; idx < phba->cfg_fcp_io_channel; idx++) {
 +			if (phba->sli4_hba.fcp_wq[idx] != NULL) {
 +				lpfc_sli4_queue_free(
 +					phba->sli4_hba.fcp_wq[idx]);
 +				phba->sli4_hba.fcp_wq[idx] = NULL;
 +			}
 +		}
 +		kfree(phba->sli4_hba.fcp_wq);
 +		phba->sli4_hba.fcp_wq = NULL;
 +	}
  
  	/* Release FCP CQ mapping array */
 -	lpfc_sli4_release_queue_map(&phba->sli4_hba.fcp_cq_map);
 -
 -	/* Release NVME cqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.nvme_cq,
 -					phba->cfg_nvme_io_channel);
 -
 -	/* Release NVME wqs */
 -	lpfc_sli4_release_queues(&phba->sli4_hba.nvme_wq,
 -					phba->cfg_nvme_io_channel);
 -
 -	/* Release NVME CQ mapping array */
 -	lpfc_sli4_release_queue_map(&phba->sli4_hba.nvme_cq_map);
 -
 -	lpfc_sli4_release_queues(&phba->sli4_hba.nvmet_cqset,
 -					phba->cfg_nvmet_mrq);
 -
 -	lpfc_sli4_release_queues(&phba->sli4_hba.nvmet_mrq_hdr,
 -					phba->cfg_nvmet_mrq);
 -	lpfc_sli4_release_queues(&phba->sli4_hba.nvmet_mrq_data,
 -					phba->cfg_nvmet_mrq);
 +	if (phba->sli4_hba.fcp_cq_map != NULL) {
 +		kfree(phba->sli4_hba.fcp_cq_map);
 +		phba->sli4_hba.fcp_cq_map = NULL;
 +	}
  
  	/* Release mailbox command work queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.mbx_wq);
 -
 -	/* Release ELS work queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.els_wq);
 +	if (phba->sli4_hba.mbx_wq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.mbx_wq);
 +		phba->sli4_hba.mbx_wq = NULL;
 +	}
  
  	/* Release ELS work queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.nvmels_wq);
 +	if (phba->sli4_hba.els_wq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.els_wq);
 +		phba->sli4_hba.els_wq = NULL;
 +	}
  
  	/* Release unsolicited receive queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.hdr_rq);
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.dat_rq);
 +	if (phba->sli4_hba.hdr_rq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.hdr_rq);
 +		phba->sli4_hba.hdr_rq = NULL;
 +	}
 +	if (phba->sli4_hba.dat_rq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.dat_rq);
 +		phba->sli4_hba.dat_rq = NULL;
 +	}
  
  	/* Release ELS complete queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.els_cq);
 -
 -	/* Release NVME LS complete queue */
 -	__lpfc_sli4_release_queue(&phba->sli4_hba.nvmels_cq);
 +	if (phba->sli4_hba.els_cq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.els_cq);
 +		phba->sli4_hba.els_cq = NULL;
 +	}
  
  	/* Release mailbox command complete queue */
++<<<<<<< HEAD
 +	if (phba->sli4_hba.mbx_cq != NULL) {
 +		lpfc_sli4_queue_free(phba->sli4_hba.mbx_cq);
 +		phba->sli4_hba.mbx_cq = NULL;
++=======
+ 	__lpfc_sli4_release_queue(&phba->sli4_hba.mbx_cq);
+ 
+ 	/* Everything on this list has been freed */
+ 	INIT_LIST_HEAD(&phba->sli4_hba.lpfc_wq_list);
+ }
+ 
+ int
+ lpfc_free_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *rq)
+ {
+ 	struct lpfc_rqb *rqbp;
+ 	struct lpfc_dmabuf *h_buf;
+ 	struct rqb_dmabuf *rqb_buffer;
+ 
+ 	rqbp = rq->rqbp;
+ 	while (!list_empty(&rqbp->rqb_buffer_list)) {
+ 		list_remove_head(&rqbp->rqb_buffer_list, h_buf,
+ 				 struct lpfc_dmabuf, list);
+ 
+ 		rqb_buffer = container_of(h_buf, struct rqb_dmabuf, hbuf);
+ 		(rqbp->rqb_free_buffer)(phba, rqb_buffer);
+ 		rqbp->buffer_count--;
+ 	}
+ 	return 1;
+ }
+ 
+ static int
+ lpfc_create_wq_cq(struct lpfc_hba *phba, struct lpfc_queue *eq,
+ 	struct lpfc_queue *cq, struct lpfc_queue *wq, uint16_t *cq_map,
+ 	int qidx, uint32_t qtype)
+ {
+ 	struct lpfc_sli_ring *pring;
+ 	int rc;
+ 
+ 	if (!eq || !cq || !wq) {
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 			"6085 Fast-path %s (%d) not allocated\n",
+ 			((eq) ? ((cq) ? "WQ" : "CQ") : "EQ"), qidx);
+ 		return -ENOMEM;
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	}
  
 -	/* create the Cq first */
 -	rc = lpfc_cq_create(phba, cq, eq,
 -			(qtype == LPFC_MBOX) ? LPFC_MCQ : LPFC_WCQ, qtype);
 -	if (rc) {
 -		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -			"6086 Failed setup of CQ (%d), rc = 0x%x\n",
 -			qidx, (uint32_t)rc);
 -		return rc;
 -	}
 -
 -	if (qtype != LPFC_MBOX) {
 -		/* Setup nvme_cq_map for fast lookup */
 -		if (cq_map)
 -			*cq_map = cq->queue_id;
 -
 -		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -			"6087 CQ setup: cq[%d]-id=%d, parent eq[%d]-id=%d\n",
 -			qidx, cq->queue_id, qidx, eq->queue_id);
 -
 -		/* create the wq */
 -		rc = lpfc_wq_create(phba, wq, cq, qtype);
 -		if (rc) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"6123 Fail setup fastpath WQ (%d), rc = 0x%x\n",
 -				qidx, (uint32_t)rc);
 -			/* no need to tear down cq - caller will do so */
 -			return rc;
 -		}
 -
 -		/* Bind this CQ/WQ to the NVME ring */
 -		pring = wq->pring;
 -		pring->sli.sli4.wqp = (void *)wq;
 -		cq->pring = pring;
 -
 -		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -			"2593 WQ setup: wq[%d]-id=%d assoc=%d, cq[%d]-id=%d\n",
 -			qidx, wq->queue_id, wq->assoc_qid, qidx, cq->queue_id);
 -	} else {
 -		rc = lpfc_mq_create(phba, wq, cq, LPFC_MBOX);
 -		if (rc) {
 -			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 -				"0539 Failed setup of slow-path MQ: "
 -				"rc = 0x%x\n", rc);
 -			/* no need to tear down cq - caller will do so */
 -			return rc;
 -		}
 -
 -		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 -			"2589 MBX MQ setup: wq-id=%d, parent cq-id=%d\n",
 -			phba->sli4_hba.mbx_wq->queue_id,
 -			phba->sli4_hba.mbx_cq->queue_id);
 -	}
 -
 -	return 0;
 +	return;
  }
  
  /**
@@@ -10602,19 -11088,6 +10839,22 @@@ lpfc_pci_probe_one_s4(struct pci_dev *p
  		goto out_unset_pci_mem_s4;
  	}
  
++<<<<<<< HEAD
 +	/* Initialize and populate the iocb list per host */
 +
 +	lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
 +			"2821 initialize iocb list %d.\n",
 +			phba->cfg_iocb_cnt*1024);
 +	error = lpfc_init_iocb_list(phba, phba->cfg_iocb_cnt*1024);
 +
 +	if (error) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
 +				"1413 Failed to initialize iocb list.\n");
 +		goto out_unset_driver_resource_s4;
 +	}
 +
++=======
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	INIT_LIST_HEAD(&phba->active_rrq_list);
  	INIT_LIST_HEAD(&phba->fcf.fcf_pri_list);
  
diff --cc drivers/scsi/lpfc/lpfc_mem.c
index 3fa65338d3f5,fcc05a1517c2..000000000000
--- a/drivers/scsi/lpfc/lpfc_mem.c
+++ b/drivers/scsi/lpfc/lpfc_mem.c
@@@ -540,7 -610,67 +540,71 @@@ lpfc_sli4_rb_free(struct lpfc_hba *phba
  	pci_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
  	pci_pool_free(phba->lpfc_drb_pool, dmab->dbuf.virt, dmab->dbuf.phys);
  	kfree(dmab);
++<<<<<<< HEAD
 +	return;
++=======
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_alloc - Allocate an SLI4 Receive buffer
+  * @phba: HBA to allocate a receive buffer for
+  *
+  * Description: Allocates a DMA-mapped receive buffer from the lpfc_hrb_pool PCI
+  * pool along a non-DMA-mapped container for it.
+  *
+  * Notes: Not interrupt-safe.  Must be called with no locks held.
+  *
+  * Returns:
+  *   pointer to HBQ on success
+  *   NULL on failure
+  **/
+ struct rqb_dmabuf *
+ lpfc_sli4_nvmet_alloc(struct lpfc_hba *phba)
+ {
+ 	struct rqb_dmabuf *dma_buf;
+ 
+ 	dma_buf = kzalloc(sizeof(struct rqb_dmabuf), GFP_KERNEL);
+ 	if (!dma_buf)
+ 		return NULL;
+ 
+ 	dma_buf->hbuf.virt = pci_pool_alloc(phba->lpfc_hrb_pool, GFP_KERNEL,
+ 					    &dma_buf->hbuf.phys);
+ 	if (!dma_buf->hbuf.virt) {
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->dbuf.virt = pci_pool_alloc(phba->lpfc_nvmet_drb_pool,
+ 					    GFP_KERNEL, &dma_buf->dbuf.phys);
+ 	if (!dma_buf->dbuf.virt) {
+ 		pci_pool_free(phba->lpfc_hrb_pool, dma_buf->hbuf.virt,
+ 			      dma_buf->hbuf.phys);
+ 		kfree(dma_buf);
+ 		return NULL;
+ 	}
+ 	dma_buf->total_size = LPFC_NVMET_DATA_BUF_SIZE;
+ 	return dma_buf;
+ }
+ 
+ /**
+  * lpfc_sli4_nvmet_free - Frees a receive buffer
+  * @phba: HBA buffer was allocated for
+  * @dmab: DMA Buffer container returned by lpfc_sli4_rbq_alloc
+  *
+  * Description: Frees both the container and the DMA-mapped buffers returned by
+  * lpfc_sli4_nvmet_alloc.
+  *
+  * Notes: Can be called with or without locks held.
+  *
+  * Returns: None
+  **/
+ void
+ lpfc_sli4_nvmet_free(struct lpfc_hba *phba, struct rqb_dmabuf *dmab)
+ {
+ 	pci_pool_free(phba->lpfc_hrb_pool, dmab->hbuf.virt, dmab->hbuf.phys);
+ 	pci_pool_free(phba->lpfc_nvmet_drb_pool,
+ 		      dmab->dbuf.virt, dmab->dbuf.phys);
+ 	kfree(dmab);
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  }
  
  /**
@@@ -586,3 -716,53 +650,56 @@@ lpfc_in_buf_free(struct lpfc_hba *phba
  	}
  	return;
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * lpfc_rq_buf_free - Free a RQ DMA buffer
+  * @phba: HBA buffer is associated with
+  * @mp: Buffer to free
+  *
+  * Description: Frees the given DMA buffer in the appropriate way given by
+  * reposting it to its associated RQ so it can be reused.
+  *
+  * Notes: Takes phba->hbalock.  Can be called with or without other locks held.
+  *
+  * Returns: None
+  **/
+ void
+ lpfc_rq_buf_free(struct lpfc_hba *phba, struct lpfc_dmabuf *mp)
+ {
+ 	struct lpfc_rqb *rqbp;
+ 	struct lpfc_rqe hrqe;
+ 	struct lpfc_rqe drqe;
+ 	struct rqb_dmabuf *rqb_entry;
+ 	unsigned long flags;
+ 	int rc;
+ 
+ 	if (!mp)
+ 		return;
+ 
+ 	rqb_entry = container_of(mp, struct rqb_dmabuf, hbuf);
+ 	rqbp = rqb_entry->hrq->rqbp;
+ 
+ 	spin_lock_irqsave(&phba->hbalock, flags);
+ 	list_del(&rqb_entry->hbuf.list);
+ 	hrqe.address_lo = putPaddrLow(rqb_entry->hbuf.phys);
+ 	hrqe.address_hi = putPaddrHigh(rqb_entry->hbuf.phys);
+ 	drqe.address_lo = putPaddrLow(rqb_entry->dbuf.phys);
+ 	drqe.address_hi = putPaddrHigh(rqb_entry->dbuf.phys);
+ 	rc = lpfc_sli4_rq_put(rqb_entry->hrq, rqb_entry->drq, &hrqe, &drqe);
+ 	if (rc < 0) {
+ 		(rqbp->rqb_free_buffer)(phba, rqb_entry);
+ 		lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 				"6409 Cannot post to RQ %d: %x %x\n",
+ 				rqb_entry->hrq->queue_id,
+ 				rqb_entry->hrq->host_index,
+ 				rqb_entry->hrq->hba_index);
+ 	} else {
+ 		list_add_tail(&rqb_entry->hbuf.list, &rqbp->rqb_buffer_list);
+ 		rqbp->buffer_count++;
+ 	}
+ 
+ 	spin_unlock_irqrestore(&phba->hbalock, flags);
+ }
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
diff --cc drivers/scsi/lpfc/lpfc_sli.c
index c54385fd9058,d68ee3ee299a..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli.c
+++ b/drivers/scsi/lpfc/lpfc_sli.c
@@@ -6296,19 -6513,62 +6296,66 @@@ lpfc_set_host_data(struct lpfc_hba *phb
  		 (phba->hba_flag & HBA_FCOE_MODE) ? "FCoE" : "FC");
  }
  
+ static int
+ lpfc_post_rq_buffer(struct lpfc_hba *phba, struct lpfc_queue *hrq,
+ 		    struct lpfc_queue *drq, int count)
+ {
+ 	int rc, i;
+ 	struct lpfc_rqe hrqe;
+ 	struct lpfc_rqe drqe;
+ 	struct lpfc_rqb *rqbp;
+ 	struct rqb_dmabuf *rqb_buffer;
+ 	LIST_HEAD(rqb_buf_list);
+ 
+ 	rqbp = hrq->rqbp;
+ 	for (i = 0; i < count; i++) {
+ 		/* IF RQ is already full, don't bother */
+ 		if (rqbp->buffer_count + i >= rqbp->entry_count - 1)
+ 			break;
+ 		rqb_buffer = rqbp->rqb_alloc_buffer(phba);
+ 		if (!rqb_buffer)
+ 			break;
+ 		rqb_buffer->hrq = hrq;
+ 		rqb_buffer->drq = drq;
+ 		list_add_tail(&rqb_buffer->hbuf.list, &rqb_buf_list);
+ 	}
+ 	while (!list_empty(&rqb_buf_list)) {
+ 		list_remove_head(&rqb_buf_list, rqb_buffer, struct rqb_dmabuf,
+ 				 hbuf.list);
+ 
+ 		hrqe.address_lo = putPaddrLow(rqb_buffer->hbuf.phys);
+ 		hrqe.address_hi = putPaddrHigh(rqb_buffer->hbuf.phys);
+ 		drqe.address_lo = putPaddrLow(rqb_buffer->dbuf.phys);
+ 		drqe.address_hi = putPaddrHigh(rqb_buffer->dbuf.phys);
+ 		rc = lpfc_sli4_rq_put(hrq, drq, &hrqe, &drqe);
+ 		if (rc < 0) {
+ 			rqbp->rqb_free_buffer(phba, rqb_buffer);
+ 		} else {
+ 			list_add_tail(&rqb_buffer->hbuf.list,
+ 				      &rqbp->rqb_buffer_list);
+ 			rqbp->buffer_count++;
+ 		}
+ 	}
+ 	return 1;
+ }
+ 
  /**
 - * lpfc_sli4_hba_setup - SLI4 device initialization PCI function
 + * lpfc_sli4_hba_setup - SLI4 device intialization PCI function
   * @phba: Pointer to HBA context object.
   *
 - * This function is the main SLI4 device initialization PCI function. This
 - * function is called by the HBA initialization code, HBA reset code and
 + * This function is the main SLI4 device intialization PCI function. This
 + * function is called by the HBA intialization code, HBA reset code and
   * HBA error attention handler code. Caller is not required to hold any
   * locks.
   **/
  int
  lpfc_sli4_hba_setup(struct lpfc_hba *phba)
  {
++<<<<<<< HEAD
 +	int rc;
++=======
+ 	int rc, i, cnt;
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	LPFC_MBOXQ_t *mboxq;
  	struct lpfc_mqe *mqe;
  	uint8_t *vpd;
@@@ -6611,19 -6892,126 +6658,142 @@@
  				"0582 Error %d during els sgl post "
  				"operation\n", rc);
  		rc = -ENODEV;
++<<<<<<< HEAD
 +		goto out_free_mbox;
 +	}
 +
 +	/* register the allocated scsi sgl pool to the port */
 +	rc = lpfc_sli4_repost_scsi_sgl_list(phba);
 +	if (unlikely(rc)) {
 +		lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
 +				"0383 Error %d during scsi sgl post "
 +				"operation\n", rc);
 +		/* Some Scsi buffers were moved to the abort scsi list */
 +		/* A pci function reset will repost them */
 +		rc = -ENODEV;
 +		goto out_free_mbox;
++=======
+ 		goto out_destroy_queue;
+ 	}
+ 	phba->sli4_hba.els_xri_cnt = rc;
+ 
+ 	if (phba->nvmet_support) {
+ 		/* update host nvmet xri-sgl sizes and mappings */
+ 		rc = lpfc_sli4_nvmet_sgl_update(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6308 Failed to update nvmet-sgl size "
+ 					"and mapping: %d\n", rc);
+ 			goto out_destroy_queue;
+ 		}
+ 
+ 		/* register the nvmet sgl pool to the port */
+ 		rc = lpfc_sli4_repost_sgl_list(
+ 			phba,
+ 			&phba->sli4_hba.lpfc_nvmet_sgl_list,
+ 			phba->sli4_hba.nvmet_xri_cnt);
+ 		if (unlikely(rc < 0)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"3117 Error %d during nvmet "
+ 					"sgl post\n", rc);
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
+ 		phba->sli4_hba.nvmet_xri_cnt = rc;
+ 
+ 		cnt = phba->cfg_iocb_cnt * 1024;
+ 		/* We need 1 iocbq for every SGL, for IO processing */
+ 		cnt += phba->sli4_hba.nvmet_xri_cnt;
+ 		/* Initialize and populate the iocb list per host */
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
+ 				"2821 initialize iocb list %d total %d\n",
+ 				phba->cfg_iocb_cnt, cnt);
+ 		rc = lpfc_init_iocb_list(phba, cnt);
+ 		if (rc) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"1413 Failed to init iocb list.\n");
+ 			goto out_destroy_queue;
+ 		}
+ 
+ 		lpfc_nvmet_create_targetport(phba);
+ 	} else {
+ 		/* update host scsi xri-sgl sizes and mappings */
+ 		rc = lpfc_sli4_scsi_sgl_update(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6309 Failed to update scsi-sgl size "
+ 					"and mapping: %d\n", rc);
+ 			goto out_destroy_queue;
+ 		}
+ 
+ 		/* update host nvme xri-sgl sizes and mappings */
+ 		rc = lpfc_sli4_nvme_sgl_update(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6082 Failed to update nvme-sgl size "
+ 					"and mapping: %d\n", rc);
+ 			goto out_destroy_queue;
+ 		}
+ 
+ 		cnt = phba->cfg_iocb_cnt * 1024;
+ 		/* Initialize and populate the iocb list per host */
+ 		lpfc_printf_log(phba, KERN_INFO, LOG_INIT,
+ 				"2820 initialize iocb list %d total %d\n",
+ 				phba->cfg_iocb_cnt, cnt);
+ 		rc = lpfc_init_iocb_list(phba, cnt);
+ 		if (rc) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_INIT,
+ 					"6301 Failed to init iocb list.\n");
+ 			goto out_destroy_queue;
+ 		}
+ 	}
+ 
+ 	if (phba->nvmet_support && phba->cfg_nvmet_mrq) {
+ 		/* Post initial buffers to all RQs created */
+ 		for (i = 0; i < phba->cfg_nvmet_mrq; i++) {
+ 			rqbp = phba->sli4_hba.nvmet_mrq_hdr[i]->rqbp;
+ 			INIT_LIST_HEAD(&rqbp->rqb_buffer_list);
+ 			rqbp->rqb_alloc_buffer = lpfc_sli4_nvmet_alloc;
+ 			rqbp->rqb_free_buffer = lpfc_sli4_nvmet_free;
+ 			rqbp->entry_count = LPFC_NVMET_RQE_DEF_COUNT;
+ 			rqbp->buffer_count = 0;
+ 
+ 			lpfc_post_rq_buffer(
+ 				phba, phba->sli4_hba.nvmet_mrq_hdr[i],
+ 				phba->sli4_hba.nvmet_mrq_data[i],
+ 				LPFC_NVMET_RQE_DEF_COUNT);
+ 		}
+ 	}
+ 
+ 	if (phba->cfg_enable_fc4_type & LPFC_ENABLE_FCP) {
+ 		/* register the allocated scsi sgl pool to the port */
+ 		rc = lpfc_sli4_repost_scsi_sgl_list(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"0383 Error %d during scsi sgl post "
+ 					"operation\n", rc);
+ 			/* Some Scsi buffers were moved to abort scsi list */
+ 			/* A pci function reset will repost them */
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
+ 	}
+ 
+ 	if ((phba->cfg_enable_fc4_type & LPFC_ENABLE_NVME) &&
+ 	    (phba->nvmet_support == 0)) {
+ 
+ 		/* register the allocated nvme sgl pool to the port */
+ 		rc = lpfc_repost_nvme_sgl_list(phba);
+ 		if (unlikely(rc)) {
+ 			lpfc_printf_log(phba, KERN_ERR, LOG_MBOX | LOG_SLI,
+ 					"6116 Error %d during nvme sgl post "
+ 					"operation\n", rc);
+ 			/* Some NVME buffers were moved to abort nvme list */
+ 			/* A pci function reset will repost them */
+ 			rc = -ENODEV;
+ 			goto out_destroy_queue;
+ 		}
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	}
  
  	/* Post the rpi header region to the device. */
@@@ -17266,3 -18607,217 +17437,220 @@@ lpfc_drain_txq(struct lpfc_hba *phba
  
  	return txq_cnt;
  }
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * lpfc_wqe_bpl2sgl - Convert the bpl/bde to a sgl.
+  * @phba: Pointer to HBA context object.
+  * @pwqe: Pointer to command WQE.
+  * @sglq: Pointer to the scatter gather queue object.
+  *
+  * This routine converts the bpl or bde that is in the WQE
+  * to a sgl list for the sli4 hardware. The physical address
+  * of the bpl/bde is converted back to a virtual address.
+  * If the WQE contains a BPL then the list of BDE's is
+  * converted to sli4_sge's. If the WQE contains a single
+  * BDE then it is converted to a single sli_sge.
+  * The WQE is still in cpu endianness so the contents of
+  * the bpl can be used without byte swapping.
+  *
+  * Returns valid XRI = Success, NO_XRI = Failure.
+  */
+ static uint16_t
+ lpfc_wqe_bpl2sgl(struct lpfc_hba *phba, struct lpfc_iocbq *pwqeq,
+ 		 struct lpfc_sglq *sglq)
+ {
+ 	uint16_t xritag = NO_XRI;
+ 	struct ulp_bde64 *bpl = NULL;
+ 	struct ulp_bde64 bde;
+ 	struct sli4_sge *sgl  = NULL;
+ 	struct lpfc_dmabuf *dmabuf;
+ 	union lpfc_wqe *wqe;
+ 	int numBdes = 0;
+ 	int i = 0;
+ 	uint32_t offset = 0; /* accumulated offset in the sg request list */
+ 	int inbound = 0; /* number of sg reply entries inbound from firmware */
+ 	uint32_t cmd;
+ 
+ 	if (!pwqeq || !sglq)
+ 		return xritag;
+ 
+ 	sgl  = (struct sli4_sge *)sglq->sgl;
+ 	wqe = &pwqeq->wqe;
+ 	pwqeq->iocb.ulpIoTag = pwqeq->iotag;
+ 
+ 	cmd = bf_get(wqe_cmnd, &wqe->generic.wqe_com);
+ 	if (cmd == CMD_XMIT_BLS_RSP64_WQE)
+ 		return sglq->sli4_xritag;
+ 	numBdes = pwqeq->rsvd2;
+ 	if (numBdes) {
+ 		/* The addrHigh and addrLow fields within the WQE
+ 		 * have not been byteswapped yet so there is no
+ 		 * need to swap them back.
+ 		 */
+ 		if (pwqeq->context3)
+ 			dmabuf = (struct lpfc_dmabuf *)pwqeq->context3;
+ 		else
+ 			return xritag;
+ 
+ 		bpl  = (struct ulp_bde64 *)dmabuf->virt;
+ 		if (!bpl)
+ 			return xritag;
+ 
+ 		for (i = 0; i < numBdes; i++) {
+ 			/* Should already be byte swapped. */
+ 			sgl->addr_hi = bpl->addrHigh;
+ 			sgl->addr_lo = bpl->addrLow;
+ 
+ 			sgl->word2 = le32_to_cpu(sgl->word2);
+ 			if ((i+1) == numBdes)
+ 				bf_set(lpfc_sli4_sge_last, sgl, 1);
+ 			else
+ 				bf_set(lpfc_sli4_sge_last, sgl, 0);
+ 			/* swap the size field back to the cpu so we
+ 			 * can assign it to the sgl.
+ 			 */
+ 			bde.tus.w = le32_to_cpu(bpl->tus.w);
+ 			sgl->sge_len = cpu_to_le32(bde.tus.f.bdeSize);
+ 			/* The offsets in the sgl need to be accumulated
+ 			 * separately for the request and reply lists.
+ 			 * The request is always first, the reply follows.
+ 			 */
+ 			switch (cmd) {
+ 			case CMD_GEN_REQUEST64_WQE:
+ 				/* add up the reply sg entries */
+ 				if (bpl->tus.f.bdeFlags == BUFF_TYPE_BDE_64I)
+ 					inbound++;
+ 				/* first inbound? reset the offset */
+ 				if (inbound == 1)
+ 					offset = 0;
+ 				bf_set(lpfc_sli4_sge_offset, sgl, offset);
+ 				bf_set(lpfc_sli4_sge_type, sgl,
+ 					LPFC_SGE_TYPE_DATA);
+ 				offset += bde.tus.f.bdeSize;
+ 				break;
+ 			case CMD_FCP_TRSP64_WQE:
+ 				bf_set(lpfc_sli4_sge_offset, sgl, 0);
+ 				bf_set(lpfc_sli4_sge_type, sgl,
+ 					LPFC_SGE_TYPE_DATA);
+ 				break;
+ 			case CMD_FCP_TSEND64_WQE:
+ 			case CMD_FCP_TRECEIVE64_WQE:
+ 				bf_set(lpfc_sli4_sge_type, sgl,
+ 					bpl->tus.f.bdeFlags);
+ 				if (i < 3)
+ 					offset = 0;
+ 				else
+ 					offset += bde.tus.f.bdeSize;
+ 				bf_set(lpfc_sli4_sge_offset, sgl, offset);
+ 				break;
+ 			}
+ 			sgl->word2 = cpu_to_le32(sgl->word2);
+ 			bpl++;
+ 			sgl++;
+ 		}
+ 	} else if (wqe->gen_req.bde.tus.f.bdeFlags == BUFF_TYPE_BDE_64) {
+ 		/* The addrHigh and addrLow fields of the BDE have not
+ 		 * been byteswapped yet so they need to be swapped
+ 		 * before putting them in the sgl.
+ 		 */
+ 		sgl->addr_hi = cpu_to_le32(wqe->gen_req.bde.addrHigh);
+ 		sgl->addr_lo = cpu_to_le32(wqe->gen_req.bde.addrLow);
+ 		sgl->word2 = le32_to_cpu(sgl->word2);
+ 		bf_set(lpfc_sli4_sge_last, sgl, 1);
+ 		sgl->word2 = cpu_to_le32(sgl->word2);
+ 		sgl->sge_len = cpu_to_le32(wqe->gen_req.bde.tus.f.bdeSize);
+ 	}
+ 	return sglq->sli4_xritag;
+ }
+ 
+ /**
+  * lpfc_sli4_issue_wqe - Issue an SLI4 Work Queue Entry (WQE)
+  * @phba: Pointer to HBA context object.
+  * @ring_number: Base sli ring number
+  * @pwqe: Pointer to command WQE.
+  **/
+ int
+ lpfc_sli4_issue_wqe(struct lpfc_hba *phba, uint32_t ring_number,
+ 		    struct lpfc_iocbq *pwqe)
+ {
+ 	union lpfc_wqe *wqe = &pwqe->wqe;
+ 	struct lpfc_nvmet_rcv_ctx *ctxp;
+ 	struct lpfc_queue *wq;
+ 	struct lpfc_sglq *sglq;
+ 	struct lpfc_sli_ring *pring;
+ 	unsigned long iflags;
+ 
+ 	/* NVME_LS and NVME_LS ABTS requests. */
+ 	if (pwqe->iocb_flag & LPFC_IO_NVME_LS) {
+ 		pring =  phba->sli4_hba.nvmels_wq->pring;
+ 		spin_lock_irqsave(&pring->ring_lock, iflags);
+ 		sglq = __lpfc_sli_get_els_sglq(phba, pwqe);
+ 		if (!sglq) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_BUSY;
+ 		}
+ 		pwqe->sli4_lxritag = sglq->sli4_lxritag;
+ 		pwqe->sli4_xritag = sglq->sli4_xritag;
+ 		if (lpfc_wqe_bpl2sgl(phba, pwqe, sglq) == NO_XRI) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		bf_set(wqe_xri_tag, &pwqe->wqe.xmit_bls_rsp.wqe_com,
+ 		       pwqe->sli4_xritag);
+ 		if (lpfc_sli4_wq_put(phba->sli4_hba.nvmels_wq, wqe)) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		lpfc_sli_ringtxcmpl_put(phba, pring, pwqe);
+ 		spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 		return 0;
+ 	}
+ 
+ 	/* NVME_FCREQ and NVME_ABTS requests */
+ 	if (pwqe->iocb_flag & LPFC_IO_NVME) {
+ 		/* Get the IO distribution (hba_wqidx) for WQ assignment. */
+ 		pring = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx]->pring;
+ 
+ 		spin_lock_irqsave(&pring->ring_lock, iflags);
+ 		wq = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx];
+ 		bf_set(wqe_cqid, &wqe->generic.wqe_com,
+ 		      phba->sli4_hba.nvme_cq[pwqe->hba_wqidx]->queue_id);
+ 		if (lpfc_sli4_wq_put(wq, wqe)) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		lpfc_sli_ringtxcmpl_put(phba, pring, pwqe);
+ 		spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 		return 0;
+ 	}
+ 
+ 	/* NVMET requests */
+ 	if (pwqe->iocb_flag & LPFC_IO_NVMET) {
+ 		/* Get the IO distribution (hba_wqidx) for WQ assignment. */
+ 		pring = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx]->pring;
+ 
+ 		spin_lock_irqsave(&pring->ring_lock, iflags);
+ 		ctxp = pwqe->context2;
+ 		sglq = ctxp->ctxbuf->sglq;
+ 		if (pwqe->sli4_xritag ==  NO_XRI) {
+ 			pwqe->sli4_lxritag = sglq->sli4_lxritag;
+ 			pwqe->sli4_xritag = sglq->sli4_xritag;
+ 		}
+ 		bf_set(wqe_xri_tag, &pwqe->wqe.xmit_bls_rsp.wqe_com,
+ 		       pwqe->sli4_xritag);
+ 		wq = phba->sli4_hba.nvme_wq[pwqe->hba_wqidx];
+ 		bf_set(wqe_cqid, &wqe->generic.wqe_com,
+ 		      phba->sli4_hba.nvme_cq[pwqe->hba_wqidx]->queue_id);
+ 		if (lpfc_sli4_wq_put(wq, wqe)) {
+ 			spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 			return WQE_ERROR;
+ 		}
+ 		lpfc_sli_ringtxcmpl_put(phba, pring, pwqe);
+ 		spin_unlock_irqrestore(&pring->ring_lock, iflags);
+ 		return 0;
+ 	}
+ 	return WQE_ERROR;
+ }
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
diff --cc drivers/scsi/lpfc/lpfc_sli4.h
index 10078254ebc7,19e2f190ea2e..000000000000
--- a/drivers/scsi/lpfc/lpfc_sli4.h
+++ b/drivers/scsi/lpfc/lpfc_sli4.h
@@@ -568,14 -610,22 +568,24 @@@ struct lpfc_sli4_hba 
  	uint16_t rpi_hdrs_in_use; /* must post rpi hdrs if set. */
  	uint16_t next_xri; /* last_xri - max_cfg_param.xri_base = used */
  	uint16_t next_rpi;
 -	uint16_t nvme_xri_max;
 -	uint16_t nvme_xri_cnt;
 -	uint16_t nvme_xri_start;
  	uint16_t scsi_xri_max;
  	uint16_t scsi_xri_cnt;
 -	uint16_t scsi_xri_start;
  	uint16_t els_xri_cnt;
++<<<<<<< HEAD
 +	uint16_t scsi_xri_start;
 +	struct list_head lpfc_free_sgl_list;
 +	struct list_head lpfc_sgl_list;
 +	struct list_head lpfc_abts_els_sgl_list;
++=======
+ 	uint16_t nvmet_xri_cnt;
+ 	uint16_t nvmet_ctx_cnt;
+ 	struct list_head lpfc_els_sgl_list;
+ 	struct list_head lpfc_abts_els_sgl_list;
+ 	struct list_head lpfc_nvmet_sgl_list;
+ 	struct list_head lpfc_abts_nvmet_ctx_list;
+ 	struct list_head lpfc_nvmet_ctx_list;
++>>>>>>> 6c621a2229b0 (scsi: lpfc: Separate NVMET RQ buffer posting from IO resources SGL/iocbq/context)
  	struct list_head lpfc_abts_scsi_buf_list;
 -	struct list_head lpfc_abts_nvme_buf_list;
  	struct lpfc_sglq **lpfc_sglq_active_list;
  	struct list_head lpfc_rpi_hdr_list;
  	unsigned long *rpi_bmask;
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc.h
* Unmerged path drivers/scsi/lpfc/lpfc_attr.c
* Unmerged path drivers/scsi/lpfc/lpfc_crtn.h
* Unmerged path drivers/scsi/lpfc/lpfc_init.c
* Unmerged path drivers/scsi/lpfc/lpfc_mem.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.c
* Unmerged path drivers/scsi/lpfc/lpfc_nvmet.h
* Unmerged path drivers/scsi/lpfc/lpfc_sli.c
* Unmerged path drivers/scsi/lpfc/lpfc_sli4.h
