IB/mlx4: Add contig support for control objects

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Guy Levi <guyle@mellanox.com>
commit ed8637d3615b38bd4d12ba5eb8ee6a0c3888e754
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ed8637d3.failed

Taking advantage of the optimization which was introduced in previous
commit ("IB/mlx4: Use optimal numbers of MTT entries") to optimize the
MTT usage for QP and CQ.

	Signed-off-by: Guy Levi <guyle@mellanox.com>
	Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit ed8637d3615b38bd4d12ba5eb8ee6a0c3888e754)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx4/cq.c
#	drivers/infiniband/hw/mlx4/mr.c
#	drivers/infiniband/hw/mlx4/qp.c
diff --cc drivers/infiniband/hw/mlx4/cq.c
index 4c5afa6429df,bf4f14a1b4fc..000000000000
--- a/drivers/infiniband/hw/mlx4/cq.c
+++ b/drivers/infiniband/hw/mlx4/cq.c
@@@ -146,8 -148,10 +148,15 @@@ static int mlx4_ib_get_cq_umem(struct m
  	if (IS_ERR(*umem))
  		return PTR_ERR(*umem);
  
++<<<<<<< HEAD
 +	err = mlx4_mtt_init(dev->dev, ib_umem_page_count(*umem),
 +			    ilog2((*umem)->page_size), &buf->mtt);
++=======
+ 	n = ib_umem_page_count(*umem);
+ 	shift = mlx4_ib_umem_calc_optimal_mtt_size(*umem, 0, &n);
+ 	err = mlx4_mtt_init(dev->dev, n, shift, &buf->mtt);
+ 
++>>>>>>> ed8637d3615b (IB/mlx4: Add contig support for control objects)
  	if (err)
  		goto err_buf;
  
diff --cc drivers/infiniband/hw/mlx4/mr.c
index 433bcdbdd680,313bfb9ccb71..000000000000
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@@ -131,6 -245,128 +131,131 @@@ out
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Calculate optimal mtt size based on contiguous pages.
+  * Function will return also the number of pages that are not aligned to the
+  * calculated mtt_size to be added to total number of pages. For that we should
+  * check the first chunk length & last chunk length and if not aligned to
+  * mtt_size we should increment the non_aligned_pages number. All chunks in the
+  * middle already handled as part of mtt shift calculation for both their start
+  * & end addresses.
+  */
+ int mlx4_ib_umem_calc_optimal_mtt_size(struct ib_umem *umem, u64 start_va,
+ 				       int *num_of_mtts)
+ {
+ 	u64 block_shift = MLX4_MAX_MTT_SHIFT;
+ 	u64 min_shift = umem->page_shift;
+ 	u64 last_block_aligned_end = 0;
+ 	u64 current_block_start = 0;
+ 	u64 first_block_start = 0;
+ 	u64 current_block_len = 0;
+ 	u64 last_block_end = 0;
+ 	struct scatterlist *sg;
+ 	u64 current_block_end;
+ 	u64 misalignment_bits;
+ 	u64 next_block_start;
+ 	u64 total_len = 0;
+ 	int i;
+ 
+ 	for_each_sg(umem->sg_head.sgl, sg, umem->nmap, i) {
+ 		/*
+ 		 * Initialization - save the first chunk start as the
+ 		 * current_block_start - block means contiguous pages.
+ 		 */
+ 		if (current_block_len == 0 && current_block_start == 0) {
+ 			current_block_start = sg_dma_address(sg);
+ 			first_block_start = current_block_start;
+ 			/*
+ 			 * Find the bits that are different between the physical
+ 			 * address and the virtual address for the start of the
+ 			 * MR.
+ 			 * umem_get aligned the start_va to a page boundary.
+ 			 * Therefore, we need to align the start va to the same
+ 			 * boundary.
+ 			 * misalignment_bits is needed to handle the  case of a
+ 			 * single memory region. In this case, the rest of the
+ 			 * logic will not reduce the block size.  If we use a
+ 			 * block size which is bigger than the alignment of the
+ 			 * misalignment bits, we might use the virtual page
+ 			 * number instead of the physical page number, resulting
+ 			 * in access to the wrong data.
+ 			 */
+ 			misalignment_bits =
+ 			(start_va & (~(((u64)(BIT(umem->page_shift))) - 1ULL)))
+ 			^ current_block_start;
+ 			block_shift = min(alignment_of(misalignment_bits),
+ 					  block_shift);
+ 		}
+ 
+ 		/*
+ 		 * Go over the scatter entries and check if they continue the
+ 		 * previous scatter entry.
+ 		 */
+ 		next_block_start = sg_dma_address(sg);
+ 		current_block_end = current_block_start	+ current_block_len;
+ 		/* If we have a split (non-contig.) between two blocks */
+ 		if (current_block_end != next_block_start) {
+ 			block_shift = mlx4_ib_umem_calc_block_mtt
+ 					(next_block_start,
+ 					 current_block_end,
+ 					 block_shift);
+ 
+ 			/*
+ 			 * If we reached the minimum shift for 4k page we stop
+ 			 * the loop.
+ 			 */
+ 			if (block_shift <= min_shift)
+ 				goto end;
+ 
+ 			/*
+ 			 * If not saved yet we are in first block - we save the
+ 			 * length of first block to calculate the
+ 			 * non_aligned_pages number at the end.
+ 			 */
+ 			total_len += current_block_len;
+ 
+ 			/* Start a new block */
+ 			current_block_start = next_block_start;
+ 			current_block_len = sg_dma_len(sg);
+ 			continue;
+ 		}
+ 		/* The scatter entry is another part of the current block,
+ 		 * increase the block size.
+ 		 * An entry in the scatter can be larger than 4k (page) as of
+ 		 * dma mapping which merge some blocks together.
+ 		 */
+ 		current_block_len += sg_dma_len(sg);
+ 	}
+ 
+ 	/* Account for the last block in the total len */
+ 	total_len += current_block_len;
+ 	/* Add to the first block the misalignment that it suffers from. */
+ 	total_len += (first_block_start & ((1ULL << block_shift) - 1ULL));
+ 	last_block_end = current_block_start + current_block_len;
+ 	last_block_aligned_end = round_up(last_block_end, 1 << block_shift);
+ 	total_len += (last_block_aligned_end - last_block_end);
+ 
+ 	if (total_len & ((1ULL << block_shift) - 1ULL))
+ 		pr_warn("misaligned total length detected (%llu, %llu)!",
+ 			total_len, block_shift);
+ 
+ 	*num_of_mtts = total_len >> block_shift;
+ end:
+ 	if (block_shift < min_shift) {
+ 		/*
+ 		 * If shift is less than the min we set a warning and return the
+ 		 * min shift.
+ 		 */
+ 		pr_warn("umem_calc_optimal_mtt_size - unexpected shift %lld\n", block_shift);
+ 
+ 		block_shift = min_shift;
+ 	}
+ 	return block_shift;
+ }
+ 
++>>>>>>> ed8637d3615b (IB/mlx4: Add contig support for control objects)
  struct ib_mr *mlx4_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
  				  u64 virt_addr, int access_flags,
  				  struct ib_udata *udata)
diff --cc drivers/infiniband/hw/mlx4/qp.c
index e22093ceae68,f807a6278d44..000000000000
--- a/drivers/infiniband/hw/mlx4/qp.c
+++ b/drivers/infiniband/hw/mlx4/qp.c
@@@ -1112,8 -1102,10 +1114,15 @@@ static int create_qp_common(struct mlx4
  			goto err;
  		}
  
++<<<<<<< HEAD
 +		err = mlx4_mtt_init(dev->dev, ib_umem_page_count(qp->umem),
 +				    ilog2(qp->umem->page_size), &qp->mtt);
++=======
+ 		n = ib_umem_page_count(qp->umem);
+ 		shift = mlx4_ib_umem_calc_optimal_mtt_size(qp->umem, 0, &n);
+ 		err = mlx4_mtt_init(dev->dev, n, shift, &qp->mtt);
+ 
++>>>>>>> ed8637d3615b (IB/mlx4: Add contig support for control objects)
  		if (err)
  			goto err_buf;
  
* Unmerged path drivers/infiniband/hw/mlx4/cq.c
diff --git a/drivers/infiniband/hw/mlx4/mlx4_ib.h b/drivers/infiniband/hw/mlx4/mlx4_ib.h
index 3d13dad5ca70..a65f90a7d592 100644
--- a/drivers/infiniband/hw/mlx4/mlx4_ib.h
+++ b/drivers/infiniband/hw/mlx4/mlx4_ib.h
@@ -935,5 +935,7 @@ struct ib_rwq_ind_table
 			      struct ib_rwq_ind_table_init_attr *init_attr,
 			      struct ib_udata *udata);
 int mlx4_ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
+int mlx4_ib_umem_calc_optimal_mtt_size(struct ib_umem *umem, u64 start_va,
+				       int *num_of_mtts);
 
 #endif /* MLX4_IB_H */
* Unmerged path drivers/infiniband/hw/mlx4/mr.c
* Unmerged path drivers/infiniband/hw/mlx4/qp.c
