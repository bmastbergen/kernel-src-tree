blk-mq: add shallow depth option for blk_mq_get_tag()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Omar Sandoval <osandov@fb.com>
commit 229a92873f3afc20b0d91aaaec08cbc11689dd8b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/229a9287.failed

Wire up the sbitmap_get_shallow() operation to the tag code so that a
caller can limit the number of tags available to it.

	Signed-off-by: Omar Sandoval <osandov@fb.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 229a92873f3afc20b0d91aaaec08cbc11689dd8b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-tag.c
diff --cc block/blk-mq-tag.c
index 7e6885bccaac,d0be72ccb091..000000000000
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@@ -144,134 -90,53 +144,145 @@@ static inline bool hctx_may_queue(struc
  	return atomic_read(&hctx->nr_active) < depth;
  }
  
 -static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 -			    struct sbitmap_queue *bt)
 +static int __bt_get_word(struct blk_align_bitmap *bm, unsigned int last_tag)
  {
++<<<<<<< HEAD
 +	int tag, org_last_tag = last_tag;
++=======
+ 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+ 	    !hctx_may_queue(data->hctx, bt))
+ 		return -1;
+ 	if (data->shallow_depth)
+ 		return __sbitmap_queue_get_shallow(bt, data->shallow_depth);
+ 	else
+ 		return __sbitmap_queue_get(bt);
+ }
++>>>>>>> 229a92873f3a (blk-mq: add shallow depth option for blk_mq_get_tag())
 +
 +	while (1) {
 +		tag = find_next_zero_bit(&bm->word, bm->depth, last_tag);
 +		if (unlikely(tag >= bm->depth)) {
 +			/*
 +			 * We started with an offset, and we didn't reset the
 +			 * offset to 0 in a failure case, so start from 0 to
 +			 * exhaust the map.
 +			 */
 +			if (org_last_tag && last_tag) {
 +				last_tag = org_last_tag = 0;
 +				continue;
 +			}
 +			return -1;
 +		}
  
 -unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 +		if (!test_and_set_bit(tag, &bm->word))
 +			break;
 +
 +		last_tag = tag + 1;
 +		if (last_tag >= bm->depth - 1)
 +			last_tag = 0;
 +	}
 +
 +	return tag;
 +}
 +
 +/*
 + * Straight forward bitmap tag implementation, where each bit is a tag
 + * (cleared == free, and set == busy). The small twist is using per-cpu
 + * last_tag caches, which blk-mq stores in the blk_mq_ctx software queue
 + * contexts. This enables us to drastically limit the space searched,
 + * without dirtying an extra shared cacheline like we would if we stored
 + * the cache value inside the shared blk_mq_bitmap_tags structure. On top
 + * of that, each word of tags is in a separate cacheline. This means that
 + * multiple users will tend to stick to different cachelines, at least
 + * until the map is exhausted.
 + */
 +static int __bt_get(struct blk_mq_hw_ctx *hctx, struct blk_mq_bitmap_tags *bt,
 +		    unsigned int *tag_cache)
  {
 -	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 -	struct sbitmap_queue *bt;
 -	struct sbq_wait_state *ws;
 -	DEFINE_WAIT(wait);
 -	unsigned int tag_offset;
 -	bool drop_ctx;
 -	int tag;
 +	unsigned int last_tag, org_last_tag;
 +	int index, i, tag;
 +
 +	if (!hctx_may_queue(hctx, bt))
 +		return -1;
  
 -	if (data->flags & BLK_MQ_REQ_RESERVED) {
 -		if (unlikely(!tags->nr_reserved_tags)) {
 -			WARN_ON_ONCE(1);
 -			return BLK_MQ_TAG_FAIL;
 +	last_tag = org_last_tag = *tag_cache;
 +	index = TAG_TO_INDEX(bt, last_tag);
 +
 +	for (i = 0; i < bt->map_nr; i++) {
 +		tag = __bt_get_word(&bt->map[index], TAG_TO_BIT(bt, last_tag));
 +		if (tag != -1) {
 +			tag += (index << bt->bits_per_word);
 +			goto done;
  		}
 -		bt = &tags->breserved_tags;
 -		tag_offset = 0;
 -	} else {
 -		bt = &tags->bitmap_tags;
 -		tag_offset = tags->nr_reserved_tags;
 +
 +		/*
 +		 * Jump to next index, and reset the last tag to be the
 +		 * first tag of that index
 +		 */
 +		index++;
 +		last_tag = (index << bt->bits_per_word);
 +
 +		if (index >= bt->map_nr) {
 +			index = 0;
 +			last_tag = 0;
 +		}
 +	}
 +
 +	*tag_cache = 0;
 +	return -1;
 +
 +	/*
 +	 * Only update the cache from the allocation path, if we ended
 +	 * up using the specific cached tag.
 +	 */
 +done:
 +	if (tag == org_last_tag) {
 +		last_tag = tag + 1;
 +		if (last_tag >= bt->depth - 1)
 +			last_tag = 0;
 +
 +		*tag_cache = last_tag;
  	}
  
 -	tag = __blk_mq_get_tag(data, bt);
 +	return tag;
 +}
 +
 +static struct bt_wait_state *bt_wait_ptr(struct blk_mq_bitmap_tags *bt,
 +					 struct blk_mq_hw_ctx *hctx)
 +{
 +	struct bt_wait_state *bs;
 +	int wait_index;
 +
 +	if (!hctx)
 +		return &bt->bs[0];
 +
 +	wait_index = atomic_read(&hctx->wait_index);
 +	bs = &bt->bs[wait_index];
 +	bt_index_atomic_inc(&hctx->wait_index);
 +	return bs;
 +}
 +
 +static int bt_get(struct blk_mq_alloc_data *data,
 +		struct blk_mq_bitmap_tags *bt,
 +		struct blk_mq_hw_ctx *hctx,
 +		unsigned int *last_tag)
 +{
 +	struct bt_wait_state *bs;
 +	DEFINE_WAIT(wait);
 +	int tag;
 +
 +	tag = __bt_get(hctx, bt, last_tag);
  	if (tag != -1)
 -		goto found_tag;
 +		return tag;
  
  	if (data->flags & BLK_MQ_REQ_NOWAIT)
 -		return BLK_MQ_TAG_FAIL;
 +		return -1;
  
 -	ws = bt_wait_ptr(bt, data->hctx);
 -	drop_ctx = data->ctx == NULL;
 +	bs = bt_wait_ptr(bt, hctx);
  	do {
 -		prepare_to_wait(&ws->wait, &wait, TASK_UNINTERRUPTIBLE);
 +		prepare_to_wait(&bs->wait, &wait, TASK_UNINTERRUPTIBLE);
  
 -		tag = __blk_mq_get_tag(data, bt);
 +		tag = __bt_get(hctx, bt, last_tag);
  		if (tag != -1)
  			break;
  
* Unmerged path block/blk-mq-tag.c
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 2d50f02667c4..6fe765332bc2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -100,6 +100,7 @@ struct blk_mq_alloc_data {
 	/* input parameter */
 	struct request_queue *q;
 	unsigned int flags;
+	unsigned int shallow_depth;
 
 	/* input & output parameter */
 	struct blk_mq_ctx *ctx;
