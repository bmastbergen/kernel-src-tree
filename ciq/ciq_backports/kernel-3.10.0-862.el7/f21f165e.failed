KVM: VMX: introduce alloc_loaded_vmcs

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit f21f165ef922c2146cc5bdc620f542953c41714b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/f21f165e.failed

Group together the calls to alloc_vmcs and loaded_vmcs_init.  Soon we'll also
allocate an MSR bitmap there.

	Cc: stable@vger.kernel.org       # prereq for Spectre mitigation
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f21f165ef922c2146cc5bdc620f542953c41714b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index 06b6ba430b58,ab4b9bc99a52..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -6758,59 -7152,15 +6769,67 @@@ static int nested_vmx_check_vmptr(struc
  	return 0;
  }
  
 -static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 +/*
 + * Emulate the VMXON instruction.
 + * Currently, we just remember that VMX is active, and do not save or even
 + * inspect the argument to VMXON (the so-called "VMXON pointer") because we
 + * do not currently need to store anything in that guest-allocated memory
 + * region. Consequently, VMCLEAR and VMPTRLD also do not verify that the their
 + * argument is different from the VMXON pointer (which the spec says they do).
 + */
 +static int handle_vmon(struct kvm_vcpu *vcpu)
  {
 +	struct kvm_segment cs;
  	struct vcpu_vmx *vmx = to_vmx(vcpu);
  	struct vmcs *shadow_vmcs;
++<<<<<<< HEAD
 +	const u64 VMXON_NEEDED_FEATURES = FEATURE_CONTROL_LOCKED
 +		| FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;
 +
 +	/* The Intel VMX Instruction Reference lists a bunch of bits that
 +	 * are prerequisite to running VMXON, most notably cr4.VMXE must be
 +	 * set to 1 (see vmx_set_cr4() for when we allow the guest to set this).
 +	 * Otherwise, we should fail with #UD. We test these now:
 +	 */
 +	if (!kvm_read_cr4_bits(vcpu, X86_CR4_VMXE) ||
 +	    !kvm_read_cr0_bits(vcpu, X86_CR0_PE) ||
 +	    (vmx_get_rflags(vcpu) & X86_EFLAGS_VM)) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	vmx_get_segment(vcpu, &cs, VCPU_SREG_CS);
 +	if (is_long_mode(vcpu) && !cs.l) {
 +		kvm_queue_exception(vcpu, UD_VECTOR);
 +		return 1;
 +	}
 +
 +	if (vmx_get_cpl(vcpu)) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
 +
 +	if (nested_vmx_check_vmptr(vcpu, EXIT_REASON_VMON, NULL))
 +		return 1;
 +
 +	if (vmx->nested.vmxon) {
 +		nested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
 +		skip_emulated_instruction(vcpu);
 +		return 1;
 +	}
 +
 +	if ((vmx->msr_ia32_feature_control & VMXON_NEEDED_FEATURES)
 +			!= VMXON_NEEDED_FEATURES) {
 +		kvm_inject_gp(vcpu, 0);
 +		return 1;
 +	}
++=======
+ 	int r;
+ 
+ 	r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
+ 	if (r < 0)
+ 		goto out_vmcs02;
++>>>>>>> f21f165ef922 (KVM: VMX: introduce alloc_loaded_vmcs)
  
  	if (cpu_has_vmx_msr_bitmap()) {
  		vmx->nested.msr_bitmap =
@@@ -8903,17 -9555,11 +8922,19 @@@ static struct kvm_vcpu *vmx_create_vcpu
  	if (!vmx->guest_msrs)
  		goto free_pml;
  
- 	vmx->loaded_vmcs = &vmx->vmcs01;
- 	vmx->loaded_vmcs->vmcs = alloc_vmcs();
- 	vmx->loaded_vmcs->shadow_vmcs = NULL;
- 	if (!vmx->loaded_vmcs->vmcs)
+ 	err = alloc_loaded_vmcs(&vmx->vmcs01);
+ 	if (err < 0)
  		goto free_msrs;
++<<<<<<< HEAD
 +	if (!vmm_exclusive)
 +		kvm_cpu_vmxon(__pa(per_cpu(vmxarea, raw_smp_processor_id())));
 +	loaded_vmcs_init(vmx->loaded_vmcs);
 +	if (!vmm_exclusive)
 +		kvm_cpu_vmxoff();
++=======
++>>>>>>> f21f165ef922 (KVM: VMX: introduce alloc_loaded_vmcs)
  
+ 	vmx->loaded_vmcs = &vmx->vmcs01;
  	cpu = get_cpu();
  	vmx_vcpu_load(&vmx->vcpu, cpu);
  	vmx->vcpu.cpu = cpu;
* Unmerged path arch/x86/kvm/vmx.c
