mlx4: use order-0 pages for RX

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Eric Dumazet <edumazet@google.com>
commit b5a54d9a313645ec9607dc557b67d9325c28884c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/b5a54d9a.failed

Use of order-3 pages is problematic in some cases.

This patch might add three kinds of regression :

1) a CPU performance regression, but we will add later page
recycling and performance should be back.

2) TCP receiver could grow its receive window slightly slower,
   because skb->len/skb->truesize ratio will decrease.
   This is mostly ok, we prefer being conservative to not risk OOM,
   and eventually tune TCP better in the future.
   This is consistent with other drivers using 2048 per ethernet frame.

3) Because we allocate one page per RX slot, we consume more
   memory for the ring buffers. XDP already had this constraint anyway.

	Signed-off-by: Eric Dumazet <edumazet@google.com>
	Acked-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit b5a54d9a313645ec9607dc557b67d9325c28884c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx4/en_rx.c
#	drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
diff --cc drivers/net/ethernet/mellanox/mlx4/en_rx.c
index 984f22166c89,069ea09185fb..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
@@@ -56,21 -58,10 +56,27 @@@ static int mlx4_alloc_pages(struct mlx4
  	struct page *page;
  	dma_addr_t dma;
  
++<<<<<<< HEAD
 +	for (order = frag_info->order; ;) {
 +		gfp_t gfp = _gfp;
 +
 +		if (order)
 +			gfp |= __GFP_COMP | __GFP_NOWARN | __GFP_NOMEMALLOC;
 +		page = alloc_pages(gfp, order);
 +		if (likely(page))
 +			break;
 +		if (--order < 0 ||
 +		    ((PAGE_SIZE << order) < frag_info->frag_size))
 +			return -ENOMEM;
 +	}
 +	dma = dma_map_page(priv->ddev, page, 0, PAGE_SIZE << order,
 +			   frag_info->dma_dir);
++=======
+ 	page = alloc_page(gfp);
+ 	if (unlikely(!page))
+ 		return -ENOMEM;
+ 	dma = dma_map_page(priv->ddev, page, 0, PAGE_SIZE, priv->dma_dir);
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  	if (unlikely(dma_mapping_error(priv->ddev, dma))) {
  		put_page(page);
  		return -ENOMEM;
@@@ -125,11 -115,10 +130,15 @@@ out
  	while (i--) {
  		if (page_alloc[i].page != ring_alloc[i].page) {
  			dma_unmap_page(priv->ddev, page_alloc[i].dma,
++<<<<<<< HEAD
 +				page_alloc[i].page_size,
 +				priv->frag_info[i].dma_dir);
++=======
+ 				       PAGE_SIZE, priv->dma_dir);
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  			page = page_alloc[i].page;
  			/* Revert changes done by mlx4_alloc_pages */
- 			page_ref_sub(page, page_alloc[i].page_size /
+ 			page_ref_sub(page, PAGE_SIZE /
  					   priv->frag_info[i].frag_stride - 1);
  			put_page(page);
  		}
@@@ -144,9 -133,10 +153,16 @@@ static void mlx4_en_free_frag(struct ml
  	const struct mlx4_en_frag_info *frag_info = &priv->frag_info[i];
  	u32 next_frag_end = frags[i].page_offset + 2 * frag_info->frag_stride;
  
++<<<<<<< HEAD
 +	if (next_frag_end > frags[i].page_size)
 +		dma_unmap_page(priv->ddev, frags[i].dma, frags[i].page_size,
 +			       frag_info->dma_dir);
++=======
+ 
+ 	if (next_frag_end > PAGE_SIZE)
+ 		dma_unmap_page(priv->ddev, frags[i].dma, PAGE_SIZE,
+ 			       priv->dma_dir);
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  
  	if (frags[i].page)
  		put_page(frags[i].page);
@@@ -177,11 -166,10 +192,15 @@@ out
  
  		page_alloc = &ring->page_alloc[i];
  		dma_unmap_page(priv->ddev, page_alloc->dma,
++<<<<<<< HEAD
 +			       page_alloc->page_size,
 +			       priv->frag_info[i].dma_dir);
++=======
+ 			       PAGE_SIZE, priv->dma_dir);
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  		page = page_alloc->page;
  		/* Revert changes done by mlx4_alloc_pages */
- 		page_ref_sub(page, page_alloc->page_size /
+ 		page_ref_sub(page, PAGE_SIZE /
  				   priv->frag_info[i].frag_stride - 1);
  		put_page(page);
  		page_alloc->page = NULL;
@@@ -203,9 -191,9 +222,13 @@@ static void mlx4_en_destroy_allocator(s
  		       i, page_count(page_alloc->page));
  
  		dma_unmap_page(priv->ddev, page_alloc->dma,
++<<<<<<< HEAD
 +				page_alloc->page_size, frag_info->dma_dir);
++=======
+ 			       PAGE_SIZE, priv->dma_dir);
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  		while (page_alloc->page_offset + frag_info->frag_stride <
- 		       page_alloc->page_size) {
+ 		       PAGE_SIZE) {
  			put_page(page_alloc->page);
  			page_alloc->page_offset += frag_info->frag_stride;
  		}
@@@ -1129,37 -1166,53 +1152,81 @@@ int mlx4_en_poll_rx_cq(struct napi_stru
  	return done;
  }
  
 +static const int frag_sizes[] = {
 +	FRAG_SZ0,
 +	FRAG_SZ1,
 +	FRAG_SZ2,
 +	FRAG_SZ3
 +};
 +
  void mlx4_en_calc_rx_buf(struct net_device *dev)
  {
 +	enum dma_data_direction dma_dir = PCI_DMA_FROMDEVICE;
  	struct mlx4_en_priv *priv = netdev_priv(dev);
 -	int eff_mtu = MLX4_EN_EFF_MTU(dev->mtu);
 +	/* VLAN_HLEN is added twice,to support skb vlan tagged with multiple
 +	 * headers. (For example: ETH_P_8021Q and ETH_P_8021AD).
 +	 */
 +	int eff_mtu = dev->mtu + ETH_HLEN + (2 * VLAN_HLEN);
 +	int order = MLX4_EN_ALLOC_PREFER_ORDER;
 +	u32 align = SMP_CACHE_BYTES;
 +	int buf_size = 0;
  	int i = 0;
  
++<<<<<<< HEAD
 +	while (buf_size < eff_mtu) {
 +		priv->frag_info[i].order = order;
 +		priv->frag_info[i].frag_size =
 +			(eff_mtu > buf_size + frag_sizes[i]) ?
 +				frag_sizes[i] : eff_mtu - buf_size;
 +		priv->frag_info[i].frag_prefix_size = buf_size;
 +		priv->frag_info[i].frag_stride =
 +				ALIGN(priv->frag_info[i].frag_size, align);
 +		priv->frag_info[i].dma_dir = dma_dir;
 +		buf_size += priv->frag_info[i].frag_size;
 +		i++;
++=======
+ 	/* bpf requires buffers to be set up as 1 packet per page.
+ 	 * This only works when num_frags == 1.
+ 	 */
+ 	if (priv->tx_ring_num[TX_XDP]) {
+ 		priv->frag_info[0].frag_size = eff_mtu;
+ 		/* This will gain efficient xdp frame recycling at the
+ 		 * expense of more costly truesize accounting
+ 		 */
+ 		priv->frag_info[0].frag_stride = PAGE_SIZE;
+ 		priv->dma_dir = PCI_DMA_BIDIRECTIONAL;
+ 		priv->rx_headroom = XDP_PACKET_HEADROOM;
+ 		i = 1;
+ 	} else {
+ 		int frag_size_max = 2048, buf_size = 0;
+ 
+ 		/* should not happen, right ? */
+ 		if (eff_mtu > PAGE_SIZE + (MLX4_EN_MAX_RX_FRAGS - 1) * 2048)
+ 			frag_size_max = PAGE_SIZE;
+ 
+ 		while (buf_size < eff_mtu) {
+ 			int frag_stride, frag_size = eff_mtu - buf_size;
+ 			int pad, nb;
+ 
+ 			if (i < MLX4_EN_MAX_RX_FRAGS - 1)
+ 				frag_size = min(frag_size, frag_size_max);
+ 
+ 			priv->frag_info[i].frag_size = frag_size;
+ 			frag_stride = ALIGN(frag_size, SMP_CACHE_BYTES);
+ 			/* We can only pack 2 1536-bytes frames in on 4K page
+ 			 * Therefore, each frame would consume more bytes (truesize)
+ 			 */
+ 			nb = PAGE_SIZE / frag_stride;
+ 			pad = (PAGE_SIZE - nb * frag_stride) / nb;
+ 			pad &= ~(SMP_CACHE_BYTES - 1);
+ 			priv->frag_info[i].frag_stride = frag_stride + pad;
+ 
+ 			buf_size += frag_size;
+ 			i++;
+ 		}
+ 		priv->dma_dir = PCI_DMA_FROMDEVICE;
+ 		priv->rx_headroom = 0;
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  	}
  
  	priv->num_frags = i;
diff --cc drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
index d8f46d99701e,6c80117006ed..000000000000
--- a/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
+++ b/drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
@@@ -102,24 -102,8 +102,27 @@@
  /* Use the maximum between 16384 and a single page */
  #define MLX4_EN_ALLOC_SIZE	PAGE_ALIGN(16384)
  
++<<<<<<< HEAD
 +#define MLX4_EN_ALLOC_PREFER_ORDER min_t(int, get_order(32768),		\
 +					 PAGE_ALLOC_COSTLY_ORDER)
 +
 +/* Receive fragment sizes; we use at most 3 fragments (for 9600 byte MTU
 + * and 4K allocations) */
 +enum {
 +	FRAG_SZ0 = 1536 - NET_IP_ALIGN,
 +	FRAG_SZ1 = 4096,
 +	FRAG_SZ2 = 4096,
 +	FRAG_SZ3 = MLX4_EN_ALLOC_SIZE
 +};
++=======
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  #define MLX4_EN_MAX_RX_FRAGS	4
  
 +#ifndef CONFIG_GENERIC_HARDIRQS
 +/* Minimum packet number till arming the CQ */
 +#define MLX4_EN_MIN_RX_ARM	2097152
 +#endif
 +
  /* Maximum ring sizes */
  #define MLX4_EN_MAX_TX_SIZE	8192
  #define MLX4_EN_MAX_RX_SIZE	8192
@@@ -583,8 -572,10 +585,15 @@@ struct mlx4_en_priv 
  	u32 rx_ring_num;
  	u32 rx_skb_size;
  	struct mlx4_en_frag_info frag_info[MLX4_EN_MAX_RX_FRAGS];
++<<<<<<< HEAD
 +	u16 num_frags;
 +	u16 log_rx_info;
++=======
+ 	u8 num_frags;
+ 	u8 log_rx_info;
+ 	u8 dma_dir;
+ 	u16 rx_headroom;
++>>>>>>> b5a54d9a3136 (mlx4: use order-0 pages for RX)
  
  	struct mlx4_en_tx_ring **tx_ring[MLX4_EN_NUM_TX_TYPES];
  	struct mlx4_en_rx_ring *rx_ring[MAX_RX_RINGS];
* Unmerged path drivers/net/ethernet/mellanox/mlx4/en_rx.c
* Unmerged path drivers/net/ethernet/mellanox/mlx4/mlx4_en.h
