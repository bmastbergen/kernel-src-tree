x86/intel_rdt: Cleanup namespace to support RDT monitoring

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [x86] intel_rdt: Cleanup namespace to support RDT monitoring (Jiri Olsa) [1457533]
Rebuild_FUZZ: 96.43%
commit-author Vikas Shivappa <vikas.shivappa@linux.intel.com>
commit 1b5c0b7583173b787b5c93ff89838a950d0e23ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1b5c0b75.failed

Few of the data-structures have generic names although they are RDT
allocation specific. Rename them to be allocation specific to
accommodate RDT monitoring. E.g. s/enabled/alloc_enabled/

No functional change.

	Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: ravi.v.shankar@intel.com
	Cc: tony.luck@intel.com
	Cc: fenghua.yu@intel.com
	Cc: peterz@infradead.org
	Cc: eranian@google.com
	Cc: vikas.shivappa@intel.com
	Cc: ak@linux.intel.com
	Cc: davidcc@google.com
	Cc: reinette.chatre@intel.com
Link: http://lkml.kernel.org/r/1501017287-28083-7-git-send-email-vikas.shivappa@linux.intel.com

(cherry picked from commit 1b5c0b7583173b787b5c93ff89838a950d0e23ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/intel_rdt.h
#	arch/x86/include/asm/intel_rdt_sched.h
#	arch/x86/kernel/cpu/intel_rdt.c
#	arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
#	arch/x86/kernel/cpu/intel_rdt_schemata.c
diff --cc arch/x86/include/asm/intel_rdt.h
index 06f50d0ed14f,29630af30c38..000000000000
--- a/arch/x86/include/asm/intel_rdt.h
+++ b/arch/x86/include/asm/intel_rdt.h
@@@ -136,11 -100,88 +136,95 @@@ struct msr_param 
  	int			high;
  };
  
++<<<<<<< HEAD:arch/x86/include/asm/intel_rdt.h
++=======
+ /**
+  * struct rdt_cache - Cache allocation related data
+  * @cbm_len:		Length of the cache bit mask
+  * @min_cbm_bits:	Minimum number of consecutive bits to be set
+  * @cbm_idx_mult:	Multiplier of CBM index
+  * @cbm_idx_offset:	Offset of CBM index. CBM index is computed by:
+  *			closid * cbm_idx_multi + cbm_idx_offset
+  *			in a cache bit mask
+  */
+ struct rdt_cache {
+ 	unsigned int	cbm_len;
+ 	unsigned int	min_cbm_bits;
+ 	unsigned int	cbm_idx_mult;
+ 	unsigned int	cbm_idx_offset;
+ };
+ 
+ /**
+  * struct rdt_membw - Memory bandwidth allocation related data
+  * @max_delay:		Max throttle delay. Delay is the hardware
+  *			representation for memory bandwidth.
+  * @min_bw:		Minimum memory bandwidth percentage user can request
+  * @bw_gran:		Granularity at which the memory bandwidth is allocated
+  * @delay_linear:	True if memory B/W delay is in linear scale
+  * @mb_map:		Mapping of memory B/W percentage to memory B/W delay
+  */
+ struct rdt_membw {
+ 	u32		max_delay;
+ 	u32		min_bw;
+ 	u32		bw_gran;
+ 	u32		delay_linear;
+ 	u32		*mb_map;
+ };
+ 
+ /**
+  * struct rdt_resource - attributes of an RDT resource
+  * @alloc_enabled:	Is allocation enabled on this machine
+  * @alloc_capable:	Is allocation available on this machine
+  * @name:		Name to use in "schemata" file
+  * @num_closid:		Number of CLOSIDs available
+  * @cache_level:	Which cache level defines scope of this resource
+  * @default_ctrl:	Specifies default cache cbm or memory B/W percent.
+  * @msr_base:		Base MSR address for CBMs
+  * @msr_update:		Function pointer to update QOS MSRs
+  * @data_width:		Character width of data when displaying
+  * @domains:		All domains for this resource
+  * @cache:		Cache allocation related data
+  * @info_files:		resctrl info files for the resource
+  * @nr_info_files:	Number of info files
+  * @format_str:		Per resource format string to show domain value
+  * @parse_ctrlval:	Per resource function pointer to parse control values
+  */
+ struct rdt_resource {
+ 	bool			alloc_enabled;
+ 	bool			alloc_capable;
+ 	char			*name;
+ 	int			num_closid;
+ 	int			cache_level;
+ 	u32			default_ctrl;
+ 	unsigned int		msr_base;
+ 	void (*msr_update)	(struct rdt_domain *d, struct msr_param *m,
+ 				 struct rdt_resource *r);
+ 	int			data_width;
+ 	struct list_head	domains;
+ 	struct rdt_cache	cache;
+ 	struct rdt_membw	membw;
+ 	struct rftype		*info_files;
+ 	int			nr_info_files;
+ 	const char		*format_str;
+ 	int (*parse_ctrlval)	(char *buf, struct rdt_resource *r,
+ 				 struct rdt_domain *d);
+ };
+ 
+ void rdt_get_cache_infofile(struct rdt_resource *r);
+ void rdt_get_mba_infofile(struct rdt_resource *r);
+ int parse_cbm(char *buf, struct rdt_resource *r, struct rdt_domain *d);
+ int parse_bw(char *buf, struct rdt_resource *r,  struct rdt_domain *d);
+ 
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring):arch/x86/kernel/cpu/intel_rdt.h
  extern struct mutex rdtgroup_mutex;
  
  extern struct rdt_resource rdt_resources_all[];
  extern struct rdtgroup rdtgroup_default;
++<<<<<<< HEAD:arch/x86/include/asm/intel_rdt.h
 +extern struct static_key rdt_enable_key;
++=======
+ DECLARE_STATIC_KEY_FALSE(rdt_alloc_enable_key);
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring):arch/x86/kernel/cpu/intel_rdt.h
  
  int __init rdtgroup_init(void);
  
diff --cc arch/x86/kernel/cpu/intel_rdt.c
index ad087dd4421e,835e1ff8449f..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt.c
+++ b/arch/x86/kernel/cpu/intel_rdt.c
@@@ -124,11 -170,11 +124,19 @@@ static inline bool cache_alloc_hsw_prob
  			return false;
  
  		r->num_closid = 4;
++<<<<<<< HEAD
 +		r->cbm_len = 20;
 +		r->max_cbm = max_cbm;
 +		r->min_cbm_bits = 2;
 +		r->capable = true;
 +		r->enabled = true;
++=======
+ 		r->default_ctrl = max_cbm;
+ 		r->cache.cbm_len = 20;
+ 		r->cache.min_cbm_bits = 2;
+ 		r->alloc_capable = true;
+ 		r->alloc_enabled = true;
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  
  		return true;
  	}
@@@ -136,19 -182,68 +144,80 @@@
  	return false;
  }
  
++<<<<<<< HEAD
 +static void rdt_get_config(int idx, struct rdt_resource *r)
++=======
+ /*
+  * rdt_get_mb_table() - get a mapping of bandwidth(b/w) percentage values
+  * exposed to user interface and the h/w understandable delay values.
+  *
+  * The non-linear delay values have the granularity of power of two
+  * and also the h/w does not guarantee a curve for configured delay
+  * values vs. actual b/w enforced.
+  * Hence we need a mapping that is pre calibrated so the user can
+  * express the memory b/w as a percentage value.
+  */
+ static inline bool rdt_get_mb_table(struct rdt_resource *r)
+ {
+ 	/*
+ 	 * There are no Intel SKUs as of now to support non-linear delay.
+ 	 */
+ 	pr_info("MBA b/w map not implemented for cpu:%d, model:%d",
+ 		boot_cpu_data.x86, boot_cpu_data.x86_model);
+ 
+ 	return false;
+ }
+ 
+ static bool rdt_get_mem_config(struct rdt_resource *r)
+ {
+ 	union cpuid_0x10_3_eax eax;
+ 	union cpuid_0x10_x_edx edx;
+ 	u32 ebx, ecx;
+ 
+ 	cpuid_count(0x00000010, 3, &eax.full, &ebx, &ecx, &edx.full);
+ 	r->num_closid = edx.split.cos_max + 1;
+ 	r->membw.max_delay = eax.split.max_delay + 1;
+ 	r->default_ctrl = MAX_MBA_BW;
+ 	if (ecx & MBA_IS_LINEAR) {
+ 		r->membw.delay_linear = true;
+ 		r->membw.min_bw = MAX_MBA_BW - r->membw.max_delay;
+ 		r->membw.bw_gran = MAX_MBA_BW - r->membw.max_delay;
+ 	} else {
+ 		if (!rdt_get_mb_table(r))
+ 			return false;
+ 	}
+ 	r->data_width = 3;
+ 	rdt_get_mba_infofile(r);
+ 
+ 	r->alloc_capable = true;
+ 	r->alloc_enabled = true;
+ 
+ 	return true;
+ }
+ 
+ static void rdt_get_cache_config(int idx, struct rdt_resource *r)
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  {
  	union cpuid_0x10_1_eax eax;
 -	union cpuid_0x10_x_edx edx;
 +	union cpuid_0x10_1_edx edx;
  	u32 ebx, ecx;
  
  	cpuid_count(0x00000010, idx, &eax.full, &ebx, &ecx, &edx.full);
  	r->num_closid = edx.split.cos_max + 1;
++<<<<<<< HEAD
 +	r->cbm_len = eax.split.cbm_len + 1;
 +	r->max_cbm = BIT_MASK(eax.split.cbm_len + 1) - 1;
 +	r->data_width = (r->cbm_len + 3) / 4;
 +	r->capable = true;
 +	r->enabled = true;
++=======
+ 	r->cache.cbm_len = eax.split.cbm_len + 1;
+ 	r->default_ctrl = BIT_MASK(eax.split.cbm_len + 1) - 1;
+ 	r->data_width = (r->cache.cbm_len + 3) / 4;
+ 	rdt_get_cache_infofile(r);
+ 	r->alloc_capable = true;
+ 	r->alloc_enabled = true;
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  }
  
  static void rdt_get_cdp_l3_config(int type)
@@@ -157,76 -252,68 +226,83 @@@
  	struct rdt_resource *r = &rdt_resources_all[type];
  
  	r->num_closid = r_l3->num_closid / 2;
++<<<<<<< HEAD
 +	r->cbm_len = r_l3->cbm_len;
 +	r->max_cbm = r_l3->max_cbm;
 +	r->data_width = (r->cbm_len + 3) / 4;
 +	r->capable = true;
++=======
+ 	r->cache.cbm_len = r_l3->cache.cbm_len;
+ 	r->default_ctrl = r_l3->default_ctrl;
+ 	r->data_width = (r->cache.cbm_len + 3) / 4;
+ 	r->alloc_capable = true;
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  	/*
  	 * By default, CDP is disabled. CDP can be enabled by mount parameter
  	 * "cdp" during resctrl file system mount time.
  	 */
- 	r->enabled = false;
+ 	r->alloc_enabled = false;
  }
  
 -static int get_cache_id(int cpu, int level)
 +/**
 + * Choose a width for the resource name
 + * and resource data based on the resource that has
 + * widest name and cbm.
 + */
 +static void rdt_init_padding(void)
  {
 -	struct cpu_cacheinfo *ci = get_cpu_cacheinfo(cpu);
 -	int i;
 +	struct rdt_resource *r;
 +	int cl;
  
 -	for (i = 0; i < ci->num_leaves; i++) {
 -		if (ci->info_list[i].level == level)
 -			return ci->info_list[i].id;
 -	}
 +	for_each_enabled_rdt_resource(r) {
 +		cl = strlen(r->name);
 +		if (cl > max_name_width)
 +			max_name_width = cl;
  
 -	return -1;
 +		if (r->data_width > max_data_width)
 +			max_data_width = r->data_width;
 +	}
  }
  
 -/*
 - * Map the memory b/w percentage value to delay values
 - * that can be written to QOS_MSRs.
 - * There are currently no SKUs which support non linear delay values.
 - */
 -static u32 delay_bw_map(unsigned long bw, struct rdt_resource *r)
 +static inline bool get_rdt_resources(void)
  {
 -	if (r->membw.delay_linear)
 -		return MAX_MBA_BW - bw;
 +	bool ret = false;
  
 -	pr_warn_once("Non Linear delay-bw map not supported but queried\n");
 -	return r->default_ctrl;
 -}
 +	if (cache_alloc_hsw_probe())
 +		return true;
  
 -static void
 -mba_wrmsr(struct rdt_domain *d, struct msr_param *m, struct rdt_resource *r)
 -{
 -	unsigned int i;
 +	if (!boot_cpu_has(X86_FEATURE_RDT_A))
 +		return false;
 +
 +	if (boot_cpu_has(X86_FEATURE_CAT_L3)) {
 +		rdt_get_config(1, &rdt_resources_all[RDT_RESOURCE_L3]);
 +		if (boot_cpu_has(X86_FEATURE_CDP_L3)) {
 +			rdt_get_cdp_l3_config(RDT_RESOURCE_L3DATA);
 +			rdt_get_cdp_l3_config(RDT_RESOURCE_L3CODE);
 +		}
 +		ret = true;
 +	}
 +	if (boot_cpu_has(X86_FEATURE_CAT_L2)) {
 +		/* CPUID 0x10.2 fields are same format at 0x10.1 */
 +		rdt_get_config(2, &rdt_resources_all[RDT_RESOURCE_L2]);
 +		ret = true;
 +	}
 +
 +	rdt_init_padding();
  
 -	/*  Write the delay values for mba. */
 -	for (i = m->low; i < m->high; i++)
 -		wrmsrl(r->msr_base + i, delay_bw_map(d->ctrl_val[i], r));
 +	return ret;
  }
  
 -static void
 -cat_wrmsr(struct rdt_domain *d, struct msr_param *m, struct rdt_resource *r)
 +static int get_cache_id(int cpu, int level)
  {
 -	unsigned int i;
 -
 -	for (i = m->low; i < m->high; i++)
 -		wrmsrl(r->msr_base + cbm_idx(r, i), d->ctrl_val[i]);
 +	return get_cpu_cache_id(cpu, level);
  }
  
 -void rdt_ctrl_update(void *arg)
 +void rdt_cbm_update(void *arg)
  {
 -	struct msr_param *m = arg;
 +	struct msr_param *m = (struct msr_param *)arg;
  	struct rdt_resource *r = m->res;
 -	int cpu = smp_processor_id();
 +	int i, cpu = smp_processor_id();
  	struct rdt_domain *d;
  
  	list_for_each_entry(d, &r->domains, list) {
@@@ -316,8 -422,7 +392,12 @@@ static void domain_add_cpu(int cpu, str
  
  	d->id = id;
  
++<<<<<<< HEAD
 +	d->cbm = kmalloc_array(r->num_closid, sizeof(*d->cbm), GFP_KERNEL);
 +	if (!d->cbm) {
++=======
+ 	if (r->alloc_capable && domain_setup_ctrlval(r, d)) {
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  		kfree(d);
  		return;
  	}
@@@ -367,14 -464,11 +447,19 @@@ static int intel_rdt_online_cpu(unsigne
  	struct rdt_resource *r;
  
  	mutex_lock(&rdtgroup_mutex);
++<<<<<<< HEAD
 +	for_each_capable_rdt_resource(r)
 +		domain_add_cpu(cpu, r, notifier);
 +
++=======
+ 	for_each_alloc_capable_rdt_resource(r)
+ 		domain_add_cpu(cpu, r);
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  	/* The cpu is set in default rdtgroup after online. */
  	cpumask_set_cpu(cpu, &rdtgroup_default.cpu_mask);
 -	clear_closid(cpu);
 +	if (notifier)
 +		clear_closid(cpu);
 +
  	mutex_unlock(&rdtgroup_mutex);
  
  	return 0;
@@@ -398,68 -492,55 +483,75 @@@ static int intel_rdt_offline_cpu(unsign
  	return 0;
  }
  
 -/*
 - * Choose a width for the resource name and resource data based on the
 - * resource that has widest name and cbm.
 - */
 -static __init void rdt_init_padding(void)
 +static int
 +rdt_cpu_notify(struct notifier_block *self, unsigned long action, void *hcpu)
  {
 -	struct rdt_resource *r;
 -	int cl;
 +       unsigned int cpu = (long)hcpu;
  
 -	for_each_alloc_capable_rdt_resource(r) {
 -		cl = strlen(r->name);
 -		if (cl > max_name_width)
 -			max_name_width = cl;
 +       switch (action & ~CPU_TASKS_FROZEN) {
  
 -		if (r->data_width > max_data_width)
 -			max_data_width = r->data_width;
 -	}
 +       case CPU_ONLINE:
 +       case CPU_DOWN_FAILED:
 +               intel_rdt_online_cpu(cpu, true);
 +               break;
 +
 +       case CPU_UP_CANCELED:
 +       case CPU_DOWN_PREPARE:
 +               intel_rdt_offline_cpu(cpu);
 +               break;
 +       default:
 +               break;
 +       }
 +
 +       return NOTIFY_OK;
  }
  
 -static __init bool get_rdt_resources(void)
 +static void __init rdt_cpu_setup(void *dummy)
  {
 -	bool ret = false;
 +	struct rdt_resource *r;
 +	int i;
  
 -	if (cache_alloc_hsw_probe())
 -		return true;
 +	clear_closid(smp_processor_id());
  
 -	if (!boot_cpu_has(X86_FEATURE_RDT_A))
 -		return false;
++<<<<<<< HEAD
 +	for_each_capable_rdt_resource(r) {
 +		for (i = 0; i < r->num_closid; i++) {
 +			int idx = cbm_idx(r, i);
++=======
++	for_each_alloc_capable_rdt_resource(r) {
++		cl = strlen(r->name);
++		if (cl > max_name_width)
++			max_name_width = cl;
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  
 -	if (boot_cpu_has(X86_FEATURE_CAT_L3)) {
 -		rdt_get_cache_config(1, &rdt_resources_all[RDT_RESOURCE_L3]);
 -		if (boot_cpu_has(X86_FEATURE_CDP_L3)) {
 -			rdt_get_cdp_l3_config(RDT_RESOURCE_L3DATA);
 -			rdt_get_cdp_l3_config(RDT_RESOURCE_L3CODE);
 +			wrmsrl(r->msr_base + idx, r->max_cbm);
  		}
 -		ret = true;
 -	}
 -	if (boot_cpu_has(X86_FEATURE_CAT_L2)) {
 -		/* CPUID 0x10.2 fields are same format at 0x10.1 */
 -		rdt_get_cache_config(2, &rdt_resources_all[RDT_RESOURCE_L2]);
 -		ret = true;
  	}
 +}
  
 -	if (boot_cpu_has(X86_FEATURE_MBA)) {
 -		if (rdt_get_mem_config(&rdt_resources_all[RDT_RESOURCE_MBA]))
 -			ret = true;
 +static struct notifier_block rdt_cpu_nb = {
 +	.notifier_call  = rdt_cpu_notify,
 +	.priority	= -INT_MAX,
 +};
 +
 +static int __init rdt_notifier_init(void)
 +{
 +	unsigned int cpu;
 +
 +	for_each_online_cpu(cpu) {
 +		intel_rdt_online_cpu(cpu, false);
 +		/*
 +		 * RHEL7 - The upstream hotplug notification invokes the
 +		 *         callbacks on related cpus, but that's not the
 +		 *         case of the RHEL7 notification support.
 +		 *         Following call ensures we run all the msr
 +		 *         initialization setup on related cpus.
 +		 */
 +		smp_call_function_single(cpu, rdt_cpu_setup, NULL, 1);
  	}
  
 -	return ret;
 +	__register_cpu_notifier(&rdt_cpu_nb);
 +	return 0;
  }
  
  static int __init intel_rdt_late_init(void)
diff --cc arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
index 1c3603d97e9d,ce63d1011e03..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
+++ b/arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
@@@ -31,11 -32,11 +31,16 @@@
  
  #include <uapi/linux/magic.h>
  
 -#include <asm/intel_rdt_sched.h>
 -#include "intel_rdt.h"
 +#include <asm/intel_rdt.h>
 +#include <asm/intel_rdt_common.h>
  
++<<<<<<< HEAD
 +struct static_key rdt_enable_key;
 +struct kernfs_root *rdt_root;
++=======
+ DEFINE_STATIC_KEY_FALSE(rdt_alloc_enable_key);
+ static struct kernfs_root *rdt_root;
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  struct rdtgroup rdtgroup_default;
  LIST_HEAD(rdt_all_groups);
  
@@@ -765,7 -835,7 +771,11 @@@ static struct dentry *rdt_mount(struct 
  	/*
  	 * resctrl file system can only be mounted once.
  	 */
++<<<<<<< HEAD
 +	if (static_key_false(&rdt_enable_key)) {
++=======
+ 	if (static_branch_unlikely(&rdt_alloc_enable_key)) {
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  		dentry = ERR_PTR(-EBUSY);
  		goto out;
  	}
@@@ -789,7 -859,7 +799,11 @@@
  	if (IS_ERR(dentry))
  		goto out_destroy;
  
++<<<<<<< HEAD
 +	static_key_slow_inc(&rdt_enable_key);
++=======
+ 	static_branch_enable(&rdt_alloc_enable_key);
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  	goto out;
  
  out_destroy:
@@@ -917,11 -987,11 +931,19 @@@ static void rdt_kill_sb(struct super_bl
  	mutex_lock(&rdtgroup_mutex);
  
  	/*Put everything back to default values. */
++<<<<<<< HEAD
 +	for_each_enabled_rdt_resource(r)
 +		reset_all_cbms(r);
 +	cdp_disable();
 +	rmdir_all_sub();
 +	static_key_slow_dec(&rdt_enable_key);
++=======
+ 	for_each_alloc_enabled_rdt_resource(r)
+ 		reset_all_ctrls(r);
+ 	cdp_disable();
+ 	rmdir_all_sub();
+ 	static_branch_disable(&rdt_alloc_enable_key);
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  	kernfs_kill_sb(sb);
  	mutex_unlock(&rdtgroup_mutex);
  }
diff --cc arch/x86/kernel/cpu/intel_rdt_schemata.c
index 6efa856bc635,952156c2688d..000000000000
--- a/arch/x86/kernel/cpu/intel_rdt_schemata.c
+++ b/arch/x86/kernel/cpu/intel_rdt_schemata.c
@@@ -141,6 -188,17 +141,20 @@@ done
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static int rdtgroup_parse_resource(char *resname, char *tok, int closid)
+ {
+ 	struct rdt_resource *r;
+ 
+ 	for_each_alloc_enabled_rdt_resource(r) {
+ 		if (!strcmp(resname, r->name) && closid < r->num_closid)
+ 			return parse_line(tok, r);
+ 	}
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  ssize_t rdtgroup_schemata_write(struct kernfs_open_file *of,
  				char *buf, size_t nbytes, loff_t off)
  {
@@@ -163,9 -221,10 +177,13 @@@
  
  	closid = rdtgrp->closid;
  
++<<<<<<< HEAD
 +	for_each_enabled_rdt_resource(r)
++=======
+ 	for_each_alloc_enabled_rdt_resource(r) {
++>>>>>>> 1b5c0b758317 (x86/intel_rdt: Cleanup namespace to support RDT monitoring)
  		list_for_each_entry(dom, &r->domains, list)
 -			dom->have_new_ctrl = false;
 -	}
 +			dom->have_new_cbm = false;
  
  	while ((tok = strsep(&buf, "\n")) != NULL) {
  		resname = strim(strsep(&tok, ":"));
@@@ -173,22 -232,12 +191,22 @@@
  			ret = -EINVAL;
  			goto out;
  		}
 -		ret = rdtgroup_parse_resource(resname, tok, closid);
 -		if (ret)
 +		for_each_enabled_rdt_resource(r) {
 +			if (!strcmp(resname, r->name) &&
 +			    closid < r->num_closid) {
 +				ret = parse_line(tok, r);
 +				if (ret)
 +					goto out;
 +				break;
 +			}
 +		}
 +		if (!r->name) {
 +			ret = -EINVAL;
  			goto out;
 +		}
  	}
  
- 	for_each_enabled_rdt_resource(r) {
+ 	for_each_alloc_enabled_rdt_resource(r) {
  		ret = update_domains(r, closid);
  		if (ret)
  			goto out;
* Unmerged path arch/x86/include/asm/intel_rdt_sched.h
* Unmerged path arch/x86/include/asm/intel_rdt.h
* Unmerged path arch/x86/include/asm/intel_rdt_sched.h
* Unmerged path arch/x86/kernel/cpu/intel_rdt.c
* Unmerged path arch/x86/kernel/cpu/intel_rdt_rdtgroup.c
* Unmerged path arch/x86/kernel/cpu/intel_rdt_schemata.c
