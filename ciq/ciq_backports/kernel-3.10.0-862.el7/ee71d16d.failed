s390/mm: make TASK_SIZE independent from the number of page table levels

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Martin Schwidefsky <schwidefsky@de.ibm.com>
commit ee71d16d22bb268c1f6a64ef6d3654ace5f1e8c7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/ee71d16d.failed

The TASK_SIZE for a process should be maximum possible size of the address
space, 2GB for a 31-bit process and 8PB for a 64-bit process. The number
of page table levels required for a given memory layout is a consequence
of the mapped memory areas and their location.

	Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
(cherry picked from commit ee71d16d22bb268c1f6a64ef6d3654ace5f1e8c7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/include/asm/mman.h
#	arch/s390/include/asm/processor.h
#	arch/s390/kvm/kvm-s390.c
#	arch/s390/mm/gmap.c
#	arch/s390/mm/mmap.c
#	arch/s390/mm/pgalloc.c
diff --cc arch/s390/include/asm/mman.h
index 9977e08df5bd,b79813d9cf68..000000000000
--- a/arch/s390/include/asm/mman.h
+++ b/arch/s390/include/asm/mman.h
@@@ -8,8 -8,4 +8,11 @@@
  
  #include <uapi/asm/mman.h>
  
++<<<<<<< HEAD
 +#if !defined(__ASSEMBLY__) && defined(CONFIG_64BIT)
 +int s390_mmap_check(unsigned long addr, unsigned long len, unsigned long flags);
 +#define arch_mmap_check(addr, len, flags) s390_mmap_check(addr, len, flags)
 +#endif
++=======
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
  #endif /* __S390_MMAN_H__ */
diff --cc arch/s390/include/asm/processor.h
index ce3a4381002e,60d395fdc864..000000000000
--- a/arch/s390/include/asm/processor.h
+++ b/arch/s390/include/asm/processor.h
@@@ -41,30 -90,17 +41,35 @@@ extern void execve_tail(void)
  /*
   * User space process size: 2GB for 31 bit, 4TB or 8PT for 64 bit.
   */
 +#ifndef CONFIG_64BIT
 +
 +#define TASK_SIZE		(1UL << 31)
 +#define TASK_MAX_SIZE		(1UL << 31)
 +#define TASK_UNMAPPED_BASE	(1UL << 30)
 +
 +#else /* CONFIG_64BIT */
  
- #define TASK_SIZE_OF(tsk)	((tsk)->mm ? \
- 				 (tsk)->mm->context.asce_limit : TASK_MAX_SIZE)
+ #define TASK_SIZE_OF(tsk)	(test_tsk_thread_flag(tsk, TIF_31BIT) ? \
+ 					(1UL << 31) : (1UL << 53))
  #define TASK_UNMAPPED_BASE	(test_thread_flag(TIF_31BIT) ? \
  					(1UL << 30) : (1UL << 41))
  #define TASK_SIZE		TASK_SIZE_OF(current)
- #define TASK_MAX_SIZE		(1UL << 53)
+ #define TASK_SIZE_MAX		(1UL << 53)
  
++<<<<<<< HEAD
 +#endif /* CONFIG_64BIT */
 +
 +#ifndef CONFIG_64BIT
 +#define STACK_TOP		(1UL << 31)
 +#define STACK_TOP_MAX		(1UL << 31)
 +#else /* CONFIG_64BIT */
 +#define STACK_TOP		(1UL << (test_thread_flag(TIF_31BIT) ? 31:42))
++=======
+ #define STACK_TOP		(test_thread_flag(TIF_31BIT) ? \
+ 					(1UL << 31) : (1UL << 42))
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
  #define STACK_TOP_MAX		(1UL << 42)
 +#endif /* CONFIG_64BIT */
  
  #define HAVE_ARCH_PICK_MMAP_LAYOUT
  
diff --cc arch/s390/kvm/kvm-s390.c
index 9b2d6973d202,28983836c0f7..000000000000
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@@ -203,83 -559,968 +203,92 @@@ static int kvm_vm_ioctl_enable_cap(stru
  	return r;
  }
  
 -static int kvm_s390_get_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
 +long kvm_arch_vm_ioctl(struct file *filp,
 +		       unsigned int ioctl, unsigned long arg)
  {
 -	int ret;
 -
 -	switch (attr->attr) {
 -	case KVM_S390_VM_MEM_LIMIT_SIZE:
 -		ret = 0;
 -		VM_EVENT(kvm, 3, "QUERY: max guest memory: %lu bytes",
 -			 kvm->arch.mem_limit);
 -		if (put_user(kvm->arch.mem_limit, (u64 __user *)attr->addr))
 -			ret = -EFAULT;
 -		break;
 -	default:
 -		ret = -ENXIO;
 -		break;
 -	}
 -	return ret;
 -}
 +	struct kvm *kvm = filp->private_data;
 +	void __user *argp = (void __user *)arg;
 +	int r;
  
 -static int kvm_s390_set_mem_control(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret;
 -	unsigned int idx;
 -	switch (attr->attr) {
 -	case KVM_S390_VM_MEM_ENABLE_CMMA:
 -		ret = -ENXIO;
 -		if (!sclp.has_cmma)
 -			break;
 +	switch (ioctl) {
 +	case KVM_S390_INTERRUPT: {
 +		struct kvm_s390_interrupt s390int;
  
 -		ret = -EBUSY;
 -		VM_EVENT(kvm, 3, "%s", "ENABLE: CMMA support");
 -		mutex_lock(&kvm->lock);
 -		if (!kvm->created_vcpus) {
 -			kvm->arch.use_cmma = 1;
 -			ret = 0;
 -		}
 -		mutex_unlock(&kvm->lock);
 -		break;
 -	case KVM_S390_VM_MEM_CLR_CMMA:
 -		ret = -ENXIO;
 -		if (!sclp.has_cmma)
 -			break;
 -		ret = -EINVAL;
 -		if (!kvm->arch.use_cmma)
 +		r = -EFAULT;
 +		if (copy_from_user(&s390int, argp, sizeof(s390int)))
  			break;
 -
 -		VM_EVENT(kvm, 3, "%s", "RESET: CMMA states");
 -		mutex_lock(&kvm->lock);
 -		idx = srcu_read_lock(&kvm->srcu);
 -		s390_reset_cmma(kvm->arch.gmap->mm);
 -		srcu_read_unlock(&kvm->srcu, idx);
 -		mutex_unlock(&kvm->lock);
 -		ret = 0;
 -		break;
 -	case KVM_S390_VM_MEM_LIMIT_SIZE: {
 -		unsigned long new_limit;
 -
 -		if (kvm_is_ucontrol(kvm))
 -			return -EINVAL;
 -
 -		if (get_user(new_limit, (u64 __user *)attr->addr))
 -			return -EFAULT;
 -
 -		if (kvm->arch.mem_limit != KVM_S390_NO_MEM_LIMIT &&
 -		    new_limit > kvm->arch.mem_limit)
 -			return -E2BIG;
 -
 -		if (!new_limit)
 -			return -EINVAL;
 -
 -		/* gmap_create takes last usable address */
 -		if (new_limit != KVM_S390_NO_MEM_LIMIT)
 -			new_limit -= 1;
 -
 -		ret = -EBUSY;
 -		mutex_lock(&kvm->lock);
 -		if (!kvm->created_vcpus) {
 -			/* gmap_create will round the limit up */
 -			struct gmap *new = gmap_create(current->mm, new_limit);
 -
 -			if (!new) {
 -				ret = -ENOMEM;
 -			} else {
 -				gmap_remove(kvm->arch.gmap);
 -				new->private = kvm;
 -				kvm->arch.gmap = new;
 -				ret = 0;
 -			}
 -		}
 -		mutex_unlock(&kvm->lock);
 -		VM_EVENT(kvm, 3, "SET: max guest address: %lu", new_limit);
 -		VM_EVENT(kvm, 3, "New guest asce: 0x%pK",
 -			 (void *) kvm->arch.gmap->asce);
 -		break;
 -	}
 -	default:
 -		ret = -ENXIO;
 +		r = kvm_s390_inject_vm(kvm, &s390int);
  		break;
  	}
 -	return ret;
 -}
 -
 -static void kvm_s390_vcpu_crypto_setup(struct kvm_vcpu *vcpu);
 -
 -static int kvm_s390_vm_set_crypto(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	struct kvm_vcpu *vcpu;
 -	int i;
 -
 -	if (!test_kvm_facility(kvm, 76))
 -		return -EINVAL;
 -
 -	mutex_lock(&kvm->lock);
 -	switch (attr->attr) {
 -	case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
 -		get_random_bytes(
 -			kvm->arch.crypto.crycb->aes_wrapping_key_mask,
 -			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 -		kvm->arch.crypto.aes_kw = 1;
 -		VM_EVENT(kvm, 3, "%s", "ENABLE: AES keywrapping support");
 -		break;
 -	case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
 -		get_random_bytes(
 -			kvm->arch.crypto.crycb->dea_wrapping_key_mask,
 -			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 -		kvm->arch.crypto.dea_kw = 1;
 -		VM_EVENT(kvm, 3, "%s", "ENABLE: DEA keywrapping support");
 -		break;
 -	case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
 -		kvm->arch.crypto.aes_kw = 0;
 -		memset(kvm->arch.crypto.crycb->aes_wrapping_key_mask, 0,
 -			sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 -		VM_EVENT(kvm, 3, "%s", "DISABLE: AES keywrapping support");
 -		break;
 -	case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
 -		kvm->arch.crypto.dea_kw = 0;
 -		memset(kvm->arch.crypto.crycb->dea_wrapping_key_mask, 0,
 -			sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 -		VM_EVENT(kvm, 3, "%s", "DISABLE: DEA keywrapping support");
 +	case KVM_ENABLE_CAP: {
 +		struct kvm_enable_cap cap;
 +		r = -EFAULT;
 +		if (copy_from_user(&cap, argp, sizeof(cap)))
 +			break;
 +		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
  		break;
 -	default:
 -		mutex_unlock(&kvm->lock);
 -		return -ENXIO;
  	}
 -
 -	kvm_for_each_vcpu(i, vcpu, kvm) {
 -		kvm_s390_vcpu_crypto_setup(vcpu);
 -		exit_sie(vcpu);
 -	}
 -	mutex_unlock(&kvm->lock);
 -	return 0;
 -}
 -
 -static int kvm_s390_set_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	u8 gtod_high;
 -
 -	if (copy_from_user(&gtod_high, (void __user *)attr->addr,
 -					   sizeof(gtod_high)))
 -		return -EFAULT;
 -
 -	if (gtod_high != 0)
 -		return -EINVAL;
 -	VM_EVENT(kvm, 3, "SET: TOD extension: 0x%x", gtod_high);
 -
 -	return 0;
 -}
 -
 -static int kvm_s390_set_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	u64 gtod;
 -
 -	if (copy_from_user(&gtod, (void __user *)attr->addr, sizeof(gtod)))
 -		return -EFAULT;
 -
 -	kvm_s390_set_tod_clock(kvm, gtod);
 -	VM_EVENT(kvm, 3, "SET: TOD base: 0x%llx", gtod);
 -	return 0;
 -}
 -
 -static int kvm_s390_set_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret;
 -
 -	if (attr->flags)
 -		return -EINVAL;
 -
 -	switch (attr->attr) {
 -	case KVM_S390_VM_TOD_HIGH:
 -		ret = kvm_s390_set_tod_high(kvm, attr);
 -		break;
 -	case KVM_S390_VM_TOD_LOW:
 -		ret = kvm_s390_set_tod_low(kvm, attr);
 -		break;
  	default:
 -		ret = -ENXIO;
 -		break;
 +		r = -ENOTTY;
  	}
 -	return ret;
 -}
 -
 -static int kvm_s390_get_tod_high(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	u8 gtod_high = 0;
 -
 -	if (copy_to_user((void __user *)attr->addr, &gtod_high,
 -					 sizeof(gtod_high)))
 -		return -EFAULT;
 -	VM_EVENT(kvm, 3, "QUERY: TOD extension: 0x%x", gtod_high);
  
 -	return 0;
 +	return r;
  }
  
 -static int kvm_s390_get_tod_low(struct kvm *kvm, struct kvm_device_attr *attr)
 +int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
  {
 -	u64 gtod;
 -
 -	gtod = kvm_s390_get_tod_clock_fast(kvm);
 -	if (copy_to_user((void __user *)attr->addr, &gtod, sizeof(gtod)))
 -		return -EFAULT;
 -	VM_EVENT(kvm, 3, "QUERY: TOD base: 0x%llx", gtod);
 +	int rc;
 +	char debug_name[16];
  
 -	return 0;
 -}
 +	rc = -EINVAL;
 +#ifdef CONFIG_KVM_S390_UCONTROL
 +	if (type & ~KVM_VM_S390_UCONTROL)
 +		goto out_err;
 +	if ((type & KVM_VM_S390_UCONTROL) && (!capable(CAP_SYS_ADMIN)))
 +		goto out_err;
 +#else
 +	if (type)
 +		goto out_err;
 +#endif
  
 -static int kvm_s390_get_tod(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret;
 +	rc = s390_enable_sie();
 +	if (rc)
 +		goto out_err;
  
 -	if (attr->flags)
 -		return -EINVAL;
 +	rc = -ENOMEM;
  
 -	switch (attr->attr) {
 -	case KVM_S390_VM_TOD_HIGH:
 -		ret = kvm_s390_get_tod_high(kvm, attr);
 -		break;
 -	case KVM_S390_VM_TOD_LOW:
 -		ret = kvm_s390_get_tod_low(kvm, attr);
 -		break;
 -	default:
 -		ret = -ENXIO;
 -		break;
 -	}
 -	return ret;
 -}
 +	kvm->arch.sca = (struct sca_block *) get_zeroed_page(GFP_KERNEL);
 +	if (!kvm->arch.sca)
 +		goto out_err;
  
 -static int kvm_s390_set_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	struct kvm_s390_vm_cpu_processor *proc;
 -	u16 lowest_ibc, unblocked_ibc;
 -	int ret = 0;
 +	sprintf(debug_name, "kvm-%u", current->pid);
  
 -	mutex_lock(&kvm->lock);
 -	if (kvm->created_vcpus) {
 -		ret = -EBUSY;
 -		goto out;
 -	}
 -	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
 -	if (!proc) {
 -		ret = -ENOMEM;
 -		goto out;
 -	}
 -	if (!copy_from_user(proc, (void __user *)attr->addr,
 -			    sizeof(*proc))) {
 -		kvm->arch.model.cpuid = proc->cpuid;
 -		lowest_ibc = sclp.ibc >> 16 & 0xfff;
 -		unblocked_ibc = sclp.ibc & 0xfff;
 -		if (lowest_ibc && proc->ibc) {
 -			if (proc->ibc > unblocked_ibc)
 -				kvm->arch.model.ibc = unblocked_ibc;
 -			else if (proc->ibc < lowest_ibc)
 -				kvm->arch.model.ibc = lowest_ibc;
 -			else
 -				kvm->arch.model.ibc = proc->ibc;
 -		}
 -		memcpy(kvm->arch.model.fac_list, proc->fac_list,
 -		       S390_ARCH_FAC_LIST_SIZE_BYTE);
 -		VM_EVENT(kvm, 3, "SET: guest ibc: 0x%4.4x, guest cpuid: 0x%16.16llx",
 -			 kvm->arch.model.ibc,
 -			 kvm->arch.model.cpuid);
 -		VM_EVENT(kvm, 3, "SET: guest faclist: 0x%16.16llx.%16.16llx.%16.16llx",
 -			 kvm->arch.model.fac_list[0],
 -			 kvm->arch.model.fac_list[1],
 -			 kvm->arch.model.fac_list[2]);
 -	} else
 -		ret = -EFAULT;
 -	kfree(proc);
 -out:
 -	mutex_unlock(&kvm->lock);
 -	return ret;
 -}
 +	kvm->arch.dbf = debug_register(debug_name, 8, 2, 8 * sizeof(long));
 +	if (!kvm->arch.dbf)
 +		goto out_nodbf;
  
 -static int kvm_s390_set_processor_feat(struct kvm *kvm,
 -				       struct kvm_device_attr *attr)
 -{
 -	struct kvm_s390_vm_cpu_feat data;
 -	int ret = -EBUSY;
 +	spin_lock_init(&kvm->arch.float_int.lock);
 +	INIT_LIST_HEAD(&kvm->arch.float_int.list);
  
 -	if (copy_from_user(&data, (void __user *)attr->addr, sizeof(data)))
 -		return -EFAULT;
 -	if (!bitmap_subset((unsigned long *) data.feat,
 -			   kvm_s390_available_cpu_feat,
 -			   KVM_S390_VM_CPU_FEAT_NR_BITS))
 -		return -EINVAL;
 +	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 +	VM_EVENT(kvm, 3, "%s", "vm created");
  
 -	mutex_lock(&kvm->lock);
 -	if (!atomic_read(&kvm->online_vcpus)) {
 -		bitmap_copy(kvm->arch.cpu_feat, (unsigned long *) data.feat,
 -			    KVM_S390_VM_CPU_FEAT_NR_BITS);
 -		ret = 0;
 -	}
 -	mutex_unlock(&kvm->lock);
 -	return ret;
 -}
 -
 -static int kvm_s390_set_processor_subfunc(struct kvm *kvm,
 -					  struct kvm_device_attr *attr)
 -{
 -	/*
 -	 * Once supported by kernel + hw, we have to store the subfunctions
 -	 * in kvm->arch and remember that user space configured them.
 -	 */
 -	return -ENXIO;
 -}
 -
 -static int kvm_s390_set_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret = -ENXIO;
 -
 -	switch (attr->attr) {
 -	case KVM_S390_VM_CPU_PROCESSOR:
 -		ret = kvm_s390_set_processor(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_PROCESSOR_FEAT:
 -		ret = kvm_s390_set_processor_feat(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
 -		ret = kvm_s390_set_processor_subfunc(kvm, attr);
 -		break;
 -	}
 -	return ret;
 -}
 -
 -static int kvm_s390_get_processor(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	struct kvm_s390_vm_cpu_processor *proc;
 -	int ret = 0;
 -
 -	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
 -	if (!proc) {
 -		ret = -ENOMEM;
 -		goto out;
 -	}
 -	proc->cpuid = kvm->arch.model.cpuid;
 -	proc->ibc = kvm->arch.model.ibc;
 -	memcpy(&proc->fac_list, kvm->arch.model.fac_list,
 -	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 -	VM_EVENT(kvm, 3, "GET: guest ibc: 0x%4.4x, guest cpuid: 0x%16.16llx",
 -		 kvm->arch.model.ibc,
 -		 kvm->arch.model.cpuid);
 -	VM_EVENT(kvm, 3, "GET: guest faclist: 0x%16.16llx.%16.16llx.%16.16llx",
 -		 kvm->arch.model.fac_list[0],
 -		 kvm->arch.model.fac_list[1],
 -		 kvm->arch.model.fac_list[2]);
 -	if (copy_to_user((void __user *)attr->addr, proc, sizeof(*proc)))
 -		ret = -EFAULT;
 -	kfree(proc);
 -out:
 -	return ret;
 -}
 -
 -static int kvm_s390_get_machine(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	struct kvm_s390_vm_cpu_machine *mach;
 -	int ret = 0;
 -
 -	mach = kzalloc(sizeof(*mach), GFP_KERNEL);
 -	if (!mach) {
 -		ret = -ENOMEM;
 -		goto out;
 -	}
 -	get_cpu_id((struct cpuid *) &mach->cpuid);
 -	mach->ibc = sclp.ibc;
 -	memcpy(&mach->fac_mask, kvm->arch.model.fac_mask,
 -	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 -	memcpy((unsigned long *)&mach->fac_list, S390_lowcore.stfle_fac_list,
 -	       sizeof(S390_lowcore.stfle_fac_list));
 -	VM_EVENT(kvm, 3, "GET: host ibc:  0x%4.4x, host cpuid:  0x%16.16llx",
 -		 kvm->arch.model.ibc,
 -		 kvm->arch.model.cpuid);
 -	VM_EVENT(kvm, 3, "GET: host facmask:  0x%16.16llx.%16.16llx.%16.16llx",
 -		 mach->fac_mask[0],
 -		 mach->fac_mask[1],
 -		 mach->fac_mask[2]);
 -	VM_EVENT(kvm, 3, "GET: host faclist:  0x%16.16llx.%16.16llx.%16.16llx",
 -		 mach->fac_list[0],
 -		 mach->fac_list[1],
 -		 mach->fac_list[2]);
 -	if (copy_to_user((void __user *)attr->addr, mach, sizeof(*mach)))
 -		ret = -EFAULT;
 -	kfree(mach);
 -out:
 -	return ret;
 -}
 -
 -static int kvm_s390_get_processor_feat(struct kvm *kvm,
 -				       struct kvm_device_attr *attr)
 -{
 -	struct kvm_s390_vm_cpu_feat data;
 -
 -	bitmap_copy((unsigned long *) data.feat, kvm->arch.cpu_feat,
 -		    KVM_S390_VM_CPU_FEAT_NR_BITS);
 -	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
 -		return -EFAULT;
 -	return 0;
 -}
 -
 -static int kvm_s390_get_machine_feat(struct kvm *kvm,
 -				     struct kvm_device_attr *attr)
 -{
 -	struct kvm_s390_vm_cpu_feat data;
 -
 -	bitmap_copy((unsigned long *) data.feat,
 -		    kvm_s390_available_cpu_feat,
 -		    KVM_S390_VM_CPU_FEAT_NR_BITS);
 -	if (copy_to_user((void __user *)attr->addr, &data, sizeof(data)))
 -		return -EFAULT;
 -	return 0;
 -}
 -
 -static int kvm_s390_get_processor_subfunc(struct kvm *kvm,
 -					  struct kvm_device_attr *attr)
 -{
 -	/*
 -	 * Once we can actually configure subfunctions (kernel + hw support),
 -	 * we have to check if they were already set by user space, if so copy
 -	 * them from kvm->arch.
 -	 */
 -	return -ENXIO;
 -}
 -
 -static int kvm_s390_get_machine_subfunc(struct kvm *kvm,
 -					struct kvm_device_attr *attr)
 -{
 -	if (copy_to_user((void __user *)attr->addr, &kvm_s390_available_subfunc,
 -	    sizeof(struct kvm_s390_vm_cpu_subfunc)))
 -		return -EFAULT;
 -	return 0;
 -}
 -static int kvm_s390_get_cpu_model(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret = -ENXIO;
 -
 -	switch (attr->attr) {
 -	case KVM_S390_VM_CPU_PROCESSOR:
 -		ret = kvm_s390_get_processor(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_MACHINE:
 -		ret = kvm_s390_get_machine(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_PROCESSOR_FEAT:
 -		ret = kvm_s390_get_processor_feat(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_MACHINE_FEAT:
 -		ret = kvm_s390_get_machine_feat(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
 -		ret = kvm_s390_get_processor_subfunc(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_MACHINE_SUBFUNC:
 -		ret = kvm_s390_get_machine_subfunc(kvm, attr);
 -		break;
 -	}
 -	return ret;
 -}
 -
 -static int kvm_s390_vm_set_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret;
 -
 -	switch (attr->group) {
 -	case KVM_S390_VM_MEM_CTRL:
 -		ret = kvm_s390_set_mem_control(kvm, attr);
 -		break;
 -	case KVM_S390_VM_TOD:
 -		ret = kvm_s390_set_tod(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_MODEL:
 -		ret = kvm_s390_set_cpu_model(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CRYPTO:
 -		ret = kvm_s390_vm_set_crypto(kvm, attr);
 -		break;
 -	default:
 -		ret = -ENXIO;
 -		break;
 -	}
 -
 -	return ret;
 -}
 -
 -static int kvm_s390_vm_get_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret;
 -
 -	switch (attr->group) {
 -	case KVM_S390_VM_MEM_CTRL:
 -		ret = kvm_s390_get_mem_control(kvm, attr);
 -		break;
 -	case KVM_S390_VM_TOD:
 -		ret = kvm_s390_get_tod(kvm, attr);
 -		break;
 -	case KVM_S390_VM_CPU_MODEL:
 -		ret = kvm_s390_get_cpu_model(kvm, attr);
 -		break;
 -	default:
 -		ret = -ENXIO;
 -		break;
 -	}
 -
 -	return ret;
 -}
 -
 -static int kvm_s390_vm_has_attr(struct kvm *kvm, struct kvm_device_attr *attr)
 -{
 -	int ret;
 -
 -	switch (attr->group) {
 -	case KVM_S390_VM_MEM_CTRL:
 -		switch (attr->attr) {
 -		case KVM_S390_VM_MEM_ENABLE_CMMA:
 -		case KVM_S390_VM_MEM_CLR_CMMA:
 -			ret = sclp.has_cmma ? 0 : -ENXIO;
 -			break;
 -		case KVM_S390_VM_MEM_LIMIT_SIZE:
 -			ret = 0;
 -			break;
 -		default:
 -			ret = -ENXIO;
 -			break;
 -		}
 -		break;
 -	case KVM_S390_VM_TOD:
 -		switch (attr->attr) {
 -		case KVM_S390_VM_TOD_LOW:
 -		case KVM_S390_VM_TOD_HIGH:
 -			ret = 0;
 -			break;
 -		default:
 -			ret = -ENXIO;
 -			break;
 -		}
 -		break;
 -	case KVM_S390_VM_CPU_MODEL:
 -		switch (attr->attr) {
 -		case KVM_S390_VM_CPU_PROCESSOR:
 -		case KVM_S390_VM_CPU_MACHINE:
 -		case KVM_S390_VM_CPU_PROCESSOR_FEAT:
 -		case KVM_S390_VM_CPU_MACHINE_FEAT:
 -		case KVM_S390_VM_CPU_MACHINE_SUBFUNC:
 -			ret = 0;
 -			break;
 -		/* configuring subfunctions is not supported yet */
 -		case KVM_S390_VM_CPU_PROCESSOR_SUBFUNC:
 -		default:
 -			ret = -ENXIO;
 -			break;
 -		}
 -		break;
 -	case KVM_S390_VM_CRYPTO:
 -		switch (attr->attr) {
 -		case KVM_S390_VM_CRYPTO_ENABLE_AES_KW:
 -		case KVM_S390_VM_CRYPTO_ENABLE_DEA_KW:
 -		case KVM_S390_VM_CRYPTO_DISABLE_AES_KW:
 -		case KVM_S390_VM_CRYPTO_DISABLE_DEA_KW:
 -			ret = 0;
 -			break;
 -		default:
 -			ret = -ENXIO;
 -			break;
 -		}
 -		break;
 -	default:
 -		ret = -ENXIO;
 -		break;
 -	}
 -
 -	return ret;
 -}
 -
 -static long kvm_s390_get_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 -{
 -	uint8_t *keys;
 -	uint64_t hva;
 -	int i, r = 0;
 -
 -	if (args->flags != 0)
 -		return -EINVAL;
 -
 -	/* Is this guest using storage keys? */
 -	if (!mm_use_skey(current->mm))
 -		return KVM_S390_GET_SKEYS_NONE;
 -
 -	/* Enforce sane limit on memory allocation */
 -	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
 -		return -EINVAL;
 -
 -	keys = kmalloc_array(args->count, sizeof(uint8_t),
 -			     GFP_KERNEL | __GFP_NOWARN);
 -	if (!keys)
 -		keys = vmalloc(sizeof(uint8_t) * args->count);
 -	if (!keys)
 -		return -ENOMEM;
 -
 -	down_read(&current->mm->mmap_sem);
 -	for (i = 0; i < args->count; i++) {
 -		hva = gfn_to_hva(kvm, args->start_gfn + i);
 -		if (kvm_is_error_hva(hva)) {
 -			r = -EFAULT;
 -			break;
 -		}
 -
 -		r = get_guest_storage_key(current->mm, hva, &keys[i]);
 -		if (r)
 -			break;
 -	}
 -	up_read(&current->mm->mmap_sem);
 -
 -	if (!r) {
 -		r = copy_to_user((uint8_t __user *)args->skeydata_addr, keys,
 -				 sizeof(uint8_t) * args->count);
 -		if (r)
 -			r = -EFAULT;
 -	}
 -
 -	kvfree(keys);
 -	return r;
 -}
 -
 -static long kvm_s390_set_skeys(struct kvm *kvm, struct kvm_s390_skeys *args)
 -{
 -	uint8_t *keys;
 -	uint64_t hva;
 -	int i, r = 0;
 -
 -	if (args->flags != 0)
 -		return -EINVAL;
 -
 -	/* Enforce sane limit on memory allocation */
 -	if (args->count < 1 || args->count > KVM_S390_SKEYS_MAX)
 -		return -EINVAL;
 -
 -	keys = kmalloc_array(args->count, sizeof(uint8_t),
 -			     GFP_KERNEL | __GFP_NOWARN);
 -	if (!keys)
 -		keys = vmalloc(sizeof(uint8_t) * args->count);
 -	if (!keys)
 -		return -ENOMEM;
 -
 -	r = copy_from_user(keys, (uint8_t __user *)args->skeydata_addr,
 -			   sizeof(uint8_t) * args->count);
 -	if (r) {
 -		r = -EFAULT;
 -		goto out;
 -	}
 -
 -	/* Enable storage key handling for the guest */
 -	r = s390_enable_skey();
 -	if (r)
 -		goto out;
 -
 -	down_read(&current->mm->mmap_sem);
 -	for (i = 0; i < args->count; i++) {
 -		hva = gfn_to_hva(kvm, args->start_gfn + i);
 -		if (kvm_is_error_hva(hva)) {
 -			r = -EFAULT;
 -			break;
 -		}
 -
 -		/* Lowest order bit is reserved */
 -		if (keys[i] & 0x01) {
 -			r = -EINVAL;
 -			break;
 -		}
 -
 -		r = set_guest_storage_key(current->mm, hva, keys[i], 0);
 -		if (r)
 -			break;
 -	}
 -	up_read(&current->mm->mmap_sem);
 -out:
 -	kvfree(keys);
 -	return r;
 -}
 -
 -long kvm_arch_vm_ioctl(struct file *filp,
 -		       unsigned int ioctl, unsigned long arg)
 -{
 -	struct kvm *kvm = filp->private_data;
 -	void __user *argp = (void __user *)arg;
 -	struct kvm_device_attr attr;
 -	int r;
 -
 -	switch (ioctl) {
 -	case KVM_S390_INTERRUPT: {
 -		struct kvm_s390_interrupt s390int;
 -
 -		r = -EFAULT;
 -		if (copy_from_user(&s390int, argp, sizeof(s390int)))
 -			break;
 -		r = kvm_s390_inject_vm(kvm, &s390int);
 -		break;
 -	}
 -	case KVM_ENABLE_CAP: {
 -		struct kvm_enable_cap cap;
 -		r = -EFAULT;
 -		if (copy_from_user(&cap, argp, sizeof(cap)))
 -			break;
 -		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
 -		break;
 -	}
 -	case KVM_CREATE_IRQCHIP: {
 -		struct kvm_irq_routing_entry routing;
 -
 -		r = -EINVAL;
 -		if (kvm->arch.use_irqchip) {
 -			/* Set up dummy routing. */
 -			memset(&routing, 0, sizeof(routing));
 -			r = kvm_set_irq_routing(kvm, &routing, 0, 0);
 -		}
 -		break;
 -	}
 -	case KVM_SET_DEVICE_ATTR: {
 -		r = -EFAULT;
 -		if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
 -			break;
 -		r = kvm_s390_vm_set_attr(kvm, &attr);
 -		break;
 -	}
 -	case KVM_GET_DEVICE_ATTR: {
 -		r = -EFAULT;
 -		if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
 -			break;
 -		r = kvm_s390_vm_get_attr(kvm, &attr);
 -		break;
 -	}
 -	case KVM_HAS_DEVICE_ATTR: {
 -		r = -EFAULT;
 -		if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
 -			break;
 -		r = kvm_s390_vm_has_attr(kvm, &attr);
 -		break;
 -	}
 -	case KVM_S390_GET_SKEYS: {
 -		struct kvm_s390_skeys args;
 -
 -		r = -EFAULT;
 -		if (copy_from_user(&args, argp,
 -				   sizeof(struct kvm_s390_skeys)))
 -			break;
 -		r = kvm_s390_get_skeys(kvm, &args);
 -		break;
 -	}
 -	case KVM_S390_SET_SKEYS: {
 -		struct kvm_s390_skeys args;
 -
 -		r = -EFAULT;
 -		if (copy_from_user(&args, argp,
 -				   sizeof(struct kvm_s390_skeys)))
 -			break;
 -		r = kvm_s390_set_skeys(kvm, &args);
 -		break;
 -	}
 -	default:
 -		r = -ENOTTY;
 -	}
 -
 -	return r;
 -}
 -
 -static int kvm_s390_query_ap_config(u8 *config)
 -{
 -	u32 fcn_code = 0x04000000UL;
 -	u32 cc = 0;
 -
 -	memset(config, 0, 128);
 -	asm volatile(
 -		"lgr 0,%1\n"
 -		"lgr 2,%2\n"
 -		".long 0xb2af0000\n"		/* PQAP(QCI) */
 -		"0: ipm %0\n"
 -		"srl %0,28\n"
 -		"1:\n"
 -		EX_TABLE(0b, 1b)
 -		: "+r" (cc)
 -		: "r" (fcn_code), "r" (config)
 -		: "cc", "0", "2", "memory"
 -	);
 -
 -	return cc;
 -}
 -
 -static int kvm_s390_apxa_installed(void)
 -{
 -	u8 config[128];
 -	int cc;
 -
 -	if (test_facility(12)) {
 -		cc = kvm_s390_query_ap_config(config);
 -
 -		if (cc)
 -			pr_err("PQAP(QCI) failed with cc=%d", cc);
 -		else
 -			return config[0] & 0x40;
 -	}
 -
 -	return 0;
 -}
 -
 -static void kvm_s390_set_crycb_format(struct kvm *kvm)
 -{
 -	kvm->arch.crypto.crycbd = (__u32)(unsigned long) kvm->arch.crypto.crycb;
 -
 -	if (kvm_s390_apxa_installed())
 -		kvm->arch.crypto.crycbd |= CRYCB_FORMAT2;
 -	else
 -		kvm->arch.crypto.crycbd |= CRYCB_FORMAT1;
 -}
 -
 -static u64 kvm_s390_get_initial_cpuid(void)
 -{
 -	struct cpuid cpuid;
 -
 -	get_cpu_id(&cpuid);
 -	cpuid.version = 0xff;
 -	return *((u64 *) &cpuid);
 -}
 -
 -static void kvm_s390_crypto_init(struct kvm *kvm)
 -{
 -	if (!test_kvm_facility(kvm, 76))
 -		return;
 -
 -	kvm->arch.crypto.crycb = &kvm->arch.sie_page2->crycb;
 -	kvm_s390_set_crycb_format(kvm);
 -
 -	/* Enable AES/DEA protected key functions by default */
 -	kvm->arch.crypto.aes_kw = 1;
 -	kvm->arch.crypto.dea_kw = 1;
 -	get_random_bytes(kvm->arch.crypto.crycb->aes_wrapping_key_mask,
 -			 sizeof(kvm->arch.crypto.crycb->aes_wrapping_key_mask));
 -	get_random_bytes(kvm->arch.crypto.crycb->dea_wrapping_key_mask,
 -			 sizeof(kvm->arch.crypto.crycb->dea_wrapping_key_mask));
 -}
 -
 -static void sca_dispose(struct kvm *kvm)
 -{
 -	if (kvm->arch.use_esca)
 -		free_pages_exact(kvm->arch.sca, sizeof(struct esca_block));
 -	else
 -		free_page((unsigned long)(kvm->arch.sca));
 -	kvm->arch.sca = NULL;
 -}
 -
 -int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 -{
 -	gfp_t alloc_flags = GFP_KERNEL;
 -	int i, rc;
 -	char debug_name[16];
 -	static unsigned long sca_offset;
 -
 -	rc = -EINVAL;
 -#ifdef CONFIG_KVM_S390_UCONTROL
 -	if (type & ~KVM_VM_S390_UCONTROL)
 -		goto out_err;
 -	if ((type & KVM_VM_S390_UCONTROL) && (!capable(CAP_SYS_ADMIN)))
 -		goto out_err;
 -#else
 -	if (type)
 -		goto out_err;
 -#endif
 -
 -	rc = s390_enable_sie();
 -	if (rc)
 -		goto out_err;
 -
 -	rc = -ENOMEM;
 -
 -	ratelimit_state_init(&kvm->arch.sthyi_limit, 5 * HZ, 500);
 -
 -	kvm->arch.use_esca = 0; /* start with basic SCA */
 -	if (!sclp.has_64bscao)
 -		alloc_flags |= GFP_DMA;
 -	rwlock_init(&kvm->arch.sca_lock);
 -	kvm->arch.sca = (struct bsca_block *) get_zeroed_page(alloc_flags);
 -	if (!kvm->arch.sca)
 -		goto out_err;
 -	spin_lock(&kvm_lock);
 -	sca_offset += 16;
 -	if (sca_offset + sizeof(struct bsca_block) > PAGE_SIZE)
 -		sca_offset = 0;
 -	kvm->arch.sca = (struct bsca_block *)
 -			((char *) kvm->arch.sca + sca_offset);
 -	spin_unlock(&kvm_lock);
 -
 -	sprintf(debug_name, "kvm-%u", current->pid);
 -
 -	kvm->arch.dbf = debug_register(debug_name, 32, 1, 7 * sizeof(long));
 -	if (!kvm->arch.dbf)
 -		goto out_err;
 -
 -	kvm->arch.sie_page2 =
 -	     (struct sie_page2 *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 -	if (!kvm->arch.sie_page2)
 -		goto out_err;
 -
 -	/* Populate the facility mask initially. */
 -	memcpy(kvm->arch.model.fac_mask, S390_lowcore.stfle_fac_list,
 -	       sizeof(S390_lowcore.stfle_fac_list));
 -	for (i = 0; i < S390_ARCH_FAC_LIST_SIZE_U64; i++) {
 -		if (i < kvm_s390_fac_list_mask_size())
 -			kvm->arch.model.fac_mask[i] &= kvm_s390_fac_list_mask[i];
 -		else
 -			kvm->arch.model.fac_mask[i] = 0UL;
 -	}
 -
 -	/* Populate the facility list initially. */
 -	kvm->arch.model.fac_list = kvm->arch.sie_page2->fac_list;
 -	memcpy(kvm->arch.model.fac_list, kvm->arch.model.fac_mask,
 -	       S390_ARCH_FAC_LIST_SIZE_BYTE);
 -
 -	set_kvm_facility(kvm->arch.model.fac_mask, 74);
 -	set_kvm_facility(kvm->arch.model.fac_list, 74);
 -
 -	kvm->arch.model.cpuid = kvm_s390_get_initial_cpuid();
 -	kvm->arch.model.ibc = sclp.ibc & 0x0fff;
 -
 -	kvm_s390_crypto_init(kvm);
 -
 -	spin_lock_init(&kvm->arch.float_int.lock);
 -	for (i = 0; i < FIRQ_LIST_COUNT; i++)
 -		INIT_LIST_HEAD(&kvm->arch.float_int.lists[i]);
 -	init_waitqueue_head(&kvm->arch.ipte_wq);
 -	mutex_init(&kvm->arch.ipte_mutex);
 -
 -	debug_register_view(kvm->arch.dbf, &debug_sprintf_view);
 -	VM_EVENT(kvm, 3, "vm created with type %lu", type);
 -
 -	if (type & KVM_VM_S390_UCONTROL) {
 -		kvm->arch.gmap = NULL;
 -		kvm->arch.mem_limit = KVM_S390_NO_MEM_LIMIT;
 -	} else {
 -		if (sclp.hamax == U64_MAX)
 -			kvm->arch.mem_limit = TASK_SIZE_MAX;
 -		else
 -			kvm->arch.mem_limit = min_t(unsigned long, TASK_SIZE_MAX,
 -						    sclp.hamax + 1);
 -		kvm->arch.gmap = gmap_create(current->mm, kvm->arch.mem_limit - 1);
 -		if (!kvm->arch.gmap)
 -			goto out_err;
 -		kvm->arch.gmap->private = kvm;
 -		kvm->arch.gmap->pfault_enabled = 0;
 +	if (type & KVM_VM_S390_UCONTROL) {
 +		kvm->arch.gmap = NULL;
 +	} else {
++<<<<<<< HEAD
 +		kvm->arch.gmap = gmap_alloc(current->mm);
++=======
++		if (sclp.hamax == U64_MAX)
++			kvm->arch.mem_limit = TASK_SIZE_MAX;
++		else
++			kvm->arch.mem_limit = min_t(unsigned long, TASK_SIZE_MAX,
++						    sclp.hamax + 1);
++		kvm->arch.gmap = gmap_create(current->mm, kvm->arch.mem_limit - 1);
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
 +		if (!kvm->arch.gmap)
 +			goto out_nogmap;
 +		kvm->arch.gmap->private = kvm;
  	}
  
  	kvm->arch.css_support = 0;
diff --cc arch/s390/mm/mmap.c
index 88bf1e650652,eed233ce59dd..000000000000
--- a/arch/s390/mm/mmap.c
+++ b/arch/s390/mm/mmap.c
@@@ -92,9 -89,8 +92,9 @@@ arch_get_unmapped_area(struct file *fil
  	struct mm_struct *mm = current->mm;
  	struct vm_area_struct *vma;
  	struct vm_unmapped_area_info info;
 +	int do_color_align;
  
- 	if (len > TASK_SIZE - mmap_min_addr)
+ 	if (len > mm->context.asce_limit - mmap_min_addr)
  		return -ENOMEM;
  
  	if (flags & MAP_FIXED)
@@@ -103,8 -99,9 +103,14 @@@
  	if (addr) {
  		addr = PAGE_ALIGN(addr);
  		vma = find_vma(mm, addr);
++<<<<<<< HEAD
 +		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
 +		    (!vma || addr + len <= vm_start_gap(vma)))
++=======
+ 		if (mm->context.asce_limit - len >= addr &&
+ 		    addr >= mmap_min_addr &&
+ 		    (!vma || addr + len <= vma->vm_start))
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
  			return addr;
  	}
  
@@@ -115,8 -108,11 +121,16 @@@
  	info.flags = 0;
  	info.length = len;
  	info.low_limit = mm->mmap_base;
++<<<<<<< HEAD
 +	info.high_limit = TASK_SIZE;
 +	info.align_mask = do_color_align ? (mmap_align_mask << PAGE_SHIFT) : 0;
++=======
+ 	info.high_limit = mm->context.asce_limit;
+ 	if (filp || (flags & MAP_SHARED))
+ 		info.align_mask = MMAP_ALIGN_MASK << PAGE_SHIFT;
+ 	else
+ 		info.align_mask = 0;
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
  	info.align_offset = pgoff << PAGE_SHIFT;
  	return vm_unmapped_area(&info);
  }
@@@ -130,10 -126,9 +144,10 @@@ arch_get_unmapped_area_topdown(struct f
  	struct mm_struct *mm = current->mm;
  	unsigned long addr = addr0;
  	struct vm_unmapped_area_info info;
 +	int do_color_align;
  
  	/* requested length too big for entire address space */
- 	if (len > TASK_SIZE - mmap_min_addr)
+ 	if (len > mm->context.asce_limit - mmap_min_addr)
  		return -ENOMEM;
  
  	if (flags & MAP_FIXED)
@@@ -177,47 -172,6 +192,50 @@@
  	return addr;
  }
  
++<<<<<<< HEAD
 +#ifndef CONFIG_64BIT
 +
 +/*
 + * This function, called very early during the creation of a new
 + * process VM image, sets up which VM layout function to use:
 + */
 +void arch_pick_mmap_layout(struct mm_struct *mm)
 +{
 +	unsigned long random_factor = 0UL;
 +
 +	if (current->flags & PF_RANDOMIZE)
 +		random_factor = arch_mmap_rnd();
 +
 +	/*
 +	 * Fall back to the standard layout if the personality
 +	 * bit is set, or if the expected stack growth is unlimited:
 +	 */
 +	if (mmap_is_legacy()) {
 +		mm->mmap_base = mmap_base_legacy(random_factor);
 +		mm->get_unmapped_area = arch_get_unmapped_area;
 +		mm->unmap_area = arch_unmap_area;
 +	} else {
 +		mm->mmap_base = mmap_base(random_factor);
 +		mm->get_unmapped_area = arch_get_unmapped_area_topdown;
 +		mm->unmap_area = arch_unmap_area_topdown;
 +	}
 +}
 +
 +#else
 +
 +int s390_mmap_check(unsigned long addr, unsigned long len, unsigned long flags)
 +{
 +	if (is_compat_task() || (TASK_SIZE >= (1UL << 53)))
 +		return 0;
 +	if (!(flags & MAP_FIXED))
 +		addr = 0;
 +	if ((addr + len) >= TASK_SIZE)
 +		return crst_table_upgrade(current->mm);
 +	return 0;
 +}
 +
++=======
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
  static unsigned long
  s390_get_unmapped_area(struct file *filp, unsigned long addr,
  		unsigned long len, unsigned long pgoff, unsigned long flags)
@@@ -229,7 -183,8 +247,12 @@@
  	area = arch_get_unmapped_area(filp, addr, len, pgoff, flags);
  	if (!(area & ~PAGE_MASK))
  		return area;
++<<<<<<< HEAD
 +	if (area == -ENOMEM && !is_compat_task() && TASK_SIZE < (1UL << 53)) {
++=======
+ 	if (area == -ENOMEM && !is_compat_task() &&
+ 	    current->mm->context.asce_limit < TASK_SIZE_MAX) {
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
  		/* Upgrade the page table to 4 levels and retry. */
  		rc = crst_table_upgrade(mm);
  		if (rc)
@@@ -251,7 -206,8 +274,12 @@@ s390_get_unmapped_area_topdown(struct f
  	area = arch_get_unmapped_area_topdown(filp, addr, len, pgoff, flags);
  	if (!(area & ~PAGE_MASK))
  		return area;
++<<<<<<< HEAD
 +	if (area == -ENOMEM && !is_compat_task() && TASK_SIZE < (1UL << 53)) {
++=======
+ 	if (area == -ENOMEM && !is_compat_task() &&
+ 	    current->mm->context.asce_limit < TASK_SIZE_MAX) {
++>>>>>>> ee71d16d22bb (s390/mm: make TASK_SIZE independent from the number of page table levels)
  		/* Upgrade the page table to 4 levels and retry. */
  		rc = crst_table_upgrade(mm);
  		if (rc)
* Unmerged path arch/s390/mm/gmap.c
* Unmerged path arch/s390/mm/pgalloc.c
* Unmerged path arch/s390/include/asm/mman.h
* Unmerged path arch/s390/include/asm/processor.h
* Unmerged path arch/s390/kvm/kvm-s390.c
* Unmerged path arch/s390/mm/gmap.c
diff --git a/arch/s390/mm/gup.c b/arch/s390/mm/gup.c
index 58889b77a1de..954de1c2d583 100644
--- a/arch/s390/mm/gup.c
+++ b/arch/s390/mm/gup.c
@@ -225,7 +225,7 @@ int __get_user_pages_fast(unsigned long start, int nr_pages, int write,
 	addr = start;
 	len = (unsigned long) nr_pages << PAGE_SHIFT;
 	end = start + len;
-	if ((end <= start) || (end > TASK_SIZE))
+	if ((end <= start) || (end > mm->context.asce_limit))
 		return 0;
 
 	local_irq_save(flags);
* Unmerged path arch/s390/mm/mmap.c
* Unmerged path arch/s390/mm/pgalloc.c
