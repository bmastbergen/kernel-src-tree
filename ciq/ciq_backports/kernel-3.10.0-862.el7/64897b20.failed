cpufreq: intel_pstate: Fix policy data management in passive mode

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [cpufreq] intel_pstate: Fix policy data management in passive mode (Prarit Bhargava) [1465349]
Rebuild_FUZZ: 92.56%
commit-author Rafael J. Wysocki <rafael.j.wysocki@intel.com>
commit 64897b20eed29cee2b998fb5ba362e65523dba3c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/64897b20.failed

The policy->cpuinfo.max_freq and policy->max updates in
intel_cpufreq_turbo_update() are excessive as they are done for no
good reason and may lead to problems in principle, so they should be
dropped.  However, after dropping them intel_cpufreq_turbo_update()
becomes almost entirely pointless, because the check made by it is
made again down the road in intel_pstate_prepare_request().  The
only thing in it that still needs to be done is the call to
update_turbo_state(), so drop intel_cpufreq_turbo_update() altogether
and make its callers invoke update_turbo_state() directly instead of
it.

In addition to that, fix intel_cpufreq_verify_policy() so that it
checks global.no_turbo in addition to global.turbo_disabled when
updating policy->cpuinfo.max_freq to make it consistent with
intel_pstate_verify_policy().

Fixes: 001c76f05b01 (cpufreq: intel_pstate: Generic governors support)
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 64897b20eed29cee2b998fb5ba362e65523dba3c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/intel_pstate.c
diff --cc drivers/cpufreq/intel_pstate.c
index 9b85edc571c0,283491f742d3..000000000000
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@@ -1574,6 -2252,204 +1574,207 @@@ static struct cpufreq_driver intel_psta
  	.name		= "intel_pstate",
  };
  
++<<<<<<< HEAD
++=======
+ static int intel_cpufreq_verify_policy(struct cpufreq_policy *policy)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 
+ 	update_turbo_state();
+ 	policy->cpuinfo.max_freq = global.no_turbo || global.turbo_disabled ?
+ 			cpu->pstate.max_freq : cpu->pstate.turbo_freq;
+ 
+ 	cpufreq_verify_within_cpu_limits(policy);
+ 
+ 	return 0;
+ }
+ 
+ static int intel_cpufreq_target(struct cpufreq_policy *policy,
+ 				unsigned int target_freq,
+ 				unsigned int relation)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 	struct cpufreq_freqs freqs;
+ 	int target_pstate;
+ 
+ 	update_turbo_state();
+ 
+ 	freqs.old = policy->cur;
+ 	freqs.new = target_freq;
+ 
+ 	cpufreq_freq_transition_begin(policy, &freqs);
+ 	switch (relation) {
+ 	case CPUFREQ_RELATION_L:
+ 		target_pstate = DIV_ROUND_UP(freqs.new, cpu->pstate.scaling);
+ 		break;
+ 	case CPUFREQ_RELATION_H:
+ 		target_pstate = freqs.new / cpu->pstate.scaling;
+ 		break;
+ 	default:
+ 		target_pstate = DIV_ROUND_CLOSEST(freqs.new, cpu->pstate.scaling);
+ 		break;
+ 	}
+ 	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
+ 	if (target_pstate != cpu->pstate.current_pstate) {
+ 		cpu->pstate.current_pstate = target_pstate;
+ 		wrmsrl_on_cpu(policy->cpu, MSR_IA32_PERF_CTL,
+ 			      pstate_funcs.get_val(cpu, target_pstate));
+ 	}
+ 	freqs.new = target_pstate * cpu->pstate.scaling;
+ 	cpufreq_freq_transition_end(policy, &freqs, false);
+ 
+ 	return 0;
+ }
+ 
+ static unsigned int intel_cpufreq_fast_switch(struct cpufreq_policy *policy,
+ 					      unsigned int target_freq)
+ {
+ 	struct cpudata *cpu = all_cpu_data[policy->cpu];
+ 	int target_pstate;
+ 
+ 	update_turbo_state();
+ 
+ 	target_pstate = DIV_ROUND_UP(target_freq, cpu->pstate.scaling);
+ 	target_pstate = intel_pstate_prepare_request(cpu, target_pstate);
+ 	intel_pstate_update_pstate(cpu, target_pstate);
+ 	return target_pstate * cpu->pstate.scaling;
+ }
+ 
+ static int intel_cpufreq_cpu_init(struct cpufreq_policy *policy)
+ {
+ 	int ret = __intel_pstate_cpu_init(policy);
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	policy->cpuinfo.transition_latency = INTEL_CPUFREQ_TRANSITION_LATENCY;
+ 	/* This reflects the intel_pstate_get_cpu_pstates() setting. */
+ 	policy->cur = policy->cpuinfo.min_freq;
+ 
+ 	return 0;
+ }
+ 
+ static struct cpufreq_driver intel_cpufreq = {
+ 	.flags		= CPUFREQ_CONST_LOOPS,
+ 	.verify		= intel_cpufreq_verify_policy,
+ 	.target		= intel_cpufreq_target,
+ 	.fast_switch	= intel_cpufreq_fast_switch,
+ 	.init		= intel_cpufreq_cpu_init,
+ 	.exit		= intel_pstate_cpu_exit,
+ 	.stop_cpu	= intel_cpufreq_stop_cpu,
+ 	.name		= "intel_cpufreq",
+ };
+ 
+ static struct cpufreq_driver *intel_pstate_driver = &intel_pstate;
+ 
+ static void intel_pstate_driver_cleanup(void)
+ {
+ 	unsigned int cpu;
+ 
+ 	get_online_cpus();
+ 	for_each_online_cpu(cpu) {
+ 		if (all_cpu_data[cpu]) {
+ 			if (intel_pstate_driver == &intel_pstate)
+ 				intel_pstate_clear_update_util_hook(cpu);
+ 
+ 			kfree(all_cpu_data[cpu]);
+ 			all_cpu_data[cpu] = NULL;
+ 		}
+ 	}
+ 	put_online_cpus();
+ }
+ 
+ static int intel_pstate_register_driver(void)
+ {
+ 	int ret;
+ 
+ 	intel_pstate_init_limits(&global);
+ 
+ 	ret = cpufreq_register_driver(intel_pstate_driver);
+ 	if (ret) {
+ 		intel_pstate_driver_cleanup();
+ 		return ret;
+ 	}
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 	driver_registered = true;
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	if (intel_pstate_driver == &intel_pstate && !hwp_active &&
+ 	    pstate_funcs.get_target_pstate != get_target_pstate_use_cpu_load)
+ 		intel_pstate_debug_expose_params();
+ 
+ 	return 0;
+ }
+ 
+ static int intel_pstate_unregister_driver(void)
+ {
+ 	if (hwp_active)
+ 		return -EBUSY;
+ 
+ 	if (intel_pstate_driver == &intel_pstate && !hwp_active &&
+ 	    pstate_funcs.get_target_pstate != get_target_pstate_use_cpu_load)
+ 		intel_pstate_debug_hide_params();
+ 
+ 	mutex_lock(&intel_pstate_limits_lock);
+ 	driver_registered = false;
+ 	mutex_unlock(&intel_pstate_limits_lock);
+ 
+ 	cpufreq_unregister_driver(intel_pstate_driver);
+ 	intel_pstate_driver_cleanup();
+ 
+ 	return 0;
+ }
+ 
+ static ssize_t intel_pstate_show_status(char *buf)
+ {
+ 	if (!driver_registered)
+ 		return sprintf(buf, "off\n");
+ 
+ 	return sprintf(buf, "%s\n", intel_pstate_driver == &intel_pstate ?
+ 					"active" : "passive");
+ }
+ 
+ static int intel_pstate_update_status(const char *buf, size_t size)
+ {
+ 	int ret;
+ 
+ 	if (size == 3 && !strncmp(buf, "off", size))
+ 		return driver_registered ?
+ 			intel_pstate_unregister_driver() : -EINVAL;
+ 
+ 	if (size == 6 && !strncmp(buf, "active", size)) {
+ 		if (driver_registered) {
+ 			if (intel_pstate_driver == &intel_pstate)
+ 				return 0;
+ 
+ 			ret = intel_pstate_unregister_driver();
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		intel_pstate_driver = &intel_pstate;
+ 		return intel_pstate_register_driver();
+ 	}
+ 
+ 	if (size == 7 && !strncmp(buf, "passive", size)) {
+ 		if (driver_registered) {
+ 			if (intel_pstate_driver != &intel_pstate)
+ 				return 0;
+ 
+ 			ret = intel_pstate_unregister_driver();
+ 			if (ret)
+ 				return ret;
+ 		}
+ 
+ 		intel_pstate_driver = &intel_cpufreq;
+ 		return intel_pstate_register_driver();
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
++>>>>>>> 64897b20eed2 (cpufreq: intel_pstate: Fix policy data management in passive mode)
  static int no_load __initdata;
  static int no_hwp __initdata;
  static int hwp_only __initdata;
* Unmerged path drivers/cpufreq/intel_pstate.c
