blk-mq: Restart a single queue if tag sets are shared

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Bart Van Assche <bart.vanassche@sandisk.com>
commit 6d8c6c0f97ad8a3517c42b179c1dc8e77397d0e2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/6d8c6c0f.failed

To improve scalability, if hardware queues are shared, restart
a single hardware queue in round-robin fashion. Rename
blk_mq_sched_restart_queues() to reflect the new semantics.
Remove blk_mq_sched_mark_restart_queue() because this function
has no callers. Remove flag QUEUE_FLAG_RESTART because this
patch removes the code that uses this flag.

	Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: Hannes Reinecke <hare@suse.com>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit 6d8c6c0f97ad8a3517c42b179c1dc8e77397d0e2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq-sched.c
#	block/blk-mq-sched.h
#	block/blk-mq.c
#	include/linux/blkdev.h
diff --cc block/blk-mq.c
index d9bfe0c6bc0e,572966f49596..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -354,18 -330,25 +354,27 @@@ out_queue_exit
  }
  EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
  
 -void __blk_mq_finish_request(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 -			     struct request *rq)
 +static void __blk_mq_free_request(struct blk_mq_hw_ctx *hctx,
 +				  struct blk_mq_ctx *ctx, struct request *rq)
  {
 -	const int sched_tag = rq->internal_tag;
 +	const int tag = rq->tag;
  	struct request_queue *q = rq->q;
  
 -	if (rq->rq_flags & RQF_MQ_INFLIGHT)
 +	if (rq->cmd_flags & REQ_MQ_INFLIGHT)
  		atomic_dec(&hctx->nr_active);
 -
 -	wbt_done(q->rq_wb, &rq->issue_stat);
 -	rq->rq_flags = 0;
 +	rq->cmd_flags = 0;
  
  	clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags);
++<<<<<<< HEAD
 +	blk_mq_put_tag(hctx, tag, &ctx->last_tag);
++=======
+ 	clear_bit(REQ_ATOM_POLL_SLEPT, &rq->atomic_flags);
+ 	if (rq->tag != -1)
+ 		blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
+ 	if (sched_tag != -1)
+ 		blk_mq_sched_completed_request(hctx, rq);
+ 	blk_mq_sched_restart(hctx);
++>>>>>>> 6d8c6c0f97ad (blk-mq: Restart a single queue if tag sets are shared)
  	blk_queue_exit(q);
  }
  
diff --cc include/linux/blkdev.h
index ba3405333171,7548f332121a..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -567,10 -603,13 +567,20 @@@ struct request_queue 
  #define QUEUE_FLAG_SAME_FORCE  18	/* force complete on same CPU */
  #define QUEUE_FLAG_DEAD        19	/* queue tear-down finished */
  #define QUEUE_FLAG_INIT_DONE   20	/* queue is initialized */
++<<<<<<< HEAD
 +#define QUEUE_FLAG_UNPRIV_SGIO 21	/* SG_IO free for unprivileged users */
 +#define QUEUE_FLAG_NO_SG_MERGE 22	/* don't attempt to merge SG segments*/
 +#define QUEUE_FLAG_SG_GAPS     23	/* queue doesn't support SG gaps */
 +#define QUEUE_FLAG_DAX         24	/* device supports DAX */
++=======
+ #define QUEUE_FLAG_NO_SG_MERGE 21	/* don't attempt to merge SG segments*/
+ #define QUEUE_FLAG_POLL	       22	/* IO polling enabled if set */
+ #define QUEUE_FLAG_WC	       23	/* Write back caching */
+ #define QUEUE_FLAG_FUA	       24	/* device supports FUA writes */
+ #define QUEUE_FLAG_FLUSH_NQ    25	/* flush not queueuable */
+ #define QUEUE_FLAG_DAX         26	/* device supports DAX */
+ #define QUEUE_FLAG_STATS       27	/* track rq completion times */
++>>>>>>> 6d8c6c0f97ad (blk-mq: Restart a single queue if tag sets are shared)
  
  #define QUEUE_FLAG_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
  				 (1 << QUEUE_FLAG_STACKABLE)	|	\
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-sched.h
* Unmerged path block/blk-mq-sched.c
* Unmerged path block/blk-mq-sched.h
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blkdev.h
