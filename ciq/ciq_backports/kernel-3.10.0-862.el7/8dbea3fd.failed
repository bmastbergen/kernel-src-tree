iommu/amd: Introduce amd_iommu_update_ga()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
Rebuild_CHGLOG: - [iommu] amd: Introduce amd_iommu_update_ga() (Jerry Snitselaar) [1411581]
Rebuild_FUZZ: 92.31%
commit-author Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
commit 8dbea3fd7becd4af8ca882c3132be4b1a857e301
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/8dbea3fd.failed

Introduces a new IOMMU API, amd_iommu_update_ga(), which allows
KVM (SVM) to update existing posted interrupt IOMMU IRTE when
load/unload vcpu.

	Signed-off-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 8dbea3fd7becd4af8ca882c3132be4b1a857e301)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/amd_iommu.c
#	drivers/iommu/amd_iommu_types.h
diff --cc drivers/iommu/amd_iommu.c
index c50870eecb8e,089e1ed4deec..000000000000
--- a/drivers/iommu/amd_iommu.c
+++ b/drivers/iommu/amd_iommu.c
@@@ -4052,12 -3998,314 +4052,322 @@@ struct irq_remap_ops amd_iommu_irq_ops 
  	.disable		= amd_iommu_disable,
  	.reenable		= amd_iommu_reenable,
  	.enable_faulting	= amd_iommu_enable_faulting,
 -	.get_ir_irq_domain	= get_ir_irq_domain,
 -	.get_irq_domain		= get_irq_domain,
 +	.setup_ioapic_entry	= setup_ioapic_entry,
 +	.set_affinity		= set_affinity,
 +	.free_irq		= free_irq,
 +	.compose_msi_msg	= compose_msi_msg,
 +	.msi_alloc_irq		= msi_alloc_irq,
 +	.msi_setup_irq		= msi_setup_irq,
 +	.alloc_hpet_msi		= alloc_hpet_msi,
  };
++<<<<<<< HEAD
++=======
+ 
+ static void irq_remapping_prepare_irte(struct amd_ir_data *data,
+ 				       struct irq_cfg *irq_cfg,
+ 				       struct irq_alloc_info *info,
+ 				       int devid, int index, int sub_handle)
+ {
+ 	struct irq_2_irte *irte_info = &data->irq_2_irte;
+ 	struct msi_msg *msg = &data->msi_entry;
+ 	struct IO_APIC_route_entry *entry;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[devid];
+ 
+ 	if (!iommu)
+ 		return;
+ 
+ 	data->irq_2_irte.devid = devid;
+ 	data->irq_2_irte.index = index + sub_handle;
+ 	iommu->irte_ops->prepare(data->entry, apic->irq_delivery_mode,
+ 				 apic->irq_dest_mode, irq_cfg->vector,
+ 				 irq_cfg->dest_apicid);
+ 
+ 	switch (info->type) {
+ 	case X86_IRQ_ALLOC_TYPE_IOAPIC:
+ 		/* Setup IOAPIC entry */
+ 		entry = info->ioapic_entry;
+ 		info->ioapic_entry = NULL;
+ 		memset(entry, 0, sizeof(*entry));
+ 		entry->vector        = index;
+ 		entry->mask          = 0;
+ 		entry->trigger       = info->ioapic_trigger;
+ 		entry->polarity      = info->ioapic_polarity;
+ 		/* Mask level triggered irqs. */
+ 		if (info->ioapic_trigger)
+ 			entry->mask = 1;
+ 		break;
+ 
+ 	case X86_IRQ_ALLOC_TYPE_HPET:
+ 	case X86_IRQ_ALLOC_TYPE_MSI:
+ 	case X86_IRQ_ALLOC_TYPE_MSIX:
+ 		msg->address_hi = MSI_ADDR_BASE_HI;
+ 		msg->address_lo = MSI_ADDR_BASE_LO;
+ 		msg->data = irte_info->index;
+ 		break;
+ 
+ 	default:
+ 		BUG_ON(1);
+ 		break;
+ 	}
+ }
+ 
+ struct amd_irte_ops irte_32_ops = {
+ 	.prepare = irte_prepare,
+ 	.activate = irte_activate,
+ 	.deactivate = irte_deactivate,
+ 	.set_affinity = irte_set_affinity,
+ 	.set_allocated = irte_set_allocated,
+ 	.is_allocated = irte_is_allocated,
+ 	.clear_allocated = irte_clear_allocated,
+ };
+ 
+ struct amd_irte_ops irte_128_ops = {
+ 	.prepare = irte_ga_prepare,
+ 	.activate = irte_ga_activate,
+ 	.deactivate = irte_ga_deactivate,
+ 	.set_affinity = irte_ga_set_affinity,
+ 	.set_allocated = irte_ga_set_allocated,
+ 	.is_allocated = irte_ga_is_allocated,
+ 	.clear_allocated = irte_ga_clear_allocated,
+ };
+ 
+ static int irq_remapping_alloc(struct irq_domain *domain, unsigned int virq,
+ 			       unsigned int nr_irqs, void *arg)
+ {
+ 	struct irq_alloc_info *info = arg;
+ 	struct irq_data *irq_data;
+ 	struct amd_ir_data *data = NULL;
+ 	struct irq_cfg *cfg;
+ 	int i, ret, devid;
+ 	int index = -1;
+ 
+ 	if (!info)
+ 		return -EINVAL;
+ 	if (nr_irqs > 1 && info->type != X86_IRQ_ALLOC_TYPE_MSI &&
+ 	    info->type != X86_IRQ_ALLOC_TYPE_MSIX)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * With IRQ remapping enabled, don't need contiguous CPU vectors
+ 	 * to support multiple MSI interrupts.
+ 	 */
+ 	if (info->type == X86_IRQ_ALLOC_TYPE_MSI)
+ 		info->flags &= ~X86_IRQ_ALLOC_CONTIGUOUS_VECTORS;
+ 
+ 	devid = get_devid(info);
+ 	if (devid < 0)
+ 		return -EINVAL;
+ 
+ 	ret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, arg);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (info->type == X86_IRQ_ALLOC_TYPE_IOAPIC) {
+ 		if (get_irq_table(devid, true))
+ 			index = info->ioapic_pin;
+ 		else
+ 			ret = -ENOMEM;
+ 	} else {
+ 		index = alloc_irq_index(devid, nr_irqs);
+ 	}
+ 	if (index < 0) {
+ 		pr_warn("Failed to allocate IRTE\n");
+ 		goto out_free_parent;
+ 	}
+ 
+ 	for (i = 0; i < nr_irqs; i++) {
+ 		irq_data = irq_domain_get_irq_data(domain, virq + i);
+ 		cfg = irqd_cfg(irq_data);
+ 		if (!irq_data || !cfg) {
+ 			ret = -EINVAL;
+ 			goto out_free_data;
+ 		}
+ 
+ 		ret = -ENOMEM;
+ 		data = kzalloc(sizeof(*data), GFP_KERNEL);
+ 		if (!data)
+ 			goto out_free_data;
+ 
+ 		if (!AMD_IOMMU_GUEST_IR_GA(amd_iommu_guest_ir))
+ 			data->entry = kzalloc(sizeof(union irte), GFP_KERNEL);
+ 		else
+ 			data->entry = kzalloc(sizeof(struct irte_ga),
+ 						     GFP_KERNEL);
+ 		if (!data->entry) {
+ 			kfree(data);
+ 			goto out_free_data;
+ 		}
+ 
+ 		irq_data->hwirq = (devid << 16) + i;
+ 		irq_data->chip_data = data;
+ 		irq_data->chip = &amd_ir_chip;
+ 		irq_remapping_prepare_irte(data, cfg, info, devid, index, i);
+ 		irq_set_status_flags(virq + i, IRQ_MOVE_PCNTXT);
+ 	}
+ 
+ 	return 0;
+ 
+ out_free_data:
+ 	for (i--; i >= 0; i--) {
+ 		irq_data = irq_domain_get_irq_data(domain, virq + i);
+ 		if (irq_data)
+ 			kfree(irq_data->chip_data);
+ 	}
+ 	for (i = 0; i < nr_irqs; i++)
+ 		free_irte(devid, index + i);
+ out_free_parent:
+ 	irq_domain_free_irqs_common(domain, virq, nr_irqs);
+ 	return ret;
+ }
+ 
+ static void irq_remapping_free(struct irq_domain *domain, unsigned int virq,
+ 			       unsigned int nr_irqs)
+ {
+ 	struct irq_2_irte *irte_info;
+ 	struct irq_data *irq_data;
+ 	struct amd_ir_data *data;
+ 	int i;
+ 
+ 	for (i = 0; i < nr_irqs; i++) {
+ 		irq_data = irq_domain_get_irq_data(domain, virq  + i);
+ 		if (irq_data && irq_data->chip_data) {
+ 			data = irq_data->chip_data;
+ 			irte_info = &data->irq_2_irte;
+ 			free_irte(irte_info->devid, irte_info->index);
+ 			kfree(data->entry);
+ 			kfree(data);
+ 		}
+ 	}
+ 	irq_domain_free_irqs_common(domain, virq, nr_irqs);
+ }
+ 
+ static void irq_remapping_activate(struct irq_domain *domain,
+ 				   struct irq_data *irq_data)
+ {
+ 	struct amd_ir_data *data = irq_data->chip_data;
+ 	struct irq_2_irte *irte_info = &data->irq_2_irte;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[irte_info->devid];
+ 
+ 	if (iommu)
+ 		iommu->irte_ops->activate(data->entry, irte_info->devid,
+ 					  irte_info->index);
+ }
+ 
+ static void irq_remapping_deactivate(struct irq_domain *domain,
+ 				     struct irq_data *irq_data)
+ {
+ 	struct amd_ir_data *data = irq_data->chip_data;
+ 	struct irq_2_irte *irte_info = &data->irq_2_irte;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[irte_info->devid];
+ 
+ 	if (iommu)
+ 		iommu->irte_ops->deactivate(data->entry, irte_info->devid,
+ 					    irte_info->index);
+ }
+ 
+ static struct irq_domain_ops amd_ir_domain_ops = {
+ 	.alloc = irq_remapping_alloc,
+ 	.free = irq_remapping_free,
+ 	.activate = irq_remapping_activate,
+ 	.deactivate = irq_remapping_deactivate,
+ };
+ 
+ static int amd_ir_set_affinity(struct irq_data *data,
+ 			       const struct cpumask *mask, bool force)
+ {
+ 	struct amd_ir_data *ir_data = data->chip_data;
+ 	struct irq_2_irte *irte_info = &ir_data->irq_2_irte;
+ 	struct irq_cfg *cfg = irqd_cfg(data);
+ 	struct irq_data *parent = data->parent_data;
+ 	struct amd_iommu *iommu = amd_iommu_rlookup_table[irte_info->devid];
+ 	int ret;
+ 
+ 	if (!iommu)
+ 		return -ENODEV;
+ 
+ 	ret = parent->chip->irq_set_affinity(parent, mask, force);
+ 	if (ret < 0 || ret == IRQ_SET_MASK_OK_DONE)
+ 		return ret;
+ 
+ 	/*
+ 	 * Atomically updates the IRTE with the new destination, vector
+ 	 * and flushes the interrupt entry cache.
+ 	 */
+ 	iommu->irte_ops->set_affinity(ir_data->entry, irte_info->devid,
+ 			    irte_info->index, cfg->vector, cfg->dest_apicid);
+ 
+ 	/*
+ 	 * After this point, all the interrupts will start arriving
+ 	 * at the new destination. So, time to cleanup the previous
+ 	 * vector allocation.
+ 	 */
+ 	send_cleanup_vector(cfg);
+ 
+ 	return IRQ_SET_MASK_OK_DONE;
+ }
+ 
+ static void ir_compose_msi_msg(struct irq_data *irq_data, struct msi_msg *msg)
+ {
+ 	struct amd_ir_data *ir_data = irq_data->chip_data;
+ 
+ 	*msg = ir_data->msi_entry;
+ }
+ 
+ static struct irq_chip amd_ir_chip = {
+ 	.irq_ack = ir_ack_apic_edge,
+ 	.irq_set_affinity = amd_ir_set_affinity,
+ 	.irq_compose_msi_msg = ir_compose_msi_msg,
+ };
+ 
+ int amd_iommu_create_irq_domain(struct amd_iommu *iommu)
+ {
+ 	iommu->ir_domain = irq_domain_add_tree(NULL, &amd_ir_domain_ops, iommu);
+ 	if (!iommu->ir_domain)
+ 		return -ENOMEM;
+ 
+ 	iommu->ir_domain->parent = arch_get_ir_parent_domain();
+ 	iommu->msi_domain = arch_create_msi_irq_domain(iommu->ir_domain);
+ 
+ 	return 0;
+ }
+ 
+ int amd_iommu_update_ga(int cpu, bool is_run, void *data)
+ {
+ 	unsigned long flags;
+ 	struct amd_iommu *iommu;
+ 	struct irq_remap_table *irt;
+ 	struct amd_ir_data *ir_data = (struct amd_ir_data *)data;
+ 	int devid = ir_data->irq_2_irte.devid;
+ 	struct irte_ga *entry = (struct irte_ga *) ir_data->entry;
+ 	struct irte_ga *ref = (struct irte_ga *) ir_data->ref;
+ 
+ 	if (!AMD_IOMMU_GUEST_IR_VAPIC(amd_iommu_guest_ir) ||
+ 	    !ref || !entry || !entry->lo.fields_vapic.guest_mode)
+ 		return 0;
+ 
+ 	iommu = amd_iommu_rlookup_table[devid];
+ 	if (!iommu)
+ 		return -ENODEV;
+ 
+ 	irt = get_irq_table(devid, false);
+ 	if (!irt)
+ 		return -ENODEV;
+ 
+ 	spin_lock_irqsave(&irt->lock, flags);
+ 
+ 	if (ref->lo.fields_vapic.guest_mode) {
+ 		if (cpu >= 0)
+ 			ref->lo.fields_vapic.destination = cpu;
+ 		ref->lo.fields_vapic.is_run = is_run;
+ 		barrier();
+ 	}
+ 
+ 	spin_unlock_irqrestore(&irt->lock, flags);
+ 
+ 	iommu_flush_irt(iommu, devid);
+ 	iommu_completion_wait(iommu);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(amd_iommu_update_ga);
++>>>>>>> 8dbea3fd7bec (iommu/amd: Introduce amd_iommu_update_ga())
  #endif
diff --cc drivers/iommu/amd_iommu_types.h
index c9e00df52a4f,60018a8fb655..000000000000
--- a/drivers/iommu/amd_iommu_types.h
+++ b/drivers/iommu/amd_iommu_types.h
@@@ -668,4 -722,111 +668,114 @@@ static inline int get_hpet_devid(int id
  	return -EINVAL;
  }
  
++<<<<<<< HEAD
++=======
+ enum amd_iommu_intr_mode_type {
+ 	AMD_IOMMU_GUEST_IR_LEGACY,
+ 
+ 	/* This mode is not visible to users. It is used when
+ 	 * we cannot fully enable vAPIC and fallback to only support
+ 	 * legacy interrupt remapping via 128-bit IRTE.
+ 	 */
+ 	AMD_IOMMU_GUEST_IR_LEGACY_GA,
+ 	AMD_IOMMU_GUEST_IR_VAPIC,
+ };
+ 
+ #define AMD_IOMMU_GUEST_IR_GA(x)	(x == AMD_IOMMU_GUEST_IR_VAPIC || \
+ 					 x == AMD_IOMMU_GUEST_IR_LEGACY_GA)
+ 
+ #define AMD_IOMMU_GUEST_IR_VAPIC(x)	(x == AMD_IOMMU_GUEST_IR_VAPIC)
+ 
+ union irte {
+ 	u32 val;
+ 	struct {
+ 		u32 valid	: 1,
+ 		    no_fault	: 1,
+ 		    int_type	: 3,
+ 		    rq_eoi	: 1,
+ 		    dm		: 1,
+ 		    rsvd_1	: 1,
+ 		    destination	: 8,
+ 		    vector	: 8,
+ 		    rsvd_2	: 8;
+ 	} fields;
+ };
+ 
+ union irte_ga_lo {
+ 	u64 val;
+ 
+ 	/* For int remapping */
+ 	struct {
+ 		u64 valid	: 1,
+ 		    no_fault	: 1,
+ 		    /* ------ */
+ 		    int_type	: 3,
+ 		    rq_eoi	: 1,
+ 		    dm		: 1,
+ 		    /* ------ */
+ 		    guest_mode	: 1,
+ 		    destination	: 8,
+ 		    rsvd	: 48;
+ 	} fields_remap;
+ 
+ 	/* For guest vAPIC */
+ 	struct {
+ 		u64 valid	: 1,
+ 		    no_fault	: 1,
+ 		    /* ------ */
+ 		    ga_log_intr	: 1,
+ 		    rsvd1	: 3,
+ 		    is_run	: 1,
+ 		    /* ------ */
+ 		    guest_mode	: 1,
+ 		    destination	: 8,
+ 		    rsvd2	: 16,
+ 		    ga_tag	: 32;
+ 	} fields_vapic;
+ };
+ 
+ union irte_ga_hi {
+ 	u64 val;
+ 	struct {
+ 		u64 vector	: 8,
+ 		    rsvd_1	: 4,
+ 		    ga_root_ptr	: 40,
+ 		    rsvd_2	: 12;
+ 	} fields;
+ };
+ 
+ struct irte_ga {
+ 	union irte_ga_lo lo;
+ 	union irte_ga_hi hi;
+ };
+ 
+ struct irq_2_irte {
+ 	u16 devid; /* Device ID for IRTE table */
+ 	u16 index; /* Index into IRTE table*/
+ };
+ 
+ struct amd_ir_data {
+ 	struct irq_2_irte irq_2_irte;
+ 	struct msi_msg msi_entry;
+ 	void *entry;    /* Pointer to union irte or struct irte_ga */
+ 	void *ref;      /* Pointer to the actual irte */
+ };
+ 
+ struct amd_irte_ops {
+ 	void (*prepare)(void *, u32, u32, u8, u32);
+ 	void (*activate)(void *, u16, u16);
+ 	void (*deactivate)(void *, u16, u16);
+ 	void (*set_affinity)(void *, u16, u16, u8, u32);
+ 	void *(*get)(struct irq_remap_table *, int);
+ 	void (*set_allocated)(struct irq_remap_table *, int);
+ 	bool (*is_allocated)(struct irq_remap_table *, int);
+ 	void (*clear_allocated)(struct irq_remap_table *, int);
+ };
+ 
+ #ifdef CONFIG_IRQ_REMAP
+ extern struct amd_irte_ops irte_32_ops;
+ extern struct amd_irte_ops irte_128_ops;
+ #endif
+ 
++>>>>>>> 8dbea3fd7bec (iommu/amd: Introduce amd_iommu_update_ga())
  #endif /* _ASM_X86_AMD_IOMMU_TYPES_H */
* Unmerged path drivers/iommu/amd_iommu.c
* Unmerged path drivers/iommu/amd_iommu_types.h
diff --git a/include/linux/amd-iommu.h b/include/linux/amd-iommu.h
index 465d096a5f4b..d8d48aca0eb7 100644
--- a/include/linux/amd-iommu.h
+++ b/include/linux/amd-iommu.h
@@ -179,6 +179,9 @@ static inline int amd_iommu_detect(void) { return -ENODEV; }
 /* IOMMU AVIC Function */
 extern int amd_iommu_register_ga_log_notifier(int (*notifier)(u32));
 
+extern int
+amd_iommu_update_ga(int cpu, bool is_run, void *data);
+
 #else /* defined(CONFIG_AMD_IOMMU) && defined(CONFIG_IRQ_REMAP) */
 
 static inline int
@@ -187,6 +190,12 @@ amd_iommu_register_ga_log_notifier(int (*notifier)(u32))
 	return 0;
 }
 
+static inline int
+amd_iommu_update_ga(int cpu, bool is_run, void *data)
+{
+	return 0;
+}
+
 #endif /* defined(CONFIG_AMD_IOMMU) && defined(CONFIG_IRQ_REMAP) */
 
 #endif /* _ASM_X86_AMD_IOMMU_H */
