IB/ipoib: Use NAPI in UD/TX flows

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Erez Shitrit <erezsh@mellanox.com>
commit 8966e28d2e40cfc9f694bd02dabc49afb78d7160
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/8966e28d.failed

Instead of explicit call to poll_cq of the tx ring, use the NAPI mechanism
to handle the completions of each packet that has been sent to the HW.

The next major changes were taken:
 * The driver init completion function in the creation of the send CQ,
   that function triggers the napi scheduling.
 * The driver uses CQ for RX for both modes UD and CM, and CQ for TX
   for CM and UD.

	Cc: Kamal Heib <kamalh@mellanox.com>
	Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
	Reviewed-by: Alex Vesker <valex@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 8966e28d2e40cfc9f694bd02dabc49afb78d7160)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/ulp/ipoib/ipoib_ib.c
#	drivers/infiniband/ulp/ipoib/ipoib_main.c
diff --cc drivers/infiniband/ulp/ipoib/ipoib_ib.c
index 7efa57b9215e,3b96cdaf9a83..000000000000
--- a/drivers/infiniband/ulp/ipoib/ipoib_ib.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_ib.c
@@@ -622,8 -642,13 +642,13 @@@ void ipoib_send(struct net_device *dev
  	skb_orphan(skb);
  	skb_dst_drop(skb);
  
+ 	if (netif_queue_stopped(dev))
+ 		if (ib_req_notify_cq(priv->send_cq, IB_CQ_NEXT_COMP |
+ 				     IB_CQ_REPORT_MISSED_EVENTS))
+ 			ipoib_warn(priv, "request notify on send CQ failed\n");
+ 
  	rc = post_send(priv, priv->tx_head & (ipoib_sendq_size - 1),
 -		       address, dqpn, tx_req, phead, hlen);
 +		       address->ah, dqpn, tx_req, phead, hlen);
  	if (unlikely(rc)) {
  		ipoib_warn(priv, "post_send failed, error %d\n", rc);
  		++dev->stats.tx_errors;
@@@ -634,13 -659,11 +659,17 @@@
  	} else {
  		netif_trans_update(dev);
  
 -		rc = priv->tx_head;
 +		address->last_send = priv->tx_head;
  		++priv->tx_head;
  	}
++<<<<<<< HEAD
 +
 +	if (unlikely(priv->tx_head - priv->tx_tail > MAX_SEND_CQE))
 +		while (poll_tx(priv))
 +			; /* nothing */
++=======
+ 	return rc;
++>>>>>>> 8966e28d2e40 (IB/ipoib: Use NAPI in UD/TX flows)
  }
  
  static void __ipoib_reap_ah(struct net_device *dev)
@@@ -736,8 -775,8 +781,13 @@@ int ipoib_ib_dev_stop_default(struct ne
  	struct ipoib_tx_buf *tx_req;
  	int i;
  
++<<<<<<< HEAD
 +	if (test_and_clear_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))
 +		napi_disable(&priv->napi);
++=======
+ 	if (test_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))
+ 		ipoib_napi_disable(dev);
++>>>>>>> 8966e28d2e40 (IB/ipoib: Use NAPI in UD/TX flows)
  
  	ipoib_cm_dev_stop(dev);
  
@@@ -813,11 -854,6 +862,14 @@@ int ipoib_ib_dev_stop(struct net_devic
  	return 0;
  }
  
++<<<<<<< HEAD
 +void ipoib_ib_tx_timer_func(unsigned long ctx)
 +{
 +	drain_tx_cq((struct net_device *)ctx);
 +}
 +
++=======
++>>>>>>> 8966e28d2e40 (IB/ipoib: Use NAPI in UD/TX flows)
  int ipoib_ib_dev_open_default(struct net_device *dev)
  {
  	struct ipoib_dev_priv *priv = ipoib_priv(dev);
@@@ -838,17 -874,14 +890,22 @@@
  	ret = ipoib_cm_dev_open(dev);
  	if (ret) {
  		ipoib_warn(priv, "ipoib_cm_dev_open returned %d\n", ret);
 -		goto out;
 +		goto dev_stop;
  	}
  
++<<<<<<< HEAD
 +	if (!test_and_set_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))
 +		napi_enable(&priv->napi);
++=======
+ 	if (!test_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))
+ 		ipoib_napi_enable(dev);
++>>>>>>> 8966e28d2e40 (IB/ipoib: Use NAPI in UD/TX flows)
  
  	return 0;
 -out:
 +dev_stop:
 +	if (!test_and_set_bit(IPOIB_FLAG_INITIALIZED, &priv->flags))
 +		napi_enable(&priv->napi);
 +	ipoib_ib_dev_stop(dev);
  	return -1;
  }
  
diff --cc drivers/infiniband/ulp/ipoib/ipoib_main.c
index d4551f56d0a2,12b7f911f0e5..000000000000
--- a/drivers/infiniband/ulp/ipoib/ipoib_main.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_main.c
@@@ -1699,6 -1653,8 +1715,11 @@@ static int ipoib_dev_init_default(struc
  {
  	struct ipoib_dev_priv *priv = ipoib_priv(dev);
  
++<<<<<<< HEAD
++=======
+ 	ipoib_napi_add(dev);
+ 
++>>>>>>> 8966e28d2e40 (IB/ipoib: Use NAPI in UD/TX flows)
  	/* Allocate RX/TX "rings" to hold queued skbs */
  	priv->rx_ring =	kzalloc(ipoib_recvq_size * sizeof *priv->rx_ring,
  				GFP_KERNEL);
@@@ -1719,8 -1676,10 +1740,15 @@@
  		goto out_tx_ring_cleanup;
  	}
  
++<<<<<<< HEAD
 +	setup_timer(&priv->poll_timer, ipoib_ib_tx_timer_func,
 +		    (unsigned long)dev);
++=======
+ 	/* after qp created set dev address */
+ 	priv->dev->dev_addr[1] = (priv->qp->qp_num >> 16) & 0xff;
+ 	priv->dev->dev_addr[2] = (priv->qp->qp_num >>  8) & 0xff;
+ 	priv->dev->dev_addr[3] = (priv->qp->qp_num) & 0xff;
++>>>>>>> 8966e28d2e40 (IB/ipoib: Use NAPI in UD/TX flows)
  
  	return 0;
  
diff --git a/drivers/infiniband/ulp/ipoib/ipoib.h b/drivers/infiniband/ulp/ipoib/ipoib.h
index 14297c8d0195..aee15a8843a3 100644
--- a/drivers/infiniband/ulp/ipoib/ipoib.h
+++ b/drivers/infiniband/ulp/ipoib/ipoib.h
@@ -328,7 +328,8 @@ struct ipoib_dev_priv {
 
 	struct net_device *dev;
 
-	struct napi_struct napi;
+	struct napi_struct send_napi;
+	struct napi_struct recv_napi;
 
 	unsigned long flags;
 
@@ -404,7 +405,6 @@ struct ipoib_dev_priv {
 #endif
 	u64	hca_caps;
 	struct ipoib_ethtool_st ethtool;
-	struct timer_list poll_timer;
 	unsigned max_send_sge;
 	bool sm_fullmember_sendonly_support;
 };
@@ -470,9 +470,10 @@ extern struct workqueue_struct *ipoib_workqueue;
 
 /* functions */
 
-int ipoib_poll(struct napi_struct *napi, int budget);
-void ipoib_ib_completion(struct ib_cq *cq, void *dev_ptr);
-void ipoib_send_comp_handler(struct ib_cq *cq, void *dev_ptr);
+int ipoib_rx_poll(struct napi_struct *napi, int budget);
+int ipoib_tx_poll(struct napi_struct *napi, int budget);
+void ipoib_ib_rx_completion(struct ib_cq *cq, void *ctx_ptr);
+void ipoib_ib_tx_completion(struct ib_cq *cq, void *ctx_ptr);
 
 struct ipoib_ah *ipoib_create_ah(struct net_device *dev,
 				 struct ib_pd *pd, struct ib_ah_attr *attr);
diff --git a/drivers/infiniband/ulp/ipoib/ipoib_cm.c b/drivers/infiniband/ulp/ipoib/ipoib_cm.c
index c831c3332eea..7d64e596cfb6 100644
--- a/drivers/infiniband/ulp/ipoib/ipoib_cm.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_cm.c
@@ -755,30 +755,35 @@ void ipoib_cm_send(struct net_device *dev, struct sk_buff *skb, struct ipoib_cm_
 		return;
 	}
 
+	if ((priv->tx_head - priv->tx_tail) == ipoib_sendq_size - 1) {
+		ipoib_dbg(priv, "TX ring 0x%x full, stopping kernel net queue\n",
+			  tx->qp->qp_num);
+		netif_stop_queue(dev);
+	}
+
 	skb_orphan(skb);
 	skb_dst_drop(skb);
 
+	if (netif_queue_stopped(dev))
+		if (ib_req_notify_cq(priv->send_cq, IB_CQ_NEXT_COMP |
+				     IB_CQ_REPORT_MISSED_EVENTS)) {
+			ipoib_warn(priv, "IPoIB/CM:request notify on send CQ failed\n");
+			napi_schedule(&priv->send_napi);
+		}
+
 	rc = post_send(priv, tx, tx->tx_head & (ipoib_sendq_size - 1), tx_req);
 	if (unlikely(rc)) {
-		ipoib_warn(priv, "post_send failed, error %d\n", rc);
+		ipoib_warn(priv, "IPoIB/CM:post_send failed, error %d\n", rc);
 		++dev->stats.tx_errors;
 		ipoib_dma_unmap_tx(priv, tx_req);
 		dev_kfree_skb_any(skb);
+
+		if (netif_queue_stopped(dev))
+			netif_wake_queue(dev);
 	} else {
 		netif_trans_update(dev);
 		++tx->tx_head;
 		++priv->tx_head;
-		if ((priv->tx_head - priv->tx_tail) == ipoib_sendq_size) {
-			ipoib_dbg(priv, "TX ring 0x%x full, stopping kernel net queue\n",
-				  tx->qp->qp_num);
-			netif_stop_queue(dev);
-			rc = ib_req_notify_cq(priv->send_cq,
-				IB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS);
-			if (rc < 0)
-				ipoib_warn(priv, "request notify on send CQ failed\n");
-			else if (rc)
-				ipoib_send_comp_handler(priv->send_cq, dev);
-		}
 	}
 }
 
@@ -813,9 +818,10 @@ void ipoib_cm_handle_tx_wc(struct net_device *dev, struct ib_wc *wc)
 
 	++tx->tx_tail;
 	++priv->tx_tail;
-	if (unlikely((priv->tx_head - priv->tx_tail) == ipoib_sendq_size >> 1) &&
-	    netif_queue_stopped(dev) &&
-	    test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags))
+
+	if (unlikely(netif_queue_stopped(dev) &&
+		     (priv->tx_head - priv->tx_tail) <= ipoib_sendq_size >> 1 &&
+		     test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags)))
 		netif_wake_queue(dev);
 
 	if (wc->status != IB_WC_SUCCESS &&
@@ -1044,7 +1050,7 @@ static struct ib_qp *ipoib_cm_create_tx_qp(struct net_device *dev, struct ipoib_
 {
 	struct ipoib_dev_priv *priv = ipoib_priv(dev);
 	struct ib_qp_init_attr attr = {
-		.send_cq		= priv->recv_cq,
+		.send_cq		= priv->send_cq,
 		.recv_cq		= priv->recv_cq,
 		.srq			= priv->cm.srq,
 		.cap.max_send_wr	= ipoib_sendq_size,
@@ -1221,9 +1227,9 @@ timeout:
 		tx_req = &p->tx_ring[p->tx_tail & (ipoib_sendq_size - 1)];
 		ipoib_dma_unmap_tx(priv, tx_req);
 		dev_kfree_skb_any(tx_req->skb);
+		netif_tx_lock_bh(p->dev);
 		++p->tx_tail;
 		++priv->tx_tail;
-		netif_tx_lock_bh(p->dev);
 		if (unlikely(priv->tx_head - priv->tx_tail == ipoib_sendq_size >> 1) &&
 		    netif_queue_stopped(p->dev) &&
 		    test_bit(IPOIB_FLAG_ADMIN_UP, &priv->flags))
* Unmerged path drivers/infiniband/ulp/ipoib/ipoib_ib.c
* Unmerged path drivers/infiniband/ulp/ipoib/ipoib_main.c
diff --git a/drivers/infiniband/ulp/ipoib/ipoib_verbs.c b/drivers/infiniband/ulp/ipoib/ipoib_verbs.c
index 87b6f205d1fc..f25eefb78837 100644
--- a/drivers/infiniband/ulp/ipoib/ipoib_verbs.c
+++ b/drivers/infiniband/ulp/ipoib/ipoib_verbs.c
@@ -144,7 +144,7 @@ int ipoib_transport_dev_init(struct net_device *dev, struct ib_device *ca)
 	};
 	struct ib_cq_init_attr cq_attr = {};
 
-	int ret, size;
+	int ret, size, req_vec;
 	int i;
 
 	size = ipoib_recvq_size + 1;
@@ -159,17 +159,21 @@ int ipoib_transport_dev_init(struct net_device *dev, struct ib_device *ca)
 		if (ret != -ENOSYS)
 			return -ENODEV;
 
+	req_vec = (priv->port - 1) * 2;
+
 	cq_attr.cqe = size;
-	priv->recv_cq = ib_create_cq(priv->ca, ipoib_ib_completion, NULL,
-				     dev, &cq_attr);
+	cq_attr.comp_vector = req_vec % priv->ca->num_comp_vectors;
+	priv->recv_cq = ib_create_cq(priv->ca, ipoib_ib_rx_completion, NULL,
+				     priv, &cq_attr);
 	if (IS_ERR(priv->recv_cq)) {
 		printk(KERN_WARNING "%s: failed to create receive CQ\n", ca->name);
 		goto out_cm_dev_cleanup;
 	}
 
 	cq_attr.cqe = ipoib_sendq_size;
-	priv->send_cq = ib_create_cq(priv->ca, ipoib_send_comp_handler, NULL,
-				     dev, &cq_attr);
+	cq_attr.comp_vector = (req_vec + 1) % priv->ca->num_comp_vectors;
+	priv->send_cq = ib_create_cq(priv->ca, ipoib_ib_tx_completion, NULL,
+				     priv, &cq_attr);
 	if (IS_ERR(priv->send_cq)) {
 		printk(KERN_WARNING "%s: failed to create send CQ\n", ca->name);
 		goto out_free_recv_cq;
@@ -196,6 +200,9 @@ int ipoib_transport_dev_init(struct net_device *dev, struct ib_device *ca)
 		goto out_free_send_cq;
 	}
 
+	if (ib_req_notify_cq(priv->send_cq, IB_CQ_NEXT_COMP))
+		goto out_free_send_cq;
+
 	for (i = 0; i < MAX_SKB_FRAGS + 1; ++i)
 		priv->tx_sge[i].lkey = priv->pd->local_dma_lkey;
 
