IB/core: Change completion channel to use the reworked objects schema

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Matan Barak <matanb@mellanox.com>
commit 1e7710f3f6563940bb6bbc94aa8eadfd344a86af
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/1e7710f3.failed

This patch adds the standard fd based type - completion_channel.
The completion_channel is now prefixed with ib_uobject, similarly
to the rest of the uobjects.
This requires a few changes:
(1) We define a new completion channel fd based object type.
(2) completion_event and async_event are now two different types.
    This means they use different fops.
(3) We release the completion_channel exactly as we release other
    idr based objects.
(4) Since ib_uobjects are already kref-ed, we only add the kref to the
    async event.

A fd object requires filling out several parameters. Its op pointer
should point to uverbs_fd_ops and its size should be at least the
size if ib_uobject. We use a macro to make the type declaration
easier.

	Signed-off-by: Matan Barak <matanb@mellanox.com>
	Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
	Signed-off-by: Doug Ledford <dledford@redhat.com>
(cherry picked from commit 1e7710f3f6563940bb6bbc94aa8eadfd344a86af)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/uverbs.h
#	drivers/infiniband/core/uverbs_cmd.c
#	drivers/infiniband/core/uverbs_std_types.c
#	include/rdma/uverbs_std_types.h
#	include/rdma/uverbs_types.h
diff --cc drivers/infiniband/core/uverbs.h
index e1bedf0bac04,826f82748718..000000000000
--- a/drivers/infiniband/core/uverbs.h
+++ b/drivers/infiniband/core/uverbs.h
@@@ -120,9 -128,13 +128,9 @@@ struct ib_uverbs_file 
  	struct ib_uverbs_device		       *device;
  	struct ib_ucontext		       *ucontext;
  	struct ib_event_handler			event_handler;
- 	struct ib_uverbs_event_file	       *async_file;
+ 	struct ib_uverbs_async_event_file       *async_file;
  	struct list_head			list;
  	int					is_closed;
 -
 -	struct idr		idr;
 -	/* spinlock protects write access to idr */
 -	spinlock_t		idr_lock;
  };
  
  struct ib_uverbs_event {
@@@ -176,29 -190,14 +184,35 @@@ struct ib_ucq_object 
  	u32			async_events_reported;
  };
  
++<<<<<<< HEAD
 +extern spinlock_t ib_uverbs_idr_lock;
 +extern struct idr ib_uverbs_pd_idr;
 +extern struct idr ib_uverbs_mr_idr;
 +extern struct idr ib_uverbs_mw_idr;
 +extern struct idr ib_uverbs_ah_idr;
 +extern struct idr ib_uverbs_cq_idr;
 +extern struct idr ib_uverbs_qp_idr;
 +extern struct idr ib_uverbs_srq_idr;
 +extern struct idr ib_uverbs_xrcd_idr;
 +extern struct idr ib_uverbs_rule_idr;
 +extern struct idr ib_uverbs_wq_idr;
 +extern struct idr ib_uverbs_rwq_ind_tbl_idr;
 +
 +void idr_remove_uobj(struct idr *idp, struct ib_uobject *uobj);
 +
 +struct file *ib_uverbs_alloc_event_file(struct ib_uverbs_file *uverbs_file,
 +					struct ib_device *ib_dev,
 +					int is_async);
++=======
+ extern const struct file_operations uverbs_event_fops;
+ void ib_uverbs_init_event_file(struct ib_uverbs_event_file *ev_file);
+ struct file *ib_uverbs_alloc_async_event_file(struct ib_uverbs_file *uverbs_file,
+ 					      struct ib_device *ib_dev);
++>>>>>>> 1e7710f3f656 (IB/core: Change completion channel to use the reworked objects schema)
  void ib_uverbs_free_async_event_file(struct ib_uverbs_file *uverbs_file);
- struct ib_uverbs_event_file *ib_uverbs_lookup_comp_file(int fd);
  
  void ib_uverbs_release_ucq(struct ib_uverbs_file *file,
- 			   struct ib_uverbs_event_file *ev_file,
+ 			   struct ib_uverbs_completion_event_file *ev_file,
  			   struct ib_ucq_object *uobj);
  void ib_uverbs_release_uevent(struct ib_uverbs_file *file,
  			      struct ib_uevent_object *uobj);
diff --cc drivers/infiniband/core/uverbs_cmd.c
index 9f5575bf9d2c,b9024fa31b18..000000000000
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@@ -43,267 -47,22 +43,286 @@@
  #include "uverbs.h"
  #include "core_priv.h"
  
++<<<<<<< HEAD
 +struct uverbs_lock_class {
 +	struct lock_class_key	key;
 +	char			name[16];
 +};
 +
 +static struct uverbs_lock_class pd_lock_class	= { .name = "PD-uobj" };
 +static struct uverbs_lock_class mr_lock_class	= { .name = "MR-uobj" };
 +static struct uverbs_lock_class mw_lock_class	= { .name = "MW-uobj" };
 +static struct uverbs_lock_class cq_lock_class	= { .name = "CQ-uobj" };
 +static struct uverbs_lock_class qp_lock_class	= { .name = "QP-uobj" };
 +static struct uverbs_lock_class ah_lock_class	= { .name = "AH-uobj" };
 +static struct uverbs_lock_class srq_lock_class	= { .name = "SRQ-uobj" };
 +static struct uverbs_lock_class xrcd_lock_class = { .name = "XRCD-uobj" };
 +static struct uverbs_lock_class rule_lock_class = { .name = "RULE-uobj" };
 +static struct uverbs_lock_class wq_lock_class = { .name = "WQ-uobj" };
 +static struct uverbs_lock_class rwq_ind_table_lock_class = { .name = "IND_TBL-uobj" };
 +
 +/*
 + * The ib_uobject locking scheme is as follows:
 + *
 + * - ib_uverbs_idr_lock protects the uverbs idrs themselves, so it
 + *   needs to be held during all idr write operations.  When an object is
 + *   looked up, a reference must be taken on the object's kref before
 + *   dropping this lock.  For read operations, the rcu_read_lock()
 + *   and rcu_write_lock() but similarly the kref reference is grabbed
 + *   before the rcu_read_unlock().
 + *
 + * - Each object also has an rwsem.  This rwsem must be held for
 + *   reading while an operation that uses the object is performed.
 + *   For example, while registering an MR, the associated PD's
 + *   uobject.mutex must be held for reading.  The rwsem must be held
 + *   for writing while initializing or destroying an object.
 + *
 + * - In addition, each object has a "live" flag.  If this flag is not
 + *   set, then lookups of the object will fail even if it is found in
 + *   the idr.  This handles a reader that blocks and does not acquire
 + *   the rwsem until after the object is destroyed.  The destroy
 + *   operation will set the live flag to 0 and then drop the rwsem;
 + *   this will allow the reader to acquire the rwsem, see that the
 + *   live flag is 0, and then drop the rwsem and its reference to
 + *   object.  The underlying storage will not be freed until the last
 + *   reference to the object is dropped.
 + */
 +
 +static void init_uobj(struct ib_uobject *uobj, u64 user_handle,
 +		      struct ib_ucontext *context, struct uverbs_lock_class *c)
 +{
 +	uobj->user_handle = user_handle;
 +	uobj->context     = context;
 +	kref_init(&uobj->ref);
 +	init_rwsem(&uobj->mutex);
 +	lockdep_set_class_and_name(&uobj->mutex, &c->key, c->name);
 +	uobj->live        = 0;
 +}
 +
 +static void release_uobj(struct kref *kref)
 +{
 +	kfree_rcu(container_of(kref, struct ib_uobject, ref), rcu);
 +}
 +
 +static void put_uobj(struct ib_uobject *uobj)
 +{
 +	kref_put(&uobj->ref, release_uobj);
 +}
 +
 +static void put_uobj_read(struct ib_uobject *uobj)
 +{
 +	up_read(&uobj->mutex);
 +	put_uobj(uobj);
 +}
 +
 +static void put_uobj_write(struct ib_uobject *uobj)
 +{
 +	up_write(&uobj->mutex);
 +	put_uobj(uobj);
 +}
 +
 +static int idr_add_uobj(struct idr *idr, struct ib_uobject *uobj)
 +{
 +	int ret;
 +
 +	idr_preload(GFP_KERNEL);
 +	spin_lock(&ib_uverbs_idr_lock);
 +
 +	ret = idr_alloc(idr, uobj, 0, 0, GFP_NOWAIT);
 +	if (ret >= 0)
 +		uobj->id = ret;
 +
 +	spin_unlock(&ib_uverbs_idr_lock);
 +	idr_preload_end();
 +
 +	return ret < 0 ? ret : 0;
 +}
 +
 +void idr_remove_uobj(struct idr *idr, struct ib_uobject *uobj)
 +{
 +	spin_lock(&ib_uverbs_idr_lock);
 +	idr_remove(idr, uobj->id);
 +	spin_unlock(&ib_uverbs_idr_lock);
 +}
 +
 +static struct ib_uobject *__idr_get_uobj(struct idr *idr, int id,
 +					 struct ib_ucontext *context)
 +{
 +	struct ib_uobject *uobj;
 +
 +	rcu_read_lock();
 +	uobj = idr_find(idr, id);
 +	if (uobj) {
 +		if (uobj->context == context)
 +			kref_get(&uobj->ref);
 +		else
 +			uobj = NULL;
 +	}
 +	rcu_read_unlock();
 +
 +	return uobj;
 +}
 +
 +static struct ib_uobject *idr_read_uobj(struct idr *idr, int id,
 +					struct ib_ucontext *context, int nested)
 +{
 +	struct ib_uobject *uobj;
 +
 +	uobj = __idr_get_uobj(idr, id, context);
 +	if (!uobj)
 +		return NULL;
 +
 +	if (nested)
 +		down_read_nested(&uobj->mutex, SINGLE_DEPTH_NESTING);
 +	else
 +		down_read(&uobj->mutex);
 +	if (!uobj->live) {
 +		put_uobj_read(uobj);
 +		return NULL;
 +	}
 +
 +	return uobj;
 +}
 +
 +static struct ib_uobject *idr_write_uobj(struct idr *idr, int id,
 +					 struct ib_ucontext *context)
 +{
 +	struct ib_uobject *uobj;
 +
 +	uobj = __idr_get_uobj(idr, id, context);
 +	if (!uobj)
 +		return NULL;
 +
 +	down_write(&uobj->mutex);
 +	if (!uobj->live) {
 +		put_uobj_write(uobj);
 +		return NULL;
 +	}
 +
 +	return uobj;
 +}
 +
 +static void *idr_read_obj(struct idr *idr, int id, struct ib_ucontext *context,
 +			  int nested)
 +{
 +	struct ib_uobject *uobj;
 +
 +	uobj = idr_read_uobj(idr, id, context, nested);
 +	return uobj ? uobj->object : NULL;
 +}
 +
 +static struct ib_pd *idr_read_pd(int pd_handle, struct ib_ucontext *context)
 +{
 +	return idr_read_obj(&ib_uverbs_pd_idr, pd_handle, context, 0);
 +}
 +
 +static void put_pd_read(struct ib_pd *pd)
 +{
 +	put_uobj_read(pd->uobject);
 +}
 +
 +static struct ib_cq *idr_read_cq(int cq_handle, struct ib_ucontext *context, int nested)
 +{
 +	return idr_read_obj(&ib_uverbs_cq_idr, cq_handle, context, nested);
 +}
 +
 +static void put_cq_read(struct ib_cq *cq)
 +{
 +	put_uobj_read(cq->uobject);
 +}
 +
 +static struct ib_ah *idr_read_ah(int ah_handle, struct ib_ucontext *context)
 +{
 +	return idr_read_obj(&ib_uverbs_ah_idr, ah_handle, context, 0);
 +}
 +
 +static void put_ah_read(struct ib_ah *ah)
 +{
 +	put_uobj_read(ah->uobject);
 +}
 +
 +static struct ib_qp *idr_read_qp(int qp_handle, struct ib_ucontext *context)
 +{
 +	return idr_read_obj(&ib_uverbs_qp_idr, qp_handle, context, 0);
 +}
 +
 +static struct ib_wq *idr_read_wq(int wq_handle, struct ib_ucontext *context)
 +{
 +	return idr_read_obj(&ib_uverbs_wq_idr, wq_handle, context, 0);
 +}
 +
 +static void put_wq_read(struct ib_wq *wq)
 +{
 +	put_uobj_read(wq->uobject);
 +}
 +
 +static struct ib_rwq_ind_table *idr_read_rwq_indirection_table(int ind_table_handle,
 +							       struct ib_ucontext *context)
 +{
 +	return idr_read_obj(&ib_uverbs_rwq_ind_tbl_idr, ind_table_handle, context, 0);
 +}
 +
 +static void put_rwq_indirection_table_read(struct ib_rwq_ind_table *ind_table)
 +{
 +	put_uobj_read(ind_table->uobject);
 +}
 +
 +static struct ib_qp *idr_write_qp(int qp_handle, struct ib_ucontext *context)
 +{
 +	struct ib_uobject *uobj;
 +
 +	uobj = idr_write_uobj(&ib_uverbs_qp_idr, qp_handle, context);
 +	return uobj ? uobj->object : NULL;
 +}
 +
 +static void put_qp_read(struct ib_qp *qp)
 +{
 +	put_uobj_read(qp->uobject);
 +}
 +
 +static void put_qp_write(struct ib_qp *qp)
 +{
 +	put_uobj_write(qp->uobject);
 +}
 +
 +static struct ib_srq *idr_read_srq(int srq_handle, struct ib_ucontext *context)
 +{
 +	return idr_read_obj(&ib_uverbs_srq_idr, srq_handle, context, 0);
 +}
 +
 +static void put_srq_read(struct ib_srq *srq)
 +{
 +	put_uobj_read(srq->uobject);
 +}
 +
 +static struct ib_xrcd *idr_read_xrcd(int xrcd_handle, struct ib_ucontext *context,
 +				     struct ib_uobject **uobj)
 +{
 +	*uobj = idr_read_uobj(&ib_uverbs_xrcd_idr, xrcd_handle, context, 0);
 +	return *uobj ? (*uobj)->object : NULL;
 +}
 +
 +static void put_xrcd_read(struct ib_uobject *uobj)
 +{
 +	put_uobj_read(uobj);
++=======
+ static struct ib_uverbs_completion_event_file *
+ ib_uverbs_lookup_comp_file(int fd, struct ib_ucontext *context)
+ {
+ 	struct ib_uobject *uobj = uobj_get_read(uobj_get_type(comp_channel),
+ 						fd, context);
+ 	struct ib_uobject_file *uobj_file;
+ 
+ 	if (IS_ERR(uobj))
+ 		return (void *)uobj;
+ 
+ 	uverbs_uobject_get(uobj);
+ 	uobj_put_read(uobj);
+ 
+ 	uobj_file = container_of(uobj, struct ib_uobject_file, uobj);
+ 	return container_of(uobj_file, struct ib_uverbs_completion_event_file,
+ 			    uobj_file);
++>>>>>>> 1e7710f3f656 (IB/core: Change completion channel to use the reworked objects schema)
  }
  
  ssize_t ib_uverbs_get_context(struct ib_uverbs_file *file,
@@@ -1378,17 -978,16 +1395,18 @@@ static struct ib_ucq_object *create_cq(
  	if (cmd->comp_vector >= file->device->num_comp_vectors)
  		return ERR_PTR(-EINVAL);
  
 -	obj  = (struct ib_ucq_object *)uobj_alloc(uobj_get_type(cq),
 -						  file->ucontext);
 -	if (IS_ERR(obj))
 -		return obj;
 +	obj = kmalloc(sizeof *obj, GFP_KERNEL);
 +	if (!obj)
 +		return ERR_PTR(-ENOMEM);
 +
 +	init_uobj(&obj->uobject, cmd->user_handle, file->ucontext, &cq_lock_class);
 +	down_write(&obj->uobject.mutex);
  
  	if (cmd->comp_channel >= 0) {
- 		ev_file = ib_uverbs_lookup_comp_file(cmd->comp_channel);
- 		if (!ev_file) {
- 			ret = -EINVAL;
+ 		ev_file = ib_uverbs_lookup_comp_file(cmd->comp_channel,
+ 						     file->ucontext);
+ 		if (IS_ERR(ev_file)) {
+ 			ret = PTR_ERR(ev_file);
  			goto err;
  		}
  	}
* Unmerged path drivers/infiniband/core/uverbs_std_types.c
* Unmerged path include/rdma/uverbs_std_types.h
* Unmerged path include/rdma/uverbs_types.h
* Unmerged path drivers/infiniband/core/uverbs.h
* Unmerged path drivers/infiniband/core/uverbs_cmd.c
diff --git a/drivers/infiniband/core/uverbs_main.c b/drivers/infiniband/core/uverbs_main.c
index 1b7098d63376..41508c66d9f3 100644
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@ -167,37 +167,37 @@ static struct kobj_type ib_uverbs_dev_ktype = {
 	.release = ib_uverbs_release_dev,
 };
 
-static void ib_uverbs_release_event_file(struct kref *ref)
+static void ib_uverbs_release_async_event_file(struct kref *ref)
 {
-	struct ib_uverbs_event_file *file =
-		container_of(ref, struct ib_uverbs_event_file, ref);
+	struct ib_uverbs_async_event_file *file =
+		container_of(ref, struct ib_uverbs_async_event_file, ref);
 
 	kfree(file);
 }
 
 void ib_uverbs_release_ucq(struct ib_uverbs_file *file,
-			  struct ib_uverbs_event_file *ev_file,
+			  struct ib_uverbs_completion_event_file *ev_file,
 			  struct ib_ucq_object *uobj)
 {
 	struct ib_uverbs_event *evt, *tmp;
 
 	if (ev_file) {
-		spin_lock_irq(&ev_file->lock);
+		spin_lock_irq(&ev_file->ev_file.lock);
 		list_for_each_entry_safe(evt, tmp, &uobj->comp_list, obj_list) {
 			list_del(&evt->list);
 			kfree(evt);
 		}
-		spin_unlock_irq(&ev_file->lock);
+		spin_unlock_irq(&ev_file->ev_file.lock);
 
-		kref_put(&ev_file->ref, ib_uverbs_release_event_file);
+		uverbs_uobject_put(&ev_file->uobj_file.uobj);
 	}
 
-	spin_lock_irq(&file->async_file->lock);
+	spin_lock_irq(&file->async_file->ev_file.lock);
 	list_for_each_entry_safe(evt, tmp, &uobj->async_list, obj_list) {
 		list_del(&evt->list);
 		kfree(evt);
 	}
-	spin_unlock_irq(&file->async_file->lock);
+	spin_unlock_irq(&file->async_file->ev_file.lock);
 }
 
 void ib_uverbs_release_uevent(struct ib_uverbs_file *file,
@@ -205,12 +205,12 @@ void ib_uverbs_release_uevent(struct ib_uverbs_file *file,
 {
 	struct ib_uverbs_event *evt, *tmp;
 
-	spin_lock_irq(&file->async_file->lock);
+	spin_lock_irq(&file->async_file->ev_file.lock);
 	list_for_each_entry_safe(evt, tmp, &uobj->event_list, obj_list) {
 		list_del(&evt->list);
 		kfree(evt);
 	}
-	spin_unlock_irq(&file->async_file->lock);
+	spin_unlock_irq(&file->async_file->ev_file.lock);
 }
 
 static void ib_uverbs_detach_umcast(struct ib_qp *qp,
@@ -372,10 +372,12 @@ static void ib_uverbs_release_file(struct kref *ref)
 	kfree(file);
 }
 
-static ssize_t ib_uverbs_event_read(struct file *filp, char __user *buf,
-				    size_t count, loff_t *pos)
+static ssize_t ib_uverbs_event_read(struct ib_uverbs_event_file *file,
+				    struct ib_uverbs_file *uverbs_file,
+				    struct file *filp, char __user *buf,
+				    size_t count, loff_t *pos,
+				    bool is_async)
 {
-	struct ib_uverbs_event_file *file = filp->private_data;
 	struct ib_uverbs_event *event;
 	int eventsz;
 	int ret = 0;
@@ -394,12 +396,12 @@ static ssize_t ib_uverbs_event_read(struct file *filp, char __user *buf,
 			 * and wake_up() guarentee this will see the null set
 			 * without using RCU
 			 */
-					     !file->uverbs_file->device->ib_dev)))
+					     !uverbs_file->device->ib_dev)))
 			return -ERESTARTSYS;
 
 		/* If device was disassociated and no event exists set an error */
 		if (list_empty(&file->event_list) &&
-		    !file->uverbs_file->device->ib_dev)
+		    !uverbs_file->device->ib_dev)
 			return -EIO;
 
 		spin_lock_irq(&file->lock);
@@ -407,7 +409,7 @@ static ssize_t ib_uverbs_event_read(struct file *filp, char __user *buf,
 
 	event = list_entry(file->event_list.next, struct ib_uverbs_event, list);
 
-	if (file->is_async)
+	if (is_async)
 		eventsz = sizeof (struct ib_uverbs_async_event_desc);
 	else
 		eventsz = sizeof (struct ib_uverbs_comp_event_desc);
@@ -437,11 +439,31 @@ static ssize_t ib_uverbs_event_read(struct file *filp, char __user *buf,
 	return ret;
 }
 
-static unsigned int ib_uverbs_event_poll(struct file *filp,
+static ssize_t ib_uverbs_async_event_read(struct file *filp, char __user *buf,
+					  size_t count, loff_t *pos)
+{
+	struct ib_uverbs_async_event_file *file = filp->private_data;
+
+	return ib_uverbs_event_read(&file->ev_file, file->uverbs_file, filp,
+				    buf, count, pos, true);
+}
+
+static ssize_t ib_uverbs_comp_event_read(struct file *filp, char __user *buf,
+					 size_t count, loff_t *pos)
+{
+	struct ib_uverbs_completion_event_file *comp_ev_file =
+		filp->private_data;
+
+	return ib_uverbs_event_read(&comp_ev_file->ev_file,
+				    comp_ev_file->uobj_file.ufile, filp,
+				    buf, count, pos, false);
+}
+
+static unsigned int ib_uverbs_event_poll(struct ib_uverbs_event_file *file,
+					 struct file *filp,
 					 struct poll_table_struct *wait)
 {
 	unsigned int pollflags = 0;
-	struct ib_uverbs_event_file *file = filp->private_data;
 
 	poll_wait(filp, &file->poll_wait, wait);
 
@@ -453,49 +475,98 @@ static unsigned int ib_uverbs_event_poll(struct file *filp,
 	return pollflags;
 }
 
-static int ib_uverbs_event_fasync(int fd, struct file *filp, int on)
+static unsigned int ib_uverbs_async_event_poll(struct file *filp,
+					       struct poll_table_struct *wait)
+{
+	return ib_uverbs_event_poll(filp->private_data, filp, wait);
+}
+
+static unsigned int ib_uverbs_comp_event_poll(struct file *filp,
+					      struct poll_table_struct *wait)
+{
+	struct ib_uverbs_completion_event_file *comp_ev_file =
+		filp->private_data;
+
+	return ib_uverbs_event_poll(&comp_ev_file->ev_file, filp, wait);
+}
+
+static int ib_uverbs_async_event_fasync(int fd, struct file *filp, int on)
 {
 	struct ib_uverbs_event_file *file = filp->private_data;
 
 	return fasync_helper(fd, filp, on, &file->async_queue);
 }
 
-static int ib_uverbs_event_close(struct inode *inode, struct file *filp)
+static int ib_uverbs_comp_event_fasync(int fd, struct file *filp, int on)
 {
-	struct ib_uverbs_event_file *file = filp->private_data;
+	struct ib_uverbs_completion_event_file *comp_ev_file =
+		filp->private_data;
+
+	return fasync_helper(fd, filp, on, &comp_ev_file->ev_file.async_queue);
+}
+
+static int ib_uverbs_async_event_close(struct inode *inode, struct file *filp)
+{
+	struct ib_uverbs_async_event_file *file = filp->private_data;
+	struct ib_uverbs_file *uverbs_file = file->uverbs_file;
 	struct ib_uverbs_event *entry, *tmp;
 	int closed_already = 0;
 
-	mutex_lock(&file->uverbs_file->device->lists_mutex);
-	spin_lock_irq(&file->lock);
-	closed_already = file->is_closed;
-	file->is_closed = 1;
-	list_for_each_entry_safe(entry, tmp, &file->event_list, list) {
+	mutex_lock(&uverbs_file->device->lists_mutex);
+	spin_lock_irq(&file->ev_file.lock);
+	closed_already = file->ev_file.is_closed;
+	file->ev_file.is_closed = 1;
+	list_for_each_entry_safe(entry, tmp, &file->ev_file.event_list, list) {
 		if (entry->counter)
 			list_del(&entry->obj_list);
 		kfree(entry);
 	}
-	spin_unlock_irq(&file->lock);
+	spin_unlock_irq(&file->ev_file.lock);
 	if (!closed_already) {
 		list_del(&file->list);
-		if (file->is_async)
-			ib_unregister_event_handler(&file->uverbs_file->
-				event_handler);
+		ib_unregister_event_handler(&uverbs_file->event_handler);
+	}
+	mutex_unlock(&uverbs_file->device->lists_mutex);
+
+	kref_put(&uverbs_file->ref, ib_uverbs_release_file);
+	kref_put(&file->ref, ib_uverbs_release_async_event_file);
+
+	return 0;
+}
+
+static int ib_uverbs_comp_event_close(struct inode *inode, struct file *filp)
+{
+	struct ib_uverbs_completion_event_file *file = filp->private_data;
+	struct ib_uverbs_event *entry, *tmp;
+
+	spin_lock_irq(&file->ev_file.lock);
+	list_for_each_entry_safe(entry, tmp, &file->ev_file.event_list, list) {
+		if (entry->counter)
+			list_del(&entry->obj_list);
+		kfree(entry);
 	}
-	mutex_unlock(&file->uverbs_file->device->lists_mutex);
+	spin_unlock_irq(&file->ev_file.lock);
 
-	kref_put(&file->uverbs_file->ref, ib_uverbs_release_file);
-	kref_put(&file->ref, ib_uverbs_release_event_file);
+	uverbs_close_fd(filp);
 
 	return 0;
 }
 
-static const struct file_operations uverbs_event_fops = {
+const struct file_operations uverbs_event_fops = {
 	.owner	 = THIS_MODULE,
-	.read	 = ib_uverbs_event_read,
-	.poll    = ib_uverbs_event_poll,
-	.release = ib_uverbs_event_close,
-	.fasync  = ib_uverbs_event_fasync,
+	.read	 = ib_uverbs_comp_event_read,
+	.poll    = ib_uverbs_comp_event_poll,
+	.release = ib_uverbs_comp_event_close,
+	.fasync  = ib_uverbs_comp_event_fasync,
+	.llseek	 = no_llseek,
+};
+
+static const struct file_operations uverbs_async_event_fops = {
+	.owner	 = THIS_MODULE,
+	.read	 = ib_uverbs_async_event_read,
+	.poll    = ib_uverbs_async_event_poll,
+	.release = ib_uverbs_async_event_close,
+	.fasync  = ib_uverbs_async_event_fasync,
 	.llseek	 = no_llseek,
 };
 
@@ -542,15 +613,15 @@ static void ib_uverbs_async_handler(struct ib_uverbs_file *file,
 	struct ib_uverbs_event *entry;
 	unsigned long flags;
 
-	spin_lock_irqsave(&file->async_file->lock, flags);
-	if (file->async_file->is_closed) {
-		spin_unlock_irqrestore(&file->async_file->lock, flags);
+	spin_lock_irqsave(&file->async_file->ev_file.lock, flags);
+	if (file->async_file->ev_file.is_closed) {
+		spin_unlock_irqrestore(&file->async_file->ev_file.lock, flags);
 		return;
 	}
 
 	entry = kmalloc(sizeof *entry, GFP_ATOMIC);
 	if (!entry) {
-		spin_unlock_irqrestore(&file->async_file->lock, flags);
+		spin_unlock_irqrestore(&file->async_file->ev_file.lock, flags);
 		return;
 	}
 
@@ -559,13 +630,13 @@ static void ib_uverbs_async_handler(struct ib_uverbs_file *file,
 	entry->desc.async.reserved   = 0;
 	entry->counter               = counter;
 
-	list_add_tail(&entry->list, &file->async_file->event_list);
+	list_add_tail(&entry->list, &file->async_file->ev_file.event_list);
 	if (obj_list)
 		list_add_tail(&entry->obj_list, obj_list);
-	spin_unlock_irqrestore(&file->async_file->lock, flags);
+	spin_unlock_irqrestore(&file->async_file->ev_file.lock, flags);
 
-	wake_up_interruptible(&file->async_file->poll_wait);
-	kill_fasync(&file->async_file->async_queue, SIGIO, POLL_IN);
+	wake_up_interruptible(&file->async_file->ev_file.poll_wait);
+	kill_fasync(&file->async_file->ev_file.async_queue, SIGIO, POLL_IN);
 }
 
 void ib_uverbs_cq_event_handler(struct ib_event *event, void *context_ptr)
@@ -628,15 +699,23 @@ void ib_uverbs_event_handler(struct ib_event_handler *handler,
 
 void ib_uverbs_free_async_event_file(struct ib_uverbs_file *file)
 {
-	kref_put(&file->async_file->ref, ib_uverbs_release_event_file);
+	kref_put(&file->async_file->ref, ib_uverbs_release_async_event_file);
 	file->async_file = NULL;
 }
 
-struct file *ib_uverbs_alloc_event_file(struct ib_uverbs_file *uverbs_file,
-					struct ib_device	*ib_dev,
-					int is_async)
+void ib_uverbs_init_event_file(struct ib_uverbs_event_file *ev_file)
 {
-	struct ib_uverbs_event_file *ev_file;
+	spin_lock_init(&ev_file->lock);
+	INIT_LIST_HEAD(&ev_file->event_list);
+	init_waitqueue_head(&ev_file->poll_wait);
+	ev_file->is_closed   = 0;
+	ev_file->async_queue = NULL;
+}
+
+struct file *ib_uverbs_alloc_async_event_file(struct ib_uverbs_file *uverbs_file,
+					      struct ib_device	*ib_dev)
+{
+	struct ib_uverbs_async_event_file *ev_file;
 	struct file *filp;
 	int ret;
 
@@ -644,16 +723,11 @@ struct file *ib_uverbs_alloc_event_file(struct ib_uverbs_file *uverbs_file,
 	if (!ev_file)
 		return ERR_PTR(-ENOMEM);
 
-	kref_init(&ev_file->ref);
-	spin_lock_init(&ev_file->lock);
-	INIT_LIST_HEAD(&ev_file->event_list);
-	init_waitqueue_head(&ev_file->poll_wait);
+	ib_uverbs_init_event_file(&ev_file->ev_file);
 	ev_file->uverbs_file = uverbs_file;
 	kref_get(&ev_file->uverbs_file->ref);
-	ev_file->async_queue = NULL;
-	ev_file->is_closed   = 0;
-
-	filp = anon_inode_getfile("[infinibandevent]", &uverbs_event_fops,
+	kref_init(&ev_file->ref);
+	filp = anon_inode_getfile("[infinibandevent]", &uverbs_async_event_fops,
 				  ev_file, O_RDONLY);
 	if (IS_ERR(filp))
 		goto err_put_refs;
@@ -663,64 +737,33 @@ struct file *ib_uverbs_alloc_event_file(struct ib_uverbs_file *uverbs_file,
 		      &uverbs_file->device->uverbs_events_file_list);
 	mutex_unlock(&uverbs_file->device->lists_mutex);
 
-	if (is_async) {
-		WARN_ON(uverbs_file->async_file);
-		uverbs_file->async_file = ev_file;
-		kref_get(&uverbs_file->async_file->ref);
-		INIT_IB_EVENT_HANDLER(&uverbs_file->event_handler,
-				      ib_dev,
-				      ib_uverbs_event_handler);
-		ret = ib_register_event_handler(&uverbs_file->event_handler);
-		if (ret)
-			goto err_put_file;
-
-		/* At that point async file stuff was fully set */
-		ev_file->is_async = 1;
-	}
+	WARN_ON(uverbs_file->async_file);
+	uverbs_file->async_file = ev_file;
+	kref_get(&uverbs_file->async_file->ref);
+	INIT_IB_EVENT_HANDLER(&uverbs_file->event_handler,
+			      ib_dev,
+			      ib_uverbs_event_handler);
+	ret = ib_register_event_handler(&uverbs_file->event_handler);
+	if (ret)
+		goto err_put_file;
+
+	/* At that point async file stuff was fully set */
 
 	return filp;
 
 err_put_file:
 	fput(filp);
-	kref_put(&uverbs_file->async_file->ref, ib_uverbs_release_event_file);
+	kref_put(&uverbs_file->async_file->ref,
+		 ib_uverbs_release_async_event_file);
 	uverbs_file->async_file = NULL;
 	return ERR_PTR(ret);
 
 err_put_refs:
 	kref_put(&ev_file->uverbs_file->ref, ib_uverbs_release_file);
-	kref_put(&ev_file->ref, ib_uverbs_release_event_file);
+	kref_put(&ev_file->ref, ib_uverbs_release_async_event_file);
 	return filp;
 }
 
-/*
- * Look up a completion event file by FD.  If lookup is successful,
- * takes a ref to the event file struct that it returns; if
- * unsuccessful, returns NULL.
- */
-struct ib_uverbs_event_file *ib_uverbs_lookup_comp_file(int fd)
-{
-	struct ib_uverbs_event_file *ev_file = NULL;
-	struct fd f = fdget(fd);
-
-	if (!f.file)
-		return NULL;
-
-	if (f.file->f_op != &uverbs_event_fops)
-		goto out;
-
-	ev_file = f.file->private_data;
-	if (ev_file->is_async) {
-		ev_file = NULL;
-		goto out;
-	}
-
-	kref_get(&ev_file->ref);
-
-out:
-	fdput(f);
-	return ev_file;
-}
-
 static int verify_command_mask(struct ib_device *ib_dev, __u32 command)
 {
 	u64 mask;
@@ -1012,7 +1055,8 @@ static int ib_uverbs_close(struct inode *inode, struct file *filp)
 	mutex_unlock(&file->device->lists_mutex);
 
 	if (file->async_file)
-		kref_put(&file->async_file->ref, ib_uverbs_release_event_file);
+		kref_put(&file->async_file->ref,
+			 ib_uverbs_release_async_event_file);
 
 	kref_put(&file->ref, ib_uverbs_release_file);
 	kobject_put(&dev->kobj);
@@ -1211,7 +1255,7 @@ static void ib_uverbs_free_hw_resources(struct ib_uverbs_device *uverbs_dev,
 					struct ib_device *ib_dev)
 {
 	struct ib_uverbs_file *file;
-	struct ib_uverbs_event_file *event_file;
+	struct ib_uverbs_async_event_file *event_file;
 	struct ib_event event;
 
 	/* Pending running commands to terminate */
@@ -1258,21 +1302,20 @@ static void ib_uverbs_free_hw_resources(struct ib_uverbs_device *uverbs_dev,
 	while (!list_empty(&uverbs_dev->uverbs_events_file_list)) {
 		event_file = list_first_entry(&uverbs_dev->
 					      uverbs_events_file_list,
-					      struct ib_uverbs_event_file,
+					      struct ib_uverbs_async_event_file,
 					      list);
-		spin_lock_irq(&event_file->lock);
-		event_file->is_closed = 1;
-		spin_unlock_irq(&event_file->lock);
+		spin_lock_irq(&event_file->ev_file.lock);
+		event_file->ev_file.is_closed = 1;
+		spin_unlock_irq(&event_file->ev_file.lock);
 
 		list_del(&event_file->list);
-		if (event_file->is_async) {
-			ib_unregister_event_handler(&event_file->uverbs_file->
-						    event_handler);
-			event_file->uverbs_file->event_handler.device = NULL;
-		}
+		ib_unregister_event_handler(
+			&event_file->uverbs_file->event_handler);
+		event_file->uverbs_file->event_handler.device =
+			NULL;
 
-		wake_up_interruptible(&event_file->poll_wait);
-		kill_fasync(&event_file->async_queue, SIGIO, POLL_IN);
+		wake_up_interruptible(&event_file->ev_file.poll_wait);
+		kill_fasync(&event_file->ev_file.async_queue, SIGIO, POLL_IN);
 	}
 	mutex_unlock(&uverbs_dev->lists_mutex);
 }
* Unmerged path drivers/infiniband/core/uverbs_std_types.c
* Unmerged path include/rdma/uverbs_std_types.h
* Unmerged path include/rdma/uverbs_types.h
